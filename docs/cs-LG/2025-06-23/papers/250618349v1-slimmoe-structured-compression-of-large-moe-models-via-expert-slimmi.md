---
layout: default
title: SlimMoE: Structured Compression of Large MoE Models via Expert Slimming and Distillation
---

# SlimMoE: Structured Compression of Large MoE Models via Expert Slimming and Distillation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.18349" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.18349v1</a>
  <a href="https://arxiv.org/pdf/2506.18349.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.18349v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.18349v1', 'SlimMoE: Structured Compression of Large MoE Models via Expert Slimming and Distillation')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Zichong Li, Chen Liang, Zixuan Zhang, Ilgee Hong, Young Jin Kim, Weizhu Chen, Tuo Zhao

**ÂàÜÁ±ª**: cs.LG, cs.CL

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-23

**üîó ‰ª£Á†Å/È°πÁõÆ**: [HUGGINGFACE](https://huggingface.co/microsoft/Phi-mini-MoE-instruct) | [HUGGINGFACE](https://huggingface.co/microsoft/Phi-tiny-MoE-instruct)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫SlimMoE‰ª•Ëß£ÂÜ≥Â§ßËßÑÊ®°MoEÊ®°ÂûãÁöÑÂéãÁº©‰∏éÈÉ®ÁΩ≤ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Ê∑∑Âêà‰∏ìÂÆ∂` `Ê®°ÂûãÂéãÁº©` `Áü•ËØÜËí∏È¶è` `Ê∑±Â∫¶Â≠¶‰π†` `ËµÑÊ∫ê‰ºòÂåñ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑMoEÊ®°ÂûãÂú®ÂÜÖÂ≠òÈúÄÊ±Ç‰∏äÊûÅ‰∏∫Â∫ûÂ§ßÔºåÂØºËá¥Âú®ËµÑÊ∫êÂèóÈôêÁéØÂ¢É‰∏≠Èöæ‰ª•ËøõË°åÂæÆË∞ÉÂíåÈÉ®ÁΩ≤„ÄÇ
2. SlimMoEÈÄöËøáÂ§öÈò∂ÊÆµÂéãÁº©Ê°ÜÊû∂ÔºåÁ≤æÁÆÄ‰∏ìÂÆ∂Âπ∂ÈÄöËøáÁü•ËØÜËΩ¨ÁßªÊù•ÂáèÂ∞ëÂèÇÊï∞Êï∞ÈáèÔºåÈÅøÂÖç‰∫ÜÊÄßËÉΩ‰∏ãÈôç„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÂéãÁº©ÂêéÁöÑPhi-mini-MoEÂíåPhi-tiny-MoEÂú®ÊÄßËÉΩ‰∏ä‰ºò‰∫éÂêåÁ±ªÊ®°ÂûãÔºå‰∏îÂª∂ËøüÊòæËëóÈôç‰Ωé„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Ê∑∑Âêà‰∏ìÂÆ∂ÔºàMoEÔºâÊû∂ÊûÑÂ∑≤Êàê‰∏∫Êâ©Â±ïÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂπ∂‰øùÊåÅÊé®ÁêÜÊïàÁéáÁöÑÂº∫Â§ßËåÉÂºè„ÄÇÁÑ∂ËÄåÔºåÂÖ∂Â∑®Â§ßÁöÑÂÜÖÂ≠òÈúÄÊ±Ç‰ΩøÂæóÂú®ËµÑÊ∫êÂèóÈôêÁéØÂ¢É‰∏≠ËøõË°åÂæÆË∞ÉÊàñÈÉ®ÁΩ≤ÂèòÂæóÊûÅ‰∏∫ÊòÇË¥µ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÊåëÊàòÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜSlimMoEÔºå‰∏Ä‰∏™Â§öÈò∂ÊÆµÂéãÁº©Ê°ÜÊû∂ÔºåÊó®Âú®Â∞ÜÂ§ßÂûãMoEÊ®°ÂûãËΩ¨Âåñ‰∏∫Êõ¥Â∞è„ÄÅÊõ¥È´òÊïàÁöÑÂèò‰ΩìÔºåËÄåÊó†ÈúÄ‰ªéÂ§¥ÂºÄÂßãËÆ≠ÁªÉ„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÁ≤æÁÆÄ‰∏ìÂÆ∂ÂíåÈÄöËøá‰∏≠Èó¥Èò∂ÊÆµËΩ¨ÁßªÁü•ËØÜÔºåÁ≥ªÁªüÊÄßÂú∞ÂáèÂ∞ëÂèÇÊï∞Êï∞ÈáèÔºåÊúâÊïàÁºìËß£‰∫Ü‰∏ÄÊ¨°ÊÄßÂâ™ÊûùÊñπÊ≥ï‰∏≠Â∏∏ËßÅÁöÑÊÄßËÉΩ‰∏ãÈôçÈóÆÈ¢ò„ÄÇÂÆûÈ™åË°®ÊòéÔºåÂéãÁº©ÂêéÁöÑÊ®°ÂûãÂú®ÊÄßËÉΩ‰∏ä‰ºò‰∫éÂêåÁ±ªÊ®°ÂûãÔºåÂπ∂‰∏éÊõ¥Â§ßÊ®°ÂûãÁ´û‰∫â„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Â§ßÂûãMoEÊ®°ÂûãÂú®ËµÑÊ∫êÂèóÈôêÁéØÂ¢É‰∏≠ÁöÑÈ´òÂÜÖÂ≠òÈúÄÊ±ÇÂíåÂæÆË∞ÉÂõ∞ÈöæÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®ËøõË°åÊ®°ÂûãÂéãÁº©Êó∂ÔºåÂæÄÂæÄ‰ºöÂØºËá¥ÊÄßËÉΩÊòæËëó‰∏ãÈôç„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöSlimMoEÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÂ§öÈò∂ÊÆµÁöÑÂéãÁº©ËøáÁ®ãÔºåÈÄêÊ≠•Á≤æÁÆÄÊ®°Âûã‰∏≠ÁöÑ‰∏ìÂÆ∂ÔºåÂπ∂ÈÄöËøáÁü•ËØÜËí∏È¶èÊù•‰øùÊåÅÊ®°ÂûãÊÄßËÉΩ„ÄÇËøôÁßçËÆæËÆ°Êó®Âú®ÊúâÊïàÈôç‰ΩéÂèÇÊï∞Êï∞ÈáèÔºåÂêåÊó∂ÂáèÂ∞ë‰∏ÄÊ¨°ÊÄßÂâ™ÊûùÂ∏¶Êù•ÁöÑÊÄßËÉΩÊçüÂ§±„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöSlimMoEÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨Â§ö‰∏™Èò∂ÊÆµÔºåÈ¶ñÂÖàËøõË°å‰∏ìÂÆ∂ÁöÑÁ≤æÁÆÄÔºåÁÑ∂ÂêéÈÄöËøá‰∏≠Èó¥Èò∂ÊÆµÁöÑÁü•ËØÜËΩ¨ÁßªÔºåÊúÄÂêéÂæóÂà∞ÂéãÁº©ÂêéÁöÑÊ®°Âûã„ÄÇÊØè‰∏™Èò∂ÊÆµÈÉΩÊúâÊòéÁ°ÆÁöÑÁõÆÊ†áÔºå‰ª•Á°Æ‰øùÊúÄÁªàÊ®°ÂûãÁöÑÊÄßËÉΩÂíåÊïàÁéá„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊú¨ÊñáÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÂ∞ÜÁªìÊûÑÂåñÂâ™Êûù‰∏éÂàÜÈò∂ÊÆµËí∏È¶èÁõ∏ÁªìÂêàÔºåÂΩ¢Êàê‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂéãÁº©Á≠ñÁï•„ÄÇËøô‰∏éÁé∞ÊúâÁöÑ‰∏ÄÊ¨°ÊÄßÂâ™ÊûùÊñπÊ≥ïÊú¨Ë¥®‰∏ä‰∏çÂêåÔºåÂêéËÄÖÂæÄÂæÄÊó†Ê≥ïÊúâÊïà‰øùÊåÅÊ®°ÂûãÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Ê®°ÂûãÂéãÁº©ËøáÁ®ã‰∏≠ÔºåÂÖ≥ÈîÆÁöÑÂèÇÊï∞ËÆæÁΩÆÂíåÊçüÂ§±ÂáΩÊï∞ËÆæËÆ°Á°Æ‰øù‰∫ÜÁü•ËØÜÁöÑÊúâÊïàËΩ¨Áßª„ÄÇÊ≠§Â§ñÔºåÁΩëÁªúÁªìÊûÑÁöÑË∞ÉÊï¥‰πüÁªèËøáÁ≤æÂøÉËÆæËÆ°Ôºå‰ª•ÈÄÇÂ∫î‰∏çÂêåÈò∂ÊÆµÁöÑÈúÄÊ±Ç„ÄÇÂÖ∑‰ΩìÁªÜËäÇÂåÖÊã¨‰ΩøÁî®400BÁöÑËÆ≠ÁªÉÊï∞ÊçÆËøõË°åÂéãÁº©ÔºåÊúÄÁªàÂæóÂà∞ÁöÑPhi-mini-MoEÂíåPhi-tiny-MoEÂú®ÂèÇÊï∞Êï∞Èáè‰∏äÂ§ßÂπÖÂáèÂ∞ë„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåPhi-mini-MoEÂú®ÊøÄÊ¥ªÂèÇÊï∞Êï∞ÈáèÂáèÂ∞ëËá≥2/3ÁöÑÊÉÖÂÜµ‰∏ãÔºåÊÄßËÉΩ‰∏éPhi-3-miniÁõ∏ÂΩìÔºå‰∏îÂú®MMLUËØÑÂàÜ‰∏ä‰∏éLlama 3.1 8BÊ®°ÂûãÁõ∏Â™≤ÁæéÔºåÂ∞ΩÁÆ°Âª∂ËøüÊòæËëóÈôç‰Ωé„ÄÇËøô‰∫õÁªìÊûúÂ±ïÁ§∫‰∫ÜSlimMoEÂú®Ê®°ÂûãÂéãÁº©‰∏éÊÄßËÉΩ‰øùÊåÅÊñπÈù¢ÁöÑÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®Âú∫ÊôØÂåÖÊã¨Â≠¶ÊúØÁ†îÁ©∂„ÄÅËµÑÊ∫êÂèóÈôêÁöÑÂ∑•‰∏öÂ∫îÁî®‰ª•ÂèäÈúÄË¶ÅÈ´òÊïàÊé®ÁêÜÁöÑÁßªÂä®ËÆæÂ§á„ÄÇSlimMoEÁöÑÂéãÁº©Ê®°ÂûãËÉΩÂ§üÂú®Âçï‰∏™GPU‰∏äËøõË°åÂæÆË∞ÉÔºå‰ΩøÂÖ∂Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÂÖ∑ÊúâËæÉÈ´òÁöÑ‰ª∑ÂÄº„ÄÇÊú™Êù•ÔºåÈöèÁùÄMoEÊû∂ÊûÑÁöÑÂπøÊ≥õÈááÁî®ÔºåSlimMoEÂèØËÉΩ‰ºöÊé®Âä®Êõ¥Â§öÈ´òÊïàÊ®°ÂûãÁöÑÂºÄÂèë‰∏éÂ∫îÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> The Mixture of Experts (MoE) architecture has emerged as a powerful paradigm for scaling large language models (LLMs) while maintaining inference efficiency. However, their enormous memory requirements make them prohibitively expensive to fine-tune or deploy in resource-constrained environments. To address this challenge, we introduce SlimMoE, a multi-stage compression framework for transforming large MoE models into much smaller, efficient variants without incurring the prohibitive costs of training from scratch. Our method systematically reduces parameter counts by slimming experts and transferring knowledge through intermediate stages, effectively mitigating the performance degradation common in one-shot pruning approaches. Using this framework, we compress Phi 3.5-MoE (41.9B total/6.6B activated parameters) to create Phi-mini-MoE (7.6B total/2.4B activated parameters) and Phi-tiny-MoE (3.8B total/1.1B activated parameters) using only 400B tokens--less than 10% of the original model's training data. These compressed models can be fine-tuned on a single GPU (A100 for Phi-mini-MoE, A6000 for Phi-tiny-MoE), making them highly suitable for academic and resource-limited settings. Our experiments demonstrate that these compressed models outperform others of similar size and remain competitive with larger models. For instance, Phi-mini-MoE achieves similar or better performance to Phi-3-mini using only 2/3 of the activated parameters and yields comparable MMLU scores to Llama 3.1 8B despite having significantly lower latency. Our findings demonstrate that structured pruning combined with staged distillation offers an effective path to creating high-quality, compact MoE models, paving the way for broader adoption of MoE architectures. We make our models publicly available at https://huggingface.co/microsoft/Phi-mini-MoE-instruct and https://huggingface.co/microsoft/Phi-tiny-MoE-instruct .

