---
layout: default
title: Enhancing Semi-Supervised Multi-View Graph Convolutional Networks via Supervised Contrastive Learning and Self-Training
---

# Enhancing Semi-Supervised Multi-View Graph Convolutional Networks via Supervised Contrastive Learning and Self-Training

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.13770" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.13770v1</a>
  <a href="https://arxiv.org/pdf/2512.13770.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.13770v1" onclick="toggleFavorite(this, '2512.13770v1', 'Enhancing Semi-Supervised Multi-View Graph Convolutional Networks via Supervised Contrastive Learning and Self-Training')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Huaiyuan Xiao, Fadi Dornaika, Jingjun Bi

**åˆ†ç±»**: cs.LG, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-15

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/HuaiyuanXiao/MVSupGCN)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMV-SupGCNï¼Œé€šè¿‡ç›‘ç£å¯¹æ¯”å­¦ä¹ å’Œè‡ªè®­ç»ƒå¢å¼ºåŠç›‘ç£å¤šè§†å›¾å›¾å·ç§¯ç½‘ç»œ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¤šè§†å›¾å­¦ä¹ ` `å›¾å·ç§¯ç½‘ç»œ` `åŠç›‘ç£å­¦ä¹ ` `å¯¹æ¯”å­¦ä¹ ` `è‡ªè®­ç»ƒ` `ç›‘ç£å¯¹æ¯”å­¦ä¹ ` `å›¾æ„å»º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºGCNçš„å¤šè§†å›¾å­¦ä¹ æ–¹æ³•éš¾ä»¥å……åˆ†åˆ©ç”¨è·¨è§†å›¾äº’è¡¥ä¿¡æ¯ï¼Œå¯¼è‡´ç‰¹å¾è¡¨ç¤ºæ¬¡ä¼˜å’Œæ€§èƒ½å—é™ã€‚
2. æå‡ºMV-SupGCNï¼Œç»“åˆç›‘ç£å¯¹æ¯”å­¦ä¹ ã€å¤šå›¾æ„å»ºå’Œè‡ªè®­ç»ƒï¼Œä»¥æå‡åŠç›‘ç£å¤šè§†å›¾å­¦ä¹ æ€§èƒ½ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒMV-SupGCNåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¶…è¶Šäº†ç°æœ‰æœ€ä¼˜æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºäºå›¾å·ç§¯ç½‘ç»œ(GCN)çš„å¤šè§†å›¾å­¦ä¹ ä¸ºæ•´åˆå¼‚æ„è§†å›¾çš„ç»“æ„ä¿¡æ¯æä¾›äº†ä¸€ä¸ªå¼ºå¤§çš„æ¡†æ¶ï¼Œä»è€Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å»ºæ¨¡å¤æ‚çš„å¤šè§†å›¾æ•°æ®ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•é€šå¸¸ä¸èƒ½å……åˆ†åˆ©ç”¨è·¨è§†å›¾çš„äº’è¡¥ä¿¡æ¯ï¼Œå¯¼è‡´æ¬¡ä¼˜çš„ç‰¹å¾è¡¨ç¤ºå’Œæœ‰é™çš„æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MV-SupGCNï¼Œä¸€ä¸ªåŠç›‘ç£GCNæ¨¡å‹ï¼Œå®ƒé›†æˆäº†å‡ ä¸ªäº’è¡¥çš„ç»„ä»¶ï¼Œå…·æœ‰æ¸…æ™°çš„åŠ¨æœºå’Œç›¸äº’åŠ å¼ºçš„ä½œç”¨ã€‚é¦–å…ˆï¼Œä¸ºäº†æ›´å¥½åœ°æ•è·åˆ¤åˆ«æ€§ç‰¹å¾å¹¶æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªè”åˆæŸå¤±å‡½æ•°ï¼Œå°†äº¤å‰ç†µæŸå¤±ä¸ç›‘ç£å¯¹æ¯”æŸå¤±ç›¸ç»“åˆï¼Œé¼“åŠ±æ¨¡å‹åŒæ—¶æœ€å°åŒ–ç±»å†…æ–¹å·®å’Œæœ€å¤§åŒ–æ½œåœ¨ç©ºé—´ä¸­çš„ç±»é—´å¯åˆ†æ€§ã€‚å…¶æ¬¡ï¼Œè®¤è¯†åˆ°å•ä¸€å›¾æ„å»ºæ–¹æ³•çš„ä¸ç¨³å®šæ€§å’Œä¸å®Œæ•´æ€§ï¼Œæˆ‘ä»¬å°†åŸºäºKNNå’ŒåŠç›‘ç£çš„å›¾æ„å»ºæ–¹æ³•ç»“åˆåœ¨æ¯ä¸ªè§†å›¾ä¸Šï¼Œä»è€Œå¢å¼ºäº†æ•°æ®ç»“æ„è¡¨ç¤ºçš„é²æ£’æ€§ï¼Œå¹¶é™ä½äº†æ³›åŒ–è¯¯å·®ã€‚ç¬¬ä¸‰ï¼Œä¸ºäº†æœ‰æ•ˆåœ°åˆ©ç”¨å¤§é‡çš„æœªæ ‡è®°æ•°æ®å¹¶å¢å¼ºå¤šä¸ªè§†å›¾ä¹‹é—´çš„è¯­ä¹‰å¯¹é½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é›†æˆäº†å¯¹æ¯”å­¦ä¹ ï¼Œä»¥å¢å¼ºå¤šè§†å›¾åµŒå…¥ä¹‹é—´çš„ä¸€è‡´æ€§å¹¶æ•è·æœ‰æ„ä¹‰çš„è§†å›¾é—´å…³ç³»ï¼Œä»¥åŠä¼ªæ ‡ç­¾ï¼Œå®ƒæä¾›äº†é¢å¤–çš„ç›‘ç£ï¼Œåº”ç”¨äºäº¤å‰ç†µå’Œå¯¹æ¯”æŸå¤±å‡½æ•°ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚å¤§é‡çš„å®éªŒè¡¨æ˜ï¼ŒMV-SupGCNå§‹ç»ˆä¼˜äºå¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„æœ€å…ˆè¿›æ–¹æ³•ï¼ŒéªŒè¯äº†æˆ‘ä»¬é›†æˆæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤šè§†å›¾å›¾å·ç§¯ç½‘ç»œæ–¹æ³•åœ¨åˆ©ç”¨ä¸åŒè§†å›¾ä¹‹é—´çš„äº’è¡¥ä¿¡æ¯æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå¯¼è‡´å­¦ä¹ åˆ°çš„ç‰¹å¾è¡¨ç¤ºä¸å¤Ÿå…·æœ‰åŒºåˆ†æ€§ï¼Œæ¨¡å‹æ³›åŒ–èƒ½åŠ›è¾ƒå·®ã€‚æ­¤å¤–ï¼Œå•ä¸€çš„å›¾æ„å»ºæ–¹æ³•å®¹æ˜“å—åˆ°å™ªå£°çš„å½±å“ï¼Œå¯¼è‡´æ•°æ®ç»“æ„è¡¨ç¤ºä¸ç¨³å®šã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMV-SupGCNçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ç»“åˆç›‘ç£å¯¹æ¯”å­¦ä¹ ã€å¤šå›¾æ„å»ºå’Œè‡ªè®­ç»ƒï¼Œæ¥å¢å¼ºæ¨¡å‹å¯¹å¤šè§†å›¾æ•°æ®çš„ç†è§£å’Œæ³›åŒ–èƒ½åŠ›ã€‚ç›‘ç£å¯¹æ¯”å­¦ä¹ æ—¨åœ¨å­¦ä¹ æ›´å…·åŒºåˆ†æ€§çš„ç‰¹å¾è¡¨ç¤ºï¼Œå¤šå›¾æ„å»ºå¢å¼ºæ•°æ®ç»“æ„è¡¨ç¤ºçš„é²æ£’æ€§ï¼Œè‡ªè®­ç»ƒåˆ™åˆ©ç”¨æœªæ ‡è®°æ•°æ®æå‡æ¨¡å‹æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMV-SupGCNçš„æ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) å¤šè§†å›¾å›¾æ„å»ºï¼šå¯¹æ¯ä¸ªè§†å›¾åˆ†åˆ«ä½¿ç”¨KNNå’ŒåŠç›‘ç£æ–¹æ³•æ„å»ºå›¾ï¼›2) ç‰¹å¾ç¼–ç ï¼šä½¿ç”¨å›¾å·ç§¯ç½‘ç»œå¯¹æ¯ä¸ªè§†å›¾çš„å›¾ç»“æ„æ•°æ®è¿›è¡Œç‰¹å¾ç¼–ç ï¼›3) ç›‘ç£å¯¹æ¯”å­¦ä¹ ï¼šç»“åˆäº¤å‰ç†µæŸå¤±å’Œç›‘ç£å¯¹æ¯”æŸå¤±ï¼Œä¼˜åŒ–ç‰¹å¾è¡¨ç¤ºï¼›4) è‡ªè®­ç»ƒï¼šä½¿ç”¨ä¼ªæ ‡ç­¾å¯¹æœªæ ‡è®°æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå¹¶å°†å…¶çº³å…¥äº¤å‰ç†µæŸå¤±å’Œå¯¹æ¯”æŸå¤±ä¸­ï¼›5) å¤šè§†å›¾ä¸€è‡´æ€§ï¼šé€šè¿‡å¯¹æ¯”å­¦ä¹ ï¼Œå¢å¼ºå¤šè§†å›¾åµŒå…¥ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šMV-SupGCNçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) ç»“åˆç›‘ç£å¯¹æ¯”å­¦ä¹ å’Œäº¤å‰ç†µæŸå¤±ï¼Œæå‡ç‰¹å¾è¡¨ç¤ºçš„åŒºåˆ†æ€§ï¼›2) é‡‡ç”¨å¤šå›¾æ„å»ºæ–¹æ³•ï¼Œå¢å¼ºæ•°æ®ç»“æ„è¡¨ç¤ºçš„é²æ£’æ€§ï¼›3) å°†å¯¹æ¯”å­¦ä¹ å’Œä¼ªæ ‡ç­¾èå…¥è‡ªè®­ç»ƒæ¡†æ¶ï¼Œæœ‰æ•ˆåˆ©ç”¨æœªæ ‡è®°æ•°æ®å¹¶å¢å¼ºå¤šè§†å›¾ä¸€è‡´æ€§ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒMV-SupGCNæ›´å…¨é¢åœ°è€ƒè™‘äº†å¤šè§†å›¾å­¦ä¹ ä¸­çš„å…³é”®é—®é¢˜ï¼Œå¹¶æå‡ºäº†ç›¸åº”çš„è§£å†³æ–¹æ¡ˆã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å›¾æ„å»ºæ–¹é¢ï¼Œç»“åˆäº†KNNå’ŒåŠç›‘ç£å›¾æ„å»ºæ–¹æ³•ï¼Œå…·ä½“å‚æ•°è®¾ç½®æœªçŸ¥ã€‚æŸå¤±å‡½æ•°æ–¹é¢ï¼Œä½¿ç”¨äº†äº¤å‰ç†µæŸå¤±å’Œç›‘ç£å¯¹æ¯”æŸå¤±çš„åŠ æƒå’Œï¼Œæƒé‡å‚æ•°æœªçŸ¥ã€‚åœ¨è‡ªè®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¼ªæ ‡ç­¾çš„ç”Ÿæˆæ–¹å¼å’Œç½®ä¿¡åº¦é˜ˆå€¼æœªçŸ¥ã€‚ç½‘ç»œç»“æ„æ–¹é¢ï¼ŒGCNçš„å…·ä½“å±‚æ•°å’Œéšè—å±‚ç»´åº¦æœªçŸ¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMV-SupGCNåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æœ€ä¼˜æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨æ•°æ®é›†DBLPä¸Šï¼ŒMV-SupGCNçš„å‡†ç¡®ç‡æ¯”ç°æœ‰æœ€ä¼˜æ–¹æ³•æé«˜äº†è¶…è¿‡2%ã€‚è¿™äº›ç»“æœéªŒè¯äº†MV-SupGCNåœ¨åŠç›‘ç£å¤šè§†å›¾å­¦ä¹ ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MV-SupGCNå¯åº”ç”¨äºå„ç§å¤šè§†å›¾æ•°æ®åˆ†æä»»åŠ¡ï¼Œä¾‹å¦‚ç¤¾äº¤ç½‘ç»œåˆ†æã€å›¾åƒåˆ†ç±»ã€ç”Ÿç‰©ä¿¡æ¯å­¦ç­‰ã€‚é€šè¿‡æ•´åˆæ¥è‡ªä¸åŒæ¥æºçš„ä¿¡æ¯ï¼ŒMV-SupGCNèƒ½å¤Ÿæ›´å‡†ç¡®åœ°ç†è§£å¤æ‚æ•°æ®ï¼Œä¸ºå†³ç­–æä¾›æ›´å¯é çš„ä¾æ®ã€‚è¯¥ç ”ç©¶çš„æˆæœæœ‰åŠ©äºæ¨åŠ¨å¤šè§†å›¾å­¦ä¹ åœ¨å®é™…åº”ç”¨ä¸­çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The advent of graph convolutional network (GCN)-based multi-view learning provides a powerful framework for integrating structural information from heterogeneous views, enabling effective modeling of complex multi-view data. However, existing methods often fail to fully exploit the complementary information across views, leading to suboptimal feature representations and limited performance. To address this, we propose MV-SupGCN, a semi-supervised GCN model that integrates several complementary components with clear motivations and mutual reinforcement. First, to better capture discriminative features and improve model generalization, we design a joint loss function that combines Cross-Entropy loss with Supervised Contrastive loss, encouraging the model to simultaneously minimize intra-class variance and maximize inter-class separability in the latent space. Second, recognizing the instability and incompleteness of single graph construction methods, we combine both KNN-based and semi-supervised graph construction approaches on each view, thereby enhancing the robustness of the data structure representation and reducing generalization error. Third, to effectively utilize abundant unlabeled data and enhance semantic alignment across multiple views, we propose a unified framework that integrates contrastive learning in order to enforce consistency among multi-view embeddings and capture meaningful inter-view relationships, together with pseudo-labeling, which provides additional supervision applied to both the cross-entropy and contrastive loss functions to enhance model generalization. Extensive experiments demonstrate that MV-SupGCN consistently surpasses state-of-the-art methods across multiple benchmarks, validating the effectiveness of our integrated approach. The source code is available at https://github.com/HuaiyuanXiao/MVSupGCN

