---
layout: default
title: TROFI: Trajectory-Ranked Offline Inverse Reinforcement Learning
---

# TROFI: Trajectory-Ranked Offline Inverse Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.22008" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.22008v1</a>
  <a href="https://arxiv.org/pdf/2506.22008.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.22008v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.22008v1', 'TROFI: Trajectory-Ranked Offline Inverse Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Alessandro Sestini, Joakim Bergdahl, Konrad Tollmar, Andrew D. Bagdanov, Linus GisslÃ©n

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-27

**å¤‡æ³¨**: Published at Reinforcement Learning and Video Games Workshop at RLC 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTROFIä»¥è§£å†³ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­çš„å¥–åŠ±å‡½æ•°ç¼ºå¤±é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `ç¦»çº¿å¼ºåŒ–å­¦ä¹ ` `é€†å¼ºåŒ–å­¦ä¹ ` `å¥–åŠ±å‡½æ•°` `äººç±»åå¥½` `ç­–ç•¥å­¦ä¹ ` `è§†é¢‘æ¸¸æˆå¼€å‘` `æœºå™¨äººæ§åˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¾èµ–äºé¢„å®šä¹‰çš„å¥–åŠ±å‡½æ•°ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå°¤å…¶æ˜¯è§†é¢‘æ¸¸æˆå¼€å‘ä¸­ï¼Œå¥–åŠ±å‡½æ•°çš„å¯ç”¨æ€§å¾€å¾€ä¸ç¡®å®šã€‚
2. TROFIé€šè¿‡ä»äººç±»åå¥½ä¸­å­¦ä¹ å¥–åŠ±å‡½æ•°ï¼Œè¿›è€Œå¯¹åŸå§‹æ•°æ®é›†è¿›è¡Œæ ‡è®°ï¼Œä½¿å¾—åœ¨æ²¡æœ‰æœ€ä¼˜è½¨è¿¹çš„æƒ…å†µä¸‹ä¹Ÿèƒ½æœ‰æ•ˆè®­ç»ƒç­–ç•¥ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒTROFIåœ¨D4RLåŸºå‡†ä¸Šè¡¨ç°ä¼˜äºåŸºçº¿ï¼Œå¹¶ä¸”åœ¨3Dæ¸¸æˆç¯å¢ƒä¸­éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºäº†è‰¯å¥½çš„ç­–ç•¥å­¦ä¹ èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œæ™ºèƒ½ä½“ä»…ä½¿ç”¨å›ºå®šçš„å­˜å‚¨è¿‡æ¸¡æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œä½†è¿™éœ€è¦æ•°æ®é›†ç”±å¥–åŠ±å‡½æ•°æ ‡è®°ã€‚åœ¨è§†é¢‘æ¸¸æˆå¼€å‘ç­‰åº”ç”¨åœºæ™¯ä¸­ï¼Œå¥–åŠ±å‡½æ•°çš„å¯ç”¨æ€§å¹¶ä¸æ€»æ˜¯å¾—åˆ°ä¿è¯ã€‚æœ¬æ–‡æå‡ºäº†è½¨è¿¹æ’åç¦»çº¿é€†å¼ºåŒ–å­¦ä¹ ï¼ˆTROFIï¼‰ï¼Œä¸€ç§åœ¨æ²¡æœ‰é¢„å®šä¹‰å¥–åŠ±å‡½æ•°çš„æƒ…å†µä¸‹æœ‰æ•ˆå­¦ä¹ ç­–ç•¥çš„æ–°æ–¹æ³•ã€‚TROFIé¦–å…ˆä»äººç±»åå¥½ä¸­å­¦ä¹ å¥–åŠ±å‡½æ•°ï¼Œç„¶ååˆ©ç”¨è¯¥å‡½æ•°å¯¹åŸå§‹æ•°æ®é›†è¿›è¡Œæ ‡è®°ï¼Œä½¿å…¶å¯ç”¨äºç­–ç•¥è®­ç»ƒã€‚ä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸éœ€è¦æœ€ä¼˜è½¨è¿¹ã€‚é€šè¿‡åœ¨D4RLåŸºå‡†ä¸Šçš„å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†TROFIåœ¨æ€§èƒ½ä¸Šå§‹ç»ˆä¼˜äºåŸºçº¿ï¼Œå¹¶ä¸”ä¸ä½¿ç”¨çœŸå®å¥–åŠ±å­¦ä¹ ç­–ç•¥çš„æ•ˆæœç›¸å½“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨3Dæ¸¸æˆç¯å¢ƒä¸­éªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­ç¼ºä¹é¢„å®šä¹‰å¥–åŠ±å‡½æ•°çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºæœ€ä¼˜è½¨è¿¹è¿›è¡Œè®­ç»ƒï¼Œé™åˆ¶äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„çµæ´»æ€§å’Œé€‚ç”¨æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šTROFIçš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡äººç±»åå¥½å­¦ä¹ å¥–åŠ±å‡½æ•°ï¼Œå¹¶åˆ©ç”¨è¯¥å‡½æ•°å¯¹åŸå§‹æ•°æ®é›†è¿›è¡Œæ ‡è®°ï¼Œä»è€Œä½¿å¾—ç­–ç•¥å­¦ä¹ ä¸å†ä¾èµ–äºæœ€ä¼˜è½¨è¿¹ã€‚è¿™æ ·çš„è®¾è®¡ä½¿å¾—æ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­æ›´å…·é€‚åº”æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTROFIçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆæ˜¯ä»äººç±»åå¥½ä¸­å­¦ä¹ å¥–åŠ±å‡½æ•°ï¼Œå…¶æ¬¡æ˜¯åˆ©ç”¨å­¦ä¹ åˆ°çš„å¥–åŠ±å‡½æ•°å¯¹åŸå§‹æ•°æ®é›†è¿›è¡Œæ ‡è®°ï¼Œæœ€ååœ¨æ ‡è®°åçš„æ•°æ®é›†ä¸Šè®­ç»ƒç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šTROFIçš„æœ€å¤§åˆ›æ–°åœ¨äºå…¶ä¸ä¾èµ–äºæœ€ä¼˜è½¨è¿¹ï¼Œèƒ½å¤Ÿåœ¨ç¼ºä¹å¥–åŠ±å‡½æ•°çš„æƒ…å†µä¸‹æœ‰æ•ˆå­¦ä¹ ç­–ç•¥ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„é€†å¼ºåŒ–å­¦ä¹ æ–¹æ³•å½¢æˆäº†é²œæ˜å¯¹æ¯”ï¼Œåè€…é€šå¸¸éœ€è¦æœ€ä¼˜è½¨è¿¹ä½œä¸ºè¾“å…¥ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒTROFIé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–å¥–åŠ±æ¨¡å‹ï¼Œå¹¶åœ¨ç½‘ç»œç»“æ„ä¸Šè¿›è¡Œäº†è°ƒæ•´ï¼Œä»¥ç¡®ä¿å¥–åŠ±å‡½æ•°èƒ½å¤Ÿæœ‰æ•ˆåœ°åæ˜ äººç±»åå¥½ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨å®éªŒéƒ¨åˆ†è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨D4RLåŸºå‡†æµ‹è¯•ä¸­ï¼ŒTROFIçš„è¡¨ç°å§‹ç»ˆä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œä¸”åœ¨æŸäº›ä»»åŠ¡ä¸­ä¸ä½¿ç”¨çœŸå®å¥–åŠ±å‡½æ•°å­¦ä¹ çš„ç­–ç•¥æ•ˆæœç›¸å½“ï¼Œå±•ç¤ºäº†å…¶å¼ºå¤§çš„ç­–ç•¥å­¦ä¹ èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œåœ¨3Dæ¸¸æˆç¯å¢ƒä¸­çš„éªŒè¯è¿›ä¸€æ­¥æ”¯æŒäº†TROFIçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

TROFIçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ï¼Œå°¤å…¶æ˜¯åœ¨è§†é¢‘æ¸¸æˆå¼€å‘ã€æœºå™¨äººæ§åˆ¶å’Œäººæœºäº¤äº’ç­‰åœºæ™¯ä¸­ã€‚é€šè¿‡æœ‰æ•ˆå­¦ä¹ ç­–ç•¥è€Œä¸ä¾èµ–äºæ˜ç¡®çš„å¥–åŠ±å‡½æ•°ï¼ŒTROFIèƒ½å¤ŸåŠ é€Ÿæ™ºèƒ½ä½“çš„è®­ç»ƒè¿‡ç¨‹ï¼Œå¹¶æé«˜å…¶åœ¨å¤æ‚ç¯å¢ƒä¸­çš„é€‚åº”èƒ½åŠ›ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½ä¼šæ¨åŠ¨æ›´å¤šåŸºäºäººç±»åå¥½çš„æ™ºèƒ½ç³»ç»Ÿçš„å¼€å‘ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In offline reinforcement learning, agents are trained using only a fixed set of stored transitions derived from a source policy. However, this requires that the dataset be labeled by a reward function. In applied settings such as video game development, the availability of the reward function is not always guaranteed. This paper proposes Trajectory-Ranked OFfline Inverse reinforcement learning (TROFI), a novel approach to effectively learn a policy offline without a pre-defined reward function. TROFI first learns a reward function from human preferences, which it then uses to label the original dataset making it usable for training the policy. In contrast to other approaches, our method does not require optimal trajectories. Through experiments on the D4RL benchmark we demonstrate that TROFI consistently outperforms baselines and performs comparably to using the ground truth reward to learn policies. Additionally, we validate the efficacy of our method in a 3D game environment. Our studies of the reward model highlight the importance of the reward function in this setting: we show that to ensure the alignment of a value function to the actual future discounted reward, it is fundamental to have a well-engineered and easy-to-learn reward function.

