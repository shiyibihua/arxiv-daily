---
layout: default
title: The Hidden Link Between RLHF and Contrastive Learning
---

# The Hidden Link Between RLHF and Contrastive Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.22578" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.22578v2</a>
  <a href="https://arxiv.org/pdf/2506.22578.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.22578v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.22578v2', 'The Hidden Link Between RLHF and Contrastive Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xufei Lv, Kehai Chen, Haoyuan Sun, Xuefeng Bai, Min Zhang, Houde Liu, Kehai Chen

**åˆ†ç±»**: cs.LG, cs.AI, stat.ML

**å‘å¸ƒæ—¥æœŸ**: 2025-06-27 (æ›´æ–°: 2025-10-13)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºäº’ä¿¡æ¯ä¼˜åŒ–æ–¹æ³•ä»¥æå‡äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ æ•ˆæœ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `äººç±»åé¦ˆ` `å¼ºåŒ–å­¦ä¹ ` `äº’ä¿¡æ¯æœ€å¤§åŒ–` `å¯¹æ¯”å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨ç†èƒ½åŠ›` `ä¼˜åŒ–æ–¹æ³•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„RLHFå’ŒDPOæ–¹æ³•åœ¨æ¿€åŠ±å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œæœªèƒ½è¶…è¶ŠåŸºç¡€æ¨¡å‹çš„èƒ½åŠ›ã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡äº’ä¿¡æ¯æœ€å¤§åŒ–çš„è§†è§’ï¼Œé‡æ–°è§£é‡ŠRLHFå’ŒDPOï¼Œå¹¶å¼•å…¥äº’ä¿¡æ¯ä¼˜åŒ–ï¼ˆMIOï¼‰ä½œä¸ºæ”¹è¿›æ–¹æ¡ˆã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMIOåœ¨å¤šä¸ªæ¨ç†å’Œæ•°å­¦åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—ç¼“è§£äº†DPOçš„åæœŸæ€§èƒ½ä¸‹é™é—®é¢˜ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸äººç±»ä»·å€¼è§‚çš„å¯¹é½é—®é¢˜å¼•èµ·äº†å¹¿æ³›å…³æ³¨ï¼Œå°¤å…¶æ˜¯å¼ºåŒ–å­¦ä¹ æ¥è‡ªäººç±»åé¦ˆï¼ˆRLHFï¼‰å’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ã€‚æœ¬æ–‡å±•ç¤ºäº†RLHFå’ŒDPOå¯ä»¥ä»äº’ä¿¡æ¯æœ€å¤§åŒ–çš„è§’åº¦è¿›è¡Œè§£é‡Šï¼Œæ­ç¤ºäº†ä¸å¯¹æ¯”å­¦ä¹ çš„æ·±åˆ»è”ç³»ã€‚é€šè¿‡è¿™ä¸€æ¡†æ¶ï¼ŒRLHFå’ŒDPOè¢«è§†ä¸ºåŸºäºåŸºç¡€æ¨¡å‹çš„æ­£è´Ÿæ ·æœ¬è¿›è¡Œå¯¹æ¯”å­¦ä¹ çš„æ–¹æ³•ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†äº’ä¿¡æ¯ä¼˜åŒ–ï¼ˆMIOï¼‰ï¼Œå¹¶é€šè¿‡ç†è®ºåˆ†æå’Œå®è¯è¯„ä¼°è¡¨æ˜ï¼ŒMIOåœ¨å¤šä¸ªæ¨ç†å’Œæ•°å­¦åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºç«äº‰åŠ›æˆ–ä¼˜è¶Šæ€§ï¼Œç¼“è§£äº†DPOåœ¨åæœŸé€‰æ‹©ä¼¼ç„¶æ€§ä¸‹é™çš„é—®é¢˜ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰RLHFå’ŒDPOæ–¹æ³•åœ¨æ¿€åŠ±å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›æ–¹é¢çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨åæœŸé€‰æ‹©ä¼¼ç„¶æ€§ä¸‹é™çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å°†RLHFå’ŒDPOè§†ä¸ºåŸºäºäº’ä¿¡æ¯æœ€å¤§åŒ–çš„å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œæå‡ºäº’ä¿¡æ¯ä¼˜åŒ–ï¼ˆMIOï¼‰ä»¥æ›¿ä»£ä¼ ç»Ÿçš„Donsker-Varadhanï¼ˆDVï¼‰ç•Œé™ï¼Œåˆ©ç”¨Jensen-Shannonï¼ˆJSï¼‰äº’ä¿¡æ¯ä¼°è®¡å™¨ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬æ•°æ®é‡‡é›†ã€æ­£è´Ÿæ ·æœ¬ç”Ÿæˆã€äº’ä¿¡æ¯è®¡ç®—å’Œä¼˜åŒ–è¿‡ç¨‹ã€‚é€šè¿‡å¯¹æ¯”å­¦ä¹ çš„æ–¹å¼ï¼Œåˆ©ç”¨åŸºç¡€æ¨¡å‹ç”Ÿæˆæ ·æœ¬å¹¶è¿›è¡Œä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šMIOçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå¼•å…¥JSäº’ä¿¡æ¯ä¼°è®¡å™¨ï¼Œæ›¿ä»£äº†DV/MINEç•Œé™ï¼Œä»è€Œæä¾›äº†æ›´æœ‰æ•ˆçš„äº’ä¿¡æ¯ä¼˜åŒ–æ–¹æ³•ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬æ ·æœ¬ç”Ÿæˆç­–ç•¥ã€æŸå¤±å‡½æ•°çš„é€‰æ‹©ä»¥åŠä¼˜åŒ–ç®—æ³•çš„å®ç°ï¼Œç¡®ä¿äº†æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å’Œç¨³å®šæ€§ã€‚é€šè¿‡è¿™äº›è®¾è®¡ï¼ŒMIOåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMIOåœ¨å¤šä¸ªæ¨ç†å’Œæ•°å­¦åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºDPOï¼Œæ˜¾è‘—ç¼“è§£äº†åæœŸé€‰æ‹©ä¼¼ç„¶æ€§ä¸‹é™çš„é—®é¢˜ã€‚å…·ä½“è€Œè¨€ï¼ŒMIOåœ¨æŸäº›ä»»åŠ¡ä¸Šæå‡äº†10%ä»¥ä¸Šçš„æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ™ºèƒ½åŠ©æ‰‹ç­‰ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚æœªæ¥ï¼ŒMIOæ–¹æ³•å¯èƒ½ä¼šåœ¨æ›´å¤šé¢†åŸŸä¸­æ¨å¹¿åº”ç”¨ï¼Œæ¨åŠ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„è¿›ä¸€æ­¥å‘å±•ä¸ä¼˜åŒ–ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Alignment of large language models (LLMs) with human values has recently garnered significant attention, with prominent examples including the canonical yet costly Reinforcement Learning from Human Feedback (RLHF) and the simple Direct Preference Optimization (DPO). In this work, we demonstrate that both RLHF and DPO can be interpreted from the perspective of mutual information (MI) maximization, uncovering a profound connection to contrastive learning. Within this framework, both RLHF and DPO can be interpreted as methods that performing contrastive learning based on the positive and negative samples derived from base model, leveraging the Donsker-Varadhan (DV) lower bound on MI (equivalently, the MINE estimator). Such paradigm further illuminates why RLHF may not intrinsically incentivize reasoning capacities in LLMs beyond what is already present in the base model. Building on the perspective, we replace the DV/MINE bound with the Jensen-Shannon (JS) MI estimator and propose the Mutual Information Optimization (MIO). Comprehensive theoretical analysis and extensive empirical evaluations demonstrate that MIO mitigates the late-stage decline in chosen-likelihood observed in DPO, achieving competitive or superior performance across various challenging reasoning and mathematical benchmarks.

