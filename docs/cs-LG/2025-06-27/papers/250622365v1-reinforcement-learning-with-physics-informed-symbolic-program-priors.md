---
layout: default
title: Reinforcement Learning with Physics-Informed Symbolic Program Priors for Zero-Shot Wireless Indoor Navigation
---

# Reinforcement Learning with Physics-Informed Symbolic Program Priors for Zero-Shot Wireless Indoor Navigation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.22365" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.22365v1</a>
  <a href="https://arxiv.org/pdf/2506.22365.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.22365v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.22365v1', 'Reinforcement Learning with Physics-Informed Symbolic Program Priors for Zero-Shot Wireless Indoor Navigation')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Tao Li, Haozhe Lei, Mingsheng Yin, Yaqi Hu

**ÂàÜÁ±ª**: cs.LG, cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-27

**Â§áÊ≥®**: Spotlight paper at Reinforcement Learning Conference 2025, Workshop on Inductive Biases in Reinforcement Learning

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Áâ©ÁêÜ‰ø°ÊÅØÁ¨¶Âè∑Á®ãÂ∫èÂÖàÈ™åÁöÑÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂‰ª•Ëß£ÂÜ≥Èõ∂Ê†∑Êú¨ÂÆ§ÂÜÖÂØºËà™ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Âº∫ÂåñÂ≠¶‰π†` `Áâ©ÁêÜ‰ø°ÊÅØ` `Á¨¶Âè∑Á®ãÂ∫è` `ÂÆ§ÂÜÖÂØºËà™` `Á•ûÁªèÁ¨¶Âè∑ÈõÜÊàê` `Ê†∑Êú¨ÊïàÁéá` `ÂΩíÁ∫≥ÂÅèÁΩÆ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®Â∞ÜÁâ©ÁêÜÂÖàÈ™åËûçÂÖ•Âº∫ÂåñÂ≠¶‰π†‰∏≠Êó∂ÔºåÂæÄÂæÄÈúÄË¶ÅÂ§ßÈáèÁöÑ‰∫∫Â∑•Âπ≤È¢ÑÂíåÈ¢ÜÂüüÁü•ËØÜÔºåÈôêÂà∂‰∫ÜÂÖ∂ÊôÆÈÄÇÊÄß„ÄÇ
2. Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁâ©ÁêÜ‰ø°ÊÅØÁ®ãÂ∫èÂºïÂØºÁöÑÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂ÔºàPiPRLÔºâÔºåÈÄöËøáÁ¨¶Âè∑ÁºñÁ®ãÂ∞ÜÁâ©ÁêÜÂÖàÈ™å‰ª•‰∫∫Á±ªÂèØËØªÁöÑÂΩ¢ÂºèÊï¥ÂêàËøõRL‰ª£ÁêÜ‰∏≠„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåPiPRLÂú®ÂÆ§ÂÜÖÂØºËà™‰ªªÂä°‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåËÆ≠ÁªÉÊó∂Èó¥ÂáèÂ∞ëË∂ÖËøá26%ÔºåÊòæËëóÊèêÂçá‰∫ÜÊ†∑Êú¨ÊïàÁéáÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Âú®‰ΩøÁî®Âº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâËß£ÂÜ≥Áâ©ÁêÜÊéßÂà∂‰ªªÂä°Êó∂ÔºåÁºñÁ†ÅÁâ©ÁêÜÂÖàÈ™åÁöÑÂΩíÁ∫≥ÂÅèÁΩÆÂèØ‰ª•ÊèêÈ´òËÆ≠ÁªÉÁöÑÊ†∑Êú¨ÊïàÁéáÂπ∂Â¢ûÂº∫ÊµãËØïÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåÂΩìÂâçÂ∞ÜËøô‰∫õÁâ©ÁêÜ‰ø°ÊÅØÂΩíÁ∫≥ÂÅèÁΩÆÁ∫≥ÂÖ•RLÁöÑÂÅöÊ≥ï‰∏çÂèØÈÅøÂÖçÂú∞ÈúÄË¶ÅÂ§ßÈáèÁöÑ‰∫∫Â∑•Âä≥Âä®ÂíåÈ¢ÜÂüü‰∏ì‰∏öÁü•ËØÜÔºå‰ΩøÂÖ∂ÂØπÊôÆÈÄöÁî®Êà∑ËÄåË®ÄÈöæ‰ª•ÂÆûÁé∞„ÄÇÊú¨Á†îÁ©∂Êé¢Á¥¢‰∫Ü‰∏ÄÁßçÁ¨¶Âè∑ÊñπÊ≥ïÔºåÂ∞ÜÁâ©ÁêÜ‰ø°ÊÅØÂΩíÁ∫≥ÂÅèÁΩÆÊèêÁÇºÂà∞RL‰ª£ÁêÜ‰∏≠ÔºåÁâ©ÁêÜÂÖàÈ™å‰ª•‰∫∫Á±ªÂèØËØª‰∏îËá™ÁÑ∂ÂèØËß£ÈáäÁöÑÈ¢ÜÂüüÁâπÂÆöËØ≠Ë®ÄÔºàDSLÔºâË°®Ëææ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥DSLÂÖàÈ™åÊó†Ê≥ïÁõ¥Êé•ËΩ¨Âåñ‰∏∫ÂèØÂÆûÊñΩÁ≠ñÁï•ÁöÑÈóÆÈ¢òÔºåÊàë‰ª¨ÂºÄÂèë‰∫Ü‰∏Ä‰∏™Áâ©ÁêÜ‰ø°ÊÅØÁ®ãÂ∫èÂºïÂØºÁöÑRLÔºàPiPRLÔºâÊ°ÜÊû∂ÔºåÂ∫îÁî®‰∫éÂÆ§ÂÜÖÂØºËà™„ÄÇPiPRLÈááÁî®Â±ÇÊ¨°ÂåñÂíåÊ®°ÂùóÂåñÁöÑÁ•ûÁªèÁ¨¶Âè∑ÈõÜÊàêÔºåÂÖÉÁ¨¶Âè∑Á®ãÂ∫èÊé•Êî∂Êù•Ëá™Á•ûÁªèÊÑüÁü•Ê®°ÂùóÁöÑËØ≠‰πâÁâπÂæÅÔºåËøô‰∫õÁâπÂæÅÊûÑÊàê‰∫ÜÁºñÁ†ÅÁâ©ÁêÜÂÖàÈ™åÂπ∂ÂºïÂØº‰ΩéÁ∫ßÁ•ûÁªèÊéßÂà∂Âô®RLËøáÁ®ãÁöÑÁ¨¶Âè∑ÁºñÁ®ãÂü∫Á°Ä„ÄÇÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåPiPRLÂú®ÊÄßËÉΩ‰∏äÂßãÁªà‰ºò‰∫éÁ∫ØÁ¨¶Âè∑ÊàñÁ•ûÁªèÁ≠ñÁï•ÔºåÂπ∂ÈÄöËøáÁ®ãÂ∫èÂåñÁöÑÂΩíÁ∫≥ÂÅèÁΩÆÂ∞ÜËÆ≠ÁªÉÊó∂Èó¥ÂáèÂ∞ë‰∫ÜË∂ÖËøá26%„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨Á†îÁ©∂Êó®Âú®Ëß£ÂÜ≥Âú®Áâ©ÁêÜÊéßÂà∂‰ªªÂä°‰∏≠ÔºåÂ¶Ç‰ΩïÊúâÊïàÂú∞Â∞ÜÁâ©ÁêÜÂÖàÈ™åËûçÂÖ•Âº∫ÂåñÂ≠¶‰π†‰ª£ÁêÜÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏‰æùËµñ‰∫éÂ§ßÈáèÁöÑ‰∫∫Â∑•ËÆæËÆ°ÂíåÈ¢ÜÂüüÁü•ËØÜÔºåÂØºËá¥ÊôÆÈÄÇÊÄßÂ∑ÆÂíåÊïàÁéá‰Ωé‰∏ã„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁ¨¶Âè∑ÊñπÊ≥ïÔºåÈÄöËøáÈ¢ÜÂüüÁâπÂÆöËØ≠Ë®ÄÔºàDSLÔºâÂ∞ÜÁâ©ÁêÜÂÖàÈ™åË°®Ëææ‰∏∫ÂèØËØªÁöÑÁ¨¶Âè∑Á®ãÂ∫èÔºå‰ªéËÄåÂáèÂ∞ëÂØπÈ¢ÜÂüü‰∏ìÂÆ∂ÁöÑ‰æùËµñÔºåÂπ∂ÊèêÈ´òRLÁöÑÊ†∑Êú¨ÊïàÁéáÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöPiPRLÊ°ÜÊû∂ÈááÁî®Â±ÇÊ¨°ÂåñÂíåÊ®°ÂùóÂåñÁöÑËÆæËÆ°Ôºå‰∏ªË¶ÅÂåÖÊã¨‰∏Ä‰∏™Á•ûÁªèÊÑüÁü•Ê®°ÂùóÂíå‰∏Ä‰∏™ÂÖÉÁ¨¶Âè∑Á®ãÂ∫è„ÄÇÁ•ûÁªèÊÑüÁü•Ê®°ÂùóÊèêÂèñËØ≠‰πâÁâπÂæÅÔºåÂÖÉÁ¨¶Âè∑Á®ãÂ∫èÂàôÂà©Áî®Ëøô‰∫õÁâπÂæÅËøõË°åÁ¨¶Âè∑ÁºñÁ®ãÔºåÊúÄÁªàÂºïÂØº‰ΩéÁ∫ßÁ•ûÁªèÊéßÂà∂Âô®ÁöÑRLËøáÁ®ã„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞Âú®‰∫éÂ∞ÜÁâ©ÁêÜ‰ø°ÊÅØÂΩíÁ∫≥ÂÅèÁΩÆ‰ª•Á¨¶Âè∑Á®ãÂ∫èÁöÑÂΩ¢ÂºèÂºïÂÖ•RL‰∏≠ÔºåÂΩ¢Êàê‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÁ•ûÁªèÁ¨¶Âè∑ÈõÜÊàêÊñπÊ≥ïÔºå‰∏é‰º†ÁªüÁöÑÁ∫ØÁ¨¶Âè∑ÊàñÁ∫ØÁ•ûÁªèÊñπÊ≥ïÁõ∏ÊØîÔºåÊòæËëóÊèêÈ´ò‰∫ÜÊÄßËÉΩÂíåÊïàÁéá„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆæËÆ°‰∏≠Ôºå‰ΩøÁî®‰∫ÜÁâπÂÆöÁöÑÊçüÂ§±ÂáΩÊï∞Êù•‰ºòÂåñÁ¨¶Âè∑Á®ãÂ∫èÁöÑÁîüÊàêÔºåÂπ∂ÈÄöËøáÊ®°ÂùóÂåñÁªìÊûÑ‰ΩøÂæóÂêÑ‰∏™ÈÉ®ÂàÜÂèØ‰ª•Áã¨Á´ã‰ºòÂåñÔºåÁ°Æ‰øù‰∫ÜÁ≥ªÁªüÁöÑÁÅµÊ¥ªÊÄßÂíåÂèØÊâ©Â±ïÊÄß„ÄÇÂÖ∑‰ΩìÁöÑÁΩëÁªúÁªìÊûÑÂíåÂèÇÊï∞ËÆæÁΩÆÂú®ÂÆûÈ™å‰∏≠ÁªèËøáÂ§öÊ¨°Ë∞É‰ºòÔºå‰ª•ËææÂà∞ÊúÄ‰Ω≥ÊïàÊûú„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåPiPRLÊ°ÜÊû∂Âú®ÂÆ§ÂÜÖÂØºËà™‰ªªÂä°‰∏≠Ë°®Áé∞‰ºò‰∫é‰º†ÁªüÁöÑÁ∫ØÁ¨¶Âè∑ÊàñÁ•ûÁªèÁ≠ñÁï•ÔºåËÆ≠ÁªÉÊó∂Èó¥ÂáèÂ∞ëË∂ÖËøá26%„ÄÇËøô‰∏ÄÊòæËëóÊèêÂçá‰∏ç‰ªÖÊèêÈ´ò‰∫ÜÊ†∑Êú¨ÊïàÁéáÔºåËøòÂ¢ûÂº∫‰∫ÜÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõÔºåÂ±ïÁ§∫‰∫ÜÁâ©ÁêÜ‰ø°ÊÅØÂΩíÁ∫≥ÂÅèÁΩÆÂú®Âº∫ÂåñÂ≠¶‰π†‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨ÂÆ§ÂÜÖÂØºËà™„ÄÅÊú∫Âô®‰∫∫ÊéßÂà∂ÂíåËá™Âä®È©æÈ©∂Á≠âÂú∫ÊôØ„ÄÇÈÄöËøáÂ∞ÜÁâ©ÁêÜÂÖàÈ™åÊúâÊïàÊï¥ÂêàËøõÂº∫ÂåñÂ≠¶‰π†ÔºåËÉΩÂ§üÊòæËëóÊèêÈ´òÁ≥ªÁªüÁöÑÊô∫ËÉΩÂåñÊ∞¥Âπ≥ÂíåÈÄÇÂ∫îËÉΩÂäõÔºåÊú™Êù•ÂèØÂπøÊ≥õÂ∫îÁî®‰∫éÊô∫ËÉΩÂÆ∂Â±Ö„ÄÅÊó†‰∫∫Êú∫ÂØºËà™Á≠âÂÆûÈôÖÂú∫ÊôØÔºåÊé®Âä®Áõ∏ÂÖ≥ÊäÄÊúØÁöÑÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> When using reinforcement learning (RL) to tackle physical control tasks, inductive biases that encode physics priors can help improve sample efficiency during training and enhance generalization in testing. However, the current practice of incorporating these helpful physics-informed inductive biases inevitably runs into significant manual labor and domain expertise, making them prohibitive for general users. This work explores a symbolic approach to distill physics-informed inductive biases into RL agents, where the physics priors are expressed in a domain-specific language (DSL) that is human-readable and naturally explainable. Yet, the DSL priors do not translate directly into an implementable policy due to partial and noisy observations and additional physical constraints in navigation tasks. To address this gap, we develop a physics-informed program-guided RL (PiPRL) framework with applications to indoor navigation. PiPRL adopts a hierarchical and modularized neuro-symbolic integration, where a meta symbolic program receives semantically meaningful features from a neural perception module, which form the bases for symbolic programming that encodes physics priors and guides the RL process of a low-level neural controller. Extensive experiments demonstrate that PiPRL consistently outperforms purely symbolic or neural policies and reduces training time by over 26% with the help of the program-based inductive biases.

