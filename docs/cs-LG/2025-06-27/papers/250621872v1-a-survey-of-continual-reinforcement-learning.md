---
layout: default
title: A Survey of Continual Reinforcement Learning
---

# A Survey of Continual Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21872" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.21872v1</a>
  <a href="https://arxiv.org/pdf/2506.21872.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21872v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21872v1', 'A Survey of Continual Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chaofan Pan, Xin Yang, Yanhua Li, Wei Wei, Tianrui Li, Bo An, Jiye Liang

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-27

**å¤‡æ³¨**: This work has been submitted to the IEEE TPAMI

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæŒç»­å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä»¥è§£å†³åŠ¨æ€ç¯å¢ƒä¸­çš„çŸ¥è¯†ä¿æŒé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æŒç»­å­¦ä¹ ` `å¼ºåŒ–å­¦ä¹ ` `çŸ¥è¯†ä¿æŒ` `åŠ¨æ€ç¯å¢ƒ` `æ™ºèƒ½ä½“é€‚åº”æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¾èµ–å¤§é‡æ•°æ®å’Œè®¡ç®—èµ„æºï¼Œä¸”åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„æ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚
2. è®ºæ–‡æå‡ºæŒç»­å¼ºåŒ–å­¦ä¹ ï¼Œé€šè¿‡æŒç»­å­¦ä¹ å’ŒçŸ¥è¯†ä¿æŒæ¥è§£å†³å¼ºåŒ–å­¦ä¹ çš„å±€é™æ€§ã€‚
3. é€šè¿‡å¯¹ç°æœ‰ç ”ç©¶çš„ç³»ç»Ÿå›é¡¾ï¼Œæå‡ºäº†æ–°çš„CRLæ–¹æ³•åˆ†ç±»æ³•ï¼Œå¹¶åˆ†æäº†æœªæ¥ç ”ç©¶æ–¹å‘ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ˜¯è§£å†³åºåˆ—å†³ç­–é—®é¢˜çš„é‡è¦æœºå™¨å­¦ä¹ èŒƒå¼ã€‚è¿‘å¹´æ¥ï¼Œæ·±åº¦ç¥ç»ç½‘ç»œçš„å¿«é€Ÿå‘å±•ä½¿å¾—è¯¥é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼ŒRLç›®å‰ä¾èµ–äºå¤§é‡çš„è®­ç»ƒæ•°æ®å’Œè®¡ç®—èµ„æºï¼Œå…¶åœ¨ä»»åŠ¡é—´çš„æ³›åŒ–èƒ½åŠ›æœ‰é™ï¼Œé™åˆ¶äº†å…¶åœ¨åŠ¨æ€å’Œç°å®ç¯å¢ƒä¸­çš„åº”ç”¨ã€‚éšç€æŒç»­å­¦ä¹ ï¼ˆCLï¼‰çš„å…´èµ·ï¼ŒæŒç»­å¼ºåŒ–å­¦ä¹ ï¼ˆCRLï¼‰ä½œä¸ºä¸€ç§æœ‰å‰æ™¯çš„ç ”ç©¶æ–¹å‘åº”è¿è€Œç”Ÿï¼Œæ—¨åœ¨ä½¿æ™ºèƒ½ä½“èƒ½å¤ŸæŒç»­å­¦ä¹ ã€é€‚åº”æ–°ä»»åŠ¡å¹¶ä¿ç•™å…ˆå‰è·å¾—çš„çŸ¥è¯†ã€‚æœ¬æ–‡å¯¹CRLè¿›è¡Œäº†å…¨é¢çš„å®¡æŸ¥ï¼Œé‡ç‚¹å…³æ³¨å…¶æ ¸å¿ƒæ¦‚å¿µã€æŒ‘æˆ˜å’Œæ–¹æ³•è®ºï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„CRLæ–¹æ³•åˆ†ç±»æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡è¦è§£å†³çš„å…·ä½“é—®é¢˜æ˜¯å¦‚ä½•åœ¨åŠ¨æ€ç¯å¢ƒä¸­å®ç°æ™ºèƒ½ä½“çš„æŒç»­å­¦ä¹ ä¸çŸ¥è¯†ä¿æŒã€‚ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨é¢å¯¹æ–°ä»»åŠ¡æ—¶ï¼Œå¾€å¾€æ— æ³•æœ‰æ•ˆåˆ©ç”¨ä¹‹å‰è·å¾—çš„çŸ¥è¯†ï¼Œå¯¼è‡´å­¦ä¹ æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒè§£å†³æ€è·¯æ˜¯å¼•å…¥æŒç»­å­¦ä¹ çš„æ¦‚å¿µï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨å­¦ä¹ æ–°ä»»åŠ¡çš„åŒæ—¶ï¼Œä¿ç•™å’Œåˆ©ç”¨ä¹‹å‰çš„çŸ¥è¯†ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨æé«˜å­¦ä¹ çš„çµæ´»æ€§å’Œæ•ˆç‡ï¼Œé€‚åº”ä¸æ–­å˜åŒ–çš„ç¯å¢ƒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬çŸ¥è¯†å­˜å‚¨æ¨¡å—ã€ä»»åŠ¡é€‚åº”æ¨¡å—å’ŒçŸ¥è¯†è½¬ç§»æ¨¡å—ã€‚çŸ¥è¯†å­˜å‚¨æ¨¡å—è´Ÿè´£ä¿å­˜æ™ºèƒ½ä½“åœ¨ä¸åŒä»»åŠ¡ä¸­è·å¾—çš„çŸ¥è¯†ï¼Œä»»åŠ¡é€‚åº”æ¨¡å—åˆ™ç”¨äºå¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡ï¼Œè€ŒçŸ¥è¯†è½¬ç§»æ¨¡å—åˆ™å¸®åŠ©æ™ºèƒ½ä½“åœ¨æ–°ä»»åŠ¡ä¸­æœ‰æ•ˆåˆ©ç”¨æ—§çŸ¥è¯†ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†ä¸€ç§æ–°çš„CRLæ–¹æ³•åˆ†ç±»æ³•ï¼Œå°†ç°æœ‰æ–¹æ³•ä»çŸ¥è¯†å­˜å‚¨å’Œè½¬ç§»çš„è§’åº¦è¿›è¡Œåˆ†ç±»ã€‚è¿™ç§åˆ†ç±»æ³•æœ‰åŠ©äºæ›´å¥½åœ°ç†è§£å’Œæ¯”è¾ƒä¸åŒCRLæ–¹æ³•çš„ä¼˜ç¼ºç‚¹ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼Œè®ºæ–‡å¼ºè°ƒäº†æŸå¤±å‡½æ•°çš„è®¾è®¡ï¼Œä»¥å¹³è¡¡æ–°æ—§çŸ¥è¯†çš„å­¦ä¹ ï¼ŒåŒæ—¶æå‡ºäº†é€‚åº”æ€§è°ƒæ•´çš„å‚æ•°è®¾ç½®ï¼Œä»¥ä¼˜åŒ–æ™ºèƒ½ä½“åœ¨ä¸åŒä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’Œè®­ç»ƒæµç¨‹ä¹Ÿè¿›è¡Œäº†è¯¦ç»†è¯´æ˜ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæå‡ºçš„CRLæ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†ä»»åŠ¡ä¸Šå‡ä¼˜äºä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šã€‚é€šè¿‡å¯¹æ¯”åˆ†æï¼ŒéªŒè¯äº†æ–°åˆ†ç±»æ³•çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†é‡è¦å‚è€ƒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ã€ä¸ªæ€§åŒ–æ¨èç³»ç»Ÿç­‰ã€‚åœ¨è¿™äº›åŠ¨æ€ç¯å¢ƒä¸­ï¼Œæ™ºèƒ½ä½“éœ€è¦ä¸æ–­é€‚åº”æ–°æƒ…å†µå¹¶ä¿ç•™å…ˆå‰çš„ç»éªŒï¼Œä»è€Œæé«˜å†³ç­–çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½æ¨åŠ¨æ™ºèƒ½ä½“åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œæå‡å…¶æ™ºèƒ½æ°´å¹³ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement Learning (RL) is an important machine learning paradigm for solving sequential decision-making problems. Recent years have witnessed remarkable progress in this field due to the rapid development of deep neural networks. However, the success of RL currently relies on extensive training data and computational resources. In addition, RL's limited ability to generalize across tasks restricts its applicability in dynamic and real-world environments. With the arisen of Continual Learning (CL), Continual Reinforcement Learning (CRL) has emerged as a promising research direction to address these limitations by enabling agents to learn continuously, adapt to new tasks, and retain previously acquired knowledge. In this survey, we provide a comprehensive examination of CRL, focusing on its core concepts, challenges, and methodologies. Firstly, we conduct a detailed review of existing works, organizing and analyzing their metrics, tasks, benchmarks, and scenario settings. Secondly, we propose a new taxonomy of CRL methods, categorizing them into four types from the perspective of knowledge storage and/or transfer. Finally, our analysis highlights the unique challenges of CRL and provides practical insights into future directions.

