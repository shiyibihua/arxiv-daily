---
layout: default
title: Projected Compression: Trainable Projection for Efficient Transformer Compression
---

# Projected Compression: Trainable Projection for Efficient Transformer Compression

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.22255" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.22255v1</a>
  <a href="https://arxiv.org/pdf/2506.22255.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.22255v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.22255v1', 'Projected Compression: Trainable Projection for Efficient Transformer Compression')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Maciej Stefaniak, MichaÅ‚ Krutul, Jan MaÅ‚aÅ›nicki, Maciej PiÃ³ro, Jakub Krajewski, Sebastian Jaszczur, Marek Cygan, Kamil Adamczewski, Jan Ludziejewski

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-27

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºProjected Compressionä»¥è§£å†³Transformeræ¨¡å‹å‹ç¼©é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ¨¡å‹å‹ç¼©` `Transformer` `æŠ•å½±æ¨¡å—` `æ·±åº¦å­¦ä¹ ` `è‡ªç„¶è¯­è¨€å¤„ç†` `è®¡ç®—æ•ˆç‡` `å¯è®­ç»ƒå‚æ•°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤§å‹è¯­è¨€æ¨¡å‹çš„è§„æ¨¡ä¸æ–­å¢åŠ ï¼Œå¯¼è‡´æ¨ç†æ—¶é—´å’Œè®¡ç®—éœ€æ±‚æ˜¾è‘—ä¸Šå‡ï¼Œç°æœ‰çš„å‹ç¼©æ–¹æ³•éš¾ä»¥æœ‰æ•ˆè§£å†³è¿™ä¸€é—®é¢˜ã€‚
2. æœ¬æ–‡æå‡ºçš„Projected Compressionæ–¹æ³•é€šè¿‡è®­ç»ƒå¯å­¦ä¹ çš„æŠ•å½±æƒé‡ï¼Œä¿ç•™åŸå§‹æ¨¡å‹å‚æ•°çš„åŒæ—¶å®ç°æ¨¡å‹å‹ç¼©ï¼Œé¿å…äº†é¢å¤–çš„è®¡ç®—å¼€é”€ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒProjected Compressionåœ¨é«˜è´¨é‡æ¨¡å‹ä¸Šä¼˜äºä¼ ç»Ÿçš„ç¡¬å‰ªæå’Œé‡è®­ç»ƒæ–¹æ³•ï¼Œä¸”æ€§èƒ½æå‡ä¸tokenæ•°é‡å‘ˆæ­£ç›¸å…³ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹çš„è§„æ¨¡ä¸æ–­å¢åŠ ä»¥æå‡æ€§èƒ½ï¼Œä½†è¿™ä¹Ÿå¯¼è‡´äº†æ¨ç†æ—¶é—´å’Œè®¡ç®—éœ€æ±‚çš„å¢åŠ ã€‚å› æ­¤ï¼Œæ¨¡å‹å¤§å°å‡å°‘çš„æ–¹æ³•å—åˆ°è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¨¡å‹å‹ç¼©æŠ€æœ¯â€”â€”Projected Compressionï¼Œé€šè¿‡åˆ©ç”¨æŠ•å½±æ¨¡å—æ¥å‡å°‘æ¨¡å‹æƒé‡ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬é¦–å…ˆè®­ç»ƒé¢å¤–çš„å¯è®­ç»ƒæŠ•å½±æƒé‡ï¼Œå¹¶ä¿ç•™å¯¹æ‰€æœ‰åŸå§‹æ¨¡å‹å‚æ•°çš„è®¿é—®ã€‚éšåï¼Œè¿™äº›æŠ•å½±è¢«åˆå¹¶ä¸ºä¸€ä¸ªä½ç»´çš„ä¹˜ç§¯çŸ©é˜µï¼Œä»è€Œå½¢æˆä¸€ä¸ªæ ‡å‡†çš„ã€å°ºå¯¸å‡å°çš„åŸºäºTransformerçš„æ¨¡å‹ã€‚ä¸éœ€è¦é¢å¤–è®¡ç®—å¼€é”€çš„æ›¿ä»£æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ¯ä¸ªtokençš„è®¡ç®—æ­¥éª¤ä¸­ä¸åŸºç¡€æ¨¡å‹çš„FLOPsç›¸åŒ¹é…ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒProjected Compressionåœ¨é«˜è´¨é‡æ¨¡å‹ä¸Šä¼˜äºå¯æ¯”è¾ƒçš„ç¡¬å‰ªæå’Œé‡è®­ç»ƒæ–¹æ³•ã€‚æ­¤å¤–ï¼Œæ€§èƒ½æå‡ä¸tokenæ•°é‡çš„å¢åŠ å‘ˆè‰¯å¥½æ‰©å±•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹Transformeræ¨¡å‹åœ¨æ¨ç†æ—¶çš„è®¡ç®—æ•ˆç‡é—®é¢˜ã€‚ç°æœ‰çš„æ¨¡å‹å‹ç¼©æ–¹æ³•å¾€å¾€éœ€è¦é¢å¤–çš„è®¡ç®—å¼€é”€ï¼Œå¯¼è‡´å®é™…åº”ç”¨ä¸­æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºçš„Projected Compressionæ–¹æ³•é€šè¿‡è®­ç»ƒå¯å­¦ä¹ çš„æŠ•å½±æƒé‡ï¼Œå°†æ¨¡å‹æƒé‡å‹ç¼©ä¸ºä½ç»´ä¹˜ç§¯çŸ©é˜µï¼Œä»è€Œåœ¨ä¿ç•™åŸå§‹æ¨¡å‹å‚æ•°çš„åŒæ—¶å‡å°‘æ¨¡å‹å¤§å°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆè®­ç»ƒå¯å­¦ä¹ çš„æŠ•å½±æƒé‡ï¼Œç„¶åå°†è¿™äº›æƒé‡åˆå¹¶ä¸ºä½ç»´çŸ©é˜µï¼Œæœ€ç»ˆå½¢æˆä¸€ä¸ªå‹ç¼©çš„Transformeræ¨¡å‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºé€šè¿‡æŠ•å½±æ¨¡å—å®ç°æ¨¡å‹å‹ç¼©ï¼Œè€Œä¸å¢åŠ é¢å¤–çš„è®¡ç®—å¼€é”€ï¼Œè¿™ä¸ä¼ ç»Ÿçš„å‰ªæå’Œé‡è®­ç»ƒæ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡ç‚¹å…³æ³¨æŠ•å½±æƒé‡çš„è®­ç»ƒè¿‡ç¨‹å’Œåˆå¹¶ç­–ç•¥ï¼Œç¡®ä¿åœ¨å‹ç¼©è¿‡ç¨‹ä¸­å°½å¯èƒ½ä¿ç•™æ¨¡å‹çš„æ€§èƒ½å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒProjected Compressionåœ¨é«˜è´¨é‡æ¨¡å‹ä¸Šç›¸è¾ƒäºä¼ ç»Ÿçš„ç¡¬å‰ªæå’Œé‡è®­ç»ƒæ–¹æ³•å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦åœ¨å¤šä¸ªtokenæ•°é‡ä¸‹å‡è¡¨ç°è‰¯å¥½ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œå¯æ‰©å±•æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Projected Compressionæ–¹æ³•å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦é«˜æ•ˆæ¨ç†çš„å¤§å‹è¯­è¨€æ¨¡å‹åœºæ™¯ä¸­ï¼Œå¦‚å®æ—¶ç¿»è¯‘ã€æ™ºèƒ½åŠ©æ‰‹å’Œè‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸã€‚é€šè¿‡é™ä½æ¨¡å‹çš„è®¡ç®—éœ€æ±‚ï¼Œè¯¥æŠ€æœ¯å¯ä»¥ä½¿å¾—åœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šè¿è¡Œå¤æ‚æ¨¡å‹æˆä¸ºå¯èƒ½ï¼Œæ¨åŠ¨AIæŠ€æœ¯çš„æ™®åŠå’Œåº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models have steadily increased in size to achieve improved performance; however, this growth has also led to greater inference time and computational demands. Consequently, there is rising interest in model size reduction methods. To address this issue, we propose Projected Compression, a novel model compression technique, that reduces model weights by utilizing projection modules. Specifically, we first train additional trainable projections weights and preserve access to all the original model parameters. Subsequently, these projections are merged into a lower-dimensional product matrix, resulting in a reduced-size standard Transformer-based model. Unlike alternative approaches that require additional computational overhead, our method matches the base model's per-token computation step in FLOPs. Experimental results show that Projected Compression outperforms the comparable hard pruning and retraining approach on higher quality models. Moreover, the performance margin scales well with the number of tokens.

