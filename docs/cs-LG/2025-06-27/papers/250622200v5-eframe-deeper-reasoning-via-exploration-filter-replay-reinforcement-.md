---
layout: default
title: EFRame: Deeper Reasoning via Exploration-Filter-Replay Reinforcement Learning Framework
---

# EFRame: Deeper Reasoning via Exploration-Filter-Replay Reinforcement Learning Framework

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.22200" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.22200v5</a>
  <a href="https://arxiv.org/pdf/2506.22200.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.22200v5" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.22200v5', 'EFRame: Deeper Reasoning via Exploration-Filter-Replay Reinforcement Learning Framework')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chen Wang, Lai Wei, Yanzhi Zhang, Chenyang Shao, Zedong Dan, Weiran Huang, Yuzhi Zhang, Yue Wang

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-27 (æ›´æ–°: 2025-10-10)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/597358816/EFRame)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEFRameæ¡†æ¶ä»¥è§£å†³GRPOåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„ä¸è¶³**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `æ¨ç†èƒ½åŠ›` `æ¢ç´¢ç­–ç•¥` `æ ·æœ¬è¿‡æ»¤` `ç»éªŒé‡æ”¾` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„GRPOæ–¹æ³•åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­é¢ä¸´æ¢ç´¢ä¸è¶³å’Œè®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ï¼Œé™åˆ¶äº†å…¶æœ‰æ•ˆæ€§ã€‚
2. EFRameæ¡†æ¶é€šè¿‡é¢å¤–å›åˆã€åœ¨çº¿è¿‡æ»¤å’Œç»éªŒé‡æ”¾ä¸‰æ–¹é¢å¢å¼ºGRPOï¼Œæ—¨åœ¨å®ç°æ›´æ·±å±‚æ¬¡çš„æ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒEFRameåœ¨å¤šä¸ªæ¨ç†åŸºå‡†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨Geometry3Kä¸Šç›¸è¾ƒäºGRPOæå‡äº†37.9%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„è¿›å±•æ˜¾è‘—æå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚å°½ç®¡ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ä½œä¸ºä¸€ç§è½»é‡çº§çš„è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰å˜ä½“æé«˜äº†æ•ˆç‡ï¼Œä½†å…¶æ¢ç´¢èƒ½åŠ›æœ‰é™å’Œè®­ç»ƒä¸ç¨³å®šæ€§é™åˆ¶äº†å…¶åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†EFRameï¼Œä¸€ä¸ªæ¢ç´¢-è¿‡æ»¤-é‡æ”¾æ¡†æ¶ï¼Œé€šè¿‡é¢å¤–çš„å›åˆå®ç°æ›´æ·±å±‚æ¬¡å’Œæ›´æœ‰é’ˆå¯¹æ€§çš„æ¢ç´¢ï¼Œåœ¨çº¿è¿‡æ»¤å»é™¤ä½è´¨é‡æ ·æœ¬ä»¥ç¨³å®šæ¢¯åº¦å¹¶åŠ é€Ÿè®­ç»ƒï¼Œä»¥åŠç»éªŒé‡æ”¾æ”¾å¤§ç¨€æœ‰ä½†ä¿¡æ¯ä¸°å¯Œçš„è½¨è¿¹ä»¥å®ç°ç¨³å®šæ”¶æ•›ã€‚å®éªŒè¡¨æ˜ï¼ŒEFRameåœ¨å¤šç§æ¨ç†åŸºå‡†ä¸Šå–å¾—äº†ä¸€è‡´çš„æå‡ï¼ŒåŒ…æ‹¬åœ¨Geometry3Kä¸Šç›¸è¾ƒäºGRPOå®ç°äº†37.9%çš„ç›¸å¯¹æ”¹å–„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰GRPOæ–¹æ³•åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„æ¢ç´¢ä¸è¶³å’Œè®­ç»ƒä¸ç¨³å®šæ€§é—®é¢˜ï¼Œè¿™äº›é—®é¢˜é™åˆ¶äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šEFRameæ¡†æ¶é€šè¿‡å¼•å…¥é¢å¤–çš„å›åˆä»¥å®ç°æ›´æ·±å±‚æ¬¡çš„æ¢ç´¢ï¼ŒåŒæ—¶é€šè¿‡åœ¨çº¿è¿‡æ»¤å’Œç»éªŒé‡æ”¾æ¥æé«˜è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ•ˆç‡ã€‚è¿™æ ·çš„è®¾è®¡æ—¨åœ¨å¹³è¡¡æ¢ç´¢ä¸æ•ˆç‡ï¼Œç¡®ä¿æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šEFRameçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼š1) é¢å¤–å›åˆç”¨äºæ·±åº¦æ¢ç´¢ï¼›2) åœ¨çº¿è¿‡æ»¤æ¨¡å—ç”¨äºå»é™¤ä½è´¨é‡æ ·æœ¬ï¼›3) ç»éªŒé‡æ”¾æ¨¡å—ç”¨äºæ”¾å¤§ç¨€æœ‰ä¸”ä¿¡æ¯ä¸°å¯Œçš„è½¨è¿¹ã€‚è¿™äº›æ¨¡å—å…±åŒæ„æˆäº†ä¸€ä¸ªç»Ÿä¸€çš„è®­ç»ƒå¾ªç¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šEFRameçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶æ¢ç´¢-è¿‡æ»¤-é‡æ”¾çš„ç»Ÿä¸€æ¡†æ¶ï¼Œæ˜¾è‘—æå‡äº†GRPOçš„è®­ç»ƒæ•ˆç‡å’Œç¨³å®šæ€§ã€‚è¿™ç§è®¾è®¡ä¸ä¼ ç»Ÿæ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºå…¶èƒ½å¤ŸåŠ¨æ€è°ƒæ•´æ¢ç´¢ç­–ç•¥å’Œæ ·æœ¬è´¨é‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼ŒEFRameé‡‡ç”¨äº†è‡ªé€‚åº”çš„è¿‡æ»¤é˜ˆå€¼å’Œé‡æ”¾ç­–ç•¥ï¼Œä»¥ç¡®ä¿åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å§‹ç»ˆä¿æŒé«˜è´¨é‡çš„æ ·æœ¬æµã€‚æ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°çš„è®¾è®¡ä¹Ÿè€ƒè™‘äº†æ ·æœ¬çš„å¤šæ ·æ€§å’Œä¿¡æ¯é‡ï¼Œä»¥è¿›ä¸€æ­¥æå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å®éªŒä¸­ï¼ŒEFRameåœ¨å¤šä¸ªæ¨ç†åŸºå‡†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨Geometry3Kæ•°æ®é›†ä¸Šå®ç°äº†37.9%çš„ç›¸å¯¹æå‡ï¼Œç›¸è¾ƒäºGRPOæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ã€‚è¿™ä¸€ç»“æœéªŒè¯äº†EFRameåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å’Œä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

EFRameæ¡†æ¶å…·æœ‰å¹¿æ³›çš„æ½œåœ¨åº”ç”¨åœºæ™¯ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¤æ‚æ¨ç†çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­ï¼Œå¦‚é—®ç­”ç³»ç»Ÿã€å¯¹è¯ç”Ÿæˆå’Œæ–‡æœ¬ç†è§£ç­‰ã€‚å…¶æå‡çš„æ¨ç†èƒ½åŠ›å’Œè®­ç»ƒç¨³å®šæ€§å°†ä¸ºå®é™…åº”ç”¨æä¾›æ›´å¼ºçš„æ”¯æŒï¼Œæ¨åŠ¨æ™ºèƒ½åŠ©æ‰‹å’Œè‡ªåŠ¨åŒ–ç³»ç»Ÿçš„å‘å±•ã€‚æœªæ¥ï¼ŒEFRameçš„è®¾è®¡ç†å¿µä¹Ÿå¯èƒ½è¢«åº”ç”¨äºå…¶ä»–é¢†åŸŸçš„å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in reinforcement learning (RL) have significantly enhanced the reasoning capabilities of large language models (LLMs). Group Relative Policy Optimization (GRPO), a lightweight variant of Proximal Policy Optimization (PPO), improves efficiency but suffers from limited exploration and training instability, limiting its effectiveness on complex reasoning tasks. To address these challenges, we introduce EFRame, an Exploration-Filter-Replay framework that augments GRPO across three dimensions: additional rollouts enable deeper and more targeted exploration, online filtering removes low-quality samples to stabilize gradients and accelerate training, and experience replay amplifies rare yet informative trajectories for stable convergence. This unified framework establishes a principled training cycle that balances exploration, efficiency, and stability. Experiments on diverse reasoning benchmarks demonstrate that EFRame achieves consistent gains, including a 37.9\% relative improvement on Geometry3K over GRPO. EFRame further supports fine-grained sample categorization and precise entropy control, highlighting it as a robust solution for advancing deeper reasoning in LLMs. Our code is available at https://github.com/597358816/EFRame.

