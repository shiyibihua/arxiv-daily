---
layout: default
title: OptScale: Probabilistic Optimality for Inference-time Scaling
---

# OptScale: Probabilistic Optimality for Inference-time Scaling

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.22376" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.22376v4</a>
  <a href="https://arxiv.org/pdf/2506.22376.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.22376v4" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.22376v4', 'OptScale: Probabilistic Optimality for Inference-time Scaling')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Youkang Wang, Jian Wang, Rubing Chen, Xiao-Yong Wei

**ÂàÜÁ±ª**: cs.LG, cs.AI, cs.CL

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-27 (Êõ¥Êñ∞: 2025-12-19)

**Â§áÊ≥®**: Accepted by AAAI-2026

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫OptScale‰ª•Ëß£ÂÜ≥Êé®ÁêÜÊó∂Èó¥Áº©ÊîæÁöÑÊïàÁéáÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Êé®ÁêÜÊó∂Èó¥Áº©Êîæ` `Ê¶ÇÁéáÊ°ÜÊû∂` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `Ê†∑Êú¨ÈÄâÊã©` `ËÆ°ÁÆóÊïàÁéá` `Â§çÊùÇÊé®ÁêÜ` `ÊÄßËÉΩËØÑ‰º∞`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊé®ÁêÜÊó∂Èó¥Áº©ÊîæÊñπÊ≥ïÂ§ö‰æùËµñÂêØÂèëÂºèÁ≠ñÁï•ÔºåÁº∫‰πèÁ≥ªÁªüÊÄßÁöÑÁêÜËÆ∫ÊîØÊåÅÔºåÂØºËá¥ÊïàÁéá‰Ωé‰∏ã„ÄÇ
2. Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊ¶ÇÁéáÊ°ÜÊû∂ÔºåÂΩ¢ÂºèÂåñÊé®ÁêÜÊó∂Èó¥Áº©ÊîæÁöÑÊúÄ‰ºòÊÄßÔºåÂπ∂Êé®ÂØºÂá∫Ê†∑Êú¨Êï∞ÈáèÁöÑÁêÜËÆ∫‰∏ãÁïå„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåOptScaleÂú®Â§ö‰∏™Êé®ÁêÜÂü∫ÂáÜ‰∏äÊòæËëóÈôç‰Ωé‰∫ÜÈááÊ†∑ÂºÄÈîÄÔºåÂêåÊó∂‰øùÊåÅÊàñÊèêÂçá‰∫ÜÊé®ÁêÜÊÄßËÉΩ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êé®ÁêÜÊó∂Èó¥Áº©ÊîæÂ∑≤Êàê‰∏∫ÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÊé®ÁêÜÊÄßËÉΩÁöÑÈáçË¶ÅÊäÄÊúØ„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÊñπÊ≥ïÂæÄÂæÄ‰æùËµñ‰∫éÂêØÂèëÂºèÁ≠ñÁï•ËøõË°åÂπ∂Ë°åÈááÊ†∑ÔºåÁº∫‰πèÁ≥ªÁªüÊÄßÁöÑÁêÜËÆ∫Âü∫Á°Ä„ÄÇ‰∏∫Ê≠§ÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊ¶ÇÁéáÊ°ÜÊû∂ÔºåÂΩ¢ÂºèÂåñÊé®ÁêÜÊó∂Èó¥Áº©ÊîæÁöÑÊúÄ‰ºòÊÄßÔºåÂÅáËÆæÂπ∂Ë°åÊ†∑Êú¨ÊòØÁã¨Á´ãÂêåÂàÜÂ∏ÉÁöÑÔºåÂπ∂‰∏îBest-of-$N$ÈÄâÊã©Á≠ñÁï•ÈÅµÂæ™ÂèØ‰º∞ËÆ°ÁöÑÊ¶ÇÁéáÂàÜÂ∏É„ÄÇÂú®Ê≠§Ê°ÜÊû∂‰∏ãÔºåÊàë‰ª¨Êé®ÂØºÂá∫ÂÆûÁé∞ÁõÆÊ†áÊÄßËÉΩÊ∞¥Âπ≥ÊâÄÈúÄÊ†∑Êú¨Êï∞ÈáèÁöÑÁêÜËÆ∫‰∏ãÁïåÔºå‰∏∫ËÆ°ÁÆóÈ´òÊïàÁöÑÁº©ÊîæÊèê‰æõ‰∫ÜÈ¶ñ‰∏™Á≥ªÁªüÊÄßÊåáÂØº„ÄÇÂü∫‰∫éËøô‰∏ÄÊ¥ûÂØüÔºåÊàë‰ª¨ÂºÄÂèë‰∫ÜOptScaleÁÆóÊ≥ïÔºåÂä®ÊÄÅÁ°ÆÂÆöÊúÄ‰ºòÈááÊ†∑ÂìçÂ∫îÊï∞Èáè„ÄÇOptScaleÂà©Áî®ËØ≠Ë®ÄÊ®°ÂûãÈ¢ÑÊµãÂô®‰º∞ËÆ°Ê¶ÇÁéáÂÖàÈ™åÂèÇÊï∞Ôºå‰ªéËÄåÂÜ≥ÂÆöÊª°Ë∂≥È¢ÑÂÆö‰πâÊÄßËÉΩÈòàÂÄºÂíåÁΩÆ‰ø°Ê∞¥Âπ≥ÊâÄÈúÄÁöÑÊúÄÂ∞èÊ†∑Êú¨Êï∞Èáè„ÄÇÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåOptScaleÊòæËëóÂáèÂ∞ë‰∫ÜÈááÊ†∑ÂºÄÈîÄÔºåÂêåÊó∂Âú®Êé®ÁêÜÊÄßËÉΩ‰∏ä‰∏éÊúÄÂÖàËøõÁöÑÊñπÊ≥ïÁõ∏ÂΩìÊàñÊõ¥‰ºò„ÄÇÊàë‰ª¨ÁöÑÂ∑•‰Ωú‰∏∫Êé®ÁêÜÊó∂Èó¥Áº©ÊîæÊèê‰æõ‰∫ÜÁêÜËÆ∫Âü∫Á°ÄÂíåÂÆûÈôÖËß£ÂÜ≥ÊñπÊ°àÔºåÂ°´Ë°•‰∫ÜLLMsÂú®Â§çÊùÇÊé®ÁêÜÂ∫îÁî®‰∏≠ÁöÑÂÖ≥ÈîÆÁ©∫ÁôΩ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâÊé®ÁêÜÊó∂Èó¥Áº©ÊîæÊñπÊ≥ïÁº∫‰πèÁêÜËÆ∫Âü∫Á°ÄÂíåÊïàÁéá‰Ωé‰∏ãÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂæÄÂæÄ‰æùËµñÂêØÂèëÂºèÁ≠ñÁï•ÔºåÂØºËá¥Âú®Â§çÊùÇÊé®ÁêÜ‰ªªÂä°‰∏≠Ë°®Áé∞‰∏ç‰Ω≥„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊ¶ÇÁéáÊ°ÜÊû∂ÔºåÂÅáËÆæÂπ∂Ë°åÊ†∑Êú¨ÊòØÁã¨Á´ãÂêåÂàÜÂ∏ÉÁöÑÔºåÂπ∂Êé®ÂØºÂá∫ÂÆûÁé∞ÁõÆÊ†áÊÄßËÉΩÊâÄÈúÄÁöÑÊ†∑Êú¨Êï∞ÈáèÁöÑÁêÜËÆ∫‰∏ãÁïå„ÄÇËøô‰∏ÄÊ°ÜÊû∂‰∏∫ËÆ°ÁÆóÈ´òÊïàÁöÑÊé®ÁêÜÊó∂Èó¥Áº©ÊîæÊèê‰æõ‰∫ÜÁ≥ªÁªüÊÄßÊåáÂØº„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏Ä‰∏™ËØ≠Ë®ÄÊ®°ÂûãÈ¢ÑÊµãÂô®ÔºåÁî®‰∫é‰º∞ËÆ°Ê¶ÇÁéáÂÖàÈ™åÂèÇÊï∞ÔºåÂπ∂Ê†πÊçÆËøô‰∫õÂèÇÊï∞Âä®ÊÄÅÁ°ÆÂÆöÊúÄ‰ºòÁöÑÊ†∑Êú¨Êï∞Èáè„ÄÇËØ•Ê°ÜÊû∂ÁöÑ‰∏ªË¶ÅÊ®°ÂùóÂåÖÊã¨Ê†∑Êú¨ÁîüÊàê„ÄÅÊÄßËÉΩËØÑ‰º∞ÂíåÊúÄ‰ºòÈÄâÊã©„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞Âú®‰∫éÈ¶ñÊ¨°ÊèêÂá∫‰∫ÜÂü∫‰∫éÊ¶ÇÁéáÁöÑÊé®ÁêÜÊó∂Èó¥Áº©ÊîæÁêÜËÆ∫ÔºåÊèê‰æõ‰∫ÜÊ†∑Êú¨Êï∞ÈáèÁöÑÁêÜËÆ∫‰∏ãÁïåÔºå‰∏éÁé∞ÊúâÊñπÊ≥ïÁöÑÂêØÂèëÂºèÁ≠ñÁï•ÂΩ¢ÊàêÈ≤úÊòéÂØπÊØî„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂèÇÊï∞ËÆæÁΩÆ‰∏äÔºåOptScaleÈÄöËøáËØ≠Ë®ÄÊ®°ÂûãÈ¢ÑÊµãÂô®‰º∞ËÆ°ÂÖàÈ™åÂèÇÊï∞ÔºåÂπ∂ËÆæÂÆöÊÄßËÉΩÈòàÂÄºÂíåÁΩÆ‰ø°Ê∞¥Âπ≥Ôºå‰ª•Á°Æ‰øùÊâÄÈúÄÊ†∑Êú¨Êï∞ÈáèÁöÑÊúÄÂ∞èÂåñ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Âú®Â§ö‰∏™Êé®ÁêÜÂü∫ÂáÜÔºàÂ¶ÇMATH-500„ÄÅGSM8K„ÄÅAIMEÂíåAMCÔºâ‰∏äÁöÑÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåOptScaleÂú®ÂáèÂ∞ëÈááÊ†∑ÂºÄÈîÄÁöÑÂêåÊó∂ÔºåÊé®ÁêÜÊÄßËÉΩ‰∏éÊúÄÂÖàËøõÁöÑÊñπÊ≥ïÁõ∏ÂΩìÊàñÊõ¥‰ºòÔºåÂ±ïÁé∞Âá∫ÊòæËëóÁöÑÊïàÁéáÊèêÂçá„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ‰∏≠ÁöÑÂ§çÊùÇÊé®ÁêÜ‰ªªÂä°ÔºåÂ¶ÇÊï∞Â≠¶ÈóÆÈ¢òÊ±ÇËß£„ÄÅÈÄªËæëÊé®ÁêÜÂíåÂØπËØùÁ≥ªÁªüÁ≠â„ÄÇÈÄöËøáÊèê‰æõÈ´òÊïàÁöÑÊé®ÁêÜÊó∂Èó¥Áº©ÊîæÊñπÊ°àÔºåOptScaleËÉΩÂ§üÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÊòæËëóÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõÔºåÈôç‰ΩéËÆ°ÁÆóÊàêÊú¨ÔºåÊé®Âä®Êô∫ËÉΩÁ≥ªÁªüÁöÑÂπøÊ≥õÂ∫îÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Inference-time scaling has emerged as a powerful technique for enhancing the reasoning performance of Large Language Models (LLMs). However, existing approaches often rely on heuristic strategies for parallel sampling, lacking a principled foundation. To address this gap, we propose a probabilistic framework that formalizes the optimality of inference-time scaling under the assumption that parallel samples are independently and identically distributed (i.i.d.), and where the Best-of-$N$ selection strategy follows a probability distribution that can be estimated. Within this framework, we derive a theoretical lower bound on the required number of samples to achieve a target performance level, providing the first principled guidance for compute-efficient scaling. Leveraging this insight, we develop \textsc{OptScale}, a practical algorithm that dynamically determines the optimal number of sampled responses. \textsc{OptScale} employs a language model-based predictor to estimate probabilistic prior parameters, enabling the decision of the minimal number of samples needed that satisfy predefined performance thresholds and confidence levels. Extensive experiments on representative reasoning benchmarks (including MATH-500, GSM8K, AIME, and AMC) demonstrate that \textsc{OptScale} significantly reduces sampling overhead while remaining better or on par with state-of-the-art reasoning performance. Our work offers both a theoretical foundation and a practical solution for principled inference-time scaling, addressing a critical gap in the efficient deployment of LLMs for complex reasoning.

