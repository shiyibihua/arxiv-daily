---
layout: default
title: Smooth-Distill: A Self-distillation Framework for Multitask Learning with Wearable Sensor Data
---

# Smooth-Distill: A Self-distillation Framework for Multitask Learning with Wearable Sensor Data

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2507.00061" class="toolbar-btn" target="_blank">üìÑ arXiv: 2507.00061v1</a>
  <a href="https://arxiv.org/pdf/2507.00061.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2507.00061v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2507.00061v1', 'Smooth-Distill: A Self-distillation Framework for Multitask Learning with Wearable Sensor Data')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Hoang-Dieu Vu, Duc-Nghia Tran, Quang-Tu Pham, Hieu H. Pham, Nicolas Vuillerme, Duc-Tan Tran

**ÂàÜÁ±ª**: cs.LG, cs.AI, eess.SP

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-27

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/Kuan2vn/smooth)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Smooth-DistillÊ°ÜÊû∂‰ª•Ëß£ÂÜ≥ÂèØÁ©øÊà¥‰º†ÊÑüÂô®Êï∞ÊçÆÁöÑÂ§ö‰ªªÂä°Â≠¶‰π†ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Ëá™Ëí∏È¶è` `Â§ö‰ªªÂä°Â≠¶‰π†` `ÂèØÁ©øÊà¥‰º†ÊÑüÂô®` `‰∫∫Á±ªÊ¥ªÂä®ËØÜÂà´` `Âä†ÈÄüÂ∫¶ËÆ°Êï∞ÊçÆ` `Ê®°ÂûãËÆ≠ÁªÉÊïàÁéá` `Ê∑±Â∫¶Â≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÂ§ö‰ªªÂä°Â≠¶‰π†ÊñπÊ≥ïÈÄöÂ∏∏ÈúÄË¶ÅÁã¨Á´ãÁöÑÊïôÂ∏àÂíåÂ≠¶ÁîüÊ®°ÂûãÔºåÂØºËá¥ËÆ°ÁÆóÂºÄÈîÄÂ§ß‰∏îËÆ≠ÁªÉÊïàÁéá‰Ωé„ÄÇ
2. Smooth-DistillÊ°ÜÊû∂ÈÄöËøá‰ΩøÁî®Ê®°ÂûãËá™Ë∫´ÁöÑÂπ≥ÊªëÂéÜÂè≤ÁâàÊú¨‰Ωú‰∏∫ÊïôÂ∏àÔºåÁÆÄÂåñ‰∫ÜËÆ≠ÁªÉËøáÁ®ãÂπ∂ÊèêÈ´ò‰∫ÜÊïàÁéá„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåSmooth-DistillÂú®HARÂíå‰º†ÊÑüÂô®ÊîæÁΩÆÊ£ÄÊµã‰ªªÂä°‰∏≠ÂùáÊòæËëó‰ºò‰∫é‰º†ÁªüÂü∫Á∫øÔºå‰∏îÊî∂ÊïõÊ®°ÂºèÊõ¥Á®≥ÂÆöÔºåËøáÊãüÂêàÁé∞Ë±°ÂáèÂ∞ë„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨Êñá‰ªãÁªç‰∫ÜSmooth-DistillÔºå‰∏ÄÁßçÊñ∞È¢ñÁöÑËá™Ëí∏È¶èÊ°ÜÊû∂ÔºåÊó®Âú®Âà©Áî®ÂèØÁ©øÊà¥‰º†ÊÑüÂô®Êï∞ÊçÆÂêåÊó∂ËøõË°å‰∫∫Á±ªÊ¥ªÂä®ËØÜÂà´ÔºàHARÔºâÂíå‰º†ÊÑüÂô®ÊîæÁΩÆÊ£ÄÊµã„ÄÇËØ•ÊñπÊ≥ïÈááÁî®Áªü‰∏ÄÁöÑÂü∫‰∫éCNNÁöÑÊû∂ÊûÑMTL-netÔºåÂ§ÑÁêÜÂä†ÈÄüÂ∫¶ËÆ°Êï∞ÊçÆÂπ∂‰∏∫ÊØè‰∏™‰ªªÂä°ÂàÜÊîØËæìÂá∫„ÄÇ‰∏é‰º†ÁªüÁöÑËí∏È¶èÊñπÊ≥ï‰∏çÂêåÔºåËØ•Ê°ÜÊû∂‰ΩøÁî®Ê®°ÂûãËá™Ë∫´ÁöÑÂπ≥ÊªëÂéÜÂè≤ÁâàÊú¨‰Ωú‰∏∫ÊïôÂ∏àÔºåÊòæËëóÈôç‰Ωé‰∫ÜËÆ≠ÁªÉËÆ°ÁÆóÂºÄÈîÄÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜÊÄßËÉΩ‰ºòÂäø„ÄÇ‰∏∫ÊîØÊåÅÊú¨Á†îÁ©∂ÔºåÊàë‰ª¨ÂºÄÂèë‰∫Ü‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑÂä†ÈÄüÂ∫¶ËÆ°Êï∞ÊçÆÈõÜÔºåÊçïÊçâ‰∫Ü‰∏âÁßç‰∏çÂêå‰Ω©Êà¥‰ΩçÁΩÆ‰∏ãÁöÑ12Áßç‰∏çÂêåÁù°Âßø„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåSmooth-DistillÂú®‰∏çÂêåËØÑ‰º∞Âú∫ÊôØ‰∏≠ÂßãÁªà‰ºò‰∫éÊõø‰ª£ÊñπÊ≥ïÔºåÂú®‰∫∫Á±ªÊ¥ªÂä®ËØÜÂà´ÂíåËÆæÂ§áÊîæÁΩÆÊ£ÄÊµã‰ªªÂä°‰∏≠ÂùáÂèñÂæó‰∫ÜÊòæËëóÊîπÂñÑ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥ÂèØÁ©øÊà¥‰º†ÊÑüÂô®Êï∞ÊçÆÁöÑÂ§ö‰ªªÂä°Â≠¶‰π†ÈóÆÈ¢òÔºåÂ∞§ÂÖ∂ÊòØ‰∫∫Á±ªÊ¥ªÂä®ËØÜÂà´Âíå‰º†ÊÑüÂô®ÊîæÁΩÆÊ£ÄÊµã„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏‰æùËµñ‰∫éÁã¨Á´ãÁöÑÊïôÂ∏àÂíåÂ≠¶ÁîüÊ®°ÂûãÔºåÂØºËá¥ËÆ≠ÁªÉÊïàÁéá‰Ωé‰∏ãÂíåËÆ°ÁÆóËµÑÊ∫êÊµ™Ë¥π„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöSmooth-DistillÊ°ÜÊû∂ÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®Ê®°ÂûãËá™Ë∫´ÁöÑÂπ≥ÊªëÂéÜÂè≤ÁâàÊú¨‰Ωú‰∏∫ÊïôÂ∏àÊ®°ÂûãÔºå‰ªéËÄåÂáèÂ∞ëËÆ°ÁÆóÂºÄÈîÄÂπ∂ÊèêÈ´òËÆ≠ÁªÉÊïàÁéá„ÄÇËøôÁßçËÆæËÆ°‰ΩøÂæóÊ®°ÂûãÂú®Ëá™ÊàëËí∏È¶è‰∏≠‰øùÊåÅÊÄßËÉΩ‰ºòÂäø„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑ‰∏∫MTL-netÔºåÈááÁî®Áªü‰∏ÄÁöÑCNNÁªìÊûÑÂ§ÑÁêÜÂä†ÈÄüÂ∫¶ËÆ°Êï∞ÊçÆÔºåÂàÜÊîØÂá∫‰∏§‰∏™ËæìÂá∫‰ª•ÂàÜÂà´ÂÆåÊàêHARÂíå‰º†ÊÑüÂô®ÊîæÁΩÆÊ£ÄÊµã‰ªªÂä°„ÄÇÊ°ÜÊû∂ÁöÑËÆæËÆ°‰ΩøÂæó‰∏§‰∏™‰ªªÂä°ÂèØ‰ª•ÂÖ±‰∫´ÁâπÂæÅÔºåÂ¢ûÂº∫‰∫ÜÊ®°ÂûãÁöÑÂ≠¶‰π†ËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞Âú®‰∫é‰ΩøÁî®Âπ≥ÊªëÁöÑÂéÜÂè≤Ê®°Âûã‰Ωú‰∏∫ÊïôÂ∏àÔºåÈÅøÂÖç‰∫Ü‰º†ÁªüËí∏È¶èÊñπÊ≥ïÁöÑÂ§çÊùÇÊÄß„ÄÇËøôÁßçÊñπÊ≥ï‰∏ç‰ªÖÈôç‰Ωé‰∫ÜËÆ°ÁÆóÊàêÊú¨ÔºåËøòÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑÁ®≥ÂÆöÊÄßÂíåÊî∂ÊïõÈÄüÂ∫¶„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÁΩëÁªúÁªìÊûÑ‰∏äÔºåMTL-netÈááÁî®‰∫ÜÂç∑ÁßØÂ±ÇÂíåÂÖ®ËøûÊé•Â±ÇÁöÑÁªÑÂêàÔºåÊçüÂ§±ÂáΩÊï∞ËÆæËÆ°‰∏∫Â§ö‰ªªÂä°ÊçüÂ§±ÔºåÁ°Æ‰øù‰∏§‰∏™‰ªªÂä°ÁöÑÂ≠¶‰π†Áõ∏ËæÖÁõ∏Êàê„ÄÇÂÖ≥ÈîÆÂèÇÊï∞ËÆæÁΩÆÁªèËøáÂ§öÊ¨°ÂÆûÈ™åË∞É‰ºòÔºå‰ª•ËææÂà∞ÊúÄ‰Ω≥ÊÄßËÉΩ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåSmooth-DistillÂú®HARÂíå‰º†ÊÑüÂô®ÊîæÁΩÆÊ£ÄÊµã‰ªªÂä°‰∏≠ÂùáÊòæËëó‰ºò‰∫é‰º†ÁªüÂü∫Á∫øÔºåÂÖ∑‰ΩìË°®Áé∞‰∏∫Âú®Â§ö‰∏™ËØÑ‰º∞Âú∫ÊôØ‰∏≠ÊèêÈ´ò‰∫ÜËØÜÂà´ÂáÜÁ°ÆÁéáÂíåÈôç‰Ωé‰∫ÜËøáÊãüÂêàÁé∞Ë±°ÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®ËÆ≠ÁªÉÊïàÁéáÂíåÊ®°ÂûãÊÄßËÉΩ‰∏äÁöÑÂèåÈáç‰ºòÂäø„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Êô∫ËÉΩÂÆ∂Â±Ö„ÄÅÂÅ•Â∫∑ÁõëÊµãÂíåËøêÂä®ÂàÜÊûêÁ≠âÔºåËÉΩÂ§ü‰∏∫‰∫∫Á±ªÊ¥ªÂä®ËØÜÂà´Á≥ªÁªüÊèê‰æõÈ´òÊïàÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇÊú™Êù•ÔºåËØ•Ê°ÜÊû∂ÂèØËÉΩÂú®ËµÑÊ∫êÂèóÈôêÁöÑÂπ≥Âè∞‰∏äÂÆûÁé∞Êõ¥È¢ëÁπÅÁöÑÊ®°ÂûãÊõ¥Êñ∞ÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄº„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> This paper introduces Smooth-Distill, a novel self-distillation framework designed to simultaneously perform human activity recognition (HAR) and sensor placement detection using wearable sensor data. The proposed approach utilizes a unified CNN-based architecture, MTL-net, which processes accelerometer data and branches into two outputs for each respective task. Unlike conventional distillation methods that require separate teacher and student models, the proposed framework utilizes a smoothed, historical version of the model itself as the teacher, significantly reducing training computational overhead while maintaining performance benefits. To support this research, we developed a comprehensive accelerometer-based dataset capturing 12 distinct sleep postures across three different wearing positions, complementing two existing public datasets (MHealth and WISDM). Experimental results show that Smooth-Distill consistently outperforms alternative approaches across different evaluation scenarios, achieving notable improvements in both human activity recognition and device placement detection tasks. This method demonstrates enhanced stability in convergence patterns during training and exhibits reduced overfitting compared to traditional multitask learning baselines. This framework contributes to the practical implementation of knowledge distillation in human activity recognition systems, offering an effective solution for multitask learning with accelerometer data that balances accuracy and training efficiency. More broadly, it reduces the computational cost of model training, which is critical for scenarios requiring frequent model updates or training on resource-constrained platforms. The code and model are available at https://github.com/Kuan2vn/smooth\_distill.

