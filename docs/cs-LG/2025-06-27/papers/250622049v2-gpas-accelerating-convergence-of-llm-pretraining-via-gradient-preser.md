---
layout: default
title: GPAS: Accelerating Convergence of LLM Pretraining via Gradient-Preserving Activation Scaling
---

# GPAS: Accelerating Convergence of LLM Pretraining via Gradient-Preserving Activation Scaling

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.22049" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.22049v2</a>
  <a href="https://arxiv.org/pdf/2506.22049.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.22049v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.22049v2', 'GPAS: Accelerating Convergence of LLM Pretraining via Gradient-Preserving Activation Scaling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tianhao Chen, Xin Xu, Zijing Liu, Pengxiang Li, Xinyuan Song, Ajay Kumar Jaiswal, Fan Zhang, Jishan Hu, Yang Wang, Hao Chen, Shizhe Diao, Shiwei Liu, Yu Li, Lu Yin, Can Yang

**åˆ†ç±»**: cs.LG, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-27 (æ›´æ–°: 2025-07-03)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/dandingsky/GPAS)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGPASä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒä¸­çš„æ¿€æ´»æ–¹å·®é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `é¢„è®­ç»ƒ` `æ¿€æ´»æ–¹å·®` `æ¢¯åº¦ä¿æŒ` `æ·±åº¦å­¦ä¹ ` `Transformer` `æ¨¡å‹ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é¢„å±‚å½’ä¸€åŒ–Transformeråœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­æ¿€æ´»æ–¹å·®å‘ˆæŒ‡æ•°å¢é•¿ï¼Œé™åˆ¶äº†æ·±å±‚å­¦ä¹ èƒ½åŠ›ã€‚
2. GPASé€šè¿‡ç¼©å°ä¸­é—´æ¿€æ´»å€¼è€Œä¿æŒå…¶æ¢¯åº¦ä¸å˜ï¼Œè§£å†³äº†æ¿€æ´»æ–¹å·®é—®é¢˜ï¼Œé¿å…äº†æ¢¯åº¦æ¶ˆå¤±ã€‚
3. åœ¨71Måˆ°1Bçš„æ¨¡å‹è§„æ¨¡ä¸Šï¼ŒGPASå‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå¹¶é€‚ç”¨äºå¤šç§æ¶æ„ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°ä»£å¤§è¯­è¨€æ¨¡å‹ï¼Œå¦‚LLaMAã€Qwenå’ŒDeepSeekç³»åˆ—ï¼Œä¸»è¦é‡‡ç”¨é¢„å±‚å½’ä¸€åŒ–ï¼ˆPre-LNï¼‰Transformeræ¶æ„ã€‚å°½ç®¡Pre-LNåœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­ç¨³å®šä¸”å¯æ‰©å±•è‡³å¤§æ¨¡å‹ï¼Œä½†å…¶æ¿€æ´»æ–¹å·®åœ¨å±‚é—´å‘ˆæŒ‡æ•°å¢é•¿ï¼Œå¯¼è‡´å¿«æ·è¿æ¥ä¸»å¯¼å­å±‚è¾“å‡ºï¼Œä»è€Œé™åˆ¶äº†æ·±å±‚çš„å­¦ä¹ èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•çš„æŠ€æœ¯â€”â€”æ¢¯åº¦ä¿æŒæ¿€æ´»ç¼©æ”¾ï¼ˆGPASï¼‰ï¼Œé€šè¿‡ç¼©å°ä¸­é—´æ¿€æ´»å€¼è€Œä¿æŒå…¶æ¢¯åº¦ä¸å˜ï¼Œé¿å…äº†æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒGPASåœ¨71Måˆ°1Bçš„ä¸åŒæ¨¡å‹è§„æ¨¡ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä¸”åœ¨å…¶ä»–æ¶æ„å¦‚Sandwich-LNå’ŒDeepNormä¸­ä¹Ÿå±•ç°äº†è‰¯å¥½çš„é€‚ç”¨æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒä¸­æ¿€æ´»æ–¹å·®çš„æŒ‡æ•°å¢é•¿é—®é¢˜ï¼Œç°æœ‰çš„é¢„å±‚å½’ä¸€åŒ–Transformeræ¶æ„åœ¨æ­¤æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œå¯¼è‡´æ·±å±‚çš„å­¦ä¹ èƒ½åŠ›å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šGPASçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ç¼©å°ä¸­é—´æ¿€æ´»å€¼ï¼ŒåŒæ—¶ä¿æŒå…¶æ¢¯åº¦ä¸å˜ï¼Œä»è€Œé¿å…æ¿€æ´»ä¿¡æ¯çš„ä¸¢å¤±å’Œæ¢¯åº¦æ¶ˆå¤±çš„é—®é¢˜ã€‚è¿™ç§è®¾è®¡ä½¿å¾—æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿæ›´å¥½åœ°ä¿æŒä¿¡æ¯æµåŠ¨ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGPASçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ¿€æ´»ç¼©æ”¾æ¨¡å—ï¼Œè¯¥æ¨¡å—åœ¨æ¯ä¸€å±‚çš„å‰å‘ä¼ æ’­ä¸­åº”ç”¨ï¼Œå…·ä½“æµç¨‹ä¸ºï¼šè®¡ç®—ä¸­é—´æ¿€æ´»å€¼ï¼Œè¿›è¡Œç¼©æ”¾å¤„ç†ï¼Œç„¶åä¼ é€’è‡³åç»­å±‚ï¼ŒåŒæ—¶ä¿æŒæ¢¯åº¦çš„åŸå§‹çŠ¶æ€ã€‚

**å…³é”®åˆ›æ–°**ï¼šGPASçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶æ¢¯åº¦ä¿æŒæœºåˆ¶ï¼Œè¿™ä¸ä¼ ç»Ÿçš„æ¿€æ´»ç¼©æ”¾æ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ï¼Œåè€…é€šå¸¸ä¼šå¯¼è‡´æ¢¯åº¦æ¶ˆå¤±ï¼Œä»è€Œå½±å“æ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®ç°GPASæ—¶ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬æ¿€æ´»ç¼©æ”¾æ¯”ä¾‹çš„é€‰æ‹©ï¼Œä»¥åŠå¦‚ä½•åœ¨ä¸åŒæ¨¡å‹è§„æ¨¡ä¸‹è°ƒæ•´è¯¥æ¯”ä¾‹ä»¥è·å¾—æœ€ä½³æ•ˆæœã€‚æ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„çš„è®¾è®¡ä¹Ÿéœ€ä¸GPASç›¸é€‚åº”ï¼Œä»¥ç¡®ä¿å…¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGPASåœ¨ä¸åŒè§„æ¨¡çš„æ¨¡å‹ï¼ˆä»71Måˆ°1Bï¼‰ä¸Šå‡å®ç°äº†æ€§èƒ½æå‡ï¼Œå°¤å…¶åœ¨æ·±å±‚ç½‘ç»œä¸­è¡¨ç°å°¤ä¸ºçªå‡ºã€‚ä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼ŒGPASåœ¨è®­ç»ƒç¨³å®šæ€§å’Œæ”¶æ•›é€Ÿåº¦ä¸Šå‡æœ‰æ˜¾è‘—æ”¹å–„ï¼Œè¯æ˜äº†å…¶åœ¨å¤šç§æ¶æ„ä¸­çš„æœ‰æ•ˆæ€§å’Œé€‚ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

GPASçš„ç ”ç©¶æˆæœå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨å¤§è¯­è¨€æ¨¡å‹çš„é¢„è®­ç»ƒå’Œå¾®è°ƒé˜¶æ®µã€‚é€šè¿‡æ”¹å–„æ¿€æ´»æ–¹å·®é—®é¢˜ï¼ŒGPASèƒ½å¤Ÿæå‡æ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›ï¼Œè¿›è€Œåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ–‡æœ¬ç”Ÿæˆç­‰é¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ã€‚æœªæ¥ï¼ŒGPASä¹Ÿå¯èƒ½è¢«åº”ç”¨äºå…¶ä»–æ·±åº¦å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒä¼˜åŒ–ä¸­ï¼Œæ¨åŠ¨æ›´é«˜æ•ˆçš„æ¨¡å‹å¼€å‘ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Modern Large Language Models, such as the LLaMA, Qwen and DeepSeek series, predominantly adopt the Pre-LayerNorm (Pre-LN) Transformer architecture. While being stable during pretraining and scalable to large model sizes, Pre-LN suffers from an exponential growth in activation variance across layers, causing the shortcut to dominate over sub-layer outputs in the residual connection and limiting the learning capacity of deeper layers. To mitigate this issue, we propose Gradient-Preserving Activation Scaling (GPAS), a simple technique that can be used in combination with existing approaches. GPAS works by scaling down the intermediate activations while keeping their gradients unchanged. This leaves information in the activations intact, and avoids the gradient vanishing problem associated with gradient downscaling. Extensive experiments across various model sizes from 71M to 1B show that GPAS achieves consistent performance gains. Beyond enhancing Pre-LN Transformers, GPAS also shows promise in improving alternative architectures such as Sandwich-LN and DeepNorm, demonstrating its versatility and potential for improving training dynamics in a wide range of settings. Our code is available at https://github.com/dandingsky/GPAS.

