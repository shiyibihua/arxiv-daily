---
layout: default
title: Layer Importance for Mathematical Reasoning is Forged in Pre-Training and Invariant after Post-Training
---

# Layer Importance for Mathematical Reasoning is Forged in Pre-Training and Invariant after Post-Training

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.22638" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.22638v2</a>
  <a href="https://arxiv.org/pdf/2506.22638.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.22638v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.22638v2', 'Layer Importance for Mathematical Reasoning is Forged in Pre-Training and Invariant after Post-Training')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Aadim Nepal, Safal Shrestha, Anubhav Shrestha, Minwu Kim, Jalal Naghiyev, Ravid Shwartz-Ziv, Keith Ross

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-27 (æ›´æ–°: 2025-11-05)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå±‚é‡è¦æ€§åˆ†æä»¥ä¼˜åŒ–æ•°å­¦æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ•°å­¦æ¨ç†` `å±‚çº§æ¶ˆè` `å¤§å‹è¯­è¨€æ¨¡å‹` `é¢„è®­ç»ƒ` `åè®­ç»ƒ` `å½’ä¸€åŒ–äº’ä¿¡æ¯` `æ¨¡å‹ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨æ•°å­¦æ¨ç†èƒ½åŠ›æå‡ä¸Šå­˜åœ¨ä¸ç¡®å®šæ€§ï¼Œéš¾ä»¥åˆ¤æ–­æ˜¯ç»“æ„æ€§å˜åŒ–è¿˜æ˜¯å°å¹…è°ƒæ•´å¯¼è‡´çš„ã€‚
2. è®ºæ–‡é€šè¿‡å±‚çº§æ¶ˆèå®éªŒï¼Œæ­ç¤ºæ•°å­¦æ¨ç†ä¾èµ–äºå°‘æ•°å…³é”®å±‚ï¼Œè¿™äº›å±‚åœ¨é¢„è®­ç»ƒä¸­å½¢æˆå¹¶ä¿æŒç¨³å®šã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œç§»é™¤å…³é”®å±‚ä¼šå¯¼è‡´æ•°å­¦å‡†ç¡®ç‡ä¸‹é™80%ï¼Œè€Œäº‹å®å›å¿†ä»»åŠ¡çš„ä¸‹é™å¹…åº¦è¾ƒå°ï¼Œæ˜¾ç¤ºå‡ºå±‚çš„é‡è¦æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç»è¿‡æŒ‡ä»¤è°ƒä¼˜ã€å¼ºåŒ–å­¦ä¹ æˆ–çŸ¥è¯†è’¸é¦åï¼Œæ•°å­¦èƒ½åŠ›æœ‰æ‰€æå‡ã€‚æœ¬æ–‡æ¢è®¨è¿™äº›æå‡æ˜¯å¦æºäºå˜æ¢å™¨å±‚çš„é‡å¤§å˜åŒ–ï¼Œè¿˜æ˜¯ä»…ä»…æ˜¯ä¿æŒåŸæœ‰ç»“æ„çš„å°è°ƒæ•´ã€‚é€šè¿‡å¯¹åŸºç¡€å’Œè®­ç»ƒå˜ä½“çš„å±‚çº§æ¶ˆèå®éªŒï¼Œå‘ç°æ•°å­¦æ¨ç†ä¾èµ–äºå°‘æ•°å…³é”®å±‚ï¼Œè¿™äº›å±‚åœ¨æ‰€æœ‰åè®­ç»ƒæ–¹æ³•ä¸­ä¿æŒé‡è¦æ€§ã€‚ç§»é™¤è¿™äº›å±‚ä¼šå¯¼è‡´æ•°å­¦å‡†ç¡®ç‡ä¸‹é™å¤šè¾¾80%ï¼Œè€Œäº‹å®å›å¿†ä»»åŠ¡çš„ä¸‹é™å¹…åº¦ç›¸å¯¹è¾ƒå°ã€‚è¿™è¡¨æ˜ï¼Œé’ˆå¯¹æ•°å­¦ä»»åŠ¡çš„ä¸“ç”¨å±‚åœ¨é¢„è®­ç»ƒæœŸé—´å½¢æˆï¼Œå¹¶åœ¨åç»­ä¿æŒç¨³å®šã€‚é€šè¿‡å½’ä¸€åŒ–äº’ä¿¡æ¯ï¼ˆNMIï¼‰æµ‹é‡ï¼Œå‘ç°æ¥è¿‘è¿™äº›å…³é”®å±‚çš„æ ‡è®°ä»åŸå§‹å¥æ³•ç°‡æ¼‚ç§»ï¼Œæœå‘ä¸å¥æ³•å…³ç³»è¾ƒå¼±ä½†å¯¹ä¸‹æ¸¸ä»»åŠ¡æ›´æœ‰ç”¨çš„è¡¨ç¤ºã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨æ¢è®¨å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†èƒ½åŠ›æå‡çš„æœºåˆ¶ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½æ˜ç¡®å±‚çº§å˜åŒ–ä¸æ€§èƒ½æå‡ä¹‹é—´çš„å…³ç³»ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å±‚çº§æ¶ˆèå®éªŒï¼Œåˆ†æä¸åŒå±‚å¯¹æ•°å­¦æ¨ç†çš„è´¡çŒ®ï¼ŒéªŒè¯å…³é”®å±‚åœ¨é¢„è®­ç»ƒå’Œåè®­ç»ƒä¸­çš„ç¨³å®šæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨å±‚çº§æ¶ˆèæŠ€æœ¯ï¼Œå¯¹åŸºç¡€æ¨¡å‹å’Œç»è¿‡è®­ç»ƒçš„å˜ä½“è¿›è¡Œæ¯”è¾ƒï¼Œé‡ç‚¹å…³æ³¨æ•°å­¦æ¨ç†ç›¸å…³çš„å…³é”®å±‚ã€‚

**å…³é”®åˆ›æ–°**ï¼šå‘ç°æ•°å­¦æ¨ç†ä¾èµ–äºå°‘æ•°å…³é”®å±‚ï¼Œè¿™äº›å±‚åœ¨é¢„è®­ç»ƒé˜¶æ®µå½¢æˆå¹¶åœ¨åç»­è®­ç»ƒä¸­ä¿æŒé‡è¦æ€§ï¼ŒåŒºåˆ«äºä¼ ç»Ÿæ–¹æ³•å¯¹æ‰€æœ‰å±‚çš„å‡åŒ€å¤„ç†ã€‚

**å…³é”®è®¾è®¡**ï¼šå®éªŒä¸­ä½¿ç”¨å½’ä¸€åŒ–äº’ä¿¡æ¯ï¼ˆNMIï¼‰æ¥æµ‹é‡å±‚çº§é—´çš„å…³ç³»ï¼Œåˆ†ææ ‡è®°åœ¨å…³é”®å±‚é™„è¿‘çš„æ¼‚ç§»ç°è±¡ï¼Œæ­ç¤ºå…¶å¯¹ä¸‹æ¸¸ä»»åŠ¡çš„å½±å“ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œç§»é™¤å…³é”®å±‚å¯¼è‡´æ•°å­¦å‡†ç¡®ç‡ä¸‹é™å¤šè¾¾80%ï¼Œè€Œåœ¨äº‹å®å›å¿†ä»»åŠ¡ä¸­ï¼Œä¸‹é™å¹…åº¦ç›¸å¯¹è¾ƒå°ã€‚è¿™ä¸€å‘ç°å¼ºè°ƒäº†ç‰¹å®šå±‚åœ¨æ•°å­¦æ¨ç†ä¸­çš„é‡è¦æ€§ï¼Œæä¾›äº†å¯¹æ¨¡å‹ç»“æ„ä¼˜åŒ–çš„æ–°æ€è·¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†æ–¹é¢çš„ä¼˜åŒ–æä¾›äº†æ–°çš„è§†è§’ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨æ•™è‚²ã€è‡ªåŠ¨åŒ–æ¨ç†å’Œæ™ºèƒ½é—®ç­”ç³»ç»Ÿç­‰é¢†åŸŸã€‚æœªæ¥ï¼ŒåŸºäºå…³é”®å±‚çš„è®¾è®¡å¯ä»¥è¿›ä¸€æ­¥æå‡æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models improve at math after instruction tuning, reinforcement learning, or knowledge distillation. We ask whether these gains come from major changes in the transformer layers or from smaller adjustments that keep the original structure. Using layer-wise ablation on base and trained variants, we find that math reasoning depends on a few critical layers, which stay important across all post-training methods. Removing these layers reduces math accuracy by as much as 80%, whereas factual recall tasks only show relatively smaller drops. This suggests that specialized layers for mathematical tasks form during pre-training and remain stable afterward. As measured by Normalized Mutual Information (NMI), we find that near these critical layers, tokens drift from their original syntactic clusters toward representations aligned with tokens less syntactically related but potentially more useful for downstream task.

