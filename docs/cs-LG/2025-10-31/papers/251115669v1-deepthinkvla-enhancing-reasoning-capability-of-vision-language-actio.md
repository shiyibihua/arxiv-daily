---
layout: default
title: DeepThinkVLA: Enhancing Reasoning Capability of Vision-Language-Action Models
---

# DeepThinkVLA: Enhancing Reasoning Capability of Vision-Language-Action Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.15669" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.15669v1</a>
  <a href="https://arxiv.org/pdf/2511.15669.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.15669v1" onclick="toggleFavorite(this, '2511.15669v1', 'DeepThinkVLA: Enhancing Reasoning Capability of Vision-Language-Action Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Cheng Yin, Yankai Lin, Wang Xu, Sikyuen Tam, Xiangrui Zeng, Zhiyuan Liu, Zhouping Yin

**åˆ†ç±»**: cs.LG, cs.AI, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-10-31

**å¤‡æ³¨**: 16 pages, 6 figures, conference

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**DeepThinkVLAé€šè¿‡æ··åˆæ³¨æ„åŠ›æœºåˆ¶å’Œä¸¤é˜¶æ®µè®­ç»ƒæå‡VLAæ¨¡å‹æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹` `æœºå™¨äººæ§åˆ¶` `é“¾å¼æ€è€ƒ` `æ··åˆæ³¨æ„åŠ›æœºåˆ¶` `å¼ºåŒ–å­¦ä¹ ` `å› æœæ¨ç†` `åºåˆ—å†³ç­–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VLAæ¨¡å‹ä½¿ç”¨å•ä¸€è§£ç å™¨å¤„ç†æ¨ç†å’ŒåŠ¨ä½œï¼Œå¯¼è‡´è¿åŠ¨æ§åˆ¶ä¸ä½³ï¼Œæ¨ç†ä¸åŠ¨ä½œé—´å› æœå…³ç³»å¼±ã€‚
2. DeepThinkVLAé‡‡ç”¨æ··åˆæ³¨æ„åŠ›è§£ç å™¨ï¼Œå…ˆè¿›è¡Œåºåˆ—æ¨ç†ï¼Œå†å¹¶è¡Œç”ŸæˆåŠ¨ä½œï¼Œå¹¶ç»“åˆä¸¤é˜¶æ®µè®­ç»ƒã€‚
3. DeepThinkVLAåœ¨LIBEROæµ‹è¯•ä¸­è¾¾åˆ°97.0%çš„æˆåŠŸç‡ï¼Œæ··åˆæ¶æ„æœ¬èº«æå‡15.5%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºDeepThinkVLAï¼Œæ—¨åœ¨æå‡è§†è§‰-è¯­è¨€-åŠ¨ä½œ(VLA)æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå…‹æœç«¯åˆ°ç«¯æœºå™¨äººç­–ç•¥å¯¹æ•°æ®çš„è¿‡åº¦ä¾èµ–ã€‚ç°æœ‰æ¨¡å‹ä½¿ç”¨å•ä¸€è‡ªå›å½’è§£ç å™¨è¿›è¡Œåºåˆ—åŒ–çš„CoTæ¨ç†å’Œé«˜ç»´å¹¶è¡Œæœºå™¨äººåŠ¨ä½œï¼Œå¯¼è‡´è¿åŠ¨æ§åˆ¶æ€§èƒ½ä¸‹é™ï¼Œä¸”æ¨ç†ä¸åŠ¨ä½œä¹‹é—´ç¼ºä¹å¼ºå› æœå…³ç³»ã€‚DeepThinkVLAé€šè¿‡ç´§å¯†é›†æˆçš„æ¶æ„å’Œè®­ç»ƒç­–ç•¥è§£å†³æ­¤å†²çªã€‚è¯¥æ¨¡å‹é‡‡ç”¨æ··åˆæ³¨æ„åŠ›è§£ç å™¨ï¼Œä½¿ç”¨å› æœæ³¨æ„åŠ›ç”Ÿæˆåºåˆ—åŒ–CoTï¼Œç„¶ååˆ‡æ¢åˆ°åŒå‘æ³¨æ„åŠ›ä»¥å¿«é€Ÿå¹¶è¡Œè§£ç åŠ¨ä½œå‘é‡ã€‚é…åˆä¸¤é˜¶æ®µè®­ç»ƒæµç¨‹ï¼šé¦–å…ˆä½¿ç”¨ç›‘ç£å¾®è°ƒ(SFT)è®­ç»ƒæ¨¡å‹çš„åŸºç¡€æ¨ç†èƒ½åŠ›ï¼Œç„¶åä½¿ç”¨å¼ºåŒ–å­¦ä¹ (RL)å’Œä»»åŠ¡æˆåŠŸå¥–åŠ±ï¼Œä½¿å®Œæ•´çš„æ¨ç†-åŠ¨ä½œåºåˆ—ä¸æœŸæœ›ç»“æœå¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDeepThinkVLAåœ¨LIBEROåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†97.0%çš„æˆåŠŸç‡ï¼Œè¾¾åˆ°SOTAæ°´å¹³ã€‚æ¶ˆèå®éªŒéªŒè¯äº†è®¾è®¡çš„æœ‰æ•ˆæ€§ï¼šæ··åˆæ¶æ„æœ¬èº«æ¯”æ ‡å‡†è§£ç å™¨æ€§èƒ½é«˜å‡º15.5%ï¼Œæœ€ç»ˆçš„RLé˜¶æ®µæä¾›äº†å…³é”®çš„2%çš„æ€§èƒ½æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†è§‰-è¯­è¨€-åŠ¨ä½œ(VLA)æ¨¡å‹åœ¨æœºå™¨äººæ§åˆ¶ä»»åŠ¡ä¸­ï¼Œé€šå¸¸é‡‡ç”¨ç«¯åˆ°ç«¯çš„æ–¹å¼ï¼Œéœ€è¦å¤§é‡æ•°æ®è¿›è¡Œè®­ç»ƒã€‚ä¸ºäº†æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œå¯è§£é‡Šæ€§ï¼Œå¼•å…¥äº†Chain-of-Thought (CoT)æ¨ç†ï¼Œå³è®©æ¨¡å‹åœ¨æ‰§è¡ŒåŠ¨ä½œå‰å…ˆè¿›è¡Œæ€è€ƒã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹ä½¿ç”¨å•ä¸€çš„è‡ªå›å½’è§£ç å™¨åŒæ—¶å¤„ç†åºåˆ—åŒ–çš„CoTæ¨ç†å’Œé«˜ç»´ã€å¯å¹¶è¡Œçš„æœºå™¨äººåŠ¨ä½œï¼Œè¿™å¯¼è‡´äº†ä¸¤ä¸ªé—®é¢˜ï¼šä¸€æ˜¯è¿åŠ¨æ§åˆ¶æ€§èƒ½ä¸‹é™ï¼Œå› ä¸ºè‡ªå›å½’è§£ç å™¨ä¸é€‚åˆå¹¶è¡Œç”ŸæˆåŠ¨ä½œï¼›äºŒæ˜¯æ¨ç†å’ŒåŠ¨ä½œä¹‹é—´ç¼ºä¹å¼ºå› æœå…³ç³»ï¼Œæ¨¡å‹éš¾ä»¥å­¦ä¹ åˆ°æœ‰æ•ˆçš„æ¨ç†ç­–ç•¥ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDeepThinkVLAçš„æ ¸å¿ƒæ€è·¯æ˜¯è§£è€¦æ¨ç†å’ŒåŠ¨ä½œçš„ç”Ÿæˆè¿‡ç¨‹ï¼Œå¹¶å»ºç«‹å®ƒä»¬ä¹‹é—´çš„å¼ºå› æœå…³ç³»ã€‚å…·ä½“æ¥è¯´ï¼Œæ¨¡å‹é¦–å…ˆä½¿ç”¨è‡ªå›å½’çš„æ–¹å¼è¿›è¡ŒCoTæ¨ç†ï¼Œç„¶åæ ¹æ®æ¨ç†ç»“æœå¹¶è¡Œç”ŸæˆåŠ¨ä½œã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œè®ºæ–‡è®¾è®¡äº†ä¸€ç§æ··åˆæ³¨æ„åŠ›è§£ç å™¨ï¼Œå¹¶é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDeepThinkVLAçš„æ•´ä½“æ¶æ„åŒ…å«ä¸€ä¸ªè§†è§‰ç¼–ç å™¨ã€ä¸€ä¸ªè¯­è¨€ç¼–ç å™¨å’Œä¸€ä¸ªæ··åˆæ³¨æ„åŠ›è§£ç å™¨ã€‚è§†è§‰ç¼–ç å™¨å’Œè¯­è¨€ç¼–ç å™¨åˆ†åˆ«ç”¨äºæå–å›¾åƒå’Œæ–‡æœ¬çš„ç‰¹å¾ã€‚æ··åˆæ³¨æ„åŠ›è§£ç å™¨æ˜¯è¯¥æ¨¡å‹çš„æ ¸å¿ƒï¼Œå®ƒé¦–å…ˆä½¿ç”¨å› æœæ³¨æ„åŠ›æœºåˆ¶è¿›è¡ŒCoTæ¨ç†ï¼Œç„¶ååˆ‡æ¢åˆ°åŒå‘æ³¨æ„åŠ›æœºåˆ¶ä»¥å¹¶è¡Œç”ŸæˆåŠ¨ä½œã€‚è®­ç»ƒè¿‡ç¨‹åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µæ˜¯ç›‘ç£å¾®è°ƒ(SFT)ï¼Œä½¿ç”¨äººå·¥æ ‡æ³¨çš„CoTæ•°æ®è®­ç»ƒæ¨¡å‹çš„åŸºç¡€æ¨ç†èƒ½åŠ›ï¼›ç¬¬äºŒé˜¶æ®µæ˜¯å¼ºåŒ–å­¦ä¹ (RL)ï¼Œä½¿ç”¨ä»»åŠ¡æˆåŠŸå¥–åŠ±æ¥ä¼˜åŒ–æ¨¡å‹çš„æ¨ç†å’ŒåŠ¨ä½œç­–ç•¥ï¼Œä»è€Œå»ºç«‹æ¨ç†å’ŒåŠ¨ä½œä¹‹é—´çš„å¼ºå› æœå…³ç³»ã€‚

**å…³é”®åˆ›æ–°**ï¼šDeepThinkVLAçš„å…³é”®åˆ›æ–°åœ¨äºæ··åˆæ³¨æ„åŠ›è§£ç å™¨å’Œä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ã€‚æ··åˆæ³¨æ„åŠ›è§£ç å™¨èƒ½å¤Ÿæ ¹æ®ä»»åŠ¡çš„ä¸åŒï¼Œçµæ´»åœ°åˆ‡æ¢æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»è€Œæ›´å¥½åœ°å¤„ç†åºåˆ—åŒ–çš„æ¨ç†å’Œå¹¶è¡Œçš„åŠ¨ä½œç”Ÿæˆã€‚ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨äººå·¥æ ‡æ³¨æ•°æ®å’Œå¼ºåŒ–å­¦ä¹ å¥–åŠ±ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šæ··åˆæ³¨æ„åŠ›è§£ç å™¨åŒ…å«ä¸¤ä¸ªæ³¨æ„åŠ›æ¨¡å—ï¼šä¸€ä¸ªå› æœæ³¨æ„åŠ›æ¨¡å—å’Œä¸€ä¸ªåŒå‘æ³¨æ„åŠ›æ¨¡å—ã€‚å› æœæ³¨æ„åŠ›æ¨¡å—ç”¨äºç”ŸæˆCoTæ¨ç†ï¼Œå®ƒåªå…è®¸æ¨¡å‹å…³æ³¨ä¹‹å‰çš„tokenï¼Œä»è€Œä¿è¯æ¨ç†çš„åºåˆ—æ€§ã€‚åŒå‘æ³¨æ„åŠ›æ¨¡å—ç”¨äºç”ŸæˆåŠ¨ä½œï¼Œå®ƒå…è®¸æ¨¡å‹å…³æ³¨æ‰€æœ‰çš„tokenï¼Œä»è€Œæ›´å¥½åœ°åˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè®ºæ–‡ä½¿ç”¨äº†Adamä¼˜åŒ–å™¨ï¼Œå­¦ä¹ ç‡è®¾ç½®ä¸º1e-4ã€‚å¼ºåŒ–å­¦ä¹ é˜¶æ®µä½¿ç”¨äº†PPOç®—æ³•ï¼Œå¥–åŠ±å‡½æ•°ä¸ºä»»åŠ¡æˆåŠŸç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

DeepThinkVLAåœ¨LIBEROåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†97.0%çš„æˆåŠŸç‡ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¾¾åˆ°äº†SOTAæ°´å¹³ã€‚æ¶ˆèå®éªŒè¡¨æ˜ï¼Œæ··åˆæ³¨æ„åŠ›æ¶æ„æœ¬èº«æ¯”æ ‡å‡†è§£ç å™¨æ€§èƒ½é«˜å‡º15.5%ï¼Œæœ€ç»ˆçš„å¼ºåŒ–å­¦ä¹ é˜¶æ®µæä¾›äº†å…³é”®çš„2%çš„æ€§èƒ½æå‡ã€‚è¿™äº›ç»“æœéªŒè¯äº†DeepThinkVLAçš„æœ‰æ•ˆæ€§ï¼Œå¹¶è¡¨æ˜æ··åˆæ³¨æ„åŠ›æ¶æ„å’Œä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥æ˜¯æå‡VLAæ¨¡å‹æ¨ç†èƒ½åŠ›çš„å…³é”®ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DeepThinkVLAå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ç”¨äºå„ç§éœ€è¦æ¨ç†èƒ½åŠ›çš„æœºå™¨äººæ§åˆ¶ä»»åŠ¡ï¼Œä¾‹å¦‚å®¶åº­æœåŠ¡æœºå™¨äººã€å·¥ä¸šæœºå™¨äººå’Œè‡ªåŠ¨é©¾é©¶æ±½è½¦ã€‚é€šè¿‡æé«˜æœºå™¨äººçš„æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥ä½¿å…¶æ›´å¥½åœ°ç†è§£äººç±»æŒ‡ä»¤ï¼Œæ›´æœ‰æ•ˆåœ°å®Œæˆå¤æ‚ä»»åŠ¡ï¼Œå¹¶æ›´å¥½åœ°é€‚åº”æœªçŸ¥ç¯å¢ƒã€‚è¯¥ç ”ç©¶è¿˜æœ‰åŠ©äºæå‡VLAæ¨¡å‹åœ¨å…¶ä»–é¢†åŸŸçš„åº”ç”¨ï¼Œä¾‹å¦‚æ™ºèƒ½é—®ç­”å’Œå›¾åƒæè¿°ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Enabling Vision-Language-Action (VLA) models to "think before acting" via Chain-of-Thought (CoT) is a promising path to overcoming the data-hungry nature of end-to-end robot policies. However, progress is stalled by a fundamental conflict: existing models use a single autoregressive decoder for both sequential CoT reasoning and high-dimensional, parallelizable robot actions. This architectural mismatch degrades motor control and fails to forge a strong causal link between thought and action. We introduce DeepThinkVLA, which resolves this conflict through a tightly integrated architecture and training strategy. Architecturally, our hybrid-attention decoder generates sequential CoT with causal attention and then switches to bidirectional attention for fast, parallel decoding of action vectors. This design is complemented by a two-stage training pipeline: we first use Supervised Fine-Tuning (SFT) to teach the model foundational reasoning, then apply Reinforcement Learning (RL) with task-success rewards to causally align the full reasoning-action sequence with desired outcomes. This synergy leads to state-of-the-art performance, achieving a 97.0% success rate on the LIBERO benchmark. Our ablations confirm the design's effectiveness: the hybrid architecture alone outperforms standard decoders by 15.5%, and the final RL stage provides a crucial 2% boost to secure top performance.

