---
layout: default
title: WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning
---

# WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.13305" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.13305v1</a>
  <a href="https://arxiv.org/pdf/2509.13305.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.13305v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.13305v1', 'WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kuan Li, Zhongwang Zhang, Huifeng Yin, Rui Ye, Yida Zhao, Liwen Zhang, Litu Ou, Dingchu Zhang, Xixi Wu, Jialong Wu, Xinyu Wang, Zile Qiao, Zhen Zhang, Yong Jiang, Pengjun Xie, Fei Huang, Jingren Zhou

**åˆ†ç±»**: cs.LG, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-16

**å¤‡æ³¨**: https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**WebSailor-V2ï¼šé€šè¿‡åˆæˆæ•°æ®å’Œå¯æ‰©å±•å¼ºåŒ–å­¦ä¹ å¼¥åˆä¸“æœ‰Agentçš„å·®è·**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `ä¿¡æ¯æœç´¢` `å¼ºåŒ–å­¦ä¹ ` `Agent` `åˆæˆæ•°æ®` `ä¸ç¡®å®šæ€§æ¨ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¼€æºæ¨¡å‹åœ¨å¤æ‚ä¿¡æ¯æœç´¢ä»»åŠ¡ä¸­è¡¨ç°ä¸è¶³ï¼Œç¼ºä¹æœ‰æ•ˆé™ä½ä¸ç¡®å®šæ€§çš„æ¨ç†èƒ½åŠ›ã€‚
2. WebSailoré€šè¿‡ç”Ÿæˆé«˜ä¸ç¡®å®šæ€§ä»»åŠ¡ã€RFTå†·å¯åŠ¨å’ŒDUPOç®—æ³•ï¼Œæå‡Agentåœ¨å¤æ‚ç¯å¢ƒä¸‹çš„æ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒWebSailoråœ¨å¤æ‚ä¿¡æ¯æœç´¢ä»»åŠ¡ä¸­æ€§èƒ½ä¸ä¸“æœ‰Agentç›¸å½“ï¼Œæ˜¾è‘—ä¼˜äºå¼€æºAgentã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¶…è¶Šäººç±»è®¤çŸ¥å±€é™æ€§æ˜¯LLMè®­ç»ƒçš„å…³é”®å‰æ²¿ã€‚è¯¸å¦‚DeepResearchä¹‹ç±»çš„ä¸“æœ‰Agentç³»ç»Ÿå·²åœ¨BrowseCompç­‰æå…¶å¤æ‚çš„ä¿¡æ¯æœç´¢åŸºå‡†ä¸Šå±•ç¤ºäº†è¶…äººçš„èƒ½åŠ›ï¼Œè¿™æ˜¯ä»¥å‰æ— æ³•å®ç°çš„å£®ä¸¾ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œå®ƒä»¬çš„æˆåŠŸå–å†³äºå¼€æºæ¨¡å‹ä¸­ç¼ºä¹çš„å¤æ‚æ¨ç†æ¨¡å¼ï¼šåœ¨å¯¼èˆªå¹¿é˜”çš„ä¿¡æ¯ç¯å¢ƒæ—¶ç³»ç»Ÿåœ°é™ä½æç«¯ä¸ç¡®å®šæ€§çš„èƒ½åŠ›ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†WebSailorï¼Œè¿™æ˜¯ä¸€ç§å®Œæ•´çš„åè®­ç»ƒæ–¹æ³•ï¼Œæ—¨åœ¨çŒè¾“è¿™ç§å…³é”®èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬é€šè¿‡ç»“æ„åŒ–é‡‡æ ·å’Œä¿¡æ¯æ··æ·†ç”Ÿæˆæ–°é¢–çš„ã€é«˜ä¸ç¡®å®šæ€§çš„ä»»åŠ¡ï¼ŒRFTå†·å¯åŠ¨ï¼Œä»¥åŠä¸€ç§é«˜æ•ˆçš„Agentå¼ºåŒ–å­¦ä¹ è®­ç»ƒç®—æ³•ï¼Œå³é‡å¤é‡‡æ ·ç­–ç•¥ä¼˜åŒ–ï¼ˆDUPOï¼‰ã€‚é€šè¿‡è¿™ç§é›†æˆç®¡é“ï¼ŒWebSailoråœ¨å¤æ‚çš„ä¿¡æ¯æœç´¢ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºæ‰€æœ‰å¼€æºAgentï¼Œä¸ä¸“æœ‰Agentçš„æ€§èƒ½ç›¸åŒ¹é…ï¼Œå¹¶ç¼©å°äº†èƒ½åŠ›å·®è·ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¼€æºAgentåœ¨å¤æ‚ä¿¡æ¯æœç´¢ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³çš„é—®é¢˜ã€‚ç°æœ‰å¼€æºæ¨¡å‹ç¼ºä¹æœ‰æ•ˆé™ä½ä¸ç¡®å®šæ€§çš„æ¨ç†èƒ½åŠ›ï¼Œéš¾ä»¥åº”å¯¹ä¿¡æ¯é‡å¤§ã€å¹²æ‰°å› ç´ å¤šçš„ç¯å¢ƒã€‚ä¸“æœ‰Agentå¦‚DeepResearchåœ¨BrowseCompç­‰åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè¶…äººèƒ½åŠ›ï¼Œä½†å…¶æŠ€æœ¯ç»†èŠ‚æœªå…¬å¼€ï¼Œå¼€æºç¤¾åŒºéš¾ä»¥å¤ç°ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡åè®­ç»ƒï¼ˆpost-trainingï¼‰çš„æ–¹å¼ï¼Œä½¿å¼€æºAgentå…·å¤‡ç±»ä¼¼ä¸“æœ‰Agentçš„å¤æ‚æ¨ç†èƒ½åŠ›ã€‚å…·ä½“è€Œè¨€ï¼Œé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„åˆæˆæ•°æ®å’Œå¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œè®©Agentå­¦ä¹ å¦‚ä½•åœ¨ä¿¡æ¯ä¸ç¡®å®šæ€§æé«˜çš„ç¯å¢ƒä¸­è¿›è¡Œæœ‰æ•ˆæ¢ç´¢å’Œå†³ç­–ï¼Œä»è€Œæå‡å…¶ä¿¡æ¯æœç´¢èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šWebSailorçš„æ•´ä½“æ¡†æ¶åŒ…å«ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼š1) **é«˜ä¸ç¡®å®šæ€§ä»»åŠ¡ç”Ÿæˆ**ï¼šé€šè¿‡ç»“æ„åŒ–é‡‡æ ·å’Œä¿¡æ¯æ··æ·†æŠ€æœ¯ï¼Œç”Ÿæˆå…·æœ‰æŒ‘æˆ˜æ€§çš„ä¿¡æ¯æœç´¢ä»»åŠ¡ã€‚2) **RFTå†·å¯åŠ¨**ï¼šåˆ©ç”¨Reasoning from Task (RFT) æ–¹æ³•è¿›è¡Œå†·å¯åŠ¨ï¼Œä¸ºAgentæä¾›åˆæ­¥çš„æ¨ç†èƒ½åŠ›ã€‚3) **DUPOå¼ºåŒ–å­¦ä¹ è®­ç»ƒ**ï¼šé‡‡ç”¨Duplicating Sampling Policy Optimization (DUPO) ç®—æ³•è¿›è¡Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œæå‡Agentçš„ç­–ç•¥ä¼˜åŒ–èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§å®Œæ•´çš„åè®­ç»ƒæ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡å¼€æºAgentåœ¨å¤æ‚ä¿¡æ¯æœç´¢ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚DUPOç®—æ³•æ˜¯å¦ä¸€ä¸ªåˆ›æ–°ç‚¹ï¼Œå®ƒæ˜¯ä¸€ç§é«˜æ•ˆçš„Agentå¼ºåŒ–å­¦ä¹ è®­ç»ƒç®—æ³•ï¼Œèƒ½å¤ŸåŠ é€ŸAgentçš„å­¦ä¹ è¿‡ç¨‹å¹¶æå‡å…¶æ€§èƒ½ã€‚æ­¤å¤–ï¼Œé«˜ä¸ç¡®å®šæ€§ä»»åŠ¡ç”Ÿæˆæ–¹æ³•ä¹Ÿä¸ºAgentæä¾›äº†æ›´å…·æŒ‘æˆ˜æ€§çš„è®­ç»ƒç¯å¢ƒã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ä»»åŠ¡ç”Ÿæˆæ–¹é¢ï¼Œè®ºæ–‡é‡‡ç”¨äº†ç»“æ„åŒ–é‡‡æ ·å’Œä¿¡æ¯æ··æ·†æŠ€æœ¯ï¼Œä»¥ç¡®ä¿ç”Ÿæˆçš„ä»»åŠ¡å…·æœ‰è¶³å¤Ÿçš„ä¸ç¡®å®šæ€§å’ŒæŒ‘æˆ˜æ€§ã€‚åœ¨DUPOç®—æ³•æ–¹é¢ï¼Œè®ºæ–‡å¯èƒ½æ¶‰åŠå¯¹ç­–ç•¥æ¢¯åº¦ç®—æ³•çš„æ”¹è¿›ï¼Œä¾‹å¦‚å¼•å…¥äº†é‡å¤é‡‡æ ·æœºåˆ¶ï¼Œä»¥æé«˜æ ·æœ¬åˆ©ç”¨ç‡å’Œè®­ç»ƒæ•ˆç‡ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç­‰æŠ€æœ¯ç»†èŠ‚åœ¨è®ºæ–‡ä¸­åº”è¯¥æœ‰æ›´è¯¦ç»†çš„æè¿°ï¼ˆæœªçŸ¥ï¼‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

WebSailoråœ¨å¤æ‚ä¿¡æ¯æœç´¢ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºæ‰€æœ‰å¼€æºAgentï¼Œä¸ä¸“æœ‰Agentçš„æ€§èƒ½ç›¸åŒ¹é…ï¼Œç¼©å°äº†èƒ½åŠ›å·®è·ã€‚å…·ä½“æ€§èƒ½æ•°æ®å’Œå¯¹æ¯”åŸºçº¿éœ€è¦åœ¨è®ºæ–‡ä¸­æŸ¥æ‰¾ï¼ˆæœªçŸ¥ï¼‰ï¼Œä½†æ‘˜è¦æ˜ç¡®æŒ‡å‡ºWebSailorè¾¾åˆ°äº†ä¸ä¸“æœ‰Agentç›¸å½“çš„æ°´å¹³ï¼Œè¿™æ˜¯ä¸€ä¸ªé‡è¦çš„çªç ´ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ™ºèƒ½å®¢æœã€çŸ¥è¯†å›¾è°±æ„å»ºã€èˆ†æƒ…åˆ†æã€é‡‘èé£é™©è¯„ä¼°ç­‰é¢†åŸŸã€‚é€šè¿‡æå‡Agentçš„ä¿¡æ¯æœç´¢å’Œæ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥å¸®åŠ©ç”¨æˆ·æ›´é«˜æ•ˆåœ°è·å–æ‰€éœ€ä¿¡æ¯ï¼Œå¹¶åšå‡ºæ›´æ˜æ™ºçš„å†³ç­–ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åº”ç”¨äºæ›´å¹¿æ³›çš„é¢†åŸŸï¼Œä¾‹å¦‚è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—è¯Šæ–­ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Transcending human cognitive limitations represents a critical frontier in LLM training. Proprietary agentic systems like DeepResearch have demonstrated superhuman capabilities on extremely complex information-seeking benchmarks such as BrowseComp, a feat previously unattainable. We posit that their success hinges on a sophisticated reasoning pattern absent in open-source models: the ability to systematically reduce extreme uncertainty when navigating vast information landscapes. Based on this insight, we introduce WebSailor, a complete post-training methodology designed to instill this crucial capability. Our approach involves generating novel, high-uncertainty tasks through structured sampling and information obfuscation, RFT cold start, and an efficient agentic RL training algorithm, Duplicating Sampling Policy Optimization (DUPO). With this integrated pipeline, WebSailor significantly outperforms all open-source agents in complex information-seeking tasks, matching proprietary agents' performance and closing the capability gap.

