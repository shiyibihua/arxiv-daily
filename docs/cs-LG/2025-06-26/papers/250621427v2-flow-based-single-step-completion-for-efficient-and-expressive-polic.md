---
layout: default
title: Flow-Based Single-Step Completion for Efficient and Expressive Policy Learning
---

# Flow-Based Single-Step Completion for Efficient and Expressive Policy Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21427" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.21427v2</a>
  <a href="https://arxiv.org/pdf/2506.21427.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21427v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21427v2', 'Flow-Based Single-Step Completion for Efficient and Expressive Policy Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Prajwal Koirala, Cody Fleming

**åˆ†ç±»**: cs.LG, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-06-26 (æ›´æ–°: 2025-07-23)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå•æ­¥å®Œæˆç­–ç•¥ä»¥æé«˜ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç¦»çº¿å¼ºåŒ–å­¦ä¹ ` `ç”Ÿæˆæ¨¡å‹` `æµåŒ¹é…` `å•æ­¥å®Œæˆç­–ç•¥` `ç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ ` `æ¼”å‘˜-è¯„è®ºå®¶` `é«˜æ•ˆå†³ç­–` `å¤šæ¨¡æ€åŠ¨ä½œç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç”Ÿæˆæ¨¡å‹åœ¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­å­˜åœ¨é«˜æ¨ç†æˆæœ¬å’Œè®­ç»ƒä¸ç¨³å®šæ€§çš„é—®é¢˜ï¼Œå½±å“äº†å…¶åº”ç”¨æ•ˆæœã€‚
2. æœ¬æ–‡æå‡ºçš„å•æ­¥å®Œæˆç­–ç•¥ï¼ˆSSCPï¼‰é€šè¿‡å¢å¼ºæµåŒ¹é…ç›®æ ‡ï¼Œç›´æ¥ç”ŸæˆåŠ¨ä½œï¼Œæå‡äº†ç”Ÿæˆæ•ˆç‡ã€‚
3. SSCPåœ¨å¤šä¸ªæ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºæ‰©æ•£åŸºçº¿æ˜¾è‘—æé«˜äº†é€Ÿåº¦å’Œé€‚åº”æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç”Ÿæˆæ¨¡å‹å¦‚æ‰©æ•£å’ŒæµåŒ¹é…åœ¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­æä¾›äº†ä¸°å¯Œçš„å¤šæ¨¡æ€åŠ¨ä½œåˆ†å¸ƒï¼Œä½†å…¶è¿­ä»£é‡‡æ ·å¯¼è‡´é«˜æ¨ç†æˆæœ¬å’Œè®­ç»ƒä¸ç¨³å®šã€‚æœ¬æ–‡æå‡ºäº†å•æ­¥å®Œæˆç­–ç•¥ï¼ˆSSCPï¼‰ï¼Œé€šè¿‡å¢å¼ºçš„æµåŒ¹é…ç›®æ ‡ç›´æ¥é¢„æµ‹å®Œæˆå‘é‡ï¼Œå®ç°å‡†ç¡®çš„ä¸€æ¬¡æ€§åŠ¨ä½œç”Ÿæˆã€‚SSCPç»“åˆäº†ç”Ÿæˆæ¨¡å‹çš„è¡¨ç°åŠ›ä¸å•æ¨¡æ€ç­–ç•¥çš„è®­ç»ƒå’Œæ¨ç†æ•ˆç‡ï¼Œé¿å…äº†é•¿æ—¶é—´çš„åå‘ä¼ æ’­é“¾ã€‚è¯¥æ–¹æ³•åœ¨ç¦»çº¿ã€ç¦»çº¿åˆ°åœ¨çº¿åŠåœ¨çº¿å¼ºåŒ–å­¦ä¹ è®¾ç½®ä¸­æœ‰æ•ˆæ‰©å±•ï¼Œæ˜¾è‘—æå‡äº†é€Ÿåº¦å’Œé€‚åº”æ€§ã€‚SSCPè¿˜æ‰©å±•åˆ°ç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ ï¼Œä½¿å¹³é¢ç­–ç•¥èƒ½å¤Ÿåˆ©ç”¨å­ç›®æ ‡ç»“æ„è€Œæ— éœ€æ˜¾å¼çš„å±‚æ¬¡æ¨ç†ã€‚è¯¥æ–¹æ³•åœ¨æ ‡å‡†çš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ å’Œè¡Œä¸ºå…‹éš†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œæˆä¸ºæ·±åº¦å¼ºåŒ–å­¦ä¹ å’Œåºåˆ—å†³ç­–çš„å¤šåŠŸèƒ½ã€é«˜æ•ˆæ¡†æ¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰ç”Ÿæˆæ¨¡å‹åœ¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­ç”±äºè¿­ä»£é‡‡æ ·å¯¼è‡´çš„é«˜æ¨ç†æˆæœ¬å’Œè®­ç»ƒä¸ç¨³å®šæ€§çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•éœ€è¦é•¿æ—¶é—´çš„åå‘ä¼ æ’­ï¼Œå½±å“äº†è®­ç»ƒæ•ˆç‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šå•æ­¥å®Œæˆç­–ç•¥ï¼ˆSSCPï¼‰é€šè¿‡å¢å¼ºçš„æµåŒ¹é…ç›®æ ‡ï¼Œç›´æ¥ä»ä¸­é—´æµæ ·æœ¬é¢„æµ‹å®Œæˆå‘é‡ï¼Œå®ç°ä¸€æ¬¡æ€§åŠ¨ä½œç”Ÿæˆï¼Œé¿å…äº†é•¿æ—¶é—´çš„åå‘ä¼ æ’­é“¾ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSSCPåœ¨ç¦»çº¿æ¼”å‘˜-è¯„è®ºå®¶æ¡†æ¶ä¸­è¿ä½œï¼Œç»“åˆäº†ç”Ÿæˆæ¨¡å‹çš„è¡¨ç°åŠ›ä¸å•æ¨¡æ€ç­–ç•¥çš„é«˜æ•ˆæ€§ã€‚è¯¥æ–¹æ³•åŒ…æ‹¬ç”Ÿæˆæ¨¡å‹è®­ç»ƒã€åŠ¨ä½œç”Ÿæˆå’Œç­–ç•¥ä¼˜åŒ–ç­‰ä¸»è¦æ¨¡å—ã€‚

**å…³é”®åˆ›æ–°**ï¼šSSCPçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶ä¸€æ¬¡æ€§åŠ¨ä½œç”Ÿæˆèƒ½åŠ›ï¼Œæ˜¾è‘—æé«˜äº†ç”Ÿæˆæ•ˆç‡ï¼Œå¹¶ä¸”åœ¨ç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ ä¸­èƒ½å¤Ÿåˆ©ç”¨å­ç›®æ ‡ç»“æ„ï¼Œé¿å…äº†æ˜¾å¼å±‚æ¬¡æ¨ç†ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒSSCPé‡‡ç”¨äº†å¢å¼ºçš„æµåŒ¹é…ç›®æ ‡ä½œä¸ºæŸå¤±å‡½æ•°ï¼Œç½‘ç»œç»“æ„ä¸Šåˆ™ç»“åˆäº†ç”Ÿæˆæ¨¡å‹ä¸æ¼”å‘˜-è¯„è®ºå®¶æ¶æ„ï¼Œç¡®ä¿äº†é«˜æ•ˆçš„è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨å®éªŒéƒ¨åˆ†è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨æ ‡å‡†çš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ å’Œè¡Œä¸ºå…‹éš†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒSSCPç›¸è¾ƒäºæ‰©æ•£åŸºçº¿åœ¨é€Ÿåº¦å’Œé€‚åº”æ€§ä¸Šå®ç°äº†æ˜¾è‘—æå‡ï¼Œå…·ä½“æ€§èƒ½æ•°æ®è¡¨æ˜å…¶åœ¨å¤šä¸ªä»»åŠ¡ä¸­å‡è¡¨ç°ä¼˜å¼‚ï¼ŒéªŒè¯äº†å…¶ä½œä¸ºé«˜æ•ˆæ¡†æ¶çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ã€æ¸¸æˆAIç­‰ï¼Œèƒ½å¤Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­å®ç°é«˜æ•ˆçš„å†³ç­–åˆ¶å®šã€‚é€šè¿‡æé«˜ç¦»çº¿å¼ºåŒ–å­¦ä¹ çš„æ•ˆç‡ï¼ŒSSCPæœ‰æœ›æ¨åŠ¨æ™ºèƒ½ä½“åœ¨çœŸå®ä¸–ç•Œä¸­çš„åº”ç”¨ï¼Œæå‡å…¶é€‚åº”æ€§å’Œçµæ´»æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Generative models such as diffusion and flow-matching offer expressive policies for offline reinforcement learning (RL) by capturing rich, multimodal action distributions, but their iterative sampling introduces high inference costs and training instability due to gradient propagation across sampling steps. We propose the \textit{Single-Step Completion Policy} (SSCP), a generative policy trained with an augmented flow-matching objective to predict direct completion vectors from intermediate flow samples, enabling accurate, one-shot action generation. In an off-policy actor-critic framework, SSCP combines the expressiveness of generative models with the training and inference efficiency of unimodal policies, without requiring long backpropagation chains. Our method scales effectively to offline, offline-to-online, and online RL settings, offering substantial gains in speed and adaptability over diffusion-based baselines. We further extend SSCP to goal-conditioned RL, enabling flat policies to exploit subgoal structures without explicit hierarchical inference. SSCP achieves strong results across standard offline RL and behavior cloning benchmarks, positioning it as a versatile, expressive, and efficient framework for deep RL and sequential decision-making.

