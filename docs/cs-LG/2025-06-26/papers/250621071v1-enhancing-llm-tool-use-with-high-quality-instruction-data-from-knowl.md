---
layout: default
title: Enhancing LLM Tool Use with High-quality Instruction Data from Knowledge Graph
---

# Enhancing LLM Tool Use with High-quality Instruction Data from Knowledge Graph

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21071" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.21071v1</a>
  <a href="https://arxiv.org/pdf/2506.21071.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21071v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21071v1', 'Enhancing LLM Tool Use with High-quality Instruction Data from Knowledge Graph')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jingwei Wang, Zai Zhang, Hao Qian, Chunjing Gan, Binbin Hu, Ziqi Liu, Zhiqiang Zhang, Jun Zhou, Bin Shi, Bo Dong

**åˆ†ç±»**: cs.LG, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-26

**å¤‡æ³¨**: 20 pages, 12 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åˆ©ç”¨çŸ¥è¯†å›¾è°±ç”Ÿæˆé«˜è´¨é‡æŒ‡ä»¤æ•°æ®ä»¥æå‡LLMå·¥å…·ä½¿ç”¨èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `çŸ¥è¯†å›¾è°±` `æŒ‡ä»¤æ•°æ®` `å·¥å…·ä½¿ç”¨` `é—®é¢˜è§£å†³` `æ•°æ®ç”Ÿæˆ` `æ¨¡å‹å¾®è°ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–LLMsç”ŸæˆæŒ‡ä»¤æ•°æ®ï¼Œå¯¼è‡´ç”Ÿæˆçš„æ•°æ®è´¨é‡ä¸è¶³ï¼Œå½±å“å·¥å…·çš„æœ‰æ•ˆä½¿ç”¨ã€‚
2. æœ¬æ–‡æå‡ºåˆ©ç”¨çŸ¥è¯†å›¾è°±ç”Ÿæˆé«˜è´¨é‡æŒ‡ä»¤æ•°æ®ï¼Œé€šè¿‡æå–æŸ¥è¯¢è·¯å¾„å’Œè½¬åŒ–å®ä½“å…³ç³»æ¥å®ç°ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨å°‘é‡åˆæˆæ•°æ®è¿›è¡Œå¾®è°ƒå¯ä»¥æ˜¾è‘—æé«˜LLMsçš„å·¥å…·ä½¿ç”¨èƒ½åŠ›å’Œæ•´ä½“æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ•™å¯¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æœ‰æ•ˆä½¿ç”¨å·¥å…·å¯¹äºæå‡å…¶é—®é¢˜è§£å†³èƒ½åŠ›å’Œæ‰©å±•åº”ç”¨è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–LLMsç”ŸæˆæŒ‡ä»¤æ•°æ®ï¼Œå¯¼è‡´æ•°æ®è´¨é‡ä¸è¶³ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°æ–¹æ³•ï¼Œåˆ©ç”¨çŸ¥è¯†å›¾è°±ç”Ÿæˆé«˜è´¨é‡çš„æŒ‡ä»¤æ•°æ®ã€‚é€šè¿‡ä»çŸ¥è¯†å›¾è°±ä¸­æå–æŸ¥è¯¢è·¯å¾„ï¼Œå°†å…¶è½¬åŒ–ä¸ºç”¨æˆ·æŸ¥è¯¢ï¼Œå¹¶å°†å®ä½“é—´çš„å…³ç³»è½¬åŒ–ä¸ºå¯æ“ä½œçš„å·¥å…·ï¼Œæœ€ç»ˆç”Ÿæˆè¯¦ç»†çš„è§£å†³æ­¥éª¤ã€‚å®éªŒè¡¨æ˜ï¼Œä»…é€šè¿‡å°‘é‡åˆæˆæ•°æ®çš„å¾®è°ƒå³å¯æ˜¾è‘—æå‡LLMsçš„å·¥å…·åˆ©ç”¨ç‡å’Œæ•´ä½“èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å·¥å…·ä½¿ç”¨ä¸­çš„æœ‰æ•ˆæ€§é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•ç”Ÿæˆçš„æŒ‡ä»¤æ•°æ®è´¨é‡ä¸è¶³ï¼Œé™åˆ¶äº†æ¨¡å‹çš„åº”ç”¨æ½œåŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡åˆ©ç”¨çŸ¥è¯†å›¾è°±çš„ä¸°å¯Œè¯­ä¹‰ä¿¡æ¯ï¼Œæå–æŸ¥è¯¢è·¯å¾„å¹¶å°†å…¶è½¬åŒ–ä¸ºé«˜è´¨é‡çš„ç”¨æˆ·æŸ¥è¯¢ï¼Œä»è€Œç”Ÿæˆæœ‰æ•ˆçš„æŒ‡ä»¤æ•°æ®ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æµç¨‹åŒ…æ‹¬çŸ¥è¯†å›¾è°±çš„æŸ¥è¯¢è·¯å¾„æå–ã€ç”¨æˆ·æŸ¥è¯¢ç”Ÿæˆã€å®ä½“å…³ç³»è½¬åŒ–ä¸ºå·¥å…·ã€ä»¥åŠè¯¦ç»†è§£å†³æ­¥éª¤çš„è§£æï¼Œå½¢æˆé«˜è´¨é‡çš„æŒ‡ä»¤æ•°æ®é›†ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºå°†çŸ¥è¯†å›¾è°±ä¸LLMsç»“åˆï¼Œç”Ÿæˆé«˜è´¨é‡çš„æŒ‡ä»¤æ•°æ®ï¼Œæ˜¾è‘—æå‡äº†å·¥å…·ä½¿ç”¨çš„æœ‰æ•ˆæ€§ï¼Œä¸ä¼ ç»Ÿä¾èµ–æ¨¡å‹ç”Ÿæˆæ•°æ®çš„æ–¹æ³•æœ¬è´¨ä¸åŒã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†é€‚å½“çš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–æŒ‡ä»¤æ•°æ®çš„ç”Ÿæˆè´¨é‡ï¼ŒåŒæ—¶è®¾è®¡äº†é€‚åˆçŸ¥è¯†å›¾è°±çš„ç½‘ç»œç»“æ„ï¼Œä»¥ç¡®ä¿ä¿¡æ¯çš„æœ‰æ•ˆæå–ä¸è½¬åŒ–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨åˆæˆçš„é«˜è´¨é‡æŒ‡ä»¤æ•°æ®è¿›è¡Œå¾®è°ƒåï¼ŒLLMsåœ¨å·¥å…·ä½¿ç”¨èƒ½åŠ›ä¸Šæå‡äº†æ˜¾è‘—çš„æ€§èƒ½ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°XX%ï¼ˆå…·ä½“æ•°æ®éœ€æ ¹æ®å®éªŒç»“æœå¡«å†™ï¼‰ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•å…·æœ‰æ˜æ˜¾ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨åŒ–å®¢æœã€æ•™è‚²è¾…å¯¼ç­‰ï¼Œèƒ½å¤Ÿæå‡LLMsåœ¨å¤æ‚ä»»åŠ¡ä¸­çš„å·¥å…·ä½¿ç”¨èƒ½åŠ›ï¼Œè¿›è€Œæ‰©å±•å…¶åº”ç”¨åœºæ™¯å’Œå®é™…ä»·å€¼ã€‚æœªæ¥ï¼Œéšç€çŸ¥è¯†å›¾è°±çš„ä¸æ–­å®Œå–„ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åœ¨æ›´å¤šé¢†åŸŸå®ç°æ›´å¹¿æ³›çš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Teaching large language models (LLMs) to use tools is crucial for improving their problem-solving abilities and expanding their applications. However, effectively using tools is challenging because it requires a deep understanding of tool functionalities and user intentions. Previous methods relied mainly on LLMs to generate instruction data, but the quality of these data was often insufficient. In this paper, we propose a new method that uses knowledge graphs to generate high-quality instruction data for LLMs. Knowledge graphs are manually curated datasets rich in semantic information. We begin by extracting various query pathways from a given knowledge graph, which are transformed into a broad spectrum of user queries. We then translate the relationships between entities into actionable tools and parse the pathways of each query into detailed solution steps, thereby creating high-quality instruction data. Our experiments show that fine-tuning on just a small sample of this synthetic data can significantly improve the tool utilization and overall capabilities of LLMs.

