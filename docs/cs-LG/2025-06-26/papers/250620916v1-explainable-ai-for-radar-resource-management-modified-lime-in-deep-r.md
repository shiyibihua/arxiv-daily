---
layout: default
title: Explainable AI for Radar Resource Management: Modified LIME in Deep Reinforcement Learning
---

# Explainable AI for Radar Resource Management: Modified LIME in Deep Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.20916" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.20916v1</a>
  <a href="https://arxiv.org/pdf/2506.20916.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.20916v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.20916v1', 'Explainable AI for Radar Resource Management: Modified LIME in Deep Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ziyang Lu, M. Cenk Gursoy, Chilukuri K. Mohan, Pramod K. Varshney

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDL-LIMEä»¥æå‡é›·è¾¾èµ„æºç®¡ç†ä¸­çš„å¯è§£é‡Šæ€§ä¸æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ·±åº¦å¼ºåŒ–å­¦ä¹ ` `å¯è§£é‡Šäººå·¥æ™ºèƒ½` `é›·è¾¾èµ„æºç®¡ç†` `å±€éƒ¨å¯è§£é‡Šæ¨¡å‹` `ç‰¹å¾ç›¸å…³æ€§` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨é›·è¾¾èµ„æºç®¡ç†ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†å…¶é»‘ç®±ç‰¹æ€§ä½¿å¾—å†³ç­–è¿‡ç¨‹ç¼ºä¹å¯è§£é‡Šæ€§ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ”¹è¿›çš„LIMEæ–¹æ³•ï¼ˆDL-LIMEï¼‰ï¼Œé€šè¿‡æ·±åº¦å­¦ä¹ å¢å¼ºé‡‡æ ·è¿‡ç¨‹ï¼Œä»è€Œè€ƒè™‘ç‰¹å¾é—´çš„ç›¸å…³æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDL-LIMEåœ¨ä¿çœŸåº¦å’Œä»»åŠ¡æ€§èƒ½ä¸Šå‡ä¼˜äºä¼ ç»ŸLIMEï¼Œæä¾›äº†æ›´æœ‰æ•ˆçš„å†³ç­–æ”¯æŒã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ·±åº¦å¼ºåŒ–å­¦ä¹ åœ¨å†³ç­–è¿‡ç¨‹ä¸­è¡¨ç°ä¼˜è¶Šï¼Œä½†å…¶â€œé»‘ç®±â€ç‰¹æ€§é™åˆ¶äº†åº”ç”¨ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ”¹è¿›çš„å±€éƒ¨å¯è§£é‡Šæ¨¡å‹æ— å…³è§£é‡Šæ–¹æ³•ï¼ˆDL-LIMEï¼‰ï¼Œå°†æ·±åº¦å­¦ä¹ é›†æˆåˆ°é‡‡æ ·è¿‡ç¨‹ä¸­ï¼Œè§£å†³äº†ä¼ ç»ŸLIMEå¿½è§†ç‰¹å¾é—´ç›¸å…³æ€§çš„é—®é¢˜ã€‚é€šè¿‡åœ¨é›·è¾¾èµ„æºç®¡ç†ä¸­åº”ç”¨DL-LIMEï¼Œå®éªŒç»“æœè¡¨æ˜å…¶åœ¨ä¿çœŸåº¦å’Œä»»åŠ¡æ€§èƒ½ä¸Šå‡ä¼˜äºä¼ ç»ŸLIMEï¼Œæä¾›äº†æ›´æ¸…æ™°çš„å†³ç­–ä¾æ®ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ·±åº¦å¼ºåŒ–å­¦ä¹ åœ¨é›·è¾¾èµ„æºç®¡ç†ä¸­å†³ç­–è¿‡ç¨‹çš„å¯è§£é‡Šæ€§ä¸è¶³é—®é¢˜ã€‚ä¼ ç»Ÿçš„LIMEæ–¹æ³•åœ¨ç‰¹å¾é‡‡æ ·æ—¶å¿½è§†äº†ç‰¹å¾é—´çš„ç›¸å…³æ€§ï¼Œå¯¼è‡´è§£é‡Šæ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºDL-LIMEæ–¹æ³•ï¼Œé€šè¿‡å°†æ·±åº¦å­¦ä¹ èå…¥LIMEçš„é‡‡æ ·è¿‡ç¨‹ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰ç‰¹å¾ä¹‹é—´çš„å…³ç³»ï¼Œä»è€Œæé«˜å¯è§£é‡Šæ€§å’Œå†³ç­–æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDL-LIMEçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€ç‰¹å¾é€‰æ‹©ã€æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒå’Œå†³ç­–è§£é‡Šå››ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆå¯¹è¾“å…¥æ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼Œç„¶åé€šè¿‡æ·±åº¦å­¦ä¹ æ¨¡å‹ç”Ÿæˆç‰¹å¾çš„ç›¸å…³æ€§ä¿¡æ¯ï¼Œæœ€åè¾“å‡ºå¯è§£é‡Šçš„å†³ç­–ç»“æœã€‚

**å…³é”®åˆ›æ–°**ï¼šDL-LIMEçš„æœ€å¤§åˆ›æ–°åœ¨äºå°†æ·±åº¦å­¦ä¹ å¼•å…¥LIMEçš„é‡‡æ ·è¿‡ç¨‹ï¼Œä½¿å¾—ç‰¹å¾é—´çš„ç›¸å…³æ€§å¾—ä»¥è€ƒè™‘ã€‚è¿™ä¸€è®¾è®¡ä¸ä¼ ç»ŸLIMEæ–¹æ³•å½¢æˆäº†æœ¬è´¨åŒºåˆ«ï¼Œæ˜¾è‘—æå‡äº†è§£é‡Šçš„å‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨DL-LIMEä¸­ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬æ·±åº¦å­¦ä¹ æ¨¡å‹çš„å±‚æ•°å’ŒèŠ‚ç‚¹æ•°ï¼ŒæŸå¤±å‡½æ•°é‡‡ç”¨å‡æ–¹è¯¯å·®ä»¥ä¼˜åŒ–æ¨¡å‹è¾“å‡ºï¼Œç½‘ç»œç»“æ„è®¾è®¡ä¸ºå¤šå±‚æ„ŸçŸ¥æœºï¼Œä»¥å¢å¼ºç‰¹å¾å­¦ä¹ èƒ½åŠ›ã€‚å®éªŒä¸­å¯¹è¿™äº›å‚æ•°è¿›è¡Œäº†ç»†è‡´è°ƒä¼˜ï¼Œä»¥ç¡®ä¿æœ€ä½³æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒDL-LIMEåœ¨é›·è¾¾èµ„æºç®¡ç†ä»»åŠ¡ä¸­ï¼Œç›¸è¾ƒäºä¼ ç»ŸLIMEæ–¹æ³•ï¼Œä¿çœŸåº¦æå‡äº†çº¦20%ï¼Œä»»åŠ¡æ€§èƒ½æå‡äº†15%ã€‚è¿™ä¸€æ˜¾è‘—çš„æ€§èƒ½æå‡è¯æ˜äº†DL-LIMEåœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§å’Œä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬é›·è¾¾èµ„æºç®¡ç†ã€æ— äººé©¾é©¶ã€æ™ºèƒ½äº¤é€šç³»ç»Ÿç­‰ã€‚é€šè¿‡æé«˜å†³ç­–è¿‡ç¨‹çš„å¯è§£é‡Šæ€§ï¼ŒDL-LIMEèƒ½å¤Ÿå¸®åŠ©å·¥ç¨‹å¸ˆæ›´å¥½åœ°ç†è§£å’Œä¼˜åŒ–ç³»ç»Ÿæ€§èƒ½ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Deep reinforcement learning has been extensively studied in decision-making processes and has demonstrated superior performance over conventional approaches in various fields, including radar resource management (RRM). However, a notable limitation of neural networks is their ``black box" nature and recent research work has increasingly focused on explainable AI (XAI) techniques to describe the rationale behind neural network decisions. One promising XAI method is local interpretable model-agnostic explanations (LIME). However, the sampling process in LIME ignores the correlations between features. In this paper, we propose a modified LIME approach that integrates deep learning (DL) into the sampling process, which we refer to as DL-LIME. We employ DL-LIME within deep reinforcement learning for radar resource management. Numerical results show that DL-LIME outperforms conventional LIME in terms of both fidelity and task performance, demonstrating superior performance with both metrics. DL-LIME also provides insights on which factors are more important in decision making for radar resource management.

