---
layout: default
title: APO: Enhancing Reasoning Ability of MLLMs via Asymmetric Policy Optimization
---

# APO: Enhancing Reasoning Ability of MLLMs via Asymmetric Policy Optimization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21655" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.21655v1</a>
  <a href="https://arxiv.org/pdf/2506.21655.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21655v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21655v1', 'APO: Enhancing Reasoning Ability of MLLMs via Asymmetric Policy Optimization')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Minjie Hong, Zirun Guo, Yan Xia, Zehan Wang, Ziang Zhang, Tao Jin, Zhou Zhao

**ÂàÜÁ±ª**: cs.LG, cs.AI, cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-26

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/Indolent-Kawhi/View-R1)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫‰∏çÂØπÁß∞Á≠ñÁï•‰ºòÂåñ‰ª•ÊèêÂçáÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã` `Êé®ÁêÜËÉΩÂäõ` `Âº∫ÂåñÂ≠¶‰π†` `‰∏çÂØπÁß∞Á≠ñÁï•‰ºòÂåñ` `ÈöæÂ∫¶Ëá™ÈÄÇÂ∫îÊï£Â∫¶Ë∞ÉÊï¥` `Ê¨°‰ºòËΩ®ËøπÂ§çÊùÇÂ∫¶Ê≠£ÂàôÂåñ` `Ê®°ÂûãÊ≥õÂåñ` `Â§çÊùÇÊé®ÁêÜ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®Â§çÊùÇÊé®ÁêÜ‰ªªÂä°‰∏≠Ë°®Áé∞‰∏ç‰Ω≥ÔºåÂº∫ÂåñÂ≠¶‰π†ÁöÑÂ∫îÁî®ÂØºËá¥ÊÄßËÉΩ‰∏ãÈôçÂíåËøáÂ∫¶Êé®ÁêÜÁé∞Ë±°„ÄÇ
2. Êú¨ÊñáÊèêÂá∫‰∏çÂØπÁß∞Á≠ñÁï•‰ºòÂåñÔºàAPOÔºâÔºåÈÄöËøáÂ∞ÜÂìçÂ∫îÂàÜ‰∏∫Ê≠£Ë¥üÊ†∑Êú¨ÔºåÂä®ÊÄÅË∞ÉÊï¥KLÊï£Â∫¶ÊùÉÈáç‰ª•ÊèêÈ´òËÆ≠ÁªÉÁ®≥ÂÆöÊÄßÂíåÂà©Áî®Áéá„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåView-R1-3BÊ®°ÂûãÂú®Êé®ÁêÜËÉΩÂäõ‰∏äÂπ≥ÂùáÊèêÂçá7%ÔºåÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë∂ÖË∂ä‰∫Ü7-11BÁöÑÂÖ∂‰ªñÂ§ßÂûãMLLMs„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®Êï¥ÂêàÂ§öÊ†∑Êï∞ÊçÆÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂú®Â§çÊùÇÊé®ÁêÜ‰∏äÂ∏∏Â∏∏Èù¢‰∏¥ÊåëÊàò„ÄÇÂ∞ΩÁÆ°Âº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÂèØ‰ª•Â¢ûÂº∫LLMsÁöÑÊé®ÁêÜËÉΩÂäõÔºå‰ΩÜÂú®MLLMs‰∏≠ÁöÑÂ∫îÁî®Âç¥Â≠òÂú®ÊÄßËÉΩ‰∏ãÈôçÂíåËøáÂ∫¶Êé®ÁêÜÁ≠âÈóÆÈ¢ò„ÄÇÊú¨ÊñáÊèêÂá∫‰∏çÂØπÁß∞Á≠ñÁï•‰ºòÂåñÔºàAPOÔºâÔºåÈÄöËøáÂ∞ÜÈááÊ†∑ÂìçÂ∫îÂàÜ‰∏∫Ê≠£Ë¥ü‰∏§ÁªÑÔºåÂà©Áî®ÈöæÂ∫¶Ëá™ÈÄÇÂ∫îÊï£Â∫¶Ë∞ÉÊï¥ÔºàDADSÔºâÂíåÊ¨°‰ºòËΩ®ËøπÂ§çÊùÇÂ∫¶Ê≠£ÂàôÂåñÔºàSTCRÔºâÊù•Ëß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢ò„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂ∫îÁî®APOÁöÑView-R1-3BÊ®°ÂûãÂú®Êé®ÁêÜËÉΩÂäõ‰∏äÂπ≥ÂùáÊèêÂçá7%ÔºåÂπ∂Âú®Â§ö‰∏™Êé®ÁêÜÂü∫ÂáÜ‰∏äË∂ÖË∂ä‰∫ÜÊõ¥Â§ßÁöÑMLLMsÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜÂØπ‰∏ÄËà¨‰ªªÂä°ÁöÑËâØÂ•ΩË°®Áé∞„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®Â§çÊùÇÊé®ÁêÜ‰ªªÂä°‰∏≠ÁöÑÊÄßËÉΩ‰∏ãÈôçÂíåËøáÂ∫¶Êé®ÁêÜÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®Â∫îÁî®Âº∫ÂåñÂ≠¶‰π†Êó∂ÔºåÂ∏∏ÂØºËá¥Ê®°ÂûãÂú®‰∏ÄËà¨‰ªªÂä°‰∏äÁöÑË°®Áé∞‰∏ç‰Ω≥„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊèêÂá∫‰∏çÂØπÁß∞Á≠ñÁï•‰ºòÂåñÔºàAPOÔºâÔºåÂ∞ÜÈááÊ†∑ÂìçÂ∫îÂàÜ‰∏∫Ê≠£Ë¥ü‰∏§ÁªÑ„ÄÇÂØπÊ≠£Ê†∑Êú¨Â∫îÁî®ÈöæÂ∫¶Ëá™ÈÄÇÂ∫îÊï£Â∫¶Ë∞ÉÊï¥ÔºàDADSÔºâÔºåÂä®ÊÄÅË∞ÉÊï¥KLÊï£Â∫¶ÊùÉÈáçÔºõÂØπË¥üÊ†∑Êú¨Â∫îÁî®Ê¨°‰ºòËΩ®ËøπÂ§çÊùÇÂ∫¶Ê≠£ÂàôÂåñÔºàSTCRÔºâÔºå‰ª•ÂáèÂ∞ëËøáÂ∫¶Êé®ÁêÜ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöAPOÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏§‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöÊ≠£Ê†∑Êú¨Â§ÑÁêÜÊ®°ÂùóÂíåË¥üÊ†∑Êú¨Â§ÑÁêÜÊ®°Âùó„ÄÇÊ≠£Ê†∑Êú¨ÈÄöËøáDADSËøõË°åÂä®ÊÄÅË∞ÉÊï¥ÔºåË¥üÊ†∑Êú¨ÂàôÈÄöËøáSTCRËøõË°åÂ§çÊùÇÂ∫¶ÊéßÂà∂„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöDADSÂíåSTCRÊòØÊú¨ÊñáÁöÑÊ†∏ÂøÉÂàõÊñ∞ÔºåDADSÈÄöËøáÂä®ÊÄÅË∞ÉÊï¥KLÊï£Â∫¶ÊùÉÈáçÊù•ÊèêÈ´òËÆ≠ÁªÉÁ®≥ÂÆöÊÄßÔºåËÄåSTCRÂàôÈÄöËøáÊÉ©ÁΩöËøáÈïøÁöÑÂìçÂ∫îÊù•ÈºìÂä±ÁÆÄÊ¥ÅÊé®ÁêÜ„ÄÇËøô‰∏éÁé∞ÊúâÊñπÊ≥ïÁöÑÊú¨Ë¥®Âå∫Âà´Âú®‰∫éÊõ¥Â•ΩÂú∞Âπ≥Ë°°‰∫ÜÊé¢Á¥¢‰∏éÂà©Áî®„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®DADS‰∏≠ÔºåKLÊï£Â∫¶ÊùÉÈáçÊ†πÊçÆÊ†∑Êú¨ÈöæÂ∫¶Âä®ÊÄÅË∞ÉÊï¥ÔºõÂú®STCR‰∏≠ÔºåËÆæËÆ°‰∫ÜÊÉ©ÁΩöÊú∫Âà∂‰ª•ÈôêÂà∂ÂìçÂ∫îÈïøÂ∫¶„ÄÇËøô‰∫õËÆæËÆ°Á°Æ‰øù‰∫ÜÊ®°ÂûãÂú®‰øùÊåÅÂ∑≤ÊúâÁü•ËØÜÁöÑÂêåÊó∂ÔºåËÉΩÂ§üÊúâÊïàËøõË°åÊé®ÁêÜ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåView-R1-3BÊ®°ÂûãÂú®Êé®ÁêÜËÉΩÂäõ‰∏äÂπ≥ÂùáÊèêÂçá7%ÔºåÂπ∂Âú®Â§ö‰∏™Êé®ÁêÜÂü∫ÂáÜÊµãËØï‰∏≠Ë∂ÖË∂ä‰∫Ü7-11BÁöÑÂÖ∂‰ªñÂ§ßÂûãÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºå‰∏îÂú®‰∏ÄËà¨‰ªªÂä°‰∏ä‰øùÊåÅ‰∫ÜËâØÂ•ΩÁöÑÊÄßËÉΩÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂‰ºòË∂äÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Êô∫ËÉΩÈóÆÁ≠îÁ≥ªÁªü„ÄÅËá™Âä®ÂÜÖÂÆπÁîüÊàêÂíåÂ§öÊ®°ÊÄÅÊï∞ÊçÆÂàÜÊûêÁ≠â„ÄÇÈÄöËøáÊèêÂçáÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõÔºåËÉΩÂ§üÊõ¥Â•ΩÂú∞Â§ÑÁêÜÂ§çÊùÇÁöÑÁî®Êà∑Êü•ËØ¢Âíå‰ªªÂä°ÔºåÂÖ∑ÊúâÂπøÊ≥õÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÊú™Êù•ÂΩ±Âìç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Multimodal Large Language Models (MLLMs) are powerful at integrating diverse data, but they often struggle with complex reasoning. While Reinforcement learning (RL) can boost reasoning in LLMs, applying it to MLLMs is tricky. Common issues include a drop in performance on general tasks and the generation of overly detailed or "overthinking" reasoning. Our work investigates how the KL penalty and overthinking affect RL training in MLLMs. We propose Asymmetric Policy Optimization (APO) to address these issues, which divides the sampled responses into positive and negative groups. For positive samples, Difficulty-Adaptive Divergence Shaping (DADS) is introduced to dynamically adjust the KL divergence weight based on their difficulty. This method prevents policy entropy from dropping sharply, improves training stability, utilizes samples better, and preserves the model's existing knowledge. For negative samples, Suboptimal Trajectory Complexity Regularization (STCR) is proposed to penalize overly long responses. This helps mitigate overthinking and encourages more concise reasoning while preserving the model's explorative capacity. We apply our method to Qwen2.5-VL-3B, creating View-R1-3B. View-R1-3B significantly enhances reasoning capabilities, showing an average 7\% gain over the base model and outperforming larger MLLMs (7-11B) on various reasoning benchmarks. Importantly, unlike other reasoning-tuned MLLMs that often degrade on general tasks, View-R1-3B maintains consistent improvement, demonstrating superior generalization. These results highlight the effectiveness and broad applicability of our DADS and STCR techniques for advancing complex multimodal reasoning in MLLMs. The code will be made available at https://github.com/Indolent-Kawhi/View-R1.

