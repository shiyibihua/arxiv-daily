---
layout: default
title: Distributed Cross-Channel Hierarchical Aggregation for Foundation Models
---

# Distributed Cross-Channel Hierarchical Aggregation for Foundation Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21411" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.21411v1</a>
  <a href="https://arxiv.org/pdf/2506.21411.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21411v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21411v1', 'Distributed Cross-Channel Hierarchical Aggregation for Foundation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Aristeidis Tsaris, Isaac Lyngaas, John Lagregren, Mohamed Wahib, Larry York, Prasanna Balaprakash, Dan Lu, Feiyi Wang, Xiao Wang

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºD-CHAGä»¥è§£å†³å›¾åƒé€šé“èšåˆè®¡ç®—æ•ˆç‡ä½ä¸‹é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å›¾åƒèšåˆ` `è·¨é€šé“èšåˆ` `è®¡ç®—æ•ˆç‡` `é«˜å…‰è°±æˆåƒ` `å¤©æ°”é¢„æŠ¥` `è§†è§‰å˜æ¢å™¨` `æ¨¡å‹å¹¶è¡Œ` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åˆ†å¸ƒå¼æ–¹æ³•åœ¨å¤„ç†å›¾åƒèšåˆæ—¶è®¡ç®—æ•ˆç‡ä½ï¼Œéš¾ä»¥æ»¡è¶³å¤§è§„æ¨¡æ•°æ®é›†çš„éœ€æ±‚ã€‚
2. æå‡ºçš„D-CHAGæ–¹æ³•é€šè¿‡è·¨é€šé“å±‚æ¬¡èšåˆï¼Œä¼˜åŒ–äº†å›¾åƒæ•°æ®çš„å¤„ç†æ•ˆç‡ï¼Œå…¼å®¹å¤šç§å˜æ¢å™¨æ¶æ„ã€‚
3. åœ¨é«˜å…‰è°±æˆåƒå’Œå¤©æ°”é¢„æŠ¥ä»»åŠ¡ä¸­ï¼ŒD-CHAGå®ç°äº†75%çš„å†…å­˜å‡å°‘å’Œè¶…è¿‡ä¸¤å€çš„ååé‡æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºäºè§†è§‰çš„ç§‘å­¦åŸºç¡€æ¨¡å‹åœ¨æ¨åŠ¨ç§‘å­¦å‘ç°å’Œåˆ›æ–°æ–¹é¢å…·æœ‰é‡è¦æ½œåŠ›ã€‚è¿™ç§æ½œåŠ›æºäºå…¶èƒ½å¤Ÿèšåˆæ¥è‡ªä¸åŒæ¥æºçš„å›¾åƒï¼Œå¹¶åˆ©ç”¨å˜æ¢å™¨æ¶æ„å­¦ä¹ æ—¶ç©ºç›¸å…³æ€§ã€‚ç„¶è€Œï¼Œå›¾åƒçš„æ ‡è®°å’Œèšåˆè®¡ç®—å¯†é›†ï¼Œç°æœ‰çš„åˆ†å¸ƒå¼æ–¹æ³•æœªèƒ½å……åˆ†è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†åˆ†å¸ƒå¼è·¨é€šé“å±‚æ¬¡èšåˆï¼ˆD-CHAGï¼‰æ–¹æ³•ï¼Œæ—¨åœ¨å¤„ç†å…·æœ‰å¤§é‡é€šé“çš„å›¾åƒæ•°æ®é›†ã€‚è¯¥æ–¹æ³•å…¼å®¹ä»»ä½•æ¨¡å‹å¹¶è¡Œç­–ç•¥å’Œè§†è§‰å˜æ¢å™¨æ¶æ„ï¼Œæ˜¾è‘—æé«˜è®¡ç®—æ•ˆç‡ã€‚æˆ‘ä»¬åœ¨é«˜å…‰è°±æˆåƒå’Œå¤©æ°”é¢„æŠ¥ä»»åŠ¡ä¸Šè¯„ä¼°äº†D-CHAGï¼Œç»“æœæ˜¾ç¤ºåœ¨Frontierè¶…çº§è®¡ç®—æœºä¸Šï¼Œç»“åˆå¼ é‡å¹¶è¡Œå’Œæ¨¡å‹åˆ†ç‰‡ï¼Œå†…å­˜ä½¿ç”¨å‡å°‘äº†75%ï¼ŒæŒç»­ååé‡æé«˜äº†ä¸¤å€ä»¥ä¸Šã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰åˆ†å¸ƒå¼æ–¹æ³•åœ¨å›¾åƒé€šé“èšåˆæ—¶è®¡ç®—æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤§è§„æ¨¡å¤šé€šé“æ•°æ®é›†æ—¶çš„æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šD-CHAGæ–¹æ³•é€šè¿‡è·¨é€šé“å±‚æ¬¡èšåˆæŠ€æœ¯ï¼Œä¼˜åŒ–äº†æ•°æ®å¤„ç†æµç¨‹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå‡å°‘è®¡ç®—èµ„æºçš„æ¶ˆè€—ï¼Œå¹¶æé«˜æ•°æ®å¤„ç†çš„é€Ÿåº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šD-CHAGçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€å±‚æ¬¡èšåˆæ¨¡å—å’Œè¾“å‡ºç”Ÿæˆæ¨¡å—ã€‚æ•°æ®é¢„å¤„ç†è´Ÿè´£å°†è¾“å…¥å›¾åƒè¿›è¡Œæ ‡å‡†åŒ–ï¼Œå±‚æ¬¡èšåˆæ¨¡å—åˆ™é€šè¿‡å¹¶è¡Œå¤„ç†ä¸åŒé€šé“çš„æ•°æ®ï¼Œæœ€åè¾“å‡ºç”Ÿæˆæ¨¡å—å°†èšåˆç»“æœæ•´åˆä¸ºæœ€ç»ˆçš„æ¨¡å‹è¾“å…¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šD-CHAGçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶è·¨é€šé“èšåˆç­–ç•¥ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒæ¨¡å‹æ€§èƒ½çš„åŒæ—¶æ˜¾è‘—é™ä½å†…å­˜ä½¿ç”¨å’Œè®¡ç®—æ—¶é—´ï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•çš„ä¸²è¡Œå¤„ç†æ–¹å¼å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒD-CHAGé‡‡ç”¨äº†å¼ é‡å¹¶è¡Œå’Œæ¨¡å‹åˆ†ç‰‡æŠ€æœ¯ï¼Œä¼˜åŒ–äº†å†…å­˜ç®¡ç†ï¼Œå¹¶é€šè¿‡è°ƒæ•´ç½‘ç»œç»“æ„å’ŒæŸå¤±å‡½æ•°ï¼Œç¡®ä¿äº†æ¨¡å‹çš„é«˜æ•ˆè®­ç»ƒå’Œæ¨ç†ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨å®éªŒéƒ¨åˆ†è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨Frontierè¶…çº§è®¡ç®—æœºä¸Šï¼ŒD-CHAGæ–¹æ³•ç»“åˆå¼ é‡å¹¶è¡Œå’Œæ¨¡å‹åˆ†ç‰‡æŠ€æœ¯ï¼Œå®ç°äº†é«˜è¾¾75%çš„å†…å­˜ä½¿ç”¨å‡å°‘ï¼Œå¹¶ä¸”åœ¨å¤„ç†èƒ½åŠ›ä¸Šæå‡äº†è¶…è¿‡ä¸¤å€ï¼Œå±•ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬é«˜å…‰è°±æˆåƒã€æ°”è±¡é¢„æµ‹ã€é¥æ„ŸæŠ€æœ¯ç­‰ï¼Œèƒ½å¤Ÿä¸ºç§‘å­¦ç ”ç©¶æä¾›æ›´é«˜æ•ˆçš„æ•°æ®å¤„ç†å·¥å…·ã€‚æœªæ¥ï¼ŒD-CHAGæ–¹æ³•æœ‰æœ›åœ¨æ›´å¹¿æ³›çš„è§†è§‰ä»»åŠ¡ä¸­å¾—åˆ°åº”ç”¨ï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„æŠ€æœ¯è¿›æ­¥ä¸åˆ›æ–°ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-based scientific foundation models hold significant promise for advancing scientific discovery and innovation. This potential stems from their ability to aggregate images from diverse sources such as varying physical groundings or data acquisition systems and to learn spatio-temporal correlations using transformer architectures. However, tokenizing and aggregating images can be compute-intensive, a challenge not fully addressed by current distributed methods. In this work, we introduce the Distributed Cross-Channel Hierarchical Aggregation (D-CHAG) approach designed for datasets with a large number of channels across image modalities. Our method is compatible with any model-parallel strategy and any type of vision transformer architecture, significantly improving computational efficiency. We evaluated D-CHAG on hyperspectral imaging and weather forecasting tasks. When integrated with tensor parallelism and model sharding, our approach achieved up to a 75% reduction in memory usage and more than doubled sustained throughput on up to 1,024 AMD GPUs on the Frontier Supercomputer.

