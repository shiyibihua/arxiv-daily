---
layout: default
title: Chain-of-Thought Enhanced Shallow Transformers for Wireless Symbol Detection
---

# Chain-of-Thought Enhanced Shallow Transformers for Wireless Symbol Detection

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21093" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.21093v1</a>
  <a href="https://arxiv.org/pdf/2506.21093.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21093v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21093v1', 'Chain-of-Thought Enhanced Shallow Transformers for Wireless Symbol Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Li Fan, Peng Wang, Jing Yang, Cong Shen

**åˆ†ç±»**: cs.LG, cs.IT, eess.SP, stat.ML

**å‘å¸ƒæ—¥æœŸ**: 2025-06-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCHOOSEæ¡†æ¶ä»¥è§£å†³æ— çº¿ç¬¦å·æ£€æµ‹ä¸­çš„è®¡ç®—èµ„æºé™åˆ¶é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ— çº¿é€šä¿¡` `å˜å‹å™¨` `ç¬¦å·æ£€æµ‹` `ä¸Šä¸‹æ–‡å­¦ä¹ ` `æµ…å±‚æ¨¡å‹` `è‡ªå›å½’æ¨ç†` `è®¡ç®—æ•ˆç‡` `ç§»åŠ¨è®¾å¤‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ICLåŸºç¡€å˜å‹å™¨æ¨¡å‹ä¾èµ–äºæ·±å±‚æ¶æ„ï¼Œå¯¼è‡´é«˜æ˜‚çš„å­˜å‚¨å’Œè®¡ç®—æˆæœ¬ã€‚
2. æå‡ºCHOOSEæ¡†æ¶ï¼Œé€šè¿‡åœ¨éšè—ç©ºé—´ä¸­å¼•å…¥è‡ªå›å½’æ¨ç†æ­¥éª¤ï¼Œå¢å¼ºæµ…å±‚å˜å‹å™¨çš„æ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCHOOSEåœ¨æ€§èƒ½ä¸Šä¼˜äºä¼ ç»Ÿæµ…å±‚å˜å‹å™¨ï¼Œå¹¶ä¸æ·±å±‚å˜å‹å™¨ç›¸å½“ï¼ŒåŒæ—¶ä¿æŒé«˜æ•ˆçš„å­˜å‚¨å’Œè®¡ç®—æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å˜å‹å™¨åœ¨è§£å†³æ— çº¿é€šä¿¡é—®é¢˜æ–¹é¢å±•ç°äº†æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ä¸­ï¼Œæ¨¡å‹é€šè¿‡æç¤ºé€‚åº”æ–°ä»»åŠ¡è€Œæ— éœ€æ›´æ–°æ¨¡å‹ã€‚ç„¶è€Œï¼Œç°æœ‰åŸºäºICLçš„å˜å‹å™¨æ¨¡å‹ä¾èµ–äºæ·±å±‚æ¶æ„ä»¥å®ç°æ»¡æ„çš„æ€§èƒ½ï¼Œå¯¼è‡´å­˜å‚¨å’Œè®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚æœ¬æ–‡æå‡ºäº†CHain Of thOught Symbol dEtectionï¼ˆCHOOSEï¼‰ï¼Œä¸€ç§å¢å¼ºçš„æµ…å±‚å˜å‹å™¨æ¡†æ¶ï¼Œç”¨äºæ— çº¿ç¬¦å·æ£€æµ‹ã€‚é€šè¿‡åœ¨éšè—ç©ºé—´ä¸­å¼•å…¥è‡ªå›å½’æ½œåœ¨æ¨ç†æ­¥éª¤ï¼ŒCHOOSEæ˜¾è‘—æé«˜äº†æµ…å±‚æ¨¡å‹ï¼ˆ1-2å±‚ï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œè€Œæ— éœ€å¢åŠ æ¨¡å‹æ·±åº¦ã€‚è¿™ä¸€è®¾è®¡ä½¿å¾—è½»é‡çº§å˜å‹å™¨èƒ½å¤Ÿå®ç°ä¸æ›´æ·±æ¨¡å‹ç›¸å½“çš„æ£€æµ‹æ€§èƒ½ï¼Œé€‚åˆåœ¨èµ„æºå—é™çš„ç§»åŠ¨è®¾å¤‡ä¸Šéƒ¨ç½²ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºä¼ ç»Ÿçš„æµ…å±‚å˜å‹å™¨ï¼Œå¹¶ä¸”åœ¨å­˜å‚¨å’Œè®¡ç®—æ•ˆç‡ä¸Šä¿æŒä¼˜åŠ¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ— çº¿ç¬¦å·æ£€æµ‹ä¸­çš„è®¡ç®—èµ„æºé™åˆ¶é—®é¢˜ã€‚ç°æœ‰çš„ICLåŸºç¡€å˜å‹å™¨æ¨¡å‹é€šå¸¸éœ€è¦æ·±å±‚æ¶æ„ä»¥è·å¾—è‰¯å¥½æ€§èƒ½ï¼Œå¯¼è‡´å­˜å‚¨å’Œè®¡ç®—è´Ÿæ‹…è¿‡é‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„CHOOSEæ¡†æ¶é€šè¿‡å¼•å…¥è‡ªå›å½’æ½œåœ¨æ¨ç†æ­¥éª¤ï¼Œå¢å¼ºäº†æµ…å±‚å˜å‹å™¨çš„æ¨ç†èƒ½åŠ›ï¼Œä»è€Œåœ¨ä¸å¢åŠ æ¨¡å‹æ·±åº¦çš„æƒ…å†µä¸‹æå‡æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCHOOSEæ¡†æ¶ç”±1-2å±‚çš„æµ…å±‚å˜å‹å™¨æ„æˆï¼Œç»“åˆè‡ªå›å½’æ¨ç†æ­¥éª¤ï¼Œå½¢æˆä¸€ä¸ªé«˜æ•ˆçš„æ¨ç†è¿‡ç¨‹ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿåœ¨éšè—ç©ºé—´ä¸­è¿›è¡Œæœ‰æ•ˆçš„æ¨ç†ï¼Œé€‚åº”ä¸åŒçš„æ— çº¿ç¬¦å·æ£€æµ‹ä»»åŠ¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šCHOOSEçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºé€šè¿‡è‡ªå›å½’æ¨ç†æ­¥éª¤å¢å¼ºæµ…å±‚æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œè¿™ä¸ä¼ ç»Ÿæ·±å±‚å˜å‹å™¨ä¾èµ–äºå¤šå±‚ç»“æ„çš„æ–¹å¼å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒCHOOSEé‡‡ç”¨äº†1-2å±‚çš„æµ…å±‚å˜å‹å™¨ç»“æ„ï¼Œå¹¶é€šè¿‡ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œå‚æ•°è®¾ç½®ä¼˜åŒ–æ¨ç†è¿‡ç¨‹ï¼Œä»¥ç¡®ä¿åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­ä»èƒ½å®ç°é«˜æ•ˆçš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒCHOOSEæ¡†æ¶åœ¨æ— çº¿ç¬¦å·æ£€æµ‹ä»»åŠ¡ä¸­ï¼Œæ€§èƒ½ä¼˜äºä¼ ç»Ÿçš„æµ…å±‚å˜å‹å™¨ï¼Œå¹¶ä¸æ·±å±‚å˜å‹å™¨çš„æ€§èƒ½ç›¸å½“ã€‚å…·ä½“è€Œè¨€ï¼ŒCHOOSEåœ¨å­˜å‚¨å’Œè®¡ç®—æ•ˆç‡ä¸Šä¿æŒäº†æ˜¾è‘—ä¼˜åŠ¿ï¼Œå±•ç°å‡ºè‰¯å¥½çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç§»åŠ¨é€šä¿¡è®¾å¤‡ã€ç‰©è”ç½‘è®¾å¤‡åŠå…¶ä»–èµ„æºå—é™çš„æ— çº¿æ¥æ”¶å™¨ã€‚é€šè¿‡å®ç°é«˜æ•ˆçš„æ— çº¿ç¬¦å·æ£€æµ‹ï¼ŒCHOOSEæ¡†æ¶èƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­æé«˜æ•°æ®ä¼ è¾“çš„å¯é æ€§å’Œæ•ˆç‡ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Transformers have shown potential in solving wireless communication problems, particularly via in-context learning (ICL), where models adapt to new tasks through prompts without requiring model updates. However, prior ICL-based Transformer models rely on deep architectures with many layers to achieve satisfactory performance, resulting in substantial storage and computational costs. In this work, we propose CHain Of thOught Symbol dEtection (CHOOSE), a CoT-enhanced shallow Transformer framework for wireless symbol detection. By introducing autoregressive latent reasoning steps within the hidden space, CHOOSE significantly improves the reasoning capacity of shallow models (1-2 layers) without increasing model depth. This design enables lightweight Transformers to achieve detection performance comparable to much deeper models, making them well-suited for deployment on resource-constrained mobile devices. Experimental results demonstrate that our approach outperforms conventional shallow Transformers and achieves performance comparable to that of deep Transformers, while maintaining storage and computational efficiency. This represents a promising direction for implementing Transformer-based algorithms in wireless receivers with limited computational resources.

