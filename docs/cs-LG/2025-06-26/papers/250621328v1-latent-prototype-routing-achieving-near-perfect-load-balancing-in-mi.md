---
layout: default
title: Latent Prototype Routing: Achieving Near-Perfect Load Balancing in Mixture-of-Experts
---

# Latent Prototype Routing: Achieving Near-Perfect Load Balancing in Mixture-of-Experts

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21328" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.21328v1</a>
  <a href="https://arxiv.org/pdf/2506.21328.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21328v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21328v1', 'Latent Prototype Routing: Achieving Near-Perfect Load Balancing in Mixture-of-Experts')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiajie Yang

**åˆ†ç±»**: cs.LG, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-26

**å¤‡æ³¨**: 15 pages,4 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ½œåœ¨åŸå‹è·¯ç”±ä»¥è§£å†³æ··åˆä¸“å®¶æ¨¡å‹è´Ÿè½½ä¸å‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ··åˆä¸“å®¶æ¨¡å‹` `è´Ÿè½½å‡è¡¡` `èšç±»æ–¹æ³•` `è·¯ç”±æ¡†æ¶` `å¤§å‹è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ··åˆä¸“å®¶æ¨¡å‹åœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­å­˜åœ¨ä¸¥é‡çš„è´Ÿè½½ä¸å‡è¡¡ï¼Œå¯¼è‡´è®¡ç®—èµ„æºçš„æµªè´¹ã€‚
2. æœ¬æ–‡æå‡ºçš„æ½œåœ¨åŸå‹è·¯ç”±ï¼ˆLPRï¼‰æ¡†æ¶é€šè¿‡èšç±»æ–¹æ³•ä¼˜åŒ–ä¸“å®¶è·¯ç”±ï¼Œæ—¨åœ¨å®ç°ä¸“å®¶çš„å¹³è¡¡åˆ©ç”¨ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLPRæ˜¾è‘—é™ä½äº†ä¸“å®¶è´Ÿè½½çš„åŸºå°¼ç³»æ•°ï¼Œå¹¶æå‡äº†è´Ÿè½½æ¯”ï¼Œè¾¾åˆ°äº†è¿‘ä¹å®Œç¾çš„è´Ÿè½½å¹³è¡¡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„å·²æˆä¸ºé«˜æ•ˆæ‰©å±•å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å…³é”®ç­–ç•¥ã€‚ç„¶è€Œï¼Œç°æœ‰MoEç³»ç»Ÿå­˜åœ¨ä¸¥é‡çš„è´Ÿè½½ä¸å‡è¡¡é—®é¢˜ï¼Œå¯¼è‡´åªæœ‰å°‘æ•°ä¸“å®¶åœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­è¢«æ¿€æ´»ï¼Œä»è€Œé€ æˆæ¨¡å‹èƒ½åŠ›å’Œè®¡ç®—èµ„æºçš„æ˜¾è‘—ä½æ•ˆåˆ©ç”¨ã€‚æœ¬æ–‡é€šè¿‡èšç±»è§†è§’é‡æ–°å®¡è§†ä¸“å®¶è·¯ç”±ï¼Œæå‡ºäº†ä¸€ç§æ–°é¢–çš„è·¯ç”±æ¡†æ¶â€”â€”æ½œåœ¨åŸå‹è·¯ç”±ï¼ˆLPRï¼‰ï¼Œè¯¥æ¡†æ¶åœ¨ä¸å¦¨ç¢ä¸‹æ¸¸æ€§èƒ½çš„æƒ…å†µä¸‹ï¼Œä¿ƒè¿›äº†ä¸“å®¶çš„å¹³è¡¡åˆ©ç”¨ã€‚é€šè¿‡å¯¹å¤šä¸ªå¼€æºMoEæ¨¡å‹ï¼ˆå¦‚DeepSeek-V3ã€Qwen3-MoEå’ŒMixtralï¼‰çš„å¹¿æ³›å®éªŒï¼ŒLPRå°†ä¸“å®¶è´Ÿè½½çš„åŸºå°¼ç³»æ•°ä»0.70é™ä½åˆ°0.035ï¼Œæœ€å°-æœ€å¤§ä¸“å®¶è´Ÿè½½æ¯”ä»1e-6æå‡è‡³0.70ï¼Œå®ç°äº†è¿‘ä¹å®Œç¾çš„è´Ÿè½½å¹³è¡¡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ··åˆä¸“å®¶æ¨¡å‹ä¸­å­˜åœ¨çš„è´Ÿè½½ä¸å‡è¡¡é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¯¼è‡´åªæœ‰å°‘æ•°ä¸“å®¶è¢«æ¿€æ´»ï¼Œé€ æˆè®¡ç®—èµ„æºçš„ä½æ•ˆåˆ©ç”¨å’Œæ¨¡å‹èƒ½åŠ›çš„æµªè´¹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„æ½œåœ¨åŸå‹è·¯ç”±ï¼ˆLPRï¼‰æ¡†æ¶é€šè¿‡èšç±»è§†è§’é‡æ–°å®¡è§†ä¸“å®¶è·¯ç”±ï¼Œæ—¨åœ¨åœ¨ä¸å½±å“ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½çš„å‰æä¸‹ï¼Œä¿ƒè¿›ä¸“å®¶çš„å‡è¡¡æ¿€æ´»å’Œåˆ©ç”¨ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLPRæ¡†æ¶åŒ…æ‹¬ä¸“å®¶èšç±»ã€è·¯ç”±å†³ç­–å’Œè´Ÿè½½å‡è¡¡ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡èšç±»æ–¹æ³•è¯†åˆ«æ½œåœ¨ä¸“å®¶åŸå‹ï¼Œç„¶åæ ¹æ®è¾“å…¥æ•°æ®åŠ¨æ€é€‰æ‹©æ¿€æ´»çš„ä¸“å®¶ï¼Œæœ€åé€šè¿‡è´Ÿè½½å‡è¡¡ç­–ç•¥ä¼˜åŒ–ä¸“å®¶çš„åˆ©ç”¨ç‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šLPRçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶èšç±»è§†è§’çš„ä¸“å®¶è·¯ç”±æ–¹æ³•ï¼Œä¸ä¼ ç»Ÿçš„åŸºäºé˜ˆå€¼æˆ–éšæœºé€‰æ‹©çš„è·¯ç”±æ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å¹³è¡¡ä¸“å®¶è´Ÿè½½ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ•´ä½“æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨LPRä¸­ï¼Œé‡‡ç”¨äº†æ–°çš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–ä¸“å®¶çš„è´Ÿè½½å‡è¡¡ï¼ŒåŒæ—¶è®¾è®¡äº†åŠ¨æ€è·¯ç”±æœºåˆ¶ï¼Œä»¥ç¡®ä¿åœ¨ä¸åŒè¾“å…¥æ¡ä»¶ä¸‹èƒ½å¤Ÿçµæ´»é€‰æ‹©åˆé€‚çš„ä¸“å®¶è¿›è¡Œæ¿€æ´»ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒLPRå°†ä¸“å®¶è´Ÿè½½çš„åŸºå°¼ç³»æ•°ä»0.70é™ä½è‡³0.035ï¼Œæœ€å°-æœ€å¤§ä¸“å®¶è´Ÿè½½æ¯”ä»1e-6æå‡è‡³0.70ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„è´Ÿè½½å¹³è¡¡æ•ˆæœï¼Œæ¥è¿‘å®Œç¾çš„è´Ÿè½½å‡è¡¡æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨ç¿»è¯‘å’Œå¯¹è¯ç³»ç»Ÿç­‰å¤§å‹è¯­è¨€æ¨¡å‹çš„å¼€å‘ã€‚é€šè¿‡å®ç°æ›´é«˜æ•ˆçš„ä¸“å®¶åˆ©ç”¨ï¼ŒLPRèƒ½å¤Ÿæ˜¾è‘—æå‡æ¨¡å‹çš„æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Mixture-of-Experts (MoE) architectures have emerged as a key strategy for scaling large language models (LLMs) efficiently. However, current MoE systems suffer from severe load imbalance, where only a small subset of experts is consistently activated during training and inference, leading to significant underutilization of model capacity and computational resources. In this work, we revisit expert routing through a clustering perspective and propose Latent Prototype Routing (LPR), a novel routing framework that generalizes existing approaches while promoting balanced expert utilization without compromising downstream performance. Extensive experiments across multiple open-source MoE models -- including DeepSeek-V3, Qwen3-MoE, and Mixtral -- demonstrate that LPR reduces the Gini coefficient of expert load from 0.70 to 0.035 on average, improves the min-max expert load ratio from 1e-6 to 0.70, achieving near-perfect load balancing.

