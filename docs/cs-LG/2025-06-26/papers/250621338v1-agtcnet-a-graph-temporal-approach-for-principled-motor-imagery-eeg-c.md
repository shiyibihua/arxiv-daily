---
layout: default
title: AGTCNet: A Graph-Temporal Approach for Principled Motor Imagery EEG Classification
---

# AGTCNet: A Graph-Temporal Approach for Principled Motor Imagery EEG Classification

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21338" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.21338v1</a>
  <a href="https://arxiv.org/pdf/2506.21338.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21338v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21338v1', 'AGTCNet: A Graph-Temporal Approach for Principled Motor Imagery EEG Classification')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Galvin Brice S. Lim, Brian Godwin S. Lim, Argel A. Bandala, John Anthony C. Jose, Timothy Scott C. Chu, Edwin Sybingco

**ÂàÜÁ±ª**: cs.LG, cs.HC

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-26

**Â§áÊ≥®**: This work has been submitted to the IEEE for possible publication

**ÊúüÂàä**: IEEE Access. 13(2025) 187383-187409

**DOI**: [10.1109/ACCESS.2025.3627419](https://doi.org/10.1109/ACCESS.2025.3627419)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫AGTCNet‰ª•Ëß£ÂÜ≥ËÑëÊú∫Êé•Âè£EEGÂàÜÁ±ª‰∏≠ÁöÑÊó∂Á©∫‰æùËµñÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±ÂÖ´ÔºöÁâ©ÁêÜÂä®Áîª (Physics-based Animation)**

**ÂÖ≥ÈîÆËØç**: `ËÑëÊú∫Êé•Âè£` `EEGÂàÜÁ±ª` `ÂõæÂç∑ÁßØÁΩëÁªú` `Êó∂Á©∫ÁâπÂæÅ` `ËøêÂä®ÊÉ≥Ë±°` `Ê∑±Â∫¶Â≠¶‰π†` `Ê≥®ÊÑèÂäõÊú∫Âà∂`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑËÑëÊú∫Êé•Âè£EEGÂàÜÁ±ªÊñπÊ≥ïÊú™ËÉΩÊúâÊïàÊçïÊçâÂ§öÈÄöÈÅì‰ø°Âè∑‰∏≠ÁöÑÂ§çÊùÇÊó∂Á©∫‰æùËµñÊÄßÔºåÂØºËá¥ÂàÜÁ±ªÊÄßËÉΩ‰∏çË∂≥„ÄÇ
2. AGTCNetÈÄöËøáÂºïÂÖ•ÂõæÂç∑ÁßØÊ≥®ÊÑèÁΩëÁªúÔºåÂà©Áî®EEGÁîµÊûÅÁöÑÊãìÊâëÁªìÊûÑÊù•Â≠¶‰π†Êó∂Á©∫ÁâπÂæÅÔºå‰ªéËÄåÊèêÈ´òÂàÜÁ±ªÂáÜÁ°ÆÊÄß„ÄÇ
3. AGTCNetÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äË°®Áé∞‰ºòÂºÇÔºå‰∏ª‰ΩìÁã¨Á´ãÂàÜÁ±ªÂáÜÁ°ÆÁéáËææ66.82%ÔºåÂπ∂Âú®ÂæÆË∞ÉÂêéÊèêÂçáËá≥82.88%ÔºåÊòæÁ§∫Âá∫ÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËÑëÊú∫Êé•Âè£ÔºàBCIÔºâÊäÄÊúØÂà©Áî®ËÑëÁîµÂõæÔºàEEGÔºâ‰∏∫ËøêÂä®ÈöúÁ¢çËÄÖÊèê‰æõ‰∫Ü‰∏éÁéØÂ¢É‰∫íÂä®ÁöÑÊú∫‰ºö„ÄÇÁÑ∂ËÄåÔºåÁî±‰∫é‰∏™‰ΩìÈó¥ÂíåÊó∂Èó¥‰∏äÁöÑÁ•ûÁªèÊ¥ªÂä®Â§çÊùÇÊÄßÔºåÂºÄÂèëÂÖ∑Êúâ‰∏ª‰Ωì‰∏çÂèòÊÄßÂíå‰ºöËØù‰∏çÂèòÊÄßÁöÑBCIÁ≥ªÁªü‰ªçÁÑ∂Èù¢‰∏¥ÈáçÂ§ßÊåëÊàò„ÄÇÁé∞ÊúâÊñπÊ≥ïÊú™ËÉΩÊúâÊïàÊçïÊçâÂ§öÈÄöÈÅìEEG‰ø°Âè∑‰∏≠ÁöÑÂ§çÊùÇÊó∂Á©∫‰æùËµñÊÄß„ÄÇÊú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂõæ-Êó∂Èó¥Âç∑ÁßØÁΩëÁªúÔºàAGTCNetÔºâÔºåÈÄöËøáÂà©Áî®EEGÁîµÊûÅÁöÑÊãìÊâëÈÖçÁΩÆ‰Ωú‰∏∫ÂΩíÁ∫≥ÂÅèÁΩÆÔºåÂπ∂ÁªìÂêàÂõæÂç∑ÁßØÊ≥®ÊÑèÁΩëÁªúÔºàGCATÔºâÂÖ±ÂêåÂ≠¶‰π†Ë°®ËææÊÄßÊó∂Á©∫EEGË°®Á§∫„ÄÇAGTCNetÂú®BCI Competition IV Dataset 2a‰∏äÂÆûÁé∞‰∫Ü66.82%ÁöÑ‰∏ª‰ΩìÁã¨Á´ãÂàÜÁ±ªÂáÜÁ°ÆÁéáÔºåÂπ∂Âú®ÂæÆË∞ÉÂêéÊèêÂçáËá≥82.88%„ÄÇÂú®EEGËøêÂä®ËøêÂä®/ÊÉ≥Ë±°Êï∞ÊçÆÈõÜ‰∏äÔºåAGTCNetÂàÜÂà´Âú®4Á±ªÂíå2Á±ª‰∏ª‰ΩìÁã¨Á´ãÂàÜÁ±ª‰∏≠ËææÂà∞‰∫Ü64.14%Âíå85.22%ÁöÑÂáÜÁ°ÆÁéáÔºåËøõ‰∏ÄÊ≠•ÊèêÂçáËá≥72.13%Âíå90.54%ËøõË°å‰∏ª‰ΩìÁâπÂÆöÂàÜÁ±ª„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨Á†îÁ©∂Êó®Âú®Ëß£ÂÜ≥ËÑëÊú∫Êé•Âè£EEGÂàÜÁ±ª‰∏≠Â≠òÂú®ÁöÑ‰∏ª‰Ωì‰∏çÂèòÊÄßÂíå‰ºöËØù‰∏çÂèòÊÄßÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÊú™ËÉΩÊúâÊïàÊçïÊçâEEG‰ø°Âè∑‰∏≠ÁöÑÂ§çÊùÇÊó∂Á©∫‰æùËµñÊÄßÔºåÂØºËá¥ÂàÜÁ±ªÊÄßËÉΩ‰∏çË∂≥„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöAGTCNetÈÄöËøáÁªìÂêàÂõæÂç∑ÁßØÊ≥®ÊÑèÁΩëÁªúÔºàGCATÔºâ‰∏éEEGÁîµÊûÅÁöÑÊãìÊâëÁªìÊûÑÔºåÊó®Âú®ÂÖ±ÂêåÂ≠¶‰π†EEG‰ø°Âè∑ÁöÑÊó∂Á©∫ÁâπÂæÅÔºå‰ªéËÄåÊèêÈ´òÂàÜÁ±ªÁöÑÂáÜÁ°ÆÊÄßÂíåÈ≤ÅÊ£íÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöAGTCNetÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨ËæìÂÖ•Â±Ç„ÄÅÂõæÂç∑ÁßØÂ±Ç„ÄÅÊ≥®ÊÑèÂäõÊú∫Âà∂Â±ÇÂíåËæìÂá∫Â±Ç„ÄÇËæìÂÖ•Â±ÇÊé•Êî∂EEG‰ø°Âè∑ÔºåÂõæÂç∑ÁßØÂ±ÇÊèêÂèñÊó∂Á©∫ÁâπÂæÅÔºåÊ≥®ÊÑèÂäõÊú∫Âà∂Â±ÇÂ¢ûÂº∫ÈáçË¶ÅÁâπÂæÅÔºåÊúÄÁªàËæìÂá∫ÂàÜÁ±ªÁªìÊûú„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöAGTCNetÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÂºïÂÖ•‰∫ÜÂõæÂç∑ÁßØÊ≥®ÊÑèÁΩëÁªúÔºåËÉΩÂ§üÊúâÊïàÊçïÊçâEEG‰ø°Âè∑‰∏≠ÁöÑÂ§çÊùÇÊó∂Á©∫‰æùËµñÊÄßÔºå‰∏é‰º†ÁªüÊñπÊ≥ïÁõ∏ÊØîÔºåÊòæËëóÊèêÂçá‰∫ÜÂàÜÁ±ªÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÊ®°ÂûãÂú®ËÆæËÆ°‰∏äÈááÁî®‰∫ÜÁ¥ßÂáëÁöÑÊû∂ÊûÑÔºåÂáèÂ∞ë‰∫Ü49.87%ÁöÑÊ®°ÂûãÂ§ßÂ∞èÔºåÂπ∂‰ºòÂåñ‰∫ÜÊé®ÁêÜÊó∂Èó¥ÔºåÊèêÂçá‰∫Ü64.65%„ÄÇÊçüÂ§±ÂáΩÊï∞ÂíåÁΩëÁªúÁªìÊûÑÁªèËøáÁ≤æÂøÉËÆæËÆ°Ôºå‰ª•Á°Æ‰øùÊ®°ÂûãÁöÑÈ´òÊïàÊÄßÂíåÂáÜÁ°ÆÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

AGTCNetÂú®BCI Competition IV Dataset 2a‰∏äÂÆûÁé∞‰∫Ü66.82%ÁöÑ‰∏ª‰ΩìÁã¨Á´ãÂàÜÁ±ªÂáÜÁ°ÆÁéáÔºåÂπ∂Âú®ÂæÆË∞ÉÂêéÊèêÂçáËá≥82.88%„ÄÇÂú®EEGËøêÂä®ËøêÂä®/ÊÉ≥Ë±°Êï∞ÊçÆÈõÜ‰∏äÔºåAGTCNetÂú®4Á±ªÂíå2Á±ª‰∏ª‰ΩìÁã¨Á´ãÂàÜÁ±ª‰∏≠ÂàÜÂà´ËææÂà∞‰∫Ü64.14%Âíå85.22%ÁöÑÂáÜÁ°ÆÁéáÔºåËøõ‰∏ÄÊ≠•ÊèêÂçáËá≥72.13%Âíå90.54%ËøõË°å‰∏ª‰ΩìÁâπÂÆöÂàÜÁ±ªÔºåÊòæÁ§∫Âá∫ÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊàêÊûúÂú®ËÑëÊú∫Êé•Âè£ÊäÄÊúØ‰∏≠ÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõÔºåËÉΩÂ§üÂ∏ÆÂä©ËøêÂä®ÈöúÁ¢çËÄÖÊõ¥Â•ΩÂú∞‰∏éÁéØÂ¢É‰∫íÂä®„ÄÇAGTCNetÁöÑÈ´òÊïàÊÄßÂíåÂáÜÁ°ÆÊÄß‰ΩøÂÖ∂Âú®ÂåªÁñóÂ∫∑Â§ç„ÄÅÊô∫ËÉΩÂÆ∂Â±ÖÊéßÂà∂Á≠âÈ¢ÜÂüüÂÖ∑ÊúâÂÆûÈôÖ‰ª∑ÂÄºÔºåÊú™Êù•ÂèØËÉΩÊé®Âä®BCIÊäÄÊúØÁöÑÊôÆÂèä‰∏éÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Brain-computer interface (BCI) technology utilizing electroencephalography (EEG) marks a transformative innovation, empowering motor-impaired individuals to engage with their environment on equal footing. Despite its promising potential, developing subject-invariant and session-invariant BCI systems remains a significant challenge due to the inherent complexity and variability of neural activity across individuals and over time, compounded by EEG hardware constraints. While prior studies have sought to develop robust BCI systems, existing approaches remain ineffective in capturing the intricate spatiotemporal dependencies within multichannel EEG signals. This study addresses this gap by introducing the attentive graph-temporal convolutional network (AGTCNet), a novel graph-temporal model for motor imagery EEG (MI-EEG) classification. Specifically, AGTCNet leverages the topographic configuration of EEG electrodes as an inductive bias and integrates graph convolutional attention network (GCAT) to jointly learn expressive spatiotemporal EEG representations. The proposed model significantly outperformed existing MI-EEG classifiers, achieving state-of-the-art performance while utilizing a compact architecture, underscoring its effectiveness and practicality for BCI deployment. With a 49.87% reduction in model size, 64.65% faster inference time, and shorter input EEG signal, AGTCNet achieved a moving average accuracy of 66.82% for subject-independent classification on the BCI Competition IV Dataset 2a, which further improved to 82.88% when fine-tuned for subject-specific classification. On the EEG Motor Movement/Imagery Dataset, AGTCNet achieved moving average accuracies of 64.14% and 85.22% for 4-class and 2-class subject-independent classifications, respectively, with further improvements to 72.13% and 90.54% for subject-specific classifications.

