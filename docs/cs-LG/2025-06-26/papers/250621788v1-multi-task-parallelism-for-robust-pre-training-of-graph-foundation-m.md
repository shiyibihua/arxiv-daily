---
layout: default
title: Multi-task parallelism for robust pre-training of graph foundation models on multi-source, multi-fidelity atomistic modeling data
---

# Multi-task parallelism for robust pre-training of graph foundation models on multi-source, multi-fidelity atomistic modeling data

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21788" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.21788v1</a>
  <a href="https://arxiv.org/pdf/2506.21788.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21788v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21788v1', 'Multi-task parallelism for robust pre-training of graph foundation models on multi-source, multi-fidelity atomistic modeling data')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Massimiliano Lupo Pasini, Jong Youl Choi, Pei Zhang, Kshitij Mehta, Rylie Weaver, Ashwin M. Aji, Karl W. Schulz, Jorda Polo, Prasanna Balaprakash

**åˆ†ç±»**: cs.LG, cond-mat.mtrl-sci, cs.AI, physics.atm-clus

**å‘å¸ƒæ—¥æœŸ**: 2025-06-26

**å¤‡æ³¨**: 15 pages, 4 figures, 2 tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šä»»åŠ¡å¹¶è¡Œæ–¹æ³•ä»¥å¢å¼ºå›¾åŸºç¡€æ¨¡å‹çš„é²æ£’æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å›¾åŸºç¡€æ¨¡å‹` `å¤šä»»åŠ¡å­¦ä¹ ` `å›¾ç¥ç»ç½‘ç»œ` `åŸå­å»ºæ¨¡` `GPUåŠ é€Ÿ` `è¶…çº§è®¡ç®—æœº` `æ•°æ®å¤„ç†` `è¿ç§»å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤šæºã€å¤šä¿çœŸæ•°æ®æ—¶å­˜åœ¨ç¨³å®šæ€§ä¸è¶³å’Œè¿ç§»èƒ½åŠ›æœ‰é™çš„é—®é¢˜ã€‚
2. æå‡ºçš„å¤šä»»åŠ¡å¹¶è¡Œæ–¹æ³•é€šè¿‡GPUåŠ é€Ÿå°†è§£ç å¤´åˆ†å¸ƒåˆ°è®¡ç®—èµ„æºä¸Šï¼Œæé«˜äº†é¢„è®­ç»ƒçš„æ•ˆç‡å’Œç¨³å®šæ€§ã€‚
3. åœ¨è¶…è¿‡2400ä¸‡ä¸ªç»“æ„çš„è®­ç»ƒä¸­ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§è¶…çº§è®¡ç®—æœºä¸Šå±•ç¤ºäº†è‰¯å¥½çš„æ‰©å±•æ€§å’Œæ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å›¾åŸºç¡€æ¨¡å‹åˆ©ç”¨å›¾ç¥ç»ç½‘ç»œåœ¨åŸå­å»ºæ¨¡ä¸­å±•ç°å‡ºå¯æŒç»­å’Œé«˜æ•ˆçš„æ½œåŠ›ã€‚ä¸ºäº†è§£å†³åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­å¤„ç†å¤šæºã€å¤šä¿çœŸæ•°æ®çš„æŒ‘æˆ˜ï¼Œè¿‘æœŸç ”ç©¶é‡‡ç”¨äº†å¤šä»»åŠ¡å­¦ä¹ çš„æ–¹æ³•ï¼Œé€šè¿‡å…±äº«çš„æ¶ˆæ¯ä¼ é€’å±‚å¤„ç†è¾“å…¥çš„åŸå­ç»“æ„ï¼Œç„¶åå°†å…¶è·¯ç”±åˆ°å¤šä¸ªè§£ç å¤´ä»¥é¢„æµ‹ç‰¹å®šæ•°æ®çš„è¾“å‡ºã€‚è¿™ç§æ–¹æ³•ç¨³å®šäº†é¢„è®­ç»ƒè¿‡ç¨‹ï¼Œå¹¶å¢å¼ºäº†æ¨¡å‹åœ¨æœªæ¢ç´¢åŒ–å­¦åŒºåŸŸçš„è¿ç§»èƒ½åŠ›ã€‚å°½ç®¡åœ¨çº¦å››ç™¾ä¸‡ä¸ªç»“æ„ä¸Šçš„åˆæ­¥ç»“æœä»¤äººé¼“èˆï¼Œä½†å¯¹äºæ›´å¤§ã€æ›´å…·å¤šæ ·æ€§æ•°æ®é›†çš„æ³›åŒ–èƒ½åŠ›å’Œåœ¨è¶…çº§è®¡ç®—æœºä¸Šçš„å¯æ‰©å±•æ€§ä»å­˜åœ¨ç–‘é—®ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šä»»åŠ¡å¹¶è¡Œæ–¹æ³•ï¼Œå°†æ¯ä¸ªè§£ç å¤´åˆ†å¸ƒåˆ°è®¡ç®—èµ„æºä¸Šï¼Œå¹¶åˆ©ç”¨GPUåŠ é€Ÿã€‚è¯¥æ–¹æ³•åœ¨å¼€æºçš„HydraGNNæ¶æ„ä¸­å®ç°ï¼Œè®­ç»ƒæ•°æ®è¶…è¿‡2400ä¸‡ä¸ªç»“æ„ï¼Œå¹¶åœ¨Perlmutterã€Auroraå’ŒFrontierè¶…çº§è®¡ç®—æœºä¸Šè¿›è¡Œäº†æµ‹è¯•ï¼Œå±•ç¤ºäº†åœ¨è¿™ä¸‰ç§é«˜åº¦å¼‚æ„çš„è¶…çº§è®¡ç®—æ¶æ„ä¸Šçš„é«˜æ•ˆæ‰©å±•æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨å¤šæºã€å¤šä¿çœŸåŸå­å»ºæ¨¡æ•°æ®çš„é¢„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œç°æœ‰æ–¹æ³•åœ¨ç¨³å®šæ€§å’Œè¿ç§»èƒ½åŠ›æ–¹é¢çš„ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºçš„å¤šä»»åŠ¡å¹¶è¡Œæ–¹æ³•é€šè¿‡å°†æ¯ä¸ªè§£ç å¤´åˆ†å¸ƒåˆ°ä¸åŒçš„è®¡ç®—èµ„æºä¸Šï¼Œåˆ©ç”¨GPUåŠ é€Ÿæ¥æé«˜æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡å’Œç¨³å®šæ€§ã€‚è¿™æ ·çš„è®¾è®¡æ—¨åœ¨åº”å¯¹å¤§è§„æ¨¡æ•°æ®é›†çš„å¤„ç†éœ€æ±‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å…±äº«çš„æ¶ˆæ¯ä¼ é€’å±‚å’Œå¤šä¸ªè§£ç å¤´ï¼Œæ¶ˆæ¯ä¼ é€’å±‚è´Ÿè´£å¤„ç†è¾“å…¥çš„åŸå­ç»“æ„ï¼Œè€Œè§£ç å¤´åˆ™é’ˆå¯¹ç‰¹å®šæ•°æ®è¾“å‡ºè¿›è¡Œé¢„æµ‹ã€‚è¯¥æ–¹æ³•åœ¨å¼€æºçš„HydraGNNæ¶æ„ä¸­å®ç°ï¼Œæ”¯æŒå¤§è§„æ¨¡å¹¶è¡Œè®¡ç®—ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¤šä»»åŠ¡å¹¶è¡Œæ–¹æ³•çš„æå‡ºï¼Œä½¿å¾—æ¯ä¸ªè§£ç å¤´å¯ä»¥ç‹¬ç«‹å¹¶è¡Œå¤„ç†ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†è®­ç»ƒæ•ˆç‡å’Œæ¨¡å‹çš„å¯æ‰©å±•æ€§ã€‚è¿™ä¸ä¼ ç»Ÿçš„é›†ä¸­å¼è®­ç»ƒæ–¹æ³•å½¢æˆäº†é²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†é€‚åº”æ€§æŸå¤±å‡½æ•°å’Œä¼˜åŒ–çš„ç½‘ç»œç»“æ„ï¼Œä»¥ç¡®ä¿åœ¨å¤„ç†å¤§è§„æ¨¡æ•°æ®æ—¶çš„ç¨³å®šæ€§å’Œé«˜æ•ˆæ€§ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨å®éªŒéƒ¨åˆ†è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæå‡ºçš„æ–¹æ³•åœ¨2400ä¸‡ä¸ªç»“æ„çš„è®­ç»ƒä¸­ï¼ŒæˆåŠŸåœ¨Perlmutterã€Auroraå’ŒFrontierè¶…çº§è®¡ç®—æœºä¸Šå®ç°äº†é«˜æ•ˆçš„æ‰©å±•æ€§ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„è®­ç»ƒé€Ÿåº¦å’Œç¨³å®šæ€§ï¼Œå…·ä½“æ€§èƒ½æ•°æ®å°šæœªå…¬å¼€ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒ–å­¦åˆ†å­å»ºæ¨¡ã€ææ–™ç§‘å­¦å’Œè¯ç‰©å‘ç°ç­‰ã€‚é€šè¿‡æé«˜å›¾åŸºç¡€æ¨¡å‹åœ¨å¤šæºæ•°æ®ä¸Šçš„é²æ£’æ€§å’Œè¿ç§»èƒ½åŠ›ï¼Œèƒ½å¤ŸåŠ é€Ÿæ–°ææ–™çš„å‘ç°å’Œä¼˜åŒ–è¿‡ç¨‹ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œé•¿è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Graph foundation models using graph neural networks promise sustainable, efficient atomistic modeling. To tackle challenges of processing multi-source, multi-fidelity data during pre-training, recent studies employ multi-task learning, in which shared message passing layers initially process input atomistic structures regardless of source, then route them to multiple decoding heads that predict data-specific outputs. This approach stabilizes pre-training and enhances a model's transferability to unexplored chemical regions. Preliminary results on approximately four million structures are encouraging, yet questions remain about generalizability to larger, more diverse datasets and scalability on supercomputers. We propose a multi-task parallelism method that distributes each head across computing resources with GPU acceleration. Implemented in the open-source HydraGNN architecture, our method was trained on over 24 million structures from five datasets and tested on the Perlmutter, Aurora, and Frontier supercomputers, demonstrating efficient scaling on all three highly heterogeneous super-computing architectures.

