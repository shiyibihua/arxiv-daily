---
layout: default
title: Scalable Bayesian Low-Rank Adaptation of Large Language Models via Stochastic Variational Subspace Inference
---

# Scalable Bayesian Low-Rank Adaptation of Large Language Models via Stochastic Variational Subspace Inference

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21408" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.21408v1</a>
  <a href="https://arxiv.org/pdf/2506.21408.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21408v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21408v1', 'Scalable Bayesian Low-Rank Adaptation of Large Language Models via Stochastic Variational Subspace Inference')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Colin Samplawski, Adam D. Cobb, Manoj Acharya, Ramneet Kaur, Susmit Jha

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-26

**å¤‡æ³¨**: Accepted at UAI 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºScalaBLä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹çš„ä¸ç¡®å®šæ€§é‡åŒ–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è´å¶æ–¯æ·±åº¦å­¦ä¹ ` `ä½ç§©é€‚åº”` `è¯­è¨€æ¨¡å‹` `ä¸ç¡®å®šæ€§é‡åŒ–` `é«˜é£é™©é¢†åŸŸ` `å‚æ•°æ•ˆç‡` `æ¨¡å‹æ‰©å±•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è´å¶æ–¯æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨å¤„ç†å¤§å‹è¯­è¨€æ¨¡å‹æ—¶é¢ä¸´å‚æ•°æ‰©å±•çš„æŒ‘æˆ˜ï¼Œå¯¼è‡´æ€§èƒ½å—é™ã€‚
2. æœ¬ç ”ç©¶æå‡ºScalaBLï¼Œé€šè¿‡åœ¨ä½ç§©é€‚åº”çš„å­ç©ºé—´ä¸­è¿›è¡Œè´å¶æ–¯æ¨æ–­ï¼Œæ˜¾è‘—å‡å°‘äº†æ‰€éœ€å‚æ•°æ•°é‡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒScalaBLåœ¨æ€§èƒ½ä¸Šä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸å½“ï¼ŒåŒæ—¶èƒ½å¤Ÿå¤„ç†æ›´å¤§è§„æ¨¡çš„æ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¢«å¹¿æ³›ä½¿ç”¨ï¼Œä½†å®ƒä»¬å¸¸å¸¸ä¼šäº§ç”Ÿé”™è¯¯ä¿¡æ¯å¹¶ä¸”æ ¡å‡†æ•ˆæœä¸ä½³ã€‚å› æ­¤ï¼Œå¯¹è¿™äº›æ¨¡å‹çš„ä¸ç¡®å®šæ€§é‡åŒ–è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨è‡ªä¸»ç³»ç»Ÿå’ŒåŒ»ç–—ç­‰é«˜é£é™©é¢†åŸŸã€‚ä»¥å¾€çš„ç ”ç©¶é€šè¿‡å¯¹å¾®è°ƒæ¨¡å‹çš„ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰å‚æ•°è¿›è¡Œæ¨æ–­ï¼Œä½¿å¾—åŸºäºè´å¶æ–¯æ·±åº¦å­¦ä¹ çš„æ–¹æ³•æ›´ä¸ºå¯è¡Œã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨æ‰©å±•åˆ°æ›´å¤§è§„æ¨¡çš„LLMsæ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒä»¬éœ€è¦æ¯”LoRAæ›´å¤šçš„é¢å¤–å‚æ•°ã€‚æœ¬ç ”ç©¶æå‡ºäº†å¯æ‰©å±•çš„è´å¶æ–¯ä½ç§©é€‚åº”æ–¹æ³•ï¼ˆScalaBLï¼‰ï¼Œé€šè¿‡åœ¨rç»´å­ç©ºé—´ä¸­è¿›è¡Œè´å¶æ–¯æ¨æ–­ï¼Œåˆ©ç”¨LoRAå‚æ•°ä½œä¸ºæŠ•å½±çŸ©é˜µï¼Œå°†æ ·æœ¬æ˜ å°„åˆ°LLMçš„å…¨æƒé‡ç©ºé—´ã€‚å°½ç®¡å­ç©ºé—´ç»´åº¦è¾ƒä½ï¼Œæˆ‘ä»¬ä»èƒ½åœ¨ä»…éœ€çº¦1000ä¸ªé¢å¤–å‚æ•°çš„æƒ…å†µä¸‹ï¼Œå–å¾—ä¸æœ€å…ˆè¿›æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ï¼Œå¹¶èƒ½å¤Ÿæ‰©å±•åˆ°è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„è´å¶æ–¯LLMã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸ç¡®å®šæ€§é‡åŒ–æ–¹é¢çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨é«˜é£é™©é¢†åŸŸçš„åº”ç”¨ä¸­ï¼Œç°æœ‰æ–¹æ³•åœ¨æ‰©å±•æ€§å’Œå‚æ•°æ•ˆç‡ä¸Šå­˜åœ¨ç—›ç‚¹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„ScalaBLæ–¹æ³•é€šè¿‡åœ¨ä½ç§©é€‚åº”çš„å­ç©ºé—´ä¸­è¿›è¡Œè´å¶æ–¯æ¨æ–­ï¼Œåˆ©ç”¨LoRAå‚æ•°ä½œä¸ºæŠ•å½±çŸ©é˜µï¼Œä»è€Œæœ‰æ•ˆåœ°å°†æ ·æœ¬æ˜ å°„åˆ°LLMçš„å…¨æƒé‡ç©ºé—´ï¼Œå‡å°‘äº†é¢å¤–å‚æ•°çš„éœ€æ±‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šScalaBLçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä½ç§©é€‚åº”çš„å­ç©ºé—´æ¨æ–­æ¨¡å—å’Œå…¨æƒé‡ç©ºé—´æ˜ å°„æ¨¡å—ã€‚é¦–å…ˆï¼Œåœ¨rç»´å­ç©ºé—´ä¸­è¿›è¡Œè´å¶æ–¯æ¨æ–­ï¼Œç„¶åé€šè¿‡æŠ•å½±çŸ©é˜µå°†ç»“æœæ˜ å°„åˆ°å®Œæ•´çš„æ¨¡å‹æƒé‡ä¸­ã€‚

**å…³é”®åˆ›æ–°**ï¼šScalaBLçš„ä¸»è¦åˆ›æ–°åœ¨äºé€šè¿‡ä½ç»´å­ç©ºé—´çš„è´å¶æ–¯æ¨æ–­å®ç°äº†å¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„æœ‰æ•ˆé€‚åº”ï¼Œæ˜¾è‘—é™ä½äº†æ‰€éœ€çš„é¢å¤–å‚æ•°æ•°é‡ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”å…·æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼ŒScalaBLä»…éœ€çº¦1000ä¸ªé¢å¤–å‚æ•°ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šé‡‡ç”¨äº†é€‚åº”æ€§è´å¶æ–¯æ¨æ–­ç­–ç•¥ï¼Œç¡®ä¿äº†æ¨¡å‹çš„æœ‰æ•ˆæ€§å’Œç¨³å®šæ€§ã€‚æ•´ä½“ç½‘ç»œç»“æ„ä¿æŒäº†é«˜æ•ˆæ€§ï¼Œé€‚åº”äº†å¤§è§„æ¨¡æ¨¡å‹çš„éœ€æ±‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒScalaBLåœ¨ä»…éœ€çº¦1000ä¸ªé¢å¤–å‚æ•°çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿä¸å½“å‰æœ€å…ˆè¿›çš„è´å¶æ–¯æ–¹æ³•ç›¸åª²ç¾ï¼Œä¸”æˆåŠŸæ‰©å±•åˆ°æ‹¥æœ‰å››å€äºä»¥å¾€åŸºçº¿å‚æ•°çš„æœ€å¤§è´å¶æ–¯LLMï¼Œå±•ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½å’Œæ‰©å±•èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªä¸»ç³»ç»Ÿã€åŒ»ç–—å¥åº·ã€é‡‘èå†³ç­–ç­‰é«˜é£é™©åœºæ™¯ï¼Œèƒ½å¤Ÿä¸ºè¿™äº›é¢†åŸŸæä¾›æ›´ä¸ºå¯é çš„æ¨¡å‹ä¸ç¡®å®šæ€§é‡åŒ–ï¼Œæå‡å†³ç­–çš„å®‰å…¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚æœªæ¥ï¼ŒScalaBLæœ‰æœ›æ¨åŠ¨å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ›´å¤šå®é™…åº”ç”¨ä¸­çš„è½åœ°ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é«˜å¯é æ€§çš„ä»»åŠ¡ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Despite their widespread use, large language models (LLMs) are known to hallucinate incorrect information and be poorly calibrated. This makes the uncertainty quantification of these models of critical importance, especially in high-stakes domains, such as autonomy and healthcare. Prior work has made Bayesian deep learning-based approaches to this problem more tractable by performing inference over the low-rank adaptation (LoRA) parameters of a fine-tuned model. While effective, these approaches struggle to scale to larger LLMs due to requiring further additional parameters compared to LoRA. In this work we present $\textbf{Scala}$ble $\textbf{B}$ayesian $\textbf{L}$ow-Rank Adaptation via Stochastic Variational Subspace Inference (ScalaBL). We perform Bayesian inference in an $r$-dimensional subspace, for LoRA rank $r$. By repurposing the LoRA parameters as projection matrices, we are able to map samples from this subspace into the full weight space of the LLM. This allows us to learn all the parameters of our approach using stochastic variational inference. Despite the low dimensionality of our subspace, we are able to achieve competitive performance with state-of-the-art approaches while only requiring ${\sim}1000$ additional parameters. Furthermore, it allows us to scale up to the largest Bayesian LLM to date, with four times as a many base parameters as prior work.

