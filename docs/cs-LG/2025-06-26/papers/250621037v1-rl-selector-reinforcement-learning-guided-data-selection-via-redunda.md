---
layout: default
title: RL-Selector: Reinforcement Learning-Guided Data Selection via Redundancy Assessment
---

# RL-Selector: Reinforcement Learning-Guided Data Selection via Redundancy Assessment

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21037" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.21037v1</a>
  <a href="https://arxiv.org/pdf/2506.21037.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21037v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21037v1', 'RL-Selector: Reinforcement Learning-Guided Data Selection via Redundancy Assessment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Suorong Yang, Peijia Li, Furao Shen, Jian Zhao

**åˆ†ç±»**: cs.LG, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-26

**å¤‡æ³¨**: ICCV 2025

**æœŸåˆŠ**: ICCV 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRL-Selectorä»¥è§£å†³æ•°æ®å†—ä½™å¯¼è‡´çš„è®­ç»ƒæ•ˆç‡ä½ä¸‹é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ•°æ®é€‰æ‹©` `å¼ºåŒ–å­¦ä¹ ` `æ ·æœ¬å†—ä½™` `æ·±åº¦å­¦ä¹ ` `è®­ç»ƒæ•ˆç‡` `æ¨¡å‹æ³›åŒ–` `epsilonæ ·æœ¬è¦†ç›–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ•°æ®é€‰æ‹©æ–¹æ³•å¤šä¾èµ–é™æ€è¯„åˆ†ï¼Œæœªèƒ½å……åˆ†è€ƒè™‘æ ·æœ¬é—´çš„åŠ¨æ€å…³ç³»ï¼Œå¯¼è‡´å†—ä½™æœªèƒ½æœ‰æ•ˆå»é™¤ã€‚
2. æœ¬æ–‡æå‡ºRL-Selectorï¼Œé€šè¿‡å¼•å…¥epsilonæ ·æœ¬è¦†ç›–çš„æ¦‚å¿µï¼Œå°†æ•°æ®é€‰æ‹©é—®é¢˜è½¬åŒ–ä¸ºå¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ï¼Œä¼˜åŒ–æ ·æœ¬é€‰æ‹©ç­–ç•¥ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRL-Selectoråœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºç°æœ‰æ–¹æ³•æ˜¾è‘—æå‡äº†è®­ç»ƒæ•ˆç‡å’Œæ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°ä»£æ·±åº¦å­¦ä¹ æ¶æ„é€šå¸¸ä¾èµ–äºå¤§è§„æ¨¡æ•°æ®é›†ï¼Œä½†è®­ç»ƒè¿™äº›æ•°æ®é›†ä¼šäº§ç”Ÿé«˜æ˜‚çš„è®¡ç®—å’Œå­˜å‚¨å¼€é”€ã€‚ç°å®ä¸–ç•Œçš„æ•°æ®é›†å¾€å¾€åŒ…å«å¤§é‡å†—ä½™ï¼Œå› æ­¤éœ€è¦æ›´é«˜æ•ˆçš„æ•°æ®è®­ç»ƒèŒƒå¼ã€‚æ•°æ®é€‰æ‹©é€šè¿‡è¯†åˆ«æœ€å…·ä»£è¡¨æ€§çš„æ ·æœ¬æ¥å‡è½»å†—ä½™ï¼Œä»è€Œé™ä½è®­ç»ƒæˆæœ¬è€Œä¸å½±å“æ€§èƒ½ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–é™æ€è¯„åˆ†æŒ‡æ ‡æˆ–é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¿½è§†äº†æ‰€é€‰æ ·æœ¬åŠå…¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ¨æ€å˜åŒ–çš„ç»¼åˆå½±å“ã€‚æœ¬æ–‡å¼•å…¥äº†epsilonæ ·æœ¬è¦†ç›–çš„æ¦‚å¿µï¼Œé€šè¿‡æ ·æœ¬é—´å…³ç³»é‡åŒ–æ ·æœ¬å†—ä½™ï¼Œæ•æ‰æ•°æ®é›†çš„å†…åœ¨ç»“æ„ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬å°†æ•°æ®é€‰æ‹©é‡æ–°æ„å»ºä¸ºå¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ï¼Œæå‡ºRL-Selectorï¼Œåˆ©ç”¨ä»ä¸æ–­å˜åŒ–çš„æ•°æ®é›†åˆ†å¸ƒä¸­æ´¾ç”Ÿçš„epsilonæ ·æœ¬è¦†ç›–ä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œä¼˜åŒ–é€‰æ‹©ç­–ç•¥ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åŸºå‡†æ•°æ®é›†å’Œå¤šç§æ¶æ„ä¸Šå‡ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›åŸºçº¿ï¼Œæ‰€é€‰æ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹åœ¨è®­ç»ƒæ•ˆç‡å’Œæ³›åŒ–æ€§èƒ½ä¸Šå‡æœ‰æ‰€æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°ä»£æ·±åº¦å­¦ä¹ è®­ç»ƒä¸­å› æ•°æ®å†—ä½™å¯¼è‡´çš„è®¡ç®—å’Œå­˜å‚¨å¼€é”€è¿‡é«˜çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–é™æ€è¯„åˆ†ï¼Œæœªèƒ½æœ‰æ•ˆæ•æ‰æ ·æœ¬é—´çš„åŠ¨æ€å…³ç³»ï¼Œå¯¼è‡´å†—ä½™æ ·æœ¬æœªèƒ½è¢«æœ‰æ•ˆå»é™¤ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºäº†epsilonæ ·æœ¬è¦†ç›–çš„æ¦‚å¿µï¼Œé€šè¿‡é‡åŒ–æ ·æœ¬é—´çš„å†—ä½™å…³ç³»ï¼Œæ•æ‰æ•°æ®é›†çš„å†…åœ¨ç»“æ„ã€‚å°†æ•°æ®é€‰æ‹©è§†ä¸ºå¼ºåŒ–å­¦ä¹ é—®é¢˜ï¼Œä½¿å¾—é€‰æ‹©ç­–ç•¥èƒ½å¤ŸåŠ¨æ€ä¼˜åŒ–ï¼Œé€‚åº”ä¸æ–­å˜åŒ–çš„æ•°æ®åˆ†å¸ƒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRL-Selectorçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ ·æœ¬å†—ä½™è¯„ä¼°ã€å¼ºåŒ–å­¦ä¹ ä»£ç†å’Œé€‰æ‹©ç­–ç•¥ä¼˜åŒ–å››ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡è¯„ä¼°æ ·æœ¬é—´çš„å…³ç³»æ„å»ºepsilonæ ·æœ¬è¦†ç›–ï¼Œç„¶ååˆ©ç”¨å¼ºåŒ–å­¦ä¹ ä»£ç†æ ¹æ®å½“å‰æ•°æ®åˆ†å¸ƒä¼˜åŒ–é€‰æ‹©ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„ä¸»è¦åˆ›æ–°åœ¨äºå°†æ•°æ®é€‰æ‹©é—®é¢˜è½¬åŒ–ä¸ºå¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ï¼Œå¹¶å¼•å…¥epsilonæ ·æœ¬è¦†ç›–ä½œä¸ºå¥–åŠ±ä¿¡å·ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿé™æ€è¯„åˆ†æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºå…¶åŠ¨æ€é€‚åº”æ€§å’Œå¯¹æ ·æœ¬é—´å…³ç³»çš„æ·±åº¦æŒ–æ˜ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œæˆ‘ä»¬è®¾ç½®äº†è½»é‡çº§çš„å¼ºåŒ–å­¦ä¹ ä»£ç†ï¼Œé‡‡ç”¨äº†é€‚åº”æ€§å¥–åŠ±æœºåˆ¶ï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸æ–­æ›´æ–°æ ·æœ¬è¦†ç›–çš„è¯„ä¼°ï¼Œä»¥ç¡®ä¿é€‰æ‹©ç­–ç•¥çš„æœ‰æ•ˆæ€§å’Œå®æ—¶æ€§ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨å®éªŒéƒ¨åˆ†è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šï¼ŒRL-Selectorçš„å®éªŒç»“æœæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç›¸è¾ƒäºç°æœ‰æœ€å…ˆè¿›çš„åŸºçº¿ï¼Œè®­ç»ƒæ•ˆç‡æé«˜äº†çº¦30%ï¼Œæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ä¹Ÿå¾—åˆ°äº†å¢å¼ºï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

RL-Selectorçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¤„ç†å¤§è§„æ¨¡æ•°æ®é›†çš„æ·±åº¦å­¦ä¹ ä»»åŠ¡ä¸­ï¼Œå¦‚å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œæ¨èç³»ç»Ÿç­‰ã€‚é€šè¿‡æœ‰æ•ˆçš„æ•°æ®é€‰æ‹©ï¼Œèƒ½å¤Ÿæ˜¾è‘—é™ä½è®­ç»ƒæˆæœ¬ï¼Œæé«˜æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„å®é™…åº”ç”¨å’Œå‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Modern deep architectures often rely on large-scale datasets, but training on these datasets incurs high computational and storage overhead. Real-world datasets often contain substantial redundancies, prompting the need for more data-efficient training paradigms. Data selection has shown promise to mitigate redundancy by identifying the most representative samples, thereby reducing training costs without compromising performance. Existing methods typically rely on static scoring metrics or pretrained models, overlooking the combined effect of selected samples and their evolving dynamics during training. We introduce the concept of epsilon-sample cover, which quantifies sample redundancy based on inter-sample relationships, capturing the intrinsic structure of the dataset. Based on this, we reformulate data selection as a reinforcement learning (RL) process and propose RL-Selector, where a lightweight RL agent optimizes the selection policy by leveraging epsilon-sample cover derived from evolving dataset distribution as a reward signal. Extensive experiments across benchmark datasets and diverse architectures demonstrate that our method consistently outperforms existing state-of-the-art baselines. Models trained with our selected datasets show enhanced generalization performance with improved training efficiency.

