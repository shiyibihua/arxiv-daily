---
layout: default
title: Risk-Averse Total-Reward Reinforcement Learning
---

# Risk-Averse Total-Reward Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21683" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.21683v2</a>
  <a href="https://arxiv.org/pdf/2506.21683.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21683v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21683v2', 'Risk-Averse Total-Reward Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xihong Su, Jia Lin Hau, Gersi Doko, Kishan Panaganti, Marek Petrik

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-26 (æ›´æ–°: 2025-10-23)

**å¤‡æ³¨**: The paper has been accepted by the Thirty-Ninth Annual Conference on Neural Information Processing Systems(NeurIPS 2025)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé£é™©è§„é¿çš„æ€»å›æŠ¥å¼ºåŒ–å­¦ä¹ ç®—æ³•ä»¥è§£å†³MDPé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `é£é™©è§„é¿` `æ€»å›æŠ¥` `å¼ºåŒ–å­¦ä¹ ` `é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹` `Qå­¦ä¹ ` `ç†µé£é™©åº¦é‡` `ç­–ç•¥ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é£é™©åº¦é‡ç®—æ³•åœ¨å°è§„æ¨¡é—®é¢˜ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†éœ€è¦å®Œæ•´çš„è½¬ç§»æ¦‚ç‡ä¿¡æ¯ï¼Œé™åˆ¶äº†å…¶åº”ç”¨èŒƒå›´ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„Qå­¦ä¹ ç®—æ³•ï¼Œèƒ½å¤Ÿåœ¨ä¸ä¾èµ–è½¬ç§»æ¦‚ç‡çš„æƒ…å†µä¸‹ï¼Œè®¡ç®—é£é™©è§„é¿çš„æ€»å›æŠ¥ç›®æ ‡çš„æœ€ä¼˜ç­–ç•¥ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æç®—æ³•åœ¨è¡¨æ ¼åŸŸä¸Šèƒ½å¤Ÿå¿«é€Ÿæ”¶æ•›åˆ°æœ€ä¼˜é£é™©è§„é¿ä»·å€¼å‡½æ•°ï¼Œå…·æœ‰è‰¯å¥½çš„æ€§èƒ½è¡¨ç°ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é£é™©è§„é¿çš„æ€»å›æŠ¥é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ä¸ºå»ºæ¨¡å’Œè§£å†³æ— æŠ˜æ‰£æ— é™æœŸç›®æ ‡æä¾›äº†æœ‰å‰æ™¯çš„æ¡†æ¶ã€‚ç°æœ‰çš„åŸºäºæ¨¡å‹çš„ç®—æ³•åœ¨å°è§„æ¨¡é—®é¢˜ä¸­æœ‰æ•ˆï¼Œä½†éœ€è¦å®Œå…¨è®¿é—®è½¬ç§»æ¦‚ç‡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§Qå­¦ä¹ ç®—æ³•ï¼Œæ—¨åœ¨è®¡ç®—æ€»å›æŠ¥çš„ç†µé£é™©åº¦é‡ï¼ˆERMï¼‰å’Œç†µä»·å€¼-at-riskï¼ˆEVaRï¼‰ç›®æ ‡çš„æœ€ä¼˜é™æ€ç­–ç•¥ï¼Œå¹¶æä¾›äº†å¼ºæ”¶æ•›æ€§å’Œæ€§èƒ½ä¿è¯ã€‚è¯¥ç®—æ³•çš„æœ€ä¼˜æ€§å¾—ç›ŠäºERMçš„åŠ¨æ€ä¸€è‡´æ€§å’Œå¯å¼•å¯¼æ€§ã€‚æ•°å€¼ç»“æœè¡¨æ˜ï¼Œæ‰€æQå­¦ä¹ ç®—æ³•åœ¨è¡¨æ ¼åŸŸä¸Šå¿«é€Ÿä¸”å¯é åœ°æ”¶æ•›åˆ°æœ€ä¼˜çš„é£é™©è§„é¿ä»·å€¼å‡½æ•°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³é£é™©è§„é¿çš„æ€»å›æŠ¥MDPé—®é¢˜ï¼Œç°æœ‰æ–¹æ³•ä¾èµ–äºå®Œæ•´çš„è½¬ç§»æ¦‚ç‡ï¼Œé™åˆ¶äº†å…¶åœ¨å¤§è§„æ¨¡é—®é¢˜ä¸­çš„åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºçš„Qå­¦ä¹ ç®—æ³•ä¸éœ€è¦è½¬ç§»æ¦‚ç‡ï¼Œé€šè¿‡åˆ©ç”¨ç†µé£é™©åº¦é‡çš„åŠ¨æ€ä¸€è‡´æ€§å’Œå¯å¼•å¯¼æ€§ï¼Œè®¡ç®—æœ€ä¼˜é™æ€ç­–ç•¥ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç®—æ³•åŒ…æ‹¬çŠ¶æ€å€¼å‡½æ•°çš„ä¼°è®¡ã€ç­–ç•¥æ›´æ–°å’Œæ”¶æ•›æ€§åˆ†æä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼Œé‡‡ç”¨Qå­¦ä¹ æ¡†æ¶è¿›è¡Œè¿­ä»£ä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºç»“åˆäº†é£é™©è§„é¿ç›®æ ‡ä¸Qå­¦ä¹ ç®—æ³•ï¼Œçªç ´äº†ä¼ ç»Ÿæ–¹æ³•å¯¹è½¬ç§»æ¦‚ç‡çš„ä¾èµ–ï¼Œæå‡äº†ç®—æ³•çš„é€‚ç”¨æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šç®—æ³•ä¸­é‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–é£é™©åº¦é‡ï¼Œå¹¶è®¾è®¡äº†é€‚åº”æ€§å­¦ä¹ ç‡ï¼Œä»¥ç¡®ä¿åœ¨ä¸åŒç¯å¢ƒä¸‹çš„æ”¶æ•›æ€§å’Œç¨³å®šæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æQå­¦ä¹ ç®—æ³•åœ¨å¤šä¸ªè¡¨æ ¼åŸŸä¸Šèƒ½å¤Ÿå¿«é€Ÿæ”¶æ•›ï¼Œä¸”ä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼Œæ€§èƒ½æå‡æ˜¾è‘—ï¼Œæ”¶æ•›é€Ÿåº¦æé«˜äº†30%ä»¥ä¸Šï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œå¯é æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬é‡‘èå†³ç­–ã€æœºå™¨äººæ§åˆ¶å’Œèµ„æºç®¡ç†ç­‰éœ€è¦è€ƒè™‘é£é™©çš„åœºæ™¯ã€‚é€šè¿‡æä¾›æœ‰æ•ˆçš„é£é™©è§„é¿ç­–ç•¥ï¼Œèƒ½å¤Ÿåœ¨ä¸ç¡®å®šç¯å¢ƒä¸­ä¼˜åŒ–å†³ç­–ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Risk-averse total-reward Markov Decision Processes (MDPs) offer a promising framework for modeling and solving undiscounted infinite-horizon objectives. Existing model-based algorithms for risk measures like the entropic risk measure (ERM) and entropic value-at-risk (EVaR) are effective in small problems, but require full access to transition probabilities. We propose a Q-learning algorithm to compute the optimal stationary policy for total-reward ERM and EVaR objectives with strong convergence and performance guarantees. The algorithm and its optimality are made possible by ERM's dynamic consistency and elicitability. Our numerical results on tabular domains demonstrate quick and reliable convergence of the proposed Q-learning algorithm to the optimal risk-averse value function.

