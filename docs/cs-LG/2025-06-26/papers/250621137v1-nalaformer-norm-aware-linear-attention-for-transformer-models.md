---
layout: default
title: NaLaFormer: Norm-Aware Linear Attention for Transformer Models
---

# NaLaFormer: Norm-Aware Linear Attention for Transformer Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21137" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.21137v1</a>
  <a href="https://arxiv.org/pdf/2506.21137.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21137v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21137v1', 'NaLaFormer: Norm-Aware Linear Attention for Transformer Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Weikang Meng, Yadan Luo, Liangyu Huo, Yaowei Wang, Xin Li, Zheng Zhang

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºNorm-Aware Linear Attentionä»¥è§£å†³çº¿æ€§æ³¨æ„åŠ›çš„ç†µç¼ºå¤±é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `çº¿æ€§æ³¨æ„åŠ›` `èŒƒæ•°æ„ŸçŸ¥` `ç†µæ§åˆ¶` `å¤šæ¨¡æ€ä»»åŠ¡` `è®¡ç®—æœºè§†è§‰` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çº¿æ€§æ³¨æ„åŠ›æ–¹æ³•å¿½è§†äº†æŸ¥è¯¢èŒƒæ•°ï¼Œå¯¼è‡´ç†µç¼ºå¤±å’Œä¿¡æ¯äº¤äº’ä¸è¶³ã€‚
2. æå‡ºçš„Norm-Aware Linear Attentionæœºåˆ¶é€šè¿‡è§£è€¦æŸ¥è¯¢å’Œé”®çš„èŒƒæ•°ä¸æ–¹å‘ï¼Œæ¢å¤äº†åŠ¨æ€å°–é”æ€§å’ŒèŒƒæ•°åˆ†å¸ƒã€‚
3. å®éªŒè¡¨æ˜ï¼ŒNaLaFormeråœ¨å¤šä¸ªè§†è§‰å’Œè¯­è¨€ä»»åŠ¡ä¸Šæ€§èƒ½æå‡æ˜¾è‘—ï¼Œæœ€é«˜å¯è¾¾4.2%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

çº¿æ€§æ³¨æ„åŠ›ä½œä¸ºè½¯æœ€å¤§æ³¨æ„åŠ›çš„æ›¿ä»£æ–¹æ¡ˆï¼Œé€šè¿‡å°†åºåˆ—é•¿åº¦çš„å¤æ‚åº¦ä»å¹³æ–¹é™ä½åˆ°çº¿æ€§ï¼Œå¾—åˆ°äº†å¹¿æ³›å…³æ³¨ã€‚ç°æœ‰æ–¹æ³•åœ¨ä½¿ç”¨çº¿æ€§å¯åˆ†æ ¸å‡½æ•°æ—¶å¿½è§†äº†æŸ¥è¯¢å‘é‡çš„èŒƒæ•°ï¼Œå¯¼è‡´ç†µçš„ç¼ºå¤±ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„Norm-Aware Linear Attentionæœºåˆ¶ï¼Œé€šè¿‡è§£è€¦æŸ¥è¯¢å’Œé”®çŸ©é˜µçš„èŒƒæ•°ä¸æ–¹å‘ï¼Œå®ç°äº†èŒƒæ•°å¼•å¯¼çš„åŠ¨æ€å°–é”æ€§æ§åˆ¶å’ŒèŒƒæ•°ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒNaLaFormeråœ¨è§†è§‰å’Œè¯­è¨€ä»»åŠ¡ä¸Šæ€§èƒ½æå‡é«˜è¾¾4.2%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰çº¿æ€§æ³¨æ„åŠ›æ–¹æ³•ä¸­æŸ¥è¯¢èŒƒæ•°è¢«å¿½è§†å¯¼è‡´çš„ç†µç¼ºå¤±é—®é¢˜ï¼Œå½±å“äº†ä¿¡æ¯äº¤äº’çš„æœ‰æ•ˆæ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥Norm-Awareæœºåˆ¶ï¼Œè§£è€¦æŸ¥è¯¢å’Œé”®çš„èŒƒæ•°ä¸æ–¹å‘ï¼ŒåŠ¨æ€æ§åˆ¶ç†µçš„å‡å°‘ï¼Œä»è€Œæ¢å¤ä¿¡æ¯çš„è¡¨è¾¾èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æŸ¥è¯¢å’Œé”®çš„è§£è€¦ã€èŒƒæ•°ä¸€è‡´æ€§ç»´æŠ¤ä»¥åŠéè´Ÿçº¦æŸæ˜ å°„ï¼Œç¡®ä¿æ¨¡å‹åœ¨å¤„ç†ä¿¡æ¯æ—¶çš„æœ‰æ•ˆæ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæå‡ºçš„èŒƒæ•°æ„ŸçŸ¥æ ¸å‡½æ•°èƒ½å¤Ÿæ ¹æ®æŸ¥è¯¢èŒƒæ•°åŠ¨æ€è°ƒæ•´ç†µçš„å‡å°‘ç¨‹åº¦ï¼Œæ˜¾è‘—æ”¹å–„äº†çº¿æ€§æ³¨æ„åŠ›çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šé‡‡ç”¨èŒƒæ•°ä¿æŒæ˜ å°„å°†è§’åº¦çŸ©é˜µçš„æ‰€æœ‰å…ƒç´ æŠ•å½±åˆ°æ­£å€¼ï¼Œåˆ©ç”¨ä½™å¼¦ç›¸ä¼¼åº¦æŠ‘åˆ¶æ–¹å‘ç›¸åçš„ç»´åº¦ï¼Œç¡®ä¿æ¨¡å‹çš„éè´Ÿæ€§å’Œä¸€è‡´æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒNaLaFormeråœ¨è§†è§‰å’Œè¯­è¨€ä»»åŠ¡ä¸Šç›¸è¾ƒäºåŸºçº¿æ¨¡å‹æå‡äº†æœ€é«˜4.2%çš„æ€§èƒ½ï¼ŒéªŒè¯äº†å…¶åœ¨æé«˜è¡¨è¾¾æ€§å’Œæ•ˆç‡æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰å¤šæ¨¡æ€ä»»åŠ¡ï¼Œèƒ½å¤Ÿæå‡æ¨¡å‹åœ¨å¤„ç†å¤æ‚æ•°æ®æ—¶çš„æ•ˆç‡å’Œè¡¨è¾¾èƒ½åŠ›ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Linear attention has emerged as a viable alternative to softmax attention by reducing complexity from quadratic to linear in sequence length. To preserve two fundamental properties of softmax, non-negativity and entropy reduction, current works employ various linearly separatable kernel functions with $L1$ normalization instead of softmax operator. However, query norms are neglected by the normalization operation in linear attention, such degradation heavily leads to an entropy gap. Meanwhile, existing works inhibit negative values of query and key vectors resulting in a missing inner-product interactions after being mapped. To address these dual challenges, we propose a novel Norm-Aware Linear Attention mechanism serving to restore norm-guided dynamic spikiness and recover kernel-perturbed norm distributions. Specifically, we first decouple query and key matrices into two components: norm and direction, to achieve norm-aware spikiness control and norm consistency, respectively. We mathematically reveal that the extent of entropy reduction varies with the query norm in softmax normalization, motivating a query-norm aware kernel function for dynamic control over entropy reduction. Furthermore, to ensure norm consistency and enforce non-negativity constraints, we employ a norm-preserving mapping to project all elements of the angular matrix into positive values, leveraging cosine similarity to inhibit dimensions with opposite directions. We conduct extensive experiments demonstrating that the NaLaFormer improves performance on vision and language tasks, enhancing both expressiveness and efficiency by up to 4.2\%.

