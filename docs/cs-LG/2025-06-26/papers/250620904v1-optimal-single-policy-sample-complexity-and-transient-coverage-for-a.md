---
layout: default
title: Optimal Single-Policy Sample Complexity and Transient Coverage for Average-Reward Offline RL
---

# Optimal Single-Policy Sample Complexity and Transient Coverage for Average-Reward Offline RL

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.20904" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.20904v1</a>
  <a href="https://arxiv.org/pdf/2506.20904.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.20904v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.20904v1', 'Optimal Single-Policy Sample Complexity and Transient Coverage for Average-Reward Offline RL')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Matthew Zurek, Guy Zamir, Yudong Chen

**åˆ†ç±»**: cs.LG, cs.IT, math.OC, stat.ML

**å‘å¸ƒæ—¥æœŸ**: 2025-06-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå•ç­–ç•¥æ ·æœ¬å¤æ‚åº¦ä»¥è§£å†³å¹³å‡å¥–åŠ±ç¦»çº¿å¼ºåŒ–å­¦ä¹ é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `ç¦»çº¿å¼ºåŒ–å­¦ä¹ ` `å¹³å‡å¥–åŠ±` `é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹` `æ ·æœ¬å¤æ‚åº¦` `ç­–ç•¥å­¦ä¹ ` `åˆ†ä½æ•°è£å‰ª` `å¼±é€šä¿¡MDP`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¹³å‡å¥–åŠ±ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ—¶é¢ä¸´åˆ†å¸ƒè½¬ç§»å’Œéå‡åŒ€è¦†ç›–çš„æŒ‘æˆ˜ï¼Œç†è®ºç ”ç©¶ç›¸å¯¹ä¸è¶³ã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°ç®—æ³•ï¼ŒåŸºäºæ‚²è§‚æŠ˜æ‰£å€¼è¿­ä»£å’Œåˆ†ä½æ•°è£å‰ªæŠ€æœ¯ï¼Œä¸“æ³¨äºå•ç­–ç•¥æ ·æœ¬å¤æ‚åº¦çš„ä¼˜åŒ–ã€‚
3. é€šè¿‡ç†è®ºåˆ†æå’Œå®éªŒéªŒè¯ï¼Œå±•ç¤ºäº†æ–°æ–¹æ³•åœ¨è¦†ç›–å‡è®¾ä¸Šçš„ä¼˜åŠ¿ï¼Œæä¾›äº†æ›´ç²¾ç¡®çš„æ ·æœ¬å¤æ‚åº¦ç•Œé™ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç ”ç©¶äº†å¹³å‡å¥–åŠ±é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ä¸­çš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼ŒæŒ‡å‡ºäº†åˆ†å¸ƒè½¬ç§»å’Œéå‡åŒ€è¦†ç›–å¸¦æ¥çš„æŒ‘æˆ˜ã€‚ä»¥å¾€çš„ç ”ç©¶åœ¨å•ç­–ç•¥æ•°æ®è¦†ç›–å‡è®¾ä¸‹è·å¾—äº†æ€§èƒ½ä¿è¯ï¼Œä½†è¿™äº›ä¿è¯ä¾èµ–äºæ‰€æœ‰ç­–ç•¥çš„ç»Ÿä¸€å¤æ‚æ€§åº¦é‡ã€‚æˆ‘ä»¬æå‡ºäº†ä»…ä¾èµ–äºç›®æ ‡ç­–ç•¥çš„ç²¾ç¡®ä¿è¯ï¼Œç‰¹åˆ«æ˜¯åå·®è·¨åº¦å’Œä¸€ç§æ–°é¢–çš„ç­–ç•¥å‘½ä¸­åŠå¾„ï¼Œä»è€Œé¦–æ¬¡ä¸ºå¹³å‡å¥–åŠ±ç¦»çº¿å¼ºåŒ–å­¦ä¹ æä¾›äº†å®Œå…¨çš„å•ç­–ç•¥æ ·æœ¬å¤æ‚åº¦ç•Œé™ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é¦–æ¬¡å¤„ç†äº†ä¸€èˆ¬çš„å¼±é€šä¿¡MDPï¼Œå…‹æœäº†ä»¥å¾€ç ”ç©¶ä¸­çš„é™åˆ¶æ€§ç»“æ„å‡è®¾ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºæ‚²è§‚æŠ˜æ‰£å€¼è¿­ä»£çš„ç®—æ³•ï¼Œå¹¶é€šè¿‡æ–°é¢–çš„åˆ†ä½æ•°è£å‰ªæŠ€æœ¯å¢å¼ºäº†è¯¥ç®—æ³•ï¼Œå…è®¸ä½¿ç”¨æ›´ç²¾ç¡®çš„ç»éªŒè·¨åº¦æƒ©ç½šå‡½æ•°ã€‚æˆ‘ä»¬çš„ç®—æ³•æ— éœ€ä»»ä½•å…ˆéªŒå‚æ•°çŸ¥è¯†ï¼Œä¸”é€šè¿‡å›°éš¾ç¤ºä¾‹è¡¨æ˜ï¼Œåœ¨æˆ‘ä»¬çš„æ¡ä»¶ä¸‹å­¦ä¹ éœ€è¦è¶…å‡ºç›®æ ‡ç­–ç•¥çš„å¹³ç¨³åˆ†å¸ƒçš„è¦†ç›–å‡è®¾ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡è§£å†³çš„æ˜¯åœ¨å¹³å‡å¥–åŠ±ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œå¦‚ä½•åœ¨åˆ†å¸ƒè½¬ç§»å’Œéå‡åŒ€è¦†ç›–çš„æƒ…å†µä¸‹ï¼Œè·å¾—æœ‰æ•ˆçš„æ ·æœ¬å¤æ‚åº¦ç•Œé™ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºæ‰€æœ‰ç­–ç•¥çš„ç»Ÿä¸€å¤æ‚æ€§åº¦é‡ï¼Œé™åˆ¶äº†å…¶é€‚ç”¨æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æå‡ºä»…ä¾èµ–äºç›®æ ‡ç­–ç•¥çš„æ ·æœ¬å¤æ‚åº¦ç•Œé™ï¼Œåˆ©ç”¨åå·®è·¨åº¦å’Œç­–ç•¥å‘½ä¸­åŠå¾„æ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œèƒ½å¤Ÿæ›´å‡†ç¡®åœ°åæ˜ å•ç­–ç•¥çš„å­¦ä¹ èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ‚²è§‚æŠ˜æ‰£å€¼è¿­ä»£ç®—æ³•çš„è®¾è®¡ï¼Œç»“åˆåˆ†ä½æ•°è£å‰ªæŠ€æœ¯ã€‚ç®—æ³•çš„ä¸»è¦æ¨¡å—åŒ…æ‹¬çŠ¶æ€å€¼å‡½æ•°çš„æ›´æ–°ã€ç­–ç•¥è¯„ä¼°å’Œç­–ç•¥æ”¹è¿›ï¼Œç¡®ä¿åœ¨å¼±é€šä¿¡MDPä¸­æœ‰æ•ˆè¿è¡Œã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºé¦–æ¬¡æå‡ºäº†å•ç­–ç•¥æ ·æœ¬å¤æ‚åº¦ç•Œé™ï¼Œå¹¶å¼•å…¥äº†æ–°çš„ç­–ç•¥å‘½ä¸­åŠå¾„æ¦‚å¿µï¼Œå…‹æœäº†ä»¥å¾€ç ”ç©¶ä¸­çš„ç»“æ„å‡è®¾é™åˆ¶ã€‚

**å…³é”®è®¾è®¡**ï¼šç®—æ³•è®¾è®¡ä¸­ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬åå·®è·¨åº¦çš„è®¡ç®—å’Œåˆ†ä½æ•°è£å‰ªçš„å…·ä½“å®ç°ï¼ŒæŸå¤±å‡½æ•°é‡‡ç”¨åŸºäºç»éªŒè·¨åº¦çš„æƒ©ç½šæœºåˆ¶ï¼Œç¡®ä¿äº†ç®—æ³•çš„æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æç®—æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•åœ¨æ ·æœ¬å¤æ‚åº¦ä¸Šæœ‰æ˜¾è‘—æå‡ï¼Œå…·ä½“æ€§èƒ½æ•°æ®å±•ç¤ºäº†åœ¨å¼±é€šä¿¡MDPç¯å¢ƒä¸‹çš„æœ‰æ•ˆæ€§ï¼ŒéªŒè¯äº†ç†è®ºç•Œé™çš„å‡†ç¡®æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ã€ä¸ªæ€§åŒ–æ¨èç³»ç»Ÿç­‰ï¼Œèƒ½å¤Ÿåœ¨æ•°æ®æ”¶é›†æˆæœ¬é«˜æ˜‚æˆ–ç¯å¢ƒå˜åŒ–å‰§çƒˆçš„åœºæ™¯ä¸­ï¼Œæä¾›æœ‰æ•ˆçš„å†³ç­–æ”¯æŒã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›æ¨åŠ¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ åœ¨å®é™…åº”ç”¨ä¸­çš„å¹¿æ³›é‡‡ç”¨ï¼Œæå‡æ™ºèƒ½ç³»ç»Ÿçš„å­¦ä¹ æ•ˆç‡å’Œé€‚åº”èƒ½åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We study offline reinforcement learning in average-reward MDPs, which presents increased challenges from the perspectives of distribution shift and non-uniform coverage, and has been relatively underexamined from a theoretical perspective. While previous work obtains performance guarantees under single-policy data coverage assumptions, such guarantees utilize additional complexity measures which are uniform over all policies, such as the uniform mixing time. We develop sharp guarantees depending only on the target policy, specifically the bias span and a novel policy hitting radius, yielding the first fully single-policy sample complexity bound for average-reward offline RL. We are also the first to handle general weakly communicating MDPs, contrasting restrictive structural assumptions made in prior work. To achieve this, we introduce an algorithm based on pessimistic discounted value iteration enhanced by a novel quantile clipping technique, which enables the use of a sharper empirical-span-based penalty function. Our algorithm also does not require any prior parameter knowledge for its implementation. Remarkably, we show via hard examples that learning under our conditions requires coverage assumptions beyond the stationary distribution of the target policy, distinguishing single-policy complexity measures from previously examined cases. We also develop lower bounds nearly matching our main result.

