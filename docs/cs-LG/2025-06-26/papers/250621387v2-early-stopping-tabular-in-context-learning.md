---
layout: default
title: Early Stopping Tabular In-Context Learning
---

# Early Stopping Tabular In-Context Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21387" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.21387v2</a>
  <a href="https://arxiv.org/pdf/2506.21387.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21387v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21387v2', 'Early Stopping Tabular In-Context Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jaris KÃ¼ken, Lennart Purucker, Frank Hutter

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-26 (æ›´æ–°: 2025-06-28)

**å¤‡æ³¨**: ICML Workshop Paper

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ—©åœæœºåˆ¶ä»¥æå‡è¡¨æ ¼ä¸Šä¸‹æ–‡å­¦ä¹ çš„æ¨ç†æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¡¨æ ¼å­¦ä¹ ` `ä¸Šä¸‹æ–‡å­¦ä¹ ` `æ—©åœæœºåˆ¶` `æ¨ç†æ•ˆç‡` `Transformer`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è¡¨æ ¼åŸºç¡€æ¨¡å‹åœ¨æ¨ç†æ—¶æˆæœ¬è¾ƒé«˜ï¼Œå°¤å…¶æ˜¯é¢å¯¹å¤§è§„æ¨¡æ•°æ®é›†æ—¶ï¼Œæ•ˆç‡äºŸå¾…æå‡ã€‚
2. æœ¬æ–‡æå‡ºåœ¨ä¸Šä¸‹æ–‡å­¦ä¹ è¿‡ç¨‹ä¸­åŠ¨æ€è¯„ä¼°æ˜¯å¦æ—©åœï¼Œä»è€Œå‡å°‘ä¸å¿…è¦çš„è®¡ç®—ï¼Œæé«˜æ¨ç†é€Ÿåº¦ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ—©åœæœºåˆ¶åœ¨å°å‹åˆ†ç±»ä»»åŠ¡ä¸­åŠ é€Ÿæ¨ç†æœ€é«˜å¯è¾¾1.3å€ï¼Œåœ¨å¤§å‹ä»»åŠ¡ä¸­å¯è¾¾2.2å€ï¼Œä¸”é¢„æµ‹æ€§èƒ½ä¿æŒç¨³å®šã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¡¨æ ¼åŸºç¡€æ¨¡å‹åœ¨å¤šç§è¡¨æ ¼å­¦ä¹ ä»»åŠ¡ä¸­é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ å±•ç°äº†å¼ºå¤§çš„æ€§èƒ½ï¼Œä¸”æ— éœ€ä¸‹æ¸¸å¾®è°ƒå³å¯å®ç°ç¨³å¥çš„æ³›åŒ–ã€‚ç„¶è€Œï¼Œå…¶æ¨ç†æ—¶é—´æˆæœ¬ä»ç„¶è¾ƒé«˜ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†è¾ƒå¤§æ•°æ®é›†æ—¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºåœ¨ä¸Šä¸‹æ–‡å­¦ä¹ è¿‡ç¨‹ä¸­å®æ–½æ—©åœæœºåˆ¶ï¼Œé€šè¿‡åŠ¨æ€è¯„ä¼°åœ¨æ¯ä¸ªTransformerç¼–ç å™¨å±‚åæ˜¯å¦åœæ­¢å­¦ä¹ ã€‚ä¸€æ—¦åœæ­¢ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„é€å±‚è§£ç å™¨è§£ç åµŒå…¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ—©åœæœºåˆ¶åœ¨34ä¸ªå°å‹åˆ†ç±»ä»»åŠ¡ä¸­åŠ é€Ÿæ¨ç†é€Ÿåº¦æœ€é«˜å¯è¾¾1.3å€ï¼Œä¸”é¢„æµ‹æ€§èƒ½å‡ ä¹æ²¡æœ‰ä¸‹é™ã€‚åœ¨äº”ä¸ªè¾ƒå¤§åˆ†ç±»ä»»åŠ¡ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œé€Ÿåº¦æå‡å¯è¾¾2.2å€ï¼Œè¯æ˜äº†æ—©åœä½œä¸ºæé«˜è¡¨æ ¼ä¸Šä¸‹æ–‡å­¦ä¹ æ•ˆç‡çš„æœ‰æ•ˆå®ç”¨ç­–ç•¥çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è¡¨æ ¼åŸºç¡€æ¨¡å‹åœ¨æ¨ç†æ—¶çš„é«˜æ—¶é—´æˆæœ¬é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤§è§„æ¨¡æ•°æ®é›†æ—¶ï¼Œç°æœ‰æ–¹æ³•çš„æ•ˆç‡ä¸è¶³ä»¥æ»¡è¶³å®é™…éœ€æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ è¿‡ç¨‹ä¸­å®æ–½æ—©åœæœºåˆ¶ï¼ŒåŠ¨æ€è¯„ä¼°æ¯ä¸ªTransformerç¼–ç å™¨å±‚åæ˜¯å¦ç»§ç»­å­¦ä¹ ï¼Œä»è€Œå‡å°‘ä¸å¿…è¦çš„è®¡ç®—ï¼Œæé«˜æ¨ç†æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬Transformerç¼–ç å™¨å’Œé€å±‚è§£ç å™¨ã€‚é¦–å…ˆï¼Œè¾“å…¥æ•°æ®ç»è¿‡å¤šä¸ªç¼–ç å™¨å±‚å¤„ç†ï¼Œéšååœ¨æ¯å±‚åè¿›è¡ŒåŠ¨æ€è¯„ä¼°ï¼Œå†³å®šæ˜¯å¦åœæ­¢å­¦ä¹ ã€‚ä¸€æ—¦å†³å®šåœæ­¢ï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„è§£ç å™¨å¯¹åµŒå…¥è¿›è¡Œè§£ç ï¼Œè¾“å‡ºæœ€ç»ˆç»“æœã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥äº†åŠ¨æ€æ—©åœæœºåˆ¶ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­æ ¹æ®å½“å‰çŠ¶æ€å†³å®šæ˜¯å¦ç»§ç»­è®¡ç®—ï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•çš„å›ºå®šè®¡ç®—æµç¨‹å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒåŠ¨æ€è¯„ä¼°æœºåˆ¶çš„å®ç°ä¾èµ–äºå¯¹æ¯ä¸ªç¼–ç å™¨å±‚è¾“å‡ºçš„å®æ—¶åˆ†æï¼Œç¡®ä¿åœ¨ä¿æŒé¢„æµ‹æ€§èƒ½çš„åŒæ—¶ï¼Œæœ€å¤§é™åº¦åœ°å‡å°‘è®¡ç®—èµ„æºçš„æ¶ˆè€—ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ—©åœæœºåˆ¶åœ¨34ä¸ªå°å‹åˆ†ç±»ä»»åŠ¡ä¸­åŠ é€Ÿæ¨ç†é€Ÿåº¦æœ€é«˜å¯è¾¾1.3å€ï¼Œä¸”é¢„æµ‹æ€§èƒ½å‡ ä¹æ²¡æœ‰ä¸‹é™ã€‚åœ¨äº”ä¸ªè¾ƒå¤§åˆ†ç±»ä»»åŠ¡ä¸Šçš„è¯„ä¼°ä¸­ï¼Œé€Ÿåº¦æå‡å¯è¾¾2.2å€ï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬é‡‘èã€åŒ»ç–—å’Œå¸‚åœºåˆ†æç­‰éœ€è¦å¤„ç†å¤§é‡è¡¨æ ¼æ•°æ®çš„è¡Œä¸šã€‚é€šè¿‡æé«˜æ¨ç†æ•ˆç‡ï¼Œèƒ½å¤ŸåŠ é€Ÿå†³ç­–è¿‡ç¨‹ï¼Œæå‡ä¸šåŠ¡å“åº”é€Ÿåº¦ï¼Œå…·æœ‰æ˜¾è‘—çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Tabular foundation models have shown strong performance across various tabular learning tasks via in-context learning, offering robust generalization without any downstream finetuning. However, their inference-time costs remain high, particularly for larger datasets. To address this, we propose early-stopping the in-context learning process. We achieve this by dynamically evaluating whether to stop in-context learning after each Transformer encoder layer. Once stopped, we decode the embedding using a pre-trained layer-wise decoder. Experiments across 34 small classification tasks size show that early stopping in-context learning accelerates inference by up to x1.3 with negligible degradation in predictive performance. To assess scalability, we further evaluate our method on five larger classification tasks, achieving speedups of up to x2.2. Our results demonstrate the potential of early exiting as an effective and practical strategy for improving the efficiency of tabular in-context learning.

