---
layout: default
title: DiLoCoX: A Low-Communication Large-Scale Training Framework for Decentralized Cluster
---

# DiLoCoX: A Low-Communication Large-Scale Training Framework for Decentralized Cluster

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21263" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.21263v1</a>
  <a href="https://arxiv.org/pdf/2506.21263.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21263v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21263v1', 'DiLoCoX: A Low-Communication Large-Scale Training Framework for Decentralized Cluster')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ji Qi, WenPeng Zhu, Li Li, Ming Wu, YingJun Wu, Wu He, Xun Gao, Jason Zeng, Michael Heinrich

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDiLoCoXä»¥è§£å†³å¤§è§„æ¨¡åˆ†æ•£é›†ç¾¤è®­ç»ƒä¸­çš„ä½é€šä¿¡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åˆ†æ•£è®­ç»ƒ` `ä½é€šä¿¡` `å¤§è§„æ¨¡æ¨¡å‹` `ç®¡é“å¹¶è¡Œ` `è‡ªé€‚åº”æ¢¯åº¦å‹ç¼©` `æ·±åº¦å­¦ä¹ ` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åŸºç¡€æ¨¡å‹è®­ç»ƒæ–¹æ³•åœ¨å¤§è§„æ¨¡åˆ†æ•£é›†ç¾¤ä¸­é¢ä¸´é«˜é€šä¿¡éœ€æ±‚å’Œç½‘ç»œé€Ÿåº¦é™åˆ¶çš„æŒ‘æˆ˜ã€‚
2. DiLoCoXæ¡†æ¶é€šè¿‡ç»“åˆå¤šç§æŠ€æœ¯ï¼Œå¦‚ç®¡é“å¹¶è¡Œå’Œè‡ªé€‚åº”æ¢¯åº¦å‹ç¼©ï¼Œæ¥é™ä½é€šä¿¡éœ€æ±‚å¹¶æé«˜è®­ç»ƒæ•ˆç‡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒDiLoCoXåœ¨1Gbpsç½‘ç»œä¸Šé¢„è®­ç»ƒ107Bå‚æ•°æ¨¡å‹æ—¶ï¼Œè®­ç»ƒé€Ÿåº¦æå‡äº†357å€ï¼Œä¸”æ¨¡å‹æ”¶æ•›æ€§ä¿æŒè‰¯å¥½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºç¡€æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„åˆ†å¸ƒå¼è®­ç»ƒéœ€è¦é«˜æ°´å¹³çš„é€šä¿¡ï¼Œé€šå¸¸ä¾èµ–äºå¿«é€Ÿå¯é çš„é›†ä¸­å¼é›†ç¾¤ã€‚æœ¬æ–‡æå‡ºDiLoCoXï¼Œä¸€ä¸ªä½é€šä¿¡çš„å¤§è§„æ¨¡åˆ†æ•£é›†ç¾¤è®­ç»ƒæ¡†æ¶ï¼Œç»“åˆäº†ç®¡é“å¹¶è¡Œã€åŒä¼˜åŒ–å™¨ç­–ç•¥ã€é€šä¿¡ä¸æœ¬åœ°è®­ç»ƒçš„ä¸€æ­¥å»¶è¿Ÿé‡å ä»¥åŠè‡ªé€‚åº”æ¢¯åº¦å‹ç¼©æ–¹æ¡ˆã€‚é€šè¿‡ç†è®ºåˆ†æå’Œå®è¯éªŒè¯ï¼ŒDiLoCoXåœ¨1Gbpsç½‘ç»œä¸ŠæˆåŠŸé¢„è®­ç»ƒäº†107Bå‚æ•°çš„åŸºç¡€æ¨¡å‹ï¼Œç›¸è¾ƒäºä¼ ç»Ÿçš„AllReduceæ–¹æ³•ï¼Œè®­ç»ƒé€Ÿåº¦æå‡äº†357å€ï¼ŒåŒæ—¶æ¨¡å‹æ”¶æ•›æ€§å‡ ä¹æ²¡æœ‰ä¸‹é™ã€‚è¿™æ˜¯é¦–æ¬¡æˆåŠŸåº”ç”¨äºè¶…è¿‡1000äº¿å‚æ•°æ¨¡å‹çš„åˆ†æ•£è®­ç»ƒæ¡†æ¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨ä½é€Ÿç½‘ç»œç¯å¢ƒä¸‹è¿›è¡Œå¤§è§„æ¨¡åŸºç¡€æ¨¡å‹è®­ç»ƒæ—¶çš„é«˜é€šä¿¡éœ€æ±‚é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºé›†ä¸­å¼é›†ç¾¤ï¼Œé™åˆ¶äº†æ¨¡å‹è§„æ¨¡å’Œè®­ç»ƒæ•ˆç‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDiLoCoXé€šè¿‡å¼•å…¥ç®¡é“å¹¶è¡Œã€åŒä¼˜åŒ–å™¨ç­–ç•¥å’Œè‡ªé€‚åº”æ¢¯åº¦å‹ç¼©ç­‰æŠ€æœ¯ï¼Œæ—¨åœ¨å‡å°‘é€šä¿¡å¼€é”€å¹¶æé«˜è®­ç»ƒé€Ÿåº¦ï¼Œä»è€Œä½¿å¾—åœ¨åˆ†æ•£é›†ç¾¤ä¸­è®­ç»ƒè¶…è¿‡100äº¿å‚æ•°çš„æ¨¡å‹æˆä¸ºå¯èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDiLoCoXçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªæ¨¡å—ï¼šé¦–å…ˆæ˜¯ç®¡é“å¹¶è¡Œä»¥æé«˜è®¡ç®—æ•ˆç‡ï¼Œå…¶æ¬¡æ˜¯åŒä¼˜åŒ–å™¨ç­–ç•¥ä»¥ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹ï¼Œæœ€åæ˜¯è‡ªé€‚åº”æ¢¯åº¦å‹ç¼©ä»¥å‡å°‘é€šä¿¡é‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šDiLoCoXçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶ç»¼åˆä½¿ç”¨äº†ä¸€æ­¥å»¶è¿Ÿé‡å çš„é€šä¿¡ä¸æœ¬åœ°è®­ç»ƒç­–ç•¥ï¼Œä»¥åŠè‡ªé€‚åº”æ¢¯åº¦å‹ç¼©æ–¹æ¡ˆï¼Œè¿™ä¸ä¼ ç»Ÿçš„AllReduceæ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—é™ä½äº†é€šä¿¡éœ€æ±‚å¹¶æå‡äº†è®­ç»ƒé€Ÿåº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å…³é”®è®¾è®¡æ–¹é¢ï¼ŒDiLoCoXé‡‡ç”¨äº†ç‰¹å®šçš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ï¼Œä»¥ç¡®ä¿åœ¨ä½é€šä¿¡æ¡ä»¶ä¸‹ä»èƒ½ä¿æŒæ¨¡å‹çš„æ”¶æ•›æ€§å’Œè®­ç»ƒæ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

DiLoCoXåœ¨1Gbpsç½‘ç»œä¸ŠæˆåŠŸé¢„è®­ç»ƒ107Bå‚æ•°çš„åŸºç¡€æ¨¡å‹ï¼Œç›¸è¾ƒäºä¼ ç»Ÿçš„AllReduceæ–¹æ³•ï¼Œè®­ç»ƒé€Ÿåº¦æå‡äº†357å€ï¼ŒåŒæ—¶æ¨¡å‹çš„æ”¶æ•›æ€§å‡ ä¹æ²¡æœ‰ä¸‹é™ï¼Œå±•ç¤ºäº†å…¶åœ¨ä½é€šä¿¡ç¯å¢ƒä¸‹çš„ä¼˜è¶Šæ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DiLoCoXæ¡†æ¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„è®­ç»ƒã€åˆ†æ•£å¼è®¡ç®—ç¯å¢ƒä¸‹çš„æœºå™¨å­¦ä¹ ä»»åŠ¡ä»¥åŠéœ€è¦é«˜æ•ˆé€šä¿¡çš„æ·±åº¦å­¦ä¹ åº”ç”¨ã€‚å…¶å®é™…ä»·å€¼åœ¨äºèƒ½å¤Ÿåœ¨ä½å¸¦å®½æ¡ä»¶ä¸‹æœ‰æ•ˆè®­ç»ƒè¶…å¤§è§„æ¨¡æ¨¡å‹ï¼Œæ¨åŠ¨äººå·¥æ™ºèƒ½æŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The distributed training of foundation models, particularly large language models (LLMs), demands a high level of communication. Consequently, it is highly dependent on a centralized cluster with fast and reliable interconnects. Can we conduct training on slow networks and thereby unleash the power of decentralized clusters when dealing with models exceeding 100 billion parameters? In this paper, we propose DiLoCoX, a low-communication large-scale decentralized cluster training framework. It combines Pipeline Parallelism with Dual Optimizer Policy, One-Step-Delay Overlap of Communication and Local Training, and an Adaptive Gradient Compression Scheme. This combination significantly improves the scale of parameters and the speed of model pre-training. We justify the benefits of one-step-delay overlap of communication and local training, as well as the adaptive gradient compression scheme, through a theoretical analysis of convergence. Empirically, we demonstrate that DiLoCoX is capable of pre-training a 107B foundation model over a 1Gbps network. Compared to vanilla AllReduce, DiLoCoX can achieve a 357x speedup in distributed training while maintaining negligible degradation in model convergence. To the best of our knowledge, this is the first decentralized training framework successfully applied to models with over 100 billion parameters.

