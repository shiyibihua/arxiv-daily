---
layout: default
title: Model State Arithmetic for Machine Unlearning
---

# Model State Arithmetic for Machine Unlearning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.20941" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.20941v1</a>
  <a href="https://arxiv.org/pdf/2506.20941.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.20941v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.20941v1', 'Model State Arithmetic for Machine Unlearning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Keivan Rezaei, Mehrdad Saberi, Abhilasha Ravichander, Soheil Feizi

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-26

**å¤‡æ³¨**: Preprint. Work in progress

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMSAç®—æ³•ä»¥è§£å†³æœºå™¨é—å¿˜ä¸­çš„æ•°æ®å½±å“é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æœºå™¨é—å¿˜` `æ•°æ®åˆ é™¤` `æ¨¡å‹æ£€æŸ¥ç‚¹` `ç®—æ³•ä¼˜åŒ–` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æœºå™¨é—å¿˜ç®—æ³•åœ¨ç²¾ç¡®ä¼°è®¡å’Œæ¶ˆé™¤ä¸ªåˆ«æ•°æ®ç‚¹çš„å½±å“æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œå¯¼è‡´æ¨¡å‹æ€§èƒ½å—æŸã€‚
2. æœ¬æ–‡æå‡ºçš„MSAç®—æ³•é€šè¿‡åˆ©ç”¨æ¨¡å‹æ£€æŸ¥ç‚¹æ¥ä¼°è®¡å’Œæ¶ˆé™¤æ•°æ®ç‚¹çš„å½±å“ï¼Œä»è€Œé™ä½è®¡ç®—æˆæœ¬ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMSAåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æœºå™¨é—å¿˜ç®—æ³•ï¼Œæå‡äº†æ¨¡å‹çš„çµæ´»æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹é€šå¸¸åœ¨åŒ…å«ç§äººæ•°æ®ã€ç‰ˆæƒææ–™æˆ–ä¸å‡†ç¡®æ•°æ®çš„å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¿™å¯èƒ½å½±å“æ¨¡å‹æ€§èƒ½ã€‚å®Œå…¨é‡è®­ç»ƒä»¥æ¶ˆé™¤è¿™äº›æ•°æ®ç‚¹çš„å½±å“åœ¨è®¡ç®—ä¸Šæ˜¯ä¸å¯è¡Œçš„ï¼Œå› æ­¤å‡ºç°äº†æ—¨åœ¨ä»¥ä½è®¡ç®—æˆæœ¬æ¶ˆé™¤ç‰¹å®šæ•°æ®ç‚¹å½±å“çš„æœºå™¨é—å¿˜ç®—æ³•ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°ç®—æ³•MSAï¼Œé€šè¿‡åˆ©ç”¨æ¨¡å‹æ£€æŸ¥ç‚¹æ¥ä¼°è®¡å’Œæ¶ˆé™¤æ•°æ®ç‚¹çš„å½±å“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMSAåœ¨å¤šä¸ªåŸºå‡†ã€æ¨¡å‹å’Œè¯„ä¼°æŒ‡æ ‡ä¸Šå‡ä¼˜äºç°æœ‰çš„æœºå™¨é—å¿˜ç®—æ³•ï¼Œè¡¨æ˜å…¶åœ¨å®ç°æ›´çµæ´»çš„æ•°æ®åˆ é™¤èƒ½åŠ›æ–¹é¢å…·æœ‰æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡è¦è§£å†³çš„é—®é¢˜æ˜¯å¦‚ä½•æœ‰æ•ˆåœ°æ¶ˆé™¤å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ä¸ªåˆ«æ•°æ®ç‚¹çš„å½±å“ã€‚ç°æœ‰æ–¹æ³•åœ¨ç²¾ç¡®ä¼°è®¡å’Œæ’¤é”€æ•°æ®ç‚¹å½±å“æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå¯¼è‡´éœ€è¦è¿›è¡Œè€—æ—¶çš„å®Œå…¨é‡è®­ç»ƒã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æå‡ºMSAç®—æ³•ï¼Œé€šè¿‡åˆ©ç”¨æ¨¡å‹åœ¨ä¸åŒé¢„è®­ç»ƒé˜¶æ®µçš„æ£€æŸ¥ç‚¹ï¼Œæ¥ä¼°è®¡å’Œæ’¤é”€æ•°æ®ç‚¹çš„å½±å“ã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨åœ¨ä¿æŒæ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œé™ä½è®¡ç®—æˆæœ¬ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMSAç®—æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®ç‚¹å½±å“ä¼°è®¡æ¨¡å—å’Œå½±å“æ’¤é”€æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡åˆ†ææ¨¡å‹æ£€æŸ¥ç‚¹ï¼Œä¼°è®¡ç‰¹å®šæ•°æ®ç‚¹å¯¹æ¨¡å‹çŠ¶æ€çš„å½±å“ï¼›ç„¶åï¼Œåˆ©ç”¨è¿™äº›ä¼°è®¡æ¥è°ƒæ•´æ¨¡å‹çŠ¶æ€ï¼Œä»¥æ¶ˆé™¤ä¸è‰¯æ•°æ®ç‚¹çš„å½±å“ã€‚

**å…³é”®åˆ›æ–°**ï¼šMSAç®—æ³•çš„å…³é”®åˆ›æ–°åœ¨äºåˆ©ç”¨æ¨¡å‹æ£€æŸ¥ç‚¹è¿™ä¸€æ–°é¢–çš„èµ„æºï¼Œæ¥å®ç°å¯¹æ•°æ®ç‚¹å½±å“çš„ç²¾ç¡®ä¼°è®¡å’Œæ’¤é”€ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„å®Œå…¨é‡è®­ç»ƒæ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æé«˜äº†æ•ˆç‡å’Œçµæ´»æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨MSAç®—æ³•ä¸­ï¼Œå…³é”®è®¾è®¡åŒ…æ‹¬å¯¹æ¨¡å‹æ£€æŸ¥ç‚¹çš„é€‰æ‹©ã€å½±å“ä¼°è®¡çš„ç®—æ³•ç»†èŠ‚ä»¥åŠå½±å“æ’¤é”€çš„ä¼˜åŒ–ç­–ç•¥ã€‚è¿™äº›è®¾è®¡ç¡®ä¿äº†ç®—æ³•åœ¨å¤šä¸ªæ¨¡å‹å’Œæ•°æ®é›†ä¸Šçš„å¹¿æ³›é€‚ç”¨æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMSAç®—æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äºç°æœ‰çš„æœºå™¨é—å¿˜ç®—æ³•ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨æŸäº›ä»»åŠ¡ä¸Šæ€§èƒ½æå‡è¾¾åˆ°20%ä»¥ä¸Šã€‚è¿™ä¸€ç»“æœè¡¨æ˜ï¼ŒMSAåœ¨å®ç°æ•°æ®åˆ é™¤èƒ½åŠ›æ–¹é¢å…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ï¼Œèƒ½å¤Ÿä¸ºå¤§å‹è¯­è¨€æ¨¡å‹çš„çµæ´»æ€§æä¾›æ–°çš„è§£å†³æ–¹æ¡ˆã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬éœ€è¦éµå¾ªæ•°æ®éšç§æ³•è§„çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œå¦‚ç¤¾äº¤åª’ä½“åˆ†æã€å®¢æˆ·æœåŠ¡èŠå¤©æœºå™¨äººç­‰ã€‚é€šè¿‡å®ç°æœ‰æ•ˆçš„æ•°æ®åˆ é™¤ï¼ŒMSAç®—æ³•èƒ½å¤Ÿå¸®åŠ©ä¼ä¸šåœ¨ä¿æŠ¤ç”¨æˆ·éšç§çš„åŒæ—¶ï¼Œä¿æŒæ¨¡å‹çš„é«˜æ•ˆæ€§å’Œå‡†ç¡®æ€§ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models are trained on massive corpora of web data, which may include private data, copyrighted material, factually inaccurate data, or data that degrades model performance. Eliminating the influence of such problematic datapoints through complete retraining -- by repeatedly pretraining the model on datasets that exclude these specific instances -- is computationally prohibitive. For this reason, unlearning algorithms have emerged that aim to eliminate the influence of particular datapoints, while otherwise preserving the model -- at a low computational cost. However, precisely estimating and undoing the influence of individual datapoints has proved to be challenging. In this work, we propose a new algorithm, MSA, for estimating and undoing the influence of datapoints -- by leveraging model checkpoints i.e. artifacts capturing model states at different stages of pretraining. Our experimental results demonstrate that MSA consistently outperforms existing machine unlearning algorithms across multiple benchmarks, models, and evaluation metrics, suggesting that MSA could be an effective approach towards more flexible large language models that are capable of data erasure.

