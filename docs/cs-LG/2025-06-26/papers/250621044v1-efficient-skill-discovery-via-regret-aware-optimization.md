---
layout: default
title: Efficient Skill Discovery via Regret-Aware Optimization
---

# Efficient Skill Discovery via Regret-Aware Optimization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21044" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.21044v1</a>
  <a href="https://arxiv.org/pdf/2506.21044.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21044v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21044v1', 'Efficient Skill Discovery via Regret-Aware Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: He Zhang, Ming Zhou, Shaopeng Zhai, Ying Sun, Hui Xiong

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºåæ‚”æ„ŸçŸ¥ä¼˜åŒ–çš„é«˜æ•ˆæŠ€èƒ½å‘ç°æ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ— ç›‘ç£å­¦ä¹ ` `æŠ€èƒ½å‘ç°` `å¼ºåŒ–å­¦ä¹ ` `åæ‚”æ„ŸçŸ¥` `ç­–ç•¥å­¦ä¹ ` `é«˜ç»´ç¯å¢ƒ` `å¤šæ ·æ€§æ¢ç´¢`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æŠ€èƒ½å‘ç°æ–¹æ³•åœ¨é«˜ç»´ç¯å¢ƒä¸­æ•ˆç‡æœ‰é™ï¼Œéš¾ä»¥æœ‰æ•ˆæ¢ç´¢å¤šæ ·åŒ–çš„æŠ€èƒ½ã€‚
2. æœ¬æ–‡æå‡ºå°†æŠ€èƒ½å‘ç°è§†ä¸ºæŠ€èƒ½ç”Ÿæˆä¸ç­–ç•¥å­¦ä¹ çš„æå°æå¤§åšå¼ˆï¼Œåˆ©ç”¨åæ‚”æ„ŸçŸ¥ä¼˜åŒ–æ¥å¼•å¯¼æŠ€èƒ½æ¢ç´¢ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨æ•ˆç‡å’Œå¤šæ ·æ€§ä¸Šå‡ä¼˜äºç°æœ‰åŸºçº¿ï¼Œå¹¶åœ¨é«˜ç»´ç¯å¢ƒä¸­å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ— ç›‘ç£æŠ€èƒ½å‘ç°æ—¨åœ¨åœ¨å¼€æ”¾å¼å¼ºåŒ–å­¦ä¹ ä¸­å­¦ä¹ å¤šæ ·ä¸”å¯åŒºåˆ†çš„è¡Œä¸ºã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é€šè¿‡çº¯æ¢ç´¢ã€äº’ä¿¡æ¯ä¼˜åŒ–å’Œå­¦ä¹ æ—¶é—´è¡¨ç¤ºæ¥æé«˜å¤šæ ·æ€§ï¼Œä½†åœ¨é«˜ç»´æƒ…å†µä¸‹æ•ˆç‡æœ‰é™ã€‚æœ¬æ–‡å°†æŠ€èƒ½å‘ç°æ¡†æ¶åŒ–ä¸ºæŠ€èƒ½ç”Ÿæˆä¸ç­–ç•¥å­¦ä¹ çš„æå°æå¤§åšå¼ˆï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ—¶é—´è¡¨ç¤ºå­¦ä¹ çš„åæ‚”æ„ŸçŸ¥æ–¹æ³•ï¼Œæ‰©å±•äº†å¯å‡çº§ç­–ç•¥å¼ºåº¦çš„æŠ€èƒ½ç©ºé—´ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯æŠ€èƒ½å‘ç°ä¸ç­–ç•¥å­¦ä¹ æ˜¯å¯¹æŠ—çš„ï¼Œå¼±å¼ºåº¦æŠ€èƒ½åº”è¿›ä¸€æ­¥æ¢ç´¢ï¼Œè€Œå·²æ”¶æ•›å¼ºåº¦çš„æŠ€èƒ½åˆ™å‡å°‘æ¢ç´¢ã€‚æˆ‘ä»¬é€šè¿‡å¯å­¦ä¹ çš„æŠ€èƒ½ç”Ÿæˆå™¨æ¥å¼•å¯¼æŠ€èƒ½å‘ç°ï¼Œé¿å…é€€åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ•ˆç‡å’Œå¤šæ ·æ€§ä¸Šå‡ä¼˜äºåŸºçº¿ï¼Œå¹¶åœ¨é«˜ç»´ç¯å¢ƒä¸­å®ç°äº†15%çš„é›¶æ ·æœ¬æ”¹è¿›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡è§£å†³çš„å…·ä½“é—®é¢˜æ˜¯ç°æœ‰æ— ç›‘ç£æŠ€èƒ½å‘ç°æ–¹æ³•åœ¨é«˜ç»´ç¯å¢ƒä¸­çš„æ•ˆç‡ä¸è¶³ï¼Œå¯¼è‡´æŠ€èƒ½å¤šæ ·æ€§æ¢ç´¢å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†æŠ€èƒ½å‘ç°è§†ä¸ºæŠ€èƒ½ç”Ÿæˆä¸ç­–ç•¥å­¦ä¹ ä¹‹é—´çš„å¯¹æŠ—åšå¼ˆï¼Œå¼ºè°ƒå¼±å¼ºåº¦æŠ€èƒ½çš„è¿›ä¸€æ­¥æ¢ç´¢å’Œå·²æ”¶æ•›æŠ€èƒ½çš„å‡å°‘æ¢ç´¢ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æŠ€èƒ½ç”Ÿæˆå™¨å’Œç­–ç•¥å­¦ä¹ æ¨¡å—ï¼ŒæŠ€èƒ½ç”Ÿæˆå™¨æ ¹æ®åæ‚”å€¼è¯„ä¼°æŠ€èƒ½å¼ºåº¦ï¼Œå¹¶å¼•å¯¼æŠ€èƒ½å‘ç°è¿‡ç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥åæ‚”æ„ŸçŸ¥æœºåˆ¶ï¼Œé€šè¿‡è¯„ä¼°æŠ€èƒ½å¼ºåº¦çš„æ”¶æ•›ç¨‹åº¦æ¥ä¼˜åŒ–æŠ€èƒ½æ¢ç´¢ç­–ç•¥ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„å•ä¸€æ¢ç´¢ç­–ç•¥å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼Œä½¿ç”¨å¯å­¦ä¹ çš„æŠ€èƒ½ç”Ÿæˆå™¨æ¥åŠ¨æ€è°ƒæ•´æŠ€èƒ½ç”Ÿæˆè¿‡ç¨‹ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šè€ƒè™‘äº†æŠ€èƒ½å¼ºåº¦çš„åæ‚”å€¼ï¼Œä»¥ç¡®ä¿æŠ€èƒ½ç”Ÿæˆçš„å¤šæ ·æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨å¤šç§å¤æ‚ç¯å¢ƒä¸­å‡ä¼˜äºåŸºçº¿ï¼Œå°¤å…¶åœ¨é«˜ç»´ç¯å¢ƒä¸­å®ç°äº†15%çš„é›¶æ ·æœ¬æ”¹è¿›ï¼Œå±•ç¤ºäº†æ˜¾è‘—çš„æ•ˆç‡å’Œå¤šæ ·æ€§æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€æ¸¸æˆæ™ºèƒ½ä½“å’Œè‡ªåŠ¨åŒ–ç³»ç»Ÿç­‰ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡è¿™äº›é¢†åŸŸä¸­æŠ€èƒ½å­¦ä¹ çš„æ•ˆç‡å’Œå¤šæ ·æ€§ã€‚æœªæ¥ï¼Œéšç€æŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ï¼Œå¯èƒ½ä¼šåœ¨æ›´å¤æ‚çš„ç¯å¢ƒä¸­å®ç°æ›´å¹¿æ³›çš„åº”ç”¨ï¼Œæ¨åŠ¨æ™ºèƒ½ä½“çš„è‡ªä¸»å­¦ä¹ èƒ½åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Unsupervised skill discovery aims to learn diverse and distinguishable behaviors in open-ended reinforcement learning. For existing methods, they focus on improving diversity through pure exploration, mutual information optimization, and learning temporal representation. Despite that they perform well on exploration, they remain limited in terms of efficiency, especially for the high-dimensional situations. In this work, we frame skill discovery as a min-max game of skill generation and policy learning, proposing a regret-aware method on top of temporal representation learning that expands the discovered skill space along the direction of upgradable policy strength. The key insight behind the proposed method is that the skill discovery is adversarial to the policy learning, i.e., skills with weak strength should be further explored while less exploration for the skills with converged strength. As an implementation, we score the degree of strength convergence with regret, and guide the skill discovery with a learnable skill generator. To avoid degeneration, skill generation comes from an up-gradable population of skill generators. We conduct experiments on environments with varying complexities and dimension sizes. Empirical results show that our method outperforms baselines in both efficiency and diversity. Moreover, our method achieves a 15% zero shot improvement in high-dimensional environments, compared to existing methods.

