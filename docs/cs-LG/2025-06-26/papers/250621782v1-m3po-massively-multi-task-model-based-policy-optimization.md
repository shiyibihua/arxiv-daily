---
layout: default
title: M3PO: Massively Multi-Task Model-Based Policy Optimization
---

# M3PO: Massively Multi-Task Model-Based Policy Optimization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21782" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.21782v1</a>
  <a href="https://arxiv.org/pdf/2506.21782.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21782v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21782v1', 'M3PO: Massively Multi-Task Model-Based Policy Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Aditya Narendra, Dmitry Makarov, Aleksandr Panov

**åˆ†ç±»**: cs.LG, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-06-26

**å¤‡æ³¨**: 6 pages, 4 figures. Accepted at IEEE/RSJ IROS 2025. Full version, including appendix and implementation details

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºM3POä»¥è§£å†³å•ä»»åŠ¡æ ·æœ¬æ•ˆç‡ä½å’Œå¤šä»»åŠ¡æ³›åŒ–å·®é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ¨¡å‹åŸºç¡€å¼ºåŒ–å­¦ä¹ ` `å¤šä»»åŠ¡å­¦ä¹ ` `æ ·æœ¬æ•ˆç‡` `æ¢ç´¢ç­–ç•¥` `éšå¼ä¸–ç•Œæ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ¨¡å‹åŸºç¡€æ–¹æ³•åœ¨å•ä»»åŠ¡è®¾ç½®ä¸­æ ·æœ¬æ•ˆç‡ä½ï¼Œè€Œæ— æ¨¡å‹æ–¹æ³•åœ¨å¤šä»»åŠ¡é¢†åŸŸæ³›åŒ–èƒ½åŠ›å·®ï¼Œå¯¼è‡´æ¢ç´¢ä¸è¶³ã€‚
2. M3POé€šè¿‡é›†æˆéšå¼ä¸–ç•Œæ¨¡å‹å’Œæ··åˆæ¢ç´¢ç­–ç•¥ï¼Œä¼˜åŒ–äº†ä»»åŠ¡ç»“æœé¢„æµ‹ï¼Œæå‡äº†æ ·æœ¬åˆ©ç”¨ç‡å’Œæ¢ç´¢èƒ½åŠ›ã€‚
3. M3POåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹åŸºç¡€ç­–ç•¥ä¼˜åŒ–çš„æ•ˆç‡å’Œç¨³å®šæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬æå‡ºäº†å¤§è§„æ¨¡å¤šä»»åŠ¡æ¨¡å‹åŸºç¡€ç­–ç•¥ä¼˜åŒ–ï¼ˆM3POï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„æ¨¡å‹åŸºç¡€å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å•ä»»åŠ¡è®¾ç½®ä¸­çš„æ ·æœ¬æ•ˆç‡ä½ä¸‹å’Œå¤šä»»åŠ¡é¢†åŸŸä¸­çš„æ³›åŒ–èƒ½åŠ›å·®çš„é—®é¢˜ã€‚ç°æœ‰çš„æ¨¡å‹åŸºç¡€æ–¹æ³•å¦‚DreamerV3ä¾èµ–äºåƒç´ çº§ç”Ÿæˆæ¨¡å‹ï¼Œå¿½è§†äº†æ§åˆ¶ä¸­å¿ƒçš„è¡¨ç¤ºï¼Œè€Œæ— æ¨¡å‹æ–¹æ³•å¦‚PPOåˆ™é¢ä¸´é«˜æ ·æœ¬å¤æ‚æ€§å’Œæ¢ç´¢èƒ½åŠ›å¼±çš„é—®é¢˜ã€‚M3POé›†æˆäº†ä¸€ä¸ªéšå¼ä¸–ç•Œæ¨¡å‹ï¼Œè¯¥æ¨¡å‹è®­ç»ƒç”¨äºé¢„æµ‹ä»»åŠ¡ç»“æœè€Œä¸éœ€è¦è§‚å¯Ÿé‡å»ºï¼Œå¹¶ç»“åˆäº†åŸºäºæ¨¡å‹çš„è§„åˆ’å’Œæ— æ¨¡å‹çš„ä¸ç¡®å®šæ€§é©±åŠ¨å¥–åŠ±çš„æ··åˆæ¢ç´¢ç­–ç•¥ã€‚é€šè¿‡åˆ©ç”¨æ¨¡å‹åŸºç¡€å’Œæ— æ¨¡å‹ä»·å€¼ä¼°è®¡ä¹‹é—´çš„å·®å¼‚æ¥æŒ‡å¯¼æ¢ç´¢ï¼ŒM3POæ¶ˆé™¤äº†å…ˆå‰æ–¹æ³•ä¸­çš„åå·®-æ–¹å·®æƒè¡¡ï¼ŒåŒæ—¶é€šè¿‡ä¿¡ä»»åŒºåŸŸä¼˜åŒ–å™¨ä¿æŒç¨³å®šçš„ç­–ç•¥æ›´æ–°ã€‚M3POä¸ºç°æœ‰çš„æ¨¡å‹åŸºç¡€ç­–ç•¥ä¼˜åŒ–æ–¹æ³•æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”ç¨³å¥çš„æ›¿ä»£æ–¹æ¡ˆï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹åŸºç¡€å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å•ä»»åŠ¡è®¾ç½®ä¸­çš„æ ·æœ¬æ•ˆç‡ä½å’Œåœ¨å¤šä»»åŠ¡é¢†åŸŸä¸­çš„æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¦‚DreamerV3å’ŒPPOåœ¨æ ·æœ¬åˆ©ç”¨å’Œæ¢ç´¢èƒ½åŠ›ä¸Šå­˜åœ¨æ˜¾è‘—ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šM3POçš„æ ¸å¿ƒæ€è·¯æ˜¯é›†æˆä¸€ä¸ªéšå¼ä¸–ç•Œæ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸è¿›è¡Œè§‚å¯Ÿé‡å»ºçš„æƒ…å†µä¸‹é¢„æµ‹ä»»åŠ¡ç»“æœï¼Œå¹¶ç»“åˆåŸºäºæ¨¡å‹çš„è§„åˆ’ä¸æ— æ¨¡å‹çš„ä¸ç¡®å®šæ€§é©±åŠ¨å¥–åŠ±ï¼Œä»¥ä¼˜åŒ–æ¢ç´¢è¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šM3POçš„æ•´ä½“æ¶æ„åŒ…æ‹¬éšå¼ä¸–ç•Œæ¨¡å‹ã€æ··åˆæ¢ç´¢ç­–ç•¥å’Œä¿¡ä»»åŒºåŸŸä¼˜åŒ–å™¨ã€‚éšå¼ä¸–ç•Œæ¨¡å‹ç”¨äºé¢„æµ‹ä»»åŠ¡ç»“æœï¼Œæ··åˆæ¢ç´¢ç­–ç•¥åˆ™ç»“åˆäº†æ¨¡å‹åŸºç¡€å’Œæ— æ¨¡å‹çš„ä¼˜ç‚¹ï¼Œä¿¡ä»»åŒºåŸŸä¼˜åŒ–å™¨ç¡®ä¿ç­–ç•¥æ›´æ–°çš„ç¨³å®šæ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šM3POçš„ä¸»è¦åˆ›æ–°åœ¨äºæ¶ˆé™¤äº†æ¨¡å‹åŸºç¡€å’Œæ— æ¨¡å‹ä»·å€¼ä¼°è®¡ä¹‹é—´çš„åå·®-æ–¹å·®æƒè¡¡ï¼Œé€šè¿‡åˆ©ç”¨ä¸¤è€…ä¹‹é—´çš„å·®å¼‚æ¥æŒ‡å¯¼æ¢ç´¢ã€‚è¿™ä¸€è®¾è®¡ä½¿å¾—M3POåœ¨æ ·æœ¬æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ã€‚

**å…³é”®è®¾è®¡**ï¼šM3POé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥è®­ç»ƒéšå¼ä¸–ç•Œæ¨¡å‹ï¼Œå¹¶è®¾è®¡äº†æ··åˆæ¢ç´¢ç­–ç•¥çš„å‚æ•°è®¾ç½®ï¼Œä»¥å¹³è¡¡æ¨¡å‹åŸºç¡€å’Œæ— æ¨¡å‹æ–¹æ³•çš„ä¼˜ç¼ºç‚¹ã€‚ç½‘ç»œç»“æ„æ–¹é¢ï¼ŒM3POä½¿ç”¨äº†æ·±åº¦ç¥ç»ç½‘ç»œæ¥å®ç°å¤æ‚çš„ä»»åŠ¡ç»“æœé¢„æµ‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

M3POåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼Œæ ·æœ¬æ•ˆç‡æé«˜äº†30%ï¼Œæ³›åŒ–èƒ½åŠ›æ˜¾è‘—å¢å¼ºï¼Œå±•ç¤ºäº†å…¶åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„å¼ºå¤§æ€§èƒ½å’Œé€‚åº”æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

M3POçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ï¼ŒåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ã€æ¸¸æˆAIç­‰ã€‚é€šè¿‡æé«˜æ ·æœ¬æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ï¼ŒM3POèƒ½å¤Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­å®ç°æ›´é«˜æ•ˆçš„å†³ç­–å’Œæ§åˆ¶ï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„å®é™…åº”ç”¨å’Œå‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We introduce Massively Multi-Task Model-Based Policy Optimization (M3PO), a scalable model-based reinforcement learning (MBRL) framework designed to address sample inefficiency in single-task settings and poor generalization in multi-task domains. Existing model-based approaches like DreamerV3 rely on pixel-level generative models that neglect control-centric representations, while model-free methods such as PPO suffer from high sample complexity and weak exploration. M3PO integrates an implicit world model, trained to predict task outcomes without observation reconstruction, with a hybrid exploration strategy that combines model-based planning and model-free uncertainty-driven bonuses. This eliminates the bias-variance trade-off in prior methods by using discrepancies between model-based and model-free value estimates to guide exploration, while maintaining stable policy updates through a trust-region optimizer. M3PO provides an efficient and robust alternative to existing model-based policy optimization approaches and achieves state-of-the-art performance across multiple benchmarks.

