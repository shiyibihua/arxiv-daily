---
layout: default
title: Strict Subgoal Execution: Reliable Long-Horizon Planning in Hierarchical Reinforcement Learning
---

# Strict Subgoal Execution: Reliable Long-Horizon Planning in Hierarchical Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21039" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.21039v1</a>
  <a href="https://arxiv.org/pdf/2506.21039.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21039v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21039v1', 'Strict Subgoal Execution: Reliable Long-Horizon Planning in Hierarchical Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jaebak Hwang, Sanghyeon Lee, Jeongmo Kim, Seungyul Han

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-26

**å¤‡æ³¨**: 9 technical page followed by references and appendix

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸¥æ ¼å­ç›®æ ‡æ‰§è¡Œæ¡†æ¶ä»¥è§£å†³é•¿æ—¶é—´è§„åˆ’é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `é•¿æ—¶é—´è§„åˆ’` `å¼ºåŒ–å­¦ä¹ ` `åˆ†å±‚RL` `å­ç›®æ ‡æ‰§è¡Œ` `æ¢ç´¢ç­–ç•¥` `å›¾åŸºæ–¹æ³•` `è·¯å¾„ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. é•¿æ—¶é—´ç›®æ ‡æ¡ä»¶ä»»åŠ¡åœ¨å¼ºåŒ–å­¦ä¹ ä¸­é¢ä¸´ç›®æ ‡é¥è¿œå’Œå¥–åŠ±ç¨€ç–çš„æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆè§£å†³ã€‚
2. æå‡ºä¸¥æ ¼å­ç›®æ ‡æ‰§è¡Œï¼ˆSSEï¼‰æ¡†æ¶ï¼Œé€šè¿‡ç»“æ„æ€§çº¦æŸå®ç°å•æ­¥å­ç›®æ ‡å¯è¾¾æ€§ï¼Œå¢å¼ºæ¢ç´¢èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSSEåœ¨æ•ˆç‡å’ŒæˆåŠŸç‡ä¸Šå‡ä¼˜äºç°æœ‰çš„ç›®æ ‡æ¡ä»¶RLå’Œåˆ†å±‚RLæ–¹æ³•ï¼Œè¡¨ç°å‡ºæ˜¾è‘—çš„æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é•¿æ—¶é—´ç›®æ ‡æ¡ä»¶ä»»åŠ¡å¯¹å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æå‡ºäº†åŸºæœ¬æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨ç›®æ ‡é¥è¿œä¸”å¥–åŠ±ç¨€ç–çš„æƒ…å†µä¸‹ã€‚å°½ç®¡åˆ†å±‚å’ŒåŸºäºå›¾çš„æ–¹æ³•æä¾›äº†éƒ¨åˆ†è§£å†³æ–¹æ¡ˆï¼Œä½†å®ƒä»¬å¸¸å¸¸é¢ä¸´å­ç›®æ ‡ä¸å¯è¡Œå’Œè§„åˆ’æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸¥æ ¼å­ç›®æ ‡æ‰§è¡Œï¼ˆSSEï¼‰æ¡†æ¶ï¼Œé€šè¿‡ç»“æ„æ€§çº¦æŸé«˜å±‚å†³ç­–ï¼Œå¼ºåˆ¶å®ç°å•æ­¥å­ç›®æ ‡å¯è¾¾æ€§ã€‚ä¸ºå¢å¼ºæ¢ç´¢ï¼ŒSSEé‡‡ç”¨è§£è€¦çš„æ¢ç´¢ç­–ç•¥ï¼Œç³»ç»Ÿæ€§åœ°éå†ç›®æ ‡ç©ºé—´çš„æœªå……åˆ†æ¢ç´¢åŒºåŸŸã€‚æ­¤å¤–ï¼Œå¤±è´¥æ„ŸçŸ¥è·¯å¾„ä¼˜åŒ–é€šè¿‡æ ¹æ®è§‚å¯Ÿåˆ°çš„ä½å±‚æˆåŠŸç‡åŠ¨æ€è°ƒæ•´è¾¹ç¼˜æˆæœ¬ï¼Œä»è€Œæ”¹å–„å­ç›®æ ‡çš„å¯é æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSSEåœ¨å¤šç§é•¿æ—¶é—´åŸºå‡†æµ‹è¯•ä¸­ï¼Œæ•ˆç‡å’ŒæˆåŠŸç‡å‡ä¼˜äºç°æœ‰çš„ç›®æ ‡æ¡ä»¶RLå’Œåˆ†å±‚RLæ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³é•¿æ—¶é—´ç›®æ ‡æ¡ä»¶ä»»åŠ¡ä¸­çš„å­ç›®æ ‡ä¸å¯è¡Œå’Œè§„åˆ’æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚ç°æœ‰çš„åˆ†å±‚å’Œå›¾åŸºæ–¹æ³•åœ¨é¢å¯¹é¥è¿œç›®æ ‡æ—¶ï¼Œå¾€å¾€æ— æ³•æœ‰æ•ˆè§„åˆ’å’Œæ‰§è¡Œå­ç›®æ ‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„ä¸¥æ ¼å­ç›®æ ‡æ‰§è¡Œï¼ˆSSEï¼‰æ¡†æ¶ï¼Œé€šè¿‡ç»“æ„æ€§çº¦æŸé«˜å±‚å†³ç­–ï¼Œç¡®ä¿æ¯ä¸ªå­ç›®æ ‡çš„å•æ­¥å¯è¾¾æ€§ï¼Œä»è€Œæå‡ä»»åŠ¡çš„å¯æ‰§è¡Œæ€§å’ŒæˆåŠŸç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSSEæ¡†æ¶ä¸»è¦åŒ…æ‹¬ä¸¤ä¸ªæ¨¡å—ï¼šé«˜å±‚å†³ç­–æ¨¡å—å’Œä½å±‚æ‰§è¡Œæ¨¡å—ã€‚é«˜å±‚æ¨¡å—è´Ÿè´£ç”Ÿæˆå­ç›®æ ‡ï¼Œè€Œä½å±‚æ¨¡å—åˆ™æ‰§è¡Œè¿™äº›å­ç›®æ ‡ã€‚æ¡†æ¶è¿˜å¼•å…¥äº†è§£è€¦çš„æ¢ç´¢ç­–ç•¥å’Œå¤±è´¥æ„ŸçŸ¥è·¯å¾„ä¼˜åŒ–ï¼Œä»¥å¢å¼ºæ¢ç´¢å’Œè§„åˆ’çš„æœ‰æ•ˆæ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šSSEçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºé€šè¿‡ç»“æ„æ€§çº¦æŸç¡®ä¿å­ç›®æ ‡çš„å•æ­¥å¯è¾¾æ€§ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„è®¾è®¡æ€è·¯æœ‰æœ¬è´¨åŒºåˆ«ï¼Œåè€…å¾€å¾€å¿½è§†äº†å­ç›®æ ‡çš„å¯è¡Œæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼ŒSSEé‡‡ç”¨åŠ¨æ€è°ƒæ•´çš„è¾¹ç¼˜æˆæœ¬ï¼Œä»¥åæ˜ ä½å±‚æˆåŠŸç‡ã€‚æ­¤å¤–ï¼Œæ¢ç´¢ç­–ç•¥çš„è®¾è®¡ä½¿å¾—ç®—æ³•èƒ½å¤Ÿç³»ç»Ÿæ€§åœ°è¦†ç›–æœªå……åˆ†æ¢ç´¢çš„ç›®æ ‡åŒºåŸŸï¼Œä»è€Œæé«˜æ•´ä½“æ•ˆç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¤šç§é•¿æ—¶é—´åŸºå‡†æµ‹è¯•ä¸­ï¼ŒSSEæ¡†æ¶çš„æ•ˆç‡å’ŒæˆåŠŸç‡å‡æ˜¾è‘—ä¼˜äºç°æœ‰çš„ç›®æ ‡æ¡ä»¶RLå’Œåˆ†å±‚RLæ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸäº›ä»»åŠ¡ä¸­ï¼ŒæˆåŠŸç‡æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å¤æ‚ç¯å¢ƒä¸­çš„å¼ºå¤§é€‚åº”èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ã€æ¸¸æˆAIç­‰é•¿æ—¶é—´è§„åˆ’ä»»åŠ¡ã€‚é€šè¿‡æé«˜é•¿æ—¶é—´ç›®æ ‡æ¡ä»¶ä»»åŠ¡çš„æˆåŠŸç‡å’Œæ•ˆç‡ï¼ŒSSEæ¡†æ¶èƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­æ˜¾è‘—æå‡æ™ºèƒ½ä½“çš„è¡¨ç°ï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„å‘å±•ã€‚æœªæ¥ï¼ŒSSEè¿˜å¯èƒ½æ‰©å±•åˆ°æ›´å¤æ‚çš„å¤šä»»åŠ¡å­¦ä¹ å’Œåä½œä»»åŠ¡ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Long-horizon goal-conditioned tasks pose fundamental challenges for reinforcement learning (RL), particularly when goals are distant and rewards are sparse. While hierarchical and graph-based methods offer partial solutions, they often suffer from subgoal infeasibility and inefficient planning. We introduce Strict Subgoal Execution (SSE), a graph-based hierarchical RL framework that enforces single-step subgoal reachability by structurally constraining high-level decision-making. To enhance exploration, SSE employs a decoupled exploration policy that systematically traverses underexplored regions of the goal space. Furthermore, a failure-aware path refinement, which refines graph-based planning by dynamically adjusting edge costs according to observed low-level success rates, thereby improving subgoal reliability. Experimental results across diverse long-horizon benchmarks demonstrate that SSE consistently outperforms existing goal-conditioned RL and hierarchical RL approaches in both efficiency and success rate.

