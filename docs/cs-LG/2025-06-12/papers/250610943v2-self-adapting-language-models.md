---
layout: default
title: Self-Adapting Language Models
---

# Self-Adapting Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.10943" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.10943v2</a>
  <a href="https://arxiv.org/pdf/2506.10943.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.10943v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.10943v2', 'Self-Adapting Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Adam Zweiger, Jyothish Pari, Han Guo, Ekin AkyÃ¼rek, Yoon Kim, Pulkit Agrawal

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-12 (æ›´æ–°: 2025-09-18)

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://jyopari.github.io/posts/)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªé€‚åº”è¯­è¨€æ¨¡å‹ä»¥è§£å†³é™æ€æ¨¡å‹é€‚åº”æ€§ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è‡ªé€‚åº”æ¨¡å‹` `è¯­è¨€æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `å¾®è°ƒ` `çŸ¥è¯†æ•´åˆ` `å°‘æ ·æœ¬å­¦ä¹ ` `æ¨¡å‹ç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ç¼ºä¹åŠ¨æ€é€‚åº”èƒ½åŠ›ï¼Œæ— æ³•æ ¹æ®æ–°ä»»åŠ¡æˆ–çŸ¥è¯†è¿›è¡Œæœ‰æ•ˆè°ƒæ•´ã€‚
2. SEALæ¡†æ¶é€šè¿‡ç”Ÿæˆè‡ªç¼–è¾‘å†…å®¹ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè‡ªæˆ‘ç”Ÿæˆå¾®è°ƒæ•°æ®å’Œæ›´æ–°æŒ‡ä»¤ï¼Œç›´æ¥æ§åˆ¶é€‚åº”è¿‡ç¨‹ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒSEALåœ¨çŸ¥è¯†æ•´åˆå’Œå°‘æ ·æœ¬å­¦ä¹ ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå±•ç¤ºäº†è‡ªæˆ‘å¯¼å‘é€‚åº”çš„æ½œåŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åŠŸèƒ½å¼ºå¤§ï¼Œä½†ç¼ºä¹æ ¹æ®æ–°ä»»åŠ¡ã€çŸ¥è¯†æˆ–ç¤ºä¾‹è‡ªæˆ‘è°ƒæ•´æƒé‡çš„æœºåˆ¶ã€‚æœ¬æ–‡æå‡ºè‡ªé€‚åº”è¯­è¨€æ¨¡å‹ï¼ˆSEALï¼‰æ¡†æ¶ï¼Œä½¿LLMsèƒ½å¤Ÿé€šè¿‡ç”Ÿæˆè‡ªå·±çš„å¾®è°ƒæ•°æ®å’Œæ›´æ–°æŒ‡ä»¤è¿›è¡Œè‡ªæˆ‘é€‚åº”ã€‚æ¨¡å‹åœ¨æ¥æ”¶åˆ°æ–°è¾“å…¥æ—¶ï¼Œç”Ÿæˆè‡ªç¼–è¾‘å†…å®¹ï¼Œå¯èƒ½ä»¥ä¸åŒæ–¹å¼é‡ç»„ä¿¡æ¯ï¼ŒæŒ‡å®šä¼˜åŒ–è¶…å‚æ•°ï¼Œæˆ–è°ƒç”¨å·¥å…·è¿›è¡Œæ•°æ®å¢å¼ºå’ŒåŸºäºæ¢¯åº¦çš„æ›´æ–°ã€‚é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œè¿™äº›è‡ªç¼–è¾‘å¯¼è‡´æŒä¹…çš„æƒé‡æ›´æ–°ï¼Œå®ç°é•¿æœŸé€‚åº”ã€‚æˆ‘ä»¬ä½¿ç”¨å¼ºåŒ–å­¦ä¹ å¾ªç¯è®­ç»ƒæ¨¡å‹ç”Ÿæˆæœ‰æ•ˆçš„è‡ªç¼–è¾‘ï¼Œä»¥æ›´æ–°æ¨¡å‹çš„ä¸‹æ¸¸æ€§èƒ½ä½œä¸ºå¥–åŠ±ä¿¡å·ã€‚ä¸ä¾èµ–å•ç‹¬é€‚åº”æ¨¡å—æˆ–è¾…åŠ©ç½‘ç»œçš„å…ˆå‰æ–¹æ³•ä¸åŒï¼ŒSEALç›´æ¥åˆ©ç”¨æ¨¡å‹è‡ªèº«çš„ç”Ÿæˆæ¥æ§åˆ¶é€‚åº”è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSEALåœ¨çŸ¥è¯†æ•´åˆå’Œå°‘æ ·æœ¬æ³›åŒ–æ–¹é¢å±•ç°å‡ºè‰¯å¥½çš„å‰æ™¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é¢å¯¹æ–°ä»»åŠ¡æˆ–çŸ¥è¯†æ—¶ï¼Œæ— æ³•åŠ¨æ€è°ƒæ•´å…¶æƒé‡ï¼Œå¯¼è‡´é€‚åº”æ€§ä¸è¶³ã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸ä¾èµ–äºå•ç‹¬çš„é€‚åº”æ¨¡å—æˆ–è¾…åŠ©ç½‘ç»œï¼Œæ— æ³•å……åˆ†åˆ©ç”¨æ¨¡å‹è‡ªèº«çš„ç”Ÿæˆèƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡æå‡ºçš„SEALæ¡†æ¶å…è®¸æ¨¡å‹é€šè¿‡ç”Ÿæˆè‡ªç¼–è¾‘å†…å®¹ï¼Œè‡ªåŠ¨ç”Ÿæˆå¾®è°ƒæ•°æ®å’Œæ›´æ–°æŒ‡ä»¤ï¼Œä»è€Œå®ç°è‡ªæˆ‘é€‚åº”ã€‚è¿™ç§è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨æ¥æ”¶åˆ°æ–°è¾“å…¥æ—¶ï¼Œçµæ´»è°ƒæ•´å…¶è¡Œä¸ºã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSEALçš„æ•´ä½“æ¶æ„åŒ…æ‹¬è¾“å…¥å¤„ç†ã€ç”Ÿæˆè‡ªç¼–è¾‘ã€ä¼˜åŒ–è¶…å‚æ•°è®¾ç½®å’Œæ•°æ®å¢å¼ºç­‰æ¨¡å—ã€‚æ¨¡å‹é¦–å…ˆæ¥æ”¶æ–°è¾“å…¥ï¼Œç„¶åç”Ÿæˆè‡ªç¼–è¾‘å†…å®¹ï¼Œæœ€åé€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å®ç°æƒé‡æ›´æ–°ã€‚

**å…³é”®åˆ›æ–°**ï¼šSEALçš„ä¸»è¦åˆ›æ–°åœ¨äºç›´æ¥åˆ©ç”¨æ¨¡å‹è‡ªèº«çš„ç”Ÿæˆæ¥æ§åˆ¶é€‚åº”è¿‡ç¨‹ï¼Œè€Œä¸æ˜¯ä¾èµ–å¤–éƒ¨æ¨¡å—ã€‚è¿™ç§æ–¹æ³•æé«˜äº†é€‚åº”æ•ˆç‡ï¼Œå¹¶å‡å°‘äº†å¯¹é¢å¤–èµ„æºçš„éœ€æ±‚ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨å¼ºåŒ–å­¦ä¹ å¾ªç¯æ¥ä¼˜åŒ–è‡ªç¼–è¾‘çš„ç”Ÿæˆï¼Œå¥–åŠ±ä¿¡å·åŸºäºæ›´æ–°åæ¨¡å‹çš„ä¸‹æ¸¸æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæ¨¡å‹çš„è¶…å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡ä¹Ÿç»è¿‡ç²¾å¿ƒè°ƒæ•´ï¼Œä»¥ç¡®ä¿è‡ªé€‚åº”è¿‡ç¨‹çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSEALåœ¨çŸ¥è¯†æ•´åˆä»»åŠ¡ä¸­ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹ï¼Œæ€§èƒ½æå‡è¾¾åˆ°äº†20%ã€‚åœ¨å°‘æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸‹ï¼ŒSEALèƒ½å¤Ÿæœ‰æ•ˆæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå±•ç¤ºå‡ºå…¶åœ¨åŠ¨æ€é€‚åº”æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½åŠ©æ‰‹ã€æ•™è‚²æŠ€æœ¯ç­‰ã€‚é€šè¿‡å®ç°è‡ªæˆ‘å¯¼å‘çš„é€‚åº”èƒ½åŠ›ï¼ŒSEALå¯ä»¥ä½¿è¯­è¨€æ¨¡å‹åœ¨é¢å¯¹ä¸æ–­å˜åŒ–çš„ç”¨æˆ·éœ€æ±‚å’ŒçŸ¥è¯†æ›´æ–°æ—¶ï¼Œä¿æŒé«˜æ•ˆçš„æ€§èƒ½ã€‚è¿™å°†å¯¹ä¸ªæ€§åŒ–æœåŠ¡å’Œå®æ—¶ä¿¡æ¯å¤„ç†äº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) are powerful but static; they lack mechanisms to adapt their weights in response to new tasks, knowledge, or examples. We introduce Self-Adapting LLMs (SEAL), a framework that enables LLMs to self-adapt by generating their own finetuning data and update directives. Given a new input, the model produces a self-edit-a generation that may restructure the information in different ways, specify optimization hyperparameters, or invoke tools for data augmentation and gradient-based updates. Through supervised finetuning (SFT), these self-edits result in persistent weight updates, enabling lasting adaptation. To train the model to produce effective self-edits, we use a reinforcement learning loop with the downstream performance of the updated model as the reward signal. Unlike prior approaches that rely on separate adaptation modules or auxiliary networks, SEAL directly uses the model's own generation to control its adaptation process. Experiments on knowledge incorporation and few-shot generalization show that SEAL is a promising step toward language models capable of self-directed adaptation. Our website and code is available at https://jyopari.github.io/posts/seal.

