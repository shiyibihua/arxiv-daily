---
layout: default
title: Logarithmic Smoothing for Adaptive PAC-Bayesian Off-Policy Learning
---

# Logarithmic Smoothing for Adaptive PAC-Bayesian Off-Policy Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.10664" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.10664v1</a>
  <a href="https://arxiv.org/pdf/2506.10664.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.10664v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.10664v1', 'Logarithmic Smoothing for Adaptive PAC-Bayesian Off-Policy Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Maxime Haddouche, Otmane Sakhi

**åˆ†ç±»**: stat.ML, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-12

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªé€‚åº”PAC-Bayesianç¦»çº¿å­¦ä¹ çš„æ–°æ–¹æ³•ä»¥æé«˜æ•°æ®è´¨é‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è‡ªé€‚åº”å­¦ä¹ ` `PAC-Bayesian` `ç¦»çº¿å­¦ä¹ ` `å¯¹æ•°å¹³æ»‘` `ç­–ç•¥ä¼˜åŒ–` `æ•°æ®è´¨é‡` `åŠ¨æ€ç¯å¢ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç¦»çº¿å­¦ä¹ æ–¹æ³•åœ¨æ•°æ®è´¨é‡å’Œç­–ç•¥ä¼˜åŒ–æ–¹é¢å­˜åœ¨å±€é™ï¼Œéš¾ä»¥é€‚åº”åŠ¨æ€ç¯å¢ƒã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºPAC-Bayesianç†è®ºçš„è‡ªé€‚åº”ç¦»çº¿å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡è¿­ä»£ä¼˜åŒ–ç­–ç•¥æ¥æé«˜æ•°æ®è´¨é‡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é™æ€è®¾ç½®ä¸‹ä¸ç°æœ‰é¢†å…ˆæ–¹æ³•è¡¨ç°ç›¸å½“ï¼Œè€Œåœ¨åŠ¨æ€ç¯å¢ƒä¸­åˆ™æ˜¾è‘—æå‡äº†æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç¦»çº¿å­¦ä¹ æ˜¯ä»é™æ€è¡Œä¸ºç­–ç•¥ä¸‹æ”¶é›†çš„æ—¥å¿—äº¤äº’ä¸­å­¦ä¹ æœ€ä¼˜ç­–ç•¥çš„ä¸»è¦æ¡†æ¶ã€‚æœ¬æ–‡ç ”ç©¶äº†è‡ªé€‚åº”ç¦»çº¿å­¦ä¹ çš„æ›´çµæ´»è®¾ç½®ï¼Œé€šè¿‡è¿­ä»£ä¼˜åŒ–ç­–ç•¥ä»¥æ”¶é›†æ›´é«˜è´¨é‡çš„æ•°æ®ã€‚åŸºäºPAC-Bayesianå­¦ä¹ ä¸å¯¹æ•°å¹³æ»‘ï¼ˆLSï¼‰çš„æˆåŠŸï¼Œæœ¬æ–‡å°†è¯¥æ¡†æ¶æ‰©å±•åˆ°è‡ªé€‚åº”åœºæ™¯ï¼Œå¹¶åˆ©ç”¨åœ¨çº¿PAC-Bayesianç†è®ºçš„å·¥å…·ã€‚æˆ‘ä»¬è¯æ˜äº†å¯¹LSä¼°è®¡å™¨çš„åˆç†è°ƒæ•´è‡ªç„¶é€‚åº”å¤šè½®éƒ¨ç½²ï¼Œå¹¶åœ¨æ¸©å’Œæ¡ä»¶ä¸‹å®ç°æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ã€‚æˆ‘ä»¬çš„ç®—æ³•åœ¨é™æ€è®¾ç½®ä¸‹ä¸é¢†å…ˆçš„ç¦»çº¿æ–¹æ³•è¡¨ç°ç›¸å½“ï¼Œè€Œåœ¨å…è®¸ä¸­é—´ç­–ç•¥éƒ¨ç½²æ—¶æ˜¾è‘—ä¼˜äºå®ƒä»¬ã€‚å¤šç§åœºæ™¯çš„å®è¯è¯„ä¼°çªæ˜¾äº†è‡ªé€‚åº”æ•°æ®æ”¶é›†çš„ä¼˜åŠ¿åŠPAC-Bayesianå…¬å¼çš„å¼ºå¤§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è‡ªé€‚åº”ç¦»çº¿å­¦ä¹ ä¸­ç­–ç•¥ä¼˜åŒ–ä¸æ•°æ®è´¨é‡æå‡çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•åœ¨åŠ¨æ€ç¯å¢ƒä¸­éš¾ä»¥æœ‰æ•ˆåˆ©ç”¨å†å²æ•°æ®ï¼Œå¯¼è‡´å­¦ä¹ æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºäº†ä¸€ç§åŸºäºPAC-Bayesianç†è®ºçš„å¯¹æ•°å¹³æ»‘æ–¹æ³•ï¼Œé€šè¿‡è¿­ä»£ä¼˜åŒ–ç­–ç•¥å¹¶æ”¶é›†æ›´é«˜è´¨é‡çš„æ•°æ®ï¼Œä»è€Œæé«˜å­¦ä¹ æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€ç­–ç•¥è¿­ä»£å’Œæ€§èƒ½è¯„ä¼°ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚åœ¨æ¯ä¸€è½®ä¸­ï¼Œç­–ç•¥æ ¹æ®æ”¶é›†çš„æ•°æ®è¿›è¡Œæ›´æ–°ï¼Œå¹¶åœ¨ä¸‹ä¸€è½®ä¸­é‡æ–°éƒ¨ç½²ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¯¹LSä¼°è®¡å™¨çš„è°ƒæ•´ï¼Œä½¿å…¶èƒ½å¤Ÿé€‚åº”å¤šè½®ç­–ç•¥éƒ¨ç½²ï¼Œä»è€Œå®ç°æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ã€‚è¿™ä¸€è®¾è®¡ä¸ä¼ ç»Ÿé™æ€æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºåŠ¨æ€é€‚åº”æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†é€‚åº”æ€§å­¦ä¹ ç‡å’ŒæŸå¤±å‡½æ•°è®¾è®¡ï¼Œä»¥ç¡®ä¿åœ¨ä¸åŒç­–ç•¥è¿­ä»£ä¸­ä¿æŒç¨³å®šæ€§å’Œæ”¶æ•›æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæœ¬æ–‡æ–¹æ³•åœ¨é™æ€è®¾ç½®ä¸‹çš„æ€§èƒ½ä¸é¢†å…ˆçš„ç¦»çº¿æ–¹æ³•ç›¸å½“ï¼Œè€Œåœ¨å…è®¸ä¸­é—´ç­–ç•¥éƒ¨ç½²çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½æå‡å¹…åº¦è¶…è¿‡20%ã€‚å¤šè½®å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨ä¸åŒåœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§ï¼Œå±•ç°äº†è‡ªé€‚åº”æ•°æ®æ”¶é›†çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åœ¨çº¿æ¨èç³»ç»Ÿã€åŠ¨æ€å¹¿å‘ŠæŠ•æ”¾å’Œè‡ªé€‚åº”æ§åˆ¶ç­‰ã€‚é€šè¿‡æé«˜æ•°æ®æ”¶é›†çš„è´¨é‡å’Œæ•ˆç‡ï¼Œèƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­å®ç°æ›´ä¼˜çš„å†³ç­–æ”¯æŒï¼Œæå‡ç”¨æˆ·ä½“éªŒå’Œç³»ç»Ÿæ€§èƒ½ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åœ¨æ›´å¤šå¤æ‚ç¯å¢ƒä¸­æ¨å¹¿åº”ç”¨ï¼Œæ¨åŠ¨è‡ªé€‚åº”å­¦ä¹ æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Off-policy learning serves as the primary framework for learning optimal policies from logged interactions collected under a static behavior policy. In this work, we investigate the more practical and flexible setting of adaptive off-policy learning, where policies are iteratively refined and re-deployed to collect higher-quality data. Building on the success of PAC-Bayesian learning with Logarithmic Smoothing (LS) in static settings, we extend this framework to the adaptive scenario using tools from online PAC-Bayesian theory. Furthermore, we demonstrate that a principled adjustment to the LS estimator naturally accommodates multiple rounds of deployment and yields faster convergence rates under mild conditions. Our method matches the performance of leading offline approaches in static settings, and significantly outperforms them when intermediate policy deployments are allowed. Empirical evaluations across diverse scenarios highlight both the advantages of adaptive data collection and the strength of the PAC-Bayesian formulation.

