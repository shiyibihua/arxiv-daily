---
layout: default
title: NoLoCo: No-all-reduce Low Communication Training Method for Large Models
---

# NoLoCo: No-all-reduce Low Communication Training Method for Large Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.10911" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.10911v1</a>
  <a href="https://arxiv.org/pdf/2506.10911.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.10911v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.10911v1', 'NoLoCo: No-all-reduce Low Communication Training Method for Large Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jari Kolehmainen, Nikolay Blagoev, John Donaghy, OÄŸuzhan Ersoy, Christopher Nies

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-12

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºNoLoCoä»¥è§£å†³å¤§æ¨¡å‹è®­ç»ƒä¸­çš„é€šä¿¡ç“¶é¢ˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§æ¨¡å‹è®­ç»ƒ` `ä½é€šä¿¡è®­ç»ƒ` `ä¼˜åŒ–ç®—æ³•` `NesterovåŠ¨é‡` `éšå¼åŒæ­¥` `æ¨¡å‹æ”¶æ•›` `åŠ é€Ÿå™¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤§æ¨¡å‹è®­ç»ƒæ–¹æ³•ä¾èµ–äºé«˜å¸¦å®½é›†ç¾¤ï¼Œæˆæœ¬é«˜ä¸”å¯æ‰©å±•æ€§å·®ï¼Œé™åˆ¶äº†æ¨¡å‹è§„æ¨¡ã€‚
2. NoLoCoé€šè¿‡ä¸æ˜¾å¼åŒæ­¥æ‰€æœ‰æ¨¡å‹å‚æ•°ï¼Œåˆ©ç”¨éšæœºé€‰æ‹©çš„æƒé‡è¿›è¡Œéšå¼åŒæ­¥ï¼Œé™ä½äº†é€šä¿¡å¼€é”€ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒNoLoCoåœ¨å¤šç§åŠ é€Ÿå™¨æ•°é‡å’Œæ¨¡å‹è§„æ¨¡ä¸‹ï¼Œé€šä¿¡å¼€é”€æ˜¾è‘—ä½äºç°æœ‰æ–¹æ³•ï¼Œå¹¶ä¸”æ”¶æ•›é€Ÿåº¦æé«˜äº†4%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è¯­è¨€æ¨¡å‹çš„è®­ç»ƒé€šå¸¸ä¾èµ–äºé«˜å¸¦å®½äº’è¿çš„è®¡ç®—é›†ç¾¤ï¼Œç„¶è€Œï¼Œéšç€é›†ç¾¤è§„æ¨¡çš„æ‰©å¤§ï¼Œæˆæœ¬å’Œå¯è¡Œæ€§é—®é¢˜æ—¥ç›Šçªå‡ºã€‚ç°æœ‰çš„ä½é€šä¿¡è®­ç»ƒæ–¹æ³•ä»éœ€è¿›è¡Œæ¨¡å‹å‚æ•°çš„åŒæ­¥ï¼Œå¯¼è‡´åœ¨ä½å¸¦å®½ç½‘ç»œä¸Šå¼€é”€è¾ƒå¤§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä¼˜åŒ–æ–¹æ³•NoLoCoï¼Œè¯¥æ–¹æ³•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸æ˜¾å¼åŒæ­¥æ‰€æœ‰æ¨¡å‹å‚æ•°ï¼Œä»è€Œæ¶ˆé™¤äº†é›†ä½“é€šä¿¡çš„éœ€æ±‚ã€‚NoLoCoé€šè¿‡ä¸€ç§æ–°å˜ä½“çš„NesterovåŠ¨é‡ä¼˜åŒ–å™¨ï¼Œåˆ©ç”¨éšæœºé€‰æ‹©çš„å…¶ä»–æ¨¡å‹æƒé‡è¿›è¡Œéƒ¨åˆ†å¹³å‡ï¼Œä»è€Œå®ç°éšå¼åŒæ­¥ã€‚æˆ‘ä»¬å¯¹NoLoCoè¿›è¡Œäº†ç†è®ºæ”¶æ•›æ€§åˆ†æï¼Œå¹¶åœ¨ä¸åŒè§„æ¨¡çš„æ¨¡å‹ä¸Šè¿›è¡Œäº†å®è¯éªŒè¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨å¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒä¸­ï¼Œç”±äºä¾èµ–é«˜å¸¦å®½é›†ç¾¤è€Œå¯¼è‡´çš„é€šä¿¡ç“¶é¢ˆé—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¦‚DiLoCoä»éœ€è¿›è¡Œæ¨¡å‹å‚æ•°çš„å…¨å±€åŒæ­¥ï¼Œå¢åŠ äº†è®­ç»ƒçš„å¤æ‚æ€§å’Œå¼€é”€ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šNoLoCoçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ä¸æ˜¾å¼åŒæ­¥æ‰€æœ‰æ¨¡å‹å‚æ•°ï¼Œåˆ©ç”¨éšæœºé€‰æ‹©çš„å…¶ä»–æ¨¡å‹æƒé‡è¿›è¡Œéƒ¨åˆ†å¹³å‡ï¼Œä»è€Œå®ç°éšå¼åŒæ­¥ï¼Œé¿å…äº†é›†ä½“é€šä¿¡çš„éœ€æ±‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šNoLoCoçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸€ä¸ªä¼˜åŒ–å™¨æ¨¡å—ï¼Œè¯¥æ¨¡å—é‡‡ç”¨NesterovåŠ¨é‡ä¼˜åŒ–å™¨çš„å˜ä½“ï¼Œé€šè¿‡éšæœºé€‰æ‹©çš„æ¨¡å‹æƒé‡è¿›è¡Œéƒ¨åˆ†å¹³å‡ã€‚è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹å‚æ•°åœ¨ä¸åŒåŠ é€Ÿå™¨é—´è¿›è¡Œéšå¼åŒæ­¥ï¼Œè€Œä¸éœ€è¦å…¨å±€åŒæ­¥æ­¥éª¤ã€‚

**å…³é”®åˆ›æ–°**ï¼šNoLoCoçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶ä¸éœ€è¦å…¨å±€é˜»å¡é€šä¿¡ï¼Œæ˜¾è‘—é™ä½äº†é€šä¿¡å¼€é”€ï¼Œå¹¶ä¸”åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®ç°äº†æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ã€‚è¿™ä¸€æ–¹æ³•ä¸ç°æœ‰çš„ä½é€šä¿¡è®­ç»ƒæ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨NoLoCoä¸­ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬éšæœºé€‰æ‹©çš„æƒé‡æ¯”ä¾‹å’ŒåŠ¨é‡å› å­è®¾ç½®ã€‚æŸå¤±å‡½æ•°ä¸ä¼ ç»Ÿä¼˜åŒ–æ–¹æ³•ç›¸ä¼¼ï¼Œä½†é€šè¿‡éšå¼åŒæ­¥æœºåˆ¶æé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒNoLoCoåœ¨ä¸åŒè§„æ¨¡çš„æ¨¡å‹ï¼ˆä»1.25äº¿åˆ°68äº¿å‚æ•°ï¼‰ä¸Šï¼Œé€šä¿¡å¼€é”€æ˜¾è‘—ä½äºå®Œå…¨åˆ†ç‰‡æ•°æ®å¹¶è¡Œè®­ç»ƒæ–¹æ³•ï¼Œä¸”æ”¶æ•›é€Ÿåº¦æé«˜äº†4%ã€‚ä¸DiLoCoç›¸æ¯”ï¼ŒNoLoCoçš„åŒæ­¥æ­¥éª¤é€Ÿåº¦å¿«ä¸€ä¸ªæ•°é‡çº§ï¼Œæå¤§åœ°å‡å°‘äº†åŠ é€Ÿå™¨çš„ç©ºé—²æ—¶é—´ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

NoLoCoæ–¹æ³•åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹è®­ç»ƒä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶é€‚ç”¨äºèµ„æºæœ‰é™çš„ç¯å¢ƒã€‚å…¶ä½é€šä¿¡å¼€é”€çš„ç‰¹æ€§ä½¿å¾—åœ¨ä½å¸¦å®½ç½‘ç»œä¸‹è¿›è¡Œå¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒæˆä¸ºå¯èƒ½ï¼Œæ¨åŠ¨äº†AIæ¨¡å‹çš„æ™®åŠä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Training large language models is generally done via optimization methods on clusters containing tens of thousands of accelerators, communicating over a high-bandwidth interconnect. Scaling up these clusters is expensive and can become impractical, imposing limits on the size of models that can be trained. Several recent studies have proposed training methods that are less communication intensive, avoiding the need for a highly connected compute cluster. These state-of-the-art low communication training methods still employ a synchronization step for model parameters, which, when performed over all model replicas, can become costly on a low-bandwidth network.
>   In this work, we propose a novel optimization method, NoLoCo, that does not explicitly synchronize all model parameters during training and, as a result, does not require any collective communication. NoLoCo implicitly synchronizes model weights via a novel variant of the Nesterov momentum optimizer by partially averaging model weights with a randomly selected other one. We provide both a theoretical convergence analysis for our proposed optimizer as well as empirical results from language model training.
>   We benchmark NoLoCo on a wide range of accelerator counts and model sizes, between 125M to 6.8B parameters. Our method requires significantly less communication overhead than fully sharded data parallel training or even widely used low communication training method, DiLoCo. The synchronization step itself is estimated to be one magnitude faster than the all-reduce used in DiLoCo for few hundred accelerators training over the internet. We also do not have any global blocking communication that reduces accelerator idling time. Compared to DiLoCo, we also observe up to $4\%$ faster convergence rate with wide range of model sizes and accelerator counts.

