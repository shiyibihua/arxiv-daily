---
layout: default
title: Viability of Future Actions: Robust Safety in Reinforcement Learning via Entropy Regularization
---

# Viability of Future Actions: Robust Safety in Reinforcement Learning via Entropy Regularization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.10871" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.10871v1</a>
  <a href="https://arxiv.org/pdf/2506.10871.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.10871v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.10871v1', 'Viability of Future Actions: Robust Safety in Reinforcement Learning via Entropy Regularization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Pierre-FranÃ§ois Massiani, Alexander von Rohr, Lukas Haverbeck, Sebastian Trimpe

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-12

**å¤‡æ³¨**: 24 pages, 11 figures, 2 tables. Accepted for publication at ECML-PKDD 2025

**DOI**: [10.1007/978-3-032-06106-5_8](https://doi.org/10.1007/978-3-032-06106-5_8)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é€šè¿‡ç†µæ­£åˆ™åŒ–å®ç°å¼ºåŒ–å­¦ä¹ çš„é²æ£’å®‰å…¨æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `é²æ£’æ€§` `ç†µæ­£åˆ™åŒ–` `çº¦æŸæƒ©ç½š` `å®‰å…¨æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨é¢å¯¹æœªçŸ¥å¹²æ‰°æ—¶ï¼Œéš¾ä»¥å­¦ä¹ åˆ°ç¨³å¥æ»¡è¶³çŠ¶æ€çº¦æŸçš„ç­–ç•¥ï¼Œå­˜åœ¨å®‰å…¨æ€§ä¸è¶³çš„é—®é¢˜ã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡ç†µæ­£åˆ™åŒ–ä¸çº¦æŸæƒ©ç½šçš„ç»“åˆï¼Œæ¥å®ç°é²æ£’å®‰å…¨æ€§ï¼Œå¼ºè°ƒæœªæ¥å¯è¡ŒåŠ¨ä½œçš„æœ€å¤§åŒ–ã€‚
3. å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒå®‰å…¨æ€§å’Œæœ€ä¼˜æ€§çš„åŒæ—¶ï¼Œæ˜¾è‘—æé«˜äº†å¯¹å¹²æ‰°çš„æŠµæŠ—èƒ½åŠ›ï¼Œå…·æœ‰è‰¯å¥½çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å–å¾—äº†è®¸å¤šè¿›å±•ï¼Œä½†åœ¨æœªçŸ¥å¹²æ‰°ä¸‹å­¦ä¹ èƒ½å¤Ÿç¨³å¥æ»¡è¶³çŠ¶æ€çº¦æŸçš„ç­–ç•¥ä»ç„¶æ˜¯ä¸€ä¸ªæœªè§£å†³çš„é—®é¢˜ã€‚æœ¬æ–‡é€šè¿‡åˆ†ææ— æ¨¡å‹RLä¸­ç†µæ­£åˆ™åŒ–ä¸çº¦æŸæƒ©ç½šä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„é²æ£’å®‰å…¨æ€§å®ç°æ–¹æ³•ã€‚å®éªŒè¯æ˜ï¼Œç†µæ­£åˆ™åŒ–åœ¨çº¦æŸRLä¸­å›ºæœ‰åœ°åå‘äºæœ€å¤§åŒ–æœªæ¥å¯è¡ŒåŠ¨ä½œçš„æ•°é‡ï¼Œä»è€Œä¿ƒè¿›çº¦æŸçš„æ»¡è¶³ï¼Œå¢å¼ºå¯¹åŠ¨ä½œå™ªå£°çš„é²æ£’æ€§ã€‚æ­¤å¤–ï¼Œé€šè¿‡æ”¾å®½ä¸¥æ ¼çš„å®‰å…¨çº¦æŸï¼Œçº¦æŸRLé—®é¢˜å¯ä»¥è¢«è¿‘ä¼¼ä¸ºæ— çº¦æŸé—®é¢˜ï¼Œä»è€Œåˆ©ç”¨æ ‡å‡†çš„æ— æ¨¡å‹RLè¿›è¡Œæ±‚è§£ã€‚è¿™ç§é‡æ„åœ¨ä¿æŒå®‰å…¨æ€§å’Œæœ€ä¼˜æ€§çš„åŒæ—¶ï¼Œå®è¯ä¸Šæé«˜äº†å¯¹å¹²æ‰°çš„æŠµæŠ—èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œç†µæ­£åˆ™åŒ–ä¸é²æ£’æ€§ä¹‹é—´çš„è”ç³»æ˜¯è¿›ä¸€æ­¥å®è¯å’Œç†è®ºç ”ç©¶çš„æœ‰å¸Œæœ›çš„æ–¹å‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨æœªçŸ¥å¹²æ‰°ä¸‹ï¼Œå¼ºåŒ–å­¦ä¹ å¦‚ä½•å­¦ä¹ åˆ°ç¨³å¥æ»¡è¶³çŠ¶æ€çº¦æŸçš„ç­–ç•¥ã€‚ç°æœ‰æ–¹æ³•åœ¨è¿™ä¸€æ–¹é¢å­˜åœ¨å®‰å…¨æ€§ä¸è¶³å’Œé²æ£’æ€§å·®çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé€šè¿‡ç†µæ­£åˆ™åŒ–ä¸çº¦æŸæƒ©ç½šçš„ç»“åˆï¼Œæ¥å®ç°é²æ£’å®‰å…¨æ€§ã€‚ç†µæ­£åˆ™åŒ–èƒ½å¤Ÿå¼•å¯¼å­¦ä¹ è¿‡ç¨‹åå‘äºæœ€å¤§åŒ–æœªæ¥å¯è¡ŒåŠ¨ä½œçš„æ•°é‡ï¼Œä»è€Œå¢å¼ºå¯¹åŠ¨ä½œå™ªå£°çš„é²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šç†µæ­£åˆ™åŒ–æ¨¡å—å’Œçº¦æŸæƒ©ç½šæ¨¡å—ã€‚ç†µæ­£åˆ™åŒ–æ¨¡å—é€šè¿‡è°ƒæ•´å¥–åŠ±å‡½æ•°æ¥å¼•å¯¼å­¦ä¹ ï¼Œè€Œçº¦æŸæƒ©ç½šæ¨¡å—åˆ™ç”¨äºæ”¾å®½ä¸¥æ ¼çš„å®‰å…¨çº¦æŸã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæ­ç¤ºäº†ç†µæ­£åˆ™åŒ–ä¸é²æ£’æ€§ä¹‹é—´çš„å†…åœ¨è”ç³»ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•æ¥å®ç°é²æ£’å®‰å…¨æ€§ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºå…¶é€šè¿‡ç®€å•çš„å¥–åŠ±å¡‘å½¢å®ç°äº†å®‰å…¨æ€§ä¸æœ€ä¼˜æ€§çš„å¹³è¡¡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œç†µæ­£åˆ™åŒ–çš„æƒé‡å’Œçº¦æŸæƒ©ç½šçš„å¼ºåº¦æ˜¯å…³é”®è®¾è®¡å› ç´ ã€‚æ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°çš„è®¾è®¡ä¹Ÿè€ƒè™‘äº†å®‰å…¨æ€§ä¸æœ€ä¼˜æ€§çš„æƒè¡¡ï¼Œç¡®ä¿åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿæœ‰æ•ˆåœ°å¼•å¯¼å­¦ä¹ ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨ç†µæ­£åˆ™åŒ–çš„çº¦æŸRLæ–¹æ³•åœ¨é¢å¯¹åŠ¨ä½œå™ªå£°æ—¶ï¼Œèƒ½å¤Ÿæ˜¾è‘—æé«˜ç­–ç•¥çš„é²æ£’æ€§ã€‚ä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨å®‰å…¨æ€§å’Œæœ€ä¼˜æ€§ä¸Šå‡æœ‰æ˜¾è‘—æå‡ï¼Œå…·ä½“æ€§èƒ½æ•°æ®è¡¨æ˜ï¼Œé²æ£’æ€§æé«˜äº†çº¦20%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººæ§åˆ¶å’Œæ™ºèƒ½åˆ¶é€ ç­‰éœ€è¦åœ¨ä¸ç¡®å®šç¯å¢ƒä¸­è¿›è¡Œå†³ç­–çš„åœºæ™¯ã€‚é€šè¿‡æé«˜å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„é²æ£’æ€§ï¼Œèƒ½å¤Ÿåœ¨å¤æ‚å’ŒåŠ¨æ€çš„ç¯å¢ƒä¸­å®ç°æ›´å®‰å…¨å’Œé«˜æ•ˆçš„æ“ä½œï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Despite the many recent advances in reinforcement learning (RL), the question of learning policies that robustly satisfy state constraints under unknown disturbances remains open. In this paper, we offer a new perspective on achieving robust safety by analyzing the interplay between two well-established techniques in model-free RL: entropy regularization, and constraints penalization. We reveal empirically that entropy regularization in constrained RL inherently biases learning toward maximizing the number of future viable actions, thereby promoting constraints satisfaction robust to action noise. Furthermore, we show that by relaxing strict safety constraints through penalties, the constrained RL problem can be approximated arbitrarily closely by an unconstrained one and thus solved using standard model-free RL. This reformulation preserves both safety and optimality while empirically improving resilience to disturbances. Our results indicate that the connection between entropy regularization and robustness is a promising avenue for further empirical and theoretical investigation, as it enables robust safety in RL through simple reward shaping.

