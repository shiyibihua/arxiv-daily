---
layout: default
title: Sequential-Parallel Duality in Prefix Scannable Models
---

# Sequential-Parallel Duality in Prefix Scannable Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.10918" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.10918v1</a>
  <a href="https://arxiv.org/pdf/2506.10918.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.10918v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.10918v1', 'Sequential-Parallel Duality in Prefix Scannable Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Morris Yau, Sharut Gupta, Valerie Engelmayer, Kazuki Irie, Stefanie Jegelka, Jacob Andreas

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-12

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå‰ç¼€å¯æ‰«ææ¨¡å‹ä»¥å®ç°é«˜æ•ˆåºåˆ—æ¨ç†**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `ç¥ç»åºåˆ—æ¨¡å‹` `å‰ç¼€å¯æ‰«ææ¨¡å‹` `å¹¶è¡Œæ¨ç†` `çŠ¶æ€ç©ºé—´æ¨¡å‹` `è¯­è¨€å»ºæ¨¡` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç¥ç»åºåˆ—æ¨¡å‹åœ¨å¹¶è¡Œè®­ç»ƒä¸åºåˆ—æ¨ç†æ•ˆç‡ä¹‹é—´å­˜åœ¨æƒè¡¡ï¼Œéš¾ä»¥åŒæ—¶æ»¡è¶³ä¸¤è€…çš„éœ€æ±‚ã€‚
2. æœ¬æ–‡æå‡ºå‰ç¼€å¯æ‰«ææ¨¡å‹ï¼ˆPSMsï¼‰ï¼Œé€šè¿‡æ”¾å®½èšåˆæ“ä½œç¬¦ï¼Œæ”¯æŒä»»æ„å‡½æ•°ä»¥å®ç°é«˜æ•ˆæ¨ç†ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒPSMsåœ¨è¯­è¨€å»ºæ¨¡å’Œåˆæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ¨ç†æ•ˆç‡ä¸çŠ¶æ€ç©ºé—´æ¨¡å‹ç›¸å½“ï¼Œä¸”åœ¨é•¿åº¦æ³›åŒ–ä¸Šä¼˜äºç°æœ‰æ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°ä»£ç¥ç»åºåˆ—æ¨¡å‹æ—¨åœ¨å®ç°å¯å¹¶è¡Œè®­ç»ƒå’Œå¿«é€Ÿåºåˆ—æ¨ç†çš„åŒé‡ç›®æ ‡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç±»ç§°ä¸ºå‰ç¼€å¯æ‰«ææ¨¡å‹ï¼ˆPSMsï¼‰çš„æ–°æ¨¡å‹ï¼Œé€šè¿‡æ”¾å®½çŠ¶æ€èšåˆæ“ä½œç¬¦ï¼Œå…è®¸ä»»æ„å‡½æ•°ï¼ˆå¦‚softmaxæ³¨æ„åŠ›ï¼‰ï¼Œä»è€Œç»Ÿä¸€äº†å¤šç§ç°æœ‰æ¶æ„ã€‚æˆ‘ä»¬åœ¨å°è§„æ¨¡è¯­è¨€å»ºæ¨¡å’Œåˆæˆä»»åŠ¡ä¸Šå¯¹è¿™äº›æ¨¡å‹è¿›è¡Œäº†å®è¯è¯„ä¼°ï¼Œå‘ç°PSMsåœ¨ä¿æŒè¡¨è¾¾èƒ½åŠ›çš„åŒæ—¶ï¼Œæ¨ç†æ•ˆç‡ä¸çŠ¶æ€ç©ºé—´æ¨¡å‹ç›¸åŒ¹é…ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹è¡¨ç°å‡ºæ›´å¥½çš„é•¿åº¦æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰ç¥ç»åºåˆ—æ¨¡å‹åœ¨å¹¶è¡Œè®­ç»ƒä¸å¿«é€Ÿæ¨ç†ä¹‹é—´çš„çŸ›ç›¾ï¼Œç°æœ‰æ–¹æ³•åœ¨è¿™ä¸¤æ–¹é¢çš„è¡¨ç°å¾€å¾€æ— æ³•å…¼é¡¾ï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºå‰ç¼€å¯æ‰«ææ¨¡å‹ï¼ˆPSMsï¼‰ï¼Œé€šè¿‡å®šä¹‰æ›´å¹¿æ³›çš„çŠ¶æ€èšåˆæ“ä½œç¬¦ï¼Œå…è®¸ä½¿ç”¨ä»»æ„å‡½æ•°ï¼ˆå¦‚softmaxæ³¨æ„åŠ›ï¼‰ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„å¹¶è¡Œè¯„ä¼°ä¸çº¿æ€§æ—¶é—´çš„åºåˆ—æ¨ç†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPSMsçš„æ•´ä½“æ¶æ„åŒ…æ‹¬çŠ¶æ€ç©ºé—´æ¨¡å‹çš„åŸºç¡€ï¼Œç»“åˆç»å…¸çš„å¹¶è¡Œå‰ç¼€æ‰«æç®—æ³•ï¼Œåˆ©ç”¨è‡ªå®šä¹‰çš„èšåˆæ“ä½œç¬¦è¿›è¡ŒçŠ¶æ€æ›´æ–°ã€‚æ¨¡å‹çš„è®¾è®¡å…è®¸çµæ´»çš„èšåˆæ–¹å¼ï¼Œæ”¯æŒå¤šç§ç°æœ‰æ¶æ„çš„ç»Ÿä¸€ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæ”¾å®½äº†çŠ¶æ€èšåˆæ“ä½œç¬¦çš„é™åˆ¶ï¼Œä½¿å¾—PSMsèƒ½å¤Ÿæ”¯æŒæ›´å¤æ‚çš„èšåˆå‡½æ•°ï¼Œè¿›è€Œç»Ÿä¸€äº†å¤šç§ç°æœ‰æ¨¡å‹ï¼ˆå¦‚Mambaå’ŒGLAï¼‰ï¼Œå¹¶å¼•å…¥äº†æ–°çš„æ¨¡å‹ç»“æ„ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œå…³é”®å‚æ•°è®¾ç½®åŒ…æ‹¬èšåˆæ“ä½œç¬¦çš„é€‰æ‹©ï¼ŒæŸå¤±å‡½æ•°çš„è®¾è®¡ï¼Œä»¥åŠç½‘ç»œç»“æ„çš„çµæ´»æ€§ï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸­çš„é€‚åº”æ€§å’Œæ•ˆç‡ã€‚å…·ä½“çš„å®ç°ç»†èŠ‚åŒ…æ‹¬O(1)çš„æ¯ä¸ªtokenè®¡ç®—å’Œlog(N)çš„å†…å­˜ä½¿ç”¨ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPSMsåœ¨å°è§„æ¨¡è¯­è¨€å»ºæ¨¡ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ¨ç†æ•ˆç‡ä¸çŠ¶æ€ç©ºé—´æ¨¡å‹ç›¸å½“ï¼Œä¸”åœ¨é•¿åº¦æ³›åŒ–èƒ½åŠ›ä¸Šä¼˜äºä¼ ç»Ÿçš„å˜æ¢å™¨æ¶æ„ã€‚å…·ä½“è€Œè¨€ï¼ŒPSMsåœ¨æŸäº›ä»»åŠ¡ä¸­å®ç°äº†O(1)çš„æ¯tokenè®¡ç®—ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„å®ç”¨æ€§å’Œæ•ˆç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ—¶é—´åºåˆ—åˆ†æå’Œæœºå™¨äººæ§åˆ¶ç­‰ã€‚é€šè¿‡æé«˜åºåˆ—æ¨ç†çš„æ•ˆç‡ï¼ŒPSMså¯ä»¥åœ¨å®æ—¶ç³»ç»Ÿä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„å¿«é€Ÿå“åº”èƒ½åŠ›å’Œå¤„ç†å¤æ‚ä»»åŠ¡çš„èƒ½åŠ›ã€‚æœªæ¥ï¼ŒPSMsæœ‰æœ›åœ¨å¤šæ¨¡æ€å­¦ä¹ å’Œå¤§è§„æ¨¡æ•°æ®å¤„ç†ç­‰é¢†åŸŸå±•ç°æ›´å¤§çš„ä»·å€¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Modern neural sequence models are designed to meet the dual mandate of parallelizable training and fast sequential inference. Recent developments have given rise to various models, such as Gated Linear Attention (GLA) and Mamba, that achieve such ``sequential-parallel duality.'' This raises a natural question: can we characterize the full class of neural sequence models that support near-constant-time parallel evaluation and linear-time, constant-space sequential inference? We begin by describing a broad class of such models -- state space models -- as those whose state updates can be computed using the classic parallel prefix scan algorithm with a custom associative aggregation operator. We then define a more general class, Prefix-Scannable Models (PSMs), by relaxing the state aggregation operator to allow arbitrary (potentially non-associative) functions such as softmax attention. This generalization unifies many existing architectures, including element-wise RNNs (e.g., Mamba) and linear transformers (e.g., GLA, Mamba2, mLSTM), while also introducing new models with softmax-like operators that achieve O(1) amortized compute per token and log(N) memory for sequence length N. We empirically evaluate such models on illustrative small-scale language modeling and canonical synthetic tasks, including state tracking and associative recall. Empirically, we find that PSMs retain the expressivity of transformer-based architectures while matching the inference efficiency of state space models -- in some cases exhibiting better length generalization than either.

