---
layout: default
title: MNN-LLM: A Generic Inference Engine for Fast Large Language Model Deployment on Mobile Devices
---

# MNN-LLM: A Generic Inference Engine for Fast Large Language Model Deployment on Mobile Devices

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.10443" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.10443v1</a>
  <a href="https://arxiv.org/pdf/2506.10443.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.10443v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.10443v1', 'MNN-LLM: A Generic Inference Engine for Fast Large Language Model Deployment on Mobile Devices')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhaode Wang, Jingbang Yang, Xinyu Qian, Shiwen Xing, Xiaotang Jiang, Chengfei Lv, Shengyu Zhang

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-12

**å¤‡æ³¨**: 7 pages, 5 figures. Published in the Proceedings of the 6th ACM International Conference on Multimedia in Asia Workshops (MMAsia '24 Workshops). The final authenticated version is available at https://dl.acm.org/doi/10.1145/3700410.3702126

**DOI**: [10.1145/3700410.3702126](https://doi.org/10.1145/3700410.3702126)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMNN-LLMä»¥è§£å†³ç§»åŠ¨è®¾å¤‡ä¸Šå¤§è¯­è¨€æ¨¡å‹æ¨ç†é€Ÿåº¦æ…¢çš„é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `ç§»åŠ¨è®¾å¤‡` `æ¨ç†ä¼˜åŒ–` `æ¨¡å‹é‡åŒ–` `è¾¹ç¼˜è®¡ç®—` `æ··åˆç²¾åº¦è¿ç®—` `å¤šæ ¸è´Ÿè½½å‡è¡¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§è¯­è¨€æ¨¡å‹åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šçš„æ¨ç†é€Ÿåº¦æ…¢ä¸”å†…å­˜æ¶ˆè€—å¤§ï¼Œé™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚
2. MNN-LLMæ¡†æ¶é€šè¿‡æ¨¡å‹é‡åŒ–å’Œæ··åˆå­˜å‚¨ç­‰æŠ€æœ¯ï¼Œä¼˜åŒ–äº†å¤§è¯­è¨€æ¨¡å‹åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šçš„éƒ¨ç½²ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒMNN-LLMåœ¨æ¨ç†é€Ÿåº¦ä¸Šç›¸æ¯”ä¸»æµæ¡†æ¶æå‡äº†8.6å€ï¼Œæ˜¾è‘—æé«˜äº†æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶åºå¤§çš„è§„æ¨¡å¯¼è‡´æ¨ç†æ—¶æ¶ˆè€—å¤§é‡è®¡ç®—èµ„æºï¼Œæˆæœ¬é«˜æ˜‚ã€‚å› æ­¤ï¼Œè¾¹ç¼˜è®¾å¤‡æ¨ç†æˆä¸ºä¸€ç§æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚æœ¬æ–‡æå‡ºäº†MNN-LLMæ¡†æ¶ï¼Œæ—¨åœ¨åŠ é€Ÿå¤§è¯­è¨€æ¨¡å‹åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šçš„éƒ¨ç½²ã€‚MNN-LLMé€šè¿‡æ¨¡å‹é‡åŒ–å’ŒDRAM-Flashæ··åˆå­˜å‚¨æ¥é™ä½å†…å­˜ä½¿ç”¨ï¼Œå¹¶æ ¹æ®ç§»åŠ¨CPUæŒ‡ä»¤é›†å’ŒGPUç‰¹æ€§é‡æ–°æ’åˆ—æƒé‡å’Œè¾“å…¥ï¼ŒåŒæ—¶é‡‡ç”¨å¤šæ ¸è´Ÿè½½å‡è¡¡ã€æ··åˆç²¾åº¦æµ®ç‚¹è¿ç®—å’Œå‡ ä½•è®¡ç®—ç­‰ç­–ç•¥æ¥æå‡æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒMNN-LLMç›¸æ¯”å½“å‰ä¸»æµçš„LLMä¸“ç”¨æ¡†æ¶å®ç°äº†é«˜è¾¾8.6å€çš„é€Ÿåº¦æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šæ¨ç†æ—¶çš„é€Ÿåº¦æ…¢å’Œå†…å­˜æ¶ˆè€—å¤§çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šè¡¨ç°ä¸ä½³ï¼Œå¯¼è‡´åº”ç”¨å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMNN-LLMé€šè¿‡æ¨¡å‹é‡åŒ–å’ŒDRAM-Flashæ··åˆå­˜å‚¨æ¥é™ä½å†…å­˜ä½¿ç”¨ï¼ŒåŒæ—¶ä¼˜åŒ–è®¡ç®—è¿‡ç¨‹ä»¥æé«˜æ¨ç†é€Ÿåº¦ã€‚è¯¥è®¾è®¡æ—¨åœ¨å……åˆ†åˆ©ç”¨ç§»åŠ¨è®¾å¤‡çš„ç¡¬ä»¶ç‰¹æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMNN-LLMçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ¨¡å‹é‡åŒ–æ¨¡å—ã€å­˜å‚¨ç®¡ç†æ¨¡å—å’Œæ¨ç†ä¼˜åŒ–æ¨¡å—ã€‚æ¨¡å‹é‡åŒ–æ¨¡å—è´Ÿè´£å‡å°‘æ¨¡å‹å¤§å°ï¼Œå­˜å‚¨ç®¡ç†æ¨¡å—ä¼˜åŒ–æ•°æ®å­˜å–ï¼Œæ¨ç†ä¼˜åŒ–æ¨¡å—åˆ™é€šè¿‡å¤šæ ¸è´Ÿè½½å‡è¡¡å’Œæ··åˆç²¾åº¦è¿ç®—æå‡è®¡ç®—æ•ˆç‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šMNN-LLMçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶é’ˆå¯¹ç§»åŠ¨è®¾å¤‡ç‰¹æ€§è¿›è¡Œçš„æƒé‡å’Œè¾“å…¥é‡æ’ï¼Œä»¥åŠé‡‡ç”¨æ··åˆç²¾åº¦è¿ç®—å’Œå‡ ä½•è®¡ç®—ç­–ç•¥ï¼Œè¿™äº›è®¾è®¡æ˜¾è‘—æå‡äº†æ¨ç†æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼ŒMNN-LLMé‡‡ç”¨äº†é€‚åº”ç§»åŠ¨CPUå’ŒGPUçš„æŒ‡ä»¤é›†ï¼Œä¼˜åŒ–äº†å†…å­˜è®¿é—®æ¨¡å¼ï¼Œå¹¶åœ¨å¤šæ ¸å¤„ç†ä¸Šå®ç°äº†è´Ÿè½½å‡è¡¡ï¼Œä»¥ç¡®ä¿é«˜æ•ˆçš„æ¨ç†è¿‡ç¨‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

MNN-LLMåœ¨å®éªŒä¸­å®ç°äº†é«˜è¾¾8.6å€çš„é€Ÿåº¦æå‡ï¼Œç›¸æ¯”äºå½“å‰ä¸»æµçš„LLMä¸“ç”¨æ¡†æ¶ï¼Œæ˜¾è‘—æé«˜äº†æ¨ç†æ•ˆç‡ã€‚è¿™ä¸€ç»“æœè¡¨æ˜ï¼ŒMNN-LLMåœ¨ç§»åŠ¨è®¾å¤‡ä¸Šçš„åº”ç”¨æ½œåŠ›å·¨å¤§ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè§£å†³å¤§è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ç“¶é¢ˆã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MNN-LLMæ¡†æ¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç§»åŠ¨æ™ºèƒ½è®¾å¤‡ã€è¾¹ç¼˜è®¡ç®—å’Œå®æ—¶è‡ªç„¶è¯­è¨€å¤„ç†ç­‰åœºæ™¯ã€‚å…¶é«˜æ•ˆçš„æ¨ç†èƒ½åŠ›ä½¿å¾—å¤§è¯­è¨€æ¨¡å‹èƒ½å¤Ÿåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­è¿è¡Œï¼Œæ¨åŠ¨äº†æ™ºèƒ½åŠ©æ‰‹ã€èŠå¤©æœºå™¨äººç­‰åº”ç”¨çš„å‘å±•ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have demonstrated exceptional performance across a variety of tasks. However, their substantial scale leads to significant computational resource consumption during inference, resulting in high costs. Consequently, edge device inference presents a promising solution. The primary challenges of edge inference include memory usage and inference speed. This paper introduces MNN-LLM, a framework specifically designed to accelerate the deployment of large language models on mobile devices. MNN-LLM addresses the runtime characteristics of LLMs through model quantization and DRAM-Flash hybrid storage, effectively reducing memory usage. It rearranges weights and inputs based on mobile CPU instruction sets and GPU characteristics while employing strategies such as multicore load balancing, mixed-precision floating-point operations, and geometric computations to enhance performance. Notably, MNN-LLM achieves up to a 8.6x speed increase compared to current mainstream LLM-specific frameworks.

