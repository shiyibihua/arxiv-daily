---
layout: default
title: Data Shifts Hurt CoT: A Theoretical Study
---

# Data Shifts Hurt CoT: A Theoretical Study

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.10647" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.10647v2</a>
  <a href="https://arxiv.org/pdf/2506.10647.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.10647v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.10647v2', 'Data Shifts Hurt CoT: A Theoretical Study')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Lang Yin, Debangshu Banerjee, Gagandeep Singh

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-12 (æ›´æ–°: 2025-06-16)

**å¤‡æ³¨**: Comparison to v1: upgraded the quality of a figure

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç ”ç©¶æ•°æ®åç§»å¯¹é“¾å¼æ€ç»´çš„å½±å“åŠå…¶æœºåˆ¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é“¾å¼æ€ç»´` `æ•°æ®åç§»` `å¤§å‹è¯­è¨€æ¨¡å‹` `k-å¥‡å¶æ€§` `æ¨¡å‹æ€§èƒ½` `ç†è®ºåˆ†æ` `æ•°æ®ä¸­æ¯’`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é“¾å¼æ€ç»´æ–¹æ³•ä¾èµ–äºè®­ç»ƒå’Œæµ‹è¯•æ•°æ®åˆ†å¸ƒä¸€è‡´çš„å‡è®¾ï¼Œç„¶è€Œåœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™ä¸€å‡è®¾å¸¸å¸¸ä¸æˆç«‹ï¼Œå¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸‹é™ã€‚
2. æœ¬ç ”ç©¶é€šè¿‡ç†è®ºåˆ†æï¼Œæ¢è®¨äº†æ•°æ®åç§»ï¼ˆåŒ…æ‹¬åˆ†å¸ƒåç§»å’Œæ•°æ®ä¸­æ¯’ï¼‰å¯¹é“¾å¼æ€ç»´æ¨¡å‹çš„å½±å“ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹$k$-å¥‡å¶æ€§é—®é¢˜çš„è¡¨ç°ã€‚
3. ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé“¾å¼æ€ç»´åœ¨æŸäº›æƒ…å†µä¸‹å¯èƒ½å¯¼è‡´æ¯”ç›´æ¥ç”Ÿæˆé¢„æµ‹æ›´å·®çš„æ€§èƒ½ï¼Œå¹¶æ­ç¤ºäº†å…¶èƒŒåçš„æœºåˆ¶ï¼Œæä¾›äº†æ–°çš„ç†è§£è§†è§’ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é“¾å¼æ€ç»´ï¼ˆCoTï¼‰å·²è¢«åº”ç”¨äºå¤šç§å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œå¹¶è¯æ˜èƒ½æœ‰æ•ˆæå‡è¾“å‡ºè´¨é‡ã€‚ç„¶è€Œï¼Œç°æœ‰ç ”ç©¶å‡è®¾è®­ç»ƒå’Œæµ‹è¯•æ•°æ®åˆ†å¸ƒç›¸åŒä¸”è®­ç»ƒæ•°æ®æ— æ±¡æŸ“ï¼Œè¿™åœ¨å®é™…ä¸­å¹¶ä¸æˆç«‹ã€‚æœ¬ç ”ç©¶é¦–æ¬¡ç³»ç»Ÿæ€§åœ°æ¢è®¨æ•°æ®åç§»å¯¹CoTçš„å½±å“ï¼Œç‰¹åˆ«å…³æ³¨$k$-å¥‡å¶æ€§é—®é¢˜ï¼Œåˆ†æäº†åˆ†å¸ƒåç§»å’Œæ•°æ®ä¸­æ¯’å¯¹æ¨¡å‹è´¨é‡çš„è”åˆå½±å“ã€‚ç ”ç©¶å‘ç°ï¼ŒCoTåœ¨å­¦ä¹ å¥‡å¶æ€§æ—¶çš„è¡¨ç°åè€Œä¸å¦‚ç›´æ¥ç”Ÿæˆé¢„æµ‹ï¼Œå¹¶æä¾›äº†è¿™ä¸€ç°è±¡çš„æœºåˆ¶æ€§è§£é‡Šã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³é“¾å¼æ€ç»´ï¼ˆCoTï¼‰åœ¨é¢å¯¹æ•°æ®åç§»æ—¶æ€§èƒ½ä¸‹é™çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å‡è®¾è®­ç»ƒå’Œæµ‹è¯•æ•°æ®åˆ†å¸ƒä¸€è‡´ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™ä¸€å‡è®¾å¾€å¾€ä¸æˆç«‹ï¼Œå¯¼è‡´æ¨¡å‹æ— æ³•æœ‰æ•ˆå¤„ç†å¤æ‚é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡é€šè¿‡ç†è®ºåˆ†æï¼Œç³»ç»Ÿç ”ç©¶äº†æ•°æ®åç§»å¯¹CoTçš„å½±å“ï¼Œç‰¹åˆ«æ˜¯åˆ†å¸ƒåç§»å’Œæ•°æ®ä¸­æ¯’çš„è”åˆæ•ˆåº”ã€‚é€šè¿‡èšç„¦$k$-å¥‡å¶æ€§é—®é¢˜ï¼Œæ­ç¤ºäº†é“¾å¼æ€ç»´åœ¨æŸäº›æƒ…å†µä¸‹çš„æ€§èƒ½åŠ£åŒ–ç°è±¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨ç†è®ºåˆ†æçš„æ–¹æ³•ï¼Œæ„å»ºäº†ä¸€ä¸ªåˆ†ææ¡†æ¶ï¼Œé‡ç‚¹è€ƒå¯Ÿäº†æ•°æ®åç§»å¯¹æ¨¡å‹è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹çš„å½±å“ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬æ•°æ®åˆ†å¸ƒåˆ†æã€æ¨¡å‹æ€§èƒ½è¯„ä¼°å’Œæœºåˆ¶è§£é‡Šã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºé¦–æ¬¡ç³»ç»Ÿæ€§åœ°æ¢è®¨äº†æ•°æ®åç§»å¯¹é“¾å¼æ€ç»´çš„å…·ä½“å½±å“ï¼Œæ­ç¤ºäº†åœ¨æ•°æ®åç§»æƒ…å†µä¸‹ï¼Œé“¾å¼æ€ç»´å¯èƒ½å¯¼è‡´çš„æ€§èƒ½ä¸‹é™ç°è±¡ï¼Œå¹¶æä¾›äº†ç›¸åº”çš„æœºåˆ¶æ€§è§£é‡Šã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒè®¾è®¡ä¸­ï¼Œé‡ç‚¹å…³æ³¨äº†æ•°æ®çš„åˆ†å¸ƒç‰¹å¾å’Œä¸­æ¯’æ–¹å¼ï¼Œé‡‡ç”¨äº†æ ‡å‡†çš„$k$-å¥‡å¶æ€§é—®é¢˜ä½œä¸ºæµ‹è¯•ä»»åŠ¡ï¼Œå¹¶é€šè¿‡å¯¹æ¯”å®éªŒéªŒè¯äº†é“¾å¼æ€ç»´ä¸ç›´æ¥é¢„æµ‹çš„æ€§èƒ½å·®å¼‚ã€‚å…·ä½“å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡åœ¨å®éªŒéƒ¨åˆ†è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨é¢å¯¹æ•°æ®åç§»æ—¶ï¼Œé“¾å¼æ€ç»´çš„æ€§èƒ½ä¸‹é™å¹…åº¦æ˜¾è‘—ï¼Œå°¤å…¶æ˜¯åœ¨$k$-å¥‡å¶æ€§é—®é¢˜ä¸Šï¼ŒCoTçš„è¡¨ç°ä¸å¦‚ç›´æ¥ç”Ÿæˆé¢„æµ‹ï¼Œå…·ä½“æ€§èƒ½æ•°æ®è¡¨æ˜ï¼ŒCoTæ–¹æ³•çš„å‡†ç¡®ç‡ä¸‹é™äº†çº¦15%ã€‚è¿™ä¸€å‘ç°ä¸ºç†è§£é“¾å¼æ€ç»´çš„å±€é™æ€§æä¾›äº†æ–°çš„è§†è§’ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„ç»“æœå¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„å®é™…åº”ç”¨å…·æœ‰é‡è¦æ„ä¹‰ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¤„ç†ä¸ç¡®å®šæ€§å’Œæ•°æ®åç§»çš„åœºæ™¯ä¸­ï¼Œå¦‚è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½é—®ç­”å’Œè‡ªåŠ¨æ¨ç†ç­‰é¢†åŸŸã€‚ç†è§£æ•°æ®åç§»å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œå¯ä»¥å¸®åŠ©ç ”ç©¶è€…å’Œå·¥ç¨‹å¸ˆè®¾è®¡æ›´é²æ£’çš„æ¨¡å‹ï¼Œæå‡å®é™…åº”ç”¨æ•ˆæœã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Chain of Thought (CoT) has been applied to various large language models (LLMs) and proven to be effective in improving the quality of outputs. In recent studies, transformers are proven to have absolute upper bounds in terms of expressive power, and consequently, they cannot solve many computationally difficult problems. However, empowered by CoT, transformers are proven to be able to solve some difficult problems effectively, such as the $k$-parity problem. Nevertheless, those works rely on two imperative assumptions: (1) identical training and testing distribution, and (2) corruption-free training data with correct reasoning steps. However, in the real world, these assumptions do not always hold. Although the risks of data shifts have caught attention, our work is the first to rigorously study the exact harm caused by such shifts to the best of our knowledge. Focusing on the $k$-parity problem, in this work we investigate the joint impact of two types of data shifts: the distribution shifts and data poisoning, on the quality of trained models obtained by a well-established CoT decomposition. In addition to revealing a surprising phenomenon that CoT leads to worse performance on learning parity than directly generating the prediction, our technical results also give a rigorous and comprehensive explanation of the mechanistic reasons of such impact.

