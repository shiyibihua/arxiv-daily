---
layout: default
title: Graph-MLLM: Harnessing Multimodal Large Language Models for Multimodal Graph Learning
---

# Graph-MLLM: Harnessing Multimodal Large Language Models for Multimodal Graph Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.10282" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.10282v1</a>
  <a href="https://arxiv.org/pdf/2506.10282.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.10282v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.10282v1', 'Graph-MLLM: Harnessing Multimodal Large Language Models for Multimodal Graph Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiajin Liu, Dongzhe Fan, Jiacheng Shen, Chuanhao Ji, Daochen Zha, Qiaoyu Tan

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-12

**å¤‡æ³¨**: 16 pages, 4 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGraph-MLLMä»¥è§£å†³å¤šæ¨¡æ€å›¾å­¦ä¹ çš„è¯„ä¼°ä¸æ•´åˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å›¾å­¦ä¹ ` `å¤§è¯­è¨€æ¨¡å‹` `å›¾ç¥ç»ç½‘ç»œ` `ç‰¹å¾èåˆ` `è¯„ä¼°åŸºå‡†` `ç¤¾äº¤ç½‘ç»œ` `åŒ»ç–—åº”ç”¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ¨¡æ€å›¾å­¦ä¹ æ–¹æ³•ç¼ºä¹ç»Ÿä¸€çš„åŸºå‡†ï¼Œéš¾ä»¥å…¬å¹³è¯„ä¼°ä¸åŒæ–¹æ³•çš„è¿›å±•ä¸æ•ˆæœã€‚
2. æœ¬æ–‡æå‡ºGraph-MLLMï¼Œé€šè¿‡ç³»ç»Ÿè¯„ä¼°ä¸‰ç§å¤šæ¨¡æ€å›¾å­¦ä¹ èŒƒå¼ï¼Œå¡«è¡¥äº†è¿™ä¸€ç©ºç™½ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè”åˆè§†è§‰ä¸æ–‡æœ¬å±æ€§çš„ä½¿ç”¨æ˜¾è‘—æå‡äº†å›¾å­¦ä¹ æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨ç‰¹å®šMMGsä¸Šè¿›è¡Œå¾®è°ƒæ—¶ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è¡¨ç¤ºå’Œç†è§£å¤šæ ·åŒ–æ¨¡æ€æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†é€šå¸¸ä»…å…³æ³¨æ¨¡æ€é—´çš„é…å¯¹å¯¹é½ï¼Œå¿½è§†äº†æ•°æ®ç‚¹é—´çš„ç»“æ„å…³ç³»ã€‚å°†å¤šæ¨¡æ€ä¸ç»“æ„åŒ–å›¾ä¿¡æ¯ç»“åˆå¯¹äºç¤¾äº¤ç½‘ç»œã€åŒ»ç–—ä¿å¥å’Œæ¨èç³»ç»Ÿç­‰å®é™…åº”ç”¨è‡³å…³é‡è¦ã€‚ç°æœ‰çš„å¤šæ¨¡æ€å›¾å­¦ä¹ æ–¹æ³•ä¸»è¦åˆ†ä¸ºä¸‰ç§èŒƒå¼ï¼šç¼–ç å™¨ã€å¯¹é½å™¨å’Œé¢„æµ‹å™¨ã€‚æœ¬æ–‡æå‡ºGraph-MLLMï¼Œä¸€ä¸ªå…¨é¢çš„å¤šæ¨¡æ€å›¾å­¦ä¹ åŸºå‡†ï¼Œé€šè¿‡ç³»ç»Ÿè¯„ä¼°è¿™ä¸‰ç§èŒƒå¼åœ¨å…­ä¸ªä¸åŒé¢†åŸŸçš„æ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚å®éªŒè¡¨æ˜ï¼Œè”åˆè€ƒè™‘èŠ‚ç‚¹çš„è§†è§‰å’Œæ–‡æœ¬å±æ€§æœ‰åŠ©äºå›¾å­¦ä¹ ï¼Œä¸”å°†è§†è§‰å±æ€§è½¬æ¢ä¸ºæ–‡æœ¬æè¿°èƒ½è¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚æˆ‘ä»¬å¸Œæœ›å¼€æºçš„åº“èƒ½ä¿ƒè¿›å¿«é€Ÿã€å…¬å¹³çš„è¯„ä¼°ï¼Œå¹¶æ¿€å‘è¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥åˆ›æ–°ç ”ç©¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å›¾å­¦ä¹ ä¸­ç¼ºä¹ç»Ÿä¸€è¯„ä¼°åŸºå‡†çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¦‚ä½•åˆ©ç”¨å¤šæ¨¡æ€ä¿¡æ¯å’Œå›¾ç»“æ„æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºGraph-MLLMåŸºå‡†ï¼Œé€šè¿‡ç³»ç»Ÿè¯„ä¼°ä¸‰ç§ä¸åŒçš„å¤šæ¨¡æ€å›¾å­¦ä¹ èŒƒå¼ï¼Œæ¢ç´¢å¤šæ¨¡æ€ç‰¹å¾èåˆå¯¹å›¾å­¦ä¹ çš„å½±å“ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šç¼–ç å™¨ï¼ˆMLLMä½œä¸ºç‰¹å¾èåˆå·¥å…·ï¼‰ã€å¯¹é½å™¨ï¼ˆåœ¨è¯­è¨€æˆ–éšç©ºé—´ä¸­å¯¹é½å¤šæ¨¡æ€å±æ€§ï¼‰å’Œé¢„æµ‹å™¨ï¼ˆä½œä¸ºç‹¬ç«‹æ¨ç†å™¨ï¼‰ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ä¸ªç»¼åˆæ€§çš„åŸºå‡†ï¼Œèƒ½å¤Ÿå…¬å¹³åœ°è¯„ä¼°ä¸åŒçš„å¤šæ¨¡æ€å›¾å­¦ä¹ æ–¹æ³•ï¼Œå¹¶å‘ç°äº†è§†è§‰å±æ€§è½¬æ–‡æœ¬æè¿°çš„æœ‰æ•ˆæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œä½¿ç”¨äº†é¢„è®­ç»ƒçš„æ–‡æœ¬-å›¾åƒå¯¹é½æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰ä½œä¸ºç¼–ç å™¨ï¼Œå¹¶é€šè¿‡å¾®è°ƒMLLMsæ¥ä¼˜åŒ–ç‰¹å®šMMGsçš„æ€§èƒ½ã€‚å®éªŒä¸­è¿˜æ¢ç´¢äº†ä¸åŒæŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„çš„è®¾ç½®ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè”åˆä½¿ç”¨è§†è§‰å’Œæ–‡æœ¬å±æ€§çš„èŠ‚ç‚¹åœ¨å›¾å­¦ä¹ ä¸­è¡¨ç°ä¼˜è¶Šï¼Œå°¤å…¶æ˜¯é€šè¿‡å°†è§†è§‰å±æ€§è½¬åŒ–ä¸ºæ–‡æœ¬æè¿°ï¼Œæ€§èƒ½æå‡æ˜¾è‘—ã€‚åœ¨å¤§å¤šæ•°åœºæ™¯ä¸‹ï¼Œå¾®è°ƒMLLMså¯å®ç°æœ€å…ˆè¿›çš„ç»“æœï¼Œå°½ç®¡æ²¡æœ‰æ˜¾å¼çš„å›¾ç»“æ„ä¿¡æ¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç¤¾äº¤ç½‘ç»œåˆ†æã€åŒ»ç–—æ•°æ®å¤„ç†å’Œä¸ªæ€§åŒ–æ¨èç³»ç»Ÿç­‰ã€‚é€šè¿‡æœ‰æ•ˆæ•´åˆå¤šæ¨¡æ€ä¿¡æ¯ä¸å›¾ç»“æ„ï¼Œèƒ½å¤Ÿæå‡è¿™äº›é¢†åŸŸçš„æ™ºèƒ½åŒ–æ°´å¹³å’Œå†³ç­–èƒ½åŠ›ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in representing and understanding diverse modalities. However, they typically focus on modality alignment in a pairwise manner while overlooking structural relationships across data points. Integrating multimodality with structured graph information (i.e., multimodal graphs, MMGs) is essential for real-world applications such as social networks, healthcare, and recommendation systems. Existing MMG learning methods fall into three paradigms based on how they leverage MLLMs: Encoder, Aligner, and Predictor. MLLM-as-Encoder focuses on enhancing graph neural networks (GNNs) via multimodal feature fusion; MLLM-as-Aligner aligns multimodal attributes in language or hidden space to enable LLM-based graph reasoning; MLLM-as-Predictor treats MLLMs as standalone reasoners with in-context learning or fine-tuning. Despite their advances, the MMG field lacks a unified benchmark to fairly evaluate across these approaches, making it unclear what progress has been made. To bridge this gap, we present Graph-MLLM, a comprehensive benchmark for multimodal graph learning by systematically evaluating these three paradigms across six datasets with different domains. Through extensive experiments, we observe that jointly considering the visual and textual attributes of the nodes benefits graph learning, even when using pre-trained text-to-image alignment models (e.g., CLIP) as encoders. We also find that converting visual attributes into textual descriptions further improves performance compared to directly using visual inputs. Moreover, we observe that fine-tuning MLLMs on specific MMGs can achieve state-of-the-art results in most scenarios, even without explicit graph structure information. We hope that our open-sourced library will facilitate rapid, equitable evaluation and inspire further innovative research in this field.

