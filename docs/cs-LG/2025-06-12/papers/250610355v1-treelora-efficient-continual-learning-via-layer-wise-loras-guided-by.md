---
layout: default
title: TreeLoRA: Efficient Continual Learning via Layer-Wise LoRAs Guided by a Hierarchical Gradient-Similarity Tree
---

# TreeLoRA: Efficient Continual Learning via Layer-Wise LoRAs Guided by a Hierarchical Gradient-Similarity Tree

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.10355" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.10355v1</a>
  <a href="https://arxiv.org/pdf/2506.10355.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.10355v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.10355v1', 'TreeLoRA: Efficient Continual Learning via Layer-Wise LoRAs Guided by a Hierarchical Gradient-Similarity Tree')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yu-Yang Qian, Yuan-Ze Xu, Zhen-Yu Zhang, Peng Zhao, Zhi-Hua Zhou

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-12

**å¤‡æ³¨**: ICML 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTreeLoRAä»¥è§£å†³é«˜æ•ˆæŒç»­å­¦ä¹ é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æŒç»­å­¦ä¹ ` `ä½ç§©é€‚é…å™¨` `ä»»åŠ¡ç›¸ä¼¼æ€§` `ç¨€ç–æ›´æ–°` `å¤§å‹é¢„è®­ç»ƒæ¨¡å‹` `è®¡ç®—æ•ˆç‡` `è§†è§‰å¤„ç†` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æŒç»­å­¦ä¹ æ–¹æ³•åœ¨å¤„ç†å¤§å‹é¢„è®­ç»ƒæ¨¡å‹æ—¶é¢ä¸´è®¡ç®—æ•ˆç‡ä½ä¸‹å’Œç¾éš¾æ€§é—å¿˜çš„é—®é¢˜ã€‚
2. æœ¬æ–‡æå‡ºTreeLoRAï¼Œé€šè¿‡å±‚æ¬¡æ¢¯åº¦ç›¸ä¼¼æ€§æ„å»ºä½ç§©é€‚é…å™¨ï¼Œç»“åˆå¸¦å­æŠ€æœ¯å’Œç¨€ç–æ¢¯åº¦æ›´æ–°ä»¥æé«˜æ•ˆç‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTreeLoRAåœ¨è§†è§‰å˜æ¢å™¨å’Œå¤§å‹è¯­è¨€æ¨¡å‹ä¸Šå‡è¡¨ç°å‡ºæ˜¾è‘—çš„æ•ˆç‡å’Œæ•ˆæœæå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨è®¸å¤šå®é™…åº”ç”¨ä¸­ï¼Œæ•°æ®ä»¥æµå¼ç¯å¢ƒæ”¶é›†ï¼Œå­¦ä¹ ä»»åŠ¡é¡ºåºå‡ºç°ï¼Œå› æ­¤éœ€è¦æŒç»­å­¦ä¹ ï¼ˆCLï¼‰ä»¥åœ¨çº¿æ›´æ–°æ¨¡å‹ï¼Œé€‚åº”æ–°ä»»åŠ¡å¹¶ä¿ç•™è¿‡å»çŸ¥è¯†ä»¥é˜²æ­¢ç¾éš¾æ€§é—å¿˜ã€‚æœ¬æ–‡æå‡ºTreeLoRAï¼ˆä½ç§©é€‚é…å™¨çš„K-Dæ ‘ï¼‰ï¼Œé€šè¿‡åˆ©ç”¨å±‚æ¬¡æ¢¯åº¦ç›¸ä¼¼æ€§æ„å»ºå±‚çº§é€‚é…å™¨ï¼Œä»¥å®ç°é«˜æ•ˆçš„æŒç»­å­¦ä¹ ï¼Œç‰¹åˆ«é€‚ç”¨äºå¤§å‹é¢„è®­ç»ƒæ¨¡å‹ï¼ˆLPMsï¼‰ã€‚ä¸ºå‡å°‘ä»»åŠ¡ç›¸ä¼¼æ€§ä¼°è®¡çš„è®¡ç®—è´Ÿæ‹…ï¼Œé‡‡ç”¨å¸¦å­æŠ€æœ¯å¼€å‘åŸºäºä¸‹ç½®ä¿¡ç•Œçš„ç®—æ³•ä»¥é«˜æ•ˆæ¢ç´¢ä»»åŠ¡ç»“æ„ã€‚æ­¤å¤–ï¼Œä½¿ç”¨ç¨€ç–æ¢¯åº¦æ›´æ–°ä¿ƒè¿›å‚æ•°ä¼˜åŒ–ï¼Œä½¿è¯¥æ–¹æ³•æ›´é€‚åˆLPMsã€‚ç†è®ºåˆ†ææ”¯æŒäº†æˆ‘ä»¬æ–¹æ³•çš„åˆç†æ€§ï¼Œå®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨è§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨æµå¼ç¯å¢ƒä¸­è¿›è¡ŒæŒç»­å­¦ä¹ æ—¶ï¼Œå¦‚ä½•æœ‰æ•ˆæ›´æ–°å¤§å‹é¢„è®­ç»ƒæ¨¡å‹çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†ä»»åŠ¡ç›¸ä¼¼æ€§å’Œè®¡ç®—æ•ˆç‡æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå®¹æ˜“å¯¼è‡´ç¾éš¾æ€§é—å¿˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„TreeLoRAæ–¹æ³•é€šè¿‡æ„å»ºå±‚æ¬¡æ¢¯åº¦ç›¸ä¼¼æ€§æ ‘ï¼Œåˆ©ç”¨ä½ç§©é€‚é…å™¨æ¥å®ç°é«˜æ•ˆçš„æŒç»­å­¦ä¹ ã€‚è¿™ç§è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨é€‚åº”æ–°ä»»åŠ¡çš„åŒæ—¶ï¼Œä¿ç•™å·²æœ‰çŸ¥è¯†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä»»åŠ¡ç›¸ä¼¼æ€§ä¼°è®¡æ¨¡å—ã€ä½ç§©é€‚é…å™¨æ„å»ºæ¨¡å—å’Œç¨€ç–æ¢¯åº¦æ›´æ–°æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡å¸¦å­æŠ€æœ¯é«˜æ•ˆä¼°è®¡ä»»åŠ¡ç›¸ä¼¼æ€§ï¼Œç„¶åæ„å»ºé€‚é…å™¨ï¼Œæœ€åè¿›è¡Œå‚æ•°ä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºåˆ©ç”¨å±‚æ¬¡æ¢¯åº¦ç›¸ä¼¼æ€§æ„å»ºä½ç§©é€‚é…å™¨ï¼Œå¹¶ç»“åˆå¸¦å­æŠ€æœ¯è¿›è¡Œä»»åŠ¡ç»“æ„æ¢ç´¢ã€‚è¿™ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æé«˜äº†è®¡ç®—æ•ˆç‡å’Œé€‚åº”æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†ç¨€ç–æ¢¯åº¦æ›´æ–°ç­–ç•¥ï¼Œå¹¶è®¾è®¡äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–é€‚é…å™¨çš„æ€§èƒ½ã€‚ç½‘ç»œç»“æ„ä¸Šï¼Œä½ç§©é€‚é…å™¨çš„è®¾è®¡ä½¿å¾—æ¨¡å‹åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶ï¼Œå‡å°‘äº†è®¡ç®—å¼€é”€ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒTreeLoRAåœ¨è§†è§‰å˜æ¢å™¨å’Œå¤§å‹è¯­è¨€æ¨¡å‹ä¸Šå‡æ˜¾è‘—æé«˜äº†æŒç»­å­¦ä¹ çš„æ•ˆç‡ã€‚åœ¨å¤šä¸ªä»»åŠ¡ä¸Šï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼ŒéªŒè¯äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†ç­‰å¤šä¸ªé¢†åŸŸï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å®æ—¶åœ¨çº¿å­¦ä¹ çš„åœºæ™¯ä¸­ï¼Œå¦‚è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½åŠ©æ‰‹ç­‰ã€‚é€šè¿‡æé«˜æŒç»­å­¦ä¹ çš„æ•ˆç‡ï¼ŒTreeLoRAèƒ½å¤Ÿå¸®åŠ©æ¨¡å‹æ›´å¥½åœ°é€‚åº”åŠ¨æ€å˜åŒ–çš„ç¯å¢ƒï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Many real-world applications collect data in a streaming environment, where learning tasks are encountered sequentially. This necessitates continual learning (CL) to update models online, enabling adaptation to new tasks while preserving past knowledge to prevent catastrophic forgetting. Nowadays, with the flourish of large pre-trained models (LPMs), efficiency has become increasingly critical for CL, due to their substantial computational demands and growing parameter sizes. In this paper, we introduce TreeLoRA (K-D Tree of Low-Rank Adapters), a novel approach that constructs layer-wise adapters by leveraging hierarchical gradient similarity to enable efficient CL, particularly for LPMs. To reduce the computational burden of task similarity estimation, we employ bandit techniques to develop an algorithm based on lower confidence bounds to efficiently explore the task structure. Furthermore, we use sparse gradient updates to facilitate parameter optimization, making the approach better suited for LPMs. Theoretical analysis is provided to justify the rationale behind our approach, and experiments on both vision transformers (ViTs) and large language models (LLMs) demonstrate the effectiveness and efficiency of our approach across various domains, including vision and natural language processing tasks.

