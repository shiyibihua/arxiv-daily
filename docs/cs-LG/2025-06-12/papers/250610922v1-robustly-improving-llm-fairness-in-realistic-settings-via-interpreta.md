---
layout: default
title: Robustly Improving LLM Fairness in Realistic Settings via Interpretability
---

# Robustly Improving LLM Fairness in Realistic Settings via Interpretability

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.10922" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.10922v1</a>
  <a href="https://arxiv.org/pdf/2506.10922.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.10922v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.10922v1', 'Robustly Improving LLM Fairness in Realistic Settings via Interpretability')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Adam Karvonen, Samuel Marks

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-12

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é€šè¿‡å¯è§£é‡Šæ€§æ–¹æ³•æå‡LLMåœ¨æ‹›è˜ä¸­çš„å…¬å¹³æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `åè§ç¼“è§£` `æ‹›è˜å…¬å¹³æ€§` `å¯è§£é‡Šæ€§` `å†…éƒ¨åè§è¯†åˆ«` `ä»¿å°„æ¦‚å¿µç¼–è¾‘` `æ¨¡å‹æ€§èƒ½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ååè§æ–¹æ³•åœ¨å¼•å…¥ç°å®ä¸Šä¸‹æ–‡æ—¶å¤±æ•ˆï¼Œå¯¼è‡´æ‹›è˜ä¸­çš„ç§æ—å’Œæ€§åˆ«åè§æ˜¾è‘—å¢åŠ ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡å†…éƒ¨åè§ç¼“è§£ï¼Œè¯†åˆ«å¹¶ä¸­å’Œæ¨¡å‹æ¿€æ´»ä¸­çš„æ•æ„Ÿå±æ€§æ–¹å‘ï¼Œä»¥å®ç°æ›´ç¨³å¥çš„åè§å‡å°‘ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨è¯¥æ–¹æ³•åï¼Œåè§æ°´å¹³é€šå¸¸é™è‡³1%ä»¥ä¸‹ï¼Œä¸”æ¨¡å‹æ€§èƒ½ä¿æŒè‰¯å¥½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é«˜é£é™©æ‹›è˜åº”ç”¨ä¸­è¶Šæ¥è¶Šå¤šåœ°è¢«ä½¿ç”¨ï¼Œç›´æ¥å½±å“äººä»¬çš„èŒä¸šç”Ÿæ¶¯ã€‚å°½ç®¡å…ˆå‰ç ”ç©¶è¡¨æ˜ç®€å•çš„ååè§æç¤ºå¯ä»¥æ¶ˆé™¤æ§åˆ¶è¯„ä¼°ä¸­çš„äººå£ç»Ÿè®¡åè§ï¼Œä½†æˆ‘ä»¬å‘ç°è¿™äº›ç¼“è§£æªæ–½åœ¨å¼•å…¥ç°å®ä¸Šä¸‹æ–‡ç»†èŠ‚æ—¶å¤±æ•ˆã€‚æˆ‘ä»¬é€šè¿‡å†…éƒ¨åè§ç¼“è§£æ¥è§£å†³è¿™äº›é—®é¢˜ï¼šé€šè¿‡è¯†åˆ«å’Œä¸­å’Œæ¨¡å‹æ¿€æ´»ä¸­çš„æ•æ„Ÿå±æ€§æ–¹å‘ï¼Œæˆ‘ä»¬åœ¨æ‰€æœ‰æµ‹è¯•åœºæ™¯ä¸­å®ç°äº†ç¨³å¥çš„åè§å‡å°‘ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œæ‹›è˜å®è·µè€…åº”é‡‡ç”¨æ›´ç°å®çš„è¯„ä¼°æ–¹æ³•ï¼Œå¹¶è€ƒè™‘å†…éƒ¨ç¼“è§£ç­–ç•¥ä»¥å®ç°å…¬å¹³ç»“æœã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ‹›è˜åº”ç”¨ä¸­å­˜åœ¨çš„ç§æ—å’Œæ€§åˆ«åè§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¼•å…¥ç°å®ä¸Šä¸‹æ–‡æ—¶è¡¨ç°ä¸ä½³ï¼Œå¯¼è‡´åè§æœªèƒ½æœ‰æ•ˆæ¶ˆé™¤ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å†…éƒ¨åè§ç¼“è§£æŠ€æœ¯ï¼Œè¯†åˆ«æ¨¡å‹æ¿€æ´»ä¸­çš„ç§æ—å’Œæ€§åˆ«ç›¸å…³æ–¹å‘ï¼Œå¹¶åœ¨æ¨ç†æ—¶è¿›è¡Œä»¿å°„æ¦‚å¿µç¼–è¾‘ï¼Œä»è€Œå®ç°åè§çš„æœ‰æ•ˆå‡å°‘ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œè¯†åˆ«æ•æ„Ÿå±æ€§æ–¹å‘ï¼›å…¶æ¬¡ï¼Œåœ¨æ¨ç†é˜¶æ®µåº”ç”¨ä»¿å°„ç¼–è¾‘ï¼›æœ€åï¼Œè¯„ä¼°æ¨¡å‹çš„åè§æ°´å¹³å’Œæ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºé€šè¿‡ç®€å•çš„åˆæˆæ•°æ®é›†æå–çš„æ–¹å‘èƒ½å¤Ÿåœ¨å¤šç§æ¨¡å‹ä¸­ç¨³å¥åœ°æ¨å¹¿ï¼Œæ˜¾è‘—é™ä½åè§æ°´å¹³ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæä¾›äº†æ›´ä¸ºæœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–åè§å‡å°‘ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„æ•´ä½“æ€§èƒ½ï¼Œç¡®ä¿åœ¨ä¸åŒæ¨¡å‹å’Œåœºæ™¯ä¸‹å‡èƒ½æœ‰æ•ˆåº”ç”¨ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé€šè¿‡å†…éƒ¨åè§ç¼“è§£æ–¹æ³•ï¼Œæ¨¡å‹çš„ç§æ—å’Œæ€§åˆ«åè§æ°´å¹³é€šå¸¸é™è‡³1%ä»¥ä¸‹ï¼Œä¸”å§‹ç»ˆä½äº2.5%ã€‚åœ¨å¼•å…¥ç°å®ä¸Šä¸‹æ–‡åï¼Œåè§æ˜¾è‘—é™ä½ï¼Œæ¨¡å‹åœ¨å¤šä¸ªå•†ä¸šå’Œå¼€æºæ¨¡å‹ä¸­å‡è¡¨ç°å‡ºä¸€è‡´çš„æ•ˆæœï¼Œæ˜¾ç¤ºå‡ºè¯¥æ–¹æ³•çš„å¹¿æ³›é€‚ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬é«˜é£é™©æ‹›è˜ã€è‡ªåŠ¨åŒ–äººåŠ›èµ„æºç®¡ç†å’Œå…¬å¹³æ€§è¯„ä¼°ç­‰ã€‚é€šè¿‡å¼•å…¥æ›´ç°å®çš„è¯„ä¼°æ–¹æ³•å’Œå†…éƒ¨ç¼“è§£ç­–ç•¥ï¼Œèƒ½å¤Ÿå¸®åŠ©ä¼ä¸šåœ¨æ‹›è˜è¿‡ç¨‹ä¸­å®ç°æ›´å…¬å¹³çš„ç»“æœï¼Œå‡å°‘æ½œåœ¨çš„åè§å½±å“ï¼Œå¯¹ç¤¾ä¼šå…¬å¹³å…·æœ‰é‡è¦ä»·å€¼ã€‚æœªæ¥ï¼Œè¿™ä¸€æ–¹æ³•å¯èƒ½æ‰©å±•åˆ°å…¶ä»–é¢†åŸŸï¼Œå¦‚ä¿¡è´·å®¡æ‰¹å’ŒåŒ»ç–—å†³ç­–ç­‰ï¼Œæ¨åŠ¨æ›´å¹¿æ³›çš„å…¬å¹³æ€§å®è·µã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) are increasingly deployed in high-stakes hiring applications, making decisions that directly impact people's careers and livelihoods. While prior studies suggest simple anti-bias prompts can eliminate demographic biases in controlled evaluations, we find these mitigations fail when realistic contextual details are introduced. We address these failures through internal bias mitigation: by identifying and neutralizing sensitive attribute directions within model activations, we achieve robust bias reduction across all tested scenarios. Across leading commercial (GPT-4o, Claude 4 Sonnet, Gemini 2.5 Flash) and open-source models (Gemma-2 27B, Gemma-3, Mistral-24B), we find that adding realistic context such as company names, culture descriptions from public careers pages, and selective hiring constraints (e.g.,``only accept candidates in the top 10\%") induces significant racial and gender biases (up to 12\% differences in interview rates). When these biases emerge, they consistently favor Black over White candidates and female over male candidates across all tested models and scenarios. Moreover, models can infer demographics and become biased from subtle cues like college affiliations, with these biases remaining invisible even when inspecting the model's chain-of-thought reasoning. To address these limitations, our internal bias mitigation identifies race and gender-correlated directions and applies affine concept editing at inference time. Despite using directions from a simple synthetic dataset, the intervention generalizes robustly, consistently reducing bias to very low levels (typically under 1\%, always below 2.5\%) while largely maintaining model performance. Our findings suggest that practitioners deploying LLMs for hiring should adopt more realistic evaluation methodologies and consider internal mitigation strategies for equitable outcomes.

