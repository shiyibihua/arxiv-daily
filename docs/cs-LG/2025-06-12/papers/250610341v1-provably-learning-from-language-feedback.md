---
layout: default
title: Provably Learning from Language Feedback
---

# Provably Learning from Language Feedback

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.10341" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.10341v1</a>
  <a href="https://arxiv.org/pdf/2506.10341.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.10341v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.10341v1', 'Provably Learning from Language Feedback')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Wanqiao Xu, Allen Nie, Ruijie Zheng, Aditya Modi, Adith Swaminathan, Ching-An Cheng

**åˆ†ç±»**: cs.LG, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-12

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHELiXç®—æ³•ä»¥è§£å†³è¯­è¨€åé¦ˆå­¦ä¹ é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­è¨€åé¦ˆå­¦ä¹ ` `HELiXç®—æ³•` `è½¬ç§»é€ƒé¿ç»´åº¦` `æ— æ‚”å­¦ä¹ ` `äº¤äº’å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨ä»è¯­è¨€åé¦ˆä¸­å­¦ä¹ æ—¶ç¼ºä¹ç³»ç»Ÿçš„ç†è®ºæ¡†æ¶ï¼Œéš¾ä»¥å¤„ç†æ½œåœ¨å¥–åŠ±çš„æƒ…å†µã€‚
2. è®ºæ–‡æå‡ºäº†HELiXç®—æ³•ï¼Œé€šè¿‡å¼•å…¥è½¬ç§»é€ƒé¿ç»´åº¦æ¥é‡åŒ–LLFé—®é¢˜çš„å¤æ‚æ€§ï¼Œä»è€Œå®ç°æœ‰æ•ˆå­¦ä¹ ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒHELiXåœ¨å¤šä¸ªé¢†åŸŸä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå­¦ä¹ é€Ÿåº¦æ˜¾è‘—å¿«äºä¼ ç»Ÿçš„å¥–åŠ±å­¦ä¹ æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†çš„å‡ºç°ï¼Œä»è§‚å¯Ÿå’Œè¯­è¨€åé¦ˆä¸­è¿›è¡Œäº¤äº’å­¦ä¹ çš„ç ”ç©¶é€æ¸å¢å¤šã€‚å°½ç®¡å·²æœ‰ä»¤äººå°è±¡æ·±åˆ»çš„å®è¯å±•ç¤ºï¼Œä½†å¯¹è¿™äº›å†³ç­–é—®é¢˜çš„åŸåˆ™æ€§æ¡†æ¶ä»ç„¶ç¼ºä¹ã€‚æœ¬æ–‡æ­£å¼åŒ–äº†è¯­è¨€åé¦ˆå­¦ä¹ ï¼ˆLLFï¼‰é—®é¢˜ï¼Œæå‡ºäº†è¶³å¤Ÿçš„å‡è®¾ä»¥å®ç°å°½ç®¡å­˜åœ¨æ½œåœ¨å¥–åŠ±çš„å­¦ä¹ ï¼Œå¹¶å¼•å…¥äº†â€œè½¬ç§»é€ƒé¿ç»´åº¦â€ä½œä¸ºå¤æ‚æ€§åº¦é‡ï¼Œæ¥è¡¨å¾LLFé—®é¢˜çš„éš¾åº¦ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè½¬ç§»é€ƒé¿ç»´åº¦èƒ½å¤Ÿæ•æ‰åé¦ˆä¿¡æ¯å¦‚ä½•æ”¹å˜LLFé—®é¢˜çš„å­¦ä¹ å¤æ‚æ€§ã€‚æˆ‘ä»¬å±•ç¤ºäº†ä»ä¸°å¯Œè¯­è¨€åé¦ˆä¸­å­¦ä¹ çš„é€Ÿåº¦å¯ä»¥æ¯”ä»å¥–åŠ±ä¸­å­¦ä¹ å¿«å¾—å¤šã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ— æ‚”ç®—æ³•HELiXï¼Œé€šè¿‡åºåˆ—äº¤äº’å¯è¯æ˜åœ°è§£å†³LLFé—®é¢˜ï¼Œå…¶æ€§èƒ½ä¿è¯ä¸é—®é¢˜çš„è½¬ç§»é€ƒé¿ç»´åº¦æˆæ¯”ä¾‹ã€‚é€šè¿‡å¤šä¸ªå®è¯é¢†åŸŸçš„å®éªŒï¼ŒHELiXåœ¨é‡å¤æç¤ºLLMæ—¶è¡¨ç°è‰¯å¥½ï¼Œå³ä½¿è¿™ç§æ–¹æ³•å¹¶ä¸æ€»æ˜¯å¯é ã€‚æˆ‘ä»¬çš„è´¡çŒ®æ ‡å¿—ç€æœç€è®¾è®¡åŸåˆ™æ€§äº’åŠ¨å­¦ä¹ ç®—æ³•ä»é€šç”¨è¯­è¨€åé¦ˆè¿ˆå‡ºçš„ç¬¬ä¸€æ­¥ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä»è¯­è¨€åé¦ˆä¸­å­¦ä¹ çš„å¤æ‚æ€§é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†æ½œåœ¨å¥–åŠ±æ—¶å­˜åœ¨ç†è®ºä¸è¶³ï¼Œå¯¼è‡´å­¦ä¹ æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†è½¬ç§»é€ƒé¿ç»´åº¦ä½œä¸ºå¤æ‚æ€§åº¦é‡ï¼Œå¸®åŠ©ç†è§£åé¦ˆä¿¡æ¯å¦‚ä½•å½±å“å­¦ä¹ è¿‡ç¨‹ï¼Œå¹¶è®¾è®¡äº†HELiXç®—æ³•ä»¥å®ç°é«˜æ•ˆçš„å­¦ä¹ ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šHELiXç®—æ³•é€šè¿‡åºåˆ—äº¤äº’è¿›è¡Œå­¦ä¹ ï¼Œä¸»è¦åŒ…æ‹¬åé¦ˆä¿¡æ¯çš„å¤„ç†ã€å­¦ä¹ ç­–ç•¥çš„æ›´æ–°å’Œæ€§èƒ½è¯„ä¼°ä¸‰ä¸ªæ¨¡å—ã€‚

**å…³é”®åˆ›æ–°**ï¼šè½¬ç§»é€ƒé¿ç»´åº¦çš„å¼•å…¥æ˜¯æœ¬æ–‡çš„æ ¸å¿ƒåˆ›æ–°ï¼Œå®ƒä¸ºç†è§£å’Œè§£å†³LLFé—®é¢˜æä¾›äº†æ–°çš„è§†è§’ï¼Œæ˜¾è‘—æå‡äº†å­¦ä¹ æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šHELiXç®—æ³•çš„è®¾è®¡åŒ…æ‹¬å¯¹åé¦ˆä¿¡æ¯çš„åŠ¨æ€è°ƒæ•´ã€æŸå¤±å‡½æ•°çš„ä¼˜åŒ–ä»¥åŠä¸è½¬ç§»é€ƒé¿ç»´åº¦çš„å…³è”æ€§åˆ†æï¼Œä»¥ç¡®ä¿ç®—æ³•åœ¨ä¸åŒåœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒHELiXç®—æ³•åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼Œå…¶å­¦ä¹ é€Ÿåº¦æé«˜äº†æ•°å€ï¼Œå°¤å…¶åœ¨å¤„ç†å¤æ‚åé¦ˆæ—¶ï¼Œèƒ½å¤Ÿæ˜¾è‘—å‡å°‘å­¦ä¹ æ—¶é—´ï¼Œæå‡å­¦ä¹ æ•ˆæœã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€æ•™è‚²æŠ€æœ¯å’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡æœ‰æ•ˆåˆ©ç”¨è¯­è¨€åé¦ˆï¼ŒHELiXç®—æ³•èƒ½å¤Ÿæå‡æœºå™¨å­¦ä¹ ç³»ç»Ÿçš„é€‚åº”æ€§å’Œæ™ºèƒ½åŒ–æ°´å¹³ï¼Œæœªæ¥å¯èƒ½åœ¨è‡ªåŠ¨åŒ–å†³ç­–å’Œä¸ªæ€§åŒ–æ¨èç­‰æ–¹é¢å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Interactively learning from observation and language feedback is an increasingly studied area driven by the emergence of large language model (LLM) agents. While impressive empirical demonstrations have been shown, so far a principled framing of these decision problems remains lacking. In this paper, we formalize the Learning from Language Feedback (LLF) problem, assert sufficient assumptions to enable learning despite latent rewards, and introduce $\textit{transfer eluder dimension}$ as a complexity measure to characterize the hardness of LLF problems. We show that transfer eluder dimension captures the intuition that information in the feedback changes the learning complexity of the LLF problem. We demonstrate cases where learning from rich language feedback can be exponentially faster than learning from reward. We develop a no-regret algorithm, called $\texttt{HELiX}$, that provably solves LLF problems through sequential interactions, with performance guarantees that scale with the transfer eluder dimension of the problem. Across several empirical domains, we show that $\texttt{HELiX}$ performs well even when repeatedly prompting LLMs does not work reliably. Our contributions mark a first step towards designing principled interactive learning algorithms from generic language feedback.

