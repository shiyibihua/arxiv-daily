---
layout: default
title: Reinforcement Learning Driven Generalizable Feature Representation for Cross-User Activity Recognition
---

# Reinforcement Learning Driven Generalizable Feature Representation for Cross-User Activity Recognition

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.01031" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.01031v1</a>
  <a href="https://arxiv.org/pdf/2509.01031.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.01031v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.01031v1', 'Reinforcement Learning Driven Generalizable Feature Representation for Cross-User Activity Recognition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiaozhou Ye, Kevin I-Kai Wang

**åˆ†ç±»**: cs.LG, cs.AI, cs.HC

**å‘å¸ƒæ—¥æœŸ**: 2025-08-31

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTPRL-DGæ¡†æ¶ä»¥è§£å†³è·¨ç”¨æˆ·æ´»åŠ¨è¯†åˆ«é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `äººç±»æ´»åŠ¨è¯†åˆ«` `è·¨ç”¨æˆ·æ³›åŒ–` `å¼ºåŒ–å­¦ä¹ ` `è‡ªå›å½’ç”Ÿæˆ` `æ—¶é—´åºåˆ—åˆ†æ` `åŒ»ç–—ä¿å¥` `æ™ºèƒ½ç¯å¢ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„HARæ–¹æ³•åœ¨é¢å¯¹ä¸åŒç”¨æˆ·çš„å¤šæ ·æ€§æ—¶è¡¨ç°ä¸ä½³ï¼Œå®¹æ˜“è¿‡æ‹Ÿåˆäºç‰¹å®šç”¨æˆ·çš„æ¨¡å¼ï¼Œå¯¼è‡´åœ¨æœªè§ç”¨æˆ·ä¸Šçš„æ€§èƒ½ä¸‹é™ã€‚
2. æœ¬æ–‡æå‡ºTPRL-DGæ¡†æ¶ï¼Œå°†ç‰¹å¾æå–è§†ä¸ºå¼ºåŒ–å­¦ä¹ é©±åŠ¨çš„åºåˆ—å†³ç­–è¿‡ç¨‹ï¼Œåˆ©ç”¨è‡ªå›å½’ç”Ÿæˆå™¨ç”Ÿæˆæ—¶é—´æ ‡è®°ï¼Œä¼˜åŒ–ç”¨æˆ·ä¸å˜çš„æ´»åŠ¨åŠ¨æ€ã€‚
3. åœ¨DSADSå’ŒPAMAP2æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTPRL-DGåœ¨è·¨ç”¨æˆ·æ³›åŒ–æ–¹é¢è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œä¸”æ— éœ€é’ˆå¯¹æ¯ä¸ªç”¨æˆ·çš„æ ¡å‡†ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

äººç±»æ´»åŠ¨è¯†åˆ«ï¼ˆHARï¼‰åœ¨åŒ»ç–—ä¿å¥ã€å¥èº«è¿½è¸ªå’Œæ™ºèƒ½ç¯å¢ƒä¸­è‡³å…³é‡è¦ï¼Œä½†ç”±äºç”¨æˆ·é—´çš„å¤šæ ·æ€§ï¼Œä¼ ç»Ÿçš„ç›‘ç£å­¦ä¹ æ–¹æ³•å¸¸å¸¸æ— æ³•æœ‰æ•ˆæ³›åŒ–ã€‚ç°æœ‰çš„é¢†åŸŸæ³›åŒ–æ–¹æ³•å¾€å¾€å¿½è§†æ—¶é—´ä¾èµ–æ€§æˆ–ä¾èµ–äºä¸åˆ‡å®é™…çš„é¢†åŸŸç‰¹å®šæ ‡ç­¾ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶â€”â€”æ—¶é—´ä¿æŒå¼ºåŒ–å­¦ä¹ é¢†åŸŸæ³›åŒ–ï¼ˆTPRL-DGï¼‰ï¼Œå°†ç‰¹å¾æå–é‡æ–°å®šä¹‰ä¸ºç”±å¼ºåŒ–å­¦ä¹ é©±åŠ¨çš„åºåˆ—å†³ç­–è¿‡ç¨‹ã€‚TPRL-DGåˆ©ç”¨åŸºäºTransformerçš„è‡ªå›å½’ç”Ÿæˆå™¨ç”Ÿæˆæ•æ‰ç”¨æˆ·ä¸å˜æ´»åŠ¨åŠ¨æ€çš„æ—¶é—´æ ‡è®°ï¼Œé€šè¿‡å¤šç›®æ ‡å¥–åŠ±å‡½æ•°ä¼˜åŒ–ï¼Œå¹³è¡¡ç±»åˆ«åŒºåˆ†å’Œè·¨ç”¨æˆ·ä¸å˜æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTPRL-DGåœ¨DSADSå’ŒPAMAP2æ•°æ®é›†ä¸Šè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†è·¨ç”¨æˆ·çš„è¯†åˆ«å‡†ç¡®æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è·¨ç”¨æˆ·æ´»åŠ¨è¯†åˆ«ä¸­çš„æ³›åŒ–é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€å› ç”¨æˆ·é—´çš„è¿åŠ¨æ¨¡å¼ã€ä¼ æ„Ÿå™¨ä½ç½®å’Œç”Ÿç†ç‰¹å¾çš„å·®å¼‚è€Œè¡¨ç°ä¸ä½³ï¼Œå¯¼è‡´åœ¨æœªè§ç”¨æˆ·ä¸Šçš„è¯†åˆ«å‡†ç¡®æ€§ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šTPRL-DGæ¡†æ¶é€šè¿‡å°†ç‰¹å¾æå–è§†ä¸ºä¸€ä¸ªåºåˆ—å†³ç­–è¿‡ç¨‹ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ æ¥ä¼˜åŒ–ç‰¹å¾çš„æå–ï¼Œæ—¨åœ¨æ•æ‰ç”¨æˆ·ä¸å˜çš„æ´»åŠ¨åŠ¨æ€ï¼Œä»è€Œæé«˜è·¨ç”¨æˆ·çš„æ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä¸‰ä¸ªæ¨¡å—ï¼šè‡ªå›å½’ç”Ÿæˆå™¨ç”¨äºç”Ÿæˆæ—¶é—´æ ‡è®°ï¼Œå¼ºåŒ–å­¦ä¹ æ¨¡å—ç”¨äºä¼˜åŒ–ç‰¹å¾æå–è¿‡ç¨‹ï¼Œä»¥åŠå¤šç›®æ ‡å¥–åŠ±å‡½æ•°ç”¨äºå¹³è¡¡ç±»åˆ«åŒºåˆ†å’Œè·¨ç”¨æˆ·ä¸å˜æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šTPRL-DGçš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥äº†å¼ºåŒ–å­¦ä¹ é©±åŠ¨çš„é¢†åŸŸæ³›åŒ–æ–¹æ³•ï¼Œé‡‡ç”¨è‡ªå›å½’çš„æ—¶é—´æ ‡è®°ç”Ÿæˆæ–¹å¼ï¼Œä¸”è®¾è®¡äº†æ— æ ‡ç­¾çš„å¥–åŠ±æœºåˆ¶ï¼Œæ¶ˆé™¤äº†å¯¹ç›®æ ‡ç”¨æˆ·æ³¨é‡Šçš„éœ€æ±‚ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†åŸºäºTransformerçš„ç”Ÿæˆå™¨ï¼Œè®¾ç½®äº†å¤šç›®æ ‡å¥–åŠ±å‡½æ•°ä»¥ä¼˜åŒ–ç‰¹å¾æå–ï¼Œå¹¶ç¡®ä¿æ—¶é—´ä¸€è‡´æ€§ï¼Œå…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨å®éªŒä¸­è¿›è¡Œäº†éªŒè¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨DSADSå’ŒPAMAP2æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTPRL-DGåœ¨è·¨ç”¨æˆ·è¯†åˆ«å‡†ç¡®æ€§ä¸Šè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°XX%ï¼Œä¸”æ— éœ€è¿›è¡Œæ¯ä¸ªç”¨æˆ·çš„æ ¡å‡†ï¼Œæ˜¾ç¤ºå‡ºå…¶ä¼˜è¶Šçš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ä¸ªæ€§åŒ–åŒ»ç–—ã€é€‚åº”æ€§å¥èº«è¿½è¸ªå’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥ç¯å¢ƒã€‚é€šè¿‡å­¦ä¹ ç”¨æˆ·ä¸å˜çš„æ—¶é—´æ¨¡å¼ï¼ŒTPRL-DGèƒ½å¤Ÿä¿ƒè¿›HARç³»ç»Ÿçš„å¯æ‰©å±•æ€§ï¼Œä¸ºæœªæ¥çš„æ™ºèƒ½å¥åº·ç›‘æµ‹å’Œä¸ªæ€§åŒ–æœåŠ¡æä¾›æ”¯æŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Human Activity Recognition (HAR) using wearable sensors is crucial for healthcare, fitness tracking, and smart environments, yet cross-user variability -- stemming from diverse motion patterns, sensor placements, and physiological traits -- hampers generalization in real-world settings. Conventional supervised learning methods often overfit to user-specific patterns, leading to poor performance on unseen users. Existing domain generalization approaches, while promising, frequently overlook temporal dependencies or depend on impractical domain-specific labels. We propose Temporal-Preserving Reinforcement Learning Domain Generalization (TPRL-DG), a novel framework that redefines feature extraction as a sequential decision-making process driven by reinforcement learning. TPRL-DG leverages a Transformer-based autoregressive generator to produce temporal tokens that capture user-invariant activity dynamics, optimized via a multi-objective reward function balancing class discrimination and cross-user invariance. Key innovations include: (1) an RL-driven approach for domain generalization, (2) autoregressive tokenization to preserve temporal coherence, and (3) a label-free reward design eliminating the need for target user annotations. Evaluations on the DSADS and PAMAP2 datasets show that TPRL-DG surpasses state-of-the-art methods in cross-user generalization, achieving superior accuracy without per-user calibration. By learning robust, user-invariant temporal patterns, TPRL-DG enables scalable HAR systems, facilitating advancements in personalized healthcare, adaptive fitness tracking, and context-aware environments.

