---
layout: default
title: Scale, Don't Fine-tune: Guiding Multimodal LLMs for Efficient Visual Place Recognition at Test-Time
---

# Scale, Don't Fine-tune: Guiding Multimodal LLMs for Efficient Visual Place Recognition at Test-Time

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.02129" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.02129v1</a>
  <a href="https://arxiv.org/pdf/2509.02129.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.02129v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.02129v1', 'Scale, Don\'t Fine-tune: Guiding Multimodal LLMs for Efficient Visual Place Recognition at Test-Time')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jintao Cheng, Weibin Li, Jiehao Luo, Xiaoyu Tang, Zhijian He, Jin Wu, Yao Zou, Wei Zhang

**åˆ†ç±»**: cs.LG, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-02

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæµ‹è¯•æ—¶ç¼©æ”¾æ¡†æ¶ï¼Œå¼•å¯¼å¤šæ¨¡æ€LLMå®ç°é«˜æ•ˆè§†è§‰å®šä½ï¼Œæ— éœ€å¾®è°ƒã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰å®šä½` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `æµ‹è¯•æ—¶ç¼©æ”¾` `é›¶æ ·æœ¬å­¦ä¹ ` `è·¨é¢†åŸŸæ³›åŒ–` `æç¤ºå·¥ç¨‹` `ä¸ç¡®å®šæ€§æ„ŸçŸ¥` `æœºå™¨äººå¯¼èˆª`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VPRæ–¹æ³•è®¡ç®—å¼€é”€å¤§ï¼Œå¾®è°ƒåè·¨é¢†åŸŸè¿ç§»èƒ½åŠ›æœ‰é™ï¼Œéš¾ä»¥é€‚åº”æ–°ç¯å¢ƒã€‚
2. æå‡ºæµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰æ¡†æ¶ï¼Œåˆ©ç”¨MLLMçš„è§†è§‰-è¯­è¨€å¯¹é½èƒ½åŠ›ï¼Œå®ç°é›¶æ ·æœ¬VPRã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è·¨é¢†åŸŸVPRä»»åŠ¡ä¸Šæ€§èƒ½æ˜¾è‘—æå‡ï¼Œè®¡ç®—æ•ˆç‡æé«˜é«˜è¾¾210å€ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰å®šä½ï¼ˆVPRï¼‰å·²ç»ä»æ‰‹å·¥è®¾è®¡çš„æè¿°ç¬¦å‘å±•åˆ°æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œä½†ä»ç„¶å­˜åœ¨æ˜¾è‘—çš„æŒ‘æˆ˜ã€‚ç›®å‰çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMsï¼‰å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ï¼Œå¢å¼ºäº†è¯­ä¹‰ç†è§£ï¼Œä½†å­˜åœ¨è®¡ç®—å¼€é”€é«˜ä»¥åŠå¾®è°ƒæ—¶è·¨é¢†åŸŸè¿ç§»èƒ½åŠ›æœ‰é™çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„é›¶æ ·æœ¬æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰ï¼Œåˆ©ç”¨MLLMsçš„è§†è§‰-è¯­è¨€å¯¹é½èƒ½åŠ›ï¼Œé€šè¿‡åŸºäºæŒ‡å¯¼çš„æ–¹æ³•è¿›è¡Œç›´æ¥ç›¸ä¼¼æ€§è¯„åˆ†ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡é‡‡ç”¨ç”Ÿæˆé•¿åº¦å¯æ§JSONè¾“å‡ºçš„ç»“æ„åŒ–æç¤ºï¼Œæ¶ˆé™¤äº†ä¸¤é˜¶æ®µå¤„ç†ã€‚å…·æœ‰ä¸ç¡®å®šæ€§æ„ŸçŸ¥è‡ªæ´½æ€§ï¼ˆUASCï¼‰çš„TTSæ¡†æ¶èƒ½å¤Ÿåœ¨æ²¡æœ‰é¢å¤–è®­ç»ƒæˆæœ¬çš„æƒ…å†µä¸‹å®ç°å®æ—¶é€‚åº”ï¼Œä»è€Œåœ¨ä¸åŒçš„ç¯å¢ƒä¸­å®ç°å“è¶Šçš„æ³›åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè·¨é¢†åŸŸVPRæ€§èƒ½å¾—åˆ°äº†æ˜¾è‘—æé«˜ï¼Œè®¡ç®—æ•ˆç‡æé«˜äº†é«˜è¾¾210å€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè§†è§‰å®šä½ï¼ˆVPRï¼‰æ—¨åœ¨ç¡®å®šæŸ¥è¯¢å›¾åƒåœ¨å·²çŸ¥ç¯å¢ƒä¸­çš„ä½ç½®ã€‚ç°æœ‰åŸºäºæ·±åº¦å­¦ä¹ çš„VPRæ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åŸºäºè§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMsï¼‰å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ–¹æ³•ï¼Œè™½ç„¶æå‡äº†è¯­ä¹‰ç†è§£èƒ½åŠ›ï¼Œä½†é€šå¸¸éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºï¼Œå¹¶ä¸”åœ¨è·¨é¢†åŸŸåº”ç”¨æ—¶éœ€è¦è¿›è¡Œå¾®è°ƒï¼Œè¿™é™åˆ¶äº†å®ƒä»¬çš„æ³›åŒ–èƒ½åŠ›å’Œæ•ˆç‡ã€‚å› æ­¤ï¼Œå¦‚ä½•åœ¨ä¸è¿›è¡Œå¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œåˆ©ç”¨MLLMçš„å¼ºå¤§èƒ½åŠ›å®ç°é«˜æ•ˆä¸”æ³›åŒ–çš„VPRæ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è§†è§‰-è¯­è¨€å¯¹é½èƒ½åŠ›ï¼Œé€šè¿‡æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰æ¡†æ¶ï¼Œç›´æ¥å¯¹æŸ¥è¯¢å›¾åƒå’Œå‚è€ƒå›¾åƒè¿›è¡Œç›¸ä¼¼æ€§è¯„åˆ†ï¼Œè€Œæ— éœ€è¿›è¡Œä¼ ç»Ÿçš„ç‰¹å¾æå–å’ŒåŒ¹é…ã€‚é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æç¤ºï¼ˆPromptï¼‰ï¼Œå¼•å¯¼MLLMç”Ÿæˆç»“æ„åŒ–çš„JSONè¾“å‡ºï¼Œä»è€Œå®ç°å¯æ§çš„ç›¸ä¼¼åº¦è®¡ç®—ã€‚è¿™ç§æ–¹æ³•é¿å…äº†å¯¹MLLMè¿›è¡Œå¾®è°ƒï¼Œé™ä½äº†è®¡ç®—æˆæœ¬ï¼Œå¹¶æé«˜äº†è·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) **å›¾åƒç¼–ç **ï¼šä½¿ç”¨è§†è§‰ç¼–ç å™¨ï¼ˆä¾‹å¦‚CLIPï¼‰æå–æŸ¥è¯¢å›¾åƒå’Œå‚è€ƒå›¾åƒçš„è§†è§‰ç‰¹å¾ã€‚2) **æç¤ºå·¥ç¨‹**ï¼šè®¾è®¡ç»“æ„åŒ–çš„æç¤ºï¼Œå¼•å¯¼MLLMç”ŸæˆåŒ…å«ç›¸ä¼¼åº¦è¯„åˆ†çš„JSONæ ¼å¼è¾“å‡ºã€‚æç¤ºä¸­åŒ…å«å›¾åƒçš„æè¿°ä¿¡æ¯ï¼Œä»¥åŠè¦æ±‚MLLMå¯¹å›¾åƒå¯¹è¿›è¡Œç›¸ä¼¼åº¦è¯„ä¼°çš„æŒ‡ä»¤ã€‚3) **MLLMæ¨ç†**ï¼šå°†å›¾åƒç‰¹å¾å’Œæç¤ºè¾“å…¥MLLMï¼ŒMLLMæ ¹æ®æç¤ºç”ŸæˆJSONæ ¼å¼çš„ç›¸ä¼¼åº¦è¯„åˆ†ã€‚4) **æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰**ï¼šé€šè¿‡è°ƒæ•´æç¤ºä¸­çš„å‚æ•°ï¼Œä¾‹å¦‚ç¼©æ”¾å› å­ï¼Œæ¥ä¼˜åŒ–ç›¸ä¼¼åº¦è¯„åˆ†ï¼Œæé«˜VPRçš„å‡†ç¡®æ€§ã€‚5) **ä¸ç¡®å®šæ€§æ„ŸçŸ¥è‡ªæ´½æ€§ï¼ˆUASCï¼‰**ï¼šåˆ©ç”¨å¤šæ¬¡æ¨ç†ç»“æœçš„ä¸ç¡®å®šæ€§æ¥è¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰æ¡†æ¶ï¼Œæ— éœ€å¾®è°ƒå³å¯åˆ©ç”¨MLLMè¿›è¡ŒVPRã€‚2) è®¾è®¡äº†ç»“æ„åŒ–çš„æç¤ºï¼Œå¼•å¯¼MLLMç”Ÿæˆå¯æ§çš„JSONè¾“å‡ºï¼Œä»è€Œå®ç°ç›´æ¥çš„ç›¸ä¼¼åº¦è¯„åˆ†ã€‚3) å¼•å…¥äº†ä¸ç¡®å®šæ€§æ„ŸçŸ¥è‡ªæ´½æ€§ï¼ˆUASCï¼‰æœºåˆ¶ï¼Œè¿›ä¸€æ­¥æå‡äº†VPRçš„é²æ£’æ€§ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•é¿å…äº†å¯¹MLLMè¿›è¡Œå¾®è°ƒï¼Œé™ä½äº†è®¡ç®—æˆæœ¬ï¼Œå¹¶æé«˜äº†è·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼š1) **æç¤ºè®¾è®¡**ï¼šæç¤ºåŒ…å«å›¾åƒçš„æè¿°ä¿¡æ¯ï¼ˆä¾‹å¦‚åœºæ™¯ç±»å‹ã€ç‰©ä½“ç­‰ï¼‰ï¼Œä»¥åŠè¦æ±‚MLLMå¯¹å›¾åƒå¯¹è¿›è¡Œç›¸ä¼¼åº¦è¯„ä¼°çš„æŒ‡ä»¤ã€‚æç¤ºçš„ç»“æ„å’Œå†…å®¹å¯¹MLLMçš„è¾“å‡ºè´¨é‡æœ‰é‡è¦å½±å“ã€‚2) **ç¼©æ”¾å› å­**ï¼šTTSæ¡†æ¶é€šè¿‡è°ƒæ•´æç¤ºä¸­çš„ç¼©æ”¾å› å­æ¥ä¼˜åŒ–ç›¸ä¼¼åº¦è¯„åˆ†ã€‚ç¼©æ”¾å› å­çš„é€‰æ‹©éœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡è¿›è¡Œè°ƒæ•´ã€‚3) **ä¸ç¡®å®šæ€§åº¦é‡**ï¼šUASCæœºåˆ¶åˆ©ç”¨å¤šæ¬¡æ¨ç†ç»“æœçš„æ–¹å·®æ¥è¡¡é‡ä¸ç¡®å®šæ€§ã€‚ä¸ç¡®å®šæ€§é«˜çš„ç»“æœä¼šè¢«èµ‹äºˆè¾ƒä½çš„æƒé‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªè·¨é¢†åŸŸVPRæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸäº›æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•çš„å‡†ç¡®ç‡è¶…è¿‡äº†ç°æœ‰æ–¹æ³•ï¼Œå¹¶ä¸”è®¡ç®—æ•ˆç‡æé«˜äº†é«˜è¾¾210å€ã€‚æ­¤å¤–ï¼Œä¸ç¡®å®šæ€§æ„ŸçŸ¥è‡ªæ´½æ€§ï¼ˆUASCï¼‰æœºåˆ¶è¿›ä¸€æ­¥æå‡äº†VPRçš„é²æ£’æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ã€å¢å¼ºç°å®ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œæœºå™¨äººå¯ä»¥åœ¨æœªçŸ¥ç¯å¢ƒä¸­åˆ©ç”¨è¯¥æ–¹æ³•è¿›è¡Œå®šä½å’Œå¯¼èˆªï¼Œè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿå¯ä»¥åˆ©ç”¨è¯¥æ–¹æ³•è¯†åˆ«åœ°æ ‡å»ºç­‘ï¼Œå¢å¼ºç°å®åº”ç”¨å¯ä»¥åˆ©ç”¨è¯¥æ–¹æ³•å°†è™šæ‹Ÿç‰©ä½“ä¸çœŸå®åœºæ™¯è¿›è¡Œå¯¹é½ã€‚è¯¥æ–¹æ³•æ— éœ€å¾®è°ƒçš„ç‰¹æ€§ä½¿å…¶èƒ½å¤Ÿå¿«é€Ÿéƒ¨ç½²åˆ°æ–°çš„ç¯å¢ƒä¸­ï¼Œå…·æœ‰å¾ˆé«˜çš„å®ç”¨ä»·å€¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Visual Place Recognition (VPR) has evolved from handcrafted descriptors to deep learning approaches, yet significant challenges remain. Current approaches, including Vision Foundation Models (VFMs) and Multimodal Large Language Models (MLLMs), enhance semantic understanding but suffer from high computational overhead and limited cross-domain transferability when fine-tuned. To address these limitations, we propose a novel zero-shot framework employing Test-Time Scaling (TTS) that leverages MLLMs' vision-language alignment capabilities through Guidance-based methods for direct similarity scoring. Our approach eliminates two-stage processing by employing structured prompts that generate length-controllable JSON outputs. The TTS framework with Uncertainty-Aware Self-Consistency (UASC) enables real-time adaptation without additional training costs, achieving superior generalization across diverse environments. Experimental results demonstrate significant improvements in cross-domain VPR performance with up to 210$\times$ computational efficiency gains.

