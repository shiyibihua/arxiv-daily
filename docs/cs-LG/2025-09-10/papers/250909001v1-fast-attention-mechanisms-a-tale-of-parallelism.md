---
layout: default
title: Fast attention mechanisms: a tale of parallelism
---

# Fast attention mechanisms: a tale of parallelism

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.09001" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.09001v1</a>
  <a href="https://arxiv.org/pdf/2509.09001.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.09001v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.09001v1', 'Fast attention mechanisms: a tale of parallelism')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jingwen Liu, Hantao Yu, Clayton Sanford, Alexandr Andoni, Daniel Hsu

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-10

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºANNAæ³¨æ„åŠ›æœºåˆ¶ï¼ŒåŠ é€ŸTransformerå¹¶ä¿æŒå¤§è§„æ¨¡å¹¶è¡Œè®¡ç®—èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `æ³¨æ„åŠ›æœºåˆ¶` `Transformer` `è¿‘ä¼¼æœ€è¿‘é‚»æœç´¢` `å¤§è§„æ¨¡å¹¶è¡Œè®¡ç®—` `æ¨¡å‹åŠ é€Ÿ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. Transformerçš„äºŒæ¬¡å¤æ‚åº¦é™åˆ¶äº†å…¶åœ¨å¤§è§„æ¨¡ä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œæˆä¸ºä¸€ä¸ªäºŸå¾…è§£å†³çš„é—®é¢˜ã€‚
2. è®ºæ–‡æå‡ºè¿‘ä¼¼æœ€è¿‘é‚»æ³¨æ„åŠ›ï¼ˆANNAï¼‰æœºåˆ¶ï¼Œæ—¨åœ¨é™ä½è®¡ç®—å¤æ‚åº¦ï¼ŒåŒæ—¶ä¿æŒTransformerçš„è¡¨è¾¾èƒ½åŠ›ã€‚
3. ç†è®ºè¯æ˜ANNA-Transformeråœ¨MPCç®—æ³•æ¨¡æ‹Ÿå’Œå…³é”®æ¨ç†ä»»åŠ¡ä¸Šå…·æœ‰ä¼˜è¶Šæ€§èƒ½ï¼Œå¹¶èƒ½æ¨¡æ‹Ÿä½ç§©Transformerã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

Transformeræ¨¡å‹å…·æœ‰æ¨¡æ‹Ÿå¤§è§„æ¨¡å¹¶è¡Œè®¡ç®—ï¼ˆMPCï¼‰ç®—æ³•çš„è¡¨å¾èƒ½åŠ›ï¼Œä½†å…¶äºŒæ¬¡æ—¶é—´å¤æ‚åº¦ä¸¥é‡é™åˆ¶äº†å…¶å¯æ‰©å±•æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œç§°ä¸ºè¿‘ä¼¼æœ€è¿‘é‚»æ³¨æ„åŠ›ï¼ˆANNAï¼‰ï¼Œå®ƒå…·æœ‰äºšäºŒæ¬¡æ—¶é—´å¤æ‚åº¦ã€‚æˆ‘ä»¬è¯æ˜äº†ANNA-Transformerï¼šï¼ˆ1ï¼‰åœ¨åŒ¹é…MPCç®—æ³•èƒ½åŠ›æ–¹é¢ï¼Œä¿ç•™äº†å…ˆå‰ä¸ºæ ‡å‡†æ³¨æ„åŠ›æœºåˆ¶å»ºç«‹çš„è¡¨è¾¾èƒ½åŠ›ï¼›ï¼ˆ2ï¼‰èƒ½å¤Ÿä»¥æ¥è¿‘æœ€ä¼˜çš„æ·±åº¦è§£å†³è¯¸å¦‚Match2å’Œk-hopç­‰å…³é”®æ¨ç†ä»»åŠ¡ã€‚åˆ©ç”¨MPCæ¡†æ¶ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥è¯æ˜äº†å¸¸æ•°æ·±åº¦çš„ANNA-Transformerå¯ä»¥æ¨¡æ‹Ÿå¸¸æ•°æ·±åº¦çš„ä½ç§©Transformerï¼Œä»è€Œä¸ºæ¨ç†ä¸€ç±»å¹¿æ³›çš„é«˜æ•ˆæ³¨æ„åŠ›è¿‘ä¼¼æ–¹æ³•æä¾›äº†ä¸€ç§ç»Ÿä¸€çš„æ–¹å¼ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šTransformerä¸­çš„æ ‡å‡†æ³¨æ„åŠ›æœºåˆ¶å…·æœ‰äºŒæ¬¡æ—¶é—´å¤æ‚åº¦ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨å¤„ç†é•¿åºåˆ—æ—¶çš„å¯æ‰©å±•æ€§ã€‚ç°æœ‰çš„é«˜æ•ˆæ³¨æ„åŠ›æœºåˆ¶å¾€å¾€éš¾ä»¥ä¿è¯è¡¨è¾¾èƒ½åŠ›ï¼Œæˆ–è€…ç¼ºä¹ç»Ÿä¸€çš„ç†è®ºæ¡†æ¶æ¥åˆ†æå…¶æ€§èƒ½ã€‚å› æ­¤ï¼Œå¦‚ä½•è®¾è®¡ä¸€ç§æ—¢é«˜æ•ˆåˆå…·æœ‰è¶³å¤Ÿè¡¨è¾¾èƒ½åŠ›çš„æ³¨æ„åŠ›æœºåˆ¶æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨è¿‘ä¼¼æœ€è¿‘é‚»æœç´¢ï¼ˆApproximate Nearest Neighbor Search, ANNSï¼‰æ¥è¿‘ä¼¼è®¡ç®—æ³¨æ„åŠ›æƒé‡ã€‚é€šè¿‡åªå…³æ³¨ä¸æŸ¥è¯¢å‘é‡æœ€ç›¸å…³çš„é”®å‘é‡ï¼Œå¯ä»¥æ˜¾è‘—å‡å°‘è®¡ç®—é‡ï¼Œä»è€Œé™ä½æ—¶é—´å¤æ‚åº¦ã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨åœ¨è®¡ç®—æ•ˆç‡å’Œæ¨¡å‹è¡¨è¾¾èƒ½åŠ›ä¹‹é—´å–å¾—å¹³è¡¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šANNA-Transformerçš„æ•´ä½“æ¶æ„ä¸æ ‡å‡†Transformerç±»ä¼¼ï¼Œä¸»è¦åŒºåˆ«åœ¨äºæ³¨æ„åŠ›æœºåˆ¶çš„å®ç°ã€‚åœ¨ANNAä¸­ï¼Œå¯¹äºæ¯ä¸ªæŸ¥è¯¢å‘é‡ï¼Œé¦–å…ˆä½¿ç”¨ANNSç®—æ³•æ‰¾åˆ°å…¶è¿‘ä¼¼æœ€è¿‘é‚»çš„é”®å‘é‡é›†åˆã€‚ç„¶åï¼Œåªåœ¨è¿™ä¸ªé›†åˆä¸Šè®¡ç®—æ³¨æ„åŠ›æƒé‡ï¼Œå¹¶è¿›è¡ŒåŠ æƒæ±‚å’Œã€‚æ•´ä¸ªè¿‡ç¨‹å¯ä»¥å¹¶è¡ŒåŒ–ï¼Œä»è€Œè¿›ä¸€æ­¥æé«˜è®¡ç®—æ•ˆç‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šANNAçš„å…³é”®åˆ›æ–°åœ¨äºå°†è¿‘ä¼¼æœ€è¿‘é‚»æœç´¢å¼•å…¥åˆ°æ³¨æ„åŠ›æœºåˆ¶ä¸­ã€‚ä¸æ ‡å‡†æ³¨æ„åŠ›æœºåˆ¶éœ€è¦è®¡ç®—æ‰€æœ‰æŸ¥è¯¢-é”®å¯¹ä¹‹é—´çš„ç›¸ä¼¼åº¦ä¸åŒï¼ŒANNAåªè®¡ç®—ä¸æŸ¥è¯¢å‘é‡æœ€ç›¸å…³çš„é”®å‘é‡çš„ç›¸ä¼¼åº¦ï¼Œä»è€Œæ˜¾è‘—é™ä½äº†è®¡ç®—å¤æ‚åº¦ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜ä»ç†è®ºä¸Šè¯æ˜äº†ANNA-Transformerçš„è¡¨è¾¾èƒ½åŠ›ï¼Œå¹¶è¯æ˜å…¶å¯ä»¥æ¨¡æ‹Ÿä½ç§©Transformerã€‚

**å…³é”®è®¾è®¡**ï¼šANNAçš„å…³é”®è®¾è®¡åŒ…æ‹¬ANNSç®—æ³•çš„é€‰æ‹©ã€è¿‘é‚»æ•°é‡çš„è®¾ç½®ä»¥åŠæ³¨æ„åŠ›æƒé‡çš„è®¡ç®—æ–¹å¼ã€‚ANNSç®—æ³•çš„é€‰æ‹©ä¼šå½±å“è®¡ç®—æ•ˆç‡å’Œè¿‘ä¼¼ç²¾åº¦ã€‚è¿‘é‚»æ•°é‡çš„è®¾ç½®éœ€è¦åœ¨è®¡ç®—å¤æ‚åº¦å’Œæ¨¡å‹è¡¨è¾¾èƒ½åŠ›ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚æ³¨æ„åŠ›æƒé‡çš„è®¡ç®—æ–¹å¼å¯ä»¥é‡‡ç”¨æ ‡å‡†çš„softmaxå‡½æ•°ï¼Œä¹Ÿå¯ä»¥é‡‡ç”¨å…¶ä»–æ›´é«˜æ•ˆçš„è¿‘ä¼¼æ–¹æ³•ã€‚è®ºæ–‡ä¸­å…·ä½“ä½¿ç”¨çš„ANNSç®—æ³•å’Œå‚æ•°è®¾ç½®æœªçŸ¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡é€šè¿‡ç†è®ºåˆ†æè¯æ˜äº†ANNA-Transformeråœ¨MPCç®—æ³•æ¨¡æ‹Ÿå’Œå…³é”®æ¨ç†ä»»åŠ¡ï¼ˆå¦‚Match2å’Œk-hopï¼‰ä¸Šå…·æœ‰æ¥è¿‘æœ€ä¼˜çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜è¯æ˜äº†å¸¸æ•°æ·±åº¦çš„ANNA-Transformerå¯ä»¥æ¨¡æ‹Ÿå¸¸æ•°æ·±åº¦çš„ä½ç§©Transformerï¼Œä»è€Œä¸ºä¸€ç±»å¹¿æ³›çš„é«˜æ•ˆæ³¨æ„åŠ›è¿‘ä¼¼æ–¹æ³•æä¾›äº†ä¸€ç§ç»Ÿä¸€çš„ç†è®ºæ¡†æ¶ã€‚å…·ä½“çš„å®éªŒæ•°æ®æœªçŸ¥ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

ANNAæ³¨æ„åŠ›æœºåˆ¶å¯ä»¥åº”ç”¨äºå„ç§éœ€è¦å¤„ç†é•¿åºåˆ—çš„ä»»åŠ¡ï¼Œä¾‹å¦‚è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„æœºå™¨ç¿»è¯‘ã€æ–‡æœ¬æ‘˜è¦å’Œé—®ç­”ç³»ç»Ÿï¼Œä»¥åŠè®¡ç®—æœºè§†è§‰ä¸­çš„è§†é¢‘ç†è§£å’Œå›¾åƒç”Ÿæˆã€‚è¯¥æ–¹æ³•é™ä½äº†è®¡ç®—å¤æ‚åº¦ï¼Œä½¿å¾—Transformeræ¨¡å‹èƒ½å¤Ÿå¤„ç†æ›´é•¿çš„åºåˆ—ï¼Œä»è€Œæå‡æ¨¡å‹æ€§èƒ½å’Œåº”ç”¨èŒƒå›´ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶ä¸ºè®¾è®¡é«˜æ•ˆæ³¨æ„åŠ›æœºåˆ¶æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Transformers have the representational capacity to simulate Massively Parallel Computation (MPC) algorithms, but they suffer from quadratic time complexity, which severely limits their scalability. We introduce an efficient attention mechanism called Approximate Nearest Neighbor Attention (ANNA) with sub-quadratic time complexity. We prove that ANNA-transformers (1) retain the expressive power previously established for standard attention in terms of matching the capabilities of MPC algorithms, and (2) can solve key reasoning tasks such as Match2 and $k$-hop with near-optimal depth. Using the MPC framework, we further prove that constant-depth ANNA-transformers can simulate constant-depth low-rank transformers, thereby providing a unified way to reason about a broad class of efficient attention approximations.

