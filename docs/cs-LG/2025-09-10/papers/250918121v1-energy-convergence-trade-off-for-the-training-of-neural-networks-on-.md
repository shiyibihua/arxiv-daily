---
layout: default
title: Energy-convergence trade off for the training of neural networks on bio-inspired hardware
---

# Energy-convergence trade off for the training of neural networks on bio-inspired hardware

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.18121" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.18121v1</a>
  <a href="https://arxiv.org/pdf/2509.18121.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.18121v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.18121v1', 'Energy-convergence trade off for the training of neural networks on bio-inspired hardware')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Nikhil Garg, Paul Uriarte Vicandi, Yanming Zhang, Alexandre Baigol, Donato Francesco Falcone, Saketh Ram Mamidala, Bert Jan Offrein, Laura BÃ©gon-Lours

**åˆ†ç±»**: cs.ET, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-10

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºèƒ½é‡æ”¶æ•›æƒè¡¡æ–¹æ³•ä»¥ä¼˜åŒ–ç¥ç»ç½‘ç»œè®­ç»ƒ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation)**

**å…³é”®è¯**: `ç¥ç»ç½‘ç»œè®­ç»ƒ` `å¿†é˜»å™¨` `èƒ½æ•ˆä¼˜åŒ–` `è¾¹ç¼˜è®¡ç®—` `é“ç”µçªè§¦` `çŸ­è„‰å†²ç¼–ç¨‹` `ç¡¬ä»¶æ„ŸçŸ¥`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨æé™è¾¹ç¼˜è®¡ç®—ä¸­é¢ä¸´èƒ½æ•ˆä¸æ€§èƒ½ä¹‹é—´çš„æƒè¡¡æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨ç¥ç»ç½‘ç»œè®­ç»ƒæ—¶ã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºé“ç”µçªè§¦å™¨ä»¶çš„è®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡çŸ­è„‰å†²ç¼–ç¨‹ä¸å®šåˆ¶è®­ç»ƒæ¥ä¼˜åŒ–èƒ½é‡ä½¿ç”¨ä¸å‡†ç¡®æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒçŸ­è„‰å†²ç¼–ç¨‹ç»“åˆå®šåˆ¶è®­ç»ƒæ˜¾è‘—æé«˜äº†å­¦ä¹ æ•ˆç‡ï¼Œå‡å°‘äº†æ€»èƒ½é‡æ¶ˆè€—ï¼ŒåŒæ—¶ä¿æŒäº†è¾ƒé«˜çš„åˆ†ç±»å‡†ç¡®æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¯ç©¿æˆ´ä¼ æ„Ÿå™¨å’Œæ¤å…¥è®¾å¤‡çš„å¹¿æ³›åº”ç”¨ï¼Œäººå·¥æ™ºèƒ½å¤„ç†éœ€æ±‚é€æ¸å‘æé™è¾¹ç¼˜è½¬ç§»ï¼Œè¿«åˆ‡éœ€è¦è¶…ä½åŠŸè€—ä»¥å®ç°æŒç»­è¿è¡Œã€‚å—å¤§è„‘å¯å‘çš„æ–°å…´å¿†é˜»å™¨è®¾å¤‡æœ‰æœ›é€šè¿‡æ¶ˆé™¤è®¡ç®—ä¸å­˜å‚¨ä¹‹é—´çš„æ˜‚è´µæ•°æ®ä¼ è¾“æ¥åŠ é€Ÿç¥ç»ç½‘ç»œè®­ç»ƒã€‚ç„¶è€Œï¼Œæ€§èƒ½ä¸èƒ½æ•ˆä¹‹é—´çš„å¹³è¡¡ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ¬æ–‡ç ”ç©¶äº†åŸºäºHfO2/ZrO2è¶…æ™¶æ ¼çš„é“ç”µçªè§¦å™¨ä»¶ï¼Œå¹¶å°†å®éªŒæµ‹å¾—çš„æƒé‡æ›´æ–°è¾“å…¥åˆ°ç¡¬ä»¶æ„ŸçŸ¥çš„ç¥ç»ç½‘ç»œæ¨¡æ‹Ÿä¸­ã€‚ç ”ç©¶å‘ç°ï¼Œè¾ƒçŸ­çš„è„‰å†²å®½åº¦è™½ç„¶é™ä½äº†æ¯æ¬¡æ›´æ–°çš„èƒ½é‡ï¼Œä½†éœ€è¦æ›´å¤šçš„è®­ç»ƒå‘¨æœŸï¼Œä»èƒ½åœ¨ä¸ç‰ºç‰²å‡†ç¡®åº¦çš„å‰æä¸‹å‡å°‘æ€»èƒ½é‡ã€‚é€šè¿‡åˆ†æåŸå› ï¼Œæå‡ºäº†â€œå¯¹ç§°ç‚¹è½¬ç§»â€æŠ€æœ¯ï¼Œä»¥è§£å†³ä¸å¯¹ç§°æ›´æ–°é—®é¢˜å¹¶æ¢å¤å‡†ç¡®æ€§ã€‚è¿™äº›ç»“æœçªæ˜¾äº†å‡†ç¡®æ€§ã€æ”¶æ•›é€Ÿåº¦å’Œèƒ½é‡ä½¿ç”¨ä¹‹é—´çš„æƒè¡¡ï¼Œè¡¨æ˜çŸ­è„‰å†²ç¼–ç¨‹ä¸å®šåˆ¶è®­ç»ƒæ˜¾è‘—æå‡äº†ç‰‡ä¸Šå­¦ä¹ æ•ˆç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨æé™è¾¹ç¼˜è®¡ç®—ä¸­ç¥ç»ç½‘ç»œè®­ç»ƒçš„èƒ½æ•ˆä¸æ€§èƒ½ä¹‹é—´çš„å¹³è¡¡é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨èƒ½é‡æ¶ˆè€—å’Œè®­ç»ƒæ•ˆç‡ä¸Šå­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨ä½¿ç”¨ä¼ ç»Ÿè®¡ç®—æ¶æ„æ—¶ï¼Œæ•°æ®ä¼ è¾“æˆæœ¬é«˜æ˜‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨åŸºäºHfO2/ZrO2è¶…æ™¶æ ¼çš„é“ç”µçªè§¦å™¨ä»¶ï¼Œé€šè¿‡çŸ­è„‰å†²ç¼–ç¨‹æ¥é™ä½æ¯æ¬¡æ›´æ–°çš„èƒ½é‡æ¶ˆè€—ï¼ŒåŒæ—¶ç»“åˆå®šåˆ¶çš„è®­ç»ƒç­–ç•¥æ¥æé«˜æ”¶æ•›é€Ÿåº¦å’Œå‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å®éªŒæµ‹é‡çš„æƒé‡æ›´æ–°è¾“å…¥åˆ°ç¡¬ä»¶æ„ŸçŸ¥çš„ç¥ç»ç½‘ç»œæ¨¡æ‹Ÿä¸­ï¼Œé‡‡ç”¨ä¸åŒè„‰å†²å®½åº¦è¿›è¡Œè®­ç»ƒï¼Œè¯„ä¼°èƒ½é‡æ¶ˆè€—ä¸åˆ†ç±»å‡†ç¡®æ€§ä¹‹é—´çš„å…³ç³»ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬çªè§¦å™¨ä»¶çš„è®¾è®¡ã€æƒé‡æ›´æ–°çš„å®éªŒæµ‹é‡å’Œç¥ç»ç½‘ç»œçš„è®­ç»ƒæ¨¡æ‹Ÿã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†â€œå¯¹ç§°ç‚¹è½¬ç§»â€æŠ€æœ¯ï¼Œè§£å†³äº†ä¸å¯¹ç§°æ›´æ–°å¯¼è‡´çš„å‡†ç¡®æ€§ä¸‹é™é—®é¢˜ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„è®­ç»ƒæ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨ç¡¬ä»¶ç‰¹æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œè„‰å†²å®½åº¦ä»20 nsåˆ°0.2 msä¸ç­‰ï¼Œè¾ƒçŸ­çš„è„‰å†²é™ä½äº†æ¯æ¬¡æ›´æ–°çš„èƒ½é‡ï¼Œä½†éœ€è¦æ›´å¤šçš„è®­ç»ƒå‘¨æœŸã€‚é‡‡ç”¨æ··åˆç²¾åº¦çš„éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰æ–¹æ³•æ¥ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹ï¼Œå¹¶é€šè¿‡è°ƒæ•´æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„æ¥æé«˜å‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨çŸ­è„‰å†²ç¼–ç¨‹ä¸å®šåˆ¶è®­ç»ƒçš„ç»“åˆï¼Œèƒ½å¤Ÿåœ¨ä¸ç‰ºç‰²åˆ†ç±»å‡†ç¡®æ€§çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—é™ä½æ€»èƒ½é‡æ¶ˆè€—ã€‚å…·ä½“è€Œè¨€ï¼ŒçŸ­è„‰å†²ç¼–ç¨‹ä½¿å¾—æ¯æ¬¡æ›´æ–°çš„èƒ½é‡é™ä½ï¼ŒåŒæ—¶æé«˜äº†å­¦ä¹ æ•ˆç‡ï¼Œå±•ç¤ºäº†åœ¨æé™è¾¹ç¼˜è®¡ç®—ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¯ç©¿æˆ´è®¾å¤‡ã€æ¤å…¥å¼åŒ»ç–—è®¾å¤‡å’Œå…¶ä»–éœ€è¦é«˜æ•ˆèƒ½é‡ç®¡ç†çš„è¾¹ç¼˜è®¡ç®—åœºæ™¯ã€‚é€šè¿‡ä¼˜åŒ–ç¥ç»ç½‘ç»œè®­ç»ƒè¿‡ç¨‹ï¼Œå¯ä»¥å®ç°æ›´é•¿çš„è®¾å¤‡ç»­èˆªæ—¶é—´å’Œæ›´é«˜çš„å®æ—¶å¤„ç†èƒ½åŠ›ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The increasing deployment of wearable sensors and implantable devices is shifting AI processing demands to the extreme edge, necessitating ultra-low power for continuous operation. Inspired by the brain, emerging memristive devices promise to accelerate neural network training by eliminating costly data transfers between compute and memory. Though, balancing performance and energy efficiency remains a challenge. We investigate ferroelectric synaptic devices based on HfO2/ZrO2 superlattices and feed their experimentally measured weight updates into hardware-aware neural network simulations. Across pulse widths from 20 ns to 0.2 ms, shorter pulses lower per-update energy but require more training epochs while still reducing total energy without sacrificing accuracy. Classification accuracy using plain stochastic gradient descent (SGD) is diminished compared to mixed-precision SGD. We analyze the causes and propose a ``symmetry point shifting'' technique, addressing asymmetric updates and restoring accuracy. These results highlight a trade-off among accuracy, convergence speed, and energy use, showing that short-pulse programming with tailored training significantly enhances on-chip learning efficiency.

