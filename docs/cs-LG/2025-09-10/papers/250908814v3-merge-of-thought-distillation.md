---
layout: default
title: Merge-of-Thought Distillation
---

# Merge-of-Thought Distillation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.08814" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.08814v3</a>
  <a href="https://arxiv.org/pdf/2509.08814.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.08814v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.08814v3', 'Merge-of-Thought Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhanming Shen, Zeyu Qin, Zenan Huang, Hao Chen, Jiaqi Hu, Yihong Zhuang, Guoshan Lu, Gang Chen, Junbo Zhao

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-10 (æ›´æ–°: 2025-10-16)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMerge-of-Thought Distillationï¼Œè§£å†³é•¿é“¾CoTæ¨¡å‹è’¸é¦ä¸­å¤šæ•™å¸ˆå†²çªé—®é¢˜ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ€ç»´é“¾è’¸é¦` `å¤šæ•™å¸ˆå­¦ä¹ ` `æ¨¡å‹èåˆ` `çŸ¥è¯†è’¸é¦` `é•¿é“¾æ¨ç†` `æƒé‡åˆå¹¶` `å…±è¯†æ¨ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰CoTæ¨¡å‹è’¸é¦ä¾èµ–å•ä¸€æ•™å¸ˆï¼Œå¿½ç•¥äº†å¤šæ•™å¸ˆCoTè¯­æ–™åº“çš„æ½œåŠ›ï¼Œå¯¼è‡´å­¦ç”Ÿæ¨¡å‹æ€§èƒ½å—é™ã€‚
2. æå‡ºMerge-of-Thought Distillation (MoT)ï¼Œé€šè¿‡äº¤æ›¿å¾®è°ƒå’Œæƒé‡åˆå¹¶ï¼Œèåˆå¤šæ•™å¸ˆçš„æ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒMoTåœ¨æ•°å­¦æ¨ç†ä¸Šè¶…è¶Šäº†å¤šä¸ªå¼ºå¤§æ¨¡å‹ï¼Œå¹¶æå‡äº†é€šç”¨æ¨ç†èƒ½åŠ›ï¼Œé™ä½äº†ç¾éš¾æ€§é—å¿˜ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é’ˆå¯¹é•¿é“¾æ€ç»´ï¼ˆCoTï¼‰æ¨¡å‹çš„é«˜æ•ˆæ¨ç†è’¸é¦æ—¥ç›Šå—åˆ°å•ä¸€oracleæ•™å¸ˆå‡è®¾çš„é™åˆ¶ï¼Œè€Œå®é™…ä¸­å­˜åœ¨å¤šä¸ªå€™é€‰æ•™å¸ˆå’Œä¸æ–­å¢é•¿çš„CoTè¯­æ–™åº“ã€‚æœ¬æ–‡é‡æ–°å®¡è§†äº†æ•™å¸ˆé€‰æ‹©é—®é¢˜ï¼Œå‘ç°ä¸åŒçš„å­¦ç”Ÿæœ‰ä¸åŒçš„â€œæœ€ä½³æ•™å¸ˆâ€ï¼Œç”šè‡³å¯¹äºåŒä¸€ä¸ªå­¦ç”Ÿï¼Œæœ€ä½³æ•™å¸ˆä¹Ÿå¯èƒ½å› æ•°æ®é›†è€Œå¼‚ã€‚å› æ­¤ï¼Œä¸ºäº†å°†å¤šä¸ªæ•™å¸ˆçš„æ¨ç†èƒ½åŠ›ç»Ÿä¸€åˆ°ä¸€ä¸ªå­¦ç”Ÿä¸­ï¼Œä»¥å…‹æœä¸åŒæ•™å¸ˆç›‘ç£ä¹‹é—´çš„å†²çªï¼Œæˆ‘ä»¬æå‡ºäº†Merge-of-Thought Distillationï¼ˆMoTï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§æ¡†æ¶ï¼Œåœ¨ç‰¹å®šäºæ•™å¸ˆçš„ç›‘ç£å¾®è°ƒåˆ†æ”¯å’Œç”±æ­¤äº§ç”Ÿçš„å­¦ç”Ÿå˜ä½“çš„æƒé‡ç©ºé—´åˆå¹¶ä¹‹é—´äº¤æ›¿è¿›è¡Œã€‚åœ¨ç«èµ›æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­ï¼Œä»…ä½¿ç”¨çº¦200ä¸ªCoTæ ·æœ¬ï¼Œå°†MoTåº”ç”¨äºQwen3-14Bå­¦ç”Ÿæ¨¡å‹ï¼Œå°±è¶…è¶Šäº†åŒ…æ‹¬Deepseek-R1ã€Qwen3-32Bå’ŒOpenAI-O1åœ¨å†…çš„å¼ºå¤§æ¨¡å‹ï¼Œå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚æ­¤å¤–ï¼ŒMoTå§‹ç»ˆä¼˜äºæœ€ä½³å•æ•™å¸ˆè’¸é¦ï¼Œåœ¨æé«˜æ•°å­¦ä¹‹å¤–çš„é€šç”¨æ¨ç†èƒ½åŠ›çš„åŒæ—¶ï¼Œå‡å°‘äº†ç¾éš¾æ€§é—å¿˜ï¼Œå¹¶å¯¹åˆ†å¸ƒåç§»å’ŒåŒçº§æ•™å¸ˆè¡¨ç°å‡ºé²æ£’æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬è¯æ˜äº†MoTé€šè¿‡æ¶ˆé™¤ç‰¹å®šäºæ•™å¸ˆçš„å½’çº³åå·®å’Œæ•™å¸ˆé—´çš„å†²çªï¼ŒåŒæ—¶é‡å¤åŠ å¼ºå¯¹å…±è¯†æ¨ç†ç‰¹å¾çš„å­¦ä¹ ï¼Œä»è€Œæ‹¥æœ‰å…±è¯†CoTã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒMoTæ˜¯ä¸€ç§ç®€å•æœ‰æ•ˆçš„é€”å¾„ï¼Œå¯ä»¥å°†æ¥è‡ªä¸åŒæ•™å¸ˆçš„é•¿CoTèƒ½åŠ›é«˜æ•ˆåœ°æç‚¼åˆ°ç´§å‡‘çš„å­¦ç”Ÿæ¨¡å‹ä¸­ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰é•¿é“¾æ€ç»´ï¼ˆCoTï¼‰æ¨¡å‹çš„çŸ¥è¯†è’¸é¦æ–¹æ³•é€šå¸¸ä¾èµ–äºå•ä¸€çš„â€œoracleâ€æ•™å¸ˆæ¨¡å‹ã€‚ç„¶è€Œï¼Œå®é™…æƒ…å†µæ˜¯å­˜åœ¨å¤šä¸ªCoTæ•™å¸ˆæ¨¡å‹ï¼Œå¹¶ä¸”è¿™äº›æ•™å¸ˆæ¨¡å‹ä¹‹é—´å¯èƒ½å­˜åœ¨æ¨ç†ä¸Šçš„å†²çªã€‚ç®€å•åœ°é€‰æ‹©ä¸€ä¸ªæ•™å¸ˆè¿›è¡Œè’¸é¦å¯èƒ½ä¼šå¯¼è‡´å­¦ç”Ÿæ¨¡å‹ç»§æ‰¿è¯¥æ•™å¸ˆçš„ç‰¹å®šåå·®ï¼Œè€Œå¿½ç•¥äº†å…¶ä»–æ•™å¸ˆçš„ä¼˜ç‚¹ã€‚å› æ­¤ï¼Œå¦‚ä½•æœ‰æ•ˆåœ°åˆ©ç”¨å¤šä¸ªæ•™å¸ˆçš„çŸ¥è¯†ï¼Œå…‹æœæ•™å¸ˆé—´çš„å†²çªï¼Œæå‡å­¦ç”Ÿæ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMoTçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡äº¤æ›¿è¿›è¡Œæ•™å¸ˆç‰¹å®šå¾®è°ƒå’Œæƒé‡ç©ºé—´åˆå¹¶ï¼Œä»è€Œå°†å¤šä¸ªæ•™å¸ˆçš„æ¨ç†èƒ½åŠ›èåˆåˆ°å­¦ç”Ÿæ¨¡å‹ä¸­ã€‚è¿™ç§æ–¹æ³•å…è®¸å­¦ç”Ÿæ¨¡å‹å­¦ä¹ æ¯ä¸ªæ•™å¸ˆçš„ç‹¬ç‰¹è§†è§’ï¼Œå¹¶é€šè¿‡æƒé‡åˆå¹¶æ¥æ¶ˆé™¤æ•™å¸ˆé—´çš„å†²çªï¼Œæœ€ç»ˆè·å¾—ä¸€ä¸ªå…·æœ‰å…±è¯†æ¨ç†èƒ½åŠ›çš„æ¨¡å‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMoTæ¡†æ¶åŒ…å«ä»¥ä¸‹ä¸»è¦æ­¥éª¤ï¼š1) åˆå§‹åŒ–å­¦ç”Ÿæ¨¡å‹ï¼›2) å¯¹æ¯ä¸ªæ•™å¸ˆï¼Œä½¿ç”¨å…¶CoTæ•°æ®å¯¹å­¦ç”Ÿæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¾—åˆ°å¤šä¸ªç‰¹å®šäºæ•™å¸ˆçš„å­¦ç”Ÿæ¨¡å‹å˜ä½“ï¼›3) å°†è¿™äº›å­¦ç”Ÿæ¨¡å‹å˜ä½“çš„æƒé‡è¿›è¡Œåˆå¹¶ï¼Œå¾—åˆ°ä¸€ä¸ªèåˆäº†å¤šæ•™å¸ˆçŸ¥è¯†çš„å­¦ç”Ÿæ¨¡å‹ï¼›4) é‡å¤æ­¥éª¤2å’Œ3ï¼Œç›´åˆ°æ¨¡å‹æ”¶æ•›ã€‚

**å…³é”®åˆ›æ–°**ï¼šMoTçš„å…³é”®åˆ›æ–°åœ¨äºå…¶äº¤æ›¿å¾®è°ƒå’Œæƒé‡åˆå¹¶çš„ç­–ç•¥ã€‚è¿™ç§ç­–ç•¥å…è®¸å­¦ç”Ÿæ¨¡å‹åœ¨å­¦ä¹ æ¯ä¸ªæ•™å¸ˆçš„ç‹¬ç‰¹çŸ¥è¯†çš„åŒæ—¶ï¼Œé€šè¿‡æƒé‡åˆå¹¶æ¥æ¶ˆé™¤æ•™å¸ˆé—´çš„å†²çªï¼Œä»è€Œè·å¾—æ›´é²æ£’å’Œæ³›åŒ–çš„æ¨ç†èƒ½åŠ›ã€‚ä¸ä¼ ç»Ÿçš„å•æ•™å¸ˆè’¸é¦æ–¹æ³•ç›¸æ¯”ï¼ŒMoTèƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨å¤šæ•™å¸ˆçš„ä¼˜åŠ¿ï¼Œæå‡å­¦ç”Ÿæ¨¡å‹çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šMoTçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) æ•™å¸ˆç‰¹å®šå¾®è°ƒçš„å­¦ä¹ ç‡å’Œè®­ç»ƒè½®æ•°ï¼›2) æƒé‡åˆå¹¶çš„ç­–ç•¥ï¼Œä¾‹å¦‚ç®€å•çš„å¹³å‡æˆ–æ›´å¤æ‚çš„åŠ æƒå¹³å‡ï¼›3) æŸå¤±å‡½æ•°çš„è®¾è®¡ï¼Œä¾‹å¦‚ä½¿ç”¨äº¤å‰ç†µæŸå¤±æˆ–çŸ¥è¯†è’¸é¦æŸå¤±ï¼›4) å­¦ç”Ÿæ¨¡å‹çš„é€‰æ‹©ï¼Œä¾‹å¦‚é€‰æ‹©ä¸€ä¸ªä¸æ•™å¸ˆæ¨¡å‹ç»“æ„ç›¸ä¼¼çš„æ¨¡å‹æˆ–ä¸€ä¸ªæ›´å°çš„æ¨¡å‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ç«èµ›æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­ï¼Œä»…ä½¿ç”¨çº¦200ä¸ªCoTæ ·æœ¬ï¼Œå°†MoTåº”ç”¨äºQwen3-14Bå­¦ç”Ÿæ¨¡å‹ï¼Œå°±è¶…è¶Šäº†Deepseek-R1ã€Qwen3-32Bå’ŒOpenAI-O1ç­‰å¼ºå¤§æ¨¡å‹ã€‚MoTè¿˜ä¼˜äºæœ€ä½³å•æ•™å¸ˆè’¸é¦ï¼Œæå‡äº†é€šç”¨æ¨ç†èƒ½åŠ›ï¼Œå‡å°‘äº†ç¾éš¾æ€§é—å¿˜ï¼Œå¹¶å¯¹åˆ†å¸ƒåç§»å’ŒåŒçº§æ•™å¸ˆè¡¨ç°å‡ºé²æ£’æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MoTå¯åº”ç”¨äºå„ç§éœ€è¦é•¿é“¾æ¨ç†çš„åœºæ™¯ï¼Œå¦‚æ•°å­¦é—®é¢˜æ±‚è§£ã€ä»£ç ç”Ÿæˆã€çŸ¥è¯†å›¾è°±æ¨ç†ç­‰ã€‚é€šè¿‡å°†å¤šä¸ªä¸“å®¶æ¨¡å‹çš„çŸ¥è¯†æç‚¼åˆ°ä¸€ä¸ªç´§å‡‘çš„å­¦ç”Ÿæ¨¡å‹ä¸­ï¼Œå¯ä»¥é™ä½éƒ¨ç½²æˆæœ¬ï¼Œæé«˜æ¨ç†æ•ˆç‡ï¼Œå¹¶æå‡æ¨¡å‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ–¹æ³•åœ¨æ•™è‚²ã€é‡‘èã€åŒ»ç–—ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Efficient reasoning distillation for long chain-of-thought (CoT) models is increasingly constrained by the assumption of a single oracle teacher, despite the practical availability of multiple candidate teachers and growing CoT corpora. We revisit teacher selection and observe that different students have different "best teachers," and even for the same student, the best teacher can vary across datasets. Therefore, to unify multiple teachers' reasoning abilities into a student to overcome conflicts among various teachers' supervision, we propose Merge-of-Thought Distillation (MoT), a lightweight framework that alternates between teacher-specific supervised fine-tuning branches and weight-space merging of the resulting student variants. On competition math benchmarks, using only about 200 CoT samples, applying MoT to a Qwen3-14B student surpasses strong models including Deepseek-R1, Qwen3-32B, and OpenAI-O1, demonstrating substantial gains. Besides, MoT consistently outperforms the best single-teacher distillation, improves general reasoning beyond mathematics while reducing catastrophic forgetting, and shows robustness to distribution-shifted and peer-level teachers. Finally, we have demonstrated MoT possesses consensus CoT by eliminating teacher-specific inductive biases and inter-teacher conflicts while repeatedly reinforcing the learning of consensus reasoning features. These results position MoT as a simple, effective route to efficiently distilling long CoT capabilities from diverse teachers into compact students.

