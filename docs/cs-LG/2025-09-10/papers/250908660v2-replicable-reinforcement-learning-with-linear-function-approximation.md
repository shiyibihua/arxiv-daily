---
layout: default
title: Replicable Reinforcement Learning with Linear Function Approximation
---

# Replicable Reinforcement Learning with Linear Function Approximation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.08660" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.08660v2</a>
  <a href="https://arxiv.org/pdf/2509.08660.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.08660v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.08660v2', 'Replicable Reinforcement Learning with Linear Function Approximation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Eric Eaton, Marcel Hussing, Michael Kearns, Aaron Roth, Sikata Bela Sengupta, Jessica Sorrell

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-10 (æ›´æ–°: 2025-10-01)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é’ˆå¯¹çº¿æ€§å‡½æ•°é€¼è¿‘çš„å¼ºåŒ–å­¦ä¹ ï¼Œæå‡ºå¯å¤ç°ç®—æ³•ä»¥æå‡å®éªŒä¸€è‡´æ€§ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `å¯å¤ç°æ€§` `çº¿æ€§å‡½æ•°é€¼è¿‘` `é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹` `éšæœºè®¾è®¡å›å½’`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„å®éªŒç»“æœéš¾ä»¥å¤ç°ï¼Œå½±å“äº†ç ”ç©¶çš„å¯é æ€§ï¼Œå°¤å…¶æ˜¯åœ¨ä¸ç¨³å®šçš„å‡½æ•°é€¼è¿‘åœºæ™¯ä¸‹ã€‚
2. è®ºæ–‡æå‡ºå¯å¤ç°çš„çº¿æ€§å‡½æ•°é€¼è¿‘å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œä¿è¯åœ¨ç›¸åŒåˆ†å¸ƒæ ·æœ¬ä¸Šå¤šæ¬¡è¿è¡Œç»“æœçš„ä¸€è‡´æ€§ã€‚
3. é€šè¿‡å®éªŒéªŒè¯äº†æ‰€æç®—æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨æå‡ç¥ç»ç­–ç•¥ä¸€è‡´æ€§æ–¹é¢çš„æ½œåŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å®éªŒç»“æœçš„å¯å¤ç°æ€§æ˜¯åŒ…æ‹¬æœºå™¨å­¦ä¹ åœ¨å†…çš„è®¸å¤šç§‘å­¦é¢†åŸŸé¢ä¸´çš„æŒ‘æˆ˜ã€‚æœ€è¿‘å…³äºæœºå™¨å­¦ä¹ ç†è®ºçš„å·¥ä½œå°†å¯å¤ç°æ€§å½¢å¼åŒ–ä¸ºï¼šç®—æ³•åœ¨æ¥è‡ªç›¸åŒåˆ†å¸ƒçš„ä¸åŒæ ·æœ¬ä¸Šæ‰§è¡Œä¸¤æ¬¡æ—¶ï¼Œåº”äº§ç”Ÿç›¸åŒçš„ç»“æœã€‚å¯¹äºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è€Œè¨€ï¼Œå¯è¯æ˜å¯å¤ç°çš„ç®—æ³•å°¤å…¶é‡è¦ï¼Œå› ä¸ºå·²çŸ¥RLç®—æ³•åœ¨å®è·µä¸­æ˜¯ä¸ç¨³å®šçš„ã€‚è™½ç„¶è¡¨æ ¼å‹RLè®¾ç½®å­˜åœ¨å¯å¤ç°çš„ç®—æ³•ï¼Œä½†å°†è¿™äº›ä¿è¯æ‰©å±•åˆ°æ›´å®ç”¨çš„å‡½æ•°é€¼è¿‘è®¾ç½®ä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾çš„é—®é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ä¸ºRLä¸­çš„çº¿æ€§å‡½æ•°é€¼è¿‘å¼€å‘å¯å¤ç°çš„æ–¹æ³•å–å¾—äº†è¿›å±•ã€‚æˆ‘ä»¬é¦–å…ˆä»‹ç»äº†ä¸¤ç§ç”¨äºå¯å¤ç°éšæœºè®¾è®¡å›å½’å’Œéä¸­å¿ƒåæ–¹å·®ä¼°è®¡çš„æœ‰æ•ˆç®—æ³•ï¼Œæ¯ç§ç®—æ³•éƒ½å…·æœ‰ç‹¬ç«‹çš„æ„ä¹‰ã€‚ç„¶åï¼Œæˆ‘ä»¬åˆ©ç”¨è¿™äº›å·¥å…·ï¼Œåœ¨ç”Ÿæˆæ¨¡å‹å’Œ episodic è®¾ç½®ä¸­ï¼Œä¸ºçº¿æ€§é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹æä¾›äº†ç¬¬ä¸€ä¸ªå¯è¯æ˜æœ‰æ•ˆçš„å¯å¤ç°RLç®—æ³•ã€‚æœ€åï¼Œæˆ‘ä»¬å¯¹ç®—æ³•è¿›è¡Œäº†å®éªŒè¯„ä¼°ï¼Œå¹¶å±•ç¤ºäº†å®ƒä»¬å¦‚ä½•æ¿€å‘æ›´ä¸€è‡´çš„ç¥ç»ç­–ç•¥ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œå°¤å…¶æ˜¯åœ¨ä½¿ç”¨å‡½æ•°é€¼è¿‘æ—¶ï¼Œå­˜åœ¨è®­ç»ƒç»“æœä¸ç¨³å®šã€éš¾ä»¥å¤ç°çš„é—®é¢˜ã€‚å³ä½¿åœ¨ç›¸åŒç¯å¢ƒå’Œè¶…å‚æ•°ä¸‹ï¼Œå¤šæ¬¡è®­ç»ƒå¾—åˆ°çš„ç­–ç•¥ä¹Ÿå¯èƒ½å·®å¼‚å¾ˆå¤§ã€‚è¿™ä½¿å¾—ç®—æ³•çš„è¯„ä¼°å’Œæ¯”è¾ƒå˜å¾—å›°éš¾ï¼Œä¹Ÿé˜»ç¢äº†å¼ºåŒ–å­¦ä¹ åœ¨å®é™…åº”ç”¨ä¸­çš„éƒ¨ç½²ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯è®¾è®¡å¯å¤ç°çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œå³ç®—æ³•çš„è¾“å‡ºåªä¾èµ–äºè¾“å…¥æ•°æ®çš„åˆ†å¸ƒï¼Œè€Œä¸ä¾èµ–äºç‰¹å®šçš„æ ·æœ¬ã€‚é€šè¿‡ç¡®ä¿ç®—æ³•çš„ç¡®å®šæ€§ï¼Œå¯ä»¥ä¿è¯åœ¨ç›¸åŒåˆ†å¸ƒçš„æ•°æ®ä¸Šå¤šæ¬¡è¿è¡Œå¾—åˆ°ç›¸åŒçš„ç»“æœã€‚é’ˆå¯¹çº¿æ€§å‡½æ•°é€¼è¿‘ï¼Œè®ºæ–‡åˆ©ç”¨å¯å¤ç°çš„å›å½’å’Œåæ–¹å·®ä¼°è®¡æ–¹æ³•ï¼Œæ„å»ºå¯å¤ç°çš„ç­–ç•¥å­¦ä¹ ç®—æ³•ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) æå‡ºå¯å¤ç°çš„éšæœºè®¾è®¡å›å½’ç®—æ³•ï¼›2) æå‡ºå¯å¤ç°çš„éä¸­å¿ƒåæ–¹å·®ä¼°è®¡ç®—æ³•ï¼›3) åŸºäºä¸Šè¿°ä¸¤ç§ç®—æ³•ï¼Œæ„å»ºå¯å¤ç°çš„çº¿æ€§é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œåˆ†åˆ«é’ˆå¯¹ç”Ÿæˆæ¨¡å‹å’Œ episodic è®¾ç½®ï¼›4) é€šè¿‡å®éªŒéªŒè¯ç®—æ³•çš„å¯å¤ç°æ€§å’Œæ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†é¦–ä¸ªé’ˆå¯¹çº¿æ€§å‡½æ•°é€¼è¿‘çš„å¯å¤ç°å¼ºåŒ–å­¦ä¹ ç®—æ³•ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥ç®—æ³•èƒ½å¤Ÿä¿è¯åœ¨ç›¸åŒæ•°æ®åˆ†å¸ƒä¸‹å¤šæ¬¡è¿è¡Œå¾—åˆ°ç›¸åŒçš„ç»“æœï¼Œä»è€Œè§£å†³äº†å¼ºåŒ–å­¦ä¹ ç®—æ³•éš¾ä»¥å¤ç°çš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†ä¸¤ç§ç‹¬ç«‹çš„ã€å…·æœ‰é‡è¦æ„ä¹‰çš„å¯å¤ç°ç®—æ³•ï¼šéšæœºè®¾è®¡å›å½’å’Œéä¸­å¿ƒåæ–¹å·®ä¼°è®¡ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨å¯å¤ç°çš„å›å½’ç®—æ³•æ¥ä¼°è®¡ä»·å€¼å‡½æ•°ï¼›2) ä½¿ç”¨å¯å¤ç°çš„åæ–¹å·®ä¼°è®¡ç®—æ³•æ¥ä¼°è®¡çŠ¶æ€è½¬ç§»æ¦‚ç‡ï¼›3) å°†ä¸Šè¿°ä¸¤ç§ä¼°è®¡æ–¹æ³•ç»“åˆèµ·æ¥ï¼Œæ„å»ºå¯å¤ç°çš„ç­–ç•¥è¿­ä»£ç®—æ³•ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°å–å†³äºå…·ä½“çš„çº¿æ€§MDPæ¨¡å‹å’Œå¼ºåŒ–å­¦ä¹ ç®—æ³•ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡é€šè¿‡å®éªŒéªŒè¯äº†æ‰€æç®—æ³•çš„å¯å¤ç°æ€§ï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨æå‡ç¥ç»ç­–ç•¥ä¸€è‡´æ€§æ–¹é¢çš„æ½œåŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ç›¸æ¯”ï¼Œæ‰€æç®—æ³•èƒ½å¤Ÿæ˜¾è‘—é™ä½ç­–ç•¥çš„æ–¹å·®ï¼Œæé«˜ç­–ç•¥çš„ç¨³å®šæ€§ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œå¯¹æ¯”åŸºçº¿åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå¯¹ç»“æœä¸€è‡´æ€§è¦æ±‚è¾ƒé«˜çš„å¼ºåŒ–å­¦ä¹ åº”ç”¨åœºæ™¯ï¼Œä¾‹å¦‚è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—å†³ç­–ç­‰ã€‚é€šè¿‡ä½¿ç”¨å¯å¤ç°çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œå¯ä»¥æé«˜ç³»ç»Ÿçš„å¯é æ€§å’Œå®‰å…¨æ€§ï¼Œé™ä½å› ç®—æ³•ä¸ç¨³å®šè€Œå¯¼è‡´çš„é£é™©ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶ä¹Ÿæœ‰åŠ©äºæ¨åŠ¨å¼ºåŒ–å­¦ä¹ ç†è®ºçš„å‘å±•ï¼Œä¸ºè®¾è®¡æ›´ç¨³å®šã€æ›´å¯é çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•æä¾›ç†è®ºåŸºç¡€ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Replication of experimental results has been a challenge faced by many scientific disciplines, including the field of machine learning. Recent work on the theory of machine learning has formalized replicability as the demand that an algorithm produce identical outcomes when executed twice on different samples from the same distribution. Provably replicable algorithms are especially interesting for reinforcement learning (RL), where algorithms are known to be unstable in practice. While replicable algorithms exist for tabular RL settings, extending these guarantees to more practical function approximation settings has remained an open problem. In this work, we make progress by developing replicable methods for linear function approximation in RL. We first introduce two efficient algorithms for replicable random design regression and uncentered covariance estimation, each of independent interest. We then leverage these tools to provide the first provably efficient replicable RL algorithms for linear Markov decision processes in both the generative model and episodic settings. Finally, we evaluate our algorithms experimentally and show how they can inspire more consistent neural policies.

