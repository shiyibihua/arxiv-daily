---
layout: default
title: Partially Equivariant Reinforcement Learning in Symmetry-Breaking Environments
---

# Partially Equivariant Reinforcement Learning in Symmetry-Breaking Environments

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.00915" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.00915v1</a>
  <a href="https://arxiv.org/pdf/2512.00915.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.00915v1" onclick="toggleFavorite(this, '2512.00915v1', 'Partially Equivariant Reinforcement Learning in Symmetry-Breaking Environments')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Junwoo Chang, Minwoo Park, Joohwan Seo, Roberto Horowitz, Jongmin Lee, Jongeun Choi

**åˆ†ç±»**: cs.LG, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-11-30

**å¤‡æ³¨**: 27 pages, 10 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºéƒ¨åˆ†ç­‰å˜å¼ºåŒ–å­¦ä¹ ï¼Œè§£å†³å¯¹ç§°ç ´ç¼ºç¯å¢ƒä¸‹çš„æ³›åŒ–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `ç¾¤å¯¹ç§°æ€§` `ç­‰å˜æ€§` `å¯¹ç§°ç ´ç¼º` `æ³›åŒ–èƒ½åŠ›`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°å®ç¯å¢ƒä¸­çš„å¯¹ç§°æ€§é€šå¸¸æ˜¯å±€éƒ¨è€Œéå…¨å±€çš„ï¼Œè¿™å¯¼è‡´ä¼ ç»Ÿç¾¤ä¸å˜å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¯¹ç§°ç ´ç¼ºåŒºåŸŸäº§ç”Ÿè¯¯å·®å¹¶æ‰©æ•£ã€‚
2. è®ºæ–‡æå‡ºéƒ¨åˆ†ç¾¤ä¸å˜MDP (PI-MDP) æ¡†æ¶ï¼Œæ ¹æ®å¯¹ç§°æ€§ä¿æŒæƒ…å†µé€‰æ‹©æ€§åœ°åº”ç”¨ç¾¤ä¸å˜æˆ–æ ‡å‡†è´å°”æ›¼å¤‡ä»½ï¼Œå‡å°‘è¯¯å·®ä¼ æ’­ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæå‡ºçš„ PE-DQN å’Œ PE-SAC ç®—æ³•åœ¨å¤šä¸ªä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ï¼ŒéªŒè¯äº†é€‰æ‹©æ€§å¯¹ç§°æ€§åˆ©ç”¨çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç¾¤å¯¹ç§°æ€§ä¸ºå¼ºåŒ–å­¦ä¹ (RL)æä¾›äº†ä¸€ç§å¼ºå¤§çš„å½’çº³åç½®ï¼Œé€šè¿‡ç¾¤ä¸å˜é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(MDP)å®ç°è·¨å¯¹ç§°çŠ¶æ€å’ŒåŠ¨ä½œçš„æœ‰æ•ˆæ³›åŒ–ã€‚ç„¶è€Œï¼Œç°å®ç¯å¢ƒå‡ ä¹ä»æœªå®ç°å®Œå…¨ç¾¤ä¸å˜çš„MDPï¼›åŠ¨åŠ›å­¦ã€é©±åŠ¨é™åˆ¶å’Œå¥–åŠ±è®¾è®¡é€šå¸¸ä¼šæ‰“ç ´å¯¹ç§°æ€§ï¼Œè€Œä¸”é€šå¸¸åªæ˜¯å±€éƒ¨æ‰“ç ´ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¦‚æœé‡‡ç”¨ç¾¤ä¸å˜çš„è´å°”æ›¼å¤‡ä»½ï¼Œå±€éƒ¨å¯¹ç§°æ€§ç ´ç¼ºä¼šå¼•å…¥è¯¯å·®ï¼Œå¹¶åœ¨æ•´ä¸ªçŠ¶æ€-åŠ¨ä½œç©ºé—´ä¸­ä¼ æ’­ï¼Œå¯¼è‡´å…¨å±€ä»·å€¼ä¼°è®¡è¯¯å·®ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†éƒ¨åˆ†ç¾¤ä¸å˜MDP(PI-MDP)ï¼Œå®ƒæ ¹æ®å¯¹ç§°æ€§ä¿æŒçš„ä½ç½®é€‰æ‹©æ€§åœ°åº”ç”¨ç¾¤ä¸å˜æˆ–æ ‡å‡†è´å°”æ›¼å¤‡ä»½ã€‚è¯¥æ¡†æ¶å‡è½»äº†å±€éƒ¨å¯¹ç§°æ€§ç ´ç¼ºå¸¦æ¥çš„è¯¯å·®ä¼ æ’­ï¼ŒåŒæ—¶ä¿æŒäº†ç­‰å˜çš„ä¼˜åŠ¿ï¼Œä»è€Œæé«˜äº†æ ·æœ¬æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚åœ¨æ­¤æ¡†æ¶çš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†å®ç”¨çš„RLç®—æ³•â€”â€”ç”¨äºç¦»æ•£æ§åˆ¶çš„Partially Equivariant (PE)-DQNå’Œç”¨äºè¿ç»­æ§åˆ¶çš„PE-SACâ€”â€”å®ƒä»¬ç»“åˆäº†ç­‰å˜çš„ä¼˜åŠ¿å’Œå¯¹å¯¹ç§°æ€§ç ´ç¼ºçš„é²æ£’æ€§ã€‚åœ¨Grid-Worldã€è¿åŠ¨å’Œæ“ä½œåŸºå‡†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒPE-DQNå’ŒPE-SACæ˜æ˜¾ä¼˜äºåŸºçº¿ï¼Œçªå‡ºäº†é€‰æ‹©æ€§å¯¹ç§°æ€§åˆ©ç”¨å¯¹äºé²æ£’å’Œæ ·æœ¬é«˜æ•ˆRLçš„é‡è¦æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºç¾¤å¯¹ç§°æ€§çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¾èµ–äºå®Œå…¨ç¾¤ä¸å˜çš„MDPå‡è®¾ï¼Œä½†åœ¨å®é™…ç¯å¢ƒä¸­ï¼Œç”±äºåŠ¨åŠ›å­¦ã€åŠ¨ä½œé™åˆ¶æˆ–å¥–åŠ±å‡½æ•°çš„è®¾è®¡ï¼Œå¯¹ç§°æ€§å¾€å¾€æ˜¯å±€éƒ¨ç ´ç¼ºçš„ã€‚è¿™ç§å±€éƒ¨å¯¹ç§°æ€§ç ´ç¼ºä¼šå¯¼è‡´ä»·å€¼ä¼°è®¡è¯¯å·®åœ¨æ•´ä¸ªçŠ¶æ€ç©ºé—´ä¼ æ’­ï¼Œé™ä½ç®—æ³•çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€æƒ³æ˜¯åªåœ¨å¯¹ç§°æ€§ä¿æŒçš„åŒºåŸŸåˆ©ç”¨ç¾¤ä¸å˜æ€§ï¼Œè€Œåœ¨å¯¹ç§°æ€§ç ´ç¼ºçš„åŒºåŸŸä½¿ç”¨æ ‡å‡†çš„è´å°”æ›¼å¤‡ä»½ã€‚é€šè¿‡è¿™ç§é€‰æ‹©æ€§çš„å¯¹ç§°æ€§åˆ©ç”¨ï¼Œå¯ä»¥é¿å…è¯¯å·®ä»å¯¹ç§°æ€§ç ´ç¼ºåŒºåŸŸä¼ æ’­åˆ°æ•´ä¸ªçŠ¶æ€ç©ºé—´ï¼Œä»è€Œæé«˜ä»·å€¼ä¼°è®¡çš„å‡†ç¡®æ€§å’Œç®—æ³•çš„é²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡æå‡ºäº†éƒ¨åˆ†ç¾¤ä¸å˜MDP (PI-MDP) æ¡†æ¶ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªå…³é”®éƒ¨åˆ†ï¼š1) å¯¹ç§°æ€§æ£€æµ‹å™¨ï¼Œç”¨äºåˆ¤æ–­å½“å‰çŠ¶æ€-åŠ¨ä½œå¯¹æ˜¯å¦æ»¡è¶³ç¾¤ä¸å˜æ€§ï¼›2) é€‰æ‹©æ€§è´å°”æ›¼å¤‡ä»½ï¼Œæ ¹æ®å¯¹ç§°æ€§æ£€æµ‹å™¨çš„ç»“æœï¼Œé€‰æ‹©ä½¿ç”¨ç¾¤ä¸å˜è´å°”æ›¼å¤‡ä»½æˆ–æ ‡å‡†è´å°”æ›¼å¤‡ä»½æ¥æ›´æ–°ä»·å€¼å‡½æ•°ã€‚åŸºäº PI-MDP æ¡†æ¶ï¼Œè®ºæ–‡è¿›ä¸€æ­¥æå‡ºäº† PE-DQN å’Œ PE-SAC ä¸¤ç§å…·ä½“çš„ç®—æ³•ï¼Œåˆ†åˆ«ç”¨äºç¦»æ•£å’Œè¿ç»­æ§åˆ¶ä»»åŠ¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†éƒ¨åˆ†ç¾¤ä¸å˜MDP (PI-MDP) çš„æ¦‚å¿µï¼Œå¹¶è®¾è®¡äº†ç›¸åº”çš„ç®—æ³•æ¡†æ¶ã€‚ä¸ä¼ ç»Ÿçš„ç¾¤ä¸å˜å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼ŒPI-MDP èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”å®é™…ç¯å¢ƒä¸­æ™®éå­˜åœ¨çš„å±€éƒ¨å¯¹ç§°æ€§ç ´ç¼ºç°è±¡ï¼Œä»è€Œæé«˜ç®—æ³•çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šPE-DQN å’Œ PE-SAC ç®—æ³•çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨ç¥ç»ç½‘ç»œæ¥è¿‘ä¼¼å¯¹ç§°æ€§æ£€æµ‹å™¨ï¼Œé€šè¿‡è®­ç»ƒæ¥å­¦ä¹ çŠ¶æ€-åŠ¨ä½œå¯¹çš„å¯¹ç§°æ€§ï¼›2) è®¾è®¡åˆé€‚çš„æŸå¤±å‡½æ•°ï¼Œé¼“åŠ±å¯¹ç§°æ€§æ£€æµ‹å™¨è¾“å‡ºå‡†ç¡®çš„å¯¹ç§°æ€§åˆ¤æ–­ç»“æœï¼›3) åœ¨ç¾¤ä¸å˜è´å°”æ›¼å¤‡ä»½ä¸­ä½¿ç”¨åˆé€‚çš„ç¾¤è¡¨ç¤ºï¼Œä»¥ä¿è¯ä»·å€¼å‡½æ•°çš„ç­‰å˜æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ Grid-Worldã€locomotion å’Œ manipulation ç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒPE-DQN å’Œ PE-SAC ç®—æ³•å‡æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨ locomotion ä»»åŠ¡ä¸­ï¼ŒPE-SAC ç®—æ³•çš„æ€§èƒ½æ¯” SAC ç®—æ³•æé«˜äº† 20% ä»¥ä¸Šï¼Œè¯æ˜äº†éƒ¨åˆ†ç­‰å˜å¼ºåŒ–å­¦ä¹ åœ¨å¯¹ç§°ç ´ç¼ºç¯å¢ƒä¸‹çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººæ§åˆ¶ã€æ¸¸æˆAIã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸï¼Œå°¤å…¶æ˜¯åœ¨è¿™äº›é¢†åŸŸä¸­å­˜åœ¨éƒ¨åˆ†å¯¹ç§°æ€§çš„åœºæ™¯ã€‚ä¾‹å¦‚ï¼Œåœ¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­ï¼Œæœºå™¨äººçš„æŸäº›å…³èŠ‚å¯èƒ½å…·æœ‰æ—‹è½¬å¯¹ç§°æ€§ï¼Œè€Œå…¶ä»–å…³èŠ‚åˆ™å—åˆ°ç‰©ç†é™åˆ¶ã€‚é€šè¿‡åˆ©ç”¨éƒ¨åˆ†ç­‰å˜å¼ºåŒ–å­¦ä¹ ï¼Œå¯ä»¥æé«˜æœºå™¨äººåœ¨è¿™äº›ä»»åŠ¡ä¸­çš„å­¦ä¹ æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä»è€Œé™ä½å¼€å‘æˆæœ¬å’Œæé«˜ç³»ç»Ÿæ€§èƒ½ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Group symmetries provide a powerful inductive bias for reinforcement learning (RL), enabling efficient generalization across symmetric states and actions via group-invariant Markov Decision Processes (MDPs). However, real-world environments almost never realize fully group-invariant MDPs; dynamics, actuation limits, and reward design usually break symmetries, often only locally. Under group-invariant Bellman backups for such cases, local symmetry-breaking introduces errors that propagate across the entire state-action space, resulting in global value estimation errors. To address this, we introduce Partially group-Invariant MDP (PI-MDP), which selectively applies group-invariant or standard Bellman backups depending on where symmetry holds. This framework mitigates error propagation from locally broken symmetries while maintaining the benefits of equivariance, thereby enhancing sample efficiency and generalizability. Building on this framework, we present practical RL algorithms -- Partially Equivariant (PE)-DQN for discrete control and PE-SAC for continuous control -- that combine the benefits of equivariance with robustness to symmetry-breaking. Experiments across Grid-World, locomotion, and manipulation benchmarks demonstrate that PE-DQN and PE-SAC significantly outperform baselines, highlighting the importance of selective symmetry exploitation for robust and sample-efficient RL.

