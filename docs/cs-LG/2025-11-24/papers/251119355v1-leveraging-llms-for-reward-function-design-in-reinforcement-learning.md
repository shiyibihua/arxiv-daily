---
layout: default
title: Leveraging LLMs for reward function design in reinforcement learning control tasks
---

# Leveraging LLMs for reward function design in reinforcement learning control tasks

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.19355" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.19355v1</a>
  <a href="https://arxiv.org/pdf/2511.19355.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.19355v1" onclick="toggleFavorite(this, '2511.19355v1', 'Leveraging LLMs for reward function design in reinforcement learning control tasks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Franklin Cardenoso, Wouter Caarls

**åˆ†ç±»**: cs.LG, cs.AI, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-11-24

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLEARN-Optï¼Œåˆ©ç”¨LLMè‡ªä¸»è®¾è®¡å¼ºåŒ–å­¦ä¹ æ§åˆ¶ä»»åŠ¡çš„å¥–åŠ±å‡½æ•°ï¼Œæ— éœ€äººå·¥å¹²é¢„ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `å¥–åŠ±å‡½æ•°è®¾è®¡` `å¤§å‹è¯­è¨€æ¨¡å‹` `è‡ªåŠ¨åŒ–` `æ— ç›‘ç£å­¦ä¹ ` `æœºå™¨äººæ§åˆ¶` `è‡ªä¸»ç³»ç»Ÿ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¼ºåŒ–å­¦ä¹ å¥–åŠ±å‡½æ•°è®¾è®¡ä¾èµ–äººå·¥ç»éªŒï¼Œè€—æ—¶ä¸”æ˜“å‡ºé”™ï¼Œç¼ºä¹è‡ªåŠ¨åŒ–æ–¹æ³•ã€‚
2. LEARN-Optåˆ©ç”¨LLMä»æ–‡æœ¬æè¿°ä¸­è‡ªä¸»ç”Ÿæˆã€è¯„ä¼°å’Œä¼˜åŒ–å¥–åŠ±å‡½æ•°ï¼Œæ— éœ€äººå·¥å¹²é¢„ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒLEARN-Optæ€§èƒ½åª²ç¾æˆ–ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸”èƒ½åˆ©ç”¨ä½æˆæœ¬LLMæ‰¾åˆ°é«˜æ€§èƒ½å¥–åŠ±å‡½æ•°ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨å¼ºåŒ–å­¦ä¹ (RL)ä¸­ï¼Œè®¾è®¡æœ‰æ•ˆçš„å¥–åŠ±å‡½æ•°æ˜¯ä¸€ä¸ªé‡è¦çš„ç“¶é¢ˆï¼Œé€šå¸¸éœ€è¦å¤§é‡çš„äººå·¥ä¸“ä¸šçŸ¥è¯†ï¼Œå¹¶ä¸”éå¸¸è€—æ—¶ã€‚å…ˆå‰çš„å·¥ä½œå’Œå¤§å‹è¯­è¨€æ¨¡å‹(LLM)çš„æœ€æ–°è¿›å±•å·²ç»è¯æ˜äº†å®ƒä»¬åœ¨è‡ªåŠ¨ç”Ÿæˆå¥–åŠ±å‡½æ•°æ–¹é¢çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•é€šå¸¸éœ€è¦åˆæ­¥çš„è¯„ä¼°æŒ‡æ ‡ã€äººå·¥è®¾è®¡çš„åé¦ˆæ¥æ”¹è¿›è¿‡ç¨‹ï¼Œæˆ–è€…ä½¿ç”¨ç¯å¢ƒæºä»£ç ä½œä¸ºä¸Šä¸‹æ–‡ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºLLMçš„å®Œå…¨è‡ªä¸»ä¸”æ¨¡å‹æ— å…³çš„æ¡†æ¶LEARN-Opt (LLM-based Evaluator and Analyzer for Reward functioN Optimization)ï¼Œå®ƒæ— éœ€åˆæ­¥æŒ‡æ ‡å’Œç¯å¢ƒæºä»£ç ä½œä¸ºä¸Šä¸‹æ–‡ï¼Œå³å¯ä»ç³»ç»Ÿå’Œä»»åŠ¡ç›®æ ‡çš„æ–‡æœ¬æè¿°ä¸­ç”Ÿæˆã€æ‰§è¡Œå’Œè¯„ä¼°å¥–åŠ±å‡½æ•°å€™é€‰ã€‚LEARN-Optçš„ä¸»è¦è´¡çŒ®åœ¨äºå®ƒèƒ½å¤Ÿè‡ªä¸»åœ°ä»ç³»ç»Ÿæè¿°å’Œä»»åŠ¡ç›®æ ‡ä¸­æ¨å¯¼å‡ºæ€§èƒ½æŒ‡æ ‡ï¼Œä»è€Œå®ç°å¯¹å¥–åŠ±å‡½æ•°çš„æ— ç›‘ç£è¯„ä¼°å’Œé€‰æ‹©ã€‚å®éªŒè¡¨æ˜ï¼ŒLEARN-Optçš„æ€§èƒ½ä¸EUREKAç­‰æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸å½“æˆ–æ›´å¥½ï¼ŒåŒæ—¶éœ€è¦æ›´å°‘çš„å…ˆéªŒçŸ¥è¯†ã€‚æˆ‘ä»¬å‘ç°è‡ªåŠ¨å¥–åŠ±è®¾è®¡æ˜¯ä¸€ä¸ªé«˜æ–¹å·®é—®é¢˜ï¼Œå¹³å‡æƒ…å†µä¸‹çš„å€™é€‰å¥–åŠ±å‡½æ•°ä¼šå¤±è´¥ï¼Œéœ€è¦å¤šæ¬¡è¿è¡Œæ‰èƒ½æ‰¾åˆ°æœ€ä½³å€™é€‰å¥–åŠ±å‡½æ•°ã€‚æœ€åï¼Œæˆ‘ä»¬è¡¨æ˜LEARN-Optå¯ä»¥é‡Šæ”¾ä½æˆæœ¬LLMçš„æ½œåŠ›ï¼Œæ‰¾åˆ°ä¸å¤§å‹æ¨¡å‹ç›¸å½“ç”šè‡³æ›´å¥½çš„é«˜æ€§èƒ½å€™é€‰å¥–åŠ±å‡½æ•°ã€‚è¿™ç§æ€§èƒ½è¯æ˜äº†å®ƒåœ¨ä¸éœ€è¦ä»»ä½•åˆæ­¥çš„äººå·¥å®šä¹‰çš„æŒ‡æ ‡çš„æƒ…å†µä¸‹ç”Ÿæˆé«˜è´¨é‡å¥–åŠ±å‡½æ•°çš„æ½œåŠ›ï¼Œä»è€Œå‡å°‘äº†å·¥ç¨‹å¼€é”€å¹¶å¢å¼ºäº†æ³›åŒ–æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¼ºåŒ–å­¦ä¹ ä¸­å¥–åŠ±å‡½æ•°è®¾è®¡é«˜åº¦ä¾èµ–äººå·¥ã€è€—æ—¶ä¸”ç¼ºä¹é€šç”¨æ€§çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦é¢„å®šä¹‰çš„è¯„ä¼°æŒ‡æ ‡ã€äººå·¥åé¦ˆæˆ–ç¯å¢ƒæºä»£ç ï¼Œé™åˆ¶äº†è‡ªåŠ¨åŒ–ç¨‹åº¦å’Œåº”ç”¨èŒƒå›´ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¼ºå¤§ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼Œä»ç³»ç»Ÿå’Œä»»åŠ¡çš„æ–‡æœ¬æè¿°ä¸­è‡ªåŠ¨æ¨å¯¼å‡ºå¥–åŠ±å‡½æ•°çš„è¯„ä¼°æŒ‡æ ‡ï¼Œä»è€Œå®ç°æ— ç›‘ç£çš„å¥–åŠ±å‡½æ•°ä¼˜åŒ–ã€‚è¿™ç§æ–¹æ³•é¿å…äº†äººå·¥å®šä¹‰æŒ‡æ ‡çš„éœ€è¦ï¼Œæé«˜äº†è‡ªåŠ¨åŒ–ç¨‹åº¦å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLEARN-Optæ¡†æ¶åŒ…å«ä»¥ä¸‹ä¸»è¦é˜¶æ®µï¼š1) LLMæ ¹æ®ç³»ç»Ÿæè¿°å’Œä»»åŠ¡ç›®æ ‡ç”Ÿæˆå¤šä¸ªå€™é€‰å¥–åŠ±å‡½æ•°ï¼›2) LLMè‡ªä¸»åœ°ä»ç³»ç»Ÿæè¿°å’Œä»»åŠ¡ç›®æ ‡ä¸­æå–æ€§èƒ½æŒ‡æ ‡ï¼›3) ä½¿ç”¨æå–çš„æ€§èƒ½æŒ‡æ ‡è¯„ä¼°æ¯ä¸ªå€™é€‰å¥–åŠ±å‡½æ•°ï¼›4) æ ¹æ®è¯„ä¼°ç»“æœé€‰æ‹©æœ€ä½³å¥–åŠ±å‡½æ•°ã€‚æ•´ä¸ªè¿‡ç¨‹æ— éœ€äººå·¥å¹²é¢„ï¼Œå®ç°äº†å®Œå…¨è‡ªä¸»çš„å¥–åŠ±å‡½æ•°è®¾è®¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šLEARN-Optçš„å…³é”®åˆ›æ–°åœ¨äºå…¶å®Œå…¨è‡ªä¸»çš„å¥–åŠ±å‡½æ•°ä¼˜åŒ–æµç¨‹ï¼Œæ— éœ€ä»»ä½•äººå·¥å®šä¹‰çš„è¯„ä¼°æŒ‡æ ‡æˆ–ç¯å¢ƒæºä»£ç ã€‚å®ƒé€šè¿‡LLMè‡ªä¸»åœ°ä»ç³»ç»Ÿæè¿°å’Œä»»åŠ¡ç›®æ ‡ä¸­æå–æ€§èƒ½æŒ‡æ ‡ï¼Œå®ç°äº†æ— ç›‘ç£çš„å¥–åŠ±å‡½æ•°è¯„ä¼°å’Œé€‰æ‹©ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•éœ€è¦äººå·¥å¹²é¢„æˆ–é¢„å®šä¹‰æŒ‡æ ‡å½¢æˆäº†é²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šLEARN-Optçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨LLMç”Ÿæˆå¤šæ ·åŒ–çš„å€™é€‰å¥–åŠ±å‡½æ•°ï¼Œæ¢ç´¢ä¸åŒçš„å¥–åŠ±ç­–ç•¥ï¼›2) è®¾è®¡æœ‰æ•ˆçš„LLMæç¤ºå·¥ç¨‹ï¼Œå¼•å¯¼LLMå‡†ç¡®æå–æ€§èƒ½æŒ‡æ ‡ï¼›3) é‡‡ç”¨å¤šè½®è¿è¡Œç­–ç•¥ï¼Œå…‹æœè‡ªåŠ¨å¥–åŠ±è®¾è®¡çš„é«˜æ–¹å·®é—®é¢˜ï¼Œæ‰¾åˆ°æ›´é²æ£’çš„å¥–åŠ±å‡½æ•°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒLEARN-Optåœ¨å¼ºåŒ–å­¦ä¹ æ§åˆ¶ä»»åŠ¡ä¸­å–å¾—äº†ä¸EUREKAç­‰å…ˆè¿›æ–¹æ³•ç›¸å½“æˆ–æ›´å¥½çš„æ€§èƒ½ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒLEARN-Optæ— éœ€ä»»ä½•äººå·¥å®šä¹‰çš„æŒ‡æ ‡ï¼Œå¹¶ä¸”èƒ½å¤Ÿåˆ©ç”¨ä½æˆæœ¬LLMæ‰¾åˆ°é«˜æ€§èƒ½çš„å¥–åŠ±å‡½æ•°ï¼Œè¯æ˜äº†å…¶åœ¨é™ä½å·¥ç¨‹å¼€é”€å’Œæé«˜æ³›åŒ–æ€§æ–¹é¢çš„æ½œåŠ›ã€‚å®éªŒè¿˜æ­ç¤ºäº†è‡ªåŠ¨å¥–åŠ±è®¾è®¡çš„é«˜æ–¹å·®ç‰¹æ€§ï¼Œå¼ºè°ƒäº†å¤šè½®è¿è¡Œçš„é‡è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºæœºå™¨äººæ§åˆ¶ã€æ¸¸æˆAIã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸï¼Œé™ä½å¼ºåŒ–å­¦ä¹ åº”ç”¨é—¨æ§›ï¼ŒåŠ é€Ÿæ™ºèƒ½ç³»ç»Ÿçš„å¼€å‘ã€‚é€šè¿‡è‡ªåŠ¨åŒ–å¥–åŠ±å‡½æ•°è®¾è®¡ï¼Œå¯ä»¥å‡å°‘å¯¹é¢†åŸŸä¸“å®¶çš„ä¾èµ–ï¼Œæé«˜å¼€å‘æ•ˆç‡ï¼Œå¹¶æ¢ç´¢æ›´ä¼˜çš„æ§åˆ¶ç­–ç•¥ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›æ‰©å±•åˆ°æ›´å¤æ‚çš„ä»»åŠ¡å’Œç¯å¢ƒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The challenge of designing effective reward functions in reinforcement learning (RL) represents a significant bottleneck, often requiring extensive human expertise and being time-consuming. Previous work and recent advancements in large language models (LLMs) have demonstrated their potential for automating the generation of reward functions. However, existing methodologies often require preliminary evaluation metrics, human-engineered feedback for the refinement process, or the use of environmental source code as context. To address these limitations, this paper introduces LEARN-Opt (LLM-based Evaluator and Analyzer for Reward functioN Optimization). This LLM-based, fully autonomous, and model-agnostic framework eliminates the need for preliminary metrics and environmental source code as context to generate, execute, and evaluate reward function candidates from textual descriptions of systems and task objectives. LEARN-Opt's main contribution lies in its ability to autonomously derive performance metrics directly from the system description and the task objective, enabling unsupervised evaluation and selection of reward functions. Our experiments indicate that LEARN-Opt achieves performance comparable to or better to that of state-of-the-art methods, such as EUREKA, while requiring less prior knowledge. We find that automated reward design is a high-variance problem, where the average-case candidate fails, requiring a multi-run approach to find the best candidates. Finally, we show that LEARN-Opt can unlock the potential of low-cost LLMs to find high-performing candidates that are comparable to, or even better than, those of larger models. This demonstrated performance affirms its potential to generate high-quality reward functions without requiring any preliminary human-defined metrics, thereby reducing engineering overhead and enhancing generalizability.

