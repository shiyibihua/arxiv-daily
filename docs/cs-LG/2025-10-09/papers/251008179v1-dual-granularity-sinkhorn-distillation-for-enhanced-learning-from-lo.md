---
layout: default
title: Dual-granularity Sinkhorn Distillation for Enhanced Learning from Long-tailed Noisy Data
---

# Dual-granularity Sinkhorn Distillation for Enhanced Learning from Long-tailed Noisy Data

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.08179" class="toolbar-btn" target="_blank">üìÑ arXiv: 2510.08179v1</a>
  <a href="https://arxiv.org/pdf/2510.08179.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.08179v1" onclick="toggleFavorite(this, '2510.08179v1', 'Dual-granularity Sinkhorn Distillation for Enhanced Learning from Long-tailed Noisy Data')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Feng Hong, Yu Huang, Zihua Zhao, Zhihan Zhou, Jiangchao Yao, Dongsheng Li, Ya Zhang, Yanfeng Wang

**ÂàÜÁ±ª**: cs.LG, cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-09

**Â§áÊ≥®**: 25 pages, 2 figures

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫ÂèåÁ≤íÂ∫¶SinkhornËí∏È¶è(D-SINK)Ê°ÜÊû∂ÔºåÊèêÂçáÈïøÂ∞æÂô™Â£∞Êï∞ÊçÆ‰∏ãÁöÑÊ®°ÂûãÂ≠¶‰π†ËÉΩÂäõ„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `ÈïøÂ∞æÂ≠¶‰π†` `Ê†áÁ≠æÂô™Â£∞` `Áü•ËØÜËí∏È¶è` `SinkhornÁÆóÊ≥ï` `ÊúÄ‰ºò‰º†Ëæì`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÂÆûÊï∞ÊçÆÈõÜÂ∏∏ÂêåÊó∂Â≠òÂú®Á±ªÂà´‰∏çÂπ≥Ë°°ÂíåÊ†áÁ≠æÂô™Â£∞ÔºåÁé∞ÊúâÊñπÊ≥ïÈöæ‰ª•ÊúâÊïàÁªìÂêàÂ§ÑÁêÜ„ÄÇ
2. D-SINKÊ°ÜÊû∂Âà©Áî®Á±ªÂà´‰∏çÂπ≥Ë°°ÂíåÊ†áÁ≠æÂô™Â£∞Âú®‰∏çÂêåÁ≤íÂ∫¶‰∏äÁöÑÁâπÊÄßÔºåÂçèÂêåÂà©Áî®Âº±ËæÖÂä©Ê®°Âûã„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåD-SINKÊòæËëóÊèêÂçá‰∫ÜÊ®°ÂûãÂú®ÈïøÂ∞æÂô™Â£∞Êï∞ÊçÆ‰∏äÁöÑÈ≤ÅÊ£íÊÄßÂíåÊÄßËÉΩ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Ê∑±Â∫¶Â≠¶‰π†ÁöÑÁé∞ÂÆû‰∏ñÁïåÊï∞ÊçÆÈõÜÁªèÂ∏∏Èù¢‰∏¥Á±ªÂà´‰∏çÂπ≥Ë°°ÂíåÊ†áÁ≠æÂô™Â£∞ÁöÑÂèåÈáçÊåëÊàòÔºåËøô‰ºöÈòªÁ¢çÊ®°ÂûãÊÄßËÉΩ„ÄÇËôΩÁÑ∂Â∑≤ÁªèÂ≠òÂú®ÈíàÂØπÊØè‰∏™ÈóÆÈ¢òÁöÑËß£ÂÜ≥ÊñπÊ≥ïÔºå‰ΩÜÊúâÊïàÂú∞ÁªìÂêàÂÆÉ‰ª¨Âπ∂ÈùûÊòì‰∫ãÔºåÂõ†‰∏∫Âå∫ÂàÜÁúüÂÆûÁöÑÂ∞æÈÉ®Ê†∑Êú¨ÂíåÂô™Â£∞Êï∞ÊçÆÈùûÂ∏∏Âõ∞ÈöæÔºåËøôÈÄöÂ∏∏‰ºöÂØºËá¥ÂÜ≤Á™ÅÁöÑ‰ºòÂåñÁ≠ñÁï•„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËßÜËßíÔºöÊàë‰ª¨Ê≤°Êúâ‰ªéÂ§¥ÂºÄÂßãÂºÄÂèëÊñ∞ÁöÑÂ§çÊùÇÊäÄÊúØÔºåËÄåÊòØÊé¢Á¥¢ÂçèÂêåÂà©Áî®Â∑≤Âª∫Á´ãÁöÑ„ÄÅÂçïÁã¨‚ÄúËæÉÂº±‚ÄùÁöÑËæÖÂä©Ê®°Âûã‚Äî‚ÄîËøô‰∫õÊ®°Âûã‰∏ìÈó®Áî®‰∫éËß£ÂÜ≥Á±ªÂà´‰∏çÂπ≥Ë°°ÊàñÊ†áÁ≠æÂô™Â£∞Ôºå‰ΩÜ‰∏çËÉΩÂêåÊó∂Ëß£ÂÜ≥‰∏§ËÄÖ„ÄÇËøôÁßçËßÇÁÇπÁöÑÂä®Êú∫ÊòØÔºåÁ±ªÂà´‰∏çÂπ≥Ë°°Ôºà‰∏ÄÁßçÂàÜÂ∏ÉÂ±ÇÈù¢ÁöÑÈóÆÈ¢òÔºâÂíåÊ†áÁ≠æÂô™Â£∞Ôºà‰∏ÄÁßçÊ†∑Êú¨Â±ÇÈù¢ÁöÑÈóÆÈ¢òÔºâÂú®‰∏çÂêåÁöÑÁ≤íÂ∫¶‰∏äËøê‰ΩúÔºåËøôÊÑèÂë≥ÁùÄÊØèÁßçÈ≤ÅÊ£íÊÄßÊú∫Âà∂ÂéüÂàô‰∏äÈÉΩÂèØ‰ª•Êèê‰æõ‰∫íË°•ÁöÑ‰ºòÂäøËÄå‰∏ç‰ºöÂèëÁîüÂÜ≤Á™Å„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜÂèåÁ≤íÂ∫¶SinkhornËí∏È¶èÔºàD-SINKÔºâÔºåËøôÊòØ‰∏Ä‰∏™Êñ∞È¢ñÁöÑÊ°ÜÊû∂ÔºåÈÄöËøá‰ªéËøôÁßç‚ÄúËæÉÂº±‚ÄùÁöÑ„ÄÅÂçï‰∏ÄÁî®ÈÄîÁöÑËæÖÂä©Ê®°Âûã‰∏≠ÊèêÂèñÂíåÊï¥Âêà‰∫íË°•ÁöÑËßÅËß£Êù•Â¢ûÂº∫ÂèåÈáçÈ≤ÅÊ£íÊÄß„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåD-SINK‰ΩøÁî®ÊúÄ‰ºò‰º†Ëæì‰ºòÂåñÁöÑÊõø‰ª£Ê†áÁ≠æÂàÜÈÖçÔºå‰ª•‰ΩøÁõÆÊ†áÊ®°ÂûãÁöÑÊ†∑Êú¨Á∫ßÈ¢ÑÊµã‰∏éÂô™Â£∞È≤ÅÊ£íÁöÑËæÖÂä©Ê®°ÂûãÂØπÈΩêÔºåÂπ∂‰ΩøÂÖ∂Á±ªÂà´ÂàÜÂ∏É‰∏é‰∏çÂπ≥Ë°°È≤ÅÊ£íÁöÑËæÖÂä©Ê®°ÂûãÂØπÈΩê„ÄÇÂú®Âü∫ÂáÜÊï∞ÊçÆÈõÜ‰∏äÁöÑÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåD-SINKÊòæÁùÄÊèêÈ´ò‰∫ÜÈ≤ÅÊ£íÊÄßÔºåÂπ∂Âú®‰ªéÈïøÂ∞æÂô™Â£∞Êï∞ÊçÆ‰∏≠Â≠¶‰π†ÊñπÈù¢ÂèñÂæó‰∫ÜÂº∫Â§ßÁöÑÁªèÈ™åÊÄßËÉΩ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Ê∑±Â∫¶Â≠¶‰π†‰∏≠ÈïøÂ∞æÂàÜÂ∏ÉÂíåÊ†áÁ≠æÂô™Â£∞ÂêåÊó∂Â≠òÂú®ÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏ÈíàÂØπÂçï‰∏ÄÈóÆÈ¢òËÆæËÆ°ÔºåÈöæ‰ª•ÊúâÊïàÁªìÂêàÔºå‰∏îÂÆπÊòìÂú®Âå∫ÂàÜÁúüÂÆûÂ∞æÈÉ®Ê†∑Êú¨ÂíåÂô™Â£∞Êï∞ÊçÆÊó∂‰∫ßÁîüÂÜ≤Á™ÅÁöÑ‰ºòÂåñÁ≠ñÁï•„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®Á±ªÂà´‰∏çÂπ≥Ë°°ÔºàÂàÜÂ∏ÉÂ±ÇÈù¢ÔºâÂíåÊ†áÁ≠æÂô™Â£∞ÔºàÊ†∑Êú¨Â±ÇÈù¢ÔºâÂú®‰∏çÂêåÁ≤íÂ∫¶‰∏äËøê‰ΩúÁöÑÁâπÊÄßÔºåÂçèÂêåÂà©Áî®‰∏§‰∏™‚ÄúËæÉÂº±‚ÄùÁöÑËæÖÂä©Ê®°ÂûãÔºå‰∏Ä‰∏™‰∏ìÊ≥®‰∫éÂ§ÑÁêÜÁ±ªÂà´‰∏çÂπ≥Ë°°ÔºåÂè¶‰∏Ä‰∏™‰∏ìÊ≥®‰∫éÂ§ÑÁêÜÊ†áÁ≠æÂô™Â£∞„ÄÇÈÄöËøáÁü•ËØÜËí∏È¶èÔºåÂ∞ÜËøô‰∏§‰∏™Ê®°ÂûãÁöÑ‰∫íË°•‰ºòÂäø‰º†ÈÄíÁªôÁõÆÊ†áÊ®°ÂûãÔºå‰ªéËÄåÂ¢ûÂº∫ÂÖ∂È≤ÅÊ£íÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöD-SINKÊ°ÜÊû∂ÂåÖÂê´‰∏â‰∏™‰∏ªË¶ÅÈÉ®ÂàÜÔºöÁõÆÊ†áÊ®°Âûã„ÄÅÁ±ªÂà´‰∏çÂπ≥Ë°°È≤ÅÊ£íËæÖÂä©Ê®°ÂûãÂíåÂô™Â£∞È≤ÅÊ£íËæÖÂä©Ê®°Âûã„ÄÇÈ¶ñÂÖàÔºåÂàÜÂà´ËÆ≠ÁªÉ‰∏§‰∏™ËæÖÂä©Ê®°Âûã„ÄÇÁÑ∂ÂêéÔºåÂà©Áî®SinkhornÁÆóÊ≥ïËÆ°ÁÆóÊúÄ‰ºò‰º†ËæìÁü©ÈòµÔºåËØ•Áü©ÈòµÁî®‰∫éÂ∞ÜËæÖÂä©Ê®°ÂûãÁöÑÈ¢ÑÊµãÁªìÊûúÂàÜÈÖçÁªôÁõÆÊ†áÊ®°Âûã„ÄÇÊúÄÂêéÔºåÈÄöËøáÁü•ËØÜËí∏È¶èÊçüÂ§±ÂáΩÊï∞Ôºå‰ΩøÁõÆÊ†áÊ®°ÂûãÁöÑÈ¢ÑÊµãÁªìÊûú‰∏éËæÖÂä©Ê®°ÂûãÁöÑÈ¢ÑÊµãÁªìÊûúÂØπÈΩê„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöD-SINKÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂÖ∂ÂèåÁ≤íÂ∫¶Ëí∏È¶èÁ≠ñÁï•„ÄÇÂÆÉ‰∏çÊòØÁõ¥Êé•‰ªéÂéüÂßãÊï∞ÊçÆ‰∏≠Â≠¶‰π†ÔºåËÄåÊòØ‰ªé‰∏§‰∏™ÂÖ∑Êúâ‰∫íË°•‰ºòÂäøÁöÑËæÖÂä©Ê®°Âûã‰∏≠Â≠¶‰π†„ÄÇËøôÁßçÊñπÊ≥ïËÉΩÂ§üÊõ¥Â•ΩÂú∞Â§ÑÁêÜÈïøÂ∞æÂàÜÂ∏ÉÂíåÊ†áÁ≠æÂô™Â£∞Â∏¶Êù•ÁöÑÊåëÊàòÔºåÈÅøÂÖç‰∫ÜÂçï‰∏ÄÊ®°ÂûãÂú®‰ºòÂåñËøáÁ®ã‰∏≠ÂèØËÉΩÂá∫Áé∞ÁöÑÂÜ≤Á™Å„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöD-SINKÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ‰ΩøÁî®SinkhornÁÆóÊ≥ïËøõË°åÊúÄ‰ºò‰º†ËæìÔºå‰ª•ÂÆûÁé∞Ê†∑Êú¨Á∫ßÂà´ÁöÑÂØπÈΩêÔºõ2) ËÆæËÆ°‰∫ÜÁü•ËØÜËí∏È¶èÊçüÂ§±ÂáΩÊï∞ÔºåËØ•ÂáΩÊï∞ÂêåÊó∂ËÄÉËôë‰∫ÜÊ†∑Êú¨Á∫ßÂà´ÁöÑÈ¢ÑÊµãÂØπÈΩêÂíåÁ±ªÂà´ÂàÜÂ∏ÉÁöÑÂØπÈΩêÔºõ3) ËæÖÂä©Ê®°ÂûãÁöÑÈÄâÊã©ÔºåÈúÄË¶Å‰øùËØÅ‰∏Ä‰∏™Ê®°ÂûãÂØπÁ±ªÂà´‰∏çÂπ≥Ë°°ÂÖ∑ÊúâÈ≤ÅÊ£íÊÄßÔºåÂè¶‰∏Ä‰∏™Ê®°ÂûãÂØπÊ†áÁ≠æÂô™Â£∞ÂÖ∑ÊúâÈ≤ÅÊ£íÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåD-SINKÂú®Â§ö‰∏™Âü∫ÂáÜÊï∞ÊçÆÈõÜ‰∏äÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇ‰æãÂ¶ÇÔºåÂú®CIFAR-10Êï∞ÊçÆÈõÜ‰∏äÔºåD-SINKÁöÑÊÄßËÉΩÊèêÂçá‰∫Ü5%‰ª•‰∏ä„ÄÇÊ≠§Â§ñÔºåD-SINKÂú®ÈïøÂ∞æÂàÜÂ∏ÉÂíåÊ†áÁ≠æÂô™Â£∞ÂêåÊó∂Â≠òÂú®ÁöÑÂú∫ÊôØ‰∏ãÔºåË°®Áé∞Âá∫Êõ¥Âº∫ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂõæÂÉèËØÜÂà´„ÄÅËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÁ≠âÈ¢ÜÂüüÔºåÂ∞§ÂÖ∂ÊòØÂú®Êï∞ÊçÆË¥®Èáè‰∏çÈ´ò„ÄÅÁ±ªÂà´ÂàÜÂ∏É‰∏çÂπ≥Ë°°ÁöÑÂú∫ÊôØ‰∏ãÔºå‰æãÂ¶ÇÂåªÁñóËØäÊñ≠„ÄÅÈáëËûçÈ£éÊéßÁ≠â„ÄÇÈÄöËøáÊèêÂçáÊ®°ÂûãÂú®ÈïøÂ∞æÂô™Â£∞Êï∞ÊçÆ‰∏äÁöÑÂ≠¶‰π†ËÉΩÂäõÔºåÂèØ‰ª•ÊèêÈ´òÊ®°ÂûãÁöÑÊ≥õÂåñÊÄßËÉΩÂíåÂÆûÈôÖÂ∫îÁî®‰ª∑ÂÄºÔºåÈôç‰ΩéÊï∞ÊçÆÊ†áÊ≥®ÊàêÊú¨„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Real-world datasets for deep learning frequently suffer from the co-occurring challenges of class imbalance and label noise, hindering model performance. While methods exist for each issue, effectively combining them is non-trivial, as distinguishing genuine tail samples from noisy data proves difficult, often leading to conflicting optimization strategies. This paper presents a novel perspective: instead of primarily developing new complex techniques from scratch, we explore synergistically leveraging well-established, individually 'weak' auxiliary models - specialized for tackling either class imbalance or label noise but not both. This view is motivated by the insight that class imbalance (a distributional-level concern) and label noise (a sample-level concern) operate at different granularities, suggesting that robustness mechanisms for each can in principle offer complementary strengths without conflict. We propose Dual-granularity Sinkhorn Distillation (D-SINK), a novel framework that enhances dual robustness by distilling and integrating complementary insights from such 'weak', single-purpose auxiliary models. Specifically, D-SINK uses an optimal transport-optimized surrogate label allocation to align the target model's sample-level predictions with a noise-robust auxiliary and its class distributions with an imbalance-robust one. Extensive experiments on benchmark datasets demonstrate that D-SINK significantly improves robustness and achieves strong empirical performance in learning from long-tailed noisy data.

