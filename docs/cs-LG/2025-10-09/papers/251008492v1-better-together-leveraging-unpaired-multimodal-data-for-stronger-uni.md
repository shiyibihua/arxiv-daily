---
layout: default
title: Better Together: Leveraging Unpaired Multimodal Data for Stronger Unimodal Models
---

# Better Together: Leveraging Unpaired Multimodal Data for Stronger Unimodal Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.08492" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.08492v1</a>
  <a href="https://arxiv.org/pdf/2510.08492.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.08492v1" onclick="toggleFavorite(this, '2510.08492v1', 'Better Together: Leveraging Unpaired Multimodal Data for Stronger Unimodal Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sharut Gupta, Shobhita Sundaram, Chenyu Wang, Stefanie Jegelka, Phillip Isola

**åˆ†ç±»**: cs.LG, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-09

**å¤‡æ³¨**: 63 pages, 29 tables, and 47 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºUMLï¼Œåˆ©ç”¨éé…å¯¹å¤šæ¨¡æ€æ•°æ®å¢å¼ºå•æ¨¡æ€æ¨¡å‹è¡¨ç¤ºå­¦ä¹ **

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `éé…å¯¹å­¦ä¹ ` `å¤šæ¨¡æ€å­¦ä¹ ` `è¡¨ç¤ºå­¦ä¹ ` `å•æ¨¡æ€æ¨¡å‹` `è·¨æ¨¡æ€èåˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¾èµ–é…å¯¹å¤šæ¨¡æ€æ•°æ®ï¼Œé™åˆ¶äº†å…¶åº”ç”¨èŒƒå›´å’Œæ•°æ®æ•ˆç‡ï¼Œéš¾ä»¥æœ‰æ•ˆåˆ©ç”¨å¤§é‡éé…å¯¹æ•°æ®ã€‚
2. UMLé€šè¿‡å‚æ•°å…±äº«ï¼Œäº¤æ›¿å¤„ç†ä¸åŒæ¨¡æ€çš„éé…å¯¹æ•°æ®ï¼Œå­¦ä¹ è·¨æ¨¡æ€ç»“æ„ï¼Œæå‡å•æ¨¡æ€è¡¨ç¤ºã€‚
3. å®éªŒè¡¨æ˜ï¼ŒUMLåˆ©ç”¨éé…å¯¹æ–‡æœ¬ã€éŸ³é¢‘ã€å›¾åƒæ•°æ®ï¼Œæ˜¾è‘—æå‡äº†å›¾åƒå’ŒéŸ³é¢‘ä»»åŠ¡çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¼ ç»Ÿçš„å¤šæ¨¡æ€å­¦ä¹ å™¨ä¸“æ³¨äºä¸ºè§†è§‰é—®ç­”ç­‰ä»»åŠ¡å¯»æ‰¾ç»Ÿä¸€çš„è¡¨ç¤ºï¼Œä½†ä¸¥é‡ä¾èµ–äºé…å¯¹æ•°æ®é›†ã€‚ç„¶è€Œï¼Œä¸€ä¸ªè¢«å¿½è§†ä½†å¯èƒ½å¼ºå¤§çš„é—®é¢˜æ˜¯ï¼šèƒ½å¦åˆ©ç”¨è¾…åŠ©çš„éé…å¯¹å¤šæ¨¡æ€æ•°æ®æ¥ç›´æ¥å¢å¼ºç›®æ ‡æ¨¡æ€ä¸­çš„è¡¨ç¤ºå­¦ä¹ ï¼Ÿæˆ‘ä»¬æå‡ºäº†UMLï¼šéé…å¯¹å¤šæ¨¡æ€å­¦ä¹ å™¨ï¼Œè¿™æ˜¯ä¸€ç§æ¨¡æ€æ— å…³çš„è®­ç»ƒèŒƒå¼ï¼Œå…¶ä¸­å•ä¸ªæ¨¡å‹äº¤æ›¿å¤„ç†æ¥è‡ªä¸åŒæ¨¡æ€çš„è¾“å…¥ï¼ŒåŒæ—¶åœ¨å®ƒä»¬ä¹‹é—´å…±äº«å‚æ•°ã€‚è¿™ç§è®¾è®¡åˆ©ç”¨äº†ä¸åŒæ¨¡æ€æ˜¯å…±äº«åº•å±‚ç°å®çš„æŠ•å½±è¿™ä¸€å‡è®¾ï¼Œå…è®¸æ¨¡å‹ä»è·¨æ¨¡æ€ç»“æ„ä¸­å—ç›Šï¼Œè€Œæ— éœ€æ˜¾å¼é…å¯¹ã€‚ç†è®ºä¸Šï¼Œåœ¨çº¿æ€§æ•°æ®ç”Ÿæˆå‡è®¾ä¸‹ï¼Œæˆ‘ä»¬è¯æ˜äº†éé…å¯¹è¾…åŠ©æ•°æ®å¯ä»¥äº§ç”Ÿæ¯”å•æ¨¡æ€è®­ç»ƒæ›´å…·ä¿¡æ¯é‡çš„æ•°æ®ç”Ÿæˆè¿‡ç¨‹è¡¨ç¤ºã€‚åœ¨å®éªŒä¸Šï¼Œæˆ‘ä»¬è¡¨æ˜ä½¿ç”¨æ¥è‡ªè¾…åŠ©æ¨¡æ€ï¼ˆå¦‚æ–‡æœ¬ã€éŸ³é¢‘æˆ–å›¾åƒï¼‰çš„éé…å¯¹æ•°æ®å¯ä»¥æŒç»­æé«˜å„ç§å•æ¨¡æ€ç›®æ ‡ï¼ˆå¦‚å›¾åƒå’ŒéŸ³é¢‘ï¼‰çš„ä¸‹æ¸¸æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¦‚ä½•åˆ©ç”¨å¤§é‡å­˜åœ¨çš„éé…å¯¹å¤šæ¨¡æ€æ•°æ®æ¥æå‡å•æ¨¡æ€æ¨¡å‹çš„è¡¨ç¤ºå­¦ä¹ èƒ½åŠ›çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºé…å¯¹çš„å¤šæ¨¡æ€æ•°æ®ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬çš„åº”ç”¨èŒƒå›´ï¼Œå¹¶ä¸”æ— æ³•æœ‰æ•ˆåˆ©ç”¨äº’è”ç½‘ä¸Šå¤§é‡çš„éé…å¯¹æ•°æ®ã€‚ç°æœ‰æ–¹æ³•çš„ç—›ç‚¹åœ¨äºæ•°æ®è·å–æˆæœ¬é«˜æ˜‚ï¼Œä¸”æ¨¡å‹æ³›åŒ–èƒ½åŠ›å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å‡è®¾ä¸åŒæ¨¡æ€çš„æ•°æ®æ˜¯åŒä¸€ä¸ªåº•å±‚ç°å®çš„ä¸åŒæŠ•å½±ã€‚å› æ­¤ï¼Œå³ä½¿æ•°æ®æ˜¯éé…å¯¹çš„ï¼Œæ¨¡å‹ä¹Ÿå¯ä»¥é€šè¿‡å­¦ä¹ ä¸åŒæ¨¡æ€ä¹‹é—´çš„å…±äº«ç»“æ„æ¥æå‡å¯¹å•æ¨¡æ€æ•°æ®çš„ç†è§£ã€‚é€šè¿‡åœ¨ä¸åŒæ¨¡æ€ä¹‹é—´å…±äº«å‚æ•°ï¼Œæ¨¡å‹å¯ä»¥ä»å…¶ä»–æ¨¡æ€çš„çŸ¥è¯†ä¸­å—ç›Šï¼Œä»è€Œæå‡è‡ªèº«çš„è¡¨ç¤ºå­¦ä¹ èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šUML (Unpaired Multimodal Learner) çš„æ•´ä½“æ¡†æ¶åŒ…å«ä¸€ä¸ªå…±äº«çš„ç¼–ç å™¨å’Œä¸€ä¸ªé’ˆå¯¹ä¸åŒæ¨¡æ€çš„ç‰¹å®šè§£ç å™¨ã€‚æ¨¡å‹äº¤æ›¿åœ°æ¥æ”¶æ¥è‡ªä¸åŒæ¨¡æ€çš„è¾“å…¥ï¼Œå¹¶ä½¿ç”¨å…±äº«ç¼–ç å™¨æå–ç‰¹å¾è¡¨ç¤ºã€‚ç„¶åï¼Œä½¿ç”¨ç‰¹å®šæ¨¡æ€çš„è§£ç å™¨å°†ç‰¹å¾è¡¨ç¤ºé‡æ„ä¸ºåŸå§‹è¾“å…¥ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹å¯ä»¥å­¦ä¹ åˆ°ä¸åŒæ¨¡æ€ä¹‹é—´çš„å…±äº«ç»“æ„ï¼Œå¹¶æå‡å¯¹å•æ¨¡æ€æ•°æ®çš„ç†è§£ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†ä¸€ä¸ªæ¨¡æ€æ— å…³çš„è®­ç»ƒèŒƒå¼ï¼Œè¯¥èŒƒå¼å…è®¸æ¨¡å‹åˆ©ç”¨éé…å¯¹çš„å¤šæ¨¡æ€æ•°æ®æ¥æå‡å•æ¨¡æ€æ¨¡å‹çš„è¡¨ç¤ºå­¦ä¹ èƒ½åŠ›ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒUML ä¸éœ€è¦é…å¯¹çš„æ•°æ®ï¼Œå› æ­¤å¯ä»¥åˆ©ç”¨æ›´å¤šçš„æ•°æ®æ¥è®­ç»ƒæ¨¡å‹ã€‚æ­¤å¤–ï¼ŒUML é€šè¿‡å‚æ•°å…±äº«æ¥å­¦ä¹ ä¸åŒæ¨¡æ€ä¹‹é—´çš„å…±äº«ç»“æ„ï¼Œä»è€Œæå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šUML çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨å…±äº«ç¼–ç å™¨æ¥æå–ä¸åŒæ¨¡æ€çš„ç‰¹å¾è¡¨ç¤ºï¼›2) ä½¿ç”¨ç‰¹å®šæ¨¡æ€çš„è§£ç å™¨æ¥é‡æ„åŸå§‹è¾“å…¥ï¼›3) ä½¿ç”¨äº¤æ›¿è®­ç»ƒçš„æ–¹å¼æ¥è®­ç»ƒæ¨¡å‹ï¼Œå³æ¯æ¬¡åªä½¿ç”¨ä¸€ä¸ªæ¨¡æ€çš„æ•°æ®æ¥æ›´æ–°æ¨¡å‹å‚æ•°ï¼›4) ä½¿ç”¨é‡æ„æŸå¤±å‡½æ•°æ¥è¡¡é‡æ¨¡å‹é‡æ„åŸå§‹è¾“å…¥çš„èƒ½åŠ›ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®å–å†³äºå…·ä½“çš„ä»»åŠ¡å’Œæ•°æ®é›†ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒUMLåœ¨å›¾åƒå’ŒéŸ³é¢‘ä»»åŠ¡ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨ImageNetå›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šï¼Œä½¿ç”¨éé…å¯¹çš„æ–‡æœ¬æ•°æ®ä½œä¸ºè¾…åŠ©ä¿¡æ¯ï¼ŒUMLçš„æ€§èƒ½æå‡äº†å¤šä¸ªç™¾åˆ†ç‚¹ã€‚åœ¨éŸ³é¢‘åˆ†ç±»ä»»åŠ¡ä¸Šï¼Œä½¿ç”¨éé…å¯¹çš„å›¾åƒæ•°æ®ä½œä¸ºè¾…åŠ©ä¿¡æ¯ï¼ŒUMLä¹Ÿå–å¾—äº†ç±»ä¼¼çš„æ€§èƒ½æå‡ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒUMLå¯ä»¥æœ‰æ•ˆåœ°åˆ©ç”¨éé…å¯¹çš„å¤šæ¨¡æ€æ•°æ®æ¥æå‡å•æ¨¡æ€æ¨¡å‹çš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºè®¡ç®—æœºè§†è§‰ã€è¯­éŸ³è¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œå¯ä»¥åˆ©ç”¨éé…å¯¹çš„å›¾åƒå’Œæ–‡æœ¬æ•°æ®æ¥æå‡å›¾åƒåˆ†ç±»æˆ–æ–‡æœ¬åˆ†ç±»æ¨¡å‹çš„æ€§èƒ½ã€‚åœ¨æœºå™¨äººé¢†åŸŸï¼Œå¯ä»¥åˆ©ç”¨éé…å¯¹çš„è§†è§‰å’Œè§¦è§‰æ•°æ®æ¥æå‡æœºå™¨äººçš„æ„ŸçŸ¥èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥åº”ç”¨äºåŒ»ç–—å½±åƒåˆ†æã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿é˜”çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Traditional multimodal learners find unified representations for tasks like visual question answering, but rely heavily on paired datasets. However, an overlooked yet potentially powerful question is: can one leverage auxiliary unpaired multimodal data to directly enhance representation learning in a target modality? We introduce UML: Unpaired Multimodal Learner, a modality-agnostic training paradigm in which a single model alternately processes inputs from different modalities while sharing parameters across them. This design exploits the assumption that different modalities are projections of a shared underlying reality, allowing the model to benefit from cross-modal structure without requiring explicit pairs. Theoretically, under linear data-generating assumptions, we show that unpaired auxiliary data can yield representations strictly more informative about the data-generating process than unimodal training. Empirically, we show that using unpaired data from auxiliary modalities -- such as text, audio, or images -- consistently improves downstream performance across diverse unimodal targets such as image and audio. Our project page: https://unpaired-multimodal.github.io/

