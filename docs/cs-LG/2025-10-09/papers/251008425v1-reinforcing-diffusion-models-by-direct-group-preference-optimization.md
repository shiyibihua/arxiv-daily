---
layout: default
title: Reinforcing Diffusion Models by Direct Group Preference Optimization
---

# Reinforcing Diffusion Models by Direct Group Preference Optimization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.08425" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.08425v1</a>
  <a href="https://arxiv.org/pdf/2510.08425.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.08425v1" onclick="toggleFavorite(this, '2510.08425v1', 'Reinforcing Diffusion Models by Direct Group Preference Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yihong Luo, Tianyang Hu, Jing Tang

**åˆ†ç±»**: cs.LG, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-09

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/Luo-Yihong/DGPO)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç›´æ¥ç¾¤ä½“åå¥½ä¼˜åŒ–(DGPO)ï¼ŒåŠ é€Ÿå¹¶æå‡æ‰©æ•£æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ‰©æ•£æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `ç¾¤ä½“åå¥½ä¼˜åŒ–` `ç¡®å®šæ€§é‡‡æ ·` `åœ¨çº¿å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨æ‰©æ•£æ¨¡å‹ä¸Šåº”ç”¨å¼ºåŒ–å­¦ä¹ æ—¶ï¼Œä¾èµ–ä½æ•ˆçš„SDEé‡‡æ ·å™¨å¼•å…¥éšæœºæ€§ï¼Œå¯¼è‡´è®­ç»ƒç¼“æ…¢ã€‚
2. DGPOç›´æ¥ä»ç¾¤ä½“åå¥½å­¦ä¹ ï¼Œæ— éœ€ç­–ç•¥æ¢¯åº¦ï¼Œä»è€Œå¯ä»¥ä½¿ç”¨é«˜æ•ˆçš„ç¡®å®šæ€§ODEé‡‡æ ·å™¨ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒDGPOæ¯”ç°æœ‰æ–¹æ³•å¿«20å€ï¼Œå¹¶åœ¨é¢†åŸŸå†…å’Œé¢†åŸŸå¤–å¥–åŠ±æŒ‡æ ‡ä¸Šè¡¨ç°æ›´ä¼˜ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡ç¾¤ä½“ç›¸å¯¹åå¥½ä¼˜åŒ–(GRPO)ç­‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•æ˜¾è‘—æå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä½†å°†å…¶åº”ç”¨äºæ‰©æ•£æ¨¡å‹ä»ç„¶å……æ»¡æŒ‘æˆ˜ã€‚ç‰¹åˆ«æ˜¯ï¼ŒGRPOéœ€è¦ä¸€ä¸ªéšæœºç­–ç•¥ï¼Œè€Œæœ€å…·æˆæœ¬æ•ˆç›Šçš„æ‰©æ•£é‡‡æ ·å™¨æ˜¯åŸºäºç¡®å®šæ€§ODEçš„ã€‚æœ€è¿‘çš„å·¥ä½œé€šè¿‡ä½¿ç”¨ä½æ•ˆçš„åŸºäºSDEçš„é‡‡æ ·å™¨æ¥å¼•å…¥éšæœºæ€§æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†è¿™ç§å¯¹æ¨¡å‹æ— å…³çš„é«˜æ–¯å™ªå£°çš„ä¾èµ–å¯¼è‡´æ”¶æ•›é€Ÿåº¦ç¼“æ…¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªå†²çªï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ ç®—æ³•â€”â€”ç›´æ¥ç¾¤ä½“åå¥½ä¼˜åŒ–(DGPO)ï¼Œå®ƒå®Œå…¨æ‘’å¼ƒäº†ç­–ç•¥æ¢¯åº¦æ¡†æ¶ã€‚DGPOç›´æ¥ä»ç¾¤ä½“å±‚é¢çš„åå¥½ä¸­å­¦ä¹ ï¼Œåˆ©ç”¨ç¾¤ä½“å†…æ ·æœ¬çš„ç›¸å¯¹ä¿¡æ¯ã€‚è¿™ç§è®¾è®¡æ¶ˆé™¤äº†å¯¹ä½æ•ˆéšæœºç­–ç•¥çš„éœ€æ±‚ï¼Œä»è€Œå¯ä»¥ä½¿ç”¨é«˜æ•ˆçš„ç¡®å®šæ€§ODEé‡‡æ ·å™¨å¹¶åŠ å¿«è®­ç»ƒé€Ÿåº¦ã€‚å¤§é‡ç»“æœè¡¨æ˜ï¼ŒDGPOçš„è®­ç»ƒé€Ÿåº¦æ¯”ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•å¿«çº¦20å€ï¼Œå¹¶ä¸”åœ¨é¢†åŸŸå†…å’Œé¢†åŸŸå¤–çš„å¥–åŠ±æŒ‡æ ‡ä¸Šéƒ½å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ã€‚ä»£ç å¯åœ¨https://github.com/Luo-Yihong/DGPOè·å–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ–¹æ³•åœ¨å°†å¼ºåŒ–å­¦ä¹ åº”ç”¨äºæ‰©æ•£æ¨¡å‹æ—¶ï¼Œé¢ä¸´ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ï¼šGRPOç­‰ç®—æ³•éœ€è¦éšæœºç­–ç•¥ï¼Œè€Œæ‰©æ•£æ¨¡å‹ä¸­æœ€æœ‰æ•ˆçš„é‡‡æ ·å™¨æ˜¯åŸºäºç¡®å®šæ€§å¸¸å¾®åˆ†æ–¹ç¨‹(ODE)çš„ã€‚ä¸ºäº†æ»¡è¶³GRPOçš„éœ€æ±‚ï¼Œä¸€äº›å·¥ä½œå°è¯•ä½¿ç”¨åŸºäºéšæœºå¾®åˆ†æ–¹ç¨‹(SDE)çš„é‡‡æ ·å™¨æ¥å¼•å…¥éšæœºæ€§ï¼Œä½†è¿™ç§æ–¹æ³•æ•ˆç‡ä½ä¸‹ï¼Œæ”¶æ•›é€Ÿåº¦æ…¢ã€‚å› æ­¤ï¼Œå¦‚ä½•é«˜æ•ˆåœ°å°†å¼ºåŒ–å­¦ä¹ åº”ç”¨äºæ‰©æ•£æ¨¡å‹ï¼ŒåŒæ—¶é¿å…ä½¿ç”¨ä½æ•ˆçš„SDEé‡‡æ ·å™¨ï¼Œæ˜¯ä¸€ä¸ªäºŸå¾…è§£å†³çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDGPOçš„æ ¸å¿ƒæ€è·¯æ˜¯ç›´æ¥ä»ç¾¤ä½“å±‚é¢çš„åå¥½ä¸­å­¦ä¹ ï¼Œè€Œæ— éœ€æ˜¾å¼åœ°æ„å»ºå’Œä¼˜åŒ–ç­–ç•¥ã€‚å®ƒåˆ©ç”¨ç¾¤ä½“å†…æ ·æœ¬çš„ç›¸å¯¹ä¿¡æ¯ï¼Œé€šè¿‡æ¯”è¾ƒä¸åŒæ ·æœ¬çš„ä¼˜åŠ£æ¥æŒ‡å¯¼æ¨¡å‹çš„è®­ç»ƒã€‚è¿™ç§æ–¹æ³•é¿å…äº†å¯¹éšæœºç­–ç•¥çš„ä¾èµ–ï¼Œä»è€Œå¯ä»¥ä½¿ç”¨é«˜æ•ˆçš„ç¡®å®šæ€§ODEé‡‡æ ·å™¨ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDGPOçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼š1) ä»æ‰©æ•£æ¨¡å‹ä¸­é‡‡æ ·ä¸€ç»„æ ·æœ¬ï¼›2) æ ¹æ®å¥–åŠ±å‡½æ•°æˆ–äººç±»åé¦ˆï¼Œå¯¹è¿™äº›æ ·æœ¬è¿›è¡Œæ’åºï¼Œå½¢æˆç¾¤ä½“åå¥½ï¼›3) ä½¿ç”¨ç¾¤ä½“åå¥½ä¿¡æ¯ï¼Œç›´æ¥æ›´æ–°æ‰©æ•£æ¨¡å‹çš„å‚æ•°ï¼Œè€Œæ— éœ€è®¡ç®—ç­–ç•¥æ¢¯åº¦ã€‚DGPOç®—æ³•æ˜¯ä¸€ä¸ªåœ¨çº¿å­¦ä¹ ç®—æ³•ï¼Œè¿™æ„å‘³ç€å®ƒå¯ä»¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸æ–­åœ°æ”¶é›†æ•°æ®å¹¶æ›´æ–°æ¨¡å‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šDGPOæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå®ƒæ‘’å¼ƒäº†ä¼ ç»Ÿçš„ç­–ç•¥æ¢¯åº¦æ¡†æ¶ï¼Œç›´æ¥ä»ç¾¤ä½“åå¥½ä¸­å­¦ä¹ ã€‚è¿™ä½¿å¾—å®ƒèƒ½å¤Ÿé¿å…å¯¹éšæœºç­–ç•¥çš„ä¾èµ–ï¼Œä»è€Œå¯ä»¥ä½¿ç”¨é«˜æ•ˆçš„ç¡®å®šæ€§ODEé‡‡æ ·å™¨ã€‚æ­¤å¤–ï¼ŒDGPOè¿˜åˆ©ç”¨äº†ç¾¤ä½“å†…æ ·æœ¬çš„ç›¸å¯¹ä¿¡æ¯ï¼Œè¿™æœ‰åŠ©äºæé«˜å­¦ä¹ æ•ˆç‡å’Œç¨³å®šæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šDGPOçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨ç¾¤ä½“åå¥½æŸå¤±å‡½æ•°ï¼Œè¯¥å‡½æ•°é¼“åŠ±æ¨¡å‹ç”Ÿæˆæ›´ç¬¦åˆç¾¤ä½“åå¥½çš„æ ·æœ¬ï¼›2) ä½¿ç”¨é«˜æ•ˆçš„ç¡®å®šæ€§ODEé‡‡æ ·å™¨ï¼Œä¾‹å¦‚DDIMæˆ–PLMSï¼Œæ¥ç”Ÿæˆæ ·æœ¬ï¼›3) ä½¿ç”¨åœ¨çº¿å­¦ä¹ çš„æ–¹å¼ï¼Œä¸æ–­åœ°æ”¶é›†æ•°æ®å¹¶æ›´æ–°æ¨¡å‹ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å½¢å¼å’Œä¼˜åŒ–ç®—æ³•å¯ä»¥æ ¹æ®å…·ä½“çš„åº”ç”¨åœºæ™¯è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

DGPOåœ¨å®éªŒä¸­è¡¨ç°å‡ºè‰²ï¼Œè®­ç»ƒé€Ÿåº¦æ¯”ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•å¿«çº¦20å€ï¼Œå¹¶ä¸”åœ¨é¢†åŸŸå†…å’Œé¢†åŸŸå¤–çš„å¥–åŠ±æŒ‡æ ‡ä¸Šéƒ½å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ã€‚è¿™è¡¨æ˜DGPOèƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨ç¾¤ä½“åå¥½ä¿¡æ¯ï¼Œå¹¶é¿å…å¯¹ä½æ•ˆéšæœºç­–ç•¥çš„ä¾èµ–ã€‚å®éªŒç»“æœå……åˆ†è¯æ˜äº†DGPOçš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DGPOå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚å›¾åƒç”Ÿæˆã€æ–‡æœ¬ç”Ÿæˆã€éŸ³é¢‘ç”Ÿæˆç­‰ã€‚å®ƒå¯ä»¥ç”¨äºè®­ç»ƒé«˜è´¨é‡çš„æ‰©æ•£æ¨¡å‹ï¼Œä»è€Œç”Ÿæˆæ›´é€¼çœŸã€æ›´ç¬¦åˆç”¨æˆ·éœ€æ±‚çš„æ ·æœ¬ã€‚æ­¤å¤–ï¼ŒDGPOè¿˜å¯ä»¥åº”ç”¨äºä¸ªæ€§åŒ–æ¨èã€é£æ ¼è¿ç§»ç­‰é¢†åŸŸï¼Œä¸ºç”¨æˆ·æä¾›æ›´åŠ ä¸ªæ€§åŒ–çš„æœåŠ¡ã€‚æœªæ¥ï¼ŒDGPOæœ‰æœ›æˆä¸ºæ‰©æ•£æ¨¡å‹å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„é‡è¦å·¥å…·ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> While reinforcement learning methods such as Group Relative Preference Optimization (GRPO) have significantly enhanced Large Language Models, adapting them to diffusion models remains challenging. In particular, GRPO demands a stochastic policy, yet the most cost-effective diffusion samplers are based on deterministic ODEs. Recent work addresses this issue by using inefficient SDE-based samplers to induce stochasticity, but this reliance on model-agnostic Gaussian noise leads to slow convergence. To resolve this conflict, we propose Direct Group Preference Optimization (DGPO), a new online RL algorithm that dispenses with the policy-gradient framework entirely. DGPO learns directly from group-level preferences, which utilize relative information of samples within groups. This design eliminates the need for inefficient stochastic policies, unlocking the use of efficient deterministic ODE samplers and faster training. Extensive results show that DGPO trains around 20 times faster than existing state-of-the-art methods and achieves superior performance on both in-domain and out-of-domain reward metrics. Code is available at https://github.com/Luo-Yihong/DGPO.

