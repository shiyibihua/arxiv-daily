---
layout: default
title: Zero-Shot Policy Transfer in Reinforcement Learning using Buckingham's Pi Theorem
---

# Zero-Shot Policy Transfer in Reinforcement Learning using Buckingham's Pi Theorem

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.08768" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.08768v1</a>
  <a href="https://arxiv.org/pdf/2510.08768.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.08768v1" onclick="toggleFavorite(this, '2510.08768v1', 'Zero-Shot Policy Transfer in Reinforcement Learning using Buckingham\'s Pi Theorem')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Francisco Pascoa, Ian Lalonde, Alexandre Girard

**åˆ†ç±»**: cs.LG, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-10-09

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åˆ©ç”¨ç™½é‡‘æ±‰Ï€å®šç†å®ç°å¼ºåŒ–å­¦ä¹ ä¸­çš„é›¶æ ·æœ¬ç­–ç•¥è¿ç§»**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `é›¶æ ·æœ¬è¿ç§»` `ç™½é‡‘æ±‰Ï€å®šç†` `é‡çº²åˆ†æ` `æœºå™¨äººæ§åˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¼ºåŒ–å­¦ä¹ ç­–ç•¥åœ¨é¢å¯¹ç‰©ç†å‚æ•°å˜åŒ–æ—¶æ³›åŒ–èƒ½åŠ›ä¸è¶³ï¼Œé™åˆ¶äº†å…¶åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨ã€‚
2. è®ºæ–‡æå‡ºåŸºäºç™½é‡‘æ±‰Ï€å®šç†çš„é›¶æ ·æœ¬è¿ç§»æ–¹æ³•ï¼Œé€šè¿‡ç¼©æ”¾ç­–ç•¥çš„è¾“å…¥å’Œè¾“å‡ºæ¥é€‚åº”æ–°çš„ç¯å¢ƒã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŠ¨æ€ç›¸ä¼¼ç¯å¢ƒä¸­ä¿æŒæ€§èƒ½ï¼Œå¹¶åœ¨éç›¸ä¼¼ç¯å¢ƒä¸­æ˜¾è‘—ä¼˜äºæœ´ç´ è¿ç§»æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼ºåŒ–å­¦ä¹ (RL)ç­–ç•¥é€šå¸¸éš¾ä»¥æ³›åŒ–åˆ°å…·æœ‰ä¸åŒç‰©ç†å‚æ•°çš„æ–°æœºå™¨äººã€ä»»åŠ¡æˆ–ç¯å¢ƒï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨ç°å®ä¸–ç•Œä¸­çš„é€‚ç”¨æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç™½é‡‘æ±‰Ï€å®šç†çš„ç®€å•é›¶æ ·æœ¬è¿ç§»æ–¹æ³•æ¥è§£å†³è¿™ä¸€é™åˆ¶ã€‚è¯¥æ–¹æ³•é€šè¿‡æ— é‡çº²ç©ºé—´ç¼©æ”¾å…¶è¾“å…¥ï¼ˆè§‚å¯Ÿï¼‰å’Œè¾“å‡ºï¼ˆåŠ¨ä½œï¼‰æ¥ä½¿é¢„è®­ç»ƒç­–ç•¥é€‚åº”æ–°çš„ç³»ç»Ÿç¯å¢ƒï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚è¯¥æ–¹æ³•åœ¨ä¸‰ä¸ªå¤æ‚åº¦é€’å¢çš„ç¯å¢ƒä¸­è¿›è¡Œäº†è¯„ä¼°ï¼Œåˆ†åˆ«æ˜¯æ¨¡æ‹Ÿæ‘†ã€ç”¨äºæ¨¡æ‹Ÿåˆ°çœŸå®éªŒè¯çš„ç‰©ç†æ‘†ä»¥åŠé«˜ç»´HalfCheetahã€‚ç»“æœè¡¨æ˜ï¼Œç¼©æ”¾è¿ç§»åœ¨åŠ¨æ€ç›¸ä¼¼çš„ç¯å¢ƒä¸­æ²¡æœ‰æ€§èƒ½æŸå¤±ã€‚æ­¤å¤–ï¼Œåœ¨éç›¸ä¼¼ç¯å¢ƒä¸­ï¼Œç¼©æ”¾ç­–ç•¥å§‹ç»ˆä¼˜äºæœ´ç´ è¿ç§»ï¼Œæ˜¾è‘—æ‰©å¤§äº†åŸå§‹ç­–ç•¥ä»ç„¶æœ‰æ•ˆçš„ç¯å¢ƒèŒƒå›´ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œé‡çº²åˆ†æä¸ºå¢å¼ºRLç­–ç•¥çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›æä¾›äº†ä¸€ç§å¼ºå¤§è€Œå®ç”¨çš„å·¥å…·ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå¼ºåŒ–å­¦ä¹ ç­–ç•¥åœ¨ä¸åŒç‰©ç†å‚æ•°çš„æœºå™¨äººæˆ–ç¯å¢ƒä¸­æ³›åŒ–èƒ½åŠ›å·®ï¼Œå¯¼è‡´éœ€è¦é’ˆå¯¹æ¯ä¸ªæ–°ç¯å¢ƒé‡æ–°è®­ç»ƒç­–ç•¥ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥å®ç°é›¶æ ·æœ¬è¿ç§»ï¼Œå³æ— éœ€é‡æ–°è®­ç»ƒå³å¯å°†ç­–ç•¥åº”ç”¨åˆ°æ–°ç¯å¢ƒã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šåˆ©ç”¨ç™½é‡‘æ±‰Ï€å®šç†è¿›è¡Œé‡çº²åˆ†æï¼Œå°†ç‰©ç†å‚æ•°ä¸åŒçš„ç³»ç»Ÿæ˜ å°„åˆ°æ— é‡çº²ç©ºé—´ã€‚é€šè¿‡åœ¨æ— é‡çº²ç©ºé—´ä¸­ç¼©æ”¾ç­–ç•¥çš„è¾“å…¥ï¼ˆè§‚å¯Ÿï¼‰å’Œè¾“å‡ºï¼ˆåŠ¨ä½œï¼‰ï¼Œä½¿ç­–ç•¥èƒ½å¤Ÿé€‚åº”æ–°çš„ç³»ç»Ÿç¯å¢ƒï¼Œä»è€Œå®ç°é›¶æ ·æœ¬è¿ç§»ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯åˆ©ç”¨ç‰©ç†ç³»ç»Ÿçš„ç›¸ä¼¼æ€§ï¼Œåœ¨æ— é‡çº²ç©ºé—´ä¸­æ‰¾åˆ°å¯¹åº”å…³ç³»ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æµç¨‹åŒ…æ‹¬ï¼š1) å¯¹åŸå§‹ç¯å¢ƒè¿›è¡Œé‡çº²åˆ†æï¼Œç¡®å®šæ— é‡çº²å‚æ•°ï¼›2) ä½¿ç”¨åŸå§‹ç¯å¢ƒè®­ç»ƒå¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼›3) å¯¹ç›®æ ‡ç¯å¢ƒè¿›è¡Œé‡çº²åˆ†æï¼Œç¡®å®šæ— é‡çº²å‚æ•°ï¼›4) æ ¹æ®æ— é‡çº²å‚æ•°çš„æ¯”ä¾‹å…³ç³»ï¼Œç¼©æ”¾ç›®æ ‡ç¯å¢ƒçš„è§‚å¯Ÿå’ŒåŠ¨ä½œç©ºé—´ï¼›5) å°†ç¼©æ”¾åçš„è§‚å¯Ÿè¾“å…¥åˆ°é¢„è®­ç»ƒç­–ç•¥ä¸­ï¼Œå¾—åˆ°ç¼©æ”¾åçš„åŠ¨ä½œè¾“å‡ºã€‚

**å…³é”®åˆ›æ–°**ï¼šå°†ç™½é‡‘æ±‰Ï€å®šç†åº”ç”¨äºå¼ºåŒ–å­¦ä¹ ç­–ç•¥è¿ç§»ï¼Œé€šè¿‡é‡çº²åˆ†æå®ç°é›¶æ ·æœ¬è¿ç§»ï¼Œæ— éœ€é‡æ–°è®­ç»ƒç­–ç•¥ã€‚ä¸ç°æœ‰è¿ç§»å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•ä¸éœ€è¦ä»»ä½•ç›®æ ‡ç¯å¢ƒçš„æ•°æ®ï¼Œæ˜¯ä¸€ç§çœŸæ­£çš„é›¶æ ·æœ¬æ–¹æ³•ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®åœ¨äºå¦‚ä½•é€‰æ‹©åˆé€‚çš„ç‰©ç†å‚æ•°è¿›è¡Œé‡çº²åˆ†æï¼Œä»¥åŠå¦‚ä½•ç¡®å®šæ— é‡çº²å‚æ•°çš„ç¼©æ”¾æ¯”ä¾‹ã€‚è®ºæ–‡ä¸­å…·ä½“å®ç°ä¾èµ–äºå¯¹ç‰¹å®šç¯å¢ƒçš„ç‰©ç†çŸ¥è¯†ï¼Œä¾‹å¦‚æ‘†çš„é•¿åº¦ã€è´¨é‡ã€é‡åŠ›åŠ é€Ÿåº¦ç­‰ã€‚æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ä¸åŸå§‹å¼ºåŒ–å­¦ä¹ ç­–ç•¥ç›¸åŒï¼Œæ— éœ€ä¿®æ”¹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŠ¨æ€ç›¸ä¼¼çš„ç¯å¢ƒä¸­æ²¡æœ‰æ€§èƒ½æŸå¤±ï¼Œåœ¨éç›¸ä¼¼çš„ç¯å¢ƒä¸­æ˜¾è‘—ä¼˜äºæœ´ç´ è¿ç§»æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨HalfCheetahç¯å¢ƒä¸­ï¼Œç¼©æ”¾ç­–ç•¥çš„æ€§èƒ½æ˜æ˜¾é«˜äºæœªç¼©æ”¾çš„ç­–ç•¥ï¼Œæ‰©å¤§äº†ç­–ç•¥æœ‰æ•ˆçš„ç¯å¢ƒèŒƒå›´ã€‚åœ¨ç‰©ç†æ‘†å®éªŒä¸­ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨æ¨¡æ‹Ÿåˆ°çœŸå®è¿ç§»ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨åŒ–ç³»ç»Ÿç­‰é¢†åŸŸï¼Œå°¤å…¶é€‚ç”¨äºéœ€è¦å¿«é€Ÿéƒ¨ç½²åˆ°ä¸åŒç‰©ç†å‚æ•°ç¯å¢ƒä¸­çš„åœºæ™¯ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥å°†ä¸€ä¸ªåœ¨ç‰¹å®šå°ºå¯¸çš„æœºå™¨äººä¸Šè®­ç»ƒçš„ç­–ç•¥ï¼Œé›¶æ ·æœ¬è¿ç§»åˆ°ä¸åŒå°ºå¯¸çš„æœºå™¨äººä¸Šï¼Œä»è€Œé™ä½å¼€å‘æˆæœ¬å’Œæ—¶é—´ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºæ¨¡æ‹Ÿåˆ°çœŸå®çš„è¿ç§»ï¼Œæé«˜å¼ºåŒ–å­¦ä¹ ç­–ç•¥åœ¨çœŸå®ä¸–ç•Œä¸­çš„é²æ£’æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement learning (RL) policies often fail to generalize to new robots, tasks, or environments with different physical parameters, a challenge that limits their real-world applicability. This paper presents a simple, zero-shot transfer method based on Buckingham's Pi Theorem to address this limitation. The method adapts a pre-trained policy to new system contexts by scaling its inputs (observations) and outputs (actions) through a dimensionless space, requiring no retraining. The approach is evaluated against a naive transfer baseline across three environments of increasing complexity: a simulated pendulum, a physical pendulum for sim-to-real validation, and the high-dimensional HalfCheetah. Results demonstrate that the scaled transfer exhibits no loss of performance on dynamically similar contexts. Furthermore, on non-similar contexts, the scaled policy consistently outperforms the naive transfer, significantly expanding the volume of contexts where the original policy remains effective. These findings demonstrate that dimensional analysis provides a powerful and practical tool to enhance the robustness and generalization of RL policies.

