---
layout: default
title: Reinforcement Learning-Driven Edge Management for Reliable Multi-view 3D Reconstruction
---

# Reinforcement Learning-Driven Edge Management for Reliable Multi-view 3D Reconstruction

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.08839" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.08839v1</a>
  <a href="https://arxiv.org/pdf/2510.08839.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.08839v1" onclick="toggleFavorite(this, '2510.08839v1', 'Reinforcement Learning-Driven Edge Management for Reliable Multi-view 3D Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Motahare Mounesan, Sourya Saha, Houchao Gan, Md. Nurul Absur, Saptarshi Debroy

**åˆ†ç±»**: cs.LG, cs.AI, cs.CV, cs.DC, cs.GR, cs.MM

**å‘å¸ƒæ—¥æœŸ**: 2025-10-09

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå¼ºåŒ–å­¦ä¹ çš„è¾¹ç¼˜ç®¡ç†æ¡†æ¶ï¼Œæå‡å¤šè§†è§’3Dé‡å»ºåœ¨åŠ¨æ€ç¯å¢ƒä¸‹çš„å¯é æ€§ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¤šè§†è§’3Dé‡å»º` `è¾¹ç¼˜è®¡ç®—` `å¼ºåŒ–å­¦ä¹ ` `èµ„æºç®¡ç†` `Q-learning`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è¾¹ç¼˜è®¡ç®—èµ„æºçš„ä¸ç¡®å®šæ€§å¯¹å®æ—¶å¤šè§†è§’3Dé‡å»ºçš„å¯é æ€§æ„æˆæŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨ç«ç¾æ•‘æ´ç­‰å…³é”®åº”ç”¨ä¸­ã€‚
2. è®ºæ–‡æå‡ºä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„è¾¹ç¼˜èµ„æºç®¡ç†æ¡†æ¶ï¼Œé€šè¿‡ç›¸æœºå’ŒæœåŠ¡å™¨é€‰æ‹©ï¼Œä¼˜åŒ–èµ„æºåˆ†é…ï¼Œæå‡é‡å»ºè´¨é‡å’Œæ•ˆç‡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨åŠ¨æ€ç¯å¢ƒä¸­èƒ½å¤Ÿæœ‰æ•ˆå¹³è¡¡ç«¯åˆ°ç«¯å»¶è¿Ÿå’Œé‡å»ºè´¨é‡ï¼Œä»è€Œæé«˜åº”ç”¨å¯é æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ (RL)çš„è¾¹ç¼˜èµ„æºç®¡ç†æ¡†æ¶ï¼Œç”¨äºæé«˜å¤šè§†è§’3Dé‡å»ºçš„å¯é æ€§ã€‚è¯¥æ¡†æ¶æ—¨åœ¨è§£å†³è¾¹ç¼˜èµ„æºåŠ¨æ€æ€§å’Œä¸å¯é¢„æµ‹æ€§å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œä¾‹å¦‚å›¾åƒè´¨é‡ä¸‹é™ã€ç½‘ç»œè¿æ¥ä¸ç¨³å®šå’ŒæœåŠ¡å™¨è´Ÿè½½æ³¢åŠ¨ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä¸¤ä¸ªååŒçš„Q-learningæ™ºèƒ½ä½“ï¼Œåˆ†åˆ«è´Ÿè´£ç›¸æœºé€‰æ‹©å’ŒæœåŠ¡å™¨é€‰æ‹©ï¼Œå®Œå…¨åœ¨çº¿è¿è¡Œï¼Œå¹¶é€šè¿‡ä¸è¾¹ç¼˜ç¯å¢ƒçš„äº¤äº’å­¦ä¹ ç­–ç•¥ã€‚ä¸ºäº†æ”¯æŒåœ¨çœŸå®çº¦æŸä¸‹çš„å­¦ä¹ å¹¶è¯„ä¼°ç³»ç»Ÿæ€§èƒ½ï¼Œæˆ‘ä»¬å®ç°äº†ä¸€ä¸ªåˆ†å¸ƒå¼æµ‹è¯•å¹³å°ï¼ŒåŒ…æ‹¬å®éªŒå®¤æ‰˜ç®¡çš„ç»ˆç«¯è®¾å¤‡å’ŒFABRICåŸºç¡€è®¾æ–½æ‰˜ç®¡çš„è¾¹ç¼˜æœåŠ¡å™¨ï¼Œä»¥æ¨¡æ‹Ÿæ™ºèƒ½åŸå¸‚è¾¹ç¼˜åŸºç¡€è®¾æ–½åœ¨çœŸå®ä¸­æ–­åœºæ™¯ä¸‹çš„æƒ…å†µã€‚ç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ¡†æ¶é€šè¿‡æœ‰æ•ˆåœ°å¹³è¡¡ç«¯åˆ°ç«¯å»¶è¿Ÿå’Œé‡å»ºè´¨é‡ï¼Œæé«˜äº†åŠ¨æ€ç¯å¢ƒä¸­çš„åº”ç”¨å¯é æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šåœ¨è¾¹ç¼˜è®¡ç®—ç¯å¢ƒä¸‹ï¼Œå®æ—¶å¤šè§†è§’3Dé‡å»ºé¢ä¸´èµ„æºåŠ¨æ€å˜åŒ–å’Œä¸å¯é¢„æµ‹çš„æŒ‘æˆ˜ï¼Œä¾‹å¦‚ç½‘ç»œå¸¦å®½æ³¢åŠ¨ã€æœåŠ¡å™¨è´Ÿè½½å˜åŒ–ä»¥åŠç›¸æœºå›¾åƒè´¨é‡å—æŸã€‚è¿™äº›å› ç´ ä¼šå¯¼è‡´é‡å»ºå»¶è¿Ÿå¢åŠ ã€é‡å»ºè´¨é‡ä¸‹é™ï¼Œä¸¥é‡å½±å“ä¾èµ–äºå®æ—¶3Dä¿¡æ¯çš„åº”ç”¨ï¼Œå¦‚ç«ç¾æ•‘æ´ç­‰ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥åœ¨èµ„æºå—é™å’Œæ˜“å—å¹²æ‰°çš„ç¯å¢ƒä¸­ä¿è¯é‡å»ºçš„å¯é æ€§å’Œæ•ˆç‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼Œè®©æ™ºèƒ½ä½“é€šè¿‡ä¸è¾¹ç¼˜ç¯å¢ƒçš„äº¤äº’ï¼Œå­¦ä¹ å¦‚ä½•åœ¨åŠ¨æ€å˜åŒ–çš„èµ„æºæ¡ä»¶ä¸‹ï¼Œé€‰æ‹©æœ€ä½³çš„ç›¸æœºç»„åˆå’ŒæœåŠ¡å™¨èµ„æºï¼Œä»è€Œåœ¨é‡å»ºè´¨é‡å’Œå»¶è¿Ÿä¹‹é—´å–å¾—å¹³è¡¡ã€‚é€šè¿‡åœ¨çº¿å­¦ä¹ ï¼Œæ™ºèƒ½ä½“èƒ½å¤Ÿé€‚åº”ç¯å¢ƒå˜åŒ–ï¼Œåšå‡ºæœ€ä¼˜å†³ç­–ï¼Œä¿è¯é‡å»ºçš„å¯é æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªååŒçš„Q-learningæ™ºèƒ½ä½“ï¼šç›¸æœºé€‰æ‹©æ™ºèƒ½ä½“å’ŒæœåŠ¡å™¨é€‰æ‹©æ™ºèƒ½ä½“ã€‚ç›¸æœºé€‰æ‹©æ™ºèƒ½ä½“è´Ÿè´£ä»å¤šä¸ªå¯ç”¨ç›¸æœºä¸­é€‰æ‹©æœ€ä½³çš„ç›¸æœºç»„åˆï¼Œä»¥è·å¾—é«˜è´¨é‡çš„å›¾åƒæ•°æ®ã€‚æœåŠ¡å™¨é€‰æ‹©æ™ºèƒ½ä½“è´Ÿè´£é€‰æ‹©åˆé€‚çš„è¾¹ç¼˜æœåŠ¡å™¨æ¥å¤„ç†é‡å»ºä»»åŠ¡ï¼Œä»¥é™ä½å»¶è¿Ÿã€‚è¿™ä¸¤ä¸ªæ™ºèƒ½ä½“ç›¸äº’åä½œï¼Œå…±åŒä¼˜åŒ–æ•´ä¸ªé‡å»ºæµç¨‹ã€‚æ•´ä¸ªç³»ç»Ÿåœ¨ä¸€ä¸ªåˆ†å¸ƒå¼æµ‹è¯•å¹³å°ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œè¯¥å¹³å°æ¨¡æ‹Ÿäº†æ™ºèƒ½åŸå¸‚è¾¹ç¼˜åŸºç¡€è®¾æ–½åœ¨çœŸå®ä¸­æ–­åœºæ™¯ä¸‹çš„æƒ…å†µã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†å¼ºåŒ–å­¦ä¹ åº”ç”¨äºè¾¹ç¼˜è®¡ç®—ç¯å¢ƒä¸‹çš„å¤šè§†è§’3Dé‡å»ºèµ„æºç®¡ç†ã€‚ä¸ä¼ ç»Ÿçš„é™æ€èµ„æºåˆ†é…æ–¹æ³•ä¸åŒï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ ¹æ®ç¯å¢ƒçš„åŠ¨æ€å˜åŒ–ï¼Œå®æ—¶è°ƒæ•´èµ„æºåˆ†é…ç­–ç•¥ï¼Œä»è€Œæé«˜é‡å»ºçš„å¯é æ€§å’Œæ•ˆç‡ã€‚æ­¤å¤–ï¼Œä¸¤ä¸ªååŒçš„Q-learningæ™ºèƒ½ä½“çš„è®¾è®¡ï¼Œä½¿å¾—ç›¸æœºé€‰æ‹©å’ŒæœåŠ¡å™¨é€‰æ‹©èƒ½å¤ŸååŒä¼˜åŒ–ï¼Œè¿›ä¸€æ­¥æå‡äº†ç³»ç»Ÿæ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šç›¸æœºé€‰æ‹©æ™ºèƒ½ä½“çš„çŠ¶æ€ç©ºé—´åŒ…æ‹¬ç›¸æœºçš„å›¾åƒè´¨é‡ã€ç½‘ç»œè¿æ¥çŠ¶æ€ç­‰ä¿¡æ¯ã€‚æœåŠ¡å™¨é€‰æ‹©æ™ºèƒ½ä½“çš„çŠ¶æ€ç©ºé—´åŒ…æ‹¬æœåŠ¡å™¨çš„è´Ÿè½½ã€ç½‘ç»œå»¶è¿Ÿç­‰ä¿¡æ¯ã€‚ä¸¤ä¸ªæ™ºèƒ½ä½“éƒ½ä½¿ç”¨Q-learningç®—æ³•è¿›è¡Œå­¦ä¹ ï¼Œå¥–åŠ±å‡½æ•°çš„è®¾è®¡æ—¨åœ¨å¹³è¡¡é‡å»ºè´¨é‡å’Œå»¶è¿Ÿã€‚å…·ä½“çš„å¥–åŠ±å‡½æ•°å½¢å¼å’ŒQ-learningçš„å‚æ•°è®¾ç½®ï¼ˆå¦‚å­¦ä¹ ç‡ã€æŠ˜æ‰£å› å­ç­‰ï¼‰éœ€è¦æ ¹æ®å®é™…åº”ç”¨åœºæ™¯è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆå¹³è¡¡ç«¯åˆ°ç«¯å»¶è¿Ÿå’Œé‡å»ºè´¨é‡ï¼Œä»è€Œæé«˜åº”ç”¨å¯é æ€§ã€‚å…·ä½“è€Œè¨€ï¼Œåœ¨æ¨¡æ‹Ÿçš„æ™ºèƒ½åŸå¸‚è¾¹ç¼˜åŸºç¡€è®¾æ–½ä¸­æ–­åœºæ™¯ä¸‹ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæ˜¾è‘—é™ä½é‡å»ºå»¶è¿Ÿï¼ŒåŒæ—¶ä¿è¯é‡å»ºè´¨é‡åœ¨å¯æ¥å—çš„èŒƒå›´å†…ã€‚ä¸ä¼ ç»Ÿçš„é™æ€èµ„æºåˆ†é…æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”ç¯å¢ƒå˜åŒ–ï¼Œæä¾›æ›´ç¨³å®šçš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå¤šç§éœ€è¦å®æ—¶3Dé‡å»ºçš„è¾¹ç¼˜è®¡ç®—åœºæ™¯ï¼Œä¾‹å¦‚ï¼šç«ç¾æ•‘æ´ã€æ™ºèƒ½äº¤é€šã€å·¥ä¸šè‡ªåŠ¨åŒ–ã€å¢å¼ºç°å®ç­‰ã€‚é€šè¿‡ä¼˜åŒ–è¾¹ç¼˜èµ„æºç®¡ç†ï¼Œå¯ä»¥æé«˜è¿™äº›åº”ç”¨åœ¨èµ„æºå—é™å’ŒåŠ¨æ€å˜åŒ–ç¯å¢ƒä¸‹çš„å¯é æ€§å’Œæ•ˆç‡ï¼Œä¸ºç”¨æˆ·æä¾›æ›´ä¼˜è´¨çš„æœåŠ¡ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥æ‰©å±•åˆ°å…¶ä»–ç±»å‹çš„è¾¹ç¼˜è®¡ç®—åº”ç”¨ï¼Œä¾‹å¦‚è§†é¢‘åˆ†æã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Real-time multi-view 3D reconstruction is a mission-critical application for key edge-native use cases, such as fire rescue, where timely and accurate 3D scene modeling enables situational awareness and informed decision-making. However, the dynamic and unpredictable nature of edge resource availability introduces disruptions, such as degraded image quality, unstable network links, and fluctuating server loads, which challenge the reliability of the reconstruction pipeline. In this work, we present a reinforcement learning (RL)-based edge resource management framework for reliable 3D reconstruction to ensure high quality reconstruction within a reasonable amount of time, despite the system operating under a resource-constrained and disruption-prone environment. In particular, the framework adopts two cooperative Q-learning agents, one for camera selection and one for server selection, both of which operate entirely online, learning policies through interactions with the edge environment. To support learning under realistic constraints and evaluate system performance, we implement a distributed testbed comprising lab-hosted end devices and FABRIC infrastructure-hosted edge servers to emulate smart city edge infrastructure under realistic disruption scenarios. Results show that the proposed framework improves application reliability by effectively balancing end-to-end latency and reconstruction quality in dynamic environments.

