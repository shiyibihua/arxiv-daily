---
layout: default
title: DEAS: DEtached value learning with Action Sequence for Scalable Offline RL
---

# DEAS: DEtached value learning with Action Sequence for Scalable Offline RL

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.07730" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.07730v1</a>
  <a href="https://arxiv.org/pdf/2510.07730.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.07730v1" onclick="toggleFavorite(this, '2510.07730v1', 'DEAS: DEtached value learning with Action Sequence for Scalable Offline RL')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Changyeon Kim, Haeone Lee, Younggyo Seo, Kimin Lee, Yuke Zhu

**åˆ†ç±»**: cs.LG, cs.AI, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-10-09

**å¤‡æ³¨**: Project website: https://changyeon.site/deas

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**DEASï¼šåˆ©ç”¨åŠ¨ä½œåºåˆ—å’Œè§£è€¦ä»·å€¼å­¦ä¹ å®ç°å¯æ‰©å±•çš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ **

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç¦»çº¿å¼ºåŒ–å­¦ä¹ ` `åŠ¨ä½œåºåˆ—` `ä»·å€¼å­¦ä¹ ` `è§£è€¦ä»·å€¼å­¦ä¹ ` `é•¿æ—¶åºä»»åŠ¡` `æœºå™¨äººæ§åˆ¶` `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤„ç†å¤æ‚ã€é•¿æ—¶åºå†³ç­–ä»»åŠ¡æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œéš¾ä»¥æœ‰æ•ˆåˆ©ç”¨ç¦»çº¿æ•°æ®ã€‚
2. DEASé€šè¿‡å¼•å…¥åŠ¨ä½œåºåˆ—è¿›è¡Œä»·å€¼å­¦ä¹ ï¼Œå¹¶ç»“åˆè§£è€¦ä»·å€¼å­¦ä¹ æ¥é¿å…ä»·å€¼é«˜ä¼°ï¼Œä»è€Œæå‡æ€§èƒ½ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒDEASåœ¨é•¿æ—¶åºä»»åŠ¡ä¸Šä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå¹¶èƒ½æå‡è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹åœ¨æœºå™¨äººä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸ºè®­ç»ƒæ™ºèƒ½ä½“æä¾›äº†ä¸€ç§æ— éœ€æ˜‚è´µçš„åœ¨çº¿äº¤äº’çš„æœ‰æ•ˆæ–¹æ³•ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•åœ¨å¤„ç†å¤æ‚çš„ã€é•¿æ—¶åºåºåˆ—å†³ç­–é—®é¢˜æ—¶ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œåä¸ºDEtached value learning with Action Sequence (DEAS)ï¼Œå®ƒåˆ©ç”¨åŠ¨ä½œåºåˆ—è¿›è¡Œä»·å€¼å­¦ä¹ ã€‚è¿™äº›æ—¶é—´ä¸Šæ‰©å±•çš„åŠ¨ä½œæä¾›äº†æ¯”å•æ­¥åŠ¨ä½œæ›´ä¸°å¯Œçš„ä¿¡æ¯ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡åŠé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹Qå­¦ä¹ çš„é€‰é¡¹æ¡†æ¶è¿›è¡Œè§£é‡Šï¼Œä»è€Œé€šè¿‡ä¸€æ¬¡è€ƒè™‘æ›´é•¿çš„åºåˆ—æ¥å‡å°‘æœ‰æ•ˆçš„è§„åˆ’èŒƒå›´ã€‚ç„¶è€Œï¼Œåœ¨actor-criticç®—æ³•ä¸­ç›´æ¥é‡‡ç”¨è¿™æ ·çš„åºåˆ—ä¼šå¼•å…¥è¿‡åº¦çš„ä»·å€¼é«˜ä¼°ï¼Œæˆ‘ä»¬é€šè¿‡è§£è€¦ä»·å€¼å­¦ä¹ æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè¯¥æ–¹æ³•å°†ä»·å€¼ä¼°è®¡å¼•å¯¼åˆ°ç¦»çº¿æ•°æ®é›†ä¸­å®ç°é«˜å›æŠ¥çš„åˆ†å¸ƒå†…åŠ¨ä½œã€‚æˆ‘ä»¬è¯æ˜äº†DEASåœ¨OGBenchçš„å¤æ‚ã€é•¿æ—¶åºä»»åŠ¡ä¸Šå§‹ç»ˆä¼˜äºåŸºçº¿ï¼Œå¹¶ä¸”å¯ä»¥åº”ç”¨äºå¢å¼ºé¢„æµ‹åŠ¨ä½œåºåˆ—çš„å¤§è§„æ¨¡è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„æ€§èƒ½ï¼Œä»è€Œæ˜¾è‘—æé«˜RoboCasaå¨æˆ¿æ¨¡æ‹Ÿä»»åŠ¡å’ŒçœŸå®ä¸–ç•Œæ“ä½œä»»åŠ¡çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç¦»çº¿å¼ºåŒ–å­¦ä¹ æ—¨åœ¨åˆ©ç”¨é¢„å…ˆæ”¶é›†å¥½çš„é™æ€æ•°æ®é›†è®­ç»ƒæ™ºèƒ½ä½“ï¼Œé¿å…åœ¨çº¿æ¢ç´¢å¸¦æ¥çš„é«˜æ˜‚æˆæœ¬ã€‚ç„¶è€Œï¼Œåœ¨é•¿æ—¶åºä»»åŠ¡ä¸­ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆåˆ©ç”¨ç¦»çº¿æ•°æ®è¿›è¡Œä»·å€¼ä¼°è®¡ï¼Œå®¹æ˜“äº§ç”Ÿä»·å€¼é«˜ä¼°é—®é¢˜ï¼Œå¯¼è‡´ç­–ç•¥æ€§èƒ½ä¸ä½³ã€‚å°¤å…¶æ˜¯åœ¨å¤æ‚ç¯å¢ƒä¸­ï¼Œå•æ­¥åŠ¨ä½œçš„ä»·å€¼ä¿¡æ¯ä¸è¶³ä»¥æŒ‡å¯¼æ™ºèƒ½ä½“åšå‡ºæ­£ç¡®çš„å†³ç­–ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDEASçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨åŠ¨ä½œåºåˆ—æ¥æ‰©å±•åŠ¨ä½œç©ºé—´ï¼Œä»è€Œæä¾›æ›´ä¸°å¯Œçš„ä»·å€¼ä¿¡æ¯ã€‚é€šè¿‡å°†å¤šä¸ªè¿ç»­åŠ¨ä½œè§†ä¸ºä¸€ä¸ªâ€œé€‰é¡¹â€ï¼Œå¯ä»¥æœ‰æ•ˆç¼©çŸ­è§„åˆ’è§†é‡ï¼ŒåŠ é€Ÿå­¦ä¹ è¿‡ç¨‹ã€‚åŒæ—¶ï¼Œä¸ºäº†è§£å†³åŠ¨ä½œåºåˆ—å¸¦æ¥çš„ä»·å€¼é«˜ä¼°é—®é¢˜ï¼ŒDEASé‡‡ç”¨è§£è€¦ä»·å€¼å­¦ä¹ ï¼Œå°†ä»·å€¼ä¼°è®¡é™åˆ¶åœ¨ç¦»çº¿æ•°æ®é›†ä¸­è¡¨ç°è‰¯å¥½çš„åŠ¨ä½œä¸Šã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDEASé‡‡ç”¨Actor-Criticæ¡†æ¶ã€‚Actorè´Ÿè´£ç”ŸæˆåŠ¨ä½œåºåˆ—ï¼ŒCriticè´Ÿè´£è¯„ä¼°åŠ¨ä½œåºåˆ—çš„ä»·å€¼ã€‚æ•´ä½“æµç¨‹å¦‚ä¸‹ï¼š1) ä»ç¦»çº¿æ•°æ®é›†ä¸­é‡‡æ ·çŠ¶æ€ï¼›2) Actoræ ¹æ®å½“å‰çŠ¶æ€ç”ŸæˆåŠ¨ä½œåºåˆ—ï¼›3) Criticè¯„ä¼°è¯¥åŠ¨ä½œåºåˆ—çš„ä»·å€¼ï¼›4) ä½¿ç”¨è§£è€¦ä»·å€¼å­¦ä¹ æ›´æ–°Criticç½‘ç»œï¼Œä½¿å…¶æ›´å‡†ç¡®åœ°è¯„ä¼°ç¦»çº¿æ•°æ®é›†ä¸­è¡¨ç°è‰¯å¥½çš„åŠ¨ä½œåºåˆ—ï¼›5) ä½¿ç”¨ç­–ç•¥æ¢¯åº¦æ›´æ–°Actorç½‘ç»œï¼Œä½¿å…¶ç”Ÿæˆæ›´æœ‰ä»·å€¼çš„åŠ¨ä½œåºåˆ—ã€‚

**å…³é”®åˆ›æ–°**ï¼šDEASçš„å…³é”®åˆ›æ–°åœ¨äºä¸¤ä¸ªæ–¹é¢ï¼šä¸€æ˜¯åˆ©ç”¨åŠ¨ä½œåºåˆ—è¿›è¡Œä»·å€¼å­¦ä¹ ï¼ŒäºŒæ˜¯é‡‡ç”¨è§£è€¦ä»·å€¼å­¦ä¹ æ¥è§£å†³ä»·å€¼é«˜ä¼°é—®é¢˜ã€‚åŠ¨ä½œåºåˆ—èƒ½å¤Ÿæä¾›æ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¸®åŠ©æ™ºèƒ½ä½“æ›´å¥½åœ°ç†è§£ç¯å¢ƒã€‚è§£è€¦ä»·å€¼å­¦ä¹ åˆ™èƒ½å¤Ÿæœ‰æ•ˆåœ°é™åˆ¶ä»·å€¼ä¼°è®¡çš„èŒƒå›´ï¼Œé¿å…è¿‡åº¦ä¹è§‚çš„ä¼°è®¡ã€‚

**å…³é”®è®¾è®¡**ï¼šDEASçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) åŠ¨ä½œåºåˆ—çš„é•¿åº¦ï¼šéœ€è¦æ ¹æ®ä»»åŠ¡çš„å¤æ‚ç¨‹åº¦è¿›è¡Œè°ƒæ•´ï¼Œè¿‡çŸ­çš„åºåˆ—å¯èƒ½æ— æ³•æä¾›è¶³å¤Ÿçš„ä¿¡æ¯ï¼Œè¿‡é•¿çš„åºåˆ—åˆ™ä¼šå¢åŠ è®¡ç®—å¤æ‚åº¦ï¼›2) è§£è€¦ä»·å€¼å­¦ä¹ çš„å®ç°æ–¹å¼ï¼šå¯ä»¥é€šè¿‡å¼•å…¥é¢å¤–çš„æŸå¤±å‡½æ•°ï¼Œæˆ–è€…ä¿®æ”¹ä»·å€¼ç½‘ç»œçš„ç»“æ„æ¥å®ç°ï¼›3) Actorå’ŒCriticç½‘ç»œçš„ç»“æ„ï¼šå¯ä»¥é‡‡ç”¨å¸¸è§çš„ç¥ç»ç½‘ç»œç»“æ„ï¼Œå¦‚MLPæˆ–Transformerã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

DEASåœ¨OGBenchçš„é•¿æ—¶åºä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¶…è¶Šäº†ç°æœ‰çš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒDEASè¿˜æˆåŠŸåº”ç”¨äºRoboCasaå¨æˆ¿æ¨¡æ‹Ÿä»»åŠ¡å’ŒçœŸå®ä¸–ç•Œæ“ä½œä»»åŠ¡ï¼Œæ˜¾è‘—æå‡äº†è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDEASèƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨åŠ¨ä½œåºåˆ—è¿›è¡Œä»·å€¼å­¦ä¹ ï¼Œå¹¶è§£å†³ä»·å€¼é«˜ä¼°é—®é¢˜ï¼Œä»è€Œæå‡ç¦»çº¿å¼ºåŒ–å­¦ä¹ çš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DEASå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚æœºå™¨äººæ§åˆ¶ã€æ¸¸æˆAIã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸã€‚åœ¨æœºå™¨äººæ§åˆ¶ä¸­ï¼ŒDEASå¯ä»¥å¸®åŠ©æœºå™¨äººå­¦ä¹ å¤æ‚çš„åŠ¨ä½œåºåˆ—ï¼Œä»è€Œå®Œæˆæ›´å¤æ‚çš„ä»»åŠ¡ã€‚åœ¨æ¸¸æˆAIä¸­ï¼ŒDEASå¯ä»¥è®­ç»ƒå‡ºæ›´æ™ºèƒ½çš„AIå¯¹æ‰‹ï¼Œæå‡æ¸¸æˆä½“éªŒã€‚åœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼ŒDEASå¯ä»¥å¸®åŠ©è½¦è¾†å­¦ä¹ æ›´å®‰å…¨çš„é©¾é©¶ç­–ç•¥ï¼Œå‡å°‘äº‹æ•…çš„å‘ç”Ÿã€‚æ­¤å¤–ï¼ŒDEASè¿˜å¯ä»¥åº”ç”¨äºæ¨èç³»ç»Ÿã€é‡‘èäº¤æ˜“ç­‰é¢†åŸŸã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Offline reinforcement learning (RL) presents an attractive paradigm for training intelligent agents without expensive online interactions. However, current approaches still struggle with complex, long-horizon sequential decision making. In this work, we introduce DEtached value learning with Action Sequence (DEAS), a simple yet effective offline RL framework that leverages action sequences for value learning. These temporally extended actions provide richer information than single-step actions and can be interpreted through the options framework via semi-Markov decision process Q-learning, enabling reduction of the effective planning horizon by considering longer sequences at once. However, directly adopting such sequences in actor-critic algorithms introduces excessive value overestimation, which we address through detached value learning that steers value estimates toward in-distribution actions that achieve high return in the offline dataset. We demonstrate that DEAS consistently outperforms baselines on complex, long-horizon tasks from OGBench and can be applied to enhance the performance of large-scale Vision-Language-Action models that predict action sequences, significantly boosting performance in both RoboCasa Kitchen simulation tasks and real-world manipulation tasks.

