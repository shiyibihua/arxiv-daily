---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.LG - 2025-10-09
---

# cs.LGï¼ˆ2025-10-09ï¼‰

ğŸ“Š å…± **7** ç¯‡è®ºæ–‡
 | ğŸ”— **1** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (5 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (2)</a>
</div>

---


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (5 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/251008492v1-better-together-leveraging-unpaired-multimodal-data-for-stronger-uni.html">Better Together: Leveraging Unpaired Multimodal Data for Stronger Unimodal Models</a></td>
  <td>æå‡ºUMLï¼Œåˆ©ç”¨éé…å¯¹å¤šæ¨¡æ€æ•°æ®å¢å¼ºå•æ¨¡æ€æ¨¡å‹è¡¨ç¤ºå­¦ä¹ </td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.08492v1" onclick="toggleFavorite(this, '2510.08492v1', 'Better Together: Leveraging Unpaired Multimodal Data for Stronger Unimodal Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/251007910v1-mmm-quantum-chemical-molecular-representation-learning-for-combinato.html">MMM: Quantum-Chemical Molecular Representation Learning for Combinatorial Drug Recommendation</a></td>
  <td>MMMï¼šåˆ©ç”¨é‡å­åŒ–å­¦åˆ†å­è¡¨ç¤ºå­¦ä¹ è¿›è¡Œç»„åˆè¯ç‰©æ¨èï¼Œæå‡DDIé¢„æµ‹ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.07910v1" onclick="toggleFavorite(this, '2510.07910v1', 'MMM: Quantum-Chemical Molecular Representation Learning for Combinatorial Drug Recommendation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/251008839v1-reinforcement-learning-driven-edge-management-for-reliable-multi-vie.html">Reinforcement Learning-Driven Edge Management for Reliable Multi-view 3D Reconstruction</a></td>
  <td>æå‡ºåŸºäºå¼ºåŒ–å­¦ä¹ çš„è¾¹ç¼˜ç®¡ç†æ¡†æ¶ï¼Œæå‡å¤šè§†è§’3Dé‡å»ºåœ¨åŠ¨æ€ç¯å¢ƒä¸‹çš„å¯é æ€§ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.08839v1" onclick="toggleFavorite(this, '2510.08839v1', 'Reinforcement Learning-Driven Edge Management for Reliable Multi-view 3D Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/251008425v1-reinforcing-diffusion-models-by-direct-group-preference-optimization.html">Reinforcing Diffusion Models by Direct Group Preference Optimization</a></td>
  <td>æå‡ºç›´æ¥ç¾¤ä½“åå¥½ä¼˜åŒ–(DGPO)ï¼ŒåŠ é€Ÿå¹¶æå‡æ‰©æ•£æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒã€‚</td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.08425v1" onclick="toggleFavorite(this, '2510.08425v1', 'Reinforcing Diffusion Models by Direct Group Preference Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/251008179v1-dual-granularity-sinkhorn-distillation-for-enhanced-learning-from-lo.html">Dual-granularity Sinkhorn Distillation for Enhanced Learning from Long-tailed Noisy Data</a></td>
  <td>æå‡ºåŒç²’åº¦Sinkhornè’¸é¦(D-SINK)æ¡†æ¶ï¼Œæå‡é•¿å°¾å™ªå£°æ•°æ®ä¸‹çš„æ¨¡å‹å­¦ä¹ èƒ½åŠ›ã€‚</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.08179v1" onclick="toggleFavorite(this, '2510.08179v1', 'Dual-granularity Sinkhorn Distillation for Enhanced Learning from Long-tailed Noisy Data')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>6</td>
  <td><a href="./papers/251007730v1-deas-detached-value-learning-with-action-sequence-for-scalable-offli.html">DEAS: DEtached value learning with Action Sequence for Scalable Offline RL</a></td>
  <td>DEASï¼šåˆ©ç”¨åŠ¨ä½œåºåˆ—å’Œè§£è€¦ä»·å€¼å­¦ä¹ å®ç°å¯æ‰©å±•çš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ </td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.07730v1" onclick="toggleFavorite(this, '2510.07730v1', 'DEAS: DEtached value learning with Action Sequence for Scalable Offline RL')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/251008768v1-zero-shot-policy-transfer-in-reinforcement-learning-using-buckingham.html">Zero-Shot Policy Transfer in Reinforcement Learning using Buckingham's Pi Theorem</a></td>
  <td>åˆ©ç”¨ç™½é‡‘æ±‰Ï€å®šç†å®ç°å¼ºåŒ–å­¦ä¹ ä¸­çš„é›¶æ ·æœ¬ç­–ç•¥è¿ç§»</td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.08768v1" onclick="toggleFavorite(this, '2510.08768v1', 'Zero-Shot Policy Transfer in Reinforcement Learning using Buckingham&#39;s Pi Theorem')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.LG é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)