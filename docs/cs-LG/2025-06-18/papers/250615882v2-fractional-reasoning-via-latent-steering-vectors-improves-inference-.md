---
layout: default
title: Fractional Reasoning via Latent Steering Vectors Improves Inference Time Compute
---

# Fractional Reasoning via Latent Steering Vectors Improves Inference Time Compute

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.15882" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.15882v2</a>
  <a href="https://arxiv.org/pdf/2506.15882.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.15882v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.15882v2', 'Fractional Reasoning via Latent Steering Vectors Improves Inference Time Compute')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sheng Liu, Tianlang Chen, Pan Lu, Haotian Ye, Yizheng Chen, Lei Xing, James Zou

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL, eess.SP

**å‘å¸ƒæ—¥æœŸ**: 2025-06-18 (æ›´æ–°: 2025-09-25)

**å¤‡æ³¨**: 18 pages, 5 figures, Project website: https://shengliu66.github.io/fractreason/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåˆ†æ•°æ¨ç†ä»¥æå‡æ¨ç†æ—¶é—´è®¡ç®—æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ¨ç†æ·±åº¦` `æ¨¡å‹æ— å…³` `æ½œåœ¨å¼•å¯¼å‘é‡` `åŠ¨æ€è°ƒæ•´` `æ€§èƒ½æå‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨æ¨ç†è¿‡ç¨‹ä¸­é‡‡ç”¨ç»Ÿä¸€çš„æ¨ç†æ·±åº¦ï¼Œæ— æ³•é€‚åº”ä¸åŒé—®é¢˜çš„å¤æ‚æ€§ï¼Œå¯¼è‡´æ€§èƒ½ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºçš„åˆ†æ•°æ¨ç†æ¡†æ¶å…è®¸åœ¨æ¨ç†æ—¶é—´ä¸Šè¿ç»­æ§åˆ¶æ¨ç†å¼ºåº¦ï¼Œèƒ½å¤Ÿæ ¹æ®è¾“å…¥çš„å¤æ‚æ€§è°ƒæ•´æ¨ç†è¿‡ç¨‹ã€‚
3. åœ¨GSM8Kã€MATH500å’ŒGPQAç­‰æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œåˆ†æ•°æ¨ç†æ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨å¤šç§æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æµ‹è¯•æ—¶é—´è®¡ç®—å·²æˆä¸ºæå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ€§èƒ½çš„æœ‰æ•ˆèŒƒå¼ï¼Œé€šè¿‡ç”Ÿæˆå¤šä¸ªè¾“å‡ºæˆ–ç»†åŒ–ä¸ªåˆ«æ¨ç†é“¾å¯ä»¥æ˜¾è‘—æé«˜ç­”æ¡ˆçš„å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¦‚æœ€ä½³é€‰æ‹©ã€æŠ•ç¥¨å’Œè‡ªæˆ‘åæ€é€šå¸¸åœ¨è¾“å…¥ä¸Šé‡‡ç”¨ç»Ÿä¸€çš„æ¨ç†æ–¹å¼ï¼Œå¿½è§†äº†ä¸åŒé—®é¢˜å¯èƒ½éœ€è¦ä¸åŒæ·±åº¦çš„æ¨ç†ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åˆ†æ•°æ¨ç†ï¼Œè¿™æ˜¯ä¸€ç§æ— è®­ç»ƒä¸”æ¨¡å‹æ— å…³çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨æ¨ç†æ—¶é—´ä¸Šè¿ç»­æ§åˆ¶æ¨ç†å¼ºåº¦ï¼Œè¶…è¶Šå›ºå®šæŒ‡ä»¤æç¤ºçš„å±€é™ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡æå–ä¸æ›´æ·±æ¨ç†ç›¸å…³çš„æ½œåœ¨å¼•å¯¼å‘é‡å¹¶é‡æ–°åº”ç”¨å¯è°ƒç¼©æ”¾å› å­ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ ¹æ®æ¯ä¸ªè¾“å…¥çš„å¤æ‚æ€§é‡èº«å®šåˆ¶æ¨ç†è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåˆ†æ•°æ¨ç†åœ¨GSM8Kã€MATH500å’ŒGPQAç­‰å¤šç§æ¨ç†ä»»åŠ¡å’Œæ¨¡å‹ä¸Šå‡è¡¨ç°å‡ºä¸€è‡´çš„æ€§èƒ½æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³ç°æœ‰æ¨ç†æ–¹æ³•åœ¨å¤„ç†ä¸åŒå¤æ‚æ€§é—®é¢˜æ—¶çš„å±€é™æ€§ï¼Œç°æœ‰æ–¹æ³•é€šå¸¸é‡‡ç”¨ç»Ÿä¸€çš„æ¨ç†æ·±åº¦ï¼Œæ— æ³•çµæ´»é€‚åº”ä¸åŒè¾“å…¥çš„éœ€æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºçš„åˆ†æ•°æ¨ç†æ¡†æ¶é€šè¿‡æå–æ½œåœ¨å¼•å¯¼å‘é‡å¹¶åº”ç”¨å¯è°ƒç¼©æ”¾å› å­ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ ¹æ®è¾“å…¥çš„å¤æ‚æ€§åŠ¨æ€è°ƒæ•´æ¨ç†å¼ºåº¦ï¼Œä»è€Œæé«˜æ¨ç†çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šæ½œåœ¨å¼•å¯¼å‘é‡æå–æ¨¡å—å’Œæ¨ç†å¼ºåº¦è°ƒæ•´æ¨¡å—ã€‚å‰è€…è´Ÿè´£ä»æ¨¡å‹ä¸­æå–ä¸æ·±åº¦æ¨ç†ç›¸å…³çš„å‘é‡ï¼Œåè€…åˆ™æ ¹æ®è¾“å…¥çš„å¤æ‚æ€§è°ƒæ•´æ¨ç†çš„æ·±åº¦ã€‚

**å…³é”®åˆ›æ–°**ï¼šåˆ†æ•°æ¨ç†çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶è®­ç»ƒè‡ªç”±æ€§å’Œæ¨¡å‹æ— å…³æ€§ï¼Œå…è®¸åœ¨æ¨ç†æ—¶é—´ä¸Šçµæ´»æ§åˆ¶æ¨ç†å¼ºåº¦ï¼Œè¿™ä¸ä¼ ç»Ÿçš„å›ºå®šæ¨ç†æ·±åº¦æ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†å¯è°ƒç¼©æ”¾å› å­æ¥æ§åˆ¶æ¨ç†å¼ºåº¦ï¼Œå¹¶åœ¨å®éªŒä¸­éªŒè¯äº†ä¸åŒæ·±åº¦æ¨ç†å¯¹è¾“å‡ºè´¨é‡çš„å½±å“ï¼Œç¡®ä¿äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡è¿™ç§è®¾è®¡ï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨ä¸åŒçš„æ¨ç†ç­–ç•¥ä¸­ï¼ˆå¦‚æœ€ä½³é€‰æ‹©å’Œè‡ªæˆ‘åæ€ï¼‰çµæ´»è°ƒæ•´æ¨ç†æ·±åº¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œåˆ†æ•°æ¨ç†åœ¨GSM8Kã€MATH500å’ŒGPQAæ•°æ®é›†ä¸Šå‡æ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ï¼Œå°¤å…¶åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­ï¼Œå‡†ç¡®ç‡æé«˜äº†çº¦10%-15%ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œåˆ†æ•°æ¨ç†å±•ç°äº†æ›´å¼ºçš„é€‚åº”æ€§å’Œçµæ´»æ€§ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡æ¨ç†è´¨é‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€é‡‘èå’ŒåŒ»ç–—ç­‰éœ€è¦å¤æ‚æ¨ç†çš„åœºæ™¯ã€‚é€šè¿‡æå‡æ¨¡å‹åœ¨ä¸åŒå¤æ‚æ€§é—®é¢˜ä¸Šçš„æ¨ç†èƒ½åŠ›ï¼Œåˆ†æ•°æ¨ç†å¯ä»¥å¸®åŠ©ç”¨æˆ·è·å¾—æ›´å‡†ç¡®çš„ç­”æ¡ˆï¼Œè¿›è€Œæé«˜å†³ç­–è´¨é‡å’Œæ•ˆç‡ã€‚æœªæ¥ï¼Œéšç€æ¨¡å‹çš„ä¸æ–­å‘å±•ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åœ¨æ›´å¤šå®é™…åº”ç”¨ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Test-time compute has emerged as a powerful paradigm for improving the performance of large language models (LLMs), where generating multiple outputs or refining individual chains can significantly boost answer accuracy. However, existing methods like Best-of-N, majority voting, and self-reflection typically apply reasoning in a uniform way across inputs, overlooking the fact that different problems may require different levels of reasoning depth. In this work, we propose Fractional Reasoning, a training-free and model-agnostic framework that enables continuous control over reasoning intensity at inference time, going beyond the limitations of fixed instructional prompts. Our method operates by extracting the latent steering vector associated with deeper reasoning and reapplying it with a tunable scaling factor, allowing the model to tailor its reasoning process to the complexity of each input. This supports two key modes of test-time scaling: (1) improving output quality in breadth-based strategies (e.g., Best-of-N, majority voting), and (2) enhancing the correctness of individual reasoning chains in depth-based strategies (e.g., self-reflection). Experiments on GSM8K, MATH500, and GPQA demonstrate that Fractional Reasoning consistently improves performance across diverse reasoning tasks and models.

