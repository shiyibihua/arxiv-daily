---
layout: default
title: Singular Value Decomposition on Kronecker Adaptation for Large Language Model
---

# Singular Value Decomposition on Kronecker Adaptation for Large Language Model

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.15251" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.15251v1</a>
  <a href="https://arxiv.org/pdf/2506.15251.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.15251v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.15251v1', 'Singular Value Decomposition on Kronecker Adaptation for Large Language Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yee Hin Chong, Peng Qu

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSoKAä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹çš„å‚æ•°é«˜æ•ˆå¾®è°ƒé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å‚æ•°é«˜æ•ˆå¾®è°ƒ` `Kroneckerç§¯` `å¥‡å¼‚å€¼åˆ†è§£` `åŠ¨æ€ç§©é€‰æ‹©` `å¤§è¯­è¨€æ¨¡å‹` `æ¨¡å‹é€‚åº”` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•åœ¨æ¨ç†æ—¶å¯èƒ½å¼•å…¥å»¶è¿Ÿï¼Œæˆ–åœ¨æ”¶æ•›æ€§å’Œä»»åŠ¡å¤æ‚æ€§åŒ¹é…ä¸Šå­˜åœ¨ä¸è¶³ã€‚
2. è®ºæ–‡æå‡ºçš„SoKAæ–¹æ³•ç»“åˆäº†Kroneckerç§¯å¼ é‡åˆ†è§£ä¸SVDåˆå§‹åŒ–ï¼Œé‡‡ç”¨åŠ¨æ€ç§©é€‰æ‹©ä»¥æé«˜å¾®è°ƒæ•ˆç‡ã€‚
3. å®éªŒè¯æ˜ï¼ŒSoKAåœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæ‰€éœ€å¯è®­ç»ƒå‚æ•°æ˜¾è‘—ä½äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶æ”¶æ•›é€Ÿåº¦æ›´å¿«ï¼Œæ¢¯åº¦æ›´ç¨³å®šã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹é¢„è®­ç»ƒçš„Transformeræ¨¡å‹åœ¨å¤šç§è¯­è¨€å’Œæ¨ç†ä»»åŠ¡ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æˆæœï¼Œä½†å®Œå…¨å¾®è°ƒä¼šå¸¦æ¥å·¨å¤§çš„å­˜å‚¨ã€å†…å­˜å’Œè®¡ç®—å¼€é”€ã€‚å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•é€šè¿‡ä»…å­¦ä¹ å°‘é‡ç‰¹å®šä»»åŠ¡çš„å‚æ•°æ¥å‡è½»è¿™äº›æˆæœ¬ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•è¦ä¹ˆåœ¨æ¨ç†æ—¶å¼•å…¥å»¶è¿Ÿï¼ˆé€‚é…æ¨¡å—ï¼‰ï¼Œè¦ä¹ˆæ”¶æ•›ä¸ç†æƒ³ï¼ˆéšæœºåˆå§‹åŒ–çš„ä½ç§©æ›´æ–°ï¼‰ï¼Œæˆ–ä¾èµ–äºå¯èƒ½ä¸åŒ¹é…ä»»åŠ¡å¤æ‚æ€§çš„å›ºå®šç§©é€‰æ‹©ï¼ˆåŸºäºKroneckerçš„åˆ†è§£ï¼‰ã€‚æˆ‘ä»¬æå‡ºäº†SoKAï¼ˆåŸºäºKroneckeré€‚åº”çš„å¥‡å¼‚å€¼åˆ†è§£ï¼‰ï¼Œä¸€ç§ç»“åˆKroneckerç§¯å¼ é‡åˆ†è§£ä¸SVDé©±åŠ¨åˆå§‹åŒ–å’Œè°±æ„ŸçŸ¥åŠ¨æ€ç§©é€‰æ‹©çš„æ–°å‹PEFTç­–ç•¥ã€‚æˆ‘ä»¬çš„Kronecker-Product SVDï¼ˆKPSVDï¼‰ç¨‹åºå°†å®Œæ•´æƒé‡æ›´æ–°çš„ä¸»æˆåˆ†æå–ä¸ºç´§å‡‘çš„Kroneckerå› å­ï¼Œè€Œè‡ªé€‚åº”ç§©é€‰æ‹©ç®—æ³•åˆ™ä½¿ç”¨èƒ½é‡é˜ˆå€¼å’Œè‚˜ç‚¹æ ‡å‡†æ¥ä¿®å‰ªå¯å¿½ç•¥çš„æˆåˆ†ã€‚å¯¹LLaMA2-7Båœ¨ç®—æœ¯æ¨ç†ï¼ˆGSM8Kï¼‰ã€å½¢å¼æ•°å­¦ï¼ˆMATHï¼‰å’Œä»£ç ç”Ÿæˆï¼ˆMBPPï¼‰ä¸Šçš„å®è¯è¯„ä¼°è¡¨æ˜ï¼ŒSoKAä»…éœ€0.99Må¯è®­ç»ƒå‚æ•°ï¼Œæ¯”LoRA/PiSSAå°‘25%ï¼ŒåŒæ—¶åŒ¹é…æˆ–è¶…è¶ŠåŸºçº¿æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒSoKAå±•ç°å‡ºæ›´å¿«çš„æ”¶æ•›é€Ÿåº¦å’Œæ›´ç¨³å®šçš„æ¢¯åº¦ï¼Œçªæ˜¾äº†å…¶åœ¨å¤§è§„æ¨¡æ¨¡å‹é€‚åº”ä¸­çš„é²æ£’æ€§å’Œæ•ˆç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­é¢ä¸´çš„å­˜å‚¨ã€å†…å­˜å’Œè®¡ç®—å¼€é”€é—®é¢˜ã€‚ç°æœ‰çš„PEFTæ–¹æ³•å­˜åœ¨æ¨ç†å»¶è¿Ÿã€æ”¶æ•›ä¸ç†æƒ³å’Œå›ºå®šç§©é€‰æ‹©ä¸åŒ¹é…ç­‰ç—›ç‚¹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSoKAæ–¹æ³•é€šè¿‡ç»“åˆKroneckerç§¯å¼ é‡åˆ†è§£ä¸å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰é©±åŠ¨çš„åˆå§‹åŒ–ï¼Œé‡‡ç”¨è°±æ„ŸçŸ¥çš„åŠ¨æ€ç§©é€‰æ‹©æ¥ä¼˜åŒ–å‚æ•°å¾®è°ƒè¿‡ç¨‹ï¼Œä»è€Œæé«˜æ•ˆç‡å’Œæ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSoKAçš„æ•´ä½“æ¶æ„åŒ…æ‹¬KPSVDç¨‹åºç”¨äºæå–æƒé‡æ›´æ–°çš„ä¸»æˆåˆ†ï¼Œå’Œè‡ªé€‚åº”ç§©é€‰æ‹©ç®—æ³•ç”¨äºä¿®å‰ªä¸é‡è¦çš„æˆåˆ†ã€‚è¯¥æ¡†æ¶é€šè¿‡åŠ¨æ€è°ƒæ•´ç§©æ¥é€‚åº”ä¸åŒä»»åŠ¡çš„å¤æ‚æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šSoKAçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶ç»“åˆäº†Kroneckerç§¯åˆ†è§£ä¸åŠ¨æ€ç§©é€‰æ‹©ï¼Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•çš„å±€é™æ€§ï¼Œæä¾›äº†æ›´çµæ´»å’Œé«˜æ•ˆçš„å¾®è°ƒç­–ç•¥ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼ŒSoKAä»…éœ€0.99Må¯è®­ç»ƒå‚æ•°ï¼Œä½¿ç”¨èƒ½é‡é˜ˆå€¼å’Œè‚˜ç‚¹æ ‡å‡†è¿›è¡ŒåŠ¨æ€ç§©é€‰æ‹©ï¼Œç¡®ä¿äº†åœ¨ä¸åŒä»»åŠ¡ä¸­çš„é€‚åº”æ€§å’Œæ€§èƒ½ä¼˜åŒ–ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨å®éªŒéƒ¨åˆ†è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSoKAåœ¨å¤šä¸ªåŸºå‡†ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä»…éœ€0.99Må¯è®­ç»ƒå‚æ•°ï¼Œè¾ƒLoRA/PiSSAå‡å°‘25%ã€‚æ­¤å¤–ï¼ŒSoKAåœ¨æ”¶æ•›é€Ÿåº¦å’Œæ¢¯åº¦ç¨³å®šæ€§æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå±•ç°å‡ºå…¶åœ¨å¤§è§„æ¨¡æ¨¡å‹é€‚åº”ä¸­çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€ä»£ç ç”Ÿæˆå’Œæ•°å­¦æ¨ç†ç­‰ä»»åŠ¡ï¼Œèƒ½å¤Ÿæ˜¾è‘—é™ä½å¤§è¯­è¨€æ¨¡å‹çš„å¾®è°ƒæˆæœ¬ï¼Œæé«˜æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„é€‚åº”èƒ½åŠ›ã€‚æœªæ¥ï¼ŒSoKAå¯èƒ½æ¨åŠ¨æ›´é«˜æ•ˆçš„æ¨¡å‹è®­ç»ƒå’Œéƒ¨ç½²ï¼Œä¿ƒè¿›AIæŠ€æœ¯çš„å¹¿æ³›åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large pre-trained Transformer models achieve state-of-the-art results across diverse language and reasoning tasks, but full fine-tuning incurs substantial storage, memory, and computational overhead. Parameter-efficient fine-tuning (PEFT) methods mitigate these costs by learning only a small subset of task-specific parameters, yet existing approaches either introduce inference-time latency (adapter modules), suffer from suboptimal convergence (randomly initialized low-rank updates), or rely on fixed rank choices that may not match task complexity (Kronecker-based decompositions).
>   We propose SoKA (SVD on Kronecker Adaptation), a novel PEFT strategy that combines Kronecker-product tensor factorization with SVD-driven initialization and spectrum-aware dynamic rank selection. Our Kronecker-Product SVD (KPSVD) procedure extracts principal components of the full weight update into compact Kronecker factors, while an adaptive rank selection algorithm uses energy-threshold and elbow-point criteria to prune negligible components.
>   Empirical evaluation on LLaMA2-7B across arithmetic reasoning (GSM8K), formal mathematics (MATH), and code generation (MBPP) demonstrates that SoKA requires only 0.99M trainable parameters, 25% fewer than LoRA/PiSSA, while matching or exceeding baseline performance. Moreover, SoKA exhibits faster convergence and more stable gradients, highlighting its robustness and efficiency for large-scale model adaptation.

