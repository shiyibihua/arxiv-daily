---
layout: default
title: AutoRule: Reasoning Chain-of-thought Extracted Rule-based Rewards Improve Preference Learning
---

# AutoRule: Reasoning Chain-of-thought Extracted Rule-based Rewards Improve Preference Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.15651" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.15651v1</a>
  <a href="https://arxiv.org/pdf/2506.15651.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.15651v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.15651v1', 'AutoRule: Reasoning Chain-of-thought Extracted Rule-based Rewards Improve Preference Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tevin Wang, Chenyan Xiong

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-18

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/cxcscmu/AutoRule)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAutoRuleä»¥è‡ªåŠ¨åŒ–æå–è§„åˆ™æ”¹å–„åå¥½å­¦ä¹ **

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§„åˆ™æå–` `å¼ºåŒ–å­¦ä¹ ` `äººç±»åé¦ˆ` `è‡ªåŠ¨åŒ–` `åå¥½å­¦ä¹ ` `è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åŸºäºè§„åˆ™çš„å¥–åŠ±æ–¹æ³•ä¾èµ–æ‰‹åŠ¨è§„åˆ™å·¥ç¨‹ï¼Œæ•ˆç‡ä½ä¸”éš¾ä»¥æ‰©å±•ã€‚
2. AutoRuleé€šè¿‡è‡ªåŠ¨æå–ç”¨æˆ·åå¥½ä¸­çš„è§„åˆ™ï¼Œå½¢æˆåŸºäºè§„åˆ™çš„å¥–åŠ±ï¼Œç®€åŒ–äº†è¿™ä¸€è¿‡ç¨‹ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨AutoRuleè®­ç»ƒçš„æ¨¡å‹åœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œæå‡æ•ˆæœæ˜æ˜¾ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºäºè§„åˆ™çš„å¥–åŠ±ä¸ºä»äººç±»åé¦ˆä¸­æ”¹è¿›å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰æä¾›äº†ä¸€ç§æœ‰å‰æ™¯çš„ç­–ç•¥ï¼Œä½†ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºæ‰‹åŠ¨è§„åˆ™å·¥ç¨‹ã€‚æˆ‘ä»¬æå‡ºäº†AutoRuleï¼Œè¿™æ˜¯ä¸€ç§å®Œå…¨è‡ªåŠ¨åŒ–çš„æ–¹æ³•ï¼Œç”¨äºä»åå¥½åé¦ˆä¸­æå–è§„åˆ™å¹¶å°†å…¶åˆ¶å®šä¸ºåŸºäºè§„åˆ™çš„å¥–åŠ±ã€‚AutoRuleæå–è¿‡ç¨‹åˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼šåˆ©ç”¨æ¨ç†æ¨¡å‹è§£é‡Šç”¨æˆ·åå¥½ï¼Œä»è¿™äº›è§£é‡Šçš„æ¨ç†é“¾ä¸­è¯†åˆ«å€™é€‰è§„åˆ™ï¼Œå¹¶å°†å…¶åˆæˆç»Ÿä¸€çš„è§„åˆ™é›†ã€‚é€šè¿‡æœ€ç»ˆçš„è§„åˆ™é›†ï¼Œæˆ‘ä»¬ä½¿ç”¨è¯­è¨€æ¨¡å‹éªŒè¯å™¨è®¡ç®—æ¯ä¸ªè¾“å‡ºæ»¡è¶³çš„è§„åˆ™æ¯”ä¾‹ï¼Œå¹¶å°†è¯¥æŒ‡æ ‡ä½œä¸ºè¾…åŠ©å¥–åŠ±ä¸å­¦ä¹ çš„å¥–åŠ±æ¨¡å‹ä¸€èµ·ç”¨äºç­–ç•¥ä¼˜åŒ–ã€‚ä½¿ç”¨AutoRuleè®­ç»ƒLlama-3-8Bæ¨¡å‹åœ¨AlpacaEval2.0ä¸Šå®ç°äº†28.6%çš„ç›¸å¯¹æå‡ï¼Œå¹¶åœ¨MT-Benchå­é›†çš„ç¬¬äºŒè½®è¡¨ç°ä¸Šè·å¾—äº†6.1%çš„ç›¸å¯¹å¢ç›Šã€‚æˆ‘ä»¬çš„åˆ†æç¡®è®¤æå–çš„è§„åˆ™ä¸æ•°æ®é›†åå¥½é«˜åº¦ä¸€è‡´ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰åŸºäºè§„åˆ™çš„å¥–åŠ±æ–¹æ³•ä¾èµ–æ‰‹åŠ¨è§„åˆ™å·¥ç¨‹çš„é—®é¢˜ï¼Œè¿™å¯¼è‡´äº†æ•ˆç‡ä½ä¸‹å’Œéš¾ä»¥æ‰©å±•çš„å±€é™æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„AutoRuleæ–¹æ³•é€šè¿‡è‡ªåŠ¨åŒ–æå–ç”¨æˆ·åå¥½ä¸­çš„è§„åˆ™ï¼Œå½¢æˆåŸºäºè§„åˆ™çš„å¥–åŠ±ï¼Œä»è€Œæé«˜å¼ºåŒ–å­¦ä¹ çš„æ•ˆç‡å’Œæ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šAutoRuleçš„æå–è¿‡ç¨‹åˆ†ä¸ºä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆï¼Œåˆ©ç”¨æ¨ç†æ¨¡å‹è§£é‡Šç”¨æˆ·çš„åå¥½ï¼›å…¶æ¬¡ï¼Œä»è¿™äº›è§£é‡Šçš„æ¨ç†é“¾ä¸­è¯†åˆ«å€™é€‰è§„åˆ™ï¼›æœ€åï¼Œå°†å€™é€‰è§„åˆ™åˆæˆä¸€ä¸ªç»Ÿä¸€çš„è§„åˆ™é›†ã€‚

**å…³é”®åˆ›æ–°**ï¼šAutoRuleçš„æœ€å¤§åˆ›æ–°åœ¨äºå…¶å®Œå…¨è‡ªåŠ¨åŒ–çš„è§„åˆ™æå–è¿‡ç¨‹ï¼Œæ˜¾è‘—å‡å°‘äº†å¯¹äººå·¥å¹²é¢„çš„ä¾èµ–ï¼Œæå‡äº†è§„åˆ™çš„ç”Ÿæˆæ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼ŒAutoRuleä½¿ç”¨è¯­è¨€æ¨¡å‹éªŒè¯å™¨æ¥è®¡ç®—æ¯ä¸ªè¾“å‡ºæ»¡è¶³çš„è§„åˆ™æ¯”ä¾‹ï¼Œå¹¶å°†å…¶ä½œä¸ºè¾…åŠ©å¥–åŠ±ä¸å­¦ä¹ çš„å¥–åŠ±æ¨¡å‹ç»“åˆä½¿ç”¨ï¼Œä»¥ä¼˜åŒ–ç­–ç•¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨AutoRuleè®­ç»ƒçš„Llama-3-8Bæ¨¡å‹åœ¨AlpacaEval2.0ä¸Šå®ç°äº†28.6%çš„ç›¸å¯¹æå‡ï¼Œå¹¶åœ¨MT-Benchå­é›†çš„ç¬¬äºŒè½®è¡¨ç°ä¸Šè·å¾—äº†6.1%çš„ç›¸å¯¹å¢ç›Šï¼Œæ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹GRPOã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œäººæœºäº¤äº’ç­‰ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚æœªæ¥ï¼ŒAutoRuleå¯èƒ½ä¼šåœ¨æ›´å¤šé¢†åŸŸä¸­æ¨å¹¿ï¼Œä¿ƒè¿›è‡ªåŠ¨åŒ–è§„åˆ™ç”ŸæˆæŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Rule-based rewards offer a promising strategy for improving reinforcement learning from human feedback (RLHF), but current approaches often rely on manual rule engineering. We present AutoRule, a fully automated method for extracting rules from preference feedback and formulating them into rule-based rewards. AutoRule extraction operates in three stages: it leverages a reasoning model to interpret user preferences, identifies candidate rules from the reasoning chain of these interpretations, and synthesizes them into a unified rule set. Leveraging the finalized rule set, we employ language-model verifiers to compute the fraction of rules satisfied by each output, using this metric as an auxiliary reward alongside the learned reward model during policy optimization. Training a Llama-3-8B model with AutoRule results in a 28.6\% relative improvement in length-controlled win rate on AlpacaEval2.0, and a 6.1\% relative gain in second-turn performance on a held-out MT-Bench subset, compared to a GRPO baseline trained with the same learned reward model but without the rule-based auxiliary reward. Our analysis confirms that the extracted rules exhibit good agreement with dataset preference. We find that AutoRule demonstrates reduced reward hacking compared to a learned reward model when run over two episodes. Finally, our case study suggests that the extracted rules capture unique qualities valued in different datasets. The extracted rules are provided in the appendix, and the code is open-sourced at https://github.com/cxcscmu/AutoRule.

