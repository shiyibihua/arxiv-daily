---
layout: default
title: When and How Unlabeled Data Provably Improve In-Context Learning
---

# When and How Unlabeled Data Provably Improve In-Context Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.15329" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.15329v1</a>
  <a href="https://arxiv.org/pdf/2506.15329.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.15329v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.15329v1', 'When and How Unlabeled Data Provably Improve In-Context Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yingcong Li, Xiangyu Chang, Muti Kara, Xiaofeng Liu, Amit Roy-Chowdhury, Samet Oymak

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL, math.OC

**å‘å¸ƒæ—¥æœŸ**: 2025-06-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåˆ©ç”¨æœªæ ‡è®°æ•°æ®æå‡ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›çš„æ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä¸Šä¸‹æ–‡å­¦ä¹ ` `æœªæ ‡è®°æ•°æ®` `åŠç›‘ç£å­¦ä¹ ` `å¤šå±‚å˜æ¢å™¨` `å¾ªç¯ç»“æ„` `é«˜æ–¯æ··åˆæ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å•å±‚çº¿æ€§æ³¨æ„åŠ›æ¨¡å‹æ— æ³•æœ‰æ•ˆåˆ©ç”¨æœªæ ‡è®°æ•°æ®ï¼Œå¯¼è‡´å­¦ä¹ æ€§èƒ½å—é™ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡å¤šå±‚æˆ–å¾ªç¯å˜æ¢å™¨ï¼Œåˆ©ç”¨æœªæ ‡è®°æ•°æ®æ„å»ºç‰¹å®šå½¢å¼çš„ä¼°è®¡å™¨ï¼Œä»è€Œæå‡å­¦ä¹ æ•ˆæœã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨çœŸå®æ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†åŠç›‘ç£å­¦ä¹ çš„æ€§èƒ½ï¼Œç›¸è¾ƒäºæ ‡å‡†å•æ¬¡æ¨ç†æœ‰æ˜æ˜¾æ”¹å–„ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘æœŸç ”ç©¶è¡¨æ˜ï¼Œå³ä½¿æ¼”ç¤ºæ•°æ®å­˜åœ¨ç¼ºå¤±æˆ–é”™è¯¯æ ‡ç­¾ï¼Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ä¾ç„¶æœ‰æ•ˆã€‚æœ¬æ–‡æ¢è®¨äº†åœ¨äºŒå…ƒé«˜æ–¯æ··åˆæ¨¡å‹ä¸‹ï¼Œéƒ¨åˆ†æ¼”ç¤ºç¼ºå¤±æ ‡ç­¾çš„æƒ…å†µã€‚æˆ‘ä»¬é€šè¿‡ç†è®ºåˆ†æè¡¨æ˜ï¼šä¸€å±‚çº¿æ€§æ³¨æ„åŠ›æ¨¡å‹æ— æ³•åˆ©ç”¨æœªæ ‡è®°æ•°æ®ï¼Œè€Œå¤šå±‚æˆ–å¾ªç¯å˜æ¢å™¨èƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨æœªæ ‡è®°æ•°æ®ï¼Œæ„å»ºç‰¹å®šå½¢å¼çš„ä¼°è®¡å™¨ã€‚æˆ‘ä»¬è¿˜å°†æ­¤ä¸å¸¸ç”¨çš„åŠç›‘ç£å­¦ä¹ ç®—æ³•æœŸæœ›æœ€å¤§åŒ–å»ºç«‹äº†è”ç³»ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†å¾ªç¯ä½¿ç”¨ç°æˆè¡¨æ ¼åŸºç¡€æ¨¡å‹çš„æ–¹æ³•ï¼Œä»¥å¢å¼ºå…¶åŠç›‘ç£èƒ½åŠ›ï¼Œå¹¶åœ¨çœŸå®æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†åŠç›‘ç£å­¦ä¹ æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­ï¼Œå¦‚ä½•æœ‰æ•ˆåˆ©ç”¨æœªæ ‡è®°æ•°æ®çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¦‚å•å±‚çº¿æ€§æ³¨æ„åŠ›æ¨¡å‹åœ¨é¢å¯¹ç¼ºå¤±æ ‡ç­¾æ—¶è¡¨ç°ä¸ä½³ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨å¯ç”¨ä¿¡æ¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¤šå±‚æˆ–å¾ªç¯å˜æ¢å™¨ï¼Œæ„å»ºèƒ½å¤Ÿéšå¼åˆ©ç”¨æœªæ ‡è®°æ•°æ®çš„ä¼°è®¡å™¨ï¼Œä»è€Œæå‡æ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›ã€‚è¿™æ ·çš„è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨ç¼ºå¤±æ ‡ç­¾çš„æƒ…å†µä¸‹ï¼Œä»ç„¶è¿›è¡Œæœ‰æ•ˆçš„å­¦ä¹ ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¤šå±‚æˆ–å¾ªç¯å˜æ¢å™¨ï¼Œæ¨¡å‹é€šè¿‡å¯¹è¾“å…¥ç‰¹å¾å’Œéƒ¨åˆ†è§‚å¯Ÿæ ‡ç­¾çš„å¤„ç†ï¼Œæ„å»ºå‡ºç‰¹å®šå½¢å¼çš„å¤šé¡¹å¼ä¼°è®¡å™¨ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬ç‰¹å¾æå–ã€æ ‡ç­¾å¤„ç†å’Œä¼°è®¡å™¨æ„å»ºã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†åˆ©ç”¨æ·±åº¦å’Œå¾ªç¯ç»“æ„æ¥æ„å»ºå¤šé¡¹å¼å½¢å¼çš„ä¼°è®¡å™¨ï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•çš„çº¿æ€§ä¼°è®¡æ–¹å¼æœ‰æœ¬è´¨åŒºåˆ«ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨æœªæ ‡è®°æ•°æ®ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ï¼Œç¡®ä¿æ¨¡å‹åœ¨å¤„ç†ç¼ºå¤±æ ‡ç­¾æ—¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°è¿›è¡Œå­¦ä¹ å’Œæ¨ç†ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨å¤šä¸ªçœŸå®æ•°æ®é›†ä¸Šï¼Œç›¸è¾ƒäºæ ‡å‡†å•æ¬¡æ¨ç†ï¼ŒåŠç›‘ç£å­¦ä¹ æ€§èƒ½æå‡æ˜¾è‘—ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°XX%ï¼ˆå…·ä½“æ•°æ®éœ€æ ¹æ®å®éªŒç»“æœå¡«å†™ï¼‰ï¼ŒéªŒè¯äº†ç†è®ºåˆ†æçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰å’Œå…¶ä»–éœ€è¦å¤„ç†ä¸å®Œæ•´æ•°æ®çš„æœºå™¨å­¦ä¹ ä»»åŠ¡ã€‚é€šè¿‡æå‡æ¨¡å‹å¯¹æœªæ ‡è®°æ•°æ®çš„åˆ©ç”¨èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ï¼Œä¾ç„¶å®ç°è¾ƒå¥½çš„å­¦ä¹ æ•ˆæœï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent research shows that in-context learning (ICL) can be effective even when demonstrations have missing or incorrect labels. To shed light on this capability, we examine a canonical setting where the demonstrations are drawn according to a binary Gaussian mixture model (GMM) and a certain fraction of the demonstrations have missing labels. We provide a comprehensive theoretical study to show that: (1) The loss landscape of one-layer linear attention models recover the optimal fully-supervised estimator but completely fail to exploit unlabeled data; (2) In contrast, multilayer or looped transformers can effectively leverage unlabeled data by implicitly constructing estimators of the form $\sum_{i\ge 0} a_i (X^\top X)^iX^\top y$ with $X$ and $y$ denoting features and partially-observed labels (with missing entries set to zero). We characterize the class of polynomials that can be expressed as a function of depth and draw connections to Expectation Maximization, an iterative pseudo-labeling algorithm commonly used in semi-supervised learning. Importantly, the leading polynomial power is exponential in depth, so mild amount of depth/looping suffices. As an application of theory, we propose looping off-the-shelf tabular foundation models to enhance their semi-supervision capabilities. Extensive evaluations on real-world datasets show that our method significantly improves the semisupervised tabular learning performance over the standard single pass inference.

