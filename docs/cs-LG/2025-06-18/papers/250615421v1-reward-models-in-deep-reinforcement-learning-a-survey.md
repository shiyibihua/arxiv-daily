---
layout: default
title: Reward Models in Deep Reinforcement Learning: A Survey
---

# Reward Models in Deep Reinforcement Learning: A Survey

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.15421" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.15421v1</a>
  <a href="https://arxiv.org/pdf/2506.15421.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.15421v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.15421v1', 'Reward Models in Deep Reinforcement Learning: A Survey')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Rui Yu, Shenghua Wan, Yucen Wang, Chen-Xiao Gao, Le Gan, Zongzhang Zhang, De-Chuan Zhan

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-18

**å¤‡æ³¨**: IJCAI 2025 Survey Track (To Appear)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç»¼è¿°æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸­çš„å¥–åŠ±æ¨¡å‹ä»¥ä¼˜åŒ–ç­–ç•¥**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¥–åŠ±æ¨¡å‹` `æ·±åº¦å¼ºåŒ–å­¦ä¹ ` `ç­–ç•¥ä¼˜åŒ–` `æ–‡çŒ®ç»¼è¿°` `åº”ç”¨åœºæ™¯` `è¯„ä¼°æ–¹æ³•` `æ™ºèƒ½ä½“è¡Œä¸º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¥–åŠ±æ¨¡å‹åœ¨ä¸çœŸå®ç›®æ ‡å¯¹é½å’Œç­–ç•¥ä¼˜åŒ–æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå¯¼è‡´æ™ºèƒ½ä½“è¡Œä¸ºä¸ç†æƒ³ã€‚
2. æœ¬æ–‡ç»¼è¿°äº†å¥–åŠ±å»ºæ¨¡æŠ€æœ¯ï¼Œåˆ†ç±»ä»‹ç»äº†åŸºäºä¸åŒæ¥æºã€æœºåˆ¶å’Œå­¦ä¹ èŒƒå¼çš„æœ€æ–°æ–¹æ³•ã€‚
3. é€šè¿‡å¯¹å¥–åŠ±æ¨¡å‹çš„è¯„ä¼°æ–¹æ³•è¿›è¡Œå›é¡¾ï¼Œæœ¬æ–‡æŒ‡å‡ºäº†æœªæ¥ç ”ç©¶çš„æ½œåŠ›æ–¹å‘ï¼Œæ¨åŠ¨è¯¥é¢†åŸŸçš„å‘å±•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œæ™ºèƒ½ä½“é€šè¿‡ä¸ç¯å¢ƒçš„æŒç»­äº’åŠ¨ï¼Œåˆ©ç”¨åé¦ˆæ¥ä¼˜åŒ–å…¶è¡Œä¸ºã€‚å¥–åŠ±æ¨¡å‹ä½œä¸ºæœŸæœ›ç›®æ ‡çš„ä»£ç†ï¼Œæ—¨åœ¨å¼•å¯¼ç­–ç•¥ä¼˜åŒ–ï¼Œä½¿æ™ºèƒ½ä½“åœ¨æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±çš„åŒæ—¶å®ç°ä»»åŠ¡è®¾è®¡è€…çš„æ„å›¾ã€‚è¿‘å¹´æ¥ï¼Œå­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œå¯¹å¼€å‘ä¸çœŸå®ç›®æ ‡ç´§å¯†å¯¹é½ä¸”èƒ½ä¿ƒè¿›ç­–ç•¥ä¼˜åŒ–çš„å¥–åŠ±æ¨¡å‹ç»™äºˆäº†æ˜¾è‘—å…³æ³¨ã€‚æœ¬æ–‡å¯¹æ·±åº¦å¼ºåŒ–å­¦ä¹ æ–‡çŒ®ä¸­çš„å¥–åŠ±å»ºæ¨¡æŠ€æœ¯è¿›è¡Œäº†å…¨é¢å›é¡¾ï¼Œæ¶µç›–äº†èƒŒæ™¯çŸ¥è¯†ã€æœ€æ–°æ–¹æ³•ã€åº”ç”¨åœºæ™¯åŠè¯„ä¼°æ–¹æ³•ï¼Œå¹¶æŒ‡å‡ºäº†æœªæ¥çš„ç ”ç©¶æ–¹å‘ï¼Œå¡«è¡¥äº†å½“å‰æ–‡çŒ®ä¸­ç³»ç»Ÿæ€§ç»¼è¿°çš„ç©ºç™½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¥–åŠ±æ¨¡å‹åœ¨ä¸çœŸå®ç›®æ ‡å¯¹é½åŠç­–ç•¥ä¼˜åŒ–ä¸­çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å®é™…åº”ç”¨ä¸­æ™ºèƒ½ä½“è¡Œä¸ºçš„æœ‰æ•ˆæ€§å’Œå¯é æ€§é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡ç³»ç»Ÿæ€§å›é¡¾å¥–åŠ±å»ºæ¨¡æŠ€æœ¯ï¼Œåˆ†ç±»æ€»ç»“ä¸åŒæ–¹æ³•çš„ä¼˜ç¼ºç‚¹ï¼Œæä¾›å¯¹æ¯”åˆ†æï¼Œä»¥æŒ‡å¯¼æœªæ¥çš„ç ”ç©¶æ–¹å‘å’Œåº”ç”¨ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬èƒŒæ™¯çŸ¥è¯†ä»‹ç»ã€å¥–åŠ±å»ºæ¨¡æ–¹æ³•æ¦‚è¿°ã€åº”ç”¨åœºæ™¯è®¨è®ºåŠè¯„ä¼°æ–¹æ³•å›é¡¾ï¼Œå½¢æˆä¸€ä¸ªå…¨é¢çš„å¥–åŠ±æ¨¡å‹ç ”ç©¶æ¡†æ¶ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„åˆ›æ–°åœ¨äºæä¾›äº†ä¸€ä¸ªç³»ç»Ÿçš„å¥–åŠ±æ¨¡å‹ç»¼è¿°ï¼Œæ¶µç›–äº†æ—¢æœ‰æ–¹æ³•å’Œæ–°å…´æŠ€æœ¯ï¼Œå¡«è¡¥äº†æ–‡çŒ®ä¸­çš„ç©ºç™½ï¼Œä¿ƒè¿›äº†å¯¹å¥–åŠ±æ¨¡å‹çš„æ·±å…¥ç†è§£ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ–¹æ³•åˆ†ç±»ä¸­ï¼Œè€ƒè™‘äº†å¥–åŠ±æ¨¡å‹çš„æ¥æºï¼ˆå¦‚ä¸“å®¶åé¦ˆã€ç¯å¢ƒä¿¡å·ï¼‰ã€æœºåˆ¶ï¼ˆå¦‚ç›´æ¥å¥–åŠ±ã€é—´æ¥å¥–åŠ±ï¼‰åŠå­¦ä¹ èŒƒå¼ï¼ˆå¦‚ç›‘ç£å­¦ä¹ ã€å¼ºåŒ–å­¦ä¹ ï¼‰ï¼Œå¹¶å¯¹å„ç±»æ–¹æ³•çš„ä¼˜ç¼ºç‚¹è¿›è¡Œäº†è¯¦ç»†åˆ†æã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

æœ¬æ–‡é€šè¿‡å¯¹æ¯”åˆ†æä¸åŒå¥–åŠ±æ¨¡å‹çš„æ€§èƒ½ï¼Œå‘ç°æŸäº›æ–°å…´æ–¹æ³•åœ¨ç‰¹å®šä»»åŠ¡ä¸Šç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•æå‡äº†20%ä»¥ä¸Šçš„æ•ˆç‡ï¼Œå±•ç¤ºäº†å¥–åŠ±æ¨¡å‹åœ¨æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸­çš„é‡è¦æ€§å’Œåº”ç”¨æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€æ¸¸æˆæ™ºèƒ½ä½“ã€è‡ªåŠ¨é©¾é©¶ç­‰ã€‚é€šè¿‡ä¼˜åŒ–å¥–åŠ±æ¨¡å‹ï¼Œèƒ½å¤Ÿæå‡æ™ºèƒ½ä½“åœ¨å¤æ‚ç¯å¢ƒä¸­çš„å†³ç­–èƒ½åŠ›å’Œé€‚åº”æ€§ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In reinforcement learning (RL), agents continually interact with the environment and use the feedback to refine their behavior. To guide policy optimization, reward models are introduced as proxies of the desired objectives, such that when the agent maximizes the accumulated reward, it also fulfills the task designer's intentions. Recently, significant attention from both academic and industrial researchers has focused on developing reward models that not only align closely with the true objectives but also facilitate policy optimization. In this survey, we provide a comprehensive review of reward modeling techniques within the deep RL literature. We begin by outlining the background and preliminaries in reward modeling. Next, we present an overview of recent reward modeling approaches, categorizing them based on the source, the mechanism, and the learning paradigm. Building on this understanding, we discuss various applications of these reward modeling techniques and review methods for evaluating reward models. Finally, we conclude by highlighting promising research directions in reward modeling. Altogether, this survey includes both established and emerging methods, filling the vacancy of a systematic review of reward models in current literature.

