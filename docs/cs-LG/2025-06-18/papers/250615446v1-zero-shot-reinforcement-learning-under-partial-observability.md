---
layout: default
title: Zero-Shot Reinforcement Learning Under Partial Observability
---

# Zero-Shot Reinforcement Learning Under Partial Observability

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.15446" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.15446v1</a>
  <a href="https://arxiv.org/pdf/2506.15446.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.15446v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.15446v1', 'Zero-Shot Reinforcement Learning Under Partial Observability')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Scott Jeen, Tom Bewley, Jonathan M. Cullen

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-18

**å¤‡æ³¨**: Reinforcement Learning Conference 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºè®°å¿†çš„é›¶-shotå¼ºåŒ–å­¦ä¹ ä»¥è§£å†³éƒ¨åˆ†å¯è§‚æµ‹æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `é›¶-shotå­¦ä¹ ` `å¼ºåŒ–å­¦ä¹ ` `éƒ¨åˆ†å¯è§‚æµ‹æ€§` `è®°å¿†æœºåˆ¶` `æ™ºèƒ½ä½“å†³ç­–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é›¶-shotå¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨é¢å¯¹éƒ¨åˆ†å¯è§‚æµ‹æ€§æ—¶æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œé™åˆ¶äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºè®°å¿†çš„é›¶-shot RLæ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥è®°å¿†æœºåˆ¶æ¥ç¼“è§£éƒ¨åˆ†å¯è§‚æµ‹æ€§å¸¦æ¥çš„æŒ‘æˆ˜ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºè®°å¿†çš„é›¶-shot RLæ–¹æ³•åœ¨å¤šä¸ªéƒ¨åˆ†å¯è§‚æµ‹çš„ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„æ— è®°å¿†åŸºçº¿ï¼Œæå‡äº†å­¦ä¹ æ•ˆæœã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘æœŸç ”ç©¶è¡¨æ˜ï¼Œåœ¨æŸäº›å‡è®¾ä¸‹ï¼Œé›¶-shotå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•å¯ä»¥åœ¨å¥–åŠ±æ— å…³çš„é¢„è®­ç»ƒåï¼Œå¯¹ç¯å¢ƒä¸­çš„ä»»ä½•æœªè§ä»»åŠ¡è¿›è¡Œæ³›åŒ–ã€‚ç„¶è€Œï¼Œè®¸å¤šå®é™…åº”ç”¨ä¸­ï¼Œé©¬å°”å¯å¤«çŠ¶æ€ä»…éƒ¨åˆ†å¯è§‚æµ‹ã€‚æœ¬æ–‡æ¢è®¨äº†æ ‡å‡†é›¶-shot RLæ–¹æ³•åœ¨éƒ¨åˆ†å¯è§‚æµ‹æ€§ä¸‹çš„æ€§èƒ½ä¸‹é™ï¼Œå¹¶è¯æ˜äº†åŸºäºè®°å¿†çš„æ¶æ„æ˜¯æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬åœ¨çŠ¶æ€ã€å¥–åŠ±å’ŒåŠ¨æ€å˜åŒ–éƒ¨åˆ†å¯è§‚æµ‹çš„é¢†åŸŸä¸­è¯„ä¼°äº†åŸºäºè®°å¿†çš„é›¶-shot RLæ–¹æ³•ï¼Œå¹¶æ˜¾ç¤ºå‡ºç›¸è¾ƒäºæ— è®°å¿†åŸºçº¿çš„æ€§èƒ½æå‡ã€‚æˆ‘ä»¬çš„ä»£ç å·²å¼€æºï¼Œé“¾æ¥ä¸ºï¼šhttps://enjeeneer.io/projects/bfms-with-memory/

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³é›¶-shotå¼ºåŒ–å­¦ä¹ åœ¨éƒ¨åˆ†å¯è§‚æµ‹ç¯å¢ƒä¸­çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å‡è®¾èƒ½å¤Ÿå®Œå…¨è§‚å¯Ÿåˆ°é©¬å°”å¯å¤«çŠ¶æ€ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™ä¸€å‡è®¾å¾€å¾€ä¸æˆç«‹ï¼Œå¯¼è‡´å­¦ä¹ æ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å¼•å…¥è®°å¿†æœºåˆ¶ï¼Œä½¿å¾—æ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨éƒ¨åˆ†å¯è§‚æµ‹çš„ç¯å¢ƒä¸­æ›´å¥½åœ°åˆ©ç”¨å†å²ä¿¡æ¯ï¼Œä»è€Œæé«˜å†³ç­–èƒ½åŠ›ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨å¼¥è¡¥ä¿¡æ¯ç¼ºå¤±å¸¦æ¥çš„å½±å“ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬çŠ¶æ€æ„ŸçŸ¥æ¨¡å—ã€è®°å¿†æ¨¡å—å’Œå†³ç­–æ¨¡å—ã€‚çŠ¶æ€æ„ŸçŸ¥æ¨¡å—è´Ÿè´£å¤„ç†éƒ¨åˆ†å¯è§‚æµ‹çš„ä¿¡æ¯ï¼Œè®°å¿†æ¨¡å—å­˜å‚¨å†å²çŠ¶æ€å’Œå†³ç­–ä¿¡æ¯ï¼Œå†³ç­–æ¨¡å—åŸºäºå½“å‰çŠ¶æ€å’Œè®°å¿†è¿›è¡ŒåŠ¨ä½œé€‰æ‹©ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå°†è®°å¿†æœºåˆ¶æœ‰æ•ˆæ•´åˆè¿›é›¶-shot RLæ¡†æ¶ä¸­ï¼Œä½¿å¾—æ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨éƒ¨åˆ†å¯è§‚æµ‹ç¯å¢ƒä¸­ä¿æŒè¾ƒé«˜çš„å­¦ä¹ æ•ˆç‡ã€‚è¿™ä¸ä¼ ç»Ÿçš„æ— è®°å¿†æ–¹æ³•å½¢æˆäº†é²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„è®°å¿†ç½‘ç»œç»“æ„ï¼Œä»¥ä¾¿äºå­˜å‚¨å’Œæ£€ç´¢å†å²ä¿¡æ¯ã€‚åŒæ—¶ï¼ŒæŸå¤±å‡½æ•°çš„è®¾è®¡ä¹Ÿè€ƒè™‘äº†è®°å¿†çš„å½±å“ï¼Œä»¥ç¡®ä¿æ™ºèƒ½ä½“èƒ½å¤Ÿæœ‰æ•ˆåœ°å­¦ä¹ åˆ°æœ‰ç”¨çš„ç­–ç•¥ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨å®éªŒéƒ¨åˆ†æœ‰è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸºäºè®°å¿†çš„é›¶-shot RLæ–¹æ³•åœ¨å¤šä¸ªéƒ¨åˆ†å¯è§‚æµ‹ä»»åŠ¡ä¸­ï¼Œç›¸è¾ƒäºæ— è®°å¿†åŸºçº¿ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œè¯æ˜äº†è®°å¿†æœºåˆ¶åœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§å’Œé‡è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½æ¸¸æˆç­‰éœ€è¦åœ¨éƒ¨åˆ†å¯è§‚æµ‹ç¯å¢ƒä¸­è¿›è¡Œå†³ç­–çš„åœºæ™¯ã€‚é€šè¿‡æå‡é›¶-shot RLåœ¨è¿™äº›é¢†åŸŸçš„æ€§èƒ½ï¼Œèƒ½å¤ŸåŠ é€Ÿæ™ºèƒ½ä½“çš„å­¦ä¹ è¿‡ç¨‹ï¼Œé™ä½å¯¹å¤§é‡æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent work has shown that, under certain assumptions, zero-shot reinforcement learning (RL) methods can generalise to any unseen task in an environment after reward-free pre-training. Access to Markov states is one such assumption, yet, in many real-world applications, the Markov state is only partially observable. Here, we explore how the performance of standard zero-shot RL methods degrades when subjected to partially observability, and show that, as in single-task RL, memory-based architectures are an effective remedy. We evaluate our memory-based zero-shot RL methods in domains where the states, rewards and a change in dynamics are partially observed, and show improved performance over memory-free baselines. Our code is open-sourced via: https://enjeeneer.io/projects/bfms-with-memory/.

