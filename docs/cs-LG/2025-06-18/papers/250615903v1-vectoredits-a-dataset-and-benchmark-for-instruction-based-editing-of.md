---
layout: default
title: VectorEdits: A Dataset and Benchmark for Instruction-Based Editing of Vector Graphics
---

# VectorEdits: A Dataset and Benchmark for Instruction-Based Editing of Vector Graphics

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.15903" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.15903v1</a>
  <a href="https://arxiv.org/pdf/2506.15903.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.15903v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.15903v1', 'VectorEdits: A Dataset and Benchmark for Instruction-Based Editing of Vector Graphics')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Josef KuchaÅ™, Marek KadlÄÃ­k, Michal Spiegel, Michal Å tefÃ¡nik

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVectorEditsæ•°æ®é›†ä»¥è§£å†³åŸºäºæŒ‡ä»¤çš„çŸ¢é‡å›¾å½¢ç¼–è¾‘é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `çŸ¢é‡å›¾å½¢ç¼–è¾‘` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ•°æ®é›†æ„å»º` `è§†è§‰-è¯­è¨€æ¨¡å‹` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨ç”Ÿæˆå‡†ç¡®æœ‰æ•ˆçš„çŸ¢é‡å›¾å½¢ç¼–è¾‘æ—¶å­˜åœ¨æ˜¾è‘—å›°éš¾ï¼Œå°¤å…¶æ˜¯åœ¨ç†è§£å’Œæ‰§è¡Œè‡ªç„¶è¯­è¨€æŒ‡ä»¤æ–¹é¢ã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ä¸ªåŒ…å«270,000å¯¹SVGå›¾åƒå’Œå¯¹åº”è‡ªç„¶è¯­è¨€æŒ‡ä»¤çš„æ•°æ®é›†ï¼Œä»¥æ”¯æŒåŸºäºæŒ‡ä»¤çš„çŸ¢é‡å›¾å½¢ç¼–è¾‘æ¨¡å‹çš„è®­ç»ƒå’Œè¯„ä¼°ã€‚
3. åˆæ­¥å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å‰çš„å…ˆè¿›è¯­è¨€æ¨¡å‹åœ¨æ‰§è¡Œç¼–è¾‘ä»»åŠ¡æ—¶çš„è¡¨ç°ä¸ä½³ï¼Œå¼ºè°ƒäº†è¯¥é¢†åŸŸçš„ç ”ç©¶éœ€æ±‚ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬ä»‹ç»äº†ä¸€ä¸ªå¤§è§„æ¨¡çš„æ•°æ®é›†ï¼Œç”¨äºåŸºäºæŒ‡ä»¤çš„çŸ¢é‡å›¾åƒç¼–è¾‘ï¼ŒåŒ…å«è¶…è¿‡270,000å¯¹SVGå›¾åƒå’Œè‡ªç„¶è¯­è¨€ç¼–è¾‘æŒ‡ä»¤ã€‚è¯¥æ•°æ®é›†æ”¯æŒåŸºäºæ–‡æœ¬å‘½ä»¤ä¿®æ”¹çŸ¢é‡å›¾å½¢çš„æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°ã€‚æˆ‘ä»¬æè¿°äº†æ•°æ®æ”¶é›†è¿‡ç¨‹ï¼ŒåŒ…æ‹¬é€šè¿‡CLIPç›¸ä¼¼åº¦è¿›è¡Œå›¾åƒé…å¯¹å’Œä½¿ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹ç”ŸæˆæŒ‡ä»¤ã€‚åˆæ­¥å®éªŒè¡¨æ˜ï¼Œç°æœ‰çš„å…ˆè¿›å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆå‡†ç¡®æœ‰æ•ˆçš„ç¼–è¾‘æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œçªæ˜¾äº†è¯¥ä»»åŠ¡çš„å¤æ‚æ€§ã€‚ä¸ºä¿ƒè¿›è‡ªç„¶è¯­è¨€é©±åŠ¨çš„çŸ¢é‡å›¾å½¢ç”Ÿæˆä¸ç¼–è¾‘ç ”ç©¶ï¼Œæˆ‘ä»¬å°†æœ¬ç ”ç©¶ä¸­åˆ›å»ºçš„èµ„æºå…¬å¼€å‘å¸ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³åŸºäºè‡ªç„¶è¯­è¨€æŒ‡ä»¤çš„çŸ¢é‡å›¾å½¢ç¼–è¾‘é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨ç†è§£å’Œæ‰§è¡Œè¿™äº›æŒ‡ä»¤æ—¶å­˜åœ¨å‡†ç¡®æ€§å’Œæœ‰æ•ˆæ€§ä¸è¶³çš„æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªå¤§è§„æ¨¡çš„æ•°æ®é›†ï¼Œç»“åˆå›¾åƒå’Œè‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œä»¥ä¾¿è®­ç»ƒæ¨¡å‹è¿›è¡ŒçŸ¢é‡å›¾å½¢çš„æŒ‡ä»¤é©±åŠ¨ç¼–è¾‘ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹èƒ½å¤Ÿå­¦ä¹ å¦‚ä½•æ ¹æ®æ–‡æœ¬æŒ‡ä»¤è¿›è¡Œå›¾å½¢ä¿®æ”¹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€å›¾åƒé…å¯¹å’ŒæŒ‡ä»¤ç”Ÿæˆä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚æ•°æ®æ”¶é›†é€šè¿‡CLIPæ¨¡å‹è¿›è¡Œå›¾åƒç›¸ä¼¼åº¦åŒ¹é…ï¼ŒæŒ‡ä»¤ç”Ÿæˆåˆ™åˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹æ¥åˆ›å»ºè‡ªç„¶è¯­è¨€æè¿°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„æŒ‡ä»¤-å›¾åƒé…å¯¹æ•°æ®é›†ï¼Œè¿™åœ¨å½“å‰çš„ç ”ç©¶ä¸­å°šå±é¦–æ¬¡ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ•°æ®é›†æä¾›äº†æ›´ä¸°å¯Œçš„è®­ç»ƒæ ·æœ¬ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡æ¨¡å‹çš„å­¦ä¹ æ•ˆæœã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°æ®é›†æ„å»ºè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†é«˜æ•ˆçš„å›¾åƒé…å¯¹ç®—æ³•å’Œå…ˆè¿›çš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œç¡®ä¿ç”Ÿæˆçš„æŒ‡ä»¤ä¸å›¾åƒä¹‹é—´å…·æœ‰é«˜ç›¸å…³æ€§ã€‚æ­¤å¤–ï¼Œæ•°æ®é›†çš„è§„æ¨¡å’Œå¤šæ ·æ€§ä¹Ÿæ˜¯å…¶è®¾è®¡ä¸­çš„å…³é”®è€ƒè™‘å› ç´ ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå½“å‰çš„å…ˆè¿›å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ‰§è¡ŒåŸºäºæŒ‡ä»¤çš„çŸ¢é‡å›¾å½¢ç¼–è¾‘ä»»åŠ¡æ—¶è¡¨ç°ä¸ä½³ï¼Œå‡†ç¡®æ€§å’Œæœ‰æ•ˆæ€§å‡æœªè¾¾åˆ°é¢„æœŸã€‚è¿™ä¸€å‘ç°å¼ºè°ƒäº†è¯¥é¢†åŸŸçš„ç ”ç©¶éœ€æ±‚ï¼Œå¹¶ä¸ºæœªæ¥çš„æ”¹è¿›æä¾›äº†æ–¹å‘ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å›¾å½¢è®¾è®¡ã€æ¸¸æˆå¼€å‘å’Œæ•™è‚²ç­‰ã€‚é€šè¿‡æä¾›ä¸€ä¸ªæ ‡å‡†åŒ–çš„æ•°æ®é›†ï¼Œç ”ç©¶äººå‘˜å’Œå¼€å‘è€…å¯ä»¥æ›´æ–¹ä¾¿åœ°è®­ç»ƒå’Œè¯„ä¼°åŸºäºè‡ªç„¶è¯­è¨€çš„çŸ¢é‡å›¾å½¢ç¼–è¾‘æ¨¡å‹ï¼Œä»è€Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•å’Œåº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We introduce a large-scale dataset for instruction-guided vector image editing, consisting of over 270,000 pairs of SVG images paired with natural language edit instructions. Our dataset enables training and evaluation of models that modify vector graphics based on textual commands. We describe the data collection process, including image pairing via CLIP similarity and instruction generation with vision-language models. Initial experiments with state-of-the-art large language models reveal that current methods struggle to produce accurate and valid edits, underscoring the challenge of this task. To foster research in natural language-driven vector graphic generation and editing, we make our resources created within this work publicly available.

