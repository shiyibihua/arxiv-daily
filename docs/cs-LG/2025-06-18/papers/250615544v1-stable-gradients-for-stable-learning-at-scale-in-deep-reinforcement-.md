---
layout: default
title: Stable Gradients for Stable Learning at Scale in Deep Reinforcement Learning
---

# Stable Gradients for Stable Learning at Scale in Deep Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.15544" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.15544v1</a>
  <a href="https://arxiv.org/pdf/2506.15544.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.15544v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.15544v1', 'Stable Gradients for Stable Learning at Scale in Deep Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Roger Creus Castanyer, Johan Obando-Ceron, Lu Li, Pierre-Luc Bacon, Glen Berseth, Aaron Courville, Pablo Samuel Castro

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç¨³å®šæ¢¯åº¦æ–¹æ³•ä»¥è§£å†³æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„è§„æ¨¡åŒ–æŒ‘æˆ˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ·±åº¦å¼ºåŒ–å­¦ä¹ ` `æ¢¯åº¦ç¨³å®šåŒ–` `ç½‘ç»œè§„æ¨¡åŒ–` `æ€§èƒ½æå‡` `éå¹³ç¨³æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ·±åº¦å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨è§„æ¨¡åŒ–æ—¶å¸¸å‡ºç°æ€§èƒ½ä¸‹é™ï¼Œä¸”å…¶æ ¹æœ¬åŸå› å°šä¸æ¸…æ™°ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç³»åˆ—ç®€å•çš„å¹²é¢„æªæ–½ï¼Œæ—¨åœ¨ç¨³å®šæ¢¯åº¦æµï¼Œä»è€Œæé«˜ç½‘ç»œåœ¨ä¸åŒè§„æ¨¡ä¸‹çš„æ€§èƒ½ã€‚
3. é€šè¿‡åœ¨å¤šç§ç¯å¢ƒå’Œæ™ºèƒ½ä½“ä¸Šè¿›è¡ŒéªŒè¯ï¼Œå®éªŒç»“æœæ˜¾ç¤ºè¿™äº›æ–¹æ³•æœ‰æ•ˆæå‡äº†æ€§èƒ½ï¼Œå°¤å…¶åœ¨å¤§è§„æ¨¡è®¾ç½®ä¸‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ·±åº¦å¼ºåŒ–å­¦ä¹ ç½‘ç»œçš„è§„æ¨¡åŒ–é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼Œä¸”æ€§èƒ½å¸¸å¸¸ä¸‹é™ï¼Œç„¶è€Œè¿™äº›å¤±è´¥æ¨¡å¼çš„æ ¹æœ¬åŸå› å°šä¸æ˜ç¡®ã€‚æœ¬æ–‡é€šè¿‡ä¸€ç³»åˆ—å®è¯åˆ†æï¼ŒæŒ‡å‡ºéå¹³ç¨³æ€§ä¸ç”±äºæ¬¡ä¼˜æ¶æ„é€‰æ‹©å¯¼è‡´çš„æ¢¯åº¦ç—…æ€æ˜¯è§„æ¨¡åŒ–çš„ä¸»è¦æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç³»åˆ—ç›´æ¥å¹²é¢„æªæ–½ï¼Œä»¥ç¨³å®šæ¢¯åº¦æµï¼Œä»è€Œåœ¨ä¸åŒç½‘ç»œæ·±åº¦å’Œå®½åº¦ä¸‹å®ç°ç¨³å¥çš„æ€§èƒ½ã€‚è¿™äº›å¹²é¢„æªæ–½ç®€å•æ˜“è¡Œï¼Œä¸å·²æœ‰ç®—æ³•å…¼å®¹ï¼Œèƒ½å¤Ÿåœ¨å¤§è§„æ¨¡ä¸‹å®ç°å¼ºå¤§çš„æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨å¤šç§æ™ºèƒ½ä½“å’Œç¯å¢ƒå¥—ä»¶ä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„å‘ç°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ·±åº¦å¼ºåŒ–å­¦ä¹ åœ¨è§„æ¨¡åŒ–è¿‡ç¨‹ä¸­æ€§èƒ½ä¸‹é™çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€å¤æ‚ï¼Œæœªèƒ½æœ‰æ•ˆæ­ç¤ºå¯¼è‡´è¿™ä¸€é—®é¢˜çš„æ ¹æœ¬åŸå› ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ç›´æ¥å¹²é¢„æ¥ç¨³å®šæ¢¯åº¦æµï¼Œä»¥åº”å¯¹éå¹³ç¨³æ€§å’Œæ¢¯åº¦ç—…æ€çš„æŒ‘æˆ˜ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨ç®€åŒ–å®ç°è¿‡ç¨‹ï¼ŒåŒæ—¶ä¿æŒä¸ç°æœ‰ç®—æ³•çš„å…¼å®¹æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¯¹ç½‘ç»œæ·±åº¦å’Œå®½åº¦çš„è°ƒæ•´ï¼Œä»¥åŠåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹æ¢¯åº¦æµçš„ç›‘æ§å’Œå¹²é¢„ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬æ¢¯åº¦ç¨³å®šåŒ–æ¨¡å—å’Œæ€§èƒ½è¯„ä¼°æ¨¡å—ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ¢¯åº¦ç¨³å®šåŒ–æœºåˆ¶ï¼Œä¸ç°æœ‰å¤æ‚æ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—é™ä½äº†å®ç°éš¾åº¦å¹¶æé«˜äº†æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œæˆ‘ä»¬ä¼˜åŒ–äº†ç½‘ç»œæ¶æ„ï¼Œé‡‡ç”¨äº†é€‚åº”æ€§å­¦ä¹ ç‡å’Œæ­£åˆ™åŒ–æŠ€æœ¯ï¼Œä»¥ç¡®ä¿æ¢¯åº¦æµçš„ç¨³å®šæ€§ã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡ä¹Ÿç»è¿‡è°ƒæ•´ï¼Œä»¥é€‚åº”ä¸åŒè§„æ¨¡çš„ç½‘ç»œã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å¤šç§ç¯å¢ƒä¸‹æ˜¾è‘—æå‡äº†æ™ºèƒ½ä½“çš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨å¤§è§„æ¨¡è®¾ç½®ä¸­ï¼Œæ€§èƒ½æå‡å¹…åº¦å¯è¾¾20%ä»¥ä¸Šï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•è¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§å’Œç¨³å®šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€æ¸¸æˆæ™ºèƒ½ä½“ã€è‡ªåŠ¨é©¾é©¶ç­‰æ·±åº¦å¼ºåŒ–å­¦ä¹ ç›¸å…³çš„å®é™…åœºæ™¯ã€‚é€šè¿‡æé«˜æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„ç¨³å®šæ€§å’Œæ€§èƒ½ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ¨åŠ¨æ›´å¤æ‚ä»»åŠ¡çš„å®ç°ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Scaling deep reinforcement learning networks is challenging and often results in degraded performance, yet the root causes of this failure mode remain poorly understood. Several recent works have proposed mechanisms to address this, but they are often complex and fail to highlight the causes underlying this difficulty. In this work, we conduct a series of empirical analyses which suggest that the combination of non-stationarity with gradient pathologies, due to suboptimal architectural choices, underlie the challenges of scale. We propose a series of direct interventions that stabilize gradient flow, enabling robust performance across a range of network depths and widths. Our interventions are simple to implement and compatible with well-established algorithms, and result in an effective mechanism that enables strong performance even at large scales. We validate our findings on a variety of agents and suites of environments.

