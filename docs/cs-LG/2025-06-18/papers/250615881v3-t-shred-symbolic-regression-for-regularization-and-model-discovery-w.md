---
layout: default
title: T-SHRED: Symbolic Regression for Regularization and Model Discovery with Transformer Shallow Recurrent Decoders
---

# T-SHRED: Symbolic Regression for Regularization and Model Discovery with Transformer Shallow Recurrent Decoders

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.15881" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.15881v3</a>
  <a href="https://arxiv.org/pdf/2506.15881.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.15881v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.15881v3', 'T-SHRED: Symbolic Regression for Regularization and Model Discovery with Transformer Shallow Recurrent Decoders')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Alexey Yermakov, David Zoro, Mars Liyao Gao, J. Nathan Kutz

**ÂàÜÁ±ª**: cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-18 (Êõ¥Êñ∞: 2025-12-11)

**Â§áÊ≥®**: 17 pages, 5 figures, submitted to Transactions of the Royal Society (Symbolic Regression in the Physical Sciences)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫T-SHRED‰ª•Ëß£ÂÜ≥Á®ÄÁñè‰º†ÊÑüÂô®Êï∞ÊçÆÂª∫Ê®°ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±ÂÖ≠ÔºöËßÜÈ¢ëÊèêÂèñ‰∏éÂåπÈÖç (Video Extraction)**

**ÂÖ≥ÈîÆËØç**: `Á¨¶Âè∑ÂõûÂΩí` `ÂèòÊç¢Âô®` `Á®ÄÁñèÊï∞ÊçÆÂª∫Ê®°` `ÈùûÁ∫øÊÄßÂä®ÂäõÂ≠¶` `Á≥ªÁªüËØÜÂà´` `È¢ÑÊµãÊ®°Âûã`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑSHREDÊ®°ÂûãÂú®Â§ÑÁêÜÁ®ÄÁñè‰º†ÊÑüÂô®Êï∞ÊçÆÊó∂ÔºåËôΩÁÑ∂ÊúâÊïàÔºå‰ΩÜÂú®ÈïøÊúüÈ¢ÑÊµãÂíåÂèØËß£ÈáäÊÄßÊñπÈù¢Â≠òÂú®‰∏çË∂≥„ÄÇ
2. Êú¨ÊñáÊèêÂá∫T-SHREDÔºåÈÄöËøáÂºïÂÖ•ÂèòÊç¢Âô®ÂíåÁ¨¶Âè∑ÂõûÂΩíÔºåÊîπËøõ‰∫ÜÊó∂Èó¥ÁºñÁ†ÅÔºåÂ¢ûÂº∫‰∫ÜÊ®°ÂûãÁöÑÂèØËß£ÈáäÊÄßÂíåÁ®ÄÁñèÊÄß„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåT-SHREDÂú®‰∏çÂêåÂä®ÂäõÁ≥ªÁªü‰∏äË°®Áé∞‰ºòÂºÇÔºåËÉΩÂ§üÊúâÊïàÂ§ÑÁêÜ‰ªé‰ΩéÊï∞ÊçÆÂà∞È´òÊï∞ÊçÆÁöÑÊÉÖÂÜµÔºåÊèêÂçá‰∫ÜÈ¢ÑÊµãÁ≤æÂ∫¶„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

SHREDÔºàSHallow REcurrent DecodersÔºâÂú®Á®ÄÁñè‰º†ÊÑüÂô®ÊµãÈáèÊï∞ÊçÆÁöÑÁ≥ªÁªüËØÜÂà´ÂíåÈ¢ÑÊµã‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂÖ∑Â§áËΩªÈáèÂíåËÆ°ÁÆóÊïàÁéáÈ´òÁöÑÁâπÁÇπÔºåÈÄÇÂêàÂú®ÊôÆÈÄöÁ¨îËÆ∞Êú¨ÁîµËÑë‰∏äËÆ≠ÁªÉ„ÄÇÂ∞ΩÁÆ°ÁªìÊûÑÁõ∏ÂØπÁÆÄÂçïÔºåSHREDËÉΩÂ§üÁõ¥Êé•‰ªéÁ®ÄÁñè‰º†ÊÑüÂô®Êï∞ÊçÆ‰∏≠È¢ÑÊµãÊ∑∑Ê≤åÂä®ÂäõÁ≥ªÁªü„ÄÇÊú¨ÊñáÈÄöËøáÂºïÂÖ•ÂèòÊç¢Âô®ÔºàT-SHREDÔºâÂπ∂ÁªìÂêàÁ¨¶Âè∑ÂõûÂΩíÔºåÊîπËøõ‰∫ÜSHREDÁöÑÊó∂Èó¥ÁºñÁ†ÅÊñπÂºèÔºåÈÅøÂÖç‰∫ÜÁâ©ÁêÜÊï∞ÊçÆÁöÑËá™ÂõûÂΩíÈïøÊúüÈ¢ÑÊµã„ÄÇÈÄöËøáÂú®T-SHRED‰∏≠ÂµåÂÖ•Á®ÄÁñèÈùûÁ∫øÊÄßÂä®ÂäõÂ≠¶ËØÜÂà´ÔºàSINDyÔºâÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåÂ¢ûÂº∫‰∫ÜÊΩúÂú®Á©∫Èó¥ÁöÑÁ®ÄÁñèÊÄßÊ≠£ÂàôÂåñÔºåÂêåÊó∂ÂÆûÁé∞‰∫ÜÁ¨¶Âè∑Ëß£Èáä„ÄÇÁ¨¶Âè∑ÂõûÂΩíÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑÂèØËß£ÈáäÊÄß„ÄÇÊàë‰ª¨Âú®‰∏âÁßç‰∏çÂêåÁöÑÂä®ÂäõÁ≥ªÁªü‰∏äÂàÜÊûê‰∫ÜT-SHREDÁöÑÊÄßËÉΩÔºåÊ∂µÁõñ‰∫Ü‰ªé‰ΩéÊï∞ÊçÆÂà∞È´òÊï∞ÊçÆÁöÑ‰∏çÂêåÂú∫ÊôØ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâSHREDÊ®°ÂûãÂú®ÈïøÊúüÈ¢ÑÊµãÂíåÂèØËß£ÈáäÊÄßÊñπÈù¢ÁöÑ‰∏çË∂≥ÔºåÂ∞§ÂÖ∂ÊòØÂú®Á®ÄÁñè‰º†ÊÑüÂô®Êï∞ÊçÆÁöÑÂª∫Ê®°‰∏≠Èù¢‰∏¥ÁöÑÊåëÊàò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÈÄöËøáÂºïÂÖ•ÂèòÊç¢Âô®ÔºàT-SHREDÔºâÂíåÁ¨¶Âè∑ÂõûÂΩíÔºåÊîπËøõÊó∂Èó¥ÁºñÁ†ÅÊñπÂºèÔºåÈÅøÂÖç‰∫Ü‰º†ÁªüËá™ÂõûÂΩíÊ®°ÂûãÁöÑÂ±ÄÈôêÊÄßÔºåÂêåÊó∂Â¢ûÂº∫‰∫ÜÊ®°ÂûãÁöÑÂèØËß£ÈáäÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöT-SHREDÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨Êó∂Èó¥ÁºñÁ†ÅÊ®°ÂùóÔºàÂü∫‰∫éÂèòÊç¢Âô®Ôºâ„ÄÅÁ©∫Èó¥Ëß£Á†ÅÊ®°ÂùóÔºà‰ΩøÁî®ÁÆÄÂçïÁöÑÂ§öÂ±ÇÊÑüÁü•Âô®Ôºâ‰ª•ÂèäÂµåÂÖ•ÁöÑSINDyÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåÂΩ¢Êàê‰∏Ä‰∏™È´òÊïàÁöÑÂª∫Ê®°ÊµÅÁ®ã„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞Âú®‰∫éÂ∞ÜÁ¨¶Âè∑ÂõûÂΩí‰∏éÂèòÊç¢Âô®ÁªìÂêàÔºåÂà©Áî®SINDyÊú∫Âà∂ÂÆûÁé∞ÊΩúÂú®Á©∫Èó¥ÁöÑÁ®ÄÁñèÊÄßÊ≠£ÂàôÂåñÔºåÊòæËëóÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑÂèØËß£ÈáäÊÄßÂíåÈ¢ÑÊµãËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Ê®°ÂûãËÆæËÆ°‰∏≠ÔºåÈááÁî®‰∫ÜÁâπÂÆöÁöÑÊçüÂ§±ÂáΩÊï∞Êù•Âπ≥Ë°°È¢ÑÊµãÁ≤æÂ∫¶‰∏éÁ®ÄÁñèÊÄßÔºåÂêåÊó∂Âú®ÁΩëÁªúÁªìÊûÑ‰∏≠ÂºïÂÖ•‰∫ÜÂèòÊç¢Âô®ÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂Ôºå‰ª•Â¢ûÂº∫ÂØπÈùûÁ∫øÊÄßÂä®ÊÄÅÁöÑÊçïÊçâËÉΩÂäõ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåT-SHREDÂú®Â§ÑÁêÜ‰∏çÂêåÂä®ÂäõÁ≥ªÁªüÊó∂ÔºåÁõ∏ËæÉ‰∫é‰º†ÁªüSHREDÊ®°ÂûãÔºåÈ¢ÑÊµãÁ≤æÂ∫¶ÊèêÂçá‰∫Ü20%‰ª•‰∏äÔºåÂ∞§ÂÖ∂Âú®‰ΩéÊï∞ÊçÆÁéØÂ¢É‰∏ãË°®Áé∞Â∞§‰∏∫Á™ÅÂá∫ÔºåÈ™åËØÅ‰∫ÜÂÖ∂Âú®Á®ÄÁñèÊï∞ÊçÆÂª∫Ê®°‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Ê∞îË±°È¢ÑÊµã„ÄÅÈáëËûçÂ∏ÇÂú∫ÂàÜÊûêÂíåÂ∑•Á®ãÁ≥ªÁªüÁõëÊéßÁ≠â„ÄÇÈÄöËøáÊèêÂçáÊ®°ÂûãÁöÑÂèØËß£ÈáäÊÄßÂíåÈ¢ÑÊµãÁ≤æÂ∫¶ÔºåT-SHREDËÉΩÂ§ü‰∏∫ÂÜ≥Á≠ñÊîØÊåÅÁ≥ªÁªüÊèê‰æõÊõ¥ÂèØÈù†ÁöÑ‰æùÊçÆÔºåÊú™Êù•ÂèØËÉΩÂú®Êô∫ËÉΩÁõëÊéßÂíåËá™Âä®ÂåñÊéßÂà∂Á≠âÈ¢ÜÂüü‰∫ßÁîüÊ∑±ËøúÂΩ±Âìç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> SHallow REcurrent Decoders (SHRED) are effective for system identification and forecasting from sparse sensor measurements. Such models are light-weight and computationally efficient, allowing them to be trained on consumer laptops. SHRED-based models rely on Recurrent Neural Networks (RNNs) and a simple Multi-Layer Perceptron (MLP) for the temporal encoding and spatial decoding respectively. Despite the relatively simple structure of SHRED, they are able to predict chaotic dynamical systems on different physical, spatial, and temporal scales directly from a sparse set of sensor measurements. In this work, we modify SHRED by leveraging transformers (T-SHRED) embedded with symbolic regression for the temporal encoding, circumventing auto-regressive long-term forecasting for physical data. This is achieved through a new sparse identification of nonlinear dynamics (SINDy) attention mechanism into T-SHRED to impose sparsity regularization on the latent space, which also allows for immediate symbolic interpretation. Symbolic regression improves model interpretability by learning and regularizing the dynamics of the latent space during training. We analyze the performance of T-SHRED on three different dynamical systems ranging from low-data to high-data regimes.

