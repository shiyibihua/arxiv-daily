---
layout: default
title: Over-squashing in Spatiotemporal Graph Neural Networks
---

# Over-squashing in Spatiotemporal Graph Neural Networks

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.15507" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.15507v2</a>
  <a href="https://arxiv.org/pdf/2506.15507.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.15507v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.15507v2', 'Over-squashing in Spatiotemporal Graph Neural Networks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ivan Marisca, Jacob Bamberger, Cesare Alippi, Michael M. Bronstein

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-18 (æ›´æ–°: 2025-11-02)

**å¤‡æ³¨**: Accepted at NeurIPS 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ—¶ç©ºå›¾ç¥ç»ç½‘ç»œä¸­çš„è¿‡åº¦å‹ç¼©é—®é¢˜è§£å†³æ–¹æ¡ˆ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation)**

**å…³é”®è¯**: `å›¾ç¥ç»ç½‘ç»œ` `æ—¶ç©ºæ•°æ®` `ä¿¡æ¯ä¼ æ’­` `è¿‡åº¦å‹ç¼©` `å·ç§¯ç½‘ç»œ` `ç†è®ºåˆ†æ` `åŠ¨æ€ç¯å¢ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å›¾ç¥ç»ç½‘ç»œåœ¨ä¿¡æ¯ä¼ æ’­èƒ½åŠ›ä¸Šå­˜åœ¨è¿‡åº¦å‹ç¼©é—®é¢˜ï¼Œå°¤å…¶åœ¨æ—¶ç©ºå›¾ç¥ç»ç½‘ç»œä¸­è¡¨ç°å¾—å°¤ä¸ºæ˜æ˜¾ã€‚
2. è®ºæ–‡é€šè¿‡å½¢å¼åŒ–æ—¶ç©ºè¿‡åº¦å‹ç¼©é—®é¢˜ï¼Œæ­ç¤ºäº†å…¶ä¸é™æ€å›¾ç¥ç»ç½‘ç»œçš„ä¸åŒç‰¹å¾ï¼Œå¹¶æå‡ºäº†ç›¸åº”çš„ç†è®ºåˆ†æã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œå·ç§¯STGNNsåœ¨ä¿¡æ¯ä¼ æ’­ä¸Šæ›´å€¾å‘äºè¿œè·ç¦»èŠ‚ç‚¹ï¼Œè€Œéè¿‘è·ç¦»èŠ‚ç‚¹ï¼ŒéªŒè¯äº†ç†è®ºåˆ†æçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰åœ¨å¤šä¸ªé¢†åŸŸå–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†æœ€è¿‘çš„ç†è®ºç ”ç©¶æ­ç¤ºäº†å…¶ä¿¡æ¯ä¼ æ’­èƒ½åŠ›çš„åŸºæœ¬å±€é™æ€§ï¼Œå¦‚è¿‡åº¦å‹ç¼©é—®é¢˜ï¼Œå³è¿œè·ç¦»èŠ‚ç‚¹æ— æ³•æœ‰æ•ˆäº¤æ¢ä¿¡æ¯ã€‚å°½ç®¡è¿™ä¸€é—®é¢˜åœ¨é™æ€ä¸Šä¸‹æ–‡ä¸­å¾—åˆ°äº†å¹¿æ³›ç ”ç©¶ï¼Œä½†åœ¨å¤„ç†ä¸å›¾èŠ‚ç‚¹ç›¸å…³çš„åºåˆ—çš„æ—¶ç©ºå›¾ç¥ç»ç½‘ç»œï¼ˆSTGNNsï¼‰ä¸­ä»æœªå¾—åˆ°æ¢è®¨ã€‚æœ¬æ–‡æ­£å¼å®šä¹‰äº†æ—¶ç©ºè¿‡åº¦å‹ç¼©é—®é¢˜ï¼Œå¹¶å±•ç¤ºäº†å…¶ä¸é™æ€æƒ…å†µçš„ä¸åŒç‰¹å¾ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œå·ç§¯STGNNsæ›´å€¾å‘äºä»æ—¶é—´ä¸Šç›¸è·è¾ƒè¿œçš„ç‚¹ä¼ æ’­ä¿¡æ¯ï¼Œè€Œéæ—¶é—´ä¸Šç›¸è¿‘çš„ç‚¹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜äº†éµå¾ªæ—¶é—´ä¸ç©ºé—´æˆ–æ—¶é—´åç©ºé—´å¤„ç†èŒƒå¼çš„æ¶æ„åŒæ ·å—åˆ°è¿™ä¸€ç°è±¡çš„å½±å“ï¼Œä¸ºè®¡ç®—ä¸Šé«˜æ•ˆçš„å®ç°æä¾›äº†ç†è®ºä¾æ®ã€‚æˆ‘ä»¬åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„å‘ç°ï¼Œä¸ºæ›´æœ‰æ•ˆçš„è®¾è®¡æä¾›äº†æ·±å…¥çš„è§è§£å’ŒåŸåˆ™æ€§æŒ‡å¯¼ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡èšç„¦äºæ—¶ç©ºå›¾ç¥ç»ç½‘ç»œä¸­çš„è¿‡åº¦å‹ç¼©é—®é¢˜ï¼ŒæŒ‡å‡ºç°æœ‰æ–¹æ³•åœ¨å¤„ç†è¿œè·ç¦»èŠ‚ç‚¹ä¿¡æ¯ä¼ æ’­æ—¶çš„ä¸è¶³ï¼Œå¯¼è‡´ä¿¡æ¯äº¤æ¢æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å½¢å¼åŒ–æ—¶ç©ºè¿‡åº¦å‹ç¼©é—®é¢˜ï¼Œè®ºæ–‡æ­ç¤ºäº†å·ç§¯STGNNsåœ¨ä¿¡æ¯ä¼ æ’­ä¸­çš„ç‹¬ç‰¹è¡Œä¸ºï¼Œæå‡ºäº†ç†è®ºåˆ†æä»¥æŒ‡å¯¼æ›´æœ‰æ•ˆçš„ç½‘ç»œè®¾è®¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é¦–å…ˆå®šä¹‰äº†æ—¶ç©ºè¿‡åº¦å‹ç¼©çš„ç‰¹å¾ï¼Œç„¶åé€šè¿‡ç†è®ºåˆ†æå’Œå®éªŒéªŒè¯ï¼Œæ¢è®¨äº†ä¸åŒå¤„ç†èŒƒå¼å¯¹ä¿¡æ¯ä¼ æ’­çš„å½±å“ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„ä¸»è¦åˆ›æ–°åœ¨äºé¦–æ¬¡å°†è¿‡åº¦å‹ç¼©é—®é¢˜å¼•å…¥æ—¶ç©ºå›¾ç¥ç»ç½‘ç»œçš„ç ”ç©¶ï¼Œæ­ç¤ºäº†å·ç§¯ç½‘ç»œåœ¨ä¿¡æ¯ä¼ æ’­ä¸­çš„åç›´è§‰ç‰¹æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†åˆæˆå’ŒçœŸå®æ•°æ®é›†ï¼Œè®¾è®¡äº†å¤šç§ç½‘ç»œæ¶æ„ä»¥éªŒè¯ç†è®ºåˆ†æï¼Œé‡ç‚¹å…³æ³¨æ—¶é—´ä¸ç©ºé—´å¤„ç†çš„ä¸åŒç»„åˆå¯¹ä¿¡æ¯ä¼ æ’­çš„å½±å“ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œå·ç§¯STGNNsåœ¨å¤„ç†æ—¶ç©ºæ•°æ®æ—¶ï¼Œä¿¡æ¯ä¼ æ’­æ•ˆç‡è¾ƒä½ï¼Œå°¤å…¶æ˜¯åœ¨è¿œè·ç¦»èŠ‚ç‚¹ä¹‹é—´çš„ä¼ æ’­ã€‚ä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œæå‡ºçš„æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒéªŒè¯äº†ç†è®ºåˆ†æçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶ä¸ºæ—¶ç©ºå›¾ç¥ç»ç½‘ç»œçš„è®¾è®¡æä¾›äº†æ–°çš„ç†è®ºåŸºç¡€ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨äº¤é€šé¢„æµ‹ã€ç¤¾äº¤ç½‘ç»œåˆ†æå’Œè§†é¢‘ç†è§£ç­‰é¢†åŸŸã€‚é€šè¿‡ä¼˜åŒ–ä¿¡æ¯ä¼ æ’­æœºåˆ¶ï¼Œå¯ä»¥æå‡æ¨¡å‹åœ¨å¤æ‚åŠ¨æ€ç¯å¢ƒä¸­çš„è¡¨ç°ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Graph Neural Networks (GNNs) have achieved remarkable success across various domains. However, recent theoretical advances have identified fundamental limitations in their information propagation capabilities, such as over-squashing, where distant nodes fail to effectively exchange information. While extensively studied in static contexts, this issue remains unexplored in Spatiotemporal GNNs (STGNNs), which process sequences associated with graph nodes. Nonetheless, the temporal dimension amplifies this challenge by increasing the information that must be propagated. In this work, we formalize the spatiotemporal over-squashing problem and demonstrate its distinct characteristics compared to the static case. Our analysis reveals that, counterintuitively, convolutional STGNNs favor information propagation from points temporally distant rather than close in time. Moreover, we prove that architectures that follow either time-and-space or time-then-space processing paradigms are equally affected by this phenomenon, providing theoretical justification for computationally efficient implementations. We validate our findings on synthetic and real-world datasets, providing deeper insights into their operational dynamics and principled guidance for more effective designs.

