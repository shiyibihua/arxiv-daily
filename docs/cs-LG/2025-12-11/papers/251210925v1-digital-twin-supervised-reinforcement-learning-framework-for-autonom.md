---
layout: default
title: Digital Twin Supervised Reinforcement Learning Framework for Autonomous Underwater Navigation
---

# Digital Twin Supervised Reinforcement Learning Framework for Autonomous Underwater Navigation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.10925" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.10925v1</a>
  <a href="https://arxiv.org/pdf/2512.10925.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.10925v1" onclick="toggleFavorite(this, '2512.10925v1', 'Digital Twin Supervised Reinforcement Learning Framework for Autonomous Underwater Navigation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zamirddine Mari, Mohamad Motasem Nawaf, Pierre Drap

**åˆ†ç±»**: cs.LG, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-12-11

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºæ•°å­—å­ªç”Ÿç›‘ç£å¼ºåŒ–å­¦ä¹ çš„æ°´ä¸‹è‡ªä¸»å¯¼èˆªæ¡†æ¶ï¼Œæå‡å¤æ‚ç¯å¢ƒé€‚åº”æ€§ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `æ°´ä¸‹è‡ªä¸»å¯¼èˆª` `æ·±åº¦å¼ºåŒ–å­¦ä¹ ` `æ•°å­—å­ªç”Ÿ` `è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–` `æ°´ä¸‹æœºå™¨äºº`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ°´ä¸‹è‡ªä¸»å¯¼èˆªé¢ä¸´GPSç¼ºå¤±ã€ä½èƒ½è§åº¦ç­‰æŒ‘æˆ˜ï¼Œä¼ ç»Ÿæ–¹æ³•éš¾ä»¥æœ‰æ•ˆåº”å¯¹å¤æ‚ç¯å¢ƒã€‚
2. åˆ©ç”¨PPOç®—æ³•ï¼Œç»“åˆè™šæ‹Ÿç¯å¢ƒä¿¡æ¯å’Œå°„çº¿æŠ•å°„ï¼Œæ„å»ºå¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œæå‡å¯¼èˆªæ€§èƒ½ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä»¿çœŸå’ŒçœŸå®æ°´ä¸‹ç¯å¢ƒä¸­å‡ä¼˜äºDWAï¼Œå¹¶å…·å¤‡è‰¯å¥½çš„è¿ç§»èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡é’ˆå¯¹æ°´ä¸‹ç¯å¢ƒè‡ªä¸»å¯¼èˆªéš¾é¢˜ï¼Œå¦‚GPSç¼ºå¤±ã€ä½èƒ½è§åº¦å’Œæ°´ä¸‹éšœç¢ç‰©ï¼Œæå‡ºäº†ä¸€ç§åŸºäºè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰ç®—æ³•çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ç›®æ ‡å¯¼å‘å¯¼èˆªä¿¡æ¯ã€è™šæ‹Ÿå æ®æ …æ ¼å’Œæ²¿æ“ä½œåŒºåŸŸè¾¹ç•Œçš„å°„çº¿æŠ•å°„æ„å»ºè§‚æµ‹ç©ºé—´ã€‚é€šè¿‡åœ¨é€¼çœŸçš„ä»¿çœŸç¯å¢ƒä¸­è¯„ä¼°ï¼Œå¹¶å°†å­¦ä¹ åˆ°çš„ç­–ç•¥ä¸å¸¸ç”¨çš„åŠ¨æ€çª—å£æ³•ï¼ˆDWAï¼‰è¿›è¡Œæ¯”è¾ƒï¼Œç»“æœè¡¨æ˜ï¼ŒPPOç­–ç•¥åœ¨é«˜åº¦æ‚ä¹±çš„ç¯å¢ƒä¸­å§‹ç»ˆä¼˜äºDWAï¼Œè¿™ä¸»è¦å½’åŠŸäºå…¶æ›´å¥½çš„å±€éƒ¨é€‚åº”æ€§å’Œæ›´å°‘çš„ç¢°æ’ã€‚æ­¤å¤–ï¼Œé€šè¿‡æ•°å­—å­ªç”Ÿç›‘ç£ä¸‹çš„çœŸå®BlueROV2å®éªŒï¼ŒéªŒè¯äº†å­¦ä¹ åˆ°çš„è¡Œä¸ºä»ä»¿çœŸåˆ°ç°å®ä¸–ç•Œçš„è¿ç§»èƒ½åŠ›ï¼Œè¯å®äº†æ·±åº¦å¼ºåŒ–å­¦ä¹ åœ¨æ°´ä¸‹æœºå™¨äººè‡ªä¸»å¯¼èˆªä¸­çš„ç›¸å…³æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæ°´ä¸‹è‡ªä¸»å¯¼èˆªçš„ä¸»è¦é—®é¢˜åœ¨äºç¼ºä¹å¯é çš„å®šä½ä¿¡æ¯ï¼ˆGPSä¸å¯ç”¨ï¼‰ï¼Œæ°´ä¸‹ç¯å¢ƒçš„ä½èƒ½è§åº¦ï¼Œä»¥åŠå¤æ‚ç¯å¢ƒä¸­å­˜åœ¨çš„æ°´ä¸‹éšœç¢ç‰©ã€‚ç°æœ‰çš„æ–¹æ³•ï¼Œä¾‹å¦‚DWAï¼Œåœ¨é«˜åº¦åŠ¨æ€å’Œå¤æ‚çš„ç¯å¢ƒä¸­å®¹æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜ï¼Œå¯¼è‡´å¯¼èˆªæ•ˆç‡é™ä½æˆ–ç¢°æ’é£é™©å¢åŠ ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰æ¥å­¦ä¹ ä¸€ä¸ªèƒ½å¤Ÿé€‚åº”å¤æ‚æ°´ä¸‹ç¯å¢ƒçš„å¯¼èˆªç­–ç•¥ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼Œæ™ºèƒ½ä½“å¯ä»¥ä»ä¸ç¯å¢ƒçš„äº¤äº’ä¸­å­¦ä¹ ï¼Œä»è€Œæ‰¾åˆ°æœ€ä¼˜çš„å¯¼èˆªè·¯å¾„ï¼Œé¿å…éšœç¢ç‰©ï¼Œå¹¶æœ€ç»ˆè¾¾åˆ°ç›®æ ‡ã€‚æ•°å­—å­ªç”Ÿçš„å¼•å…¥ï¼Œé™ä½äº†çœŸå®ç¯å¢ƒå®éªŒçš„é£é™©ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) ä»¿çœŸç¯å¢ƒï¼šç”¨äºè®­ç»ƒå¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ï¼Œæä¾›é€¼çœŸçš„æ°´ä¸‹ç¯å¢ƒæ¨¡æ‹Ÿã€‚2) å¼ºåŒ–å­¦ä¹ æ¨¡å—ï¼šä½¿ç”¨PPOç®—æ³•è®­ç»ƒå¯¼èˆªç­–ç•¥ã€‚3) è§‚æµ‹ç©ºé—´æ„å»ºï¼šç»“åˆç›®æ ‡å¯¼å‘å¯¼èˆªä¿¡æ¯ã€è™šæ‹Ÿå æ®æ …æ ¼å’Œå°„çº¿æŠ•å°„ï¼Œä¸ºæ™ºèƒ½ä½“æä¾›ä¸°å¯Œçš„ç¯å¢ƒä¿¡æ¯ã€‚4) æ•°å­—å­ªç”Ÿç›‘ç£ï¼šåˆ©ç”¨æ•°å­—å­ªç”ŸæŠ€æœ¯ï¼Œåœ¨è™šæ‹Ÿç¯å¢ƒä¸­éªŒè¯å’Œä¼˜åŒ–ç­–ç•¥ï¼Œå‡å°‘çœŸå®ç¯å¢ƒå®éªŒçš„é£é™©ã€‚5) çœŸå®æ°´ä¸‹æœºå™¨äººå®éªŒï¼šå°†è®­ç»ƒå¥½çš„ç­–ç•¥éƒ¨ç½²åˆ°çœŸå®çš„BlueROV2æ°´ä¸‹æœºå™¨äººä¸Šè¿›è¡ŒéªŒè¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸æ•°å­—å­ªç”ŸæŠ€æœ¯ç›¸ç»“åˆï¼Œç”¨äºè§£å†³æ°´ä¸‹è‡ªä¸»å¯¼èˆªé—®é¢˜ã€‚ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•é€šå¸¸éœ€è¦åœ¨çœŸå®ç¯å¢ƒä¸­è¿›è¡Œå¤§é‡çš„å®éªŒï¼Œè¿™å¯¹äºæ°´ä¸‹æœºå™¨äººæ¥è¯´æ˜¯å±é™©ä¸”æ˜‚è´µçš„ã€‚é€šè¿‡æ•°å­—å­ªç”ŸæŠ€æœ¯ï¼Œå¯ä»¥åœ¨è™šæ‹Ÿç¯å¢ƒä¸­è¿›è¡Œç­–ç•¥çš„è®­ç»ƒå’ŒéªŒè¯ï¼Œä»è€Œé™ä½äº†çœŸå®ç¯å¢ƒå®éªŒçš„é£é™©å’Œæˆæœ¬ã€‚æ­¤å¤–ï¼Œç»“åˆå¤šç§ç¯å¢ƒä¿¡æ¯æ„å»ºè§‚æµ‹ç©ºé—´ï¼Œæå‡äº†æ™ºèƒ½ä½“å¯¹ç¯å¢ƒçš„æ„ŸçŸ¥èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­ä½¿ç”¨äº†PPOç®—æ³•ä½œä¸ºå¼ºåŒ–å­¦ä¹ çš„æ ¸å¿ƒç®—æ³•ã€‚è§‚æµ‹ç©ºé—´ç”±ä¸‰éƒ¨åˆ†ç»„æˆï¼šç›®æ ‡å¯¼å‘å¯¼èˆªä¿¡æ¯ï¼ˆç›®æ ‡æ–¹å‘å’Œè·ç¦»ï¼‰ã€è™šæ‹Ÿå æ®æ …æ ¼ï¼ˆå‘¨å›´ç¯å¢ƒçš„å±€éƒ¨åœ°å›¾ï¼‰å’Œå°„çº¿æŠ•å°„ï¼ˆæ²¿æ“ä½œåŒºåŸŸè¾¹ç•Œçš„è·ç¦»ä¿¡æ¯ï¼‰ã€‚å¥–åŠ±å‡½æ•°çš„è®¾è®¡æ—¨åœ¨é¼“åŠ±æ™ºèƒ½ä½“æœç€ç›®æ ‡å‰è¿›ï¼ŒåŒæ—¶é¿å…ç¢°æ’ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®åœ¨è®ºæ–‡ä¸­æ²¡æœ‰è¯¦ç»†è¯´æ˜ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºPPOçš„å¼ºåŒ–å­¦ä¹ ç­–ç•¥åœ¨é«˜åº¦æ‚ä¹±çš„æ°´ä¸‹ç¯å¢ƒä¸­ï¼Œå¯¼èˆªæ€§èƒ½æ˜æ˜¾ä¼˜äºä¼ ç»Ÿçš„DWAç®—æ³•ã€‚PPOç­–ç•¥èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”å±€éƒ¨ç¯å¢ƒå˜åŒ–ï¼Œå‡å°‘ç¢°æ’æ¬¡æ•°ï¼Œå¹¶æˆåŠŸåœ°å°†å­¦ä¹ åˆ°çš„ç­–ç•¥ä»ä»¿çœŸç¯å¢ƒè¿ç§»åˆ°çœŸå®çš„BlueROV2æ°´ä¸‹æœºå™¨äººä¸Šã€‚å…·ä½“æ€§èƒ½æ•°æ®æœªçŸ¥ï¼Œä½†æ•´ä½“è¡¨ç°ä¼˜äºDWAã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ°´ä¸‹ç¯å¢ƒç›‘æµ‹ã€æ°´ä¸‹èµ„æºå‹˜æ¢ã€æ°´ä¸‹åŸºç¡€è®¾æ–½ç»´æŠ¤ã€æ°´ä¸‹æœæ•‘ç­‰é¢†åŸŸã€‚é€šè¿‡è‡ªä¸»å¯¼èˆªï¼Œæ°´ä¸‹æœºå™¨äººå¯ä»¥æ›´é«˜æ•ˆã€æ›´å®‰å…¨åœ°å®Œæˆå„ç§æ°´ä¸‹ä»»åŠ¡ï¼Œé™ä½äººå·¥æ“ä½œçš„é£é™©å’Œæˆæœ¬ï¼Œæé«˜ä½œä¸šæ•ˆç‡ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›è¿›ä¸€æ­¥æ¨å¹¿åˆ°å…¶ä»–ç±»å‹çš„æœºå™¨äººå’Œå¤æ‚ç¯å¢ƒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Autonomous navigation in underwater environments remains a major challenge due to the absence of GPS, degraded visibility, and the presence of submerged obstacles. This article investigates these issues through the case of the BlueROV2, an open platform widely used for scientific experimentation. We propose a deep reinforcement learning approach based on the Proximal Policy Optimization (PPO) algorithm, using an observation space that combines target-oriented navigation information, a virtual occupancy grid, and ray-casting along the boundaries of the operational area. The learned policy is compared against a reference deterministic kinematic planner, the Dynamic Window Approach (DWA), commonly employed as a robust baseline for obstacle avoidance. The evaluation is conducted in a realistic simulation environment and complemented by validation on a physical BlueROV2 supervised by a 3D digital twin of the test site, helping to reduce risks associated with real-world experimentation. The results show that the PPO policy consistently outperforms DWA in highly cluttered environments, notably thanks to better local adaptation and reduced collisions. Finally, the experiments demonstrate the transferability of the learned behavior from simulation to the real world, confirming the relevance of deep RL for autonomous navigation in underwater robotics.

