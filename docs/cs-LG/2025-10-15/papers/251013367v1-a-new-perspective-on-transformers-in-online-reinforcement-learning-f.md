---
layout: default
title: A New Perspective on Transformers in Online Reinforcement Learning for Continuous Control
---

# A New Perspective on Transformers in Online Reinforcement Learning for Continuous Control

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.13367" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.13367v1</a>
  <a href="https://arxiv.org/pdf/2510.13367.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.13367v1" onclick="toggleFavorite(this, '2510.13367v1', 'A New Perspective on Transformers in Online Reinforcement Learning for Continuous Control')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Nikita Kachaev, Daniil Zelezetsky, Egor Cherepanov, Alexey K. Kovelev, Aleksandr I. Panov

**åˆ†ç±»**: cs.LG, cs.AI, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-10-15

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ¢ç´¢Transformeråœ¨åœ¨çº¿å¼ºåŒ–å­¦ä¹ ä¸­çš„åº”ç”¨ï¼Œå®ç°è¿ç»­æ§åˆ¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `Transformer` `åœ¨çº¿å¼ºåŒ–å­¦ä¹ ` `è¿ç»­æ§åˆ¶` `Actor-Critic` `åºåˆ—å»ºæ¨¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. Transformeråœ¨ç¦»çº¿RLä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨åœ¨çº¿æ— æ¨¡å‹RLä¸­åº”ç”¨å—é™ï¼Œä¸»è¦æŒ‘æˆ˜åœ¨äºå…¶å¯¹è®­ç»ƒè®¾ç½®å’Œæ¨¡å‹è®¾è®¡çš„æ•æ„Ÿæ€§ã€‚
2. æœ¬æ–‡æ¢ç´¢äº†Transformeråœ¨åœ¨çº¿æ— æ¨¡å‹RLä¸­è¿ç»­æ§åˆ¶çš„åº”ç”¨ï¼Œé‡ç‚¹ç ”ç©¶äº†è¾“å…¥è°ƒèŠ‚ã€ç»„ä»¶å…±äº«å’Œåºåˆ—æ•°æ®åˆ‡åˆ†ç­‰å…³é”®è®¾è®¡é—®é¢˜ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œé€šè¿‡ç¨³å®šçš„æ¶æ„å’Œè®­ç»ƒç­–ç•¥ï¼ŒTransformerå¯ä»¥åœ¨å¤šç§åœ¨çº¿RLä»»åŠ¡ä¸­å®ç°æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬å®Œå…¨å’Œéƒ¨åˆ†å¯è§‚å¯Ÿç¯å¢ƒã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡Transformeråœ¨ç¦»çº¿æˆ–åŸºäºæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ (RL)ä¸­è¡¨ç°å‡ºè‰²ä¸”åº”ç”¨å¹¿æ³›ï¼Œä½†åœ¨åœ¨çº¿æ— æ¨¡å‹RLä¸­ï¼Œç”±äºå…¶å¯¹è®­ç»ƒè®¾ç½®å’Œæ¨¡å‹è®¾è®¡å†³ç­–çš„æ•æ„Ÿæ€§ï¼Œä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚è¿™äº›è®¾è®¡å†³ç­–åŒ…æ‹¬å¦‚ä½•æ„å»ºç­–ç•¥å’Œä»·å€¼ç½‘ç»œã€å…±äº«ç»„ä»¶æˆ–å¤„ç†æ—¶é—´ä¿¡æ¯ã€‚æœ¬æ–‡è¡¨æ˜ï¼ŒTransformerå¯ä»¥ä½œä¸ºåœ¨çº¿æ— æ¨¡å‹RLä¸­è¿ç»­æ§åˆ¶çš„å¼ºå¤§åŸºçº¿ã€‚æˆ‘ä»¬ç ”ç©¶äº†å…³é”®çš„è®¾è®¡é—®é¢˜ï¼šå¦‚ä½•è°ƒèŠ‚è¾“å…¥ã€åœ¨Actorå’ŒCriticä¹‹é—´å…±äº«ç»„ä»¶ä»¥åŠå¦‚ä½•åˆ‡åˆ†åºåˆ—æ•°æ®è¿›è¡Œè®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç¨³å®šçš„æ¶æ„å’Œè®­ç»ƒç­–ç•¥èƒ½å¤Ÿåœ¨å®Œå…¨å’Œéƒ¨åˆ†å¯è§‚å¯Ÿçš„ä»»åŠ¡ä¸­ï¼Œä»¥åŠåœ¨åŸºäºå‘é‡å’Œå›¾åƒçš„ç¯å¢ƒä¸­å®ç°æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚è¿™äº›å‘ç°ä¸ºåœ¨åœ¨çº¿RLä¸­åº”ç”¨Transformeræä¾›äº†å®è·µæŒ‡å¯¼ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³Transformeråœ¨åœ¨çº¿æ— æ¨¡å‹å¼ºåŒ–å­¦ä¹ ä¸­åº”ç”¨äºè¿ç»­æ§åˆ¶ä»»åŠ¡æ—¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éš¾ä»¥è®­ç»ƒTransformerï¼Œä¸”å¯¹è¶…å‚æ•°æ•æ„Ÿï¼Œå¯¼è‡´æ€§èƒ½ä¸ç¨³å®šã€‚ç°æœ‰ç ”ç©¶è¾ƒå°‘å…³æ³¨å¦‚ä½•æœ‰æ•ˆåˆ©ç”¨Transformerå¤„ç†åœ¨çº¿RLä¸­çš„åºåˆ—æ•°æ®ï¼Œä»¥åŠå¦‚ä½•è®¾è®¡åˆé€‚çš„Actor-Criticç»“æ„ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ç³»ç»Ÿåœ°ç ”ç©¶Transformeråœ¨åœ¨çº¿RLä¸­çš„å…³é”®è®¾è®¡é€‰æ‹©ï¼Œæ‰¾åˆ°ç¨³å®šçš„æ¶æ„å’Œè®­ç»ƒç­–ç•¥ã€‚å…·ä½“è€Œè¨€ï¼Œæ¢ç´¢äº†ä¸åŒçš„è¾“å…¥è°ƒèŠ‚æ–¹æ³•ã€Actor-Criticç»„ä»¶å…±äº«ç­–ç•¥ä»¥åŠåºåˆ—æ•°æ®åˆ‡åˆ†æ–¹å¼ï¼Œæ—¨åœ¨å…‹æœTransformeråœ¨åœ¨çº¿RLä¸­çš„è®­ç»ƒå›°éš¾ï¼Œå¹¶æå‡å…¶æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶é‡‡ç”¨Actor-Criticç»“æ„ï¼Œå…¶ä¸­Actorå’ŒCriticå‡åŸºäºTransformeræ„å»ºã€‚è¾“å…¥çŠ¶æ€åºåˆ—ç»è¿‡åµŒå…¥å±‚å¤„ç†åï¼Œè¾“å…¥åˆ°Transformerç¼–ç å™¨ä¸­ã€‚Actorç½‘ç»œè¾“å‡ºåŠ¨ä½œçš„å‡å€¼å’Œæ–¹å·®ï¼ŒCriticç½‘ç»œè¾“å‡ºçŠ¶æ€-åŠ¨ä½œå€¼å‡½æ•°ã€‚è®­ç»ƒè¿‡ç¨‹é‡‡ç”¨å¸¸è§çš„ç­–ç•¥æ¢¯åº¦ç®—æ³•ï¼Œå¦‚PPOæˆ–SACã€‚è®ºæ–‡é‡ç‚¹å…³æ³¨Transformerå†…éƒ¨çš„è®¾è®¡ï¼Œè€Œéå…·ä½“çš„RLç®—æ³•ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºç³»ç»Ÿåœ°ç ”ç©¶äº†Transformeråœ¨åœ¨çº¿RLä¸­çš„åº”ç”¨ï¼Œå¹¶æå‡ºäº†å®ç”¨çš„è®¾è®¡æŒ‡å¯¼ã€‚é€šè¿‡å®éªŒéªŒè¯äº†ä¸åŒè®¾è®¡é€‰æ‹©å¯¹æ€§èƒ½çš„å½±å“ï¼Œå¹¶æ‰¾åˆ°äº†ç›¸å¯¹ç¨³å®šçš„æ¶æ„å’Œè®­ç»ƒç­–ç•¥ã€‚è¿™ä¸ºåç»­ç ”ç©¶è€…åœ¨åœ¨çº¿RLä¸­ä½¿ç”¨Transformeræä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ç ”ç©¶äº†ä»¥ä¸‹å…³é”®è®¾è®¡ï¼š1) è¾“å…¥è°ƒèŠ‚ï¼šæ¢ç´¢äº†ä¸åŒçš„çŠ¶æ€è¡¨ç¤ºæ–¹æ³•ï¼Œå¦‚ç›´æ¥ä½¿ç”¨åŸå§‹çŠ¶æ€æˆ–ä½¿ç”¨çŠ¶æ€çš„å·®åˆ†ã€‚2) Actor-Criticç»„ä»¶å…±äº«ï¼šç ”ç©¶äº†Actorå’ŒCriticä¹‹é—´å…±äº«Transformerç¼–ç å™¨çš„ä¸åŒæ–¹å¼ï¼Œå¦‚å®Œå…¨å…±äº«ã€éƒ¨åˆ†å…±äº«æˆ–ä¸å…±äº«ã€‚3) åºåˆ—æ•°æ®åˆ‡åˆ†ï¼šæ¢ç´¢äº†ä¸åŒçš„åºåˆ—é•¿åº¦å’Œåˆ‡åˆ†æ–¹å¼ï¼Œä»¥å¹³è¡¡è®¡ç®—æ•ˆç‡å’Œæ—¶é—´ä¾èµ–æ€§å»ºæ¨¡èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜å…³æ³¨äº†å­¦ä¹ ç‡ã€æ‰¹é‡å¤§å°ç­‰è¶…å‚æ•°çš„è®¾ç½®ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé€šè¿‡åˆç†çš„è®¾è®¡é€‰æ‹©ï¼ŒTransformerå¯ä»¥åœ¨å¤šç§åœ¨çº¿RLä»»åŠ¡ä¸­å–å¾—ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸å½“ç”šè‡³æ›´ä¼˜çš„æ€§èƒ½ã€‚è®ºæ–‡åœ¨å®Œå…¨å’Œéƒ¨åˆ†å¯è§‚å¯Ÿçš„ç¯å¢ƒä¸­ï¼Œä»¥åŠåœ¨åŸºäºå‘é‡å’Œå›¾åƒçš„è¾“å…¥ä¸‹ï¼ŒéªŒè¯äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒTransformeræœ‰æ½œåŠ›æˆä¸ºåœ¨çº¿RLä¸­è¿ç»­æ§åˆ¶çš„å¼ºå¤§åŸºçº¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ã€æ¸¸æˆAIç­‰é¢†åŸŸã€‚é€šè¿‡åˆ©ç”¨Transformerå¼ºå¤§çš„åºåˆ—å»ºæ¨¡èƒ½åŠ›ï¼Œå¯ä»¥æå‡æ™ºèƒ½ä½“åœ¨å¤æ‚ç¯å¢ƒä¸­çš„å†³ç­–èƒ½åŠ›å’Œé€‚åº”æ€§ã€‚æœªæ¥çš„ç ”ç©¶å¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢Transformeråœ¨å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ã€éƒ¨åˆ†å¯è§‚å¯Ÿé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ç­‰æ›´å…·æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­çš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Despite their effectiveness and popularity in offline or model-based reinforcement learning (RL), transformers remain underexplored in online model-free RL due to their sensitivity to training setups and model design decisions such as how to structure the policy and value networks, share components, or handle temporal information. In this paper, we show that transformers can be strong baselines for continuous control in online model-free RL. We investigate key design questions: how to condition inputs, share components between actor and critic, and slice sequential data for training. Our experiments reveal stable architectural and training strategies enabling competitive performance across fully and partially observable tasks, and in both vector- and image-based settings. These findings offer practical guidance for applying transformers in online RL.

