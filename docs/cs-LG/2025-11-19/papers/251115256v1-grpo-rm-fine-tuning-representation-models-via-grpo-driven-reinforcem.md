---
layout: default
title: GRPO-RM: Fine-Tuning Representation Models via GRPO-Driven Reinforcement Learning
---

# GRPO-RM: Fine-Tuning Representation Models via GRPO-Driven Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.15256" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.15256v1</a>
  <a href="https://arxiv.org/pdf/2511.15256.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.15256v1" onclick="toggleFavorite(this, '2511.15256v1', 'GRPO-RM: Fine-Tuning Representation Models via GRPO-Driven Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yanchen Xu, Ziheng Jiao, Hongyuan Zhang, Xuelong Li

**åˆ†ç±»**: cs.LG, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-19

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGRPO-RMï¼Œé€šè¿‡GRPOé©±åŠ¨çš„å¼ºåŒ–å­¦ä¹ å¾®è°ƒè¡¨å¾æ¨¡å‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è¡¨å¾å­¦ä¹ ` `å¼ºåŒ–å­¦ä¹ ` `å¾®è°ƒ` `GRPO` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤§å‹è¯­è¨€æ¨¡å‹å¾®è°ƒä¸­ï¼ŒGRPOè¡¨ç°å‡ºæœ‰æ•ˆæ€§ï¼Œä½†å…¶åœ¨è¡¨å¾æ¨¡å‹ä¸Šçš„æ³›åŒ–èƒ½åŠ›æœ‰å¾…ç ”ç©¶ã€‚
2. GRPO-RMé€šè¿‡é¢„å®šä¹‰è¾“å‡ºé›†æ›¿ä»£tokené‡‡æ ·ï¼Œå¹¶è®¾è®¡ä¸“ç”¨å¥–åŠ±å‡½æ•°ï¼Œå®ç°è¡¨å¾æ¨¡å‹çš„GRPOä¼˜åŒ–ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒGRPO-RMåœ¨å¤šä¸ªçœŸå®æ•°æ®é›†ä¸Šæœ‰æ•ˆæå‡äº†è¡¨å¾æ¨¡å‹çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºGRPO-RMï¼ˆGroup Relative Policy Optimization for Representation Modelï¼‰çš„æ–¹æ³•ï¼Œç”¨äºå¾®è°ƒè¡¨å¾æ¨¡å‹ã€‚è¯¥æ–¹æ³•å—åˆ°GRPOåœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¾®è°ƒä¸­çš„æˆåŠŸå¯å‘ï¼Œå¹¶æ¢ç´¢äº†å°†GRPOç±»ç­–ç•¥åº”ç”¨äºè¡¨å¾æ¨¡å‹åè®­ç»ƒçš„å¯èƒ½æ€§ã€‚å…·ä½“è€Œè¨€ï¼ŒGRPO-RMå»ºç«‹äº†ä¸€ä¸ªé¢„å®šä¹‰çš„è¾“å‡ºé›†åˆï¼Œä»¥åŠŸèƒ½æ€§åœ°æ›¿ä»£LLMsä¸­çš„tokenåºåˆ—é‡‡æ ·ï¼Œä»è€Œç”Ÿæˆä¸€ä¸ªè¾“å‡ºç»„ï¼Œè¿™å¯¹äºGRPOçš„æ¦‚ç‡é©±åŠ¨ä¼˜åŒ–è‡³å…³é‡è¦ã€‚æ­¤å¤–ï¼Œè¿˜è®¾è®¡äº†ä¸€ä¸ªä¸“é—¨çš„å¥–åŠ±å‡½æ•°æ¥é€‚åº”è¡¨å¾æ¨¡å‹çš„ç‰¹æ€§ã€‚åœ¨å„ç§çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼ŒéªŒè¯äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¾®è°ƒæ–¹æ³•ï¼Œå¦‚GRPOï¼Œåœ¨è¡¨å¾æ¨¡å‹ä¸Šçš„ç›´æ¥åº”ç”¨é¢ä¸´æŒ‘æˆ˜ã€‚ä¸»è¦ç—›ç‚¹åœ¨äºï¼Œè¡¨å¾æ¨¡å‹çš„è¾“å‡ºé€šå¸¸ä¸æ˜¯ç¦»æ•£çš„tokenåºåˆ—ï¼Œè€Œæ˜¯è¿ç»­çš„å‘é‡ç©ºé—´ï¼Œè¿™ä½¿å¾—ç›´æ¥åº”ç”¨åŸºäºtokenåºåˆ—é‡‡æ ·çš„GRPOæ–¹æ³•å˜å¾—å›°éš¾ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§æ–°çš„æ–¹æ³•æ¥é€‚åº”è¡¨å¾æ¨¡å‹çš„ç‰¹æ€§ï¼Œå¹¶å®ç°GRPOçš„ä¼˜åŒ–ç›®æ ‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šGRPO-RMçš„æ ¸å¿ƒæ€è·¯æ˜¯ï¼Œé€šè¿‡å»ºç«‹ä¸€ä¸ªé¢„å®šä¹‰çš„è¾“å‡ºé›†åˆæ¥æ¨¡æ‹ŸLLMsä¸­çš„tokenåºåˆ—é‡‡æ ·è¿‡ç¨‹ã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹äºç»™å®šçš„è¾“å…¥ï¼Œæ¨¡å‹ä¸æ˜¯ç›´æ¥è¾“å‡ºä¸€ä¸ªå‘é‡ï¼Œè€Œæ˜¯ä»é¢„å®šä¹‰çš„è¾“å‡ºé›†åˆä¸­é€‰æ‹©ä¸€ä¸ªæœ€åˆé€‚çš„å‘é‡ã€‚è¿™æ ·ï¼Œå°±å¯ä»¥å°†è¡¨å¾æ¨¡å‹çš„ä¼˜åŒ–é—®é¢˜è½¬åŒ–ä¸ºä¸€ä¸ªç¦»æ•£çš„é€‰æ‹©é—®é¢˜ï¼Œä»è€Œå¯ä»¥ä½¿ç”¨GRPOè¿›è¡Œä¼˜åŒ–ã€‚æ­¤å¤–ï¼Œè¿˜éœ€è¦è®¾è®¡ä¸€ä¸ªä¸“é—¨çš„å¥–åŠ±å‡½æ•°ï¼Œä»¥åæ˜ è¡¨å¾æ¨¡å‹çš„æ€§èƒ½æŒ‡æ ‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGRPO-RMçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼š1) å»ºç«‹é¢„å®šä¹‰çš„è¾“å‡ºé›†åˆï¼›2) ä½¿ç”¨è¡¨å¾æ¨¡å‹å¯¹è¾“å…¥è¿›è¡Œç¼–ç ï¼›3) ä»è¾“å‡ºé›†åˆä¸­é€‰æ‹©ä¸€ä¸ªæœ€åˆé€‚çš„å‘é‡ï¼›4) è®¡ç®—å¥–åŠ±å‡½æ•°ï¼›5) ä½¿ç”¨GRPOæ›´æ–°è¡¨å¾æ¨¡å‹çš„å‚æ•°ã€‚å…¶ä¸­ï¼Œé¢„å®šä¹‰çš„è¾“å‡ºé›†åˆå¯ä»¥æ˜¯éšæœºç”Ÿæˆçš„å‘é‡ï¼Œä¹Ÿå¯ä»¥æ˜¯åŸºäºæŸç§å…ˆéªŒçŸ¥è¯†ç”Ÿæˆçš„å‘é‡ã€‚å¥–åŠ±å‡½æ•°çš„è®¾è®¡éœ€è¦æ ¹æ®å…·ä½“çš„åº”ç”¨åœºæ™¯è¿›è¡Œè°ƒæ•´ã€‚

**å…³é”®åˆ›æ–°**ï¼šGRPO-RMçš„å…³é”®åˆ›æ–°åœ¨äºï¼Œå®ƒå°†GRPOæ–¹æ³•ä»LLMsæ‰©å±•åˆ°äº†è¡¨å¾æ¨¡å‹ã€‚é€šè¿‡å»ºç«‹é¢„å®šä¹‰çš„è¾“å‡ºé›†åˆï¼ŒGRPO-RMæˆåŠŸåœ°å°†è¡¨å¾æ¨¡å‹çš„ä¼˜åŒ–é—®é¢˜è½¬åŒ–ä¸ºä¸€ä¸ªç¦»æ•£çš„é€‰æ‹©é—®é¢˜ï¼Œä»è€Œå¯ä»¥ä½¿ç”¨GRPOè¿›è¡Œä¼˜åŒ–ã€‚æ­¤å¤–ï¼ŒGRPO-RMè¿˜è®¾è®¡äº†ä¸€ä¸ªä¸“é—¨çš„å¥–åŠ±å‡½æ•°ï¼Œä»¥é€‚åº”è¡¨å¾æ¨¡å‹çš„ç‰¹æ€§ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒGRPO-RMèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨GRPOçš„ä¼˜åŒ–èƒ½åŠ›ï¼Œä»è€Œæé«˜è¡¨å¾æ¨¡å‹çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šé¢„å®šä¹‰è¾“å‡ºé›†åˆçš„å¤§å°æ˜¯ä¸€ä¸ªå…³é”®å‚æ•°ï¼Œå®ƒå†³å®šäº†æ¨¡å‹é€‰æ‹©çš„èŒƒå›´ã€‚å¥–åŠ±å‡½æ•°çš„è®¾è®¡éœ€è¦æ ¹æ®å…·ä½“çš„åº”ç”¨åœºæ™¯è¿›è¡Œè°ƒæ•´ï¼Œä¾‹å¦‚ï¼Œå¯ä»¥ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ä½œä¸ºå¥–åŠ±å‡½æ•°ï¼Œä»¥è¡¡é‡æ¨¡å‹è¾“å‡ºä¸ç›®æ ‡å‘é‡ä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚GRPOçš„å‚æ•°ï¼Œå¦‚å­¦ä¹ ç‡å’ŒæŠ˜æ‰£å› å­ï¼Œä¹Ÿéœ€è¦è¿›è¡Œè°ƒæ•´ï¼Œä»¥è·å¾—æœ€ä½³çš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒGRPO-RMåœ¨å¤šä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨å›¾åƒæ£€ç´¢ä»»åŠ¡ä¸­ï¼ŒGRPO-RMç›¸æ¯”äºåŸºçº¿æ–¹æ³•ï¼Œå¹³å‡ç²¾åº¦å‡å€¼ï¼ˆmAPï¼‰æé«˜äº†5%ä»¥ä¸Šã€‚è¿™äº›ç»“æœéªŒè¯äº†GRPO-RMçš„æœ‰æ•ˆæ€§ï¼Œå¹¶è¡¨æ˜GRPO-RMæ˜¯ä¸€ç§æœ‰å‰æ™¯çš„è¡¨å¾æ¨¡å‹å¾®è°ƒæ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

GRPO-RMå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚å›¾åƒæ£€ç´¢ã€æ–‡æœ¬åˆ†ç±»ã€æ¨èç³»ç»Ÿç­‰ã€‚å®ƒå¯ä»¥ç”¨äºå¾®è°ƒå„ç§ç±»å‹çš„è¡¨å¾æ¨¡å‹ï¼Œä¾‹å¦‚å›¾åƒåµŒå…¥æ¨¡å‹ã€æ–‡æœ¬åµŒå…¥æ¨¡å‹ç­‰ã€‚é€šè¿‡æé«˜è¡¨å¾æ¨¡å‹çš„æ€§èƒ½ï¼ŒGRPO-RMå¯ä»¥æ˜¾è‘—æå‡è¿™äº›åº”ç”¨çš„æ•ˆæœã€‚æœªæ¥ï¼ŒGRPO-RMè¿˜å¯ä»¥åº”ç”¨äºæ›´å¤æ‚çš„åœºæ™¯ï¼Œä¾‹å¦‚å¤šæ¨¡æ€å­¦ä¹ ã€è·¨è¯­è¨€å­¦ä¹ ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The Group Relative Policy Optimization (GRPO), a reinforcement learning method used to fine-tune large language models (LLMs), has proved its effectiveness in practical applications such as DeepSeek-R1. It raises a question whether GRPO can be generalized to representation learning models. In this paper, we propose Group Relative Policy Optimization for Representation Model (GRPO-RM), and investigate the performance of GRPO-like policy in post-training representation models. Specifically, our method establishes a predefined output set to functionally replace token sequence sampling in LLMs, thereby generating an output group, which is essential for the probability-driven optimization of GRPO. In addition, a specialized reward function is designed to accommodate the properties of representation models. Extensive experiments are conducted on various real-world datasets to validate the effectiveness of our proposed method.

