---
layout: default
title: Robust Bayesian Dynamic Programming for On-policy Risk-sensitive Reinforcement Learning
---

# Robust Bayesian Dynamic Programming for On-policy Risk-sensitive Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.24580" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.24580v1</a>
  <a href="https://arxiv.org/pdf/2512.24580.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.24580v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.24580v1', 'Robust Bayesian Dynamic Programming for On-policy Risk-sensitive Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shanyu Han, Yangbo He, Yang Liu

**åˆ†ç±»**: q-fin.RM, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-12-31

**å¤‡æ³¨**: 63 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé²æ£’è´å¶æ–¯åŠ¨æ€è§„åˆ’ï¼Œç”¨äºè§£å†³ç­–ç•¥é£é™©æ•æ„Ÿå¼ºåŒ–å­¦ä¹ ä¸­çš„è½¬ç§»ä¸ç¡®å®šæ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `é£é™©æ•æ„Ÿå¼ºåŒ–å­¦ä¹ ` `é²æ£’ä¼˜åŒ–` `è´å¶æ–¯åŠ¨æ€è§„åˆ’` `è½¬ç§»ä¸ç¡®å®šæ€§` `ç›¸å¹²é£é™©åº¦é‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤„ç†ç¯å¢ƒè½¬ç§»æ¦‚ç‡ä¸ç¡®å®šæ€§æ—¶å­˜åœ¨ä¸è¶³ï¼Œå¯èƒ½å¯¼è‡´ç­–ç•¥çš„é£é™©æ•æ„Ÿæ€§é™ä½ã€‚
2. è®ºæ–‡æå‡ºä¸€ç§é²æ£’è´å¶æ–¯åŠ¨æ€è§„åˆ’æ¡†æ¶ï¼Œé€šè¿‡å†…å¤–ä¸¤å±‚é£é™©åº¦é‡åˆ†åˆ«å¤„ç†çŠ¶æ€æˆæœ¬éšæœºæ€§å’Œè½¬ç§»åŠ¨æ€ä¸ç¡®å®šæ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é£é™©æ•æ„Ÿæ€§å’Œé²æ£’æ€§æ–¹é¢å‡æœ‰æå‡ï¼Œå¹¶åœ¨æœŸæƒå¯¹å†²ç­‰å®é™…åº”ç”¨ä¸­è¡¨ç°å‡ºä¼˜åŠ¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„é£é™©æ•æ„Ÿå¼ºåŒ–å­¦ä¹ ï¼ˆRSRLï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èåˆäº†é’ˆå¯¹è½¬ç§»ä¸ç¡®å®šæ€§çš„é²æ£’æ€§ã€‚æˆ‘ä»¬å®šä¹‰äº†ä¸¤ç§æˆªç„¶ä¸åŒä½†åˆç›¸äº’å…³è”çš„é£é™©åº¦é‡ï¼šä¸€ç§å†…éƒ¨é£é™©åº¦é‡ï¼Œç”¨äºå¤„ç†çŠ¶æ€å’Œæˆæœ¬çš„éšæœºæ€§ï¼›å¦ä¸€ç§å¤–éƒ¨é£é™©åº¦é‡ï¼Œç”¨äºæ•è·è½¬ç§»åŠ¨æ€çš„ä¸ç¡®å®šæ€§ã€‚æˆ‘ä»¬çš„æ¡†æ¶é€šè¿‡å…è®¸å†…éƒ¨å’Œå¤–éƒ¨é£é™©åº¦é‡é‡‡ç”¨ä¸€èˆ¬çš„ç›¸å¹²é£é™©åº¦é‡ï¼Œç»Ÿä¸€å¹¶æ¨å¹¿äº†å¤§å¤šæ•°ç°æœ‰çš„RLæ¡†æ¶ã€‚åœ¨æ­¤æ¡†æ¶å†…ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªé£é™©æ•æ„Ÿçš„é²æ£’é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆRSRMDPï¼‰ï¼Œæ¨å¯¼äº†å…¶è´å°”æ›¼æ–¹ç¨‹ï¼Œå¹¶åœ¨ç»™å®šçš„åéªŒåˆ†å¸ƒä¸‹æä¾›äº†è¯¯å·®åˆ†æã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å¼€å‘äº†ä¸€ç§è´å¶æ–¯åŠ¨æ€è§„åˆ’ï¼ˆBayesian DPï¼‰ç®—æ³•ï¼Œè¯¥ç®—æ³•åœ¨åéªŒæ›´æ–°å’Œå€¼è¿­ä»£ä¹‹é—´äº¤æ›¿è¿›è¡Œã€‚è¯¥æ–¹æ³•é‡‡ç”¨äº†ä¸€ç§åŸºäºé£é™©çš„è´å°”æ›¼ç®—å­çš„ä¼°è®¡å™¨ï¼Œè¯¥ä¼°è®¡å™¨å°†è’™ç‰¹å¡æ´›é‡‡æ ·ä¸å‡¸ä¼˜åŒ–ç›¸ç»“åˆï¼Œå¹¶ä¸ºæ­¤è¯æ˜äº†å¼ºä¸€è‡´æ€§ä¿è¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜äº†è¯¥ç®—æ³•æ”¶æ•›åˆ°è®­ç»ƒç¯å¢ƒä¸­æ¥è¿‘æœ€ä¼˜çš„ç­–ç•¥ï¼Œå¹¶åˆ†æäº†DirichletåéªŒå’ŒCVaRä¸‹çš„æ ·æœ¬å¤æ‚åº¦å’Œè®¡ç®—å¤æ‚åº¦ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡ä¸¤ä¸ªæ•°å€¼å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚ç»“æœæ˜¾ç¤ºå‡ºä¼˜å¼‚çš„æ”¶æ•›ç‰¹æ€§ï¼ŒåŒæ—¶ç›´è§‚åœ°å±•ç¤ºäº†å…¶åœ¨é£é™©æ•æ„Ÿæ€§å’Œé²æ£’æ€§æ–¹é¢çš„ä¼˜åŠ¿ã€‚é€šè¿‡æœŸæƒå¯¹å†²çš„åº”ç”¨ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥ä»ç»éªŒä¸Šè¯æ˜äº†æ‰€æå‡ºçš„ç®—æ³•çš„ä¼˜åŠ¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤„ç†ç¯å¢ƒè½¬ç§»æ¦‚ç‡ä¸ç¡®å®šæ€§æ—¶ï¼Œå¾€å¾€ç¼ºä¹è¶³å¤Ÿçš„é²æ£’æ€§ï¼Œå¯¼è‡´å­¦ä¹ åˆ°çš„ç­–ç•¥åœ¨å®é™…åº”ç”¨ä¸­è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨é£é™©æ•æ„Ÿçš„åœºæ™¯ä¸‹ã€‚ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•é€šå¸¸å‡è®¾ç¯å¢ƒæ˜¯å®Œå…¨å·²çŸ¥çš„ï¼Œæˆ–è€…é€šè¿‡ä¸€äº›ç®€å•çš„æ­£åˆ™åŒ–æ–¹æ³•æ¥å¤„ç†ä¸ç¡®å®šæ€§ï¼Œä½†è¿™äº›æ–¹æ³•æ— æ³•æœ‰æ•ˆåœ°åº”å¯¹å¤æ‚çš„è½¬ç§»ä¸ç¡®å®šæ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†é²æ£’ä¼˜åŒ–å’Œè´å¶æ–¯åŠ¨æ€è§„åˆ’ç›¸ç»“åˆï¼Œé€šè¿‡å†…å¤–ä¸¤å±‚é£é™©åº¦é‡æ¥æ˜¾å¼åœ°å»ºæ¨¡å’Œå¤„ç†è½¬ç§»ä¸ç¡®å®šæ€§ã€‚å†…å±‚é£é™©åº¦é‡ç”¨äºå¤„ç†çŠ¶æ€å’Œæˆæœ¬çš„éšæœºæ€§ï¼Œå¤–å±‚é£é™©åº¦é‡ç”¨äºæ•è·è½¬ç§»åŠ¨æ€çš„ä¸ç¡®å®šæ€§ã€‚è¿™ç§åŒå±‚é£é™©åº¦é‡çš„è®¾è®¡å…è®¸ç®—æ³•åœ¨é¢å¯¹ä¸ç¡®å®šæ€§æ—¶ï¼Œèƒ½å¤Ÿæ›´åŠ ä¿å®ˆå’Œç¨³å¥åœ°è¿›è¡Œå†³ç­–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•æ„å»ºäº†ä¸€ä¸ªé£é™©æ•æ„Ÿçš„é²æ£’é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆRSRMDPï¼‰ï¼Œå¹¶æ¨å¯¼äº†å…¶è´å°”æ›¼æ–¹ç¨‹ã€‚æ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼š1) å®šä¹‰å†…å¤–ä¸¤å±‚ç›¸å¹²é£é™©åº¦é‡ï¼›2) æ„å»ºRSRMDPå¹¶æ¨å¯¼è´å°”æ›¼æ–¹ç¨‹ï¼›3) å¼€å‘è´å¶æ–¯åŠ¨æ€è§„åˆ’ç®—æ³•ï¼Œäº¤æ›¿è¿›è¡ŒåéªŒæ›´æ–°å’Œå€¼è¿­ä»£ï¼›4) ä½¿ç”¨è’™ç‰¹å¡æ´›é‡‡æ ·å’Œå‡¸ä¼˜åŒ–ç›¸ç»“åˆçš„ä¼°è®¡å™¨æ¥ä¼°è®¡åŸºäºé£é™©çš„è´å°”æ›¼ç®—å­ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿå¤„ç†ä¸€èˆ¬çš„ç›¸å¹²é£é™©åº¦é‡ï¼Œå¹¶ä¸”èƒ½å¤Ÿæ˜¾å¼åœ°å»ºæ¨¡å’Œå¤„ç†è½¬ç§»ä¸ç¡®å®šæ€§ã€‚ä¸ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æ›´åŠ é²æ£’ï¼Œå¹¶ä¸”èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”é£é™©æ•æ„Ÿçš„åœºæ™¯ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜æä¾›äº†ä¸€ç§åŸºäºè´å¶æ–¯åŠ¨æ€è§„åˆ’çš„ç®—æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å­¦ä¹ åˆ°æ¥è¿‘æœ€ä¼˜çš„ç­–ç•¥ã€‚

**å…³é”®è®¾è®¡**ï¼šè¯¥æ–¹æ³•çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨CVaRï¼ˆæ¡ä»¶é£é™©ä»·å€¼ï¼‰ä½œä¸ºé£é™©åº¦é‡çš„ä¸€ä¸ªå…·ä½“å®ä¾‹ï¼›2) ä½¿ç”¨DirichletåéªŒåˆ†å¸ƒæ¥å»ºæ¨¡è½¬ç§»æ¦‚ç‡çš„ä¸ç¡®å®šæ€§ï¼›3) ä½¿ç”¨è’™ç‰¹å¡æ´›é‡‡æ ·æ¥ä¼°è®¡è´å°”æ›¼ç®—å­ï¼Œå¹¶é€šè¿‡å‡¸ä¼˜åŒ–æ¥æé«˜ä¼°è®¡çš„å‡†ç¡®æ€§ï¼›4) ç®—æ³•åœ¨åéªŒæ›´æ–°å’Œå€¼è¿­ä»£ä¹‹é—´äº¤æ›¿è¿›è¡Œï¼Œä»¥ä¸æ–­æé«˜ç­–ç•¥çš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é£é™©æ•æ„Ÿæ€§å’Œé²æ£’æ€§æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ã€‚åœ¨æ•°å€¼å®éªŒä¸­ï¼Œè¯¥æ–¹æ³•è¡¨ç°å‡ºä¼˜å¼‚çš„æ”¶æ•›ç‰¹æ€§ï¼Œå¹¶ä¸”èƒ½å¤Ÿæœ‰æ•ˆåœ°åº”å¯¹è½¬ç§»ä¸ç¡®å®šæ€§ã€‚åœ¨æœŸæƒå¯¹å†²çš„åº”ç”¨ä¸­ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—é™ä½æŠ•èµ„ç»„åˆçš„é£é™©ï¼Œå¹¶è·å¾—æ›´é«˜çš„æ”¶ç›Šã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºé‡‘èé¢†åŸŸçš„æœŸæƒå¯¹å†²ã€è‡ªåŠ¨é©¾é©¶ä¸­çš„å®‰å…¨å†³ç­–ã€åŒ»ç–—é¢†åŸŸçš„ä¸ªæ€§åŒ–æ²»ç–—æ–¹æ¡ˆåˆ¶å®šç­‰é£é™©æ•æ„Ÿçš„å†³ç­–åœºæ™¯ã€‚é€šè¿‡è€ƒè™‘ç¯å¢ƒçš„ä¸ç¡®å®šæ€§ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå¸®åŠ©å†³ç­–è€…åˆ¶å®šæ›´åŠ ç¨³å¥å’Œå¯é çš„ç­–ç•¥ï¼Œé™ä½æ½œåœ¨çš„æŸå¤±ï¼Œæé«˜å†³ç­–çš„å®‰å…¨æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We propose a novel framework for risk-sensitive reinforcement learning (RSRL) that incorporates robustness against transition uncertainty. We define two distinct yet coupled risk measures: an inner risk measure addressing state and cost randomness and an outer risk measure capturing transition dynamics uncertainty. Our framework unifies and generalizes most existing RL frameworks by permitting general coherent risk measures for both inner and outer risk measures. Within this framework, we construct a risk-sensitive robust Markov decision process (RSRMDP), derive its Bellman equation, and provide error analysis under a given posterior distribution. We further develop a Bayesian Dynamic Programming (Bayesian DP) algorithm that alternates between posterior updates and value iteration. The approach employs an estimator for the risk-based Bellman operator that combines Monte Carlo sampling with convex optimization, for which we prove strong consistency guarantees. Furthermore, we demonstrate that the algorithm converges to a near-optimal policy in the training environment and analyze both the sample complexity and the computational complexity under the Dirichlet posterior and CVaR. Finally, we validate our approach through two numerical experiments. The results exhibit excellent convergence properties while providing intuitive demonstrations of its advantages in both risk-sensitivity and robustness. Empirically, we further demonstrate the advantages of the proposed algorithm through an application on option hedging.

