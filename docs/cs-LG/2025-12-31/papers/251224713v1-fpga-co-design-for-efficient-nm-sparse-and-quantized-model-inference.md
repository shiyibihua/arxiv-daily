---
layout: default
title: "FPGA Co-Design for Efficient N:M Sparse and Quantized Model Inference"
---

# FPGA Co-Design for Efficient N:M Sparse and Quantized Model Inference

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.24713" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.24713v1</a>
  <a href="https://arxiv.org/pdf/2512.24713.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.24713v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.24713v1', 'FPGA Co-Design for Efficient N:M Sparse and Quantized Model Inference')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Fen-Yu Hsieh, Yun-Chang Teng, Ding-Yong Hong, Jan-Jan Wu

**åˆ†ç±»**: cs.LG, cs.AR

**å‘å¸ƒæ—¥æœŸ**: 2025-12-31

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºFPGAçš„è½¯ç¡¬ä»¶ååŒè®¾è®¡æ¡†æ¶ï¼ŒåŠ é€Ÿç¨€ç–é‡åŒ–å¤§è¯­è¨€æ¨¡å‹æ¨ç†ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `FPGAåŠ é€Ÿ` `ç¨€ç–æ€§` `é‡åŒ–` `å¤§è¯­è¨€æ¨¡å‹` `è½¯ç¡¬ä»¶ååŒè®¾è®¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤§å‹è¯­è¨€æ¨¡å‹éƒ¨ç½²å—é™äºé«˜æ˜‚çš„è®¡ç®—å’Œå†…å­˜éœ€æ±‚ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥åœ¨èµ„æºå—é™ç¯å¢ƒä¸­æœ‰æ•ˆéƒ¨ç½²ã€‚
2. æå‡ºä¸€ç§è½¯ç¡¬ä»¶ååŒè®¾è®¡æ¡†æ¶ï¼Œç»“åˆN:Mç¨€ç–å‰ªæå’Œä½æ¯”ç‰¹é‡åŒ–ï¼Œä¼˜åŒ–LLMåœ¨FPGAä¸Šçš„æ¨ç†ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æƒé‡å­˜å‚¨ã€çŸ©é˜µä¹˜æ³•å’Œç«¯åˆ°ç«¯å»¶è¿Ÿæ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ï¼Œå¹¶æé«˜äº†LLaMA-7Bæ¨¡å‹çš„ååé‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨å„ç§è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™ç§æˆåŠŸæ˜¯ä»¥å·¨å¤§çš„è®¡ç®—å’Œå†…å­˜éœ€æ±‚ä¸ºä»£ä»·çš„ï¼Œè¿™ä¸¥é‡é˜»ç¢äº†å®ƒä»¬åœ¨èµ„æºå—é™ç¯å¢ƒä¸­çš„éƒ¨ç½²ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæœ¬æ–‡ä»‹ç»äº†ä¸€ç§è‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨æƒé‡å‰ªæå’Œä½æ¯”ç‰¹é‡åŒ–ï¼Œå¹¶æå‡ºäº†ä¸€ç§ç¡¬ä»¶-è½¯ä»¶ååŒè®¾è®¡æ–¹æ³•ï¼Œç”¨äºåœ¨ç°åœºå¯ç¼–ç¨‹é—¨é˜µåˆ—(FPGA)å¹³å°ä¸Šç”ŸæˆåŠ é€Ÿå™¨ã€‚ç‰¹åˆ«åœ°ï¼Œæˆ‘ä»¬å®ç°äº†ä¸€ä¸ªç»Ÿä¸€çš„pipelineï¼Œè¯¥pipelineåº”ç”¨N:Mç»“æ„åŒ–å‰ªæå’Œ4æ¯”ç‰¹æ•´æ•°é‡åŒ–æ¥å‡å°‘å†…å­˜å ç”¨ï¼Œç„¶åè¿›è¡Œä¼˜åŒ–çš„åé‡åŒ–å’ŒçŸ©é˜µä¹˜æ³•ï¼Œä»¥å¢å¼ºLLMåœ¨åŒ…æ‹¬CPUã€å…·æœ‰å¯†é›†å’Œ2:4ç¨€ç–å¼ é‡æ ¸å¿ƒçš„NVIDIA GPUä»¥åŠå®šåˆ¶çš„åŸºäº systolic é˜µåˆ—çš„FPGAåŠ é€Ÿå™¨ç­‰å¤šç§ç¡¬ä»¶å¹³å°ä¸Šçš„æ¨ç†ã€‚é€šè¿‡åœ¨$4096 	imes 4096$çŸ©é˜µä¸Šåˆ©ç”¨2:4ç¨€ç–æ€§å’Œé‡åŒ–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†é«˜è¾¾4å€çš„æƒé‡å­˜å‚¨å‡å°‘å’Œ1.71å€çš„çŸ©é˜µä¹˜æ³•åŠ é€Ÿï¼Œä¸å¯†é›†GPUåŸºçº¿ç›¸æ¯”ï¼Œç«¯åˆ°ç«¯å»¶è¿Ÿé™ä½äº†1.29å€ã€‚åœ¨LLaMA-7Bæ¨¡å‹ä¸Šçš„ç¼©æ”¾åˆ†æè¿›ä¸€æ­¥è¡¨æ˜ï¼Œç»“æ„åŒ–ç¨€ç–æ€§å°†æ¯tokençš„ååé‡æé«˜äº†1.36å€ã€‚è¿™äº›ç»“æœè¯æ˜äº†ç»†ç²’åº¦N:Mç¨€ç–æ€§å’Œé‡åŒ–åœ¨å®ç°é«˜æ•ˆä¸”å¯éƒ¨ç½²çš„LLMæ¨ç†æ–¹é¢çš„ååŒä½œç”¨ï¼Œè€Œæ‰€æå‡ºçš„FPGAåŠ é€Ÿå™¨ä¸ºæ”¯æŒè¶…å‡ºå›ºå®š2:4ç¡¬ä»¶çº¦æŸçš„æ›´å¹¿æ³›çš„ç¨€ç–æ¨¡å¼ç±»åˆ«æä¾›äº†ä¸€ç§çµæ´»çš„æ¶æ„è·¯å¾„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è®¡ç®—å’Œå†…å­˜éœ€æ±‚å·¨å¤§ï¼Œéš¾ä»¥åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²ã€‚ç°æœ‰çš„ä¼˜åŒ–æ–¹æ³•ï¼Œå¦‚é‡åŒ–å’Œå‰ªæï¼Œè™½ç„¶å¯ä»¥é™ä½æ¨¡å‹å¤§å°å’Œè®¡ç®—å¤æ‚åº¦ï¼Œä½†å¾€å¾€å—é™äºç¡¬ä»¶å¹³å°çš„ç‰¹å®šçº¦æŸï¼Œä¾‹å¦‚GPUçš„2:4ç¨€ç–æ€§é™åˆ¶ï¼Œç¼ºä¹çµæ´»æ€§å’Œé€šç”¨æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é‡‡ç”¨è½¯ç¡¬ä»¶ååŒè®¾è®¡çš„æ–¹æ³•ï¼Œç»“åˆN:Mç»“æ„åŒ–å‰ªæå’Œä½æ¯”ç‰¹é‡åŒ–ï¼Œå¹¶é’ˆå¯¹FPGAå¹³å°å®šåˆ¶åŠ é€Ÿå™¨ã€‚é€šè¿‡è½¯ä»¶å±‚é¢çš„ç¨€ç–åŒ–å’Œé‡åŒ–é™ä½æ¨¡å‹å¤æ‚åº¦ï¼ŒåŒæ—¶åœ¨ç¡¬ä»¶å±‚é¢è®¾è®¡çµæ´»çš„åŠ é€Ÿå™¨æ¶æ„ï¼Œä»¥å……åˆ†åˆ©ç”¨ç¨€ç–æ€§å’Œé‡åŒ–çš„ä¼˜åŠ¿ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„LLMæ¨ç†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…å«ä¸€ä¸ªç»Ÿä¸€çš„pipelineï¼Œé¦–å…ˆå¯¹LLMè¿›è¡ŒN:Mç»“æ„åŒ–å‰ªæå’Œ4æ¯”ç‰¹æ•´æ•°é‡åŒ–ï¼Œä»¥å‡å°‘å†…å­˜å ç”¨ã€‚ç„¶åï¼Œé’ˆå¯¹ä¸åŒçš„ç¡¬ä»¶å¹³å°ï¼ˆCPUã€GPUã€FPGAï¼‰è¿›è¡Œä¼˜åŒ–çš„åé‡åŒ–å’ŒçŸ©é˜µä¹˜æ³•ã€‚å¯¹äºFPGAå¹³å°ï¼Œè®¾è®¡äº†ä¸€ä¸ªåŸºäºsystolicé˜µåˆ—çš„å®šåˆ¶åŠ é€Ÿå™¨ï¼Œä»¥é«˜æ•ˆåœ°æ‰§è¡Œç¨€ç–çŸ©é˜µä¹˜æ³•ã€‚æ•´ä¸ªæ¡†æ¶æ—¨åœ¨å®ç°è‡ªåŠ¨åŒ–ï¼Œèƒ½å¤Ÿæ ¹æ®ä¸åŒçš„æ¨¡å‹å’Œç¡¬ä»¶å¹³å°è‡ªåŠ¨ç”Ÿæˆä¼˜åŒ–çš„æ¨ç†æ–¹æ¡ˆã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºè½¯ç¡¬ä»¶ååŒè®¾è®¡ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹FPGAå¹³å°å®šåˆ¶çš„åŠ é€Ÿå™¨æ¶æ„ã€‚è¯¥åŠ é€Ÿå™¨èƒ½å¤Ÿçµæ´»åœ°æ”¯æŒä¸åŒçš„N:Mç¨€ç–æ¨¡å¼ï¼Œçªç ´äº†ä¼ ç»Ÿç¡¬ä»¶å¹³å°ï¼ˆå¦‚GPUï¼‰å¯¹ç¨€ç–æ¨¡å¼çš„é™åˆ¶ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜å®ç°äº†è‡ªåŠ¨åŒ–ï¼Œèƒ½å¤Ÿæ ¹æ®ä¸åŒçš„æ¨¡å‹å’Œç¡¬ä»¶å¹³å°è‡ªåŠ¨ç”Ÿæˆä¼˜åŒ–çš„æ¨ç†æ–¹æ¡ˆï¼Œé™ä½äº†éƒ¨ç½²çš„å¤æ‚æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šè¯¥è®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) N:Mç»“æ„åŒ–å‰ªæç­–ç•¥ï¼Œåœ¨ä¿è¯æ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œæœ€å¤§é™åº¦åœ°å‡å°‘éé›¶å…ƒç´ çš„æ•°é‡ã€‚2) 4æ¯”ç‰¹æ•´æ•°é‡åŒ–ï¼Œè¿›ä¸€æ­¥é™ä½æ¨¡å‹å¤§å°å’Œè®¡ç®—å¤æ‚åº¦ã€‚3) åŸºäºsystolicé˜µåˆ—çš„FPGAåŠ é€Ÿå™¨æ¶æ„ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°æ‰§è¡Œç¨€ç–çŸ©é˜µä¹˜æ³•ã€‚4) ä¼˜åŒ–çš„åé‡åŒ–å’ŒçŸ©é˜µä¹˜æ³•å®ç°ï¼Œå……åˆ†åˆ©ç”¨ç¡¬ä»¶å¹³å°çš„ç‰¹æ€§ï¼Œæé«˜æ¨ç†é€Ÿåº¦ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.24713v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.24713v1/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.24713v1/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨4096x4096çŸ©é˜µä¸Šï¼Œä½¿ç”¨2:4ç¨€ç–æ€§å’Œé‡åŒ–åï¼Œæƒé‡å­˜å‚¨å‡å°‘äº†4å€ï¼ŒçŸ©é˜µä¹˜æ³•åŠ é€Ÿäº†1.71å€ï¼Œç«¯åˆ°ç«¯å»¶è¿Ÿç›¸æ¯”äºå¯†é›†GPUåŸºçº¿é™ä½äº†1.29å€ã€‚åœ¨LLaMA-7Bæ¨¡å‹ä¸Šçš„ç¼©æ”¾åˆ†æè¡¨æ˜ï¼Œç»“æ„åŒ–ç¨€ç–æ€§å°†æ¯tokençš„ååé‡æé«˜äº†1.36å€ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æé«˜LLMçš„æ¨ç†æ•ˆç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§èµ„æºå—é™çš„åœºæ™¯ï¼Œä¾‹å¦‚è¾¹ç¼˜è®¡ç®—è®¾å¤‡ã€ç§»åŠ¨è®¾å¤‡å’ŒåµŒå…¥å¼ç³»ç»Ÿã€‚é€šè¿‡é™ä½LLMçš„è®¡ç®—å’Œå†…å­˜éœ€æ±‚ï¼Œå¯ä»¥ä½¿è¿™äº›è®¾å¤‡èƒ½å¤Ÿè¿è¡Œå¤æ‚çš„AIæ¨¡å‹ï¼Œä»è€Œå®ç°æ™ºèƒ½å®¶å±…ã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½åŒ»ç–—ç­‰åº”ç”¨ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å¯ä»¥ä¿ƒè¿›LLMåœ¨æ›´å¤šé¢†åŸŸçš„åº”ç”¨ï¼Œä¾‹å¦‚è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨ç¿»è¯‘å’Œè¯­éŸ³è¯†åˆ«ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have demonstrated remarkable performance across a wide range of language processing tasks. However, this success comes at the cost of substantial computation and memory requirements, which significantly impedes their deployment in resource-constrained environments. To address this challenge, this work introduces an automation framework that leverages weight pruning and low-bit quantization, and presents a hardware-software co-design method that generates accelerators on the Field-Programmable Gate Array (FPGA) platform. In particular, we implement a unified pipeline that applies N:M structured pruning and 4-bit integer quantization to reduce the memory footprint, followed by optimized dequantization and matrix multiplication to enhance LLM inference on several hardware platforms, including CPUs, NVIDIA GPUs with Dense and 2:4 Sparse Tensor Cores, and a custom systolic-array-based FPGA accelerator. Utilizing 2:4 sparsity combined with quantization on $4096 \times 4096$ matrices, our approach achieves a reduction of up to $4\times$ in weight storage and a $1.71\times$ speedup in matrix multiplication, yielding a $1.29\times$ end-to-end latency reduction compared to dense GPU baselines. Scaling analysis on the LLaMA-7B model further shows that structured sparsity enhances the throughput per token by $1.36\times$. These results demonstrate the synergy of fine-grained N:M sparsity and quantization for enabling efficient and deployable LLM inference, while the proposed FPGA accelerator offers a flexible architectural path for supporting a broader class of sparsity patterns beyond the fixed 2:4 hardware constraints.

