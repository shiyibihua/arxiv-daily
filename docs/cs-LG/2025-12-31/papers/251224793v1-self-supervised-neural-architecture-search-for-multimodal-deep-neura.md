---
layout: default
title: Self-Supervised Neural Architecture Search for Multimodal Deep Neural Networks
---

# Self-Supervised Neural Architecture Search for Multimodal Deep Neural Networks

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.24793" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.24793v1</a>
  <a href="https://arxiv.org/pdf/2512.24793.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.24793v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.24793v1', 'Self-Supervised Neural Architecture Search for Multimodal Deep Neural Networks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shota Suzuki, Satoshi Ono

**åˆ†ç±»**: cs.LG, cs.NE

**å‘å¸ƒæ—¥æœŸ**: 2025-12-31

**æœŸåˆŠ**: IEICE Transactions on Information and Systems, Vol.E108.D, No. 6, pp. 640-643, 2025

**DOI**: [10.1587/transinf.2024EDL8018](https://doi.org/10.1587/transinf.2024EDL8018)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§è‡ªç›‘ç£å¤šæ¨¡æ€ç¥ç»ç½‘ç»œæ¶æ„æœç´¢æ–¹æ³•ï¼Œè§£å†³æ ‡æ³¨æ•°æ®ä¾èµ–é—®é¢˜ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç¥ç»æ¶æ„æœç´¢` `å¤šæ¨¡æ€å­¦ä¹ ` `è‡ªç›‘ç£å­¦ä¹ ` `æ·±åº¦ç¥ç»ç½‘ç»œ` `æ— ç›‘ç£å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤šæ¨¡æ€DNNçš„æ¶æ„è®¾è®¡å¤æ‚ï¼Œåˆ©ç”¨NASå¯ä»¥æœ‰æ•ˆæå‡æ€§èƒ½ï¼Œä½†ä¼ ç»ŸNASæ–¹æ³•ä¾èµ–å¤§é‡æ ‡æ³¨æ•°æ®ã€‚
2. è®ºæ–‡æå‡ºä¸€ç§è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼ŒåŒæ—¶åº”ç”¨äºæ¶æ„æœç´¢å’Œæ¨¡å‹é¢„è®­ç»ƒï¼Œä»è€Œé™ä½å¯¹æ ‡æ³¨æ•°æ®çš„éœ€æ±‚ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä»…ä½¿ç”¨æ— æ ‡æ³¨æ•°æ®ï¼ŒæˆåŠŸæœç´¢å‡ºé€‚ç”¨äºå¤šæ¨¡æ€DNNçš„æœ‰æ•ˆæ¶æ„ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºå¤šæ¨¡æ€æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰æ¶æ„æœç´¢çš„è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ–¹æ³•ã€‚ç”±äºå¤šæ¨¡æ€DNNéœ€è¦èåˆæ¥è‡ªå¤šä¸ªæ¨¡æ€çš„ç‰¹å¾ï¼Œå…¶ç»“æ„å¤æ‚æ€§ä½¿å¾—ç¥ç»æ¶æ„æœç´¢ï¼ˆNASï¼‰å—ç›ŠåŒªæµ…ã€‚ç„¶è€Œï¼Œé€šè¿‡NASæ„å»ºå¤šæ¨¡æ€DNNæ¶æ„éœ€è¦å¤§é‡çš„æ ‡æ³¨è®­ç»ƒæ•°æ®ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•å°†SSLå…¨é¢åº”ç”¨äºæ¶æ„æœç´¢å’Œæ¨¡å‹é¢„è®­ç»ƒè¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æˆåŠŸåœ°ä»æ— æ ‡æ³¨è®­ç»ƒæ•°æ®ä¸­è®¾è®¡äº†DNNæ¶æ„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå¤šæ¨¡æ€æ·±åº¦ç¥ç»ç½‘ç»œçš„è®¾è®¡éœ€è¦è€ƒè™‘ä¸åŒæ¨¡æ€æ•°æ®çš„èåˆæ–¹å¼ï¼Œæ‰‹åŠ¨è®¾è®¡è¿‡ç¨‹ç¹çä¸”éš¾ä»¥è¾¾åˆ°æœ€ä¼˜ã€‚ç¥ç»æ¶æ„æœç´¢ï¼ˆNASï¼‰å¯ä»¥è‡ªåŠ¨åŒ–è¿™ä¸€è¿‡ç¨‹ï¼Œä½†ä¼ ç»ŸNASæ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡çš„æ ‡æ³¨æ•°æ®è¿›è¡Œè®­ç»ƒå’ŒéªŒè¯ï¼Œè¿™åœ¨å®é™…åº”ç”¨ä¸­æ˜¯ä¸€ä¸ªå¾ˆå¤§çš„é™åˆ¶ï¼Œå°¤å…¶æ˜¯åœ¨æ ‡æ³¨æ•°æ®è·å–æˆæœ¬é«˜æ˜‚çš„æƒ…å†µä¸‹ã€‚å› æ­¤ï¼Œå¦‚ä½•é™ä½NASå¯¹æ ‡æ³¨æ•°æ®çš„ä¾èµ–æ˜¯æœ¬æ–‡è¦è§£å†³çš„æ ¸å¿ƒé—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ¥æ›¿ä»£æˆ–å‡å°‘å¯¹æ ‡æ³¨æ•°æ®çš„éœ€æ±‚ã€‚é€šè¿‡è®¾è®¡åˆé€‚çš„è‡ªç›‘ç£ä»»åŠ¡ï¼Œä½¿ç½‘ç»œèƒ½å¤Ÿä»æ— æ ‡æ³¨æ•°æ®ä¸­å­¦ä¹ åˆ°æœ‰ç”¨çš„ç‰¹å¾è¡¨ç¤ºï¼Œä»è€ŒæŒ‡å¯¼æ¶æ„æœç´¢è¿‡ç¨‹ã€‚è¿™æ ·ï¼Œå³ä½¿åœ¨ç¼ºä¹æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹ï¼Œä¹Ÿèƒ½æœ‰æ•ˆåœ°æœç´¢å‡ºé€‚åˆå¤šæ¨¡æ€æ•°æ®çš„ç½‘ç»œæ¶æ„ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•åŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆï¼Œåˆ©ç”¨è‡ªç›‘ç£å­¦ä¹ å¯¹å€™é€‰ç½‘ç»œè¿›è¡Œé¢„è®­ç»ƒï¼Œä½¿å…¶å…·å¤‡ä¸€å®šçš„ç‰¹å¾æå–èƒ½åŠ›ã€‚ç„¶åï¼Œåœ¨æ¶æ„æœç´¢é˜¶æ®µï¼Œä½¿ç”¨é¢„è®­ç»ƒåçš„ç½‘ç»œä½œä¸ºåŸºç¡€ï¼Œé€šè¿‡æŸç§æœç´¢ç®—æ³•ï¼ˆå…·ä½“ç®—æ³•æœªçŸ¥ï¼‰æ¥æ¢ç´¢ä¸åŒçš„ç½‘ç»œç»“æ„ã€‚æœç´¢è¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨è‡ªç›‘ç£ä»»åŠ¡çš„æ€§èƒ½ä½œä¸ºè¯„ä»·æŒ‡æ ‡ï¼Œé€‰æ‹©è¡¨ç°æœ€å¥½çš„æ¶æ„ã€‚æ•´ä¸ªæ¡†æ¶å°†è‡ªç›‘ç£å­¦ä¹ è´¯ç©¿äºæ¶æ„æœç´¢å’Œæ¨¡å‹è®­ç»ƒçš„å§‹ç»ˆã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€å…³é”®çš„åˆ›æ–°åœ¨äºå°†è‡ªç›‘ç£å­¦ä¹ ä¸ç¥ç»æ¶æ„æœç´¢ç›¸ç»“åˆï¼Œä»è€Œè§£å†³äº†å¤šæ¨¡æ€DNNæ¶æ„æœç´¢å¯¹å¤§é‡æ ‡æ³¨æ•°æ®çš„ä¾èµ–é—®é¢˜ã€‚é€šè¿‡è‡ªç›‘ç£å­¦ä¹ ï¼Œç½‘ç»œå¯ä»¥ä»æ— æ ‡æ³¨æ•°æ®ä¸­å­¦ä¹ åˆ°æœ‰ç”¨çš„ç‰¹å¾è¡¨ç¤ºï¼Œä»è€Œä¸ºæ¶æ„æœç´¢æä¾›æœ‰æ•ˆçš„æŒ‡å¯¼ã€‚è¿™ç§æ–¹æ³•ä½¿å¾—åœ¨æ ‡æ³¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ï¼Œä¹Ÿèƒ½æœ‰æ•ˆåœ°æœç´¢å‡ºé€‚åˆå¤šæ¨¡æ€æ•°æ®çš„ç½‘ç»œæ¶æ„ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­æ²¡æœ‰æ˜ç¡®ç»™å‡ºå…·ä½“çš„è‡ªç›‘ç£ä»»åŠ¡ã€æœç´¢ç®—æ³•å’Œç½‘ç»œç»“æ„ç»†èŠ‚ã€‚ä½†æ˜¯ï¼Œå¯ä»¥æ¨æµ‹ï¼Œè‡ªç›‘ç£ä»»åŠ¡çš„è®¾è®¡éœ€è¦ä¸å¤šæ¨¡æ€æ•°æ®çš„ç‰¹æ€§ç›¸é€‚åº”ï¼Œä¾‹å¦‚ï¼Œå¯ä»¥é‡‡ç”¨å¯¹æ¯”å­¦ä¹ ã€ç”Ÿæˆå¼å­¦ä¹ ç­‰æ–¹æ³•ã€‚æœç´¢ç®—æ³•å¯ä»¥é€‰æ‹©å¼ºåŒ–å­¦ä¹ ã€è¿›åŒ–ç®—æ³•ç­‰ã€‚ç½‘ç»œç»“æ„çš„è®¾è®¡éœ€è¦è€ƒè™‘ä¸åŒæ¨¡æ€æ•°æ®çš„èåˆæ–¹å¼ï¼Œä¾‹å¦‚ï¼Œå¯ä»¥é‡‡ç”¨æ³¨æ„åŠ›æœºåˆ¶ã€è·¨æ¨¡æ€Transformerç­‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡çš„ä¸»è¦äº®ç‚¹åœ¨äºæå‡ºäº†è‡ªç›‘ç£å­¦ä¹ é©±åŠ¨çš„å¤šæ¨¡æ€ç¥ç»ç½‘ç»œæ¶æ„æœç´¢æ–¹æ³•ï¼Œå®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨æ— æ ‡æ³¨æ•°æ®ä¸ŠæˆåŠŸæœç´¢å‡ºæœ‰æ•ˆçš„DNNæ¶æ„ã€‚è™½ç„¶å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œå¯¹æ¯”åŸºçº¿æœªçŸ¥ï¼Œä½†è¯¥æ–¹æ³•ä¸ºè§£å†³å¤šæ¨¡æ€DNNæ¶æ„æœç´¢çš„æ ‡æ³¨æ•°æ®ä¾èµ–é—®é¢˜æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºå¤šæ¨¡æ€æ•°æ®åˆ†æé¢†åŸŸï¼Œä¾‹å¦‚è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—è¯Šæ–­ã€è§†é¢‘ç†è§£ç­‰ã€‚åœ¨è¿™äº›é¢†åŸŸï¼Œå¾€å¾€å­˜åœ¨å¤§é‡çš„æ— æ ‡æ³¨æ•°æ®ï¼Œè€Œæ ‡æ³¨æ•°æ®çš„è·å–æˆæœ¬å¾ˆé«˜ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿåˆ©ç”¨è¿™äº›æ— æ ‡æ³¨æ•°æ®ï¼Œè‡ªåŠ¨è®¾è®¡å‡ºé«˜æ€§èƒ½çš„å¤šæ¨¡æ€DNNæ¶æ„ï¼Œä»è€Œé™ä½æ¨¡å‹å¼€å‘æˆæœ¬ï¼Œæé«˜æ¨¡å‹æ€§èƒ½ï¼ŒåŠ é€Ÿç›¸å…³æŠ€æœ¯çš„è½åœ°ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Neural architecture search (NAS), which automates the architectural design process of deep neural networks (DNN), has attracted increasing attention. Multimodal DNNs that necessitate feature fusion from multiple modalities benefit from NAS due to their structural complexity; however, constructing an architecture for multimodal DNNs through NAS requires a substantial amount of labeled training data. Thus, this paper proposes a self-supervised learning (SSL) method for architecture search of multimodal DNNs. The proposed method applies SSL comprehensively for both the architecture search and model pretraining processes. Experimental results demonstrated that the proposed method successfully designed architectures for DNNs from unlabeled training data.

