---
layout: default
title: Diffusion Language Models are Provably Optimal Parallel Samplers
---

# Diffusion Language Models are Provably Optimal Parallel Samplers

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.25014" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.25014v1</a>
  <a href="https://arxiv.org/pdf/2512.25014.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.25014v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.25014v1', 'Diffusion Language Models are Provably Optimal Parallel Samplers')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Haozhe Jiang, Nika Haghtalab, Lijie Chen

**åˆ†ç±»**: cs.LG, cs.CC

**å‘å¸ƒæ—¥æœŸ**: 2025-12-31

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºæ€ç»´é“¾çš„æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼Œå®ç°æœ€ä¼˜å¹¶è¡Œé‡‡æ ·ï¼Œå¹¶è®ºè¯ä¿®è®¢æœºåˆ¶çš„å¿…è¦æ€§ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ‰©æ•£è¯­è¨€æ¨¡å‹` `å¹¶è¡Œé‡‡æ ·` `æ€ç»´é“¾` `é‡æ©ç ` `ä¿®è®¢æœºåˆ¶` `ç©ºé—´å¤æ‚åº¦` `è¡¨è¾¾èƒ½åŠ›`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è‡ªå›å½’æ¨¡å‹æ¨ç†é€Ÿåº¦æ…¢ï¼Œæ‰©æ•£è¯­è¨€æ¨¡å‹ï¼ˆDLMï¼‰ä½œä¸ºå¹¶è¡Œç”Ÿæˆæ–¹æ¡ˆæœ‰æ½œåŠ›åŠ é€Ÿæ¨ç†ã€‚
2. è®ºæ–‡æå‡ºå¢å¼ºæ€ç»´é“¾ï¼ˆCoTï¼‰çš„DLMï¼Œè¯æ˜å…¶èƒ½ä»¥æœ€ä¼˜æ­¥éª¤æ¨¡æ‹Ÿä»»ä½•å¹¶è¡Œé‡‡æ ·ç®—æ³•ã€‚
3. å¼•å…¥é‡æ©ç æˆ–ä¿®è®¢æœºåˆ¶ï¼Œä½¿DLMåœ¨æ¨¡æ‹Ÿå¹¶è¡Œé‡‡æ ·ç®—æ³•æ—¶è¾¾åˆ°æœ€ä¼˜ç©ºé—´å¤æ‚åº¦ï¼Œå¹¶æå‡è¡¨è¾¾èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼ˆDLMsï¼‰ä½œä¸ºä¸€ç§é€šè¿‡å¹¶è¡Œtokenç”Ÿæˆå®ç°æ›´å¿«æ¨ç†çš„æ–¹æ¡ˆï¼Œæ­£é€æ¸æˆä¸ºè‡ªå›å½’æ¨¡å‹çš„æœ‰å¸Œæœ›çš„æ›¿ä»£å“ã€‚æœ¬æ–‡é€šè¿‡å½¢å¼åŒ–å¹¶è¡Œé‡‡æ ·æ¨¡å‹ï¼Œä¸ºè¿™ç§ä¼˜åŠ¿æä¾›äº†ä¸¥æ ¼çš„ç†è®ºåŸºç¡€ï¼Œè¯æ˜äº†å¢å¼ºäº†å¤šé¡¹å¼é•¿åº¦æ€ç»´é“¾ï¼ˆCoTï¼‰çš„DLMså¯ä»¥ä½¿ç”¨æœ€ä¼˜æ•°é‡çš„é¡ºåºæ­¥éª¤æ¥æ¨¡æ‹Ÿä»»ä½•å¹¶è¡Œé‡‡æ ·ç®—æ³•ã€‚å› æ­¤ï¼Œæ¯å½“ç›®æ ‡åˆ†å¸ƒå¯ä»¥ä½¿ç”¨å°‘é‡é¡ºåºæ­¥éª¤ç”Ÿæˆæ—¶ï¼Œå°±å¯ä»¥ä½¿ç”¨DLMä»¥ç›¸åŒæ•°é‡çš„æœ€ä¼˜é¡ºåºæ­¥éª¤ç”Ÿæˆè¯¥åˆ†å¸ƒã€‚ç„¶è€Œï¼Œåœ¨æ²¡æœ‰ä¿®æ”¹å…ˆå‰å·²æ­ç¤ºtokenèƒ½åŠ›çš„æƒ…å†µä¸‹ï¼Œå…·æœ‰CoTçš„DLMä»ç„¶ä¼šäº§ç”Ÿè¾ƒå¤§çš„ä¸­é—´å ç”¨ç©ºé—´ã€‚æˆ‘ä»¬è¯æ˜ï¼Œå¯ç”¨é‡æ©ç ï¼ˆå°†æœªæ©ç çš„tokenè½¬æ¢ä¸ºæ©ç ï¼‰æˆ–ä¿®è®¢ï¼ˆå°†æœªæ©ç çš„tokenè½¬æ¢ä¸ºå…¶ä»–æœªæ©ç çš„tokenï¼‰ä»¥åŠCoTï¼Œå¯ä»¥ä½¿DLMä»¥æœ€ä¼˜çš„ç©ºé—´å¤æ‚åº¦æ¨¡æ‹Ÿä»»ä½•å¹¶è¡Œé‡‡æ ·ç®—æ³•ã€‚æˆ‘ä»¬é€šè¿‡å»ºç«‹ä¸¥æ ¼çš„è¡¨è¾¾èƒ½åŠ›å·®è·è¿›ä¸€æ­¥è¯æ˜äº†ä¿®è®¢çš„ä¼˜åŠ¿ï¼šå…·æœ‰ä¿®è®¢æˆ–é‡æ©ç çš„DLMæ¯”æ²¡æœ‰ä¿®è®¢æˆ–é‡æ©ç çš„DLMå…·æœ‰æ›´å¼ºçš„è¡¨è¾¾èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç»“æœä¸ä»…ä¸ºDLMä½œä¸ºæœ€æœ‰æ•ˆçš„å¹¶è¡Œé‡‡æ ·å™¨çš„å‰æ™¯æä¾›äº†ç†è®ºä¾æ®ï¼Œè€Œä¸”è¿˜æå€¡åœ¨DLMä¸­å¯ç”¨ä¿®è®¢ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è‡ªå›å½’è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆæ–‡æœ¬æ—¶ä¾èµ–é¡ºåºè§£ç ï¼Œé€Ÿåº¦è¾ƒæ…¢ã€‚æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼ˆDLMï¼‰æ—¨åœ¨é€šè¿‡å¹¶è¡Œç”Ÿæˆtokenæ¥åŠ é€Ÿæ¨ç†è¿‡ç¨‹ã€‚ç„¶è€Œï¼ŒDLMåœ¨å¹¶è¡Œé‡‡æ ·æ–¹é¢çš„ç†è®ºä¼˜åŠ¿å’Œèƒ½åŠ›è¾¹ç•Œå°šä¸æ˜ç¡®ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡æ—¶ï¼Œå¦‚ä½•ä¿è¯æ•ˆç‡å’Œè¡¨è¾¾èƒ½åŠ›æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ç†è®ºåˆ†æï¼Œè¯æ˜å¸¦æœ‰æ€ç»´é“¾ï¼ˆCoTï¼‰çš„DLMå¯ä»¥æ¨¡æ‹Ÿä»»ä½•å¹¶è¡Œé‡‡æ ·ç®—æ³•ï¼Œå¹¶ä¸”é€šè¿‡å¼•å…¥é‡æ©ç æˆ–ä¿®è®¢æœºåˆ¶ï¼Œå¯ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–DLMçš„ç©ºé—´å¤æ‚åº¦ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨å……åˆ†å‘æŒ¥DLMå¹¶è¡Œè®¡ç®—çš„ä¼˜åŠ¿ï¼ŒåŒæ—¶ä¿è¯å…¶åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨è¾¾èƒ½åŠ›å’Œæ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡æ„å»ºäº†ä¸€ä¸ªå¹¶è¡Œé‡‡æ ·çš„ç†è®ºæ¨¡å‹ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šåˆ†æäº†DLMçš„è®¡ç®—èƒ½åŠ›ã€‚ä¸»è¦æ¡†æ¶åŒ…æ‹¬ï¼š1) å½¢å¼åŒ–å®šä¹‰å¹¶è¡Œé‡‡æ ·ç®—æ³•ï¼›2) è¯æ˜å¸¦æœ‰CoTçš„DLMå¯ä»¥æ¨¡æ‹Ÿä»»ä½•å¹¶è¡Œé‡‡æ ·ç®—æ³•ï¼›3) åˆ†æDLMåœ¨æ²¡æœ‰é‡æ©ç æˆ–ä¿®è®¢æœºåˆ¶æ—¶çš„ç©ºé—´å¤æ‚åº¦ç“¶é¢ˆï¼›4) è¯æ˜å¼•å…¥é‡æ©ç æˆ–ä¿®è®¢æœºåˆ¶å¯ä»¥è§£å†³ç©ºé—´å¤æ‚åº¦é—®é¢˜ï¼Œå¹¶æå‡è¡¨è¾¾èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) é¦–æ¬¡ä»ç†è®ºä¸Šè¯æ˜äº†å¸¦æœ‰CoTçš„DLMå¯ä»¥ä½œä¸ºæœ€ä¼˜çš„å¹¶è¡Œé‡‡æ ·å™¨ï¼›2) æå‡ºäº†é‡æ©ç å’Œä¿®è®¢æœºåˆ¶ï¼Œå¹¶è¯æ˜å…¶å¯ä»¥æ˜¾è‘—æå‡DLMçš„ç©ºé—´å¤æ‚åº¦å’Œè¡¨è¾¾èƒ½åŠ›ï¼›3) å»ºç«‹äº†DLMè¡¨è¾¾èƒ½åŠ›çš„ä¸¥æ ¼å·®è·ï¼Œè¯æ˜äº†å¸¦æœ‰ä¿®è®¢æˆ–é‡æ©ç çš„DLMæ¯”æ²¡æœ‰è¿™äº›æœºåˆ¶çš„DLMå…·æœ‰æ›´å¼ºçš„è¡¨è¾¾èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨å¤šé¡¹å¼é•¿åº¦çš„CoTæ¥å¢å¼ºDLMçš„æ¨ç†èƒ½åŠ›ï¼›2) å¼•å…¥é‡æ©ç æœºåˆ¶ï¼Œå…è®¸å°†å·²ç”Ÿæˆçš„tokené‡æ–°è½¬æ¢ä¸ºæ©ç ï¼Œä»¥ä¾¿åç»­æ­¥éª¤å¯ä»¥ä¿®æ”¹è¿™äº›tokenï¼›3) å¼•å…¥ä¿®è®¢æœºåˆ¶ï¼Œå…è®¸å°†å·²ç”Ÿæˆçš„tokenä¿®æ”¹ä¸ºå…¶ä»–tokenã€‚è¿™äº›æœºåˆ¶çš„è®¾è®¡æ—¨åœ¨è§£å†³DLMåœ¨å¹¶è¡Œé‡‡æ ·è¿‡ç¨‹ä¸­å¯èƒ½é‡åˆ°çš„ç©ºé—´å¤æ‚åº¦å’Œè¡¨è¾¾èƒ½åŠ›ç“¶é¢ˆã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡é€šè¿‡ç†è®ºè¯æ˜ï¼Œå¢å¼ºäº†æ€ç»´é“¾ï¼ˆCoTï¼‰çš„æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼ˆDLMï¼‰èƒ½å¤Ÿä»¥æœ€ä¼˜çš„é¡ºåºæ­¥éª¤æ¨¡æ‹Ÿä»»ä½•å¹¶è¡Œé‡‡æ ·ç®—æ³•ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜è¯æ˜äº†å¯ç”¨é‡æ©ç æˆ–ä¿®è®¢æœºåˆ¶å¯ä»¥ä½¿DLMä»¥æœ€ä¼˜çš„ç©ºé—´å¤æ‚åº¦æ¨¡æ‹Ÿä»»ä½•å¹¶è¡Œé‡‡æ ·ç®—æ³•ï¼Œå¹¶å»ºç«‹äº†DLMè¡¨è¾¾èƒ½åŠ›çš„ä¸¥æ ¼å·®è·ï¼Œè¯æ˜äº†å¸¦æœ‰ä¿®è®¢æˆ–é‡æ©ç çš„DLMæ¯”æ²¡æœ‰è¿™äº›æœºåˆ¶çš„DLMå…·æœ‰æ›´å¼ºçš„è¡¨è¾¾èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºéœ€è¦å¿«é€Ÿæ–‡æœ¬ç”Ÿæˆçš„åœºæ™¯ï¼Œä¾‹å¦‚æœºå™¨ç¿»è¯‘ã€æ–‡æœ¬æ‘˜è¦ã€å¯¹è¯ç³»ç»Ÿç­‰ã€‚é€šè¿‡åˆ©ç”¨DLMçš„å¹¶è¡Œè®¡ç®—èƒ½åŠ›ï¼Œå¯ä»¥æ˜¾è‘—æå‡ç”Ÿæˆé€Ÿåº¦ï¼Œä»è€Œæé«˜ç”¨æˆ·ä½“éªŒå’Œç³»ç»Ÿæ•ˆç‡ã€‚æ­¤å¤–ï¼Œé‡æ©ç å’Œä¿®è®¢æœºåˆ¶çš„å¼•å…¥ï¼Œä¹Ÿä¸ºDLMåœ¨å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡æ—¶æä¾›äº†æ›´å¼ºçš„çµæ´»æ€§å’Œè¡¨è¾¾èƒ½åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Diffusion language models (DLMs) have emerged as a promising alternative to autoregressive models for faster inference via parallel token generation. We provide a rigorous foundation for this advantage by formalizing a model of parallel sampling and showing that DLMs augmented with polynomial-length chain-of-thought (CoT) can simulate any parallel sampling algorithm using an optimal number of sequential steps. Consequently, whenever a target distribution can be generated using a small number of sequential steps, a DLM can be used to generate the distribution using the same number of optimal sequential steps. However, without the ability to modify previously revealed tokens, DLMs with CoT can still incur large intermediate footprints. We prove that enabling remasking (converting unmasked tokens to masks) or revision (converting unmasked tokens to other unmasked tokens) together with CoT further allows DLMs to simulate any parallel sampling algorithm with optimal space complexity. We further justify the advantage of revision by establishing a strict expressivity gap: DLMs with revision or remasking are strictly more expressive than those without. Our results not only provide a theoretical justification for the promise of DLMs as the most efficient parallel sampler, but also advocate for enabling revision in DLMs.

