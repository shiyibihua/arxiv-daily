---
layout: default
title: "Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space"
---

# Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.24617" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.24617v1</a>
  <a href="https://arxiv.org/pdf/2512.24617.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.24617v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.24617v1', 'Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xingwei Qu, Shaowen Wang, Zihao Huang, Kai Hua, Fan Yin, Rui-Jie Zhu, Jundong Zhou, Qiyang Min, Zihao Wang, Yizhi Li, Tianyu Zhang, He Xing, Zheng Zhang, Yuxuan Song, Tianyu Zheng, Zhiyuan Zeng, Chenghua Lin, Ge Zhang, Wenhao Huang

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-31

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŠ¨æ€å¤§æ¦‚å¿µæ¨¡å‹ï¼ˆDLCMï¼‰ï¼Œé€šè¿‡è‡ªé€‚åº”è¯­ä¹‰ç©ºé—´ä¸­çš„æ½œåœ¨æ¨ç†æå‡LLMæ•ˆç‡ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `åŠ¨æ€æ¦‚å¿µæ¨¡å‹` `è¯­ä¹‰ç©ºé—´` `æ½œåœ¨æ¨ç†` `å‹ç¼©æ„ŸçŸ¥` `åˆ†å±‚å»ºæ¨¡` `Î¼På‚æ•°åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMå¯¹æ‰€æœ‰tokené‡‡ç”¨ç»Ÿä¸€è®¡ç®—ï¼Œå¿½ç•¥äº†è¯­è¨€ä¿¡æ¯å¯†åº¦ä¸å‡çš„é—®é¢˜ï¼Œå¯¼è‡´ç®—åŠ›æµªè´¹ã€‚
2. DLCMé€šè¿‡å­¦ä¹ è¯­ä¹‰è¾¹ç•Œï¼Œå°†tokenè®¡ç®—è½¬ç§»åˆ°å‹ç¼©çš„æ¦‚å¿µç©ºé—´ï¼Œæå‡æ¨ç†æ•ˆç‡ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒDLCMåœ¨å›ºå®šFLOPsä¸‹ï¼Œé›¶æ ·æœ¬åŸºå‡†æµ‹è¯•å¹³å‡æå‡2.69%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹æ‰€æœ‰tokenåº”ç”¨ç»Ÿä¸€çš„è®¡ç®—ï¼Œç„¶è€Œè¯­è¨€çš„ä¿¡æ¯å¯†åº¦é«˜åº¦ä¸å‡åŒ€ã€‚è¿™ç§tokenç»Ÿä¸€çš„æ¨¡å¼åœ¨å±€éƒ¨å¯é¢„æµ‹çš„è·¨åº¦ä¸Šæµªè´¹äº†ç®—åŠ›ï¼ŒåŒæ—¶å¯¹è¯­ä¹‰å…³é”®çš„è½¬æ¢åˆ†é…ä¸è¶³ã€‚æˆ‘ä»¬æå‡ºäº†åŠ¨æ€å¤§æ¦‚å¿µæ¨¡å‹ï¼ˆDLCMï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ†å±‚è¯­è¨€å»ºæ¨¡æ¡†æ¶ï¼Œå®ƒä»æ½œåœ¨è¡¨ç¤ºä¸­å­¦ä¹ è¯­ä¹‰è¾¹ç•Œï¼Œå¹¶å°†è®¡ç®—ä»tokenè½¬ç§»åˆ°å‹ç¼©çš„æ¦‚å¿µç©ºé—´ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°è¿›è¡Œæ¨ç†ã€‚DLCMç«¯åˆ°ç«¯åœ°å‘ç°å¯å˜é•¿åº¦çš„æ¦‚å¿µï¼Œè€Œæ— éœ€ä¾èµ–é¢„å®šä¹‰çš„è¯­è¨€å•å…ƒã€‚åˆ†å±‚å‹ç¼©ä»æ ¹æœ¬ä¸Šæ”¹å˜äº†ç¼©æ”¾è¡Œä¸ºã€‚æˆ‘ä»¬å¼•å…¥äº†ç¬¬ä¸€ä¸ªå‹ç¼©æ„ŸçŸ¥ç¼©æ”¾å®šå¾‹ï¼Œå®ƒè§£è€¦äº†tokençº§åˆ«çš„å®¹é‡ã€æ¦‚å¿µçº§åˆ«çš„æ¨ç†å®¹é‡å’Œå‹ç¼©ç‡ï¼Œä»è€Œèƒ½å¤Ÿåœ¨å›ºå®šçš„FLOPsä¸‹è¿›è¡Œæœ‰åŸåˆ™çš„è®¡ç®—åˆ†é…ã€‚ä¸ºäº†ç¨³å®šåœ°è®­ç»ƒè¿™ç§å¼‚æ„æ¶æ„ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼€å‘äº†ä¸€ç§è§£è€¦çš„Î¼På‚æ•°åŒ–ï¼Œå®ƒæ”¯æŒè·¨å®½åº¦å’Œå‹ç¼©æœºåˆ¶çš„é›¶æ ·æœ¬è¶…å‚æ•°è¿ç§»ã€‚åœ¨å®é™…è®¾ç½®ï¼ˆR=4ï¼Œå¯¹åº”äºæ¯ä¸ªæ¦‚å¿µå¹³å‡å››ä¸ªtokenï¼‰ä¸­ï¼ŒDLCMå°†å¤§çº¦ä¸‰åˆ†ä¹‹ä¸€çš„æ¨ç†è®¡ç®—é‡æ–°åˆ†é…åˆ°ä¸€ä¸ªæ›´é«˜å®¹é‡çš„æ¨ç†éª¨å¹²ä¸­ï¼Œåœ¨åŒ¹é…çš„æ¨ç†FLOPsä¸‹ï¼Œåœ¨12ä¸ªé›¶æ ·æœ¬åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†+2.69%çš„å¹³å‡æ”¹è¿›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹æ‰€æœ‰tokené‡‡ç”¨ç»Ÿä¸€çš„è®¡ç®—æ–¹å¼ï¼Œæ²¡æœ‰è€ƒè™‘åˆ°è¯­è¨€æœ¬èº«ä¿¡æ¯å¯†åº¦çš„å·®å¼‚æ€§ã€‚è¿™ç§åšæ³•å¯¼è‡´åœ¨ä¿¡æ¯å†—ä½™çš„tokenä¸Šæµªè´¹è®¡ç®—èµ„æºï¼Œè€Œåœ¨è¯­ä¹‰å…³é”®çš„tokenä¸Šè®¡ç®—èµ„æºä¸è¶³ï¼Œé™åˆ¶äº†æ¨¡å‹çš„æ•ˆç‡å’Œæ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDLCMçš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†tokençº§åˆ«çš„è®¡ç®—è½¬ç§»åˆ°æ¦‚å¿µçº§åˆ«çš„è®¡ç®—ã€‚é€šè¿‡å­¦ä¹ tokençš„æ½œåœ¨è¡¨ç¤ºï¼Œæ¨¡å‹èƒ½å¤Ÿè‡ªåŠ¨å‘ç°è¯­ä¹‰è¾¹ç•Œï¼Œå¹¶å°†å¤šä¸ªtokenå‹ç¼©æˆä¸€ä¸ªâ€œæ¦‚å¿µâ€ã€‚ç„¶ååœ¨å‹ç¼©åçš„æ¦‚å¿µç©ºé—´ä¸­è¿›è¡Œæ¨ç†ï¼Œä»è€Œå‡å°‘è®¡ç®—é‡ï¼Œå¹¶æé«˜æ¨ç†æ•ˆç‡ã€‚è¿™ç§åˆ†å±‚å‹ç¼©çš„æ–¹å¼èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”è¯­è¨€ä¿¡æ¯å¯†åº¦çš„ä¸å‡åŒ€æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDLCMåŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) **Token Embeddingå±‚**ï¼šå°†è¾“å…¥tokenè½¬æ¢ä¸ºå‘é‡è¡¨ç¤ºã€‚2) **æ¦‚å¿µå‘ç°æ¨¡å—**ï¼šåŸºäºtoken embeddingå­¦ä¹ è¯­ä¹‰è¾¹ç•Œï¼Œå°†tokenåˆ†ç»„ä¸ºæ¦‚å¿µã€‚3) **æ¦‚å¿µEmbeddingå±‚**ï¼šå°†æ¦‚å¿µè½¬æ¢ä¸ºå‘é‡è¡¨ç¤ºã€‚4) **æ¨ç†éª¨å¹²ç½‘ç»œ**ï¼šåœ¨æ¦‚å¿µembeddingä¸Šè¿›è¡Œæ¨ç†ï¼Œä¾‹å¦‚Transformerç½‘ç»œã€‚5) **è¾“å‡ºå±‚**ï¼šå°†æ¨ç†ç»“æœæ˜ å°„å›tokençº§åˆ«ã€‚æ•´ä¸ªæ¡†æ¶æ˜¯ç«¯åˆ°ç«¯å¯è®­ç»ƒçš„ã€‚

**å…³é”®åˆ›æ–°**ï¼šDLCMçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) **åŠ¨æ€æ¦‚å¿µå‘ç°**ï¼šæ¨¡å‹èƒ½å¤Ÿè‡ªåŠ¨å­¦ä¹ è¯­ä¹‰è¾¹ç•Œï¼Œæ— éœ€é¢„å®šä¹‰çš„è¯­è¨€å•å…ƒã€‚2) **å‹ç¼©æ„ŸçŸ¥ç¼©æ”¾å®šå¾‹**ï¼šæå‡ºäº†æ–°çš„ç¼©æ”¾å®šå¾‹ï¼Œè€ƒè™‘äº†tokençº§åˆ«å®¹é‡ã€æ¦‚å¿µçº§åˆ«æ¨ç†å®¹é‡å’Œå‹ç¼©ç‡ä¹‹é—´çš„å…³ç³»ï¼Œä»è€Œèƒ½å¤Ÿåœ¨å›ºå®šFLOPsä¸‹è¿›è¡Œæ›´åˆç†çš„è®¡ç®—åˆ†é…ã€‚3) **è§£è€¦çš„Î¼På‚æ•°åŒ–**ï¼šæå‡ºäº†ä¸€ç§æ–°çš„å‚æ•°åŒ–æ–¹æ³•ï¼Œèƒ½å¤Ÿç¨³å®šè®­ç»ƒå¼‚æ„æ¶æ„ï¼Œå¹¶æ”¯æŒé›¶æ ·æœ¬è¶…å‚æ•°è¿ç§»ã€‚

**å…³é”®è®¾è®¡**ï¼š1) **æ¦‚å¿µå‘ç°æ¨¡å—**ï¼šå¯ä»¥ä½¿ç”¨å„ç§èšç±»ç®—æ³•æˆ–ç¥ç»ç½‘ç»œæ¥å®ç°ï¼Œç›®æ ‡æ˜¯å­¦ä¹ tokenä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œå¹¶å°†ç›¸ä¼¼çš„tokenåˆ†ç»„ä¸ºæ¦‚å¿µã€‚2) **å‹ç¼©ç‡R**ï¼šæ§åˆ¶æ¯ä¸ªæ¦‚å¿µåŒ…å«çš„tokenæ•°é‡ï¼ŒRè¶Šå¤§ï¼Œå‹ç¼©ç‡è¶Šé«˜ã€‚3) **æ¨ç†éª¨å¹²ç½‘ç»œ**ï¼šå¯ä»¥ä½¿ç”¨å„ç§Transformerå˜ä½“ï¼Œä¾‹å¦‚Sparse Transformeræˆ–Longformerï¼Œä»¥æé«˜æ¨ç†æ•ˆç‡ã€‚4) **æŸå¤±å‡½æ•°**ï¼šé™¤äº†æ ‡å‡†çš„è¯­è¨€å»ºæ¨¡æŸå¤±å¤–ï¼Œè¿˜å¯ä»¥æ·»åŠ æ­£åˆ™åŒ–é¡¹ï¼Œä»¥é¼“åŠ±æ¦‚å¿µçš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.24617v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.24617v1/full_training_linear.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.24617v1/robust_decay_fit.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨R=4çš„å‹ç¼©ç‡ä¸‹ï¼ŒDLCMå°†å¤§çº¦ä¸‰åˆ†ä¹‹ä¸€çš„æ¨ç†è®¡ç®—é‡æ–°åˆ†é…åˆ°ä¸€ä¸ªæ›´é«˜å®¹é‡çš„æ¨ç†éª¨å¹²ä¸­ï¼Œåœ¨12ä¸ªé›¶æ ·æœ¬åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†+2.69%çš„å¹³å‡æ”¹è¿›ã€‚è¿™è¡¨æ˜DLCMèƒ½å¤Ÿåœ¨å›ºå®šFLOPsä¸‹æ˜¾è‘—æé«˜LLMçš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DLCMå¯åº”ç”¨äºå„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œå¦‚æœºå™¨ç¿»è¯‘ã€æ–‡æœ¬æ‘˜è¦ã€é—®ç­”ç³»ç»Ÿç­‰ã€‚é€šè¿‡æé«˜LLMçš„æ¨ç†æ•ˆç‡ï¼ŒDLCMå¯ä»¥é™ä½è®¡ç®—æˆæœ¬ï¼Œå¹¶ä½¿å…¶èƒ½å¤Ÿåœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šè¿è¡Œã€‚æ­¤å¤–ï¼ŒDLCMçš„åŠ¨æ€æ¦‚å¿µå‘ç°èƒ½åŠ›å¯ä»¥å¸®åŠ©æˆ‘ä»¬æ›´å¥½åœ°ç†è§£è¯­è¨€çš„ç»“æ„å’Œè¯­ä¹‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) apply uniform computation to all tokens, despite language exhibiting highly non-uniform information density. This token-uniform regime wastes capacity on locally predictable spans while under-allocating computation to semantically critical transitions. We propose $\textbf{Dynamic Large Concept Models (DLCM)}$, a hierarchical language modeling framework that learns semantic boundaries from latent representations and shifts computation from tokens to a compressed concept space where reasoning is more efficient. DLCM discovers variable-length concepts end-to-end without relying on predefined linguistic units. Hierarchical compression fundamentally changes scaling behavior. We introduce the first $\textbf{compression-aware scaling law}$, which disentangles token-level capacity, concept-level reasoning capacity, and compression ratio, enabling principled compute allocation under fixed FLOPs. To stably train this heterogeneous architecture, we further develop a $\textbf{decoupled $Î¼$P parametrization}$ that supports zero-shot hyperparameter transfer across widths and compression regimes. At a practical setting ($R=4$, corresponding to an average of four tokens per concept), DLCM reallocates roughly one-third of inference compute into a higher-capacity reasoning backbone, achieving a $\textbf{+2.69$\%$ average improvement}$ across 12 zero-shot benchmarks under matched inference FLOPs.

