---
layout: default
title: Attribution-Guided Distillation of Matryoshka Sparse Autoencoders
---

# Attribution-Guided Distillation of Matryoshka Sparse Autoencoders

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.24975" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.24975v1</a>
  <a href="https://arxiv.org/pdf/2512.24975.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.24975v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.24975v1', 'Attribution-Guided Distillation of Matryoshka Sparse Autoencoders')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Cristina P. Martin-Linares, Jonathan P. Ling

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-12-31

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDMSAEï¼Œé€šè¿‡å½’å› å¼•å¯¼è’¸é¦Matryoshkaç¨€ç–è‡ªç¼–ç å™¨ï¼Œæå‡ç‰¹å¾ä¸€è‡´æ€§å’Œå¯è¿ç§»æ€§ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `ç¨€ç–è‡ªç¼–ç å™¨` `è’¸é¦å­¦ä¹ ` `æ¨¡å‹è§£é‡Šæ€§` `ç‰¹å¾æå–` `çŸ¥è¯†è¿ç§»`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç¨€ç–è‡ªç¼–ç å™¨å­¦ä¹ åˆ°çš„ç‰¹å¾å†—ä½™ä¸”ä¸ç¨³å®šï¼Œå¯¼è‡´è§£é‡Šæ€§å·®ï¼Œéš¾ä»¥è¿ç§»å’Œå¤ç”¨ã€‚
2. DMSAEé€šè¿‡è¿­ä»£è’¸é¦ï¼Œæå–å¹¶é‡ç”¨ä¸€è‡´ä¸”æœ‰ç”¨çš„ç‰¹å¾æ ¸å¿ƒï¼Œæå‡ç‰¹å¾çš„ç¨³å®šæ€§å’Œå¯è¿ç§»æ€§ã€‚
3. å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨DMSAEè’¸é¦å‡ºçš„ç‰¹å¾æ ¸å¿ƒè®­ç»ƒSAEï¼Œèƒ½æœ‰æ•ˆæå‡SAEBenchæŒ‡æ ‡ï¼ŒéªŒè¯äº†æ–¹æ³•æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç¨€ç–è‡ªç¼–ç å™¨(SAE)æ—¨åœ¨å°†æ¨¡å‹æ¿€æ´»è§£è€¦ä¸ºå•ä¹‰çš„ã€äººç±»å¯è§£é‡Šçš„ç‰¹å¾ã€‚ç„¶è€Œï¼Œå®è·µä¸­å­¦ä¹ åˆ°çš„ç‰¹å¾é€šå¸¸æ˜¯å†—ä½™çš„ï¼Œå¹¶ä¸”åœ¨ä¸åŒçš„è®­ç»ƒè¿è¡Œå’Œç¨€ç–åº¦æ°´å¹³ä¸Šæœ‰æ‰€ä¸åŒï¼Œè¿™ä½¿å¾—è§£é‡Šéš¾ä»¥è½¬ç§»å’Œé‡ç”¨ã€‚æˆ‘ä»¬å¼•å…¥äº†Distilled Matryoshka Sparse Autoencoders (DMSAEs)ï¼Œè¿™æ˜¯ä¸€ç§è®­ç»ƒæµç¨‹ï¼Œå®ƒæç‚¼å‡ºä¸€ä¸ªç´§å‡‘çš„ã€å§‹ç»ˆæœ‰ç”¨çš„ç‰¹å¾æ ¸å¿ƒï¼Œå¹¶é‡ç”¨å®ƒæ¥è®­ç»ƒæ–°çš„SAEã€‚DMSAEsè¿è¡Œä¸€ä¸ªè¿­ä»£è’¸é¦å¾ªç¯ï¼šè®­ç»ƒä¸€ä¸ªå…·æœ‰å…±äº«æ ¸å¿ƒçš„Matryoshka SAEï¼Œä½¿ç”¨æ¢¯åº¦Xæ¿€æ´»æ¥æµ‹é‡æ¯ä¸ªç‰¹å¾å¯¹æœ€åµŒå¥—é‡å»ºä¸­ä¸‹ä¸€ä¸ªtokenæŸå¤±çš„è´¡çŒ®ï¼Œå¹¶ä¸”åªä¿ç•™è§£é‡Šå›ºå®šæ¯”ä¾‹å½’å› çš„æœ€å°å­é›†ã€‚åªæœ‰æ ¸å¿ƒç¼–ç å™¨æƒé‡å‘é‡åœ¨å¾ªç¯ä¸­ä¼ é€’ï¼›æ ¸å¿ƒè§£ç å™¨å’Œæ‰€æœ‰éæ ¸å¿ƒæ½œåœ¨å˜é‡æ¯æ¬¡éƒ½ä¼šé‡æ–°åˆå§‹åŒ–ã€‚åœ¨Gemma-2-2Bç¬¬12å±‚æ®‹å·®æµæ¿€æ´»ä¸Šï¼Œä¸ƒä¸ªå¾ªç¯çš„è’¸é¦ï¼ˆ500M tokensï¼Œ65kå®½åº¦ï¼‰äº§ç”Ÿäº†ä¸€ä¸ªé‡å¤é€‰æ‹©çš„197ä¸ªç‰¹å¾çš„è’¸é¦æ ¸å¿ƒã€‚ä½¿ç”¨è¿™ä¸ªè’¸é¦æ ¸å¿ƒè¿›è¡Œè®­ç»ƒæé«˜äº†å‡ ä¸ªSAEBenchæŒ‡æ ‡ï¼Œå¹¶è¯æ˜äº†ä¸€è‡´çš„æ½œåœ¨ç‰¹å¾é›†å¯ä»¥åœ¨ä¸åŒçš„ç¨€ç–åº¦æ°´å¹³ä¸Šä¼ è¾“ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ç¨€ç–è‡ªç¼–ç å™¨(SAE)è®­ç»ƒä¸­ç‰¹å¾å†—ä½™ã€ä¸ç¨³å®šä»¥åŠéš¾ä»¥è¿ç§»çš„é—®é¢˜ã€‚ç°æœ‰çš„SAEè®­ç»ƒæ–¹æ³•åœ¨ä¸åŒè®­ç»ƒè½®æ¬¡å’Œç¨€ç–åº¦ä¸‹ï¼Œå­¦ä¹ åˆ°çš„ç‰¹å¾å·®å¼‚è¾ƒå¤§ï¼Œå¯¼è‡´æ¨¡å‹è§£é‡Šæ€§å·®ï¼Œä¸”éš¾ä»¥å°†å­¦åˆ°çš„çŸ¥è¯†è¿ç§»åˆ°æ–°çš„ä»»åŠ¡æˆ–æ¨¡å‹ä¸Šã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è’¸é¦çš„æ–¹å¼ï¼Œä»å¤šä¸ªSAEä¸­æå–å‡ºä¸€ä¸ªå…±äº«çš„ã€ä¸€è‡´çš„ç‰¹å¾æ ¸å¿ƒï¼Œå¹¶åˆ©ç”¨è¯¥æ ¸å¿ƒæ¥æŒ‡å¯¼åç»­SAEçš„è®­ç»ƒã€‚é€šè¿‡è¿­ä»£è’¸é¦ï¼Œé€æ­¥ç­›é€‰å‡ºå¯¹æ¨¡å‹æ€§èƒ½è´¡çŒ®æœ€å¤§çš„ç‰¹å¾ï¼Œä»è€Œè·å¾—ä¸€ä¸ªç´§å‡‘ä¸”ç¨³å®šçš„ç‰¹å¾è¡¨ç¤ºã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDMSAEçš„æ•´ä½“æ¡†æ¶æ˜¯ä¸€ä¸ªè¿­ä»£è’¸é¦å¾ªç¯ã€‚æ¯ä¸ªå¾ªç¯åŒ…å«ä»¥ä¸‹æ­¥éª¤ï¼š1) è®­ç»ƒä¸€ä¸ªMatryoshka SAEï¼Œè¯¥SAEå…·æœ‰ä¸€ä¸ªå…±äº«çš„æ ¸å¿ƒç¼–ç å™¨ï¼›2) ä½¿ç”¨æ¢¯åº¦Xæ¿€æ´»æ–¹æ³•è¯„ä¼°æ¯ä¸ªç‰¹å¾å¯¹ä¸‹ä¸€ä¸ªtokené¢„æµ‹æŸå¤±çš„è´¡çŒ®ï¼›3) é€‰æ‹©è´¡çŒ®æœ€å¤§çš„ç‰¹å¾å­é›†ä½œä¸ºæ–°çš„æ ¸å¿ƒï¼›4) å°†æ ¸å¿ƒç¼–ç å™¨çš„æƒé‡ä¼ é€’åˆ°ä¸‹ä¸€ä¸ªå¾ªç¯ï¼Œå¹¶é‡æ–°åˆå§‹åŒ–æ ¸å¿ƒè§£ç å™¨å’Œéæ ¸å¿ƒæ½œåœ¨å˜é‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šDMSAEçš„å…³é”®åˆ›æ–°åœ¨äºä½¿ç”¨å½’å› æ–¹æ³•ï¼ˆæ¢¯åº¦Xæ¿€æ´»ï¼‰æ¥æŒ‡å¯¼ç‰¹å¾é€‰æ‹©ï¼Œä»è€Œç¡®ä¿é€‰æ‹©çš„ç‰¹å¾å¯¹æ¨¡å‹æ€§èƒ½å…·æœ‰é‡è¦æ„ä¹‰ã€‚æ­¤å¤–ï¼Œé€šè¿‡è¿­ä»£è’¸é¦å’Œæ ¸å¿ƒé‡ç”¨ï¼ŒDMSAEèƒ½å¤Ÿå­¦ä¹ åˆ°æ›´åŠ ç¨³å®šå’Œå¯è¿ç§»çš„ç‰¹å¾è¡¨ç¤ºã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒDMSAEèƒ½å¤Ÿæœ‰æ•ˆåœ°å‡å°‘ç‰¹å¾å†—ä½™ï¼Œå¹¶æé«˜ç‰¹å¾çš„ä¸€è‡´æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šDMSAEçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨Matryoshka SAEä½œä¸ºåŸºç¡€æ¨¡å‹ï¼Œå…è®¸åœ¨ä¸åŒç¨€ç–åº¦ä¸‹è¿›è¡Œè®­ç»ƒï¼›2) ä½¿ç”¨æ¢¯åº¦Xæ¿€æ´»ä½œä¸ºå½’å› æ–¹æ³•ï¼Œè¯„ä¼°ç‰¹å¾çš„é‡è¦æ€§ï¼›3) è®¾ç½®å›ºå®šçš„å½’å› æ¯”ä¾‹ï¼Œæ§åˆ¶æ ¸å¿ƒçš„å¤§å°ï¼›4) è¿­ä»£è’¸é¦çš„å¾ªç¯æ¬¡æ•°å’Œè®­ç»ƒtokensæ•°é‡ç­‰è¶…å‚æ•°ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.24975v1/figures/Fig1_DMSAE_Schematic_DynamicNonCoreL0.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.24975v1/figures/k320_core_latent_origins_0-1-2-3-4-5-6-7.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.24975v1/figures/figure_distilled_vs_random_1panel.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨Gemma-2-2Bæ¨¡å‹ç¬¬12å±‚æ®‹å·®æµæ¿€æ´»ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œç»è¿‡ä¸ƒä¸ªå¾ªç¯çš„è’¸é¦ï¼ˆ500M tokensï¼Œ65kå®½åº¦ï¼‰ï¼ŒDMSAEèƒ½å¤Ÿæå–å‡ºä¸€ä¸ªåŒ…å«197ä¸ªç‰¹å¾çš„è’¸é¦æ ¸å¿ƒï¼Œè¿™äº›ç‰¹å¾åœ¨ä¸åŒçš„è®­ç»ƒè½®æ¬¡ä¸­è¢«é‡å¤é€‰æ‹©ã€‚ä½¿ç”¨è¯¥æ ¸å¿ƒè¿›è¡Œè®­ç»ƒï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡SAEBenchçš„å„é¡¹æŒ‡æ ‡ï¼Œè¯æ˜äº†DMSAEèƒ½å¤Ÿå­¦ä¹ åˆ°ç¨³å®šä¸”å¯è¿ç§»çš„ç‰¹å¾è¡¨ç¤ºã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DMSAEå¯åº”ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸï¼Œä¾‹å¦‚æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„è§£é‡Šæ€§å’Œå¯æ§æ€§ã€‚é€šè¿‡æå–æ¨¡å‹å†…éƒ¨çš„å…³é”®ç‰¹å¾ï¼Œå¯ä»¥æ›´å¥½åœ°ç†è§£æ¨¡å‹çš„è¡Œä¸ºï¼Œå¹¶è¿›è¡Œé’ˆå¯¹æ€§çš„å¹²é¢„å’Œä¼˜åŒ–ã€‚æ­¤å¤–ï¼ŒDMSAEè¿˜å¯ä»¥ç”¨äºçŸ¥è¯†è¿ç§»å’Œæ¨¡å‹å‹ç¼©ï¼Œå°†å¤§å‹æ¨¡å‹çš„çŸ¥è¯†è¿ç§»åˆ°å°å‹æ¨¡å‹ï¼Œå¹¶å‡å°‘æ¨¡å‹çš„è®¡ç®—å¤æ‚åº¦ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Sparse autoencoders (SAEs) aim to disentangle model activations into monosemantic, human-interpretable features. In practice, learned features are often redundant and vary across training runs and sparsity levels, which makes interpretations difficult to transfer and reuse. We introduce Distilled Matryoshka Sparse Autoencoders (DMSAEs), a training pipeline that distills a compact core of consistently useful features and reuses it to train new SAEs. DMSAEs run an iterative distillation cycle: train a Matryoshka SAE with a shared core, use gradient X activation to measure each feature's contribution to next-token loss in the most nested reconstruction, and keep only the smallest subset that explains a fixed fraction of the attribution. Only the core encoder weight vectors are transferred across cycles; the core decoder and all non-core latents are reinitialized each time. On Gemma-2-2B layer 12 residual stream activations, seven cycles of distillation (500M tokens, 65k width) yielded a distilled core of 197 features that were repeatedly selected. Training using this distilled core improves several SAEBench metrics and demonstrates that consistent sets of latent features can be transferred across sparsity levels

