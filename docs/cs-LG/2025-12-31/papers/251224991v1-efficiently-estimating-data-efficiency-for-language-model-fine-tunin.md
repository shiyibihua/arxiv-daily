---
layout: default
title: Efficiently Estimating Data Efficiency for Language Model Fine-tuning
---

# Efficiently Estimating Data Efficiency for Language Model Fine-tuning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.24991" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.24991v1</a>
  <a href="https://arxiv.org/pdf/2512.24991.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.24991v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.24991v1', 'Efficiently Estimating Data Efficiency for Language Model Fine-tuning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Gyung Hyun Je, Colin Raffel

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-12-31

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºæ¢¯åº¦ä½™å¼¦ç›¸ä¼¼æ€§çš„æ•°æ®æ•ˆç‡é¢„æµ‹æ–¹æ³•ï¼Œå‡å°‘LLMå¾®è°ƒçš„æ ‡æ³¨æˆæœ¬ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ•°æ®æ•ˆç‡` `è¯­è¨€æ¨¡å‹å¾®è°ƒ` `æ¢¯åº¦ä½™å¼¦ç›¸ä¼¼æ€§` `ä½ç½®ä¿¡åº¦æ ·æœ¬` `æ ‡æ³¨æˆæœ¬`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤§å‹è¯­è¨€æ¨¡å‹å¾®è°ƒéœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®ï¼Œä½†ä»»åŠ¡çš„æ•°æ®æ•ˆç‡æœªçŸ¥ï¼Œå¯¼è‡´æ ‡æ³¨æˆæœ¬é«˜æ˜‚ã€‚
2. æå‡ºåˆ©ç”¨å°‘é‡æ ‡æ³¨æ ·æœ¬ï¼Œé€šè¿‡è®¡ç®—ä½ç½®ä¿¡åº¦æ ·æœ¬çš„æ¢¯åº¦ä½™å¼¦ç›¸ä¼¼æ€§æ¥é¢„æµ‹æ•°æ®æ•ˆç‡ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆé¢„æµ‹æ•°æ®æ•ˆç‡ï¼Œå‡å°‘ä¸å¿…è¦çš„æ ‡æ³¨ï¼Œæ•´ä½“æ•°æ®æ•ˆç‡é¢„æµ‹è¯¯å·®ä¸º8.6%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è®¸å¤šä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºä¸é”™çš„é›¶æ ·æœ¬èƒ½åŠ›ï¼Œä½†å¾®è°ƒæ˜¯æé«˜å…¶æ€§èƒ½çš„å¸¸ç”¨æ–¹æ³•ã€‚ç„¶è€Œï¼Œä»»åŠ¡çš„æ•°æ®æ•ˆç‡ï¼ˆå³è¾¾åˆ°æœŸæœ›æ€§èƒ½æ‰€éœ€çš„å¾®è°ƒæ ·æœ¬æ•°é‡ï¼‰é€šå¸¸æ˜¯æœªçŸ¥çš„ï¼Œå¯¼è‡´å¢é‡æ ‡æ³¨å’Œé‡æ–°è®­ç»ƒçš„æˆæœ¬é«˜æ˜‚ã€‚æœ¬æ–‡é€šè¿‡ä¸€ç»„ç²¾å¿ƒæŒ‘é€‰çš„30ä¸ªä¸“ä¸šä»»åŠ¡ï¼Œå±•ç¤ºäº†é«˜æ€§èƒ½LLMå¯èƒ½åœ¨é›¶æ ·æœ¬å­¦ä¹ ä¸­è¡¨ç°ä¸ä½³ï¼Œä½†åœ¨å¾®è°ƒåå¯ä»¥è·å¾—æ›´å¼ºçš„æ€§èƒ½ã€‚è¿™ä¿ƒä½¿æˆ‘ä»¬éœ€è¦é¢„æµ‹ä»»åŠ¡çš„æ•°æ®æ•ˆç‡ï¼Œè€Œæ— éœ€å¢é‡æ ‡æ³¨ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å…·ä½“çš„æŒ‡æ ‡æ¥é‡åŒ–ä»»åŠ¡çš„æ•°æ®æ•ˆç‡ï¼Œå¹¶æå‡ºä½¿ç”¨ä½ç½®ä¿¡åº¦æ ·æœ¬çš„æ¢¯åº¦ä½™å¼¦ç›¸ä¼¼æ€§ï¼ŒåŸºäºå°‘é‡æ ‡è®°æ ·æœ¬æ¥é¢„æµ‹æ•°æ®æ•ˆç‡ã€‚åœ¨å„ç§å…·æœ‰ä¸åŒæ•°æ®æ•ˆç‡çš„ä»»åŠ¡ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•ï¼Œåœ¨æ•´ä½“æ•°æ®æ•ˆç‡é¢„æµ‹ä¸­è¾¾åˆ°äº†8.6%çš„è¯¯å·®ï¼Œå¹¶ä¸”é€šå¸¸åœ¨æ¯ä¸ªä»»åŠ¡ä¸Šæ¶ˆé™¤äº†æ•°ç™¾ä¸ªä¸å¿…è¦çš„æ ‡æ³¨ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œç”±äºä»»åŠ¡æ•°æ®æ•ˆç‡æœªçŸ¥è€Œå¯¼è‡´çš„æ ‡æ³¨æˆæœ¬é«˜æ˜‚é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•éœ€è¦é€šè¿‡å¢é‡æ ‡æ³¨å’Œé‡æ–°è®­ç»ƒæ¥ç¡®å®šæœ€ä½³çš„å¾®è°ƒæ•°æ®é‡ï¼Œæ•ˆç‡ä½ä¸‹ä¸”æˆæœ¬å¾ˆé«˜ã€‚å› æ­¤ï¼Œå¦‚ä½•ä»…ä½¿ç”¨å°‘é‡æ•°æ®å°±èƒ½å‡†ç¡®é¢„æµ‹ä»»åŠ¡çš„æ•°æ®æ•ˆç‡æ˜¯å…³é”®æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ¨¡å‹åœ¨å°‘é‡æ•°æ®ä¸Šè®­ç»ƒåçš„æ¢¯åº¦ä¿¡æ¯æ¥é¢„æµ‹æ•°æ®æ•ˆç‡ã€‚å…·ä½“æ¥è¯´ï¼Œä½œè€…è®¤ä¸ºï¼Œå¦‚æœæ¨¡å‹åœ¨å°‘é‡æ•°æ®ä¸Šè®­ç»ƒåï¼Œå¯¹ä½ç½®ä¿¡åº¦æ ·æœ¬çš„æ¢¯åº¦æ–¹å‘ä¸€è‡´æ€§è¾ƒé«˜ï¼Œåˆ™è¡¨æ˜è¯¥ä»»åŠ¡çš„æ•°æ®æ•ˆç‡è¾ƒé«˜ï¼Œåä¹‹åˆ™è¾ƒä½ã€‚è¿™æ˜¯å› ä¸ºæ•°æ®æ•ˆç‡é«˜çš„ä»»åŠ¡ï¼Œæ¨¡å‹æ›´å®¹æ˜“ä»å°‘é‡æ•°æ®ä¸­å­¦ä¹ åˆ°é€šç”¨çš„ç‰¹å¾è¡¨ç¤ºã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) é€‰å–å°‘é‡å·²æ ‡æ³¨çš„æ ·æœ¬ï¼›2) ä½¿ç”¨è¿™äº›æ ·æœ¬å¯¹LLMè¿›è¡Œå¾®è°ƒï¼›3) é€‰æ‹©æ¨¡å‹é¢„æµ‹ç½®ä¿¡åº¦è¾ƒä½çš„æ ·æœ¬ï¼›4) è®¡ç®—è¿™äº›ä½ç½®ä¿¡åº¦æ ·æœ¬çš„æ¢¯åº¦ï¼›5) è®¡ç®—æ¢¯åº¦ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼›6) ä½¿ç”¨æ¢¯åº¦ä½™å¼¦ç›¸ä¼¼åº¦æ¥é¢„æµ‹ä»»åŠ¡çš„æ•°æ®æ•ˆç‡ã€‚æ•´ä½“æµç¨‹ç®€å•é«˜æ•ˆï¼Œæ˜“äºå®ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºåˆ©ç”¨æ¢¯åº¦ä½™å¼¦ç›¸ä¼¼åº¦æ¥è¡¨å¾ä»»åŠ¡çš„æ•°æ®æ•ˆç‡ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºæ€§èƒ½æŒ‡æ ‡ï¼ˆå¦‚å‡†ç¡®ç‡ï¼‰çš„è¯„ä¼°æ–¹æ³•ä¸åŒï¼Œè¯¥æ–¹æ³•ç›´æ¥ä»æ¨¡å‹çš„æ¢¯åº¦ä¿¡æ¯å…¥æ‰‹ï¼Œèƒ½å¤Ÿæ›´æ—©åœ°é¢„æµ‹æ•°æ®æ•ˆç‡ï¼Œä»è€Œé¿å…äº†ä¸å¿…è¦çš„æ ‡æ³¨å’Œè®­ç»ƒã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åªéœ€è¦å°‘é‡æ ‡æ³¨æ•°æ®ï¼Œå³å¯è¿›è¡Œé¢„æµ‹ï¼Œå¤§å¤§é™ä½äº†æ ‡æ³¨æˆæœ¬ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½ç½®ä¿¡åº¦æ ·æœ¬çš„é€‰æ‹©ç­–ç•¥ï¼šä½œè€…å¯èƒ½é‡‡ç”¨é˜ˆå€¼æ³•æˆ–Top-Kæ³•æ¥é€‰æ‹©ç½®ä¿¡åº¦è¾ƒä½çš„æ ·æœ¬ï¼›2) æ¢¯åº¦è®¡ç®—æ–¹å¼ï¼šéœ€è¦æ˜ç¡®æ˜¯è®¡ç®—å“ªä¸ªå±‚æˆ–å“ªäº›å±‚çš„æ¢¯åº¦ï¼Œä»¥åŠå¦‚ä½•å¯¹æ¢¯åº¦è¿›è¡Œå½’ä¸€åŒ–ï¼›3) æ¢¯åº¦ä½™å¼¦ç›¸ä¼¼åº¦çš„è®¡ç®—æ–¹å¼ï¼šéœ€è¦æ˜ç¡®æ˜¯è®¡ç®—æ‰€æœ‰æ¢¯åº¦å¯¹ä¹‹é—´çš„å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œè¿˜æ˜¯é‡‡ç”¨å…¶ä»–ç»Ÿè®¡é‡ï¼›4) æ•°æ®æ•ˆç‡çš„é¢„æµ‹æ¨¡å‹ï¼šå¯èƒ½ä½¿ç”¨çº¿æ€§å›å½’æˆ–æ›´å¤æ‚çš„æ¨¡å‹ï¼Œå°†æ¢¯åº¦ä½™å¼¦ç›¸ä¼¼åº¦æ˜ å°„åˆ°æ•°æ®æ•ˆç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é¢„æµ‹æ•°æ®æ•ˆç‡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œæ•´ä½“æ•°æ®æ•ˆç‡é¢„æµ‹è¯¯å·®ä»…ä¸º8.6%ã€‚ä¸ä¼ ç»Ÿçš„å¢é‡æ ‡æ³¨æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—å‡å°‘ä¸å¿…è¦çš„æ ‡æ³¨ï¼Œå¹³å‡æ¯ä¸ªä»»åŠ¡å¯ä»¥èŠ‚çœæ•°ç™¾ä¸ªæ ‡æ³¨æ ·æœ¬ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å…·æœ‰å¾ˆé«˜çš„å®ç”¨ä»·å€¼ï¼Œå¯ä»¥æœ‰æ•ˆé™ä½LLMå¾®è°ƒçš„æˆæœ¬ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¯¹å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒçš„åœºæ™¯ä¸­ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥å¸®åŠ©ä¼ä¸šæˆ–ç ”ç©¶æœºæ„åœ¨æœ‰é™çš„æ ‡æ³¨é¢„ç®—ä¸‹ï¼Œé€‰æ‹©æœ€å…·æ•°æ®æ•ˆç‡çš„ä»»åŠ¡è¿›è¡Œå¾®è°ƒï¼Œä»è€Œæœ€å¤§åŒ–æ¨¡å‹çš„æ€§èƒ½æå‡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºè‡ªåŠ¨è¯„ä¼°æ•°æ®é›†çš„è´¨é‡ï¼Œå¸®åŠ©ç”¨æˆ·é€‰æ‹©æ›´é€‚åˆæ¨¡å‹è®­ç»ƒçš„æ•°æ®é›†ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> While large language models (LLMs) demonstrate reasonable zero-shot capability across many downstream tasks, fine-tuning is a common practice to improve their performance. However, a task's data efficiency--i.e., the number of fine-tuning examples needed to achieve a desired level of performance--is often unknown, resulting in costly cycles of incremental annotation and retraining. Indeed, we demonstrate across a curated set of 30 specialized tasks that performant LLMs may struggle zero-shot but can attain stronger performance after fine-tuning. This motivates the need for methods to predict a task's data efficiency without requiring incremental annotation. After introducing a concrete metric that quantifies a task's data efficiency, we propose using the gradient cosine similarity of low-confidence examples to predict data efficiency based on a small number of labeled samples. We validate our approach on a diverse set of tasks with varying data efficiencies, attaining 8.6% error in overall data efficiency prediction and typically eliminating hundreds of unnecessary annotations on each task. Our experiment results and implementation code are available on GitHub.

