---
layout: default
title: Sparse Offline Reinforcement Learning with Corruption Robustness
---

# Sparse Offline Reinforcement Learning with Corruption Robustness

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.24768" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.24768v1</a>
  <a href="https://arxiv.org/pdf/2512.24768.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.24768v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.24768v1', 'Sparse Offline Reinforcement Learning with Corruption Robustness')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Nam Phuong Tran, Andi Nika, Goran Radanovic, Long Tran-Thanh, Debmalya Mandal

**åˆ†ç±»**: stat.ML, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-12-31

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºç¨€ç–é²æ£’ä¼°è®¡çš„Actor-Criticç®—æ³•ï¼Œè§£å†³ç¦»çº¿ç¨€ç–RLä¸­çš„æ•°æ®æ±¡æŸ“é—®é¢˜ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `ç¦»çº¿å¼ºåŒ–å­¦ä¹ ` `ç¨€ç–å¼ºåŒ–å­¦ä¹ ` `é²æ£’æ€§` `æ•°æ®æ±¡æŸ“` `Actor-Critic` `é«˜ç»´æ•°æ®` `é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç¦»çº¿RLæ–¹æ³•åœ¨é«˜ç»´ç¨€ç–MDPä¸­ï¼Œé¢å¯¹æ•°æ®æ±¡æŸ“æ—¶ï¼Œé²æ£’æ€§ä¸è¶³ï¼Œéš¾ä»¥ä¿è¯ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚
2. æå‡ºåŸºäºç¨€ç–é²æ£’ä¼°è®¡çš„Actor-Criticç®—æ³•ï¼Œé¿å…ä½¿ç”¨é€ç‚¹æ‚²è§‚å¥–åŠ±ï¼Œä»è€Œæå‡ç®—æ³•çš„é²æ£’æ€§ã€‚
3. ç†è®ºåˆ†æè¡¨æ˜ï¼Œè¯¥ç®—æ³•åœ¨å•ç­–ç•¥é›†ä¸­æ€§è¦†ç›–å’Œæ•°æ®æ±¡æŸ“ä¸‹ï¼Œèƒ½å¤Ÿå­¦ä¹ åˆ°æ¥è¿‘æœ€ä¼˜çš„ç­–ç•¥ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç ”ç©¶äº†ç¦»çº¿ç¨€ç–å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­å¯¹å¼ºæ•°æ®æ±¡æŸ“çš„é²æ£’æ€§ã€‚åœ¨æˆ‘ä»¬çš„è®¾ç½®ä¸­ï¼Œä¸€ä¸ªæ”»å‡»è€…å¯ä»¥ä»»æ„æ‰°åŠ¨æ¥è‡ªé«˜ç»´ä½†ç¨€ç–é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹çš„ä¸€å°éƒ¨åˆ†æ”¶é›†åˆ°çš„è½¨è¿¹ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä¼°è®¡ä¸€ä¸ªæ¥è¿‘æœ€ä¼˜çš„ç­–ç•¥ã€‚ä¸»è¦çš„æŒ‘æˆ˜æ˜¯ï¼Œåœ¨é«˜ç»´æƒ…å†µä¸‹ï¼Œæ ·æœ¬æ•°é‡Nå°äºç‰¹å¾ç»´åº¦dï¼Œåˆ©ç”¨ç¨€ç–æ€§å¯¹äºè·å¾—éå¹³å‡¡çš„ä¿è¯è‡³å…³é‡è¦ï¼Œä½†åœ¨ç¦»çº¿RLä¸­å°šæœªå¾—åˆ°ç³»ç»Ÿç ”ç©¶ã€‚æˆ‘ä»¬åˆ†æäº†å‡åŒ€è¦†ç›–å’Œç¨€ç–å•ç­–ç•¥é›†ä¸­æ€§å‡è®¾ä¸‹çš„é—®é¢˜ã€‚è™½ç„¶æœ€å°äºŒä¹˜å€¼è¿­ä»£ï¼ˆLSVIï¼‰æ˜¯é²æ£’ç¦»çº¿RLçš„æ ‡å‡†æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨å‡åŒ€è¦†ç›–ä¸‹è¡¨ç°è‰¯å¥½ï¼Œä½†æˆ‘ä»¬è¡¨æ˜å°†ç¨€ç–æ€§é›†æˆåˆ°LSVIä¸­æ˜¯ä¸è‡ªç„¶çš„ï¼Œå¹¶ä¸”ç”±äºè¿‡äºæ‚²è§‚çš„å¥–åŠ±ï¼Œå…¶åˆ†æå¯èƒ½ä¼šå´©æºƒã€‚ä¸ºäº†å…‹æœè¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å…·æœ‰ç¨€ç–é²æ£’ä¼°è®¡å™¨oracleçš„actor-criticæ–¹æ³•ï¼Œè¯¥æ–¹æ³•é¿å…äº†é€ç‚¹æ‚²è§‚å¥–åŠ±çš„ä½¿ç”¨ï¼Œå¹¶ä¸ºå•ç­–ç•¥é›†ä¸­æ€§è¦†ç›–ä¸‹çš„ç¨€ç–ç¦»çº¿RLæä¾›äº†ç¬¬ä¸€ä¸ªéå¹³å‡¡çš„ä¿è¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†ç»“æœæ‰©å±•åˆ°å—æ±¡æŸ“çš„ç¯å¢ƒï¼Œå¹¶è¡¨æ˜æˆ‘ä»¬çš„ç®—æ³•åœ¨å¼ºæ±¡æŸ“ä¸‹ä»ç„¶æ˜¯é²æ£’çš„ã€‚æˆ‘ä»¬çš„ç»“æœåœ¨é«˜ç»´ç¨€ç–MDPä¸­æä¾›äº†ç¬¬ä¸€ä¸ªå…·æœ‰å•ç­–ç•¥é›†ä¸­æ€§è¦†ç›–å’Œæ±¡æŸ“çš„éå¹³å‡¡ä¿è¯ï¼Œè¡¨æ˜åœ¨ä¼ ç»Ÿé²æ£’ç¦»çº¿RLæŠ€æœ¯å¯èƒ½å¤±è´¥çš„æƒ…å†µä¸‹ï¼Œå­¦ä¹ æ¥è¿‘æœ€ä¼˜çš„ç­–ç•¥ä»ç„¶æ˜¯å¯èƒ½çš„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³é«˜ç»´ç¨€ç–é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ä¸­ï¼Œç¦»çº¿å¼ºåŒ–å­¦ä¹ ç®—æ³•å¯¹æ•°æ®æ±¡æŸ“çš„é²æ£’æ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚æœ€å°äºŒä¹˜å€¼è¿­ä»£ï¼ˆLSVIï¼‰ï¼Œåœ¨å¤„ç†é«˜ç»´æ•°æ®æ—¶ï¼Œéš¾ä»¥æœ‰æ•ˆåˆ©ç”¨ç¨€ç–æ€§ï¼Œå¹¶ä¸”åœ¨æ•°æ®è¢«æ±¡æŸ“çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½ä¼šæ˜¾è‘—ä¸‹é™ã€‚ä¼ ç»Ÿçš„é²æ£’ç¦»çº¿RLæŠ€æœ¯åœ¨é«˜ç»´ç¨€ç–MDPä¸­å¯èƒ½å¤±æ•ˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯è®¾è®¡ä¸€ç§åŸºäºç¨€ç–é²æ£’ä¼°è®¡çš„Actor-Criticç®—æ³•ï¼Œè¯¥ç®—æ³•é¿å…ä½¿ç”¨é€ç‚¹æ‚²è§‚å¥–åŠ±ï¼Œä»è€Œæé«˜ç®—æ³•å¯¹æ•°æ®æ±¡æŸ“çš„é²æ£’æ€§ã€‚é€šè¿‡åˆ©ç”¨ç¨€ç–æ€§ï¼Œç®—æ³•èƒ½å¤Ÿåœ¨é«˜ç»´ç¯å¢ƒä¸­æ›´æœ‰æ•ˆåœ°å­¦ä¹ ç­–ç•¥ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥ç®—æ³•é‡‡ç”¨Actor-Criticæ¡†æ¶ï¼ŒåŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) Actorç½‘ç»œï¼Œç”¨äºç”Ÿæˆç­–ç•¥ï¼›2) Criticç½‘ç»œï¼Œç”¨äºè¯„ä¼°ç­–ç•¥çš„ä»·å€¼ï¼›3) ç¨€ç–é²æ£’ä¼°è®¡å™¨Oracleï¼Œç”¨äºä¼°è®¡ä»·å€¼å‡½æ•°ï¼Œå¹¶å¯¹æ•°æ®æ±¡æŸ“å…·æœ‰é²æ£’æ€§ã€‚ç®—æ³•é€šè¿‡è¿­ä»£æ›´æ–°Actorå’ŒCriticç½‘ç»œï¼Œæœ€ç»ˆå­¦ä¹ åˆ°ä¸€ä¸ªæ¥è¿‘æœ€ä¼˜çš„ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†ç¨€ç–é²æ£’ä¼°è®¡å™¨Oracleï¼Œè¯¥Oracleèƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨æ•°æ®çš„ç¨€ç–æ€§ï¼Œå¹¶ä¸”å¯¹æ•°æ®æ±¡æŸ“å…·æœ‰é²æ£’æ€§ã€‚ä¸ä¼ ç»Ÿçš„LSVIæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•é¿å…äº†ä½¿ç”¨é€ç‚¹æ‚²è§‚å¥–åŠ±ï¼Œä»è€Œé¿å…äº†è¿‡åº¦æ‚²è§‚çš„ä¼°è®¡ï¼Œæé«˜äº†ç®—æ³•çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­ï¼Œç¨€ç–é²æ£’ä¼°è®¡å™¨Oracleçš„å…·ä½“å®ç°æ–¹å¼æœªçŸ¥ï¼Œä½†å…¶æ ¸å¿ƒåœ¨äºåˆ©ç”¨ç¨€ç–æ€§çº¦æŸï¼Œä¾‹å¦‚L1æ­£åˆ™åŒ–ï¼Œæ¥æé«˜ä¼°è®¡çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚Actorå’ŒCriticç½‘ç»œçš„å…·ä½“ç»“æ„ä¹ŸæœªçŸ¥ï¼Œä½†é€šå¸¸ä¼šé‡‡ç”¨æ·±åº¦ç¥ç»ç½‘ç»œæ¥å®ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡æä¾›äº†ç†è®ºåˆ†æï¼Œè¯æ˜äº†æ‰€æå‡ºçš„ç®—æ³•åœ¨å•ç­–ç•¥é›†ä¸­æ€§è¦†ç›–å’Œæ•°æ®æ±¡æŸ“ä¸‹ï¼Œèƒ½å¤Ÿå­¦ä¹ åˆ°æ¥è¿‘æœ€ä¼˜çš„ç­–ç•¥ã€‚è¯¥ç»“æœåœ¨é«˜ç»´ç¨€ç–MDPä¸­æä¾›äº†ç¬¬ä¸€ä¸ªå…·æœ‰å•ç­–ç•¥é›†ä¸­æ€§è¦†ç›–å’Œæ±¡æŸ“çš„éå¹³å‡¡ä¿è¯ï¼Œè¡¨æ˜åœ¨ä¼ ç»Ÿé²æ£’ç¦»çº¿RLæŠ€æœ¯å¯èƒ½å¤±è´¥çš„æƒ…å†µä¸‹ï¼Œå­¦ä¹ æ¥è¿‘æœ€ä¼˜çš„ç­–ç•¥ä»ç„¶æ˜¯å¯èƒ½çš„ã€‚å…·ä½“çš„å®éªŒç»“æœæœªçŸ¥ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºé«˜ç»´ã€æ•°æ®ç¨€ç–ä¸”æ˜“å—æ±¡æŸ“çš„å¼ºåŒ–å­¦ä¹ åœºæ™¯ï¼Œä¾‹å¦‚æ¨èç³»ç»Ÿã€é‡‘èäº¤æ˜“ã€åŒ»ç–—è¯Šæ–­ç­‰ã€‚åœ¨è¿™äº›åœºæ™¯ä¸­ï¼Œæ•°æ®è´¨é‡éš¾ä»¥ä¿è¯ï¼Œä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•å®¹æ˜“å—åˆ°æ•°æ®æ±¡æŸ“çš„å½±å“ï¼Œè€Œè¯¥ç ”ç©¶æå‡ºçš„ç®—æ³•èƒ½å¤Ÿæé«˜ç­–ç•¥çš„é²æ£’æ€§å’Œå¯é æ€§ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We investigate robustness to strong data corruption in offline sparse reinforcement learning (RL). In our setting, an adversary may arbitrarily perturb a fraction of the collected trajectories from a high-dimensional but sparse Markov decision process, and our goal is to estimate a near optimal policy. The main challenge is that, in the high-dimensional regime where the number of samples $N$ is smaller than the feature dimension $d$, exploiting sparsity is essential for obtaining non-vacuous guarantees but has not been systematically studied in offline RL. We analyse the problem under uniform coverage and sparse single-concentrability assumptions. While Least Square Value Iteration (LSVI), a standard approach for robust offline RL, performs well under uniform coverage, we show that integrating sparsity into LSVI is unnatural, and its analysis may break down due to overly pessimistic bonuses. To overcome this, we propose actor-critic methods with sparse robust estimator oracles, which avoid the use of pointwise pessimistic bonuses and provide the first non-vacuous guarantees for sparse offline RL under single-policy concentrability coverage. Moreover, we extend our results to the contaminated setting and show that our algorithm remains robust under strong contamination. Our results provide the first non-vacuous guarantees in high-dimensional sparse MDPs with single-policy concentrability coverage and corruption, showing that learning a near-optimal policy remains possible in regimes where traditional robust offline RL techniques may fail.

