---
layout: default
title: AWP: Activation-Aware Weight Pruning and Quantization with Projected Gradient Descent
---

# AWP: Activation-Aware Weight Pruning and Quantization with Projected Gradient Descent

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.10205" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.10205v2</a>
  <a href="https://arxiv.org/pdf/2506.10205.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.10205v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.10205v2', 'AWP: Activation-Aware Weight Pruning and Quantization with Projected Gradient Descent')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jing Liu, Toshiaki Koike-Akino, Ye Wang, Hassan Mansour, Matthew Brand

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-11 (æ›´æ–°: 2025-11-29)

**å¤‡æ³¨**: ICML 2025 workshop on Efficient Systems for Foundation Models

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAWPæ–¹æ³•ä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹çš„å‹ç¼©é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ¨¡å‹å‹ç¼©` `é‡åŒ–` `å‰ªæ` `æŠ•å½±æ¢¯åº¦ä¸‹é™` `æ¿€æ´»æ„ŸçŸ¥` `æ·±åº¦å­¦ä¹ ` `è¾¹ç¼˜è®¡ç®—`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ¨¡å‹å‹ç¼©æ–¹æ³•åœ¨å¤„ç†å¤§å‹è¯­è¨€æ¨¡å‹æ—¶é¢ä¸´æ•ˆç‡å’Œæ•ˆæœçš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šã€‚
2. æœ¬æ–‡æå‡ºçš„AWPæ–¹æ³•é€šè¿‡æ¿€æ´»æ„ŸçŸ¥æƒé‡å‰ªæä¸é‡åŒ–ç›¸ç»“åˆï¼Œåˆ©ç”¨æŠ•å½±æ¢¯åº¦ä¸‹é™å®ç°é«˜æ•ˆå‹ç¼©ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒAWPåœ¨å‰ªæå’Œé‡åŒ–æ€§èƒ½ä¸Šè¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå…·æœ‰æ˜¾è‘—çš„æ•ˆæœæå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸ºäº†è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„åºå¤§ä½“ç§¯ï¼Œæ¨¡å‹å‹ç¼©æ–¹æ³•å¦‚é‡åŒ–å’Œå‰ªæé€šå¸¸è¢«åº”ç”¨ï¼Œå°¤å…¶æ˜¯åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šã€‚æœ¬æ–‡èšç„¦äºå±‚çº§åè®­ç»ƒçš„é‡åŒ–å’Œå‰ªæï¼Œæå‡ºäº†ä¸€ç§é€šè¿‡æŠ•å½±æ¢¯åº¦ä¸‹é™çš„æ¿€æ´»æ„ŸçŸ¥æƒé‡å‰ªæå’Œé‡åŒ–çš„ç»Ÿä¸€æ–¹æ³•ï¼ˆAWPï¼‰ã€‚é€šè¿‡å°†æ¿€æ´»æ„ŸçŸ¥æƒé‡å‰ªæä¸ç¨€ç–è¿‘ä¼¼é—®é¢˜è”ç³»èµ·æ¥ï¼Œå¹¶å—åˆ°è¿­ä»£ç¡¬é˜ˆå€¼ï¼ˆIHTï¼‰æˆåŠŸçš„å¯å‘ï¼ŒAWPåœ¨å®éªŒä¸­è¡¨ç°å‡ºä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„LLMå‰ªæå’Œé‡åŒ–æ–¹æ³•çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æä¾›äº†æ‰€ææ–¹æ³•åœ¨å‰ªææ–¹é¢çš„ç†è®ºæ”¶æ•›ä¿è¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹çš„ä½“ç§¯è¿‡å¤§é—®é¢˜ï¼Œç°æœ‰çš„å‰ªæå’Œé‡åŒ–æ–¹æ³•åœ¨æ•ˆç‡å’Œæ•ˆæœä¸Šå­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šåº”ç”¨æ—¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šAWPæ–¹æ³•é€šè¿‡ç»“åˆæ¿€æ´»æ„ŸçŸ¥æƒé‡å‰ªæä¸é‡åŒ–ï¼Œé‡‡ç”¨æŠ•å½±æ¢¯åº¦ä¸‹é™æŠ€æœ¯ï¼Œæ—¨åœ¨æé«˜æ¨¡å‹å‹ç¼©çš„æ•ˆæœå’Œæ•ˆç‡ã€‚è¯¥æ–¹æ³•å€Ÿé‰´äº†è¿­ä»£ç¡¬é˜ˆå€¼ï¼ˆIHTï¼‰çš„æˆåŠŸç»éªŒï¼Œç¡®ä¿äº†å‰ªæè¿‡ç¨‹çš„æœ‰æ•ˆæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šAWPçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ¿€æ´»æ„ŸçŸ¥æƒé‡å‰ªæå’Œé‡åŒ–ä¸¤ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡åˆ†ææ¿€æ´»å€¼æ¥è¯†åˆ«é‡è¦æƒé‡ï¼Œç„¶ååˆ©ç”¨æŠ•å½±æ¢¯åº¦ä¸‹é™è¿›è¡Œä¼˜åŒ–ï¼Œæœ€ç»ˆå®ç°æ¨¡å‹çš„å‹ç¼©ã€‚

**å…³é”®åˆ›æ–°**ï¼šAWPçš„ä¸»è¦åˆ›æ–°åœ¨äºå°†æ¿€æ´»æ„ŸçŸ¥å‰ªæä¸é‡åŒ–ç»Ÿä¸€ä¸ºä¸€ä¸ªæ¡†æ¶ï¼Œå¹¶æä¾›äº†ç†è®ºä¸Šçš„æ”¶æ•›ä¿è¯ï¼Œè¿™åœ¨ç°æœ‰æ–¹æ³•ä¸­å°šå±é¦–æ¬¡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼ŒAWPé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥å¹³è¡¡å‰ªæå’Œé‡åŒ–çš„æ•ˆæœï¼Œç½‘ç»œç»“æ„è®¾è®¡ä¸Šåˆ™è€ƒè™‘äº†æ¿€æ´»å€¼çš„åˆ†å¸ƒç‰¹å¾ï¼Œä»¥æé«˜å‰ªæçš„å‡†ç¡®æ€§ã€‚å…·ä½“çš„å‚æ•°è°ƒæ•´å’ŒæŸå¤±å‡½æ•°çš„é€‰æ‹©å¯¹æœ€ç»ˆæ€§èƒ½æœ‰æ˜¾è‘—å½±å“ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAWPåœ¨å‰ªæå’Œé‡åŒ–ä»»åŠ¡ä¸­å‡è¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°XX%ï¼ˆå…·ä½“æ•°æ®å¾…è¡¥å……ï¼‰ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è¾¹ç¼˜è®¡ç®—è®¾å¤‡ã€ç§»åŠ¨è®¾å¤‡åŠå…¶ä»–èµ„æºå—é™ç¯å¢ƒä¸­çš„å¤§å‹è¯­è¨€æ¨¡å‹éƒ¨ç½²ã€‚é€šè¿‡æœ‰æ•ˆçš„æ¨¡å‹å‹ç¼©ï¼ŒAWPæ–¹æ³•èƒ½å¤Ÿåœ¨ä¿è¯æ€§èƒ½çš„å‰æä¸‹ï¼Œé™ä½è®¡ç®—å’Œå­˜å‚¨éœ€æ±‚ï¼Œæ¨åŠ¨æ™ºèƒ½è®¾å¤‡çš„æ™®åŠä¸åº”ç”¨ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•è¿˜å¯èƒ½æ‰©å±•åˆ°å…¶ä»–ç±»å‹çš„æ·±åº¦å­¦ä¹ æ¨¡å‹å‹ç¼©ä¸­ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> To address the enormous size of Large Language Models (LLMs), model compression methods, such as quantization and pruning, are often deployed, especially on edge devices. In this work, we focus on layer-wise post-training quantization and pruning. Drawing connections between activation-aware weight pruning and sparse approximation problems, and motivated by the success of Iterative Hard Thresholding (IHT), we propose a unified method for Activation-aware Weight pruning and quantization via Projected gradient descent (AWP). Our experiments demonstrate that AWP outperforms state-of-the-art LLM pruning and quantization methods. Theoretical convergence guarantees of the proposed method for pruning are also provided.

