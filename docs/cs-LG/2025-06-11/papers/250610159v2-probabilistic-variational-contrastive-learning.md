---
layout: default
title: Probabilistic Variational Contrastive Learning
---

# Probabilistic Variational Contrastive Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.10159" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.10159v2</a>
  <a href="https://arxiv.org/pdf/2506.10159.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.10159v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.10159v2', 'Probabilistic Variational Contrastive Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Minoh Jeong, Seonho Kim, Alfred Hero

**åˆ†ç±»**: cs.LG, stat.ML

**å‘å¸ƒæ—¥æœŸ**: 2025-06-11 (æ›´æ–°: 2025-10-06)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå˜åˆ†å¯¹æ¯”å­¦ä¹ ä»¥è§£å†³ä¸ç¡®å®šæ€§é‡åŒ–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¯¹æ¯”å­¦ä¹ ` `å˜åˆ†æ¨æ–­` `ä¸ç¡®å®šæ€§é‡åŒ–` `æ·±åº¦å­¦ä¹ ` `åµŒå…¥å­¦ä¹ ` `ä¿¡æ¯è®º` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¯¹æ¯”å­¦ä¹ æ–¹æ³•è™½ç„¶æ€§èƒ½ä¼˜è¶Šï¼Œä½†ç¼ºä¹å¯¹åµŒå…¥ä¸ç¡®å®šæ€§çš„é‡åŒ–æœºåˆ¶ï¼Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºçš„å˜åˆ†å¯¹æ¯”å­¦ä¹ ï¼ˆVCLï¼‰é€šè¿‡å°†InfoNCEæŸå¤±è§†ä¸ºé‡æ„é¡¹ï¼Œå¹¶å¼•å…¥KLæ•£åº¦æ­£åˆ™åŒ–ï¼Œæä¾›äº†ä¸€ç§æ–°çš„æ¦‚ç‡åµŒå…¥ç”Ÿæˆæ–¹å¼ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVCLåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æœ‰æ•ˆå‡è½»äº†ç»´åº¦å´©æºƒï¼Œå¹¶åœ¨åˆ†ç±»å‡†ç¡®æ€§ä¸Šè¶…è¶Šäº†ä¼ ç»Ÿçš„ç¡®å®šæ€§æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°æœ‰çš„å¯¹æ¯”å­¦ä¹ æ–¹æ³•å¦‚SimCLRå’ŒSupConé€šè¿‡ç¡®å®šæ€§åµŒå…¥å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä½†ç¼ºä¹å¯¹ä¸ç¡®å®šæ€§çš„é‡åŒ–æœºåˆ¶ã€‚æœ¬æ–‡æå‡ºäº†å˜åˆ†å¯¹æ¯”å­¦ä¹ ï¼ˆVCLï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ— è§£ç å™¨çš„æ¡†æ¶ï¼Œé€šè¿‡å°†InfoNCEæŸå¤±è§†ä¸ºæ›¿ä»£é‡æ„é¡¹ï¼Œå¹¶åœ¨å•ä½è¶…çƒé¢ä¸Šæ·»åŠ KLæ•£åº¦æ­£åˆ™åŒ–é¡¹ï¼Œä»è€Œæœ€å¤§åŒ–è¯æ®ä¸‹ç•Œï¼ˆELBOï¼‰ã€‚æˆ‘ä»¬å°†è¿‘ä¼¼åéªŒ$q_Î¸(z\|x)$å»ºæ¨¡ä¸ºæŠ•å½±æ­£æ€åˆ†å¸ƒï¼Œä½¿å¾—å¯ä»¥å¯¹æ¦‚ç‡åµŒå…¥è¿›è¡Œé‡‡æ ·ã€‚æˆ‘ä»¬çš„ä¸¤ä¸ªå®ä¾‹åŒ–æ–¹æ³•VSimCLRå’ŒVSupConç”¨æ¥è‡ª$q_Î¸(z\|x)$çš„æ ·æœ¬æ›¿ä»£ç¡®å®šæ€§åµŒå…¥ï¼Œå¹¶åœ¨æŸå¤±ä¸­åŠ å…¥å½’ä¸€åŒ–çš„KLé¡¹ã€‚å®éªŒè¡¨æ˜ï¼ŒVCLå‡è½»äº†ç»´åº¦å´©æºƒï¼Œå¢å¼ºäº†ä¸ç±»åˆ«æ ‡ç­¾çš„äº’ä¿¡æ¯ï¼Œå¹¶åœ¨åˆ†ç±»å‡†ç¡®æ€§ä¸Šä¸ç¡®å®šæ€§åŸºçº¿ç›¸åŒ¹é…æˆ–è¶…è¶Šï¼ŒåŒæ—¶é€šè¿‡åéªŒæ¨¡å‹æä¾›æœ‰æ„ä¹‰çš„ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚VCLä¸ºå¯¹æ¯”å­¦ä¹ æä¾›äº†æ¦‚ç‡åŸºç¡€ï¼Œæˆä¸ºå¯¹æ¯”æ–¹æ³•çš„æ–°åŸºç¡€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¯¹æ¯”å­¦ä¹ æ–¹æ³•å¦‚SimCLRå’ŒSupConè™½ç„¶åœ¨æ€§èƒ½ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†å®ƒä»¬ç”Ÿæˆçš„åµŒå…¥æ˜¯ç¡®å®šæ€§çš„ï¼Œç¼ºä¹å¯¹ä¸ç¡®å®šæ€§çš„é‡åŒ–ï¼Œå¯¼è‡´åœ¨æŸäº›åº”ç”¨åœºæ™¯ä¸­æ— æ³•æœ‰æ•ˆè¯„ä¼°æ¨¡å‹çš„å¯é æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡æå‡ºçš„å˜åˆ†å¯¹æ¯”å­¦ä¹ ï¼ˆVCLï¼‰é€šè¿‡å°†InfoNCEæŸå¤±è§†ä¸ºé‡æ„é¡¹ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šå¼•å…¥KLæ•£åº¦æ­£åˆ™åŒ–ï¼Œæ—¨åœ¨ç”Ÿæˆæ¦‚ç‡åµŒå…¥ï¼Œä»è€Œä¸ºå¯¹æ¯”å­¦ä¹ æä¾›ä¸ç¡®å®šæ€§é‡åŒ–çš„èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVCLçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œä½¿ç”¨æŠ•å½±æ­£æ€åˆ†å¸ƒå»ºæ¨¡è¿‘ä¼¼åéªŒ$q_Î¸(z|x)$ï¼Œç„¶åé€šè¿‡é‡‡æ ·ç”Ÿæˆæ¦‚ç‡åµŒå…¥ã€‚å…¶æ¬¡ï¼Œåœ¨æŸå¤±å‡½æ•°ä¸­åŠ å…¥å½’ä¸€åŒ–çš„KLæ•£åº¦é¡¹ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„å­¦ä¹ æ•ˆæœã€‚

**å…³é”®åˆ›æ–°**ï¼šVCLçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†å¯¹æ¯”å­¦ä¹ ä¸å˜åˆ†æ¨æ–­ç»“åˆï¼Œå½¢æˆäº†ä¸€ç§æ–°çš„æ¦‚ç‡åµŒå…¥ç”Ÿæˆæœºåˆ¶ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„ç¡®å®šæ€§åµŒå…¥æ–¹æ³•æœ¬è´¨ä¸Šä¸åŒï¼Œèƒ½å¤Ÿæä¾›æ›´ä¸°å¯Œçš„ä¿¡æ¯ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŸå¤±å‡½æ•°è®¾è®¡ä¸Šï¼ŒVCLå¼•å…¥äº†KLæ•£åº¦æ­£åˆ™åŒ–é¡¹ï¼Œå¹¶é€šè¿‡æ ·æœ¬æ›¿ä»£ç¡®å®šæ€§åµŒå…¥ï¼Œç¡®ä¿ç”Ÿæˆçš„åµŒå…¥å…·æœ‰è‰¯å¥½çš„åˆ†å¸ƒç‰¹æ€§ã€‚æ­¤å¤–ï¼Œæ¨¡å‹çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„è®¾è®¡ä¹Ÿç»è¿‡ç²¾å¿ƒè°ƒæ•´ï¼Œä»¥ä¼˜åŒ–æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒVCLæ˜¾è‘—å‡è½»äº†ç»´åº¦å´©æºƒç°è±¡ï¼Œå¢å¼ºäº†ä¸ç±»åˆ«æ ‡ç­¾çš„äº’ä¿¡æ¯ï¼Œå¹¶åœ¨åˆ†ç±»å‡†ç¡®æ€§ä¸Šä¸ç¡®å®šæ€§åŸºçº¿ç›¸åŒ¹é…æˆ–è¶…è¶Šï¼Œå±•ç¤ºäº†å…¶åœ¨ä¸ç¡®å®šæ€§ä¼°è®¡æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚å…·ä½“å®éªŒç»“æœè¡¨æ˜ï¼ŒVCLåœ¨åˆ†ç±»ä»»åŠ¡ä¸­æé«˜äº†å‡†ç¡®ç‡ï¼Œä¸”æä¾›äº†æœ‰æ„ä¹‰çš„ä¸ç¡®å®šæ€§è¯„ä¼°ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

å˜åˆ†å¯¹æ¯”å­¦ä¹ ï¼ˆVCLï¼‰åœ¨å›¾åƒåˆ†ç±»ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æä¾›ä¸ç¡®å®šæ€§é‡åŒ–ï¼ŒVCLèƒ½å¤Ÿåœ¨éœ€è¦é«˜å¯é æ€§çš„ä»»åŠ¡ä¸­ï¼Œå¦‚åŒ»ç–—å½±åƒåˆ†æå’Œè‡ªåŠ¨é©¾é©¶ï¼Œæå‡æ¨¡å‹çš„å¯ä¿¡åº¦å’Œå®‰å…¨æ€§ã€‚æœªæ¥ï¼ŒVCLå¯èƒ½æˆä¸ºå¯¹æ¯”å­¦ä¹ æ–¹æ³•çš„æ ‡å‡†é€‰æ‹©ï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Deterministic embeddings learned by contrastive learning (CL) methods such as SimCLR and SupCon achieve state-of-the-art performance but lack a principled mechanism for uncertainty quantification. We propose Variational Contrastive Learning (VCL), a decoder-free framework that maximizes the evidence lower bound (ELBO) by interpreting the InfoNCE loss as a surrogate reconstruction term and adding a KL divergence regularizer to a uniform prior on the unit hypersphere. We model the approximate posterior $q_Î¸(z\|x)$ as a projected normal distribution, enabling the sampling of probabilistic embeddings. Our two instantiation--VSimCLR and VSupCon--replace deterministic embeddings with samples from $q_Î¸(z\|x)$ and incorporate a normalized KL term into the loss. Experiments on multiple benchmarks demonstrate that VCL mitigates dimensional collapse, enhances mutual information with class labels, and matches or outperforms deterministic baselines in classification accuracy, all the while providing meaningful uncertainty estimates through the posterior model. VCL thus equips contrastive learning with a probabilistic foundation, serving as a new basis for contrastive approaches.

