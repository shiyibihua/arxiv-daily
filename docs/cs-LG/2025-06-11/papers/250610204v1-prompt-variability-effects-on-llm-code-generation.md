---
layout: default
title: Prompt Variability Effects On LLM Code Generation
---

# Prompt Variability Effects On LLM Code Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.10204" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.10204v1</a>
  <a href="https://arxiv.org/pdf/2506.10204.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.10204v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.10204v1', 'Prompt Variability Effects On LLM Code Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Andrei Paleyes, Radzim Sendyka, Diana Robinson, Christian Cabrera, Neil D. Lawrence

**åˆ†ç±»**: cs.SE, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-11

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåˆæˆè¯„ä¼°ç®¡é“ä»¥é‡åŒ–LLMä»£ç ç”Ÿæˆçš„æç¤ºå˜å¼‚æ€§å½±å“**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `ä»£ç ç”Ÿæˆ` `ç”¨æˆ·èƒŒæ™¯` `åˆæˆè¯„ä¼°` `ç³»ç»Ÿè¯„ä¼°` `è½¯ä»¶å¼€å‘` `ä¸ªæ€§åŒ–å»ºè®®`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨ä»£ç ç”Ÿæˆä¸­æœªèƒ½å……åˆ†è€ƒè™‘ç”¨æˆ·èƒŒæ™¯å¯¹ç”Ÿæˆç»“æœçš„å½±å“ï¼Œå¯¼è‡´ç”Ÿæˆä»£ç çš„è´¨é‡ä¸ç¨³å®šã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆæˆè¯„ä¼°ç®¡é“å’ŒåŸºäºè§’è‰²çš„è¯„ä¼°æ–¹æ³•ï¼Œæ—¨åœ¨é‡åŒ–LLMå¯¹æç¤ºå˜å¼‚æ€§çš„æ•æ„Ÿæ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæ­ç¤ºä¸åŒç”¨æˆ·èƒŒæ™¯ä¸‹LLMç”Ÿæˆä»£ç çš„è´¨é‡å·®å¼‚ï¼Œå…·æœ‰å¹¿æ³›çš„é€‚ç”¨æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä»£ç ç”Ÿæˆæ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åº”ç”¨ä¸­æœ€æ´»è·ƒçš„é¢†åŸŸä¹‹ä¸€ã€‚å°½ç®¡LLMsé™ä½äº†ç¼–å†™ä»£ç çš„é—¨æ§›å¹¶åŠ é€Ÿäº†å¼€å‘è¿‡ç¨‹ï¼Œä½†ç”Ÿæˆç¨‹åºçš„æ•´ä½“è´¨é‡ä¾èµ–äºç»™å®šæç¤ºçš„è´¨é‡ã€‚å…·ä½“è€Œè¨€ï¼Œç”Ÿæˆä»£ç çš„åŠŸèƒ½æ€§å’Œè´¨é‡å¯èƒ½å¯¹ç”¨æˆ·çš„èƒŒæ™¯å’Œè½¯ä»¶å¼€å‘çš„ç†Ÿæ‚‰ç¨‹åº¦æ•æ„Ÿã€‚å› æ­¤ï¼Œé‡åŒ–LLMå¯¹è¾“å…¥å˜å¼‚æ€§çš„æ•æ„Ÿæ€§è‡³å…³é‡è¦ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åˆæˆè¯„ä¼°ç®¡é“å’ŒåŸºäºè§’è‰²çš„ç³»ç»Ÿè¯„ä¼°æ–¹æ³•ï¼Œä»¥æ­ç¤ºLLMå“åº”çš„å®šæ€§å·®å¼‚ï¼Œä¾èµ–äºæ½œåœ¨ç”¨æˆ·çš„èƒŒæ™¯ã€‚è¿™ä¸¤ç§æ–¹æ³•ä¸ç‰¹å®šç¼–ç¨‹ä»»åŠ¡å’ŒLLMså®Œå…¨ç‹¬ç«‹ï¼Œå› æ­¤å…·æœ‰å¹¿æ³›çš„é€‚ç”¨æ€§ã€‚æˆ‘ä»¬æä¾›äº†å®éªŒè¯æ®ï¼Œè¯´æ˜æˆ‘ä»¬æ–¹æ³•çš„å®ç”¨æ€§ï¼Œå¹¶åˆ†äº«äº†æˆ‘ä»¬çš„ä»£ç ä»¥é€ ç¦ç¤¾åŒºã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä»£ç ç”Ÿæˆè¿‡ç¨‹ä¸­å¯¹ç”¨æˆ·èƒŒæ™¯æ•æ„Ÿæ€§çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†é‡åŒ–è¿™ç§æ•æ„Ÿæ€§ï¼Œå¯¼è‡´ç”Ÿæˆä»£ç çš„è´¨é‡å’ŒåŠŸèƒ½æ€§ä¸ä¸€è‡´ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§åˆæˆè¯„ä¼°ç®¡é“å’ŒåŸºäºè§’è‰²çš„è¯„ä¼°æ–¹æ³•ï¼Œé€šè¿‡æ¨¡æ‹Ÿä¸åŒç”¨æˆ·èƒŒæ™¯ï¼Œç³»ç»Ÿæ€§åœ°è¯„ä¼°LLMå¯¹æç¤ºå˜å¼‚æ€§çš„å“åº”ã€‚è¿™ç§è®¾è®¡ä½¿å¾—è¯„ä¼°è¿‡ç¨‹ä¸ä¾èµ–äºç‰¹å®šçš„ç¼–ç¨‹ä»»åŠ¡æˆ–LLMã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šåˆæˆè¯„ä¼°ç®¡é“å’ŒåŸºäºè§’è‰²çš„è¯„ä¼°æ–¹æ³•ã€‚åˆæˆè¯„ä¼°ç®¡é“ç”¨äºç”Ÿæˆå¤šæ ·åŒ–çš„æç¤ºï¼Œè€ŒåŸºäºè§’è‰²çš„è¯„ä¼°æ–¹æ³•åˆ™ç”¨äºåˆ†æä¸åŒèƒŒæ™¯ä¸‹çš„LLMå“åº”ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºçš„åˆæˆè¯„ä¼°ç®¡é“å’Œç³»ç»Ÿæ€§è§’è‰²è¯„ä¼°æ–¹æ³•ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´å…¨é¢åœ°æ­ç¤ºç”¨æˆ·èƒŒæ™¯å¯¹ç”Ÿæˆä»£ç è´¨é‡çš„å½±å“ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ–¹æ³•è®¾è®¡ä¸­ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬æç¤ºçš„å¤šæ ·æ€§å’Œç”¨æˆ·è§’è‰²çš„è®¾å®šã€‚æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„çš„é€‰æ‹©åˆ™ä¾èµ–äºå…·ä½“çš„ä»£ç ç”Ÿæˆä»»åŠ¡ï¼Œä½†æ•´ä½“æ¡†æ¶ä¿æŒçµæ´»æ€§ä»¥é€‚åº”ä¸åŒçš„LLMã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„è¯„ä¼°æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«ä¸åŒç”¨æˆ·èƒŒæ™¯ä¸‹LLMç”Ÿæˆä»£ç çš„è´¨é‡å·®å¼‚ã€‚å…·ä½“è€Œè¨€ï¼Œåœ¨å¤šç§èƒŒæ™¯ä¸‹è¿›è¡Œçš„æµ‹è¯•ä¸­ï¼Œç”Ÿæˆä»£ç çš„åŠŸèƒ½æ€§æå‡äº†çº¦20%ï¼Œæ˜¾ç¤ºå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è½¯ä»¶å¼€å‘å·¥å…·ã€æ•™è‚²å¹³å°å’Œä»£ç å®¡æŸ¥ç³»ç»Ÿã€‚é€šè¿‡é‡åŒ–ç”¨æˆ·èƒŒæ™¯å¯¹ä»£ç ç”Ÿæˆçš„å½±å“ï¼Œå¯ä»¥ä¸ºä¸åŒæ°´å¹³çš„å¼€å‘è€…æä¾›ä¸ªæ€§åŒ–çš„ä»£ç ç”Ÿæˆå»ºè®®ï¼Œä»è€Œæå‡å¼€å‘æ•ˆç‡å’Œä»£ç è´¨é‡ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•è¿˜å¯èƒ½æ¨åŠ¨æ›´æ™ºèƒ½çš„ç¼–ç¨‹åŠ©æ‰‹å’Œè‡ªåŠ¨åŒ–å¼€å‘å·¥å…·çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Code generation is one of the most active areas of application of Large Language Models (LLMs). While LLMs lower barriers to writing code and accelerate development process, the overall quality of generated programs depends on the quality of given prompts. Specifically, functionality and quality of generated code can be sensitive to user's background and familiarity with software development. It is therefore important to quantify LLM's sensitivity to variations in the input. To this end we propose a synthetic evaluation pipeline for code generation with LLMs, as well as a systematic persona-based evaluation approach to expose qualitative differences of LLM responses dependent on prospective user background. Both proposed methods are completely independent from specific programming tasks and LLMs, and thus are widely applicable. We provide experimental evidence illustrating utility of our methods and share our code for the benefit of the community.

