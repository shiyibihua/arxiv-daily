---
layout: default
title: Omni-DPO: A Dual-Perspective Paradigm for Dynamic Preference Learning of LLMs
---

# Omni-DPO: A Dual-Perspective Paradigm for Dynamic Preference Learning of LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.10054" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.10054v2</a>
  <a href="https://arxiv.org/pdf/2506.10054.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.10054v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.10054v2', 'Omni-DPO: A Dual-Perspective Paradigm for Dynamic Preference Learning of LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shangpin Peng, Weinong Wang, Zhuotao Tian, Senqiao Yang, Xing Wu, Haotian Xu, Chengquan Zhang, Takashi Isobe, Baotian Hu, Min Zhang

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-11 (æ›´æ–°: 2025-08-15)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/pspdada/Omni-DPO)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOmni-DPOä»¥è§£å†³åŠ¨æ€åå¥½å­¦ä¹ ä¸­çš„æ•°æ®åˆ©ç”¨é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `åŠ¨æ€åå¥½å­¦ä¹ ` `ç›´æ¥åå¥½ä¼˜åŒ–` `äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ` `æ¨¡å‹æ€§èƒ½ä¼˜åŒ–` `è‡ªé€‚åº”åŠ æƒ` `æ–‡æœ¬ç†è§£` `æ•°å­¦æ¨ç†` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰DPOæ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨åå¥½å¯¹çš„å†…åœ¨è´¨é‡å’Œå­¦ä¹ æ•ˆç”¨ï¼Œå¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚
2. Omni-DPOé€šè¿‡åŒè§†è§’ä¼˜åŒ–æ¡†æ¶ï¼Œç»“åˆåå¥½å¯¹çš„è´¨é‡å’Œæ¨¡å‹çš„å­¦ä¹ åŠ¨æ€ï¼Œæå‡æ•°æ®åˆ©ç”¨æ•ˆç‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒOmni-DPOåœ¨æ–‡æœ¬ç†è§£å’Œæ•°å­¦æ¨ç†ä»»åŠ¡ä¸­å‡æ˜¾è‘—è¶…è¶ŠåŸºçº¿æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å› å…¶ç®€å•é«˜æ•ˆæˆä¸ºäººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰çš„åŸºçŸ³ã€‚ç„¶è€Œï¼Œç°æœ‰DPOæ–¹æ³•é€šå¸¸å‡åŒ€å¯¹å¾…æ‰€æœ‰åå¥½å¯¹ï¼Œå¿½è§†å…¶å†…åœ¨è´¨é‡å’Œå­¦ä¹ æ•ˆç”¨çš„å…³é”®å·®å¼‚ï¼Œå¯¼è‡´æ•°æ®åˆ©ç”¨å’Œæ€§èƒ½çš„æ¬¡ä¼˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºOmni-DPOï¼Œä¸€ä¸ªåŒè§†è§’ä¼˜åŒ–æ¡†æ¶ï¼Œè”åˆè€ƒè™‘æ¯ä¸ªåå¥½å¯¹çš„å†…åœ¨è´¨é‡å’Œæ¨¡å‹åœ¨è¿™äº›åå¥½å¯¹ä¸Šçš„æ¼”å˜æ€§èƒ½ã€‚é€šè¿‡æ ¹æ®æ•°æ®è´¨é‡å’Œæ¨¡å‹å­¦ä¹ åŠ¨æ€è‡ªé€‚åº”åŠ æƒæ ·æœ¬ï¼ŒOmni-DPOå®ç°äº†æ›´æœ‰æ•ˆçš„è®­ç»ƒæ•°æ®åˆ©ç”¨ï¼Œå¹¶å–å¾—äº†æ›´å¥½çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOmni-DPOåœ¨å¤šä¸ªæ¨¡å‹å’ŒåŸºå‡†ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œå°¤å…¶åœ¨æ–‡æœ¬ç†è§£ä»»åŠ¡ä¸­ï¼Œä½¿ç”¨Omni-DPOå¾®è°ƒçš„Gemma-2-9b-itåœ¨Arena-HardåŸºå‡†ä¸Šæ¯”é¢†å…ˆçš„LLM Claude 3 Opusé«˜å‡º6.7åˆ†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰DPOæ–¹æ³•åœ¨å¤„ç†åå¥½å¯¹æ—¶çš„å‡åŒ€å¯¹å¾…é—®é¢˜ï¼Œå¯¼è‡´æ•°æ®åˆ©ç”¨ä¸å……åˆ†å’Œæ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šOmni-DPOé€šè¿‡åŒè§†è§’æ¡†æ¶ï¼Œåˆ†åˆ«è€ƒè™‘åå¥½å¯¹çš„å†…åœ¨è´¨é‡å’Œæ¨¡å‹çš„å­¦ä¹ åŠ¨æ€ï¼Œé‡‡ç”¨è‡ªé€‚åº”åŠ æƒç­–ç•¥æ¥ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šOmni-DPOçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®è´¨é‡è¯„ä¼°æ¨¡å—å’Œæ¨¡å‹æ€§èƒ½ç›‘æ§æ¨¡å—ï¼ŒäºŒè€…å…±åŒå½±å“æ ·æœ¬çš„åŠ æƒç­–ç•¥ï¼Œä»è€Œä¼˜åŒ–è®­ç»ƒæ•°æ®çš„é€‰æ‹©å’Œä½¿ç”¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šOmni-DPOçš„åˆ›æ–°ä¹‹å¤„åœ¨äºå…¶åŒè§†è§’ä¼˜åŒ–ç­–ç•¥ï¼ŒåŒºåˆ«äºä¼ ç»Ÿæ–¹æ³•çš„å‡åŒ€å¤„ç†æ–¹å¼ï¼Œä½¿å¾—è®­ç»ƒè¿‡ç¨‹æ›´åŠ é«˜æ•ˆå’Œçµæ´»ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼ŒOmni-DPOå¼•å…¥äº†åŠ¨æ€åŠ æƒæœºåˆ¶ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡è€ƒè™‘äº†æ ·æœ¬çš„è´¨é‡å’Œæ¨¡å‹çš„å­¦ä¹ è¿›åº¦ï¼Œç¡®ä¿è®­ç»ƒè¿‡ç¨‹çš„é€‚åº”æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å®éªŒä¸­ï¼Œä½¿ç”¨Omni-DPOå¾®è°ƒçš„Gemma-2-9b-itåœ¨Arena-HardåŸºå‡†ä¸Šæ¯”Claude 3 Opusé«˜å‡º6.7åˆ†ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚æ­¤å¤–ï¼Œåœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­ï¼ŒOmni-DPOåœ¨æ‰€æœ‰åŸºå‡†ä¸Šå‡è¶…è¶Šäº†åŸºçº¿æ–¹æ³•ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Omni-DPOçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦ä»äººç±»åé¦ˆä¸­å­¦ä¹ çš„ä»»åŠ¡ä¸­ï¼Œå¦‚å¯¹è¯ç³»ç»Ÿã€æ¨èç³»ç»Ÿå’Œè‡ªåŠ¨å†…å®¹ç”Ÿæˆç­‰ã€‚é€šè¿‡æå‡æ¨¡å‹å¯¹åå¥½æ•°æ®çš„åˆ©ç”¨æ•ˆç‡ï¼Œæœªæ¥å¯ä»¥å®ç°æ›´æ™ºèƒ½çš„äº¤äº’å’Œæ›´é«˜è´¨é‡çš„ç”Ÿæˆå†…å®¹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Direct Preference Optimization (DPO) has become a cornerstone of reinforcement learning from human feedback (RLHF) due to its simplicity and efficiency. However, existing DPO-based approaches typically treat all preference pairs uniformly, ignoring critical variations in their inherent quality and learning utility, leading to suboptimal data utilization and performance. To address this challenge, we propose Omni-DPO, a dual-perspective optimization framework that jointly accounts for (1) the inherent quality of each preference pair and (2) the model's evolving performance on those pairs. By adaptively weighting samples according to both data quality and the model's learning dynamics during training, Omni-DPO enables more effective training data utilization and achieves better performance. Experimental results on various models and benchmarks demonstrate the superiority and generalization capabilities of Omni-DPO. On textual understanding tasks, Gemma-2-9b-it finetuned with Omni-DPO beats the leading LLM, Claude 3 Opus, by a significant margin of 6.7 points on the Arena-Hard benchmark. On mathematical reasoning tasks, Omni-DPO consistently outperforms the baseline methods across all benchmarks, providing strong empirical evidence for the effectiveness and robustness of our approach. Code and models will be available at https://github.com/pspdada/Omni-DPO.

