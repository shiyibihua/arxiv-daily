---
layout: default
title: Provable Sim-to-Real Transfer via Offline Domain Randomization
---

# Provable Sim-to-Real Transfer via Offline Domain Randomization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.10133" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.10133v1</a>
  <a href="https://arxiv.org/pdf/2506.10133.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.10133v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.10133v1', 'Provable Sim-to-Real Transfer via Offline Domain Randomization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Arnaud Fickinger, Abderrahim Bendahi, Stuart Russell

**åˆ†ç±»**: cs.LG, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-06-11

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç¦»çº¿é¢†åŸŸéšæœºåŒ–ä»¥è§£å†³ä»¿çœŸåˆ°ç°å®è½¬ç§»é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é¢†åŸŸéšæœºåŒ–` `ç¦»çº¿å­¦ä¹ ` `å¼ºåŒ–å­¦ä¹ ` `ä»¿çœŸåˆ°ç°å®` `æœ€å¤§ä¼¼ç„¶ä¼°è®¡` `æœºå™¨äººæ§åˆ¶` `ç­–ç•¥ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é¢†åŸŸéšæœºåŒ–æ–¹æ³•åœ¨åˆ©ç”¨ç¦»çº¿æ•°æ®æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå¯¼è‡´ä»¿çœŸåˆ°ç°å®çš„è½¬ç§»æ•ˆæœä¸ä½³ã€‚
2. æœ¬æ–‡æå‡ºäº†ç¦»çº¿é¢†åŸŸéšæœºåŒ–ï¼ˆODRï¼‰ï¼Œé€šè¿‡æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ‹Ÿåˆä»¿çœŸå™¨å‚æ•°åˆ†å¸ƒï¼Œä»¥æ›´å¥½åœ°åˆ©ç”¨å·²æœ‰çš„ç¦»çº¿æ•°æ®ã€‚
3. ODRåœ¨ç†è®ºä¸Šè¯æ˜äº†å…¶ä¸€è‡´æ€§ï¼Œå¹¶åœ¨å®éªŒä¸­æ˜¾ç¤ºå‡ºç›¸è¾ƒäºä¼ ç»Ÿé¢†åŸŸéšæœºåŒ–æ˜¾è‘—é™ä½äº†ä»¿çœŸåˆ°ç°å®çš„è¯¯å·®ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼ºåŒ–å­¦ä¹ ä»£ç†åœ¨ä»ä»¿çœŸéƒ¨ç½²åˆ°ç°å®ä¸–ç•Œæ—¶å¸¸å¸¸é¢ä¸´å›°éš¾ã€‚é¢†åŸŸéšæœºåŒ–ï¼ˆDRï¼‰æ˜¯å‡å°‘ä»¿çœŸä¸ç°å®ä¹‹é—´å·®è·çš„ä¸»è¦ç­–ç•¥ï¼Œä½†æ ‡å‡†DRå¿½è§†äº†å·²æœ‰çš„ç¦»çº¿æ•°æ®ã€‚æœ¬æ–‡ç ”ç©¶äº†ç¦»çº¿é¢†åŸŸéšæœºåŒ–ï¼ˆODRï¼‰ï¼Œé¦–å…ˆæ ¹æ®ç¦»çº¿æ•°æ®é›†æ‹Ÿåˆä»¿çœŸå™¨å‚æ•°çš„åˆ†å¸ƒã€‚æˆ‘ä»¬å°†ODRå½¢å¼åŒ–ä¸ºå‚æ•°åŒ–ä»¿çœŸå™¨æ—ä¸Šçš„æœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼Œè¯æ˜äº†åœ¨é€‚åº¦çš„æ­£åˆ™æ€§å’Œå¯è¯†åˆ«æ€§æ¡ä»¶ä¸‹ï¼Œè¯¥ä¼°è®¡é‡çš„ä¸€è‡´æ€§ï¼Œæ˜¾ç¤ºéšç€æ•°æ®é›†çš„å¢é•¿ï¼Œå®ƒæ”¶æ•›äºçœŸå®åŠ¨æ€ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ¨å¯¼äº†è¯¯å·®ç•Œé™ï¼Œè¡¨æ˜ODRçš„ä»¿çœŸåˆ°ç°å®è¯¯å·®åœ¨æœ‰é™ä»¿çœŸå™¨æƒ…å†µä¸‹æ¯”å‡åŒ€DRæ›´ç´§ï¼Œæœ€åå¼•å…¥äº†E-DROPOï¼Œé€šè¿‡å¢åŠ ç†µå¥–åŠ±æ¥é˜²æ­¢æ–¹å·®å´©æºƒï¼Œä»è€Œåœ¨å®è·µä¸­å®ç°æ›´å¹¿æ³›çš„éšæœºåŒ–å’Œæ›´ç¨³å¥çš„é›¶-shotè½¬ç§»ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¼ºåŒ–å­¦ä¹ ä»£ç†åœ¨ä»¿çœŸåˆ°ç°å®è½¬ç§»ä¸­çš„å›°éš¾ï¼Œç°æœ‰çš„é¢†åŸŸéšæœºåŒ–æ–¹æ³•æœªèƒ½æœ‰æ•ˆåˆ©ç”¨ç¦»çº¿æ•°æ®ï¼Œå¯¼è‡´è½¬ç§»æ•ˆæœä¸ç†æƒ³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„ç¦»çº¿é¢†åŸŸéšæœºåŒ–ï¼ˆODRï¼‰é€šè¿‡æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ‹Ÿåˆä»¿çœŸå™¨å‚æ•°åˆ†å¸ƒï¼Œå……åˆ†åˆ©ç”¨å·²æœ‰çš„ç¦»çº¿æ•°æ®ï¼Œä»¥æé«˜è½¬ç§»çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šODRçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†çš„æ„å»ºã€ä»¿çœŸå™¨å‚æ•°çš„åˆ†å¸ƒæ‹Ÿåˆã€æœ€å¤§ä¼¼ç„¶ä¼°è®¡çš„å®ç°ä»¥åŠæœ€ç»ˆçš„ç­–ç•¥è®­ç»ƒã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬ç¦»çº¿æ•°æ®å¤„ç†ã€å‚æ•°ä¼°è®¡å’Œç­–ç•¥ä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šODRçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶ç†è®ºåŸºç¡€çš„å»ºç«‹ï¼Œè¯æ˜äº†åœ¨é€‚åº¦æ¡ä»¶ä¸‹ä¼°è®¡é‡çš„ä¸€è‡´æ€§ï¼Œå¹¶ä¸”åœ¨æœ‰é™ä»¿çœŸå™¨æƒ…å†µä¸‹ï¼ŒODRçš„è¯¯å·®ç•Œé™æ¯”å‡åŒ€DRæ›´ç´§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒODRä½¿ç”¨äº†æœ€å¤§ä¼¼ç„¶ä¼°è®¡ä½œä¸ºæŸå¤±å‡½æ•°ï¼Œå¹¶é€šè¿‡å¼•å…¥E-DROPOå¢åŠ ç†µå¥–åŠ±ï¼Œä»¥é˜²æ­¢æ–¹å·®å´©æºƒï¼Œç¡®ä¿æ›´å¹¿æ³›çš„éšæœºåŒ–å’Œæ›´ç¨³å¥çš„é›¶-shotè½¬ç§»ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒODRåœ¨ä»¿çœŸåˆ°ç°å®è½¬ç§»ä¸­æ˜¾è‘—é™ä½äº†è¯¯å·®ï¼Œç›¸è¾ƒäºä¼ ç»Ÿçš„é¢†åŸŸéšæœºåŒ–æ–¹æ³•ï¼ŒODRçš„è¯¯å·®ç•Œé™æé«˜äº†O(M)å€ï¼Œä¸”E-DROPOåœ¨é›¶-shotè½¬ç§»ä¸­è¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶å’Œè™šæ‹Ÿç°å®ç­‰ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡ä»¿çœŸè®­ç»ƒçš„ç°å®é€‚åº”æ€§ï¼Œå‡å°‘ç°å®ç¯å¢ƒä¸­çš„è¯•é”™æˆæœ¬ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement-learning agents often struggle when deployed from simulation to the real-world. A dominant strategy for reducing the sim-to-real gap is domain randomization (DR) which trains the policy across many simulators produced by sampling dynamics parameters, but standard DR ignores offline data already available from the real system. We study offline domain randomization (ODR), which first fits a distribution over simulator parameters to an offline dataset. While a growing body of empirical work reports substantial gains with algorithms such as DROPO, the theoretical foundations of ODR remain largely unexplored. In this work, we (i) formalize ODR as a maximum-likelihood estimation over a parametric simulator family, (ii) prove consistency of this estimator under mild regularity and identifiability conditions, showing it converges to the true dynamics as the dataset grows, (iii) derive gap bounds demonstrating ODRs sim-to-real error is up to an O(M) factor tighter than uniform DR in the finite-simulator case (and analogous gains in the continuous setting), and (iv) introduce E-DROPO, a new version of DROPO which adds an entropy bonus to prevent variance collapse, yielding broader randomization and more robust zero-shot transfer in practice.

