---
layout: default
title: Canonical Latent Representations in Conditional Diffusion Models
---

# Canonical Latent Representations in Conditional Diffusion Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.09955" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.09955v1</a>
  <a href="https://arxiv.org/pdf/2506.09955.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.09955v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.09955v1', 'Canonical Latent Representations in Conditional Diffusion Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yitao Xu, Tong Zhang, Ehsan Pajouheshgar, Sabine SÃ¼sstrunk

**åˆ†ç±»**: cs.LG, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-11

**å¤‡æ³¨**: 45 pages,41 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCLARepsä»¥è§£å†³æ¡ä»¶æ‰©æ•£æ¨¡å‹ä¸­çš„ç‰¹å¾æ··æ·†é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ¡ä»¶æ‰©æ•£æ¨¡å‹` `æ½œåœ¨è¡¨ç¤º` `ç‰¹å¾è’¸é¦` `å¯¹æŠ—é²æ£’æ€§` `å¯è§£é‡Šæ€§` `ç”Ÿæˆæ¨¡å‹` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ¡ä»¶æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨ç‰¹å¾æå–æ—¶å®¹æ˜“æ··æ·†ç±»å®šä¹‰ç‰¹å¾ä¸æ— å…³ä¸Šä¸‹æ–‡ï¼Œå¯¼è‡´è¡¨ç¤ºä¸å¤Ÿç¨³å¥ã€‚
2. æœ¬æ–‡æå‡ºè§„èŒƒæ½œåœ¨è¡¨ç¤ºï¼ˆCLARepsï¼‰ï¼Œé€šè¿‡ä¿ç•™æ ¸å¿ƒç±»åˆ«ä¿¡æ¯å¹¶ä¸¢å¼ƒæ— å…³ä¿¡å·ï¼Œæå‡äº†æ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œç´§å‡‘æ€§ã€‚
3. å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨CLARepsçš„å­¦ç”Ÿæ¨¡å‹åœ¨å¯¹æŠ—é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ä¸Šæ˜¾è‘—æå‡ï¼Œèšç„¦äºç±»åˆ«ä¿¡å·è€ŒéèƒŒæ™¯å¹²æ‰°ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼ˆCDMsï¼‰åœ¨ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶å»ºæ¨¡èƒ½åŠ›å¯¼è‡´ç±»å®šä¹‰ç‰¹å¾ä¸æ— å…³ä¸Šä¸‹æ–‡æ··æ·†ï¼Œç»™æå–ç¨³å¥ä¸”å¯è§£é‡Šçš„è¡¨ç¤ºå¸¦æ¥äº†æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†è§„èŒƒæ½œåœ¨è¡¨ç¤ºï¼ˆCLARepsï¼‰ï¼Œè¿™äº›æ½œåœ¨ç¼–ç èƒ½å¤Ÿä¿ç•™é‡è¦çš„ç±»åˆ«ä¿¡æ¯ï¼ŒåŒæ—¶ä¸¢å¼ƒéåŒºåˆ†æ€§ä¿¡å·ã€‚é€šè¿‡CLARepsï¼Œå¼€å‘äº†ä¸€ç§æ–°çš„åŸºäºæ‰©æ•£çš„ç‰¹å¾è’¸é¦èŒƒå¼CaDistillï¼Œå­¦ç”Ÿæ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä»…é€šè¿‡CLARepsè·å–æ ¸å¿ƒç±»åˆ«çŸ¥è¯†ï¼Œæœ€ç»ˆå®ç°äº†å¼ºå¤§çš„å¯¹æŠ—é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒCDMsä¸ä»…å¯ä»¥ä½œä¸ºå›¾åƒç”Ÿæˆå™¨ï¼Œè¿˜å¯ä»¥ä½œä¸ºç´§å‡‘ä¸”å¯è§£é‡Šçš„æ•™å¸ˆï¼Œæ¨åŠ¨ç¨³å¥çš„è¡¨ç¤ºå­¦ä¹ ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ¡ä»¶æ‰©æ•£æ¨¡å‹ä¸­ç±»å®šä¹‰ç‰¹å¾ä¸æ— å…³ä¸Šä¸‹æ–‡æ··æ·†çš„é—®é¢˜ï¼Œå¯¼è‡´æå–çš„è¡¨ç¤ºä¸å¤Ÿç¨³å¥å’Œå¯è§£é‡Šã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºè§„èŒƒæ½œåœ¨è¡¨ç¤ºï¼ˆCLARepsï¼‰ï¼Œè¿™äº›æ½œåœ¨ç¼–ç èƒ½å¤Ÿæœ‰æ•ˆä¿ç•™ç±»åˆ«ä¿¡æ¯ï¼ŒåŒæ—¶å»é™¤éåŒºåˆ†æ€§ä¿¡å·ï¼Œä»è€Œæå‡æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•™å¸ˆæ¨¡å‹ï¼ˆCDMï¼‰å’Œå­¦ç”Ÿæ¨¡å‹ï¼ˆCaDistillï¼‰ï¼Œæ•™å¸ˆé€šè¿‡CLARepsä¼ é€’æ ¸å¿ƒç±»åˆ«çŸ¥è¯†ï¼Œå­¦ç”Ÿæ¨¡å‹åœ¨è®­ç»ƒæ—¶ä»…ä½¿ç”¨10%çš„è®­ç»ƒæ•°æ®ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºCLARepsçš„å¼•å…¥ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨ç”Ÿæˆæ ·æœ¬æ—¶æ›´å¥½åœ°ä»£è¡¨æ¯ä¸ªç±»åˆ«ï¼ŒåŒºåˆ«äºä¼ ç»Ÿæ–¹æ³•çš„å…¨æ•°æ®å»ºæ¨¡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒCLARepsçš„ç”Ÿæˆä¾èµ–äºç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ï¼Œç¡®ä¿æå–çš„æ½œåœ¨è¡¨ç¤ºèƒ½å¤Ÿæœ‰æ•ˆå»é™¤æ— å…³ä¿¡æ¯ï¼ŒåŒæ—¶ä¿ç•™æ ¸å¿ƒç±»åˆ«ç‰¹å¾ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨CLARepsçš„å­¦ç”Ÿæ¨¡å‹åœ¨å¯¹æŠ—é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ä¸Šæ˜¾è‘—æå‡ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨æ ‡å‡†æ•°æ®é›†ä¸Šå‡†ç¡®ç‡æé«˜äº†15%ï¼Œå¹¶ä¸”åœ¨å¯¹æŠ—æ”»å‡»ä¸‹çš„æ€§èƒ½ä¿æŒç¨³å®šï¼Œæ˜¾ç¤ºå‡ºæ›´å¼ºçš„æŠ—å¹²æ‰°èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å›¾åƒç”Ÿæˆã€åˆ†ç±»ä»»åŠ¡å’Œå¯¹æŠ—å­¦ä¹ ç­‰ã€‚é€šè¿‡æä¾›å¯è§£é‡Šä¸”ç´§å‡‘çš„è¡¨ç¤ºï¼ŒCLARepså¯ä»¥åœ¨å¤šç§ç”Ÿæˆå’Œåˆ¤åˆ«ä»»åŠ¡ä¸­æå‡æ¨¡å‹çš„æ€§èƒ½ï¼Œæœªæ¥å¯èƒ½åœ¨è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—å½±åƒåˆ†æç­‰é¢†åŸŸäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Conditional diffusion models (CDMs) have shown impressive performance across a range of generative tasks. Their ability to model the full data distribution has opened new avenues for analysis-by-synthesis in downstream discriminative learning. However, this same modeling capacity causes CDMs to entangle the class-defining features with irrelevant context, posing challenges to extracting robust and interpretable representations. To this end, we identify Canonical LAtent Representations (CLAReps), latent codes whose internal CDM features preserve essential categorical information while discarding non-discriminative signals. When decoded, CLAReps produce representative samples for each class, offering an interpretable and compact summary of the core class semantics with minimal irrelevant details. Exploiting CLAReps, we develop a novel diffusion-based feature-distillation paradigm, CaDistill. While the student has full access to the training set, the CDM as teacher transfers core class knowledge only via CLAReps, which amounts to merely 10 % of the training data in size. After training, the student achieves strong adversarial robustness and generalization ability, focusing more on the class signals instead of spurious background cues. Our findings suggest that CDMs can serve not just as image generators but also as compact, interpretable teachers that can drive robust representation learning.

