---
layout: default
title: FedVLMBench: Benchmarking Federated Fine-Tuning of Vision-Language Models
---

# FedVLMBench: Benchmarking Federated Fine-Tuning of Vision-Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.09638" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.09638v1</a>
  <a href="https://arxiv.org/pdf/2506.09638.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.09638v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.09638v1', 'FedVLMBench: Benchmarking Federated Fine-Tuning of Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Weiying Zheng, Ziyue Lin, Pengxin Guo, Yuyin Zhou, Feifei Wang, Liangqiong Qu

**åˆ†ç±»**: cs.LG, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-11

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFedVLMBenchä»¥è§£å†³è”é‚¦å­¦ä¹ ä¸‹è§†è§‰-è¯­è¨€æ¨¡å‹å¾®è°ƒè¯„ä¼°é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è”é‚¦å­¦ä¹ ` `è§†è§‰-è¯­è¨€æ¨¡å‹` `å¾®è°ƒç­–ç•¥` `æ•°æ®å¼‚è´¨æ€§` `å¤šæ¨¡æ€å­¦ä¹ ` `éšç§ä¿æŠ¤` `åŸºå‡†è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰-è¯­è¨€æ¨¡å‹å¾®è°ƒæ–¹æ³•å¤§å¤šä¾èµ–é›†ä¸­å¼è®­ç»ƒï¼Œæ— æ³•æ»¡è¶³éšç§è¦æ±‚é«˜çš„é¢†åŸŸï¼Œå¦‚åŒ»ç–—ã€‚
2. æœ¬æ–‡æå‡ºFedVLMBenchï¼Œä½œä¸ºç¬¬ä¸€ä¸ªç³»ç»Ÿæ€§åŸºå‡†ï¼Œè¯„ä¼°VLMsçš„è”é‚¦å¾®è°ƒç­–ç•¥å’Œæ¨¡å‹æ¶æ„ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œ2å±‚MLPè¿æ¥å™¨åœ¨è”é‚¦å­¦ä¹ ä¸­å¯¹ç¼–ç å™¨åŸºç¡€çš„VLMsè¡¨ç°æœ€ä½³ï¼Œä¸”è§†è§‰ä»»åŠ¡å¯¹æ•°æ®å¼‚è´¨æ€§æ›´æ•æ„Ÿã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰é€šè¿‡æ•´åˆè§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯ï¼Œåœ¨è·¨æ¨¡æ€ç†è§£å’Œç”Ÿæˆæ–¹é¢å±•ç°äº†å“è¶Šèƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¾®è°ƒæ–¹æ³•å¤§å¤šä¾èµ–é›†ä¸­å¼è®­ç»ƒï¼Œéš¾ä»¥æ»¡è¶³åŒ»ç–—ç­‰é¢†åŸŸçš„éšç§éœ€æ±‚ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†FedVLMBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç³»ç»Ÿæ€§è¯„ä¼°VLMsè”é‚¦å¾®è°ƒçš„åŸºå‡†ï¼Œæ•´åˆäº†å¤šç§æ¨¡å‹æ¶æ„ã€å¾®è°ƒç­–ç•¥å’Œè”é‚¦å­¦ä¹ ç®—æ³•ã€‚é€šè¿‡å¹¿æ³›å®éªŒï¼Œå‘ç°2å±‚å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰è¿æ¥å™¨åœ¨è”é‚¦å­¦ä¹ ä¸­å¯¹ç¼–ç å™¨åŸºç¡€çš„VLMsè¡¨ç°æœ€ä½³ï¼ŒåŒæ—¶å½“å‰çš„è”é‚¦å­¦ä¹ æ–¹æ³•å¯¹è§†è§‰ä»»åŠ¡çš„æ•°æ®å¼‚è´¨æ€§æ•æ„Ÿæ€§æ˜¾è‘—é«˜äºæ–‡æœ¬ä»»åŠ¡ã€‚è¯¥åŸºå‡†ä¸ºç ”ç©¶ç¤¾åŒºæä¾›äº†é‡è¦å·¥å…·å’Œæ•°æ®é›†ï¼Œæ¨åŠ¨éšç§ä¿æŠ¤çš„å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹çš„è”é‚¦è®­ç»ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨è”é‚¦å­¦ä¹ ç¯å¢ƒä¸‹å¾®è°ƒè¯„ä¼°ç¼ºä¹ç³»ç»Ÿæ€§åŸºå‡†çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨éšç§ä¿æŠ¤å’Œæ•°æ®å¼‚è´¨æ€§æ–¹é¢å­˜åœ¨æ˜¾è‘—æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºFedVLMBenchåŸºå‡†ï¼Œæ•´åˆå¤šç§VLMæ¶æ„ã€å¾®è°ƒç­–ç•¥å’Œè”é‚¦å­¦ä¹ ç®—æ³•ï¼Œä»¥ç³»ç»Ÿæ€§è¯„ä¼°è”é‚¦å¾®è°ƒçš„æ•ˆæœå’Œé€‚ç”¨æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šFedVLMBenchåŒ…å«ä¸¤ç§ä¸»æµVLMæ¶æ„ï¼ˆåŸºäºç¼–ç å™¨å’Œæ— ç¼–ç å™¨ï¼‰ã€å››ç§å¾®è°ƒç­–ç•¥ã€äº”ç§è”é‚¦å­¦ä¹ ç®—æ³•ï¼Œä»¥åŠå…­ä¸ªè·¨é¢†åŸŸæ•°æ®é›†ï¼Œè¦†ç›–å•ä»»åŠ¡å’Œå¤šä»»åŠ¡åœºæ™¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæä¾›äº†ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æ¡†æ¶ï¼Œèƒ½å¤Ÿè¯„ä¼°ä¸åŒå¾®è°ƒç­–ç•¥å’Œæ¨¡å‹æ¶æ„åœ¨è”é‚¦å­¦ä¹ ä¸­çš„è¡¨ç°ï¼Œå¡«è¡¥äº†ç°æœ‰ç ”ç©¶çš„ç©ºç™½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†2å±‚MLPè¿æ¥å™¨ä¸å¹¶è¡Œè¿æ¥å™¨å’Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è°ƒä¼˜çš„ç»„åˆï¼Œå‘ç°å…¶åœ¨ç¼–ç å™¨åŸºç¡€VLMsä¸­çš„è¡¨ç°æœ€ä½³ã€‚åŒæ—¶ï¼Œé’ˆå¯¹æ•°æ®å¼‚è´¨æ€§è¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œæ­ç¤ºäº†è§†è§‰ä»»åŠ¡å’Œæ–‡æœ¬ä»»åŠ¡çš„æ•æ„Ÿæ€§å·®å¼‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨2å±‚MLPè¿æ¥å™¨çš„é…ç½®åœ¨ç¼–ç å™¨åŸºç¡€çš„VLMsä¸­è¡¨ç°æœ€ä½³ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨è”é‚¦å­¦ä¹ ä¸­çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè§†è§‰ä»»åŠ¡å¯¹æ•°æ®å¼‚è´¨æ€§çš„æ•æ„Ÿæ€§æ˜æ˜¾é«˜äºæ–‡æœ¬ä»»åŠ¡ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†é‡è¦çš„å‚è€ƒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒ»ç–—ã€é‡‘èç­‰å¯¹éšç§è¦æ±‚é«˜çš„è¡Œä¸šï¼Œèƒ½å¤Ÿåœ¨ä¿æŠ¤ç”¨æˆ·éšç§çš„å‰æä¸‹ï¼Œåˆ©ç”¨å¤šæ¨¡æ€æ•°æ®è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚æœªæ¥ï¼ŒFedVLMBenchå°†ä¸ºå¤šæ¨¡æ€åŸºç¡€æ¨¡å‹çš„éšç§ä¿æŠ¤è®­ç»ƒæä¾›æ ‡å‡†åŒ–å¹³å°ï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„ç ”ç©¶è¿›å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-Language Models (VLMs) have demonstrated remarkable capabilities in cross-modal understanding and generation by integrating visual and textual information. While instruction tuning and parameter-efficient fine-tuning methods have substantially improved the generalization of VLMs, most existing approaches rely on centralized training, posing challenges for deployment in domains with strict privacy requirements like healthcare. Recent efforts have introduced Federated Learning (FL) into VLM fine-tuning to address these privacy concerns, yet comprehensive benchmarks for evaluating federated fine-tuning strategies, model architectures, and task generalization remain lacking. In this work, we present \textbf{FedVLMBench}, the first systematic benchmark for federated fine-tuning of VLMs. FedVLMBench integrates two mainstream VLM architectures (encoder-based and encoder-free), four fine-tuning strategies, five FL algorithms, six multimodal datasets spanning four cross-domain single-task scenarios and two cross-domain multitask settings, covering four distinct downstream task categories. Through extensive experiments, we uncover key insights into the interplay between VLM architectures, fine-tuning strategies, data heterogeneity, and multi-task federated optimization. Notably, we find that a 2-layer multilayer perceptron (MLP) connector with concurrent connector and LLM tuning emerges as the optimal configuration for encoder-based VLMs in FL. Furthermore, current FL methods exhibit significantly higher sensitivity to data heterogeneity in vision-centric tasks than text-centric ones, across both encoder-free and encoder-based VLM architectures. Our benchmark provides essential tools, datasets, and empirical guidance for the research community, offering a standardized platform to advance privacy-preserving, federated training of multimodal foundation models.

