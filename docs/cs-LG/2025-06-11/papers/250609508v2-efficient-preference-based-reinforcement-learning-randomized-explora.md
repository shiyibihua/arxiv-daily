---
layout: default
title: Efficient Preference-Based Reinforcement Learning: Randomized Exploration Meets Experimental Design
---

# Efficient Preference-Based Reinforcement Learning: Randomized Exploration Meets Experimental Design

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.09508" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.09508v2</a>
  <a href="https://arxiv.org/pdf/2506.09508.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.09508v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.09508v2', 'Efficient Preference-Based Reinforcement Learning: Randomized Exploration Meets Experimental Design')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Andreas Schlaginhaufen, Reda Ouhamma, Maryam Kamgarpour

**åˆ†ç±»**: cs.LG, cs.AI, cs.RO, stat.ML

**å‘å¸ƒæ—¥æœŸ**: 2025-06-11 (æ›´æ–°: 2025-12-03)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºåå¥½çš„é«˜æ•ˆå¼ºåŒ–å­¦ä¹ æ–¹æ³•ä»¥è§£å†³æŸ¥è¯¢é€‰æ‹©é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `äººç±»åé¦ˆ` `åå¥½æ¯”è¾ƒ` `å®éªŒè®¾è®¡` `éšæœºæ¢ç´¢` `é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹` `å¹¶è¡ŒåŒ–æŸ¥è¯¢`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨é€‰æ‹©ä¿¡æ¯ä¸°å¯Œçš„åå¥½æŸ¥è¯¢æ—¶é¢ä¸´è®¡ç®—æŒ‘æˆ˜ï¼Œéš¾ä»¥æœ‰æ•ˆè¯†åˆ«æ½œåœ¨å¥–åŠ±ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºéšæœºæ¢ç´¢çš„å…ƒç®—æ³•ï¼Œé¿å…äº†ä¹è§‚æ–¹æ³•çš„å¤æ‚æ€§ï¼Œå¹¶å¼•å…¥æ‰¹æ¬¡ç»“æ„ä»¥ä¼˜åŒ–æŸ¥è¯¢é€‰æ‹©ã€‚
3. å®éªŒè¯æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨åå¥½æŸ¥è¯¢æ•°é‡è¾ƒå°‘çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½ä¸ä¼ ç»ŸåŸºäºå¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸å½“ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç ”ç©¶äº†åœ¨ä¸€èˆ¬é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ä¸­ï¼ŒåŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼Œé‡ç‚¹å…³æ³¨é€šè¿‡è½¨è¿¹çº§åå¥½æ¯”è¾ƒè¿›è¡Œå­¦ä¹ çš„æŒ‘æˆ˜ã€‚æ ¸å¿ƒé—®é¢˜åœ¨äºè®¾è®¡èƒ½å¤Ÿé€‰æ‹©ä¿¡æ¯é‡ä¸°å¯Œçš„åå¥½æŸ¥è¯¢çš„ç®—æ³•ï¼Œä»¥è¯†åˆ«æ½œåœ¨å¥–åŠ±å¹¶ç¡®ä¿ç†è®ºä¿è¯ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºéšæœºæ¢ç´¢çš„å…ƒç®—æ³•ï¼Œé¿å…äº†ä¹è§‚æ–¹æ³•çš„è®¡ç®—æŒ‘æˆ˜ï¼Œå¹¶ä¿æŒå¯å¤„ç†æ€§ã€‚åœ¨æ¸©å’Œçš„å¼ºåŒ–å­¦ä¹ oracleå‡è®¾ä¸‹ï¼Œæˆ‘ä»¬å»ºç«‹äº†åæ‚”å’Œæœ€åè¿­ä»£çš„ä¿è¯ã€‚ä¸ºäº†æé«˜æŸ¥è¯¢å¤æ‚åº¦ï¼Œæˆ‘ä»¬å¼•å…¥å¹¶åˆ†æäº†ä¸€ç§æ”¹è¿›ç®—æ³•ï¼Œè¯¥ç®—æ³•æ”¶é›†è½¨è¿¹å¯¹çš„æ‰¹æ¬¡ï¼Œå¹¶åº”ç”¨æœ€ä¼˜å®éªŒè®¾è®¡é€‰æ‹©ä¿¡æ¯ä¸°å¯Œçš„æ¯”è¾ƒæŸ¥è¯¢ã€‚æ‰¹æ¬¡ç»“æ„è¿˜ä½¿å¾—åå¥½æŸ¥è¯¢çš„å¹¶è¡ŒåŒ–æˆä¸ºå¯èƒ½ï¼Œè¿™åœ¨å®é™…éƒ¨ç½²ä¸­å°¤ä¸ºé‡è¦ï¼Œå› ä¸ºåé¦ˆå¯ä»¥å¹¶å‘æ”¶é›†ã€‚å®è¯è¯„ä¼°è¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨éœ€è¦è¾ƒå°‘åå¥½æŸ¥è¯¢çš„æƒ…å†µä¸‹ï¼Œä¸åŸºäºå¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•å…·æœ‰ç«äº‰åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨å¼ºåŒ–å­¦ä¹ ä¸­å¦‚ä½•æœ‰æ•ˆé€‰æ‹©åå¥½æŸ¥è¯¢çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€é¢ä¸´è®¡ç®—å¤æ‚æ€§é«˜ã€ä¿¡æ¯é‡ä¸è¶³ç­‰ç—›ç‚¹ï¼Œå¯¼è‡´å­¦ä¹ æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºéšæœºæ¢ç´¢çš„å…ƒç®—æ³•ï¼Œé€šè¿‡é€‰æ‹©ä¿¡æ¯é‡ä¸°å¯Œçš„åå¥½æŸ¥è¯¢æ¥è¯†åˆ«æ½œåœ¨å¥–åŠ±ï¼ŒåŒæ—¶ç¡®ä¿ç®—æ³•çš„å¯å¤„ç†æ€§å’Œç†è®ºä¿è¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šéšæœºæ¢ç´¢æ¨¡å—å’Œæœ€ä¼˜å®éªŒè®¾è®¡æ¨¡å—ã€‚éšæœºæ¢ç´¢æ¨¡å—è´Ÿè´£ç”Ÿæˆåå¥½æŸ¥è¯¢ï¼Œè€Œæœ€ä¼˜å®éªŒè®¾è®¡æ¨¡å—åˆ™ä¼˜åŒ–æŸ¥è¯¢é€‰æ‹©ä»¥æé«˜ä¿¡æ¯é‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥äº†æ‰¹æ¬¡ç»“æ„ï¼Œä½¿å¾—åå¥½æŸ¥è¯¢å¯ä»¥å¹¶è¡ŒåŒ–æ”¶é›†ï¼Œä»è€Œæé«˜äº†æŸ¥è¯¢æ•ˆç‡ã€‚è¿™ä¸€è®¾è®¡ä¸ä¼ ç»Ÿçš„å•ä¸€æŸ¥è¯¢æ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç®—æ³•å®ç°ä¸­ï¼Œæˆ‘ä»¬è®¾ç½®äº†é€‚å½“çš„å‚æ•°ä»¥å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ï¼Œå¹¶è®¾è®¡äº†æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–æŸ¥è¯¢é€‰æ‹©çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨åå¥½æŸ¥è¯¢æ•°é‡ä¸Šæ˜¾è‘—å‡å°‘ï¼ŒåŒæ—¶åœ¨æ€§èƒ½ä¸Šä¸ä¼ ç»Ÿçš„åŸºäºå¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸å½“ã€‚å…·ä½“è€Œè¨€ï¼Œæ‰€ææ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæ›´ä½çš„æŸ¥è¯¢å¤æ‚åº¦ï¼Œæå‡å¹…åº¦è¾¾åˆ°30%ä»¥ä¸Šã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬äººæœºäº¤äº’ã€æ¨èç³»ç»Ÿå’Œè‡ªåŠ¨åŒ–å†³ç­–ç­‰ã€‚é€šè¿‡æé«˜åå¥½æŸ¥è¯¢çš„æ•ˆç‡ï¼Œèƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­æ›´å¿«é€Ÿåœ°æ”¶é›†åé¦ˆï¼Œä»è€Œæå‡ç³»ç»Ÿçš„å­¦ä¹ èƒ½åŠ›å’Œç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½æ¨åŠ¨æ›´å¤šåŸºäºäººç±»åé¦ˆçš„æ™ºèƒ½ç³»ç»Ÿçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We study reinforcement learning from human feedback in general Markov decision processes, where agents learn from trajectory-level preference comparisons. A central challenge in this setting is to design algorithms that select informative preference queries to identify the underlying reward while ensuring theoretical guarantees. We propose a meta-algorithm based on randomized exploration, which avoids the computational challenges associated with optimistic approaches and remains tractable. We establish both regret and last-iterate guarantees under mild reinforcement learning oracle assumptions. To improve query complexity, we introduce and analyze an improved algorithm that collects batches of trajectory pairs and applies optimal experimental design to select informative comparison queries. The batch structure also enables parallelization of preference queries, which is relevant in practical deployment as feedback can be gathered concurrently. Empirical evaluation confirms that the proposed method is competitive with reward-based reinforcement learning while requiring a small number of preference queries.

