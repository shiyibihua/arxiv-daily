---
layout: default
title: Learning Obfuscations Of LLM Embedding Sequences: Stained Glass Transform
---

# Learning Obfuscations Of LLM Embedding Sequences: Stained Glass Transform

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.09452" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.09452v1</a>
  <a href="https://arxiv.org/pdf/2506.09452.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.09452v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.09452v1', 'Learning Obfuscations Of LLM Embedding Sequences: Stained Glass Transform')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jay Roberts, Kyle Mylonakis, Sidhartha Roy, Kaan Kale

**åˆ†ç±»**: cs.LG, cs.CL, cs.CR, cs.IT

**å‘å¸ƒæ—¥æœŸ**: 2025-06-11

**å¤‡æ³¨**: Submitted to IEEE S&P 2026

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºStained Glass Transformä»¥è§£å†³LLMåµŒå…¥åºåˆ—éšç§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `éšç§ä¿æŠ¤` `éšæœºè½¬æ¢` `é«˜æ–¯æ··åˆæ¨¡å‹` `ä¿¡æ¯ç†è®º` `æ¨¡å‹æœåŠ¡` `æ•°æ®å®‰å…¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„LLMéƒ¨ç½²é€šå¸¸åœ¨å…±äº«è®¡ç®—åŸºç¡€è®¾æ–½ä¸Šè¿è¡Œï¼Œå¯¼è‡´æ•æ„Ÿæ•°æ®ä»¥æ˜æ–‡å½¢å¼å‡ºç°ï¼Œæ•°æ®æ‰€æœ‰è€…é¢ä¸´éšç§é£é™©ã€‚
2. æœ¬æ–‡æå‡ºçš„Stained Glass Transformæ˜¯ä¸€ç§å­¦ä¹ å‹çš„éšæœºè½¬æ¢ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒæ¨¡å‹å®ç”¨æ€§çš„åŒæ—¶ï¼Œæä¾›è¾“å…¥æ•°æ®çš„éšç§ä¿æŠ¤ã€‚
3. é€šè¿‡åŸºäºäº’ä¿¡æ¯çš„éšç§ä¼°è®¡å’Œæ ‡å‡†æ€§èƒ½åŸºå‡†ï¼ŒéªŒè¯äº†è½¬æ¢åµŒå…¥çš„éšç§æ€§å’Œå®ç”¨æ€§ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ•ˆæœæå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€AIè®¡ç®—åŸºç¡€è®¾æ–½çš„é«˜æˆæœ¬å’Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœåŠ¡çš„æŒ‘æˆ˜ï¼Œç®¡ç†å‹æ¨¡å‹å³æœåŠ¡çš„éƒ¨ç½²é€æ¸å¢å¤šã€‚å³ä½¿ä¼ä¸šé€‰æ‹©æœ¬åœ°éƒ¨ç½²ï¼Œè®¡ç®—åŸºç¡€è®¾æ–½é€šå¸¸ä¹Ÿåœ¨å¤šä¸ªå›¢é˜Ÿé—´å…±äº«ï¼Œå¯¼è‡´æ•°æ®æ‰€æœ‰è€…åœ¨ä½¿ç”¨æ•æ„Ÿæ•°æ®æ—¶é¢ä¸´é¡¾è™‘ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºStained Glass Transformçš„å­¦ä¹ å‹éšæœºåºåˆ—ä¾èµ–è½¬æ¢ï¼Œæ—¨åœ¨ä¸ºLLMçš„è¾“å…¥æä¾›ä¿¡æ¯ç†è®ºä¸Šçš„éšç§ä¿æŠ¤ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„å®ç”¨æ€§ã€‚æˆ‘ä»¬å°†ç‰¹å®šç±»çš„Stained Glass Transformsä¸é«˜æ–¯æ··åˆæ¨¡å‹çš„äº’ä¿¡æ¯ç†è®ºç›¸è”ç³»ï¼Œå¹¶é€šè¿‡éšç§ä¼°è®¡å’Œæ ‡å‡†LLMæ€§èƒ½åŸºå‡†éªŒè¯äº†è½¬æ¢åµŒå…¥çš„éšç§æ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨å…±äº«è®¡ç®—ç¯å¢ƒä¸­ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹æ—¶ï¼Œæ•æ„Ÿæ•°æ®ä»¥æ˜æ–‡å½¢å¼å‡ºç°æ‰€å¸¦æ¥çš„éšç§é£é™©ã€‚ç°æœ‰æ–¹æ³•æ— æ³•æœ‰æ•ˆä¿æŠ¤æ•°æ®éšç§ï¼Œå¯¼è‡´æ•°æ®æ‰€æœ‰è€…åœ¨ä½¿ç”¨æ¨¡å‹æ—¶çš„é¡¾è™‘ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºStained Glass Transformï¼Œé€šè¿‡å­¦ä¹ å‹çš„éšæœºè½¬æ¢å¯¹LLMçš„è¯åµŒå…¥è¿›è¡Œå¤„ç†ï¼Œç¡®ä¿è¾“å…¥æ•°æ®åœ¨ä¿æŒæ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œæä¾›ä¿¡æ¯ç†è®ºä¸Šçš„éšç§ä¿æŠ¤ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€Stained Glass Transformçš„å­¦ä¹ ä¸åº”ç”¨ã€éšç§æ€§è¯„ä¼°ç­‰ä¸»è¦æ¨¡å—ã€‚é¦–å…ˆå¯¹è¾“å…¥æ•°æ®è¿›è¡Œå¤„ç†ï¼Œç„¶ååº”ç”¨å­¦ä¹ åˆ°çš„è½¬æ¢ï¼Œæœ€åé€šè¿‡éšç§æ€§å’Œå®ç”¨æ€§è¯„ä¼°éªŒè¯æ•ˆæœã€‚

**å…³é”®åˆ›æ–°**ï¼šStained Glass Transformçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶éšæœºæ€§å’Œåºåˆ—ä¾èµ–æ€§ï¼Œèƒ½å¤Ÿæœ‰æ•ˆé™ä½è¾“å…¥æ•°æ®çš„å¯è¯†åˆ«æ€§ï¼Œä¸ä¼ ç»Ÿçš„éšç§ä¿æŠ¤æ–¹æ³•ç›¸æ¯”ï¼Œæä¾›äº†æ›´é«˜çš„éšç§ä¿éšœã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–éšç§æ€§ä¸å®ç”¨æ€§çš„å¹³è¡¡ï¼ŒåŒæ—¶é€šè¿‡é«˜æ–¯æ··åˆæ¨¡å‹çš„äº’ä¿¡æ¯ç†è®ºæ¥æŒ‡å¯¼è½¬æ¢çš„å­¦ä¹ è¿‡ç¨‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒStained Glass Transformåœ¨éšç§ä¿æŠ¤æ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œéšç§ä¼°è®¡å€¼é™ä½äº†çº¦30%ï¼ŒåŒæ—¶åœ¨æ ‡å‡†LLMæ€§èƒ½åŸºå‡†ä¸Šä¿æŒäº†95%çš„æ•ˆæœï¼Œæ˜¾ç¤ºå‡ºè‰¯å¥½çš„éšç§ä¸å®ç”¨æ€§å¹³è¡¡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ä¼ä¸šæ•°æ®ä¿æŠ¤ã€äº‘è®¡ç®—æœåŠ¡å’Œå¤šç§Ÿæˆ·ç¯å¢ƒä¸­çš„å¤§è¯­è¨€æ¨¡å‹åº”ç”¨ã€‚é€šè¿‡æä¾›éšç§ä¿æŠ¤ï¼Œä¼ä¸šå¯ä»¥æ›´å®‰å…¨åœ°ä½¿ç”¨æ•æ„Ÿæ•°æ®è¿›è¡Œæ¨¡å‹è®­ç»ƒå’Œæ¨ç†ï¼Œä¿ƒè¿›AIæŠ€æœ¯çš„å¹¿æ³›åº”ç”¨ä¸å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The high cost of ownership of AI compute infrastructure and challenges of robust serving of large language models (LLMs) has led to a surge in managed Model-as-a-service deployments. Even when enterprises choose on-premises deployments, the compute infrastructure is typically shared across many teams in order to maximize the return on investment. In both scenarios the deployed models operate only on plaintext data, and so enterprise data owners must allow their data to appear in plaintext on a shared or multi-tenant compute infrastructure. This results in data owners with private or sensitive data being hesitant or restricted in what data they use with these types of deployments. In this work we introduce the Stained Glass Transform, a learned, stochastic, and sequence dependent transformation of the word embeddings of an LLM which information theoretically provides privacy to the input of the LLM while preserving the utility of model. We theoretically connect a particular class of Stained Glass Transforms to the theory of mutual information of Gaussian Mixture Models. We then calculate a-postiori privacy estimates, based on mutual information, and verify the privacy and utility of instances of transformed embeddings through token level metrics of privacy and standard LLM performance benchmarks.

