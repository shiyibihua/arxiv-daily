---
layout: default
title: Emergence of Superposition: Unveiling the Training Dynamics of Chain of Continuous Thought
---

# Emergence of Superposition: Unveiling the Training Dynamics of Chain of Continuous Thought

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.23365" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.23365v2</a>
  <a href="https://arxiv.org/pdf/2509.23365.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.23365v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.23365v2', 'Emergence of Superposition: Unveiling the Training Dynamics of Chain of Continuous Thought')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hanlin Zhu, Shibo Hao, Zhiting Hu, Jiantao Jiao, Stuart Russell, Yuandong Tian

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-27 (æ›´æ–°: 2025-10-06)

**å¤‡æ³¨**: 29 pages, 5 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ­ç¤ºè¿ç»­æ€ç»´é“¾ä¸­å åŠ æœºåˆ¶çš„æ¶Œç°ï¼šé€šè¿‡è®­ç»ƒåŠ¨æ€åˆ†æTransformerçš„æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¿ç»­æ€ç»´é“¾` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨ç†èƒ½åŠ›` `å åŠ æœºåˆ¶` `è®­ç»ƒåŠ¨æ€` `Transformer` `æœ‰å‘å›¾å¯è¾¾æ€§` `ç´¢å¼•åŒ¹é…logit`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›ä¸è¶³ï¼Œè¿ç»­æ€ç»´é“¾é€šè¿‡éšå¼å¹¶è¡Œæ€è€ƒæå‡æ¨ç†èƒ½åŠ›ï¼Œä½†å…¶è®­ç»ƒåŠ¨æ€å°šä¸æ˜ç¡®ã€‚
2. æœ¬æ–‡é€šè¿‡åˆ†æç®€åŒ–çš„Transformeråœ¨æœ‰å‘å›¾å¯è¾¾æ€§é—®é¢˜ä¸Šçš„è®­ç»ƒåŠ¨æ€ï¼Œæ­ç¤ºå åŠ æœºåˆ¶çš„æ¶Œç°è¿‡ç¨‹ã€‚
3. ç†è®ºåˆ†æè¡¨æ˜ï¼Œç´¢å¼•åŒ¹é…logitåœ¨è®­ç»ƒä¸­ä¼šå…ˆå¢åŠ åæœ‰ç•Œï¼Œå¹³è¡¡æ¢ç´¢å’Œåˆ©ç”¨ï¼Œå®éªŒç»“æœéªŒè¯äº†ç†è®ºã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç ”ç©¶äº†è¿ç»­æ€ç»´é“¾ï¼ˆcontinuous CoTï¼‰å¦‚ä½•æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œè¯¥æ–¹æ³•é€šè¿‡éšå¼å¹¶è¡Œæ€è€ƒå®ç°ã€‚å‰æœŸå·¥ä½œå·²è¯æ˜ï¼Œé…å¤‡è¿ç»­CoTçš„ä¸¤å±‚Transformerå¯ä»¥é€šè¿‡åœ¨è¿ç»­æ€ç»´ä¸­ä¿æŒå¤šä¸ªæ¨ç†è½¨è¿¹çš„å åŠ æ¥æœ‰æ•ˆè§£å†³æœ‰å‘å›¾å¯è¾¾æ€§é—®é¢˜ã€‚ç„¶è€Œï¼Œå åŠ æœºåˆ¶å¦‚ä½•ä»åŸºäºæ¢¯åº¦çš„è®­ç»ƒæ–¹æ³•ä¸­è‡ªç„¶å­¦ä¹ ä»ç„¶ä¸æ¸…æ¥šã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬ä»ç†è®ºä¸Šåˆ†æäº†ä¸€ä¸ªç®€åŒ–çš„ä¸¤å±‚Transformeråœ¨æœ‰å‘å›¾å¯è¾¾æ€§é—®é¢˜ä¸Šçš„è®­ç»ƒåŠ¨æ€ï¼Œæ­ç¤ºäº†å åŠ æœºåˆ¶å¦‚ä½•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ¶Œç°ï¼Œåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šï¼ˆiï¼‰è‡ªå›å½’æ‰©å±•è¿ç»­æ€ç»´çš„æ€ç»´ç”Ÿæˆé˜¶æ®µï¼Œä»¥åŠï¼ˆiiï¼‰å°†æ€ç»´è½¬åŒ–ä¸ºæœ€ç»ˆç­”æ¡ˆçš„é¢„æµ‹é˜¶æ®µã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œåœ¨ä½¿ç”¨è¿ç»­æ€ç»´è¿›è¡Œè®­ç»ƒæ—¶ï¼Œç´¢å¼•åŒ¹é…logitï¼ˆåæ˜ æ¨¡å‹å±€éƒ¨æœç´¢èƒ½åŠ›å¼ºåº¦çš„é‡è¦æŒ‡æ ‡ï¼‰å°†é¦–å…ˆå¢åŠ ï¼Œç„¶ååœ¨æ¸©å’Œçš„å‡è®¾ä¸‹ä¿æŒæœ‰ç•Œã€‚æœ‰ç•Œçš„ç´¢å¼•åŒ¹é…logitæœ‰æ•ˆåœ°å¹³è¡¡äº†æ¨ç†è¿‡ç¨‹ä¸­çš„æ¢ç´¢å’Œåˆ©ç”¨ï¼šæ¨¡å‹å°†åˆ©ç”¨å±€éƒ¨é—®é¢˜ç»“æ„æ¥è¯†åˆ«åˆç†çš„æœç´¢è½¨è¿¹ï¼Œå¹¶åœ¨ä¸ç¡®å®šå“ªä¸ªè§£å†³æ–¹æ¡ˆæ­£ç¡®æ—¶ï¼Œä¸ºå¤šä¸ªæ­¤ç±»è½¨è¿¹åˆ†é…ç›¸å½“çš„æƒé‡ä»¥è¿›è¡Œæ¢ç´¢ï¼Œä»è€Œäº§ç”Ÿå åŠ ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè·Ÿè¸ªäº†logitsçš„å¢é•¿ï¼Œè¿›ä¸€æ­¥éªŒè¯äº†æˆ‘ä»¬çš„ç†è®ºã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨ç†è§£å¹¶è§£é‡Šå¤§å‹è¯­è¨€æ¨¡å‹ä¸­ï¼Œè¿ç»­æ€ç»´é“¾ï¼ˆContinuous Chain-of-Thought, Continuous CoTï¼‰æ–¹æ³•å¦‚ä½•é€šè¿‡æ¢¯åº¦ä¸‹é™è®­ç»ƒï¼Œæ¶Œç°å‡ºå åŠ ï¼ˆSuperpositionï¼‰æœºåˆ¶ï¼Œä»è€Œæå‡æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ï¼ˆå¦‚æœ‰å‘å›¾å¯è¾¾æ€§é—®é¢˜ï¼‰ä¸Šçš„æ€§èƒ½ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹å¯¹è¿™ç§æ¶Œç°è¿‡ç¨‹çš„ç†è®ºè§£é‡Šï¼Œæ— æ³•è§£é‡Šæ¨¡å‹å¦‚ä½•å­¦ä¹ å¹¶è¡Œç»´æŠ¤å¤šä¸ªæ¨ç†è·¯å¾„ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡åˆ†æä¸€ä¸ªç®€åŒ–çš„ä¸¤å±‚Transformeræ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„åŠ¨æ€å˜åŒ–ï¼Œç‰¹åˆ«æ˜¯å…³æ³¨ç´¢å¼•åŒ¹é…logitï¼ˆindex-matching logitï¼‰çš„å˜åŒ–ã€‚ç´¢å¼•åŒ¹é…logitåæ˜ äº†æ¨¡å‹å±€éƒ¨æœç´¢çš„èƒ½åŠ›ã€‚è®ºæ–‡è®¤ä¸ºï¼Œé€šè¿‡æ§åˆ¶ç´¢å¼•åŒ¹é…logitçš„å¢é•¿ï¼Œå¯ä»¥å¹³è¡¡æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„æ¢ç´¢ï¼ˆexplorationï¼‰å’Œåˆ©ç”¨ï¼ˆexploitationï¼‰ï¼Œä»è€Œå®ç°å¤šä¸ªæ¨ç†è·¯å¾„çš„å åŠ ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡å°†è®­ç»ƒè¿‡ç¨‹åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼š(1) **æ€ç»´ç”Ÿæˆé˜¶æ®µ**ï¼šæ¨¡å‹è‡ªå›å½’åœ°æ‰©å±•è¿ç»­æ€ç»´ï¼Œç”Ÿæˆä¸€ç³»åˆ—ä¸­é—´æ¨ç†æ­¥éª¤ã€‚(2) **é¢„æµ‹é˜¶æ®µ**ï¼šæ¨¡å‹å°†ç”Ÿæˆçš„è¿ç»­æ€ç»´è½¬åŒ–ä¸ºæœ€ç»ˆç­”æ¡ˆã€‚è®ºæ–‡ä¸»è¦åˆ†æäº†ç´¢å¼•åŒ¹é…logitåœ¨è¿™ä¸¤ä¸ªé˜¶æ®µçš„å˜åŒ–ï¼Œå¹¶æ¨å¯¼äº†å…¶å¢é•¿çš„ä¸Šä¸‹ç•Œã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºä»ç†è®ºä¸Šæ­ç¤ºäº†è¿ç»­æ€ç»´é“¾ä¸­å åŠ æœºåˆ¶çš„æ¶Œç°è¿‡ç¨‹ã€‚é€šè¿‡åˆ†æç´¢å¼•åŒ¹é…logitçš„åŠ¨æ€å˜åŒ–ï¼Œè§£é‡Šäº†æ¨¡å‹å¦‚ä½•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å­¦ä¹ å¹¶è¡Œç»´æŠ¤å¤šä¸ªå¯èƒ½çš„æ¨ç†è·¯å¾„ï¼Œä»è€Œæå‡æ¨ç†èƒ½åŠ›ã€‚è¿™ä¸ä¼ ç»Ÿçš„å•è·¯å¾„æ¨ç†æ–¹æ³•å½¢æˆå¯¹æ¯”ï¼Œåè€…å®¹æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜è§£ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š(1) ä½¿ç”¨ç®€åŒ–çš„ä¸¤å±‚Transformeræ¨¡å‹ï¼Œä¾¿äºç†è®ºåˆ†æã€‚(2) å…³æ³¨ç´¢å¼•åŒ¹é…logitï¼Œä½œä¸ºè¡¡é‡æ¨¡å‹å±€éƒ¨æœç´¢èƒ½åŠ›çš„å…³é”®æŒ‡æ ‡ã€‚(3) æ¨å¯¼ç´¢å¼•åŒ¹é…logitå¢é•¿çš„ä¸Šä¸‹ç•Œï¼Œè¯æ˜å…¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šå…ˆå¢åŠ åæœ‰ç•Œã€‚(4) é€šè¿‡å®éªŒéªŒè¯ç†è®ºåˆ†æçš„æ­£ç¡®æ€§ï¼Œè§‚å¯Ÿlogitsçš„å¢é•¿æƒ…å†µã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œç´¢å¼•åŒ¹é…logitåœ¨è®­ç»ƒåˆæœŸå¿«é€Ÿå¢é•¿ï¼Œéšåè¶‹äºç¨³å®šï¼ŒéªŒè¯äº†ç†è®ºåˆ†æçš„æ­£ç¡®æ€§ã€‚é€šè¿‡è·Ÿè¸ªlogitsçš„å¢é•¿ï¼Œè§‚å¯Ÿåˆ°æ¨¡å‹åœ¨ä¸ç¡®å®šæ­£ç¡®ç­”æ¡ˆæ—¶ï¼Œä¼šä¸ºå¤šä¸ªå¯èƒ½çš„æ¨ç†è·¯å¾„åˆ†é…ç›¸ä¼¼çš„æƒé‡ï¼Œä»è€Œå®ç°å åŠ ã€‚è¿™äº›å®éªŒç»“æœä¸ºç†è§£è¿ç»­æ€ç»´é“¾çš„è®­ç»ƒåŠ¨æ€æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœæœ‰åŠ©äºæ›´å¥½åœ°ç†è§£å’Œä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå¯åº”ç”¨äºé—®ç­”ç³»ç»Ÿã€çŸ¥è¯†å›¾è°±æ¨ç†ã€è§„åˆ’å’Œå†³ç­–ç­‰é¢†åŸŸã€‚é€šè¿‡ç†è§£å åŠ æœºåˆ¶çš„æ¶Œç°ï¼Œå¯ä»¥è®¾è®¡æ›´æœ‰æ•ˆçš„è®­ç»ƒæ–¹æ³•å’Œæ¨¡å‹æ¶æ„ï¼Œæå‡æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œå¹¶å¯èƒ½æ¨åŠ¨é€šç”¨äººå·¥æ™ºèƒ½çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Previous work shows that the chain of continuous thought (continuous CoT) improves the reasoning capability of large language models (LLMs) by enabling implicit parallel thinking, and a subsequent work provided theoretical insight by showing that a two-layer transformer equipped with continuous CoT can efficiently solve directed graph reachability by maintaining a superposition of multiple reasoning traces in the continuous thought. However, it remains unclear how the superposition mechanism is naturally learned from gradient-based training methods. To fill this gap, we theoretically analyze the training dynamics of a simplified two-layer transformer on the directed graph reachability problem to unveil how the superposition mechanism emerges during training in two training stages -- (i) a thought-generation stage that autoregressively expands the continuous thought, and (ii) a prediction stage that converts the thought into the final answer. Our analysis reveals that during training using continuous thought, the index-matching logit, an important quantity which reflects the strength of the model's local search ability, will first increase and then remain bounded under mild assumptions. The bounded index-matching logit effectively balances exploration and exploitation during the reasoning process: the model will exploit local problem structures to identify plausible search traces, and assign comparable weights to multiple such traces to explore when it is uncertain about which solution is correct, which results in superposition. Our experimental results tracking the growth of logits further validate our theory.

