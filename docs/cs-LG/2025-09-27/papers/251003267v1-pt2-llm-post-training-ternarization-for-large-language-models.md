---
layout: default
title: PT$^2$-LLM: Post-Training Ternarization for Large Language Models
---

# PT$^2$-LLM: Post-Training Ternarization for Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.03267" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.03267v1</a>
  <a href="https://arxiv.org/pdf/2510.03267.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.03267v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.03267v1', 'PT$^2$-LLM: Post-Training Ternarization for Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xianglong Yan, Chengzhu Bao, Zhiteng Li, Tianao Zhang, Kaicheng Yang, Haotong Qin, Ruobing Xie, Xingwu Sun, Yulun Zhang

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-27

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/XIANGLONGYAN/PT2-LLM)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**PT$^2$-LLMï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹çš„åè®­ç»ƒä¸‰å€¼åŒ–æ¡†æ¶ï¼Œå®ç°é«˜æ•ˆå‹ç¼©ä¸åŠ é€Ÿã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `åè®­ç»ƒé‡åŒ–` `ä¸‰å€¼åŒ–` `æ¨¡å‹å‹ç¼©` `æ¨¡å‹åŠ é€Ÿ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤§è¯­è¨€æ¨¡å‹éƒ¨ç½²å—é™äºé«˜å†…å­˜å’Œè®¡ç®—éœ€æ±‚ï¼Œåè®­ç»ƒé‡åŒ–ä¸­çš„ä¸‰å€¼åŒ–æ–¹æ³•é¢ä¸´æ— è®­ç»ƒä¼˜åŒ–å’Œå¼‚å¸¸å€¼é‡åŒ–éš¾é¢˜ã€‚
2. æå‡ºPT$^2$-LLMæ¡†æ¶ï¼Œæ ¸å¿ƒæ˜¯éå¯¹ç§°ä¸‰å€¼é‡åŒ–å™¨ï¼Œé€šè¿‡è¿­ä»£ä¸‰å€¼æ‹Ÿåˆå’Œæ¿€æ´»æ„ŸçŸ¥ç½‘æ ¼å¯¹é½è¿›è¡Œä¸¤é˜¶æ®µä¼˜åŒ–ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒPT$^2$-LLMåœ¨ä½å†…å­˜æˆæœ¬ä¸‹ï¼Œæ€§èƒ½åª²ç¾SOTA 2-bit PTQæ–¹æ³•ï¼Œå¹¶åŠ é€Ÿé¢„å¡«å……å’Œè§£ç è¿‡ç¨‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œä½†å…¶åºå¤§çš„å†…å­˜å’Œè®¡ç®—éœ€æ±‚é˜»ç¢äº†éƒ¨ç½²ã€‚ä¸‰å€¼åŒ–ä½œä¸ºä¸€ç§æœ‰å‰æ™¯çš„å‹ç¼©æŠ€æœ¯ï¼Œå› å…¶æ˜¾è‘—çš„å°ºå¯¸ç¼©å‡å’Œé«˜è®¡ç®—æ•ˆç‡è€Œå¤‡å—å…³æ³¨ã€‚ç„¶è€Œï¼Œç”±äºæ— è®­ç»ƒå‚æ•°ä¼˜åŒ–ä»¥åŠå¼‚å¸¸å€¼å’Œåˆ†æ•£æƒé‡å¸¦æ¥çš„é‡åŒ–å›°éš¾ï¼Œå…¶åœ¨åè®­ç»ƒé‡åŒ–(PTQ)è®¾ç½®ä¸­çš„æ½œåŠ›ä»æœªè¢«å……åˆ†æ¢ç´¢ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†PT$^2$-LLMï¼Œä¸€ä¸ªä¸“ä¸ºLLMsé‡èº«å®šåˆ¶çš„åè®­ç»ƒä¸‰å€¼åŒ–æ¡†æ¶ã€‚å…¶æ ¸å¿ƒæ˜¯ä¸€ä¸ªéå¯¹ç§°ä¸‰å€¼é‡åŒ–å™¨ï¼Œé…å¤‡äº†ä¸€ä¸ªä¸¤é˜¶æ®µç»†åŒ–æµç¨‹ï¼š(1)è¿­ä»£ä¸‰å€¼æ‹Ÿåˆ(ITF)ï¼Œåœ¨æœ€ä¼˜ä¸‰å€¼ç½‘æ ¼æ„å»ºå’Œçµæ´»èˆå…¥ä¹‹é—´äº¤æ›¿ï¼Œä»¥æœ€å°åŒ–é‡åŒ–è¯¯å·®ï¼›(2)æ¿€æ´»æ„ŸçŸ¥ç½‘æ ¼å¯¹é½(AGA)ï¼Œè¿›ä¸€æ­¥ç»†åŒ–ä¸‰å€¼ç½‘æ ¼ï¼Œä»¥æ›´å¥½åœ°åŒ¹é…å…¨ç²¾åº¦è¾“å‡ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å³æ’å³ç”¨çš„åŸºäºç»“æ„ç›¸ä¼¼æ€§çš„é‡æ’åº(SSR)ç­–ç•¥ï¼Œè¯¥ç­–ç•¥åˆ©ç”¨åˆ—é—´ç»“æ„ç›¸ä¼¼æ€§æ¥ç®€åŒ–é‡åŒ–å¹¶å‡è½»å¼‚å¸¸å€¼çš„å½±å“ï¼Œä»è€Œè¿›ä¸€æ­¥æé«˜æ•´ä½“æ€§èƒ½ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒPT$^2$-LLMåœ¨å†…å­˜æˆæœ¬æ›´ä½çš„æƒ…å†µä¸‹ï¼Œæä¾›äº†ä¸æœ€å…ˆè¿›çš„(SOTA) 2-bit PTQæ–¹æ³•ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼ŒåŒæ—¶åŠ é€Ÿäº†é¢„å¡«å……å’Œè§£ç ï¼Œä»è€Œå®ç°äº†ç«¯åˆ°ç«¯åŠ é€Ÿã€‚ä»£ç å’Œæ¨¡å‹å°†åœ¨https://github.com/XIANGLONGYAN/PT2-LLMä¸Šæä¾›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰éƒ¨ç½²æ—¶é¢ä¸´çš„å†…å­˜å’Œè®¡ç®—èµ„æºç“¶é¢ˆé—®é¢˜ã€‚ç°æœ‰çš„åè®­ç»ƒé‡åŒ–ï¼ˆPTQï¼‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯ä¸‰å€¼åŒ–æ–¹æ³•ï¼Œåœ¨LLMsä¸Šåº”ç”¨æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦ä½“ç°åœ¨ä¸¤ä¸ªæ–¹é¢ï¼šä¸€æ˜¯ç¼ºä¹æœ‰æ•ˆçš„è®­ç»ƒæ–¹æ³•æ¥ä¼˜åŒ–ä¸‰å€¼åŒ–å‚æ•°ï¼ŒäºŒæ˜¯LLMsä¸­å­˜åœ¨çš„å¼‚å¸¸å€¼å’Œåˆ†æ•£çš„æƒé‡åˆ†å¸ƒä½¿å¾—é‡åŒ–è¿‡ç¨‹æ›´åŠ å›°éš¾ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è®¾è®¡ä¸€ç§æ— éœ€è®­ç»ƒçš„åè®­ç»ƒä¸‰å€¼åŒ–æ¡†æ¶PT$^2$-LLMï¼Œæ¥å…‹æœä¸Šè¿°æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒåœ¨äºä¸€ä¸ªéå¯¹ç§°ä¸‰å€¼é‡åŒ–å™¨ï¼Œå¹¶é€šè¿‡ä¸¤é˜¶æ®µçš„ç»†åŒ–æµç¨‹ä»¥åŠç»“æ„ç›¸ä¼¼æ€§é‡æ’åºç­–ç•¥ï¼Œæ¥æœ€å°åŒ–é‡åŒ–è¯¯å·®ï¼Œæé«˜é‡åŒ–ç²¾åº¦ï¼Œä»è€Œåœ¨ä¿è¯æ€§èƒ½çš„åŒæ—¶ï¼Œå®ç°å¯¹LLMsçš„é«˜æ•ˆå‹ç¼©å’ŒåŠ é€Ÿã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPT$^2$-LLMæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªå…³é”®æ¨¡å—ï¼š
1. **éå¯¹ç§°ä¸‰å€¼é‡åŒ–å™¨**ï¼šä½œä¸ºé‡åŒ–çš„åŸºç¡€ï¼Œå°†å…¨ç²¾åº¦æƒé‡æ˜ å°„åˆ°{-1, 0, 1}ä¸‰ä¸ªå€¼ã€‚
2. **è¿­ä»£ä¸‰å€¼æ‹Ÿåˆï¼ˆITFï¼‰**ï¼šé€šè¿‡äº¤æ›¿ä¼˜åŒ–ä¸‰å€¼ç½‘æ ¼å’Œçµæ´»çš„èˆå…¥ç­–ç•¥ï¼Œæœ€å°åŒ–é‡åŒ–è¯¯å·®ã€‚
3. **æ¿€æ´»æ„ŸçŸ¥ç½‘æ ¼å¯¹é½ï¼ˆAGAï¼‰**ï¼šæ ¹æ®æ¿€æ´»å‡½æ•°çš„è¾“å‡ºï¼Œè¿›ä¸€æ­¥è°ƒæ•´ä¸‰å€¼ç½‘æ ¼ï¼Œä½¿å…¶æ›´å¥½åœ°åŒ¹é…å…¨ç²¾åº¦è¾“å‡ºã€‚
4. **ç»“æ„ç›¸ä¼¼æ€§é‡æ’åºï¼ˆSSRï¼‰**ï¼šé€šè¿‡åˆ†ææƒé‡çŸ©é˜µåˆ—é—´çš„ç»“æ„ç›¸ä¼¼æ€§ï¼Œé‡æ–°æ’åˆ—æƒé‡ï¼Œä»¥å‡è½»å¼‚å¸¸å€¼çš„å½±å“ï¼Œå¹¶ç®€åŒ–é‡åŒ–è¿‡ç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š
1. **ä¸¤é˜¶æ®µç»†åŒ–æµç¨‹ï¼ˆITF+AGAï¼‰**ï¼šé€šè¿‡è¿­ä»£ä¼˜åŒ–ä¸‰å€¼ç½‘æ ¼å’Œæ¿€æ´»æ„ŸçŸ¥å¯¹é½ï¼Œæ›´ç²¾ç¡®åœ°æ‹Ÿåˆå…¨ç²¾åº¦æƒé‡ï¼Œæ˜¾è‘—é™ä½äº†é‡åŒ–è¯¯å·®ã€‚
2. **ç»“æ„ç›¸ä¼¼æ€§é‡æ’åºï¼ˆSSRï¼‰**ï¼šåˆ©ç”¨æƒé‡çŸ©é˜µçš„ç»“æ„ä¿¡æ¯ï¼Œç¼“è§£äº†å¼‚å¸¸å€¼å¸¦æ¥çš„é‡åŒ–å›°éš¾ï¼Œæé«˜äº†é‡åŒ–ç¨³å®šæ€§ã€‚
3. **éå¯¹ç§°ä¸‰å€¼é‡åŒ–å™¨**ï¼šç›¸è¾ƒäºå¯¹ç§°é‡åŒ–å™¨ï¼Œéå¯¹ç§°é‡åŒ–å™¨èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”LLMsä¸­éå¯¹ç§°çš„æƒé‡åˆ†å¸ƒã€‚

**å…³é”®è®¾è®¡**ï¼š
1. **è¿­ä»£ä¸‰å€¼æ‹Ÿåˆï¼ˆITFï¼‰**ï¼šé€šè¿‡è¿­ä»£çš„æ–¹å¼ï¼Œäº¤æ›¿ä¼˜åŒ–ä¸‰å€¼ç½‘æ ¼çš„é˜ˆå€¼å’Œèˆå…¥ç­–ç•¥ï¼Œä»¥æœ€å°åŒ–é‡åŒ–è¯¯å·®ã€‚å…·ä½“æ¥è¯´ï¼Œé¦–å…ˆå›ºå®šé˜ˆå€¼ï¼Œæ ¹æ®é˜ˆå€¼è¿›è¡Œèˆå…¥ï¼Œç„¶åå›ºå®šèˆå…¥ç»“æœï¼Œä¼˜åŒ–é˜ˆå€¼ï¼Œé‡å¤æ­¤è¿‡ç¨‹ã€‚
2. **æ¿€æ´»æ„ŸçŸ¥ç½‘æ ¼å¯¹é½ï¼ˆAGAï¼‰**ï¼šæ ¹æ®æ¿€æ´»å‡½æ•°çš„ç»Ÿè®¡ä¿¡æ¯ï¼Œè°ƒæ•´ä¸‰å€¼ç½‘æ ¼çš„é˜ˆå€¼ï¼Œä½¿å…¶æ›´å¥½åœ°åŒ¹é…æ¿€æ´»å‡½æ•°çš„è¾“å‡ºåˆ†å¸ƒã€‚ä¾‹å¦‚ï¼Œå¯ä»¥ä½¿ç”¨æ¿€æ´»å‡½æ•°çš„å‡å€¼å’Œæ–¹å·®æ¥è°ƒæ•´é˜ˆå€¼ã€‚
3. **ç»“æ„ç›¸ä¼¼æ€§é‡æ’åºï¼ˆSSRï¼‰**ï¼šä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ç­‰æŒ‡æ ‡æ¥è¡¡é‡æƒé‡çŸ©é˜µåˆ—é—´çš„ç»“æ„ç›¸ä¼¼æ€§ï¼Œç„¶åæ ¹æ®ç›¸ä¼¼æ€§å¯¹åˆ—è¿›è¡Œé‡æ’åºã€‚é‡æ’åºçš„ç›®æ ‡æ˜¯ä½¿ç›¸ä¼¼çš„åˆ—ç›¸é‚»ï¼Œä»è€Œå‡å°‘å¼‚å¸¸å€¼çš„å½±å“ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒPT$^2$-LLMåœ¨å¤šä¸ªLLMæ¨¡å‹ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨Llama-7Bæ¨¡å‹ä¸Šï¼ŒPT$^2$-LLMå®ç°äº†ä¸SOTA 2-bit PTQæ–¹æ³•ç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶å†…å­˜æˆæœ¬æ›´ä½ã€‚æ­¤å¤–ï¼ŒPT$^2$-LLMè¿˜åŠ é€Ÿäº†é¢„å¡«å……å’Œè§£ç è¿‡ç¨‹ï¼Œå®ç°äº†ç«¯åˆ°ç«¯çš„é€Ÿåº¦æå‡ã€‚å…·ä½“è€Œè¨€ï¼Œä¸å…¨ç²¾åº¦æ¨¡å‹ç›¸æ¯”ï¼ŒPT$^2$-LLMåœ¨è§£ç é˜¶æ®µå®ç°äº†æ˜¾è‘—çš„åŠ é€Ÿã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

PT$^2$-LLMå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ç”¨äºåœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šéƒ¨ç½²å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä¾‹å¦‚ç§»åŠ¨è®¾å¤‡ã€åµŒå…¥å¼ç³»ç»Ÿå’Œè¾¹ç¼˜è®¡ç®—è®¾å¤‡ã€‚è¯¥æŠ€æœ¯èƒ½å¤Ÿæ˜¾è‘—é™ä½LLMçš„å­˜å‚¨éœ€æ±‚å’Œè®¡ç®—å¤æ‚åº¦ï¼Œä»è€Œå®ç°æ›´å¿«çš„æ¨ç†é€Ÿåº¦å’Œæ›´ä½çš„åŠŸè€—ã€‚æ­¤å¤–ï¼ŒPT$^2$-LLMè¿˜å¯ä»¥åº”ç”¨äºäº‘è®¡ç®—å¹³å°ï¼Œä»¥æé«˜LLMæœåŠ¡çš„æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have shown impressive capabilities across diverse tasks, but their large memory and compute demands hinder deployment. Ternarization has gained attention as a promising compression technique, delivering substantial size reduction and high computational efficiency. However, its potential in the post-training quantization (PTQ) setting remains underexplored, due to the challenge of training-free parameter optimization and the quantization difficulty posed by outliers and dispersed weights. To address these issues, we propose PT$^2$-LLM, a post-training ternarization framework tailored for LLMs. At its core is an Asymmetric Ternary Quantizer equipped with a two-stage refinement pipeline: (1) Iterative Ternary Fitting (ITF), which alternates between optimal ternary grid construction and flexible rounding to minimize quantization error, and (2) Activation-aware Grid Alignment (AGA), which further refines the ternary grid to better match full-precision outputs. In addition, we propose a plug-and-play Structural Similarity-based Reordering (SSR) strategy that leverages inter-column structural similarity to ease quantization and mitigate outlier effects, further enhancing overall performance. Extensive experiments demonstrate that PT$^2$-LLM delivers competitive performance against state-of-the-art (SOTA) 2-bit PTQ methods with lower memory cost, while also accelerating both prefill and decoding to achieve end-to-end speedup. The code and models will be available at https://github.com/XIANGLONGYAN/PT2-LLM.

