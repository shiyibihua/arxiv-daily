---
layout: default
title: C$^2$GSPG: Confidence-calibrated Group Sequence Policy Gradient towards Self-aware Reasoning
---

# C$^2$GSPG: Confidence-calibrated Group Sequence Policy Gradient towards Self-aware Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.23129" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.23129v1</a>
  <a href="https://arxiv.org/pdf/2509.23129.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.23129v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.23129v1', 'C$^2$GSPG: Confidence-calibrated Group Sequence Policy Gradient towards Self-aware Reasoning')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Haotian Liu, Shuo Wang, Hongteng Xu

**ÂàÜÁ±ª**: cs.LG, cs.AI, cs.CL

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-27

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/HaotianLiu123/CCGSPG)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫C$^2$GSPGÔºåËß£ÂÜ≥Âº∫ÂåñÂ≠¶‰π†Êé®ÁêÜÊ®°Âûã‰∏≠ÁöÑËøáÂ∫¶Ëá™‰ø°ÈóÆÈ¢òÔºåÊèêÂçáËá™Áü•Êé®ÁêÜËÉΩÂäõ„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Âº∫ÂåñÂ≠¶‰π†` `Êé®ÁêÜÊ®°Âûã` `ÁΩÆ‰ø°Â∫¶Ê†°ÂáÜ` `Á≠ñÁï•Ê¢ØÂ∫¶` `Ëá™Áü•Êé®ÁêÜ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂü∫‰∫éÂº∫ÂåñÂ≠¶‰π†ÁöÑÊé®ÁêÜÊ®°ÂûãÔºåÂ¶ÇGRPOÔºåÂ≠òÂú®ËøáÂ∫¶Ëá™‰ø°ÈóÆÈ¢òÔºåÈòªÁ¢ç‰∫ÜÊ®°ÂûãÂÆûÁé∞Ëá™Áü•Êé®ÁêÜ„ÄÇ
2. C$^2$GSPGÈÄöËøáÁªÑÂ∫èÂàóÁ≠ñÁï•Ê¢ØÂ∫¶Ê°ÜÊû∂Ê∂àÈô§tokenÁ∫ßÂà´ÂÅèÂ∑ÆÔºåÂπ∂ÂºïÂÖ•ÁΩÆ‰ø°Â∫¶Ê†°ÂáÜÊ≠£ÂàôÂåñÂô®ÔºåÂ∞ÜÊ®°ÂûãÁΩÆ‰ø°Â∫¶‰∏éÂ∫èÂàóÂ•ñÂä±ÂØπÈΩê„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåC$^2$GSPGÂú®ÈÄªËæëÂíåÊï∞Â≠¶Êé®ÁêÜ‰ªªÂä°‰∏≠ÔºåÊòæËëóÊèêÂçá‰∫ÜÊé®ÁêÜÂáÜÁ°ÆÊÄßÔºåÂπ∂ÊúâÊïàÊ†°ÂáÜ‰∫ÜÊ®°ÂûãÁΩÆ‰ø°Â∫¶„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁΩÆ‰ø°Â∫¶Ê†°ÂáÜÁöÑÁªÑÂ∫èÂàóÁ≠ñÁï•Ê¢ØÂ∫¶ÊñπÊ≥ïC$^2$GSPGÔºåÊó®Âú®ÊèêÂçáÊé®ÁêÜÊÄßËÉΩÂπ∂ÊäëÂà∂ËøáÂ∫¶Ëá™‰ø°Ôºå‰ªéËÄåÂÆûÁé∞Ëá™Áü•Êé®ÁêÜÊ®°Âûã„ÄÇËØ•ÊñπÊ≥ïÂü∫‰∫éÁªÑÂ∫èÂàóÁ≠ñÁï•Ê¢ØÂ∫¶(GSPG)Ê°ÜÊû∂ÔºåÊ∂àÈô§‰∫ÜGRPOÂèäÂÖ∂Âèò‰Ωì‰∏≠Â∏∏ËßÅÁöÑtokenÁ∫ßÂà´ÂÅèÂ∑Æ„ÄÇÈÄöËøá‰ΩøÁî®ÂΩí‰∏ÄÂåñÁöÑÂ∫èÂàóÁ∫ßÂà´Ê¶ÇÁéáÂÆö‰πâÊ®°ÂûãÁΩÆ‰ø°Â∫¶ÔºåÂπ∂Â∫îÁî®‰∫§ÂèâÁÜµÊ≠£ÂàôÂåñÂô®Â∞ÜÊ®°ÂûãÁΩÆ‰ø°Â∫¶Ê†°ÂáÜÂà∞Â∫èÂàóÂ•ñÂä±„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÁΩÆ‰ø°Â∫¶Ê†°ÂáÜÊ≠£ÂàôÂåñÂô®ÂíåGSPGÂú®‰∫åÂÖÉÂ•ñÂä±‰∏ãÂçèÂêåÂ∑•‰Ωú„ÄÇÂØπ‰∫éÈùû‰∫åÂÖÉÂ•ñÂä±ÔºåÈááÁî®ÈùûÁ∫øÊÄßÂ•ñÂä±ÂΩí‰∏ÄÂåñÂíåËá™ÈÄÇÂ∫îÊ≠£ÂàôÂåñÂô®Ë£ÅÂâ™Ôºå‰ª•ÂáèËΩª‰∏§‰∏™ÁõÆÊ†á‰πãÈó¥ÁöÑÊΩúÂú®ÂÜ≤Á™Å„ÄÇÂú®ÈÄªËæëÂíåÊï∞Â≠¶Êé®ÁêÜ‰ªªÂä°‰∏≠ÂØπÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãËøõË°åÂêéËÆ≠ÁªÉÔºåÂÆûÈ™åÁªìÊûúË°®ÊòéC$^2$GSPGÂú®Êé®ÁêÜÂáÜÁ°ÆÊÄßÂíåÁΩÆ‰ø°Â∫¶Ê†°ÂáÜÊñπÈù¢Âùá‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇ‰ª£Á†ÅÂ∑≤ÂºÄÊ∫ê„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÂü∫‰∫éÂº∫ÂåñÂ≠¶‰π†ÁöÑÊé®ÁêÜÊ®°ÂûãÔºåÁâπÂà´ÊòØGroup Relative Policy Optimization (GRPO)ÂèäÂÖ∂Âèò‰ΩìÔºåÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÂÆπÊòì‰∫ßÁîüËøáÂ∫¶Ëá™‰ø°ÁöÑÈóÆÈ¢ò„ÄÇËøôÁßçËøáÂ∫¶Ëá™‰ø°‰ºöÂØºËá¥Ê®°ÂûãÂú®Êé®ÁêÜÊó∂ÂÅöÂá∫ÈîôËØØÁöÑÂà§Êñ≠ÔºåÂπ∂‰∏îÊó†Ê≥ïÂáÜÁ°ÆËØÑ‰º∞Ëá™Ë∫´Êé®ÁêÜÁªìÊûúÁöÑÂèØÈù†ÊÄßÔºå‰ªéËÄåÈòªÁ¢ç‰∫ÜËá™Áü•Êé®ÁêÜËÉΩÂäõÁöÑÊèêÂçá„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏Âú®tokenÁ∫ßÂà´ËøõË°åÁ≠ñÁï•‰ºòÂåñÔºåÂÆπÊòìÂºïÂÖ•ÂÅèÂ∑ÆÔºåÂΩ±ÂìçÊï¥‰ΩìÊé®ÁêÜÊïàÊûú„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöC$^2$GSPGÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÁΩÆ‰ø°Â∫¶Ê†°ÂáÜÊù•Ëß£ÂÜ≥ËøáÂ∫¶Ëá™‰ø°ÈóÆÈ¢ò„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåËØ•ÊñπÊ≥ïÈ¶ñÂÖà‰ΩøÁî®Â∫èÂàóÁ∫ßÂà´ÁöÑÊ¶ÇÁéáÊù•ÂÆö‰πâÊ®°ÂûãÂØπ‰∫éÊØè‰∏™Êé®ÁêÜÈóÆÈ¢òÁöÑÁΩÆ‰ø°Â∫¶ÔºåÁÑ∂ÂêéÈÄöËøá‰∏Ä‰∏™‰∫§ÂèâÁÜµÊ≠£ÂàôÂåñÂô®ÔºåÂ∞ÜËøô‰∏™ÁΩÆ‰ø°Â∫¶Ê†°ÂáÜÂà∞Â∫èÂàóÁöÑÂ•ñÂä±„ÄÇËøôÊ†∑ÂÅöÁöÑÁõÆÁöÑÊòØËÆ©Ê®°ÂûãÂú®Ëé∑ÂæóÈ´òÂ•ñÂä±ÁöÑÂ∫èÂàó‰∏äÊõ¥Âä†Ëá™‰ø°ÔºåËÄåÂú®Ëé∑Âæó‰ΩéÂ•ñÂä±ÁöÑÂ∫èÂàó‰∏äÊõ¥Âä†Ë∞®ÊÖéÔºå‰ªéËÄåÊèêÂçáÊ®°ÂûãÁöÑËá™Áü•ËÉΩÂäõ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöC$^2$GSPGÂåÖÂê´‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÁªÑÊàêÈÉ®ÂàÜÔºö1) Group Sequence Policy Gradient (GSPG)Ê°ÜÊû∂ÔºöÁî®‰∫éÂ≠¶‰π†Êé®ÁêÜÊ®°ÂûãÔºåÊ∂àÈô§tokenÁ∫ßÂà´ÁöÑÂÅèÂ∑Æ„ÄÇ2) ÁΩÆ‰ø°Â∫¶ÂÆö‰πâÔºö‰ΩøÁî®ÂΩí‰∏ÄÂåñÁöÑÂ∫èÂàóÁ∫ßÂà´Ê¶ÇÁéáÊù•ÂÆö‰πâÊ®°ÂûãÂØπ‰∫éÊØè‰∏™Êé®ÁêÜÈóÆÈ¢òÁöÑÁΩÆ‰ø°Â∫¶„ÄÇ3) ÁΩÆ‰ø°Â∫¶Ê†°ÂáÜÊ≠£ÂàôÂåñÂô®Ôºö‰ΩøÁî®‰∫§ÂèâÁÜµÊçüÂ§±ÂáΩÊï∞ÔºåÂ∞ÜÊ®°ÂûãÁΩÆ‰ø°Â∫¶Ê†°ÂáÜÂà∞Â∫èÂàóÁöÑÂ•ñÂä±„ÄÇ4) ÈùûÁ∫øÊÄßÂ•ñÂä±ÂΩí‰∏ÄÂåñÂíåËá™ÈÄÇÂ∫îÊ≠£ÂàôÂåñÂô®Ë£ÅÂâ™ÔºöÁî®‰∫éÂ§ÑÁêÜÈùû‰∫åÂÖÉÂ•ñÂä±ÁöÑÊÉÖÂÜµÔºåÂáèËΩªÁΩÆ‰ø°Â∫¶Ê†°ÂáÜÂíåÁ≠ñÁï•‰ºòÂåñÁõÆÊ†á‰πãÈó¥ÁöÑÂÜ≤Á™Å„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöC$^2$GSPGÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÊèêÂá∫‰∫ÜÁΩÆ‰ø°Â∫¶Ê†°ÂáÜÊ≠£ÂàôÂåñÂô®ÔºåÂπ∂Â∞ÜÂÖ∂‰∏éGSPGÊ°ÜÊû∂Áõ∏ÁªìÂêà„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ï‰∏çÂêåÔºåC$^2$GSPGÁõ¥Êé•Âú®Â∫èÂàóÁ∫ßÂà´ËøõË°åÁΩÆ‰ø°Â∫¶Ê†°ÂáÜÔºåÈÅøÂÖç‰∫ÜtokenÁ∫ßÂà´ÂÅèÂ∑ÆÁöÑÂΩ±Âìç„ÄÇÊ≠§Â§ñÔºåC$^2$GSPGËøòÈíàÂØπÈùû‰∫åÂÖÉÂ•ñÂä±ÁöÑÊÉÖÂÜµÔºåÊèêÂá∫‰∫ÜÈùûÁ∫øÊÄßÂ•ñÂä±ÂΩí‰∏ÄÂåñÂíåËá™ÈÄÇÂ∫îÊ≠£ÂàôÂåñÂô®Ë£ÅÂâ™Á≠âÊäÄÊúØÔºåËøõ‰∏ÄÊ≠•ÊèêÂçá‰∫ÜÊñπÊ≥ïÁöÑÈ≤ÅÊ£íÊÄßÂíåÈÄÇÁî®ÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®‰∫åÂÖÉÂ•ñÂä±ÊÉÖÂÜµ‰∏ãÔºåÁΩÆ‰ø°Â∫¶Ê†°ÂáÜÊ≠£ÂàôÂåñÂô®ÂíåGSPGÁöÑÁõÆÊ†áÂáΩÊï∞Ê¢ØÂ∫¶ÊñπÂêë‰∏ÄËá¥ÔºåÂèØ‰ª•ÂçèÂêå‰ºòÂåñ„ÄÇÂØπ‰∫éÈùû‰∫åÂÖÉÂ•ñÂä±ÔºåÈááÁî®ÈùûÁ∫øÊÄßÂ•ñÂä±ÂΩí‰∏ÄÂåñÔºàÂÖ∑‰ΩìÂΩ¢ÂºèÊú™Áü•ÔºâÊù•Ë∞ÉÊï¥Â•ñÂä±ÂàÜÂ∏ÉÔºåÂπ∂‰ΩøÁî®Ëá™ÈÄÇÂ∫îÊ≠£ÂàôÂåñÂô®Ë£ÅÂâ™ÔºàÂÖ∑‰ΩìË£ÅÂâ™Á≠ñÁï•Êú™Áü•ÔºâÊù•ÈôêÂà∂Ê≠£ÂàôÂåñÂô®ÁöÑÂΩ±ÂìçÔºåÈÅøÂÖç‰∏éÁ≠ñÁï•‰ºòÂåñÁõÆÊ†á‰∫ßÁîüÂÜ≤Á™Å„ÄÇ‰∫§ÂèâÁÜµÊçüÂ§±ÂáΩÊï∞ÁöÑÂÖ∑‰ΩìÂΩ¢Âºè‰∏∫Ê†áÂáÜÂΩ¢ÂºèÔºåÁî®‰∫éË°°ÈáèÊ®°ÂûãÁΩÆ‰ø°Â∫¶‰∏éÁõÆÊ†áÂ•ñÂä±‰πãÈó¥ÁöÑÂ∑ÆÂºÇ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåC$^2$GSPGÂú®ÈÄªËæëÂíåÊï∞Â≠¶Êé®ÁêÜ‰ªªÂä°‰∏≠ÔºåÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÂú®Êüê‰∫õ‰ªªÂä°‰∏äÔºåC$^2$GSPGÁöÑÊé®ÁêÜÂáÜÁ°ÆÁéáÊèêÂçá‰∫ÜX%ÔºàÂÖ∑‰ΩìÊï∞ÊçÆÊú™Áü•ÔºâÔºåÂπ∂‰∏îÂú®ÁΩÆ‰ø°Â∫¶Ê†°ÂáÜÊñπÈù¢‰πüÂèñÂæó‰∫ÜÊòæËëóÁöÑÊîπËøõ„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåC$^2$GSPGËÉΩÂ§üÊúâÊïàÂú∞ÊèêÂçáÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõÂíåËá™Áü•ËÉΩÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

C$^2$GSPGÂèØÂ∫îÁî®‰∫éÂêÑÁßçÈúÄË¶ÅÈÄªËæëÊé®ÁêÜÂíåÊï∞Â≠¶Êé®ÁêÜÁöÑÂú∫ÊôØÔºå‰æãÂ¶ÇÊô∫ËÉΩÈóÆÁ≠î„ÄÅÁü•ËØÜÂõæË∞±Êé®ÁêÜ„ÄÅ‰ª£Á†ÅÁîüÊàêÁ≠â„ÄÇÈÄöËøáÊèêÂçáÊ®°ÂûãÁöÑÊé®ÁêÜÂáÜÁ°ÆÊÄßÂíåÁΩÆ‰ø°Â∫¶Ê†°ÂáÜËÉΩÂäõÔºåÂèØ‰ª•ÊèêÈ´òËøô‰∫õÂ∫îÁî®Á≥ªÁªüÁöÑÂèØÈù†ÊÄßÂíåÂèØËß£ÈáäÊÄßÔºåÂπ∂‰∏∫Êú™Êù•ÁöÑËá™Áü•Êé®ÁêÜÊ®°ÂûãÁ†îÁ©∂Â•†ÂÆöÂü∫Á°Ä„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Reinforcement Learning (RL) methods, exemplified by Group Relative Policy Optimization (GRPO) and its variants, play a central role in developing reasoning models. However, these methods often suffer from a critical overconfidence issue, which prevents them from achieving self-aware reasoning models. In this study, we propose a simple yet effective confidence-calibration group sequence policy gradient method, called C$^2$GSPG, which simultaneously enhances reasoning performance while suppressing overconfidence. In principle, we propose a Group Sequence Policy Gradient (GSPG) framework for learning reasoning models, which eliminates the token-level bias commonly appearing in GRPO and its variants. In this framework, we define the model confidence for each reasoning problem using the normalized sequence-level probability, and then apply a cross-entropy regularizer to calibrate the model confidence to the sequence's reward. We demonstrate that the confidence calibration regularizer and GSPG are collaborative for binary rewards, as their objectives always share the same gradient direction. For non-binary rewards, we apply nonlinear reward normalization and adaptive regularizer clipping, mitigating the potential conflict between the two objectives. Applying C$^2$GSPG to post-train large language models in logical and mathematical reasoning tasks, we show its superiority over state-of-the-art methods in both reasoning accuracy and confidence calibration. The code of C$^2$GSPG is available at https://github.com/HaotianLiu123/CCGSPG.

