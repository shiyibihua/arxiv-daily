---
layout: default
title: TimeExpert: Boosting Long Time Series Forecasting with Temporal Mix of Experts
---

# TimeExpert: Boosting Long Time Series Forecasting with Temporal Mix of Experts

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.23145" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.23145v1</a>
  <a href="https://arxiv.org/pdf/2509.23145.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.23145v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.23145v1', 'TimeExpert: Boosting Long Time Series Forecasting with Temporal Mix of Experts')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiaowen Ma, Shuning Ge, Fan Yang, Xiangyu Li, Yun Chen, Mengting Ma, Wei Zhang, Zhipeng Liu

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-27

**å¤‡æ³¨**: Under Review

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/xwmaxwma/TimeExpert)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ—¶é—´æ··åˆä¸“å®¶ï¼ˆTMOEï¼‰æœºåˆ¶ï¼Œæå‡é•¿æ—¶åºé¢„æµ‹ç²¾åº¦ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿æ—¶åºé¢„æµ‹` `æ—¶é—´åºåˆ—åˆ†æ` `Transformer` `æ³¨æ„åŠ›æœºåˆ¶` `æ··åˆä¸“å®¶æ¨¡å‹` `æ»åæ•ˆåº”` `å¼‚å¸¸æ£€æµ‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰Transformeræ¨¡å‹åœ¨é•¿æ—¶åºé¢„æµ‹ä¸­ï¼Œæ— æ³•æœ‰æ•ˆå¤„ç†æ»åæ•ˆåº”å’Œå¼‚å¸¸ç‰‡æ®µå¸¦æ¥çš„å™ªå£°å¹²æ‰°ã€‚
2. è®ºæ–‡æå‡ºæ—¶é—´æ··åˆä¸“å®¶ï¼ˆTMOEï¼‰æœºåˆ¶ï¼Œé€šè¿‡å±€éƒ¨ä¸“å®¶é€‰æ‹©å’Œå…¨å±€ä¸“å®¶å…±äº«ï¼Œå®ç°è‡ªé€‚åº”ä¸Šä¸‹æ–‡èšåˆã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒTimeExpertå’ŒTimeExpert-Gåœ¨å¤šä¸ªé•¿æ—¶åºé¢„æµ‹åŸºå‡†ä¸Šè¶…è¶Šäº†ç°æœ‰æœ€ä½³æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºäºTransformerçš„æ¶æ„é€šè¿‡å¯¹æ‰€æœ‰æ—¶é—´æˆ³è¿›è¡Œå…¨å±€æ³¨æ„åŠ›å»ºæ¨¡ï¼Œåœ¨æ—¶é—´åºåˆ—å»ºæ¨¡ä¸­å æ®ä¸»å¯¼åœ°ä½ï¼Œä½†å…¶åˆšæ€§çš„â€œä¸€åˆ€åˆ‡â€ä¸Šä¸‹æ–‡èšåˆæ— æ³•è§£å†³å®é™…æ•°æ®ä¸­çš„ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šï¼ˆ1ï¼‰å›ºæœ‰çš„æ»åæ•ˆåº”ï¼Œå³å†å²æ—¶é—´æˆ³ä¸æŸ¥è¯¢çš„ç›¸å…³æ€§åŠ¨æ€å˜åŒ–ï¼›ï¼ˆ2ï¼‰å¼‚å¸¸ç‰‡æ®µï¼Œå¼•å…¥å™ªå£°ä¿¡å·ï¼Œé™ä½é¢„æµ‹ç²¾åº¦ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ³¨æ„åŠ›çº§åˆ«æœºåˆ¶â€”â€”æ—¶é—´æ··åˆä¸“å®¶ï¼ˆTMOEï¼‰ï¼Œå®ƒå°†é”®-å€¼ï¼ˆK-Vï¼‰å¯¹é‡æ–°æ„æƒ³ä¸ºå±€éƒ¨ä¸“å®¶ï¼ˆæ¯ä¸ªä¸“å®¶ä¸“é—¨å¤„ç†ä¸åŒçš„æ—¶é—´ä¸Šä¸‹æ–‡ï¼‰ï¼Œå¹¶é€šè¿‡å¯¹ä¸ç›¸å…³æ—¶é—´æˆ³çš„å±€éƒ¨è¿‡æ»¤ï¼Œä¸ºæ¯ä¸ªæŸ¥è¯¢æ‰§è¡Œè‡ªé€‚åº”ä¸“å®¶é€‰æ‹©ã€‚ä½œä¸ºå¯¹è¿™ç§å±€éƒ¨é€‚åº”çš„è¡¥å……ï¼Œå…±äº«çš„å…¨å±€ä¸“å®¶ä¿ç•™äº†Transformeråœ¨æ•è·é•¿æœŸä¾èµ–å…³ç³»æ–¹é¢çš„ä¼˜åŠ¿ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†æµè¡Œçš„æ—¶åºTransformeræ¡†æ¶ï¼ˆå³PatchTSTå’ŒTimerï¼‰ä¸­çš„vanillaæ³¨æ„åŠ›æœºåˆ¶æ›¿æ¢ä¸ºTMOEï¼Œæ— éœ€é¢å¤–çš„ç»“æ„ä¿®æ”¹ï¼Œä»è€Œäº§ç”Ÿæˆ‘ä»¬çš„ç‰¹å®šç‰ˆæœ¬TimeExpertå’Œé€šç”¨ç‰ˆæœ¬TimeExpert-Gã€‚åœ¨ä¸ƒä¸ªçœŸå®ä¸–ç•Œçš„é•¿æœŸé¢„æµ‹åŸºå‡†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒTimeExpertå’ŒTimeExpert-Gä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šé•¿æ—¶åºé¢„æµ‹ä»»åŠ¡ä¸­ï¼Œç°æœ‰åŸºäºTransformerçš„æ¨¡å‹éš¾ä»¥æœ‰æ•ˆå¤„ç†ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šä¸€æ˜¯æ—¶é—´åºåˆ—å›ºæœ‰çš„æ»åæ•ˆåº”ï¼Œå³ä¸åŒå†å²æ—¶é—´ç‚¹å¯¹å½“å‰é¢„æµ‹çš„å½±å“ç¨‹åº¦éšæ—¶é—´åŠ¨æ€å˜åŒ–ï¼›äºŒæ˜¯å¼‚å¸¸ç‰‡æ®µçš„å¹²æ‰°ï¼Œè¿™äº›ç‰‡æ®µä¼šå¼•å…¥å™ªå£°ï¼Œé™ä½é¢„æµ‹ç²¾åº¦ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸é‡‡ç”¨å…¨å±€æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¯¹æ‰€æœ‰æ—¶é—´ç‚¹ä¸€è§†åŒä»ï¼Œæ— æ³•è‡ªé€‚åº”åœ°é€‰æ‹©ç›¸å…³çš„æ—¶é—´ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†Transformerä¸­çš„é”®-å€¼ï¼ˆK-Vï¼‰å¯¹è§†ä¸ºä¸åŒçš„â€œä¸“å®¶â€ï¼Œæ¯ä¸ªä¸“å®¶ä¸“æ³¨äºä¸åŒçš„æ—¶é—´ä¸Šä¸‹æ–‡ã€‚é€šè¿‡å¼•å…¥ä¸€ä¸ªé€‰æ‹©æœºåˆ¶ï¼Œæ¨¡å‹å¯ä»¥æ ¹æ®å½“å‰æŸ¥è¯¢è‡ªé€‚åº”åœ°é€‰æ‹©ç›¸å…³çš„ä¸“å®¶ï¼Œä»è€Œå®ç°æ›´ç²¾ç»†çš„ä¸Šä¸‹æ–‡èšåˆã€‚åŒæ—¶ï¼Œä¿ç•™ä¸€ä¸ªå…¨å±€ä¸“å®¶æ¥æ•æ‰é•¿ç¨‹ä¾èµ–å…³ç³»ï¼Œå¼¥è¡¥å±€éƒ¨ä¸“å®¶å¯èƒ½å¿½ç•¥çš„å…¨å±€ä¿¡æ¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTimeExpertçš„æ ¸å¿ƒåœ¨äºTMOEï¼ˆTemporal Mix of Expertsï¼‰æ¨¡å—ï¼Œå®ƒæ›¿æ¢äº†æ ‡å‡†Transformerä¸­çš„æ³¨æ„åŠ›æœºåˆ¶ã€‚TMOEåŒ…å«å¤šä¸ªå±€éƒ¨ä¸“å®¶å’Œä¸€ä¸ªå…¨å±€ä¸“å®¶ã€‚å¯¹äºæ¯ä¸ªæŸ¥è¯¢ï¼ŒTMOEé¦–å…ˆè®¡ç®—æŸ¥è¯¢ä¸æ¯ä¸ªå±€éƒ¨ä¸“å®¶çš„ç›¸å…³æ€§ï¼Œç„¶åæ ¹æ®ç›¸å…³æ€§æƒé‡é€‰æ‹©åˆé€‚çš„ä¸“å®¶è¿›è¡ŒåŠ æƒèšåˆã€‚å…¨å±€ä¸“å®¶åˆ™ç›´æ¥å‚ä¸æ‰€æœ‰æŸ¥è¯¢çš„ä¸Šä¸‹æ–‡èšåˆã€‚æœ€ç»ˆçš„è¾“å‡ºæ˜¯å±€éƒ¨ä¸“å®¶å’Œå…¨å±€ä¸“å®¶çš„åŠ æƒç»„åˆã€‚

**å…³é”®åˆ›æ–°**ï¼šTMOEçš„å…³é”®åˆ›æ–°åœ¨äºå°†æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„K-Vå¯¹é‡æ–°è§£é‡Šä¸ºå±€éƒ¨ä¸“å®¶ï¼Œå¹¶å¼•å…¥äº†è‡ªé€‚åº”çš„ä¸“å®¶é€‰æ‹©æœºåˆ¶ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæ ¹æ®æŸ¥è¯¢åŠ¨æ€åœ°é€‰æ‹©æœ€ç›¸å…³çš„å†å²ä¿¡æ¯ï¼Œä»è€Œæ›´å¥½åœ°å¤„ç†æ»åæ•ˆåº”å’Œå¼‚å¸¸ç‰‡æ®µã€‚ä¸ä¼ ç»Ÿçš„æ³¨æ„åŠ›æœºåˆ¶ç›¸æ¯”ï¼ŒTMOEæ›´åŠ çµæ´»å’Œé«˜æ•ˆã€‚

**å…³é”®è®¾è®¡**ï¼šTMOEçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å±€éƒ¨ä¸“å®¶çš„æ•°é‡å’Œç»´åº¦ï¼›2) ä¸“å®¶é€‰æ‹©æœºåˆ¶ï¼Œä¾‹å¦‚ä½¿ç”¨Softmaxå‡½æ•°è®¡ç®—ä¸“å®¶æƒé‡ï¼›3) å…¨å±€ä¸“å®¶çš„æƒé‡ï¼Œå¯ä»¥é€šè¿‡å­¦ä¹ å¾—åˆ°æˆ–è®¾ç½®ä¸ºå›ºå®šå€¼ï¼›4) æŸå¤±å‡½æ•°ï¼Œé™¤äº†é¢„æµ‹è¯¯å·®å¤–ï¼Œè¿˜å¯ä»¥åŠ å…¥æ­£åˆ™åŒ–é¡¹ï¼Œé¼“åŠ±ä¸“å®¶ä¹‹é—´çš„å¤šæ ·æ€§ã€‚è®ºæ–‡å°†TMOEåº”ç”¨äºPatchTSTå’ŒTimerç­‰ç°æœ‰Transformeræ¨¡å‹ï¼ŒéªŒè¯äº†å…¶é€šç”¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒTimeExpertå’ŒTimeExpert-Gåœ¨ä¸ƒä¸ªçœŸå®ä¸–ç•Œçš„é•¿æ—¶åºé¢„æµ‹åŸºå‡†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸäº›æ•°æ®é›†ä¸Šï¼ŒTimeExpertçš„é¢„æµ‹è¯¯å·®é™ä½äº†10%ä»¥ä¸Šã€‚æ­¤å¤–ï¼ŒTimeExpert-Gä½œä¸ºé€šç”¨ç‰ˆæœ¬ï¼Œåœ¨ä¸åŒæ•°æ®é›†ä¸Šå‡è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼ŒéªŒè¯äº†TMOEçš„æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºéœ€è¦é•¿æ—¶åºé¢„æµ‹çš„é¢†åŸŸï¼Œä¾‹å¦‚ï¼šç”µåŠ›è´Ÿè·é¢„æµ‹ã€é‡‘èå¸‚åœºåˆ†æã€ä¾›åº”é“¾ç®¡ç†ã€äº¤é€šæµé‡é¢„æµ‹ã€æ°”å€™é¢„æµ‹ç­‰ã€‚é€šè¿‡æ›´å‡†ç¡®åœ°é¢„æµ‹æœªæ¥è¶‹åŠ¿ï¼Œå¯ä»¥å¸®åŠ©ä¼ä¸šå’Œæœºæ„åšå‡ºæ›´æ˜æ™ºçš„å†³ç­–ï¼Œæé«˜è¿è¥æ•ˆç‡ï¼Œé™ä½é£é™©ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Transformer-based architectures dominate time series modeling by enabling global attention over all timestamps, yet their rigid 'one-size-fits-all' context aggregation fails to address two critical challenges in real-world data: (1) inherent lag effects, where the relevance of historical timestamps to a query varies dynamically; (2) anomalous segments, which introduce noisy signals that degrade forecasting accuracy. To resolve these problems, we propose the Temporal Mix of Experts (TMOE), a novel attention-level mechanism that reimagines key-value (K-V) pairs as local experts (each specialized in a distinct temporal context) and performs adaptive expert selection for each query via localized filtering of irrelevant timestamps. Complementing this local adaptation, a shared global expert preserves the Transformer's strength in capturing long-range dependencies. We then replace the vanilla attention mechanism in popular time-series Transformer frameworks (i.e., PatchTST and Timer) with TMOE, without extra structural modifications, yielding our specific version TimeExpert and general version TimeExpert-G. Extensive experiments on seven real-world long-term forecasting benchmarks demonstrate that TimeExpert and TimeExpert-G outperform state-of-the-art methods. Code is available at https://github.com/xwmaxwma/TimeExpert.

