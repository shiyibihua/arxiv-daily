---
layout: default
title: Solve Smart, Not Often: Policy Learning for Costly MILP Re-solving
---

# Solve Smart, Not Often: Policy Learning for Costly MILP Re-solving

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.23470" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.23470v1</a>
  <a href="https://arxiv.org/pdf/2509.23470.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.23470v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.23470v1', 'Solve Smart, Not Often: Policy Learning for Costly MILP Re-solving')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Rui Ai, Hugo De Oliveira Barbalho, Sirui Li, Alexei Robsky, David Simchi-Levi, Ishai Menache

**ÂàÜÁ±ª**: cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-27

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫POCÊ°ÜÊû∂ÔºåÈÄöËøáÁ≠ñÁï•Â≠¶‰π†‰ºòÂåñ‰ª£‰ª∑ÊïèÊÑüÁöÑMILPÈáçÊ±ÇËß£ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Ê∑∑ÂêàÊï¥Êï∞Á∫øÊÄßËßÑÂàí` `Âº∫ÂåñÂ≠¶‰π†` `ËøëÁ´ØÁ≠ñÁï•‰ºòÂåñ` `ÂèòÂåñÁÇπÊ£ÄÊµã` `ÂÆûÊó∂‰ºòÂåñ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. ÂÆûÊó∂ËøêËê•‰∏≠È¢ëÁπÅÊ±ÇËß£ËÆ°ÁÆóÂØÜÈõÜÂûãMILPÈóÆÈ¢òÔºåÁé∞ÊúâÊñπÊ≥ïÂú®Âπ≥Ë°°Ê±ÇËß£ÊàêÊú¨ÂíåÊÄßËÉΩÊñπÈù¢Â≠òÂú®‰∏çË∂≥„ÄÇ
2. ÊèêÂá∫POCÊ°ÜÊû∂ÔºåÁªìÂêàËøëÁ´ØÁ≠ñÁï•‰ºòÂåñÂíåÂèòÂåñÁÇπÊ£ÄÊµãÔºåÁ≥ªÁªüÂú∞Ëß£ÂÜ≥MILPÈáçÊ±ÇËß£Êó∂Êú∫ÈÄâÊã©ÈóÆÈ¢ò„ÄÇ
3. Âú®ÂêàÊàêÂíåÁúüÂÆûÊï∞ÊçÆÈõÜ‰∏äÈ™åËØÅÔºåPOCÊ°ÜÊû∂ÊÄßËÉΩ‰ºò‰∫éÁé∞ÊúâÂü∫Á∫ø2%-17%ÔºåÂπ∂Êèê‰æõÂÆûÊó∂MILPÂü∫ÂáÜ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Âú®ÂÆûÊó∂ËøêËê•‰∏≠Ôºå‰∏Ä‰∏™Â∏∏ËßÅÁöÑÊåëÊàòÊòØÂÜ≥ÂÆö‰ΩïÊó∂ÈáçÊñ∞Ê±ÇËß£‰ºòÂåñÈóÆÈ¢òÔºå‰ª•Âèä‰ΩïÊó∂ÁªßÁª≠‰ΩøÁî®Áé∞ÊúâËß£ÂÜ≥ÊñπÊ°à„ÄÇËôΩÁÑ∂Áé∞‰ª£Êï∞ÊçÆÂπ≥Âè∞ÂèØ‰ª•È´òÈ¢ëÁéáÂú∞Êî∂ÈõÜ‰ø°ÊÅØÔºå‰ΩÜËÆ∏Â§öÂÆûÊó∂ËøêËê•ÈúÄË¶ÅÈáçÂ§çÊ±ÇËß£ËÆ°ÁÆóÂØÜÈõÜÂûãÁöÑÊ∑∑ÂêàÊï¥Êï∞Á∫øÊÄßËßÑÂàíÔºàMILPÔºâÈóÆÈ¢ò„ÄÇÂõ†Ê≠§ÔºåÁ°ÆÂÆö‰ΩïÊó∂ÈáçÊñ∞Ê±ÇËß£ÊòØ‰∏Ä‰∏™ÂÖ∑ÊúâÈáçË¶ÅÁªèÊµé‰ª∑ÂÄºÁöÑÈóÆÈ¢ò„ÄÇËøô‰∏™ÈóÆÈ¢òÂ∏¶Êù•‰∫ÜÂá†‰∏™ÊåëÊàòÔºö1ÔºâÂ¶Ç‰ΩïË°®ÂæÅËß£ÁöÑÊúÄ‰ºòÊÄßÂíåÊ±ÇËß£ÊàêÊú¨Ôºõ2ÔºâÂ¶Ç‰ΩïÊ£ÄÊµãÁéØÂ¢ÉÂèòÂåñÂπ∂ÈÄâÊã©ÊúâÁõäÁöÑÊ†∑Êú¨Êù•Ê±ÇËß£MILPÔºõ3ÔºâÈâ¥‰∫éËæÉÈïøÁöÑÊó∂Èó¥ËåÉÂõ¥ÂíåÈùûMDPÁªìÊûÑÔºå‰º†ÁªüÁöÑÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÊñπÊ≥ïÊó†Ê≥ïÁõ¥Êé•Â∫îÁî®ÔºåÂπ∂‰∏îÂÆπÊòìÂá∫Áé∞‰ª∑ÂÄºÂáΩÊï∞ÁàÜÁÇ∏„ÄÇÁé∞ÊúâÊñáÁåÆ‰∏ªË¶ÅÂÖ≥Ê≥®ÂêØÂèëÂºèÊñπÊ≥ï„ÄÅ‰ΩéÊï∞ÊçÆËÆæÁΩÆÂíåÂπ≥ÊªëÁõÆÊ†áÔºåÂæàÂ∞ëÂÖ≥Ê≥®Â∏∏ËßÅÁöÑNP-hard MILP„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™Âêç‰∏∫Proximal Policy Optimization with Change Point DetectionÔºàPOCÔºâÁöÑÊ°ÜÊû∂ÔºåËØ•Ê°ÜÊû∂Á≥ªÁªüÂú∞Êèê‰æõ‰∫Ü‰∏ÄÁßçËß£ÂÜ≥ÊñπÊ°àÔºåÁî®‰∫éÂú®ÂÜ≥ÂÆöÈÄÇÂΩìÁöÑÈáçÊ±ÇËß£Êó∂Èó¥Êó∂Âπ≥Ë°°ÊÄßËÉΩÂíåÊàêÊú¨„ÄÇÁêÜËÆ∫‰∏äÔºåÊàë‰ª¨Âª∫Á´ã‰∫ÜÈáçÊ±ÇËß£Ê¨°Êï∞‰∏éÈáçÊ±ÇËß£ÊàêÊú¨‰πãÈó¥ÁöÑÂÖ≥Á≥ª„ÄÇ‰∏∫‰∫ÜÊµãËØïÊàë‰ª¨ÁöÑÊ°ÜÊû∂ÔºåÊàë‰ª¨ÁªÑË£Ö‰∫ÜÂÖ´‰∏™ÂêàÊàêÂíåÁúüÂÆû‰∏ñÁïåÁöÑÊï∞ÊçÆÈõÜÔºåÂπ∂Ë°®ÊòéPOCÂßãÁªà‰ºò‰∫éÁé∞ÊúâÂü∫Á∫ø2%-17%„ÄÇ‰Ωú‰∏∫‰∏ÄÈ°πÈ¢ùÂ§ñÁöÑÂ•ΩÂ§ÑÔºåÊàë‰ª¨ÁöÑÂ∑•‰ΩúÈÄöËøáÂºïÂÖ•ÂÆûÊó∂MILPÂü∫ÂáÜÂíåËØÑ‰º∞Ê†áÂáÜÂ°´Ë°•‰∫ÜÊñáÁåÆ‰∏≠ÁöÑÁ©∫ÁôΩ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥ÂÆûÊó∂ËøêËê•‰∏≠Ôºå‰ΩïÊó∂ÈáçÊñ∞Ê±ÇËß£Ê∑∑ÂêàÊï¥Êï∞Á∫øÊÄßËßÑÂàíÔºàMILPÔºâ‰ª•ËææÂà∞ÊÄßËÉΩÂíåÊàêÊú¨ÁöÑÊúÄ‰Ω≥Âπ≥Ë°°ÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÔºåÂ¶ÇÂêØÂèëÂºèÁÆóÊ≥ïÔºåÂú®Â§ÑÁêÜNP-hard MILPÈóÆÈ¢òÊó∂ÊïàÊûú‰∏ç‰Ω≥Ôºå‰∏îÁº∫‰πèÂØπÊ±ÇËß£ÊàêÊú¨ÁöÑÁ≥ªÁªüËÄÉËôë„ÄÇ‰º†ÁªüÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÂú®ÈïøÊó∂ÂüüÂíåÈùûÈ©¨Â∞îÂèØÂ§´ÂÜ≥Á≠ñËøáÁ®ãÔºànon-MDPÔºâÁªìÊûÑ‰∏ãÔºåÂÆπÊòìÂá∫Áé∞‰ª∑ÂÄºÂáΩÊï∞ÁàÜÁÇ∏ÈóÆÈ¢ò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®Âº∫ÂåñÂ≠¶‰π†Êù•Â≠¶‰π†‰∏Ä‰∏™Á≠ñÁï•ÔºåËØ•Á≠ñÁï•ËÉΩÂ§üÊ†πÊçÆÁéØÂ¢ÉÂèòÂåñÂíåÊ±ÇËß£ÊàêÊú¨ÔºåÂä®ÊÄÅÂú∞ÂÜ≥ÂÆö‰ΩïÊó∂ÈáçÊñ∞Ê±ÇËß£MILP„ÄÇÈÄöËøáÂ≠¶‰π†ÔºåËØ•Á≠ñÁï•ËÉΩÂ§üÊùÉË°°Ê±ÇËß£Â∏¶Êù•ÁöÑÊÄßËÉΩÊèêÂçáÂíåÊ±ÇËß£ÊâÄÊ∂àËÄóÁöÑËÆ°ÁÆóËµÑÊ∫êÔºå‰ªéËÄåÂú®ÊÄßËÉΩÂíåÊàêÊú¨‰πãÈó¥ÊâæÂà∞ÊúÄ‰Ω≥Âπ≥Ë°°ÁÇπ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöPOCÊ°ÜÊû∂ÂåÖÂê´‰ª•‰∏ã‰∏ªË¶ÅÊ®°ÂùóÔºö1ÔºâÁéØÂ¢ÉÁä∂ÊÄÅË°®Á§∫ÔºöÂØπÁéØÂ¢ÉÂèòÂåñËøõË°åÂª∫Ê®°ÔºåÂåÖÊã¨ÈóÆÈ¢òÂèÇÊï∞ÁöÑÂèòÂåñÁ≠âÔºõ2ÔºâÂèòÂåñÁÇπÊ£ÄÊµãÔºöÊ£ÄÊµãÁéØÂ¢ÉÊòØÂê¶ÂèëÁîüÊòæËëóÂèòÂåñÔºå‰ªéËÄåËß¶ÂèëÈáçÊñ∞Ê±ÇËß£ÁöÑÂÜ≥Á≠ñÔºõ3ÔºâËøëÁ´ØÁ≠ñÁï•‰ºòÂåñÔºàPPOÔºâÔºö‰ΩøÁî®PPOÁÆóÊ≥ïÂ≠¶‰π†‰∏Ä‰∏™Á≠ñÁï•ÔºåËØ•Á≠ñÁï•Ê†πÊçÆÁéØÂ¢ÉÁä∂ÊÄÅÂíåÂèòÂåñÁÇπÊ£ÄÊµãÁªìÊûúÔºåÂÜ≥ÂÆöÊòØÂê¶ÈáçÊñ∞Ê±ÇËß£MILPÔºõ4ÔºâÂ•ñÂä±ÂáΩÊï∞ËÆæËÆ°ÔºöËÆæËÆ°ÂêàÈÄÇÁöÑÂ•ñÂä±ÂáΩÊï∞ÔºåÈºìÂä±Á≠ñÁï•Âú®ÊÄßËÉΩÊèêÂçáÂíåÊ±ÇËß£ÊàêÊú¨‰πãÈó¥ËøõË°åÊùÉË°°„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöPOCÊ°ÜÊû∂ÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÔºö1ÔºâÂ∞ÜÂèòÂåñÁÇπÊ£ÄÊµã‰∏éÂº∫ÂåñÂ≠¶‰π†Áõ∏ÁªìÂêàÔºåËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞Ê£ÄÊµãÁéØÂ¢ÉÂèòÂåñÔºåÂπ∂Ê†πÊçÆÂèòÂåñÊÉÖÂÜµË∞ÉÊï¥Ê±ÇËß£Á≠ñÁï•Ôºõ2Ôºâ‰ΩøÁî®ËøëÁ´ØÁ≠ñÁï•‰ºòÂåñÔºàPPOÔºâÁÆóÊ≥ïÔºåÈÅøÂÖç‰∫Ü‰º†ÁªüÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ï‰∏≠ÁöÑ‰ª∑ÂÄºÂáΩÊï∞ÁàÜÁÇ∏ÈóÆÈ¢òÔºõ3ÔºâÁ≥ªÁªüÂú∞ËÄÉËôë‰∫ÜÊ±ÇËß£ÊàêÊú¨ÔºåÂπ∂Âú®Â•ñÂä±ÂáΩÊï∞‰∏≠ÂØπÂÖ∂ËøõË°åÂª∫Ê®°Ôºå‰ªéËÄåÂÆûÁé∞‰∫ÜÊÄßËÉΩÂíåÊàêÊú¨‰πãÈó¥ÁöÑÂπ≥Ë°°„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöPOCÊ°ÜÊû∂ÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1ÔºâÂèòÂåñÁÇπÊ£ÄÊµãÁöÑÈòàÂÄºËÆæÁΩÆÔºåÈúÄË¶ÅÊ†πÊçÆÂÖ∑‰ΩìÈóÆÈ¢òËøõË°åË∞ÉÊï¥Ôºõ2ÔºâPPOÁÆóÊ≥ïÁöÑÂèÇÊï∞ËÆæÁΩÆÔºåÂ¶ÇÂ≠¶‰π†Áéá„ÄÅÊäòÊâ£Âõ†Â≠êÁ≠âÔºåÈúÄË¶ÅËøõË°åË∞É‰ºòÔºõ3ÔºâÂ•ñÂä±ÂáΩÊï∞ÁöÑËÆæËÆ°ÔºåÈúÄË¶Å‰ªîÁªÜËÄÉËôëÊÄßËÉΩÊèêÂçáÂíåÊ±ÇËß£ÊàêÊú¨‰πãÈó¥ÁöÑÊùÉÈáçÂÖ≥Á≥ª„ÄÇËÆ∫Êñá‰∏≠ÂÖ∑‰Ωì‰ΩøÁî®‰∫ÜÂì™‰∫õÁΩëÁªúÁªìÊûÑÂíåÊçüÂ§±ÂáΩÊï∞Á≠âÁªÜËäÇ‰ø°ÊÅØÊú™Áü•„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåPOCÊ°ÜÊû∂Âú®ÂÖ´‰∏™ÂêàÊàêÂíåÁúüÂÆû‰∏ñÁïåÁöÑÊï∞ÊçÆÈõÜ‰∏äÔºåÂßãÁªà‰ºò‰∫éÁé∞ÊúâÂü∫Á∫ø2%-17%„ÄÇËøôË°®ÊòéPOCÊ°ÜÊû∂ËÉΩÂ§üÊúâÊïàÂú∞Âπ≥Ë°°ÊÄßËÉΩÂíåÊàêÊú¨ÔºåÂπ∂Âú®ÂÆûÊó∂MILPÊ±ÇËß£ÈóÆÈ¢ò‰∏äÂèñÂæóÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇÊ≠§Â§ñÔºåËØ•Á†îÁ©∂ËøòÊèê‰æõ‰∫ÜÂÆûÊó∂MILPÂü∫ÂáÜÂíåËØÑ‰º∞Ê†áÂáÜÔºå‰∏∫ÂêéÁª≠Á†îÁ©∂Êèê‰æõ‰∫ÜÂèÇËÄÉ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂπøÊ≥õÂ∫îÁî®‰∫éÈúÄË¶ÅÂÆûÊó∂Ê±ÇËß£MILPÁöÑÂú∫ÊôØÔºåÂ¶Ç‰æõÂ∫îÈìæÁÆ°ÁêÜ„ÄÅ‰∫§ÈÄöËøêËæì„ÄÅËÉΩÊ∫êË∞ÉÂ∫¶Á≠â„ÄÇÈÄöËøáPOCÊ°ÜÊû∂Ôºå‰ºÅ‰∏öÂèØ‰ª•Êõ¥ÊúâÊïàÂú∞Âà©Áî®ËÆ°ÁÆóËµÑÊ∫êÔºåÂú®‰øùËØÅËøêËê•ÊÄßËÉΩÁöÑÂêåÊó∂ÔºåÈôç‰ΩéÊ±ÇËß£ÊàêÊú¨„ÄÇËØ•Á†îÁ©∂‰∏∫ÂÆûÊó∂‰ºòÂåñÂÜ≥Á≠ñÊèê‰æõ‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊÄùË∑ØÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖÂ∫îÁî®‰ª∑ÂÄºÂíåÊú™Êù•ÂèëÂ±ïÊΩúÂäõ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> A common challenge in real-time operations is deciding whether to re-solve an optimization problem or continue using an existing solution. While modern data platforms may collect information at high frequencies, many real-time operations require repeatedly solving computationally intensive optimization problems formulated as Mixed-Integer Linear Programs (MILPs). Determining when to re-solve is, therefore, an economically important question. This problem poses several challenges: 1) How to characterize solution optimality and solving cost; 2) How to detect environmental changes and select beneficial samples for solving the MILP; 3) Given the large time horizon and non-MDP structure, vanilla reinforcement learning (RL) methods are not directly applicable and tend to suffer from value function explosion. Existing literature largely focuses on heuristics, low-data settings, and smooth objectives, with little focus on common NP-hard MILPs. We propose a framework called Proximal Policy Optimization with Change Point Detection (POC), which systematically offers a solution for balancing performance and cost when deciding appropriate re-solving times. Theoretically, we establish the relationship between the number of re-solves and the re-solving cost. To test our framework, we assemble eight synthetic and real-world datasets, and show that POC consistently outperforms existing baselines by 2%-17%. As a side benefit, our work fills the gap in the literature by introducing real-time MILP benchmarks and evaluation criteria.

