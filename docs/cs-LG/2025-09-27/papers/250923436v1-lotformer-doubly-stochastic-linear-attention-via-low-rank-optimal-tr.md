---
layout: default
title: LOTFormer: Doubly-Stochastic Linear Attention via Low-Rank Optimal Transport
---

# LOTFormer: Doubly-Stochastic Linear Attention via Low-Rank Optimal Transport

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.23436" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.23436v1</a>
  <a href="https://arxiv.org/pdf/2509.23436.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.23436v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.23436v1', 'LOTFormer: Doubly-Stochastic Linear Attention via Low-Rank Optimal Transport')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ashkan Shahbazi, Chayne Thrash, Yikun Bai, Keaton Hamm, Navid NaderiAlizadeh, Soheil Kolouri

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-27

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**LOTFormerï¼šé€šè¿‡ä½ç§©æœ€ä¼˜ä¼ è¾“å®ç°åŒé‡éšæœºçº¿æ€§æ³¨æ„åŠ›æœºåˆ¶ï¼Œæå‡é•¿åºåˆ—å»ºæ¨¡æ•ˆç‡ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `çº¿æ€§æ³¨æ„åŠ›` `æœ€ä¼˜ä¼ è¾“` `åŒé‡éšæœº` `ä½ç§©è¿‘ä¼¼` `é•¿åºåˆ—å»ºæ¨¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»ŸTransformeræ³¨æ„åŠ›æœºåˆ¶çš„äºŒæ¬¡å¤æ‚åº¦é™åˆ¶äº†å…¶åœ¨é•¿åºåˆ—ä¸Šçš„åº”ç”¨ï¼Œå®¹æ˜“è¿‡åº¦å…³æ³¨å°‘é‡tokenã€‚
2. LOTFormeré€šè¿‡ä½ç§©æœ€ä¼˜ä¼ è¾“ï¼Œå°†æ³¨æ„åŠ›çŸ©é˜µçº¦æŸä¸ºåŒé‡éšæœºï¼Œä¿è¯ä¿¡æ¯å‡è¡¡æµåŠ¨ï¼Œæå‡é²æ£’æ€§ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒLOTFormeråœ¨é•¿åºåˆ—åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰çº¿æ€§æ³¨æ„åŠ›å’ŒåŸºäºä¼ è¾“çš„æ–¹æ³•ï¼Œå…¼é¡¾ç²¾åº¦ä¸æ•ˆç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

Transformeråœ¨å„ç§æ¨¡æ€ä¸­éƒ½è¡¨ç°å‡ºé«˜æ•ˆæ€§ã€‚ç„¶è€Œï¼Œæ ‡å‡†softmaxæ³¨æ„åŠ›æœºåˆ¶çš„äºŒæ¬¡å¤æ‚åº¦å¯¹å°†å…¶æ‰©å±•åˆ°é•¿ä¸Šä¸‹æ–‡çª—å£æ„æˆäº†æ ¹æœ¬éšœç¢ã€‚å¤§é‡å·¥ä½œé€šè¿‡çº¿æ€§æ³¨æ„åŠ›æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œçº¿æ€§æ³¨æ„åŠ›å°†æ³¨æ„åŠ›é‡æ–°è¡¨è¿°ä¸ºæ ¸å‡½æ•°ï¼Œå¹¶ç”¨æœ‰é™çš„ç‰¹å¾å›¾æ¥è¿‘ä¼¼å®ƒï¼Œä»¥å®ç°çº¿æ€§æ—¶é—´è®¡ç®—ã€‚ä¸è®¡ç®—æ‰©å±•æ­£äº¤ï¼Œå¤§å¤šæ•°æ³¨æ„åŠ›æœºåˆ¶â€”â€”æ— è®ºæ˜¯äºŒæ¬¡çš„è¿˜æ˜¯çº¿æ€§çš„â€”â€”éƒ½ä¼šäº§ç”Ÿè¡Œå½’ä¸€åŒ–çš„æ˜ å°„ï¼Œè¿™äº›æ˜ å°„å¯èƒ½ä¼šè¿‡åº¦å…³æ³¨å°‘æ•°å‡ ä¸ªtokenï¼Œä»è€Œé™ä½é²æ£’æ€§å’Œä¿¡æ¯æµã€‚å¼ºåˆ¶æ‰§è¡ŒåŒé‡éšæœºæ³¨æ„åŠ›å¯ä»¥é€šè¿‡å¹³è¡¡è¡Œå’Œåˆ—ä¹‹é—´çš„tokenå‚ä¸æ¥ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œä½†ç°æœ‰çš„åŒé‡éšæœºæ³¨æ„åŠ›æœºåˆ¶é€šå¸¸ä¼šå¼•å…¥å¤§é‡çš„å¼€é”€ï¼Œä»è€Œç ´åå¯æ‰©å±•æ€§ã€‚æˆ‘ä»¬æå‡ºäº†LOTFormerï¼Œè¿™æ˜¯ä¸€ç§æ—¢æ˜¯çº¿æ€§æ—¶é—´åˆæ˜¯åŒé‡éšæœºçš„åŸåˆ™æ€§æ³¨æ„åŠ›æœºåˆ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨äº†æ³¨æ„åŠ›å›¾å’ŒæŸ¥è¯¢å’Œé”®åº¦é‡ä¹‹é—´çš„ä¼ è¾“è®¡åˆ’ä¹‹é—´çš„è”ç³»ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡å°†å…¶é™åˆ¶åœ¨å…·æœ‰å°æ”¯æ’‘çš„å¯å­¦ä¹ æ¢è½´åº¦é‡ä¸Šæ¥çº¦æŸä¼ è¾“è®¡åˆ’ä¸ºä½ç§©ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è§£å†³äº†ä¸¤ä¸ªç†µæœ€ä¼˜ä¼ è¾“é—®é¢˜ï¼ˆqueries â†’ pivot å’Œ pivot â†’ keysï¼‰å¹¶å°†å®ƒä»¬ç»„åˆæˆä¸€ä¸ªæ¡ä»¶ï¼ˆç²˜åˆï¼‰è€¦åˆã€‚è¿™äº§ç”Ÿäº†ä¸€ä¸ªæ³¨æ„åŠ›çŸ©é˜µï¼Œè¯¥çŸ©é˜µå¯è¯æ˜æ˜¯åŒé‡éšæœºçš„ï¼Œç§©æœ€å¤šä¸ºr << nï¼Œå¹¶åœ¨O(nr)æ—¶é—´å†…åº”ç”¨äºå€¼ï¼Œè€Œæ— éœ€å½¢æˆå®Œæ•´çš„n Ã— næ˜ å°„ã€‚æ¢è½´ä½ç½®å’Œè´¨é‡æ˜¯ç«¯åˆ°ç«¯å­¦ä¹ çš„ã€‚åœ¨ç»éªŒä¸Šï¼ŒLOTFormeråœ¨Long Range ArenaåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œåœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢éƒ½è¶…è¿‡äº†ä¹‹å‰çš„çº¿æ€§å’ŒåŸºäºä¼ è¾“çš„æ³¨æ„åŠ›æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šTransformeræ¨¡å‹ä¸­çš„æ ‡å‡†softmaxæ³¨æ„åŠ›æœºåˆ¶è®¡ç®—å¤æ‚åº¦ä¸ºO(n^2)ï¼Œå…¶ä¸­næ˜¯åºåˆ—é•¿åº¦ã€‚è¿™ä½¿å¾—Transformeréš¾ä»¥å¤„ç†é•¿åºåˆ—ã€‚æ­¤å¤–ï¼Œsoftmaxæ³¨æ„åŠ›å€¾å‘äºç”Ÿæˆè¡Œå½’ä¸€åŒ–çš„æ³¨æ„åŠ›æƒé‡ï¼Œå¯¼è‡´æ¨¡å‹è¿‡åº¦å…³æ³¨å°‘æ•°å‡ ä¸ªtokenï¼Œé™ä½äº†æ¨¡å‹çš„é²æ£’æ€§å’Œä¿¡æ¯æµåŠ¨æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šLOTFormerçš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†æ³¨æ„åŠ›æœºåˆ¶è§†ä¸ºæŸ¥è¯¢å’Œé”®ä¹‹é—´çš„æœ€ä¼˜ä¼ è¾“é—®é¢˜ã€‚é€šè¿‡å¼•å…¥ä¸€ä¸ªä½ç§©çš„æ¢è½´åº¦é‡ï¼Œå°†åŸæœ¬çš„ç›´æ¥ä¼ è¾“é—®é¢˜åˆ†è§£ä¸ºä¸¤ä¸ªå­é—®é¢˜ï¼šæŸ¥è¯¢åˆ°æ¢è½´çš„ä¼ è¾“å’Œæ¢è½´åˆ°é”®çš„ä¼ è¾“ã€‚è¿™ç§åˆ†è§£ä½¿å¾—æ³¨æ„åŠ›çŸ©é˜µå…·æœ‰ä½ç§©æ€§è´¨ï¼Œä»è€Œé™ä½äº†è®¡ç®—å¤æ‚åº¦ã€‚åŒæ—¶ï¼Œé€šè¿‡æœ€ä¼˜ä¼ è¾“çš„æ€§è´¨ï¼Œä¿è¯äº†æ³¨æ„åŠ›çŸ©é˜µæ˜¯åŒé‡éšæœºçš„ï¼Œä»è€Œå¹³è¡¡äº†tokenä¹‹é—´çš„å‚ä¸åº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLOTFormeråŒ…å«ä»¥ä¸‹ä¸»è¦æ­¥éª¤ï¼š1. **æ¢è½´åº¦é‡å­¦ä¹ **ï¼šå­¦ä¹ ä¸€ç»„æ¢è½´ç‚¹çš„ä½ç½®å’Œæƒé‡ã€‚2. **æŸ¥è¯¢åˆ°æ¢è½´çš„ä¼ è¾“**ï¼šè®¡ç®—æŸ¥è¯¢å’Œæ¢è½´ä¹‹é—´çš„æœ€ä¼˜ä¼ è¾“è®¡åˆ’ã€‚3. **æ¢è½´åˆ°é”®çš„ä¼ è¾“**ï¼šè®¡ç®—æ¢è½´å’Œé”®ä¹‹é—´çš„æœ€ä¼˜ä¼ è¾“è®¡åˆ’ã€‚4. **æ³¨æ„åŠ›çŸ©é˜µæ„å»º**ï¼šå°†ä¸¤ä¸ªä¼ è¾“è®¡åˆ’ç»„åˆæˆä¸€ä¸ªæ¡ä»¶è€¦åˆï¼Œå¾—åˆ°æœ€ç»ˆçš„æ³¨æ„åŠ›çŸ©é˜µã€‚5. **å€¼åŠ æƒ**ï¼šä½¿ç”¨æ³¨æ„åŠ›çŸ©é˜µå¯¹å€¼è¿›è¡ŒåŠ æƒï¼Œå¾—åˆ°æœ€ç»ˆçš„è¾“å‡ºã€‚

**å…³é”®åˆ›æ–°**ï¼šLOTFormerçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1. **ä½ç§©æœ€ä¼˜ä¼ è¾“**ï¼šé€šè¿‡å¼•å…¥ä½ç§©æ¢è½´åº¦é‡ï¼Œå°†æ³¨æ„åŠ›è®¡ç®—çš„å¤æ‚åº¦ä»O(n^2)é™ä½åˆ°O(nr)ï¼Œå…¶ä¸­ræ˜¯æ¢è½´ç‚¹çš„æ•°é‡ï¼Œé€šå¸¸è¿œå°äºnã€‚2. **åŒé‡éšæœºæ³¨æ„åŠ›**ï¼šé€šè¿‡æœ€ä¼˜ä¼ è¾“çš„æ€§è´¨ï¼Œä¿è¯äº†æ³¨æ„åŠ›çŸ©é˜µæ˜¯åŒé‡éšæœºçš„ï¼Œä»è€Œå¹³è¡¡äº†tokenä¹‹é—´çš„å‚ä¸åº¦ï¼Œæé«˜äº†æ¨¡å‹çš„é²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šLOTFormerçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1. **ç†µæ­£åˆ™åŒ–**ï¼šåœ¨æœ€ä¼˜ä¼ è¾“é—®é¢˜ä¸­å¼•å…¥ç†µæ­£åˆ™åŒ–é¡¹ï¼Œä»¥ä¿è¯ä¼ è¾“è®¡åˆ’çš„å…‰æ»‘æ€§ã€‚2. **æ¢è½´ç‚¹æ•°é‡çš„é€‰æ‹©**ï¼šæ¢è½´ç‚¹çš„æ•°é‡réœ€è¦æ ¹æ®å…·ä½“çš„ä»»åŠ¡å’Œæ•°æ®é›†è¿›è¡Œè°ƒæ•´ï¼Œä»¥å¹³è¡¡è®¡ç®—å¤æ‚åº¦å’Œæ¨¡å‹æ€§èƒ½ã€‚3. **ç«¯åˆ°ç«¯å­¦ä¹ **ï¼šæ¢è½´ç‚¹çš„ä½ç½®å’Œæƒé‡ä»¥åŠå…¶ä»–æ¨¡å‹å‚æ•°éƒ½æ˜¯é€šè¿‡ç«¯åˆ°ç«¯çš„æ–¹å¼è¿›è¡Œå­¦ä¹ çš„ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

LOTFormeråœ¨Long Range Arena (LRA) åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†state-of-the-artçš„ç»“æœï¼Œè¶…è¶Šäº†ä¹‹å‰çš„çº¿æ€§æ³¨æ„åŠ›å’ŒåŸºäºä¼ è¾“çš„æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼ŒLOTFormeråœ¨å¤šä¸ªLRAä»»åŠ¡ä¸Šéƒ½å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒåŒæ—¶ä¿æŒäº†è¾ƒé«˜çš„è®¡ç®—æ•ˆç‡ã€‚ä¾‹å¦‚ï¼Œåœ¨Pathfinderä»»åŠ¡ä¸Šï¼ŒLOTFormerçš„å‡†ç¡®ç‡è¶…è¿‡äº†ç°æœ‰æœ€ä½³æ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

LOTFormeré€‚ç”¨äºéœ€è¦å¤„ç†é•¿åºåˆ—æ•°æ®çš„å„ç§åº”ç”¨åœºæ™¯ï¼Œä¾‹å¦‚ï¼šé•¿æ–‡æœ¬å»ºæ¨¡ã€éŸ³é¢‘å¤„ç†ã€è§†é¢‘ç†è§£ã€åŸºå› ç»„åˆ†æç­‰ã€‚å…¶é«˜æ•ˆçš„è®¡ç®—å¤æ‚åº¦å’Œé²æ£’æ€§ä½¿å…¶èƒ½å¤Ÿå¤„ç†æ›´å¤§è§„æ¨¡çš„æ•°æ®ï¼Œå¹¶æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚æœªæ¥ï¼ŒLOTFormeræœ‰æœ›æˆä¸ºé•¿åºåˆ—å»ºæ¨¡é¢†åŸŸçš„é‡è¦æŠ€æœ¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Transformers have proven highly effective across a wide range of modalities. However, the quadratic complexity of the standard softmax attention mechanism poses a fundamental barrier to scaling them to long context windows. A large body of work addresses this with linear attention, which reformulates attention as a kernel function and approximates it with finite feature maps to achieve linear-time computation. Orthogonal to computational scaling, most attention mechanisms -- both quadratic and linear -- produce row-normalized maps that can over-focus on a few tokens, degrading robustness and information flow. Enforcing doubly-stochastic attention alleviates this by balancing token participation across rows and columns, but existing doubly-stochastic attention mechanisms typically introduce substantial overhead, undermining scalability. We propose LOTFormer, a principled attention mechanism that is simultaneously linear-time and doubly-stochastic. Our approach exploits the connection between attention maps and transportation plans between query and key measures. The central idea is to constrain the transport plan to be low-rank by conditioning it on a learnable pivot measure with small support. Concretely, we solve two entropic optimal transport problems (queries $\to$ pivot and pivot $\to$ keys) and compose them into a conditional (glued) coupling. This yields an attention matrix that is provably doubly-stochastic, has rank at most $r \ll n$, and applies to values in $O(nr)$ time without forming the full $n \times n$ map. The pivot locations and masses are learned end-to-end. Empirically, LOTFormer achieves state-of-the-art results on the Long Range Arena benchmark, surpassing prior linear and transport-based attention methods in both accuracy and efficiency.

