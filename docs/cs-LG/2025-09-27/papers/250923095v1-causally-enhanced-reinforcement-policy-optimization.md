---
layout: default
title: Causally-Enhanced Reinforcement Policy Optimization
---

# Causally-Enhanced Reinforcement Policy Optimization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.23095" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.23095v1</a>
  <a href="https://arxiv.org/pdf/2509.23095.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.23095v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.23095v1', 'Causally-Enhanced Reinforcement Policy Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiangqi Wang, Yue Huang, Yujun Zhou, Xiaonan Luo, Kehan Guo, Xiangliang Zhang

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-27

**å¤‡æ³¨**: Reinforcement learning publication of 24 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå› æœå¢å¼ºç­–ç•¥ä¼˜åŒ–ï¼ˆCE-POï¼‰ï¼Œæå‡LLMæ¨ç†çš„å› æœä¸€è‡´æ€§å’Œé²æ£’æ€§ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å› æœæ¨ç†` `å¼ºåŒ–å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `ç­–ç•¥ä¼˜åŒ–` `å¥–åŠ±å¡‘é€ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMé€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ—¶ï¼Œæ˜“é‡‡ç”¨æ·å¾„ç­–ç•¥ï¼Œå¯¼è‡´æ¨ç†è¿‡ç¨‹ä¸å¿ å®ï¼Œå¯¹å› æœæ‰°åŠ¨æ•æ„Ÿã€‚
2. CE-POé€šè¿‡å¯å¾®ä»£ç†å»ºæ¨¡æç¤ºã€ç†ç”±å’Œç­”æ¡ˆä¹‹é—´çš„å› æœå…³ç³»ï¼Œå¢å¼ºç­–ç•¥ä¼˜åŒ–ï¼Œæå‡æ¨¡å‹æ¨ç†çš„å› æœä¸€è‡´æ€§ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒCE-POåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæé«˜äº†å‡†ç¡®ç‡ï¼Œå¹¶å¢å¼ºäº†å¯¹å› æœå…³ç³»ç¿»è½¬å’Œåäº‹å®ç¼–è¾‘çš„é²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§å› æœå¢å¼ºç­–ç•¥ä¼˜åŒ–ï¼ˆCE-POï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç›®æ ‡è®­ç»ƒæ—¶ï¼Œé€šè¿‡æ·å¾„ç­–ç•¥è·å¾—è¡¨é¢ä¸Šæ­£ç¡®çš„ç­”æ¡ˆï¼Œå°†æ­£ç¡®çš„è¾“å‡ºä¸è™šå‡æˆ–ä¸å¿ å®çš„æ¨ç†é…å¯¹ï¼Œå¹¶åœ¨å°çš„å› æœæ‰°åŠ¨ä¸‹æ€§èƒ½ä¸‹é™çš„é—®é¢˜ã€‚CE-POæ˜¯ä¸€ç§å³æ’å³ç”¨çš„å¥–åŠ±å¡‘é€ æ¡†æ¶ï¼Œå®ƒé€šè¿‡å¯å¾®ä»£ç†æ¥å¢å¼ºç­–ç•¥ä¼˜åŒ–ï¼Œä»¥è¡¡é‡ä»æç¤ºï¼ˆZï¼‰åˆ°ç†ç”±ï¼ˆXï¼‰åˆ°ç­”æ¡ˆï¼ˆYï¼‰çš„ç”Ÿæˆè·¯å¾„ä¸Šçš„å› æœä¸€è‡´æ€§ã€‚CE-POä½¿ç”¨åŸºäºé›…å¯æ¯”çŸ©é˜µçš„æ•æ„Ÿæ€§æ¥ä¼°è®¡æ¨¡å‹å†…éƒ¨çš„å½±å“ï¼Œä»¥å¯¹æŠ—æ–¹å¼å¼ºåŒ–è¿™äº›ä¿¡å·ä»¥æŠ‘åˆ¶å¹²æ‰°çº¿ç´¢ï¼Œå¹¶é€šè¿‡Minkowskiï¼ˆå¹‚å¹³å‡ï¼‰ç»„åˆå™¨å°†ç»“æœä¸€è‡´æ€§å¾—åˆ†ä¸ä»»åŠ¡å‡†ç¡®æ€§åé¦ˆèåˆï¼Œä»è€Œæš´éœ²äº†å‡†ç¡®æ€§å’Œä¸€è‡´æ€§ä¹‹é—´çš„å•ä¸ªå¯è°ƒæƒè¡¡ã€‚ç»Ÿä¸€çš„å¥–åŠ±ä¸PPO/GRPOé›†æˆï¼Œæ— éœ€æ¶æ„æ›´æ”¹ã€‚åœ¨æ¨ç†åŸºå‡†å’Œå› æœå‹åŠ›æµ‹è¯•ä¸­ï¼ŒCE-POå‡å°‘äº†å¥–åŠ±é»‘å®¢å’Œä¸å¿ å®çš„æ€ç»´é“¾ï¼ŒåŒæ—¶æé«˜äº†å¯¹ç›¸å…³æ€§-å› æœå…³ç³»ç¿»è½¬å’Œè½»å¾®åäº‹å®ç¼–è¾‘çš„é²æ£’æ€§ï¼Œæ‰€æœ‰è¿™äº›éƒ½ä»¥æ¥è¿‘å¯¹ç­‰çš„å‡†ç¡®æ€§å®ç°ã€‚åœ¨4ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒCE-POçš„å‡†ç¡®ç‡æ¯”åŸºçº¿å¹³å‡æé«˜äº†5.49%ï¼ˆæœ€é«˜9.58%ï¼‰ï¼ŒåŒæ—¶æé«˜äº†å¯¹ç›¸å…³æ€§-å› æœå…³ç³»ç¿»è½¬å’Œè½»å¾®åäº‹å®ç¼–è¾‘çš„é²æ£’æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå¤§å‹è¯­è¨€æ¨¡å‹åœ¨é€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒæ—¶ï¼Œå¸¸å¸¸ä¼šå­¦ä¹ åˆ°ä¸€äº›â€œæ·å¾„â€ç­–ç•¥ï¼Œå³æ¨¡å‹å¯èƒ½åœ¨è¡¨é¢ä¸Šç»™å‡ºäº†æ­£ç¡®çš„ç­”æ¡ˆï¼Œä½†å…¶æ¨ç†è¿‡ç¨‹å®é™…ä¸Šæ˜¯ä¸å¿ å®çš„ï¼Œä¾èµ–äºä¸€äº›è™šå‡çš„çº¿ç´¢ã€‚è¿™ç§ç°è±¡å¯¼è‡´æ¨¡å‹åœ¨é¢å¯¹ä¸€äº›ç»†å¾®çš„å› æœæ‰°åŠ¨æ—¶ï¼Œæ€§èƒ½ä¼šæ˜¾è‘—ä¸‹é™ã€‚å› æ­¤ï¼Œå¦‚ä½•æå‡LLMæ¨ç†çš„å› æœä¸€è‡´æ€§å’Œé²æ£’æ€§æ˜¯ä¸€ä¸ªé‡è¦çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCE-POçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥ä¸€ä¸ªå¯å¾®çš„ä»£ç†æ¨¡å‹æ¥æ˜¾å¼åœ°å»ºæ¨¡ä»æç¤ºï¼ˆZï¼‰åˆ°ç†ç”±ï¼ˆXï¼‰å†åˆ°ç­”æ¡ˆï¼ˆYï¼‰çš„å› æœå…³ç³»ã€‚è¿™ä¸ªä»£ç†æ¨¡å‹èƒ½å¤Ÿè¯„ä¼°æ¨¡å‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­çš„å› æœä¸€è‡´æ€§ï¼Œå¹¶å°†å…¶ä½œä¸ºå¥–åŠ±ä¿¡å·çš„ä¸€éƒ¨åˆ†ï¼Œå¼•å¯¼æ¨¡å‹å­¦ä¹ æ›´åŠ å¿ å®çš„æ¨ç†è¿‡ç¨‹ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒCE-POé¼“åŠ±æ¨¡å‹ä¸ä»…ä»…å…³æ³¨æœ€ç»ˆç­”æ¡ˆçš„æ­£ç¡®æ€§ï¼Œæ›´å…³æ³¨æ¨ç†è¿‡ç¨‹çš„åˆç†æ€§å’Œå¯é æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCE-POçš„æ•´ä½“æ¡†æ¶å¯ä»¥åˆ†ä¸ºä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) **å› æœå…³ç³»å»ºæ¨¡**ï¼šä½¿ç”¨åŸºäºé›…å¯æ¯”çŸ©é˜µçš„æ•æ„Ÿæ€§åˆ†ææ¥ä¼°è®¡æ¨¡å‹å†…éƒ¨çš„å½±å“ï¼Œä»è€Œæ•æ‰æç¤ºã€ç†ç”±å’Œç­”æ¡ˆä¹‹é—´çš„å› æœå…³ç³»ã€‚2) **å¯¹æŠ—æ€§å¼ºåŒ–**ï¼šé€šè¿‡å¯¹æŠ—æ€§çš„æ–¹å¼å¼ºåŒ–è¿™äº›å› æœä¿¡å·ï¼ŒæŠ‘åˆ¶é‚£äº›è™šå‡çš„ã€ä¸ç›¸å…³çš„çº¿ç´¢ã€‚3) **å¥–åŠ±èåˆ**ï¼šå°†å› æœä¸€è‡´æ€§å¾—åˆ†ä¸ä»»åŠ¡å‡†ç¡®æ€§åé¦ˆé€šè¿‡Minkowskiç»„åˆå™¨è¿›è¡Œèåˆï¼Œä»è€Œå®ç°å‡†ç¡®æ€§å’Œä¸€è‡´æ€§ä¹‹é—´çš„æƒè¡¡ã€‚4) **ç­–ç•¥ä¼˜åŒ–**ï¼šå°†èåˆåçš„å¥–åŠ±ä¿¡å·ç”¨äºPPO/GRPOç­‰ç­–ç•¥ä¼˜åŒ–ç®—æ³•ï¼Œä»è€Œè®­ç»ƒæ¨¡å‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šCE-POçš„å…³é”®åˆ›æ–°åœ¨äºå®ƒæ˜¾å¼åœ°å»ºæ¨¡äº†LLMæ¨ç†è¿‡ç¨‹ä¸­çš„å› æœå…³ç³»ï¼Œå¹¶å°†å…¶ä½œä¸ºå¥–åŠ±ä¿¡å·çš„ä¸€éƒ¨åˆ†ï¼Œå¼•å¯¼æ¨¡å‹å­¦ä¹ æ›´åŠ å¿ å®çš„æ¨ç†è¿‡ç¨‹ã€‚ä¸ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼ŒCE-POä¸ä»…ä»…å…³æ³¨æœ€ç»ˆç­”æ¡ˆçš„æ­£ç¡®æ€§ï¼Œæ›´å…³æ³¨æ¨ç†è¿‡ç¨‹çš„åˆç†æ€§å’Œå¯é æ€§ï¼Œä»è€Œæé«˜äº†æ¨¡å‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šCE-POçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨é›…å¯æ¯”çŸ©é˜µæ¥ä¼°è®¡æ¨¡å‹å†…éƒ¨çš„å½±å“ï¼Œè¿™æ˜¯ä¸€ç§å¯å¾®çš„æ–¹æ³•ï¼Œå¯ä»¥æ–¹ä¾¿åœ°ä¸ç°æœ‰çš„æ·±åº¦å­¦ä¹ æ¡†æ¶é›†æˆã€‚2) ä½¿ç”¨å¯¹æŠ—æ€§çš„æ–¹å¼å¼ºåŒ–å› æœä¿¡å·ï¼Œè¿™å¯ä»¥æœ‰æ•ˆåœ°æŠ‘åˆ¶è™šå‡çº¿ç´¢çš„å¹²æ‰°ã€‚3) ä½¿ç”¨Minkowskiç»„åˆå™¨æ¥èåˆå› æœä¸€è‡´æ€§å¾—åˆ†å’Œä»»åŠ¡å‡†ç¡®æ€§åé¦ˆï¼Œè¿™æä¾›äº†ä¸€ç§çµæ´»çš„æ–¹å¼æ¥æƒè¡¡å‡†ç¡®æ€§å’Œä¸€è‡´æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒCE-POåœ¨4ä¸ªæ•°æ®é›†ä¸Šå¹³å‡æé«˜äº†5.49%çš„å‡†ç¡®ç‡ï¼ˆæœ€é«˜9.58%ï¼‰ï¼ŒåŒæ—¶æ˜¾è‘—æé«˜äº†æ¨¡å‹å¯¹ç›¸å…³æ€§-å› æœå…³ç³»ç¿»è½¬å’Œè½»å¾®åäº‹å®ç¼–è¾‘çš„é²æ£’æ€§ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒCE-POèƒ½å¤Ÿæœ‰æ•ˆåœ°æå‡LLMæ¨ç†çš„å› æœä¸€è‡´æ€§å’Œé²æ£’æ€§ï¼Œä½¿å…¶åœ¨æ›´åŠ å¤æ‚çš„ç¯å¢ƒä¸­è¡¨ç°æ›´åŠ å‡ºè‰²ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

CE-POå¯åº”ç”¨äºå„ç§éœ€è¦é«˜å¯é æ€§å’Œé²æ£’æ€§çš„LLMåº”ç”¨åœºæ™¯ï¼Œä¾‹å¦‚åŒ»ç–—è¯Šæ–­ã€é‡‘èåˆ†æã€æ³•å¾‹å’¨è¯¢ç­‰ã€‚é€šè¿‡æå‡LLMæ¨ç†çš„å› æœä¸€è‡´æ€§ï¼Œå¯ä»¥å‡å°‘æ¨¡å‹çŠ¯é”™çš„æ¦‚ç‡ï¼Œæé«˜å†³ç­–çš„å¯é æ€§ï¼Œå¹¶å¢å¼ºæ¨¡å‹åœ¨é¢å¯¹å¯¹æŠ—æ€§æ”»å‡»æ—¶çš„é˜²å¾¡èƒ½åŠ›ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›æ¨å¹¿åˆ°æ›´å¹¿æ³›çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) trained with reinforcement objectives often achieve superficially correct answers via shortcut strategies, pairing correct outputs with spurious or unfaithful reasoning and degrading under small causal perturbations. We introduce Causally-Enhanced Policy Optimization (CE-PO), a drop-in reward-shaping framework that augments policy optimization with a differentiable proxy for causal coherence along the generation pathway from prompt (Z) to rationale (X) to answer (Y). CE-PO estimates model-internal influence with Jacobian-based sensitivities, counterfactually hardens these signals to suppress nuisance cues, and fuses the resulting coherence score with task-accuracy feedback via a Minkowski (power-mean) combiner, exposing a single tunable between accuracy and coherence trade-off. The unified reward integrates with PPO/GRPO without architectural changes. Across reasoning benchmarks and causal stress tests, CE-PO reduces reward hacking and unfaithful chain-of-thought while improving robustness to correlation-causation flips and light counterfactual edits, all at near-parity accuracy. Experimental results across 4 datasets show that CE-PO improves accuracy over baselines by 5.49% on average (up to 9.58%), while improving robustness to correlation-causation flips and light counterfactual edits.

