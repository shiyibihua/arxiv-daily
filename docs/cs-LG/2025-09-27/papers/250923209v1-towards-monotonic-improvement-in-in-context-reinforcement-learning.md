---
layout: default
title: Towards Monotonic Improvement in In-Context Reinforcement Learning
---

# Towards Monotonic Improvement in In-Context Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.23209" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.23209v1</a>
  <a href="https://arxiv.org/pdf/2509.23209.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.23209v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.23209v1', 'Towards Monotonic Improvement in In-Context Reinforcement Learning')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Wenhao Zhang, Shao Zhang, Xihuai Wang, Yang Li, Ying Wen

**ÂàÜÁ±ª**: cs.LG, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-27

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/Bluixe/towards_monotonic_improvement)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫‰∏ä‰∏ãÊñá‰ª∑ÂÄºÂºïÂØºÁöÑICRLÊñπÊ≥ïÔºåËß£ÂÜ≥ICRL‰∏≠ÂçïË∞ÉÊîπËøõÁöÑÈöæÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `‰∏ä‰∏ãÊñáÂº∫ÂåñÂ≠¶‰π†` `ICRL` `‰∏ä‰∏ãÊñá‰ª∑ÂÄº` `ÂçïË∞ÉÊîπËøõ` `Âº∫ÂåñÂ≠¶‰π†` `Â∫èÂàóÊ®°Âûã` `‰∏ä‰∏ãÊñáÊ≠ß‰πâ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. ICRLÊ®°ÂûãÂú®ÊµãËØïÊó∂Êó†Ê≥ïÂÉèËÆ≠ÁªÉÊï∞ÊçÆÈÇ£Ê†∑ÊåÅÁª≠ÊîπËøõÔºåÂ≠òÂú®‚Äú‰∏ä‰∏ãÊñáÊ≠ß‰πâ‚ÄùÈóÆÈ¢òÔºåÂç≥Ê®°ÂûãËá™Ë∫´Âä®‰ΩúÂèØËÉΩ‰∫ßÁîüËØØÂØºÊÄßÁöÑÂéÜÂè≤„ÄÇ
2. ÊèêÂá∫CV-ICRLÔºåÂà©Áî®‰∏ä‰∏ãÊñá‰ª∑ÂÄº‰Ωú‰∏∫ÊòæÂºè‰ø°Âè∑ÔºåË°®Á§∫Á≠ñÁï•Âú®ÂΩìÂâç‰∏ä‰∏ãÊñá‰∏≠ÂèØÂÆûÁé∞ÁöÑÁêÜÊÉ≥ÊÄßËÉΩÔºåÊåáÂØºÁ≠ñÁï•Â≠¶‰π†„ÄÇ
3. Âú®Dark RoomÂíåMinigridÁ≠âÁéØÂ¢ÉÁöÑÂÆûÈ™åË°®ÊòéÔºåCV-ICRLËÉΩÊúâÊïàÁºìËß£ÊÄßËÉΩ‰∏ãÈôçÔºåÊèêÂçáICRLËÉΩÂäõ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

‰∏ä‰∏ãÊñáÂº∫ÂåñÂ≠¶‰π†(ICRL)‰Ωú‰∏∫‰∏ÄÁßçÊñ∞ÂÖ¥ËåÉÂºèÔºåÈÄöËøáÂà©Áî®ËøáÂéªÁöÑÁªèÈ™å‰Ωú‰∏∫‰∏ä‰∏ãÊñáÔºåÊó†ÈúÄÊõ¥Êñ∞ÂèÇÊï∞Âç≥ÂèØÂø´ÈÄüÈÄÇÂ∫îÊñ∞‰ªªÂä°„ÄÇÊúÄËøëÁöÑÊñπÊ≥ïËÆ≠ÁªÉÂ§ßÂûãÂ∫èÂàóÊ®°ÂûãÔºåÂà©Áî®Âú®Á∫øÂº∫ÂåñÂ≠¶‰π†‰∏≠ÁöÑÂçïË∞ÉÁ≠ñÁï•ÊîπËøõÊï∞ÊçÆÔºåÊó®Âú®ÂÆûÁé∞ÊåÅÁª≠ÊîπËøõÁöÑÊµãËØïÊó∂ÊÄßËÉΩ„ÄÇÁÑ∂ËÄåÔºåÊàë‰ª¨ÁöÑÂÆûÈ™åÂàÜÊûêÊè≠Á§∫‰∫Ü‰∏Ä‰∏™ÂÖ≥ÈîÆÁº∫Èô∑ÔºöËøô‰∫õÊ®°ÂûãÂú®ÊµãËØïÊó∂Êó†Ê≥ïÂÉèËÆ≠ÁªÉÊï∞ÊçÆÈÇ£Ê†∑Ë°®Áé∞Âá∫ÊåÅÁª≠ÁöÑÊîπËøõ„ÄÇÁêÜËÆ∫‰∏äÔºåÊàë‰ª¨Â∞ÜËøôÁßçÁé∞Ë±°ËØÜÂà´‰∏∫‰∏ä‰∏ãÊñáÊ≠ß‰πâÔºåÂç≥Ê®°ÂûãËá™Ë∫´ÈöèÊú∫Âä®‰ΩúÂèØËÉΩ‰∫ßÁîü‰∏ÄÁßç‰∫§‰∫íÂéÜÂè≤ÔºåÈîôËØØÂú∞Á±ª‰ºº‰∫éËÆ≠ÁªÉÊï∞ÊçÆ‰∏≠Ê¨°‰ºòÁ≠ñÁï•ÁöÑ‰∫§‰∫íÂéÜÂè≤Ôºå‰ªéËÄåÂºïÂèë‰∏çËâØÂä®‰ΩúÈÄâÊã©ÁöÑÊÅ∂ÊÄßÂæ™ÁéØ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥‰∏ä‰∏ãÊñáÊ≠ß‰πâÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏ä‰∏ãÊñá‰ª∑ÂÄºÂà∞ËÆ≠ÁªÉÈò∂ÊÆµÔºåÂπ∂ÊèêÂá∫‰∫Ü‰∏ä‰∏ãÊñá‰ª∑ÂÄºÂºïÂØºÁöÑICRL(CV-ICRL)„ÄÇCV-ICRL‰ΩøÁî®‰∏ä‰∏ãÊñá‰ª∑ÂÄº‰Ωú‰∏∫ÊòæÂºè‰ø°Âè∑ÔºåË°®Á§∫ÁêÜËÆ∫‰∏äÁ≠ñÁï•Âú®ÂΩìÂâç‰∏ä‰∏ãÊñá‰∏≠ÂèØÂÆûÁé∞ÁöÑÁêÜÊÉ≥ÊÄßËÉΩ„ÄÇÈöèÁùÄ‰∏ä‰∏ãÊñáÁöÑÊâ©Â±ïÔºå‰∏ä‰∏ãÊñá‰ª∑ÂÄºÂèØ‰ª•ÂåÖÂê´Êõ¥Â§ö‰∏é‰ªªÂä°Áõ∏ÂÖ≥ÁöÑ‰ø°ÊÅØÔºåÂõ†Ê≠§ÁêÜÊÉ≥ÁöÑÊÄßËÉΩÂ∫îËØ•ÊòØÂçïË∞É‰∏çÂáèÁöÑ„ÄÇÊàë‰ª¨ËØÅÊòé‰∫Ü‰∏ä‰∏ãÊñá‰ª∑ÂÄºÊî∂Á¥ß‰∫ÜÁõ∏ÂØπ‰∫éÁêÜÊÉ≥ÁöÑ„ÄÅÂçïË∞ÉÊîπËøõÁ≠ñÁï•ÁöÑÊÄßËÉΩÂ∑ÆË∑ùÁöÑ‰∏ãÁïå„ÄÇÊàë‰ª¨Ëøõ‰∏ÄÊ≠•ÊèêÂá∫‰∫Ü‰∏§ÁßçÂú®ËÆ≠ÁªÉÂíåÊµãËØïÊó∂‰º∞ËÆ°‰∏ä‰∏ãÊñá‰ª∑ÂÄºÁöÑÊñπÊ≥ï„ÄÇÂú®Dark RoomÂíåMinigridÊµãËØïÂπ≥Âè∞ËøõË°åÁöÑÂÆûÈ™åË°®ÊòéÔºåCV-ICRLÊúâÊïàÂú∞ÁºìËß£‰∫ÜÊÄßËÉΩ‰∏ãÈôçÔºåÂπ∂ÊèêÈ´ò‰∫ÜÂêÑÁßç‰ªªÂä°ÂíåÁéØÂ¢É‰∏≠ÁöÑÊï¥‰ΩìICRLËÉΩÂäõ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöICRLÊó®Âú®ÈÄöËøá‰∏ä‰∏ãÊñáÂ≠¶‰π†Âø´ÈÄüÈÄÇÂ∫îÊñ∞‰ªªÂä°Ôºå‰ΩÜÁé∞ÊúâÊñπÊ≥ïÂú®ÊµãËØïÊó∂Êó†Ê≥ï‰øùËØÅÂçïË∞ÉÁöÑÊÄßËÉΩÊîπËøõ„ÄÇ‰∏ªË¶ÅÁóõÁÇπÂú®‰∫é‚Äú‰∏ä‰∏ãÊñáÊ≠ß‰πâ‚ÄùÔºåÂç≥Ê®°ÂûãËá™Ë∫´ÁöÑÈöèÊú∫Êé¢Á¥¢ÂèØËÉΩ‰∫ßÁîü‰∏éÊ¨°‰ºòÁ≠ñÁï•Áõ∏‰ººÁöÑ‰∫§‰∫íÂéÜÂè≤ÔºåÂØºËá¥Ê®°ÂûãËØØÂà§Âπ∂ÈÄâÊã©Ê¨°‰ºòÂä®‰ΩúÔºå‰ªéËÄåÈô∑ÂÖ•ÊÄßËÉΩ‰∏ãÈôçÁöÑÂæ™ÁéØ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöCV-ICRLÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÂºïÂÖ•‚Äú‰∏ä‰∏ãÊñá‰ª∑ÂÄº‚ÄùÊù•ÁºìËß£‰∏ä‰∏ãÊñáÊ≠ß‰πâ„ÄÇ‰∏ä‰∏ãÊñá‰ª∑ÂÄº‰ª£Ë°®‰∫ÜÂú®ÁªôÂÆöÂΩìÂâç‰∏ä‰∏ãÊñáÊÉÖÂÜµ‰∏ãÔºåÁ≠ñÁï•ÁêÜËÆ∫‰∏äÂèØ‰ª•ËææÂà∞ÁöÑÊúÄ‰Ω≥ÊÄßËÉΩ„ÄÇÈÄöËøáÂ∞Ü‰∏ä‰∏ãÊñá‰ª∑ÂÄº‰Ωú‰∏∫ÊòæÂºèÁöÑÊåáÂØº‰ø°Âè∑ÔºåÊ®°ÂûãÂèØ‰ª•Êõ¥Â•ΩÂú∞ÁêÜËß£ÂΩìÂâçÁä∂ÊÄÅÁöÑÊΩúÂú®‰ª∑ÂÄºÔºå‰ªéËÄåÈÅøÂÖçË¢´ËØØÂØºÊÄßÁöÑ‰∫§‰∫íÂéÜÂè≤ÊâÄËø∑ÊÉëÔºåÈÄâÊã©Êõ¥‰ºòÁöÑÂä®‰Ωú„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöCV-ICRLÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÊã¨‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÈò∂ÊÆµÔºö1) Êï∞ÊçÆÊî∂ÈõÜÔºö‰ΩøÁî®Âú®Á∫øÂº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïÊî∂ÈõÜ‰∫§‰∫íÊï∞ÊçÆÔºåÂåÖÊã¨Áä∂ÊÄÅ„ÄÅÂä®‰Ωú„ÄÅÂ•ñÂä±Á≠â„ÄÇ2) ‰∏ä‰∏ãÊñá‰ª∑ÂÄº‰º∞ËÆ°ÔºöÂú®ËÆ≠ÁªÉÂíåÊµãËØïÈò∂ÊÆµÔºåÂàÜÂà´‰ΩøÁî®‰∏çÂêåÁöÑÊñπÊ≥ï‰º∞ËÆ°‰∏ä‰∏ãÊñá‰ª∑ÂÄº„ÄÇ3) Ê®°ÂûãËÆ≠ÁªÉÔºö‰ΩøÁî®Êî∂ÈõÜÂà∞ÁöÑÊï∞ÊçÆÂíå‰º∞ËÆ°ÁöÑ‰∏ä‰∏ãÊñá‰ª∑ÂÄºËÆ≠ÁªÉICRLÊ®°Âûã„ÄÇ4) Á≠ñÁï•ÊâßË°åÔºöÂú®ÊµãËØïÁéØÂ¢É‰∏≠ÔºåICRLÊ®°ÂûãÊ†πÊçÆÂΩìÂâç‰∏ä‰∏ãÊñáÂíå‰º∞ËÆ°ÁöÑ‰∏ä‰∏ãÊñá‰ª∑ÂÄºÈÄâÊã©Âä®‰Ωú„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöCV-ICRLÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂºïÂÖ•‰∫Ü‰∏ä‰∏ãÊñá‰ª∑ÂÄºÁöÑÊ¶ÇÂøµÔºåÂπ∂Â∞ÜÂÖ∂‰Ωú‰∏∫ÊòæÂºèÁöÑÊåáÂØº‰ø°Âè∑Áî®‰∫éICRLÊ®°ÂûãÁöÑËÆ≠ÁªÉ„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåCV-ICRLËÉΩÂ§üÊõ¥Â•ΩÂú∞ÁêÜËß£ÂΩìÂâçÁä∂ÊÄÅÁöÑÊΩúÂú®‰ª∑ÂÄºÔºå‰ªéËÄåÈÅøÂÖçË¢´ËØØÂØºÊÄßÁöÑ‰∫§‰∫íÂéÜÂè≤ÊâÄËø∑ÊÉëÔºåÈÄâÊã©Êõ¥‰ºòÁöÑÂä®‰Ωú„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáËøòÊèêÂá∫‰∫Ü‰∏§Áßç‰º∞ËÆ°‰∏ä‰∏ãÊñá‰ª∑ÂÄºÁöÑÊñπÊ≥ïÔºåÂàÜÂà´ÈÄÇÁî®‰∫éËÆ≠ÁªÉÂíåÊµãËØïÈò∂ÊÆµ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏§Áßç‰º∞ËÆ°‰∏ä‰∏ãÊñá‰ª∑ÂÄºÁöÑÊñπÊ≥ï„ÄÇÂú®ËÆ≠ÁªÉÈò∂ÊÆµÔºå‰ΩøÁî®TD learningÊù•‰º∞ËÆ°‰∏ä‰∏ãÊñá‰ª∑ÂÄº„ÄÇÂú®ÊµãËØïÈò∂ÊÆµÔºå‰ΩøÁî®‰∏Ä‰∏™ÁÆÄÂçïÁöÑÂêØÂèëÂºèÊñπÊ≥ïÔºåÂç≥Ê†πÊçÆÂΩìÂâç‰∏ä‰∏ãÊñá‰∏≠ÁöÑÂ•ñÂä±Êù•‰º∞ËÆ°‰∏ä‰∏ãÊñá‰ª∑ÂÄº„ÄÇÊçüÂ§±ÂáΩÊï∞ÁöÑËÆæËÆ°ÁõÆÊ†áÊòØ‰ΩøÊ®°ÂûãÁöÑËæìÂá∫Âä®‰ΩúËÉΩÂ§üÊúÄÂ§ßÂåñÁ¥ØÁßØÂ•ñÂä±ÔºåÂêåÊó∂‰∏é‰∏ä‰∏ãÊñá‰ª∑ÂÄº‰øùÊåÅ‰∏ÄËá¥„ÄÇÂÖ∑‰ΩìÁöÑÁΩëÁªúÁªìÊûÑÊú™Áü•„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåCV-ICRLÂú®Dark RoomÂíåMinigridÁ≠âÁéØÂ¢É‰∏≠ËÉΩÂ§üÊúâÊïàÁºìËß£ÊÄßËÉΩ‰∏ãÈôçÔºåÂπ∂ÊèêÈ´òÊï¥‰ΩìICRLËÉΩÂäõ„ÄÇÂÖ∑‰ΩìÊÄßËÉΩÊï∞ÊçÆÂíåÊèêÂçáÂπÖÂ∫¶Êú™Áü•Ôºå‰ΩÜËÆ∫ÊñáÂº∫Ë∞ÉCV-ICRLËÉΩÂ§üÊõ¥Â•ΩÂú∞ÈÄÇÂ∫îÂêÑÁßç‰ªªÂä°ÂíåÁéØÂ¢É„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÊú∫Âô®‰∫∫ÊéßÂà∂„ÄÅÊ∏∏ÊàèAI„ÄÅËá™Âä®È©æÈ©∂Á≠âÈ¢ÜÂüüÔºåÂ∞§ÂÖ∂ÈÄÇÁî®‰∫éÈúÄË¶ÅÂú®Êú™Áü•ÊàñÂø´ÈÄüÂèòÂåñÁéØÂ¢É‰∏≠ËøõË°åÂÜ≥Á≠ñÁöÑ‰ªªÂä°„ÄÇÈÄöËøáÂà©Áî®ÂéÜÂè≤ÁªèÈ™åÂíå‰∏ä‰∏ãÊñá‰ø°ÊÅØÔºåCV-ICRLËÉΩÂ§üÂ∏ÆÂä©Êô∫ËÉΩ‰ΩìÂø´ÈÄüÈÄÇÂ∫îÊñ∞‰ªªÂä°ÔºåÊèêÈ´òÂÜ≥Á≠ñÊïàÁéáÂíåÊÄßËÉΩÔºåÈôç‰Ωé‰∫∫Â∑•Âπ≤È¢ÑÁöÑÈúÄÊ±ÇÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖÂ∫îÁî®‰ª∑ÂÄºÂíåÊΩúÂäõ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> In-Context Reinforcement Learning (ICRL) has emerged as a promising paradigm for developing agents that can rapidly adapt to new tasks by leveraging past experiences as context, without updating their parameters. Recent approaches train large sequence models on monotonic policy improvement data from online RL, aiming to a continue improved testing time performance. However, our experimental analysis reveals a critical flaw: these models cannot show a continue improvement like the training data during testing time. Theoretically, we identify this phenomenon as Contextual Ambiguity, where the model's own stochastic actions can generate an interaction history that misleadingly resembles that of a sub-optimal policy from the training data, initiating a vicious cycle of poor action selection. To resolve the Contextual Ambiguity, we introduce Context Value into training phase and propose Context Value Informed ICRL (CV-ICRL). CV-ICRL use Context Value as an explicit signal representing the ideal performance theoretically achievable by a policy given the current context. As the context expands, Context Value could include more task-relevant information, and therefore the ideal performance should be non-decreasing. We prove that the Context Value tightens the lower bound on the performance gap relative to an ideal, monotonically improving policy. We fruther propose two methods for estimating Context Value at both training and testing time. Experiments conducted on the Dark Room and Minigrid testbeds demonstrate that CV-ICRL effectively mitigates performance degradation and improves overall ICRL abilities across various tasks and environments. The source code and data of this paper are available at https://github.com/Bluixe/towards_monotonic_improvement .

