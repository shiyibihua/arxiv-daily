---
layout: default
title: SPEC-RL: Accelerating On-Policy Reinforcement Learning via Speculative Rollouts
---

# SPEC-RL: Accelerating On-Policy Reinforcement Learning via Speculative Rollouts

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.23232" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.23232v1</a>
  <a href="https://arxiv.org/pdf/2509.23232.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.23232v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.23232v1', 'SPEC-RL: Accelerating On-Policy Reinforcement Learning via Speculative Rollouts')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Bingshuai Liu, Ante Wang, Zijun Min, Liang Yao, Haibo Zhang, Yang Liu, Anxiang Zeng, Jinsong Su

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-27

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/ShopeeLLM/Spec-RL)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**SPEC-RLï¼šé€šè¿‡æ¨æµ‹æ€§RolloutåŠ é€ŸOn-Policyå¼ºåŒ–å­¦ä¹ ï¼Œæå‡LLMæ¨ç†æ•ˆç‡ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨æµ‹æ€§è§£ç ` `RolloutåŠ é€Ÿ` `æ€ç»´é“¾æ¨ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰RLVRæ–¹æ³•åœ¨LLMæ¨ç†ä¸­é¢ä¸´rollouté˜¶æ®µè®¡ç®—æˆæœ¬é«˜çš„æŒ‘æˆ˜ï¼Œé™åˆ¶äº†è®­ç»ƒæ•ˆç‡ã€‚
2. SPEC-RLé€šè¿‡é‡ç”¨å…ˆå‰è½¨è¿¹ç‰‡æ®µä½œä¸ºæ¨æµ‹æ€§å‰ç¼€ï¼Œç»“åˆdraft-and-verifyæœºåˆ¶ï¼Œé¿å…å†—ä½™è®¡ç®—ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒSPEC-RLåœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼Œå°†rolloutæ—¶é—´å‡å°‘2-3å€ï¼Œä¸”ä¸å½±å“ç­–ç•¥è´¨é‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¶Šæ¥è¶Šå¤šåœ°ä¾èµ–äºå…·æœ‰å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰æ¥è·å¾—å¯é çš„æ€ç»´é“¾æ¨ç†ã€‚ç„¶è€Œï¼Œè®­ç»ƒè¿‡ç¨‹ä»ç„¶å—åˆ°è®¡ç®—æˆæœ¬é«˜æ˜‚çš„rollouté˜¶æ®µçš„é™åˆ¶ã€‚ç°æœ‰çš„åŠ é€Ÿæ–¹æ³•ï¼Œå¦‚å¹¶è¡ŒåŒ–ã€ç›®æ ‡å’Œæ•°æ®é©±åŠ¨çš„ä¿®æ”¹ä»¥åŠå›æ”¾ç¼“å†²åŒºï¼Œè¦ä¹ˆæ”¶ç›Šé€’å‡ï¼Œè¦ä¹ˆå¼•å…¥åå·®ï¼Œè¦ä¹ˆå¿½ç•¥äº†è¿­ä»£ä¹‹é—´çš„å†—ä½™ã€‚æˆ‘ä»¬å‘ç°ï¼Œè¿ç»­è®­ç»ƒepochçš„rolloutç»å¸¸å…±äº«å¤§éƒ¨åˆ†é‡å ç‰‡æ®µï¼Œæµªè´¹äº†è®¡ç®—èµ„æºã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SPEC-RLï¼Œè¿™æ˜¯ä¸€ä¸ªå°†æ¨æµ‹æ€§è§£ç ä¸RL rolloutè¿‡ç¨‹ç›¸ç»“åˆçš„æ–°æ¡†æ¶ã€‚SPEC-RLé‡ç”¨å…ˆå‰çš„è½¨è¿¹ç‰‡æ®µä½œä¸ºæ¨æµ‹æ€§å‰ç¼€ï¼Œå¹¶é€šè¿‡draft-and-verifyæœºåˆ¶æ‰©å±•å®ƒä»¬ï¼Œé¿å…äº†å†—ä½™ç”Ÿæˆï¼ŒåŒæ—¶ç¡®ä¿äº†ç­–ç•¥ä¸€è‡´æ€§ã€‚åœ¨åŒ…æ‹¬GSM8Kã€MATH-500ã€OlympiadBenchã€MMLU-STEMç­‰åœ¨å†…çš„å„ç§æ•°å­¦æ¨ç†å’Œæ³›åŒ–åŸºå‡†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSPEC-RLåœ¨ä¸å½±å“ç­–ç•¥è´¨é‡çš„å‰æä¸‹ï¼Œå°†rolloutæ—¶é—´å‡å°‘äº†2-3å€ã€‚ä½œä¸ºä¸€ä¸ªçº¯ç²¹çš„rollouté˜¶æ®µå¢å¼ºï¼ŒSPEC-RLå¯ä»¥ä¸ä¸»æµç®—æ³•ï¼ˆä¾‹å¦‚ï¼ŒPPOã€GRPOã€DAPOï¼‰æ— ç¼é›†æˆï¼Œä¸ºæ‰©å±•å¤§å‹æ¨ç†æ¨¡å‹çš„RLVRæä¾›äº†ä¸€æ¡é€šç”¨ä¸”å®ç”¨çš„é€”å¾„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³åœ¨åŸºäºå¼ºåŒ–å­¦ä¹ çš„å¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒä¸­ï¼Œrollouté˜¶æ®µè®¡ç®—å¼€é”€è¿‡å¤§çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚å¹¶è¡ŒåŒ–ã€ä¿®æ”¹ç›®æ ‡å‡½æ•°æˆ–ä½¿ç”¨replay bufferï¼Œè¦ä¹ˆæ•ˆæœæœ‰é™ï¼Œè¦ä¹ˆå¼•å…¥åå·®ï¼Œå¹¶ä¸”å¿½ç•¥äº†è¿ç»­è¿­ä»£ä¹‹é—´rolloutè½¨è¿¹çš„å†—ä½™æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSPEC-RLçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨è¿ç»­è®­ç»ƒepochä¹‹é—´rolloutè½¨è¿¹çš„ç›¸ä¼¼æ€§ï¼Œé€šè¿‡æ¨æµ‹æ€§è§£ç é‡ç”¨å…ˆå‰è½¨è¿¹ç‰‡æ®µä½œä¸ºå‰ç¼€ï¼Œä»è€Œå‡å°‘é‡å¤è®¡ç®—ã€‚ç±»ä¼¼äºæ¨æµ‹æ‰§è¡Œï¼Œè¯¥æ–¹æ³•é¦–å…ˆå¿«é€Ÿç”Ÿæˆä¸€ä¸ªâ€œè‰ç¨¿â€ï¼ˆdraftï¼‰ï¼Œç„¶åé€šè¿‡éªŒè¯æœºåˆ¶ç¡®ä¿ç­–ç•¥çš„ä¸€è‡´æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSPEC-RLæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) è½¨è¿¹ç‰‡æ®µå­˜å‚¨ï¼šå­˜å‚¨å…ˆå‰epochçš„rolloutè½¨è¿¹ç‰‡æ®µï¼›2) æ¨æµ‹æ€§å‰ç¼€ç”Ÿæˆï¼šä»å­˜å‚¨çš„ç‰‡æ®µä¸­é€‰æ‹©åˆé€‚çš„ç‰‡æ®µä½œä¸ºå½“å‰rolloutçš„æ¨æµ‹æ€§å‰ç¼€ï¼›3) è‰ç¨¿ç”Ÿæˆï¼šåŸºäºæ¨æµ‹æ€§å‰ç¼€ï¼Œå¿«é€Ÿç”Ÿæˆå®Œæ•´çš„rolloutè½¨è¿¹è‰ç¨¿ï¼›4) éªŒè¯ï¼šä½¿ç”¨éªŒè¯æœºåˆ¶è¯„ä¼°è‰ç¨¿è½¨è¿¹çš„è´¨é‡ï¼Œå¹¶æ ¹æ®è¯„ä¼°ç»“æœè¿›è¡Œè°ƒæ•´æˆ–é‡æ–°ç”Ÿæˆã€‚

**å…³é”®åˆ›æ–°**ï¼šSPEC-RLçš„å…³é”®åˆ›æ–°åœ¨äºå°†æ¨æµ‹æ€§è§£ç çš„æ€æƒ³å¼•å…¥åˆ°å¼ºåŒ–å­¦ä¹ çš„rolloutè¿‡ç¨‹ä¸­ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒSPEC-RLé¿å…äº†ä»å¤´å¼€å§‹ç”Ÿæˆå®Œæ•´çš„rolloutè½¨è¿¹ï¼Œè€Œæ˜¯é€šè¿‡é‡ç”¨å’Œæ‰©å±•å…ˆå‰çš„ä¿¡æ¯æ¥åŠ é€Ÿrolloutè¿‡ç¨‹ã€‚è¿™ç§æ–¹æ³•åœ¨ä¿è¯ç­–ç•¥è´¨é‡çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ã€‚

**å…³é”®è®¾è®¡**ï¼šSPEC-RLçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å¦‚ä½•é€‰æ‹©åˆé€‚çš„æ¨æµ‹æ€§å‰ç¼€ï¼šå¯èƒ½æ¶‰åŠåˆ°ç›¸ä¼¼åº¦åº¦é‡å’Œé€‰æ‹©ç­–ç•¥ï¼›2) å¦‚ä½•è®¾è®¡æœ‰æ•ˆçš„éªŒè¯æœºåˆ¶ï¼šç¡®ä¿æ¨æµ‹æ€§rolloutçš„ç­–ç•¥ä¸€è‡´æ€§ï¼Œé¿å…å¼•å…¥åå·®ï¼›3) å¦‚ä½•å¹³è¡¡æ¨æµ‹çš„å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡ï¼šéœ€è¦æƒè¡¡è‰ç¨¿ç”Ÿæˆçš„é€Ÿåº¦å’ŒéªŒè¯çš„å¼€é”€ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSPEC-RLåœ¨GSM8Kã€MATH-500ã€OlympiadBenchå’ŒMMLU-STEMç­‰å¤šä¸ªæ•°å­¦æ¨ç†å’Œæ³›åŒ–åŸºå‡†æµ‹è¯•ä¸­ï¼Œèƒ½å¤Ÿå°†rolloutæ—¶é—´å‡å°‘2-3å€ï¼ŒåŒæ—¶ä¿æŒä¸ç°æœ‰ç®—æ³•ï¼ˆå¦‚PPOã€GRPOã€DAPOï¼‰ç›¸å½“çš„ç­–ç•¥æ€§èƒ½ã€‚è¿™è¡¨æ˜SPEC-RLæ˜¯ä¸€ç§é«˜æ•ˆä¸”é€šç”¨çš„rolloutåŠ é€Ÿæ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SPEC-RLå¯å¹¿æ³›åº”ç”¨äºéœ€è¦å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¤æ‚æ¨ç†å’Œå†³ç­–çš„ä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚æ•°å­¦é—®é¢˜æ±‚è§£ã€ä»£ç ç”Ÿæˆå’Œå¯¹è¯ç³»ç»Ÿã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—é™ä½è®­ç»ƒæˆæœ¬ï¼ŒåŠ é€Ÿæ¨¡å‹è¿­ä»£ï¼Œå¹¶æ¨åŠ¨æ›´å¼ºå¤§ã€æ›´é«˜æ•ˆçš„AIç³»ç»Ÿçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) increasingly rely on reinforcement learning with verifiable rewards (RLVR) to elicit reliable chain-of-thought reasoning. However, the training process remains bottlenecked by the computationally expensive rollout stage. Existing acceleration methods-such as parallelization, objective- and data-driven modifications, and replay buffers-either incur diminishing returns, introduce bias, or overlook redundancy across iterations. We identify that rollouts from consecutive training epochs frequently share a large portion of overlapping segments, wasting computation. To address this, we propose SPEC-RL, a novel framework that integrates SPECulative decoding with the RL rollout process. SPEC-RL reuses prior trajectory segments as speculative prefixes and extends them via a draft-and-verify mechanism, avoiding redundant generation while ensuring policy consistency. Experiments on diverse math reasoning and generalization benchmarks, including GSM8K, MATH-500, OlympiadBench, MMLU-STEM, and others, demonstrate that SPEC-RL reduces rollout time by 2-3x without compromising policy quality. As a purely rollout-stage enhancement, SPEC-RL integrates seamlessly with mainstream algorithms (e.g., PPO, GRPO, DAPO), offering a general and practical path to scale RLVR for large reasoning models. Our code is available at https://github.com/ShopeeLLM/Spec-RL

