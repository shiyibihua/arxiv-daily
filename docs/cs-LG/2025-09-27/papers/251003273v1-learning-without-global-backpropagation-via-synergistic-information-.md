---
layout: default
title: Learning without Global Backpropagation via Synergistic Information Distillation
---

# Learning without Global Backpropagation via Synergistic Information Distillation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.03273" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.03273v1</a>
  <a href="https://arxiv.org/pdf/2510.03273.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.03273v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.03273v1', 'Learning without Global Backpropagation via Synergistic Information Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chenhao Ye, Ming Tang

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-27

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/ychAlbert/sid-bp)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºååŒä¿¡æ¯è’¸é¦ï¼ˆSIDï¼‰æ¡†æ¶ï¼Œè§£å†³æ·±åº¦å­¦ä¹ åå‘ä¼ æ’­çš„æ‰©å±•æ€§ç“¶é¢ˆã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `ååŒä¿¡æ¯è’¸é¦` `å¹¶è¡Œè®­ç»ƒ` `æ— åå‘ä¼ æ’­` `ä½å†…å­˜æ¶ˆè€—` `æ·±åº¦å­¦ä¹ ` `å›¾åƒåˆ†ç±»` `æ ‡ç­¾å™ªå£°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. åå‘ä¼ æ’­å­˜åœ¨æ›´æ–°é”å®šå’Œé«˜å†…å­˜æ¶ˆè€—é—®é¢˜ï¼Œé™åˆ¶äº†æ·±åº¦å­¦ä¹ æ¨¡å‹çš„å¯æ‰©å±•æ€§ã€‚
2. ååŒä¿¡æ¯è’¸é¦ï¼ˆSIDï¼‰å°†æ·±åº¦å­¦ä¹ é‡æ„ä¸ºå±€éƒ¨ååŒç»†åŒ–é—®é¢˜çš„çº§è”ï¼Œå®ç°æ¨¡å—é—´çš„å¹¶è¡Œè®­ç»ƒã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒSIDåœ¨åˆ†ç±»ç²¾åº¦ä¸Šä¸åå‘ä¼ æ’­ç›¸å½“ç”šè‡³è¶…è¶Šï¼Œå¹¶å…·æœ‰æ›´å¥½çš„å¯æ‰©å±•æ€§å’ŒæŠ—å™ªæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åå‘ä¼ æ’­ï¼ˆBPï¼‰æ˜¯æ·±åº¦å­¦ä¹ çš„åŸºç¡€ï¼Œä½†å­˜åœ¨ä¸¤ä¸ªå…³é”®çš„å¯æ‰©å±•æ€§ç“¶é¢ˆï¼šæ›´æ–°é”å®šï¼ˆç½‘ç»œæ¨¡å—åœ¨æ•´ä¸ªåå‘ä¼ æ’­å®Œæˆå‰ä¿æŒç©ºé—²ï¼‰ä»¥åŠç”±äºå­˜å‚¨æ¿€æ´»å€¼ä»¥è®¡ç®—æ¢¯åº¦è€Œå¯¼è‡´çš„é«˜å†…å­˜æ¶ˆè€—ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„è®­ç»ƒæ¡†æ¶â€”â€”ååŒä¿¡æ¯è’¸é¦ï¼ˆSIDï¼‰ï¼Œå®ƒå°†æ·±åº¦å­¦ä¹ é‡æ–°å®šä¹‰ä¸ºä¸€ç³»åˆ—å±€éƒ¨ååŒç»†åŒ–é—®é¢˜çš„çº§è”ã€‚åœ¨SIDä¸­ï¼Œæ·±åº¦ç½‘ç»œè¢«æ„å»ºä¸ºæ¨¡å—çš„æµæ°´çº¿ï¼Œæ¯ä¸ªæ¨¡å—éƒ½è¢«æ–½åŠ ä¸€ä¸ªå±€éƒ¨ç›®æ ‡ï¼Œä»¥ç»†åŒ–å…³äºground-truthç›®æ ‡çš„æ¦‚ç‡ä¿¡å¿µã€‚è¯¥ç›®æ ‡å¹³è¡¡äº†å¯¹ç›®æ ‡çš„ä¿çœŸåº¦ä¸æ¥è‡ªå…¶å‰ç½®æ¨¡å—çš„ä¿¡å¿µçš„ä¸€è‡´æ€§ã€‚é€šè¿‡è§£è€¦æ¨¡å—ä¹‹é—´çš„åå‘ä¾èµ–å…³ç³»ï¼ŒSIDèƒ½å¤Ÿå®ç°å¹¶è¡Œè®­ç»ƒï¼Œä»è€Œæ¶ˆé™¤æ›´æ–°é”å®šå¹¶æ˜¾è‘—é™ä½å†…å­˜éœ€æ±‚ã€‚åŒæ—¶ï¼Œè¿™ç§è®¾è®¡ä¿ç•™äº†æ ‡å‡†çš„æ­£å‘æ¨ç†è¿‡ç¨‹ï¼Œä½¿SIDæˆä¸ºBPçš„å¤šåŠŸèƒ½å³æ’å³ç”¨æ›¿ä»£æ–¹æ¡ˆã€‚æˆ‘ä»¬æä¾›äº†ç†è®ºåŸºç¡€ï¼Œè¯æ˜SIDä¿è¯äº†ç½‘ç»œæ·±åº¦å¸¦æ¥çš„å•è°ƒæ€§èƒ½æå‡ã€‚åœ¨å®éªŒä¸Šï¼ŒSIDå§‹ç»ˆåŒ¹é…æˆ–è¶…è¿‡BPçš„åˆ†ç±»ç²¾åº¦ï¼Œè¡¨ç°å‡ºå“è¶Šçš„å¯æ‰©å±•æ€§å’Œå¯¹æ ‡ç­¾å™ªå£°çš„æ˜¾è‘—é²æ£’æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šä¼ ç»Ÿåå‘ä¼ æ’­ç®—æ³•åœ¨è®­ç»ƒæ·±åº¦ç¥ç»ç½‘ç»œæ—¶ï¼Œå­˜åœ¨ä¸¤ä¸ªä¸»è¦çš„ç—›ç‚¹ã€‚ä¸€æ˜¯æ›´æ–°é”å®šï¼Œå³æ¯ä¸ªç½‘ç»œæ¨¡å—å¿…é¡»ç­‰å¾…æ•´ä¸ªåå‘ä¼ æ’­è¿‡ç¨‹å®Œæˆåæ‰èƒ½æ›´æ–°å‚æ•°ï¼Œå¯¼è‡´è®­ç»ƒæ•ˆç‡ä½ä¸‹ã€‚äºŒæ˜¯é«˜å†…å­˜æ¶ˆè€—ï¼Œå› ä¸ºéœ€è¦å­˜å‚¨æ‰€æœ‰æ¿€æ´»å€¼ä»¥è®¡ç®—æ¢¯åº¦ï¼Œè¿™åœ¨è®­ç»ƒå¤§å‹æ¨¡å‹æ—¶ä¼šæˆä¸ºä¸¥é‡çš„ç“¶é¢ˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSIDçš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†æ·±åº¦å­¦ä¹ è¿‡ç¨‹åˆ†è§£ä¸ºä¸€ç³»åˆ—å±€éƒ¨ååŒç»†åŒ–é—®é¢˜ã€‚æ¯ä¸ªæ¨¡å—ä¸å†ä¾èµ–å…¨å±€çš„åå‘ä¼ æ’­ä¿¡å·ï¼Œè€Œæ˜¯é€šè¿‡å±€éƒ¨ç›®æ ‡æ¥ä¼˜åŒ–è‡ªèº«ï¼Œè¯¥ç›®æ ‡ç»“åˆäº†å¯¹çœŸå®æ ‡ç­¾çš„é¢„æµ‹å’Œæ¥è‡ªå‰ä¸€ä¸ªæ¨¡å—çš„ä¿¡å¿µã€‚è¿™ç§è§£è€¦çš„è®¾è®¡å…è®¸æ¨¡å—å¹¶è¡Œè®­ç»ƒï¼Œä»è€Œé¿å…äº†æ›´æ–°é”å®šã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSIDå°†æ·±åº¦ç½‘ç»œæ„å»ºä¸ºä¸€ä¸ªæ¨¡å—æµæ°´çº¿ã€‚æ¯ä¸ªæ¨¡å—æ¥æ”¶å‰ä¸€ä¸ªæ¨¡å—çš„è¾“å‡ºï¼ˆå³æ¦‚ç‡ä¿¡å¿µï¼‰ï¼Œå¹¶å°è¯•ç»†åŒ–è¿™ä¸ªä¿¡å¿µï¼Œä½¿å…¶æ›´æ¥è¿‘çœŸå®æ ‡ç­¾ã€‚æ¯ä¸ªæ¨¡å—éƒ½æœ‰ä¸€ä¸ªå±€éƒ¨æŸå¤±å‡½æ•°ï¼Œè¯¥æŸå¤±å‡½æ•°ç”±ä¸¤éƒ¨åˆ†ç»„æˆï¼šä¸€éƒ¨åˆ†è¡¡é‡æ¨¡å—çš„é¢„æµ‹ä¸çœŸå®æ ‡ç­¾ä¹‹é—´çš„å·®å¼‚ï¼Œå¦ä¸€éƒ¨åˆ†è¡¡é‡æ¨¡å—çš„é¢„æµ‹ä¸å‰ä¸€ä¸ªæ¨¡å—çš„ä¿¡å¿µä¹‹é—´çš„å·®å¼‚ã€‚é€šè¿‡æœ€å°åŒ–è¿™ä¸ªå±€éƒ¨æŸå¤±å‡½æ•°ï¼Œæ¯ä¸ªæ¨¡å—éƒ½èƒ½åœ¨ä¿æŒä¸å‰ä¸€ä¸ªæ¨¡å—ä¸€è‡´æ€§çš„åŒæ—¶ï¼Œæé«˜é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šSIDæœ€å…³é”®çš„åˆ›æ–°åœ¨äºå®ƒæ‰“ç ´äº†åå‘ä¼ æ’­çš„å…¨å±€ä¾èµ–æ€§ã€‚é€šè¿‡å¼•å…¥å±€éƒ¨ç›®æ ‡å’Œæ¨¡å—é—´çš„ååŒæœºåˆ¶ï¼ŒSIDå®ç°äº†æ¨¡å—çš„å¹¶è¡Œè®­ç»ƒï¼Œä»è€Œæ˜¾è‘—æé«˜äº†è®­ç»ƒæ•ˆç‡å¹¶é™ä½äº†å†…å­˜éœ€æ±‚ã€‚ä¸ä¼ ç»Ÿçš„åå‘ä¼ æ’­ç›¸æ¯”ï¼ŒSIDä¸éœ€è¦å­˜å‚¨æ‰€æœ‰æ¿€æ´»å€¼ï¼Œåªéœ€è¦å­˜å‚¨ç›¸é‚»æ¨¡å—ä¹‹é—´çš„ä¿¡å¿µä¼ é€’ä¿¡æ¯ã€‚

**å…³é”®è®¾è®¡**ï¼šSIDçš„å…³é”®è®¾è®¡åŒ…æ‹¬å±€éƒ¨æŸå¤±å‡½æ•°çš„é€‰æ‹©å’Œæ¨¡å—é—´çš„ä¿¡å¿µä¼ é€’æœºåˆ¶ã€‚è®ºæ–‡ä¸­ä½¿ç”¨äº†äº¤å‰ç†µæŸå¤±æ¥è¡¡é‡é¢„æµ‹ä¸çœŸå®æ ‡ç­¾ä¹‹é—´çš„å·®å¼‚ï¼Œå¹¶ä½¿ç”¨KLæ•£åº¦æ¥è¡¡é‡æ¨¡å—é—´çš„ä¿¡å¿µä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æä¾›äº†ä¸€ä¸ªç†è®ºæ¡†æ¶ï¼Œè¯æ˜äº†SIDèƒ½å¤Ÿä¿è¯éšç€ç½‘ç»œæ·±åº¦çš„å¢åŠ ï¼Œæ€§èƒ½å•è°ƒæå‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

SIDåœ¨å¤šä¸ªå›¾åƒåˆ†ç±»æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜ï¼Œåœ¨ä¿è¯ç”šè‡³è¶…è¿‡åå‘ä¼ æ’­ç²¾åº¦çš„å‰æä¸‹ï¼ŒSIDæ˜¾è‘—é™ä½äº†å†…å­˜æ¶ˆè€—å¹¶å®ç°äº†å¹¶è¡Œè®­ç»ƒã€‚ä¾‹å¦‚ï¼Œåœ¨ImageNetæ•°æ®é›†ä¸Šï¼ŒSIDçš„åˆ†ç±»ç²¾åº¦ä¸åå‘ä¼ æ’­ç›¸å½“ï¼Œä½†è®­ç»ƒé€Ÿåº¦æ›´å¿«ï¼Œå†…å­˜å ç”¨æ›´å°‘ã€‚æ­¤å¤–ï¼ŒSIDè¿˜è¡¨ç°å‡ºå¯¹æ ‡ç­¾å™ªå£°çš„é²æ£’æ€§ï¼Œåœ¨å­˜åœ¨å™ªå£°æ ‡ç­¾çš„æƒ…å†µä¸‹ï¼ŒSIDçš„æ€§èƒ½ä¼˜äºåå‘ä¼ æ’­ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SIDå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦è®­ç»ƒå¤§è§„æ¨¡æ·±åº¦å­¦ä¹ æ¨¡å‹çš„åœºæ™¯ä¸­ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥åº”ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰ç­‰é¢†åŸŸï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸‹ï¼ŒSIDçš„ä½å†…å­˜æ¶ˆè€—å’Œå¹¶è¡Œè®­ç»ƒèƒ½åŠ›ä½¿å…¶æˆä¸ºä¸€ä¸ªæœ‰å¸å¼•åŠ›çš„é€‰æ‹©ã€‚æ­¤å¤–ï¼ŒSIDçš„æŠ—å™ªæ€§ä½¿å…¶åœ¨å¤„ç†å¸¦æœ‰å™ªå£°æ ‡ç­¾çš„æ•°æ®é›†æ—¶å…·æœ‰ä¼˜åŠ¿ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Backpropagation (BP), while foundational to deep learning, imposes two critical scalability bottlenecks: update locking, where network modules remain idle until the entire backward pass completes, and high memory consumption due to storing activations for gradient computation. To address these limitations, we introduce Synergistic Information Distillation (SID), a novel training framework that reframes deep learning as a cascade of local cooperative refinement problems. In SID, a deep network is structured as a pipeline of modules, each imposed with a local objective to refine a probabilistic belief about the ground-truth target. This objective balances fidelity to the target with consistency to the belief from its preceding module. By decoupling the backward dependencies between modules, SID enables parallel training and hence eliminates update locking and drastically reduces memory requirements. Meanwhile, this design preserves the standard feed-forward inference pass, making SID a versatile drop-in replacement for BP. We provide a theoretical foundation, proving that SID guarantees monotonic performance improvement with network depth. Empirically, SID consistently matches or surpasses the classification accuracy of BP, exhibiting superior scalability and pronounced robustness to label noise.Code is available at: https://github.com/ychAlbert/sid-bp

