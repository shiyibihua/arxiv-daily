---
layout: default
title: PATCH: Learnable Tile-level Hybrid Sparsity for LLMs
---

# PATCH: Learnable Tile-level Hybrid Sparsity for LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.23410" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.23410v2</a>
  <a href="https://arxiv.org/pdf/2509.23410.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.23410v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.23410v2', 'PATCH: Learnable Tile-level Hybrid Sparsity for LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Younes Hourri, Mohammad Mozaffari, Maryam Mehri Dehnavi

**åˆ†ç±»**: cs.LG, cs.AI, cs.PF

**å‘å¸ƒæ—¥æœŸ**: 2025-09-27 (æ›´æ–°: 2025-10-15)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**PATCHï¼šé¢å‘LLMçš„å¯å­¦ä¹ ç“¦ç‰‡çº§æ··åˆç¨€ç–æ¡†æ¶ï¼Œå®ç°ç²¾åº¦ä¸åŠ é€Ÿçš„å¹³è¡¡ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `æ¨¡å‹å‰ªæ` `ç¨€ç–åŒ–` `æ··åˆç¨€ç–` `ç¡¬ä»¶åŠ é€Ÿ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å‰ªææ–¹æ³•åœ¨LLMä¸Šçš„åº”ç”¨å—é™äºéç»“æ„åŒ–ç¨€ç–å¸¦æ¥çš„ä¸è§„åˆ™è®¿å­˜å’ŒåŠç»“æ„åŒ–ç¨€ç–å¸¦æ¥çš„ç²¾åº¦æŸå¤±ã€‚
2. PATCHé€šè¿‡å¯å­¦ä¹ çš„ç“¦ç‰‡çº§æ©ç é€‰æ‹©æœºåˆ¶ï¼Œå®ç°è¿ç»­ç¨€ç–ç‡ï¼Œä»è€Œåœ¨ç²¾åº¦å’ŒåŠ é€Ÿä¹‹é—´å–å¾—å¹³è¡¡ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒPATCHåœ¨å¤šç§æ¨¡å‹ä¸Šå‡èƒ½ç¼©å°ä¸å¯†é›†æ¨¡å‹ç²¾åº¦å·®è·ï¼Œå¹¶åœ¨LLaMA-2 7Bä¸Šå®ç°æ˜¾è‘—åŠ é€Ÿå’Œç²¾åº¦æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ€§èƒ½å“è¶Šï¼Œä½†éƒ¨ç½²æ—¶é¢ä¸´å·¨å¤§çš„å†…å­˜å’Œè®¡ç®—æˆæœ¬ã€‚æ¨¡å‹å‰ªææ˜¯é™ä½è¿™äº›å¼€é”€çš„æœ‰æ•ˆæ–¹æ³•ï¼Œä½†ç°æœ‰æ–¹æ³•é¢ä¸´æŒ‘æˆ˜ï¼šéç»“æ„åŒ–ç¨€ç–æ€§è™½ç„¶èƒ½ä¿æŒç²¾åº¦ï¼Œä½†ä¼šäº§ç”Ÿä¸è§„åˆ™çš„è®¿é—®æ¨¡å¼ï¼Œé˜»ç¢GPUåŠ é€Ÿï¼›è€ŒåŠç»“æ„åŒ–çš„2:4ç¨€ç–æ€§è™½ç„¶å¯¹ç¡¬ä»¶å‹å¥½ï¼Œä½†å¼ºåˆ¶æ‰§è¡Œ50%çš„å›ºå®šæ¨¡å¼ï¼Œä¼šé™ä½æ¨¡å‹è´¨é‡ã€‚ä¸ºäº†å¼¥åˆè¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†PATCHï¼Œä¸€ç§æ··åˆç¨€ç–æ¡†æ¶ï¼Œèƒ½å¤Ÿå®ç°0%åˆ°50%ä¹‹é—´çš„è¿ç»­ç¨€ç–ç‡ã€‚PATCHå°†æƒé‡çŸ©é˜µåˆ’åˆ†ä¸ºç“¦ç‰‡ï¼Œå¹¶é€šè¿‡å¯å­¦ä¹ çš„æ©ç é€‰æ‹©æœºåˆ¶ï¼Œå°†æ¯ä¸ªç“¦ç‰‡åˆ†é…ä¸ºå¯†é›†æˆ–2:4ç¨€ç–ã€‚è¿™ç§è®¾è®¡æä¾›äº†å¯¹ç²¾åº¦-åŠ é€Ÿæƒè¡¡çš„ç»†ç²’åº¦æ§åˆ¶ï¼Œå¹¶æ”¯æŒè·¨å±‚éå‡åŒ€ç¨€ç–æ€§ï¼Œä»è€Œå¸¦æ¥å“è¶Šçš„æ•´ä½“è´¨é‡ã€‚åœ¨0.5Båˆ°8Bå‚æ•°çš„æ¨¡å‹ä¸Šï¼ŒPATCHå§‹ç»ˆç¼©å°ä¸å¯†é›†æ¨¡å‹ç²¾åº¦ä¹‹é—´çš„å·®è·ï¼ŒåŒæ—¶æä¾›å®é™…çš„åŠ é€Ÿã€‚ä¾‹å¦‚ï¼Œåœ¨é…å¤‡A6000 GPUçš„LLaMA-2 7Bæ¨¡å‹ä¸Šï¼ŒPATCHå®ç°äº†æ¯”å¯†é›†åŸºçº¿1.18x-1.38xçš„ç«¯åˆ°ç«¯åŠ é€Ÿï¼Œå¹¶ä¸”ç›¸æ¯”äºæœ€å…ˆè¿›çš„2:4å‰ªææ–¹æ³•MaskLLMï¼Œç²¾åº¦æé«˜äº†0.37%-2.96%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰LLMå‰ªææ–¹æ³•è¦ä¹ˆé‡‡ç”¨éç»“æ„åŒ–ç¨€ç–ï¼Œè™½ç„¶ç²¾åº¦é«˜ï¼Œä½†è®¿å­˜ä¸è§„åˆ™ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨GPUåŠ é€Ÿï¼›è¦ä¹ˆé‡‡ç”¨åŠç»“æ„åŒ–ç¨€ç–ï¼ˆå¦‚2:4ç¨€ç–ï¼‰ï¼Œè™½ç„¶ç¡¬ä»¶å‹å¥½ï¼Œä½†å›ºå®šç¨€ç–æ¯”ä¾‹é™åˆ¶äº†æ¨¡å‹è¡¨è¾¾èƒ½åŠ›ï¼Œå¯¼è‡´ç²¾åº¦ä¸‹é™ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§æ–¹æ³•ï¼Œæ—¢èƒ½ä¿è¯ç¡¬ä»¶åŠ é€Ÿå‹å¥½æ€§ï¼Œåˆèƒ½çµæ´»æ§åˆ¶ç¨€ç–æ¯”ä¾‹ï¼Œä»¥å®ç°ç²¾åº¦å’Œæ•ˆç‡çš„å¹³è¡¡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šPATCHçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†æƒé‡çŸ©é˜µåˆ’åˆ†ä¸ºç“¦ç‰‡ï¼ˆtileï¼‰ï¼Œå¹¶ä¸ºæ¯ä¸ªç“¦ç‰‡é€‰æ‹©åˆé€‚çš„ç¨€ç–æ¨¡å¼ï¼šå¯†é›†æˆ–2:4ç¨€ç–ã€‚é€šè¿‡å¯å­¦ä¹ çš„æ©ç é€‰æ‹©æœºåˆ¶ï¼ŒPATCHèƒ½å¤Ÿè‡ªåŠ¨å­¦ä¹ æ¯ä¸ªç“¦ç‰‡çš„æœ€ä½³ç¨€ç–æ¨¡å¼ï¼Œä»è€Œå®ç°å…¨å±€éå‡åŒ€çš„ç¨€ç–åˆ†å¸ƒã€‚è¿™ç§è®¾è®¡å…è®¸æ¨¡å‹åœ¨ä¸åŒå±‚æˆ–ä¸åŒåŒºåŸŸé‡‡ç”¨ä¸åŒçš„ç¨€ç–ç‡ï¼Œä»¥é€‚åº”ä¸åŒçš„å­¦ä¹ éœ€æ±‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPATCHæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š1) å°†æƒé‡çŸ©é˜µåˆ’åˆ†ä¸ºå¤§å°ç›¸ç­‰çš„ç“¦ç‰‡ï¼›2) ä¸ºæ¯ä¸ªç“¦ç‰‡ç”Ÿæˆä¸€ä¸ªå¯å­¦ä¹ çš„æ©ç ï¼Œç”¨äºæŒ‡ç¤ºè¯¥ç“¦ç‰‡åº”é‡‡ç”¨å¯†é›†æ¨¡å¼è¿˜æ˜¯2:4ç¨€ç–æ¨¡å¼ï¼›3) æ ¹æ®æ©ç å¯¹æƒé‡çŸ©é˜µè¿›è¡Œç¨€ç–åŒ–ï¼›4) åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒåŒæ—¶ä¼˜åŒ–æ¨¡å‹æƒé‡å’Œæ©ç ï¼Œä»¥æ‰¾åˆ°æœ€ä½³çš„ç¨€ç–æ¨¡å¼ã€‚

**å…³é”®åˆ›æ–°**ï¼šPATCHçš„å…³é”®åˆ›æ–°åœ¨äºå¼•å…¥äº†å¯å­¦ä¹ çš„ç“¦ç‰‡çº§æ··åˆç¨€ç–æ¨¡å¼ã€‚ä¸ä¼ ç»Ÿçš„å›ºå®šç¨€ç–æ¨¡å¼ç›¸æ¯”ï¼ŒPATCHèƒ½å¤Ÿæ ¹æ®æ•°æ®è‡ªé€‚åº”åœ°è°ƒæ•´ç¨€ç–ç‡ï¼Œä»è€Œæ›´å¥½åœ°å¹³è¡¡ç²¾åº¦å’Œæ•ˆç‡ã€‚æ­¤å¤–ï¼ŒPATCHé‡‡ç”¨ç“¦ç‰‡çº§çš„ç¨€ç–åŒ–æ–¹å¼ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨ç¡¬ä»¶åŠ é€Ÿå™¨ï¼Œæé«˜è®¡ç®—æ•ˆç‡ã€‚ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºï¼ŒPATCHä¸å†å±€é™äºå•ä¸€çš„ç¨€ç–æ¨¡å¼ï¼Œè€Œæ˜¯å…è®¸æ¨¡å‹åœ¨ä¸åŒçš„åŒºåŸŸé‡‡ç”¨ä¸åŒçš„ç¨€ç–æ¨¡å¼ï¼Œä»è€Œå®ç°æ›´çµæ´»çš„ç¨€ç–åŒ–ã€‚

**å…³é”®è®¾è®¡**ï¼šPATCHçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ç“¦ç‰‡å¤§å°çš„é€‰æ‹©ï¼šç“¦ç‰‡å¤§å°éœ€è¦æ ¹æ®ç¡¬ä»¶ç‰¹æ€§å’Œæ¨¡å‹å¤§å°è¿›è¡Œè°ƒæ•´ï¼Œä»¥è·å¾—æœ€ä½³çš„æ€§èƒ½ï¼›2) æ©ç å­¦ä¹ æœºåˆ¶ï¼šæ©ç å¯ä»¥é€šè¿‡å¤šç§æ–¹å¼è¿›è¡Œå­¦ä¹ ï¼Œä¾‹å¦‚ä½¿ç”¨sigmoidå‡½æ•°æˆ–softmaxå‡½æ•°ï¼›3) æŸå¤±å‡½æ•°çš„è®¾è®¡ï¼šæŸå¤±å‡½æ•°éœ€è¦åŒæ—¶è€ƒè™‘æ¨¡å‹çš„ç²¾åº¦å’Œç¨€ç–ç‡ï¼Œä»¥é¿å…è¿‡åº¦ç¨€ç–åŒ–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨LLaMA-2 7Bæ¨¡å‹ä¸Šï¼ŒPATCHåœ¨A6000 GPUä¸Šå®ç°äº†1.18x-1.38xçš„ç«¯åˆ°ç«¯åŠ é€Ÿï¼ŒåŒæ—¶ç›¸æ¯”äºæœ€å…ˆè¿›çš„2:4å‰ªææ–¹æ³•MaskLLMï¼Œç²¾åº¦æé«˜äº†0.37%-2.96%ã€‚æ­¤å¤–ï¼Œåœ¨0.5Båˆ°8Bå‚æ•°çš„å¤šä¸ªæ¨¡å‹ä¸Šï¼ŒPATCHå‡èƒ½æœ‰æ•ˆç¼©å°ä¸å¯†é›†æ¨¡å‹ç²¾åº¦ä¹‹é—´çš„å·®è·ï¼Œè¯æ˜äº†å…¶åœ¨ä¸åŒæ¨¡å‹ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

PATCHæ¡†æ¶å¯åº”ç”¨äºå„ç§å¤§å‹è¯­è¨€æ¨¡å‹çš„å‹ç¼©å’ŒåŠ é€Ÿï¼Œå°¤å…¶é€‚ç”¨äºèµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡æˆ–äº‘ç«¯éƒ¨ç½²ã€‚é€šè¿‡åœ¨ç²¾åº¦æŸå¤±å¯æ¥å—çš„èŒƒå›´å†…æ˜¾è‘—é™ä½æ¨¡å‹å¤§å°å’Œè®¡ç®—å¤æ‚åº¦ï¼ŒPATCHèƒ½å¤Ÿä½¿LLMåœ¨æ›´å¤šåœºæ™¯ä¸‹å¾—ä»¥åº”ç”¨ï¼Œä¾‹å¦‚ç§»åŠ¨è®¾å¤‡ä¸Šçš„æ™ºèƒ½åŠ©æ‰‹ã€ä½åŠŸè€—ç‰©è”ç½‘è®¾å¤‡ä¸Šçš„è‡ªç„¶è¯­è¨€å¤„ç†ç­‰ã€‚æœªæ¥ï¼ŒPATCHæœ‰æœ›æˆä¸ºLLMé«˜æ•ˆéƒ¨ç½²çš„å…³é”®æŠ€æœ¯ä¹‹ä¸€ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) deliver impressive performance but incur prohibitive memory and compute costs at deployment. Model pruning is an effective way to reduce these overheads, yet existing approaches face challenges: unstructured sparsity, where nonzeros can appear anywhere, preserves accuracy but yields irregular access patterns that prevent GPU acceleration, while semi-structured 2:4 sparsity is hardware-friendly but enforces a rigid 50% pattern that degrades model quality. To bridge this gap, we introduce PATCH, a hybrid sparsity framework that enables a continuous sparsity ratio between 0% and 50%. PATCH partitions weight matrices into tiles, assigning each tile to be either dense or 2:4 sparse via a learnable mask selection mechanism. This design provides fine-grained control over accuracy-acceleration tradeoffs and supports non-uniform sparsity across layers, leading to superior overall quality. Across models from 0.5B to 8B parameters, PATCH consistently narrows the gap to dense accuracy while delivering practical speedups. For instance, on LLaMA-2 7B with an A6000 GPU, PATCH achieves 1.18x-1.38x end-to-end speedup over dense baselines while improving accuracy by 0.37%-2.96% compared to the state-of-the-art 2:4 pruning method, MaskLLM.

