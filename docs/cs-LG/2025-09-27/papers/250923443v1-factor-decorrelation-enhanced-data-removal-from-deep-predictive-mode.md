---
layout: default
title: Factor Decorrelation Enhanced Data Removal from Deep Predictive Models
---

# Factor Decorrelation Enhanced Data Removal from Deep Predictive Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.23443" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.23443v1</a>
  <a href="https://arxiv.org/pdf/2509.23443.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.23443v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.23443v1', 'Factor Decorrelation Enhanced Data Removal from Deep Predictive Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Wenhao Yang, Lin Li, Xiaohui Tao, Kaize Shi

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-27

**å¤‡æ³¨**: accepted by NeurIPS 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå› å­è§£è€¦å¢å¼ºçš„æ•°æ®ç§»é™¤æ–¹æ³•ï¼Œæå‡æ·±åº¦é¢„æµ‹æ¨¡å‹åœ¨åˆ†å¸ƒåç§»ä¸‹çš„æ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ•°æ®ç§»é™¤` `å› å­è§£è€¦` `åˆ†å¸ƒå¤–æ³›åŒ–` `æ·±åº¦å­¦ä¹ ` `éšç§ä¿æŠ¤`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ•°æ®ç§»é™¤æ–¹æ³•åœ¨æ·±åº¦æ¨¡å‹ä¸­æ˜“å¼•èµ·åˆ†å¸ƒåç§»ï¼Œå¯¼è‡´æ¨¡å‹åœ¨åˆ†å¸ƒå¤–æ•°æ®ä¸Šæ€§èƒ½ä¸‹é™ã€‚
2. æå‡ºå› å­è§£è€¦å’ŒæŸå¤±æ‰°åŠ¨çš„æ•°æ®ç§»é™¤æ–¹æ³•ï¼Œé™ä½ç‰¹å¾å†—ä½™å’Œç‰¹å¾é—´ç›¸å…³æ€§ï¼Œé˜²æ­¢æ•°æ®æ³„éœ²ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæå‡äº†æ¨¡å‹åœ¨åˆ†å¸ƒåç§»ä¸‹çš„é¢„æµ‹ç²¾åº¦å’Œé²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç”¨æˆ·éšç§ä¿æŠ¤å’Œåˆè§„æ€§è¦æ±‚æ¨¡å‹è®­ç»ƒä¸­å¿…é¡»èƒ½å¤Ÿç§»é™¤æ•æ„Ÿæ•°æ®ï¼Œä½†è¿™ä¸€è¿‡ç¨‹é€šå¸¸ä¼šå¯¼è‡´åˆ†å¸ƒåç§»ï¼Œä»è€ŒæŸå®³æ¨¡å‹æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨åˆ†å¸ƒå¤–(OOD)åœºæ™¯ä¸­ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®ç§»é™¤æ–¹æ³•ï¼Œé€šè¿‡å› å­è§£è€¦å’ŒæŸå¤±æ‰°åŠ¨æ¥å¢å¼ºæ·±åº¦é¢„æµ‹æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ï¼šï¼ˆ1ï¼‰ä¸€ä¸ªåˆ¤åˆ«æ€§ä¿æŒçš„å› å­è§£è€¦æ¨¡å—ï¼Œè¯¥æ¨¡å—é‡‡ç”¨åŠ¨æ€è‡ªé€‚åº”æƒé‡è°ƒæ•´å’Œè¿­ä»£è¡¨ç¤ºæ›´æ–°ï¼Œä»¥å‡å°‘ç‰¹å¾å†—ä½™å¹¶æœ€å°åŒ–ç‰¹å¾é—´çš„ç›¸å…³æ€§ã€‚ï¼ˆ2ï¼‰ä¸€ç§å…·æœ‰æŸå¤±æ‰°åŠ¨çš„å¹³æ»‘æ•°æ®ç§»é™¤æœºåˆ¶ï¼Œè¯¥æœºåˆ¶åˆ›å»ºäº†ä¿¡æ¯è®ºä¿éšœï¼Œä»¥é˜²æ­¢ç§»é™¤æ“ä½œæœŸé—´çš„æ•°æ®æ³„éœ²ã€‚åœ¨äº”ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºå…¶ä»–åŸºçº¿ï¼Œå¹¶ä¸”å³ä½¿åœ¨æ˜¾ç€çš„åˆ†å¸ƒåç§»ä¸‹ä¹Ÿèƒ½å§‹ç»ˆå¦‚ä¸€åœ°å®ç°é«˜é¢„æµ‹ç²¾åº¦å’Œé²æ£’æ€§ã€‚ç»“æœçªå‡ºäº†å…¶åœ¨åŒåˆ†å¸ƒå’Œåˆ†å¸ƒå¤–åœºæ™¯ä¸­çš„å“è¶Šæ•ˆç‡å’Œé€‚åº”æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­æ•°æ®ç§»é™¤åï¼Œæ¨¡å‹æ€§èƒ½æ˜¾è‘—ä¸‹é™çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨é¢å¯¹åˆ†å¸ƒå¤–ï¼ˆOODï¼‰æ•°æ®æ—¶ã€‚ç°æœ‰çš„æ•°æ®ç§»é™¤æ–¹æ³•å¾€å¾€ä¼šå¼•å…¥åˆ†å¸ƒåç§»ï¼Œå¯¼è‡´æ¨¡å‹æ³›åŒ–èƒ½åŠ›å˜å·®ï¼Œæ— æ³•æœ‰æ•ˆåº”å¯¹çœŸå®ä¸–ç•Œä¸­å¤æ‚å¤šå˜çš„æ•°æ®åˆ†å¸ƒã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å› å­è§£è€¦æ¥å‡å°‘ç‰¹å¾ä¹‹é—´çš„å†—ä½™å’Œç›¸å…³æ€§ï¼Œä»è€Œæé«˜æ¨¡å‹å¯¹æ•°æ®å˜åŒ–çš„é²æ£’æ€§ã€‚åŒæ—¶ï¼Œé‡‡ç”¨æŸå¤±æ‰°åŠ¨çš„æ–¹å¼ï¼Œåœ¨æ•°æ®ç§»é™¤è¿‡ç¨‹ä¸­åŠ å…¥ä¿¡æ¯è®ºçš„çº¦æŸï¼Œé˜²æ­¢æ•æ„Ÿä¿¡æ¯æ³„éœ²ï¼Œä¿è¯æ•°æ®ç§»é™¤çš„å®‰å…¨æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šåˆ¤åˆ«æ€§ä¿æŒçš„å› å­è§£è€¦æ¨¡å—å’Œå¸¦æœ‰æŸå¤±æ‰°åŠ¨çš„å¹³æ»‘æ•°æ®ç§»é™¤æœºåˆ¶ã€‚å› å­è§£è€¦æ¨¡å—é€šè¿‡åŠ¨æ€è‡ªé€‚åº”æƒé‡è°ƒæ•´å’Œè¿­ä»£è¡¨ç¤ºæ›´æ–°æ¥é™ä½ç‰¹å¾å†—ä½™å’Œç‰¹å¾é—´ç›¸å…³æ€§ã€‚æ•°æ®ç§»é™¤æœºåˆ¶åˆ™é€šè¿‡æŸå¤±æ‰°åŠ¨ï¼Œåœ¨ç§»é™¤æ•°æ®çš„åŒæ—¶ï¼Œé˜²æ­¢æ¨¡å‹è®°ä½è¢«ç§»é™¤çš„æ•°æ®ä¿¡æ¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†å› å­è§£è€¦æŠ€æœ¯åº”ç”¨äºæ•°æ®ç§»é™¤ä»»åŠ¡ï¼Œå¹¶ç»“åˆæŸå¤±æ‰°åŠ¨æœºåˆ¶ï¼Œå®ç°äº†åœ¨ä¿è¯æ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œæœ‰æ•ˆé˜²æ­¢æ•°æ®æ³„éœ²ã€‚åŠ¨æ€è‡ªé€‚åº”æƒé‡è°ƒæ•´å’Œè¿­ä»£è¡¨ç¤ºæ›´æ–°çš„å› å­è§£è€¦æ¨¡å—æ˜¯å¦ä¸€ä¸ªåˆ›æ–°ç‚¹ï¼Œå®ƒèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°é™ä½ç‰¹å¾å†—ä½™å’Œç›¸å…³æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šå› å­è§£è€¦æ¨¡å—ä¸­ï¼ŒåŠ¨æ€è‡ªé€‚åº”æƒé‡è°ƒæ•´æ ¹æ®ç‰¹å¾çš„é‡è¦æ€§åŠ¨æ€è°ƒæ•´æƒé‡ï¼Œè¿­ä»£è¡¨ç¤ºæ›´æ–°åˆ™é€šè¿‡å¤šæ¬¡è¿­ä»£æ¥é€æ­¥è§£è€¦ç‰¹å¾ã€‚æŸå¤±æ‰°åŠ¨æœºåˆ¶é€šè¿‡åœ¨æŸå¤±å‡½æ•°ä¸­åŠ å…¥æ‰°åŠ¨é¡¹ï¼Œä½¿å¾—æ¨¡å‹åœ¨ç§»é™¤æ•°æ®åï¼Œéš¾ä»¥é‡å»ºè¢«ç§»é™¤çš„æ•°æ®ä¿¡æ¯ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ï¼ŒåŒ…æ‹¬å¦‚ä½•é€‰æ‹©åˆé€‚çš„æ‰°åŠ¨é¡¹å’Œè°ƒæ•´è¿­ä»£æ¬¡æ•°ç­‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨äº”ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå‡ä¼˜äºå…¶ä»–åŸºçº¿æ–¹æ³•ï¼Œå°¤å…¶æ˜¯åœ¨åˆ†å¸ƒåç§»è¾ƒå¤§çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½æå‡æ›´ä¸ºæ˜¾è‘—ã€‚å…·ä½“è€Œè¨€ï¼Œè¯¥æ–¹æ³•åœ¨OODåœºæ™¯ä¸‹çš„é¢„æµ‹ç²¾åº¦å¹³å‡æå‡äº†5%-10%ï¼Œå¹¶ä¸”åœ¨æ•°æ®ç§»é™¤åï¼Œæ¨¡å‹æ€§èƒ½ä¸‹é™çš„å¹…åº¦ä¹Ÿæ˜æ˜¾å°äºå…¶ä»–æ–¹æ³•ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦æ•°æ®ç§»é™¤çš„åœºæ™¯ï¼Œä¾‹å¦‚ç”¨æˆ·éšç§ä¿æŠ¤ã€æ¨¡å‹åˆè§„æ€§å®¡æŸ¥ç­‰ã€‚åœ¨é‡‘èã€åŒ»ç–—ç­‰æ•æ„Ÿæ•°æ®é¢†åŸŸï¼Œè¯¥æ–¹æ³•å¯ä»¥å¸®åŠ©ä¼ä¸šå®‰å…¨åœ°ç§»é™¤ä¸å†éœ€è¦çš„æ•°æ®ï¼ŒåŒæ—¶ä¿è¯æ¨¡å‹æ€§èƒ½ä¸å—å¤ªå¤§å½±å“ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºå¯¹æŠ—æ¨¡å‹æ”»å‡»ï¼Œæé«˜æ¨¡å‹çš„é²æ£’æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The imperative of user privacy protection and regulatory compliance necessitates sensitive data removal in model training, yet this process often induces distributional shifts that undermine model performance-particularly in out-of-distribution (OOD) scenarios. We propose a novel data removal approach that enhances deep predictive models through factor decorrelation and loss perturbation. Our approach introduces: (1) a discriminative-preserving factor decorrelation module employing dynamic adaptive weight adjustment and iterative representation updating to reduce feature redundancy and minimize inter-feature correlations. (2) a smoothed data removal mechanism with loss perturbation that creates information-theoretic safeguards against data leakage during removal operations. Extensive experiments on five benchmark datasets show that our approach outperforms other baselines and consistently achieves high predictive accuracy and robustness even under significant distribution shifts. The results highlight its superior efficiency and adaptability in both in-distribution and out-of-distribution scenarios.

