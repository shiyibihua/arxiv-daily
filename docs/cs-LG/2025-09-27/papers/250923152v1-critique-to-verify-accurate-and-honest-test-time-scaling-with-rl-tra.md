---
layout: default
title: Critique to Verify: Accurate and Honest Test-Time Scaling with RL-Trained Verifiers
---

# Critique to Verify: Accurate and Honest Test-Time Scaling with RL-Trained Verifiers

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.23152" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.23152v1</a>
  <a href="https://arxiv.org/pdf/2509.23152.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.23152v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.23152v1', 'Critique to Verify: Accurate and Honest Test-Time Scaling with RL-Trained Verifiers')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhicheng Yang, Zhijiang Guo, Yinya Huang, Yongxin Wang, Yiwei Wang, Xiaodan Liang, Jing Tang

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-27

**å¤‡æ³¨**: 15 pages, 7 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMirror-Critiqueæ¡†æ¶ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒéªŒè¯å™¨ï¼Œæå‡å¤§è¯­è¨€æ¨¡å‹æµ‹è¯•æ—¶æ¨ç†çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `æµ‹è¯•æ—¶ç¼©æ”¾` `å¼ºåŒ–å­¦ä¹ ` `éªŒè¯å™¨è®­ç»ƒ` `æ‰¹åˆ¤å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤§è¯­è¨€æ¨¡å‹æµ‹è¯•æ—¶ç¼©æ”¾æ–¹æ³•ä¾èµ–å¥–åŠ±æ¨¡å‹é€‰æ‹©ï¼Œä½†å…¶éš¾ä»¥è¯†åˆ«å°‘é‡æ­£ç¡®ç­”æ¡ˆï¼Œé™åˆ¶äº†æ€§èƒ½ã€‚
2. Mirror-Critiqueæ¡†æ¶é€šè¿‡å¯¹æ¯”æ¨¡å‹è§£ä¸çœŸå®è§£ï¼Œåˆ©ç”¨é«˜è´¨é‡æ‰¹åˆ¤ä¿¡å·è®­ç»ƒéªŒè¯å™¨ï¼Œæå‡éªŒè¯èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒMirror-Verifieræ˜¾è‘—æå‡äº†è§£çš„å‡†ç¡®æ€§ï¼Œå¹¶å¢å¼ºäº†æ¨¡å‹è¯†åˆ«è‡ªèº«èƒ½åŠ›è¾¹ç•Œçš„è¯šå®æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºMirror-Critiqueçš„æ¡†æ¶ï¼Œæ—¨åœ¨æå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æµ‹è¯•æ—¶é€šè¿‡è§£é‡‡æ ·å’Œèšåˆè¿›è¡Œç¼©æ”¾çš„æ¨ç†æ€§èƒ½ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸é‡‡ç”¨å¥–åŠ±æ¨¡å‹é€‰æ‹©ï¼Œä½†å…¶æ— æ³•æœ‰æ•ˆè¯†åˆ«å°‘é‡ä½†æ­£ç¡®çš„ç­”æ¡ˆï¼Œé™åˆ¶äº†å…¶æ•ˆæœã€‚Mirror-Critiqueé€šè¿‡å¯¹æ¯”æ¨¡å‹ç”Ÿæˆçš„è§£ä¸çœŸå®è§£ï¼Œåˆ©ç”¨ä¸°å¯Œçš„æ‰¹åˆ¤ä¿¡å·è®­ç»ƒéªŒè¯å™¨ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•ç¼ºä¹ä¿¡æ¯æ€§æ‰¹åˆ¤ä¿¡å·çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶ä½¿ç”¨ä¸€ä¸ªå°å‹æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹ï¼Œé€šè¿‡æ‹’ç»é‡‡æ ·åˆæˆé«˜è´¨é‡çš„æ‰¹åˆ¤æ•°æ®ï¼Œæ•™å¯¼éªŒè¯å™¨è¯†åˆ«é”™è¯¯åŠå…¶åŸå› ã€‚åˆæˆæ•°æ®ç”¨äºå†·å¯åŠ¨RLVRè¿‡ç¨‹ä¸­çš„LLMsï¼Œè¿›ä¸€æ­¥æé«˜éªŒè¯èƒ½åŠ›ã€‚æœ€ç»ˆçš„Mirror-Verifieré€šè¿‡ä¸ºæ¯ä¸ªè§£ç”Ÿæˆå¤šä¸ªæ‰¹åˆ¤ï¼Œå¹¶å°†å®ƒä»¬èšåˆæˆä¸€ä¸ªéªŒè¯åˆ†æ•°ï¼Œç”¨äºåŠ æƒæŠ•ç¥¨æˆ–é€‰æ‹©æ€§æ‹’ç»å›ç­”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMirror-Verifieråœ¨è§£çš„å‡†ç¡®æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºå¤šæ•°æŠ•ç¥¨ï¼Œå¹¶æé«˜äº†æ±‚è§£å™¨è¯†åˆ«å’Œæ‹’ç»å›ç­”è¶…å‡ºå…¶èƒ½åŠ›èŒƒå›´é—®é¢˜çš„è¯šå®æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§è¯­è¨€æ¨¡å‹åœ¨æµ‹è¯•æ—¶è¿›è¡Œç¼©æ”¾çš„æ–¹æ³•ï¼Œé€šå¸¸ä¾èµ–äºå¥–åŠ±æ¨¡å‹æ¥é€‰æ‹©æœ€ä½³ç­”æ¡ˆã€‚ç„¶è€Œï¼Œè¿™äº›å¥–åŠ±æ¨¡å‹å¾€å¾€æ— æ³•å‡†ç¡®è¯†åˆ«é‚£äº›å°‘æ•°ä½†æ­£ç¡®çš„ç­”æ¡ˆï¼Œå¯¼è‡´æœ€ç»ˆæ€§èƒ½æå‡æœ‰é™ï¼Œç”šè‡³ä¸å¦‚ç®€å•çš„å¤šæ•°æŠ•ç¥¨ç­–ç•¥ã€‚é—®é¢˜çš„æ ¸å¿ƒåœ¨äºï¼ŒéªŒè¯å™¨åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç¼ºä¹è¶³å¤Ÿçš„ä¿¡æ¯æ€§æ‰¹åˆ¤ä¿¡å·ï¼Œéš¾ä»¥åŒºåˆ†æ­£ç¡®å’Œé”™è¯¯çš„ç­”æ¡ˆï¼Œå°¤å…¶æ˜¯å½“æ­£ç¡®ç­”æ¡ˆå±äºå°‘æ•°æƒ…å†µæ—¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMirror-Critiqueçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥æ›´ä¸°å¯Œçš„æ‰¹åˆ¤ä¿¡å·æ¥è®­ç»ƒéªŒè¯å™¨ã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥æ–¹æ³•é€šè¿‡å¯¹æ¯”æ¨¡å‹ç”Ÿæˆçš„è§£ä¸çœŸå®è§£ï¼Œè®©éªŒè¯å™¨å­¦ä¹ è¯†åˆ«é”™è¯¯çš„åŸå› å’Œæ–¹å¼ã€‚è¿™ç§å¯¹æ¯”å­¦ä¹ çš„æ–¹å¼èƒ½å¤Ÿæä¾›æ›´å¼ºçš„ç›‘ç£ä¿¡å·ï¼Œå¸®åŠ©éªŒè¯å™¨æ›´å¥½åœ°ç†è§£æ­£ç¡®ç­”æ¡ˆçš„ç‰¹å¾ï¼Œä»è€Œæé«˜å…¶è¯†åˆ«å°‘æ•°æ­£ç¡®ç­”æ¡ˆçš„èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMirror-Critiqueæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) **æ‰¹åˆ¤æ•°æ®åˆæˆ**ï¼šä½¿ç”¨ä¸€ä¸ªå°å‹æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹ï¼Œé€šè¿‡æ‹’ç»é‡‡æ ·çš„æ–¹å¼ï¼Œç”Ÿæˆé«˜è´¨é‡çš„æ‰¹åˆ¤æ•°æ®ã€‚è¿™äº›æ•°æ®ä¸ä»…åŒ…å«å¯¹é”™è¯¯ç­”æ¡ˆçš„å¦å®šï¼Œè¿˜åŒ…å«å¯¹é”™è¯¯åŸå› çš„è§£é‡Šã€‚2) **éªŒè¯å™¨è®­ç»ƒ**ï¼šä½¿ç”¨åˆæˆçš„æ‰¹åˆ¤æ•°æ®å†·å¯åŠ¨å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ï¼Œè®­ç»ƒéªŒè¯å™¨ã€‚éªŒè¯å™¨å­¦ä¹ æ ¹æ®æ¨¡å‹ç”Ÿæˆçš„è§£å’Œæ‰¹åˆ¤ä¿¡æ¯ï¼Œç»™å‡ºä¸€ä¸ªéªŒè¯åˆ†æ•°ã€‚3) **è§£è¯„ä¼°ä¸èšåˆ**ï¼šåœ¨æµ‹è¯•æ—¶ï¼Œå¯¹äºæ¯ä¸ªå€™é€‰è§£ï¼ŒéªŒè¯å™¨ç”Ÿæˆå¤šä¸ªæ‰¹åˆ¤ï¼Œå¹¶å°†è¿™äº›æ‰¹åˆ¤èšåˆæˆä¸€ä¸ªéªŒè¯åˆ†æ•°ã€‚è¯¥åˆ†æ•°ç”¨äºåŠ æƒæŠ•ç¥¨æˆ–é€‰æ‹©æ€§æ‹’ç»å›ç­”ã€‚

**å…³é”®åˆ›æ–°**ï¼šMirror-Critiqueçš„å…³é”®åˆ›æ–°åœ¨äºå¼•å…¥äº†å¯¹æ¯”å­¦ä¹ çš„æ€æƒ³ï¼Œé€šè¿‡å¯¹æ¯”æ¨¡å‹ç”Ÿæˆçš„è§£ä¸çœŸå®è§£ï¼Œä¸ºéªŒè¯å™¨æä¾›äº†æ›´ä¸°å¯Œçš„æ‰¹åˆ¤ä¿¡å·ã€‚è¿™ç§æ–¹æ³•ä¸ä»…èƒ½å¤Ÿæé«˜éªŒè¯å™¨çš„å‡†ç¡®æ€§ï¼Œè¿˜èƒ½å¤Ÿå¢å¼ºå…¶è¯†åˆ«è‡ªèº«èƒ½åŠ›è¾¹ç•Œçš„è¯šå®æ€§ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒMirror-Critiqueä¸å†ä»…ä»…ä¾èµ–å¥–åŠ±æ¨¡å‹ï¼Œè€Œæ˜¯é€šè¿‡æ›´ç»†ç²’åº¦çš„æ‰¹åˆ¤ä¿¡æ¯æ¥æŒ‡å¯¼éªŒè¯å™¨çš„å­¦ä¹ ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ‰¹åˆ¤æ•°æ®åˆæˆé˜¶æ®µï¼Œä½¿ç”¨äº†æ‹’ç»é‡‡æ ·ç­–ç•¥æ¥ä¿è¯æ‰¹åˆ¤æ•°æ®çš„è´¨é‡ã€‚å…·ä½“æ¥è¯´ï¼Œåªæœ‰å½“ç”Ÿæˆçš„æ‰¹åˆ¤ä¿¡æ¯èƒ½å¤Ÿå‡†ç¡®æè¿°æ¨¡å‹è§£çš„é”™è¯¯æ—¶ï¼Œæ‰ä¼šè¢«ä¿ç•™ã€‚åœ¨éªŒè¯å™¨è®­ç»ƒé˜¶æ®µï¼Œä½¿ç”¨äº†å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œä»¥æœ€å¤§åŒ–éªŒè¯å™¨å¯¹æ­£ç¡®ç­”æ¡ˆçš„éªŒè¯åˆ†æ•°ï¼Œå¹¶æœ€å°åŒ–å¯¹é”™è¯¯ç­”æ¡ˆçš„éªŒè¯åˆ†æ•°ã€‚æ­¤å¤–ï¼Œè¿˜è®¾è®¡äº†ä¸€ç§èšåˆç­–ç•¥ï¼Œå°†å¤šä¸ªæ‰¹åˆ¤ä¿¡æ¯èšåˆæˆä¸€ä¸ªæœ€ç»ˆçš„éªŒè¯åˆ†æ•°ï¼Œä»¥æé«˜è¯„ä¼°çš„é²æ£’æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMirror-Verifieråœ¨è§£çš„å‡†ç¡®æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºå¤šæ•°æŠ•ç¥¨ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦è¯†åˆ«å°‘æ•°æ­£ç¡®ç­”æ¡ˆçš„åœºæ™¯ä¸‹ã€‚æ­¤å¤–ï¼ŒMirror-Verifierè¿˜æé«˜äº†æ±‚è§£å™¨è¯†åˆ«å’Œæ‹’ç»å›ç­”è¶…å‡ºå…¶èƒ½åŠ›èŒƒå›´é—®é¢˜çš„è¯šå®æ€§ï¼Œé™ä½äº†æ¨¡å‹ç»™å‡ºé”™è¯¯ç­”æ¡ˆçš„æ¦‚ç‡ã€‚å…·ä½“çš„æ€§èƒ½æå‡æ•°æ®åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†çš„å±•ç¤ºå’Œåˆ†æã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Mirror-Critiqueæ¡†æ¶å¯åº”ç”¨äºå„ç§éœ€è¦å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œæ¨ç†å’Œå†³ç­–çš„åœºæ™¯ï¼Œä¾‹å¦‚é—®ç­”ç³»ç»Ÿã€ä»£ç ç”Ÿæˆã€æ–‡æœ¬æ‘˜è¦ç­‰ã€‚é€šè¿‡æé«˜æ¨¡å‹æ¨ç†çš„å‡†ç¡®æ€§å’Œå¯é æ€§ï¼Œè¯¥æ¡†æ¶å¯ä»¥æå‡ç”¨æˆ·ä½“éªŒï¼Œå¹¶é™ä½æ¨¡å‹å‡ºé”™çš„é£é™©ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜å¯ä»¥ç”¨äºè¯„ä¼°å’Œæ”¹è¿›å¤§è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Œä¿ƒè¿›å…¶åœ¨æ›´å¹¿æ³›é¢†åŸŸçš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Test-time scaling via solution sampling and aggregation has become a key paradigm for improving the reasoning performance of Large Language Models (LLMs). While reward model selection is commonly employed in this approach, it often fails to identify minority-yet-correct answers, which limits its effectiveness beyond that of simple majority voting. We argue that this limitation stems from a lack of informative critique signals during verifier training. To bridge this gap, we introduce Mirror-Critique, a framework that trains a verifier with informative critiques. Our key insight is to leverage the rich critique signal by contrasting model-generated solutions with ground-truth solutions. We deploy a small instruction-tuned model to synthesize high-quality critique data with rejection sampling that teaches the verifier not only what is wrong, but also why. The synthetic data is used to cold-start the LLMs in the RLVR process to further improve the verification ability. The resulting Mirror-Verifier is deployed to evaluate candidate solutions by generating multiple critiques per solution, aggregating them into a verify score used for weighted voting or selective abstention. The experimental results show that our Mirror-Verifier significantly outperforms majority voting in terms of solution accuracy and also improves the solver's honesty to recognize and abstain from answering beyond its capability boundaries.

