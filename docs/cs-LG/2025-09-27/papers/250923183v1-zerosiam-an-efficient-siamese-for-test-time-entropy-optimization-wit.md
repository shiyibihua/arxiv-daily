---
layout: default
title: ZeroSiam: An Efficient Siamese for Test-Time Entropy Optimization without Collapse
---

# ZeroSiam: An Efficient Siamese for Test-Time Entropy Optimization without Collapse

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.23183" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.23183v1</a>
  <a href="https://arxiv.org/pdf/2509.23183.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.23183v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.23183v1', 'ZeroSiam: An Efficient Siamese for Test-Time Entropy Optimization without Collapse')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Guohao Chen, Shuaicheng Niu, Deyu Chen, Jiahao Yang, Zitian Zhang, Mingkui Tan, Pengcheng Wu, Zhiqi Shen

**åˆ†ç±»**: cs.LG, cs.NI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-27

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºZeroSiamï¼Œé€šè¿‡Siameseæ¶æ„å’Œç†µä¼˜åŒ–è§£å†³æµ‹è¯•æ—¶æ¨¡å‹åå¡Œé—®é¢˜ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æµ‹è¯•æ—¶è‡ªé€‚åº”` `ç†µæœ€å°åŒ–` `Siameseç½‘ç»œ` `æ¨¡å‹åå¡Œ` `éå¯¹ç§°å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æµ‹è¯•æ—¶ç†µæœ€å°åŒ–èƒ½ä½¿æ¨¡å‹é€‚åº”æ–°ç¯å¢ƒï¼Œæå‡æ¨ç†èƒ½åŠ›ï¼Œä½†æ˜“å¯¼è‡´æ¨¡å‹åå¡Œã€‚
2. ZeroSiamé€šè¿‡éå¯¹ç§°Siameseæ¶æ„å’Œæ•£åº¦å¯¹é½ï¼Œæœ‰æ•ˆé˜²æ­¢æ¨¡å‹åå¡Œï¼Œå¹¶æ­£åˆ™åŒ–å­¦ä¹ ä¿¡å·ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒZeroSiamåœ¨è§†è§‰å’Œè¯­è¨€ä»»åŠ¡ä¸Šï¼Œå¯¹å¤šç§æ¨¡å‹å‡è¡¨ç°å‡ºç¨³å®šä¸”ä¼˜è¶Šçš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºZeroSiamï¼Œä¸€ç§é«˜æ•ˆçš„éå¯¹ç§°Siameseæ¶æ„ï¼Œä¸“ä¸ºæµ‹è¯•æ—¶ç†µæœ€å°åŒ–è®¾è®¡ã€‚çº¯ç²¹çš„ç†µæœ€å°åŒ–å¯èƒ½å€¾å‘äºéæ³›åŒ–çš„æ·å¾„ï¼Œä¾‹å¦‚è†¨èƒ€logitèŒƒæ•°å¹¶å°†æ‰€æœ‰é¢„æµ‹é©±åŠ¨åˆ°ä¸»å¯¼ç±»åˆ«ä»¥å‡å°‘ç†µï¼Œä»è€Œå¯¼è‡´æ¨¡å‹åå¡Œã€‚ZeroSiamé€šè¿‡éå¯¹ç§°æ•£åº¦å¯¹é½æ¥é˜²æ­¢åå¡Œï¼Œè¿™é€šè¿‡å¯å­¦ä¹ çš„é¢„æµ‹å™¨å’Œåˆ†ç±»å™¨å‰çš„åœæ­¢æ¢¯åº¦ç®—å­æœ‰æ•ˆå®ç°ã€‚ç»éªŒå’Œç†è®ºè¯æ®è¡¨æ˜ï¼ŒZeroSiamä¸ä»…å¯ä»¥é˜²æ­¢åå¡Œï¼Œè¿˜å¯ä»¥å¸æ”¶å’Œæ­£åˆ™åŒ–æœ‰åçš„å­¦ä¹ ä¿¡å·ï¼Œä»è€Œæé«˜æ€§èƒ½ï¼Œå³ä½¿æ²¡æœ‰å‘ç”Ÿåå¡Œã€‚å¤§é‡ç»“æœè¡¨æ˜ï¼ŒZeroSiamåœ¨ä½¿ç”¨å¯å¿½ç•¥çš„å¼€é”€çš„æƒ…å†µä¸‹ï¼Œæ¯”å…ˆå‰çš„æ–¹æ³•è¡¨ç°æ›´ç¨³å®šï¼Œè¯æ˜äº†å…¶åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æµ‹è¯•åœºæ™¯å’Œå„ç§æ¨¡å‹ï¼ˆåŒ…æ‹¬ç‰¹åˆ«å®¹æ˜“å‘ç”Ÿåå¡Œçš„å°å‹æ¨¡å‹ï¼‰ä¸Šçš„è§†è§‰é€‚åº”å’Œå¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæµ‹è¯•æ—¶è‡ªé€‚åº”æ—¨åœ¨åˆ©ç”¨æ¨¡å‹è‡ªèº«çš„é¢„æµ‹åœ¨æ¨ç†é˜¶æ®µæŒç»­æ”¹è¿›ï¼Œä½†ç›´æ¥æœ€å°åŒ–é¢„æµ‹ç†µå®¹æ˜“å¯¼è‡´æ¨¡å‹åå¡Œï¼Œå³è¾“å‡ºæ’å®šçš„one-hotå‘é‡ï¼Œä»è€Œè·å¾—æä½çš„ç†µå€¼ï¼Œä½†ä¸§å¤±äº†æ³›åŒ–èƒ½åŠ›ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆé˜²æ­¢è¿™ç§åå¡Œç°è±¡ï¼Œå°¤å…¶æ˜¯åœ¨å°å‹æ¨¡å‹ä¸Šæ›´ä¸ºæ˜æ˜¾ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šZeroSiamçš„æ ¸å¿ƒæ€è·¯æ˜¯å¼•å…¥éå¯¹ç§°çš„Siameseç½‘ç»œç»“æ„ï¼Œé€šè¿‡ä¸€ä¸ªå¯å­¦ä¹ çš„é¢„æµ‹å™¨å’Œä¸€ä¸ªåœæ­¢æ¢¯åº¦æ“ä½œï¼Œå®ç°éå¯¹ç§°çš„æ•£åº¦å¯¹é½ã€‚è¿™ç§è®¾è®¡èƒ½å¤Ÿæœ‰æ•ˆåœ°é˜²æ­¢æ¨¡å‹åå¡Œï¼ŒåŒæ—¶è¿˜èƒ½å¸æ”¶å’Œæ­£åˆ™åŒ–æœ‰åçš„å­¦ä¹ ä¿¡å·ï¼Œä»è€Œæå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šZeroSiamåŒ…å«ä¸¤ä¸ªåˆ†æ”¯ï¼Œä¸€ä¸ªåˆ†æ”¯æ˜¯åŸå§‹æ¨¡å‹ï¼Œå¦ä¸€ä¸ªåˆ†æ”¯æ˜¯å¸¦æœ‰å¯å­¦ä¹ é¢„æµ‹å™¨çš„æ¨¡å‹ã€‚åŸå§‹æ¨¡å‹çš„è¾“å‡ºç»è¿‡åˆ†ç±»å™¨å¾—åˆ°é¢„æµ‹ç»“æœï¼Œè€Œå¦ä¸€ä¸ªåˆ†æ”¯çš„è¾“å‡ºç»è¿‡é¢„æµ‹å™¨åï¼Œä¸åŸå§‹æ¨¡å‹çš„è¾“å‡ºè¿›è¡Œæ•£åº¦å¯¹é½ã€‚åœ¨é¢„æµ‹å™¨ä¹‹å‰ï¼Œä½¿ç”¨åœæ­¢æ¢¯åº¦æ“ä½œï¼Œé˜»æ­¢æ¢¯åº¦ä»åŸå§‹æ¨¡å‹æµå‘é¢„æµ‹å™¨ï¼Œä»è€Œå®ç°éå¯¹ç§°æ€§ã€‚æ•´ä½“æµç¨‹æ˜¯åœ¨æµ‹è¯•æ—¶ï¼Œåˆ©ç”¨é¢„æµ‹ç»“æœçš„ç†µå’Œæ•£åº¦å¯¹é½æŸå¤±æ¥æ›´æ–°æ¨¡å‹å‚æ•°ã€‚

**å…³é”®åˆ›æ–°**ï¼šZeroSiamçš„å…³é”®åˆ›æ–°åœ¨äºå…¶éå¯¹ç§°çš„Siameseæ¶æ„å’Œéå¯¹ç§°æ•£åº¦å¯¹é½æ–¹å¼ã€‚ä¼ ç»Ÿçš„Siameseç½‘ç»œé€šå¸¸æ˜¯å¯¹ç§°çš„ï¼Œè€ŒZeroSiamé€šè¿‡å¼•å…¥å¯å­¦ä¹ çš„é¢„æµ‹å™¨å’Œåœæ­¢æ¢¯åº¦æ“ä½œï¼Œæ‰“ç ´äº†è¿™ç§å¯¹ç§°æ€§ï¼Œä»è€Œæ›´å¥½åœ°é˜²æ­¢äº†æ¨¡å‹åå¡Œã€‚æ­¤å¤–ï¼Œéå¯¹ç§°çš„æ•£åº¦å¯¹é½èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å¸æ”¶å’Œæ­£åˆ™åŒ–æœ‰åçš„å­¦ä¹ ä¿¡å·ã€‚

**å…³é”®è®¾è®¡**ï¼šZeroSiamçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å¯å­¦ä¹ çš„é¢„æµ‹å™¨ï¼Œç”¨äºå°†ä¸€ä¸ªåˆ†æ”¯çš„è¾“å‡ºæ˜ å°„åˆ°å¦ä¸€ä¸ªåˆ†æ”¯çš„è¾“å‡ºç©ºé—´ï¼›2) åœæ­¢æ¢¯åº¦æ“ä½œï¼Œé˜»æ­¢æ¢¯åº¦ä»åŸå§‹æ¨¡å‹æµå‘é¢„æµ‹å™¨ï¼Œå®ç°éå¯¹ç§°æ€§ï¼›3) æ•£åº¦å¯¹é½æŸå¤±ï¼Œç”¨äºè¡¡é‡ä¸¤ä¸ªåˆ†æ”¯è¾“å‡ºä¹‹é—´çš„å·®å¼‚ï¼Œå¹¶ä¿ƒä½¿æ¨¡å‹å­¦ä¹ åˆ°æ›´åŠ é²æ£’çš„ç‰¹å¾è¡¨ç¤ºã€‚æŸå¤±å‡½æ•°é€šå¸¸åŒ…å«ç†µæŸå¤±å’Œæ•£åº¦å¯¹é½æŸå¤±ä¸¤éƒ¨åˆ†ï¼Œé€šè¿‡è°ƒæ•´ä¸¤è€…çš„æƒé‡æ¥å¹³è¡¡ç†µæœ€å°åŒ–å’Œé˜²æ­¢åå¡Œã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒZeroSiamåœ¨è§†è§‰é€‚åº”å’Œå¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†ä»»åŠ¡ä¸Šå‡è¡¨ç°å‡ºè‰²ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šï¼ŒZeroSiamç›¸æ¯”äºç°æœ‰æ–¹æ³•ï¼Œåœ¨é˜²æ­¢æ¨¡å‹åå¡Œçš„åŒæ—¶ï¼Œå–å¾—äº†æ›´é«˜çš„å‡†ç¡®ç‡ã€‚å°¤å…¶æ˜¯åœ¨å°å‹æ¨¡å‹ä¸Šï¼ŒZeroSiamçš„ä¼˜åŠ¿æ›´åŠ æ˜æ˜¾ï¼Œè¯æ˜äº†å…¶åœ¨èµ„æºå—é™ç¯å¢ƒä¸‹çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

ZeroSiamå¯åº”ç”¨äºå„ç§éœ€è¦æµ‹è¯•æ—¶è‡ªé€‚åº”çš„åœºæ™¯ï¼Œä¾‹å¦‚å›¾åƒè¯†åˆ«ã€ç›®æ ‡æ£€æµ‹ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰ã€‚å°¤å…¶é€‚ç”¨äºèµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡æˆ–å°å‹æ¨¡å‹ï¼Œèƒ½å¤Ÿæå‡æ¨¡å‹åœ¨æœªçŸ¥ç¯å¢ƒä¸‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ–¹æ³•åœ¨åŒ»ç–—è¯Šæ–­ã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸå…·æœ‰æ½œåœ¨çš„åº”ç”¨ä»·å€¼ï¼Œå¯ä»¥æé«˜æ¨¡å‹åœ¨å¤æ‚å’ŒåŠ¨æ€ç¯å¢ƒä¸­çš„å¯é æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Test-time entropy minimization helps adapt a model to novel environments and incentivize its reasoning capability, unleashing the model's potential during inference by allowing it to evolve and improve in real-time using its own predictions, achieving promising performance. However, pure entropy minimization can favor non-generalizable shortcuts, such as inflating the logit norm and driving all predictions to a dominant class to reduce entropy, risking collapsed solutions (e.g., constant one-hot outputs) that trivially minimize the objective without meaningful learning. In this paper, we introduce ZeroSiam, an efficient asymmetric Siamese architecture tailored for test-time entropy minimization. ZeroSiam prevents collapse through asymmetric divergence alignment, which is efficiently achieved by a learnable predictor and a stop-gradient operator before the classifier. We provide empirical and theoretical evidence that ZeroSiam not only prevents collapse solutions, but also absorbs and regularizes biased learning signals, enhancing performance even when no collapse occurs. Despite its simplicity, extensive results show that ZeroSiam performs more stably over prior methods using negligible overhead, demonstrating efficacy on both vision adaptation and large language model reasoning tasks across challenging test scenarios and diverse models, including tiny models that are particularly collapse-prone.

