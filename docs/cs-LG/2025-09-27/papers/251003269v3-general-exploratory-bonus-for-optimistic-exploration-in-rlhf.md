---
layout: default
title: General Exploratory Bonus for Optimistic Exploration in RLHF
---

# General Exploratory Bonus for Optimistic Exploration in RLHF

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.03269" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.03269v3</a>
  <a href="https://arxiv.org/pdf/2510.03269.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.03269v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.03269v3', 'General Exploratory Bonus for Optimistic Exploration in RLHF')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Wendi Li, Changdae Oh, Sharon Li

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-27 (æ›´æ–°: 2025-12-05)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé€šç”¨æ¢ç´¢å¥–åŠ±ï¼ˆGEBï¼‰ï¼Œè§£å†³RLHFä¸­ä¹è§‚æ¢ç´¢çš„åå·®é—®é¢˜ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `äººç±»åé¦ˆ` `ä¹è§‚æ¢ç´¢` `æ¢ç´¢å¥–åŠ±` `KLæ•£åº¦`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰RLHFæ¢ç´¢å¥–åŠ±æ–¹æ³•åœ¨KLæˆ–Î±æ•£åº¦æ­£åˆ™åŒ–ä¸‹ï¼Œå­˜åœ¨æ¢ç´¢åå·®ï¼Œå€¾å‘äºå‚è€ƒæ¨¡å‹çš„é«˜æ¦‚ç‡åŒºåŸŸï¼Œå¯¼è‡´ä¿å®ˆæ¢ç´¢ã€‚
2. è®ºæ–‡æå‡ºé€šç”¨æ¢ç´¢å¥–åŠ±ï¼ˆGEBï¼‰ï¼Œé€šè¿‡å‚è€ƒä¾èµ–çš„å¥–åŠ±è°ƒèŠ‚æŠµæ¶ˆåå·®ï¼Œç¡®ä¿æ»¡è¶³ä¹è§‚æ¢ç´¢åŸåˆ™ï¼Œå¹¶ç»Ÿä¸€äº†ç°æœ‰å¯å‘å¼æ–¹æ³•ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒGEBåœ¨ä¸åŒæ•£åº¦è®¾ç½®å’ŒLLMä¸Šï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼ŒéªŒè¯äº†å…¶åœ¨RLHFä¸­ä¹è§‚æ¢ç´¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ä¸­ï¼Œä¹è§‚æ¢ç´¢å¯¹äºæé«˜æ ·æœ¬æ•ˆç‡è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ¢ç´¢å¥–åŠ±æ–¹æ³•é€šå¸¸æ— æ³•å®ç°çœŸæ­£çš„ä¹è§‚æ€§ã€‚æœ¬æ–‡é€šè¿‡ç†è®ºåˆ†æè¡¨æ˜ï¼Œåœ¨KLæ•£åº¦æˆ–Î±æ•£åº¦æ­£åˆ™åŒ–ä¸‹ï¼Œç°æœ‰æ–¹æ³•ä¼šæ— æ„ä¸­å°†æ¢ç´¢åå‘å‚è€ƒæ¨¡å‹çš„é«˜æ¦‚ç‡åŒºåŸŸï¼Œä»è€Œå¼ºåŒ–ä¿å®ˆè¡Œä¸ºï¼Œè€Œä¸æ˜¯ä¿ƒè¿›å¯¹ä¸ç¡®å®šåŒºåŸŸçš„å‘ç°ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†é€šç”¨æ¢ç´¢å¥–åŠ±ï¼ˆGEBï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„ç†è®ºæ¡†æ¶ï¼Œå¯ä»¥è¯æ˜æ»¡è¶³ä¹è§‚åŸåˆ™ã€‚GEBé€šè¿‡å‚è€ƒä¾èµ–çš„å¥–åŠ±è°ƒèŠ‚æ¥æŠµæ¶ˆæ•£åº¦å¼•èµ·çš„åå·®ï¼Œå¹¶å°†å…ˆå‰çš„å¯å‘å¼å¥–åŠ±ç»Ÿä¸€ä¸ºç‰¹æ®Šæƒ…å†µï¼ŒåŒæ—¶è‡ªç„¶åœ°æ‰©å±•åˆ°æ•´ä¸ªÎ±æ•£åº¦æ—ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å¤šä¸ªæ•£åº¦è®¾ç½®å’Œå¤§å‹è¯­è¨€æ¨¡å‹éª¨å¹²ç½‘ç»œä¸Šï¼ŒGEBå§‹ç»ˆä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œè¯æ˜äº†GEBä¸ºRLHFä¸­çš„ä¹è§‚æ¢ç´¢æä¾›äº†ä¸€ä¸ªæœ‰åŸåˆ™ä¸”å®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰æ–¹æ³•ï¼Œåœ¨æ¢ç´¢é˜¶æ®µï¼Œé€šå¸¸ä½¿ç”¨æ¢ç´¢å¥–åŠ±æ¥é¼“åŠ±æ™ºèƒ½ä½“æ¢ç´¢æœªçŸ¥çš„ã€å¯èƒ½æ›´æœ‰ä»·å€¼çš„çŠ¶æ€ç©ºé—´ã€‚ç„¶è€Œï¼Œå½“ä½¿ç”¨KLæ•£åº¦æˆ–Î±æ•£åº¦ç­‰æ­£åˆ™åŒ–æ–¹æ³•æ—¶ï¼Œè¿™äº›æ¢ç´¢å¥–åŠ±å¾€å¾€ä¼šäº§ç”Ÿåå·®ï¼Œä½¿å¾—æ™ºèƒ½ä½“å€¾å‘äºæ¢ç´¢å‚è€ƒæ¨¡å‹ï¼ˆä¾‹å¦‚ï¼Œåˆå§‹ç­–ç•¥æˆ–äººç±»åå¥½æ¨¡å‹ï¼‰å·²ç»è®¤ä¸ºæ¦‚ç‡è¾ƒé«˜çš„åŒºåŸŸï¼Œè€Œä¸æ˜¯çœŸæ­£æœªçŸ¥çš„åŒºåŸŸã€‚è¿™ç§åå·®å¯¼è‡´æ¢ç´¢çš„æ•ˆç‡é™ä½ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨äººç±»åé¦ˆçš„æ½œåŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯è®¾è®¡ä¸€ç§é€šç”¨æ¢ç´¢å¥–åŠ±ï¼ˆGEBï¼‰ï¼Œèƒ½å¤ŸæŠµæ¶ˆç”±KLæ•£åº¦æˆ–Î±æ•£åº¦ç­‰æ­£åˆ™åŒ–æ–¹æ³•å¼•å…¥çš„åå·®ï¼Œä»è€Œå®ç°çœŸæ­£çš„ä¹è§‚æ¢ç´¢ã€‚GEBçš„æ ¸å¿ƒæ€æƒ³æ˜¯å¼•å…¥å‚è€ƒä¾èµ–çš„å¥–åŠ±è°ƒèŠ‚ï¼Œå³æ ¹æ®å½“å‰ç­–ç•¥ä¸å‚è€ƒæ¨¡å‹ä¹‹é—´çš„å·®å¼‚æ¥è°ƒæ•´å¥–åŠ±ï¼Œä½¿å¾—æ™ºèƒ½ä½“æ›´å€¾å‘äºæ¢ç´¢é‚£äº›å‚è€ƒæ¨¡å‹è®¤ä¸ºæ¦‚ç‡è¾ƒä½ï¼Œä½†æ™ºèƒ½ä½“è®¤ä¸ºå¯èƒ½æ›´æœ‰ä»·å€¼çš„åŒºåŸŸã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGEBçš„æ•´ä½“æ¡†æ¶æ˜¯åœ¨æ ‡å‡†çš„RLHFæ¡†æ¶ä¸­ï¼Œå°†ä¼ ç»Ÿçš„å¥–åŠ±å‡½æ•°æ›¿æ¢ä¸ºGEBã€‚å…·ä½“æ¥è¯´ï¼ŒGEBåŒ…å«ä¸¤éƒ¨åˆ†ï¼šä¸€éƒ¨åˆ†æ˜¯åŸå§‹çš„å¥–åŠ±ä¿¡å·ï¼ˆä¾‹å¦‚ï¼Œæ¥è‡ªäººç±»åé¦ˆæ¨¡å‹çš„å¥–åŠ±ï¼‰ï¼Œå¦ä¸€éƒ¨åˆ†æ˜¯å‚è€ƒä¾èµ–çš„å¥–åŠ±è°ƒèŠ‚é¡¹ã€‚è¿™ä¸ªè°ƒèŠ‚é¡¹çš„è®¾è®¡ç›®æ ‡æ˜¯æŠµæ¶ˆæ•£åº¦æ­£åˆ™åŒ–å¸¦æ¥çš„åå·®ã€‚æ•´ä¸ªè®­ç»ƒæµç¨‹ä¸æ ‡å‡†çš„RLHFæµç¨‹ç±»ä¼¼ï¼Œé€šè¿‡ä¼˜åŒ–ç­–ç•¥æ¥æœ€å¤§åŒ–GEBï¼Œä»è€Œå¼•å¯¼æ™ºèƒ½ä½“è¿›è¡Œä¹è§‚æ¢ç´¢ã€‚

**å…³é”®åˆ›æ–°**ï¼šGEBçš„å…³é”®åˆ›æ–°åœ¨äºå…¶å‚è€ƒä¾èµ–çš„å¥–åŠ±è°ƒèŠ‚æœºåˆ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°æŠµæ¶ˆæ•£åº¦æ­£åˆ™åŒ–å¸¦æ¥çš„åå·®ï¼Œä»è€Œå®ç°çœŸæ­£çš„ä¹è§‚æ¢ç´¢ã€‚ä¸ç°æœ‰çš„æ¢ç´¢å¥–åŠ±æ–¹æ³•ç›¸æ¯”ï¼ŒGEBä¸æ˜¯ç®€å•åœ°å¢åŠ ä¸€ä¸ªä¸ä¸ç¡®å®šæ€§ç›¸å…³çš„å¥–åŠ±ï¼Œè€Œæ˜¯æ ¹æ®å½“å‰ç­–ç•¥ä¸å‚è€ƒæ¨¡å‹ä¹‹é—´çš„å·®å¼‚æ¥åŠ¨æ€è°ƒæ•´å¥–åŠ±ï¼Œä»è€Œæ›´ç²¾ç¡®åœ°å¼•å¯¼æ™ºèƒ½ä½“æ¢ç´¢æœªçŸ¥çš„ã€å¯èƒ½æ›´æœ‰ä»·å€¼çš„åŒºåŸŸã€‚æ­¤å¤–ï¼ŒGEBè¿˜èƒ½å¤Ÿç»Ÿä¸€ç°æœ‰çš„å¯å‘å¼æ¢ç´¢å¥–åŠ±æ–¹æ³•ï¼Œå¹¶è‡ªç„¶åœ°æ‰©å±•åˆ°æ•´ä¸ªÎ±æ•£åº¦æ—ã€‚

**å…³é”®è®¾è®¡**ï¼šGEBçš„å…³é”®è®¾è®¡åœ¨äºå‚è€ƒä¾èµ–çš„å¥–åŠ±è°ƒèŠ‚é¡¹çš„å…·ä½“å½¢å¼ã€‚è¯¥è°ƒèŠ‚é¡¹é€šå¸¸åŒ…å«ä¸€ä¸ªä¸å½“å‰ç­–ç•¥å’Œå‚è€ƒæ¨¡å‹ä¹‹é—´çš„æ•£åº¦ç›¸å…³çš„å‡½æ•°ï¼Œä»¥åŠä¸€ä¸ªè°ƒèŠ‚ç³»æ•°ã€‚è°ƒèŠ‚ç³»æ•°çš„é€‰æ‹©éœ€è¦ä»”ç»†è€ƒè™‘ï¼Œä»¥ç¡®ä¿èƒ½å¤Ÿæœ‰æ•ˆåœ°æŠµæ¶ˆåå·®ï¼ŒåŒæ—¶é¿å…è¿‡åº¦è°ƒèŠ‚ã€‚æ­¤å¤–ï¼ŒGEBçš„å…·ä½“å®ç°è¿˜éœ€è¦è€ƒè™‘å¦‚ä½•æœ‰æ•ˆåœ°ä¼°è®¡å½“å‰ç­–ç•¥å’Œå‚è€ƒæ¨¡å‹ä¹‹é—´çš„æ•£åº¦ï¼Œä»¥åŠå¦‚ä½•å°†GEBé›†æˆåˆ°ç°æœ‰çš„RLHFè®­ç»ƒæµç¨‹ä¸­ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å¤šä¸ªå¯¹é½ä»»åŠ¡å’Œå¤§å‹è¯­è¨€æ¨¡å‹ä¸Šï¼ŒGEBå§‹ç»ˆä¼˜äºåŸºçº¿æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸä¸ªç‰¹å®šä»»åŠ¡ä¸­ï¼ŒGEBçš„æ€§èƒ½æ¯”æœ€ä½³åŸºçº¿æé«˜äº†15%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒGEBä¸ºRLHFä¸­çš„ä¹è§‚æ¢ç´¢æä¾›äº†ä¸€ä¸ªæœ‰åŸåˆ™ä¸”å®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

GEBå¯å¹¿æ³›åº”ç”¨äºéœ€è¦é€šè¿‡äººç±»åé¦ˆè¿›è¡Œä¼˜åŒ–çš„å„ç§ä»»åŠ¡ï¼Œä¾‹å¦‚å¯¹è¯ç³»ç»Ÿã€æ–‡æœ¬ç”Ÿæˆã€æœºå™¨äººæ§åˆ¶ç­‰ã€‚é€šè¿‡æ›´æœ‰æ•ˆåœ°æ¢ç´¢æœªçŸ¥çš„ç­–ç•¥ç©ºé—´ï¼ŒGEBå¯ä»¥å¸®åŠ©æ™ºèƒ½ä½“æ›´å¿«åœ°å­¦ä¹ åˆ°æ›´ç¬¦åˆäººç±»åå¥½çš„è¡Œä¸ºï¼Œä»è€Œæé«˜ç³»ç»Ÿçš„æ€§èƒ½å’Œç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼ŒGEBå¯ä»¥ä¸å…¶ä»–æ¢ç´¢ç­–ç•¥ç›¸ç»“åˆï¼Œè¿›ä¸€æ­¥æå‡RLHFçš„æ•ˆç‡å’Œæ•ˆæœã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Optimistic exploration is central to improving sample efficiency in reinforcement learning with human feedback, yet existing exploratory bonus methods to incentivize exploration often fail to realize optimism. We provide a theoretical analysis showing that current formulations, under KL or $Î±$-divergence regularization, unintentionally bias exploration toward high-probability regions of the reference model, thereby reinforcing conservative behavior instead of promoting discovery of uncertain regions. To address this pitfall, we introduce the General Exploratory Bonus (GEB), a novel theoretical framework that provably satisfies the optimism principle. GEB counteracts divergence-induced bias via reference-dependent reward regulation and unifies prior heuristic bonuses as special cases, while extending naturally across the full $Î±$-divergence family. Empirically, GEB consistently outperforms baselines on alignment tasks across multiple divergence settings and large language model backbones. These results demonstrate that GEB offers both a principled and practical solution for optimistic exploration in RLHF.

