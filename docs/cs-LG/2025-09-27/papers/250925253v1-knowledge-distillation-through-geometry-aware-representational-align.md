---
layout: default
title: Knowledge distillation through geometry-aware representational alignment
---

# Knowledge distillation through geometry-aware representational alignment

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25253" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25253v1</a>
  <a href="https://arxiv.org/pdf/2509.25253.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25253v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25253v1', 'Knowledge distillation through geometry-aware representational alignment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Prajjwal Bhattarai, Mohammad Amjad, Dmytro Zhylko, Tuka Alhanai

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-27

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå‡ ä½•æ„ŸçŸ¥çš„è¡¨å¾å¯¹é½çŸ¥è¯†è’¸é¦æ–¹æ³•ï¼Œæå‡è¯­è¨€æ¨¡å‹æ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `çŸ¥è¯†è’¸é¦` `ç‰¹å¾å¯¹é½` `å‡ ä½•æ„ŸçŸ¥` `Procrustesè·ç¦»` `FrobeniusèŒƒæ•°` `è¯­è¨€æ¨¡å‹` `æ¨¡å‹å‹ç¼©`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç‰¹å¾è’¸é¦æ–¹æ³•éš¾ä»¥æœ‰æ•ˆæ•æ‰æ•™å¸ˆæ¨¡å‹çš„ç‰¹å¾ç©ºé—´ç»“æ„ï¼Œé™åˆ¶äº†è’¸é¦æ•ˆæœã€‚
2. è®ºæ–‡æå‡ºåˆ©ç”¨Procrustesè·ç¦»å’Œç‰¹å¾GramçŸ©é˜µçš„FrobeniusèŒƒæ•°ä½œä¸ºè’¸é¦æŸå¤±ï¼Œå¯¹é½ç‰¹å¾å‡ ä½•ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨BERTå’ŒOPTç­‰è¯­è¨€æ¨¡å‹ä¸Šï¼Œåˆ†ç±»å’ŒæŒ‡ä»¤è·Ÿéšä»»åŠ¡çš„æ€§èƒ½å‡æœ‰æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

çŸ¥è¯†è’¸é¦æ˜¯ä¸€ç§å¸¸è§çš„å°†èƒ½åŠ›ä»å¤§å‹æ¨¡å‹è¿ç§»åˆ°å°å‹æ¨¡å‹çš„èŒƒå¼ã€‚ä¼ ç»Ÿçš„è’¸é¦æ–¹æ³•åˆ©ç”¨æ•™å¸ˆå’Œå­¦ç”Ÿæ¨¡å‹è¾“å‡ºçš„æ¦‚ç‡åˆ†å¸ƒå·®å¼‚ï¼Œè€ŒåŸºäºç‰¹å¾çš„è’¸é¦æ–¹æ³•é€šå¸¸æœ€å°åŒ–éšè—å±‚è¡¨å¾ä¹‹é—´æ¬§å‡ é‡Œå¾—èŒƒæ•°çš„å˜ä½“ã€‚å…¶ä¸»è¦ç›®æ ‡æ˜¯è®©å­¦ç”Ÿæ¨¡å‹æ¨¡ä»¿æ•™å¸ˆæ¨¡å‹çš„ç‰¹å¾ç©ºé—´ç»“æ„ã€‚æœ¬æ–‡ä»ç†è®ºä¸Šè¯æ˜ï¼Œç°æœ‰çš„ç‰¹å¾è’¸é¦æ–¹æ³•ï¼Œå¦‚åŸºäºæŠ•å½±çš„å‡æ–¹è¯¯å·®æŸå¤±æˆ–ä¸­å¿ƒæ ¸å¯¹é½ï¼ˆCKAï¼‰ï¼Œå³ä½¿åœ¨é›¶æŸå¤±ä¸‹ä¹Ÿæ— æ³•æ•è·ç‰¹å¾ç»“æ„ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºä½¿ç”¨Procrustesè·ç¦»å’Œç‰¹å¾GramçŸ©é˜µçš„FrobeniusèŒƒæ•°ï¼ˆè¿™äº›è·ç¦»åœ¨è¡¨å¾å¯¹é½çš„ä¸Šä¸‹æ–‡ä¸­å·²ç»å¾ˆå¸¸è§ï¼‰ä½œä¸ºè’¸é¦æŸå¤±ã€‚å®éªŒè¡¨æ˜ï¼Œé€šè¿‡æœ¬æ–‡æ–¹æ³•è¿›è¡Œçš„ç‰¹å¾è’¸é¦åœ¨åˆ†ç±»å’ŒæŒ‡ä»¤è·Ÿéšä»»åŠ¡ä¸­ï¼Œè·¨è¯­è¨€æ¨¡å‹ç³»åˆ—ï¼ˆBERTå’ŒOPTï¼‰çš„è’¸é¦æ€§èƒ½ä¸Šå®ç°äº†é«˜è¾¾2ä¸ªç™¾åˆ†ç‚¹çš„ç»Ÿè®¡æ˜¾è‘—æå‡ï¼Œå±•ç¤ºäº†å°†ç‰¹å¾å‡ ä½•é›†æˆåˆ°ç°æœ‰è’¸é¦æ–¹æ³•ä¸­çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„åŸºäºç‰¹å¾çš„çŸ¥è¯†è’¸é¦æ–¹æ³•ï¼Œä¾‹å¦‚åŸºäºæŠ•å½±çš„å‡æ–¹è¯¯å·®æŸå¤±æˆ–ä¸­å¿ƒæ ¸å¯¹é½ï¼ˆCKAï¼‰ï¼Œåœ¨å¯¹é½æ•™å¸ˆå’Œå­¦ç”Ÿæ¨¡å‹çš„ç‰¹å¾è¡¨ç¤ºæ—¶ï¼Œæ— æ³•æœ‰æ•ˆåœ°æ•æ‰æ•™å¸ˆæ¨¡å‹çš„ç‰¹å¾ç©ºé—´å‡ ä½•ç»“æ„ã€‚å³ä½¿æŸå¤±å‡½æ•°è¾¾åˆ°é›¶ï¼Œå­¦ç”Ÿæ¨¡å‹ä¹Ÿå¯èƒ½æ— æ³•å­¦ä¹ åˆ°ä¸æ•™å¸ˆæ¨¡å‹ç›¸ä¼¼çš„ç‰¹å¾ç©ºé—´ç»“æ„ï¼Œä»è€Œé™åˆ¶äº†çŸ¥è¯†è¿ç§»çš„æ•ˆæœã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨Procrustesè·ç¦»å’Œç‰¹å¾GramçŸ©é˜µçš„FrobeniusèŒƒæ•°æ¥åº¦é‡å’Œå¯¹é½æ•™å¸ˆå’Œå­¦ç”Ÿæ¨¡å‹ä¹‹é—´çš„ç‰¹å¾è¡¨ç¤ºã€‚è¿™äº›åº¦é‡æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰ç‰¹å¾ç©ºé—´ä¸­çš„å‡ ä½•å…³ç³»ï¼Œä¾‹å¦‚ç‰¹å¾ä¹‹é—´çš„è·ç¦»å’Œè§’åº¦ï¼Œä»è€Œä½¿å­¦ç”Ÿæ¨¡å‹èƒ½å¤Ÿæ›´å‡†ç¡®åœ°æ¨¡ä»¿æ•™å¸ˆæ¨¡å‹çš„ç‰¹å¾ç©ºé—´ç»“æ„ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š1. ä½¿ç”¨æ•™å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹å¤„ç†ç›¸åŒçš„è¾“å…¥æ•°æ®ã€‚2. æå–æ•™å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹çš„ä¸­é—´å±‚ç‰¹å¾è¡¨ç¤ºã€‚3. ä½¿ç”¨Procrustesè·ç¦»æˆ–ç‰¹å¾GramçŸ©é˜µçš„FrobeniusèŒƒæ•°è®¡ç®—æ•™å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹ç‰¹å¾è¡¨ç¤ºä¹‹é—´çš„è·ç¦»ã€‚4. å°†è¯¥è·ç¦»ä½œä¸ºè’¸é¦æŸå¤±ï¼Œç”¨äºè®­ç»ƒå­¦ç”Ÿæ¨¡å‹ã€‚é€šè¿‡æœ€å°åŒ–è¯¥æŸå¤±ï¼Œå­¦ç”Ÿæ¨¡å‹å¯ä»¥å­¦ä¹ åˆ°ä¸æ•™å¸ˆæ¨¡å‹ç›¸ä¼¼çš„ç‰¹å¾ç©ºé—´ç»“æ„ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºä½¿ç”¨äº†Procrustesè·ç¦»å’Œç‰¹å¾GramçŸ©é˜µçš„FrobeniusèŒƒæ•°ä½œä¸ºè’¸é¦æŸå¤±ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºæ¬§å‡ é‡Œå¾—è·ç¦»çš„æŸå¤±å‡½æ•°ç›¸æ¯”ï¼Œè¿™äº›åº¦é‡æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰ç‰¹å¾ç©ºé—´ä¸­çš„å‡ ä½•å…³ç³»ï¼Œä»è€Œæé«˜äº†çŸ¥è¯†è’¸é¦çš„æ•ˆæœã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1. é€‰æ‹©åˆé€‚çš„ä¸­é—´å±‚ç‰¹å¾è¡¨ç¤ºè¿›è¡Œå¯¹é½ã€‚2. é€‰æ‹©åˆé€‚çš„Procrustesè·ç¦»æˆ–ç‰¹å¾GramçŸ©é˜µçš„FrobeniusèŒƒæ•°è®¡ç®—æ–¹æ³•ã€‚3. è°ƒæ•´è’¸é¦æŸå¤±åœ¨æ€»æŸå¤±ä¸­çš„æƒé‡ã€‚4. å®éªŒä¸­ä½¿ç”¨äº†BERTå’ŒOPTç­‰è¯­è¨€æ¨¡å‹ï¼Œå¹¶åœ¨åˆ†ç±»å’ŒæŒ‡ä»¤è·Ÿéšä»»åŠ¡ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨Procrustesè·ç¦»å’Œç‰¹å¾GramçŸ©é˜µçš„FrobeniusèŒƒæ•°ä½œä¸ºè’¸é¦æŸå¤±ï¼Œåœ¨BERTå’ŒOPTç­‰è¯­è¨€æ¨¡å‹ä¸Šï¼Œåˆ†ç±»å’ŒæŒ‡ä»¤è·Ÿéšä»»åŠ¡çš„æ€§èƒ½å‡æœ‰æ˜¾è‘—æå‡ï¼Œæœ€é«˜å¯è¾¾2ä¸ªç™¾åˆ†ç‚¹ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°æé«˜çŸ¥è¯†è’¸é¦çš„æ•ˆæœï¼Œå¹¶å…·æœ‰å¹¿æ³›çš„é€‚ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦æ¨¡å‹å‹ç¼©å’ŒåŠ é€Ÿçš„åœºæ™¯ï¼Œä¾‹å¦‚ç§»åŠ¨è®¾å¤‡ä¸Šçš„è‡ªç„¶è¯­è¨€å¤„ç†ã€è¾¹ç¼˜è®¡ç®—å’Œèµ„æºå—é™ç¯å¢ƒä¸‹çš„æ¨¡å‹éƒ¨ç½²ã€‚é€šè¿‡çŸ¥è¯†è’¸é¦ï¼Œå¯ä»¥å°†å¤§å‹ã€å¤æ‚çš„æ¨¡å‹å‹ç¼©æˆå°å‹ã€é«˜æ•ˆçš„æ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒè¾ƒé«˜çš„æ€§èƒ½æ°´å¹³ï¼Œä»è€Œé™ä½è®¡ç®—æˆæœ¬å’Œå»¶è¿Ÿï¼Œæé«˜ç”¨æˆ·ä½“éªŒã€‚è¯¥æ–¹æ³•åœ¨å·¥ä¸šç•Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Knowledge distillation is a common paradigm for transferring capabilities from larger models to smaller ones. While traditional distillation methods leverage a probabilistic divergence over the output of the teacher and student models, feature-based distillation methods often minimize variants of Euclidean norms between the hidden layer representations. The main goal is for the student to mimic the structure of the feature space of the teacher. In this work, we theoretically show that existing feature distillation methods, such as projection based mean squared loss or Centered Kernel Alignment (CKA), cannot capture the feature structure, even under zero loss. We then motivate the use of Procrustes distance and the Frobenius norm of Feature Gram Matrix, distances already common in the context of measuring representational alignment, as distillation losses. We show that feature distillation through our method showcases statistically significant improvement in distillation performance across language models families (BERT and OPT) in classification and instruction-following tasks by up to 2 percentage points, showcasing the potential of integrating feature geometry into existing distillation methods.

