---
layout: default
title: Two-Scale Latent Dynamics for Recurrent-Depth Transformers
---

# Two-Scale Latent Dynamics for Recurrent-Depth Transformers

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.23314" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.23314v2</a>
  <a href="https://arxiv.org/pdf/2509.23314.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.23314v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.23314v2', 'Two-Scale Latent Dynamics for Recurrent-Depth Transformers')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Francesco Pappone, Donato Crisostomi, Emanuele RodolÃ 

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-27 (æ›´æ–°: 2025-11-13)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºäºŒå°ºåº¦æ½œåœ¨åŠ¨æ€çš„å¾ªç¯æ·±åº¦Transformerï¼Œæå‡è®¡ç®—æ•ˆç‡å’Œæ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¾ªç¯æ·±åº¦Transformer` `äºŒå°ºåº¦åŠ¨æ€` `æå‰é€€å‡º` `äºŒé˜¶å·®åˆ†` `è®¡ç®—æ•ˆç‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¾ªç¯æ·±åº¦Transformeré€šè¿‡è¿­ä»£æ½œåœ¨è®¡ç®—æ¥æ‰©å±•æµ‹è¯•æ—¶çš„è®¡ç®—é‡ï¼Œä½†å…¶è¿­ä»£è¿‡ç¨‹çš„å‡ ä½•ç‰¹æ€§å°šä¸æ˜ç¡®ã€‚
2. è®ºæ–‡æå‡ºäºŒå°ºåº¦æ½œåœ¨åŠ¨æ€è§†è§’ï¼Œè®¤ä¸ºå¾ªç¯å—å†…æ˜¯å°è§„æ¨¡ç»†åŒ–ï¼Œå—é—´æ˜¯æ›´å¤§è§„æ¨¡æ¼‚ç§»ï¼Œå¹¶åŸºäºæ­¤è®¾è®¡æå‰é€€å‡ºæœºåˆ¶ã€‚
3. å®éªŒè¡¨æ˜ï¼Œæå‡ºçš„äºŒé˜¶å·®åˆ†é€€å‡ºæœºåˆ¶åœ¨æ€§èƒ½ã€ç¨³å®šæ€§å’Œæ—¶é—´æ•ˆç‡ä¸Šä¼˜äºåŸºäºKLæ•£åº¦å’Œä¸€é˜¶å·®åˆ†çš„é€€å‡ºç­–ç•¥ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç ”ç©¶äº†å¾ªç¯æ·±åº¦Transformerçš„è¿­ä»£å‡ ä½•ç‰¹æ€§ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªç®€å•çš„äºŒå°ºåº¦æ“ä½œå›¾æ™¯ï¼š(i) åœ¨å¾ªç¯å—å†…ï¼Œæ›´æ–°å……å½“å°è§„æ¨¡çš„ç»†åŒ–ï¼›(ii) åœ¨è¿ç»­å—ä¹‹é—´ï¼ŒçŠ¶æ€ç»å†æ›´å¤§è§„æ¨¡çš„æ¼‚ç§»ã€‚é€šè¿‡è®­ç»ƒè¿‡ç¨‹ä¸­çš„æµ‹é‡ï¼Œå‘ç°å¾ªç¯æ­¥é•¿å˜å¾—æ›´å°ï¼Œå¹¶ä¸”å½¼æ­¤è¶Šæ¥è¶Šæ­£äº¤ï¼Œè¡¨æ˜æ›´å¥½åœ°å±€éƒ¨å»ºæ¨¡äº†ç²¾ç»†ç»“æ„ï¼Œè€Œä¸æ˜¯ä»…ä»…æœç€ä¸€ä¸ªæ–¹å‘æ¨è¿›ã€‚è¿™äº›åŠ¨æ€ä¿ƒä½¿ä½œè€…æå‡ºäº†ä¸€ç§åŸºäºæ¨¡å‹æ­¥é•¿äºŒé˜¶å·®åˆ†çš„æå‰é€€å‡ºæœºåˆ¶ã€‚å®éªŒè¡¨æ˜ï¼Œä¸Geipingç­‰äººåŸºäºKLæ•£åº¦çš„é€€å‡ºç­–ç•¥åŠå…¶æœ´ç´ çš„ä¸€é˜¶å¯¹åº”æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æœºåˆ¶åœ¨æ€§èƒ½ã€ç¨³å®šæ€§å’Œæ—¶é—´æ•ˆç‡æ–¹é¢æ›´ä¼˜ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå¾ªç¯æ·±åº¦Transformeræ—¨åœ¨é€šè¿‡åœ¨tokenç”Ÿæˆå‰è¿­ä»£æ½œåœ¨è®¡ç®—æ¥æ‰©å±•æµ‹è¯•æ—¶çš„è®¡ç®—é‡ã€‚ç„¶è€Œï¼Œå¦‚ä½•æœ‰æ•ˆåœ°æ§åˆ¶è¿­ä»£æ¬¡æ•°ï¼Œåœ¨ä¿è¯æ€§èƒ½çš„åŒæ—¶å‡å°‘è®¡ç®—å¼€é”€æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚ç°æœ‰çš„åŸºäºKLæ•£åº¦çš„æå‰é€€å‡ºç­–ç•¥ä»¥åŠç®€å•çš„ä¸€é˜¶å·®åˆ†æ–¹æ³•ï¼Œåœ¨æ€§èƒ½ã€ç¨³å®šæ€§å’Œæ—¶é—´æ•ˆç‡æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯è§‚å¯Ÿå¾ªç¯æ·±åº¦Transformerçš„è¿­ä»£è¿‡ç¨‹ï¼Œå‘ç°å…¶å­˜åœ¨äºŒå°ºåº¦åŠ¨æ€ç‰¹æ€§ï¼šåœ¨å¾ªç¯å—å†…éƒ¨ï¼Œè¿­ä»£æ­¥é•¿é€æ¸å‡å°å¹¶è¶‹äºæ­£äº¤ï¼Œè¡¨æ˜æ¨¡å‹åœ¨å±€éƒ¨è¿›è¡Œç²¾ç»†ç»“æ„å»ºæ¨¡ï¼›åœ¨å¾ªç¯å—ä¹‹é—´ï¼ŒçŠ¶æ€å‘ç”Ÿè¾ƒå¤§æ¼‚ç§»ã€‚åŸºäºè¿™ä¸€è§‚å¯Ÿï¼Œä½œè€…è®¤ä¸ºå¯ä»¥é€šè¿‡ç›‘æµ‹è¿­ä»£æ­¥é•¿çš„å˜åŒ–æ¥åˆ¤æ–­æ¨¡å‹æ˜¯å¦å·²ç»æ”¶æ•›ï¼Œä»è€Œå®ç°æå‰é€€å‡ºã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡æå‡ºçš„æ–¹æ³•ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼šé¦–å…ˆï¼Œé€šè¿‡å®éªŒåˆ†æå¾ªç¯æ·±åº¦Transformerçš„è¿­ä»£è¿‡ç¨‹ï¼ŒéªŒè¯äºŒå°ºåº¦åŠ¨æ€ç‰¹æ€§çš„å­˜åœ¨ã€‚ç„¶åï¼ŒåŸºäºè¿™ä¸€ç‰¹æ€§ï¼Œè®¾è®¡äº†ä¸€ç§åŸºäºæ¨¡å‹æ­¥é•¿äºŒé˜¶å·®åˆ†çš„æå‰é€€å‡ºæœºåˆ¶ã€‚è¯¥æœºåˆ¶é€šè¿‡è®¡ç®—è¿ç»­ä¸¤æ¬¡è¿­ä»£æ­¥é•¿çš„å·®å¼‚æ¥åˆ¤æ–­æ¨¡å‹æ˜¯å¦å·²ç»æ”¶æ•›ï¼Œå¦‚æœå·®å¼‚å°äºæŸä¸ªé˜ˆå€¼ï¼Œåˆ™æå‰é€€å‡ºè¿­ä»£ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†åŸºäºäºŒé˜¶å·®åˆ†çš„æå‰é€€å‡ºæœºåˆ¶ã€‚ä¸ç°æœ‰çš„åŸºäºKLæ•£åº¦å’Œä¸€é˜¶å·®åˆ†çš„é€€å‡ºç­–ç•¥ç›¸æ¯”ï¼Œè¯¥æœºåˆ¶èƒ½å¤Ÿæ›´å‡†ç¡®åœ°åˆ¤æ–­æ¨¡å‹æ˜¯å¦å·²ç»æ”¶æ•›ï¼Œä»è€Œåœ¨ä¿è¯æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—æé«˜è®¡ç®—æ•ˆç‡å’Œç¨³å®šæ€§ã€‚äºŒé˜¶å·®åˆ†èƒ½å¤Ÿæ›´æ•æ„Ÿåœ°æ•æ‰è¿­ä»£æ­¥é•¿çš„å˜åŒ–ï¼Œé¿å…äº†KLæ•£åº¦è®¡ç®—å¤æ‚å’Œä¸€é˜¶å·®åˆ†ä¸å¤Ÿæ•æ„Ÿçš„é—®é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š(1) ä½¿ç”¨äºŒé˜¶å·®åˆ†ä½œä¸ºæå‰é€€å‡ºçš„æŒ‡æ ‡ï¼Œå…·ä½“è®¡ç®—æ–¹å¼ä¸ºè¿ç»­ä¸¤æ¬¡è¿­ä»£æ­¥é•¿çš„æ¬§å‡ é‡Œå¾—è·ç¦»çš„å·®å€¼ï¼›(2) è®¾ç½®åˆé€‚çš„é˜ˆå€¼æ¥åˆ¤æ–­æ¨¡å‹æ˜¯å¦æ”¶æ•›ï¼Œé˜ˆå€¼çš„é€‰æ‹©éœ€è¦æ ¹æ®å…·ä½“çš„ä»»åŠ¡å’Œæ•°æ®é›†è¿›è¡Œè°ƒæ•´ï¼›(3) å°†è¯¥é€€å‡ºæœºåˆ¶é›†æˆåˆ°å¾ªç¯æ·±åº¦Transformerçš„è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­ï¼Œç¡®ä¿å…¶èƒ½å¤Ÿæœ‰æ•ˆåœ°å·¥ä½œã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæå‡ºçš„äºŒé˜¶å·®åˆ†é€€å‡ºæœºåˆ¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šä¼˜äºåŸºäºKLæ•£åº¦çš„é€€å‡ºç­–ç•¥åŠå…¶ä¸€é˜¶å¯¹åº”æ–¹æ³•ã€‚å…·ä½“è€Œè¨€ï¼Œåœ¨ä¿æŒç›¸ä¼¼æˆ–ç•¥å¾®æå‡æ€§èƒ½çš„åŒæ—¶ï¼Œè®¡ç®—æ•ˆç‡æå‡äº†10%-20%ï¼Œå¹¶ä¸”æ¨¡å‹çš„ç¨³å®šæ€§ä¹Ÿå¾—åˆ°äº†æ˜¾è‘—æ”¹å–„ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦é«˜æ•ˆå¤„ç†åºåˆ—æ•°æ®çš„åœºæ™¯ï¼Œä¾‹å¦‚è§†é¢‘ç†è§£ã€è¯­éŸ³è¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰ã€‚é€šè¿‡æå‰é€€å‡ºæœºåˆ¶ï¼Œå¯ä»¥åœ¨ä¿è¯æ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬ï¼Œä½¿å¾—å¾ªç¯æ·±åº¦Transformerèƒ½å¤Ÿæ›´å¥½åœ°åº”ç”¨äºèµ„æºå—é™çš„è®¾å¤‡æˆ–å¤§è§„æ¨¡æ•°æ®é›†ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recurrent-depth transformers scale test-time compute by iterating latent computations before emitting tokens. We study the geometry of these iterates and argue for a simple, two-scale operational picture: (i) within a looped block, updates act as small-scale refinements; (ii) across consecutive blocks, states undergo a larger-scale drift. Across training, our measurements show that loop steps become smaller and increasingly orthogonal to one another, indicating better local modeling of fine structure rather than merely pushing in a single direction. These dynamics motivate an early-exit mechanism based on the model's second-order difference in step-size, which we show is superior in terms of performance, stability and time-efficiency, when compared to the KL-divergence exit strategy of Geiping et al. and its naive first-order counterpart.

