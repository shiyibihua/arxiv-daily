---
layout: default
title: Quant-dLLM: Post-Training Extreme Low-Bit Quantization for Diffusion Large Language Models
---

# Quant-dLLM: Post-Training Extreme Low-Bit Quantization for Diffusion Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.03274" class="toolbar-btn" target="_blank">üìÑ arXiv: 2510.03274v1</a>
  <a href="https://arxiv.org/pdf/2510.03274.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.03274v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.03274v1', 'Quant-dLLM: Post-Training Extreme Low-Bit Quantization for Diffusion Large Language Models')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Tianao Zhang, Zhiteng Li, Xianglong Yan, Haotong Qin, Yong Guo, Yulun Zhang

**ÂàÜÁ±ª**: cs.LG, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-27

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/ZTA2785/Quant-dLLM)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**Quant-dLLMÔºöÈù¢ÂêëÊâ©Êï£Â§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÂêéËÆ≠ÁªÉÊûÅ‰ΩéÊØîÁâπÈáèÂåñÊ°ÜÊû∂**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Êâ©Êï£Â§ßËØ≠Ë®ÄÊ®°Âûã` `ÂêéËÆ≠ÁªÉÈáèÂåñ` `ÊûÅ‰ΩéÊØîÁâπÈáèÂåñ` `Ê®°ÂûãÂéãÁº©` `Êé©Á†ÅÊ†°ÂáÜÊ®°Êãü`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂêéËÆ≠ÁªÉÈáèÂåñÊñπÊ≥ïÂú®Áõ¥Êé•Â∫îÁî®‰∫éÊâ©Êï£Â§ßËØ≠Ë®ÄÊ®°ÂûãÊó∂ÔºåÂú®ÊûÅ‰ΩéÊØîÁâπÔºàÂ¶Ç2ÊØîÁâπÔºâ‰∏ãÊÄßËÉΩÊòæËëó‰∏ãÈôç„ÄÇ
2. Quant-dLLMÈÄöËøáÊé©Á†ÅÊ†°ÂáÜÊ®°Êãü(MCS)ÂíåÊï∞ÊçÆÊÑüÁü•‰ªªÊÑèÈò∂ÈáèÂåñÂô®(DAQ)Á≠âÊäÄÊúØÔºå‰∏ìÈó®‰∏∫dLLMsËÆæËÆ°Ë∂Ö‰ΩéÊØîÁâπÈáèÂåñÊñπÊ°à„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåQuant-dLLMÂú®2ÊØîÁâπÁ≤æÂ∫¶‰∏ãÔºåÁõ∏ÊØî‰∫éÂ∞ÜARÊ®°ÂûãÈáèÂåñÊñπÊ≥ïËøÅÁßªÂà∞dLLM‰∏äÔºåËÉΩÂèñÂæóÊõ¥È´òÁöÑÂáÜÁ°ÆÁéá„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êâ©Êï£Â§ßËØ≠Ë®ÄÊ®°Âûã(dLLMs)Êèê‰æõÂèåÂêë‰∏ä‰∏ãÊñáÂíåÁÅµÊ¥ªÁöÑÊé©Á†ÅÂéªÂô™ÁîüÊàêÔºåÊ≠£Êàê‰∏∫Ëá™ÂõûÂΩí(AR) LLMsÁöÑ‰∏Ä‰∏™Âºï‰∫∫Ê≥®ÁõÆÁöÑÊõø‰ª£ÊñπÊ°à„ÄÇÁÑ∂ËÄåÔºå‰∏éAR LLMs‰∏ÄÊ†∑ÔºåÂÆÉ‰ª¨ÁöÑÊ®°ÂûãÂ§ßÂ∞èÊåÅÁª≠Â¢ûÈïøÔºåËøô‰øÉ‰Ωø‰∫∫‰ª¨ÂØπÈÉ®ÁΩ≤ËøõË°åÊùÉÈáçÂéãÁº©„ÄÇËôΩÁÑ∂ÂêéËÆ≠ÁªÉÈáèÂåñ(PTQ)ÂØπ‰∫éAR LLMsÊòØÊúâÊïàÁöÑÔºå‰ΩÜÁõ¥Êé•Â∞ÜÂÖ∂Â∫îÁî®‰∫é2ÊØîÁâπÁöÑdLLMs‰ºöÂØºËá¥‰∏ç‰ª§‰∫∫Êª°ÊÑèÁöÑÊÄßËÉΩ„ÄÇ‰∏∫‰∫ÜÂ∫îÂØπËøô‰∫õÊåëÊàòÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜQuant-dLLMÔºåËøôÊòØ‰∏Ä‰∏™‰∏∫dLLMsÈáèË∫´ÂÆöÂà∂ÁöÑË∂Ö‰ΩéÊØîÁâπPTQÊ°ÜÊû∂„ÄÇÁî±‰∫édLLMs‰∏≠ÁöÑÊé©Á†ÅÂéªÂô™ÊøÄÊ¥ª‰∏éÊ†áÂáÜPTQÊñπÊ≥ïÂÅáËÆæÁöÑÂÆåÂÖ®ÂèØËßÅ‰ø°Âè∑‰∏çÂêåÔºåÊàë‰ª¨ÂºïÂÖ•‰∫ÜÊé©Á†ÅÊ†°ÂáÜÊ®°Êãü(MCS)Êù•‰ΩøÊ†°ÂáÜ‰∏éÊó∂Èó¥Ê≠•Áõ∏ÂÖ≥ÁöÑÊé©Á†ÅÂØπÈΩêÔºå‰ªéËÄå‰∫ßÁîüÊõ¥ÂèØÈù†ÁöÑÊ†°ÂáÜ„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊï∞ÊçÆÊÑüÁü•‰ªªÊÑèÈò∂ÈáèÂåñÂô®(DAQ)ÔºåÂÆÉÈÄöËøá‰ºòÂåñÁÆóÊ≥ïÂ≠¶‰π†Ë∂Ö‰ΩéÊØîÁâπÊùÉÈáçË°®Á§∫„ÄÇÂÆÉÊâßË°åÁî±Êàë‰ª¨Ê®°ÊãüÁöÑÊ†°ÂáÜÊï∞ÊçÆÊåáÂØºÁöÑËø≠‰ª£Ëøë‰ºº„ÄÇÊ≠§Â§ñÔºåÂú®‰∏•Ê†ºÁöÑ2ÊØîÁâπÈ¢ÑÁÆó‰∏ãÔºåÊàë‰ª¨ÂºïÂÖ•‰∫ÜËá™ÈÄÇÂ∫îÂàÜÂùóÊ∑∑ÂêàÁ≤æÂ∫¶(ABMP)ÔºåËøôÊòØ‰∏ÄÁßçÂü∫‰∫éÊïèÊÑüÊÄßÁöÑÁ≤æÂ∫¶ÂàÜÈÖçÊñπÊ°àÔºåÂèØ‰ª•Ëá™ÈÄÇÂ∫îÂú∞Âú®ÈÄöÈÅìÁªÑ‰πãÈó¥ÂàÜÈÖçÊØîÁâπÂÆΩÂ∫¶„ÄÇÂΩìÈôêÂà∂‰∏∫2ÊØîÁâπÁ≤æÂ∫¶Êó∂ÔºåQuant-dLLMÂßãÁªàÊØîdLLMs‰∏äÊúÄÂÖàËøõÁöÑ(SOTA) AR-transfer PTQÊñπÊ≥ïËé∑ÂæóÊõ¥È´òÁöÑÁ≤æÂ∫¶„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Êâ©Êï£Â§ßËØ≠Ë®ÄÊ®°ÂûãÔºàdLLMsÔºâÂú®ÈÉ®ÁΩ≤Êó∂Ê®°Âûã‰ΩìÁßØËøáÂ§ßÔºåÈöæ‰ª•ËøõË°å‰ΩéÊàêÊú¨ÈÉ®ÁΩ≤ÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÁöÑÂêéËÆ≠ÁªÉÈáèÂåñÔºàPTQÔºâÊñπÊ≥ïËôΩÁÑ∂Âú®Ëá™ÂõûÂΩíÔºàARÔºâLLMs‰∏äË°®Áé∞ËâØÂ•ΩÔºå‰ΩÜÁõ¥Êé•Â∫îÁî®‰∫édLLMsÔºåÂ∞§ÂÖ∂ÊòØÂú®ÊûÅ‰ΩéÊØîÁâπÔºàÂ¶Ç2-bitÔºâÈáèÂåñÊó∂ÔºåÊÄßËÉΩ‰ºöÊÄ•Ââß‰∏ãÈôç„ÄÇËøôÊòØÂõ†‰∏∫dLLMsÁöÑÊé©Á†ÅÂéªÂô™ÊøÄÊ¥ª‰∏éARÊ®°Âûã‰∏çÂêåÔºåÊ†áÂáÜPTQÊñπÊ≥ïÊó†Ê≥ïÊúâÊïàÂ§ÑÁêÜ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈíàÂØπdLLMsÁöÑÁâπÊÄßÔºåËÆæËÆ°‰∏ìÈó®ÁöÑPTQÊ°ÜÊû∂„ÄÇÈÄöËøáÊ®°ÊãüdLLMsÁöÑÊé©Á†ÅËøáÁ®ãËøõË°åÊ†°ÂáÜÔºå‰ΩøÈáèÂåñËøáÁ®ãÊõ¥Â•ΩÂú∞ÈÄÇÂ∫îdLLMsÁöÑÊøÄÊ¥ªÂàÜÂ∏É„ÄÇÂêåÊó∂ÔºåËÆæËÆ°Êï∞ÊçÆÊÑüÁü•ÁöÑÈáèÂåñÂô®ÔºåÂ≠¶‰π†Êõ¥ÈÄÇÂêàdLLMsÁöÑÊùÉÈáçË°®Á§∫ÔºåÂπ∂Ê†πÊçÆ‰∏çÂêåÈÄöÈÅìÁöÑÈáçË¶ÅÊÄßËá™ÈÄÇÂ∫îÂú∞ÂàÜÈÖçÊØîÁâπÊï∞„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöQuant-dLLMÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰∏â‰∏™Ê†∏ÂøÉÊ®°ÂùóÔºö1) Êé©Á†ÅÊ†°ÂáÜÊ®°ÊãüÔºàMCSÔºâÔºöÊ®°ÊãüdLLMsÁöÑÊé©Á†ÅËøáÁ®ãÔºåÁîüÊàêÊõ¥ÂèØÈù†ÁöÑÊ†°ÂáÜÊï∞ÊçÆ„ÄÇ2) Êï∞ÊçÆÊÑüÁü•‰ªªÊÑèÈò∂ÈáèÂåñÂô®ÔºàDAQÔºâÔºöÈÄöËøá‰ºòÂåñÁÆóÊ≥ïÂ≠¶‰π†Ë∂Ö‰ΩéÊØîÁâπÊùÉÈáçË°®Á§∫„ÄÇ3) Ëá™ÈÄÇÂ∫îÂàÜÂùóÊ∑∑ÂêàÁ≤æÂ∫¶ÔºàABMPÔºâÔºöÊ†πÊçÆÈÄöÈÅìÊïèÊÑüÊÄßËá™ÈÄÇÂ∫îÂú∞ÂàÜÈÖçÊØîÁâπÂÆΩÂ∫¶„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÈíàÂØπdLLMsÁöÑÁâπÊÄßÔºåÊèêÂá∫‰∫ÜÊé©Á†ÅÊ†°ÂáÜÊ®°ÊãüÔºàMCSÔºâÂíåÊï∞ÊçÆÊÑüÁü•‰ªªÊÑèÈò∂ÈáèÂåñÂô®ÔºàDAQÔºâ„ÄÇMCSÈÄöËøáÊ®°ÊãüdLLMsÁöÑÊé©Á†ÅËøáÁ®ãÔºåËß£ÂÜ≥‰∫ÜÊ†áÂáÜPTQÊñπÊ≥ïÊó†Ê≥ïÊúâÊïàÂ§ÑÁêÜdLLMsÊøÄÊ¥ªÂàÜÂ∏ÉÁöÑÈóÆÈ¢ò„ÄÇDAQÂàôÈÄöËøá‰ºòÂåñÁÆóÊ≥ïÔºåÂ≠¶‰π†Êõ¥ÈÄÇÂêàdLLMsÁöÑÊùÉÈáçË°®Á§∫ÔºåÊèêÈ´ò‰∫ÜÈáèÂåñÂêéÁöÑÊ®°ÂûãÊÄßËÉΩ„ÄÇABMPËøõ‰∏ÄÊ≠•ÊèêÂçá‰∫ÜÈáèÂåñÊïàÁéáÔºåÂú®ÊúâÈôêÁöÑÊØîÁâπÈ¢ÑÁÆó‰∏ãÔºåÂÆûÁé∞‰∫ÜÊõ¥Â•ΩÁöÑÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÊé©Á†ÅÊ†°ÂáÜÊ®°ÊãüÔºàMCSÔºâÁöÑÂÖ≥ÈîÆÂú®‰∫éÊ®°ÊãüÊó∂Èó¥Ê≠•Áõ∏ÂÖ≥ÁöÑÊé©Á†ÅËøáÁ®ãÔºåÁîüÊàêÊõ¥ÁúüÂÆûÁöÑÊ†°ÂáÜÊï∞ÊçÆ„ÄÇÊï∞ÊçÆÊÑüÁü•‰ªªÊÑèÈò∂ÈáèÂåñÂô®ÔºàDAQÔºâÁöÑÂÖ≥ÈîÆÂú®‰∫éËÆæËÆ°ÂêàÈÄÇÁöÑ‰ºòÂåñÁÆóÊ≥ïÔºåÂ≠¶‰π†Ë∂Ö‰ΩéÊØîÁâπÊùÉÈáçË°®Á§∫„ÄÇËá™ÈÄÇÂ∫îÂàÜÂùóÊ∑∑ÂêàÁ≤æÂ∫¶ÔºàABMPÔºâÁöÑÂÖ≥ÈîÆÂú®‰∫éËÆæËÆ°ÂêàÈÄÇÁöÑÊïèÊÑüÊÄßÊåáÊ†áÔºåÂπ∂Ê†πÊçÆÊïèÊÑüÊÄßÊåáÊ†áËá™ÈÄÇÂ∫îÂú∞ÂàÜÈÖçÊØîÁâπÂÆΩÂ∫¶„ÄÇÂÖ∑‰ΩìÁöÑÊçüÂ§±ÂáΩÊï∞ÂíåÁΩëÁªúÁªìÊûÑÁ≠âÁªÜËäÇÊú™Âú®ÊëòË¶Å‰∏≠ËØ¶ÁªÜÊèèËø∞„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Quant-dLLMÂú®2ÊØîÁâπÁ≤æÂ∫¶‰∏ãÔºåÁõ∏ÊØî‰∫éÁõ¥Êé•Â∞ÜARÊ®°ÂûãÁöÑPTQÊñπÊ≥ïÂ∫îÁî®‰∫édLLMsÔºåÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇÂÖ∑‰ΩìÁöÑÊï∞ÊçÆÂíåÂØπÊØîÂü∫Á∫øÈúÄË¶ÅÂú®ËÆ∫ÊñáÂÖ®Êñá‰∏≠Êü•Êâæ„ÄÇËØ•ÊñπÊ≥ïÂú®ÊûÅ‰ΩéÊØîÁâπÈáèÂåñ‰∏ãÔºå‰øùÊåÅ‰∫ÜËæÉÈ´òÁöÑÊ®°ÂûãÁ≤æÂ∫¶ÔºåËØÅÊòé‰∫ÜÂÖ∂Âú®dLLMsÈáèÂåñÊñπÈù¢ÁöÑÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂêÑÁßçÈúÄË¶ÅÈÉ®ÁΩ≤Êâ©Êï£Â§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÂú∫ÊôØÔºå‰æãÂ¶ÇÁßªÂä®ËÆæÂ§á„ÄÅËæπÁºòËÆ°ÁÆóËÆæÂ§áÁ≠âËµÑÊ∫êÂèóÈôêÁöÑÁéØÂ¢É„ÄÇÈÄöËøáÊûÅ‰ΩéÊØîÁâπÈáèÂåñÔºåÂèØ‰ª•ÊòæËëóÈôç‰ΩéÊ®°Âûã‰ΩìÁßØÂíåËÆ°ÁÆóÂ§çÊùÇÂ∫¶Ôºå‰ªéËÄåÂÆûÁé∞dLLMsÂú®Ëøô‰∫õËÆæÂ§á‰∏äÁöÑÈ´òÊïàÈÉ®ÁΩ≤ÂíåÂ∫îÁî®„ÄÇËøôÂØπ‰∫éÊé®Âä®dLLMsÂú®Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ„ÄÅÂõæÂÉèÁîüÊàêÁ≠âÈ¢ÜÂüüÁöÑÂπøÊ≥õÂ∫îÁî®ÂÖ∑ÊúâÈáçË¶ÅÊÑè‰πâ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Diffusion large language models (dLLMs), which offer bidirectional context and flexible masked-denoising generation, are emerging as a compelling alternative to autoregressive (AR) LLMs. However, like AR LLMs, their model sizes continue to grow, motivating weight compression for deployment. Although post-training quantization (PTQ) is effective for AR LLMs, directly transferring it to dLLMs at 2-bit leads to unsatisfactory performance. To tackle these challenges, we propose Quant-dLLM, an ultra-low-bit PTQ framework tailored to dLLMs. Since masked-denoising activations in dLLMs differ from the fully visible signals assumed by standard PTQ methods, we introduce Masked Calibration Simulation (MCS) to align calibration with the timestep-dependent masking, which yields more reliable calibrations. Moreover, we propose a Data-aware Any-order Quantizer (DAQ) that learns ultra-low-bit weight representations via an optimization algorithm. It performs iterative approximation guided by our simulated calibration data. In addition, under a strict 2-bit budget, we introduce Adaptive Blockwise Mixed Precision (ABMP), a sensitivity-based precision allocation scheme that adaptively assigns bit width across channel groups. When restricted to 2-bit precision, Quant-dLLM consistently achieves higher accuracy than state-of-the-art (SOTA) AR-transfer PTQ methods on dLLMs. The code and models will be available at: https://github.com/ZTA2785/Quant-dLLM.

