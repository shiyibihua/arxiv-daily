---
layout: default
title: Bridging the Gap Between Promise and Performance for Microscaling FP4 Quantization
---

# Bridging the Gap Between Promise and Performance for Microscaling FP4 Quantization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.23202" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.23202v2</a>
  <a href="https://arxiv.org/pdf/2509.23202.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.23202v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.23202v2', 'Bridging the Gap Between Promise and Performance for Microscaling FP4 Quantization')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Vage Egiazarian, Roberto L. Castro, Denis Kuznedelev, Andrei Panferov, Eldar Kurtic, Shubhra Pandit, Alexandre Marques, Mark Kurtz, Saleh Ashkboos, Torsten Hoefler, Dan Alistarh

**ÂàÜÁ±ª**: cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-27 (Êõ¥Êñ∞: 2025-10-16)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫MR-GPTQÔºåÈíàÂØπFP4ÈáèÂåñÁâπÊÄß‰ºòÂåñGPTQÁÆóÊ≥ïÔºåÊèêÂçáLLMÊé®ÁêÜÊÄßËÉΩ„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `FP4ÈáèÂåñ` `GPTQ` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `Ê®°ÂûãÊé®ÁêÜ` `Á°¨‰ª∂Âä†ÈÄü`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®FP4ÈáèÂåñ‰∏≠Èù¢‰∏¥ÊåëÊàòÔºåNVFP4ÁöÑÂ∞èÁªÑÂ§ßÂ∞èÈôêÂà∂‰∫ÜÁ¶ªÁæ§ÂÄºÁºìËß£ÔºåMXFP4ÁöÑ‰∫åÊ¨°ÂπÇÁº©ÊîæÈáèÂåñÂºïÂÖ•‰∫ÜÈ´òËØØÂ∑Æ„ÄÇ
2. MR-GPTQÈÄöËøáÂùóÁä∂HadamardÂèòÊç¢ÂíåÊ†ºÂºèÁâπÂÆö‰ºòÂåñÔºå‰∏∫FP4ÈáèË∫´ÂÆöÂà∂ÈáèÂåñËøáÁ®ãÔºåËß£ÂÜ≥‰∫ÜÁé∞ÊúâÊñπÊ≥ïÁöÑÂ±ÄÈôêÊÄß„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåMR-GPTQÂú®NVIDIA B200ÂíåRTX5090‰∏äÂÆûÁé∞‰∫ÜÊòæËëóÁöÑÂä†ÈÄüÔºåÂπ∂ÊèêÂçá‰∫ÜMXFP4ÁöÑÁ≤æÂ∫¶ÔºåÊé•ËøëNVFP4ÁöÑÊ∞¥Âπ≥„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÂÖ®Èù¢Á†îÁ©∂‰∫ÜNVIDIAÂíåAMD GPU‰∏äÁ°¨‰ª∂Âä†ÈÄüÁöÑÂæÆÁº©Êîæ4‰ΩçÊµÆÁÇπÊ†ºÂºèMXFP4ÂíåNVFP4Âú®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÊé®ÁêÜ‰∏≠ÁöÑÂ∫îÁî®ÔºåÊè≠Á§∫‰∫ÜÂÖ∂ÊâøËØ∫‰∏éÂÆûÈôÖÊÄßËÉΩ‰πãÈó¥ÁöÑÂ∑ÆË∑ù„ÄÇÂàÜÊûêË°®ÊòéÔºåÁî±‰∫éNVFP4ÁöÑÂ∞èÁªÑÂ§ßÂ∞èÊäµÊ∂à‰∫Ü‰º†ÁªüÁöÑÁ¶ªÁæ§ÂÄºÁºìËß£ÊäÄÊúØÔºå‰ª•ÂèäMXFP4ÁöÑ‰∫åÊ¨°ÂπÇÁº©ÊîæÈáèÂåñÂØºËá¥ÁöÑÈ´òËØØÂ∑ÆÔºåÁé∞ÊúâÊñπÊ≥ïÂú®FP4‰∏äË°®Áé∞‰∏ç‰Ω≥„ÄÇ‰∏∫Ê≠§ÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫Micro-Rotated-GPTQÔºàMR-GPTQÔºâÁöÑGPTQÈáèÂåñÁÆóÊ≥ïÂèò‰ΩìÔºåÈÄöËøáÂùóÁä∂HadamardÂèòÊç¢ÂíåÁâπÂÆö‰∫éÊ†ºÂºèÁöÑ‰ºòÂåñÔºå‰∏ìÈó®ÈíàÂØπFP4ÁöÑÁã¨ÁâπÂ±ûÊÄßÂÆöÂà∂ÈáèÂåñËøáÁ®ã„ÄÇÈÄöËøáÈ´òÊÄßËÉΩGPUÂÜÖÊ†∏ÔºåMR-GPTQÊ†ºÂºèÂÆûÁé∞‰∫ÜÂèØÂøΩÁï•ÁöÑÂºÄÈîÄÔºåÂπ∂ÈÄöËøáÊóãËΩ¨ËûçÂêàÂà∞ÊùÉÈáç‰∏≠‰ª•ÂèäÂø´ÈÄüÂú®Á∫øËÆ°ÁÆóÊøÄÊ¥ªÊù•ÂÆûÁé∞„ÄÇÂú®NVIDIA B200‰∏äÔºåÂ±ÇÁ∫ßÂä†ÈÄüÈ´òËææ3.6ÂÄçÔºåÁ´ØÂà∞Á´ØÂä†ÈÄüÈ´òËææ2.2ÂÄçÔºõÂú®RTX5090‰∏äÔºåÂ±ÇÁ∫ßÂä†ÈÄüÈ´òËææ6ÂÄçÔºåÁ´ØÂà∞Á´ØÂä†ÈÄüÈ´òËææ4ÂÄç„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMR-GPTQÂåπÈÖçÊàñ‰ºò‰∫éÊúÄÂÖàËøõÁöÑÁ≤æÂ∫¶ÔºåÊòæËëóÊèêÂçá‰∫ÜMXFP4ÁöÑÊÄßËÉΩÔºå‰ΩøÂÖ∂Êé•ËøëNVFP4ÁöÑÁ≤æÂ∫¶„ÄÇÁªìËÆ∫ÊòØÔºåFP4Âπ∂ÈùûINT4ÁöÑËá™Âä®ÂçáÁ∫ßÔºå‰ΩÜÂÉèMR-GPTQËøôÊ†∑ÈíàÂØπÊ†ºÂºèÁöÑ‰∏ìÁî®ÊñπÊ≥ïÂèØ‰ª•ÂºÄÂêØÁ≤æÂ∫¶-ÊÄßËÉΩÊùÉË°°ÁöÑÊñ∞ÂâçÊ≤ø„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Âú®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÊé®ÁêÜ‰∏≠‰ΩøÁî®ÂæÆÁº©Êîæ4‰ΩçÊµÆÁÇπÊ†ºÂºèÔºàFP4ÔºåÂÖ∑‰ΩìÂåÖÊã¨MXFP4ÂíåNVFP4ÔºâËøõË°åÈáèÂåñÊó∂ÔºåÁé∞ÊúâÊñπÊ≥ïÊó†Ê≥ïÂÖÖÂàÜÂèëÊå•ÂÖ∂Á°¨‰ª∂Âä†ÈÄü‰ºòÂäøÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®FP4ÈáèÂåñ‰∏≠Èù¢‰∏¥Á≤æÂ∫¶ÊçüÂ§±ÂíåÊÄßËÉΩÁì∂È¢àÔºåÊó†Ê≥ïÂÆûÁé∞ÁêÜËÆ∫‰∏äÁöÑÂä†ÈÄüÊïàÊûú„ÄÇNVFP4ÁöÑÂ∞èÁªÑÂ§ßÂ∞èÈôêÂà∂‰∫ÜÁ¶ªÁæ§ÂÄºÁºìËß£Á≠ñÁï•ÁöÑÂ∫îÁî®ÔºåËÄåMXFP4ÁöÑ‰∫åÊ¨°ÂπÇÁº©ÊîæÈáèÂåñÂºïÂÖ•‰∫ÜËæÉÂ§ßÁöÑÈáèÂåñËØØÂ∑Æ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈíàÂØπFP4Ê†ºÂºèÁöÑÁâπÊÄßÔºåÂÆöÂà∂ÈáèÂåñÁÆóÊ≥ï„ÄÇÈÄöËøáÂºïÂÖ•Micro-Rotated-GPTQ (MR-GPTQ)Ôºå‰∏ÄÁßçÂü∫‰∫éGPTQÁöÑÂèò‰ΩìÔºåÂà©Áî®ÂùóÁä∂HadamardÂèòÊç¢ÂíåÊ†ºÂºèÁâπÂÆöÁöÑ‰ºòÂåñÔºåÊù•ÂáèÂ∞ëÈáèÂåñËØØÂ∑ÆÂπ∂ÊèêÈ´òÊé®ÁêÜÈÄüÂ∫¶„ÄÇËøôÁßçÂÆöÂà∂ÂåñÁöÑÊñπÊ≥ïÊó®Âú®Âº•ÂêàFP4ÁöÑÁêÜËÆ∫ÊÄßËÉΩ‰∏éÂÆûÈôÖÊÄßËÉΩ‰πãÈó¥ÁöÑÂ∑ÆË∑ù„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöMR-GPTQÁöÑÊï¥‰ΩìÊ°ÜÊû∂Âü∫‰∫éGPTQÁÆóÊ≥ïÔºå‰∏ªË¶ÅÂåÖÂê´‰ª•‰∏ãÂá†‰∏™Èò∂ÊÆµÔºö1) ÊùÉÈáçÈ¢ÑÂ§ÑÁêÜÔºö‰ΩøÁî®ÂùóÁä∂HadamardÂèòÊç¢ÂØπÊùÉÈáçËøõË°åÊóãËΩ¨Ôºå‰ª•ÂáèÂ∞ëÈáèÂåñËØØÂ∑Æ„ÄÇ2) ÈáèÂåñÔºö‰ΩøÁî®ÈíàÂØπMXFP4ÂíåNVFP4Ê†ºÂºè‰ºòÂåñÁöÑÈáèÂåñÊñπÊ≥ïÔºåÂ∞ÜÊùÉÈáçËΩ¨Êç¢‰∏∫FP4Ê†ºÂºè„ÄÇ3) Êé®ÁêÜÔºö‰ΩøÁî®‰ºòÂåñÁöÑGPUÂÜÖÊ†∏ËøõË°åÊé®ÁêÜÔºåÂ∞ÜÊóãËΩ¨ËûçÂêàÂà∞ÊùÉÈáç‰∏≠ÔºåÂπ∂Âø´ÈÄüÂú®Á∫øËÆ°ÁÆóÊøÄÊ¥ª„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÈíàÂØπFP4Ê†ºÂºèÁöÑÁâπÊÄßÔºåÂØπGPTQÁÆóÊ≥ïËøõË°å‰∫ÜÂÆöÂà∂Âåñ‰øÆÊîπ„ÄÇÂÖ∑‰ΩìÂåÖÊã¨Ôºö1) ÂùóÁä∂HadamardÂèòÊç¢ÔºöÈÄöËøáÊóãËΩ¨ÊùÉÈáçÔºåÂáèÂ∞ëÈáèÂåñËØØÂ∑ÆÔºåÊèêÈ´òÁ≤æÂ∫¶„ÄÇ2) Ê†ºÂºèÁâπÂÆö‰ºòÂåñÔºöÈíàÂØπMXFP4ÂíåNVFP4ÁöÑÁâπÊÄßÔºåËÆæËÆ°‰∫Ü‰∏çÂêåÁöÑÈáèÂåñÁ≠ñÁï•Ôºå‰ª•ÊúÄÂ§ßÁ®ãÂ∫¶Âú∞ÂáèÂ∞ëÈáèÂåñËØØÂ∑Æ„ÄÇ3) È´òÊÄßËÉΩGPUÂÜÖÊ†∏ÔºöÂºÄÂèë‰∫Ü‰ºòÂåñÁöÑGPUÂÜÖÊ†∏ÔºåÂÆûÁé∞‰∫ÜÊóãËΩ¨ËûçÂêàÂíåÂø´ÈÄüÊøÄÊ¥ªËÆ°ÁÆóÔºåÊèêÈ´ò‰∫ÜÊé®ÁêÜÈÄüÂ∫¶„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöMR-GPTQÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ÂùóÂ§ßÂ∞èÁöÑÈÄâÊã©ÔºöHadamardÂèòÊç¢ÁöÑÂùóÂ§ßÂ∞èÈúÄË¶ÅÊ†πÊçÆÊ®°ÂûãÁöÑÁªìÊûÑÂíåFP4Ê†ºÂºèÁöÑÁâπÊÄßËøõË°åË∞ÉÊï¥„ÄÇ2) ÈáèÂåñÁ≠ñÁï•ÔºöÈíàÂØπMXFP4ÁöÑ‰∫åÊ¨°ÂπÇÁº©ÊîæÁâπÊÄßÔºåËÆæËÆ°‰∫ÜÁâπÂÆöÁöÑÈáèÂåñÁ≠ñÁï•Ôºå‰ª•ÂáèÂ∞ëÈáèÂåñËØØÂ∑Æ„ÄÇ3) ÊçüÂ§±ÂáΩÊï∞Ôºö‰ΩøÁî®GPTQÁöÑÊçüÂ§±ÂáΩÊï∞Ôºå‰ª•ÊúÄÂ∞èÂåñÈáèÂåñÂêéÁöÑÊ®°Âûã‰∏éÂéüÂßãÊ®°Âûã‰πãÈó¥ÁöÑÂ∑ÆÂºÇ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMR-GPTQÂú®NVIDIA B200‰∏äÂÆûÁé∞‰∫ÜÈ´òËææ3.6ÂÄçÁöÑÂ±ÇÁ∫ßÂä†ÈÄüÂíå2.2ÂÄçÁöÑÁ´ØÂà∞Á´ØÂä†ÈÄüÔºåÂú®RTX5090‰∏äÂÆûÁé∞‰∫ÜÈ´òËææ6ÂÄçÁöÑÂ±ÇÁ∫ßÂä†ÈÄüÂíå4ÂÄçÁöÑÁ´ØÂà∞Á´ØÂä†ÈÄü„ÄÇÊ≠§Â§ñÔºåMR-GPTQÊòæËëóÊèêÂçá‰∫ÜMXFP4ÁöÑÁ≤æÂ∫¶Ôºå‰ΩøÂÖ∂Êé•ËøëNVFP4ÁöÑÊ∞¥Âπ≥ÔºåËØÅÊòé‰∫ÜÂÖ∂Âú®FP4ÈáèÂåñÊñπÈù¢ÁöÑÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂêÑÁßçÈúÄË¶Å‰ΩéÁ≤æÂ∫¶ÈáèÂåñÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜÂú∫ÊôØÔºå‰æãÂ¶ÇËæπÁºòËÆæÂ§áÈÉ®ÁΩ≤„ÄÅËµÑÊ∫êÂèóÈôêÁéØÂ¢É‰∏ãÁöÑÊ®°ÂûãÂä†ÈÄüÁ≠â„ÄÇÈÄöËøáÊèêÂçáFP4ÈáèÂåñÁöÑÁ≤æÂ∫¶ÂíåÊÄßËÉΩÔºåÂèØ‰ª•Èôç‰ΩéÊ®°ÂûãÈÉ®ÁΩ≤ÁöÑÊàêÊú¨ÔºåÂπ∂ÊèêÈ´òÊé®ÁêÜÊïàÁéáÔºå‰ªéËÄåÊé®Âä®LLMÂú®Êõ¥ÂπøÊ≥õÈ¢ÜÂüüÁöÑÂ∫îÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> The recent hardware-accelerated microscaling 4-bit floating-point formats such as MXFP4 and NVFP4, supported on NVIDIA and AMD GPUs, promise to revolutionize large language model (LLM) inference. Yet, their practical benefits remain unproven. We present the first comprehensive study of MXFP4 and NVFP4 for post-training quantization, revealing gaps between their promise and real-world performance. Our analysis shows that state-of-the-art methods struggle with FP4, due to two key issues: (1) NVFP4's small group size provably neutralizes traditional outlier mitigation techniques; (2) MXFP4's power-of-two scale quantization severely degrades accuracy due to high induced error. To bridge this gap, we introduce Micro-Rotated-GPTQ (MR-GPTQ), a variant of the classic GPTQ quantization algorithm that tailors the quantization process to FP4's unique properties, by using block-wise Hadamard transforms and format-specific optimizations. We support our proposal with a set of high-performance GPU kernels that enable the MR-GPTQ format with negligible overhead, by rotation fusion into the weights, and fast online computation of the activations. This leads to speedups vs. FP16 of up to 3.6x layer-wise, and 2.2x end-to-end on NVIDIA B200, and of 6x layer-wise and 4x end-to-end on RTX5090. Our extensive empirical evaluation demonstrates that MR-GPTQ matches or outperforms state-of-the-art accuracy, significantly boosting MXFP4, to the point where it can near the accuracy that of NVFP4. We conclude that, while FP4 is not an automatic upgrade over INT4, format-specialized methods like MR-GPTQ can unlock a new frontier of accuracy-performance trade-offs.

