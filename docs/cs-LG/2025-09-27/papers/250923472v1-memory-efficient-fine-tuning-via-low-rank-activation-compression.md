---
layout: default
title: Memory-Efficient Fine-Tuning via Low-Rank Activation Compression
---

# Memory-Efficient Fine-Tuning via Low-Rank Activation Compression

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.23472" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.23472v1</a>
  <a href="https://arxiv.org/pdf/2509.23472.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.23472v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.23472v1', 'Memory-Efficient Fine-Tuning via Low-Rank Activation Compression')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiang-Xin Shi, Wen-Da Wei, Jin-Fei Qi, Xuanyu Chen, Tong Wei, Yu-Feng Li

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-27

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/shijxcs/meft)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLoRActï¼Œé€šè¿‡ä½ç§©æ¿€æ´»å‹ç¼©å®ç°é«˜æ•ˆçš„å‚æ•°å¾®è°ƒï¼Œæ˜¾è‘—é™ä½å†…å­˜å ç”¨ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å‚æ•°é«˜æ•ˆå¾®è°ƒ` `ä½ç§©æ¿€æ´»å‹ç¼©` `å†…å­˜ä¼˜åŒ–` `æ¨¡å‹éƒ¨ç½²` `æ­£äº¤åˆ†è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•è™½ç„¶å‡å°‘äº†å¯è®­ç»ƒå‚æ•°ï¼Œä½†å†…å­˜å¼€é”€ä»ç„¶å·¨å¤§ï¼Œé™åˆ¶äº†å®é™…éƒ¨ç½²ã€‚
2. LoRActé€šè¿‡è§‚å¯Ÿåˆ°æ¿€æ´»çš„ä½ç§©ç‰¹æ€§ï¼Œæå‡ºåœ¨çº¿ä½ç§©æ¿€æ´»å‹ç¼©ï¼Œæ— éœ€æ ¡å‡†æ•°æ®ï¼Œæ›´çµæ´»é€šç”¨ã€‚
3. LoRActé‡‡ç”¨æ–°å‹é‡‡æ ·æ­£äº¤åˆ†è§£ç®—æ³•ï¼Œæå‡è®¡ç®—æ•ˆç‡ï¼Œå¹¶å‡å°‘äº†80%çš„æ¿€æ´»å†…å­˜ï¼Œæ€§èƒ½æœ‰ç«äº‰åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€åŸºç¡€æ¨¡å‹çš„å‘å±•ï¼Œå‚æ•°é«˜æ•ˆå¾®è°ƒèŒƒå¼å—åˆ°äº†å¹¿æ³›å…³æ³¨ã€‚å°½ç®¡å·²ç»æå‡ºäº†è®¸å¤šå‡å°‘å¯è®­ç»ƒå‚æ•°æ•°é‡çš„æ–¹æ³•ï¼Œä½†å…¶å·¨å¤§çš„å†…å­˜å¼€é”€ä»ç„¶æ˜¯é˜»ç¢å®é™…éƒ¨ç½²çš„å…³é”®ç“¶é¢ˆã€‚æœ¬æ–‡è§‚å¯Ÿåˆ°ï¼Œæ¨¡å‹æ¿€æ´»æ„æˆäº†å†…å­˜æ¶ˆè€—çš„ä¸»è¦æ¥æºï¼Œå°¤å…¶æ˜¯åœ¨å¤§æ‰¹é‡å’Œé•¿ä¸Šä¸‹æ–‡é•¿åº¦ä¸‹ï¼›ç„¶è€Œï¼Œæ¿€æ´»çš„ç§©å§‹ç»ˆä¿æŒè¾ƒä½ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å†…å­˜é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•ï¼šä½ç§©æ¿€æ´»å‹ç¼©ï¼ˆLoRActï¼‰ã€‚ä¸å…ˆå‰çš„å·¥ä½œä¸åŒï¼ŒLoRActæä¾›äº†ä¸€ç§æ›´çµæ´»å’Œé€šç”¨çš„å‹ç¼©ç­–ç•¥ï¼Œå¯ä»¥åœ¨å‰å‘ä¼ æ’­æœŸé—´åœ¨çº¿åº”ç”¨ï¼Œè€Œæ— éœ€ä»»ä½•æ ¡å‡†æ•°æ®ã€‚æ­¤å¤–ï¼ŒLoRActè¿˜åŒ…å«ä¸€ç§ä¸“é—¨ä¸ºä½ç§©çŸ©é˜µè®¾è®¡çš„æ–°å‹åŸºäºé‡‡æ ·çš„æ­£äº¤åˆ†è§£ç®—æ³•ï¼Œä¸å¹¿æ³›ä½¿ç”¨çš„RSVDç›¸æ¯”ï¼Œæä¾›äº†æ›´é«˜çš„è®¡ç®—æ•ˆç‡å’Œæ›´ä¸¥æ ¼çš„è¯¯å·®ç•Œé™ã€‚åœ¨è§†è§‰å’Œè¯­è¨€ä»»åŠ¡ä¸Šçš„å®éªŒè¯æ˜äº†LoRActçš„æœ‰æ•ˆæ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä¸å¹¿æ³›é‡‡ç”¨çš„LoRAæ–¹æ³•ç›¸æ¯”ï¼ŒLoRActè¿›ä¸€æ­¥å‡å°‘äº†çº¦80%çš„æ¿€æ´»å†…å­˜ï¼ŒåŒæ—¶ä¿æŒäº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚æºä»£ç å¯åœ¨https://github.com/shijxcs/meftè·å–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å‚æ•°é«˜æ•ˆå¾®è°ƒä¸­æ¨¡å‹æ¿€æ´»å¸¦æ¥çš„å·¨å¤§å†…å­˜å¼€é”€é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•è™½ç„¶å‡å°‘äº†å¯è®­ç»ƒå‚æ•°çš„æ•°é‡ï¼Œä½†å¿½ç•¥äº†æ¿€æ´»æ‰€å ç”¨çš„å†…å­˜ï¼Œå°¤å…¶æ˜¯åœ¨å¤§æ‰¹é‡å’Œé•¿ä¸Šä¸‹æ–‡åœºæ™¯ä¸‹ï¼Œè¿™æˆä¸ºäº†æ¨¡å‹éƒ¨ç½²çš„ç“¶é¢ˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ¨¡å‹æ¿€æ´»çš„ä½ç§©ç‰¹æ€§ï¼Œé€šè¿‡ä½ç§©çŸ©é˜µåˆ†è§£å’Œå‹ç¼©æŠ€æœ¯ï¼Œåœ¨ä¸æ˜¾è‘—å½±å“æ¨¡å‹æ€§èƒ½çš„å‰æä¸‹ï¼Œå¤§å¹…é™ä½æ¿€æ´»æ‰€å ç”¨çš„å†…å­˜ç©ºé—´ã€‚æ ¸å¿ƒåœ¨äºåœ¨çº¿å‹ç¼©ï¼Œé¿å…äº†ç¦»çº¿æ ¡å‡†çš„éœ€è¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLoRActçš„æ ¸å¿ƒæ¡†æ¶æ˜¯åœ¨å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œå¯¹æ¨¡å‹çš„æ¿€æ´»è¿›è¡Œä½ç§©åˆ†è§£å’Œå‹ç¼©ã€‚å…·ä½“æµç¨‹åŒ…æ‹¬ï¼š1) æ¿€æ´»é‡‡æ ·ï¼šä»æ¿€æ´»çŸ©é˜µä¸­é‡‡æ ·ä¸€éƒ¨åˆ†åˆ—ï¼›2) æ­£äº¤åˆ†è§£ï¼šå¯¹é‡‡æ ·å¾—åˆ°çš„çŸ©é˜µè¿›è¡Œæ­£äº¤åˆ†è§£ï¼Œå¾—åˆ°ä½ç§©è¡¨ç¤ºï¼›3) æ¿€æ´»é‡æ„ï¼šåˆ©ç”¨ä½ç§©è¡¨ç¤ºé‡æ„æ¿€æ´»çŸ©é˜µï¼Œå¹¶è¿›è¡Œåç»­è®¡ç®—ã€‚æ•´ä¸ªè¿‡ç¨‹åœ¨çº¿è¿›è¡Œï¼Œæ— éœ€é¢å¤–çš„æ ¡å‡†æ•°æ®ã€‚

**å…³é”®åˆ›æ–°**ï¼šLoRActçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†ä¸€ç§çµæ´»é€šç”¨çš„åœ¨çº¿ä½ç§©æ¿€æ´»å‹ç¼©ç­–ç•¥ï¼Œæ— éœ€æ ¡å‡†æ•°æ®ï¼›2) è®¾è®¡äº†ä¸€ç§åŸºäºé‡‡æ ·çš„æ­£äº¤åˆ†è§£ç®—æ³•ï¼Œä¸“é—¨é’ˆå¯¹ä½ç§©çŸ©é˜µä¼˜åŒ–ï¼Œæé«˜äº†è®¡ç®—æ•ˆç‡ï¼Œå¹¶æä¾›äº†æ›´ä¸¥æ ¼çš„è¯¯å·®ç•Œé™ã€‚è¿™ç§ç®—æ³•ä¼˜äºä¼ ç»Ÿçš„éšæœºå¥‡å¼‚å€¼åˆ†è§£ï¼ˆRSVDï¼‰ã€‚

**å…³é”®è®¾è®¡**ï¼šLoRActçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) é‡‡æ ·ç­–ç•¥ï¼šå¦‚ä½•é€‰æ‹©æœ€å…·ä»£è¡¨æ€§çš„æ¿€æ´»åˆ—è¿›è¡Œé‡‡æ ·ï¼Œä»¥ä¿è¯é‡æ„çš„å‡†ç¡®æ€§ï¼›2) ä½ç§©ç»´åº¦é€‰æ‹©ï¼šå¦‚ä½•ç¡®å®šä½ç§©è¡¨ç¤ºçš„ç»´åº¦ï¼Œä»¥åœ¨å†…å­˜å ç”¨å’Œæ€§èƒ½ä¹‹é—´å–å¾—å¹³è¡¡ï¼›3) æ­£äº¤åˆ†è§£ç®—æ³•ï¼šå…·ä½“é‡‡ç”¨å“ªç§æ­£äº¤åˆ†è§£ç®—æ³•ï¼Œä»¥åŠå¦‚ä½•ä¼˜åŒ–ç®—æ³•çš„è®¡ç®—æ•ˆç‡ã€‚è®ºæ–‡æå‡ºçš„é‡‡æ ·æ­£äº¤åˆ†è§£ç®—æ³•æ˜¯å…³é”®ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒLoRActåœ¨è§†è§‰å’Œè¯­è¨€ä»»åŠ¡ä¸Šå‡è¡¨ç°å‡ºè‰²ã€‚ä¸LoRAç›¸æ¯”ï¼ŒLoRActè¿›ä¸€æ­¥å‡å°‘äº†çº¦80%çš„æ¿€æ´»å†…å­˜ï¼ŒåŒæ—¶ä¿æŒäº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚è¿™è¡¨æ˜LoRActæ˜¯ä¸€ç§æœ‰æ•ˆçš„å†…å­˜é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼Œå¯ä»¥åœ¨ä¸ç‰ºç‰²æ€§èƒ½çš„å‰æä¸‹ï¼Œæ˜¾è‘—é™ä½å†…å­˜å ç”¨ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

LoRActå¯åº”ç”¨äºå„ç§éœ€è¦é«˜æ•ˆå¾®è°ƒçš„å¤§å‹æ¨¡å‹ï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šï¼Œä¾‹å¦‚ç§»åŠ¨è®¾å¤‡å’Œè¾¹ç¼˜è®¡ç®—è®¾å¤‡ã€‚è¯¥æ–¹æ³•å¯ä»¥é™ä½æ¨¡å‹éƒ¨ç½²çš„å†…å­˜éœ€æ±‚ï¼Œä½¿å¾—åœ¨è¿™äº›è®¾å¤‡ä¸Šè¿è¡Œå¤§å‹æ¨¡å‹æˆä¸ºå¯èƒ½ã€‚æ­¤å¤–ï¼ŒLoRActè¿˜å¯ä»¥åŠ é€Ÿæ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ï¼Œæé«˜æ¨¡å‹çš„æ•ˆç‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The parameter-efficient fine-tuning paradigm has garnered significant attention with the advancement of foundation models. Although numerous methods have been proposed to reduce the number of trainable parameters, their substantial memory overhead remains a critical bottleneck that hinders practical deployment. In this paper, we observe that model activations constitute a major source of memory consumption, especially under large batch sizes and long context lengths; however, the rank of the activations remains consistently low. Motivated by this insight, we propose a memory-efficient fine-tuning approach Low-Rank Activation Compression (LoRAct). Unlike prior work, LoRAct provides a more flexible and versatile compressing strategy that can be applied online during the forward pass without the need for any calibration data. Moreover, LoRAct incorporates a novel sampling-based orthogonal decomposition algorithm specifically designed for low-rank matrices, offering improved computational efficiency and a tighter error bound compared to the widely used RSVD. Experiments on both vision and language tasks demonstrate the effectiveness of LoRAct. Notably, LoRAct further reduces activation memory by approximately 80% in comparison with the widely adopted LoRA method, while maintaining competitive performance. The source code is available at https://github.com/shijxcs/meft.

