---
layout: default
title: Tracing the Representation Geometry of Language Models from Pretraining to Post-training
---

# Tracing the Representation Geometry of Language Models from Pretraining to Post-training

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.23024" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.23024v1</a>
  <a href="https://arxiv.org/pdf/2509.23024.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.23024v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.23024v1', 'Tracing the Representation Geometry of Language Models from Pretraining to Post-training')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Melody Zixuan Li, Kumar Krishna Agrawal, Arna Ghosh, Komal Kumar Teru, Adam Santoro, Guillaume Lajoie, Blake A. Richards

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-27

**å¤‡æ³¨**: 33 pages, 14 figures, 9 tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå‡ ä½•è¡¨ç¤ºè¿½è¸ªæ–¹æ³•ä»¥æ­ç¤ºè¯­è¨€æ¨¡å‹çš„å¤æ‚èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­è¨€æ¨¡å‹` `å‡ ä½•è¡¨ç¤º` `é¢„è®­ç»ƒ` `åè®­ç»ƒ` `è°±æ–¹æ³•` `æœ‰æ•ˆç§©` `ç‰¹å¾è°±è¡°å‡` `ä»»åŠ¡æ€§èƒ½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è®­ç»ƒæŒ‡æ ‡å¦‚æŸå¤±å‡½æ•°æ— æ³•æœ‰æ•ˆè§£é‡Šå¤§å‹è¯­è¨€æ¨¡å‹ä¸­å¤æ‚èƒ½åŠ›çš„å‡ºç°ï¼Œå¯¼è‡´å¯¹æ¨¡å‹è¡Œä¸ºçš„ç†è§£ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§è°±æ–¹æ³•ï¼Œé€šè¿‡æœ‰æ•ˆç§©å’Œç‰¹å¾è°±è¡°å‡æ¥ç ”ç©¶è¯­è¨€æ¨¡å‹åœ¨é¢„è®­ç»ƒå’Œåè®­ç»ƒä¸­çš„è¡¨ç¤ºå‡ ä½•å˜åŒ–ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ¨¡å‹åœ¨è‡ªå›å½’é¢„è®­ç»ƒè¿‡ç¨‹ä¸­ç»å†äº†ä¸‰ä¸ªå‡ ä½•é˜¶æ®µï¼Œä¸”åè®­ç»ƒè¿›ä¸€æ­¥å½±å“äº†æ¨¡å‹çš„å‡ ä½•ç‰¹æ€§å’Œä»»åŠ¡æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ ‡å‡†è®­ç»ƒæŒ‡æ ‡å¦‚æŸå¤±æ— æ³•è§£é‡Šå¤§å‹è¯­è¨€æ¨¡å‹ä¸­å¤æ‚èƒ½åŠ›çš„å‡ºç°ã€‚æœ¬æ–‡é‡‡ç”¨è°±æ–¹æ³•ç ”ç©¶é¢„è®­ç»ƒå’Œåè®­ç»ƒä¸­å­¦ä¹ è¡¨ç¤ºçš„å‡ ä½•ç‰¹æ€§ï¼Œæµ‹é‡æœ‰æ•ˆç§©ï¼ˆRankMeï¼‰å’Œç‰¹å¾è°±è¡°å‡ï¼ˆ$Î±$-ReQï¼‰ã€‚é€šè¿‡å¯¹OLMoå’ŒPythiaæ¨¡å‹çš„åˆ†æï¼Œå‘ç°è‡ªå›å½’é¢„è®­ç»ƒè¿‡ç¨‹ä¸­å­˜åœ¨ä¸‰ä¸ªå‡ ä½•é˜¶æ®µï¼šåˆå§‹çš„â€œçƒ­èº«â€é˜¶æ®µè¡¨ç°å‡ºå¿«é€Ÿçš„è¡¨ç¤ºå´©æºƒï¼Œæ¥ç€æ˜¯â€œå¯»æ±‚ç†µâ€é˜¶æ®µï¼Œæµå½¢çš„ç»´åº¦æ˜¾è‘—æ‰©å±•ï¼Œæœ€åæ˜¯â€œå¯»æ±‚å‹ç¼©â€é˜¶æ®µï¼Œé€‰æ‹©æ€§ä¿ç•™ä¸»ç‰¹å¾æ–¹å‘çš„æ–¹å·®ã€‚åè®­ç»ƒè¿›ä¸€æ­¥æ”¹å˜å‡ ä½•ç‰¹æ€§ï¼ŒSFTå’ŒDPOé©±åŠ¨â€œå¯»æ±‚ç†µâ€åŠ¨æ€ï¼Œè€ŒRLVRåˆ™å¢å¼ºå¥–åŠ±å¯¹é½ä½†é™ä½ç”Ÿæˆå¤šæ ·æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è®­ç»ƒæŒ‡æ ‡æ— æ³•è§£é‡Šå¤§å‹è¯­è¨€æ¨¡å‹å¤æ‚èƒ½åŠ›çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨é¢„è®­ç»ƒå’Œåè®­ç»ƒé˜¶æ®µçš„å‡ ä½•è¡¨ç¤ºå˜åŒ–ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥è°±æ–¹æ³•ï¼Œæµ‹é‡æœ‰æ•ˆç§©å’Œç‰¹å¾è°±è¡°å‡ï¼Œåˆ†æè¯­è¨€æ¨¡å‹åœ¨ä¸åŒè®­ç»ƒé˜¶æ®µçš„å‡ ä½•ç‰¹æ€§ï¼Œä»¥æ­ç¤ºå…¶èƒ½åŠ›çš„æ¼”å˜è¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶åˆ†ä¸ºé¢„è®­ç»ƒå’Œåè®­ç»ƒä¸¤ä¸ªä¸»è¦é˜¶æ®µã€‚åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼Œæ¨¡å‹ç»å†â€œçƒ­èº«â€ã€â€œå¯»æ±‚ç†µâ€å’Œâ€œå¯»æ±‚å‹ç¼©â€ä¸‰ä¸ªå‡ ä½•é˜¶æ®µï¼›åœ¨åè®­ç»ƒé˜¶æ®µï¼ŒSFTã€DPOå’ŒRLVRç­‰æ–¹æ³•è¿›ä¸€æ­¥å½±å“å‡ ä½•ç‰¹æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæå‡ºäº†åŸºäºæœ‰æ•ˆç§©å’Œç‰¹å¾è°±è¡°å‡çš„å‡ ä½•è¡¨ç¤ºè¿½è¸ªæ–¹æ³•ï¼Œæ­ç¤ºäº†è¯­è¨€æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„éå•è°ƒå‡ ä½•å˜åŒ–ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæä¾›äº†æ›´æ·±å±‚æ¬¡çš„ç†è§£ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œä½¿ç”¨äº†OLMoå’ŒPythiaæ¨¡å‹ï¼Œé‡ç‚¹å…³æ³¨ä¸åŒè®­ç»ƒé˜¶æ®µçš„è¡¨ç¤ºå˜åŒ–ï¼Œç‰¹åˆ«æ˜¯ç»´åº¦æ‰©å±•å’Œæ–¹å·®é€‰æ‹©æ€§ä¿ç•™çš„æœºåˆ¶ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨è‡ªå›å½’é¢„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹ç»å†äº†ä¸‰ä¸ªå‡ ä½•é˜¶æ®µï¼Œç‰¹åˆ«æ˜¯åœ¨â€œå¯»æ±‚å‹ç¼©â€é˜¶æ®µï¼Œæ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„æ€§èƒ½æ˜¾è‘—æå‡ï¼Œè¡¨ç°å‡ºæ›´å¥½çš„æ–¹å·®é€‰æ‹©æ€§ã€‚åè®­ç»ƒé˜¶æ®µçš„SFTå’ŒDPOæ–¹æ³•ä½¿å¾—æ¨¡å‹åœ¨ç‰¹å®šæ•°æ®é›†ä¸Šçš„è¡¨ç°å¾—åˆ°æ”¹å–„ï¼Œä½†åœ¨åˆ†å¸ƒå¤–çš„é²æ£’æ€§ä¸Šæœ‰æ‰€ä¸‹é™ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶ä¸ºç†è§£å¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹æä¾›äº†æ–°çš„è§†è§’ï¼Œèƒ½å¤Ÿå¸®åŠ©ç ”ç©¶äººå‘˜ä¼˜åŒ–æ¨¡å‹è®¾è®¡å’Œè®­ç»ƒç­–ç•¥ï¼Œæå‡æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯åº”ç”¨äºå¤šç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œæ¨åŠ¨æ™ºèƒ½å¯¹è¯ç³»ç»Ÿã€æ–‡æœ¬ç”Ÿæˆç­‰é¢†åŸŸçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Standard training metrics like loss fail to explain the emergence of complex capabilities in large language models. We take a spectral approach to investigate the geometry of learned representations across pretraining and post-training, measuring effective rank (RankMe) and eigenspectrum decay ($Î±$-ReQ). With OLMo (1B-7B) and Pythia (160M-12B) models, we uncover a consistent non-monotonic sequence of three geometric phases during autoregressive pretraining. The initial "warmup" phase exhibits rapid representational collapse. This is followed by an "entropy-seeking" phase, where the manifold's dimensionality expands substantially, coinciding with peak n-gram memorization. Subsequently, a "compression-seeking" phase imposes anisotropic consolidation, selectively preserving variance along dominant eigendirections while contracting others, a transition marked with significant improvement in downstream task performance. We show these phases can emerge from a fundamental interplay of cross-entropy optimization under skewed token frequencies and representational bottlenecks ($d \ll \|V\|$). Post-training further transforms geometry: SFT and DPO drive "entropy-seeking" dynamics to integrate specific instructional or preferential data, improving in-distribution performance while degrading out-of-distribution robustness. Conversely, RLVR induces "compression-seeking", enhancing reward alignment but reducing generation diversity.

