---
layout: default
title: Adaptive Token-Weighted Differential Privacy for LLMs: Not All Tokens Require Equal Protection
---

# Adaptive Token-Weighted Differential Privacy for LLMs: Not All Tokens Require Equal Protection

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.23246" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.23246v1</a>
  <a href="https://arxiv.org/pdf/2509.23246.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.23246v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.23246v1', 'Adaptive Token-Weighted Differential Privacy for LLMs: Not All Tokens Require Equal Protection')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Manjiang Yu, Priyanka Singh, Xue Li, Yang Cao

**ÂàÜÁ±ª**: cs.LG, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-27

**Â§áÊ≥®**: 18 pages

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Ëá™ÈÄÇÂ∫îTokenÂä†ÊùÉÂ∑ÆÂàÜÈöêÁßÅ(ATDP)ÊñπÊ≥ïÔºåÂä†ÈÄüLLMÁöÑÂ∑ÆÂàÜÈöêÁßÅËÆ≠ÁªÉÂπ∂ÊèêÂçáÊïèÊÑü‰ø°ÊÅØ‰øùÊä§„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â∑ÆÂàÜÈöêÁßÅ` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `ÈöêÁßÅ‰øùÊä§` `Ëá™ÈÄÇÂ∫îÂä†ÊùÉ` `ÊïèÊÑü‰ø°ÊÅØ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâDPSGDÊñπÊ≥ïÂØπÊâÄÊúâÊ¢ØÂ∫¶Ê≥®ÂÖ•Áªü‰∏ÄÂô™Â£∞ÔºåÂØºËá¥LLMËÆ≠ÁªÉÊó∂Èó¥ËøáÈïø‰∏îÁ≤æÂ∫¶‰∏ãÈôç„ÄÇ
2. ATDPÈÄöËøáËá™ÈÄÇÂ∫îÂú∞Ë∞ÉÊï¥ÊïèÊÑüÂíåÈùûÊïèÊÑütokenÁöÑÊ¢ØÂ∫¶ÊùÉÈáçÔºåÈõÜ‰∏≠Âô™Â£∞‰∫éÊïèÊÑütokenÔºå‰ªéËÄåÂä†ÈÄüËÆ≠ÁªÉ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåATDPËÉΩÊòæËëóÂáèÂ∞ëDPËÆ≠ÁªÉÊó∂Èó¥ÔºàÁ∫¶90%ÔºâÔºåÂêåÊó∂‰øùÊåÅÊàñÊèêÂçáÈöêÁßÅ‰øùÊä§ÂíåÊ®°ÂûãÁ≤æÂ∫¶„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã(LLM)ÁªèÂ∏∏‰ºöËÆ∞ÂøÜÊïèÊÑüÊàñ‰∏™‰∫∫‰ø°ÊÅØÔºåÂºïÂèë‰∫Ü‰∏•ÈáçÁöÑÈöêÁßÅÈóÆÈ¢ò„ÄÇÁé∞ÊúâÁöÑÂ∑ÆÂàÜÈöêÁßÅÈöèÊú∫Ê¢ØÂ∫¶‰∏ãÈôç(DPSGD)Âèò‰ΩìÂØπÊØè‰∏™Ê¢ØÂ∫¶Ê≠•È™§Ê≥®ÂÖ•Áªü‰∏ÄÂô™Â£∞ÔºåÊòæËëóÂª∂Èïø‰∫ÜËÆ≠ÁªÉÊó∂Èó¥Âπ∂Èôç‰Ωé‰∫ÜÊ®°ÂûãÁ≤æÂ∫¶„ÄÇÊú¨ÊñáÊèêÂá∫ÔºåÂ∞ÜÂô™Â£∞‰∏ªË¶ÅÈõÜ‰∏≠Âú®‰∏éÊïèÊÑütokenÁõ∏ÂÖ≥ÁöÑÊ¢ØÂ∫¶‰∏äÔºåÂèØ‰ª•ÊòæËëóÂáèÂ∞ëDPËÆ≠ÁªÉÊó∂Èó¥ÔºåÂä†Âº∫ÂØπÊïèÊÑü‰ø°ÊÅØÁöÑ‰øùÊä§ÔºåÂêåÊó∂‰øùÊåÅÊ®°ÂûãÂú®ÈùûÊïèÊÑüÊï∞ÊçÆ‰∏äÁöÑÊÄßËÉΩ„ÄÇ‰∏∫Ê≠§ÔºåÊú¨ÊñáÊèêÂá∫Ëá™ÈÄÇÂ∫îTokenÂä†ÊùÉÂ∑ÆÂàÜÈöêÁßÅ(ATDP)ÔºåÂÆÉÊòØvanilla DP-SGDÁöÑÊîπËøõÁâàÊú¨ÔºåËá™ÈÄÇÂ∫îÂú∞‰∏∫ÊïèÊÑüÂíåÈùûÊïèÊÑütokenÂàÜÈÖç‰∏çÂêåÁöÑÊ¢ØÂ∫¶ÊùÉÈáç„ÄÇÈÄöËøáÂú®ËÆ≠ÁªÉÊó©ÊúüÈò∂ÊÆµÈááÁî®Êõ¥Â§ßÁöÑÂô™Â£∞Â∞∫Â∫¶ÔºåATDPËøÖÈÄüÊâ∞‰π±‰∫ÜÂØπÊïèÊÑüÂÜÖÂÆπÁöÑËÆ∞ÂøÜ„ÄÇÂõ†Ê≠§ÔºåATDP‰ªÖÈúÄË¶ÅÂú®Ê†áÂáÜÂæÆË∞ÉÂêéËøõË°åÂá†‰∏™È¢ùÂ§ñÁöÑËΩªÈáèÁ∫ßÂêéÂ§ÑÁêÜepochÔºå‰∏ªË¶ÅÈíàÂØπ‰∏éÊïèÊÑütokenÂØπÂ∫îÁöÑÂèÇÊï∞Ê≥®ÂÖ•ÊúâÈíàÂØπÊÄßÁöÑÂô™Â£∞Ôºå‰ªéËÄåÊúÄÂ§ßÈôêÂ∫¶Âú∞ÂáèÂ∞ëÂØπÊ®°ÂûãÈÄöÁî®ËÉΩÂäõÁöÑÂΩ±Âìç„ÄÇATDPÂèØ‰ª•Êó†ÁºùÈõÜÊàêÂà∞‰ªª‰ΩïÁé∞ÊúâÁöÑÂü∫‰∫éDPÁöÑÂæÆË∞Épipeline‰∏≠ÔºåÊàñËÄÖÁõ¥Êé•Â∫îÁî®‰∫éÈùûÁßÅÊúâÊ®°ÂûãÔºå‰Ωú‰∏∫‰∏ÄÁßçÂø´ÈÄüÁöÑÈöêÁßÅÂ¢ûÂº∫Êé™ÊñΩ„ÄÇÊ≠§Â§ñÔºåÁªìÂêàÂàùÂßãÁöÑredactedÂæÆË∞ÉÈò∂ÊÆµÔºåATDPÂΩ¢Êàê‰∫Ü‰∏Ä‰∏™ÁÆÄÂåñÁöÑDP pipelineÔºåÂÆûÁé∞‰∫Ü‰∏éÊúÄÂÖàËøõÁöÑDP-SGDÊñπÊ≥ïÁõ∏ÂΩìÁöÑcanary‰øùÊä§ÔºåÊòæËëóÈôç‰Ωé‰∫ÜDPÂæÆË∞ÉÁöÑËÆ°ÁÆóÂºÄÈîÄÔºåÂ∞ÜËÆ≠ÁªÉÊó∂Èó¥Áº©Áü≠‰∫ÜÁ∫¶90%ÔºåÂêåÊó∂ÂÆûÁé∞‰∫ÜÁõ∏ÂΩìÊàñÊõ¥‰ºòË∂äÁöÑÈöêÁßÅ‰øùÊä§ÂíåÊúÄÂ∞èÁöÑÁ≤æÂ∫¶‰∏ãÈôç„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂÆπÊòìËÆ∞ÂøÜËÆ≠ÁªÉÊï∞ÊçÆ‰∏≠ÁöÑÊïèÊÑü‰ø°ÊÅØÔºåÁõ¥Êé•ÂèëÂ∏ÉÂ≠òÂú®ÈöêÁßÅÊ≥ÑÈú≤È£éÈô©„ÄÇ‰º†ÁªüÁöÑÂ∑ÆÂàÜÈöêÁßÅËÆ≠ÁªÉÊñπÊ≥ïÔºåÂ¶ÇDPSGDÔºåÈÄöËøáÂú®Ê¢ØÂ∫¶‰∏≠Ê∑ªÂä†Âô™Â£∞Êù•‰øùÊä§ÈöêÁßÅÔºå‰ΩÜËøôÁßçÊñπÊ≥ï‰ºöÊòæËëóÂ¢ûÂä†ËÆ≠ÁªÉÊó∂Èó¥ÂíåÈôç‰ΩéÊ®°ÂûãÊÄßËÉΩ„ÄÇÁé∞ÊúâÁöÑÊñπÊ≥ïÊó†Ê≥ïÂú®ÈöêÁßÅ‰øùÊä§„ÄÅÊ®°ÂûãÊÄßËÉΩÂíåËÆ≠ÁªÉÊïàÁéá‰πãÈó¥ÂèñÂæóËâØÂ•ΩÁöÑÂπ≥Ë°°„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂπ∂ÈùûÊâÄÊúâtokenÈÉΩÈúÄË¶ÅÂêåÁ≠âÁ®ãÂ∫¶ÁöÑÈöêÁßÅ‰øùÊä§„ÄÇÊïèÊÑütokenÔºà‰æãÂ¶ÇÔºåÂåÖÂê´‰∏™‰∫∫‰ø°ÊÅØÁöÑtokenÔºâÊØîÈùûÊïèÊÑütokenÊõ¥ÈúÄË¶Å‰øùÊä§„ÄÇÂõ†Ê≠§ÔºåÂèØ‰ª•Ëá™ÈÄÇÂ∫îÂú∞Ë∞ÉÊï¥‰∏çÂêåtokenÁöÑÊ¢ØÂ∫¶ÊùÉÈáçÔºåÂØπÊïèÊÑütokenÁöÑÊ¢ØÂ∫¶ÊñΩÂä†Êõ¥Â§ßÁöÑÂô™Â£∞Ôºå‰ªéËÄåÂú®‰øùËØÅÈöêÁßÅÁöÑÂêåÊó∂ÔºåÂáèÂ∞ëÂØπÊ®°ÂûãÊï¥‰ΩìÊÄßËÉΩÁöÑÂΩ±ÂìçÔºåÂπ∂Âä†ÈÄüËÆ≠ÁªÉËøáÁ®ã„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöATDPÊñπÊ≥ï‰∏ªË¶ÅÂåÖÂê´‰ª•‰∏ãÂá†‰∏™Èò∂ÊÆµÔºö1) Ê†áÂáÜÊ®°ÂûãÂæÆË∞ÉÔºöÈ¶ñÂÖà‰ΩøÁî®Ê†áÂáÜÊñπÊ≥ïÂØπÊ®°ÂûãËøõË°åÂæÆË∞ÉÔºå‰ΩøÂÖ∂ÂÖ∑Â§áÂü∫Êú¨ÁöÑËØ≠Ë®ÄËÉΩÂäõ„ÄÇ2) ÊïèÊÑü‰ø°ÊÅØÊì¶Èô§ÔºàÂèØÈÄâÔºâÔºö‰ΩøÁî®redacted fine-tuningÊñπÊ≥ïÔºåÂàùÊ≠•ÂáèÂ∞ëÊ®°ÂûãÂØπÊïèÊÑü‰ø°ÊÅØÁöÑËÆ∞ÂøÜ„ÄÇ3) Ëá™ÈÄÇÂ∫îTokenÂä†ÊùÉÔºöÊ†πÊçÆtokenÁöÑÊïèÊÑüÁ®ãÂ∫¶ÔºåËá™ÈÄÇÂ∫îÂú∞Ë∞ÉÊï¥Ê¢ØÂ∫¶ÊùÉÈáç„ÄÇÊïèÊÑütokenÁöÑÊ¢ØÂ∫¶ÊùÉÈáçËæÉÈ´òÔºåÈùûÊïèÊÑütokenÁöÑÊ¢ØÂ∫¶ÊùÉÈáçËæÉ‰Ωé„ÄÇ4) Â∑ÆÂàÜÈöêÁßÅËÆ≠ÁªÉÔºö‰ΩøÁî®Ë∞ÉÊï¥ÂêéÁöÑÊ¢ØÂ∫¶ÊùÉÈáçËøõË°åDPSGDËÆ≠ÁªÉÔºåÂØπÊ¢ØÂ∫¶Ê∑ªÂä†Âô™Â£∞‰ª•‰øùÊä§ÈöêÁßÅ„ÄÇ5) ËΩªÈáèÁ∫ßÂêéÂ§ÑÁêÜÔºöÂú®DPËÆ≠ÁªÉÂêéÔºåËøõË°åÂ∞ëÈáèÁöÑÈ¢ùÂ§ñËÆ≠ÁªÉÔºå‰ª•Ëøõ‰∏ÄÊ≠•ÊèêÂçáÊ®°ÂûãÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöATDPÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éËá™ÈÄÇÂ∫îÁöÑtokenÂä†ÊùÉÊú∫Âà∂„ÄÇÂÆÉËÉΩÂ§üÊ†πÊçÆtokenÁöÑÊïèÊÑüÁ®ãÂ∫¶Âä®ÊÄÅÂú∞Ë∞ÉÊï¥Ê¢ØÂ∫¶ÊùÉÈáçÔºå‰ªéËÄåÂÆûÁé∞Êõ¥Á≤æÁªÜÂåñÁöÑÈöêÁßÅ‰øùÊä§„ÄÇ‰∏é‰º†ÁªüÁöÑDPSGDÊñπÊ≥ïÁõ∏ÊØîÔºåATDPËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞Âà©Áî®Âô™Â£∞È¢ÑÁÆóÔºåÂú®‰øùËØÅÈöêÁßÅÁöÑÂêåÊó∂ÔºåÂáèÂ∞ëÂØπÊ®°ÂûãÊÄßËÉΩÁöÑÂΩ±Âìç„ÄÇÊ≠§Â§ñÔºåATDPÂèØ‰ª•‰∏éÁé∞ÊúâÁöÑDPËÆ≠ÁªÉpipelineÊó†ÁºùÈõÜÊàêÔºåÂÖ∑ÊúâËâØÂ•ΩÁöÑÈÄöÁî®ÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöATDPÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) TokenÊïèÊÑüÂ∫¶ËØÑ‰º∞ÔºöÈúÄË¶Å‰∏ÄÁßçÊñπÊ≥ïÊù•ËØÑ‰º∞tokenÁöÑÊïèÊÑüÁ®ãÂ∫¶„ÄÇËÆ∫Êñá‰∏≠ÂèØËÉΩ‰ΩøÁî®‰∫ÜÂêØÂèëÂºèÊñπÊ≥ïÊàñÈ¢ÑËÆ≠ÁªÉÁöÑÊïèÊÑüÂ∫¶Ê£ÄÊµãÊ®°Âûã„ÄÇ2) Ê¢ØÂ∫¶ÊùÉÈáçË∞ÉÊï¥Á≠ñÁï•ÔºöÈúÄË¶ÅËÆæËÆ°‰∏ÄÁßçÁ≠ñÁï•ÔºåÊ†πÊçÆtokenÁöÑÊïèÊÑüÁ®ãÂ∫¶Êù•Ë∞ÉÊï¥Ê¢ØÂ∫¶ÊùÉÈáç„ÄÇ‰æãÂ¶ÇÔºåÂèØ‰ª•‰ΩøÁî®‰∏Ä‰∏™ÂáΩÊï∞Â∞ÜtokenÁöÑÊïèÊÑüÂ∫¶Êò†Â∞ÑÂà∞Ê¢ØÂ∫¶ÊùÉÈáç„ÄÇ3) Âô™Â£∞Â∞∫Â∫¶Ë∞ÉÊï¥ÔºöÂú®ËÆ≠ÁªÉÁöÑ‰∏çÂêåÈò∂ÊÆµÔºåÂèØ‰ª•Ë∞ÉÊï¥Âô™Â£∞ÁöÑÂ∞∫Â∫¶„ÄÇ‰æãÂ¶ÇÔºåÂú®ËÆ≠ÁªÉÂàùÊúüÂèØ‰ª•‰ΩøÁî®ËæÉÂ§ßÁöÑÂô™Â£∞Â∞∫Â∫¶Ôºå‰ª•Âø´ÈÄüÊâ∞‰π±Ê®°ÂûãÂØπÊïèÊÑü‰ø°ÊÅØÁöÑËÆ∞ÂøÜ„ÄÇ4) ÊçüÂ§±ÂáΩÊï∞ÔºöÂèØ‰ª•‰ΩøÁî®Ê†áÂáÜÁöÑ‰∫§ÂèâÁÜµÊçüÂ§±ÂáΩÊï∞ÔºåÊàñËÄÖÊ†πÊçÆÂÖ∑‰Ωì‰ªªÂä°ËøõË°åË∞ÉÊï¥„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ATDPÂú®ÂÆûÈ™å‰∏≠Ë°®Áé∞Âá∫ÊòæËëóÁöÑ‰ºòÂäø„ÄÇ‰∏é‰º†ÁªüÁöÑDPSGDÊñπÊ≥ïÁõ∏ÊØîÔºåATDPËÉΩÂ§üÂ∞ÜDPËÆ≠ÁªÉÊó∂Èó¥Áº©Áü≠Á∫¶90%ÔºåÂêåÊó∂ÂÆûÁé∞Áõ∏ÂΩìÊàñÊõ¥‰ºòË∂äÁöÑÈöêÁßÅ‰øùÊä§ÂíåÊúÄÂ∞èÁöÑÁ≤æÂ∫¶‰∏ãÈôç„ÄÇÂú®canary‰øùÊä§ÊñπÈù¢ÔºåATDPËææÂà∞‰∫Ü‰∏éÊúÄÂÖàËøõÁöÑDP-SGDÊñπÊ≥ïÁõ∏ÂΩìÁöÑÊ∞¥Âπ≥„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåATDPÊòØ‰∏ÄÁßçÈ´òÊïà‰∏îÊúâÊïàÁöÑLLMÈöêÁßÅ‰øùÊä§ÊñπÊ≥ï„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ATDPÂèØÂ∫îÁî®‰∫éÂêÑÁßçÈúÄË¶Å‰øùÊä§Áî®Êà∑ÈöêÁßÅÁöÑLLMÂ∫îÁî®Âú∫ÊôØÔºå‰æãÂ¶ÇÔºöÂåªÁñóÂÅ•Â∫∑È¢ÜÂüüÁöÑÁóÖÂéÜÂàÜÊûê„ÄÅÈáëËûçÈ¢ÜÂüüÁöÑ‰∫§ÊòìËÆ∞ÂΩïÂàÜÊûê„ÄÅÊïôËÇ≤È¢ÜÂüüÁöÑ‰∏™ÊÄßÂåñËæÖÂØºÁ≠â„ÄÇËØ•ÊñπÊ≥ïËÉΩÂ§üÊúâÊïàÈò≤Ê≠¢LLMÊ≥ÑÈú≤Áî®Êà∑ÊïèÊÑü‰ø°ÊÅØÔºåÂêåÊó∂‰øùÊåÅÊ®°ÂûãÁöÑÂèØÁî®ÊÄßÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖÂ∫îÁî®‰ª∑ÂÄºÂíåÂπøÈòîÁöÑÂ∫îÁî®ÂâçÊôØ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Large language models (LLMs) frequently memorize sensitive or personal information, raising significant privacy concerns. Existing variants of differential privacy stochastic gradient descent (DPSGD) inject uniform noise into every gradient step, significantly extending training time and reducing model accuracy. We propose that concentrating noise primarily on gradients associated with sensitive tokens can substantially decrease DP training time, strengthen the protection of sensitive information, and simultaneously preserve the model's performance on non-sensitive data. We operationalize this insight through Adaptive Token-Weighted Differential Privacy (ATDP), a modification of vanilla DP-SGD that adaptively assigns different gradient weights to sensitive and non-sensitive tokens. By employing a larger noise scale at the early stage of training, ATDP rapidly disrupts memorization of sensitive content. As a result, ATDP only requires a few additional epochs of lightweight post-processing following standard fine-tuning, injecting targeted noise primarily on parameters corresponding to sensitive tokens, thus minimally affecting the model's general capabilities. ATDP can be seamlessly integrated into any existing DP-based fine-tuning pipeline or directly applied to non-private models as a fast privacy-enhancing measure. Additionally, combined with an initial redacted fine-tuning phase, ATDP forms a streamlined DP pipeline that achieves comparable canary protection to state-of-the-art DP-SGD methods, significantly reduces the computational overhead of DP fine-tuning, shortening training time by approximately 90 percent, while achieving comparable or superior privacy protection and minimal accuracy degradation.

