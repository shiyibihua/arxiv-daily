---
layout: default
title: Trust Region Reward Optimization and Proximal Inverse Reward Optimization Algorithm
---

# Trust Region Reward Optimization and Proximal Inverse Reward Optimization Algorithm

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.23135" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.23135v3</a>
  <a href="https://arxiv.org/pdf/2509.23135.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.23135v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.23135v3', 'Trust Region Reward Optimization and Proximal Inverse Reward Optimization Algorithm')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Yang Chen, Menglin Zou, Jiaqi Zhang, Yitan Zhang, Junyi Yang, Gael Gendron, Libo Zhang, Jiamou Liu, Michael J. Witbrock

**ÂàÜÁ±ª**: cs.LG, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-27 (Êõ¥Êñ∞: 2025-10-13)

**Â§áÊ≥®**: Accepted to NeurIPS 2025. Title used at submission and review: PIRO: Toward Stable Reward Learning for Inverse RL via Monotonic Policy Divergence Reduction

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫‰ø°ËµñÂüüÂ•ñÂä±‰ºòÂåñ(TRRO)Ê°ÜÊû∂ÔºåËß£ÂÜ≥ÈÄÜÂº∫ÂåñÂ≠¶‰π†‰∏≠rewardÂíåpolicyËÅîÂêàÂ≠¶‰π†ÁöÑ‰∏çÁ®≥ÂÆöÊÄßÈóÆÈ¢ò„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `ÈÄÜÂº∫ÂåñÂ≠¶‰π†` `Â•ñÂä±ÂáΩÊï∞Â≠¶‰π†` `‰ø°ËµñÂüü‰ºòÂåñ` `Á≠ñÁï•Ê®°‰ªø` `Êú∫Âô®‰∫∫ÊéßÂà∂`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÈÄÜÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÂ∏∏‰ΩøÁî®ÂØπÊäóËÆ≠ÁªÉÔºåÂØºËá¥Â•ñÂä±ÂíåÁ≠ñÁï•‰ºòÂåñ‰∫§ÊõøËøõË°åÔºåËÆ≠ÁªÉËøáÁ®ã‰∏çÁ®≥ÂÆö„ÄÇ
2. ËÆ∫ÊñáÊèêÂá∫‰ø°ËµñÂüüÂ•ñÂä±‰ºòÂåñ(TRRO)Ê°ÜÊû∂ÔºåÈÄöËøáÊúÄÂ§ßÂåñ‰∏ìÂÆ∂Ë°å‰∏∫ÁöÑÂèØËÉΩÊÄßÊù•ËÅîÂêàÂ≠¶‰π†Â•ñÂä±ÂíåÁ≠ñÁï•Ôºå‰øùËØÅÂçïË∞ÉÊîπËøõ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåÊèêÂá∫ÁöÑËøëÁ´ØÈÄÜÂ•ñÂä±‰ºòÂåñ(PIRO)ÁÆóÊ≥ïÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ËææÂà∞ÊàñË∂ÖËøá‰∫ÜÁé∞ÊúâÊúÄ‰Ω≥ÊñπÊ≥ïÔºåÂπ∂ÂÖ∑ÊúâÈ´òÊ†∑Êú¨ÊïàÁéá„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÈÄÜÂº∫ÂåñÂ≠¶‰π†(IRL)Êó®Âú®Â≠¶‰π†‰∏Ä‰∏™Â•ñÂä±ÂáΩÊï∞Êù•Ëß£Èáä‰∏ìÂÆ∂ÊºîÁ§∫„ÄÇÁé∞ÊúâÁöÑIRLÊñπÊ≥ïÈÄöÂ∏∏ÈááÁî®ÂØπÊäó(minimax)ÂÖ¨ÂºèÔºåÂú®Â•ñÂä±ÂíåÁ≠ñÁï•‰ºòÂåñ‰πãÈó¥‰∫§ÊõøËøõË°åÔºåËøôÂæÄÂæÄÂØºËá¥ËÆ≠ÁªÉ‰∏çÁ®≥ÂÆö„ÄÇÊúÄËøëÁöÑÈùûÂØπÊäóIRLÊñπÊ≥ïÈÄöËøáÂü∫‰∫éËÉΩÈáèÁöÑÂÖ¨ÂºèËÅîÂêàÂ≠¶‰π†Â•ñÂä±ÂíåÁ≠ñÁï•Ôºå‰ªéËÄåÊèêÈ´ò‰∫ÜÁ®≥ÂÆöÊÄßÔºå‰ΩÜÁº∫‰πèÂΩ¢ÂºèÂåñÁöÑ‰øùËØÅ„ÄÇÊú¨ÊñáÂº•Ë°•‰∫ÜËøô‰∏ÄÂ∑ÆË∑ù„ÄÇÈ¶ñÂÖàÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏Ä‰∏™Áªü‰∏ÄÁöÑËßÜËßíÔºåË°®ÊòéÂÖ∏ÂûãÁöÑÈùûÂØπÊäóÊñπÊ≥ïÊòæÂºèÊàñÈöêÂºèÂú∞ÊúÄÂ§ßÂåñ‰∫Ü‰∏ìÂÆ∂Ë°å‰∏∫ÁöÑÂèØËÉΩÊÄßÔºåËøôÁ≠â‰ª∑‰∫éÊúÄÂ∞èÂåñÊúüÊúõÂõûÊä•Â∑ÆË∑ù„ÄÇËøô‰∏ÄÊ¥ûÂØüÂºïÂá∫‰∫ÜÊú¨ÊñáÁöÑ‰∏ªË¶ÅË¥°ÁåÆÔºö‰ø°ËµñÂüüÂ•ñÂä±‰ºòÂåñ(TRRO)ÔºåËøôÊòØ‰∏Ä‰∏™ÈÄöËøáMinorization-MaximizationËøáÁ®ã‰øùËØÅËøôÁßçÂèØËÉΩÊÄßÂçïË∞ÉÊîπËøõÁöÑÊ°ÜÊû∂„ÄÇÊú¨ÊñáÂ∞ÜTRROÂÆû‰æãÂåñ‰∏∫ËøëÁ´ØÈÄÜÂ•ñÂä±‰ºòÂåñ(PIRO)ÔºåËøôÊòØ‰∏ÄÁßçÂÆûÁî®‰∏îÁ®≥ÂÆöÁöÑIRLÁÆóÊ≥ï„ÄÇÂú®ÁêÜËÆ∫‰∏äÔºåTRRO‰∏∫Ê≠£ÂêëÂº∫ÂåñÂ≠¶‰π†‰∏≠ÁöÑ‰ø°ËµñÂüüÁ≠ñÁï•‰ºòÂåñ(TRPO)ÁöÑÁ®≥ÂÆöÊÄß‰øùËØÅÊèê‰æõ‰∫ÜIRLÂØπÂ∫î„ÄÇÂú®ÁªèÈ™å‰∏äÔºåPIROÂú®Â•ñÂä±ÊÅ¢Â§ç„ÄÅÁ≠ñÁï•Ê®°‰ªøÊñπÈù¢‰∏éÊúÄÂÖàËøõÁöÑÂü∫Á∫øÊñπÊ≥ïÁõ∏ÂåπÈÖçÊàñË∂ÖËøáÔºåÂπ∂Âú®MuJoCoÂíåGym-RoboticsÂü∫ÂáÜÊµãËØï‰ª•ÂèäÁúüÂÆûÁöÑÂä®Áâ©Ë°å‰∏∫Âª∫Ê®°‰ªªÂä°‰∏≠ÂÖ∑ÊúâÂæàÈ´òÁöÑÊ†∑Êú¨ÊïàÁéá„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÈÄÜÂº∫ÂåñÂ≠¶‰π†Êó®Âú®‰ªé‰∏ìÂÆ∂ÊºîÁ§∫‰∏≠Â≠¶‰π†Â•ñÂä±ÂáΩÊï∞ÔºåÁé∞ÊúâÊñπÊ≥ïÂ¶ÇÂØπÊäóÈÄÜÂº∫ÂåñÂ≠¶‰π†(Adversarial IRL)ÈÄöËøáminimaxÊ°ÜÊû∂‰∫§Êõø‰ºòÂåñÂ•ñÂä±ÂáΩÊï∞ÂíåÁ≠ñÁï•ÔºåÂØºËá¥ËÆ≠ÁªÉ‰∏çÁ®≥ÂÆöÔºåÈöæ‰ª•Êî∂Êïõ„ÄÇÈùûÂØπÊäóÊñπÊ≥ïËôΩÁÑ∂ÊèêÈ´ò‰∫ÜÁ®≥ÂÆöÊÄßÔºå‰ΩÜÁº∫‰πèÁêÜËÆ∫‰øùËØÅÔºåÊó†Ê≥ïÁ°Æ‰øùÂ≠¶‰π†ËøáÁ®ãÁöÑÂçïË∞ÉÊîπËøõ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞ÜÈùûÂØπÊäóIRLÊñπÊ≥ïÁªü‰∏ÄÂà∞‰∏Ä‰∏™ÊúÄÂ§ßÂåñ‰∏ìÂÆ∂Ë°å‰∏∫ÂèØËÉΩÊÄßÁöÑÊ°ÜÊû∂‰∏ãÔºåÂπ∂ËØÅÊòéËøôÁ≠â‰ª∑‰∫éÊúÄÂ∞èÂåñÊúüÊúõÂõûÊä•Â∑ÆË∑ù„ÄÇÂü∫‰∫éÊ≠§ÔºåÊèêÂá∫‰ø°ËµñÂüüÂ•ñÂä±‰ºòÂåñ(TRRO)Ê°ÜÊû∂ÔºåÈÄöËøáMinorization-Maximization (MM)ËøáÁ®ãÔºå‰øùËØÅ‰∏ìÂÆ∂Ë°å‰∏∫ÂèØËÉΩÊÄßÁöÑÂçïË∞ÉÊîπËøõÔºå‰ªéËÄåÁ®≥ÂÆöÂú∞Â≠¶‰π†Â•ñÂä±ÂáΩÊï∞„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöTRROÊ°ÜÊû∂ÂåÖÂê´‰ª•‰∏ã‰∏ªË¶ÅÊ≠•È™§Ôºö
1.  **Â•ñÂä±ÂáΩÊï∞Êõ¥Êñ∞**Ôºö‰ΩøÁî®MMÁÆóÊ≥ïÔºåÂú®‰ø°ËµñÂüüÂÜÖÊõ¥Êñ∞Â•ñÂä±ÂáΩÊï∞Ôºå‰øùËØÅ‰∏ìÂÆ∂Ë°å‰∏∫ÂèØËÉΩÊÄßÂ¢ûÂä†„ÄÇ
2.  **Á≠ñÁï•‰ºòÂåñ**ÔºöÊ†πÊçÆÊõ¥Êñ∞ÂêéÁöÑÂ•ñÂä±ÂáΩÊï∞Ôºå‰ºòÂåñÁ≠ñÁï•Ôºå‰ΩøÂÖ∂ËÉΩÂ§üÊõ¥Â•ΩÂú∞Ê®°‰ªø‰∏ìÂÆ∂Ë°å‰∏∫„ÄÇ
3.  **‰ø°ËµñÂüüÁ∫¶Êùü**ÔºöÈÄöËøá‰ø°ËµñÂüüÁ∫¶ÊùüÔºåÈôêÂà∂Â•ñÂä±ÂáΩÊï∞ÁöÑÊõ¥Êñ∞ÂπÖÂ∫¶ÔºåÈò≤Ê≠¢ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÁöÑÂâßÁÉàÂèòÂåñÔºåÊèêÈ´òÁ®≥ÂÆöÊÄß„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöTRROÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÔºö
1.  **Áªü‰∏ÄËßÜËßí**ÔºöÂ∞ÜÁé∞ÊúâÈùûÂØπÊäóIRLÊñπÊ≥ïÁªü‰∏ÄÂà∞ÊúÄÂ§ßÂåñ‰∏ìÂÆ∂Ë°å‰∏∫ÂèØËÉΩÊÄßÁöÑÊ°ÜÊû∂‰∏ã„ÄÇ
2.  **ÂçïË∞ÉÊîπËøõ‰øùËØÅ**ÔºöÈÄöËøáMMËøáÁ®ãÔºå‰øùËØÅ‰∏ìÂÆ∂Ë°å‰∏∫ÂèØËÉΩÊÄßÁöÑÂçïË∞ÉÊîπËøõÔºåÊèê‰æõÁêÜËÆ∫‰∏äÁöÑÁ®≥ÂÆöÊÄß‰øùËØÅ„ÄÇ
3.  **‰ø°ËµñÂüüÁ∫¶Êùü**ÔºöÂºïÂÖ•‰ø°ËµñÂüüÁ∫¶ÊùüÔºåÈôêÂà∂Â•ñÂä±ÂáΩÊï∞ÁöÑÊõ¥Êñ∞ÂπÖÂ∫¶ÔºåËøõ‰∏ÄÊ≠•ÊèêÈ´òËÆ≠ÁªÉÁ®≥ÂÆöÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöTRROÊ°ÜÊû∂ÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö
1.  **MMÁÆóÊ≥ï**Ôºö‰ΩøÁî®MMÁÆóÊ≥ïËøõË°åÂ•ñÂä±ÂáΩÊï∞Êõ¥Êñ∞ÔºåÂÖ∑‰ΩìÂΩ¢ÂºèÈúÄË¶ÅÊ†πÊçÆÂÆûÈôÖÈóÆÈ¢òËøõË°åÈÄâÊã©„ÄÇ
2.  **‰ø°ËµñÂüüÂçäÂæÑ**Ôºö‰ø°ËµñÂüüÂçäÂæÑÁöÑËÆæÁΩÆÈúÄË¶ÅÊ†πÊçÆÂÖ∑‰ΩìÈóÆÈ¢òËøõË°åË∞ÉÊï¥ÔºåËøáÂ∞è‰ºöÂØºËá¥Êî∂ÊïõÈÄüÂ∫¶ÊÖ¢ÔºåËøáÂ§ß‰ºöÂØºËá¥ËÆ≠ÁªÉ‰∏çÁ®≥ÂÆö„ÄÇ
3.  **ËøëÁ´ØÁ≠ñÁï•‰ºòÂåñ(PPO)**ÔºöÂèØ‰ª•‰ΩøÁî®PPOÁ≠âÁÆóÊ≥ïËøõË°åÁ≠ñÁï•‰ºòÂåñÔºå‰ª•ÊèêÈ´òÊ†∑Êú¨ÊïàÁéáÂíåÁ®≥ÂÆöÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊèêÂá∫ÁöÑPIROÁÆóÊ≥ïÂú®MuJoCoÂíåGym-RoboticsÂü∫ÂáÜÊµãËØï‰∏≠ÔºåÂú®Â•ñÂä±ÊÅ¢Â§çÂíåÁ≠ñÁï•Ê®°‰ªøÊñπÈù¢‰∏éÊúÄÂÖàËøõÁöÑÂü∫Á∫øÊñπÊ≥ïÁõ∏ÂåπÈÖçÊàñË∂ÖËøáÔºåÂπ∂‰∏îÂÖ∑ÊúâÊõ¥È´òÁöÑÊ†∑Êú¨ÊïàÁéá„ÄÇÊ≠§Â§ñÔºåPIROËøòÂú®ÁúüÂÆûÁöÑÂä®Áâ©Ë°å‰∏∫Âª∫Ê®°‰ªªÂä°‰∏≠ÂèñÂæó‰∫ÜËâØÂ•ΩÁöÑÊïàÊûúÔºåÈ™åËØÅ‰∫ÜÂÖ∂Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÊΩúÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÊú∫Âô®‰∫∫ÊéßÂà∂„ÄÅËá™Âä®È©æÈ©∂„ÄÅÊ∏∏ÊàèAIÁ≠âÈ¢ÜÂüü„ÄÇÈÄöËøáÊ®°‰ªø‰∏ìÂÆ∂Ë°å‰∏∫ÔºåÂèØ‰ª•ËÆ≠ÁªÉÂá∫È´òÊÄßËÉΩÁöÑÊô∫ËÉΩ‰ΩìÔºåËß£ÂÜ≥Â§çÊùÇÁéØÂ¢É‰∏ãÁöÑÂÜ≥Á≠ñÈóÆÈ¢ò„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ïËøòÂèØ‰ª•Áî®‰∫éÂä®Áâ©Ë°å‰∏∫Âª∫Ê®°ÔºåÂàÜÊûêÂä®Áâ©ÁöÑÂ≠¶‰π†Êú∫Âà∂Ôºå‰∏∫ÁîüÁâ©Â≠¶Á†îÁ©∂Êèê‰æõÊñ∞ÁöÑÂ∑•ÂÖ∑„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Inverse Reinforcement Learning (IRL) learns a reward function to explain expert demonstrations. Modern IRL methods often use the adversarial (minimax) formulation that alternates between reward and policy optimization, which often lead to unstable training. Recent non-adversarial IRL approaches improve stability by jointly learning reward and policy via energy-based formulations but lack formal guarantees. This work bridges this gap. We first present a unified view showing canonical non-adversarial methods explicitly or implicitly maximize the likelihood of expert behavior, which is equivalent to minimizing the expected return gap. This insight leads to our main contribution: Trust Region Reward Optimization (TRRO), a framework that guarantees monotonic improvement in this likelihood via a Minorization-Maximization process. We instantiate TRRO into Proximal Inverse Reward Optimization (PIRO), a practical and stable IRL algorithm. Theoretically, TRRO provides the IRL counterpart to the stability guarantees of Trust Region Policy Optimization (TRPO) in forward RL. Empirically, PIRO matches or surpasses state-of-the-art baselines in reward recovery, policy imitation with high sample efficiency on MuJoCo and Gym-Robotics benchmarks and a real-world animal behavior modeling task.

