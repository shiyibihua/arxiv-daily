---
layout: default
title: SDQ-LLM: Sigma-Delta Quantization for 1-bit LLMs of any size
---

# SDQ-LLM: Sigma-Delta Quantization for 1-bit LLMs of any size

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.03275" class="toolbar-btn" target="_blank">üìÑ arXiv: 2510.03275v1</a>
  <a href="https://arxiv.org/pdf/2510.03275.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.03275v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.03275v1', 'SDQ-LLM: Sigma-Delta Quantization for 1-bit LLMs of any size')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Junhao Xia, Ming Zhao, Limin Xiao, Xiujun Zhang

**ÂàÜÁ±ª**: cs.LG, cs.AI, cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-27

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/Dreamlittlecat/LLM-Quant-Factory)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**SDQ-LLMÔºöÈù¢Âêë‰ªªÊÑèËßÑÊ®°LLMÁöÑSigma-DeltaÈáèÂåñÔºåÂÆûÁé∞È´òÊïà1ÊØîÁâπÈáèÂåñ„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§ßËØ≠Ë®ÄÊ®°Âûã` `ÈáèÂåñ` `‰ΩéÊØîÁâπÈáèÂåñ` `Sigma-DeltaÈáèÂåñ` `Ê®°ÂûãÂéãÁº©` `ËøáÈááÊ†∑` `Ê®°Âûã‰ºòÂåñ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâLLMÈù¢‰∏¥ËÆ°ÁÆóÂíåÂÜÖÂ≠òÊåëÊàòÔºåÊûÅ‰ΩéÊØîÁâπÈáèÂåñÊòØÈ´òÊïàÈÉ®ÁΩ≤ÁöÑÂÖ≥ÈîÆÔºå‰ΩÜ‰ºö‰∏•ÈáçÂΩ±ÂìçÊ®°ÂûãÁ≤æÂ∫¶„ÄÇ
2. SDQ-LLMÈÄöËøáSigma-DeltaÈáèÂåñÂíåÂèØË∞ÉËøáÈááÊ†∑ÁéáÔºåÂ∞ÜÈ´òÁ≤æÂ∫¶ÊùÉÈáçÁºñÁ†Å‰∏∫1ÊØîÁâπÊàñ1.58ÊØîÁâπÔºåÂπ∂Áî®Âä†Ê≥ï‰ª£Êõø‰πòÊ≥ï„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåSDQ-LLMÂç≥‰ΩøÂú®‰ΩéËøáÈááÊ†∑Áéá‰∏ãÔºå‰πüËÉΩÂú®OPTÂíåLLaMAÊ®°Âûã‰∏äÂÆûÁé∞È´òÊïà‰∏îÈ´òÁ≤æÂ∫¶ÁöÑÊÄßËÉΩ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫SDQ-LLMÔºå‰∏ÄÁßçÈíàÂØπ‰ªªÊÑèËßÑÊ®°Â§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑSigma-DeltaÈáèÂåñÊ°ÜÊû∂ÔºåÊó®Âú®ÂÆûÁé∞ÊûÅ‰ΩéÊØîÁâπÈáèÂåñÔºåÂêåÊó∂‰øùÁïôËØ≠Ë®ÄÊé®ÁêÜËÉΩÂäõ„ÄÇSDQ-LLMÁöÑÊòæËëóÁâπÁÇπÊòØÂèØËøûÁª≠Ë∞ÉËäÇÁöÑËøáÈááÊ†∑ÁéáÔºàOSRÔºâÔºåÈÄöËøáÈÄâÊã©ÂàÜÊï∞OSRÔºà‰æãÂ¶Ç2.5ÂÄçÔºâÂä®ÊÄÅÈÄÇÂ∫îÂÜÖÂ≠òÊàñVRAMÁ∫¶ÊùüÔºå‰ªéËÄåÂú®Ê®°ÂûãÂ§ßÂ∞èÂíåÁ≤æÂ∫¶‰πãÈó¥ÂÆûÁé∞ÊúÄ‰Ω≥ÊùÉË°°„ÄÇSDQ-LLMÈááÁî®ËøáÈááÊ†∑ÁªìÂêàSigma-DeltaÈáèÂåñÂô®ÔºåÂ∞ÜLLMÊùÉÈáç‰∫åÂÄºÂåñÊàñ‰∏âÂÄºÂåñÔºåÂ∞ÜÈ´òÁ≤æÂ∫¶ÂèÇÊï∞ÁºñÁ†Å‰∏∫1ÊØîÁâπÊàñ1.58ÊØîÁâπË°®Á§∫ÔºåÂπ∂Â∞ÜÁ∫øÊÄßÂ±Ç‰∏≠ÁöÑ‰πòÊ≥ïËøêÁÆóÊõøÊç¢‰∏∫Âä†Ê≥ïÔºå‰ªéËÄåÊòæËëóÊèêÈ´òÊûÅ‰ΩéÊØîÁâπÈáèÂåñ‰∏ãÁöÑÊé®ÁêÜÊïàÁéá„ÄÇ‰∏∫‰∫ÜËøõ‰∏ÄÊ≠•ÂáèÂ∞ëÈáèÂåñÁ≤æÂ∫¶ÊçüÂ§±ÔºåÊàë‰ª¨Âú®ÈáèÂåñÂâçÂºïÂÖ•Âü∫‰∫éHadamardÁöÑÊùÉÈáçÂπ≥ÊªëÔºåÊèêÈ´òÊùÉÈáçË°®Á§∫ÁöÑÁ®≥ÂÆöÊÄßÂíåÈ≤ÅÊ£íÊÄß„ÄÇÊ≠§Â§ñÔºå‰∏∫‰∫ÜÂÖÖÂàÜÂà©Áî®OSRÁöÑËøûÁª≠ÊÄßÂπ∂ÂáèÂ∞ëÁ≤æÂ∫¶ÊçüÂ§±ÔºåÊàë‰ª¨ËÆ§ËØÜÂà∞ÈáèÂåñÊïèÊÑüÊÄß‰∏éÊùÉÈáçÊñπÂ∑Æ‰πãÈó¥ÁöÑÁõ∏ÂÖ≥ÊÄßÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßçÁªÜÁ≤íÂ∫¶ÁöÑ„ÄÅÂ±ÇÂíåÁ∫øÊÄßÂ±ÇÁ∫ßÂà´ÁöÑOSRÂàÜÈÖçÁ≠ñÁï•MultiOSR„ÄÇËØ•Á≠ñÁï•Âü∫‰∫éÊùÉÈáçÊñπÂ∑ÆÂíåÂèÇÊï∞ËßÑÊ®°ÔºåÂú®Â±Ç‰πãÈó¥ÂíåÊØèÂ±ÇÂÜÖÈÉ®ÂàÜÈÖçOSR„ÄÇÂú®OPTÂíåLLaMAÊ®°ÂûãÁ≥ªÂàó‰∏äÁöÑÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåÂç≥‰ΩøÂú®È´òÂ∫¶ÊøÄËøõÁöÑ‰ΩéOSRËÆæÁΩÆ‰∏ãÔºåSDQ-LLM‰πüËÉΩÂÆûÁé∞Êõ¥È´òÊïàÂíåÈ´òÁ≤æÂ∫¶ÁöÑÊÄßËÉΩ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÈÉ®ÁΩ≤Èù¢‰∏¥ÁùÄÂ∑®Â§ßÁöÑËÆ°ÁÆóÂíåÂÜÖÂ≠òÂéãÂäõ„ÄÇÊûÅ‰ΩéÊØîÁâπÈáèÂåñÊòØÈôç‰ΩéËµÑÊ∫êÊ∂àËÄóÁöÑÊúâÊïàÊñπÊ≥ïÔºå‰ΩÜÁé∞ÊúâÊñπÊ≥ïÂú®Â§ßÂπÖÂéãÁº©Ê®°ÂûãÁöÑÂêåÊó∂ÔºåÂæÄÂæÄ‰ºöÈÄ†Êàê‰∏•ÈáçÁöÑÁ≤æÂ∫¶ÊçüÂ§±ÔºåÂ∞§ÂÖ∂ÊòØÂú®1ÊØîÁâπÈáèÂåñÁöÑÊÉÖÂÜµ‰∏ã„ÄÇÂõ†Ê≠§ÔºåÂ¶Ç‰ΩïÂú®‰øùËØÅÊ®°ÂûãÊÄßËÉΩÁöÑÂâçÊèê‰∏ãÔºåÂÆûÁé∞LLMÁöÑÊûÅ‰ΩéÊØîÁâπÈáèÂåñÊòØ‰∏Ä‰∏™ÂÖ≥ÈîÆÈóÆÈ¢ò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöSDQ-LLMÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®Sigma-DeltaÈáèÂåñÂô®ÂíåÂèØË∞ÉËäÇÁöÑËøáÈááÊ†∑ÁéáÔºàOSRÔºâÊù•ÂÆûÁé∞ÊùÉÈáçÁöÑÊúâÊïàÂéãÁº©ÂíåÁ≤æÂ∫¶‰øùÊåÅ„ÄÇSigma-DeltaÈáèÂåñÂô®ËÉΩÂ§üÂ∞ÜÈ´òÁ≤æÂ∫¶ÂèÇÊï∞ÁºñÁ†Å‰∏∫‰ΩéÊØîÁâπË°®Á§∫ÔºåËÄåOSRÂàôÂÖÅËÆ∏Âú®Ê®°ÂûãÂ§ßÂ∞èÂíåÁ≤æÂ∫¶‰πãÈó¥ËøõË°åÊùÉË°°„ÄÇÈÄöËøáÂä®ÊÄÅË∞ÉÊï¥OSRÔºåSDQ-LLMÂèØ‰ª•ÈÄÇÂ∫î‰∏çÂêåÁöÑÂÜÖÂ≠òÊàñVRAMÁ∫¶Êùü„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáËøòÈÄöËøáHadamardÊùÉÈáçÂπ≥ÊªëÂíåÁªÜÁ≤íÂ∫¶ÁöÑOSRÂàÜÈÖçÁ≠ñÁï•Êù•Ëøõ‰∏ÄÊ≠•ÂáèÂ∞ëÈáèÂåñËØØÂ∑Æ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöSDQ-LLMÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÊã¨‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÊ≠•È™§Ôºö1) HadamardÊùÉÈáçÂπ≥ÊªëÔºöÂØπÂéüÂßãÊùÉÈáçËøõË°åÈ¢ÑÂ§ÑÁêÜÔºåÊèêÈ´òÊùÉÈáçË°®Á§∫ÁöÑÁ®≥ÂÆöÊÄßÂíåÈ≤ÅÊ£íÊÄß„ÄÇ2) Sigma-DeltaÈáèÂåñÔºö‰ΩøÁî®Sigma-DeltaÈáèÂåñÂô®Â∞ÜÊùÉÈáçËΩ¨Êç¢‰∏∫1ÊØîÁâπÊàñ1.58ÊØîÁâπË°®Á§∫„ÄÇ3) ËøáÈááÊ†∑ÔºöÈÄöËøáË∞ÉÊï¥OSRÊù•ÊéßÂà∂ÈáèÂåñÂêéÁöÑÊ®°ÂûãÂ§ßÂ∞èÂíåÁ≤æÂ∫¶„ÄÇ4) MultiOSRÂàÜÈÖçÔºöÊ†πÊçÆÊùÉÈáçÊñπÂ∑ÆÂíåÂèÇÊï∞ËßÑÊ®°ÔºåÂú®‰∏çÂêåÂ±ÇÂíåÁ∫øÊÄßÂ±Ç‰πãÈó¥ÂàÜÈÖçOSR„ÄÇ5) Êé®ÁêÜÔºö‰ΩøÁî®ÈáèÂåñÂêéÁöÑÊùÉÈáçËøõË°åÊé®ÁêÜÔºåÂ∞ÜÁ∫øÊÄßÂ±Ç‰∏≠ÁöÑ‰πòÊ≥ïËøêÁÆóÊõøÊç¢‰∏∫Âä†Ê≥ïËøêÁÆó„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöSDQ-LLMÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫é‰ª•‰∏ãÂá†‰∏™ÊñπÈù¢Ôºö1) ÂèØËøûÁª≠Ë∞ÉËäÇÁöÑOSRÔºöÂÖÅËÆ∏Âú®Ê®°ÂûãÂ§ßÂ∞èÂíåÁ≤æÂ∫¶‰πãÈó¥ËøõË°åÂä®ÊÄÅÊùÉË°°ÔºåÈÄÇÂ∫î‰∏çÂêåÁöÑËµÑÊ∫êÁ∫¶Êùü„ÄÇ2) HadamardÊùÉÈáçÂπ≥ÊªëÔºöÂáèÂ∞ëÈáèÂåñËØØÂ∑ÆÔºåÊèêÈ´òÊ®°ÂûãÊÄßËÉΩ„ÄÇ3) MultiOSRÂàÜÈÖçÁ≠ñÁï•ÔºöÊ†πÊçÆÊùÉÈáçÊñπÂ∑ÆÂíåÂèÇÊï∞ËßÑÊ®°ÔºåÂú®‰∏çÂêåÂ±ÇÂíåÁ∫øÊÄßÂ±Ç‰πãÈó¥ÂàÜÈÖçOSRÔºåËøõ‰∏ÄÊ≠•ÊèêÈ´òÈáèÂåñÁ≤æÂ∫¶„ÄÇ4) Â∞Ü‰πòÊ≥ïËøêÁÆóÊõøÊç¢‰∏∫Âä†Ê≥ïËøêÁÆóÔºöÊòæËëóÊèêÈ´òÊé®ÁêÜÊïàÁéá„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**Ôºö1) ËøáÈááÊ†∑ÁéáOSRÁöÑËøûÁª≠ÂèØË∞ÉÊÄßÔºåÂÖÅËÆ∏Áî®Êà∑Ê†πÊçÆÂÆûÈôÖËµÑÊ∫êÊÉÖÂÜµÈÄâÊã©ÂêàÈÄÇÁöÑOSRÂÄºÔºå‰æãÂ¶Ç2.5ÂÄç„ÄÇ2) HadamardÊùÉÈáçÂπ≥ÊªëÁöÑÂÖ∑‰ΩìÂÆûÁé∞ÊñπÂºèÔºåÂåÖÊã¨HadamardÁü©ÈòµÁöÑÈÄâÊã©ÂíåÂ∫îÁî®„ÄÇ3) MultiOSRÂàÜÈÖçÁ≠ñÁï•ÁöÑÁªÜËäÇÔºåÂåÖÊã¨ÊùÉÈáçÊñπÂ∑ÆÂíåÂèÇÊï∞ËßÑÊ®°ÁöÑËÆ°ÁÆóÊñπÊ≥ïÔºå‰ª•ÂèäOSRÁöÑÂàÜÈÖçÊØî‰æã„ÄÇ4) Sigma-DeltaÈáèÂåñÂô®ÁöÑÂÖ∑‰ΩìÂèÇÊï∞ËÆæÁΩÆÔºå‰æãÂ¶ÇÈáèÂåñÊ≠•ÈïøÂíåÈòàÂÄº„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåSDQ-LLMÂú®OPTÂíåLLaMAÊ®°ÂûãÁ≥ªÂàó‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ‰æãÂ¶ÇÔºåÂú®‰ΩéËøáÈááÊ†∑ÁéáËÆæÁΩÆ‰∏ãÔºåSDQ-LLM‰ªçÁÑ∂ËÉΩÂ§ü‰øùÊåÅËæÉÈ´òÁöÑÁ≤æÂ∫¶ÔºåÂπ∂‰∏îÂú®Êüê‰∫õÊÉÖÂÜµ‰∏ãÁîöËá≥Ë∂ÖËøá‰∫ÜÂÖ∂‰ªñÈáèÂåñÊñπÊ≥ï„ÄÇÊ≠§Â§ñÔºåSDQ-LLMËøòËÉΩÂ§üÊòæËëóÈôç‰ΩéÊ®°ÂûãÁöÑÂÜÖÂ≠òÂç†Áî®ÂíåËÆ°ÁÆóÂ§çÊùÇÂ∫¶ÔºåÊèêÈ´òÊé®ÁêÜÈÄüÂ∫¶„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

SDQ-LLMÂú®ËµÑÊ∫êÂèóÈôêÁöÑËÆæÂ§á‰∏äÈÉ®ÁΩ≤Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊñπÈù¢ÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºå‰æãÂ¶ÇÁßªÂä®ËÆæÂ§á„ÄÅÂµåÂÖ•ÂºèÁ≥ªÁªüÂíåËæπÁºòËÆ°ÁÆóËÆæÂ§á„ÄÇËØ•ÊäÄÊúØÂèØ‰ª•Èôç‰ΩéLLMÁöÑÂÜÖÂ≠òÂç†Áî®ÂíåËÆ°ÁÆóÂ§çÊùÇÂ∫¶Ôºå‰ΩøÂÖ∂ËÉΩÂ§üÂú®Ëøô‰∫õËÆæÂ§á‰∏äÈ´òÊïàËøêË°åÔºå‰ªéËÄåÂÆûÁé∞Êô∫ËÉΩÂä©Êâã„ÄÅËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÁ≠âÂ∫îÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Large language models (LLMs) face significant computational and memory challenges, making extremely low-bit quantization crucial for their efficient deployment. In this work, we introduce SDQ-LLM: Sigma-Delta Quantization for 1-bit LLMs of any size, a novel framework that enables extremely low-bit quantization of LLMs while preserving their linguistic reasoning capabilities. A distinctive feature of SDQ-LLM is the continuous adjustability of the Over-Sampling Ratio (OSR), enabling dynamic adaptation to memory or VRAM constraints by selecting fractional OSR (e.g. 2.5 times) for an optimal trade-off between model size and accuracy. SDQ-LLM uses upsampling combined with Sigma-Delta Quantizer to binarize or ternarize LLMs weights, encoding high-precision parameters into 1-bit or 1.58-bit representations, replacing the multiplication operations within linear layers with addition. This approach significantly enhances inference efficiency under extremely low-bit quantization. To further reduce the loss of quantization precision, we incorporate Hadamard-based weight smoothing prior to quantization, improving the stability and robustness of the weight representations. Furthermore, to fully leverage the continuity of the OSR and reduce precision loss, recognizing the correlation between quantization sensitivity and weight variance, we propose a fine-grained, layer- and linear-wise OSR allocation strategy, MultiOSR. This strategy distributes OSR both across layers and within each layer, based on weight variance and parameter scale. Finally, extensive experiments on OPT and LLaMA model families demonstrate that SDQ-LLM achieves a more efficient and high-precision performance even under highly aggressive low-OSR settings. Our code is available at https://github.com/Dreamlittlecat/LLM-Quant-Factory.

