---
layout: default
title: S-Chain: Structured Visual Chain-of-Thought For Medicine
---

# S-Chain: Structured Visual Chain-of-Thought For Medicine

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.22728" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.22728v1</a>
  <a href="https://arxiv.org/pdf/2510.22728.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.22728v1" onclick="toggleFavorite(this, '2510.22728v1', 'S-Chain: Structured Visual Chain-of-Thought For Medicine')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Khai Le-Duc, Duy M. H. Nguyen, Phuong T. H. Trinh, Tien-Phat Nguyen, Nghiem T. Diep, An Ngo, Tung Vu, Trinh Vuong, Anh-Tien Nguyen, Mau Nguyen, Van Trung Hoang, Khai-Nguyen Nguyen, Hy Nguyen, Chris Ngo, Anji Liu, Nhat Ho, Anne-Christin Hauschild, Khanh Xuan Nguyen, Thanh Nguyen-Tang, Pengtao Xie, Daniel Sonntag, James Zou, Mathias Niepert, Anh Totti Nguyen

**åˆ†ç±»**: cs.LG, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-26

**å¤‡æ³¨**: First version

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºS-Chainæ•°æ®é›†ï¼Œç”¨äºæå‡åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œè§†è§‰ grounding å‡†ç¡®æ€§ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åŒ»å­¦è§†è§‰é—®ç­”` `è§†è§‰è¯­è¨€æ¨¡å‹` `æ€ç»´é“¾` `è§†è§‰ grounding` `æ•°æ®é›†æ„å»º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. åŒ»å­¦VQAæ¨¡å‹ç¼ºä¹ç²¾ç¡®çš„è§†è§‰ grounding å’Œå¯è§£é‡Šæ€§ï¼Œé™åˆ¶äº†å…¶åœ¨ä¸´åºŠå†³ç­–ä¸­çš„åº”ç”¨ã€‚
2. S-Chainæ•°æ®é›†é€šè¿‡ä¸“å®¶æ ‡æ³¨çš„ç»“æ„åŒ–è§†è§‰CoTï¼Œå°†è§†è§‰åŒºåŸŸä¸æ¨ç†æ­¥éª¤æ˜¾å¼é“¾æ¥ï¼Œæå‡æ¨¡å‹æ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒåŸºäºS-Chainè®­ç»ƒçš„æ¨¡å‹åœ¨å¯è§£é‡Šæ€§ã€grounding ä¿çœŸåº¦å’Œé²æ£’æ€§æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä¸­å¯é çš„æ¨ç†ä¸ä»…éœ€è¦å‡†ç¡®çš„é¢„æµ‹ï¼Œè¿˜éœ€è¦æ–‡æœ¬è§£é‡Šä¸è§†è§‰è¯æ®ä¹‹é—´çš„é€æ˜å¯¹é½ã€‚å°½ç®¡æ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºåœ¨åŒ»å­¦è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä¸­æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å°šæ— å¤§è§„æ¨¡ä¸“å®¶çº§æ•°æ®é›†æ•è·å…·æœ‰ç²¾ç¡®è§†è§‰ grounding çš„é€æ­¥æ¨ç†ã€‚æˆ‘ä»¬æ¨å‡ºäº†S-Chainï¼Œè¿™æ˜¯é¦–ä¸ªåŒ…å«12,000å¼ ä¸“å®¶æ ‡æ³¨åŒ»å­¦å›¾åƒçš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå¸¦æœ‰è¾¹ç•Œæ¡†å’Œç»“æ„åŒ–è§†è§‰CoTï¼ˆSV-CoTï¼‰ï¼Œæ˜ç¡®åœ°å°†è§†è§‰åŒºåŸŸé“¾æ¥åˆ°æ¨ç†æ­¥éª¤ã€‚è¯¥æ•°æ®é›†è¿˜æ”¯æŒ16ç§è¯­è¨€ï¼Œæ€»å…±è¶…è¿‡70ä¸‡ä¸ªVQAå¯¹ï¼Œå…·æœ‰å¹¿æ³›çš„å¤šè¯­è¨€é€‚ç”¨æ€§ã€‚æˆ‘ä»¬ä½¿ç”¨S-Chain å¯¹æœ€å…ˆè¿›çš„åŒ»å­¦VLMsï¼ˆExGra-Medã€LLaVA-Medï¼‰å’Œé€šç”¨VLMsï¼ˆQwen2.5-VLã€InternVL2.5ï¼‰è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œç»“æœè¡¨æ˜SV-CoTç›‘ç£æ˜¾è‘—æé«˜äº†å¯è§£é‡Šæ€§ã€grounding ä¿çœŸåº¦å’Œé²æ£’æ€§ã€‚é™¤äº†åŸºå‡†æµ‹è¯•ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜ç ”ç©¶äº†å®ƒä¸æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„ååŒä½œç”¨ï¼Œæ­ç¤ºäº†é¢†åŸŸçŸ¥è¯†å’Œè§†è§‰ grounding åœ¨è‡ªå›å½’æ¨ç†è¿‡ç¨‹ä¸­çš„ç›¸äº’ä½œç”¨ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æœºåˆ¶ï¼ŒåŠ å¼ºäº†è§†è§‰è¯æ®å’Œæ¨ç†ä¹‹é—´çš„å¯¹é½ï¼Œæé«˜äº†å¯é æ€§å’Œæ•ˆç‡ã€‚S-Chain ä¸º grounded åŒ»å­¦æ¨ç†å»ºç«‹äº†ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œå¹¶ä¸ºæ›´å€¼å¾—ä¿¡èµ–å’Œå¯è§£é‡Šçš„åŒ»å­¦VLMs é“ºå¹³äº†é“è·¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šåŒ»å­¦è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä»»åŠ¡æ—¨åœ¨æ ¹æ®åŒ»å­¦å›¾åƒå›ç­”ç›¸å…³é—®é¢˜ã€‚ç°æœ‰çš„åŒ»å­¦VQAæ¨¡å‹é€šå¸¸ç¼ºä¹é€æ˜çš„æ¨ç†è¿‡ç¨‹å’Œç²¾ç¡®çš„è§†è§‰ groundingï¼Œéš¾ä»¥è§£é‡Šå…¶å†³ç­–ä¾æ®ï¼Œé™åˆ¶äº†å…¶åœ¨ä¸´åºŠå®è·µä¸­çš„åº”ç”¨ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥å°†æ–‡æœ¬æ¨ç†æ­¥éª¤ä¸å›¾åƒä¸­çš„å…·ä½“åŒºåŸŸå¯¹åº”èµ·æ¥ï¼Œå¯¼è‡´æ¨¡å‹å¯èƒ½ä¾èµ–äºå›¾åƒä¸­çš„æ— å…³ä¿¡æ¯æˆ–äº§ç”Ÿå¹»è§‰ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªå¤§è§„æ¨¡çš„ã€å…·æœ‰ç»“æ„åŒ–è§†è§‰æ€ç»´é“¾ï¼ˆSV-CoTï¼‰æ ‡æ³¨çš„åŒ»å­¦å›¾åƒæ•°æ®é›†S-Chainã€‚é€šè¿‡æ˜¾å¼åœ°å°†å›¾åƒä¸­çš„è§†è§‰åŒºåŸŸä¸æ¨ç†æ­¥éª¤è”ç³»èµ·æ¥ï¼ŒS-Chainæ—¨åœ¨æå‡åŒ»å­¦VQAæ¨¡å‹çš„å¯è§£é‡Šæ€§ã€grounding ä¿çœŸåº¦å’Œé²æ£’æ€§ã€‚è¿™ç§ç»“æ„åŒ–çš„æ ‡æ³¨æ–¹å¼ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ åˆ°æ›´åŠ å¯é çš„æ¨ç†è·¯å¾„ï¼Œä»è€Œåšå‡ºæ›´å‡†ç¡®çš„é¢„æµ‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šS-Chainæ•°æ®é›†åŒ…å«12,000å¼ ä¸“å®¶æ ‡æ³¨çš„åŒ»å­¦å›¾åƒï¼Œæ¯å¼ å›¾åƒéƒ½åŒ…å«è¾¹ç•Œæ¡†å’Œç»“æ„åŒ–è§†è§‰CoTï¼ˆSV-CoTï¼‰æ ‡æ³¨ã€‚SV-CoTæ ‡æ³¨æ˜ç¡®åœ°å°†è§†è§‰åŒºåŸŸé“¾æ¥åˆ°æ¨ç†æ­¥éª¤ï¼Œå½¢æˆä¸€ä¸ªæ¨ç†é“¾ã€‚è¯¥æ•°æ®é›†è¿˜æ”¯æŒ16ç§è¯­è¨€ï¼Œæ€»å…±è¶…è¿‡70ä¸‡ä¸ªVQAå¯¹ã€‚è®ºæ–‡è¿˜æå‡ºäº†ä¸€ç§æ–°çš„æœºåˆ¶ï¼Œç”¨äºåŠ å¼ºè§†è§‰è¯æ®å’Œæ¨ç†ä¹‹é—´çš„å¯¹é½ã€‚è¯¥æœºåˆ¶çš„å…·ä½“å®ç°ç»†èŠ‚æœªçŸ¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šS-Chainæ•°æ®é›†æ˜¯é¦–ä¸ªå¤§è§„æ¨¡çš„ã€å…·æœ‰ä¸“å®¶æ ‡æ³¨çš„ç»“æ„åŒ–è§†è§‰CoTçš„åŒ»å­¦å›¾åƒæ•°æ®é›†ã€‚ä¸ä»¥å¾€çš„æ•°æ®é›†ç›¸æ¯”ï¼ŒS-Chainæä¾›äº†æ›´ä¸°å¯Œçš„æ ‡æ³¨ä¿¡æ¯ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æŒ‡å¯¼æ¨¡å‹å­¦ä¹ ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†ä¸€ç§æ–°çš„æœºåˆ¶ï¼Œç”¨äºåŠ å¼ºè§†è§‰è¯æ®å’Œæ¨ç†ä¹‹é—´çš„å¯¹é½ï¼Œè¿›ä¸€æ­¥æå‡äº†æ¨¡å‹æ€§èƒ½ã€‚S-Chainçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶ç»“æ„åŒ–çš„æ ‡æ³¨æ–¹å¼ï¼Œå®ƒä½¿å¾—æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ åˆ°æ›´åŠ å¯é çš„æ¨ç†è·¯å¾„ï¼Œä»è€Œåšå‡ºæ›´å‡†ç¡®çš„é¢„æµ‹ã€‚

**å…³é”®è®¾è®¡**ï¼šæ•°æ®é›†åŒ…å«12,000å¼ åŒ»å­¦å›¾åƒï¼Œå¹¶ä½¿ç”¨ä¸“å®¶è¿›è¡Œæ ‡æ³¨ï¼Œç¡®ä¿æ ‡æ³¨çš„è´¨é‡å’Œå‡†ç¡®æ€§ã€‚æ¯å¼ å›¾åƒéƒ½åŒ…å«è¾¹ç•Œæ¡†å’Œç»“æ„åŒ–è§†è§‰CoTï¼ˆSV-CoTï¼‰æ ‡æ³¨ï¼Œæ˜ç¡®åœ°å°†è§†è§‰åŒºåŸŸé“¾æ¥åˆ°æ¨ç†æ­¥éª¤ã€‚æ•°æ®é›†æ”¯æŒ16ç§è¯­è¨€ï¼Œä»¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚è®ºæ–‡è¿˜ç ”ç©¶äº†S-Chainä¸æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„ååŒä½œç”¨ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„æœºåˆ¶æ¥åŠ å¼ºè§†è§‰è¯æ®å’Œæ¨ç†ä¹‹é—´çš„å¯¹é½ï¼Œä½†å…·ä½“çš„æŠ€æœ¯ç»†èŠ‚æœªè¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨S-Chainæ•°æ®é›†è®­ç»ƒçš„åŒ»å­¦VLMsï¼ˆExGra-Medã€LLaVA-Medï¼‰å’Œé€šç”¨VLMsï¼ˆQwen2.5-VLã€InternVL2.5ï¼‰åœ¨å¯è§£é‡Šæ€§ã€grounding ä¿çœŸåº¦å’Œé²æ£’æ€§æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ã€‚å…·ä½“æ€§èƒ½æ•°æ®å’Œæå‡å¹…åº¦æœªçŸ¥ï¼Œä½†è®ºæ–‡å¼ºè°ƒäº†SV-CoTç›‘ç£çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå¼€å‘æ›´å€¼å¾—ä¿¡èµ–å’Œå¯è§£é‡Šçš„åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œè¾…åŠ©åŒ»ç”Ÿè¿›è¡Œç–¾ç—…è¯Šæ–­ã€æ²»ç–—æ–¹æ¡ˆåˆ¶å®šç­‰å†³ç­–ã€‚é€šè¿‡æä¾›æ¸…æ™°çš„æ¨ç†è¿‡ç¨‹å’Œè§†è§‰è¯æ®ï¼Œå¢å¼ºåŒ»ç”Ÿå¯¹æ¨¡å‹çš„ä¿¡ä»»åº¦ï¼Œæé«˜ä¸´åºŠå†³ç­–çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚æœªæ¥å¯æ‰©å±•åˆ°å…¶ä»–åŒ»å­¦å½±åƒé¢†åŸŸï¼Œå¦‚ç—…ç†åˆ‡ç‰‡åˆ†æç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Faithful reasoning in medical vision-language models (VLMs) requires not only accurate predictions but also transparent alignment between textual rationales and visual evidence. While Chain-of-Thought (CoT) prompting has shown promise in medical visual question answering (VQA), no large-scale expert-level dataset has captured stepwise reasoning with precise visual grounding. We introduce S-Chain, the first large-scale dataset of 12,000 expert-annotated medical images with bounding boxes and structured visual CoT (SV-CoT), explicitly linking visual regions to reasoning steps. The dataset further supports 16 languages, totaling over 700k VQA pairs for broad multilingual applicability. Using S-Chain, we benchmark state-of-the-art medical VLMs (ExGra-Med, LLaVA-Med) and general-purpose VLMs (Qwen2.5-VL, InternVL2.5), showing that SV-CoT supervision significantly improves interpretability, grounding fidelity, and robustness. Beyond benchmarking, we study its synergy with retrieval-augmented generation, revealing how domain knowledge and visual grounding interact during autoregressive reasoning. Finally, we propose a new mechanism that strengthens the alignment between visual evidence and reasoning, improving both reliability and efficiency. S-Chain establishes a new benchmark for grounded medical reasoning and paves the way toward more trustworthy and explainable medical VLMs.

