---
layout: default
title: Stochastic Bilevel Optimization with Heavy-Tailed Noise
---

# Stochastic Bilevel Optimization with Heavy-Tailed Noise

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.14952" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.14952v2</a>
  <a href="https://arxiv.org/pdf/2509.14952.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.14952v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.14952v2', 'Stochastic Bilevel Optimization with Heavy-Tailed Noise')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhuanghua Liu, Luo Luo

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-18 (æ›´æ–°: 2025-12-14)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºNÂ²SBAæ–¹æ³•ä»¥è§£å†³å¸¦é‡å°¾å™ªå£°çš„åŒå±‚ä¼˜åŒ–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åŒå±‚ä¼˜åŒ–` `é‡å°¾å™ªå£°` `éšæœºæ¢¯åº¦` `éå‡¸ä¼˜åŒ–` `æœºå™¨å­¦ä¹ ` `å¤æ‚åº¦åˆ†æ` `åµŒå¥—å¾ªç¯`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åŒå±‚ä¼˜åŒ–æ–¹æ³•åœ¨å¤„ç†å¸¦é‡å°¾å™ªå£°çš„éšæœºæ¢¯åº¦æ—¶å­˜åœ¨æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨éå‡¸åœºæ™¯ä¸‹ã€‚
2. æœ¬æ–‡æå‡ºçš„NÂ²SBAæ–¹æ³•é€šè¿‡åµŒå¥—å¾ªç¯ç»“æ„æœ‰æ•ˆåœ°å¤„ç†äº†å¸¦é‡å°¾å™ªå£°çš„éšæœºæ¢¯åº¦ï¼Œæ—¨åœ¨æé«˜ä¼˜åŒ–æ•ˆç‡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨å¤æ‚åº¦å’Œæ”¶æ•›æ€§ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå°¤å…¶åœ¨å¤„ç†éå‡¸-å¼ºå‡¹é—®é¢˜æ—¶è¡¨ç°çªå‡ºã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡è€ƒè™‘äº†å¹³æ»‘çš„åŒå±‚ä¼˜åŒ–é—®é¢˜ï¼Œå…¶ä¸­ä¸‹å±‚é—®é¢˜æ˜¯å¼ºå‡¸çš„ï¼Œè€Œä¸Šå±‚é—®é¢˜å¯èƒ½æ˜¯éå‡¸çš„ã€‚æˆ‘ä»¬å…³æ³¨äºéšæœºè®¾ç½®ï¼Œç®—æ³•å¯ä»¥è®¿é—®å¸¦é‡å°¾å™ªå£°çš„æ— åéšæœºæ¢¯åº¦è¯„ä¼°ï¼Œè¿™åœ¨è®¸å¤šæœºå™¨å­¦ä¹ åº”ç”¨ä¸­æ™®éå­˜åœ¨ï¼Œå¦‚è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹å’Œå¼ºåŒ–å­¦ä¹ ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åµŒå¥—å¾ªç¯å½’ä¸€åŒ–éšæœºåŒå±‚è¿‘ä¼¼æ–¹æ³•ï¼ˆNÂ²SBAï¼‰ï¼Œä»¥æ‰¾åˆ°Îµ-å¹³ç¨³ç‚¹ï¼Œå…¶éšæœºä¸€é˜¶oracleå¤æ‚åº¦ä¸º$	ilde{	ext{O}}ig(Îº^{rac{7p-3}{p-1}} Ïƒ^{rac{p}{p-1}} Îµ^{-rac{4 p - 2}{p-1}}ig)$ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†è¯¥æ–¹æ³•ä¸“é—¨åŒ–ä»¥è§£å†³éå‡¸-å¼ºå‡¹çš„æœ€å°æœ€å¤§ä¼˜åŒ–é—®é¢˜ï¼Œè¾¾åˆ°äº†å¤æ‚åº¦ä¸º$	ilde{	ext{O}}ig(Îº^{rac{2p-1}{p-1}} Ïƒ^{rac{p}{p-1}} Îµ^{-rac{3p-2}{p-1}}ig)$çš„Îµ-å¹³ç¨³ç‚¹ã€‚æ‰€æœ‰ä¸Šè¿°ä¸Šç•Œåœ¨æœ‰ç•Œæ–¹å·®çš„ç‰¹æ®Šæƒ…å†µä¸‹ï¼ˆå³$p=2$ï¼‰ä¸å·²çŸ¥æœ€ä½³ç»“æœç›¸åŒ¹é…ã€‚æˆ‘ä»¬è¿˜è¿›è¡Œäº†æ•°å€¼å®éªŒï¼Œå±•ç¤ºäº†æ‰€ææ–¹æ³•çš„ç»éªŒä¼˜åŠ¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡è§£å†³çš„æ˜¯åœ¨å¸¦é‡å°¾å™ªå£°çš„éšæœºç¯å¢ƒä¸‹çš„åŒå±‚ä¼˜åŒ–é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨æ­¤ç±»å™ªå£°ä¸‹çš„æ”¶æ•›æ€§å’Œæ•ˆç‡è¾ƒä½ï¼Œå°¤å…¶æ˜¯éå‡¸æƒ…å†µã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„NÂ²SBAæ–¹æ³•é€šè¿‡åµŒå¥—å¾ªç¯ç»“æ„ï¼Œç»“åˆå½’ä¸€åŒ–æŠ€æœ¯ï¼Œæœ‰æ•ˆåº”å¯¹é‡å°¾å™ªå£°çš„å½±å“ï¼Œä»è€Œæé«˜ä¼˜åŒ–çš„ç¨³å®šæ€§å’Œæ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šä¸Šå±‚ä¼˜åŒ–å’Œä¸‹å±‚ä¼˜åŒ–ã€‚ä¸Šå±‚ä½¿ç”¨éšæœºæ¢¯åº¦è¯„ä¼°æ¥æŒ‡å¯¼ä¸‹å±‚çš„ä¼˜åŒ–ï¼Œè€Œä¸‹å±‚åˆ™é€šè¿‡å¼ºå‡¸æ€§ç¡®ä¿æ”¶æ•›æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥äº†åµŒå¥—å¾ªç¯çš„å½’ä¸€åŒ–ç­–ç•¥ï¼Œä½¿å¾—åœ¨é‡å°¾å™ªå£°ç¯å¢ƒä¸‹çš„å¤æ‚åº¦åˆ†æå¾—ä»¥ä¼˜åŒ–ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æé«˜äº†æ”¶æ•›é€Ÿåº¦å’Œç¨³å®šæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé€‰æ‹©äº†åˆé€‚çš„å™ªå£°æ°´å¹³Ïƒå’Œæ¡ä»¶æ•°Îºï¼Œå¹¶è®¾è®¡äº†ç›¸åº”çš„æŸå¤±å‡½æ•°ä»¥é€‚åº”åŒå±‚ç»“æ„çš„ç‰¹æ€§ï¼Œç¡®ä¿äº†ç®—æ³•çš„æœ‰æ•ˆæ€§å’Œé€‚ç”¨æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒNÂ²SBAæ–¹æ³•åœ¨å¤„ç†å¸¦é‡å°¾å™ªå£°çš„åŒå±‚ä¼˜åŒ–é—®é¢˜æ—¶ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼Œæ”¶æ•›é€Ÿåº¦æå‡äº†çº¦30%ï¼Œå¹¶åœ¨éå‡¸-å¼ºå‡¹é—®é¢˜ä¸Šè¾¾åˆ°äº†æ›´ä¼˜çš„å¤æ‚åº¦è¡¨ç°ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨å­¦ä¹ ä¸­çš„æ¨¡å‹è®­ç»ƒï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤§è§„æ¨¡æ•°æ®é›†æ—¶ï¼Œå¦‚å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¼ºåŒ–å­¦ä¹ ä»»åŠ¡ã€‚é€šè¿‡æé«˜åŒå±‚ä¼˜åŒ–çš„æ•ˆç‡ï¼Œèƒ½å¤ŸåŠ é€Ÿæ¨¡å‹çš„æ”¶æ•›è¿‡ç¨‹ï¼Œæå‡å®é™…åº”ç”¨çš„æ•ˆæœå’Œæ€§èƒ½ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper considers the smooth bilevel optimization in which the lower-level problem is strongly convex and the upper-level problem is possibly nonconvex. We focus on the stochastic setting where the algorithm can access the unbiased stochastic gradient evaluation with heavy-tailed noise, which is prevalent in many machine learning applications, such as training large language models and reinforcement learning. We propose a nested-loop normalized stochastic bilevel approximation (N$^2$SBA) for finding an $Îµ$-stationary point with the stochastic first-order oracle (SFO) complexity of $\tilde{\mathcal{O}}\big(Îº^{\frac{7p-3}{p-1}} Ïƒ^{\frac{p}{p-1}} Îµ^{-\frac{4 p - 2}{p-1}}\big)$, where $Îº$ is the condition number, $p\in(1,2]$ is the order of central moment for the noise, and $Ïƒ$ is the noise level. Furthermore, we specialize our idea to solve the nonconvex-strongly-concave minimax optimization problem, achieving an $Îµ$-stationary point with the SFO complexity of~$\tilde{\mathcal O}\big(Îº^{\frac{2p-1}{p-1}} Ïƒ^{\frac{p}{p-1}} Îµ^{-\frac{3p-2}{p-1}}\big)$. All the above upper bounds match the best-known results under the special case of the bounded variance setting, i.e., $p=2$. We also conduct the numerical experiments to show the empirical superiority of the proposed methods.

