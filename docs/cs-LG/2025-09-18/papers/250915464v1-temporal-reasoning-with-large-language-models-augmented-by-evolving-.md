---
layout: default
title: Temporal Reasoning with Large Language Models Augmented by Evolving Knowledge Graphs
---

# Temporal Reasoning with Large Language Models Augmented by Evolving Knowledge Graphs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.15464" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.15464v1</a>
  <a href="https://arxiv.org/pdf/2509.15464.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.15464v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.15464v1', 'Temporal Reasoning with Large Language Models Augmented by Evolving Knowledge Graphs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Junhong Lin, Song Wang, Xiaojie Guo, Julian Shun, Yada Zhu

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEvoReasonerä»¥è§£å†³åŠ¨æ€çŸ¥è¯†æ¨ç†é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åŠ¨æ€çŸ¥è¯†æ¨ç†` `çŸ¥è¯†å›¾è°±æ¼”å˜` `æ—¶é—´æ„ŸçŸ¥æ¨ç†` `å¤§å‹è¯­è¨€æ¨¡å‹` `å¤šè·³æ¨ç†` `ä¿¡æ¯æ£€ç´¢` `æ™ºèƒ½é—®ç­”ç³»ç»Ÿ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•é€šå¸¸å‡è®¾çŸ¥è¯†å›¾è°±æ˜¯é™æ€çš„ï¼Œå¿½è§†äº†çœŸå®ä¸–ç•Œæ•°æ®ä¸­å›ºæœ‰çš„æ—¶é—´åŠ¨æ€å’Œäº‹å®ä¸ä¸€è‡´æ€§ã€‚
2. æœ¬æ–‡æå‡ºEvoReasonerç®—æ³•ï¼Œç»“åˆEvoKGæ¨¡å—ï¼Œèƒ½å¤ŸåŠ¨æ€æ›´æ–°çŸ¥è¯†å›¾è°±å¹¶è¿›è¡Œæ—¶é—´æ„ŸçŸ¥æ¨ç†ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨è¯¥æ–¹æ³•çš„8Bå‚æ•°æ¨¡å‹åœ¨åŠ¨æ€é—®ç­”ä»»åŠ¡ä¸­è¾¾åˆ°äº†671Bæ¨¡å‹çš„æ€§èƒ½ï¼Œæ˜¾è‘—æå‡äº†å°å‹æ¨¡å‹çš„è¡¨ç°ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è®¸å¤šè¯­è¨€ç†è§£ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤„ç†ä¸æ–­æ¼”å˜çš„çŸ¥è¯†æ—¶å´é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†EvoReasonerï¼Œä¸€ç§æ—¶é—´æ„ŸçŸ¥çš„å¤šè·³æ¨ç†ç®—æ³•ï¼Œç»“åˆäº†EvoKGæ¨¡å—ä»¥åŠ¨æ€æ›´æ–°çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰ã€‚EvoReasoneré€šè¿‡å…¨å±€-å±€éƒ¨å®ä½“å®šä½ã€å¤šè·¯å¾„åˆ†è§£å’Œæ—¶é—´åŸºç¡€è¯„åˆ†æ¥å¤„ç†æ—¶é—´å˜åŒ–çš„çŸ¥è¯†ï¼Œè€ŒEvoKGåˆ™é€šè¿‡åŸºäºç½®ä¿¡åº¦çš„çŸ›ç›¾è§£å†³å’Œæ—¶é—´è¶‹åŠ¿è·Ÿè¸ªæ¥ç¡®ä¿KGçš„å‡†ç¡®æ€§å’Œæ—¶æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŠ¨æ€é—®ç­”ä»»åŠ¡ä¸­ä¼˜äºç°æœ‰çš„åŸºçº¿ï¼Œç¼©å°äº†å°å‹å’Œå¤§å‹LLMsä¹‹é—´çš„æ€§èƒ½å·®è·ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†åŠ¨æ€çŸ¥è¯†æ—¶çš„æ¨ç†èƒ½åŠ›ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆåº”å¯¹çŸ¥è¯†å›¾è°±çš„æ—¶é—´å˜åŒ–å’Œäº‹å®ä¸ä¸€è‡´æ€§é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºEvoReasonerç®—æ³•ï¼Œé€šè¿‡ç»“åˆæ—¶é—´æ„ŸçŸ¥çš„æ¨ç†æœºåˆ¶å’ŒåŠ¨æ€æ›´æ–°çš„çŸ¥è¯†å›¾è°±ï¼Œæ¥å¢å¼ºLLMsåœ¨åŠ¨æ€é—®ç­”ä¸­çš„è¡¨ç°ã€‚è®¾è®¡çš„æ ¸å¿ƒåœ¨äºæœ‰æ•ˆæ•´åˆæ—¶é—´ä¿¡æ¯ä¸çŸ¥è¯†å›¾è°±çš„æ¼”å˜ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬EvoReasonerå’ŒEvoKGä¸¤ä¸ªä¸»è¦æ¨¡å—ã€‚EvoReasonerè´Ÿè´£å¤šè·³æ¨ç†å’Œæ—¶é—´åŸºç¡€è¯„åˆ†ï¼Œè€ŒEvoKGåˆ™è´Ÿè´£ä»éç»“æ„åŒ–æ–‡æ¡£ä¸­å¢é‡æ›´æ–°çŸ¥è¯†å›¾è°±ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºEvoReasonerçš„å¤šè·¯å¾„åˆ†è§£å’Œå…¨å±€-å±€éƒ¨å®ä½“å®šä½ç­–ç•¥ï¼Œä»¥åŠEvoKGçš„å™ªå£°å®¹å¿æ¼”å˜æœºåˆ¶ï¼Œè¿™äº›è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿå¤„ç†æ—¶é—´åŠ¨æ€çš„çŸ¥è¯†ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨EvoKGä¸­ï¼Œé‡‡ç”¨åŸºäºç½®ä¿¡åº¦çš„çŸ›ç›¾è§£å†³ç­–ç•¥å’Œæ—¶é—´è¶‹åŠ¿è·Ÿè¸ªæœºåˆ¶ï¼Œç¡®ä¿çŸ¥è¯†å›¾è°±çš„å‡†ç¡®æ€§å’Œæ—¶æ•ˆæ€§ã€‚æ¨¡å‹çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç»è¿‡ä¼˜åŒ–ï¼Œä»¥æ”¯æŒåŠ¨æ€æ›´æ–°å’Œæ¨ç†è¿‡ç¨‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒEvoReasoneråœ¨åŠ¨æ€é—®ç­”åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œ8Bå‚æ•°æ¨¡å‹çš„æ€§èƒ½ä¸671Bæ¨¡å‹ç›¸å½“ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æå‡å¹…åº¦ï¼Œç¼©å°äº†å°å‹å’Œå¤§å‹LLMsä¹‹é—´çš„æ€§èƒ½å·®è·ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½é—®ç­”ç³»ç»Ÿã€çŸ¥è¯†ç®¡ç†å’Œä¿¡æ¯æ£€ç´¢ç­‰ã€‚é€šè¿‡ç»“åˆæ—¶é—´æ¨ç†ä¸çŸ¥è¯†å›¾è°±æ¼”å˜ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡ç³»ç»Ÿåœ¨å¤„ç†åŠ¨æ€ä¿¡æ¯æ—¶çš„å‡†ç¡®æ€§å’Œå“åº”é€Ÿåº¦ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) excel at many language understanding tasks but struggle to reason over knowledge that evolves. To address this, recent work has explored augmenting LLMs with knowledge graphs (KGs) to provide structured, up-to-date information. However, many existing approaches assume a static snapshot of the KG and overlook the temporal dynamics and factual inconsistencies inherent in real-world data. To address the challenge of reasoning over temporally shifting knowledge, we propose EvoReasoner, a temporal-aware multi-hop reasoning algorithm that performs global-local entity grounding, multi-route decomposition, and temporally grounded scoring. To ensure that the underlying KG remains accurate and up-to-date, we introduce EvoKG, a noise-tolerant KG evolution module that incrementally updates the KG from unstructured documents through confidence-based contradiction resolution and temporal trend tracking. We evaluate our approach on temporal QA benchmarks and a novel end-to-end setting where the KG is dynamically updated from raw documents. Our method outperforms both prompting-based and KG-enhanced baselines, effectively narrowing the gap between small and large LLMs on dynamic question answering. Notably, an 8B-parameter model using our approach matches the performance of a 671B model prompted seven months later. These results highlight the importance of combining temporal reasoning with KG evolution for robust and up-to-date LLM performance. Our code is publicly available at github.com/junhongmit/TREK.

