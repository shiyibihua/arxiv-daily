---
layout: default
title: CoopQ: Cooperative Game Inspired Layerwise Mixed Precision Quantization for LLMs
---

# CoopQ: Cooperative Game Inspired Layerwise Mixed Precision Quantization for LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.15455" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.15455v2</a>
  <a href="https://arxiv.org/pdf/2509.15455.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.15455v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.15455v2', 'CoopQ: Cooperative Game Inspired Layerwise Mixed Precision Quantization for LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Junchen Zhao, Ali Derakhshan, Jayden Kana Hyman, Junhao Dong, Sangeetha Abdu Jyothi, Ian Harris

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-18 (æ›´æ–°: 2025-12-12)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCoopQï¼Œåˆ©ç”¨åˆä½œåšå¼ˆä¼˜åŒ–LLMæ··åˆç²¾åº¦é‡åŒ–ï¼Œæ˜¾è‘—æå‡ä½æ¯”ç‰¹é‡åŒ–æ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ··åˆç²¾åº¦é‡åŒ–` `å¤§å‹è¯­è¨€æ¨¡å‹` `åˆä½œåšå¼ˆ` `Shapleyå€¼` `ä½æ¯”ç‰¹é‡åŒ–` `æ¨¡å‹å‹ç¼©` `é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ··åˆç²¾åº¦é‡åŒ–æ–¹æ³•åœ¨ä½æ¯”ç‰¹ï¼ˆ<4bitï¼‰æ—¶æ€§èƒ½ä¸‹é™ï¼ŒåŸå› æ˜¯å¿½ç•¥äº†å±‚é—´ä¾èµ–å…³ç³»ï¼Œä»…å…³æ³¨å­¤ç«‹çš„å±‚çº§æŒ‡æ ‡ã€‚
2. CoopQå°†æ··åˆç²¾åº¦é‡åŒ–è§†ä¸ºå±‚ä¹‹é—´çš„åˆä½œåšå¼ˆï¼Œåˆ©ç”¨Shapleyå€¼è¯„ä¼°å±‚æ•æ„Ÿæ€§å’Œå±‚é—´äº¤äº’ï¼ŒæŒ‡å¯¼é‡åŒ–ç­–ç•¥ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒCoopQåœ¨Llama-3ã€Gemma-2å’ŒQwen-3ç­‰æ¨¡å‹ä¸Šï¼Œç›¸æ¯”ç°æœ‰æ–¹æ³•ï¼Œæ˜¾è‘—é™ä½äº†ä½æ¯”ç‰¹é‡åŒ–ä¸‹çš„å›°æƒ‘åº¦ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)å±•ç°äº†å¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†å…¶æ•°åäº¿çš„å‚æ•°è§„æ¨¡ä½¿å¾—åœ¨è®¾å¤‡ç«¯æˆ–ä½èµ„æºç¯å¢ƒä¸‹çš„éƒ¨ç½²å˜å¾—å›°éš¾ã€‚æ··åˆç²¾åº¦é‡åŒ–æä¾›äº†ä¸€ä¸ªæœ‰å¸å¼•åŠ›çš„è§£å†³æ–¹æ¡ˆï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨å¹³å‡ç²¾åº¦é™è‡³4æ¯”ç‰¹ä»¥ä¸‹æ—¶è¡¨ç°ä¸ä½³ï¼Œå› ä¸ºå®ƒä»¬ä¾èµ–äºå­¤ç«‹çš„ã€ç‰¹å®šäºå±‚çš„æŒ‡æ ‡ï¼Œå¿½ç•¥äº†å½±å“æ•´ä½“æ€§èƒ½çš„å…³é”®å±‚é—´äº¤äº’ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬é¦–å…ˆå°†æ··åˆç²¾åº¦é‡åŒ–é—®é¢˜æ„å»ºä¸ºå±‚ä¹‹é—´çš„åˆä½œåšå¼ˆï¼Œå¹¶å¼•å…¥åŸºäºShapleyå€¼çš„æ¸è¿›é‡åŒ–ä¼°è®¡(SPQE)ï¼Œä»¥æœ‰æ•ˆåœ°è·å¾—å±‚æ•æ„Ÿæ€§å’Œå±‚é—´äº¤äº’çš„ç²¾ç¡®Shapleyä¼°è®¡ã€‚åˆ©ç”¨SPQEä¼°è®¡ï¼Œæˆ‘ä»¬æå‡ºäº†å—åˆä½œåšå¼ˆå¯å‘çš„æ··åˆç²¾åº¦é‡åŒ–(CoopQ)ï¼Œå®ƒå°†è¿™äº›Shapleyä¼°è®¡è½¬åŒ–ä¸ºäºŒå…ƒäºŒæ¬¡ä¼˜åŒ–å…¬å¼ï¼Œåœ¨ä¸¥æ ¼çš„å†…å­˜çº¦æŸä¸‹ä¸ºå±‚åˆ†é…2æˆ–4æ¯”ç‰¹ç²¾åº¦ã€‚åœ¨Llama-3ã€Gemma-2å’ŒQwen-3æ¨¡å‹ä¸Šï¼Œè·¨ä¸‰ä¸ªç‹¬ç«‹çš„PTQåç«¯(Quantoã€HQQã€GPTQ)è¿›è¡Œçš„å…¨é¢å®éªŒè¡¨æ˜ï¼Œä¸ä»…ä¾èµ–äºå­¤ç«‹æŒ‡æ ‡çš„æ–¹æ³•ç›¸æ¯”ï¼ŒCoopQå…·æœ‰å¯æ‰©å±•æ€§å’Œå§‹ç»ˆä¼˜è¶Šçš„æ€§èƒ½ã€‚åœ¨4æ¯”ç‰¹åˆ°2æ¯”ç‰¹çš„å¹³å‡ç²¾åº¦èŒƒå›´å†…ï¼ŒCoopQç›¸å¯¹äºæœ€ä½³åŸºçº¿é™ä½äº†20-80%çš„å›°æƒ‘åº¦ï¼Œå¹¶ä¸”éšç€æ¯”ç‰¹å®½åº¦çš„æ”¶ç´§ï¼Œè¿™ä¸€ä¼˜åŠ¿è¶Šæ¥è¶Šå¤§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä½æ¯”ç‰¹æ··åˆç²¾åº¦é‡åŒ–æ—¶æ€§èƒ½æ˜¾è‘—ä¸‹é™çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦åŸºäºå­¤ç«‹çš„å±‚çº§æŒ‡æ ‡è¿›è¡Œé‡åŒ–ï¼Œå¿½ç•¥äº†å±‚ä¸å±‚ä¹‹é—´çš„ç›¸äº’ä¾èµ–å…³ç³»ï¼Œå¯¼è‡´æ•´ä½“æ€§èƒ½ä¸ä½³ã€‚å°¤å…¶æ˜¯åœ¨å¹³å‡ç²¾åº¦ä½äº4æ¯”ç‰¹æ—¶ï¼Œè¿™ç§é—®é¢˜æ›´åŠ çªå‡ºã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡å°†æ··åˆç²¾åº¦é‡åŒ–é—®é¢˜å»ºæ¨¡ä¸ºä¸€ä¸ªåˆä½œåšå¼ˆï¼Œå…¶ä¸­æ¯ä¸€å±‚éƒ½æ˜¯ä¸€ä¸ªå‚ä¸è€…ã€‚é€šè¿‡è¯„ä¼°æ¯ä¸€å±‚å¯¹æ•´ä½“æ€§èƒ½çš„è´¡çŒ®ï¼ˆShapleyå€¼ï¼‰ï¼Œå¹¶è€ƒè™‘å±‚é—´çš„ç›¸äº’ä½œç”¨ï¼Œæ¥ç¡®å®šæ¯ä¸€å±‚çš„æœ€ä½³é‡åŒ–ç²¾åº¦ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæ›´å‡†ç¡®åœ°åæ˜ æ¯ä¸€å±‚çš„é‡è¦æ€§ï¼Œä»è€Œä¼˜åŒ–æ•´ä½“é‡åŒ–ç­–ç•¥ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCoopQåŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼š1) Shapley-based Progressive Quantization Estimation (SPQE)ï¼šä½¿ç”¨SPQEæœ‰æ•ˆåœ°ä¼°è®¡æ¯ä¸€å±‚çš„Shapleyå€¼ï¼Œåæ˜ å…¶å¯¹æ¨¡å‹æ€§èƒ½çš„è´¡çŒ®ã€‚SPQEé€šè¿‡é€æ­¥é‡åŒ–ä¸åŒçš„å±‚å­é›†ï¼Œå¹¶è§‚å¯Ÿæ€§èƒ½å˜åŒ–ï¼Œæ¥è¿‘ä¼¼è®¡ç®—Shapleyå€¼ã€‚2) Cooperative Game Inspired Mixed-Precision Quantizationï¼šå°†SPQEä¼°è®¡çš„Shapleyå€¼è½¬åŒ–ä¸ºä¸€ä¸ªäºŒå…ƒäºŒæ¬¡ä¼˜åŒ–é—®é¢˜ï¼Œç›®æ ‡æ˜¯åœ¨æ»¡è¶³å†…å­˜çº¦æŸçš„æ¡ä»¶ä¸‹ï¼Œæœ€å¤§åŒ–æ¨¡å‹çš„æ€§èƒ½ã€‚è¯¥ä¼˜åŒ–é—®é¢˜å†³å®šäº†æ¯ä¸€å±‚åº”è¯¥ä½¿ç”¨2æ¯”ç‰¹è¿˜æ˜¯4æ¯”ç‰¹ç²¾åº¦ã€‚

**å…³é”®åˆ›æ–°**ï¼šCoopQçš„å…³é”®åˆ›æ–°åœ¨äºå°†åˆä½œåšå¼ˆç†è®ºå¼•å…¥åˆ°æ··åˆç²¾åº¦é‡åŒ–ä¸­ã€‚é€šè¿‡Shapleyå€¼æ¥é‡åŒ–å±‚é—´ä¾èµ–å…³ç³»ï¼Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•ä¸­å­¤ç«‹åœ°è¯„ä¼°æ¯ä¸€å±‚çš„å±€é™æ€§ã€‚SPQEç®—æ³•èƒ½å¤Ÿé«˜æ•ˆåœ°ä¼°è®¡Shapleyå€¼ï¼Œä½¿å¾—CoopQèƒ½å¤Ÿåº”ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ã€‚

**å…³é”®è®¾è®¡**ï¼šCoopQä½¿ç”¨äºŒå…ƒäºŒæ¬¡ä¼˜åŒ–æ¥ç¡®å®šæ¯ä¸€å±‚çš„é‡åŒ–ç²¾åº¦ã€‚ç›®æ ‡å‡½æ•°æ˜¯åŸºäºShapleyå€¼æ„å»ºçš„ï¼Œåæ˜ äº†æ¯ä¸€å±‚å¯¹æ¨¡å‹æ€§èƒ½çš„è´¡çŒ®ã€‚çº¦æŸæ¡ä»¶æ˜¯å†…å­˜çº¦æŸï¼Œç¡®ä¿é‡åŒ–åçš„æ¨¡å‹å¤§å°ä¸è¶…è¿‡é¢„è®¾çš„é˜ˆå€¼ã€‚SPQEç®—æ³•é€šè¿‡é€æ­¥é‡åŒ–ä¸åŒçš„å±‚å­é›†ï¼Œå¹¶è§‚å¯Ÿæ€§èƒ½å˜åŒ–ï¼Œæ¥è¿‘ä¼¼è®¡ç®—Shapleyå€¼ã€‚å…·ä½“è€Œè¨€ï¼ŒSPQEé‡‡ç”¨äº†ä¸€ç§progressiveçš„æ–¹å¼ï¼Œé€æ­¥å¢åŠ é‡åŒ–çš„å±‚æ•°ï¼Œä»è€Œå‡å°‘è®¡ç®—å¤æ‚åº¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒCoopQåœ¨Llama-3ã€Gemma-2å’ŒQwen-3ç­‰æ¨¡å‹ä¸Šï¼Œç›¸æ¯”äºç°æœ‰æœ€ä½³åŸºçº¿ï¼Œåœ¨å¹³å‡ç²¾åº¦ä¸º4æ¯”ç‰¹åˆ°2æ¯”ç‰¹çš„èŒƒå›´å†…ï¼Œé™ä½äº†20-80%çš„å›°æƒ‘åº¦ã€‚å¹¶ä¸”ï¼Œéšç€æ¯”ç‰¹å®½åº¦çš„é™ä½ï¼ŒCoopQçš„ä¼˜åŠ¿æ›´åŠ æ˜æ˜¾ï¼Œè¯æ˜äº†å…¶åœ¨ä½æ¯”ç‰¹é‡åŒ–æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

CoopQå¯åº”ç”¨äºåœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šéƒ¨ç½²å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä¾‹å¦‚ç§»åŠ¨è®¾å¤‡ã€åµŒå…¥å¼ç³»ç»Ÿå’Œè¾¹ç¼˜è®¡ç®—è®¾å¤‡ã€‚é€šè¿‡é™ä½æ¨¡å‹å¤§å°å’Œè®¡ç®—å¤æ‚åº¦ï¼ŒCoopQä½¿å¾—è¿™äº›è®¾å¤‡èƒ½å¤Ÿè¿è¡Œæ›´å¼ºå¤§çš„AIæ¨¡å‹ï¼Œä»è€Œå®ç°æ›´æ™ºèƒ½çš„åº”ç”¨ï¼Œå¦‚æœ¬åœ°åŒ–çš„è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½åŠ©æ‰‹å’Œå®æ—¶ç¿»è¯‘ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) promise impressive capabilities, yet their multi-billion-parameter scale makes on-device or low-resource deployment prohibitive. Mixed-precision quantization offers a compelling solution, but existing methods struggle when the average precision drops below four bits, as they rely on isolated, layer-specific metrics that overlook critical inter-layer interactions affecting overall performance. To address these limitations, we first frame the mixed-precision quantization problem as a cooperative game among layers and introduce Shapley-based Progressive Quantization Estimation (SPQE) to efficiently obtain accurate Shapley estimates of layer sensitivities and inter-layer interactions. Leveraging the SPQE estimates, we propose Cooperative Game Inspired Mixed-Precision Quantization (CoopQ) which translates these Shapley estimates into a binary quadratic optimization formulation, assigning either 2 or 4-bit precision to layers under strict memory constraints. Comprehensive experiments conducted on Llama-3, Gemma-2, and Qwen-3 models across three independent PTQ backends (Quanto, HQQ, GPTQ) demonstrate CoopQ's scalability and consistently superior performance compared to methods relying solely on isolated metrics. Across average precisions spanning 4 bit down to 2 bit, CoopQ cuts Perplexity by 20 - 80 % relative to the best baseline, with the margin growing as the bit-width tightens.

