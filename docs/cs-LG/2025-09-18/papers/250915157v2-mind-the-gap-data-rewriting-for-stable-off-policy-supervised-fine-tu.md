---
layout: default
title: Mind the Gap: Data Rewriting for Stable Off-Policy Supervised Fine-Tuning
---

# Mind the Gap: Data Rewriting for Stable Off-Policy Supervised Fine-Tuning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.15157" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.15157v2</a>
  <a href="https://arxiv.org/pdf/2509.15157.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.15157v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.15157v2', 'Mind the Gap: Data Rewriting for Stable Off-Policy Supervised Fine-Tuning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shiwan Zhao, Xuyang Zhao, Jiaming Zhou, Aobo Kong, Qicheng Li, Yong Qin

**åˆ†ç±»**: cs.LG, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-18 (æ›´æ–°: 2025-09-19)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/NKU-HLT/Off-Policy-SFT)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ•°æ®é‡å†™æ¡†æ¶ï¼Œè§£å†³SFTä¸­Off-Policyå­¦ä¹ çš„åˆ†å¸ƒåç§»é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç›‘ç£å¾®è°ƒ` `Off-Policyå­¦ä¹ ` `æ•°æ®é‡å†™` `é‡è¦æ€§é‡‡æ ·` `ç­–ç•¥å¯¹é½` `æ•°å­¦æ¨ç†` `å¤§è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. SFTé¢ä¸´Off-Policyå­¦ä¹ çš„æŒ‘æˆ˜ï¼Œå³è®­ç»ƒæ•°æ®ä¸ç›®æ ‡ç­–ç•¥å­˜åœ¨åˆ†å¸ƒå·®å¼‚ï¼Œå¯¼è‡´è®­ç»ƒä¸ç¨³å®šã€‚
2. è®ºæ–‡æå‡ºæ•°æ®é‡å†™æ¡†æ¶ï¼Œé€šè¿‡å¼•å¯¼æ¨¡å‹é‡æ–°ç”Ÿæˆé”™è¯¯ç­”æ¡ˆï¼Œä¸»åŠ¨ç¼©å°è®­ç»ƒæ•°æ®ä¸ç›®æ ‡ç­–ç•¥çš„å·®è·ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»ŸSFTå’ŒåŠ¨æ€å¾®è°ƒæ–¹æ³•ï¼Œæå‡äº†æ¨¡å‹æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹çš„ç›‘ç£å¾®è°ƒ(SFT)å¯ä»¥è¢«è§†ä¸ºä¸€ä¸ªOff-Policyå­¦ä¹ é—®é¢˜ï¼Œå…¶ä¸­ä¸“å®¶æ¼”ç¤ºæ¥è‡ªå›ºå®šçš„è¡Œä¸ºç­–ç•¥ï¼Œè€Œè®­ç»ƒæ—¨åœ¨ä¼˜åŒ–ç›®æ ‡ç­–ç•¥ã€‚é‡è¦æ€§é‡‡æ ·æ˜¯æ ¡æ­£è¿™ç§åˆ†å¸ƒä¸åŒ¹é…çš„æ ‡å‡†å·¥å…·ï¼Œä½†å¤§çš„ç­–ç•¥å·®è·ä¼šå¯¼è‡´æƒé‡å€¾æ–œã€é«˜æ–¹å·®å’Œä¸ç¨³å®šçš„ä¼˜åŒ–ã€‚ç°æœ‰æ–¹æ³•é€šè¿‡KLæƒ©ç½šæˆ–è£å‰ªæ¥ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œè¿™äº›æ–¹æ³•è¢«åŠ¨åœ°é™åˆ¶æ›´æ–°ï¼Œè€Œä¸æ˜¯ä¸»åŠ¨åœ°ç¼©å°å·®è·ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ•°æ®é‡å†™æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨è®­ç»ƒå‰ä¸»åŠ¨ç¼©å°ç­–ç•¥å·®è·ã€‚å¯¹äºæ¯ä¸ªé—®é¢˜ï¼Œæ­£ç¡®çš„æ¨¡å‹ç”Ÿæˆè§£å†³æ–¹æ¡ˆè¢«ä¿ç•™ä¸ºOn-Policyæ•°æ®ï¼Œè€Œé”™è¯¯çš„è§£å†³æ–¹æ¡ˆé€šè¿‡å¼•å¯¼é‡æ–°è§£å†³æ¥é‡å†™ï¼Œä»…åœ¨éœ€è¦æ—¶æ‰å›é€€åˆ°ä¸“å®¶æ¼”ç¤ºã€‚è¿™ä½¿è®­ç»ƒåˆ†å¸ƒä¸ç›®æ ‡ç­–ç•¥å¯¹é½ï¼Œä»è€Œé™ä½æ–¹å·®å¹¶æé«˜ç¨³å®šæ€§ã€‚ä¸ºäº†å¤„ç†é‡å†™åçš„æ®‹ä½™ä¸åŒ¹é…ï¼Œæˆ‘ä»¬è¿˜åœ¨è®­ç»ƒæœŸé—´åº”ç”¨é‡è¦æ€§é‡‡æ ·ï¼Œå½¢æˆä¸€ä¸ªä¸¤é˜¶æ®µæ–¹æ³•ï¼Œå°†æ•°æ®çº§å¯¹é½ä¸è½»é‡çº§ä¼˜åŒ–çº§æ ¡æ­£ç›¸ç»“åˆã€‚åœ¨äº”ä¸ªæ•°å­¦æ¨ç†åŸºå‡†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œç›¸å¯¹äºvanilla SFTå’Œæœ€å…ˆè¿›çš„åŠ¨æ€å¾®è°ƒ(DFT)æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å…·æœ‰ä¸€è‡´ä¸”æ˜¾è‘—çš„ä¼˜åŠ¿ã€‚æ•°æ®å’Œä»£ç å°†åœ¨https://github.com/NKU-HLT/Off-Policy-SFTä¸Šå‘å¸ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä¸­ï¼Œè®­ç»ƒæ•°æ®ï¼ˆä¸“å®¶æ¼”ç¤ºï¼‰æ¥è‡ªå›ºå®šçš„è¡Œä¸ºç­–ç•¥ï¼Œè€Œç›®æ ‡æ˜¯ä¼˜åŒ–ä¸€ä¸ªä¸åŒçš„ç›®æ ‡ç­–ç•¥ã€‚è¿™æ„æˆäº†ä¸€ä¸ªOff-Policyå­¦ä¹ é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¦‚KLæ•£åº¦æƒ©ç½šæˆ–æ¢¯åº¦è£å‰ªï¼Œè¯•å›¾ç¼“è§£ç”±æ­¤äº§ç”Ÿçš„åˆ†å¸ƒåç§»ï¼Œä½†è¿™äº›æ–¹æ³•æ˜¯è¢«åŠ¨çš„ï¼Œæ— æ³•æœ‰æ•ˆç¼©å°ç­–ç•¥å·®è·ï¼Œå¯¼è‡´è®­ç»ƒä¸ç¨³å®šå’Œæ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæ ¸å¿ƒæ€æƒ³æ˜¯åœ¨è®­ç»ƒå‰ä¸»åŠ¨ç¼©å°ç­–ç•¥å·®è·ã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹äºæ¯ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œå¦‚æœæ¨¡å‹èƒ½å¤Ÿç”Ÿæˆæ­£ç¡®çš„ç­”æ¡ˆï¼Œåˆ™ä¿ç•™è¯¥ç­”æ¡ˆä½œä¸ºOn-Policyæ•°æ®ã€‚å¦‚æœæ¨¡å‹ç”Ÿæˆäº†é”™è¯¯çš„ç­”æ¡ˆï¼Œåˆ™å¼•å¯¼æ¨¡å‹é‡æ–°è§£å†³é—®é¢˜ï¼Œç”Ÿæˆæ­£ç¡®çš„ç­”æ¡ˆã€‚åªæœ‰åœ¨æ¨¡å‹æ— æ³•ç”Ÿæˆæ­£ç¡®ç­”æ¡ˆæ—¶ï¼Œæ‰ä½¿ç”¨ä¸“å®¶æ¼”ç¤ºã€‚è¿™æ ·ï¼Œè®­ç»ƒæ•°æ®æ›´æ¥è¿‘ç›®æ ‡ç­–ç•¥ï¼Œä»è€Œå‡å°‘äº†åˆ†å¸ƒåç§»ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•æ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µæ¡†æ¶ã€‚ç¬¬ä¸€é˜¶æ®µæ˜¯æ•°æ®é‡å†™é˜¶æ®µï¼Œå¦‚ä¸Šæ‰€è¿°ï¼Œé€šè¿‡å¼•å¯¼æ¨¡å‹é‡æ–°è§£å†³é—®é¢˜æ¥ç”Ÿæˆæ›´ç¬¦åˆç›®æ ‡ç­–ç•¥çš„è®­ç»ƒæ•°æ®ã€‚ç¬¬äºŒé˜¶æ®µæ˜¯è®­ç»ƒé˜¶æ®µï¼Œä½¿ç”¨é‡å†™åçš„æ•°æ®è¿›è¡ŒSFTã€‚ä¸ºäº†è¿›ä¸€æ­¥å¤„ç†æ®‹ä½™çš„åˆ†å¸ƒåç§»ï¼Œåœ¨è®­ç»ƒé˜¶æ®µè¿˜ä½¿ç”¨äº†é‡è¦æ€§é‡‡æ ·ã€‚

**å…³é”®åˆ›æ–°**ï¼šå…³é”®åˆ›æ–°åœ¨äºæ•°æ®é‡å†™ç­–ç•¥ï¼Œå®ƒä¸»åŠ¨åœ°å°†è®­ç»ƒæ•°æ®å‘ç›®æ ‡ç­–ç•¥é æ‹¢ï¼Œè€Œä¸æ˜¯è¢«åŠ¨åœ°é™åˆ¶æ›´æ–°ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°ç¼©å°ç­–ç•¥å·®è·ï¼Œä»è€Œæé«˜è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨æ•°æ®å±‚é¢è§£å†³äº†Off-Policyå­¦ä¹ çš„é—®é¢˜ï¼Œè€Œç°æœ‰æ–¹æ³•ä¸»è¦åœ¨ä¼˜åŒ–å±‚é¢è¿›è¡Œè°ƒæ•´ã€‚

**å…³é”®è®¾è®¡**ï¼šæ•°æ®é‡å†™é˜¶æ®µçš„å…³é”®åœ¨äºå¦‚ä½•å¼•å¯¼æ¨¡å‹é‡æ–°è§£å†³é—®é¢˜ã€‚è®ºæ–‡ä¸­å¯èƒ½ä½¿ç”¨äº†ç‰¹å®šçš„æç¤ºå·¥ç¨‹æˆ–çº¦æŸè§£ç æ–¹æ³•æ¥å®ç°è¿™ä¸€ç‚¹ã€‚é‡è¦æ€§é‡‡æ ·åœ¨è®­ç»ƒé˜¶æ®µç”¨äºæ ¡æ­£æ•°æ®é‡å†™åä»ç„¶å­˜åœ¨çš„åˆ†å¸ƒåç§»ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„å¯èƒ½ä¸æ ‡å‡†çš„SFTæ–¹æ³•ç›¸åŒï¼Œä½†è®­ç»ƒæ•°æ®æ˜¯ç»è¿‡é‡å†™åçš„æ•°æ®ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨äº”ä¸ªæ•°å­¦æ¨ç†åŸºå‡†ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¶…è¿‡äº†vanilla SFTå’Œæœ€å…ˆè¿›çš„åŠ¨æ€å¾®è°ƒ(DFT)æ–¹æ³•ã€‚å…·ä½“çš„æ•°æ®æå‡å¹…åº¦éœ€è¦åœ¨è®ºæ–‡ä¸­æŸ¥æ‰¾ï¼Œä½†æ‘˜è¦ä¸­æ˜ç¡®æŒ‡å‡ºæ˜¯â€œä¸€è‡´ä¸”æ˜¾è‘—çš„ä¼˜åŠ¿â€ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦ä½¿ç”¨Off-Policyæ•°æ®è¿›è¡Œæ¨¡å‹å¾®è°ƒçš„åœºæ™¯ï¼Œä¾‹å¦‚æœºå™¨äººæ§åˆ¶ã€å¯¹è¯ç³»ç»Ÿã€æ¨èç³»ç»Ÿç­‰ã€‚é€šè¿‡æ•°æ®é‡å†™ï¼Œå¯ä»¥æé«˜æ¨¡å‹åœ¨è¿™äº›åœºæ™¯ä¸‹çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ï¼Œé™ä½è®­ç»ƒæˆæœ¬ï¼Œå¹¶åŠ é€Ÿæ¨¡å‹çš„éƒ¨ç½²ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Supervised fine-tuning (SFT) of large language models can be viewed as an off-policy learning problem, where expert demonstrations come from a fixed behavior policy while training aims to optimize a target policy. Importance sampling is the standard tool for correcting this distribution mismatch, but large policy gaps lead to skewed weights, high variance, and unstable optimization. Existing methods mitigate this issue with KL penalties or clipping, which passively restrict updates rather than actively reducing the gap. We propose a simple yet effective data rewriting framework that proactively shrinks the policy gap before training. For each problem, correct model-generated solutions are kept as on-policy data, while incorrect ones are rewritten through guided re-solving, falling back to expert demonstrations only when needed. This aligns the training distribution with the target policy, reducing variance and improving stability. To handle residual mismatch after rewriting, we additionally apply importance sampling during training, forming a two-stage approach that combines data-level alignment with lightweight optimization-level correction. Experiments on five mathematical reasoning benchmarks show consistent and significant gains over both vanilla SFT and the state-of-the-art Dynamic Fine-Tuning (DFT) approach. Data and code will be released at https://github.com/NKU-HLT/Off-Policy-SFT.

