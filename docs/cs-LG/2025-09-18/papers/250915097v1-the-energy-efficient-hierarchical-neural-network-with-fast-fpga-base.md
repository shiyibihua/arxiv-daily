---
layout: default
title: The Energy-Efficient Hierarchical Neural Network with Fast FPGA-Based Incremental Learning
---

# The Energy-Efficient Hierarchical Neural Network with Fast FPGA-Based Incremental Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.15097" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.15097v1</a>
  <a href="https://arxiv.org/pdf/2509.15097.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.15097v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.15097v1', 'The Energy-Efficient Hierarchical Neural Network with Fast FPGA-Based Incremental Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mohammad Saleh Vahdatpour, Huaiyuan Chu, Yanqing Zhang

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-18

**å¤‡æ³¨**: Published at IJCNN 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºFPGAåŠ é€Ÿçš„èƒ½é‡é«˜æ•ˆåˆ†å±‚ç¥ç»ç½‘ç»œï¼Œç”¨äºå¿«é€Ÿå¢é‡å­¦ä¹ ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åˆ†å±‚ç¥ç»ç½‘ç»œ` `FPGAåŠ é€Ÿ` `å¢é‡å­¦ä¹ ` `èƒ½é‡æ•ˆç‡` `è¾¹ç¼˜è®¡ç®—`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ·±åº¦å­¦ä¹ æ¨¡å‹æ—¥ç›Šå¢é•¿çš„è®¡ç®—å’Œèƒ½æºéœ€æ±‚æ˜¯å¯æŒç»­AIå‘å±•çš„é‡è¦ç“¶é¢ˆã€‚
2. è®ºæ–‡æå‡ºä¸€ç§æ··åˆæ¡†æ¶ï¼Œç»“åˆåˆ†å±‚åˆ†è§£ã€FPGAåŠ é€Ÿçš„ç›´æ¥æ–¹ç¨‹æ±‚è§£å’Œå¢é‡å­¦ä¹ ï¼Œé™ä½è®¡ç®—æˆæœ¬ã€‚
3. è¯¥æ–¹æ³•åœ¨ä¿æŒæ¨¡å‹é«˜æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ï¼Œé€‚ç”¨äºè¾¹ç¼˜éƒ¨ç½²å’Œå®æ—¶è‡ªé€‚åº”ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ·±åº¦å­¦ä¹ ï¼Œç‰¹åˆ«æ˜¯åŸºç¡€æ¨¡å‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç­‰å¤§è§„æ¨¡æ¶æ„ï¼Œæ—¥ç›Šå¢é•¿çš„è®¡ç®—å’Œèƒ½æºéœ€æ±‚å¯¹å¯æŒç»­æ€§æå‡ºäº†é‡å¤§æŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„åŸºäºæ¢¯åº¦çš„è®­ç»ƒæ–¹æ³•æ•ˆç‡ä½ä¸‹ï¼Œéœ€è¦å¤§é‡çš„è¿­ä»£æ›´æ–°å’Œé«˜åŠŸè€—ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ··åˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†åˆ†å±‚åˆ†è§£ä¸åŸºäºFPGAçš„ç›´æ¥æ–¹ç¨‹æ±‚è§£å’Œå¢é‡å­¦ä¹ ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†ç¥ç»ç½‘ç»œåˆ†ä¸ºä¸¤ä¸ªåŠŸèƒ½å±‚ï¼šä¸‹å±‚é€šè¿‡FPGAä¸Šçš„å•æ­¥æ–¹ç¨‹æ±‚è§£è¿›è¡Œä¼˜åŒ–ï¼Œä»¥å®ç°é«˜æ•ˆä¸”å¯å¹¶è¡ŒåŒ–çš„ç‰¹å¾æå–ï¼Œè€Œä¸Šå±‚é‡‡ç”¨è‡ªé€‚åº”å¢é‡å­¦ä¹ æ¥æ”¯æŒæŒç»­æ›´æ–°ï¼Œæ— éœ€å®Œå…¨é‡æ–°è®­ç»ƒã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤åˆLLMæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨ä¸¤ä¸ªå±‚æ¬¡ä¸Šæ˜¾å¼éƒ¨ç½²LLMæ¨¡å—ã€‚ä¸‹å±‚LLMä»¥æœ€å°çš„èƒ½é‡å¼€é”€å¤„ç†å¯é‡ç”¨çš„è¡¨ç¤ºå­¦ä¹ ï¼Œè€Œä¸Šå±‚LLMé€šè¿‡èƒ½é‡æ„ŸçŸ¥çš„æ›´æ–°æ‰§è¡Œè‡ªé€‚åº”å†³ç­–ã€‚è¿™ç§é›†æˆè®¾è®¡å¢å¼ºäº†å¯æ‰©å±•æ€§ï¼Œå‡å°‘äº†å†—ä½™è®¡ç®—ï¼Œå¹¶ç¬¦åˆå¯æŒç»­AIçš„åŸåˆ™ã€‚ç†è®ºåˆ†æå’Œæ¶æ„è§è§£è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¿æŒé«˜æ¨¡å‹æ€§èƒ½çš„åŒæ—¶æ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ï¼Œä½¿å…¶éå¸¸é€‚åˆè¾¹ç¼˜éƒ¨ç½²å’Œèƒ½æºå—é™ç¯å¢ƒä¸­çš„å®æ—¶è‡ªé€‚åº”ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºå’Œèƒ½é‡æ¶ˆè€—ï¼Œä¼ ç»Ÿçš„åŸºäºæ¢¯åº¦çš„è®­ç»ƒæ–¹æ³•æ•ˆç‡ä½ä¸‹ï¼Œéš¾ä»¥æ»¡è¶³å¯æŒç»­AIçš„éœ€æ±‚ã€‚å¦‚ä½•åœ¨ä¿è¯æ¨¡å‹æ€§èƒ½çš„å‰æä¸‹ï¼Œé™ä½è®¡ç®—æˆæœ¬å’Œèƒ½æºæ¶ˆè€—ï¼Œæ˜¯æœ¬æ–‡è¦è§£å†³çš„æ ¸å¿ƒé—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†ç¥ç»ç½‘ç»œè¿›è¡Œåˆ†å±‚åˆ†è§£ï¼Œä¸‹å±‚è´Ÿè´£é«˜æ•ˆçš„ç‰¹å¾æå–ï¼Œä¸Šå±‚è´Ÿè´£è‡ªé€‚åº”å†³ç­–ã€‚ä¸‹å±‚é‡‡ç”¨FPGAåŠ é€Ÿçš„ç›´æ¥æ–¹ç¨‹æ±‚è§£ï¼Œå®ç°å•æ­¥ä¼˜åŒ–ï¼Œå‡å°‘è¿­ä»£æ¬¡æ•°ï¼›ä¸Šå±‚é‡‡ç”¨å¢é‡å­¦ä¹ ï¼Œæ”¯æŒæŒç»­æ›´æ–°ï¼Œé¿å…å®Œå…¨é‡æ–°è®­ç»ƒã€‚é€šè¿‡è¿™ç§åˆ†å±‚ç»“æ„å’Œæ··åˆè®­ç»ƒæ–¹æ³•ï¼Œé™ä½æ•´ä½“çš„è®¡ç®—å¤æ‚åº¦å’Œèƒ½é‡æ¶ˆè€—ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦å±‚æ¬¡ï¼šä¸‹å±‚ä¸ºç‰¹å¾æå–å±‚ï¼Œé‡‡ç”¨FPGAåŠ é€Ÿçš„ç›´æ¥æ–¹ç¨‹æ±‚è§£æ–¹æ³•è¿›è¡Œä¼˜åŒ–ï¼›ä¸Šå±‚ä¸ºå†³ç­–å±‚ï¼Œé‡‡ç”¨è‡ªé€‚åº”å¢é‡å­¦ä¹ æ–¹æ³•è¿›è¡Œæ›´æ–°ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†å¤åˆLLMæ¡†æ¶ï¼Œå°†LLMæ¨¡å—éƒ¨ç½²åœ¨ä¸¤ä¸ªå±‚æ¬¡ä¸Šï¼Œä¸‹å±‚LLMè´Ÿè´£å¯é‡ç”¨çš„è¡¨ç¤ºå­¦ä¹ ï¼Œä¸Šå±‚LLMè´Ÿè´£èƒ½é‡æ„ŸçŸ¥çš„è‡ªé€‚åº”å†³ç­–ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†åˆ†å±‚åˆ†è§£ã€FPGAåŠ é€Ÿå’Œå¢é‡å­¦ä¹ ç›¸ç»“åˆï¼Œæ„å»ºäº†ä¸€ç§èƒ½é‡é«˜æ•ˆçš„ç¥ç»ç½‘ç»œæ¶æ„ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºæ¢¯åº¦çš„è®­ç»ƒæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬å’Œèƒ½æºæ¶ˆè€—ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹æ€§èƒ½ã€‚å¤åˆLLMæ¡†æ¶çš„æå‡ºï¼Œè¿›ä¸€æ­¥æå‡äº†æ¨¡å‹çš„å¯æ‰©å±•æ€§å’Œé€‚åº”æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šä¸‹å±‚FPGAåŠ é€Ÿçš„ç›´æ¥æ–¹ç¨‹æ±‚è§£ï¼Œå…·ä½“å®ç°æ–¹å¼æœªçŸ¥ï¼Œä½†å¼ºè°ƒäº†å•æ­¥ä¼˜åŒ–å’Œå¹¶è¡Œè®¡ç®—çš„ä¼˜åŠ¿ã€‚ä¸Šå±‚è‡ªé€‚åº”å¢é‡å­¦ä¹ çš„å…·ä½“ç®—æ³•æœªçŸ¥ï¼Œä½†å¼ºè°ƒäº†æ”¯æŒæŒç»­æ›´æ–°å’Œé¿å…å®Œå…¨é‡æ–°è®­ç»ƒçš„ç‰¹æ€§ã€‚å¤åˆLLMæ¡†æ¶ä¸­ï¼ŒLLMæ¨¡å—çš„å…·ä½“é€‰æ‹©å’Œé…ç½®æœªçŸ¥ï¼Œä½†å¼ºè°ƒäº†èƒ½é‡æ„ŸçŸ¥çš„æ›´æ–°æœºåˆ¶ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡é€šè¿‡ç†è®ºåˆ†æå’Œæ¶æ„è®¾è®¡ï¼ŒéªŒè¯äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œä½†å…·ä½“çš„å®éªŒæ•°æ®å’Œå¯¹æ¯”åŸºçº¿æœªçŸ¥ã€‚è®ºæ–‡å£°ç§°è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨ä¿æŒé«˜æ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬ï¼Œä½¿å…¶éå¸¸é€‚åˆè¾¹ç¼˜éƒ¨ç½²å’Œèƒ½æºå—é™ç¯å¢ƒä¸­çš„å®æ—¶è‡ªé€‚åº”ã€‚å…·ä½“çš„æ€§èƒ½æå‡å¹…åº¦æœªçŸ¥ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè¾¹ç¼˜è®¡ç®—è®¾å¤‡ã€åµŒå…¥å¼ç³»ç»Ÿå’Œç§»åŠ¨è®¾å¤‡ç­‰èƒ½æºå—é™çš„ç¯å¢ƒä¸­ï¼Œå®ç°é«˜æ•ˆçš„å®æ—¶æ¨ç†å’Œè‡ªé€‚åº”å­¦ä¹ ã€‚ä¾‹å¦‚ï¼Œå¯ç”¨äºæ™ºèƒ½ç›‘æ§ã€è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªç­‰é¢†åŸŸï¼Œæå‡è®¾å¤‡åœ¨èµ„æºæœ‰é™æ¡ä»¶ä¸‹çš„æ™ºèƒ½åŒ–æ°´å¹³ï¼Œå¹¶é™ä½ç¢³æ’æ”¾ï¼Œä¿ƒè¿›å¯æŒç»­AIçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The rising computational and energy demands of deep learning, particularly in large-scale architectures such as foundation models and large language models (LLMs), pose significant challenges to sustainability. Traditional gradient-based training methods are inefficient, requiring numerous iterative updates and high power consumption. To address these limitations, we propose a hybrid framework that combines hierarchical decomposition with FPGA-based direct equation solving and incremental learning. Our method divides the neural network into two functional tiers: lower layers are optimized via single-step equation solving on FPGAs for efficient and parallelizable feature extraction, while higher layers employ adaptive incremental learning to support continual updates without full retraining. Building upon this foundation, we introduce the Compound LLM framework, which explicitly deploys LLM modules across both hierarchy levels. The lower-level LLM handles reusable representation learning with minimal energy overhead, while the upper-level LLM performs adaptive decision-making through energy-aware updates. This integrated design enhances scalability, reduces redundant computation, and aligns with the principles of sustainable AI. Theoretical analysis and architectural insights demonstrate that our method reduces computational costs significantly while preserving high model performance, making it well-suited for edge deployment and real-time adaptation in energy-constrained environments.

