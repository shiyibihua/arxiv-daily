---
layout: default
title: Self-Improving Embodied Foundation Models
---

# Self-Improving Embodied Foundation Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.15155" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.15155v1</a>
  <a href="https://arxiv.org/pdf/2509.15155.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.15155v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.15155v1', 'Self-Improving Embodied Foundation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Seyed Kamyar Seyed Ghasemipour, Ayzaan Wahid, Jonathan Tompson, Pannag Sanketi, Igor Mordatch

**åˆ†ç±»**: cs.LG, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-18

**å¤‡æ³¨**: Appearing in the Conference on Neural Information Processing Systems (NeurIPS 2025)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªæå‡å…·èº«åŸºç¡€æ¨¡å‹ï¼Œé€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒå®ç°æœºå™¨äººè‡ªä¸»æŠ€èƒ½å­¦ä¹ ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å…·èº«æ™ºèƒ½` `åŸºç¡€æ¨¡å‹` `æœºå™¨äººå­¦ä¹ ` `è‡ªä¸»å­¦ä¹ ` `å¼ºåŒ–å­¦ä¹ ` `æ¨¡ä»¿å­¦ä¹ ` `æ­¥æ•°é¢„æµ‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æœºå™¨äººæ§åˆ¶æ–¹æ³•ä¾èµ–å¤§é‡äººå·¥æ ‡æ³¨æ•°æ®ï¼Œæ³›åŒ–æ€§å·®ï¼Œè€Œå…·èº«åŸºç¡€æ¨¡å‹åœ¨ä½çº§æ§åˆ¶ä¸­çš„åº”ç”¨ä»å—é™ã€‚
2. å—å¤§å‹è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ å¾®è°ƒçš„å¯å‘ï¼Œæå‡ºä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼šç›‘ç£å¾®è°ƒå’Œè‡ªæå‡ï¼Œæå‡æ¨¡å‹æ€§èƒ½ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ¯”å•çº¯å¢åŠ æ¨¡ä»¿æ•°æ®æ›´é«˜æ•ˆï¼ŒæˆåŠŸç‡æ›´é«˜ï¼Œå¹¶èƒ½è‡ªä¸»å­¦ä¹ æ–°æŠ€èƒ½ï¼Œå…·å¤‡æ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µåè®­ç»ƒæ–¹æ³•ï¼Œç”¨äºæå‡å…·èº«åŸºç¡€æ¨¡å‹åœ¨æœºå™¨äººä½çº§æ§åˆ¶ä¸­çš„æ€§èƒ½ã€‚ç¬¬ä¸€é˜¶æ®µæ˜¯ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œåˆ©ç”¨è¡Œä¸ºå…‹éš†å’Œæ­¥æ•°é¢„æµ‹ç›®æ ‡å¯¹é¢„è®­ç»ƒçš„åŸºç¡€æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚ç¬¬äºŒé˜¶æ®µæ˜¯è‡ªæå‡ï¼Œæ­¥æ•°é¢„æµ‹èƒ½å¤Ÿæå–å‡ºè‰¯å¥½å½¢çŠ¶çš„å¥–åŠ±å‡½æ•°å’Œé²æ£’çš„æˆåŠŸæ£€æµ‹å™¨ï¼Œä»è€Œä½¿æœºå™¨äººé›†ç¾¤èƒ½å¤Ÿåœ¨æœ€å°‘çš„äººå·¥ç›‘ç£ä¸‹è‡ªä¸»åœ°ç»ƒä¹ ä¸‹æ¸¸ä»»åŠ¡ã€‚åœ¨çœŸå®å’Œæ¨¡æ‹Ÿæœºå™¨äººä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å…·èº«åŸºç¡€æ¨¡å‹ä¸Šå–å¾—äº†æ˜¾è‘—æˆæœã€‚ä¸æ‰©å±•æ¨¡ä»¿æ•°æ®æ”¶é›†ç›¸æ¯”ï¼ŒSFTå’Œè‡ªæå‡çš„ç»“åˆæ›´å…·æ ·æœ¬æ•ˆç‡ï¼Œå¹¶èƒ½äº§ç”Ÿå…·æœ‰æ›´é«˜æˆåŠŸç‡çš„ç­–ç•¥ã€‚æ­¤å¤–ï¼Œç½‘ç»œè§„æ¨¡é¢„è®­ç»ƒå’Œè‡ªæå‡çš„ç»“åˆæ˜¯å®ç°è¿™ç§æ ·æœ¬æ•ˆç‡çš„å…³é”®ã€‚è¯¥æ–¹æ³•è¿˜èƒ½å¤Ÿè‡ªä¸»åœ°ç»ƒä¹ å’Œè·å–æ–°çš„æŠ€èƒ½ï¼Œè¿™äº›æŠ€èƒ½å¯ä»¥æ³›åŒ–åˆ°è®­ç»ƒæœŸé—´ä½¿ç”¨çš„æ¨¡ä»¿å­¦ä¹ æ•°æ®é›†ä¸­è§‚å¯Ÿåˆ°çš„è¡Œä¸ºä¹‹å¤–ã€‚è¿™äº›å‘ç°çªå‡ºäº†å°†é¢„è®­ç»ƒçš„åŸºç¡€æ¨¡å‹ä¸åœ¨çº¿è‡ªæå‡ç›¸ç»“åˆä»¥å®ç°æœºå™¨äººè‡ªä¸»æŠ€èƒ½è·å–çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æœºå™¨äººæ§åˆ¶æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯è¡Œä¸ºå…‹éš†ï¼Œä¾èµ–äºå¤§é‡çš„ä¸“å®¶æ¼”ç¤ºæ•°æ®ï¼Œæˆæœ¬é«˜æ˜‚ä¸”æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚å…·èº«åŸºç¡€æ¨¡å‹è™½ç„¶åœ¨å…¶ä»–é¢†åŸŸè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æœºå™¨äººä½çº§æ§åˆ¶ä»»åŠ¡ä¸­çš„åº”ç”¨ä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œç¼ºä¹æœ‰æ•ˆçš„è®­ç»ƒæ–¹æ³•æ¥å……åˆ†åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„æ½œåŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šå€Ÿé‰´å¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›è¡Œå¾®è°ƒçš„æˆåŠŸç»éªŒï¼Œè®ºæ–‡æå‡ºä¸€ç§ä¸¤é˜¶æ®µçš„åè®­ç»ƒæ–¹æ³•ï¼Œå³ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œè‡ªæå‡ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ï¼Œç»“åˆå°‘é‡æ¨¡ä»¿æ•°æ®å’Œè‡ªä¸»æ¢ç´¢ï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿè‡ªä¸»åœ°å­¦ä¹ å’Œæå‡æŠ€èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼š
1. **ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰**ï¼šä½¿ç”¨è¡Œä¸ºå…‹éš†å’Œæ­¥æ•°é¢„æµ‹ä½œä¸ºè¾…åŠ©ç›®æ ‡ï¼Œå¯¹é¢„è®­ç»ƒçš„å…·èº«åŸºç¡€æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚è¡Œä¸ºå…‹éš†ç”¨äºå­¦ä¹ æ¨¡ä»¿æ•°æ®é›†ä¸­çš„è¡Œä¸ºï¼Œæ­¥æ•°é¢„æµ‹åˆ™ç”¨äºä¼°è®¡å½“å‰çŠ¶æ€è·ç¦»ç›®æ ‡çŠ¶æ€çš„è·ç¦»ï¼Œä»è€Œæä¾›æ›´ä¸°å¯Œçš„ç›‘ç£ä¿¡å·ã€‚
2. **è‡ªæå‡**ï¼šåˆ©ç”¨SFTé˜¶æ®µè®­ç»ƒçš„æ­¥æ•°é¢„æµ‹æ¨¡å‹ï¼Œæ„å»ºå¥–åŠ±å‡½æ•°å’ŒæˆåŠŸæ£€æµ‹å™¨ã€‚ç„¶åï¼Œä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆä¾‹å¦‚PPOï¼‰è®©æœºå™¨äººè‡ªä¸»åœ°ä¸ç¯å¢ƒäº¤äº’ï¼Œé€šè¿‡æœ€å¤§åŒ–å¥–åŠ±æ¥ä¸æ–­æå‡ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†é¢„è®­ç»ƒçš„å…·èº«åŸºç¡€æ¨¡å‹ä¸åœ¨çº¿è‡ªæå‡ç›¸ç»“åˆï¼Œå®ç°äº†æœºå™¨äººè‡ªä¸»æŠ€èƒ½å­¦ä¹ ã€‚ä¸ä¼ ç»Ÿçš„æ¨¡ä»¿å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ï¼Œå¹¶é€šè¿‡è‡ªä¸»æ¢ç´¢æ¥å‘ç°æ–°çš„æŠ€èƒ½ï¼Œä»è€Œçªç ´äº†æ¨¡ä»¿æ•°æ®é›†çš„é™åˆ¶ã€‚

**å…³é”®è®¾è®¡**ï¼š
* **æ­¥æ•°é¢„æµ‹**ï¼šä½¿ç”¨ä¸€ä¸ªç¥ç»ç½‘ç»œæ¥é¢„æµ‹å½“å‰çŠ¶æ€è·ç¦»ç›®æ ‡çŠ¶æ€çš„æ­¥æ•°ï¼Œè¯¥é¢„æµ‹å€¼è¢«ç”¨ä½œSFTé˜¶æ®µçš„è¾…åŠ©æŸå¤±å‡½æ•°ï¼Œå¹¶åœ¨è‡ªæå‡é˜¶æ®µç”¨äºæ„å»ºå¥–åŠ±å‡½æ•°ã€‚
* **å¥–åŠ±å‡½æ•°è®¾è®¡**ï¼šå¥–åŠ±å‡½æ•°åŸºäºæ­¥æ•°é¢„æµ‹çš„è´Ÿå˜åŒ–é‡ï¼Œé¼“åŠ±æœºå™¨äººæœç€ç›®æ ‡çŠ¶æ€å‰è¿›ã€‚åŒæ—¶ï¼Œä½¿ç”¨æˆåŠŸæ£€æµ‹å™¨æ¥åˆ¤æ–­ä»»åŠ¡æ˜¯å¦å®Œæˆï¼Œå¹¶ç»™äºˆé¢å¤–çš„å¥–åŠ±ã€‚
* **å¼ºåŒ–å­¦ä¹ ç®—æ³•**ï¼šä½¿ç”¨è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰ç®—æ³•è¿›è¡Œç­–ç•¥ä¼˜åŒ–ï¼Œé€šè¿‡è£å‰ªç­–ç•¥æ›´æ–°å¹…åº¦æ¥ä¿è¯è®­ç»ƒçš„ç¨³å®šæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨çœŸå®å’Œæ¨¡æ‹Ÿæœºå™¨äººä¸Šå‡å–å¾—äº†æ˜¾è‘—æˆæœã€‚ä¸å•çº¯å¢åŠ æ¨¡ä»¿æ•°æ®ç›¸æ¯”ï¼ŒSFTå’Œè‡ªæå‡çš„ç»“åˆåœ¨æ ·æœ¬æ•ˆç‡ä¸Šæå‡æ˜¾è‘—ï¼ŒæˆåŠŸç‡ä¹Ÿæ›´é«˜ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä½¿æœºå™¨äººè‡ªä¸»å­¦ä¹ æ–°çš„æŠ€èƒ½ï¼Œè¿™äº›æŠ€èƒ½å¯ä»¥æ³›åŒ–åˆ°æ¨¡ä»¿æ•°æ®é›†ä¸­æœªè§‚å¯Ÿåˆ°çš„è¡Œä¸ºï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨è‡ªä¸»æŠ€èƒ½è·å–æ–¹é¢çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºå„ç§æœºå™¨äººè‡ªä¸»æ“ä½œä»»åŠ¡ï¼Œä¾‹å¦‚å®¶åº­æœåŠ¡æœºå™¨äººã€å·¥ä¸šè‡ªåŠ¨åŒ–æœºå™¨äººå’ŒåŒ»ç–—æœºå™¨äººç­‰ã€‚é€šè¿‡è‡ªä¸»å­¦ä¹ å’Œæå‡æŠ€èƒ½ï¼Œæœºå™¨äººèƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”å¤æ‚å¤šå˜çš„ç¯å¢ƒï¼Œå®Œæˆå„ç§ä»»åŠ¡ï¼Œä»è€Œæé«˜ç”Ÿäº§æ•ˆç‡å’ŒæœåŠ¡è´¨é‡ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›æ¨åŠ¨æœºå™¨äººæŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ï¼Œå®ç°æ›´æ™ºèƒ½ã€æ›´è‡ªä¸»çš„æœºå™¨äººç³»ç»Ÿã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Foundation models trained on web-scale data have revolutionized robotics, but their application to low-level control remains largely limited to behavioral cloning. Drawing inspiration from the success of the reinforcement learning stage in fine-tuning large language models, we propose a two-stage post-training approach for robotics. The first stage, Supervised Fine-Tuning (SFT), fine-tunes pretrained foundation models using both: a) behavioral cloning, and b) steps-to-go prediction objectives. In the second stage, Self-Improvement, steps-to-go prediction enables the extraction of a well-shaped reward function and a robust success detector, enabling a fleet of robots to autonomously practice downstream tasks with minimal human supervision. Through extensive experiments on real-world and simulated robot embodiments, our novel post-training recipe unveils significant results on Embodied Foundation Models. First, we demonstrate that the combination of SFT and Self-Improvement is significantly more sample-efficient than scaling imitation data collection for supervised learning, and that it leads to policies with significantly higher success rates. Further ablations highlight that the combination of web-scale pretraining and Self-Improvement is the key to this sample-efficiency. Next, we demonstrate that our proposed combination uniquely unlocks a capability that current methods cannot achieve: autonomously practicing and acquiring novel skills that generalize far beyond the behaviors observed in the imitation learning datasets used during training. These findings highlight the transformative potential of combining pretrained foundation models with online Self-Improvement to enable autonomous skill acquisition in robotics. Our project website can be found at https://self-improving-efms.github.io .

