---
layout: default
title: Modeling Transformers as complex networks to analyze learning dynamics
---

# Modeling Transformers as complex networks to analyze learning dynamics

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.15269" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.15269v1</a>
  <a href="https://arxiv.org/pdf/2509.15269.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.15269v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.15269v1', 'Modeling Transformers as complex networks to analyze learning dynamics')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Elisabetta Rocchetti

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**å°†Transformerå»ºæ¨¡ä¸ºå¤æ‚ç½‘ç»œï¼Œåˆ†æLLMçš„å­¦ä¹ åŠ¨æ€**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å¤æ‚ç½‘ç»œ` `æœºç†å¯è§£é‡Šæ€§` `å­¦ä¹ åŠ¨æ€` `Transformer` `å› æœæ¨æ–­` `å›¾è®º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤§å‹è¯­è¨€æ¨¡å‹å­¦ä¹ åŠ¨æ€çš„æœºç†å¯è§£é‡Šæ€§ç ”ç©¶é¢ä¸´æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆç†è§£æ¨¡å‹å†…éƒ¨çš„å¤æ‚äº¤äº’ã€‚
2. è®ºæ–‡æå‡ºå°†Transformeræ¨¡å‹è¡¨ç¤ºä¸ºå¤æ‚ç½‘ç»œï¼Œé€šè¿‡åˆ†æç½‘ç»œæ‹“æ‰‘ç»“æ„çš„å˜åŒ–æ¥ç†è§£æ¨¡å‹çš„å­¦ä¹ è¿‡ç¨‹ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ­ç¤ºæ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸åŒé˜¶æ®µçš„æ¼”åŒ–æ¨¡å¼ï¼Œå¹¶è¯†åˆ«å…³é”®çš„ä¿¡æ¯ä¼ æ’­å’Œæ”¶é›†ç»„ä»¶ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ä»å¤æ‚ç½‘ç»œç†è®ºï¼ˆCNTï¼‰çš„è§’åº¦ç ”ç©¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è·å¾—å¤æ‚èƒ½åŠ›çš„è¿‡ç¨‹ã€‚æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œå°†åŸºäºTransformerçš„LLMè¡¨ç¤ºä¸ºä¸€ä¸ªæœ‰å‘åŠ æƒå›¾ï¼Œå…¶ä¸­èŠ‚ç‚¹æ˜¯æ¨¡å‹çš„è®¡ç®—ç»„ä»¶ï¼ˆæ³¨æ„åŠ›å¤´å’ŒMLPï¼‰ï¼Œè¾¹è¡¨ç¤ºå› æœå½±å“ï¼Œé€šè¿‡åŸºäºå¹²é¢„çš„æ¶ˆèæŠ€æœ¯æµ‹é‡ã€‚é€šè¿‡è·Ÿè¸ªPythia-14Mæ¨¡å‹åœ¨å…¸å‹å½’çº³ä»»åŠ¡ä¸Šçš„143ä¸ªè®­ç»ƒæ£€æŸ¥ç‚¹çš„ç»„ä»¶å›¾çš„æ¼”å˜ï¼Œåˆ†æäº†ä¸€ç³»åˆ—å›¾è®ºæŒ‡æ ‡ã€‚ç»“æœè¡¨æ˜ï¼Œç½‘ç»œç»“æ„ç»å†äº†æ¢ç´¢ã€å·©å›ºå’Œç»†åŒ–ç­‰ä¸åŒçš„é˜¶æ®µã€‚å…·ä½“è€Œè¨€ï¼Œè¯†åˆ«å‡ºç¨³å®šçš„ä¿¡æ¯ä¼ æ’­å™¨ç»„ä»¶å±‚æ¬¡ç»“æ„å’ŒåŠ¨æ€çš„ä¿¡æ¯æ”¶é›†å™¨ç»„ä»¶é›†åˆï¼Œè¿™äº›ç»„ä»¶çš„è§’è‰²åœ¨å…³é”®çš„å­¦ä¹ èŠ‚ç‚¹ä¸Šé‡æ–°é…ç½®ã€‚è¿™é¡¹å·¥ä½œè¡¨æ˜ï¼Œç»„ä»¶çº§åˆ«çš„ç½‘ç»œè§†è§’ä¸ºå¯è§†åŒ–å’Œç†è§£é©±åŠ¨LLMä¸­åŠŸèƒ½ç”µè·¯å½¢æˆçš„è‡ªç»„ç»‡åŸåˆ™æä¾›äº†ä¸€ä¸ªå¼ºå¤§çš„å®è§‚è§†è§’ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆç†è§£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¦‚ä½•è·å¾—å¤æ‚èƒ½åŠ›ï¼Œç¼ºä¹å¯¹æ¨¡å‹å†…éƒ¨ç»„ä»¶ä¹‹é—´äº¤äº’çš„å®è§‚è§†è§’ã€‚ä¼ ç»Ÿçš„åˆ†ææ–¹æ³•å¾€å¾€éš¾ä»¥æ­ç¤ºæ¨¡å‹å­¦ä¹ åŠ¨æ€çš„æœ¬è´¨ï¼Œé˜»ç¢äº†å¯¹æ¨¡å‹è¡Œä¸ºçš„æ·±å…¥ç†è§£ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†Transformeræ¨¡å‹è§†ä¸ºä¸€ä¸ªå¤æ‚ç½‘ç»œï¼Œå…¶ä¸­æ¨¡å‹çš„è®¡ç®—ç»„ä»¶ï¼ˆæ³¨æ„åŠ›å¤´å’ŒMLPï¼‰ä½œä¸ºèŠ‚ç‚¹ï¼Œç»„ä»¶ä¹‹é—´çš„å› æœå½±å“ä½œä¸ºè¾¹ã€‚é€šè¿‡åˆ†æè¿™ä¸ªç½‘ç»œçš„æ‹“æ‰‘ç»“æ„å’Œæ¼”åŒ–è¿‡ç¨‹ï¼Œå¯ä»¥æ­ç¤ºæ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å­¦ä¹ åŠ¨æ€çš„æ¨¡å¼ï¼Œå¹¶è¯†åˆ«å…³é”®çš„ç»„ä»¶åŠå…¶ä½œç”¨ã€‚è¿™ç§æ–¹æ³•æä¾›äº†ä¸€ç§å®è§‚çš„è§†è§’ï¼Œæœ‰åŠ©äºç†è§£æ¨¡å‹å†…éƒ¨çš„è‡ªç»„ç»‡åŸåˆ™ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) å°†Transformeræ¨¡å‹è¡¨ç¤ºä¸ºä¸€ä¸ªæœ‰å‘åŠ æƒå›¾ï¼ŒèŠ‚ç‚¹ä¸ºæ¨¡å‹çš„è®¡ç®—ç»„ä»¶ï¼Œè¾¹è¡¨ç¤ºç»„ä»¶ä¹‹é—´çš„å› æœå½±å“ã€‚2) ä½¿ç”¨åŸºäºå¹²é¢„çš„æ¶ˆèæŠ€æœ¯æ¥æµ‹é‡ç»„ä»¶ä¹‹é—´çš„å› æœå½±å“ï¼Œç¡®å®šè¾¹çš„æƒé‡ã€‚3) åœ¨æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå®šæœŸè®°å½•ç½‘ç»œçš„æ‹“æ‰‘ç»“æ„å’Œæƒé‡ã€‚4) ä½¿ç”¨å›¾è®ºæŒ‡æ ‡ï¼ˆä¾‹å¦‚ï¼ŒèŠ‚ç‚¹ä¸­å¿ƒæ€§ã€èšç±»ç³»æ•°ç­‰ï¼‰æ¥åˆ†æç½‘ç»œçš„æ¼”åŒ–è¿‡ç¨‹ï¼Œè¯†åˆ«å…³é”®çš„å­¦ä¹ é˜¶æ®µå’Œç»„ä»¶ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå°†å¤æ‚ç½‘ç»œç†è®ºåº”ç”¨äºåˆ†æTransformeræ¨¡å‹çš„å­¦ä¹ åŠ¨æ€ã€‚ä¸ä¼ ç»Ÿçš„åˆ†ææ–¹æ³•ç›¸æ¯”ï¼Œè¿™ç§æ–¹æ³•æä¾›äº†ä¸€ç§å®è§‚çš„è§†è§’ï¼Œèƒ½å¤Ÿæ­ç¤ºæ¨¡å‹å†…éƒ¨ç»„ä»¶ä¹‹é—´çš„å¤æ‚äº¤äº’ï¼Œå¹¶è¯†åˆ«å…³é”®çš„å­¦ä¹ é˜¶æ®µå’Œç»„ä»¶ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜æå‡ºäº†ä¸€ç§åŸºäºå¹²é¢„çš„æ¶ˆèæŠ€æœ¯æ¥æµ‹é‡ç»„ä»¶ä¹‹é—´çš„å› æœå½±å“ï¼Œä¸ºæ„å»ºå‡†ç¡®çš„ç½‘ç»œè¡¨ç¤ºæä¾›äº†åŸºç¡€ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä½¿ç”¨Pythia-14Mæ¨¡å‹è¿›è¡Œå®éªŒï¼Œè¯¥æ¨¡å‹åœ¨ä¸€ä¸ªå…¸å‹çš„å½’çº³ä»»åŠ¡ä¸Šè¿›è¡Œè®­ç»ƒã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¯éš”ä¸€æ®µæ—¶é—´è®°å½•æ¨¡å‹çš„ç½‘ç»œç»“æ„ï¼Œå¹¶è®¡ç®—ä¸€ç³»åˆ—å›¾è®ºæŒ‡æ ‡ã€‚è®ºæ–‡è¿˜è®¾è®¡äº†ä¸€å¥—æŒ‡æ ‡æ¥è¡¡é‡ç»„ä»¶çš„ä¿¡æ¯ä¼ æ’­å’Œæ”¶é›†èƒ½åŠ›ï¼Œå¹¶åˆ†æè¿™äº›æŒ‡æ ‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„å˜åŒ–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ­ç¤ºæ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç»å†æ¢ç´¢ã€å·©å›ºå’Œç»†åŒ–ç­‰ä¸åŒé˜¶æ®µã€‚ç ”ç©¶å‘ç°ï¼Œæ¨¡å‹ä¸­å­˜åœ¨ç¨³å®šçš„ä¿¡æ¯ä¼ æ’­å™¨ç»„ä»¶å±‚æ¬¡ç»“æ„å’ŒåŠ¨æ€çš„ä¿¡æ¯æ”¶é›†å™¨ç»„ä»¶é›†åˆï¼Œè¿™äº›ç»„ä»¶çš„è§’è‰²åœ¨å…³é”®çš„å­¦ä¹ èŠ‚ç‚¹ä¸Šé‡æ–°é…ç½®ã€‚é€šè¿‡åˆ†æè¿™äº›ç»„ä»¶çš„æ¼”åŒ–è¿‡ç¨‹ï¼Œå¯ä»¥æ·±å…¥ç†è§£æ¨¡å‹çš„å­¦ä¹ åŠ¨æ€ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹çš„æœºç†å¯è§£é‡Šæ€§ç ”ç©¶ï¼Œå¸®åŠ©ç ”ç©¶äººå‘˜ç†è§£æ¨¡å‹çš„å­¦ä¹ è¿‡ç¨‹å’Œå†…éƒ¨æœºåˆ¶ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºä¼˜åŒ–æ¨¡å‹ç»“æ„ã€æé«˜è®­ç»ƒæ•ˆç‡ï¼Œå¹¶ä¸ºå¼€å‘æ›´å¼ºå¤§çš„LLMæä¾›æŒ‡å¯¼ã€‚è¯¥ç ”ç©¶è¿˜æœ‰åŠ©äºæé«˜äººä»¬å¯¹äººå·¥æ™ºèƒ½ç³»ç»Ÿè¡Œä¸ºçš„ä¿¡ä»»åº¦å’Œå¯æ§æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The process by which Large Language Models (LLMs) acquire complex capabilities during training remains a key open question in mechanistic interpretability. This project investigates whether these learning dynamics can be characterized through the lens of Complex Network Theory (CNT). I introduce a novel methodology to represent a Transformer-based LLM as a directed, weighted graph where nodes are the model's computational components (attention heads and MLPs) and edges represent causal influence, measured via an intervention-based ablation technique. By tracking the evolution of this component-graph across 143 training checkpoints of the Pythia-14M model on a canonical induction task, I analyze a suite of graph-theoretic metrics. The results reveal that the network's structure evolves through distinct phases of exploration, consolidation, and refinement. Specifically, I identify the emergence of a stable hierarchy of information spreader components and a dynamic set of information gatherer components, whose roles reconfigure at key learning junctures. This work demonstrates that a component-level network perspective offers a powerful macroscopic lens for visualizing and understanding the self-organizing principles that drive the formation of functional circuits in LLMs.

