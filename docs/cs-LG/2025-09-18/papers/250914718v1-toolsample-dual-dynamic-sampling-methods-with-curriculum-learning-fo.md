---
layout: default
title: ToolSample: Dual Dynamic Sampling Methods with Curriculum Learning for RL-based Tool Learning
---

# ToolSample: Dual Dynamic Sampling Methods with Curriculum Learning for RL-based Tool Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.14718" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.14718v1</a>
  <a href="https://arxiv.org/pdf/2509.14718.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.14718v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.14718v1', 'ToolSample: Dual Dynamic Sampling Methods with Curriculum Learning for RL-based Tool Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zihao Feng, Xiaoxue Wang, Bowen Wu, Hailong Cao, Tiejun Zhao, Qun Yu, Baoxun Wang

**åˆ†ç±»**: cs.LG, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDSCLæ¡†æ¶ï¼Œé€šè¿‡åŒé‡åŠ¨æ€é‡‡æ ·ä¸è¯¾ç¨‹å­¦ä¹ æå‡RLå·¥å…·å­¦ä¹ æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `å·¥å…·å­¦ä¹ ` `åŠ¨æ€é‡‡æ ·` `è¯¾ç¨‹å­¦ä¹ ` `å¤šä»»åŠ¡å­¦ä¹ ` `LLM` `å¥–åŠ±å‡½æ•°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. åŸºäºå¼ºåŒ–å­¦ä¹ çš„LLMå·¥å…·å­¦ä¹ æ•ˆç‡å—å¤§é‡ç®€å•æ ·æœ¬å½±å“ï¼Œç°æœ‰åŠ¨æ€é‡‡æ ·æ–¹æ³•éš¾ä»¥é€‚åº”å…¶å¤šä»»åŠ¡å’Œç²¾ç»†å¥–åŠ±ç‰¹æ€§ã€‚
2. DSCLæ¡†æ¶é€šè¿‡å¥–åŠ±é©±åŠ¨çš„åŠ¨æ€é‡‡æ ·å’Œä»»åŠ¡é©±åŠ¨çš„åŠ¨æ€è¯¾ç¨‹å­¦ä¹ ï¼Œé’ˆå¯¹æ€§åœ°è§£å†³å·¥å…·å­¦ä¹ ä¸­çš„æ•ˆç‡ç“¶é¢ˆã€‚
3. å®éªŒè¡¨æ˜ï¼ŒDSCLåœ¨BFCLv3åŸºå‡†æµ‹è¯•ä¸­æ€§èƒ½æå‡3.29%ï¼ŒéªŒè¯äº†å…¶åœ¨å·¥å…·å­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºåŠ¨æ€é‡‡æ ·ä¸è¯¾ç¨‹å­¦ä¹ ï¼ˆDSCLï¼‰çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„LLMå·¥å…·å­¦ä¹ ä¸­ï¼Œå› ç®€å•æ ·æœ¬è¿‡å¤šè€Œå¯¼è‡´çš„æ•ˆç‡ä½ä¸‹é—®é¢˜ã€‚ç°æœ‰åŠ¨æ€é‡‡æ ·æŠ€æœ¯éš¾ä»¥é€‚åº”å·¥å…·å­¦ä¹ ä¸­å›ºæœ‰çš„å¤šä»»åŠ¡ç»“æ„å’Œç²¾ç»†åŒ–å¥–åŠ±æœºåˆ¶ã€‚DSCLä¸“é—¨é’ˆå¯¹å·¥å…·å­¦ä¹ çš„ç‰¹ç‚¹è¿›è¡Œè®¾è®¡ï¼ŒåŒ…æ‹¬ç›¸äº’ä¾èµ–çš„å­ä»»åŠ¡å’Œå¤šå€¼å¥–åŠ±å‡½æ•°ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šåŸºäºå¥–åŠ±çš„åŠ¨æ€é‡‡æ ·ï¼Œåˆ©ç”¨å¤šç»´å¥–åŠ±ç»Ÿè®¡ä¿¡æ¯ï¼ˆå‡å€¼å’Œæ–¹å·®ï¼‰æ¥ä¼˜å…ˆé€‰æ‹©æœ‰ä»·å€¼çš„æ•°æ®ï¼›ä»¥åŠåŸºäºä»»åŠ¡çš„åŠ¨æ€è¯¾ç¨‹å­¦ä¹ ï¼Œè‡ªé€‚åº”åœ°å°†è®­ç»ƒé‡ç‚¹æ”¾åœ¨æŒæ¡ç¨‹åº¦è¾ƒä½çš„å­ä»»åŠ¡ä¸Šã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œè¯æ˜DSCLæ˜¾è‘—æé«˜äº†è®­ç»ƒæ•ˆç‡å’Œæ¨¡å‹æ€§èƒ½ï¼Œåœ¨BFCLv3åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†3.29%çš„æ”¹è¿›ã€‚è¯¥æ–¹æ³•æä¾›äº†ä¸€ç§å®šåˆ¶åŒ–çš„è§£å†³æ–¹æ¡ˆï¼Œæœ‰æ•ˆåœ°åˆ©ç”¨äº†å·¥å…·å­¦ä¹ ä¸­å¤æ‚çš„å¥–åŠ±ä¿¡å·å’Œå­ä»»åŠ¡åŠ¨æ€ï¼Œä»è€Œè·å¾—å“è¶Šçš„æˆæœã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šåœ¨åŸºäºå¼ºåŒ–å­¦ä¹ çš„LLMå·¥å…·å­¦ä¹ ä¸­ï¼Œéšç€è®­ç»ƒçš„è¿›è¡Œï¼Œä¼šäº§ç”Ÿå¤§é‡çš„ç®€å•æ ·æœ¬ï¼Œè¿™äº›æ ·æœ¬çš„å­¦ä¹ ä»·å€¼é€æ¸é™ä½ï¼Œå¯¼è‡´è®­ç»ƒæ•ˆç‡ä½ä¸‹ã€‚ç°æœ‰çš„åŠ¨æ€é‡‡æ ·æ–¹æ³•é€šå¸¸æ˜¯ä¸ºé€šç”¨å¼ºåŒ–å­¦ä¹ ä»»åŠ¡è®¾è®¡çš„ï¼Œæ— æ³•æœ‰æ•ˆå¤„ç†å·¥å…·å­¦ä¹ ä¸­å¤æ‚çš„å¤šä»»åŠ¡ç»“æ„å’Œç²¾ç»†åŒ–çš„å¥–åŠ±æœºåˆ¶ï¼Œä¾‹å¦‚ä¸åŒå­ä»»åŠ¡çš„å¥–åŠ±å°ºåº¦ä¸ä¸€è‡´ï¼Œä»¥åŠå¥–åŠ±çš„ç¨€ç–æ€§ç­‰é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDSCLçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å·¥å…·å­¦ä¹ ä¸­ç‰¹æœ‰çš„å¤šä»»åŠ¡ç»“æ„å’Œå¤šå€¼å¥–åŠ±å‡½æ•°ï¼Œè®¾è®¡ä¸€ç§åŒé‡åŠ¨æ€é‡‡æ ·ç­–ç•¥ã€‚é¦–å…ˆï¼Œé€šè¿‡å¥–åŠ±ç»Ÿè®¡ä¿¡æ¯ï¼ˆå‡å€¼å’Œæ–¹å·®ï¼‰æ¥è¯„ä¼°æ ·æœ¬çš„ä»·å€¼ï¼Œä¼˜å…ˆé€‰æ‹©ä¿¡æ¯é‡å¤§çš„æ ·æœ¬è¿›è¡Œè®­ç»ƒã€‚å…¶æ¬¡ï¼Œæ ¹æ®å„ä¸ªå­ä»»åŠ¡çš„æŒæ¡ç¨‹åº¦ï¼ŒåŠ¨æ€è°ƒæ•´è®­ç»ƒçš„é‡ç‚¹ï¼Œä¼˜å…ˆè®­ç»ƒæŒæ¡ç¨‹åº¦è¾ƒä½çš„å­ä»»åŠ¡ï¼Œä»è€Œæé«˜æ•´ä½“çš„å­¦ä¹ æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDSCLæ¡†æ¶ä¸»è¦åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒæ¨¡å—ï¼šå¥–åŠ±é©±åŠ¨çš„åŠ¨æ€é‡‡æ ·ï¼ˆReward-Based Dynamic Samplingï¼‰å’Œä»»åŠ¡é©±åŠ¨çš„åŠ¨æ€è¯¾ç¨‹å­¦ä¹ ï¼ˆTask-Based Dynamic Curriculum Learningï¼‰ã€‚å¥–åŠ±é©±åŠ¨çš„åŠ¨æ€é‡‡æ ·æ¨¡å—è´Ÿè´£æ ¹æ®æ ·æœ¬çš„å¥–åŠ±ç»Ÿè®¡ä¿¡æ¯ï¼Œè®¡ç®—æ ·æœ¬çš„ä¼˜å…ˆçº§ï¼Œå¹¶æ ¹æ®ä¼˜å…ˆçº§é€‰æ‹©æ ·æœ¬è¿›è¡Œè®­ç»ƒã€‚ä»»åŠ¡é©±åŠ¨çš„åŠ¨æ€è¯¾ç¨‹å­¦ä¹ æ¨¡å—è´Ÿè´£æ ¹æ®å„ä¸ªå­ä»»åŠ¡çš„è®­ç»ƒè¿›åº¦ï¼ŒåŠ¨æ€è°ƒæ•´å„ä¸ªå­ä»»åŠ¡çš„è®­ç»ƒæƒé‡ï¼Œä¼˜å…ˆè®­ç»ƒæŒæ¡ç¨‹åº¦è¾ƒä½çš„å­ä»»åŠ¡ã€‚è¿™ä¸¤ä¸ªæ¨¡å—ç›¸äº’åä½œï¼Œå…±åŒæé«˜å·¥å…·å­¦ä¹ çš„æ•ˆç‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šDSCLçš„å…³é”®åˆ›æ–°åœ¨äºå…¶åŒé‡åŠ¨æ€é‡‡æ ·ç­–ç•¥ï¼Œå³åŒæ—¶è€ƒè™‘æ ·æœ¬çš„å¥–åŠ±ä»·å€¼å’Œå­ä»»åŠ¡çš„æŒæ¡ç¨‹åº¦ã€‚ä¸ä¼ ç»Ÿçš„åŠ¨æ€é‡‡æ ·æ–¹æ³•ç›¸æ¯”ï¼ŒDSCLèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨å·¥å…·å­¦ä¹ ä¸­çš„ä¿¡æ¯ï¼Œä»è€Œæé«˜è®­ç»ƒæ•ˆç‡ã€‚æ­¤å¤–ï¼ŒDSCLè¿˜å¼•å…¥äº†è¯¾ç¨‹å­¦ä¹ çš„æ€æƒ³ï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´å­ä»»åŠ¡çš„è®­ç»ƒæƒé‡ï¼Œè¿›ä¸€æ­¥æé«˜äº†å­¦ä¹ æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å¥–åŠ±é©±åŠ¨çš„åŠ¨æ€é‡‡æ ·æ¨¡å—ä¸­ï¼Œä½¿ç”¨äº†å¥–åŠ±çš„å‡å€¼å’Œæ–¹å·®ä½œä¸ºæ ·æœ¬ä»·å€¼çš„è¯„ä¼°æŒ‡æ ‡ã€‚å‡å€¼åæ˜ äº†æ ·æœ¬çš„å¹³å‡å¥–åŠ±ï¼Œæ–¹å·®åæ˜ äº†æ ·æœ¬å¥–åŠ±çš„ä¸ç¡®å®šæ€§ã€‚é€šè¿‡ç»¼åˆè€ƒè™‘å‡å€¼å’Œæ–¹å·®ï¼Œå¯ä»¥æ›´å‡†ç¡®åœ°è¯„ä¼°æ ·æœ¬çš„ä»·å€¼ã€‚åœ¨ä»»åŠ¡é©±åŠ¨çš„åŠ¨æ€è¯¾ç¨‹å­¦ä¹ æ¨¡å—ä¸­ï¼Œä½¿ç”¨äº†å„ä¸ªå­ä»»åŠ¡çš„è®­ç»ƒæŸå¤±ä½œä¸ºå­ä»»åŠ¡æŒæ¡ç¨‹åº¦çš„è¯„ä¼°æŒ‡æ ‡ã€‚è®­ç»ƒæŸå¤±è¶Šå¤§ï¼Œè¯´æ˜å­ä»»åŠ¡çš„æŒæ¡ç¨‹åº¦è¶Šä½ï¼Œåº”è¯¥ç»™äºˆæ›´é«˜çš„è®­ç»ƒæƒé‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒDSCLæ¡†æ¶åœ¨BFCLv3åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç›¸æ¯”äºç°æœ‰æœ€ä½³åŸºçº¿æ–¹æ³•ï¼Œæ€§èƒ½æå‡äº†3.29%ã€‚è¿™è¡¨æ˜DSCLèƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨å·¥å…·å­¦ä¹ ä¸­çš„å¤æ‚å¥–åŠ±ä¿¡å·å’Œå­ä»»åŠ¡åŠ¨æ€ï¼Œä»è€Œæé«˜è®­ç»ƒæ•ˆç‡å’Œæ¨¡å‹æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå®éªŒè¿˜éªŒè¯äº†DSCLæ¡†æ¶çš„å„ä¸ªç»„æˆéƒ¨åˆ†çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DSCLæ¡†æ¶å¯åº”ç”¨äºå„ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„LLMå·¥å…·å­¦ä¹ åœºæ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½ä½“åˆ©ç”¨å¤–éƒ¨APIå®Œæˆå¤æ‚ä»»åŠ¡ã€æœºå™¨äººæ§åˆ¶ç­‰ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæå‡æ¨¡å‹è®­ç»ƒæ•ˆç‡ï¼Œé™ä½è®¡ç®—æˆæœ¬ï¼Œå¹¶æé«˜æ¨¡å‹æ€§èƒ½ï¼Œä»è€ŒåŠ é€Ÿç›¸å…³æŠ€æœ¯çš„è½åœ°å’Œåº”ç”¨ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•è¿˜å¯æ‰©å±•åˆ°å…¶ä»–å¤šä»»åŠ¡å­¦ä¹ å’Œå¥–åŠ±ç¨€ç–çš„ç¯å¢ƒä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> While reinforcement learning (RL) is increasingly used for LLM-based tool learning, its efficiency is often hampered by an overabundance of simple samples that provide diminishing learning value as training progresses. Existing dynamic sampling techniques are ill-suited for the multi-task structure and fine-grained reward mechanisms inherent to tool learning. This paper introduces Dynamic Sampling with Curriculum Learning (DSCL), a framework specifically designed to address this challenge by targeting the unique characteristics of tool learning: its multiple interdependent sub-tasks and multi-valued reward functions. DSCL features two core components: Reward-Based Dynamic Sampling, which uses multi-dimensional reward statistics (mean and variance) to prioritize valuable data, and Task-Based Dynamic Curriculum Learning, which adaptively focuses training on less-mastered sub-tasks. Through extensive experiments, we demonstrate that DSCL significantly improves training efficiency and model performance over strong baselines, achieving a 3.29\% improvement on the BFCLv3 benchmark. Our method provides a tailored solution that effectively leverages the complex reward signals and sub-task dynamics within tool learning to achieve superior results.

