---
layout: default
title: Reinforcement Learning Agent for a 2D Shooter Game
---

# Reinforcement Learning Agent for a 2D Shooter Game

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.15042" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.15042v1</a>
  <a href="https://arxiv.org/pdf/2509.15042.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.15042v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.15042v1', 'Reinforcement Learning Agent for a 2D Shooter Game')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Thomas Ackermann, Moritz Spang, Hamza A. A. Gardi

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç»“åˆæ¨¡ä»¿å­¦ä¹ ä¸å¼ºåŒ–å­¦ä¹ çš„æ··åˆè®­ç»ƒæ–¹æ³•ï¼Œæå‡2Då°„å‡»æ¸¸æˆAIæ™ºèƒ½ä½“æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `æ¨¡ä»¿å­¦ä¹ ` `æ··åˆè®­ç»ƒ` `æ¸¸æˆAI` `å¤šæ™ºèƒ½ä½“` `è¡Œä¸ºå…‹éš†` `æ·±åº¦Qç½‘ç»œ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ åœ¨å¤æ‚æ¸¸æˆç¯å¢ƒä¸­é¢ä¸´å¥–åŠ±ç¨€ç–ã€è®­ç»ƒä¸ç¨³å®šå’Œæ ·æœ¬æ•ˆç‡ä½ç­‰æŒ‘æˆ˜ã€‚
2. è®ºæ–‡æå‡ºä¸€ç§æ··åˆè®­ç»ƒæ–¹æ³•ï¼Œç»“åˆæ¨¡ä»¿å­¦ä¹ åˆå§‹åŒ–å’Œå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ï¼Œæå‡æ™ºèƒ½ä½“æ€§èƒ½ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ··åˆæ–¹æ³•æ˜¾è‘—ä¼˜äºçº¯å¼ºåŒ–å­¦ä¹ ï¼Œèƒœç‡ç¨³å®šè¶…è¿‡70%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ··åˆè®­ç»ƒæ–¹æ³•ï¼Œå°†ç¦»çº¿æ¨¡ä»¿å­¦ä¹ ä¸åœ¨çº¿å¼ºåŒ–å­¦ä¹ ç›¸ç»“åˆï¼Œç”¨äºè®­ç»ƒ2Då°„å‡»æ¸¸æˆä¸­çš„AIæ™ºèƒ½ä½“ã€‚å¤æ‚æ¸¸æˆç¯å¢ƒä¸­çš„å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“å¸¸é¢ä¸´å¥–åŠ±ç¨€ç–ã€è®­ç»ƒä¸ç¨³å®šå’Œæ ·æœ¬æ•ˆç‡ä½ç­‰é—®é¢˜ã€‚æœ¬æ–‡å®ç°äº†ä¸€ä¸ªå¤šå¤´ç¥ç»ç½‘ç»œï¼Œå…·æœ‰ç”¨äºè¡Œä¸ºå…‹éš†å’ŒQå­¦ä¹ çš„ç‹¬ç«‹è¾“å‡ºï¼Œå¹¶é€šè¿‡å¸¦æœ‰æ³¨æ„åŠ›æœºåˆ¶çš„å…±äº«ç‰¹å¾æå–å±‚è¿›è¡Œç»Ÿä¸€ã€‚çº¯æ·±åº¦Qç½‘ç»œï¼ˆDQNï¼‰çš„åˆå§‹å®éªŒè¡¨ç°å‡ºæ˜¾è‘—çš„ä¸ç¨³å®šæ€§ï¼Œæ™ºèƒ½ä½“ç»å¸¸é€€å›åˆ°è¾ƒå·®çš„ç­–ç•¥ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ··åˆæ–¹æ³•ï¼Œé¦–å…ˆåˆ©ç”¨åŸºäºè§„åˆ™çš„æ™ºèƒ½ä½“çš„æ¼”ç¤ºæ•°æ®è¿›è¡Œè¡Œä¸ºå…‹éš†ï¼Œç„¶åè¿‡æ¸¡åˆ°å¼ºåŒ–å­¦ä¹ ã€‚æˆ‘ä»¬çš„æ··åˆæ–¹æ³•å§‹ç»ˆèƒ½å¤Ÿä»¥è¶…è¿‡70%çš„èƒœç‡å‡»è´¥åŸºäºè§„åˆ™çš„å¯¹æ‰‹ï¼Œæ˜¾è‘—ä¼˜äºçº¯å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œåè€…è¡¨ç°å‡ºé«˜æ–¹å·®å’Œé¢‘ç¹çš„æ€§èƒ½ä¸‹é™ã€‚å¤šå¤´æ¶æ„å®ç°äº†å­¦ä¹ æ¨¡å¼ä¹‹é—´çš„æœ‰æ•ˆçŸ¥è¯†è½¬ç§»ï¼ŒåŒæ—¶ä¿æŒäº†è®­ç»ƒçš„ç¨³å®šæ€§ã€‚ç»“æœè¡¨æ˜ï¼Œå°†åŸºäºæ¼”ç¤ºçš„åˆå§‹åŒ–ä¸å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ç›¸ç»“åˆï¼Œä¸ºåœ¨çº¯æ¢ç´¢ä¸è¶³çš„å¤æ‚å¤šæ™ºèƒ½ä½“ç¯å¢ƒä¸­å¼€å‘æ¸¸æˆAIæ™ºèƒ½ä½“æä¾›äº†ä¸€ä¸ªç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³2Då°„å‡»æ¸¸æˆä¸­ï¼Œå¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“è®­ç»ƒä¸ç¨³å®šã€æ ·æœ¬æ•ˆç‡ä½çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚çº¯æ·±åº¦Qç½‘ç»œï¼ˆDQNï¼‰ï¼Œåœ¨å¤æ‚æ¸¸æˆç¯å¢ƒä¸­éš¾ä»¥æœ‰æ•ˆæ¢ç´¢ï¼Œå¯¼è‡´æ™ºèƒ½ä½“ç­–ç•¥ä¸ç¨³å®šï¼Œæ€§èƒ½æ³¢åŠ¨å¤§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯ç»“åˆæ¨¡ä»¿å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ çš„ä¼˜åŠ¿ã€‚é¦–å…ˆï¼Œåˆ©ç”¨æ¨¡ä»¿å­¦ä¹ ä»ä¸“å®¶æ¼”ç¤ºæ•°æ®ä¸­å­¦ä¹ ä¸€ä¸ªè¾ƒå¥½çš„åˆå§‹ç­–ç•¥ï¼Œé¿å…äº†ä»é›¶å¼€å§‹æ¢ç´¢çš„å›°éš¾ã€‚ç„¶åï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥ä¼˜åŒ–ç­–ç•¥ï¼Œä½¿å…¶èƒ½å¤Ÿé€‚åº”æ›´å¤æ‚çš„æ¸¸æˆåœºæ™¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼š1) æ¨¡ä»¿å­¦ä¹ é˜¶æ®µï¼šä½¿ç”¨è¡Œä¸ºå…‹éš†ï¼ˆBehavioral Cloningï¼‰æ–¹æ³•ï¼Œåˆ©ç”¨åŸºäºè§„åˆ™çš„æ™ºèƒ½ä½“çš„æ¼”ç¤ºæ•°æ®è®­ç»ƒå¤šå¤´ç¥ç»ç½‘ç»œã€‚2) å¼ºåŒ–å­¦ä¹ é˜¶æ®µï¼šä½¿ç”¨Qå­¦ä¹ æ–¹æ³•ï¼Œåœ¨æ¸¸æˆç¯å¢ƒä¸­ä¸å¯¹æ‰‹äº¤äº’ï¼Œå¹¶æ ¹æ®å¥–åŠ±ä¿¡å·æ›´æ–°ç­–ç•¥ã€‚å¤šå¤´ç¥ç»ç½‘ç»œåŒ…å«å…±äº«çš„ç‰¹å¾æå–å±‚å’Œç‹¬ç«‹çš„è¡Œä¸ºå…‹éš†ä¸Qå­¦ä¹ è¾“å‡ºå¤´ã€‚

**å…³é”®åˆ›æ–°**ï¼šå…³é”®åˆ›æ–°åœ¨äºå°†æ¨¡ä»¿å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ æ— ç¼ç»“åˆï¼Œåˆ©ç”¨å¤šå¤´ç¥ç»ç½‘ç»œå®ç°çŸ¥è¯†è¿ç§»ã€‚æ¨¡ä»¿å­¦ä¹ æä¾›äº†ä¸€ä¸ªè‰¯å¥½çš„åˆå§‹åŒ–ï¼ŒåŠ é€Ÿäº†å¼ºåŒ–å­¦ä¹ çš„æ”¶æ•›é€Ÿåº¦ï¼Œå¹¶æé«˜äº†è®­ç»ƒçš„ç¨³å®šæ€§ã€‚å¤šå¤´ç½‘ç»œç»“æ„å…è®¸æ¨¡å‹åŒæ—¶å­¦ä¹ æ¨¡ä»¿ç­–ç•¥å’Œä¼˜åŒ–Qå‡½æ•°ï¼Œé¿å…äº†ç¾éš¾æ€§é—å¿˜ã€‚

**å…³é”®è®¾è®¡**ï¼šå¤šå¤´ç¥ç»ç½‘ç»œåŒ…å«å…±äº«çš„å·ç§¯ç‰¹å¾æå–å±‚å’Œä¸¤ä¸ªç‹¬ç«‹çš„è¾“å‡ºå¤´ï¼šä¸€ä¸ªç”¨äºè¡Œä¸ºå…‹éš†ï¼Œè¾“å‡ºåŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒï¼›å¦ä¸€ä¸ªç”¨äºQå­¦ä¹ ï¼Œè¾“å‡ºæ¯ä¸ªåŠ¨ä½œçš„Qå€¼ã€‚è¡Œä¸ºå…‹éš†çš„æŸå¤±å‡½æ•°ä¸ºäº¤å‰ç†µæŸå¤±ï¼ŒQå­¦ä¹ çš„æŸå¤±å‡½æ•°ä¸ºå‡æ–¹è¯¯å·®æŸå¤±ã€‚è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé¦–å…ˆä½¿ç”¨è¡Œä¸ºå…‹éš†æŸå¤±è¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶ååˆ‡æ¢åˆ°Qå­¦ä¹ æŸå¤±è¿›è¡Œå¾®è°ƒã€‚è®ºæ–‡è¿˜ä½¿ç”¨äº†æ³¨æ„åŠ›æœºåˆ¶æ¥å¢å¼ºç‰¹å¾æå–å±‚çš„è¡¨è¾¾èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ··åˆæ–¹æ³•åœ¨2Då°„å‡»æ¸¸æˆä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¸çº¯å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä»¥è¶…è¿‡70%çš„èƒœç‡ç¨³å®šå‡»è´¥åŸºäºè§„åˆ™çš„å¯¹æ‰‹ï¼Œä¸”è®­ç»ƒè¿‡ç¨‹æ›´åŠ ç¨³å®šï¼Œæ–¹å·®æ›´å°ã€‚çº¯å¼ºåŒ–å­¦ä¹ æ–¹æ³•åˆ™è¡¨ç°å‡ºé«˜æ–¹å·®å’Œé¢‘ç¹çš„æ€§èƒ½ä¸‹é™ï¼Œéš¾ä»¥è¾¾åˆ°ç¨³å®šçš„é«˜èƒœç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ¸¸æˆAIå¼€å‘ï¼Œç‰¹åˆ«æ˜¯éœ€è¦æ™ºèƒ½ä½“å…·å¤‡å¿«é€Ÿå­¦ä¹ å’Œé€‚åº”èƒ½åŠ›çš„å¤šæ™ºèƒ½ä½“å¯¹æŠ—ç¯å¢ƒã€‚æ­¤å¤–ï¼Œè¯¥æ··åˆè®­ç»ƒæ–¹æ³•ä¹Ÿå¯æ¨å¹¿åˆ°å…¶ä»–å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ç­‰ï¼Œå°¤å…¶æ˜¯åœ¨ç¼ºä¹å¥–åŠ±ä¿¡å·æˆ–æ¢ç´¢ç©ºé—´å·¨å¤§çš„åœºæ™¯ä¸‹ï¼Œå…·æœ‰é‡è¦çš„åº”ç”¨ä»·å€¼å’Œæ½œåŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement learning agents in complex game environments often suffer from sparse rewards, training instability, and poor sample efficiency. This paper presents a hybrid training approach that combines offline imitation learning with online reinforcement learning for a 2D shooter game agent. We implement a multi-head neural network with separate outputs for behavioral cloning and Q-learning, unified by shared feature extraction layers with attention mechanisms. Initial experiments using pure deep Q-Networks exhibited significant instability, with agents frequently reverting to poor policies despite occasional good performance. To address this, we developed a hybrid methodology that begins with behavioral cloning on demonstration data from rule-based agents, then transitions to reinforcement learning. Our hybrid approach achieves consistently above 70% win rate against rule-based opponents, substantially outperforming pure reinforcement learning methods which showed high variance and frequent performance degradation. The multi-head architecture enables effective knowledge transfer between learning modes while maintaining training stability. Results demonstrate that combining demonstration-based initialization with reinforcement learning optimization provides a robust solution for developing game AI agents in complex multi-agent environments where pure exploration proves insufficient.

