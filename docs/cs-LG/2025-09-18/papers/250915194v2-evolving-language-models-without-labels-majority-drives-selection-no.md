---
layout: default
title: Evolving Language Models without Labels: Majority Drives Selection, Novelty Promotes Variation
---

# Evolving Language Models without Labels: Majority Drives Selection, Novelty Promotes Variation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.15194" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.15194v2</a>
  <a href="https://arxiv.org/pdf/2509.15194.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.15194v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.15194v2', 'Evolving Language Models without Labels: Majority Drives Selection, Novelty Promotes Variation')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Yujun Zhou, Zhenwen Liang, Haolin Liu, Wenhao Yu, Kishan Panaganti, Linfeng Song, Dian Yu, Xiangliang Zhang, Haitao Mi, Dong Yu

**ÂàÜÁ±ª**: cs.LG, cs.CL

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-18 (Êõ¥Êñ∞: 2025-10-01)

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/YujunZhou/EVOL-RL)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**EVOL-RLÔºö‰∏ÄÁßçÊó†Ê†áÁ≠æËøõÂåñËØ≠Ë®ÄÊ®°ÂûãÊ°ÜÊû∂ÔºåÈÄöËøáÂ§öÊï∞È©±Âä®ÈÄâÊã©ÂíåÊñ∞È¢ñÊÄß‰øÉËøõÂèòÂºÇÂÆûÁé∞Ëá™ÊèêÂçá„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËØ≠Ë®ÄÊ®°ÂûãËá™ÊèêÂçá` `Êó†ÁõëÁù£Â≠¶‰π†` `Âº∫ÂåñÂ≠¶‰π†` `ËøõÂåñÁÆóÊ≥ï` `Êñ∞È¢ñÊÄßÂ•ñÂä±` `Â§öÊ†∑ÊÄß‰øùÊåÅ` `È¢ÜÂüüÊ≥õÂåñ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâËá™ÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÊñπÊ≥ïËøáÂ∫¶‰æùËµñËá™ÊàëÁ°ÆËÆ§‰ø°Âè∑ÔºåÂØºËá¥Ê®°ÂûãÂÄæÂêë‰∫éÂ§öÊï∞ÊîØÊåÅÁöÑËß£ÂÜ≥ÊñπÊ°àÔºåÈÄ†ÊàêÂ§öÊ†∑ÊÄßÂ¥©Ê∫É„ÄÇ
2. EVOL-RLÊ°ÜÊû∂Ê®°‰ªøËøõÂåñÂéüÂàôÔºåÁªìÂêàÂ§öÊï∞ÊäïÁ•®ÁöÑÁ®≥ÂÆöÊÄßÈîöÁÇπÂíåÊñ∞È¢ñÊÄßÊÑüÁü•Â•ñÂä±ÔºåÂπ≥Ë°°ÈÄâÊã©‰∏éÂèòÂºÇ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåEVOL-RLÊòæËëóÊèêÂçá‰∫ÜÊ®°ÂûãÂú®Êï∞Â≠¶Êé®ÁêÜÂíåÊõ¥ÂπøÊ≥õ‰ªªÂä°‰∏äÁöÑÊÄßËÉΩÔºåÂπ∂ÊúâÊïàÈò≤Ê≠¢‰∫ÜÈ¢ÜÂüüÂÜÖÂ§öÊ†∑ÊÄßÂ¥©Ê∫É„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâË∂äÊù•Ë∂äÂ§öÂú∞ÈááÁî®Âü∫‰∫éÂèØÈ™åËØÅÂ•ñÂä±ÁöÑÂº∫ÂåñÂ≠¶‰π†ÔºàRLVRÔºâËøõË°åËÆ≠ÁªÉÔºå‰ΩÜÂÆûÈôÖÈÉ®ÁΩ≤ÈúÄË¶ÅÊ®°ÂûãËÉΩÂ§üÂú®Ê≤°ÊúâÊ†áÁ≠æÊàñÂ§ñÈÉ®ËØÑÂà§ÁöÑÊÉÖÂÜµ‰∏ãËøõË°åËá™ÊàëÊîπËøõ„ÄÇÁé∞ÊúâÁöÑËá™ÊàëÊîπËøõÊñπÊ≥ï‰∏ªË¶Å‰æùËµñ‰∫éËá™ÊàëÁ°ÆËÆ§‰ø°Âè∑Ôºà‰æãÂ¶ÇÔºåÁΩÆ‰ø°Â∫¶„ÄÅÁÜµÊàñ‰∏ÄËá¥ÊÄßÔºâÊù•ÁîüÊàêÂ•ñÂä±„ÄÇËøôÁßç‰æùËµñÈ©±‰ΩøÊ®°ÂûãÂÄæÂêë‰∫éËøáÂ∫¶Ëá™‰ø°„ÄÅÂ§öÊï∞ÊîØÊåÅÁöÑËß£ÂÜ≥ÊñπÊ°àÔºåÂØºËá¥ÁÜµÂ¥©Ê∫ÉÔºå‰ªéËÄåÈôç‰Ωépass@nÂíåÊé®ÁêÜÂ§çÊùÇÂ∫¶„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜEVOL-RLÔºå‰∏Ä‰∏™Êó†Ê†áÁ≠æÊ°ÜÊû∂ÔºåÂÆÉÂèçÊò†‰∫ÜÂπ≥Ë°°ÈÄâÊã©‰∏éÂèòÂºÇÁöÑËøõÂåñÂéüÂàô„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåEVOL-RL‰øùÁïôÂ§öÊï∞ÊäïÁ•®ÁöÑÁ≠îÊ°à‰Ωú‰∏∫Á®≥ÂÆöÊÄßÁöÑÈîöÁÇπÔºå‰ΩÜÂ¢ûÂä†‰∫Ü‰∏Ä‰∏™Êñ∞È¢ñÊÄßÊÑüÁü•Â•ñÂä±ÔºåËØ•Â•ñÂä±Ê†πÊçÆÊØè‰∏™ÈááÊ†∑Ëß£ÁöÑÊé®ÁêÜ‰∏éÂÖ∂‰ªñÂπ∂ÂèëÁîüÊàêÁöÑÂìçÂ∫îÁöÑÂ∑ÆÂºÇÁ®ãÂ∫¶ÂØπÂÖ∂ËøõË°åËØÑÂàÜ„ÄÇËøôÁßçÂ§öÊï∞È©±Âä®Á®≥ÂÆö+Êñ∞È¢ñÊÄßÈ©±Âä®Êé¢Á¥¢ÁöÑËßÑÂàôÂèçÊò†‰∫ÜÂèòÂºÇ-ÈÄâÊã©ÂéüÂàôÔºöÈÄâÊã©Èò≤Ê≠¢ÊºÇÁßªÔºåËÄåÊñ∞È¢ñÊÄßÈò≤Ê≠¢Â¥©Ê∫É„ÄÇËØÑ‰º∞ÁªìÊûúË°®ÊòéÔºåEVOL-RLÂßãÁªà‰ºò‰∫é‰ªÖÂ§öÊï∞Âü∫Á∫øÔºõ‰æãÂ¶ÇÔºåÂú®Êó†Ê†áÁ≠æAIME24‰∏äËÆ≠ÁªÉÂ∞ÜQwen3-4B-Base AIME25ÁöÑpass@1‰ªéÂü∫Á∫øÁöÑ4.6%ÊèêÂçáÂà∞16.4%Ôºåpass@16‰ªé18.5%ÊèêÂçáÂà∞37.9%„ÄÇEVOL-RL‰∏ç‰ªÖÈò≤Ê≠¢‰∫ÜÈ¢ÜÂüüÂÜÖÂ§öÊ†∑ÊÄßÂ¥©Ê∫ÉÔºåËøòÊèêÈ´ò‰∫ÜÈ¢ÜÂüüÂ§ñÊ≥õÂåñËÉΩÂäõÔºà‰ªéÊï∞Â≠¶Êé®ÁêÜÂà∞Êõ¥ÂπøÊ≥õÁöÑ‰ªªÂä°Ôºå‰æãÂ¶ÇGPQA„ÄÅMMLU-ProÂíåBBEHÔºâ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®Êó†Ê†áÁ≠æÊàñÂ§ñÈÉ®ËØÑÂà§ÁöÑÊÉÖÂÜµ‰∏ãËøõË°åËá™ÊàëÊîπËøõÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ï‰æùËµñ‰∫éËá™ÊàëÁ°ÆËÆ§‰ø°Âè∑ÔºåÂØºËá¥Ê®°ÂûãËøáÂ∫¶Ëá™‰ø°ÔºåÂÄæÂêë‰∫éÂ§öÊï∞Á≠îÊ°àÔºåÈÄ†ÊàêÂ§öÊ†∑ÊÄßÔºàÁÜµÔºâÂ¥©Ê∫ÉÔºåÊúÄÁªàÂΩ±ÂìçÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõÂíåÊ≥õÂåñÊÄßËÉΩ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂÄüÈâ¥ÁîüÁâ©ËøõÂåñ‰∏≠ÁöÑ‚ÄúÈÄâÊã©‰∏éÂèòÂºÇ‚ÄùÂéüÂàô„ÄÇÈÄöËøá‰øùÁïôÂ§öÊï∞ÊäïÁ•®ÁöÑÁ≠îÊ°à‰Ωú‰∏∫Á®≥ÂÆöÊÄßÁöÑÈîöÁÇπÔºàÈÄâÊã©ÔºâÔºåÂπ∂ÂºïÂÖ•Êñ∞È¢ñÊÄßÊÑüÁü•Â•ñÂä±Êù•ÈºìÂä±Ê®°ÂûãÊé¢Á¥¢‰∏çÂêåÁöÑÊé®ÁêÜË∑ØÂæÑÔºàÂèòÂºÇÔºâÔºå‰ªéËÄåÂú®Á®≥ÂÆöÊÄßÂíåÊé¢Á¥¢ÊÄß‰πãÈó¥ÂèñÂæóÂπ≥Ë°°„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöEVOL-RLÊ°ÜÊû∂ÂåÖÂê´‰ª•‰∏ã‰∏ªË¶ÅÊ≠•È™§Ôºö1) ‰ΩøÁî®ËØ≠Ë®ÄÊ®°ÂûãÁîüÊàêÂ§ö‰∏™ÂÄôÈÄâÁ≠îÊ°àÔºõ2) ÂØπËøô‰∫õÁ≠îÊ°àËøõË°åÂ§öÊï∞ÊäïÁ•®ÔºåÈÄâÂá∫ÊúÄÂ∏∏ËßÅÁöÑÁ≠îÊ°à‰Ωú‰∏∫ÈîöÁÇπÔºõ3) ËÆ°ÁÆóÊØè‰∏™ÂÄôÈÄâÁ≠îÊ°àÁöÑÊñ∞È¢ñÊÄßÂæóÂàÜÔºåËØ•ÂæóÂàÜË°°Èáè‰∫ÜËØ•Á≠îÊ°àÁöÑÊé®ÁêÜËøáÁ®ã‰∏éÂÖ∂‰ªñÁ≠îÊ°àÁöÑÂ∑ÆÂºÇÁ®ãÂ∫¶Ôºõ4) Â∞ÜÂ§öÊï∞ÊäïÁ•®ÁªìÊûúÂíåÊñ∞È¢ñÊÄßÂæóÂàÜÁªìÂêàËµ∑Êù•‰Ωú‰∏∫Â•ñÂä±‰ø°Âè∑ÔºåÁî®‰∫éÊõ¥Êñ∞ËØ≠Ë®ÄÊ®°Âûã„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞ÁÇπÂú®‰∫éÂºïÂÖ•‰∫ÜÊñ∞È¢ñÊÄßÊÑüÁü•Â•ñÂä±„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ï‰ªÖ‰æùËµñ‰∫éËá™ÊàëÁ°ÆËÆ§‰ø°Âè∑‰∏çÂêåÔºåEVOL-RLÈÄöËøáÈºìÂä±Ê®°ÂûãÊé¢Á¥¢‰∏çÂêåÁöÑÊé®ÁêÜË∑ØÂæÑÊù•Èò≤Ê≠¢Â§öÊ†∑ÊÄßÂ¥©Ê∫ÉÔºå‰ªéËÄåÊèêÈ´òÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÊñ∞È¢ñÊÄßÂæóÂàÜÁöÑËÆ°ÁÆóÊòØÂÖ≥ÈîÆËÆæËÆ°‰πã‰∏Ä„ÄÇËÆ∫Êñá‰∏≠ÂÖ∑‰ΩìÂ¶Ç‰ΩïË°°ÈáèÊé®ÁêÜËøáÁ®ãÁöÑÂ∑ÆÂºÇÊÄßÊú™Áü•„ÄÇÊ≠§Â§ñÔºåÂ¶Ç‰ΩïÂπ≥Ë°°Â§öÊï∞ÊäïÁ•®ÁªìÊûúÂíåÊñ∞È¢ñÊÄßÂæóÂàÜÂú®Â•ñÂä±‰ø°Âè∑‰∏≠ÁöÑÊùÉÈáç‰πüËá≥ÂÖ≥ÈáçË¶ÅÔºåÂÖ∑‰ΩìÂÆûÁé∞ÁªÜËäÇÊú™Áü•„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåEVOL-RLÂú®Êó†Ê†áÁ≠æAIME24Êï∞ÊçÆÈõÜ‰∏äËÆ≠ÁªÉÂêéÔºåÊòæËëóÊèêÂçá‰∫ÜQwen3-4B-BaseÊ®°ÂûãÂú®AIME25Êï∞ÊçÆÈõÜ‰∏äÁöÑÊÄßËÉΩÔºåpass@1ÊåáÊ†á‰ªéÂü∫Á∫øÁöÑ4.6%ÊèêÂçáÂà∞16.4%Ôºåpass@16ÊåáÊ†á‰ªé18.5%ÊèêÂçáÂà∞37.9%„ÄÇÊ≠§Â§ñÔºåEVOL-RLËøòÊèêÈ´ò‰∫ÜÊ®°ÂûãÂú®GPQA„ÄÅMMLU-ProÂíåBBEHÁ≠âÈ¢ÜÂüüÂ§ñ‰ªªÂä°‰∏äÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

EVOL-RLÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºåÂèØÁî®‰∫éËÆ≠ÁªÉÊó†ÈúÄ‰∫∫Â∑•Ê†áÊ≥®ÊàñÂ§ñÈÉ®ÂèçÈ¶àÂç≥ÂèØËá™ÊàëÊîπËøõÁöÑËØ≠Ë®ÄÊ®°Âûã„ÄÇËøôÂØπ‰∫éËµÑÊ∫êÊúâÈôêÊàñÈöæ‰ª•Ëé∑ÂèñÈ´òË¥®ÈáèÊ†áÊ≥®Êï∞ÊçÆÁöÑÂú∫ÊôØÂ∞§‰∏∫ÈáçË¶Å„ÄÇËØ•ÊñπÊ≥ïÂèØ‰ª•Â∫îÁî®‰∫éÂêÑÁßçËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ‰ªªÂä°Ôºå‰æãÂ¶ÇÈóÆÁ≠î„ÄÅÊñáÊú¨ÁîüÊàêÂíåÊú∫Âô®ÁøªËØëÔºåÂπ∂ÊúâÊúõÊèêÂçáÊ®°ÂûãÁöÑÈ≤ÅÊ£íÊÄßÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Large language models (LLMs) are increasingly trained with reinforcement learning from verifiable rewards (RLVR), yet real-world deployment demands models that can self-improve without labels or external judges. Existing self-improvement approaches primarily rely on self-confirmation signals (e.g., confidence, entropy, or consistency) to generate rewards. This reliance drives models toward over-confident, majority-favored solutions, causing an entropy collapse that degrades pass@n and reasoning complexity. To address this, we propose EVOL-RL, a label-free framework that mirrors the evolutionary principle of balancing selection with variation. Concretely, EVOL-RL retains the majority-voted answer as an anchor for stability, but adds a novelty-aware reward that scores each sampled solution by how different its reasoning is from other concurrently generated responses. This majority-for-stability + novelty-for-exploration rule mirrors the variation-selection principle: selection prevents drift, while novelty prevents collapse. Evaluation results show that EVOL-RL consistently outperforms the majority-only baseline; e.g., training on label-free AIME24 lifts Qwen3-4B-Base AIME25 pass@1 from baseline's 4.6% to 16.4%, and pass@16 from 18.5% to 37.9%. EVOL-RL not only prevents in-domain diversity collapse but also improves out-of-domain generalization (from math reasoning to broader tasks, e.g., GPQA, MMLU-Pro, and BBEH). The code is available at: https://github.com/YujunZhou/EVOL-RL.

