---
layout: default
title: FlowRL: Matching Reward Distributions for LLM Reasoning
---

# FlowRL: Matching Reward Distributions for LLM Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.15207" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.15207v3</a>
  <a href="https://arxiv.org/pdf/2509.15207.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.15207v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.15207v3', 'FlowRL: Matching Reward Distributions for LLM Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xuekai Zhu, Daixuan Cheng, Dinghuai Zhang, Hengli Li, Kaiyan Zhang, Che Jiang, Youbang Sun, Ermo Hua, Yuxin Zuo, Xingtai Lv, Qizheng Zhang, Lin Chen, Fanghao Shao, Bo Xue, Yunchong Song, Zhenjie Yang, Ganqu Cui, Ning Ding, Jianfeng Gao, Xiaodong Liu, Bowen Zhou, Hongyuan Mei, Zhouhan Lin

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-18 (æ›´æ–°: 2025-11-04)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**FlowRLï¼šé€šè¿‡åŒ¹é…å¥–åŠ±åˆ†å¸ƒæå‡å¤§è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `å¥–åŠ±åˆ†å¸ƒåŒ¹é…` `æ¨ç†` `æµé‡å¹³è¡¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºå¥–åŠ±æœ€å¤§åŒ–çš„LLMå¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œæ˜“è¿‡åº¦ä¼˜åŒ–ä¸»è¦å¥–åŠ±ä¿¡å·ï¼Œå¿½ç•¥å¤šæ ·æ¨ç†è·¯å¾„ã€‚
2. FlowRLé€šè¿‡æµé‡å¹³è¡¡åŒ¹é…å¥–åŠ±åˆ†å¸ƒï¼Œé¼“åŠ±å¤šæ ·åŒ–æ¢ç´¢å’Œæ³›åŒ–æ¨ç†è½¨è¿¹ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒFlowRLåœ¨æ•°å­¦å’Œä»£ç æ¨ç†ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºPPOå’ŒGRPOã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºFlowRLï¼Œä¸€ç§é€šè¿‡æµé‡å¹³è¡¡åŒ¹é…å®Œæ•´å¥–åŠ±åˆ†å¸ƒè€Œéæœ€å¤§åŒ–å¥–åŠ±çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚ç°æœ‰çš„å…ˆè¿›æ¨ç†æ¨¡å‹é‡‡ç”¨å¥–åŠ±æœ€å¤§åŒ–æ–¹æ³•ï¼ˆä¾‹å¦‚ï¼ŒPPOå’ŒGRPOï¼‰ï¼Œè¿™å¾€å¾€è¿‡åº¦ä¼˜åŒ–ä¸»è¦çš„å¥–åŠ±ä¿¡å·ï¼Œè€Œå¿½ç•¥äº†ä¸å¸¸è§ä½†æœ‰æ•ˆçš„æ¨ç†è·¯å¾„ï¼Œä»è€Œé™ä½äº†å¤šæ ·æ€§ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬ä½¿ç”¨å¯å­¦ä¹ çš„é…åˆ†å‡½æ•°å°†æ ‡é‡å¥–åŠ±è½¬æ¢ä¸ºå½’ä¸€åŒ–çš„ç›®æ ‡åˆ†å¸ƒï¼Œç„¶åæœ€å°åŒ–ç­–ç•¥å’Œç›®æ ‡åˆ†å¸ƒä¹‹é—´çš„åå‘KLæ•£åº¦ã€‚æˆ‘ä»¬å°†è¿™ä¸ªæƒ³æ³•å®ç°ä¸ºä¸€ç§æµé‡å¹³è¡¡ä¼˜åŒ–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä¿ƒè¿›äº†å¤šæ ·åŒ–çš„æ¢ç´¢å’Œå¯æ³›åŒ–çš„æ¨ç†è½¨è¿¹ã€‚æˆ‘ä»¬åœ¨æ•°å­¦å’Œä»£ç æ¨ç†ä»»åŠ¡ä¸Šè¿›è¡Œäº†å®éªŒï¼šFlowRLåœ¨æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æ¯”GRPOå¹³å‡æé«˜10.0ï¼…å’Œæ¯”PPOå¹³å‡æé«˜5.1ï¼…çš„æ˜¾è‘—æ”¹è¿›ï¼Œå¹¶ä¸”åœ¨ä»£ç æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å§‹ç»ˆæ›´å¥½ã€‚è¿™äº›ç»“æœçªå‡ºäº†å¥–åŠ±åˆ†å¸ƒåŒ¹é…æ˜¯LLMå¼ºåŒ–å­¦ä¹ ä¸­æœ‰æ•ˆæ¢ç´¢å’Œå¤šæ ·åŒ–æ¨ç†çš„å…³é”®æ­¥éª¤ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œå¦‚PPOå’ŒGRPOï¼Œä¸»è¦é€šè¿‡æœ€å¤§åŒ–å¥–åŠ±æ¥ä¼˜åŒ–æ¨¡å‹ã€‚è¿™ç§æ–¹æ³•çš„ç¼ºç‚¹åœ¨äºï¼Œå®ƒå®¹æ˜“è¿‡åº¦å…³æ³¨é‚£äº›é¢‘ç‡è¾ƒé«˜çš„å¥–åŠ±ä¿¡å·ï¼Œè€Œå¿½ç•¥äº†é‚£äº›è™½ç„¶ä¸å¸¸è§ä½†åŒæ ·æœ‰æ•ˆçš„æ¨ç†è·¯å¾„ã€‚è¿™å¯¼è‡´æ¨¡å‹æ¢ç´¢ä¸è¶³ï¼Œæ¨ç†è·¯å¾„å•ä¸€ï¼Œæ³›åŒ–èƒ½åŠ›å—é™ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§æ–¹æ³•èƒ½å¤Ÿé¼“åŠ±æ¨¡å‹æ¢ç´¢æ›´å¤šæ ·åŒ–çš„æ¨ç†è·¯å¾„ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ•´ä½“æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šFlowRLçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†å¥–åŠ±æœ€å¤§åŒ–é—®é¢˜è½¬åŒ–ä¸ºå¥–åŠ±åˆ†å¸ƒåŒ¹é…é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒä¸æ˜¯ç®€å•åœ°æœ€å¤§åŒ–æ ‡é‡å¥–åŠ±ï¼Œè€Œæ˜¯å°†æ ‡é‡å¥–åŠ±è½¬æ¢ä¸ºä¸€ä¸ªå½’ä¸€åŒ–çš„ç›®æ ‡åˆ†å¸ƒï¼Œç„¶åé€šè¿‡æœ€å°åŒ–ç­–ç•¥ç”Ÿæˆçš„åˆ†å¸ƒä¸ç›®æ ‡åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚æ¥ä¼˜åŒ–æ¨¡å‹ã€‚è¿™ç§æ–¹æ³•çš„å…³é”®åœ¨äºï¼Œå®ƒé¼“åŠ±æ¨¡å‹æ¢ç´¢æ•´ä¸ªå¥–åŠ±åˆ†å¸ƒï¼Œè€Œä¸ä»…ä»…æ˜¯é‚£äº›å…·æœ‰æœ€é«˜å¥–åŠ±çš„è·¯å¾„ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šFlowRLçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼š1) ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆæ¨ç†è½¨è¿¹ï¼›2) ä½¿ç”¨å¥–åŠ±æ¨¡å‹å¯¹æ¨ç†è½¨è¿¹è¿›è¡Œè¯„ä¼°ï¼Œå¾—åˆ°æ ‡é‡å¥–åŠ±ï¼›3) ä½¿ç”¨å¯å­¦ä¹ çš„é…åˆ†å‡½æ•°å°†æ ‡é‡å¥–åŠ±è½¬æ¢ä¸ºå½’ä¸€åŒ–çš„ç›®æ ‡åˆ†å¸ƒï¼›4) è®¡ç®—ç­–ç•¥ç”Ÿæˆçš„åˆ†å¸ƒä¸ç›®æ ‡åˆ†å¸ƒä¹‹é—´çš„åå‘KLæ•£åº¦ï¼›5) ä½¿ç”¨ä¼˜åŒ–ç®—æ³•ï¼ˆå¦‚Adamï¼‰æœ€å°åŒ–åå‘KLæ•£åº¦ï¼Œä»è€Œæ›´æ–°æ¨¡å‹å‚æ•°ã€‚

**å…³é”®åˆ›æ–°**ï¼šFlowRLæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå®ƒå°†å¥–åŠ±æœ€å¤§åŒ–é—®é¢˜è½¬åŒ–ä¸ºå¥–åŠ±åˆ†å¸ƒåŒ¹é…é—®é¢˜ã€‚ä¸ä¼ ç»Ÿçš„å¥–åŠ±æœ€å¤§åŒ–æ–¹æ³•ç›¸æ¯”ï¼ŒFlowRLèƒ½å¤Ÿæ›´å¥½åœ°å¹³è¡¡æ¢ç´¢å’Œåˆ©ç”¨ï¼Œé¼“åŠ±æ¨¡å‹æ¢ç´¢æ›´å¤šæ ·åŒ–çš„æ¨ç†è·¯å¾„ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ•´ä½“æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒFlowRLè¿˜å¼•å…¥äº†å¯å­¦ä¹ çš„é…åˆ†å‡½æ•°ï¼Œç”¨äºå°†æ ‡é‡å¥–åŠ±è½¬æ¢ä¸ºå½’ä¸€åŒ–çš„ç›®æ ‡åˆ†å¸ƒï¼Œè¿™ä½¿å¾—FlowRLèƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”ä¸åŒçš„ä»»åŠ¡å’Œå¥–åŠ±ç»“æ„ã€‚

**å…³é”®è®¾è®¡**ï¼šFlowRLçš„å…³é”®è®¾è®¡åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š1) ä½¿ç”¨åå‘KLæ•£åº¦ä½œä¸ºæŸå¤±å‡½æ•°ï¼Œç”¨äºè¡¡é‡ç­–ç•¥ç”Ÿæˆçš„åˆ†å¸ƒä¸ç›®æ ‡åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ï¼›2) ä½¿ç”¨å¯å­¦ä¹ çš„é…åˆ†å‡½æ•°ï¼Œç”¨äºå°†æ ‡é‡å¥–åŠ±è½¬æ¢ä¸ºå½’ä¸€åŒ–çš„ç›®æ ‡åˆ†å¸ƒï¼›3) ä½¿ç”¨Adamä¼˜åŒ–ç®—æ³•ï¼Œç”¨äºæœ€å°åŒ–åå‘KLæ•£åº¦ï¼Œä»è€Œæ›´æ–°æ¨¡å‹å‚æ•°ã€‚é…åˆ†å‡½æ•°çš„å…·ä½“å½¢å¼æœªçŸ¥ï¼Œè®ºæ–‡ä¸­å¯èƒ½æ²¡æœ‰è¯¦ç»†è¯´æ˜å…¶ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

FlowRLåœ¨æ•°å­¦å’Œä»£ç æ¨ç†ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­ï¼ŒFlowRLæ¯”GRPOå¹³å‡æé«˜äº†10.0ï¼…ï¼Œæ¯”PPOå¹³å‡æé«˜äº†5.1ï¼…ã€‚åœ¨ä»£ç æ¨ç†ä»»åŠ¡ä¸­ï¼ŒFlowRLä¹Ÿè¡¨ç°å‡ºäº†ä¸€è‡´çš„ä¼˜è¶Šæ€§ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œå¥–åŠ±åˆ†å¸ƒåŒ¹é…æ˜¯LLMå¼ºåŒ–å­¦ä¹ ä¸­æœ‰æ•ˆæ¢ç´¢å’Œå¤šæ ·åŒ–æ¨ç†çš„å…³é”®æ­¥éª¤ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

FlowRLå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ä»¥åº”ç”¨äºå„ç§éœ€è¦å¤æ‚æ¨ç†èƒ½åŠ›çš„åœºæ™¯ï¼Œä¾‹å¦‚æ•°å­¦é—®é¢˜æ±‚è§£ã€ä»£ç ç”Ÿæˆã€çŸ¥è¯†å›¾è°±æ¨ç†ã€å¯¹è¯ç”Ÿæˆç­‰ã€‚é€šè¿‡é¼“åŠ±æ¨¡å‹æ¢ç´¢æ›´å¤šæ ·åŒ–çš„æ¨ç†è·¯å¾„ï¼ŒFlowRLå¯ä»¥æé«˜æ¨¡å‹åœ¨è¿™äº›ä»»åŠ¡ä¸Šçš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä»è€Œä¸ºå®é™…åº”ç”¨å¸¦æ¥æ›´å¤§çš„ä»·å€¼ã€‚æ­¤å¤–ï¼ŒFlowRLè¿˜å¯ä»¥ç”¨äºæé«˜æ¨¡å‹çš„é²æ£’æ€§å’Œå®‰å…¨æ€§ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°åº”å¯¹å„ç§æŒ‘æˆ˜å’Œæ”»å‡»ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We propose FlowRL: matching the full reward distribution via flow balancing instead of maximizing rewards in large language model (LLM) reinforcement learning (RL). Recent advanced reasoning models adopt reward-maximizing methods (\eg, PPO and GRPO), which tend to over-optimize dominant reward signals while neglecting less frequent but valid reasoning paths, thus reducing diversity. In contrast, we transform scalar rewards into a normalized target distribution using a learnable partition function, and then minimize the reverse KL divergence between the policy and the target distribution. We implement this idea as a flow-balanced optimization method that promotes diverse exploration and generalizable reasoning trajectories. We conduct experiments on math and code reasoning tasks: FlowRL achieves a significant average improvement of $10.0\%$ over GRPO and $5.1\%$ over PPO on math benchmarks, and performs consistently better on code reasoning tasks. These results highlight reward distribution-matching as a key step toward efficient exploration and diverse reasoning in LLM reinforcement learning.

