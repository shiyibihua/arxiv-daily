---
layout: default
title: SIRI: Scaling Iterative Reinforcement Learning with Interleaved Compression
---

# SIRI: Scaling Iterative Reinforcement Learning with Interleaved Compression

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25176" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25176v1</a>
  <a href="https://arxiv.org/pdf/2509.25176.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25176v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25176v1', 'SIRI: Scaling Iterative Reinforcement Learning with Interleaved Compression')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Haoming Wen, Yushi Bai, Juanzi Li, Jie Tang

**åˆ†ç±»**: cs.LG, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29

**å¤‡æ³¨**: In submission

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**SIRIï¼šé€šè¿‡äº¤é”™å‹ç¼©æ‰©å±•è¿­ä»£å¼ºåŒ–å­¦ä¹ ï¼Œæå‡å¤§å‹æ¨ç†æ¨¡å‹çš„æ•ˆç‡ä¸å‡†ç¡®æ€§ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨ç†æ•ˆç‡` `æ¨¡å‹å‹ç¼©` `è¿­ä»£è®­ç»ƒ` `åŠ¨æ€è§„åˆ’` `tokenä¼˜åŒ–` `AIME24`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤§å‹æ¨ç†æ¨¡å‹å­˜åœ¨é‡å¤æ€è€ƒæ¨¡å¼ï¼Œå‡å°‘å†—ä½™tokenå¸¸å¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œå¦‚ä½•åœ¨æ•ˆç‡ä¸æ€§èƒ½é—´å–å¾—å¹³è¡¡æ˜¯æ ¸å¿ƒé—®é¢˜ã€‚
2. SIRIé€šè¿‡äº¤é”™å‹ç¼©å’Œæ‰©å±•æ¨ç†é¢„ç®—ï¼ŒåŠ¨æ€è°ƒæ•´rollouté•¿åº¦ï¼Œè¿«ä½¿æ¨¡å‹åœ¨æœ‰é™ä¸Šä¸‹æ–‡ä¸­è¿›è¡Œç²¾ç¡®å†³ç­–ï¼Œæé«˜æ¨ç†å¯†åº¦ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒSIRIåœ¨é™ä½tokenä½¿ç”¨é‡çš„åŒæ—¶æ˜¾è‘—æå‡æ€§èƒ½ï¼Œä¾‹å¦‚åœ¨AIME24ä¸Šï¼ŒSIRI-lowæ€§èƒ½æå‡43.2%ï¼Œtokenä½¿ç”¨é‡å‡å°‘46.9%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•SIRIï¼Œå³é€šè¿‡äº¤é”™å‹ç¼©æ‰©å±•çš„è¿­ä»£å¼ºåŒ–å­¦ä¹ ï¼Œç”¨äºå¤§å‹æ¨ç†æ¨¡å‹(LRM)ï¼Œä»¥å®ç°æ›´é«˜æ•ˆå’Œå‡†ç¡®çš„æ¨ç†ã€‚ç°æœ‰ç ”ç©¶è¡¨æ˜LRMä¸­å­˜åœ¨é‡å¤çš„æ€è€ƒæ¨¡å¼ï¼Œè€Œå‡å°‘è¿™äº›æ¨¡å¼é€šå¸¸ä¼šç‰ºç‰²æ€§èƒ½ã€‚æœ¬æ–‡è¡¨æ˜ï¼Œé€šè¿‡ä¸€ç§åœ¨è®­ç»ƒæœŸé—´è¿­ä»£åœ°äº¤æ›¿å‹ç¼©å’Œæ‰©å±•æ¨ç†é¢„ç®—çš„è®­ç»ƒæœºåˆ¶ï¼Œå¯ä»¥å…‹æœè¿™ç§æƒè¡¡ï¼Œå…·ä½“æ–¹æ³•æ˜¯åŠ¨æ€è°ƒæ•´æœ€å¤§rollouté•¿åº¦ã€‚å‹ç¼©é˜¶æ®µç¼©çŸ­rollouté•¿åº¦ï¼Œè¿«ä½¿æ¨¡å‹åœ¨æœ‰é™çš„ä¸Šä¸‹æ–‡ä¸­åšå‡ºç²¾ç¡®ä¸”æœ‰ä»·å€¼çš„å†³ç­–ï¼Œä»è€Œæœ‰æ•ˆåœ°å‡å°‘å†—ä½™tokenå¹¶å¢åŠ æ¨ç†å¯†åº¦ã€‚æ‰©å±•é˜¶æ®µåˆ™æ”¾å®½é•¿åº¦é™åˆ¶ï¼Œä¸ºæ¨¡å‹åœ¨é•¿æ—¶ç¨‹è®¾ç½®ä¸­æ¢ç´¢å’Œè§„åˆ’æä¾›ç©ºé—´ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°ç»è¿‡æ¯ä¸ªå‹ç¼©-æ‰©å±•å¾ªç¯åï¼Œå³ä½¿æ¨¡å‹çš„è¾“å‡ºé•¿åº¦å‡å°‘ï¼Œå…¶æ€§èƒ½ä¹Ÿä¼šæé«˜ï¼Œä»è€Œç¨³æ­¥åœ°å°†å…¶æ¨å‘æ€§èƒ½-æ•ˆç‡æƒè¡¡çš„å¸•ç´¯æ‰˜å‰æ²¿ã€‚åœ¨DeepSeek-R1-Distill-Qwen-1.5Bä¸Šè®­ç»ƒï¼Œç»è¿‡ä¸‰æ¬¡è¿­ä»£åï¼ŒSIRI-lowåœ¨AIME24ä¸Šçš„æ€§èƒ½æé«˜äº†43.2%ï¼ŒåŒæ—¶å‡å°‘äº†46.9%çš„tokenä½¿ç”¨é‡ï¼Œè€ŒSIRI-highå®ç°äº†ä¸å…¶ä»–æ‰€æœ‰æ–¹æ³•ç›¸æ¯”æœ€é«˜çš„å‡†ç¡®ç‡ã€‚æˆ‘ä»¬çš„å‘ç°æ­ç¤ºäº†åœ¨è®­ç»ƒæœŸé—´å‘¨æœŸæ€§åœ°æŒ¯è¡LRMçš„è¾“å‡ºæˆªæ–­é•¿åº¦ä»¥åŠ¨æ€å¹³è¡¡æ¨ç†ä¸­çš„æ¢ç´¢å’Œæ•ˆç‡çš„æ½œåŠ›ï¼Œä»è€Œæ”¶æ•›åˆ°ä¸¤è€…ä¹‹é—´çš„æœ€ä½³â€œç”œèœœç‚¹â€ã€‚æˆ‘ä»¬çš„æ¨¡å‹å·²å…¬å¼€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMï¼‰ä¸­å­˜åœ¨çš„æ¨ç†æ•ˆç‡é—®é¢˜ï¼Œå…·ä½“è¡¨ç°ä¸ºæ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­å­˜åœ¨å¤§é‡çš„é‡å¤æ€è€ƒæ¨¡å¼ï¼Œå¯¼è‡´tokenä½¿ç”¨é‡å¤§ï¼Œæ¨ç†é€Ÿåº¦æ…¢ã€‚ç°æœ‰æ–¹æ³•è¯•å›¾é€šè¿‡å‡å°‘tokenä½¿ç”¨æ¥æé«˜æ•ˆç‡ï¼Œä½†å¾€å¾€ä¼šç‰ºç‰²æ¨¡å‹çš„æ¨ç†æ€§èƒ½ï¼Œæ— æ³•åœ¨æ•ˆç‡å’Œæ€§èƒ½ä¹‹é—´å–å¾—å¹³è¡¡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSIRIçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è¿­ä»£åœ°äº¤æ›¿å‹ç¼©å’Œæ‰©å±•æ¨ç†é¢„ç®—æ¥åŠ¨æ€å¹³è¡¡æ¢ç´¢å’Œæ•ˆç‡ã€‚å‹ç¼©é˜¶æ®µè¿«ä½¿æ¨¡å‹åœ¨æœ‰é™çš„ä¸Šä¸‹æ–‡ä¸­åšå‡ºæ›´ç²¾ç¡®çš„å†³ç­–ï¼Œå‡å°‘å†—ä½™tokenï¼›æ‰©å±•é˜¶æ®µåˆ™å…è®¸æ¨¡å‹åœ¨æ›´é•¿çš„horizonä¸­è¿›è¡Œæ¢ç´¢å’Œè§„åˆ’ã€‚é€šè¿‡è¿™ç§å‘¨æœŸæ€§çš„è°ƒæ•´ï¼Œæ¨¡å‹å¯ä»¥é€æ­¥å­¦ä¹ åˆ°æ›´é«˜æ•ˆçš„æ¨ç†ç­–ç•¥ï¼Œæœ€ç»ˆè¾¾åˆ°æ€§èƒ½å’Œæ•ˆç‡çš„â€œç”œèœœç‚¹â€ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSIRIçš„è®­ç»ƒè¿‡ç¨‹åŒ…å«å¤šä¸ªè¿­ä»£å¾ªç¯ï¼Œæ¯ä¸ªå¾ªç¯åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼šå‹ç¼©é˜¶æ®µå’Œæ‰©å±•é˜¶æ®µã€‚åœ¨å‹ç¼©é˜¶æ®µï¼Œæ¨¡å‹çš„æœ€å¤§rollouté•¿åº¦è¢«ç¼©çŸ­ï¼Œè¿«ä½¿æ¨¡å‹åœ¨æœ‰é™çš„ä¸Šä¸‹æ–‡ä¸­è¿›è¡Œæ¨ç†ã€‚åœ¨æ‰©å±•é˜¶æ®µï¼Œæ¨¡å‹çš„æœ€å¤§rollouté•¿åº¦è¢«æ”¾å®½ï¼Œå…è®¸æ¨¡å‹è¿›è¡Œæ›´é•¿è¿œçš„è§„åˆ’ã€‚è¿™ä¸¤ä¸ªé˜¶æ®µäº¤æ›¿è¿›è¡Œï¼Œç›´åˆ°æ¨¡å‹æ”¶æ•›ã€‚

**å…³é”®åˆ›æ–°**ï¼šSIRIçš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§äº¤é”™å‹ç¼©å’Œæ‰©å±•çš„è®­ç»ƒæœºåˆ¶ï¼Œèƒ½å¤ŸåŠ¨æ€åœ°è°ƒæ•´æ¨¡å‹çš„æ¨ç†é¢„ç®—ï¼Œä»è€Œåœ¨æ€§èƒ½å’Œæ•ˆç‡ä¹‹é—´å–å¾—æ›´å¥½çš„å¹³è¡¡ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒSIRIä¸éœ€è¦æ‰‹åŠ¨è®¾è®¡å¤æ‚çš„tokenå‹ç¼©ç­–ç•¥ï¼Œè€Œæ˜¯é€šè¿‡è®­ç»ƒè‡ªåŠ¨å­¦ä¹ åˆ°æœ€ä¼˜çš„æ¨ç†ç­–ç•¥ã€‚

**å…³é”®è®¾è®¡**ï¼šSIRIçš„å…³é”®è®¾è®¡åœ¨äºrollouté•¿åº¦çš„åŠ¨æ€è°ƒæ•´ç­–ç•¥ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨å‹ç¼©é˜¶æ®µï¼Œrollouté•¿åº¦ä¼šé€æ¸å‡å°ï¼Œè¿«ä½¿æ¨¡å‹åšå‡ºæ›´ç²¾ç¡®çš„å†³ç­–ã€‚åœ¨æ‰©å±•é˜¶æ®µï¼Œrollouté•¿åº¦ä¼šé€æ¸å¢å¤§ï¼Œå…è®¸æ¨¡å‹è¿›è¡Œæ›´é•¿è¿œçš„è§„åˆ’ã€‚rollouté•¿åº¦çš„å…·ä½“è°ƒæ•´ç­–ç•¥ï¼ˆä¾‹å¦‚ï¼Œå‡å°/å¢å¤§çš„å¹…åº¦ï¼‰å¯ä»¥æ ¹æ®å…·ä½“çš„ä»»åŠ¡å’Œæ¨¡å‹è¿›è¡Œè°ƒæ•´ã€‚è®ºæ–‡ä¸­å¹¶æœªæ˜ç¡®ç»™å‡ºå…·ä½“çš„rollouté•¿åº¦è°ƒæ•´å‡½æ•°ï¼Œè¿™éƒ¨åˆ†å¯èƒ½éœ€è¦æ ¹æ®å®é™…æƒ…å†µè¿›è¡Œæ¢ç´¢ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

SIRIåœ¨DeepSeek-R1-Distill-Qwen-1.5Bæ¨¡å‹ä¸Šè¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œç»è¿‡ä¸‰æ¬¡è¿­ä»£åï¼ŒSIRI-lowåœ¨AIME24æ•°æ®é›†ä¸Šçš„æ€§èƒ½æé«˜äº†43.2%ï¼ŒåŒæ—¶tokenä½¿ç”¨é‡å‡å°‘äº†46.9%ã€‚SIRI-highå®ç°äº†ä¸å…¶ä»–æ‰€æœ‰æ–¹æ³•ç›¸æ¯”æœ€é«˜çš„å‡†ç¡®ç‡ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒSIRIèƒ½å¤Ÿæœ‰æ•ˆåœ°æé«˜å¤§å‹æ¨ç†æ¨¡å‹çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SIRIæ–¹æ³•å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ä»¥åº”ç”¨äºå„ç§éœ€è¦é«˜æ•ˆæ¨ç†çš„å¤§å‹è¯­è¨€æ¨¡å‹ä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚é—®ç­”ç³»ç»Ÿã€å¯¹è¯ç”Ÿæˆã€ä»£ç ç”Ÿæˆç­‰ã€‚é€šè¿‡é™ä½tokenä½¿ç”¨é‡ï¼ŒSIRIå¯ä»¥æ˜¾è‘—é™ä½æ¨¡å‹çš„è®¡ç®—æˆæœ¬å’Œå»¶è¿Ÿï¼Œä½¿å…¶æ›´æ˜“äºéƒ¨ç½²åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­ã€‚æ­¤å¤–ï¼ŒSIRIè¿˜å¯ä»¥æé«˜æ¨¡å‹çš„æ¨ç†å‡†ç¡®æ€§ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°è§£å†³å¤æ‚çš„æ¨ç†é—®é¢˜ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We introduce SIRI, Scaling Iterative Reinforcement Learning with Interleaved Compression, a simple yet effective RL approach for Large Reasoning Models (LRMs) that enables more efficient and accurate reasoning. Existing studies have observed repetitive thinking patterns in LRMs, and attempts to reduce them often come at the cost of performance. In this paper, we show that this trade-off can be overcome through a training regime that iteratively alternates between compressing and expanding the reasoning budget, by dynamically adjusting the maximum rollout length during training. The compression phase cuts the rollout length, forcing the model to make precise and valuable decisions within a limited context, which effectively reduces redundant tokens and increases reasoning density. The expansion phase then relaxes the length limit, providing space for the model to explore and plan in long-horizon settings. Remarkably, we find that after each compression-expansion cycle, the model's performance improves even as its output length decreases, steadily pushing it closer to the Pareto frontier in the performance-efficiency trade-off. Training on DeepSeek-R1-Distill-Qwen-1.5B, SIRI-low improves performance on AIME24 by 43.2% while reducing token usage by 46.9% after three iterations, and SIRI-high achieves the highest accuracy compared to all other methods (Figure 1). Our findings shed light on the potential of periodically oscillating the LRM's output truncation length during training to dynamically balance exploration and efficiency in reasoning, converging towards an optimal "sweet spot" between the two. Our models are publicly available.

