---
layout: default
title: Safe In-Context Reinforcement Learning
---

# Safe In-Context Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25582" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25582v1</a>
  <a href="https://arxiv.org/pdf/2509.25582.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25582v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25582v1', 'Safe In-Context Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Amir Moeini, Minjae Kwon, Alper Kamil Bozkurt, Yuichi Motai, Rohan Chandra, Lu Feng, Shangtong Zhang

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå®‰å…¨ä¸Šä¸‹æ–‡å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œè§£å†³æ— å‚æ•°æ›´æ–°é€‚åº”è¿‡ç¨‹ä¸­çš„å®‰å…¨çº¦æŸé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `ä¸Šä¸‹æ–‡å¼ºåŒ–å­¦ä¹ ` `å®‰å…¨å¼ºåŒ–å­¦ä¹ ` `çº¦æŸé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹` `æ— å‚æ•°æ›´æ–°` `åœ¨çº¿é€‚åº”`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»ŸICRLæ–¹æ³•åœ¨é€‚åº”æ–°ä»»åŠ¡æ—¶ç¼ºä¹å¯¹å®‰å…¨æ€§çš„è€ƒè™‘ï¼Œå¯èƒ½å¯¼è‡´è¿åçº¦æŸæˆ–äº§ç”Ÿé«˜æˆæœ¬çš„è¡Œä¸ºã€‚
2. æœ¬æ–‡æå‡ºå®‰å…¨ICRLæ–¹æ³•ï¼Œåœ¨é€‚åº”è¿‡ç¨‹ä¸­åŒæ—¶ä¼˜åŒ–å¥–åŠ±å’Œæœ€å°åŒ–æˆæœ¬ï¼Œç¡®ä¿æ™ºèƒ½ä½“åœ¨å®‰å…¨çº¦æŸå†…è¿è¡Œã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä½¿æ™ºèƒ½ä½“æ ¹æ®æˆæœ¬é¢„ç®—è°ƒæ•´è¡Œä¸ºï¼Œåœ¨å¥–åŠ±å’Œå®‰å…¨æ€§ä¹‹é—´å–å¾—å¹³è¡¡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ èŒƒå¼ï¼Œå³å®‰å…¨ä¸Šä¸‹æ–‡å¼ºåŒ–å­¦ä¹ ï¼ˆICRLï¼‰ã€‚ICRLæ˜¯ä¸€ç§æ–°å…´çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œç»è¿‡é¢„è®­ç»ƒåï¼Œæ™ºèƒ½ä½“æ— éœ€ä»»ä½•å‚æ•°æ›´æ–°å³å¯é€‚åº”åˆ†å¸ƒå¤–çš„æµ‹è¯•ä»»åŠ¡ã€‚æ™ºèƒ½ä½“é€šè¿‡ä¸æ–­æ‰©å±•å…¶ç­–ç•¥ç¥ç»ç½‘ç»œçš„è¾“å…¥ï¼ˆå³ä¸Šä¸‹æ–‡ï¼‰æ¥å®ç°è¿™ä¸€ç‚¹ã€‚ä¾‹å¦‚ï¼Œè¾“å…¥å¯ä»¥æ˜¯æ™ºèƒ½ä½“ç›´åˆ°å½“å‰æ—¶é—´æ­¥æ‰€èƒ½è®¿é—®çš„æ‰€æœ‰å†å²ç»éªŒã€‚æ™ºèƒ½ä½“çš„æ€§èƒ½éšç€è¾“å…¥çš„å¢é•¿è€Œæé«˜ï¼Œè€Œæ— éœ€ä»»ä½•å‚æ•°æ›´æ–°ã€‚æœ¬æ–‡æå‡ºäº†ç¬¬ä¸€ä¸ªåœ¨çº¦æŸé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹æ¡†æ¶ä¸‹ï¼Œä¿ƒè¿›ICRLé€‚åº”è¿‡ç¨‹å®‰å…¨æ€§çš„æ–¹æ³•ã€‚æ¢å¥è¯è¯´ï¼Œåœ¨æ— å‚æ•°æ›´æ–°çš„é€‚åº”è¿‡ç¨‹ä¸­ï¼Œæ™ºèƒ½ä½“ä¸ä»…æœ€å¤§åŒ–å¥–åŠ±ï¼Œè¿˜æœ€å°åŒ–é¢å¤–çš„æˆæœ¬å‡½æ•°ã€‚æˆ‘ä»¬è¿˜è¯æ˜äº†æˆ‘ä»¬çš„æ™ºèƒ½ä½“ä¸»åŠ¨å¯¹æˆæœ¬å®¹å¿åº¦çš„é˜ˆå€¼ï¼ˆå³é¢„ç®—ï¼‰åšå‡ºååº”ã€‚æˆæœ¬é¢„ç®—è¶Šé«˜ï¼Œæ™ºèƒ½ä½“çš„è¡Œä¸ºå°±è¶Šæ¿€è¿›ï¼Œè€Œæˆæœ¬é¢„ç®—è¶Šä½ï¼Œæ™ºèƒ½ä½“çš„è¡Œä¸ºå°±è¶Šä¿å®ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„ä¸Šä¸‹æ–‡å¼ºåŒ–å­¦ä¹ ï¼ˆICRLï¼‰æ–¹æ³•ä¸»è¦å…³æ³¨äºåœ¨æ— éœ€å‚æ•°æ›´æ–°çš„æƒ…å†µä¸‹é€‚åº”æ–°çš„ä»»åŠ¡ï¼Œä½†å¿½ç•¥äº†é€‚åº”è¿‡ç¨‹ä¸­çš„å®‰å…¨æ€§ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œæ™ºèƒ½ä½“å¯èƒ½éœ€è¦åœ¨æ»¡è¶³æŸäº›çº¦æŸæ¡ä»¶ï¼ˆä¾‹å¦‚ï¼Œèµ„æºé™åˆ¶ã€å®‰å…¨è·ç¦»ç­‰ï¼‰çš„æƒ…å†µä¸‹è¿›è¡Œå­¦ä¹ å’Œå†³ç­–ã€‚å› æ­¤ï¼Œå¦‚ä½•åœ¨ICRLæ¡†æ¶ä¸‹ä¿è¯æ™ºèƒ½ä½“çš„å®‰å…¨æ€§ï¼Œé¿å…è¿åçº¦æŸæˆ–äº§ç”Ÿè¿‡é«˜çš„æˆæœ¬ï¼Œæ˜¯ä¸€ä¸ªé‡è¦çš„ç ”ç©¶é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†çº¦æŸé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆCMDPï¼‰çš„æ¦‚å¿µå¼•å…¥åˆ°ICRLæ¡†æ¶ä¸­ã€‚é€šè¿‡åœ¨å¥–åŠ±å‡½æ•°ä¹‹å¤–å¼•å…¥ä¸€ä¸ªæˆæœ¬å‡½æ•°ï¼Œå¹¶è®¾å®šä¸€ä¸ªæˆæœ¬é¢„ç®—ï¼Œæ™ºèƒ½ä½“éœ€è¦åœ¨æœ€å¤§åŒ–å¥–åŠ±çš„åŒæ—¶ï¼Œæœ€å°åŒ–æˆæœ¬ï¼Œå¹¶ç¡®ä¿æ€»æˆæœ¬ä¸è¶…è¿‡é¢„ç®—ã€‚è¿™æ ·ï¼Œæ™ºèƒ½ä½“å°±å¯ä»¥åœ¨é€‚åº”æ–°ä»»åŠ¡çš„è¿‡ç¨‹ä¸­ï¼ŒåŒæ—¶è€ƒè™‘æ€§èƒ½å’Œå®‰å…¨æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼š1ï¼‰ä¸Šä¸‹æ–‡ç¼–ç å™¨ï¼šç”¨äºå°†å†å²ç»éªŒç¼–ç æˆä¸Šä¸‹æ–‡å‘é‡ï¼›2ï¼‰ç­–ç•¥ç½‘ç»œï¼šæ ¹æ®ä¸Šä¸‹æ–‡å‘é‡å’Œå½“å‰çŠ¶æ€ï¼Œè¾“å‡ºåŠ¨ä½œï¼›3ï¼‰ä»·å€¼ç½‘ç»œï¼šç”¨äºä¼°è®¡å½“å‰çŠ¶æ€çš„ä»·å€¼ï¼›4ï¼‰æˆæœ¬ç½‘ç»œï¼šç”¨äºä¼°è®¡å½“å‰çŠ¶æ€çš„æˆæœ¬ã€‚æ™ºèƒ½ä½“é€šè¿‡ä¸ç¯å¢ƒäº¤äº’ï¼Œæ”¶é›†ç»éªŒæ•°æ®ï¼Œå¹¶ä½¿ç”¨è¿™äº›æ•°æ®æ¥è®­ç»ƒç­–ç•¥ç½‘ç»œã€ä»·å€¼ç½‘ç»œå’Œæˆæœ¬ç½‘ç»œã€‚åœ¨é€‚åº”æ–°ä»»åŠ¡æ—¶ï¼Œæ™ºèƒ½ä½“æ— éœ€æ›´æ–°ç½‘ç»œå‚æ•°ï¼Œè€Œæ˜¯é€šè¿‡è°ƒæ•´ä¸Šä¸‹æ–‡å‘é‡æ¥é€‚åº”æ–°çš„ç¯å¢ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå°†CMDPçš„æ¦‚å¿µå¼•å…¥åˆ°ICRLæ¡†æ¶ä¸­ï¼Œæå‡ºäº†ä¸€ç§å®‰å…¨ICRLæ–¹æ³•ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨æ— éœ€å‚æ•°æ›´æ–°çš„æƒ…å†µä¸‹ï¼Œä½¿æ™ºèƒ½ä½“åœ¨é€‚åº”æ–°ä»»åŠ¡çš„åŒæ—¶ï¼Œä¿è¯å®‰å…¨æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜èƒ½å¤Ÿä½¿æ™ºèƒ½ä½“æ ¹æ®æˆæœ¬é¢„ç®—è°ƒæ•´è¡Œä¸ºï¼Œåœ¨å¥–åŠ±å’Œå®‰å…¨æ€§ä¹‹é—´å–å¾—å¹³è¡¡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å…·ä½“å®ç°ä¸Šï¼Œæœ¬æ–‡å¯èƒ½é‡‡ç”¨äº†æŸç§ç‰¹å®šçš„ç­–ç•¥æ¢¯åº¦ç®—æ³•ï¼ˆä¾‹å¦‚ï¼ŒPPOæˆ–TRPOï¼‰æ¥è®­ç»ƒç­–ç•¥ç½‘ç»œã€ä»·å€¼ç½‘ç»œå’Œæˆæœ¬ç½‘ç»œã€‚æˆæœ¬å‡½æ•°çš„è®¾è®¡éœ€è¦æ ¹æ®å…·ä½“çš„åº”ç”¨åœºæ™¯æ¥ç¡®å®šã€‚ä¾‹å¦‚ï¼Œåœ¨æœºå™¨äººå¯¼èˆªä»»åŠ¡ä¸­ï¼Œæˆæœ¬å‡½æ•°å¯ä»¥å®šä¹‰ä¸ºä¸éšœç¢ç‰©çš„è·ç¦»çš„å€’æ•°ã€‚æˆæœ¬é¢„ç®—çš„è®¾ç½®ä¹Ÿéœ€è¦æ ¹æ®å…·ä½“çš„åº”ç”¨åœºæ™¯æ¥è°ƒæ•´ã€‚è¾ƒé«˜çš„æˆæœ¬é¢„ç®—å…è®¸æ™ºèƒ½ä½“é‡‡å–æ›´æ¿€è¿›çš„ç­–ç•¥ï¼Œè€Œè¾ƒä½çš„æˆæœ¬é¢„ç®—åˆ™ä¼šè¿«ä½¿æ™ºèƒ½ä½“é‡‡å–æ›´ä¿å®ˆçš„ç­–ç•¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡é€šè¿‡å®éªŒéªŒè¯äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨ä¿è¯å®‰å…¨æ€§çš„å‰æä¸‹ï¼Œä½¿æ™ºèƒ½ä½“é€‚åº”æ–°çš„ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œå®éªŒè¿˜è¡¨æ˜ï¼Œæ™ºèƒ½ä½“èƒ½å¤Ÿæ ¹æ®æˆæœ¬é¢„ç®—è°ƒæ•´è¡Œä¸ºï¼Œåœ¨å¥–åŠ±å’Œå®‰å…¨æ€§ä¹‹é—´å–å¾—å¹³è¡¡ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸä¸ªå®éªŒä¸­ï¼Œå½“æˆæœ¬é¢„ç®—è¾ƒé«˜æ—¶ï¼Œæ™ºèƒ½ä½“èƒ½å¤Ÿæ›´å¿«åœ°åˆ°è¾¾ç›®æ ‡ä½ç½®ï¼Œä½†åŒæ—¶ä¹Ÿä¼šæ‰¿æ‹…æ›´é«˜çš„é£é™©ï¼›è€Œå½“æˆæœ¬é¢„ç®—è¾ƒä½æ—¶ï¼Œæ™ºèƒ½ä½“åˆ™ä¼šé‡‡å–æ›´ä¿å®ˆçš„ç­–ç•¥ï¼Œä»¥ç¡®ä¿å®‰å…¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººã€è‡ªåŠ¨é©¾é©¶ã€é‡‘èäº¤æ˜“ç­‰å¯¹å®‰å…¨æ€§è¦æ±‚è¾ƒé«˜çš„é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨æœºå™¨äººå¯¼èˆªä¸­ï¼Œå¯ä»¥åˆ©ç”¨è¯¥æ–¹æ³•ä½¿æœºå™¨äººåœ¨é¿å¼€éšœç¢ç‰©çš„åŒæ—¶ï¼Œå°½å¿«åˆ°è¾¾ç›®æ ‡ä½ç½®ã€‚åœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼Œå¯ä»¥åˆ©ç”¨è¯¥æ–¹æ³•ä½¿è½¦è¾†åœ¨éµå®ˆäº¤é€šè§„åˆ™çš„åŒæ—¶ï¼Œå°½å¯èƒ½æé«˜è¡Œé©¶æ•ˆç‡ã€‚åœ¨é‡‘èäº¤æ˜“ä¸­ï¼Œå¯ä»¥åˆ©ç”¨è¯¥æ–¹æ³•ä½¿äº¤æ˜“å‘˜åœ¨æ§åˆ¶é£é™©çš„åŒæ—¶ï¼Œè·å–å°½å¯èƒ½é«˜çš„æ”¶ç›Šã€‚è¯¥æ–¹æ³•å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œæœ‰æœ›æ¨åŠ¨å®‰å…¨å¼ºåŒ–å­¦ä¹ çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In-context reinforcement learning (ICRL) is an emerging RL paradigm where the agent, after some pretraining procedure, is able to adapt to out-of-distribution test tasks without any parameter updates. The agent achieves this by continually expanding the input (i.e., the context) to its policy neural networks. For example, the input could be all the history experience that the agent has access to until the current time step. The agent's performance improves as the input grows, without any parameter updates. In this work, we propose the first method that promotes the safety of ICRL's adaptation process in the framework of constrained Markov Decision Processes. In other words, during the parameter-update-free adaptation process, the agent not only maximizes the reward but also minimizes an additional cost function. We also demonstrate that our agent actively reacts to the threshold (i.e., budget) of the cost tolerance. With a higher cost budget, the agent behaves more aggressively, and with a lower cost budget, the agent behaves more conservatively.

