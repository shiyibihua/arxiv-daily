---
layout: default
title: OrthAlign: Orthogonal Subspace Decomposition for Non-Interfering Multi-Objective Alignment
---

# OrthAlign: Orthogonal Subspace Decomposition for Non-Interfering Multi-Objective Alignment

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.24610" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.24610v2</a>
  <a href="https://arxiv.org/pdf/2509.24610.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.24610v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.24610v2', 'OrthAlign: Orthogonal Subspace Decomposition for Non-Interfering Multi-Objective Alignment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Liang Lin, Zhihao Xu, Junhao Dong, Jian Zhao, Yuchen Yuan, Guibin Zhang, Miao Yu, Yiming Zhang, Zhengtao Yao, Huahui Yi, Dongrui Liu, Xinfeng Li, Kun Wang

**åˆ†ç±»**: cs.LG, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29 (æ›´æ–°: 2025-09-30)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**OrthAlignï¼šæ­£äº¤å­ç©ºé—´åˆ†è§£è§£å†³å¤§æ¨¡å‹å¤šç›®æ ‡å¯¹é½ä¸­çš„å†²çªé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹å¯¹é½` `å¤šç›®æ ‡ä¼˜åŒ–` `æ­£äº¤å­ç©ºé—´åˆ†è§£` `æ¢¯åº¦å†²çª` `å‚æ•°æ›´æ–°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§æ¨¡å‹å¯¹é½æ–¹æ³•åœ¨å¤šç›®æ ‡ä¼˜åŒ–æ—¶ï¼Œéš¾ä»¥é¿å…ä¸åŒåå¥½é—´çš„å†²çªå’Œæƒè¡¡ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚
2. OrthAligné€šè¿‡æ­£äº¤å­ç©ºé—´åˆ†è§£ï¼Œå°†å‚æ•°æ›´æ–°ç©ºé—´åˆ†è§£ä¸ºäº’ä¸å¹²æ‰°çš„å­ç©ºé—´ï¼Œä»è€Œè§£å†³æ¢¯åº¦å†²çªã€‚
3. å®éªŒè¡¨æ˜ï¼ŒOrthAlignåœ¨å¤šä¸ªåå¥½ç»´åº¦ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå¹¶ä¿è¯äº†è®­ç»ƒçš„ç¨³å®šæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹é½åœ¨è§£å†³å¤šä¸ªäººç±»åå¥½æ—¶é¢ä¸´ä¸€ä¸ªå…³é”®å›°å¢ƒï¼šåœ¨ä¸€ä¸ªç»´åº¦ä¸Šçš„æ”¹è¿›å¸¸å¸¸ä»¥ç‰ºç‰²å…¶ä»–ç»´åº¦ä¸ºä»£ä»·ï¼Œä»è€Œåœ¨è¯¸å¦‚helpfulnesså’Œharmlessnessç­‰ç›¸äº’ç«äº‰çš„ç›®æ ‡ä¹‹é—´äº§ç”Ÿä¸å¯é¿å…çš„æƒè¡¡ã€‚ä»¥å¾€çš„å·¥ä½œä¸»è¦é›†ä¸­äºåŸºäºçº¦æŸçš„ä¼˜åŒ–ç®—æ³•å’Œæ•°æ®é€‰æ‹©ç­–ç•¥æ¥ç¼“è§£å†²çªï¼Œä½†è¿™äº›æ–¹æ³•å¿½ç•¥äº†åœ¨å‚æ•°å±‚é¢ç›´æ¥è§£å†³å†²çªçš„æ ¹æœ¬é—®é¢˜ã€‚æœ¬æ–‡æå‡ºOrthAlignï¼Œè¿™æ˜¯ä¸€ç§åˆ›æ–°çš„æ–¹æ³•ï¼Œå®ƒé€šè¿‡åˆ©ç”¨æ­£äº¤å­ç©ºé—´åˆ†è§£æ¥ä»æ ¹æœ¬ä¸Šè§£å†³å¤šç›®æ ‡åå¥½å¯¹é½ä¸­çš„æ¢¯åº¦çº§å†²çªï¼Œä»è€Œå¼€åˆ›äº†ä¸€ç§æ–°çš„èŒƒä¾‹ã€‚OrthAlignç­–ç•¥æ€§åœ°å°†å‚æ•°æ›´æ–°ç©ºé—´åˆ†è§£ä¸ºæ­£äº¤å­ç©ºé—´ï¼Œç¡®ä¿é’ˆå¯¹ä¸åŒåå¥½çš„ä¼˜åŒ–å‘ç”Ÿåœ¨æ•°å­¦ä¸Šäº’ä¸å¹²æ‰°çš„æ–¹å‘ä¸Šã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æä¾›äº†ç†è®ºä¿è¯ï¼Œè¯æ˜å½“å‚æ•°å¢é‡æ»¡è¶³æ­£äº¤å­ç©ºé—´çº¦æŸå’Œè°±èŒƒæ•°ç•Œé™æ—¶ï¼Œæ‰€å¾—åˆ°çš„æ›´æ–°è¡¨ç°å‡ºçº¿æ€§Lipschitzå¢é•¿è€Œä¸æ˜¯æŒ‡æ•°ä¸ç¨³å®šï¼Œä»è€Œç¡®ä¿æ‰€æœ‰åå¥½ç»´åº¦ä¸Šçš„ç¨³å®šæ”¶æ•›ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼šI. åœ¨helpfulã€harmlesså’Œtruthfulç»´åº¦ä¸Šè¿›è¡Œå¤šç›®æ ‡å¯¹é½åï¼ŒOrthAlignå®ç°äº†34.61%åˆ°50.89%çš„æœ€å¤§å•åå¥½æ”¹è¿›ã€‚II. å¹³å‡æ€»ä½“å¥–åŠ±æé«˜äº†13.96%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå¤§å‹è¯­è¨€æ¨¡å‹å¯¹é½æ—¨åœ¨ä½¿æ¨¡å‹çš„è¡Œä¸ºç¬¦åˆäººç±»çš„åå¥½ï¼Œä¾‹å¦‚helpfulnessï¼ˆä¹äºåŠ©äººï¼‰ã€harmlessnessï¼ˆæ— å®³ï¼‰å’Œtruthfulnessï¼ˆçœŸå®ï¼‰ã€‚ç„¶è€Œï¼Œç›´æ¥ä¼˜åŒ–è¿™äº›ç›®æ ‡é€šå¸¸ä¼šå¯¼è‡´å†²çªï¼Œå³æé«˜ä¸€ä¸ªç›®æ ‡çš„æ€§èƒ½å¯èƒ½ä¼šé™ä½å¦ä¸€ä¸ªç›®æ ‡çš„æ€§èƒ½ã€‚ç°æœ‰çš„æ–¹æ³•ï¼Œå¦‚åŸºäºçº¦æŸçš„ä¼˜åŒ–å’Œæ•°æ®é€‰æ‹©ï¼Œè¯•å›¾ç¼“è§£è¿™äº›å†²çªï¼Œä½†æœªèƒ½ä»æ ¹æœ¬ä¸Šè§£å†³å‚æ•°å±‚é¢çš„å†²çªé—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šOrthAlignçš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†å‚æ•°æ›´æ–°ç©ºé—´åˆ†è§£ä¸ºå¤šä¸ªæ­£äº¤å­ç©ºé—´ï¼Œæ¯ä¸ªå­ç©ºé—´å¯¹åº”ä¸€ä¸ªç‰¹å®šçš„åå¥½ç›®æ ‡ã€‚é€šè¿‡åœ¨è¿™äº›æ­£äº¤å­ç©ºé—´ä¸­è¿›è¡Œä¼˜åŒ–ï¼Œå¯ä»¥ç¡®ä¿ä¸åŒåå¥½ç›®æ ‡çš„æ›´æ–°äº’ä¸å¹²æ‰°ï¼Œä»è€Œé¿å…å†²çªå’Œæƒè¡¡ã€‚è¿™ç§æ–¹æ³•ç±»ä¼¼äºåœ¨å¤šç»´ç©ºé—´ä¸­å¯»æ‰¾ç›¸äº’å‚ç›´çš„æ–¹å‘ï¼Œä½¿å¾—æ²¿ç€ä¸€ä¸ªæ–¹å‘çš„ç§»åŠ¨ä¸ä¼šå½±å“å…¶ä»–æ–¹å‘çš„ä½ç½®ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šOrthAlignçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼š1. **æ¢¯åº¦è®¡ç®—**ï¼šè®¡ç®—æ¨¡å‹åœ¨æ¯ä¸ªåå¥½ç›®æ ‡ä¸Šçš„æ¢¯åº¦ã€‚2. **å­ç©ºé—´åˆ†è§£**ï¼šä½¿ç”¨æ­£äº¤å­ç©ºé—´åˆ†è§£æŠ€æœ¯ï¼Œå°†å‚æ•°æ›´æ–°ç©ºé—´åˆ†è§£ä¸ºå¤šä¸ªæ­£äº¤å­ç©ºé—´ï¼Œæ¯ä¸ªå­ç©ºé—´å¯¹åº”ä¸€ä¸ªåå¥½ç›®æ ‡ã€‚3. **æ¢¯åº¦æŠ•å½±**ï¼šå°†æ¯ä¸ªåå¥½ç›®æ ‡çš„æ¢¯åº¦æŠ•å½±åˆ°å…¶å¯¹åº”çš„æ­£äº¤å­ç©ºé—´ä¸­ã€‚4. **å‚æ•°æ›´æ–°**ï¼šä½¿ç”¨æŠ•å½±åçš„æ¢¯åº¦æ›´æ–°æ¨¡å‹å‚æ•°ã€‚5. **ç¨³å®šæ€§ä¿è¯**ï¼šé€šè¿‡è°±èŒƒæ•°çº¦æŸï¼Œç¡®ä¿å‚æ•°æ›´æ–°çš„ç¨³å®šæ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šOrthAlignæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå…¶ä½¿ç”¨æ­£äº¤å­ç©ºé—´åˆ†è§£æ¥è§£å†³å¤šç›®æ ‡å¯¹é½ä¸­çš„æ¢¯åº¦å†²çªã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒOrthAlignç›´æ¥åœ¨å‚æ•°å±‚é¢è§£å†³å†²çªï¼Œè€Œä¸æ˜¯è¯•å›¾é€šè¿‡çº¦æŸæˆ–æ•°æ®é€‰æ‹©æ¥ç¼“è§£å†²çªã€‚è¿™ç§æ–¹æ³•å¯ä»¥æ›´æœ‰æ•ˆåœ°é¿å…ä¸åŒåå¥½ç›®æ ‡ä¹‹é—´çš„æƒè¡¡ï¼Œå¹¶æé«˜æ•´ä½“æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šOrthAlignçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1. **æ­£äº¤å­ç©ºé—´åˆ†è§£æ–¹æ³•**ï¼šå…·ä½“é‡‡ç”¨ä½•ç§æ­£äº¤åŒ–æ–¹æ³•ï¼ˆä¾‹å¦‚Gram-Schmidtæ­£äº¤åŒ–ï¼‰æ¥åˆ†è§£å‚æ•°ç©ºé—´ã€‚2. **è°±èŒƒæ•°çº¦æŸ**ï¼šä¸ºäº†ä¿è¯è®­ç»ƒçš„ç¨³å®šæ€§ï¼Œè®ºæ–‡å¼•å…¥äº†è°±èŒƒæ•°çº¦æŸæ¥é™åˆ¶å‚æ•°æ›´æ–°çš„å¹…åº¦ã€‚3. **æŸå¤±å‡½æ•°è®¾è®¡**ï¼šé’ˆå¯¹ä¸åŒçš„åå¥½ç›®æ ‡ï¼Œè®¾è®¡åˆé€‚çš„æŸå¤±å‡½æ•°æ¥æŒ‡å¯¼æ¨¡å‹çš„è®­ç»ƒã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

OrthAlignåœ¨helpfulã€harmlesså’Œtruthfulä¸‰ä¸ªç»´åº¦ä¸Šè¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç›®æ ‡å¯¹é½åï¼Œå®ç°äº†34.61%åˆ°50.89%çš„æœ€å¤§å•åå¥½æ”¹è¿›ã€‚æ­¤å¤–ï¼ŒOrthAlignçš„å¹³å‡æ€»ä½“å¥–åŠ±æé«˜äº†13.96%ï¼Œè¯æ˜äº†å…¶åœ¨è§£å†³å¤šç›®æ ‡å†²çªæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒOrthAlignæ˜¯ä¸€ç§æœ‰å‰æ™¯çš„å¤šç›®æ ‡å¯¹é½æ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

OrthAlignæŠ€æœ¯å¯åº”ç”¨äºå„ç§éœ€è¦å¤šç›®æ ‡å¯¹é½çš„å¤§å‹è¯­è¨€æ¨¡å‹åº”ç”¨åœºæ™¯ï¼Œä¾‹å¦‚å¯¹è¯ç³»ç»Ÿã€å†…å®¹ç”Ÿæˆå’Œæ™ºèƒ½åŠ©æ‰‹ã€‚é€šè¿‡ç¡®ä¿ä¸åŒç›®æ ‡ä¹‹é—´çš„å¹³è¡¡ï¼Œå¯ä»¥æé«˜æ¨¡å‹çš„æ•´ä½“æ€§èƒ½å’Œç”¨æˆ·æ»¡æ„åº¦ã€‚è¯¥æŠ€æœ¯è¿˜æœ‰æ½œåŠ›åº”ç”¨äºå…¶ä»–æœºå™¨å­¦ä¹ é¢†åŸŸï¼Œä¾‹å¦‚å¤šä»»åŠ¡å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language model (LLM) alignment faces a critical dilemma when addressing multiple human preferences: improvements in one dimension frequently come at the expense of others, creating unavoidable trade-offs between competing objectives like helpfulness and harmlessness. While prior work mainly focuses on constraint-based optimization algorithms and data selection strategies to mitigate conflicts, these approaches overlook the fundamental issue of resolving conflicts directly at the parameter level. In this paper, we present OrthAlign, an innovative approach that pioneers a new paradigm by leveraging orthogonal subspace decomposition to fundamentally resolve gradient-level conflicts in multi-objective preference alignment. OrthAlign strategically decomposes parameter update spaces into orthogonal subspaces, ensuring that optimization toward different preferences occurs in mathematically non-interfering directions. Building upon this, we provide theoretical guarantees demonstrating that when parameter increments satisfy both orthogonal subspace constraints and spectral norm bounds, the resulting updates exhibit linear Lipschitz growth rather than exponential instability, ensuring stable convergence across all preference dimensions. Extensive experiments show that: I. OrthAlign achieves maximum single-preference improvements ranging from 34.61% to 50.89% after multiple-objective alignment across helpful, harmless, and truthful dimensions. II. With an average overall reward improvement of 13.96%.

