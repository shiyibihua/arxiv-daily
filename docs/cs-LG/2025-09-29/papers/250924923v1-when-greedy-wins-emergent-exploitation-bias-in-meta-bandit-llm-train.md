---
layout: default
title: When Greedy Wins: Emergent Exploitation Bias in Meta-Bandit LLM Training
---

# When Greedy Wins: Emergent Exploitation Bias in Meta-Bandit LLM Training

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.24923" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.24923v1</a>
  <a href="https://arxiv.org/pdf/2509.24923.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.24923v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.24923v1', 'When Greedy Wins: Emergent Exploitation Bias in Meta-Bandit LLM Training')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sanxing Chen, Xiaoyin Chen, Yukun Huang, Roy Xie, Bhuwan Dhingra

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**å…ƒ-Bandit LLMè®­ç»ƒä¸­æ¶Œç°çš„è´ªå©ªåˆ©ç”¨åå·®ç ”ç©¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `åºåˆ—å†³ç­–` `å¤šè‡‚è€è™æœº` `å¼ºåŒ–å­¦ä¹ ` `ç›‘ç£å¾®è°ƒ` `æ¢ç´¢åˆ©ç”¨` `è´ªå©ªåˆ©ç”¨åå·®`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åºåˆ—å†³ç­–ä»»åŠ¡ä¸­æ¢ç´¢ä¸è¶³ï¼Œå¯¼è‡´æ¬¡ä¼˜è¡¨ç°ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆæå‡å…¶æ¢ç´¢èƒ½åŠ›ã€‚
2. é€šè¿‡ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒLLMï¼Œå¹¶è®¾è®¡ç­–ç•¥æ€§å¥–åŠ±ä¿¡å·ï¼Œé¼“åŠ±æ›´æœ‰æ•ˆçš„æ¢ç´¢å’Œåˆ©ç”¨ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•è®­ç»ƒçš„æ™ºèƒ½ä½“åœ¨å¤šè‡‚è€è™æœºä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ³›åŒ–èƒ½åŠ›å¼ºï¼Œä½†å­˜åœ¨è´ªå©ªåˆ©ç”¨åå·®ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹(LLM)æœ‰æ½œåŠ›æˆä¸ºè‡ªä¸»æ™ºèƒ½ä½“ï¼Œä½†å®ƒä»¬åœ¨åºåˆ—å†³ç­–ä¸­å¸¸å¸¸è¿›è¡Œæ¬¡ä¼˜æ¢ç´¢ã€‚æœ€è¿‘çš„ç ”ç©¶è¯•å›¾é€šè¿‡ç›‘ç£å¾®è°ƒ(SFT)æˆ–å¼ºåŒ–å­¦ä¹ (RL)æ¥å¢å¼ºLLMçš„èƒ½åŠ›ï¼Œä»è€Œæé«˜åœ¨ç»å…¸å¤šè‡‚è€è™æœºä»»åŠ¡ä¸Šçš„åæ‚”å€¼ã€‚ç„¶è€Œï¼Œè¿™äº›å­¦ä¹ æ–¹æ³•å¦‚ä½•å¡‘é€ æ¢ç´¢ç­–ç•¥ä»¥åŠå®ƒä»¬çš„æ³›åŒ–èƒ½åŠ›å¦‚ä½•ä»ç„¶ä¸æ¸…æ¥šã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨SFTåœ¨ä¸“å®¶è½¨è¿¹ä¸Šè®­ç»ƒLLMä»¥åŠä½¿ç”¨RLå’Œä¸€ç³»åˆ—å®šåˆ¶çš„å¥–åŠ±ä¿¡å·ï¼ˆåŒ…æ‹¬ç”¨äºå‡å°‘æ–¹å·®çš„ç­–ç•¥æ€§ã€åæ‚”å€¼å¡‘é€ çš„å¥–åŠ±ï¼Œä»¥åŠæ”¯æŒoracleæ¨¡ä»¿çš„ç®—æ³•å¥–åŠ±ï¼‰æ¥ç ”ç©¶è¿™ä¸¤ç§èŒƒå¼ã€‚ç”±æ­¤äº§ç”Ÿçš„æ™ºèƒ½ä½“ä¼˜äºé¢„è®­ç»ƒæ¨¡å‹ï¼Œå¹¶ä¸”å®ç°äº†ä¸ä¸Šé™ç½®ä¿¡åŒºé—´(UCB)å’Œæ±¤æ™®æ£®é‡‡æ ·ç›¸å½“çš„æ€§èƒ½ï¼Œå¹¶ä¸”å¯¹6å€æ›´é•¿çš„horizonå’Œè·¨è€è™æœºæ—çš„æ³›åŒ–èƒ½åŠ›å¾ˆå¼ºã€‚è¡Œä¸ºåˆ†æè¡¨æ˜ï¼Œæ”¶ç›Šé€šå¸¸æºäºæ›´å¤æ‚ä½†æ›´è´ªå©ªçš„åˆ©ç”¨ï¼šRL/SFTæ™ºèƒ½ä½“æ¯”é¢„è®­ç»ƒæ¨¡å‹æ›´å®¹æ˜“å‡ºç°æ—©æœŸç¾éš¾æ€§å¤±è´¥ï¼Œè¿‡æ—©åœ°æ”¾å¼ƒæ¢ç´¢ã€‚æ­¤å¤–ï¼Œè¢«è®­ç»ƒæ¥æ¨¡ä»¿UCBçš„æ™ºèƒ½ä½“é€šè¿‡é‡‡ç”¨æ›´å…·åˆ©ç”¨æ€§çš„å˜ä½“æ¥å­¦ä¼šè¶…è¶Šå®ƒä»¬çš„è€å¸ˆã€‚æˆ‘ä»¬çš„å‘ç°é˜æ˜äº†æ¯ç§è®­ç»ƒèŒƒå¼ä½•æ—¶æ›´å¯å–ï¼Œå¹¶æå€¡å®šåˆ¶çš„å¥–åŠ±è®¾è®¡å’Œè¶…è¶Šå¹³å‡åæ‚”å€¼çš„è¯„ä¼°ï¼Œä»¥ä¿ƒè¿›ç¨³å¥çš„æ¢ç´¢è¡Œä¸ºã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åºåˆ—å†³ç­–ä»»åŠ¡ï¼ˆç‰¹åˆ«æ˜¯å¤šè‡‚è€è™æœºé—®é¢˜ï¼‰ä¸­æ¢ç´¢ç­–ç•¥æ¬¡ä¼˜çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚ç›´æ¥ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹æˆ–ç®€å•åœ°è¿›è¡Œç›‘ç£å¾®è°ƒï¼Œæ— æ³•æœ‰æ•ˆåœ°å¹³è¡¡æ¢ç´¢å’Œåˆ©ç”¨ï¼Œå¯¼è‡´åæ‚”å€¼è¾ƒé«˜ã€‚æ­¤å¤–ï¼Œç°æœ‰æ–¹æ³•å¯¹æ¢ç´¢ç­–ç•¥çš„å¡‘é€ æ–¹å¼ä»¥åŠæ³›åŒ–èƒ½åŠ›ç¼ºä¹æ·±å…¥ç†è§£ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸¤ç§èŒƒå¼æ¥è®­ç»ƒLLMï¼Œå¹¶è®¾è®¡å®šåˆ¶çš„å¥–åŠ±ä¿¡å·ï¼Œä»¥é¼“åŠ±æ›´æœ‰æ•ˆçš„æ¢ç´¢å’Œåˆ©ç”¨ã€‚é€šè¿‡æ¨¡ä»¿ä¸“å®¶è½¨è¿¹ï¼ˆSFTï¼‰æˆ–ä¼˜åŒ–ç‰¹å®šå¥–åŠ±å‡½æ•°ï¼ˆRLï¼‰ï¼Œä½¿LLMå­¦ä¹ åˆ°æ›´å¥½çš„ç­–ç•¥ã€‚åŒæ—¶ï¼Œé€šè¿‡è¡Œä¸ºåˆ†æï¼Œæ·±å…¥ç†è§£ä¸åŒè®­ç»ƒæ–¹æ³•å¯¹æ¢ç´¢ç­–ç•¥çš„å½±å“ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦é˜¶æ®µï¼š1) æ•°æ®å‡†å¤‡ï¼šç”Ÿæˆæˆ–æ”¶é›†å¤šè‡‚è€è™æœºä»»åŠ¡çš„ä¸“å®¶è½¨è¿¹æ•°æ®ã€‚2) æ¨¡å‹è®­ç»ƒï¼šä½¿ç”¨SFTæˆ–RLè®­ç»ƒLLMã€‚SFTä½¿ç”¨ä¸“å®¶è½¨è¿¹è¿›è¡Œç›‘ç£å­¦ä¹ ï¼ŒRLä½¿ç”¨å®šåˆ¶çš„å¥–åŠ±å‡½æ•°è¿›è¡Œè®­ç»ƒã€‚3) ç­–ç•¥è¯„ä¼°ï¼šåœ¨ä¸åŒé•¿åº¦çš„horizonå’Œä¸åŒè€è™æœºæ—ä¸Šè¯„ä¼°è®­ç»ƒåçš„LLMçš„æ€§èƒ½ï¼ŒåŒ…æ‹¬å¹³å‡åæ‚”å€¼å’Œæ¢ç´¢è¡Œä¸ºã€‚4) è¡Œä¸ºåˆ†æï¼šåˆ†æLLMçš„æ¢ç´¢ç­–ç•¥ï¼Œä¾‹å¦‚æ¢ç´¢çš„æ·±åº¦å’Œå¹¿åº¦ï¼Œä»¥åŠå¯¹æ—©æœŸå¤±è´¥çš„ååº”ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æ·±å…¥ç ”ç©¶äº†SFTå’ŒRLä¸¤ç§è®­ç»ƒèŒƒå¼å¯¹LLMæ¢ç´¢ç­–ç•¥çš„å½±å“ï¼Œæ­ç¤ºäº†è´ªå©ªåˆ©ç”¨åå·®ã€‚2) è®¾è®¡äº†å®šåˆ¶çš„å¥–åŠ±ä¿¡å·ï¼ŒåŒ…æ‹¬ç­–ç•¥æ€§ã€åæ‚”å€¼å¡‘é€ çš„å¥–åŠ±ï¼Œä»¥åŠæ”¯æŒoracleæ¨¡ä»¿çš„ç®—æ³•å¥–åŠ±ï¼Œä»¥æé«˜RLè®­ç»ƒçš„æ•ˆç‡å’Œç¨³å®šæ€§ã€‚3) é€šè¿‡è¡Œä¸ºåˆ†æï¼Œæ­ç¤ºäº†RL/SFTæ™ºèƒ½ä½“æ¯”é¢„è®­ç»ƒæ¨¡å‹æ›´å®¹æ˜“å‡ºç°æ—©æœŸç¾éš¾æ€§å¤±è´¥ï¼Œè¿‡æ—©åœ°æ”¾å¼ƒæ¢ç´¢ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨RLè®­ç»ƒä¸­ï¼Œä½¿ç”¨äº†å¤šç§å®šåˆ¶çš„å¥–åŠ±å‡½æ•°ï¼Œä¾‹å¦‚ï¼š1) Regret-shaped rewardï¼šæ ¹æ®åæ‚”å€¼çš„å¤§å°æ¥è°ƒæ•´å¥–åŠ±ï¼Œä»¥é¼“åŠ±æ›´æœ‰æ•ˆçš„æ¢ç´¢ã€‚2) Algorithmic rewardï¼šæ¨¡ä»¿UCBç®—æ³•çš„å¥–åŠ±å‡½æ•°ï¼Œä½¿LLMå­¦ä¹ UCBçš„ç­–ç•¥ã€‚åœ¨SFTè®­ç»ƒä¸­ï¼Œä½¿ç”¨äº†ä¸“å®¶è½¨è¿¹æ•°æ®ï¼Œè¿™äº›è½¨è¿¹ç”±UCBæˆ–Thompson Samplingç­‰ç®—æ³•ç”Ÿæˆã€‚å…³é”®å‚æ•°åŒ…æ‹¬å­¦ä¹ ç‡ã€batch sizeã€horizoné•¿åº¦ç­‰ã€‚ç½‘ç»œç»“æ„ä½¿ç”¨äº†æ ‡å‡†çš„Transformerç»“æ„ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé€šè¿‡SFTå’ŒRLè®­ç»ƒçš„LLMåœ¨å¤šè‡‚è€è™æœºä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºé¢„è®­ç»ƒæ¨¡å‹ï¼Œå¹¶è¾¾åˆ°äº†ä¸UCBå’ŒThompson Samplingç›¸å½“çš„æ€§èƒ½ã€‚å°¤å…¶æ˜¯åœ¨æ³›åŒ–èƒ½åŠ›æ–¹é¢ï¼Œè®­ç»ƒåçš„LLMåœ¨6å€æ›´é•¿çš„horizonå’Œè·¨è€è™æœºæ—ä¸Šè¡¨ç°å‡ºå¾ˆå¼ºçš„é²æ£’æ€§ã€‚ç„¶è€Œï¼Œè¡Œä¸ºåˆ†ææ­ç¤ºäº†RL/SFTæ™ºèƒ½ä½“å­˜åœ¨è´ªå©ªåˆ©ç”¨åå·®ï¼Œæ›´å®¹æ˜“å‡ºç°æ—©æœŸç¾éš¾æ€§å¤±è´¥ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå¼€å‘æ›´æ™ºèƒ½ã€æ›´è‡ªä¸»çš„LLMæ™ºèƒ½ä½“ï¼Œç”¨äºè§£å†³å®é™…çš„åºåˆ—å†³ç­–é—®é¢˜ï¼Œä¾‹å¦‚æ¨èç³»ç»Ÿã€èµ„æºåˆ†é…ã€è‡ªåŠ¨é©¾é©¶ç­‰ã€‚é€šè¿‡ç†è§£å’Œç¼“è§£è´ªå©ªåˆ©ç”¨åå·®ï¼Œå¯ä»¥æé«˜LLMæ™ºèƒ½ä½“çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä½¿å…¶åœ¨å¤æ‚ç¯å¢ƒä¸­åšå‡ºæ›´æ˜æ™ºçš„å†³ç­–ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶ä¹Ÿä¸ºè®¾è®¡æ›´æœ‰æ•ˆçš„LLMè®­ç»ƒæ–¹æ³•æä¾›äº†æŒ‡å¯¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> While Large Language Models (LLMs) hold promise to become autonomous agents, they often explore suboptimally in sequential decision-making. Recent work has sought to enhance this capability via supervised fine-tuning (SFT) or reinforcement learning (RL), improving regret on the classic multi-armed bandit task. However, it remains unclear how these learning methods shape exploration strategies and how well they generalize. We investigate both paradigms by training LLMs with SFT on expert trajectories and RL with a range of tailored reward signals including a strategic, regret-shaped reward to reduce variance, and an algorithmic reward that enables oracle imitation. The resulting agents outperform pre-trained models and achieve performance comparable to Upper Confidence Bound (UCB) and Thompson Sampling, with robust generalization to 6x longer horizons and across bandit families. Behavioral analysis reveals that gains often stem from more sophisticated but greedier exploitation: RL/SFT agents are more prone to early catastrophic failure than pre-trained models, prematurely abandoning exploration. Furthermore, agents trained to imitate UCB learn to outperform their teacher by adopting more exploitative variants. Our findings clarify when each training paradigm is preferable and advocate tailored reward design and evaluation beyond average regret to promote robust exploratory behavior.

