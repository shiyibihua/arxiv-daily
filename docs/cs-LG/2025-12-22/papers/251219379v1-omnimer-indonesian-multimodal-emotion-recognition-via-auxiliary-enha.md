---
layout: default
title: OmniMER: Indonesian Multimodal Emotion Recognition via Auxiliary-Enhanced LLM Adaptation
---

# OmniMER: Indonesian Multimodal Emotion Recognition via Auxiliary-Enhanced LLM Adaptation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.19379" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.19379v1</a>
  <a href="https://arxiv.org/pdf/2512.19379.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.19379v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.19379v1', 'OmniMER: Indonesian Multimodal Emotion Recognition via Auxiliary-Enhanced LLM Adaptation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xueming Yan, Boyan Xu, Yaochu Jin, Lixian Xiao, Wenlong Ye, Runyang Cai, Zeqi Zheng, Jingfa Liu, Aimin Yang

**åˆ†ç±»**: cs.LG, cs.AI, cs.MM

**å‘å¸ƒæ—¥æœŸ**: 2025-12-22

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/yanxm01/INDOMER)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOmniMERä»¥è§£å†³å°å°¼å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«` `å°å°¼è¯­` `æƒ…æ„Ÿè®¡ç®—` `è¾…åŠ©ä»»åŠ¡` `è·¨æ¨¡æ€èåˆ` `æ•°æ®é›†æ„å»º` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å°å°¼è¯­åœ¨å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«ç ”ç©¶ä¸­æœåŠ¡ä¸è¶³ï¼Œé¢ä¸´è·¨æ¨¡æ€ä¸ä¸€è‡´æ€§å’Œé•¿å°¾ç±»åˆ†å¸ƒç­‰æŒ‘æˆ˜ã€‚
2. æå‡ºOmniMERæ¡†æ¶ï¼Œé€šè¿‡æƒ…æ„Ÿå…³é”®è¯æå–ã€é¢éƒ¨è¡¨æƒ…åˆ†æå’ŒéŸ³é¢‘éŸµå¾‹åˆ†æç­‰è¾…åŠ©ä»»åŠ¡å¢å¼ºæƒ…æ„Ÿè¯†åˆ«èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒOmniMERåœ¨æƒ…æ„Ÿåˆ†ç±»å’Œè¯†åˆ«ä¸Šåˆ†åˆ«æå‡äº†7.6å’Œ22.1ä¸ªç»å¯¹ç‚¹ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°å°¼è¯­ä½œä¸ºä¸€ç§æœ‰è¶…è¿‡2äº¿ä½¿ç”¨è€…çš„è¯­è¨€ï¼Œåœ¨å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«ç ”ç©¶ä¸­ä»ç„¶å¤„äºæœåŠ¡ä¸è¶³çš„çŠ¶æ€ã€‚æˆ‘ä»¬å¼•å…¥äº†IndoMERï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªé’ˆå¯¹å°å°¼çš„å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«åŸºå‡†ï¼ŒåŒ…å«æ¥è‡ª203ä½è¯´è¯è€…çš„1,944ä¸ªè§†é¢‘ç‰‡æ®µï¼Œå…·æœ‰æ—¶é—´å¯¹é½çš„æ–‡æœ¬ã€éŸ³é¢‘å’Œè§†è§‰æ³¨é‡Šï¼Œæ¶µç›–ä¸ƒç§æƒ…æ„Ÿç±»åˆ«ã€‚è¯¥æ•°æ®é›†å±•ç¤ºäº†è¯¸å¦‚è·¨æ¨¡æ€ä¸ä¸€è‡´æ€§å’Œå—å°å°¼æ–‡åŒ–äº¤æµè§„èŒƒå½±å“çš„é•¿å°¾ç±»åˆ†å¸ƒç­‰ç°å®æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†OmniMERï¼Œä¸€ä¸ªåŸºäºQwen2.5-Omniçš„å¤šæ¨¡æ€é€‚åº”æ¡†æ¶ï¼Œé€šè¿‡æƒ…æ„Ÿå…³é”®è¯æå–ã€é¢éƒ¨è¡¨æƒ…åˆ†æå’ŒéŸ³é¢‘éŸµå¾‹åˆ†æç­‰ä¸‰ä¸ªè¾…åŠ©æ¨¡æ€ç‰¹å®šæ„ŸçŸ¥ä»»åŠ¡æ¥å¢å¼ºæƒ…æ„Ÿè¯†åˆ«ã€‚è¿™äº›è¾…åŠ©ä»»åŠ¡å¸®åŠ©æ¨¡å‹åœ¨èåˆå‰è¯†åˆ«æ¯ç§æ¨¡æ€ä¸­çš„æƒ…æ„Ÿç›¸å…³çº¿ç´¢ï¼Œä»è€Œå‡å°‘åœ¨ä½èµ„æºç¯å¢ƒä¸‹å¯¹è™šå‡ç›¸å…³æ€§çš„ä¾èµ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOmniMERåœ¨æƒ…æ„Ÿåˆ†ç±»å’Œæƒ…æ„Ÿè¯†åˆ«ä¸Šåˆ†åˆ«è¾¾åˆ°äº†0.582å’Œ0.454çš„Macro-F1ï¼Œè¾ƒåŸºçº¿æ¨¡å‹åˆ†åˆ«æå‡äº†7.6å’Œ22.1ä¸ªç»å¯¹ç‚¹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å°å°¼è¯­å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«ä¸­çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯è·¨æ¨¡æ€ä¸ä¸€è‡´æ€§å’Œé•¿å°¾ç±»åˆ†å¸ƒçš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨ä½èµ„æºç¯å¢ƒä¸‹å®¹æ˜“ä¾èµ–è™šå‡ç›¸å…³æ€§ï¼Œå¯¼è‡´è¯†åˆ«æ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºOmniMERæ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥ä¸‰ä¸ªè¾…åŠ©æ¨¡æ€ç‰¹å®šæ„ŸçŸ¥ä»»åŠ¡ï¼Œå¸®åŠ©æ¨¡å‹åœ¨èåˆå‰è¯†åˆ«æƒ…æ„Ÿç›¸å…³çº¿ç´¢ï¼Œä»è€Œæé«˜æƒ…æ„Ÿè¯†åˆ«çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šOmniMERæ¡†æ¶åŸºäºQwen2.5-Omniï¼ŒåŒ…å«ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šæƒ…æ„Ÿå…³é”®è¯æå–ã€é¢éƒ¨è¡¨æƒ…åˆ†æå’ŒéŸ³é¢‘éŸµå¾‹åˆ†æã€‚æ¯ä¸ªæ¨¡å—åˆ†åˆ«å¤„ç†æ–‡æœ¬ã€è§†é¢‘å’ŒéŸ³é¢‘æ•°æ®ï¼Œæœ€ç»ˆè¿›è¡Œèåˆä»¥å®ç°æƒ…æ„Ÿè¯†åˆ«ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºé€šè¿‡è¾…åŠ©ä»»åŠ¡çš„è®¾è®¡ï¼Œå¢å¼ºäº†æ¨¡å‹å¯¹ä¸åŒæ¨¡æ€ä¸­æƒ…æ„Ÿçº¿ç´¢çš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œæ˜¾è‘—å‡å°‘äº†å¯¹è™šå‡ç›¸å…³æ€§çš„ä¾èµ–ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒOmniMERåœ¨å¤šæ¨¡æ€èåˆä¸­æ›´å…·æœ‰æ•ˆæ€§å’Œé€‚åº”æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ¯ä¸ªè¾…åŠ©ä»»åŠ¡çš„è¾“å‡ºï¼Œå¹¶é€šè¿‡è°ƒèŠ‚è¶…å‚æ•°æ¥å¹³è¡¡ä¸åŒæ¨¡æ€çš„å½±å“ï¼Œç¡®ä¿æ¨¡å‹åœ¨å¤šæ¨¡æ€èåˆæ—¶çš„ç¨³å®šæ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.19379v1/pdf/France.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.19379v1/x1.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.19379v1/x2.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒOmniMERåœ¨æƒ…æ„Ÿåˆ†ç±»ä»»åŠ¡ä¸­è¾¾åˆ°äº†0.582çš„Macro-F1ï¼Œåœ¨æƒ…æ„Ÿè¯†åˆ«ä»»åŠ¡ä¸­è¾¾åˆ°äº†0.454ï¼Œåˆ†åˆ«è¾ƒåŸºçº¿æ¨¡å‹æå‡äº†7.6å’Œ22.1ä¸ªç»å¯¹ç‚¹ã€‚æ­¤å¤–ï¼Œè·¨è¯­è¨€è¯„ä¼°è¡¨æ˜è¯¥æ¡†æ¶å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨ç¤¾äº¤åª’ä½“åˆ†æã€æƒ…æ„Ÿè®¡ç®—å’Œäººæœºäº¤äº’ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æå‡å°å°¼è¯­çš„æƒ…æ„Ÿè¯†åˆ«èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œåˆ†æå°å°¼æ–‡åŒ–ä¸­çš„æƒ…æ„Ÿè¡¨è¾¾ï¼Œä¿ƒè¿›ç›¸å…³æŠ€æœ¯åœ¨ä¸œå—äºšåœ°åŒºçš„è½åœ°ä¸å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Indonesian, spoken by over 200 million people, remains underserved in multimodal emotion recognition research despite its dominant presence on Southeast Asian social media platforms. We introduce IndoMER, the first multimodal emotion recognition benchmark for Indonesian, comprising 1,944 video segments from 203 speakers with temporally aligned text, audio, and visual annotations across seven emotion categories. The dataset exhibits realistic challenges including cross-modal inconsistency and long-tailed class distributions shaped by Indonesian cultural communication norms. To address these challenges, we propose OmniMER, a multimodal adaptation framework built upon Qwen2.5-Omni that enhances emotion recognition through three auxiliary modality-specific perception tasks: emotion keyword extraction for text, facial expression analysis for video, and prosody analysis for audio. These auxiliary tasks help the model identify emotion-relevant cues in each modality before fusion, reducing reliance on spurious correlations in low-resource settings. Experiments on IndoMER show that OmniMER achieves 0.582 Macro-F1 on sentiment classification and 0.454 on emotion recognition, outperforming the base model by 7.6 and 22.1 absolute points respectively. Cross-lingual evaluation on the Chinese CH-SIMS dataset further demonstrates the generalizability of the proposed framework. The dataset and code are publicly available. https://github.com/yanxm01/INDOMER

