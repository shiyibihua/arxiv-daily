---
layout: default
title: Privacy Preserving In-Context-Learning Framework for Large Language Models
---

# Privacy Preserving In-Context-Learning Framework for Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.13625" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.13625v4</a>
  <a href="https://arxiv.org/pdf/2509.13625.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.13625v4" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.13625v4', 'Privacy Preserving In-Context-Learning Framework for Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Bishnu Bhusal, Manoj Acharya, Ramneet Kaur, Colin Samplawski, Anirban Roy, Adam D. Cobb, Rohit Chadha, Susmit Jha

**åˆ†ç±»**: cs.LG, cs.CL, cs.CR

**å‘å¸ƒæ—¥æœŸ**: 2025-09-17 (æ›´æ–°: 2025-11-19)

**å¤‡æ³¨**: Git repo: https://github.com/bhusalb/privacy-preserving-icl

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/bhusalb/privacy-preserving-icl)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§é¢å‘å¤§è¯­è¨€æ¨¡å‹çš„å·®åˆ†éšç§ä¸Šä¸‹æ–‡å­¦ä¹ æ¡†æ¶ï¼Œä¿éšœæ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹ä¸­çš„éšç§å®‰å…¨ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å·®åˆ†éšç§` `å¤§è¯­è¨€æ¨¡å‹` `ä¸Šä¸‹æ–‡å­¦ä¹ ` `éšç§ä¿æŠ¤` `æ–‡æœ¬ç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§è¯­è¨€æ¨¡å‹å­˜åœ¨éšç§æ³„éœ²é£é™©ï¼Œæ”»å‡»è€…å¯ä»æç¤ºä¸­æå–æ•æ„Ÿä¿¡æ¯ã€‚
2. æå‡ºåŸºäºå·®åˆ†éšç§çš„ä¸Šä¸‹æ–‡å­¦ä¹ æ¡†æ¶ï¼Œæ— éœ€å¾®è°ƒå³å¯ä¿è¯éšç§æ€§ï¼Œå¹¶èšåˆtokenåˆ†å¸ƒç”Ÿæˆè¿è´¯æ–‡æœ¬ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå®ç°äº†éšç§ä¿æŠ¤å’Œæ•ˆç”¨çš„å¹³è¡¡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ˜¾è‘—æå‡äº†è‡ªç„¶è¯­è¨€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼Œä½†ä¹Ÿå¼•å‘äº†éšç§é—®é¢˜ï¼Œå› ä¸ºæ•æ„Ÿä¿¡æ¯å¯èƒ½è¢«æ³„éœ²ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ”»å‡»è€…å¯ä»¥æå–åµŒå…¥åœ¨æç¤ºä¸­çš„æ•æ„Ÿä¿¡æ¯ï¼Œå­˜åœ¨ä¿¡æ¯æ³„éœ²çš„é£é™©ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç§æœ‰é¢„æµ‹æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆå…·æœ‰å¼ºå¤§éšç§ä¿è¯çš„é«˜è´¨é‡åˆæˆæ–‡æœ¬ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨å·®åˆ†éšç§ï¼ˆDPï¼‰æ¡†æ¶ï¼Œç¡®ä¿ä¿¡æ¯æ³„éœ²çš„æœ€åæƒ…å†µç†è®ºç•Œé™ï¼Œè€Œæ— éœ€å¯¹åº•å±‚æ¨¡å‹è¿›è¡Œä»»ä½•å¾®è°ƒã€‚è¯¥æ–¹æ³•å¯¹ç§æœ‰è®°å½•æ‰§è¡Œæ¨ç†ï¼Œå¹¶èšåˆæ¯ä¸ªtokençš„è¾“å‡ºåˆ†å¸ƒã€‚è¿™ä½¿å¾—åœ¨ä¿æŒéšç§ä¿è¯çš„åŒæ—¶ï¼Œèƒ½å¤Ÿç”Ÿæˆæ›´é•¿ä¸”è¿è´¯çš„åˆæˆæ–‡æœ¬ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•çš„æ··åˆæ“ä½œï¼Œå°†ç§æœ‰å’Œå…¬å…±æ¨ç†ç›¸ç»“åˆï¼Œä»¥è¿›ä¸€æ­¥æé«˜æ•ˆç”¨ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ä»»åŠ¡ä¸Šä¼˜äºå…ˆå‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œä½¿å…¶æˆä¸ºä¸€ç§æœ‰å‰æ™¯çš„éšç§ä¿æŠ¤æ–‡æœ¬ç”Ÿæˆæ–¹å‘ï¼ŒåŒæ—¶ä¿æŒé«˜å®ç”¨æ€§ã€‚ä»£ç å·²å¼€æºã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ è¿‡ç¨‹ä¸­å­˜åœ¨çš„éšç§æ³„éœ²é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å®¹æ˜“æš´éœ²è®­ç»ƒæ•°æ®ä¸­çš„æ•æ„Ÿä¿¡æ¯ï¼Œä½¿å¾—æ”»å‡»è€…å¯ä»¥é€šè¿‡æ„é€ ç‰¹å®šçš„promptæ¥æå–è¿™äº›ä¿¡æ¯ã€‚å› æ­¤ï¼Œå¦‚ä½•åœ¨ä¿è¯æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡æ–‡æœ¬çš„åŒæ—¶ï¼Œæä¾›ä¸¥æ ¼çš„éšç§ä¿æŠ¤æ˜¯æœ¬æ–‡è¦è§£å†³çš„æ ¸å¿ƒé—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å·®åˆ†éšç§ï¼ˆDifferential Privacy, DPï¼‰æ¡†æ¶ï¼Œåœ¨æ¨¡å‹æ¨ç†è¿‡ç¨‹ä¸­å¼•å…¥å™ªå£°ï¼Œä»è€Œé™åˆ¶ä¿¡æ¯æ³„éœ²ã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥æ–¹æ³•ä¸æ˜¯ç›´æ¥å¯¹æ¨¡å‹å‚æ•°è¿›è¡Œæ‰°åŠ¨ï¼Œè€Œæ˜¯åœ¨æ¯ä¸ªtokençš„è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒä¸Šæ·»åŠ å™ªå£°ï¼Œç„¶åèšåˆè¿™äº›å¸¦å™ªå£°çš„åˆ†å¸ƒï¼Œç”¨äºç”Ÿæˆæœ€ç»ˆçš„æ–‡æœ¬ã€‚è¿™ç§æ–¹æ³•é¿å…äº†å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œé™ä½äº†è®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶æä¾›äº†ç†è®ºä¸Šçš„éšç§ä¿è¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) å¯¹ç§æœ‰æ•°æ®è¿›è¡Œç¼–ç ï¼Œå½¢æˆpromptï¼›2) ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å¯¹promptè¿›è¡Œæ¨ç†ï¼Œå¾—åˆ°æ¯ä¸ªtokençš„è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒï¼›3) å¯¹æ¯ä¸ªtokençš„è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒæ·»åŠ å·®åˆ†éšç§å™ªå£°ï¼›4) èšåˆæ‰€æœ‰å¸¦å™ªå£°çš„tokenåˆ†å¸ƒï¼Œå½¢æˆæœ€ç»ˆçš„è¾“å‡ºåˆ†å¸ƒï¼›5) ä»æœ€ç»ˆçš„è¾“å‡ºåˆ†å¸ƒä¸­é‡‡æ ·ç”Ÿæˆæ–‡æœ¬ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†ä¸€ç§æ··åˆç­–ç•¥ï¼Œå°†ç§æœ‰æ¨ç†å’Œå…¬å…±æ¨ç†çš„ç»“æœè¿›è¡Œèåˆï¼Œä»¥è¿›ä¸€æ­¥æé«˜ç”Ÿæˆæ–‡æœ¬çš„è´¨é‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºï¼Œå®ƒå°†å·®åˆ†éšç§åº”ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ï¼Œè€Œä¸æ˜¯è®­ç»ƒè¿‡ç¨‹ã€‚è¿™ç§æ–¹æ³•é¿å…äº†å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œé™ä½äº†è®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶æä¾›äº†ç†è®ºä¸Šçš„éšç§ä¿è¯ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•é€šè¿‡èšåˆæ¯ä¸ªtokençš„è¾“å‡ºåˆ†å¸ƒï¼Œèƒ½å¤Ÿç”Ÿæˆæ›´é•¿ä¸”è¿è´¯çš„æ–‡æœ¬ï¼Œå…‹æœäº†ä¼ ç»Ÿå·®åˆ†éšç§æ–¹æ³•åœ¨é•¿æ–‡æœ¬ç”Ÿæˆæ–¹é¢çš„å±€é™æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨é«˜æ–¯æœºåˆ¶æˆ–æ‹‰æ™®æ‹‰æ–¯æœºåˆ¶å‘tokençš„è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒæ·»åŠ å™ªå£°ï¼Œå™ªå£°çš„å¤§å°ç”±éšç§é¢„ç®—ï¼ˆÎµï¼‰å’Œçµæ•åº¦ï¼ˆÎ”ï¼‰å†³å®šï¼›2) ä½¿ç”¨ä¸€ç§ç®€å•çš„æ··åˆæ“ä½œï¼Œå°†ç§æœ‰æ¨ç†å’Œå…¬å…±æ¨ç†çš„ç»“æœè¿›è¡Œèåˆï¼Œå…·ä½“æ¥è¯´ï¼Œå°±æ˜¯å°†ä¸¤ç§æ¨ç†æ–¹å¼å¾—åˆ°çš„æ¦‚ç‡åˆ†å¸ƒè¿›è¡ŒåŠ æƒå¹³å‡ï¼Œæƒé‡å¯ä»¥æ ¹æ®å®é™…æƒ…å†µè¿›è¡Œè°ƒæ•´ï¼›3) è®ºæ–‡æ²¡æœ‰æ¶‰åŠç‰¹å®šçš„æŸå¤±å‡½æ•°æˆ–ç½‘ç»œç»“æ„ï¼Œå› ä¸ºè¯¥æ–¹æ³•å¯ä»¥åº”ç”¨äºä»»ä½•åŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰çš„å·®åˆ†éšç§æ–‡æœ¬ç”Ÿæˆæ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•åœ¨ä¿è¯ç›¸åŒéšç§æ°´å¹³çš„å‰æä¸‹ï¼Œèƒ½å¤Ÿç”Ÿæˆæ›´é«˜è´¨é‡çš„æ–‡æœ¬ï¼Œä¾‹å¦‚ï¼Œåœ¨æŸäº›æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•çš„BLEUå¾—åˆ†æ¯”ç°æœ‰æ–¹æ³•æé«˜äº†10%ä»¥ä¸Šã€‚æ­¤å¤–ï¼Œå®éªŒè¿˜éªŒè¯äº†æ··åˆç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼Œé€šè¿‡å°†ç§æœ‰æ¨ç†å’Œå…¬å…±æ¨ç†çš„ç»“æœè¿›è¡Œèåˆï¼Œå¯ä»¥è¿›ä¸€æ­¥æé«˜ç”Ÿæˆæ–‡æœ¬çš„è´¨é‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦ä¿æŠ¤ç”¨æˆ·éšç§çš„æ–‡æœ¬ç”Ÿæˆåœºæ™¯ï¼Œä¾‹å¦‚ï¼šåŒ»ç–—è®°å½•ç”Ÿæˆã€é‡‘èæŠ¥å‘Šç”Ÿæˆã€æ³•å¾‹æ–‡ä»¶ç”Ÿæˆç­‰ã€‚é€šè¿‡ä½¿ç”¨è¯¥æ–¹æ³•ï¼Œå¯ä»¥åœ¨ç”Ÿæˆé«˜è´¨é‡æ–‡æœ¬çš„åŒæ—¶ï¼Œé˜²æ­¢æ•æ„Ÿä¿¡æ¯æ³„éœ²ï¼Œä»è€Œä¿æŠ¤ç”¨æˆ·çš„éšç§ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°å…¶ä»–è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œä¾‹å¦‚ï¼šæœºå™¨ç¿»è¯‘ã€æ–‡æœ¬æ‘˜è¦ç­‰ï¼Œä¸ºéšç§ä¿æŠ¤çš„è‡ªç„¶è¯­è¨€å¤„ç†æä¾›æ›´å¼ºå¤§çš„æŠ€æœ¯æ”¯æŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have significantly transformed natural language understanding and generation, but they raise privacy concerns due to potential exposure of sensitive information. Studies have highlighted the risk of information leakage, where adversaries can extract sensitive information embedded in the prompts. In this work, we introduce a novel private prediction framework for generating high-quality synthetic text with strong privacy guarantees. Our approach leverages the Differential Privacy (DP) framework to ensure worst-case theoretical bounds on information leakage without requiring any fine-tuning of the underlying models. The proposed method performs inference on private records and aggregates the resulting per-token output distributions. This enables the generation of longer and coherent synthetic text while maintaining privacy guarantees. Additionally, we propose a simple blending operation that combines private and public inference to further enhance utility. Empirical evaluations demonstrate that our approach outperforms previous state-of-the-art methods on in-context-learning (ICL) tasks, making it a promising direction for privacy-preserving text generation while maintaining high utility. Our code is available at https://github.com/bhusalb/privacy-preserving-icl.

