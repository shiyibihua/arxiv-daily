---
layout: default
title: MACTAS: Self-Attention-Based Module for Inter-Agent Communication in Multi-Agent Reinforcement Learning
---

# MACTAS: Self-Attention-Based Module for Inter-Agent Communication in Multi-Agent Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.13661" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.13661v2</a>
  <a href="https://arxiv.org/pdf/2508.13661.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.13661v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.13661v2', 'MACTAS: Self-Attention-Based Module for Inter-Agent Communication in Multi-Agent Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Maciej Wojtala, Bogusz StefaÅ„czyk, Dominik Bogucki, Åukasz Lepak, Jakub Strykowski, PaweÅ‚ WawrzyÅ„ski

**åˆ†ç±»**: cs.LG, cs.MA

**å‘å¸ƒæ—¥æœŸ**: 2025-08-19 (æ›´æ–°: 2025-10-15)

**å¤‡æ³¨**: Submitted for AAMAS 2026

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªæ³¨æ„åŠ›æ¨¡å—ä»¥æå‡å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ä¸­çš„é€šä¿¡æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ` `è‡ªæ³¨æ„åŠ›æœºåˆ¶` `é€šä¿¡æ¨¡å—` `ä¿¡æ¯äº¤æ¢` `å¥–åŠ±é©±åŠ¨å­¦ä¹ ` `æ€§èƒ½æå‡` `æ™ºèƒ½ä½“åä½œ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ é€šä¿¡åè®®å¤æ‚ä¸”ä¸å¯å¾®åˆ†ï¼Œé™åˆ¶äº†æ™ºèƒ½ä½“çš„å­¦ä¹ èƒ½åŠ›ã€‚
2. æœ¬æ–‡æå‡ºçš„è‡ªæ³¨æ„åŠ›é€šä¿¡æ¨¡å—å…è®¸æ™ºèƒ½ä½“åœ¨å¥–åŠ±é©±åŠ¨ä¸‹ç”Ÿæˆæ¶ˆæ¯ï¼Œæå‡äº†é€šä¿¡æ•ˆç‡ã€‚
3. åœ¨SMACå’ŒSMACv2åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•å®ç°äº†å¤šä¸ªåœ°å›¾çš„æœ€å…ˆè¿›æ€§èƒ½ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é€šä¿¡å¯¹äºäººç±»æ™ºèƒ½ä½“å…±åŒæ‰§è¡Œå¤æ‚ä»»åŠ¡è‡³å…³é‡è¦ï¼Œè¿™æ¿€å‘äº†å¯¹å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰ä¸­é€šä¿¡æœºåˆ¶çš„å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„MARLé€šä¿¡åè®®å¾€å¾€å¤æ‚ä¸”ä¸å¯å¾®åˆ†ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè‡ªæ³¨æ„åŠ›çš„é€šä¿¡æ¨¡å—ï¼Œèƒ½å¤Ÿåœ¨MARLä¸­å®ç°æ™ºèƒ½ä½“ä¹‹é—´çš„ä¿¡æ¯äº¤æ¢ã€‚è¯¥æ–¹æ³•å®Œå…¨å¯å¾®åˆ†ï¼Œä½¿å¾—æ™ºèƒ½ä½“èƒ½å¤Ÿä»¥å¥–åŠ±é©±åŠ¨çš„æ–¹å¼å­¦ä¹ ç”Ÿæˆæ¶ˆæ¯ã€‚è¯¥æ¨¡å—å¯ä»¥ä¸ä»»ä½•åŠ¨ä½œä»·å€¼å‡½æ•°åˆ†è§£æ–¹æ³•æ— ç¼é›†æˆï¼Œå¹¶å¯è§†ä¸ºè¿™äº›åˆ†è§£çš„æ‰©å±•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨SMACå’ŒSMACv2åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œåœ¨å¤šä¸ªåœ°å›¾ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ä¸­é€šä¿¡åè®®å¤æ‚ä¸”ä¸å¯å¾®åˆ†çš„é—®é¢˜ï¼Œè¿™ä½¿å¾—æ™ºèƒ½ä½“æ— æ³•æœ‰æ•ˆå­¦ä¹ å’Œä¼˜åŒ–å…¶é€šä¿¡ç­–ç•¥ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºäº†ä¸€ç§åŸºäºè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„é€šä¿¡æ¨¡å—ï¼Œä½¿å¾—æ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨å¥–åŠ±é©±åŠ¨ä¸‹ç”Ÿæˆå’Œäº¤æ¢ä¿¡æ¯ï¼Œä»è€Œæå‡å­¦ä¹ æ•ˆç‡å’Œä»»åŠ¡æ‰§è¡Œèƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬è‡ªæ³¨æ„åŠ›é€šä¿¡æ¨¡å—å’ŒåŠ¨ä½œä»·å€¼å‡½æ•°åˆ†è§£æ–¹æ³•ã€‚æ™ºèƒ½ä½“é€šè¿‡è¯¥æ¨¡å—è¿›è¡Œä¿¡æ¯äº¤æ¢ï¼Œæ¨¡å—çš„è®¾è®¡å…è®¸ä¸ç°æœ‰çš„åˆ†è§£æ–¹æ³•æ— ç¼ç»“åˆã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ä¸ªå®Œå…¨å¯å¾®åˆ†çš„é€šä¿¡æ¨¡å—ï¼Œä¸”å…¶å¯è®­ç»ƒå‚æ•°æ•°é‡ä¸æ™ºèƒ½ä½“æ•°é‡æ— å…³ï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¨¡å—è®¾è®¡ä¸­åŒ…å«å›ºå®šæ•°é‡çš„å¯è®­ç»ƒå‚æ•°ï¼ŒæŸå¤±å‡½æ•°é€šè¿‡å¥–åŠ±ä¿¡å·è¿›è¡Œä¼˜åŒ–ï¼Œç½‘ç»œç»“æ„é‡‡ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶ä»¥å¢å¼ºä¿¡æ¯ä¼ é€’çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„è‡ªæ³¨æ„åŠ›é€šä¿¡æ¨¡å—åœ¨SMACå’ŒSMACv2åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†å¤šä¸ªåœ°å›¾çš„æœ€å…ˆè¿›æ€§èƒ½ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•ï¼Œæ€§èƒ½æå‡æ˜¾è‘—ï¼Œå…·ä½“æå‡å¹…åº¦æœªçŸ¥ï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦åä½œå’Œé€šä¿¡çš„å¤æ‚ä»»åŠ¡ä¸­ï¼Œå¦‚æ— äººæœºç¼–é˜Ÿã€æ™ºèƒ½äº¤é€šç³»ç»Ÿå’Œæœºå™¨äººå›¢é˜Ÿåˆä½œç­‰ã€‚é€šè¿‡æå‡æ™ºèƒ½ä½“ä¹‹é—´çš„é€šä¿¡æ•ˆç‡ï¼Œå¯ä»¥æ˜¾è‘—æé«˜ç³»ç»Ÿçš„æ•´ä½“æ€§èƒ½å’Œä»»åŠ¡å®Œæˆç‡ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨ç›¸å…³é¢†åŸŸçš„æŠ€æœ¯è¿›æ­¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Communication is essential for the collective execution of complex tasks by human agents, motivating interest in communication mechanisms for multi-agent reinforcement learning (MARL). However, existing communication protocols in MARL are often complex and non-differentiable. In this work, we introduce a self-attention-based communication module that exchanges information between the agents in MARL. Our proposed approach is fully differentiable, allowing agents to learn to generate messages in a reward-driven manner. The module can be seamlessly integrated with any action-value function decomposition method and can be viewed as an extension of such decompositions. Notably, it includes a fixed number of trainable parameters, independent of the number of agents. Experimental results on the SMAC and SMACv2 benchmarks demonstrate the effectiveness of our approach, which achieves state-of-the-art performance on a number of maps.

