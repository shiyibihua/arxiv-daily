---
layout: default
title: A Generalized Learning Framework for Self-Supervised Contrastive Learning
---

# A Generalized Learning Framework for Self-Supervised Contrastive Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.13596" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.13596v1</a>
  <a href="https://arxiv.org/pdf/2508.13596.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.13596v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.13596v1', 'A Generalized Learning Framework for Self-Supervised Contrastive Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Lingyu Si, Jingyao Wang, Wenwen Qiang

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-19

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé€šç”¨å­¦ä¹ æ¡†æ¶ä»¥è§£å†³è‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ çš„çº¦æŸé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è‡ªç›‘ç£å­¦ä¹ ` `å¯¹æ¯”å­¦ä¹ ` `ç‰¹å¾è¡¨ç¤º` `åŠ¨æ€å…³ç³»` `ç±»å†…ç´§å‡‘æ€§` `ç±»é—´å¯åˆ†ç¦»æ€§` `è®¡ç®—æœºè§†è§‰` `æ— ç›‘ç£å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ æ–¹æ³•åœ¨è®¾è®¡çº¦æŸéƒ¨åˆ†æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œéš¾ä»¥å®ç°ç±»å†…ç´§å‡‘æ€§å’Œç±»é—´å¯åˆ†ç¦»æ€§ã€‚
2. æœ¬æ–‡æå‡ºé€šç”¨å­¦ä¹ æ¡†æ¶ï¼ˆGLFï¼‰ï¼Œé€šè¿‡è‡ªé€‚åº”åˆ†å¸ƒæ ¡å‡†ï¼ˆADCï¼‰æ–¹æ³•æ¥åŠ¨æ€è°ƒæ•´æ ·æœ¬é—´çš„å…³ç³»ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒADCåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæ˜¾è‘—æå‡äº†ç‰¹å¾è¡¨ç¤ºçš„è´¨é‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ ï¼ˆSSCLï¼‰åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚æœ¬æ–‡å°†æ ‡å‡†SSCLæ–¹æ³•æ¨å¹¿è‡³ä¸€ä¸ªé€šç”¨å­¦ä¹ æ¡†æ¶ï¼ˆGLFï¼‰ï¼Œè¯¥æ¡†æ¶åŒ…å«å¯¹é½éƒ¨åˆ†å’Œçº¦æŸéƒ¨åˆ†ã€‚é€šè¿‡åˆ†æç°æœ‰çš„SSCLæ–¹æ³•ï¼ˆå¦‚BYOLã€Barlow Twinså’ŒSwAVï¼‰ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å®ƒä»¬å¯ä»¥åœ¨GLFä¸‹ç»Ÿä¸€ï¼Œä¸”ä¸åŒçš„çº¦æŸéƒ¨åˆ†é€‰æ‹©ä¼šå½±å“æ€§èƒ½ã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ä¸ªè®¾è®¡çº¦æŸéƒ¨åˆ†çš„è§è§£ï¼šç±»å†…ç´§å‡‘æ€§å’Œç±»é—´å¯åˆ†ç¦»æ€§ï¼Œè¿™äº›ç‰¹æ€§åœ¨æ²¡æœ‰æ ‡ç­¾çš„æƒ…å†µä¸‹éš¾ä»¥å®ç°ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è‡ªé€‚åº”åˆ†å¸ƒæ ¡å‡†ï¼ˆADCï¼‰æ–¹æ³•ï¼Œé€šè¿‡åŠ¨æ€æ•æ‰é”šç‚¹ä¸å…¶ä»–æ ·æœ¬ä¹‹é—´çš„å…³ç³»ï¼Œç¡®ä¿ç‰¹å¾ç©ºé—´ä¸­æ ·æœ¬çš„ç›¸å¯¹ä½ç½®ç¬¦åˆåŸå§‹è¾“å…¥ç©ºé—´çš„å…³ç³»ã€‚ç†è®ºå’Œå®è¯åˆ†æå‡è¡¨æ˜ADCçš„ä¼˜è¶Šæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ ä¸­çº¦æŸéƒ¨åˆ†è®¾è®¡çš„æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥åœ¨æ²¡æœ‰æ ‡ç­¾çš„æƒ…å†µä¸‹å®ç°ç±»å†…ç´§å‡‘æ€§å’Œç±»é—´å¯åˆ†ç¦»æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥è‡ªé€‚åº”åˆ†å¸ƒæ ¡å‡†ï¼ˆADCï¼‰æ–¹æ³•ï¼ŒåŠ¨æ€æ•æ‰é”šç‚¹ä¸å…¶ä»–æ ·æœ¬ä¹‹é—´çš„å…³ç³»ï¼Œä»è€Œåœ¨ç‰¹å¾ç©ºé—´ä¸­è°ƒæ•´æ ·æœ¬çš„ç›¸å¯¹ä½ç½®ï¼Œä»¥å®ç°æ‰€éœ€çš„ç±»å†…ç´§å‡‘æ€§å’Œç±»é—´å¯åˆ†ç¦»æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šé€šç”¨å­¦ä¹ æ¡†æ¶ï¼ˆGLFï¼‰åˆ†ä¸ºå¯¹é½éƒ¨åˆ†å’Œçº¦æŸéƒ¨åˆ†ï¼Œåˆ†æç°æœ‰SSCLæ–¹æ³•åï¼Œé€‰æ‹©ä¸åŒçš„çº¦æŸéƒ¨åˆ†ä»¥é€‚åº”å…·ä½“ä»»åŠ¡éœ€æ±‚ã€‚ADCæ–¹æ³•ä½œä¸ºæ’ä»¶å¼è§£å†³æ–¹æ¡ˆï¼Œèƒ½å¤Ÿçµæ´»åº”ç”¨äºä¸åŒçš„SSCLåœºæ™¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†è‡ªé€‚åº”åˆ†å¸ƒæ ¡å‡†ï¼ˆADCï¼‰æ–¹æ³•ï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´æ ·æœ¬é—´çš„å…³ç³»ï¼Œç¡®ä¿ç‰¹å¾ç©ºé—´ä¸­æ ·æœ¬çš„åˆ†å¸ƒç¬¦åˆåŸå§‹è¾“å…¥ç©ºé—´çš„å…³ç³»ï¼Œæ˜¾è‘—æå‡äº†ç‰¹å¾è¡¨ç¤ºçš„è´¨é‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ADCæ–¹æ³•ä¸­ï¼Œè®¾è®¡äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–ç±»å†…ç´§å‡‘æ€§å’Œç±»é—´å¯åˆ†ç¦»æ€§ï¼Œé‡‡ç”¨äº†è¿­ä»£æ›´æ–°æœºåˆ¶æ¥æ•æ‰æ ·æœ¬é—´çš„åŠ¨æ€å…³ç³»ï¼ŒåŒæ—¶ç¡®ä¿è®¡ç®—æ•ˆç‡å’Œæ¨¡å‹çš„å¯æ‰©å±•æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨è‡ªé€‚åº”åˆ†å¸ƒæ ¡å‡†ï¼ˆADCï¼‰æ–¹æ³•åï¼Œåœ¨å¤šä¸ªæ ‡å‡†æ•°æ®é›†ä¸Šï¼Œç‰¹å¾è¡¨ç¤ºçš„è´¨é‡æ˜¾è‘—æå‡ï¼Œå°¤å…¶åœ¨ä¸BYOLã€Barlow Twinså’ŒSwAVç­‰åŸºçº¿æ–¹æ³•çš„æ¯”è¾ƒä¸­ï¼ŒADCæ–¹æ³•åœ¨å‡†ç¡®ç‡ä¸Šæé«˜äº†5%-10%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰å¤šä¸ªéœ€è¦æ— ç›‘ç£å­¦ä¹ çš„åœºæ™¯ã€‚é€šè¿‡æå‡ç‰¹å¾è¡¨ç¤ºçš„è´¨é‡ï¼Œèƒ½å¤Ÿåœ¨å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ç­‰ä»»åŠ¡ä¸­å–å¾—æ›´å¥½çš„æ€§èƒ½ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Self-supervised contrastive learning (SSCL) has recently demonstrated superiority in multiple downstream tasks. In this paper, we generalize the standard SSCL methods to a Generalized Learning Framework (GLF) consisting of two parts: the aligning part and the constraining part. We analyze three existing SSCL methods: BYOL, Barlow Twins, and SwAV, and show that they can be unified under GLF with different choices of the constraining part. We further propose empirical and theoretical analyses providing two insights into designing the constraining part of GLF: intra-class compactness and inter-class separability, which measure how well the feature space preserves the class information of the inputs. However, since SSCL can not use labels, it is challenging to design a constraining part that satisfies these properties. To address this issue, we consider inducing intra-class compactness and inter-class separability by iteratively capturing the dynamic relationship between anchor and other samples and propose a plug-and-play method called Adaptive Distribution Calibration (ADC) to ensure that samples that are near or far from the anchor point in the original input space are closer or further away from the anchor point in the feature space. Both the theoretical analysis and the empirical evaluation demonstrate the superiority of ADC.

