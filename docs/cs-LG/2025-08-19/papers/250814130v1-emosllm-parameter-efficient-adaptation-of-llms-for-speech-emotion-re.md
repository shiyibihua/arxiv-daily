---
layout: default
title: EmoSLLM: Parameter-Efficient Adaptation of LLMs for Speech Emotion Recognition
---

# EmoSLLM: Parameter-Efficient Adaptation of LLMs for Speech Emotion Recognition

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.14130" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.14130v1</a>
  <a href="https://arxiv.org/pdf/2508.14130.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.14130v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.14130v1', 'EmoSLLM: Parameter-Efficient Adaptation of LLMs for Speech Emotion Recognition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hugo Thimonier, Antony Perzo, Renaud Seguier

**åˆ†ç±»**: eess.AS, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-19

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEmoSLLMä»¥é«˜æ•ˆè§£å†³è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«` `å¤šæ¨¡æ€èåˆ` `å¤§å‹è¯­è¨€æ¨¡å‹` `ä½ç§©é€‚åº”` `äººæœºäº¤äº’` `å¿ƒç†å¥åº·ç›‘æµ‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«æ–¹æ³•éš¾ä»¥æœ‰æ•ˆæ•´åˆè¯­è¨€å’Œå‰¯è¯­è¨€ä¿¡æ¯ï¼Œå¯¼è‡´è¯†åˆ«å‡†ç¡®ç‡ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡éŸ³é¢‘å’Œæ–‡æœ¬è¡¨ç¤ºå¾®è°ƒLLMçš„æ–°æ–¹æ³•ï¼Œåˆ©ç”¨ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰å®ç°å‚æ•°é«˜æ•ˆçš„å¾®è°ƒã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨æ ‡å‡†åŸºå‡†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šå¤§å¤šæ•°ç°æœ‰æ–¹æ³•ï¼Œä¸”å‚æ•°é‡æ˜¾è‘—å‡å°‘ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«æ˜¯ä¸€é¡¹å¤æ‚çš„ä»»åŠ¡ï¼Œéœ€è¦æ•æ‰è¯­è¨€å’Œå‰¯è¯­è¨€çº¿ç´¢ï¼Œå¹¿æ³›åº”ç”¨äºäººæœºäº¤äº’å’Œå¿ƒç†å¥åº·ç›‘æµ‹ã€‚è¿‘æœŸç ”ç©¶è¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€ä»¥å¤–çš„ä»»åŠ¡ä¸­ä¹Ÿè¡¨ç°å‡ºè‰²ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡éŸ³é¢‘å’Œæ–‡æœ¬è¡¨ç¤ºå¾®è°ƒLLMä»¥è¿›è¡Œæƒ…æ„Ÿé¢„æµ‹ã€‚æˆ‘ä»¬é¦–å…ˆä½¿ç”¨éŸ³é¢‘ç‰¹å¾æå–å™¨æå–éŸ³é¢‘ç‰¹å¾ï¼Œç„¶åé€šè¿‡å¯å­¦ä¹ çš„æ¥å£æ¨¡å—å°†å…¶æ˜ å°„åˆ°LLMçš„è¡¨ç¤ºç©ºé—´ã€‚LLMçš„è¾“å…¥åŒ…æ‹¬è½¬æ¢åçš„éŸ³é¢‘ç‰¹å¾ã€è‡ªç„¶è¯­è¨€å½¢å¼çš„é™„åŠ ç‰¹å¾ï¼ˆå¦‚è½¬å½•æ–‡æœ¬ï¼‰ä»¥åŠæè¿°æƒ…æ„Ÿé¢„æµ‹ä»»åŠ¡çš„æ–‡æœ¬æç¤ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨æ ‡å‡†æƒ…æ„Ÿè¯†åˆ«åŸºå‡†ä¸Šè¶…è¶Šäº†æ–‡çŒ®ä¸­æ‰€æœ‰ä½†ä¸€ä¸ªç°æœ‰çš„è¯­éŸ³-æ–‡æœ¬LLMï¼ŒåŒæ—¶æ‰€éœ€å‚æ•°ä¸åˆ°ç«äº‰æ–¹æ³•çš„ä¸€åŠï¼Œæ˜¾ç¤ºäº†æˆ‘ä»¬æ–¹æ³•åœ¨å¤šæ¨¡æ€è¾“å…¥æ•´åˆä¸­çš„æœ‰æ•ˆæ€§å’Œè®¡ç®—æ•ˆç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ä¸­çš„å¤šæ¨¡æ€ä¿¡æ¯æ•´åˆé—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨æ•æ‰è¯­è¨€ä¸å‰¯è¯­è¨€çº¿ç´¢æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå¯¼è‡´æƒ…æ„Ÿè¯†åˆ«æ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºé€šè¿‡éŸ³é¢‘ç‰¹å¾å’Œæ–‡æœ¬ä¿¡æ¯å…±åŒå¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œåˆ©ç”¨å¯å­¦ä¹ çš„æ¥å£æ¨¡å—å°†éŸ³é¢‘ç‰¹å¾æ˜ å°„åˆ°LLMçš„è¡¨ç¤ºç©ºé—´ï¼Œä»¥å®ç°æ›´å‡†ç¡®çš„æƒ…æ„Ÿé¢„æµ‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬éŸ³é¢‘ç‰¹å¾æå–æ¨¡å—ã€å¯å­¦ä¹ æ¥å£æ¨¡å—å’ŒLLMã€‚éŸ³é¢‘ç‰¹å¾æå–æ¨¡å—è´Ÿè´£æå–éŸ³é¢‘ä¿¡å·ä¸­çš„ç‰¹å¾ï¼Œæ¥å£æ¨¡å—å°†è¿™äº›ç‰¹å¾è½¬æ¢ä¸ºLLMå¯æ¥å—çš„æ ¼å¼ï¼Œæœ€åLLMç»“åˆæ–‡æœ¬è¾“å…¥è¿›è¡Œæƒ…æ„Ÿé¢„æµ‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„ä¸»è¦åˆ›æ–°åœ¨äºç»“åˆéŸ³é¢‘å’Œæ–‡æœ¬ä¿¡æ¯ï¼Œé€šè¿‡ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰å®ç°é«˜æ•ˆçš„å‚æ•°å¾®è°ƒï¼Œæ˜¾è‘—é™ä½äº†æ¨¡å‹çš„å‚æ•°éœ€æ±‚ï¼ŒåŒæ—¶æå‡äº†æƒ…æ„Ÿè¯†åˆ«çš„å‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä½ç§©é€‚åº”æŠ€æœ¯ï¼Œä¼˜åŒ–äº†å‚æ•°è®¾ç½®ï¼Œç¡®ä¿åœ¨ä¿æŒæ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œå‡å°‘è®¡ç®—èµ„æºçš„æ¶ˆè€—ã€‚æŸå¤±å‡½æ•°è®¾è®¡ä¸Šï¼Œæˆ‘ä»¬ç»“åˆäº†å¤šæ¨¡æ€è¾“å…¥çš„ç‰¹æ€§ï¼Œä»¥æé«˜æ¨¡å‹çš„å­¦ä¹ æ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒEmoSLLMåœ¨æ ‡å‡†æƒ…æ„Ÿè¯†åˆ«åŸºå‡†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†æ–‡çŒ®ä¸­æ‰€æœ‰ä½†ä¸€ä¸ªç°æœ‰çš„è¯­éŸ³-æ–‡æœ¬LLMï¼Œä¸”æ‰€éœ€å‚æ•°é‡ä¸åˆ°ç«äº‰æ–¹æ³•çš„ä¸€åŠï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å¤šæ¨¡æ€è¾“å…¥æ•´åˆä¸­çš„é«˜æ•ˆæ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨æƒ…æ„Ÿè¯†åˆ«é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨æ™ºèƒ½å®¢æœã€å¿ƒç†å¥åº·ç›‘æµ‹å’Œäººæœºäº¤äº’ç­‰åœºæ™¯ä¸­ã€‚é€šè¿‡å‡†ç¡®è¯†åˆ«ç”¨æˆ·æƒ…æ„Ÿï¼Œç³»ç»Ÿèƒ½å¤Ÿæä¾›æ›´ä¸ºä¸ªæ€§åŒ–çš„æœåŠ¡ï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•è¿˜å¯èƒ½æ‰©å±•åˆ°å…¶ä»–å¤šæ¨¡æ€ä»»åŠ¡ï¼Œå¦‚è§†é¢‘åˆ†æå’Œç¤¾äº¤åª’ä½“æƒ…æ„Ÿåˆ†æç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Emotion recognition from speech is a challenging task that requires capturing both linguistic and paralinguistic cues, with critical applications in human-computer interaction and mental health monitoring. Recent works have highlighted the ability of Large Language Models (LLMs) to perform tasks outside of the sole natural language area. In particular, recent approaches have investigated coupling LLMs with other data modalities by using pre-trained backbones and different fusion mechanisms. This work proposes a novel approach that fine-tunes an LLM with audio and text representations for emotion prediction. Our method first extracts audio features using an audio feature extractor, which are then mapped into the LLM's representation space via a learnable interfacing module. The LLM takes as input (1) the transformed audio features, (2) additional features in the form of natural language (e.g., the transcript), and (3) a textual prompt describing the emotion prediction task. To efficiently adapt the LLM to this multimodal task, we employ Low-Rank Adaptation (LoRA), enabling parameter-efficient fine-tuning. Experimental results on standard emotion recognition benchmarks demonstrate that our model outperforms all but one existing Speech-Text LLMs in the literature, while requiring less than half the parameters of competing approaches. This highlights our approach's effectiveness in integrating multi-modal inputs for speech-based emotion understanding while maintaining significant computational efficiency.

