---
layout: default
title: Revisiting Diffusion Q-Learning: From Iterative Denoising to One-Step Action Generation
---

# Revisiting Diffusion Q-Learning: From Iterative Denoising to One-Step Action Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.13904" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.13904v2</a>
  <a href="https://arxiv.org/pdf/2508.13904.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.13904v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.13904v2', 'Revisiting Diffusion Q-Learning: From Iterative Denoising to One-Step Action Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Thanh Nguyen, Chang D. Yoo

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-19 (æ›´æ–°: 2025-10-01)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOne-Step Flow Q-Learningä»¥è§£å†³DQLè®­ç»ƒä¸æ¨ç†æ•ˆç‡ä½ä¸‹é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ‰©æ•£Qå­¦ä¹ ` `å¼ºåŒ–å­¦ä¹ ` `åŠ¨ä½œç”Ÿæˆ` `æµåŒ¹é…` `é€Ÿåº¦åœº` `ç¦»çº¿å­¦ä¹ ` `æœºå™¨äººæ§åˆ¶` `è‡ªåŠ¨é©¾é©¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ‰©æ•£Qå­¦ä¹ æ–¹æ³•ä¾èµ–å¤šæ­¥å»å™ªï¼Œå¯¼è‡´è®­ç»ƒå’Œæ¨ç†æ•ˆç‡ä½ä¸‹ï¼Œä¸”è¿‡ç¨‹è„†å¼±ã€‚
2. æœ¬æ–‡æå‡ºçš„One-Step Flow Q-Learningæ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨æ— éœ€è¾…åŠ©æ¨¡å—çš„æƒ…å†µä¸‹å®ç°æœ‰æ•ˆçš„ä¸€æ­¥åŠ¨ä½œç”Ÿæˆã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒOFQLåœ¨D4RLåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œè¶…è¶Šäº†æ‰€æœ‰åŸºçº¿æ–¹æ³•ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ‰©æ•£Qå­¦ä¹ ï¼ˆDQLï¼‰å·²æˆä¸ºç¦»çº¿å¼ºåŒ–å­¦ä¹ çš„é«˜æ•ˆèŒƒå¼ï¼Œä½†å…¶ä¾èµ–äºå¤šæ­¥å»å™ªçš„åŠ¨ä½œç”Ÿæˆæ–¹å¼å¯¼è‡´è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ç¼“æ…¢ä¸”è„†å¼±ã€‚ç°æœ‰åŠ é€ŸDQLçš„æ–¹æ³•é€šå¸¸ä¾èµ–è¾…åŠ©æ¨¡å—æˆ–ç­–ç•¥è’¸é¦ï¼Œç‰ºç‰²äº†ç®€å•æ€§æˆ–æ€§èƒ½ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ¡†æ¶One-Step Flow Q-Learningï¼ˆOFQLï¼‰ï¼Œå®ç°äº†åœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­æœ‰æ•ˆçš„ä¸€æ­¥åŠ¨ä½œç”Ÿæˆï¼Œæ— éœ€è¾…åŠ©æ¨¡å—æˆ–è’¸é¦ã€‚OFQLåœ¨æµåŒ¹é…ï¼ˆFMï¼‰èŒƒå¼ä¸‹é‡æ–°æ„å»ºDQLç­–ç•¥ï¼Œé€šè¿‡å­¦ä¹ å¹³å‡é€Ÿåº¦åœºç›´æ¥æ”¯æŒå‡†ç¡®çš„ä¸€æ­¥åŠ¨ä½œç”Ÿæˆï¼Œæ˜¾è‘—æé«˜äº†å­¦ä¹ é€Ÿåº¦å’Œé²æ£’æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOFQLåœ¨D4RLåŸºå‡†æµ‹è¯•ä¸­ä¸ä»…æ˜¾è‘—å‡å°‘äº†è®­ç»ƒå’Œæ¨ç†çš„è®¡ç®—é‡ï¼Œè¿˜å¤§å¹…è¶…è¶Šäº†å¤šæ­¥DQLï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ‰©æ•£Qå­¦ä¹ ï¼ˆDQLï¼‰åœ¨åŠ¨ä½œç”Ÿæˆè¿‡ç¨‹ä¸­å¯¹å¤šæ­¥å»å™ªçš„ä¾èµ–ï¼Œè¿™å¯¼è‡´äº†è®­ç»ƒå’Œæ¨ç†çš„ä½æ•ˆä¸è„†å¼±æ€§ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦è¾…åŠ©æ¨¡å—æˆ–ç­–ç•¥è’¸é¦ï¼Œé€ æˆäº†ç®€å•æ€§ä¸æ€§èƒ½ä¹‹é—´çš„æƒè¡¡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºOne-Step Flow Q-Learningï¼ˆOFQLï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°é«˜æ•ˆçš„ä¸€æ­¥åŠ¨ä½œç”Ÿæˆã€‚é€šè¿‡åœ¨æµåŒ¹é…ï¼ˆFMï¼‰èŒƒå¼ä¸‹é‡æ–°æ„å»ºDQLç­–ç•¥ï¼ŒOFQLå­¦ä¹ ä¸€ä¸ªå¹³å‡é€Ÿåº¦åœºï¼Œç›´æ¥æ”¯æŒå‡†ç¡®çš„ä¸€æ­¥åŠ¨ä½œç”Ÿæˆï¼Œä»è€Œæ¶ˆé™¤äº†å¤šæ­¥å»å™ªçš„éœ€æ±‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šOFQLçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ç­–ç•¥å­¦ä¹ æ¨¡å—å’ŒåŠ¨ä½œç”Ÿæˆæ¨¡å—ã€‚ç­–ç•¥å­¦ä¹ æ¨¡å—è´Ÿè´£å­¦ä¹ å¹³å‡é€Ÿåº¦åœºï¼Œè€ŒåŠ¨ä½œç”Ÿæˆæ¨¡å—åˆ™åˆ©ç”¨è¯¥é€Ÿåº¦åœºè¿›è¡Œé«˜æ•ˆçš„ä¸€æ­¥åŠ¨ä½œç”Ÿæˆã€‚æ•´ä¸ªè¿‡ç¨‹ä¸éœ€è¦é¢å¤–çš„è¾…åŠ©æ¨¡å—æˆ–è’¸é¦æ­¥éª¤ã€‚

**å…³é”®åˆ›æ–°**ï¼šOFQLçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºé€šè¿‡å­¦ä¹ å¹³å‡é€Ÿåº¦åœºæ¥å®ç°ç›´æ¥çš„ä¸€æ­¥åŠ¨ä½œç”Ÿæˆï¼Œè¿™ä¸ä¼ ç»Ÿçš„å¤šæ­¥å»å™ªæ–¹æ³•æœ¬è´¨ä¸Šä¸åŒã€‚æ­¤è®¾è®¡æ˜¾è‘—æé«˜äº†å­¦ä¹ çš„é€Ÿåº¦å’Œé²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼ŒOFQLé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–é€Ÿåº¦åœºçš„å­¦ä¹ ï¼Œå¹¶è®¾è®¡äº†é«˜æ•ˆçš„ç½‘ç»œç»“æ„ä»¥æ”¯æŒå¿«é€Ÿçš„åŠ¨ä½œç”Ÿæˆã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨D4RLåŸºå‡†æµ‹è¯•ä¸­ï¼ŒOFQLæ˜¾è‘—å‡å°‘äº†è®­ç»ƒå’Œæ¨ç†çš„è®¡ç®—é‡ï¼Œä¸”åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†å¤šæ­¥DQLï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ã€‚å…·ä½“è€Œè¨€ï¼ŒOFQLåœ¨å¤šä¸ªä»»åŠ¡ä¸Šå‡è¡¨ç°å‡ºè‰²ï¼Œæå‡å¹…åº¦è¾¾åˆ°XX%ï¼ˆå…·ä½“æ•°æ®å¾…è¡¥å……ï¼‰ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ã€æ¸¸æˆæ™ºèƒ½ç­‰éœ€è¦é«˜æ•ˆå†³ç­–çš„åœºæ™¯ã€‚OFQLçš„é«˜æ•ˆæ€§å’Œé²æ£’æ€§ä½¿å…¶åœ¨å®é™…åº”ç”¨ä¸­å…·æœ‰è¾ƒå¤§çš„ä»·å€¼ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡æ™ºèƒ½ä½“åœ¨å¤æ‚ç¯å¢ƒä¸­çš„è¡¨ç°ã€‚æœªæ¥ï¼ŒOFQLå¯èƒ½ä¼šæ¨åŠ¨æ›´å¤šåŸºäºå¼ºåŒ–å­¦ä¹ çš„åº”ç”¨å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Diffusion Q-Learning (DQL) has established diffusion policies as a high-performing paradigm for offline reinforcement learning, but its reliance on multi-step denoising for action generation renders both training and inference slow and fragile. Existing efforts to accelerate DQL toward one-step denoising typically rely on auxiliary modules or policy distillation, sacrificing either simplicity or performance. It remains unclear whether a one-step policy can be trained directly without such trade-offs. To this end, we introduce One-Step Flow Q-Learning (OFQL), a novel framework that enables effective one-step action generation during both training and inference, without auxiliary modules or distillation. OFQL reformulates the DQL policy within the Flow Matching (FM) paradigm but departs from conventional FM by learning an average velocity field that directly supports accurate one-step action generation. This design removes the need for multi-step denoising and backpropagation-through-time updates, resulting in substantially faster and more robust learning. Extensive experiments on the D4RL benchmark show that OFQL, despite generating actions in a single step, not only significantly reduces computation during both training and inference but also outperforms multi-step DQL by a large margin. Furthermore, OFQL surpasses all other baselines, achieving state-of-the-art performance in D4RL.

