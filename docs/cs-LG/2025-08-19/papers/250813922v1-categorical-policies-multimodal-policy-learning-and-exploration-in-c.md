---
layout: default
title: Categorical Policies: Multimodal Policy Learning and Exploration in Continuous Control
---

# Categorical Policies: Multimodal Policy Learning and Exploration in Continuous Control

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.13922" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.13922v1</a>
  <a href="https://arxiv.org/pdf/2508.13922.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.13922v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.13922v1', 'Categorical Policies: Multimodal Policy Learning and Exploration in Continuous Control')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: SM Mazharul Islam, Manfred Huber

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-19

**å¤‡æ³¨**: 6 pages, 4 figures; Has been submitted and accepted at IEEE SMC, 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåˆ†ç±»ç­–ç•¥ä»¥è§£å†³è¿ç»­æ§åˆ¶ä¸­çš„å¤šæ¨¡æ€æ¢ç´¢é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ·±åº¦å¼ºåŒ–å­¦ä¹ ` `å¤šæ¨¡æ€ç­–ç•¥` `åˆ†ç±»åˆ†å¸ƒ` `è¿ç»­æ§åˆ¶` `æ¢ç´¢ç­–ç•¥` `æœºå™¨äººæ§åˆ¶` `è‡ªåŠ¨é©¾é©¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é«˜æ–¯ç­–ç•¥é™åˆ¶äº†å­¦ä¹ è¡Œä¸ºçš„å¤šæ ·æ€§ï¼Œéš¾ä»¥åº”å¯¹å¤æ‚çš„å†³ç­–ç¯å¢ƒã€‚
2. æœ¬æ–‡æå‡ºåˆ†ç±»ç­–ç•¥ï¼Œé€šè¿‡ä¸­é—´çš„åˆ†ç±»åˆ†å¸ƒå»ºæ¨¡å¤šæ¨¡æ€è¡Œä¸ºï¼Œå¢å¼ºæ¢ç´¢èƒ½åŠ›ã€‚
3. åœ¨DeepMindæ§åˆ¶å¥—ä»¶ç¯å¢ƒä¸­ï¼Œåˆ†ç±»ç­–ç•¥çš„å­¦ä¹ æ”¿ç­–æ”¶æ•›æ›´å¿«ï¼Œè¡¨ç°ä¼˜äºæ ‡å‡†é«˜æ–¯ç­–ç•¥ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œç°æœ‰çš„ç­–ç•¥é€šå¸¸ä»…é€šè¿‡é«˜æ–¯åˆ†å¸ƒæ¥å‚æ•°åŒ–ï¼Œé™åˆ¶äº†å­¦ä¹ è¡Œä¸ºçš„å•æ¨¡æ€ç‰¹æ€§ã€‚è®¸å¤šå®é™…å†³ç­–é—®é¢˜æ›´å€¾å‘äºå¤šæ¨¡æ€ç­–ç•¥ï¼Œä»¥ä¾¿åœ¨ç¨€ç–å¥–åŠ±ã€å¤æ‚åŠ¨æ€æˆ–éœ€è¦é€‚åº”ä¸åŒç¯å¢ƒçš„æƒ…å†µä¸‹è¿›è¡Œæ›´æœ‰æ•ˆçš„æ¢ç´¢ã€‚æœ¬æ–‡æå‡ºäº†åˆ†ç±»ç­–ç•¥ï¼Œé€šè¿‡ä¸­é—´çš„åˆ†ç±»åˆ†å¸ƒå»ºæ¨¡å¤šæ¨¡æ€è¡Œä¸ºï¼Œå¹¶åœ¨é‡‡æ ·æ¨¡å¼çš„åŸºç¡€ä¸Šç”Ÿæˆè¾“å‡ºåŠ¨ä½œã€‚æˆ‘ä»¬æ¢ç´¢äº†ä¸¤ç§é‡‡æ ·æ–¹æ¡ˆï¼Œç¡®ä¿äº†å¯å¾®åˆ†çš„ç¦»æ•£æ½œåœ¨ç»“æ„ï¼ŒåŒæ—¶ä¿æŒé«˜æ•ˆçš„åŸºäºæ¢¯åº¦çš„ä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåˆ†ç±»åˆ†å¸ƒåœ¨è¿ç»­æ§åˆ¶ä¸­ä¸ºç»“æ„åŒ–æ¢ç´¢å’Œå¤šæ¨¡æ€è¡Œä¸ºè¡¨ç¤ºæä¾›äº†å¼ºæœ‰åŠ›çš„å·¥å…·ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰æ·±åº¦å¼ºåŒ–å­¦ä¹ ç­–ç•¥å•æ¨¡æ€çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨è¿ç»­æ§åˆ¶ä»»åŠ¡ä¸­ï¼Œä¼ ç»Ÿçš„é«˜æ–¯åˆ†å¸ƒç­–ç•¥å¾€å¾€å¯¼è‡´æ¢ç´¢ä¸è¶³ï¼Œéš¾ä»¥åº”å¯¹å¤æ‚çš„ç¯å¢ƒåŠ¨æ€å’Œç¨€ç–å¥–åŠ±ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„åˆ†ç±»ç­–ç•¥ï¼Œé€šè¿‡å¼•å…¥ä¸­é—´çš„åˆ†ç±»åˆ†å¸ƒæ¥å»ºæ¨¡å¤šæ¨¡æ€è¡Œä¸ºï¼Œå…è®¸åœ¨æ¢ç´¢è¿‡ç¨‹ä¸­é€‰æ‹©ä¸åŒçš„è¡Œä¸ºæ¨¡å¼ï¼Œä»è€Œæé«˜ç­–ç•¥çš„çµæ´»æ€§å’Œé€‚åº”æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ˜¯æ½œåœ¨åˆ†ç±»åˆ†å¸ƒçš„ç”Ÿæˆæ¨¡å—ï¼Œè¯¥æ¨¡å—è´Ÿè´£æ ¹æ®ç¯å¢ƒçŠ¶æ€é€‰æ‹©è¡Œä¸ºæ¨¡å¼ï¼›å…¶æ¬¡æ˜¯åŸºäºæ‰€é€‰æ¨¡å¼ç”Ÿæˆå…·ä½“åŠ¨ä½œçš„è¾“å‡ºæ¨¡å—ã€‚æ•´ä¸ªè¿‡ç¨‹ä¿æŒå¯å¾®åˆ†æ€§ï¼Œä»¥ä¾¿äºæ¢¯åº¦ä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†åˆ†ç±»åˆ†å¸ƒæ¥è¡¨ç¤ºå¤šæ¨¡æ€è¡Œä¸ºï¼Œè¿™ä¸ä¼ ç»Ÿçš„å•ä¸€é«˜æ–¯åˆ†å¸ƒç­–ç•¥å½¢æˆäº†æœ¬è´¨åŒºåˆ«ï¼Œä½¿å¾—ç­–ç•¥èƒ½å¤Ÿåœ¨ä¸åŒçš„ç¯å¢ƒä¸Šä¸‹æ–‡ä¸­è¿›è¡Œæ›´æœ‰æ•ˆçš„æ¢ç´¢ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†å¯å¾®åˆ†çš„ç¦»æ•£æ½œåœ¨ç»“æ„ï¼Œç¡®ä¿äº†åœ¨é‡‡æ ·è¿‡ç¨‹ä¸­èƒ½å¤Ÿé«˜æ•ˆåœ°è¿›è¡Œæ¢¯åº¦ä¼˜åŒ–ã€‚æ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°çš„è®¾è®¡ä¹Ÿè€ƒè™‘äº†å¤šæ¨¡æ€è¡Œä¸ºçš„è¡¨è¾¾èƒ½åŠ›ï¼Œä»¥æé«˜ç­–ç•¥çš„å­¦ä¹ æ•ˆç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨åˆ†ç±»ç­–ç•¥çš„å­¦ä¹ æ”¿ç­–åœ¨DeepMindæ§åˆ¶å¥—ä»¶ä¸­æ”¶æ•›é€Ÿåº¦æ˜¾è‘—æå‡ï¼Œè¡¨ç°ä¼˜äºä¼ ç»Ÿçš„é«˜æ–¯ç­–ç•¥ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨å¤šæ¨¡æ€æ¢ç´¢ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ã€æ¸¸æˆAIç­‰éœ€è¦å¤æ‚å†³ç­–çš„åœºæ™¯ã€‚é€šè¿‡å¢å¼ºç­–ç•¥çš„å¤šæ¨¡æ€æ¢ç´¢èƒ½åŠ›ï¼Œèƒ½å¤Ÿæé«˜ç³»ç»Ÿåœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„é€‚åº”æ€§å’Œæ•ˆç‡ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> A policy in deep reinforcement learning (RL), either deterministic or stochastic, is commonly parameterized as a Gaussian distribution alone, limiting the learned behavior to be unimodal. However, the nature of many practical decision-making problems favors a multimodal policy that facilitates robust exploration of the environment and thus to address learning challenges arising from sparse rewards, complex dynamics, or the need for strategic adaptation to varying contexts. This issue is exacerbated in continuous control domains where exploration usually takes place in the vicinity of the predicted optimal action, either through an additive Gaussian noise or the sampling process of a stochastic policy. In this paper, we introduce Categorical Policies to model multimodal behavior modes with an intermediate categorical distribution, and then generate output action that is conditioned on the sampled mode. We explore two sampling schemes that ensure differentiable discrete latent structure while maintaining efficient gradient-based optimization. By utilizing a latent categorical distribution to select the behavior mode, our approach naturally expresses multimodality while remaining fully differentiable via the sampling tricks. We evaluate our multimodal policy on a set of DeepMind Control Suite environments, demonstrating that through better exploration, our learned policies converge faster and outperform standard Gaussian policies. Our results indicate that the Categorical distribution serves as a powerful tool for structured exploration and multimodal behavior representation in continuous control.

