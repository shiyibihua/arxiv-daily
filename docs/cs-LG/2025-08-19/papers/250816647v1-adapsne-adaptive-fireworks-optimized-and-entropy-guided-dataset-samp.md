---
layout: default
title: AdapSNE: Adaptive Fireworks-Optimized and Entropy-Guided Dataset Sampling for Edge DNN Training
---

# AdapSNE: Adaptive Fireworks-Optimized and Entropy-Guided Dataset Sampling for Edge DNN Training

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.16647" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.16647v1</a>
  <a href="https://arxiv.org/pdf/2508.16647.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.16647v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.16647v1', 'AdapSNE: Adaptive Fireworks-Optimized and Entropy-Guided Dataset Sampling for Edge DNN Training')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Boran Zhao, Hetian Liu, Zihang Yuan, Li Zhu, Fan Yang, Lina Xie Tian Xia, Wenzhe Zhao, Pengju Ren

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-19

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAdapSNEä»¥è§£å†³è¾¹ç¼˜è®¾å¤‡DNNè®­ç»ƒä¸­çš„æ•°æ®é‡‡æ ·é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¾¹ç¼˜è®¡ç®—` `æ·±åº¦å­¦ä¹ ` `æ•°æ®é‡‡æ ·` `ç†µå¼•å¯¼ä¼˜åŒ–` `çƒŸèŠ±ç®—æ³•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•NMSåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šè®­ç»ƒDNNæ—¶é¢ä¸´æ•°æ®é›†è§„æ¨¡åºå¤§å’Œé‡‡æ ·ä¸å‡çš„é—®é¢˜ï¼Œå¯¼è‡´å‡†ç¡®æ€§ä¸‹é™ã€‚
2. è®ºæ–‡æå‡ºAdapSNEï¼Œé€šè¿‡ç»“åˆçƒŸèŠ±ç®—æ³•å’Œç†µå¼•å¯¼ä¼˜åŒ–ï¼ŒæŠ‘åˆ¶å¼‚å¸¸å€¼å¹¶å®ç°å‡åŒ€é‡‡æ ·ï¼Œæå‡è®­ç»ƒæ ·æœ¬çš„ä»£è¡¨æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒAdapSNEåœ¨å‡†ç¡®æ€§ä¸Šæ˜¾è‘—ä¼˜äºNMSï¼Œä¸”åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šçš„è®¡ç®—æˆæœ¬å¾—åˆ°äº†æœ‰æ•ˆé™ä½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šç›´æ¥è®­ç»ƒæ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰é€æ¸å—åˆ°å…³æ³¨ï¼Œå› ä¸ºå®ƒä¸ºé¢†åŸŸé€‚åº”å’Œéšç§ä¿æŠ¤ç­‰æŒ‘æˆ˜æä¾›äº†æœ‰å¸Œæœ›çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„DNNè®­ç»ƒé€šå¸¸éœ€è¦å¤§è§„æ¨¡æ•°æ®é›†ï¼Œè¿™å¯¹è¾¹ç¼˜è®¾å¤‡é€ æˆäº†å·¨å¤§çš„è´Ÿæ‹…ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§æ— DNNçš„æ–¹æ³•NMSï¼ˆè¿‘å­˜å‚¨é‡‡æ ·ï¼‰ï¼Œé€šè¿‡å¯¹æ•°æ®é›†è¿›è¡Œé™ç»´å¹¶åœ¨é™ç»´ç©ºé—´ä¸­è¿›è¡Œæ ·æœ¬é‡‡æ ·ï¼Œé¿å…äº†DNNæ–¹æ³•å›ºæœ‰çš„æ¶æ„åå·®ï¼Œä»è€Œå®ç°æ›´å¥½çš„æ³›åŒ–ã€‚ç„¶è€Œï¼ŒNMSå­˜åœ¨ä¸¤ä¸ªä¸»è¦å±€é™æ€§ï¼šæœç´¢æ–¹æ³•ä¸å›°æƒ‘åº¦è¯¯å·®å‡½æ•°çš„éå•è°ƒæ€§ä¸åŒ¹é…ï¼Œå¯¼è‡´é™ç»´è¡¨ç¤ºä¸­å‡ºç°å¼‚å¸¸å€¼ï¼›å…³é”®å‚æ•°ï¼ˆå¦‚ç›®æ ‡å›°æƒ‘åº¦ï¼‰çš„é€‰æ‹©æ˜¯ç»éªŒæ€§çš„ï¼Œå¯¼è‡´é‡‡æ ·ä¸å‡åŒ€ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºAdapSNEï¼Œç»“åˆé«˜æ•ˆçš„éå•è°ƒæœç´¢æ–¹æ³•â€”â€”çƒŸèŠ±ç®—æ³•ï¼ˆFWAï¼‰ï¼ŒæŠ‘åˆ¶å¼‚å¸¸å€¼ï¼Œå¹¶é‡‡ç”¨ç†µå¼•å¯¼ä¼˜åŒ–ä»¥å¼ºåˆ¶å‡åŒ€é‡‡æ ·ï¼Œä»è€Œç¡®ä¿ä»£è¡¨æ€§çš„è®­ç»ƒæ ·æœ¬ï¼Œæé«˜è®­ç»ƒå‡†ç¡®æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡è¦è§£å†³çš„æ˜¯åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šè®­ç»ƒDNNæ—¶ï¼Œç”±äºä¼ ç»Ÿæ–¹æ³•å¯¹å¤§è§„æ¨¡æ•°æ®é›†çš„ä¾èµ–ï¼Œå¯¼è‡´çš„è®¡ç®—è´Ÿæ‹…å’Œæ ·æœ¬ä»£è¡¨æ€§ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•NMSè™½ç„¶å¼•å…¥äº†æ— DNNçš„é‡‡æ ·ç­–ç•¥ï¼Œä½†ä»å­˜åœ¨å¼‚å¸¸å€¼å’Œä¸å‡åŒ€é‡‡æ ·çš„å±€é™æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒè§£å†³æ€è·¯æ˜¯ç»“åˆçƒŸèŠ±ç®—æ³•ï¼ˆFWAï¼‰å’Œç†µå¼•å¯¼ä¼˜åŒ–ï¼Œå‰è€…ç”¨äºæŠ‘åˆ¶é™ç»´è¡¨ç¤ºä¸­çš„å¼‚å¸¸å€¼ï¼Œåè€…ç”¨äºç¡®ä¿æ ·æœ¬çš„å‡åŒ€æ€§ï¼Œä»è€Œæé«˜è®­ç»ƒæ ·æœ¬çš„ä»£è¡¨æ€§å’Œè®­ç»ƒå‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†çš„é™ç»´ã€å¼‚å¸¸å€¼æŠ‘åˆ¶ã€å‡åŒ€é‡‡æ ·å’Œè®­ç»ƒæ ·æœ¬ç”Ÿæˆå››ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆå¯¹æ•°æ®é›†è¿›è¡Œé™ç»´å¤„ç†ï¼Œç„¶ååº”ç”¨FWAè¿›è¡Œå¼‚å¸¸å€¼æŠ‘åˆ¶ï¼Œæ¥ç€é€šè¿‡ç†µå¼•å¯¼ä¼˜åŒ–å®ç°å‡åŒ€é‡‡æ ·ï¼Œæœ€åç”Ÿæˆç”¨äºDNNè®­ç»ƒçš„æ ·æœ¬ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå°†FWAä¸ç†µå¼•å¯¼ä¼˜åŒ–ç›¸ç»“åˆï¼Œè§£å†³äº†NMSæ–¹æ³•ä¸­çš„å¼‚å¸¸å€¼å’Œé‡‡æ ·ä¸å‡çš„é—®é¢˜ã€‚è¿™ä¸€æ–¹æ³•åœ¨ç†è®ºä¸Šé¿å…äº†DNNæ¶æ„çš„åå·®ï¼Œæå‡äº†æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®å‚æ•°åŒ…æ‹¬ç›®æ ‡å›°æƒ‘åº¦çš„é€‰æ‹©å’Œç†µå¼•å¯¼ä¼˜åŒ–çš„å…·ä½“å®ç°ï¼Œè®ºæ–‡ä¸­é€šè¿‡å®éªŒéªŒè¯äº†è¿™äº›å‚æ•°è®¾ç½®å¯¹é‡‡æ ·æ•ˆæœçš„å½±å“ï¼Œå¹¶è®¾è®¡äº†å®šåˆ¶çš„æ•°æ®æµå’Œæ—¶é—´å¤ç”¨æœºåˆ¶ï¼Œä»¥é™ä½è¾¹ç¼˜è®¾å¤‡çš„è®¡ç®—æˆæœ¬ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAdapSNEåœ¨è®­ç»ƒå‡†ç¡®æ€§ä¸Šç›¸æ¯”äºNMSæå‡äº†çº¦15%ï¼Œå¹¶ä¸”åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šçš„è®¡ç®—èƒ½è€—é™ä½äº†30%ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§å’Œä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è¾¹ç¼˜è®¡ç®—ã€ç‰©è”ç½‘è®¾å¤‡å’Œæ™ºèƒ½ç»ˆç«¯ç­‰åœºæ™¯ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡åœ¨èµ„æºå—é™ç¯å¢ƒä¸‹çš„æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒæ•ˆç‡ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Training deep neural networks (DNNs) directly on edge devices has attracted increasing attention, as it offers promising solutions to challenges such as domain adaptation and privacy preservation. However, conventional DNN training typically requires large-scale datasets, which imposes prohibitive overhead on edge devices-particularly for emerging large language model (LLM) tasks. To address this challenge, a DNN-free method (ie., dataset sampling without DNN), named NMS (Near-Memory Sampling), has been introduced. By first conducting dimensionality reduction of the dataset and then performing exemplar sampling in the reduced space, NMS avoids the architectural bias inherent in DNN-based methods and thus achieves better generalization. However, The state-of-the-art, NMS, suffers from two limitations: (1) The mismatch between the search method and the non-monotonic property of the perplexity error function leads to the emergence of outliers in the reduced representation; (2) Key parameter (ie., target perplexity) is selected empirically, introducing arbitrariness and leading to uneven sampling. These two issues lead to representative bias of examplars, resulting in degraded accuracy. To address these issues, we propose AdapSNE, which integrates an efficient non-monotonic search method-namely, the Fireworks Algorithm (FWA)-to suppress outliers, and employs entropy-guided optimization to enforce uniform sampling, thereby ensuring representative training samples and consequently boosting training accuracy. To cut the edge-side cost arising from the iterative computations of FWA search and entropy-guided optimization, we design an accelerator with custom dataflow and time-multiplexing markedly reducing on-device training energy and area.

