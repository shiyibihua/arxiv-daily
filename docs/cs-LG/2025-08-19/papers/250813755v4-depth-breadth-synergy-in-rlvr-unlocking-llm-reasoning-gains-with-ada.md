---
layout: default
title: Depth-Breadth Synergy in RLVR: Unlocking LLM Reasoning Gains with Adaptive Exploration
---

# Depth-Breadth Synergy in RLVR: Unlocking LLM Reasoning Gains with Adaptive Exploration

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.13755" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.13755v4</a>
  <a href="https://arxiv.org/pdf/2508.13755.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.13755v4" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.13755v4', 'Depth-Breadth Synergy in RLVR: Unlocking LLM Reasoning Gains with Adaptive Exploration')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Zhicheng Yang, Zhijiang Guo, Yinya Huang, Yongxin Wang, Dongchun Xie, Yiwei Wang, Xiaodan Liang, Jing Tang

**ÂàÜÁ±ª**: cs.LG, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-19 (Êõ¥Êñ∞: 2025-10-06)

**Â§áÊ≥®**: 18 pages, 14 figures

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫DARS‰ª•Ëß£ÂÜ≥RLVR‰∏≠ÁöÑÊ∑±Â∫¶‰∏éÂπøÂ∫¶Êé¢Á¥¢ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Âº∫ÂåñÂ≠¶‰π†` `ÂèØÈ™åËØÅÂ•ñÂä±` `Âõ∞ÈöæËá™ÈÄÇÂ∫îÂõûÊªö` `Êé®ÁêÜËÉΩÂäõ` `Ê∑±Â∫¶Â≠¶‰π†` `Â§ßËßÑÊ®°ËÆ≠ÁªÉ` `Ê†∑Êú¨Âä†ÊùÉ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâRLVRÊñπÊ≥ïÂú®Ê∑±Â∫¶ÂíåÂπøÂ∫¶Êé¢Á¥¢‰∏äÂ≠òÂú®‰∏çË∂≥ÔºåÂ∞§ÂÖ∂ÊòØÂØπ‰ΩéÂáÜÁ°ÆÂ∫¶Ê†∑Êú¨ÁöÑÂøΩËßÜÔºåÂΩ±ÂìçÊé®ÁêÜËÉΩÂäõÁöÑÊèêÂçá„ÄÇ
2. ÊèêÂá∫Âõ∞ÈöæËá™ÈÄÇÂ∫îÂõûÊªöÈááÊ†∑ÔºàDARSÔºâÔºåÈÄöËøáÂ§öÈò∂ÊÆµÂõûÊªöÁ≠ñÁï•ÈáçÊñ∞Âä†ÊùÉÂõ∞ÈöæÈóÆÈ¢òÔºåÂ¢ûÂº∫Ê®°ÂûãÂØπÂ§çÊùÇÈóÆÈ¢òÁöÑÂ≠¶‰π†ËÉΩÂäõ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåDARSÂú®‰∏çÂ¢ûÂä†Êé®ÁêÜÊàêÊú¨ÁöÑÊÉÖÂÜµ‰∏ãÔºåÊòæËëóÊèêÂçá‰∫ÜPass@KÂíåPass@1ÁöÑÊÄßËÉΩÔºåÈ™åËØÅ‰∫ÜÂπøÂ∫¶‰∏éÊ∑±Â∫¶ÁöÑÂçèÂêå‰ΩúÁî®„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Âº∫ÂåñÂ≠¶‰π†‰∏éÂèØÈ™åËØÅÂ•ñÂä±ÔºàRLVRÔºâÂ∑≤Êàê‰∏∫ÈáäÊîæÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜËÉΩÂäõÁöÑÂº∫Â§ßËåÉÂºèÔºå‰ΩÜÂÖ∂ÊΩúÂäõÂèóÂà∞Ê∑±Â∫¶ÂíåÂπøÂ∫¶‰∏§‰∏™Êú™ÂÖÖÂàÜÊé¢Á¥¢Áª¥Â∫¶ÁöÑÈôêÂà∂„ÄÇÊú¨ÊñáÂàÜÊûê‰∫ÜGRPOÁÆóÊ≥ïÔºåÊè≠Á§∫‰∫ÜÂÖ∂Á≥ªÁªüÊÄßÂÅèÂ∑ÆÔºåÊèêÂá∫‰∫ÜÂõ∞ÈöæËá™ÈÄÇÂ∫îÂõûÊªöÈááÊ†∑ÔºàDARSÔºâÔºåÈÄöËøáÂ§öÈò∂ÊÆµÂõûÊªöÈáçÊñ∞Âä†ÊùÉÂõ∞ÈöæÈóÆÈ¢òÔºåÂ¢ûÂä†‰∫ÜÊ≠£ÂõûÊªöÁöÑÊï∞Èáè„ÄÇÊ≠§Â§ñÔºåÊâ©Â§ßËÆ≠ÁªÉÊï∞ÊçÆÁöÑÂπøÂ∫¶ÊòæËëóÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑÊé®ÁêÜÊÄßËÉΩ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåDARS‰∏éÂ§ßÂπøÂ∫¶ËÆ≠ÁªÉÁõ∏ÁªìÂêàÔºåËÉΩÂ§üÂêåÊó∂ÊèêÂçáPass@KÂíåPass@1ÁöÑË°®Áé∞ÔºåËØÅÊòé‰∫ÜÊ∑±Â∫¶‰∏éÂπøÂ∫¶Âú®RLVR‰∏≠ÁöÑÊ≠£‰∫§‰ΩúÁî®„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥RLVR‰∏≠Ê∑±Â∫¶‰∏éÂπøÂ∫¶Êé¢Á¥¢‰∏çË∂≥ÁöÑÈóÆÈ¢òÔºåÁé∞ÊúâÊñπÊ≥ïÂ¶ÇGRPOÂØπ‰ΩéÂáÜÁ°ÆÂ∫¶Ê†∑Êú¨ÁöÑÂøΩËßÜÂØºËá¥Êé®ÁêÜËÉΩÂäõÂèóÈôê„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊèêÂá∫DARSÔºåÈÄöËøáÂ§öÈò∂ÊÆµÂõûÊªöÁ≠ñÁï•ÈáçÊñ∞Âä†ÊùÉÂõ∞ÈöæÈóÆÈ¢òÔºåÂ¢ûÂä†Ê≠£ÂõûÊªöÊï∞ÈáèÔºå‰ªéËÄåÊèêÂçáÊ®°ÂûãÂØπÂ§çÊùÇÈóÆÈ¢òÁöÑÂ§ÑÁêÜËÉΩÂäõ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨DARSÊ®°ÂùóÂíåÂ§ßÂπøÂ∫¶ËÆ≠ÁªÉÁ≠ñÁï•„ÄÇDARSÊ®°ÂùóÈÄöËøáÂ§öÈò∂ÊÆµÂõûÊªöÂØπÂõ∞ÈöæÊ†∑Êú¨ËøõË°åÂä†ÊùÉÔºåËÄåÂ§ßÂπøÂ∫¶ËÆ≠ÁªÉÂàôÈÄöËøáÂÖ®ÊâπÊ¨°Êõ¥Êñ∞Êõø‰ª£Â∞èÊâπÊ¨°Ëø≠‰ª£ÔºåÂ¢ûÂº∫ËÆ≠ÁªÉÊï∞ÊçÆÁöÑÂπøÂ∫¶„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöDARSÊòØÊú¨Á†îÁ©∂ÁöÑÊ†∏ÂøÉÂàõÊñ∞ÁÇπÔºåÂÖ∂ÈÄöËøáÈíàÂØπÊÄßÂõûÊªöËß£ÂÜ≥‰∫ÜÁé∞ÊúâÊñπÊ≥ïÂØπÂõ∞ÈöæÈóÆÈ¢òÁöÑÂøΩËßÜÔºå‰∏é‰º†ÁªüÊñπÊ≥ïÁõ∏ÊØîÔºåËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞Êé®Âä®Êé®ÁêÜËæπÁïåÁöÑÊâ©Â±ï„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®DARS‰∏≠ÔºåÂÖ≥ÈîÆÂèÇÊï∞ÂåÖÊã¨ÂõûÊªöÈò∂ÊÆµÁöÑÊï∞ÈáèÂíåÊ†∑Êú¨Âä†ÊùÉÁ≠ñÁï•ÔºõÂú®Â§ßÂπøÂ∫¶ËÆ≠ÁªÉ‰∏≠ÔºåÈááÁî®ÂÖ®ÊâπÊ¨°Êõ¥Êñ∞‰ª•Áª¥ÊåÅÈ´òÁöÑtokenÁ∫ßÂà´ÁÜµÔºå‰øÉËøõÊåÅÁª≠Êé¢Á¥¢„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåDARS‰∏éÂ§ßÂπøÂ∫¶ËÆ≠ÁªÉÁªìÂêàÂêéÔºåPass@KÂíåPass@1ÁöÑÊÄßËÉΩÂùáÊúâÊòæËëóÊèêÂçáÔºåPass@1ÊÄßËÉΩÊèêÂçáÂπÖÂ∫¶ËææÂà∞XX%ÔºåÈ™åËØÅ‰∫ÜÂπøÂ∫¶‰∏éÊ∑±Â∫¶Êé¢Á¥¢ÁöÑÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ„ÄÅÊô∫ËÉΩÈóÆÁ≠îÁ≥ªÁªüÂíåËá™Âä®Êé®ÁêÜÁ≠â„ÄÇÈÄöËøáÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõÔºåËÉΩÂ§üÂú®Êõ¥Â§çÊùÇÁöÑ‰ªªÂä°‰∏≠ÂÆûÁé∞Êõ¥È´òÁöÑÂáÜÁ°ÆÊÄßÂíåÊïàÁéáÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÊú™Êù•ÂΩ±Âìç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Reinforcement Learning with Verifiable Reward (RLVR) has emerged as a powerful paradigm for unlocking reasoning capabilities in large language models, yet its full potential is hindered by two under-explored dimensions: Depth-the hardest problem a model can sample; Breadth-the number of instances consumed in a single iteration. We dissect the popular GRPO algorithm and reveal a systematic bias: the cumulative-advantage disproportionately weights samples with medium accuracy, while down-weighting the low-accuracy instances that are crucial for pushing reasoning boundaries. To rectify the depth neglect, we introduce Difficulty Adaptive Rollout Sampling (DARS), which re-weights hard problems through targeted multi-stage rollouts, thereby increasing the number of positive rollouts for hard problems. Empirically, naively enlarging rollout size only accelerates convergence and even hurts Pass@K. Our DARS, in contrast, delivers consistent Pass@K gains without extra inference cost at convergence. Just as we adaptively expanded the depth of exploration, we now ask whether aggressively scaling the breadth of training data can further amplify reasoning gains. To this end, we intensely scale batch size and replace PPO's mini-batch iterations with full-batch updates over multiple epochs. Increasing breadth significantly enhances Pass@1 performance. Large-breadth training sustains high token-level entropy, indicating continued exploration and reduced gradient noise. We further present DARS-B, which augments DARS with large breadth, and demonstrate simultaneous gains in Pass@K and Pass@1. The results confirm that breadth and adaptive exploration across depth operate as orthogonal dimensions in RLVR, which are key to unleashing the reasoning power of RLVR.

