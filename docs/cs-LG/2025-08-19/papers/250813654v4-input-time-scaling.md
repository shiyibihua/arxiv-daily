---
layout: default
title: Input-Time Scaling
---

# Input-Time Scaling

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.13654" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.13654v4</a>
  <a href="https://arxiv.org/pdf/2508.13654.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.13654v4" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.13654v4', 'Input-Time Scaling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Rapheal Huang, Weilong Guo

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-19 (æ›´æ–°: 2025-09-12)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè¾“å…¥æ—¶é—´ç¼©æ”¾æ–¹æ³•ä»¥æå‡å¤§è¯­è¨€æ¨¡å‹æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¾“å…¥æ—¶é—´ç¼©æ”¾` `å¤§å‹è¯­è¨€æ¨¡å‹` `è®­ç»ƒ-æµ‹è¯•å…±è®¾è®¡` `å…ƒçŸ¥è¯†` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°æ®å’Œè®­ç»ƒçš„ç¼©æ”¾æ–¹é¢å­˜åœ¨å±€é™ï¼Œæ¨ç†æ—¶é—´çš„ä¼˜åŒ–ä¹Ÿæœªèƒ½å……åˆ†åˆ©ç”¨è¾“å…¥çš„æ½œåŠ›ã€‚
2. æœ¬æ–‡æå‡ºçš„è¾“å…¥æ—¶é—´ç¼©æ”¾æ–¹æ³•ï¼Œé€šè¿‡åœ¨è®­ç»ƒå’Œæµ‹è¯•é˜¶æ®µä¼˜åŒ–è¾“å…¥ï¼Œåˆ©ç”¨å…ƒçŸ¥è¯†æå‡æ¨¡å‹æ€§èƒ½ã€‚
3. åœ¨Qwen2.5-32B-Instructæ¨¡å‹ä¸Šï¼Œå®éªŒç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨AIME24å’ŒAIME25ä»»åŠ¡ä¸Šè¾¾åˆ°äº†76.7%çš„SOTAæ€§èƒ½ï¼Œä¸”é€šè¿‡æ¨¡å‹æŠ•ç¥¨è¿›ä¸€æ­¥æå‡äº†ç»“æœã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šå¸¸ä¾èµ–äºåæœŸè®­ç»ƒå’Œæ¨ç†æ—¶é—´çš„ç¼©æ”¾æ–¹æ³•ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç¼©æ”¾èŒƒå¼â€”â€”è¾“å…¥æ—¶é—´ç¼©æ”¾ï¼Œé€šè¿‡åœ¨æŸ¥è¯¢ä¸ŠæŠ•å…¥èµ„æºæ¥è¡¥å……ä¹‹å‰çš„ç¼©æ”¾æ–¹æ³•ã€‚åœ¨è®­ç»ƒå’Œæµ‹è¯•è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨LLMsçš„å…ƒçŸ¥è¯†ï¼Œé€šè¿‡ä¸åŒç­–ç•¥ä¼˜åŒ–è¾“å…¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°äº†ä¸€ç§æ–°çš„ç°è±¡â€”â€”è®­ç»ƒ-æµ‹è¯•å…±è®¾è®¡ï¼Œè¦æ±‚åœ¨è®­ç»ƒå’Œæµ‹è¯•ä¸­æ•´ä½“åº”ç”¨æŸ¥è¯¢ç­–ç•¥ã€‚å®éªŒè¡¨æ˜ï¼Œä½è´¨é‡æ•°æ®é›†ä¹Ÿèƒ½è¡¨ç°å‡ºè‰²ï¼Œç”šè‡³åœ¨æ·»åŠ æ— å…³ä¿¡æ¯çš„æƒ…å†µä¸‹ï¼Œä½¿ç”¨æœ€å°è¿‡æ»¤æ•°æ®é›†çš„éšæœº1kç¤ºä¾‹ä¹Ÿèƒ½è·å¾—æœ€ä½³æ€§èƒ½ã€‚è¿™äº›å‘ç°æŒ‘æˆ˜äº†â€œåƒåœ¾è¿›ï¼Œåƒåœ¾å‡ºâ€çš„ä¼ ç»Ÿè§‚ç‚¹ï¼Œå¹¶ä¸â€œå°‘å³æ˜¯å¤šâ€çš„ç°è±¡ç›¸å…¼å®¹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°æ®å’Œæ¨ç†æ—¶é—´ç¼©æ”¾æ–¹é¢çš„ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯å¦‚ä½•æœ‰æ•ˆåˆ©ç”¨è¾“å…¥ä¿¡æ¯ä»¥æå‡æ¨¡å‹æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºè¾“å…¥æ—¶é—´ç¼©æ”¾çš„æ¦‚å¿µï¼Œé€šè¿‡åœ¨è®­ç»ƒå’Œæµ‹è¯•é˜¶æ®µæ•´ä½“åº”ç”¨æŸ¥è¯¢ç­–ç•¥ï¼Œåˆ©ç”¨å…ƒçŸ¥è¯†ä¼˜åŒ–è¾“å…¥ï¼Œä»è€Œæå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æµç¨‹åŒ…æ‹¬æ•°æ®å‡†å¤‡ã€æ¨¡å‹è®­ç»ƒå’Œæµ‹è¯•é˜¶æ®µã€‚åœ¨è®­ç»ƒé˜¶æ®µï¼Œåº”ç”¨ä¸åŒçš„æŸ¥è¯¢ç­–ç•¥æ¥ä¼˜åŒ–è¾“å…¥ï¼›åœ¨æµ‹è¯•é˜¶æ®µï¼Œä¿æŒä¸€è‡´çš„ç­–ç•¥ä»¥ç¡®ä¿æ€§èƒ½æå‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†è®­ç»ƒ-æµ‹è¯•å…±è®¾è®¡çš„æ¦‚å¿µï¼Œå¼ºè°ƒåœ¨ä¸¤ä¸ªé˜¶æ®µä¸­ä¸€è‡´æ€§çš„é‡è¦æ€§ï¼Œè¿™ä¸ä¼ ç»Ÿçš„å•ä¸€é˜¶æ®µä¼˜åŒ–æ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œä½¿ç”¨äº†éšæœºé€‰æ‹©çš„1kç¤ºä¾‹è¿›è¡Œè®­ç»ƒï¼Œå‘ç°å³ä½¿åœ¨ä½è´¨é‡æ•°æ®é›†ä¸Šï¼Œæ¨¡å‹ä¹Ÿèƒ½è·å¾—è¾ƒå¥½çš„æ€§èƒ½ï¼ŒæŒ‘æˆ˜äº†ä¼ ç»Ÿçš„é«˜è´¨é‡æ•°æ®é›†çš„å¿…è¦æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨è¾“å…¥æ—¶é—´ç¼©æ”¾æ–¹æ³•ï¼ŒQwen2.5-32B-Instructæ¨¡å‹åœ¨AIME24å’ŒAIME25ä»»åŠ¡ä¸Šè¾¾åˆ°äº†76.7%çš„SOTAæ€§èƒ½ã€‚é€šè¿‡ä¸‰æ¨¡å‹çš„å¤šæ•°æŠ•ç¥¨ï¼Œè¿›ä¸€æ­¥æå‡è‡³AIME24çš„90.0%å’ŒAIME25çš„80%ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿå’Œå¯¹è¯ç”Ÿæˆç­‰ã€‚é€šè¿‡ä¼˜åŒ–è¾“å…¥ç­–ç•¥ï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨èµ„æºæœ‰é™çš„æƒ…å†µä¸‹å®ç°æ›´é«˜çš„æ¨ç†èƒ½åŠ›ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Current Large Language Models (LLMs) are usually post-trained on large-scale carefully curated datasets (data & training scaling) and doing reasoning in test time (inference time scaling). In this work, we present a new scaling paradigm, Input-Time Scaling, to complement previous scaling methods by putting resources on queries (input time). During training and testing, we utilize meta-knowledge from LLMs to refine inputs with different strategies. We also discover a new phenomenon, train-test co-design. It requires us to apply query strategies during training and testing as a whole. Only applying strategies on training or testing would seriously degrade the performance gained. We are also surprised to find that seemingly low data quality datasets can perform better. We can get the best performance even by adding irrelevant information to the queries, with randomly selected 1k examples from a minimally filtered dataset. These findings contradict the widely held inductive bias, "garbage in, garbage out". Curating datasets with seemingly high-quality data can even potentially limit the performance ceiling. In addition, models trained on more data with similar quality (15k VS 1k) perform worse, the intuition of simply scaling the size should also be carefully inspected. The good news is that our findings are compatible with the Less is More phenomenon. 1K examples are enough to invoke high-level reasoning ability. With experiments on Qwen2.5-32B-Instruct, we are able to reach SOTA performance among 32B models on AIME24(76.7%) and AIME25(76.7%) pass@1. We can further achieve AIME24(76.7%) and AIME25(80%) with a majority vote of three models. Starting from DeepSeek-R1-Distill-Qwen-32B, the result would be 90.0% on AIME24 and 80.0% on AIME25. To facilitate reproducibility and further research, we are working on open-source our datasets, data pipelines, evaluation results, and checkpoints.

