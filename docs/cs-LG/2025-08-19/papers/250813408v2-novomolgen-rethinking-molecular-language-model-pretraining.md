---
layout: default
title: NovoMolGen: Rethinking Molecular Language Model Pretraining
---

# NovoMolGen: Rethinking Molecular Language Model Pretraining

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.13408" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.13408v2</a>
  <a href="https://arxiv.org/pdf/2508.13408.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.13408v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.13408v2', 'NovoMolGen: Rethinking Molecular Language Model Pretraining')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kamran Chitsaz, Roshan Balaji, Quentin Fournier, Nirav Pravinbhai Bhatt, Sarath Chandar

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-19 (æ›´æ–°: 2025-08-22)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºNovoMolGenä»¥æå‡åˆ†å­ç”Ÿæˆæ•ˆç‡ä¸æ•ˆæœ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åˆ†å­ç”Ÿæˆ` `æ·±åº¦å­¦ä¹ ` `å˜æ¢å™¨æ¨¡å‹` `è¯ç‰©å‘ç°` `åŒ–å­¦ç©ºé—´æ¢ç´¢` `å¤§è¯­è¨€æ¨¡å‹` `ç”Ÿæˆæ¨¡å‹` `é¢„è®­ç»ƒç­–ç•¥`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åˆ†å­ç”Ÿæˆæ–¹æ³•åœ¨æ¢ç´¢åŒ–å­¦ç©ºé—´æ—¶æ•ˆç‡ä¸è¶³ï¼Œéš¾ä»¥æ»¡è¶³è®¾è®¡å…·æœ‰ç‰¹å®šå±æ€§çš„åˆ†å­éœ€æ±‚ã€‚
2. æœ¬æ–‡æå‡ºNovoMolGenï¼Œé€šè¿‡åœ¨å¤§é‡åˆ†å­ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç³»ç»Ÿæ€§ç ”ç©¶è¯­è¨€å»ºæ¨¡å®è·µå¯¹åˆ†å­ç”Ÿæˆçš„å½±å“ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºNovoMolGenåœ¨å¤šä¸ªåˆ†å­ç”Ÿæˆä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œå»ºç«‹äº†æ–°çš„æ€§èƒ½åŸºå‡†ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è®¾è®¡å…·æœ‰ç‰¹å®šå±æ€§çš„å…¨æ–°åˆ†å­éœ€è¦é«˜æ•ˆæ¢ç´¢å¹¿é˜”çš„åŒ–å­¦ç©ºé—´ï¼Œç°æœ‰çš„æ·±åº¦ç”Ÿæˆæ¨¡å‹åœ¨å°åˆ†å­è®¾è®¡æ–¹é¢å–å¾—äº†ä¸€å®šè¿›å±•ï¼Œä½†åŸºäºå­—ç¬¦ä¸²è¡¨ç¤ºçš„åˆ†å­å¤§è¯­è¨€æ¨¡å‹ï¼ˆMol-LLMsï¼‰å› å…¶å¯æ‰©å±•æ€§è€Œå—åˆ°å…³æ³¨ã€‚æœ¬æ–‡æå‡ºNovoMolGenï¼Œä¸€ä¸ªåœ¨15äº¿ä¸ªåˆ†å­ä¸Šé¢„è®­ç»ƒçš„å˜æ¢å™¨åŸºç¡€æ¨¡å‹ï¼Œç³»ç»Ÿæ€§åœ°ç ”ç©¶äº†æ–‡æœ¬è¡¨ç¤ºã€åˆ†è¯ç­–ç•¥ã€æ¨¡å‹è§„æ¨¡å’Œæ•°æ®é›†è§„æ¨¡å¯¹åˆ†å­ç”Ÿæˆæ€§èƒ½çš„å½±å“ã€‚ç ”ç©¶å‘ç°ï¼Œé¢„è®­ç»ƒæœŸé—´çš„æ€§èƒ½æŒ‡æ ‡ä¸å®é™…ä¸‹æ¸¸æ€§èƒ½ä¹‹é—´å­˜åœ¨å¼±ç›¸å…³æ€§ï¼Œæ­ç¤ºäº†åˆ†å­ä¸é€šç”¨NLPè®­ç»ƒåŠ¨æ€ä¹‹é—´çš„é‡è¦åŒºåˆ«ã€‚NovoMolGenåœ¨æ— çº¦æŸå’Œç›®æ ‡å¯¼å‘çš„åˆ†å­ç”Ÿæˆä»»åŠ¡ä¸­å‡æ˜¾è‘—è¶…è¶Šäº†ä¹‹å‰çš„Mol-LLMså’Œä¸“é—¨çš„ç”Ÿæˆæ¨¡å‹ï¼Œå¥ å®šäº†é«˜æ•ˆåˆ†å­å»ºæ¨¡ç­–ç•¥çš„åšå®åŸºç¡€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰åˆ†å­ç”Ÿæˆæ¨¡å‹åœ¨æ¢ç´¢åŒ–å­¦ç©ºé—´æ—¶æ•ˆç‡ä½ä¸‹å’Œæ€§èƒ½ä¸ç¨³å®šçš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨é¢„è®­ç»ƒä¸å®é™…åº”ç”¨ä¹‹é—´å­˜åœ¨æ€§èƒ½å·®è·ï¼Œé™åˆ¶äº†å…¶åœ¨åˆ†å­è®¾è®¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šNovoMolGené€šè¿‡åœ¨15äº¿ä¸ªåˆ†å­ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç³»ç»Ÿæ€§åœ°åˆ†ææ–‡æœ¬è¡¨ç¤ºã€åˆ†è¯ç­–ç•¥ã€æ¨¡å‹è§„æ¨¡å’Œæ•°æ®é›†è§„æ¨¡å¯¹åˆ†å­ç”Ÿæˆçš„å½±å“ï¼Œä»è€Œä¼˜åŒ–ç”Ÿæˆæ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šNovoMolGené‡‡ç”¨å˜æ¢å™¨æ¶æ„ï¼ŒåŒ…å«æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹è®­ç»ƒå’Œæ€§èƒ½è¯„ä¼°ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚æ•°æ®é¢„å¤„ç†é˜¶æ®µåŒ…æ‹¬åˆ†å­è¡¨ç¤ºçš„æ ‡å‡†åŒ–å’Œåˆ†è¯ç­–ç•¥çš„é€‰æ‹©ï¼Œæ¨¡å‹è®­ç»ƒé˜¶æ®µåˆ™ä¾§é‡äºä¼˜åŒ–æ¨¡å‹å‚æ•°å’Œç»“æ„ï¼Œæ€§èƒ½è¯„ä¼°é˜¶æ®µé€šè¿‡å¤šé¡¹ä»»åŠ¡éªŒè¯æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šNovoMolGençš„åˆ›æ–°åœ¨äºå…¶ç³»ç»Ÿæ€§ç ”ç©¶äº†è¯­è¨€å»ºæ¨¡å®è·µå¯¹åˆ†å­ç”Ÿæˆçš„å½±å“ï¼Œæ­ç¤ºäº†åˆ†å­ç”Ÿæˆä¸é€šç”¨NLPè®­ç»ƒä¹‹é—´çš„æœ¬è´¨åŒºåˆ«ï¼Œæå‡ºäº†æ–°çš„é¢„è®­ç»ƒç­–ç•¥ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¨¡å‹é‡‡ç”¨äº†å¤šå±‚å˜æ¢å™¨ç»“æ„ï¼Œä½¿ç”¨äº†ç‰¹å®šçš„åˆ†è¯ç­–ç•¥ä»¥é€‚åº”åˆ†å­ç»“æ„ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šè€ƒè™‘äº†ç”Ÿæˆè´¨é‡ä¸å¤šæ ·æ€§çš„å¹³è¡¡ï¼Œæ¨¡å‹è§„æ¨¡ç»è¿‡ä¼˜åŒ–ä»¥ç¡®ä¿è®­ç»ƒæ•ˆç‡ä¸ç”Ÿæˆæ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

NovoMolGenåœ¨æ— çº¦æŸå’Œç›®æ ‡å¯¼å‘çš„åˆ†å­ç”Ÿæˆä»»åŠ¡ä¸­å‡æ˜¾è‘—è¶…è¶Šäº†ä¹‹å‰çš„Mol-LLMsï¼Œå»ºç«‹äº†æ–°çš„æ€§èƒ½åŸºå‡†ï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦è¶…è¿‡äº†20%ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨åˆ†å­ç”Ÿæˆé¢†åŸŸçš„å¼ºå¤§èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

NovoMolGençš„ç ”ç©¶æˆæœåœ¨è¯ç‰©å‘ç°ã€ææ–™ç§‘å­¦å’ŒåŒ–å­¦åˆæˆç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡é«˜æ•ˆç”Ÿæˆå…·æœ‰ç‰¹å®šæ€§è´¨çš„åˆ†å­ï¼Œèƒ½å¤ŸåŠ é€Ÿæ–°è¯çš„ç ”å‘å’Œæ–°ææ–™çš„è®¾è®¡ï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„æŠ€æœ¯è¿›æ­¥ä¸åˆ›æ–°ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Designing de-novo molecules with desired property profiles requires efficient exploration of the vast chemical space ranging from $10^{23}$ to $10^{60}$ possible synthesizable candidates. While various deep generative models have been developed to design small molecules using diverse input representations, Molecular Large Language Models (Mol-LLMs) based on string representations have emerged as a scalable approach capable of exploring billions of molecules. However, there remains limited understanding regarding how standard language modeling practices such as textual representations, tokenization strategies, model size, and dataset scale impact molecular generation performance. In this work, we systematically investigate these critical aspects by introducing NovoMolGen, a family of transformer-based foundation models pretrained on 1.5 billion molecules for de-novo molecule generation. Through extensive empirical analyses, we identify a weak correlation between performance metrics measured during pretraining and actual downstream performance, revealing important distinctions between molecular and general NLP training dynamics. NovoMolGen establishes new state-of-the-art results, substantially outperforming prior Mol-LLMs and specialized generative models in both unconstrained and goal-directed molecular generation tasks, thus providing a robust foundation for advancing efficient and effective molecular modeling strategies.

