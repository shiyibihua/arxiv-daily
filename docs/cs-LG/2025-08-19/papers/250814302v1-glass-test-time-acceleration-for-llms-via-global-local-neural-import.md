---
layout: default
title: GLASS: Test-Time Acceleration for LLMs via Global-Local Neural Importance Aggregation
---

# GLASS: Test-Time Acceleration for LLMs via Global-Local Neural Importance Aggregation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.14302" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.14302v1</a>
  <a href="https://arxiv.org/pdf/2508.14302.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.14302v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.14302v1', 'GLASS: Test-Time Acceleration for LLMs via Global-Local Neural Importance Aggregation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Amirmohsen Sattarifard, Sepehr Lavasani, Ehsan Imani, Kunlin Zhang, Hanlin Xu, Fengyu Sun, Negar Hassanpour, Chao Gao

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-19

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºA/I-GLASSä»¥è§£å†³LLMsåœ¨è¾¹ç¼˜ç¡¬ä»¶ä¸Šçš„åŠ¨æ€å‰ªæé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åŠ¨æ€å‰ªæ` `å¤§å‹è¯­è¨€æ¨¡å‹` `è¾¹ç¼˜è®¡ç®—` `ç¥ç»ç½‘ç»œ` `æ¨ç†æ•ˆç‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨åŠ¨æ€å‰ªææ—¶å­˜åœ¨é”å®šå•ä¸€ç¨€ç–æ¨¡å¼æˆ–å¢åŠ è¿è¡Œæ—¶å¼€é”€çš„é—®é¢˜ï¼Œå¯¼è‡´æ€§èƒ½å—é™ã€‚
2. æœ¬æ–‡æå‡ºA/I-GLASSï¼Œé€šè¿‡æ¿€æ´»å’Œå½±å“çš„å…¨å±€-å±€éƒ¨ç¥ç»é‡è¦æ€§èšåˆï¼ŒåŠ¨æ€é€‰æ‹©å‰é¦ˆç½‘ç»œå•å…ƒï¼Œé¿å…äº†è®­ç»ƒè¿‡ç¨‹ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGLASSåœ¨é•¿æ–‡æœ¬ç”Ÿæˆåœºæ™¯ä¸­æ˜¾è‘—æå‡æ€§èƒ½ï¼Œè¶…è¶Šäº†ä»¥å¾€çš„è®­ç»ƒæ— å…³æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨è¾¹ç¼˜ç¡¬ä»¶ä¸Šéƒ¨ç½²å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰éœ€è¦è¿›è¡Œæ¿€è¿›çš„ã€åŠæ—¶æ„ŸçŸ¥çš„åŠ¨æ€å‰ªæï¼Œä»¥å‡å°‘è®¡ç®—é‡è€Œä¸é™ä½è´¨é‡ã€‚ç°æœ‰çš„é™æ€æˆ–åŸºäºé¢„æµ‹çš„æ–¹æ³•è¦ä¹ˆé”å®šå•ä¸€ç¨€ç–æ¨¡å¼ï¼Œè¦ä¹ˆå¢åŠ é¢å¤–çš„è¿è¡Œæ—¶å¼€é”€ï¼Œè€Œæœ€è¿‘ä¾èµ–å•ä¸€æç¤ºç»Ÿè®¡çš„é›¶-shotæ–¹æ³•åœ¨çŸ­æç¤ºæˆ–é•¿ç”Ÿæˆåœºæ™¯ä¸­è¡¨ç°ä¸ä½³ã€‚æœ¬æ–‡æå‡ºäº†A/I-GLASSï¼šåŸºäºæ¿€æ´»å’Œå½±å“çš„å…¨å±€-å±€éƒ¨ç¥ç»é‡è¦æ€§èšåˆï¼Œç”¨äºå‰é¦ˆç½‘ç»œçš„ç¨€ç–åŒ–ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œé€šè¿‡å¯¹æç¤ºå±€éƒ¨å’Œæ¨¡å‹å†…åœ¨å…¨å±€ç¥ç»å…ƒç»Ÿè®¡çš„æ’åèšåˆï¼ŒåŠ¨æ€é€‰æ‹©å‰é¦ˆç½‘ç»œå•å…ƒã€‚å®éªŒè¯æ˜ï¼ŒGLASSåœ¨å¤šä¸ªLLMså’ŒåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºä»¥å¾€çš„è®­ç»ƒæ— å…³æ–¹æ³•ï¼Œå°¤å…¶æ˜¯åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„é•¿æ–‡æœ¬ç”Ÿæˆåœºæ™¯ä¸­ï¼Œä¸”ä¸ä¾èµ–è¾…åŠ©é¢„æµ‹å™¨æˆ–å¢åŠ æ¨ç†å¼€é”€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¾¹ç¼˜ç¡¬ä»¶ä¸Šéƒ¨ç½²æ—¶çš„åŠ¨æ€å‰ªæé—®é¢˜ã€‚ç°æœ‰æ–¹æ³•è¦ä¹ˆå›ºå®šç¨€ç–æ¨¡å¼ï¼Œå¯¼è‡´çµæ´»æ€§ä¸è¶³ï¼Œè¦ä¹ˆå¢åŠ é¢å¤–çš„è¿è¡Œæ—¶å¼€é”€ï¼Œå½±å“æ¨ç†æ•ˆç‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šA/I-GLASSé€šè¿‡èšåˆæç¤ºå±€éƒ¨å’Œæ¨¡å‹å…¨å±€ç¥ç»å…ƒçš„ç»Ÿè®¡ä¿¡æ¯ï¼ŒåŠ¨æ€é€‰æ‹©å‰é¦ˆç½‘ç»œå•å…ƒï¼Œé¿å…äº†ä¾èµ–è®­ç»ƒçš„è¿‡ç¨‹ï¼Œä»è€Œæé«˜äº†æ¨¡å‹çš„æ¨ç†æ•ˆç‡å’Œçµæ´»æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šæ¿€æ´»åŸºäºçš„é€‰æ‹©å’Œå½±å“åŸºäºçš„é€‰æ‹©ã€‚é¦–å…ˆï¼Œé€šè¿‡å¯¹æç¤ºçš„å±€éƒ¨ä¿¡æ¯è¿›è¡Œåˆ†æï¼Œè¯†åˆ«å‡ºé‡è¦çš„ç¥ç»å…ƒï¼›å…¶æ¬¡ï¼Œç»“åˆæ¨¡å‹çš„å…¨å±€ä¿¡æ¯ï¼Œè¿›è¡Œé‡è¦æ€§èšåˆï¼Œæœ€ç»ˆåŠ¨æ€é€‰æ‹©å‰é¦ˆç½‘ç»œçš„å•å…ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šA/I-GLASSçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶è®­ç»ƒæ— å…³çš„åŠ¨æ€é€‰æ‹©æœºåˆ¶ï¼Œé€šè¿‡å…¨å±€å’Œå±€éƒ¨ä¿¡æ¯çš„ç»“åˆï¼Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•çš„å±€é™æ€§ï¼Œæ˜¾è‘—æå‡äº†åœ¨é•¿æ–‡æœ¬ç”Ÿæˆåœºæ™¯ä¸­çš„è¡¨ç°ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒA/I-GLASSä½¿ç”¨äº†ç‰¹å®šçš„èšåˆç®—æ³•æ¥å¤„ç†ç¥ç»å…ƒçš„é‡è¦æ€§è¯„åˆ†ï¼Œå¹¶ä¸”åœ¨é€‰æ‹©è¿‡ç¨‹ä¸­æ²¡æœ‰å¼•å…¥é¢å¤–çš„é¢„æµ‹å™¨ï¼Œä»è€Œä¿æŒäº†æ¨ç†çš„é«˜æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒGLASSåœ¨å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹å’ŒåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨é•¿æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œç›¸è¾ƒäºä»¥å¾€çš„è®­ç»ƒæ— å…³æ–¹æ³•ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°æ˜¾è‘—æ°´å¹³ï¼Œä¸”æ²¡æœ‰å¢åŠ æ¨ç†å¼€é”€ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è¾¹ç¼˜è®¡ç®—è®¾å¤‡ä¸Šçš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œå¦‚æ™ºèƒ½åŠ©æ‰‹ã€å®æ—¶ç¿»è¯‘å’Œå†…å®¹ç”Ÿæˆç­‰ã€‚é€šè¿‡æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†æ•ˆç‡ï¼ŒA/I-GLASSèƒ½å¤Ÿåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­å®ç°æ›´é«˜æ•ˆçš„åº”ç”¨ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Deploying Large Language Models (LLMs) on edge hardware demands aggressive, prompt-aware dynamic pruning to reduce computation without degrading quality. Static or predictor-based schemes either lock in a single sparsity pattern or incur extra runtime overhead, and recent zero-shot methods that rely on statistics from a single prompt fail on short prompt and/or long generation scenarios. We introduce A/I-GLASS: Activation- and Impact-based Global-Local neural importance Aggregation for feed-forward network SparSification, two training-free methods that dynamically select FFN units using a rank-aggregation of prompt local and model-intrinsic global neuron statistics. Empirical results across multiple LLMs and benchmarks demonstrate that GLASS significantly outperforms prior training-free methods, particularly in challenging long-form generation scenarios, without relying on auxiliary predictors or adding any inference overhead.

