---
layout: default
title: RewardRank: Optimizing True Learning-to-Rank Utility
---

# RewardRank: Optimizing True Learning-to-Rank Utility

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.14180" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.14180v2</a>
  <a href="https://arxiv.org/pdf/2508.14180.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.14180v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.14180v2', 'RewardRank: Optimizing True Learning-to-Rank Utility')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Gaurav Bhatt, Kiran Koshy Thekumparampil, Tanmay Gangwani, Tesi Xiao, Leonid Sigal

**åˆ†ç±»**: cs.IR, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-19 (æ›´æ–°: 2025-10-17)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/GauravBh1010tt/RewardRank)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRewardRankä»¥ä¼˜åŒ–çœŸå®å­¦ä¹ æ’åºæ•ˆç”¨**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å­¦ä¹ æ’åº` `åäº‹å®ä¼˜åŒ–` `ç”¨æˆ·è¡Œä¸ºå»ºæ¨¡` `æ•°æ®é©±åŠ¨æ–¹æ³•` `æ¨èç³»ç»Ÿ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ’åºç³»ç»Ÿå¾€å¾€ä¾èµ–äºç®€åŒ–çš„ç”¨æˆ·è¡Œä¸ºå‡è®¾ï¼Œå¯¼è‡´æ— æ³•æœ‰æ•ˆæå‡çœŸå®çš„ç”¨æˆ·æ•ˆç”¨ã€‚
2. è®ºæ–‡æå‡ºRewardRankæ¡†æ¶ï¼Œé€šè¿‡å­¦ä¹ ç”¨æˆ·äº¤äº’æ•°æ®ä¸­çš„å¥–åŠ±æ¨¡å‹ï¼Œç›´æ¥ä¼˜åŒ–åäº‹å®æ•ˆç”¨ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRewardRankåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ä¼ ç»Ÿæ–¹æ³•ï¼Œæå‡äº†ç¦»çº¿ç›¸å…³æ€§æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¼ ç»Ÿçš„æ’åºç³»ç»Ÿé€šå¸¸ä¼˜åŒ–ç¦»çº¿ä»£ç†ç›®æ ‡ï¼Œè¿™äº›ç›®æ ‡ä¾èµ–äºå¯¹ç”¨æˆ·è¡Œä¸ºçš„è¿‡äºç®€åŒ–çš„å‡è®¾ï¼Œå¾€å¾€å¿½è§†äº†ä½ç½®åå·®å’Œé¡¹ç›®å¤šæ ·æ€§ç­‰å› ç´ ã€‚å› æ­¤ï¼Œè¿™äº›æ¨¡å‹åœ¨åœ¨çº¿A/Bæµ‹è¯•ä¸­æ— æ³•æé«˜çœŸå®çš„åäº‹å®æ•ˆç”¨ï¼Œå¦‚ç‚¹å‡»ç‡æˆ–è´­ä¹°æ¦‚ç‡ã€‚æˆ‘ä»¬æå‡ºäº†RewardRankï¼Œè¿™æ˜¯ä¸€ç§æ•°æ®é©±åŠ¨çš„å­¦ä¹ æ’åºæ¡†æ¶ï¼Œæ—¨åœ¨æœ€å¤§åŒ–åäº‹å®æ•ˆç”¨ã€‚RewardRanké¦–å…ˆä»è®°å½•çš„ç”¨æˆ·äº¤äº’ä¸­å­¦ä¹ å¥–åŠ±æ¨¡å‹ï¼Œé¢„æµ‹ä»»ä½•æ’åºçš„æ•ˆç”¨ï¼Œç„¶åè®­ç»ƒæ’åºå™¨ä½¿ç”¨å¯å¾®åˆ†çš„è½¯æ’åˆ—æ“ä½œæ¥æœ€å¤§åŒ–è¯¥å¥–åŠ±ã€‚ä¸ºäº†å®ç°ä¸¥æ ¼å’Œå¯é‡å¤çš„è¯„ä¼°ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸¤ä¸ªåŸºå‡†å¥—ä»¶ï¼šå‚æ•°åŒ–Oracleè¯„ä¼°ï¼ˆPO-Evalï¼‰å’ŒLLMä½œä¸ºç”¨æˆ·è¯„ä¼°ï¼ˆLAU-Evalï¼‰ã€‚RewardRankåœ¨è¿™ä¸¤ä¸ªåŸºå‡†ä¸Šéƒ½å®ç°äº†æœ€é«˜çš„åäº‹å®æ•ˆç”¨ï¼Œå¹¶è¯æ˜ä¼˜åŒ–ç»å…¸æŒ‡æ ‡å¦‚NDCGå¯¹äºæœ€å¤§åŒ–çœŸå®ç”¨æˆ·æ•ˆç”¨æ˜¯æ¬¡ä¼˜çš„ã€‚æœ€ç»ˆï¼Œä½¿ç”¨æ¥è‡ªBaidu-ULTRæ•°æ®é›†çš„çœŸå®ç”¨æˆ·åé¦ˆï¼ŒRewardRankåœ¨ç¦»çº¿ç›¸å…³æ€§æ€§èƒ½ä¸Šå»ºç«‹äº†æ–°çš„çŠ¶æ€ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå­¦ä¹ æ’åºå¯ä»¥é‡æ–°è¡¨è¿°ä¸ºåäº‹å®æ•ˆç”¨çš„ç›´æ¥ä¼˜åŒ–ï¼Œä¸”ä»¥çº¯æ•°æ®é©±åŠ¨çš„æ–¹å¼å®ç°ï¼Œæ— éœ€ä¾èµ–ä½ç½®åå·®ç­‰æ˜¾å¼å»ºæ¨¡å‡è®¾ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡è¦è§£å†³çš„é—®é¢˜æ˜¯ç°æœ‰æ’åºç³»ç»Ÿåœ¨ä¼˜åŒ–ç”¨æˆ·æ•ˆç”¨æ—¶çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯æœªèƒ½è€ƒè™‘ä½ç½®åå·®å’Œé¡¹ç›®å¤šæ ·æ€§ç­‰å› ç´ ï¼Œå¯¼è‡´åœ¨çº¿æµ‹è¯•ä¸­çš„æ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šRewardRankçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ•°æ®é©±åŠ¨çš„æ–¹æ³•ï¼Œç›´æ¥ä»ç”¨æˆ·äº¤äº’ä¸­å­¦ä¹ å¥–åŠ±æ¨¡å‹ï¼Œå¹¶åˆ©ç”¨è¯¥æ¨¡å‹ä¼˜åŒ–æ’åºï¼Œè€Œä¸æ˜¯ä¾èµ–äºä¼ ç»Ÿçš„ä»£ç†ç›®æ ‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRewardRankçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ˜¯å¥–åŠ±æ¨¡å‹çš„å­¦ä¹ ï¼Œæ¥ç€æ˜¯åŸºäºè¯¥æ¨¡å‹çš„æ’åºå™¨è®­ç»ƒï¼Œä½¿ç”¨å¯å¾®åˆ†çš„è½¯æ’åˆ—æ“ä½œæ¥å®ç°ä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå°†å­¦ä¹ æ’åºé—®é¢˜é‡æ–°å®šä¹‰ä¸ºåäº‹å®æ•ˆç”¨çš„ç›´æ¥ä¼˜åŒ–ï¼Œé¿å…äº†ä¼ ç»Ÿæ–¹æ³•ä¸­çš„ç®€åŒ–å‡è®¾ï¼Œä»è€Œæ›´çœŸå®åœ°åæ˜ ç”¨æˆ·è¡Œä¸ºã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒRewardRankä½¿ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥æœ€å¤§åŒ–é¢„æµ‹çš„å¥–åŠ±ï¼Œå¹¶é€šè¿‡å¯å¾®åˆ†çš„æ“ä½œå®ç°äº†æ’åºçš„ä¼˜åŒ–ï¼Œç¡®ä¿äº†æ¨¡å‹çš„çµæ´»æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å®éªŒä¸­ï¼ŒRewardRankåœ¨ä¸¤ä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡å®ç°äº†æœ€é«˜çš„åäº‹å®æ•ˆç”¨ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„NDCGä¼˜åŒ–æ–¹æ³•ï¼Œæ˜¾ç¤ºå‡ºä¼˜åŒ–ç”¨æˆ·æ•ˆç”¨çš„æœ‰æ•ˆæ€§ã€‚å…·ä½“è€Œè¨€ï¼ŒRewardRankåœ¨Baidu-ULTRæ•°æ®é›†ä¸Šè®¾ç«‹äº†æ–°çš„ç¦»çº¿ç›¸å…³æ€§æ€§èƒ½è®°å½•ï¼Œè¯æ˜äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç”µå­å•†åŠ¡ã€æœç´¢å¼•æ“å’Œæ¨èç³»ç»Ÿç­‰ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡ç”¨æˆ·ä½“éªŒå’Œå•†ä¸šè½¬åŒ–ç‡ã€‚é€šè¿‡ä¼˜åŒ–çœŸå®çš„ç”¨æˆ·æ•ˆç”¨ï¼ŒRewardRankæœ‰æœ›åœ¨å®é™…åº”ç”¨ä¸­å¸¦æ¥æ˜¾è‘—çš„ç»æµæ•ˆç›Šå’Œç”¨æˆ·æ»¡æ„åº¦æå‡ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥æ‰©å±•åˆ°æ›´å¤šçš„åœ¨çº¿å­¦ä¹ å’Œä¸ªæ€§åŒ–æ¨èåœºæ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Traditional ranking systems optimize offline proxy objectives that rely on oversimplified assumptions about user behavior, often neglecting factors such as position bias and item diversity. Consequently, these models fail to improve true counterfactual utilities such as such as click-through rate or purchase probability, when evaluated in online A/B tests. We introduce RewardRank, a data-driven learning-to-rank (LTR) framework for counterfactual utility maximization. RewardRank first learns a reward model that predicts the utility of any ranking directly from logged user interactions, and then trains a ranker to maximize this reward using a differentiable soft permutation operator. To enable rigorous and reproducible evaluation, we further propose two benchmark suites: (i) Parametric Oracle Evaluation (PO-Eval), which employs an open-source click model as a counterfactual oracle on the Baidu-ULTR dataset, and (ii) LLM-as-User Evaluation (LAU-Eval), which simulates realistic user behavior via large language models on the Amazon-KDD-Cup dataset. RewardRank achieves the highest counterfactual utility across both benchmarks and demonstrates that optimizing classical metrics such as NDCG is sub-optimal for maximizing true user utility. Finally, using real user feedback from the Baidu-ULTR dataset, RewardRank establishes a new state of the art in offline relevance performance. Overall, our results show that learning-to-rank can be reformulated as direct optimization of counterfactual utility, achieved in a purely data-driven manner without relying on explicit modeling assumptions such as position bias. Our code is available at: $https://github.com/GauravBh1010tt/RewardRank$

