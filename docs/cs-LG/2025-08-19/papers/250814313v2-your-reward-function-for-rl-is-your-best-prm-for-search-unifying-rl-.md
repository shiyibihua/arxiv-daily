---
layout: default
title: Your Reward Function for RL is Your Best PRM for Search: Unifying RL and Search-Based TTS
---

# Your Reward Function for RL is Your Best PRM for Search: Unifying RL and Search-Based TTS

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.14313" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.14313v2</a>
  <a href="https://arxiv.org/pdf/2508.14313.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.14313v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.14313v2', 'Your Reward Function for RL is Your Best PRM for Search: Unifying RL and Search-Based TTS')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Can Jin, Yang Zhou, Qixin Zhang, Hongwu Peng, Di Zhang, Marco Pavone, Ligong Han, Zhang-Wei Hong, Tong Che, Dimitris N. Metaxas

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-19 (æ›´æ–°: 2025-08-22)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAIRL-Sä»¥ç»Ÿä¸€å¼ºåŒ–å­¦ä¹ ä¸åŸºäºæœç´¢çš„æµ‹è¯•æ—¶ç¼©æ”¾é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `æœç´¢ç®—æ³•` `è¿‡ç¨‹å¥–åŠ±æ¨¡å‹` `å¯¹æŠ—æ€§å­¦ä¹ ` `åŠ¨æ€ä¼˜åŒ–` `æ¨ç†èƒ½åŠ›` `å¤§å‹è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨ä¼˜åŒ–ç¨€ç–å¥–åŠ±æ—¶é¢ä¸´ä¸ç¨³å®šå’Œä½æ ·æœ¬æ•ˆç‡çš„é—®é¢˜ï¼ŒåŸºäºæœç´¢çš„æŠ€æœ¯åˆ™ä¾èµ–æ˜‚è´µçš„æ ‡ç­¾æ•°æ®ï¼Œä¸”åœ¨åˆ†å¸ƒè½¬ç§»æ—¶è¡¨ç°ä¸ä½³ã€‚
2. æœ¬æ–‡æå‡ºAIRL-Sï¼Œé€šè¿‡å¯¹æŠ—æ€§é€†å¼ºåŒ–å­¦ä¹ å’Œç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼Œç›´æ¥ä»æ­£ç¡®æ¨ç†è½¨è¿¹ä¸­å­¦ä¹ åŠ¨æ€è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼Œæ¶ˆé™¤äº†å¯¹æ ‡è®°æ•°æ®çš„éœ€æ±‚ã€‚
3. åœ¨å…«ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒAIRL-Så¹³å‡æå‡9%çš„æ€§èƒ½ï¼Œä¸”åœ¨å¤šä¸ªæœç´¢ç®—æ³•ä¸­è¡¨ç°ä¼˜äºæ‰€æœ‰åŸºäºæ ‡è®°æ•°æ®è®­ç»ƒçš„åŸºçº¿è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­é¢ä¸´ä¸¤å¤§ä¸»è¦æŒ‘æˆ˜ï¼šä¸€æ˜¯å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•ä¼˜åŒ–ç¨€ç–çš„åŸºäºç»“æœçš„å¥–åŠ±ï¼Œå¯¼è‡´ä¸ç¨³å®šå’Œä½æ ·æœ¬æ•ˆç‡ï¼›äºŒæ˜¯åŸºäºæœç´¢çš„æŠ€æœ¯ä¾èµ–ç‹¬ç«‹è®­ç»ƒçš„é™æ€è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰ï¼Œéœ€è¦æ˜‚è´µçš„äººå·¥æˆ–LLMç”Ÿæˆæ ‡ç­¾ï¼Œå¹¶åœ¨åˆ†å¸ƒè½¬ç§»ä¸‹è¡¨ç°ä¸ä½³ã€‚æœ¬æ–‡æå‡ºAIRL-Sï¼Œé¦–æ¬¡è‡ªç„¶ç»Ÿä¸€RLå’ŒåŸºäºæœç´¢çš„TTSã€‚æˆ‘ä»¬åˆ©ç”¨å¯¹æŠ—æ€§é€†å¼ºåŒ–å­¦ä¹ ï¼ˆAIRLï¼‰ç»“åˆç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œç›´æ¥ä»æ­£ç¡®æ¨ç†è½¨è¿¹ä¸­å­¦ä¹ åŠ¨æ€PRMï¼Œæ¶ˆé™¤äº†å¯¹æ ‡è®°ä¸­é—´è¿‡ç¨‹æ•°æ®çš„éœ€æ±‚ã€‚åœ¨æ¨ç†æ—¶ï¼Œç”Ÿæˆçš„PRMåŒæ—¶ä½œä¸ºRLå›æ»šçš„è¯„è®ºè€…å’Œæœ‰æ•ˆæŒ‡å¯¼æœç´¢ç¨‹åºçš„å¯å‘å¼æ–¹æ³•ï¼Œå¢å¼ºäº†è·¨ä»»åŠ¡æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å…«ä¸ªåŸºå‡†æµ‹è¯•ä¸­å¹³å‡æå‡9%ï¼Œä¸GPT-4oç›¸åŒ¹é…ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æµ‹è¯•æ—¶ç¼©æ”¾ä¸­çš„ä¸¤å¤§æŒ‘æˆ˜ï¼šå¼ºåŒ–å­¦ä¹ æ–¹æ³•çš„ä¸ç¨³å®šæ€§å’Œä½æ ·æœ¬æ•ˆç‡ï¼Œä»¥åŠåŸºäºæœç´¢çš„æŠ€æœ¯å¯¹æ ‡è®°æ•°æ®çš„ä¾èµ–å’Œåœ¨åˆ†å¸ƒè½¬ç§»ä¸‹çš„æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸­å­¦ä¹ åˆ°çš„å¥–åŠ±å‡½æ•°è§†ä¸ºæŒ‡å¯¼ä¸‹æ¸¸æœç´¢çš„ç†æƒ³è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰ï¼Œé€šè¿‡å¯¹æŠ—æ€§é€†å¼ºåŒ–å­¦ä¹ ï¼ˆAIRLï¼‰å’Œç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ˜¯é€šè¿‡AIRLå­¦ä¹ åŠ¨æ€PRMï¼Œå…¶æ¬¡æ˜¯åœ¨æ¨ç†é˜¶æ®µå°†è¯¥PRMç”¨äºå¼ºåŒ–å­¦ä¹ å›æ»šå’Œæœç´¢ç¨‹åºçš„æŒ‡å¯¼ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†RLå’ŒåŸºäºæœç´¢çš„æ–¹æ³•è‡ªç„¶ç»Ÿä¸€ï¼Œåˆ©ç”¨åŠ¨æ€PRMæ¶ˆé™¤äº†å¯¹æ ‡è®°ä¸­é—´è¿‡ç¨‹æ•°æ®çš„éœ€æ±‚ï¼Œå¢å¼ºäº†æ¨¡å‹çš„é€‚åº”æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼Œé‡‡ç”¨äº†å¯¹æŠ—æ€§å­¦ä¹ çš„æŸå¤±å‡½æ•°å’ŒåŠ¨æ€æ›´æ–°çš„ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ï¼Œç¡®ä¿PRMèƒ½å¤Ÿå®æ—¶åæ˜ æ­£ç¡®çš„æ¨ç†è½¨è¿¹ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ•´ä½“æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAIRL-Såœ¨å…«ä¸ªåŸºå‡†æµ‹è¯•ä¸­å¹³å‡æå‡9%çš„æ€§èƒ½ï¼ŒæˆåŠŸåŒ¹é…GPT-4oçš„è¡¨ç°ã€‚æ­¤å¤–ï¼Œåœ¨å¤šä¸ªæœç´¢ç®—æ³•ä¸­ï¼ŒAIRL-Sçš„åŠ¨æ€PRMå§‹ç»ˆä¼˜äºæ‰€æœ‰åŸºäºæ ‡è®°æ•°æ®è®­ç»ƒçš„åŸºçº¿PRMï¼Œå±•ç°å‡ºæ˜¾è‘—çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤æ‚æ¨ç†ä»»åŠ¡ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œè‡ªåŠ¨ä»£ç ç”Ÿæˆç­‰ã€‚é€šè¿‡æä¾›ä¸€ç§æ›´ä¸ºç¨³å¥å’Œç»æµé«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼ŒAIRL-Sèƒ½å¤Ÿåœ¨å¤šç§å®é™…åœºæ™¯ä¸­æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Test-time scaling (TTS) for large language models (LLMs) has thus far fallen into two largely separate paradigms: (1) reinforcement learning (RL) methods that optimize sparse outcome-based rewards, yet suffer from instability and low sample efficiency; and (2) search-based techniques guided by independently trained, static process reward models (PRMs), which require expensive human- or LLM-generated labels and often degrade under distribution shifts. In this paper, we introduce AIRL-S, the first natural unification of RL-based and search-based TTS. Central to AIRL-S is the insight that the reward function learned during RL training inherently represents the ideal PRM for guiding downstream search. Specifically, we leverage adversarial inverse reinforcement learning (AIRL) combined with group relative policy optimization (GRPO) to learn a dense, dynamic PRM directly from correct reasoning traces, entirely eliminating the need for labeled intermediate process data. At inference, the resulting PRM simultaneously serves as the critic for RL rollouts and as a heuristic to effectively guide search procedures, facilitating robust reasoning chain extension, mitigating reward hacking, and enhancing cross-task generalization. Experimental results across eight benchmarks, including mathematics, scientific reasoning, and code generation, demonstrate that our unified approach improves performance by 9 % on average over the base model, matching GPT-4o. Furthermore, when integrated into multiple search algorithms, our PRM consistently outperforms all baseline PRMs trained with labeled data. These results underscore that, indeed, your reward function for RL is your best PRM for search, providing a robust and cost-effective solution to complex reasoning tasks in LLMs.

