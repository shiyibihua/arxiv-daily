---
layout: default
title: Curriculum Reinforcement Learning from Easy to Hard Tasks Improves LLM Reasoning
---

# Curriculum Reinforcement Learning from Easy to Hard Tasks Improves LLM Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.06632" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.06632v2</a>
  <a href="https://arxiv.org/pdf/2506.06632.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.06632v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.06632v2', 'Curriculum Reinforcement Learning from Easy to Hard Tasks Improves LLM Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shubham Parashar, Shurui Gui, Xiner Li, Hongyi Ling, Sushil Vemuri, Blake Olson, Eric Li, Yu Zhang, James Caverlee, Dileep Kalathil, Shuiwang Ji

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-07 (æ›´æ–°: 2025-11-02)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/divelab/E2H-Reasoning)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºE2H Reasonerä»¥æå‡å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `æ¨ç†èƒ½åŠ›` `è¯¾ç¨‹å­¦ä¹ ` `å¤§è¯­è¨€æ¨¡å‹` `ä»»åŠ¡è°ƒåº¦` `æ¨¡å‹è®­ç»ƒ` `æ ·æœ¬å¤æ‚åº¦` `E2H Reasoner`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤„ç†æœ¬è´¨ä¸Šå›°éš¾çš„æ¨ç†ä»»åŠ¡æ—¶æ•ˆæœä¸ä½³ï¼Œå®¹æ˜“å¯¼è‡´æ¨¡å‹è¿‡æ‹Ÿåˆã€‚
2. æœ¬ç ”ç©¶æå‡ºE2H Reasonerï¼Œé€šè¿‡ä»ç®€å•åˆ°å›°éš¾çš„ä»»åŠ¡è°ƒåº¦ï¼Œå¸®åŠ©å¤§è¯­è¨€æ¨¡å‹é€æ­¥å»ºç«‹æ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒE2H Reasoneræ˜¾è‘—æå‡äº†å°å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œç›¸è¾ƒäºä¼ ç»ŸRLæ–¹æ³•æ•ˆæœæ›´ä½³ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶æ—¨åœ¨é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æå‡è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚å°½ç®¡ç°æœ‰çš„RLåè®­ç»ƒæ¨¡å‹å¦‚DeepSeek-R1åœ¨æ•°å­¦å’Œç¼–ç ä»»åŠ¡ä¸Šå±•ç°äº†æ¨ç†èƒ½åŠ›ï¼Œä½†å•é RLåœ¨æœ¬è´¨ä¸Šå›°éš¾çš„ä»»åŠ¡ä¸Šæå‡æ¨ç†æ•ˆæœçš„ç ”ç©¶è¾ƒå°‘ã€‚æˆ‘ä»¬å€Ÿé‰´äº†è¯¾ç¨‹å­¦ä¹ çš„ç†å¿µï¼Œæå‡ºäº†ä»ç®€å•åˆ°å›°éš¾ï¼ˆE2Hï¼‰çš„ä»»åŠ¡è°ƒåº¦æ–¹æ³•ï¼Œä½¿å¤§è¯­è¨€æ¨¡å‹èƒ½å¤Ÿé€æ­¥å»ºç«‹æ¨ç†æŠ€èƒ½ã€‚é€šè¿‡å®éªŒè¯æ˜ï¼ŒE2H Reasoneræ˜¾è‘—æå‡äº†å°å‹è¯­è¨€æ¨¡å‹ï¼ˆ1.5Båˆ°3Bï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨ä»…ä½¿ç”¨ä¼ ç»ŸRLè®­ç»ƒæ—¶è¡¨ç°ä¸ä½³çš„æƒ…å†µä¸‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨æ¨ç†ä»»åŠ¡ä¸­æ•ˆæœä¸ä½³çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨é¢å¯¹å¤æ‚ä»»åŠ¡æ—¶ï¼Œæ¨¡å‹å®¹æ˜“è¿‡æ‹Ÿåˆï¼Œå¯¼è‡´æ¨ç†èƒ½åŠ›ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºE2H Reasonerï¼Œé€šè¿‡è¯¾ç¨‹å­¦ä¹ çš„ç†å¿µï¼Œä»ç®€å•ä»»åŠ¡é€æ­¥è¿‡æ¸¡åˆ°å›°éš¾ä»»åŠ¡ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿé€æ­¥æŒæ¡æ¨ç†æŠ€èƒ½ã€‚è¿™ç§é€æ­¥å­¦ä¹ çš„æ–¹å¼æœ‰åŠ©äºæ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸Šæ›´å¥½åœ°è¿›è¡Œæ¨ç†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šE2H Reasonerçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä»»åŠ¡è°ƒåº¦æ¨¡å—å’Œå¼ºåŒ–å­¦ä¹ æ¨¡å—ã€‚ä»»åŠ¡è°ƒåº¦æ¨¡å—è´Ÿè´£æ ¹æ®æ¨¡å‹çš„å­¦ä¹ è¿›åº¦è°ƒæ•´ä»»åŠ¡éš¾åº¦ï¼Œè€Œå¼ºåŒ–å­¦ä¹ æ¨¡å—åˆ™ç”¨äºä¼˜åŒ–æ¨¡å‹çš„æ¨ç†ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„ä¸»è¦åˆ›æ–°åœ¨äºæå‡ºäº†E2Hçš„ä»»åŠ¡è°ƒåº¦ç­–ç•¥ï¼Œå¼ºè°ƒäº†åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­é€æ­¥å¢åŠ ä»»åŠ¡éš¾åº¦çš„é‡è¦æ€§ã€‚è¿™ä¸ä¼ ç»Ÿçš„ç›´æ¥å­¦ä¹ æ–¹æ³•å½¢æˆäº†é²œæ˜å¯¹æ¯”ï¼Œåè€…å¾€å¾€å¿½è§†äº†ä»»åŠ¡éš¾åº¦çš„æ¸è¿›æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œæˆ‘ä»¬è®¾ç½®äº†é€‚å½“çš„ä»»åŠ¡éš¾åº¦è¡°å‡ç­–ç•¥ï¼Œä»¥é˜²æ­¢æ¨¡å‹è¿‡æ‹Ÿåˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åœ¨ç†è®ºä¸Šå»ºç«‹äº†E2H Reasonerçš„æ”¶æ•›æ€§ä¿è¯ï¼Œå¹¶æ¨å¯¼äº†æœ‰é™æ ·æœ¬å¤æ‚åº¦ç•Œé™ï¼Œç¡®ä¿åœ¨é€‚å½“çš„ä»»åŠ¡åˆ†è§£å’Œæ¡ä»¶ä¸‹ï¼Œè¯¾ç¨‹å­¦ä¹ é˜¶æ®µæ‰€éœ€çš„æ ·æœ¬æ€»æ•°å°‘äºç›´æ¥å­¦ä¹ ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒE2H Reasoneråœ¨å¤šä¸ªé¢†åŸŸçš„æ¨ç†èƒ½åŠ›ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚å…·ä½“è€Œè¨€ï¼Œå°å‹è¯­è¨€æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°æå‡å¹…åº¦è¾¾åˆ°äº†XX%ï¼Œæœ‰æ•ˆè§£å†³äº†åœ¨ä»…ä½¿ç”¨ä¼ ç»ŸRLè®­ç»ƒæ—¶çš„æ€§èƒ½ç“¶é¢ˆã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€è‡ªåŠ¨åŒ–æ¨ç†ç³»ç»Ÿå’Œæ™ºèƒ½åŠ©æ‰‹ç­‰ã€‚é€šè¿‡æå‡å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼ŒE2H Reasonerå¯ä»¥åœ¨å¤æ‚é—®é¢˜æ±‚è§£ã€ç¼–ç¨‹è¾…åŠ©å’Œæ™ºèƒ½é—®ç­”ç­‰åœºæ™¯ä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œæœªæ¥å¯èƒ½å¯¹äººæœºäº¤äº’å’Œæ™ºèƒ½å†³ç­–äº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We aim to improve the reasoning capabilities of language models via reinforcement learning (RL). Recent RL post-trained models like DeepSeek-R1 have demonstrated reasoning abilities on mathematical and coding tasks. However, prior studies suggest that using RL alone to improve reasoning on inherently difficult tasks is less effective. Here, we draw inspiration from curriculum learning and propose to schedule tasks from easy to hard (E2H), allowing LLMs to build reasoning skills gradually. Our method is termed E2H Reasoner. Empirically, we observe that, although easy tasks are important initially, fading them out through appropriate scheduling is essential in preventing overfitting. Theoretically, we establish convergence guarantees for E2H Reasoner within an approximate policy iteration framework. We derive finite-sample complexity bounds and show that when tasks are appropriately decomposed and conditioned, learning through curriculum stages requires fewer total samples than direct learning. Experiments across multiple domains show that E2H Reasoner significantly improves the reasoning ability of small LLMs (1.5B to 3B), which otherwise struggle when trained with vanilla RL alone, highlighting the effectiveness of our method. Our code can be found on https://github.com/divelab/E2H-Reasoning.

