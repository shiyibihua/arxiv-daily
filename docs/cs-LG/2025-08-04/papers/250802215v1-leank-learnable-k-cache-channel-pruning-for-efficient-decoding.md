---
layout: default
title: LeanK: Learnable K Cache Channel Pruning for Efficient Decoding
---

# LeanK: Learnable K Cache Channel Pruning for Efficient Decoding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.02215" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.02215v1</a>
  <a href="https://arxiv.org/pdf/2508.02215.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.02215v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.02215v1', 'LeanK: Learnable K Cache Channel Pruning for Efficient Decoding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yike Zhang, Zhiyuan He, Huiqiang Jiang, Chengruidong Zhang, Yuqing Yang, Jianyong Wang, Lili Qiu

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-04

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLeanKä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹è§£ç æ•ˆç‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿ä¸Šä¸‹æ–‡å¤„ç†` `é”®å€¼ç¼“å­˜` `é€šé“ç¨€ç–æ€§` `è§£ç ä¼˜åŒ–` `å¤§è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤§è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡æ—¶ï¼Œå› é”®å€¼ç¼“å­˜çš„å¢é•¿è€Œé¢ä¸´æ˜¾è‘—çš„æ•ˆç‡é—®é¢˜ã€‚
2. LeanKé€šè¿‡å­¦ä¹ ä¸é‡è¦çš„é”®ç¼“å­˜é€šé“ï¼Œåˆ©ç”¨é™æ€é€šé“ç¨€ç–æ€§æ¥ä¼˜åŒ–è§£ç è¿‡ç¨‹ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒLeanKèƒ½å¤Ÿå®ç°Kç¼“å­˜æœ€å¤šå‡å°‘70%ï¼Œå¹¶åŠ é€Ÿæ³¨æ„åŠ›è®¡ç®—1.3å€ï¼Œä¸”ä¸å½±å“æ¨¡å‹å‡†ç¡®æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡æ—¶é¢ä¸´æ•ˆç‡æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯éšç€é”®å€¼ï¼ˆKVï¼‰ç¼“å­˜çš„å¢é•¿ã€‚æœ¬æ–‡æå‡ºLeanKï¼Œä¸€ç§åŸºäºå­¦ä¹ çš„æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨é™æ€é€šé“ç¨€ç–æ€§æ¥ä¿®å‰ªä¸é‡è¦çš„é”®ï¼ˆKï¼‰ç¼“å­˜é€šé“ã€‚LeanKé‡‡ç”¨æ–°é¢–çš„ä¸¤é˜¶æ®µè®­ç»ƒè¿‡ç¨‹ï¼Œå­¦ä¹ é€šé“çº§é™æ€æ©ç ï¼Œä»¥æ»¡è¶³ç‰¹å®šçš„ç¨€ç–ç‡å’Œç¡¬ä»¶å¯¹é½è¦æ±‚ã€‚LeanKåœ¨ä¸ç‰ºç‰²å‡†ç¡®æ€§çš„æƒ…å†µä¸‹ï¼Œå‡å°‘äº†GPUå†…å­˜å¹¶åŠ é€Ÿäº†è§£ç è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒKç¼“å­˜å‡å°‘äº†å¤šè¾¾70%ï¼ŒVç¼“å­˜å‡å°‘äº†16%-18%ã€‚å®šåˆ¶è§£ç å†…æ ¸ä½¿å¾—æ³¨æ„åŠ›è®¡ç®—åŠ é€Ÿè¾¾åˆ°1.3å€ã€‚æˆ‘ä»¬è¿˜é€šè¿‡åˆ†æå­¦ä¹ åˆ°çš„é‡è¦æ€§åˆ†å¸ƒï¼Œæä¾›äº†å¯¹é•¿ä¸Šä¸‹æ–‡æ¨ç†ä¸­æ¨¡å‹é€šé“å’Œæ³¨æ„åŠ›å¤´çš„æ·±å…¥è§è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸­å› é”®å€¼ç¼“å­˜å¢é•¿å¯¼è‡´çš„è§£ç æ•ˆç‡ä½ä¸‹é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤§è§„æ¨¡æ•°æ®æ—¶ï¼Œå¾€å¾€æ— æ³•æœ‰æ•ˆåˆ©ç”¨å†…å­˜å’Œè®¡ç®—èµ„æºï¼Œå¯¼è‡´æ€§èƒ½ç“¶é¢ˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šLeanKçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å­¦ä¹ ä¸é‡è¦çš„é”®ç¼“å­˜é€šé“ï¼Œåˆ©ç”¨é™æ€é€šé“ç¨€ç–æ€§æ¥ä¼˜åŒ–è§£ç è¿‡ç¨‹ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒLeanKèƒ½å¤Ÿåœ¨ä¿æŒæ¨¡å‹å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œæ˜¾è‘—å‡å°‘å†…å­˜å ç”¨å’ŒåŠ é€Ÿè®¡ç®—ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLeanKçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µæ˜¯å­¦ä¹ é€šé“çº§é™æ€æ©ç ï¼Œä»¥æ»¡è¶³ç‰¹å®šçš„ç¨€ç–ç‡ï¼›ç¬¬äºŒé˜¶æ®µæ˜¯æ ¹æ®å­¦ä¹ åˆ°çš„æ©ç è¿›è¡Œè§£ç ä¼˜åŒ–ã€‚è¯¥æ–¹æ³•ç»“åˆäº†é™æ€é€šé“ç¨€ç–æ€§ä¸ç¡¬ä»¶å¯¹é½éœ€æ±‚ï¼Œç¡®ä¿äº†é«˜æ•ˆçš„èµ„æºåˆ©ç”¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šLeanKçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶å­¦ä¹ æœºåˆ¶ï¼Œé€šè¿‡é™æ€æ©ç çš„æ–¹å¼å®ç°äº†é€šé“çš„æœ‰æ•ˆä¿®å‰ªã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„æ‰‹åŠ¨ç¨€ç–åŒ–æ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´çµæ´»åœ°é€‚åº”ä¸åŒçš„ç¡¬ä»¶ç¯å¢ƒå’Œä»»åŠ¡éœ€æ±‚ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒLeanKé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–é€šé“æ©ç çš„å­¦ä¹ è¿‡ç¨‹ï¼Œå¹¶åœ¨ç½‘ç»œç»“æ„ä¸­å¼•å…¥äº†å®šåˆ¶çš„è§£ç å†…æ ¸ï¼Œä»¥æå‡æ³¨æ„åŠ›è®¡ç®—çš„æ•ˆç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLeanKåœ¨Kç¼“å­˜æ–¹é¢å®ç°äº†æœ€é«˜70%çš„å†…å­˜å‡å°‘ï¼ŒåŒæ—¶Vç¼“å­˜ä¹Ÿå‡å°‘äº†16%-18%ã€‚æ­¤å¤–ï¼Œå®šåˆ¶çš„è§£ç å†…æ ¸ä½¿å¾—æ³¨æ„åŠ›è®¡ç®—é€Ÿåº¦æå‡äº†1.3å€ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„è§£ç æ•ˆç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

LeanKçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¤„ç†é•¿æ–‡æœ¬æˆ–é•¿åºåˆ—æ•°æ®çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­ã€‚é€šè¿‡æé«˜è§£ç æ•ˆç‡ï¼ŒLeanKèƒ½å¤Ÿå¸®åŠ©å¼€å‘æ›´é«˜æ•ˆçš„å¯¹è¯ç³»ç»Ÿã€æ–‡æœ¬ç”Ÿæˆæ¨¡å‹å’Œæœºå™¨ç¿»è¯‘ç³»ç»Ÿï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å®é™…åº”ç”¨å’Œå‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) enable long-context tasks but face efficiency challenges due to the growing key-value (KV) cache. We propose LeanK, a learning-based method that prunes unimportant key (K) cache channels by leveraging static channel sparsity. With a novel two-stage training process, LeanK learns channel-wise static mask that could satisfy specific sparsity ratio and hardware alignment requirement. LeanK reduces GPU memory and accelerates decoding without sacrificing accuracy. Experiments demonstrate up to 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel enables 1.3x speedup for attention computation. We also provide insights into model channels and attention heads during long-context inference by analyzing the learned importance distribution. Our code is available at https://aka.ms/LeanK.

