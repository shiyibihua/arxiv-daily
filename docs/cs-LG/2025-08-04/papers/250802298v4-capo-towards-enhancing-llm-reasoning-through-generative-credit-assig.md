---
layout: default
title: CAPO: Towards Enhancing LLM Reasoning through Generative Credit Assignment
---

# CAPO: Towards Enhancing LLM Reasoning through Generative Credit Assignment

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.02298" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.02298v4</a>
  <a href="https://arxiv.org/pdf/2508.02298.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.02298v4" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.02298v4', 'CAPO: Towards Enhancing LLM Reasoning through Generative Credit Assignment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Guofu Xie, Yunsheng Shi, Hongtao Tian, Ting Yao, Xiao Zhang

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-04 (æ›´æ–°: 2025-10-20)

**å¤‡æ³¨**: Work in progress

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCAPOä»¥è§£å†³LLMæ¨ç†ä¸­çš„å¥–åŠ±åˆ†é…é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `å¥–åŠ±åˆ†é…` `æ¨ç†èƒ½åŠ›` `ç”Ÿæˆæ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„RLVRæ–¹æ³•å¯¹æ¯ä¸ªtokenåˆ†é…ç›¸åŒå¥–åŠ±ï¼Œå¯¼è‡´ç²¾ç¡®ä¿¡ç”¨åˆ†é…å›°éš¾ï¼Œå½±å“æ¨¡å‹æ¨ç†èƒ½åŠ›ã€‚
2. CAPOåˆ©ç”¨é€šç”¨LLMä½œä¸ºç”Ÿæˆè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼Œç›´æ¥ç”Ÿæˆæ­¥éª¤æ‰¹è¯„ï¼Œæä¾›ç¡®å®šæ€§tokençº§åˆ«ä¿¡ç”¨ã€‚
3. åœ¨å¤šä¸ªæ•°å­¦åŸºå‡†å’Œé¢†åŸŸå¤–åŸºå‡†ä¸Šï¼ŒCAPOåœ¨æ€§èƒ½ä¸Šä¼˜äºä¼ ç»Ÿçš„ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ å¾®è°ƒæ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰é€šè¿‡åŸºäºè§„åˆ™çš„äºŒå…ƒåé¦ˆæå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰RLVRæ–¹æ³•é€šå¸¸å¯¹æ¯ä¸ªtokenåˆ†é…ç›¸åŒçš„å¥–åŠ±ï¼Œè¿™ç§ç²—ç²’åº¦åé¦ˆå¦¨ç¢äº†ç²¾ç¡®çš„ä¿¡ç”¨åˆ†é…ï¼Œä½¿å¾—æ¨¡å‹éš¾ä»¥è¯†åˆ«æˆåŠŸæˆ–å¤±è´¥çš„æ¨ç†æ­¥éª¤ï¼Œå¸¸å¯¼è‡´æ¬¡ä¼˜ç­–ç•¥ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•è€Œé«˜æ•ˆçš„æ–¹æ³•â€”â€”ä¿¡ç”¨åˆ†é…ç­–ç•¥ä¼˜åŒ–ï¼ˆCAPOï¼‰ã€‚CAPOç›´æ¥åˆ©ç”¨ç°æˆçš„é€šç”¨LLMä½œä¸ºç”Ÿæˆè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆLLM-as-GenPRMï¼‰ï¼Œé€šè¿‡ä¸€æ¬¡æ€§ç”Ÿæˆæ‰€æœ‰æ­¥éª¤çš„æ‰¹è¯„ï¼Œæä¾›ç¡®å®šæ€§çš„tokençº§åˆ«ä¿¡ç”¨ã€‚å®éªŒè¡¨æ˜ï¼ŒCAPOåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºåŸºäºç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ çš„å¾®è°ƒæ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰RLVRæ–¹æ³•ä¸­ç²—ç²’åº¦å¥–åŠ±åˆ†é…çš„é—®é¢˜ï¼Œå¯¼è‡´æ¨¡å‹éš¾ä»¥è¯†åˆ«æ¨ç†æ­¥éª¤çš„æˆåŠŸä¸å¤±è´¥ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCAPOé€šè¿‡åˆ©ç”¨ç°æˆçš„LLMä½œä¸ºç”Ÿæˆè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼Œç›´æ¥ç”Ÿæˆæ¯ä¸ªæ­¥éª¤çš„æ‰¹è¯„ï¼Œä»è€Œå®ç°ç²¾ç¡®çš„tokençº§åˆ«ä¿¡ç”¨åˆ†é…ã€‚è¿™æ ·çš„è®¾è®¡é¿å…äº†è®­ç»ƒè¾…åŠ©æ¨¡å‹çš„å¤æ‚æ€§ï¼Œå¹¶æé«˜äº†åé¦ˆçš„å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCAPOçš„æ•´ä½“æ¶æ„åŒ…æ‹¬è¾“å…¥LLMç”Ÿæˆçš„æ­¥éª¤æ‰¹è¯„ï¼ŒåŸºäºæ¯ä¸ªæ­¥éª¤çš„æ­£ç¡®æ€§è¿›è¡Œä¿¡ç”¨åˆ†é…ï¼Œæœ€åé€šè¿‡æŠ•ç¥¨æœºåˆ¶å¢å¼ºåé¦ˆçš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šCAPOçš„ä¸»è¦åˆ›æ–°åœ¨äºå°†LLMä½œä¸ºç”Ÿæˆè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼Œæä¾›ç¡®å®šæ€§çš„åé¦ˆï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œé¿å…äº†å¯¹é«˜è´¨é‡ç›‘ç£æ ‡ç­¾çš„ä¾èµ–ï¼Œå¹¶æé«˜äº†åœ¨çº¿å¼ºåŒ–å­¦ä¹ çš„æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒCAPOé‡‡ç”¨äº†æŠ•ç¥¨æœºåˆ¶æ¥å¤„ç†ç”Ÿæˆçš„æ‰¹è¯„ï¼Œç¡®ä¿åé¦ˆçš„å¯é æ€§ã€‚æ­¤å¤–ï¼Œæ¨¡å‹çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡ä¹Ÿç»è¿‡ç²¾å¿ƒè°ƒæ•´ï¼Œä»¥ä¼˜åŒ–å­¦ä¹ è¿‡ç¨‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¤šä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ä¸­ï¼ŒCAPOç›¸è¾ƒäºä¼ ç»Ÿçš„ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ å¾®è°ƒæ–¹æ³•ï¼Œè¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå…·ä½“å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCAPOåœ¨å››ä¸ªæŒ‘æˆ˜æ€§æ•°å­¦åŸºå‡†ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæå‡å¹…åº¦è¾¾åˆ°XX%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

CAPOçš„ç ”ç©¶æˆæœåœ¨æ•™è‚²ã€è‡ªåŠ¨åŒ–æ¨ç†å’Œæ™ºèƒ½é—®ç­”ç³»ç»Ÿç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æå‡LLMçš„æ¨ç†èƒ½åŠ›ï¼Œè¯¥æ–¹æ³•å¯ä»¥å¸®åŠ©å¼€å‘æ›´æ™ºèƒ½çš„å¯¹è¯ç³»ç»Ÿå’Œè‡ªåŠ¨åŒ–å†³ç­–æ”¯æŒå·¥å…·ï¼Œæ¨åŠ¨äººå·¥æ™ºèƒ½æŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement Learning with Verifiable Rewards (RLVR) has improved the reasoning abilities of Large Language Models (LLMs) by using rule-based binary feedback. However, current RLVR methods typically assign the same reward to every token. This coarse-grained feedback hampers precise credit assignment, making it hard for models to identify which reasoning steps lead to success or failure, and often results in suboptimal policies. Methods like PPO provide credit assignment by value estimation, but yield inaccurate and unverifiable signals due to limited sampling. On the other hand, methods using Process Reward Models can provide step-wise rewards but suffer from several key limitations: they require high-quality process supervision labels, the feedback is unreliable due to probabilistic reward modeling, and their application in online reinforcement learning (RL) is time-consuming. To overcome these limitations, we introduce a simple but efficient method-Credit Assignment Policy Optimization (CAPO). Instead of training auxiliary models, CAPO directly leverages an off-the-shelf, general-purpose LLM as a Generative Process Reward Model (LLM-as-GenPRM) to generate all step-wise critique by one pass only based on the correctness of the step itself, providing deterministic token-level credits to refine the tokens that were originally assigned identical rule-based rewards. To further enhance the accuracy and robustness, we employ voting mechanisms that scale with the number of generated critiques. Extensive experiments on various backbones like Llama and Qwen models show that CAPO consistently outperforms supervised learning-based and RL-based fine-tuning methods across four challenging mathematical benchmarks and three out-of-domain benchmarks. Further analysis shows that CAPO can help the model to foster the learning of correct reasoning pathways leading to correct answers.

