---
layout: default
title: Language Model Guided Reinforcement Learning in Quantitative Trading
---

# Language Model Guided Reinforcement Learning in Quantitative Trading

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.02366" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.02366v3</a>
  <a href="https://arxiv.org/pdf/2508.02366.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.02366v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.02366v3', 'Language Model Guided Reinforcement Learning in Quantitative Trading')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Adam Darmanin, Vince Vella

**åˆ†ç±»**: cs.LG, cs.CL, q-fin.TR

**å‘å¸ƒæ—¥æœŸ**: 2025-08-04 (æ›´æ–°: 2025-10-25)

**å¤‡æ³¨**: 12 pages (4 pages appendix and references) and 6 figures. Accepted for presentation at FLLM 2025, Vienna

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè¯­è¨€æ¨¡å‹å¼•å¯¼çš„å¼ºåŒ–å­¦ä¹ ä»¥ä¼˜åŒ–é‡åŒ–äº¤æ˜“ç­–ç•¥**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é‡åŒ–äº¤æ˜“` `å¼ºåŒ–å­¦ä¹ ` `è¯­è¨€æ¨¡å‹` `ç­–ç•¥ç”Ÿæˆ` `é£é™©ç®¡ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨é‡åŒ–äº¤æ˜“ä¸­é¢ä¸´çŸ­è§†è¡Œä¸ºå’Œå†³ç­–ä¸é€æ˜çš„é—®é¢˜ï¼Œé™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ··åˆæ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆé«˜å±‚æ¬¡äº¤æ˜“ç­–ç•¥ï¼Œä»è€Œå¼•å¯¼å¼ºåŒ–å­¦ä¹ ä»£ç†è¿›è¡Œå†³ç­–ã€‚
3. å®éªŒè¯æ˜ï¼ŒLLMå¼•å¯¼çš„ä»£ç†åœ¨æ”¶ç›Šå’Œé£é™©æŒ‡æ ‡ä¸Šç›¸è¾ƒäºæœªå¼•å¯¼çš„RLåŸºçº¿æœ‰æ˜¾è‘—æå‡ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç®—æ³•äº¤æ˜“éœ€è¦åœ¨çŸ­æœŸæˆ˜æœ¯å†³ç­–ä¸é•¿æœŸè´¢åŠ¡ç›®æ ‡ä¹‹é—´ä¿æŒä¸€è‡´ã€‚å°½ç®¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²è¢«åº”ç”¨äºæ­¤ç±»é—®é¢˜ï¼Œä½†ç”±äºçŸ­è§†è¡Œä¸ºå’Œä¸é€æ˜æ”¿ç­–ï¼Œå…¶é‡‡ç”¨å—åˆ°é™åˆ¶ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç»è¿‡è‰¯å¥½ç»“æ„åŒ–æç¤ºçš„å¼•å¯¼ä¸‹ï¼Œæä¾›äº†äº’è¡¥çš„æˆ˜ç•¥æ¨ç†å’Œå¤šæ¨¡æ€ä¿¡å·è§£é‡Šã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ··åˆæ¡†æ¶ï¼Œåˆ©ç”¨LLMsç”Ÿæˆé«˜å±‚æ¬¡äº¤æ˜“ç­–ç•¥ä»¥æŒ‡å¯¼RLä»£ç†ã€‚æˆ‘ä»¬è¯„ä¼°äº†LLMç”Ÿæˆç­–ç•¥çš„ç»æµåˆç†æ€§ï¼Œå¹¶é€šè¿‡å¤æ™®æ¯”ç‡ï¼ˆSRï¼‰å’Œæœ€å¤§å›æ’¤ï¼ˆMDDï¼‰æ¯”è¾ƒLLMå¼•å¯¼çš„ä»£ç†ä¸æœªå¼•å¯¼çš„RLåŸºçº¿çš„è¡¨ç°ã€‚å®è¯ç»“æœè¡¨æ˜ï¼ŒLLMå¼•å¯¼æ˜¾è‘—æ”¹å–„äº†ç›¸å¯¹äºæ ‡å‡†RLçš„æ”¶ç›Šå’Œé£é™©æŒ‡æ ‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³é‡åŒ–äº¤æ˜“ä¸­å¼ºåŒ–å­¦ä¹ æ–¹æ³•çš„çŸ­è§†è¡Œä¸ºå’Œå†³ç­–ä¸é€æ˜æ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æœ‰æ•ˆç»“åˆçŸ­æœŸå†³ç­–ä¸é•¿æœŸç›®æ ‡ï¼Œå¯¼è‡´ç­–ç•¥æ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆé«˜å±‚æ¬¡çš„äº¤æ˜“ç­–ç•¥ï¼Œä»¥æ­¤å¼•å¯¼å¼ºåŒ–å­¦ä¹ ä»£ç†çš„å†³ç­–è¿‡ç¨‹ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨åˆ©ç”¨LLMsçš„æˆ˜ç•¥æ¨ç†èƒ½åŠ›ï¼Œæå‡RLåœ¨å¤æ‚é‡‘èç¯å¢ƒä¸­çš„è¡¨ç°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼ŒLLMsæ ¹æ®å¸‚åœºæ•°æ®å’Œé¢„è®¾æç¤ºç”Ÿæˆäº¤æ˜“ç­–ç•¥ï¼›å…¶æ¬¡ï¼ŒRLä»£ç†æ ¹æ®è¿™äº›ç­–ç•¥è¿›è¡Œå†³ç­–å’Œæ‰§è¡Œã€‚æ•´ä¸ªæµç¨‹é€šè¿‡åé¦ˆæœºåˆ¶ä¸æ–­ä¼˜åŒ–ç­–ç•¥å’Œå†³ç­–ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå°†LLMsä¸RLç›¸ç»“åˆï¼Œå½¢æˆäº†ä¸€ç§æ–°å‹çš„å†³ç­–æ”¯æŒç³»ç»Ÿã€‚è¿™ä¸€æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºé€šè¿‡é«˜å±‚æ¬¡çš„ç­–ç•¥å¼•å¯¼ï¼Œå…‹æœäº†ä¼ ç»ŸRLçš„çŸ­è§†é—®é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸Šï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æç¤ºç»“æ„ä»¥å¼•å¯¼LLMsç”Ÿæˆæœ‰æ•ˆç­–ç•¥ï¼ŒåŒæ—¶åœ¨RLè®­ç»ƒä¸­å¼•å…¥äº†åŸºäºLLMè¾“å‡ºçš„å¥–åŠ±æœºåˆ¶ï¼Œä»¥ç¡®ä¿ç­–ç•¥çš„ç»æµåˆç†æ€§å’Œæ‰§è¡Œæ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLLMå¼•å¯¼çš„ä»£ç†åœ¨å¤æ™®æ¯”ç‡ï¼ˆSRï¼‰å’Œæœ€å¤§å›æ’¤ï¼ˆMDDï¼‰æ–¹é¢å‡ä¼˜äºæœªå¼•å¯¼çš„RLåŸºçº¿ï¼Œå…·ä½“æå‡å¹…åº¦åœ¨20%ä»¥ä¸Šã€‚è¿™è¡¨æ˜LLMçš„å¼•å¯¼æ˜¾è‘—æ”¹å–„äº†äº¤æ˜“ç­–ç•¥çš„é£é™©æ”¶ç›Šç‰¹æ€§ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬é‡‘èå¸‚åœºçš„é‡åŒ–äº¤æ˜“ã€æŠ•èµ„ç»„åˆç®¡ç†ä»¥åŠé£é™©æ§åˆ¶ç­‰ã€‚é€šè¿‡å¼•å…¥è¯­è¨€æ¨¡å‹çš„æˆ˜ç•¥æ¨ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿå¸®åŠ©äº¤æ˜“è€…åœ¨å¤æ‚å¸‚åœºç¯å¢ƒä¸­åšå‡ºæ›´ä¸ºç²¾å‡†çš„å†³ç­–ï¼Œæå‡äº¤æ˜“æ•ˆç‡å’Œæ”¶ç›Šã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½ä¼šåœ¨æ›´å¹¿æ³›çš„é‡‘èç§‘æŠ€é¢†åŸŸäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Algorithmic trading requires short-term tactical decisions consistent with long-term financial objectives. Reinforcement Learning (RL) has been applied to such problems, but adoption is limited by myopic behaviour and opaque policies. Large Language Models (LLMs) offer complementary strategic reasoning and multi-modal signal interpretation when guided by well-structured prompts. This paper proposes a hybrid framework in which LLMs generate high-level trading strategies to guide RL agents. We evaluate (i) the economic rationale of LLM-generated strategies through expert review, and (ii) the performance of LLM-guided agents against unguided RL baselines using Sharpe Ratio (SR) and Maximum Drawdown (MDD). Empirical results indicate that LLM guidance improves both return and risk metrics relative to standard RL.

