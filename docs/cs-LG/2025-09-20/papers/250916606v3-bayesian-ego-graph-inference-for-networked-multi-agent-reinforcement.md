---
layout: default
title: Bayesian Ego-graph Inference for Networked Multi-Agent Reinforcement Learning
---

# Bayesian Ego-graph Inference for Networked Multi-Agent Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.16606" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.16606v3</a>
  <a href="https://arxiv.org/pdf/2509.16606.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.16606v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.16606v3', 'Bayesian Ego-graph Inference for Networked Multi-Agent Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Wei Duan, Jie Lu, Junyu Xuan

**åˆ†ç±»**: cs.MA, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-20 (æ›´æ–°: 2025-12-16)

**å¤‡æ³¨**: Accepted at NeurIPS 2025 https://openreview.net/forum?id=3qeTs05bRL

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºBayesGï¼Œé€šè¿‡è´å¶æ–¯æ¨æ–­å­¦ä¹ ç¨€ç–äº¤äº’ç»“æ„ï¼Œè§£å†³ç½‘ç»œåŒ–å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ` `å›¾ç¥ç»ç½‘ç»œ` `è´å¶æ–¯æ¨æ–­` `å˜åˆ†æ¨æ–­` `åŠ¨æ€å›¾` `å»ä¸­å¿ƒåŒ–` `äº¤é€šæ§åˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰Networked-MARLæ–¹æ³•å‡è®¾é™æ€é‚»åŸŸï¼Œéš¾ä»¥é€‚åº”åŠ¨æ€ç¯å¢ƒï¼Œä¸­å¿ƒåŒ–æ–¹æ³•ä¾èµ–å…¨å±€ä¿¡æ¯ï¼Œä¸é€‚ç”¨äºå»ä¸­å¿ƒåŒ–ç³»ç»Ÿã€‚
2. BayesGé€šè¿‡è´å¶æ–¯å˜åˆ†æ¨æ–­å­¦ä¹ ç¨€ç–çš„ã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„äº¤äº’ç»“æ„ï¼Œæ¯ä¸ªæ™ºèƒ½ä½“åŸºäºé‡‡æ ·å­å›¾è¿›è¡Œå†³ç­–ã€‚
3. BayesGåœ¨äº¤é€šæ§åˆ¶ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ™ºèƒ½ä½“æ•°é‡é«˜è¾¾167ä¸ªï¼Œè¯æ˜äº†å…¶å¯æ‰©å±•æ€§ã€æ•ˆç‡å’Œæ€§èƒ½ä¼˜åŠ¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨ç½‘ç»œåŒ–å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆNetworked-MARLï¼‰ä¸­ï¼Œå»ä¸­å¿ƒåŒ–çš„æ™ºèƒ½ä½“å¿…é¡»åœ¨å±€éƒ¨å¯è§‚æµ‹æ€§å’Œå›ºå®šç‰©ç†å›¾ä¸Šçš„å—é™é€šä¿¡ä¸‹è¡ŒåŠ¨ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸å‡è®¾é™æ€é‚»åŸŸï¼Œé™åˆ¶äº†å¯¹åŠ¨æ€æˆ–å¼‚æ„ç¯å¢ƒçš„é€‚åº”æ€§ã€‚è™½ç„¶ä¸­å¿ƒåŒ–æ¡†æ¶å¯ä»¥å­¦ä¹ åŠ¨æ€å›¾ï¼Œä½†å®ƒä»¬ä¾èµ–äºå…¨å±€çŠ¶æ€è®¿é—®å’Œä¸­å¿ƒåŒ–åŸºç¡€è®¾æ–½ï¼Œè¿™åœ¨å®é™…çš„å»ä¸­å¿ƒåŒ–ç³»ç»Ÿä¸­æ˜¯ä¸åˆ‡å®é™…çš„ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºéšæœºå›¾ç­–ç•¥çš„Networked-MARLæ–¹æ³•ï¼Œå…¶ä¸­æ¯ä¸ªæ™ºèƒ½ä½“æ ¹æ®å…¶å±€éƒ¨ç‰©ç†é‚»åŸŸä¸Šçš„é‡‡æ ·å­å›¾æ¥å†³å®šå…¶å†³ç­–ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¼•å…¥äº†BayesGï¼Œä¸€ä¸ªå»ä¸­å¿ƒåŒ–çš„actoræ¡†æ¶ï¼Œå®ƒé€šè¿‡è´å¶æ–¯å˜åˆ†æ¨æ–­å­¦ä¹ ç¨€ç–çš„ã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„äº¤äº’ç»“æ„ã€‚æ¯ä¸ªæ™ºèƒ½ä½“åœ¨å…¶è‡ªæˆ‘å›¾ä¸Šè¿è¡Œï¼Œå¹¶é‡‡æ ·ä¸€ä¸ªæ½œåœ¨çš„é€šä¿¡æ©ç æ¥æŒ‡å¯¼æ¶ˆæ¯ä¼ é€’å’Œç­–ç•¥è®¡ç®—ã€‚å˜åˆ†åˆ†å¸ƒä½¿ç”¨è¯æ®ä¸‹ç•Œï¼ˆELBOï¼‰ç›®æ ‡ä¸ç­–ç•¥ä¸€èµ·è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿè”åˆå­¦ä¹ äº¤äº’æ‹“æ‰‘å’Œå†³ç­–ç­–ç•¥ã€‚BayesGåœ¨å…·æœ‰å¤šè¾¾167ä¸ªæ™ºèƒ½ä½“çš„å¤§è§„æ¨¡äº¤é€šæ§åˆ¶ä»»åŠ¡ä¸­ä¼˜äºå¼ºå¤§çš„MARLåŸºçº¿ï¼Œè¯æ˜äº†å…¶å“è¶Šçš„å¯æ‰©å±•æ€§ã€æ•ˆç‡å’Œæ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ç½‘ç»œåŒ–å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆNetworked-MARLï¼‰ä¸­ï¼Œæ™ºèƒ½ä½“å¦‚ä½•åœ¨å±€éƒ¨è§‚æµ‹å’Œå—é™é€šä¿¡æ¡ä»¶ä¸‹ï¼Œå­¦ä¹ åŠ¨æ€å˜åŒ–çš„äº¤äº’ç»“æ„çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ç—›ç‚¹åœ¨äºä¾èµ–é™æ€é‚»åŸŸå‡è®¾ï¼Œæ— æ³•æœ‰æ•ˆé€‚åº”åŠ¨æ€æˆ–å¼‚æ„ç¯å¢ƒï¼Œè€Œä¸­å¿ƒåŒ–æ–¹æ³•åˆéš¾ä»¥åœ¨å®é™…å»ä¸­å¿ƒåŒ–ç³»ç»Ÿä¸­åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯è®©æ¯ä¸ªæ™ºèƒ½ä½“å­¦ä¹ ä¸€ä¸ªç¨€ç–çš„ã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„äº¤äº’ç»“æ„ï¼Œè€Œä¸æ˜¯ä¾èµ–é¢„å®šä¹‰çš„é™æ€é‚»åŸŸã€‚é€šè¿‡è´å¶æ–¯å˜åˆ†æ¨æ–­ï¼Œæ™ºèƒ½ä½“å¯ä»¥æ ¹æ®å½“å‰çŠ¶æ€é‡‡æ ·ä¸€ä¸ªå­å›¾ï¼Œå¹¶åŸºäºè¯¥å­å›¾è¿›è¡Œæ¶ˆæ¯ä¼ é€’å’Œç­–ç•¥è®¡ç®—ã€‚è¿™ç§æ–¹æ³•å…è®¸æ™ºèƒ½ä½“åŠ¨æ€åœ°è°ƒæ•´å…¶äº¤äº’å¯¹è±¡ï¼Œä»è€Œæ›´å¥½åœ°é€‚åº”ç¯å¢ƒå˜åŒ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šBayesGæ˜¯ä¸€ä¸ªå»ä¸­å¿ƒåŒ–çš„actoræ¡†æ¶ã€‚æ¯ä¸ªæ™ºèƒ½ä½“ç»´æŠ¤ä¸€ä¸ªego-graphï¼Œè¡¨ç¤ºå…¶å±€éƒ¨ç‰©ç†é‚»åŸŸã€‚æ™ºèƒ½ä½“é¦–å…ˆæ ¹æ®è‡ªèº«çŠ¶æ€å’Œé‚»å±…çŠ¶æ€ï¼Œä½¿ç”¨å˜åˆ†æ¨æ–­ç½‘ç»œé‡‡æ ·ä¸€ä¸ªæ½œåœ¨çš„é€šä¿¡æ©ç ã€‚è¯¥æ©ç ç”¨äºè¿‡æ»¤ego-graphä¸­çš„è¾¹ï¼Œä»è€Œå½¢æˆä¸€ä¸ªç¨€ç–çš„äº¤äº’å­å›¾ã€‚ç„¶åï¼Œæ™ºèƒ½ä½“åœ¨è¯¥å­å›¾ä¸Šè¿›è¡Œæ¶ˆæ¯ä¼ é€’ï¼Œèšåˆé‚»å±…ä¿¡æ¯ã€‚æœ€åï¼Œæ™ºèƒ½ä½“åŸºäºèšåˆåçš„ä¿¡æ¯è®¡ç®—ç­–ç•¥å¹¶æ‰§è¡ŒåŠ¨ä½œã€‚æ•´ä¸ªè¿‡ç¨‹æ˜¯ç«¯åˆ°ç«¯å¯è®­ç»ƒçš„ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºä½¿ç”¨è´å¶æ–¯å˜åˆ†æ¨æ–­æ¥å­¦ä¹ åŠ¨æ€çš„äº¤äº’ç»“æ„ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒBayesGä¸éœ€è¦é¢„å®šä¹‰é™æ€é‚»åŸŸï¼Œè€Œæ˜¯å…è®¸æ™ºèƒ½ä½“æ ¹æ®ç¯å¢ƒåŠ¨æ€åœ°é€‰æ‹©äº¤äº’å¯¹è±¡ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”åŠ¨æ€å’Œå¼‚æ„ç¯å¢ƒï¼Œå¹¶æé«˜æ™ºèƒ½ä½“çš„åä½œæ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šBayesGçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰æ¥å­¦ä¹ æ½œåœ¨çš„é€šä¿¡æ©ç ï¼ŒVAEçš„ç›®æ ‡æ˜¯æœ€å¤§åŒ–è¯æ®ä¸‹ç•Œï¼ˆELBOï¼‰ï¼Œä»è€Œé¼“åŠ±å­¦ä¹ ç¨€ç–çš„äº¤äº’ç»“æ„ï¼›2) ä½¿ç”¨GNNè¿›è¡Œæ¶ˆæ¯ä¼ é€’ï¼Œèšåˆé‚»å±…ä¿¡æ¯ï¼›3) ä½¿ç”¨Actor-Criticæ¡†æ¶è¿›è¡Œç­–ç•¥å­¦ä¹ ï¼ŒActorç½‘ç»œåŸºäºèšåˆåçš„ä¿¡æ¯è¾“å‡ºåŠ¨ä½œï¼ŒCriticç½‘ç»œè¯„ä¼°åŠ¨ä½œçš„ä»·å€¼ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

BayesGåœ¨å¤§å‹äº¤é€šæ§åˆ¶ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨åŒ…å«167ä¸ªæ™ºèƒ½ä½“çš„åœºæ™¯ä¸­ï¼ŒBayesGä¼˜äºå¤šä¸ªå¼ºå¤§çš„MARLåŸºçº¿ï¼Œè¯æ˜äº†å…¶å“è¶Šçš„å¯æ‰©å±•æ€§ã€æ•ˆç‡å’Œæ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBayesGèƒ½å¤Ÿæœ‰æ•ˆåœ°å­¦ä¹ ç¨€ç–çš„äº¤äº’ç»“æ„ï¼Œå¹¶æé«˜æ™ºèƒ½ä½“çš„åä½œæ•ˆç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

BayesGé€‚ç”¨äºéœ€è¦å»ä¸­å¿ƒåŒ–åä½œä¸”äº¤äº’ç»“æ„åŠ¨æ€å˜åŒ–çš„åœºæ™¯ï¼Œä¾‹å¦‚äº¤é€šæ§åˆ¶ã€æœºå™¨äººé›†ç¾¤ã€ä¼ æ„Ÿå™¨ç½‘ç»œç­‰ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæé«˜ç³»ç»Ÿçš„å¯æ‰©å±•æ€§ã€é²æ£’æ€§å’Œé€‚åº”æ€§ï¼Œåœ¨æ™ºèƒ½äº¤é€šã€æ™ºèƒ½åˆ¶é€ ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In networked multi-agent reinforcement learning (Networked-MARL), decentralized agents must act under local observability and constrained communication over fixed physical graphs. Existing methods often assume static neighborhoods, limiting adaptability to dynamic or heterogeneous environments. While centralized frameworks can learn dynamic graphs, their reliance on global state access and centralized infrastructure is impractical in real-world decentralized systems. We propose a stochastic graph-based policy for Networked-MARL, where each agent conditions its decision on a sampled subgraph over its local physical neighborhood. Building on this formulation, we introduce BayesG, a decentralized actor-framework that learns sparse, context-aware interaction structures via Bayesian variational inference. Each agent operates over an ego-graph and samples a latent communication mask to guide message passing and policy computation. The variational distribution is trained end-to-end alongside the policy using an evidence lower bound (ELBO) objective, enabling agents to jointly learn both interaction topology and decision-making strategies. BayesG outperforms strong MARL baselines on large-scale traffic control tasks with up to 167 agents, demonstrating superior scalability, efficiency, and performance.

