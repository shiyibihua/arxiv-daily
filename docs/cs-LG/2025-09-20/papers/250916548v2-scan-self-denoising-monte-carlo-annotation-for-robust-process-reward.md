---
layout: default
title: SCAN: Self-Denoising Monte Carlo Annotation for Robust Process Reward Learning
---

# SCAN: Self-Denoising Monte Carlo Annotation for Robust Process Reward Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.16548" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.16548v2</a>
  <a href="https://arxiv.org/pdf/2509.16548.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.16548v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.16548v2', 'SCAN: Self-Denoising Monte Carlo Annotation for Robust Process Reward Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuyang Ding, Xinyu Shi, Juntao Li, Xiaobo Liang, Zhaopeng Tu, Min Zhang

**åˆ†ç±»**: cs.LG, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-20 (æ›´æ–°: 2025-10-14)

**å¤‡æ³¨**: NeurIPS 2025. Project page: https://scan-prm.github.io/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSCANè‡ªé™å™ªè’™ç‰¹å¡æ´›æ ‡æ³¨æ–¹æ³•ï¼Œç”¨äºç¨³å¥çš„è¿‡ç¨‹å¥–åŠ±å­¦ä¹ ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¿‡ç¨‹å¥–åŠ±æ¨¡å‹` `è’™ç‰¹å¡æ´›ä¼°è®¡` `è‡ªé™å™ª` `å¼±ç›‘ç£å­¦ä¹ ` `æ•°æ®åˆæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. äººå·¥æ ‡æ³¨è¿‡ç¨‹å¥–åŠ±æ¨¡å‹æ•°æ®æˆæœ¬é«˜ã€å¯æ‰©å±•æ€§å·®ï¼Œè’™ç‰¹å¡æ´›ä¼°è®¡åˆæˆæ•°æ®å™ªå£°å¤§ï¼Œæ˜“è¿‡æ‹Ÿåˆã€‚
2. æå‡ºSCANæ¡†æ¶ï¼Œé€šè¿‡è‡ªé™å™ªç­–ç•¥ç”Ÿæˆé«˜è´¨é‡åˆæˆæ•°æ®ï¼Œå¹¶é‡‡ç”¨å®¹é”™å­¦ä¹ æ–¹æ³•è®­ç»ƒPRMã€‚
3. å®éªŒè¡¨æ˜ï¼ŒSCANä»…ç”¨å°‘é‡åˆæˆæ•°æ®å³å¯è¶…è¶Šå¤§è§„æ¨¡äººå·¥æ ‡æ³¨æ•°æ®è®­ç»ƒçš„æ¨¡å‹ï¼Œä¸”æ€§èƒ½éšæ•°æ®è§„æ¨¡å¢å¤§è€Œæå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‡ç¨‹å¥–åŠ±æ¨¡å‹(PRM)èƒ½å¤Ÿæä¾›ç»†ç²’åº¦çš„ã€æ­¥éª¤çº§åˆ«çš„è¯„ä¼°ï¼Œä»è€Œä¿ƒè¿›å¤§å‹è¯­è¨€æ¨¡å‹(LLM)ä¸­æ›´æ·±å±‚æ¬¡çš„æ¨ç†è¿‡ç¨‹ï¼Œåœ¨æ•°å­¦æ¨ç†ç­‰å¤æ‚ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œç”±äºäººå·¥æ ‡æ³¨æ•°æ®çš„é«˜æˆæœ¬å’Œæœ‰é™çš„å¯æ‰©å±•æ€§ï¼Œå¼€å‘PRMå…·æœ‰æŒ‘æˆ˜æ€§ã€‚æ¥è‡ªè’™ç‰¹å¡æ´›(MC)ä¼°è®¡çš„åˆæˆæ•°æ®æ˜¯ä¸€ä¸ªæœ‰å¸Œæœ›çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†å­˜åœ¨é«˜å™ªå£°æ¯”çš„é—®é¢˜ï¼Œè¿™å¯èƒ½å¯¼è‡´è¿‡æ‹Ÿåˆå¹¶é˜»ç¢å¤§è§„æ¨¡è®­ç»ƒã€‚æœ¬æ–‡å¯¹æ¥è‡ªMCä¼°è®¡çš„åˆæˆæ•°æ®ä¸­çš„å™ªå£°åˆ†å¸ƒè¿›è¡Œäº†åˆæ­¥ç ”ç©¶ï¼Œå‘ç°æ ‡æ³¨æ¨¡å‹ç”±äºå…¶æ ‡æ³¨èƒ½åŠ›çš„é™åˆ¶ï¼Œå¾€å¾€ä¼šä½ä¼°å’Œé«˜ä¼°æ­¥éª¤çš„æ­£ç¡®æ€§ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªé™å™ªè’™ç‰¹å¡æ´›æ ‡æ³¨(SCAN)ï¼Œè¿™æ˜¯ä¸€ç§é«˜æ•ˆçš„æ•°æ®åˆæˆå’Œå®¹é”™å­¦ä¹ æ¡†æ¶ã€‚æˆ‘ä»¬çš„å…³é”®å‘ç°è¡¨æ˜ï¼š(1)å³ä½¿æ˜¯è½»é‡çº§æ¨¡å‹(ä¾‹å¦‚ï¼Œ15äº¿å‚æ•°)ä¹Ÿå¯ä»¥é€šè¿‡è‡ªé™å™ªç­–ç•¥ç”Ÿæˆé«˜è´¨é‡çš„æ ‡æ³¨ï¼Œä½¿PRMèƒ½å¤Ÿä»¥ä»…ä¸ºåŸå§‹MCä¼°è®¡6%çš„æ¨ç†æˆæœ¬å®ç°å“è¶Šçš„æ€§èƒ½ã€‚(2)é€šè¿‡æˆ‘ä»¬ç¨³å¥çš„å­¦ä¹ ç­–ç•¥ï¼ŒPRMå¯ä»¥æœ‰æ•ˆåœ°ä»è¿™ç§å¼±ç›‘ç£ä¸­å­¦ä¹ ï¼Œåœ¨ProcessBenchä¸­å®ç°äº†39.2 F1å€¼çš„æå‡(ä»19.9åˆ°59.1)ã€‚å°½ç®¡åªä½¿ç”¨äº†ç´§å‡‘çš„åˆæˆæ•°æ®é›†ï¼Œæˆ‘ä»¬çš„æ¨¡å‹è¶…è¶Šäº†å¼ºå¤§çš„åŸºçº¿ï¼ŒåŒ…æ‹¬é‚£äº›åœ¨å¤§å‹äººå·¥æ ‡æ³¨æ•°æ®é›†(å¦‚PRM800K)ä¸Šè®­ç»ƒçš„æ¨¡å‹ã€‚æ­¤å¤–ï¼Œéšç€æˆ‘ä»¬æ‰©å¤§åˆæˆæ•°æ®è§„æ¨¡ï¼Œæ€§èƒ½æŒç»­æé«˜ï¼Œçªå‡ºäº†SCANåœ¨å¯æ‰©å±•ã€ç»æµé«˜æ•ˆå’Œç¨³å¥çš„PRMè®­ç»ƒæ–¹é¢çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰è®­ç»ƒä¸­äººå·¥æ ‡æ³¨æ•°æ®æˆæœ¬é«˜æ˜‚ä¸”éš¾ä»¥æ‰©å±•çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äººå·¥æ ‡æ³¨æˆ–è’™ç‰¹å¡æ´›ï¼ˆMCï¼‰ä¼°è®¡ç”Ÿæˆçš„åˆæˆæ•°æ®ï¼Œä½†MCä¼°è®¡çš„åˆæˆæ•°æ®å™ªå£°è¿‡å¤§ï¼Œå¯¼è‡´æ¨¡å‹è®­ç»ƒæ—¶å®¹æ˜“è¿‡æ‹Ÿåˆï¼Œå½±å“PRMçš„æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨è‡ªé™å™ªç­–ç•¥ç”Ÿæˆé«˜è´¨é‡çš„åˆæˆæ•°æ®ï¼Œå¹¶è®¾è®¡ä¸€ç§å®¹é”™å­¦ä¹ æ–¹æ³•ï¼Œä½¿PRMèƒ½å¤Ÿä»è¿™äº›å¼±ç›‘ç£æ•°æ®ä¸­æœ‰æ•ˆå­¦ä¹ ã€‚é€šè¿‡åˆ†æMCä¼°è®¡å™ªå£°çš„åˆ†å¸ƒï¼Œå‘ç°æ ‡æ³¨æ¨¡å‹ä¼šç³»ç»Ÿæ€§åœ°ä½ä¼°å’Œé«˜ä¼°æ­¥éª¤çš„æ­£ç¡®æ€§ï¼Œä»è€Œè®¾è®¡è‡ªé™å™ªç­–ç•¥æ¥çº æ­£è¿™äº›åå·®ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSCANæ¡†æ¶ä¸»è¦åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼šæ•°æ®åˆæˆé˜¶æ®µå’Œæ¨¡å‹è®­ç»ƒé˜¶æ®µã€‚åœ¨æ•°æ®åˆæˆé˜¶æ®µï¼Œé¦–å…ˆä½¿ç”¨MCä¼°è®¡ç”Ÿæˆåˆå§‹çš„åˆæˆæ•°æ®ï¼Œç„¶ååˆ©ç”¨è‡ªé™å™ªç­–ç•¥å¯¹è¿™äº›æ•°æ®è¿›è¡Œæ¸…æ´—å’Œæ ¡æ­£ï¼Œç”Ÿæˆé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ã€‚åœ¨æ¨¡å‹è®­ç»ƒé˜¶æ®µï¼Œä½¿ç”¨å®¹é”™å­¦ä¹ æ–¹æ³•ï¼Œè®­ç»ƒPRMæ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿä»å™ªå£°æ•°æ®ä¸­å­¦ä¹ åˆ°æœ‰æ•ˆçš„å¥–åŠ±ä¿¡å·ã€‚

**å…³é”®åˆ›æ–°**ï¼šSCANçš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†è‡ªé™å™ªè’™ç‰¹å¡æ´›æ ‡æ³¨æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ†æMCä¼°è®¡å™ªå£°çš„åˆ†å¸ƒï¼Œè®¾è®¡äº†ä¸€ç§è‡ªé™å™ªç­–ç•¥ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°é™ä½åˆæˆæ•°æ®ä¸­çš„å™ªå£°ï¼Œä»è€Œæé«˜PRMçš„è®­ç»ƒæ•ˆæœã€‚ä¸ä¼ ç»Ÿçš„MCä¼°è®¡æ–¹æ³•ç›¸æ¯”ï¼ŒSCANèƒ½å¤Ÿç”Ÿæˆæ›´é«˜è´¨é‡çš„åˆæˆæ•°æ®ï¼Œä»è€Œé™ä½äº†å¯¹å¤§è§„æ¨¡äººå·¥æ ‡æ³¨æ•°æ®çš„ä¾èµ–ã€‚

**å…³é”®è®¾è®¡**ï¼šè‡ªé™å™ªç­–ç•¥çš„å…·ä½“å®ç°åŒ…æ‹¬ï¼š(1) ä½¿ç”¨å¤šä¸ªä¸åŒçš„æ ‡æ³¨æ¨¡å‹è¿›è¡Œæ ‡æ³¨ï¼Œç„¶åå¯¹æ ‡æ³¨ç»“æœè¿›è¡Œé›†æˆï¼Œä»¥å‡å°‘å•ä¸ªæ¨¡å‹çš„åå·®ã€‚(2) è®¾è®¡ä¸€ç§å™ªå£°æ„ŸçŸ¥æŸå¤±å‡½æ•°ï¼Œè¯¥å‡½æ•°èƒ½å¤Ÿæ ¹æ®æ•°æ®çš„å™ªå£°æ°´å¹³è°ƒæ•´æŸå¤±æƒé‡ï¼Œä»è€Œä½¿æ¨¡å‹æ›´åŠ å…³æ³¨é«˜è´¨é‡çš„æ•°æ®ã€‚(3) ä½¿ç”¨æ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œå¢åŠ æ•°æ®çš„å¤šæ ·æ€§ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSCANæ¡†æ¶åœ¨ProcessBenchæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒF1å€¼ä»19.9æå‡åˆ°59.1ï¼Œæå‡å¹…åº¦é«˜è¾¾39.2ã€‚å³ä½¿ä½¿ç”¨ç´§å‡‘çš„åˆæˆæ•°æ®é›†ï¼ŒSCANè®­ç»ƒçš„æ¨¡å‹ä¹Ÿè¶…è¶Šäº†åœ¨å¤§å‹äººå·¥æ ‡æ³¨æ•°æ®é›†ï¼ˆå¦‚PRM800Kï¼‰ä¸Šè®­ç»ƒçš„å¼ºå¤§åŸºçº¿ã€‚æ­¤å¤–ï¼Œéšç€åˆæˆæ•°æ®è§„æ¨¡çš„æ‰©å¤§ï¼ŒSCANçš„æ€§èƒ½æŒç»­æå‡ï¼ŒéªŒè¯äº†å…¶å¯æ‰©å±•æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SCANæ¡†æ¶å¯åº”ç”¨äºå„ç§éœ€è¦è¿‡ç¨‹å¥–åŠ±æ¨¡å‹çš„åœºæ™¯ï¼Œä¾‹å¦‚æ•°å­¦æ¨ç†ã€ä»£ç ç”Ÿæˆã€æœºå™¨äººæ§åˆ¶ç­‰ã€‚é€šè¿‡é™ä½å¯¹äººå·¥æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼ŒSCANèƒ½å¤Ÿæ˜¾è‘—é™ä½PRMçš„è®­ç»ƒæˆæœ¬ï¼Œå¹¶æé«˜å…¶å¯æ‰©å±•æ€§ã€‚è¯¥ç ”ç©¶æˆæœæœ‰åŠ©äºæ¨åŠ¨LLMåœ¨å¤æ‚ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Process reward models (PRMs) offer fine-grained, step-level evaluations that facilitate deeper reasoning processes in large language models (LLMs), proving effective in complex tasks like mathematical reasoning. However, developing PRMs is challenging due to the high cost and limited scalability of human-annotated data. Synthetic data from Monte Carlo (MC) estimation is a promising alternative but suffers from a high noise ratio, which can cause overfitting and hinder large-scale training. In this work, we conduct a preliminary study on the noise distribution in synthetic data from MC estimation, identifying that annotation models tend to both underestimate and overestimate step correctness due to limitations in their annotation capabilities. Building on these insights, we propose Self-Denoising Monte Carlo Annotation (SCAN), an efficient data synthesis and noise-tolerant learning framework. Our key findings indicate that: (1) Even lightweight models (e.g., 1.5B parameters) can produce high-quality annotations through a self-denoising strategy, enabling PRMs to achieve superior performance with only 6% the inference cost required by vanilla MC estimation. (2) With our robust learning strategy, PRMs can effectively learn from this weak supervision, achieving a 39.2 F1 score improvement (from 19.9 to 59.1) in ProcessBench. Despite using only a compact synthetic dataset, our models surpass strong baselines, including those trained on large-scale human-annotated datasets such as PRM800K. Furthermore, performance continues to improve as we scale up the synthetic data, highlighting the potential of SCAN for scalable, cost-efficient, and robust PRM training.

