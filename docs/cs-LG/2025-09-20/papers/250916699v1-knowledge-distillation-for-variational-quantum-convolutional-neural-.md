---
layout: default
title: Knowledge Distillation for Variational Quantum Convolutional Neural Networks on Heterogeneous Data
---

# Knowledge Distillation for Variational Quantum Convolutional Neural Networks on Heterogeneous Data

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.16699" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.16699v1</a>
  <a href="https://arxiv.org/pdf/2509.16699.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.16699v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.16699v1', 'Knowledge Distillation for Variational Quantum Convolutional Neural Networks on Heterogeneous Data')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kai Yu, Binbin Cai, Song Lin

**åˆ†ç±»**: quant-ph, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-20

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§å¼‚æ„æ•°æ®ä¸‹å˜åˆ†é‡å­å·ç§¯ç¥ç»ç½‘ç»œçš„çŸ¥è¯†è’¸é¦æ¡†æ¶ï¼Œè§£å†³åˆ†å¸ƒå¼é‡å­æœºå™¨å­¦ä¹ ä¸­çš„æ¨¡å‹èšåˆéš¾é¢˜ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `é‡å­æœºå™¨å­¦ä¹ ` `çŸ¥è¯†è’¸é¦` `å˜åˆ†é‡å­å·ç§¯ç¥ç»ç½‘ç»œ` `å¼‚æ„æ•°æ®` `è”é‚¦å­¦ä¹ ` `åˆ†å¸ƒå¼å­¦ä¹ ` `é‡å­é—¨æ•°é‡ä¼°è®¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. åˆ†å¸ƒå¼é‡å­æœºå™¨å­¦ä¹ é¢ä¸´å¼‚æ„æ•°æ®å’Œå±€éƒ¨æ¨¡å‹å·®å¼‚çš„æŒ‘æˆ˜ï¼Œé˜»ç¢å…¨å±€æ¨¡å‹èšåˆã€‚
2. æå‡ºåŸºäºçŸ¥è¯†è’¸é¦çš„VQCNNæ¡†æ¶ï¼Œåˆ©ç”¨é‡å­é—¨æ•°é‡ä¼°è®¡å’Œç²’å­ç¾¤ä¼˜åŒ–æ„å»ºèµ„æºè‡ªé€‚åº”çš„ä¸ªæ€§åŒ–æ¨¡å‹ã€‚
3. å®éªŒè¡¨æ˜ï¼Œèšåˆåçš„å…¨å±€æ¨¡å‹å‡†ç¡®ç‡æ¥è¿‘é›†ä¸­å¼è®­ç»ƒï¼Œæœ‰æ•ˆå¤„ç†å¼‚æ„æ€§å¹¶é™ä½èµ„æºæ¶ˆè€—ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹å¼‚æ„æ•°æ®ä¸‹å˜åˆ†é‡å­å·ç§¯ç¥ç»ç½‘ç»œï¼ˆVQCNNï¼‰çš„çŸ¥è¯†è’¸é¦æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³åˆ†å¸ƒå¼é‡å­æœºå™¨å­¦ä¹ ä¸­ç”±äºå®¢æˆ·ç«¯æ•°æ®å¼‚æ„æ€§å’Œå±€éƒ¨æ¨¡å‹ç»“æ„å·®å¼‚å¯¼è‡´çš„å…¨å±€æ¨¡å‹èšåˆéš¾é¢˜ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸€ä¸ªåŸºäºå®¢æˆ·ç«¯æ•°æ®çš„é‡å­é—¨æ•°é‡ä¼°è®¡æœºåˆ¶ï¼Œç”¨äºæŒ‡å¯¼èµ„æºè‡ªé€‚åº”çš„VQCNNç”µè·¯æ„å»ºã€‚é‡‡ç”¨ç²’å­ç¾¤ä¼˜åŒ–ç®—æ³•é«˜æ•ˆç”Ÿæˆé’ˆå¯¹æœ¬åœ°æ•°æ®ç‰¹å¾çš„ä¸ªæ€§åŒ–é‡å­æ¨¡å‹ã€‚åœ¨èšåˆè¿‡ç¨‹ä¸­ï¼Œä¸€ç§ç»“åˆè½¯æ ‡ç­¾å’Œç¡¬æ ‡ç­¾ç›‘ç£çš„çŸ¥è¯†è’¸é¦ç­–ç•¥åˆ©ç”¨å…¬å…±æ•°æ®é›†æ•´åˆæ¥è‡ªå¼‚æ„å®¢æˆ·ç«¯çš„çŸ¥è¯†ï¼Œå½¢æˆå…¨å±€æ¨¡å‹ï¼ŒåŒæ—¶é¿å…å‚æ•°æš´éœ²å’Œéšç§æ³„éœ²ã€‚ç†è®ºåˆ†æè¡¨æ˜ï¼Œè¯¥æ¡†æ¶å—ç›Šäºé‡å­é«˜ç»´è¡¨ç¤ºï¼Œä¼˜äºç»å…¸æ–¹æ³•ï¼Œå¹¶é€šè¿‡ä»…äº¤æ¢æ¨¡å‹ç´¢å¼•å’Œæµ‹è¯•è¾“å‡ºæ¥æœ€å°åŒ–é€šä¿¡ã€‚åœ¨PennyLaneå¹³å°ä¸Šçš„å¤§é‡ä»¿çœŸéªŒè¯äº†é—¨æ•°é‡ä¼°è®¡å’ŒåŸºäºè’¸é¦çš„èšåˆçš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œèšåˆçš„å…¨å±€æ¨¡å‹å®ç°äº†æ¥è¿‘å®Œå…¨ç›‘ç£é›†ä¸­å¼è®­ç»ƒçš„å‡†ç¡®ç‡ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°å¤„ç†å¼‚æ„æ€§ï¼Œé™ä½èµ„æºæ¶ˆè€—ï¼Œå¹¶ä¿æŒæ€§èƒ½ï¼Œçªå‡ºäº†å…¶åœ¨å¯æ‰©å±•å’Œä¿æŠ¤éšç§çš„åˆ†å¸ƒå¼é‡å­å­¦ä¹ ä¸­çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³åˆ†å¸ƒå¼é‡å­æœºå™¨å­¦ä¹ ä¸­ï¼Œç”±äºå„ä¸ªå®¢æˆ·ç«¯æ•°æ®åˆ†å¸ƒçš„å¼‚æ„æ€§ä»¥åŠæœ¬åœ°æ¨¡å‹ç»“æ„å·®å¼‚ï¼Œå¯¼è‡´å…¨å±€æ¨¡å‹éš¾ä»¥æœ‰æ•ˆèšåˆçš„é—®é¢˜ã€‚ç°æœ‰çš„æ–¹æ³•éš¾ä»¥åœ¨ä¿è¯æ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œå…¼é¡¾èµ„æºæ¶ˆè€—å’Œéšç§ä¿æŠ¤ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨çŸ¥è¯†è’¸é¦æŠ€æœ¯ï¼Œå°†å„ä¸ªå®¢æˆ·ç«¯è®­ç»ƒå¾—åˆ°çš„ä¸ªæ€§åŒ–é‡å­æ¨¡å‹çš„çŸ¥è¯†è¿ç§»åˆ°ä¸€ä¸ªå…¨å±€æ¨¡å‹ä¸­ã€‚é€šè¿‡è½¯æ ‡ç­¾å’Œç¡¬æ ‡ç­¾çš„ç»“åˆï¼Œåœ¨ä¿æŠ¤å®¢æˆ·ç«¯éšç§çš„åŒæ—¶ï¼Œå®ç°çŸ¥è¯†çš„æœ‰æ•ˆèšåˆã€‚åŒæ—¶ï¼Œåˆ©ç”¨é‡å­é—¨æ•°é‡ä¼°è®¡æœºåˆ¶ï¼Œæ ¹æ®å®¢æˆ·ç«¯æ•°æ®ç‰¹ç‚¹è‡ªé€‚åº”åœ°è°ƒæ•´é‡å­ç”µè·¯çš„å¤æ‚åº¦ï¼Œä»è€Œé™ä½èµ„æºæ¶ˆè€—ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) **å®¢æˆ·ç«¯æ¨¡å‹æ„å»º**ï¼šæ ¹æ®å®¢æˆ·ç«¯æ•°æ®ï¼Œåˆ©ç”¨é‡å­é—¨æ•°é‡ä¼°è®¡æœºåˆ¶ç¡®å®šVQCNNç”µè·¯çš„å¤æ‚åº¦ï¼Œå¹¶ä½¿ç”¨ç²’å­ç¾¤ä¼˜åŒ–ç®—æ³•è®­ç»ƒä¸ªæ€§åŒ–é‡å­æ¨¡å‹ã€‚2) **çŸ¥è¯†è’¸é¦**ï¼šåˆ©ç”¨å…¬å…±æ•°æ®é›†ï¼Œé€šè¿‡è½¯æ ‡ç­¾å’Œç¡¬æ ‡ç­¾çš„ç»“åˆï¼Œå°†å®¢æˆ·ç«¯æ¨¡å‹çš„çŸ¥è¯†è¿ç§»åˆ°å…¨å±€æ¨¡å‹ã€‚3) **å…¨å±€æ¨¡å‹èšåˆ**ï¼šå°†å„ä¸ªå®¢æˆ·ç«¯çš„çŸ¥è¯†è’¸é¦åˆ°å…¨å±€æ¨¡å‹ä¸­ï¼Œå¾—åˆ°æœ€ç»ˆçš„å…¨å±€æ¨¡å‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†åŸºäºé‡å­é—¨æ•°é‡ä¼°è®¡çš„èµ„æºè‡ªé€‚åº”VQCNNç”µè·¯æ„å»ºæ–¹æ³•ï¼Œå¯ä»¥æ ¹æ®å®¢æˆ·ç«¯æ•°æ®ç‰¹ç‚¹åŠ¨æ€è°ƒæ•´é‡å­ç”µè·¯çš„å¤æ‚åº¦ã€‚2) æå‡ºäº†ç»“åˆè½¯æ ‡ç­¾å’Œç¡¬æ ‡ç­¾çš„çŸ¥è¯†è’¸é¦ç­–ç•¥ï¼Œå¯ä»¥åœ¨ä¿æŠ¤å®¢æˆ·ç«¯éšç§çš„åŒæ—¶ï¼Œå®ç°çŸ¥è¯†çš„æœ‰æ•ˆèšåˆã€‚3) å°†çŸ¥è¯†è’¸é¦æŠ€æœ¯åº”ç”¨äºåˆ†å¸ƒå¼é‡å­æœºå™¨å­¦ä¹ ï¼Œä¸ºè§£å†³å¼‚æ„æ•°æ®ä¸‹çš„æ¨¡å‹èšåˆé—®é¢˜æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚

**å…³é”®è®¾è®¡**ï¼š1) **é‡å­é—¨æ•°é‡ä¼°è®¡**ï¼šåŸºäºå®¢æˆ·ç«¯æ•°æ®ï¼Œä¼°è®¡æ‰€éœ€çš„é‡å­é—¨æ•°é‡ï¼Œç”¨äºæŒ‡å¯¼VQCNNç”µè·¯çš„æ„å»ºã€‚2) **ç²’å­ç¾¤ä¼˜åŒ–**ï¼šç”¨äºé«˜æ•ˆåœ°è®­ç»ƒä¸ªæ€§åŒ–é‡å­æ¨¡å‹ã€‚3) **è½¯æ ‡ç­¾å’Œç¡¬æ ‡ç­¾ç»“åˆçš„çŸ¥è¯†è’¸é¦**ï¼šåˆ©ç”¨å…¬å…±æ•°æ®é›†ï¼Œé€šè¿‡æœ€å°åŒ–å…¨å±€æ¨¡å‹å’Œå®¢æˆ·ç«¯æ¨¡å‹åœ¨è½¯æ ‡ç­¾å’Œç¡¬æ ‡ç­¾ä¸Šçš„å·®å¼‚ï¼Œå®ç°çŸ¥è¯†è¿ç§»ã€‚4) **æŸå¤±å‡½æ•°**ï¼šé‡‡ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°å’Œå‡æ–¹è¯¯å·®æŸå¤±å‡½æ•°ï¼Œåˆ†åˆ«ç”¨äºç¡¬æ ‡ç­¾å’Œè½¯æ ‡ç­¾çš„ç›‘ç£ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„çŸ¥è¯†è’¸é¦æ¡†æ¶åœ¨å¼‚æ„æ•°æ®ä¸‹èƒ½å¤Ÿæœ‰æ•ˆåœ°èšåˆVQCNNæ¨¡å‹ï¼Œèšåˆåçš„å…¨å±€æ¨¡å‹å‡†ç¡®ç‡æ¥è¿‘å®Œå…¨ç›‘ç£çš„é›†ä¸­å¼è®­ç»ƒã€‚ä¸ä¼ ç»Ÿçš„è”é‚¦å¹³å‡ç®—æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨ä¿è¯æ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½äº†é€šä¿¡æˆæœ¬å’Œèµ„æºæ¶ˆè€—ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè”é‚¦å­¦ä¹ ã€è¾¹ç¼˜è®¡ç®—ç­‰åœºæ™¯ï¼Œå°¤å…¶é€‚ç”¨äºæ•°æ®åˆ†å¸ƒå¼‚æ„ä¸”å¯¹éšç§ä¿æŠ¤æœ‰è¾ƒé«˜è¦æ±‚çš„åº”ç”¨ï¼Œä¾‹å¦‚åŒ»ç–—å¥åº·ã€é‡‘èé£æ§ç­‰é¢†åŸŸã€‚æœªæ¥å¯è¿›ä¸€æ­¥æ¢ç´¢åœ¨æ›´å¤æ‚çš„é‡å­æ¨¡å‹å’Œæ•°æ®é›†ä¸Šçš„åº”ç”¨ï¼Œå¹¶ç ”ç©¶æ›´é«˜æ•ˆçš„çŸ¥è¯†è’¸é¦ç­–ç•¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Distributed quantum machine learning faces significant challenges due to heterogeneous client data and variations in local model structures, which hinder global model aggregation. To address these challenges, we propose a knowledge distillation framework for variational quantum convolutional neural networks on heterogeneous data. The framework features a quantum gate number estimation mechanism based on client data, which guides the construction of resource-adaptive VQCNN circuits. Particle swarm optimization is employed to efficiently generate personalized quantum models tailored to local data characteristics. During aggregation, a knowledge distillation strategy integrating both soft-label and hard-label supervision consolidates knowledge from heterogeneous clients using a public dataset, forming a global model while avoiding parameter exposure and privacy leakage. Theoretical analysis shows that proposed framework benefits from quantum high-dimensional representation, offering advantages over classical approaches, and minimizes communication by exchanging only model indices and test outputs. Extensive simulations on the PennyLane platform validate the effectiveness of the gate number estimation and distillation-based aggregation. Experimental results demonstrate that the aggregated global model achieves accuracy close to fully supervised centralized training. These results shown that proposed methods can effectively handle heterogeneity, reduce resource consumption, and maintain performance, highlighting its potential for scalable and privacy-preserving distributed quantum learning.

