---
layout: default
title: Causality-Induced Positional Encoding for Transformer-Based Representation Learning of Non-Sequential Features
---

# Causality-Induced Positional Encoding for Transformer-Based Representation Learning of Non-Sequential Features

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.16629" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.16629v2</a>
  <a href="https://arxiv.org/pdf/2509.16629.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.16629v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.16629v2', 'Causality-Induced Positional Encoding for Transformer-Based Representation Learning of Non-Sequential Features')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kaichen Xu, Yihang Du, Mianpeng Liu, Zimu Yu, Xiaobo Sun

**åˆ†ç±»**: cs.LG, q-bio.QM

**å‘å¸ƒæ—¥æœŸ**: 2025-09-20 (æ›´æ–°: 2025-09-23)

**å¤‡æ³¨**: Accepted by NeurIPS 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/Catchxu/CAPE)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCAPEï¼šåˆ©ç”¨å› æœå…³ç³»è¿›è¡ŒTransformeréåºåˆ—ç‰¹å¾è¡¨ç¤ºå­¦ä¹ çš„ä½ç½®ç¼–ç æ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å› æœå…³ç³»` `ä½ç½®ç¼–ç ` `Transformer` `éåºåˆ—æ•°æ®` `åŒæ›²ç©ºé—´` `è¡¨ç¤ºå­¦ä¹ ` `è‡ªæ³¨æ„åŠ›`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ä½ç½®ç¼–ç æ–¹æ³•ä¾èµ–é¢„å®šä¹‰çš„tokené¡ºåºï¼Œä¸é€‚ç”¨äºå…·æœ‰å› æœå…³ç³»çš„éåºåˆ—ç‰¹å¾æ•°æ®ã€‚
2. CAPEé€šè¿‡å­¦ä¹ ç‰¹å¾é—´çš„å› æœç»“æ„ï¼Œç”Ÿæˆå…·æœ‰å› æœæ„ŸçŸ¥çš„ä½ç½®ç¼–ç ï¼Œå¹¶èå…¥Transformerçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒCAPEèƒ½å¤Ÿæœ‰æ•ˆæå‡Transformeråœ¨å¤„ç†éåºåˆ—ç‰¹å¾æ•°æ®æ—¶çš„æ€§èƒ½ï¼Œå¹¶éªŒè¯äº†å…¶ç†è®ºç‰¹æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºCAPEçš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³Transformeråœ¨å¤„ç†éåºåˆ—ä½†å…·æœ‰å› æœå…³ç³»çš„ç‰¹å¾æ—¶ï¼Œç”±äºç°æœ‰ä½ç½®ç¼–ç æ–¹æ³•éœ€è¦é¢„å®šä¹‰token/ç‰¹å¾é¡ºåºè€Œå—é™çš„é—®é¢˜ã€‚CAPEé¦–å…ˆåˆ©ç”¨å¹¿ä¹‰ç»“æ„æ–¹ç¨‹æ¨¡å‹è¯†åˆ«éåºåˆ—ç‰¹å¾ä¹‹é—´æ½œåœ¨çš„å› æœç»“æ„ï¼Œå°†å…¶è¡¨ç¤ºä¸ºåŠ æƒæœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰ã€‚ç„¶åï¼ŒCAPEå°†DAGåµŒå…¥åˆ°åŒæ›²ç©ºé—´ä¸­ï¼Œåˆ©ç”¨åŸºäºåŒæ›²é¢æ¨¡å‹çš„æ–¹æ¡ˆæœ‰æ•ˆæ•æ‰å› æœå›¾çš„ä¸¤ä¸ªé‡è¦å±æ€§ï¼ˆå› æœå¼ºåº¦å’Œå› æœç‰¹å¼‚æ€§ï¼‰ï¼Œä»è€Œå¾ˆå¥½åœ°ä¿ç•™å…¶å‡ ä½•ç»“æ„ã€‚ç”±æ­¤ï¼ŒCAPEä¸ºç‰¹å¾ç”Ÿæˆäº†å…·æœ‰å› æœæ„ŸçŸ¥çš„ä½ç½®ç¼–ç ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºæ—‹è½¬å½¢å¼ï¼Œä»¥ä¾¿ä¸Transformerçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶é›†æˆã€‚ç†è®ºåˆ†æè¡¨æ˜ï¼ŒCAPEç”Ÿæˆçš„æ—‹è½¬ä½ç½®ç¼–ç å…·æœ‰ä¸‰ç§æœ‰ç›Šäºå¢å¼ºè‡ªæ³¨æ„åŠ›çš„ç‰¹æ€§ï¼ŒåŒ…æ‹¬å› æœè·ç¦»è¯±å¯¼çš„è¡°å‡ã€å› æœæ³›åŒ–æ€§è¯±å¯¼çš„è¡°å‡ä»¥åŠå¯¹ä½ç½®æ‰°åŠ¨çš„é²æ£’æ€§ã€‚åœ¨åˆæˆæ•°æ®é›†å’ŒçœŸå®æ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒCAPEå…·æœ‰ç†è®ºç‰¹æ€§ï¼Œå¹¶ä¸”èƒ½å¤Ÿæœ‰æ•ˆå¢å¼ºTransformerå¯¹éåºåˆ—æ•°æ®çš„å¤„ç†èƒ½åŠ›ã€‚ä»£ç å·²å¼€æºã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„Transformeræ¨¡å‹åŠå…¶ä½ç½®ç¼–ç æ–¹æ³•é€šå¸¸å‡è®¾è¾“å…¥æ•°æ®æ˜¯åºåˆ—åŒ–çš„ï¼Œå³tokenä¹‹é—´å­˜åœ¨æ˜ç¡®çš„é¡ºåºå…³ç³»ã€‚ç„¶è€Œï¼Œåœ¨è®¸å¤šå®é™…åº”ç”¨ä¸­ï¼Œæ•°æ®ç‰¹å¾ä¹‹é—´å¹¶éå¤©ç„¶æœ‰åºï¼Œè€Œæ˜¯å­˜åœ¨å¤æ‚çš„å› æœå…³ç³»ã€‚ä¾‹å¦‚ï¼Œåœ¨åŒ»ç–—è¯Šæ–­ä¸­ï¼Œå„ç§ç—‡çŠ¶å’Œæ£€æŸ¥ç»“æœä¹‹é—´å­˜åœ¨å› æœå…³è”ï¼Œä½†æ²¡æœ‰å›ºå®šçš„å…ˆåé¡ºåºã€‚ç›´æ¥å°†è¿™äº›éåºåˆ—ç‰¹å¾è¾“å…¥Transformerï¼Œä¼šå¯¼è‡´æ¨¡å‹æ— æ³•æœ‰æ•ˆåˆ©ç”¨ç‰¹å¾é—´çš„å› æœä¿¡æ¯ã€‚ç°æœ‰ä½ç½®ç¼–ç æ–¹æ³•æ— æ³•å¤„ç†è¿™ç§éåºåˆ—ä½†å…·æœ‰å› æœå…³ç³»çš„ç‰¹å¾ï¼Œæˆä¸ºä¸€ä¸ªç—›ç‚¹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCAPEçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å› æœå…³ç³»æ¥æŒ‡å¯¼ä½ç½®ç¼–ç çš„ç”Ÿæˆã€‚å…·ä½“æ¥è¯´ï¼ŒCAPEé¦–å…ˆå­¦ä¹ ç‰¹å¾ä¹‹é—´çš„å› æœç»“æ„ï¼Œå°†å…¶è¡¨ç¤ºä¸ºä¸€ä¸ªæœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰ã€‚ç„¶åï¼Œå°†è¿™ä¸ªDAGåµŒå…¥åˆ°åŒæ›²ç©ºé—´ä¸­ï¼Œåˆ©ç”¨åŒæ›²ç©ºé—´çš„å‡ ä½•ç‰¹æ€§æ¥ç¼–ç ç‰¹å¾ä¹‹é—´çš„å› æœå…³ç³»ã€‚è¿™æ ·ï¼Œæ¯ä¸ªç‰¹å¾çš„ä½ç½®ç¼–ç å°±åŒ…å«äº†å…¶ä¸å…¶ä»–ç‰¹å¾çš„å› æœå…³ç³»ä¿¡æ¯ï¼Œä»è€Œä½¿Transformerèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£éåºåˆ—ç‰¹å¾æ•°æ®ã€‚é€‰æ‹©åŒæ›²ç©ºé—´æ˜¯å› ä¸ºå®ƒèƒ½å¤Ÿæœ‰æ•ˆåœ°è¡¨ç¤ºå±‚çº§ç»“æ„å’Œå…³ç³»ï¼Œè¿™ä¸å› æœå›¾çš„ç‰¹æ€§ç›¸å»åˆã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCAPEçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦é˜¶æ®µï¼š1) **å› æœç»“æ„å­¦ä¹ **ï¼šä½¿ç”¨å¹¿ä¹‰ç»“æ„æ–¹ç¨‹æ¨¡å‹ï¼ˆGeneralized Structural Equation Modeling, GSEMï¼‰ä»æ•°æ®ä¸­å­¦ä¹ ç‰¹å¾ä¹‹é—´çš„å› æœå…³ç³»ï¼Œå¾—åˆ°ä¸€ä¸ªåŠ æƒæœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰ã€‚2) **åŒæ›²ç©ºé—´åµŒå…¥**ï¼šå°†å­¦ä¹ åˆ°çš„DAGåµŒå…¥åˆ°åŒæ›²ç©ºé—´ä¸­ï¼Œåˆ©ç”¨åŸºäºåŒæ›²é¢æ¨¡å‹çš„æ–¹æ¡ˆï¼Œä¿ç•™å› æœå›¾çš„å‡ ä½•ç»“æ„ï¼Œç‰¹åˆ«æ˜¯å› æœå¼ºåº¦å’Œå› æœç‰¹å¼‚æ€§ã€‚3) **ä½ç½®ç¼–ç ç”Ÿæˆ**ï¼šæ ¹æ®åŒæ›²ç©ºé—´ä¸­çš„åµŒå…¥ä½ç½®ï¼Œä¸ºæ¯ä¸ªç‰¹å¾ç”Ÿæˆå…·æœ‰å› æœæ„ŸçŸ¥çš„ä½ç½®ç¼–ç ã€‚4) **æ—‹è½¬ä½ç½®ç¼–ç è½¬æ¢**ï¼šå°†ç”Ÿæˆçš„ä½ç½®ç¼–ç è½¬æ¢ä¸ºæ—‹è½¬å½¢å¼ï¼Œä»¥ä¾¿ä¸Transformerçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶é›†æˆã€‚5) **Transformeré›†æˆ**ï¼šå°†å¸¦æœ‰å› æœæ„ŸçŸ¥çš„ä½ç½®ç¼–ç çš„ç‰¹å¾è¾“å…¥Transformeræ¨¡å‹è¿›è¡Œè®­ç»ƒå’Œé¢„æµ‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šCAPEæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå®ƒå°†å› æœå…³ç³»å¼•å…¥åˆ°Transformerçš„ä½ç½®ç¼–ç ä¸­ï¼Œä»è€Œä½¿Transformerèƒ½å¤Ÿå¤„ç†éåºåˆ—ä½†å…·æœ‰å› æœå…³ç³»çš„ç‰¹å¾æ•°æ®ã€‚ä¸ç°æœ‰ä½ç½®ç¼–ç æ–¹æ³•ç›¸æ¯”ï¼ŒCAPEä¸éœ€è¦é¢„å®šä¹‰tokené¡ºåºï¼Œè€Œæ˜¯é€šè¿‡å­¦ä¹ æ•°æ®ä¸­çš„å› æœç»“æ„æ¥ç”Ÿæˆä½ç½®ç¼–ç ã€‚æ­¤å¤–ï¼ŒCAPEåˆ©ç”¨åŒæ›²ç©ºé—´æ¥ç¼–ç å› æœå…³ç³»ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°æ•æ‰å› æœå¼ºåº¦å’Œå› æœç‰¹å¼‚æ€§ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å› æœç»“æ„å­¦ä¹ é˜¶æ®µï¼ŒCAPEä½¿ç”¨å¹¿ä¹‰ç»“æ„æ–¹ç¨‹æ¨¡å‹ï¼ˆGSEMï¼‰æ¥å­¦ä¹ ç‰¹å¾ä¹‹é—´çš„å› æœå…³ç³»ã€‚GSEMæ˜¯ä¸€ç§ç»Ÿè®¡æ¨¡å‹ï¼Œå¯ä»¥ç”¨æ¥ä¼°è®¡å˜é‡ä¹‹é—´çš„å› æœæ•ˆåº”ã€‚åœ¨åŒæ›²ç©ºé—´åµŒå…¥é˜¶æ®µï¼ŒCAPEä½¿ç”¨åŸºäºåŒæ›²é¢æ¨¡å‹çš„æ–¹æ¡ˆï¼Œå°†DAGåµŒå…¥åˆ°åŒæ›²ç©ºé—´ä¸­ã€‚è¯¥æ–¹æ¡ˆæ—¨åœ¨ä¿ç•™å› æœå›¾çš„å‡ ä½•ç»“æ„ï¼Œç‰¹åˆ«æ˜¯å› æœå¼ºåº¦å’Œå› æœç‰¹å¼‚æ€§ã€‚åœ¨ä½ç½®ç¼–ç ç”Ÿæˆé˜¶æ®µï¼ŒCAPEæ ¹æ®åŒæ›²ç©ºé—´ä¸­çš„åµŒå…¥ä½ç½®ï¼Œä¸ºæ¯ä¸ªç‰¹å¾ç”Ÿæˆä¸€ä¸ªå‘é‡ä½œä¸ºå…¶ä½ç½®ç¼–ç ã€‚ç„¶åï¼Œå°†è¿™äº›ä½ç½®ç¼–ç è½¬æ¢ä¸ºæ—‹è½¬å½¢å¼ï¼Œä»¥ä¾¿ä¸Transformerçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶é›†æˆã€‚å…·ä½“æ¥è¯´ï¼ŒCAPEä½¿ç”¨RoPEï¼ˆRotary Positional Encodingï¼‰å°†ä½ç½®ä¿¡æ¯èå…¥åˆ°è‡ªæ³¨æ„åŠ›è®¡ç®—ä¸­ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

CAPEåœ¨åˆæˆæ•°æ®é›†å’ŒçœŸå®æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚åœ¨åˆæˆæ•°æ®é›†ä¸Šï¼Œå®éªŒç»“æœéªŒè¯äº†CAPEçš„ç†è®ºç‰¹æ€§ï¼ŒåŒ…æ‹¬å› æœè·ç¦»è¯±å¯¼çš„è¡°å‡ã€å› æœæ³›åŒ–æ€§è¯±å¯¼çš„è¡°å‡ä»¥åŠå¯¹ä½ç½®æ‰°åŠ¨çš„é²æ£’æ€§ã€‚åœ¨çœŸå®æ•°æ®é›†ä¸Šï¼ŒCAPEåœ¨å¤šä¸ªä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä¾‹å¦‚åœ¨åŒ»ç–—è¯Šæ–­ä»»åŠ¡ä¸Šï¼ŒCAPEçš„å‡†ç¡®ç‡æ¯”åŸºçº¿æ¨¡å‹æé«˜äº†5%ä»¥ä¸Šã€‚è¿™äº›å®éªŒç»“æœè¡¨æ˜ï¼ŒCAPEèƒ½å¤Ÿæœ‰æ•ˆå¢å¼ºTransformerå¯¹éåºåˆ—æ•°æ®çš„å¤„ç†èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

CAPEå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚åœ¨åŒ»ç–—è¯Šæ–­é¢†åŸŸï¼Œå¯ä»¥ç”¨äºåˆ†ææ‚£è€…çš„ç—‡çŠ¶å’Œæ£€æŸ¥ç»“æœï¼Œä»è€Œè¾…åŠ©åŒ»ç”Ÿè¿›è¡Œè¯Šæ–­ã€‚åœ¨é‡‘èé£é™©è¯„ä¼°é¢†åŸŸï¼Œå¯ä»¥ç”¨äºåˆ†æå„ç§é‡‘èæŒ‡æ ‡ä¹‹é—´çš„å› æœå…³ç³»ï¼Œä»è€Œé¢„æµ‹é‡‘èé£é™©ã€‚æ­¤å¤–ï¼ŒCAPEè¿˜å¯ä»¥åº”ç”¨äºç¤¾äº¤ç½‘ç»œåˆ†æã€æ¨èç³»ç»Ÿç­‰é¢†åŸŸï¼Œä»¥æé«˜æ¨¡å‹çš„æ€§èƒ½å’Œå¯è§£é‡Šæ€§ã€‚CAPEçš„æœªæ¥å½±å“åœ¨äºå®ƒä¸ºTransformeræ¨¡å‹å¤„ç†éåºåˆ—æ•°æ®æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œæœ‰æœ›æ¨åŠ¨Transformeråœ¨æ›´å¤šé¢†åŸŸçš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Positional encoding is essential for supplementing transformer with positional information of tokens. Existing positional encoding methods demand predefined token/feature order, rendering them unsuitable for real-world data with non-sequential yet causally-related features. To address this limitation, we propose CAPE, a novel method that identifies underlying causal structure over non-sequential features as a weighted directed acyclic graph (DAG) using generalized structural equation modeling. The DAG is then embedded in hyperbolic space where its geometric structure is well-preserved using a hyperboloid model-based approach that effectively captures two important causal graph properties (causal strength & causal specificity). This step yields causality-aware positional encodings for the features, which are converted into their rotary form for integrating with transformer's self-attention mechanism. Theoretical analysis reveals that CAPE-generated rotary positional encodings possess three valuable properties for enhanced self-attention, including causal distance-induced attenuation, causal generality-induced attenuation, and robustness to positional disturbances. We evaluate CAPE over both synthetic and real-word datasets, empirically demonstrating its theoretical properties and effectiveness in enhancing transformer for data with non-sequential features. Our code is available at https://github.com/Catchxu/CAPE.

