---
layout: default
title: $\boldsymbolÎ»$-Orthogonality Regularization for Compatible Representation Learning
---

# $\boldsymbolÎ»$-Orthogonality Regularization for Compatible Representation Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.16664" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.16664v2</a>
  <a href="https://arxiv.org/pdf/2509.16664.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.16664v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.16664v2', '$\boldsymbolÎ»$-Orthogonality Regularization for Compatible Representation Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Simone Ricci, NiccolÃ² Biondi, Federico Pernici, Ioannis Patras, Alberto Del Bimbo

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-20 (æ›´æ–°: 2025-10-20)

**å¤‡æ³¨**: Accepted at NeurIPS2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/miccunifi/lambda_orthogonality.git) | [GITHUB](https://github.com/miccunifi/lambda)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºÎ»-æ­£äº¤æ­£åˆ™åŒ–ï¼Œç”¨äºå…¼å®¹è¡¨å¾å­¦ä¹ ï¼Œæå‡æ¨¡å‹æ›´æ–°åçš„é›¶æ ·æœ¬æ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è¡¨å¾å­¦ä¹ ` `å…¼å®¹æ€§` `æ­£äº¤æ­£åˆ™åŒ–` `ä»¿å°„å˜æ¢` `æ¨¡å‹æ›´æ–°` `é›¶æ ·æœ¬å­¦ä¹ ` `çŸ¥è¯†è¿ç§»`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•éš¾ä»¥åœ¨é€‚åº”æ–°æ•°æ®åˆ†å¸ƒçš„åŒæ—¶ï¼Œä¿æŒæ¨¡å‹æ›´æ–°å‰åè¡¨å¾ç©ºé—´çš„ä¸€è‡´æ€§ï¼Œä»¿å°„å˜æ¢æ˜“æ”¹å˜åŸå§‹è¡¨å¾ï¼Œæ­£äº¤å˜æ¢é™åˆ¶äº†é€‚åº”æ€§ã€‚
2. æå‡ºÎ»-æ­£äº¤æ­£åˆ™åŒ–ï¼Œé€šè¿‡åœ¨å­¦ä¹ ä»¿å°„å˜æ¢æ—¶æ–½åŠ ä¸€ä¸ªå®½æ¾çš„æ­£äº¤çº¦æŸï¼Œå®ç°åˆ†å¸ƒç‰¹å®šé€‚åº”ï¼ŒåŒæ—¶ä¿ç•™åŸå§‹å­¦ä¹ çš„è¡¨å¾ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å„ç§æ¶æ„å’Œæ•°æ®é›†ä¸Šï¼Œèƒ½å¤Ÿä¿ç•™æ¨¡å‹çš„é›¶æ ·æœ¬æ€§èƒ½ï¼Œå¹¶ç¡®ä¿æ¨¡å‹æ›´æ–°ä¹‹é—´çš„å…¼å®¹æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ£€ç´¢ç³»ç»Ÿä¾èµ–äºæ—¥ç›Šå¼ºå¤§çš„æ¨¡å‹å­¦ä¹ åˆ°çš„è¡¨å¾ã€‚ç„¶è€Œï¼Œç”±äºé«˜æ˜‚çš„è®­ç»ƒæˆæœ¬å’Œå­¦ä¹ åˆ°çš„è¡¨å¾çš„ä¸ä¸€è‡´æ€§ï¼Œä¿ƒè¿›è¡¨å¾ä¹‹é—´çš„é€šä¿¡å¹¶ç¡®ä¿ç‹¬ç«‹è®­ç»ƒçš„ç¥ç»ç½‘ç»œä¹‹é—´çš„å…¼å®¹æ€§å˜å¾—éå¸¸é‡è¦ã€‚åœ¨æ–‡çŒ®ä¸­ï¼Œé€šå¸¸ä½¿ç”¨ä¸¤ç§ä¸»è¦æ–¹æ³•æ¥è°ƒæ•´ä¸åŒçš„å­¦ä¹ è¡¨å¾ï¼šä»¿å°„å˜æ¢ï¼Œå®ƒèƒ½å¾ˆå¥½åœ°é€‚åº”ç‰¹å®šåˆ†å¸ƒï¼Œä½†ä¼šæ˜¾è‘—æ”¹å˜åŸå§‹è¡¨å¾ï¼›æ­£äº¤å˜æ¢ï¼Œå®ƒä»¥ä¸¥æ ¼çš„å‡ ä½•çº¦æŸä¿æŒåŸå§‹ç»“æ„ï¼Œä½†é™åˆ¶äº†é€‚åº”æ€§ã€‚ä¸€ä¸ªå…³é”®æŒ‘æˆ˜æ˜¯è°ƒæ•´æ›´æ–°æ¨¡å‹çš„æ½œåœ¨ç©ºé—´ï¼Œä½¿å…¶ä¸å…ˆå‰æ¨¡å‹åœ¨ä¸‹æ¸¸åˆ†å¸ƒä¸Šçš„æ½œåœ¨ç©ºé—´å¯¹é½ï¼ŒåŒæ—¶ä¿ç•™æ–°å­¦ä¹ çš„è¡¨å¾ç©ºé—´ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ–½åŠ äº†ä¸€ä¸ªå®½æ¾çš„æ­£äº¤çº¦æŸï¼Œå³Î»-æ­£äº¤æ­£åˆ™åŒ–ï¼ŒåŒæ—¶å­¦ä¹ ä»¿å°„å˜æ¢ï¼Œä»¥è·å¾—ç‰¹å®šäºåˆ†å¸ƒçš„é€‚åº”ï¼ŒåŒæ—¶ä¿ç•™åŸå§‹å­¦ä¹ çš„è¡¨å¾ã€‚è·¨å„ç§æ¶æ„å’Œæ•°æ®é›†çš„å¹¿æ³›å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œè¡¨æ˜å®ƒä¿ç•™äº†æ¨¡å‹çš„é›¶æ ·æœ¬æ€§èƒ½ï¼Œå¹¶ç¡®ä¿äº†æ¨¡å‹æ›´æ–°ä¹‹é—´çš„å…¼å®¹æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³æ¨¡å‹æ›´æ–°åï¼Œæ–°æ—§æ¨¡å‹è¡¨å¾ç©ºé—´ä¸å…¼å®¹çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚ç›´æ¥ä½¿ç”¨ä»¿å°„å˜æ¢ï¼Œè™½ç„¶å¯ä»¥é€‚åº”æ–°çš„æ•°æ®åˆ†å¸ƒï¼Œä½†ä¼šæ˜¾è‘—æ”¹å˜åŸå§‹æ¨¡å‹çš„è¡¨å¾ï¼›è€Œæ­£äº¤å˜æ¢è™½ç„¶èƒ½ä¿æŒåŸå§‹ç»“æ„ï¼Œä½†é€‚åº”æ€§æœ‰é™ã€‚å› æ­¤ï¼Œå¦‚ä½•åœ¨é€‚åº”æ–°åˆ†å¸ƒçš„åŒæ—¶ï¼Œä¿æŒæ¨¡å‹æ›´æ–°å‰åè¡¨å¾ç©ºé—´çš„ä¸€è‡´æ€§æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åœ¨å­¦ä¹ ä»¿å°„å˜æ¢çš„åŒæ—¶ï¼Œå¼•å…¥ä¸€ä¸ªå®½æ¾çš„æ­£äº¤çº¦æŸï¼Œå³Î»-æ­£äº¤æ­£åˆ™åŒ–ã€‚è¿™æ ·æ—¢èƒ½ä¿è¯æ¨¡å‹é€‚åº”æ–°çš„æ•°æ®åˆ†å¸ƒï¼Œåˆèƒ½å°½å¯èƒ½åœ°ä¿ç•™åŸå§‹æ¨¡å‹çš„è¡¨å¾ç©ºé—´ç»“æ„ã€‚é€šè¿‡æ§åˆ¶Î»çš„å€¼ï¼Œå¯ä»¥è°ƒèŠ‚æ­£äº¤çº¦æŸçš„å¼ºåº¦ï¼Œä»è€Œå¹³è¡¡é€‚åº”æ€§å’Œç»“æ„ä¿æŒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶æ˜¯åœ¨å·²æœ‰çš„æ¨¡å‹è¡¨å¾åŸºç¡€ä¸Šï¼Œå­¦ä¹ ä¸€ä¸ªä»¿å°„å˜æ¢çŸ©é˜µã€‚è¯¥ä»¿å°„å˜æ¢çŸ©é˜µå—åˆ°Î»-æ­£äº¤æ­£åˆ™åŒ–çš„çº¦æŸã€‚å…·ä½“æµç¨‹å¦‚ä¸‹ï¼š1. ä½¿ç”¨åŸå§‹æ¨¡å‹æå–ç‰¹å¾ï¼›2. å­¦ä¹ ä»¿å°„å˜æ¢çŸ©é˜µï¼Œè¯¥çŸ©é˜µå°†åŸå§‹ç‰¹å¾æ˜ å°„åˆ°æ–°çš„è¡¨å¾ç©ºé—´ï¼›3. åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨Î»-æ­£äº¤æ­£åˆ™åŒ–çº¦æŸä»¿å°„å˜æ¢çŸ©é˜µï¼Œä½¿å…¶å°½å¯èƒ½æ¥è¿‘æ­£äº¤çŸ©é˜µã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†Î»-æ­£äº¤æ­£åˆ™åŒ–ã€‚ä¸ä¼ ç»Ÿçš„æ­£äº¤æ­£åˆ™åŒ–ç›¸æ¯”ï¼ŒÎ»-æ­£äº¤æ­£åˆ™åŒ–æ›´åŠ çµæ´»ï¼Œå…è®¸ä»¿å°„å˜æ¢çŸ©é˜µåœ¨ä¸€å®šç¨‹åº¦ä¸Šåç¦»æ­£äº¤çŸ©é˜µï¼Œä»è€Œæ›´å¥½åœ°é€‚åº”æ–°çš„æ•°æ®åˆ†å¸ƒã€‚ä¸ç›´æ¥ä½¿ç”¨ä»¿å°„å˜æ¢ç›¸æ¯”ï¼ŒÎ»-æ­£äº¤æ­£åˆ™åŒ–èƒ½å¤Ÿæ›´å¥½åœ°ä¿ç•™åŸå§‹æ¨¡å‹çš„è¡¨å¾ç©ºé—´ç»“æ„ã€‚

**å…³é”®è®¾è®¡**ï¼šÎ»-æ­£äº¤æ­£åˆ™åŒ–çš„å…·ä½“å½¢å¼ä¸ºï¼š`||A^T A - Î»I||_F^2`ï¼Œå…¶ä¸­Aæ˜¯ä»¿å°„å˜æ¢çŸ©é˜µï¼ŒIæ˜¯å•ä½çŸ©é˜µï¼ŒÎ»æ˜¯ä¸€ä¸ªè¶…å‚æ•°ï¼Œç”¨äºæ§åˆ¶æ­£äº¤çº¦æŸçš„å¼ºåº¦ã€‚æŸå¤±å‡½æ•°ç”±ä¸¤éƒ¨åˆ†ç»„æˆï¼šä¸€éƒ¨åˆ†æ˜¯ä¸‹æ¸¸ä»»åŠ¡çš„æŸå¤±ï¼Œå¦ä¸€éƒ¨åˆ†æ˜¯Î»-æ­£äº¤æ­£åˆ™åŒ–é¡¹ã€‚é€šè¿‡è°ƒæ•´Î»çš„å€¼ï¼Œå¯ä»¥å¹³è¡¡ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½å’Œè¡¨å¾ç©ºé—´çš„ç»“æ„ä¿æŒã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡é€šè¿‡åœ¨å¤šä¸ªæ•°æ®é›†å’Œæ¨¡å‹æ¶æ„ä¸Šè¿›è¡Œå®éªŒï¼ŒéªŒè¯äº†Î»-æ­£äº¤æ­£åˆ™åŒ–çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨ä¿æŒæ¨¡å‹é›¶æ ·æœ¬æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—æé«˜æ¨¡å‹æ›´æ–°å‰åçš„è¡¨å¾ç©ºé—´å…¼å®¹æ€§ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œå¯¹æ¯”åŸºçº¿åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†å±•ç¤ºï¼Œè¯æ˜äº†è¯¥æ–¹æ³•ä¼˜äºç°æœ‰çš„ä»¿å°„å˜æ¢å’Œæ­£äº¤å˜æ¢æ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦æ¨¡å‹æŒç»­æ›´æ–°å’Œæ¼”è¿›çš„åœºæ™¯ï¼Œä¾‹å¦‚ï¼šæ¨èç³»ç»Ÿã€å›¾åƒæ£€ç´¢ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰ã€‚é€šè¿‡ä¿æŒæ¨¡å‹æ›´æ–°å‰åçš„è¡¨å¾ç©ºé—´å…¼å®¹æ€§ï¼Œå¯ä»¥å‡å°‘é‡æ–°è®­ç»ƒæ¨¡å‹çš„æˆæœ¬ï¼Œå¹¶æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚å°¤å…¶æ˜¯åœ¨æ•°æ®åˆ†å¸ƒä¸æ–­å˜åŒ–çš„åœºæ™¯ä¸‹ï¼Œè¯¥æ–¹æ³•å…·æœ‰é‡è¦çš„å®é™…ä»·å€¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Retrieval systems rely on representations learned by increasingly powerful models. However, due to the high training cost and inconsistencies in learned representations, there is significant interest in facilitating communication between representations and ensuring compatibility across independently trained neural networks. In the literature, two primary approaches are commonly used to adapt different learned representations: affine transformations, which adapt well to specific distributions but can significantly alter the original representation, and orthogonal transformations, which preserve the original structure with strict geometric constraints but limit adaptability. A key challenge is adapting the latent spaces of updated models to align with those of previous models on downstream distributions while preserving the newly learned representation spaces. In this paper, we impose a relaxed orthogonality constraint, namely $Î»$-Orthogonality regularization, while learning an affine transformation, to obtain distribution-specific adaptation while retaining the original learned representations. Extensive experiments across various architectures and datasets validate our approach, demonstrating that it preserves the model's zero-shot performance and ensures compatibility across model updates. Code available at: \href{https://github.com/miccunifi/lambda_orthogonality.git}{https://github.com/miccunifi/lambda\_orthogonality}.

