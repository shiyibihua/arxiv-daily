---
layout: default
title: HypeMARL: Multi-Agent Reinforcement Learning For High-Dimensional, Parametric, and Distributed Systems
---

# HypeMARL: Multi-Agent Reinforcement Learning For High-Dimensional, Parametric, and Distributed Systems

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.16709" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.16709v1</a>
  <a href="https://arxiv.org/pdf/2509.16709.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.16709v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.16709v1', 'HypeMARL: Multi-Agent Reinforcement Learning For High-Dimensional, Parametric, and Distributed Systems')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Nicol√≤ Botteghi, Matteo Tomasetto, Urban Fasel, Francesco Braghin, Andrea Manzoni

**ÂàÜÁ±ª**: cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-20

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**HypeMARLÔºöÁî®‰∫éÈ´òÁª¥„ÄÅÂèÇÊï∞ÂåñÂíåÂàÜÂ∏ÉÂºèÁ≥ªÁªüÁöÑÂ§öÊô∫ËÉΩ‰ΩìÂº∫ÂåñÂ≠¶‰π†**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊô∫ËÉΩ‰ΩìÂº∫ÂåñÂ≠¶‰π†` `Ë∂ÖÁΩëÁªú` `‰ΩçÁΩÆÁºñÁ†Å` `ÂÅèÂæÆÂàÜÊñπÁ®ãÊéßÂà∂` `ÂàÜÂ∏ÉÂºèÁ≥ªÁªü`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. ‰º†ÁªüMARLÂú®PDEÁ∫¶ÊùüÊéßÂà∂Á≠âÈóÆÈ¢ò‰∏≠ÔºåÁî±‰∫éÊô∫ËÉΩ‰ΩìÂ±ÄÈÉ®ÊÄßÈôêÂà∂ÔºåÈöæ‰ª•ÂÆûÁé∞ÂÖ®Â±ÄÊúÄ‰ºòÁöÑÈõÜ‰ΩìË°å‰∏∫„ÄÇ
2. HypeMARLÂà©Áî®Ë∂ÖÁΩëÁªúÂèÇÊï∞ÂåñÊô∫ËÉΩ‰ΩìÁ≠ñÁï•Âíå‰ª∑ÂÄºÂáΩÊï∞ÔºåÁªìÂêàÊ≠£Âº¶‰ΩçÁΩÆÁºñÁ†ÅÔºå‰ΩøÊô∫ËÉΩ‰ΩìÊÑüÁü•Á≥ªÁªüÂèÇÊï∞ÂíåÁõ∏ÂØπ‰ΩçÁΩÆ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåHypeMARLÂú®ÊéßÂà∂ÊÄßËÉΩ„ÄÅÂèÇÊï∞‰æùËµñÊÄßÂ§ÑÁêÜÂíåÊ†∑Êú¨ÊïàÁéáÊñπÈù¢‰ºò‰∫éÁé∞ÊúâÂéª‰∏≠ÂøÉÂåñMARLÁÆóÊ≥ï„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Ê∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†Â∑≤Êàê‰∏∫ÊéßÂà∂ÂÅèÂæÆÂàÜÊñπÁ®ã(PDEs)ÊèèËø∞ÁöÑÂ§çÊùÇÂä®ÊÄÅÁ≥ªÁªüÁöÑÊúâÊïàÂèçÈ¶àÊéßÂà∂Á≠ñÁï•„ÄÇÈíàÂØπÁä∂ÊÄÅÂíåÊéßÂà∂ÂèòÈáèÂùá‰∏∫È´òÁª¥ÁöÑÂàÜÂ∏ÉÂºèÈóÆÈ¢òÔºåÂ§öÊô∫ËÉΩ‰ΩìÂº∫ÂåñÂ≠¶‰π†(MARL)Ë¢´ËÆ§‰∏∫ÊòØ‰∏ÄÁßçÂèØÊâ©Â±ïÁöÑÈôçÁª¥ÊñπÊ≥ï„ÄÇÈÄöËøáÂàÜÊï£ÂºèËÆ≠ÁªÉÂíåÊâßË°åÔºåÂ§ö‰∏™Êô∫ËÉΩ‰Ωì‰ªÖ‰æùËµñÂ±ÄÈÉ®Áä∂ÊÄÅÂíåÂ•ñÂä±‰ø°ÊÅØÂçèÂêåÂ∑•‰ΩúÔºåÂºïÂØºÁ≥ªÁªüËææÂà∞ÁõÆÊ†áÁä∂ÊÄÅ„ÄÇÁÑ∂ËÄåÔºåÂΩìÊô∫ËÉΩ‰ΩìÁöÑÈõÜ‰Ωì„ÄÅÈùûÂ±ÄÈÉ®Ë°å‰∏∫ÂØπÊúÄÂ§ßÂåñÂ•ñÂä±ÂáΩÊï∞Ëá≥ÂÖ≥ÈáçË¶ÅÊó∂ÔºàÂ¶ÇPDEÁ∫¶ÊùüÁöÑÊúÄ‰ºòÊéßÂà∂ÈóÆÈ¢òÔºâÔºåÂ±ÄÈÉ®ÊÄßÂéüÂàôÂèØËÉΩÊàê‰∏∫ÈôêÂà∂Âõ†Á¥†„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜHypeMARLÔºå‰∏ÄÁßç‰∏ìÈó®‰∏∫ÊéßÂà∂È´òÁª¥„ÄÅÂèÇÊï∞ÂåñÂíåÂàÜÂ∏ÉÂºèÁ≥ªÁªüËÆæËÆ°ÁöÑÂéª‰∏≠ÂøÉÂåñMARLÁÆóÊ≥ï„ÄÇHypeMARLÈááÁî®Ë∂ÖÁΩëÁªúÊúâÊïàÂú∞ÂèÇÊï∞ÂåñÊô∫ËÉΩ‰ΩìÁöÑÁ≠ñÁï•Âíå‰ª∑ÂÄºÂáΩÊï∞Ôºå‰ΩøÂÖ∂ËÉΩÂ§üÊÑüÁü•Á≥ªÁªüÂèÇÊï∞ÂíåÊô∫ËÉΩ‰ΩìÁöÑÁõ∏ÂØπ‰ΩçÁΩÆÔºåÂêéËÄÖÈÄöËøáÊ≠£Âº¶‰ΩçÁΩÆÁºñÁ†ÅËøõË°åÁºñÁ†Å„ÄÇÂú®ÂØÜÂ∫¶ÂíåÊµÅÈáèÊéßÂà∂Á≠âÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÊéßÂà∂ÈóÆÈ¢ò‰∏äÔºåÂÆûÈ™åË°®ÊòéHypeMARL (i)ÂèØ‰ª•ÈÄöËøáÊô∫ËÉΩ‰ΩìÁöÑÈõÜ‰ΩìË°å‰∏∫ÊúâÊïàÂú∞ÊéßÂà∂Á≥ªÁªüÔºå‰ºò‰∫éÊúÄÂÖàËøõÁöÑÂéª‰∏≠ÂøÉÂåñMARLÁÆóÊ≥ïÔºå(ii)ÂèØ‰ª•ÊúâÊïàÂú∞Â§ÑÁêÜÂèÇÊï∞‰æùËµñÊÄßÔºå(iii)ÈúÄË¶ÅÊúÄÂ∞ëÁöÑË∂ÖÂèÇÊï∞Ë∞ÉÊï¥Ôºå‰ª•Âèä(iv)ÈÄöËøáÂÖ∂Âü∫‰∫éÊ®°ÂûãÁöÑÊâ©Â±ïMB-HypeMARLÔºåÂèØ‰ª•ÂáèÂ∞ëÁ∫¶10ÂÄçÁöÑÊòÇË¥µÁéØÂ¢É‰∫§‰∫íÔºåMB-HypeMARL‰æùËµñ‰∫éËÆ°ÁÆóÈ´òÊïàÁöÑÂü∫‰∫éÊ∑±Â∫¶Â≠¶‰π†ÁöÑÊõø‰ª£Ê®°ÂûãÔºå‰ª•ÊúÄÂ∞èÁöÑÁ≠ñÁï•ÊÄßËÉΩ‰∏ãÈôçÊù•Â±ÄÈÉ®ÈÄºËøëÂä®ÊÄÅ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥È´òÁª¥„ÄÅÂèÇÊï∞ÂåñÂíåÂàÜÂ∏ÉÂºèÁ≥ªÁªüÁöÑÊéßÂà∂ÈóÆÈ¢òÔºåÁâπÂà´ÊòØÈÇ£‰∫õÈúÄË¶ÅÊô∫ËÉΩ‰Ωì‰πãÈó¥ËøõË°åÈùûÂ±ÄÈÉ®Âçè‰ΩúÊâçËÉΩÂÆûÁé∞ÊúÄ‰ºòÊéßÂà∂ÁöÑÂú∫ÊôØ„ÄÇÁé∞ÊúâÁöÑÂéª‰∏≠ÂøÉÂåñMARLÊñπÊ≥ïÁî±‰∫éÊô∫ËÉΩ‰ΩìÂè™‰æùËµñÂ±ÄÈÉ®‰ø°ÊÅØÔºåÈöæ‰ª•Â≠¶‰π†Âà∞ÂÖ®Â±ÄÊúÄ‰ºòÁöÑÈõÜ‰ΩìË°å‰∏∫ÔºåÂØºËá¥ÊéßÂà∂ÊÄßËÉΩÂèóÈôê„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®Ë∂ÖÁΩëÁªúÊù•ÂèÇÊï∞ÂåñÊØè‰∏™Êô∫ËÉΩ‰ΩìÁöÑÁ≠ñÁï•Âíå‰ª∑ÂÄºÂáΩÊï∞Ôºå‰ΩøÂæóÊô∫ËÉΩ‰ΩìÁöÑË°å‰∏∫ËÉΩÂ§ü‰æùËµñ‰∫éÂÖ®Â±ÄÁ≥ªÁªüÂèÇÊï∞‰ª•ÂèäÊô∫ËÉΩ‰Ωì‰πãÈó¥ÁöÑÁõ∏ÂØπ‰ΩçÁΩÆÂÖ≥Á≥ª„ÄÇÈÄöËøáËøôÁßçÊñπÂºèÔºåÊô∫ËÉΩ‰ΩìÂèØ‰ª•Â≠¶‰π†Âà∞Êõ¥ÊúâÊïàÁöÑÈõÜ‰ΩìË°å‰∏∫Ôºå‰ªéËÄåÊèêÂçáÊï¥‰ΩìÊéßÂà∂ÊÄßËÉΩ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöHypeMARLÈááÁî®Âéª‰∏≠ÂøÉÂåñÁöÑËÆ≠ÁªÉÂíåÊâßË°åÊ°ÜÊû∂„ÄÇÊØè‰∏™Êô∫ËÉΩ‰ΩìÈÉΩÊúâËá™Â∑±ÁöÑÁ≠ñÁï•Âíå‰ª∑ÂÄºÂáΩÊï∞ÔºåËøô‰∫õÂáΩÊï∞Áî±Ë∂ÖÁΩëÁªúÂèÇÊï∞Âåñ„ÄÇË∂ÖÁΩëÁªúÊé•Êî∂Á≥ªÁªüÂèÇÊï∞ÂíåÊô∫ËÉΩ‰ΩìÁöÑ‰ΩçÁΩÆÁºñÁ†Å‰Ωú‰∏∫ËæìÂÖ•ÔºåÁîüÊàêÊô∫ËÉΩ‰ΩìÁ≠ñÁï•Âíå‰ª∑ÂÄºÂáΩÊï∞ÁöÑÂèÇÊï∞„ÄÇÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÔºåÊô∫ËÉΩ‰ΩìÊ†πÊçÆÂ±ÄÈÉ®Áä∂ÊÄÅÂíåÂ•ñÂä±‰ø°ÊÅØËøõË°åÂ≠¶‰π†„ÄÇ‰∏∫‰∫ÜÊèêÈ´òÊ†∑Êú¨ÊïàÁéáÔºåËÆ∫ÊñáËøòÊèêÂá∫‰∫ÜMB-HypeMARLÔºåÂÆÉ‰ΩøÁî®Ê∑±Â∫¶Â≠¶‰π†Ê®°ÂûãÊù•Ëøë‰ººÁéØÂ¢ÉÂä®ÊÄÅÔºå‰ªéËÄåÂáèÂ∞ë‰∏éÁúüÂÆûÁéØÂ¢ÉÁöÑ‰∫§‰∫íÊ¨°Êï∞„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöHypeMARLÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫é‰ΩøÁî®Ë∂ÖÁΩëÁªúÊù•ÂèÇÊï∞ÂåñÊô∫ËÉΩ‰ΩìÁöÑÁ≠ñÁï•Âíå‰ª∑ÂÄºÂáΩÊï∞ÔºåÂπ∂ÁªìÂêàÊ≠£Âº¶‰ΩçÁΩÆÁºñÁ†ÅÊù•Ë°®Á§∫Êô∫ËÉΩ‰Ωì‰πãÈó¥ÁöÑÁõ∏ÂØπ‰ΩçÁΩÆÂÖ≥Á≥ª„ÄÇËøôÁßçÊñπÊ≥ï‰ΩøÂæóÊô∫ËÉΩ‰ΩìËÉΩÂ§üÊÑüÁü•ÂÖ®Â±Ä‰ø°ÊÅØÔºå‰ªéËÄåÂ≠¶‰π†Âà∞Êõ¥ÊúâÊïàÁöÑÈõÜ‰ΩìË°å‰∏∫„ÄÇ‰∏é‰º†ÁªüÁöÑÂéª‰∏≠ÂøÉÂåñMARLÊñπÊ≥ïÁõ∏ÊØîÔºåHypeMARLËÉΩÂ§üÊõ¥Â•ΩÂú∞Â§ÑÁêÜÈúÄË¶ÅÈùûÂ±ÄÈÉ®Âçè‰ΩúÁöÑÊéßÂà∂ÈóÆÈ¢ò„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöHypeMARL‰ΩøÁî®Ê≠£Âº¶‰ΩçÁΩÆÁºñÁ†ÅÊù•Ë°®Á§∫Êô∫ËÉΩ‰ΩìÁöÑ‰ΩçÁΩÆ‰ø°ÊÅØÔºåËøôÊòØ‰∏ÄÁßçÂ∏∏Áî®ÁöÑ‰ΩçÁΩÆÁºñÁ†ÅÊñπÊ≥ïÔºåÂèØ‰ª•ÊúâÊïàÂú∞Ë°®Á§∫È´òÁª¥Á©∫Èó¥‰∏≠ÁöÑ‰ΩçÁΩÆÂÖ≥Á≥ª„ÄÇË∂ÖÁΩëÁªúÁöÑËÆæËÆ°ÈúÄË¶Å‰ªîÁªÜËÄÉËôëÔºå‰ª•Á°Æ‰øùÂÖ∂ËÉΩÂ§üÊúâÊïàÂú∞ÊèêÂèñÁ≥ªÁªüÂèÇÊï∞Âíå‰ΩçÁΩÆÁºñÁ†Å‰∏≠ÁöÑ‰ø°ÊÅØÔºåÂπ∂ÁîüÊàêÂêàÈÄÇÁöÑÁ≠ñÁï•Âíå‰ª∑ÂÄºÂáΩÊï∞ÂèÇÊï∞„ÄÇMB-HypeMARL‰∏≠ÁöÑÁéØÂ¢ÉÂä®ÊÄÅÊ®°ÂûãÂèØ‰ª•‰ΩøÁî®ÂêÑÁßçÊ∑±Â∫¶Â≠¶‰π†Ê®°ÂûãÊù•ÊûÑÂª∫Ôºå‰æãÂ¶ÇÁ•ûÁªèÁΩëÁªúÊàñÈ´òÊñØËøáÁ®ã„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåHypeMARLÂú®ÂØÜÂ∫¶ÂíåÊµÅÈáèÊéßÂà∂Á≠âÈóÆÈ¢ò‰∏ä‰ºò‰∫éÁé∞ÊúâÁöÑÂéª‰∏≠ÂøÉÂåñMARLÁÆóÊ≥ï„ÄÇÈÄöËøáÊô∫ËÉΩ‰ΩìÁöÑÈõÜ‰ΩìË°å‰∏∫ÔºåHypeMARLËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞ÊéßÂà∂Á≥ªÁªü„ÄÇÊ≠§Â§ñÔºåMB-HypeMARLÈÄöËøá‰ΩøÁî®Âü∫‰∫éÊ∑±Â∫¶Â≠¶‰π†ÁöÑÊõø‰ª£Ê®°ÂûãÔºåËÉΩÂ§üÂ∞ÜÁéØÂ¢É‰∫§‰∫íÊ¨°Êï∞ÂáèÂ∞ëÁ∫¶10ÂÄçÔºåÂêåÊó∂‰øùÊåÅËâØÂ•ΩÁöÑÁ≠ñÁï•ÊÄßËÉΩ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

HypeMARLÈÄÇÁî®‰∫éÂêÑÁßçÈ´òÁª¥„ÄÅÂèÇÊï∞ÂåñÂíåÂàÜÂ∏ÉÂºèÁ≥ªÁªüÁöÑÊéßÂà∂ÈóÆÈ¢òÔºå‰æãÂ¶ÇÔºö‰∫§ÈÄöÊµÅÈáè‰ºòÂåñ„ÄÅËÉΩÊ∫êÂàÜÈÖç„ÄÅÊú∫Âô®‰∫∫ÈõÜÁæ§ÊéßÂà∂„ÄÅ‰ª•ÂèäÂÖ∂‰ªñÊ∂âÂèäÂÅèÂæÆÂàÜÊñπÁ®ãÁ∫¶ÊùüÁöÑÊéßÂà∂‰ªªÂä°„ÄÇËØ•ÊñπÊ≥ïËÉΩÂ§üÊèêÂçáÊéßÂà∂ÊÄßËÉΩÔºåÈôç‰ΩéËÆ°ÁÆóÊàêÊú¨ÔºåÂπ∂ÂáèÂ∞ëÂØπÁéØÂ¢ÉÁöÑ‰∫§‰∫íÊ¨°Êï∞ÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖÂ∫îÁî®‰ª∑ÂÄº„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Deep reinforcement learning has recently emerged as a promising feedback control strategy for complex dynamical systems governed by partial differential equations (PDEs). When dealing with distributed, high-dimensional problems in state and control variables, multi-agent reinforcement learning (MARL) has been proposed as a scalable approach for breaking the curse of dimensionality. In particular, through decentralized training and execution, multiple agents cooperate to steer the system towards a target configuration, relying solely on local state and reward information. However, the principle of locality may become a limiting factor whenever a collective, nonlocal behavior of the agents is crucial to maximize the reward function, as typically happens in PDE-constrained optimal control problems. In this work, we propose HypeMARL: a decentralized MARL algorithm tailored to the control of high-dimensional, parametric, and distributed systems. HypeMARL employs hypernetworks to effectively parametrize the agents' policies and value functions with respect to the system parameters and the agents' relative positions, encoded by sinusoidal positional encoding. Through the application on challenging control problems, such as density and flow control, we show that HypeMARL (i) can effectively control systems through a collective behavior of the agents, outperforming state-of-the-art decentralized MARL, (ii) can efficiently deal with parametric dependencies, (iii) requires minimal hyperparameter tuning and (iv) can reduce the amount of expensive environment interactions by a factor of ~10 thanks to its model-based extension, MB-HypeMARL, which relies on computationally efficient deep learning-based surrogate models approximating the dynamics locally, with minimal deterioration of the policy performance.

