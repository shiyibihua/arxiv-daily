---
layout: default
title: Geometric Mixture Classifier (GMC): A Discriminative Per-Class Mixture of Hyperplanes
---

# Geometric Mixture Classifier (GMC): A Discriminative Per-Class Mixture of Hyperplanes

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.16769" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.16769v1</a>
  <a href="https://arxiv.org/pdf/2509.16769.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.16769v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.16769v1', 'Geometric Mixture Classifier (GMC): A Discriminative Per-Class Mixture of Hyperplanes')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Prasanth K K, Shubham Sharma

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-20

**å¤‡æ³¨**: 21 pages, 6 figures, 14 tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå‡ ä½•æ··åˆåˆ†ç±»å™¨(GMC)ï¼Œç”¨æ¯ç±»è¶…å¹³é¢æ··åˆæ¨¡å‹è§£å†³å¤šæ¨¡æ€åˆ†ç±»é—®é¢˜ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€åˆ†ç±»` `è¶…å¹³é¢æ··åˆæ¨¡å‹` `å‡ ä½•æ··åˆåˆ†ç±»å™¨` `è½¯OR` `éšæœºå‚…é‡Œå¶ç‰¹å¾` `å¯è§£é‡Šæ€§` `é«˜æ•ˆæ¨ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»Ÿçº¿æ€§æ¨¡å‹åœ¨å¤„ç†å¤šæ¨¡æ€æ•°æ®æ—¶æ€§èƒ½å—é™ï¼Œè€Œé«˜å®¹é‡æ¨¡å‹ç‰ºç‰²äº†è§£é‡Šæ€§å’Œæ•ˆç‡ã€‚
2. GMCå°†æ¯ä¸ªç±»åˆ«è¡¨ç¤ºä¸ºè¶…å¹³é¢çš„æ··åˆï¼Œé€šè¿‡è½¯ORç»„åˆå¹³é¢åˆ†æ•°ï¼Œå¹¶ä½¿ç”¨RFFè¿›è¡Œéçº¿æ€§æ˜ å°„ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒGMCåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šä¼˜äºçº¿æ€§åŸºçº¿ï¼Œå¹¶ä¸RBF-SVMç­‰æ¨¡å‹å…·æœ‰ç«äº‰åŠ›ï¼ŒåŒæ—¶ä¿æŒäº†é«˜æ•ˆçš„æ¨ç†é€Ÿåº¦ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è®¸å¤šç°å®ä¸–ç•Œçš„ç±»åˆ«æ˜¯å¤šæ¨¡æ€çš„ï¼Œå•ä¸ªç±»åˆ«åœ¨ç‰¹å¾ç©ºé—´ä¸­å æ®ä¸ç›¸äº¤çš„åŒºåŸŸã€‚ä¼ ç»Ÿçš„çº¿æ€§æ¨¡å‹ï¼ˆå¦‚é€»è¾‘å›å½’ã€çº¿æ€§SVMï¼‰ä½¿ç”¨å•ä¸ªå…¨å±€è¶…å¹³é¢ï¼Œåœ¨è¿™ç§æ•°æ®ä¸Šè¡¨ç°ä¸ä½³ï¼Œè€Œé«˜å®¹é‡æ–¹æ³•ï¼ˆå¦‚æ ¸SVMã€æ·±åº¦ç½‘ç»œï¼‰è™½ç„¶å¯ä»¥æ‹Ÿåˆå¤šæ¨¡æ€ç»“æ„ï¼Œä½†ç‰ºç‰²äº†è§£é‡Šæ€§ã€éœ€è¦æ›´é‡çš„è°ƒå‚å’Œæ›´é«˜çš„è®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬æå‡ºäº†å‡ ä½•æ··åˆåˆ†ç±»å™¨ï¼ˆGMCï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åˆ¤åˆ«æ¨¡å‹ï¼Œå°†æ¯ä¸ªç±»åˆ«è¡¨ç¤ºä¸ºè¶…å¹³é¢çš„æ··åˆã€‚åœ¨æ¯ä¸ªç±»åˆ«ä¸­ï¼ŒGMCé€šè¿‡æ¸©åº¦æ§åˆ¶çš„è½¯ORï¼ˆlog-sum-expï¼‰ç»„åˆå¹³é¢åˆ†æ•°ï¼Œå¹³æ»‘åœ°é€¼è¿‘æœ€å¤§å€¼ï¼›åœ¨ç±»åˆ«ä¹‹é—´ï¼Œæ ‡å‡†softmaxäº§ç”Ÿæ¦‚ç‡åéªŒã€‚GMCå¯ä»¥é€‰æ‹©ä½¿ç”¨éšæœºå‚…é‡Œå¶ç‰¹å¾ï¼ˆRFFï¼‰è¿›è¡Œéçº¿æ€§æ˜ å°„ï¼ŒåŒæ—¶ä¿æŒæ¨ç†åœ¨å¹³é¢å’Œç‰¹å¾æ•°é‡ä¸Šçš„çº¿æ€§ã€‚æˆ‘ä»¬æå‡ºçš„å®ç”¨è®­ç»ƒæ–¹æ³•ï¼šå‡ ä½•æ„ŸçŸ¥k-meansåˆå§‹åŒ–ã€åŸºäºè½®å»“ç³»æ•°çš„å¹³é¢é¢„ç®—ã€alphaé€€ç«ã€ä½¿ç”¨æ„ŸçŸ¥L2æ­£åˆ™åŒ–ã€æ ‡ç­¾å¹³æ»‘å’Œæ—©åœï¼Œä½¿GMCå³æ’å³ç”¨ã€‚åœ¨åˆæˆå¤šæ¨¡æ€æ•°æ®é›†ï¼ˆmoons, circles, blobs, spiralsï¼‰å’Œè¡¨æ ¼/å›¾åƒåŸºå‡†ï¼ˆiris, wine, WDBC, digitsï¼‰ä¸Šï¼ŒGMCå§‹ç»ˆä¼˜äºçº¿æ€§åŸºçº¿å’Œk-NNï¼Œä¸RBF-SVMã€éšæœºæ£®æ—å’Œå°å‹MLPå…·æœ‰ç«äº‰åŠ›ï¼Œå¹¶é€šè¿‡æ¯ä¸ªå¹³é¢å’Œç±»åˆ«è´£ä»»å¯è§†åŒ–æä¾›å‡ ä½•è‡ªçœã€‚æ¨ç†åœ¨å¹³é¢å’Œç‰¹å¾ä¸Šçº¿æ€§ç¼©æ”¾ï¼Œä½¿GMCå¯¹CPUå‹å¥½ï¼Œæ¯ä¸ªç¤ºä¾‹çš„å»¶è¿Ÿä¸ºä¸ªä½æ•°å¾®ç§’ï¼Œé€šå¸¸æ¯”RBF-SVMå’Œç´§å‡‘å‹MLPæ›´å¿«ã€‚äº‹åæ¸©åº¦ç¼©æ”¾å°†ECEä»å¤§çº¦0.06é™ä½åˆ°0.02ã€‚å› æ­¤ï¼ŒGMCåœ¨å‡†ç¡®æ€§ã€å¯è§£é‡Šæ€§å’Œæ•ˆç‡ä¹‹é—´å–å¾—äº†è‰¯å¥½çš„å¹³è¡¡ï¼šå®ƒæ¯”çº¿æ€§æ¨¡å‹æ›´å…·è¡¨ç°åŠ›ï¼Œå¹¶ä¸”æ¯”æ ¸æ¨¡å‹æˆ–æ·±åº¦æ¨¡å‹æ›´è½»ã€æ›´é€æ˜ã€æ›´å¿«ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€åˆ†ç±»é—®é¢˜ï¼Œå³ç±»åˆ«åœ¨ç‰¹å¾ç©ºé—´ä¸­å‘ˆç°å¤šä¸ªä¸ç›¸äº¤åŒºåŸŸçš„æƒ…å†µã€‚ç°æœ‰çº¿æ€§æ¨¡å‹æ— æ³•æœ‰æ•ˆå¤„ç†æ­¤ç±»æ•°æ®ï¼Œè€Œé«˜å®¹é‡æ¨¡å‹ï¼ˆå¦‚æ·±åº¦å­¦ä¹ ï¼‰è™½ç„¶å¯ä»¥æ‹Ÿåˆå¤æ‚åˆ†å¸ƒï¼Œä½†è®¡ç®—æˆæœ¬é«˜æ˜‚ä¸”ç¼ºä¹å¯è§£é‡Šæ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†æ¯ä¸ªç±»åˆ«è¡¨ç¤ºä¸ºå¤šä¸ªè¶…å¹³é¢çš„æ··åˆã€‚æ¯ä¸ªè¶…å¹³é¢æ•æ‰ç±»åˆ«çš„ä¸€ä¸ªæ¨¡æ€ï¼Œé€šè¿‡ç»„åˆå¤šä¸ªè¶…å¹³é¢çš„è¾“å‡ºæ¥å®ç°å¯¹å¤šæ¨¡æ€æ•°æ®çš„æœ‰æ•ˆå»ºæ¨¡ã€‚è¿™ç§æ–¹æ³•åœ¨è¡¨è¾¾èƒ½åŠ›å’Œè®¡ç®—æ•ˆç‡ä¹‹é—´å–å¾—äº†å¹³è¡¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGMCçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼š1) ä½¿ç”¨å‡ ä½•æ„ŸçŸ¥k-meansåˆå§‹åŒ–è¶…å¹³é¢ï¼›2) æ ¹æ®è½®å»“ç³»æ•°è¿›è¡Œå¹³é¢é¢„ç®—ï¼Œæ§åˆ¶æ¨¡å‹å¤æ‚åº¦ï¼›3) ä½¿ç”¨æ¸©åº¦æ§åˆ¶çš„è½¯ORï¼ˆlog-sum-expï¼‰ç»„åˆåŒä¸€ç±»åˆ«å†…ä¸åŒè¶…å¹³é¢çš„è¾“å‡ºï¼›4) ä½¿ç”¨softmaxå‡½æ•°è¿›è¡Œè·¨ç±»åˆ«åˆ†ç±»ï¼›5) å¯é€‰åœ°ä½¿ç”¨éšæœºå‚…é‡Œå¶ç‰¹å¾ï¼ˆRFFï¼‰è¿›è¡Œéçº¿æ€§æ˜ å°„ã€‚

**å…³é”®åˆ›æ–°**ï¼šGMCçš„å…³é”®åˆ›æ–°åœ¨äºä½¿ç”¨è¶…å¹³é¢æ··åˆæ¨¡å‹æ¥è¡¨ç¤ºæ¯ä¸ªç±»åˆ«ï¼Œå¹¶é‡‡ç”¨è½¯ORæ“ä½œè¿›è¡Œç»„åˆã€‚è¿™ç§æ–¹æ³•æ—¢èƒ½æ•æ‰å¤šæ¨¡æ€æ•°æ®çš„å¤æ‚ç»“æ„ï¼Œåˆèƒ½ä¿æŒæ¨¡å‹çš„çº¿æ€§æ€§å’Œå¯è§£é‡Šæ€§ã€‚æ­¤å¤–ï¼Œå‡ ä½•æ„ŸçŸ¥åˆå§‹åŒ–å’Œå¹³é¢é¢„ç®—ç­–ç•¥æœ‰åŠ©äºæé«˜æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šGMCçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å‡ ä½•æ„ŸçŸ¥k-meansåˆå§‹åŒ–ï¼Œç¡®ä¿è¶…å¹³é¢èƒ½å¤Ÿè¦†ç›–ç±»åˆ«çš„ä¸åŒæ¨¡æ€ï¼›2) åŸºäºè½®å»“ç³»æ•°çš„å¹³é¢é¢„ç®—ï¼Œé¿å…æ¨¡å‹è¿‡æ‹Ÿåˆï¼›3) æ¸©åº¦å‚æ•°æ§åˆ¶è½¯ORçš„å¹³æ»‘ç¨‹åº¦ï¼Œå½±å“æ¨¡å‹å¯¹ä¸åŒæ¨¡æ€çš„æ•æ„Ÿåº¦ï¼›4) ä½¿ç”¨ä½¿ç”¨æ„ŸçŸ¥L2æ­£åˆ™åŒ–ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆï¼›5) ä½¿ç”¨æ ‡ç­¾å¹³æ»‘å’Œæ—©åœè¿›ä¸€æ­¥æé«˜æ³›åŒ–æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

GMCåœ¨åˆæˆå¤šæ¨¡æ€æ•°æ®é›†å’ŒçœŸå®æ•°æ®é›†ä¸Šå‡å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šï¼ŒGMCä¼˜äºçº¿æ€§åŸºçº¿å’Œk-NNï¼Œå¹¶ä¸RBF-SVMã€éšæœºæ£®æ—å’Œå°å‹MLPå…·æœ‰ç«äº‰åŠ›ã€‚æ­¤å¤–ï¼ŒGMCå…·æœ‰é«˜æ•ˆçš„æ¨ç†é€Ÿåº¦ï¼Œæ¯ä¸ªç¤ºä¾‹çš„å»¶è¿Ÿä¸ºä¸ªä½æ•°å¾®ç§’ï¼Œé€šå¸¸æ¯”RBF-SVMå’Œç´§å‡‘å‹MLPæ›´å¿«ã€‚äº‹åæ¸©åº¦ç¼©æ”¾å°†ECEä»å¤§çº¦0.06é™ä½åˆ°0.02ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

GMCé€‚ç”¨äºéœ€è¦å¤„ç†å¤šæ¨¡æ€æ•°æ®çš„åˆ†ç±»ä»»åŠ¡ï¼Œä¾‹å¦‚å›¾åƒè¯†åˆ«ã€æ–‡æœ¬åˆ†ç±»ã€ç”Ÿç‰©ä¿¡æ¯å­¦ç­‰ã€‚å…¶é«˜æ•ˆçš„æ¨ç†é€Ÿåº¦å’Œè‰¯å¥½çš„å¯è§£é‡Šæ€§ä½¿å…¶åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­ä¹Ÿå…·æœ‰åº”ç”¨æ½œåŠ›ã€‚æœªæ¥å¯ä»¥æ¢ç´¢GMCåœ¨æ›´å¤æ‚çš„å¤šæ¨¡æ€æ•°æ®é›†ä¸Šçš„åº”ç”¨ï¼Œå¹¶ç ”ç©¶å¦‚ä½•è‡ªåŠ¨ç¡®å®šè¶…å¹³é¢çš„æ•°é‡å’Œå‚æ•°ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Many real world categories are multimodal, with single classes occupying disjoint regions in feature space. Classical linear models (logistic regression, linear SVM) use a single global hyperplane and perform poorly on such data, while high-capacity methods (kernel SVMs, deep nets) fit multimodal structure but at the expense of interpretability, heavier tuning, and higher computational cost. We propose the Geometric Mixture Classifier (GMC), a discriminative model that represents each class as a mixture of hyperplanes. Within each class, GMC combines plane scores via a temperature-controlled soft-OR (log-sum-exp), smoothly approximating the max; across classes, standard softmax yields probabilistic posteriors. GMC optionally uses Random Fourier Features (RFF) for nonlinear mappings while keeping inference linear in the number of planes and features. Our practical training recipe: geometry-aware k-means initialization, silhouette-based plane budgeting, alpha annealing, usage-aware L2 regularization, label smoothing, and early stopping, makes GMC plug-and-play. Across synthetic multimodal datasets (moons, circles, blobs, spirals) and tabular/image benchmarks (iris, wine, WDBC, digits), GMC consistently outperforms linear baselines and k-NN, is competitive with RBF-SVM, Random Forests, and small MLPs, and provides geometric introspection via per-plane and class responsibility visualizations. Inference scales linearly in planes and features, making GMC CPU-friendly, with single-digit microsecond latency per example, often faster than RBF-SVM and compact MLPs. Post-hoc temperature scaling reduces ECE from about 0.06 to 0.02. GMC thus strikes a favorable balance of accuracy, interpretability, and efficiency: it is more expressive than linear models and lighter, more transparent, and faster than kernel or deep models.

