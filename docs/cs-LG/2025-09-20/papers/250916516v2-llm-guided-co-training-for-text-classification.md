---
layout: default
title: LLM-Guided Co-Training for Text Classification
---

# LLM-Guided Co-Training for Text Classification

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.16516" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.16516v2</a>
  <a href="https://arxiv.org/pdf/2509.16516.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.16516v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.16516v2', 'LLM-Guided Co-Training for Text Classification')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Md Mezbaur Rahman, Cornelia Caragea

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-20 (æ›´æ–°: 2025-09-23)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLLMå¼•å¯¼çš„ååŒè®­ç»ƒæ–¹æ³•ï¼Œæå‡æ–‡æœ¬åˆ†ç±»åœ¨åŠç›‘ç£å­¦ä¹ ä¸­çš„æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ–‡æœ¬åˆ†ç±»` `åŠç›‘ç£å­¦ä¹ ` `ååŒè®­ç»ƒ` `å¤§å‹è¯­è¨€æ¨¡å‹` `ä¼ªæ ‡ç­¾`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŠç›‘ç£å­¦ä¹ æ–¹æ³•åœ¨åˆ©ç”¨æœªæ ‡æ³¨æ•°æ®æ—¶æ•ˆç‡è¾ƒä½ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®é‡åºå¤§æ—¶ï¼Œéš¾ä»¥å……åˆ†æŒ–æ˜ä¿¡æ¯ã€‚
2. åˆ©ç”¨LLMçš„çŸ¥è¯†ï¼Œä¸ºæœªæ ‡æ³¨æ•°æ®ç”Ÿæˆä¼ªæ ‡ç­¾ï¼Œå¹¶è®¾è®¡åŠ¨æ€åŠ æƒæœºåˆ¶ï¼ŒæŒ‡å¯¼ä¸¤ä¸ªç¼–ç å™¨ç½‘ç»œè¿›è¡ŒååŒè®­ç»ƒã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ–‡æœ¬åˆ†ç±»æ•°æ®é›†ä¸Šè¶…è¶Šäº†ç°æœ‰åŠç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç”±å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¼•å¯¼çš„åŠ æƒååŒè®­ç»ƒæ–¹æ³•ã€‚åœ¨è¯¥æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨LLMå¯¹æœªæ ‡æ³¨æ•°æ®ç”Ÿæˆçš„æ ‡ç­¾ä½œä¸ºç›®æ ‡æ ‡ç­¾ï¼Œå¹¶ååŒè®­ç»ƒä¸¤ä¸ªåŸºäºç¼–ç å™¨çš„ç½‘ç»œï¼Œè¿™ä¸¤ä¸ªç½‘ç»œé€šè¿‡å¤šæ¬¡è¿­ä»£äº’ç›¸è®­ç»ƒï¼šé¦–å…ˆï¼Œæ‰€æœ‰æ ·æœ¬é€šè¿‡æ¯ä¸ªç½‘ç»œè¿›è¡Œå‰å‘ä¼ æ’­ï¼Œå¹¶è®°å½•æ¯ä¸ªç½‘ç»œå¯¹LLMæ ‡ç­¾ç½®ä¿¡åº¦çš„å†å²ä¼°è®¡ï¼›å…¶æ¬¡ï¼Œæ ¹æ®æ¯ä¸ªç½‘ç»œå¯¹LLMæ ‡ç­¾è´¨é‡çš„ç½®ä¿¡åº¦ï¼Œä¸ºæ¯ä¸ªæ ·æœ¬æ¨å¯¼å‡ºä¸€ä¸ªåŠ¨æ€é‡è¦æ€§æƒé‡ï¼›æœ€åï¼Œä¸¤ä¸ªç½‘ç»œç›¸äº’äº¤æ¢é‡è¦æ€§æƒé‡â€”â€”æ¯ä¸ªç½‘ç»œåå‘ä¼ æ’­æ‰€æœ‰æ ·æœ¬ï¼Œå¹¶ä½¿ç”¨æ¥è‡ªå…¶å¯¹ç­‰ç½‘ç»œçš„é‡è¦æ€§æƒé‡è¿›è¡ŒåŠ æƒï¼Œä»è€Œæ›´æ–°å…¶è‡ªèº«å‚æ•°ã€‚é€šè¿‡ç­–ç•¥æ€§åœ°åˆ©ç”¨LLMç”Ÿæˆçš„æŒ‡å¯¼ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„åŠç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œå°¤å…¶æ˜¯åœ¨å…·æœ‰å¤§é‡æœªæ ‡æ³¨æ•°æ®çš„åœºæ™¯ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨äº”ä¸ªåŸºå‡†æ•°æ®é›†ä¸­çš„å››ä¸ªä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨14ç§æ¯”è¾ƒæ–¹æ³•ä¸­ï¼Œæ ¹æ®Friedmanæ£€éªŒæ’åç¬¬ä¸€ã€‚æˆ‘ä»¬çš„ç»“æœçªå‡ºäº†åŠç›‘ç£å­¦ä¹ çš„ä¸€ä¸ªæ–°æ–¹å‘â€”â€”LLMä½œä¸ºçŸ¥è¯†æ”¾å¤§å™¨ï¼Œä½¿éª¨å¹²ååŒè®­ç»ƒæ¨¡å‹èƒ½å¤Ÿé«˜æ•ˆåœ°å®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­ï¼ŒåŠç›‘ç£å­¦ä¹ æ–¹æ³•åœ¨åˆ©ç”¨å¤§é‡æœªæ ‡æ³¨æ•°æ®æ—¶æ€§èƒ½æå‡æœ‰é™çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆåˆ©ç”¨æœªæ ‡æ³¨æ•°æ®ä¸­è•´å«çš„ä¿¡æ¯ï¼Œå¯¼è‡´æ¨¡å‹æ€§èƒ½å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¼ºå¤§çŸ¥è¯†ï¼Œä¸ºæœªæ ‡æ³¨æ•°æ®ç”Ÿæˆä¼ªæ ‡ç­¾ï¼Œå¹¶å°†å…¶ä½œä¸ºç›®æ ‡æ ‡ç­¾æ¥æŒ‡å¯¼ååŒè®­ç»ƒè¿‡ç¨‹ã€‚é€šè¿‡LLMçš„æŒ‡å¯¼ï¼Œæ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å­¦ä¹ æœªæ ‡æ³¨æ•°æ®ä¸­çš„ä¿¡æ¯ï¼Œä»è€Œæå‡åˆ†ç±»æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦é˜¶æ®µï¼š1) ä½¿ç”¨LLMä¸ºæœªæ ‡æ³¨æ•°æ®ç”Ÿæˆä¼ªæ ‡ç­¾ï¼›2) åˆå§‹åŒ–ä¸¤ä¸ªåŸºäºç¼–ç å™¨çš„ç½‘ç»œï¼›3) è¿­ä»£è®­ç»ƒï¼šæ¯ä¸ªç½‘ç»œå¯¹æ‰€æœ‰æ ·æœ¬è¿›è¡Œå‰å‘ä¼ æ’­ï¼Œè¯„ä¼°å¯¹LLMæ ‡ç­¾çš„ç½®ä¿¡åº¦ï¼Œå¹¶è®¡ç®—åŠ¨æ€é‡è¦æ€§æƒé‡ï¼›ä¸¤ä¸ªç½‘ç»œäº¤æ¢é‡è¦æ€§æƒé‡ï¼Œå¹¶ä½¿ç”¨å¯¹æ–¹çš„æƒé‡æ›´æ–°è‡ªèº«å‚æ•°ã€‚

**å…³é”®åˆ›æ–°**ï¼šå…³é”®åˆ›æ–°åœ¨äºå°†LLMçš„çŸ¥è¯†èå…¥åˆ°ååŒè®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå¹¶è®¾è®¡äº†åŠ¨æ€é‡è¦æ€§æƒé‡æœºåˆ¶ã€‚ä¼ ç»ŸååŒè®­ç»ƒæ–¹æ³•é€šå¸¸å¹³ç­‰å¯¹å¾…æ‰€æœ‰æœªæ ‡æ³¨æ•°æ®ï¼Œè€Œè¯¥æ–¹æ³•æ ¹æ®ç½‘ç»œå¯¹LLMæ ‡ç­¾çš„ç½®ä¿¡åº¦åŠ¨æ€è°ƒæ•´æ ·æœ¬æƒé‡ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°åˆ©ç”¨é«˜è´¨é‡çš„ä¼ªæ ‡ç­¾ã€‚

**å…³é”®è®¾è®¡**ï¼šåŠ¨æ€é‡è¦æ€§æƒé‡çš„è®¡ç®—åŸºäºç½‘ç»œå¯¹LLMæ ‡ç­¾çš„ç½®ä¿¡åº¦ã€‚å…·ä½“è€Œè¨€ï¼Œç½‘ç»œä¼šè®°å½•å¯¹æ¯ä¸ªæ ·æœ¬LLMæ ‡ç­¾çš„ç½®ä¿¡åº¦å†å²ä¼°è®¡ï¼Œå¹¶æ ¹æ®è¿™äº›å†å²ä¼°è®¡è®¡ç®—é‡è¦æ€§æƒé‡ã€‚æŸå¤±å‡½æ•°é‡‡ç”¨äº¤å‰ç†µæŸå¤±ï¼Œä¼˜åŒ–å™¨é‡‡ç”¨Adamã€‚ç½‘ç»œç»“æ„é‡‡ç”¨Transformerç¼–ç å™¨ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨äº”ä¸ªåŸºå‡†æ–‡æœ¬åˆ†ç±»æ•°æ®é›†ä¸­çš„å››ä¸ªä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä¸14ç§å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨Friedmanæ£€éªŒä¸­æ’åç¬¬ä¸€ï¼Œè¡¨æ˜å…¶æ€§èƒ½å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚å°¤å…¶æ˜¯åœ¨å…·æœ‰å¤§é‡æœªæ ‡æ³¨æ•°æ®çš„åœºæ™¯ä¸‹ï¼Œè¯¥æ–¹æ³•çš„æ€§èƒ½æå‡æ›´ä¸ºæ˜æ˜¾ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºå„ç§æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ï¼Œå°¤å…¶æ˜¯åœ¨æ ‡æ³¨æ•°æ®ç¨€ç¼ºä½†å­˜åœ¨å¤§é‡æœªæ ‡æ³¨æ•°æ®çš„åœºæ™¯ä¸‹ï¼Œä¾‹å¦‚æƒ…æ„Ÿåˆ†æã€åƒåœ¾é‚®ä»¶æ£€æµ‹ã€æ–°é—»åˆ†ç±»ç­‰ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨LLMçš„çŸ¥è¯†ï¼Œæå‡æ¨¡å‹æ€§èƒ½ï¼Œé™ä½äººå·¥æ ‡æ³¨æˆæœ¬ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In this paper, we introduce a novel weighted co-training approach that is guided by Large Language Models (LLMs). Namely, in our co-training approach, we use LLM labels on unlabeled data as target labels and co-train two encoder-only based networks that train each other over multiple iterations: first, all samples are forwarded through each network and historical estimates of each network's confidence in the LLM label are recorded; second, a dynamic importance weight is derived for each sample according to each network's belief in the quality of the LLM label for that sample; finally, the two networks exchange importance weights with each other -- each network back-propagates all samples weighted with the importance weights coming from its peer network and updates its own parameters. By strategically utilizing LLM-generated guidance, our approach significantly outperforms conventional SSL methods, particularly in settings with abundant unlabeled data. Empirical results show that it achieves state-of-the-art performance on 4 out of 5 benchmark datasets and ranks first among 14 compared methods according to the Friedman test. Our results highlight a new direction in semi-supervised learning -- where LLMs serve as knowledge amplifiers, enabling backbone co-training models to achieve state-of-the-art performance efficiently.

