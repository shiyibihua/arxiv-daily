---
layout: default
title: Conversational Orientation Reasoning: Egocentric-to-Allocentric Navigation with Multimodal Chain-of-Thought
---

# Conversational Orientation Reasoning: Egocentric-to-Allocentric Navigation with Multimodal Chain-of-Thought

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.18200" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.18200v1</a>
  <a href="https://arxiv.org/pdf/2509.18200.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.18200v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.18200v1', 'Conversational Orientation Reasoning: Egocentric-to-Allocentric Navigation with Multimodal Chain-of-Thought')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yu Ti Huang

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-20

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šæ¨¡æ€é“¾å¼æ€è€ƒæ¡†æ¶ï¼Œè§£å†³å¤æ‚ç¯å¢ƒä¸‹è¯­éŸ³å¯¹è¯çš„æœå‘æ¨ç†é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æœå‘æ¨ç†` `å¤šæ¨¡æ€å­¦ä¹ ` `é“¾å¼æ€è€ƒ` `å…·èº«å¯¼èˆª` `è¯­éŸ³è¯†åˆ«` `ç©ºé—´å…³ç³»` `è¯¾ç¨‹å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¯¹è¯Agentéš¾ä»¥å°†è‡ªæˆ‘ä¸­å¿ƒçš„æŒ‡ä»¤è½¬åŒ–ä¸ºç¯å¢ƒä¸­å¿ƒçš„æœå‘ï¼Œå°¤å…¶æ˜¯åœ¨GPSä¿¡å·å¼±ä¸”ç¼ºä¹è¯¦ç»†åœ°å›¾çš„å¤æ‚ç¯å¢ƒä¸­ã€‚
2. è®ºæ–‡æå‡ºå¤šæ¨¡æ€é“¾å¼æ€è€ƒ(MCoT)æ¡†æ¶ï¼Œé€šè¿‡æå–ç©ºé—´å…³ç³»ã€æ˜ å°„åæ ‡æ–¹å‘å’Œæ¨æ–­ç”¨æˆ·æœå‘çš„ä¸‰æ­¥æ¨ç†è¿‡ç¨‹ï¼Œè§£å†³æœå‘æ¨ç†é—®é¢˜ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒMCoTåœ¨çœŸå®åœºæ™¯çš„è¯­éŸ³è½¬å½•ä¸­å®ç°äº†é«˜ç²¾åº¦çš„æœå‘æ¨ç†ï¼Œä¸”å¯¹å™ªå£°ã€è¯­è¨€å˜å¼‚å’Œé¢†åŸŸè½¬ç§»å…·æœ‰é²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åŸºå‡†æµ‹è¯•â€”â€”å¯¹è¯æœå‘æ¨ç†(COR)ï¼Œç”¨äºè¯„ä¼°åœ¨çœŸå®ç¯å¢ƒä¸­ï¼Œä»ä¼ ç»Ÿä¸­æ–‡å¯¹è¯å¯¼èˆªä¸­è¿›è¡Œè‡ªæˆ‘ä¸­å¿ƒåˆ°ä»¥ç¯å¢ƒä¸ºä¸­å¿ƒçš„æœå‘æ¨ç†èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨éè‹±è¯­å’ŒASRè½¬å½•åœºæ™¯ä¸‹ã€‚åŒæ—¶ï¼Œæå‡ºäº†ä¸€ç§å¤šæ¨¡æ€é“¾å¼æ€è€ƒ(MCoT)æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡ç»“æ„åŒ–çš„ä¸‰æ­¥æ¨ç†è¿‡ç¨‹ï¼Œæ•´åˆäº†ASRè½¬å½•çš„è¯­éŸ³å’Œåœ°æ ‡åæ ‡ï¼š(1)æå–ç©ºé—´å…³ç³»ï¼Œ(2)å°†åæ ‡æ˜ å°„åˆ°ç»å¯¹æ–¹å‘ï¼Œ(3)æ¨æ–­ç”¨æˆ·æœå‘ã€‚é€šè¿‡è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œåœ¨Taiwan-LLM-13B-v2.0-Chatä¸Šé€æ­¥æ„å»ºè¿™äº›èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒMCoTåœ¨å¹²å‡€çš„æ–‡æœ¬è½¬å½•ä¸­å®ç°äº†100%çš„æœå‘å‡†ç¡®ç‡ï¼Œåœ¨ASRè½¬å½•ä¸­è¾¾åˆ°äº†98.1%ï¼Œæ˜¾è‘—ä¼˜äºå•æ¨¡æ€å’Œéç»“æ„åŒ–åŸºçº¿ã€‚æ­¤å¤–ï¼ŒMCoTåœ¨å˜ˆæ‚çš„å¯¹è¯æ¡ä»¶ä¸‹ï¼ŒåŒ…æ‹¬ASRè¯†åˆ«é”™è¯¯å’Œå¤šè¯­è¨€ä»£ç åˆ‡æ¢ï¼Œä¹Ÿè¡¨ç°å‡ºé²æ£’æ€§ã€‚è¯¥æ¨¡å‹åœ¨è·¨é¢†åŸŸè¯„ä¼°ä¸­ä¿æŒäº†é«˜ç²¾åº¦ï¼Œå¹¶å¯¹è¯­è¨€å˜å¼‚ã€é¢†åŸŸè½¬ç§»å’ŒæŒ‡ä»£æ­§ä¹‰å…·æœ‰å¼¹æ€§ã€‚è¿™äº›å‘ç°çªå‡ºäº†ç»“æ„åŒ–MCoTç©ºé—´æ¨ç†ä½œä¸ºä¸€ç§å¯è§£é‡Šå’Œèµ„æºé«˜æ•ˆçš„å…·èº«å¯¼èˆªè·¯å¾„çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³åœ¨å®¤å†…æˆ–å¤æ‚ç¯å¢ƒä¸­ï¼Œç”±äºGPSä¿¡å·å¼±ä¸”ç¼ºä¹è¯¦ç»†åœ°å›¾ï¼Œå¯¹è¯Agentéš¾ä»¥å°†ç”¨æˆ·ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è¯­éŸ³æŒ‡ä»¤ï¼ˆä¾‹å¦‚â€œæˆ‘çš„å³è¾¹â€ï¼‰è½¬æ¢ä¸ºä»¥ç¯å¢ƒä¸ºä¸­å¿ƒçš„ç»å¯¹æ–¹å‘ï¼ˆä¾‹å¦‚åŒ—/ä¸œ/å—/è¥¿ï¼‰çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†ASRè½¬å½•é”™è¯¯ã€å¤šè¯­è¨€ç¯å¢ƒä»¥åŠé¢†åŸŸè¿ç§»æ—¶è¡¨ç°ä¸ä½³ï¼Œç¼ºä¹é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤šæ¨¡æ€ä¿¡æ¯ï¼ˆè¯­éŸ³å’Œåœ°æ ‡åæ ‡ï¼‰ä»¥åŠé“¾å¼æ€è€ƒ(Chain-of-Thought, CoT)çš„æ¨ç†æ–¹å¼ï¼Œå°†å¤æ‚çš„æœå‘æ¨ç†ä»»åŠ¡åˆ†è§£ä¸ºå¤šä¸ªå¯è§£é‡Šçš„æ­¥éª¤ã€‚é€šè¿‡é€æ­¥æ¨ç†ï¼Œæ¨¡å‹å¯ä»¥æ›´å¥½åœ°ç†è§£ç”¨æˆ·æ„å›¾ï¼Œå¹¶å‡†ç¡®åœ°æ¨æ–­å‡ºç”¨æˆ·çš„æœå‘ã€‚è¿™ç§æ–¹æ³•å€Ÿé‰´äº†äººç±»è§£å†³ç©ºé—´æ¨ç†é—®é¢˜çš„æ–¹å¼ï¼Œå³å…ˆç†è§£ç›¸å¯¹ä½ç½®å…³ç³»ï¼Œå†å°†å…¶æ˜ å°„åˆ°ç»å¯¹æ–¹å‘ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMCoTæ¡†æ¶åŒ…å«ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼š(1) **ç©ºé—´å…³ç³»æå–**ï¼šä»ASRè½¬å½•çš„è¯­éŸ³ä¸­æå–ç”¨æˆ·ä¸åœ°æ ‡ä¹‹é—´çš„ç©ºé—´å…³ç³»ï¼ˆä¾‹å¦‚â€œåœ¨æˆ‘çš„å·¦è¾¹â€ï¼‰ã€‚(2) **åæ ‡åˆ°æ–¹å‘æ˜ å°„**ï¼šåˆ©ç”¨åœ°æ ‡çš„åæ ‡ä¿¡æ¯ï¼Œå°†ç›¸å¯¹ç©ºé—´å…³ç³»æ˜ å°„åˆ°ç»å¯¹æ–¹å‘ï¼ˆä¾‹å¦‚â€œå·¦è¾¹æ˜¯åŒ—æ–¹â€ï¼‰ã€‚(3) **ç”¨æˆ·æœå‘æ¨æ–­**ï¼šç»¼åˆæ‰€æœ‰åœ°æ ‡çš„æ–¹ä½ä¿¡æ¯ï¼Œæ¨æ–­å‡ºç”¨æˆ·çš„å½“å‰æœå‘ã€‚æ•´ä¸ªæ¡†æ¶ä½¿ç”¨Taiwan-LLM-13B-v2.0-Chatä½œä¸ºåŸºç¡€æ¨¡å‹ï¼Œå¹¶é€šè¿‡è¯¾ç¨‹å­¦ä¹ ç­–ç•¥é€æ­¥è®­ç»ƒæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†å¤šæ¨¡æ€ä¿¡æ¯å’Œé“¾å¼æ€è€ƒæ–¹æ³•ç»“åˆèµ·æ¥ï¼Œç”¨äºè§£å†³æœå‘æ¨ç†é—®é¢˜ã€‚ä¸ä¼ ç»Ÿçš„å•æ¨¡æ€æ–¹æ³•ç›¸æ¯”ï¼ŒMCoTèƒ½å¤Ÿåˆ©ç”¨åœ°æ ‡çš„åæ ‡ä¿¡æ¯æ¥æé«˜æ¨ç†çš„å‡†ç¡®æ€§ã€‚ä¸éç»“æ„åŒ–çš„æ¨ç†æ–¹æ³•ç›¸æ¯”ï¼ŒMCoTçš„é“¾å¼æ€è€ƒè¿‡ç¨‹ä½¿å¾—æ¨ç†è¿‡ç¨‹æ›´åŠ å¯è§£é‡Šï¼Œå¹¶ä¸”èƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†å¤æ‚çš„ç©ºé—´å…³ç³»ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜é’ˆå¯¹éè‹±è¯­å’ŒASRè½¬å½•åœºæ™¯è¿›è¡Œäº†ä¼˜åŒ–ï¼Œä½¿å…¶æ›´é€‚ç”¨äºå®é™…åº”ç”¨ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡é‡‡ç”¨è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œé€æ­¥è®­ç»ƒæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚é¦–å…ˆï¼Œæ¨¡å‹å­¦ä¹ ä»å¹²å‡€çš„æ–‡æœ¬è½¬å½•ä¸­æå–ç©ºé—´å…³ç³»ã€‚ç„¶åï¼Œæ¨¡å‹å­¦ä¹ å°†åæ ‡æ˜ å°„åˆ°ç»å¯¹æ–¹å‘ã€‚æœ€åï¼Œæ¨¡å‹å­¦ä¹ ç»¼åˆæ‰€æœ‰ä¿¡æ¯ï¼Œæ¨æ–­ç”¨æˆ·çš„æœå‘ã€‚æŸå¤±å‡½æ•°æ–¹é¢ï¼Œè®ºæ–‡å¯èƒ½é‡‡ç”¨äº†äº¤å‰ç†µæŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ¨¡å‹çš„åˆ†ç±»æ€§èƒ½ã€‚åœ¨ç½‘ç»œç»“æ„æ–¹é¢ï¼ŒTaiwan-LLM-13B-v2.0-Chatä½œä¸ºåŸºç¡€æ¨¡å‹ï¼Œå¯èƒ½ä½¿ç”¨äº†Transformeræ¶æ„ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

MCoTåœ¨å¹²å‡€çš„æ–‡æœ¬è½¬å½•ä¸­å®ç°äº†100%çš„æœå‘å‡†ç¡®ç‡ï¼Œåœ¨ASRè½¬å½•ä¸­è¾¾åˆ°äº†98.1%ï¼Œæ˜¾è‘—ä¼˜äºå•æ¨¡æ€å’Œéç»“æ„åŒ–åŸºçº¿ã€‚å³ä½¿åœ¨å­˜åœ¨ASRè¯†åˆ«é”™è¯¯å’Œå¤šè¯­è¨€ä»£ç åˆ‡æ¢ç­‰å™ªå£°æ¡ä»¶ä¸‹ï¼ŒMCoTä»ç„¶è¡¨ç°å‡ºå¾ˆå¼ºçš„é²æ£’æ€§ã€‚æ­¤å¤–ï¼ŒMCoTåœ¨è·¨é¢†åŸŸè¯„ä¼°ä¸­ä¿æŒäº†é«˜ç²¾åº¦ï¼Œå¹¶å¯¹è¯­è¨€å˜å¼‚ã€é¢†åŸŸè½¬ç§»å’ŒæŒ‡ä»£æ­§ä¹‰å…·æœ‰å¼¹æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå®¤å†…å¯¼èˆªã€æœºå™¨äººå¯¼èˆªã€æ™ºèƒ½å®¶å±…ã€å¢å¼ºç°å®ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨å•†åœºæˆ–åšç‰©é¦†ç­‰å¤æ‚ç¯å¢ƒä¸­ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡è¯­éŸ³æŒ‡ä»¤å¼•å¯¼æœºå™¨äººæˆ–ARè®¾å¤‡è¿›è¡Œå¯¼èˆªã€‚è¯¥æŠ€æœ¯è¿˜å¯ä»¥å¸®åŠ©è§†åŠ›éšœç¢äººå£«è¿›è¡Œå®‰å…¨å¯¼èˆªï¼Œæé«˜ç”Ÿæ´»è´¨é‡ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›ä¸æ›´é«˜çº§çš„AIæŠ€æœ¯ç»“åˆï¼Œå®ç°æ›´æ™ºèƒ½ã€æ›´è‡ªç„¶çš„äº¤äº’ä½“éªŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Conversational agents must translate egocentric utterances (e.g., "on my right") into allocentric orientations (N/E/S/W). This challenge is particularly critical in indoor or complex facilities where GPS signals are weak and detailed maps are unavailable. While chain-of-thought (CoT) prompting has advanced reasoning in language and vision tasks, its application to multimodal spatial orientation remains underexplored. We introduce Conversational Orientation Reasoning (COR), a new benchmark designed for Traditional Chinese conversational navigation projected from real-world environments, addressing egocentric-to-allocentric reasoning in non-English and ASR-transcribed scenarios. We propose a multimodal chain-of-thought (MCoT) framework, which integrates ASR-transcribed speech with landmark coordinates through a structured three-step reasoning process: (1) extracting spatial relations, (2) mapping coordinates to absolute directions, and (3) inferring user orientation. A curriculum learning strategy progressively builds these capabilities on Taiwan-LLM-13B-v2.0-Chat, a mid-sized model representative of resource-constrained settings. Experiments show that MCoT achieves 100% orientation accuracy on clean transcripts and 98.1% with ASR transcripts, substantially outperforming unimodal and non-structured baselines. Moreover, MCoT demonstrates robustness under noisy conversational conditions, including ASR recognition errors and multilingual code-switching. The model also maintains high accuracy in cross-domain evaluation and resilience to linguistic variation, domain shift, and referential ambiguity. These findings highlight the potential of structured MCoT spatial reasoning as a path toward interpretable and resource-efficient embodied navigation.

