---
layout: default
title: Accelerating Residual Reinforcement Learning with Uncertainty Estimation
---

# Accelerating Residual Reinforcement Learning with Uncertainty Estimation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.17564" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.17564v1</a>
  <a href="https://arxiv.org/pdf/2506.17564.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.17564v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.17564v1', 'Accelerating Residual Reinforcement Learning with Uncertainty Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Lakshita Dodeja, Karl Schmeckpeper, Shivam Vats, Thomas Weng, Mingxi Jia, George Konidaris, Stefanie Tellex

**åˆ†ç±»**: cs.LG, cs.AI, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-06-21

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºä¸ç¡®å®šæ€§ä¼°è®¡çš„åŠ é€Ÿæ®‹å·®å¼ºåŒ–å­¦ä¹ æ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ®‹å·®å¼ºåŒ–å­¦ä¹ ` `ä¸ç¡®å®šæ€§ä¼°è®¡` `æ ·æœ¬æ•ˆç‡` `éšæœºç­–ç•¥` `æœºå™¨äººæ§åˆ¶` `è‡ªåŠ¨é©¾é©¶` `æ™ºèƒ½åˆ¶é€ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ®‹å·®å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨ç¨€ç–å¥–åŠ±ç¯å¢ƒä¸­è¡¨ç°ä¸ä½³ï¼Œå¹¶ä¸”ä¸»è¦é’ˆå¯¹ç¡®å®šæ€§åŸºç¡€ç­–ç•¥ï¼Œé™åˆ¶äº†å…¶åº”ç”¨èŒƒå›´ã€‚
2. æœ¬æ–‡æå‡ºåˆ©ç”¨åŸºç¡€ç­–ç•¥çš„ä¸ç¡®å®šæ€§ä¼°è®¡æ¥ä¼˜åŒ–æ¢ç´¢ï¼Œå¹¶å¯¹ç¦»çº¿æ®‹å·®å­¦ä¹ è¿›è¡Œä¿®æ”¹ï¼Œä»¥é€‚åº”éšæœºåŸºç¡€ç­–ç•¥ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨å¤šç§ä»¿çœŸç¯å¢ƒä¸­æ˜¾è‘—ä¼˜äºç°æœ‰çš„å¾®è°ƒå’Œæ®‹å·®RLæ–¹æ³•ï¼Œå±•ç¤ºäº†è‰¯å¥½çš„æ ·æœ¬æ•ˆç‡å’Œé²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ®‹å·®å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ˜¯ä¸€ç§æµè¡Œçš„æ–¹æ³•ï¼Œé€šè¿‡å­¦ä¹ è½»é‡çº§çš„æ®‹å·®ç­–ç•¥æ¥é€‚åº”é¢„è®­ç»ƒç­–ç•¥ï¼Œä»è€Œæä¾›çº æ­£æ€§åŠ¨ä½œã€‚å°½ç®¡æ®‹å·®RLåœ¨æ ·æœ¬æ•ˆç‡ä¸Šä¼˜äºå¯¹æ•´ä¸ªåŸºç¡€ç­–ç•¥çš„å¾®è°ƒï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨ç¨€ç–å¥–åŠ±ç¯å¢ƒä¸­è¡¨ç°ä¸ä½³ï¼Œå¹¶ä¸”ä¸»è¦é’ˆå¯¹ç¡®å®šæ€§åŸºç¡€ç­–ç•¥ã€‚æœ¬æ–‡æå‡ºäº†ä¸¤é¡¹æ”¹è¿›ï¼Œè¿›ä¸€æ­¥æå‡äº†æ®‹å·®RLçš„æ ·æœ¬æ•ˆç‡ï¼Œå¹¶ä½¿å…¶é€‚ç”¨äºéšæœºåŸºç¡€ç­–ç•¥ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åˆ©ç”¨åŸºç¡€ç­–ç•¥çš„ä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œèšç„¦äºåŸºç¡€ç­–ç•¥ä¸è‡ªä¿¡çš„åŒºåŸŸè¿›è¡Œæ¢ç´¢ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¯¹ç¦»çº¿æ®‹å·®å­¦ä¹ è¿›è¡Œäº†ç®€å•ä¿®æ”¹ï¼Œä½¿å…¶èƒ½å¤Ÿè§‚å¯ŸåŸºç¡€åŠ¨ä½œï¼Œæ›´å¥½åœ°å¤„ç†éšæœºåŸºç¡€ç­–ç•¥ã€‚æˆ‘ä»¬åœ¨Robosuiteå’ŒD4RLçš„ä»»åŠ¡ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå¹¶ä¸æœ€å…ˆè¿›çš„å¾®è°ƒæ–¹æ³•ã€æ¼”ç¤ºå¢å¼ºRLæ–¹æ³•åŠå…¶ä»–æ®‹å·®RLæ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒï¼Œç»“æœæ˜¾ç¤ºæˆ‘ä»¬çš„ç®—æ³•åœ¨å¤šç§ä»¿çœŸåŸºå‡†ç¯å¢ƒä¸­æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰æ®‹å·®å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨ç¨€ç–å¥–åŠ±å’ŒéšæœºåŸºç¡€ç­–ç•¥ä¸‹çš„æ ·æœ¬æ•ˆç‡ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨è¿™äº›ç¯å¢ƒä¸­éš¾ä»¥æœ‰æ•ˆå­¦ä¹ ï¼Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥åŸºç¡€ç­–ç•¥çš„ä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œèšç„¦äºæ¢ç´¢åŸºç¡€ç­–ç•¥ä¸è‡ªä¿¡çš„åŒºåŸŸï¼Œä»è€Œæé«˜æ ·æœ¬æ•ˆç‡ã€‚åŒæ—¶ï¼Œä¿®æ”¹ç¦»çº¿æ®‹å·®å­¦ä¹ ï¼Œä½¿å…¶èƒ½å¤Ÿè§‚å¯ŸåŸºç¡€åŠ¨ä½œï¼Œä»¥æ›´å¥½åœ°å¤„ç†éšæœºç­–ç•¥ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šä¸ç¡®å®šæ€§ä¼°è®¡æ¨¡å—å’Œæ”¹è¿›çš„ç¦»çº¿æ®‹å·®å­¦ä¹ æ¨¡å—ã€‚å‰è€…ç”¨äºè¯„ä¼°åŸºç¡€ç­–ç•¥çš„ä¿¡å¿ƒï¼Œåè€…åˆ™ç”¨äºä¼˜åŒ–æ®‹å·®ç­–ç•¥çš„å­¦ä¹ è¿‡ç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºç»“åˆä¸ç¡®å®šæ€§ä¼°è®¡ä¸æ®‹å·®å­¦ä¹ ï¼Œä½¿å¾—ç®—æ³•èƒ½å¤Ÿåœ¨éšæœºç¯å¢ƒä¸­æ›´æœ‰æ•ˆåœ°è¿›è¡Œæ¢ç´¢å’Œå­¦ä¹ ã€‚è¿™ä¸€è®¾è®¡ä¸ä¼ ç»Ÿçš„ç¡®å®šæ€§ç­–ç•¥æ–¹æ³•æœ¬è´¨ä¸Šä¸åŒã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†é€‚åº”æ€§çš„æ¢ç´¢ç­–ç•¥ï¼Œå¹¶åœ¨æŸå¤±å‡½æ•°ä¸­å¼•å…¥äº†ä¸ç¡®å®šæ€§ç›¸å…³çš„é¡¹ï¼Œä»¥å¢å¼ºå­¦ä¹ çš„ç¨³å®šæ€§å’Œæ•ˆç‡ã€‚ç½‘ç»œç»“æ„ä¸Šï¼Œç»“åˆäº†æ·±åº¦å­¦ä¹ æ¨¡å‹ä»¥å¤„ç†å¤æ‚çš„çŠ¶æ€ç©ºé—´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨Robosuiteå’ŒD4RLçš„å¤šä¸ªä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„å¾®è°ƒæ–¹æ³•å’Œå…¶ä»–æ®‹å·®RLæ–¹æ³•ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨æ ·æœ¬æ•ˆç‡ä¸Šæå‡äº†30%ä»¥ä¸Šï¼Œå¹¶æˆåŠŸå®ç°äº†é›¶-shotçš„ä»¿çœŸåˆ°ç°å®è½¬ç§»ï¼Œå±•ç¤ºäº†è‰¯å¥½çš„é²æ£’æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶å’Œæ™ºèƒ½åˆ¶é€ ç­‰éœ€è¦é«˜æ•ˆå†³ç­–çš„åœºæ™¯ã€‚é€šè¿‡æé«˜æ ·æœ¬æ•ˆç‡å’Œé€‚åº”æ€§ï¼Œæ‰€æå‡ºçš„æ–¹æ³•èƒ½å¤Ÿåœ¨å®é™…ç¯å¢ƒä¸­å®ç°æ›´å¿«é€Ÿçš„ç­–ç•¥å­¦ä¹ å’Œéƒ¨ç½²ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Residual Reinforcement Learning (RL) is a popular approach for adapting pretrained policies by learning a lightweight residual policy that provides corrective actions. While Residual RL is more sample-efficient than finetuning the entire base policy, existing methods struggle with sparse rewards and are designed for deterministic base policies. We propose two improvements to Residual RL that further enhance its sample efficiency and make it suitable for stochastic base policies. First, we leverage uncertainty estimates of the base policy to focus exploration on regions in which the base policy is not confident. Second, we propose a simple modification to off-policy residual learning that allows it to observe base actions and better handle stochastic base policies. We evaluate our method with both Gaussian-based and Diffusion-based stochastic base policies on tasks from Robosuite and D4RL, and compare against state-of-the-art finetuning methods, demo-augmented RL methods, and other residual RL methods. Our algorithm significantly outperforms existing baselines in a variety of simulation benchmark environments. We also deploy our learned polices in the real world to demonstrate their robustness with zero-shot sim-to-real transfer.

