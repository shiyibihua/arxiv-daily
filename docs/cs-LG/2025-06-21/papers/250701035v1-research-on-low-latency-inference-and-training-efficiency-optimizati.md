---
layout: default
title: Research on Low-Latency Inference and Training Efficiency Optimization for Graph Neural Network and Large Language Model-Based Recommendation Systems
---

# Research on Low-Latency Inference and Training Efficiency Optimization for Graph Neural Network and Large Language Model-Based Recommendation Systems

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2507.01035" class="toolbar-btn" target="_blank">üìÑ arXiv: 2507.01035v1</a>
  <a href="https://arxiv.org/pdf/2507.01035.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2507.01035v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2507.01035v1', 'Research on Low-Latency Inference and Training Efficiency Optimization for Graph Neural Network and Large Language Model-Based Recommendation Systems')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Yushang Zhao, Haotian Lyu, Yike Peng, Aijia Sun, Feng Jiang, Xinyue Han

**ÂàÜÁ±ª**: cs.LG, cs.AI, cs.PF

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-21

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫‰ΩéÂª∂ËøüÊé®ÁêÜ‰∏éËÆ≠ÁªÉÊïàÁéá‰ºòÂåñÊñπÊ≥ï‰ª•ÊèêÂçáÊé®ËçêÁ≥ªÁªüÊÄßËÉΩ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÂõæÁ•ûÁªèÁΩëÁªú` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `Êé®ËçêÁ≥ªÁªü` `‰ΩéÂª∂ËøüÊé®ÁêÜ` `ËÆ≠ÁªÉÊïàÁéá` `Á°¨‰ª∂Âä†ÈÄü` `FPGA` `LoRA`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÊé®ËçêÁ≥ªÁªüÂú®Â§ÑÁêÜÂ§çÊùÇÁî®Êà∑-Áâ©ÂìÅ‰∫§‰∫íÊó∂Èù¢‰∏¥ËÆ°ÁÆóÁì∂È¢àÔºåÂØºËá¥Êé®ÁêÜÂª∂ËøüÂíåËÆ≠ÁªÉÊïàÁéá‰Ωé‰∏ã„ÄÇ
2. Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊ∑∑ÂêàGNNÂíåLLMÁöÑÈõÜÊàêÊû∂ÊûÑÔºåÁªìÂêàÈáèÂåñ„ÄÅLoRA„ÄÅËí∏È¶èÁ≠â‰ºòÂåñÁ≠ñÁï•ÔºåÂπ∂Âà©Áî®FPGAÂíåDeepSpeedËøõË°åÁ°¨‰ª∂Âä†ÈÄü„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºå‰ºòÂåñÂêéÁöÑÁ≥ªÁªüÂú®Âª∂ËøüÂíåÂáÜÁ°ÆÊÄß‰∏äÂùáÊúâÊòæËëóÊèêÂçáÔºåLoRAÊäÄÊúØÊúâÊïàÁº©Áü≠‰∫ÜËÆ≠ÁªÉÊó∂Èó¥„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÈöèÁùÄÂú®Á∫øÊúçÂä°ÁöÑ‰∏çÊñ≠Â¢ûÂä†ÔºåÂØπÈ´òÈÄüÂ∫¶ÂíåÈ´òÊïàÊé®ËçêÁ≥ªÁªüÁöÑÈúÄÊ±ÇÊó•ÁõäÂ¢ûÈïøÔºåËÉΩÂ§üÂÆûÊó∂Â§ÑÁêÜÂ§çÊùÇÁöÑÁî®Êà∑-Áâ©ÂìÅ‰∫§‰∫í„ÄÇÊú¨ÊñáÁ†îÁ©∂‰∫ÜÊ∑∑ÂêàÂõæÁ•ûÁªèÁΩëÁªúÔºàGNNÔºâÂíåÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÊé®ËçêÁ≥ªÁªü‰∏≠ÁöÑËÆ°ÁÆóÁì∂È¢àÔºåÊó®Âú®‰ºòÂåñÂÖ∂Êé®ÁêÜÂª∂ËøüÂíåËÆ≠ÁªÉÊïàÁéá„ÄÇÈááÁî®‰∫ÜÂπøÊ≥õÁöÑÊñπÊ≥ïÔºåÂåÖÊã¨Ê∑∑ÂêàGNN-LLMÈõÜÊàêÊû∂ÊûÑ‰ºòÂåñÁ≠ñÁï•ÔºàÈáèÂåñ„ÄÅLoRA„ÄÅËí∏È¶èÔºâÂíåÁ°¨‰ª∂Âä†ÈÄüÔºàFPGA„ÄÅDeepSpeedÔºâÔºåÂú®R 4.4.2ÁéØÂ¢É‰∏ãËøõË°åÂÆûÈ™å„ÄÇÁªìÊûúÊòæÁ§∫ÔºåÊúÄ‰Ω≥ÁöÑÊ∑∑Âêà+FPGA+DeepSpeedÈÖçÁΩÆÂú®40-60ÊØ´ÁßíÂª∂Ëøü‰∏ãÂÆûÁé∞‰∫Ü13.6%ÁöÑÂáÜÁ°ÆÁéáÊèêÂçáÔºàNDCG@10: 0.75ÔºâÔºåËÄåLoRAÂ∞ÜËÆ≠ÁªÉÊó∂Èó¥Áº©Áü≠‰∫Ü66%Ôºà3.8Â∞èÊó∂Ôºâ„ÄÇÊó†ËÆ∫Âú®ÂáÜÁ°ÆÊÄßËøòÊòØÊïàÁéáÊñπÈù¢ÔºåÁ°¨‰ª∂-ËΩØ‰ª∂ÂçèÂêåËÆæËÆ°ÂíåÂèÇÊï∞È´òÊïàË∞É‰ºò‰ΩøÂæóÊ∑∑ÂêàÊ®°ÂûãÁöÑË°®Áé∞‰ºò‰∫éÁã¨Á´ãÂÆûÁé∞ÁöÑGNNÊàñLLMÊñπÊ≥ï„ÄÇÂª∫ËÆÆÂú®ÂÆûÊó∂ÈÉ®ÁΩ≤‰∏≠‰ΩøÁî®FPGAÂíåLoRA„ÄÇÊú™Êù•ÁöÑÂ∑•‰ΩúÂ∫îÊ∂âÂèäËÅîÈÇ¶Â≠¶‰π†ÂíåÂÖàËøõÁöÑËûçÂêàÊû∂ÊûÑÔºå‰ª•ÂÆûÁé∞Êõ¥Â•ΩÁöÑÂèØÊâ©Â±ïÊÄßÂíåÈöêÁßÅ‰øùÊä§„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Ê∑∑ÂêàÂõæÁ•ûÁªèÁΩëÁªúÔºàGNNÔºâÂíåÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÊé®ËçêÁ≥ªÁªü‰∏≠ÁöÑÊé®ÁêÜÂª∂ËøüÂíåËÆ≠ÁªÉÊïàÁéá‰Ωé‰∏ãÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®Â§ÑÁêÜÂ§çÊùÇÁî®Êà∑-Áâ©ÂìÅ‰∫§‰∫íÊó∂ÔºåËÆ°ÁÆóÁì∂È¢à‰∏•ÈáçÂΩ±Âìç‰∫ÜÂÆûÊó∂ÊÄßËÉΩÂíåÁî®Êà∑‰ΩìÈ™å„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÈÄöËøáËÆæËÆ°Ê∑∑ÂêàGNN-LLMÈõÜÊàêÊû∂ÊûÑÔºåÂπ∂ÁªìÂêàÈáèÂåñ„ÄÅLoRAÂíåËí∏È¶èÁ≠â‰ºòÂåñÁ≠ñÁï•ÔºåËÆ∫ÊñáÊó®Âú®ÊèêÂçáÊé®ËçêÁ≥ªÁªüÁöÑÊé®ÁêÜÈÄüÂ∫¶ÂíåËÆ≠ÁªÉÊïàÁéá„ÄÇËøôÊ†∑ÁöÑËÆæËÆ°ËÉΩÂ§üÊúâÊïàÂà©Áî®Á°¨‰ª∂Âä†ÈÄüÔºåÈôç‰ΩéÂª∂Ëøü„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨Ê∑∑ÂêàGNNÂíåLLMÁöÑÈõÜÊàêÔºåÈááÁî®ÈáèÂåñÂíåLoRAËøõË°åÊ®°Âûã‰ºòÂåñÔºåÂêåÊó∂Âà©Áî®FPGAÂíåDeepSpeedËøõË°åÁ°¨‰ª∂Âä†ÈÄü„ÄÇ‰∏ªË¶ÅÊ®°ÂùóÂåÖÊã¨Êï∞ÊçÆÈ¢ÑÂ§ÑÁêÜ„ÄÅÊ®°ÂûãËÆ≠ÁªÉ„ÄÅÊé®ÁêÜ‰ºòÂåñÂíåÁ°¨‰ª∂Âä†ÈÄü„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞Âú®‰∫éÂ∞ÜGNNÂíåLLMÁöÑ‰ºòÂäøÁªìÂêàÔºåÈÄöËøáÁ°¨‰ª∂-ËΩØ‰ª∂ÂçèÂêåËÆæËÆ°ÂíåÂèÇÊï∞È´òÊïàË∞É‰ºòÔºå‰ΩøÂæóÊ∑∑ÂêàÊ®°ÂûãÂú®ÊÄßËÉΩ‰∏äË∂ÖË∂ä‰∫ÜÁã¨Á´ãÂÆûÁé∞ÁöÑGNNÊàñLLMÊñπÊ≥ï„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂèÇÊï∞ËÆæÁΩÆ‰∏äÔºåÈááÁî®‰∫ÜLoRAÊäÄÊúØ‰ª•ÂáèÂ∞ëËÆ≠ÁªÉÊó∂Èó¥ÔºåÂêåÊó∂Âú®ÁΩëÁªúÁªìÊûÑ‰∏äËøõË°å‰∫ÜÈáèÂåñÂ§ÑÁêÜÔºå‰ª•Èôç‰ΩéÊé®ÁêÜÂª∂Ëøü„ÄÇÊçüÂ§±ÂáΩÊï∞ÁöÑÈÄâÊã©‰πüÁªèËøáÁ≤æÂøÉËÆæËÆ°Ôºå‰ª•Á°Æ‰øùÊ®°ÂûãÁöÑÂáÜÁ°ÆÊÄßÂíåÊïàÁéá„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÊúÄ‰Ω≥ÁöÑÊ∑∑Âêà+FPGA+DeepSpeedÈÖçÁΩÆÂú®40-60ÊØ´ÁßíÁöÑÂª∂Ëøü‰∏ãÂÆûÁé∞‰∫Ü13.6%ÁöÑÂáÜÁ°ÆÁéáÊèêÂçáÔºàNDCG@10: 0.75ÔºâÔºåËÄåLoRAÊäÄÊúØ‰ΩøËÆ≠ÁªÉÊó∂Èó¥Áº©Áü≠‰∫Ü66%Ôºà3.8Â∞èÊó∂ÔºâÔºåÊòæËëóÊèêÈ´ò‰∫ÜÊé®ËçêÁ≥ªÁªüÁöÑÊÄßËÉΩ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Âú®Á∫øÊé®ËçêÁ≥ªÁªü„ÄÅÁîµÂ≠êÂïÜÂä°Âπ≥Âè∞ÂíåÁ§æ‰∫§Â™í‰ΩìÁ≠âÔºåËÉΩÂ§üÊúâÊïàÊèêÂçáÁî®Êà∑‰ΩìÈ™åÂíåÁ≥ªÁªüÂìçÂ∫îÈÄüÂ∫¶„ÄÇÈÄöËøá‰ºòÂåñÊé®ÁêÜÂíåËÆ≠ÁªÉÊïàÁéáÔºåËØ•ÊñπÊ≥ïÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÂÖ∑ÊúâÈáçË¶ÅÁöÑ‰ª∑ÂÄºÔºåÊú™Êù•ÂèØËÉΩÊé®Âä®‰∏™ÊÄßÂåñÊé®ËçêÊäÄÊúØÁöÑËøõ‰∏ÄÊ≠•ÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> The incessant advent of online services demands high speed and efficient recommender systems (ReS) that can maintain real-time performance along with processing very complex user-item interactions. The present study, therefore, considers computational bottlenecks involved in hybrid Graph Neural Network (GNN) and Large Language Model (LLM)-based ReS with the aim optimizing their inference latency and training efficiency. An extensive methodology was used: hybrid GNN-LLM integrated architecture-optimization strategies(quantization, LoRA, distillation)-hardware acceleration (FPGA, DeepSpeed)-all under R 4.4.2. Experimental improvements were significant, with the optimal Hybrid + FPGA + DeepSpeed configuration reaching 13.6% more accuracy (NDCG@10: 0.75) at 40-60ms of latency, while LoRA brought down training time by 66% (3.8 hours) in comparison to the non-optimized baseline. Irrespective of domain, such as accuracy or efficiency, it can be established that hardware-software co-design and parameter-efficient tuning permit hybrid models to outperform GNN or LLM approaches implemented independently. It recommends the use of FPGA as well as LoRA for real-time deployment. Future work should involve federated learning along with advanced fusion architectures for better scalability and privacy preservation. Thus, this research marks the fundamental groundwork concerning next-generation ReS balancing low-latency response with cutting-edge personalization.

