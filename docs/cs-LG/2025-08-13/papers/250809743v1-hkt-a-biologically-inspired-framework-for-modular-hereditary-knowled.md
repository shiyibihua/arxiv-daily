---
layout: default
title: HKT: A Biologically Inspired Framework for Modular Hereditary Knowledge Transfer in Neural Networks
---

# HKT: A Biologically Inspired Framework for Modular Hereditary Knowledge Transfer in Neural Networks

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.09743" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.09743v1</a>
  <a href="https://arxiv.org/pdf/2508.09743.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.09743v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.09743v1', 'HKT: A Biologically Inspired Framework for Modular Hereditary Knowledge Transfer in Neural Networks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yanick Chistian Tchenko, Felix Mohr, Hicham Hadj Abdelkader, Hedi Tabia

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-13

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHKTæ¡†æ¶ä»¥ä¼˜åŒ–å°å‹ç¥ç»ç½‘ç»œçš„çŸ¥è¯†ä¼ é€’**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `é—ä¼ çŸ¥è¯†è½¬ç§»` `ç¥ç»ç½‘ç»œ` `çŸ¥è¯†è’¸é¦` `ç”Ÿç‰©å¯å‘` `ç‰¹å¾æå–` `æ¨¡å‹ä¼˜åŒ–` `è§†è§‰ä»»åŠ¡` `èµ„æºå—é™ç¯å¢ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç¥ç»ç½‘ç»œæ–¹æ³•åœ¨æå‡æ€§èƒ½æ—¶ï¼Œå¾€å¾€ç‰ºç‰²äº†æ¨¡å‹çš„å¯é›†æˆæ€§å’Œæ•ˆç‡ï¼Œå¯¼è‡´å°å‹æ¨¡å‹éš¾ä»¥æœ‰æ•ˆåˆ©ç”¨çŸ¥è¯†ã€‚
2. æœ¬æ–‡æå‡ºé—ä¼ çŸ¥è¯†è½¬ç§»ï¼ˆHKTï¼‰æ¡†æ¶ï¼Œé€šè¿‡æ¨¡ä»¿ç”Ÿç‰©é—ä¼ æœºåˆ¶ï¼Œä¼˜åŒ–å°å‹æ¨¡å‹çš„çŸ¥è¯†ä¼ é€’ï¼Œå¢å¼ºå…¶ä»»åŠ¡èƒ½åŠ›ã€‚
3. HKTåœ¨å…‰æµã€å›¾åƒåˆ†ç±»å’Œè¯­ä¹‰åˆ†å‰²ç­‰å¤šé¡¹è§†è§‰ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—è¶…è¶Šä¼ ç»Ÿè’¸é¦æ–¹æ³•ï¼Œæå‡äº†å­æ¨¡å‹çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨ç¥ç»ç½‘ç»œç ”ç©¶ä¸­ï¼Œæ¨¡å‹æ€§èƒ½é€šå¸¸éšç€æ·±åº¦å’Œå®¹é‡çš„å¢åŠ è€Œæå‡ï¼Œä½†è¿™å¾€å¾€ä¼šå½±å“å¯é›†æˆæ€§å’Œæ•ˆç‡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡ç»“æ„åŒ–çŸ¥è¯†ç»§æ‰¿æ¥å¢å¼ºå°å‹å¯éƒ¨ç½²æ¨¡å‹èƒ½åŠ›çš„ç­–ç•¥ï¼Œç§°ä¸ºé—ä¼ çŸ¥è¯†è½¬ç§»ï¼ˆHKTï¼‰ã€‚HKTå€Ÿé‰´ç”Ÿç‰©é—ä¼ æœºåˆ¶ï¼Œé€šè¿‡æå–ã€è½¬ç§»å’Œæ··åˆä¸‰ä¸ªç”Ÿç‰©å¯å‘çš„ç»„ä»¶ï¼Œé€‰æ‹©æ€§åœ°å°†ä»»åŠ¡ç›¸å…³ç‰¹å¾ä»å¤§å‹é¢„è®­ç»ƒçˆ¶ç½‘ç»œä¼ é€’åˆ°å°å‹å­æ¨¡å‹ã€‚ä¸ä¼ ç»ŸçŸ¥è¯†è’¸é¦ä¸åŒï¼ŒHKTé‡‡ç”¨å¤šé˜¶æ®µç‰¹å¾è½¬ç§»è¿‡ç¨‹ï¼Œç¡®ä¿äº†ç»§æ‰¿å’Œæœ¬åœ°è¡¨ç¤ºçš„å¯¹é½ä¸é€‰æ‹©æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHKTåœ¨å¤šä¸ªè§†è§‰ä»»åŠ¡ä¸­æ˜¾è‘—æå‡äº†å­æ¨¡å‹çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒå…¶ç´§å‡‘æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å°å‹ç¥ç»ç½‘ç»œåœ¨æ€§èƒ½å’Œæ•ˆç‡ä¸Šçš„çŸ›ç›¾ï¼Œç°æœ‰æ–¹æ³•å¦‚çŸ¥è¯†è’¸é¦æ— æ³•æœ‰æ•ˆåˆ©ç”¨å¤§å‹æ¨¡å‹çš„çŸ¥è¯†ï¼Œå¯¼è‡´å°å‹æ¨¡å‹æ€§èƒ½ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šHKTæ¡†æ¶é€šè¿‡æ¨¡ä»¿ç”Ÿç‰©é—ä¼ æœºåˆ¶ï¼Œé‡‡ç”¨æå–ã€è½¬ç§»å’Œæ··åˆçš„æ–¹å¼ï¼Œé€‰æ‹©æ€§åœ°å°†å¤§å‹é¢„è®­ç»ƒæ¨¡å‹çš„ç‰¹å¾ä¼ é€’ç»™å°å‹æ¨¡å‹ï¼Œä»è€Œæå‡å…¶æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šHKTçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šç‰¹å¾æå–ï¼ˆExtractionï¼‰ã€ç‰¹å¾è½¬ç§»ï¼ˆTransferï¼‰å’Œç‰¹å¾æ··åˆï¼ˆMixtureï¼‰ï¼Œå¹¶å¼•å…¥é—ä¼ æ³¨æ„åŠ›æœºåˆ¶ï¼ˆGAï¼‰æ¥ä¼˜åŒ–ç»§æ‰¿å’Œæœ¬åœ°è¡¨ç¤ºçš„æ•´åˆã€‚

**å…³é”®åˆ›æ–°**ï¼šHKTçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶ç”Ÿç‰©å¯å‘çš„ç‰¹å¾è½¬ç§»è¿‡ç¨‹ï¼ŒåŒºåˆ«äºä¼ ç»Ÿçš„çŸ¥è¯†è’¸é¦æ–¹æ³•ï¼ŒHKTå…è®¸é€‰æ‹©æ€§å’Œå¤šé˜¶æ®µçš„ç‰¹å¾ä¼ é€’ï¼Œå¢å¼ºäº†æ¨¡å‹çš„çµæ´»æ€§å’Œæ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šHKTæ¡†æ¶ä¸­çš„é—ä¼ æ³¨æ„åŠ›æœºåˆ¶ç¡®ä¿äº†ç»§æ‰¿ç‰¹å¾ä¸æœ¬åœ°ç‰¹å¾çš„æœ‰æ•ˆæ•´åˆï¼Œå…·ä½“å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡å°šæœªè¯¦ç»†æŠ«éœ²ï¼Œéœ€è¿›ä¸€æ­¥ç ”ç©¶ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒHKTåœ¨å…‰æµã€å›¾åƒåˆ†ç±»ï¼ˆCIFAR-10ï¼‰å’Œè¯­ä¹‰åˆ†å‰²ï¼ˆLiTSï¼‰ç­‰ä»»åŠ¡ä¸­ï¼Œå­æ¨¡å‹çš„æ€§èƒ½æ˜¾è‘—æå‡ï¼Œè¶…è¶Šä¼ ç»Ÿè’¸é¦æ–¹æ³•ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°äº†XX%ï¼ˆå…·ä½“æ•°æ®éœ€æŸ¥é˜…åŸæ–‡ï¼‰ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

HKTæ¡†æ¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶é€‚ç”¨äºèµ„æºå—é™çš„ç¯å¢ƒä¸­ï¼Œå¦‚ç§»åŠ¨è®¾å¤‡å’ŒåµŒå…¥å¼ç³»ç»Ÿã€‚é€šè¿‡ä¼˜åŒ–å°å‹ç¥ç»ç½‘ç»œçš„æ€§èƒ½ï¼ŒHKTå¯ä»¥åœ¨å›¾åƒå¤„ç†ã€è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—å½±åƒåˆ†æç­‰é¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ï¼Œæ¨åŠ¨æ™ºèƒ½åº”ç”¨çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> A prevailing trend in neural network research suggests that model performance improves with increasing depth and capacity - often at the cost of integrability and efficiency. In this paper, we propose a strategy to optimize small, deployable models by enhancing their capabilities through structured knowledge inheritance. We introduce Hereditary Knowledge Transfer (HKT), a biologically inspired framework for modular and selective transfer of task-relevant features from a larger, pretrained parent network to a smaller child model. Unlike standard knowledge distillation, which enforces uniform imitation of teacher outputs, HKT draws inspiration from biological inheritance mechanisms - such as memory RNA transfer in planarians - to guide a multi-stage process of feature transfer. Neural network blocks are treated as functional carriers, and knowledge is transmitted through three biologically motivated components: Extraction, Transfer, and Mixture (ETM). A novel Genetic Attention (GA) mechanism governs the integration of inherited and native representations, ensuring both alignment and selectivity. We evaluate HKT across diverse vision tasks, including optical flow (Sintel, KITTI), image classification (CIFAR-10), and semantic segmentation (LiTS), demonstrating that it significantly improves child model performance while preserving its compactness. The results show that HKT consistently outperforms conventional distillation approaches, offering a general-purpose, interpretable, and scalable solution for deploying high-performance neural networks in resource-constrained environments.

