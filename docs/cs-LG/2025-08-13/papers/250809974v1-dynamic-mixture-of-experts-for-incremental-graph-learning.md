---
layout: default
title: Dynamic Mixture-of-Experts for Incremental Graph Learning
---

# Dynamic Mixture-of-Experts for Incremental Graph Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.09974" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.09974v1</a>
  <a href="https://arxiv.org/pdf/2508.09974.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.09974v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.09974v1', 'Dynamic Mixture-of-Experts for Incremental Graph Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Lecheng Kong, Theodore Vasiloudis, Seongjun Yun, Han Xie, Xiang Song

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-13

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŠ¨æ€ä¸“å®¶æ··åˆæ¨¡å‹ä»¥è§£å†³å¢é‡å›¾å­¦ä¹ é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¢é‡å­¦ä¹ ` `å›¾ç¥ç»ç½‘ç»œ` `æ··åˆä¸“å®¶` `ç¾éš¾æ€§é—å¿˜` `æœºå™¨å­¦ä¹ ` `æ¨¡å‹é€‚åº”æ€§` `ç¨€ç–è®¡ç®—`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å›¾æœºå™¨å­¦ä¹ æ–¹æ³•åœ¨å¢é‡å­¦ä¹ ä¸­å®¹æ˜“å‡ºç°ç¾éš¾æ€§é—å¿˜ï¼Œæ— æ³•æœ‰æ•ˆä¿ç•™å…ˆå‰çŸ¥è¯†ã€‚
2. æœ¬æ–‡æå‡ºåŠ¨æ€æ··åˆä¸“å®¶ï¼ˆDyMoEï¼‰æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥æ–°çš„ä¸“å®¶ç½‘ç»œæ¥é€‚åº”æ–°æ•°æ®ï¼ŒåŒæ—¶ä¿æŒæ—§ä¸“å®¶çš„èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ¨¡å‹åœ¨ç±»åˆ«å¢é‡å­¦ä¹ ä¸­ç›¸è¾ƒäºæœ€ä½³åŸºçº¿æé«˜äº†4.92%çš„ç›¸å¯¹å‡†ç¡®ç‡ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å›¾å¢é‡å­¦ä¹ æ˜¯ä¸€ç§å­¦ä¹ èŒƒå¼ï¼Œæ—¨åœ¨ä½¿è®­ç»ƒå¥½çš„æ¨¡å‹èƒ½å¤Ÿé€‚åº”ä¸æ–­å¢åŠ çš„å›¾å’Œæ•°æ®ï¼Œè€Œæ— éœ€å¯¹å®Œæ•´æ•°æ®é›†è¿›è¡Œé‡æ–°è®­ç»ƒã€‚ç„¶è€Œï¼Œå¸¸è§„å›¾æœºå™¨å­¦ä¹ æ–¹æ³•åœ¨å¢é‡å­¦ä¹ ä¸­å®¹æ˜“å‡ºç°ç¾éš¾æ€§é—å¿˜ï¼Œå¯¼è‡´å…ˆå‰å­¦ä¹ çš„çŸ¥è¯†è¢«æ–°çŸ¥è¯†è¦†ç›–ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŠ¨æ€æ··åˆä¸“å®¶ï¼ˆDyMoEï¼‰æ–¹æ³•ï¼Œè®¾è®¡äº†å®šåˆ¶çš„æ­£åˆ™åŒ–æŸå¤±å‡½æ•°ï¼Œä»¥å¸®åŠ©æ–°ä¸“å®¶æœ‰æ•ˆå­¦ä¹ æ–°æ•°æ®ï¼ŒåŒæ—¶ä¿æŒç°æœ‰ä¸“å®¶è§£å†³æ—§ä»»åŠ¡çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ç±»åˆ«å¢é‡å­¦ä¹ ä¸­ç›¸è¾ƒäºæœ€ä½³åŸºçº¿æé«˜äº†4.92%çš„ç›¸å¯¹å‡†ç¡®ç‡ï¼Œå±•ç¤ºäº†å…¶å“è¶Šçš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¢é‡å›¾å­¦ä¹ ä¸­çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆä¿ç•™ä¸åŒæ—¶é—´ç‚¹å­¦ä¹ çš„çŸ¥è¯†ï¼Œå¯¼è‡´æ–°æ—§çŸ¥è¯†å†²çªã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºåŠ¨æ€æ··åˆä¸“å®¶ï¼ˆDyMoEï¼‰æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥é’ˆå¯¹æ–°æ•°æ®å—çš„ä¸“å®¶ç½‘ç»œï¼Œç»“åˆå®šåˆ¶çš„æ­£åˆ™åŒ–æŸå¤±å‡½æ•°ï¼Œä½¿å¾—æ–°æ—§ä¸“å®¶èƒ½å¤ŸååŒå·¥ä½œã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸€ä¸ªDyMoEå›¾ç¥ç»ç½‘ç»œå±‚ï¼ŒåŠ¨æ€æ·»åŠ æ–°ä¸“å®¶ï¼Œå¹¶é€šè¿‡ç¨€ç–MoEç­–ç•¥ä»…é€‰æ‹©æœ€ç›¸å…³çš„ä¸“å®¶è¿›è¡Œé¢„æµ‹ï¼Œä»è€Œé™ä½è®¡ç®—æˆæœ¬ã€‚

**å…³é”®åˆ›æ–°**ï¼šåŠ¨æ€æ··åˆä¸“å®¶æ¨¡å‹çš„è®¾è®¡ä½¿å¾—ä¸åŒæ—¶é—´ç‚¹çš„çŸ¥è¯†èƒ½å¤Ÿè¢«æœ‰æ•ˆåˆ©ç”¨ï¼Œé¿å…äº†ä¼ ç»Ÿæ–¹æ³•ä¸­çŸ¥è¯†çš„å†²çªå’Œé—å¿˜ã€‚

**å…³é”®è®¾è®¡**ï¼šå®šåˆ¶çš„æ­£åˆ™åŒ–æŸå¤±å‡½æ•°åˆ©ç”¨æ•°æ®åºåˆ—ä¿¡æ¯ï¼Œç¡®ä¿æ—§ä¸“å®¶åœ¨å­¦ä¹ æ–°ä»»åŠ¡æ—¶ä»èƒ½ä¿æŒå¯¹æ—§ä»»åŠ¡çš„è§£å†³èƒ½åŠ›ï¼ŒåŒæ—¶å¼•å…¥ç¨€ç–MoEç­–ç•¥ï¼Œä»…æ¿€æ´»å‰kä¸ªæœ€ç›¸å…³çš„ä¸“å®¶ä»¥å‡å°‘è®¡ç®—å¼€é”€ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒDyMoEæ¨¡å‹åœ¨ç±»åˆ«å¢é‡å­¦ä¹ ä»»åŠ¡ä¸­ç›¸è¾ƒäºæœ€ä½³åŸºçº¿æé«˜äº†4.92%çš„ç›¸å¯¹å‡†ç¡®ç‡ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å¤„ç†å¢é‡æ•°æ®æ—¶çš„ä¼˜è¶Šæ€§èƒ½å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç¤¾äº¤ç½‘ç»œåˆ†æã€æ¨èç³»ç»Ÿå’Œæ™ºèƒ½äº¤é€šç­‰é¢†åŸŸï¼Œèƒ½å¤Ÿåœ¨ä¸æ–­å˜åŒ–çš„æ•°æ®ç¯å¢ƒä¸­ä¿æŒæ¨¡å‹çš„æœ‰æ•ˆæ€§å’Œå‡†ç¡®æ€§ã€‚æœªæ¥ï¼Œéšç€æ•°æ®çš„æŒç»­å¢é•¿ï¼ŒDyMoEæ–¹æ³•æœ‰æœ›åœ¨å®æ—¶å­¦ä¹ å’Œåœ¨çº¿æ¨ç†ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Graph incremental learning is a learning paradigm that aims to adapt trained models to continuously incremented graphs and data over time without the need for retraining on the full dataset. However, regular graph machine learning methods suffer from catastrophic forgetting when applied to incremental learning settings, where previously learned knowledge is overridden by new knowledge. Previous approaches have tried to address this by treating the previously trained model as an inseparable unit and using techniques to maintain old behaviors while learning new knowledge. These approaches, however, do not account for the fact that previously acquired knowledge at different timestamps contributes differently to learning new tasks. Some prior patterns can be transferred to help learn new data, while others may deviate from the new data distribution and be detrimental. To address this, we propose a dynamic mixture-of-experts (DyMoE) approach for incremental learning. Specifically, a DyMoE GNN layer adds new expert networks specialized in modeling the incoming data blocks. We design a customized regularization loss that utilizes data sequence information so existing experts can maintain their ability to solve old tasks while helping the new expert learn the new data effectively. As the number of data blocks grows over time, the computational cost of the full mixture-of-experts (MoE) model increases. To address this, we introduce a sparse MoE approach, where only the top-$k$ most relevant experts make predictions, significantly reducing the computation time. Our model achieved 4.92\% relative accuracy increase compared to the best baselines on class incremental learning, showing the model's exceptional power.

