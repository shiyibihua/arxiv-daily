---
layout: default
title: Less is More: Learning Graph Tasks with Just LLMs
---

# Less is More: Learning Graph Tasks with Just LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.10115" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.10115v1</a>
  <a href="https://arxiv.org/pdf/2508.10115.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.10115v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.10115v1', 'Less is More: Learning Graph Tasks with Just LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sola Shirai, Kavitha Srinivas, Julian Dolby, Michael Katz, Horst Samulowitz, Shirin Sohrabi

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-13

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è§£å†³å›¾ä»»åŠ¡çš„æ–°æ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å›¾æ¨ç†` `å›¾ä»»åŠ¡` `æŒ‡å¯¼æ€§æ€ç»´é“¾` `æ¨¡å‹è®­ç»ƒ` `æ³›åŒ–èƒ½åŠ›`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å›¾æ¨ç†ä¸­ä¾èµ–äºä¸“é—¨çš„å›¾ç¼–ç æ¨¡å‹ï¼Œé™åˆ¶äº†å…¶çµæ´»æ€§å’Œé€‚ç”¨æ€§ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡æŒ‡å¯¼æ€§æ€ç»´é“¾è®­ç»ƒLLMsï¼Œä½¿å…¶èƒ½å¤Ÿç›´æ¥å­¦ä¹ å›¾ä»»åŠ¡ï¼Œé¿å…äº†å¤æ‚çš„å›¾ç¼–ç è¿‡ç¨‹ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œç»è¿‡è®­ç»ƒçš„LLMsåœ¨å¤šç§å›¾ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ¨å¹¿åˆ°æ–°ä»»åŠ¡å’Œç»“æ„ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œæ¨ç†å›¾ç»“æ„å¯ä»¥å¸®åŠ©è§£å†³è®¸å¤šé—®é¢˜ã€‚ä»¥å¾€çš„ç ”ç©¶å°è¯•é€šè¿‡ä¼˜åŒ–å›¾çš„æ–‡æœ¬åºåˆ—åŒ–å’Œç»“åˆå›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰ä¸LLMsæ¥æå‡å›¾æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•çš„ä¼˜ç¼ºç‚¹å°šä¸æ˜ç¡®ã€‚æœ¬æ–‡é€šè¿‡å®è¯ç ”ç©¶å›ç­”äº†ä¸‰ä¸ªå…³é”®é—®é¢˜ï¼šLLMsæ˜¯å¦èƒ½åœ¨æ²¡æœ‰ä¸“é—¨å›¾ç¼–ç æ¨¡å‹çš„æƒ…å†µä¸‹å­¦ä¹ è§£å†³åŸºæœ¬å›¾ä»»åŠ¡ï¼ŸLLMsæ˜¯å¦èƒ½å°†å­¦ä¹ åˆ°çš„è§£å†³æ–¹æ¡ˆæ¨å¹¿åˆ°æœªè§è¿‡çš„å›¾ç»“æ„æˆ–ä»»åŠ¡ï¼Ÿä¸åŒæ–¹æ³•å­¦ä¹ å›¾ä»»åŠ¡çš„ä¼˜ç¼ºç‚¹æ˜¯ä»€ä¹ˆï¼Ÿç ”ç©¶è¡¨æ˜ï¼Œå³ä½¿æ˜¯å°å‹LLMsï¼Œé€šè¿‡ä½¿ç”¨æŒ‡å¯¼æ€§æ€ç»´é“¾è§£å†³æ–¹æ¡ˆè¿›è¡Œè®­ç»ƒï¼Œä¹Ÿèƒ½å­¦ä¹ è§£å†³å›¾ä»»åŠ¡ï¼Œå¹¶ä¸”è¿™ç§è®­ç»ƒèƒ½å¤Ÿæ¨å¹¿åˆ°æ–°çš„ä»»åŠ¡å’Œå›¾ç»“æ„ï¼Œæ— éœ€ä¸“é—¨çš„å›¾ç¼–ç å™¨ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¦‚ä½•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç›´æ¥å­¦ä¹ å’Œè§£å†³å›¾ä»»åŠ¡çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºä¸“é—¨çš„å›¾ç¼–ç æ¨¡å‹ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹çš„çµæ´»æ€§å’Œæ¨å¹¿èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æŒ‡å¯¼æ€§æ€ç»´é“¾ï¼ˆchain-of-thoughtï¼‰è®­ç»ƒLLMsï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨æ²¡æœ‰ä¸“é—¨å›¾ç¼–ç çš„æƒ…å†µä¸‹ï¼Œç›´æ¥å­¦ä¹ è§£å†³å›¾ä»»åŠ¡ã€‚è¿™ç§æ–¹æ³•åˆ©ç”¨äº†LLMså¼ºå¤§çš„è¯­è¨€ç†è§£èƒ½åŠ›ï¼Œç®€åŒ–äº†å›¾ä»»åŠ¡çš„å¤„ç†æµç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®å‡†å¤‡ã€æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°ä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚åœ¨æ•°æ®å‡†å¤‡é˜¶æ®µï¼Œæ„å»ºäº†å¤šç§å›¾ä»»åŠ¡çš„æ•°æ®é›†ï¼›åœ¨æ¨¡å‹è®­ç»ƒé˜¶æ®µï¼Œä½¿ç”¨æŒ‡å¯¼æ€§æ€ç»´é“¾å¯¹LLMsè¿›è¡Œè®­ç»ƒï¼›åœ¨è¯„ä¼°é˜¶æ®µï¼Œé€šè¿‡å¯¹æ¯”å®éªŒéªŒè¯æ¨¡å‹çš„æ¨å¹¿èƒ½åŠ›å’Œæ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒæ–¹æ³•ï¼Œä½¿å¾—LLMsèƒ½å¤Ÿåœ¨æ²¡æœ‰ä¸“é—¨å›¾ç¼–ç å™¨çš„æƒ…å†µä¸‹ï¼Œå­¦ä¹ å’Œè§£å†³å›¾ä»»åŠ¡ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºï¼Œåè€…é€šå¸¸éœ€è¦å¤æ‚çš„å›¾ç»“æ„è¡¨ç¤ºã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶é€šè¿‡å¤šè½®è¿­ä»£è°ƒæ•´æ¨¡å‹å‚æ•°ï¼Œä»¥ç¡®ä¿å…¶åœ¨ä¸åŒå›¾ç»“æ„ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡æŒ‡å¯¼æ€§æ€ç»´é“¾è®­ç»ƒçš„LLMsåœ¨å¤šä¸ªå›¾ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè§£å†³æœªè§è¿‡çš„å›¾ç»“æ„é—®é¢˜ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæ¨¡å‹åœ¨å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ä¸Šå‡æœ‰æ˜¾è‘—æå‡ï¼Œå…·ä½“æ€§èƒ½æ•°æ®å°šæœªæŠ«éœ²ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç¤¾äº¤ç½‘ç»œåˆ†æã€çŸ¥è¯†å›¾è°±æ„å»ºã€æ¨èç³»ç»Ÿç­‰ã€‚é€šè¿‡ç®€åŒ–å›¾ä»»åŠ¡çš„å¤„ç†æµç¨‹ï¼Œèƒ½å¤Ÿæé«˜è¿™äº›é¢†åŸŸä¸­æ¨¡å‹çš„çµæ´»æ€§å’Œé€‚åº”æ€§ï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„è¿›ä¸€æ­¥å‘å±•ã€‚æœªæ¥ï¼Œéšç€LLMsçš„ä¸æ–­è¿›æ­¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½ä¼šåœ¨æ›´å¹¿æ³›çš„å›¾æ¨ç†ä»»åŠ¡ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> For large language models (LLMs), reasoning over graphs could help solve many problems. Prior work has tried to improve LLM graph reasoning by examining how best to serialize graphs as text and by combining GNNs and LLMs. However, the merits of such approaches remain unclear, so we empirically answer the following research questions: (1) Can LLMs learn to solve fundamental graph tasks without specialized graph encoding models?, (2) Can LLMs generalize learned solutions to unseen graph structures or tasks?, and (3) What are the merits of competing approaches to learn graph tasks? We show that even small LLMs can learn to solve graph tasks by training them with instructive chain-of-thought solutions, and this training generalizes, without specialized graph encoders, to new tasks and graph structures.

