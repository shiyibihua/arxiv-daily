---
layout: default
title: Nested-ReFT: Efficient Reinforcement Learning for Large Language Model Fine-Tuning via Off-Policy Rollouts
---

# Nested-ReFT: Efficient Reinforcement Learning for Large Language Model Fine-Tuning via Off-Policy Rollouts

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.10123" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.10123v2</a>
  <a href="https://arxiv.org/pdf/2508.10123.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.10123v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.10123v2', 'Nested-ReFT: Efficient Reinforcement Learning for Large Language Model Fine-Tuning via Off-Policy Rollouts')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Maxime Heuillet, Yufei Cui, Boxing Chen, Audrey Durand, Prasanna Parthasarathi

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-13 (æ›´æ–°: 2025-11-22)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºNested-ReFTä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒçš„é«˜è®¡ç®—æˆæœ¬é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `å¤§è¯­è¨€æ¨¡å‹` `å¾®è°ƒ` `è®¡ç®—æ•ˆç‡` `æ•°å­¦æ¨ç†` `ç¦»çº¿ç­–ç•¥` `åŠ¨æ€å±‚é€‰æ‹©`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ReFTæ–¹æ³•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­éœ€è¦å¤šæ¬¡æ¨ç†ï¼Œå¯¼è‡´è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚
2. Nested-ReFTé€šè¿‡éƒ¨åˆ†å±‚ä½œä¸ºè¡Œä¸ºæ¨¡å‹ç”Ÿæˆç¦»çº¿ç­–ç•¥çš„å®Œæˆï¼Œé™ä½äº†æ¨ç†æˆæœ¬ï¼Œæå‡äº†è®­ç»ƒæ•ˆç‡ã€‚
3. å®éªŒè¯æ˜ï¼ŒNested-ReFTåœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†ä¸Šè¡¨ç°å‡ºæ›´é«˜çš„è®¡ç®—æ•ˆç‡ï¼Œæå‡å¹…åº¦æ˜¾è‘—ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨å¤æ‚é¢†åŸŸå¦‚æ•°å­¦æ¨ç†ä¸­ï¼Œå…ˆè¿›çš„æ¨ç†èƒ½åŠ›å¯ä»¥é€šè¿‡å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å¾®è°ƒï¼ˆReFTï¼‰æ¥å®ç°ã€‚ä¼ ç»ŸReFTæ¡†æ¶ä¸­ï¼Œè¡Œä¸ºæ¨¡å‹ä¸ºæ¯ä¸ªé—®é¢˜ç”Ÿæˆå¤šä¸ªç­”æ¡ˆï¼Œéšåé€šè¿‡å¥–åŠ±å‡½æ•°è¿›è¡Œè¯„åˆ†ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­éœ€è¦å¤šæ¬¡æ¨ç†ï¼Œå¯¼è‡´è®¡ç®—æˆæœ¬æ˜¾è‘—ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ReFTæ¡†æ¶Nested-ReFTï¼Œåˆ©ç”¨ç›®æ ‡æ¨¡å‹çš„éƒ¨åˆ†å±‚ä½œä¸ºè¡Œä¸ºæ¨¡å‹ï¼Œåœ¨è®­ç»ƒæœŸé—´ç”Ÿæˆç¦»çº¿ç­–ç•¥çš„å®Œæˆï¼Œä»è€Œé™ä½æ¨ç†æˆæœ¬ã€‚ç†è®ºåˆ†æè¡¨æ˜ï¼ŒNested-ReFTèƒ½å¤Ÿæä¾›æ— åçš„æ¢¯åº¦ä¼°è®¡ï¼Œå¹¶æ§åˆ¶æ–¹å·®ã€‚å®è¯åˆ†ææ˜¾ç¤ºï¼Œåœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†å’Œæ¨¡å‹è§„æ¨¡ä¸Šï¼Œè®¡ç®—æ•ˆç‡å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä¼ ç»ŸReFTæ–¹æ³•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é«˜æ˜‚çš„è®¡ç®—æˆæœ¬é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•éœ€è¦å¤šæ¬¡æ¨ç†ç”Ÿæˆç­”æ¡ˆï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šNested-ReFTçš„æ ¸å¿ƒæ€æƒ³æ˜¯åˆ©ç”¨ç›®æ ‡æ¨¡å‹çš„éƒ¨åˆ†å±‚ä½œä¸ºè¡Œä¸ºæ¨¡å‹ï¼Œç”Ÿæˆç¦»çº¿ç­–ç•¥çš„å®Œæˆï¼Œä»è€Œå‡å°‘æ¨ç†æ­¥éª¤ï¼Œé™ä½è®¡ç®—è´Ÿæ‹…ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬è¡Œä¸ºæ¨¡å‹å’Œç›®æ ‡æ¨¡å‹ï¼Œè¡Œä¸ºæ¨¡å‹åœ¨è®­ç»ƒæœŸé—´åŠ¨æ€é€‰æ‹©å±‚è¿›è¡Œæ¨ç†ï¼Œç”Ÿæˆå¤šä¸ªç­”æ¡ˆä¾›åç»­è¯„åˆ†ã€‚

**å…³é”®åˆ›æ–°**ï¼šNested-ReFTçš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥äº†åŠ¨æ€å±‚è·³è¿‡æœºåˆ¶ï¼Œä½¿å¾—æ¨ç†è¿‡ç¨‹æ›´åŠ é«˜æ•ˆï¼ŒåŒæ—¶ä¿æŒæ— åçš„æ¢¯åº¦ä¼°è®¡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†åŠ¨æ€å±‚é€‰æ‹©çš„ç­–ç•¥ï¼Œç¡®ä¿åœ¨æ¯ä¸ªæ‰¹æ¬¡ä¸­æ ¹æ®éœ€è¦é€‰æ‹©ä¸åŒçš„å±‚è¿›è¡Œæ¨ç†ï¼Œä¼˜åŒ–äº†è®¡ç®—èµ„æºçš„ä½¿ç”¨ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒNested-ReFTåœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†ä¸Šå®ç°äº†æ˜¾è‘—çš„è®¡ç®—æ•ˆç‡æå‡ï¼Œå…·ä½“è¡¨ç°ä¸ºæ¯ç§’å¤„ç†çš„tokenæ•°é‡å¤§å¹…å¢åŠ ï¼Œä¸”åœ¨æ€§èƒ½ä¸Šä¸åŸºçº¿ReFTæ–¹æ³•ç›¸å½“ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€é‡‘èå’Œç§‘å­¦ç ”ç©¶ç­‰éœ€è¦å¤æ‚æ¨ç†çš„åœºæ™¯ã€‚é€šè¿‡æé«˜å¤§è¯­è¨€æ¨¡å‹åœ¨è¿™äº›é¢†åŸŸçš„æ¨ç†èƒ½åŠ›ï¼ŒNested-ReFTèƒ½å¤Ÿä¸ºå†³ç­–æ”¯æŒå’Œè‡ªåŠ¨åŒ–åˆ†ææä¾›æ›´é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œæœªæ¥å¯èƒ½å¯¹ç›¸å…³è¡Œä¸šäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Advanced reasoning in LLMs on challenging domains like mathematical reasoning can be tackled using verifiable rewards based reinforced fine-tuning (ReFT). In standard ReFT frameworks, a behavior model generates multiple completions with answers per problem, for the answer to be then scored by a reward function. While such RL post-training methods demonstrate significant performance improvements across challenging reasoning domains, the computational cost of generating completions during training with multiple inference steps makes the training cost non-trivial. To address this, we draw inspiration from off-policy RL, and speculative decoding to introduce a novel ReFT framework, dubbed Nested-ReFT, where a subset of layers of the target model acts as the behavior model to generate off-policy completions during training. The behavior model configured with dynamic layer skipping per batch during training decreases the inference cost compared to the standard ReFT frameworks. Our theoretical analysis shows that Nested-ReFT yields unbiased gradient estimates with controlled variance. Our empirical analysis demonstrates improved computational efficiency measured as tokens/sec across multiple math reasoning benchmarks and model sizes. Additionally, we explore three variants of bias mitigation to minimize the off-policyness in the gradient updates that allows for maintaining performance that matches the baseline ReFT performance.

