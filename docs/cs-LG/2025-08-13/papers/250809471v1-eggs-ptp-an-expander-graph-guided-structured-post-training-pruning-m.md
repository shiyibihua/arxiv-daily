---
layout: default
title: EGGS-PTP: An Expander-Graph Guided Structured Post-training Pruning Method for Large Language Models
---

# EGGS-PTP: An Expander-Graph Guided Structured Post-training Pruning Method for Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.09471" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.09471v1</a>
  <a href="https://arxiv.org/pdf/2508.09471.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.09471v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.09471v1', 'EGGS-PTP: An Expander-Graph Guided Structured Post-training Pruning Method for Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Omar Bazarbachi, Zijun Sun, Yanning Shen

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-13

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEGGS-PTPä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹çš„è®¡ç®—ä¸å†…å­˜æŒ‘æˆ˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `ç»“æ„åŒ–å‰ªæ` `æ‰©å±•å›¾` `åè®­ç»ƒä¼˜åŒ–` `æ¨¡å‹å‹ç¼©` `è®¡ç®—æ•ˆç‡` `å†…å­˜ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤§è¯­è¨€æ¨¡å‹åœ¨è®¡ç®—å’Œå†…å­˜æ–¹é¢é¢ä¸´ä¸¥å³»æŒ‘æˆ˜ï¼Œå¯¼è‡´éƒ¨ç½²å›°éš¾ã€‚
2. EGGS-PTPé€šè¿‡æ‰©å±•å›¾ç†è®ºæŒ‡å¯¼N:Mç»“æ„åŒ–å‰ªæï¼Œä¼˜åŒ–æ¨¡å‹çš„å¤§å°å’Œè®¡ç®—æ•ˆç‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒEGGS-PTPåœ¨åŠ é€Ÿå’Œå†…å­˜èŠ‚çœæ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶åœ¨å‡†ç¡®æ€§ä¸Šè¶…è¶Šäº†ä¼ ç»Ÿå‰ªææ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¹¿æ³›åº”ç”¨åŠè§„æ¨¡ä¸æ–­æ‰©å¤§ï¼Œéƒ¨ç½²è¿™äº›åºå¤§åŸºç¡€æ¨¡å‹æ‰€é¢ä¸´çš„è®¡ç®—å’Œå†…å­˜æŒ‘æˆ˜æ—¥ç›Šä¸¥é‡ã€‚è¿™å‡¸æ˜¾äº†å¼€å‘æ›´é«˜æ•ˆæ¨¡å‹å˜ä½“çš„è¿«åˆ‡éœ€æ±‚ã€‚ä¸ºåº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†EGGS-PTPï¼šä¸€ç§åŸºäºæ‰©å±•å›¾çš„ç»“æ„åŒ–åè®­ç»ƒå‰ªææ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å›¾è®ºæŒ‡å¯¼N:Mç»“æ„åŒ–å‰ªæçš„è®¾è®¡ï¼Œæœ‰æ•ˆå‡å°‘æ¨¡å‹å¤§å°å’Œè®¡ç®—éœ€æ±‚ã€‚é€šè¿‡å¼•å…¥æ‰©å±•å›¾çš„æ¦‚å¿µï¼ŒEGGS-PTPç¡®ä¿äº†å‰ªæç½‘ç»œå†…çš„ä¿¡æ¯æµåŠ¨ï¼Œä¿ç•™äº†æ¨¡å‹çš„åŸºæœ¬åŠŸèƒ½ã€‚å¤§é‡æ•°å€¼å®éªŒè¡¨æ˜ï¼ŒEGGS-PTPä¸ä»…å› ç»“æ„ç¨€ç–æ€§å®ç°äº†æ˜¾è‘—çš„åŠ é€Ÿå’Œå†…å­˜èŠ‚çœï¼Œè¿˜åœ¨å¤šä¸ªLLMçš„å‡†ç¡®æ€§ä¸Šè¶…è¶Šäº†ç°æœ‰çš„ç»“æ„åŒ–å‰ªææŠ€æœ¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨è®¡ç®—å’Œå†…å­˜ä½¿ç”¨ä¸Šçš„é«˜æˆæœ¬é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å‰ªæè¿‡ç¨‹ä¸­å¾€å¾€æ— æ³•æœ‰æ•ˆä¿æŒæ¨¡å‹æ€§èƒ½ï¼Œå¯¼è‡´ä¿¡æ¯æµå¤±å’ŒåŠŸèƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šEGGS-PTPé€šè¿‡å¼•å…¥æ‰©å±•å›¾çš„æ¦‚å¿µï¼Œè®¾è®¡äº†ä¸€ç§ç»“æ„åŒ–çš„åè®­ç»ƒå‰ªææ–¹æ³•ï¼Œç¡®ä¿åœ¨å‰ªæè¿‡ç¨‹ä¸­ä¿¡æ¯æµçš„æœ‰æ•ˆæ€§ï¼Œä»è€Œä¿ç•™æ¨¡å‹çš„æ ¸å¿ƒåŠŸèƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šEGGS-PTPçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ‰©å±•å›¾æ„å»ºã€N:Mç»“æ„åŒ–å‰ªæå’Œæ¨¡å‹è¯„ä¼°å››ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡å›¾è®ºæ„å»ºæ‰©å±•å›¾ï¼Œç„¶ååœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œç»“æ„åŒ–å‰ªæï¼Œæœ€åè¯„ä¼°å‰ªæåçš„æ¨¡å‹æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šEGGS-PTPçš„åˆ›æ–°åœ¨äºå°†æ‰©å±•å›¾ç†è®ºåº”ç”¨äºæ¨¡å‹å‰ªæï¼Œç¡®ä¿äº†å‰ªæåæ¨¡å‹çš„ä¿¡æ¯æµåŠ¨æ€§ï¼Œä¸ä¼ ç»Ÿå‰ªææ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°ä¿ç•™æ¨¡å‹çš„åŠŸèƒ½å’Œå‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨EGGS-PTPä¸­ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬å‰ªææ¯”ä¾‹ã€ç»“æ„åŒ–æ–¹å¼ï¼ˆN:Mï¼‰ï¼Œä»¥åŠæŸå¤±å‡½æ•°çš„è®¾è®¡ï¼Œç¡®ä¿åœ¨å‰ªæè¿‡ç¨‹ä¸­å°½é‡å‡å°‘æ€§èƒ½æŸå¤±ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒEGGS-PTPåœ¨å¤šä¸ªå¤§è¯­è¨€æ¨¡å‹ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨åŠ é€Ÿå’Œå†…å­˜èŠ‚çœæ–¹é¢çš„æå‡å¹…åº¦è¶…è¿‡äº†ç°æœ‰ç»“æ„åŒ–å‰ªææŠ€æœ¯ï¼Œä¸”åœ¨å‡†ç¡®æ€§ä¸Šä¿æŒäº†è¾ƒé«˜æ°´å¹³ï¼Œå±•ç¤ºäº†å…¶ä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

EGGS-PTPæ–¹æ³•å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦é«˜æ•ˆéƒ¨ç½²å¤§è¯­è¨€æ¨¡å‹çš„åœºæ™¯ä¸­ï¼Œå¦‚æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨ç¿»è¯‘å’Œå†…å®¹ç”Ÿæˆç­‰é¢†åŸŸã€‚é€šè¿‡é™ä½è®¡ç®—å’Œå†…å­˜éœ€æ±‚ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä½¿å¾—å¤§è¯­è¨€æ¨¡å‹åœ¨èµ„æºæœ‰é™çš„ç¯å¢ƒä¸­æ›´ä¸ºå¯è¡Œï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„æ™®åŠä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As Large Language Models (LLMs) become more widely adopted and scale up in size, the computational and memory challenges involved in deploying these massive foundation models have grown increasingly severe. This underscores the urgent need to develop more efficient model variants. Faced with this challenge, the present work introduces EGGS-PTP: an Expander-Graph Guided Structured Post-training Pruning method. The proposed approach leverages graph theory to guide the design of N:M structured pruning, effectively reducing model size and computational demands. By incorporating concepts from expander graphs, EGGS-PTP ensures information flow within the pruned network, preserving essential model functionality. Extensive numerical experiments demonstrate that EGGS-PTP not only achieves significant acceleration and memory savings due to structured sparsity but also outperforms existing structured pruning techniques in terms of accuracy across various LLMs.

