---
layout: default
title: Beyond Scaling Law: A Data-Efficient Distillation Framework for Reasoning
---

# Beyond Scaling Law: A Data-Efficient Distillation Framework for Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.09883" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.09883v1</a>
  <a href="https://arxiv.org/pdf/2508.09883.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.09883v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.09883v1', 'Beyond Scaling Law: A Data-Efficient Distillation Framework for Reasoning')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Xiaojun Wu, Xiaoguang Jiang, Huiyang Li, Jucai Zhai, Dengfeng Liu, Qiaobo Hao, Huang Liu, Zhiguo Yang, Ji Xie, Ninglun Gu, Jin Yang, Kailai Zhang, Yelun Bao, Jun Wang

**ÂàÜÁ±ª**: cs.LG, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-13

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Êï∞ÊçÆÈ´òÊïàËí∏È¶èÊ°ÜÊû∂‰ª•‰ºòÂåñÊé®ÁêÜËÉΩÂäõ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Êï∞ÊçÆÈ´òÊïàËí∏È¶è` `Êé®ÁêÜËÉΩÂäõ` `ÊïôÂ∏àÊ®°ÂûãÈÄâÊã©` `Â∞èËØ≠ÊñôÂ∫ì` `Â§öÊ†∑ÂåñÊé®ÁêÜËΩ®Ëøπ` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `Êï∞Â≠¶Êé®ÁêÜ` `‰ª£Á†ÅÁîüÊàê`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®Êé®ÁêÜËÉΩÂäõÊèêÂçá‰∏ä‰æùËµñ‰∫éÂ§ßËßÑÊ®°Êï∞ÊçÆÈõÜÔºåÂØºËá¥ËÆ°ÁÆóÊàêÊú¨È´ò‰∏îÊïàÊûú‰∏çÁ®≥ÂÆö„ÄÇ
2. ÊèêÂá∫ÁöÑÊï∞ÊçÆÈ´òÊïàËí∏È¶èÊ°ÜÊû∂ÈÄöËøá‰ºòÂåñÊïôÂ∏àÊ®°ÂûãÈÄâÊã©ÂíåÂ∞èËØ≠ÊñôÂ∫ì‰ΩøÁî®ÔºåÊèêÂçáÊé®ÁêÜËÉΩÂäõ„ÄÇ
3. Âú®Êï∞Â≠¶Êé®ÁêÜÂíå‰ª£Á†ÅÁîüÊàê‰ªªÂä°‰∏≠ÔºåDEDÊñπÊ≥ï‰ª•0.8kÁ§∫‰æãÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩÔºåË∂ÖË∂ä‰∫ÜÁé∞ÊúâÊñπÊ≥ï„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ÁÆóÊ≥ïÁºñÁ†ÅÂíåÊï∞Â≠¶ÈóÆÈ¢òËß£ÂÜ≥Á≠â‰ªªÂä°‰∏≠Â±ïÁé∞Âá∫ÂçìË∂äÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÂ∞ΩÁÆ°Áé∞ÊúâÊñπÊ≥ïÈÄöËøáÊâ©Â±ïËØ≠ÊñôÂ∫ìÂíåÁªìÂêàÂº∫ÂåñÂ≠¶‰π†‰∏éÁõëÁù£ÂæÆË∞ÉÁöÑÂ§öÈò∂ÊÆµËÆ≠ÁªÉÊù•ÊèêÂçáÊé®ÁêÜËÉΩÂäõÔºå‰ΩÜÊé®ÁêÜÁöÑËßÑÊ®°Ê≥ïÂàô‰ªçÂú®ÂΩ¢Êàê‰∏≠ÔºåÂØºËá¥ËÆ°ÁÆóÊàêÊú¨Â¢ûÂä†„ÄÇ‰∏∫Ê≠§ÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊï∞ÊçÆÈ´òÊïàËí∏È¶èÊ°ÜÊû∂ÔºàDEDÔºâÔºåÊó®Âú®‰ºòÂåñÊé®ÁêÜËí∏È¶èÁöÑÂ∏ïÁ¥ØÊâòÂâçÊ≤ø„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÂåÖÊã¨‰∏â‰∏™ÂÖ≥ÈîÆÁÇπÔºöÈ¶ñÂÖàÔºåÈÄöËøáÂØπÈ¢ÜÂÖàÊé®ÁêÜLLMsÁöÑÂÖ®Èù¢ÊØîËæÉÔºåÂºÄÂèë‰∫Ü‰∏ÄÁßçÈÄâÊã©ÊúÄ‰Ω≥ÊïôÂ∏àÊ®°ÂûãÁöÑÊñπÊ≥ïÔºõÂÖ∂Ê¨°ÔºåÁ≤æÂøÉÁ≠ñÂàíÁöÑÂ∞èËØ≠ÊñôÂ∫ìÂú®È¢ÜÂüüÂÜÖÂ§ñËÉΩÂäõ‰πãÈó¥ÂÆûÁé∞‰∫ÜÂπ≥Ë°°ÔºõÊúÄÂêéÔºåÂ§öÊ†∑ÂåñÁöÑÊé®ÁêÜËΩ®ËøπÈºìÂä±Â≠¶ÁîüÊ®°ÂûãÂèëÂ±ïÁ®≥ÂÅ•ÁöÑÊé®ÁêÜÊäÄËÉΩ„ÄÇÊàë‰ª¨Âú®Êï∞Â≠¶Êé®ÁêÜÂíå‰ª£Á†ÅÁîüÊàê‰ªªÂä°‰∏äÈ™åËØÅ‰∫ÜËØ•ÊñπÊ≥ïÔºåÂèñÂæó‰∫ÜÊúÄÂÖàËøõÁöÑÁªìÊûúÔºå‰ªÖÈúÄ0.8kÁ≤æÂøÉÁ≠ñÂàíÁöÑÁ§∫‰æãÔºåÈÅøÂÖç‰∫ÜÂ§ßËßÑÊ®°Êâ©Â±ïÁöÑÈúÄÊ±Ç„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâÊé®ÁêÜËí∏È¶èÊñπÊ≥ïÂú®ËÆ°ÁÆóÊàêÊú¨ÂíåÈ¢ÜÂüüÂ§ñÊÄßËÉΩ‰∏ãÈôçÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏‰æùËµñ‰∫éÂ§ßËßÑÊ®°Êï∞ÊçÆÈõÜÔºåÂØºËá¥ËµÑÊ∫êÊµ™Ë¥πÂíåÊïàÊûú‰∏ç‰Ω≥„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊï∞ÊçÆÈ´òÊïàËí∏È¶èÊ°ÜÊû∂ÔºàDEDÔºâÔºåÈÄöËøá‰ºòÂåñÊïôÂ∏àÊ®°ÂûãÈÄâÊã©Âíå‰ΩøÁî®Â∞èËßÑÊ®°ËØ≠ÊñôÂ∫ìÔºåÊèêÂçáÊé®ÁêÜËÉΩÂäõÔºåÂêåÊó∂‰øùÊåÅÈ¢ÜÂüüÂ§ñÊÄßËÉΩ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöDEDÊ°ÜÊû∂ÂåÖÊã¨‰∏â‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöÊïôÂ∏àÊ®°ÂûãÈÄâÊã©„ÄÅËØ≠ÊñôÂ∫ì‰ºòÂåñÂíåÂ§öÊ†∑ÂåñÊé®ÁêÜËΩ®ËøπÁîüÊàê„ÄÇÈ¶ñÂÖàÔºåÈÄöËøáÂØπÊØîÂàÜÊûêÈÄâÊã©ÊúÄ‰Ω≥ÊïôÂ∏àÊ®°ÂûãÔºõÂÖ∂Ê¨°Ôºå‰ΩøÁî®Á≤æÂøÉÁ≠ñÂàíÁöÑÂ∞èËØ≠ÊñôÂ∫ìÔºõÊúÄÂêéÔºåÁîüÊàêÂ§öÊ†∑ÂåñÁöÑÊé®ÁêÜËΩ®Ëøπ‰ª•Â¢ûÂº∫Â≠¶ÁîüÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞Âú®‰∫éÊèêÂá∫‰∫Ü‰ºòÂåñÊïôÂ∏àÊ®°ÂûãÈÄâÊã©ÁöÑÊñπÊ≥ïÔºå‰ª•ÂèäÈÄöËøáÂ∞èËØ≠ÊñôÂ∫ìÂÆûÁé∞È¢ÜÂüüÂÜÖÂ§ñËÉΩÂäõÁöÑÂπ≥Ë°°„ÄÇËøô‰∏éÁé∞ÊúâÊñπÊ≥ï‰æùËµñ‰∫éÂ§ßËßÑÊ®°Êï∞ÊçÆÈõÜÁöÑÊú¨Ë¥®Âå∫Âà´ÊòæËëó„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Ê®°ÂûãËÆ≠ÁªÉ‰∏≠ÔºåÈááÁî®‰∫ÜÁâπÂÆöÁöÑÊçüÂ§±ÂáΩÊï∞ÂíåÂèÇÊï∞ËÆæÁΩÆÔºå‰ª•Á°Æ‰øùÊïôÂ∏àÊ®°ÂûãÁöÑÊúâÊïàÊÄßÂíåÂ≠¶ÁîüÊ®°ÂûãÁöÑÂ≠¶‰π†ÊïàÁéá„ÄÇÈÄöËøáÂ§öÊ†∑ÂåñÁöÑÊé®ÁêÜËΩ®ËøπËÆæËÆ°ÔºåÂ¢ûÂº∫‰∫ÜÂ≠¶ÁîüÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÂÖ∑‰ΩìÁöÑÁΩëÁªúÁªìÊûÑÂíåÂèÇÊï∞ËÆæÁΩÆÂú®ÂÆûÈ™å‰∏≠ËøõË°å‰∫ÜËØ¶ÁªÜÈ™åËØÅ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Âú®Êï∞Â≠¶Êé®ÁêÜÔºàAIME 2024/2025ÔºåMATH-500ÔºâÂíå‰ª£Á†ÅÁîüÊàêÔºàLiveCodeBenchÔºâ‰ªªÂä°‰∏≠ÔºåDEDÊñπÊ≥ï‰ªÖ‰ΩøÁî®0.8kÁ≤æÂøÉÁ≠ñÂàíÁöÑÁ§∫‰æãÔºåÂèñÂæó‰∫ÜÊúÄÂÖàËøõÁöÑÁªìÊûúÔºåÊòæËëóË∂ÖË∂ä‰∫ÜÁé∞ÊúâÊñπÊ≥ïÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®Êé®ÁêÜËÉΩÂäõÊèêÂçá‰∏äÁöÑÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨ÊïôËÇ≤„ÄÅÁºñÁ®ãËæÖÂä©ÂíåËá™Âä®ÂåñÊé®ÁêÜÁ≥ªÁªü„ÄÇÈÄöËøáÊèêÂçáÊé®ÁêÜËÉΩÂäõÔºåDEDÊ°ÜÊû∂ËÉΩÂ§üÂú®Êõ¥Â∞ëÁöÑÊï∞ÊçÆ‰∏ãÂÆûÁé∞Êõ¥È´òÊïàÁöÑÂ≠¶‰π†ÔºåÂÖ∑ÊúâÂπøÊ≥õÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÊú™Êù•ÂΩ±ÂìçÔºåÂ∞§ÂÖ∂ÊòØÂú®ËµÑÊ∫êÂèóÈôêÁöÑÁéØÂ¢É‰∏≠„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Large language models (LLMs) demonstrate remarkable reasoning capabilities in tasks such as algorithmic coding and mathematical problem-solving. Recent methods have improved reasoning through expanded corpus and multistage training combining reinforcement learning and supervised fine-tuning. Although some methods suggest that small but targeted dataset can incentivize reasoning via only distillation, a reasoning scaling laws is still taking shape, increasing computational costs. To address this, we propose a data-efficient distillation framework (DED) that optimizes the Pareto frontier of reasoning distillation. Inspired by the on-policy learning and diverse roll-out strategies of reinforcement learning, the key idea of our approach is threefold: (1) We identify that benchmark scores alone do not determine an effective teacher model. Through comprehensive comparisons of leading reasoning LLMs, we develop a method to select an optimal teacher model. (2) While scaling distillation can enhance reasoning, it often degrades out-of-domain performance. A carefully curated, smaller corpus achieves a balanced trade-off between in-domain and out-of-domain capabilities. (3) Diverse reasoning trajectories encourage the student model to develop robust reasoning skills. We validate our method through evaluations on mathematical reasoning (AIME 2024/2025, MATH-500) and code generation (LiveCodeBench), achieving state-of-the-art results with only 0.8k carefully curated examples, bypassing the need for extensive scaling. Our systematic analysis demonstrates that DED outperforms existing methods by considering factors beyond superficial hardness, token length, or teacher model capability. This work offers a practical and efficient pathway to advanced reasoning while preserving general capabilities.

