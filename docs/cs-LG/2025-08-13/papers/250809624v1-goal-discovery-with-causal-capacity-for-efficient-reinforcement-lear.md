---
layout: default
title: Goal Discovery with Causal Capacity for Efficient Reinforcement Learning
---

# Goal Discovery with Causal Capacity for Efficient Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.09624" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.09624v1</a>
  <a href="https://arxiv.org/pdf/2508.09624.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.09624v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.09624v1', 'Goal Discovery with Causal Capacity for Efficient Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yan Yu, Yaodong Yang, Zhengbo Lu, Chengdong Ma, Wengang Zhou, Houqiang Li

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-13

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå› æœèƒ½åŠ›ç›®æ ‡å‘ç°æ¡†æ¶ä»¥æå‡å¼ºåŒ–å­¦ä¹ æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å› æœæ¨æ–­` `å¼ºåŒ–å­¦ä¹ ` `ç›®æ ‡å‘ç°` `æ™ºèƒ½ä½“æ¢ç´¢` `å¤šç›®æ ‡ä»»åŠ¡` `è’™ç‰¹å¡æ´›æ–¹æ³•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤æ‚åœºæ™¯ä¸­éš¾ä»¥æœ‰æ•ˆæµ‹é‡å› æœå…³ç³»ï¼Œå¯¼è‡´æ™ºèƒ½ä½“æ¢ç´¢æ•ˆç‡ä½ä¸‹ã€‚
2. æå‡ºå› æœèƒ½åŠ›åº¦é‡ï¼Œè¯†åˆ«å…³é”®å†³ç­–ç‚¹ï¼Œå¹¶å°†å…¶ä½œä¸ºå­ç›®æ ‡å¼•å¯¼æ™ºèƒ½ä½“è¿›è¡Œæœ‰ç›®çš„çš„æ¢ç´¢ã€‚
3. å®éªŒè¯æ˜ï¼ŒGDCCåœ¨å¤šç›®æ ‡ä»»åŠ¡ä¸­æ˜¾è‘—æé«˜æˆåŠŸç‡ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•è¡¨ç°æ›´ä½³ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å› æœæ¨æ–­å¯¹äººç±»æ¢ç´¢ä¸–ç•Œè‡³å…³é‡è¦ï¼Œå¯ä»¥é€šè¿‡å»ºæ¨¡ä½¿æ™ºèƒ½ä½“åœ¨å¼ºåŒ–å­¦ä¹ ä¸­é«˜æ•ˆæ¢ç´¢ç¯å¢ƒã€‚ç°æœ‰ç ”ç©¶è¡¨æ˜ï¼Œå»ºç«‹åŠ¨ä½œä¸çŠ¶æ€è½¬ç§»ä¹‹é—´çš„å› æœå…³ç³»èƒ½å¤Ÿå¢å¼ºæ™ºèƒ½ä½“æ¨ç†æ”¿ç­–å¦‚ä½•å½±å“æœªæ¥è½¨è¿¹ï¼Œä»è€Œä¿ƒè¿›æœ‰é’ˆå¯¹æ€§çš„æ¢ç´¢ã€‚ç„¶è€Œï¼Œç”±äºå¤æ‚åœºæ™¯ä¸­çŠ¶æ€-åŠ¨ä½œç©ºé—´çš„åºå¤§ï¼Œå› æœå…³ç³»çš„æµ‹é‡é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å› æœèƒ½åŠ›ç›®æ ‡å‘ç°æ¡†æ¶ï¼ˆGDCCï¼‰ï¼Œé€šè¿‡æ¨å¯¼çŠ¶æ€ç©ºé—´ä¸­çš„å› æœèƒ½åŠ›åº¦é‡ï¼Œè¯†åˆ«å…³é”®å†³ç­–ç‚¹ï¼Œä¼˜åŒ–æ¢ç´¢è¿‡ç¨‹ã€‚å®éªŒè¯æ˜ï¼Œå…·æœ‰é«˜å› æœèƒ½åŠ›çš„çŠ¶æ€ä¸é¢„æœŸå­ç›®æ ‡ä¸€è‡´ï¼ŒGDCCåœ¨å¤šç›®æ ‡ä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†æˆåŠŸç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨å¤æ‚ç¯å¢ƒä¸­å› æœå…³ç³»æµ‹é‡çš„å›°éš¾ï¼Œç°æœ‰æ–¹æ³•æ— æ³•æœ‰æ•ˆå¼•å¯¼æ™ºèƒ½ä½“è¿›è¡Œé«˜æ•ˆæ¢ç´¢ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºå› æœèƒ½åŠ›åº¦é‡ï¼Œè¡¨ç¤ºæ™ºèƒ½ä½“è¡Œä¸ºå¯¹æœªæ¥è½¨è¿¹çš„æœ€å¤§å½±å“ï¼Œé€šè¿‡è¯†åˆ«å…³é”®å†³ç­–ç‚¹æ¥ä¼˜åŒ–æ¢ç´¢è¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGDCCæ¡†æ¶åŒ…æ‹¬å› æœèƒ½åŠ›çš„è®¡ç®—ã€å…³é”®ç‚¹è¯†åˆ«å’Œæ¢ç´¢ä¼˜åŒ–ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼Œé‡‡ç”¨è’™ç‰¹å¡æ´›æ–¹æ³•è¿›è¡Œå…³é”®ç‚¹çš„è¯†åˆ«ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥å› æœèƒ½åŠ›åº¦é‡ï¼Œèƒ½å¤Ÿåœ¨é«˜ç»´è¿ç»­ç¯å¢ƒä¸­æœ‰æ•ˆè¯†åˆ«æ™ºèƒ½ä½“çš„å…³é”®å†³ç­–ç‚¹ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæä¾›äº†æ›´å…·é’ˆå¯¹æ€§çš„æ¢ç´¢ç­–ç•¥ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼Œé‡‡ç”¨è’™ç‰¹å¡æ´›æ–¹æ³•è¿›è¡Œå…³é”®ç‚¹è¯†åˆ«ï¼Œå¹¶é’ˆå¯¹ç¦»æ•£å’Œè¿ç»­çŠ¶æ€ç©ºé—´è¿›è¡Œäº†ä¼˜åŒ–ï¼Œç¡®ä¿åœ¨å¤æ‚ç¯å¢ƒä¸­ä¿æŒé«˜æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGDCCåœ¨å¤šç›®æ ‡ä»»åŠ¡ä¸­æˆåŠŸç‡æ˜¾è‘—æé«˜ï¼Œå°¤å…¶æ˜¯åœ¨é«˜å› æœèƒ½åŠ›çŠ¶æ€ä¸‹ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•æå‡å¹…åº¦è¾¾åˆ°XX%ï¼ˆå…·ä½“æ•°æ®å¾…è¡¥å……ï¼‰ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ã€æ¸¸æˆæ™ºèƒ½ä½“ç­‰ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡æ™ºèƒ½ä½“åœ¨å¤æ‚ç¯å¢ƒä¸­çš„æ¢ç´¢æ•ˆç‡ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Causal inference is crucial for humans to explore the world, which can be modeled to enable an agent to efficiently explore the environment in reinforcement learning. Existing research indicates that establishing the causality between action and state transition will enhance an agent to reason how a policy affects its future trajectory, thereby promoting directed exploration. However, it is challenging to measure the causality due to its intractability in the vast state-action space of complex scenarios. In this paper, we propose a novel Goal Discovery with Causal Capacity (GDCC) framework for efficient environment exploration. Specifically, we first derive a measurement of causality in state space, \emph{i.e.,} causal capacity, which represents the highest influence of an agent's behavior on future trajectories. After that, we present a Monte Carlo based method to identify critical points in discrete state space and further optimize this method for continuous high-dimensional environments. Those critical points are used to uncover where the agent makes important decisions in the environment, which are then regarded as our subgoals to guide the agent to make exploration more purposefully and efficiently. Empirical results from multi-objective tasks demonstrate that states with high causal capacity align with our expected subgoals, and our GDCC achieves significant success rate improvements compared to baselines.

