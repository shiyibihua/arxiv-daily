---
layout: default
title: HierMoE: Accelerating MoE Training with Hierarchical Token Deduplication and Expert Swap
---

# HierMoE: Accelerating MoE Training with Hierarchical Token Deduplication and Expert Swap

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.09591" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.09591v1</a>
  <a href="https://arxiv.org/pdf/2508.09591.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.09591v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.09591v1', 'HierMoE: Accelerating MoE Training with Hierarchical Token Deduplication and Expert Swap')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Wenxiang Lin, Xinglin Pan, Lin Zhang, Shaohuai Shi, Xuan Wang, Xiaowen Chu

**ÂàÜÁ±ª**: cs.DC, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-13

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫HierMoE‰ª•Ëß£ÂÜ≥MoEÊ®°ÂûãËÆ≠ÁªÉ‰∏≠ÁöÑÈÄö‰ø°‰∏éË¥üËΩΩ‰∏çÂùáÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `‰∏ìÂÆ∂Ê∑∑ÂêàÊ®°Âûã` `Á®ÄÁñèÊøÄÊ¥ª` `ÂàÜÂ∏ÉÂºèËÆ≠ÁªÉ` `GPUË¥üËΩΩÂùáË°°` `ÈÄö‰ø°‰ºòÂåñ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâMoEÊ®°ÂûãÂú®Âä®ÊÄÅÊøÄÊ¥ª‰∏ìÂÆ∂Êó∂ÔºåÂØºËá¥GPUÈó¥ÈÄö‰ø°ÂíåË¥üËΩΩ‰∏çÂùáÔºåÂΩ±ÂìçËÆ≠ÁªÉÊïàÁéá„ÄÇ
2. Êú¨ÊñáÊèêÂá∫HierMoEÔºåÈÄöËøá‰ª§ÁâåÂéªÈáçÂíå‰∏ìÂÆ∂‰∫§Êç¢ÊäÄÊúØÔºå‰ºòÂåñMoEÊ®°ÂûãÁöÑËÆ≠ÁªÉËøáÁ®ã„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåHierMoEÂú®ÈÄö‰ø°ÈÄüÂ∫¶ÂíåÁ´ØÂà∞Á´ØËÆ≠ÁªÉ‰∏äÂùáÊòæËëó‰ºò‰∫éÁé∞ÊúâÁöÑMoEËÆ≠ÁªÉÁ≥ªÁªü„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Á®ÄÁñèÊøÄÊ¥ªÁöÑ‰∏ìÂÆ∂Ê∑∑ÂêàÔºàMoEÔºâÂèòÊç¢Âô®Âõ†ÂÖ∂Á®ÄÁñèÊÄßËÄåÊàê‰∏∫Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÂ∏∏ËßÅÊû∂ÊûÑÔºåËÉΩÂ§üÂú®ËæÉ‰ΩéËÆ°ÁÆóÈúÄÊ±Ç‰∏ãËΩªÊùæÊâ©Â±ïÊ®°ÂûãËßÑÊ®°„ÄÇÁÑ∂ËÄåÔºåMoEÊ®°ÂûãÂú®Âä®ÊÄÅÈÄâÊã©ÊøÄÊ¥ªÁâπÂÆö‰∏ìÂÆ∂Êó∂ÔºåÂèØËÉΩÂØºËá¥ÈÄö‰ø°ÂíåË¥üËΩΩ‰∏çÂùáË°°ÔºåÈòªÁ¢ç‰∫ÜÂàÜÂ∏ÉÂºèÁ≥ªÁªüÁöÑÂèØÊâ©Â±ïÊÄß„ÄÇ‰∏∫Ê≠§ÔºåÊú¨ÊñáÊèêÂá∫HierMoEÔºåÈÄöËøá‰∏§ÁßçÊãìÊâëÊÑüÁü•ÊäÄÊúØÂä†ÈÄüMoEÊ®°ÂûãËÆ≠ÁªÉÔºö1Ôºâ‰ª§ÁâåÂéªÈáç‰ª•ÂáèÂ∞ëÈÄö‰ø°ÊµÅÈáèÔºå2Ôºâ‰∏ìÂÆ∂‰∫§Êç¢‰ª•Âπ≥Ë°°GPUÈó¥ÁöÑÂ∑•‰ΩúË¥üËΩΩ„ÄÇÊàë‰ª¨Âú®‰∏çÂêåÊ®°ÂûãÈÖçÁΩÆÂíåÁ°¨‰ª∂ÁéØÂ¢É‰∏ãÂª∫Á´ãÁêÜËÆ∫Ê®°ÂûãÔºå‰ª•ÂÆûÁé∞ÊúÄ‰Ω≥ÁöÑ‰ª§ÁâåÂéªÈáçÂíå‰∏ìÂÆ∂‰∫§Êç¢Á≠ñÁï•„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåHierMoEÂú®32-GPUÈõÜÁæ§‰∏äÂÆûÁé∞‰∫Ü1.55ÂÄçËá≥3.32ÂÄçÁöÑÈÄö‰ø°Âä†ÈÄüÔºåÂπ∂‰∏îÁ´ØÂà∞Á´ØËÆ≠ÁªÉÈÄüÂ∫¶ÊèêÂçá‰∫Ü1.18ÂÄçËá≥1.27ÂÄçÔºåÁõ∏ËæÉ‰∫éÁé∞ÊúâÁöÑMoEËÆ≠ÁªÉÁ≥ªÁªüË°®Áé∞‰ºòÂºÇ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥MoEÊ®°ÂûãËÆ≠ÁªÉ‰∏≠Áî±‰∫éÂä®ÊÄÅÊøÄÊ¥ª‰∏ìÂÆ∂ÂØºËá¥ÁöÑÈÄö‰ø°Ë¥üÊãÖÂíåË¥üËΩΩ‰∏çÂùáÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®GPUÈõÜÁæ§‰∏≠Èù¢‰∏¥ÊòæËëóÁöÑÈÄö‰ø°Âª∂ËøüÂíåË¥üËΩΩ‰∏çÂπ≥Ë°°ÔºåÂΩ±ÂìçËÆ≠ÁªÉÊïàÁéá„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöHierMoEÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøá‰ª§ÁâåÂéªÈáçÂíå‰∏ìÂÆ∂‰∫§Êç¢Êù•ÂáèÂ∞ëÈÄö‰ø°ÊµÅÈáèÂíåÂùáË°°GPUË¥üËΩΩ„ÄÇËøôÁßçËÆæËÆ°Êó®Âú®ÊèêÈ´òMoEÊ®°ÂûãÁöÑËÆ≠ÁªÉÊïàÁéáÔºåÂ∞§ÂÖ∂ÊòØÂú®Â§ßËßÑÊ®°ÂàÜÂ∏ÉÂºèÁéØÂ¢É‰∏≠„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöHierMoEÁ≥ªÁªüÂü∫‰∫éMegatron-LMÊûÑÂª∫Ôºå‰∏ªË¶ÅÂåÖÊã¨‰∏§‰∏™Ê®°ÂùóÔºö‰ª§ÁâåÂéªÈáçÊ®°ÂùóË¥üË¥£ÂáèÂ∞ëÈáçÂ§ç‰ª§ÁâåÁöÑÈÄö‰ø°Ôºå‰∏ìÂÆ∂‰∫§Êç¢Ê®°ÂùóÂàôÂú®‰∏çÂêåGPUÈó¥Âä®ÊÄÅË∞ÉÊï¥Ë¥üËΩΩ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊú¨ÊñáÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÊèêÂá∫‰∫ÜÊãìÊâëÊÑüÁü•ÁöÑ‰ª§ÁâåÂéªÈáçÂíå‰∏ìÂÆ∂‰∫§Êç¢Á≠ñÁï•ÔºåËøô‰∏éÁé∞ÊúâÊñπÊ≥ïÁöÑÈùôÊÄÅË¥üËΩΩÂàÜÈÖçÂíåÁÆÄÂçïÈÄö‰ø°Á≠ñÁï•ÊúâÊú¨Ë¥®Âå∫Âà´„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂÆûÁé∞‰∏≠ÔºåHierMoEÂØπ‰ª§ÁâåÁöÑÂéªÈáçÁ≠ñÁï•Âíå‰∏ìÂÆ∂ÁöÑ‰∫§Êç¢Á≠ñÁï•ËøõË°å‰∫ÜÁêÜËÆ∫Âª∫Ê®°Ôºå‰ª•ÈÄÇÂ∫î‰∏çÂêåÁöÑÊ®°ÂûãÈÖçÁΩÆÂíåÁ°¨‰ª∂ÁéØÂ¢ÉÔºåÁ°Æ‰øùÂú®ÂêÑÁßçÊù°‰ª∂‰∏ãÁöÑÊúÄ‰Ω≥ÊÄßËÉΩ„ÄÇÂÖ∑‰ΩìÁöÑÂèÇÊï∞ËÆæÁΩÆÂíåÊçüÂ§±ÂáΩÊï∞ËÆæËÆ°‰πüÁªèËøáÁ≤æÂøÉË∞ÉÊï¥Ôºå‰ª•‰ºòÂåñËÆ≠ÁªÉÊïàÊûú„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåHierMoEÂú®32-GPUÈõÜÁæ§‰∏äÂÆûÁé∞‰∫Ü1.55ÂÄçËá≥3.32ÂÄçÁöÑÈÄö‰ø°Âä†ÈÄüÔºåÁ´ØÂà∞Á´ØËÆ≠ÁªÉÈÄüÂ∫¶ÊèêÂçá‰∫Ü1.18ÂÄçËá≥1.27ÂÄçÔºåÊòæËëó‰ºò‰∫éÁé∞ÊúâÁöÑMoEËÆ≠ÁªÉÁ≥ªÁªüÔºåÂ¶ÇTutel-2DH„ÄÅSmartMoEÂíåMegatron-LMÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®ÊÄßËÉΩ‰∏äÁöÑÊòæËëóÊèêÂçá„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

HierMoEÁöÑÁ†îÁ©∂ÊàêÊûúÂèØÂπøÊ≥õÂ∫îÁî®‰∫éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑËÆ≠ÁªÉÔºåÂ∞§ÂÖ∂ÊòØÂú®ÈúÄË¶ÅÈ´òÊïàÂàÜÂ∏ÉÂºèËÆ°ÁÆóÁöÑÂú∫ÊôØ‰∏≠ÔºåÂ¶ÇËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ„ÄÅÊú∫Âô®ÁøªËØëÂíåÂØπËØùÁ≥ªÁªüÁ≠âÈ¢ÜÂüü„ÄÇÂÖ∂‰ºòÂåñÁöÑËÆ≠ÁªÉÊïàÁéáÂ∞ÜÊé®Âä®Êõ¥Â§ßËßÑÊ®°Ê®°ÂûãÁöÑÂºÄÂèë‰∏éÂ∫îÁî®ÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÊú™Êù•ÂΩ±Âìç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> The sparsely activated mixture-of-experts (MoE) transformer has become a common architecture for large language models (LLMs) due to its sparsity, which requires fewer computational demands while easily scaling the model size. In MoE models, each MoE layer requires to dynamically choose tokens to activate particular experts for computation while the activated experts may not be located in the same device or GPU as the token. However, this leads to substantial communication and load imbalances across all GPUs, which obstructs the scalability of distributed systems within a GPU cluster. To this end, we introduce HierMoE to accelerate the training of MoE models by two topology-aware techniques: 1) token deduplication to reduce the communication traffic, and 2) expert swap to balance the workloads among all GPUs. To enable the above two proposed approaches to be more general, we build theoretical models aimed at achieving the best token duplication and expert swap strategy under different model configurations and hardware environments. We implement our prototype HierMoE system atop Megatron-LM and conduct experiments on a 32-GPU cluster with DeepSeek-V3 and Qwen3-30B-A3B models. Experimental results show that our HierMoE achieves $1.55\times$ to $3.32\times$ faster communication and delivers $1.18\times$ to $1.27\times$ faster end-to-end training compared to state-of-the-art MoE training systems, Tutel-2DH, SmartMoE, and Megatron-LM.

