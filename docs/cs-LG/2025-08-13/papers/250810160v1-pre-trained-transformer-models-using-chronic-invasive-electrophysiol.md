---
layout: default
title: Pre-trained Transformer-models using chronic invasive electrophysiology for symptom decoding without patient-individual training
---

# Pre-trained Transformer-models using chronic invasive electrophysiology for symptom decoding without patient-individual training

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.10160" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.10160v1</a>
  <a href="https://arxiv.org/pdf/2508.10160.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.10160v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.10160v1', 'Pre-trained Transformer-models using chronic invasive electrophysiology for symptom decoding without patient-individual training')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Timon Merk, Saeed Salehi, Richard M. Koehler, Qiming Cui, Maria Olaru, Amelia Hahn, Nicole R. Provenza, Simon Little, Reza Abbasi-Asl, Phil A. Starr, Wolf-Julian Neumann

**åˆ†ç±»**: cs.HC, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-13

**å¤‡æ³¨**: 5 pages, 6 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºé¢„è®­ç»ƒTransformeræ¨¡å‹çš„ç—‡çŠ¶è§£ç æ–¹æ³•ä»¥è§£å†³ä¸ªä½“åŒ–è®­ç»ƒé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç¥ç»è§£ç ` `æ·±è„‘åˆºæ¿€` `é¢„è®­ç»ƒæ¨¡å‹` `ä¸ªä½“åŒ–åŒ»ç–—` `Transformer` `ç—‡çŠ¶è§£ç ` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨ç¥ç»è§£ç ä¸­é€šå¸¸éœ€è¦ä¸ªä½“åŒ–è®­ç»ƒï¼Œé™åˆ¶äº†å…¶å¹¿æ³›åº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºé¢„è®­ç»ƒTransformeræ¨¡å‹çš„è§£ç æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨æ— ä¸ªä½“åŒ–è®­ç»ƒçš„æƒ…å†µä¸‹è¿›è¡Œç—‡çŠ¶è§£ç ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è§£ç å¸•é‡‘æ£®ç—…ç—‡çŠ¶æ—¶è¡¨ç°ä¼˜å¼‚ï¼Œä¸”å…·æœ‰è¾ƒé«˜çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç¥ç»è§£ç ç—…ç†å’Œç”Ÿç†çŠ¶æ€èƒ½å¤Ÿå®ç°ä¸ªä½“åŒ–çš„é—­ç¯ç¥ç»è°ƒèŠ‚æ²»ç–—ã€‚è¿‘æœŸé¢„è®­ç»ƒçš„å¤§è§„æ¨¡åŸºç¡€æ¨¡å‹ä¸ºæ— ä¸ªä½“åŒ–è®­ç»ƒçš„çŠ¶æ€ä¼°è®¡æä¾›äº†å¯èƒ½æ€§ã€‚æœ¬æ–‡å±•ç¤ºäº†ä¸€ç§åŸºäºæ…¢æ€§çºµå‘æ·±è„‘åˆºæ¿€è®°å½•çš„åŸºç¡€æ¨¡å‹ï¼Œè¯¥æ¨¡å‹è®­ç»ƒæ—¶é—´è¶…è¿‡24å¤©ï¼Œèƒ½å¤Ÿé€‚åº”é•¿æœŸç—‡çŠ¶æ³¢åŠ¨ï¼Œæ‰©å±•ä¸Šä¸‹æ–‡çª—å£è‡³30åˆ†é’Ÿã€‚æ­¤å¤–ï¼Œæå‡ºäº†ä¸€ç§é’ˆå¯¹ç¥ç»ç”µç”Ÿç†æ•°æ®çš„ä¼˜åŒ–é¢„è®­ç»ƒæŸå¤±å‡½æ•°ï¼Œä»¥çº æ­£å¸¸è§æ©è”½è‡ªç¼–ç å™¨æŸå¤±å‡½æ•°å› 1-over-fåŠŸç‡æ³•åˆ™å¯¼è‡´çš„é¢‘ç‡åå·®ã€‚é€šè¿‡ä¸‹æ¸¸ä»»åŠ¡å±•ç¤ºäº†åœ¨ä¸è¿›è¡Œä¸ªä½“åŒ–è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œåˆ©ç”¨ç•™ä¸€æ³•äº¤å‰éªŒè¯è§£ç å¸•é‡‘æ£®ç—…ç—‡çŠ¶çš„èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰ç¥ç»è§£ç æ–¹æ³•éœ€è¦ä¸ªä½“åŒ–è®­ç»ƒçš„é—®é¢˜ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨ä¸´åºŠä¸­çš„åº”ç”¨å’Œæ¨å¹¿ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºæ‚£è€…ä¸ªä½“æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå¯¼è‡´æ¨¡å‹æ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºé¢„è®­ç»ƒTransformeræ¨¡å‹çš„è§£ç æ–¹æ³•ï¼Œåˆ©ç”¨æ…¢æ€§æ·±è„‘åˆºæ¿€è®°å½•è¿›è¡Œè®­ç»ƒï¼Œèƒ½å¤Ÿåœ¨ä¸ä¾èµ–ä¸ªä½“åŒ–æ•°æ®çš„æƒ…å†µä¸‹å®ç°ç—‡çŠ¶è§£ç ã€‚é€šè¿‡ä¼˜åŒ–æŸå¤±å‡½æ•°ï¼Œæå‡äº†æ¨¡å‹å¯¹ç¥ç»ç”µç”Ÿç†æ•°æ®çš„é€‚åº”æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€é¢„è®­ç»ƒæ¨¡å‹æ„å»ºã€æŸå¤±å‡½æ•°ä¼˜åŒ–å’Œä¸‹æ¸¸ä»»åŠ¡è§£ç å››ä¸ªä¸»è¦æ¨¡å—ã€‚æ•°æ®æ”¶é›†é˜¶æ®µä½¿ç”¨æ…¢æ€§æ·±è„‘åˆºæ¿€è®°å½•ï¼Œæ¨¡å‹æ„å»ºé˜¶æ®µé‡‡ç”¨Transformeræ¶æ„ï¼ŒæŸå¤±å‡½æ•°ä¼˜åŒ–åˆ™é’ˆå¯¹é¢‘ç‡åå·®è¿›è¡Œè°ƒæ•´ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§ä¼˜åŒ–çš„é¢„è®­ç»ƒæŸå¤±å‡½æ•°ï¼Œèƒ½å¤Ÿæœ‰æ•ˆçº æ­£å¸¸è§æ©è”½è‡ªç¼–ç å™¨æŸå¤±å‡½æ•°çš„é¢‘ç‡åå·®é—®é¢˜ã€‚è¿™ä¸€åˆ›æ–°ä½¿å¾—æ¨¡å‹åœ¨å¤„ç†ç¥ç»ç”µç”Ÿç†æ•°æ®æ—¶è¡¨ç°å‡ºæ›´å¥½çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŸå¤±å‡½æ•°è®¾è®¡ä¸Šï¼Œè€ƒè™‘äº†1-over-fåŠŸç‡æ³•åˆ™çš„å½±å“ï¼Œç¡®ä¿æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”ä¸åŒé¢‘ç‡çš„ä¿¡å·ã€‚æ­¤å¤–ï¼Œæ¨¡å‹çš„ä¸Šä¸‹æ–‡çª—å£è¢«æ‰©å±•è‡³30åˆ†é’Ÿï¼Œä»¥æ•æ‰é•¿æœŸç—‡çŠ¶æ³¢åŠ¨ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨ç•™ä¸€æ³•äº¤å‰éªŒè¯çš„æ–¹å¼ï¼Œæ¨¡å‹åœ¨è§£ç å¸•é‡‘æ£®ç—…ç—‡çŠ¶æ—¶è¡¨ç°å‡ºè‰²ï¼Œå‡†ç¡®ç‡æ˜¾è‘—é«˜äºä¼ ç»Ÿæ–¹æ³•ï¼ŒéªŒè¯äº†å…¶åœ¨æ— ä¸ªä½“åŒ–è®­ç»ƒæƒ…å†µä¸‹çš„æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç¥ç»è°ƒèŠ‚æ²»ç–—ã€ç¥ç»ç§‘å­¦ç ”ç©¶å’Œä¸ªä½“åŒ–åŒ»ç–—ã€‚é€šè¿‡å®ç°æ— ä¸ªä½“åŒ–è®­ç»ƒçš„ç—‡çŠ¶è§£ç ï¼Œèƒ½å¤Ÿä¸ºä¸´åºŠæä¾›æ›´é«˜æ•ˆçš„æ²»ç–—æ–¹æ¡ˆï¼Œæå‡æ‚£è€…çš„ç”Ÿæ´»è´¨é‡ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åœ¨å…¶ä»–ç¥ç»ç–¾ç—…çš„è§£ç å’Œæ²»ç–—ä¸­å¾—åˆ°åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Neural decoding of pathological and physiological states can enable patient-individualized closed-loop neuromodulation therapy. Recent advances in pre-trained large-scale foundation models offer the potential for generalized state estimation without patient-individual training. Here we present a foundation model trained on chronic longitudinal deep brain stimulation recordings spanning over 24 days. Adhering to long time-scale symptom fluctuations, we highlight the extended context window of 30 minutes. We present an optimized pre-training loss function for neural electrophysiological data that corrects for the frequency bias of common masked auto-encoder loss functions due to the 1-over-f power law. We show in a downstream task the decoding of Parkinson's disease symptoms with leave-one-subject-out cross-validation without patient-individual training.

