---
layout: default
title: Noise Hypernetworks: Amortizing Test-Time Compute in Diffusion Models
---

# Noise Hypernetworks: Amortizing Test-Time Compute in Diffusion Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.09968" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.09968v1</a>
  <a href="https://arxiv.org/pdf/2508.09968.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.09968v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.09968v1', 'Noise Hypernetworks: Amortizing Test-Time Compute in Diffusion Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Luca Eyring, Shyamgopal Karthik, Alexey Dosovitskiy, Nataniel Ruiz, Zeynep Akata

**åˆ†ç±»**: cs.LG, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-13

**å¤‡æ³¨**: Project page: https://noisehypernetworks.github.io/

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/ExplainableML/HyperNoise)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå™ªå£°è¶…ç½‘ç»œä»¥è§£å†³æ‰©æ•£æ¨¡å‹æ¨ç†æ•ˆç‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å™ªå£°è¶…ç½‘ç»œ` `æ‰©æ•£æ¨¡å‹` `æµ‹è¯•æ—¶é—´æ‰©å±•` `è®¡ç®—æ•ˆç‡` `ç”Ÿæˆæ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æµ‹è¯•æ—¶é—´æ‰©å±•æ–¹æ³•åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ˜¾è‘—å¢åŠ äº†è®¡ç®—æ—¶é—´ï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹ï¼Œé™åˆ¶äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å¯è¡Œæ€§ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§å™ªå£°è¶…ç½‘ç»œï¼Œé€šè¿‡è°ƒèŠ‚åˆå§‹è¾“å…¥å™ªå£°æ¥æ•´åˆæµ‹è¯•æ—¶é—´æ‰©å±•çŸ¥è¯†ï¼Œä»è€Œå‡å°‘æ¨ç†è¿‡ç¨‹ä¸­çš„è®¡ç®—å¼€é”€ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒæ¨¡å‹è´¨é‡çš„åŒæ—¶ï¼Œè®¡ç®—æˆæœ¬æ˜¾è‘—é™ä½ï¼Œæ¢å¤äº†å¤§éƒ¨åˆ†æ˜¾å¼æµ‹è¯•æ—¶é—´ä¼˜åŒ–å¸¦æ¥çš„è´¨é‡æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æµ‹è¯•æ—¶é—´æ‰©å±•çš„æ–°èŒƒå¼åœ¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œç”Ÿæˆè§†è§‰æ¨¡å‹ä¸­å–å¾—äº†æ˜¾è‘—çªç ´ï¼Œå…è®¸æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­åˆ†é…é¢å¤–è®¡ç®—ä»¥åº”å¯¹å¤æ‚é—®é¢˜ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•çš„ä¸€ä¸ªé‡è¦é™åˆ¶æ˜¯è®¡ç®—æ—¶é—´çš„å¤§å¹…å¢åŠ ï¼Œä½¿å¾—è¿‡ç¨‹åœ¨è®¸å¤šåº”ç”¨ä¸­å˜å¾—ç¼“æ…¢ä¸”ä¸åˆ‡å®é™…ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å°†æµ‹è¯•æ—¶é—´æ‰©å±•çŸ¥è¯†é›†æˆåˆ°æ¨¡å‹ä¸­çš„è§£å†³æ–¹æ¡ˆï¼Œå…·ä½“æ˜¯ç”¨å™ªå£°è¶…ç½‘ç»œæ›¿ä»£æ‰©æ•£æ¨¡å‹ä¸­çš„å¥–åŠ±å¼•å¯¼å™ªå£°ä¼˜åŒ–ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç†è®ºåŸºç¡€çš„æ¡†æ¶ï¼Œé€šè¿‡å¯å¤„ç†çš„å™ªå£°ç©ºé—´ç›®æ ‡æ¥å­¦ä¹ å¥–åŠ±å€¾æ–œåˆ†å¸ƒï¼Œä»è€Œåœ¨ä¿æŒåŸºç¡€æ¨¡å‹ä¿çœŸåº¦çš„åŒæ—¶ä¼˜åŒ–æ‰€éœ€ç‰¹æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶ï¼Œæ¢å¤äº†æ˜¾è‘—çš„è´¨é‡æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ‰©æ•£æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­å› æµ‹è¯•æ—¶é—´æ‰©å±•è€Œå¯¼è‡´çš„è®¡ç®—æ•ˆç‡ä½ä¸‹é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨æå‡æ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—å¢åŠ äº†æ¨ç†æ—¶é—´ï¼Œä½¿å¾—å®é™…åº”ç”¨å—åˆ°é™åˆ¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥å™ªå£°è¶…ç½‘ç»œæ¥æ›¿ä»£ä¼ ç»Ÿçš„å¥–åŠ±å¼•å¯¼å™ªå£°ä¼˜åŒ–ï¼Œä»è€Œåœ¨åè®­ç»ƒé˜¶æ®µæœ‰æ•ˆæ•´åˆæµ‹è¯•æ—¶é—´æ‰©å±•çš„çŸ¥è¯†ï¼Œå‡å°‘æ¨ç†æ—¶çš„è®¡ç®—å¼€é”€ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å™ªå£°è¶…ç½‘ç»œæ¨¡å—ï¼Œè¯¥æ¨¡å—è´Ÿè´£è°ƒèŠ‚è¾“å…¥å™ªå£°ï¼Œå¹¶é€šè¿‡ä¸€ä¸ªç†è®ºåŸºç¡€çš„æ¡†æ¶æ¥å­¦ä¹ å¥–åŠ±å€¾æ–œåˆ†å¸ƒã€‚è¯¥æ¡†æ¶ç¡®ä¿åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­ä¿æŒåŸºç¡€æ¨¡å‹çš„ä¿çœŸåº¦ï¼ŒåŒæ—¶å®ç°æ‰€éœ€ç‰¹æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥å™ªå£°è¶…ç½‘ç»œï¼Œæ›¿ä»£äº†ä¼ ç»Ÿçš„å™ªå£°ä¼˜åŒ–æ–¹æ³•ã€‚è¿™ä¸€è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨æ¨ç†æ—¶åŠ¨æ€è°ƒæ•´å™ªå£°ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬å™ªå£°ç©ºé—´ç›®æ ‡çš„æ„å»ºï¼Œè¯¥ç›®æ ‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€šè¿‡ç‰¹å®šçš„æŸå¤±å‡½æ•°è¿›è¡Œä¼˜åŒ–ï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨æ¨ç†æ—¶èƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨å­¦ä¹ åˆ°çš„çŸ¥è¯†ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨å™ªå£°è¶…ç½‘ç»œçš„æ–¹æ³•åœ¨ä¿æŒæ¨¡å‹è´¨é‡çš„åŒæ—¶ï¼Œè®¡ç®—æˆæœ¬é™ä½äº†æ˜¾è‘—æ¯”ä¾‹ã€‚ä¸ä¼ ç»Ÿçš„æ˜¾å¼æµ‹è¯•æ—¶é—´ä¼˜åŒ–ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æ¢å¤äº†å¤§éƒ¨åˆ†è´¨é‡æå‡ï¼Œä¸”æ¨ç†é€Ÿåº¦æ›´å¿«ï¼Œé€‚ç”¨äºæ›´å¹¿æ³›çš„åº”ç”¨åœºæ™¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰éœ€è¦é«˜æ•ˆæ¨ç†çš„ç”Ÿæˆæ¨¡å‹ã€‚é€šè¿‡å‡å°‘æ¨ç†è¿‡ç¨‹ä¸­çš„è®¡ç®—å¼€é”€ï¼Œæœ¬æ–‡çš„æ–¹æ³•å¯ä»¥ä½¿å¾—å¤æ‚æ¨¡å‹åœ¨å®æ—¶åº”ç”¨ä¸­å˜å¾—æ›´åŠ å¯è¡Œï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The new paradigm of test-time scaling has yielded remarkable breakthroughs in Large Language Models (LLMs) (e.g. reasoning models) and in generative vision models, allowing models to allocate additional computation during inference to effectively tackle increasingly complex problems. Despite the improvements of this approach, an important limitation emerges: the substantial increase in computation time makes the process slow and impractical for many applications. Given the success of this paradigm and its growing usage, we seek to preserve its benefits while eschewing the inference overhead. In this work we propose one solution to the critical problem of integrating test-time scaling knowledge into a model during post-training. Specifically, we replace reward guided test-time noise optimization in diffusion models with a Noise Hypernetwork that modulates initial input noise. We propose a theoretically grounded framework for learning this reward-tilted distribution for distilled generators, through a tractable noise-space objective that maintains fidelity to the base model while optimizing for desired characteristics. We show that our approach recovers a substantial portion of the quality gains from explicit test-time optimization at a fraction of the computational cost. Code is available at https://github.com/ExplainableML/HyperNoise

