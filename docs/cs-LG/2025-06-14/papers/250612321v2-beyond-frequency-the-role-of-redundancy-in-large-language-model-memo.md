---
layout: default
title: Beyond Frequency: The Role of Redundancy in Large Language Model Memorization
---

# Beyond Frequency: The Role of Redundancy in Large Language Model Memorization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.12321" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.12321v2</a>
  <a href="https://arxiv.org/pdf/2506.12321.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.12321v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.12321v2', 'Beyond Frequency: The Role of Redundancy in Large Language Model Memorization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jie Zhang, Qinghua Zhao, Chi-ho Lin, Zhongfeng Kang, Lei Li

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-14 (æ›´æ–°: 2025-08-29)

**å¤‡æ³¨**: 8 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ­ç¤ºå†—ä½™åœ¨å¤§è¯­è¨€æ¨¡å‹è®°å¿†ä¸­çš„é‡è¦æ€§ä»¥é™ä½éšç§é£é™©**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `è®°å¿†åŒ–` `éšç§ä¿æŠ¤` `æ•°æ®é¢„å¤„ç†` `å†—ä½™åˆ†æ` `å…¬å¹³æ€§è¯„ä¼°` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç ”ç©¶ä¸»è¦å…³æ³¨æ ‡è®°é¢‘ç‡ä¸è®°å¿†åŒ–ä¹‹é—´çš„å…³ç³»ï¼Œæœªèƒ½æ·±å…¥æ¢è®¨å†—ä½™å¯¹è®°å¿†åŒ–çš„å½±å“ã€‚
2. è®ºæ–‡é€šè¿‡æ‰°åŠ¨æ ·æœ¬å‰ç¼€ï¼Œåˆ†æå†—ä½™ä¸è®°å¿†åŒ–ä¹‹é—´çš„å…³ç³»ï¼Œæå‡ºå†—ä½™æŒ‡å¯¼çš„æ•°æ®é¢„å¤„ç†æ–¹æ³•ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œä½å†—ä½™æ ·æœ¬çš„è®°å¿†åŒ–è„†å¼±æ€§æ˜¾è‘—é«˜äºé«˜å†—ä½™æ ·æœ¬ï¼Œä¸”åœ¨æ‰°åŠ¨ä¸‹è®°å¿†æ ·æœ¬çš„è¡¨ç°ä¸‹é™æ›´ä¸ºæ˜æ˜¾ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è¯­è¨€æ¨¡å‹çš„è®°å¿†åŒ–é—®é¢˜åœ¨å…¶å‚æ•°è§„æ¨¡è¾¾åˆ°æ•°åäº¿æ—¶ï¼Œå¸¦æ¥äº†éšç§å’Œå…¬å¹³æ€§æ–¹é¢çš„é‡å¤§é£é™©ã€‚å°½ç®¡ä»¥å¾€ç ”ç©¶å·²æ¢è®¨äº†è®°å¿†åŒ–ä¸æ ‡è®°é¢‘ç‡å’Œé‡å¤æ¨¡å¼ç­‰å› ç´ ä¹‹é—´çš„å…³ç³»ï¼Œä½†æœ¬ç ”ç©¶æ­ç¤ºäº†ä¸åŒçš„å“åº”æ¨¡å¼ï¼šé¢‘ç‡å¯¹è®°å¿†æ ·æœ¬çš„å½±å“å¾®ä¹å…¶å¾®ï¼Œè€Œå¯¹éè®°å¿†æ ·æœ¬çš„å½±å“æ˜¾è‘—ã€‚é€šè¿‡å¯¹æ ·æœ¬å‰ç¼€çš„æ‰°åŠ¨åˆ†æï¼Œæˆ‘ä»¬å‘ç°å†—ä½™ä¸è®°å¿†æ¨¡å¼å­˜åœ¨ç›¸å…³æ€§ï¼Œçº¦79%çš„è®°å¿†æ ·æœ¬ä¸ºä½å†—ä½™æ ·æœ¬ï¼Œä¸”è¿™äº›æ ·æœ¬çš„è„†å¼±æ€§æ˜¯é«˜å†—ä½™æ ·æœ¬çš„ä¸¤å€ã€‚è¿™äº›å‘ç°ä¸ºæ•°æ®é¢„å¤„ç†æä¾›äº†å†—ä½™æŒ‡å¯¼çš„æ–¹æ³•ï¼Œä»è€Œé™ä½éšç§é£é™©å¹¶ç¼“è§£æ¨¡å‹éƒ¨ç½²ä¸­çš„åè§é—®é¢˜ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹è®°å¿†åŒ–å¸¦æ¥çš„éšç§å’Œå…¬å¹³æ€§é£é™©ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨æ ‡è®°é¢‘ç‡ï¼Œæœªèƒ½å……åˆ†è€ƒè™‘å†—ä½™å¯¹è®°å¿†åŒ–çš„å½±å“ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé€šè¿‡æ‰°åŠ¨æ ·æœ¬å‰ç¼€æ¥åˆ†æå†—ä½™ä¸è®°å¿†åŒ–ä¹‹é—´çš„å…³ç³»ï¼Œæ­ç¤ºå†—ä½™æ ·æœ¬åœ¨è®°å¿†åŒ–ä¸­çš„é‡è¦æ€§ï¼Œå¹¶æ¢ç´¢å†—ä½™æŒ‡å¯¼çš„æ•°æ®é¢„å¤„ç†æ–¹æ³•ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨äº†å¯¹æ¯”å®éªŒè®¾è®¡ï¼Œé¦–å…ˆå¯¹æ ·æœ¬è¿›è¡Œæ‰°åŠ¨ï¼Œç„¶åé‡åŒ–æ‰°åŠ¨å¼ºåº¦ï¼Œæœ€ååˆ†æå†—ä½™æ ·æœ¬çš„è®°å¿†åŒ–è¡¨ç°ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬æ ·æœ¬æ‰°åŠ¨ã€è®°å¿†åŒ–åˆ†æå’Œå†—ä½™è¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºé¦–æ¬¡ç³»ç»Ÿæ€§åœ°æ­ç¤ºäº†å†—ä½™ä¸è®°å¿†åŒ–ä¹‹é—´çš„å…³ç³»ï¼ŒæŒ‡å‡ºä½å†—ä½™æ ·æœ¬çš„è„†å¼±æ€§æ˜¾è‘—é«˜äºé«˜å†—ä½™æ ·æœ¬ï¼Œè¿™ä¸€å‘ç°ä¸ç°æœ‰ç ”ç©¶çš„é‡ç‚¹ä¸åŒã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æ‰°åŠ¨ç­–ç•¥ï¼Œé€šè¿‡æ”¹å˜æ ‡è®°ä½ç½®æ¥é‡åŒ–æ‰°åŠ¨å¼ºåº¦ï¼ŒåŒæ—¶åˆ†æäº†ä¸åŒå†—ä½™æ°´å¹³æ ·æœ¬çš„è®°å¿†åŒ–è¡¨ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œçº¦79%çš„è®°å¿†æ ·æœ¬ä¸ºä½å†—ä½™æ ·æœ¬ï¼Œè¿™äº›æ ·æœ¬åœ¨æ‰°åŠ¨ä¸‹çš„è¡¨ç°ä¸‹é™å¹…åº¦ä¸º0.6ï¼Œè€Œéè®°å¿†æ ·æœ¬ä»…ä¸‹é™0.01ï¼Œè¡¨æ˜å†—ä½™å†…å®¹åœ¨è®°å¿†åŒ–ä¸­æ—¢æ˜¾è‘—åˆè„†å¼±ï¼Œå…·æœ‰é‡è¦çš„å®ç”¨ä»·å€¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ•°æ®éšç§ä¿æŠ¤å’Œå…¬å¹³æ€§è¯„ä¼°ç­‰ã€‚é€šè¿‡å†—ä½™æŒ‡å¯¼çš„æ•°æ®é¢„å¤„ç†æ–¹æ³•ï¼Œå¯ä»¥æœ‰æ•ˆé™ä½æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„éšç§é£é™©ï¼Œå¹¶æé«˜æ¨¡å‹çš„å…¬å¹³æ€§ï¼Œç¡®ä¿æ›´å¹¿æ³›çš„ç¤¾ä¼šæ¥å—åº¦ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Memorization in large language models poses critical risks for privacy and fairness as these systems scale to billions of parameters. While previous studies established correlations between memorization and factors like token frequency and repetition patterns, we revealed distinct response patterns: frequency increases minimally impact memorized samples (e.g. 0.09) while substantially affecting non-memorized samples (e.g., 0.25), with consistency observed across model scales. Through counterfactual analysis by perturbing sample prefixes and quantifying perturbation strength through token positional changes, we demonstrate that redundancy correlates with memorization patterns. Our findings establish that: about 79% of memorized samples are low-redundancy, these low-redundancy samples exhibit 2-fold higher vulnerability than high-redundancy ones, and consequently memorized samples drop by 0.6 under perturbation while non-memorized samples drop by only 0.01, indicating that more redundant content becomes both more memorable and more fragile. These findings suggest potential redundancy-guided approaches for data preprocessing, thereby reducing privacy risks and mitigating bias to ensure fairness in model deployments.

