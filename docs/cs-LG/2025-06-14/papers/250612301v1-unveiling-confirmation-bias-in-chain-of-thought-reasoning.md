---
layout: default
title: Unveiling Confirmation Bias in Chain-of-Thought Reasoning
---

# Unveiling Confirmation Bias in Chain-of-Thought Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.12301" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.12301v1</a>
  <a href="https://arxiv.org/pdf/2506.12301.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.12301v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.12301v1', 'Unveiling Confirmation Bias in Chain-of-Thought Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yue Wan, Xiaowei Jia, Xiang Lorraine Li

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-14

**æœŸåˆŠ**: ACL 2025 Findings

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/yuewan2/biasedcot)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ­ç¤ºé“¾å¼æ€ç»´æ¨ç†ä¸­çš„ç¡®è®¤åå·®ä»¥æå‡æ¨ç†æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é“¾å¼æ€ç»´` `ç¡®è®¤åå·®` `æ¨ç†èƒ½åŠ›` `å¤§å‹è¯­è¨€æ¨¡å‹` `è®¤çŸ¥å¿ƒç†å­¦` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ¨¡å‹ä¿¡å¿µ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é“¾å¼æ€ç»´æ¨ç†æ–¹æ³•åœ¨ä¸åŒä»»åŠ¡ä¸­è¡¨ç°ä¸ä¸€è‡´ï¼Œç¼ºä¹å¯¹æ¨ç†è¿‡ç¨‹çš„æ·±å…¥ç†è§£ã€‚
2. æœ¬ç ”ç©¶æå‡ºé€šè¿‡ç¡®è®¤åå·®çš„è§†è§’åˆ†ææ¨¡å‹ä¿¡å¿µå¯¹æ¨ç†ç”Ÿæˆå’Œç­”æ¡ˆé¢„æµ‹çš„å½±å“ï¼Œæä¾›æ–°çš„ç†è§£æ¡†æ¶ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œç¡®è®¤åå·®æ˜¾è‘—å½±å“LLMsçš„æ¨ç†è¿‡ç¨‹å’Œç­”æ¡ˆé¢„æµ‹ï¼Œæ­ç¤ºäº†æ¨ç†æ•ˆæœçš„æ½œåœ¨åŸå› ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºå·²è¢«å¹¿æ³›åº”ç”¨äºå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼ŒCoTæ¨ç†åœ¨ä¸åŒæ¨ç†ç±»å‹çš„ä»»åŠ¡ä¸­æ•ˆæœä¸ä¸€ã€‚æœ¬ç ”ç©¶é€šè¿‡è®¤çŸ¥å¿ƒç†å­¦ä¸­çš„ç¡®è®¤åå·®è§†è§’ï¼Œæ¢è®¨æ¨¡å‹å†…éƒ¨ä¿¡å¿µå¦‚ä½•å½±å“æ¨ç†ç”Ÿæˆå’Œç­”æ¡ˆé¢„æµ‹ã€‚æˆ‘ä»¬å°†CoTåˆ†è§£ä¸ºä¸¤ä¸ªé˜¶æ®µï¼Œè¿›è¡Œæ¨¡å‹ä¿¡å¿µã€æ¨ç†å±æ€§å’Œé˜¶æ®µæ€§è¡¨ç°çš„ç›¸å…³æ€§åˆ†æã€‚ç»“æœæ˜¾ç¤ºï¼Œç¡®è®¤åå·®åœ¨LLMsä¸­å­˜åœ¨ï¼Œæ¨¡å‹ä¿¡å¿µä¸ä»…æ‰­æ›²æ¨ç†è¿‡ç¨‹ï¼Œè¿˜å½±å“æ¨ç†çš„åˆ©ç”¨æ–¹å¼ã€‚æ­¤å¤–ï¼Œä»»åŠ¡å¯¹ç¡®è®¤åå·®çš„è„†å¼±æ€§ä¸ä¿¡å¿µå¼ºåº¦ä¹‹é—´çš„ç›¸äº’ä½œç”¨ä¸ºCoTåœ¨ä¸åŒä»»åŠ¡å’Œæ¨¡å‹ä¸­çš„æœ‰æ•ˆæ€§æä¾›äº†è§£é‡Šã€‚è¯¥ç ”ç©¶ä¸ºæ”¹å–„æç¤ºç­–ç•¥ä»¥å‡è½»ç¡®è®¤åå·®ã€æå‡æ¨ç†æ€§èƒ½æä¾›äº†é‡è¦è§è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³é“¾å¼æ€ç»´æ¨ç†åœ¨ä¸åŒä»»åŠ¡ä¸­è¡¨ç°ä¸ä¸€è‡´çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†è€ƒè™‘æ¨¡å‹å†…éƒ¨ä¿¡å¿µå¯¹æ¨ç†è¿‡ç¨‹çš„å½±å“ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡è®¤çŸ¥å¿ƒç†å­¦ä¸­çš„ç¡®è®¤åå·®ç†è®ºï¼Œåˆ†ææ¨¡å‹ä¿¡å¿µå¦‚ä½•å½±å“æ¨ç†ç”Ÿæˆå’Œç­”æ¡ˆé¢„æµ‹ï¼Œæå‡ºå°†CoTæ¨ç†åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µçš„åˆ†ææ¡†æ¶ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šæ¨ç†ç”Ÿæˆï¼ˆQåˆ°Rï¼‰å’Œæ¨ç†å¼•å¯¼çš„ç­”æ¡ˆé¢„æµ‹ï¼ˆQRåˆ°Aï¼‰ï¼Œé€šè¿‡ç›¸å…³æ€§åˆ†ææ¢è®¨æ¨¡å‹ä¿¡å¿µä¸æ¨ç†è¡¨ç°ä¹‹é—´çš„å…³ç³»ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°åœ¨äºå°†ç¡®è®¤åå·®å¼•å…¥LLMsçš„æ¨ç†åˆ†æä¸­ï¼Œæ­ç¤ºäº†æ¨¡å‹ä¿¡å¿µå¯¹æ¨ç†è¿‡ç¨‹çš„æ‰­æ›²ä½œç”¨ï¼Œæä¾›äº†æ–°çš„è§†è§’æ¥ç†è§£CoTçš„æœ‰æ•ˆæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨ç›´æ¥é—®ç­”æ¦‚ç‡æ¥è¿‘ä¼¼æ¨¡å‹ä¿¡å¿µï¼Œè®¾è®¡äº†å¤šç§ä»»åŠ¡ä»¥è¯„ä¼°ä¸åŒæ¨ç†ç±»å‹çš„è„†å¼±æ€§ï¼Œå¹¶åˆ†æä¿¡å¿µå¼ºåº¦å¯¹æ¨ç†æ•ˆæœçš„å½±å“ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ¨¡å‹ä¿¡å¿µçš„ç¡®è®¤åå·®æ˜¾è‘—å½±å“æ¨ç†ç”Ÿæˆå’Œç­”æ¡ˆé¢„æµ‹ï¼Œå°¤å…¶åœ¨æŸäº›ä»»åŠ¡ä¸­ï¼Œæ¨ç†æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šã€‚è¿™ä¸€å‘ç°ä¸ºç†è§£LLMsçš„æ¨ç†æœºåˆ¶æä¾›äº†æ–°çš„è§†è§’ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿå’Œæ•™è‚²æŠ€æœ¯ç­‰ã€‚é€šè¿‡æ”¹è¿›æç¤ºç­–ç•¥ä»¥å‡è½»ç¡®è®¤åå·®ï¼Œå¯ä»¥æå‡æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Chain-of-thought (CoT) prompting has been widely adopted to enhance the reasoning capabilities of large language models (LLMs). However, the effectiveness of CoT reasoning is inconsistent across tasks with different reasoning types. This work presents a novel perspective to understand CoT behavior through the lens of \textit{confirmation bias} in cognitive psychology. Specifically, we examine how model internal beliefs, approximated by direct question-answering probabilities, affect both reasoning generation ($Q \to R$) and reasoning-guided answer prediction ($QR \to A$) in CoT. By decomposing CoT into a two-stage process, we conduct a thorough correlation analysis in model beliefs, rationale attributes, and stage-wise performance. Our results provide strong evidence of confirmation bias in LLMs, such that model beliefs not only skew the reasoning process but also influence how rationales are utilized for answer prediction. Furthermore, the interplay between task vulnerability to confirmation bias and the strength of beliefs also provides explanations for CoT effectiveness across reasoning tasks and models. Overall, this study provides a valuable insight for the needs of better prompting strategies that mitigate confirmation bias to enhance reasoning performance. Code is available at \textit{https://github.com/yuewan2/biasedcot}.

