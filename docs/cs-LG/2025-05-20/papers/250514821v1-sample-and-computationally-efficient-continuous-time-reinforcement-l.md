---
layout: default
title: Sample and Computationally Efficient Continuous-Time Reinforcement Learning with General Function Approximation
---

# Sample and Computationally Efficient Continuous-Time Reinforcement Learning with General Function Approximation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14821" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14821v1</a>
  <a href="https://arxiv.org/pdf/2505.14821.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14821v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14821v1', 'Sample and Computationally Efficient Continuous-Time Reinforcement Learning with General Function Approximation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Runze Zhao, Yue Yu, Adams Yiyue Zhu, Chen Yang, Dongruo Zhou

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20

**å¤‡æ³¨**: 28 pages, 4 figures, 5 tables. Accepted to UAI 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§é«˜æ•ˆçš„è¿ç»­æ—¶é—´å¼ºåŒ–å­¦ä¹ ç®—æ³•ä»¥è§£å†³æ ·æœ¬å’Œè®¡ç®—æ•ˆç‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è¿ç»­æ—¶é—´å¼ºåŒ–å­¦ä¹ ` `æ ·æœ¬æ•ˆç‡` `è®¡ç®—æ•ˆç‡` `å‡½æ•°é€¼è¿‘` `ç­–ç•¥æ›´æ–°` `æ¨¡å‹åŸºæ–¹æ³•` `å†³ç­–ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è¿ç»­æ—¶é—´å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨æ ·æœ¬æ•ˆç‡å’Œè®¡ç®—æ•ˆç‡ä¸Šå­˜åœ¨ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸€èˆ¬å‡½æ•°é€¼è¿‘çš„æƒ…å†µä¸‹ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ¨¡å‹çš„CTRLç®—æ³•ï¼Œåˆ©ç”¨ä¹è§‚ç½®ä¿¡é›†å®ç°æ ·æœ¬å¤æ‚åº¦ä¿è¯ï¼Œæå‡äº†å­¦ä¹ æ•ˆç‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æç®—æ³•åœ¨è¿ç»­æ§åˆ¶ä»»åŠ¡ä¸­è¡¨ç°å‡ºä¸ç°æœ‰æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ï¼Œä½†ç­–ç•¥æ›´æ–°å’Œå›åˆæ¬¡æ•°æ˜¾è‘—å‡å°‘ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿ç»­æ—¶é—´å¼ºåŒ–å­¦ä¹ ï¼ˆCTRLï¼‰ä¸ºåœ¨æŒç»­æ¼”å˜çš„ç¯å¢ƒä¸­è¿›è¡Œåºåˆ—å†³ç­–æä¾›äº†ä¸€ä¸ªåŸåˆ™æ€§æ¡†æ¶ã€‚å°½ç®¡å…¶åœ¨å®è·µä¸­å–å¾—äº†æˆåŠŸï¼Œä½†åœ¨ä¸€èˆ¬å‡½æ•°é€¼è¿‘çš„èƒŒæ™¯ä¸‹ï¼ŒCTRLçš„ç†è®ºç†è§£ä»ç„¶æœ‰é™ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ¨¡å‹çš„CTRLç®—æ³•ï¼Œæ—¨åœ¨å®ç°æ ·æœ¬å’Œè®¡ç®—æ•ˆç‡çš„åŒé‡æå‡ã€‚æˆ‘ä»¬åˆ©ç”¨åŸºäºä¹è§‚çš„ç½®ä¿¡é›†ï¼Œé¦–æ¬¡ä¸ºä¸€èˆ¬å‡½æ•°é€¼è¿‘çš„CTRLå»ºç«‹äº†æ ·æœ¬å¤æ‚åº¦ä¿è¯ï¼Œè¡¨æ˜å¯ä»¥é€šè¿‡$N$æ¬¡æµ‹é‡ä»¥$	ilde{O}(	ext{sqrt}(d_{	ext{R} } + d_{	ext{F} })N^{-1/2})$çš„æ¬¡ä¼˜æ€§é—´éš™å­¦ä¹ åˆ°è¿‘ä¼¼æœ€ä¼˜ç­–ç•¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç»“æ„åŒ–ç­–ç•¥æ›´æ–°å’Œæ›¿ä»£æµ‹é‡ç­–ç•¥ï¼Œæ˜¾è‘—å‡å°‘äº†ç­–ç•¥æ›´æ–°å’Œå›åˆæ¬¡æ•°ï¼ŒåŒæ—¶ä¿æŒäº†ç«äº‰åŠ›çš„æ ·æœ¬æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨è¿ç»­æ§åˆ¶ä»»åŠ¡å’Œæ‰©æ•£æ¨¡å‹å¾®è°ƒä¸­ï¼Œæˆ‘ä»¬çš„ç®—æ³•è¡¨ç°å‡ºè‰²ï¼Œä¸”ç­–ç•¥æ›´æ–°å’Œå›åˆæ¬¡æ•°æ˜¾è‘—å‡å°‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è¿ç»­æ—¶é—´å¼ºåŒ–å­¦ä¹ ä¸­æ ·æœ¬å’Œè®¡ç®—æ•ˆç‡ä¸è¶³çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨ä¸€èˆ¬å‡½æ•°é€¼è¿‘çš„ç¯å¢ƒä¸‹ï¼Œç°æœ‰æ–¹æ³•ç¼ºä¹ç†è®ºæ”¯æŒå’Œæ•ˆç‡ä¿éšœã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ¨¡å‹çš„CTRLç®—æ³•ï¼Œåˆ©ç”¨ä¹è§‚ç½®ä¿¡é›†æ¥å»ºç«‹æ ·æœ¬å¤æ‚åº¦çš„ç†è®ºä¿è¯ï¼Œä»è€Œåœ¨æ ·æœ¬ä½¿ç”¨ä¸Šå®ç°æ›´é«˜æ•ˆçš„å­¦ä¹ ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ˜¯åŸºäºä¹è§‚ç½®ä¿¡é›†çš„æ ·æœ¬å¤æ‚åº¦åˆ†æï¼Œå…¶æ¬¡æ˜¯ç»“æ„åŒ–ç­–ç•¥æ›´æ–°å’Œæ›¿ä»£æµ‹é‡ç­–ç•¥çš„å®æ–½ï¼Œä»¥å‡å°‘ç­–ç•¥æ›´æ–°çš„é¢‘ç‡å’Œå›åˆæ¬¡æ•°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºé¦–æ¬¡ä¸ºä¸€èˆ¬å‡½æ•°é€¼è¿‘çš„CTRLæä¾›äº†æ ·æœ¬å¤æ‚åº¦ä¿è¯ï¼Œå¹¶é€šè¿‡ç»“æ„åŒ–ç­–ç•¥æ›´æ–°æ˜¾è‘—é™ä½äº†è®¡ç®—è´Ÿæ‹…ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”å…·æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬å¯¹ä¹è§‚ç½®ä¿¡é›†çš„æ„å»ºã€æ ·æœ¬å¤æ‚åº¦çš„æ•°å­¦æ¨å¯¼ï¼Œä»¥åŠåœ¨ç­–ç•¥æ›´æ–°ä¸­å¼•å…¥çš„ç»“æ„åŒ–æ–¹æ³•ï¼Œç¡®ä¿åœ¨å‡å°‘æ›´æ–°æ¬¡æ•°çš„åŒæ—¶ä¿æŒå­¦ä¹ æ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æç®—æ³•åœ¨è¿ç»­æ§åˆ¶ä»»åŠ¡ä¸­ä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼Œç­–ç•¥æ›´æ–°æ¬¡æ•°å‡å°‘äº†50%ä»¥ä¸Šï¼ŒåŒæ—¶ä¿æŒäº†ç›¸ä¼¼çš„æ€§èƒ½æ°´å¹³ï¼Œå±•ç¤ºäº†æ˜¾è‘—çš„æ ·æœ¬æ•ˆç‡æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ã€é‡‘èå†³ç­–ç­‰éœ€è¦å®æ—¶å†³ç­–çš„åœºæ™¯ã€‚é€šè¿‡æé«˜æ ·æœ¬å’Œè®¡ç®—æ•ˆç‡ï¼Œèƒ½å¤Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­å®ç°æ›´å¿«é€Ÿçš„å­¦ä¹ å’Œé€‚åº”ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Continuous-time reinforcement learning (CTRL) provides a principled framework for sequential decision-making in environments where interactions evolve continuously over time. Despite its empirical success, the theoretical understanding of CTRL remains limited, especially in settings with general function approximation. In this work, we propose a model-based CTRL algorithm that achieves both sample and computational efficiency. Our approach leverages optimism-based confidence sets to establish the first sample complexity guarantee for CTRL with general function approximation, showing that a near-optimal policy can be learned with a suboptimality gap of $\tilde{O}(\sqrt{d_{\mathcal{R} } + d_{\mathcal{F} }}N^{-1/2})$ using $N$ measurements, where $d_{\mathcal{R} }$ and $d_{\mathcal{F} }$ denote the distributional Eluder dimensions of the reward and dynamic functions, respectively, capturing the complexity of general function approximation in reinforcement learning. Moreover, we introduce structured policy updates and an alternative measurement strategy that significantly reduce the number of policy updates and rollouts while maintaining competitive sample efficiency. We implemented experiments to backup our proposed algorithms on continuous control tasks and diffusion model fine-tuning, demonstrating comparable performance with significantly fewer policy updates and rollouts.

