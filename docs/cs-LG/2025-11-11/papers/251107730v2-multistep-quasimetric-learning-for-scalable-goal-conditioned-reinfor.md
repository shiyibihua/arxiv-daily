---
layout: default
title: Multistep Quasimetric Learning for Scalable Goal-conditioned Reinforcement Learning
---

# Multistep Quasimetric Learning for Scalable Goal-conditioned Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.07730" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.07730v2</a>
  <a href="https://arxiv.org/pdf/2511.07730.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.07730v2" onclick="toggleFavorite(this, '2511.07730v2', 'Multistep Quasimetric Learning for Scalable Goal-conditioned Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Bill Chunyuan Zheng, Vivek Myers, Benjamin Eysenbach, Sergey Levine

**åˆ†ç±»**: cs.LG, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-11-11 (æ›´æ–°: 2025-11-14)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šæ­¥å‡†åº¦é‡å­¦ä¹ ï¼Œè§£å†³å¯æ‰©å±•çš„ã€é•¿æ—¶ç¨‹ç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `ç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ ` `å‡†åº¦é‡å­¦ä¹ ` `å¤šæ­¥å›æŠ¥` `é•¿æ—¶ç¨‹ä»»åŠ¡` `æœºå™¨äººæ“ä½œ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ ï¼ˆGCRLï¼‰æ–¹æ³•åœ¨é•¿æ—¶ç¨‹ä»»åŠ¡ä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œéš¾ä»¥å‡†ç¡®ä¼°è®¡è§‚æµ‹é—´çš„æ—¶é—´è·ç¦»ã€‚
2. è®ºæ–‡æå‡ºä¸€ç§åŸºäºå¤šæ­¥è’™ç‰¹å¡æ´›å›æŠ¥çš„å‡†åº¦é‡å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨æ›´æœ‰æ•ˆåœ°å­¦ä¹ è§‚æµ‹é—´çš„è·ç¦»åº¦é‡ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é•¿æ—¶ç¨‹æ¨¡æ‹Ÿä»»åŠ¡å’ŒçœŸå®æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­ï¼Œå‡ä¼˜äºç°æœ‰GCRLæ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨ç¯å¢ƒä¸­å­¦ä¹ å¦‚ä½•è¾¾åˆ°ç›®æ ‡æ˜¯äººå·¥æ™ºèƒ½é¢†åŸŸä¸€ä¸ªé•¿æœŸå­˜åœ¨çš„æŒ‘æˆ˜ï¼Œç„¶è€Œï¼Œå¯¹äºç°ä»£æ–¹æ³•æ¥è¯´ï¼Œåœ¨é•¿æ—¶ç¨‹ä¸Šè¿›è¡Œæ¨ç†ä»ç„¶æ˜¯ä¸€ä¸ªéš¾é¢˜ã€‚å…³é”®é—®é¢˜æ˜¯å¦‚ä½•ä¼°è®¡è§‚æµ‹å¯¹ä¹‹é—´çš„æ—¶é—´è·ç¦»ã€‚è™½ç„¶æ—¶åºå·®åˆ†æ–¹æ³•åˆ©ç”¨å±€éƒ¨æ›´æ–°æ¥æä¾›æœ€ä¼˜æ€§ä¿è¯ï¼Œä½†å®ƒä»¬é€šå¸¸æ¯”æ‰§è¡Œå…¨å±€æ›´æ–°çš„è’™ç‰¹å¡æ´›æ–¹æ³•ï¼ˆä¾‹å¦‚ï¼Œä½¿ç”¨å¤šæ­¥å›æŠ¥ï¼‰è¡¨ç°æ›´å·®ï¼Œè€Œè’™ç‰¹å¡æ´›æ–¹æ³•ç¼ºä¹è¿™ç§ä¿è¯ã€‚æˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•å°†è¿™äº›æ–¹æ³•é›†æˆåˆ°ä¸€ä¸ªå®ç”¨çš„GCRLæ–¹æ³•ä¸­ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨å¤šæ­¥è’™ç‰¹å¡æ´›å›æŠ¥æ¥æ‹Ÿåˆå‡†åº¦é‡è·ç¦»ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é•¿æ—¶ç¨‹æ¨¡æ‹Ÿä»»åŠ¡ï¼ˆæœ€å¤š4000æ­¥ï¼‰ä¸Šä¼˜äºç°æœ‰çš„GCRLæ–¹æ³•ï¼Œå³ä½¿ä½¿ç”¨è§†è§‰è§‚æµ‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚æˆ‘ä»¬è¿˜è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥åœ¨çœŸå®ä¸–ç•Œçš„æœºå™¨äººæ“ä½œé¢†åŸŸï¼ˆBridgeè®¾ç½®ï¼‰ä¸­å®ç°æ‹¼æ¥ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¯ç¬¬ä¸€ä¸ªç«¯åˆ°ç«¯çš„GCRLæ–¹æ³•ï¼Œå¯ä»¥åœ¨è¿™ä¸ªçœŸå®ä¸–ç•Œçš„æ“ä½œé¢†åŸŸä¸­ï¼Œä»è§†è§‰è§‚æµ‹çš„æ— æ ‡ç­¾ç¦»çº¿æ•°æ®é›†ä¸­å®ç°å¤šæ­¥æ‹¼æ¥ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ ï¼ˆGCRLï¼‰ä¸­ï¼Œæ™ºèƒ½ä½“åœ¨é•¿æ—¶ç¨‹ä»»åŠ¡ä¸­éš¾ä»¥æœ‰æ•ˆå­¦ä¹ çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚æ—¶åºå·®åˆ†æ–¹æ³•ï¼Œè™½ç„¶å…·æœ‰å±€éƒ¨æœ€ä¼˜æ€§ä¿è¯ï¼Œä½†åœ¨é•¿æ—¶ç¨‹ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚è€Œè’™ç‰¹å¡æ´›æ–¹æ³•è™½ç„¶èƒ½è¿›è¡Œå…¨å±€æ›´æ–°ï¼Œä½†ç¼ºä¹ç†è®ºä¿è¯ï¼Œä¸”æ–¹å·®è¾ƒé«˜ã€‚å› æ­¤ï¼Œå¦‚ä½•ç»“åˆä¸¤è€…çš„ä¼˜ç‚¹ï¼Œè®¾è®¡ä¸€ç§æ—¢èƒ½æœ‰æ•ˆåˆ©ç”¨å…¨å±€ä¿¡æ¯ï¼Œåˆèƒ½ä¿è¯å­¦ä¹ ç¨³å®šæ€§çš„GCRLæ–¹æ³•æ˜¯æœ¬è®ºæ–‡è¦è§£å†³çš„æ ¸å¿ƒé—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†æ—¶åºå·®åˆ†æ–¹æ³•çš„å±€éƒ¨æ›´æ–°å’Œè’™ç‰¹å¡æ´›æ–¹æ³•çš„å¤šæ­¥å›æŠ¥ç›¸ç»“åˆï¼Œé€šè¿‡å¤šæ­¥è’™ç‰¹å¡æ´›å›æŠ¥æ¥å­¦ä¹ ä¸€ä¸ªå‡†åº¦é‡è·ç¦»å‡½æ•°ã€‚è¯¥å‡†åº¦é‡è·ç¦»å‡½æ•°èƒ½å¤Ÿæ›´å‡†ç¡®åœ°ä¼°è®¡è§‚æµ‹ä¹‹é—´çš„æ—¶åºè·ç¦»ï¼Œä»è€ŒæŒ‡å¯¼æ™ºèƒ½ä½“æ›´å¥½åœ°å®Œæˆé•¿æ—¶ç¨‹ä»»åŠ¡ã€‚è¿™ç§ç»“åˆåˆ©ç”¨äº†è’™ç‰¹å¡æ´›æ–¹æ³•çš„å…¨å±€ä¿¡æ¯ï¼ŒåŒæ—¶é€šè¿‡å‡†åº¦é‡å­¦ä¹ æ¥çº¦æŸå­¦ä¹ è¿‡ç¨‹ï¼Œæé«˜å­¦ä¹ çš„ç¨³å®šæ€§å’Œæ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1ï¼‰ä»ç¦»çº¿æ•°æ®é›†ä¸­é‡‡æ ·çŠ¶æ€ã€ç›®æ ‡å’ŒåŠ¨ä½œåºåˆ—ï¼›2ï¼‰ä½¿ç”¨å¤šæ­¥è’™ç‰¹å¡æ´›å›æŠ¥è®¡ç®—çŠ¶æ€å’Œç›®æ ‡ä¹‹é—´çš„å›æŠ¥ï¼›3ï¼‰ä½¿ç”¨å‡†åº¦é‡å­¦ä¹ æ–¹æ³•ï¼Œè®­ç»ƒä¸€ä¸ªç¥ç»ç½‘ç»œæ¥é¢„æµ‹çŠ¶æ€å’Œç›®æ ‡ä¹‹é—´çš„è·ç¦»ï¼Œè¯¥è·ç¦»ä¸å¤šæ­¥è’™ç‰¹å¡æ´›å›æŠ¥ç›¸ä¸€è‡´ï¼›4ï¼‰ä½¿ç”¨å­¦ä¹ åˆ°çš„è·ç¦»å‡½æ•°ä½œä¸ºå¥–åŠ±å‡½æ•°ï¼Œè®­ç»ƒå¼ºåŒ–å­¦ä¹ ç­–ç•¥ã€‚æ•´ä½“æ¡†æ¶æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„GCRLæµç¨‹ï¼Œå¯ä»¥ä»æ— æ ‡ç­¾çš„ç¦»çº¿æ•°æ®é›†ä¸­å­¦ä¹ ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†å¤šæ­¥è’™ç‰¹å¡æ´›å›æŠ¥å¼•å…¥åˆ°å‡†åº¦é‡å­¦ä¹ ä¸­ï¼Œå¹¶å°†å…¶åº”ç”¨äºGCRLã€‚ä¸ä¼ ç»Ÿçš„å‡†åº¦é‡å­¦ä¹ æ–¹æ³•ä¸åŒï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å¤šæ­¥å›æŠ¥æ¥æä¾›æ›´ä¸°å¯Œçš„ç›‘ç£ä¿¡å·ï¼Œä»è€Œå­¦ä¹ åˆ°æ›´å‡†ç¡®çš„è·ç¦»åº¦é‡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•æ˜¯ç¬¬ä¸€ä¸ªç«¯åˆ°ç«¯çš„GCRLæ–¹æ³•ï¼Œå¯ä»¥åœ¨çœŸå®ä¸–ç•Œçš„æœºå™¨äººæ“ä½œé¢†åŸŸä¸­ï¼Œä»è§†è§‰è§‚æµ‹çš„æ— æ ‡ç­¾ç¦»çº¿æ•°æ®é›†ä¸­å®ç°å¤šæ­¥æ‹¼æ¥ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä½¿ç”¨ç¥ç»ç½‘ç»œæ¥è¡¨ç¤ºå‡†åº¦é‡è·ç¦»å‡½æ•°ï¼ŒæŸå¤±å‡½æ•°é‡‡ç”¨å‡æ–¹è¯¯å·®æŸå¤±ï¼Œç”¨äºè¡¡é‡é¢„æµ‹è·ç¦»ä¸å¤šæ­¥è’™ç‰¹å¡æ´›å›æŠ¥ä¹‹é—´çš„å·®å¼‚ã€‚å¤šæ­¥å›æŠ¥çš„æ­¥æ•°æ˜¯ä¸€ä¸ªå…³é”®å‚æ•°ï¼Œéœ€è¦æ ¹æ®ä»»åŠ¡çš„é•¿åº¦è¿›è¡Œè°ƒæ•´ã€‚ç½‘ç»œç»“æ„çš„é€‰æ‹©ä¹Ÿéœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡è¿›è¡Œè°ƒæ•´ï¼Œä¾‹å¦‚ï¼Œå¯¹äºè§†è§‰ä»»åŠ¡ï¼Œå¯ä»¥ä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œæ¥æå–å›¾åƒç‰¹å¾ã€‚æ­¤å¤–ï¼Œä¸ºäº†æé«˜å­¦ä¹ çš„ç¨³å®šæ€§ï¼Œå¯ä»¥ä½¿ç”¨ä¸€äº›æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œä¾‹å¦‚æƒé‡è¡°å‡å’Œdropoutã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥æ–¹æ³•åœ¨é•¿æ—¶ç¨‹æ¨¡æ‹Ÿä»»åŠ¡ï¼ˆæœ€é•¿4000æ­¥ï¼‰ä¸­ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„GCRLæ–¹æ³•ã€‚åœ¨çœŸå®ä¸–ç•Œçš„æœºå™¨äººæ“ä½œä»»åŠ¡ï¼ˆBridge setupï¼‰ä¸­ï¼Œè¯¥æ–¹æ³•æˆåŠŸå®ç°äº†å¤šæ­¥æ‹¼æ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç«¯åˆ°ç«¯çš„GCRLæ–¹æ³•åœ¨è¯¥é¢†åŸŸå–å¾—çš„æˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°å­¦ä¹ ç¯å¢ƒä¸­çš„è·ç¦»åº¦é‡ï¼Œå¹¶å°†å…¶åº”ç”¨äºé•¿æ—¶ç¨‹ä»»åŠ¡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºæœºå™¨äººæ“ä½œã€è‡ªåŠ¨é©¾é©¶ã€æ¸¸æˆAIç­‰é¢†åŸŸã€‚é€šè¿‡å­¦ä¹ ç¯å¢ƒä¸­çš„è·ç¦»åº¦é‡ï¼Œæ™ºèƒ½ä½“å¯ä»¥æ›´å¥½åœ°è§„åˆ’è·¯å¾„ã€å®Œæˆå¤æ‚ä»»åŠ¡ã€‚ç‰¹åˆ«æ˜¯åœ¨çœŸå®æœºå™¨äººæ“ä½œé¢†åŸŸï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä»æ— æ ‡ç­¾çš„ç¦»çº¿æ•°æ®ä¸­å­¦ä¹ ï¼Œé™ä½äº†æ•°æ®æ”¶é›†çš„æˆæœ¬ï¼ŒåŠ é€Ÿäº†æœºå™¨äººæ™ºèƒ½åŒ–çš„è¿›ç¨‹ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åº”ç”¨äºæ›´å¤æ‚çš„ç¯å¢ƒå’Œä»»åŠ¡ï¼Œä¾‹å¦‚å¤šæ™ºèƒ½ä½“åä½œã€äººæœºäº¤äº’ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Learning how to reach goals in an environment is a longstanding challenge in AI, yet reasoning over long horizons remains a challenge for modern methods. The key question is how to estimate the temporal distance between pairs of observations. While temporal difference methods leverage local updates to provide optimality guarantees, they often perform worse than Monte Carlo methods that perform global updates (e.g., with multi-step returns), which lack such guarantees. We show how these approaches can be integrated into a practical GCRL method that fits a quasimetric distance using a multistep Monte-Carlo return. We show our method outperforms existing GCRL methods on long-horizon simulated tasks with up to 4000 steps, even with visual observations. We also demonstrate that our method can enable stitching in the real-world robotic manipulation domain (Bridge setup). Our approach is the first end-to-end GCRL method that enables multistep stitching in this real-world manipulation domain from an unlabeled offline dataset of visual observations.

