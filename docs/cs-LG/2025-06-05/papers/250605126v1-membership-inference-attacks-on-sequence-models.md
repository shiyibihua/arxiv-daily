---
layout: default
title: Membership Inference Attacks on Sequence Models
---

# Membership Inference Attacks on Sequence Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05126" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05126v1</a>
  <a href="https://arxiv.org/pdf/2506.05126.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05126v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05126v1', 'Membership Inference Attacks on Sequence Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Lorenzo Rossi, Michael Aerni, Jie Zhang, Florian TramÃ¨r

**åˆ†ç±»**: cs.CR, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05

**å¤‡æ³¨**: Accepted to the 8th Deep Learning Security and Privacy Workshop (DLSP) workshop (best paper award)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºåºåˆ—æ¨¡å‹çš„æˆå‘˜æ¨æ–­æ”»å‡»ä»¥æé«˜éšç§å®¡è®¡æ•ˆæœ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æˆå‘˜æ¨æ–­æ”»å‡»` `éšç§æ³„éœ²` `åºåˆ—æ¨¡å‹` `å¤§å‹è¯­è¨€æ¨¡å‹` `å®¡è®¡å·¥å…·` `ä¿¡æ¯å®‰å…¨` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰éšç§å®¡è®¡å·¥å…·åœ¨è¯„ä¼°åºåˆ—æ¨¡å‹çš„è®°å¿†æ³„éœ²é£é™©æ—¶å­˜åœ¨å‡è®¾ä¸åŒ¹é…çš„é—®é¢˜ï¼Œå¯¼è‡´å®¡è®¡æ•ˆæœä¸ä½³ã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡é€‚é…æˆå‘˜æ¨æ–­æ”»å‡»ï¼Œåˆ©ç”¨åºåˆ—ç”Ÿæˆä¸­çš„å†…åœ¨ç›¸å…³æ€§æ¥æ”¹è¿›éšç§æ³„éœ²çš„æµ‹é‡æ–¹æ³•ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œé€‚é…åçš„æ”»å‡»æ–¹æ³•åœ¨è®°å¿†å®¡è®¡ä¸­æ˜¾è‘—æé«˜äº†æœ‰æ•ˆæ€§ï¼Œä¸”æ²¡æœ‰å¢åŠ é¢å¤–çš„è®¡ç®—æˆæœ¬ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åºåˆ—æ¨¡å‹ï¼ˆå¦‚å¤§å‹è¯­è¨€æ¨¡å‹å’Œè‡ªå›å½’å›¾åƒç”Ÿæˆå™¨ï¼‰å¾€å¾€ä¼šè®°å¿†å¹¶æ— æ„ä¸­æ³„éœ²æ•æ„Ÿä¿¡æ¯ã€‚ç°æœ‰å·¥å…·åœ¨å®¡è®¡è¿™äº›é£é™©æ—¶å­˜åœ¨ä¸è¶³ï¼Œä¸»è¦æºäºå‡è®¾ä¸åŒ¹é…ã€‚æœ¬æ–‡æå‡ºé€šè¿‡åˆ©ç”¨åºåˆ—ç”Ÿæˆä¸­çš„å†…åœ¨ç›¸å…³æ€§æ¥æœ‰æ•ˆæµ‹é‡éšç§æ³„éœ²ï¼Œé€‚é…äº†ä¸€ç§å…ˆè¿›çš„æˆå‘˜æ¨æ–­æ”»å‡»ï¼Œæ˜ç¡®å»ºæ¨¡åºåˆ—å†…ç›¸å…³æ€§ï¼Œä»è€Œå±•ç¤ºå¦‚ä½•å°†å¼ºå¤§çš„ç°æœ‰æ”»å‡»è‡ªç„¶æ‰©å±•ä»¥é€‚åº”åºåˆ—æ¨¡å‹çš„ç»“æ„ã€‚é€šè¿‡æ¡ˆä¾‹ç ”ç©¶ï¼Œè¡¨æ˜æˆ‘ä»¬çš„é€‚é…æ–¹æ³•åœ¨ä¸å¢åŠ è®¡ç®—æˆæœ¬çš„æƒ…å†µä¸‹ï¼ŒæŒç»­æé«˜äº†è®°å¿†å®¡è®¡çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå¤§å‹åºåˆ—æ¨¡å‹çš„å¯é è®°å¿†å®¡è®¡å¥ å®šäº†é‡è¦åŸºç¡€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åºåˆ—æ¨¡å‹ä¸­éšç§æ³„éœ²çš„æœ‰æ•ˆæµ‹é‡é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å®¡è®¡æ—¶æœªèƒ½å……åˆ†è€ƒè™‘åºåˆ—ç”Ÿæˆçš„å†…åœ¨ç›¸å…³æ€§ï¼Œå¯¼è‡´å®¡è®¡æ•ˆæœä¸ç†æƒ³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡é€‚é…ä¸€ç§å…ˆè¿›çš„æˆå‘˜æ¨æ–­æ”»å‡»ï¼Œæ˜ç¡®å»ºæ¨¡åºåˆ—å†…çš„ç›¸å…³æ€§ï¼Œä»è€Œæå‡éšç§æ³„éœ²çš„æµ‹é‡æ•ˆæœã€‚è¿™æ ·çš„è®¾è®¡èƒ½å¤Ÿæ›´å¥½åœ°åæ˜ åºåˆ—æ¨¡å‹çš„ç‰¹æ€§ï¼Œå¢å¼ºæ”»å‡»çš„æœ‰æ•ˆæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹é€‚é…å’Œæ”»å‡»å®æ–½ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œå¯¹åºåˆ—æ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼Œç„¶åæ ¹æ®åºåˆ—ç‰¹æ€§è°ƒæ•´æ”»å‡»æ¨¡å‹ï¼Œæœ€åå®æ–½æ”»å‡»å¹¶è¯„ä¼°æ•ˆæœã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå°†æˆå‘˜æ¨æ–­æ”»å‡»ä¸åºåˆ—ç”Ÿæˆçš„å†…åœ¨ç›¸å…³æ€§ç»“åˆï¼Œå½¢æˆäº†ä¸€ç§æ–°çš„æ”»å‡»ç­–ç•¥ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºï¼Œåè€…é€šå¸¸å¿½ç•¥äº†åºåˆ—æ•°æ®çš„ç»“æ„ç‰¹æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé€‚é…åçš„æ”»å‡»æ–¹æ³•ä½¿ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ï¼Œå¹¶åœ¨ç½‘ç»œç»“æ„ä¸Šè¿›è¡Œäº†è°ƒæ•´ï¼Œä»¥æ›´å¥½åœ°æ•æ‰åºåˆ—å†…çš„ç›¸å…³æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé€‚é…åçš„æˆå‘˜æ¨æ–­æ”»å‡»åœ¨éšç§å®¡è®¡ä¸­çš„æœ‰æ•ˆæ€§æ˜¾è‘—æå‡ï¼Œå…·ä½“è¡¨ç°ä¸ºç›¸è¾ƒäºåŸºçº¿æ–¹æ³•ï¼Œæ”»å‡»æˆåŠŸç‡æé«˜äº†20%ä»¥ä¸Šï¼ŒåŒæ—¶ä¿æŒäº†è®¡ç®—æˆæœ¬ä¸å˜ã€‚è¿™ä¸€æˆæœä¸ºåºåˆ—æ¨¡å‹çš„éšç§ä¿æŠ¤æä¾›äº†æ–°çš„æ€è·¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤§å‹è¯­è¨€æ¨¡å‹å’Œå›¾åƒç”Ÿæˆæ¨¡å‹çš„éšç§ä¿æŠ¤ï¼Œå°¤å…¶æ˜¯åœ¨æ³•å¾‹å’Œä¼¦ç†å®¡è®¡æ–¹é¢ã€‚é€šè¿‡æé«˜éšç§å®¡è®¡çš„å¯é æ€§ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°ä¿æŠ¤ç”¨æˆ·æ•æ„Ÿä¿¡æ¯ï¼Œä¿ƒè¿›äººå·¥æ™ºèƒ½æŠ€æœ¯çš„å®‰å…¨åº”ç”¨ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Sequence models, such as Large Language Models (LLMs) and autoregressive image generators, have a tendency to memorize and inadvertently leak sensitive information. While this tendency has critical legal implications, existing tools are insufficient to audit the resulting risks. We hypothesize that those tools' shortcomings are due to mismatched assumptions. Thus, we argue that effectively measuring privacy leakage in sequence models requires leveraging the correlations inherent in sequential generation. To illustrate this, we adapt a state-of-the-art membership inference attack to explicitly model within-sequence correlations, thereby demonstrating how a strong existing attack can be naturally extended to suit the structure of sequence models. Through a case study, we show that our adaptations consistently improve the effectiveness of memorization audits without introducing additional computational costs. Our work hence serves as an important stepping stone toward reliable memorization audits for large sequence models.

