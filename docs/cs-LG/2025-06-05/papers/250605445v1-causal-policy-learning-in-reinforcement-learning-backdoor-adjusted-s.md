---
layout: default
title: Causal Policy Learning in Reinforcement Learning: Backdoor-Adjusted Soft Actor-Critic
---

# Causal Policy Learning in Reinforcement Learning: Backdoor-Adjusted Soft Actor-Critic

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05445" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05445v1</a>
  <a href="https://arxiv.org/pdf/2506.05445.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05445v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05445v1', 'Causal Policy Learning in Reinforcement Learning: Backdoor-Adjusted Soft Actor-Critic')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Thanh Vinh Vo, Young Lee, Haozhe Ma, Chien Lu, Tze-Yun Leong

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05

**å¤‡æ³¨**: Preprint

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDoSACä»¥è§£å†³å¼ºåŒ–å­¦ä¹ ä¸­çš„éšæ€§æ··æ·†é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `å› æœæ¨æ–­` `ç­–ç•¥å­¦ä¹ ` `åé—¨è°ƒæ•´` `è½¯æ¼”å‘˜è¯„è®ºå®¶` `éšæ€§æ··æ·†` `é²æ£’æ€§` `æ³›åŒ–èƒ½åŠ›`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•é€šå¸¸å¿½è§†éšæ€§æ··æ·†å› ç´ ï¼Œå¯¼è‡´å­¦ä¹ çš„ç­–ç•¥å­˜åœ¨åå·®å’Œä¸å¯é æ€§ã€‚
2. æœ¬æ–‡æå‡ºDoSACï¼Œé€šè¿‡å› æœå¹²é¢„ä¼°è®¡å’Œåé—¨è°ƒæ•´æ¥çº æ­£éšæ€§æ··æ·†ï¼Œæå‡ç­–ç•¥å­¦ä¹ çš„æœ‰æ•ˆæ€§ã€‚
3. å®éªŒè¯æ˜ï¼ŒDoSACåœ¨è¿ç»­æ§åˆ¶åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºä¼ ç»ŸåŸºçº¿ï¼Œå±•ç°å‡ºæ›´å¥½çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšæ€§æ··æ·†å› ç´ ä¼šå½±å“å¼ºåŒ–å­¦ä¹ ä¸­çš„çŠ¶æ€å’ŒåŠ¨ä½œï¼Œä»è€Œå¯¼è‡´ç­–ç•¥å­¦ä¹ çš„åå·®ï¼Œè¿›è€Œäº§ç”Ÿæ¬¡ä¼˜æˆ–ä¸å¯æ³›åŒ–çš„è¡Œä¸ºã€‚å¤§å¤šæ•°å¼ºåŒ–å­¦ä¹ ç®—æ³•å¿½è§†è¿™ä¸€é—®é¢˜ï¼Œä»…åŸºäºç»Ÿè®¡å…³è”ä»è§‚å¯Ÿè½¨è¿¹ä¸­å­¦ä¹ ç­–ç•¥ã€‚æœ¬æ–‡æå‡ºDoSACï¼ˆå¸¦æœ‰åé—¨è°ƒæ•´çš„è½¯æ¼”å‘˜è¯„è®ºå®¶ï¼‰ï¼Œè¿™æ˜¯å¯¹SACç®—æ³•çš„åŸåˆ™æ€§æ‰©å±•ï¼Œé€šè¿‡å› æœå¹²é¢„ä¼°è®¡æ¥çº æ­£éšæ€§æ··æ·†ã€‚DoSACåˆ©ç”¨åé—¨å‡†åˆ™ä¼°è®¡å¹²é¢„ç­–ç•¥ï¼Œè€Œæ— éœ€è®¿é—®çœŸå®çš„æ··æ·†å› ç´ æˆ–å› æœæ ‡ç­¾ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå¯å­¦ä¹ çš„åé—¨é‡æ„å™¨ï¼Œä»å½“å‰çŠ¶æ€æ¨æ–­ä¼ªè¿‡å»å˜é‡ï¼ˆå…ˆå‰çŠ¶æ€å’ŒåŠ¨ä½œï¼‰ï¼Œä»¥ä¾¿ä»è§‚å¯Ÿæ•°æ®ä¸­è¿›è¡Œåé—¨è°ƒæ•´ã€‚å®éªŒè¯æ˜ï¼ŒDoSACåœ¨æ··æ·†è®¾ç½®ä¸‹ä¼˜äºåŸºçº¿ï¼Œå±•ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ã€æ³›åŒ–èƒ½åŠ›å’Œç­–ç•¥å¯é æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³éšæ€§æ··æ·†å› ç´ å¯¹å¼ºåŒ–å­¦ä¹ ç­–ç•¥å­¦ä¹ çš„å½±å“ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºç»Ÿè®¡å…³è”ï¼Œå¯¼è‡´ç­–ç•¥çš„æ¬¡ä¼˜æ€§å’Œä¸å¯æ³›åŒ–æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºDoSACï¼Œé€šè¿‡å› æœå¹²é¢„ä¼°è®¡æ¥çº æ­£éšæ€§æ··æ·†ï¼Œåˆ©ç”¨åé—¨å‡†åˆ™åœ¨ä¸éœ€è¦çœŸå®æ··æ·†å› ç´ çš„æƒ…å†µä¸‹è¿›è¡Œç­–ç•¥å­¦ä¹ ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDoSACé›†æˆåœ¨è½¯æ¼”å‘˜è¯„è®ºå®¶æ¡†æ¶ä¸­ï¼Œä¸»è¦æ¨¡å—åŒ…æ‹¬åé—¨é‡æ„å™¨å’Œç­–ç•¥è®¡ç®—æ¨¡å—ï¼Œåè€…è´Ÿè´£ä¼°è®¡å¹²é¢„ç­–ç•¥åŠå…¶ç†µã€‚

**å…³é”®åˆ›æ–°**ï¼šå¼•å…¥å¯å­¦ä¹ çš„åé—¨é‡æ„å™¨ï¼Œèƒ½å¤Ÿä»å½“å‰çŠ¶æ€æ¨æ–­ä¼ªè¿‡å»å˜é‡ï¼Œå®ç°ä»è§‚å¯Ÿæ•°æ®çš„åé—¨è°ƒæ•´ï¼Œè¿™æ˜¯ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç½‘ç»œç»“æ„ä¸Šï¼Œåé—¨é‡æ„å™¨è®¾è®¡ä¸ºä¸€ä¸ªæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ŒæŸå¤±å‡½æ•°è€ƒè™‘äº†é‡æ„çš„å‡†ç¡®æ€§å’Œç­–ç•¥çš„ç†µï¼Œç¡®ä¿ç­–ç•¥å­¦ä¹ çš„ç¨³å®šæ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDoSACåœ¨å¤šä¸ªè¿ç»­æ§åˆ¶åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºä¼ ç»ŸåŸºçº¿ï¼Œç‰¹åˆ«æ˜¯åœ¨æ··æ·†è®¾ç½®ä¸‹ï¼Œè¡¨ç°å‡ºæ›´é«˜çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½å†³ç­–ç³»ç»Ÿç­‰ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡è¿™äº›é¢†åŸŸä¸­ç­–ç•¥å­¦ä¹ çš„å¯é æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Hidden confounders that influence both states and actions can bias policy learning in reinforcement learning (RL), leading to suboptimal or non-generalizable behavior. Most RL algorithms ignore this issue, learning policies from observational trajectories based solely on statistical associations rather than causal effects. We propose DoSAC (Do-Calculus Soft Actor-Critic with Backdoor Adjustment), a principled extension of the SAC algorithm that corrects for hidden confounding via causal intervention estimation. DoSAC estimates the interventional policy $Ï€(a \| \mathrm{do}(s))$ using the backdoor criterion, without requiring access to true confounders or causal labels. To achieve this, we introduce a learnable Backdoor Reconstructor that infers pseudo-past variables (previous state and action) from the current state to enable backdoor adjustment from observational data. This module is integrated into a soft actor-critic framework to compute both the interventional policy and its entropy. Empirical results on continuous control benchmarks show that DoSAC outperforms baselines under confounded settings, with improved robustness, generalization, and policy reliability.

