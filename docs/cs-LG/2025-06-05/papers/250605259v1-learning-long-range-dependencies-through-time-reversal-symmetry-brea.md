---
layout: default
title: Learning long range dependencies through time reversal symmetry breaking
---

# Learning long range dependencies through time reversal symmetry breaking

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05259" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.05259v1</a>
  <a href="https://arxiv.org/pdf/2506.05259.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05259v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05259v1', 'Learning long range dependencies through time reversal symmetry breaking')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Guillaume Pourcel, Maxence Ernoult

**ÂàÜÁ±ª**: cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-05

**Â§áÊ≥®**: 45 pages

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫RHELÁÆóÊ≥ï‰ª•Ëß£ÂÜ≥ÈïøÁ®ã‰æùËµñÂ≠¶‰π†ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `ÈïøÁ®ã‰æùËµñ` `ÈÄíÂΩíÁ•ûÁªèÁΩëÁªú` `ÂìàÂØÜÈ°øÁ≥ªÁªü` `Êó∂Èó¥Â∫èÂàóÈ¢ÑÊµã` `Ê∑±Â∫¶Â≠¶‰π†ÁÆóÊ≥ï`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÈÄíÂΩíÁ•ûÁªèÁΩëÁªúÔºàRNNÔºâÂú®Â§ÑÁêÜÈïøÁ®ã‰æùËµñÊó∂Ë°®Áé∞‰∏ç‰Ω≥ÔºåÂ∞§ÂÖ∂ÊòØÂú®Â∫èÂàóÈïøÂ∫¶ËæÉÂ§ßÁöÑÊÉÖÂÜµ‰∏ã„ÄÇ
2. ËÆ∫ÊñáÊèêÂá∫ÁöÑRHELÁÆóÊ≥ïÈÄöËøáÁâ©ÁêÜËΩ®ËøπÁöÑÊúâÈôêÂ∑ÆÂàÜËÆ°ÁÆóÊ¢ØÂ∫¶ÔºåÊòæËëóÂáèÂ∞ë‰∫ÜËÆ°ÁÆóÂ§çÊùÇÂ∫¶ÔºåÂπ∂‰øùÊåÅ‰∫ÜÈ´òÊïàÊÄß„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåRHELÂú®Â§öÁßçÊó∂Èó¥Â∫èÂàó‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫‰∏éBPTTÁõ∏ÂΩìÁöÑÊÄßËÉΩÔºå‰∏îÂú®Â§ÑÁêÜÈïøÂ∫èÂàóÊó∂ÂÖ∑Êúâ‰ºòÂäø„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Ê∑±Â∫¶Áä∂ÊÄÅÁ©∫Èó¥Ê®°ÂûãÔºàSSMsÔºâÈáçÊñ∞ÁÇπÁáÉ‰∫ÜÂü∫‰∫éÁâ©ÁêÜÁöÑËÆ°ÁÆóËåÉÂºèÔºåRNNÂèØ‰ª•Ëá™ÁÑ∂Âú∞ÂµåÂÖ•Âà∞Âä®ÊÄÅÁ≥ªÁªü‰∏≠„ÄÇËøôÈúÄË¶ÅÈÅµÂæ™Ê†∏ÂøÉÁâ©ÁêÜÂéüÂàôÁöÑ‰∏ìÁî®Â≠¶‰π†ÁÆóÊ≥ïÔºå‰ª•ÂèäÈ´òÊïàÁöÑÁ≥ªÁªüÊ®°ÊãüÊäÄÊúØ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜÈÄíÂΩíÂìàÂØÜÈ°øÂõûÂ£∞Â≠¶‰π†ÔºàRHELÔºâÁÆóÊ≥ïÔºåËØ•ÁÆóÊ≥ïÂèØ‰ª•ËØÅÊòéÂú∞ËÆ°ÁÆóÈùûËÄóÊï£ÂìàÂØÜÈ°øÁ≥ªÁªüÁâ©ÁêÜËΩ®ËøπÁöÑÊçüÂ§±Ê¢ØÂ∫¶„ÄÇRHEL‰ªÖÈúÄ‰∏â‰∏™‚ÄúÂâçÂêë‰º†ÈÄí‚ÄùÔºåËÄå‰∏çÈúÄË¶ÅÊòæÂºèÁöÑÈõÖÂèØÊØîËÆ°ÁÆóÔºå‰πü‰∏ç‰ºöÂºïÂÖ•Ê¢ØÂ∫¶‰º∞ËÆ°ÁöÑÊñπÂ∑Æ„ÄÇÊàë‰ª¨È¶ñÂÖàÂú®ËøûÁª≠Êó∂Èó¥‰∏≠‰ªãÁªçRHELÔºåÂπ∂ËØÅÊòéÂÖ∂‰∏éËøûÁª≠‰º¥ÈöèÁä∂ÊÄÅÊñπÊ≥ïÁöÑÂΩ¢ÂºèÁ≠â‰ª∑„ÄÇ‰∏∫‰∫Ü‰øÉËøõÈÄöËøáRHELËÆ≠ÁªÉÁöÑÂìàÂØÜÈ°øÁ≥ªÁªüÁöÑÊ®°ÊãüÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜRHELÁöÑÁ¶ªÊï£Êó∂Èó¥ÁâàÊú¨ÔºåËØ•ÁâàÊú¨Âú®Â∫îÁî®‰∫éÊàë‰ª¨Áß∞‰πã‰∏∫ÂìàÂØÜÈ°øÈÄíÂΩíÂçïÂÖÉÔºàHRUsÔºâÁöÑÈÄíÂΩíÊ®°ÂùóÊó∂Á≠â‰ª∑‰∫éÊó∂Èó¥ÂèçÂêë‰º†Êí≠ÔºàBPTTÔºâ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥ÈïøÁ®ã‰æùËµñÂ≠¶‰π†‰∏≠ÁöÑËÆ°ÁÆóÊïàÁéáÈóÆÈ¢òÔºåÁé∞ÊúâÊñπÊ≥ïÂ¶ÇBPTTÂú®Â§ÑÁêÜÂ§ßËßÑÊ®°Ê®°ÂûãÊó∂ËÆ°ÁÆóÂ§çÊùÇÂ∫¶È´òÔºå‰∏îÂú®ÈïøÂ∫èÂàó‰∏äÂÆπÊòìÂá∫Áé∞Ê¢ØÂ∫¶Ê∂àÂ§±ÊàñÁàÜÁÇ∏ÁöÑÈóÆÈ¢ò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöRHELÁÆóÊ≥ïÈÄöËøáÂºïÂÖ•Áâ©ÁêÜËΩ®ËøπÁöÑÊúâÈôêÂ∑ÆÂàÜËÆ°ÁÆóÊçüÂ§±Ê¢ØÂ∫¶ÔºåÈÅøÂÖç‰∫ÜÊòæÂºèÁöÑÈõÖÂèØÊØîËÆ°ÁÆóÔºå‰ªéËÄåÊèêÈ´ò‰∫ÜËÆ°ÁÆóÊïàÁéáÂíåÁ®≥ÂÆöÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöRHELÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨ËøûÁª≠Êó∂Èó¥ÂíåÁ¶ªÊï£Êó∂Èó¥‰∏§ÁßçÂΩ¢ÂºèÔºåËøûÁª≠Êó∂Èó¥ÂΩ¢Âºè‰∏é‰º¥ÈöèÁä∂ÊÄÅÊñπÊ≥ïÁ≠â‰ª∑ÔºåËÄåÁ¶ªÊï£Êó∂Èó¥ÂΩ¢ÂºèÂàô‰∏éBPTTÁõ∏ÂØπÂ∫î„ÄÇ‰∏ªË¶ÅÊ®°ÂùóÂåÖÊã¨ÂìàÂØÜÈ°øÈÄíÂΩíÂçïÂÖÉÔºàHRUsÔºâÂíåÂìàÂØÜÈ°øÁä∂ÊÄÅÁ©∫Èó¥Ê®°ÂûãÔºàHSSMsÔºâ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöRHELÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÂÖ∂ÈÄöËøáÁâ©ÁêÜÂéüÂàôËÆ°ÁÆóÊ¢ØÂ∫¶ÁöÑËÉΩÂäõÔºåÊòæËëóÈôç‰Ωé‰∫ÜËÆ°ÁÆóÂ§çÊùÇÂ∫¶ÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜÊ®°ÂûãÁöÑË°®ËææËÉΩÂäõÔºå‰∏é‰º†ÁªüÁöÑÂèçÂêë‰º†Êí≠ÊñπÊ≥ïÁõ∏ÊØîÂÖ∑ÊúâÊú¨Ë¥®Âå∫Âà´„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöRHELÂú®ÂèÇÊï∞ËÆæÁΩÆ‰∏äÈááÁî®‰∫ÜÁâ©ÁêÜÁ≥ªÁªüÁöÑÁâπÊÄßÔºåÊçüÂ§±ÂáΩÊï∞ËÆæËÆ°ÈÅµÂæ™Áâ©ÁêÜËΩ®ËøπÁöÑÂèòÂåñÔºåÁΩëÁªúÁªìÊûÑÂàôÂü∫‰∫éÂìàÂØÜÈ°øÂä®ÂäõÂ≠¶ÊûÑÂª∫ÔºåÁ°Æ‰øù‰∫ÜÊ®°ÂûãÁöÑÁ®≥ÂÆöÊÄßÂíåÂèØÊâ©Â±ïÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåRHELÂú®Â§ÑÁêÜÂ∫èÂàóÈïøÂ∫¶ËææÂà∞Á∫¶50kÁöÑ‰ªªÂä°‰∏≠ÔºåÊÄßËÉΩ‰∏éBPTTÁõ∏ÂΩìÔºå‰∏îÂú®ËÆ°ÁÆóÊïàÁéá‰∏äÂÖ∑ÊúâÊòéÊòæ‰ºòÂäø„ÄÇËøôË°®ÊòéRHELËÉΩÂ§üÊúâÊïàÂ∫îÂØπÈïøÁ®ã‰æùËµñÈóÆÈ¢òÔºåÊé®Âä®Áâ©ÁêÜÂü∫Á°ÄÁöÑËá™Â≠¶‰π†Á≥ªÁªüÁöÑÂèëÂ±ï„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Êó∂Èó¥Â∫èÂàóÈ¢ÑÊµã„ÄÅÈáëËûçÊï∞ÊçÆÂàÜÊûê„ÄÅÊ∞îÂÄôÊ®°ÂûãÁ≠â„ÄÇÈÄöËøáÂºïÂÖ•RHELÁÆóÊ≥ïÔºåËÉΩÂ§üËÆæËÆ°Âá∫Êõ¥ÂÖ∑ËÉΩÊïàÁöÑËá™Â≠¶‰π†Á≥ªÁªüÔºåÈÄÇÁî®‰∫éÈúÄË¶ÅÂ§ÑÁêÜÈïøÂ∫èÂàóÊï∞ÊçÆÁöÑÂú∫ÊôØÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÊú™Êù•ÂΩ±Âìç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Deep State Space Models (SSMs) reignite physics-grounded compute paradigms, as RNNs could natively be embodied into dynamical systems. This calls for dedicated learning algorithms obeying to core physical principles, with efficient techniques to simulate these systems and guide their design. We propose Recurrent Hamiltonian Echo Learning (RHEL), an algorithm which provably computes loss gradients as finite differences of physical trajectories of non-dissipative, Hamiltonian systems. In ML terms, RHEL only requires three "forward passes" irrespective of model size, without explicit Jacobian computation, nor incurring any variance in the gradient estimation. Motivated by the physical realization of our algorithm, we first introduce RHEL in continuous time and demonstrate its formal equivalence with the continuous adjoint state method. To facilitate the simulation of Hamiltonian systems trained by RHEL, we propose a discrete-time version of RHEL which is equivalent to Backpropagation Through Time (BPTT) when applied to a class of recurrent modules which we call Hamiltonian Recurrent Units (HRUs). This setting allows us to demonstrate the scalability of RHEL by generalizing these results to hierarchies of HRUs, which we call Hamiltonian SSMs (HSSMs). We apply RHEL to train HSSMs with linear and nonlinear dynamics on a variety of time-series tasks ranging from mid-range to long-range classification and regression with sequence length reaching $\sim 50k$. We show that RHEL consistently matches the performance of BPTT across all models and tasks. This work opens new doors for the design of scalable, energy-efficient physical systems endowed with self-learning capabilities for sequence modelling.

