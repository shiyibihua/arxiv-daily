---
layout: default
title: Mixture-of-Experts Meets In-Context Reinforcement Learning
---

# Mixture-of-Experts Meets In-Context Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05426" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05426v3</a>
  <a href="https://arxiv.org/pdf/2506.05426.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05426v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05426v3', 'Mixture-of-Experts Meets In-Context Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Wenhao Wu, Fuhong Liu, Haoru Li, Zican Hu, Daoyi Dong, Chunlin Chen, Zhi Wang

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05 (æ›´æ–°: 2025-10-28)

**å¤‡æ³¨**: 28 pages, 13 figures, 17 tables

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/NJU-RL/T2MIR)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºT2MIRæ¡†æ¶ä»¥è§£å†³ICRLä¸­çš„å¤šæ¨¡æ€ä¸ä»»åŠ¡å¼‚è´¨æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `ä¸Šä¸‹æ–‡å¼ºåŒ–å­¦ä¹ ` `æ··åˆä¸“å®¶` `å¤šæ¨¡æ€å­¦ä¹ ` `å¯¹æ¯”å­¦ä¹ ` `å†³ç­–æ¨¡å‹` `ä»»åŠ¡è·¯ç”±` `æ·±åº¦å­¦ä¹ ` `æ™ºèƒ½ç³»ç»Ÿ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ICRLæ–¹æ³•åœ¨å¤„ç†å¤šæ¨¡æ€çŠ¶æ€-åŠ¨ä½œ-å¥–åŠ±æ•°æ®æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œéš¾ä»¥æœ‰æ•ˆé€‚åº”å¤šæ ·åŒ–çš„å†³ç­–ä»»åŠ¡ã€‚
2. æœ¬æ–‡æå‡ºçš„T2MIRæ¡†æ¶é€šè¿‡å¼•å…¥æ··åˆä¸“å®¶æ¶æ„ï¼Œåˆ©ç”¨token-wiseå’Œtask-wise MoEæ¥è§£å†³å¤šæ¨¡æ€æ€§å’Œä»»åŠ¡å¼‚è´¨æ€§é—®é¢˜ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒT2MIRåœ¨å¤šä¸ªåŸºçº¿æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—æå‡äº†ICRLçš„å­¦ä¹ èƒ½åŠ›ï¼Œå±•ç¤ºäº†å…¶åœ¨è¯­è¨€å’Œè§†è§‰é¢†åŸŸçš„æ½œåŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨ä¸Šä¸‹æ–‡å¼ºåŒ–å­¦ä¹ ï¼ˆICRLï¼‰ä¸­ï¼Œé€šè¿‡æç¤ºæ¡ä»¶ä½¿å¼ºåŒ–å­¦ä¹ ä»£ç†é€‚åº”ä¸‹æ¸¸ä»»åŠ¡çš„æ½œåŠ›æ—¥ç›Šæ˜¾ç°ã€‚ç„¶è€Œï¼ŒçŠ¶æ€-åŠ¨ä½œ-å¥–åŠ±æ•°æ®çš„å†…åœ¨å¤šæ¨¡æ€æ€§å’Œå†³ç­–ä»»åŠ¡çš„å¤šæ ·æ€§ä¸å¼‚è´¨æ€§ä»ç„¶æ˜¯æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†T2MIRï¼ˆåŸºäºTokenå’Œä»»åŠ¡çš„æ··åˆä¸“å®¶æ¡†æ¶ï¼‰ï¼Œå°†æ··åˆä¸“å®¶ï¼ˆMoEï¼‰çš„æ¶æ„åˆ›æ–°å¼•å…¥åŸºäºå˜æ¢å™¨çš„å†³ç­–æ¨¡å‹ã€‚T2MIRç”¨ä¸¤ä¸ªå¹¶è¡Œå±‚æ›¿ä»£å‰é¦ˆå±‚ï¼šä¸€ä¸ªæ˜¯æ•æ‰å¤šæ¨¡æ€è¾“å…¥æ ‡è®°è¯­ä¹‰çš„token-wise MoEï¼Œå¦ä¸€ä¸ªæ˜¯å°†å¤šæ ·ä»»åŠ¡è·¯ç”±åˆ°ä¸“é—¨ä¸“å®¶çš„task-wise MoEã€‚é€šè¿‡å¼•å…¥å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œæœ€å¤§åŒ–ä»»åŠ¡ä¸å…¶è·¯ç”±è¡¨ç¤ºä¹‹é—´çš„äº’ä¿¡æ¯ï¼Œä»è€Œæ›´ç²¾ç¡®åœ°æ•æ‰ä»»åŠ¡ç›¸å…³ä¿¡æ¯ã€‚å®éªŒè¡¨æ˜ï¼ŒT2MIRæ˜¾è‘—æå‡äº†ICRLçš„å­¦ä¹ èƒ½åŠ›ï¼Œè¶…è¶Šäº†å¤šç§åŸºçº¿æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä¸Šä¸‹æ–‡å¼ºåŒ–å­¦ä¹ ï¼ˆICRLï¼‰ä¸­å­˜åœ¨çš„å¤šæ¨¡æ€çŠ¶æ€-åŠ¨ä½œ-å¥–åŠ±æ•°æ®å¤„ç†å’Œå†³ç­–ä»»åŠ¡å¼‚è´¨æ€§çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥å……åˆ†åˆ©ç”¨è¿™äº›ç‰¹æ€§ï¼Œå¯¼è‡´å­¦ä¹ æ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šT2MIRæ¡†æ¶é€šè¿‡å¼•å…¥æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„ï¼Œåˆ†åˆ«ä½¿ç”¨token-wiseå’Œtask-wise MoEæ¥æ•æ‰è¾“å…¥çš„å¤šæ¨¡æ€è¯­ä¹‰å’Œä»»åŠ¡ç‰¹å¾ï¼Œä»è€Œæé«˜å­¦ä¹ çš„é€‚åº”æ€§å’Œæ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šT2MIRçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªå¹¶è¡Œçš„MoEå±‚ï¼štoken-wise MoEè´Ÿè´£å¤„ç†è¾“å…¥æ ‡è®°çš„å¤šæ¨¡æ€ä¿¡æ¯ï¼Œtask-wise MoEåˆ™å°†ä»»åŠ¡è·¯ç”±åˆ°ä¸“é—¨çš„ä¸“å®¶ï¼Œä»¥å‡å°‘æ¢¯åº¦å†²çªã€‚ä¸¤è€…çš„è¾“å‡ºè¢«è¿æ¥åè¾“å…¥åˆ°ä¸‹ä¸€å±‚ã€‚

**å…³é”®åˆ›æ–°**ï¼šT2MIRçš„ä¸»è¦åˆ›æ–°åœ¨äºå°†æ··åˆä¸“å®¶æ¶æ„å¼•å…¥ICRLï¼Œåˆ©ç”¨å¯¹æ¯”å­¦ä¹ æ–¹æ³•æœ€å¤§åŒ–ä»»åŠ¡ä¸è·¯ç”±è¡¨ç¤ºä¹‹é—´çš„äº’ä¿¡æ¯ï¼Œä»è€Œå®ç°æ›´ç²¾ç¡®çš„ä»»åŠ¡ä¿¡æ¯æ•æ‰ã€‚è¿™ä¸€è®¾è®¡ä¸ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼ŒT2MIRé‡‡ç”¨äº†å¯¹æ¯”å­¦ä¹ æŸå¤±å‡½æ•°ï¼Œä»¥å¢å¼ºä»»åŠ¡è·¯ç”±çš„å‡†ç¡®æ€§ã€‚ç½‘ç»œç»“æ„ä¸Šï¼Œtoken-wiseå’Œtask-wise MoEçš„è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å¤šæ ·åŒ–çš„è¾“å…¥å’Œä»»åŠ¡åˆ†å¸ƒã€‚å®éªŒä¸­ï¼Œæ¨¡å‹çš„å„ä¸ªç»„ä»¶ç»è¿‡ç²¾å¿ƒè°ƒä¼˜ï¼Œä»¥ç¡®ä¿æœ€ä½³æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒT2MIRåœ¨å¤šä¸ªåŸºçº¿æµ‹è¯•ä¸­è¡¨ç°æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œæå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œå±•ç¤ºäº†å…¶åœ¨ICRLé¢†åŸŸçš„å¼ºå¤§èƒ½åŠ›å’Œåº”ç”¨å‰æ™¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

T2MIRæ¡†æ¶åœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¤„ç†å¤æ‚å†³ç­–ä»»åŠ¡çš„åœºæ™¯ä¸­ï¼Œå¦‚è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½æœºå™¨äººå’Œä¸ªæ€§åŒ–æ¨èç³»ç»Ÿç­‰ã€‚å…¶åˆ›æ–°çš„å¤šæ¨¡æ€å¤„ç†èƒ½åŠ›å’Œä»»åŠ¡é€‚åº”æ€§å°†æ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›æ–°çš„æ€è·¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In-context reinforcement learning (ICRL) has emerged as a promising paradigm for adapting RL agents to downstream tasks through prompt conditioning. However, two notable challenges remain in fully harnessing in-context learning within RL domains: the intrinsic multi-modality of the state-action-reward data and the diverse, heterogeneous nature of decision tasks. To tackle these challenges, we propose T2MIR (Token- and Task-wise MoE for In-context RL), an innovative framework that introduces architectural advances of mixture-of-experts (MoE) into transformer-based decision models. T2MIR substitutes the feedforward layer with two parallel layers: a token-wise MoE that captures distinct semantics of input tokens across multiple modalities, and a task-wise MoE that routes diverse tasks to specialized experts for managing a broad task distribution with alleviated gradient conflicts. To enhance task-wise routing, we introduce a contrastive learning method that maximizes the mutual information between the task and its router representation, enabling more precise capture of task-relevant information. The outputs of two MoE components are concatenated and fed into the next layer. Comprehensive experiments show that T2MIR significantly facilitates in-context learning capacity and outperforms various types of baselines. We bring the potential and promise of MoE to ICRL, offering a simple and scalable architectural enhancement to advance ICRL one step closer toward achievements in language and vision communities. Our code is available at https://github.com/NJU-RL/T2MIR.

