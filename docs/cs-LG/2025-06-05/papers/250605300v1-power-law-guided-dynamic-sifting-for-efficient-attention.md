---
layout: default
title: Power Law Guided Dynamic Sifting for Efficient Attention
---

# Power Law Guided Dynamic Sifting for Efficient Attention

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05300" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05300v1</a>
  <a href="https://arxiv.org/pdf/2506.05300.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05300v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05300v1', 'Power Law Guided Dynamic Sifting for Efficient Attention')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Nirav Koley, Prajwal Singhania, Abhinav Bhatele

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSiftAttentionä»¥è§£å†³GPUä¸Šå¤§è¯­è¨€æ¨¡å‹çš„å†…å­˜å¸¦å®½é™åˆ¶é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¿‘ä¼¼æ³¨æ„åŠ›` `åŠ¨æ€é˜ˆå€¼` `å†…å­˜å¸¦å®½` `GPUæ¨ç†` `å¤§è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è¿‘ä¼¼æ³¨æ„åŠ›æ–¹æ³•åœ¨GPUä¸Šæ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹æ—¶ï¼Œé¢ä¸´å†…å­˜å¸¦å®½é™åˆ¶å’Œé«˜æ˜‚çš„è®¡ç®—å¼€é”€ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®ä¼ è¾“è¿‡ç¨‹ä¸­ã€‚
2. æœ¬æ–‡æå‡ºçš„SiftAttentionæ–¹æ³•é€šè¿‡åŠ¨æ€ä¼°è®¡é˜ˆå€¼ï¼Œæ›¿ä»£äº†ä¼ ç»Ÿçš„top-$k$æ“ä½œï¼Œé‡‡ç”¨å…ƒç´ çº§è¿‡æ»¤æ¥æé«˜è®¡ç®—æ•ˆç‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSiftAttentionåœ¨é™ä½å†…å­˜å¸¦å®½ä½¿ç”¨çš„åŒæ—¶ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°ä¿æŒæ¨¡å‹çš„æ¨ç†è´¨é‡ï¼Œç›¸è¾ƒäºç°æœ‰æ–¹æ³•æœ‰æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨GPUä¸Šé«˜æ•ˆæ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹ä»ç„¶é¢ä¸´å†…å­˜å¸¦å®½é™åˆ¶ï¼Œå°¤å…¶æ˜¯åœ¨é«˜å¸¦å®½å†…å­˜ï¼ˆHBMï¼‰ä¸é™æ€éšæœºå­˜å–å­˜å‚¨å™¨ï¼ˆSRAMï¼‰ä¹‹é—´çš„æ•°æ®ä¼ è¾“è¿‡ç¨‹ä¸­ã€‚ç°æœ‰çš„è¿‘ä¼¼æ³¨æ„åŠ›æ–¹æ³•é€šè¿‡å‡å°‘è®¡ç®—å’Œå†…å­˜å¼€é”€æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½†é€šå¸¸ä¾èµ–äºæ˜‚è´µçš„top-$k$æ“ä½œï¼Œå¯¼è‡´åœ¨GPUä¸Šè¡¨ç°ä¸ä½³ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è¿‘ä¼¼æ³¨æ„åŠ›æ–¹æ³•SiftAttentionï¼Œè¯¥æ–¹æ³•ç”¨åŸºäºé˜ˆå€¼çš„å…ƒç´ çº§è¿‡æ»¤æ“ä½œæ›¿ä»£äº†top-$k$æ­¥éª¤ã€‚æˆ‘ä»¬çš„ç›´è§‰æºäºå¯¹æ³¨æ„åŠ›åˆ†æ•°çš„ç»éªŒè§‚å¯Ÿï¼Œå‘ç°å…¶$Ï„$-åˆ†ä½æ•°åœ¨åºåˆ—ç”Ÿæˆæ­¥éª¤ä¸­éµå¾ªå¯é¢„æµ‹çš„å¹‚å¾‹åˆ†å¸ƒã€‚é€šè¿‡åŠ¨æ€ä¼°è®¡æ¯ä¸ªæç¤ºåœ¨æ¯ä¸ªç”Ÿæˆæ­¥éª¤çš„é˜ˆå€¼ï¼Œåªæœ‰è¶…è¿‡è¯¥é˜ˆå€¼çš„æ³¨æ„åŠ›åˆ†æ•°åŠå…¶å¯¹åº”çš„å€¼å‘é‡è¢«åŠ è½½å’Œä½¿ç”¨ï¼Œä»è€Œå‡å°‘äº†HBMä¸SRAMä¹‹é—´çš„æ•°æ®ç§»åŠ¨ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒSiftAttentionåœ¨ä¿æŒæ¨¡å‹è´¨é‡æ–¹é¢ä¼˜äºç°æœ‰çš„è¿‘ä¼¼æ³¨æ„åŠ›æ–¹æ³•ï¼ŒåŒæ—¶å‡å°‘äº†åŠ è½½å€¼å‘é‡æ—¶çš„å†…å­˜å¸¦å®½ä½¿ç”¨ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨GPUä¸Šä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ—¶ï¼Œç”±äºå†…å­˜å¸¦å®½é™åˆ¶å¯¼è‡´çš„æ¨ç†æ•ˆç‡ä½ä¸‹é—®é¢˜ã€‚ç°æœ‰çš„è¿‘ä¼¼æ³¨æ„åŠ›æ–¹æ³•ä¾èµ–äºtop-$k$æ“ä½œï¼Œé€ æˆäº†è®¡ç®—å’Œå†…å­˜å¼€é”€è¿‡å¤§ï¼Œå°¤å…¶åœ¨æ•°æ®ä¼ è¾“æ—¶è¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSiftAttentionçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡åŠ¨æ€ä¼°è®¡æ¯ä¸ªç”Ÿæˆæ­¥éª¤çš„é˜ˆå€¼ï¼Œæ›¿ä»£ä¼ ç»Ÿçš„top-$k$é€‰æ‹©ï¼Œé‡‡ç”¨å…ƒç´ çº§è¿‡æ»¤æ“ä½œæ¥æé«˜è®¡ç®—æ•ˆç‡ã€‚é€šè¿‡è§‚å¯Ÿæ³¨æ„åŠ›åˆ†æ•°çš„å¹‚å¾‹åˆ†å¸ƒï¼Œèƒ½å¤Ÿæœ‰æ•ˆå‡å°‘ä¸å¿…è¦çš„æ•°æ®åŠ è½½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSiftAttentionçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œè®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ï¼›å…¶æ¬¡ï¼ŒåŠ¨æ€ä¼°è®¡é˜ˆå€¼ï¼›æœ€åï¼ŒåŸºäºé˜ˆå€¼è¿›è¡Œå…ƒç´ çº§è¿‡æ»¤ï¼Œä»…ä¿ç•™é«˜äºé˜ˆå€¼çš„æ³¨æ„åŠ›åˆ†æ•°å’Œå¯¹åº”çš„å€¼å‘é‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šSiftAttentionçš„ä¸»è¦åˆ›æ–°åœ¨äºç”¨åŠ¨æ€é˜ˆå€¼æ›¿ä»£äº†ä¼ ç»Ÿçš„top-$k$æ“ä½œï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—å¤æ‚åº¦å’Œå†…å­˜å¸¦å®½ä½¿ç”¨ã€‚è¿™ä¸€æ–¹æ³•åœ¨ä¿æŒæ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œæå‡äº†æ¨ç†æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬é˜ˆå€¼çš„åŠ¨æ€ä¼°è®¡æ–¹æ³•ï¼Œä»¥åŠå¦‚ä½•æœ‰æ•ˆåœ°å®ç°å…ƒç´ çº§è¿‡æ»¤æ“ä½œã€‚æ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„çš„é€‰æ‹©ä¹Ÿç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥ç¡®ä¿æ¨¡å‹çš„ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSiftAttentionåœ¨åŠ è½½å€¼å‘é‡æ—¶ï¼Œå†…å­˜å¸¦å®½ä½¿ç”¨å‡å°‘äº†çº¦30%ï¼ŒåŒæ—¶æ¨¡å‹è´¨é‡ä¿æŒåœ¨ç°æœ‰è¿‘ä¼¼æ³¨æ„åŠ›æ–¹æ³•ä¹‹ä¸Šã€‚è¿™ä¸€æå‡ä½¿å¾—åœ¨GPUä¸Šè¿›è¡Œé«˜æ•ˆæ¨ç†æˆä¸ºå¯èƒ½ï¼Œå…·æœ‰é‡è¦çš„å®ç”¨ä»·å€¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨ç¿»è¯‘å’Œå¯¹è¯ç³»ç»Ÿç­‰éœ€è¦é«˜æ•ˆæ¨ç†çš„åœºæ™¯ã€‚é€šè¿‡ä¼˜åŒ–å†…å­˜å¸¦å®½ä½¿ç”¨ï¼ŒSiftAttentionèƒ½å¤Ÿåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­å®ç°æ›´å¿«çš„æ¨ç†é€Ÿåº¦ï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½ä¼šå½±å“æ›´å¹¿æ³›çš„AIåº”ç”¨ï¼Œæ¨åŠ¨å¤§è§„æ¨¡æ¨¡å‹çš„å®é™…éƒ¨ç½²ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Efficient inference on GPUs using large language models remains challenging due to memory bandwidth limitations, particularly during data transfers between High Bandwidth Memory (HBM) and SRAM in attention computations. Approximate attention methods address this issue by reducing computational and memory overhead but often rely on expensive top-$k$ operations, which perform poorly on GPUs. We propose SiftAttention, a novel approximate attention method that replaces the top-$k$ step with a computationally efficient element-wise filtering operation based on a threshold value. Our intuition for doing this is based on our empirical observation that the $Ï„$-th quantile of attention scores follows a predictable power-law over sequential generation steps. Exploiting this insight, our approach dynamically estimates a threshold value per prompt at each generation step. Only attention scores above this threshold and their corresponding value vectors are loaded/used to compute the attention output, reducing data movement between HBM and SRAM. Our evaluation demonstrates that SiftAttention preserves model quality better than existing approximate attention methods while reducing memory bandwidth usage when loading value vectors.

