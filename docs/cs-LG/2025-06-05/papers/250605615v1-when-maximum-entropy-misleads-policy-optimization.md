---
layout: default
title: When Maximum Entropy Misleads Policy Optimization
---

# When Maximum Entropy Misleads Policy Optimization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05615" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05615v1</a>
  <a href="https://arxiv.org/pdf/2506.05615.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05615v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05615v1', 'When Maximum Entropy Misleads Policy Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ruipeng Zhang, Ya-Chien Chang, Sicun Gao

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05

**æœŸåˆŠ**: ICML 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åˆ†ææœ€å¤§ç†µå¼ºåŒ–å­¦ä¹ åœ¨æ§åˆ¶ä»»åŠ¡ä¸­çš„è¯¯å¯¼æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æœ€å¤§ç†µå¼ºåŒ–å­¦ä¹ ` `ç­–ç•¥ä¼˜åŒ–` `æ§åˆ¶ä»»åŠ¡` `å¥–åŠ±è®¾è®¡` `ç¨³å¥æ€§ä¸æœ€ä¼˜æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æœ€å¤§ç†µå¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨æŸäº›æ§åˆ¶ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ï¼Œæ— æ³•æœ‰æ•ˆåº”å¯¹éœ€è¦ç²¾ç¡®æ§åˆ¶çš„åœºæ™¯ã€‚
2. æœ¬æ–‡é€šè¿‡åˆ†æç¨³å¥æ€§ä¸æœ€ä¼˜æ€§ä¹‹é—´çš„æƒè¡¡ï¼Œæå‡ºäº†åœ¨å¤æ‚æ§åˆ¶ä»»åŠ¡ä¸­å¹³è¡¡ç†µæœ€å¤§åŒ–ä¸å¥–åŠ±è®¾è®¡çš„æ–°æ€è·¯ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒMaxEntç®—æ³•åœ¨ç‰¹å®šä»»åŠ¡ä¸­å­˜åœ¨è¯¯å¯¼æ€§ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œä¸”ä¸éMaxEntç®—æ³•ç›¸æ¯”ï¼Œå­˜åœ¨æ˜¾è‘—å·®è·ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ€å¤§ç†µå¼ºåŒ–å­¦ä¹ ï¼ˆMaxEnt RLï¼‰æ¡†æ¶æ˜¯å®ç°é«˜æ•ˆå­¦ä¹ å’Œç¨³å¥æ€§èƒ½çš„ä¸»è¦æ–¹æ³•ã€‚ç„¶è€Œï¼ŒMaxEntæ–¹æ³•åœ¨æŸäº›æ€§èƒ½å…³é”®çš„æ§åˆ¶é—®é¢˜ä¸Šè¡¨ç°ä¸ä½³ï¼Œè€ŒéMaxEntç®—æ³•å´èƒ½æˆåŠŸå­¦ä¹ ã€‚æœ¬æ–‡åˆ†æäº†ç¨³å¥æ€§ä¸æœ€ä¼˜æ€§ä¹‹é—´çš„æƒè¡¡å¦‚ä½•å½±å“MaxEntç®—æ³•åœ¨å¤æ‚æ§åˆ¶ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼šå°½ç®¡ç†µæœ€å¤§åŒ–å¢å¼ºäº†æ¢ç´¢å’Œç¨³å¥æ€§ï¼Œä½†ä¹Ÿå¯èƒ½è¯¯å¯¼ç­–ç•¥ä¼˜åŒ–ï¼Œå¯¼è‡´åœ¨éœ€è¦ç²¾ç¡®ã€ä½ç†µç­–ç•¥çš„ä»»åŠ¡ä¸­å¤±è´¥ã€‚é€šè¿‡å¯¹å¤šç§æ§åˆ¶é—®é¢˜çš„å®éªŒï¼Œæˆ‘ä»¬å…·ä½“å±•ç¤ºäº†è¿™ç§è¯¯å¯¼æ•ˆåº”ï¼Œå¹¶ä¸ºåœ¨æŒ‘æˆ˜æ€§æ§åˆ¶é—®é¢˜ä¸­å¹³è¡¡å¥–åŠ±è®¾è®¡å’Œç†µæœ€å¤§åŒ–æä¾›äº†æ›´å¥½çš„ç†è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æœ€å¤§ç†µå¼ºåŒ–å­¦ä¹ åœ¨å¤æ‚æ§åˆ¶ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦ç²¾ç¡®æ§åˆ¶çš„åœºæ™¯ä¸­ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æœ‰æ•ˆå­¦ä¹ ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡é€šè¿‡åˆ†æç†µæœ€å¤§åŒ–å¯¹ç­–ç•¥ä¼˜åŒ–çš„å½±å“ï¼Œæå‡ºåœ¨è®¾è®¡å¥–åŠ±æ—¶éœ€è€ƒè™‘ç†µçš„å¹³è¡¡ï¼Œä»¥é¿å…è¯¯å¯¼ç­–ç•¥å­¦ä¹ ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨å®éªŒå¯¹æ¯”çš„æ–¹æ³•ï¼Œè¯„ä¼°ä¸åŒæ§åˆ¶ä»»åŠ¡ä¸­MaxEntç®—æ³•ä¸éMaxEntç®—æ³•çš„è¡¨ç°ï¼Œåˆ†æå…¶åœ¨ç­–ç•¥ä¼˜åŒ–ä¸­çš„è¯¯å¯¼æ€§ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬ä»»åŠ¡è®¾è®¡ã€ç®—æ³•å®ç°å’Œæ€§èƒ½è¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæ­ç¤ºäº†ç†µæœ€å¤§åŒ–åœ¨æŸäº›ä»»åŠ¡ä¸­å¯èƒ½å¯¼è‡´çš„ç­–ç•¥è¯¯å¯¼ï¼Œå¼ºè°ƒäº†åœ¨è®¾è®¡å¼ºåŒ–å­¦ä¹ ç®—æ³•æ—¶éœ€ç»¼åˆè€ƒè™‘ç¨³å¥æ€§ä¸æœ€ä¼˜æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œè®¾ç½®äº†ä¸åŒçš„å¥–åŠ±å‡½æ•°å’Œç†µæƒé‡ï¼Œä»¥è§‚å¯Ÿå…¶å¯¹å­¦ä¹ æ•ˆæœçš„å½±å“ï¼Œé‡‡ç”¨äº†å¤šç§æ§åˆ¶ä»»åŠ¡è¿›è¡ŒéªŒè¯ï¼Œç¡®ä¿ç»“æœçš„å¯é æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ç‰¹å®šæ§åˆ¶ä»»åŠ¡ä¸­ï¼ŒMaxEntç®—æ³•çš„è¡¨ç°æ˜¾è‘—ä½äºéMaxEntç®—æ³•ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦ä½ç†µç­–ç•¥çš„åœºæ™¯ä¸­ï¼Œæ€§èƒ½ä¸‹é™å¹…åº¦å¯è¾¾30%ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†ç†µæœ€å¤§åŒ–åœ¨æŸäº›æƒ…å†µä¸‹çš„è¯¯å¯¼æ€§ï¼Œæä¾›äº†æ–°çš„è§†è§’æ¥æ”¹è¿›å¼ºåŒ–å­¦ä¹ ç®—æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶å’Œæ™ºèƒ½å†³ç­–ç³»ç»Ÿç­‰ï¼Œèƒ½å¤Ÿä¸ºè®¾è®¡æ›´é«˜æ•ˆçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•æä¾›ç†è®ºæ”¯æŒã€‚é€šè¿‡ä¼˜åŒ–å¥–åŠ±è®¾è®¡å’Œç†µæœ€å¤§åŒ–çš„å¹³è¡¡ï¼Œæœªæ¥å¯ä»¥æå‡è¿™äº›ç³»ç»Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­çš„è¡¨ç°å’Œé€‚åº”èƒ½åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The Maximum Entropy Reinforcement Learning (MaxEnt RL) framework is a leading approach for achieving efficient learning and robust performance across many RL tasks. However, MaxEnt methods have also been shown to struggle with performance-critical control problems in practice, where non-MaxEnt algorithms can successfully learn. In this work, we analyze how the trade-off between robustness and optimality affects the performance of MaxEnt algorithms in complex control tasks: while entropy maximization enhances exploration and robustness, it can also mislead policy optimization, leading to failure in tasks that require precise, low-entropy policies. Through experiments on a variety of control problems, we concretely demonstrate this misleading effect. Our analysis leads to better understanding of how to balance reward design and entropy maximization in challenging control problems.

