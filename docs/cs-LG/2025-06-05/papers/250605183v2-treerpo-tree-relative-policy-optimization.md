---
layout: default
title: TreeRPO: Tree Relative Policy Optimization
---

# TreeRPO: Tree Relative Policy Optimization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05183" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05183v2</a>
  <a href="https://arxiv.org/pdf/2506.05183.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05183v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05183v2', 'TreeRPO: Tree Relative Policy Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhicheng Yang, Zhijiang Guo, Yinya Huang, Xiaodan Liang, Yiwei Wang, Jing Tang

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05 (æ›´æ–°: 2025-09-27)

**å¤‡æ³¨**: 13pages, 6 figures

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/yangzhch6/TreeRPO)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTreeRPOä»¥ä¼˜åŒ–æ¨ç†è¿‡ç¨‹ä¸­çš„å¥–åŠ±ä¿¡å·**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `å¥–åŠ±ä¼˜åŒ–` `æ¨ç†è¿‡ç¨‹` `æ ‘é‡‡æ ·` `å¤§å‹è¯­è¨€æ¨¡å‹` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ¨¡å‹è®­ç»ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„å¥–åŠ±ä¿¡å·æŒ‡å¯¼ä¸è¶³ï¼Œéš¾ä»¥ä¼˜åŒ–ä¸­é—´æ­¥éª¤ï¼Œå½±å“æ¨¡å‹æ€§èƒ½ã€‚
2. æœ¬æ–‡æå‡ºTreeRPOï¼Œé€šè¿‡æ ‘é‡‡æ ·ç›´æ¥ä¼°è®¡æ¨ç†æ­¥éª¤çš„å¥–åŠ±ï¼Œé¿å…äº†å•ç‹¬æ­¥éª¤å¥–åŠ±æ¨¡å‹çš„å±€é™æ€§ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒTreeRPOåœ¨Qwen-2.5-Mathçš„æµ‹è¯•åŸºå‡†ä¸Šæ˜¾è‘—æé«˜äº†å‡†ç¡®ç‡ï¼Œå¹¶å‡å°‘äº†å“åº”é•¿åº¦ï¼Œå±•ç°äº†ä¼˜è¶Šçš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•å±•ç°äº†å“è¶Šçš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨å…¨è½¨è¿¹å±‚é¢å®šä¹‰çš„å¥–åŠ±å¯¹ä¼˜åŒ–æ¨ç†è¿‡ç¨‹ä¸­çš„ä¸­é—´æ­¥éª¤æŒ‡å¯¼ä¸è¶³ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†TreeRPOï¼Œä¸€ç§é€šè¿‡æ ‘é‡‡æ ·ä¼°è®¡å„æ¨ç†æ­¥éª¤å¥–åŠ±æ•°å­¦æœŸæœ›çš„æ–°æ–¹æ³•ã€‚ä¸ä¾èµ–å•ç‹¬æ­¥éª¤å¥–åŠ±æ¨¡å‹çš„å…ˆå‰æ–¹æ³•ä¸åŒï¼ŒTreeRPOç›´æ¥é€šè¿‡é‡‡æ ·è¿‡ç¨‹ä¼°è®¡è¿™äº›å¥–åŠ±ã€‚åŸºäºGRPOçš„ç»„ç›¸å¯¹å¥–åŠ±è®­ç»ƒæœºåˆ¶ï¼ŒTreeRPOåˆ›æ–°æ€§åœ°æ ¹æ®æ ‘é‡‡æ ·ç”Ÿæˆçš„æ­¥éª¤çº§ç»„è®¡ç®—å¥–åŠ±ï¼Œæ˜¾è‘—å¢å¼ºäº†å­¦ä¹ è¿‡ç¨‹å’ŒLLMsçš„æ•´ä½“æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTreeRPOæ˜¾è‘—æé«˜äº†Qwen-2.5-Mathåœ¨æµ‹è¯•åŸºå‡†ä¸Šçš„å¹³å‡Pass@1å‡†ç¡®ç‡ï¼Œä»19.0%æå‡è‡³35.5%ã€‚æ­¤å¤–ï¼ŒTreeRPOåœ¨æ€§èƒ½ä¸Šæ¯”GRPOæå‡äº†2.9%ï¼ŒåŒæ—¶å¹³å‡å“åº”é•¿åº¦å‡å°‘äº†18.1%ï¼Œå±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§å’Œé«˜æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„å¥–åŠ±ä¿¡å·ä¸è¶³çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸­é—´æ­¥éª¤çš„ä¼˜åŒ–æŒ‡å¯¼ä¸Šå­˜åœ¨çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–å…¨è½¨è¿¹å¥–åŠ±ï¼Œå¯¼è‡´å¯¹æ¨ç†è¿‡ç¨‹çš„ç»†ç²’åº¦ä¼˜åŒ–èƒ½åŠ›ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šTreeRPOçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ ‘é‡‡æ ·ç›´æ¥ä¼°è®¡å„æ¨ç†æ­¥éª¤çš„å¥–åŠ±æœŸæœ›ï¼Œè€Œä¸æ˜¯ä¾èµ–äºå•ç‹¬çš„æ­¥éª¤å¥–åŠ±æ¨¡å‹ã€‚è¿™ç§è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­è·å¾—æ›´ä¸ºç»†è‡´å’Œå¯†é›†çš„å¥–åŠ±ä¿¡å·ï¼Œä»è€Œæå‡å­¦ä¹ æ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTreeRPOçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ ‘é‡‡æ ·æ¨¡å—å’Œå¥–åŠ±è®¡ç®—æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡æ ‘é‡‡æ ·ç”Ÿæˆæ¨ç†è¿‡ç¨‹çš„ä¸åŒè·¯å¾„ï¼Œç„¶ååœ¨è¿™äº›è·¯å¾„ä¸Šè®¡ç®—æ­¥éª¤çº§çš„å¥–åŠ±ï¼Œæœ€åå°†è¿™äº›å¥–åŠ±ç”¨äºæ¨¡å‹çš„è®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šTreeRPOçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶é€šè¿‡æ ‘é‡‡æ ·ç”Ÿæˆçš„æ­¥éª¤çº§ç»„æ¥è®¡ç®—å¥–åŠ±ï¼Œè¿™ä¸€æ–¹æ³•ä¸ç°æœ‰çš„ä¾èµ–å…¨è½¨è¿¹å¥–åŠ±çš„æ–¹å¼æœ‰æœ¬è´¨åŒºåˆ«ï¼Œèƒ½å¤Ÿæä¾›æ›´ä¸ºç»†è‡´çš„åé¦ˆä¿¡å·ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒTreeRPOé‡‡ç”¨äº†åŸºäºç»„ç›¸å¯¹å¥–åŠ±çš„è®­ç»ƒæœºåˆ¶ï¼Œå…³é”®å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°çš„è®¾è®¡æ—¨åœ¨æœ€å¤§åŒ–å¥–åŠ±ä¿¡å·çš„æœ‰æ•ˆæ€§å’Œå¯†åº¦ï¼Œç¡®ä¿æ¨¡å‹åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­èƒ½å¤Ÿè·å¾—å……åˆ†çš„æŒ‡å¯¼ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®å°†åœ¨ä»£ç ä¸­è¯¦ç»†è¯´æ˜ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTreeRPOæ˜¾è‘—æé«˜äº†Qwen-2.5-Mathåœ¨æµ‹è¯•åŸºå‡†ä¸Šçš„å¹³å‡Pass@1å‡†ç¡®ç‡ï¼Œä»19.0%æå‡è‡³35.5%ã€‚æ­¤å¤–ï¼ŒTreeRPOåœ¨æ€§èƒ½ä¸Šæ¯”GRPOæå‡äº†2.9%ï¼ŒåŒæ—¶å¹³å‡å“åº”é•¿åº¦å‡å°‘äº†18.1%ï¼Œå±•ç¤ºäº†å…¶åœ¨æœ‰æ•ˆæ€§å’Œé«˜æ•ˆæ€§ä¸Šçš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

TreeRPOçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¤æ‚æ¨ç†èƒ½åŠ›çš„ä»»åŠ¡ä¸­ï¼Œå¦‚è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿå’Œè‡ªåŠ¨åŒ–å†³ç­–æ”¯æŒç­‰ã€‚é€šè¿‡ä¼˜åŒ–æ¨ç†è¿‡ç¨‹ä¸­çš„å¥–åŠ±ä¿¡å·ï¼ŒTreeRPOèƒ½å¤Ÿæå‡æ¨¡å‹çš„æ¨ç†å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have shown remarkable reasoning capabilities through Reinforcement Learning with Verifiable Rewards (RLVR) methods. However, a key limitation of existing approaches is that rewards defined at the full trajectory level provide insufficient guidance for optimizing the intermediate steps of a reasoning process. To address this, we introduce \textbf{\name}, a novel method that estimates the mathematical expectations of rewards at various reasoning steps using tree sampling. Unlike prior methods that rely on a separate step reward model, \name directly estimates these rewards through this sampling process. Building on the group-relative reward training mechanism of GRPO, \name innovatively computes rewards based on step-level groups generated during tree sampling. This advancement allows \name to produce fine-grained and dense reward signals, significantly enhancing the learning process and overall performance of LLMs. Experimental results demonstrate that our \name algorithm substantially improves the average Pass@1 accuracy of Qwen-2.5-Math on test benchmarks, increasing it from 19.0\% to 35.5\%. Furthermore, \name significantly outperforms GRPO by 2.9\% in performance while simultaneously reducing the average response length by 18.1\%, showcasing its effectiveness and efficiency. Our code will be available at \href{https://github.com/yangzhch6/TreeRPO}{https://github.com/yangzhch6/TreeRPO}.

