---
layout: default
title: Dissecting Long-Chain-of-Thought Reasoning Models: An Empirical Study
---

# Dissecting Long-Chain-of-Thought Reasoning Models: An Empirical Study

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.04913" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.04913v2</a>
  <a href="https://arxiv.org/pdf/2506.04913.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.04913v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.04913v2', 'Dissecting Long-Chain-of-Thought Reasoning Models: An Empirical Study')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yongyu Mu, Jiali Zeng, Bei Li, Xinyan Guan, Fandong Meng, Jie Zhou, Tong Xiao, Jingbo Zhu

**åˆ†ç±»**: cs.LG, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05 (æ›´æ–°: 2025-11-10)

**å¤‡æ³¨**: Working in process

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/takagi97/Dissect-Long-Reason-Models)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç³»ç»Ÿåˆ†æé•¿é“¾æ¨ç†æ¨¡å‹ä»¥æå‡æ¨ç†èƒ½åŠ›ä¸æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿é“¾æ¨ç†` `å¼ºåŒ–å­¦ä¹ ` `è´Ÿæ ·æœ¬è®­ç»ƒ` `æ¨ç†æ•ˆç‡` `æ•°æ®ä½æ•ˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é•¿é“¾æ¨ç†æ¨¡å‹åœ¨è®­ç»ƒåŠ¨æ€ä¸Šå­˜åœ¨ä¸ç¡®å®šæ€§ï¼Œä¸”åœ¨æŸäº›æƒ…å†µä¸‹è¡¨ç°å‡ºåç›´è§‰çš„è¡Œä¸ºã€‚
2. è®ºæ–‡é€šè¿‡åˆ†ææ­£è´Ÿæ ·æœ¬çš„ä½œç”¨ï¼Œæå‡ºäº†åˆ©ç”¨è´Ÿæ ·æœ¬æ¥æå‡æ¨¡å‹æ¨ç†æ€§èƒ½çš„ç­–ç•¥ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè´Ÿæ ·æœ¬è®­ç»ƒåœ¨å†·å¯åŠ¨åœºæ™¯ä¸‹èƒ½æ˜¾è‘—æå‡æ¨ç†æ•ˆæœï¼Œä¸”ç›¸å¯¹é•¿åº¦å¥–åŠ±å’Œç¦»çº¿æ ·æœ¬æ³¨å…¥ç­–ç•¥æœ‰æ•ˆæé«˜äº†æ¨ç†æ•ˆç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡é€šè¿‡æ‰©å±•å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒé•¿é“¾æ¨ç†æ¨¡å‹å–å¾—äº†è¿›å±•ï¼Œä½†å…¶è®­ç»ƒåŠ¨æ€ä»ä¸å¤Ÿæ¸…æ™°ï¼Œä¸”å­˜åœ¨ä¸€äº›åç›´è§‰çš„è¡Œä¸ºã€‚æœ¬æ–‡é‡ç‚¹åˆ†æäº†æ­£è´Ÿæ ·æœ¬åœ¨æ‰©å±•RLä¸­çš„ä½œç”¨ï¼Œå‘ç°æ­£æ ·æœ¬æœ‰åŠ©äºç²¾ç¡®æ‹Ÿåˆè®­ç»ƒæ•°æ®ï¼Œè€Œè´Ÿæ ·æœ¬æ˜¾è‘—å¢å¼ºäº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ¢è®¨äº†ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ä¸­çš„æ•°æ®ä½æ•ˆé—®é¢˜ï¼Œå¹¶æå‡ºäº†ç›¸å¯¹é•¿åº¦å¥–åŠ±å’Œç¦»çº¿æ ·æœ¬æ³¨å…¥ç­‰ç­–ç•¥æ¥æå‡æ¨ç†æ•ˆç‡ã€‚æœ€åï¼Œç ”ç©¶äº†ä¸åŒæ¨ç†æ¨¡å‹å’ŒåŸºå‡†æµ‹è¯•ä¸­çš„ä¸ç¨³å®šæ€§ï¼ŒæŒ‡å‡ºè´ªå©ªè§£ç å¯èƒ½æ‰­æ›²è¯„ä¼°ç»“æœã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³é•¿é“¾æ¨ç†æ¨¡å‹è®­ç»ƒä¸­çš„åŠ¨æ€ä¸ç¡®å®šæ€§å’Œæ•°æ®ä½æ•ˆé—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨ä½¿ç”¨æ­£è´Ÿæ ·æœ¬æ—¶æœªèƒ½å……åˆ†åˆ©ç”¨è´Ÿæ ·æœ¬çš„æ½œåŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡ç³»ç»Ÿåˆ†ææ­£è´Ÿæ ·æœ¬åœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„ä½œç”¨ï¼Œæå‡ºè´Ÿæ ·æœ¬è®­ç»ƒèƒ½å¤Ÿåœ¨å†·å¯åŠ¨åœºæ™¯ä¸‹å®ç°å¼ºæ¨ç†æ€§èƒ½ï¼Œä¸”èƒ½æ”¹å–„æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶åˆ†ä¸ºä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šæ­£è´Ÿæ ·æœ¬åˆ†æã€æ•°æ®ä½æ•ˆä¼˜åŒ–ç­–ç•¥ï¼ˆç›¸å¯¹é•¿åº¦å¥–åŠ±å’Œç¦»çº¿æ ·æœ¬æ³¨å…¥ï¼‰ä»¥åŠæ¨ç†æ¨¡å‹æ€§èƒ½è¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºæ­ç¤ºäº†è´Ÿæ ·æœ¬åœ¨è®­ç»ƒä¸­çš„é‡è¦æ€§ï¼Œå°¤å…¶æ˜¯åœ¨é›¶å¼ºåŒ–å­¦ä¹ è®¾ç½®ä¸‹ï¼Œè´Ÿæ ·æœ¬è®­ç»ƒèƒ½ç‹¬ç«‹å®ç°ä¼˜å¼‚çš„æ¨ç†æ•ˆæœã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†ç›¸å¯¹é•¿åº¦å¥–åŠ±æœºåˆ¶å’Œç¦»çº¿æ ·æœ¬æ³¨å…¥ç­–ç•¥ï¼Œä»¥æé«˜æ ·æœ¬åˆ©ç”¨ç‡å’Œæ¨ç†æ•ˆç‡ï¼ŒåŒæ—¶åœ¨æ¨¡å‹è¯„ä¼°ä¸­æ³¨æ„é¿å…è´ªå©ªè§£ç å¸¦æ¥çš„ç»“æœæ‰­æ›²ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œè´Ÿæ ·æœ¬è®­ç»ƒåœ¨å†·å¯åŠ¨åœºæ™¯ä¸‹çš„æ¨ç†æ€§èƒ½æ˜¾è‘—ä¼˜äºä»…ä½¿ç”¨æ­£æ ·æœ¬çš„æ¨¡å‹ï¼Œä¸”åœ¨ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ä¸­ï¼Œé€šè¿‡å¼•å…¥ç›¸å¯¹é•¿åº¦å¥–åŠ±å’Œç¦»çº¿æ ·æœ¬æ³¨å…¥ï¼Œæ¨ç†æ•ˆç‡æå‡äº†è¶…è¿‡30%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿå’Œè‡ªåŠ¨æ¨ç†ç­‰ã€‚é€šè¿‡æå‡é•¿é“¾æ¨ç†æ¨¡å‹çš„æ•ˆç‡å’Œé²æ£’æ€§ï¼Œèƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­å®ç°æ›´é«˜çš„å‡†ç¡®æ€§å’Œç”¨æˆ·æ»¡æ„åº¦ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿåœ¨å¤æ‚ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Despite recent progress in training long-chain-of-thought reasoning models via scaling reinforcement learning (RL), its underlying training dynamics remain poorly understood, and several counterintuitive behaviors persist. This work focuses on three key aspects: (1) We systematically analyze the roles of positive and negative samples in scaling RL, revealing that positive samples mainly facilitate precise fitting to the training data, whereas negative samples significantly enhance generalization and robustness. Interestingly, while positive samples are essential for convergence in the zero-RL setting, training on negative samples alone suffices to attain strong reasoning performance and even better generalization in cold-start scenarios. (2) We identify substantial data inefficiency in group relative policy optimization, where over half of the samples yield zero advantage. To address this, we explore two strategies, including relative length rewards and offline sample injection, to leverage these data better and enhance reasoning efficiency and capability. (3) We investigate unstable performance across various reasoning models and benchmarks, attributing instability to uncertain problems with ambiguous outcomes, and demonstrate that greedy decoding can distort evaluation by flipping the correctness of responses. Our code is available at: https://github.com/takagi97/Dissect-Long-Reason-Models.

