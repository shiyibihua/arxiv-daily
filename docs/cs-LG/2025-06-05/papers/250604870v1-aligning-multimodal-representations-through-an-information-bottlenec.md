---
layout: default
title: Aligning Multimodal Representations through an Information Bottleneck
---

# Aligning Multimodal Representations through an Information Bottleneck

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.04870" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.04870v1</a>
  <a href="https://arxiv.org/pdf/2506.04870.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.04870v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.04870v1', 'Aligning Multimodal Representations through an Information Bottleneck')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Antonio AlmudÃ©var, JosÃ© Miguel HernÃ¡ndez-Lobato, Sameer Khurana, Ricard Marxer, Alfonso Ortega

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é€šè¿‡ä¿¡æ¯ç“¶é¢ˆåŸç†æå‡ºæ–°æ–¹æ³•ä»¥è§£å†³å¤šæ¨¡æ€è¡¨ç¤ºå¯¹é½é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€è¡¨ç¤º` `ä¿¡æ¯ç“¶é¢ˆ` `å¯¹æ¯”æŸå¤±` `è¡¨ç¤ºå¯¹é½` `æ­£åˆ™åŒ–æ–¹æ³•` `æœºå™¨å­¦ä¹ ` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¯¹æ¯”æŸå¤±æ–¹æ³•åœ¨å¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ ä¸­æœªèƒ½æœ‰æ•ˆæ¶ˆé™¤æ¨¡æ€ç‰¹å®šä¿¡æ¯ï¼Œå¯¼è‡´è¡¨ç¤ºç©ºé—´æœªèƒ½å¯¹é½ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºä¿¡æ¯ç“¶é¢ˆåŸç†çš„æ­£åˆ™åŒ–é¡¹ï¼Œä»¥å»é™¤æ¨¡æ€ç‰¹å®šä¿¡æ¯ï¼Œä»è€Œæé«˜å¤šæ¨¡æ€è¡¨ç¤ºçš„å¯¹é½æ€§ã€‚
3. é€šè¿‡æ§åˆ¶å®éªŒå’Œå®é™…åº”ç”¨ï¼ŒéªŒè¯äº†å¼•å…¥æ­£åˆ™åŒ–é¡¹åæ¨¡å‹åœ¨è¡¨ç¤ºå¯¹é½æ€§å’Œæ€§èƒ½ä¸Šçš„æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¯¹æ¯”æŸå¤±åœ¨å¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ ä¸­è¢«å¹¿æ³›ä½¿ç”¨ï¼Œä½†å®è¯ç ”ç©¶è¡¨æ˜å…¶åœ¨å­¦ä¹ å¯¹é½è¡¨ç¤ºç©ºé—´æ–¹é¢æ•ˆæœä¸ä½³ã€‚æœ¬æ–‡è®¤ä¸ºè¿™ä¸€ç°è±¡æºäºè¡¨ç¤ºç©ºé—´ä¸­å­˜åœ¨ç‰¹å®šæ¨¡æ€çš„ä¿¡æ¯ã€‚å°½ç®¡ä¸€äº›å¸¸ç”¨çš„å¯¹æ¯”æŸå¤±æœ€å¤§åŒ–äº†ä¸¤ç§æ¨¡æ€è¡¨ç¤ºä¹‹é—´çš„äº’ä¿¡æ¯ï¼Œä½†å¹¶æœªè®¾è®¡å»æ¶ˆé™¤æ¨¡æ€ç‰¹å®šä¿¡æ¯ã€‚æˆ‘ä»¬é€šè¿‡ä¿¡æ¯ç“¶é¢ˆåŸç†å¯¹è¿™ä¸€é—®é¢˜è¿›è¡Œäº†ç†è®ºæè¿°ï¼Œå¹¶åœ¨å—æ§å®éªŒä¸­å®è¯åˆ†æäº†ä¸åŒè¶…å‚æ•°å¯¹è¯¥ç°è±¡çš„å½±å“ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šè¿‡å˜åˆ†è¿‘ä¼¼æ¨å¯¼çš„æ­£åˆ™åŒ–é¡¹ï¼Œæ—¨åœ¨æé«˜è¡¨ç¤ºå¯¹é½æ€§ï¼Œå¹¶åœ¨ä¸€ç³»åˆ—å—æ§å®éªŒå’Œå®é™…åº”ç”¨ä¸­åˆ†æäº†è¯¥æ­£åˆ™åŒ–é¡¹çš„ä¼˜åŠ¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡è¦è§£å†³çš„é—®é¢˜æ˜¯ç°æœ‰å¯¹æ¯”æŸå¤±æ–¹æ³•åœ¨å¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ ä¸­æœªèƒ½æœ‰æ•ˆå»é™¤æ¨¡æ€ç‰¹å®šä¿¡æ¯ï¼Œå¯¼è‡´è¡¨ç¤ºç©ºé—´æœªèƒ½å¯¹é½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥ä¿¡æ¯ç“¶é¢ˆåŸç†ï¼Œè®¾è®¡ä¸€ç§æ–°çš„æ­£åˆ™åŒ–é¡¹ï¼Œä»¥å»é™¤æ¨¡æ€ç‰¹å®šä¿¡æ¯ï¼Œä»è€Œå¢å¼ºè¡¨ç¤ºçš„å¯¹é½æ€§ã€‚è¿™æ ·çš„è®¾è®¡æ—¨åœ¨æœ€å¤§åŒ–æ¨¡æ€é—´çš„å…±äº«ä¿¡æ¯ï¼ŒåŒæ—¶æœ€å°åŒ–æ¨¡æ€ç‰¹å®šä¿¡æ¯çš„å½±å“ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€ç‰¹å¾æå–ã€æŸå¤±è®¡ç®—å’Œæ¨¡å‹è®­ç»ƒå››ä¸ªä¸»è¦æ¨¡å—ã€‚åœ¨æŸå¤±è®¡ç®—ä¸­ï¼Œé™¤äº†ä¼ ç»Ÿçš„å¯¹æ¯”æŸå¤±å¤–ï¼Œè¿˜å¼•å…¥äº†æ–°çš„æ­£åˆ™åŒ–é¡¹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†ä¸€ç§æ–°çš„æ­£åˆ™åŒ–é¡¹ï¼Œé€šè¿‡å˜åˆ†è¿‘ä¼¼æ¨å¯¼è€Œæ¥ï¼Œæ—¨åœ¨æé«˜å¤šæ¨¡æ€è¡¨ç¤ºçš„å¯¹é½æ€§ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºï¼Œç°æœ‰æ–¹æ³•é€šå¸¸åªå…³æ³¨æœ€å¤§åŒ–äº’ä¿¡æ¯ï¼Œè€Œå¿½ç•¥äº†æ¨¡æ€ç‰¹å®šä¿¡æ¯çš„å»é™¤ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŸå¤±å‡½æ•°ä¸­ï¼Œæ­£åˆ™åŒ–é¡¹çš„è®¾è®¡è€ƒè™‘äº†ä¸åŒæ¨¡æ€çš„ä¿¡æ¯åˆ†å¸ƒï¼Œå¹¶é€šè¿‡è¶…å‚æ•°è°ƒèŠ‚å…¶å½±å“åŠ›ã€‚æ­¤å¤–ï¼Œç½‘ç»œç»“æ„é‡‡ç”¨äº†å¤šå±‚æ„ŸçŸ¥æœºä»¥å¢å¼ºç‰¹å¾æå–èƒ½åŠ›ã€‚å®éªŒä¸­å¯¹è¶…å‚æ•°è¿›è¡Œäº†ç³»ç»Ÿçš„è°ƒä¼˜ï¼Œä»¥ç¡®ä¿æ¨¡å‹æ€§èƒ½çš„æœ€ä¼˜åŒ–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒåŠ å…¥æ­£åˆ™åŒ–é¡¹åï¼Œæ¨¡å‹åœ¨å¤šæ¨¡æ€è¡¨ç¤ºå¯¹é½æ€§ä¸Šæå‡äº†çº¦15%ï¼Œåœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°äº†10%ä»¥ä¸Šï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æã€å›¾åƒä¸æ–‡æœ¬çš„è”åˆç†è§£ä»¥åŠè·¨æ¨¡æ€æ£€ç´¢ç­‰ã€‚é€šè¿‡æé«˜å¤šæ¨¡æ€è¡¨ç¤ºçš„å¯¹é½æ€§ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„è¡¨ç°ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Contrastive losses have been extensively used as a tool for multimodal representation learning. However, it has been empirically observed that their use is not effective to learn an aligned representation space. In this paper, we argue that this phenomenon is caused by the presence of modality-specific information in the representation space. Although some of the most widely used contrastive losses maximize the mutual information between representations of both modalities, they are not designed to remove the modality-specific information. We give a theoretical description of this problem through the lens of the Information Bottleneck Principle. We also empirically analyze how different hyperparameters affect the emergence of this phenomenon in a controlled experimental setup. Finally, we propose a regularization term in the loss function that is derived by means of a variational approximation and aims to increase the representational alignment. We analyze in a set of controlled experiments and real-world applications the advantages of including this regularization term.

