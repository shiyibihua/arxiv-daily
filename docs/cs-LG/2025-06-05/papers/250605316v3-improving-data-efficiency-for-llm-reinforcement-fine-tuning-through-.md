---
layout: default
title: Improving Data Efficiency for LLM Reinforcement Fine-tuning Through Difficulty-targeted Online Data Selection and Rollout Replay
---

# Improving Data Efficiency for LLM Reinforcement Fine-tuning Through Difficulty-targeted Online Data Selection and Rollout Replay

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05316" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05316v3</a>
  <a href="https://arxiv.org/pdf/2506.05316.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05316v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05316v3', 'Improving Data Efficiency for LLM Reinforcement Fine-tuning Through Difficulty-targeted Online Data Selection and Rollout Replay')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yifan Sun, Jingyan Shen, Yibin Wang, Tianyu Chen, Zhendong Wang, Mingyuan Zhou, Huan Zhang

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05 (æ›´æ–°: 2025-10-28)

**å¤‡æ³¨**: Accepted at NeurIPS 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/ASTRAL-Group/data-efficient-llm-rl)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºéš¾åº¦é’ˆå¯¹çš„åœ¨çº¿æ•°æ®é€‰æ‹©ä¸å›æ”¾é‡æ”¾ä»¥æé«˜LLMå¼ºåŒ–å¾®è°ƒçš„æ•°æ®æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `æ•°æ®é€‰æ‹©` `å›æ”¾é‡æ”¾` `è‡ªé€‚åº”éš¾åº¦` `å¤§å‹è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„LLMå¼ºåŒ–å¾®è°ƒæ–¹æ³•åœ¨æ•°æ®æ•ˆç‡ä¸Šå­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œå¯¼è‡´èµ„æºæ¶ˆè€—é«˜ä¸”è®­ç»ƒæ—¶é—´é•¿ã€‚
2. æœ¬æ–‡æå‡ºçš„éš¾åº¦é’ˆå¯¹çš„åœ¨çº¿æ•°æ®é€‰æ‹©å’Œå›æ”¾é‡æ”¾æŠ€æœ¯ï¼Œæ—¨åœ¨é€šè¿‡è‡ªé€‚åº”éš¾åº¦å¼•å¯¼æ•°æ®é€‰æ‹©ï¼Œä¼˜åŒ–å­¦ä¹ ä¿¡å·çš„è·å–ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨6ä¸ªLLM-æ•°æ®é›†ç»„åˆä¸­å°†å¾®è°ƒæ—¶é—´å‡å°‘äº†23%è‡³62%ï¼Œä¸”æ€§èƒ½ä¸åŸGRPOç®—æ³•ç›¸å½“ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²æˆä¸ºå¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ‰æ•ˆæ–¹æ³•ï¼Œå°¤å…¶æ˜¯åœ¨å¢å¼ºæ¨ç†èƒ½åŠ›æ–¹é¢ã€‚ç„¶è€Œï¼ŒRLå¾®è°ƒä»ç„¶é«˜åº¦ä¾èµ–èµ„æºï¼Œç°æœ‰ç ”ç©¶åœ¨æ•°æ®æ•ˆç‡é—®é¢˜ä¸Šå…³æ³¨ä¸è¶³ã€‚æœ¬æ–‡æå‡ºä¸¤ç§æŠ€æœ¯ä»¥æé«˜LLM RLå¾®è°ƒçš„æ•°æ®æ•ˆç‡ï¼šéš¾åº¦é’ˆå¯¹çš„åœ¨çº¿æ•°æ®é€‰æ‹©å’Œå›æ”¾é‡æ”¾ã€‚æˆ‘ä»¬å¼•å…¥è‡ªé€‚åº”éš¾åº¦çš„æ¦‚å¿µæ¥æŒ‡å¯¼åœ¨çº¿æ•°æ®é€‰æ‹©ï¼Œä¼˜å…ˆé€‰æ‹©ä¸­ç­‰éš¾åº¦çš„é—®é¢˜ï¼Œä»¥è·å–æ›´å…·ä¿¡æ¯é‡çš„å­¦ä¹ ä¿¡å·ã€‚ä¸ºé«˜æ•ˆä¼°è®¡è‡ªé€‚åº”éš¾åº¦ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§åŸºäºæ³¨æ„åŠ›çš„æ¡†æ¶ï¼Œä»…éœ€å¯¹å°å‹å‚è€ƒé—®é¢˜é›†è¿›è¡Œå›æ”¾ã€‚å‰©ä½™é—®é¢˜çš„è‡ªé€‚åº”éš¾åº¦åˆ™åŸºäºä¸è¯¥é›†çš„ç›¸ä¼¼æ€§è¿›è¡Œä¼°ç®—ã€‚ä¸ºè¿›ä¸€æ­¥é™ä½å›æ”¾æˆæœ¬ï¼Œæˆ‘ä»¬å¼•å…¥äº†å—ä¼ ç»ŸRLä¸­ç»éªŒå›æ”¾å¯å‘çš„å›æ”¾é‡æ”¾æœºåˆ¶ï¼Œé‡ç”¨è¿‘æœŸçš„å›æ”¾ï¼Œé™ä½æ¯æ­¥è®¡ç®—é‡ï¼ŒåŒæ—¶ä¿æŒç¨³å®šæ›´æ–°ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨6ä¸ªLLM-æ•°æ®é›†ç»„åˆä¸­å°†RLå¾®è°ƒæ—¶é—´å‡å°‘äº†23%è‡³62%ï¼ŒåŒæ—¶è¾¾åˆ°ä¸åŸGRPOç®—æ³•ç›¸åŒçš„æ€§èƒ½æ°´å¹³ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³LLMå¼ºåŒ–å¾®è°ƒè¿‡ç¨‹ä¸­çš„æ•°æ®æ•ˆç‡ä½ä¸‹é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€å¿½è§†äº†å¦‚ä½•æœ‰æ•ˆé€‰æ‹©æ•°æ®ï¼Œå¯¼è‡´èµ„æºæµªè´¹å’Œè®­ç»ƒæ—¶é—´å»¶é•¿ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé€šè¿‡è‡ªé€‚åº”éš¾åº¦æ¥æŒ‡å¯¼åœ¨çº¿æ•°æ®é€‰æ‹©ï¼Œä¼˜å…ˆé€‰æ‹©ä¸­ç­‰éš¾åº¦çš„é—®é¢˜ï¼Œä»¥è·å–æ›´å…·ä¿¡æ¯é‡çš„å­¦ä¹ ä¿¡å·ã€‚åŒæ—¶ï¼Œé‡‡ç”¨å›æ”¾é‡æ”¾æœºåˆ¶ä»¥é™ä½è®¡ç®—æˆæœ¬ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼š1ï¼‰éš¾åº¦é’ˆå¯¹çš„åœ¨çº¿æ•°æ®é€‰æ‹©æ¨¡å—ï¼Œé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶ä¼°è®¡é—®é¢˜çš„è‡ªé€‚åº”éš¾åº¦ï¼›2ï¼‰å›æ”¾é‡æ”¾æ¨¡å—ï¼Œé‡ç”¨è¿‘æœŸçš„å›æ”¾ä»¥é™ä½æ¯æ­¥è®¡ç®—é‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥è‡ªé€‚åº”éš¾åº¦çš„æ¦‚å¿µå’Œå›æ”¾é‡æ”¾æœºåˆ¶ï¼Œè¿™ä¸ä¼ ç»Ÿçš„RLå¾®è°ƒæ–¹æ³•åœ¨æ•°æ®é€‰æ‹©å’Œè®¡ç®—æ•ˆç‡ä¸Šæœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è‡ªé€‚åº”éš¾åº¦ä¼°è®¡ä¸­ï¼Œä½¿ç”¨å°å‹å‚è€ƒé›†è¿›è¡Œå›æ”¾ï¼Œå‰©ä½™é—®é¢˜çš„éš¾åº¦é€šè¿‡ä¸å‚è€ƒé›†çš„ç›¸ä¼¼æ€§è¿›è¡Œä¼°ç®—ã€‚å›æ”¾é‡æ”¾æœºåˆ¶åˆ™é€šè¿‡é‡ç”¨è¿‘æœŸçš„å›æ”¾æ¥é™ä½è®¡ç®—å¼€é”€ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡åœ¨å®éªŒéƒ¨åˆ†è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨6ä¸ªLLM-æ•°æ®é›†ç»„åˆä¸­ï¼Œå¾®è°ƒæ—¶é—´å‡å°‘äº†23%è‡³62%ï¼ŒåŒæ—¶åœ¨æ€§èƒ½ä¸Šä¸åŸGRPOç®—æ³•æŒå¹³ï¼Œå±•ç¤ºäº†æ˜¾è‘—çš„æ•ˆç‡æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ™ºèƒ½é—®ç­”ç­‰ã€‚é€šè¿‡æé«˜LLMçš„å¾®è°ƒæ•ˆç‡ï¼Œå¯ä»¥åŠ é€Ÿæ¨¡å‹çš„å¼€å‘ä¸éƒ¨ç½²ï¼Œé™ä½è®¡ç®—èµ„æºçš„éœ€æ±‚ï¼Œä»è€Œåœ¨å®é™…åº”ç”¨ä¸­å…·æœ‰é‡è¦çš„ç»æµä»·å€¼å’Œç¤¾ä¼šå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement learning (RL) has become an effective approach for fine-tuning large language models (LLMs), particularly to enhance their reasoning capabilities. However, RL fine-tuning remains highly resource-intensive, and existing work has largely overlooked the problem of data efficiency. In this paper, we propose two techniques to improve data efficiency in LLM RL fine-tuning: difficulty-targeted online data selection and rollout replay. We introduce the notion of adaptive difficulty to guide online data selection, prioritizing questions of moderate difficulty that are more likely to yield informative learning signals. To estimate adaptive difficulty efficiently, we develop an attention-based framework that requires rollouts for only a small reference set of questions. The adaptive difficulty of the remaining questions is then estimated based on their similarity to this set. To further reduce rollout cost, we introduce a rollout replay mechanism inspired by experience replay in traditional RL. This technique reuses recent rollouts, lowering per-step computation while maintaining stable updates. Experiments across 6 LLM-dataset combinations show that our method reduces RL fine-tuning time by 23% to 62% while reaching the same level of performance as the original GRPO algorithm. Our code is available at https://github.com/ASTRAL-Group/data-efficient-llm-rl.

