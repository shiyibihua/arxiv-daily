---
layout: default
title: Sample Complexity and Representation Ability of Test-time Scaling Paradigms
---

# Sample Complexity and Representation Ability of Test-time Scaling Paradigms

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05295" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05295v2</a>
  <a href="https://arxiv.org/pdf/2506.05295.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05295v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05295v2', 'Sample Complexity and Representation Ability of Test-time Scaling Paradigms')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Baihe Huang, Shanda Li, Tianhao Wu, Yiming Yang, Ameet Talwalkar, Kannan Ramchandran, Michael I. Jordan, Jiantao Jiao

**åˆ†ç±»**: cs.LG, cs.AI, stat.ML

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05 (æ›´æ–°: 2025-06-12)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæµ‹è¯•æ—¶ç¼©æ”¾èŒƒå¼ä»¥æå‡å¤§è¯­è¨€æ¨¡å‹çš„æ ·æœ¬æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ ·æœ¬æ•ˆç‡` `æµ‹è¯•æ—¶ç¼©æ”¾` `è‡ªä¸€è‡´æ€§` `æœ€ä½³é€‰æ‹©` `è‡ªæˆ‘ä¿®æ­£` `å¤šä»»åŠ¡å­¦ä¹ ` `Transformer`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æµ‹è¯•æ—¶ç­–ç•¥åœ¨æ ·æœ¬æ•ˆç‡æ–¹é¢çš„ç†è®ºç†è§£ä¸è¶³ï¼Œé™åˆ¶äº†å…¶åº”ç”¨æ•ˆæœã€‚
2. è®ºæ–‡é€šè¿‡å»ºç«‹è‡ªä¸€è‡´æ€§å’Œæœ€ä½³é€‰æ‹©çš„æ ·æœ¬éœ€æ±‚å·®å¼‚ï¼Œæå‡ºäº†æ›´é«˜æ•ˆçš„æµ‹è¯•æ—¶ç­–ç•¥ã€‚
3. å®éªŒè¯æ˜è‡ªæˆ‘ä¿®æ­£æ–¹æ³•åœ¨å¤šä»»åŠ¡å¤„ç†ä¸­çš„æœ‰æ•ˆæ€§ï¼Œæ˜¾è‘—æå‡äº†Transformerçš„è¡¨ç°ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æµ‹è¯•æ—¶ç¼©æ”¾èŒƒå¼æ˜¾è‘—æå‡äº†å¤§è¯­è¨€æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸Šçš„èƒ½åŠ›ã€‚å°½ç®¡å…¶åœ¨å®è·µä¸­çš„æˆåŠŸæ˜¾è‘—ï¼Œä½†å¯¹å„ç§æµ‹è¯•æ—¶ç­–ç•¥ï¼ˆå¦‚è‡ªä¸€è‡´æ€§ã€æœ€ä½³é€‰æ‹©å’Œè‡ªæˆ‘ä¿®æ­£ï¼‰çš„æ ·æœ¬æ•ˆç‡çš„ç†è®ºç†è§£ä»ç„¶æœ‰é™ã€‚æœ¬æ–‡é¦–å…ˆå»ºç«‹äº†ä¸¤ä¸ªé‡å¤é‡‡æ ·ç­–ç•¥ä¹‹é—´çš„åˆ†ç¦»ç»“æœï¼šè‡ªä¸€è‡´æ€§éœ€è¦Î˜(1/Î”Â²)ä¸ªæ ·æœ¬æ‰èƒ½äº§ç”Ÿæ­£ç¡®ç­”æ¡ˆï¼Œè€Œæœ€ä½³é€‰æ‹©åªéœ€Î˜(1/Î”)ï¼Œå…¶ä¸­Î”<1è¡¨ç¤ºæ­£ç¡®ç­”æ¡ˆä¸ç¬¬äºŒå¯èƒ½ç­”æ¡ˆä¹‹é—´çš„æ¦‚ç‡å·®è·ã€‚æ¥ç€ï¼Œæˆ‘ä»¬å±•ç¤ºäº†è‡ªæˆ‘ä¿®æ­£æ–¹æ³•åœ¨éªŒè¯åé¦ˆä¸‹çš„è¡¨ç°åŠ›ç»“æœï¼šå®ƒä½¿å¾—Transformerèƒ½å¤Ÿåœ¨æµ‹è¯•æ—¶æ¨¡æ‹Ÿåœ¨çº¿å­¦ä¹ ï¼Œä»è€Œåœ¨æ²¡æœ‰ç‰¹å®šä»»åŠ¡çŸ¥è¯†çš„æƒ…å†µä¸‹è§£å†³å¤šä¸ªä»»åŠ¡ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡å®éªŒè¯å®äº†ç†è®ºç»“æœï¼Œå±•ç¤ºäº†è‡ªæˆ‘ä¿®æ­£æ–¹æ³•çš„å®é™…æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æµ‹è¯•æ—¶ç¼©æ”¾èŒƒå¼åœ¨æ ·æœ¬æ•ˆç‡ä¸Šçš„ç†è®ºç†è§£ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°å—é™äºæ ·æœ¬éœ€æ±‚é«˜çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æ¯”è¾ƒä¸åŒçš„é‡å¤é‡‡æ ·ç­–ç•¥ï¼Œæå‡ºæœ€ä½³é€‰æ‹©ç­–ç•¥åœ¨æ ·æœ¬éœ€æ±‚ä¸Šæ›´ä¸ºé«˜æ•ˆï¼ŒåŒæ—¶å¼•å…¥è‡ªæˆ‘ä¿®æ­£æ–¹æ³•ï¼Œä½¿å¾—Transformerèƒ½å¤Ÿåœ¨æµ‹è¯•æ—¶å¤„ç†å¤šä»»åŠ¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬è‡ªä¸€è‡´æ€§ã€æœ€ä½³é€‰æ‹©å’Œè‡ªæˆ‘ä¿®æ­£ä¸‰ç§ç­–ç•¥ï¼Œé‡ç‚¹åœ¨äºå¦‚ä½•é€šè¿‡éªŒè¯åé¦ˆå®ç°åœ¨çº¿å­¦ä¹ ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå°†Transformerçš„è¡¨ç¤ºèƒ½åŠ›ä»å•ä»»åŠ¡æ‰©å±•åˆ°å¤šä»»åŠ¡è®¾ç½®ï¼Œå…è®¸å…¶åœ¨æ²¡æœ‰ç‰¹å®šä»»åŠ¡çŸ¥è¯†çš„æƒ…å†µä¸‹è§£å†³å¤šä¸ªä»»åŠ¡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è‡ªæˆ‘ä¿®æ­£æ–¹æ³•ä¸­ï¼Œè®¾è®¡äº†ç‰¹å®šçš„éªŒè¯åé¦ˆæœºåˆ¶ï¼Œç¡®ä¿Transformerèƒ½å¤Ÿæœ‰æ•ˆåœ°ä»å¤šä¸ªä¸“å®¶ä¸­å­¦ä¹ ï¼ŒåŒæ—¶ä¼˜åŒ–äº†æ ·æœ¬çš„ä½¿ç”¨æ•ˆç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè‡ªæˆ‘ä¿®æ­£æ–¹æ³•åœ¨å¤šä¸ªä»»åŠ¡ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„è‡ªä¸€è‡´æ€§å’Œæœ€ä½³é€‰æ‹©ç­–ç•¥ï¼Œæ ·æœ¬éœ€æ±‚å‡å°‘äº†çº¦50%ï¼Œæœ‰æ•ˆæå‡äº†æ¨¡å‹çš„æ•´ä½“æ€§èƒ½ï¼ŒéªŒè¯äº†ç†è®ºåˆ†æçš„æ­£ç¡®æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œå¤šä»»åŠ¡å­¦ä¹ ç­‰ã€‚é€šè¿‡æå‡å¤§è¯­è¨€æ¨¡å‹çš„æ ·æœ¬æ•ˆç‡ï¼Œèƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­æ›´å¿«é€Ÿåœ°é€‚åº”ä¸åŒä»»åŠ¡ï¼Œé™ä½è®¡ç®—èµ„æºæ¶ˆè€—ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Test-time scaling paradigms have significantly advanced the capabilities of large language models (LLMs) on complex tasks. Despite their empirical success, theoretical understanding of the sample efficiency of various test-time strategies -- such as self-consistency, best-of-$n$, and self-correction -- remains limited. In this work, we first establish a separation result between two repeated sampling strategies: self-consistency requires $Î˜(1/Î”^2)$ samples to produce the correct answer, while best-of-$n$ only needs $Î˜(1/Î”)$, where $Î”< 1$ denotes the probability gap between the correct and second most likely answers. Next, we present an expressiveness result for the self-correction approach with verifier feedback: it enables Transformers to simulate online learning over a pool of experts at test time. Therefore, a single Transformer architecture can provably solve multiple tasks without prior knowledge of the specific task associated with a user query, extending the representation theory of Transformers from single-task to multi-task settings. Finally, we empirically validate our theoretical results, demonstrating the practical effectiveness of self-correction methods.

