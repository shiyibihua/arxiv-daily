---
layout: default
title: TabFlex: Scaling Tabular Learning to Millions with Linear Attention
---

# TabFlex: Scaling Tabular Learning to Millions with Linear Attention

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05584" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05584v1</a>
  <a href="https://arxiv.org/pdf/2506.05584.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05584v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05584v1', 'TabFlex: Scaling Tabular Learning to Millions with Linear Attention')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuchen Zeng, Tuan Dinh, Wonjun Kang, Andreas C Mueller

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05

**å¤‡æ³¨**: 30 pages, ICML 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTabFlexä»¥è§£å†³å¤§è§„æ¨¡è¡¨æ ¼å­¦ä¹ æ•ˆç‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¡¨æ ¼å­¦ä¹ ` `çº¿æ€§æ³¨æ„åŠ›` `å¤§è§„æ¨¡æ•°æ®` `æœºå™¨å­¦ä¹ ` `æ•°æ®å¤„ç†` `æ¨¡å‹ä¼˜åŒ–` `æ•ˆç‡æå‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤§è§„æ¨¡å¤æ‚è¡¨æ ¼æ•°æ®é›†æ—¶æ•ˆç‡ä½ä¸‹ï¼Œéš¾ä»¥æ»¡è¶³å®é™…åº”ç”¨éœ€æ±‚ã€‚
2. è®ºæ–‡æå‡ºçš„TabFlexé€šè¿‡å¼•å…¥çº¿æ€§æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ˜¾è‘—æå‡äº†å¤„ç†å¤§è§„æ¨¡è¡¨æ ¼æ•°æ®é›†çš„æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTabFlexåœ¨æ•ˆç‡ä¸Šç›¸æ¯”TabPFNæå‡è¶…è¿‡2å€ï¼Œä¸”åœ¨å¤šç§æ•°æ®é›†ä¸Šè¶…è¶Šäº†25ä¸ªåŸºçº¿æ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›è¿›è¡Œè¡¨æ ¼åˆ†ç±»å·²å—åˆ°å¹¿æ³›å…³æ³¨ï¼Œå°¤å…¶æ˜¯åœ¨æ— éœ€è®­ç»ƒçš„æƒ…å†µä¸‹é€‚åº”ä¸åŒæ•°æ®é›†ã€‚å°½ç®¡è¿‘æœŸçš„TabPFNåœ¨å°è§„æ¨¡è¡¨æ ¼æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤„ç†å¤§è§„æ¨¡å¤æ‚æ•°æ®é›†æ—¶å´é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡é€šè¿‡å¼•å…¥çº¿æ€§æ³¨æ„åŠ›æœºåˆ¶ï¼Œæå‡äº†TabPFNåœ¨å¤§æ•°æ®é›†ä¸Šçš„æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚æˆ‘ä»¬çš„æ¨¡å‹TabFlexèƒ½å¤Ÿé«˜æ•ˆå¤„ç†å…·æœ‰æ•°åƒä¸ªç‰¹å¾å’Œæ•°ç™¾ä¸ªç±»åˆ«çš„è¡¨æ ¼æ•°æ®é›†ï¼Œèƒ½å¤Ÿæ— ç¼æ‰©å±•åˆ°æ•°ç™¾ä¸‡ä¸ªæ ·æœ¬ã€‚ä¾‹å¦‚ï¼ŒTabFlexåœ¨5ç§’å†…å¤„ç†è¶…è¿‡ä¸€ç™¾ä¸‡æ ·æœ¬çš„æ‰‘å…‹æ‰‹æ•°æ®é›†ã€‚æˆ‘ä»¬çš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒTabFlexåœ¨æ•ˆç‡ä¸Šç›¸æ¯”TabPFNæå‡è¶…è¿‡2å€ï¼Œç›¸æ¯”XGBoostæå‡1.5å€ï¼Œå¹¶åœ¨å¤šç§æ•°æ®é›†ä¸Šè¶…è¶Šäº†25ä¸ªåŸºçº¿æ¨¡å‹ã€‚æ­¤å¤–ï¼ŒTabFlexåœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šä»ç„¶è¡¨ç°å‡ºè‰²ï¼Œç»“åˆé™ç»´å’Œæ•°æ®é‡‡æ ·ç­‰æ•°æ®é«˜æ•ˆæŠ€æœ¯æ—¶ï¼Œæ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è¡¨æ ¼å­¦ä¹ æ–¹æ³•åœ¨å¤„ç†å¤§è§„æ¨¡å¤æ‚æ•°æ®é›†æ—¶çš„æ•ˆç‡é—®é¢˜ï¼Œå°¤å…¶æ˜¯TabPFNåœ¨æ­¤ç±»æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥çº¿æ€§æ³¨æ„åŠ›æœºåˆ¶ï¼ŒTabFlexèƒ½å¤Ÿåœ¨ä¿æŒæ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½è®¡ç®—å¤æ‚åº¦ï¼Œä»è€Œæé«˜å¤„ç†é€Ÿåº¦å’Œå¯æ‰©å±•æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTabFlexçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€ç‰¹å¾æå–ã€çº¿æ€§æ³¨æ„åŠ›æœºåˆ¶åº”ç”¨å’Œæœ€ç»ˆåˆ†ç±»æ¨¡å—ã€‚æ¯ä¸ªæ¨¡å—ç›¸äº’åä½œï¼Œä»¥å®ç°é«˜æ•ˆçš„æ•°æ®å¤„ç†å’Œåˆ†ç±»ã€‚

**å…³é”®åˆ›æ–°**ï¼šTabFlexçš„ä¸»è¦åˆ›æ–°åœ¨äºé‡‡ç”¨çº¿æ€§æ³¨æ„åŠ›æœºåˆ¶æ›¿ä»£ä¼ ç»Ÿçš„å¤æ‚åº¦ä¸ºå¹³æ–¹çº§åˆ«çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œä»è€Œå®ç°äº†åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šçš„é«˜æ•ˆå¤„ç†ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼ŒTabFlexä½¿ç”¨äº†ä¼˜åŒ–çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ï¼Œç¡®ä¿åœ¨å¤„ç†æ•°åƒç‰¹å¾å’Œæ•°ç™¾ç±»åˆ«æ—¶ï¼Œèƒ½å¤Ÿä¿æŒé«˜æ•ˆçš„è®¡ç®—å’Œå‡†ç¡®çš„åˆ†ç±»ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒTabFlexåœ¨å¤„ç†æ•ˆç‡ä¸Šç›¸æ¯”TabPFNæå‡è¶…è¿‡2å€ï¼Œä¸”åœ¨ä¸XGBoostçš„æ¯”è¾ƒä¸­æå‡1.5å€ã€‚æ­¤å¤–ï¼ŒTabFlexåœ¨25ä¸ªåŸºçº¿æ¨¡å‹ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå±•ç°å‡ºå¼ºå¤§çš„å¤„ç†èƒ½åŠ›å’Œé«˜æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

TabFlexçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼ŒåŒ…æ‹¬é‡‘èé£æ§ã€åŒ»ç–—æ•°æ®åˆ†æå’Œå¸‚åœºè¥é”€ç­‰ã€‚å…¶é«˜æ•ˆçš„å¤„ç†èƒ½åŠ›å’Œé€‚åº”æ€§ä½¿å…¶èƒ½å¤Ÿåœ¨å¤§è§„æ¨¡æ•°æ®ç¯å¢ƒä¸­å¿«é€Ÿå“åº”ï¼Œæå‡å†³ç­–æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚æœªæ¥ï¼ŒTabFlexå¯èƒ½ä¼šæ¨åŠ¨æ›´å¤šåŸºäºè¡¨æ ¼æ•°æ®çš„æ™ºèƒ½åº”ç”¨çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Leveraging the in-context learning (ICL) capability of Large Language Models (LLMs) for tabular classification has gained significant attention for its training-free adaptability across diverse datasets. Recent advancements, like TabPFN, excel in small-scale tabular datasets but struggle to scale for large and complex datasets. Our work enhances the efficiency and scalability of TabPFN for larger datasets by incorporating linear attention mechanisms as a scalable alternative to complexity-quadratic self-attention. Our model, TabFlex, efficiently handles tabular datasets with thousands of features and hundreds of classes, scaling seamlessly to millions of samples. For instance, TabFlex processes the poker-hand dataset with over a million samples in just 5 seconds. Our extensive evaluations demonstrate that TabFlex can achieve over a 2x speedup compared to TabPFN and a 1.5x speedup over XGBoost, outperforming 25 tested baselines in terms of efficiency across a diverse range of datasets. Furthermore, TabFlex remains highly effective on large-scale datasets, delivering strong performance with significantly reduced computational costs, especially when combined with data-efficient techniques such as dimensionality reduction and data sampling.

