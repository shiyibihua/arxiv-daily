---
layout: default
title: PCDVQ: Enhancing Vector Quantization for Large Language Models via Polar Coordinate Decoupling
---

# PCDVQ: Enhancing Vector Quantization for Large Language Models via Polar Coordinate Decoupling

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05432" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.05432v2</a>
  <a href="https://arxiv.org/pdf/2506.05432.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05432v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05432v2', 'PCDVQ: Enhancing Vector Quantization for Large Language Models via Polar Coordinate Decoupling')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Yuxuan Yue, Zukang Xu, Zhihang Yuan, Dawei Yang, Jianlong Wu, Liqiang Nie

**ÂàÜÁ±ª**: cs.LG, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-05 (Êõ¥Êñ∞: 2025-06-26)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫PCDVQ‰ª•Ëß£ÂÜ≥Â§ßËØ≠Ë®ÄÊ®°ÂûãÈáèÂåñÁ≤æÂ∫¶‰∏çË∂≥ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÂêëÈáèÈáèÂåñ` `ÊûÅÂùêÊ†áËß£ËÄ¶` `Â§ßËØ≠Ë®ÄÊ®°Âûã` `ËæπÁºòËÆ°ÁÆó` `Ê®°ÂûãÂéãÁº©` `Èõ∂-shotÂ≠¶‰π†` `ÂàÜÂ∏ÉÂØπÈΩê`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÂêëÈáèÈáèÂåñÊñπÊ≥ïÂú®Â§ÑÁêÜÂ§ßËØ≠Ë®ÄÊ®°ÂûãÊó∂ÔºåÂæÄÂæÄÊó†Ê≥ïÊúâÊïàÂπ≥Ë°°ÊñπÂêëÂíåÂπÖÂ∫¶ÁöÑÈáèÂåñÁ≤æÂ∫¶ÔºåÂØºËá¥ÊÄßËÉΩ‰∏ãÈôç„ÄÇ
2. Êú¨ÊñáÊèêÂá∫ÁöÑPCDVQÊñπÊ≥ïÈÄöËøáÊûÅÂùêÊ†áËß£ËÄ¶ÔºåÂ∞ÜÂêëÈáèÁöÑÊñπÂêëÂíåÂπÖÂ∫¶ËøõË°åÁã¨Á´ãÈáèÂåñÔºå‰ªéËÄåÊèêÈ´òÈáèÂåñÁ≤æÂ∫¶„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåPCDVQÂú®2ÊØîÁâπÈáèÂåñ‰∏ãÁöÑÈõ∂-shot‰ªªÂä°ÂáÜÁ°ÆÁéáÊØîÂü∫Á∫øÊñπÊ≥ïÊèêÈ´ò‰∫ÜËá≥Â∞ë1.5%ÔºåÂ±ïÁé∞‰∫ÜÂÖ∂ÊúâÊïàÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ËæπÁºòÈÉ®ÁΩ≤‰∏≠Èù¢‰∏¥ÂèÇÊï∞ËßÑÊ®°Â∫ûÂ§ßÁöÑÊåëÊàò„ÄÇÂêëÈáèÈáèÂåñÔºàVQÔºâ‰Ωú‰∏∫‰∏ÄÁßçËÅöÁ±ªÂü∫Á°ÄÁöÑÈáèÂåñÊñπÊ≥ïÔºåÂõ†ÂÖ∂ÊûÅ‰ΩéÁöÑÊØîÁâπÊï∞ÔºàÁîöËá≥ÂèØËææ2ÊØîÁâπÔºâÂíåËæÉÈ´òÁöÑÂáÜÁ°ÆÊÄßËÄåÂπøÊ≥õÂ∫îÁî®„ÄÇÁé∞ÊúâVQÊñπÊ≥ïÈÄöÂ∏∏‰ª•ËÄ¶ÂêàÁöÑÊñπÂºèÂØπÂêëÈáèËøõË°åÈáèÂåñÔºå‰ΩÜÁ†îÁ©∂ÂèëÁé∞ÔºåÊñπÂêëÂØπÈáèÂåñÁöÑÊïèÊÑüÊÄßÊòæËëóÈ´ò‰∫éÂπÖÂ∫¶„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜÊûÅÂùêÊ†áËß£ËÄ¶ÂêëÈáèÈáèÂåñÔºàPCDVQÔºâÔºåÈÄöËøáÊûÅÂùêÊ†áËß£ËÄ¶ÂíåÂàÜÂ∏ÉÂØπÈΩêÁ†ÅÊú¨ÊûÑÂª∫‰∏§‰∏™ÂÖ≥ÈîÆÊ®°ÂùóÔºåÂÆûÁé∞ÊñπÂêëÂíåÂπÖÂ∫¶ÁöÑÁã¨Á´ãÈáèÂåñ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåPCDVQÂú®2ÊØîÁâπÊ∞¥Âπ≥‰∏ãÁöÑÈõ∂-shotÂáÜÁ°ÆÁéáËá≥Â∞ëÊèêÈ´ò‰∫Ü1.5%„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâÂêëÈáèÈáèÂåñÊñπÊ≥ïÂú®Â§ßËØ≠Ë®ÄÊ®°Âûã‰∏≠ÈáèÂåñÁ≤æÂ∫¶‰∏çË∂≥ÁöÑÈóÆÈ¢òÔºåÂ∞§ÂÖ∂ÊòØÊñπÂêëÂíåÂπÖÂ∫¶ËÄ¶ÂêàÈáèÂåñÂØºËá¥ÁöÑÊÄßËÉΩ‰∏ãÈôç„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊèêÂá∫ÊûÅÂùêÊ†áËß£ËÄ¶ÁöÑÊÄùÊÉ≥ÔºåÂ∞ÜÂêëÈáèËΩ¨Êç¢‰∏∫ÊûÅÂùêÊ†áË°®Á§∫ÔºåÂàÜÂà´ÂØπÊñπÂêëÂíåÂπÖÂ∫¶ËøõË°åÁã¨Á´ãÈáèÂåñÔºå‰ª•ÂáèÂ∞ëÈáèÂåñËØØÂ∑Æ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöPCDVQÊ°ÜÊû∂ÂåÖÂê´‰∏§‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöÊûÅÂùêÊ†áËß£ËÄ¶ÔºàPCDÔºâÂíåÂàÜÂ∏ÉÂØπÈΩêÁ†ÅÊú¨ÊûÑÂª∫ÔºàDACCÔºâ„ÄÇPCDÊ®°ÂùóË¥üË¥£Â∞ÜÂêëÈáèËΩ¨Êç¢‰∏∫ÊûÅÂùêÊ†áÂΩ¢ÂºèÂπ∂ËøõË°åÁã¨Á´ãÈáèÂåñÔºåDACCÊ®°ÂùóÂàô‰ºòÂåñÊñπÂêëÂíåÂπÖÂ∫¶ÁöÑÁ†ÅÊú¨‰ª•Á¨¶ÂêàÊ∫êÂàÜÂ∏É„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞Âú®‰∫éÊèêÂá∫‰∫ÜÊûÅÂùêÊ†áËß£ËÄ¶ÁöÑÈáèÂåñÊñπÊ≥ïÔºåÁ™ÅÁ†¥‰∫Ü‰º†ÁªüVQÊñπÊ≥ïÁöÑËÄ¶ÂêàÈôêÂà∂ÔºåÊòæËëóÊèêÈ´ò‰∫ÜÈáèÂåñÁ≤æÂ∫¶„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆæËÆ°‰∏≠ÔºåÈááÁî®‰∫ÜÈÄÇÂ∫îÊÄßËÅöÁ±ª‰∏≠ÂøÉËÆæÁΩÆÔºåÁ°Æ‰øùÊñπÂêëÂíåÂπÖÂ∫¶ÁöÑÈáèÂåñËÉΩÂ§üÊúâÊïàÂèçÊò†Ê∫êÊï∞ÊçÆÁöÑÂàÜÂ∏ÉÁâπÂæÅÔºåÂêåÊó∂‰ºòÂåñ‰∫ÜÊçüÂ§±ÂáΩÊï∞‰ª•Âπ≥Ë°°ÊñπÂêëÂíåÂπÖÂ∫¶ÁöÑÈáèÂåñËØØÂ∑Æ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåPCDVQÂú®2ÊØîÁâπÈáèÂåñ‰∏ãÁöÑÈõ∂-shot‰ªªÂä°ÂáÜÁ°ÆÁéáÊØîÂü∫Á∫øÊñπÊ≥ïÊèêÈ´ò‰∫ÜËá≥Â∞ë1.5%„ÄÇËøô‰∏ÄÊèêÂçá‰∏ç‰ªÖÈ™åËØÅ‰∫ÜÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÔºå‰πü‰∏∫Â§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÈ´òÊïàÈÉ®ÁΩ≤Êèê‰æõ‰∫ÜÊñ∞ÁöÑÊÄùË∑Ø„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨ËæπÁºòËÆ°ÁÆó„ÄÅÁßªÂä®ËÆæÂ§á‰∏äÁöÑÂ§ßËØ≠Ë®ÄÊ®°ÂûãÈÉ®ÁΩ≤‰ª•ÂèäÂÆûÊó∂Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ‰ªªÂä°„ÄÇÈÄöËøáÊèêÈ´òÈáèÂåñÁ≤æÂ∫¶ÔºåPCDVQËÉΩÂ§üÂú®ËµÑÊ∫êÂèóÈôêÁöÑÁéØÂ¢É‰∏≠ÂÆûÁé∞È´òÊïàÁöÑÊ®°ÂûãÊé®ÁêÜÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Large Language Models (LLMs) face significant challenges in edge deployment due to their massive parameter scale. Vector Quantization (VQ), a clustering-based quantization method, serves as a prevalent solution to this issue for its extremely low-bit (even at 2-bit) and considerable accuracy. Since a vector is a quantity in mathematics and physics that has both direction and magnitude, existing VQ works typically quantize them in a coupled manner. However, we find that direction exhibits significantly greater sensitivity to quantization compared to the magnitude. For instance, when separately clustering the directions and magnitudes of weight vectors in LLaMA-2-7B, the accuracy drop of zero-shot tasks are 46.5\% and 2.3\%, respectively. This gap even increases with the reduction of clustering centers. Further, Euclidean distance, a common metric to access vector similarities in current VQ works, places greater emphasis on reducing the magnitude error. This property is contrary to the above finding, unavoidably leading to larger quantization errors. To these ends, this paper proposes Polar Coordinate Decoupled Vector Quantization (PCDVQ), an effective and efficient VQ framework consisting of two key modules: 1) Polar Coordinate Decoupling (PCD), which transforms vectors into their polar coordinate representations and perform independent quantization of the direction and magnitude parameters.2) Distribution Aligned Codebook Construction (DACC), which optimizes the direction and magnitude codebooks in accordance with the source distribution. Experimental results show that PCDVQ outperforms baseline methods at 2-bit level by at least 1.5\% zero-shot accuracy, establishing a novel paradigm for accurate and highly compressed LLMs.

