---
layout: default
title: Towards Better Generalization via Distributional Input Projection Network
---

# Towards Better Generalization via Distributional Input Projection Network

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.04690" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.04690v2</a>
  <a href="https://arxiv.org/pdf/2506.04690.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.04690v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.04690v2', 'Towards Better Generalization via Distributional Input Projection Network')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yifan Hao, Yanxin Lu, Hanning Zhang, Xinwei Shen, Tong Zhang

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05 (æ›´æ–°: 2025-09-29)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåˆ†å¸ƒå¼è¾“å…¥æŠ•å½±ç½‘ç»œä»¥æå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ¨¡å‹æ³›åŒ–` `æ·±åº¦å­¦ä¹ ` `è¾“å…¥æŠ•å½±` `å¯¹æŠ—æ”»å‡»` `åˆ†å¸ƒå¤–è¾“å…¥` `æŸå¤±å‡½æ•°` `ç¥ç»ç½‘ç»œ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤„ç†è¿‡å‚æ•°åŒ–æ¨¡å‹æ—¶ï¼Œè®­ç»ƒæŸå¤±å¯¹æ³›åŒ–æ€§èƒ½çš„æŒ‡ç¤ºæœ‰é™ï¼Œä¸”ç›´æ¥å¼ºåˆ¶å¹³æ»‘æ€§å­˜åœ¨å›°éš¾ã€‚
2. æœ¬æ–‡æå‡ºDIPNetï¼Œé€šè¿‡å°†è¾“å…¥æŠ•å½±ä¸ºå¯å­¦ä¹ çš„åˆ†å¸ƒï¼Œæ¥å®ç°æ›´å¹³æ»‘çš„æŸå¤±æ™¯è§‚ï¼Œä»è€Œæå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒDIPNetåœ¨å¤šç§æ¨¡å‹æ¶æ„ä¸Šå‡èƒ½æå‡æµ‹è¯•æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨å¯¹æŠ—æ”»å‡»å’Œåˆ†å¸ƒå¤–è¾“å…¥çš„æƒ…å†µä¸‹è¡¨ç°ä¼˜å¼‚ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€è¿‡å‚æ•°åŒ–æ¨¡å‹çš„æ™®éåº”ç”¨ï¼Œä»…ä¾èµ–è®­ç»ƒæŸå¤±å¯¹æ³›åŒ–æ€§èƒ½çš„è¯„ä¼°å·²æ˜¾ä¸è¶³ã€‚å¹³æ»‘æ€§ä¸æ³›åŒ–èƒ½åŠ›çš„æå‡æœ‰å…³ï¼Œä½†åœ¨ç¥ç»ç½‘ç»œä¸­ç›´æ¥å¼ºåˆ¶å¹³æ»‘æ€§ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†åˆ†å¸ƒå¼è¾“å…¥æŠ•å½±ç½‘ç»œï¼ˆDIPNetï¼‰ï¼Œè¯¥æ¡†æ¶åœ¨æ¯ä¸€å±‚å°†è¾“å…¥æŠ•å½±ä¸ºå¯å­¦ä¹ çš„åˆ†å¸ƒã€‚è¿™ç§åˆ†å¸ƒè¡¨ç¤ºä½¿å¾—æŸå¤±å‡½æ•°åœ¨è¾“å…¥æ–¹é¢æ›´åŠ å¹³æ»‘ï¼Œä»è€Œä¿ƒè¿›æ›´å¥½çš„æ³›åŒ–ã€‚ç†è®ºåˆ†æè¡¨æ˜ï¼ŒDIPNeté™ä½äº†å±€éƒ¨å¹³æ»‘æ€§åº¦é‡å’Œç½‘ç»œçš„Lipschitzå¸¸æ•°ï¼Œè¿›è€Œæå‡äº†æ³›åŒ–æ€§èƒ½ã€‚é€šè¿‡åœ¨å¤šç§æ¶æ„å’Œä»»åŠ¡ä¸Šè¿›è¡Œå®è¯éªŒè¯ï¼ŒDIPNetåœ¨æ ‡å‡†è®¾ç½®ã€å¯¹æŠ—æ”»å‡»ã€åˆ†å¸ƒå¤–è¾“å…¥å’Œæ¨ç†åŸºå‡†æµ‹è¯•ä¸­å‡æ˜¾è‘—æé«˜äº†æµ‹è¯•æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è¿‡å‚æ•°åŒ–æ¨¡å‹æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨è®­ç»ƒæŸå¤±ä¸æ³›åŒ–æ€§èƒ½ä¹‹é—´çš„å…³è”æ€§è¾ƒå¼±ï¼Œä¸”éš¾ä»¥ç›´æ¥å®ç°å¹³æ»‘æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDIPNetçš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†è¾“å…¥æ˜ å°„ä¸ºå¯å­¦ä¹ çš„åˆ†å¸ƒï¼Œè¿™ç§æ–¹æ³•é€šè¿‡å¼•å…¥åˆ†å¸ƒè¡¨ç¤ºæ¥å¹³æ»‘æŸå¤±å‡½æ•°ï¼Œä»è€Œæ”¹å–„æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDIPNetçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªå±‚æ¬¡ï¼Œæ¯ä¸€å±‚éƒ½å°†è¾“å…¥æŠ•å½±ä¸ºä¸€ä¸ªåˆ†å¸ƒã€‚è¯¥æ¡†æ¶é€šè¿‡å­¦ä¹ è¿™äº›åˆ†å¸ƒæ¥ä¼˜åŒ–æŸå¤±å‡½æ•°ï¼Œä½¿å¾—æŸå¤±æ™¯è§‚æ›´åŠ å¹³æ»‘ã€‚

**å…³é”®åˆ›æ–°**ï¼šDIPNetçš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥äº†åˆ†å¸ƒå¼è¾“å…¥æŠ•å½±çš„æ¦‚å¿µï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œå®ƒé€šè¿‡å­¦ä¹ åˆ†å¸ƒæ¥é™ä½Lipschitzå¸¸æ•°ï¼Œä»è€Œæœ‰æ•ˆæå‡æ³›åŒ–æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒDIPNeté‡‡ç”¨äº†å¯å­¦ä¹ çš„åˆ†å¸ƒå‚æ•°ï¼Œå¹¶åœ¨æŸå¤±å‡½æ•°ä¸­å¼•å…¥äº†å¹³æ»‘æ€§çº¦æŸï¼Œç¡®ä¿ç½‘ç»œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿè‡ªé€‚åº”è°ƒæ•´è¾“å…¥çš„åˆ†å¸ƒç‰¹æ€§ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’ŒæŸå¤±å‡½æ•°è®¾è®¡ç»†èŠ‚åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†é˜è¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDIPNetåœ¨å¤šç§æ¨¡å‹æ¶æ„ä¸Šå‡æ˜¾è‘—æå‡äº†æµ‹è¯•æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨å¯¹æŠ—æ”»å‡»å’Œåˆ†å¸ƒå¤–è¾“å…¥çš„æƒ…å†µä¸‹ï¼ŒDIPNetçš„è¡¨ç°ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œæå‡å¹…åº¦è¾¾åˆ°10%ä»¥ä¸Šï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œé€‚ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DIPNetçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é«˜æ³›åŒ–èƒ½åŠ›çš„ä»»åŠ¡ä¸­ï¼Œå¦‚å›¾åƒåˆ†ç±»ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œå¯¹æŠ—å­¦ä¹ ç­‰ã€‚é€šè¿‡æå‡æ¨¡å‹çš„æ³›åŒ–æ€§èƒ½ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­æé«˜ç³»ç»Ÿçš„é²æ£’æ€§å’Œå¯é æ€§ï¼Œæœªæ¥å¯èƒ½å¯¹æ·±åº¦å­¦ä¹ æ¨¡å‹çš„è®¾è®¡å’Œä¼˜åŒ–äº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As overparameterized models become increasingly prevalent, training loss alone offers limited insight into generalization performance. While smoothness has been linked to improved generalization across various settings, directly enforcing smoothness in neural networks remains challenging. To address this, we introduce Distributional Input Projection Networks (DIPNet), a novel framework that projects inputs into learnable distributions at each layer. This distributional representation induces a smoother loss landscape with respect to the input, promoting better generalization. We provide theoretical analysis showing that DIPNet reduces both local smoothness measures and the Lipschitz constant of the network, contributing to improved generalization performance. Empirically, we validate DIPNet across a wide range of architectures and tasks, including Vision Transformers (ViTs), Large Language Models (LLMs), ResNet and MLPs. Our method consistently enhances test performance under standard settings, adversarial attacks, out-of-distribution inputs, and reasoning benchmarks. We demonstrate that the proposed input projection strategy can be seamlessly integrated into existing models, providing a general and effective approach for boosting generalization performance in modern deep learning.

