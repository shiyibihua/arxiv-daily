---
layout: default
title: FPTQuant: Function-Preserving Transforms for LLM Quantization
---

# FPTQuant: Function-Preserving Transforms for LLM Quantization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.04985" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.04985v1</a>
  <a href="https://arxiv.org/pdf/2506.04985.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.04985v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.04985v1', 'FPTQuant: Function-Preserving Transforms for LLM Quantization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Boris van Breugel, Yelysei Bondarenko, Paul Whatmough, Markus Nagel

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFPTQuantä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹é‡åŒ–æ•ˆç‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `é‡åŒ–` `å‡½æ•°ä¿æŒå˜æ¢` `æ¨ç†æ•ˆç‡` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ¨¡å‹ä¼˜åŒ–` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é‡åŒ–æ–¹æ³•åœ¨å¤„ç†å¤§è¯­è¨€æ¨¡å‹æ—¶ï¼Œå®¹æ˜“å› å¤§å¹…åº¦å¼‚å¸¸å€¼å¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚
2. FPTQuanté€šè¿‡å¼•å…¥å››ç§å‡½æ•°ä¿æŒå˜æ¢ï¼Œæ—¨åœ¨ä¼˜åŒ–å˜æ¢å™¨çš„é‡åŒ–è¿‡ç¨‹ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„åŠŸèƒ½ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒFPTQuantåœ¨å‡†ç¡®æ€§ä¸é€Ÿåº¦ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¾¾åˆ°æœ€é«˜3.9å€çš„åŠ é€Ÿæ•ˆæœã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†æ—¶éœ€è¦å¤§é‡è®¡ç®—å’Œèƒ½é‡ã€‚è™½ç„¶é‡åŒ–æƒé‡å’Œæ¿€æ´»å€¼å¯ä»¥æé«˜æ•ˆç‡ï¼Œä½†ç®€å•çš„é‡åŒ–æ–¹æ³•å¯èƒ½ä¼šå› å¤§å¹…åº¦å¼‚å¸¸å€¼è€Œæ˜¾è‘—é™ä½æ€§èƒ½ã€‚æœ¬æ–‡æå‡ºFPTQuantï¼Œä»‹ç»äº†å››ç§æ–°é¢–ã€è½»é‡ä¸”è¡¨è¾¾èƒ½åŠ›å¼ºçš„å‡½æ•°ä¿æŒå˜æ¢ï¼ˆFPTsï¼‰ï¼Œä»¥ä¿ƒè¿›å˜æ¢å™¨çš„é‡åŒ–ã€‚è¿™äº›å˜æ¢æ—¨åœ¨åœ¨ä¿æŒæ¨¡å‹åŠŸèƒ½çš„åŒæ—¶ï¼Œä½¿ä¸­é—´æ¿€æ´»åˆ†å¸ƒæ›´é€‚åˆé‡åŒ–ã€‚FPTQuantæ— éœ€è‡ªå®šä¹‰å†…æ ¸ï¼Œå‡ ä¹ä¸å¢åŠ æ¨ç†å¼€é”€ï¼Œæ”¯æŒé™æ€INT4é‡åŒ–ï¼Œå¹¶åœ¨æ€§èƒ½ä¸Šå®ç°äº†é«˜è¾¾3.9å€çš„åŠ é€Ÿï¼Œä¸”åœ¨å‡†ç¡®æ€§ä¸é€Ÿåº¦çš„æƒè¡¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨é‡åŒ–è¿‡ç¨‹ä¸­å› å¤§å¹…åº¦å¼‚å¸¸å€¼å¯¼è‡´çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚ç°æœ‰çš„ç®€å•é‡åŒ–æ–¹æ³•æœªèƒ½æœ‰æ•ˆå¤„ç†è¿™äº›å¼‚å¸¸å€¼ï¼Œå½±å“äº†æ¨¡å‹çš„æ¨ç†æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šFPTQuantçš„æ ¸å¿ƒæ€è·¯æ˜¯è®¾è®¡å››ç§å‡½æ•°ä¿æŒå˜æ¢ï¼ˆFPTsï¼‰ï¼Œé€šè¿‡è°ƒæ•´ä¸­é—´æ¿€æ´»åˆ†å¸ƒï¼Œä½¿å…¶æ›´é€‚åˆé‡åŒ–ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„åŠŸèƒ½ã€‚è¿™æ ·çš„è®¾è®¡åˆ©ç”¨äº†å˜æ¢å™¨æ“ä½œä¸­çš„ç­‰å˜æ€§å’Œç‹¬ç«‹æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šFPTQuantçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å››ä¸ªä¸»è¦æ¨¡å—ï¼š1) é’ˆå¯¹æŸ¥è¯¢å’Œé”®çš„å¯åˆå¹¶é¢„RoPEå˜æ¢ï¼›2) é’ˆå¯¹å€¼çš„å¯åˆå¹¶å˜æ¢ï¼›3) MLPå—å†…çš„å¯åˆå¹¶ç¼©æ”¾å˜æ¢ï¼›4) ä¾¿å®œçš„åŠ¨æ€ç¼©æ”¾å˜æ¢ã€‚è¿™äº›æ¨¡å—å…±åŒä½œç”¨ä»¥ä¼˜åŒ–é‡åŒ–è¿‡ç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šFPTQuantçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶è½»é‡çº§çš„å‡½æ•°ä¿æŒå˜æ¢è®¾è®¡ï¼Œèƒ½å¤Ÿåœ¨ä¸å¢åŠ æ¨ç†å¼€é”€çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—æé«˜é‡åŒ–æ¨¡å‹çš„æ€§èƒ½ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºå…¶å¯¹ä¸­é—´æ¿€æ´»åˆ†å¸ƒçš„ä¼˜åŒ–ã€‚

**å…³é”®è®¾è®¡**ï¼šFPTQuantåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é‡‡ç”¨å±€éƒ¨å’Œç«¯åˆ°ç«¯çš„æ–¹å¼ï¼Œä»¥å‡å°‘å¼‚å¸¸å€¼ï¼Œå¹¶ç¡®ä¿é‡åŒ–æ¨¡å‹ä¸å…¨ç²¾åº¦æ¨¡å‹çš„è¾“å‡ºåŒ¹é…ã€‚è¯¥æ–¹æ³•æ— éœ€è‡ªå®šä¹‰å†…æ ¸ï¼Œä¸”åœ¨æ¨ç†æ—¶å‡ ä¹ä¸å¢åŠ é¢å¤–å¼€é”€ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

FPTQuantåœ¨å®éªŒä¸­å®ç°äº†æœ€é«˜3.9å€çš„é€Ÿåº¦æå‡ï¼Œç›¸è¾ƒäºå…¨ç²¾åº¦æ¨¡å‹åœ¨å‡†ç¡®æ€§ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä»…ç•¥ä½äºä¸€ç§é€Ÿåº¦æ…¢è¾¾29%çš„æ–¹æ³•ã€‚è¿™è¡¨æ˜FPTQuantåœ¨å‡†ç¡®æ€§ä¸é€Ÿåº¦ä¹‹é—´è¾¾æˆäº†è‰¯å¥½çš„å¹³è¡¡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

FPTQuantçš„ç ”ç©¶æˆæœåœ¨å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†æ•ˆç‡ä¼˜åŒ–æ–¹é¢å…·æœ‰é‡è¦åº”ç”¨ä»·å€¼ï¼Œèƒ½å¤Ÿå¹¿æ³›åº”ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ™ºèƒ½åŠ©æ‰‹ç­‰é¢†åŸŸã€‚å…¶é«˜æ•ˆçš„é‡åŒ–æ–¹æ³•å°†æ¨åŠ¨å¤§è§„æ¨¡æ¨¡å‹åœ¨èµ„æºå—é™ç¯å¢ƒä¸­çš„åº”ç”¨ï¼Œæå‡å®é™…éƒ¨ç½²çš„å¯è¡Œæ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) require substantial compute, and thus energy, at inference time. While quantizing weights and activations is effective at improving efficiency, naive quantization of LLMs can significantly degrade performance due to large magnitude outliers. This paper describes FPTQuant, which introduces four novel, lightweight, and expressive function-preserving transforms (FPTs) to facilitate quantization of transformers: (1) a mergeable pre-RoPE transform for queries and keys, (2) a mergeable transform for values, (3) a mergeable scaling transform within the MLP block, and (4) a cheap, dynamic scaling transform. By leveraging the equivariances and independencies inherent to canonical transformer operation, we designed these FPTs to maintain the model's function while shaping the intermediate activation distributions to be more quantization friendly. FPTQuant requires no custom kernels and adds virtually no overhead during inference. The FPTs are trained both locally to reduce outliers, and end-to-end such that the outputs of the quantized and full-precision models match. FPTQuant enables static INT4 quantization with minimal overhead and shows SOTA speed-up of up to 3.9 times over FP. Empirically, FPTQuant has an excellent accuracy-speed trade-off -- it is performing on par or exceeding most prior work and only shows slightly lower accuracy compared to a method that is up to 29% slower.

