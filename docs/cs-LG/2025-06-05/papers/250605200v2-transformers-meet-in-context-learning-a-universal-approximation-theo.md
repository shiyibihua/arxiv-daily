---
layout: default
title: Transformers Meet In-Context Learning: A Universal Approximation Theory
---

# Transformers Meet In-Context Learning: A Universal Approximation Theory

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05200" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05200v2</a>
  <a href="https://arxiv.org/pdf/2506.05200.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05200v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05200v2', 'Transformers Meet In-Context Learning: A Universal Approximation Theory')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Gen Li, Yuchen Jiao, Yu Huang, Yuting Wei, Yuxin Chen

**åˆ†ç±»**: cs.LG, math.ST, stat.ML

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05 (æ›´æ–°: 2025-08-28)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé€šç”¨é€¼è¿‘ç†è®ºä»¥è§£é‡Šå˜æ¢å™¨çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä¸Šä¸‹æ–‡å­¦ä¹ ` `å˜æ¢å™¨` `é€šç”¨é€¼è¿‘ç†è®º` `æœºå™¨å­¦ä¹ ` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¸»è¦å°†å˜æ¢å™¨è§†ä¸ºä¼˜åŒ–ç®—æ³•çš„é€¼è¿‘å™¨ï¼Œæœªèƒ½å……åˆ†è§£é‡Šå…¶ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›çš„æœºåˆ¶ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šç”¨é€¼è¿‘ç†è®ºï¼Œç»“åˆäº†å‡½æ•°é€¼è¿‘ä¸ç®—æ³•é€¼è¿‘çš„è§†è§’ï¼Œå±•ç¤ºå˜æ¢å™¨å¦‚ä½•åœ¨æµ‹è¯•æ—¶è¿›è¡Œæœ‰æ•ˆé¢„æµ‹ã€‚
3. é€šè¿‡ç†è®ºåˆ†æï¼Œè¯æ˜äº†å˜æ¢å™¨èƒ½å¤Ÿåœ¨ä¸æ›´æ–°æƒé‡çš„æƒ…å†µä¸‹ï¼ŒåŸºäºå°‘é‡ç¤ºä¾‹è¿›è¡Œé«˜æ•ˆçš„ä»»åŠ¡æ‰§è¡Œã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹å…·å¤‡ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼Œå³åœ¨æµ‹è¯•æ—¶é€šè¿‡å°‘é‡è¾“å…¥è¾“å‡ºç¤ºä¾‹æ‰§è¡Œæ–°ä»»åŠ¡ï¼Œè€Œæ— éœ€å‚æ•°æ›´æ–°ã€‚æœ¬æ–‡å‘å±•äº†ä¸€ç§é€šç”¨é€¼è¿‘ç†è®ºï¼Œä»¥é˜æ˜å˜æ¢å™¨å¦‚ä½•å®ç°ä¸Šä¸‹æ–‡å­¦ä¹ ã€‚æˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•æ„å»ºä¸€ä¸ªå˜æ¢å™¨ï¼Œåœ¨æ²¡æœ‰è¿›ä¸€æ­¥æƒé‡æ›´æ–°çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤ŸåŸºäºå°‘é‡å™ªå£°ä¸Šä¸‹æ–‡ç¤ºä¾‹è¿›è¡Œé¢„æµ‹ï¼Œå¹¶ä¸”é£é™©æå°ã€‚ä¸ä¹‹å‰å°†å˜æ¢å™¨è§†ä¸ºç»Ÿè®¡å­¦ä¹ ä»»åŠ¡ä¼˜åŒ–ç®—æ³•ï¼ˆå¦‚æ¢¯åº¦ä¸‹é™ï¼‰é€¼è¿‘å™¨çš„ç ”ç©¶ä¸åŒï¼Œæˆ‘ä»¬å°†å·´ä¼¦çš„é€šç”¨å‡½æ•°é€¼è¿‘ç†è®ºä¸ç®—æ³•é€¼è¿‘å™¨è§†è§’ç›¸ç»“åˆï¼Œæä¾›äº†ä¸å—ä¼˜åŒ–ç®—æ³•æœ‰æ•ˆæ€§é™åˆ¶çš„é€¼è¿‘ä¿è¯ï¼Œè¿œè¶…çº¿æ€§å›å½’ç­‰å‡¸é—®é¢˜ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¦‚ä½•è§£é‡Šå˜æ¢å™¨åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†æ­ç¤ºå…¶æœºåˆ¶å’Œåº”ç”¨èŒƒå›´çš„é™åˆ¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šç”¨é€¼è¿‘ç†è®ºï¼Œè¡¨æ˜å˜æ¢å™¨èƒ½å¤Ÿåœ¨æµ‹è¯•æ—¶åˆ©ç”¨å°‘é‡ç¤ºä¾‹è¿›è¡Œæœ‰æ•ˆé¢„æµ‹ï¼Œè€Œæ— éœ€å‚æ•°æ›´æ–°ã€‚è¯¥ç†è®ºç»“åˆäº†å‡½æ•°é€¼è¿‘ä¸ç®—æ³•é€¼è¿‘çš„è§†è§’ï¼Œæä¾›äº†æ›´å¹¿æ³›çš„é€‚ç”¨æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œæ„å»ºä¸€ä¸ªå˜æ¢å™¨ä»¥æ‰¾åˆ°ç›®æ ‡å‡½æ•°çš„çº¿æ€§è¡¨ç¤ºï¼›å…¶æ¬¡ï¼Œåˆ©ç”¨å°‘é‡ä¸Šä¸‹æ–‡ç¤ºä¾‹è¿›è¡Œé¢„æµ‹ï¼Œç¡®ä¿é£é™©æœ€å°åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†å·´ä¼¦çš„é€šç”¨å‡½æ•°é€¼è¿‘ç†è®ºä¸ç®—æ³•é€¼è¿‘è§†è§’ç»“åˆï¼Œæä¾›äº†ä¸å—ä¼˜åŒ–ç®—æ³•æœ‰æ•ˆæ€§é™åˆ¶çš„é€¼è¿‘ä¿è¯ï¼Œæ‰©å±•äº†å˜æ¢å™¨çš„åº”ç”¨èŒƒå›´ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†å°çš„$	ext{l}_1$-èŒƒæ•°æ¥è¡¨ç¤ºç›®æ ‡å‡½æ•°ï¼Œå¹¶é€šè¿‡å˜æ¢å™¨åœ¨æµ‹è¯•æ—¶è§£å†³ç±»ä¼¼Lassoçš„é—®é¢˜ï¼Œç¡®ä¿äº†é«˜æ•ˆçš„çº¿æ€§è¡¨ç¤ºä¸é¢„æµ‹èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„å˜æ¢å™¨åœ¨å¤šä¸ªä»»åŠ¡ä¸Šå‡èƒ½å®ç°æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼Œé£é™©é™ä½è‡³æå°æ°´å¹³ï¼Œå±•ç¤ºäº†å…¶åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­çš„å¼ºå¤§èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨ç¿»è¯‘å’Œå¯¹è¯ç³»ç»Ÿç­‰ï¼Œèƒ½å¤Ÿæå‡æ¨¡å‹åœ¨æ–°ä»»åŠ¡ä¸Šçš„é€‚åº”èƒ½åŠ›ï¼Œå‡å°‘å¯¹å¤§é‡æ ‡æ³¨æ•°æ®çš„ä¾èµ–ã€‚æœªæ¥å¯èƒ½å½±å“æ¨¡å‹è®¾è®¡å’Œè®­ç»ƒç­–ç•¥ï¼Œæ¨åŠ¨æ›´é«˜æ•ˆçš„å­¦ä¹ æ–¹æ³•å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models are capable of in-context learning, the ability to perform new tasks at test time using a handful of input-output examples, without parameter updates. We develop a universal approximation theory to elucidate how transformers enable in-context learning. For a general class of functions (each representing a distinct task), we demonstrate how to construct a transformer that, without any further weight updates, can predict based on a few noisy in-context examples with vanishingly small risk. Unlike prior work that frames transformers as approximators of optimization algorithms (e.g., gradient descent) for statistical learning tasks, we integrate Barron's universal function approximation theory with the algorithm approximator viewpoint. Our approach yields approximation guarantees that are not constrained by the effectiveness of the optimization algorithms being mimicked, extending far beyond convex problems like linear regression. The key is to show that (i) any target function can be nearly linearly represented, with small $\ell_1$-norm, over a set of universal features, and (ii) a transformer can be constructed to find the linear representation -- akin to solving Lasso -- at test time.

