---
layout: default
title: Ravan: Multi-Head Low-Rank Adaptation for Federated Fine-Tuning
---

# Ravan: Multi-Head Low-Rank Adaptation for Federated Fine-Tuning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05568" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05568v1</a>
  <a href="https://arxiv.org/pdf/2506.05568.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05568v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05568v1', 'Ravan: Multi-Head Low-Rank Adaptation for Federated Fine-Tuning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Arian Raje, Baris Askin, Divyansh Jhunjhunwala, Gauri Joshi

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRavanä»¥è§£å†³è”é‚¦å¾®è°ƒä¸­çš„ä½ç§©é€‚åº”é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è”é‚¦å­¦ä¹ ` `ä½ç§©é€‚åº”` `å¤§å‹è¯­è¨€æ¨¡å‹` `å¤šå¤´æœºåˆ¶` `æ¨¡å‹å¾®è°ƒ` `éšç§ä¿æŠ¤` `è¾¹ç¼˜è®¡ç®—`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„LoRAæ–¹æ³•åœ¨è”é‚¦å­¦ä¹ ç¯å¢ƒä¸­å‡†ç¡®æ€§ä¸‹é™ï¼Œä¸»è¦ç”±äºå®¢æˆ·ç«¯ä¹‹é—´çš„æ•°æ®å’Œè®¡ç®—å¼‚è´¨æ€§ã€‚
2. Ravané€šè¿‡å¤šå¤´ä½ç§©é€‚åº”æ–¹æ³•ï¼Œå°†æƒé‡æ›´æ–°é‡æ–°å‚æ•°åŒ–ä¸ºå¤šä¸ªLoRAå¤´çš„å’Œï¼Œä»è€Œæé«˜æ¨¡å‹çš„è¡¨ç°åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRavanåœ¨è§†è§‰å’Œè¯­è¨€åŸºå‡†ä¸Šæµ‹è¯•å‡†ç¡®ç‡æé«˜äº†2-8%ï¼Œä¼˜äºä»¥å¾€çš„å‚æ•°é«˜æ•ˆåŸºçº¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å°šæœªæœ‰æ•ˆåˆ©ç”¨è¾¹ç¼˜è®¾å¤‡çš„æ•°æ®ï¼Œè€Œè”é‚¦å­¦ä¹ ï¼ˆFLï¼‰æä¾›äº†ä¸€ç§åœ¨ä¸ä¼ è¾“ç§æœ‰æ•°æ®çš„æƒ…å†µä¸‹åä½œå¾®è°ƒLLMsçš„æœ‰å‰æ™¯çš„èŒƒå¼ã€‚ç°æœ‰çš„ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æ–¹æ³•åœ¨FLç¯å¢ƒä¸­é¢ä¸´å‡†ç¡®æ€§ä¸‹é™çš„é—®é¢˜ï¼Œä¸»è¦ç”±äºå®¢æˆ·ç«¯ä¹‹é—´çš„æ•°æ®å’Œè®¡ç®—å¼‚è´¨æ€§ã€‚æœ¬æ–‡æå‡ºäº†Ravanï¼Œä¸€ç§è‡ªé€‚åº”çš„å¤šå¤´LoRAæ–¹æ³•ï¼Œé€šè¿‡å°†æƒé‡æ›´æ–°é‡æ–°å‚æ•°åŒ–ä¸ºå¤šä¸ªLoRAå¤´çš„å’Œï¼Œå¹³è¡¡äº†å‚æ•°æ•ˆç‡å’Œæ¨¡å‹è¡¨ç°åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒRavanåœ¨è§†è§‰å’Œè¯­è¨€åŸºå‡†ä¸Šç›¸è¾ƒäºå…ˆå‰çš„å‚æ•°é«˜æ•ˆåŸºçº¿æé«˜äº†2-8%çš„æµ‹è¯•å‡†ç¡®ç‡ï¼Œæˆä¸ºè”é‚¦å¾®è°ƒLLMsçš„ç¨³å¥ä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è”é‚¦å­¦ä¹ ä¸­ä½ç§©é€‚åº”æ–¹æ³•ï¼ˆLoRAï¼‰åœ¨å‡†ç¡®æ€§ä¸Šçš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å®¢æˆ·ç«¯æ•°æ®å’Œè®¡ç®—èƒ½åŠ›ä¸å‡è¡¡çš„æƒ…å†µä¸‹ï¼Œå¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸‹é™çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šRavançš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡è‡ªé€‚åº”çš„å¤šå¤´LoRAæ–¹æ³•ï¼Œå°†æƒé‡æ›´æ–°è¡¨ç¤ºä¸ºå¤šä¸ªLoRAå¤´çš„å’Œï¼Œä»è€Œåœ¨ä¿æŒå‚æ•°æ•ˆç‡çš„åŒæ—¶å¢å¼ºæ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚é€šè¿‡è®­ç»ƒè½»é‡çº§çš„ç¼©æ”¾å› å­ï¼Œä¼˜åŒ–è¿‡ç¨‹èƒ½å¤Ÿé›†ä¸­åœ¨æœ€æœ‰ç”¨çš„å¤´ä¸Šï¼Œæ¢å¤æ›´é«˜ç§©çš„æ›´æ–°è¿‘ä¼¼ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRavançš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªLoRAå¤´ï¼Œæ¯ä¸ªå¤´ç”±æ ¸å¿ƒçŸ©é˜µå’Œç¼©æ”¾å› å­ç»„æˆã€‚å®¢æˆ·ç«¯ä»…ä¸Šä¼ ç¼©æ”¾å› å­å’Œæ ¸å¿ƒçŸ©é˜µçš„ä¹˜ç§¯ï¼Œé¿å…äº†å¢åŠ é€šä¿¡å‚æ•°çš„è´Ÿæ‹…ã€‚

**å…³é”®åˆ›æ–°**ï¼šRavançš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥äº†å¤šå¤´æœºåˆ¶å’Œè‡ªé€‚åº”ç¼©æ”¾å› å­ï¼Œä½¿å¾—æ¨¡å‹åœ¨ä¸å¢åŠ é€šä¿¡å¼€é”€çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”ä¸åŒå®¢æˆ·ç«¯çš„æ•°æ®ç‰¹æ€§ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šRavançš„è®¾è®¡ä¸­ï¼Œæ ¸å¿ƒçŸ©é˜µå’Œç¼©æ”¾å› å­æ˜¯å¯è®­ç»ƒçš„ï¼Œä¼˜åŒ–è¿‡ç¨‹ä¸­å…³æ³¨æœ€æœ‰æ•ˆçš„LoRAå¤´ï¼Œç¡®ä¿äº†æ¨¡å‹çš„é«˜æ•ˆæ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒRavanåœ¨è§†è§‰å’Œè¯­è¨€åŸºå‡†æµ‹è¯•ä¸­ç›¸è¾ƒäºä»¥å¾€çš„å‚æ•°é«˜æ•ˆåŸºçº¿æé«˜äº†2-8%çš„æµ‹è¯•å‡†ç¡®ç‡ï¼Œå±•ç¤ºäº†å…¶åœ¨è”é‚¦å¾®è°ƒä¸­çš„ä¼˜è¶Šæ€§å’Œå¯æ‰©å±•æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½æ‰‹æœºã€ç‰©è”ç½‘è®¾å¤‡ç­‰è¾¹ç¼˜è®¡ç®—åœºæ™¯ï¼Œèƒ½å¤Ÿåœ¨ä¿æŠ¤ç”¨æˆ·éšç§çš„å‰æä¸‹ï¼Œæå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚æœªæ¥ï¼ŒRavanå¯èƒ½åœ¨æ›´å¤šçš„è”é‚¦å­¦ä¹ åº”ç”¨ä¸­å¾—åˆ°æ¨å¹¿ï¼Œæ¨åŠ¨ä¸ªæ€§åŒ–AIæœåŠ¡çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have not yet effectively leveraged the vast amounts of edge-device data, and federated learning (FL) offers a promising paradigm to collaboratively fine-tune LLMs without transferring private edge data to the cloud. To operate within the computation and communication constraints of edge devices, recent literature on federated fine-tuning of LLMs proposes the use of low-rank adaptation (LoRA) and similar parameter-efficient methods. However, LoRA-based methods suffer from accuracy degradation in FL settings, primarily because of data and computational heterogeneity across clients. We propose \textsc{Ravan}, an adaptive multi-head LoRA method that balances parameter efficiency and model expressivity by reparameterizing the weight updates as the sum of multiple LoRA heads $s_i\textbf{B}_i\textbf{H}_i\textbf{A}_i$ in which only the core matrices $\textbf{H}_i$ and their lightweight scaling factors $s_i$ are trained. These trainable scaling factors let the optimization focus on the most useful heads, recovering a higher-rank approximation of the full update without increasing the number of communicated parameters since clients upload $s_i\textbf{H}_i$ directly. Experiments on vision and language benchmarks show that \textsc{Ravan} improves test accuracy by 2-8\% over prior parameter-efficient baselines, making it a robust and scalable solution for federated fine-tuning of LLMs.

