---
layout: default
title: Enhancing Delta Compression in LLMs via SVD-based Quantization Error Minimization
---

# Enhancing Delta Compression in LLMs via SVD-based Quantization Error Minimization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.11087" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.11087v2</a>
  <a href="https://arxiv.org/pdf/2506.11087.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.11087v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.11087v2', 'Enhancing Delta Compression in LLMs via SVD-based Quantization Error Minimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Boya Xiong, Shuo Wang, Weifeng Ge, Guanhua Chen, Yun Chen

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05 (æ›´æ–°: 2025-09-27)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDeltaMixæ¡†æ¶ä»¥è§£å†³LLMsçš„é‡åŒ–è¯¯å·®é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¢é‡å‹ç¼©` `é‡åŒ–è¯¯å·®` `æ··åˆç²¾åº¦` `å¥‡å¼‚å€¼åˆ†è§£` `å¤§è¯­è¨€æ¨¡å‹` `å¾®è°ƒ` `çº¿æ€§æ•´æ•°è§„åˆ’` `æ¨¡å‹ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¢é‡å‹ç¼©æ–¹æ³•åœ¨é«˜å‹ç¼©æ¯”ä¸‹è¡¨ç°ä¸è¶³ï¼Œä¸»è¦ä¾èµ–ç»éªŒï¼Œç¼ºä¹ç†è®ºæ”¯æŒã€‚
2. æå‡ºDeltaMixæ¡†æ¶ï¼Œé€šè¿‡è‡ªé€‚åº”æ··åˆç²¾åº¦å‹ç¼©æ¥æœ€å°åŒ–SVDç©ºé—´ä¸­çš„é‡åŒ–è¯¯å·®ï¼Œè§£å†³ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDeltaMixåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå°¤å…¶åœ¨ç‰¹å®šä»»åŠ¡ä¸Šæå‡å¹…åº¦æ˜¾è‘—ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¾®è°ƒæ˜¯å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€‚åº”å¤šæ ·åŒ–åº”ç”¨çš„é‡è¦è¿‡ç¨‹ã€‚åœ¨å¤šç§Ÿæˆ·æœåŠ¡ç­‰åœºæ™¯ä¸­ï¼Œéƒ¨ç½²äº†å¤§é‡ä»åŒä¸€åŸºç¡€æ¨¡å‹å¾®è°ƒçš„LLMsä»¥æ»¡è¶³å¤æ‚çš„ç”¨æˆ·éœ€æ±‚ã€‚è¿‘æœŸç ”ç©¶æ¢ç´¢äº†å¢é‡å‹ç¼©æ–¹æ³•æ¥é‡åŒ–å’Œå‹ç¼©å®šåˆ¶LLMä¸ç›¸åº”åŸºç¡€æ¨¡å‹ä¹‹é—´çš„å¢é‡æƒé‡ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨é«˜å‹ç¼©æ¯”ä¸‹è¡¨ç°ä¸è¶³ï¼Œä¸»è¦ç”±äºå…¶ç»éªŒæ€§è´¨ã€‚æœ¬æ–‡æå‡ºäº†DeltaMixï¼Œä¸€ä¸ªè‡ªé€‚åº”æ··åˆç²¾åº¦å¢é‡å‹ç¼©æ¡†æ¶ï¼Œæ—¨åœ¨æœ€å°åŒ–å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰ç©ºé—´ä¸­çš„é‡åŒ–è¯¯å·®ï¼Œè€Œä¸æ–½åŠ é¢å¤–å‡è®¾ã€‚DeltaMixä¸ºæ··åˆç²¾åº¦å‹ç¼©çš„å¿…è¦æ€§æä¾›äº†ç†è®ºä¾æ®ï¼Œå¹¶æå‡ºäº†ä¸€ç§å®é™…çš„é‡åŒ–è§£å†³æ–¹æ¡ˆï¼Œæ¶‰åŠè§£å†³0/1çº¿æ€§æ•´æ•°è§„åˆ’é—®é¢˜åŠé‡å»ºç›®æ ‡ä¿®æ­£æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDeltaMixåœ¨å¤šä¸ªæ¨¡å‹å’ŒåŸºå‡†æµ‹è¯•ä¸­å§‹ç»ˆä¼˜äºæ‰€æœ‰åŸºçº¿æ–¹æ³•ï¼Œå°¤å…¶åœ¨AIME2024å’ŒGQAä»»åŠ¡ä¸­ï¼ŒDeltaMixåœ¨7Bå‚æ•°æ¨¡å‹ä¸Šåˆ†åˆ«è¶…è¶Šæœ€ä½³åŸºçº¿Delta-CoMe 22.3%å’Œ6.1%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¢é‡å‹ç¼©æ–¹æ³•åœ¨é«˜å‹ç¼©æ¯”ä¸‹çš„é‡åŒ–è¯¯å·®é—®é¢˜ã€‚è¿™äº›æ–¹æ³•é€šå¸¸ä¾èµ–ç»éªŒï¼Œå¯¼è‡´åœ¨å®é™…åº”ç”¨ä¸­æ€§èƒ½ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDeltaMixæ¡†æ¶çš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡è‡ªé€‚åº”æ··åˆç²¾åº¦æ¥ä¼˜åŒ–å¢é‡æƒé‡çš„é‡åŒ–è¿‡ç¨‹ï¼Œé¿å…äº†å¯¹é¢å¤–å‡è®¾çš„ä¾èµ–ï¼Œä»è€Œæé«˜äº†å‹ç¼©æ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDeltaMixçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ˜¯åŸºäºSVDçš„é‡åŒ–è¯¯å·®æœ€å°åŒ–ï¼Œå…¶æ¬¡æ˜¯é€šè¿‡è§£å†³0/1çº¿æ€§æ•´æ•°è§„åˆ’é—®é¢˜æ¥å®ç°é‡åŒ–ç›®æ ‡çš„ä¿®æ­£ã€‚

**å…³é”®åˆ›æ–°**ï¼šDeltaMixçš„ä¸»è¦åˆ›æ–°åœ¨äºæä¾›äº†æ··åˆç²¾åº¦å‹ç¼©çš„ç†è®ºä¾æ®ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„é‡åŒ–è§£å†³æ–¹æ¡ˆï¼Œæ˜¾è‘—æå‡äº†å‹ç¼©æ€§èƒ½ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒDeltaMixåœ¨ç†è®ºå’Œå®è·µä¸Šå‡è¡¨ç°å‡ºæ›´å¼ºçš„é€‚åº”æ€§å’Œæœ‰æ•ˆæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼ŒDeltaMixé‡‡ç”¨äº†åŠ¨æ€è°ƒæ•´çš„æ··åˆç²¾åº¦ç­–ç•¥ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡è€ƒè™‘äº†é‡åŒ–è¯¯å·®çš„å½±å“ï¼Œç½‘ç»œç»“æ„åˆ™åŸºäºSVDè¿›è¡Œä¼˜åŒ–ï¼Œç¡®ä¿äº†å‹ç¼©åçš„æ¨¡å‹æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDeltaMixåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨AIME2024å’ŒGQAä»»åŠ¡ä¸Šï¼Œåˆ†åˆ«è¶…è¶Šæœ€ä½³åŸºçº¿Delta-CoMe 22.3%å’Œ6.1%ã€‚è¿™ä¸€æ˜¾è‘—æå‡è¯æ˜äº†DeltaMixåœ¨å¢é‡å‹ç¼©é¢†åŸŸçš„æœ‰æ•ˆæ€§å’Œåˆ›æ–°æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤šç§Ÿæˆ·æœåŠ¡ã€å®šåˆ¶åŒ–è¯­è¨€æ¨¡å‹çš„éƒ¨ç½²å’Œä¼˜åŒ–ç­‰ã€‚DeltaMixæ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆæå‡LLMsçš„å‹ç¼©æ€§èƒ½ï¼Œé™ä½å­˜å‚¨å’Œè®¡ç®—æˆæœ¬ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ï¼Œå°¤å…¶åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­å°¤ä¸ºé‡è¦ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Fine-tuning is a crucial process for adapting large language models (LLMs) to diverse applications. In certain scenarios, like multi-tenant serving, a large number of LLMs finetuned from the same base model are deployed to meet complex requirements for users. Recent works explore delta-compression approaches to quantize and compress the delta weights between the customized LLM and the corresponding base model. However, they exhibit inadequate performance at high compression ratios due to their empirical nature. In this work, we introduce DeltaMix, an adaptive mixed-precision delta-compression framework designed to minimize quantization error in the singular value decomposition (SVD) space without imposing additional assumptions. DeltaMix provides a theoretical justification for the necessity of mixed-precision compression and presents a practical quantization solution that involves solving a 0/1 linear integer programming problem alongside a reconstruction target correction method. Experimental results across multiple models and benchmarks illustrate that DeltaMix consistently outperforms all baseline methods. Notably, on tasks such as AIME2024 and GQA, DeltaMix exceeds the performance of the best baseline, Delta-CoMe, by 22.3\% and 6.1\% for 7B parameter models, respectively.

