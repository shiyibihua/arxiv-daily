---
layout: default
title: Mitigating Degree Bias Adaptively with Hard-to-Learn Nodes in Graph Contrastive Learning
---

# Mitigating Degree Bias Adaptively with Hard-to-Learn Nodes in Graph Contrastive Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05214" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05214v1</a>
  <a href="https://arxiv.org/pdf/2506.05214.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05214v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05214v1', 'Mitigating Degree Bias Adaptively with Hard-to-Learn Nodes in Graph Contrastive Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jingyu Hu, Hongbo Bo, Jun Hong, Xiaowei Liu, Weiru Liu

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHARæŸå¤±ä»¥é€‚åº”æ€§ç¼“è§£å›¾å¯¹æ¯”å­¦ä¹ ä¸­çš„åº¦åå·®é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å›¾ç¥ç»ç½‘ç»œ` `èŠ‚ç‚¹åˆ†ç±»` `å¯¹æ¯”å­¦ä¹ ` `åº¦åå·®` `è‡ªé€‚åº”åŠ æƒ` `å®éªŒæ¡†æ¶` `HARæŸå¤±`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å›¾å¯¹æ¯”å­¦ä¹ æ–¹æ³•åœ¨å¤„ç†èŠ‚ç‚¹åˆ†ç±»æ—¶ï¼Œæœªèƒ½æœ‰æ•ˆè§£å†³ä¸åŒåº¦èŠ‚ç‚¹çš„é¢„æµ‹æ€§èƒ½å·®å¼‚é—®é¢˜ï¼Œå¯¼è‡´ä½åº¦èŠ‚ç‚¹ä¿¡æ¯ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºçš„HARå¯¹æ¯”æŸå¤±é€šè¿‡è‡ªé€‚åº”åŠ æƒæ­£è´Ÿæ ·æœ¬ï¼Œç»“åˆèŠ‚ç‚¹æ ‡ç­¾ï¼Œå¢åŠ äº†æ­£æ ·æœ¬å¯¹çš„æ•°é‡ï¼Œä»è€Œç¼“è§£åº¦åå·®ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒSHARPåœ¨å››ä¸ªæ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨å…¨å±€å’Œåº¦çº§åˆ«ä¸Šçš„æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰åœ¨èŠ‚ç‚¹åˆ†ç±»ä»»åŠ¡ä¸­å¸¸å¸¸å—åˆ°åº¦åå·®çš„å½±å“ï¼Œå³ä¸åŒåº¦çš„èŠ‚ç‚¹é¢„æµ‹æ€§èƒ½å·®å¼‚æ˜¾è‘—ã€‚å°½ç®¡å·²æœ‰å¤šç§åŸºäºå›¾å¯¹æ¯”å­¦ä¹ ï¼ˆGCLï¼‰çš„æ–¹æ³•è¯•å›¾ç¼“è§£è¿™ä¸€åå·®ï¼Œä½†ç”±äºæ­£æ ·æœ¬æ•°é‡æœ‰é™ä»¥åŠå¯¹æ‰€æœ‰æ­£è´Ÿæ ·æœ¬çš„å‡ç­‰åŠ æƒï¼Œä½åº¦èŠ‚ç‚¹ä»ç„¶é¢ä¸´ä¿¡æ¯ä¸è¶³å’Œå™ªå£°å¹²æ‰°çš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç¡¬åº¦è‡ªé€‚åº”é‡åŠ æƒï¼ˆHARï¼‰å¯¹æ¯”æŸå¤±ï¼Œé€šè¿‡åˆ©ç”¨èŠ‚ç‚¹æ ‡ç­¾å¢åŠ æ›´å¤šæ­£æ ·æœ¬ï¼Œå¹¶æ ¹æ®å­¦ä¹ éš¾åº¦è‡ªé€‚åº”åœ°åŠ æƒæ­£è´Ÿæ ·æœ¬ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªåä¸ºSHARPçš„å®éªŒæ¡†æ¶ï¼Œå°†HARæ‰©å±•åˆ°æ›´å¹¿æ³›çš„åœºæ™¯ã€‚ç†è®ºåˆ†æå’Œå®éªŒç»“æœéªŒè¯äº†SHARPçš„æœ‰æ•ˆæ€§ï¼Œå››ä¸ªæ•°æ®é›†çš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSHARPåœ¨å…¨å±€å’Œåº¦çº§åˆ«ä¸Šå‡ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å›¾ç¥ç»ç½‘ç»œåœ¨èŠ‚ç‚¹åˆ†ç±»ä»»åŠ¡ä¸­å­˜åœ¨çš„åº¦åå·®é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•ç”±äºæ­£æ ·æœ¬æ•°é‡æœ‰é™å’Œå‡ç­‰åŠ æƒï¼Œå¯¼è‡´ä½åº¦èŠ‚ç‚¹è·å–çš„ä¿¡æ¯ä¸è¶³ä¸”å™ªå£°è¾ƒå¤šã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºç¡¬åº¦è‡ªé€‚åº”é‡åŠ æƒï¼ˆHARï¼‰å¯¹æ¯”æŸå¤±ï¼Œé€šè¿‡åˆ©ç”¨èŠ‚ç‚¹æ ‡ç­¾å¢åŠ æ­£æ ·æœ¬å¯¹ï¼Œå¹¶æ ¹æ®æ ·æœ¬çš„å­¦ä¹ éš¾åº¦è‡ªé€‚åº”åœ°è°ƒæ•´æ­£è´Ÿæ ·æœ¬çš„æƒé‡ï¼Œä»è€Œæœ‰æ•ˆç¼“è§£åº¦åå·®ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬HARæŸå¤±çš„è®¾è®¡ä¸å®ç°ï¼Œç»“åˆèŠ‚ç‚¹æ ‡ç­¾ç”Ÿæˆæ›´å¤šæ­£æ ·æœ¬ï¼Œå¹¶é€šè¿‡SHARPå®éªŒæ¡†æ¶è¿›è¡ŒéªŒè¯ã€‚SHARPæ¡†æ¶æ‰©å±•äº†HARçš„åº”ç”¨åœºæ™¯ï¼Œå¢å¼ºäº†å®éªŒçš„å¹¿æ³›æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šHARæŸå¤±çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºè‡ªé€‚åº”åŠ æƒæœºåˆ¶ï¼Œèƒ½å¤Ÿæ ¹æ®æ ·æœ¬çš„å­¦ä¹ éš¾åº¦åŠ¨æ€è°ƒæ•´æƒé‡ï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•çš„é™æ€å‡ç­‰åŠ æƒå½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŸå¤±å‡½æ•°è®¾è®¡ä¸­ï¼ŒHARæŸå¤±å¼•å…¥äº†å­¦ä¹ éš¾åº¦çš„æ¦‚å¿µï¼Œå…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„è®¾è®¡æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†è¯´æ˜ï¼Œéœ€å‚è€ƒè®ºæ–‡çš„å…·ä½“å†…å®¹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSHARPåœ¨å››ä¸ªæ•°æ®é›†ä¸Šå‡æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå…·ä½“æå‡å¹…åº¦åœ¨å…¨å±€å’Œåº¦çº§åˆ«ä¸Šå‡æœ‰ä½“ç°ï¼ŒéªŒè¯äº†HARæŸå¤±åœ¨ç¼“è§£åº¦åå·®æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç¤¾äº¤ç½‘ç»œåˆ†æã€æ¨èç³»ç»Ÿä»¥åŠç”Ÿç‰©ä¿¡æ¯å­¦ç­‰é¢†åŸŸï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†èŠ‚ç‚¹åˆ†ç±»å’Œå›¾ç»“æ„æ•°æ®æ—¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡æ¨¡å‹çš„æ€§èƒ½å’Œé²æ£’æ€§ã€‚æœªæ¥ï¼ŒHARæŸå¤±å’ŒSHARPæ¡†æ¶çš„è®¾è®¡ç†å¿µä¹Ÿå¯æ¨å¹¿è‡³å…¶ä»–å›¾å­¦ä¹ ä»»åŠ¡ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Graph Neural Networks (GNNs) often suffer from degree bias in node classification tasks, where prediction performance varies across nodes with different degrees. Several approaches, which adopt Graph Contrastive Learning (GCL), have been proposed to mitigate this bias. However, the limited number of positive pairs and the equal weighting of all positives and negatives in GCL still lead to low-degree nodes acquiring insufficient and noisy information. This paper proposes the Hardness Adaptive Reweighted (HAR) contrastive loss to mitigate degree bias. It adds more positive pairs by leveraging node labels and adaptively weights positive and negative pairs based on their learning hardness. In addition, we develop an experimental framework named SHARP to extend HAR to a broader range of scenarios. Both our theoretical analysis and experiments validate the effectiveness of SHARP. The experimental results across four datasets show that SHARP achieves better performance against baselines at both global and degree levels.

