---
layout: default
title: Training Task Reasoning LLM Agents for Multi-turn Task Planning via Single-turn Reinforcement Learning
---

# Training Task Reasoning LLM Agents for Multi-turn Task Planning via Single-turn Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.20616" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.20616v2</a>
  <a href="https://arxiv.org/pdf/2509.20616.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.20616v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.20616v2', 'Training Task Reasoning LLM Agents for Multi-turn Task Planning via Single-turn Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hanjiang Hu, Changliu Liu, Na Li, Yebin Wang

**åˆ†ç±»**: cs.LG, eess.SY

**å‘å¸ƒæ—¥æœŸ**: 2025-09-24 (æ›´æ–°: 2025-12-08)

**å¤‡æ³¨**: Accepted by IEEE Control Systems Letters (L-CSS)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå•è½®å¼ºåŒ–å­¦ä¹ è®­ç»ƒä»»åŠ¡æ¨ç†LLM Agentï¼Œè§£å†³å¤šè½®ä»»åŠ¡è§„åˆ’é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `ä»»åŠ¡è§„åˆ’` `å¼ºåŒ–å­¦ä¹ ` `å•è½®æ¨ç†` `Group Relative Policy Optimization`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤šè½®ä»»åŠ¡è§„åˆ’ä¸­ï¼ŒLLM Agenté¢ä¸´å¥–åŠ±ç¨€ç–ã€ä¿¡ç”¨åˆ†é…å›°éš¾å’Œè®¡ç®—å¼€é”€å¤§çš„æŒ‘æˆ˜ã€‚
2. è¯¥è®ºæ–‡å°†å¤šè½®ä»»åŠ¡è§„åˆ’è½¬åŒ–ä¸ºå•è½®ä»»åŠ¡æ¨ç†ï¼Œåˆ©ç”¨ä¸“å®¶è½¨è¿¹æä¾›å¯†é›†å¥–åŠ±ï¼Œå¹¶é€šè¿‡GRPOè¿›è¡Œç­–ç•¥ä¼˜åŒ–ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨å•è½®GRPOè®­ç»ƒçš„15äº¿å‚æ•°æ¨¡å‹ä¼˜äºé«˜è¾¾140äº¿å‚æ•°çš„åŸºçº¿æ¨¡å‹ï¼Œé•¿æ—¶ç¨‹ä»»åŠ¡æˆåŠŸç‡è¾¾70%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨çŸ¥è¯†è·å–ã€æ¨ç†å’Œå·¥å…·ä½¿ç”¨æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½¿å…¶æˆä¸ºè‡ªä¸»Agentåº”ç”¨çš„æœ‰å¸Œæœ›çš„å€™é€‰è€…ã€‚ç„¶è€Œï¼Œè®­ç»ƒLLM Agentè¿›è¡Œå¤æ‚çš„å¤šè½®ä»»åŠ¡è§„åˆ’é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç¨€ç–çš„episodeå¥–åŠ±ã€è·¨é•¿æ—¶ç¨‹çš„ä¿¡ç”¨åˆ†é…ä»¥åŠå¤šè½®äº¤äº’ç¯å¢ƒä¸­å¼ºåŒ–å­¦ä¹ çš„è®¡ç®—å¼€é”€ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œå°†å¤šè½®ä»»åŠ¡è§„åˆ’è½¬åŒ–ä¸ºå•è½®ä»»åŠ¡æ¨ç†é—®é¢˜ï¼Œä»è€Œå¯ä»¥é€šè¿‡Group Relative Policy Optimization (GRPO)ä»¥åŠæ¥è‡ªä¸“å®¶è½¨è¿¹çš„å¯†é›†ä¸”å¯éªŒè¯çš„å¥–åŠ±æ¥å®ç°æœ‰æ•ˆçš„ç­–ç•¥ä¼˜åŒ–ã€‚æˆ‘ä»¬çš„ç†è®ºåˆ†æè¡¨æ˜ï¼Œå•è½®ä»»åŠ¡æ¨ç†ä¸­GRPOçš„æ”¹è¿›ä¼šå¯¼è‡´æœ€å°è½®æ•°ä¸‹å¤šè½®æˆåŠŸæ¦‚ç‡çš„ä¸‹ç•Œï¼Œä»¥åŠæ¨å¹¿åˆ°å…·æœ‰è¾ƒçŸ­æ—¶ç¨‹çš„å­ä»»åŠ¡ã€‚åœ¨å¤æ‚ä»»åŠ¡è§„åˆ’åŸºå‡†ä¸Šçš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬ä½¿ç”¨å•è½®GRPOè®­ç»ƒçš„15äº¿å‚æ•°æ¨¡å‹ä¸é«˜è¾¾140äº¿å‚æ•°çš„æ›´å¤§åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œå®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œå¯¹äºé•¿æ—¶ç¨‹è§„åˆ’ä»»åŠ¡çš„æˆåŠŸç‡è¾¾åˆ°70%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³è®­ç»ƒLLM Agentè¿›è¡Œå¤æ‚å¤šè½®ä»»åŠ¡è§„åˆ’æ—¶é‡åˆ°çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å¥–åŠ±ç¨€ç–æ€§ã€é•¿æ—¶ç¨‹ä¿¡ç”¨åˆ†é…å›°éš¾ä»¥åŠå¼ºåŒ–å­¦ä¹ çš„è®¡ç®—å¼€é”€ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆè®­ç»ƒLLM Agentå®Œæˆå¤æ‚ä»»åŠ¡ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¤šæ­¥äº¤äº’å’Œè§„åˆ’çš„åœºæ™¯ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæ ¸å¿ƒæ€è·¯æ˜¯å°†å¤æ‚çš„å¤šè½®ä»»åŠ¡è§„åˆ’é—®é¢˜åˆ†è§£ä¸ºæ›´ç®€å•çš„å•è½®ä»»åŠ¡æ¨ç†é—®é¢˜ã€‚é€šè¿‡è¿™ç§è½¬åŒ–ï¼Œå¯ä»¥åˆ©ç”¨ä¸“å®¶è½¨è¿¹æä¾›å¯†é›†ä¸”å¯éªŒè¯çš„å¥–åŠ±ä¿¡å·ï¼Œä»è€Œé¿å…äº†ç¨€ç–å¥–åŠ±å¸¦æ¥çš„è®­ç»ƒéš¾é¢˜ã€‚åŒæ—¶ï¼Œé‡‡ç”¨Group Relative Policy Optimization (GRPO) ç®—æ³•è¿›è¡Œç­–ç•¥ä¼˜åŒ–ï¼Œæé«˜è®­ç»ƒæ•ˆç‡å’Œæ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªå…³é”®æ­¥éª¤ï¼š1) å°†å¤šè½®ä»»åŠ¡è§„åˆ’é—®é¢˜è½¬åŒ–ä¸ºå•è½®ä»»åŠ¡æ¨ç†é—®é¢˜ï¼›2) åˆ©ç”¨ä¸“å®¶è½¨è¿¹ç”Ÿæˆå¯†é›†å¥–åŠ±ä¿¡å·ï¼›3) ä½¿ç”¨GRPOç®—æ³•ä¼˜åŒ–LLM Agentçš„ç­–ç•¥ï¼›4) åœ¨å¤æ‚ä»»åŠ¡è§„åˆ’åŸºå‡†ä¸Šè¿›è¡Œå®éªŒè¯„ä¼°ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒåœ¨äºå°†å¤æ‚é—®é¢˜ç®€åŒ–ï¼Œå¹¶åˆ©ç”¨ä¸“å®¶çŸ¥è¯†åŠ é€Ÿå­¦ä¹ è¿‡ç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå°†å¤šè½®ä»»åŠ¡è§„åˆ’é—®é¢˜è½¬åŒ–ä¸ºå•è½®ä»»åŠ¡æ¨ç†é—®é¢˜ã€‚è¿™ç§è½¬åŒ–ä½¿å¾—å¯ä»¥ä½¿ç”¨æ›´æœ‰æ•ˆçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆå¦‚GRPOï¼‰è¿›è¡Œè®­ç»ƒï¼Œå¹¶åˆ©ç”¨ä¸“å®¶è½¨è¿¹æä¾›çš„å¯†é›†å¥–åŠ±ä¿¡å·æ¥å…‹æœå¥–åŠ±ç¨€ç–æ€§é—®é¢˜ã€‚ä¸ä¼ ç»Ÿçš„ç›´æ¥åœ¨å¤šè½®äº¤äº’ç¯å¢ƒä¸­è¿›è¡Œå¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†è®­ç»ƒæ•ˆç‡å’Œæ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨ä¸“å®¶è½¨è¿¹ç”Ÿæˆå¯†é›†å¥–åŠ±å‡½æ•°ï¼Œå¥–åŠ±å‡½æ•°çš„è®¾è®¡éœ€è¦èƒ½å¤Ÿå‡†ç¡®åæ˜ Agentçš„è¡Œä¸ºä¸ç›®æ ‡ä¹‹é—´çš„å…³ç³»ï¼›2) é‡‡ç”¨Group Relative Policy Optimization (GRPO) ç®—æ³•è¿›è¡Œç­–ç•¥ä¼˜åŒ–ï¼ŒGRPOç®—æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨ä¸“å®¶æ•°æ®ï¼Œå¹¶é¿å…ç­–ç•¥å´©æºƒï¼›3) é’ˆå¯¹å…·ä½“çš„ä»»åŠ¡è§„åˆ’é—®é¢˜ï¼Œè®¾è®¡åˆé€‚çš„å•è½®ä»»åŠ¡æ¨ç†é—®é¢˜ï¼Œç¡®ä¿è½¬åŒ–åçš„é—®é¢˜èƒ½å¤Ÿä¿ç•™åŸå§‹é—®é¢˜çš„å…³é”®ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨å•è½®GRPOè®­ç»ƒçš„15äº¿å‚æ•°æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡è§„åˆ’åŸºå‡†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒæˆåŠŸç‡è¾¾åˆ°70%ï¼Œä¼˜äºé«˜è¾¾140äº¿å‚æ•°çš„åŸºçº¿æ¨¡å‹ã€‚è¿™è¡¨æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°è®­ç»ƒLLM Agentè¿›è¡Œé•¿æ—¶ç¨‹ä»»åŠ¡è§„åˆ’ï¼Œå¹¶ä¸”åœ¨å‚æ•°æ•ˆç‡æ–¹é¢å…·æœ‰ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦å¤æ‚ä»»åŠ¡è§„åˆ’çš„è‡ªä¸»Agenté¢†åŸŸï¼Œä¾‹å¦‚æœºå™¨äººå¯¼èˆªã€æ™ºèƒ½å®¶å±…æ§åˆ¶ã€è‡ªåŠ¨åŒ–å®¢æœã€æ¸¸æˆAIç­‰ã€‚é€šè¿‡æé«˜LLM Agentçš„ä»»åŠ¡è§„åˆ’èƒ½åŠ›ï¼Œå¯ä»¥å®ç°æ›´æ™ºèƒ½ã€æ›´é«˜æ•ˆçš„è‡ªåŠ¨åŒ–ç³»ç»Ÿï¼Œä»è€Œæå‡ç”Ÿäº§æ•ˆç‡å’ŒæœåŠ¡è´¨é‡ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯å’Œå®é™…ä»·å€¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have demonstrated remarkable capabilities in knowledge acquisition, reasoning, and tool use, making them promising candidates for autonomous agent applications. However, training LLM agents for complex multi-turn task planning faces significant challenges, including sparse episode-wise rewards, credit assignment across long horizons, and the computational overhead of reinforcement learning in multi-turn interaction settings. To this end, this paper introduces a novel approach that transforms multi-turn task planning into single-turn task reasoning problems, enabling efficient policy optimization through Group Relative Policy Optimization (GRPO) with dense and verifiable reward from expert trajectories. Our theoretical analysis shows that GRPO improvement on single-turn task reasoning results in a lower bound of the multi-turn success probability under the minimal turns, as well as the generalization to subtasks with shorter horizons. Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning tasks.

