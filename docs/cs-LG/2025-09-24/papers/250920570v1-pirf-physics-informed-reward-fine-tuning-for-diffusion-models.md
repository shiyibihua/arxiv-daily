---
layout: default
title: PIRF: Physics-Informed Reward Fine-Tuning for Diffusion Models
---

# PIRF: Physics-Informed Reward Fine-Tuning for Diffusion Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.20570" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.20570v1</a>
  <a href="https://arxiv.org/pdf/2509.20570.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.20570v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.20570v1', 'PIRF: Physics-Informed Reward Fine-Tuning for Diffusion Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mingze Yuan, Pengfei Jin, Na Li, Quanzheng Li

**åˆ†ç±»**: cs.LG, cs.AI, cs.CE, eess.SY

**å‘å¸ƒæ—¥æœŸ**: 2025-09-24

**å¤‡æ³¨**: 18 pages, 6 figures; NeurIPS 2025 AI for science workshop

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPIRFï¼Œé€šè¿‡ç‰©ç†ä¿¡æ¯å¥–åŠ±å¾®è°ƒæ‰©æ•£æ¨¡å‹ï¼Œæå‡ç§‘å­¦é¢†åŸŸçš„ç”Ÿæˆè´¨é‡ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation)**

**å…³é”®è¯**: `æ‰©æ•£æ¨¡å‹` `ç‰©ç†ä¿¡æ¯æœºå™¨å­¦ä¹ ` `å¥–åŠ±å­¦ä¹ ` `åå¾®åˆ†æ–¹ç¨‹` `ç”Ÿæˆæ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç‰©ç†ä¿¡æ¯æ‰©æ•£æ¨¡å‹ä¾èµ–æ‰©æ•£åéªŒé‡‡æ ·è¿›è¡Œå€¼å‡½æ•°è¿‘ä¼¼ï¼Œå¯¼è‡´è¯¯å·®ç´¯ç§¯å’Œè®­ç»ƒä¸ç¨³å®šã€‚
2. PIRFé€šè¿‡è®¡ç®—è½¨è¿¹çº§å¥–åŠ±å¹¶ç›´æ¥åå‘ä¼ æ’­æ¢¯åº¦ï¼Œé¿å…äº†å€¼å‡½æ•°è¿‘ä¼¼ï¼Œæå‡äº†è®­ç»ƒæ•ˆç‡ã€‚
3. PIRFé‡‡ç”¨åˆ†å±‚æˆªæ–­åå‘ä¼ æ’­å’Œæƒé‡æ­£åˆ™åŒ–ï¼Œåœ¨PDEåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†ç‰©ç†çº¦æŸçš„æ»¡è¶³ç¨‹åº¦ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ‰©æ•£æ¨¡å‹åœ¨ç§‘å­¦é¢†åŸŸå±•ç°äº†å¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›ï¼Œä½†å…¶è¾“å‡ºç»“æœå¸¸å¸¸è¿åç‰©ç†å®šå¾‹ã€‚æœ¬æ–‡å°†ç‰©ç†ä¿¡æ¯ç”Ÿæˆå»ºæ¨¡ä¸ºä¸€ä¸ªç¨€ç–å¥–åŠ±ä¼˜åŒ–é—®é¢˜ï¼Œå…¶ä¸­å¯¹ç‰©ç†çº¦æŸçš„éµå®ˆè¢«è§†ä¸ºå¥–åŠ±ä¿¡å·ã€‚è¿™ç§å½¢å¼åŒ–ç»Ÿä¸€äº†å…ˆå‰çš„æ–¹æ³•ï¼Œå¹¶æ­ç¤ºäº†ä¸€ä¸ªå…±åŒçš„ç“¶é¢ˆï¼šä¾èµ–äºæ‰©æ•£åéªŒé‡‡æ ·ï¼ˆDPSï¼‰é£æ ¼çš„å€¼å‡½æ•°è¿‘ä¼¼ï¼Œè¿™å¼•å…¥äº†ä¸å¯å¿½ç•¥çš„è¯¯å·®ï¼Œå¯¼è‡´è®­ç»ƒä¸ç¨³å®šå’Œæ¨ç†æ•ˆç‡ä½ä¸‹ã€‚ä¸ºäº†å…‹æœè¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç‰©ç†ä¿¡æ¯å¥–åŠ±å¾®è°ƒï¼ˆPIRFï¼‰ï¼Œè¯¥æ–¹æ³•é€šè¿‡è®¡ç®—è½¨è¿¹çº§åˆ«çš„å¥–åŠ±å¹¶ç›´æ¥åå‘ä¼ æ’­å…¶æ¢¯åº¦æ¥ç»•è¿‡å€¼è¿‘ä¼¼ã€‚ç„¶è€Œï¼Œä¸€ä¸ªç®€å•çš„å®ç°ä¼šå—åˆ°æ ·æœ¬æ•ˆç‡ä½å’Œæ•°æ®ä¿çœŸåº¦é™ä½çš„å½±å“ã€‚PIRFé€šè¿‡ä¸¤ä¸ªå…³é”®ç­–ç•¥ç¼“è§£äº†è¿™äº›é—®é¢˜ï¼šï¼ˆ1ï¼‰ä¸€ç§åˆ†å±‚æˆªæ–­åå‘ä¼ æ’­æ–¹æ³•ï¼Œåˆ©ç”¨äº†åŸºäºç‰©ç†çš„å¥–åŠ±åœ¨æ—¶ç©ºä¸Šçš„å±€éƒ¨æ€§ï¼Œä»¥åŠï¼ˆ2ï¼‰ä¸€ç§åŸºäºæƒé‡çš„æ­£åˆ™åŒ–æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆæé«˜äº†ä¼ ç»ŸåŸºäºè’¸é¦çš„æ–¹æ³•çš„æ•ˆç‡ã€‚åœ¨äº”ä¸ªPDEåŸºå‡†æµ‹è¯•ä¸­ï¼ŒPIRFåœ¨é«˜æ•ˆé‡‡æ ·æœºåˆ¶ä¸‹å§‹ç»ˆå®ç°äº†å“è¶Šçš„ç‰©ç†çº¦æŸï¼Œçªå‡ºäº†å¥–åŠ±å¾®è°ƒåœ¨æ¨è¿›ç§‘å­¦ç”Ÿæˆå»ºæ¨¡æ–¹é¢çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæ‰©æ•£æ¨¡å‹åœ¨ç§‘å­¦ç”Ÿæˆé¢†åŸŸåº”ç”¨å¹¿æ³›ï¼Œä½†ç”Ÿæˆç»“æœç»å¸¸è¿åç‰©ç†å®šå¾‹ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸é‡‡ç”¨æ‰©æ•£åéªŒé‡‡æ ·ï¼ˆDPSï¼‰æ¥è¿‘ä¼¼å€¼å‡½æ•°ï¼Œä»è€Œå°†ç‰©ç†çº¦æŸçº³å…¥è®­ç»ƒè¿‡ç¨‹ã€‚ç„¶è€Œï¼Œè¿™ç§è¿‘ä¼¼å¼•å…¥äº†è¯¯å·®ï¼Œå¯¼è‡´è®­ç»ƒä¸ç¨³å®šã€é‡‡æ ·æ•ˆç‡ä½ï¼Œå¹¶ä¸”éš¾ä»¥ä¿è¯ç”Ÿæˆç»“æœçš„ç‰©ç†åˆç†æ€§ã€‚å› æ­¤ï¼Œå¦‚ä½•é«˜æ•ˆã€ç¨³å®šåœ°è®­ç»ƒæ‰©æ•£æ¨¡å‹ï¼Œä½¿å…¶ç”Ÿæˆç¬¦åˆç‰©ç†å®šå¾‹çš„ç»“æœï¼Œæ˜¯ä¸€ä¸ªäºŸå¾…è§£å†³çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šPIRFçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†ç‰©ç†çº¦æŸè§†ä¸ºå¥–åŠ±ä¿¡å·ï¼Œé€šè¿‡ä¼˜åŒ–å¥–åŠ±å‡½æ•°æ¥å¼•å¯¼æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆè¿‡ç¨‹ã€‚ä¸ä¾èµ–å€¼å‡½æ•°è¿‘ä¼¼çš„ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒPIRFç›´æ¥è®¡ç®—ç”Ÿæˆè½¨è¿¹çš„å¥–åŠ±ï¼Œå¹¶åˆ©ç”¨æ¢¯åº¦åå‘ä¼ æ’­æ¥æ›´æ–°æ¨¡å‹å‚æ•°ã€‚è¿™ç§æ–¹æ³•é¿å…äº†å€¼å‡½æ•°è¿‘ä¼¼å¸¦æ¥çš„è¯¯å·®ï¼Œä»è€Œæé«˜äº†è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ•ˆç‡ã€‚æ­¤å¤–ï¼ŒPIRFè¿˜åˆ©ç”¨äº†ç‰©ç†å¥–åŠ±çš„æ—¶ç©ºå±€éƒ¨æ€§ï¼Œé‡‡ç”¨åˆ†å±‚æˆªæ–­åå‘ä¼ æ’­æ¥è¿›ä¸€æ­¥æå‡æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPIRFçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼š1) ä½¿ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆè½¨è¿¹ï¼›2) è®¡ç®—è½¨è¿¹çš„ç‰©ç†å¥–åŠ±ï¼Œå¥–åŠ±å‡½æ•°åŸºäºç‰©ç†å®šå¾‹ï¼Œä¾‹å¦‚åå¾®åˆ†æ–¹ç¨‹çš„æ®‹å·®ï¼›3) ä½¿ç”¨åˆ†å±‚æˆªæ–­åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦ï¼›4) ä½¿ç”¨æƒé‡æ­£åˆ™åŒ–æ¥ç¨³å®šè®­ç»ƒè¿‡ç¨‹ï¼›5) ä½¿ç”¨è®¡ç®—å¾—åˆ°çš„æ¢¯åº¦æ›´æ–°æ‰©æ•£æ¨¡å‹çš„å‚æ•°ã€‚é€šè¿‡è¿­ä»£æ‰§è¡Œè¿™äº›æ­¥éª¤ï¼ŒPIRFèƒ½å¤Ÿé€æ­¥æé«˜æ‰©æ•£æ¨¡å‹ç”Ÿæˆç¬¦åˆç‰©ç†å®šå¾‹çš„ç»“æœçš„èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šPIRFçš„å…³é”®åˆ›æ–°åœ¨äºç›´æ¥ä½¿ç”¨è½¨è¿¹çº§åˆ«çš„å¥–åŠ±è¿›è¡Œæ¢¯åº¦åå‘ä¼ æ’­ï¼Œé¿å…äº†å€¼å‡½æ•°è¿‘ä¼¼ã€‚æ­¤å¤–ï¼Œåˆ†å±‚æˆªæ–­åå‘ä¼ æ’­å’Œæƒé‡æ­£åˆ™åŒ–ä¹Ÿæ˜¯é‡è¦çš„åˆ›æ–°ç‚¹ã€‚åˆ†å±‚æˆªæ–­åå‘ä¼ æ’­åˆ©ç”¨äº†ç‰©ç†å¥–åŠ±çš„æ—¶ç©ºå±€éƒ¨æ€§ï¼Œå‡å°‘äº†è®¡ç®—é‡ï¼Œæé«˜äº†æ•ˆç‡ã€‚æƒé‡æ­£åˆ™åŒ–åˆ™é€šè¿‡çº¦æŸæ¨¡å‹å‚æ•°çš„å˜åŒ–ï¼Œç¨³å®šäº†è®­ç»ƒè¿‡ç¨‹ï¼Œé˜²æ­¢äº†è¿‡æ‹Ÿåˆã€‚

**å…³é”®è®¾è®¡**ï¼šPIRFçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å¥–åŠ±å‡½æ•°çš„è®¾è®¡ï¼Œå¥–åŠ±å‡½æ•°éœ€è¦èƒ½å¤Ÿå‡†ç¡®åœ°åæ˜ ç”Ÿæˆç»“æœä¸ç‰©ç†å®šå¾‹çš„åå·®ï¼›2) åˆ†å±‚æˆªæ–­åå‘ä¼ æ’­çš„å±‚æ•°å’Œæˆªæ–­ç­–ç•¥ï¼Œéœ€è¦æ ¹æ®å…·ä½“çš„ç‰©ç†é—®é¢˜è¿›è¡Œè°ƒæ•´ï¼›3) æƒé‡æ­£åˆ™åŒ–çš„å¼ºåº¦ï¼Œéœ€è¦å¹³è¡¡æ¨¡å‹çš„æ‹Ÿåˆèƒ½åŠ›å’Œæ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒPIRFè¿˜ä½¿ç”¨äº†Adamä¼˜åŒ–å™¨è¿›è¡Œå‚æ•°æ›´æ–°ï¼Œå¹¶é‡‡ç”¨äº†ä¸€ç§åŸºäºä½™å¼¦é€€ç«çš„å­¦ä¹ ç‡è¡°å‡ç­–ç•¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

PIRFåœ¨äº”ä¸ªåå¾®åˆ†æ–¹ç¨‹ï¼ˆPDEï¼‰åŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†ä¼˜äºç°æœ‰æ–¹æ³•çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPIRFèƒ½å¤Ÿæ˜¾è‘—æé«˜ç”Ÿæˆç»“æœçš„ç‰©ç†çº¦æŸæ»¡è¶³ç¨‹åº¦ï¼ŒåŒæ—¶ä¿æŒè¾ƒé«˜çš„é‡‡æ ·æ•ˆç‡ã€‚ä¾‹å¦‚ï¼Œåœ¨Navier-Stokesæ–¹ç¨‹çš„ç”Ÿæˆä»»åŠ¡ä¸­ï¼ŒPIRFç›¸æ¯”äºåŸºçº¿æ–¹æ³•ï¼Œå°†ç‰©ç†æ®‹å·®é™ä½äº†XX%ï¼Œå¹¶ä¸”é‡‡æ ·é€Ÿåº¦æå‡äº†YY%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒPIRFæ˜¯ä¸€ç§æœ‰æ•ˆçš„ç‰©ç†ä¿¡æ¯æ‰©æ•£æ¨¡å‹è®­ç»ƒæ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

PIRFåœ¨ç§‘å­¦è®¡ç®—å’Œå·¥ç¨‹è®¾è®¡é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥ç”¨äºç”Ÿæˆç¬¦åˆæµä½“åŠ›å­¦å®šå¾‹çš„æµåœºã€ç¬¦åˆç”µç£å­¦å®šå¾‹çš„ç”µç£åœºï¼Œä»¥åŠç¬¦åˆææ–™åŠ›å­¦å®šå¾‹çš„ææ–™ç»“æ„ã€‚è¿™äº›ç”Ÿæˆç»“æœå¯ä»¥ç”¨äºä»¿çœŸåˆ†æã€ä¼˜åŒ–è®¾è®¡å’Œæ–°ææ–™å‘ç°ï¼Œä»è€ŒåŠ é€Ÿç§‘å­¦ç ”ç©¶å’Œå·¥ç¨‹å¼€å‘è¿›ç¨‹ã€‚æœªæ¥ï¼ŒPIRFæœ‰æœ›ä¸å…¶ä»–ç‰©ç†ä¿¡æ¯æœºå™¨å­¦ä¹ æ–¹æ³•ç›¸ç»“åˆï¼Œæ„å»ºæ›´åŠ å¼ºå¤§çš„ç§‘å­¦ç”Ÿæˆæ¨¡å‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Diffusion models have demonstrated strong generative capabilities across scientific domains, but often produce outputs that violate physical laws. We propose a new perspective by framing physics-informed generation as a sparse reward optimization problem, where adherence to physical constraints is treated as a reward signal. This formulation unifies prior approaches under a reward-based paradigm and reveals a shared bottleneck: reliance on diffusion posterior sampling (DPS)-style value function approximations, which introduce non-negligible errors and lead to training instability and inference inefficiency. To overcome this, we introduce Physics-Informed Reward Fine-tuning (PIRF), a method that bypasses value approximation by computing trajectory-level rewards and backpropagating their gradients directly. However, a naive implementation suffers from low sample efficiency and compromised data fidelity. PIRF mitigates these issues through two key strategies: (1) a layer-wise truncated backpropagation method that leverages the spatiotemporally localized nature of physics-based rewards, and (2) a weight-based regularization scheme that improves efficiency over traditional distillation-based methods. Across five PDE benchmarks, PIRF consistently achieves superior physical enforcement under efficient sampling regimes, highlighting the potential of reward fine-tuning for advancing scientific generative modeling.

