---
layout: default
title: Diffusion Policies with Value-Conditional Optimization for Offline Reinforcement Learning
---

# Diffusion Policies with Value-Conditional Optimization for Offline Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.08922" class="toolbar-btn" target="_blank">üìÑ arXiv: 2511.08922v1</a>
  <a href="https://arxiv.org/pdf/2511.08922.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.08922v1" onclick="toggleFavorite(this, '2511.08922v1', 'Diffusion Policies with Value-Conditional Optimization for Offline Reinforcement Learning')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Yunchang Ma, Tenglong Liu, Yixing Lan, Xin Yin, Changxin Zhang, Xinglong Zhang, Xin Xu

**ÂàÜÁ±ª**: cs.LG, cs.AI, cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-12

**Â§áÊ≥®**: IROS 2025

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫DIVOÔºåÈÄöËøá‰ª∑ÂÄºÊù°‰ª∂‰ºòÂåñÊâ©Êï£Á≠ñÁï•Ëß£ÂÜ≥Á¶ªÁ∫øÂº∫ÂåñÂ≠¶‰π†‰∏≠ÁöÑËøá‰º∞ËÆ°ÈóÆÈ¢ò„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Á¶ªÁ∫øÂº∫ÂåñÂ≠¶‰π†` `Êâ©Êï£Ê®°Âûã` `‰ª∑ÂÄºÊù°‰ª∂‰ºòÂåñ` `‰ºòÂäøÂáΩÊï∞` `Á≠ñÁï•Â≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Á¶ªÁ∫øÂº∫ÂåñÂ≠¶‰π†‰∏≠ÔºåÂÄºËøá‰º∞ËÆ°ÊòØÁ≠ñÁï•ÊÄßËÉΩÁì∂È¢àÔºåÁé∞ÊúâÊñπÊ≥ï‰øùÂÆàÊÄßËøáÂº∫ÔºåÈöæ‰ª•Âπ≥Ë°°Ë°®ËææËÉΩÂäõÂíåÊïàÁéá„ÄÇ
2. DIVOÂà©Áî®‰ºòÂäøÂÄºÂºïÂØºÊâ©Êï£Ê®°ÂûãËÆ≠ÁªÉÔºåÁ≤æÁ°ÆÂØπÈΩêÊï∞ÊçÆÈõÜÂàÜÂ∏ÉÔºåÈÄâÊã©ÊÄßÊâ©Â±ïÈ´ò‰ºòÂäøÂä®‰ΩúËæπÁïå„ÄÇ
3. DIVOÂú®D4RLÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂ∞§ÂÖ∂Âú®AntMazeÁ≠âÁ®ÄÁñèÂ•ñÂä±ÁéØÂ¢É‰∏≠ÔºåÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Âú®Á¶ªÁ∫øÂº∫ÂåñÂ≠¶‰π†‰∏≠ÔºåÁî±‰∫éÂàÜÂ∏ÉÂ§ñ(OOD)Âä®‰ΩúÂØºËá¥ÁöÑÂÄºËøá‰º∞ËÆ°‰∏•ÈáçÈôêÂà∂‰∫ÜÁ≠ñÁï•ÊÄßËÉΩ„ÄÇÊúÄËøëÔºåÊâ©Êï£Ê®°ÂûãÂõ†ÂÖ∂Âº∫Â§ßÁöÑÂàÜÂ∏ÉÂåπÈÖçËÉΩÂäõËÄåË¢´Âà©Áî®ÔºåÈÄöËøáË°å‰∏∫Á≠ñÁï•Á∫¶ÊùüÊù•Âº∫Âà∂‰øùÂÆàÊÄß„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏ÂØπ‰ΩéË¥®ÈáèÊï∞ÊçÆÈõÜ‰∏≠ÁöÑÂÜó‰ΩôÂä®‰ΩúËøõË°åÊó†Â∑ÆÂà´Ê≠£ÂàôÂåñÔºåÂØºËá¥ËøáÂ∫¶‰øùÂÆà‰ª•ÂèäÊâ©Êï£Ê®°ÂûãË°®ËææËÉΩÂäõÂíåÊïàÁéá‰πãÈó¥ÁöÑ‰∏çÂπ≥Ë°°„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊñπÊ≥ïÔºåÂç≥ÂÖ∑Êúâ‰ª∑ÂÄºÊù°‰ª∂‰ºòÂåñÁöÑÊâ©Êï£Á≠ñÁï•(DIVO)ÔºåËØ•ÊñπÊ≥ïÂà©Áî®Êâ©Êï£Ê®°ÂûãÁîüÊàêÈ´òË¥®Èáè„ÄÅÂπøÊ≥õË¶ÜÁõñÁöÑÂàÜÂ∏ÉÂÜÖÁä∂ÊÄÅ-Âä®‰ΩúÊ†∑Êú¨ÔºåÂêåÊó∂‰øÉËøõÊúâÊïàÁöÑÁ≠ñÁï•ÊîπËøõ„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåDIVOÂºïÂÖ•‰∫Ü‰∏ÄÁßç‰∫åÂÖÉÂä†ÊùÉÊú∫Âà∂ÔºåËØ•Êú∫Âà∂Âà©Áî®Á¶ªÁ∫øÊï∞ÊçÆÈõÜ‰∏≠Âä®‰ΩúÁöÑ‰ºòÂäøÂÄºÊù•ÊåáÂØºÊâ©Êï£Ê®°ÂûãËÆ≠ÁªÉ„ÄÇËøô‰ΩøÂæóËÉΩÂ§üÊõ¥Á≤æÁ°ÆÂú∞‰∏éÊï∞ÊçÆÈõÜÁöÑÂàÜÂ∏ÉÂØπÈΩêÔºåÂêåÊó∂ÈÄâÊã©ÊÄßÂú∞Êâ©Â±ïÈ´ò‰ºòÂäøÂä®‰ΩúÁöÑËæπÁïå„ÄÇÂú®Á≠ñÁï•ÊîπËøõËøáÁ®ã‰∏≠ÔºåDIVOÂä®ÊÄÅÂú∞ËøáÊª§Êù•Ëá™Êâ©Êï£Ê®°ÂûãÁöÑÈ´òÂõûÊä•ÊΩúÂäõÂä®‰ΩúÔºåÊúâÊïàÂú∞ÂºïÂØºÂ≠¶‰π†Âà∞ÁöÑÁ≠ñÁï•ÊúùÁùÄÊõ¥Â•ΩÁöÑÊÄßËÉΩÂèëÂ±ï„ÄÇËøôÁßçÊñπÊ≥ïÂú®Á¶ªÁ∫øÂº∫ÂåñÂ≠¶‰π†‰∏≠ÂÆûÁé∞‰∫Ü‰øùÂÆàÊÄßÂíåÂèØÊé¢Á¥¢ÊÄß‰πãÈó¥ÁöÑÂÖ≥ÈîÆÂπ≥Ë°°„ÄÇÊàë‰ª¨Âú®D4RLÂü∫ÂáÜ‰∏äËØÑ‰º∞‰∫ÜDIVOÔºåÂπ∂Â∞ÜÂÖ∂‰∏éÊúÄÂÖàËøõÁöÑÂü∫Á∫øËøõË°åÊØîËæÉ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåDIVOÂÆûÁé∞‰∫ÜÂçìË∂äÁöÑÊÄßËÉΩÔºåÂú®ËøêÂä®‰ªªÂä°‰∏≠ÂÆûÁé∞‰∫ÜÂπ≥ÂùáÂõûÊä•ÁöÑÊòæËëóÊèêÈ´òÔºåÂπ∂‰∏îÂú®ÂÖ∑ÊúâÁ®ÄÁñèÂ•ñÂä±ÁöÑÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑAntMazeÈ¢ÜÂüü‰∏≠‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁ¶ªÁ∫øÂº∫ÂåñÂ≠¶‰π†Èù¢‰∏¥ÂÄºÂáΩÊï∞Ëøá‰º∞ËÆ°ÁöÑÈóÆÈ¢òÔºåÂ∞§ÂÖ∂ÊòØÂú®Êï∞ÊçÆÈõÜË¥®Èáè‰∏çÈ´òÁöÑÊÉÖÂÜµ‰∏ã„ÄÇÁé∞ÊúâÊñπÊ≥ïÔºåÂ¶Ç‰ΩøÁî®Êâ©Êï£Ê®°ÂûãÁ∫¶ÊùüÁ≠ñÁï•ÔºåÂÆπÊòìÂØπÊâÄÊúâÂä®‰ΩúËøõË°åËøáÂ∫¶‰øùÂÆàÁöÑÊ≠£ÂàôÂåñÔºåÈôêÂà∂‰∫ÜÁ≠ñÁï•ÁöÑÊé¢Á¥¢ËÉΩÂäõÂíåÊÄßËÉΩÊèêÂçá„ÄÇËøôÁßç‰∏ÄÂàÄÂàáÁöÑÊñπÊ≥ïÂøΩÁï•‰∫ÜÊï∞ÊçÆÈõÜ‰∏≠‰∏çÂêåÂä®‰ΩúÁöÑ‰ª∑ÂÄºÂ∑ÆÂºÇÔºåÂØºËá¥Ê¨°‰ºòËß£„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöDIVOÁöÑÊ†∏ÂøÉÂú®‰∫éÂà©Áî®Âä®‰ΩúÁöÑ‰ºòÂäøÂÄºÊù•ÊåáÂØºÊâ©Êï£Ê®°ÂûãÁöÑËÆ≠ÁªÉÔºå‰ªéËÄåÂÆûÁé∞Êõ¥Á≤æÁªÜÁöÑÁ≠ñÁï•Á∫¶Êùü„ÄÇÈÄöËøá‰ºòÂäøÂÄºÔºåDIVOËÉΩÂ§üÂå∫ÂàÜÊúâ‰ª∑ÂÄºÂíåÊó†‰ª∑ÂÄºÁöÑÂä®‰ΩúÔºåÂπ∂ÈÄâÊã©ÊÄßÂú∞ÂØπÈ´ò‰ª∑ÂÄºÂä®‰ΩúËøõË°åÊé¢Á¥¢ÔºåÈÅøÂÖçÂØπÊâÄÊúâÂä®‰ΩúËøõË°åÊó†Â∑ÆÂà´ÁöÑ‰øùÂÆàÁ∫¶Êùü„ÄÇËøôÁßçÊñπÊ≥ïÊó®Âú®Âπ≥Ë°°‰øùÂÆàÊÄßÂíåÊé¢Á¥¢ÊÄßÔºå‰ªéËÄåÂú®Á¶ªÁ∫øÊï∞ÊçÆÈõÜ‰∏≠Â≠¶‰π†Âà∞Êõ¥Â•ΩÁöÑÁ≠ñÁï•„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöDIVOÂåÖÂê´‰∏§‰∏™‰∏ªË¶ÅÈò∂ÊÆµÔºöÊâ©Êï£Ê®°ÂûãËÆ≠ÁªÉÂíåÁ≠ñÁï•ÊîπËøõ„ÄÇÂú®Êâ©Êï£Ê®°ÂûãËÆ≠ÁªÉÈò∂ÊÆµÔºåDIVO‰ΩøÁî®‰∫åÂÖÉÂä†ÊùÉÊú∫Âà∂ÔºåÊ†πÊçÆÂä®‰ΩúÁöÑ‰ºòÂäøÂÄºÂØπÊâ©Êï£Ê®°ÂûãÁöÑÊçüÂ§±ÂáΩÊï∞ËøõË°åÂä†ÊùÉ„ÄÇ‰ºòÂäøÂÄºÈ´òÁöÑÂä®‰ΩúÂú®ËÆ≠ÁªÉ‰∏≠Ëé∑ÂæóÊõ¥È´òÁöÑÊùÉÈáçÔºå‰ªéËÄåÂºïÂØºÊâ©Êï£Ê®°ÂûãÊõ¥Â§öÂú∞ÂÖ≥Ê≥®Ëøô‰∫õÂä®‰Ωú„ÄÇÂú®Á≠ñÁï•ÊîπËøõÈò∂ÊÆµÔºåDIVO‰ªéÊâ©Êï£Ê®°Âûã‰∏≠ÈááÊ†∑Âä®‰ΩúÔºåÂπ∂Ê†πÊçÆÂÖ∂ÊΩúÂú®ÂõûÊä•ËøõË°åËøáÊª§ÔºåÈÄâÊã©Êõ¥ÊúâÂèØËÉΩÂ∏¶Êù•È´òÂõûÊä•ÁöÑÂä®‰ΩúÊù•Êõ¥Êñ∞Á≠ñÁï•„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöDIVOÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫é‰ª∑ÂÄºÊù°‰ª∂‰ºòÂåñÔºåÂç≥Âà©Áî®Âä®‰ΩúÁöÑ‰ºòÂäøÂÄºÊù•ÊåáÂØºÊâ©Êï£Ê®°ÂûãÁöÑËÆ≠ÁªÉÂíåÁ≠ñÁï•ÊîπËøõ„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ï‰∏çÂêåÔºåDIVO‰∏çÊòØÂØπÊâÄÊúâÂä®‰ΩúËøõË°åÊó†Â∑ÆÂà´ÁöÑÁ∫¶ÊùüÔºåËÄåÊòØÊ†πÊçÆÂÖ∂‰ª∑ÂÄºËøõË°åÈÄâÊã©ÊÄßÁöÑÁ∫¶ÊùüÂíåÊé¢Á¥¢„ÄÇËøôÁßçÊñπÊ≥ïËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞Âà©Áî®Á¶ªÁ∫øÊï∞ÊçÆÔºåÂ≠¶‰π†Âà∞Êõ¥Â•ΩÁöÑÁ≠ñÁï•„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöDIVOÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ‰∫åÂÖÉÂä†ÊùÉÊú∫Âà∂ÔºåÁî®‰∫éÊ†πÊçÆÂä®‰ΩúÁöÑ‰ºòÂäøÂÄºÂØπÊâ©Êï£Ê®°ÂûãÁöÑÊçüÂ§±ÂáΩÊï∞ËøõË°åÂä†ÊùÉ„ÄÇÂÖ∑‰ΩìÊù•ËØ¥Ôºå‰ºòÂäøÂÄºÈ´ò‰∫éÊüê‰∏™ÈòàÂÄºÁöÑÂä®‰ΩúË¢´Ëµã‰∫àÊõ¥È´òÁöÑÊùÉÈáçÔºåËÄå‰ºòÂäøÂÄº‰Ωé‰∫éÈòàÂÄºÁöÑÂä®‰ΩúÂàôË¢´Ëµã‰∫àËæÉ‰ΩéÁöÑÊùÉÈáç„ÄÇ2) Âä®ÊÄÅËøáÊª§Êú∫Âà∂ÔºåÁî®‰∫éÂú®Á≠ñÁï•ÊîπËøõÈò∂ÊÆµ‰ªéÊâ©Êï£Ê®°Âûã‰∏≠ÈááÊ†∑Âä®‰ΩúÔºåÂπ∂Ê†πÊçÆÂÖ∂ÊΩúÂú®ÂõûÊä•ËøõË°åËøáÊª§„ÄÇËØ•Êú∫Âà∂ÈÄâÊã©Êõ¥ÊúâÂèØËÉΩÂ∏¶Êù•È´òÂõûÊä•ÁöÑÂä®‰ΩúÊù•Êõ¥Êñ∞Á≠ñÁï•„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

DIVOÂú®D4RLÂü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇÂú®ËøêÂä®‰ªªÂä°‰∏≠ÔºåDIVOÁöÑÂπ≥ÂùáÂõûÊä•ÊòæËëóÈ´ò‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇÂú®ÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑAntMazeÁéØÂ¢É‰∏≠ÔºåDIVO‰πü‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåËØÅÊòé‰∫ÜÂÖ∂Âú®Á®ÄÁñèÂ•ñÂä±ÁéØÂ¢É‰∏ãÁöÑÊúâÊïàÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåDIVOËÉΩÂ§üÊúâÊïàÂú∞Âπ≥Ë°°‰øùÂÆàÊÄßÂíåÊé¢Á¥¢ÊÄßÔºå‰ªéËÄåÂ≠¶‰π†Âà∞Êõ¥Â•ΩÁöÑÁ≠ñÁï•„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

DIVOÂú®Êú∫Âô®‰∫∫ÊéßÂà∂„ÄÅËá™Âä®È©æÈ©∂„ÄÅÊ∏∏ÊàèAIÁ≠âÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØ„ÄÇÂÆÉÂèØ‰ª•Âà©Áî®Á¶ªÁ∫øÊï∞ÊçÆÂ≠¶‰π†È´òÊÄßËÉΩÁ≠ñÁï•ÔºåÊó†ÈúÄÂú®Á∫ø‰∫§‰∫íÔºåÈôç‰Ωé‰∫ÜÂ≠¶‰π†ÊàêÊú¨ÂíåÈ£éÈô©„ÄÇÂ∞§ÂÖ∂ÊòØÂú®Êï∞ÊçÆËé∑ÂèñÂõ∞ÈöæÊàñÊàêÊú¨È´òÊòÇÁöÑÂú∫ÊôØ‰∏ãÔºåDIVOÁöÑ‰ª∑ÂÄºÊõ¥Âä†Á™ÅÂá∫„ÄÇÊú™Êù•ÔºåDIVOÂèØ‰ª•Ëøõ‰∏ÄÊ≠•Êâ©Â±ïÂà∞Â§öÊô∫ËÉΩ‰ΩìÂº∫ÂåñÂ≠¶‰π†„ÄÅÂÖÉÂº∫ÂåñÂ≠¶‰π†Á≠âÈ¢ÜÂüü„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> In offline reinforcement learning, value overestimation caused by out-of-distribution (OOD) actions significantly limits policy performance. Recently, diffusion models have been leveraged for their strong distribution-matching capabilities, enforcing conservatism through behavior policy constraints. However, existing methods often apply indiscriminate regularization to redundant actions in low-quality datasets, resulting in excessive conservatism and an imbalance between the expressiveness and efficiency of diffusion modeling. To address these issues, we propose DIffusion policies with Value-conditional Optimization (DIVO), a novel approach that leverages diffusion models to generate high-quality, broadly covered in-distribution state-action samples while facilitating efficient policy improvement. Specifically, DIVO introduces a binary-weighted mechanism that utilizes the advantage values of actions in the offline dataset to guide diffusion model training. This enables a more precise alignment with the dataset's distribution while selectively expanding the boundaries of high-advantage actions. During policy improvement, DIVO dynamically filters high-return-potential actions from the diffusion model, effectively guiding the learned policy toward better performance. This approach achieves a critical balance between conservatism and explorability in offline RL. We evaluate DIVO on the D4RL benchmark and compare it against state-of-the-art baselines. Empirical results demonstrate that DIVO achieves superior performance, delivering significant improvements in average returns across locomotion tasks and outperforming existing methods in the challenging AntMaze domain, where sparse rewards pose a major difficulty.

