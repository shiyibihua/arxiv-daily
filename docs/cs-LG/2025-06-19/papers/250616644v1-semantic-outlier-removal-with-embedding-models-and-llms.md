---
layout: default
title: Semantic Outlier Removal with Embedding Models and LLMs
---

# Semantic Outlier Removal with Embedding Models and LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.16644" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.16644v1</a>
  <a href="https://arxiv.org/pdf/2506.16644.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.16644v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.16644v1', 'Semantic Outlier Removal with Embedding Models and LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Eren Akbiyik, JoÃ£o Almeida, Rik Melis, Ritu Sriram, Viviana Petrescu, VilhjÃ¡lmur VilhjÃ¡lmsson

**åˆ†ç±»**: cs.LG, cs.IR

**å‘å¸ƒæ—¥æœŸ**: 2025-06-19

**å¤‡æ³¨**: Accepted to the 63rd Annual Meeting of the Association for Computational Linguistics (ACL 2025) Industry Track, 10 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSOREæ–¹æ³•ä»¥è§£å†³å¤šè¯­è¨€æ–‡æœ¬ä¸­å†—ä½™å†…å®¹å»é™¤é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­ä¹‰å¼‚å¸¸å»é™¤` `å¤šè¯­è¨€å¤„ç†` `æ–‡æœ¬æŒ–æ˜` `åµŒå…¥æ¨¡å‹` `è¿‘ä¼¼æœ€è¿‘é‚»æœç´¢` `ä¿¡æ¯æå–` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ–‡æœ¬å¤„ç†æ–¹æ³•åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­æ•ˆæœä¸ä½³ï¼Œéš¾ä»¥å¤„ç†ä¸Šä¸‹æ–‡æ•æ„Ÿçš„å†…å®¹ï¼Œå¯¼è‡´å†—ä½™ä¿¡æ¯éš¾ä»¥å»é™¤ã€‚
2. SOREæ–¹æ³•é€šè¿‡åˆ©ç”¨å¤šè¯­è¨€å¥å­åµŒå…¥å’Œè¿‘ä¼¼æœ€è¿‘é‚»æœç´¢ï¼Œè¯†åˆ«å¹¶å‰”é™¤ä¸å¿…è¦çš„æ–‡æœ¬æ®µè½ï¼Œæä¾›äº†ä¸€ç§é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSOREåœ¨å¤šç§åœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿç»“æ„åŒ–æ–¹æ³•ï¼Œç²¾åº¦æ¥è¿‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä¸”æˆæœ¬æ˜¾è‘—é™ä½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°ä»£æ–‡æœ¬å¤„ç†æµç¨‹éœ€è¦æœ‰æ•ˆçš„æ–¹æ³•æ¥å»é™¤å¤šä½™å†…å®¹ï¼ŒåŒæ—¶ä¿ç•™æ–‡æ¡£çš„æ ¸å¿ƒä¿¡æ¯ã€‚ä¼ ç»Ÿæ–¹æ³•å¦‚HTMLæ¨¡æ¿æå–æˆ–å…³é”®è¯è¿‡æ»¤åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­å¸¸å¸¸å¤±æ•ˆï¼Œå¹¶ä¸”éš¾ä»¥å¤„ç†ä¸Šä¸‹æ–‡æ•æ„Ÿçš„ç»†å¾®å·®åˆ«ã€‚æœ¬æ–‡æå‡ºäº†SOREï¼ˆè¯­ä¹‰å¼‚å¸¸å»é™¤ï¼‰ï¼Œä¸€ç§æˆæœ¬æ•ˆç›Šé«˜ã€é€æ˜çš„æ–¹æ³•ï¼Œåˆ©ç”¨å¤šè¯­è¨€å¥å­åµŒå…¥å’Œè¿‘ä¼¼æœ€è¿‘é‚»æœç´¢æ¥è¯†åˆ«å’Œå‰”é™¤ä¸å¿…è¦çš„æ–‡æœ¬æ®µè½ã€‚é€šè¿‡é¦–å…ˆåˆ©ç”¨å…ƒæ•°æ®åµŒå…¥è¯†åˆ«æ ¸å¿ƒå†…å®¹ï¼Œç„¶åæ ‡è®°ä¸é¢„å®šä¹‰å¼‚å¸¸ç»„ç›¸ä¼¼æˆ–æ˜¾è‘—åç¦»æ ¸å¿ƒçš„æ®µè½ï¼ŒSOREä»¥æä½çš„æˆæœ¬å®ç°äº†æ¥è¿‘å¤§å‹è¯­è¨€æ¨¡å‹çš„æå–ç²¾åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSOREåœ¨HTMLæ•°æ®é›†ä¸Šä¼˜äºç»“æ„åŒ–æ–¹æ³•ï¼Œå¹¶åœ¨å¤šç§åœºæ™¯ä¸­è¡¨ç°å‡ºé«˜ç²¾åº¦ã€‚è¯¥ç³»ç»Ÿç›®å‰å·²åœ¨ç”Ÿäº§ä¸­éƒ¨ç½²ï¼Œæ¯å¤©å¤„ç†æ•°ç™¾ä¸‡ä»½æ–‡æ¡£ï¼Œä¿æŒé«˜æ•ˆå’Œå‡†ç¡®ã€‚ä¸ºäº†ä¿ƒè¿›å¯é‡å¤æ€§å’Œè¿›ä¸€æ­¥ç ”ç©¶ï¼Œæˆ‘ä»¬å‘å¸ƒäº†å®ç°å’Œè¯„ä¼°æ•°æ®é›†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šè¯­è¨€æ–‡æœ¬å¤„ç†ä¸­å†—ä½™å†…å®¹çš„å»é™¤é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¦‚HTMLæ¨¡æ¿æå–å’Œå…³é”®è¯è¿‡æ»¤åœ¨å¤„ç†å¤šè¯­è¨€æ–‡æœ¬æ—¶å¸¸å¸¸å¤±æ•ˆï¼Œæ— æ³•æœ‰æ•ˆæ•æ‰ä¸Šä¸‹æ–‡çš„ç»†å¾®å·®åˆ«ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSOREæ–¹æ³•çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤šè¯­è¨€å¥å­åµŒå…¥å’Œè¿‘ä¼¼æœ€è¿‘é‚»æœç´¢ï¼Œé¦–å…ˆè¯†åˆ«æ ¸å¿ƒå†…å®¹ï¼Œç„¶åæ ‡è®°ä¸é¢„å®šä¹‰å¼‚å¸¸ç»„ç›¸ä¼¼æˆ–æ˜¾è‘—åç¦»æ ¸å¿ƒçš„æ–‡æœ¬æ®µè½ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„å†—ä½™å†…å®¹å»é™¤ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSOREçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ˜¯å…ƒæ•°æ®åµŒå…¥æ¨¡å—ï¼Œç”¨äºè¯†åˆ«æ–‡æ¡£çš„æ ¸å¿ƒå†…å®¹ï¼›å…¶æ¬¡æ˜¯å¼‚å¸¸æ£€æµ‹æ¨¡å—ï¼Œé€šè¿‡è¿‘ä¼¼æœ€è¿‘é‚»æœç´¢æ¥æ ‡è®°ä¸å¿…è¦çš„æ–‡æœ¬æ®µè½ã€‚

**å…³é”®åˆ›æ–°**ï¼šSOREçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶ç»“åˆäº†å¤šè¯­è¨€å¥å­åµŒå…¥ä¸è¿‘ä¼¼æœ€è¿‘é‚»æœç´¢çš„æŠ€æœ¯ï¼Œæ˜¾è‘—æé«˜äº†æ–‡æœ¬å¤„ç†çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼Œå°¤å…¶æ˜¯åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒSOREåœ¨æˆæœ¬å’Œæ€§èƒ½ä¸Šéƒ½æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒSOREé‡‡ç”¨äº†é«˜æ•ˆçš„åµŒå…¥æ¨¡å‹å’Œä¼˜åŒ–çš„æœç´¢ç®—æ³•ï¼Œç¡®ä¿åœ¨å¤„ç†å¤§è§„æ¨¡æ–‡æ¡£æ—¶ä»èƒ½ä¿æŒé«˜ç²¾åº¦ã€‚æ­¤å¤–ï¼Œç³»ç»Ÿçš„å‚æ•°è®¾ç½®ç»è¿‡ç²¾å¿ƒè°ƒæ•´ï¼Œä»¥é€‚åº”ä¸åŒè¯­è¨€å’Œæ–‡æœ¬ç±»å‹çš„éœ€æ±‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSOREåœ¨å¤„ç†HTMLæ•°æ®é›†æ—¶çš„ç²¾åº¦æ˜¾è‘—é«˜äºä¼ ç»Ÿç»“æ„åŒ–æ–¹æ³•ï¼Œæ¥è¿‘å¤§å‹è¯­è¨€æ¨¡å‹çš„æå–ç²¾åº¦ã€‚å…·ä½“è€Œè¨€ï¼ŒSOREåœ¨å¤šç§åœºæ™¯ä¸‹çš„è¡¨ç°å‡ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤šè¯­è¨€æ–‡æœ¬å¤„ç†ä¸­çš„æœ‰æ•ˆæ€§å’Œé«˜æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SOREæ–¹æ³•å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶é€‚ç”¨äºéœ€è¦å¤„ç†å¤§é‡å¤šè¯­è¨€æ–‡æ¡£çš„åœºæ™¯ï¼Œå¦‚æ–°é—»èšåˆã€ç¤¾äº¤åª’ä½“ç›‘æ§å’Œåœ¨çº¿å†…å®¹ç®¡ç†ç­‰ã€‚å…¶é«˜æ•ˆçš„å†—ä½™å†…å®¹å»é™¤èƒ½åŠ›å°†å¤§å¤§æå‡ä¿¡æ¯æå–çš„è´¨é‡å’Œæ•ˆç‡ï¼Œæœªæ¥å¯èƒ½åœ¨æ›´å¤šé¢†åŸŸå¾—åˆ°æ¨å¹¿å’Œåº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Modern text processing pipelines demand robust methods to remove extraneous content while preserving a document's core message. Traditional approaches such as HTML boilerplate extraction or keyword filters often fail in multilingual settings and struggle with context-sensitive nuances, whereas Large Language Models (LLMs) offer improved quality at high computational cost. We introduce SORE (Semantic Outlier Removal), a cost-effective, transparent method that leverages multilingual sentence embeddings and approximate nearest-neighbor search to identify and excise unwanted text segments. By first identifying core content via metadata embedding and then flagging segments that either closely match predefined outlier groups or deviate significantly from the core, SORE achieves near-LLM extraction precision at a fraction of the cost. Experiments on HTML datasets demonstrate that SORE outperforms structural methods and yield high precision in diverse scenarios. Our system is currently deployed in production, processing millions of documents daily across multiple languages while maintaining both efficiency and accuracy. To facilitate reproducibility and further research, we release our implementation and evaluation datasets.

