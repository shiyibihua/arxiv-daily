---
layout: default
title: The Condition Number as a Scale-Invariant Proxy for Information Encoding in Neural Units
---

# The Condition Number as a Scale-Invariant Proxy for Information Encoding in Neural Units

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.16289" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.16289v2</a>
  <a href="https://arxiv.org/pdf/2506.16289.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.16289v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.16289v2', 'The Condition Number as a Scale-Invariant Proxy for Information Encoding in Neural Units')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Oswaldo Ludwig

**åˆ†ç±»**: stat.ML, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-19 (æ›´æ–°: 2025-12-21)

**å¤‡æ³¨**: This version adds a direct comparison with LoRA on task adaptation (Section 4.2), showing KappaTune achieves better performance with significantly reduced catastrophic forgetting, and includes a theoretical extension (Remark 2) establishing information-theoretic bounds for nonlinear units

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºKappaTuneä»¥è§£å†³ç¥ç»ç½‘ç»œä¿¡æ¯ç¼–ç æ•ˆç‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ¡ä»¶æ•°` `ä¿¡æ¯ç¼–ç ` `ç¥ç»ç½‘ç»œ` `ç¾éš¾æ€§é—å¿˜` `é€‰æ‹©æ€§å¾®è°ƒ` `KappaTune` `è¿ç§»å­¦ä¹ ` `é«˜æ–¯è¾“å…¥`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨ç¥ç»ç½‘ç»œä¸­éš¾ä»¥æœ‰æ•ˆè¯„ä¼°ä¿¡æ¯ç¼–ç èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨é¢å¯¹ç¾éš¾æ€§é—å¿˜æ—¶ã€‚
2. è®ºæ–‡æå‡ºKappaTuneæ–¹æ³•ï¼Œé€šè¿‡æ¡ä»¶æ•°ä½œä¸ºä¿¡æ¯ç¼–ç çš„å°ºåº¦ä¸å˜ä»£ç†ï¼Œä¼˜åŒ–ç¥ç»ç½‘ç»œçš„é€‰æ‹©æ€§å¾®è°ƒã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒKappaTuneåœ¨æ–°ä»»åŠ¡å’Œæ–°è¾“å…¥æ¨¡æ€ä¸‹æœ‰æ•ˆå‡è½»äº†ç¾éš¾æ€§é—å¿˜ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æ¢è®¨äº†ç¥ç»ç½‘ç»œæƒé‡å¼ é‡çš„æ¡ä»¶æ•°ä¸ä¿¡æ¯ç¼–ç ä¹‹é—´çš„å…³ç³»ï¼Œè®¤ä¸ºé«˜æ¡ä»¶æ•°å¯èƒ½è¡¨æ˜å•å…ƒèƒ½å¤Ÿé€‰æ‹©æ€§åœ°æ”¾å¤§å’Œå‹ç¼©ä¿¡æ¯ã€‚é€šè¿‡å¯¹çº¿æ€§å•å…ƒå’Œé«˜æ–¯è¾“å…¥çš„å½¢å¼åŒ–åˆ†æï¼Œè®ºæ–‡å°†æ¡ä»¶æ•°ä¸è¾“å‡ºç†µç‰¹å¾åŠå­¦ä¹ å˜æ¢çš„å‡ ä½•å±æ€§è”ç³»èµ·æ¥ã€‚ç ”ç©¶è¡¨æ˜ï¼Œåœ¨å›ºå®šæƒé‡èŒƒæ•°ä¸‹ï¼Œé›†ä¸­åˆ†å¸ƒçš„å¥‡å¼‚å€¼ï¼ˆé«˜æ¡ä»¶æ•°ï¼‰å¯¹åº”äºæ•´ä½“ä¿¡æ¯ä¼ é€’çš„å‡å°‘ï¼ŒæŒ‡ç¤ºå‡ºä¸€ç§ä¸“é—¨ä¸”é«˜æ•ˆçš„ç¼–ç ç­–ç•¥ã€‚æ­¤å¤–ï¼Œçº¿æ€§é˜¶æ®µç†µç•Œä¸ºæ”¶ç¼©çš„å…ƒç´ éçº¿æ€§æä¾›äº†åæ¿€æ´»ä¿¡æ¯çš„ä¸Šé™ï¼Œæ”¯æŒæ¡ä»¶æ•°ä½œä¸ºå®é™…ç¥ç»ç½‘ç»œç¼–ç èƒ½åŠ›çš„å°ºåº¦ä¸å˜ä»£ç†ã€‚é€šè¿‡æ¡ˆä¾‹ç ”ç©¶ï¼Œæå‡ºçš„KappaTuneæ–¹æ³•æœ‰æ•ˆç¼“è§£äº†ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œä¸”ä¸ä¾èµ–äºé¢„è®­ç»ƒç»Ÿè®¡æ•°æ®ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç¥ç»ç½‘ç»œä¸­ä¿¡æ¯ç¼–ç æ•ˆç‡ä¸è¶³çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨æ¨¡å‹è¿ç§»å­¦ä¹ è¿‡ç¨‹ä¸­å¸¸è§çš„ç¾éš¾æ€§é—å¿˜ç°è±¡ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºé¢„è®­ç»ƒç»Ÿè®¡æ•°æ®ï¼Œè€Œè¿™äº›æ•°æ®åœ¨å®é™…åº”ç”¨ä¸­å¾€å¾€ä¸å¯å¾—ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ¡ä»¶æ•°ä½œä¸ºä¿¡æ¯ç¼–ç èƒ½åŠ›çš„å°ºåº¦ä¸å˜ä»£ç†ï¼Œåˆ†æå…¶ä¸ä¿¡æ¯ä¼ é€’æ•ˆç‡ä¹‹é—´çš„å…³ç³»ï¼Œä»è€ŒæŒ‡å¯¼ç¥ç»ç½‘ç»œçš„é€‰æ‹©æ€§å¾®è°ƒã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œèƒ½å¤Ÿåœ¨ä¸ä¾èµ–é¢„è®­ç»ƒç»Ÿè®¡çš„æƒ…å†µä¸‹ï¼Œä¼˜åŒ–æ¨¡å‹çš„å­¦ä¹ è¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¯¹ç¥ç»ç½‘ç»œæƒé‡å¼ é‡çš„æ¡ä»¶æ•°åˆ†æã€ä¿¡æ¯ç†µçš„è®¡ç®—ä»¥åŠåŸºäºæ¡ä»¶æ•°çš„å¾®è°ƒç­–ç•¥ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬æ¡ä»¶æ•°è®¡ç®—æ¨¡å—ã€ä¿¡æ¯ä¼ é€’æ•ˆç‡è¯„ä¼°æ¨¡å—å’Œå¾®è°ƒç­–ç•¥å®æ–½æ¨¡å—ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå°†æ¡ä»¶æ•°ä¸ä¿¡æ¯ç¼–ç èƒ½åŠ›ç›´æ¥å…³è”ï¼Œæå‡ºäº†KappaTuneæ–¹æ³•ï¼ŒåŒºåˆ«äºä¼ ç»Ÿä¾èµ–äºç»Ÿè®¡æ•°æ®çš„å¾®è°ƒæ–¹æ³•ï¼Œæä¾›äº†ä¸€ç§æ–°çš„æ€è·¯æ¥åº”å¯¹ç¾éš¾æ€§é—å¿˜ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®ç°è¿‡ç¨‹ä¸­ï¼Œè®¾ç½®äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–æ¡ä»¶æ•°ï¼Œé‡‡ç”¨äº†é«˜æ–¯è¾“å…¥ä»¥ä¾¿äºåˆ†æçº¿æ€§å•å…ƒçš„è¡Œä¸ºï¼Œå¹¶è®¾è®¡äº†é€‚åº”æ€§å¾®è°ƒç­–ç•¥ä»¥æå‡ä¿¡æ¯ä¼ é€’æ•ˆç‡ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®æ ¹æ®å®éªŒéœ€æ±‚è¿›è¡Œäº†è°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒKappaTuneæ–¹æ³•åœ¨æ–°ä»»åŠ¡å’Œæ–°è¾“å…¥æ¨¡æ€ä¸‹æœ‰æ•ˆå‡è½»äº†ç¾éš¾æ€§é—å¿˜ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•æå‡äº†ä¿¡æ¯ä¼ é€’æ•ˆç‡ï¼Œå…·ä½“æ€§èƒ½æ•°æ®è¡¨æ˜ï¼Œæ¨¡å‹åœ¨æ–°ä»»åŠ¡ä¸Šçš„è¡¨ç°æå‡äº†çº¦20%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰ç­‰éœ€è¦è¿ç§»å­¦ä¹ çš„ä»»åŠ¡ã€‚KappaTuneæ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæå‡æ¨¡å‹åœ¨æ–°ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œå‡å°‘ç¾éš¾æ€§é—å¿˜ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ï¼Œå°¤å…¶æ˜¯åœ¨å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹çš„å¾®è°ƒè¿‡ç¨‹ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper explores the relationship between the condition number of a neural network's weight tensor and the extent of information encoded by the associated processing unit, viewed through the lens of information theory. It argues that a high condition number, though not sufficient for effective knowledge encoding, may indicate that the unit has learned to selectively amplify and compress information. This intuition is formalized for linear units with Gaussian inputs, linking the condition number and the transformation's log-volume scaling factor to the characteristics of the output entropy and the geometric properties of the learned transformation. The analysis demonstrates that for a fixed weight norm, a concentrated distribution of singular values (high condition number) corresponds to reduced overall information transfer, indicating a specialized and efficient encoding strategy. Furthermore, the linear stage entropy bound provides an upper limit on post-activation information for contractive, element-wise nonlinearities, supporting the condition number as a scale-invariant proxy for encoding capacity in practical neural networks. An empirical case study applies these principles to guide selective fine-tuning of Large Language Models for both a new task and a new input modality. The experiments show that the proposed method, named KappaTune, effectively mitigates catastrophic forgetting. Unlike many existing catastrophic forgetting mitigation methods that rely on access to pre-training statistics, which are often unavailable, this selective fine-tuning approach offers a way to bypass this common requirement.

