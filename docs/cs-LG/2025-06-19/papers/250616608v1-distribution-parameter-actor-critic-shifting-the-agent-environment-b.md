---
layout: default
title: Distribution Parameter Actor-Critic: Shifting the Agent-Environment Boundary for Diverse Action Spaces
---

# Distribution Parameter Actor-Critic: Shifting the Agent-Environment Boundary for Diverse Action Spaces

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.16608" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.16608v1</a>
  <a href="https://arxiv.org/pdf/2506.16608.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.16608v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.16608v1', 'Distribution Parameter Actor-Critic: Shifting the Agent-Environment Boundary for Diverse Action Spaces')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiamin He, A. Rupam Mahmood, Martha White

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-19

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåˆ†å¸ƒå‚æ•°æ¼”å‘˜-è¯„è®ºå®¶ä»¥è§£å†³å¤šæ ·åŒ–åŠ¨ä½œç©ºé—´é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `åˆ†å¸ƒå‚æ•°` `æ¼”å‘˜-è¯„è®ºå®¶` `è¿ç»­æ§åˆ¶` `å¤šæ ·åŒ–åŠ¨ä½œç©ºé—´`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤„ç†å¤šæ ·åŒ–åŠ¨ä½œç©ºé—´æ—¶å­˜åœ¨å±€é™ï¼Œå°¤å…¶æ˜¯åœ¨ç¦»æ•£ä¸è¿ç»­åŠ¨ä½œçš„æ··åˆåœºæ™¯ä¸­ã€‚
2. è®ºæ–‡æå‡ºå°†åˆ†å¸ƒå‚æ•°ä½œä¸ºåŠ¨ä½œè¿›è¡Œé‡æ–°å‚æ•°åŒ–ï¼Œè®¾è®¡äº†åˆ†å¸ƒå‚æ•°ç­–ç•¥æ¢¯åº¦ï¼ˆDPPGï¼‰ä»¥é™ä½å­¦ä¹ æ–¹å·®ã€‚
3. DPACåœ¨MuJoCoè¿ç»­æ§åˆ¶ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºTD3ï¼Œä¸”åœ¨ç¦»æ•£åŠ¨ä½œç©ºé—´ä¸­ä¹Ÿå±•ç°å‡ºç«äº‰åŠ›ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå°†åˆ†å¸ƒå‚æ•°è§†ä¸ºåŠ¨ä½œï¼Œé‡æ–°å®šä¹‰äº†æ™ºèƒ½ä½“ä¸ç¯å¢ƒä¹‹é—´çš„è¾¹ç•Œã€‚è¿™ç§é‡æ–°å‚æ•°åŒ–ä½¿å¾—æ–°çš„åŠ¨ä½œç©ºé—´è¿ç»­ï¼Œæ— è®ºåŸå§‹åŠ¨ä½œç±»å‹ï¼ˆç¦»æ•£ã€è¿ç»­ã€æ··åˆç­‰ï¼‰å¦‚ä½•ã€‚åœ¨è¿™ç§æ–°å‚æ•°åŒ–ä¸‹ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§å¹¿ä¹‰çš„ç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦ä¼°è®¡å™¨â€”â€”åˆ†å¸ƒå‚æ•°ç­–ç•¥æ¢¯åº¦ï¼ˆDPPGï¼‰ï¼Œå…¶æ–¹å·®ä½äºåŸå§‹åŠ¨ä½œç©ºé—´ä¸­çš„æ¢¯åº¦ã€‚å°½ç®¡åœ¨åˆ†å¸ƒå‚æ•°ä¸Šå­¦ä¹ è¯„è®ºå®¶é¢ä¸´æ–°æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ’å€¼è¯„è®ºå®¶å­¦ä¹ ï¼ˆICLï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„å¢å¼ºå­¦ä¹ ç­–ç•¥ï¼Œå¾—åˆ°äº†æ¥è‡ªèµŒåšè®¾ç½®çš„å¯ç¤ºã€‚åŸºäºå¼ºåŸºçº¿TD3ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å®ç”¨çš„åŸºäºDPPGçš„æ¼”å‘˜-è¯„è®ºå®¶ç®—æ³•â€”â€”åˆ†å¸ƒå‚æ•°æ¼”å‘˜-è¯„è®ºå®¶ï¼ˆDPACï¼‰ã€‚å®è¯ç»“æœè¡¨æ˜ï¼ŒDPACåœ¨OpenAI Gymå’ŒDeepMind Control Suiteçš„MuJoCoè¿ç»­æ§åˆ¶ä»»åŠ¡ä¸­ä¼˜äºTD3ï¼Œå¹¶åœ¨ç›¸åŒçš„ç¦»æ•£åŠ¨ä½œç©ºé—´ç¯å¢ƒä¸­è¡¨ç°å‡ºç«äº‰åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤šæ ·åŒ–åŠ¨ä½œç©ºé—´ä¸­çš„å±€é™æ€§ï¼Œå°¤å…¶æ˜¯ç¦»æ•£ä¸è¿ç»­åŠ¨ä½œæ··åˆåœºæ™¯ä¸‹çš„å­¦ä¹ æ•ˆç‡ä½ä¸‹é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å°†åˆ†å¸ƒå‚æ•°è§†ä¸ºåŠ¨ä½œï¼Œé‡æ–°å®šä¹‰æ™ºèƒ½ä½“ä¸ç¯å¢ƒçš„è¾¹ç•Œï¼Œä»è€Œä½¿å¾—åŠ¨ä½œç©ºé—´å˜ä¸ºè¿ç»­ã€‚è¿™ç§è®¾è®¡ä½¿å¾—ç­–ç•¥å­¦ä¹ æ›´åŠ çµæ´»ï¼Œèƒ½å¤Ÿé€‚åº”ä¸åŒç±»å‹çš„åŠ¨ä½œã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬åˆ†å¸ƒå‚æ•°ç­–ç•¥æ¢¯åº¦ï¼ˆDPPGï¼‰ä¼°è®¡å™¨å’Œæ’å€¼è¯„è®ºå®¶å­¦ä¹ ï¼ˆICLï¼‰ç­–ç•¥ã€‚DPPGç”¨äºè®¡ç®—æ¢¯åº¦ï¼ŒICLåˆ™ç”¨äºä¼˜åŒ–è¯„è®ºå®¶ç½‘ç»œçš„å­¦ä¹ è¿‡ç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå°†åˆ†å¸ƒå‚æ•°ä½œä¸ºåŠ¨ä½œè¿›è¡Œå¤„ç†ï¼Œè¿™ä¸€æ–¹æ³•æ˜¾è‘—é™ä½äº†å­¦ä¹ è¿‡ç¨‹ä¸­çš„æ–¹å·®ï¼Œå¹¶æé«˜äº†ç­–ç•¥çš„ç¨³å®šæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨DPPGä¸­ï¼Œé‡‡ç”¨äº†æ–°çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„è®¾è®¡ï¼Œä»¥é€‚åº”åˆ†å¸ƒå‚æ•°çš„å­¦ä¹ éœ€æ±‚ï¼ŒåŒæ—¶åœ¨æ’å€¼è¯„è®ºå®¶å­¦ä¹ ä¸­å¼•å…¥äº†åŸºäºèµŒåšè®¾ç½®çš„å¯ç¤ºï¼Œä»¥å¢å¼ºå­¦ä¹ æ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDPACåœ¨MuJoCoè¿ç»­æ§åˆ¶ä»»åŠ¡ä¸­ç›¸è¾ƒäºTD3æå‡äº†æ€§èƒ½ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨å¤šä¸ªä»»åŠ¡ä¸­è·å¾—äº†æ›´é«˜çš„å¹³å‡å›æŠ¥ï¼ŒéªŒè¯äº†å…¶åœ¨å¤æ‚ç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§å’Œä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€æ¸¸æˆæ™ºèƒ½ä½“ä»¥åŠå¤æ‚å†³ç­–ç³»ç»Ÿç­‰ã€‚é€šè¿‡æœ‰æ•ˆå¤„ç†å¤šæ ·åŒ–çš„åŠ¨ä½œç©ºé—´ï¼ŒDPACèƒ½å¤Ÿåœ¨æ›´å¹¿æ³›çš„å®é™…åœºæ™¯ä¸­å®ç°é«˜æ•ˆçš„å†³ç­–åˆ¶å®šï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We introduce a novel reinforcement learning (RL) framework that treats distribution parameters as actions, redefining the boundary between agent and environment. This reparameterization makes the new action space continuous, regardless of the original action type (discrete, continuous, mixed, etc.). Under this new parameterization, we develop a generalized deterministic policy gradient estimator, Distribution Parameter Policy Gradient (DPPG), which has lower variance than the gradient in the original action space. Although learning the critic over distribution parameters poses new challenges, we introduce interpolated critic learning (ICL), a simple yet effective strategy to enhance learning, supported by insights from bandit settings. Building on TD3, a strong baseline for continuous control, we propose a practical DPPG-based actor-critic algorithm, Distribution Parameter Actor-Critic (DPAC). Empirically, DPAC outperforms TD3 in MuJoCo continuous control tasks from OpenAI Gym and DeepMind Control Suite, and demonstrates competitive performance on the same environments with discretized action spaces.

