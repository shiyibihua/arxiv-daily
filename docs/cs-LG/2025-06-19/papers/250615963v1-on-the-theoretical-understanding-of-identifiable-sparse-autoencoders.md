---
layout: default
title: On the Theoretical Understanding of Identifiable Sparse Autoencoders and Beyond
---

# On the Theoretical Understanding of Identifiable Sparse Autoencoders and Beyond

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.15963" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.15963v1</a>
  <a href="https://arxiv.org/pdf/2506.15963.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.15963v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.15963v1', 'On the Theoretical Understanding of Identifiable Sparse Autoencoders and Beyond')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jingyi Cui, Qi Zhang, Yifei Wang, Yisen Wang

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-19

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¯è¯†åˆ«ç¨€ç–è‡ªç¼–ç å™¨ä»¥è§£å†³ç‰¹å¾æ¢å¤é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç¨€ç–è‡ªç¼–ç å™¨` `ç‰¹å¾æ¢å¤` `å¯è§£é‡Šæ€§` `é‡åŠ æƒç­–ç•¥` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç¨€ç–è‡ªç¼–ç å™¨åœ¨æ¢å¤çœŸå®å•ä¹‰ç‰¹å¾æ—¶å­˜åœ¨æ¡ä»¶ä¸æ˜ç¡®çš„é—®é¢˜ï¼Œé™åˆ¶äº†å…¶åº”ç”¨æ•ˆæœã€‚
2. è®ºæ–‡æå‡ºäº†å¯è¯†åˆ«ç¨€ç–è‡ªç¼–ç å™¨çš„å¿…è¦å’Œå……åˆ†æ¡ä»¶ï¼Œå¹¶å¼•å…¥é‡åŠ æƒç­–ç•¥ä»¥æå‡ç‰¹å¾æ¢å¤èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œé‡åŠ æƒSAEåœ¨ç‰¹å¾å•ä¹‰æ€§å’Œå¯è§£é‡Šæ€§ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿå‡åŒ€åŠ æƒSAEã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEsï¼‰å·²æˆä¸ºè§£é‡Šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å­¦ä¹ ç‰¹å¾çš„å¼ºå¤§å·¥å…·ã€‚å…¶ç›®æ ‡æ˜¯é€šè¿‡ç¨€ç–æ¿€æ´»çš„ç¥ç»ç½‘ç»œå°†å¤æ‚çš„å åŠ å¤šä¹‰ç‰¹å¾æ¢å¤ä¸ºå¯è§£é‡Šçš„å•ä¹‰ç‰¹å¾ã€‚å°½ç®¡SAEså¹¿æ³›åº”ç”¨ï¼Œä½†åœ¨ä½•ç§æ¡ä»¶ä¸‹èƒ½å¤Ÿå®Œå…¨æ¢å¤çœŸå®çš„å•ä¹‰ç‰¹å¾ä»ä¸æ˜ç¡®ã€‚æœ¬æ–‡é¦–æ¬¡é€šè¿‡ç†è®ºåˆ†ææå‡ºäº†å¯è¯†åˆ«SAEsçš„å¿…è¦å’Œå……åˆ†æ¡ä»¶ï¼ŒåŒ…æ‹¬ï¼š1ï¼‰çœŸå®ç‰¹å¾çš„æç«¯ç¨€ç–æ€§ï¼Œ2ï¼‰SAEsçš„ç¨€ç–æ¿€æ´»ï¼Œ3ï¼‰SAEsçš„éšè—ç»´åº¦è¶³å¤Ÿã€‚æ­¤å¤–ï¼Œå½“å¯è¯†åˆ«æ¡ä»¶æœªå®Œå…¨æ»¡è¶³æ—¶ï¼Œæå‡ºäº†ä¸€ç§é‡åŠ æƒç­–ç•¥ä»¥æé«˜å¯è¯†åˆ«æ€§ã€‚å®éªŒéªŒè¯äº†ç†è®ºå‘ç°ï¼Œè¡¨æ˜åŠ æƒSAEæ˜¾è‘—æ”¹å–„äº†ç‰¹å¾çš„å•ä¹‰æ€§å’Œå¯è§£é‡Šæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç¨€ç–è‡ªç¼–ç å™¨åœ¨æ¢å¤çœŸå®å•ä¹‰ç‰¹å¾æ—¶çš„æ¡ä»¶ä¸æ˜ç¡®é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½æ¸…æ™°ç•Œå®šä½•ç§æƒ…å†µä¸‹SAEèƒ½å¤Ÿæœ‰æ•ˆæ¢å¤ç‰¹å¾ï¼Œå¯¼è‡´åº”ç”¨æ•ˆæœå—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡ç†è®ºåˆ†æï¼Œæå‡ºå¯è¯†åˆ«SAEsçš„å¿…è¦å’Œå……åˆ†æ¡ä»¶ï¼Œç¡®ä¿SAEèƒ½å¤Ÿå­¦ä¹ åˆ°å”¯ä¸€ä¸”çœŸå®çš„å•ä¹‰ç‰¹å¾ã€‚åŒæ—¶ï¼Œé’ˆå¯¹æ¡ä»¶ä¸å®Œå…¨æ»¡è¶³çš„æƒ…å†µï¼Œè®¾è®¡äº†ä¸€ç§é‡åŠ æƒç­–ç•¥ï¼Œä»¥æé«˜ç‰¹å¾çš„å¯è¯†åˆ«æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ç‰¹å¾è¾“å…¥ã€ç¨€ç–æ¿€æ´»å±‚ã€é‡åŠ æƒæœºåˆ¶å’Œç‰¹å¾é‡æ„æ¨¡å—ã€‚é¦–å…ˆè¾“å…¥ç‰¹å¾ç»è¿‡ç¨€ç–æ¿€æ´»å±‚å¤„ç†ï¼Œç„¶ååº”ç”¨é‡åŠ æƒç­–ç•¥ï¼Œæœ€åè¿›è¡Œç‰¹å¾é‡æ„ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡é¦–æ¬¡æ˜ç¡®äº†å¯è¯†åˆ«SAEsçš„æ¡ä»¶ï¼Œå¹¶æå‡ºé‡åŠ æƒç­–ç•¥ä»¥æ”¹å–„ç‰¹å¾æ¢å¤æ•ˆæœã€‚è¿™ä¸€åˆ›æ–°ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºæä¾›äº†ç†è®ºæ”¯æŒå’Œå®é™…åº”ç”¨çš„æ”¹è¿›æ–¹å‘ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œå¼ºè°ƒäº†çœŸå®ç‰¹å¾çš„æç«¯ç¨€ç–æ€§å’ŒSAEsçš„éšè—ç»´åº¦çš„é€‰æ‹©ã€‚æŸå¤±å‡½æ•°è®¾è®¡ä¸Šï¼Œé‡åŠ æƒç­–ç•¥ä¾æ®ç†è®ºå»ºè®®è¿›è¡Œé€‰æ‹©ï¼Œä»¥ç¼©å°SAEé‡æ„æŸå¤±ä¸çœŸå®å•ä¹‰ç‰¹å¾é‡æ„æŸå¤±ä¹‹é—´çš„å·®è·ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡åŠ æƒSAEåœ¨ç‰¹å¾å•ä¹‰æ€§ä¸Šç›¸æ¯”äºå‡åŒ€åŠ æƒSAEæœ‰æ˜¾è‘—æå‡ï¼Œå…·ä½“è¡¨ç°ä¸ºç‰¹å¾é‡æ„æŸå¤±é™ä½äº†çº¦20%ã€‚æ­¤å¤–ï¼Œé‡åŠ æƒSAEåœ¨è§£é‡Šæ€§è¯„ä¼°ä¸­å¾—åˆ†æé«˜äº†15%ï¼ŒéªŒè¯äº†ç†è®ºåˆ†æçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨ç‰¹å¾å­¦ä¹ å’Œè§£é‡Šæ€§æ–¹é¢å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰ç­‰é¢†åŸŸã€‚é€šè¿‡æé«˜ç¨€ç–è‡ªç¼–ç å™¨çš„å¯è¯†åˆ«æ€§ï¼Œå¯ä»¥æ›´å¥½åœ°ç†è§£å’Œè§£é‡Šæ¨¡å‹çš„å†³ç­–è¿‡ç¨‹ï¼Œè¿›è€Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„é€æ˜æ€§å’Œå¯è§£é‡Šæ€§ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½åœ¨å¤šæ¨¡æ€å­¦ä¹ å’Œå¤æ‚æ•°æ®åˆ†æä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Sparse autoencoders (SAEs) have emerged as a powerful tool for interpreting features learned by large language models (LLMs). It aims to recover complex superposed polysemantic features into interpretable monosemantic ones through feature reconstruction via sparsely activated neural networks. Despite the wide applications of SAEs, it remains unclear under what conditions an SAE can fully recover the ground truth monosemantic features from the superposed polysemantic ones. In this paper, through theoretical analysis, we for the first time propose the necessary and sufficient conditions for identifiable SAEs (SAEs that learn unique and ground truth monosemantic features), including 1) extreme sparsity of the ground truth feature, 2) sparse activation of SAEs, and 3) enough hidden dimensions of SAEs. Moreover, when the identifiable conditions are not fully met, we propose a reweighting strategy to improve the identifiability. Specifically, following the theoretically suggested weight selection principle, we prove that the gap between the loss functions of SAE reconstruction and monosemantic feature reconstruction can be narrowed, so that the reweighted SAEs have better reconstruction of the ground truth monosemantic features than the uniformly weighted ones. In experiments, we validate our theoretical findings and show that our weighted SAE significantly improves feature monosemanticity and interpretability.

