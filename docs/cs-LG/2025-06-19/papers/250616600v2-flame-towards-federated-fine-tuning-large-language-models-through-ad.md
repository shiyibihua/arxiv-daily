---
layout: default
title: FLAME: Towards Federated Fine-Tuning Large Language Models Through Adaptive SMoE
---

# FLAME: Towards Federated Fine-Tuning Large Language Models Through Adaptive SMoE

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.16600" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.16600v2</a>
  <a href="https://arxiv.org/pdf/2506.16600.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.16600v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.16600v2', 'FLAME: Towards Federated Fine-Tuning Large Language Models Through Adaptive SMoE')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Khiem Le, Tuan Tran, Ting Hua, Nitesh V. Chawla

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-19 (æ›´æ–°: 2025-07-14)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFLAMEæ¡†æ¶ä»¥è§£å†³è”é‚¦å­¦ä¹ ä¸­çš„èµ„æºé€‚åº”æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è”é‚¦å­¦ä¹ ` `å¤§è¯­è¨€æ¨¡å‹` `èµ„æºé€‚åº”æ€§` `ç¨€ç–ä¸“å®¶æ··åˆ` `æ¨¡å‹å¾®è°ƒ` `è®¡ç®—èµ„æº` `æ¿€æ´»æ„ŸçŸ¥èšåˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„LoRAè”é‚¦å¾®è°ƒæ–¹æ³•ä¾èµ–äºå‹ç¼©å…¨å±€LoRAçŸ©é˜µï¼Œå¯¼è‡´ä¿¡æ¯æŸå¤±å’Œæ€§èƒ½ä¸‹é™ã€‚
2. FLAMEæ¡†æ¶é€šè¿‡ä¿ç•™å®Œæ•´çš„å…¨å±€LoRAçŸ©é˜µï¼Œå¹¶æ ¹æ®å®¢æˆ·ç«¯éœ€æ±‚è°ƒæ•´æ¿€æ´»çš„ä¸“å®¶æ•°é‡ï¼Œæå‡äº†é€‚åº”æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒFLAMEåœ¨å¤šç§è®¡ç®—ç¯å¢ƒä¸‹å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå±•ç°å‡ºæ›´å¼ºçš„æ€§èƒ½å’Œé€‚åº”æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°æœ‰çš„èµ„æºé€‚åº”æ€§LoRAè”é‚¦å¾®è°ƒæ–¹æ³•å…è®¸å®¢æˆ·ç«¯ä½¿ç”¨å‹ç¼©çš„å…¨å±€LoRAçŸ©é˜µè¿›è¡Œæ¨¡å‹å¾®è°ƒï¼Œä½†è¿™ç§å‹ç¼©ä¼šå¯¼è‡´ä¿¡æ¯æŸå¤±ï¼Œä»è€Œå½±å“æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†FLAMEï¼Œä¸€ä¸ªåŸºäºç¨€ç–ä¸“å®¶æ··åˆ(SMoE)æ¶æ„çš„è”é‚¦å­¦ä¹ æ¡†æ¶ã€‚FLAMEä¿ç•™å®Œæ•´çš„å…¨å±€LoRAçŸ©é˜µï¼Œé€šè¿‡ä¸ºæ¯ä¸ªå®¢æˆ·ç«¯è°ƒæ•´æ¿€æ´»çš„ä¸“å®¶æ•°é‡æ¥å®ç°å®¢æˆ·ç«¯é€‚åº”æ€§ã€‚å°½ç®¡å°†SMoEå¼•å…¥è”é‚¦å­¦ä¹ å¸¦æ¥äº†è¾“å‡ºå¹…åº¦ä¸åŒ¹é…å’Œä¸“å®¶è®­ç»ƒè´¨é‡ä¸å¹³è¡¡ç­‰æŒ‘æˆ˜ï¼ŒFLAMEé€šè¿‡è½»é‡çº§çš„é‡ç¼©æ”¾æœºåˆ¶å’Œæ¿€æ´»æ„ŸçŸ¥èšåˆæ–¹æ¡ˆæœ‰æ•ˆåº”å¯¹è¿™äº›é—®é¢˜ã€‚å®éªŒè¯æ˜ï¼ŒFLAMEåœ¨å¤šç§è®¡ç®—ç¯å¢ƒä¸­å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæä¾›äº†ä¸€ç§ç¨³å¥æœ‰æ•ˆçš„èµ„æºé€‚åº”æ€§è”é‚¦å­¦ä¹ è§£å†³æ–¹æ¡ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„èµ„æºé€‚åº”æ€§LoRAè”é‚¦å¾®è°ƒæ–¹æ³•ç”±äºå‹ç¼©å…¨å±€LoRAçŸ©é˜µï¼Œå¯¼è‡´ä¿¡æ¯æŸå¤±ï¼Œä»è€Œå½±å“æ¨¡å‹æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šFLAMEæ¡†æ¶é€šè¿‡ä¿ç•™å®Œæ•´çš„å…¨å±€LoRAçŸ©é˜µï¼Œå¹¶æ ¹æ®æ¯ä¸ªå®¢æˆ·ç«¯çš„è®¡ç®—èµ„æºåŠ¨æ€è°ƒæ•´æ¿€æ´»çš„ä¸“å®¶æ•°é‡ï¼Œä»¥å®ç°æ›´å¥½çš„é€‚åº”æ€§å’Œæ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šFLAMEçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å…¨å±€LoRAçŸ©é˜µçš„ä¿ç•™ã€åŠ¨æ€æ¿€æ´»ä¸“å®¶çš„æœºåˆ¶ã€è½»é‡çº§é‡ç¼©æ”¾æœºåˆ¶å’Œæ¿€æ´»æ„ŸçŸ¥èšåˆæ–¹æ¡ˆï¼Œç¡®ä¿äº†åœ¨ä¸åŒå®¢æˆ·ç«¯ä¹‹é—´çš„æœ‰æ•ˆåä½œã€‚

**å…³é”®åˆ›æ–°**ï¼šFLAMEçš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥äº†ç¨€ç–ä¸“å®¶æ··åˆ(SMoE)æ¶æ„ï¼Œè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•ä¸­ç”±äºå‹ç¼©å¯¼è‡´çš„ä¿¡æ¯æŸå¤±é—®é¢˜ï¼Œå¹¶é€šè¿‡åŠ¨æ€æ¿€æ´»æœºåˆ¶æé«˜äº†æ¨¡å‹çš„é€‚åº”æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šFLAMEè®¾è®¡äº†è½»é‡çº§çš„é‡ç¼©æ”¾æœºåˆ¶æ¥å¤„ç†éƒ¨åˆ†ä¸“å®¶æ¿€æ´»å¸¦æ¥çš„è¾“å‡ºå¹…åº¦ä¸åŒ¹é…é—®é¢˜ï¼ŒåŒæ—¶é‡‡ç”¨æ¿€æ´»æ„ŸçŸ¥èšåˆæ–¹æ¡ˆä»¥å¹³è¡¡ä¸åŒå®¢æˆ·ç«¯çš„ä¸“å®¶è®­ç»ƒè´¨é‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

FLAMEåœ¨å¤šç§è®¡ç®—ç¯å¢ƒä¸‹çš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œå…¶æ€§èƒ½ä¼˜äºç°æœ‰çš„LoRAè”é‚¦å¾®è°ƒæ–¹æ³•ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°15%-30%ã€‚é€šè¿‡åŠ¨æ€è°ƒæ•´æ¿€æ´»çš„ä¸“å®¶æ•°é‡ï¼ŒFLAMEèƒ½å¤Ÿåœ¨ä¿æŒæ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œé€‚åº”ä¸åŒå®¢æˆ·ç«¯çš„è®¡ç®—èµ„æºã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

FLAMEæ¡†æ¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶åœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šè¿›è¡Œå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„å¾®è°ƒæ—¶ï¼Œå¯ä»¥æœ‰æ•ˆæå‡æ¨¡å‹æ€§èƒ½å’Œé€‚åº”æ€§ã€‚è¯¥ç ”ç©¶ä¸ºæœªæ¥çš„è”é‚¦å­¦ä¹ æä¾›äº†æ–°çš„æ€è·¯ï¼Œå¯èƒ½åœ¨åŒ»ç–—ã€é‡‘èç­‰é¢†åŸŸçš„éšç§ä¿æŠ¤å’Œä¸ªæ€§åŒ–æœåŠ¡ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Existing resource-adaptive LoRA federated fine-tuning methods enable clients to fine-tune models using compressed versions of global LoRA matrices, in order to accommodate various compute resources across clients. This compression requirement will lead to suboptimal performance due to information loss. To address this, we propose FLAME, a novel federated learning framework based on the Sparse Mixture-of-Experts (SMoE) architecture. Unlike prior approaches, FLAME retains full (uncompressed) global LoRA matrices and achieves client-side adaptability by varying the number of activated experts per client. However, incorporating SMoE into federated learning introduces unique challenges, specifically, the mismatch in output magnitude from partial expert activation and the imbalance in expert training quality across clients. FLAME tackles these challenges through a lightweight rescaling mechanism and an activation-aware aggregation scheme. Empirical results across diverse computational settings demonstrate that FLAME consistently outperforms existing methods, providing a robust and effective solution for resource-adaptive federated learning.

