---
layout: default
title: Drag-and-Drop LLMs: Zero-Shot Prompt-to-Weights
---

# Drag-and-Drop LLMs: Zero-Shot Prompt-to-Weights

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.16406" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.16406v1</a>
  <a href="https://arxiv.org/pdf/2506.16406.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.16406v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.16406v1', 'Drag-and-Drop LLMs: Zero-Shot Prompt-to-Weights')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhiyuan Liang, Dongwen Tang, Yuhao Zhou, Xuanlei Zhao, Mingjia Shi, Wangbo Zhao, Zekai Li, Peihao Wang, Konstantin SchÃ¼rholt, Damian Borth, Michael M. Bronstein, Yang You, Zhangyang Wang, Kai Wang

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-19

**å¤‡æ³¨**: We propose a method that can generate LoRA parameters in seconds

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://jerryliang24.github.io/DnD)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDrag-and-Drop LLMsä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹å®šåˆ¶çš„é«˜æˆæœ¬é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å‚æ•°é«˜æ•ˆå¾®è°ƒ` `ä½ç§©é€‚åº”` `æç¤ºæ¡ä»¶ç”Ÿæˆ` `è·¨é¢†åŸŸæ³›åŒ–` `å¤§è¯­è¨€æ¨¡å‹å®šåˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ä»éœ€ä¸ºæ¯ä¸ªä¸‹æ¸¸ä»»åŠ¡è¿›è¡Œå•ç‹¬çš„ä¼˜åŒ–ï¼Œå¯¼è‡´é«˜æ˜‚çš„æ—¶é—´å’Œè®¡ç®—æˆæœ¬ã€‚
2. DnDé€šè¿‡æç¤ºæ¡ä»¶çš„å‚æ•°ç”Ÿæˆï¼Œç›´æ¥å°†æœªæ ‡è®°çš„ä»»åŠ¡æç¤ºæ˜ å°„åˆ°LoRAæƒé‡æ›´æ–°ï¼Œæ¶ˆé™¤äº†æ¯ä¸ªä»»åŠ¡çš„è®­ç»ƒéœ€æ±‚ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDnDåœ¨æ€§èƒ½ä¸Šæ¯”æœ€å¼ºçš„LoRAè®­ç»ƒæ–¹æ³•æå‡äº†30%ï¼Œå¹¶ä¸”åœ¨æœªè§æ•°æ®ä¸Šå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°ä»£å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•å¦‚ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰è™½ç„¶é™ä½äº†å®šåˆ¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æˆæœ¬ï¼Œä½†ä»éœ€é’ˆå¯¹æ¯ä¸ªä¸‹æ¸¸æ•°æ®é›†è¿›è¡Œå•ç‹¬çš„ä¼˜åŒ–ã€‚æœ¬æ–‡æå‡ºäº†Drag-and-Drop LLMsï¼ˆDnDï¼‰ï¼Œä¸€ç§åŸºäºæç¤ºçš„å‚æ•°ç”Ÿæˆå™¨ï¼Œé€šè¿‡å°†å°‘é‡æœªæ ‡è®°çš„ä»»åŠ¡æç¤ºç›´æ¥æ˜ å°„åˆ°LoRAæƒé‡æ›´æ–°ï¼Œæ¶ˆé™¤äº†æ¯ä¸ªä»»åŠ¡çš„è®­ç»ƒéœ€æ±‚ã€‚DnDåœ¨å¤šæ ·åŒ–çš„æç¤º-æ£€æŸ¥ç‚¹å¯¹ä¸Šè®­ç»ƒåï¼Œèƒ½å¤Ÿåœ¨å‡ ç§’å†…ç”Ÿæˆç‰¹å®šä»»åŠ¡çš„å‚æ•°ï¼Œè¡¨ç°å‡ºé«˜è¾¾12,000å€çš„ä½å¼€é”€å’Œåœ¨æœªè§çš„å¸¸è¯†æ¨ç†ã€æ•°å­¦ã€ç¼–ç åŠå¤šæ¨¡æ€åŸºå‡†ä¸Šå¹³å‡æå‡30%çš„æ€§èƒ½ï¼Œä¸”åœ¨è·¨é¢†åŸŸæ³›åŒ–æ–¹é¢è¡¨ç°ç¨³å¥ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒåŸºäºæç¤ºçš„å‚æ•°ç”Ÿæˆæ˜¯å¿«é€Ÿä¸“é—¨åŒ–LLMsçš„å¯è¡Œæ›¿ä»£æ–¹æ¡ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå½“å‰çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•å¦‚LoRAè™½ç„¶é™ä½äº†å®šåˆ¶LLMsçš„æˆæœ¬ï¼Œä½†ä»éœ€ä¸ºæ¯ä¸ªä¸‹æ¸¸ä»»åŠ¡è¿›è¡Œå•ç‹¬çš„ä¼˜åŒ–ï¼Œå¯¼è‡´æ—¶é—´å’Œè®¡ç®—èµ„æºçš„æµªè´¹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDnDçš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡æç¤ºæ¡ä»¶ç”Ÿæˆå‚æ•°ï¼Œç›´æ¥å°†å°‘é‡æœªæ ‡è®°çš„ä»»åŠ¡æç¤ºæ˜ å°„åˆ°LoRAæƒé‡æ›´æ–°ï¼Œä»è€Œæ¶ˆé™¤æ¯ä¸ªä»»åŠ¡çš„è®­ç»ƒè¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDnDçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸€ä¸ªè½»é‡çº§æ–‡æœ¬ç¼–ç å™¨å’Œä¸€ä¸ªçº§è”çš„è¶…å·ç§¯è§£ç å™¨ã€‚æ–‡æœ¬ç¼–ç å™¨å°†æ¯ä¸ªæç¤ºæ‰¹æ¬¡æç‚¼ä¸ºæ¡ä»¶åµŒå…¥ï¼Œè§£ç å™¨åˆ™å°†è¿™äº›åµŒå…¥è½¬æ¢ä¸ºå®Œæ•´çš„LoRAçŸ©é˜µã€‚

**å…³é”®åˆ›æ–°**ï¼šDnDçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶æç¤ºæ¡ä»¶çš„å‚æ•°ç”Ÿæˆæ–¹æ³•ï¼Œè¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„åŸºäºæ¢¯åº¦çš„é€‚åº”æ–¹æ³•æœ¬è´¨ä¸Šä¸åŒï¼Œèƒ½å¤Ÿå¿«é€Ÿç”Ÿæˆä»»åŠ¡ç‰¹å®šçš„å‚æ•°ã€‚

**å…³é”®è®¾è®¡**ï¼šDnDçš„è®¾è®¡åŒ…æ‹¬å¯¹æç¤ºçš„ç¼–ç å’Œè§£ç è¿‡ç¨‹ï¼Œä½¿ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ï¼Œä»¥ç¡®ä¿ç”Ÿæˆçš„LoRAçŸ©é˜µèƒ½å¤Ÿæœ‰æ•ˆæå‡æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚é€šè¿‡åœ¨å¤šæ ·åŒ–çš„æç¤º-æ£€æŸ¥ç‚¹å¯¹ä¸Šè¿›è¡Œè®­ç»ƒï¼ŒDnDèƒ½å¤Ÿå®ç°å¿«é€Ÿçš„å‚æ•°ç”Ÿæˆã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

DnDåœ¨å®éªŒä¸­æ˜¾ç¤ºå‡ºé«˜è¾¾12,000å€çš„ä½å¼€é”€ï¼Œç›¸è¾ƒäºä¼ ç»Ÿçš„å…¨å¾®è°ƒæ–¹æ³•ï¼Œä¸”åœ¨æœªè§çš„å¸¸è¯†æ¨ç†ã€æ•°å­¦ã€ç¼–ç å’Œå¤šæ¨¡æ€åŸºå‡†ä¸Šå¹³å‡æå‡äº†30%çš„æ€§èƒ½ã€‚è¿™äº›ç»“æœè¡¨æ˜DnDåœ¨è·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›ä¸Šä¹Ÿè¡¨ç°å‡ºè‰²ï¼Œå°½ç®¡ä»æœªè§è¿‡ç›®æ ‡æ•°æ®æˆ–æ ‡ç­¾ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿå’Œå¤šæ¨¡æ€å­¦ä¹ ç­‰ã€‚DnDçš„å¿«é€Ÿå‚æ•°ç”Ÿæˆèƒ½åŠ›ä½¿å¾—åœ¨ä¸åŒä»»åŠ¡ä¸Šå®šåˆ¶å¤§å‹è¯­è¨€æ¨¡å‹å˜å¾—æ›´åŠ é«˜æ•ˆï¼Œé™ä½äº†å¼€å‘æˆæœ¬ï¼Œæå‡äº†æ¨¡å‹çš„é€‚åº”æ€§å’Œå®ç”¨æ€§ï¼Œæœªæ¥å¯èƒ½åœ¨å•†ä¸šå’Œå­¦æœ¯ç•Œäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Modern Parameter-Efficient Fine-Tuning (PEFT) methods such as low-rank adaptation (LoRA) reduce the cost of customizing large language models (LLMs), yet still require a separate optimization run for every downstream dataset. We introduce \textbf{Drag-and-Drop LLMs (\textit{DnD})}, a prompt-conditioned parameter generator that eliminates per-task training by mapping a handful of unlabeled task prompts directly to LoRA weight updates. A lightweight text encoder distills each prompt batch into condition embeddings, which are then transformed by a cascaded hyper-convolutional decoder into the full set of LoRA matrices. Once trained in a diverse collection of prompt-checkpoint pairs, DnD produces task-specific parameters in seconds, yielding i) up to \textbf{12,000$\times$} lower overhead than full fine-tuning, ii) average gains up to \textbf{30\%} in performance over the strongest training LoRAs on unseen common-sense reasoning, math, coding, and multimodal benchmarks, and iii) robust cross-domain generalization despite never seeing the target data or labels. Our results demonstrate that prompt-conditioned parameter generation is a viable alternative to gradient-based adaptation for rapidly specializing LLMs. Our project is available at \href{https://jerryliang24.github.io/DnD}{https://jerryliang24.github.io/DnD}.

