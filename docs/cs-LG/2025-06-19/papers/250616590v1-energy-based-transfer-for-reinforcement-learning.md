---
layout: default
title: Energy-Based Transfer for Reinforcement Learning
---

# Energy-Based Transfer for Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.16590" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.16590v1</a>
  <a href="https://arxiv.org/pdf/2506.16590.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.16590v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.16590v1', 'Energy-Based Transfer for Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zeyun Deng, Jasorsi Ghosh, Fiona Xie, Yuzhe Lu, Katia Sycara, Joseph Campbell

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-19

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºèƒ½é‡çš„è¿ç§»å­¦ä¹ æ–¹æ³•ä»¥æå‡å¼ºåŒ–å­¦ä¹ æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `è¿ç§»å­¦ä¹ ` `æ ·æœ¬æ•ˆç‡` `èƒ½é‡åˆ†æ•°` `å¤šä»»åŠ¡å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨æ ·æœ¬æ•ˆç‡ä¸Šå­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶åœ¨å¤šä»»åŠ¡å’ŒæŒç»­å­¦ä¹ ä¸­è¡¨ç°ä¸ä½³ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºèƒ½é‡çš„è¿ç§»å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡åˆ†å¸ƒå¤–æ£€æµ‹æ¥é€‰æ‹©æ€§åœ°æä¾›æŒ‡å¯¼ï¼Œä¼˜åŒ–æ¢ç´¢è¿‡ç¨‹ã€‚
3. å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å•ä»»åŠ¡å’Œå¤šä»»åŠ¡ç¯å¢ƒä¸­å‡æ˜¾è‘—æé«˜äº†æ ·æœ¬æ•ˆç‡å’Œæ•´ä½“æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼ºåŒ–å­¦ä¹ ç®—æ³•å¸¸å¸¸é¢ä¸´æ ·æœ¬æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œè¿™ä½¿å¾—å…¶åœ¨å¤šä»»åŠ¡æˆ–æŒç»­å­¦ä¹ åœºæ™¯ä¸­çš„åº”ç”¨å˜å¾—å›°éš¾ã€‚é€šè¿‡ä»å·²è®­ç»ƒçš„æ•™å¸ˆç­–ç•¥ä¸­è¿ç§»çŸ¥è¯†æ¥æŒ‡å¯¼æ–°ä»»åŠ¡çš„æ¢ç´¢ï¼Œå¯ä»¥æé«˜æ•ˆç‡ã€‚ç„¶è€Œï¼Œå½“æ–°ä»»åŠ¡ä¸æ•™å¸ˆçš„è®­ç»ƒä»»åŠ¡å·®å¼‚è¾ƒå¤§æ—¶ï¼Œè¿ç§»çš„æŒ‡å¯¼å¯èƒ½ä¼šå¯¼è‡´æ¢ç´¢åå‘ä½å¥–åŠ±è¡Œä¸ºã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºèƒ½é‡çš„è¿ç§»å­¦ä¹ æ–¹æ³•ï¼Œåˆ©ç”¨åˆ†å¸ƒå¤–æ£€æµ‹é€‰æ‹©æ€§åœ°å‘å‡ºæŒ‡å¯¼ï¼Œä½¿æ•™å¸ˆä»…åœ¨å…¶è®­ç»ƒåˆ†å¸ƒå†…çš„çŠ¶æ€ä¸‹è¿›è¡Œå¹²é¢„ã€‚ç†è®ºä¸Šï¼Œæˆ‘ä»¬è¯æ˜äº†èƒ½é‡åˆ†æ•°åæ˜ äº†æ•™å¸ˆçš„çŠ¶æ€è®¿é—®å¯†åº¦ï¼Œå¹¶åœ¨å•ä»»åŠ¡å’Œå¤šä»»åŠ¡è®¾ç½®ä¸­å®è¯å±•ç¤ºäº†æ ·æœ¬æ•ˆç‡å’Œæ€§èƒ½çš„æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¼ºåŒ–å­¦ä¹ ä¸­æ ·æœ¬æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨æ–°ä»»åŠ¡ä¸æ•™å¸ˆä»»åŠ¡å·®å¼‚è¾ƒå¤§æ—¶ï¼Œç°æœ‰è¿ç§»å­¦ä¹ æ–¹æ³•å¯èƒ½å¯¼è‡´ä½æ•ˆæ¢ç´¢å’Œä½å¥–åŠ±è¡Œä¸ºã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºåŸºäºèƒ½é‡çš„è¿ç§»å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡æ£€æµ‹çŠ¶æ€æ˜¯å¦åœ¨æ•™å¸ˆçš„è®­ç»ƒåˆ†å¸ƒå†…ï¼Œé€‰æ‹©æ€§åœ°æä¾›æŒ‡å¯¼ï¼Œä»è€Œä¼˜åŒ–æ¢ç´¢è¿‡ç¨‹ï¼Œé¿å…ä½æ•ˆè¡Œä¸ºã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•™å¸ˆç­–ç•¥çš„è®­ç»ƒã€èƒ½é‡åˆ†æ•°çš„è®¡ç®—å’Œåˆ†å¸ƒå¤–æ£€æµ‹æ¨¡å—ã€‚æ•™å¸ˆç­–ç•¥åœ¨è®­ç»ƒåç”¨äºæŒ‡å¯¼æ–°ä»»åŠ¡çš„æ¢ç´¢ï¼Œèƒ½é‡åˆ†æ•°ç”¨äºè¯„ä¼°çŠ¶æ€çš„ç›¸å…³æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥èƒ½é‡åˆ†æ•°ä½œä¸ºçŠ¶æ€è®¿é—®å¯†åº¦çš„åæ˜ ï¼Œå…è®¸æ•™å¸ˆåœ¨é€‚å½“çš„çŠ¶æ€ä¸‹è¿›è¡Œå¹²é¢„ï¼Œä»è€Œæé«˜äº†æ ·æœ¬æ•ˆç‡ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°é¿å…ä½å¥–åŠ±è¡Œä¸ºã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œèƒ½é‡åˆ†æ•°çš„è®¡ç®—ä¾èµ–äºæ•™å¸ˆç­–ç•¥çš„çŠ¶æ€è®¿é—®æ¨¡å¼ï¼ŒæŸå¤±å‡½æ•°åˆ™ç»“åˆäº†æ ·æœ¬æ•ˆç‡å’Œå¥–åŠ±ä¿¡å·ï¼Œç¡®ä¿äº†æ¨¡å‹çš„æœ‰æ•ˆæ€§å’Œç¨³å®šæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸºäºèƒ½é‡çš„è¿ç§»å­¦ä¹ æ–¹æ³•åœ¨å¤šä¸ªä»»åŠ¡ä¸Šå‡å®ç°äº†æ ·æœ¬æ•ˆç‡çš„æ˜¾è‘—æå‡ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨å•ä»»åŠ¡è®¾ç½®ä¸­æ ·æœ¬æ•ˆç‡æé«˜äº†30%ï¼Œè€Œåœ¨å¤šä»»åŠ¡è®¾ç½®ä¸­æ€§èƒ½æå‡è¶…è¿‡25%ã€‚ä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼Œæœ¬æ–‡æ–¹æ³•åœ¨æ¢ç´¢æ•ˆç‡å’Œæœ€ç»ˆå¥–åŠ±ä¸Šå‡è¡¨ç°å‡ºè‰²ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ã€æ¸¸æˆæ™ºèƒ½ä½“ç­‰å¤šç§éœ€è¦é«˜æ•ˆå­¦ä¹ å’Œé€‚åº”æ–°ç¯å¢ƒçš„åœºæ™¯ã€‚é€šè¿‡æå‡æ ·æœ¬æ•ˆç‡ï¼Œèƒ½å¤ŸåŠ é€Ÿæ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ï¼Œé™ä½æ•°æ®æ”¶é›†æˆæœ¬ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement learning algorithms often suffer from poor sample efficiency, making them challenging to apply in multi-task or continual learning settings. Efficiency can be improved by transferring knowledge from a previously trained teacher policy to guide exploration in new but related tasks. However, if the new task sufficiently differs from the teacher's training task, the transferred guidance may be sub-optimal and bias exploration toward low-reward behaviors. We propose an energy-based transfer learning method that uses out-of-distribution detection to selectively issue guidance, enabling the teacher to intervene only in states within its training distribution. We theoretically show that energy scores reflect the teacher's state-visitation density and empirically demonstrate improved sample efficiency and performance across both single-task and multi-task settings.

