---
layout: default
title: Optimizing MoE Routers: Design, Implementation, and Evaluation in Transformer Models
---

# Optimizing MoE Routers: Design, Implementation, and Evaluation in Transformer Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.16419" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.16419v1</a>
  <a href="https://arxiv.org/pdf/2506.16419.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.16419v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.16419v1', 'Optimizing MoE Routers: Design, Implementation, and Evaluation in Transformer Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Daniel Fidel Harvey, George Weale, Berk Yilmaz

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-19

**å¤‡æ³¨**: All authors contributed equally. 11 pages, 6 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ä¼˜åŒ–MoEè·¯ç”±å™¨ä»¥æå‡Transformeræ¨¡å‹æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ··åˆä¸“å®¶` `è·¯ç”±å™¨ä¼˜åŒ–` `Transformeræ¨¡å‹` `æ¨¡å‹æ€§èƒ½` `å¤§è§„æ¨¡éƒ¨ç½²`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„MoEæ¶æ„åœ¨è·¯ç”±æ¨¡å—æ€§èƒ½ä¸ä½³æ—¶ï¼Œå¯èƒ½å¯¼è‡´è´Ÿè½½ä¸å‡å’Œå‡†ç¡®æ€§ä¸‹é™ã€‚
2. æœ¬æ–‡æå‡ºäº†å…­ç§ä¸åŒçš„è·¯ç”±å™¨æ¶æ„ï¼Œæ—¨åœ¨ä¼˜åŒ–ä»¤ç‰Œåˆ†é…ï¼Œæé«˜æ¨¡å‹æ€§èƒ½ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œçº¿æ€§è·¯ç”±å™¨é€Ÿåº¦å¿«ï¼Œè€ŒMLPå’Œæ³¨æ„åŠ›è·¯ç”±å™¨åœ¨è¡¨ç°åŠ›ä¸Šæ›´å…·ä¼˜åŠ¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„æå‡äº†å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„å¯æ‰©å±•æ€§ï¼Œä½†å…¶æ€§èƒ½ä¾èµ–äºå°†ä»¤ç‰Œç§»åŠ¨åˆ°ä¸“é—¨ä¸“å®¶çš„è·¯ç”±æ¨¡å—ã€‚ç³Ÿç³•çš„è·¯ç”±å¯èƒ½å¯¼è‡´è´Ÿè½½ä¸å¹³è¡¡å’Œå‡†ç¡®æ€§é™ä½ã€‚æœ¬æ–‡è®¾è®¡å¹¶å®ç°äº†ä¸åŒçš„è·¯ç”±å™¨æ¶æ„ï¼Œä»¥è§£å†³è¿™äº›é™åˆ¶ã€‚æˆ‘ä»¬å®éªŒäº†å…­ç§ä¸åŒçš„è·¯ç”±å™¨å˜ä½“ï¼ŒåŒ…æ‹¬çº¿æ€§ã€æ³¨æ„åŠ›ã€å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰ã€æ··åˆã€å“ˆå¸Œå’Œæ–°æå‡ºçš„MLP-Hadamardã€‚é€šè¿‡ä½¿ç”¨BERTå’ŒQwen1.5-MoEæ¨¡å‹ï¼Œæˆ‘ä»¬å¯¹è¿™äº›è·¯ç”±å™¨è¿›è¡Œäº†å‚æ•°æ•ˆç‡ã€æ¨ç†å»¶è¿Ÿã€è·¯ç”±ç†µå’Œä¸“å®¶åˆ©ç”¨æ¨¡å¼çš„ç‰¹å¾åˆ†æã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºå‡ºæ˜æ˜¾çš„æƒè¡¡ï¼šçº¿æ€§è·¯ç”±å™¨æä¾›é€Ÿåº¦ï¼Œè€ŒMLPå’Œæ³¨æ„åŠ›è·¯ç”±å™¨åˆ™æä¾›æ›´å¤§çš„è¡¨ç°åŠ›ã€‚MLP-Hadamardè·¯ç”±å™¨åœ¨ç»“æ„åŒ–ç¨€ç–è·¯ç”±æ–¹é¢å±•ç°äº†ç‹¬ç‰¹èƒ½åŠ›ã€‚æˆ‘ä»¬æˆåŠŸæ›¿æ¢å¹¶å¾®è°ƒäº†å¤æ‚çš„é‡åŒ–Qwen1.5-MoEæ¨¡å‹ä¸­çš„è‡ªå®šä¹‰è·¯ç”±å™¨ã€‚æ­¤ç ”ç©¶æä¾›äº†MoEè·¯ç”±å™¨è®¾è®¡çš„æ¯”è¾ƒåˆ†æï¼Œå¹¶ä¸ºä¼˜åŒ–å…¶æ€§èƒ½ä»¥å®ç°é«˜æ•ˆçš„å¤§è§„æ¨¡æ¨¡å‹éƒ¨ç½²æä¾›äº†è§è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„ä¸­è·¯ç”±æ¨¡å—æ€§èƒ½ä¸è¶³çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å¯èƒ½å¯¼è‡´è´Ÿè½½ä¸å‡å’Œæ¨¡å‹å‡†ç¡®æ€§ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡è®¾è®¡å’Œå®ç°å¤šç§è·¯ç”±å™¨æ¶æ„ï¼Œä¼˜åŒ–ä»¤ç‰Œçš„åˆ†é…è¿‡ç¨‹ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ•´ä½“æ€§èƒ½å’Œæ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶ä¸­å®ç°äº†å…­ç§è·¯ç”±å™¨å˜ä½“ï¼ŒåŒ…æ‹¬çº¿æ€§ã€æ³¨æ„åŠ›ã€MLPã€æ··åˆã€å“ˆå¸Œå’Œæ–°æå‡ºçš„MLP-Hadamardï¼Œé‡‡ç”¨BERTå’ŒQwen1.5-MoEæ¨¡å‹è¿›è¡Œè¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šMLP-Hadamardè·¯ç”±å™¨åœ¨ç»“æ„åŒ–ç¨€ç–è·¯ç”±æ–¹é¢å±•ç°äº†ç‹¬ç‰¹èƒ½åŠ›ï¼ŒåŒºåˆ«äºä¼ ç»Ÿçš„è·¯ç”±å™¨è®¾è®¡ï¼Œæä¾›äº†æ›´é«˜çš„å‚æ•°æ•ˆç‡å’Œæ¨ç†é€Ÿåº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡ç‚¹è€ƒè™‘äº†è·¯ç”±å™¨çš„å‚æ•°è®¾ç½®ã€æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ï¼Œç¡®ä¿ä¸åŒè·¯ç”±å™¨åœ¨æ¨ç†å»¶è¿Ÿå’Œä¸“å®¶åˆ©ç”¨ç‡æ–¹é¢çš„ä¼˜åŒ–ã€‚é€šè¿‡å¾®è°ƒè‡ªå®šä¹‰è·¯ç”±å™¨ï¼Œæå‡äº†å¤æ‚æ¨¡å‹çš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œçº¿æ€§è·¯ç”±å™¨åœ¨æ¨ç†é€Ÿåº¦ä¸Šå…·æœ‰æ˜æ˜¾ä¼˜åŠ¿ï¼Œè€ŒMLPå’Œæ³¨æ„åŠ›è·¯ç”±å™¨åœ¨è¡¨ç°åŠ›ä¸Šæ›´ä¸ºå‡ºè‰²ã€‚MLP-Hadamardè·¯ç”±å™¨åœ¨ç»“æ„åŒ–ç¨€ç–è·¯ç”±æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œæä¾›äº†æ–°çš„ä¼˜åŒ–æ€è·¯ã€‚æ•´ä½“ä¸Šï¼Œæ¨¡å‹çš„å‚æ•°æ•ˆç‡å’Œæ¨ç†å»¶è¿Ÿå¾—åˆ°äº†æ˜¾è‘—æ”¹å–„ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„ä¼˜åŒ–å’Œéƒ¨ç½²ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é«˜æ•ˆæ¨ç†å’Œå‡†ç¡®æ€§çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­ã€‚é€šè¿‡ä¼˜åŒ–è·¯ç”±å™¨è®¾è®¡ï¼Œå¯ä»¥åœ¨å®é™…åº”ç”¨ä¸­æ˜¾è‘—æå‡æ¨¡å‹çš„å“åº”é€Ÿåº¦å’Œå¤„ç†èƒ½åŠ›ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Mixture of Experts (MoE) architectures increase large language model scalability, yet their performance depends on the router module that moves tokens to specialized experts. Bad routing can load imbalance and reduced accuracy. This project designed and implemented different router architectures within Transformer models to fix these limitations. We experimented with six distinct router variants Linear, Attention, Multi-Layer Perceptron (MLP), Hybrid, Hash, and our new MLP-Hadamard. We characterized these routers using BERT and the Qwen1.5-MoE model, looking at parameter efficiency, inference latency, routing entropy, and expert utilization patterns. Our evaluations showed distinct trade-offs: Linear routers offer speed, while MLP and Attention routers provide greater expressiveness. The MLP-Hadamard router shows a unique capability for structured, sparse routing. We successfully replaced and fine-tuned custom routers within the complex, quantized Qwen1.5-MoE model. This work provides a comparative analysis of MoE router designs and offers insights into optimizing their performance for efficient and effective large-scale model deployment.

