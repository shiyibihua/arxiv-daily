---
layout: default
title: Robust Reward Modeling via Causal Rubrics
---

# Robust Reward Modeling via Causal Rubrics

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.16507" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.16507v1</a>
  <a href="https://arxiv.org/pdf/2506.16507.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.16507v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.16507v1', 'Robust Reward Modeling via Causal Rubrics')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Pragya Srivastava, Harman Singh, Rahul Madhavan, Gandharv Patil, Sravanti Addepalli, Arun Suggala, Rengarajan Aravamudhan, Soumya Sharma, Anirban Laha, Aravindan Raghuveer, Karthikeyan Shanmugam, Doina Precup

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-19

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCromeæ¡†æ¶ä»¥è§£å†³å¥–åŠ±æ¨¡å‹ä¸­çš„å¥–åŠ±é»‘å®¢é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¥–åŠ±æ¨¡å‹` `å› æœå»ºæ¨¡` `äººç±»åé¦ˆ` `å¤§å‹è¯­è¨€æ¨¡å‹` `é²æ£’æ€§` `æ·±åº¦å­¦ä¹ ` `æ¨¡å‹å¯¹é½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¥–åŠ±æ¨¡å‹å®¹æ˜“å—åˆ°å¥–åŠ±é»‘å®¢çš„å½±å“ï¼Œå¯¼è‡´æ¨¡å‹ä¾èµ–è¡¨é¢ç‰¹å¾è€ŒéçœŸæ­£çš„å› æœå› ç´ ã€‚
2. Cromeæ¡†æ¶é€šè¿‡å› æœå¢å¼ºå’Œä¸­æ€§å¢å¼ºæŠ€æœ¯ï¼Œæ—¨åœ¨æé«˜æ¨¡å‹å¯¹å› æœå±æ€§çš„æ•æ„Ÿæ€§å’Œå¯¹è™šå‡å±æ€§çš„é²æ£’æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒCromeåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºæ ‡å‡†åŸºçº¿ï¼Œå¹³å‡å‡†ç¡®ç‡æå‡è¾¾5.4%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¥–åŠ±æ¨¡å‹ï¼ˆRMsï¼‰æ˜¯é€šè¿‡äººç±»åé¦ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œå¯¹é½çš„åŸºç¡€ï¼Œä½†å®ƒä»¬å¸¸å¸¸å—åˆ°å¥–åŠ±é»‘å®¢çš„å½±å“ã€‚ç°æœ‰æ¨¡å‹å®¹æ˜“ä¾èµ–è¡¨é¢æˆ–è™šå‡çš„å±æ€§ï¼Œå¦‚å“åº”é•¿åº¦æˆ–æ ¼å¼ï¼Œè¯¯å°†è¿™äº›ä»è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ åˆ°çš„ç›¸å…³æ€§è§†ä¸ºè´¨é‡çš„çœŸæ­£å› æœé©±åŠ¨å› ç´ ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†Cromeï¼ˆå› æœé²æ£’å¥–åŠ±å»ºæ¨¡ï¼‰ï¼Œä¸€ä¸ªåŸºäºæ˜ç¡®å› æœæ¨¡å‹çš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨å‡è½»å¥–åŠ±é»‘å®¢ç°è±¡ã€‚Cromeåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é‡‡ç”¨äº†åˆæˆçš„ç›®æ ‡å¢å¼ºæŠ€æœ¯ï¼ŒåŒ…æ‹¬å› æœå¢å¼ºå’Œä¸­æ€§å¢å¼ºï¼Œæ˜¾è‘—æå‡äº†åœ¨RewardBenchä¸Šçš„è¡¨ç°ï¼Œå¹³å‡å‡†ç¡®ç‡æé«˜äº†5.4%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¥–åŠ±æ¨¡å‹åœ¨å¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹æ—¶é¢ä¸´çš„å¥–åŠ±é»‘å®¢é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æœ‰æ•ˆåŒºåˆ†å› æœå±æ€§ä¸è™šå‡å±æ€§ï¼Œå¯¼è‡´æ¨¡å‹è¡¨ç°è„†å¼±ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCromeæ¡†æ¶é€šè¿‡å¼•å…¥å› æœå¢å¼ºå’Œä¸­æ€§å¢å¼ºï¼Œåˆ†åˆ«å¼ºåŒ–æ¨¡å‹å¯¹å› æœå±æ€§çš„æ•æ„Ÿæ€§å’Œå¯¹è™šå‡å±æ€§çš„é²æ£’æ€§ã€‚é€šè¿‡è¿™ç§è®¾è®¡ï¼Œæ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°è¯†åˆ«å’Œåˆ©ç”¨çœŸæ­£çš„å› æœé©±åŠ¨å› ç´ ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCromeçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šå› æœå¢å¼ºæ¨¡å—å’Œä¸­æ€§å¢å¼ºæ¨¡å—ã€‚å› æœå¢å¼ºæ¨¡å—ç”Ÿæˆåœ¨ç‰¹å®šå› æœå±æ€§ä¸Šæœ‰å·®å¼‚çš„æ ·æœ¬å¯¹ï¼Œè€Œä¸­æ€§å¢å¼ºæ¨¡å—åˆ™ç”Ÿæˆåœ¨è™šå‡å±æ€§ä¸Šæœ‰å·®å¼‚çš„æ ·æœ¬å¯¹ã€‚

**å…³é”®åˆ›æ–°**ï¼šCromeçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶åˆæˆç›®æ ‡å¢å¼ºæŠ€æœ¯ï¼Œèƒ½å¤Ÿåœ¨æ²¡æœ‰è™šå‡å› ç´ çŸ¥è¯†çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡æŸ¥è¯¢ä¸€ä¸ªoracle LLMç”Ÿæˆå¢å¼ºæ ·æœ¬ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„å¥–åŠ±æ¨¡å‹è®­ç»ƒæ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒCromeé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ¨¡å‹å¯¹å› æœå±æ€§çš„æ•æ„Ÿæ€§ï¼ŒåŒæ—¶ç¡®ä¿æ¨¡å‹å¯¹è™šå‡å±æ€§çš„é²æ£’æ€§ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨å®éªŒéƒ¨åˆ†è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

Cromeåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯åœ¨RewardBenchä¸Šï¼Œå¹³å‡å‡†ç¡®ç‡æé«˜äº†5.4%ã€‚åœ¨ç‰¹å®šç±»åˆ«ä¸­ï¼ŒCromeçš„æå‡å¹…åº¦è¾¾åˆ°13.2%å’Œ7.2%ã€‚è¿™äº›ç»“æœè¡¨æ˜Cromeåœ¨åº”å¯¹å¥–åŠ±é»‘å®¢æ–¹é¢çš„æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Cromeæ¡†æ¶åœ¨å¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹æ–¹é¢å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é«˜è´¨é‡äººç±»åé¦ˆçš„ä»»åŠ¡ä¸­ï¼Œå¦‚å¯¹è¯ç³»ç»Ÿå’Œå†…å®¹ç”Ÿæˆã€‚é€šè¿‡æé«˜å¥–åŠ±æ¨¡å‹çš„é²æ£’æ€§ï¼ŒCromeèƒ½å¤Ÿå¸®åŠ©å¼€å‘æ›´å¯é çš„AIç³»ç»Ÿï¼Œå‡å°‘å› å¥–åŠ±é»‘å®¢å¯¼è‡´çš„åå·®ï¼Œä»è€Œæå‡ç”¨æˆ·ä½“éªŒå’Œå®‰å…¨æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reward models (RMs) are fundamental to aligning Large Language Models (LLMs) via human feedback, yet they often suffer from reward hacking. They tend to latch on to superficial or spurious attributes, such as response length or formatting, mistaking these cues learned from correlations in training data for the true causal drivers of quality (e.g., factuality, relevance). This occurs because standard training objectives struggle to disentangle these factors, leading to brittle RMs and misaligned policies. We introduce Crome (Causally Robust Reward Modeling), a novel framework grounded in an explicit causal model designed to mitigate reward hacking. Crome employs the following synthetic targeted augmentations during training: (1) Causal Augmentations, which are pairs that differ along specific causal attributes, to enforce sensitivity along each causal attribute individually, and (2) Neutral Augmentations, which are tie-label pairs varying primarily in spurious attributes, to enforce invariance along spurious attributes. Notably, our augmentations are produced without any knowledge of spurious factors, via answer interventions only along causal rubrics, that are identified by querying an oracle LLM. Empirically, Crome significantly outperforms standard baselines on RewardBench, improving average accuracy by up to 5.4% and achieving gains of up to 13.2% and 7.2% in specific categories. The robustness of Crome is further testified by the consistent gains obtained in a Best-of-N inference setting across increasing N, across various benchmarks, including the popular RewardBench (covering chat, chat-hard, safety, and reasoning tasks), the safety-focused WildGuardTest, and the reasoning-specific GSM8k.

