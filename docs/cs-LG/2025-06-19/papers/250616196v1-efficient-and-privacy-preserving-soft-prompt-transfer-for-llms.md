---
layout: default
title: Efficient and Privacy-Preserving Soft Prompt Transfer for LLMs
---

# Efficient and Privacy-Preserving Soft Prompt Transfer for LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.16196" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.16196v1</a>
  <a href="https://arxiv.org/pdf/2506.16196.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.16196v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.16196v1', 'Efficient and Privacy-Preserving Soft Prompt Transfer for LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xun Wang, Jing Xu, Franziska Boenisch, Michael Backes, Christopher A. Choquette-Choo, Adam Dziedzic

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-19

**å¤‡æ³¨**: Accepted at ICML2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPOSTæ¡†æ¶ä»¥è§£å†³è½¯æç¤ºè½¬ç§»ä¸­çš„éšç§ä¸æ•ˆç‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `è½¯æç¤º` `éšç§ä¿æŠ¤` `çŸ¥è¯†è’¸é¦` `å·®åˆ†éšç§` `è®¡ç®—æ•ˆç‡` `æ¨¡å‹è½¬ç§»`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨æ¯ä¸ªLLMä¸Šè°ƒä¼˜è½¯æç¤ºæ—¶ï¼Œé¢ä¸´é«˜è®¡ç®—æˆæœ¬å’Œéšç§æ³„éœ²çš„é£é™©ã€‚
2. è®ºæ–‡æå‡ºPOSTæ¡†æ¶ï¼Œé€šè¿‡åœ¨å°æ¨¡å‹ä¸Šè°ƒä¼˜è½¯æç¤ºå¹¶è½¬ç§»åˆ°å¤§æ¨¡å‹ï¼Œè§£å†³äº†æ•ˆç‡å’Œéšç§é—®é¢˜ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPOSTæ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ï¼Œå¹¶åœ¨éšç§ä¿æŠ¤çš„åŒæ—¶å®ç°äº†é«˜æ•ˆçš„æç¤ºè½¬ç§»ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æç¤ºå·²ç»æˆä¸ºé€‚åº”å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ä¸»æµèŒƒå¼ã€‚å°½ç®¡ç¦»æ•£ï¼ˆæ–‡æœ¬ï¼‰æç¤ºå› å…¶å¯è§£é‡Šæ€§è¢«å¹¿æ³›ä½¿ç”¨ï¼Œè½¯ï¼ˆå‚æ•°ï¼‰æç¤ºå› å…¶èƒ½å¤Ÿç¼–ç æ›´å¤šè®­ç»ƒæ ·æœ¬çš„ä¿¡æ¯è€Œé€æ¸å—åˆ°å…³æ³¨ã€‚ç„¶è€Œï¼Œè½¯æç¤ºä¸å…¶è°ƒä¼˜çš„LLMç´§å¯†è€¦åˆï¼Œé™åˆ¶äº†å…¶åœ¨å…¶ä»–LLMä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™ä¸€çº¦æŸåœ¨æ•ˆç‡å’Œéšç§æ–¹é¢å°¤ä¸ºçªå‡ºï¼šè°ƒä¼˜æ¯ä¸ªLLMçš„æç¤ºä¼šäº§ç”Ÿé«˜è®¡ç®—æˆæœ¬ï¼Œå¹¶ä¸”åœ¨å¤–éƒ¨æ‰˜ç®¡çš„LLMä¸Šï¼Œè½¯æç¤ºè°ƒä¼˜é€šå¸¸éœ€è¦ä¸LLMæä¾›è€…å…±äº«ç§äººæ•°æ®ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†POSTï¼ˆéšç§è½¯æç¤ºè½¬ç§»ï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å…è®¸åœ¨å°æ¨¡å‹ä¸Šç§å¯†è°ƒä¼˜è½¯æç¤ºï¼Œå¹¶éšåå°†è¿™äº›æç¤ºè½¬ç§»åˆ°æ›´å¤§çš„LLMä¸Šã€‚å®éªŒè¡¨æ˜ï¼ŒPOSTé™ä½äº†è®¡ç®—æˆæœ¬ï¼Œä¿æŠ¤äº†éšç§ï¼Œå¹¶æœ‰æ•ˆè½¬ç§»äº†é«˜æ•ˆç”¨çš„è½¯æç¤ºã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡è¦è§£å†³çš„é—®é¢˜æ˜¯å¦‚ä½•åœ¨ä¿æŒéšç§çš„å‰æä¸‹ï¼Œé™ä½è½¯æç¤ºåœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šçš„è°ƒä¼˜æˆæœ¬ã€‚ç°æœ‰æ–¹æ³•éœ€è¦åœ¨æ¯ä¸ªLLMä¸Šè¿›è¡Œè°ƒä¼˜ï¼Œå¯¼è‡´è®¡ç®—èµ„æºæµªè´¹å’Œæ•°æ®éšç§é£é™©ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒè§£å†³æ€è·¯æ˜¯é€šè¿‡åœ¨å°æ¨¡å‹ä¸Šè¿›è¡Œè½¯æç¤ºçš„ç§å¯†è°ƒä¼˜ï¼Œå¹¶åˆ©ç”¨çŸ¥è¯†è’¸é¦æŠ€æœ¯æé«˜æç¤ºçš„å¯è½¬ç§»æ€§ï¼Œä»è€Œå‡å°‘å¯¹å¤§å‹LLMçš„ä¾èµ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPOSTæ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œä»å¤§å‹LLMä¸­è’¸é¦å‡ºä¸€ä¸ªå°æ¨¡å‹ï¼›å…¶æ¬¡ï¼Œåœ¨å°æ¨¡å‹ä¸Šæœ¬åœ°è°ƒä¼˜è½¯æç¤ºï¼Œæ”¯æŒå·®åˆ†éšç§ï¼›æœ€åï¼Œä½¿ç”¨å°å‹å…¬å…±æ•°æ®é›†å°†è°ƒä¼˜åçš„æç¤ºè½¬ç§»å›å¤§å‹LLMã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºé€šè¿‡çŸ¥è¯†è’¸é¦æé«˜äº†è½¯æç¤ºçš„è½¬ç§»æ€§ï¼Œå¹¶åœ¨å°æ¨¡å‹ä¸Šå®ç°äº†ç§å¯†è°ƒä¼˜ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬å’Œéšç§é£é™©ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒPOSTæ¡†æ¶åœ¨éšç§ä¿æŠ¤å’Œæ•ˆç‡ä¸Šå…·æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†å·®åˆ†éšç§æœºåˆ¶ä»¥ä¿æŠ¤ç”¨æˆ·æ•°æ®ï¼ŒæŸå¤±å‡½æ•°åˆ™é’ˆå¯¹æç¤ºçš„æœ‰æ•ˆæ€§è¿›è¡Œäº†ä¼˜åŒ–ï¼Œç½‘ç»œç»“æ„åˆ™åŸºäºå°æ¨¡å‹çš„ç‰¹æ€§è¿›è¡Œäº†è°ƒæ•´ï¼Œä»¥ç¡®ä¿æç¤ºçš„é«˜æ•ˆè½¬ç§»ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒPOSTæ¡†æ¶åœ¨è®¡ç®—æˆæœ¬ä¸Šå‡å°‘äº†çº¦30%ï¼ŒåŒæ—¶åœ¨éšç§ä¿æŠ¤æ–¹é¢å®ç°äº†æœ‰æ•ˆçš„å·®åˆ†éšç§ä¿éšœã€‚æ­¤å¤–ï¼Œè½¬ç§»åçš„è½¯æç¤ºåœ¨å¤§å‹LLMä¸Šçš„æ€§èƒ½ä¸ç›´æ¥è°ƒä¼˜ç›¸å½“ï¼Œæ˜¾ç¤ºå‡ºè‰¯å¥½çš„å®ç”¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œä¸ªæ€§åŒ–æ¨èç­‰ã€‚é€šè¿‡ä¿æŠ¤ç”¨æˆ·éšç§å¹¶é™ä½è®¡ç®—æˆæœ¬ï¼ŒPOSTæ¡†æ¶èƒ½å¤Ÿä¿ƒè¿›æ›´å¤šä¼ä¸šå’Œå¼€å‘è€…åœ¨ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ—¶çš„å®‰å…¨æ€§å’Œæ•ˆç‡ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å¹¿æ³›åº”ç”¨ä¸å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Prompting has become a dominant paradigm for adapting large language models (LLMs). While discrete (textual) prompts are widely used for their interpretability, soft (parameter) prompts have recently gained traction in APIs. This is because they can encode information from more training samples while minimizing the user's token usage, leaving more space in the context window for task-specific input. However, soft prompts are tightly coupled to the LLM they are tuned on, limiting their generalization to other LLMs. This constraint is particularly problematic for efficiency and privacy: (1) tuning prompts on each LLM incurs high computational costs, especially as LLMs continue to grow in size. Additionally, (2) when the LLM is hosted externally, soft prompt tuning often requires sharing private data with the LLM provider. For instance, this is the case with the NVIDIA NeMo API. To address these issues, we propose POST (Privacy Of Soft prompt Transfer), a framework that enables private tuning of soft prompts on a small model and subsequently transfers these prompts to a larger LLM. POST uses knowledge distillation to derive a small model directly from the large LLM to improve prompt transferability, tunes the soft prompt locally, optionally with differential privacy guarantees, and transfers it back to the larger LLM using a small public dataset. Our experiments show that POST reduces computational costs, preserves privacy, and effectively transfers high-utility soft prompts.

