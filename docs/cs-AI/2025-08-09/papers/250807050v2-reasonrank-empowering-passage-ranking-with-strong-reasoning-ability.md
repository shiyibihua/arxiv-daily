---
layout: default
title: ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability
---

# ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.07050" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.07050v2</a>
  <a href="https://arxiv.org/pdf/2508.07050.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.07050v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.07050v2', 'ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Wenhan Liu, Xinyu Ma, Weiwei Sun, Yutao Zhu, Yuchen Li, Dawei Yin, Zhicheng Dou

**åˆ†ç±»**: cs.IR, cs.AI, cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-09 (æ›´æ–°: 2025-08-22)

**å¤‡æ³¨**: 21 pages

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/8421BCD/ReasonRank)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºReasonRankä»¥è§£å†³å¤æ‚åœºæ™¯ä¸‹çš„æ®µè½æ’åé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ®µè½æ’å` `æ¨ç†æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `æ•°æ®åˆæˆ` `ä¿¡æ¯æ£€ç´¢`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ¨ç†å¯†é›†å‹é‡æ’åæ–¹æ³•åœ¨å¤æ‚åœºæ™¯ä¸­è¡¨ç°ä¸ä½³ï¼Œä¸»è¦ç”±äºç¼ºä¹é«˜è´¨é‡çš„æ¨ç†è®­ç»ƒæ•°æ®ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªåŠ¨åŒ–çš„è®­ç»ƒæ•°æ®åˆæˆæ¡†æ¶ï¼Œå¹¶ç»“åˆå†·å¯åŠ¨å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ çš„ä¸¤é˜¶æ®µåè®­ç»ƒæ–¹æ³•ï¼Œä»¥å¢å¼ºé‡æ’åèƒ½åŠ›ã€‚
3. ReasonRankåœ¨BRIGHTåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†40.6çš„æœ€æ–°æ€§èƒ½ï¼Œç›¸è¾ƒäºç°æœ‰åŸºçº¿æ˜¾è‘—æå‡ï¼Œå¹¶ä¸”å»¶è¿Ÿæ›´ä½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„åˆ—è¡¨æ’ååœ¨è®¸å¤šæ®µè½æ’åä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚éšç€å¤§å‹æ¨ç†æ¨¡å‹çš„å‘å±•ï¼Œé€æ­¥æ¨ç†åœ¨æµ‹è¯•é˜¶æ®µæœ‰åŠ©äºæé«˜åˆ—è¡¨æ’åæ€§èƒ½ã€‚ç„¶è€Œï¼Œç”±äºæ¨ç†å¯†é›†å‹è®­ç»ƒæ•°æ®çš„ç¨€ç¼ºï¼Œç°æœ‰çš„é‡æ’åæ–¹æ³•åœ¨å¤æ‚æ’ååœºæ™¯ä¸­è¡¨ç°ä¸ä½³ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªåŠ¨åŒ–çš„æ¨ç†å¯†é›†å‹è®­ç»ƒæ•°æ®åˆæˆæ¡†æ¶ï¼Œå¹¶è®¾è®¡äº†è‡ªä¸€è‡´æ€§æ•°æ®è¿‡æ»¤æœºåˆ¶ï¼Œä»¥ç¡®ä¿æ•°æ®è´¨é‡ã€‚æ­¤å¤–ï¼Œæå‡ºäº†ä¸¤é˜¶æ®µçš„åè®­ç»ƒæ–¹æ³•ï¼Œç»“åˆå†·å¯åŠ¨ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ï¼Œæ˜¾è‘—æå‡äº†é‡æ’åèƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒReasonRankåœ¨BRIGHTåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†40.6çš„æœ€æ–°æ€§èƒ½ï¼Œä¸”å»¶è¿Ÿæ˜¾è‘—ä½äºç‚¹å¯¹ç‚¹é‡æ’åæ–¹æ³•Rank1ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ¨ç†å¯†é›†å‹é‡æ’åæ–¹æ³•åœ¨å¤æ‚åœºæ™¯ä¸­è¡¨ç°ä¸ä½³çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å› ç¼ºä¹é«˜è´¨é‡è®­ç»ƒæ•°æ®è€Œå—åˆ°é™åˆ¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºä¸€ç§è‡ªåŠ¨åŒ–çš„è®­ç»ƒæ•°æ®åˆæˆæ¡†æ¶ï¼Œé€šè¿‡å¤šé¢†åŸŸçš„æŸ¥è¯¢å’Œæ®µè½ç”Ÿæˆé«˜è´¨é‡æ ‡ç­¾ï¼Œå¹¶ç»“åˆä¸¤é˜¶æ®µçš„åè®­ç»ƒæ–¹æ³•ï¼Œæå‡é‡æ’åæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®åˆæˆæ¨¡å—ã€è‡ªä¸€è‡´æ€§æ•°æ®è¿‡æ»¤æœºåˆ¶ã€å†·å¯åŠ¨ç›‘ç£å¾®è°ƒé˜¶æ®µå’Œå¼ºåŒ–å­¦ä¹ é˜¶æ®µï¼Œç¡®ä¿æ¨¡å‹åœ¨æ¨ç†èƒ½åŠ›å’Œæ’åæ€§èƒ½ä¸Šçš„æå‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºè®¾è®¡äº†å¤šè§†è§’æ’åå¥–åŠ±æœºåˆ¶ï¼Œç›¸è¾ƒäºä¼ ç»Ÿçš„åŸºäºæ’åæŒ‡æ ‡çš„å¥–åŠ±ï¼Œæ›´æœ‰æ•ˆåœ°æå‡äº†æ¨¡å‹çš„æ’åèƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å†·å¯åŠ¨é˜¶æ®µé‡‡ç”¨ç›‘ç£å¾®è°ƒï¼Œå¼ºåŒ–å­¦ä¹ é˜¶æ®µåˆ™åˆ©ç”¨å¤šè§†è§’å¥–åŠ±æœºåˆ¶ï¼Œç¡®ä¿æ¨¡å‹åœ¨æ¨ç†å’Œæ’åèƒ½åŠ›ä¸Šå‡å¾—åˆ°å¢å¼ºã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

ReasonRankåœ¨BRIGHTåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†40.6çš„æœ€æ–°æ€§èƒ½ï¼Œç›¸è¾ƒäºç°æœ‰åŸºçº¿æ˜¾è‘—æå‡ï¼Œä¸”åœ¨å»¶è¿Ÿæ–¹é¢è¡¨ç°ä¼˜äºç‚¹å¯¹ç‚¹é‡æ’åæ–¹æ³•Rank1ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å®é™…åº”ç”¨ä¸­çš„é«˜æ•ˆæ€§å’Œå¯é æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ä¿¡æ¯æ£€ç´¢ã€é—®ç­”ç³»ç»Ÿå’Œæ¨èç³»ç»Ÿç­‰ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡å¤æ‚æŸ¥è¯¢åœºæ™¯ä¸‹çš„æ®µè½æ’åè´¨é‡ã€‚æœªæ¥ï¼ŒReasonRankæœ‰æœ›åœ¨æ›´å¹¿æ³›çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œæ¨åŠ¨æ™ºèƒ½æœç´¢å’Œä¿¡æ¯è·å–çš„è¿›æ­¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Model (LLM) based listwise ranking has shown superior performance in many passage ranking tasks. With the development of Large Reasoning Models, many studies have demonstrated that step-by-step reasoning during test-time helps improve listwise ranking performance. However, due to the scarcity of reasoning-intensive training data, existing rerankers perform poorly in many complex ranking scenarios and the ranking ability of reasoning-intensive rerankers remains largely underdeveloped. In this paper, we first propose an automated reasoning-intensive training data synthesis framework, which sources training queries and passages from diverse domains and applies DeepSeek-R1 to generate high-quality training labels. A self-consistency data filtering mechanism is designed to ensure the data quality. To empower the listwise reranker with strong reasoning ability, we further propose a two-stage post-training approach, which includes a cold-start supervised fine-tuning (SFT) stage for reasoning pattern learning and a reinforcement learning (RL) stage for further ranking ability enhancement. During the RL stage, based on the nature of listwise ranking, we design a multi-view ranking reward, which is more effective than a ranking metric-based reward. Extensive experiments demonstrate that our trained reasoning-intensive reranker \textbf{ReasonRank} outperforms existing baselines significantly and also achieves much lower latency than pointwise reranker Rank1. \textbf{Through further experiments, our ReasonRank has achieved state-of-the-art (SOTA) performance 40.6 on the BRIGHT leaderboard\footnote{https://brightbenchmark.github.io/}.} Our codes are available at https://github.com/8421BCD/ReasonRank.

