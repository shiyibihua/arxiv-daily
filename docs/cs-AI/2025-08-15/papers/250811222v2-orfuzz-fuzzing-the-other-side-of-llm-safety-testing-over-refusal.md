---
layout: default
title: ORFuzz: Fuzzing the "Other Side" of LLM Safety -- Testing Over-Refusal
---

# ORFuzz: Fuzzing the "Other Side" of LLM Safety -- Testing Over-Refusal

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.11222" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.11222v2</a>
  <a href="https://arxiv.org/pdf/2508.11222.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.11222v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.11222v2', 'ORFuzz: Fuzzing the &quot;Other Side&quot; of LLM Safety -- Testing Over-Refusal')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Haonan Zhang, Dongxia Wang, Yi Liu, Kexin Chen, Jiashui Wang, Xinlei Ying, Long Liu, Wenhai Wang

**åˆ†ç±»**: cs.SE, cs.AI, cs.CL, cs.IR

**å‘å¸ƒæ—¥æœŸ**: 2025-08-15 (æ›´æ–°: 2025-12-05)

**å¤‡æ³¨**: Accepted by ASE 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºORFuzzæ¡†æ¶ä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹çš„è¿‡åº¦æ‹’ç»é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `è¿‡åº¦æ‹’ç»` `å®‰å…¨æ€§æµ‹è¯•` `è¿›åŒ–æµ‹è¯•` `è‡ªåŠ¨åŒ–æµ‹è¯•` `äººç±»å¯¹é½è¯„åˆ¤` `æµ‹è¯•ç”Ÿæˆ` `äººå·¥æ™ºèƒ½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å½“å‰æ–¹æ³•æ— æ³•æœ‰æ•ˆæµ‹è¯•å¤§è¯­è¨€æ¨¡å‹çš„è¿‡åº¦æ‹’ç»ç°è±¡ï¼Œå­˜åœ¨åŸºå‡†æµ‹è¯•ç¼ºé™·å’Œæµ‹è¯•ç”Ÿæˆèƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚
2. æœ¬æ–‡æå‡ºçš„ORFuzzæ¡†æ¶é€šè¿‡å®‰å…¨ç±»åˆ«æ„ŸçŸ¥çš„ç§å­é€‰æ‹©å’Œè‡ªé€‚åº”å˜å¼‚ä¼˜åŒ–ï¼Œç³»ç»Ÿæ€§åœ°æ£€æµ‹LLMçš„è¿‡åº¦æ‹’ç»ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒORFuzzç”Ÿæˆçš„è¿‡åº¦æ‹’ç»å®ä¾‹é€Ÿç‡è¾¾åˆ°6.98%ï¼Œæ˜¯ç°æœ‰åŸºçº¿çš„ä¸¤å€ï¼Œä¸”æ–°åŸºå‡†ORFuzzSetåœ¨10ä¸ªä¸åŒLLMä¸Šè¡¨ç°ä¼˜å¼‚ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¶Šæ¥è¶Šå¤šåœ°è¡¨ç°å‡ºè¿‡åº¦æ‹’ç»ç°è±¡ï¼Œå³ç”±äºè¿‡äºä¿å®ˆçš„å®‰å…¨æªæ–½é”™è¯¯åœ°æ‹’ç»è‰¯æ€§æŸ¥è¯¢ï¼Œè¿™ä¸€å…³é”®åŠŸèƒ½ç¼ºé™·å‰Šå¼±äº†å®ƒä»¬çš„å¯é æ€§å’Œå¯ç”¨æ€§ã€‚ç°æœ‰æµ‹è¯•æ–¹æ³•æ˜æ˜¾ä¸è¶³ï¼Œå­˜åœ¨åŸºå‡†æµ‹è¯•ç¼ºé™·å’Œæœ‰é™çš„æµ‹è¯•ç”Ÿæˆèƒ½åŠ›ã€‚æœ¬æ–‡é¦–æ¬¡æå‡ºäº†è¿›åŒ–æµ‹è¯•æ¡†æ¶ORFuzzï¼Œç³»ç»Ÿæ£€æµ‹å’Œåˆ†æLLMçš„è¿‡åº¦æ‹’ç»ã€‚ORFuzzç‹¬ç‰¹åœ°æ•´åˆäº†ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šå®‰å…¨ç±»åˆ«æ„ŸçŸ¥çš„ç§å­é€‰æ‹©ã€ä½¿ç”¨æ¨ç†LLMçš„è‡ªé€‚åº”å˜å¼‚ä¼˜åŒ–ä»¥åŠç»è¿‡éªŒè¯çš„OR-Judgeäººç±»å¯¹é½è¯„åˆ¤æ¨¡å‹ã€‚æˆ‘ä»¬çš„è¯„ä¼°è¡¨æ˜ï¼ŒORFuzzç”Ÿæˆçš„è¿‡åº¦æ‹’ç»å®ä¾‹çš„é€Ÿç‡è¶…è¿‡é¢†å…ˆåŸºçº¿çš„ä¸¤å€ï¼Œå½¢æˆäº†æ–°çš„åŸºå‡†ORFuzzSetï¼ŒåŒ…å«1855ä¸ªé«˜åº¦å¯è½¬ç§»çš„æµ‹è¯•ç”¨ä¾‹ï¼Œæ˜¾è‘—æå‡äº†ç°æœ‰æ•°æ®é›†çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨å®‰å…¨æ€§è¿‡åº¦ä¿å®ˆä¸‹å¯¼è‡´çš„è¿‡åº¦æ‹’ç»é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨æµ‹è¯•æ­¤ç±»è¡Œä¸ºæ—¶å­˜åœ¨åŸºå‡†ä¸å®Œå–„å’Œæµ‹è¯•ç”Ÿæˆèƒ½åŠ›æœ‰é™çš„ç—›ç‚¹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šORFuzzæ¡†æ¶é€šè¿‡ç»“åˆå®‰å…¨ç±»åˆ«æ„ŸçŸ¥çš„ç§å­é€‰æ‹©ã€è‡ªé€‚åº”å˜å¼‚ä¼˜åŒ–å’Œäººç±»å¯¹é½çš„è¯„åˆ¤æ¨¡å‹ï¼Œç³»ç»Ÿæ€§åœ°æ£€æµ‹å’Œåˆ†æè¿‡åº¦æ‹’ç»ç°è±¡ï¼Œä»¥æé«˜æµ‹è¯•çš„å…¨é¢æ€§å’Œæœ‰æ•ˆæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šORFuzzçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šå®‰å…¨ç±»åˆ«æ„ŸçŸ¥çš„ç§å­é€‰æ‹©æ¨¡å—ã€ä½¿ç”¨æ¨ç†LLMçš„è‡ªé€‚åº”å˜å¼‚ä¼˜åŒ–æ¨¡å—å’ŒOR-Judgeè¯„åˆ¤æ¨¡å‹ã€‚è¿™äº›æ¨¡å—ååŒå·¥ä½œï¼Œä»¥ç”Ÿæˆå¤šæ ·åŒ–çš„æµ‹è¯•ç”¨ä¾‹å¹¶è¯„ä¼°å…¶æœ‰æ•ˆæ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šORFuzzçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶ç»¼åˆæ€§æµ‹è¯•æ¡†æ¶ï¼Œç‰¹åˆ«æ˜¯å¼•å…¥äº†äººç±»å¯¹é½çš„è¯„åˆ¤æ¨¡å‹OR-Judgeï¼Œä»¥å‡†ç¡®åæ˜ ç”¨æˆ·å¯¹æ¯’æ€§å’Œæ‹’ç»çš„æ„ŸçŸ¥ï¼Œè¿™åœ¨ç°æœ‰æ–¹æ³•ä¸­å°šå±é¦–æ¬¡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œç§å­é€‰æ‹©åŸºäºå®‰å…¨ç±»åˆ«ï¼Œå˜å¼‚ä¼˜åŒ–é€šè¿‡æ¨ç†LLMè¿›è¡Œï¼ŒOR-Judgeæ¨¡å‹ç»è¿‡éªŒè¯ä»¥ç¡®ä¿å…¶è¯„åˆ¤çš„å‡†ç¡®æ€§ã€‚è¿™äº›è®¾è®¡ç¡®ä¿äº†æµ‹è¯•ç”¨ä¾‹çš„æœ‰æ•ˆæ€§å’Œå¤šæ ·æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒORFuzzç”Ÿæˆçš„è¿‡åº¦æ‹’ç»å®ä¾‹çš„å¹³å‡é€Ÿç‡ä¸º6.98%ï¼Œæ˜¯é¢†å…ˆåŸºçº¿çš„ä¸¤å€ï¼Œä¸”æ–°åŸºå‡†ORFuzzSetåœ¨10ä¸ªä¸åŒLLMä¸Šçš„å¹³å‡è¿‡åº¦æ‹’ç»ç‡è¾¾åˆ°63.56%ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ•°æ®é›†ï¼Œå±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤§è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§æµ‹è¯•ã€äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„å¯é æ€§è¯„ä¼°ä»¥åŠç›¸å…³è½¯ä»¶å¼€å‘ã€‚ORFuzzæ¡†æ¶ä¸ºå¼€å‘æ›´å¯é å’Œå¯ä¿¡èµ–çš„LLMåŸºç¡€è½¯ä»¶ç³»ç»Ÿæä¾›äº†é‡è¦å·¥å…·ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨æ›´å¹¿æ³›çš„AIåº”ç”¨å’Œæ ‡å‡†åŒ–æµ‹è¯•æµç¨‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) increasingly exhibit over-refusal - erroneously rejecting benign queries due to overly conservative safety measures - a critical functional flaw that undermines their reliability and usability. Current methods for testing this behavior are demonstrably inadequate, suffering from flawed benchmarks and limited test generation capabilities, as highlighted by our empirical user study. To the best of our knowledge, this paper introduces the first evolutionary testing framework, ORFuzz, for the systematic detection and analysis of LLM over-refusals. ORFuzz uniquely integrates three core components: (1) safety category-aware seed selection for comprehensive test coverage, (2) adaptive mutator optimization using reasoning LLMs to generate effective test cases, and (3) OR-Judge, a human-aligned judge model validated to accurately reflect user perception of toxicity and refusal. Our extensive evaluations demonstrate that ORFuzz generates diverse, validated over-refusal instances at a rate (6.98% average) more than double that of leading baselines, effectively uncovering vulnerabilities. Furthermore, ORFuzz's outputs form the basis of ORFuzzSet, a new benchmark of 1,855 highly transferable test cases that achieves a superior 63.56% average over-refusal rate across 10 diverse LLMs, significantly outperforming existing datasets. ORFuzz and ORFuzzSet provide a robust automated testing framework and a valuable community resource, paving the way for developing more reliable and trustworthy LLM-based software systems.

