---
layout: default
title: Scaling Laws for Energy Efficiency of Local LLMs
---

# Scaling Laws for Energy Efficiency of Local LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.16531" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.16531v1</a>
  <a href="https://arxiv.org/pdf/2512.16531.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.16531v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.16531v1', 'Scaling Laws for Energy Efficiency of Local LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ander Alvarez, Alessandro Genuardi, Nilotpal Sinha, Antonio Tiene, Samuel Mugel, RomÃ¡n OrÃºs

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é’ˆå¯¹æœ¬åœ°LLMï¼Œæ­ç¤ºCPUèƒ½æ•ˆç¼©æ”¾è§„å¾‹ï¼Œå¹¶æå‡ºé‡å­å¯å‘å‹ç¼©ä¼˜åŒ–æ–¹æ¡ˆã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æœ¬åœ°LLM` `CPUæ¨ç†` `èƒ½æ•ˆä¼˜åŒ–` `ç¼©æ”¾è§„å¾‹` `é‡å­å¯å‘å‹ç¼©`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²LLMæ—¶ï¼Œéš¾ä»¥åœ¨ç²¾åº¦ã€è®¡ç®—å’Œèƒ½è€—é—´å¹³è¡¡ï¼Œå°¤å…¶ç¼ºä¹å¯¹CPUä¸Šæ¨ç†çš„æ·±å…¥ç ”ç©¶ã€‚
2. è®ºæ–‡é€šè¿‡åœ¨ä¸»æµCPUä¸Šè¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œæ­ç¤ºäº†è¯­è¨€æ¨¡å‹å’Œè§†è§‰-è¯­è¨€æ¨¡å‹åœ¨CPUä¸Šçš„è®¡ç®—è´Ÿè½½ç¼©æ”¾è§„å¾‹ã€‚
3. å®éªŒè¡¨æ˜ï¼Œé‡å­å¯å‘å‹ç¼©èƒ½æ˜¾è‘—é™ä½CPUå’Œå†…å­˜ä½¿ç”¨ï¼ŒåŒæ—¶ä¿æŒæˆ–æå‡è¯­ä¹‰å‡†ç¡®æ€§ï¼Œä¸ºè¾¹ç¼˜æ¨ç†æä¾›ä¼˜åŒ–æ–¹æ¡ˆã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²æœ¬åœ°å¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰-è¯­è¨€æ¨¡å‹éœ€è¦åœ¨ç²¾åº¦ã€è®¡ç®—å’Œèƒ½æºé¢„ç®—ä¹‹é—´å–å¾—å¹³è¡¡ã€‚å°½ç®¡å›¾å½¢å¤„ç†å™¨åœ¨ç°ä»£äººå·¥æ™ºèƒ½éƒ¨ç½²ä¸­å æ®ä¸»å¯¼åœ°ä½ï¼Œä½†å¤§å¤šæ•°æ¶ˆè´¹çº§ç¡¬ä»¶ï¼ˆåŒ…æ‹¬ç¬”è®°æœ¬ç”µè„‘ã€å°å¼æœºã€å·¥ä¸šæ§åˆ¶å™¨å’ŒåµŒå…¥å¼ç³»ç»Ÿï¼‰ä¾èµ–äºä¸­å¤®å¤„ç†å™¨ã€‚æœ¬æ–‡ç³»ç»Ÿåœ°å¯¹ä¸¤ç§å¹¿æ³›ç”¨äºæœ¬åœ°æ¨ç†çš„ä¸­å¤®å¤„ç†å™¨è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼šMacBook Pro M2ï¼ˆä»£è¡¨ä¸»æµç¬”è®°æœ¬ç”µè„‘çº§éƒ¨ç½²ï¼‰å’Œ Raspberry Pi 5ï¼ˆä»£è¡¨å—é™çš„ä½åŠŸè€—åµŒå…¥å¼ç¯å¢ƒï¼‰ã€‚é€šè¿‡ç»Ÿä¸€çš„æ–¹æ³•ï¼Œå³è¿ç»­é‡‡æ ·å¤„ç†å™¨å’Œå†…å­˜ä½¿ç”¨æƒ…å†µä»¥åŠæ›²çº¿ä¸‹é¢ç§¯ç§¯åˆ†ï¼Œæˆ‘ä»¬æè¿°äº†è¯­è¨€æ¨¡å‹çš„è®¡ç®—è´Ÿè½½å¦‚ä½•éšè¾“å…¥æ–‡æœ¬é•¿åº¦ç¼©æ”¾ï¼Œä»¥åŠè§†è§‰-è¯­è¨€æ¨¡å‹çš„è®¡ç®—è´Ÿè½½å¦‚ä½•éšå›¾åƒåˆ†è¾¨ç‡ç¼©æ”¾ã€‚æˆ‘ä»¬å‘ç°äº†ä¸¤ä¸ªç»éªŒç¼©æ”¾è§„å¾‹ï¼šï¼ˆ1ï¼‰è¯­è¨€æ¨¡å‹æ¨ç†çš„è®¡ç®—æˆæœ¬ä¸tokené•¿åº¦è¿‘ä¼¼çº¿æ€§ç¼©æ”¾ï¼›ï¼ˆ2ï¼‰è§†è§‰-è¯­è¨€æ¨¡å‹è¡¨ç°å‡ºç”±é¢„å¤„ç†é©±åŠ¨çš„â€œåˆ†è¾¨ç‡è†ç‚¹â€ï¼Œå³è®¡ç®—é‡åœ¨å†…éƒ¨åˆ†è¾¨ç‡ä¸Šé™ä¹‹ä¸Šä¿æŒä¸å˜ï¼Œå¹¶åœ¨å…¶ä¹‹ä¸‹æ€¥å‰§ä¸‹é™ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¡¨æ˜ï¼Œé‡å­å¯å‘å‹ç¼©å¯å°†å¤„ç†å™¨å’Œå†…å­˜ä½¿ç”¨é‡é™ä½é«˜è¾¾71.9%ï¼Œå¹¶å°†èƒ½è€—é™ä½é«˜è¾¾62%ï¼ŒåŒæ—¶ä¿æŒæˆ–æé«˜è¯­ä¹‰å‡†ç¡®æ€§ã€‚è¿™äº›ç»“æœç³»ç»Ÿåœ°é‡åŒ–äº†å¤šæ¨¡æ€ä¸­å¤®å¤„ç†å™¨åœ¨æœ¬åœ°è¯­è¨€å’Œè§†è§‰-è¯­è¨€å·¥ä½œè´Ÿè½½ä¸­çš„ç¼©æ”¾è§„å¾‹ï¼Œå¹¶ç¡®å®šäº†æ¨¡å‹å‹ç¼©å’Œè¾“å…¥åˆ†è¾¨ç‡é¢„å¤„ç†æ˜¯å¯æŒç»­è¾¹ç¼˜æ¨ç†çš„æœ‰æ•ˆä¸”ä½æˆæœ¬çš„æ‰‹æ®µã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ—¶ï¼Œå¦‚ä½•åœ¨ä¸­å¤®å¤„ç†å™¨ï¼ˆCPUï¼‰ä¸Šå®ç°é«˜æ•ˆæ¨ç†çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨GPUåŠ é€Ÿï¼Œè€Œå¿½ç•¥äº†å¤§é‡ä¾èµ–CPUçš„è®¾å¤‡ï¼Œä¸”ç¼ºä¹å¯¹CPUä¸ŠLLM/VLMæ¨ç†çš„è®¡ç®—è´Ÿè½½ç¼©æ”¾è§„å¾‹çš„ç³»ç»Ÿç ”ç©¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ç³»ç»Ÿæ€§çš„åŸºå‡†æµ‹è¯•ï¼Œé‡åŒ–LLMå’ŒVLMåœ¨ä¸åŒCPUå¹³å°ä¸Šçš„è®¡ç®—è´Ÿè½½ä¸è¾“å…¥æ•°æ®è§„æ¨¡ï¼ˆtokené•¿åº¦ã€å›¾åƒåˆ†è¾¨ç‡ï¼‰ä¹‹é—´çš„å…³ç³»ï¼Œä»è€Œæ­ç¤ºç»éªŒæ€§çš„ç¼©æ”¾è§„å¾‹ã€‚æ­¤å¤–ï¼Œæ¢ç´¢æ¨¡å‹å‹ç¼©æŠ€æœ¯ï¼Œä»¥é™ä½è®¡ç®—å’Œå†…å­˜éœ€æ±‚ï¼Œæå‡èƒ½æ•ˆã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡é‡‡ç”¨ç»Ÿä¸€çš„åŸºå‡†æµ‹è¯•æ–¹æ³•ï¼ŒåŒ…æ‹¬ï¼š
1.  é€‰æ‹©ä»£è¡¨æ€§çš„CPUå¹³å°ï¼šMacBook Pro M2 (ç¬”è®°æœ¬ç”µè„‘çº§) å’Œ Raspberry Pi 5 (åµŒå…¥å¼ç³»ç»Ÿçº§)ã€‚
2.  é€‰æ‹©LLMå’ŒVLMæ¨¡å‹è¿›è¡Œæµ‹è¯•ã€‚
3.  è¿ç»­é‡‡æ ·å¤„ç†å™¨å’Œå†…å­˜ä½¿ç”¨æƒ…å†µï¼Œå¹¶è®¡ç®—æ›²çº¿ä¸‹é¢ç§¯ï¼ˆAUCï¼‰ä½œä¸ºè®¡ç®—è´Ÿè½½çš„åº¦é‡ã€‚
4.  åˆ†æè®¡ç®—è´Ÿè½½ä¸è¾“å…¥æ•°æ®è§„æ¨¡ä¹‹é—´çš„å…³ç³»ï¼Œæ‹Ÿåˆç»éªŒç¼©æ”¾è§„å¾‹ã€‚
5.  åº”ç”¨é‡å­å¯å‘å‹ç¼©æŠ€æœ¯ï¼Œè¯„ä¼°å…¶å¯¹æ€§èƒ½å’Œèƒ½æ•ˆçš„å½±å“ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š
1.  é¦–æ¬¡ç³»ç»Ÿæ€§åœ°ç ”ç©¶äº†LLMå’ŒVLMåœ¨CPUä¸Šçš„è®¡ç®—è´Ÿè½½ç¼©æ”¾è§„å¾‹ï¼Œæ­ç¤ºäº†tokené•¿åº¦ä¸è®¡ç®—æˆæœ¬çš„çº¿æ€§å…³ç³»ï¼Œä»¥åŠVLMçš„â€œåˆ†è¾¨ç‡è†ç‚¹â€ç°è±¡ã€‚
2.  éªŒè¯äº†é‡å­å¯å‘å‹ç¼©æŠ€æœ¯åœ¨é™ä½CPUå’Œå†…å­˜ä½¿ç”¨æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œå¹¶è¯æ˜å…¶èƒ½åœ¨ä¿æŒæˆ–æå‡è¯­ä¹‰å‡†ç¡®æ€§çš„å‰æä¸‹ï¼Œæ˜¾è‘—é™ä½èƒ½è€—ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š
1.  ç»Ÿä¸€çš„åŸºå‡†æµ‹è¯•æ–¹æ³•ï¼Œç¡®ä¿ä¸åŒå¹³å°å’Œæ¨¡å‹ä¹‹é—´ç»“æœçš„å¯æ¯”æ€§ã€‚
2.  ä½¿ç”¨æ›²çº¿ä¸‹é¢ç§¯ï¼ˆAUCï¼‰ä½œä¸ºè®¡ç®—è´Ÿè½½çš„åº¦é‡ï¼Œèƒ½å¤Ÿæ›´å‡†ç¡®åœ°åæ˜ å®é™…çš„è®¡ç®—æˆæœ¬ã€‚
3.  é‡‡ç”¨é‡å­å¯å‘å‹ç¼©æŠ€æœ¯ï¼Œåˆ©ç”¨é‡å­åŠ›å­¦åŸç†è¿›è¡Œæ¨¡å‹å‚æ•°çš„å‹ç¼©ï¼Œé™ä½æ¨¡å‹å¤§å°å’Œè®¡ç®—å¤æ‚åº¦ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16531v1/figures/updated/llm_mac_cpu_auc.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16531v1/figures/updated/llm_rpi_cpu_auc.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16531v1/figures/updated/vlm_mac_cpu_auc.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯­è¨€æ¨¡å‹æ¨ç†çš„è®¡ç®—æˆæœ¬ä¸tokené•¿åº¦è¿‘ä¼¼çº¿æ€§ç¼©æ”¾ï¼›è§†è§‰-è¯­è¨€æ¨¡å‹å­˜åœ¨â€œåˆ†è¾¨ç‡è†ç‚¹â€ï¼Œè®¡ç®—é‡åœ¨å†…éƒ¨åˆ†è¾¨ç‡ä¸Šé™ä¹‹ä¸Šä¿æŒä¸å˜ï¼Œä¹‹ä¸‹æ€¥å‰§ä¸‹é™ã€‚é‡å­å¯å‘å‹ç¼©å¯å°†å¤„ç†å™¨å’Œå†…å­˜ä½¿ç”¨é‡é™ä½é«˜è¾¾71.9%ï¼Œå¹¶å°†èƒ½è€—é™ä½é«˜è¾¾62%ï¼ŒåŒæ—¶ä¿æŒæˆ–æé«˜è¯­ä¹‰å‡†ç¡®æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§è¾¹ç¼˜è®¡ç®—åœºæ™¯ï¼Œä¾‹å¦‚ï¼šåœ¨ä½åŠŸè€—åµŒå…¥å¼è®¾å¤‡ä¸Šéƒ¨ç½²æœ¬åœ°LLMï¼Œå®ç°ç¦»çº¿è¯­éŸ³åŠ©æ‰‹ã€æ™ºèƒ½å®¶å±…æ§åˆ¶ç­‰åŠŸèƒ½ï¼›åœ¨ç¬”è®°æœ¬ç”µè„‘ä¸Šä¼˜åŒ–VLMæ¨ç†ï¼Œæå‡å›¾åƒå¤„ç†å’Œè§†è§‰æœç´¢çš„æ•ˆç‡ã€‚ç ”ç©¶ç»“æœæœ‰åŠ©äºå¼€å‘è€…æ ¹æ®ç¡¬ä»¶èµ„æºé€‰æ‹©åˆé€‚çš„æ¨¡å‹å’Œä¼˜åŒ–ç­–ç•¥ï¼Œå®ç°å¯æŒç»­çš„è¾¹ç¼˜AIåº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Deploying local large language models and vision-language models on edge devices requires balancing accuracy with constrained computational and energy budgets. Although graphics processors dominate modern artificial-intelligence deployment, most consumer hardware--including laptops, desktops, industrial controllers, and embedded systems--relies on central processing units. Despite this, the computational laws governing central-processing-unit-only inference for local language and vision-language workloads remain largely unexplored. We systematically benchmark large language and vision-language models on two representative central-processing-unit tiers widely used for local inference: a MacBook Pro M2, reflecting mainstream laptop-class deployment, and a Raspberry Pi 5, representing constrained, low-power embedded settings. Using a unified methodology based on continuous sampling of processor and memory usage together with area-under-curve integration, we characterize how computational load scales with input text length for language models and with image resolution for vision-language models. We uncover two empirical scaling laws: (1) computational cost for language-model inference scales approximately linearly with token length; and (2) vision-language models exhibit a preprocessing-driven "resolution knee", where compute remains constant above an internal resolution clamp and decreases sharply below it. Beyond these laws, we show that quantum-inspired compression reduces processor and memory usage by up to 71.9% and energy consumption by up to 62%, while preserving or improving semantic accuracy. These results provide a systematic quantification of multimodal central-processing-unit-only scaling for local language and vision-language workloads, and they identify model compression and input-resolution preprocessing as effective, low-cost levers for sustainable edge inference.

