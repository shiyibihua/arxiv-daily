---
layout: default
title: Scaling Laws for Energy Efficiency of Local LLMs
---

# Scaling Laws for Energy Efficiency of Local LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.16531" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.16531v1</a>
  <a href="https://arxiv.org/pdf/2512.16531.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.16531v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.16531v1', 'Scaling Laws for Energy Efficiency of Local LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ander Alvarez, Alessandro Genuardi, Nilotpal Sinha, Antonio Tiene, Samuel Mugel, RomÃ¡n OrÃºs

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é’ˆå¯¹æœ¬åœ°LLMï¼Œæ­ç¤ºCPUèƒ½æ•ˆç¼©æ”¾è§„å¾‹ï¼Œå¹¶æå‡ºé‡å­å¯å‘å‹ç¼©ä¼˜åŒ–æ–¹æ¡ˆã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æœ¬åœ°LLM` `CPUæ¨ç†` `èƒ½æ•ˆä¼˜åŒ–` `ç¼©æ”¾è§„å¾‹` `é‡å­å¯å‘å‹ç¼©`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨CPUä¸Šéƒ¨ç½²æœ¬åœ°LLMæ—¶ï¼Œç¼ºä¹å¯¹è®¡ç®—å’Œèƒ½è€—ç¼©æ”¾è§„å¾‹çš„ç³»ç»Ÿç ”ç©¶ï¼Œéš¾ä»¥ä¼˜åŒ–ã€‚
2. è®ºæ–‡é€šè¿‡åœ¨ä¸»æµCPUä¸Šè¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œæ­ç¤ºäº†LLMå’ŒVLMåœ¨CPUä¸Šçš„è®¡ç®—è´Ÿè½½ç¼©æ”¾è§„å¾‹ã€‚
3. å®éªŒè¡¨æ˜ï¼Œé‡å­å¯å‘å‹ç¼©èƒ½æ˜¾è‘—é™ä½CPUå’Œå†…å­˜ä½¿ç”¨ï¼Œå¹¶é™ä½èƒ½è€—ï¼ŒåŒæ—¶ä¿æŒæˆ–æå‡è¯­ä¹‰ç²¾åº¦ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²æœ¬åœ°å¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹éœ€è¦åœ¨ç²¾åº¦ã€è®¡ç®—å’Œèƒ½æºé¢„ç®—ä¹‹é—´å–å¾—å¹³è¡¡ã€‚å°½ç®¡å›¾å½¢å¤„ç†å™¨åœ¨ç°ä»£äººå·¥æ™ºèƒ½éƒ¨ç½²ä¸­å æ®ä¸»å¯¼åœ°ä½ï¼Œä½†å¤§å¤šæ•°æ¶ˆè´¹çº§ç¡¬ä»¶ï¼ˆåŒ…æ‹¬ç¬”è®°æœ¬ç”µè„‘ã€å°å¼æœºã€å·¥ä¸šæ§åˆ¶å™¨å’ŒåµŒå…¥å¼ç³»ç»Ÿï¼‰ä¾èµ–äºä¸­å¤®å¤„ç†å™¨ã€‚æœ¬æ–‡ç³»ç»Ÿåœ°å¯¹ä¸¤ç§å¹¿æ³›ç”¨äºæœ¬åœ°æ¨ç†çš„ä¸­å¤®å¤„ç†å™¨è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼šMacBook Pro M2ï¼ˆä¸»æµç¬”è®°æœ¬ç”µè„‘çº§éƒ¨ç½²ï¼‰å’Œ Raspberry Pi 5ï¼ˆå—é™çš„ä½åŠŸè€—åµŒå…¥å¼ç¯å¢ƒï¼‰ã€‚é€šè¿‡ç»Ÿä¸€çš„æ–¹æ³•ï¼Œå³è¿ç»­é‡‡æ ·å¤„ç†å™¨å’Œå†…å­˜ä½¿ç”¨æƒ…å†µä»¥åŠæ›²çº¿ä¸‹é¢ç§¯ç§¯åˆ†ï¼Œæˆ‘ä»¬æè¿°äº†è¯­è¨€æ¨¡å‹çš„è®¡ç®—è´Ÿè½½å¦‚ä½•éšè¾“å…¥æ–‡æœ¬é•¿åº¦ç¼©æ”¾ï¼Œä»¥åŠè§†è§‰è¯­è¨€æ¨¡å‹çš„è®¡ç®—è´Ÿè½½å¦‚ä½•éšå›¾åƒåˆ†è¾¨ç‡ç¼©æ”¾ã€‚æˆ‘ä»¬å‘ç°äº†ä¸¤ä¸ªç»éªŒç¼©æ”¾è§„å¾‹ï¼š(1) è¯­è¨€æ¨¡å‹æ¨ç†çš„è®¡ç®—æˆæœ¬ä¸tokené•¿åº¦è¿‘ä¼¼çº¿æ€§ç¼©æ”¾ï¼›(2) è§†è§‰è¯­è¨€æ¨¡å‹è¡¨ç°å‡ºç”±é¢„å¤„ç†é©±åŠ¨çš„â€œåˆ†è¾¨ç‡è†ç‚¹â€ï¼Œå³è®¡ç®—é‡åœ¨å†…éƒ¨åˆ†è¾¨ç‡ä¸Šé™ä¹‹ä¸Šä¿æŒä¸å˜ï¼Œè€Œåœ¨å…¶ä¹‹ä¸‹æ€¥å‰§ä¸‹é™ã€‚æ­¤å¤–ï¼Œé‡å­å¯å‘å‹ç¼©å¯å°†å¤„ç†å™¨å’Œå†…å­˜ä½¿ç”¨é‡é™ä½é«˜è¾¾71.9%ï¼Œå¹¶å°†èƒ½è€—é™ä½é«˜è¾¾62%ï¼ŒåŒæ—¶ä¿æŒæˆ–æé«˜è¯­ä¹‰å‡†ç¡®æ€§ã€‚è¿™äº›ç»“æœç³»ç»Ÿåœ°é‡åŒ–äº†å¤šæ¨¡æ€ä¸­å¤®å¤„ç†å™¨åœ¨æœ¬åœ°è¯­è¨€å’Œè§†è§‰è¯­è¨€å·¥ä½œè´Ÿè½½ä¸­çš„ç¼©æ”¾è§„å¾‹ï¼Œå¹¶ç¡®å®šäº†æ¨¡å‹å‹ç¼©å’Œè¾“å…¥åˆ†è¾¨ç‡é¢„å¤„ç†æ˜¯å¯æŒç»­è¾¹ç¼˜æ¨ç†çš„æœ‰æ•ˆä¸”ä½æˆæœ¬çš„æ‰‹æ®µã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³åœ¨CPUä¸Šéƒ¨ç½²æœ¬åœ°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ—¶ï¼Œç¼ºä¹å¯¹è®¡ç®—èµ„æºå’Œèƒ½è€—éœ€æ±‚ç¼©æ”¾è§„å¾‹çš„ç†è§£çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é’ˆå¯¹GPUè¿›è¡Œä¼˜åŒ–ï¼Œå¿½ç•¥äº†å¤§é‡ä¾èµ–CPUçš„è¾¹ç¼˜è®¾å¤‡ï¼Œå¯¼è‡´åœ¨è¿™äº›è®¾å¤‡ä¸Šéƒ¨ç½²LLM/VLMæ—¶æ•ˆç‡ä½ä¸‹ï¼Œéš¾ä»¥æ»¡è¶³èµ„æºçº¦æŸã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ç³»ç»Ÿæ€§çš„åŸºå‡†æµ‹è¯•ï¼Œé‡åŒ–LLMå’ŒVLMåœ¨ä¸åŒCPUå¹³å°ä¸Šçš„è®¡ç®—è´Ÿè½½ä¸è¾“å…¥æ•°æ®è§„æ¨¡ï¼ˆtokené•¿åº¦ã€å›¾åƒåˆ†è¾¨ç‡ï¼‰ä¹‹é—´çš„å…³ç³»ï¼Œä»è€Œæ­ç¤ºå…¶ç¼©æ”¾è§„å¾‹ã€‚æ­¤å¤–ï¼Œæ¢ç´¢æ¨¡å‹å‹ç¼©æŠ€æœ¯ï¼Œé™ä½èµ„æºå ç”¨ï¼Œæå‡èƒ½æ•ˆã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡é‡‡ç”¨çš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ï¼š
1.  é€‰æ‹©ä»£è¡¨æ€§çš„CPUå¹³å°ï¼šMacBook Pro M2 (ç¬”è®°æœ¬ç”µè„‘çº§) å’Œ Raspberry Pi 5 (åµŒå…¥å¼ç³»ç»Ÿçº§)ã€‚
2.  é€‰æ‹©LLMå’ŒVLMæ¨¡å‹è¿›è¡Œæµ‹è¯•ã€‚
3.  è®¾è®¡ç»Ÿä¸€çš„åŸºå‡†æµ‹è¯•æ–¹æ³•ï¼šè¿ç»­é‡‡æ ·CPUå’Œå†…å­˜ä½¿ç”¨æƒ…å†µï¼Œå¹¶è®¡ç®—æ›²çº¿ä¸‹é¢ç§¯ã€‚
4.  åˆ†æè®¡ç®—è´Ÿè½½ä¸è¾“å…¥æ•°æ®è§„æ¨¡ä¹‹é—´çš„å…³ç³»ï¼Œå‘ç°ç¼©æ”¾è§„å¾‹ã€‚
5.  åº”ç”¨é‡å­å¯å‘å‹ç¼©æŠ€æœ¯ï¼Œé™ä½æ¨¡å‹å¤§å°å’Œè®¡ç®—å¤æ‚åº¦ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š
1.  é¦–æ¬¡ç³»ç»Ÿæ€§åœ°ç ”ç©¶äº†LLMå’ŒVLMåœ¨CPUä¸Šçš„è®¡ç®—è´Ÿè½½ç¼©æ”¾è§„å¾‹ï¼Œå¡«è¡¥äº†è¯¥é¢†åŸŸçš„ç©ºç™½ã€‚
2.  å‘ç°äº†è§†è§‰è¯­è¨€æ¨¡å‹ä¸­ç”±é¢„å¤„ç†é©±åŠ¨çš„â€œåˆ†è¾¨ç‡è†ç‚¹â€ç°è±¡ã€‚
3.  éªŒè¯äº†é‡å­å¯å‘å‹ç¼©æŠ€æœ¯åœ¨é™ä½CPUèµ„æºå ç”¨å’Œèƒ½è€—æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼š
1.  åŸºå‡†æµ‹è¯•æ–¹æ³•ï¼šé‡‡ç”¨è¿ç»­é‡‡æ ·CPUå’Œå†…å­˜ä½¿ç”¨æƒ…å†µï¼Œé€šè¿‡æ›²çº¿ä¸‹é¢ç§¯ç§¯åˆ†æ¥é‡åŒ–è®¡ç®—è´Ÿè½½ã€‚
2.  é‡å­å¯å‘å‹ç¼©ï¼šå…·ä½“å‹ç¼©ç®—æ³•æœªçŸ¥ï¼Œä½†å¼ºè°ƒå…¶åœ¨é™ä½æ¨¡å‹å¤§å°å’Œè®¡ç®—å¤æ‚åº¦æ–¹é¢çš„ä½œç”¨ã€‚
3.  å®éªŒè®¾è®¡ï¼šé’ˆå¯¹ä¸åŒæ¨¡å‹å’Œå¹³å°ï¼Œè®¾è®¡åˆç†çš„è¾“å…¥æ•°æ®è§„æ¨¡èŒƒå›´ï¼Œç¡®ä¿ç»“æœçš„å¯é æ€§ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16531v1/figures/updated/llm_mac_cpu_auc.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16531v1/figures/updated/llm_rpi_cpu_auc.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16531v1/figures/updated/vlm_mac_cpu_auc.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯­è¨€æ¨¡å‹æ¨ç†çš„è®¡ç®—æˆæœ¬ä¸tokené•¿åº¦è¿‘ä¼¼çº¿æ€§ç¼©æ”¾ï¼›è§†è§‰è¯­è¨€æ¨¡å‹å­˜åœ¨â€œåˆ†è¾¨ç‡è†ç‚¹â€ç°è±¡ã€‚é‡å­å¯å‘å‹ç¼©å¯å°†å¤„ç†å™¨å’Œå†…å­˜ä½¿ç”¨é‡é™ä½é«˜è¾¾71.9%ï¼Œå¹¶å°†èƒ½è€—é™ä½é«˜è¾¾62%ï¼ŒåŒæ—¶ä¿æŒæˆ–æé«˜è¯­ä¹‰å‡†ç¡®æ€§ã€‚è¿™äº›æ•°æ®çªå‡ºäº†æ¨¡å‹å‹ç¼©åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šçš„é‡è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§è¾¹ç¼˜è®¾å¤‡ä¸Šçš„æœ¬åœ°LLM/VLMéƒ¨ç½²ï¼Œä¾‹å¦‚æ™ºèƒ½å®¶å±…è®¾å¤‡ã€å·¥ä¸šæ§åˆ¶å™¨ã€ç§»åŠ¨æœºå™¨äººç­‰ã€‚é€šè¿‡ç†è§£è®¡ç®—è´Ÿè½½çš„ç¼©æ”¾è§„å¾‹ï¼Œå¯ä»¥æ›´å¥½åœ°è¿›è¡Œèµ„æºåˆ†é…å’Œæ¨¡å‹ä¼˜åŒ–ï¼Œå®ç°æ›´é«˜æ•ˆã€æ›´èŠ‚èƒ½çš„æœ¬åœ°AIåº”ç”¨ã€‚æ­¤å¤–ï¼Œæ¨¡å‹å‹ç¼©æŠ€æœ¯å¯ä»¥è¿›ä¸€æ­¥é™ä½éƒ¨ç½²æˆæœ¬ï¼Œæ‰©å¤§åº”ç”¨èŒƒå›´ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Deploying local large language models and vision-language models on edge devices requires balancing accuracy with constrained computational and energy budgets. Although graphics processors dominate modern artificial-intelligence deployment, most consumer hardware--including laptops, desktops, industrial controllers, and embedded systems--relies on central processing units. Despite this, the computational laws governing central-processing-unit-only inference for local language and vision-language workloads remain largely unexplored. We systematically benchmark large language and vision-language models on two representative central-processing-unit tiers widely used for local inference: a MacBook Pro M2, reflecting mainstream laptop-class deployment, and a Raspberry Pi 5, representing constrained, low-power embedded settings. Using a unified methodology based on continuous sampling of processor and memory usage together with area-under-curve integration, we characterize how computational load scales with input text length for language models and with image resolution for vision-language models. We uncover two empirical scaling laws: (1) computational cost for language-model inference scales approximately linearly with token length; and (2) vision-language models exhibit a preprocessing-driven "resolution knee", where compute remains constant above an internal resolution clamp and decreases sharply below it. Beyond these laws, we show that quantum-inspired compression reduces processor and memory usage by up to 71.9% and energy consumption by up to 62%, while preserving or improving semantic accuracy. These results provide a systematic quantification of multimodal central-processing-unit-only scaling for local language and vision-language workloads, and they identify model compression and input-resolution preprocessing as effective, low-cost levers for sustainable edge inference.

