---
layout: default
title: AMUSE: Audio-Visual Benchmark and Alignment Framework for Agentic Multi-Speaker Understanding
---

# AMUSE: Audio-Visual Benchmark and Alignment Framework for Agentic Multi-Speaker Understanding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.16250" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.16250v1</a>
  <a href="https://arxiv.org/pdf/2512.16250.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.16250v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.16250v1', 'AMUSE: Audio-Visual Benchmark and Alignment Framework for Agentic Multi-Speaker Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sanjoy Chowdhury, Karren D. Yang, Xudong Liu, Fartash Faghri, Pavan Kumar Anasosalu Vasu, Oncel Tuzel, Dinesh Manocha, Chun-Liang Li, Raviteja Vemulapalli

**åˆ†ç±»**: cs.AI, cs.MA

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**AMUSEï¼šç”¨äºAgenticå¤šè¯´è¯äººç†è§£çš„éŸ³è§†é¢‘åŸºå‡†å’Œå¯¹é½æ¡†æ¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šè¯´è¯äººç†è§£` `éŸ³è§†é¢‘åˆ†æ` `Agenticæ¨ç†` `å¤šæ¨¡æ€å­¦ä¹ ` `åŸºå‡†æµ‹è¯•` `å¥–åŠ±ä¼˜åŒ–` `è‡ªè¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ¨¡æ€å¤§æ¨¡å‹åœ¨å¤šè¯´è¯äººå¯¹è¯åœºæ™¯ä¸­ï¼Œç¼ºä¹æœ‰æ•ˆçš„agenticæ¨ç†èƒ½åŠ›ï¼Œéš¾ä»¥è·Ÿè¸ªè¯´è¯äººã€ç†è§£è§’è‰²å’Œäº‹ä»¶ã€‚
2. è®ºæ–‡æå‡ºAMUSEåŸºå‡†æµ‹è¯•æ¨¡å‹åœ¨agenticä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œå¹¶è®¾è®¡RAFTæ¡†æ¶ï¼Œé€šè¿‡å¥–åŠ±ä¼˜åŒ–å’Œè‡ªè¯„ä¼°æå‡æ¨¡å‹æ€§èƒ½ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒRAFTæ¡†æ¶åœ¨AMUSEåŸºå‡†ä¸Šæ˜¾è‘—æå‡äº†å¤šæ¨¡æ€å¤§æ¨¡å‹çš„æ€§èƒ½ï¼Œç›¸å¯¹ç²¾åº¦æå‡é«˜è¾¾39.52%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†AMUSEï¼Œä¸€ä¸ªæ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨å¤šè¯´è¯äººã€ä»¥å¯¹è¯ä¸ºä¸­å¿ƒçš„åœºæ™¯ä¸‹agenticæ¨ç†èƒ½åŠ›çš„åŸºå‡†ã€‚ç°æœ‰MLLMå¦‚GPT-4oå’ŒQwen3-Omniåœ¨æ„ŸçŸ¥æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨éœ€è¦è·Ÿè¸ªè¯´è¯äººã€ç»´æŠ¤è§’è‰²ä»¥åŠç†è§£è·¨æ—¶é—´äº‹ä»¶çš„åœºæ™¯ä¸­è¡¨ç°ä¸è¶³ã€‚AMUSEåŒ…å«é›¶æ ·æœ¬ã€å¼•å¯¼å’Œagenticä¸‰ç§æ¨¡å¼ï¼Œä»¥åŠå…­ä¸ªä»»åŠ¡æ—ï¼ŒåŒ…æ‹¬æ—¶ç©ºè¯´è¯äººå®šä½å’Œå¤šæ¨¡æ€å¯¹è¯æ‘˜è¦ã€‚å®éªŒè¡¨æ˜ï¼Œç°æœ‰æ¨¡å‹åœ¨å¤šè¯´è¯äººæ¨ç†æ–¹é¢è¡¨ç°è¾ƒå¼±ï¼Œä¸”åœ¨ä¸åŒè¯„ä¼°æ¨¡å¼ä¸‹è¡Œä¸ºä¸ä¸€è‡´ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºäº†RAFTï¼Œä¸€ä¸ªæ•°æ®é«˜æ•ˆçš„agenticå¯¹é½æ¡†æ¶ï¼Œå®ƒç»“åˆäº†å¥–åŠ±ä¼˜åŒ–ã€å†…åœ¨å¤šæ¨¡æ€è‡ªè¯„ä¼°ä»¥åŠé€‰æ‹©æ€§å‚æ•°è°ƒæ•´ã€‚ä½¿ç”¨RAFTï¼Œåœ¨AMUSEåŸºå‡†ä¸Šå–å¾—äº†é«˜è¾¾39.52%çš„ç›¸å¯¹ç²¾åº¦æå‡ã€‚AMUSEå’ŒRAFTå…±åŒä¸ºç ”ç©¶å¤šæ¨¡æ€æ¨¡å‹ä¸­çš„agenticæ¨ç†å¹¶æå‡å…¶èƒ½åŠ›æä¾›äº†ä¸€ä¸ªå®ç”¨å¹³å°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨å¤„ç†å¤šè¯´è¯äººéŸ³è§†é¢‘å¯¹è¯åœºæ™¯æ—¶ï¼Œç¼ºä¹æœ‰æ•ˆçš„agenticæ¨ç†èƒ½åŠ›çš„é—®é¢˜ã€‚ç°æœ‰MLLMåœ¨æ„ŸçŸ¥æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨éœ€è¦ç†è§£è°åœ¨è¯´è¯ã€è§’è‰²æ˜¯ä»€ä¹ˆä»¥åŠäº‹ä»¶å¦‚ä½•éšæ—¶é—´æ¼”å˜ç­‰å¤æ‚åœºæ™¯ä¸­è¡¨ç°ä¸è¶³ã€‚è¿™äº›åœºæ™¯å¯¹äºä¼šè¯è§†é¢‘åŠ©æ‰‹å’Œä¼šè®®åˆ†æç­‰åº”ç”¨è‡³å…³é‡è¦ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥å°†å¤æ‚çš„éŸ³è§†é¢‘äº¤äº’åˆ†è§£ä¸ºè§„åˆ’ã€å®šä½å’Œåæ€ç­‰æ­¥éª¤ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°å’Œæå‡MLLMåœ¨agenticå¤šè¯´è¯äººç†è§£æ–¹é¢çš„åŸºå‡†ï¼ˆAMUSEï¼‰å’Œä¸€ä¸ªå¯¹é½æ¡†æ¶ï¼ˆRAFTï¼‰ã€‚é€šè¿‡AMUSEï¼Œå¯ä»¥ç³»ç»Ÿåœ°è¯„ä¼°ç°æœ‰æ¨¡å‹çš„ä¸è¶³ï¼Œå¹¶åˆ©ç”¨RAFTæ¡†æ¶ï¼Œé€šè¿‡å¥–åŠ±ä¼˜åŒ–å’Œå†…åœ¨è‡ªè¯„ä¼°ï¼Œæå‡æ¨¡å‹åœ¨è¿™äº›ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚RAFTæ¡†æ¶æ—¨åœ¨ä½¿æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œæ¨ç†å¤šè¯´è¯äººå¯¹è¯åœºæ™¯ä¸­çš„å¤æ‚äº¤äº’ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ï¼šAMUSEåŸºå‡†å’ŒRAFTå¯¹é½æ¡†æ¶ã€‚AMUSEåŸºå‡†åŒ…å«å…­ä¸ªä»»åŠ¡æ—ï¼Œæ¶µç›–æ—¶ç©ºè¯´è¯äººå®šä½ã€å¤šæ¨¡æ€å¯¹è¯æ‘˜è¦ç­‰ã€‚RAFTæ¡†æ¶åˆ™åŒ…å«å¥–åŠ±ä¼˜åŒ–æ¨¡å—ï¼Œè¯¥æ¨¡å—åˆ©ç”¨å¥–åŠ±ä¿¡å·æ¥æŒ‡å¯¼æ¨¡å‹çš„å­¦ä¹ ï¼›å†…åœ¨å¤šæ¨¡æ€è‡ªè¯„ä¼°æ¨¡å—ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹è‡ªèº«çš„è¡¨ç°ï¼›ä»¥åŠé€‰æ‹©æ€§å‚æ•°è°ƒæ•´æ¨¡å—ï¼Œç”¨äºé«˜æ•ˆåœ°æ›´æ–°æ¨¡å‹å‚æ•°ã€‚æ•´ä¸ªæµç¨‹æ—¨åœ¨ä½¿æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œæ¨ç†å¤šè¯´è¯äººå¯¹è¯åœºæ™¯ä¸­çš„å¤æ‚äº¤äº’ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†AMUSEåŸºå‡†å’ŒRAFTå¯¹é½æ¡†æ¶ã€‚AMUSEåŸºå‡†ä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°MLLMåœ¨agenticå¤šè¯´è¯äººç†è§£æ–¹é¢çš„èƒ½åŠ›ï¼Œè€ŒRAFTæ¡†æ¶åˆ™æä¾›äº†ä¸€ç§æ•°æ®é«˜æ•ˆçš„æ–¹æ³•æ¥æå‡æ¨¡å‹åœ¨è¿™äº›ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚RAFTæ¡†æ¶ç»“åˆäº†å¥–åŠ±ä¼˜åŒ–ã€å†…åœ¨è‡ªè¯„ä¼°å’Œé€‰æ‹©æ€§å‚æ•°è°ƒæ•´ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨æ•°æ®ï¼Œå¹¶é¿å…è¿‡åº¦æ‹Ÿåˆã€‚

**å…³é”®è®¾è®¡**ï¼šRAFTæ¡†æ¶çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨å¥–åŠ±æ¨¡å‹æ¥è¯„ä¼°æ¨¡å‹çš„è¡Œä¸ºï¼Œå¹¶æä¾›å¥–åŠ±ä¿¡å·ï¼›2) ä½¿ç”¨å†…åœ¨å¤šæ¨¡æ€è‡ªè¯„ä¼°æ¥è¯„ä¼°æ¨¡å‹è‡ªèº«çš„è¡¨ç°ï¼Œå¹¶æä¾›åé¦ˆï¼›3) ä½¿ç”¨é€‰æ‹©æ€§å‚æ•°è°ƒæ•´æ¥é«˜æ•ˆåœ°æ›´æ–°æ¨¡å‹å‚æ•°ï¼Œé¿å…è¿‡åº¦æ‹Ÿåˆã€‚å…·ä½“çš„å‚æ•°è®¾ç½®ã€æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç­‰ç»†èŠ‚åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ï¼Œä¾‹å¦‚ï¼Œå¥–åŠ±æ¨¡å‹çš„è®¾è®¡ã€è‡ªè¯„ä¼°æŒ‡æ ‡çš„é€‰æ‹©ä»¥åŠå‚æ•°æ›´æ–°ç­–ç•¥çš„åˆ¶å®šã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16250v1/figures/teaser.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16250v1/figures/eval-modes-short.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16250v1/figures/raft-revised.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒRAFTæ¡†æ¶åœ¨AMUSEåŸºå‡†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨å¤šä¸ªä»»åŠ¡ä¸Šï¼ŒRAFTæ¡†æ¶çš„æ€§èƒ½ä¼˜äºç°æœ‰çš„åŸºçº¿æ¨¡å‹ï¼Œç›¸å¯¹ç²¾åº¦æå‡é«˜è¾¾39.52%ã€‚è¿™è¡¨æ˜RAFTæ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆåœ°æå‡MLLMåœ¨agenticå¤šè¯´è¯äººç†è§£æ–¹é¢çš„èƒ½åŠ›ã€‚å®éªŒè¿˜è¡¨æ˜ï¼ŒRAFTæ¡†æ¶å…·æœ‰æ•°æ®é«˜æ•ˆæ€§ï¼Œèƒ½å¤Ÿåœ¨è¾ƒå°‘çš„æ•°æ®ä¸‹å–å¾—è‰¯å¥½çš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºä¼šè¯è§†é¢‘åŠ©æ‰‹ã€ä¼šè®®åˆ†æã€æ™ºèƒ½å®¢æœç­‰é¢†åŸŸã€‚é€šè¿‡æå‡æ¨¡å‹åœ¨å¤šè¯´è¯äººåœºæ™¯ä¸‹çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥å®ç°æ›´è‡ªç„¶ã€æ›´æ™ºèƒ½çš„äººæœºäº¤äº’ã€‚ä¾‹å¦‚ï¼Œåœ¨ä¼šè®®åˆ†æä¸­ï¼Œæ¨¡å‹å¯ä»¥è‡ªåŠ¨è¯†åˆ«å‘è¨€äººã€æ€»ç»“ä¼šè®®å†…å®¹ï¼Œå¹¶æå–å…³é”®ä¿¡æ¯ã€‚åœ¨æ™ºèƒ½å®¢æœä¸­ï¼Œæ¨¡å‹å¯ä»¥æ›´å¥½åœ°ç†è§£ç”¨æˆ·æ„å›¾ï¼Œå¹¶æä¾›æ›´å‡†ç¡®çš„å›ç­”ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent multimodal large language models (MLLMs) such as GPT-4o and Qwen3-Omni show strong perception but struggle in multi-speaker, dialogue-centric settings that demand agentic reasoning tracking who speaks, maintaining roles, and grounding events across time. These scenarios are central to multimodal audio-video understanding, where models must jointly reason over audio and visual streams in applications such as conversational video assistants and meeting analytics. We introduce AMUSE, a benchmark designed around tasks that are inherently agentic, requiring models to decompose complex audio-visual interactions into planning, grounding, and reflection steps. It evaluates MLLMs across three modes zero-shot, guided, and agentic and six task families, including spatio-temporal speaker grounding and multimodal dialogue summarization. Across all modes, current models exhibit weak multi-speaker reasoning and inconsistent behavior under both non-agentic and agentic evaluation. Motivated by the inherently agentic nature of these tasks and recent advances in LLM agents, we propose RAFT, a data-efficient agentic alignment framework that integrates reward optimization with intrinsic multimodal self-evaluation as reward and selective parameter adaptation for data and parameter efficient updates. Using RAFT, we achieve up to 39.52\% relative improvement in accuracy on our benchmark. Together, AMUSE and RAFT provide a practical platform for examining agentic reasoning in multimodal models and improving their capabilities.

