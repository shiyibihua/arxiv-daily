---
layout: default
title: AMUSE: Audio-Visual Benchmark and Alignment Framework for Agentic Multi-Speaker Understanding
---

# AMUSE: Audio-Visual Benchmark and Alignment Framework for Agentic Multi-Speaker Understanding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.16250" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.16250v1</a>
  <a href="https://arxiv.org/pdf/2512.16250.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.16250v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.16250v1', 'AMUSE: Audio-Visual Benchmark and Alignment Framework for Agentic Multi-Speaker Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sanjoy Chowdhury, Karren D. Yang, Xudong Liu, Fartash Faghri, Pavan Kumar Anasosalu Vasu, Oncel Tuzel, Dinesh Manocha, Chun-Liang Li, Raviteja Vemulapalli

**åˆ†ç±»**: cs.AI, cs.MA

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**AMUSEï¼šç”¨äºAgenticå¤šè¯´è¯äººç†è§£çš„è§†å¬åŸºå‡†å’Œå¯¹é½æ¡†æ¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `Agenticæ¨ç†` `å¤šè¯´è¯äººç†è§£` `è§†å¬èåˆ` `åŸºå‡†æµ‹è¯•` `å¥–åŠ±å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰MLLMåœ¨å¤šè¯´è¯äººå¯¹è¯åœºæ™¯ä¸­ï¼Œç¼ºä¹æœ‰æ•ˆçš„agenticæ¨ç†èƒ½åŠ›ï¼Œéš¾ä»¥è·Ÿè¸ªè¯´è¯äººã€ç†è§£è§’è‰²å’Œäº‹ä»¶ã€‚
2. æå‡ºRAFTæ¡†æ¶ï¼Œé€šè¿‡å¥–åŠ±ä¼˜åŒ–å’Œå¤šæ¨¡æ€è‡ªè¯„ä¼°ï¼Œå®ç°æ•°æ®é«˜æ•ˆçš„agenticå¯¹é½ï¼Œæå‡æ¨¡å‹æ€§èƒ½ã€‚
3. åœ¨AMUSEåŸºå‡†æµ‹è¯•ä¸­ï¼ŒRAFTæ¡†æ¶å®ç°äº†é«˜è¾¾39.52%çš„ç›¸å¯¹ç²¾åº¦æå‡ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†AMUSEï¼Œä¸€ä¸ªæ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨å¤šè¯´è¯äººã€ä»¥å¯¹è¯ä¸ºä¸­å¿ƒçš„åœºæ™¯ä¸‹agenticæ¨ç†èƒ½åŠ›çš„åŸºå‡†ã€‚ç°æœ‰MLLMå¦‚GPT-4oå’ŒQwen3-Omniåœ¨æ„ŸçŸ¥æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨éœ€è¦è·Ÿè¸ªè¯´è¯è€…ã€ç»´æŠ¤è§’è‰²ä»¥åŠç†è§£è·¨æ—¶é—´äº‹ä»¶çš„æ­¤ç±»åœºæ™¯ä¸­è¡¨ç°ä¸ä½³ã€‚AMUSEå›´ç»•æœ¬è´¨ä¸Šæ˜¯agenticçš„ä»»åŠ¡è®¾è®¡ï¼Œè¦æ±‚æ¨¡å‹å°†å¤æ‚çš„è§†å¬äº¤äº’åˆ†è§£ä¸ºè§„åˆ’ã€ç†è§£å’Œåæ€æ­¥éª¤ã€‚å®ƒåœ¨é›¶æ ·æœ¬ã€å¼•å¯¼å’Œagenticä¸‰ç§æ¨¡å¼ä»¥åŠå…­ä¸ªä»»åŠ¡æ—ï¼ˆåŒ…æ‹¬æ—¶ç©ºè¯´è¯äººå®šä½å’Œå¤šæ¨¡æ€å¯¹è¯æ‘˜è¦ï¼‰ä¸­è¯„ä¼°MLLMã€‚ç»“æœè¡¨æ˜ï¼Œå½“å‰æ¨¡å‹åœ¨éagenticå’Œagenticè¯„ä¼°ä¸‹éƒ½è¡¨ç°å‡ºè¾ƒå¼±çš„å¤šè¯´è¯äººæ¨ç†å’Œä¸ä¸€è‡´çš„è¡Œä¸ºã€‚å—ä»»åŠ¡çš„agenticæœ¬è´¨å’ŒLLM agentæœ€æ–°è¿›å±•çš„å¯å‘ï¼Œæœ¬æ–‡æå‡ºRAFTï¼Œä¸€ç§æ•°æ®é«˜æ•ˆçš„agenticå¯¹é½æ¡†æ¶ï¼Œå®ƒå°†å¥–åŠ±ä¼˜åŒ–ä¸å†…åœ¨å¤šæ¨¡æ€è‡ªæˆ‘è¯„ä¼°ï¼ˆä½œä¸ºå¥–åŠ±ï¼‰å’Œé€‰æ‹©æ€§å‚æ•°é€‚åº”ç›¸ç»“åˆï¼Œä»¥å®ç°æ•°æ®å’Œå‚æ•°é«˜æ•ˆçš„æ›´æ–°ã€‚ä½¿ç”¨RAFTï¼Œåœ¨åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†é«˜è¾¾39.52ï¼…çš„ç›¸å¯¹ç²¾åº¦æå‡ã€‚AMUSEå’ŒRAFTå…±åŒä¸ºæ£€æŸ¥å¤šæ¨¡æ€æ¨¡å‹ä¸­çš„agenticæ¨ç†å¹¶æé«˜å…¶èƒ½åŠ›æä¾›äº†ä¸€ä¸ªå®ç”¨çš„å¹³å°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨å¤„ç†å¤šè¯´è¯äººã€ä»¥å¯¹è¯ä¸ºä¸­å¿ƒçš„è§†å¬åœºæ™¯æ—¶ï¼Œç¼ºä¹æœ‰æ•ˆçš„agenticæ¨ç†èƒ½åŠ›çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥å‡†ç¡®è·Ÿè¸ªè¯´è¯äººèº«ä»½ã€ç»´æŠ¤è§’è‰²ä¸€è‡´æ€§ï¼Œä»¥åŠç†è§£è·¨æ—¶é—´å‘ç”Ÿçš„äº‹ä»¶ï¼Œå¯¼è‡´åœ¨ä¼šè¯è§†é¢‘åŠ©æ‰‹å’Œä¼šè®®åˆ†æç­‰åº”ç”¨ä¸­è¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨agenticæ¡†æ¶æ¥æå‡MLLMåœ¨å¤šè¯´è¯äººè§†å¬åœºæ™¯ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡å°†å¤æ‚çš„è§†å¬äº¤äº’åˆ†è§£ä¸ºè§„åˆ’ã€ç†è§£å’Œåæ€ç­‰æ­¥éª¤ï¼Œå¹¶ç»“åˆå¥–åŠ±ä¼˜åŒ–å’Œå¤šæ¨¡æ€è‡ªè¯„ä¼°ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œå¤„ç†å¤šè¯´è¯äººå¯¹è¯ä¸­çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ï¼šAMUSEåŸºå‡†æµ‹è¯•å’ŒRAFTå¯¹é½æ¡†æ¶ã€‚AMUSEåŸºå‡†ç”¨äºè¯„ä¼°MLLMåœ¨ä¸åŒagenticæ¨¡å¼ä¸‹çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬ã€å¼•å¯¼å’Œagenticæ¨¡å¼ã€‚RAFTæ¡†æ¶åˆ™ç”¨äºæå‡MLLMçš„agenticæ¨ç†èƒ½åŠ›ï¼Œå®ƒé€šè¿‡å°†å¥–åŠ±ä¼˜åŒ–ä¸å†…åœ¨å¤šæ¨¡æ€è‡ªè¯„ä¼°ç›¸ç»“åˆï¼Œå¹¶é‡‡ç”¨é€‰æ‹©æ€§å‚æ•°é€‚åº”ç­–ç•¥ï¼Œå®ç°æ•°æ®å’Œå‚æ•°é«˜æ•ˆçš„æ›´æ–°ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†RAFTï¼ˆReward-Aware Fine-Tuningï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†å¥–åŠ±ä¼˜åŒ–ä¸å†…åœ¨å¤šæ¨¡æ€è‡ªè¯„ä¼°ç›¸ç»“åˆï¼Œä»¥æå‡MLLMçš„agenticæ¨ç†èƒ½åŠ›ã€‚ä¸ä¼ ç»Ÿçš„å¾®è°ƒæ–¹æ³•ç›¸æ¯”ï¼ŒRAFTèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨æ•°æ®ï¼Œå¹¶æ ¹æ®æ¨¡å‹çš„è‡ªèº«è¯„ä¼°ç»“æœè¿›è¡Œå‚æ•°è°ƒæ•´ï¼Œä»è€Œå®ç°æ›´å¥½çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šRAFTæ¡†æ¶çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨å¤šæ¨¡æ€è‡ªè¯„ä¼°ä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œå¼•å¯¼æ¨¡å‹å­¦ä¹ æ›´æœ‰æ•ˆçš„agenticç­–ç•¥ï¼›2) é‡‡ç”¨é€‰æ‹©æ€§å‚æ•°é€‚åº”ç­–ç•¥ï¼Œåªæ›´æ–°ä¸agenticæ¨ç†ç›¸å…³çš„å‚æ•°ï¼Œä»è€Œæé«˜è®­ç»ƒæ•ˆç‡ï¼›3) è®¾è®¡äº†AMUSEåŸºå‡†æµ‹è¯•ï¼Œç”¨äºå…¨é¢è¯„ä¼°MLLMåœ¨å¤šè¯´è¯äººè§†å¬åœºæ™¯ä¸‹çš„agenticæ¨ç†èƒ½åŠ›ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16250v1/figures/teaser.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16250v1/figures/eval-modes-short.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16250v1/figures/raft-revised.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨RAFTæ¡†æ¶åï¼Œæ¨¡å‹åœ¨AMUSEåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†é«˜è¾¾39.52%çš„ç›¸å¯¹ç²¾åº¦æå‡ã€‚è¿™ä¸€æ˜¾è‘—çš„æ€§èƒ½æå‡éªŒè¯äº†RAFTæ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œè¡¨æ˜å…¶èƒ½å¤Ÿæ˜¾è‘—æé«˜MLLMåœ¨å¤šè¯´è¯äººè§†å¬åœºæ™¯ä¸‹çš„agenticæ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œå®éªŒè¿˜è¡¨æ˜ï¼ŒRAFTæ¡†æ¶å…·æœ‰æ•°æ®é«˜æ•ˆæ€§ï¼Œèƒ½å¤Ÿåœ¨å°‘é‡æ•°æ®ä¸Šå®ç°æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºä¼šè¯è§†é¢‘åŠ©æ‰‹ã€ä¼šè®®åˆ†æã€æ™ºèƒ½å®¢æœç­‰é¢†åŸŸã€‚é€šè¿‡æå‡æ¨¡å‹åœ¨å¤šè¯´è¯äººåœºæ™¯ä¸‹çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥å®ç°æ›´è‡ªç„¶ã€æ›´æ™ºèƒ½çš„äººæœºäº¤äº’ï¼Œæé«˜å·¥ä½œæ•ˆç‡å’Œç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åœ¨æ•™è‚²ã€åŒ»ç–—ç­‰é¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent multimodal large language models (MLLMs) such as GPT-4o and Qwen3-Omni show strong perception but struggle in multi-speaker, dialogue-centric settings that demand agentic reasoning tracking who speaks, maintaining roles, and grounding events across time. These scenarios are central to multimodal audio-video understanding, where models must jointly reason over audio and visual streams in applications such as conversational video assistants and meeting analytics. We introduce AMUSE, a benchmark designed around tasks that are inherently agentic, requiring models to decompose complex audio-visual interactions into planning, grounding, and reflection steps. It evaluates MLLMs across three modes zero-shot, guided, and agentic and six task families, including spatio-temporal speaker grounding and multimodal dialogue summarization. Across all modes, current models exhibit weak multi-speaker reasoning and inconsistent behavior under both non-agentic and agentic evaluation. Motivated by the inherently agentic nature of these tasks and recent advances in LLM agents, we propose RAFT, a data-efficient agentic alignment framework that integrates reward optimization with intrinsic multimodal self-evaluation as reward and selective parameter adaptation for data and parameter efficient updates. Using RAFT, we achieve up to 39.52\% relative improvement in accuracy on our benchmark. Together, AMUSE and RAFT provide a practical platform for examining agentic reasoning in multimodal models and improving their capabilities.

