---
layout: default
title: Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning
---

# Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.16917" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.16917v1</a>
  <a href="https://arxiv.org/pdf/2512.16917.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.16917v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.16917v1', 'Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Qihao Liu, Luoxin Ye, Wufei Ma, Yu-Cheng Chou, Alan Yuille

**åˆ†ç±»**: cs.AI, cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGenerative Adversarial Reasonerï¼Œé€šè¿‡å¯¹æŠ—å¼ºåŒ–å­¦ä¹ æå‡LLMçš„æ¨ç†èƒ½åŠ›ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `å¯¹æŠ—å­¦ä¹ ` `æ•°å­¦æ¨ç†` `æ¨ç†é“¾` `å¥–åŠ±å¡‘é€ ` `çŸ¥è¯†è’¸é¦`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMåœ¨æ•°å­¦æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ä»å­˜åœ¨è®¡ç®—é”™è¯¯ã€é€»è¾‘è„†å¼±å’Œè¡¨é¢åˆç†ä½†æ— æ•ˆçš„æ­¥éª¤ç­‰è¿‡ç¨‹æ€§é”™è¯¯ã€‚
2. è®ºæ–‡æå‡ºGenerative Adversarial Reasonerï¼Œé€šè¿‡å¯¹æŠ—å¼ºåŒ–å­¦ä¹ ååŒè®­ç»ƒLLMæ¨ç†å™¨å’Œåˆ¤åˆ«å™¨ï¼Œæå‡æ¨ç†çš„é€»è¾‘ä¸€è‡´æ€§å’Œå‡†ç¡®æ€§ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç°æœ‰RLå¾®è°ƒæ–¹æ³•ï¼Œåœ¨AIME24æ•°æ®é›†ä¸ŠDeepSeekæ¨¡å‹æå‡é«˜è¾¾10ä¸ªç™¾åˆ†ç‚¹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºGenerative Adversarial Reasoner çš„ on-policy è”åˆè®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¯¹æŠ—å¼ºåŒ–å­¦ä¹ ååŒè¿›åŒ– LLM æ¨ç†å™¨å’ŒåŸºäº LLM çš„åˆ¤åˆ«å™¨ï¼Œä»è€Œå¢å¼º LLM çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é‡‡ç”¨è®¡ç®—é«˜æ•ˆçš„å®¡æŸ¥æœºåˆ¶ï¼Œå°†æ¯ä¸ªæ¨ç†é“¾åˆ’åˆ†ä¸ºé€»è¾‘å®Œæ•´çš„ã€é•¿åº¦ç›¸å½“çš„ç‰‡æ®µï¼Œåˆ¤åˆ«å™¨é€šè¿‡ç®€æ´ã€ç»“æ„åŒ–çš„ç†ç”±è¯„ä¼°æ¯ä¸ªç‰‡æ®µçš„åˆç†æ€§ã€‚å­¦ä¹ è¿‡ç¨‹è€¦åˆäº†äº’è¡¥ä¿¡å·ï¼šLLM æ¨ç†å™¨å› äº§ç”Ÿæ­£ç¡®ç­”æ¡ˆçš„é€»è¾‘ä¸€è‡´æ­¥éª¤è€Œè·å¾—å¥–åŠ±ï¼Œè€Œåˆ¤åˆ«å™¨å› æ­£ç¡®æ£€æµ‹æ¨ç†è¿‡ç¨‹ä¸­çš„é”™è¯¯æˆ–åŒºåˆ†æ¨ç†è½¨è¿¹è€Œè·å¾—å¥–åŠ±ã€‚è¿™äº§ç”Ÿäº†å¯†é›†çš„ã€è‰¯å¥½æ ¡å‡†çš„ã€on-policy çš„æ­¥çº§å¥–åŠ±ï¼Œè¡¥å……äº†ç¨€ç–çš„ç²¾ç¡®åŒ¹é…ä¿¡å·ï¼Œæ”¹å–„äº†ä¿¡ç”¨åˆ†é…ï¼Œæé«˜äº†æ ·æœ¬æ•ˆç‡ï¼Œå¹¶å¢å¼ºäº† LLM çš„æ•´ä½“æ¨ç†è´¨é‡ã€‚åœ¨å„ç§æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•ç›¸å¯¹äºä½¿ç”¨æ ‡å‡† RL åè®­ç»ƒçš„å¼ºå¤§åŸºçº¿ï¼Œå®ç°äº†æŒç»­çš„æ”¶ç›Šã€‚ç‰¹åˆ«æ˜¯åœ¨ AIME24 ä¸Šï¼Œæˆ‘ä»¬å°† DeepSeek-R1-Distill-Qwen-7B ä» 54.0 æé«˜åˆ° 61.3 (+7.3)ï¼Œå°† DeepSeek-R1-Distill-Llama-8B ä» 43.7 æé«˜åˆ° 53.7 (+10.0)ã€‚æ¨¡å—åŒ–åˆ¤åˆ«å™¨è¿˜èƒ½å¤Ÿçµæ´»åœ°è¿›è¡Œå¥–åŠ±å¡‘é€ ï¼Œä»¥å®ç°è¯¸å¦‚æ•™å¸ˆçŸ¥è¯†è’¸é¦ã€åå¥½å¯¹é½å’ŒåŸºäºæ•°å­¦è¯æ˜çš„æ¨ç†ç­‰ç›®æ ‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ•°å­¦æ¨ç†è¿‡ç¨‹ä¸­å‡ºç°çš„é€»è¾‘é”™è¯¯ã€è®¡ç®—é”™è¯¯ä»¥åŠæ¨ç†æ­¥éª¤æ— æ•ˆç­‰é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºç¨€ç–çš„å¥–åŠ±ä¿¡å·ï¼ˆä¾‹å¦‚ï¼Œæœ€ç»ˆç­”æ¡ˆæ˜¯å¦æ­£ç¡®ï¼‰ï¼Œå¯¼è‡´ä¿¡ç”¨åˆ†é…å›°éš¾ï¼Œè®­ç»ƒæ•ˆç‡ä½ä¸‹ï¼Œéš¾ä»¥æœ‰æ•ˆæå‡LLMçš„æ¨ç†èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¯¹æŠ—å¼ºåŒ–å­¦ä¹ ï¼ŒåŒæ—¶è®­ç»ƒä¸€ä¸ªLLMæ¨ç†å™¨å’Œä¸€ä¸ªLLMåˆ¤åˆ«å™¨ã€‚æ¨ç†å™¨è´Ÿè´£ç”Ÿæˆæ¨ç†æ­¥éª¤ï¼Œåˆ¤åˆ«å™¨è´Ÿè´£è¯„ä¼°æ¯ä¸ªæ¨ç†æ­¥éª¤çš„åˆç†æ€§ã€‚é€šè¿‡è¿™ç§å¯¹æŠ—çš„æ–¹å¼ï¼Œæ¨ç†å™¨å¯ä»¥å­¦ä¹ åˆ°æ›´ç»†ç²’åº¦çš„å¥–åŠ±ä¿¡å·ï¼Œä»è€Œæ›´å¥½åœ°ä¼˜åŒ–æ¨ç†è¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGenerative Adversarial Reasoner (GAR) åŒ…å«ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šLLMæ¨ç†å™¨å’ŒLLMåˆ¤åˆ«å™¨ã€‚æ¨ç†å™¨è´Ÿè´£ç”Ÿæˆæ¨ç†é“¾ï¼Œåˆ¤åˆ«å™¨è´Ÿè´£è¯„ä¼°æ¨ç†é“¾ä¸­æ¯ä¸ªç‰‡æ®µçš„åˆç†æ€§ï¼Œå¹¶ç»™å‡ºç»“æ„åŒ–çš„ç†ç”±ã€‚è®­ç»ƒè¿‡ç¨‹é‡‡ç”¨on-policyå¼ºåŒ–å­¦ä¹ ï¼Œæ¨ç†å™¨æ ¹æ®åˆ¤åˆ«å™¨çš„åé¦ˆè°ƒæ•´ç­–ç•¥ï¼Œåˆ¤åˆ«å™¨æ ¹æ®æ¨ç†å™¨çš„è¾“å‡ºè°ƒæ•´åˆ¤åˆ«èƒ½åŠ›ã€‚æ•´ä¸ªæ¡†æ¶é€šè¿‡å¯¹æŠ—çš„æ–¹å¼ï¼Œä¸æ–­æå‡æ¨ç†å™¨å’Œåˆ¤åˆ«å™¨çš„èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥äº†å¯¹æŠ—å¼ºåŒ–å­¦ä¹ æ¥æå‡LLMçš„æ¨ç†èƒ½åŠ›ã€‚ä¸ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼ŒGARèƒ½å¤Ÿæä¾›æ›´å¯†é›†ã€æ›´ç»†ç²’åº¦çš„å¥–åŠ±ä¿¡å·ï¼Œä»è€Œæ›´å¥½åœ°æŒ‡å¯¼æ¨ç†å™¨çš„è®­ç»ƒã€‚æ­¤å¤–ï¼ŒGARçš„æ¨¡å—åŒ–åˆ¤åˆ«å™¨è®¾è®¡ä½¿å¾—å¯ä»¥çµæ´»åœ°è¿›è¡Œå¥–åŠ±å¡‘é€ ï¼Œä»¥é€‚åº”ä¸åŒçš„ç›®æ ‡ï¼Œä¾‹å¦‚æ•™å¸ˆçŸ¥è¯†è’¸é¦ã€åå¥½å¯¹é½å’ŒåŸºäºæ•°å­¦è¯æ˜çš„æ¨ç†ã€‚

**å…³é”®è®¾è®¡**ï¼šGARçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š(1) è®¡ç®—é«˜æ•ˆçš„å®¡æŸ¥æœºåˆ¶ï¼Œå°†æ¨ç†é“¾åˆ’åˆ†ä¸ºé€»è¾‘å®Œæ•´çš„ç‰‡æ®µï¼›(2) åˆ¤åˆ«å™¨è¾“å‡ºçš„ç»“æ„åŒ–ç†ç”±ï¼Œæä¾›æ›´ä¸°å¯Œçš„åé¦ˆä¿¡æ¯ï¼›(3) åŸºäºå¯¹æŠ—å¼ºåŒ–å­¦ä¹ çš„è®­ç»ƒç­–ç•¥ï¼ŒååŒä¼˜åŒ–æ¨ç†å™¨å’Œåˆ¤åˆ«å™¨ï¼›(4) çµæ´»çš„å¥–åŠ±å¡‘é€ æœºåˆ¶ï¼Œå¯ä»¥æ ¹æ®ä¸åŒçš„ç›®æ ‡è¿›è¡Œè°ƒæ•´ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç­‰ç»†èŠ‚æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†è¯´æ˜ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16917v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16917v1/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16917v1/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒGenerative Adversarial Reasoner åœ¨å¤šä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æå‡ã€‚åœ¨ AIME24 æ•°æ®é›†ä¸Šï¼ŒDeepSeek-R1-Distill-Qwen-7B æ¨¡å‹çš„å‡†ç¡®ç‡ä» 54.0% æé«˜åˆ° 61.3% (+7.3%)ï¼ŒDeepSeek-R1-Distill-Llama-8B æ¨¡å‹çš„å‡†ç¡®ç‡ä» 43.7% æé«˜åˆ° 53.7% (+10.0%)ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæå‡ LLM çš„æ¨ç†èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºéœ€è¦å¤æ‚æ¨ç†èƒ½åŠ›çš„å„ç§é¢†åŸŸï¼Œä¾‹å¦‚æ•°å­¦è§£é¢˜ã€ç§‘å­¦ç ”ç©¶ã€ä»£ç ç”Ÿæˆã€é€»è¾‘æ¨ç†ç­‰ã€‚é€šè¿‡æå‡LLMçš„æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥æé«˜è‡ªåŠ¨åŒ–ç³»ç»Ÿçš„æ™ºèƒ½åŒ–æ°´å¹³ï¼Œå‡å°‘äººå·¥å¹²é¢„ï¼Œæé«˜å·¥ä½œæ•ˆç‡ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åº”ç”¨äºæ›´å¹¿æ³›çš„é¢†åŸŸï¼Œä¾‹å¦‚åŒ»ç–—è¯Šæ–­ã€é‡‘èåˆ†æç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) with explicit reasoning capabilities excel at mathematical reasoning yet still commit process errors, such as incorrect calculations, brittle logic, and superficially plausible but invalid steps. In this paper, we introduce Generative Adversarial Reasoner, an on-policy joint training framework designed to enhance reasoning by co-evolving an LLM reasoner and an LLM-based discriminator through adversarial reinforcement learning. A compute-efficient review schedule partitions each reasoning chain into logically complete slices of comparable length, and the discriminator evaluates each slice's soundness with concise, structured justifications. Learning couples complementary signals: the LLM reasoner is rewarded for logically consistent steps that yield correct answers, while the discriminator earns rewards for correctly detecting errors or distinguishing traces in the reasoning process. This produces dense, well-calibrated, on-policy step-level rewards that supplement sparse exact-match signals, improving credit assignment, increasing sample efficiency, and enhancing overall reasoning quality of LLMs. Across various mathematical benchmarks, the method delivers consistent gains over strong baselines with standard RL post-training. Specifically, on AIME24, we improve DeepSeek-R1-Distill-Qwen-7B from 54.0 to 61.3 (+7.3) and DeepSeek-R1-Distill-Llama-8B from 43.7 to 53.7 (+10.0). The modular discriminator also enables flexible reward shaping for objectives such as teacher distillation, preference alignment, and mathematical proof-based reasoning.

