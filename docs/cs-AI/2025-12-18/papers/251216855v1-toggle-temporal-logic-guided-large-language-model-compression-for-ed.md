---
layout: default
title: TOGGLE: Temporal Logic-Guided Large Language Model Compression for Edge
---

# TOGGLE: Temporal Logic-Guided Large Language Model Compression for Edge

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.16855" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.16855v1</a>
  <a href="https://arxiv.org/pdf/2512.16855.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.16855v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.16855v1', 'TOGGLE: Temporal Logic-Guided Large Language Model Compression for Edge')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Khurram Khalil, Khaza Anuarul Hoque

**åˆ†ç±»**: cs.AI, cs.LO

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

**å¤‡æ³¨**: Published in the IEEE ICCAD 2025 conference

**DOI**: [10.1109/ICCAD66269.2025.11240962](https://doi.org/10.1109/ICCAD66269.2025.11240962)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTOGGLEï¼Œåˆ©ç”¨æ—¶åºé€»è¾‘å¼•å¯¼LLMå‹ç¼©ï¼Œå®ç°è¾¹ç¼˜è®¾å¤‡é«˜æ•ˆéƒ¨ç½²ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹å‹ç¼©` `è¾¹ç¼˜è®¡ç®—` `å½¢å¼åŒ–æ–¹æ³•` `ä¿¡å·æ—¶åºé€»è¾‘` `è´å¶æ–¯ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMå‹ç¼©æ–¹æ³•åœ¨é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶ï¼Œå®¹æ˜“æŸå¤±å…³é”®è¯­è¨€å±æ€§ï¼Œä¸”ç¼ºä¹å¯¹æ¨¡å‹è¡Œä¸ºä¿æŒçš„æ­£å¼ä¿è¯ã€‚
2. TOGGLEåˆ©ç”¨ä¿¡å·æ—¶åºé€»è¾‘(STL)å½¢å¼åŒ–åœ°æŒ‡å®šå’Œæ‰§è¡Œå‹ç¼©è¿‡ç¨‹ä¸­çš„è¯­è¨€å±æ€§ï¼Œä¿è¯å‹ç¼©åçš„æ¨¡å‹æ»¡è¶³ç‰¹å®šçš„è¯­è¨€çº¦æŸã€‚
3. å®éªŒè¡¨æ˜ï¼ŒTOGGLEåœ¨å¤šç§LLMæ¶æ„ä¸Šå®ç°äº†æ˜¾è‘—çš„è®¡ç®—æˆæœ¬å’Œæ¨¡å‹å¤§å°é™ä½ï¼ŒåŒæ—¶ä¿æŒäº†è¯­è¨€å±æ€§ï¼Œæ— éœ€é‡æ–°è®­ç»ƒæˆ–å¾®è°ƒã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹(LLM)åœ¨è‡ªç„¶è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºï¼Œé™åˆ¶äº†å…¶åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šçš„éƒ¨ç½²ã€‚ç°æœ‰çš„å‹ç¼©æŠ€æœ¯ï¼Œå¦‚é‡åŒ–å’Œå‰ªæï¼Œé€šå¸¸ä¼šé™ä½å…³é”®çš„è¯­è¨€å±æ€§ï¼Œå¹¶ä¸”ç¼ºä¹å¯¹ä¿æŒæ¨¡å‹è¡Œä¸ºçš„æ­£å¼ä¿è¯ã€‚æˆ‘ä»¬æå‡ºäº†æ—¶é—´é€»è¾‘å¼•å¯¼çš„å¤§å‹è¯­è¨€æ¨¡å‹å‹ç¼©(TOGGLE)ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨ä¿¡å·æ—¶åºé€»è¾‘(STL)æ¥æ­£å¼åœ°æŒ‡å®šå’Œæ‰§è¡Œå‹ç¼©è¿‡ç¨‹ä¸­çš„è¯­è¨€å±æ€§ã€‚TOGGLEé‡‡ç”¨STLé²æ£’æ€§å¼•å¯¼çš„è´å¶æ–¯ä¼˜åŒ–ï¼Œç³»ç»Ÿåœ°æ¢ç´¢åˆ†å±‚é‡åŒ–å’Œå‰ªæé…ç½®ï¼Œç”Ÿæˆå‹ç¼©æ¨¡å‹ï¼Œåœ¨ä¸é‡æ–°è®­ç»ƒæˆ–å¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œæ­£å¼æ»¡è¶³æŒ‡å®šçš„è¯­è¨€çº¦æŸã€‚åœ¨å››ä¸ªLLMæ¶æ„(GPT-2ã€DeepSeek-V2 7Bã€LLaMA 3 8Bå’ŒMistral 7B)ä¸Šè¯„ä¼°TOGGLEï¼Œæˆ‘ä»¬å®ç°äº†é«˜è¾¾3.3å€çš„è®¡ç®—æˆæœ¬(FLOPs)é™ä½å’Œé«˜è¾¾68.8%çš„æ¨¡å‹å¤§å°é™ä½ï¼ŒåŒæ—¶æ»¡è¶³æ‰€æœ‰è¯­è¨€å±æ€§ã€‚TOGGLEä»£è¡¨äº†å½¢å¼åŒ–æ–¹æ³•é¦–æ¬¡é›†æˆåˆ°LLMå‹ç¼©ä¸­ï¼Œä»è€Œèƒ½å¤Ÿåœ¨è¾¹ç¼˜ç¡¬ä»¶ä¸Šé«˜æ•ˆã€å¯éªŒè¯åœ°éƒ¨ç½²LLMã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²çš„é—®é¢˜ã€‚ç°æœ‰çš„é‡åŒ–å’Œå‰ªæç­‰å‹ç¼©æŠ€æœ¯è™½ç„¶èƒ½é™ä½è®¡ç®—æˆæœ¬ï¼Œä½†å¾€å¾€ä¼šæŸå®³LLMçš„å…³é”®è¯­è¨€å±æ€§ï¼Œå¹¶ä¸”ç¼ºä¹å¯¹å‹ç¼©åæ¨¡å‹è¡Œä¸ºçš„æ­£å¼ä¿è¯ï¼Œéš¾ä»¥ç¡®ä¿å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å¯é æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šTOGGLEçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ä¿¡å·æ—¶åºé€»è¾‘ï¼ˆSTLï¼‰æ¥å½¢å¼åŒ–åœ°æè¿°LLMéœ€è¦æ»¡è¶³çš„è¯­è¨€å±æ€§ï¼Œå¹¶åœ¨å‹ç¼©è¿‡ç¨‹ä¸­ï¼Œé€šè¿‡ä¼˜åŒ–ç®—æ³•æ¥ç¡®ä¿å‹ç¼©åçš„æ¨¡å‹ä»ç„¶æ»¡è¶³è¿™äº›å±æ€§ã€‚è¿™ç§æ–¹æ³•é¿å…äº†ä¼ ç»Ÿå‹ç¼©æ–¹æ³•ä¸­å¸¸è§çš„è¯­è¨€å±æ€§æŸå¤±é—®é¢˜ï¼Œå¹¶æä¾›äº†å¯¹æ¨¡å‹è¡Œä¸ºçš„æ­£å¼ä¿è¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTOGGLEæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) **STLå±æ€§å®šä¹‰**ï¼šä½¿ç”¨STLè¯­è¨€æ¥å½¢å¼åŒ–åœ°æè¿°LLMéœ€è¦æ»¡è¶³çš„è¯­è¨€å±æ€§ï¼Œä¾‹å¦‚è¯­æ³•æ­£ç¡®æ€§ã€è¯­ä¹‰ä¸€è‡´æ€§ç­‰ã€‚2) **é²æ£’æ€§è¯„ä¼°**ï¼šè®¾è®¡é²æ£’æ€§å‡½æ•°æ¥è¯„ä¼°LLMåœ¨å¤šå¤§ç¨‹åº¦ä¸Šæ»¡è¶³å®šä¹‰çš„STLå±æ€§ã€‚é²æ£’æ€§å€¼è¶Šé«˜ï¼Œè¡¨ç¤ºæ¨¡å‹è¶Šç¬¦åˆè¯¥å±æ€§ã€‚3) **è´å¶æ–¯ä¼˜åŒ–**ï¼šä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–ç®—æ³•æ¥æœç´¢æœ€ä½³çš„é‡åŒ–å’Œå‰ªæé…ç½®ï¼Œç›®æ ‡æ˜¯æœ€å°åŒ–æ¨¡å‹å¤§å°å’Œè®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶æœ€å¤§åŒ–STLå±æ€§çš„é²æ£’æ€§ã€‚4) **æ¨¡å‹å‹ç¼©**ï¼šæ ¹æ®è´å¶æ–¯ä¼˜åŒ–å¾—åˆ°çš„é…ç½®ï¼Œå¯¹LLMè¿›è¡Œé‡åŒ–å’Œå‰ªæï¼Œå¾—åˆ°å‹ç¼©åçš„æ¨¡å‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šTOGGLEæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå°†å½¢å¼åŒ–æ–¹æ³•ï¼ˆSTLï¼‰å¼•å…¥åˆ°LLMå‹ç¼©ä¸­ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºç»éªŒçš„å‹ç¼©æ–¹æ³•ä¸åŒï¼ŒTOGGLEèƒ½å¤Ÿå¯¹å‹ç¼©åçš„æ¨¡å‹è¡Œä¸ºæä¾›æ­£å¼ä¿è¯ï¼Œç¡®ä¿å…¶æ»¡è¶³ç‰¹å®šçš„è¯­è¨€å±æ€§ã€‚è¿™æ˜¯é¦–æ¬¡å°†å½¢å¼åŒ–éªŒè¯æŠ€æœ¯åº”ç”¨äºLLMå‹ç¼©é¢†åŸŸã€‚

**å…³é”®è®¾è®¡**ï¼šTOGGLEçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) **STLå±æ€§çš„é€‰æ‹©**ï¼šé€‰æ‹©åˆé€‚çš„STLå±æ€§æ¥æè¿°LLMçš„å…³é”®è¯­è¨€èƒ½åŠ›ï¼Œä¾‹å¦‚ä½¿ç”¨STLæ¥æè¿°æ¨¡å‹ç”Ÿæˆè¯­æ³•æ­£ç¡®çš„å¥å­çš„èƒ½åŠ›ã€‚2) **é²æ£’æ€§å‡½æ•°çš„å®šä¹‰**ï¼šè®¾è®¡èƒ½å¤Ÿå‡†ç¡®è¯„ä¼°æ¨¡å‹æ˜¯å¦æ»¡è¶³STLå±æ€§çš„é²æ£’æ€§å‡½æ•°ã€‚3) **è´å¶æ–¯ä¼˜åŒ–ç®—æ³•çš„é…ç½®**ï¼šé€‰æ‹©åˆé€‚çš„è´å¶æ–¯ä¼˜åŒ–ç®—æ³•ï¼Œå¹¶è°ƒæ•´å…¶å‚æ•°ï¼Œä»¥é«˜æ•ˆåœ°æœç´¢æœ€ä½³çš„é‡åŒ–å’Œå‰ªæé…ç½®ã€‚4) **é‡åŒ–å’Œå‰ªæç­–ç•¥**ï¼šé€‰æ‹©åˆé€‚çš„é‡åŒ–å’Œå‰ªæç­–ç•¥ï¼Œä¾‹å¦‚ä½¿ç”¨ä¸åŒçš„é‡åŒ–æ¯”ç‰¹æ•°å’Œå‰ªææ¯”ä¾‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

TOGGLEåœ¨GPT-2ã€DeepSeek-V2 7Bã€LLaMA 3 8Bå’ŒMistral 7Bç­‰å¤šç§LLMæ¶æ„ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼ŒTOGGLEèƒ½å¤Ÿå®ç°é«˜è¾¾3.3å€çš„è®¡ç®—æˆæœ¬(FLOPs)é™ä½å’Œé«˜è¾¾68.8%çš„æ¨¡å‹å¤§å°é™ä½ï¼ŒåŒæ—¶æ»¡è¶³æ‰€æœ‰æŒ‡å®šçš„è¯­è¨€å±æ€§ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒTOGGLEæ˜¯ä¸€ç§é«˜æ•ˆä¸”å¯é çš„LLMå‹ç¼©æ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

TOGGLEçš„åº”ç”¨åœºæ™¯å¹¿æ³›ï¼ŒåŒ…æ‹¬åœ¨æ™ºèƒ½æ‰‹æœºã€æ— äººæœºã€æœºå™¨äººç­‰èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²LLMï¼Œå®ç°æœ¬åœ°åŒ–çš„è‡ªç„¶è¯­è¨€å¤„ç†èƒ½åŠ›ã€‚è¯¥ç ”ç©¶çš„å®é™…ä»·å€¼åœ¨äºé™ä½äº†LLMçš„éƒ¨ç½²é—¨æ§›ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨æ›´å¤šåœºæ™¯ä¸‹åº”ç”¨ã€‚æœªæ¥ï¼ŒTOGGLEæœ‰æœ›æ¨åŠ¨è¾¹ç¼˜æ™ºèƒ½çš„å‘å±•ï¼Œå®ç°æ›´æ™ºèƒ½ã€æ›´é«˜æ•ˆçš„è¾¹ç¼˜è®¡ç®—ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) deliver exceptional performance across natural language tasks but demand substantial computational resources, limiting their deployment on resource-constrained edge devices. Existing compression techniques, such as quantization and pruning, often degrade critical linguistic properties and lack formal guarantees for preserving model behavior. We propose Temporal Logic-Guided Large Language Model Compression (TOGGLE), a novel framework that leverages Signal Temporal Logic (STL) to formally specify and enforce linguistic properties during compression. TOGGLE employs an STL robustness-guided Bayesian optimization to systematically explore layer-wise quantization and pruning configurations, generating compressed models that formally satisfy specified linguistic constraints without retraining or fine-tuning. Evaluating TOGGLE on four LLM architectures (GPT-2, DeepSeek-V2 7B, LLaMA 3 8B, and Mistral 7B), we achieve up to 3.3x reduction in computational costs (FLOPs) and up to a 68.8% reduction in model size while satisfying all linguistic properties. TOGGLE represents the first integration of formal methods into LLM compression, enabling efficient, verifiable deployment of LLMs on edge hardware.

