---
layout: default
title: TOGGLE: Temporal Logic-Guided Large Language Model Compression for Edge
---

# TOGGLE: Temporal Logic-Guided Large Language Model Compression for Edge

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.16855" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.16855v1</a>
  <a href="https://arxiv.org/pdf/2512.16855.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.16855v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.16855v1', 'TOGGLE: Temporal Logic-Guided Large Language Model Compression for Edge')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Khurram Khalil, Khaza Anuarul Hoque

**åˆ†ç±»**: cs.AI, cs.LO

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

**å¤‡æ³¨**: Published in the IEEE ICCAD 2025 conference

**DOI**: [10.1109/ICCAD66269.2025.11240962](https://doi.org/10.1109/ICCAD66269.2025.11240962)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**TOGGLEï¼šæ—¶åºé€»è¾‘å¼•å¯¼çš„å¤§è¯­è¨€æ¨¡å‹è¾¹ç¼˜å‹ç¼©æ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹å‹ç¼©` `è¾¹ç¼˜è®¡ç®—` `å½¢å¼åŒ–æ–¹æ³•` `ä¿¡å·æ—¶åºé€»è¾‘` `è´å¶æ–¯ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMå‹ç¼©æ–¹æ³•åœ¨é™ä½è®¡ç®—èµ„æºéœ€æ±‚çš„åŒæ—¶ï¼Œå¾€å¾€ä¼šæŸå®³æ¨¡å‹çš„å…³é”®è¯­è¨€å±æ€§ï¼Œä¸”ç¼ºä¹å¯¹æ¨¡å‹è¡Œä¸ºçš„æ­£å¼ä¿è¯ã€‚
2. TOGGLEåˆ©ç”¨ä¿¡å·æ—¶åºé€»è¾‘ï¼ˆSTLï¼‰æ¥å½¢å¼åŒ–åœ°æŒ‡å®šå’Œå¼ºåˆ¶æ‰§è¡Œå‹ç¼©è¿‡ç¨‹ä¸­çš„è¯­è¨€å±æ€§ï¼Œç¡®ä¿å‹ç¼©åçš„æ¨¡å‹æ»¡è¶³ç‰¹å®šçš„è¯­è¨€çº¦æŸã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒTOGGLEåœ¨æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬å’Œæ¨¡å‹å¤§å°çš„åŒæ—¶ï¼Œèƒ½å¤Ÿä¿æŒLLMçš„è¯­è¨€å±æ€§ï¼Œå®ç°é«˜æ•ˆä¸”å¯éªŒè¯çš„è¾¹ç¼˜éƒ¨ç½²ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºï¼Œé™åˆ¶äº†å…¶åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šçš„éƒ¨ç½²ã€‚ç°æœ‰çš„å‹ç¼©æŠ€æœ¯ï¼Œå¦‚é‡åŒ–å’Œå‰ªæï¼Œé€šå¸¸ä¼šé™ä½å…³é”®çš„è¯­è¨€å±æ€§ï¼Œå¹¶ä¸”ç¼ºä¹å¯¹æ¨¡å‹è¡Œä¸ºä¿æŒçš„æ­£å¼ä¿è¯ã€‚æˆ‘ä»¬æå‡ºäº†æ—¶åºé€»è¾‘å¼•å¯¼çš„å¤§è¯­è¨€æ¨¡å‹å‹ç¼©ï¼ˆTOGGLEï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨ä¿¡å·æ—¶åºé€»è¾‘ï¼ˆSTLï¼‰æ¥æ­£å¼åœ°æŒ‡å®šå’Œå¼ºåˆ¶æ‰§è¡Œå‹ç¼©è¿‡ç¨‹ä¸­çš„è¯­è¨€å±æ€§ã€‚TOGGLEé‡‡ç”¨STLé²æ£’æ€§å¼•å¯¼çš„è´å¶æ–¯ä¼˜åŒ–ï¼Œç³»ç»Ÿåœ°æ¢ç´¢é€å±‚é‡åŒ–å’Œå‰ªæé…ç½®ï¼Œç”Ÿæˆå‹ç¼©æ¨¡å‹ï¼Œåœ¨ä¸é‡æ–°è®­ç»ƒæˆ–å¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œæ­£å¼åœ°æ»¡è¶³æŒ‡å®šçš„è¯­è¨€çº¦æŸã€‚åœ¨å››ä¸ªLLMæ¶æ„ï¼ˆGPT-2ã€DeepSeek-V2 7Bã€LLaMA 3 8Bå’ŒMistral 7Bï¼‰ä¸Šè¯„ä¼°TOGGLEï¼Œæˆ‘ä»¬å®ç°äº†é«˜è¾¾3.3å€çš„è®¡ç®—æˆæœ¬ï¼ˆFLOPsï¼‰é™ä½å’Œé«˜è¾¾68.8%çš„æ¨¡å‹å¤§å°é™ä½ï¼ŒåŒæ—¶æ»¡è¶³æ‰€æœ‰è¯­è¨€å±æ€§ã€‚TOGGLEä»£è¡¨äº†å½¢å¼åŒ–æ–¹æ³•é¦–æ¬¡é›†æˆåˆ°LLMå‹ç¼©ä¸­ï¼Œä»è€Œèƒ½å¤Ÿåœ¨è¾¹ç¼˜ç¡¬ä»¶ä¸Šé«˜æ•ˆã€å¯éªŒè¯åœ°éƒ¨ç½²LLMã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²çš„é—®é¢˜ã€‚ç°æœ‰å‹ç¼©æ–¹æ³•ï¼ˆå¦‚é‡åŒ–å’Œå‰ªæï¼‰è™½ç„¶èƒ½å‡å°æ¨¡å‹ä½“ç§¯å’Œè®¡ç®—é‡ï¼Œä½†å¸¸å¸¸ä¼šé™ä½æ¨¡å‹çš„è¯­è¨€èƒ½åŠ›ï¼Œå¹¶ä¸”ç¼ºä¹å½¢å¼åŒ–çš„éªŒè¯æ‰‹æ®µæ¥ä¿è¯å‹ç¼©åçš„æ¨¡å‹ä»ç„¶æ»¡è¶³ç‰¹å®šçš„è¯­è¨€å±æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šTOGGLEçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ä¿¡å·æ—¶åºé€»è¾‘ï¼ˆSTLï¼‰æ¥å½¢å¼åŒ–åœ°æè¿°LLMéœ€è¦æ»¡è¶³çš„è¯­è¨€å±æ€§ï¼Œå¹¶åœ¨æ¨¡å‹å‹ç¼©è¿‡ç¨‹ä¸­ï¼Œé€šè¿‡ä¼˜åŒ–ç®—æ³•æ¥å¯»æ‰¾æ»¡è¶³è¿™äº›å±æ€§çš„æœ€ä½³å‹ç¼©é…ç½®ã€‚è¿™æ ·å¯ä»¥åœ¨ä¿è¯æ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œå°½å¯èƒ½åœ°å‡å°æ¨¡å‹ä½“ç§¯å’Œè®¡ç®—é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTOGGLEæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) **STLå±æ€§å®šä¹‰æ¨¡å—**ï¼šä½¿ç”¨STLè¯­è¨€å®šä¹‰LLMéœ€è¦æ»¡è¶³çš„è¯­è¨€å±æ€§ï¼Œä¾‹å¦‚ï¼ŒæŸäº›ç‰¹å®šè¯è¯­å¿…é¡»å‡ºç°åœ¨è¾“å‡ºä¸­ï¼Œæˆ–è€…è¾“å‡ºå¿…é¡»ç¬¦åˆæŸç§è¯­æ³•ç»“æ„ã€‚2) **é²æ£’æ€§è¯„ä¼°æ¨¡å—**ï¼šè¯„ä¼°å½“å‰æ¨¡å‹å¯¹äºSTLå±æ€§çš„æ»¡è¶³ç¨‹åº¦ï¼Œå³è®¡ç®—é²æ£’æ€§å¾—åˆ†ã€‚é²æ£’æ€§å¾—åˆ†è¶Šé«˜ï¼Œè¡¨ç¤ºæ¨¡å‹è¶Šèƒ½æ»¡è¶³å¯¹åº”çš„STLå±æ€§ã€‚3) **è´å¶æ–¯ä¼˜åŒ–æ¨¡å—**ï¼šä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–ç®—æ³•æ¥æœç´¢æœ€ä½³çš„é‡åŒ–å’Œå‰ªæé…ç½®ã€‚è¯¥æ¨¡å—ä»¥é²æ£’æ€§å¾—åˆ†ä½œä¸ºä¼˜åŒ–ç›®æ ‡ï¼Œæ—¨åœ¨æ‰¾åˆ°æ—¢èƒ½å‡å°æ¨¡å‹ä½“ç§¯å’Œè®¡ç®—é‡ï¼Œåˆèƒ½ä¿è¯æ¨¡å‹æ»¡è¶³STLå±æ€§çš„å‹ç¼©é…ç½®ã€‚4) **æ¨¡å‹å‹ç¼©æ¨¡å—**ï¼šæ ¹æ®è´å¶æ–¯ä¼˜åŒ–æ¨¡å—è¾“å‡ºçš„é…ç½®ï¼Œå¯¹LLMè¿›è¡Œé‡åŒ–å’Œå‰ªæã€‚

**å…³é”®åˆ›æ–°**ï¼šTOGGLEçš„å…³é”®åˆ›æ–°åœ¨äºå°†å½¢å¼åŒ–æ–¹æ³•ï¼ˆSTLï¼‰å¼•å…¥åˆ°LLMå‹ç¼©ä¸­ã€‚ä¸ä¼ ç»Ÿçš„å‹ç¼©æ–¹æ³•ä¸åŒï¼ŒTOGGLEèƒ½å¤Ÿæä¾›å½¢å¼åŒ–çš„ä¿è¯ï¼Œç¡®ä¿å‹ç¼©åçš„æ¨¡å‹ä»ç„¶æ»¡è¶³ç‰¹å®šçš„è¯­è¨€å±æ€§ã€‚æ­¤å¤–ï¼ŒTOGGLEè¿˜é‡‡ç”¨é²æ£’æ€§å¼•å¯¼çš„è´å¶æ–¯ä¼˜åŒ–ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æœç´¢æœ€ä½³çš„å‹ç¼©é…ç½®ã€‚

**å…³é”®è®¾è®¡**ï¼šTOGGLEçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) **STLå±æ€§çš„é€‰å–**ï¼šé€‰æ‹©åˆé€‚çš„STLå±æ€§å¯¹äºä¿è¯å‹ç¼©åæ¨¡å‹çš„è¯­è¨€èƒ½åŠ›è‡³å…³é‡è¦ã€‚è®ºæ–‡ä¸­å¯èƒ½ç»™å‡ºäº†é€‰æ‹©STLå±æ€§çš„ä¸€äº›æŒ‡å¯¼åŸåˆ™ã€‚2) **é²æ£’æ€§å‡½æ•°çš„å®šä¹‰**ï¼šé²æ£’æ€§å‡½æ•°éœ€è¦èƒ½å¤Ÿå‡†ç¡®åœ°åæ˜ æ¨¡å‹å¯¹äºSTLå±æ€§çš„æ»¡è¶³ç¨‹åº¦ã€‚ä¸åŒçš„STLå±æ€§å¯èƒ½éœ€è¦ä¸åŒçš„é²æ£’æ€§å‡½æ•°ã€‚3) **è´å¶æ–¯ä¼˜åŒ–ç®—æ³•çš„å‚æ•°è®¾ç½®**ï¼šè´å¶æ–¯ä¼˜åŒ–ç®—æ³•çš„å‚æ•°è®¾ç½®ä¼šå½±å“æœç´¢æ•ˆç‡å’Œæœ€ç»ˆç»“æœã€‚è®ºæ–‡ä¸­å¯èƒ½ç»™å‡ºäº†å‚æ•°è®¾ç½®çš„ä¸€äº›å»ºè®®ã€‚4) **é‡åŒ–å’Œå‰ªæç­–ç•¥çš„é€‰æ‹©**ï¼šTOGGLEå¯ä»¥æ”¯æŒä¸åŒçš„é‡åŒ–å’Œå‰ªæç­–ç•¥ã€‚è®ºæ–‡ä¸­å¯èƒ½æ¯”è¾ƒäº†ä¸åŒç­–ç•¥çš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

TOGGLEåœ¨GPT-2ã€DeepSeek-V2 7Bã€LLaMA 3 8Bå’ŒMistral 7Bå››ä¸ªLLMæ¶æ„ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼ŒTOGGLEèƒ½å¤Ÿå®ç°é«˜è¾¾3.3å€çš„è®¡ç®—æˆæœ¬ï¼ˆFLOPsï¼‰é™ä½å’Œé«˜è¾¾68.8%çš„æ¨¡å‹å¤§å°é™ä½ï¼ŒåŒæ—¶æ»¡è¶³æ‰€æœ‰æŒ‡å®šçš„è¯­è¨€å±æ€§ã€‚è¿™äº›ç»“æœè¡¨æ˜TOGGLEåœ¨LLMå‹ç¼©æ–¹é¢å…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

TOGGLEçš„åº”ç”¨åœºæ™¯å¹¿æ³›ï¼ŒåŒ…æ‹¬åœ¨æ™ºèƒ½æ‰‹æœºã€æ— äººæœºã€æœºå™¨äººç­‰èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²LLMã€‚è¯¥æŠ€æœ¯èƒ½å¤Ÿä½¿è¿™äº›è®¾å¤‡åœ¨æœ¬åœ°è¿è¡Œå¤æ‚çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œä¾‹å¦‚è¯­éŸ³åŠ©æ‰‹ã€æœºå™¨ç¿»è¯‘ã€æ™ºèƒ½å®¢æœç­‰ï¼Œä»è€Œæé«˜å“åº”é€Ÿåº¦ã€ä¿æŠ¤ç”¨æˆ·éšç§ï¼Œå¹¶é™ä½å¯¹äº‘ç«¯æœåŠ¡å™¨çš„ä¾èµ–ã€‚æœªæ¥ï¼ŒTOGGLEæœ‰æœ›æ¨åŠ¨LLMåœ¨ç‰©è”ç½‘ã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸçš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) deliver exceptional performance across natural language tasks but demand substantial computational resources, limiting their deployment on resource-constrained edge devices. Existing compression techniques, such as quantization and pruning, often degrade critical linguistic properties and lack formal guarantees for preserving model behavior. We propose Temporal Logic-Guided Large Language Model Compression (TOGGLE), a novel framework that leverages Signal Temporal Logic (STL) to formally specify and enforce linguistic properties during compression. TOGGLE employs an STL robustness-guided Bayesian optimization to systematically explore layer-wise quantization and pruning configurations, generating compressed models that formally satisfy specified linguistic constraints without retraining or fine-tuning. Evaluating TOGGLE on four LLM architectures (GPT-2, DeepSeek-V2 7B, LLaMA 3 8B, and Mistral 7B), we achieve up to 3.3x reduction in computational costs (FLOPs) and up to a 68.8% reduction in model size while satisfying all linguistic properties. TOGGLE represents the first integration of formal methods into LLM compression, enabling efficient, verifiable deployment of LLMs on edge hardware.

