---
layout: default
title: AdaptCache: KV Cache Native Storage Hierarchy for Low-Delay and High-Quality Language Model Serving
---

# AdaptCache: KV Cache Native Storage Hierarchy for Low-Delay and High-Quality Language Model Serving

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.00105" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.00105v1</a>
  <a href="https://arxiv.org/pdf/2509.00105.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.00105v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.00105v1', 'AdaptCache: KV Cache Native Storage Hierarchy for Low-Delay and High-Quality Language Model Serving')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shaoting Feng, Hanchen Li, Kuntai Du, Zhuohan Gu, Yuhan Liu, Jiayi Yao, Siddhant Ray, Samuel Shen, Yihua Cheng, Ganesh Ananthanarayanan, Junchen Jiang

**åˆ†ç±»**: cs.OS, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-28

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAdaptCacheä»¥è§£å†³LLMæœåŠ¡ä¸­çš„KVç¼“å­˜å»¶è¿Ÿé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `KVç¼“å­˜` `æœ‰æŸå‹ç¼©` `å»¶è¿Ÿä¼˜åŒ–` `ç”Ÿæˆè´¨é‡` `åŠ¨æ€ç­–ç•¥` `æ™ºèƒ½æœåŠ¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMæœåŠ¡ç³»ç»Ÿåœ¨å¤„ç†ä¸Šä¸‹æ–‡æ—¶å­˜åœ¨å†—ä½™è®¡ç®—ï¼Œå¯¼è‡´KVç¼“å­˜çš„åŠ è½½å»¶è¿Ÿè¾ƒé«˜ï¼Œå½±å“æœåŠ¡æ•ˆç‡ã€‚
2. æœ¬æ–‡æå‡ºAdaptCacheï¼Œé€šè¿‡æœ‰æŸå‹ç¼©æŠ€æœ¯ä¼˜åŒ–KVç¼“å­˜çš„å­˜å‚¨å’ŒåŠ è½½ç­–ç•¥ï¼Œä»¥æé«˜DRAMå‘½ä¸­ç‡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒAdaptCacheåœ¨å»¶è¿Ÿå’Œç”Ÿæˆè´¨é‡ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰é™æ€å‹ç¼©æ–¹æ³•ï¼Œæå‡å¹…åº¦å¯è¾¾55%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åº”ç”¨é€šå¸¸ä¼šé‡ç”¨å…ˆå‰å¤„ç†çš„ä¸Šä¸‹æ–‡ï¼Œå¦‚èŠå¤©è®°å½•å’Œæ–‡æ¡£ï¼Œè¿™å¯¼è‡´äº†æ˜¾è‘—çš„å†—ä½™è®¡ç®—ã€‚ç°æœ‰çš„LLMæœåŠ¡ç³»ç»Ÿé€šè¿‡å­˜å‚¨å·²å¤„ç†ä¸Šä¸‹æ–‡çš„KVç¼“å­˜æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚ç„¶è€Œï¼Œéšç€LLMåº”ç”¨çš„æ‰©å±•ï¼ŒKVç¼“å­˜çš„æ€»å¤§å°å˜å¾—è¿‡äºåºå¤§ï¼Œéœ€åŒæ—¶ä½¿ç”¨DRAMå’ŒSSDè¿›è¡Œå­˜å‚¨ã€‚ä»¥å¾€å°†KVç¼“å­˜å­˜å‚¨åœ¨DRAMå’ŒSSDä¸­çš„æ–¹æ³•å­˜åœ¨é«˜åŠ è½½å»¶è¿Ÿçš„é—®é¢˜ï¼Œä¸»è¦æ˜¯å› ä¸ºå¤§å¤šæ•°KVç¼“å­˜å‘½ä¸­æ¥è‡ªSSDï¼ŒåŠ è½½é€Ÿåº¦è¾ƒæ…¢ã€‚ä¸ºæé«˜DRAMä¸­çš„KVç¼“å­˜å‘½ä¸­ç‡ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æœ‰æŸKVç¼“å­˜å‹ç¼©çš„æ–¹æ³•ï¼Œæ—¨åœ¨æœ€å¤§åŒ–DRAMå‘½ä¸­ç‡å¹¶æœ€å°åŒ–åŠ è½½å»¶è¿Ÿï¼ŒåŒæ—¶ä¸æ˜¾è‘—é™ä½ç”Ÿæˆè´¨é‡ã€‚ä¸å¤šç§é™æ€å‹ç¼©åŸºçº¿ç›¸æ¯”ï¼ŒAdaptCacheåœ¨ç›¸åŒè´¨é‡ä¸‹å®ç°äº†1.43è‡³2.4å€çš„å»¶è¿ŸèŠ‚çœï¼Œå¹¶åœ¨ç›¸åŒå»¶è¿Ÿä¸‹æé«˜äº†6%è‡³55%çš„è´¨é‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹æœåŠ¡ä¸­KVç¼“å­˜çš„é«˜å»¶è¿Ÿé—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨DRAMå’ŒSSDä¸­å­˜å‚¨KVç¼“å­˜ï¼Œå¯¼è‡´åŠ è½½å»¶è¿Ÿè¾ƒé«˜ï¼Œå½±å“æœåŠ¡æ•ˆç‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºäº†ä¸€ç§æœ‰æŸKVç¼“å­˜å‹ç¼©ç³»ç»Ÿï¼ŒåŠ¨æ€é€‰æ‹©å‹ç¼©ç®—æ³•ã€å‹ç¼©ç‡å’Œè®¾å¤‡æ”¾ç½®ï¼Œä»¥æœ€å¤§åŒ–DRAMå‘½ä¸­ç‡å¹¶æœ€å°åŒ–åŠ è½½å»¶è¿Ÿï¼ŒåŒæ—¶ä¿æŒç”Ÿæˆè´¨é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç³»ç»Ÿä¸»è¦åŒ…æ‹¬KVç¼“å­˜ç®¡ç†æ¨¡å—ã€å‹ç¼©å†³ç­–æ¨¡å—å’ŒåŠ è½½ä¼˜åŒ–æ¨¡å—ã€‚KVç¼“å­˜ç®¡ç†æ¨¡å—è´Ÿè´£ç¼“å­˜çš„å­˜å‚¨å’Œæ£€ç´¢ï¼Œå‹ç¼©å†³ç­–æ¨¡å—æ ¹æ®ä¸Šä¸‹æ–‡ç‰¹å¾é€‰æ‹©åˆé€‚çš„å‹ç¼©ç­–ç•¥ï¼ŒåŠ è½½ä¼˜åŒ–æ¨¡å—åˆ™è´Ÿè´£é«˜æ•ˆåŠ è½½ç¼“å­˜ã€‚

**å…³é”®åˆ›æ–°**ï¼šAdaptCacheçš„åˆ›æ–°åœ¨äºå…¶åŠ¨æ€å‹ç¼©ç­–ç•¥ï¼Œèƒ½å¤Ÿæ ¹æ®ä¸åŒKVç¼“å­˜æ¡ç›®çš„ç‰¹æ€§è¿›è¡Œä¼˜åŒ–ï¼Œä¸ä¼ ç»Ÿé™æ€å‹ç¼©æ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æé«˜äº†ç¼“å­˜å‘½ä¸­ç‡å’Œé™ä½äº†å»¶è¿Ÿã€‚

**å…³é”®è®¾è®¡**ï¼šç³»ç»Ÿè®¾è®¡ä¸­è€ƒè™‘äº†å‹ç¼©ç®—æ³•çš„é€‰æ‹©ã€å‹ç¼©ç‡çš„è°ƒæ•´ä»¥åŠå­˜å‚¨è®¾å¤‡çš„åˆç†åˆ†é…ç­‰å…³é”®å‚æ•°ï¼Œä»¥ç¡®ä¿åœ¨ä¸æ˜¾è‘—é™ä½ç”Ÿæˆè´¨é‡çš„å‰æä¸‹ï¼Œæœ€å¤§åŒ–æ€§èƒ½æå‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAdaptCacheåœ¨å»¶è¿Ÿæ–¹é¢å®ç°äº†1.43è‡³2.4å€çš„èŠ‚çœï¼ŒåŒæ—¶åœ¨ç›¸åŒå»¶è¿Ÿæ¡ä»¶ä¸‹ï¼Œç”Ÿæˆè´¨é‡æå‡äº†6%è‡³55%ã€‚ä¸å¤šç§é™æ€å‹ç¼©åŸºçº¿ç›¸æ¯”ï¼Œè¡¨ç°å‡ºæ˜¾è‘—çš„ä¼˜åŠ¿ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½å®¢æœã€åœ¨çº¿æ•™è‚²å’Œå®æ—¶ç¿»è¯‘ç­‰éœ€è¦é«˜æ•ˆå¤„ç†å¤§é‡ä¸Šä¸‹æ–‡çš„LLMåº”ç”¨ã€‚é€šè¿‡ä¼˜åŒ–KVç¼“å­˜çš„å­˜å‚¨å’ŒåŠ è½½ï¼ŒAdaptCacheèƒ½å¤Ÿæ˜¾è‘—æå‡ç³»ç»Ÿå“åº”é€Ÿåº¦å’Œç”¨æˆ·ä½“éªŒï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œåº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language model (LLM) applications often reuse previously processed context, such as chat history and documents, which introduces significant redundant computation. Existing LLM serving systems address such redundant computation by storing the KV caches of processed context and loading the corresponding KV cache when a new request reuses the context. Further, as these LLM applications scale, the total size of KV caches becomes excessively large and requires both DRAM and SSD for full storage.
>   However, prior work that stores KV caches in DRAM and SSD suffers from high loading delays, as most KV cache hits come from SSD, which is slow to load. To increase the KV cache hit rate on DRAM, we identify lossy KV cache compression as a promising approach. We design a lossy compression system that decides the compression algorithm, compression rate and device placement for each KV cache entry to maximise DRAM hits and minimise loading delay without significantly degrading generation quality. Compared to various static compression baselines across three tasks, our system AdaptCache achieves 1.43--2.4 x delay savings at the same quality and 6--55% quality improvements at the same delay.

