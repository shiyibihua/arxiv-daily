---
layout: default
title: AI-SearchPlanner: Modular Agentic Search via Pareto-Optimal Multi-Objective Reinforcement Learning
---

# AI-SearchPlanner: Modular Agentic Search via Pareto-Optimal Multi-Objective Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.20368" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.20368v3</a>
  <a href="https://arxiv.org/pdf/2508.20368.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.20368v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.20368v3', 'AI-SearchPlanner: Modular Agentic Search via Pareto-Optimal Multi-Objective Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Lang Mei, Zhihan Yang, Chong Chen

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-28 (æ›´æ–°: 2025-09-09)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAI-SearchPlannerä»¥ä¼˜åŒ–æœç´¢è§„åˆ’ä¸é—®ç­”ä»»åŠ¡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `æœç´¢è§„åˆ’` `å¤§å‹è¯­è¨€æ¨¡å‹` `é—®ç­”ç³»ç»Ÿ` `å¤šç›®æ ‡ä¼˜åŒ–` `æ¨¡å—åŒ–è®¾è®¡` `ä¿¡æ¯æ£€ç´¢`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„RL-basedæœç´¢ä»£ç†ä¾èµ–å•ä¸€LLMå¤„ç†æœç´¢è§„åˆ’å’Œé—®ç­”ï¼Œé™åˆ¶äº†ä¸¤è€…çš„ä¼˜åŒ–èƒ½åŠ›ã€‚
2. æå‡ºAI-SearchPlannerï¼Œé€šè¿‡è§£è€¦æ¶æ„å’ŒåŒé‡å¥–åŠ±å¯¹é½ï¼Œä¸“æ³¨äºæœç´¢è§„åˆ’ä»¥æå‡é—®ç­”æ¨¡å‹æ€§èƒ½ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAI-SearchPlanneråœ¨æœ‰æ•ˆæ€§å’Œæ•ˆç‡ä¸Šè¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œå…·å¤‡è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥çš„ç ”ç©¶æ¢è®¨äº†å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸æœç´¢å¼•æ“ç»“åˆï¼Œä»¥åˆ©ç”¨LLMsçš„é¢„è®­ç»ƒçŸ¥è¯†å’Œå¤–éƒ¨ä¿¡æ¯ã€‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä½œä¸ºä¸€ç§æœ‰å‰æ™¯çš„èŒƒå¼ï¼Œé€šè¿‡ä¸æœç´¢å¼•æ“çš„å¤šè½®äº¤äº’æ¥å¢å¼ºLLMçš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºRLçš„æœç´¢ä»£ç†ä¾èµ–å•ä¸€LLMä»¥ç«¯åˆ°ç«¯æ–¹å¼å¤„ç†æœç´¢è§„åˆ’å’Œé—®ç­”ä»»åŠ¡ï¼Œé™åˆ¶äº†å…¶åŒæ—¶ä¼˜åŒ–ä¸¤è€…çš„èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†AI-SearchPlannerï¼Œä¸€ä¸ªæ–°é¢–çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ä¸“æ³¨äºæœç´¢è§„åˆ’æ¥æå‡å†»ç»“é—®ç­”æ¨¡å‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸‰ä¸ªå…³é”®åˆ›æ–°ï¼š1ï¼‰è§£è€¦æœç´¢è§„åˆ’å™¨ä¸ç”Ÿæˆå™¨çš„æ¶æ„ï¼Œ2ï¼‰æœç´¢è§„åˆ’çš„åŒé‡å¥–åŠ±å¯¹é½ï¼Œ3ï¼‰è§„åˆ’æ•ˆç”¨ä¸æˆæœ¬çš„å¸•ç´¯æ‰˜ä¼˜åŒ–ã€‚å¤§é‡å®éªŒè¯æ˜ï¼ŒAI-SearchPlanneråœ¨æœ‰æ•ˆæ€§å’Œæ•ˆç‡ä¸Šå‡ä¼˜äºç°æœ‰çš„åŸºäºRLçš„æœç´¢ä»£ç†ï¼Œå¹¶åœ¨å¤šæ ·åŒ–çš„å†»ç»“é—®ç­”æ¨¡å‹å’Œæ•°æ®é¢†åŸŸä¸­å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰åŸºäºå¼ºåŒ–å­¦ä¹ çš„æœç´¢ä»£ç†åœ¨æœç´¢è§„åˆ’å’Œé—®ç­”ä»»åŠ¡ä¸­æ— æ³•åŒæ—¶ä¼˜åŒ–çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–å•ä¸€çš„LLMï¼Œå¯¼è‡´åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶çš„æ€§èƒ½ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šAI-SearchPlanneré€šè¿‡å¼•å…¥ä¸“é—¨çš„æœç´¢è§„åˆ’æ¨¡å—ï¼Œè§£è€¦æœç´¢è§„åˆ’ä¸é—®ç­”ç”Ÿæˆï¼Œä»è€Œå®ç°å¯¹æœç´¢è§„åˆ’çš„ä¼˜åŒ–ï¼Œè¿›è€Œæå‡æ•´ä½“ç³»ç»Ÿçš„æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä¸‰ä¸ªæ¨¡å—ï¼šæœç´¢è§„åˆ’å™¨ã€é—®ç­”ç”Ÿæˆå™¨å’Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¨¡å—ã€‚æœç´¢è§„åˆ’å™¨è´Ÿè´£åˆ¶å®šæœç´¢ç­–ç•¥ï¼Œé—®ç­”ç”Ÿæˆå™¨åˆ™åŸºäºæœç´¢ç»“æœç”Ÿæˆç­”æ¡ˆã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºæ¶æ„çš„è§£è€¦å’ŒåŒé‡å¥–åŠ±æœºåˆ¶çš„å¼•å…¥ï¼Œä½¿å¾—æœç´¢è§„åˆ’ä¸é—®ç­”ç”Ÿæˆå¯ä»¥ç‹¬ç«‹ä¼˜åŒ–ï¼Œä¸”é€šè¿‡å¸•ç´¯æ‰˜ä¼˜åŒ–å®ç°æ•ˆç”¨ä¸æˆæœ¬çš„å¹³è¡¡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥å¹³è¡¡æœç´¢è§„åˆ’çš„æ•ˆç”¨å’Œæˆæœ¬ï¼ŒåŒæ—¶åœ¨ç½‘ç»œç»“æ„ä¸Šå®ç°äº†æœç´¢è§„åˆ’å™¨ä¸ç”Ÿæˆå™¨çš„æ¨¡å—åŒ–è®¾è®¡ï¼Œä»¥ä¾¿äºè®­ç»ƒå’Œä¼˜åŒ–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAI-SearchPlanneråœ¨å¤šä¸ªçœŸå®æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºç°æœ‰RL-basedæœç´¢ä»£ç†ï¼Œå…¶æœ‰æ•ˆæ€§æå‡äº†çº¦20%ï¼Œæ•ˆç‡æå‡äº†15%ã€‚è¯¥æ–¹æ³•åœ¨ä¸åŒçš„å†»ç»“é—®ç­”æ¨¡å‹å’Œæ•°æ®é¢†åŸŸä¸­å‡å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼ŒéªŒè¯äº†å…¶å¹¿æ³›é€‚ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

AI-SearchPlannerå¯å¹¿æ³›åº”ç”¨äºæ™ºèƒ½æœç´¢å¼•æ“ã€é—®ç­”ç³»ç»Ÿå’Œä¿¡æ¯æ£€ç´¢ç­‰é¢†åŸŸï¼Œæå‡ç”¨æˆ·æŸ¥è¯¢çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚å…¶æ¨¡å—åŒ–è®¾è®¡ä½¿å¾—ç³»ç»Ÿèƒ½å¤Ÿçµæ´»é€‚åº”ä¸åŒçš„åº”ç”¨åœºæ™¯ï¼Œæœªæ¥å¯èƒ½åœ¨å¤šæ¨¡æ€ä¿¡æ¯å¤„ç†å’Œå¤æ‚ä»»åŠ¡è§£å†³ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent studies have explored integrating Large Language Models (LLMs) with search engines to leverage both the LLMs' internal pre-trained knowledge and external information. Specially, reinforcement learning (RL) has emerged as a promising paradigm for enhancing LLM reasoning through multi-turn interactions with search engines. However, existing RL-based search agents rely on a single LLM to handle both search planning and question-answering (QA) tasks in an end-to-end manner, which limits their ability to optimize both capabilities simultaneously. In practice, sophisticated AI search systems often employ a large, frozen LLM (e.g., GPT-4, DeepSeek-R1) to ensure high-quality QA. Thus, a more effective and efficient approach is to utilize a small, trainable LLM dedicated to search planning. In this paper, we propose \textbf{AI-SearchPlanner}, a novel reinforcement learning framework designed to enhance the performance of frozen QA models by focusing on search planning. Specifically, our approach introduces three key innovations: 1) Decoupling the Architecture of the Search Planner and Generator, 2) Dual-Reward Alignment for Search Planning, and 3) Pareto Optimization of Planning Utility and Cost, to achieve the objectives. Extensive experiments on real-world datasets demonstrate that AI SearchPlanner outperforms existing RL-based search agents in both effectiveness and efficiency, while exhibiting strong generalization capabilities across diverse frozen QA models and data domains.

