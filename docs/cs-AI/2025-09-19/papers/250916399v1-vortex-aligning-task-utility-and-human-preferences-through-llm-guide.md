---
layout: default
title: VORTEX: Aligning Task Utility and Human Preferences through LLM-Guided Reward Shaping
---

# VORTEX: Aligning Task Utility and Human Preferences through LLM-Guided Reward Shaping

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.16399" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.16399v1</a>
  <a href="https://arxiv.org/pdf/2509.16399.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.16399v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.16399v1', 'VORTEX: Aligning Task Utility and Human Preferences through LLM-Guided Reward Shaping')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Guojun Xiong, Milind Tambe

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-19

**å¤‡æ³¨**: 28pages, 19figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**VORTEXï¼šé€šè¿‡LLMå¼•å¯¼çš„å¥–åŠ±å¡‘é€ å¯¹é½ä»»åŠ¡æ•ˆç”¨å’Œäººç±»åå¥½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `äººæœºåä½œ` `å¥–åŠ±å¡‘é€ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `å¤šç›®æ ‡ä¼˜åŒ–` `ç¤¾ä¼šå½±å“ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰AIå†³ç­–ç³»ç»Ÿéš¾ä»¥ç›´æ¥é€‚åº”ä»¥è‡ªç„¶è¯­è¨€è¡¨è¾¾çš„äººç±»åå¥½ï¼Œå¯¼è‡´ä¼˜åŒ–ç›®æ ‡ä¸å®é™…éœ€æ±‚è„±èŠ‚ã€‚
2. VORTEXæ¡†æ¶é€šè¿‡LLMç”Ÿæˆå¥–åŠ±å¡‘é€ å‡½æ•°ï¼Œåœ¨ä¼˜åŒ–ä»»åŠ¡æ•ˆç”¨çš„åŒæ—¶ï¼Œèå…¥äººç±»åé¦ˆï¼Œå®ç°äºŒè€…å¯¹é½ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒVORTEXåœ¨ä¿æŒä»»åŠ¡æ€§èƒ½çš„åŒæ—¶ï¼Œèƒ½æ›´å¥½åœ°æ»¡è¶³äººç±»åå¥½ï¼Œä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨ç¤¾ä¼šå½±å“ä¼˜åŒ–ä¸­ï¼ŒAIå†³ç­–ç³»ç»Ÿé€šå¸¸ä¾èµ–äºä¼˜åŒ–è‰¯å¥½æ ¡å‡†çš„æ•°å­¦ç›®æ ‡çš„æ±‚è§£å™¨ã€‚ç„¶è€Œï¼Œè¿™äº›æ±‚è§£å™¨æ— æ³•ç›´æ¥é€‚åº”ä¸æ–­å˜åŒ–çš„äººç±»åå¥½ï¼Œè¿™äº›åå¥½é€šå¸¸ä»¥è‡ªç„¶è¯­è¨€è€Œéæ­£å¼çº¦æŸæ¥è¡¨è¾¾ã€‚æœ€è¿‘çš„æ–¹æ³•é€šè¿‡ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»åå¥½æè¿°ä¸­ç”Ÿæˆæ–°çš„å¥–åŠ±å‡½æ•°æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚è™½ç„¶çµæ´»ï¼Œä½†å®ƒä»¬æœ‰ç‰ºç‰²ç³»ç»Ÿæ ¸å¿ƒæ•ˆç”¨ä¿è¯çš„é£é™©ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†	exttt{VORTEX}ï¼Œä¸€ä¸ªè¯­è¨€å¼•å¯¼çš„å¥–åŠ±å¡‘é€ æ¡†æ¶ï¼Œå®ƒåœ¨è‡ªé€‚åº”åœ°ç»“åˆäººç±»åé¦ˆçš„åŒæ—¶ï¼Œä¿ç•™äº†å·²å»ºç«‹çš„ä¼˜åŒ–ç›®æ ‡ã€‚é€šè¿‡å°†é—®é¢˜å½¢å¼åŒ–ä¸ºå¤šç›®æ ‡ä¼˜åŒ–ï¼Œæˆ‘ä»¬ä½¿ç”¨LLMåŸºäºå£å¤´å¼ºåŒ–å’Œæ–‡æœ¬æ¢¯åº¦æç¤ºæ›´æ–°æ¥è¿­ä»£åœ°ç”Ÿæˆå¡‘é€ å¥–åŠ±ã€‚è¿™å…è®¸åˆ©ç›Šç›¸å…³è€…é€šè¿‡è‡ªç„¶è¯­è¨€æ¥å¼•å¯¼å†³ç­–è¡Œä¸ºï¼Œè€Œæ— éœ€ä¿®æ”¹æ±‚è§£å™¨æˆ–æŒ‡å®šæƒè¡¡æƒé‡ã€‚æˆ‘ä»¬æä¾›äº†	exttt{VORTEX}æ”¶æ•›åˆ°æ•ˆç”¨å’Œåå¥½æ»¡è¶³ä¹‹é—´çš„å¸•ç´¯æ‰˜æœ€ä¼˜æƒè¡¡çš„ç†è®ºä¿è¯ã€‚åœ¨ç°å®ä¸–ç•Œåˆ†é…ä»»åŠ¡ä¸­çš„ç»éªŒç»“æœè¡¨æ˜ï¼Œ	exttt{VORTEX}åœ¨æ»¡è¶³äººç±»å¯¹é½çš„è¦†ç›–ç›®æ ‡çš„åŒæ—¶ï¼Œä¿æŒäº†è¾ƒé«˜çš„ä»»åŠ¡æ€§èƒ½ï¼Œä¼˜äºåŸºçº¿ã€‚è¿™é¡¹å·¥ä½œå¼•å…¥äº†ä¸€ç§å®ç”¨ä¸”å…·æœ‰ç†è®ºåŸºç¡€çš„èŒƒä¾‹ï¼Œç”¨äºåœ¨è‡ªç„¶è¯­è¨€æŒ‡å¯¼ä¸‹çš„äººæœºåä½œä¼˜åŒ–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³AIå†³ç­–ç³»ç»Ÿåœ¨ç¤¾ä¼šå½±å“ä¼˜åŒ–ä¸­ï¼Œéš¾ä»¥æœ‰æ•ˆæ•´åˆä»¥è‡ªç„¶è¯­è¨€è¡¨è¾¾çš„äººç±»åå¥½è¿™ä¸€é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•è¦ä¹ˆéš¾ä»¥é€‚åº”åŠ¨æ€å˜åŒ–çš„äººç±»åå¥½ï¼Œè¦ä¹ˆåœ¨èå…¥åå¥½æ—¶ç‰ºç‰²äº†ç³»ç»ŸåŸæœ‰çš„æ•ˆç”¨ä¿è¯ã€‚è¿™å¯¼è‡´å†³ç­–ç»“æœä¸å®é™…éœ€æ±‚å­˜åœ¨åå·®ï¼Œå½±å“äº†ç³»ç»Ÿçš„å®é™…åº”ç”¨æ•ˆæœã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†äººç±»åå¥½èå…¥åˆ°å¥–åŠ±å‡½æ•°ä¸­ï¼Œé€šè¿‡LLMç”Ÿæˆå¥–åŠ±å¡‘é€ é¡¹ï¼Œåœ¨ä¸æ”¹å˜åŸæœ‰ä¼˜åŒ–ç›®æ ‡çš„å‰æä¸‹ï¼Œå¼•å¯¼æ±‚è§£å™¨æœç€æ»¡è¶³äººç±»åå¥½çš„æ–¹å‘è¿›è¡Œä¼˜åŒ–ã€‚è¿™ç§æ–¹æ³•å°†é—®é¢˜è½¬åŒ–ä¸ºå¤šç›®æ ‡ä¼˜åŒ–é—®é¢˜ï¼Œåœ¨æ•ˆç”¨å’Œåå¥½ä¹‹é—´å¯»æ±‚å¸•ç´¯æ‰˜æœ€ä¼˜è§£ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVORTEXæ¡†æ¶åŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) ä»»åŠ¡æ±‚è§£å™¨ï¼šè´Ÿè´£ä¼˜åŒ–åŸæœ‰çš„æ•°å­¦ç›®æ ‡ã€‚2) LLMå¥–åŠ±å¡‘é€ å™¨ï¼šåŸºäºäººç±»åé¦ˆç”Ÿæˆå¥–åŠ±å¡‘é€ é¡¹ã€‚3) å¤šç›®æ ‡ä¼˜åŒ–å™¨ï¼šåœ¨ä»»åŠ¡æ•ˆç”¨å’Œåå¥½æ»¡è¶³ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚æ¡†æ¶çš„æµç¨‹æ˜¯ï¼šé¦–å…ˆï¼Œä»»åŠ¡æ±‚è§£å™¨ç”Ÿæˆåˆå§‹è§£ï¼›ç„¶åï¼Œäººç±»æä¾›è‡ªç„¶è¯­è¨€åé¦ˆï¼›æ¥ç€ï¼ŒLLMå¥–åŠ±å¡‘é€ å™¨æ ¹æ®åé¦ˆç”Ÿæˆå¥–åŠ±å¡‘é€ é¡¹ï¼›æœ€åï¼Œå¤šç›®æ ‡ä¼˜åŒ–å™¨ç»“åˆåŸæœ‰å¥–åŠ±å’Œå¡‘é€ å¥–åŠ±ï¼Œæ›´æ–°è§£ã€‚è¿™ä¸ªè¿‡ç¨‹è¿­ä»£è¿›è¡Œï¼Œç›´åˆ°æ”¶æ•›ã€‚

**å…³é”®åˆ›æ–°**ï¼šVORTEXçš„å…³é”®åˆ›æ–°åœ¨äºä½¿ç”¨LLMè¿›è¡Œå¥–åŠ±å¡‘é€ ï¼Œå¹¶å°†å…¶å½¢å¼åŒ–ä¸ºå¤šç›®æ ‡ä¼˜åŒ–é—®é¢˜ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒVORTEXèƒ½å¤Ÿåœ¨ä¸ä¿®æ”¹æ±‚è§£å™¨æˆ–æŒ‡å®šæƒè¡¡æƒé‡çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡è‡ªç„¶è¯­è¨€å¼•å¯¼å†³ç­–è¡Œä¸ºï¼Œæ›´åŠ çµæ´»å’Œæ˜“äºä½¿ç”¨ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æä¾›äº†ç†è®ºä¿è¯ï¼Œè¯æ˜VORTEXèƒ½å¤Ÿæ”¶æ•›åˆ°å¸•ç´¯æ‰˜æœ€ä¼˜è§£ã€‚

**å…³é”®è®¾è®¡**ï¼šVORTEXçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨æ–‡æœ¬æ¢¯åº¦æç¤ºæ›´æ–°LLMï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£äººç±»åé¦ˆã€‚2) å°†å¥–åŠ±å¡‘é€ é—®é¢˜å½¢å¼åŒ–ä¸ºå¤šç›®æ ‡ä¼˜åŒ–é—®é¢˜ï¼Œå¹¶ä½¿ç”¨å¸•ç´¯æ‰˜æœ€ä¼˜æ€§ä½œä¸ºä¼˜åŒ–ç›®æ ‡ã€‚3) è®¾è®¡åˆé€‚çš„å¥–åŠ±å‡½æ•°ï¼Œä»¥å¹³è¡¡ä»»åŠ¡æ•ˆç”¨å’Œåå¥½æ»¡è¶³ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„å–å†³äºå…·ä½“çš„åº”ç”¨åœºæ™¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨èµ„æºåˆ†é…ä»»åŠ¡ä¸­ï¼ŒVORTEXåœ¨æ»¡è¶³äººç±»å¯¹é½çš„è¦†ç›–ç›®æ ‡æ–¹é¢ä¼˜äºåŸºçº¿æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒäº†è¾ƒé«˜çš„ä»»åŠ¡æ€§èƒ½ã€‚å…·ä½“è€Œè¨€ï¼ŒVORTEXåœ¨è¦†ç›–ç‡æŒ‡æ ‡ä¸Šæå‡äº†XX%ï¼Œåœ¨ä»»åŠ¡æ•ˆç”¨æŒ‡æ ‡ä¸Šä¸åŸºçº¿æ–¹æ³•æŒå¹³ã€‚è¿™è¯æ˜äº†VORTEXåœ¨å¹³è¡¡ä»»åŠ¡æ•ˆç”¨å’Œåå¥½æ»¡è¶³æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

VORTEXæ¡†æ¶å¯åº”ç”¨äºå„ç§ç¤¾ä¼šå½±å“ä¼˜åŒ–åœºæ™¯ï¼Œä¾‹å¦‚èµ„æºåˆ†é…ã€å…¬å…±æœåŠ¡è°ƒåº¦ã€åŒ»ç–—èµ„æºé…ç½®ç­‰ã€‚é€šè¿‡èå…¥åˆ©ç›Šç›¸å…³è€…çš„åå¥½ï¼ŒVORTEXèƒ½å¤Ÿç”Ÿæˆæ›´å…¬å¹³ã€æ›´ç¬¦åˆå®é™…éœ€æ±‚çš„å†³ç­–æ–¹æ¡ˆï¼Œæé«˜ç³»ç»Ÿçš„ç¤¾ä¼šæ•ˆç›Šå’Œç”¨æˆ·æ»¡æ„åº¦ã€‚æœªæ¥ï¼ŒVORTEXæœ‰æœ›æˆä¸ºäººæœºåä½œä¼˜åŒ–é¢†åŸŸçš„é‡è¦å·¥å…·ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In social impact optimization, AI decision systems often rely on solvers that optimize well-calibrated mathematical objectives. However, these solvers cannot directly accommodate evolving human preferences, typically expressed in natural language rather than formal constraints. Recent approaches address this by using large language models (LLMs) to generate new reward functions from preference descriptions. While flexible, they risk sacrificing the system's core utility guarantees. In this paper, we propose \texttt{VORTEX}, a language-guided reward shaping framework that preserves established optimization goals while adaptively incorporating human feedback. By formalizing the problem as multi-objective optimization, we use LLMs to iteratively generate shaping rewards based on verbal reinforcement and text-gradient prompt updates. This allows stakeholders to steer decision behavior via natural language without modifying solvers or specifying trade-off weights. We provide theoretical guarantees that \texttt{VORTEX} converges to Pareto-optimal trade-offs between utility and preference satisfaction. Empirical results in real-world allocation tasks demonstrate that \texttt{VORTEX} outperforms baselines in satisfying human-aligned coverage goals while maintaining high task performance. This work introduces a practical and theoretically grounded paradigm for human-AI collaborative optimization guided by natural language.

