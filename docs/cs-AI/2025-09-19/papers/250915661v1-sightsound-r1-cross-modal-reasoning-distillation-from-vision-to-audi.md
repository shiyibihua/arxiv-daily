---
layout: default
title: SightSound-R1: Cross-Modal Reasoning Distillation from Vision to Audio Language Models
---

# SightSound-R1: Cross-Modal Reasoning Distillation from Vision to Audio Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.15661" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.15661v1</a>
  <a href="https://arxiv.org/pdf/2509.15661.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.15661v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.15661v1', 'SightSound-R1: Cross-Modal Reasoning Distillation from Vision to Audio Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Qiaolin Wang, Xilin Jiang, Linyang He, Junkai Wu, Nima Mesgarani

**åˆ†ç±»**: cs.SD, cs.AI, cs.CL, eess.AS

**å‘å¸ƒæ—¥æœŸ**: 2025-09-19

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSightSound-R1ï¼Œé€šè¿‡è·¨æ¨¡æ€è’¸é¦æå‡å¬è§‰è¯­è¨€æ¨¡å‹åœ¨å¤æ‚å£°æ™¯ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è·¨æ¨¡æ€å­¦ä¹ ` `çŸ¥è¯†è’¸é¦` `è§†å¬é—®ç­”` `å¬è§‰è¯­è¨€æ¨¡å‹` `æ€ç»´é“¾` `éŸ³é¢‘ç†è§£` `æ¨ç†èƒ½åŠ›`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¬è§‰è¯­è¨€æ¨¡å‹åœ¨å¤æ‚å£°æ™¯æ¨ç†èƒ½åŠ›ä¸Šå­˜åœ¨ä¸è¶³ï¼Œç¼ºä¹å¤§è§„æ¨¡æ€ç»´é“¾éŸ³é¢‘æ•°æ®æ˜¯ä¸»è¦ç“¶é¢ˆã€‚
2. SightSound-R1é€šè¿‡è·¨æ¨¡æ€è’¸é¦ï¼Œå°†è§†è§‰è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›è¿ç§»åˆ°å¬è§‰è¯­è¨€æ¨¡å‹ï¼Œå¼¥è¡¥æ•°æ®å’Œæ¨¡æ€å·®è·ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒSightSound-R1æ˜¾è‘—æå‡äº†å¬è§‰è¯­è¨€æ¨¡å‹åœ¨è§†å¬é—®ç­”ä»»åŠ¡ä¸Šçš„æ¨ç†æ€§èƒ½ï¼Œä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹å¬è§‰è¯­è¨€æ¨¡å‹(LALM)åœ¨éŸ³é¢‘ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤æ‚å£°æ™¯ä¸­çš„æ¨ç†èƒ½åŠ›ä»è½åäºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹(LVLM)ã€‚ä¸è§†è§‰é¢†åŸŸç›¸æ¯”ï¼Œç¼ºä¹å¤§è§„æ¨¡çš„æ€ç»´é“¾éŸ³é¢‘æ•°æ®æ¥æ•™å¯¼LALMé€æ­¥æ¨ç†æ˜¯ä¸€ä¸ªç“¶é¢ˆã€‚ä¸ºäº†è§„é¿è¿™ç§æ•°æ®å’Œæ¨¡æ€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†SightSound-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªè·¨æ¨¡æ€è’¸é¦æ¡†æ¶ï¼Œå®ƒå°†æ›´å¼ºçš„LVLMæ•™å¸ˆçš„å…ˆè¿›æ¨ç†èƒ½åŠ›è½¬ç§»åˆ°è¾ƒå¼±çš„LALMå­¦ç”Ÿèº«ä¸Šï¼Œåœ¨åŒä¸€ä¸ªè§†å¬é—®ç­”(AVQA)æ•°æ®é›†ä¸Šè¿›è¡Œã€‚SightSound-R1åŒ…æ‹¬ä¸‰ä¸ªæ ¸å¿ƒæ­¥éª¤ï¼š(i)æµ‹è¯•æ—¶ç¼©æ”¾ï¼Œä»LVLMæ•™å¸ˆç”Ÿæˆä»¥éŸ³é¢‘ä¸ºä¸­å¿ƒçš„æ€ç»´é“¾(CoT)ï¼›(ii)éŸ³é¢‘å¼•å¯¼éªŒè¯ï¼Œä»¥è¿‡æ»¤å¹»è§‰ï¼›(iii)ä¸€ä¸ªè’¸é¦ç®¡é“ï¼ŒåŒ…æ‹¬ç›‘ç£å¾®è°ƒ(SFT)ï¼Œç„¶åæ˜¯ç”¨äºLALMå­¦ç”Ÿçš„ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–(GRPO)ã€‚ç»“æœè¡¨æ˜ï¼ŒSightSound-R1æé«˜äº†LALMåœ¨é¢†åŸŸå†…AVQAæµ‹è¯•é›†ä»¥åŠæœªè§è¿‡çš„å¬è§‰åœºæ™¯å’Œé—®é¢˜ä¸­çš„æ¨ç†æ€§èƒ½ï¼Œä¼˜äºé¢„è®­ç»ƒå’Œä»…æ ‡ç­¾è’¸é¦çš„åŸºçº¿ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¾—å‡ºç»“è®ºï¼Œè§†è§‰æ¨ç†å¯ä»¥æœ‰æ•ˆåœ°è½¬ç§»åˆ°éŸ³é¢‘æ¨¡å‹ï¼Œå¹¶å¯ä»¥é€šè¿‡ä¸°å¯Œçš„è§†å¬æ•°æ®è¿›è¡Œæ‰©å±•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¬è§‰è¯­è¨€æ¨¡å‹(LALM)åœ¨å¤æ‚å£°æ™¯ä¸­æ¨ç†èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹å¤§è§„æ¨¡çš„æ€ç»´é“¾éŸ³é¢‘æ•°æ®ï¼Œéš¾ä»¥è®­ç»ƒLALMè¿›è¡Œé€æ­¥æ¨ç†ï¼Œå¯¼è‡´å…¶æ€§èƒ½è½åäºè§†è§‰è¯­è¨€æ¨¡å‹(LVLM)ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è·¨æ¨¡æ€è’¸é¦ï¼Œå°†æ›´å¼ºå¤§çš„LVLMçš„æ¨ç†èƒ½åŠ›è¿ç§»åˆ°è¾ƒå¼±çš„LALMã€‚åˆ©ç”¨LVLMåœ¨è§†è§‰æ¨ç†æ–¹é¢çš„ä¼˜åŠ¿ï¼Œç”Ÿæˆä¼ªæ ‡ç­¾æ•°æ®ï¼Œå¹¶ä»¥æ­¤è®­ç»ƒLALMï¼Œä»è€Œæå‡å…¶åœ¨å¬è§‰åœºæ™¯ä¸‹çš„æ¨ç†èƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•æœ‰æ•ˆåœ°åˆ©ç”¨äº†ç°æœ‰çš„è§†å¬æ•°æ®ï¼Œé¿å…äº†ç›´æ¥æ”¶é›†å¤§è§„æ¨¡æ€ç»´é“¾éŸ³é¢‘æ•°æ®çš„å›°éš¾ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSightSound-R1æ¡†æ¶åŒ…å«ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼š1) **æµ‹è¯•æ—¶ç¼©æ”¾(Test-time Scaling)**ï¼šåˆ©ç”¨LVLMç”Ÿæˆä»¥éŸ³é¢‘ä¸ºä¸­å¿ƒçš„æ€ç»´é“¾(CoT)ã€‚é€šè¿‡è°ƒæ•´LVLMçš„è¾“å…¥ï¼Œä½¿å…¶æ›´åŠ å…³æ³¨éŸ³é¢‘ä¿¡æ¯ï¼Œä»è€Œç”Ÿæˆæ›´é€‚åˆLALMå­¦ä¹ çš„æ¨ç†è¿‡ç¨‹ã€‚2) **éŸ³é¢‘å¼•å¯¼éªŒè¯(Audio-grounded Validation)**ï¼šè¿‡æ»¤LVLMç”Ÿæˆçš„æ€ç»´é“¾ä¸­çš„å¹»è§‰ã€‚é€šè¿‡éŸ³é¢‘ä¿¡æ¯éªŒè¯æ¨ç†æ­¥éª¤çš„åˆç†æ€§ï¼Œå»é™¤ä¸éŸ³é¢‘å†…å®¹ä¸ç¬¦çš„æ¨ç†è·¯å¾„ï¼Œä¿è¯æ•°æ®çš„è´¨é‡ã€‚3) **è’¸é¦ç®¡é“(Distillation Pipeline)**ï¼šä½¿ç”¨ç›‘ç£å¾®è°ƒ(SFT)å’Œç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–(GRPO)è®­ç»ƒLALMã€‚é¦–å…ˆä½¿ç”¨SFTå¯¹LALMè¿›è¡Œåˆæ­¥è®­ç»ƒï¼Œç„¶åä½¿ç”¨GRPOè¿›ä¸€æ­¥ä¼˜åŒ–å…¶æ¨ç†èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šSightSound-R1çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„è·¨æ¨¡æ€è’¸é¦æ–¹æ³•ï¼Œå°†è§†è§‰æ¨ç†èƒ½åŠ›è¿ç§»åˆ°å¬è§‰æ¨¡å‹ã€‚é€šè¿‡æµ‹è¯•æ—¶ç¼©æ”¾å’ŒéŸ³é¢‘å¼•å¯¼éªŒè¯ï¼Œä¿è¯äº†ç”Ÿæˆä¼ªæ ‡ç­¾æ•°æ®çš„è´¨é‡ï¼Œå…‹æœäº†ç›´æ¥æ”¶é›†å¤§è§„æ¨¡æ€ç»´é“¾éŸ³é¢‘æ•°æ®çš„å›°éš¾ã€‚æ­¤å¤–ï¼ŒGRPOçš„ä½¿ç”¨è¿›ä¸€æ­¥æå‡äº†LALMçš„æ¨ç†æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æµ‹è¯•æ—¶ç¼©æ”¾é˜¶æ®µï¼Œè®ºæ–‡å¯èƒ½ä½¿ç”¨äº†æ³¨æ„åŠ›æœºåˆ¶æˆ–å…¶ä»–æ–¹æ³•æ¥è°ƒæ•´LVLMçš„è¾“å…¥ï¼Œä½¿å…¶æ›´åŠ å…³æ³¨éŸ³é¢‘ä¿¡æ¯ã€‚åœ¨éŸ³é¢‘å¼•å¯¼éªŒè¯é˜¶æ®µï¼Œå¯èƒ½ä½¿ç”¨äº†éŸ³é¢‘ç‰¹å¾æå–å™¨å’Œç›¸ä¼¼åº¦åº¦é‡æ–¹æ³•æ¥åˆ¤æ–­æ¨ç†æ­¥éª¤çš„åˆç†æ€§ã€‚åœ¨è’¸é¦ç®¡é“ä¸­ï¼ŒSFTä½¿ç”¨äº†äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼ŒGRPOå¯èƒ½ä½¿ç”¨äº†å¼ºåŒ–å­¦ä¹ ä¸­çš„ç­–ç•¥æ¢¯åº¦ç®—æ³•ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚å¯èƒ½åœ¨è®ºæ–‡çš„å®éªŒéƒ¨åˆ†æœ‰æ›´è¯¦ç»†çš„æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

SightSound-R1åœ¨AVQAæµ‹è¯•é›†ä¸Šæ˜¾è‘—æå‡äº†LALMçš„æ¨ç†æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨æœªè§è¿‡çš„å¬è§‰åœºæ™¯å’Œé—®é¢˜ä¸­ä¹Ÿè¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚ç›¸è¾ƒäºé¢„è®­ç»ƒå’Œä»…æ ‡ç­¾è’¸é¦çš„åŸºçº¿æ–¹æ³•ï¼ŒSightSound-R1å–å¾—äº†æ˜æ˜¾çš„æ€§èƒ½æå‡ï¼Œè¯æ˜äº†è§†è§‰æ¨ç†å¯ä»¥æœ‰æ•ˆåœ°è½¬ç§»åˆ°éŸ³é¢‘æ¨¡å‹ï¼Œå¹¶å¯ä»¥é€šè¿‡ä¸°å¯Œçš„è§†å¬æ•°æ®è¿›è¡Œæ‰©å±•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ™ºèƒ½å®‰é˜²ã€æ™ºèƒ½å®¶å±…ã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨æ™ºèƒ½å®‰é˜²ä¸­ï¼Œå¯ä»¥åˆ©ç”¨è¯¥æŠ€æœ¯æå‡å¯¹å¼‚å¸¸å£°éŸ³äº‹ä»¶çš„è¯†åˆ«å’Œæ¨ç†èƒ½åŠ›ï¼›åœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼Œå¯ä»¥å¸®åŠ©è½¦è¾†æ›´å¥½åœ°ç†è§£å‘¨å›´ç¯å¢ƒçš„å£°éŸ³ä¿¡æ¯ï¼Œæé«˜é©¾é©¶å®‰å…¨æ€§ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›æ¨åŠ¨å¬è§‰æ™ºèƒ½çš„å‘å±•ï¼Œå®ç°æ›´è‡ªç„¶ã€æ›´æ™ºèƒ½çš„äººæœºäº¤äº’ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> While large audio-language models (LALMs) have demonstrated state-of-the-art audio understanding, their reasoning capability in complex soundscapes still falls behind large vision-language models (LVLMs). Compared to the visual domain, one bottleneck is the lack of large-scale chain-of-thought audio data to teach LALM stepwise reasoning. To circumvent this data and modality gap, we present SightSound-R1, a cross-modal distillation framework that transfers advanced reasoning from a stronger LVLM teacher to a weaker LALM student on the same audio-visual question answering (AVQA) dataset. SightSound-R1 consists of three core steps: (i) test-time scaling to generate audio-focused chains of thought (CoT) from an LVLM teacher, (ii) audio-grounded validation to filter hallucinations, and (iii) a distillation pipeline with supervised fine-tuning (SFT) followed by Group Relative Policy Optimization (GRPO) for the LALM student. Results show that SightSound-R1 improves LALM reasoning performance both in the in-domain AVQA test set as well as in unseen auditory scenes and questions, outperforming both pretrained and label-only distilled baselines. Thus, we conclude that vision reasoning can be effectively transferred to audio models and scaled with abundant audio-visual data.

