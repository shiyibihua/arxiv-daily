---
layout: default
title: Attention Schema-based Attention Control (ASAC): A Cognitive-Inspired Approach for Attention Management in Transformers
---

# Attention Schema-based Attention Control (ASAC): A Cognitive-Inspired Approach for Attention Management in Transformers

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.16058" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.16058v1</a>
  <a href="https://arxiv.org/pdf/2509.16058.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.16058v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.16058v1', 'Attention Schema-based Attention Control (ASAC): A Cognitive-Inspired Approach for Attention Management in Transformers')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Krati Saxena, Federico Jurado Ruiz, Guido Manzi, Dianbo Liu, Alex Lamb

**ÂàÜÁ±ª**: cs.AI, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-19

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Âü∫‰∫éÊ≥®ÊÑèÂäõÊ®°ÂºèÁöÑÊ≥®ÊÑèÂäõÊéßÂà∂ÔºàASACÔºâÊ®°ÂùóÔºåÊèêÂçáTransformerÊ®°ÂûãÁöÑÊ≥®ÊÑèÂäõÂíåÂ≠¶‰π†ÊïàÁéá„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±ÂõõÔºöÁîüÊàêÂºèÂä®‰Ωú (Generative Motion)**

**ÂÖ≥ÈîÆËØç**: `Ê≥®ÊÑèÂäõÊú∫Âà∂` `Transformer` `Ê≥®ÊÑèÂäõÊ®°ÂºèÁêÜËÆ∫` `ÂêëÈáèÈáèÂåñÂèòÂàÜËá™ÁºñÁ†ÅÂô®` `ËÆ§Áü•ËÆ°ÁÆó`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâTransformerÊ®°ÂûãÂú®Ê≥®ÊÑèÂäõÊú∫Âà∂‰∏äÂ≠òÂú®ÊïàÁéáÁì∂È¢àÔºåÁº∫‰πèÂØπÊ≥®ÊÑèÂäõÂàÜÈÖçÁöÑÊúâÊïàÁÆ°ÁêÜ„ÄÇ
2. ASACÊ®°ÂùóÈÄöËøáÊ®°Êãü‰∫∫Á±ªËÆ§Áü•‰∏≠ÁöÑÊ≥®ÊÑèÂäõÊ®°ÂºèÔºå‰ΩøÁî®VQVAEÊòæÂºèÂª∫Ê®°ÂíåÊéßÂà∂Ê≥®ÊÑèÂäõÂàÜÈÖç„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåASACËÉΩÂ§üÊèêÂçáÂàÜÁ±ªÁ≤æÂ∫¶„ÄÅÂä†ÈÄüÂ≠¶‰π†ÔºåÂπ∂Â¢ûÂº∫Ê®°ÂûãÂú®ÂêÑÁßçÁéØÂ¢É‰∏ãÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÂèóÂà∞ËÆ§Áü•ÁßëÂ≠¶‰∏≠Ê≥®ÊÑèÂäõÊ®°ÂºèÁêÜËÆ∫ÔºàASTÔºâÁöÑÂêØÂèëÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÊ≥®ÊÑèÂäõÊ®°ÂºèÁöÑÊ≥®ÊÑèÂäõÊéßÂà∂ÔºàASACÔºâÊñπÊ≥ïÔºåÂπ∂Â∞ÜÂÖ∂ÈõÜÊàêÂà∞TransformerÊû∂ÊûÑ‰∏≠„ÄÇASACÊ®°Âùó‰ΩøÁî®ÂêëÈáèÈáèÂåñÂèòÂàÜËá™ÁºñÁ†ÅÂô®ÔºàVQVAEÔºâ‰Ωú‰∏∫Ê≥®ÊÑèÂäõÊäΩË±°Âô®ÂíåÊéßÂà∂Âô®Ôºå‰ªéËÄåÂÆûÁé∞Á≤æÁ°ÆÁöÑÊ≥®ÊÑèÂäõÁÆ°ÁêÜ„ÄÇÈÄöËøáÊòæÂºèÂú∞Âª∫Ê®°Ê≥®ÊÑèÂäõÂàÜÈÖçÔºåËØ•ÊñπÊ≥ïÊó®Âú®ÊèêÈ´òÁ≥ªÁªüÊïàÁéá„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåASACÂú®ËßÜËßâÂíåËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÈ¢ÜÂüüÂùáËÉΩÊúâÊïàÊèêÈ´òÂàÜÁ±ªÁ≤æÂ∫¶Âπ∂Âä†ÈÄüÂ≠¶‰π†ËøáÁ®ã„ÄÇÊ≠§Â§ñÔºåËØ•Ê®°ÂûãÂú®Âô™Â£∞ÂíåÂàÜÂ∏ÉÂ§ñÊï∞ÊçÆÈõÜ‰∏äË°®Áé∞Âá∫ËâØÂ•ΩÁöÑÈ≤ÅÊ£íÊÄßÂíåÊ≥õÂåñËÉΩÂäõÔºåÂπ∂Âú®Â§ö‰ªªÂä°ËÆæÁΩÆ‰∏≠Ë°®Áé∞Âá∫ÊîπËøõÁöÑÊÄßËÉΩ„ÄÇÂàùÊ≠•ÂÆûÈ™åËøòË°®ÊòéÔºåÂü∫‰∫éÊ≥®ÊÑèÂäõÊ®°ÂºèÁöÑÊ®°ÂùóÂ¢ûÂº∫‰∫ÜÂØπÂØπÊäóÊîªÂáªÁöÑÊäµÊäóÂäõÔºå‰ºòÂåñ‰∫ÜÊ≥®ÊÑèÂäõ‰ª•ÊèêÈ´òÂ≠¶‰π†ÊïàÁéáÔºåÂπ∂‰øÉËøõ‰∫ÜÊúâÊïàÁöÑËøÅÁßªÂ≠¶‰π†ÂíåÂ∞ëÈáèÊ†∑Êú¨Â≠¶‰π†„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâTransformerÊ®°ÂûãËôΩÁÑ∂ÈÄöËøáÊ≥®ÊÑèÂäõÊú∫Âà∂ÂÆûÁé∞‰∫ÜÊÄßËÉΩÊèêÂçáÔºå‰ΩÜÁº∫‰πèÂØπÊ≥®ÊÑèÂäõÂàÜÈÖçÁöÑÊúâÊïàÁÆ°ÁêÜÔºåÂØºËá¥ËÆ°ÁÆóËµÑÊ∫êÊµ™Ë¥πÂíåÂ≠¶‰π†ÊïàÁéá‰Ωé‰∏ã„ÄÇÂ∞§ÂÖ∂ÊòØÂú®Â§çÊùÇ‰ªªÂä°ÂíåÂô™Â£∞ÁéØÂ¢É‰∏ãÔºåÊ®°ÂûãÈöæ‰ª•ÂáÜÁ°ÆËÅöÁÑ¶ÂÖ≥ÈîÆ‰ø°ÊÅØÔºåÂΩ±ÂìçÊúÄÁªàÊÄßËÉΩ„ÄÇÂõ†Ê≠§ÔºåÈúÄË¶Å‰∏ÄÁßçÊñπÊ≥ïÊù•Êõ¥ÊúâÊïàÂú∞ÊéßÂà∂Âíå‰ºòÂåñÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåÊèêÈ´òÊ®°ÂûãÁöÑÂ≠¶‰π†ÊïàÁéáÂíåÈ≤ÅÊ£íÊÄß„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊú¨ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂÄüÈâ¥ËÆ§Áü•ÁßëÂ≠¶‰∏≠ÁöÑÊ≥®ÊÑèÂäõÊ®°ÂºèÁêÜËÆ∫ÔºàASTÔºâÔºåËØ•ÁêÜËÆ∫ËÆ§‰∏∫‰∫∫Á±ªÈÄöËøáÊûÑÂª∫Ëá™Ë∫´Ê≥®ÊÑèÂäõÁöÑÊ®°ÂûãÊù•ÁÆ°ÁêÜËÆ§Áü•ËµÑÊ∫ê„ÄÇÂõ†Ê≠§Ôºå‰ΩúËÄÖÊèêÂá∫Â∞ÜÊ≥®ÊÑèÂäõÊ®°ÂºèÁöÑÊ¶ÇÂøµÂºïÂÖ•Âà∞Á•ûÁªèÁΩëÁªú‰∏≠ÔºåÈÄöËøáÊòæÂºèÂú∞Âª∫Ê®°Ê≥®ÊÑèÂäõÂàÜÈÖçÔºåÂÆûÁé∞ÂØπÊ≥®ÊÑèÂäõÊú∫Âà∂ÁöÑÊúâÊïàÊéßÂà∂Âíå‰ºòÂåñ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöASACÊ®°Âùó‰∏ªË¶ÅÂåÖÂê´‰∏§‰∏™ÈÉ®ÂàÜÔºöÊ≥®ÊÑèÂäõÊäΩË±°Âô®ÂíåÊ≥®ÊÑèÂäõÊéßÂà∂Âô®„ÄÇÊ≥®ÊÑèÂäõÊäΩË±°Âô®Ë¥üË¥£‰ªéÂéüÂßãÊ≥®ÊÑèÂäõÊùÉÈáç‰∏≠ÊèêÂèñÂÖ≥ÈîÆ‰ø°ÊÅØÔºåÂΩ¢ÊàêÂØπÂΩìÂâçÊ≥®ÊÑèÂäõÁä∂ÊÄÅÁöÑÊäΩË±°Ë°®Á§∫„ÄÇÊ≥®ÊÑèÂäõÊéßÂà∂Âô®ÂàôÂü∫‰∫éËØ•ÊäΩË±°Ë°®Á§∫ÔºåÂä®ÊÄÅÂú∞Ë∞ÉÊï¥Ê≥®ÊÑèÂäõÂàÜÈÖçÔºå‰ªéËÄåÂºïÂØºÊ®°ÂûãÂÖ≥Ê≥®Êõ¥ÈáçË¶ÅÁöÑ‰ø°ÊÅØ„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºå‰ΩúËÄÖ‰ΩøÁî®VQVAE‰Ωú‰∏∫Ê≥®ÊÑèÂäõÊäΩË±°Âô®ÂíåÊéßÂà∂Âô®ÔºåVQVAEËÉΩÂ§üÂ∞ÜÈ´òÁª¥ÁöÑÊ≥®ÊÑèÂäõÊùÉÈáçÂéãÁº©ÊàêÁ¶ªÊï£ÁöÑÊΩúÂú®Ë°®Á§∫ÔºåÂπ∂Âà©Áî®Ëøô‰∫õÁ¶ªÊï£Ë°®Á§∫Êù•ÊéßÂà∂Ê≥®ÊÑèÂäõÁöÑÂàÜÈÖç„ÄÇÊï¥‰∏™ASACÊ®°ÂùóÂèØ‰ª•ÂµåÂÖ•Âà∞TransformerÊû∂ÊûÑÁöÑÂêÑ‰∏™Â±Ç‰∏≠Ôºå‰∏éÁé∞ÊúâÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂ÂçèÂêåÂ∑•‰Ωú„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöASACÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞ÁÇπÂú®‰∫éÂ∞ÜËÆ§Áü•ÁßëÂ≠¶‰∏≠ÁöÑÊ≥®ÊÑèÂäõÊ®°ÂºèÁêÜËÆ∫ÂºïÂÖ•Âà∞Á•ûÁªèÁΩëÁªú‰∏≠ÔºåÂπ∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÊ≥®ÊÑèÂäõÊ®°ÂºèÁöÑÊ≥®ÊÑèÂäõÊéßÂà∂ÊñπÊ≥ï„ÄÇ‰∏é‰º†ÁªüÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂Áõ∏ÊØîÔºåASACËÉΩÂ§üÊòæÂºèÂú∞Âª∫Ê®°Ê≥®ÊÑèÂäõÂàÜÈÖçÔºå‰ªéËÄåÂÆûÁé∞ÂØπÊ≥®ÊÑèÂäõÊú∫Âà∂ÁöÑÊõ¥ÊúâÊïàÊéßÂà∂Âíå‰ºòÂåñ„ÄÇÊ≠§Â§ñÔºå‰ΩøÁî®VQVAE‰Ωú‰∏∫Ê≥®ÊÑèÂäõÊäΩË±°Âô®ÂíåÊéßÂà∂Âô®ÔºåËÉΩÂ§üÊúâÊïàÂú∞ÂéãÁº©ÂíåË°®Á§∫Ê≥®ÊÑèÂäõ‰ø°ÊÅØÔºåÂπ∂ÂÆûÁé∞ÂØπÊ≥®ÊÑèÂäõÂàÜÈÖçÁöÑÂä®ÊÄÅË∞ÉÊï¥„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöASACÊ®°ÂùóÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ‰ΩøÁî®VQVAE‰Ωú‰∏∫Ê≥®ÊÑèÂäõÊäΩË±°Âô®ÂíåÊéßÂà∂Âô®ÔºåVQVAEÁöÑÁ†ÅÊú¨Â§ßÂ∞èÂíåÁª¥Â∫¶ÈúÄË¶ÅÊ†πÊçÆÂÖ∑‰Ωì‰ªªÂä°ËøõË°åË∞ÉÊï¥„ÄÇ2) Â¶Ç‰ΩïÂ∞ÜASACÊ®°ÂùóÂµåÂÖ•Âà∞TransformerÊû∂ÊûÑ‰∏≠Ôºå‰ΩúËÄÖÂ∞ÜASACÊ®°ÂùóÂµåÂÖ•Âà∞TransformerÁöÑÊØè‰∏™Ê≥®ÊÑèÂäõÂ±Ç‰πãÂêéÔºå‰∏éÁé∞ÊúâÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂ÂçèÂêåÂ∑•‰Ωú„ÄÇ3) ÊçüÂ§±ÂáΩÊï∞ÁöÑËÆæËÆ°ÔºåÈô§‰∫ÜVQVAEËá™Ë∫´ÁöÑÈáçÊûÑÊçüÂ§±ÂíåÈáèÂåñÊçüÂ§±Â§ñÔºå‰ΩúËÄÖËøòÂºïÂÖ•‰∫Ü‰∏Ä‰∏™ËæÖÂä©ÊçüÂ§±ÂáΩÊï∞ÔºåÁî®‰∫éÈºìÂä±ASACÊ®°ÂùóÂ≠¶‰π†Âà∞Êõ¥ÊúâÊÑè‰πâÁöÑÊ≥®ÊÑèÂäõË°®Á§∫„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåASACÊ®°ÂùóÂú®ËßÜËßâÂíåËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ‰ªªÂä°‰∏≠ÂùáÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ‰æãÂ¶ÇÔºåÂú®ÂõæÂÉèÂàÜÁ±ª‰ªªÂä°‰∏≠ÔºåASACËÉΩÂ§üÊèêÈ´òÂàÜÁ±ªÁ≤æÂ∫¶Âπ∂Âä†ÈÄüÂ≠¶‰π†ËøáÁ®ã„ÄÇÊ≠§Â§ñÔºåASACËøòÂú®Âô™Â£∞ÂíåÂàÜÂ∏ÉÂ§ñÊï∞ÊçÆÈõÜ‰∏äË°®Áé∞Âá∫ËâØÂ•ΩÁöÑÈ≤ÅÊ£íÊÄßÂíåÊ≥õÂåñËÉΩÂäõÔºåÂπ∂Âú®Â§ö‰ªªÂä°ËÆæÁΩÆ‰∏≠Ë°®Áé∞Âá∫ÊîπËøõÁöÑÊÄßËÉΩ„ÄÇÂàùÊ≠•ÂÆûÈ™åËøòË°®ÊòéÔºåASACÂ¢ûÂº∫‰∫ÜÂØπÂØπÊäóÊîªÂáªÁöÑÊäµÊäóÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ASACÊ®°ÂùóÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºåÂèØÂ∫îÁî®‰∫éÂõæÂÉèÂàÜÁ±ª„ÄÅËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ„ÄÅÁõÆÊ†áÊ£ÄÊµãÁ≠âÈ¢ÜÂüü„ÄÇÈÄöËøáÊèêÈ´òÊ®°ÂûãÁöÑÊ≥®ÊÑèÂäõÂíåÂ≠¶‰π†ÊïàÁéáÔºåASACËÉΩÂ§üÊèêÂçáÊ®°ÂûãÂú®ËµÑÊ∫êÂèóÈôêÁéØÂ¢É‰∏ãÁöÑÊÄßËÉΩÔºåÂπ∂Â¢ûÂº∫Ê®°ÂûãÂú®Âô™Â£∞ÂíåÂØπÊäóÊîªÂáª‰∏ãÁöÑÈ≤ÅÊ£íÊÄß„ÄÇÊ≠§Â§ñÔºåASACËøòÂèØ‰ª•Â∫îÁî®‰∫éËøÅÁßªÂ≠¶‰π†ÂíåÂ∞ëÈáèÊ†∑Êú¨Â≠¶‰π†ÔºåÂ∏ÆÂä©Ê®°ÂûãÊõ¥Âø´Âú∞ÈÄÇÂ∫îÊñ∞ÁöÑ‰ªªÂä°ÂíåÁéØÂ¢É„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Attention mechanisms have become integral in AI, significantly enhancing model performance and scalability by drawing inspiration from human cognition. Concurrently, the Attention Schema Theory (AST) in cognitive science posits that individuals manage their attention by creating a model of the attention itself, effectively allocating cognitive resources. Inspired by AST, we introduce ASAC (Attention Schema-based Attention Control), which integrates the attention schema concept into artificial neural networks. Our initial experiments focused on embedding the ASAC module within transformer architectures. This module employs a Vector-Quantized Variational AutoEncoder (VQVAE) as both an attention abstractor and controller, facilitating precise attention management. By explicitly modeling attention allocation, our approach aims to enhance system efficiency. We demonstrate ASAC's effectiveness in both the vision and NLP domains, highlighting its ability to improve classification accuracy and expedite the learning process. Our experiments with vision transformers across various datasets illustrate that the attention controller not only boosts classification accuracy but also accelerates learning. Furthermore, we have demonstrated the model's robustness and generalization capabilities across noisy and out-of-distribution datasets. In addition, we have showcased improved performance in multi-task settings. Quick experiments reveal that the attention schema-based module enhances resilience to adversarial attacks, optimizes attention to improve learning efficiency, and facilitates effective transfer learning and learning from fewer examples. These promising results establish a connection between cognitive science and machine learning, shedding light on the efficient utilization of attention mechanisms in AI systems.

