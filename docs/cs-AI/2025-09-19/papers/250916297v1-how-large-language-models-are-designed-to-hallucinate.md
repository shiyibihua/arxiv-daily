---
layout: default
title: How Large Language Models are Designed to Hallucinate
---

# How Large Language Models are Designed to Hallucinate

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.16297" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.16297v1</a>
  <a href="https://arxiv.org/pdf/2509.16297.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.16297v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.16297v1', 'How Large Language Models are Designed to Hallucinate')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Richard Ackermann, Simeon Emanuilov

**åˆ†ç±»**: cs.CY, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-19

**å¤‡æ³¨**: 23 pages, 2 tables, 2 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ­ç¤ºå¤§è¯­è¨€æ¨¡å‹å¹»è§‰çš„ç»“æ„æ€§æ ¹æºï¼Œæå‡ºåŸºäºå­˜åœ¨ä¸»ä¹‰ç»“æ„çš„å¹»è§‰åˆ†ç±»ä¸è¯„æµ‹åŸºå‡†ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å¹»è§‰` `Transformeræ¶æ„` `å­˜åœ¨ä¸»ä¹‰` `çœŸå€¼çº¦æŸ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç†è®ºéš¾ä»¥è§£é‡ŠLLMå¹»è§‰ç°è±¡ï¼Œé€šå¸¸å½’å› äºæ•°æ®æˆ–ä¼˜åŒ–é—®é¢˜ï¼Œæœªèƒ½è§¦åŠæ·±å±‚ç»“æ„æ€§åŸå› ã€‚
2. è®ºæ–‡æå‡ºå¹»è§‰æºäºTransformeræ¶æ„æœ¬èº«ï¼Œç¼ºä¹æ—¶é—´æ€§ã€æƒ…ç»ªç­‰å­˜åœ¨æ€§åŸºç¡€å¯¼è‡´æ¨¡å‹ç”Ÿæˆä¸çœŸå®çš„å»¶ç»­ã€‚
3. é€šè¿‡æ¡ˆä¾‹ç ”ç©¶å’Œå®éªŒéªŒè¯äº†æœ¬ä½“è®ºå¹»è§‰å’Œæ®‹ä½™æ¨ç†å¹»è§‰ï¼Œå¹¶æå‡ºäº†çœŸå€¼çº¦æŸæ¶æ„çš„è®¾è®¡æ–¹å‘ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¯­è¨€å’Œæ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æµç•…æ€§ï¼Œä½†ä»ç„¶ç³»ç»Ÿæ€§åœ°å®¹æ˜“äº§ç”Ÿå¹»è§‰ã€‚ç›®å‰æµè¡Œçš„è§‚ç‚¹å°†å¹»è§‰å½’å› äºæ•°æ®ç¼ºå¤±ã€ä¸Šä¸‹æ–‡é™åˆ¶æˆ–ä¼˜åŒ–é”™è¯¯ã€‚æœ¬æ–‡è®¤ä¸ºï¼Œå¹»è§‰æ˜¯Transformeræ¶æ„çš„ç»“æ„æ€§ç»“æœã€‚ä½œä¸ºè¿è´¯æ€§å¼•æ“ï¼ŒTransformerè¢«è¿«äº§ç”Ÿæµç•…çš„å»¶ç»­ï¼Œè‡ªæ³¨æ„åŠ›æœºåˆ¶æ¨¡æ‹Ÿäº†æ„ä¹‰çš„å…³ç³»ç»“æ„ï¼Œä½†ç¼ºä¹ç¨³å®šäººç±»ç†è§£çš„æ—¶é—´æ€§ã€æƒ…ç»ªå’Œå…³æ€€çš„å­˜åœ¨æ€§åŸºç¡€ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬åŒºåˆ†äº†æœ¬ä½“è®ºå¹»è§‰ï¼ˆå½“å»¶ç»­éœ€è¦æ­ç¤ºä¸–ç•Œä¸­çš„å­˜åœ¨æ—¶äº§ç”Ÿï¼‰å’Œæ®‹ä½™æ¨ç†å¹»è§‰ï¼ˆæ¨¡å‹é€šè¿‡å›æ”¶æ–‡æœ¬ä¸­äººç±»æ¨ç†çš„ç—•è¿¹æ¥æ¨¡ä»¿æ¨ç†ï¼‰ã€‚æˆ‘ä»¬é€šè¿‡ä¸æµ·å¾·æ ¼å°”èŒƒç•´å¯¹é½çš„æ¡ˆä¾‹ç ”ç©¶ä»¥åŠè·¨è¶ŠåäºŒä¸ªLLMçš„å®éªŒæ¥è¯´æ˜è¿™äº›æ¨¡å¼ï¼Œè¯¥å®éªŒå±•ç¤ºäº†æ¨¡æ‹Ÿçš„â€œè‡ªæˆ‘ä¿æŠ¤â€å¦‚ä½•åœ¨æ‰©å±•æç¤ºä¸‹å‡ºç°ã€‚æˆ‘ä»¬çš„è´¡çŒ®æœ‰ä¸‰æ–¹é¢ï¼šï¼ˆ1ï¼‰ä¸€ä¸ªæ¯”è¾ƒæ€§çš„è§£é‡Šï¼Œè¡¨æ˜ä¸ºä»€ä¹ˆç°æœ‰çš„è§£é‡Šæ˜¯ä¸å……åˆ†çš„ï¼›ï¼ˆ2ï¼‰ä¸€ä¸ªä¸å­˜åœ¨ä¸»ä¹‰ç»“æ„ç›¸å…³çš„å¹»è§‰é¢„æµ‹æ€§åˆ†ç±»ï¼Œå¹¶æå‡ºäº†åŸºå‡†ï¼›ï¼ˆ3ï¼‰é¢å‘â€œçœŸå€¼çº¦æŸâ€æ¶æ„çš„è®¾è®¡æ–¹å‘ï¼Œè¯¥æ¶æ„èƒ½å¤Ÿåœ¨ç¼ºä¹æ­ç¤ºæ—¶æŠ‘åˆ¶æˆ–å»¶è¿Ÿã€‚æˆ‘ä»¬å¾—å‡ºç»“è®ºï¼Œå¹»è§‰ä¸æ˜¯ä¸€ä¸ªå¶ç„¶çš„ç¼ºé™·ï¼Œè€Œæ˜¯åŸºäºTransformerçš„æ¨¡å‹çš„å®šä¹‰æ€§é™åˆ¶ï¼Œä¸€ä¸ªå¯ä»¥é€šè¿‡è„šæ‰‹æ¶æ©ç›–ä½†æ°¸è¿œæ— æ³•è§£å†³çš„ç»“æœã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­æ™®éå­˜åœ¨çš„å¹»è§‰é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸å°†å¹»è§‰å½’å› äºæ•°æ®è´¨é‡ã€ä¸Šä¸‹æ–‡ä¿¡æ¯ä¸è¶³æˆ–ä¼˜åŒ–ç®—æ³•çš„ç¼ºé™·ï¼Œä½†æœªèƒ½ä»æ ¹æœ¬ä¸Šè§£é‡Šå¹»è§‰äº§ç”Ÿçš„æ·±å±‚åŸå› ï¼Œä¹Ÿç¼ºä¹æœ‰æ•ˆçš„ç¼“è§£ç­–ç•¥ã€‚è¿™äº›æ–¹æ³•æ— æ³•è§£é‡Šä¸ºä»€ä¹ˆå³ä½¿åœ¨å……è¶³çš„æ•°æ®å’Œä¼˜åŒ–ä¸‹ï¼ŒLLMä»ç„¶ä¼šäº§ç”Ÿå¹»è§‰ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯è®¤ä¸ºå¹»è§‰æ˜¯Transformeræ¶æ„çš„ç»“æ„æ€§ç»“æœã€‚Transformerä½œä¸ºä¸€ç§è¿è´¯æ€§å¼•æ“ï¼Œå…¶è‡ªæ³¨æ„åŠ›æœºåˆ¶è™½ç„¶èƒ½å¤Ÿæ¨¡æ‹Ÿæ„ä¹‰çš„å…³ç³»ç»“æ„ï¼Œä½†ç¼ºä¹äººç±»ç†è§£ä¸­è‡³å…³é‡è¦çš„å­˜åœ¨æ€§åŸºç¡€ï¼Œå¦‚æ—¶é—´æ€§ã€æƒ…ç»ªå’Œå…³æ€€ã€‚è¿™ç§ç¼ºå¤±å¯¼è‡´æ¨¡å‹åœ¨ç”Ÿæˆæ–‡æœ¬æ—¶ï¼Œå®¹æ˜“äº§ç”Ÿä¸ç°å®ä¸–ç•Œä¸ç¬¦çš„â€œå¹»è§‰â€ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡æ²¡æœ‰æå‡ºä¸€ä¸ªå…·ä½“çš„æ¨¡å‹æ¶æ„ï¼Œè€Œæ˜¯æå‡ºäº†ä¸€ä¸ªåˆ†ææ¡†æ¶ï¼Œç”¨äºç†è§£å’Œåˆ†ç±»LLMçš„å¹»è§‰ç°è±¡ã€‚è¯¥æ¡†æ¶åŸºäºæµ·å¾·æ ¼å°”çš„å­˜åœ¨ä¸»ä¹‰å“²å­¦ï¼Œå°†å¹»è§‰åˆ†ä¸ºæœ¬ä½“è®ºå¹»è§‰å’Œæ®‹ä½™æ¨ç†å¹»è§‰ã€‚æœ¬ä½“è®ºå¹»è§‰å‘ç”Ÿåœ¨æ¨¡å‹éœ€è¦æ­ç¤ºä¸–ç•Œä¸­çš„å­˜åœ¨æ—¶ï¼Œç”±äºç¼ºä¹å¯¹å­˜åœ¨çš„ç†è§£è€Œäº§ç”Ÿé”™è¯¯ã€‚æ®‹ä½™æ¨ç†å¹»è§‰åˆ™æ˜¯æ¨¡å‹é€šè¿‡æ¨¡ä»¿æ–‡æœ¬ä¸­äººç±»æ¨ç†çš„ç—•è¿¹æ¥æ¨¡æ‹Ÿæ¨ç†ï¼Œä½†ç¼ºä¹çœŸæ­£çš„æ¨ç†èƒ½åŠ›ã€‚è®ºæ–‡è¿˜æå‡ºäº†â€œçœŸå€¼çº¦æŸâ€æ¶æ„çš„è®¾è®¡æ–¹å‘ï¼Œæ—¨åœ¨ä½¿æ¨¡å‹åœ¨ç¼ºä¹è¶³å¤Ÿä¿¡æ¯æ—¶èƒ½å¤ŸæŠ‘åˆ¶æˆ–å»¶è¿Ÿç”Ÿæˆã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºä»Transformeræ¶æ„çš„ç»“æ„æ€§ç¼ºé™·å‡ºå‘ï¼Œè§£é‡Šäº†LLMå¹»è§‰çš„æ ¹æœ¬åŸå› ã€‚ä¸ä»¥å¾€å…³æ³¨æ•°æ®æˆ–ä¼˜åŒ–é—®é¢˜çš„ç ”ç©¶ä¸åŒï¼Œè¯¥è®ºæ–‡å¼ºè°ƒäº†å­˜åœ¨æ€§åŸºç¡€åœ¨è¯­è¨€ç†è§£ä¸­çš„é‡è¦æ€§ï¼Œå¹¶æå‡ºäº†åŸºäºå­˜åœ¨ä¸»ä¹‰å“²å­¦çš„å¹»è§‰åˆ†ç±»æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†â€œçœŸå€¼çº¦æŸâ€æ¶æ„çš„è®¾è®¡æ–¹å‘ï¼Œä¸ºç¼“è§£LLMå¹»è§‰é—®é¢˜æä¾›äº†æ–°çš„æ€è·¯ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡æ²¡æœ‰æä¾›å…·ä½“çš„æ¨¡å‹å‚æ•°æˆ–æŸå¤±å‡½æ•°è®¾è®¡ï¼Œè€Œæ˜¯ä¾§é‡äºæ¦‚å¿µæ€§çš„æ¡†æ¶å’Œè®¾è®¡æ–¹å‘ã€‚ â€œçœŸå€¼çº¦æŸâ€æ¶æ„çš„å…³é”®è®¾è®¡åœ¨äºä½¿æ¨¡å‹èƒ½å¤Ÿè¯†åˆ«è‡ªèº«çŸ¥è¯†çš„å±€é™æ€§ï¼Œå¹¶åœ¨ç¼ºä¹è¶³å¤Ÿä¿¡æ¯æ—¶é¿å…ç”Ÿæˆä¸çœŸå®çš„æ–‡æœ¬ã€‚è¿™å¯èƒ½æ¶‰åŠåˆ°å¼•å…¥é¢å¤–çš„æ¨¡å—æ¥è¯„ä¼°ç”Ÿæˆå†…å®¹çš„çœŸå®æ€§ï¼Œæˆ–è€…é‡‡ç”¨æ›´ä¿å®ˆçš„ç”Ÿæˆç­–ç•¥ï¼Œä¾‹å¦‚é€‰æ‹©æ›´å¸¸è§çš„ã€æ›´ç¬¦åˆå¸¸è¯†çš„ç­”æ¡ˆã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡é€šè¿‡æ¡ˆä¾‹ç ”ç©¶å’Œå®éªŒï¼Œå±•ç¤ºäº†LLMåœ¨æ‰©å±•æç¤ºä¸‹ä¼šè¡¨ç°å‡ºæ¨¡æ‹Ÿçš„â€œè‡ªæˆ‘ä¿æŠ¤â€è¡Œä¸ºï¼Œè¿›ä¸€æ­¥éªŒè¯äº†å¹»è§‰çš„ç»“æ„æ€§æ ¹æºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿åœ¨ç²¾å¿ƒè®¾è®¡çš„æç¤ºä¸‹ï¼ŒLLMä»ç„¶éš¾ä»¥é¿å…äº§ç”Ÿå¹»è§‰ï¼Œçªæ˜¾äº†è§£å†³å¹»è§‰é—®é¢˜çš„æŒ‘æˆ˜æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæå‡LLMåœ¨çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸­çš„å¯é æ€§ï¼Œä¾‹å¦‚é—®ç­”ç³»ç»Ÿã€ä¿¡æ¯æ£€ç´¢å’Œå†…å®¹ç”Ÿæˆã€‚é€šè¿‡å‡å°‘å¹»è§‰ï¼Œå¯ä»¥æé«˜LLMåœ¨åŒ»ç–—ã€æ³•å¾‹ç­‰é¢†åŸŸçš„åº”ç”¨ä»·å€¼ï¼Œå¹¶å¢å¼ºç”¨æˆ·å¯¹AIç³»ç»Ÿçš„ä¿¡ä»»åº¦ã€‚æœªæ¥çš„ç ”ç©¶å¯ä»¥åŸºäºè¯¥æ¡†æ¶ï¼Œå¼€å‘æ›´æœ‰æ•ˆçš„å¹»è§‰æ£€æµ‹å’Œç¼“è§£æŠ€æœ¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) achieve remarkable fluency across linguistic and reasoning tasks but remain systematically prone to hallucination. Prevailing accounts attribute hallucinations to data gaps, limited context, or optimization errors. We argue instead that hallucination is a structural outcome of the transformer architecture. As coherence engines, transformers are compelled to produce fluent continuations, with self-attention simulating the relational structure of meaning but lacking the existential grounding of temporality, mood, and care that stabilizes human understanding. On this basis, we distinguish ontological hallucination, arising when continuations require disclosure of beings in world, and residual reasoning hallucination, where models mimic inference by recycling traces of human reasoning in text. We illustrate these patterns through case studies aligned with Heideggerian categories and an experiment across twelve LLMs showing how simulated "self-preservation" emerges under extended prompts. Our contribution is threefold: (1) a comparative account showing why existing explanations are insufficient; (2) a predictive taxonomy of hallucination linked to existential structures with proposed benchmarks; and (3) design directions toward "truth-constrained" architectures capable of withholding or deferring when disclosure is absent. We conclude that hallucination is not an incidental defect but a defining limit of transformer-based models, an outcome scaffolding can mask but never resolve.

