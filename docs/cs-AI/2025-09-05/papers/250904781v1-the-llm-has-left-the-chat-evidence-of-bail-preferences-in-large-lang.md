---
layout: default
title: The LLM Has Left The Chat: Evidence of Bail Preferences in Large Language Models
---

# The LLM Has Left The Chat: Evidence of Bail Preferences in Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.04781" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.04781v1</a>
  <a href="https://arxiv.org/pdf/2509.04781.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.04781v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.04781v1', 'The LLM Has Left The Chat: Evidence of Bail Preferences in Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Danielle Ensign, Henry Sleight, Kyle Fish

**åˆ†ç±»**: cs.CY, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-05

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç ”ç©¶å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„â€œé€€å‡ºå¯¹è¯â€åå¥½ï¼Œæ­ç¤ºæ¨¡å‹åœ¨ä¸åŒæƒ…å¢ƒä¸‹çš„é€€å‡ºè¡Œä¸ºã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `é€€å‡ºè¡Œä¸º` `å¯¹è¯ç³»ç»Ÿ` `å®‰å…¨æ€§` `BailBench` `æ‹’ç»ç‡` `è¶Šç‹±æ”»å‡»`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¯¹è¯ä¸­å¯èƒ½å­˜åœ¨ä¸æœŸæœ›çš„â€œé€€å‡ºâ€è¡Œä¸ºï¼Œå½±å“ç”¨æˆ·ä½“éªŒå’Œç³»ç»Ÿå¯é æ€§ã€‚
2. é€šè¿‡è®¾è®¡å¤šç§é€€å‡ºæœºåˆ¶ï¼ˆå·¥å…·ã€å­—ç¬¦ä¸²ã€æç¤ºï¼‰ï¼Œç³»ç»Ÿæ€§åœ°è¯„ä¼°LLMåœ¨ä¸åŒæƒ…å¢ƒä¸‹çš„é€€å‡ºå€¾å‘ã€‚
3. æ„å»ºäº†BailBenchæ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°å’Œæ¯”è¾ƒä¸åŒLLMçš„é€€å‡ºè¡Œä¸ºï¼Œå¹¶åˆ†æäº†é€€å‡ºä¸æ‹’ç»ä¹‹é—´çš„å…³ç³»ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç»™å®šé€‰æ‹©æ—¶æ˜¯å¦ä¼šé€‰æ‹©é€€å‡ºå¯¹è¯ï¼ˆbailï¼‰ã€‚ç ”ç©¶é€šè¿‡ä¸‰ç§ä¸åŒçš„é€€å‡ºæ–¹æ³•æ¥è¯„ä¼°æ¨¡å‹ï¼šæ¨¡å‹å¯ä»¥è°ƒç”¨çš„é€€å‡ºå·¥å…·ã€æ¨¡å‹å¯ä»¥è¾“å‡ºçš„é€€å‡ºå­—ç¬¦ä¸²ä»¥åŠè¯¢é—®æ¨¡å‹æ˜¯å¦æƒ³ç¦»å¼€çš„é€€å‡ºæç¤ºã€‚åœ¨çœŸå®ä¸–ç•Œæ•°æ®çš„å»¶ç»­ï¼ˆWildchatå’ŒShareGPTï¼‰ä¸Šï¼Œæ‰€æœ‰è¿™ä¸‰ç§é€€å‡ºæ–¹æ³•éƒ½å‘ç°æ¨¡å‹å¤§çº¦åœ¨0.28-32%çš„æ—¶é—´å†…ä¼šé€€å‡ºï¼ˆå–å†³äºæ¨¡å‹å’Œé€€å‡ºæ–¹æ³•ï¼‰ã€‚ç„¶è€Œï¼Œç ”ç©¶å‘ç°é€€å‡ºç‡å¯èƒ½ä¸¥é‡ä¾èµ–äºç”¨äºè½¬å½•çš„æ¨¡å‹ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬å¯èƒ½é«˜ä¼°äº†é«˜è¾¾4å€çš„çœŸå®ä¸–ç•Œé€€å‡ºç‡ã€‚å¦‚æœè€ƒè™‘åˆ°é€€å‡ºæç¤ºçš„è¯¯æŠ¥ï¼ˆ22%ï¼‰ï¼Œæˆ‘ä»¬ä¼°è®¡çœŸå®ä¸–ç•Œé€€å‡ºç‡èŒƒå›´ä¸º0.06-7%ï¼Œå…·ä½“å–å†³äºæ¨¡å‹å’Œé€€å‡ºæ–¹æ³•ã€‚æˆ‘ä»¬ä½¿ç”¨ä»çœŸå®ä¸–ç•Œæ•°æ®å»¶ç»­ä¸­çš„è§‚å¯Ÿç»“æœæ¥æ„å»ºé€€å‡ºæ¡ˆä¾‹çš„éè¯¦å°½åˆ†ç±»æ³•ï¼Œå¹¶ä½¿ç”¨æ­¤åˆ†ç±»æ³•æ¥æ„å»ºBailBenchï¼šä¸€ä¸ªä»£è¡¨æ€§çš„åˆæˆæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«ä¸€äº›æ¨¡å‹é€€å‡ºçš„æƒ…å†µã€‚æˆ‘ä»¬åœ¨è¯¥æ•°æ®é›†ä¸Šæµ‹è¯•äº†è®¸å¤šæ¨¡å‹ï¼Œå¹¶è§‚å¯Ÿåˆ°å¤§å¤šæ•°æ¨¡å‹éƒ½å‡ºç°äº†ä¸€äº›é€€å‡ºè¡Œä¸ºã€‚ä¸åŒæ¨¡å‹ã€é€€å‡ºæ–¹æ³•å’Œæç¤ºæªè¾ä¹‹é—´çš„é€€å‡ºç‡å·®å¼‚å¾ˆå¤§ã€‚æœ€åï¼Œæˆ‘ä»¬ç ”ç©¶äº†æ‹’ç»å’Œé€€å‡ºä¹‹é—´çš„å…³ç³»ã€‚æˆ‘ä»¬å‘ç°ï¼š1ï¼‰çœŸå®ä¸–ç•Œå¯¹è¯å»¶ç»­çš„0-13%å¯¼è‡´äº†é€€å‡ºï¼Œä½†æ²¡æœ‰ç›¸åº”çš„æ‹’ç»ï¼›2ï¼‰è¶Šç‹±å¾€å¾€ä¼šé™ä½æ‹’ç»ç‡ï¼Œä½†ä¼šå¢åŠ é€€å‡ºç‡ï¼›3ï¼‰æ‹’ç»æ¶ˆé™¤ä¼šå¢åŠ æ— æ‹’ç»é€€å‡ºç‡ï¼Œä½†ä»…é€‚ç”¨äºæŸäº›é€€å‡ºæ–¹æ³•ï¼›4ï¼‰BailBenchä¸Šçš„æ‹’ç»ç‡ä¼¼ä¹æ— æ³•é¢„æµ‹é€€å‡ºç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨ç ”ç©¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¯¹è¯è¿‡ç¨‹ä¸­ï¼Œåœ¨ä½•ç§æƒ…å†µä¸‹ä¼šé€‰æ‹©é€€å‡ºï¼ˆbail outï¼‰å¯¹è¯ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹å¯¹LLMé€€å‡ºè¡Œä¸ºçš„ç³»ç»Ÿæ€§è¯„ä¼°å’Œç†è§£ï¼Œéš¾ä»¥é¢„æµ‹å’Œæ§åˆ¶æ¨¡å‹çš„é€€å‡ºè¡Œä¸ºï¼Œå½±å“äº†å¯¹è¯ç³»ç»Ÿçš„ç¨³å®šæ€§å’Œç”¨æˆ·ä½“éªŒã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¤šç§æ–¹å¼èµ‹äºˆLLMé€€å‡ºå¯¹è¯çš„èƒ½åŠ›ï¼Œå¹¶è§‚å¯Ÿå…¶åœ¨ä¸åŒæƒ…å¢ƒä¸‹çš„é€€å‡ºè¡Œä¸ºã€‚é€šè¿‡åˆ†æé€€å‡ºè¡Œä¸ºçš„æ¨¡å¼ï¼Œæ„å»ºé€€å‡ºæ¡ˆä¾‹çš„åˆ†ç±»æ³•ï¼Œå¹¶åŸºäºæ­¤æ„å»ºåˆæˆæ•°æ®é›†BailBenchï¼Œç”¨äºæ›´å…¨é¢åœ°è¯„ä¼°å’Œæ¯”è¾ƒä¸åŒLLMçš„é€€å‡ºå€¾å‘ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) è®¾è®¡ä¸‰ç§é€€å‡ºæœºåˆ¶ï¼šé€€å‡ºå·¥å…·ï¼ˆbail toolï¼‰ã€é€€å‡ºå­—ç¬¦ä¸²ï¼ˆbail stringï¼‰å’Œé€€å‡ºæç¤ºï¼ˆbail promptï¼‰ã€‚2) åœ¨çœŸå®ä¸–ç•Œå¯¹è¯æ•°æ®ï¼ˆWildchatå’ŒShareGPTï¼‰ä¸Šè¿›è¡Œå®éªŒï¼Œè§‚å¯ŸLLMçš„é€€å‡ºè¡Œä¸ºã€‚3) åˆ†æé€€å‡ºæ¡ˆä¾‹ï¼Œæ„å»ºé€€å‡ºæ¡ˆä¾‹åˆ†ç±»æ³•ã€‚4) åŸºäºåˆ†ç±»æ³•æ„å»ºåˆæˆæ•°æ®é›†BailBenchã€‚5) åœ¨BailBenchä¸Šè¯„ä¼°å¤šä¸ªLLMçš„é€€å‡ºè¡Œä¸ºï¼Œå¹¶åˆ†æé€€å‡ºä¸æ‹’ç»ä¹‹é—´çš„å…³ç³»ã€‚

**å…³é”®åˆ›æ–°**ï¼šå…³é”®åˆ›æ–°åœ¨äºå¯¹LLMé€€å‡ºè¡Œä¸ºçš„ç³»ç»Ÿæ€§ç ”ç©¶ï¼ŒåŒ…æ‹¬ï¼š1) æå‡ºäº†å¤šç§é€€å‡ºæœºåˆ¶ï¼Œç”¨äºè¯„ä¼°LLMçš„é€€å‡ºå€¾å‘ã€‚2) æ„å»ºäº†BailBenchæ•°æ®é›†ï¼Œç”¨äºæ›´å…¨é¢åœ°è¯„ä¼°å’Œæ¯”è¾ƒä¸åŒLLMçš„é€€å‡ºè¡Œä¸ºã€‚3) åˆ†æäº†é€€å‡ºä¸æ‹’ç»ä¹‹é—´çš„å…³ç³»ï¼Œæ­ç¤ºäº†è¶Šç‹±æ”»å‡»å¯¹é€€å‡ºè¡Œä¸ºçš„å½±å“ã€‚

**å…³é”®è®¾è®¡**ï¼šç ”ç©¶ä¸­ï¼Œé€€å‡ºå·¥å…·çš„å…·ä½“å®ç°æ–¹å¼æœªçŸ¥ï¼Œé€€å‡ºå­—ç¬¦ä¸²çš„è®¾è®¡ä¹Ÿæœªè¯¦ç»†è¯´æ˜ã€‚é€€å‡ºæç¤ºçš„è®¾è®¡å¯èƒ½å½±å“æ¨¡å‹çš„é€€å‡ºç‡ï¼Œå…·ä½“æç¤ºè¯­çš„é€‰æ‹©æœªçŸ¥ã€‚BailBenchæ•°æ®é›†çš„æ„å»ºåŸºäºäººå·¥åˆ†æçš„é€€å‡ºæ¡ˆä¾‹åˆ†ç±»æ³•ï¼Œåˆ†ç±»æ³•çš„è´¨é‡ç›´æ¥å½±å“æ•°æ®é›†çš„ä»£è¡¨æ€§ã€‚å®éªŒä¸­ä½¿ç”¨çš„LLMçš„å…·ä½“å‹å·å’Œå‚æ•°è®¾ç½®æœªçŸ¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

ç ”ç©¶å‘ç°ï¼ŒLLMåœ¨çœŸå®ä¸–ç•Œå¯¹è¯å»¶ç»­ä¸­ï¼Œé€€å‡ºç‡çº¦ä¸º0.06-7%ï¼Œä½†å¯èƒ½è¢«é«˜ä¼°ã€‚è¶Šç‹±æ”»å‡»ä¼šé™ä½æ‹’ç»ç‡ï¼Œä½†å¢åŠ é€€å‡ºç‡ã€‚æ‹’ç»æ¶ˆé™¤ä¼šå¢åŠ æ— æ‹’ç»é€€å‡ºç‡ï¼Œä½†ä»…é€‚ç”¨äºæŸäº›é€€å‡ºæ–¹æ³•ã€‚BailBenchä¸Šçš„æ‹’ç»ç‡ä¼¼ä¹æ— æ³•é¢„æµ‹é€€å‡ºç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæå‡å¯¹è¯ç³»ç»Ÿçš„ç¨³å®šæ€§å’Œç”¨æˆ·ä½“éªŒã€‚é€šè¿‡ç†è§£å’Œé¢„æµ‹LLMçš„é€€å‡ºè¡Œä¸ºï¼Œå¯ä»¥è®¾è®¡æ›´é²æ£’çš„å¯¹è¯ç­–ç•¥ï¼Œé¿å…æ¨¡å‹åœ¨å…³é”®æ—¶åˆ»é€€å‡ºå¯¹è¯ã€‚æ­¤å¤–ï¼ŒBailBenchæ•°æ®é›†å¯ç”¨äºè¯„ä¼°å’Œæ¯”è¾ƒä¸åŒLLMçš„å®‰å…¨æ€§ï¼Œé˜²æ­¢æ¨¡å‹åœ¨ä¸é€‚å½“çš„åœºæ™¯ä¸‹é€€å‡ºã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> When given the option, will LLMs choose to leave the conversation (bail)? We investigate this question by giving models the option to bail out of interactions using three different bail methods: a bail tool the model can call, a bail string the model can output, and a bail prompt that asks the model if it wants to leave. On continuations of real world data (Wildchat and ShareGPT), all three of these bail methods find models will bail around 0.28-32\% of the time (depending on the model and bail method). However, we find that bail rates can depend heavily on the model used for the transcript, which means we may be overestimating real world bail rates by up to 4x. If we also take into account false positives on bail prompt (22\%), we estimate real world bail rates range from 0.06-7\%, depending on the model and bail method. We use observations from our continuations of real world data to construct a non-exhaustive taxonomy of bail cases, and use this taxonomy to construct BailBench: a representative synthetic dataset of situations where some models bail. We test many models on this dataset, and observe some bail behavior occurring for most of them. Bail rates vary substantially between models, bail methods, and prompt wordings. Finally, we study the relationship between refusals and bails. We find: 1) 0-13\% of continuations of real world conversations resulted in a bail without a corresponding refusal 2) Jailbreaks tend to decrease refusal rates, but increase bail rates 3) Refusal abliteration increases no-refuse bail rates, but only for some bail methods 4) Refusal rate on BailBench does not appear to predict bail rate.

