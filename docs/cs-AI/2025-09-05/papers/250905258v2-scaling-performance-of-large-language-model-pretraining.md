---
layout: default
title: Scaling Performance of Large Language Model Pretraining
---

# Scaling Performance of Large Language Model Pretraining

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.05258" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.05258v2</a>
  <a href="https://arxiv.org/pdf/2509.05258.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.05258v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.05258v2', 'Scaling Performance of Large Language Model Pretraining')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Alexander Interrante-Grant, Carla Varela-Rosa, Suhaas Narayan, Chris Connelly, Albert Reuther

**åˆ†ç±»**: cs.DC, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-05 (æ›´æ–°: 2025-10-09)

**æœŸåˆŠ**: Proc. IEEE High Performance Extreme Computing Conference (HPEC), 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ­ç§˜LLMé¢„è®­ç»ƒï¼šæ¢ç´¢åˆ†å¸ƒå¼è®­ç»ƒã€å¤§æ•°æ®ç®¡ç†åŠæ•°æ®å¹¶è¡Œæ‰©å±•ç­–ç•¥**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `é¢„è®­ç»ƒ` `åˆ†å¸ƒå¼è®­ç»ƒ` `æ•°æ®å¹¶è¡Œ` `GPUä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMè®­ç»ƒè®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œä½†æ‰©å±•æ€§èƒ½å’Œè®­ç»ƒç»†èŠ‚ç¼ºä¹å…¬å¼€ä¿¡æ¯ï¼Œé˜»ç¢äº†ç ”ç©¶å’Œåº”ç”¨ã€‚
2. æœ¬æ–‡æ—¨åœ¨æ­ç¤ºLLMé¢„è®­ç»ƒæµæ°´çº¿çš„å…³é”®è¦ç´ ï¼Œèšç„¦åˆ†å¸ƒå¼è®­ç»ƒã€å¤§æ•°æ®ç®¡ç†å’Œæ•°æ®å¹¶è¡Œæ‰©å±•ã€‚
3. ç ”ç©¶é‡ç‚¹åœ¨äºå……åˆ†åˆ©ç”¨GPUè®¡ç®—èƒ½åŠ›ï¼Œä¸ºå¤§è§„æ¨¡LLMè®­ç»ƒæä¾›å®ç”¨æŒ‡å¯¼å’Œæ€§èƒ½ä¼˜åŒ–å»ºè®®ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§è‡ªç„¶è¯­è¨€å¤„ç†åº”ç”¨ä¸­è¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ã€‚è®­ç»ƒè¿™äº›æ¨¡å‹æ˜¯ä¸€é¡¹è®¡ç®—æˆæœ¬æé«˜çš„ä»»åŠ¡ï¼›é¢†å…ˆçš„äººå·¥æ™ºèƒ½ç ”ç©¶å…¬å¸æ­£åœ¨æŠ•èµ„æ•°åäº¿ç¾å…ƒç”¨äºè¶…çº§è®¡ç®—åŸºç¡€è®¾æ–½ï¼Œä»¥ä¾¿åœ¨æ—¥ç›Šåºå¤§çš„æ•°æ®é›†ä¸Šè®­ç»ƒè¶Šæ¥è¶Šå¤§çš„æ¨¡å‹ã€‚ç„¶è€Œï¼Œå…³äºè¿™äº›å¤§å‹è®­ç»ƒæµæ°´çº¿çš„æ‰©å±•æ€§èƒ½å’Œè®­ç»ƒè€ƒè™‘å› ç´ çš„å…¬å¼€ä¿¡æ¯éå¸¸å°‘ã€‚å¤„ç†è¶…å¤§æ•°æ®é›†å’Œæ¨¡å‹å¯èƒ½éå¸¸å¤æ‚ï¼Œå¹¶ä¸”åœ¨å…¬å¼€æ–‡çŒ®ä¸­ï¼Œå…³äºè°ƒæ•´è®­ç»ƒæ€§èƒ½ä»¥æ‰©å±•å¤§å‹è¯­è¨€æ¨¡å‹çš„å®ç”¨å»ºè®®å¾ˆå°‘ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ—¨åœ¨æ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒæµæ°´çº¿çš„ä¸€äº›ç¥ç§˜ä¹‹å¤„â€”â€”ç‰¹åˆ«æ˜¯åœ¨åˆ†å¸ƒå¼è®­ç»ƒã€è·¨æ•°ç™¾ä¸ªèŠ‚ç‚¹ç®¡ç†å¤§å‹æ•°æ®é›†ä»¥åŠæ‰©å±•æ•°æ®å¹¶è¡Œæ€§æ–¹é¢ï¼Œé‡ç‚¹æ˜¯å……åˆ†åˆ©ç”¨å¯ç”¨çš„GPUè®¡ç®—èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œç”±äºæ¨¡å‹å’Œæ•°æ®é›†è§„æ¨¡çš„ä¸æ–­å¢é•¿ï¼Œå¯¼è‡´çš„è®­ç»ƒæ•ˆç‡ä½ä¸‹å’Œèµ„æºåˆ©ç”¨ç‡ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨åˆ†å¸ƒå¼è®­ç»ƒã€å¤§æ•°æ®ç®¡ç†å’Œæ•°æ®å¹¶è¡Œæ‰©å±•æ–¹é¢å­˜åœ¨è¯¸å¤šæŒ‘æˆ˜ï¼Œç¼ºä¹å…¬å¼€çš„ã€å®ç”¨çš„ä¼˜åŒ–ç­–ç•¥ï¼Œä½¿å¾—ç ”ç©¶äººå‘˜éš¾ä»¥å……åˆ†åˆ©ç”¨ç°æœ‰çš„è®¡ç®—èµ„æºã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ·±å…¥åˆ†æLLMé¢„è®­ç»ƒæµæ°´çº¿çš„å„ä¸ªç¯èŠ‚ï¼Œæ‰¾å‡ºå½±å“æ€§èƒ½çš„å…³é”®å› ç´ ï¼Œå¹¶é’ˆå¯¹æ€§åœ°æå‡ºä¼˜åŒ–ç­–ç•¥ã€‚é‡ç‚¹å…³æ³¨åˆ†å¸ƒå¼è®­ç»ƒä¸­çš„æ•°æ®å¹¶è¡Œæ€§æ‰©å±•ï¼Œä»¥åŠå¦‚ä½•æœ‰æ•ˆåœ°ç®¡ç†å’Œåˆ©ç”¨å¤§è§„æ¨¡æ•°æ®é›†ï¼Œä»è€Œæœ€å¤§é™åº¦åœ°æé«˜GPUçš„è®¡ç®—åˆ©ç”¨ç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡æ²¡æœ‰æ˜ç¡®æå‡ºä¸€ä¸ªå…¨æ–°çš„æŠ€æœ¯æ¡†æ¶ï¼Œè€Œæ˜¯ä¾§é‡äºå¯¹ç°æœ‰LLMé¢„è®­ç»ƒæµç¨‹çš„ä¼˜åŒ–ã€‚å…¶ç ”ç©¶æ–¹æ³•å¯ä»¥ç†è§£ä¸ºï¼š1) åˆ†æç°æœ‰é¢„è®­ç»ƒæµç¨‹çš„ç“¶é¢ˆï¼›2) é’ˆå¯¹ç“¶é¢ˆæå‡ºä¼˜åŒ–ç­–ç•¥ï¼Œä¾‹å¦‚æ”¹è¿›æ•°æ®åŠ è½½å’Œåˆ†å‘æœºåˆ¶ï¼Œä¼˜åŒ–é€šä¿¡ç­–ç•¥ç­‰ï¼›3) é€šè¿‡å®éªŒéªŒè¯ä¼˜åŒ–ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå…¶å¯¹LLMé¢„è®­ç»ƒæµç¨‹çš„æ·±å…¥å‰–æå’Œå¯¹ä¼˜åŒ–ç­–ç•¥çš„å®è·µæ¢ç´¢ã€‚è™½ç„¶æ²¡æœ‰æå‡ºå…¨æ–°çš„ç®—æ³•æˆ–æ¨¡å‹ç»“æ„ï¼Œä½†å…¶å¯¹åˆ†å¸ƒå¼è®­ç»ƒã€å¤§æ•°æ®ç®¡ç†å’Œæ•°æ®å¹¶è¡Œæ‰©å±•çš„ç»éªŒæ€»ç»“å’Œå»ºè®®ï¼Œå¯¹äºå®é™…åº”ç”¨å…·æœ‰é‡è¦çš„æŒ‡å¯¼æ„ä¹‰ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡æ²¡æœ‰è¯¦ç»†æè¿°å…·ä½“çš„å‚æ•°è®¾ç½®ã€æŸå¤±å‡½æ•°æˆ–ç½‘ç»œç»“æ„ï¼Œè€Œæ˜¯ä¾§é‡äºç³»ç»Ÿå±‚é¢çš„ä¼˜åŒ–ã€‚å…³é”®è®¾è®¡ä½“ç°åœ¨å¦‚ä½•æœ‰æ•ˆåœ°å°†å¤§è§„æ¨¡æ•°æ®é›†åˆ†å‘åˆ°å„ä¸ªè®¡ç®—èŠ‚ç‚¹ï¼Œå¦‚ä½•ä¼˜åŒ–æ•°æ®å¹¶è¡Œè®­ç»ƒä¸­çš„é€šä¿¡å¼€é”€ï¼Œä»¥åŠå¦‚ä½•æœ€å¤§é™åº¦åœ°åˆ©ç”¨GPUçš„è®¡ç®—èƒ½åŠ›ã€‚è¿™äº›è®¾è®¡ç»†èŠ‚éœ€è¦åœ¨å®é™…çš„å®éªŒä¸­è¿›è¡Œè°ƒæ•´å’Œä¼˜åŒ–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

ç”±äºè®ºæ–‡æ‘˜è¦ä¸­æ²¡æœ‰æä¾›å…·ä½“çš„å®éªŒç»“æœå’Œæ€§èƒ½æ•°æ®ï¼Œå› æ­¤æ— æ³•æ€»ç»“å®éªŒäº®ç‚¹ã€‚ä½†å¯ä»¥æ¨æµ‹ï¼Œè¯¥è®ºæ–‡å¯èƒ½é€šè¿‡å®éªŒéªŒè¯äº†å…¶æå‡ºçš„ä¼˜åŒ–ç­–ç•¥åœ¨æé«˜è®­ç»ƒæ•ˆç‡å’ŒGPUåˆ©ç”¨ç‡æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å¯èƒ½ä¸ç°æœ‰çš„è®­ç»ƒæ–¹æ³•è¿›è¡Œäº†å¯¹æ¯”ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒçš„åœºæ™¯ï¼Œä¾‹å¦‚è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨ç¿»è¯‘ã€æ–‡æœ¬ç”Ÿæˆã€å¯¹è¯ç³»ç»Ÿç­‰ã€‚é€šè¿‡ä¼˜åŒ–è®­ç»ƒæµç¨‹ï¼Œå¯ä»¥é™ä½è®­ç»ƒæˆæœ¬ï¼Œç¼©çŸ­è®­ç»ƒæ—¶é—´ï¼Œå¹¶æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å¯ä»¥ä¸ºæ„å»ºæ›´å¤§è§„æ¨¡çš„è¯­è¨€æ¨¡å‹æä¾›æœ‰ç›Šçš„å‚è€ƒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) show best-in-class performance across a wide range of natural language processing applications. Training these models is an extremely computationally expensive task; frontier Artificial Intelligence (AI) research companies are investing billions of dollars into supercomputing infrastructure to train progressively larger models on increasingly massive datasets. Unfortunately, very little information about the scaling performance and training considerations of these large training pipelines is released publicly. Working with very large datasets and models can be complex and practical recommendations are scarce in the public literature for tuning training performance when scaling up large language models. In this paper, we aim to demystify the large language model pretraining pipeline somewhat - in particular with respect to distributed training, managing large datasets across hundreds of nodes, and scaling up data parallelism with an emphasis on fully leveraging available GPU compute capacity.

