---
layout: default
title: TalkToAgent: A Human-centric Explanation of Reinforcement Learning Agents with Large Language Models
---

# TalkToAgent: A Human-centric Explanation of Reinforcement Learning Agents with Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.04809" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.04809v2</a>
  <a href="https://arxiv.org/pdf/2509.04809.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.04809v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.04809v2', 'TalkToAgent: A Human-centric Explanation of Reinforcement Learning Agents with Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Haechang Kim, Hao Chen, Can Li, Jong Min Lee

**åˆ†ç±»**: cs.AI, cs.HC

**å‘å¸ƒæ—¥æœŸ**: 2025-09-05 (æ›´æ–°: 2025-09-08)

**å¤‡æ³¨**: 31 pages total

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTalkToAgentï¼Œåˆ©ç”¨LLMå®ç°äººæœºäº¤äº’å¼å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“è§£é‡Š**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¯è§£é‡Šå¼ºåŒ–å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `äººæœºäº¤äº’` `åäº‹å®è§£é‡Š` `å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¯è§£é‡Šå¼ºåŒ–å­¦ä¹ æ–¹æ³•éš¾ä»¥è¢«é¢†åŸŸä¸“å®¶ç†è§£ï¼Œä¸”å·¥å…·è¦†ç›–èŒƒå›´æœ‰é™ï¼Œç”¨æˆ·éš¾ä»¥é€‰æ‹©ã€‚
2. TalkToAgentåˆ©ç”¨å¤šæ™ºèƒ½ä½“LLMæ¡†æ¶ï¼Œå°†ç”¨æˆ·æŸ¥è¯¢æ˜ å°„åˆ°XRLå·¥å…·ï¼Œå¹¶æä¾›è‡ªç„¶è¯­è¨€è§£é‡Šã€‚
3. å®éªŒè¡¨æ˜ï¼ŒTalkToAgentèƒ½å‡†ç¡®æ˜ å°„ç”¨æˆ·æŸ¥è¯¢ï¼Œæœ‰æ•ˆè§£é‡Šæ™ºèƒ½ä½“è¡Œä¸ºï¼Œå¹¶å‡å°‘åäº‹å®ç”Ÿæˆå¤±è´¥ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¯è§£é‡Šå¼ºåŒ–å­¦ä¹ (XRL)åœ¨æé«˜å¼ºåŒ–å­¦ä¹ (RL)æ™ºèƒ½ä½“çš„é€æ˜åº¦æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç”±äºXRLç»“æœçš„ç†è§£éš¾åº¦ä»¥åŠç°æœ‰XRLæ–¹æ³•è¦†ç›–èŒƒå›´çš„å±€é™æ€§ï¼Œå¤æ‚çš„RLç­–ç•¥ä¸é¢†åŸŸä¸“å®¶ä¹‹é—´ä»ç„¶å­˜åœ¨å·®è·ï¼Œç”¨æˆ·ä¸ç¡®å®šè¯¥ä½¿ç”¨å“ªç§å·¥å…·ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†TalkToAgentï¼Œä¸€ä¸ªå¤šæ™ºèƒ½ä½“å¤§å‹è¯­è¨€æ¨¡å‹(LLM)æ¡†æ¶ï¼Œå®ƒä¸ºRLç­–ç•¥æä¾›äº¤äº’å¼çš„è‡ªç„¶è¯­è¨€è§£é‡Šã€‚è¯¥æ¶æ„åŒ…å«äº”ä¸ªä¸“é—¨çš„LLMæ™ºèƒ½ä½“ï¼ˆåè°ƒå™¨ã€è§£é‡Šå™¨ã€ç¼–ç å™¨ã€è¯„ä¼°å™¨å’Œè°ƒè¯•å™¨ï¼‰ï¼Œä½¿TalkToAgentèƒ½å¤Ÿè‡ªåŠ¨å°†ç”¨æˆ·æŸ¥è¯¢æ˜ å°„åˆ°ç›¸å…³çš„XRLå·¥å…·ï¼Œå¹¶æ ¹æ®å…³é”®çŠ¶æ€å˜é‡ã€é¢„æœŸç»“æœæˆ–åäº‹å®è§£é‡Šæ¥é˜æ˜æ™ºèƒ½ä½“çš„è¡Œä¸ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ä»å®šæ€§çš„è¡Œä¸ºæè¿°ç”šè‡³æ–°çš„åŸºäºè§„åˆ™çš„ç­–ç•¥ä¸­æ¨å¯¼å‡ºæ›¿ä»£æ–¹æ¡ˆï¼Œæ‰©å±•äº†å…ˆå‰çš„åäº‹å®è§£é‡Šã€‚æˆ‘ä»¬åœ¨å››ç½è¿‡ç¨‹æ§åˆ¶é—®é¢˜ï¼ˆä¸€ä¸ªè‘—åçš„éçº¿æ€§æ§åˆ¶åŸºå‡†ï¼‰ä¸ŠéªŒè¯äº†TalkToAgentã€‚ç»“æœè¡¨æ˜ï¼ŒTalkToAgentæˆåŠŸåœ°å°†ç”¨æˆ·æŸ¥è¯¢æ˜ å°„åˆ°XRLä»»åŠ¡ï¼Œå¹¶å…·æœ‰å¾ˆé«˜çš„å‡†ç¡®æ€§ï¼Œå¹¶ä¸”ç¼–ç å™¨-è°ƒè¯•å™¨äº¤äº’æœ€å¤§é™åº¦åœ°å‡å°‘äº†åäº‹å®ç”Ÿæˆçš„å¤±è´¥ã€‚æ­¤å¤–ï¼Œå®šæ€§è¯„ä¼°è¯å®äº†TalkToAgentæœ‰æ•ˆåœ°è§£é‡Šäº†æ™ºèƒ½ä½“çš„è¡Œä¸ºï¼Œå¹¶åœ¨é—®é¢˜é¢†åŸŸå†…å¯¹å…¶å«ä¹‰è¿›è¡Œäº†æƒ…å¢ƒåŒ–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¯è§£é‡Šå¼ºåŒ–å­¦ä¹ (XRL)æ–¹æ³•ç”Ÿæˆçš„è§£é‡Šéš¾ä»¥è¢«é¢†åŸŸä¸“å®¶ç†è§£ï¼Œå¹¶ä¸”å„ç§XRLå·¥å…·ä¹‹é—´ç¼ºä¹æ•´åˆï¼Œç”¨æˆ·éš¾ä»¥é€‰æ‹©åˆé€‚çš„å·¥å…·æ¥ç†è§£å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“çš„è¡Œä¸ºã€‚è¿™å¯¼è‡´äº†RLç­–ç•¥çš„é€æ˜åº¦ä¸è¶³ï¼Œé˜»ç¢äº†å…¶åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šTalkToAgentçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹(LLM)çš„å¼ºå¤§è‡ªç„¶è¯­è¨€å¤„ç†èƒ½åŠ›ï¼Œæ„å»ºä¸€ä¸ªå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿç†è§£ç”¨æˆ·çš„æŸ¥è¯¢ï¼Œè‡ªåŠ¨é€‰æ‹©åˆé€‚çš„XRLå·¥å…·ï¼Œå¹¶ä»¥è‡ªç„¶è¯­è¨€çš„å½¢å¼å‘ç”¨æˆ·è§£é‡Šå¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“çš„è¡Œä¸ºã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¼¥åˆäº†RLç­–ç•¥ä¸é¢†åŸŸä¸“å®¶ä¹‹é—´çš„é¸¿æ²Ÿï¼Œæé«˜äº†RLç­–ç•¥çš„å¯ç†è§£æ€§å’Œå¯ä¿¡åº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTalkToAgentçš„æ•´ä½“æ¶æ„åŒ…å«äº”ä¸ªä¸»è¦çš„LLMæ™ºèƒ½ä½“ï¼šåè°ƒå™¨(Coordinator)ã€è§£é‡Šå™¨(Explainer)ã€ç¼–ç å™¨(Coder)ã€è¯„ä¼°å™¨(Evaluator)å’Œè°ƒè¯•å™¨(Debugger)ã€‚åè°ƒå™¨è´Ÿè´£æ¥æ”¶ç”¨æˆ·æŸ¥è¯¢ï¼Œå¹¶å°†å…¶åˆ†é…ç»™åˆé€‚çš„æ™ºèƒ½ä½“è¿›è¡Œå¤„ç†ã€‚è§£é‡Šå™¨è´Ÿè´£æ ¹æ®æ™ºèƒ½ä½“çš„è¡Œä¸ºç”Ÿæˆè‡ªç„¶è¯­è¨€è§£é‡Šã€‚ç¼–ç å™¨è´Ÿè´£å°†è‡ªç„¶è¯­è¨€æè¿°è½¬æ¢ä¸ºå¯æ‰§è¡Œçš„ä»£ç æˆ–ç­–ç•¥ã€‚è¯„ä¼°å™¨è´Ÿè´£è¯„ä¼°ç”Ÿæˆçš„ä»£ç æˆ–ç­–ç•¥çš„æ€§èƒ½ã€‚è°ƒè¯•å™¨è´Ÿè´£ä¿®å¤ç¼–ç å™¨ç”Ÿæˆçš„ä»£ç ä¸­çš„é”™è¯¯ã€‚è¿™äº›æ™ºèƒ½ä½“ååŒå·¥ä½œï¼Œå…±åŒå®Œæˆç”¨æˆ·æå‡ºçš„XRLä»»åŠ¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šTalkToAgentçš„å…³é”®åˆ›æ–°åœ¨äºå…¶å¤šæ™ºèƒ½ä½“LLMæ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿè‡ªåŠ¨å°†ç”¨æˆ·æŸ¥è¯¢æ˜ å°„åˆ°ç›¸å…³çš„XRLå·¥å…·ï¼Œå¹¶ä»¥è‡ªç„¶è¯­è¨€çš„å½¢å¼æä¾›è§£é‡Šã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜æ‰©å±•äº†å…ˆå‰çš„åäº‹å®è§£é‡Šï¼Œèƒ½å¤Ÿä»å®šæ€§çš„è¡Œä¸ºæè¿°ç”šè‡³æ–°çš„åŸºäºè§„åˆ™çš„ç­–ç•¥ä¸­æ¨å¯¼å‡ºæ›¿ä»£æ–¹æ¡ˆã€‚è¿™ç§äº¤äº’å¼çš„è§£é‡Šæ–¹å¼ä½¿å¾—ç”¨æˆ·èƒ½å¤Ÿæ›´æ·±å…¥åœ°ç†è§£å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“çš„è¡Œä¸ºã€‚

**å…³é”®è®¾è®¡**ï¼šTalkToAgentçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š(1) äº”ä¸ªLLMæ™ºèƒ½ä½“çš„è§’è‰²å®šä¹‰å’Œåä½œæœºåˆ¶ï¼›(2) ç”¨æˆ·æŸ¥è¯¢åˆ°XRLå·¥å…·çš„æ˜ å°„ç­–ç•¥ï¼›(3) è‡ªç„¶è¯­è¨€è§£é‡Šçš„ç”Ÿæˆæ–¹æ³•ï¼›(4) åäº‹å®è§£é‡Šçš„æ¨å¯¼æ–¹æ³•ï¼›(5) ç¼–ç å™¨å’Œè°ƒè¯•å™¨ä¹‹é—´çš„è¿­ä»£ä¼˜åŒ–è¿‡ç¨‹ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®ã€æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç­‰æŠ€æœ¯ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœªè¯¦ç»†è¯´æ˜ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒTalkToAgentèƒ½å¤Ÿä»¥é«˜å‡†ç¡®ç‡å°†ç”¨æˆ·æŸ¥è¯¢æ˜ å°„åˆ°XRLä»»åŠ¡ã€‚ç¼–ç å™¨-è°ƒè¯•å™¨äº¤äº’æ˜¾è‘—å‡å°‘äº†åäº‹å®ç”Ÿæˆçš„å¤±è´¥ã€‚å®šæ€§è¯„ä¼°è¯å®ï¼ŒTalkToAgentèƒ½å¤Ÿæœ‰æ•ˆåœ°è§£é‡Šæ™ºèƒ½ä½“çš„è¡Œä¸ºï¼Œå¹¶åœ¨é—®é¢˜é¢†åŸŸå†…å¯¹å…¶å«ä¹‰è¿›è¡Œæƒ…å¢ƒåŒ–ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œå¯¹æ¯”åŸºçº¿åœ¨æ‘˜è¦ä¸­æœªæä¾›ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

TalkToAgentå¯åº”ç”¨äºå„ç§éœ€è¦äººæœºåä½œçš„å¼ºåŒ–å­¦ä¹ åœºæ™¯ï¼Œä¾‹å¦‚æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½æ¨èç³»ç»Ÿç­‰ã€‚é€šè¿‡æä¾›å¯è§£é‡Šçš„æ™ºèƒ½ä½“è¡Œä¸ºï¼Œå¯ä»¥æé«˜ç”¨æˆ·å¯¹ç³»ç»Ÿçš„ä¿¡ä»»åº¦ï¼Œå¹¶ä¿ƒè¿›äººä¸æ™ºèƒ½ä½“ä¹‹é—´çš„æœ‰æ•ˆåä½œã€‚è¯¥ç ”ç©¶çš„æœªæ¥å½±å“åœ¨äºæ¨åŠ¨å¯è§£é‡Šäººå·¥æ™ºèƒ½çš„å‘å±•ï¼Œä½¿AIç³»ç»Ÿæ›´åŠ é€æ˜ã€å¯ä¿¡å’Œæ˜“äºç†è§£ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Explainable Reinforcement Learning (XRL) has emerged as a promising approach in improving the transparency of Reinforcement Learning (RL) agents. However, there remains a gap between complex RL policies and domain experts, due to the limited comprehensibility of XRL results and isolated coverage of current XRL approaches that leave users uncertain about which tools to employ. To address these challenges, we introduce TalkToAgent, a multi-agent Large Language Models (LLM) framework that delivers interactive, natural language explanations for RL policies. The architecture with five specialized LLM agents (Coordinator, Explainer, Coder, Evaluator, and Debugger) enables TalkToAgent to automatically map user queries to relevant XRL tools and clarify an agent's actions in terms of either key state variables, expected outcomes, or counterfactual explanations. Moreover, our approach extends previous counterfactual explanations by deriving alternative scenarios from qualitative behavioral descriptions, or even new rule-based policies. We validated TalkToAgent on quadruple-tank process control problem, a well-known nonlinear control benchmark. Results demonstrated that TalkToAgent successfully mapped user queries into XRL tasks with high accuracy, and coder-debugger interactions minimized failures in counterfactual generation. Furthermore, qualitative evaluation confirmed that TalkToAgent effectively interpreted agent's actions and contextualized their meaning within the problem domain.

