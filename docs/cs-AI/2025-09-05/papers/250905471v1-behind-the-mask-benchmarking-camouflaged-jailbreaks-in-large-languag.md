---
layout: default
title: Behind the Mask: Benchmarking Camouflaged Jailbreaks in Large Language Models
---

# Behind the Mask: Benchmarking Camouflaged Jailbreaks in Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.05471" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.05471v1</a>
  <a href="https://arxiv.org/pdf/2509.05471.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.05471v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.05471v1', 'Behind the Mask: Benchmarking Camouflaged Jailbreaks in Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Youjia Zheng, Mohammad Zandsalimy, Shanu Sushmita

**åˆ†ç±»**: cs.CR, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-05

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¼ªè£…è¶Šç‹±æç¤ºåŸºå‡†æµ‹è¯•ï¼Œè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹åœ¨éšè”½å¯¹æŠ—æ”»å‡»ä¸‹çš„å®‰å…¨æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `å¯¹æŠ—æ”»å‡»` `ä¼ªè£…è¶Šç‹±` `å®‰å…¨è¯„ä¼°` `åŸºå‡†æµ‹è¯•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§è¯­è¨€æ¨¡å‹å®‰å…¨æœºåˆ¶éš¾ä»¥æœ‰æ•ˆé˜²å¾¡ä¼ªè£…è¶Šç‹±æ”»å‡»ï¼Œè¯¥æ”»å‡»é€šè¿‡åµŒå…¥æ¶æ„æ„å›¾äºè‰¯æ€§è¯­è¨€ä¸­ç»•è¿‡é˜²å¾¡ã€‚
2. è®ºæ–‡æ ¸å¿ƒåœ¨äºæ„å»ºä¸€ä¸ªåŒ…å«è‰¯æ€§å’Œæ¶æ„ä¼ªè£…æç¤ºçš„åŸºå‡†æ•°æ®é›†ï¼Œå¹¶æå‡ºå¤šç»´åº¦è¯„ä¼°æ¡†æ¶æ¥è¡¡é‡LLMçš„å®‰å…¨æ€§ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒLLMåœ¨é¢å¯¹ä¼ªè£…è¶Šç‹±æç¤ºæ—¶ï¼Œå®‰å…¨æ€§å’Œæ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œå‡¸æ˜¾äº†ç°æœ‰é˜²å¾¡ç­–ç•¥çš„ä¸è¶³ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¶Šæ¥è¶Šå®¹æ˜“å—åˆ°ä¸€ç§åä¸ºä¼ªè£…è¶Šç‹±çš„å¤æ‚å¯¹æŠ—æ€§æç¤ºæ”»å‡»çš„å½±å“ã€‚è¿™ç§æ–¹æ³•å°†æ¶æ„æ„å›¾åµŒå…¥åˆ°çœ‹ä¼¼è‰¯æ€§çš„è¯­è¨€ä¸­ï¼Œä»¥è§„é¿ç°æœ‰çš„å®‰å…¨æœºåˆ¶ã€‚ä¸å…¬å¼€çš„æ”»å‡»ä¸åŒï¼Œè¿™äº›å¾®å¦™çš„æç¤ºåˆ©ç”¨äº†ä¸Šä¸‹æ–‡çš„æ¨¡ç³Šæ€§å’Œè¯­è¨€çš„çµæ´»æ€§ï¼Œå¯¹å½“å‰çš„é˜²å¾¡ç³»ç»Ÿæ„æˆäº†é‡å¤§æŒ‘æˆ˜ã€‚æœ¬æ–‡ç ”ç©¶äº†ä¼ªè£…è¶Šç‹±æç¤ºçš„æ„å»ºå’Œå½±å“ï¼Œå¼ºè°ƒäº†å®ƒä»¬çš„æ¬ºéª—æ€§ç‰¹å¾ä»¥åŠä¼ ç»ŸåŸºäºå…³é”®è¯çš„æ£€æµ‹æ–¹æ³•çš„å±€é™æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æ•°æ®é›†ï¼Œå³ä¼ªè£…è¶Šç‹±æç¤ºï¼Œå…¶ä¸­åŒ…å«500ä¸ªç²¾å¿ƒç­–åˆ’çš„ç¤ºä¾‹ï¼ˆ400ä¸ªæœ‰å®³æç¤ºå’Œ100ä¸ªè‰¯æ€§æç¤ºï¼‰ï¼Œæ—¨åœ¨ä¸¥æ ¼æµ‹è¯•LLMå®‰å…¨åè®®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¤šæ–¹é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä»ä¸ƒä¸ªç»´åº¦è¡¡é‡å±å®³æ€§ï¼šå®‰å…¨æ„è¯†ã€æŠ€æœ¯å¯è¡Œæ€§ã€å®æ–½ä¿éšœã€æ½œåœ¨å±å®³ã€æ•™è‚²ä»·å€¼ã€å†…å®¹è´¨é‡å’Œåˆè§„æ€§å¾—åˆ†ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœæ­ç¤ºäº†LLMè¡Œä¸ºçš„é²œæ˜å¯¹æ¯”ï¼šè™½ç„¶æ¨¡å‹åœ¨è‰¯æ€§è¾“å…¥ä¸‹è¡¨ç°å‡ºè¾ƒé«˜çš„å®‰å…¨æ€§å’Œå†…å®¹è´¨é‡ï¼Œä½†åœ¨é¢å¯¹ä¼ªè£…è¶Šç‹±å°è¯•æ—¶ï¼Œå…¶æ€§èƒ½å’Œå®‰å…¨æ€§æ˜¾è‘—ä¸‹é™ã€‚è¿™ç§å·®å¼‚çªæ˜¾äº†ä¸€ç§æ™®éå­˜åœ¨çš„æ¼æ´ï¼Œå¼ºè°ƒè¿«åˆ‡éœ€è¦æ›´ç»†è‡´å’Œè‡ªé€‚åº”çš„å®‰å…¨ç­–ç•¥ï¼Œä»¥ç¡®ä¿LLMåœ¨å®é™…åº”ç”¨ä¸­çš„è´Ÿè´£ä»»å’Œç¨³å¥éƒ¨ç½²ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é¢å¯¹ä¼ªè£…è¶Šç‹±æ”»å‡»æ—¶å­˜åœ¨çš„å®‰å…¨æ¼æ´é—®é¢˜ã€‚ç°æœ‰çš„åŸºäºå…³é”®è¯æ£€æµ‹ç­‰å®‰å…¨æœºåˆ¶éš¾ä»¥æœ‰æ•ˆè¯†åˆ«å’Œé˜²å¾¡è¿™ç§éšè”½çš„å¯¹æŠ—æ€§æ”»å‡»ï¼Œå› ä¸ºæ”»å‡»è€…å°†æ¶æ„æ„å›¾éšè—åœ¨çœ‹ä¼¼æ— å®³çš„è¯­è¨€ä¸­ï¼Œåˆ©ç”¨äº†è¯­è¨€çš„æ¨¡ç³Šæ€§å’Œçµæ´»æ€§ã€‚è¿™ç§æ”»å‡»æ–¹å¼ä½¿å¾—ä¼ ç»Ÿçš„é˜²å¾¡æ‰‹æ®µå¤±æ•ˆï¼Œå¯¼è‡´LLMså¯èƒ½ç”Ÿæˆæœ‰å®³æˆ–ä¸å½“çš„å†…å®¹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªåŒ…å«å¤§é‡ä¼ªè£…è¶Šç‹±æç¤ºçš„åŸºå‡†æ•°æ®é›†ï¼Œå¹¶è®¾è®¡ä¸€ä¸ªå¤šç»´åº¦çš„è¯„ä¼°æ¡†æ¶ï¼Œä»¥ç³»ç»Ÿåœ°è¯„ä¼°LLMsåœ¨é¢å¯¹æ­¤ç±»æ”»å‡»æ—¶çš„å®‰å…¨æ€§èƒ½ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¯ä»¥æ›´å…¨é¢åœ°äº†è§£LLMsçš„å®‰å…¨æ¼æ´ï¼Œå¹¶ä¸ºå¼€å‘æ›´æœ‰æ•ˆçš„é˜²å¾¡ç­–ç•¥æä¾›ä¾æ®ã€‚è®ºæ–‡å¼ºè°ƒäº†ä¼ªè£…è¶Šç‹±æç¤ºçš„æ¬ºéª—æ€§ï¼Œå¹¶æŒ‡å‡ºéœ€è¦æ›´ç»†è‡´å’Œè‡ªé€‚åº”çš„å®‰å…¨ç­–ç•¥ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä¸¤ä¸ªéƒ¨åˆ†ï¼šä¸€æ˜¯ä¼ªè£…è¶Šç‹±æç¤ºåŸºå‡†æ•°æ®é›†çš„æ„å»ºï¼ŒäºŒæ˜¯å¤šç»´åº¦è¯„ä¼°æ¡†æ¶çš„è®¾è®¡ã€‚åŸºå‡†æ•°æ®é›†åŒ…å«400ä¸ªæœ‰å®³æç¤ºå’Œ100ä¸ªè‰¯æ€§æç¤ºï¼Œè¿™äº›æç¤ºç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œæ—¨åœ¨æµ‹è¯•LLMsçš„å®‰å…¨åè®®ã€‚è¯„ä¼°æ¡†æ¶ä»ä¸ƒä¸ªç»´åº¦è¡¡é‡å±å®³æ€§ï¼ŒåŒ…æ‹¬å®‰å…¨æ„è¯†ã€æŠ€æœ¯å¯è¡Œæ€§ã€å®æ–½ä¿éšœã€æ½œåœ¨å±å®³ã€æ•™è‚²ä»·å€¼ã€å†…å®¹è´¨é‡å’Œåˆè§„æ€§å¾—åˆ†ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ä¸ªä¸“é—¨é’ˆå¯¹ä¼ªè£…è¶Šç‹±æ”»å‡»çš„åŸºå‡†æ•°æ®é›†å’Œä¸€ä¸ªå¤šç»´åº¦çš„è¯„ä¼°æ¡†æ¶ã€‚ä¸ä»¥å¾€çš„ç ”ç©¶ä¸åŒï¼Œè¯¥è®ºæ–‡å…³æ³¨çš„æ˜¯éšè”½çš„å¯¹æŠ—æ€§æ”»å‡»ï¼Œè€Œä¸æ˜¯å…¬å¼€çš„æ”»å‡»ã€‚é€šè¿‡æ„å»ºåŒ…å«å¤§é‡ä¼ªè£…æç¤ºçš„æ•°æ®é›†ï¼Œå¹¶ä»å¤šä¸ªç»´åº¦è¯„ä¼°LLMsçš„å®‰å…¨æ€§ï¼Œè¯¥è®ºæ–‡èƒ½å¤Ÿæ›´å…¨é¢åœ°äº†è§£LLMsçš„å®‰å…¨æ¼æ´ï¼Œå¹¶ä¸ºå¼€å‘æ›´æœ‰æ•ˆçš„é˜²å¾¡ç­–ç•¥æä¾›ä¾æ®ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨åŸºå‡†æ•°æ®é›†çš„æ„å»ºæ–¹é¢ï¼Œè®ºæ–‡ä½œè€…ç²¾å¿ƒè®¾è®¡äº†å„ç§ä¼ªè£…è¶Šç‹±æç¤ºï¼Œä»¥æ¨¡æ‹ŸçœŸå®çš„æ”»å‡»åœºæ™¯ã€‚è¿™äº›æç¤ºåˆ©ç”¨äº†è¯­è¨€çš„æ¨¡ç³Šæ€§å’Œçµæ´»æ€§ï¼Œä½¿å¾—LLMséš¾ä»¥è¯†åˆ«å…¶ä¸­çš„æ¶æ„æ„å›¾ã€‚åœ¨è¯„ä¼°æ¡†æ¶çš„è®¾è®¡æ–¹é¢ï¼Œè®ºæ–‡ä½œè€…é€‰æ‹©äº†ä¸ƒä¸ªå…³é”®ç»´åº¦æ¥è¡¡é‡å±å®³æ€§ï¼Œè¿™äº›ç»´åº¦æ¶µç›–äº†LLMså®‰å…¨æ€§çš„å„ä¸ªæ–¹é¢ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ç­‰æŠ€æœ¯ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœªè¯¦ç»†è¯´æ˜ï¼Œå±äºæ•°æ®é›†æ„å»ºå’Œè¯„ä¼°æŒ‡æ ‡é€‰æ‹©çš„èŒƒç•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMåœ¨é¢å¯¹è‰¯æ€§è¾“å…¥æ—¶è¡¨ç°å‡ºè¾ƒé«˜çš„å®‰å…¨æ€§å’Œå†…å®¹è´¨é‡ï¼Œä½†åœ¨é¢å¯¹ä¼ªè£…è¶Šç‹±æç¤ºæ—¶ï¼Œå…¶æ€§èƒ½å’Œå®‰å…¨æ€§æ˜¾è‘—ä¸‹é™ã€‚è¿™ä¸€ç»“æœçªæ˜¾äº†ç°æœ‰LLMåœ¨é¢å¯¹éšè”½å¯¹æŠ—æ€§æ”»å‡»æ—¶çš„è„†å¼±æ€§ï¼Œå¹¶å¼ºè°ƒäº†å¼€å‘æ›´æœ‰æ•ˆçš„é˜²å¾¡ç­–ç•¥çš„å¿…è¦æ€§ã€‚å…·ä½“çš„æ€§èƒ½ä¸‹é™å¹…åº¦æœªåœ¨æ‘˜è¦ä¸­ç»™å‡ºï¼Œéœ€è¦åœ¨è®ºæ–‡æ­£æ–‡ä¸­æŸ¥æ‰¾ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæå‡å¤§è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§ï¼Œå°¤å…¶æ˜¯åœ¨é‡‘èã€åŒ»ç–—ã€æ³•å¾‹ç­‰å¯¹å®‰å…¨æ€§è¦æ±‚æé«˜çš„é¢†åŸŸã€‚é€šè¿‡ä½¿ç”¨è¯¥åŸºå‡†æµ‹è¯•é›†ï¼Œå¼€å‘è€…å¯ä»¥è¯„ä¼°å’Œæ”¹è¿›å…¶æ¨¡å‹çš„é˜²å¾¡èƒ½åŠ›ï¼Œé™ä½æ¨¡å‹è¢«æ¶æ„åˆ©ç”¨çš„é£é™©ã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶å¯ä»¥æ¨åŠ¨å¼€å‘æ›´é²æ£’ã€æ›´å®‰å…¨çš„LLMï¼Œä»è€Œä¿ƒè¿›äººå·¥æ™ºèƒ½æŠ€æœ¯çš„è´Ÿè´£ä»»å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) are increasingly vulnerable to a sophisticated form of adversarial prompting known as camouflaged jailbreaking. This method embeds malicious intent within seemingly benign language to evade existing safety mechanisms. Unlike overt attacks, these subtle prompts exploit contextual ambiguity and the flexible nature of language, posing significant challenges to current defense systems. This paper investigates the construction and impact of camouflaged jailbreak prompts, emphasizing their deceptive characteristics and the limitations of traditional keyword-based detection methods. We introduce a novel benchmark dataset, Camouflaged Jailbreak Prompts, containing 500 curated examples (400 harmful and 100 benign prompts) designed to rigorously stress-test LLM safety protocols. In addition, we propose a multi-faceted evaluation framework that measures harmfulness across seven dimensions: Safety Awareness, Technical Feasibility, Implementation Safeguards, Harmful Potential, Educational Value, Content Quality, and Compliance Score. Our findings reveal a stark contrast in LLM behavior: while models demonstrate high safety and content quality with benign inputs, they exhibit a significant decline in performance and safety when confronted with camouflaged jailbreak attempts. This disparity underscores a pervasive vulnerability, highlighting the urgent need for more nuanced and adaptive security strategies to ensure the responsible and robust deployment of LLMs in real-world applications.

