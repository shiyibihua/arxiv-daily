---
layout: default
title: VoltanaLLM: Feedback-Driven Frequency Control and State-Space Routing for Energy-Efficient LLM Serving
---

# VoltanaLLM: Feedback-Driven Frequency Control and State-Space Routing for Energy-Efficient LLM Serving

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.04827" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.04827v2</a>
  <a href="https://arxiv.org/pdf/2509.04827.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.04827v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.04827v2', 'VoltanaLLM: Feedback-Driven Frequency Control and State-Space Routing for Energy-Efficient LLM Serving')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiahuan Yu, Aryan Taneja, Junfeng Lin, Minjia Zhang

**åˆ†ç±»**: cs.DC, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-05 (æ›´æ–°: 2025-09-14)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/Supercomputing-System-AI-Lab/VoltanaLLM)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**VoltanaLLMï¼šé¢å‘èŠ‚èƒ½LLMæœåŠ¡çš„åé¦ˆé©±åŠ¨é¢‘ç‡æ§åˆ¶ä¸çŠ¶æ€ç©ºé—´è·¯ç”±**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `èŠ‚èƒ½æœåŠ¡` `é¢‘ç‡æ§åˆ¶` `è¯·æ±‚è·¯ç”±` `æ§åˆ¶ç†è®º` `æœåŠ¡æ°´å¹³ç›®æ ‡` `çŠ¶æ€ç©ºé—´è·¯ç”±`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMæœåŠ¡ç³»ç»Ÿé¢ä¸´é«˜æ˜‚çš„èƒ½æºæˆæœ¬ï¼Œé˜»ç¢äº†å…¶å¯æŒç»­å’Œç»æµé«˜æ•ˆçš„éƒ¨ç½²ï¼Œå°¤å…¶æ˜¯åœ¨äº¤äº’å¼åº”ç”¨ä¸­ã€‚
2. VoltanaLLMé€šè¿‡æ§åˆ¶ç†è®ºæ–¹æ³•ï¼ŒååŒè®¾è®¡é¢‘ç‡ç¼©æ”¾å’Œè¯·æ±‚è·¯ç”±ï¼Œå®ç°ç»†ç²’åº¦çš„é˜¶æ®µç‰¹å®šæ§åˆ¶ï¼Œä»è€Œä¼˜åŒ–èƒ½æºæ•ˆç‡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒVoltanaLLMåœ¨ä¿æŒè¿‘ä¹å®Œç¾çš„SLOè¾¾æˆç‡çš„åŒæ—¶ï¼Œå®ç°äº†é«˜è¾¾36.3%çš„èŠ‚èƒ½æ•ˆæœã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœåŠ¡ç³»ç»Ÿè¶Šæ¥è¶Šå¤šåœ°æ”¯æŒäº¤äº’å¼åº”ç”¨ï¼Œå¦‚å®æ—¶èŠå¤©åŠ©æ‰‹ã€ä»£ç ç”Ÿæˆå·¥å…·å’Œæ™ºèƒ½ä½“å·¥ä½œæµã€‚ç„¶è€Œï¼ŒLLMæ¨ç†çš„èƒ½æºæˆæœ¬æ€¥å‰§ä¸Šå‡ï¼Œå¯¹å¯æŒç»­å’Œç»æµé«˜æ•ˆçš„éƒ¨ç½²æå‡ºäº†æ—¥ç›Šä¸¥å³»çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é¢å‘æœåŠ¡æ°´å¹³ç›®æ ‡ï¼ˆSLOï¼‰æ„ŸçŸ¥ã€èŠ‚èƒ½LLMæœåŠ¡çš„ç³»ç»ŸVoltanaLLMï¼Œè¯¥ç³»ç»Ÿä»æ§åˆ¶ç†è®ºçš„è§’åº¦æ„å»ºã€‚VoltanaLLMå…±åŒè®¾è®¡äº†æ–°å…´çš„é¢„å¡«å……/è§£ç åˆ†ç¦»æ¶æ„ä¸­çš„é¢‘ç‡ç¼©æ”¾å’Œè¯·æ±‚è·¯ç”±ï¼Œåˆ©ç”¨å®ƒä»¬è§£è€¦çš„æ‰§è¡Œæ¥å®ç°ç»†ç²’åº¦çš„é˜¶æ®µç‰¹å®šæ§åˆ¶ã€‚å®ƒç”±ä¸€ä¸ªåé¦ˆé©±åŠ¨çš„é¢‘ç‡æ§åˆ¶å™¨ç»„æˆï¼Œè¯¥æ§åˆ¶å™¨åŠ¨æ€åœ°è°ƒæ•´é¢„å¡«å……å’Œè§£ç é˜¶æ®µçš„GPUé¢‘ç‡ï¼Œä»¥åŠä¸€ä¸ªçŠ¶æ€ç©ºé—´è·¯ç”±å™¨ï¼Œè¯¥è·¯ç”±å™¨æ¢ç´¢è·¨é¢‘ç‡ç¼©æ”¾å®ä¾‹çš„è·¯ç”±å†³ç­–ï¼Œä»¥åœ¨å»¶è¿Ÿçº¦æŸä¸‹æœ€å°åŒ–èƒ½é‡ã€‚æˆ‘ä»¬åœ¨SGLangä¸­å®ç°äº†VoltanaLLMï¼Œå¹¶åœ¨å¤šä¸ªæœ€å…ˆè¿›çš„LLMå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šè¯„ä¼°äº†å…¶æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼ŒVoltanaLLMåœ¨ä¿æŒæ¥è¿‘å®Œç¾çš„SLOè¾¾æˆç‡çš„åŒæ—¶ï¼Œå®ç°äº†é«˜è¾¾36.3%çš„èŠ‚èƒ½æ•ˆæœï¼Œä¸ºå¯æŒç»­å’Œæ™ºèƒ½çš„LLMæœåŠ¡é“ºå¹³äº†é“è·¯ã€‚VoltanaLLMçš„ä»£ç å·²åœ¨GitHubä¸Šå¼€æºã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœåŠ¡ä¸­æ—¥ç›Šå¢é•¿çš„èƒ½æºæ¶ˆè€—é—®é¢˜ã€‚ç°æœ‰çš„LLMæœåŠ¡ç³»ç»Ÿï¼Œå°¤å…¶æ˜¯åœ¨æ”¯æŒäº¤äº’å¼åº”ç”¨æ—¶ï¼Œé¢ä¸´ç€é«˜æ˜‚çš„æ¨ç†èƒ½æºæˆæœ¬ã€‚ä¼ ç»Ÿçš„é¢‘ç‡æ§åˆ¶å’Œè¯·æ±‚è·¯ç”±æ–¹æ³•æ— æ³•å……åˆ†åˆ©ç”¨é¢„å¡«å……å’Œè§£ç é˜¶æ®µçš„è§£è€¦ç‰¹æ€§ï¼Œå¯¼è‡´èƒ½æºæ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šVoltanaLLMçš„æ ¸å¿ƒæ€è·¯æ˜¯ä»æ§åˆ¶ç†è®ºçš„è§’åº¦å‡ºå‘ï¼Œå°†LLMæœåŠ¡ç³»ç»Ÿè§†ä¸ºä¸€ä¸ªå¯æ§ç³»ç»Ÿã€‚é€šè¿‡åé¦ˆé©±åŠ¨çš„é¢‘ç‡æ§åˆ¶å’ŒçŠ¶æ€ç©ºé—´è·¯ç”±ï¼ŒåŠ¨æ€åœ°è°ƒæ•´GPUé¢‘ç‡å’Œè¯·æ±‚è·¯ç”±ç­–ç•¥ï¼Œä»¥åœ¨æ»¡è¶³å»¶è¿Ÿçº¦æŸçš„å‰æä¸‹æœ€å°åŒ–èƒ½æºæ¶ˆè€—ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿå®ç°ç»†ç²’åº¦çš„é˜¶æ®µç‰¹å®šæ§åˆ¶ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°åˆ©ç”¨è®¡ç®—èµ„æºã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVoltanaLLMçš„æ•´ä½“æ¶æ„åŒ…å«ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šåé¦ˆé©±åŠ¨çš„é¢‘ç‡æ§åˆ¶å™¨å’ŒçŠ¶æ€ç©ºé—´è·¯ç”±å™¨ã€‚é¢‘ç‡æ§åˆ¶å™¨è´Ÿè´£åŠ¨æ€è°ƒæ•´é¢„å¡«å……å’Œè§£ç é˜¶æ®µçš„GPUé¢‘ç‡ï¼Œä»¥ä¼˜åŒ–èƒ½æºæ•ˆç‡ã€‚çŠ¶æ€ç©ºé—´è·¯ç”±å™¨åˆ™æ ¹æ®å½“å‰ç³»ç»Ÿçš„çŠ¶æ€ï¼ˆå¦‚å»¶è¿Ÿã€è´Ÿè½½ç­‰ï¼‰ï¼Œé€‰æ‹©åˆé€‚çš„å®ä¾‹è¿›è¡Œè¯·æ±‚è·¯ç”±ï¼Œä»¥æ»¡è¶³å»¶è¿Ÿçº¦æŸã€‚è¿™ä¸¤ä¸ªæ¨¡å—ååŒå·¥ä½œï¼Œå…±åŒå®ç°èŠ‚èƒ½çš„LLMæœåŠ¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šVoltanaLLMçš„å…³é”®åˆ›æ–°åœ¨äºå…¶ååŒè®¾è®¡çš„é¢‘ç‡æ§åˆ¶å’Œè¯·æ±‚è·¯ç”±æœºåˆ¶ã€‚ä¼ ç»Ÿçš„é¢‘ç‡æ§åˆ¶å’Œè¯·æ±‚è·¯ç”±é€šå¸¸æ˜¯ç‹¬ç«‹è¿›è¡Œçš„ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨é¢„å¡«å……å’Œè§£ç é˜¶æ®µçš„è§£è€¦ç‰¹æ€§ã€‚VoltanaLLMé€šè¿‡å°†è¿™ä¸¤ä¸ªæ¨¡å—é›†æˆåœ¨ä¸€èµ·ï¼Œå®ç°äº†ç»†ç²’åº¦çš„é˜¶æ®µç‰¹å®šæ§åˆ¶ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°ä¼˜åŒ–èƒ½æºæ•ˆç‡ã€‚æ­¤å¤–ï¼ŒVoltanaLLMè¿˜é‡‡ç”¨äº†çŠ¶æ€ç©ºé—´è·¯ç”±ï¼Œèƒ½å¤Ÿæ ¹æ®ç³»ç»Ÿçš„å®æ—¶çŠ¶æ€åŠ¨æ€åœ°è°ƒæ•´è·¯ç”±ç­–ç•¥ï¼Œä»è€Œæ›´å¥½åœ°æ»¡è¶³å»¶è¿Ÿçº¦æŸã€‚

**å…³é”®è®¾è®¡**ï¼šé¢‘ç‡æ§åˆ¶å™¨é‡‡ç”¨PIDæ§åˆ¶ç®—æ³•ï¼Œæ ¹æ®å»¶è¿Ÿåé¦ˆåŠ¨æ€è°ƒæ•´GPUé¢‘ç‡ã€‚çŠ¶æ€ç©ºé—´è·¯ç”±å™¨ä½¿ç”¨é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰æ¥å»ºæ¨¡è¯·æ±‚è·¯ç”±é—®é¢˜ï¼Œå¹¶ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•æ¥å­¦ä¹ æœ€ä¼˜çš„è·¯ç”±ç­–ç•¥ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ç­‰æŠ€æœ¯ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

VoltanaLLMåœ¨å¤šä¸ªæœ€å…ˆè¿›çš„LLMå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVoltanaLLMåœ¨ä¿æŒæ¥è¿‘å®Œç¾çš„SLOè¾¾æˆç‡çš„åŒæ—¶ï¼Œå®ç°äº†é«˜è¾¾36.3%çš„èŠ‚èƒ½æ•ˆæœã€‚ä¸ä¼ ç»Ÿçš„LLMæœåŠ¡ç³»ç»Ÿç›¸æ¯”ï¼ŒVoltanaLLMèƒ½å¤Ÿæ˜¾è‘—é™ä½èƒ½æºæ¶ˆè€—ï¼Œä»è€Œé™ä½è¿è¥æˆæœ¬å¹¶æé«˜å¯æŒç»­æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

VoltanaLLMé€‚ç”¨äºå„ç§éœ€è¦é«˜æ•ˆLLMæœåŠ¡çš„åœºæ™¯ï¼Œå¦‚å®æ—¶èŠå¤©æœºå™¨äººã€ä»£ç ç”Ÿæˆå·¥å…·ã€æ™ºèƒ½ä½“å·¥ä½œæµç­‰ã€‚è¯¥ç ”ç©¶æˆæœæœ‰åŠ©äºé™ä½LLMæœåŠ¡çš„è¿è¥æˆæœ¬ï¼Œæé«˜èƒ½æºåˆ©ç”¨ç‡ï¼Œå¹¶ä¿ƒè¿›LLMæŠ€æœ¯çš„å¯æŒç»­å‘å±•ã€‚æœªæ¥ï¼ŒVoltanaLLMå¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°æ”¯æŒæ›´å¤šç±»å‹çš„LLMå’Œç¡¬ä»¶å¹³å°ï¼Œå¹¶ä¸å…¶ä»–ä¼˜åŒ–æŠ€æœ¯ç›¸ç»“åˆï¼Œä»¥å®ç°æ›´é«˜çš„èƒ½æºæ•ˆç‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Modern Large Language Model (LLM) serving systems increasingly support interactive applications, like real-time chat assistants, code generation tools, and agentic workflows. However, the soaring energy cost of LLM inference presents a growing challenge for sustainable and cost-effective deployment. This paper introduces VoltanaLLM, a system for SLO-aware, energy-efficient LLM serving, built from a control theory perspective. VoltanaLLM co-designs frequency scaling and request routing in emerging prefill/decode disaggregated architectures, leveraging their decoupled execution to enable fine-grained phase-specific control. It consists of a feedback-driven frequency controller that dynamically adapts GPU frequency for prefill and decode phases, and a state-space router that explores routing decisions across frequency-scaled instances to minimize energy under latency constraints. We implement VoltanaLLM in SGLang and evaluate its performance over multiple state-of-the-art LLMs and real-world datasets. The results demonstrate that VoltanaLLM achieves up to 36.3% energy savings while maintaining near-perfect SLO attainment rate, paving the way for sustainable and intelligent LLM serving. Code of VoltanaLLM is open-sourced on GitHub: https://github.com/Supercomputing-System-AI-Lab/VoltanaLLM.

