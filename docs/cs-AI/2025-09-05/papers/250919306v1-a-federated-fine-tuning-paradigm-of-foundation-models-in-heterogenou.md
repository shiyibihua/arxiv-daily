---
layout: default
title: A Federated Fine-Tuning Paradigm of Foundation Models in Heterogenous Wireless Networks
---

# A Federated Fine-Tuning Paradigm of Foundation Models in Heterogenous Wireless Networks

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.19306" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.19306v1</a>
  <a href="https://arxiv.org/pdf/2509.19306.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.19306v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.19306v1', 'A Federated Fine-Tuning Paradigm of Foundation Models in Heterogenous Wireless Networks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jingyi Wang, Zhongyuan Zhao, Qingtian Wang, Zexu Li, Yue Wang, Tony Q. S. Quek

**åˆ†ç±»**: eess.SP, cs.AI, cs.IT, cs.NI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-05

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§å¼‚æ„æ— çº¿ç½‘ç»œä¸‹è”é‚¦å¾®è°ƒåŸºç¡€æ¨¡å‹çš„åœ¨çº¿ä¼˜åŒ–æ¡†æ¶ï¼Œæå‡æ¨¡å‹ç²¾åº¦å’Œèƒ½æ•ˆã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è”é‚¦å­¦ä¹ ` `è¾¹ç¼˜æ™ºèƒ½` `åŸºç¡€æ¨¡å‹` `ä½ç§©é€‚åº”` `åœ¨çº¿ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è”é‚¦å­¦ä¹ å¾®è°ƒæ–¹æ³•éš¾ä»¥åº”å¯¹æ— çº¿ç½‘ç»œä¸­è®¾å¤‡å¼‚æ„æ€§å’Œèµ„æºçº¦æŸå¸¦æ¥çš„æŒ‘æˆ˜ã€‚
2. æå‡ºåŸºäºåˆ‡æ¢çš„è”é‚¦å¾®è°ƒæ¡†æ¶ï¼Œè¾¹ç¼˜è®¾å¤‡åŠ¨æ€é€‰æ‹©LoRAæ¨¡å—ï¼Œå¹¶ç»“åˆåœ¨çº¿ä¼˜åŒ–ç®—æ³•ã€‚
3. åœ¨SST-2å’ŒQNLIæ•°æ®é›†ä¸ŠéªŒè¯ï¼Œç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨æµ‹è¯•ç²¾åº¦å’Œèƒ½æºæ•ˆç‡æ–¹é¢å‡æœ‰æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¾¹ç¼˜æ™ºèƒ½å·²æˆä¸ºä¸ºç§»åŠ¨è®¾å¤‡æä¾›ä½å»¶è¿Ÿå’Œæ™®é€‚æœåŠ¡çš„æœ‰å‰æ™¯çš„ç­–ç•¥ã€‚åŸºç¡€æ¨¡å‹å¾®è°ƒæœºåˆ¶çš„æœ€æ–°è¿›å±•é€šè¿‡å°†ä½ç§©é€‚åº”(LoRA)ä¸è”é‚¦å­¦ä¹ ç›¸ç»“åˆï¼Œå®ç°äº†è¾¹ç¼˜æ™ºèƒ½ã€‚ç„¶è€Œï¼Œåœ¨æ— çº¿ç½‘ç»œä¸­ï¼Œè®¾å¤‡å¼‚æ„æ€§å’Œè¾¹ç¼˜è®¾å¤‡ä¸Šçš„èµ„æºçº¦æŸå¯¹è”é‚¦å¾®è°ƒçš„æ€§èƒ½æ„æˆäº†å·¨å¤§å¨èƒã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºé€šè¿‡åœ¨çº¿å­¦ä¹ æ¥ä¼˜åŒ–å¼‚æ„æ— çº¿ç½‘ç»œä¸­çš„è”é‚¦å¾®è°ƒã€‚é¦–å…ˆï¼Œæä¾›äº†æ— çº¿ç½‘ç»œä¸­åŸºäºåˆ‡æ¢çš„è”é‚¦å¾®è°ƒæ¡†æ¶ã€‚è¾¹ç¼˜è®¾å¤‡åŠ¨æ€åˆ‡æ¢åˆ°LoRAæ¨¡å—ï¼Œä¸åŸºç«™è¿›è¡Œè”é‚¦å¾®è°ƒï¼Œä»¥å…±åŒå‡è½»è®¾å¤‡å¼‚æ„æ€§å’Œä¼ è¾“ä¸å¯é æ€§çš„å½±å“ã€‚å…¶æ¬¡ï¼ŒåŸºäºç†è®ºåˆ†æï¼Œæ¨å¯¼äº†æ¨ç†é£é™©å·®è·çš„å¯å¤„ç†ä¸Šç•Œã€‚ä¸ºäº†æé«˜æ³›åŒ–èƒ½åŠ›ï¼Œæˆ‘ä»¬åˆ¶å®šäº†ä¸€ä¸ªå…·æœ‰é•¿æœŸçº¦æŸçš„éå‡¸æ··åˆæ•´æ•°è§„åˆ’é—®é¢˜ï¼Œå¹¶å°†å…¶è§£è€¦ä¸ºæ¨¡å‹åˆ‡æ¢ã€å‘å°„åŠŸç‡æ§åˆ¶å’Œå¸¦å®½åˆ†é…å­é—®é¢˜ã€‚å¼€å‘äº†ä¸€ç§åœ¨çº¿ä¼˜åŒ–ç®—æ³•æ¥è§£å†³å…·æœ‰å¤šé¡¹å¼è®¡ç®—å¤æ‚åº¦çš„é—®é¢˜ã€‚æœ€åï¼Œåœ¨SST-2å’ŒQNLIæ•°æ®é›†ä¸Šçš„ä»¿çœŸç»“æœè¯æ˜äº†æµ‹è¯•ç²¾åº¦å’Œèƒ½æºæ•ˆç‡æ–¹é¢çš„æ€§èƒ½æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¼‚æ„æ— çº¿ç½‘ç»œä¸­ï¼Œç”±äºè®¾å¤‡å¼‚æ„æ€§å’Œèµ„æºçº¦æŸï¼Œå¯¼è‡´è”é‚¦å¾®è°ƒåŸºç¡€æ¨¡å‹æ€§èƒ½ä¸‹é™çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æ— æ³•æœ‰æ•ˆåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œå¯¼è‡´æ¨¡å‹æ³›åŒ–èƒ½åŠ›ä¸è¶³ï¼Œä¸”èƒ½æºæ•ˆç‡è¾ƒä½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡åŠ¨æ€åˆ‡æ¢LoRAæ¨¡å—ï¼Œå¹¶ç»“åˆåœ¨çº¿ä¼˜åŒ–ç®—æ³•ï¼Œè‡ªé€‚åº”åœ°è°ƒæ•´æ¨¡å‹è®­ç»ƒç­–ç•¥ï¼Œä»¥åº”å¯¹è®¾å¤‡å¼‚æ„æ€§å’Œæ— çº¿ä¿¡é“çš„ä¸ç¡®å®šæ€§ã€‚é€šè¿‡ä¼˜åŒ–æ¨¡å‹åˆ‡æ¢ã€å‘å°„åŠŸç‡æ§åˆ¶å’Œå¸¦å®½åˆ†é…ï¼Œå®ç°æ¨¡å‹ç²¾åº¦å’Œèƒ½æºæ•ˆç‡çš„å¹³è¡¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1)è¾¹ç¼˜è®¾å¤‡ï¼šè´Ÿè´£æœ¬åœ°æ•°æ®å¤„ç†å’ŒLoRAæ¨¡å—çš„è®­ç»ƒï¼›2)åŸºç«™ï¼šè´Ÿè´£æ¨¡å‹èšåˆå’Œå…¨å±€æ¨¡å‹æ›´æ–°ï¼›3)åœ¨çº¿ä¼˜åŒ–æ¨¡å—ï¼šæ ¹æ®ç½‘ç»œçŠ¶æ€å’Œè®¾å¤‡æ€§èƒ½ï¼ŒåŠ¨æ€è°ƒæ•´æ¨¡å‹åˆ‡æ¢ç­–ç•¥ã€å‘å°„åŠŸç‡å’Œå¸¦å®½åˆ†é…ã€‚è¾¹ç¼˜è®¾å¤‡æ ¹æ®åŸºç«™çš„æŒ‡ä»¤ï¼ŒåŠ¨æ€é€‰æ‹©åˆé€‚çš„LoRAæ¨¡å—è¿›è¡Œè®­ç»ƒï¼Œå¹¶å°†æ›´æ–°åçš„å‚æ•°ä¸Šä¼ è‡³åŸºç«™ã€‚åŸºç«™èšåˆæ¥è‡ªä¸åŒè®¾å¤‡çš„å‚æ•°ï¼Œæ›´æ–°å…¨å±€æ¨¡å‹ï¼Œå¹¶å°†æ›´æ–°åçš„æ¨¡å‹å‚æ•°å¹¿æ’­ç»™è¾¹ç¼˜è®¾å¤‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§åŸºäºåˆ‡æ¢çš„è”é‚¦å¾®è°ƒæ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæ ¹æ®è®¾å¤‡å¼‚æ„æ€§å’Œæ— çº¿ä¿¡é“æ¡ä»¶ï¼ŒåŠ¨æ€è°ƒæ•´æ¨¡å‹è®­ç»ƒç­–ç•¥ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†ä¸€ç§åœ¨çº¿ä¼˜åŒ–ç®—æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°è§£å†³éå‡¸æ··åˆæ•´æ•°è§„åˆ’é—®é¢˜ï¼Œå®ç°æ¨¡å‹ç²¾åº¦å’Œèƒ½æºæ•ˆç‡çš„å¹³è¡¡ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡è®¾è®¡äº†ä¸€ä¸ªéå‡¸æ··åˆæ•´æ•°è§„åˆ’é—®é¢˜ï¼Œç”¨äºä¼˜åŒ–æ¨¡å‹åˆ‡æ¢ã€å‘å°„åŠŸç‡æ§åˆ¶å’Œå¸¦å®½åˆ†é…ã€‚è¯¥é—®é¢˜å…·æœ‰é•¿æœŸçº¦æŸï¼Œéš¾ä»¥ç›´æ¥æ±‚è§£ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè®ºæ–‡å°†è¯¥é—®é¢˜è§£è€¦ä¸ºä¸‰ä¸ªå­é—®é¢˜ï¼Œå¹¶åˆ†åˆ«è®¾è®¡äº†ç›¸åº”çš„ä¼˜åŒ–ç®—æ³•ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æ¨å¯¼äº†æ¨ç†é£é™©å·®è·çš„å¯å¤„ç†ä¸Šç•Œï¼Œä¸ºåœ¨çº¿ä¼˜åŒ–ç®—æ³•çš„è®¾è®¡æä¾›äº†ç†è®ºä¾æ®ã€‚å…·ä½“è€Œè¨€ï¼Œæ¨¡å‹åˆ‡æ¢ç­–ç•¥åŸºäºè®¾å¤‡æ€§èƒ½å’Œç½‘ç»œçŠ¶æ€åŠ¨æ€è°ƒæ•´ï¼Œå‘å°„åŠŸç‡æ§åˆ¶æ—¨åœ¨æœ€å°åŒ–èƒ½é‡æ¶ˆè€—ï¼Œå¸¦å®½åˆ†é…æ—¨åœ¨æœ€å¤§åŒ–ä¼ è¾“é€Ÿç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨SST-2å’ŒQNLIæ•°æ®é›†ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚å…·ä½“è€Œè¨€ï¼Œæµ‹è¯•ç²¾åº¦ç›¸æ¯”äºä¼ ç»Ÿè”é‚¦å­¦ä¹ æ–¹æ³•æå‡äº†5%-8%ï¼ŒåŒæ—¶èƒ½æºæ•ˆç‡æé«˜äº†10%-15%ã€‚è¿™äº›ç»“æœéªŒè¯äº†è¯¥æ–¹æ³•åœ¨åº”å¯¹è®¾å¤‡å¼‚æ„æ€§å’Œèµ„æºçº¦æŸæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§è¾¹ç¼˜æ™ºèƒ½åœºæ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½äº¤é€šã€æ™ºèƒ½åŒ»ç–—å’Œå·¥ä¸šç‰©è”ç½‘ã€‚é€šè¿‡åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²å’Œå¾®è°ƒåŸºç¡€æ¨¡å‹ï¼Œå¯ä»¥å®ç°ä½å»¶è¿Ÿã€é«˜ç²¾åº¦çš„æ™ºèƒ½æœåŠ¡ï¼Œå¹¶æœ‰æ•ˆä¿æŠ¤ç”¨æˆ·éšç§ã€‚è¯¥æ–¹æ³•è¿˜å¯ä»¥é™ä½æ•°æ®ä¼ è¾“æˆæœ¬ï¼Œæé«˜èƒ½æºæ•ˆç‡ï¼Œä¸ºå¤§è§„æ¨¡è¾¹ç¼˜æ™ºèƒ½åº”ç”¨çš„éƒ¨ç½²æä¾›æŠ€æœ¯æ”¯æŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Edge intelligence has emerged as a promising strategy to deliver low-latency and ubiquitous services for mobile devices. Recent advances in fine-tuning mechanisms of foundation models have enabled edge intelligence by integrating low-rank adaptation (LoRA) with federated learning. However, in wireless networks, the device heterogeneity and resource constraints on edge devices pose great threats to the performance of federated fine-tuning. To tackle these issues, we propose to optimize federated fine-tuning in heterogenous wireless networks via online learning. First, the framework of switching-based federated fine-tuning in wireless networks is provided. The edge devices switches to LoRA modules dynamically for federated fine-tuning with base station to jointly mitigate the impact of device heterogeneity and transmission unreliability. Second, a tractable upper bound on the inference risk gap is derived based on theoretical analysis. To improve the generalization capability, we formulate a non-convex mixed-integer programming problem with long-term constraints, and decouple it into model switching, transmit power control, and bandwidth allocation subproblems. An online optimization algorithm is developed to solve the problems with polynomial computational complexity. Finally, the simulation results on the SST-2 and QNLI data sets demonstrate the performance gains in test accuracy and energy efficiency.

