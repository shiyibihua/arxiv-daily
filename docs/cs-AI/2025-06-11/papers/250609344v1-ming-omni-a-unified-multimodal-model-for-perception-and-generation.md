---
layout: default
title: Ming-Omni: A Unified Multimodal Model for Perception and Generation
---

# Ming-Omni: A Unified Multimodal Model for Perception and Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.09344" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.09344v1</a>
  <a href="https://arxiv.org/pdf/2506.09344.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.09344v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.09344v1', 'Ming-Omni: A Unified Multimodal Model for Perception and Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Inclusion AI, Biao Gong, Cheng Zou, Chuanyang Zheng, Chunluan Zhou, Canxiang Yan, Chunxiang Jin, Chunjie Shen, Dandan Zheng, Fudong Wang, Furong Xu, GuangMing Yao, Jun Zhou, Jingdong Chen, Jianxin Sun, Jiajia Liu, Jianjiang Zhu, Jun Peng, Kaixiang Ji, Kaiyou Song, Kaimeng Ren, Libin Wang, Lixiang Ru, Lele Xie, Longhua Tan, Lyuxin Xue, Lan Wang, Mochen Bai, Ning Gao, Pei Chen, Qingpei Guo, Qinglong Zhang, Qiang Xu, Rui Liu, Ruijie Xiong, Sirui Gao, Tinghao Liu, Taisong Li, Weilong Chai, Xinyu Xiao, Xiaomei Wang, Xiaoxue Chen, Xiao Lu, Xiaoyu Li, Xingning Dong, Xuzheng Yu, Yi Yuan, Yuting Gao, Yunxiao Sun, Yipeng Chen, Yifei Wu, Yongjie Lyu, Ziping Ma, Zipeng Feng, Zhijiang Fang, Zhihao Qiu, Ziyuan Huang, Zhengyu He

**åˆ†ç±»**: cs.AI, cs.CL, cs.CV, cs.LG, cs.SD, eess.AS

**å‘å¸ƒæ—¥æœŸ**: 2025-06-11

**å¤‡æ³¨**: 18 pages,8 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMing-Omniä»¥è§£å†³å¤šæ¨¡æ€å¤„ç†ä¸ç”Ÿæˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ¨¡å‹` `è¯­éŸ³ç”Ÿæˆ` `å›¾åƒç”Ÿæˆ` `ä¸Šä¸‹æ–‡æ„ŸçŸ¥` `ç»Ÿä¸€æ¡†æ¶` `é«˜æ•ˆèåˆ` `å¼€æºæ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ¨¡æ€æ¨¡å‹åœ¨å¤„ç†ä¸åŒæ¨¡æ€æ•°æ®æ—¶é€šå¸¸éœ€è¦å¤šä¸ªæ¨¡å‹ï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹å’Œèµ„æºæµªè´¹ã€‚
2. Ming-Omnié€šè¿‡ç»Ÿä¸€çš„æ¡†æ¶å’Œä¸“ç”¨ç¼–ç å™¨ï¼Œèƒ½å¤ŸåŒæ—¶å¤„ç†å›¾åƒã€æ–‡æœ¬ã€éŸ³é¢‘å’Œè§†é¢‘ï¼Œç®€åŒ–äº†å¤šæ¨¡æ€ä»»åŠ¡çš„æ‰§è¡Œã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMing-Omniåœ¨å¤šæ¨¡æ€ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨è¯­éŸ³å’Œå›¾åƒç”Ÿæˆæ–¹é¢å…·æœ‰æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬æå‡ºäº†Ming-Omniï¼Œä¸€ä¸ªç»Ÿä¸€çš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œèƒ½å¤Ÿå¤„ç†å›¾åƒã€æ–‡æœ¬ã€éŸ³é¢‘å’Œè§†é¢‘ï¼Œå¹¶åœ¨è¯­éŸ³å’Œå›¾åƒç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ã€‚Ming-Omnié‡‡ç”¨ä¸“ç”¨ç¼–ç å™¨ä»ä¸åŒæ¨¡æ€ä¸­æå–æ ‡è®°ï¼Œéšåé€šè¿‡é…å¤‡æ–°æå‡ºçš„æ¨¡æ€ç‰¹å®šè·¯ç”±å™¨çš„MoEæ¶æ„Lingè¿›è¡Œå¤„ç†ã€‚è¿™ä¸€è®¾è®¡ä½¿å¾—å•ä¸€æ¨¡å‹èƒ½å¤Ÿé«˜æ•ˆå¤„ç†å’Œèåˆå¤šæ¨¡æ€è¾“å…¥ï¼Œæ”¯æŒå¤šç§ä»»åŠ¡è€Œæ— éœ€å•ç‹¬æ¨¡å‹ã€ä»»åŠ¡ç‰¹å®šå¾®è°ƒæˆ–ç»“æ„é‡è®¾è®¡ã€‚Ming-Omniè¶…è¶Šäº†ä¼ ç»Ÿå¤šæ¨¡æ€æ¨¡å‹ï¼Œæ”¯æŒéŸ³é¢‘å’Œå›¾åƒç”Ÿæˆï¼Œé›†æˆäº†å…ˆè¿›çš„éŸ³é¢‘è§£ç å™¨å’Œé«˜è´¨é‡å›¾åƒç”Ÿæˆæ¨¡å—ï¼Œèƒ½å¤Ÿè¿›è¡Œä¸Šä¸‹æ–‡æ„ŸçŸ¥èŠå¤©ã€æ–‡æœ¬è½¬è¯­éŸ³å’Œå¤šæ ·åŒ–å›¾åƒç¼–è¾‘ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMing-Omniä¸ºç»Ÿä¸€æ„ŸçŸ¥å’Œç”Ÿæˆæä¾›äº†å¼ºæœ‰åŠ›çš„è§£å†³æ–¹æ¡ˆã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒMing-Omniæ˜¯æˆ‘ä»¬æ‰€çŸ¥çš„ç¬¬ä¸€ä¸ªåœ¨æ¨¡æ€æ”¯æŒä¸Šä¸GPT-4oç›¸åŒ¹é…çš„å¼€æºæ¨¡å‹ï¼Œæˆ‘ä»¬å‘å¸ƒäº†æ‰€æœ‰ä»£ç å’Œæ¨¡å‹æƒé‡ï¼Œä»¥é¼“åŠ±ç¤¾åŒºè¿›ä¸€æ­¥ç ”ç©¶å’Œå¼€å‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤šæ¨¡æ€æ¨¡å‹åœ¨å¤„ç†å’Œç”Ÿæˆå¤šç§æ¨¡æ€æ•°æ®æ—¶çš„æ•ˆç‡ä½ä¸‹å’Œå¤æ‚æ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦å¤šä¸ªä¸“ç”¨æ¨¡å‹ï¼Œå¯¼è‡´èµ„æºæµªè´¹å’Œç»´æŠ¤å›°éš¾ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMing-Omniçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ä¸€ä¸ªç»Ÿä¸€çš„å¤šæ¨¡æ€æ¨¡å‹æ¥å¤„ç†ä¸åŒç±»å‹çš„æ•°æ®ï¼Œé‡‡ç”¨ä¸“ç”¨ç¼–ç å™¨æå–ç‰¹å¾ï¼Œå¹¶é€šè¿‡MoEæ¶æ„è¿›è¡Œé«˜æ•ˆèåˆã€‚è¿™ç§è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸éœ€è¦é¢å¤–å¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œæ”¯æŒå¤šç§ä»»åŠ¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMing-Omniçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªä¸“ç”¨ç¼–ç å™¨ç”¨äºä¸åŒæ¨¡æ€çš„è¾“å…¥ï¼Œéšåé€šè¿‡Lingæ¨¡å—è¿›è¡Œå¤„ç†ã€‚Lingæ¨¡å—é‡‡ç”¨äº†æ–°æå‡ºçš„æ¨¡æ€ç‰¹å®šè·¯ç”±å™¨ï¼Œèƒ½å¤Ÿæ ¹æ®è¾“å…¥çš„æ¨¡æ€åŠ¨æ€é€‰æ‹©å¤„ç†è·¯å¾„ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„å¤šæ¨¡æ€èåˆã€‚

**å…³é”®åˆ›æ–°**ï¼šMing-Omniçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶ç»Ÿä¸€çš„å¤šæ¨¡æ€å¤„ç†èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯éŸ³é¢‘å’Œå›¾åƒç”Ÿæˆçš„é›†æˆã€‚è¿™ä½¿å¾—æ¨¡å‹ä¸ä»…èƒ½è¿›è¡Œä¼ ç»Ÿçš„æ–‡æœ¬å¤„ç†ï¼Œè¿˜èƒ½è¿›è¡Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„èŠå¤©å’Œå¤šæ ·åŒ–çš„å›¾åƒç¼–è¾‘ï¼Œè¶…è¶Šäº†ç°æœ‰çš„å¤šæ¨¡æ€æ¨¡å‹ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼ŒMing-Omnié‡‡ç”¨äº†å…ˆè¿›çš„éŸ³é¢‘è§£ç å™¨ä»¥ç”Ÿæˆè‡ªç„¶çš„è¯­éŸ³ï¼Œå¹¶ç»“åˆMing-Lite-Uniè¿›è¡Œé«˜è´¨é‡å›¾åƒç”Ÿæˆã€‚æ¨¡å‹çš„æŸå¤±å‡½æ•°å’Œå‚æ•°è®¾ç½®ç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥ç¡®ä¿åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­å®ç°æœ€ä½³æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMing-Omniåœ¨å¤šæ¨¡æ€ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å“è¶Šï¼Œå°¤å…¶åœ¨è¯­éŸ³ç”Ÿæˆå’Œå›¾åƒç”Ÿæˆæ–¹é¢ï¼Œä¸ç°æœ‰åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œæ€§èƒ½æå‡æ˜¾è‘—ã€‚å…·ä½“è€Œè¨€ï¼ŒMing-Omniåœ¨ç”Ÿæˆè‡ªç„¶è¯­éŸ³çš„å‡†ç¡®æ€§å’Œå›¾åƒè´¨é‡ä¸Šå‡è¾¾åˆ°äº†æ–°çš„é«˜åº¦ï¼Œå±•ç¤ºäº†å…¶å¼ºå¤§çš„å¤šæ¨¡æ€å¤„ç†èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Ming-Omniçš„æ½œåœ¨åº”ç”¨åœºæ™¯åŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€å†…å®¹åˆ›ä½œã€æ•™è‚²åŸ¹è®­ç­‰é¢†åŸŸã€‚å…¶ç»Ÿä¸€çš„å¤šæ¨¡æ€å¤„ç†èƒ½åŠ›ä½¿å¾—ç”¨æˆ·èƒ½å¤Ÿåœ¨ä¸åŒç±»å‹çš„æ•°æ®ä¹‹é—´æ— ç¼åˆ‡æ¢ï¼Œæå‡äº†äº¤äº’ä½“éªŒå’Œå·¥ä½œæ•ˆç‡ã€‚æœªæ¥ï¼ŒMing-Omniæœ‰æœ›åœ¨æ›´å¤šå®é™…åº”ç”¨ä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œæ¨åŠ¨å¤šæ¨¡æ€æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We propose Ming-Omni, a unified multimodal model capable of processing images, text, audio, and video, while demonstrating strong proficiency in both speech and image generation. Ming-Omni employs dedicated encoders to extract tokens from different modalities, which are then processed by Ling, an MoE architecture equipped with newly proposed modality-specific routers. This design enables a single model to efficiently process and fuse multimodal inputs within a unified framework, thereby facilitating diverse tasks without requiring separate models, task-specific fine-tuning, or structural redesign. Importantly, Ming-Omni extends beyond conventional multimodal models by supporting audio and image generation. This is achieved through the integration of an advanced audio decoder for natural-sounding speech and Ming-Lite-Uni for high-quality image generation, which also allow the model to engage in context-aware chatting, perform text-to-speech conversion, and conduct versatile image editing. Our experimental results showcase Ming-Omni offers a powerful solution for unified perception and generation across all modalities. Notably, our proposed Ming-Omni is the first open-source model we are aware of to match GPT-4o in modality support, and we release all code and model weights to encourage further research and development in the community.

