---
layout: default
title: Intent Factored Generation: Unleashing the Diversity in Your Language Model
---

# Intent Factored Generation: Unleashing the Diversity in Your Language Model

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.09659" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.09659v1</a>
  <a href="https://arxiv.org/pdf/2506.09659.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.09659v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.09659v1', 'Intent Factored Generation: Unleashing the Diversity in Your Language Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Eltayeb Ahmed, Uljad Berdica, Martha Elliott, Danijela Horak, Jakob N. Foerster

**åˆ†ç±»**: cs.AI, cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-11

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ„å›¾åˆ†è§£ç”Ÿæˆæ–¹æ³•ä»¥è§£å†³è¯­è¨€æ¨¡å‹æ ·æœ¬å¤šæ ·æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­è¨€æ¨¡å‹` `ç”Ÿæˆæ¨¡å‹` `å¤šæ ·æ€§å¢å¼º` `æ„å›¾åˆ†è§£` `å¯¹è¯ç³»ç»Ÿ` `å¼ºåŒ–å­¦ä¹ ` `å†…å®¹ç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨ç”Ÿæˆå¤šæ ·åŒ–æ ·æœ¬æ—¶ä»…åœ¨è¯å…ƒçº§åˆ«è¿›è¡Œæ“ä½œï¼Œå¯¼è‡´ç”Ÿæˆçš„å†…å®¹ç¼ºä¹æ¢ç´¢æ€§å’Œå¸å¼•åŠ›ã€‚
2. æœ¬æ–‡æå‡ºçš„æ„å›¾åˆ†è§£ç”Ÿæˆï¼ˆIFGï¼‰æ–¹æ³•é€šè¿‡å°†é‡‡æ ·è¿‡ç¨‹åˆ†ä¸ºæ„å›¾é‡‡æ ·å’Œæœ€ç»ˆå“åº”ç”Ÿæˆä¸¤ä¸ªé˜¶æ®µï¼Œæå‡äº†ç”Ÿæˆçš„å¤šæ ·æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨æ•°å­¦å’Œä»£ç ä»»åŠ¡ä¸Šæé«˜äº†pass@kå’ŒåŸºäºåé¦ˆçš„å¼ºåŒ–å­¦ä¹ æ•ˆæœï¼ŒåŒæ—¶åœ¨å¯¹è¯ç”Ÿæˆä¸­ä¿æŒäº†é«˜è´¨é‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä»å¤§å‹è¯­è¨€æ¨¡å‹ä¸­è·å–å¤šä¸ªæœ‰æ„ä¹‰ä¸”å¤šæ ·åŒ–çš„é«˜è´¨é‡æ ·æœ¬ä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾æ€§æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä»…åœ¨è¯å…ƒçº§åˆ«ä¸Šå¢åŠ å¤šæ ·æ€§ï¼Œå¯¼è‡´ç”Ÿæˆçš„å“åº”ç¼ºä¹æ¢ç´¢æ€§å’Œå¸å¼•åŠ›ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†æ„å›¾åˆ†è§£ç”Ÿæˆï¼ˆIFGï¼‰æ–¹æ³•ï¼Œå°†é‡‡æ ·è¿‡ç¨‹åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆé‡‡æ ·è¯­ä¹‰å¯†é›†çš„æ„å›¾ï¼Œç„¶ååœ¨æ­¤åŸºç¡€ä¸Šç”Ÿæˆæœ€ç»ˆå“åº”ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œèƒ½å¤Ÿåœ¨æ„å›¾é˜¶æ®µä½¿ç”¨è¾ƒé«˜çš„æ¸©åº¦ä»¥ä¿ƒè¿›æ¦‚å¿µå¤šæ ·æ€§ï¼Œè€Œåœ¨æœ€ç»ˆç”Ÿæˆé˜¶æ®µä½¿ç”¨è¾ƒä½çš„æ¸©åº¦ä»¥ç¡®ä¿è¾“å‡ºçš„ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§ä»»åŠ¡ä¸Šå‡è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—æé«˜äº†ç”Ÿæˆçš„å¤šæ ·æ€§å’Œè´¨é‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä»å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ç”Ÿæˆå¤šæ ·åŒ–é«˜è´¨é‡æ ·æœ¬çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€åœ¨è¯å…ƒçº§åˆ«è¿›è¡Œå¤šæ ·æ€§å¢å¼ºï¼Œå¯¼è‡´ç”Ÿæˆå†…å®¹çš„é‡å¤æ€§å’Œç¼ºä¹æ·±åº¦ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„æ„å›¾åˆ†è§£ç”Ÿæˆï¼ˆIFGï¼‰æ–¹æ³•é€šè¿‡å°†ç”Ÿæˆè¿‡ç¨‹åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼Œé¦–å…ˆç”Ÿæˆè¯­ä¹‰æ„å›¾ï¼Œç„¶ååŸºäºè¯¥æ„å›¾ç”Ÿæˆæœ€ç»ˆå“åº”ï¼Œä»è€Œæé«˜ç”Ÿæˆå†…å®¹çš„å¤šæ ·æ€§å’Œä¸€è‡´æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šIFGæ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šæ„å›¾é‡‡æ ·æ¨¡å—å’Œå“åº”ç”Ÿæˆæ¨¡å—ã€‚åœ¨æ„å›¾é‡‡æ ·é˜¶æ®µï¼Œä½¿ç”¨è¾ƒé«˜çš„æ¸©åº¦ä»¥ä¿ƒè¿›å¤šæ ·æ€§ï¼›åœ¨å“åº”ç”Ÿæˆé˜¶æ®µï¼Œä½¿ç”¨è¾ƒä½çš„æ¸©åº¦ä»¥ç¡®ä¿è¾“å‡ºçš„è¿è´¯æ€§å’Œä¸€è‡´æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•çš„åˆ›æ–°åœ¨äºå°†ç”Ÿæˆè¿‡ç¨‹åˆ†è§£ä¸ºæ„å›¾å’Œå“åº”ä¸¤ä¸ªé˜¶æ®µï¼Œè¿™ä¸€è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨ä¿æŒç”Ÿæˆè´¨é‡çš„åŒæ—¶ï¼Œæ˜¾è‘—æé«˜æ ·æœ¬çš„å¤šæ ·æ€§ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒIFGèƒ½å¤Ÿæ›´å¥½åœ°æ¢ç´¢ä¸åŒçš„æ¦‚å¿µå’Œæ€è·¯ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å…·ä½“å®ç°ä¸­ï¼Œæ¨¡å‹åœ¨æ¯ä¸ªç”Ÿæˆæ­¥éª¤ä¹‹å‰è¢«æç¤ºæ˜ç¡®å…¶æ„å›¾ï¼Œè¿™å¯¹äºæ¨ç†ä»»åŠ¡å°¤ä¸ºé‡è¦ã€‚æ­¤å¤–ï¼Œç»“åˆç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰è¿›è¡ŒæŒ‡ä»¤è°ƒä¼˜ï¼Œä»¥å¢å¼ºå¯¹è¯çš„å¤šæ ·æ€§è€Œä¸ç‰ºç‰²å¥–åŠ±ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒIFGæ–¹æ³•åœ¨æ•°å­¦å’Œä»£ç ä»»åŠ¡ä¸Šæ˜¾è‘—æé«˜äº†pass@kå’ŒåŸºäºåé¦ˆçš„å¼ºåŒ–å­¦ä¹ æ•ˆæœã€‚æ­¤å¤–ï¼Œåœ¨å¯¹è¯ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œè¯¥æ–¹æ³•ç»“åˆç›´æ¥åå¥½ä¼˜åŒ–ï¼ŒæˆåŠŸæå‡äº†ç”Ÿæˆçš„å¤šæ ·æ€§ï¼ŒåŒæ—¶ä¿æŒäº†é«˜è´¨é‡çš„è¾“å‡ºã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½å¯¹è¯ç³»ç»Ÿã€å†…å®¹ç”Ÿæˆå’Œæ•™è‚²è¾…åŠ©å·¥å…·ç­‰ã€‚é€šè¿‡æé«˜ç”Ÿæˆå†…å®¹çš„å¤šæ ·æ€§å’Œè´¨é‡ï¼ŒIFGæ–¹æ³•èƒ½å¤Ÿæå‡ç”¨æˆ·ä½“éªŒï¼Œå¢å¼ºäººæœºäº¤äº’çš„è‡ªç„¶æ€§å’Œæœ‰æ•ˆæ€§ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•è¿˜å¯èƒ½åœ¨å…¶ä»–ç”Ÿæˆä»»åŠ¡ä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œæ¨åŠ¨è¯­è¨€æ¨¡å‹çš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Obtaining multiple meaningfully diverse, high quality samples from Large Language Models for a fixed prompt remains an open challenge. Current methods for increasing diversity often only operate at the token-level, paraphrasing the same response. This is problematic because it leads to poor exploration on reasoning problems and to unengaging, repetitive conversational agents. To address this we propose Intent Factored Generation (IFG), factorising the sampling process into two stages. First, we sample a semantically dense intent, e.g., a summary or keywords. Second, we sample the final response conditioning on both the original prompt and the intent from the first stage. This allows us to use a higher temperature during the intent step to promote conceptual diversity, and a lower temperature during the final generation to ensure the outputs are coherent and self-consistent. Additionally, we find that prompting the model to explicitly state its intent for each step of the chain-of-thought before generating the step is beneficial for reasoning tasks. We demonstrate our method's effectiveness across a diverse set of tasks. We show this method improves both pass@k and Reinforcement Learning from Verifier Feedback on maths and code tasks. For instruction-tuning, we combine IFG with Direct Preference Optimisation to increase conversational diversity without sacrificing reward. Finally, we achieve higher diversity while maintaining the quality of generations on a general language modelling task, using a new dataset of reader comments and news articles that we collect and open-source. In summary, we present a simple method of increasing the sample diversity of LLMs while maintaining performance. This method can be implemented by changing the prompt and varying the temperature during generation, making it easy to integrate into many algorithms for gains across various applications.

