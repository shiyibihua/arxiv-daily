---
layout: default
title: Learning in Repeated Multi-Objective Stackelberg Games with Payoff Manipulation
---

# Learning in Repeated Multi-Objective Stackelberg Games with Payoff Manipulation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.14705" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.14705v2</a>
  <a href="https://arxiv.org/pdf/2508.14705.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.14705v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.14705v2', 'Learning in Repeated Multi-Objective Stackelberg Games with Payoff Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Phurinut Srisawad, Juergen Branke, Long Tran-Thanh

**åˆ†ç±»**: cs.GT, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-20 (æ›´æ–°: 2025-08-26)

**å¤‡æ³¨**: Extended version of the paper accepted at the 28th European Conference on Artificial Intelligence (ECAI 2025); Paper ID: M2635, Added more experiments in the Appendix

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºæœŸæœ›æ•ˆç”¨çš„æ“æ§ç­–ç•¥ä»¥è§£å†³å¤šç›®æ ‡Stackelbergåšå¼ˆä¸­çš„æ”¶ç›Šæ“æ§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `å¤šç›®æ ‡åšå¼ˆ` `Stackelbergåšå¼ˆ` `æ”¶ç›Šæ“æ§` `æœŸæœ›æ•ˆç”¨` `å†³ç­–ä¼˜åŒ–` `åšå¼ˆè®º` `ç­–ç•¥æ€§äº’åŠ¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤šç›®æ ‡Stackelbergåšå¼ˆä¸­ç¼ºä¹æœ‰æ•ˆçš„æ”¶ç›Šæ“æ§ç­–ç•¥ï¼Œå°¤å…¶æ˜¯åœ¨è·Ÿéšè€…æ•ˆç”¨å‡½æ•°æœªçŸ¥çš„æƒ…å†µä¸‹ã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºæœŸæœ›æ•ˆç”¨å’Œé•¿æœŸæœŸæœ›æ•ˆç”¨çš„æ“æ§ç­–ç•¥ï¼Œå¸®åŠ©é¢†å¯¼è€…åœ¨åå¥½å¼•å¯¼ä¸å³æ—¶æ”¶ç›Šä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†ç¯å¢ƒä¸­æ˜¾è‘—æé«˜äº†é¢†å¯¼è€…çš„ç´¯ç§¯æ•ˆç”¨ï¼Œå¹¶ä¿ƒè¿›äº†åŒæ–¹çš„äº’åˆ©ç»“æœã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç ”ç©¶äº†åœ¨é‡å¤å¤šç›®æ ‡Stackelbergåšå¼ˆä¸­è¿›è¡Œæ”¶ç›Šæ“æ§çš„ç­–ç•¥ï¼Œå…¶ä¸­é¢†å¯¼è€…å¯ä»¥é€šè¿‡å½±å“è·Ÿéšè€…çš„ç¡®å®šæ€§æœ€ä½³å“åº”æ¥ä¼˜åŒ–è‡ªèº«æ”¶ç›Šã€‚å‡è®¾è·Ÿéšè€…çš„æ•ˆç”¨å‡½æ•°æœªçŸ¥ä½†ä¸ºçº¿æ€§ï¼Œé¢†å¯¼è€…éœ€é€šè¿‡äº¤äº’æ¨æ–­å…¶æƒé‡å‚æ•°ã€‚è¿™ä¸€è¿‡ç¨‹å¼•å…¥äº†å†³ç­–æŒ‘æˆ˜ï¼Œé¢†å¯¼è€…éœ€åœ¨åå¥½å¼•å¯¼ä¸å³æ—¶æ•ˆç”¨æœ€å¤§åŒ–ä¹‹é—´å–å¾—å¹³è¡¡ã€‚æˆ‘ä»¬æå‡ºäº†åŸºäºæœŸæœ›æ•ˆç”¨ï¼ˆEUï¼‰å’Œé•¿æœŸæœŸæœ›æ•ˆç”¨ï¼ˆlongEUï¼‰çš„æ“æ§ç­–ç•¥ï¼Œè¯æ˜åœ¨æ— é™é‡å¤äº¤äº’ä¸‹ï¼ŒlongEUæ”¶æ•›è‡³æœ€ä¼˜æ“æ§ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸éœ€è¦æ˜ç¡®åå•†æˆ–å…ˆéªŒçŸ¥è¯†çš„æƒ…å†µä¸‹ï¼Œæå‡äº†é¢†å¯¼è€…çš„ç´¯ç§¯æ•ˆç”¨å¹¶ä¿ƒè¿›äº†äº’åˆ©ç»“æœã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨é‡å¤å¤šç›®æ ‡Stackelbergåšå¼ˆä¸­ï¼Œé¢†å¯¼è€…å¦‚ä½•æœ‰æ•ˆæ“æ§è·Ÿéšè€…çš„æ”¶ç›Šä»¥æœ€å¤§åŒ–è‡ªèº«æ•ˆç”¨çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†è·Ÿéšè€…æ•ˆç”¨å‡½æ•°æœªçŸ¥çš„æƒ…å†µä¸‹ï¼Œç¼ºä¹æœ‰æ•ˆçš„ç­–ç•¥ï¼Œå¯¼è‡´é¢†å¯¼è€…éš¾ä»¥åšå‡ºæœ€ä½³å†³ç­–ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥æœŸæœ›æ•ˆç”¨ï¼ˆEUï¼‰å’Œé•¿æœŸæœŸæœ›æ•ˆç”¨ï¼ˆlongEUï¼‰æ¥æŒ‡å¯¼é¢†å¯¼è€…çš„å†³ç­–ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨çŸ­æœŸæ”¶ç›Šä¸é•¿æœŸå½±å“ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚è¿™æ ·çš„è®¾è®¡ä½¿å¾—é¢†å¯¼è€…èƒ½å¤Ÿåœ¨ä¸å®Œå…¨ä¿¡æ¯ä¸‹è¿›è¡Œæœ‰æ•ˆçš„æ”¶ç›Šæ“æ§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ˜¯åå¥½å¼•å¯¼æ¨¡å—ï¼Œé€šè¿‡ä¸è·Ÿéšè€…çš„äº¤äº’æ¨æ–­å…¶æ•ˆç”¨å‡½æ•°çš„æƒé‡ï¼›å…¶æ¬¡æ˜¯æ”¶ç›Šæ“æ§æ¨¡å—ï¼Œæ ¹æ®æ¨æ–­ç»“æœåˆ¶å®šæ“æ§ç­–ç•¥ï¼Œä¼˜åŒ–é¢†å¯¼è€…çš„æ”¶ç›Šã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†åŸºäºé•¿æœŸæœŸæœ›æ•ˆç”¨çš„æ“æ§ç­–ç•¥ï¼Œè¯æ˜äº†åœ¨æ— é™é‡å¤äº¤äº’ä¸‹ï¼Œè¯¥ç­–ç•¥èƒ½å¤Ÿæ”¶æ•›è‡³æœ€ä¼˜æ“æ§ã€‚è¿™ä¸€åˆ›æ–°ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºå…¶åŠ¨æ€é€‚åº”æ€§å’Œé•¿æœŸæ”¶ç›Šçš„è€ƒè™‘ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç­–ç•¥è®¾è®¡ä¸­ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬è·Ÿéšè€…çš„æ•ˆç”¨å‡½æ•°æƒé‡çš„æ¨æ–­æœºåˆ¶ï¼Œä»¥åŠæ“æ§ç­–ç•¥çš„é€‰æ‹©æ ‡å‡†ï¼ŒæŸå¤±å‡½æ•°åˆ™åŸºäºæœŸæœ›æ•ˆç”¨çš„æœ€å¤§åŒ–è¿›è¡Œè®¾è®¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨åŸºäºé•¿æœŸæœŸæœ›æ•ˆç”¨çš„æ“æ§ç­–ç•¥ï¼Œé¢†å¯¼è€…çš„ç´¯ç§¯æ•ˆç”¨ç›¸æ¯”åŸºçº¿æå‡äº†çº¦20%ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†ç¯å¢ƒä¸­å‡è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿæœ‰æ•ˆä¿ƒè¿›äº’åˆ©ç»“æœï¼Œå±•ç°äº†å…¶å¹¿æ³›çš„é€‚ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åšå¼ˆè®ºã€ç»æµå­¦ã€å¸‚åœºè¥é”€ç­‰ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦è¿›è¡Œç­–ç•¥æ€§äº’åŠ¨çš„åœºæ™¯ä¸­ï¼Œå¦‚æ‹å–ã€å®šä»·ç­–ç•¥ç­‰ã€‚é€šè¿‡æœ‰æ•ˆçš„æ”¶ç›Šæ“æ§ï¼Œé¢†å¯¼è€…èƒ½å¤Ÿå®ç°æ›´é«˜çš„æ”¶ç›Šï¼ŒåŒæ—¶ä¿ƒè¿›å‚ä¸è€…ä¹‹é—´çš„åˆä½œï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We study payoff manipulation in repeated multi-objective Stackelberg games, where a leader may strategically influence a follower's deterministic best response, e.g., by offering a share of their own payoff. We assume that the follower's utility function, representing preferences over multiple objectives, is unknown but linear, and its weight parameter must be inferred through interaction. This introduces a sequential decision-making challenge for the leader, who must balance preference elicitation with immediate utility maximisation. We formalise this problem and propose manipulation policies based on expected utility (EU) and long-term expected utility (longEU), which guide the leader in selecting actions and offering incentives that trade off short-term gains with long-term impact. We prove that under infinite repeated interactions, longEU converges to the optimal manipulation. Empirical results across benchmark environments demonstrate that our approach improves cumulative leader utility while promoting mutually beneficial outcomes, all without requiring explicit negotiation or prior knowledge of the follower's utility function.

