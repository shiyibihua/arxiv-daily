---
layout: default
title: MoEcho: Exploiting Side-Channel Attacks to Compromise User Privacy in Mixture-of-Experts LLMs
---

# MoEcho: Exploiting Side-Channel Attacks to Compromise User Privacy in Mixture-of-Experts LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.15036" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.15036v1</a>
  <a href="https://arxiv.org/pdf/2508.15036.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.15036v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.15036v1', 'MoEcho: Exploiting Side-Channel Attacks to Compromise User Privacy in Mixture-of-Experts LLMs')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Ruyi Ding, Tianhong Xu, Xinyi Shen, Aidong Adam Ding, Yunsi Fei

**ÂàÜÁ±ª**: cs.CR, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-20

**Â§áÊ≥®**: This paper will appear in CCS 2025

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫MoEcho‰ª•Ëß£ÂÜ≥Mixture-of-ExpertsÊ®°Âûã‰∏≠ÁöÑÁî®Êà∑ÈöêÁßÅÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Ê∑∑Âêà‰∏ìÂÆ∂` `‰æß‰ø°ÈÅìÊîªÂáª` `Áî®Êà∑ÈöêÁßÅ` `ÂÆâÂÖ®ÂàÜÊûê` `Ê∑±Â∫¶Â≠¶‰π†` `ÂèòÊç¢Âô®Êû∂ÊûÑ` `Â§ßËßÑÊ®°AIÊúçÂä°`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑMixture-of-ExpertsÊ®°ÂûãÂú®ÈöêÁßÅ‰øùÊä§ÊñπÈù¢Â≠òÂú®‰∏•ÈáçÊºèÊ¥ûÔºåÂ∞§ÂÖ∂ÊòØÂú®Âä®ÊÄÅË∑ØÁî±Êú∫Âà∂‰∏ãÔºåÊîªÂáªËÄÖÂèØ‰ª•Âà©Áî®ËæìÂÖ•‰æùËµñÁöÑÊøÄÊ¥ªÊ®°ÂºèËøõË°å‰æß‰ø°ÈÅìÊîªÂáª„ÄÇ
2. ËÆ∫ÊñáÊèêÂá∫MoEchoÔºåÈÄöËøáÂèëÁé∞Êñ∞ÁöÑÊû∂ÊûÑ‰æß‰ø°ÈÅìÔºåÊè≠Á§∫‰∫ÜMoEÊ®°ÂûãÂú®‰∏çÂêåËÆ°ÁÆóÂπ≥Âè∞‰∏äÁöÑÈöêÁßÅÊ≥ÑÈú≤È£éÈô©ÔºåÂπ∂ÊèêÂá∫‰∫ÜÂõõÁßçÂÖ∑‰ΩìÁöÑÊîªÂáªÊñπÊ≥ï„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMoEchoËÉΩÂ§üÊúâÊïàÂú∞Âà©Áî®Ëøô‰∫õ‰æß‰ø°ÈÅìËøõË°åÁî®Êà∑ÈöêÁßÅÊîªÂáªÔºåÂº∫Ë∞É‰∫ÜÂú®‰ΩøÁî®MoEÊû∂ÊûÑÊó∂ÈúÄË¶ÅÂä†Âº∫ÂÆâÂÖ®Èò≤Êä§Êé™ÊñΩ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÂèòÊç¢Âô®Êû∂ÊûÑÂ∑≤Êàê‰∏∫Áé∞‰ª£‰∫∫Â∑•Êô∫ËÉΩÁöÑÂü∫Áü≥ÔºåÊé®Âä®‰∫ÜËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ„ÄÅËÆ°ÁÆóÊú∫ËßÜËßâÂíåÂ§öÊ®°ÊÄÅÂ≠¶‰π†Á≠âÂ∫îÁî®ÁöÑÊòæËëóËøõÂ±ï„ÄÇÈöèÁùÄËøô‰∫õÊ®°ÂûãÂú®ÊÄßËÉΩ‰∏äÁöÑÁàÜÁÇ∏ÊÄßÂ¢ûÈïøÔºåÂÆûÊñΩÊïàÁéá‰ªçÁÑ∂ÊòØ‰∏Ä‰∏™ÂÖ≥ÈîÆÊåëÊàò„ÄÇÊ∑∑Âêà‰∏ìÂÆ∂ÔºàMoEÔºâÊû∂ÊûÑÈÄöËøáÈÄâÊã©ÊÄßÊøÄÊ¥ª‰∏ìÈó®ÁöÑÂ≠êÁΩëÁªúÔºà‰∏ìÂÆ∂ÔºâÔºåÂú®Ê®°ÂûãÂáÜÁ°ÆÊÄßÂíåËÆ°ÁÆóÊàêÊú¨‰πãÈó¥Êèê‰æõ‰∫ÜÁã¨ÁâπÁöÑÂπ≥Ë°°„ÄÇÁÑ∂ËÄåÔºåMoEÊû∂ÊûÑ‰∏≠ÁöÑËá™ÈÄÇÂ∫îË∑ØÁî±Êú∫Âà∂Êó†ÊÑè‰∏≠‰∏∫ÈöêÁßÅÊ≥ÑÈú≤ÊâìÂºÄ‰∫ÜÊñ∞ÁöÑÊîªÂáªÈù¢„ÄÇÊú¨ÊñáÊèêÂá∫MoEchoÔºåÂèëÁé∞‰∫Ü‰∏ÄÁßç‰æß‰ø°ÈÅìÂàÜÊûêÊîªÂáªÈù¢ÔºåËÉΩÂ§üÂú®Âü∫‰∫éMoEÁöÑÁ≥ªÁªü‰∏≠Âç±ÂÆ≥Áî®Êà∑ÈöêÁßÅ„ÄÇÊàë‰ª¨‰ªãÁªç‰∫ÜÂõõÁßçÊñ∞ÂûãÁöÑÊû∂ÊûÑ‰æß‰ø°ÈÅìÔºåÂàÜÂà´Âú®‰∏çÂêåËÆ°ÁÆóÂπ≥Âè∞‰∏äÔºåÂåÖÊã¨CPUÁöÑÁºìÂ≠òÂç†Áî®ÈÄöÈÅìÂíåÈ°µÈù¢ÁΩÆÊç¢+ÈáçËΩΩÔºå‰ª•ÂèäGPUÁöÑÊÄßËÉΩËÆ°Êï∞Âô®ÂíåTLBÈ©±ÈÄê+ÈáçËΩΩ„ÄÇÈÄöËøáÂà©Áî®Ëøô‰∫õÊºèÊ¥ûÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÂõõÁßçÊúâÊïà‰æµÁäØÁî®Êà∑ÈöêÁßÅÁöÑÊîªÂáªÊñπÊ≥ïÔºåÂº∫Ë∞É‰∫ÜÂú®ÂºÄÂèëÈ´òÊïàÁöÑÂ§ßËßÑÊ®°AIÊúçÂä°Êó∂ÔºåÈíàÂØπMoEÊ®°ÂûãÁöÑÂÆâÂÖ®ÂíåÈöêÁßÅÂ®ÅËÉÅÈúÄË¶ÅÂèäÊó∂ÊúâÊïàÁöÑÈò≤Êä§„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Mixture-of-ExpertsÊ®°Âûã‰∏≠Áî±‰∫éËá™ÈÄÇÂ∫îË∑ØÁî±Êú∫Âà∂ÂØºËá¥ÁöÑÁî®Êà∑ÈöêÁßÅÊ≥ÑÈú≤ÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÊú™ËÉΩÊúâÊïàËØÜÂà´ÂíåÈò≤ËåÉËøô‰∫õ‰æß‰ø°ÈÅìÊîªÂáªÔºåÈÄ†ÊàêÁî®Êà∑Êï∞ÊçÆÁöÑÊΩúÂú®È£éÈô©„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáËØÜÂà´ÂíåÂà©Áî®MoEÊû∂ÊûÑ‰∏≠ÁöÑ‰æß‰ø°ÈÅìÔºåÊè≠Á§∫ÂÖ∂Âú®‰∏çÂêåËÆ°ÁÆóÂπ≥Âè∞‰∏äÁöÑÈöêÁßÅÊ≥ÑÈú≤È£éÈô©„ÄÇÈÄöËøáËÆæËÆ°ÁâπÂÆöÁöÑÊîªÂáªÊñπÊ≥ïÔºåËÉΩÂ§üÊúâÊïàÂú∞‰ªéÊ®°ÂûãÁöÑÊâßË°å‰∏≠ÊèêÂèñÁî®Êà∑ÊïèÊÑü‰ø°ÊÅØ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨ÂõõÁßçÊñ∞ÂûãÁöÑÊû∂ÊûÑ‰æß‰ø°ÈÅìÔºåÂàÜÂà´‰∏∫CPUÁöÑÁºìÂ≠òÂç†Áî®ÈÄöÈÅì„ÄÅÈ°µÈù¢ÁΩÆÊç¢+ÈáçËΩΩÔºå‰ª•ÂèäGPUÁöÑÊÄßËÉΩËÆ°Êï∞Âô®ÂíåTLBÈ©±ÈÄê+ÈáçËΩΩ„ÄÇÊØèÁßç‰æß‰ø°ÈÅìÈÉΩÈíàÂØπ‰∏çÂêåÁöÑÁ°¨‰ª∂ÁâπÊÄßËøõË°å‰ºòÂåñÔºå‰ª•ÂÆûÁé∞È´òÊïàÁöÑÊîªÂáª„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞ÁÇπÂú®‰∫éÈ¶ñÊ¨°Âú®ËøêË°åÊó∂ÂØπÊµÅË°åÁöÑMoEÁªìÊûÑËøõË°åÂÆâÂÖ®ÂàÜÊûêÔºåÊè≠Á§∫‰∫ÜÂÖ∂Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÂÆâÂÖ®ÈöêÊÇ£Ôºå‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåÊèê‰æõ‰∫ÜÊõ¥‰∏∫Ê∑±ÂÖ•ÁöÑÊîªÂáªËßÜËßí„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÊîªÂáªËÆæËÆ°‰∏≠ÔºåÈááÁî®‰∫ÜÁâπÂÆöÁöÑÂèÇÊï∞ËÆæÁΩÆÂíåÊçüÂ§±ÂáΩÊï∞Ôºå‰ª•‰ºòÂåñÊîªÂáªÊïàÊûú„ÄÇÁΩëÁªúÁªìÊûÑÊñπÈù¢ÔºåÈíàÂØπ‰∏çÂêåÁöÑ‰æß‰ø°ÈÅìËÆæËÆ°‰∫ÜÁõ∏Â∫îÁöÑÊîªÂáªÊµÅÁ®ãÔºåÁ°Æ‰øùËÉΩÂ§üÊúâÊïàÊèêÂèñÁî®Êà∑ÈöêÁßÅ‰ø°ÊÅØ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåMoEchoËÉΩÂ§üÊàêÂäüÂÆûÊñΩÂõõÁßçÊîªÂáªÔºåÊòæËëóÊèêÈ´ò‰∫ÜÂØπÁî®Êà∑ÈöêÁßÅÁöÑ‰æµÁäØËÉΩÂäõ„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåPrompt Inference AttackÂíåResponse Reconstruction AttackÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ë∂ÖËøá80%ÁöÑÊàêÂäüÁéáÔºåË°®ÊòéÂú®Áé∞ÊúâMoEÊû∂ÊûÑ‰∏≠Â≠òÂú®‰∏•ÈáçÁöÑÂÆâÂÖ®ÈöêÊÇ£„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨ÂÆâÂÖ®ÊïèÊÑüÁöÑAIÊúçÂä°ÔºåÂ¶ÇÈáëËûç„ÄÅÂåªÁñóÂíåÁ§æ‰∫§Â™í‰ΩìÁ≠â„ÄÇÈÄöËøáËØÜÂà´ÂíåÈò≤ËåÉMoEÊ®°Âûã‰∏≠ÁöÑÈöêÁßÅÊ≥ÑÈú≤È£éÈô©ÔºåÂèØ‰ª•‰∏∫Áî®Êà∑Êèê‰æõÊõ¥ÂÆâÂÖ®ÁöÑÊúçÂä°ÔºåÊèêÂçáÁî®Êà∑‰ø°‰ªªÂ∫¶„ÄÇÊú™Êù•ÔºåËøô‰∏ÄÁ†îÁ©∂Â∞ÜÊé®Âä®ÂØπAIÊ®°ÂûãÂÆâÂÖ®ÊÄßÁöÑÊ∑±ÂÖ•Êé¢ËÆ®Ôºå‰øÉËøõÊõ¥ÂÆâÂÖ®ÁöÑAIÊäÄÊúØÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> The transformer architecture has become a cornerstone of modern AI, fueling remarkable progress across applications in natural language processing, computer vision, and multimodal learning. As these models continue to scale explosively for performance, implementation efficiency remains a critical challenge. Mixture of Experts (MoE) architectures, selectively activating specialized subnetworks (experts), offer a unique balance between model accuracy and computational cost. However, the adaptive routing in MoE architectures, where input tokens are dynamically directed to specialized experts based on their semantic meaning inadvertently opens up a new attack surface for privacy breaches. These input-dependent activation patterns leave distinctive temporal and spatial traces in hardware execution, which adversaries could exploit to deduce sensitive user data. In this work, we propose MoEcho, discovering a side channel analysis based attack surface that compromises user privacy on MoE based systems. Specifically, in MoEcho, we introduce four novel architectural side channels on different computing platforms, including Cache Occupancy Channels and Pageout+Reload on CPUs, and Performance Counter and TLB Evict+Reload on GPUs, respectively. Exploiting these vulnerabilities, we propose four attacks that effectively breach user privacy in large language models (LLMs) and vision language models (VLMs) based on MoE architectures: Prompt Inference Attack, Response Reconstruction Attack, Visual Inference Attack, and Visual Reconstruction Attack. MoEcho is the first runtime architecture level security analysis of the popular MoE structure common in modern transformers, highlighting a serious security and privacy threat and calling for effective and timely safeguards when harnessing MoE based models for developing efficient large scale AI services.

