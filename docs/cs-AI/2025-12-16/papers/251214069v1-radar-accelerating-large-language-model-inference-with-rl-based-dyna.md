---
layout: default
title: RADAR: Accelerating Large Language Model Inference With RL-Based Dynamic Draft Trees
---

# RADAR: Accelerating Large Language Model Inference With RL-Based Dynamic Draft Trees

**arXiv**: [2512.14069v1](https://arxiv.org/abs/2512.14069) | [PDF](https://arxiv.org/pdf/2512.14069.pdf)

**ä½œè€…**: Junjie Ma, Jinlong Li

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**å¤‡æ³¨**: 5 pages, 2 figures

**ðŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/minaduki-sora/RADAR)

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRADARæ–¹æ³•ï¼ŒåŸºäºŽå¼ºåŒ–å­¦ä¹ çš„åŠ¨æ€è‰ç¨¿æ ‘åŠ é€Ÿå¤§åž‹è¯­è¨€æ¨¡åž‹æŽ¨ç†ï¼Œè§£å†³æŽ¨æµ‹é‡‡æ ·ä¸­è‰ç¨¿æ¨¡åž‹è°ƒç”¨ç¼ºä¹çµæ´»æ€§çš„é—®é¢˜ã€‚**

**å…³é”®è¯**: `å¤§åž‹è¯­è¨€æ¨¡åž‹æŽ¨ç†åŠ é€Ÿ` `æŽ¨æµ‹é‡‡æ ·` `å¼ºåŒ–å­¦ä¹ åº”ç”¨` `åŠ¨æ€è‰ç¨¿æ ‘` `é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹` `ç¦»çº¿å¼ºåŒ–å­¦ä¹ ` `è®¡ç®—ä¼˜åŒ–` `å®žæ—¶å†³ç­–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æŽ¨æµ‹é‡‡æ ·æ–¹æ³•ä¸­ï¼Œè‰ç¨¿æ¨¡åž‹è°ƒç”¨æ¬¡æ•°å›ºå®šä¸ºè¶…å‚æ•°ï¼Œå¯¼è‡´è®¡ç®—å†—ä½™å’Œçµæ´»æ€§ä¸è¶³ï¼Œé™åˆ¶äº†æŽ¨ç†æ•ˆçŽ‡æå‡ã€‚
2. RADARå°†è‰ç¨¿æ ‘ç”Ÿæˆå»ºæ¨¡ä¸ºMDPï¼Œåˆ©ç”¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ è®­ç»ƒé¢„æµ‹æ¨¡åž‹ï¼ŒåŠ¨æ€å†³å®šè‰ç¨¿æ¨¡åž‹è°ƒç”¨ï¼Œä¼˜åŒ–å€™é€‰ä»¤ç‰Œç”Ÿæˆè¿‡ç¨‹ã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒRADARåœ¨å¤šä¸ªLLMså’Œä»»åŠ¡ä¸Šå®žçŽ°3.17x-4.82xåŠ é€Ÿï¼Œæ˜¾è‘—è¶…è¶Šè‡ªå›žå½’åŸºçº¿ï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

çŽ°ä»£å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰çš„æŽ¨ç†è¿‡ç¨‹æ˜‚è´µä¸”ç¼“æ…¢ï¼ŒæŽ¨æµ‹é‡‡æ ·å·²æˆä¸ºè§£å†³æ­¤é—®é¢˜çš„æœ‰æ•ˆæ–¹æ¡ˆï¼Œä½†æŽ¨æµ‹é‡‡æ ·ä¸­ç”¨äºŽç”Ÿæˆå€™é€‰ä»¤ç‰Œçš„è‰ç¨¿æ¨¡åž‹è°ƒç”¨æ¬¡æ•°æ˜¯ä¸€ä¸ªé¢„è®¾çš„è¶…å‚æ•°ï¼Œç¼ºä¹çµæ´»æ€§ã€‚ä¸ºäº†æ›´æœ‰æ•ˆåœ°ç”Ÿæˆå’Œåˆ©ç”¨å€™é€‰ä»¤ç‰Œï¼Œæˆ‘ä»¬æå‡ºäº†RADARï¼Œä¸€ç§åŸºäºŽå¼ºåŒ–å­¦ä¹ åŠ¨æ€è‰ç¨¿æ ‘çš„æ–°åž‹æŽ¨æµ‹é‡‡æ ·æ–¹æ³•ã€‚RADARå°†è‰ç¨¿æ ‘ç”Ÿæˆè¿‡ç¨‹å»ºæ¨¡ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ï¼Œå¹¶é‡‡ç”¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ è®­ç»ƒé¢„æµ‹æ¨¡åž‹ï¼Œä»Žè€Œå®žçŽ°å¯¹è‰ç¨¿æ¨¡åž‹è°ƒç”¨çš„å®žæ—¶å†³ç­–ï¼Œå‡å°‘å†—ä½™è®¡ç®—å¹¶è¿›ä¸€æ­¥åŠ é€ŸæŽ¨ç†ã€‚åœ¨ä¸‰ä¸ªLLMså’Œå››ä¸ªä»»åŠ¡ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒRADARç›¸æ¯”è‡ªå›žå½’è§£ç åŸºçº¿å®žçŽ°äº†3.17å€è‡³4.82å€çš„åŠ é€Ÿã€‚ä»£ç å¯åœ¨https://github.com/minaduki-sora/RADARèŽ·å–ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

RADARçš„æ•´ä½“æ¡†æž¶åŸºäºŽæŽ¨æµ‹é‡‡æ ·ï¼Œä½†åˆ›æ–°æ€§åœ°å¼•å…¥åŠ¨æ€è‰ç¨¿æ ‘ç”Ÿæˆæœºåˆ¶ã€‚æ ¸å¿ƒæ–¹æ³•æ˜¯å°†è‰ç¨¿æ ‘æž„å»ºè¿‡ç¨‹è§†ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œå…¶ä¸­çŠ¶æ€åŒ…æ‹¬å½“å‰ä¸Šä¸‹æ–‡å’Œå€™é€‰ä»¤ç‰Œï¼ŒåŠ¨ä½œæ˜¯å†³å®šæ˜¯å¦ç»§ç»­è°ƒç”¨è‰ç¨¿æ¨¡åž‹æ‰©å±•æ ‘ã€‚é€šè¿‡ç¦»çº¿å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸€ä¸ªé¢„æµ‹æ¨¡åž‹ï¼Œè¯¥æ¨¡åž‹å­¦ä¹ æœ€ä¼˜ç­–ç•¥ï¼Œå®žæ—¶è¯„ä¼°æ¯ä¸ªèŠ‚ç‚¹çš„æ‰©å±•ä»·å€¼ï¼Œä»Žè€ŒåŠ¨æ€è°ƒæ•´è‰ç¨¿æ¨¡åž‹è°ƒç”¨æ¬¡æ•°ï¼Œå‡å°‘ä¸å¿…è¦çš„è®¡ç®—ã€‚ä¸ŽçŽ°æœ‰æŽ¨æµ‹é‡‡æ ·æ–¹æ³•çš„ä¸»è¦åŒºåˆ«åœ¨äºŽï¼ŒRADARä¸å†ä¾èµ–å›ºå®šè¶…å‚æ•°ï¼Œè€Œæ˜¯é€šè¿‡å¼ºåŒ–å­¦ä¹ å®žçŽ°è‡ªé€‚åº”å†³ç­–ï¼Œæé«˜äº†çµæ´»æ€§å’Œæ•ˆçŽ‡ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

åœ¨ä¸‰ä¸ªä¸åŒè§„æ¨¡çš„å¤§åž‹è¯­è¨€æ¨¡åž‹å’Œå››ä¸ªå¤šæ ·åŒ–ä»»åŠ¡ï¼ˆå¦‚æ–‡æœ¬ç”Ÿæˆå’Œé—®ç­”ï¼‰ä¸Šï¼ŒRADARç›¸æ¯”æ ‡å‡†è‡ªå›žå½’è§£ç åŸºçº¿å®žçŽ°äº†3.17å€åˆ°4.82å€çš„æŽ¨ç†åŠ é€Ÿï¼Œæœ€é«˜æå‡è¾¾4.82å€ï¼Œæ˜¾è‘—å‡å°‘äº†å»¶è¿Ÿï¼ŒåŒæ—¶ä¿æŒè¾“å‡ºè´¨é‡ï¼Œè¯æ˜Žäº†åŠ¨æ€å†³ç­–åœ¨ä¼˜åŒ–æŽ¨æµ‹é‡‡æ ·ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

RADARé€‚ç”¨äºŽéœ€è¦é«˜æ•ˆæŽ¨ç†çš„å¤§åž‹è¯­è¨€æ¨¡åž‹åœºæ™¯ï¼Œå¦‚å®žæ—¶å¯¹è¯ç³»ç»Ÿã€ä»£ç ç”Ÿæˆã€æ–‡æœ¬æ‘˜è¦å’Œæœºå™¨ç¿»è¯‘ã€‚å…¶åŠ é€Ÿèƒ½åŠ›å¯é™ä½Žè®¡ç®—æˆæœ¬ï¼Œæå‡å“åº”é€Ÿåº¦ï¼Œåœ¨äº‘è®¡ç®—ã€è¾¹ç¼˜è®¾å¤‡å’ŒAIæœåŠ¡ä¸­å…·æœ‰å®žé™…ä»·å€¼ï¼Œä¿ƒè¿›LLMsåœ¨èµ„æºå—é™çŽ¯å¢ƒä¸‹çš„éƒ¨ç½²ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Inference with modern Large Language Models (LLMs) is expensive and slow, and speculative sampling has emerged as an effective solution to this problem, however, the number of the calls to the draft model for generating candidate tokens in speculative sampling is a preset hyperparameter, lacking flexibility. To generate and utilize the candidate tokens more effectively, we propose RADAR, a novel speculative sampling method with RL-based dynamic draft trees. RADAR formulates the draft tree generation process as a Markov Decision Process (MDP) and employs offline reinforcement learning to train a prediction model, which enables real-time decision on the calls to the draft model, reducing redundant computations and further accelerating inference. Evaluations across three LLMs and four tasks show that RADAR achieves a speedup of 3.17x-4.82x over the auto-regressive decoding baseline. The code is available at https://github.com/minaduki-sora/RADAR.

