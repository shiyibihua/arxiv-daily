---
layout: default
title: From Context to EDUs: Faithful and Structured Context Compression via Elementary Discourse Unit Decomposition
---

# From Context to EDUs: Faithful and Structured Context Compression via Elementary Discourse Unit Decomposition

**arXiv**: [2512.14244v1](https://arxiv.org/abs/2512.14244) | [PDF](https://arxiv.org/pdf/2512.14244.pdf)

**ä½œè€…**: Yiqing Zhou, Yu Lei, Shuzheng Si, Qingyan Sun, Wei Wang, Yifei Wu, Hao Wen, Gang Chen, Fanchao Qi, Maosong Sun

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽåŸºæœ¬è¯è¯­å•å…ƒçš„ä¸Šä¸‹æ–‡åŽ‹ç¼©å™¨ï¼Œé€šè¿‡ç»“æž„åŒ–åˆ†è§£ä¸Žé€‰æ‹©è§£å†³é•¿æ–‡æœ¬å¤„ç†ä¸­çš„è®¡ç®—æˆæœ¬ä¸Žå™ªå£°é—®é¢˜ã€‚**

**å…³é”®è¯**: `ä¸Šä¸‹æ–‡åŽ‹ç¼©` `åŸºæœ¬è¯è¯­å•å…ƒ` `ç»“æž„å…³ç³»æ ‘` `é•¿æ–‡æœ¬å¤„ç†` `å¤§è¯­è¨€æ¨¡åž‹` `è®¡ç®—æ•ˆçŽ‡` `ä¸‹æ¸¸ä»»åŠ¡å¢žå¼º` `æ˜¾å¼åŽ‹ç¼©æ¡†æž¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰åŽ‹ç¼©æ–¹æ³•å¸¸ç ´åæ–‡æœ¬å±€éƒ¨è¿žè´¯æ€§æˆ–ä¾èµ–éšå¼ç¼–ç ï¼Œå¯¼è‡´ä½ç½®åå·®å’ŒAPIä¸å…¼å®¹é—®é¢˜ã€‚
2. æå‡ºåŸºäºŽåŸºæœ¬è¯è¯­å•å…ƒçš„æ˜¾å¼åŽ‹ç¼©æ¡†æž¶ï¼Œé€šè¿‡ç»“æž„å…³ç³»æ ‘åˆ†è§£å’ŒæŸ¥è¯¢ç›¸å…³å­æ ‘é€‰æ‹©å®žçŽ°å¿ å®žåŽ‹ç¼©ã€‚
3. åœ¨StructBenchæ•°æ®é›†ä¸Šå®žçŽ°æœ€å…ˆè¿›ç»“æž„é¢„æµ‹å‡†ç¡®æ€§ï¼Œæ˜¾è‘—é™ä½Žè®¡ç®—æˆæœ¬å¹¶æå‡ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç®¡ç†é•¿ä¸Šä¸‹æ–‡æ˜¯å¤§è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰çš„å…³é”®ç“¶é¢ˆï¼Œå°¤å…¶åœ¨é•¿æ–‡æ¡£é—®ç­”å’Œè‡ªä¸»ä»£ç†ç­‰åº”ç”¨ä¸­ï¼Œé•¿è¾“å…¥å¯¼è‡´é«˜è®¡ç®—æˆæœ¬å’Œå™ªå£°å¼•å…¥ã€‚çŽ°æœ‰åŽ‹ç¼©æŠ€æœ¯å¸¸é€šè¿‡ç¦»æ•£ä»¤ç‰Œç§»é™¤ç ´åå±€éƒ¨è¿žè´¯æ€§ï¼Œæˆ–ä¾èµ–éšå¼æ½œåœ¨ç¼–ç ï¼Œå­˜åœ¨ä½ç½®åå·®ä¸”ä¸Žé—­æºAPIä¸å…¼å®¹ã€‚ä¸ºåº”å¯¹è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºŽåŸºæœ¬è¯è¯­å•å…ƒï¼ˆEDUï¼‰çš„ä¸Šä¸‹æ–‡åŽ‹ç¼©å™¨ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ˜¾å¼åŽ‹ç¼©æ¡†æž¶ï¼Œæ—¨åœ¨ä¿ç•™å…¨å±€ç»“æž„å’Œç»†ç²’åº¦ç»†èŠ‚ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†ä¸Šä¸‹æ–‡åŽ‹ç¼©é‡æ–°è¡¨è¿°ä¸ºâ€œå…ˆç»“æž„åŽé€‰æ‹©â€çš„è¿‡ç¨‹ï¼šé¦–å…ˆï¼ŒLingoEDUå°†çº¿æ€§æ–‡æœ¬è½¬æ¢ä¸ºåŸºäºŽæºç´¢å¼•é”šå®šçš„åŸºæœ¬è¯è¯­å•å…ƒç»“æž„å…³ç³»æ ‘ï¼Œä»¥æ¶ˆé™¤å¹»è§‰ï¼›å…¶æ¬¡ï¼Œè½»é‡çº§æŽ’åæ¨¡å—é€‰æ‹©æŸ¥è¯¢ç›¸å…³çš„å­æ ‘è¿›è¡Œçº¿æ€§åŒ–ã€‚ä¸ºä¸¥æ ¼è¯„ä¼°ç»“æž„ç†è§£ï¼Œæˆ‘ä»¬å‘å¸ƒäº†StructBenchï¼Œä¸€ä¸ªåŒ…å«248ä¸ªå¤šæ ·åŒ–æ–‡æ¡£çš„æ‰‹åŠ¨æ ‡æ³¨æ•°æ®é›†ã€‚å®žè¯ç»“æžœè¡¨æ˜Žï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç»“æž„é¢„æµ‹å‡†ç¡®æ€§ä¸Šè¾¾åˆ°æœ€å…ˆè¿›æ°´å¹³ï¼Œæ˜¾è‘—ä¼˜äºŽå‰æ²¿LLMsï¼ŒåŒæ—¶é™ä½Žæˆæœ¬ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç»“æž„æ„ŸçŸ¥åŽ‹ç¼©åœ¨ä»Žé•¿ä¸Šä¸‹æ–‡ä»»åŠ¡åˆ°å¤æ‚æ·±åº¦æœç´¢åœºæ™¯çš„ä¸‹æ¸¸ä»»åŠ¡ä¸­å¤§å¹…æå‡äº†æ€§èƒ½ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

è®ºæ–‡æå‡ºEDU-based Context Compressoræ¡†æž¶ï¼Œæ•´ä½“æµç¨‹ä¸ºç»“æž„-then-é€‰æ‹©ã€‚é¦–å…ˆï¼ŒLingoEDUæ¨¡å—å°†çº¿æ€§æ–‡æœ¬åˆ†è§£ä¸ºåŸºæœ¬è¯è¯­å•å…ƒï¼ˆEDUï¼‰ï¼Œæž„å»ºä¸¥æ ¼é”šå®šæºç´¢å¼•çš„ç»“æž„å…³ç³»æ ‘ï¼Œç¡®ä¿åŽ‹ç¼©å¿ å®žæ€§ã€‚å…¶æ¬¡ï¼Œè½»é‡çº§æŽ’åæ¨¡å—åŸºäºŽæŸ¥è¯¢ç›¸å…³æ€§é€‰æ‹©å­æ ‘è¿›è¡Œçº¿æ€§åŒ–è¾“å‡ºã€‚å…³é”®åˆ›æ–°åœ¨äºŽæ˜¾å¼ç»“æž„åŒ–åŽ‹ç¼©ï¼Œé¿å…äº†éšå¼ç¼–ç çš„åå·®ï¼Œå¹¶ä¿æŒå…¨å±€ä¸Žå±€éƒ¨ç»†èŠ‚ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•çš„ä¸»è¦åŒºåˆ«åœ¨äºŽå¼ºè°ƒç»“æž„ä¿ç•™å’Œæ˜¾å¼ç´¢å¼•ï¼Œè€Œéžä¾èµ–ç¦»æ•£ä»¤ç‰Œç§»é™¤æˆ–æ½œåœ¨è¡¨ç¤ºã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

åœ¨StructBenchæ•°æ®é›†ä¸Šï¼Œæ–¹æ³•å®žçŽ°æœ€å…ˆè¿›çš„ç»“æž„é¢„æµ‹å‡†ç¡®æ€§ï¼Œæ˜¾è‘—ä¼˜äºŽå‰æ²¿LLMsï¼ŒåŒæ—¶åŽ‹ç¼©æˆæœ¬é™ä½Žï¼Œå¹¶åœ¨é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡å’Œå¤æ‚æœç´¢ä¸­å¸¦æ¥æ€§èƒ½æå‡ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶é€‚ç”¨äºŽé•¿æ–‡æ¡£é—®ç­”ã€è‡ªä¸»ä»£ç†ã€æ·±åº¦æœç´¢ç­‰åœºæ™¯ï¼Œèƒ½æœ‰æ•ˆé™ä½ŽLLMsçš„è®¡ç®—å¼€é”€å’Œå™ªå£°å¹²æ‰°ï¼Œæå‡å¤„ç†æ•ˆçŽ‡å’Œå‡†ç¡®æ€§ï¼Œå…·æœ‰å®žé™…éƒ¨ç½²ä»·å€¼ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Managing extensive context remains a critical bottleneck for Large Language Models (LLMs), particularly in applications like long-document question answering and autonomous agents where lengthy inputs incur high computational costs and introduce noise. Existing compression techniques often disrupt local coherence through discrete token removal or rely on implicit latent encoding that suffers from positional bias and incompatibility with closed-source APIs. To address these limitations, we introduce the EDU-based Context Compressor, a novel explicit compression framework designed to preserve both global structure and fine-grained details. Our approach reformulates context compression as a structure-then-select process. First, our LingoEDU transforms linear text into a structural relation tree of Elementary Discourse Units (EDUs) which are anchored strictly to source indices to eliminate hallucination. Second, a lightweight ranking module selects query-relevant sub-trees for linearization. To rigorously evaluate structural understanding, we release StructBench, a manually annotated dataset of 248 diverse documents. Empirical results demonstrate that our method achieves state-of-the-art structural prediction accuracy and significantly outperforms frontier LLMs while reducing costs. Furthermore, our structure-aware compression substantially enhances performance across downstream tasks ranging from long-context tasks to complex Deep Search scenarios.

