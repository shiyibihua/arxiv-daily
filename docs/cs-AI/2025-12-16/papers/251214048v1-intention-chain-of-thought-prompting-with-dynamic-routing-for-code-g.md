---
layout: default
title: Intention Chain-of-Thought Prompting with Dynamic Routing for Code Generation
---

# Intention Chain-of-Thought Prompting with Dynamic Routing for Code Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.14048" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.14048v1</a>
  <a href="https://arxiv.org/pdf/2512.14048.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.14048v1" onclick="toggleFavorite(this, '2512.14048v1', 'Intention Chain-of-Thought Prompting with Dynamic Routing for Code Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shen Li, Li Huang, Shaoxiong Zhan, Weifeng Sun, Tao Yin, Zhongxin Liu, Meng Yan

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**å¤‡æ³¨**: Accepted at AAAI-2026

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRoutingGenä»¥è§£å†³ä»£ç ç”Ÿæˆä¸­çš„æ¨ç†æ•ˆç‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä»£ç ç”Ÿæˆ` `é“¾å¼æ€ç»´` `åŠ¨æ€è·¯ç”±` `æ„å›¾å»ºæ¨¡` `æ¨ç†æ•ˆç‡` `å¤§å‹è¯­è¨€æ¨¡å‹` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é“¾å¼æ€ç»´æç¤ºæ–¹æ³•åœ¨ç®€å•ä»»åŠ¡ä¸Šå®¹æ˜“å¯¼è‡´è¿‡åº¦æ€è€ƒï¼ŒåŒæ—¶ç¼ºä¹å¯¹ä»£ç ç”Ÿæˆä¸­æ ¸å¿ƒæ„å›¾çš„å»ºæ¨¡ã€‚
2. æœ¬æ–‡æå‡ºRoutingGenæ¡†æ¶ï¼ŒåŠ¨æ€è°ƒæ•´æç¤ºç­–ç•¥ï¼Œç®€å•ä»»åŠ¡ä½¿ç”¨å°‘é‡ç¤ºä¾‹ï¼Œå¤æ‚ä»»åŠ¡é‡‡ç”¨æ„å›¾é“¾å¼æ€ç»´ï¼ˆICoTï¼‰ç­–ç•¥ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºRoutingGenåœ¨å…­ä¸ªæ ‡å‡†ä»£ç ç”ŸæˆåŸºå‡†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¹³å‡å‡å°‘46.37%çš„ä»¤ç‰Œä½¿ç”¨é‡ï¼Œå¹¶è¶…è¶Šå…­ä¸ªç°æœ‰æç¤ºåŸºçº¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä»£ç ç”Ÿæˆä¸­å±•ç°å‡ºå¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›ã€‚ç°æœ‰çš„é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºæ–¹æ³•é€šè¿‡å¼•å¯¼ä¸­é—´æ­¥éª¤æ¥å¢å¼ºæ¨¡å‹æ¨ç†ï¼Œä½†å­˜åœ¨ä¸¤ä¸ªä¸»è¦å±€é™ï¼šä¸€æ˜¯ç»Ÿä¸€åº”ç”¨å¯¼è‡´ç®€å•ä»»åŠ¡çš„è¿‡åº¦æ€è€ƒï¼ŒäºŒæ˜¯ç¼ºä¹ä»£ç ç”Ÿæˆä¸­çš„æ„å›¾æŠ½è±¡ï¼Œæœªèƒ½æœ‰æ•ˆå»ºæ¨¡æ ¸å¿ƒç®—æ³•è®¾è®¡å’Œæ•ˆç‡ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„éš¾åº¦æ„ŸçŸ¥è·¯ç”±æ¡†æ¶RoutingGenï¼ŒåŠ¨æ€è°ƒæ•´ä»£ç ç”Ÿæˆçš„æç¤ºç­–ç•¥ã€‚å¯¹äºç®€å•ä»»åŠ¡ï¼Œé‡‡ç”¨å°‘é‡ç¤ºä¾‹æç¤ºï¼›å¯¹äºå¤æ‚ä»»åŠ¡ï¼Œå¼•å…¥æ„å›¾é“¾å¼æ€ç»´ï¼ˆICoTï¼‰ç­–ç•¥ï¼Œå¼•å¯¼æ¨¡å‹æ•æ‰ä»»åŠ¡æ„å›¾ã€‚å®éªŒè¡¨æ˜ï¼ŒRoutingGenåœ¨å¤§å¤šæ•°è®¾ç½®ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶å¹³å‡å‡å°‘äº†46.37%çš„æ€»ä»¤ç‰Œä½¿ç”¨é‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰é“¾å¼æ€ç»´æç¤ºæ–¹æ³•åœ¨ä»£ç ç”Ÿæˆä¸­çš„æ•ˆç‡é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨ç®€å•ä»»åŠ¡ä¸Šå¯¼è‡´çš„è¿‡åº¦æ€è€ƒåŠç¼ºä¹æ„å›¾å»ºæ¨¡çš„ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šRoutingGenæ¡†æ¶é€šè¿‡åŠ¨æ€è°ƒæ•´æç¤ºç­–ç•¥ï¼Œé’ˆå¯¹ä¸åŒå¤æ‚åº¦çš„ä»»åŠ¡é‡‡ç”¨ä¸åŒçš„æ¨ç†æ–¹å¼ï¼Œä»¥æé«˜æ¨ç†æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚å¯¹äºç®€å•ä»»åŠ¡ï¼Œä½¿ç”¨å°‘é‡ç¤ºä¾‹æç¤ºï¼›è€Œå¯¹äºå¤æ‚ä»»åŠ¡ï¼Œåˆ™å¼•å…¥æ„å›¾é“¾å¼æ€ç»´ï¼ˆICoTï¼‰æ¥æ•æ‰æ ¸å¿ƒç®—æ³•é€»è¾‘å’Œæ—¶é—´å¤æ‚åº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRoutingGençš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šä¸€æ˜¯éš¾åº¦æ„ŸçŸ¥æ¨¡å—ï¼Œæ ¹æ®ä»»åŠ¡å¤æ‚åº¦é€‰æ‹©æç¤ºç­–ç•¥ï¼›äºŒæ˜¯æ„å›¾é“¾å¼æ€ç»´æ¨¡å—ï¼Œæä¾›ç»“æ„åŒ–æ¨ç†ä»¥æ•æ‰ä»»åŠ¡æ„å›¾ã€‚

**å…³é”®åˆ›æ–°**ï¼šRoutingGençš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶åŠ¨æ€è·¯ç”±æœºåˆ¶ï¼Œèƒ½å¤Ÿæ ¹æ®ä»»åŠ¡çš„å¤æ‚æ€§çµæ´»è°ƒæ•´æç¤ºç­–ç•¥ï¼Œä¸ç°æœ‰æ–¹æ³•çš„å›ºå®šæç¤ºæ–¹å¼å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒRoutingGené‡‡ç”¨äº†å°‘é‡ç¤ºä¾‹æç¤ºå’Œæ„å›¾é“¾å¼æ€ç»´çš„ç»“åˆï¼Œç¡®ä¿åœ¨ç®€å•ä»»åŠ¡ä¸­é«˜æ•ˆæ¨ç†ï¼ŒåŒæ—¶åœ¨å¤æ‚ä»»åŠ¡ä¸­æ·±å…¥ç†è§£ä»»åŠ¡æ„å›¾ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒRoutingGenåœ¨å¤§å¤šæ•°è®¾ç½®ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚ä»»åŠ¡ä¸Šï¼ŒICoTç­–ç•¥è¶…è¶Šäº†å…­ä¸ªç°æœ‰çš„æç¤ºåŸºçº¿ï¼Œä¸”å¹³å‡å‡å°‘äº†46.37%çš„ä»¤ç‰Œä½¿ç”¨é‡ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ•ˆç‡æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è½¯ä»¶å¼€å‘ã€è‡ªåŠ¨åŒ–ç¼–ç¨‹å’Œæ•™è‚²ç­‰ã€‚é€šè¿‡æé«˜ä»£ç ç”Ÿæˆçš„æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼ŒRoutingGenèƒ½å¤Ÿå¸®åŠ©å¼€å‘è€…æ›´å¿«é€Ÿåœ°å®ç°åŠŸèƒ½ï¼ŒåŒæ—¶ä¸ºæ•™è‚²é¢†åŸŸæä¾›æ›´æ™ºèƒ½çš„ç¼–ç¨‹å­¦ä¹ å·¥å…·ï¼Œæœªæ¥å¯èƒ½å¯¹ç¼–ç¨‹æ•™è‚²å’Œè½¯ä»¶å¼€å‘æµç¨‹äº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) exhibit strong generative capabilities and have shown great potential in code generation. Existing chain-of-thought (CoT) prompting methods enhance model reasoning by eliciting intermediate steps, but suffer from two major limitations: First, their uniform application tends to induce overthinking on simple tasks. Second, they lack intention abstraction in code generation, such as explicitly modeling core algorithmic design and efficiency, leading models to focus on surface-level structures while neglecting the global problem objective. Inspired by the cognitive economy principle of engaging structured reasoning only when necessary to conserve cognitive resources, we propose RoutingGen, a novel difficulty-aware routing framework that dynamically adapts prompting strategies for code generation. For simple tasks, it adopts few-shot prompting; for more complex ones, it invokes a structured reasoning strategy, termed Intention Chain-of-Thought (ICoT), which we introduce to guide the model in capturing task intention, such as the core algorithmic logic and its time complexity. Experiments across three models and six standard code generation benchmarks show that RoutingGen achieves state-of-the-art performance in most settings, while reducing total token usage by 46.37% on average across settings. Furthermore, ICoT outperforms six existing prompting baselines on challenging benchmarks.

