---
layout: default
title: Towards Nepali-language LLMs: Efficient GPT training with a Nepali BPE tokenizer
---

# Towards Nepali-language LLMs: Efficient GPT training with a Nepali BPE tokenizer

**arXiv**: [2512.14585v1](https://arxiv.org/abs/2512.14585) | [PDF](https://arxiv.org/pdf/2512.14585.pdf)

**ä½œè€…**: Adarsha Shrestha, Basanta Pokharel, Binit Shrestha, Smriti Adhikari, Dinesh Gothe

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**å¤‡æ³¨**: Work in progress

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽGPT-2çš„å°¼æ³Šå°”è¯­å¤§è¯­è¨€æ¨¡åž‹ï¼Œé€šè¿‡å®šåˆ¶BPEåˆ†è¯å™¨å’Œé«˜æ•ˆè®­ç»ƒç­–ç•¥è§£å†³ä½Žèµ„æºè¯­è¨€ç”Ÿæˆéš¾é¢˜ã€‚**

**å…³é”®è¯**: `ä½Žèµ„æºè¯­è¨€å¤„ç†` `å°¼æ³Šå°”è¯­å¤§è¯­è¨€æ¨¡åž‹` `å­—èŠ‚å¯¹ç¼–ç åˆ†è¯å™¨` `GPT-2æž¶æž„` `FlashAttentionä¼˜åŒ–` `æ–‡æœ¬ç”Ÿæˆ` `è‡ªç„¶è¯­è¨€å¤„ç†` `è®­ç»ƒç­–ç•¥ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ–¹æ³•ä¸»è¦åŸºäºŽåŸºç¡€ç¼–ç å™¨æž¶æž„ï¼Œéš¾ä»¥æ»¡è¶³å°¼æ³Šå°”è¯­å¤æ‚è¯­æ³•å’Œé»ç€æ€§å½¢æ€ä¸‹çš„æ–‡æœ¬ç”Ÿæˆéœ€æ±‚ã€‚
2. è®ºæ–‡æå‡ºåŸºäºŽGPT-2çš„æ¨¡åž‹ï¼Œç»“åˆå®šåˆ¶BPEåˆ†è¯å™¨å’ŒGPT-3å¯å‘çš„è®­ç»ƒç­–ç•¥ï¼Œä¼˜åŒ–å­¦ä¹ çŽ‡ä¸Žæž¶æž„ã€‚
3. æ¨¡åž‹åœ¨å°¼æ³Šå°”è¯­æ•°æ®é›†ä¸Šè®­ç»ƒåŽï¼Œå›°æƒ‘åº¦è¾¾21.80ï¼Œèƒ½ç”Ÿæˆè¿žè´¯çš„æ–°é—»æ–‡æœ¬ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°¼æ³Šå°”è¯­ä½œä¸ºä¸€ç§ä½Žèµ„æºè¯­è¨€ï¼Œæ‹¥æœ‰è¶…è¿‡3200ä¸‡ä½¿ç”¨è€…ï¼Œå› å…¶å¤æ‚çš„è¯­æ³•ã€é»ç€æ€§å½¢æ€å’Œé«˜è´¨é‡è¯­æ–™åº“çš„æœ‰é™å¯ç”¨æ€§ï¼Œåœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸæŒç»­é¢ä¸´æŒ‘æˆ˜ã€‚è¿„ä»Šä¸ºæ­¢çš„å¤§å¤šæ•°åŠªåŠ›éƒ½é›†ä¸­åœ¨åŸºç¡€ç¼–ç å™¨æž¶æž„ä¸Šï¼Œè¿™äº›æž¶æž„å¯¹äºŽå°¼æ³Šå°”è¯­ç‰¹å®šçš„æ–‡æœ¬ç”Ÿæˆä»ç„¶ä¸è¶³ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåŸºäºŽGPT-2çš„å°¼æ³Šå°”è¯­è¯­è¨€æ¨¡åž‹ï¼Œé‡‡ç”¨äº†å—GPT-3å¯å‘çš„å¤šç§è®­ç»ƒç­–ç•¥ï¼ŒåŒ…æ‹¬ä¼˜åŒ–çš„å­¦ä¹ çŽ‡è°ƒåº¦ã€æ‰¹æ¬¡ç¼©æ”¾å’Œæž¶æž„æ”¹è¿›ã€‚ä¸€ä¸ªå®šåˆ¶çš„16kå­—èŠ‚å¯¹ç¼–ç åˆ†è¯å™¨ä¸“é—¨åœ¨å°¼æ³Šå°”è¯­æ–‡æœ¬ä¸Šè®­ç»ƒï¼Œä»¥ç¡®ä¿æ›´ä¸€è‡´çš„åˆ†å‰²å’Œæ”¹è¿›çš„è¾“å…¥è¡¨ç¤ºã€‚è¯¥æ¨¡åž‹åœ¨ä¸€ä¸ªç»„åˆæ•°æ®é›†ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼Œè¯¥æ•°æ®é›†åŒ…æ‹¬10.75GBæ¸…æ´—åŽçš„å°¼æ³Šå°”BERTaè¯­æ–™åº“å’Œé¢å¤–çš„ç½‘ç»œçˆ¬å–çš„å°¼æ³Šå°”æ–°é—»æ–‡ç« ã€‚é›†æˆäº†FlashAttentionä»¥å‡å°‘å†…å­˜ä½¿ç”¨å¹¶ç¨³å®šè®­ç»ƒã€‚ç»è¿‡ä¸¤ä¸ªè®­ç»ƒå‘¨æœŸåŽï¼Œæ¨¡åž‹å®žçŽ°äº†3.168177çš„è®­ç»ƒæŸå¤±ã€3.081982çš„éªŒè¯æŸå¤±å’Œ21.80çš„æœ€ç»ˆå›°æƒ‘åº¦ï¼Œå±•ç¤ºäº†å…¶ç”Ÿæˆè¿žè´¯çš„å°¼æ³Šå°”æ–°é—»é£Žæ ¼æ–‡æœ¬çš„èƒ½åŠ›ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

è®ºæ–‡é‡‡ç”¨GPT-2ä½œä¸ºåŸºç¡€æž¶æž„ï¼Œæ ¸å¿ƒåˆ›æ–°åœ¨äºŽå®šåˆ¶16k BPEåˆ†è¯å™¨ä¸“é—¨é’ˆå¯¹å°¼æ³Šå°”è¯­è®­ç»ƒï¼Œç¡®ä¿æ›´å‡†ç¡®çš„åˆ†è¯å’Œè¾“å…¥è¡¨ç¤ºã€‚æ–¹æ³•æ•´åˆäº†å—GPT-3å¯å‘çš„è®­ç»ƒç­–ç•¥ï¼Œå¦‚ä¼˜åŒ–å­¦ä¹ çŽ‡è°ƒåº¦å’Œæ‰¹æ¬¡ç¼©æ”¾ï¼Œå¹¶å¼•å…¥FlashAttentionä»¥æå‡è®­ç»ƒæ•ˆçŽ‡å’Œç¨³å®šæ€§ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œä¸»è¦åŒºåˆ«åœ¨äºŽä»ŽåŸºç¡€ç¼–ç å™¨è½¬å‘ç”Ÿæˆå¼æ¨¡åž‹ï¼Œå¹¶é’ˆå¯¹å°¼æ³Šå°”è¯­ä½Žèµ„æºç‰¹æ€§è¿›è¡Œå®šåˆ¶åŒ–ä¼˜åŒ–ï¼Œè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•åœ¨æ–‡æœ¬ç”Ÿæˆä¸Šçš„ä¸è¶³ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

æ¨¡åž‹åœ¨è®­ç»ƒåŽè¾¾åˆ°21.80çš„å›°æƒ‘åº¦ï¼Œè®­ç»ƒæŸå¤±å’ŒéªŒè¯æŸå¤±åˆ†åˆ«ä¸º3.168177å’Œ3.081982ï¼ŒæˆåŠŸç”Ÿæˆè¿žè´¯çš„å°¼æ³Šå°”æ–°é—»é£Žæ ¼æ–‡æœ¬ï¼Œè¯æ˜Žäº†å®šåˆ¶åˆ†è¯å™¨å’Œé«˜æ•ˆè®­ç»ƒç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶å¯åº”ç”¨äºŽå°¼æ³Šå°”è¯­æ–°é—»è‡ªåŠ¨ç”Ÿæˆã€èŠå¤©æœºå™¨äººã€å†…å®¹åˆ›ä½œå’Œæœºå™¨ç¿»è¯‘ç­‰é¢†åŸŸï¼Œä¸ºä½Žèµ„æºè¯­è¨€çš„è‡ªç„¶è¯­è¨€å¤„ç†æä¾›å®žé™…è§£å†³æ–¹æ¡ˆï¼Œä¿ƒè¿›å°¼æ³Šå°”è¯­ç¤¾åŒºçš„æ•°å­—åŒ–å‘å±•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Nepali, a low-resource language spoken by over 32 million people, continues to face challenges in natural language processing (NLP) due to its complex grammar, agglutinative morphology, and limited availability of high-quality corpora. Most efforts to date have centered on basic encoder architectures; they remain insufficient for Nepali-specific text generation. This study presents a GPT-2-based Nepali language model trained using several training strategies inspired by GPT-3, including optimized learning rate schedules, batch scaling, and architectural refinements. A custom 16k Byte-Pair Encoding (BPE) tokenizer was trained exclusively on Nepali text to ensure more consistent segmentation and improved input representation. The model was pretrained on a combined dataset comprising a 10.75GB cleaned NepBERTa corpus and additional web-scraped Nepali news articles. FlashAttention was integrated to reduce memory usage and stabilize training. After two epochs, the model achieved a training loss of 3.168177, a validation loss of 3.081982, and a final perplexity of 21.80, demonstrating its capability to generate coherent Nepali news-style text.

