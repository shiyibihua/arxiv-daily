---
layout: default
title: TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs
---

# TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs

**arXiv**: [2512.14698v1](https://arxiv.org/abs/2512.14698) | [PDF](https://arxiv.org/pdf/2512.14698.pdf)

**ä½œè€…**: Jun Zhang, Teng Wang, Yuying Ge, Yixiao Ge, Xinhao Li, Ying Shan, Limin Wang

**åˆ†ç±»**: cs.CV, cs.AI, cs.CL, cs.MM

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**å¤‡æ³¨**: Project Page: https://timelens-arc-lab.github.io/

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTimeLensåŸºå‡†ä¸Žæ¨¡åž‹ï¼Œé€šè¿‡é«˜è´¨é‡æ•°æ®å’Œç®—æ³•è®¾è®¡æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹çš„è§†é¢‘æ—¶åºå®šä½èƒ½åŠ›**

**å…³é”®è¯**: `è§†é¢‘æ—¶åºå®šä½` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `åŸºå‡†è¯„ä¼°` `é«˜è´¨é‡æ•°æ®` `å¼ºåŒ–å­¦ä¹ è®­ç»ƒ` `è§†é¢‘ç†è§£` `å¼€æºæ¨¡åž‹` `ç®—æ³•è®¾è®¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰VTGåŸºå‡†å­˜åœ¨è´¨é‡é—®é¢˜ï¼Œå¯¼è‡´æ¨¡åž‹è¯„ä¼°ä¸å¯é ï¼Œä¸”è®­ç»ƒæ•°æ®å™ªå£°å¤§ï¼Œé™åˆ¶äº†MLLMsåœ¨è§†é¢‘æ—¶åºå®šä½ä¸­çš„æ€§èƒ½æå‡ã€‚
2. è®ºæ–‡ä»Žæ•°æ®è´¨é‡å’Œç®—æ³•è®¾è®¡å…¥æ‰‹ï¼Œæž„å»ºé«˜è´¨é‡åŸºå‡†TimeLens-Benchå’Œè®­ç»ƒé›†TimeLens-100Kï¼Œå¹¶è®¾è®¡äº¤æ›¿æ–‡æœ¬ç¼–ç å’ŒRLVRè®­ç»ƒèŒƒå¼ã€‚
3. TimeLensæ¨¡åž‹åœ¨VTGä»»åŠ¡ä¸­è¾¾åˆ°å¼€æºæ¨¡åž‹æœ€ä¼˜ï¼Œè¶…è¶ŠGPT-5ç­‰ä¸“æœ‰æ¨¡åž‹ï¼ŒéªŒè¯äº†é«˜è´¨é‡æ•°æ®å’Œç®—æ³•è®¾è®¡çš„æœ‰æ•ˆæ€§ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡å¹¶æœªæå‡ºæ–°æ–¹æ³•ï¼Œè€Œæ˜¯ä¸ºè§†é¢‘ç†è§£çš„æ ¸å¿ƒèƒ½åŠ›â€”â€”è§†é¢‘æ—¶åºå®šä½ï¼ˆVTGï¼‰å»ºç«‹äº†ä¸€ä¸ªç›´æŽ¥ã€æ¸è¿›ä½†è‡³å…³é‡è¦çš„åŸºçº¿ã€‚å°½ç®¡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹ï¼ˆMLLMsï¼‰åœ¨å¤šç§è§†é¢‘ç†è§£ä»»åŠ¡ä¸­è¡¨çŽ°å‡ºè‰²ï¼Œä½†ä¼˜åŒ–å…¶VTGèƒ½åŠ›çš„æ–¹æ¡ˆä»æœªè¢«å……åˆ†æŽ¢ç´¢ã€‚æœ¬æ–‡æå‡ºTimeLensï¼Œä»Žæ•°æ®è´¨é‡å’Œç®—æ³•è®¾è®¡ä¸¤ä¸ªä¸»è¦ç»´åº¦ï¼Œç³»ç»Ÿæ€§åœ°ç ”ç©¶å¦‚ä½•æž„å»ºå…·æœ‰å¼ºå¤§VTGèƒ½åŠ›çš„MLLMsã€‚æˆ‘ä»¬é¦–å…ˆæ­ç¤ºäº†çŽ°æœ‰VTGåŸºå‡†ä¸­çš„å…³é”®è´¨é‡é—®é¢˜ï¼Œå¹¶å¼•å…¥äº†TimeLens-Benchï¼Œå®ƒåŒ…å«ä¸‰ä¸ªæµè¡ŒåŸºå‡†çš„ç²¾å¿ƒé‡æ–°æ ‡æ³¨ç‰ˆæœ¬ï¼Œéµå¾ªä¸¥æ ¼çš„è´¨é‡æ ‡å‡†ã€‚æˆ‘ä»¬çš„åˆ†æžæ˜¾ç¤ºï¼Œä¸Žæ—§åŸºå‡†ç›¸æ¯”ï¼Œæ¨¡åž‹æŽ’åå‘ç”Ÿäº†æ˜¾è‘—å˜åŒ–ï¼Œè¯å®žäº†å…ˆå‰è¯„ä¼°æ ‡å‡†çš„ä¸å¯é æ€§ã€‚æˆ‘ä»¬è¿˜é€šè¿‡è‡ªåŠ¨é‡æ–°æ ‡æ³¨æµç¨‹è§£å†³äº†è®­ç»ƒæ•°æ®ä¸­çš„å™ªå£°é—®é¢˜ï¼Œç”Ÿæˆäº†TimeLens-100Kï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡ã€é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®é›†ã€‚åŸºäºŽæˆ‘ä»¬çš„æ•°æ®åŸºç¡€ï¼Œæˆ‘ä»¬æ·±å…¥æŽ¢ç´¢äº†ç®—æ³•è®¾è®¡åŽŸåˆ™ï¼Œå¾—å‡ºä¸€ç³»åˆ—æœ‰æ„ä¹‰çš„è§è§£å’Œæœ‰æ•ˆä¸”é«˜æ•ˆçš„å®žè·µã€‚è¿™äº›åŒ…æ‹¬ç”¨äºŽæ—¶é—´è¡¨ç¤ºçš„äº¤æ›¿æ–‡æœ¬ç¼–ç ã€ä½œä¸ºè®­ç»ƒèŒƒå¼çš„å…æ€è€ƒå¼ºåŒ–å­¦ä¹ ä¸Žå¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æ–¹æ³•ï¼Œä»¥åŠç²¾å¿ƒè®¾è®¡çš„RLVRè®­ç»ƒæ–¹æ¡ˆã€‚è¿™äº›åŠªåŠ›æœ€ç»ˆå½¢æˆäº†TimeLensæ¨¡åž‹ç³»åˆ—ï¼Œè¿™æ˜¯ä¸€ç»„åœ¨å¼€æºæ¨¡åž‹ä¸­å…·æœ‰æœ€å…ˆè¿›VTGæ€§èƒ½çš„MLLMsï¼Œç”šè‡³è¶…è¶Šäº†GPT-5å’ŒGemini-2.5-Flashç­‰ä¸“æœ‰æ¨¡åž‹ã€‚æ‰€æœ‰ä»£ç ã€æ•°æ®å’Œæ¨¡åž‹éƒ½å°†å‘å¸ƒä»¥ä¿ƒè¿›æœªæ¥ç ”ç©¶ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

TimeLensçš„æ•´ä½“æ¡†æž¶åŸºäºŽå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹ï¼Œä¸“æ³¨äºŽæå‡è§†é¢‘æ—¶åºå®šä½èƒ½åŠ›ã€‚å…³é”®æŠ€æœ¯åˆ›æ–°ç‚¹åŒ…æ‹¬ï¼šåœ¨æ•°æ®å±‚é¢ï¼Œé€šè¿‡ä¸¥æ ¼æ ‡å‡†é‡æ–°æ ‡æ³¨çŽ°æœ‰åŸºå‡†ï¼Œæž„å»ºTimeLens-Benchä»¥è§£å†³è¯„ä¼°é—®é¢˜ï¼Œå¹¶åˆ©ç”¨è‡ªåŠ¨æµç¨‹ç”Ÿæˆé«˜è´¨é‡è®­ç»ƒé›†TimeLens-100Kï¼›åœ¨ç®—æ³•å±‚é¢ï¼Œå¼•å…¥äº¤æ›¿æ–‡æœ¬ç¼–ç æ¥æœ‰æ•ˆè¡¨ç¤ºæ—¶é—´ä¿¡æ¯ï¼Œå¹¶é‡‡ç”¨å…æ€è€ƒå¼ºåŒ–å­¦ä¹ ä¸Žå¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰ä½œä¸ºè®­ç»ƒèŒƒå¼ï¼Œä¼˜åŒ–æ¨¡åž‹è¾“å‡ºã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•çš„ä¸»è¦åŒºåˆ«åœ¨äºŽï¼Œå®ƒç³»ç»Ÿæ€§åœ°æ•´åˆäº†é«˜è´¨é‡æ•°æ®æž„å»ºå’Œç®—æ³•è®¾è®¡ï¼Œè€Œéžå•ä¸€æŠ€æœ¯æ”¹è¿›ï¼Œå¼ºè°ƒåŸºå‡†å¯é æ€§å’Œè®­ç»ƒæ•ˆçŽ‡ï¼Œä¸ºVTGä»»åŠ¡æä¾›äº†å¯å¤çŽ°çš„åŸºçº¿æ–¹æ¡ˆã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

TimeLensæ¨¡åž‹åœ¨VTGä»»åŠ¡ä¸­è¾¾åˆ°å¼€æºæ¨¡åž‹æœ€ä¼˜æ€§èƒ½ï¼Œè¶…è¶ŠGPT-5å’ŒGemini-2.5-Flashç­‰ä¸“æœ‰æ¨¡åž‹ï¼›TimeLens-BenchåŸºå‡†æ­ç¤ºäº†æ—§åŸºå‡†çš„ä¸å¯é æ€§ï¼Œå¯¼è‡´æ¨¡åž‹æŽ’åæ˜¾è‘—å˜åŒ–ï¼›é«˜è´¨é‡æ•°æ®é›†TimeLens-100Kæœ‰æ•ˆæå‡äº†è®­ç»ƒæ•ˆæžœã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶å¯åº”ç”¨äºŽè§†é¢‘å†…å®¹åˆ†æžã€æ™ºèƒ½ç›‘æŽ§ã€è§†é¢‘æ£€ç´¢å’Œç¼–è¾‘ç­‰é¢†åŸŸï¼Œé€šè¿‡æå‡è§†é¢‘æ—¶åºå®šä½ç²¾åº¦ï¼Œæ”¯æŒæ›´å‡†ç¡®çš„è§†é¢‘ç†è§£ä»»åŠ¡ï¼Œå¦‚äº‹ä»¶æ£€æµ‹ã€åœºæ™¯åˆ†å‰²å’Œé—®ç­”ç³»ç»Ÿï¼Œå…·æœ‰å®žé™…ä»·å€¼ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> This paper does not introduce a novel method but instead establishes a straightforward, incremental, yet essential baseline for video temporal grounding (VTG), a core capability in video understanding. While multimodal large language models (MLLMs) excel at various video understanding tasks, the recipes for optimizing them for VTG remain under-explored. In this paper, we present TimeLens, a systematic investigation into building MLLMs with strong VTG ability, along two primary dimensions: data quality and algorithmic design. We first expose critical quality issues in existing VTG benchmarks and introduce TimeLens-Bench, comprising meticulously re-annotated versions of three popular benchmarks with strict quality criteria. Our analysis reveals dramatic model re-rankings compared to legacy benchmarks, confirming the unreliability of prior evaluation standards. We also address noisy training data through an automated re-annotation pipeline, yielding TimeLens-100K, a large-scale, high-quality training dataset. Building on our data foundation, we conduct in-depth explorations of algorithmic design principles, yielding a series of meaningful insights and effective yet efficient practices. These include interleaved textual encoding for time representation, a thinking-free reinforcement learning with verifiable rewards (RLVR) approach as the training paradigm, and carefully designed recipes for RLVR training. These efforts culminate in TimeLens models, a family of MLLMs with state-of-the-art VTG performance among open-source models and even surpass proprietary models such as GPT-5 and Gemini-2.5-Flash. All codes, data, and models will be released to facilitate future research.

