---
layout: default
title: PerfCoder: Large Language Models for Interpretable Code Performance Optimization
---

# PerfCoder: Large Language Models for Interpretable Code Performance Optimization

**arXiv**: [2512.14018v1](https://arxiv.org/abs/2512.14018) | [PDF](https://arxiv.org/pdf/2512.14018.pdf)

**ä½œè€…**: Jiuding Yang, Shengyao Lu, Hongxuan Liu, Shayan Shirahmad Gale Bagi, Zahra Fazel, Tomasz Czajkowski, Di Niu

**åˆ†ç±»**: cs.SE, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPerfCoderç³»åˆ—å¤§è¯­è¨€æ¨¡åž‹ï¼Œé€šè¿‡å¯è§£é‡Šçš„å®šåˆ¶åŒ–ä¼˜åŒ–ç”Ÿæˆé«˜æ€§èƒ½ä»£ç ï¼Œè§£å†³çŽ°æœ‰æ¨¡åž‹æ€§èƒ½ä¼˜åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **å¼ºåŒ–å­¦ä¹ **

**å…³é”®è¯**: `ä»£ç æ€§èƒ½ä¼˜åŒ–` `å¤§è¯­è¨€æ¨¡åž‹` `å¯è§£é‡Šæ€§` `å¼ºåŒ–å­¦ä¹ å¾®è°ƒ` `è½¯ä»¶å·¥ç¨‹` `è‡ªåŠ¨ä»£ç ç”Ÿæˆ` `ä¼˜åŒ–ç­–ç•¥` `è¿è¡Œæ—¶æµ‹é‡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰å¤§è¯­è¨€æ¨¡åž‹åœ¨ä»£ç ç”Ÿæˆä¸­ç¼ºä¹æ€§èƒ½ä¼˜åŒ–ç›‘ç£ï¼Œéš¾ä»¥ç”Ÿæˆé«˜æ€§èƒ½ä»£ç ï¼Œä¸»è¦å—é™äºŽæ•°æ®ç¨€ç¼ºå’Œç­–ç•¥æŒ‡å¯¼ä¸è¶³ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæå‡ºPerfCoderæ¨¡åž‹ï¼Œé€šè¿‡åŸºäºŽçœŸå®žä¼˜åŒ–è½¨è¿¹çš„å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ï¼Œå®žçŽ°å¯è§£é‡Šçš„å®šåˆ¶åŒ–ä»£ç ä¼˜åŒ–ï¼Œç›´æŽ¥åº”ç”¨æ”¹è¿›ç­–ç•¥ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨PIEåŸºå‡†æµ‹è¯•ä¸­ï¼ŒPerfCoderåœ¨è¿è¡Œæ—¶åŠ é€Ÿå’Œä¼˜åŒ–çŽ‡ä¸Šè¶…è¶Šæ‰€æœ‰çŽ°æœ‰æ¨¡åž‹ï¼Œå¹¶æå‡32Bæ¨¡åž‹å’ŒGPT-5çš„ä»£ç ä¼˜åŒ–æ€§èƒ½ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è¯­è¨€æ¨¡åž‹åœ¨è‡ªåŠ¨ä»£ç ç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶ç”Ÿæˆé«˜æ€§èƒ½ä»£ç çš„èƒ½åŠ›ä»ç„¶æœ‰é™â€”â€”è¿™æ˜¯çŽ°å®žä¸–ç•Œè½¯ä»¶ç³»ç»Ÿä¸­çš„å…³é”®éœ€æ±‚ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œå½“å‰å¤§è¯­è¨€æ¨¡åž‹ä¹‹æ‰€ä»¥éš¾ä»¥èƒœä»»ï¼Œä¸ä»…æ˜¯å› ä¸ºæ•°æ®ç¨€ç¼ºï¼Œæ›´é‡è¦çš„æ˜¯ç¼ºä¹æŒ‡å¯¼å¯è§£é‡Šä¸”æœ‰æ•ˆæ€§èƒ½æ”¹è¿›çš„ç›‘ç£ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†PerfCoderï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨è®¾è®¡ç”¨äºŽé€šè¿‡å¯è§£é‡Šçš„å®šåˆ¶åŒ–ä¼˜åŒ–ä»Žæºä»£ç ç”Ÿæˆæ€§èƒ½å¢žå¼ºä»£ç çš„å¤§è¯­è¨€æ¨¡åž‹å®¶æ—ã€‚PerfCoderåœ¨ç²¾å¿ƒç­–åˆ’çš„çœŸå®žä¸–ç•Œä¼˜åŒ–è½¨è¿¹é›†åˆä¸Šè¿›è¡Œäº†å¾®è°ƒï¼Œè¿™äº›è½¨è¿¹å¸¦æœ‰å¯è¯»çš„äººå·¥æ³¨é‡Šï¼Œå¹¶é€šè¿‡ä½¿ç”¨è¿è¡Œæ—¶æµ‹é‡çš„å¼ºåŒ–å¾®è°ƒè¿›è¡Œåå¥½å¯¹é½ï¼Œä½¿å…¶èƒ½å¤Ÿæå‡ºé’ˆå¯¹ç‰¹å®šè¾“å…¥çš„æ”¹è¿›ç­–ç•¥å¹¶ç›´æŽ¥åº”ç”¨ï¼Œè€Œæ— éœ€ä¾èµ–è¿­ä»£ä¼˜åŒ–ã€‚åœ¨PIEä»£ç æ€§èƒ½åŸºå‡†æµ‹è¯•ä¸­ï¼ŒPerfCoderåœ¨è¿è¡Œæ—¶åŠ é€Ÿå’Œæœ‰æ•ˆä¼˜åŒ–çŽ‡æ–¹é¢å‡è¶…è¶Šäº†æ‰€æœ‰çŽ°æœ‰æ¨¡åž‹ï¼Œè¡¨æ˜Žæ€§èƒ½ä¼˜åŒ–ä¸èƒ½ä»…é€šè¿‡è§„æ¨¡å®žçŽ°ï¼Œè€Œéœ€è¦ä¼˜åŒ–ç­–ç•¥æ„è¯†ã€‚æ­¤å¤–ï¼ŒPerfCoderå¯ä»¥ç”Ÿæˆå…³äºŽæºä»£ç çš„å¯è§£é‡Šåé¦ˆï¼Œå½“åœ¨è§„åˆ’å™¨ä¸Žä¼˜åŒ–å™¨ååŒå·¥ä½œæµç¨‹ä¸­ä½œä¸ºè¾“å…¥æä¾›ç»™æ›´å¤§çš„å¤§è¯­è¨€æ¨¡åž‹æ—¶ï¼Œå¯ä»¥è¿›ä¸€æ­¥æ”¹å–„ç»“æžœã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†32Bæ¨¡åž‹å’ŒGPT-5åœ¨ä»£ç ä¼˜åŒ–æ–¹é¢çš„æ€§èƒ½æå‡åˆ°äº†æ–°æ°´å¹³ï¼Œå¤§å¹…è¶…è¶Šäº†å®ƒä»¬çš„åŽŸå§‹æ€§èƒ½ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡åž‹åœ¨è‡ªåŠ¨ä»£ç ç”Ÿæˆä¸­éš¾ä»¥ç”Ÿæˆé«˜æ€§èƒ½ä»£ç çš„é—®é¢˜ã€‚çŽ°æœ‰æ–¹æ³•çš„ç—›ç‚¹åŒ…æ‹¬ï¼šæ¨¡åž‹ç¼ºä¹é’ˆå¯¹æ€§èƒ½ä¼˜åŒ–çš„ç›‘ç£ï¼Œå¯¼è‡´ç”Ÿæˆçš„ä»£ç æ•ˆçŽ‡ä½Žä¸‹ï¼›ä¾èµ–è¿­ä»£ä¼˜åŒ–è¿‡ç¨‹ï¼Œå¢žåŠ äº†è®¡ç®—å¼€é”€ï¼›ä¸”çŽ°æœ‰æ¨¡åž‹é€šå¸¸ä»…å…³æ³¨ä»£ç åŠŸèƒ½æ­£ç¡®æ€§ï¼Œè€Œå¿½è§†è¿è¡Œæ—¶æ€§èƒ½ï¼Œè¿™åœ¨çŽ°å®žè½¯ä»¶ç³»ç»Ÿä¸­è‡³å…³é‡è¦ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒè§£å†³æ€è·¯æ˜¯è®¾è®¡ä¸€ä¸ªä¸“é—¨ç”¨äºŽä»£ç æ€§èƒ½ä¼˜åŒ–çš„å¤§è¯­è¨€æ¨¡åž‹PerfCoderï¼Œé€šè¿‡å¼•å…¥å¯è§£é‡Šçš„ä¼˜åŒ–ç­–ç•¥å’Œç›‘ç£ä¿¡å·ï¼Œä½¿æ¨¡åž‹èƒ½å¤Ÿç›´æŽ¥ä»Žæºä»£ç ç”Ÿæˆæ€§èƒ½å¢žå¼ºçš„ä»£ç ã€‚è¿™æ ·è®¾è®¡çš„åŽŸå› åœ¨äºŽï¼Œæ€§èƒ½ä¼˜åŒ–éœ€è¦æ¨¡åž‹ç†è§£ä»£ç çš„è¿è¡Œæ—¶è¡Œä¸ºå¹¶åº”ç”¨å®šåˆ¶åŒ–æ”¹è¿›ï¼Œè€Œä¸ä»…ä»…æ˜¯æ‰©å¤§æ¨¡åž‹è§„æ¨¡æˆ–ä¾èµ–é€šç”¨ä»£ç ç”Ÿæˆæ•°æ®ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šæ•´ä½“æž¶æž„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆï¼ŒåŸºäºŽç²¾å¿ƒç­–åˆ’çš„çœŸå®žä¸–ç•Œä¼˜åŒ–è½¨è¿¹é›†åˆè¿›è¡Œå¾®è°ƒï¼Œè¿™äº›è½¨è¿¹å¸¦æœ‰å¯è¯»çš„äººå·¥æ³¨é‡Šï¼Œä»¥æä¾›ç›‘ç£ä¿¡å·ï¼›å…¶æ¬¡ï¼Œé€šè¿‡å¼ºåŒ–å¾®è°ƒä½¿ç”¨è¿è¡Œæ—¶æµ‹é‡è¿›è¡Œåå¥½å¯¹é½ï¼Œç¡®ä¿æ¨¡åž‹æå‡ºçš„ä¼˜åŒ–ç­–ç•¥èƒ½æœ‰æ•ˆæå‡æ€§èƒ½ã€‚æµç¨‹ä¸Šï¼Œæ¨¡åž‹æŽ¥æ”¶æºä»£ç ä½œä¸ºè¾“å…¥ï¼Œç›´æŽ¥è¾“å‡ºä¼˜åŒ–åŽçš„ä»£ç å’Œå¯è§£é‡Šçš„åé¦ˆï¼Œæ— éœ€è¿­ä»£æ­¥éª¤ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºŽå°†å¯è§£é‡Šæ€§å’Œæ€§èƒ½ä¼˜åŒ–ç»“åˆåˆ°å¤§è¯­è¨€æ¨¡åž‹ä¸­ï¼Œé€šè¿‡ä¼˜åŒ–è½¨è¿¹çš„ç›‘ç£å’Œå¼ºåŒ–å­¦ä¹ å¯¹é½ï¼Œä½¿æ¨¡åž‹å…·å¤‡ä¼˜åŒ–ç­–ç•¥æ„è¯†ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºŽï¼ŒPerfCoderä¸ä¾èµ–è¿­ä»£ä¼˜åŒ–æˆ–é€šç”¨ä»£ç ç”Ÿæˆï¼Œè€Œæ˜¯ä¸“é—¨é’ˆå¯¹æ€§èƒ½æ”¹è¿›è¿›è¡Œè®­ç»ƒï¼Œä»Žè€Œæ›´é«˜æ•ˆåœ°ç”Ÿæˆé«˜æ€§èƒ½ä»£ç ã€‚

**å…³é”®è®¾è®¡**ï¼šæŠ€æœ¯ç»†èŠ‚åŒ…æ‹¬ï¼šä½¿ç”¨çœŸå®žä¸–ç•Œä¼˜åŒ–è½¨è¿¹æ•°æ®é›†è¿›è¡Œå¾®è°ƒï¼Œè¿™äº›æ•°æ®åŒ…å«æºä»£ç ã€ä¼˜åŒ–ç‰ˆæœ¬å’Œäººå·¥æ³¨é‡Šï¼›é‡‡ç”¨å¼ºåŒ–å­¦ä¹ æ¡†æž¶ï¼Œä»¥è¿è¡Œæ—¶æµ‹é‡ä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œå¯¹æ¨¡åž‹è¿›è¡Œåå¥½å¯¹é½ï¼›æ¨¡åž‹æž¶æž„åŸºäºŽå¤§è¯­è¨€æ¨¡åž‹ï¼Œå…·ä½“å‚æ•°è®¾ç½®æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†è¯´æ˜Žï¼Œä½†å¼ºè°ƒé€šè¿‡ç›‘ç£å’Œå¼ºåŒ–å¾®è°ƒæ¥æå‡æ€§èƒ½ä¼˜åŒ–èƒ½åŠ›ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

åœ¨PIEä»£ç æ€§èƒ½åŸºå‡†æµ‹è¯•ä¸­ï¼ŒPerfCoderåœ¨è¿è¡Œæ—¶åŠ é€Ÿå’Œæœ‰æ•ˆä¼˜åŒ–çŽ‡æ–¹é¢å‡è¶…è¶Šæ‰€æœ‰çŽ°æœ‰æ¨¡åž‹ï¼Œå…·ä½“æ€§èƒ½æ•°æ®æœªåœ¨æ‘˜è¦ä¸­ç»™å‡ºï¼Œä½†å¼ºè°ƒäº†å…¶æ˜¾è‘—ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼ŒPerfCoderé€šè¿‡å¯è§£é‡Šåé¦ˆï¼Œåœ¨è§„åˆ’å™¨ä¸Žä¼˜åŒ–å™¨ååŒå·¥ä½œæµç¨‹ä¸­ï¼Œå°†32Bæ¨¡åž‹å’ŒGPT-5çš„ä»£ç ä¼˜åŒ–æ€§èƒ½æå‡åˆ°æ–°æ°´å¹³ï¼Œå¤§å¹…è¶…è¶ŠåŽŸå§‹æ€§èƒ½ï¼Œè¯æ˜Žäº†ä¼˜åŒ–ç­–ç•¥æ„è¯†çš„é‡è¦æ€§ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨è½¯ä»¶å·¥ç¨‹å’Œç³»ç»Ÿä¼˜åŒ–é¢†åŸŸå…·æœ‰å¹¿æ³›æ½œåœ¨åº”ç”¨ï¼Œå¯ç”¨äºŽè‡ªåŠ¨ä»£ç æ€§èƒ½è°ƒä¼˜ã€ç¼–è¯‘å™¨ä¼˜åŒ–è¾…åŠ©ã€å®žæ—¶ç³»ç»Ÿä»£ç ç”Ÿæˆç­‰åœºæ™¯ã€‚å®žé™…ä»·å€¼åœ¨äºŽæå‡è½¯ä»¶è¿è¡Œæ•ˆçŽ‡ï¼Œå‡å°‘èµ„æºæ¶ˆè€—ï¼Œæœªæ¥å¯èƒ½æŽ¨åŠ¨æ™ºèƒ½ç¼–ç¨‹åŠ©æ‰‹çš„å‘å±•ï¼Œä½¿å¼€å‘è€…æ›´ä¸“æ³¨äºŽé«˜å±‚æ¬¡è®¾è®¡è€Œéžåº•å±‚ä¼˜åŒ–ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large language models (LLMs) have achieved remarkable progress in automatic code generation, yet their ability to produce high-performance code remains limited--a critical requirement in real-world software systems. We argue that current LLMs struggle not only due to data scarcity but, more importantly, because they lack supervision that guides interpretable and effective performance improvements. In this work, we introduce PerfCoder, a family of LLMs specifically designed to generate performance-enhanced code from source code via interpretable, customized optimizations. PerfCoder is fine-tuned on a curated collection of real-world optimization trajectories with human-readable annotations, and preference-aligned by reinforcement fine-tuning using runtime measurements, enabling it to propose input-specific improvement strategies and apply them directly without relying on iterative refinement. On the PIE code performance benchmark, PerfCoder surpasses all existing models in both runtime speedup and effective optimization rate, demonstrating that performance optimization cannot be achieved by scale alone but requires optimization stratetgy awareness. In addition, PerfCoder can generate interpretable feedback about the source code, which, when provided as input to a larger LLM in a planner-and-optimizer cooperative workflow, can further improve outcomes. Specifically, we elevate the performance of 32B models and GPT-5 to new levels on code optimization, substantially surpassing their original performance.

