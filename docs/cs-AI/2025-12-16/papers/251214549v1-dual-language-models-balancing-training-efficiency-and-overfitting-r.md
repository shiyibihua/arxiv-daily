---
layout: default
title: Dual Language Models: Balancing Training Efficiency and Overfitting Resilience
---

# Dual Language Models: Balancing Training Efficiency and Overfitting Resilience

**arXiv**: [2512.14549v1](https://arxiv.org/abs/2512.14549) | [PDF](https://arxiv.org/pdf/2512.14549.pdf)

**ä½œè€…**: David Samuel, Lucas Georges Gabriel Charpentier

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŒç›®æ ‡è®­ç»ƒæ–¹æ³•ä»¥å¹³è¡¡è¯­è¨€æ¨¡åž‹è®­ç»ƒæ•ˆçŽ‡ä¸Žè¿‡æ‹Ÿåˆé²æ£’æ€§**

**å…³é”®è¯**: `åŒç›®æ ‡è®­ç»ƒ` `è‡ªå›žå½’æ¨¡åž‹` `æŽ©ç æ‰©æ•£æ¨¡åž‹` `è®­ç»ƒæ•ˆçŽ‡` `è¿‡æ‹Ÿåˆé²æ£’æ€§` `è¯­è¨€æ¨¡åž‹ä¼˜åŒ–` `ä¸‹æ¸¸æ€§èƒ½` `æ•°æ®é‡å¤`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè‡ªå›žå½’æ¨¡åž‹è®­ç»ƒæ•ˆçŽ‡é«˜ä½†æ˜“è¿‡æ‹Ÿåˆï¼ŒæŽ©ç æ‰©æ•£æ¨¡åž‹é²æ£’æ€§å¼ºä½†è®­ç»ƒæ•ˆçŽ‡ä½Žï¼ŒçŽ°æœ‰å•ç›®æ ‡æ–¹æ³•éš¾ä»¥å¹³è¡¡ä¸¤è€…ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆè‡ªå›žå½’å’ŒæŽ©ç æ‰©æ•£è®­ç»ƒç›®æ ‡ï¼Œé€šè¿‡åŒç›®æ ‡è®­ç»ƒå®žçŽ°æ•ˆçŽ‡ä¸Žé²æ£’æ€§çš„å¹³è¡¡ï¼Œæ— éœ€æž¶æž„ä¿®æ”¹ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨50ä¸ªæ¨¡åž‹å®žéªŒä¸­ï¼ŒåŒç›®æ ‡è®­ç»ƒåœ¨æ‰€æœ‰è®¾ç½®ä¸‹å‡ä¼˜äºŽå•ç›®æ ‡ï¼Œæœ€ä¼˜æ¯”ä¾‹ç¨³å®šï¼Œæå‡äº†ä¸‹æ¸¸æ€§èƒ½ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç»“åˆè‡ªå›žå½’å’ŒæŽ©ç æ‰©æ•£è®­ç»ƒç›®æ ‡ï¼Œæ— éœ€ä¿®æ”¹æ¨¡åž‹æž¶æž„ï¼Œå®žçŽ°äº†çµæ´»çš„è¯­è¨€æ¨¡åž‹ï¼Œå…¶æ€§èƒ½ä¼˜äºŽå•ç›®æ ‡æ¨¡åž‹ã€‚è‡ªå›žå½’å»ºæ¨¡å› å…¶è®­ç»ƒæ•ˆçŽ‡é«˜è€Œæµè¡Œï¼Œä½†ä»£ä»·æ˜¯å¯¹è¿‡æ‹Ÿåˆæ•æ„Ÿï¼›è€ŒæŽ©ç æ‰©æ•£æ¨¡åž‹è®­ç»ƒæ•ˆçŽ‡è¾ƒä½Žï¼Œä½†å¯¹è¿‡æ‹Ÿåˆæ›´å…·é²æ£’æ€§ã€‚æœ¬å·¥ä½œè¯æ˜ŽåŒç›®æ ‡è®­ç»ƒèƒ½å…¼é¡¾ä¸¤è€…ä¼˜åŠ¿ã€‚ä¸ºç¡®å®šä¸¤ä¸ªç›®æ ‡ä¹‹é—´çš„æœ€ä¼˜æ¯”ä¾‹ï¼Œæˆ‘ä»¬åœ¨ä¸åŒæ•°æ®é‡å¤æ°´å¹³ä¸‹è®­ç»ƒå’Œè¯„ä¼°äº†50ä¸ªè¯­è¨€æ¨¡åž‹ã€‚ç»“æžœè¡¨æ˜Žï¼Œåœ¨æ‰€æœ‰è¯„ä¼°è®¾ç½®ä¸‹ï¼Œç»“åˆä¸¤ä¸ªç›®æ ‡æ˜¯æœ€ä¼˜çš„ï¼Œä¸”æ— è®ºé’ˆå¯¹è‡ªå›žå½’è¿˜æ˜¯æŽ©ç æ‰©æ•£ä¸‹æ¸¸æ€§èƒ½ï¼Œæœ€ä¼˜æ¯”ä¾‹ç›¸ä¼¼ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

è®ºæ–‡æå‡ºä¸€ç§åŒç›®æ ‡è®­ç»ƒæ¡†æž¶ï¼Œæ ¸å¿ƒæ–¹æ³•æ˜¯åœ¨æ ‡å‡†è¯­è¨€æ¨¡åž‹æž¶æž„åŸºç¡€ä¸Šï¼ŒåŒæ—¶åº”ç”¨è‡ªå›žå½’å’ŒæŽ©ç æ‰©æ•£è®­ç»ƒç›®æ ‡ã€‚å…³é”®æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºŽæ— éœ€ä¿®æ”¹æ¨¡åž‹æž¶æž„ï¼Œé€šè¿‡ä¼˜åŒ–ä¸¤ä¸ªç›®æ ‡çš„æ··åˆæ¯”ä¾‹ï¼Œå®žçŽ°è®­ç»ƒæ•ˆçŽ‡å’Œè¿‡æ‹Ÿåˆé²æ£’æ€§çš„å¹³è¡¡ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•çš„ä¸»è¦åŒºåˆ«åœ¨äºŽï¼ŒçŽ°æœ‰å·¥ä½œé€šå¸¸ä¸“æ³¨äºŽå•ä¸€ç›®æ ‡ï¼Œè€Œæœ¬æ–¹æ³•ç»“åˆäº†ä¸¤ç§ç›®æ ‡çš„ä¼˜åŠ¿ï¼Œé¿å…äº†å„è‡ªç¼ºç‚¹ï¼Œæä¾›äº†ä¸€ç§æ›´çµæ´»çš„è®­ç»ƒç­–ç•¥ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

æœ€é‡è¦çš„å®žéªŒç»“æžœæ˜¯ï¼ŒåŒç›®æ ‡è®­ç»ƒåœ¨æ‰€æœ‰è¯„ä¼°è®¾ç½®ä¸‹å‡ä¼˜äºŽå•ç›®æ ‡æ¨¡åž‹ï¼Œæœ€ä¼˜æ··åˆæ¯”ä¾‹åœ¨ä¸åŒæ•°æ®é‡å¤æ°´å¹³ä¸‹ä¿æŒç›¸ä¼¼ï¼Œæ˜¾è‘—æå‡äº†ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ï¼Œè¯æ˜Žäº†æ–¹æ³•çš„æ™®é€‚æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶å¯åº”ç”¨äºŽè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸï¼Œå¦‚æ–‡æœ¬ç”Ÿæˆã€æœºå™¨ç¿»è¯‘å’Œè¯­è¨€ç†è§£ä»»åŠ¡ï¼Œé€šè¿‡æå‡æ¨¡åž‹è®­ç»ƒæ•ˆçŽ‡å’Œé²æ£’æ€§ï¼Œé™ä½Žè¿‡æ‹Ÿåˆé£Žé™©ï¼Œé€‚ç”¨äºŽæ•°æ®æœ‰é™æˆ–é‡å¤åœºæ™¯ï¼Œå…·æœ‰å®žé™…ä»·å€¼ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> This paper combines autoregressive and masked-diffusion training objectives without any architectural modifications, resulting in flexible language models that outperform single-objective models. Autoregressive modeling has been a popular approach, partly because of its training efficiency; however, that comes at the cost of sensitivity to overfitting. On the other hand, masked-diffusion models are less efficient to train while being more resilient to overfitting. In this work, we demonstrate that dual-objective training achieves the best of both worlds. To derive the optimal ratio between both objectives, we train and evaluate 50 language models under varying levels of data repetition. We show that it is optimal to combine both objectives under all evaluated settings and that the optimal ratio is similar whether targeting autoregressive or masked-diffusion downstream performance.

