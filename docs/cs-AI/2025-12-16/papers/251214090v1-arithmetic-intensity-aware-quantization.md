---
layout: default
title: Arithmetic-Intensity-Aware Quantization
---

# Arithmetic-Intensity-Aware Quantization

**arXiv**: [2512.14090v1](https://arxiv.org/abs/2512.14090) | [PDF](https://arxiv.org/pdf/2512.14090.pdf)

**ä½œè€…**: Taig Singh, Shreshth Rajan, Nikhil Iyer

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç®—æœ¯å¼ºåº¦æ„ŸçŸ¥é‡åŒ–ï¼ˆAIQï¼‰æ¡†æž¶ï¼Œé€šè¿‡æ··åˆç²¾åº¦é‡åŒ–è§£å†³å†…å­˜å¸¦å®½é™åˆ¶ä¸‹çš„ç¥žç»ç½‘ç»œæŽ¨ç†æ€§èƒ½é—®é¢˜ã€‚**

**å…³é”®è¯**: `æ··åˆç²¾åº¦é‡åŒ–` `ç®—æœ¯å¼ºåº¦ä¼˜åŒ–` `åŽè®­ç»ƒé‡åŒ–` `å†…å­˜å¸¦å®½é™åˆ¶` `ç¥žç»ç½‘ç»œæŽ¨ç†` `æ¨¡åž‹åŽ‹ç¼©` `è¾¹ç¼˜è®¡ç®—` `æ€§èƒ½æå‡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°ä»£ç¥žç»ç½‘ç»œæŽ¨ç†å—å†…å­˜å¸¦å®½é™åˆ¶ï¼Œè€Œéžè®¡ç®—èƒ½åŠ›ï¼Œå¯¼è‡´åžåé‡ç“¶é¢ˆï¼ŒçŽ°æœ‰é‡åŒ–æ–¹æ³•å¦‚å…¨å±€å‡åŒ€é‡åŒ–å¯èƒ½æœªå……åˆ†è€ƒè™‘å†…å­˜æ•ˆçŽ‡ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæå‡ºAIQæ¡†æž¶ï¼Œé€šè¿‡åŽè®­ç»ƒæœç´¢ç®—æ³•ä¼˜åŒ–æ¯å±‚ä½å®½ï¼Œä»¥æœ€å¤§åŒ–ç®—æœ¯å¼ºåº¦å¹¶æœ€å°åŒ–ç²¾åº¦æŸå¤±ï¼Œå®žçŽ°æ··åˆç²¾åº¦é‡åŒ–ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨ResNet-20/CIFAR-10ä¸Šæå‡ç®—æœ¯å¼ºåº¦çº¦50%ï¼ŒMobileNetV2ä¸Šåžåé‡æé«˜1.66å€ï¼Œå‡ä¿æŒç²¾åº¦æŸå¤±åœ¨1ä¸ªç™¾åˆ†ç‚¹å†…ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€çŽ°ä»£ç¥žç»ç½‘ç»œæ—¥ç›Šå—å†…å­˜é™åˆ¶ï¼ŒæŽ¨ç†åžåé‡æ›´å¤šåœ°å—DRAMå¸¦å®½è€Œéžè®¡ç®—èƒ½åŠ›åˆ¶çº¦ã€‚æœ¬æ–‡æå‡ºäº†ç®—æœ¯å¼ºåº¦æ„ŸçŸ¥é‡åŒ–ï¼ˆAIQï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ··åˆç²¾åº¦é‡åŒ–æ¡†æž¶ï¼Œé€šè¿‡é€‰æ‹©æ¯å±‚ä½å®½æ¥æœ€å¤§åŒ–ç®—æœ¯å¼ºåº¦ï¼ˆAIï¼‰åŒæ—¶æœ€å°åŒ–ç²¾åº¦æŸå¤±ã€‚AIQæ˜¯ä¸€ç§åŽè®­ç»ƒé‡åŒ–æ–¹æ³•ï¼Œä½¿ç”¨æœç´¢ç®—æ³•ä¼˜åŒ–æ¯å±‚é‡åŒ–æ–¹æ¡ˆï¼Œä»¥æœ€å°åŒ–ç®—æœ¯å¼ºåº¦å’Œç²¾åº¦çš„åŠ æƒæŸå¤±ã€‚åœ¨ResNet-20/CIFAR-10ä¸Šï¼ŒAIQç›¸æ¯”FP32åŸºçº¿å°†ç®—æœ¯å¼ºåº¦æé«˜äº†çº¦50%ï¼ŒåŒæ—¶ä¿æŒæµ‹è¯•ç²¾åº¦åœ¨çº¦1ä¸ªç™¾åˆ†ç‚¹å†…ï¼Œå¹¶ä¼˜äºŽå…¨å±€å‡åŒ€é‡åŒ–æ–¹æ¡ˆã€‚åœ¨å†…å­˜å—é™çš„MobileNetV2æž¶æž„ä¸Šï¼ŒAIQé…ç½®å®žçŽ°äº†æ¯”FP32åŸºçº¿é«˜1.66å€çš„åžåé‡ï¼ŒåŒæ—¶ä¿æŒæµ‹è¯•ç²¾åº¦åœ¨1ä¸ªç™¾åˆ†ç‚¹å†…ã€‚æˆ‘ä»¬è¿˜å‘çŽ°AIQè‡ªç„¶åœ°æ›´æ¿€è¿›åœ°é‡åŒ–è¾ƒå¤§çš„å±‚ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

AIQæ˜¯ä¸€ä¸ªåŽè®­ç»ƒæ··åˆç²¾åº¦é‡åŒ–æ¡†æž¶ï¼Œæ•´ä½“æ¡†æž¶åŒ…æ‹¬åŸºäºŽæœç´¢ç®—æ³•ä¼˜åŒ–æ¯å±‚é‡åŒ–ä½å®½ï¼Œä»¥æœ€å°åŒ–ç®—æœ¯å¼ºåº¦å’Œç²¾åº¦çš„åŠ æƒæŸå¤±å‡½æ•°ã€‚å…³é”®æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºŽå¼•å…¥ç®—æœ¯å¼ºåº¦ä½œä¸ºé‡åŒ–å†³ç­–çš„æ ¸å¿ƒæŒ‡æ ‡ï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´ä½å®½æ¥å¹³è¡¡å†…å­˜å¸¦å®½åˆ©ç”¨å’Œæ¨¡åž‹ç²¾åº¦ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•çš„ä¸»è¦åŒºåˆ«åœ¨äºŽï¼ŒAIQä¸ä¾èµ–å…¨å±€ç»Ÿä¸€é‡åŒ–ï¼Œè€Œæ˜¯é’ˆå¯¹æ¯å±‚ç‰¹æ€§è¿›è¡Œå®šåˆ¶åŒ–é‡åŒ–ï¼Œæ›´æœ‰æ•ˆåœ°ç¼“è§£å†…å­˜ç“¶é¢ˆï¼Œç›¸æ¯”ä¼ ç»Ÿæ–¹æ³•å¦‚å…¨å±€å‡åŒ€é‡åŒ–ï¼Œèƒ½æ›´å¥½åœ°æå‡æŽ¨ç†æ€§èƒ½ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

åœ¨ResNet-20/CIFAR-10ä¸Šï¼ŒAIQå°†ç®—æœ¯å¼ºåº¦æé«˜çº¦50%ï¼Œç²¾åº¦æŸå¤±æŽ§åˆ¶åœ¨1ä¸ªç™¾åˆ†ç‚¹å†…ï¼›åœ¨MobileNetV2ä¸Šï¼Œåžåé‡æå‡1.66å€ï¼ŒåŒæ ·ä¿æŒç²¾åº¦ç¨³å®šï¼Œæ˜¾è‘—ä¼˜äºŽåŸºçº¿æ–¹æ³•ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶é€‚ç”¨äºŽè¾¹ç¼˜è®¡ç®—ã€ç§»åŠ¨è®¾å¤‡å’Œç‰©è”ç½‘ç­‰å†…å­˜å—é™åœºæ™¯ï¼Œé€šè¿‡ä¼˜åŒ–ç¥žç»ç½‘ç»œæŽ¨ç†çš„å†…å­˜æ•ˆçŽ‡ï¼Œæå‡å®žæ—¶åº”ç”¨å¦‚è®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†çš„éƒ¨ç½²æ€§èƒ½ï¼Œå…·æœ‰å®žé™…ä»·å€¼ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> As modern neural networks become increasingly memory-bound, inference throughput is limited by DRAM bandwidth rather than compute. We present Arithmetic-Intensity-Aware Quantization (AIQ), a mixed precision quantization framework that chooses per-layer bit-widths to maximize arithmetic intensity (AI) while minimizing accuracy loss. AIQ is a post-training quantization method that uses search algorithms over per-layer quantization schemes to minimize a weighted loss over AI and accuracy. On ResNet-20/CIFAR-10, AIQ increases AI by ~50% over an FP32 baseline while keeping test accuracy within ~1 percentage point, and outperforming global uniform quantization schemes. On a memory-bound MobileNetV2 architecture, AIQ configurations give a 1.66x higher throughput than the FP32 baseline while keeping test accuracy within 1 percentage point. We also find that AIQ naturally quantizes larger layers more aggressively.

