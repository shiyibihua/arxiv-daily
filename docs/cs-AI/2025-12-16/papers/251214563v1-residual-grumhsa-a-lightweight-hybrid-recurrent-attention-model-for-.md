---
layout: default
title: Residual GRU+MHSA: A Lightweight Hybrid Recurrent Attention Model for Cardiovascular Disease Detection
---

# Residual GRU+MHSA: A Lightweight Hybrid Recurrent Attention Model for Cardiovascular Disease Detection

**arXiv**: [2512.14563v1](https://arxiv.org/abs/2512.14563) | [PDF](https://arxiv.org/pdf/2512.14563.pdf)

**ä½œè€…**: Tejaswani Dash, Gautam Datla, Anudeep Vurity, Tazeem Ahmad, Mohd Adnan, Saima Rafi, Saisha Patro, Saina Patro

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**å¤‡æ³¨**: Accepted in IEEE Bigdata 2025- Learning Representations with Limited Supervision

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºResidual GRU+MHSAè½»é‡æ··åˆå¾ªç¯æ³¨æ„åŠ›æ¨¡å‹ï¼Œä»¥æå‡å¿ƒè¡€ç®¡ç–¾ç—…æ£€æµ‹çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **è§†è§‰é‡Œç¨‹è®¡** **å¼ºåŒ–å­¦ä¹ **

<<<<<<< HEAD
**å…³é”®è¯**: `å¿ƒè¡€ç®¡ç–¾ç—…æ£€æµ‹` `è½»é‡æ··åˆæ¶æ„` `æ®‹å·®åŒå‘GRU` `å¤šå¤´è‡ªæ³¨æ„åŠ›` `è¡¨æ ¼æ•°æ®å»ºæ¨¡` `ä¸´åºŠé£é™©é¢„æµ‹` `èµ„æºå—é™éƒ¨ç½²` `æ·±åº¦å­¦ä¹ ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¿ƒè¡€ç®¡ç–¾ç—…æ£€æµ‹ä¾èµ–ä¼ ç»Ÿæ‰‹å·¥ç‰¹å¾å’Œä¸“å®¶ç»éªŒï¼Œæœºå™¨å­¦ä¹ æ–¹æ³•åœ¨å™ªå£°å’Œå¼‚è´¨ä¸´åºŠæ•°æ®ä¸Šæ³›åŒ–å›°éš¾ã€‚
2. æå‡ºè½»é‡æ··åˆæ¶æ„ï¼Œç»“åˆæ®‹å·®åŒå‘GRUã€é€šé“é‡åŠ æƒå’Œå¤šå¤´è‡ªæ³¨æ„åŠ›æ± åŒ–ï¼Œä»¥æ•è·åºåˆ—å’Œå…¨å±€ç‰¹å¾ã€‚
3. åœ¨UCIå¿ƒè„ç—…æ•°æ®é›†ä¸Šï¼Œæ¨¡å‹å‡†ç¡®ç‡è¾¾0.861ï¼Œä¼˜äºç»å…¸å’Œæ·±åº¦å­¦ä¹ åŸºçº¿ï¼Œæ¶ˆèç ”ç©¶éªŒè¯å„æ¨¡å—è´¡çŒ®ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¿ƒè¡€ç®¡ç–¾ç—…æ˜¯å…¨çƒä¸»è¦æ­»å› ï¼Œéœ€è¦å¯é é«˜æ•ˆçš„é¢„æµ‹å·¥å…·ä»¥æ”¯æŒæ—©æœŸå¹²é¢„ã€‚ä¼ ç»Ÿè¯Šæ–­æ–¹æ³•ä¾èµ–æ‰‹å·¥ç‰¹å¾å’Œä¸´åºŠä¸“å®¶ç»éªŒï¼Œè€Œæœºå™¨å­¦ä¹ æ–¹æ³•è™½æé«˜å¯é‡å¤æ€§ï¼Œä½†å¸¸éš¾ä»¥åœ¨å™ªå£°å’Œå¼‚è´¨ä¸´åºŠæ•°æ®ä¸Šæ³›åŒ–ã€‚æœ¬æ–‡æå‡ºResidual GRU with Multi-Head Self-Attentionï¼Œä¸€ç§ä¸ºè¡¨æ ¼ä¸´åºŠè®°å½•è®¾è®¡çš„ç´§å‡‘æ·±åº¦å­¦ä¹ æ¶æ„ã€‚è¯¥æ¨¡å‹é›†æˆæ®‹å·®åŒå‘é—¨æ§å¾ªç¯å•å…ƒç”¨äºç‰¹å¾åˆ—çš„åºåˆ—å»ºæ¨¡ã€é€šé“é‡åŠ æƒå—ï¼Œä»¥åŠå¸¦å¯å­¦ä¹ åˆ†ç±»ä»¤ç‰Œçš„å¤šå¤´è‡ªæ³¨æ„åŠ›æ± åŒ–ä»¥æ•è·å…¨å±€ä¸Šä¸‹æ–‡ã€‚æˆ‘ä»¬åœ¨UCIå¿ƒè„ç—…æ•°æ®é›†ä¸Šä½¿ç”¨5æŠ˜åˆ†å±‚äº¤å‰éªŒè¯è¯„ä¼°æ¨¡å‹ï¼Œå¹¶ä¸é€»è¾‘å›å½’ã€éšæœºæ£®æ—ã€æ”¯æŒå‘é‡æœºç­‰ç»å…¸æ–¹æ³•ï¼Œä»¥åŠDeepMLPã€å·ç§¯ç½‘ç»œã€å¾ªç¯ç½‘ç»œå’ŒTransformerç­‰ç°ä»£æ·±åº¦å­¦ä¹ åŸºçº¿è¿›è¡Œæ¯”è¾ƒã€‚æ‰€ææ¨¡å‹è¾¾åˆ°0.861çš„å‡†ç¡®ç‡ã€0.860çš„å®F1ã€0.908çš„ROC-AUCå’Œ0.904çš„PR-AUCï¼Œä¼˜äºæ‰€æœ‰åŸºçº¿ã€‚æ¶ˆèç ”ç©¶ç¡®è®¤äº†æ®‹å·®å¾ªç¯ã€é€šé“é—¨æ§å’Œæ³¨æ„åŠ›æ± åŒ–çš„ä¸ªä½“è´¡çŒ®ã€‚t-SNEå¯è§†åŒ–è¿›ä¸€æ­¥è¡¨æ˜ï¼Œä¸åŸå§‹ç‰¹å¾ç›¸æ¯”ï¼Œå­¦ä¹ åˆ°çš„åµŒå…¥åœ¨ç–¾ç—…å’Œéç–¾ç—…ç±»åˆ«é—´å±•ç°å‡ºæ›´æ¸…æ™°çš„åˆ†ç¦»ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œè½»é‡æ··åˆå¾ªç¯å’ŒåŸºäºæ³¨æ„åŠ›çš„æ¶æ„ä¸ºä¸´åºŠé£é™©é¢„æµ‹æä¾›äº†å‡†ç¡®æ€§ä¸æ•ˆç‡ä¹‹é—´çš„å¼ºå¹³è¡¡ï¼Œæ”¯æŒåœ¨èµ„æºå—é™çš„åŒ»ç–—ç¯å¢ƒä¸­éƒ¨ç½²ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¿ƒè¡€ç®¡ç–¾ç—…æ£€æµ‹ä¸­ï¼Œä¼ ç»Ÿæ–¹æ³•ä¾èµ–æ‰‹å·¥ç‰¹å¾å’Œä¸“å®¶ç»éªŒï¼Œè€Œæœºå™¨å­¦ä¹ æ–¹æ³•åœ¨å™ªå£°å’Œå¼‚è´¨ä¸´åºŠè¡¨æ ¼æ•°æ®ä¸Šæ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•çš„ç—›ç‚¹åŒ…æ‹¬ç‰¹å¾å·¥ç¨‹å¤æ‚ã€æ¨¡å‹å¯¹æ•°æ®å™ªå£°æ•æ„Ÿï¼Œä»¥åŠæ·±åº¦å­¦ä¹ æ¨¡å‹å¦‚Transformerè®¡ç®—å¼€é”€å¤§ï¼Œä¸é€‚åˆèµ„æºå—é™çš„åŒ»ç–—ç¯å¢ƒã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®¾è®¡ä¸€ä¸ªè½»é‡æ··åˆæ·±åº¦å­¦ä¹ æ¶æ„ï¼Œç»“åˆå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰çš„åºåˆ—å»ºæ¨¡èƒ½åŠ›å’Œè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„å…¨å±€ä¸Šä¸‹æ–‡æ•è·èƒ½åŠ›ï¼Œé€šè¿‡æ®‹å·®è¿æ¥å’Œé€šé“é‡åŠ æƒå¢å¼ºç‰¹å¾è¡¨ç¤ºï¼Œä»¥æé«˜æ¨¡å‹åœ¨ä¸´åºŠè¡¨æ ¼æ•°æ®ä¸Šçš„å‡†ç¡®æ€§å’Œæ³›åŒ–æ€§ï¼ŒåŒæ—¶ä¿æŒè®¡ç®—æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œä½¿ç”¨æ®‹å·®åŒå‘é—¨æ§å¾ªç¯å•å…ƒï¼ˆResidual Bidirectional GRUï¼‰å¯¹ç‰¹å¾åˆ—è¿›è¡Œåºåˆ—å»ºæ¨¡ï¼Œå¤„ç†è¡¨æ ¼æ•°æ®çš„æ—¶åºæˆ–é¡ºåºä¾èµ–ï¼›å…¶æ¬¡ï¼Œå¼•å…¥é€šé“é‡åŠ æƒå—ï¼ˆChannel Reweighting Blockï¼‰ï¼Œé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶åŠ¨æ€è°ƒæ•´ç‰¹å¾é€šé“çš„é‡è¦æ€§ï¼›æœ€åï¼Œé‡‡ç”¨å¤šå¤´è‡ªæ³¨æ„åŠ›æ± åŒ–ï¼ˆMulti-Head Self-Attention Poolingï¼‰ä¸å¯å­¦ä¹ åˆ†ç±»ä»¤ç‰Œï¼Œèšåˆå…¨å±€ä¿¡æ¯å¹¶è¾“å‡ºåˆ†ç±»ç»“æœã€‚æµç¨‹ä¸ºè¾“å…¥è¡¨æ ¼æ•°æ®â†’æ®‹å·®åŒå‘GRUå¤„ç†â†’é€šé“é‡åŠ æƒâ†’å¤šå¤´è‡ªæ³¨æ„åŠ›æ± åŒ–â†’åˆ†ç±»è¾“å‡ºã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹æ˜¯è½»é‡æ··åˆæ¶æ„çš„è®¾è®¡ï¼Œå°†æ®‹å·®å¾ªç¯ã€é€šé“é—¨æ§å’Œæ³¨æ„åŠ›æ± åŒ–æœ‰æœºç»“åˆï¼Œæœ¬è´¨åŒºåˆ«åœ¨äºå®ƒé¿å…äº†ä¼ ç»ŸTransformerçš„é«˜è®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶é€šè¿‡å¾ªç¯ç½‘ç»œæ•è·å±€éƒ¨åºåˆ—æ¨¡å¼ï¼Œé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶å¢å¼ºå…¨å±€ç‰¹å¾äº¤äº’ï¼Œå®ç°äº†å‡†ç¡®æ€§ä¸æ•ˆç‡çš„å¹³è¡¡ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œå®ƒä¸“é—¨é’ˆå¯¹è¡¨æ ¼ä¸´åºŠæ•°æ®ä¼˜åŒ–ï¼Œå‡å°‘äº†å‚æ•°æ•°é‡ï¼Œæ›´é€‚åˆéƒ¨ç½²åœ¨èµ„æºå—é™çš„åŒ»ç–—åœºæ™¯ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®å‚æ•°è®¾ç½®åŒ…æ‹¬ä½¿ç”¨åŒå‘GRUå¤„ç†ç‰¹å¾åºåˆ—ï¼Œéšè—å±‚å¤§å°æœªçŸ¥ï¼›é€šé“é‡åŠ æƒå—å¯èƒ½åŸºäºæ³¨æ„åŠ›æƒé‡è°ƒæ•´ç‰¹å¾ï¼›å¤šå¤´è‡ªæ³¨æ„åŠ›æ± åŒ–ä¸­ï¼Œå¤´æ•°æœªçŸ¥ï¼Œä½†åŒ…å«å¯å­¦ä¹ åˆ†ç±»ä»¤ç‰Œä»¥èšåˆä¿¡æ¯ã€‚æŸå¤±å‡½æ•°å¯èƒ½ä½¿ç”¨äº¤å‰ç†µæŸå¤±è¿›è¡ŒäºŒåˆ†ç±»ä»»åŠ¡ã€‚ç½‘ç»œç»“æ„ç´§å‡‘ï¼Œæ•´ä½“å‚æ•°è¾ƒå°‘ï¼Œä»¥æ”¯æŒè½»é‡åŒ–éƒ¨ç½²ã€‚å…·ä½“è¶…å‚æ•°å¦‚å­¦ä¹ ç‡ã€æ‰¹æ¬¡å¤§å°åœ¨è®ºæ–‡ä¸­æœªè¯¦ç»†è¯´æ˜ï¼Œä½†é€šè¿‡5æŠ˜åˆ†å±‚äº¤å‰éªŒè¯è¿›è¡Œä¼˜åŒ–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨UCIå¿ƒè„ç—…æ•°æ®é›†ä¸Šï¼ŒResidual GRU+MHSAæ¨¡å‹è¾¾åˆ°0.861å‡†ç¡®ç‡ã€0.860å®F1ã€0.908 ROC-AUCå’Œ0.904 PR-AUCï¼Œä¼˜äºé€»è¾‘å›å½’ã€éšæœºæ£®æ—ã€æ”¯æŒå‘é‡æœºåŠDeepMLPã€å·ç§¯ç½‘ç»œã€å¾ªç¯ç½‘ç»œå’ŒTransformerç­‰åŸºçº¿ã€‚æ¶ˆèç ”ç©¶ç¡®è®¤æ®‹å·®å¾ªç¯ã€é€šé“é—¨æ§å’Œæ³¨æ„åŠ›æ± åŒ–å‡è´¡çŒ®æ€§èƒ½æå‡ï¼Œt-SNEå¯è§†åŒ–æ˜¾ç¤ºå­¦ä¹ åµŒå…¥åœ¨ç±»åˆ«é—´åˆ†ç¦»æ›´æ¸…æ™°ï¼ŒéªŒè¯äº†æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶ä¸»è¦åº”ç”¨äºå¿ƒè¡€ç®¡ç–¾ç—…æ—©æœŸæ£€æµ‹å’Œé£é™©é¢„æµ‹ï¼ŒåŸºäºè¡¨æ ¼ä¸´åºŠè®°å½•å¦‚ç”µå­å¥åº·æ¡£æ¡ˆã€‚å…¶å®é™…ä»·å€¼åœ¨äºæä¾›é«˜æ•ˆã€å‡†ç¡®çš„è‡ªåŠ¨åŒ–è¯Šæ–­å·¥å…·ï¼Œæ”¯æŒä¸´åºŠå†³ç­–ï¼Œå‡å°‘å¯¹ä¸“å®¶ç»éªŒçš„ä¾èµ–ã€‚æœªæ¥å½±å“å¯èƒ½æ‰©å±•åˆ°å…¶ä»–æ…¢æ€§ç—…é¢„æµ‹ï¼Œä¿ƒè¿›èµ„æºå—é™åŒ»ç–—ç¯å¢ƒä¸­çš„æ™ºèƒ½åŒ»ç–—éƒ¨ç½²ï¼Œæå‡å…¬å…±å«ç”Ÿæ°´å¹³ã€‚
=======
**å…³é”®è¯**: `å¿ƒè¡€ç®¡ç–¾ç—…æ£€æµ‹` `è½»é‡æ·±åº¦å­¦ä¹ ` `æ®‹å·®å¾ªç¯ç½‘ç»œ` `å¤šå¤´è‡ªæ³¨æ„åŠ›` `ä¸´åºŠé£é™©é¢„æµ‹` `è¡¨æ ¼æ•°æ®å¤„ç†` `åŒ»ç–—äººå·¥æ™ºèƒ½` `æ¨¡å‹æ•ˆç‡ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¾èµ–æ‰‹å·¥ç‰¹å¾æˆ–éš¾ä»¥æ³›åŒ–åˆ°å™ªå£°ä¸´åºŠæ•°æ®ï¼Œé™åˆ¶äº†å¿ƒè¡€ç®¡ç–¾ç—…é¢„æµ‹çš„å¯é æ€§ã€‚
2. æå‡ºResidual GRU+MHSAæ¨¡å‹ï¼Œç»“åˆæ®‹å·®å¾ªç¯å•å…ƒå’Œæ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥è½»é‡æ¶æ„æ•è·åºåˆ—å’Œå…¨å±€ç‰¹å¾ã€‚
3. åœ¨UCIæ•°æ®é›†ä¸Šï¼Œæ¨¡å‹å‡†ç¡®ç‡è¾¾0.861ï¼Œä¼˜äºä¼ ç»Ÿå’Œæ·±åº¦å­¦ä¹ åŸºçº¿ï¼Œå¹¶é€šè¿‡æ¶ˆèéªŒè¯äº†ç»„ä»¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¿ƒè¡€ç®¡ç–¾ç—…æ˜¯å…¨çƒä¸»è¦æ­»å› ï¼Œéœ€è¦å¯é é«˜æ•ˆçš„é¢„æµ‹å·¥å…·ä»¥æ”¯æŒæ—©æœŸå¹²é¢„ã€‚ä¼ ç»Ÿè¯Šæ–­æ–¹æ³•ä¾èµ–æ‰‹å·¥ç‰¹å¾å’Œä¸´åºŠä¸“å®¶ç»éªŒï¼Œè€Œæœºå™¨å­¦ä¹ æ–¹æ³•è™½æé«˜å¯é‡å¤æ€§ï¼Œä½†å¸¸éš¾ä»¥æ³›åŒ–åˆ°å™ªå£°å’Œå¼‚æ„ä¸´åºŠæ•°æ®ã€‚æœ¬ç ”ç©¶æå‡ºResidual GRU with Multi-Head Self-Attentionï¼Œä¸€ç§ç´§å‡‘çš„æ·±åº¦å­¦ä¹ æ¶æ„ï¼Œä¸“ä¸ºè¡¨æ ¼ä¸´åºŠè®°å½•è®¾è®¡ã€‚è¯¥æ¨¡å‹é›†æˆæ®‹å·®åŒå‘é—¨æ§å¾ªç¯å•å…ƒç”¨äºç‰¹å¾åˆ—çš„åºåˆ—å»ºæ¨¡ã€é€šé“é‡åŠ æƒå—ï¼Œä»¥åŠå¸¦å¯å­¦ä¹ åˆ†ç±»ä»¤ç‰Œçš„å¤šå¤´è‡ªæ³¨æ„åŠ›æ± åŒ–ä»¥æ•è·å…¨å±€ä¸Šä¸‹æ–‡ã€‚æˆ‘ä»¬åœ¨UCIå¿ƒè„ç—…æ•°æ®é›†ä¸Šä½¿ç”¨5æŠ˜åˆ†å±‚äº¤å‰éªŒè¯è¯„ä¼°æ¨¡å‹ï¼Œå¹¶ä¸é€»è¾‘å›å½’ã€éšæœºæ£®æ—ã€æ”¯æŒå‘é‡æœºç­‰ç»å…¸æ–¹æ³•ï¼Œä»¥åŠDeepMLPã€å·ç§¯ç½‘ç»œã€å¾ªç¯ç½‘ç»œå’ŒTransformerç­‰ç°ä»£æ·±åº¦å­¦ä¹ åŸºçº¿è¿›è¡Œæ¯”è¾ƒã€‚æ‰€ææ¨¡å‹è¾¾åˆ°0.861çš„å‡†ç¡®ç‡ã€0.860çš„å®F1ã€0.908çš„ROC-AUCå’Œ0.904çš„PR-AUCï¼Œä¼˜äºæ‰€æœ‰åŸºçº¿ã€‚æ¶ˆèç ”ç©¶ç¡®è®¤äº†æ®‹å·®å¾ªç¯ã€é€šé“é—¨æ§å’Œæ³¨æ„åŠ›æ± åŒ–çš„ä¸ªä½“è´¡çŒ®ã€‚t-SNEå¯è§†åŒ–è¿›ä¸€æ­¥è¡¨æ˜ï¼Œä¸åŸå§‹ç‰¹å¾ç›¸æ¯”ï¼Œå­¦ä¹ åˆ°çš„åµŒå…¥åœ¨ç–¾ç—…å’Œéç–¾ç—…ç±»åˆ«é—´å±•ç°å‡ºæ›´æ¸…æ™°çš„åˆ†ç¦»ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œè½»é‡æ··åˆå¾ªç¯å’Œæ³¨æ„åŠ›æ¶æ„åœ¨ä¸´åºŠé£é™©é¢„æµ‹ä¸­æä¾›äº†å‡†ç¡®æ€§å’Œæ•ˆç‡ä¹‹é—´çš„å¼ºå¹³è¡¡ï¼Œæ”¯æŒåœ¨èµ„æºå—é™çš„åŒ»ç–—ç¯å¢ƒä¸­éƒ¨ç½²ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

æ•´ä½“æ¡†æ¶ä¸ºè½»é‡æ··åˆæ¨¡å‹ï¼Œä¸“ä¸ºè¡¨æ ¼ä¸´åºŠæ•°æ®è®¾è®¡ã€‚å…³é”®æŠ€æœ¯åˆ›æ–°åŒ…æ‹¬ï¼šä½¿ç”¨æ®‹å·®åŒå‘GRUå¤„ç†ç‰¹å¾åˆ—åºåˆ—ï¼Œå¢å¼ºæ¢¯åº¦æµåŠ¨ï¼›å¼•å…¥é€šé“é‡åŠ æƒå—åŠ¨æ€è°ƒæ•´ç‰¹å¾é‡è¦æ€§ï¼›ç»“åˆå¤šå¤´è‡ªæ³¨æ„åŠ›æ± åŒ–ä¸å¯å­¦ä¹ åˆ†ç±»ä»¤ç‰Œï¼Œæ•è·å…¨å±€ä¸Šä¸‹æ–‡å¹¶ä¼˜åŒ–åˆ†ç±»ã€‚ä¸ç°æœ‰æ–¹æ³•çš„ä¸»è¦åŒºåˆ«åœ¨äºï¼Œå®ƒèåˆå¾ªç¯å’Œæ³¨æ„åŠ›æœºåˆ¶äºç´§å‡‘æ¶æ„ï¼Œé¿å…äº†ä¼ ç»Ÿæ–¹æ³•çš„æ‰‹å·¥ç‰¹å¾ä¾èµ–å’Œæ·±åº¦å­¦ä¹ æ¨¡å‹çš„è¿‡å‚æ•°åŒ–ï¼Œå®ç°é«˜æ•ˆä¸”å‡†ç¡®çš„é¢„æµ‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

æ¨¡å‹åœ¨UCIå¿ƒè„ç—…æ•°æ®é›†ä¸Šå–å¾—0.861å‡†ç¡®ç‡ã€0.860å®F1ã€0.908 ROC-AUCå’Œ0.904 PR-AUCï¼Œå…¨é¢è¶…è¶Šé€»è¾‘å›å½’ã€éšæœºæ£®æ—ã€æ”¯æŒå‘é‡æœºåŠDeepMLPã€å·ç§¯ç½‘ç»œã€å¾ªç¯ç½‘ç»œå’ŒTransformerç­‰åŸºçº¿ï¼Œå¹¶é€šè¿‡æ¶ˆèå’Œt-SNEå¯è§†åŒ–éªŒè¯äº†ç»„ä»¶è´¡çŒ®å’Œç‰¹å¾åˆ†ç¦»æ•ˆæœã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶å¯åº”ç”¨äºå¿ƒè¡€ç®¡ç–¾ç—…æ—©æœŸç­›æŸ¥å’Œé£é™©é¢„æµ‹ï¼Œç‰¹åˆ«é€‚åˆèµ„æºå—é™çš„åŒ»ç–—ç¯å¢ƒï¼Œå¦‚ç¤¾åŒºè¯Šæ‰€æˆ–è¿œç¨‹åŒ»ç–—ç³»ç»Ÿï¼Œé€šè¿‡è½»é‡æ¨¡å‹éƒ¨ç½²æå‡è¯Šæ–­æ•ˆç‡å’Œå¯åŠæ€§ã€‚
>>>>>>> 1c05e1c356e1f28c2e5e6e14cf6811c0d5120ab7

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Cardiovascular disease (CVD) remains the leading cause of mortality worldwide, underscoring the need for reliable and efficient predictive tools that support early intervention. Traditional diagnostic approaches rely on handcrafted features and clinician expertise, while machine learning methods improve reproducibility but often struggle to generalize across noisy and heterogeneous clinical data. In this work, we propose Residual GRU with Multi-Head Self-Attention, a compact deep learning architecture designed for tabular clinical records. The model integrates residual bidirectional gated recurrent units for sequential modeling of feature columns, a channel reweighting block, and multi-head self-attention pooling with a learnable classification token to capture global context. We evaluate the model on the UCI Heart Disease dataset using 5-fold stratified cross-validation and compare it against classical methods such as Logistic Regression, Random Forest, and Support Vector Machines, as well as modern deep learning baselines including DeepMLP, convolutional networks, recurrent networks, and Transformers. The proposed model achieves an accuracy of 0.861, macro-F1 of 0.860, ROC-AUC of 0.908, and PR-AUC of 0.904, outperforming all baselines. Ablation studies confirm the individual contributions of residual recurrence, channel gating, and attention pooling. t-SNE visualizations further indicate that the learned embeddings exhibit clearer separation between disease and non-disease classes compared to raw features. These results demonstrate that lightweight hybrid recurrent and attention-based architectures provide a strong balance between accuracy and efficiency for clinical risk prediction, supporting deployment in resource-constrained healthcare settings.

