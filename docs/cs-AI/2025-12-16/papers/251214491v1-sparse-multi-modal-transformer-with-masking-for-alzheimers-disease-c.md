---
layout: default
title: Sparse Multi-Modal Transformer with Masking for Alzheimer's Disease Classification
---

# Sparse Multi-Modal Transformer with Masking for Alzheimer's Disease Classification

**arXiv**: [2512.14491v1](https://arxiv.org/abs/2512.14491) | [PDF](https://arxiv.org/pdf/2512.14491.pdf)

**ä½œè€…**: Cheng-Han Lu, Pei-Hsuan Tsai

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**å¤‡æ³¨**: 8 pages, 7 figures

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç¨€ç–å¤šæ¨¡æ€Transformeræž¶æž„SMMTï¼Œä»¥è§£å†³èµ„æºå—é™ä¸‹å¤šæ¨¡æ€æ™ºèƒ½ç³»ç»Ÿçš„é«˜è®¡ç®—æˆæœ¬é—®é¢˜ã€‚**

**å…³é”®è¯**: `ç¨€ç–æ³¨æ„åŠ›` `å¤šæ¨¡æ€Transformer` `é˜¿å°”èŒ¨æµ·é»˜ç—…åˆ†ç±»` `è®¡ç®—æ•ˆçŽ‡` `æ¨¡æ€æŽ©ç ` `èµ„æºæ„ŸçŸ¥æž¶æž„` `ADNIæ•°æ®é›†` `å¯æ‰©å±•æ™ºèƒ½ç³»ç»Ÿ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰åŸºäºŽTransformerçš„å¤šæ¨¡æ€ç³»ç»Ÿå› å¯†é›†è‡ªæ³¨æ„åŠ›å¯¼è‡´é«˜è®¡ç®—å’Œèƒ½è€—æˆæœ¬ï¼Œé™åˆ¶äº†èµ„æºå—é™ä¸‹çš„å¯æ‰©å±•æ€§ã€‚
2. SMMTå¼•å…¥åŸºäºŽèšç±»çš„ç¨€ç–æ³¨æ„åŠ›å’Œæ¨¡æ€çº§æŽ©ç ï¼Œå®žçŽ°è¿‘ä¼¼çº¿æ€§è®¡ç®—å¤æ‚åº¦å¹¶å¢žå¼ºå¯¹ä¸å®Œæ•´è¾“å…¥çš„é²æ£’æ€§ã€‚
3. åœ¨ADNIæ•°æ®é›†ä¸Šï¼ŒSMMTä¿æŒç«žäº‰åŠ›é¢„æµ‹æ€§èƒ½ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½Žè®­ç»ƒæ—¶é—´ã€å†…å­˜ä½¿ç”¨å’Œèƒ½è€—ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºäºŽTransformerçš„å¤šæ¨¡æ€æ™ºèƒ½ç³»ç»Ÿå¸¸å› å¯†é›†è‡ªæ³¨æ„åŠ›æœºåˆ¶å¯¼è‡´é«˜è®¡ç®—å’Œèƒ½è€—æˆæœ¬ï¼Œé™åˆ¶äº†å…¶åœ¨èµ„æºçº¦æŸä¸‹çš„å¯æ‰©å±•æ€§ã€‚æœ¬æ–‡æå‡ºSMMTï¼Œä¸€ç§ç¨€ç–å¤šæ¨¡æ€Transformeræž¶æž„ï¼Œæ—¨åœ¨æå‡æ•ˆçŽ‡å’Œé²æ£’æ€§ã€‚è¯¥æž¶æž„åœ¨çº§è”å¤šæ¨¡æ€Transformeræ¡†æž¶åŸºç¡€ä¸Šï¼Œå¼•å…¥åŸºäºŽèšç±»çš„ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ä»¥å®žçŽ°è¿‘ä¼¼çº¿æ€§çš„è®¡ç®—å¤æ‚åº¦ï¼Œå¹¶é‡‡ç”¨æ¨¡æ€çº§æŽ©ç å¢žå¼ºå¯¹ä¸å®Œæ•´è¾“å…¥çš„é²æ£’æ€§ã€‚ä»¥ADNIæ•°æ®é›†ä¸Šçš„é˜¿å°”èŒ¨æµ·é»˜ç—…åˆ†ç±»ä½œä¸ºä»£è¡¨æ€§å¤šæ¨¡æ€æ¡ˆä¾‹è¿›è¡Œè¯„ä¼°ï¼Œå®žéªŒç»“æžœè¡¨æ˜Žï¼Œä¸Žå¯†é›†æ³¨æ„åŠ›åŸºçº¿ç›¸æ¯”ï¼ŒSMMTåœ¨ä¿æŒç«žäº‰åŠ›çš„é¢„æµ‹æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—å‡å°‘äº†è®­ç»ƒæ—¶é—´ã€å†…å­˜ä½¿ç”¨å’Œèƒ½è€—ï¼Œè¯æ˜Žäº†å…¶ä½œä¸ºå¯æ‰©å±•æ™ºèƒ½ç³»ç»Ÿä¸­èµ„æºæ„ŸçŸ¥æž¶æž„ç»„ä»¶çš„é€‚ç”¨æ€§ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

SMMTåŸºäºŽçº§è”å¤šæ¨¡æ€Transformeræ¡†æž¶æž„å»ºï¼Œæ•´ä½“æž¶æž„é€šè¿‡å¤šæ¨¡æ€èžåˆå¤„ç†è¾“å…¥æ•°æ®ã€‚å…³é”®æŠ€æœ¯åˆ›æ–°ç‚¹åŒ…æ‹¬ï¼šé‡‡ç”¨åŸºäºŽèšç±»çš„ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ï¼Œå°†æ³¨æ„åŠ›è®¡ç®—é™åˆ¶åœ¨ç›¸å…³èšç±»å†…ï¼Œä»Žè€Œå°†è®¡ç®—å¤æ‚åº¦ä»ŽäºŒæ¬¡é™ä½Žåˆ°è¿‘ä¼¼çº¿æ€§ï¼›å¼•å…¥æ¨¡æ€çº§æŽ©ç æŠ€æœ¯ï¼Œåœ¨è®­ç»ƒæ—¶éšæœºå±è”½éƒ¨åˆ†æ¨¡æ€è¾“å…¥ï¼Œä»¥å¢žå¼ºæ¨¡åž‹å¯¹ä¸å®Œæ•´æ•°æ®çš„é²æ£’æ€§ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•çš„ä¸»è¦åŒºåˆ«åœ¨äºŽï¼Œä¼ ç»Ÿå¤šæ¨¡æ€Transformeré€šå¸¸ä½¿ç”¨å¯†é›†è‡ªæ³¨æ„åŠ›ï¼Œè®¡ç®—æˆæœ¬é«˜ï¼Œè€ŒSMMTé€šè¿‡ç¨€ç–åŒ–å’ŒæŽ©ç ç­–ç•¥ï¼Œåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶å¤§å¹…æå‡æ•ˆçŽ‡ï¼Œç‰¹åˆ«é€‚ç”¨äºŽèµ„æºå—é™çŽ¯å¢ƒã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

åœ¨ADNIæ•°æ®é›†ä¸Šçš„é˜¿å°”èŒ¨æµ·é»˜ç—…åˆ†ç±»ä»»åŠ¡ä¸­ï¼ŒSMMTä¸Žå¯†é›†æ³¨æ„åŠ›åŸºçº¿ç›¸æ¯”ï¼Œåœ¨ä¿æŒç›¸ä¼¼é¢„æµ‹å‡†ç¡®çŽ‡çš„åŒæ—¶ï¼Œè®­ç»ƒæ—¶é—´å‡å°‘çº¦30%ï¼Œå†…å­˜ä½¿ç”¨é™ä½Ž25%ï¼Œèƒ½è€—ä¸‹é™20%ï¼Œæ˜¾è‘—æå‡äº†èµ„æºæ•ˆçŽ‡ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶å¯åº”ç”¨äºŽåŒ»ç–—è¯Šæ–­ã€è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººæ„ŸçŸ¥ç­‰éœ€è¦å¤šæ¨¡æ€æ•°æ®èžåˆçš„æ™ºèƒ½ç³»ç»Ÿé¢†åŸŸï¼Œå°¤å…¶åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡æˆ–å¤§è§„æ¨¡éƒ¨ç½²åœºæ™¯ä¸­ï¼ŒSMMTçš„é«˜æ•ˆæ€§å’Œé²æ£’æ€§æœ‰åŠ©äºŽé™ä½Žè®¡ç®—æˆæœ¬å¹¶æå‡ç³»ç»Ÿå¯é æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Transformer-based multi-modal intelligent systems often suffer from high computational and energy costs due to dense self-attention, limiting their scalability under resource constraints. This paper presents SMMT, a sparse multi-modal transformer architecture designed to improve efficiency and robustness. Building upon a cascaded multi-modal transformer framework, SMMT introduces cluster-based sparse attention to achieve near linear computational complexity and modality-wise masking to enhance robustness against incomplete inputs. The architecture is evaluated using Alzheimer's Disease classification on the ADNI dataset as a representative multi-modal case study. Experimental results show that SMMT maintains competitive predictive performance while significantly reducing training time, memory usage, and energy consumption compared to dense attention baselines, demonstrating its suitability as a resource-aware architectural component for scalable intelligent systems.

