---
layout: default
title: Efficient-DLM: From Autoregressive to Diffusion Language Models, and Beyond in Speed
---

# Efficient-DLM: From Autoregressive to Diffusion Language Models, and Beyond in Speed

**arXiv**: [2512.14067v1](https://arxiv.org/abs/2512.14067) | [PDF](https://arxiv.org/pdf/2512.14067.pdf)

**‰ΩúËÄÖ**: Yonggan Fu, Lexington Whalen, Zhifan Ye, Xin Dong, Shizhe Diao, Jingyu Liu, Chengyue Wu, Hao Zhang, Enze Xie, Song Han, Maksim Khadkevich, Jan Kautz, Yingyan Celine Lin, Pavlo Molchanov

**ÂàÜÁ±ª**: cs.CL, cs.AI, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-12-16

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

<<<<<<< HEAD
**ÊèêÂá∫Efficient-DLMÊ°ÜÊû∂ÔºåÈÄöËøáÊîπËøõAR-to-dLMËΩ¨Êç¢ÊñπÊ≥ïÔºåÂÆûÁé∞È´òÊïàÊâ©Êï£ËØ≠Ë®ÄÊ®°ÂûãÔºåÂú®‰øùÊåÅÂáÜÁ°ÆÊÄßÁöÑÂêåÊó∂Â§ßÂπÖÊèêÂçáÁîüÊàêÈÄüÂ∫¶„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ËßÜËßâÈáåÁ®ãËÆ°**

**ÂÖ≥ÈîÆËØç**: `Êâ©Êï£ËØ≠Ë®ÄÊ®°Âûã` `Ëá™ÂõûÂΩíÊ®°ÂûãËΩ¨Êç¢` `Ê≥®ÊÑèÂäõÊ®°Âºè‰ºòÂåñ` `Âπ∂Ë°åÊñáÊú¨ÁîüÊàê` `ËÆ≠ÁªÉÁ≠ñÁï•ÊîπËøõ` `Ê®°ÂûãÊïàÁéáÊèêÂçá` `Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ` `Ê∑±Â∫¶Â≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Ê†∏ÂøÉÈóÆÈ¢òÔºöÁé∞ÊúâAR-to-dLMËΩ¨Êç¢ÊñπÊ≥ïÂú®Ê≥®ÊÑèÂäõÊ®°ÂºèÂíåËÆ≠ÁªÉÁõÆÊ†á‰∏äÂ≠òÂú®Â±ÄÈôêÊÄßÔºåÂØºËá¥ËΩ¨Êç¢ÂêéÊ®°ÂûãÈöæ‰ª•Âú®‰øùÊåÅARÊ®°ÂûãÂáÜÁ°ÆÊÄßÁöÑÂêåÊó∂ÂÆûÁé∞È´òÊïàÂπ∂Ë°åÁîüÊàê„ÄÇ
2. ÊñπÊ≥ïË¶ÅÁÇπÔºöÊèêÂá∫Âü∫‰∫éÂùóÁä∂Ê≥®ÊÑèÂäõÊ®°ÂºèÁöÑÊåÅÁª≠È¢ÑËÆ≠ÁªÉÊñπÊ°àÂíå‰ΩçÁΩÆÁõ∏ÂÖ≥Êé©Á†ÅÁ≠ñÁï•Ôºå‰ºòÂåñAR-to-dLMËΩ¨Êç¢ËøáÁ®ãÔºåÂÆûÁé∞ÂáÜÁ°ÆÊÄß‰∏éÊïàÁéáÁöÑÂπ≥Ë°°„ÄÇ
3. ÂÆûÈ™åÊàñÊïàÊûúÔºöEfficient-DLM 8BÊ®°ÂûãÂú®ÂáÜÁ°ÆÁéáÂíåÂêûÂêêÈáè‰∏äÂùáÊòæËëóË∂ÖË∂äDream 7BÂíåQwen3 4BÁ≠âÂü∫Á∫øÊ®°ÂûãÔºåÈ™åËØÅ‰∫ÜÊñπÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êâ©Êï£ËØ≠Ë®ÄÊ®°ÂûãÔºàdLMsÔºâ‰Ωú‰∏∫‰∏ÄÁßçÊîØÊåÅÂπ∂Ë°å„ÄÅÈùûËá™ÂõûÂΩíÁîüÊàêÁöÑÊñ∞ËåÉÂºèÔºåÂ±ïÁé∞Âá∫Â∑®Â§ßÊΩúÂäõÔºå‰ΩÜÂÖ∂‰ªéÂ§¥ÂºÄÂßãËÆ≠ÁªÉÊó∂ÁöÑÂ≠¶‰π†ÊïàÁéá‰ªçËêΩÂêé‰∫éËá™ÂõûÂΩíÔºàARÔºâËØ≠Ë®ÄÊ®°Âûã„ÄÇ‰∏∫Ê≠§ÔºåÊú¨ÊñáÁ†îÁ©∂AR-to-dLMËΩ¨Êç¢ÊñπÊ≥ïÔºåÊó®Âú®Â∞ÜÈ¢ÑËÆ≠ÁªÉÁöÑARÊ®°ÂûãËΩ¨Âåñ‰∏∫È´òÊïàÁöÑdLMsÔºåÂú®‰øùÊåÅARÊ®°Âûã‰ªªÂä°ÂáÜÁ°ÆÊÄßÁöÑÂêåÊó∂ÊòæËëóÊèêÂçáÁîüÊàêÈÄüÂ∫¶„ÄÇÊàë‰ª¨ÈÄöËøáÂàÜÊûêÁé∞ÊúâAR-to-dLMÊñπÊ≥ïÂú®Ê≥®ÊÑèÂäõÊ®°ÂºèÂíåÁõÆÊ†áÂáΩÊï∞‰∏äÁöÑÂ±ÄÈôêÊÄßÔºåÊèêÂá∫‰∫ÜÊõ¥ÊúâÊïàÁöÑËΩ¨Êç¢ÂéüÂàôÂíåÊñπÊ≥ï„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåÈ¶ñÂÖàÁ≥ªÁªüÊØîËæÉ‰∫Ü‰∏çÂêåÁöÑÊ≥®ÊÑèÂäõÊ®°ÂºèÔºåÂèëÁé∞‰øùÊåÅÈ¢ÑËÆ≠ÁªÉARÊùÉÈáçÂàÜÂ∏ÉÂØπÊúâÊïàËΩ¨Êç¢Ëá≥ÂÖ≥ÈáçË¶Å„ÄÇÂõ†Ê≠§ÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏ÄÁßçÂü∫‰∫éÂùóÁä∂Ê≥®ÊÑèÂäõÊ®°ÂºèÁöÑÊåÅÁª≠È¢ÑËÆ≠ÁªÉÊñπÊ°àÔºåËØ•ÊñπÊ°àÂú®ÂùóÈó¥‰øùÊåÅÂõ†ÊûúÊÄßÔºåÂêåÊó∂Âú®ÂùóÂÜÖÂÆûÁé∞ÂèåÂêëÂª∫Ê®°„ÄÇÊàë‰ª¨ÂèëÁé∞ËøôÁßçÊñπÊ≥ïÊØîÂÆåÂÖ®ÂèåÂêëÂª∫Ê®°ËÉΩÊõ¥Â•ΩÂú∞‰øùÁïôÈ¢ÑËÆ≠ÁªÉARÊ®°ÂûãÁöÑÊùÉÈáçÂàÜÂ∏ÉÔºåÂπ∂ÂÖ∑ÊúâÊîØÊåÅKVÁºìÂ≠òÁöÑ‰ºòÂäøÔºåÂÆûÁé∞‰∫ÜÂáÜÁ°ÆÊÄßÂíåÊïàÁéáÁöÑÂèåËµ¢„ÄÇÂÖ∂Ê¨°Ôºå‰∏∫ÁºìËß£ËÆ≠ÁªÉ‰∏éÊµãËØïÊó∂Êé©Á†ÅÊ†áËÆ∞ÂàÜÂ∏ÉÔºàÂùáÂåÄÂàÜÂ∏É‰∏éÈ´òÂ∫¶‰ªéÂ∑¶Âà∞Âè≥ÂàÜÂ∏ÉÔºâ‰πãÈó¥ÁöÑÂ∑ÆË∑ùÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßç‰ΩçÁΩÆÁõ∏ÂÖ≥ÁöÑÊ†áËÆ∞Êé©Á†ÅÁ≠ñÁï•ÔºåÂú®ËÆ≠ÁªÉÊó∂ÂØπÂêéÁª≠Ê†áËÆ∞Ëµã‰∫àÊõ¥È´òÁöÑÊé©Á†ÅÊ¶ÇÁéáÔºå‰ª•Êõ¥Â•ΩÂú∞Ê®°ÊãüÊµãËØïÊó∂ÁöÑË°å‰∏∫„ÄÇÂü∫‰∫éÊ≠§Ê°ÜÊû∂ÔºåÊàë‰ª¨Ê∑±ÂÖ•Á†îÁ©∂‰∫ÜdLMsÁöÑÊ≥®ÊÑèÂäõÊ®°Âºè„ÄÅËÆ≠ÁªÉÂä®ÊÄÅÂíåÂÖ∂‰ªñËÆæËÆ°ÈÄâÊã©Ôºå‰∏∫ÂèØÊâ©Â±ïÁöÑAR-to-dLMËΩ¨Êç¢Êèê‰æõ‰∫ÜÂÆûÁî®ËßÅËß£„ÄÇËøô‰∫õÁ†îÁ©∂ÂÇ¨Áîü‰∫ÜEfficient-DLMÁ≥ªÂàóÊ®°ÂûãÔºåÂÖ∂Âú®ÊÄßËÉΩ‰∏äË∂ÖË∂ä‰∫ÜÂΩìÂâçÊúÄÂÖàËøõÁöÑARÊ®°ÂûãÂíådLMs„ÄÇ‰æãÂ¶ÇÔºåÊàë‰ª¨ÁöÑEfficient-DLM 8BÊ®°ÂûãÂú®ÂáÜÁ°ÆÁéá‰∏äÊØîDream 7BÂíåQwen3 4BÂàÜÂà´È´òÂá∫+5.4%Âíå+2.7%ÔºåÂêåÊó∂ÂêûÂêêÈáèÂàÜÂà´ÊèêÂçá‰∫Ü4.5ÂÄçÂíå2.7ÂÄç„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Â¶Ç‰ΩïÂ∞ÜÈ¢ÑËÆ≠ÁªÉÁöÑËá™ÂõûÂΩíÔºàARÔºâËØ≠Ë®ÄÊ®°ÂûãÈ´òÊïàËΩ¨Êç¢‰∏∫Êâ©Êï£ËØ≠Ë®ÄÊ®°ÂûãÔºàdLMÔºâÔºå‰ª•Âú®‰øùÊåÅARÊ®°Âûã‰ªªÂä°ÂáÜÁ°ÆÊÄßÁöÑÂêåÊó∂ÔºåÂà©Áî®dLMÁöÑÂπ∂Ë°åÁîüÊàê‰ºòÂäøÊèêÂçáÈÄüÂ∫¶„ÄÇÁé∞ÊúâAR-to-dLMÊñπÊ≥ïÁöÑÁóõÁÇπÂú®‰∫éÂÖ∂Ê≥®ÊÑèÂäõÊ®°ÂºèÂíåËÆ≠ÁªÉÁõÆÊ†áËÆæËÆ°‰∏çÂΩìÔºåÂØºËá¥ËΩ¨Êç¢ÂêéÊ®°ÂûãÊùÉÈáçÂàÜÂ∏ÉÂÅèÁ¶ªÈ¢ÑËÆ≠ÁªÉARÊ®°ÂûãÔºå‰∏îËÆ≠ÁªÉ‰∏éÊµãËØïÊó∂ÁöÑÊé©Á†ÅÂàÜÂ∏É‰∏çÂåπÈÖçÔºåÂΩ±ÂìçÊ®°ÂûãÊÄßËÉΩ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøá‰ºòÂåñÊ≥®ÊÑèÂäõÊ®°ÂºèÂíåÊé©Á†ÅÁ≠ñÁï•ÔºåÂÆûÁé∞Êõ¥ÊúâÊïàÁöÑAR-to-dLMËΩ¨Êç¢„ÄÇÂÖ∑‰ΩìÂåÖÊã¨Ôºö1ÔºâÈááÁî®ÂùóÁä∂Ê≥®ÊÑèÂäõÊ®°ÂºèÔºåÂú®ÂùóÂÜÖËøõË°åÂèåÂêëÂª∫Ê®°‰ª•ÊçïÊçâ‰∏ä‰∏ãÊñáÔºåÂêåÊó∂Âú®ÂùóÈó¥‰øùÊåÅÂõ†ÊûúÊÄß‰ª•‰øùÁïôÈ¢ÑËÆ≠ÁªÉÊùÉÈáçÂàÜÂ∏ÉÔºõ2ÔºâÂºïÂÖ•‰ΩçÁΩÆÁõ∏ÂÖ≥ÁöÑÊé©Á†ÅÁ≠ñÁï•Ôºå‰ΩøËÆ≠ÁªÉÊó∂ÁöÑÊé©Á†ÅÂàÜÂ∏ÉÊõ¥Êé•ËøëÊµãËØïÊó∂ÁöÑ‰ªéÂ∑¶Âà∞Âè≥Ê®°ÂºèÔºåÂáèÂ∞ëËÆ≠ÁªÉ-ÊµãËØïÂ∑ÆË∑ù„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊ°ÜÊû∂ÂàÜ‰∏∫‰∏§‰∏™‰∏ªË¶ÅÈò∂ÊÆµÔºöÈ¶ñÂÖàÔºåÂü∫‰∫éÈ¢ÑËÆ≠ÁªÉÁöÑARÊ®°ÂûãÔºåÈÄöËøáÊåÅÁª≠È¢ÑËÆ≠ÁªÉËøõË°åAR-to-dLMËΩ¨Êç¢Ôºå‰ΩøÁî®ÂùóÁä∂Ê≥®ÊÑèÂäõÊ®°ÂºèÂíå‰ΩçÁΩÆÁõ∏ÂÖ≥Êé©Á†ÅÁ≠ñÁï•‰ºòÂåñÊ®°ÂûãÔºõÂÖ∂Ê¨°ÔºåÂú®ËΩ¨Êç¢ÂêéÁöÑdLM‰∏äËøõË°åÊé®ÁêÜÔºåÂà©Áî®ÂÖ∂Âπ∂Ë°åÁîüÊàêËÉΩÂäõÂÆûÁé∞È´òÈÄüÊñáÊú¨ÁîüÊàê„ÄÇÊ°ÜÊû∂ËøòÂåÖÊã¨ÂØπÊ≥®ÊÑèÂäõÊ®°Âºè„ÄÅËÆ≠ÁªÉÂä®ÊÄÅÁöÑÁ≥ªÁªüÂàÜÊûêÔºå‰ª•ÊåáÂØºËÆæËÆ°ÈÄâÊã©„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞ÊòØÂùóÁä∂Ê≥®ÊÑèÂäõÊ®°ÂºèÂíå‰ΩçÁΩÆÁõ∏ÂÖ≥Êé©Á†ÅÁ≠ñÁï•ÁöÑÁªìÂêà„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåÂùóÁä∂Ê≥®ÊÑèÂäõÊ®°ÂºèÂú®‰øùÁïôÈ¢ÑËÆ≠ÁªÉÊùÉÈáçÂàÜÂ∏ÉÂíåÊîØÊåÅKVÁºìÂ≠òÊñπÈù¢Êõ¥ÂÖ∑‰ºòÂäøÔºåËÄå‰ΩçÁΩÆÁõ∏ÂÖ≥Êé©Á†ÅÁ≠ñÁï•Áõ¥Êé•ÈíàÂØπËÆ≠ÁªÉ-ÊµãËØïÂàÜÂ∏É‰∏çÂåπÈÖçÈóÆÈ¢òÔºåÊèêÂçá‰∫ÜÊ®°ÂûãÊ≥õÂåñËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1ÔºâÂùóÁä∂Ê≥®ÊÑèÂäõÊ®°ÂºèÔºöÂ∞ÜËæìÂÖ•Â∫èÂàóÂàíÂàÜ‰∏∫ÂùóÔºåÂùóÂÜÖ‰ΩøÁî®ÂèåÂêëÊ≥®ÊÑèÂäõÔºåÂùóÈó¥‰ΩøÁî®Âõ†ÊûúÊ≥®ÊÑèÂäõÔºåÂÖ∑‰ΩìÂùóÂ§ßÂ∞èÊ†πÊçÆÂÆûÈ™å‰ºòÂåñÔºõ2Ôºâ‰ΩçÁΩÆÁõ∏ÂÖ≥Êé©Á†ÅÁ≠ñÁï•ÔºöÂú®ËÆ≠ÁªÉÊó∂ÔºåÂØπÂ∫èÂàó‰∏≠Èù†ÂêéÁöÑÊ†áËÆ∞Ëµã‰∫àÊõ¥È´òÁöÑÊé©Á†ÅÊ¶ÇÁéáÔºåÊ®°ÊãüÊµãËØïÊó∂‰ªéÂ∑¶Âà∞Âè≥ÁöÑÁîüÊàêËøáÁ®ãÔºõ3ÔºâÊçüÂ§±ÂáΩÊï∞ÔºöÂü∫‰∫éÊâ©Êï£Ê®°ÂûãÁöÑÂéªÂô™ÁõÆÊ†áÔºåÁªìÂêàÊé©Á†ÅÈ¢ÑÊµã‰ªªÂä°ÔºåÂÖ∑‰ΩìÂèÇÊï∞Â¶ÇÂ≠¶‰π†Áéá„ÄÅÊâπÂ§ßÂ∞èÈÄöËøáÁΩëÊ†ºÊêúÁ¥¢Á°ÆÂÆöÔºõ4ÔºâÁΩëÁªúÁªìÊûÑÔºöÊ≤øÁî®È¢ÑËÆ≠ÁªÉARÊ®°ÂûãÁöÑTransformerÊû∂ÊûÑÔºå‰ΩÜË∞ÉÊï¥Ê≥®ÊÑèÂäõÊú∫Âà∂‰ª•ÈÄÇÂ∫îdLMÈúÄÊ±Ç„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåEfficient-DLMÁ≥ªÂàóÊ®°ÂûãÂú®ÊÄßËÉΩÂíåÊïàÁéá‰∏äÂùáÂèñÂæóÊòæËëóÊèêÂçá„ÄÇ‰ª•Efficient-DLM 8B‰∏∫‰æãÔºåÂú®ÂáÜÁ°ÆÊÄßÊñπÈù¢ÔºåÁõ∏ÊØîDream 7BÂíåQwen3 4BÔºåÂàÜÂà´ÊèêÈ´ò‰∫Ü+5.4%Âíå+2.7%ÔºõÂú®ÂêûÂêêÈáèÊñπÈù¢ÔºåÂàÜÂà´ËææÂà∞4.5ÂÄçÂíå2.7ÂÄçÁöÑÊèêÂçá„ÄÇËøô‰∫õÁªìÊûúÂü∫‰∫éÊ†áÂáÜÂü∫ÂáÜÊµãËØïÔºåÈ™åËØÅ‰∫ÜÊâÄÊèêÊñπÊ≥ïÂú®AR-to-dLMËΩ¨Êç¢‰∏≠ÁöÑÊúâÊïàÊÄßÔºå‰∏∫ÂêéÁª≠Á†îÁ©∂Êèê‰æõ‰∫ÜÂº∫ÊúâÂäõÂü∫Á∫ø„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂Âú®Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõÔºåÁâπÂà´ÊòØÂú®ÈúÄË¶ÅÈ´òÈÄüÊñáÊú¨ÁîüÊàêÁöÑÂú∫ÊôØ‰∏≠ÔºåÂ¶ÇÂÆûÊó∂ÂØπËØùÁ≥ªÁªü„ÄÅÂÜÖÂÆπÂàõ‰ΩúËæÖÂä©„ÄÅ‰ª£Á†ÅÁîüÊàêÂíåÊú∫Âô®ÁøªËØë„ÄÇEfficient-DLMÊ°ÜÊû∂ÈÄöËøáÊèêÂçáÊâ©Êï£ËØ≠Ë®ÄÊ®°ÂûãÁöÑÊïàÁéáÔºå‰∏∫ÂÆûÈôÖÈÉ®ÁΩ≤Êèê‰æõ‰∫ÜÊõ¥ÂèØË°åÁöÑËß£ÂÜ≥ÊñπÊ°àÔºåÊú™Êù•ÂèØËÉΩÊé®Âä®Âπ∂Ë°åÁîüÊàêÊ®°ÂûãÂú®ËæπÁºòËÆæÂ§áÂíåÂ§ßËßÑÊ®°ÊúçÂä°‰∏≠ÁöÑÊôÆÂèäÔºåÈôç‰ΩéËÆ°ÁÆóÊàêÊú¨Âπ∂ÊîπÂñÑÁî®Êà∑‰ΩìÈ™å„ÄÇ
=======
**ÊèêÂá∫Efficient-DLMÊ°ÜÊû∂ÔºåÈÄöËøáÊîπËøõARÂà∞dLMËΩ¨Êç¢ÊñπÊ≥ïÔºåÂÆûÁé∞È´òÊïàÊâ©Êï£ËØ≠Ë®ÄÊ®°ÂûãÔºåÂú®‰øùÊåÅ‰ªªÂä°ÂáÜÁ°ÆÊÄßÁöÑÂêåÊó∂ÊèêÂçáÁîüÊàêÈÄüÂ∫¶„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ËßÜËßâÈáåÁ®ãËÆ°**

**ÂÖ≥ÈîÆËØç**: `Êâ©Êï£ËØ≠Ë®ÄÊ®°Âûã` `Ëá™ÂõûÂΩíÊ®°ÂûãËΩ¨Êç¢` `Ê≥®ÊÑèÂäõÊ®°Âºè‰ºòÂåñ` `‰ΩçÁΩÆ‰æùËµñÊé©Á†Å` `È´òÊïàÊñáÊú¨ÁîüÊàê` `KVÁºìÂ≠ò` `ËøûÁª≠È¢ÑËÆ≠ÁªÉ` `ÈùûËá™ÂõûÂΩíÁîüÊàê`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâAR-to-dLMËΩ¨Êç¢ÊñπÊ≥ïÂú®Ê≥®ÊÑèÂäõÊ®°ÂºèÂíåÁõÆÊ†á‰∏äÂ≠òÂú®Â±ÄÈôêÊÄßÔºåÂØºËá¥Â≠¶‰π†ÊïàÁéá‰ΩéÂíåËÆ≠ÁªÉ-ÊµãËØïÂàÜÂ∏É‰∏çÂåπÈÖç„ÄÇ
2. ÊèêÂá∫ÂùóÁ∫ßÊ≥®ÊÑèÂäõÊ®°ÂºèÂíå‰ΩçÁΩÆ‰æùËµñÊé©Á†ÅÁ≠ñÁï•Ôºå‰ª•‰øùÊåÅÈ¢ÑËÆ≠ÁªÉÊùÉÈáçÂàÜÂ∏ÉÂπ∂Ê®°ÊãüÊµãËØïË°å‰∏∫ÔºåÂÆûÁé∞È´òÊïàËΩ¨Êç¢„ÄÇ
3. Efficient-DLMÁ≥ªÂàóÊ®°ÂûãÂú®ÂáÜÁ°ÆÊÄßÂíåÂêûÂêêÈáè‰∏äÊòæËëóË∂ÖË∂äÁé∞ÊúâARÂíådLMÊ®°ÂûãÔºåÂ¶Ç8BÁâàÊú¨Áõ∏ÊØîÂü∫ÂáÜÊ®°ÂûãÊèêÂçáÂáÜÁ°ÆÁéáÂπ∂Âä†ÈÄüÁîüÊàê„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êâ©Êï£ËØ≠Ë®ÄÊ®°ÂûãÔºàdLMsÔºâ‰Ωú‰∏∫‰∏ÄÁßçÂπ∂Ë°å„ÄÅÈùûËá™ÂõûÂΩíÁîüÊàêËåÉÂºèÂ±ïÁé∞Âá∫ÊΩúÂäõÔºå‰ΩÜÂÖ∂‰ªéÂ§¥ËÆ≠ÁªÉÁöÑÂ≠¶‰π†ÊïàÁéáËêΩÂêé‰∫éËá™ÂõûÂΩíÔºàARÔºâËØ≠Ë®ÄÊ®°Âûã„ÄÇ‰∏∫Ê≠§ÔºåÊú¨Á†îÁ©∂Êé¢Á¥¢ARÂà∞dLMÁöÑËΩ¨Êç¢ÊñπÊ≥ïÔºåÂ∞ÜÈ¢ÑËÆ≠ÁªÉÁöÑARÊ®°ÂûãËΩ¨Âåñ‰∏∫È´òÊïàÁöÑdLMsÔºåÂú®‰øùÊåÅARÊ®°Âûã‰ªªÂä°ÂáÜÁ°ÆÊÄßÁöÑÂêåÊó∂ÊèêÂçáÈÄüÂ∫¶„ÄÇÊàë‰ª¨ÈÄöËøáÂàÜÊûêÁé∞ÊúâAR-to-dLMÊñπÊ≥ïÂú®Ê≥®ÊÑèÂäõÊ®°ÂºèÂíåÁõÆÊ†á‰∏äÁöÑÂ±ÄÈôêÊÄßÔºåÊèêÂá∫Êõ¥ÊúâÊïàÁöÑËΩ¨Êç¢ÂéüÂàôÂíåÊñπÊ≥ï„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåÈ¶ñÂÖàÁ≥ªÁªüÊØîËæÉ‰∏çÂêåÊ≥®ÊÑèÂäõÊ®°ÂºèÔºåÂèëÁé∞‰øùÊåÅÈ¢ÑËÆ≠ÁªÉARÊùÉÈáçÂàÜÂ∏ÉÂØπÊúâÊïàËΩ¨Êç¢Ëá≥ÂÖ≥ÈáçË¶ÅÔºåÂõ†Ê≠§ÂºïÂÖ•Âü∫‰∫éÂùóÁ∫ßÊ≥®ÊÑèÂäõÊ®°ÂºèÁöÑËøûÁª≠È¢ÑËÆ≠ÁªÉÊñπÊ°àÔºåÂú®ÂùóÈó¥‰øùÊåÅÂõ†ÊûúÊÄß„ÄÅÂùóÂÜÖÂÆûÁé∞ÂèåÂêëÂª∫Ê®°ÔºåËøôÊØîÂÆåÂÖ®ÂèåÂêëÂª∫Ê®°Êõ¥Â•ΩÂú∞‰øùÁïôÊùÉÈáçÂàÜÂ∏ÉÔºåÂπ∂ÊîØÊåÅKVÁºìÂ≠òÔºåÂÆûÁé∞ÂáÜÁ°ÆÊÄßÂíåÊïàÁéáÁöÑÂèåËµ¢„ÄÇÂÖ∂Ê¨°Ôºå‰∏∫ÁºìËß£ËÆ≠ÁªÉ‰∏éÊµãËØï‰∏≠Êé©Á†ÅÊ†áËÆ∞ÂàÜÂ∏ÉÔºàÂùáÂåÄvs.È´òÂ∫¶‰ªéÂ∑¶Âà∞Âè≥ÔºâÁöÑÂ∑ÆË∑ùÔºåÊèêÂá∫‰ΩçÁΩÆ‰æùËµñÁöÑÊ†áËÆ∞Êé©Á†ÅÁ≠ñÁï•ÔºåÂú®ËÆ≠ÁªÉ‰∏≠‰∏∫ÂêéÁª≠Ê†áËÆ∞ÂàÜÈÖçÊõ¥È´òÊé©Á†ÅÊ¶ÇÁéá‰ª•Êõ¥Â•ΩÊ®°ÊãüÊµãËØïË°å‰∏∫„ÄÇÂü∫‰∫éÊ≠§Ê°ÜÊû∂ÔºåÊàë‰ª¨Ê∑±ÂÖ•Á†îÁ©∂‰∫ÜdLMsÁöÑÊ≥®ÊÑèÂäõÊ®°Âºè„ÄÅËÆ≠ÁªÉÂä®ÊÄÅÂíåÂÖ∂‰ªñËÆæËÆ°ÈÄâÊã©Ôºå‰∏∫ÂèØÊâ©Â±ïÁöÑAR-to-dLMËΩ¨Êç¢Êèê‰æõÂÆûÁî®ËßÅËß£„ÄÇËøô‰∫õÁ†îÁ©∂ÂÇ¨Áîü‰∫ÜEfficient-DLMÁ≥ªÂàóÊ®°ÂûãÔºåÂÖ∂ÊÄßËÉΩË∂ÖË∂äÊúÄÂÖàËøõÁöÑARÊ®°ÂûãÂíådLMsÔºå‰æãÂ¶ÇÔºåÊàë‰ª¨ÁöÑEfficient-DLM 8BÁõ∏ÊØîDream 7BÂíåQwen3 4BÔºåÂáÜÁ°ÆÁéáÂàÜÂà´ÊèêÂçá+5.4%Âíå+2.7%ÔºåÂêûÂêêÈáèÂàÜÂà´ÊèêÈ´ò4.5ÂÄçÂíå2.7ÂÄç„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

ËÆ∫ÊñáÊèêÂá∫Efficient-DLMÊ°ÜÊû∂ÔºåÊ†∏ÂøÉÊñπÊ≥ïÂåÖÊã¨AR-to-dLMËΩ¨Êç¢ÁöÑËøûÁª≠È¢ÑËÆ≠ÁªÉÊñπÊ°à„ÄÇÊï¥‰ΩìÊ°ÜÊû∂Âü∫‰∫éÈ¢ÑËÆ≠ÁªÉARÊ®°ÂûãÔºåÈÄöËøáÊîπËøõÊ≥®ÊÑèÂäõÊ®°ÂºèÂíåÊé©Á†ÅÁ≠ñÁï•ÂÆûÁé∞È´òÊïàÊâ©Êï£Âª∫Ê®°„ÄÇÂÖ≥ÈîÆÊäÄÊúØÂàõÊñ∞ÁÇπÂú®‰∫éÂùóÁ∫ßÊ≥®ÊÑèÂäõÊ®°ÂºèÔºåÂÆÉÂú®ÂùóÈó¥‰øùÊåÅÂõ†ÊûúÊÄß‰ª•‰øùÁïôARÊùÉÈáçÂàÜÂ∏ÉÔºåÂùóÂÜÖÂÆûÁé∞ÂèåÂêëÂª∫Ê®°‰ª•ÊîØÊåÅÂπ∂Ë°åÁîüÊàêÔºåÂêåÊó∂ÂºïÂÖ•‰ΩçÁΩÆ‰æùËµñÁöÑÊé©Á†ÅÁ≠ñÁï•Êù•ÁºìËß£ËÆ≠ÁªÉ-ÊµãËØïÂàÜÂ∏ÉÂ∑ÆË∑ù„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁöÑ‰∏ªË¶ÅÂå∫Âà´Âú®‰∫éÈÅøÂÖç‰∫ÜÂÆåÂÖ®ÂèåÂêëÂª∫Ê®°ÂØºËá¥ÁöÑÊùÉÈáçÂàÜÂ∏ÉÁ†¥ÂùèÔºåÂπ∂‰ºòÂåñ‰∫ÜÊé©Á†ÅËøáÁ®ã‰ª•Êõ¥Â•ΩÂú∞Ê®°ÊãüÂÆûÈôÖÁîüÊàêÂú∫ÊôØÔºå‰ªéËÄåÂú®ÂáÜÁ°ÆÊÄßÂíåÊïàÁéá‰∏äÂèñÂæóÂπ≥Ë°°„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Efficient-DLM 8BÊ®°ÂûãÁõ∏ÊØîDream 7BÂíåQwen3 4BÔºåÂáÜÁ°ÆÁéáÂàÜÂà´ÊèêÂçá5.4%Âíå2.7%ÔºåÂêûÂêêÈáèÊèêÈ´ò4.5ÂÄçÂíå2.7ÂÄçÔºåÂ±ïÁ§∫‰∫ÜÂú®‰øùÊåÅ‰ªªÂä°ÂáÜÁ°ÆÊÄßÁöÑÂêåÊó∂ÊòæËëóÂä†ÈÄüÁîüÊàêÁöÑ‰ºòÂäø„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÂèØÂ∫îÁî®‰∫éÈúÄË¶ÅÈ´òÊïàÊñáÊú¨ÁîüÊàêÁöÑËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ‰ªªÂä°ÔºåÂ¶ÇÊú∫Âô®ÁøªËØë„ÄÅÊñáÊú¨ÊëòË¶ÅÂíåÂØπËØùÁ≥ªÁªüÔºåÈÄöËøáÊèêÂçáÊâ©Êï£ËØ≠Ë®ÄÊ®°ÂûãÁöÑÁîüÊàêÈÄüÂ∫¶ÂíåÂáÜÁ°ÆÊÄßÔºåÊîØÊåÅÂ§ßËßÑÊ®°ÂÆûÊó∂Â∫îÁî®ÔºåÈôç‰ΩéËÆ°ÁÆóÊàêÊú¨„ÄÇ
>>>>>>> 1c05e1c356e1f28c2e5e6e14cf6811c0d5120ab7

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Diffusion language models (dLMs) have emerged as a promising paradigm that enables parallel, non-autoregressive generation, but their learning efficiency lags behind that of autoregressive (AR) language models when trained from scratch. To this end, we study AR-to-dLM conversion to transform pretrained AR models into efficient dLMs that excel in speed while preserving AR models' task accuracy. We achieve this by identifying limitations in the attention patterns and objectives of existing AR-to-dLM methods and then proposing principles and methodologies for more effective AR-to-dLM conversion. Specifically, we first systematically compare different attention patterns and find that maintaining pretrained AR weight distributions is critical for effective AR-to-dLM conversion. As such, we introduce a continuous pretraining scheme with a block-wise attention pattern, which remains causal across blocks while enabling bidirectional modeling within each block. We find that this approach can better preserve pretrained AR models' weight distributions than fully bidirectional modeling, in addition to its known benefit of enabling KV caching, and leads to a win-win in accuracy and efficiency. Second, to mitigate the training-test gap in mask token distributions (uniform vs. highly left-to-right), we propose a position-dependent token masking strategy that assigns higher masking probabilities to later tokens during training to better mimic test-time behavior. Leveraging this framework, we conduct extensive studies of dLMs' attention patterns, training dynamics, and other design choices, providing actionable insights into scalable AR-to-dLM conversion. These studies lead to the Efficient-DLM family, which outperforms state-of-the-art AR models and dLMs, e.g., our Efficient-DLM 8B achieves +5.4%/+2.7% higher accuracy with 4.5x/2.7x higher throughput compared to Dream 7B and Qwen3 4B, respectively.

