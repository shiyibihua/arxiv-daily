---
layout: default
title: Sparsity-Controllable Dynamic Top-p MoE for Large Foundation Model Pre-training
---

# Sparsity-Controllable Dynamic Top-p MoE for Large Foundation Model Pre-training

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.13996" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.13996v1</a>
  <a href="https://arxiv.org/pdf/2512.13996.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.13996v1" onclick="toggleFavorite(this, '2512.13996v1', 'Sparsity-Controllable Dynamic Top-p MoE for Large Foundation Model Pre-training')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Can Jin, Hongwu Peng, Mingcan Xiang, Qixin Zhang, Xiangchi Yuan, Amit Hasan, Ohiremen Dibua, Yifan Gong, Yan Kang, Dimitris N. Metaxas

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDTop-p MoEï¼Œå®ç°ç¨€ç–åº¦å¯æ§çš„åŠ¨æ€Top-pè·¯ç”±ï¼Œæå‡å¤§æ¨¡å‹é¢„è®­ç»ƒæ•ˆæœã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ··åˆä¸“å®¶æ¨¡å‹` `MoE` `Top-pè·¯ç”±` `åŠ¨æ€ç¨€ç–æ€§` `PIæ§åˆ¶å™¨` `å¤§æ¨¡å‹é¢„è®­ç»ƒ` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ‰©æ•£æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰Top-k MoEè·¯ç”±ç­–ç•¥å¯¹æ‰€æœ‰tokené‡‡ç”¨ç»Ÿä¸€ç¨€ç–åº¦ï¼Œå¿½ç•¥äº†tokenéš¾åº¦çš„å·®å¼‚ï¼Œé™åˆ¶äº†æ¨¡å‹æ€§èƒ½ã€‚
2. DTop-p MoEåˆ©ç”¨PIæ§åˆ¶å™¨åŠ¨æ€è°ƒæ•´Top-pé˜ˆå€¼ï¼Œå®ç°ç¨€ç–åº¦å¯æ§ï¼Œå¹¶è‡ªé€‚åº”åœ°ä¸ºä¸åŒtokenåˆ†é…è®¡ç®—èµ„æºã€‚
3. å®éªŒè¡¨æ˜ï¼ŒDTop-påœ¨å¤šç§æ¨¡å‹å’Œæ•°æ®é›†ä¸Šä¼˜äºTop-kå’Œå›ºå®šé˜ˆå€¼Top-pï¼Œå¹¶å±•ç°å‡ºè‰¯å¥½çš„æ‰©å±•æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç¨€ç–æ··åˆä¸“å®¶(MoE)æ¶æ„é€šè¿‡ä»…æ¿€æ´»æ¯ä¸ªè¾“å…¥tokençš„ä¸“å®¶å­é›†æ¥æœ‰æ•ˆåœ°æ‰©å±•æ¨¡å‹å®¹é‡ã€‚ç„¶è€Œï¼Œæ ‡å‡†çš„Top-kè·¯ç”±ç­–ç•¥æ–½åŠ äº†ä¸€ç§ç»Ÿä¸€çš„ç¨€ç–æ¨¡å¼ï¼Œå¿½ç•¥äº†tokenéš¾åº¦çš„å˜åŒ–ã€‚è™½ç„¶Top-pè·¯ç”±æä¾›äº†ä¸€ç§çµæ´»çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†ç°æœ‰çš„å®ç°é€šå¸¸ä¾èµ–äºå›ºå®šçš„å…¨å±€æ¦‚ç‡é˜ˆå€¼ï¼Œè¿™å¯¼è‡´äº†ä¸å¯æ§çš„è®¡ç®—æˆæœ¬å’Œå¯¹è¶…å‚æ•°é€‰æ‹©çš„æ•æ„Ÿæ€§ã€‚æœ¬æ–‡æå‡ºäº†DTop-p MoEï¼Œä¸€ç§ç¨€ç–åº¦å¯æ§çš„åŠ¨æ€Top-pè·¯ç”±æœºåˆ¶ã€‚ä¸ºäº†è§£å†³ä¼˜åŒ–ä¸å¯å¾®é˜ˆå€¼çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬åˆ©ç”¨æ¯”ä¾‹-ç§¯åˆ†(PI)æ§åˆ¶å™¨åŠ¨æ€è°ƒæ•´æ¦‚ç‡é˜ˆå€¼ï¼Œä½¿è¿è¡Œæ¿€æ´»çš„ä¸“å®¶ç¨€ç–åº¦ä¸æŒ‡å®šçš„targetå¯¹é½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŠ¨æ€è·¯ç”±å½’ä¸€åŒ–æœºåˆ¶ï¼Œè¯¥æœºåˆ¶è‡ªé€‚åº”åœ°è°ƒæ•´å±‚çº§çš„è·¯ç”±logitsï¼Œå…è®¸ä¸åŒçš„å±‚å­¦ä¹ ä¸åŒçš„ä¸“å®¶é€‰æ‹©æ¨¡å¼ï¼ŒåŒæ—¶ä½¿ç”¨å…¨å±€æ¦‚ç‡é˜ˆå€¼ã€‚åœ¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£Transformerä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDTop-på§‹ç»ˆä¼˜äºTop-kå’Œå›ºå®šé˜ˆå€¼Top-påŸºçº¿ã€‚æˆ‘ä»¬çš„åˆ†æè¯å®ï¼ŒDTop-pä¿æŒå¯¹æ¿€æ´»ä¸“å®¶æ•°é‡çš„ç²¾ç¡®æ§åˆ¶ï¼ŒåŒæ—¶è‡ªé€‚åº”åœ°åœ¨ä¸åŒçš„tokenå’Œå±‚ä¹‹é—´åˆ†é…èµ„æºã€‚æ­¤å¤–ï¼ŒDTop-påœ¨ä¸“å®¶ç²’åº¦ã€ä¸“å®¶å®¹é‡ã€æ¨¡å‹å¤§å°å’Œæ•°æ®é›†å¤§å°æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„ç¼©æ”¾ç‰¹æ€§ï¼Œä¸ºå¤§è§„æ¨¡MoEé¢„è®­ç»ƒæä¾›äº†ä¸€ä¸ªé²æ£’çš„æ¡†æ¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰Top-kè·¯ç”±åœ¨MoEæ¨¡å‹ä¸­å¼ºåˆ¶æ‰§è¡Œç»Ÿä¸€çš„ç¨€ç–æ€§ï¼Œæ— æ³•æ ¹æ®è¾“å…¥tokençš„å¤æ‚ç¨‹åº¦åŠ¨æ€è°ƒæ•´æ¿€æ´»çš„ä¸“å®¶æ•°é‡ã€‚å›ºå®šé˜ˆå€¼çš„Top-pè·¯ç”±è™½ç„¶å¯ä»¥è‡ªé€‚åº”åœ°é€‰æ‹©ä¸“å®¶ï¼Œä½†å¯¹è¶…å‚æ•°æ•æ„Ÿï¼Œä¸”éš¾ä»¥æ§åˆ¶è®¡ç®—æˆæœ¬ï¼Œå¯¼è‡´è®­ç»ƒä¸ç¨³å®šå’Œæ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDTop-p MoEçš„æ ¸å¿ƒåœ¨äºä½¿ç”¨ä¸€ä¸ªæ¯”ä¾‹-ç§¯åˆ†(PI)æ§åˆ¶å™¨æ¥åŠ¨æ€è°ƒæ•´Top-pè·¯ç”±çš„æ¦‚ç‡é˜ˆå€¼ã€‚é€šè¿‡å°†å®é™…æ¿€æ´»çš„ä¸“å®¶æ•°é‡ä¸é¢„è®¾çš„ç›®æ ‡ç¨€ç–åº¦è¿›è¡Œæ¯”è¾ƒï¼ŒPIæ§åˆ¶å™¨è‡ªåŠ¨è°ƒæ•´é˜ˆå€¼ï¼Œä»è€Œå®ç°å¯¹è®¡ç®—æˆæœ¬çš„ç²¾ç¡®æ§åˆ¶ï¼Œå¹¶å…è®¸æ¨¡å‹æ ¹æ®tokençš„éš¾åº¦è‡ªé€‚åº”åœ°é€‰æ‹©ä¸“å®¶ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDTop-p MoEçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä¸€ä¸ªæ ‡å‡†çš„MoEå±‚ï¼Œå…¶ä¸­æ¯ä¸ªtokené€šè¿‡è·¯ç”±ç½‘ç»œé€‰æ‹©ä¸€ç»„ä¸“å®¶ã€‚å…³é”®åœ¨äºè·¯ç”±æœºåˆ¶ï¼Œå®ƒåŒ…å«ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼šé¦–å…ˆï¼Œè®¡ç®—æ¯ä¸ªtokenåˆ°å„ä¸ªä¸“å®¶çš„logitsã€‚ç„¶åï¼Œåº”ç”¨åŠ¨æ€Top-pè·¯ç”±ï¼Œå…¶ä¸­æ¦‚ç‡é˜ˆå€¼ç”±PIæ§åˆ¶å™¨åŠ¨æ€è°ƒæ•´ã€‚æœ€åï¼Œä½¿ç”¨åŠ¨æ€è·¯ç”±å½’ä¸€åŒ–æ¥è°ƒæ•´æ¯å±‚çš„logitsåˆ†å¸ƒï¼Œä½¿å¾—ä¸åŒå±‚å¯ä»¥å­¦ä¹ ä¸åŒçš„ä¸“å®¶é€‰æ‹©æ¨¡å¼ã€‚

**å…³é”®åˆ›æ–°**ï¼šDTop-p MoEçš„å…³é”®åˆ›æ–°åœ¨äºä½¿ç”¨PIæ§åˆ¶å™¨æ¥åŠ¨æ€è°ƒæ•´Top-pé˜ˆå€¼ï¼Œä»è€Œå®ç°ç¨€ç–åº¦å¯æ§ã€‚ä¸å›ºå®šé˜ˆå€¼çš„Top-pè·¯ç”±ç›¸æ¯”ï¼ŒDTop-på¯ä»¥è‡ªåŠ¨é€‚åº”ä¸åŒçš„è®­ç»ƒé˜¶æ®µå’Œæ•°æ®åˆ†å¸ƒï¼Œé¿å…äº†æ‰‹åŠ¨è°ƒæ•´è¶…å‚æ•°çš„éº»çƒ¦ã€‚æ­¤å¤–ï¼ŒåŠ¨æ€è·¯ç”±å½’ä¸€åŒ–å…è®¸ä¸åŒå±‚å­¦ä¹ ä¸åŒçš„ä¸“å®¶é€‰æ‹©æ¨¡å¼ï¼Œè¿›ä¸€æ­¥æé«˜äº†æ¨¡å‹çš„çµæ´»æ€§å’Œè¡¨è¾¾èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šPIæ§åˆ¶å™¨çš„è®¾è®¡æ˜¯DTop-pçš„å…³é”®ã€‚æ§åˆ¶å™¨æ ¹æ®å®é™…æ¿€æ´»çš„ä¸“å®¶æ•°é‡ä¸ç›®æ ‡ç¨€ç–åº¦ä¹‹é—´çš„è¯¯å·®æ¥è°ƒæ•´é˜ˆå€¼ã€‚æ¯”ä¾‹é¡¹æ ¹æ®å½“å‰è¯¯å·®è¿›è¡Œè°ƒæ•´ï¼Œç§¯åˆ†é¡¹åˆ™ç´¯ç§¯å†å²è¯¯å·®ï¼Œä»è€Œå®ç°æ›´ç¨³å®šçš„æ§åˆ¶ã€‚åŠ¨æ€è·¯ç”±å½’ä¸€åŒ–é€šè¿‡å¯¹æ¯å±‚çš„logitsè¿›è¡Œç¼©æ”¾å’Œå¹³ç§»æ¥å®ç°ï¼Œå…¶å‚æ•°æ˜¯å¯å­¦ä¹ çš„ï¼Œå…è®¸ç½‘ç»œè‡ªåŠ¨å­¦ä¹ æœ€ä½³çš„logitsåˆ†å¸ƒã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.13996v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.13996v1/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.13996v1/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒDTop-p MoEåœ¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£Transformerä¸Šå‡ä¼˜äºTop-kå’Œå›ºå®šé˜ˆå€¼Top-påŸºçº¿ã€‚ä¾‹å¦‚ï¼Œåœ¨è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒä¸­ï¼ŒDTop-påœ¨ä¿æŒç›¸åŒè®¡ç®—æˆæœ¬çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿæ˜¾è‘—æé«˜æ¨¡å‹çš„å›°æƒ‘åº¦ã€‚æ­¤å¤–ï¼ŒDTop-påœ¨ä¸åŒä¸“å®¶ç²’åº¦ã€ä¸“å®¶å®¹é‡ã€æ¨¡å‹å¤§å°å’Œæ•°æ®é›†å¤§å°æ–¹é¢å‡è¡¨ç°å‡ºè‰¯å¥½çš„æ‰©å±•æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DTop-p MoEé€‚ç”¨äºå„ç§éœ€è¦å¤§è§„æ¨¡æ¨¡å‹å’Œé«˜æ•ˆè®¡ç®—çš„åœºæ™¯ï¼Œä¾‹å¦‚è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„å¤§å‹è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒã€å›¾åƒç”Ÿæˆä¸­çš„æ‰©æ•£æ¨¡å‹è®­ç»ƒç­‰ã€‚å®ƒå¯ä»¥é™ä½è®¡ç®—æˆæœ¬ï¼Œæé«˜è®­ç»ƒæ•ˆç‡ï¼Œå¹¶æå‡æ¨¡å‹æ€§èƒ½ï¼Œä»è€ŒåŠ é€ŸAIæŠ€æœ¯çš„åº”ç”¨å’Œå‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Sparse Mixture-of-Experts (MoE) architectures effectively scale model capacity by activating only a subset of experts for each input token. However, the standard Top-k routing strategy imposes a uniform sparsity pattern that ignores the varying difficulty of tokens. While Top-p routing offers a flexible alternative, existing implementations typically rely on a fixed global probability threshold, which results in uncontrolled computational costs and sensitivity to hyperparameter selection. In this paper, we propose DTop-p MoE, a sparsity-controllable dynamic Top-p routing mechanism. To resolve the challenge of optimizing a non-differentiable threshold, we utilize a Proportional-Integral (PI) Controller that dynamically adjusts the probability threshold to align the running activated-expert sparsity with a specified target. Furthermore, we introduce a dynamic routing normalization mechanism that adapts layer-wise routing logits, allowing different layers to learn distinct expert-selection patterns while utilizing a global probability threshold. Extensive experiments on Large Language Models and Diffusion Transformers demonstrate that DTop-p consistently outperforms both Top-k and fixed-threshold Top-p baselines. Our analysis confirms that DTop-p maintains precise control over the number of activated experts while adaptively allocating resources across different tokens and layers. Furthermore, DTop-p exhibits strong scaling properties with respect to expert granularity, expert capacity, model size, and dataset size, offering a robust framework for large-scale MoE pre-training.

