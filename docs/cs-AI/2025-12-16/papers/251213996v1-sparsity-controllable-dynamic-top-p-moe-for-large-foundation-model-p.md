---
layout: default
title: Sparsity-Controllable Dynamic Top-p MoE for Large Foundation Model Pre-training
---

# Sparsity-Controllable Dynamic Top-p MoE for Large Foundation Model Pre-training

**arXiv**: [2512.13996v1](https://arxiv.org/abs/2512.13996) | [PDF](https://arxiv.org/pdf/2512.13996.pdf)

**ä½œè€…**: Can Jin, Hongwu Peng, Mingcan Xiang, Qixin Zhang, Xiangchi Yuan, Amit Hasan, Ohiremen Dibua, Yifan Gong, Yan Kang, Dimitris N. Metaxas

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDTop-p MoEä»¥è§£å†³ç¨€ç–ä¸“å®¶æ··åˆæ¨¡åž‹ä¸­åŠ¨æ€æŽ§åˆ¶æ¿€æ´»ä¸“å®¶æ•°é‡çš„é—®é¢˜**

**å…³é”®è¯**: `ç¨€ç–ä¸“å®¶æ··åˆ` `åŠ¨æ€è·¯ç”±æœºåˆ¶` `æ¯”ä¾‹ç§¯åˆ†æŽ§åˆ¶å™¨` `å¤§è§„æ¨¡é¢„è®­ç»ƒ` `æ¨¡åž‹æ‰©å±•æ€§` `è®¡ç®—æˆæœ¬æŽ§åˆ¶` `è‡ªé€‚åº”èµ„æºåˆ†é…` `åŸºç¡€æ¨¡åž‹ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰Top-kè·¯ç”±é‡‡ç”¨ç»Ÿä¸€ç¨€ç–æ¨¡å¼ï¼Œå¿½ç•¥ä»¤ç‰Œéš¾åº¦å·®å¼‚ï¼Œè€ŒTop-pè·¯ç”±ä¾èµ–å›ºå®šé˜ˆå€¼ï¼Œå¯¼è‡´è®¡ç®—æˆæœ¬ä¸å¯æŽ§å’Œè¶…å‚æ•°æ•æ„Ÿã€‚
2. æå‡ºDTop-p MoEï¼Œä½¿ç”¨PIæŽ§åˆ¶å™¨åŠ¨æ€è°ƒæ•´æ¦‚çŽ‡é˜ˆå€¼ä»¥æŽ§åˆ¶ç¨€ç–åº¦ï¼Œå¹¶å¼•å…¥åŠ¨æ€è·¯ç”±å½’ä¸€åŒ–æœºåˆ¶è‡ªé€‚åº”è°ƒæ•´å±‚é—´è·¯ç”±é€»è¾‘ã€‚
3. å®žéªŒè¡¨æ˜ŽDTop-påœ¨å¤§åž‹è¯­è¨€æ¨¡åž‹å’Œæ‰©æ•£å˜æ¢å™¨ä¸­ä¼˜äºŽåŸºçº¿ï¼Œèƒ½ç²¾ç¡®æŽ§åˆ¶æ¿€æ´»ä¸“å®¶æ•°é‡å¹¶è‡ªé€‚åº”åˆ†é…èµ„æºï¼Œå…·æœ‰å¼ºæ‰©å±•æ€§ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç¨€ç–ä¸“å®¶æ··åˆï¼ˆMoEï¼‰æž¶æž„é€šè¿‡ä¸ºæ¯ä¸ªè¾“å…¥ä»¤ç‰Œä»…æ¿€æ´»ä¸“å®¶å­é›†æ¥æœ‰æ•ˆæ‰©å±•æ¨¡åž‹å®¹é‡ã€‚ç„¶è€Œï¼Œæ ‡å‡†çš„Top-kè·¯ç”±ç­–ç•¥é‡‡ç”¨ç»Ÿä¸€çš„ç¨€ç–æ¨¡å¼ï¼Œå¿½ç•¥äº†ä»¤ç‰Œéš¾åº¦çš„å˜åŒ–ã€‚è™½ç„¶Top-pè·¯ç”±æä¾›äº†çµæ´»çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†çŽ°æœ‰å®žçŽ°é€šå¸¸ä¾èµ–äºŽå›ºå®šçš„å…¨å±€æ¦‚çŽ‡é˜ˆå€¼ï¼Œè¿™å¯¼è‡´è®¡ç®—æˆæœ¬ä¸å¯æŽ§ä¸”å¯¹è¶…å‚æ•°é€‰æ‹©æ•æ„Ÿã€‚æœ¬æ–‡æå‡ºDTop-p MoEï¼Œä¸€ç§ç¨€ç–å¯æŽ§çš„åŠ¨æ€Top-pè·¯ç”±æœºåˆ¶ã€‚ä¸ºè§£å†³ä¼˜åŒ–ä¸å¯å¾®åˆ†é˜ˆå€¼çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬åˆ©ç”¨æ¯”ä¾‹ç§¯åˆ†ï¼ˆPIï¼‰æŽ§åˆ¶å™¨åŠ¨æ€è°ƒæ•´æ¦‚çŽ‡é˜ˆå€¼ï¼Œä½¿è¿è¡Œä¸­çš„æ¿€æ´»ä¸“å®¶ç¨€ç–åº¦ä¸ŽæŒ‡å®šç›®æ ‡å¯¹é½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥åŠ¨æ€è·¯ç”±å½’ä¸€åŒ–æœºåˆ¶ï¼Œè‡ªé€‚åº”è°ƒæ•´å±‚é—´è·¯ç”±é€»è¾‘ï¼Œå…è®¸ä¸åŒå±‚å­¦ä¹ ä¸åŒçš„ä¸“å®¶é€‰æ‹©æ¨¡å¼ï¼ŒåŒæ—¶ä½¿ç”¨å…¨å±€æ¦‚çŽ‡é˜ˆå€¼ã€‚åœ¨å¤§åž‹è¯­è¨€æ¨¡åž‹å’Œæ‰©æ•£å˜æ¢å™¨ä¸Šçš„å¤§é‡å®žéªŒè¡¨æ˜Žï¼ŒDTop-på§‹ç»ˆä¼˜äºŽTop-kå’Œå›ºå®šé˜ˆå€¼Top-påŸºçº¿ã€‚æˆ‘ä»¬çš„åˆ†æžè¯å®žï¼ŒDTop-påœ¨ç²¾ç¡®æŽ§åˆ¶æ¿€æ´»ä¸“å®¶æ•°é‡çš„åŒæ—¶ï¼Œè‡ªé€‚åº”åœ°åœ¨ä¸åŒä»¤ç‰Œå’Œå±‚é—´åˆ†é…èµ„æºã€‚æ­¤å¤–ï¼ŒDTop-påœ¨ä¸“å®¶ç²’åº¦ã€ä¸“å®¶å®¹é‡ã€æ¨¡åž‹å¤§å°å’Œæ•°æ®é›†å¤§å°æ–¹é¢è¡¨çŽ°å‡ºå¼ºå¤§çš„æ‰©å±•æ€§ï¼Œä¸ºå¤§è§„æ¨¡MoEé¢„è®­ç»ƒæä¾›äº†ç¨³å¥æ¡†æž¶ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

DTop-p MoEçš„æ•´ä½“æ¡†æž¶åŸºäºŽç¨€ç–ä¸“å®¶æ··åˆæž¶æž„ï¼Œæ ¸å¿ƒåˆ›æ–°ç‚¹åŒ…æ‹¬ï¼š1ï¼‰ä½¿ç”¨æ¯”ä¾‹ç§¯åˆ†ï¼ˆPIï¼‰æŽ§åˆ¶å™¨åŠ¨æ€è°ƒæ•´Top-pè·¯ç”±çš„æ¦‚çŽ‡é˜ˆå€¼ï¼Œé€šè¿‡åé¦ˆæœºåˆ¶ä½¿æ¿€æ´»ä¸“å®¶ç¨€ç–åº¦ä¸Žç›®æ ‡å€¼å¯¹é½ï¼Œè§£å†³é˜ˆå€¼ä¸å¯å¾®åˆ†ä¼˜åŒ–é—®é¢˜ï¼›2ï¼‰å¼•å…¥åŠ¨æ€è·¯ç”±å½’ä¸€åŒ–æœºåˆ¶ï¼Œè‡ªé€‚åº”è°ƒæ•´ä¸åŒå±‚çš„è·¯ç”±é€»è¾‘ï¼Œå…è®¸å„å±‚å­¦ä¹ ç‹¬ç‰¹çš„ä¸“å®¶é€‰æ‹©æ¨¡å¼ï¼ŒåŒæ—¶ç»´æŒå…¨å±€æ¦‚çŽ‡é˜ˆå€¼ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•çš„ä¸»è¦åŒºåˆ«åœ¨äºŽï¼šTop-kè·¯ç”±å¼ºåˆ¶ç»Ÿä¸€æ¿€æ´»ä¸“å®¶æ•°é‡ï¼Œè€ŒDTop-pé€šè¿‡åŠ¨æ€é˜ˆå€¼å®žçŽ°ç¨€ç–å¯æŽ§ï¼›ç›¸æ¯”å›ºå®šé˜ˆå€¼Top-pï¼ŒDTop-pèƒ½ç²¾ç¡®æŽ§åˆ¶è®¡ç®—æˆæœ¬å¹¶å‡å°‘è¶…å‚æ•°æ•æ„Ÿæ€§ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

åœ¨å¤§åž‹è¯­è¨€æ¨¡åž‹å’Œæ‰©æ•£å˜æ¢å™¨å®žéªŒä¸­ï¼ŒDTop-p MoEä¸€è‡´ä¼˜äºŽTop-kå’Œå›ºå®šé˜ˆå€¼Top-påŸºçº¿ï¼Œèƒ½ç²¾ç¡®æŽ§åˆ¶æ¿€æ´»ä¸“å®¶æ•°é‡ï¼Œè‡ªé€‚åº”åˆ†é…èµ„æºï¼Œå¹¶åœ¨ä¸“å®¶ç²’åº¦ã€å®¹é‡ã€æ¨¡åž‹å¤§å°å’Œæ•°æ®é›†å¤§å°æ–¹é¢å±•çŽ°å‡ºå¼ºæ‰©å±•æ€§ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶é€‚ç”¨äºŽå¤§è§„æ¨¡åŸºç¡€æ¨¡åž‹é¢„è®­ç»ƒåœºæ™¯ï¼Œå¦‚å¤§åž‹è¯­è¨€æ¨¡åž‹å’Œæ‰©æ•£å˜æ¢å™¨çš„å¼€å‘ï¼Œèƒ½æœ‰æ•ˆæ‰©å±•æ¨¡åž‹å®¹é‡å¹¶ä¼˜åŒ–èµ„æºåˆ†é…ã€‚æ½œåœ¨åº”ç”¨åŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å›¾åƒç”Ÿæˆå’Œå¤šæ¨¡æ€AIç³»ç»Ÿï¼Œä¸ºæž„å»ºé«˜æ•ˆã€å¯æ‰©å±•çš„AIæ¨¡åž‹æä¾›æŠ€æœ¯æ”¯æ’‘ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Sparse Mixture-of-Experts (MoE) architectures effectively scale model capacity by activating only a subset of experts for each input token. However, the standard Top-k routing strategy imposes a uniform sparsity pattern that ignores the varying difficulty of tokens. While Top-p routing offers a flexible alternative, existing implementations typically rely on a fixed global probability threshold, which results in uncontrolled computational costs and sensitivity to hyperparameter selection. In this paper, we propose DTop-p MoE, a sparsity-controllable dynamic Top-p routing mechanism. To resolve the challenge of optimizing a non-differentiable threshold, we utilize a Proportional-Integral (PI) Controller that dynamically adjusts the probability threshold to align the running activated-expert sparsity with a specified target. Furthermore, we introduce a dynamic routing normalization mechanism that adapts layer-wise routing logits, allowing different layers to learn distinct expert-selection patterns while utilizing a global probability threshold. Extensive experiments on Large Language Models and Diffusion Transformers demonstrate that DTop-p consistently outperforms both Top-k and fixed-threshold Top-p baselines. Our analysis confirms that DTop-p maintains precise control over the number of activated experts while adaptively allocating resources across different tokens and layers. Furthermore, DTop-p exhibits strong scaling properties with respect to expert granularity, expert capacity, model size, and dataset size, offering a robust framework for large-scale MoE pre-training.

