---
layout: default
title: Sparsity-Controllable Dynamic Top-p MoE for Large Foundation Model Pre-training
---

# Sparsity-Controllable Dynamic Top-p MoE for Large Foundation Model Pre-training

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.13996" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.13996v1</a>
  <a href="https://arxiv.org/pdf/2512.13996.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.13996v1" onclick="toggleFavorite(this, '2512.13996v1', 'Sparsity-Controllable Dynamic Top-p MoE for Large Foundation Model Pre-training')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Can Jin, Hongwu Peng, Mingcan Xiang, Qixin Zhang, Xiangchi Yuan, Amit Hasan, Ohiremen Dibua, Yifan Gong, Yan Kang, Dimitris N. Metaxas

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDTop-p MoEï¼Œå®ç°ç¨€ç–æ€§å¯æ§çš„åŠ¨æ€Top-pè·¯ç”±ï¼Œæå‡å¤§æ¨¡å‹é¢„è®­ç»ƒæ•ˆæœã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ··åˆä¸“å®¶æ¨¡å‹` `MoE` `Top-pè·¯ç”±` `åŠ¨æ€ç¨€ç–æ€§` `PIæ§åˆ¶å™¨` `å¤§æ¨¡å‹é¢„è®­ç»ƒ` `Transformer` `è·¯ç”±å½’ä¸€åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰Top-k MoEè·¯ç”±ç­–ç•¥ç¨€ç–æ€§å›ºå®šï¼Œå¿½ç•¥äº†tokenéš¾åº¦çš„å·®å¼‚ï¼Œè€Œå›ºå®šé˜ˆå€¼çš„Top-pè·¯ç”±è®¡ç®—æˆæœ¬ä¸å¯æ§ä¸”å¯¹è¶…å‚æ•°æ•æ„Ÿã€‚
2. DTop-p MoEåˆ©ç”¨PIæ§åˆ¶å™¨åŠ¨æ€è°ƒæ•´Top-pæ¦‚ç‡é˜ˆå€¼ï¼Œä½¿æ¿€æ´»ä¸“å®¶ç¨€ç–æ€§ä¸ç›®æ ‡å¯¹é½ï¼Œå¹¶å¼•å…¥åŠ¨æ€è·¯ç”±å½’ä¸€åŒ–ä»¥é€‚åº”ä¸åŒå±‚çš„ä¸“å®¶é€‰æ‹©ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒDTop-påœ¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£Transformerä¸Šä¼˜äºTop-kå’Œå›ºå®šé˜ˆå€¼Top-pï¼Œå¹¶å±•ç°å‡ºè‰¯å¥½çš„ç¼©æ”¾ç‰¹æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç¨€ç–æ··åˆä¸“å®¶(MoE)æ¶æ„é€šè¿‡ä»…æ¿€æ´»æ¯ä¸ªè¾“å…¥tokençš„ä¸“å®¶å­é›†æ¥æœ‰æ•ˆåœ°æ‰©å±•æ¨¡å‹å®¹é‡ã€‚ç„¶è€Œï¼Œæ ‡å‡†çš„Top-kè·¯ç”±ç­–ç•¥æ–½åŠ äº†ä¸€ç§ç»Ÿä¸€çš„ç¨€ç–æ¨¡å¼ï¼Œå¿½ç•¥äº†tokenéš¾åº¦çš„å˜åŒ–ã€‚è™½ç„¶Top-pè·¯ç”±æä¾›äº†ä¸€ç§çµæ´»çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†ç°æœ‰çš„å®ç°é€šå¸¸ä¾èµ–äºå›ºå®šçš„å…¨å±€æ¦‚ç‡é˜ˆå€¼ï¼Œè¿™å¯¼è‡´äº†ä¸å¯æ§çš„è®¡ç®—æˆæœ¬å’Œå¯¹è¶…å‚æ•°é€‰æ‹©çš„æ•æ„Ÿæ€§ã€‚æœ¬æ–‡æå‡ºäº†DTop-p MoEï¼Œä¸€ç§ç¨€ç–æ€§å¯æ§çš„åŠ¨æ€Top-pè·¯ç”±æœºåˆ¶ã€‚ä¸ºäº†è§£å†³ä¼˜åŒ–ä¸å¯å¾®é˜ˆå€¼çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬åˆ©ç”¨æ¯”ä¾‹-ç§¯åˆ†(PI)æ§åˆ¶å™¨åŠ¨æ€è°ƒæ•´æ¦‚ç‡é˜ˆå€¼ï¼Œä½¿è¿è¡Œæ¿€æ´»çš„ä¸“å®¶ç¨€ç–æ€§ä¸æŒ‡å®šçš„targetå¯¹é½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŠ¨æ€è·¯ç”±å½’ä¸€åŒ–æœºåˆ¶ï¼Œè¯¥æœºåˆ¶è‡ªé€‚åº”åœ°è°ƒæ•´å±‚çº§çš„è·¯ç”±logitsï¼Œå…è®¸ä¸åŒçš„å±‚å­¦ä¹ ä¸åŒçš„ä¸“å®¶é€‰æ‹©æ¨¡å¼ï¼ŒåŒæ—¶ä½¿ç”¨å…¨å±€æ¦‚ç‡é˜ˆå€¼ã€‚åœ¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£Transformerä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDTop-på§‹ç»ˆä¼˜äºTop-kå’Œå›ºå®šé˜ˆå€¼Top-påŸºçº¿ã€‚æˆ‘ä»¬çš„åˆ†æè¯å®ï¼ŒDTop-pä¿æŒå¯¹æ¿€æ´»ä¸“å®¶æ•°é‡çš„ç²¾ç¡®æ§åˆ¶ï¼ŒåŒæ—¶è‡ªé€‚åº”åœ°åœ¨ä¸åŒçš„tokenå’Œå±‚ä¹‹é—´åˆ†é…èµ„æºã€‚æ­¤å¤–ï¼ŒDTop-påœ¨ä¸“å®¶ç²’åº¦ã€ä¸“å®¶å®¹é‡ã€æ¨¡å‹å¤§å°å’Œæ•°æ®é›†å¤§å°æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„ç¼©æ”¾ç‰¹æ€§ï¼Œä¸ºå¤§è§„æ¨¡MoEé¢„è®­ç»ƒæä¾›äº†ä¸€ä¸ªé²æ£’çš„æ¡†æ¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰MoEæ¨¡å‹ä¸­çš„Top-kè·¯ç”±ç­–ç•¥å¯¹æ‰€æœ‰tokené‡‡ç”¨ç›¸åŒçš„ç¨€ç–åº¦ï¼Œæ— æ³•æ ¹æ®tokençš„éš¾æ˜“ç¨‹åº¦åŠ¨æ€è°ƒæ•´è®¡ç®—èµ„æºåˆ†é…ã€‚è€ŒTop-pè·¯ç”±è™½ç„¶å¯ä»¥è‡ªé€‚åº”åœ°é€‰æ‹©ä¸“å®¶ï¼Œä½†ç°æœ‰æ–¹æ³•ä¾èµ–äºå›ºå®šçš„å…¨å±€æ¦‚ç‡é˜ˆå€¼ï¼Œå¯¼è‡´è®¡ç®—å¼€é”€éš¾ä»¥æ§åˆ¶ï¼Œä¸”å¯¹è¶…å‚æ•°çš„é€‰æ‹©éå¸¸æ•æ„Ÿã€‚è¿™é™åˆ¶äº†MoEæ¨¡å‹åœ¨å¤§è§„æ¨¡é¢„è®­ç»ƒä¸­çš„åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDTop-p MoEçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ä¸€ä¸ªæ¯”ä¾‹-ç§¯åˆ†(PI)æ§åˆ¶å™¨åŠ¨æ€åœ°è°ƒæ•´Top-pè·¯ç”±ä¸­çš„æ¦‚ç‡é˜ˆå€¼ï¼Œä»è€Œå®ç°å¯¹æ¿€æ´»ä¸“å®¶æ•°é‡çš„ç²¾ç¡®æ§åˆ¶ã€‚åŒæ—¶ï¼Œå¼•å…¥åŠ¨æ€è·¯ç”±å½’ä¸€åŒ–æœºåˆ¶ï¼Œå…è®¸ä¸åŒå±‚å­¦ä¹ ä¸åŒçš„ä¸“å®¶é€‰æ‹©æ¨¡å¼ï¼Œä»è€Œæ›´å¥½åœ°é€‚åº”ä¸åŒå±‚çš„ç‰¹å¾åˆ†å¸ƒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDTop-p MoEçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) è·¯ç”±logitsç”Ÿæˆï¼šä¸ä¼ ç»ŸMoEç±»ä¼¼ï¼Œé€šè¿‡ä¸€ä¸ªè·¯ç”±ç½‘ç»œä¸ºæ¯ä¸ªtokenç”Ÿæˆé’ˆå¯¹ä¸åŒä¸“å®¶çš„logitsã€‚2) åŠ¨æ€Top-pé€‰æ‹©ï¼šä½¿ç”¨PIæ§åˆ¶å™¨åŠ¨æ€è°ƒæ•´æ¦‚ç‡é˜ˆå€¼ï¼Œæ ¹æ®logitsé€‰æ‹©Top-pçš„ä¸“å®¶ã€‚3) åŠ¨æ€è·¯ç”±å½’ä¸€åŒ–ï¼šè‡ªé€‚åº”åœ°è°ƒæ•´å±‚çº§çš„è·¯ç”±logitsï¼Œå…è®¸ä¸åŒå±‚å­¦ä¹ ä¸åŒçš„ä¸“å®¶é€‰æ‹©æ¨¡å¼ã€‚4) ä¸“å®¶è®¡ç®—ï¼šè¢«é€‰ä¸­çš„ä¸“å®¶å¯¹tokenè¿›è¡Œå¤„ç†ã€‚5) ç»“æœèåˆï¼šå°†ä¸åŒä¸“å®¶çš„è¾“å‡ºè¿›è¡ŒåŠ æƒèåˆã€‚

**å…³é”®åˆ›æ–°**ï¼šDTop-p MoEçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†ä¸€ç§åŸºäºPIæ§åˆ¶å™¨çš„åŠ¨æ€Top-pè·¯ç”±æœºåˆ¶ï¼Œå®ç°äº†å¯¹æ¿€æ´»ä¸“å®¶æ•°é‡çš„ç²¾ç¡®æ§åˆ¶ï¼Œè§£å†³äº†å›ºå®šé˜ˆå€¼Top-pè·¯ç”±è®¡ç®—å¼€é”€ä¸å¯æ§çš„é—®é¢˜ã€‚2) å¼•å…¥äº†åŠ¨æ€è·¯ç”±å½’ä¸€åŒ–æœºåˆ¶ï¼Œå…è®¸ä¸åŒå±‚å­¦ä¹ ä¸åŒçš„ä¸“å®¶é€‰æ‹©æ¨¡å¼ï¼Œæé«˜äº†æ¨¡å‹çš„çµæ´»æ€§å’Œé€‚åº”æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼š1) PIæ§åˆ¶å™¨çš„è®¾è®¡ï¼šPIæ§åˆ¶å™¨æ ¹æ®å½“å‰æ¿€æ´»ä¸“å®¶æ•°é‡ä¸ç›®æ ‡æ•°é‡çš„å·®å€¼ï¼ŒåŠ¨æ€è°ƒæ•´æ¦‚ç‡é˜ˆå€¼ã€‚2) åŠ¨æ€è·¯ç”±å½’ä¸€åŒ–ï¼šé€šè¿‡å­¦ä¹ ä¸€ä¸ªç¼©æ”¾å› å­æ¥è°ƒæ•´æ¯ä¸€å±‚çš„è·¯ç”±logitsï¼Œä½¿å¾—ä¸åŒå±‚å¯ä»¥å­¦ä¹ åˆ°ä¸åŒçš„ä¸“å®¶é€‰æ‹©æ¨¡å¼ã€‚3) æŸå¤±å‡½æ•°ï¼šé™¤äº†å¸¸è§„çš„é¢„è®­ç»ƒæŸå¤±å¤–ï¼Œè¿˜å¯ä»¥æ·»åŠ è¾…åŠ©æŸå¤±æ¥é¼“åŠ±ä¸“å®¶ä¹‹é—´çš„è´Ÿè½½å‡è¡¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒDTop-p MoEåœ¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£Transformerä¸Šå‡ä¼˜äºTop-kå’Œå›ºå®šé˜ˆå€¼Top-påŸºçº¿ã€‚DTop-pèƒ½å¤Ÿç²¾ç¡®æ§åˆ¶æ¿€æ´»ä¸“å®¶æ•°é‡ï¼Œå¹¶è‡ªé€‚åº”åœ°åœ¨ä¸åŒtokenå’Œå±‚ä¹‹é—´åˆ†é…è®¡ç®—èµ„æºã€‚æ­¤å¤–ï¼ŒDTop-påœ¨ä¸“å®¶ç²’åº¦ã€ä¸“å®¶å®¹é‡ã€æ¨¡å‹å¤§å°å’Œæ•°æ®é›†å¤§å°æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„ç¼©æ”¾ç‰¹æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DTop-p MoEå¯åº”ç”¨äºå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ã€è§†è§‰Transformerç­‰æ¨¡å‹çš„é¢„è®­ç»ƒï¼Œå°¤å…¶é€‚ç”¨äºè®¡ç®—èµ„æºå—é™çš„åœºæ™¯ã€‚é€šè¿‡åŠ¨æ€æ§åˆ¶ç¨€ç–æ€§ï¼Œå¯ä»¥åœ¨ä¿è¯æ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œé™ä½è®¡ç®—æˆæœ¬ï¼ŒåŠ é€Ÿæ¨¡å‹è®­ç»ƒã€‚è¯¥æ–¹æ³•è¿˜å¯ç”¨äºæ¨¡å‹å‹ç¼©å’ŒçŸ¥è¯†è’¸é¦ç­‰é¢†åŸŸï¼Œæå‡æ¨¡å‹æ•ˆç‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Sparse Mixture-of-Experts (MoE) architectures effectively scale model capacity by activating only a subset of experts for each input token. However, the standard Top-k routing strategy imposes a uniform sparsity pattern that ignores the varying difficulty of tokens. While Top-p routing offers a flexible alternative, existing implementations typically rely on a fixed global probability threshold, which results in uncontrolled computational costs and sensitivity to hyperparameter selection. In this paper, we propose DTop-p MoE, a sparsity-controllable dynamic Top-p routing mechanism. To resolve the challenge of optimizing a non-differentiable threshold, we utilize a Proportional-Integral (PI) Controller that dynamically adjusts the probability threshold to align the running activated-expert sparsity with a specified target. Furthermore, we introduce a dynamic routing normalization mechanism that adapts layer-wise routing logits, allowing different layers to learn distinct expert-selection patterns while utilizing a global probability threshold. Extensive experiments on Large Language Models and Diffusion Transformers demonstrate that DTop-p consistently outperforms both Top-k and fixed-threshold Top-p baselines. Our analysis confirms that DTop-p maintains precise control over the number of activated experts while adaptively allocating resources across different tokens and layers. Furthermore, DTop-p exhibits strong scaling properties with respect to expert granularity, expert capacity, model size, and dataset size, offering a robust framework for large-scale MoE pre-training.

