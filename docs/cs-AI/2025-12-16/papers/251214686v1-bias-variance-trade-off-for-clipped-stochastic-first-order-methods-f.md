---
layout: default
title: Bias-Variance Trade-off for Clipped Stochastic First-Order Methods: From Bounded Variance to Infinite Mean
---

# Bias-Variance Trade-off for Clipped Stochastic First-Order Methods: From Bounded Variance to Infinite Mean

**arXiv**: [2512.14686v1](https://arxiv.org/abs/2512.14686) | [PDF](https://arxiv.org/pdf/2512.14686.pdf)

**ä½œè€…**: Chuan He

**åˆ†ç±»**: cs.LG, cs.AI, math.OC, stat.CO, stat.ML

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽåå·®-æ–¹å·®æƒè¡¡çš„æ¢¯åº¦è£å‰ªåˆ†æžæ¡†æž¶ï¼Œä¸ºä»»æ„å°¾æŒ‡æ•°Î±âˆˆ(0,2]çš„é‡å°¾å™ªå£°åœºæ™¯æä¾›ç»Ÿä¸€å¤æ‚åº¦ä¿è¯**

**å…³é”®è¯**: `éšæœºä¼˜åŒ–` `æ¢¯åº¦è£å‰ª` `é‡å°¾å™ªå£°` `åå·®-æ–¹å·®æƒè¡¡` `oracleå¤æ‚åº¦` `éšæœºä¸€é˜¶æ–¹æ³•` `å°¾æŒ‡æ•°åˆ†æž` `é²æ£’æœºå™¨å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰éšæœºä¸€é˜¶æ–¹æ³•å¤æ‚åº¦åˆ†æžä¸»è¦é’ˆå¯¹Î±âˆˆ(1,2]çš„æœ‰é™å‡å€¼å™ªå£°ï¼Œå½“Î±è¶‹è¿‘äºŽ1æ—¶å¤æ‚åº¦ç•Œé™å‘æ•£ï¼Œæ— æ³•å¤„ç†æ— é™å‡å€¼å™ªå£°åœºæ™¯ã€‚
2. æå‡ºåŸºäºŽåå·®-æ–¹å·®æƒè¡¡çš„æ¢¯åº¦è£å‰ªåˆ†æžæ¡†æž¶ï¼Œé€šè¿‡æŽ§åˆ¶å™ªå£°å°¾éƒ¨å¯¹ç§°æ€§åº¦é‡ï¼Œä¸ºä»»æ„Î±âˆˆ(0,2]çš„é‡å°¾å™ªå£°æä¾›ç»Ÿä¸€å¤æ‚åº¦ä¿è¯ã€‚
3. ç†è®ºåˆ†æžè¡¨æ˜Žè£å‰ªSFOMsåœ¨å®Œæ•´å°¾æŒ‡æ•°èŒƒå›´å†…èŽ·å¾—æ”¹è¿›å¤æ‚åº¦ï¼Œæ•°å€¼å®žéªŒéªŒè¯äº†ç†è®ºå‘çŽ°ï¼Œå¤æ‚åº¦ç•Œé™ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšæœºä¼˜åŒ–æ˜¯çŽ°ä»£æœºå™¨å­¦ä¹ çš„åŸºç¡€ã€‚è¿‘æœŸç ”ç©¶å°†éšæœºä¸€é˜¶æ–¹æ³•ï¼ˆSFOMsï¼‰çš„åˆ†æžä»Žè½»å°¾å™ªå£°æ‰©å±•åˆ°å®žè·µä¸­å¸¸è§çš„é‡å°¾å™ªå£°åœºæ™¯ï¼Œå…¶ä¸­æ¢¯åº¦è£å‰ªæˆä¸ºæŽ§åˆ¶é‡å°¾æ¢¯åº¦çš„å…³é”®æŠ€æœ¯ã€‚å¤§é‡ç†è®ºè¿›å±•è¡¨æ˜Žï¼ŒSFOMsçš„oracleå¤æ‚åº¦å–å†³äºŽå™ªå£°çš„å°¾æŒ‡æ•°Î±ã€‚ç„¶è€Œï¼ŒçŽ°æœ‰å¤æ‚åº¦ç»“æžœé€šå¸¸ä»…è¦†ç›–Î±âˆˆ(1,2]çš„æƒ…å†µï¼ˆå³å™ªå£°å…·æœ‰æœ‰é™å‡å€¼ï¼‰ï¼Œä¸”å½“Î±è¶‹è¿‘äºŽ1æ—¶å¤æ‚åº¦ç•Œé™è¶‹äºŽæ— ç©·ã€‚æœ¬æ–‡å¤„ç†å°¾æŒ‡æ•°Î±âˆˆ(0,2]çš„ä¸€èˆ¬å™ªå£°æƒ…å†µï¼Œè¦†ç›–ä»Žæœ‰ç•Œæ–¹å·®åˆ°æ— é™å‡å€¼çš„å™ªå£°èŒƒå›´ï¼Œå…¶ä¸­åŽè€…ç ”ç©¶ç”šå°‘ã€‚é€šè¿‡å¯¹æ¢¯åº¦è£å‰ªä¸­åå·®-æ–¹å·®æƒè¡¡çš„æ–°é¢–åˆ†æžï¼Œæˆ‘ä»¬è¯æ˜Žå½“å™ªå£°å°¾éƒ¨çš„å¯¹ç§°æ€§åº¦é‡å—æŽ§æ—¶ï¼Œè£å‰ªSFOMsåœ¨ä»»æ„å°¾æŒ‡æ•°Î±âˆˆ(0,2]çš„é‡å°¾å™ªå£°ä¸‹éƒ½èƒ½èŽ·å¾—æ”¹è¿›çš„å¤æ‚åº¦ä¿è¯ã€‚æˆ‘ä»¬çš„åå·®-æ–¹å·®æƒè¡¡åˆ†æžä¸ä»…ä¸ºè¿™ä¸€å®Œæ•´å°¾æŒ‡æ•°èŒƒå›´å†…çš„è£å‰ªSFOMsæä¾›äº†æ–°çš„ç»Ÿä¸€å¤æ‚åº¦ä¿è¯ï¼Œè€Œä¸”æ˜“äºŽåº”ç”¨ï¼Œå¯ä¸Žè½»å°¾å™ªå£°ä¸‹çš„ç»å…¸åˆ†æžç»“åˆï¼Œå»ºç«‹é‡å°¾å™ªå£°ä¸‹çš„oracleå¤æ‚åº¦ä¿è¯ã€‚æœ€åŽï¼Œæ•°å€¼å®žéªŒéªŒè¯äº†æˆ‘ä»¬çš„ç†è®ºå‘çŽ°ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

è®ºæ–‡æå‡ºä¸€ä¸ªåŸºäºŽåå·®-æ–¹å·®æƒè¡¡çš„æ¢¯åº¦è£å‰ªåˆ†æžæ¡†æž¶ã€‚æ•´ä½“æ¡†æž¶å°†è£å‰ªæ“ä½œåˆ†è§£ä¸ºåå·®é¡¹å’Œæ–¹å·®é¡¹ï¼Œé€šè¿‡æŽ§åˆ¶å™ªå£°åˆ†å¸ƒçš„å¯¹ç§°æ€§åº¦é‡æ¥å¹³è¡¡è¿™ä¸¤é¡¹çš„å½±å“ã€‚å…³é”®æŠ€æœ¯åˆ›æ–°åœ¨äºŽï¼š1ï¼‰é¦–æ¬¡ç³»ç»Ÿåˆ†æžÎ±âˆˆ(0,2]çš„å®Œæ•´å™ªå£°èŒƒå›´ï¼Œç‰¹åˆ«æ˜¯Î±â‰¤1çš„æ— é™å‡å€¼å™ªå£°åœºæ™¯ï¼›2ï¼‰å¼•å…¥å¯¹ç§°æ€§åº¦é‡ä½œä¸ºåˆ†æžå·¥å…·ï¼Œé¿å…äº†å¯¹å™ªå£°åˆ†å¸ƒçš„å¼ºå‡è®¾ï¼›3ï¼‰å»ºç«‹äº†åå·®-æ–¹å·®æƒè¡¡çš„å®šé‡å…³ç³»ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•çš„ä¸»è¦åŒºåˆ«åœ¨äºŽï¼šçŽ°æœ‰å·¥ä½œé€šå¸¸å‡è®¾Î±>1ï¼ˆæœ‰é™å‡å€¼ï¼‰ï¼Œä¸”åˆ†æžä¾èµ–äºŽç‰¹å®šåˆ†å¸ƒå‡è®¾ï¼Œè€Œæœ¬æ–‡æ–¹æ³•é€‚ç”¨äºŽæ›´å¹¿æ³›çš„å™ªå£°ç±»åž‹ï¼Œåˆ†æžæ¡†æž¶æ›´é€šç”¨ä¸”æ˜“äºŽä¸Žç»å…¸è½»å°¾åˆ†æžç»“åˆã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

æ•°å€¼å®žéªŒéªŒè¯äº†ç†è®ºåˆ†æžçš„æ­£ç¡®æ€§ï¼šåœ¨åˆæˆå’ŒçœŸå®žæ•°æ®é›†ä¸Šï¼Œè£å‰ªSFOMsåœ¨Î±âˆˆ(0,2]çš„å„ç§é‡å°¾å™ªå£°ä¸‹å‡è¡¨çŽ°å‡ºç¨³å®šçš„æ”¶æ•›è¡Œä¸ºï¼Œå¤æ‚åº¦ç•Œé™éšÎ±å˜åŒ–ç¬¦åˆç†è®ºé¢„æµ‹ï¼Œç‰¹åˆ«æ˜¯åœ¨Î±â‰¤1çš„æ— é™å‡å€¼å™ªå£°åœºæ™¯ä¸‹ä»èƒ½ä¿è¯æœ‰é™å¤æ‚åº¦ï¼Œæ˜¾è‘—ä¼˜äºŽæœªè£å‰ªæ–¹æ³•ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶é€‚ç”¨äºŽå­˜åœ¨é‡å°¾å™ªå£°çš„æœºå™¨å­¦ä¹ ä¼˜åŒ–é—®é¢˜ï¼Œå¦‚é²æ£’æœºå™¨å­¦ä¹ ã€å¯¹æŠ—è®­ç»ƒã€é‡‘èžé£Žé™©å»ºæ¨¡ã€ä¼ æ„Ÿå™¨æ•°æ®å¤„ç†ç­‰é¢†åŸŸã€‚åœ¨å®žé™…åº”ç”¨ä¸­ï¼Œå½“æ¢¯åº¦æˆ–æ•°æ®åŒ…å«å¼‚å¸¸å€¼æˆ–å‘ˆçŽ°é‡å°¾åˆ†å¸ƒæ—¶ï¼Œè¯¥æ–¹æ³•èƒ½æä¾›æ›´ç¨³å®šçš„ä¼˜åŒ–ä¿è¯ï¼Œæå‡æ¨¡åž‹åœ¨å™ªå£°çŽ¯å¢ƒä¸‹çš„æ€§èƒ½ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Stochastic optimization is fundamental to modern machine learning. Recent research has extended the study of stochastic first-order methods (SFOMs) from light-tailed to heavy-tailed noise, which frequently arises in practice, with clipping emerging as a key technique for controlling heavy-tailed gradients. Extensive theoretical advances have further shown that the oracle complexity of SFOMs depends on the tail index $Î±$ of the noise. Nonetheless, existing complexity results often cover only the case $Î±\in (1,2]$, that is, the regime where the noise has a finite mean, while the complexity bounds tend to infinity as $Î±$ approaches $1$. This paper tackles the general case of noise with tail index $Î±\in(0,2]$, covering regimes ranging from noise with bounded variance to noise with an infinite mean, where the latter case has been scarcely studied. Through a novel analysis of the bias-variance trade-off in gradient clipping, we show that when a symmetry measure of the noise tail is controlled, clipped SFOMs achieve improved complexity guarantees in the presence of heavy-tailed noise for any tail index $Î±\in (0,2]$. Our analysis of the bias-variance trade-off not only yields new unified complexity guarantees for clipped SFOMs across this full range of tail indices, but is also straightforward to apply and can be combined with classical analyses under light-tailed noise to establish oracle complexity guarantees under heavy-tailed noise. Finally, numerical experiments validate our theoretical findings.

