---
layout: default
title: SASQ: Static Activation Scaling for Quantization-Aware Training in Large Language Models
---

# SASQ: Static Activation Scaling for Quantization-Aware Training in Large Language Models

**arXiv**: [2512.14481v1](https://arxiv.org/abs/2512.14481) | [PDF](https://arxiv.org/pdf/2512.14481.pdf)

**‰ΩúËÄÖ**: Shizhuo Mao, Song Chen, Yi Kang

**ÂàÜÁ±ª**: cs.CL, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-12-16

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

<<<<<<< HEAD
**ÊèêÂá∫SASQÊ°ÜÊû∂‰ª•Ëß£ÂÜ≥Â§ßËØ≠Ë®ÄÊ®°ÂûãÈáèÂåñËÆ≠ÁªÉ‰∏≠ÊøÄÊ¥ªÈáèÂåñÂõ†Â≠ê‰ºòÂåñÁöÑÊïàÁéá‰∏éÁ≤æÂ∫¶Âπ≥Ë°°ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **Âº∫ÂåñÂ≠¶‰π†**

**ÂÖ≥ÈîÆËØç**: `Â§ßËØ≠Ë®ÄÊ®°ÂûãÈáèÂåñ` `ÊøÄÊ¥ªÈáèÂåñ` `ÈáèÂåñÊÑüÁü•ËÆ≠ÁªÉ` `ÈùôÊÄÅÊé®ÁêÜ` `ËΩªÈáèÁ∫ß‰ºòÂåñ` `Ê®°ÂûãÈÉ®ÁΩ≤` `ËæπÁºòËÆ°ÁÆó` `Á≤æÂ∫¶ÊèêÂçá`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Ê†∏ÂøÉÈóÆÈ¢òÔºöÁé∞ÊúâÈáèÂåñÊñπÊ≥ïÈù¢‰∏¥Âä®ÊÄÅÈáèÂåñËÆ°ÁÆóÂºÄÈîÄÈ´ò„ÄÅÈùôÊÄÅÈáèÂåñÁ≤æÂ∫¶‰ΩéÔºå‰ª•ÂèäÈáèÂåñÊÑüÁü•ËÆ≠ÁªÉÊùÉÈáçËÆ≠ÁªÉÊàêÊú¨È´òÁöÑÊåëÊàò„ÄÇ
2. ÊñπÊ≥ïË¶ÅÁÇπÔºöÊèêÂá∫SASQÊ°ÜÊû∂Ôºå‰ªÖ‰ºòÂåñÊøÄÊ¥ªÈáèÂåñÂõ†Â≠êËÄå‰∏çÊîπÂèòÈ¢ÑËÆ≠ÁªÉÊùÉÈáçÔºåÂÆûÁé∞ËΩªÈáèÁ∫ßÈáèÂåñÊÑüÁü•ËÆ≠ÁªÉ„ÄÇ
3. ÂÆûÈ™åÊàñÊïàÊûúÔºöÂú®LLaMA2-7B‰∏äÔºåSASQÁöÑÂõ∞ÊÉëÂ∫¶ÊØîQuaRot‰Ωé5.2%ÔºåÊØîFP16Ê®°Âûã‰Ωé4.7%ÔºåË∂ÖË∂ä‰∫ÜÁé∞ÊúâSOTAÊñπÊ°à„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§ßËØ≠Ë®ÄÊ®°ÂûãÂú®Ëá™ÁÑ∂ËØ≠Ë®Ä‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂÖ∂ËßÑÊ®°Â¢ûÈïøË∂ÖËøá‰∫ÜGPUÂÜÖÂ≠òÁöÑËøõÊ≠•ÔºåÁªôÈÉ®ÁΩ≤Â∏¶Êù•‰∫ÜÊåëÊàò„ÄÇÊ®°ÂûãÈáèÂåñÈÄöËøáÈôç‰ΩéÊùÉÈáçÂíåÊøÄÊ¥ªÁöÑÁ≤æÂ∫¶Êù•ÁºìËß£Ëøô‰∏ÄÈóÆÈ¢òÔºå‰ΩÜÁé∞ÊúâËß£ÂÜ≥ÊñπÊ°àÈù¢‰∏¥Ê†πÊú¨ÊÄßÁöÑÊùÉË°°ÔºöÂä®ÊÄÅÈáèÂåñËÆ°ÁÆóÂºÄÈîÄÈ´ò‰∏îÂú®ËæπÁºòËÆæÂ§á‰∏äÈÉ®ÁΩ≤Âõ∞ÈöæÔºåËÄåÈùôÊÄÅÈáèÂåñÂàôÁâ∫Áâ≤‰∫ÜÂáÜÁ°ÆÊÄß„ÄÇÁé∞ÊúâÁöÑÈáèÂåñÊÑüÁü•ËÆ≠ÁªÉÊñπÊ≥ïËøòÈù¢‰∏¥ÊùÉÈáçËÆ≠ÁªÉÊàêÊú¨È´òÁöÑÈóÆÈ¢ò„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜSASQÔºö‰∏Ä‰∏™‰∏ìÈó®ÈíàÂØπÊøÄÊ¥ªÈáèÂåñÂõ†Â≠êÁöÑËΩªÈáèÁ∫ßÈáèÂåñÊÑüÁü•ËÆ≠ÁªÉÊ°ÜÊû∂„ÄÇSASQ‰ªÖ‰ºòÂåñÈáèÂåñÂõ†Â≠êÔºà‰∏çÊîπÂèòÈ¢ÑËÆ≠ÁªÉÊùÉÈáçÔºâÔºåÂÆûÁé∞‰∫ÜÈ´òÁ≤æÂ∫¶ÁöÑÈùôÊÄÅÊé®ÁêÜÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜÈÉ®ÁΩ≤ÊïàÁéá„ÄÇSASQËá™ÈÄÇÂ∫îÂú∞Êà™Êñ≠‰∏Ä‰∫õÂºÇÂ∏∏ÂÄºÔºå‰ªéËÄåÈôç‰Ωé‰∫ÜÈáèÂåñÁöÑÈöæÂ∫¶ÔºåÂêåÊó∂‰øùÁïô‰∫ÜÊøÄÊ¥ªÁöÑÂàÜÂ∏ÉÁâπÊÄß„ÄÇSASQ‰∏ç‰ªÖË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÊúÄÂÖàËøõÈáèÂåñÊñπÊ°àÔºåËøò‰ºò‰∫éÁõ∏Â∫îÁöÑFP16Ê®°Âûã„ÄÇÂú®LLaMA2-7B‰∏äÔºåÂÆÉÂú®WikiText2‰∏äÁöÑÂõ∞ÊÉëÂ∫¶ÊØîQuaRot‰Ωé5.2%ÔºåÊØîFP16Ê®°Âûã‰Ωé4.7%„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Â§ßËØ≠Ë®ÄÊ®°ÂûãÈáèÂåñÈÉ®ÁΩ≤‰∏≠ÁöÑÊ†∏ÂøÉÈóÆÈ¢òÔºåÂç≥Â¶Ç‰ΩïÂú®‰øùÊåÅÈ´òÁ≤æÂ∫¶ÁöÑÂêåÊó∂ÂÆûÁé∞È´òÊïàÁöÑÈùôÊÄÅÊé®ÁêÜ„ÄÇÁé∞ÊúâÊñπÊ≥ïÁöÑÁóõÁÇπÂåÖÊã¨ÔºöÂä®ÊÄÅÈáèÂåñÂõ†ÂÆûÊó∂ËÆ°ÁÆóÈáèÂåñÂèÇÊï∞ÂØºËá¥È´òËÆ°ÁÆóÂºÄÈîÄÂíåÈÉ®ÁΩ≤Âõ∞ÈöæÔºõÈùôÊÄÅÈáèÂåñËôΩÁÑ∂ÈÉ®ÁΩ≤È´òÊïàÔºå‰ΩÜÁ≤æÂ∫¶ÊçüÂ§±ÊòæËëóÔºõËÄå‰º†ÁªüÁöÑÈáèÂåñÊÑüÁü•ËÆ≠ÁªÉÊñπÊ≥ïÈúÄË¶ÅÈáçÊñ∞ËÆ≠ÁªÉÊùÉÈáçÔºåÊàêÊú¨È´òÊòÇ‰∏îÂèØËÉΩÁ†¥ÂùèÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉËß£ÂÜ≥ÊÄùË∑ØÊòØ‰∏ìÊ≥®‰∫é‰ºòÂåñÊøÄÊ¥ªÈáèÂåñÂõ†Â≠êÔºåËÄå‰∏çÊîπÂèòÈ¢ÑËÆ≠ÁªÉÊùÉÈáç„ÄÇËøôÊ†∑ËÆæËÆ°ÁöÑÂéüÂõ†Âú®‰∫éÔºåÊøÄÊ¥ªÁöÑÂàÜÂ∏ÉÁâπÊÄßÂØπÈáèÂåñÁ≤æÂ∫¶ÂΩ±ÂìçÊõ¥Â§ßÔºå‰∏î‰ºòÂåñÈáèÂåñÂõ†Â≠êÊØîÈáçÊñ∞ËÆ≠ÁªÉÊùÉÈáçÊõ¥ËΩªÈáèÁ∫ßÔºåËÉΩÂ§üÂπ≥Ë°°Á≤æÂ∫¶‰∏éÊïàÁéá„ÄÇÈÄöËøáËá™ÈÄÇÂ∫îÊà™Êñ≠ÂºÇÂ∏∏ÂÄºÔºåÈôç‰ΩéÈáèÂåñÈöæÂ∫¶ÔºåÂêåÊó∂‰øùÁïôÊøÄÊ¥ªÁöÑÂàÜÂ∏É‰ø°ÊÅØÔºå‰ªéËÄåÂÆûÁé∞È´òÁ≤æÂ∫¶ÁöÑÈùôÊÄÅÈáèÂåñ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöSASQÁöÑÊï¥‰ΩìÊû∂ÊûÑÊòØ‰∏Ä‰∏™ËΩªÈáèÁ∫ßÁöÑÈáèÂåñÊÑüÁü•ËÆ≠ÁªÉÊ°ÜÊû∂Ôºå‰∏ªË¶ÅÂåÖÂê´‰∏§‰∏™Èò∂ÊÆµÔºöÈ¶ñÂÖàÔºåÂú®È¢ÑËÆ≠ÁªÉÊ®°ÂûãÁöÑÂü∫Á°Ä‰∏äÔºå‰ªÖÂØπÊøÄÊ¥ªÈáèÂåñÂõ†Â≠êËøõË°å‰ºòÂåñÔºõÂÖ∂Ê¨°ÔºåÈÄöËøáËá™ÈÄÇÂ∫îÊà™Êñ≠Êú∫Âà∂Â§ÑÁêÜÊøÄÊ¥ª‰∏≠ÁöÑÂºÇÂ∏∏ÂÄºÔºå‰ª•ÂáèÂ∞ëÈáèÂåñËØØÂ∑Æ„ÄÇÊµÅÁ®ã‰∏äÔºåÂÆÉÈÅøÂÖç‰∫ÜÊùÉÈáçÊõ¥Êñ∞Ôºå‰∏ìÊ≥®‰∫éÈáèÂåñÂèÇÊï∞ÁöÑË∞ÉÊï¥ÔºåÊúÄÁªàËæìÂá∫‰∏Ä‰∏™ÂèØÁõ¥Êé•Áî®‰∫éÈùôÊÄÅÊé®ÁêÜÁöÑÈáèÂåñÊ®°Âûã„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞ÁÇπÊòØÊèêÂá∫‰∫Ü‰∏ÄÁßç‰ªÖ‰ºòÂåñÊøÄÊ¥ªÈáèÂåñÂõ†Â≠êÁöÑËΩªÈáèÁ∫ßÈáèÂåñÊÑüÁü•ËÆ≠ÁªÉÊñπÊ≥ï„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁöÑÊú¨Ë¥®Âå∫Âà´Âú®‰∫éÔºöÂÆÉ‰∏çÊ∂âÂèäÊùÉÈáçËÆ≠ÁªÉÔºå‰ªéËÄåÈôç‰Ωé‰∫ÜËÆ°ÁÆóÊàêÊú¨ÂíåÈÉ®ÁΩ≤Â§çÊùÇÂ∫¶ÔºõÂêåÊó∂ÔºåÈÄöËøáËá™ÈÄÇÂ∫îÊà™Êñ≠ÂºÇÂ∏∏ÂÄºÔºåËß£ÂÜ≥‰∫ÜÊøÄÊ¥ªÂàÜÂ∏É‰∏≠ÊûÅÁ´ØÂÄºÂØπÈáèÂåñÁöÑË¥üÈù¢ÂΩ±ÂìçÔºåËøôÂú®‰º†ÁªüÈùôÊÄÅÈáèÂåñ‰∏≠Â∏∏Ë¢´ÂøΩÁï•„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö‰ΩøÁî®ÈáèÂåñÂõ†Â≠ê‰Ωú‰∏∫ÂèØ‰ºòÂåñÂèÇÊï∞ÔºåÈÄöËøáÊ¢ØÂ∫¶‰∏ãÈôçËøõË°åËÆ≠ÁªÉÔºõÂºïÂÖ•Ëá™ÈÄÇÂ∫îÊà™Êñ≠Êú∫Âà∂ÔºåÊ†πÊçÆÊøÄÊ¥ªÂàÜÂ∏ÉÂä®ÊÄÅË∞ÉÊï¥Êà™Êñ≠ÈòàÂÄºÔºå‰ª•Âπ≥Ë°°Á≤æÂ∫¶ÂíåÈáèÂåñËåÉÂõ¥ÔºõÊçüÂ§±ÂáΩÊï∞ÂèØËÉΩÂü∫‰∫éÊ®°ÂûãËæìÂá∫‰∏éÂéüÂßãÁ≤æÂ∫¶ÁöÑÂ∑ÆÂºÇÔºå‰ΩÜÂÖ∑‰ΩìÁªÜËäÇÂú®ÊëòË¶Å‰∏≠Êú™ÊòéÁ°ÆËØ¥ÊòéÔºõÁΩëÁªúÁªìÊûÑ‰øùÊåÅÈ¢ÑËÆ≠ÁªÉÊùÉÈáç‰∏çÂèòÔºå‰ªÖÂú®ÂâçÂêë‰º†Êí≠‰∏≠Â∫îÁî®ÈáèÂåñÊìç‰Ωú„ÄÇËøô‰∫õËÆæËÆ°Á°Æ‰øù‰∫ÜÊ°ÜÊû∂ÁöÑËΩªÈáèÊÄßÂíåÈ´òÊïàÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÊúÄÈáçË¶ÅÁöÑÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåSASQÂú®LLaMA2-7BÊ®°Âûã‰∏äÂèñÂæó‰∫ÜÊòæËëóÊÄßËÉΩÊèêÂçá„ÄÇÂú®WikiText2Êï∞ÊçÆÈõÜ‰∏äÔºåÂÖ∂Âõ∞ÊÉëÂ∫¶ÊØîÁé∞ÊúâÊúÄÂÖàËøõÈáèÂåñÊñπÊ°àQuaRot‰Ωé5.2%ÔºåÊØîÂéüÂßãFP16Ê®°Âûã‰Ωé4.7%ÔºåËøôË°®ÊòéSASQ‰∏ç‰ªÖË∂ÖË∂ä‰∫ÜÂÖ∂‰ªñÈáèÂåñÊñπÊ≥ïÔºåÁîöËá≥‰ºò‰∫éÂÖ®Á≤æÂ∫¶Ê®°ÂûãÔºåÂÆûÁé∞‰∫ÜÈáèÂåñÂêéÁöÑÁ≤æÂ∫¶Â¢ûÁõä„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂‰∏ªË¶ÅÂ∫îÁî®‰∫éÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑËæπÁºòÈÉ®ÁΩ≤ÂíåËµÑÊ∫êÂèóÈôêÁéØÂ¢ÉÔºåÂ¶ÇÁßªÂä®ËÆæÂ§á„ÄÅÂµåÂÖ•ÂºèÁ≥ªÁªüÂíå‰∫ëËÆ°ÁÆó‰∏≠ÁöÑÈ´òÊïàÊé®ÁêÜ„ÄÇÂÖ∂ÂÆûÈôÖ‰ª∑ÂÄºÂú®‰∫éÈôç‰ΩéÊ®°ÂûãÁöÑÂÜÖÂ≠òÂç†Áî®ÂíåËÆ°ÁÆóÂºÄÈîÄÔºåÂêåÊó∂‰øùÊåÅÈ´òÁ≤æÂ∫¶ÔºåÊúâÂä©‰∫éÊé®Âä®AIÊ®°ÂûãÂú®ÂÆûÊó∂Â∫îÁî®‰∏≠ÁöÑÊôÆÂèä„ÄÇÊú™Êù•ÂΩ±ÂìçÂèØËÉΩÊâ©Â±ïÂà∞ÂÖ∂‰ªñÂ§ßËßÑÊ®°Á•ûÁªèÁΩëÁªúÊ®°ÂûãÁöÑÈáèÂåñ‰ºòÂåñÔºåÊèêÂçáÊï¥‰ΩìAIÁ≥ªÁªüÁöÑËÉΩÊïàÊØî„ÄÇ
=======
**ÊèêÂá∫SASQÈùôÊÄÅÊøÄÊ¥ªÁº©ÊîæÊ°ÜÊû∂Ôºå‰ª•Ëß£ÂÜ≥Â§ßËØ≠Ë®ÄÊ®°ÂûãÈáèÂåñËÆ≠ÁªÉ‰∏≠Á≤æÂ∫¶‰∏éÊïàÁéáÁöÑÊùÉË°°ÈóÆÈ¢ò„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **Âº∫ÂåñÂ≠¶‰π†**

**ÂÖ≥ÈîÆËØç**: `Â§ßËØ≠Ë®ÄÊ®°Âûã` `Ê®°ÂûãÈáèÂåñ` `ÈáèÂåñÊÑüÁü•ËÆ≠ÁªÉ` `ÊøÄÊ¥ªÈáèÂåñ` `ÈùôÊÄÅÊé®ÁêÜ` `ËæπÁºòÈÉ®ÁΩ≤` `ËΩªÈáèÁ∫ßÊ°ÜÊû∂` `Á≤æÂ∫¶ÊèêÂçá`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÈáèÂåñÊñπÊ≥ïÈù¢‰∏¥Âä®ÊÄÅÈáèÂåñËÆ°ÁÆóÂºÄÈîÄÈ´ò„ÄÅÈùôÊÄÅÈáèÂåñÁ≤æÂ∫¶‰ΩéÁöÑÊ†πÊú¨ÊÄßÊùÉË°°Ôºå‰∏îÈáèÂåñÊÑüÁü•ËÆ≠ÁªÉÊàêÊú¨È´ò„ÄÇ
2. SASQÊ°ÜÊû∂‰ªÖ‰ºòÂåñÊøÄÊ¥ªÈáèÂåñÂõ†Â≠êÔºå‰∏çÊîπÂèòÈ¢ÑËÆ≠ÁªÉÊùÉÈáçÔºåÈÄöËøáËá™ÈÄÇÂ∫îÊà™Êñ≠ÂºÇÂ∏∏ÂÄºÂÆûÁé∞ËΩªÈáèÁ∫ßËÆ≠ÁªÉ„ÄÇ
3. Âú®LLaMA2-7B‰∏äÔºåSASQË∂ÖË∂äSOTAÈáèÂåñÊñπÊ°àÔºåÁîöËá≥‰ºò‰∫éFP16Ê®°ÂûãÔºåÊòæËëóÈôç‰ΩéÂõ∞ÊÉëÂ∫¶„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Ëá™ÁÑ∂ËØ≠Ë®Ä‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂÖ∂ËßÑÊ®°Â¢ûÈïøË∂ÖËøá‰∫ÜGPUÂÜÖÂ≠òÁöÑËøõÊ≠•ÔºåÂØºËá¥ÈÉ®ÁΩ≤Èù¢‰∏¥ÊåëÊàò„ÄÇÊ®°ÂûãÈáèÂåñÈÄöËøáÈôç‰ΩéÊùÉÈáçÂíåÊøÄÊ¥ªÁöÑÁ≤æÂ∫¶Êù•ÁºìËß£Ëøô‰∏ÄÈóÆÈ¢òÔºå‰ΩÜÁé∞ÊúâËß£ÂÜ≥ÊñπÊ°àÂ≠òÂú®Ê†πÊú¨ÊÄßÊùÉË°°ÔºöÂä®ÊÄÅÈáèÂåñËÆ°ÁÆóÂºÄÈîÄÈ´ò‰∏îÂú®ËæπÁºòËÆæÂ§á‰∏äÈÉ®ÁΩ≤Âõ∞ÈöæÔºåËÄåÈùôÊÄÅÈáèÂåñÂàôÁâ∫Áâ≤‰∫ÜÁ≤æÂ∫¶„ÄÇÁé∞ÊúâÁöÑÈáèÂåñÊÑüÁü•ËÆ≠ÁªÉÔºàQATÔºâÊñπÊ≥ïËøòÈù¢‰∏¥ÊùÉÈáçËÆ≠ÁªÉÊàêÊú¨È´òÁöÑÈóÆÈ¢ò„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜSASQÔºö‰∏Ä‰∏™‰∏ìÈó®ÈíàÂØπÊøÄÊ¥ªÈáèÂåñÂõ†Â≠êËÆæËÆ°ÁöÑËΩªÈáèÁ∫ßQATÊ°ÜÊû∂„ÄÇSASQ‰ªÖ‰ºòÂåñÈáèÂåñÂõ†Â≠êÔºà‰∏çÊîπÂèòÈ¢ÑËÆ≠ÁªÉÊùÉÈáçÔºâÔºåÂÆûÁé∞‰∫ÜÈ´òÁ≤æÂ∫¶ÁöÑÈùôÊÄÅÊé®ÁêÜÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜÈÉ®ÁΩ≤ÊïàÁéá„ÄÇSASQËá™ÈÄÇÂ∫îÂú∞Êà™Êñ≠‰∏Ä‰∫õÂºÇÂ∏∏ÂÄºÔºå‰ªéËÄåÈôç‰Ωé‰∫ÜÈáèÂåñÁöÑÈöæÂ∫¶ÔºåÂêåÊó∂‰øùÁïô‰∫ÜÊøÄÊ¥ªÁöÑÂàÜÂ∏ÉÁâπÊÄß„ÄÇSASQ‰∏ç‰ªÖË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑSOTAÈáèÂåñÊñπÊ°àÔºåËøò‰ºò‰∫éÁõ∏Â∫îÁöÑFP16Ê®°Âûã„ÄÇÂú®LLaMA2-7B‰∏äÔºåÂÆÉÂú®WikiText2‰∏äÂÆûÁé∞‰∫ÜÊØîQuaRot‰Ωé5.2%ÁöÑÂõ∞ÊÉëÂ∫¶ÔºåÊØîFP16Ê®°Âûã‰Ωé4.7%ÁöÑÂõ∞ÊÉëÂ∫¶„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

SASQÊòØ‰∏Ä‰∏™ËΩªÈáèÁ∫ßÁöÑÈáèÂåñÊÑüÁü•ËÆ≠ÁªÉÊ°ÜÊû∂Ôºå‰∏ìÊ≥®‰∫é‰ºòÂåñÊøÄÊ¥ªÈáèÂåñÂõ†Â≠ê„ÄÇÊï¥‰ΩìÊ°ÜÊû∂Âü∫‰∫éÈ¢ÑËÆ≠ÁªÉÁöÑÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºåÈÄöËøáÈùôÊÄÅÊñπÂºèË∞ÉÊï¥ÊøÄÊ¥ªÁöÑÈáèÂåñÂèÇÊï∞ÔºåËÄå‰∏ç‰øÆÊîπÊùÉÈáç„ÄÇÂÖ≥ÈîÆÊäÄÊúØÂàõÊñ∞ÁÇπÂåÖÊã¨ÔºöËá™ÈÄÇÂ∫îÊà™Êñ≠ÊøÄÊ¥ª‰∏≠ÁöÑÂºÇÂ∏∏ÂÄºÔºå‰ª•ÁÆÄÂåñÈáèÂåñËøáÁ®ãÂπ∂‰øùÊåÅÂàÜÂ∏ÉÁâπÊÄßÔºõ‰ªÖËÆ≠ÁªÉÈáèÂåñÂõ†Â≠êÔºåÈÅøÂÖç‰∫ÜÊùÉÈáçÊõ¥Êñ∞ÁöÑÈ´òÊàêÊú¨„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁöÑ‰∏ªË¶ÅÂå∫Âà´Âú®‰∫éÔºöÁõ∏ÊØîÂä®ÊÄÅÈáèÂåñÔºåSASQÂÆûÁé∞‰∫ÜÈùôÊÄÅÊé®ÁêÜÔºåÂáèÂ∞ë‰∫ÜËÆ°ÁÆóÂºÄÈîÄÔºõÁõ∏ÊØî‰º†ÁªüÈùôÊÄÅÈáèÂåñÔºåÂÆÉÈÄöËøáËÆ≠ÁªÉÊèêÂçá‰∫ÜÁ≤æÂ∫¶ÔºõÁõ∏ÊØîÂÖ®ÈáèQATÔºåÂÆÉÂ§ßÂπÖÈôç‰Ωé‰∫ÜËÆ≠ÁªÉË¥üÊãÖ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Âú®LLaMA2-7BÊ®°Âûã‰∏äÔºåSASQÂú®WikiText2Êï∞ÊçÆÈõÜ‰∏äÂÆûÁé∞‰∫ÜÊØîQuaRotÈáèÂåñÊñπÊ°à‰Ωé5.2%ÁöÑÂõ∞ÊÉëÂ∫¶ÔºåÁîöËá≥ÊØîÂéüÂßãFP16Ê®°Âûã‰Ωé4.7%ÁöÑÂõ∞ÊÉëÂ∫¶ÔºåÂ±ïÁ§∫‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÈÄÇÁî®‰∫éÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑËæπÁºòÈÉ®ÁΩ≤ÂíåËµÑÊ∫êÂèóÈôêÁéØÂ¢ÉÔºåÂ¶ÇÁßªÂä®ËÆæÂ§á„ÄÅÂµåÂÖ•ÂºèÁ≥ªÁªüÔºåËÉΩÊèêÂçáÊ®°ÂûãÊïàÁéáÂπ∂‰øùÊåÅÈ´òÁ≤æÂ∫¶ÔºåÂÖ∑ÊúâÂÆûÈôÖÈÉ®ÁΩ≤‰ª∑ÂÄº„ÄÇ
>>>>>>> 1c05e1c356e1f28c2e5e6e14cf6811c0d5120ab7

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Large language models (LLMs) excel at natural language tasks but face deployment challenges due to their growing size outpacing GPU memory advancements. Model quantization mitigates this issue by lowering weight and activation precision, but existing solutions face fundamental trade-offs: dynamic quantization incurs high computational overhead and poses deployment challenges on edge devices, while static quantization sacrifices accuracy. Existing approaches of quantization-aware training (QAT) further suffer from weight training costs. We propose SASQ: a lightweight QAT framework specifically tailored for activation quantization factors. SASQ exclusively optimizes only the quantization factors (without changing pre-trained weights), enabling static inference with high accuracy while maintaining deployment efficiency. SASQ adaptively truncates some outliers, thereby reducing the difficulty of quantization while preserving the distributional characteristics of the activations. SASQ not only surpasses existing SOTA quantization schemes but also outperforms the corresponding FP16 models. On LLaMA2-7B, it achieves 5.2% lower perplexity than QuaRot and 4.7% lower perplexity than the FP16 model on WikiText2.

