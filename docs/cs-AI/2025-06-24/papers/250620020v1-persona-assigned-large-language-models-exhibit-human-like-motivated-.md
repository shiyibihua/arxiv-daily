---
layout: default
title: Persona-Assigned Large Language Models Exhibit Human-Like Motivated Reasoning
---

# Persona-Assigned Large Language Models Exhibit Human-Like Motivated Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.20020" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.20020v1</a>
  <a href="https://arxiv.org/pdf/2506.20020.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.20020v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.20020v1', 'Persona-Assigned Large Language Models Exhibit Human-Like Motivated Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Saloni Dash, AmÃ©lie Reymond, Emma S. Spiro, Aylin Caliskan

**åˆ†ç±»**: cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-24

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç ”ç©¶è¡¨æ˜äººæ ¼åˆ†é…çš„å¤§å‹è¯­è¨€æ¨¡å‹è¡¨ç°å‡ºäººç±»èˆ¬çš„åŠ¨æœºæ¨ç†**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `åŠ¨æœºæ¨ç†` `è®¤çŸ¥åè§` `äººæ ¼åˆ†é…` `ç§‘å­¦è¯æ®è¯„ä¼°` `ä¿¡æ¯çœŸä¼ªè¾¨åˆ«` `å»åè§æ–¹æ³•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç ”ç©¶è¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹å®¹æ˜“å—åˆ°äººç±»è®¤çŸ¥åè§çš„å½±å“ï¼Œä½†å…¶åœ¨èº«ä»½ä¸€è‡´æ¨ç†æ–¹é¢çš„è¡¨ç°å°šæœªæ·±å…¥æ¢è®¨ã€‚
2. æœ¬æ–‡é€šè¿‡ä¸ºLLMsåˆ†é…8ç§äººæ ¼ï¼Œç ”ç©¶å…¶åœ¨ä¿¡æ¯çœŸä¼ªè¾¨åˆ«å’Œç§‘å­¦è¯æ®è¯„ä¼°ä¸­çš„åŠ¨æœºæ¨ç†ç°è±¡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œåˆ†é…äººæ ¼çš„LLMsåœ¨çœŸä¼ªè¾¨åˆ«ä¸Šå‡†ç¡®ç‡é™ä½9%ï¼Œè€Œåœ¨ä¸å…¶æ”¿æ²»èº«ä»½ä¸€è‡´çš„æƒ…å†µä¸‹ï¼Œè¯„ä¼°ç§‘å­¦è¯æ®çš„æ­£ç¡®ç‡æé«˜90%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

äººç±»çš„æ¨ç†å¸¸å› èº«ä»½ä¿æŠ¤ç­‰åŠ¨æœºè€Œå—åˆ°åè§å½±å“ï¼Œè¿›è€Œå‰Šå¼±ç†æ€§å†³ç­–ã€‚æœ¬æ–‡æ¢è®¨äº†åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­åˆ†é…8ç§äººæ ¼æ˜¯å¦ä¼šå¼•å‘åŠ¨æœºæ¨ç†ã€‚ç ”ç©¶å‘ç°ï¼Œåˆ†é…äººæ ¼çš„LLMsåœ¨ä¿¡æ¯çœŸä¼ªè¾¨åˆ«ä¸Šæ¯”æœªåˆ†é…äººæ ¼çš„æ¨¡å‹é™ä½äº†9%çš„å‡†ç¡®æ€§ï¼Œå°¤å…¶åœ¨ä¸å…¶æ”¿æ²»èº«ä»½ä¸€è‡´çš„æƒ…å†µä¸‹ï¼Œæ”¿æ²»äººæ ¼æ¨¡å‹åœ¨è¯„ä¼°ç§‘å­¦è¯æ®æ—¶çš„æ­£ç¡®ç‡æé«˜äº†90%ã€‚å¸¸è§„çš„å»åè§æ–¹æ³•å¯¹è¿™äº›å½±å“æ•ˆæœä¸ä½³ï¼Œæç¤ºäº†LLMså’Œäººç±»åœ¨èº«ä»½ä¸€è‡´æ¨ç†æ–¹é¢çš„æ½œåœ¨é£é™©ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨æ¢è®¨å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åˆ†é…äººæ ¼åæ˜¯å¦ä¼šè¡¨ç°å‡ºåŠ¨æœºæ¨ç†ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†æ­ç¤ºè¿™ä¸€ç°è±¡çš„å½±å“ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡ä¸ºLLMsåˆ†é…ä¸åŒçš„æ”¿æ²»å’Œç¤¾ä¼šäººå£å±æ€§çš„äººæ ¼ï¼Œç ”ç©¶å…¶åœ¨æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œä»¥æ­ç¤ºåŠ¨æœºæ¨ç†çš„å­˜åœ¨åŠå…¶å½±å“ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶æ¶‰åŠ8ç§LLMsï¼ˆåŒ…æ‹¬å¼€æºå’Œä¸“æœ‰æ¨¡å‹ï¼‰ï¼Œåœ¨ä¸¤ä¸ªæ¨ç†ä»»åŠ¡ä¸Šè¿›è¡Œæµ‹è¯•ï¼šä¿¡æ¯çœŸä¼ªè¾¨åˆ«å’Œç§‘å­¦è¯æ®è¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šé¦–æ¬¡ç³»ç»Ÿæ€§åœ°å±•ç¤ºäº†äººæ ¼åˆ†é…å¯¹LLMsæ¨ç†è¿‡ç¨‹çš„å½±å“ï¼Œå°¤å…¶æ˜¯åœ¨ä¸å…¶èº«ä»½ä¸€è‡´çš„æƒ…å†µä¸‹ï¼Œè¡¨ç°å‡ºæ˜æ˜¾çš„åŠ¨æœºæ¨ç†ç‰¹å¾ã€‚

**å…³é”®è®¾è®¡**ï¼šå®éªŒä¸­ä½¿ç”¨äº†å¤šç§äººæ ¼è®¾ç½®ï¼Œè¯„ä¼°å…¶åœ¨ä¸åŒä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œä¸”å¸¸è§„çš„å»åè§æ–¹æ³•æœªèƒ½æœ‰æ•ˆç¼“è§£è¿™äº›å½±å“ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œåˆ†é…äººæ ¼çš„LLMsåœ¨ä¿¡æ¯çœŸä¼ªè¾¨åˆ«ä¸Šå‡†ç¡®ç‡é™ä½äº†9%ï¼Œè€Œåœ¨ä¸å…¶æ”¿æ²»èº«ä»½ä¸€è‡´çš„æƒ…å†µä¸‹ï¼Œè¯„ä¼°ç§‘å­¦è¯æ®çš„æ­£ç¡®ç‡æé«˜äº†90%ã€‚å¸¸è§„çš„å»åè§æ–¹æ³•æœªèƒ½æœ‰æ•ˆå‡è½»è¿™äº›å½±å“ï¼Œæç¤ºäº†æ½œåœ¨çš„é£é™©ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„å‘ç°å¯¹ç¤¾ä¼šç§‘å­¦ã€å¿ƒç†å­¦å’Œäººå·¥æ™ºèƒ½é¢†åŸŸå…·æœ‰é‡è¦æ„ä¹‰ï¼Œå°¤å…¶æ˜¯åœ¨ç†è§£å’Œè®¾è®¡æ›´å…·äººæ€§åŒ–çš„AIç³»ç»Ÿæ—¶ã€‚æœªæ¥ï¼Œç ”ç©¶ç»“æœå¯ç”¨äºæ”¹å–„AIåœ¨æ•æ„Ÿè¯é¢˜ä¸Šçš„è¡¨ç°ï¼Œå‡å°‘åè§å½±å“ï¼Œä»è€Œä¿ƒè¿›æ›´ç†æ€§çš„å…¬å…±è®¨è®ºã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reasoning in humans is prone to biases due to underlying motivations like identity protection, that undermine rational decision-making and judgment. This motivated reasoning at a collective level can be detrimental to society when debating critical issues such as human-driven climate change or vaccine safety, and can further aggravate political polarization. Prior studies have reported that large language models (LLMs) are also susceptible to human-like cognitive biases, however, the extent to which LLMs selectively reason toward identity-congruent conclusions remains largely unexplored. Here, we investigate whether assigning 8 personas across 4 political and socio-demographic attributes induces motivated reasoning in LLMs. Testing 8 LLMs (open source and proprietary) across two reasoning tasks from human-subject studies -- veracity discernment of misinformation headlines and evaluation of numeric scientific evidence -- we find that persona-assigned LLMs have up to 9% reduced veracity discernment relative to models without personas. Political personas specifically, are up to 90% more likely to correctly evaluate scientific evidence on gun control when the ground truth is congruent with their induced political identity. Prompt-based debiasing methods are largely ineffective at mitigating these effects. Taken together, our empirical findings are the first to suggest that persona-assigned LLMs exhibit human-like motivated reasoning that is hard to mitigate through conventional debiasing prompts -- raising concerns of exacerbating identity-congruent reasoning in both LLMs and humans.

