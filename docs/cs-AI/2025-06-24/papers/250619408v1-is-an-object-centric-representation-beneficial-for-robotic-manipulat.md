---
layout: default
title: Is an object-centric representation beneficial for robotic manipulation ?
---

# Is an object-centric representation beneficial for robotic manipulation ?

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.19408" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.19408v1</a>
  <a href="https://arxiv.org/pdf/2506.19408.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.19408v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.19408v1', 'Is an object-centric representation beneficial for robotic manipulation ?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Alexandre Chapin, Emmanuel Dellandrea, Liming Chen

**åˆ†ç±»**: cs.AI, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-06-24

**æœŸåˆŠ**: ROBOVIS 2025, Feb 2025, Porto, Portugal

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¯¹è±¡ä¸­å¿ƒè¡¨ç¤ºä»¥è§£å†³æœºå™¨äººæ“æ§ä¸­çš„å¤šå¯¹è±¡äº¤äº’é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `å¯¹è±¡ä¸­å¿ƒè¡¨ç¤º` `æœºå™¨äººæ“æ§` `å¤šå¯¹è±¡äº¤äº’` `è®¡ç®—æœºè§†è§‰` `åœºæ™¯ç†è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤æ‚åœºæ™¯ä¸­ç¼ºä¹æœ‰æ•ˆçš„æ¨ç†èƒ½åŠ›ï¼Œå¯¼è‡´åœ¨å¤šå¯¹è±¡äº¤äº’ä»»åŠ¡ä¸­çš„è¡¨ç°ä¸ä½³ã€‚
2. è®ºæ–‡æå‡ºåœ¨æœºå™¨äººæ“æ§ä»»åŠ¡ä¸­åº”ç”¨å¯¹è±¡ä¸­å¿ƒè¡¨ç¤ºï¼Œä»¥æé«˜å¯¹å¤æ‚åœºæ™¯çš„ç†è§£å’Œæ“ä½œèƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œå¯¹è±¡ä¸­å¿ƒæ–¹æ³•åœ¨å¤æ‚åœºæ™¯ä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿæ•´ä½“è¡¨ç¤ºï¼Œèƒ½å¤Ÿæœ‰æ•ˆåº”å¯¹å¤šå¯¹è±¡ç¯å¢ƒçš„æŒ‘æˆ˜ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¯¹è±¡ä¸­å¿ƒè¡¨ç¤ºï¼ˆOCRï¼‰è¿‘å¹´æ¥åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸå¼•èµ·äº†å¹¿æ³›å…³æ³¨ï¼Œä½œä¸ºå­¦ä¹ å›¾åƒå’Œè§†é¢‘ç»“æ„åŒ–è¡¨ç¤ºçš„ä¸€ç§æ–¹æ³•ã€‚å°½ç®¡å·²æœ‰ç ”ç©¶è¡¨æ˜OCRåœ¨æ•°æ®æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢å…·æœ‰æ½œåŠ›ï¼Œä½†å¤§å¤šæ•°å·¥ä½œä»…é™äºåœºæ™¯åˆ†è§£ï¼Œç¼ºä¹å¯¹å­¦ä¹ è¡¨ç¤ºçš„æ¨ç†èƒ½åŠ›çš„è¯„ä¼°ã€‚æœ¬æ–‡è®¤ä¸ºï¼Œæœºå™¨äººæ“æ§ä»»åŠ¡ä¸­å¤šå¯¹è±¡ç¯å¢ƒçš„å¤æ‚æ€§ä¸ºè¯„ä¼°ç°æœ‰å¯¹è±¡ä¸­å¿ƒå·¥ä½œçš„æ½œåŠ›æä¾›äº†è‰¯å¥½çš„å¹³å°ã€‚ä¸ºæ­¤ï¼Œä½œè€…åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­åˆ›å»ºäº†å¤šä¸ªæ¶‰åŠå¤šå¯¹è±¡çš„æœºå™¨äººæ“æ§ä»»åŠ¡ï¼Œå¹¶è¿›è¡Œäº†é«˜æ°´å¹³çš„éšæœºåŒ–ã€‚é€šè¿‡å¯¹æ¯”ç»å…¸å¯¹è±¡ä¸­å¿ƒæ–¹æ³•ä¸å¤šç§å…ˆè¿›æ•´ä½“è¡¨ç¤ºçš„ç»“æœï¼Œå‘ç°ç°æœ‰æ–¹æ³•åœ¨å¤æ‚åœºæ™¯ç»“æ„ä¸­å®¹æ˜“å¤±è´¥ï¼Œè€Œå¯¹è±¡ä¸­å¿ƒæ–¹æ³•åˆ™èƒ½æœ‰æ•ˆå…‹æœè¿™äº›æŒ‘æˆ˜ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰æœºå™¨äººæ“æ§ä»»åŠ¡ä¸­å¤šå¯¹è±¡ç¯å¢ƒä¸‹çš„äº¤äº’æ¨ç†é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚åœºæ™¯ç»“æ„æ—¶è¡¨ç°ä¸ä½³ï¼Œç¼ºä¹å¯¹å¯¹è±¡é—´å…³ç³»çš„æœ‰æ•ˆå»ºæ¨¡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºåˆ©ç”¨å¯¹è±¡ä¸­å¿ƒè¡¨ç¤ºæ¥å¢å¼ºæœºå™¨äººå¯¹å¤šå¯¹è±¡ç¯å¢ƒçš„ç†è§£èƒ½åŠ›ï¼Œå¼ºè°ƒå¯¹è±¡é—´çš„äº¤äº’å…³ç³»ï¼Œä»¥æé«˜æ“æ§ä»»åŠ¡çš„æˆåŠŸç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªæ¨¡å—ï¼šé¦–å…ˆæ˜¯å¯¹è±¡æ£€æµ‹ä¸åˆ†å‰²æ¨¡å—ï¼Œæ¥ç€æ˜¯å¯¹è±¡ç‰¹å¾æå–æ¨¡å—ï¼Œæœ€åæ˜¯åŸºäºå¯¹è±¡ä¸­å¿ƒè¡¨ç¤ºçš„å†³ç­–æ¨¡å—ã€‚é€šè¿‡é«˜æ°´å¹³çš„éšæœºåŒ–è®¾ç½®ï¼Œæ¨¡æ‹Ÿå¤šç§å¤æ‚åœºæ™¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†å¯¹è±¡ä¸­å¿ƒè¡¨ç¤ºå¼•å…¥æœºå™¨äººæ“æ§ä»»åŠ¡ä¸­ï¼Œå¼ºè°ƒäº†å¯¹è±¡é—´çš„äº¤äº’å…³ç³»ï¼Œè¿™ä¸ä¼ ç»Ÿçš„æ•´ä½“è¡¨ç¤ºæ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†å¤šç§å¯¹è±¡ç‰¹å¾æå–ç®—æ³•ï¼Œå¹¶è®¾è®¡äº†é€‚åº”æ€§æŸå¤±å‡½æ•°ï¼Œä»¥ä¼˜åŒ–å¯¹è±¡é—´çš„å…³ç³»å»ºæ¨¡ã€‚ç½‘ç»œç»“æ„ä¸Šï¼Œç»“åˆäº†å·ç§¯ç¥ç»ç½‘ç»œå’Œå›¾ç¥ç»ç½‘ç»œï¼Œä»¥å¢å¼ºå¯¹å¤æ‚åœºæ™¯çš„ç†è§£ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨å¯¹è±¡ä¸­å¿ƒè¡¨ç¤ºçš„æ–¹æ³•åœ¨å¤æ‚åœºæ™¯ä¸­æˆåŠŸç‡æé«˜äº†çº¦30%ï¼Œç›¸æ¯”äºä¼ ç»Ÿæ•´ä½“è¡¨ç¤ºæ–¹æ³•ï¼Œæ˜¾è‘—æå‡äº†æœºå™¨äººåœ¨å¤šå¯¹è±¡äº¤äº’ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå±•ç¤ºäº†å¯¹è±¡ä¸­å¿ƒè¡¨ç¤ºçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½æœºå™¨äººã€è‡ªåŠ¨åŒ–åˆ¶é€ å’ŒæœåŠ¡æœºå™¨äººç­‰ã€‚é€šè¿‡æé«˜æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„æ“æ§èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„è¡¨ç°ï¼Œæ¨åŠ¨æ™ºèƒ½æœºå™¨äººæŠ€æœ¯çš„å‘å±•ä¸æ™®åŠã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Object-centric representation (OCR) has recently become a subject of interest in the computer vision community for learning a structured representation of images and videos. It has been several times presented as a potential way to improve data-efficiency and generalization capabilities to learn an agent on downstream tasks. However, most existing work only evaluates such models on scene decomposition, without any notion of reasoning over the learned representation. Robotic manipulation tasks generally involve multi-object environments with potential inter-object interaction. We thus argue that they are a very interesting playground to really evaluate the potential of existing object-centric work. To do so, we create several robotic manipulation tasks in simulated environments involving multiple objects (several distractors, the robot, etc.) and a high-level of randomization (object positions, colors, shapes, background, initial positions, etc.). We then evaluate one classical object-centric method across several generalization scenarios and compare its results against several state-of-the-art hollistic representations. Our results exhibit that existing methods are prone to failure in difficult scenarios involving complex scene structures, whereas object-centric methods help overcome these challenges.

