---
layout: default
title: The Hidden Cost of Readability: How Code Formatting Silently Consumes Your LLM Budget
---

# The Hidden Cost of Readability: How Code Formatting Silently Consumes Your LLM Budget

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.13666" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.13666v1</a>
  <a href="https://arxiv.org/pdf/2508.13666.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.13666v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.13666v1', 'The Hidden Cost of Readability: How Code Formatting Silently Consumes Your LLM Budget')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Dangfeng Pan, Zhensu Sun, Cenyuan Zhang, David Lo, Xiaoning Du

**ÂàÜÁ±ª**: cs.SE, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-19

**Â§áÊ≥®**: Accepted by ICSE'26 (First Cycle)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫‰ª£Á†ÅÊ†ºÂºè‰ºòÂåñÁ≠ñÁï•‰ª•Èôç‰ΩéLLMËÆ°ÁÆóÊàêÊú¨**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `‰ª£Á†ÅÊ†ºÂºèÂåñ` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `ËÆ°ÁÆóÊïàÁéá` `ÊÄßËÉΩ‰ºòÂåñ` `ÁºñÁ®ãËØ≠Ë®Ä` `ÂÆûÈ™åÁ†îÁ©∂` `Êô∫ËÉΩÁºñÁ®ã` `Â∑•ÂÖ∑ÂºÄÂèë`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞Êúâ‰ª£Á†ÅÊ†ºÂºèÂåñÊñπÊ≥ïÂú®ÊèêÂçá‰∫∫Á±ªÂèØËØªÊÄßÊñπÈù¢ÊúâÊïàÔºå‰ΩÜÂØπLLMÁöÑÊÄßËÉΩÂíåÊïàÁéáÂç¥ÈÄ†Êàê‰∫ÜÈ¢ùÂ§ñÁöÑËÆ°ÁÆóË¥üÊãÖ„ÄÇ
2. Êú¨ÊñáÊèêÂá∫ÈÄöËøáÂéªÈô§‰ª£Á†Å‰∏≠ÁöÑÊ†ºÂºèÂåñÂÖÉÁ¥†Êù•‰ºòÂåñLLMÁöÑËæìÂÖ•ÔºåÈôç‰ΩéËÆ°ÁÆóÊàêÊú¨ÔºåÂêåÊó∂‰øùÊåÅÊÄßËÉΩÁ®≥ÂÆö„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂéªÈô§Ê†ºÂºèÂåñÂÖÉÁ¥†ÂèØ‰ΩøËæìÂÖ•Ê†áËÆ∞ÂáèÂ∞ë24.5%ÔºåÂπ∂‰∏îÈÄöËøáÊèêÁ§∫ÂíåÂæÆË∞ÉÂèØËøõ‰∏ÄÊ≠•ÂáèÂ∞ëËæìÂá∫‰ª£Á†ÅÈïøÂ∫¶ÔºåÊúÄÈ´òÂèØËææ36.1%„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Ê∫ê‰ª£Á†ÅÈÄöÂ∏∏ÈÄöËøáÁº©ËøõÂíåÊç¢Ë°åÁ≠âÂÖÉÁ¥†ËøõË°åÊ†ºÂºèÂåñÔºå‰ª•ÊèêÈ´ò‰∫∫Á±ªÂºÄÂèëËÄÖÁöÑÂèØËØªÊÄß„ÄÇÁÑ∂ËÄåÔºåËøô‰∫õËßÜËßâËæÖÂä©ÂØπÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂπ∂Ê≤°ÊúâÊòæËëóÁõäÂ§ÑÔºåÂõ†‰∏∫‰ª£Á†ÅË¢´Â§ÑÁêÜ‰∏∫Á∫øÊÄßÂ∫èÂàóÁöÑÊ†áËÆ∞„ÄÇÊ≠§Â§ñÔºåËøô‰∫õÈ¢ùÂ§ñÁöÑÊ†áËÆ∞‰ºöÂØºËá¥ËÆ°ÁÆóÊàêÊú¨Â¢ûÂä†ÂíåÂìçÂ∫îÊó∂Èó¥Âª∂Èïø„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåÊú¨ÊñáÈÄöËøáÂØπÂõõÁßçÁºñÁ®ãËØ≠Ë®ÄÔºàJava„ÄÅPython„ÄÅC++„ÄÅC#ÔºâÂíåÂçÅÁßçLLMËøõË°åÂ§ßËßÑÊ®°ÂÆûÈ™åÔºåÁ≥ªÁªüÂàÜÊûê‰∫ÜÂéªÈô§Ê†ºÂºèÂåñÂÖÉÁ¥†ÂØπLLMÊÄßËÉΩÂíåÊïàÁéáÁöÑÂΩ±Âìç„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåLLMÂú®Ê†ºÂºèÂåñÂíåÊú™Ê†ºÂºèÂåñ‰ª£Á†Å‰πãÈó¥ÁöÑÊÄßËÉΩ‰øùÊåÅ‰∏ÄËá¥ÔºåËæìÂÖ•Ê†áËÆ∞Âπ≥ÂùáÂáèÂ∞ë24.5%ÔºåËæìÂá∫Ê†áËÆ∞ÂáèÂ∞ëÂæÆ‰πéÂÖ∂ÂæÆ„ÄÇËøô‰ΩøÂæóÂéªÈô§‰ª£Á†ÅÊ†ºÂºèÊàê‰∏∫ÊèêÈ´òLLMÊïàÁéáÁöÑÂÆûÁî®‰ºòÂåñÁ≠ñÁï•„ÄÇËøõ‰∏ÄÊ≠•Á†îÁ©∂Ë°®ÊòéÔºåÊèêÁ§∫ÂíåÂæÆË∞ÉLLMÂèØ‰ª•ÊòæËëóÂáèÂ∞ëËæìÂá∫‰ª£Á†ÅÈïøÂ∫¶ÔºàÊúÄÈ´òËææ36.1%ÔºâÔºåËÄå‰∏çÂΩ±ÂìçÊ≠£Á°ÆÊÄß„ÄÇ‰∏∫Ê≠§ÔºåÊú¨ÊñáÂºÄÂèë‰∫Ü‰∏ÄÁßçÂèåÂêë‰ª£Á†ÅËΩ¨Êç¢Â∑•ÂÖ∑Ôºå‰æø‰∫éÂú®Áé∞ÊúâLLMÊé®ÁêÜÂ∑•‰ΩúÊµÅ‰∏≠ÈõÜÊàêÔºåÁ°Æ‰øù‰∫∫Á±ªÂèØËØªÊÄßÂíåLLMÊïàÁéá„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥‰ª£Á†ÅÊ†ºÂºèÂåñÂØπLLMÊÄßËÉΩÂíåËÆ°ÁÆóÊàêÊú¨ÁöÑÂΩ±ÂìçÔºåÁé∞ÊúâÊñπÊ≥ïÊú™ËÉΩËÄÉËôëÊ†ºÂºèÂåñÂÖÉÁ¥†ÁöÑÂÜó‰ΩôÊÄßÔºåÂØºËá¥‰∏çÂøÖË¶ÅÁöÑËÆ°ÁÆóÂºÄÈîÄ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÈÄöËøáÂéªÈô§‰ª£Á†Å‰∏≠ÁöÑÊ†ºÂºèÂåñÂÖÉÁ¥†ÔºåËØÑ‰º∞ÂÖ∂ÂØπLLMÊÄßËÉΩÁöÑÂΩ±ÂìçÔºå‰ªéËÄåÊèêÂá∫‰∏ÄÁßç‰ºòÂåñÁ≠ñÁï•Ôºå‰ª•Èôç‰ΩéËÆ°ÁÆóÊàêÊú¨Âπ∂ÊèêÈ´òÊïàÁéá„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÁ†îÁ©∂ÈááÁî®Â§ßËßÑÊ®°ÂÆûÈ™åËÆæËÆ°ÔºåÊ∂µÁõñÂõõÁßçÁºñÁ®ãËØ≠Ë®ÄÂíåÂçÅÁßçLLMÔºåÂàÜÊûêÂéªÈô§Ê†ºÂºèÂåñÂÖÉÁ¥†ÂâçÂêéÁöÑÊÄßËÉΩÂèòÂåñÔºå‰∏ªË¶ÅÊ®°ÂùóÂåÖÊã¨Êï∞ÊçÆÈ¢ÑÂ§ÑÁêÜ„ÄÅÂÆûÈ™åËÆæËÆ°„ÄÅÊÄßËÉΩËØÑ‰º∞Á≠â„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊú¨ÊñáÁöÑÂàõÊñ∞Âú®‰∫éÁ≥ªÁªüÊÄßÂú∞ÂàÜÊûê‰∫Ü‰ª£Á†ÅÊ†ºÂºèÂØπLLMÁöÑÂΩ±ÂìçÔºåÊèêÂá∫‰∫ÜÂéªÈô§Ê†ºÂºèÂåñÂÖÉÁ¥†‰Ωú‰∏∫‰ºòÂåñÁ≠ñÁï•Ôºå‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåÊèê‰æõ‰∫ÜÊõ¥‰∏∫È´òÊïàÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂÆûÈ™å‰∏≠ÔºåËÆæÁΩÆ‰∫Ü‰∏çÂêåÁöÑÊ†ºÂºèÂåñÂÖÉÁ¥†ÁªÑÂêàÔºåÈááÁî®Ê†áÂáÜÁöÑÊÄßËÉΩËØÑ‰º∞ÊåáÊ†áÔºåÁ°Æ‰øùÂÆûÈ™åÁªìÊûúÁöÑÂèØÈù†ÊÄßÂíåÂèØÈáçÂ§çÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÂéªÈô§‰ª£Á†ÅÊ†ºÂºèÂåñÂÖÉÁ¥†ÂêéÔºåLLMÁöÑËæìÂÖ•Ê†áËÆ∞Âπ≥ÂùáÂáèÂ∞ë24.5%ÔºåËÄåËæìÂá∫Ê†áËÆ∞ÂáèÂ∞ëÂπÖÂ∫¶ÂæÆÂ∞è„ÄÇÊ≠§Â§ñÔºåÈÄöËøáÊèêÁ§∫ÂíåÂæÆË∞ÉÔºåËæìÂá∫‰ª£Á†ÅÈïøÂ∫¶ÂèØÂáèÂ∞ëÊúÄÈ´òËææ36.1%ÔºåÂú®‰øùÊåÅÊ≠£Á°ÆÊÄßÁöÑÂâçÊèê‰∏ãÊòæËëóÊèêÂçá‰∫ÜÊïàÁéá„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨‰ª£Á†ÅËá™Âä®ÁîüÊàê„ÄÅÊô∫ËÉΩÁºñÁ®ãÂä©ÊâãÂíåËΩØ‰ª∂ÂºÄÂèëÂ∑•ÂÖ∑Á≠â„ÄÇÈÄöËøá‰ºòÂåñ‰ª£Á†ÅÊ†ºÂºèÔºåËÉΩÂ§üÊúâÊïàÈôç‰ΩéLLMÁöÑËÆ°ÁÆóÊàêÊú¨ÔºåÊèêÈ´òÂºÄÂèëÊïàÁéáÔºåÊé®Âä®Êô∫ËÉΩÁºñÁ®ãÊäÄÊúØÁöÑËøõ‰∏ÄÊ≠•ÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Source code is usually formatted with elements like indentation and newlines to improve readability for human developers. However, these visual aids do not seem to be beneficial for large language models (LLMs) in the same way since the code is processed as a linear sequence of tokens. Furthermore, these additional tokens can lead to increased computational costs and longer response times for LLMs. If such formatting elements are non-essential to LLMs, we can reduce such costs by removing them from the code. To figure out the role played by formatting elements, we conduct a comprehensive empirical study to evaluate the impact of code formatting on LLM performance and efficiency. Through large-scale experiments on Fill-in-the-Middle Code Completion tasks across four programming languages (Java, Python, C++, C\#) and ten LLMs-including both commercial and open-source models-we systematically analyze token count and performance when formatting elements are removed. Key findings indicate that LLMs can maintain performance across formatted code and unformatted code, achieving an average input token reduction of 24.5\% with negligible output token reductions. This makes code format removal a practical optimization strategy for improving LLM efficiency. Further exploration reveals that both prompting and fine-tuning LLMs can lead to significant reductions (up to 36.1\%) in output code length without compromising correctness. To facilitate practical applications, we develop a bidirectional code transformation tool for format processing, which can be seamlessly integrated into existing LLM inference workflows, ensuring both human readability and LLM efficiency.

