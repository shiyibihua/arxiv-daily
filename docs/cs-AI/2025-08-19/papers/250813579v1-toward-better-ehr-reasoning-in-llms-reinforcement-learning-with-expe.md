---
layout: default
title: Toward Better EHR Reasoning in LLMs: Reinforcement Learning with Expert Attention Guidance
---

# Toward Better EHR Reasoning in LLMs: Reinforcement Learning with Expert Attention Guidance

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.13579" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.13579v1</a>
  <a href="https://arxiv.org/pdf/2508.13579.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.13579v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.13579v1', 'Toward Better EHR Reasoning in LLMs: Reinforcement Learning with Expert Attention Guidance')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yue Fang, Yuxin Guo, Jiaran Gao, Hongxin Ding, Xinke Jiang, Weibin Liao, Yongxin Xu, Yinghao Zhu, Zhibang Yang, Liantao Ma, Junfeng Zhao, Yasha Wang

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-19

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/devilran6/EAG-RL)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEAG-RLä»¥è§£å†³LLMsåœ¨EHRæ¨ç†ä¸­çš„ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç”µå­å¥åº·è®°å½•` `å¤§å‹è¯­è¨€æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `ä¸“å®¶æ¨¡å‹` `ä¸´åºŠé¢„æµ‹` `æ¨ç†èƒ½åŠ›` `æ•°æ®åˆ†æ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨EHRæ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ï¼Œä¸»è¦ç”±äºé«˜ç»´æ•°æ®çš„æ—¶é—´ç»“æ„å»ºæ¨¡å›°éš¾ã€‚
2. æœ¬æ–‡æå‡ºEAG-RLæ¡†æ¶ï¼Œé€šè¿‡ä¸“å®¶æ¨¡å‹æŒ‡å¯¼çš„å¼ºåŒ–å­¦ä¹ æ¥æå‡LLMsçš„æ¨ç†èƒ½åŠ›ã€‚
3. åœ¨çœŸå®EHRæ•°æ®é›†ä¸Šï¼ŒEAG-RLä½¿LLMsçš„æ¨ç†èƒ½åŠ›å¹³å‡æå‡14.62%ï¼Œå¹¶å¢å¼ºäº†å¯¹ç‰¹å¾æ‰°åŠ¨çš„é²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç”µå­å¥åº·è®°å½•ï¼ˆEHRï¼‰æ¨ç†ä¸­çš„èƒ½åŠ›å¯¹äºå®ç°å‡†ç¡®ä¸”å¯æ¨å¹¿çš„ä¸´åºŠé¢„æµ‹è‡³å…³é‡è¦ã€‚å°½ç®¡LLMsåœ¨åŒ»å­¦æ–‡æœ¬ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨åŸºäºEHRçš„é¢„æµ‹ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ï¼Œä¸»è¦ç”±äºé«˜ç»´æ•°æ®çš„æ—¶é—´ç»“æ„å»ºæ¨¡æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºæ··åˆèŒƒå¼ï¼ŒLLMsä»…ä½œä¸ºå›ºå®šçš„æ£€ç´¢å™¨ï¼Œè€Œä¸‹æ¸¸æ·±åº¦å­¦ä¹ æ¨¡å‹è´Ÿè´£é¢„æµ‹ï¼Œæœªèƒ½æå‡LLMsçš„å†…åœ¨æ¨ç†èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶EAG-RLï¼Œé€šè¿‡ä¸“å®¶æ³¨æ„åŠ›æŒ‡å¯¼æ¥å¢å¼ºLLMsçš„EHRæ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEAG-RLåœ¨ä¸¤ä¸ªçœŸå®EHRæ•°æ®é›†ä¸Šçš„è¡¨ç°æå‡äº†14.62%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”µå­å¥åº·è®°å½•æ¨ç†ä¸­çš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆæå‡LLMsçš„å†…åœ¨æ¨ç†èƒ½åŠ›ï¼Œä¸”åœ¨å¤„ç†é«˜ç»´æ—¶é—´ç»“æ„æ•°æ®æ—¶å­˜åœ¨å±€é™æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šEAG-RLæ¡†æ¶é€šè¿‡ä¸“å®¶æ¨¡å‹çš„æ³¨æ„åŠ›æŒ‡å¯¼ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–LLMsçš„æ¨ç†ç­–ç•¥ï¼Œä»è€Œæå‡å…¶åœ¨EHRæ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šEAG-RLåŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µä½¿ç”¨ä¸“å®¶æŒ‡å¯¼çš„è’™ç‰¹å¡ç½—æ ‘æœç´¢æ„å»ºé«˜è´¨é‡çš„é€æ­¥æ¨ç†è½¨è¿¹ï¼Œä»¥åˆå§‹åŒ–LLMsçš„ç­–ç•¥ï¼›ç¬¬äºŒé˜¶æ®µé€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥ä¼˜åŒ–ç­–ç•¥ï¼Œä½¿LLMsçš„æ³¨æ„åŠ›ä¸ä¸“å®¶æ¨¡å‹è¯†åˆ«çš„ä¸´åºŠç‰¹å¾å¯¹é½ã€‚

**å…³é”®åˆ›æ–°**ï¼šEAG-RLçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºç»“åˆä¸“å®¶æ¨¡å‹çš„æ³¨æ„åŠ›æŒ‡å¯¼ä¸å¼ºåŒ–å­¦ä¹ ï¼Œæ˜¾è‘—æå‡äº†LLMsåœ¨EHRæ¨ç†ä¸­çš„å†…åœ¨èƒ½åŠ›ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œé¿å…äº†å¯¹ä¸‹æ¸¸æ·±åº¦å­¦ä¹ æ¨¡å‹çš„ä¾èµ–ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨EAG-RLä¸­ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬ä¸“å®¶æ¨¡å‹çš„é€‰æ‹©å’Œè®­ç»ƒè¿‡ç¨‹ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ç”¨äºä¼˜åŒ–LLMsçš„æ³¨æ„åŠ›åˆ†å¸ƒï¼Œä»¥ç¡®ä¿å…¶å…³æ³¨ä¸´åºŠç›¸å…³ç‰¹å¾ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

EAG-RLåœ¨ä¸¤ä¸ªçœŸå®EHRæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLLMsçš„EHRæ¨ç†èƒ½åŠ›å¹³å‡æå‡äº†14.62%ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•å¢å¼ºäº†æ¨¡å‹å¯¹ç‰¹å¾æ‰°åŠ¨çš„é²æ£’æ€§ï¼Œå¹¶æé«˜äº†åœ¨æœªè§ä¸´åºŠé¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ä¸´åºŠå†³ç­–æ”¯æŒç³»ç»Ÿå’Œä¸ªæ€§åŒ–åŒ»ç–—ï¼Œèƒ½å¤Ÿå¸®åŠ©åŒ»ç”Ÿæ›´å‡†ç¡®åœ°è¿›è¡Œä¸´åºŠé¢„æµ‹å’Œè¯Šæ–­ã€‚æœªæ¥ï¼ŒEAG-RLæœ‰æœ›åœ¨æ›´å¹¿æ³›çš„åŒ»ç–—æ•°æ®åˆ†æå’Œæ™ºèƒ½å¥åº·ç®¡ç†ä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œæ¨åŠ¨åŒ»ç–—AIçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Improving large language models (LLMs) for electronic health record (EHR) reasoning is essential for enabling accurate and generalizable clinical predictions. While LLMs excel at medical text understanding, they underperform on EHR-based prediction tasks due to challenges in modeling temporally structured, high-dimensional data. Existing approaches often rely on hybrid paradigms, where LLMs serve merely as frozen prior retrievers while downstream deep learning (DL) models handle prediction, failing to improve the LLM's intrinsic reasoning capacity and inheriting the generalization limitations of DL models. To this end, we propose EAG-RL, a novel two-stage training framework designed to intrinsically enhance LLMs' EHR reasoning ability through expert attention guidance, where expert EHR models refer to task-specific DL models trained on EHR data. Concretely, EAG-RL first constructs high-quality, stepwise reasoning trajectories using expert-guided Monte Carlo Tree Search to effectively initialize the LLM's policy. Then, EAG-RL further optimizes the policy via reinforcement learning by aligning the LLM's attention with clinically salient features identified by expert EHR models. Extensive experiments on two real-world EHR datasets show that EAG-RL improves the intrinsic EHR reasoning ability of LLMs by an average of 14.62%, while also enhancing robustness to feature perturbations and generalization to unseen clinical domains. These results demonstrate the practical potential of EAG-RL for real-world deployment in clinical prediction tasks. Our code have been available at https://github.com/devilran6/EAG-RL.

