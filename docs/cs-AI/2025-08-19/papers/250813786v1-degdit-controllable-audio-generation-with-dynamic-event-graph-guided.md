---
layout: default
title: DegDiT: Controllable Audio Generation with Dynamic Event Graph Guided Diffusion Transformer
---

# DegDiT: Controllable Audio Generation with Dynamic Event Graph Guided Diffusion Transformer

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.13786" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.13786v1</a>
  <a href="https://arxiv.org/pdf/2508.13786.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.13786v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.13786v1', 'DegDiT: Controllable Audio Generation with Dynamic Event Graph Guided Diffusion Transformer')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yisu Liu, Chenxing Li, Wanqian Zhang, Wenfu Wang, Meng Yu, Ruibo Fu, Zheng Lin, Weiping Wang, Dong Yu

**åˆ†ç±»**: cs.SD, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-19

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDegDiTä»¥è§£å†³å¯æ§éŸ³é¢‘ç”Ÿæˆä¸­çš„æ—¶é—´å®šä½ä¸æ•ˆç‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¯æ§éŸ³é¢‘ç”Ÿæˆ` `åŠ¨æ€äº‹ä»¶å›¾` `å›¾å˜æ¢å™¨` `æ‰©æ•£æ¨¡å‹` `è´¨é‡å¹³è¡¡æ•°æ®é€‰æ‹©` `å…±è¯†åå¥½ä¼˜åŒ–` `å¤šæ¨¡æ€ç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¯æ§éŸ³é¢‘ç”Ÿæˆæ–¹æ³•åœ¨æ—¶é—´å®šä½ã€å¼€æ”¾è¯æ±‡æ‰©å±•æ€§å’Œæ•ˆç‡ä¹‹é—´å­˜åœ¨æƒè¡¡ï¼Œéš¾ä»¥æ»¡è¶³ç”¨æˆ·éœ€æ±‚ã€‚
2. DegDiTé€šè¿‡åŠ¨æ€äº‹ä»¶å›¾ç¼–ç äº‹ä»¶ï¼Œåˆ©ç”¨å›¾å˜æ¢å™¨ç”Ÿæˆä¸Šä¸‹æ–‡åŒ–çš„äº‹ä»¶åµŒå…¥ï¼ŒæŒ‡å¯¼æ‰©æ•£æ¨¡å‹è¿›è¡ŒéŸ³é¢‘ç”Ÿæˆã€‚
3. åœ¨AudioConditionã€DESEDå’ŒAudioTimeæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDegDiTåœ¨å¤šé¡¹è¯„ä¼°æŒ‡æ ‡ä¸Šå‡è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œè¡¨ç°å‡ºè‰²ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¯æ§æ–‡æœ¬åˆ°éŸ³é¢‘ç”Ÿæˆæ—¨åœ¨æ ¹æ®æ–‡æœ¬æè¿°åˆæˆéŸ³é¢‘ï¼ŒåŒæ—¶æ»¡è¶³ç”¨æˆ·æŒ‡å®šçš„çº¦æŸæ¡ä»¶ï¼ŒåŒ…æ‹¬äº‹ä»¶ç±»å‹ã€æ—¶é—´åºåˆ—ä»¥åŠèµ·æ­¢æ—¶é—´æˆ³ã€‚è¿™ä½¿å¾—ç”Ÿæˆçš„éŸ³é¢‘åœ¨å†…å®¹å’Œæ—¶é—´ç»“æ„ä¸Šéƒ½èƒ½å¾—åˆ°ç²¾ç¡®æ§åˆ¶ã€‚å°½ç®¡å·²æœ‰è¿›å±•ï¼Œç°æœ‰æ–¹æ³•åœ¨å‡†ç¡®çš„æ—¶é—´å®šä½ã€å¼€æ”¾è¯æ±‡çš„å¯æ‰©å±•æ€§å’Œå®é™…æ•ˆç‡ä¹‹é—´ä»é¢ä¸´å›ºæœ‰çš„æƒè¡¡ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†DegDiTï¼Œä¸€ä¸ªæ–°é¢–çš„åŠ¨æ€äº‹ä»¶å›¾å¼•å¯¼çš„æ‰©æ•£å˜æ¢å™¨æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°å¼€æ”¾è¯æ±‡çš„å¯æ§éŸ³é¢‘ç”Ÿæˆã€‚DegDiTå°†æè¿°ä¸­çš„äº‹ä»¶ç¼–ç ä¸ºç»“æ„åŒ–çš„åŠ¨æ€å›¾ï¼Œå›¾ä¸­çš„èŠ‚ç‚¹ä»£è¡¨è¯­ä¹‰ç‰¹å¾ã€æ—¶é—´å±æ€§å’Œäº‹ä»¶é—´è¿æ¥ã€‚é€šè¿‡å›¾å˜æ¢å™¨æ•´åˆè¿™äº›èŠ‚ç‚¹ï¼Œç”Ÿæˆä¸Šä¸‹æ–‡åŒ–çš„äº‹ä»¶åµŒå…¥ï¼Œä½œä¸ºæ‰©æ•£æ¨¡å‹çš„æŒ‡å¯¼ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†è´¨é‡å¹³è¡¡çš„æ•°æ®é€‰æ‹©ç®¡é“ï¼Œç»“åˆåˆ†å±‚äº‹ä»¶æ³¨é‡Šå’Œå¤šæ ‡å‡†è´¨é‡è¯„åˆ†ï¼Œå½¢æˆå…·æœ‰è¯­ä¹‰å¤šæ ·æ€§çš„ç²¾å¿ƒç­–åˆ’çš„æ•°æ®é›†ã€‚æ­¤å¤–ï¼Œæå‡ºçš„å…±è¯†åå¥½ä¼˜åŒ–æ–¹æ³•é€šè¿‡å¤šä¸ªå¥–åŠ±ä¿¡å·çš„å…±è¯†ä¿ƒè¿›éŸ³é¢‘ç”Ÿæˆã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDegDiTåœ¨å¤šä¸ªå®¢è§‚å’Œä¸»è§‚è¯„ä¼°æŒ‡æ ‡ä¸Šå‡å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¯æ§éŸ³é¢‘ç”Ÿæˆä¸­çš„æ—¶é—´å®šä½ä¸å‡†ç¡®ã€å¼€æ”¾è¯æ±‡æ‰©å±•æ€§ä¸è¶³å’Œç”Ÿæˆæ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€åœ¨è¿™äº›æ–¹é¢å­˜åœ¨å›ºæœ‰çš„æƒè¡¡ï¼Œå¯¼è‡´ç”ŸæˆéŸ³é¢‘çš„è´¨é‡å’Œå¤šæ ·æ€§å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDegDiTçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†äº‹ä»¶æè¿°ç¼–ç ä¸ºåŠ¨æ€å›¾ç»“æ„ï¼Œé€šè¿‡å›¾å˜æ¢å™¨æ•´åˆè¯­ä¹‰ç‰¹å¾ã€æ—¶é—´å±æ€§å’Œäº‹ä»¶é—´è¿æ¥ï¼Œç”Ÿæˆä¸Šä¸‹æ–‡åŒ–çš„äº‹ä»¶åµŒå…¥ï¼Œä»è€Œä¸ºæ‰©æ•£æ¨¡å‹æä¾›æœ‰æ•ˆæŒ‡å¯¼ã€‚è¿™æ ·çš„è®¾è®¡ä½¿å¾—ç”Ÿæˆçš„éŸ³é¢‘åœ¨å†…å®¹å’Œæ—¶é—´ç»“æ„ä¸Šéƒ½èƒ½å¾—åˆ°ç²¾ç¡®æ§åˆ¶ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDegDiTçš„æ•´ä½“æ¶æ„åŒ…æ‹¬åŠ¨æ€äº‹ä»¶å›¾æ„å»ºã€å›¾å˜æ¢å™¨æ¨¡å—å’Œæ‰©æ•£æ¨¡å‹ã€‚é¦–å…ˆï¼Œå°†æ–‡æœ¬æè¿°ä¸­çš„äº‹ä»¶è½¬åŒ–ä¸ºåŠ¨æ€å›¾ç»“æ„ï¼›ç„¶åï¼Œåˆ©ç”¨å›¾å˜æ¢å™¨ç”Ÿæˆäº‹ä»¶åµŒå…¥ï¼›æœ€åï¼Œæ‰©æ•£æ¨¡å‹æ ¹æ®è¿™äº›åµŒå…¥ç”ŸæˆéŸ³é¢‘ã€‚

**å…³é”®åˆ›æ–°**ï¼šDegDiTçš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥åŠ¨æ€äº‹ä»¶å›¾å’Œå›¾å˜æ¢å™¨çš„ç»“åˆï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ•´åˆå¤šç»´åº¦ä¿¡æ¯ï¼Œæå‡ç”ŸæˆéŸ³é¢‘çš„è´¨é‡å’Œå¤šæ ·æ€§ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„åŸºäºå›ºå®šç‰¹å¾çš„ç”Ÿæˆæ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å…³é”®è®¾è®¡ä¸Šï¼ŒDegDiTé‡‡ç”¨äº†åˆ†å±‚äº‹ä»¶æ³¨é‡Šå’Œå¤šæ ‡å‡†è´¨é‡è¯„åˆ†çš„è´¨é‡å¹³è¡¡æ•°æ®é€‰æ‹©ç®¡é“ï¼Œç¡®ä¿è®­ç»ƒæ•°æ®çš„å¤šæ ·æ€§å’Œé«˜è´¨é‡ã€‚æ­¤å¤–ï¼Œé‡‡ç”¨å…±è¯†åå¥½ä¼˜åŒ–æ–¹æ³•ï¼Œé€šè¿‡å¤šä¸ªå¥–åŠ±ä¿¡å·çš„å…±è¯†æ¥æŒ‡å¯¼ç”Ÿæˆè¿‡ç¨‹ï¼Œè¿›ä¸€æ­¥æå‡ç”Ÿæˆæ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨AudioConditionã€DESEDå’ŒAudioTimeæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDegDiTåœ¨å¤šé¡¹å®¢è§‚å’Œä¸»è§‚è¯„ä¼°æŒ‡æ ‡ä¸Šå‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå…·ä½“æå‡å¹…åº¦è¶…è¿‡äº†ç°æœ‰æ–¹æ³•çš„10%ï¼Œå±•ç°å‡ºæ˜¾è‘—çš„æ•ˆæœä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DegDiTåœ¨å¯æ§éŸ³é¢‘ç”Ÿæˆé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œèƒ½å¤Ÿç”¨äºå½±è§†é…éŸ³ã€æ¸¸æˆéŸ³æ•ˆè®¾è®¡ä»¥åŠè™šæ‹ŸåŠ©æ‰‹ç­‰åœºæ™¯ã€‚å…¶ç²¾ç¡®çš„æ—¶é—´æ§åˆ¶å’Œå¼€æ”¾è¯æ±‡èƒ½åŠ›ä½¿å¾—ç”¨æˆ·èƒ½å¤Ÿæ ¹æ®å…·ä½“éœ€æ±‚ç”Ÿæˆé«˜è´¨é‡çš„éŸ³é¢‘å†…å®¹ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Controllable text-to-audio generation aims to synthesize audio from textual descriptions while satisfying user-specified constraints, including event types, temporal sequences, and onset and offset timestamps. This enables precise control over both the content and temporal structure of the generated audio. Despite recent progress, existing methods still face inherent trade-offs among accurate temporal localization, open-vocabulary scalability, and practical efficiency. To address these challenges, we propose DegDiT, a novel dynamic event graph-guided diffusion transformer framework for open-vocabulary controllable audio generation. DegDiT encodes the events in the description as structured dynamic graphs. The nodes in each graph are designed to represent three aspects: semantic features, temporal attributes, and inter-event connections. A graph transformer is employed to integrate these nodes and produce contextualized event embeddings that serve as guidance for the diffusion model. To ensure high-quality and diverse training data, we introduce a quality-balanced data selection pipeline that combines hierarchical event annotation with multi-criteria quality scoring, resulting in a curated dataset with semantic diversity. Furthermore, we present consensus preference optimization, facilitating audio generation through consensus among multiple reward signals. Extensive experiments on AudioCondition, DESED, and AudioTime datasets demonstrate that DegDiT achieves state-of-the-art performances across a variety of objective and subjective evaluation metrics.

