---
layout: default
title: Equinox: Holistic Fair Scheduling in Serving Large Language Models
---

# Equinox: Holistic Fair Scheduling in Serving Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.16646" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.16646v1</a>
  <a href="https://arxiv.org/pdf/2508.16646.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.16646v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.16646v1', 'Equinox: Holistic Fair Scheduling in Serving Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhixiang Wei, James Yen, Jingyi Chen, Ziyang Zhang, Zhibai Huang, Chen Chen, Xingzi Yu, Yicheng Gu, Chenggang Wu, Yun Wang, Mingyuan Xia, Jie Wu, Hao Wang, Zhengwei Qi

**åˆ†ç±»**: cs.DC, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-19

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEquinoxä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹æœåŠ¡ä¸­çš„å…¬å¹³è°ƒåº¦é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `å…¬å¹³è°ƒåº¦` `èµ„æºåˆ©ç”¨` `æ··åˆé¢„æµ‹ä¸“å®¶` `è‡ªé€‚åº”æ‰¹å¤„ç†` `GPUåˆ©ç”¨ç‡` `æ€§èƒ½ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å½“å‰å¤§è¯­è¨€æ¨¡å‹æœåŠ¡é¢ä¸´è°ƒåº¦æ‚–è®ºï¼Œéš¾ä»¥åŒæ—¶æ»¡è¶³ç”¨æˆ·å’Œè¿è¥è€…çš„å…¬å¹³æ€§éœ€æ±‚ã€‚
2. æå‡ºåŒè®¡æ•°å™¨æ¡†æ¶å’Œæ··åˆé¢„æµ‹ä¸“å®¶ï¼ˆMoPEï¼‰æ–¹æ³•ï¼Œèƒ½å¤Ÿé¢„æµ‹å…³é”®æ€§èƒ½æŒ‡æ ‡ä»¥å®ç°å…¬å¹³è°ƒåº¦ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒEquinoxåœ¨ååé‡ã€å»¶è¿Ÿå’Œå…¬å¹³æ€§æ–¹é¢å‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸”GPUåˆ©ç”¨ç‡ä¿æŒé«˜æ•ˆã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡é’ˆå¯¹å½“å‰å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœåŠ¡çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§åŒè®¡æ•°å™¨æ¡†æ¶ï¼Œåˆ†åˆ«ä»ç”¨æˆ·å’Œè¿è¥è€…çš„è§’åº¦è¿›è¡Œåˆ†æã€‚ç”¨æˆ·å…¬å¹³è®¡æ•°å™¨é€šè¿‡åŠ æƒä»¤ç‰Œå’Œå»¶è¿Ÿæ¥è¡¡é‡æœåŠ¡è´¨é‡ï¼Œè€Œèµ„æºå…¬å¹³è®¡æ•°å™¨åˆ™é€šè¿‡ååé‡å’ŒGPUåˆ©ç”¨ç‡æ¥è¯„ä¼°è¿è¥æ•ˆç‡ã€‚ç”±äºè¿™äº›æŒ‡æ ‡ä»…åœ¨æ‰§è¡Œåå¯ç”¨ï¼Œå¯¼è‡´è°ƒåº¦æ‚–è®ºï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ç§ç¡®å®šæ€§çš„æ··åˆé¢„æµ‹ä¸“å®¶ï¼ˆMoPEï¼‰æ¡†æ¶ï¼Œä»¥é¢„æµ‹ç”¨æˆ·æ„ŸçŸ¥çš„å»¶è¿Ÿã€è¾“å‡ºä»¤ç‰Œã€ååé‡å’ŒGPUåˆ©ç”¨ç‡ã€‚è¿™äº›é¢„æµ‹ä½¿å¾—èƒ½å¤Ÿè®¡ç®—ç»Ÿä¸€çš„æ•´ä½“å…¬å¹³æ€§è¯„åˆ†ï¼Œä»è€Œé€šè¿‡å¯è°ƒå‚æ•°å®ç°ä¸»åŠ¨çš„å…¬å¹³æ„ŸçŸ¥è°ƒåº¦ã€‚æˆ‘ä»¬åœ¨Equinoxä¸­å®ç°äº†è¿™ä¸€æ¡†æ¶ï¼Œå¹¶è¿›è¡Œäº†è‡ªé€‚åº”æ‰¹å¤„ç†å’Œæ— åœé¡¿è°ƒåº¦ç­‰ä¼˜åŒ–ã€‚å¯¹ç”Ÿäº§è½¨è¿¹ï¼ˆShareGPTã€LMSYSï¼‰å’Œåˆæˆå·¥ä½œè´Ÿè½½çš„è¯„ä¼°è¡¨æ˜ï¼ŒEquinoxåœ¨ååé‡ä¸Šæé«˜äº†1.3å€ï¼Œé¦–æ¬¡ä»¤ç‰Œå»¶è¿Ÿé™ä½äº†60%ï¼Œå…¬å¹³æ€§æé«˜äº†13%ï¼ŒåŒæ—¶ä¿æŒ94%çš„GPUåˆ©ç”¨ç‡ï¼Œè¯æ˜äº†åœ¨å¼‚æ„å¹³å°ä¸Šå…¬å¹³æ€§åœ¨æœ‰é™å·®å¼‚ä¸‹çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å½“å‰å¤§è¯­è¨€æ¨¡å‹æœåŠ¡ä¸­çš„å…¬å¹³è°ƒåº¦é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨ç”¨æˆ·ä½“éªŒå’Œèµ„æºåˆ©ç”¨ä¹‹é—´å­˜åœ¨çŸ›ç›¾ï¼Œå¯¼è‡´è°ƒåº¦æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥åŒè®¡æ•°å™¨æ¡†æ¶ï¼Œåˆ†åˆ«ä»ç”¨æˆ·å’Œè¿è¥è€…çš„è§’åº¦è¡¡é‡å…¬å¹³æ€§ï¼Œå¹¶åˆ©ç”¨æ··åˆé¢„æµ‹ä¸“å®¶ï¼ˆMoPEï¼‰æ¡†æ¶é¢„æµ‹å…³é”®æ€§èƒ½æŒ‡æ ‡ï¼Œä»¥å®ç°ä¸»åŠ¨è°ƒåº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ç”¨æˆ·å…¬å¹³è®¡æ•°å™¨å’Œèµ„æºå…¬å¹³è®¡æ•°å™¨ï¼Œç»“åˆMoPEæ¡†æ¶è¿›è¡Œæ€§èƒ½é¢„æµ‹ï¼Œæœ€ç»ˆè®¡ç®—ç»Ÿä¸€çš„æ•´ä½“å…¬å¹³æ€§è¯„åˆ†ï¼Œæ”¯æŒè°ƒåº¦å†³ç­–ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†åŒè®¡æ•°å™¨æ¡†æ¶å’ŒMoPEé¢„æµ‹æœºåˆ¶ï¼Œä½¿å¾—è°ƒåº¦è¿‡ç¨‹èƒ½å¤Ÿåœ¨æ‰§è¡Œå‰è¿›è¡Œå…¬å¹³æ€§è¯„ä¼°ï¼Œé¿å…äº†ä¼ ç»Ÿæ–¹æ³•çš„è°ƒåº¦æ‚–è®ºã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œè®¾ç½®äº†å¯è°ƒå‚æ•°ä»¥å¹³è¡¡ç”¨æˆ·å’Œèµ„æºå…¬å¹³æ€§ï¼Œé‡‡ç”¨äº†è‡ªé€‚åº”æ‰¹å¤„ç†å’Œæ— åœé¡¿è°ƒåº¦ç­–ç•¥ï¼Œç¡®ä¿ç³»ç»Ÿåœ¨é«˜è´Ÿè½½ä¸‹ä»èƒ½ä¿æŒé«˜æ•ˆè¿è¡Œã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒEquinoxåœ¨ååé‡ä¸Šæé«˜äº†1.3å€ï¼Œé¦–æ¬¡ä»¤ç‰Œå»¶è¿Ÿé™ä½äº†60%ï¼Œå…¬å¹³æ€§æé«˜äº†13%ï¼ŒåŒæ—¶ä¿æŒ94%çš„GPUåˆ©ç”¨ç‡ï¼Œæ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•VTCï¼Œè¯æ˜äº†å…¶åœ¨å¼‚æ„å¹³å°ä¸Šçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤§è¯­è¨€æ¨¡å‹çš„åœ¨çº¿æœåŠ¡ã€äº‘è®¡ç®—å¹³å°çš„èµ„æºè°ƒåº¦ä»¥åŠå¤šç”¨æˆ·ç¯å¢ƒä¸‹çš„å…¬å¹³æ€§ä¿éšœã€‚é€šè¿‡å®ç°å…¬å¹³è°ƒåº¦ï¼Œèƒ½å¤Ÿæå‡ç”¨æˆ·ä½“éªŒï¼Œä¼˜åŒ–èµ„æºåˆ©ç”¨ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We address the limitations of current LLM serving with a dual-counter framework separating user and operator perspectives. The User Fairness Counter measures quality of service via weighted tokens and latency; the Resource Fairness Counter measures operational efficiency through throughput and GPU utilization. Since these metrics are only available post-execution, creating a scheduling paradox, we introduce a deterministic Mixture of Prediction Experts (MoPE) framework to predict user-perceived latency, output tokens, throughput, and GPU utilization. These predictions enable calculation of a unified Holistic Fairness score that balances both counters through tunable parameters for proactive fairness-aware scheduling. We implement this in Equinox, an open-source system with other optimizations like adaptive batching, and stall-free scheduling. Evaluations on production traces (ShareGPT, LMSYS) and synthetic workloads demonstrate Equinox achieves up to $1.3\times$ higher throughput, 60\% lower time-to-first-token latency, and 13\% higher fairness versus VTC while maintaining 94\% GPU utilization, proving fairness under bounded discrepancy across heterogeneous platforms.

