---
layout: default
title: Breaking the SFT Plateau: Multimodal Structured Reinforcement Learning for Chart-to-Code Generation
---

# Breaking the SFT Plateau: Multimodal Structured Reinforcement Learning for Chart-to-Code Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.13587" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.13587v1</a>
  <a href="https://arxiv.org/pdf/2508.13587.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.13587v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.13587v1', 'Breaking the SFT Plateau: Multimodal Structured Reinforcement Learning for Chart-to-Code Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Lei Chen, Xuanle Zhao, Zhixiong Zeng, Jing Huang, Liming Zheng, Yufeng Zhong, Lin Ma

**åˆ†ç±»**: cs.AI, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-19

**å¤‡æ³¨**: technical report

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šæ¨¡æ€ç»“æ„åŒ–å¼ºåŒ–å­¦ä¹ ä»¥è§£å†³å›¾è¡¨åˆ°ä»£ç ç”Ÿæˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å›¾è¡¨åˆ°ä»£ç ç”Ÿæˆ` `å¤šæ¨¡æ€å­¦ä¹ ` `å¼ºåŒ–å­¦ä¹ ` `ç»“æ„åŒ–å¥–åŠ±` `è‡ªåŠ¨åŒ–ç¼–ç¨‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç›‘ç£å¾®è°ƒæ–¹æ³•åœ¨å›¾è¡¨åˆ°ä»£ç ç”Ÿæˆä»»åŠ¡ä¸­é¢ä¸´æ€§èƒ½ç“¶é¢ˆï¼Œéš¾ä»¥æ»¡è¶³å¤æ‚æ¨ç†çš„éœ€æ±‚ã€‚
2. è®ºæ–‡æå‡ºçš„å¤šæ¨¡æ€ç»“æ„åŒ–å¼ºåŒ–å­¦ä¹ ï¼ˆMSRLï¼‰æ–¹æ³•ï¼Œé€šè¿‡å¤šå±‚æ¬¡ç»“æ„åŒ–å¥–åŠ±ç³»ç»Ÿæ¥ä¼˜åŒ–ç”Ÿæˆçš„ä»£ç è´¨é‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMSRLåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œçªç ´äº†ä¼ ç»ŸSFTæ–¹æ³•çš„å±€é™æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡å¼ºåŒ–å­¦ä¹ åœ¨è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„æ¨ç†èƒ½åŠ›å·²å¾—åˆ°éªŒè¯ï¼Œä½†åœ¨éœ€è¦æ·±å…¥ç†è§£ä¿¡æ¯ä¸°å¯Œå›¾åƒå’Œç”Ÿæˆç»“æ„åŒ–è¾“å‡ºçš„ä»»åŠ¡ä¸­ï¼Œå…¶åº”ç”¨ä»ç„¶ä¸è¶³ã€‚å›¾è¡¨åˆ°ä»£ç ç”Ÿæˆæ­£æ˜¯è¿™ä¸€æŒ‘æˆ˜çš„å…¸å‹ä¾‹å­ï¼Œè¦æ±‚å¯¹è§†è§‰å›¾è¡¨è¿›è¡Œå¤æ‚æ¨ç†ä»¥ç”Ÿæˆç»“æ„åŒ–ä»£ç ã€‚ä»…ä¾èµ–ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å¾€å¾€ä¸å¤Ÿï¼Œçªæ˜¾å‡ºæœ‰æ•ˆçš„å¼ºåŒ–å­¦ä¹ ç­–ç•¥çš„å¿…è¦æ€§ã€‚æœ¬æ–‡æå‡ºçš„å¤šæ¨¡æ€ç»“æ„åŒ–å¼ºåŒ–å­¦ä¹ ï¼ˆMSRLï¼‰æ–¹æ³•ï¼Œé€šè¿‡æ„å»ºåŒ…å«300ä¸‡ä¸ªçœŸå®ä¸–ç•ŒarXivè¡¨æ ¼çš„å›¾è¡¨-ä»£ç å¯¹çš„è®­ç»ƒè¯­æ–™åº“ï¼Œæ˜¾è‘—çªç ´äº†SFTçš„æ€§èƒ½ç“¶é¢ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMSRLåœ¨ChartMimicå’ŒReachQAåŸºå‡†ä¸Šåˆ†åˆ«æå‡äº†6.2%å’Œ9.9%çš„é«˜å±‚æ¬¡æŒ‡æ ‡ï¼Œè¾¾åˆ°äº†ä¸å…ˆè¿›é—­æºæ¨¡å‹ç«äº‰çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å›¾è¡¨åˆ°ä»£ç ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œç°æœ‰ç›‘ç£å¾®è°ƒæ–¹æ³•ï¼ˆSFTï¼‰æ‰€é¢ä¸´çš„æ€§èƒ½ç“¶é¢ˆå’Œå¤æ‚æ¨ç†èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†ä¿¡æ¯ä¸°å¯Œçš„å›¾åƒæ—¶ï¼Œå¾€å¾€æ— æ³•æœ‰æ•ˆç”Ÿæˆç»“æ„åŒ–è¾“å‡ºã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„å¤šæ¨¡æ€ç»“æ„åŒ–å¼ºåŒ–å­¦ä¹ ï¼ˆMSRLï¼‰æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥å¤šå±‚æ¬¡çš„ç»“æ„åŒ–å¥–åŠ±æœºåˆ¶ï¼Œç»“åˆæ–‡æœ¬å’Œè§†è§‰åé¦ˆï¼Œæ¥æå‡ç”Ÿæˆä»£ç çš„è´¨é‡å’Œå‡†ç¡®æ€§ã€‚è¿™æ ·çš„è®¾è®¡æ—¨åœ¨æ›´å¥½åœ°è¯„ä¼°å’Œå¥–åŠ±ç”Ÿæˆä»£ç çš„ç»†èŠ‚å’Œç»“æ„ç›¸ä¼¼æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMSRLæ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆæ˜¯åŸºäºæ–‡æœ¬çš„è§„åˆ™å¥–åŠ±ç³»ç»Ÿï¼Œç”¨äºéªŒè¯ä»£ç çš„ç»†ç²’åº¦ç»†èŠ‚ï¼›å…¶æ¬¡æ˜¯åŸºäºè§†è§‰çš„æ¨¡å‹å¥–åŠ±ç³»ç»Ÿï¼Œé€šè¿‡å°†ç”Ÿæˆçš„ä»£ç æ¸²æŸ“ä¸ºå›¾åƒå¹¶ä½¿ç”¨è¯„ä¼°æ¨¡å‹æ¥è¯„ä¼°ç»“æ„ç›¸ä¼¼æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥äº†å¤šå±‚æ¬¡çš„ç»“æ„åŒ–å¥–åŠ±ç³»ç»Ÿï¼Œç»“åˆæ–‡æœ¬å’Œè§†è§‰ä¿¡æ¯è¿›è¡Œè¯„ä¼°ï¼Œè¿™ä¸ä¼ ç»Ÿçš„å•ä¸€å¥–åŠ±æœºåˆ¶æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å…³é”®è®¾è®¡ä¸Šï¼Œè®ºæ–‡æ„å»ºäº†ä¸€ä¸ªåŒ…å«300ä¸‡ä¸ªå›¾è¡¨-ä»£ç å¯¹çš„å¤§è§„æ¨¡è®­ç»ƒè¯­æ–™åº“ï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é‡‡ç”¨äº†ä¸¤é˜¶æ®µçš„è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œä»¥ç¡®ä¿è®­ç»ƒçš„ç¨³å®šæ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMSRLæ–¹æ³•åœ¨ChartMimicå’ŒReachQAåŸºå‡†æµ‹è¯•ä¸­åˆ†åˆ«æå‡äº†6.2%å’Œ9.9%çš„é«˜å±‚æ¬¡æŒ‡æ ‡ï¼Œæ˜¾è‘—çªç ´äº†ä¼ ç»ŸSFTæ–¹æ³•çš„æ€§èƒ½ç“¶é¢ˆï¼Œè¾¾åˆ°äº†ä¸å…ˆè¿›é—­æºæ¨¡å‹çš„ç«äº‰æ°´å¹³ã€‚è¿™ä¸€æˆæœå±•ç¤ºäº†å¤šæ¨¡æ€ç»“æ„åŒ–å¥–åŠ±åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨åŒ–ç¼–ç¨‹ã€æ•°æ®å¯è§†åŒ–å·¥å…·ä»¥åŠæ™ºèƒ½åŠ©ç†ç­‰ã€‚é€šè¿‡æå‡å›¾è¡¨åˆ°ä»£ç ç”Ÿæˆçš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼ŒMSRLæ–¹æ³•èƒ½å¤Ÿä¸ºå¼€å‘è€…æä¾›æ›´å¼ºå¤§çš„å·¥å…·ï¼Œä¿ƒè¿›æ•°æ®åˆ†æå’Œå†³ç­–æ”¯æŒçš„è‡ªåŠ¨åŒ–ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½åœ¨æ›´å¹¿æ³›çš„å¤šæ¨¡æ€å­¦ä¹ ä»»åŠ¡ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> While reinforcement learning (RL) has proven highly effective for general reasoning in vision-language models, its application to tasks requiring in-depth understanding of information-rich images and generation of structured outputs remains underexplored. Chart-to-code generation exemplifies this challenge, demanding complex reasoning over visual charts to generate structured code. Supervised fine-tuning (SFT) alone is often insufficient, highlighting the need for effective RL strategies that appropriately reward structured outputs. We systematically investigate the performance plateau in SFT through large-scale experiments and propose Multimodal Structured Reinforcement Learning (MSRL) for chart-to-code generation, which substantially breaks through this plateau. We construct the largest training corpus to date, containing 3 million chart-code pairs from real-world arXiv tables to mitigate simplistic patterns of prior synthetic data. Despite reaching state-of-the-art performance, our experiments show that scaling SFT data eventually hits a plateau where further increases yield negligible improvements. Our MSRL method leverages a multi-granularity structured reward system using multimodal textual and visual feedback. At the textual level, rule-based rewards validate fine-grained code details. At the visual level, model-based rewards assess structural similarity by rendering generated code into images and employing an evaluator model. We implement this within a two-stage curriculum for training stability. Results demonstrate that MSRL significantly breaks the SFT plateau, improving high-level metrics by 6.2% and 9.9% on ChartMimic and ReachQA benchmarks respectively, achieving competitive performance with advanced closed-source models.

