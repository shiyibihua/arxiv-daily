---
layout: default
title: OccVLA: Vision-Language-Action Model with Implicit 3D Occupancy Supervision
---

# OccVLA: Vision-Language-Action Model with Implicit 3D Occupancy Supervision

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.05578" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.05578v1</a>
  <a href="https://arxiv.org/pdf/2509.05578.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.05578v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.05578v1', 'OccVLA: Vision-Language-Action Model with Implicit 3D Occupancy Supervision')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ruixun Liu, Lingyu Kong, Derun Li, Hang Zhao

**åˆ†ç±»**: cs.AI, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-06

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**OccVLAï¼šæå‡ºåŸºäºéšå¼3D Occupancyç›‘ç£çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œæå‡è‡ªåŠ¨é©¾é©¶åœºæ™¯ç†è§£ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€æ¨¡å‹` `3D Occupancy` `è‡ªåŠ¨é©¾é©¶` `å¤šæ¨¡æ€å­¦ä¹ ` `ç©ºé—´æ¨ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ç¼ºä¹é²æ£’çš„3Dç©ºé—´ç†è§£ï¼Œéš¾ä»¥æ„å»ºæœ‰æ•ˆçš„3Dè¡¨ç¤ºï¼Œä¸”VLMsä¸¢å¤±äº†ç»†ç²’åº¦ç©ºé—´ç»†èŠ‚ã€‚
2. OccVLAå°†3D occupancyä½œä¸ºé¢„æµ‹è¾“å‡ºå’Œç›‘ç£ä¿¡å·ï¼Œç›´æ¥ä»2Dè§†è§‰è¾“å…¥ä¸­å­¦ä¹ ç»†ç²’åº¦ç©ºé—´ç»“æ„ã€‚
3. OccVLAåœ¨nuScenesè½¨è¿¹è§„åˆ’å’Œ3Dè§†è§‰é—®ç­”ä»»åŠ¡ä¸Šå–å¾—äº†SOTAç»“æœï¼Œä¸”æ¨ç†é˜¶æ®µæ— é¢å¤–è®¡ç®—å¼€é”€ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹(MLLM)åœ¨è§†è§‰-è¯­è¨€æ¨ç†æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†ä»ç„¶ç¼ºä¹é²æ£’çš„3Dç©ºé—´ç†è§£ï¼Œè¿™å¯¹äºè‡ªåŠ¨é©¾é©¶è‡³å…³é‡è¦ã€‚è¿™ç§å±€é™æ€§æºäºä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼š(1)åœ¨æ²¡æœ‰æ˜‚è´µçš„äººå·¥æ ‡æ³¨çš„æƒ…å†µä¸‹ï¼Œéš¾ä»¥æ„å»ºå¯è®¿é—®ä½†æœ‰æ•ˆçš„3Dè¡¨ç¤ºï¼›(2)ç”±äºç¼ºä¹å¤§è§„æ¨¡çš„3Dè§†è§‰-è¯­è¨€é¢„è®­ç»ƒï¼ŒVLMsä¸­ç»†ç²’åº¦ç©ºé—´ç»†èŠ‚çš„ä¸¢å¤±ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†OccVLAï¼Œè¿™æ˜¯ä¸€ç§å°†3D occupancyè¡¨ç¤ºé›†æˆåˆ°ç»Ÿä¸€çš„å¤šæ¨¡æ€æ¨ç†è¿‡ç¨‹ä¸­çš„æ–°æ¡†æ¶ã€‚ä¸ä¾èµ–æ˜¾å¼3Dè¾“å…¥çš„æ–¹æ³•ä¸åŒï¼ŒOccVLAå°†å¯†é›†çš„3D occupancyè§†ä¸ºé¢„æµ‹è¾“å‡ºå’Œç›‘ç£ä¿¡å·ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿç›´æ¥ä»2Dè§†è§‰è¾“å…¥ä¸­å­¦ä¹ ç»†ç²’åº¦çš„ç©ºé—´ç»“æ„ã€‚Occupancyé¢„æµ‹è¢«è§†ä¸ºéšå¼æ¨ç†è¿‡ç¨‹ï¼Œå¯ä»¥åœ¨æ¨ç†æœŸé—´è·³è¿‡ï¼Œè€Œä¸ä¼šé™ä½æ€§èƒ½ï¼Œä»è€Œä¸ä¼šå¢åŠ é¢å¤–çš„è®¡ç®—å¼€é”€ã€‚OccVLAåœ¨nuScenesåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†è½¨è¿¹è§„åˆ’çš„æœ€å…ˆè¿›ç»“æœï¼Œå¹¶åœ¨3Dè§†è§‰é—®ç­”ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä¸ºè‡ªåŠ¨é©¾é©¶æä¾›äº†ä¸€ç§å¯æ‰©å±•ã€å¯è§£é‡Šä¸”å®Œå…¨åŸºäºè§†è§‰çš„è§£å†³æ–¹æ¡ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºè§†è§‰-è¯­è¨€çš„è‡ªåŠ¨é©¾é©¶æ¨¡å‹ç¼ºä¹å¯¹3Dç©ºé—´çš„ç²¾ç¡®ç†è§£ï¼Œå°¤å…¶æ˜¯åœ¨æ²¡æœ‰å¤§é‡3Dæ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹ã€‚ä¼ ç»Ÿæ–¹æ³•è¦ä¹ˆä¾èµ–äºæ˜¾å¼çš„3Dè¾“å…¥ï¼ˆå¦‚ç‚¹äº‘ï¼‰ï¼Œå¢åŠ äº†è®¡ç®—è´Ÿæ‹…ï¼Œè¦ä¹ˆåœ¨è§†è§‰-è¯­è¨€æ¨¡å‹çš„é¢„è®­ç»ƒä¸­å¿½ç•¥äº†3Dä¿¡æ¯çš„æœ‰æ•ˆåˆ©ç”¨ï¼Œå¯¼è‡´æ¨¡å‹åœ¨å¤„ç†ç©ºé—´å…³ç³»æ—¶èƒ½åŠ›ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šOccVLAçš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†3D occupancyä½œä¸ºä¸€ç§éšå¼çš„æ¨ç†è¿‡ç¨‹ã€‚æ¨¡å‹é€šè¿‡é¢„æµ‹3D occupancyæ¥å­¦ä¹ åœºæ™¯çš„å‡ ä½•ç»“æ„å’Œç©ºé—´å…³ç³»ï¼Œè€Œæ— éœ€åœ¨æ¨ç†æ—¶æ˜¾å¼åœ°ä½¿ç”¨3Dæ•°æ®ã€‚è¿™ç§æ–¹æ³•åˆ©ç”¨2Dè§†è§‰è¾“å…¥æ¥ç›‘ç£3D occupancyçš„é¢„æµ‹ï¼Œä»è€Œåœ¨è§†è§‰-è¯­è¨€æ¨¡å‹ä¸­æ³¨å…¥3Dç©ºé—´ç†è§£èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šOccVLAæ¡†æ¶åŒ…å«ä¸€ä¸ªè§†è§‰ç¼–ç å™¨ã€ä¸€ä¸ªè¯­è¨€æ¨¡å‹å’Œä¸€ä¸ª3D occupancyé¢„æµ‹æ¨¡å—ã€‚è§†è§‰ç¼–ç å™¨è´Ÿè´£æå–2Då›¾åƒç‰¹å¾ï¼Œè¯­è¨€æ¨¡å‹è´Ÿè´£å¤„ç†æ–‡æœ¬è¾“å…¥å¹¶è¿›è¡Œæ¨ç†ï¼Œ3D occupancyé¢„æµ‹æ¨¡å—åˆ™æ ¹æ®è§†è§‰ç‰¹å¾é¢„æµ‹åœºæ™¯çš„3D occupancyã€‚åœ¨è®­ç»ƒé˜¶æ®µï¼Œæ¨¡å‹åŒæ—¶å­¦ä¹ è§†è§‰-è¯­è¨€ä»»åŠ¡å’Œ3D occupancyé¢„æµ‹ä»»åŠ¡ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œå¯ä»¥è·³è¿‡3D occupancyé¢„æµ‹æ¨¡å—ï¼Œç›´æ¥ä½¿ç”¨è§†è§‰å’Œè¯­è¨€ç‰¹å¾è¿›è¡Œæ¨ç†ã€‚

**å…³é”®åˆ›æ–°**ï¼šOccVLAçš„å…³é”®åˆ›æ–°åœ¨äºå°†3D occupancyä½œä¸ºä¸€ç§éšå¼çš„ç›‘ç£ä¿¡å·ï¼Œä»è€Œé¿å…äº†å¯¹æ˜¾å¼3Dæ•°æ®çš„ä¾èµ–ã€‚è¿™ç§æ–¹æ³•ä¸ä»…é™ä½äº†è®¡ç®—æˆæœ¬ï¼Œè¿˜ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿä»2Dè§†è§‰è¾“å…¥ä¸­å­¦ä¹ åˆ°ç»†ç²’åº¦çš„3Dç©ºé—´ä¿¡æ¯ã€‚æ­¤å¤–ï¼ŒOccVLAæ¡†æ¶å¯ä»¥çµæ´»åœ°é›†æˆåˆ°ç°æœ‰çš„è§†è§‰-è¯­è¨€æ¨¡å‹ä¸­ï¼Œæé«˜äº†æ¨¡å‹çš„é€šç”¨æ€§å’Œå¯æ‰©å±•æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šOccVLAä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°æ¥ç›‘ç£3D occupancyçš„é¢„æµ‹ã€‚ä¸ºäº†æé«˜é¢„æµ‹çš„å‡†ç¡®æ€§ï¼Œé‡‡ç”¨äº†å¤šå°ºåº¦ç‰¹å¾èåˆçš„æ–¹æ³•ï¼Œå°†ä¸åŒå°ºåº¦çš„è§†è§‰ç‰¹å¾èåˆåœ¨ä¸€èµ·ã€‚åœ¨ç½‘ç»œç»“æ„æ–¹é¢ï¼Œä½¿ç”¨äº†Transformerç»“æ„æ¥å»ºæ¨¡è§†è§‰ç‰¹å¾å’Œè¯­è¨€ç‰¹å¾ä¹‹é—´çš„å…³ç³»ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†è¯´æ˜ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

OccVLAåœ¨nuScenesåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†è½¨è¿¹è§„åˆ’çš„æœ€å…ˆè¿›ç»“æœï¼Œè¡¨æ˜å…¶åœ¨è‡ªåŠ¨é©¾é©¶åœºæ™¯ç†è§£æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼ŒOccVLAåœ¨3Dè§†è§‰é—®ç­”ä»»åŠ¡ä¸­ä¹Ÿè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶åœ¨3Dç©ºé—´æ¨ç†æ–¹é¢çš„å¼ºå¤§èƒ½åŠ›ã€‚æ‘˜è¦ä¸­æœªæä¾›å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œæå‡å¹…åº¦ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

OccVLAå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯åº”ç”¨äºè‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªã€è™šæ‹Ÿç°å®ç­‰é¢†åŸŸã€‚åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸï¼ŒOccVLAå¯ä»¥æé«˜è½¦è¾†å¯¹å‘¨å›´ç¯å¢ƒçš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œä»è€Œæé«˜é©¾é©¶å®‰å…¨æ€§å’Œèˆ’é€‚æ€§ã€‚åœ¨æœºå™¨äººå¯¼èˆªé¢†åŸŸï¼ŒOccVLAå¯ä»¥å¸®åŠ©æœºå™¨äººæ›´å¥½åœ°ç†è§£å‘¨å›´ç¯å¢ƒï¼Œä»è€Œå®ç°æ›´æ™ºèƒ½çš„å¯¼èˆªã€‚åœ¨è™šæ‹Ÿç°å®é¢†åŸŸï¼ŒOccVLAå¯ä»¥ç”¨äºæ„å»ºæ›´é€¼çœŸçš„3Dåœºæ™¯ï¼Œä»è€Œæé«˜ç”¨æˆ·çš„æ²‰æµ¸æ„Ÿã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal large language models (MLLMs) have shown strong vision-language reasoning abilities but still lack robust 3D spatial understanding, which is critical for autonomous driving. This limitation stems from two key challenges: (1) the difficulty of constructing accessible yet effective 3D representations without expensive manual annotations, and (2) the loss of fine-grained spatial details in VLMs due to the absence of large-scale 3D vision-language pretraining. To address these challenges, we propose OccVLA, a novel framework that integrates 3D occupancy representations into a unified multimodal reasoning process. Unlike prior approaches that rely on explicit 3D inputs, OccVLA treats dense 3D occupancy as both a predictive output and a supervisory signal, enabling the model to learn fine-grained spatial structures directly from 2D visual inputs. The occupancy predictions are regarded as implicit reasoning processes and can be skipped during inference without performance degradation, thereby adding no extra computational overhead. OccVLA achieves state-of-the-art results on the nuScenes benchmark for trajectory planning and demonstrates superior performance on 3D visual question-answering tasks, offering a scalable, interpretable, and fully vision-based solution for autonomous driving.

