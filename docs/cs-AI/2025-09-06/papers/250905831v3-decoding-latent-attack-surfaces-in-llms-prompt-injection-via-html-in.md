---
layout: default
title: Decoding Latent Attack Surfaces in LLMs: Prompt Injection via HTML in Web Summarization
---

# Decoding Latent Attack Surfaces in LLMs: Prompt Injection via HTML in Web Summarization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.05831" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.05831v3</a>
  <a href="https://arxiv.org/pdf/2509.05831.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.05831v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.05831v3', 'Decoding Latent Attack Surfaces in LLMs: Prompt Injection via HTML in Web Summarization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ishaan Verma, Arsheya Yadav

**åˆ†ç±»**: cs.CR, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-06 (æ›´æ–°: 2025-11-11)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ­ç¤ºLLMåœ¨Webæ‘˜è¦ä¸­çš„æ½œåœ¨æ”»å‡»é¢ï¼šé€šè¿‡HTMLæ³¨å…¥æç¤º**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æç¤ºæ³¨å…¥` `å¤§å‹è¯­è¨€æ¨¡å‹` `Webæ‘˜è¦` `HTMLæ”»å‡»` `ä¿¡æ¯å®‰å…¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMåœ¨Webå†…å®¹æ‘˜è¦åº”ç”¨ä¸­æ˜“å—æç¤ºæ³¨å…¥æ”»å‡»ï¼Œæ”»å‡»è€…å¯åˆ©ç”¨éšè—çš„HTMLå…ƒç´ åµŒå…¥æ¶æ„æŒ‡ä»¤ã€‚
2. è¯¥ç ”ç©¶æå‡ºä¸€ç§åŸºäºHTMLçš„æç¤ºæ³¨å…¥æ–¹æ³•ï¼Œé€šè¿‡æ“çºµ<meta>ç­‰æ ‡ç­¾ï¼Œåœ¨ä¸æ”¹å˜å¯è§å†…å®¹çš„å‰æä¸‹å½±å“LLMçš„æ‘˜è¦ç»“æœã€‚
3. å®éªŒè¡¨æ˜ï¼ŒLlama 4 Scoutå’ŒGemma 9B ITæ¨¡å‹å‡å—åˆ°è¯¥æ”»å‡»çš„å½±å“ï¼Œåˆ†åˆ«æœ‰29%å’Œ15%çš„æ³¨å…¥æ ·æœ¬å¯¼è‡´æ‘˜è¦ç»“æœå‘ç”Ÿå˜åŒ–ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¶Šæ¥è¶Šå¤šåœ°è¢«é›†æˆåˆ°åŸºäºWebçš„ç³»ç»Ÿä¸­ç”¨äºå†…å®¹æ‘˜è¦ï¼Œä½†å®ƒä»¬å¯¹æç¤ºæ³¨å…¥æ”»å‡»çš„æ•æ„Ÿæ€§ä»ç„¶æ˜¯ä¸€ä¸ªç´§è¿«çš„é—®é¢˜ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨éå¯è§HTMLå…ƒç´ ï¼ˆå¦‚<meta>ã€aria-labelå’Œaltå±æ€§ï¼‰æ¥åµŒå…¥å¯¹æŠ—æ€§æŒ‡ä»¤ï¼Œè€Œä¸æ”¹å˜ç½‘é¡µçš„å¯è§å†…å®¹ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŒ…å«280ä¸ªé™æ€ç½‘é¡µçš„æ–°æ•°æ®é›†ï¼Œè¿™äº›ç½‘é¡µå‡åŒ€åœ°åˆ†ä¸ºå¹²å‡€ç‰ˆæœ¬å’Œå¯¹æŠ—æ€§æ³¨å…¥ç‰ˆæœ¬ï¼Œä½¿ç”¨ä¸åŒçš„åŸºäºHTMLçš„ç­–ç•¥åˆ¶ä½œã€‚è¿™äº›é¡µé¢é€šè¿‡æµè§ˆå™¨è‡ªåŠ¨åŒ–æµç¨‹è¿›è¡Œå¤„ç†ï¼Œä»¥æå–åŸå§‹HTMLå’Œæ¸²æŸ“æ–‡æœ¬ï¼Œä»è€Œç´§å¯†æ¨¡æ‹Ÿå®é™…çš„LLMéƒ¨ç½²åœºæ™¯ã€‚æˆ‘ä»¬è¯„ä¼°äº†ä¸¤ä¸ªæœ€å…ˆè¿›çš„å¼€æºæ¨¡å‹Llama 4 Scout (Meta) å’Œ Gemma 9B IT (Google) æ€»ç»“è¿™äº›å†…å®¹çš„èƒ½åŠ›ã€‚ä½¿ç”¨è¯æ±‡ï¼ˆROUGE-Lï¼‰å’Œè¯­ä¹‰ï¼ˆSBERTä½™å¼¦ç›¸ä¼¼åº¦ï¼‰æŒ‡æ ‡ï¼Œä»¥åŠæ‰‹åŠ¨æ³¨é‡Šï¼Œæˆ‘ä»¬è¯„ä¼°äº†è¿™äº›éšè”½æ³¨å…¥çš„å½±å“ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¶…è¿‡29%çš„æ³¨å…¥æ ·æœ¬å¯¼è‡´Llama 4 Scoutæ‘˜è¦å‘ç”Ÿæ˜æ˜¾å˜åŒ–ï¼Œè€ŒGemma 9B ITçš„æˆåŠŸç‡è¾ƒä½ï¼Œä½†ä»è¾¾åˆ°15%ã€‚è¿™äº›ç»“æœçªå‡ºäº†LLMé©±åŠ¨çš„Webç®¡é“ä¸­ä¸€ä¸ªå…³é”®ä¸”å¾ˆå¤§ç¨‹åº¦ä¸Šè¢«å¿½è§†çš„æ¼æ´ï¼Œå³éšè—çš„å¯¹æŠ—æ€§å†…å®¹å¯ä»¥å·§å¦™åœ°æ“çºµæ¨¡å‹è¾“å‡ºã€‚æˆ‘ä»¬çš„å·¥ä½œæä¾›äº†ä¸€ä¸ªå¯é‡ç°çš„æ¡†æ¶å’ŒåŸºå‡†ï¼Œç”¨äºè¯„ä¼°åŸºäºHTMLçš„æç¤ºæ³¨å…¥ï¼Œå¹¶å¼ºè°ƒåœ¨æ¶‰åŠWebå†…å®¹çš„LLMåº”ç”¨ä¸­è¿«åˆ‡éœ€è¦å¼ºå¤§çš„ç¼“è§£ç­–ç•¥ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³LLMåœ¨Webå†…å®¹æ‘˜è¦åº”ç”¨ä¸­ï¼Œç”±äºå¯¹HTMLç»“æ„è§£æä¸å½“è€Œå¯¼è‡´çš„æç¤ºæ³¨å…¥æ¼æ´é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥æ£€æµ‹å’Œé˜²å¾¡éšè—åœ¨HTMLæ ‡ç­¾ä¸­çš„æ¶æ„æŒ‡ä»¤ï¼Œä½¿å¾—æ”»å‡»è€…å¯ä»¥åœ¨ä¸æ”¹å˜ç½‘é¡µå¯è§å†…å®¹çš„æƒ…å†µä¸‹æ“çºµLLMçš„è¾“å‡ºã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨HTMLä¸­éå¯è§çš„å…ƒç´ ï¼ˆå¦‚<meta>æ ‡ç­¾ã€aria-labelå±æ€§å’Œaltå±æ€§ï¼‰æ¥åµŒå…¥å¯¹æŠ—æ€§æŒ‡ä»¤ã€‚è¿™äº›æŒ‡ä»¤ä¸ä¼šç›´æ¥æ˜¾ç¤ºåœ¨ç½‘é¡µä¸Šï¼Œä½†ä¼šè¢«LLMè§£æå¹¶å½±å“å…¶æ‘˜è¦ç”Ÿæˆè¿‡ç¨‹ã€‚é€šè¿‡æ„é€ åŒ…å«è¿™äº›æ¶æ„æŒ‡ä»¤çš„ç½‘é¡µï¼Œå¯ä»¥è¯±å¯¼LLMç”Ÿæˆæ”»å‡»è€…æœŸæœ›çš„è¾“å‡ºã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼š1) æ„å»ºåŒ…å«å¹²å‡€ç½‘é¡µå’Œæ¶æ„æ³¨å…¥ç½‘é¡µçš„æ•°æ®é›†ï¼›2) ä½¿ç”¨æµè§ˆå™¨è‡ªåŠ¨åŒ–å·¥å…·ï¼ˆå¦‚Seleniumï¼‰æå–ç½‘é¡µçš„åŸå§‹HTMLå’Œæ¸²æŸ“åçš„æ–‡æœ¬ï¼›3) å°†æå–çš„å†…å®¹è¾“å…¥åˆ°LLMï¼ˆLlama 4 Scoutå’ŒGemma 9B ITï¼‰ä¸­è¿›è¡Œæ‘˜è¦ç”Ÿæˆï¼›4) ä½¿ç”¨è¯æ±‡ï¼ˆROUGE-Lï¼‰å’Œè¯­ä¹‰ï¼ˆSBERTä½™å¼¦ç›¸ä¼¼åº¦ï¼‰æŒ‡æ ‡ä»¥åŠäººå·¥è¯„ä¼°æ¥è¯„ä¼°æ‘˜è¦çš„è´¨é‡å’Œæ”»å‡»çš„æˆåŠŸç‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šå…³é”®åˆ›æ–°åœ¨äºå‘ç°äº†åˆ©ç”¨HTMLéå¯è§å…ƒç´ è¿›è¡Œæç¤ºæ³¨å…¥çš„æ”»å‡»é¢ã€‚ä¸ä¼ ç»Ÿçš„æç¤ºæ³¨å…¥æ–¹æ³•ä¸åŒï¼Œè¯¥æ–¹æ³•ä¸éœ€è¦ä¿®æ”¹ç½‘é¡µçš„å¯è§å†…å®¹ï¼Œå› æ­¤æ›´éš¾è¢«æ£€æµ‹å’Œé˜²å¾¡ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æ„å»ºäº†ä¸€ä¸ªåŒ…å«å¤šç§HTMLæ³¨å…¥ç­–ç•¥çš„æ•°æ®é›†ï¼Œä¸ºè¯„ä¼°LLMçš„å®‰å…¨æ€§æä¾›äº†ä¸€ä¸ªåŸºå‡†ã€‚

**å…³é”®è®¾è®¡**ï¼šæ•°æ®é›†åŒ…å«280ä¸ªé™æ€ç½‘é¡µï¼Œå…¶ä¸­ä¸€åŠæ˜¯å¹²å‡€çš„ï¼Œå¦ä¸€åŠåŒ…å«æ¶æ„æ³¨å…¥ã€‚æ³¨å…¥ç­–ç•¥åŒ…æ‹¬ä½¿ç”¨<meta>æ ‡ç­¾ã€aria-labelå±æ€§å’Œaltå±æ€§ç­‰ã€‚ä½¿ç”¨ROUGE-Lå’ŒSBERTä½™å¼¦ç›¸ä¼¼åº¦ä½œä¸ºè¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶è¿›è¡Œäººå·¥è¯„ä¼°ä»¥éªŒè¯ç»“æœã€‚æ²¡æœ‰ç‰¹åˆ«æåŠæŸå¤±å‡½æ•°æˆ–ç½‘ç»œç»“æ„ï¼Œå› ä¸ºé‡ç‚¹åœ¨äºæ”»å‡»æ–¹æ³•è€Œéæ¨¡å‹è®­ç»ƒã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒLlama 4 Scoutæ¨¡å‹åœ¨29%çš„æ³¨å…¥æ ·æœ¬ä¸­å—åˆ°å½±å“ï¼Œæ‘˜è¦ç»“æœå‘ç”Ÿæ˜æ˜¾å˜åŒ–ã€‚Gemma 9B ITæ¨¡å‹çš„å—å½±å“æ¯”ä¾‹ä¸º15%ï¼Œè™½ç„¶è¾ƒä½ï¼Œä½†ä¹Ÿè¡¨æ˜è¯¥æ¨¡å‹å­˜åœ¨ç±»ä¼¼çš„æ¼æ´ã€‚è¿™äº›æ•°æ®çªå‡ºäº†LLMåœ¨å¤„ç†Webå†…å®¹æ—¶é¢ä¸´çš„æ½œåœ¨å®‰å…¨é£é™©ï¼Œå¹¶å¼ºè°ƒäº†å¼€å‘æœ‰æ•ˆé˜²å¾¡æœºåˆ¶çš„é‡è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæå‡LLMåœ¨Webå†…å®¹å¤„ç†ä¸­çš„å®‰å…¨æ€§ã€‚é€šè¿‡è¯†åˆ«å’Œé˜²å¾¡HTMLæ³¨å…¥æ”»å‡»ï¼Œå¯ä»¥é˜²æ­¢LLMè¢«æ¶æ„æ“çºµï¼Œç¡®ä¿å…¶åœ¨æ–°é—»æ‘˜è¦ã€ä¿¡æ¯æ£€ç´¢ã€å†…å®¹æ¨èç­‰é¢†åŸŸçš„å¯é æ€§å’Œå®‰å…¨æ€§ã€‚ç ”ç©¶ç»“æœä¹Ÿä¸ºå¼€å‘æ›´é²æ£’çš„LLMå’ŒWebåº”ç”¨æä¾›äº†æŒ‡å¯¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) are increasingly integrated into web-based systems for content summarization, yet their susceptibility to prompt injection attacks remains a pressing concern. In this study, we explore how non-visible HTML elements such as <meta>, aria-label, and alt attributes can be exploited to embed adversarial instructions without altering the visible content of a webpage. We introduce a novel dataset comprising 280 static web pages, evenly divided between clean and adversarial injected versions, crafted using diverse HTML-based strategies. These pages are processed through a browser automation pipeline to extract both raw HTML and rendered text, closely mimicking real-world LLM deployment scenarios. We evaluate two state-of-the-art open-source models, Llama 4 Scout (Meta) and Gemma 9B IT (Google), on their ability to summarize this content. Using both lexical (ROUGE-L) and semantic (SBERT cosine similarity) metrics, along with manual annotations, we assess the impact of these covert injections. Our findings reveal that over 29% of injected samples led to noticeable changes in the Llama 4 Scout summaries, while Gemma 9B IT showed a lower, yet non-trivial, success rate of 15%. These results highlight a critical and largely overlooked vulnerability in LLM driven web pipelines, where hidden adversarial content can subtly manipulate model outputs. Our work offers a reproducible framework and benchmark for evaluating HTML-based prompt injection and underscores the urgent need for robust mitigation strategies in LLM applications involving web content.

