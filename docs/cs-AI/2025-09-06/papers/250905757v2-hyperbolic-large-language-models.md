---
layout: default
title: Hyperbolic Large Language Models
---

# Hyperbolic Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.05757" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.05757v2</a>
  <a href="https://arxiv.org/pdf/2509.05757.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.05757v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.05757v2', 'Hyperbolic Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sarang Patil, Zeyong Zhang, Yiran Huang, Tengfei Ma, Mengjia Xu

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-06 (æ›´æ–°: 2025-12-07)

**å¤‡æ³¨**: 27 pages, 7 figures

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/sarangp2402/Hyperbolic-LLM-Models)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºåŒæ›²å‡ ä½•çš„å¤§è¯­è¨€æ¨¡å‹(HypLLMs)ï¼Œå¢å¼ºè¯­ä¹‰è¡¨ç¤ºå­¦ä¹ å’Œå¤šå°ºåº¦æ¨ç†èƒ½åŠ›ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åŒæ›²å‡ ä½•` `å¤§å‹è¯­è¨€æ¨¡å‹` `è¯­ä¹‰è¡¨ç¤ºå­¦ä¹ ` `åˆ†å±‚ç»“æ„` `éæ¬§å‡ é‡Œå¾—ç©ºé—´`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMéš¾ä»¥æœ‰æ•ˆå­¦ä¹ ç°å®ä¸–ç•Œä¸­éæ¬§å‡ é‡Œå¾—çš„ã€å…·æœ‰åˆ†å±‚ç»“æ„çš„æ•°æ®ï¼Œä¾‹å¦‚è¯­è¨€ç»“æ„å’Œç½‘ç»œã€‚
2. åˆ©ç”¨åŒæ›²å‡ ä½•ä½œä¸ºLLMçš„è¡¨ç¤ºç©ºé—´ï¼Œæ—¨åœ¨å¢å¼ºæ¨¡å‹å¯¹è¯­ä¹‰è•´å«å’Œåˆ†å±‚å…³ç³»çš„å»ºæ¨¡èƒ½åŠ›ã€‚
3. è®ºæ–‡å¯¹åŒæ›²LLMè¿›è¡Œäº†åˆ†ç±»ï¼Œå¹¶æ¢è®¨äº†å…¶åœ¨å¤šä¸ªé¢†åŸŸçš„æ½œåœ¨åº”ç”¨å’Œæœªæ¥ç ”ç©¶æ–¹å‘ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ã€å¤©æ°”é¢„æŠ¥ã€ç”Ÿç‰©è›‹ç™½è´¨æŠ˜å ã€æ–‡æœ¬ç”Ÿæˆå’Œè§£å†³æ•°å­¦é—®é¢˜ç­‰å¤šä¸ªä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸå¹¶è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œè®¸å¤šç°å®ä¸–ç•Œçš„æ•°æ®å‘ˆç°å‡ºé«˜åº¦éæ¬§å‡ é‡Œå¾—æ½œåœ¨åˆ†å±‚ç»“æ„ï¼Œä¾‹å¦‚è›‹ç™½è´¨ç½‘ç»œã€äº¤é€šç½‘ç»œã€é‡‘èç½‘ç»œã€å¤§è„‘ç½‘ç»œä»¥åŠè‡ªç„¶è¯­è¨€ä¸­çš„è¯­è¨€ç»“æ„æˆ–å¥æ³•æ ‘ã€‚åˆ©ç”¨LLMsä»è¿™äº›åŸå§‹ã€éç»“æ„åŒ–è¾“å…¥æ•°æ®ä¸­æœ‰æ•ˆåœ°å­¦ä¹ å†…åœ¨çš„è¯­ä¹‰è•´å«å’Œåˆ†å±‚å…³ç³»ä»ç„¶æ˜¯ä¸€ä¸ªæœªè¢«å……åˆ†æ¢ç´¢çš„é¢†åŸŸã€‚ç”±äºåŒæ›²å‡ ä½•åœ¨å»ºæ¨¡æ ‘çŠ¶åˆ†å±‚ç»“æ„æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä½œä¸ºä¸€ç§éæ¬§å‡ é‡Œå¾—ç©ºé—´ï¼Œå®ƒå·²è¿…é€Ÿæ™®åŠï¼Œæˆä¸ºè·¨å›¾ã€å›¾åƒã€è¯­è¨€å’Œå¤šæ¨¡æ€æ•°æ®ç­‰é¢†åŸŸçš„å¤æ‚æ•°æ®å»ºæ¨¡çš„å¯Œæœ‰è¡¨ç°åŠ›çš„æ½œåœ¨è¡¨ç¤ºç©ºé—´ã€‚æœ¬æ–‡å…¨é¢è€Œæœ‰é’ˆå¯¹æ€§åœ°é˜è¿°äº†LLMsçš„æœ€æ–°è¿›å±•ï¼Œè¿™äº›LLMsåˆ©ç”¨åŒæ›²å‡ ä½•ä½œä¸ºè¡¨ç¤ºç©ºé—´æ¥å¢å¼ºè¯­ä¹‰è¡¨ç¤ºå­¦ä¹ å’Œå¤šå°ºåº¦æ¨ç†ã€‚å…·ä½“æ¥è¯´ï¼Œæœ¬æ–‡æ ¹æ®å››ä¸ªä¸»è¦ç±»åˆ«ä»‹ç»äº†åŒæ›²LLMï¼ˆHypLLMï¼‰çš„ä¸»è¦æŠ€æœ¯åˆ†ç±»ï¼šï¼ˆ1ï¼‰é€šè¿‡exp/logæ˜ å°„çš„åŒæ›²LLMï¼›ï¼ˆ2ï¼‰åŒæ›²å¾®è°ƒæ¨¡å‹ï¼›ï¼ˆ3ï¼‰å®Œå…¨åŒæ›²LLMï¼›ï¼ˆ4ï¼‰åŒæ›²çŠ¶æ€ç©ºé—´æ¨¡å‹ã€‚æˆ‘ä»¬è¿˜æ¢è®¨äº†å…³é”®çš„æ½œåœ¨åº”ç”¨ï¼Œå¹¶æ¦‚è¿°äº†æœªæ¥çš„ç ”ç©¶æ–¹å‘ã€‚å…³é”®è®ºæ–‡ã€æ¨¡å‹ã€æ•°æ®é›†å’Œä»£ç å®ç°çš„å­˜å‚¨åº“å¯åœ¨https://github.com/sarangp2402/Hyperbolic-LLM-Modelsä¸Šæ‰¾åˆ°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å…·æœ‰å†…åœ¨åˆ†å±‚ç»“æ„çš„æ•°æ®æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚è®¸å¤šç°å®ä¸–ç•Œçš„æ•°æ®ï¼Œå¦‚è›‹ç™½è´¨ç½‘ç»œã€äº¤é€šç½‘ç»œå’Œè¯­è¨€ç»“æ„ï¼Œéƒ½å‘ˆç°å‡ºéæ¬§å‡ é‡Œå¾—çš„å‡ ä½•ç‰¹æ€§ã€‚ä¼ ç»Ÿçš„LLMé€šå¸¸åœ¨æ¬§å‡ é‡Œå¾—ç©ºé—´ä¸­è¿›è¡Œæ“ä½œï¼Œè¿™é™åˆ¶äº†å®ƒä»¬æ•æ‰è¿™äº›å¤æ‚å…³ç³»çš„èƒ½åŠ›ã€‚å› æ­¤ï¼Œå¦‚ä½•æœ‰æ•ˆåœ°åˆ©ç”¨LLMå­¦ä¹ è¿™äº›æ•°æ®çš„å†…åœ¨è¯­ä¹‰å’Œåˆ†å±‚å…³ç³»æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨åŒæ›²å‡ ä½•ä½œä¸ºLLMçš„è¡¨ç¤ºç©ºé—´ã€‚åŒæ›²ç©ºé—´èƒ½å¤Ÿæ›´è‡ªç„¶åœ°è¡¨ç¤ºæ ‘çŠ¶å’Œåˆ†å±‚ç»“æ„ï¼Œè¿™ä½¿å¾—LLMèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰æ•°æ®ä¸­çš„è¯­ä¹‰è•´å«å’Œåˆ†å±‚å…³ç³»ã€‚é€šè¿‡å°†æ•°æ®åµŒå…¥åˆ°åŒæ›²ç©ºé—´ä¸­ï¼Œæ¨¡å‹å¯ä»¥æ›´æœ‰æ•ˆåœ°å­¦ä¹ å’Œæ¨ç†è¿™äº›å…³ç³»ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡å°†åŒæ›²LLMï¼ˆHypLLMï¼‰åˆ†ä¸ºå››ä¸ªä¸»è¦ç±»åˆ«ï¼š(1) é€šè¿‡æŒ‡æ•°/å¯¹æ•°æ˜ å°„çš„åŒæ›²LLMï¼Œè¿™ç±»æ¨¡å‹åˆ©ç”¨æŒ‡æ•°å’Œå¯¹æ•°æ˜ å°„åœ¨æ¬§å‡ é‡Œå¾—ç©ºé—´å’ŒåŒæ›²ç©ºé—´ä¹‹é—´è¿›è¡Œè½¬æ¢ï¼›(2) åŒæ›²å¾®è°ƒæ¨¡å‹ï¼Œè¿™ç±»æ¨¡å‹é¦–å…ˆåœ¨æ¬§å‡ é‡Œå¾—ç©ºé—´ä¸­é¢„è®­ç»ƒï¼Œç„¶ååœ¨åŒæ›²ç©ºé—´ä¸­è¿›è¡Œå¾®è°ƒï¼›(3) å®Œå…¨åŒæ›²LLMï¼Œè¿™ç±»æ¨¡å‹å®Œå…¨åœ¨åŒæ›²ç©ºé—´ä¸­è¿›è¡Œè®­ç»ƒå’Œæ¨ç†ï¼›(4) åŒæ›²çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼Œè¿™ç±»æ¨¡å‹å°†åŒæ›²å‡ ä½•ä¸çŠ¶æ€ç©ºé—´æ¨¡å‹ç›¸ç»“åˆã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå°†åŒæ›²å‡ ä½•å¼•å…¥åˆ°LLMçš„è¡¨ç¤ºç©ºé—´ä¸­ã€‚ä¸ä¼ ç»Ÿçš„æ¬§å‡ é‡Œå¾—ç©ºé—´ç›¸æ¯”ï¼ŒåŒæ›²ç©ºé—´æ›´é€‚åˆè¡¨ç¤ºå…·æœ‰åˆ†å±‚ç»“æ„çš„æ•°æ®ã€‚è¿™ç§åˆ›æ–°ä½¿å¾—LLMèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰æ•°æ®ä¸­çš„è¯­ä¹‰è•´å«å’Œåˆ†å±‚å…³ç³»ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­æåˆ°çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼šä¸åŒçš„åŒæ›²ç©ºé—´åµŒå…¥æ–¹æ³•ï¼ˆä¾‹å¦‚åºåŠ è±çƒæ¨¡å‹ã€åŒæ›²é¢æ¨¡å‹ï¼‰ï¼Œä»¥åŠå¦‚ä½•åœ¨åŒæ›²ç©ºé—´ä¸­å®šä¹‰å’Œè®¡ç®—æ³¨æ„åŠ›æœºåˆ¶ã€‚æ­¤å¤–ï¼Œå¦‚ä½•é€‰æ‹©åˆé€‚çš„æŒ‡æ•°/å¯¹æ•°æ˜ å°„ï¼Œä»¥åŠå¦‚ä½•åœ¨åŒæ›²ç©ºé—´ä¸­è¿›è¡Œä¼˜åŒ–ä¹Ÿæ˜¯å…³é”®çš„è®¾è®¡è€ƒè™‘ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡æä¾›äº†ä¸€ä¸ªåŒæ›²LLMçš„å…¨é¢åˆ†ç±»ï¼Œå¹¶å¼€æºäº†ä¸€ä¸ªåŒ…å«å…³é”®è®ºæ–‡ã€æ¨¡å‹ã€æ•°æ®é›†å’Œä»£ç å®ç°çš„å­˜å‚¨åº“ã€‚è™½ç„¶æ‘˜è¦ä¸­æ²¡æœ‰æ˜ç¡®æåŠå…·ä½“çš„æ€§èƒ½æ•°æ®ï¼Œä½†è¯¥èµ„æºåº“ä¸ºåç»­ç ”ç©¶æä¾›äº†ä¾¿åˆ©ï¼Œå¹¶å¯èƒ½ä¿ƒè¿›åŒæ›²LLMçš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå¤šä¸ªé¢†åŸŸï¼ŒåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€çŸ¥è¯†å›¾è°±æ¨ç†ã€ç”Ÿç‰©ä¿¡æ¯å­¦å’Œç¤¾äº¤ç½‘ç»œåˆ†æã€‚ä¾‹å¦‚ï¼Œåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­ï¼Œå¯ä»¥åˆ©ç”¨HypLLMæ›´å¥½åœ°ç†è§£å¥å­çš„å¥æ³•ç»“æ„å’Œè¯­ä¹‰å…³ç³»ã€‚åœ¨ç”Ÿç‰©ä¿¡æ¯å­¦ä¸­ï¼Œå¯ä»¥ç”¨äºå»ºæ¨¡è›‹ç™½è´¨ç½‘ç»œå’ŒåŸºå› è°ƒæ§ç½‘ç»œã€‚åœ¨ç¤¾äº¤ç½‘ç»œåˆ†æä¸­ï¼Œå¯ä»¥ç”¨äºå‘ç°ç¤¾åŒºç»“æ„å’Œç”¨æˆ·ä¹‹é—´çš„å…³ç³»ã€‚è¯¥ç ”ç©¶å…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have achieved remarkable success and demonstrated superior performance across various tasks, including natural language processing (NLP), weather forecasting, biological protein folding, text generation, and solving mathematical problems. However, many real-world data exhibit highly non-Euclidean latent hierarchical anatomy, such as protein networks, transportation networks, financial networks, brain networks, and linguistic structures or syntactic trees in natural languages. Effectively learning intrinsic semantic entailment and hierarchical relationships from these raw, unstructured input data using LLMs remains an underexplored area. Due to its effectiveness in modeling tree-like hierarchical structures, hyperbolic geometry -- a non-Euclidean space -- has rapidly gained popularity as an expressive latent representation space for complex data modeling across domains such as graphs, images, languages, and multi-modal data. Here, we provide a comprehensive and contextual exposition of recent advancements in LLMs that leverage hyperbolic geometry as a representation space to enhance semantic representation learning and multi-scale reasoning. Specifically, the paper presents a taxonomy of the principal techniques of Hyperbolic LLMs (HypLLMs) in terms of four main categories: (1) hyperbolic LLMs through exp/log maps; (2) hyperbolic fine-tuned models; (3) fully hyperbolic LLMs, and (4) hyperbolic state-space models. We also explore crucial potential applications and outline future research directions. A repository of key papers, models, datasets, and code implementations is available at https://github.com/sarangp2402/Hyperbolic-LLM-Models.

