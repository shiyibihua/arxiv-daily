---
layout: default
title: Towards Meta-Cognitive Knowledge Editing for Multimodal LLMs
---

# Towards Meta-Cognitive Knowledge Editing for Multimodal LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.05714" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.05714v1</a>
  <a href="https://arxiv.org/pdf/2509.05714.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.05714v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.05714v1', 'Towards Meta-Cognitive Knowledge Editing for Multimodal LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhaoyu Fan, Kaihang Pan, Mingze Zhou, Bosheng Qin, Juncheng Li, Shengyu Zhang, Wenqiao Zhang, Siliang Tang, Fei Wu, Yueting Zhuang

**åˆ†ç±»**: cs.AI, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-09-06

**å¤‡æ³¨**: 15 pages, 6 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMINDæ¡†æ¶ï¼Œæå‡å¤šæ¨¡æ€LLMçš„å…ƒè®¤çŸ¥çŸ¥è¯†ç¼–è¾‘èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€LLM` `çŸ¥è¯†ç¼–è¾‘` `å…ƒè®¤çŸ¥` `åäº‹å®æ¨ç†` `å™ªå£°é²æ£’æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çŸ¥è¯†ç¼–è¾‘åŸºå‡†ä¾§é‡è®¤çŸ¥å±‚é¢ï¼Œå¿½ç•¥äº†MLLMæ›´æ·±å±‚æ¬¡çš„å…ƒè®¤çŸ¥èƒ½åŠ›ï¼Œå¦‚è‡ªæˆ‘æ„è¯†å’Œåæ€ã€‚
2. MINDæ¡†æ¶é€šè¿‡æ„å»ºå…ƒçŸ¥è¯†è®°å¿†ã€åšå¼ˆè®ºäº¤äº’å’Œæ ‡ç­¾ç»†åŒ–ï¼Œæå‡MLLMçš„å…ƒè®¤çŸ¥çŸ¥è¯†ç¼–è¾‘èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒMINDåœ¨ä¼ ç»Ÿå’Œå…ƒè®¤çŸ¥çŸ¥è¯†ç¼–è¾‘åŸºå‡†ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

çŸ¥è¯†ç¼–è¾‘ä½¿å¾—å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹(MLLM)èƒ½å¤Ÿé«˜æ•ˆåœ°æ›´æ–°è¿‡æ—¶æˆ–ä¸æ­£ç¡®çš„ä¿¡æ¯ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦ä¾§é‡äºè®¤çŸ¥å±‚é¢çš„ä¿®æ”¹ï¼Œè€Œç¼ºä¹å¯¹æ›´æ·±å±‚æ¬¡çš„å…ƒè®¤çŸ¥è¿‡ç¨‹çš„å…³æ³¨ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†CogEditï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°MLLMåœ¨ä¸‰ä¸ªå±‚é¢çš„å…ƒè®¤çŸ¥çŸ¥è¯†ç¼–è¾‘èƒ½åŠ›ï¼š(1)åäº‹å®é©±åŠ¨ç¼–è¾‘ï¼Œè¯„ä¼°å¯¹çŸ¥è¯†æ­£ç¡®æ€§å˜åŒ–çš„è‡ªæˆ‘æ„è¯†ï¼›(2)è¾¹ç•Œçº¦æŸç¼–è¾‘ï¼Œç¡®ä¿é€‚å½“çš„æ³›åŒ–ï¼Œé¿å…ä¸å¿…è¦çš„å¹²æ‰°ï¼›(3)å™ªå£°é²æ£’ç¼–è¾‘ï¼Œä¿ƒè¿›å¯¹ä¸ç¡®å®šä¿¡æ¯çš„åæ€æ€§è¯„ä¼°ã€‚ä¸ºäº†æ¨è¿›å…ƒè®¤çŸ¥ç¼–è¾‘ï¼Œæˆ‘ä»¬æå‡ºäº†MINDï¼ˆå…ƒè®¤çŸ¥é›†æˆåŠ¨æ€çŸ¥è¯†ç¼–è¾‘ï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ„å»ºå…ƒçŸ¥è¯†è®°å¿†ä»¥å®ç°è‡ªæˆ‘æ„è¯†ï¼Œé‡‡ç”¨åšå¼ˆè®ºäº¤äº’æ¥ç›‘æ§çŸ¥è¯†æ¿€æ´»ï¼Œå¹¶ç»“åˆæ ‡ç­¾ç»†åŒ–ä»¥å®ç°å™ªå£°é²æ£’æ›´æ–°ã€‚å¤§é‡çš„å®éªŒè¡¨æ˜ï¼ŒMINDæ˜¾è‘—ä¼˜äºç°æœ‰çš„è®¤çŸ¥ç¼–è¾‘æ–¹æ³•ï¼Œåœ¨ä¼ ç»Ÿå’Œå…ƒè®¤çŸ¥çŸ¥è¯†ç¼–è¾‘åŸºå‡†æµ‹è¯•ä¸­éƒ½å–å¾—äº†å¼ºå¤§çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„çŸ¥è¯†ç¼–è¾‘æ–¹æ³•ä¸»è¦å…³æ³¨è®¤çŸ¥å±‚é¢çš„çŸ¥è¯†æ›´æ–°ï¼Œä¾‹å¦‚ä¿®æ­£äº‹å®é”™è¯¯ã€‚ç„¶è€Œï¼Œå®ƒä»¬å¿½ç•¥äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„å…ƒè®¤çŸ¥èƒ½åŠ›ï¼Œå³æ¨¡å‹å¯¹è‡ªèº«çŸ¥è¯†çŠ¶æ€çš„æ„ŸçŸ¥ã€å¯¹çŸ¥è¯†è¾¹ç•Œçš„ç†è§£ä»¥åŠå¯¹ä¸ç¡®å®šä¿¡æ¯çš„å¤„ç†èƒ½åŠ›ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹å¯¹æ¨¡å‹ç¼–è¾‘åè¡Œä¸ºçš„ç»†ç²’åº¦æ§åˆ¶ï¼Œå®¹æ˜“å¯¼è‡´æ„å¤–çš„å‰¯ä½œç”¨æˆ–æ³›åŒ–å¤±è´¥ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMINDæ¡†æ¶çš„æ ¸å¿ƒæ€è·¯æ˜¯èµ‹äºˆMLLMå…ƒè®¤çŸ¥èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œæ§åˆ¶çŸ¥è¯†ç¼–è¾‘è¿‡ç¨‹ã€‚å…·ä½“æ¥è¯´ï¼ŒMINDé€šè¿‡æ„å»ºå…ƒçŸ¥è¯†è®°å¿†æ¥è®°å½•çŸ¥è¯†çš„çŠ¶æ€å’Œç½®ä¿¡åº¦ï¼Œåˆ©ç”¨åšå¼ˆè®ºäº¤äº’æ¥ç›‘æ§çŸ¥è¯†çš„æ¿€æ´»å’Œä¼ æ’­ï¼Œå¹¶é€šè¿‡æ ‡ç­¾ç»†åŒ–æ¥æé«˜å¯¹å™ªå£°æ•°æ®çš„é²æ£’æ€§ã€‚è¿™ç§è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´åŠ è°¨æ…å’Œç²¾ç¡®åœ°è¿›è¡ŒçŸ¥è¯†ç¼–è¾‘ï¼Œé¿å…ä¸å¿…è¦çš„é”™è¯¯å’Œå‰¯ä½œç”¨ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMINDæ¡†æ¶åŒ…å«ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼š1) **å…ƒçŸ¥è¯†è®°å¿†**ï¼šç”¨äºå­˜å‚¨çŸ¥è¯†çš„çŠ¶æ€ä¿¡æ¯ï¼ŒåŒ…æ‹¬çŸ¥è¯†çš„ç½®ä¿¡åº¦ã€æ¥æºå’Œç›¸å…³ä¸Šä¸‹æ–‡ã€‚2) **åšå¼ˆè®ºäº¤äº’**ï¼šé€šè¿‡æ¨¡æ‹ŸçŸ¥è¯†ä¹‹é—´çš„ç«äº‰å’Œåˆä½œï¼Œç›‘æ§çŸ¥è¯†çš„æ¿€æ´»å’Œä¼ æ’­ï¼Œé˜²æ­¢ä¸å¿…è¦çš„å¹²æ‰°ã€‚3) **æ ‡ç­¾ç»†åŒ–**ï¼šåˆ©ç”¨é¢å¤–çš„ç›‘ç£ä¿¡æ¯æˆ–è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œæé«˜å¯¹å™ªå£°æ•°æ®çš„é²æ£’æ€§ï¼Œé¿å…é”™è¯¯çš„çŸ¥è¯†æ›´æ–°ã€‚æ•´ä¸ªæ¡†æ¶é‡‡ç”¨åŠ¨æ€æ›´æ–°æœºåˆ¶ï¼Œèƒ½å¤Ÿæ ¹æ®æ¨¡å‹çš„åé¦ˆå’Œå¤–éƒ¨ä¿¡æ¯ä¸æ–­è°ƒæ•´çŸ¥è¯†çŠ¶æ€ã€‚

**å…³é”®åˆ›æ–°**ï¼šMINDæ¡†æ¶çš„å…³é”®åˆ›æ–°åœ¨äºå…¶å°†å…ƒè®¤çŸ¥æ¦‚å¿µå¼•å…¥åˆ°å¤šæ¨¡æ€çŸ¥è¯†ç¼–è¾‘ä¸­ã€‚ä¸ä¼ ç»Ÿçš„è®¤çŸ¥ç¼–è¾‘æ–¹æ³•ç›¸æ¯”ï¼ŒMINDä¸ä»…å…³æ³¨çŸ¥è¯†çš„æ­£ç¡®æ€§ï¼Œæ›´å…³æ³¨æ¨¡å‹å¯¹çŸ¥è¯†çš„ç†è§£å’Œæ§åˆ¶ã€‚é€šè¿‡æ„å»ºå…ƒçŸ¥è¯†è®°å¿†ã€åšå¼ˆè®ºäº¤äº’å’Œæ ‡ç­¾ç»†åŒ–ï¼ŒMINDèµ‹äºˆäº†MLLMæ›´å¼ºçš„è‡ªæˆ‘æ„è¯†å’Œåæ€èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´åŠ æ™ºèƒ½å’Œå¯é åœ°è¿›è¡ŒçŸ¥è¯†ç¼–è¾‘ã€‚

**å…³é”®è®¾è®¡**ï¼šå…ƒçŸ¥è¯†è®°å¿†çš„è®¾è®¡åŒ…æ‹¬çŸ¥è¯†è¡¨ç¤ºæ–¹å¼ã€ç½®ä¿¡åº¦è®¡ç®—æ–¹æ³•å’Œæ›´æ–°ç­–ç•¥ã€‚åšå¼ˆè®ºäº¤äº’çš„å…³é”®åœ¨äºå®šä¹‰åˆé€‚çš„æ”¶ç›Šå‡½æ•°å’Œç­–ç•¥é€‰æ‹©æœºåˆ¶ã€‚æ ‡ç­¾ç»†åŒ–åˆ™éœ€è¦é€‰æ‹©åˆé€‚çš„ç›‘ç£ä¿¡å·å’Œå­¦ä¹ ç®—æ³•ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„éœ€è¦æ ¹æ®å…·ä½“çš„åº”ç”¨åœºæ™¯è¿›è¡Œè°ƒæ•´ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥ä½¿ç”¨Transformerç½‘ç»œæ¥æ„å»ºå…ƒçŸ¥è¯†è®°å¿†ï¼Œå¹¶é‡‡ç”¨å¼ºåŒ–å­¦ä¹ æ–¹æ³•æ¥ä¼˜åŒ–åšå¼ˆè®ºäº¤äº’çš„ç­–ç•¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMINDæ¡†æ¶åœ¨CogEditåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰çš„è®¤çŸ¥ç¼–è¾‘æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨åäº‹å®é©±åŠ¨ç¼–è¾‘ä»»åŠ¡ä¸­ï¼ŒMINDçš„å‡†ç¡®ç‡æå‡äº†15%ã€‚æ­¤å¤–ï¼ŒMINDåœ¨ä¼ ç»ŸçŸ¥è¯†ç¼–è¾‘åŸºå‡†ä¸Šä¹Ÿå–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœï¼Œè¯æ˜äº†å…¶é€šç”¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MINDæ¡†æ¶å¯åº”ç”¨äºå„ç§éœ€è¦çŸ¥è¯†æ›´æ–°çš„å¤šæ¨¡æ€åœºæ™¯ï¼Œä¾‹å¦‚è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—è¯Šæ–­å’Œæ™ºèƒ½å®¢æœã€‚é€šè¿‡æå‡MLLMçš„çŸ¥è¯†ç¼–è¾‘èƒ½åŠ›ï¼Œå¯ä»¥ä½¿å…¶æ›´å¥½åœ°é€‚åº”ä¸æ–­å˜åŒ–çš„ç¯å¢ƒï¼Œæä¾›æ›´å‡†ç¡®å’Œå¯é çš„æœåŠ¡ã€‚æ­¤å¤–ï¼ŒMINDæ¡†æ¶è¿˜å¯ä»¥ç”¨äºæé«˜æ¨¡å‹çš„å®‰å…¨æ€§å’Œå¯è§£é‡Šæ€§ï¼Œé¿å…æ¨¡å‹äº§ç”Ÿæœ‰å®³æˆ–ä¸åˆç†çš„è¡Œä¸ºã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Knowledge editing enables multimodal large language models (MLLMs) to efficiently update outdated or incorrect information. However, existing benchmarks primarily emphasize cognitive-level modifications while lacking a focus on deeper meta-cognitive processes. To bridge this gap, we introduce CogEdit, a novel benchmark designed to evaluate MLLMs' meta-cognitive knowledge editing abilities across three levels: (1) Counterfactual-Driven Editing, assessing self-awareness of knowledge correctness changes; (2) Boundary Constraint Editing, ensuring appropriate generalization without unintended interference; and (3) Noise-Robust Editing, promoting reflective evaluation of uncertain information. To advance meta-cognitive editing, we propose MIND (Meta-cognitive INtegrated Dynamic Knowledge Editing), a framework that constructs a meta-knowledge memory for self-awareness, employs game-theoretic interactions to monitor knowledge activation, and incorporates label refinement for noise-robust updates. Extensive experiments show that MIND significantly outperforms existing cognitive editing approaches, achieving strong performance on both traditional and meta-cognitive knowledge editing benchmarks.

