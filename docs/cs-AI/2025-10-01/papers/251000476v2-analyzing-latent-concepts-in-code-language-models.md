---
layout: default
title: Analyzing Latent Concepts in Code Language Models
---

# Analyzing Latent Concepts in Code Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.00476" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.00476v2</a>
  <a href="https://arxiv.org/pdf/2510.00476.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.00476v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.00476v2', 'Analyzing Latent Concepts in Code Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Arushi Sharma, Vedant Pungliya, Christopher J. Quinn, Ali Jannesari

**åˆ†ç±»**: cs.SE, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-10-01 (æ›´æ–°: 2025-10-02)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä»£ç æ¦‚å¿µåˆ†æ(CoCoA)æ¡†æ¶ï¼Œç”¨äºç†è§£ä»£ç è¯­è¨€æ¨¡å‹ä¸­çš„æ½œåœ¨æ¦‚å¿µã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä»£ç è¯­è¨€æ¨¡å‹` `å¯è§£é‡Šæ€§` `æ¦‚å¿µåˆ†æ` `åéªŒè§£é‡Š` `èšç±»` `é™æ€åˆ†æ` `æç¤ºå·¥ç¨‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä»£ç è¯­è¨€æ¨¡å‹çš„å†…éƒ¨æœºåˆ¶éš¾ä»¥è§£é‡Šï¼Œé˜»ç¢äº†å…¶åœ¨éœ€è¦é«˜é€æ˜åº¦å’Œé²æ£’æ€§çš„åœºæ™¯åº”ç”¨ã€‚
2. æå‡ºCoCoAæ¡†æ¶ï¼Œé€šè¿‡èšç±»tokenåµŒå…¥æ¥å‘ç°æ¨¡å‹ä¸­çš„æ½œåœ¨æ¦‚å¿µï¼Œå¹¶è¿›è¡Œå¯è§£é‡Šæ€§åˆ†æã€‚
3. å®éªŒè¡¨æ˜CoCoAå‘ç°çš„æ¦‚å¿µå…·æœ‰ç¨³å®šæ€§ï¼Œä¸”ä¸æ¨¡å‹å¾®è°ƒè¿‡ç¨‹ä¸€è‡´ï¼Œæå‡äº†è§£é‡Šæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç†è§£åœ¨ä»£ç ä¸Šè®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹çš„å†…éƒ¨è¡Œä¸ºä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦ä¿¡ä»»ã€é€æ˜åº¦å’Œè¯­ä¹‰é²æ£’æ€§çš„åº”ç”¨ä¸­ã€‚æˆ‘ä»¬æå‡ºäº†ä»£ç æ¦‚å¿µåˆ†æï¼ˆCoCoAï¼‰ï¼šä¸€ä¸ªå…¨å±€çš„åéªŒå¯è§£é‡Šæ€§æ¡†æ¶ï¼Œé€šè¿‡å°†ä¸Šä¸‹æ–‡ç›¸å…³çš„tokenåµŒå…¥èšç±»æˆäººç±»å¯è§£é‡Šçš„æ¦‚å¿µç»„ï¼Œæ¥æ­ç¤ºä»£ç è¯­è¨€æ¨¡å‹è¡¨ç¤ºç©ºé—´ä¸­æ¶Œç°çš„è¯æ±‡ã€å¥æ³•å’Œè¯­ä¹‰ç»“æ„ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ··åˆæ ‡æ³¨æµç¨‹ï¼Œç»“åˆäº†åŸºäºé™æ€åˆ†æå·¥å…·çš„å¥æ³•å¯¹é½å’Œpromptå·¥ç¨‹åŒ–çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œä»è€Œèƒ½å¤Ÿè·¨æŠ½è±¡çº§åˆ«å¯¹æ½œåœ¨æ¦‚å¿µè¿›è¡Œå¯æ‰©å±•çš„æ ‡æ³¨ã€‚æˆ‘ä»¬åˆ†æäº†æ¦‚å¿µåœ¨ä¸åŒå±‚å’Œä¸‰ä¸ªå¾®è°ƒä»»åŠ¡ä¸­çš„åˆ†å¸ƒã€‚æ¶Œç°çš„æ¦‚å¿µèšç±»å¯ä»¥å¸®åŠ©è¯†åˆ«æ„æƒ³ä¸åˆ°çš„æ½œåœ¨äº¤äº’ï¼Œå¹¶ç”¨äºè¯†åˆ«æ¨¡å‹å­¦ä¹ è¡¨ç¤ºä¸­çš„è¶‹åŠ¿å’Œåå·®ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å°†LCAä¸å±€éƒ¨å½’å› æ–¹æ³•é›†æˆï¼Œä»¥äº§ç”ŸåŸºäºæ¦‚å¿µçš„è§£é‡Šï¼Œä»è€Œæé«˜tokençº§åˆ«æ˜¾è‘—æ€§çš„è¿è´¯æ€§å’Œå¯è§£é‡Šæ€§ã€‚è·¨å¤šä¸ªæ¨¡å‹å’Œä»»åŠ¡çš„ç»éªŒè¯„ä¼°è¡¨æ˜ï¼ŒLCAå‘ç°çš„æ¦‚å¿µåœ¨è¯­ä¹‰ä¿æŒæ‰°åŠ¨ä¸‹ä¿æŒç¨³å®šï¼ˆå¹³å‡èšç±»æ•æ„Ÿåº¦æŒ‡æ•°ï¼ŒCSI = 0.288ï¼‰ï¼Œå¹¶ä¸”éšç€å¾®è°ƒå¯é¢„æµ‹åœ°æ¼”å˜ã€‚åœ¨ä¸€é¡¹å…³äºç¼–ç¨‹è¯­è¨€åˆ†ç±»ä»»åŠ¡çš„ç”¨æˆ·ç ”ç©¶ä¸­ï¼Œä¸ä½¿ç”¨ç§¯åˆ†æ¢¯åº¦çš„tokençº§åˆ«å½’å› ç›¸æ¯”ï¼Œæ¦‚å¿µå¢å¼ºçš„è§£é‡Šæ¶ˆé™¤äº†tokenè§’è‰²çš„æ­§ä¹‰ï¼Œå¹¶å°†ä»¥äººä¸ºä¸­å¿ƒçš„å¯è§£é‡Šæ€§æé«˜äº†37ä¸ªç™¾åˆ†ç‚¹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰ä»£ç è¯­è¨€æ¨¡å‹çš„å¯è§£é‡Šæ€§ä¸è¶³ï¼Œéš¾ä»¥ç†è§£æ¨¡å‹å†…éƒ¨å¦‚ä½•è¡¨ç¤ºå’Œå¤„ç†ä»£ç çš„è¯­ä¹‰ä¿¡æ¯ã€‚è¿™é™åˆ¶äº†æ¨¡å‹åœ¨å®‰å…¨å…³é”®é¢†åŸŸçš„åº”ç”¨ï¼Œå› ä¸ºç”¨æˆ·æ— æ³•ä¿¡ä»»æ¨¡å‹çš„å†³ç­–è¿‡ç¨‹ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸å…³æ³¨äºtokençº§åˆ«çš„å½’å› ï¼Œç¼ºä¹å¯¹æ›´é«˜å±‚æ¬¡æŠ½è±¡æ¦‚å¿µçš„ç†è§£ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCoCoAçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å°†ä»£ç è¯­è¨€æ¨¡å‹ä¸­ä¸Šä¸‹æ–‡ç›¸å…³çš„tokenåµŒå…¥è¿›è¡Œèšç±»ï¼Œä»è€Œå‘ç°æ¨¡å‹å­¦ä¹ åˆ°çš„æ½œåœ¨æ¦‚å¿µã€‚è¿™äº›æ¦‚å¿µå¯ä»¥è¢«äººç±»ç†è§£å’Œè§£é‡Šï¼Œä»è€Œæé«˜æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚é€šè¿‡åˆ†æè¿™äº›æ¦‚å¿µåœ¨ä¸åŒå±‚å’Œä¸åŒä»»åŠ¡ä¸­çš„åˆ†å¸ƒï¼Œå¯ä»¥æ·±å…¥äº†è§£æ¨¡å‹çš„å†…éƒ¨å·¥ä½œæœºåˆ¶ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCoCoAæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) è·å–ä¸Šä¸‹æ–‡ç›¸å…³çš„tokenåµŒå…¥ï¼šä½¿ç”¨ä»£ç è¯­è¨€æ¨¡å‹å¯¹ä»£ç è¿›è¡Œç¼–ç ï¼Œå¾—åˆ°æ¯ä¸ªtokençš„ä¸Šä¸‹æ–‡åµŒå…¥è¡¨ç¤ºã€‚2) æ¦‚å¿µèšç±»ï¼šä½¿ç”¨èšç±»ç®—æ³•ï¼ˆå¦‚k-meansï¼‰å°†tokenåµŒå…¥èšç±»æˆä¸åŒçš„æ¦‚å¿µç»„ã€‚3) æ¦‚å¿µæ ‡æ³¨ï¼šä½¿ç”¨æ··åˆæ ‡æ³¨æµç¨‹ï¼Œç»“åˆé™æ€åˆ†æå·¥å…·å’Œpromptå·¥ç¨‹åŒ–çš„LLMï¼Œå¯¹æ¯ä¸ªæ¦‚å¿µç»„è¿›è¡Œè¯­ä¹‰æ ‡æ³¨ã€‚4) æ¦‚å¿µåˆ†æï¼šåˆ†ææ¦‚å¿µåœ¨ä¸åŒå±‚å’Œä¸åŒä»»åŠ¡ä¸­çš„åˆ†å¸ƒï¼Œä»¥åŠæ¦‚å¿µä¹‹é—´çš„å…³ç³»ã€‚5) æ¦‚å¿µå¢å¼ºçš„è§£é‡Šï¼šå°†LCAä¸å±€éƒ¨å½’å› æ–¹æ³•é›†æˆï¼Œä»¥äº§ç”ŸåŸºäºæ¦‚å¿µçš„è§£é‡Šã€‚

**å…³é”®åˆ›æ–°**ï¼šCoCoAçš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ä¸ªå…¨å±€çš„åéªŒå¯è§£é‡Šæ€§æ¡†æ¶ï¼Œèƒ½å¤Ÿè‡ªåŠ¨å‘ç°ä»£ç è¯­è¨€æ¨¡å‹ä¸­çš„æ½œåœ¨æ¦‚å¿µï¼Œå¹¶å°†å…¶ä¸äººç±»å¯ç†è§£çš„è¯­ä¹‰ä¿¡æ¯å¯¹é½ã€‚è¯¥æ¡†æ¶ç»“åˆäº†é™æ€åˆ†æå·¥å…·å’Œpromptå·¥ç¨‹åŒ–çš„LLMï¼Œå®ç°äº†å¯æ‰©å±•çš„æ¦‚å¿µæ ‡æ³¨ã€‚æ­¤å¤–ï¼ŒCoCoAè¿˜æä¾›äº†ä¸€ç§å°†æ¦‚å¿µä¿¡æ¯èå…¥åˆ°å±€éƒ¨å½’å› æ–¹æ³•ä¸­çš„æœºåˆ¶ï¼Œä»è€Œæé«˜äº†è§£é‡Šçš„è¿è´¯æ€§å’Œå¯è§£é‡Šæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šCoCoAä½¿ç”¨Transformeræ¨¡å‹ä½œä¸ºä»£ç è¯­è¨€æ¨¡å‹çš„åŸºç¡€æ¶æ„ã€‚åœ¨æ¦‚å¿µèšç±»é˜¶æ®µï¼Œå¯ä»¥ä½¿ç”¨ä¸åŒçš„èšç±»ç®—æ³•ï¼Œä¾‹å¦‚k-meansæˆ–å±‚æ¬¡èšç±»ã€‚åœ¨æ¦‚å¿µæ ‡æ³¨é˜¶æ®µï¼Œä½¿ç”¨é™æ€åˆ†æå·¥å…·æå–ä»£ç çš„å¥æ³•ä¿¡æ¯ï¼Œå¹¶ä½¿ç”¨promptå·¥ç¨‹åŒ–çš„LLMç”Ÿæˆæ¦‚å¿µçš„è¯­ä¹‰æè¿°ã€‚èšç±»æ•æ„Ÿåº¦æŒ‡æ•°ï¼ˆCSIï¼‰ç”¨äºè¯„ä¼°æ¦‚å¿µçš„ç¨³å®šæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒCoCoAå‘ç°çš„æ¦‚å¿µåœ¨è¯­ä¹‰ä¿æŒæ‰°åŠ¨ä¸‹å…·æœ‰è¾ƒé«˜çš„ç¨³å®šæ€§ï¼ˆå¹³å‡CSI = 0.288ï¼‰ï¼Œå¹¶ä¸”éšç€æ¨¡å‹å¾®è°ƒèƒ½å¤Ÿå¯é¢„æµ‹åœ°æ¼”å˜ã€‚åœ¨ä¸€é¡¹ç¼–ç¨‹è¯­è¨€åˆ†ç±»ä»»åŠ¡çš„ç”¨æˆ·ç ”ç©¶ä¸­ï¼Œæ¦‚å¿µå¢å¼ºçš„è§£é‡Šå°†ä»¥äººä¸ºä¸­å¿ƒçš„å¯è§£é‡Šæ€§æé«˜äº†37ä¸ªç™¾åˆ†ç‚¹ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„tokençº§åˆ«å½’å› æ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

CoCoAå¯åº”ç”¨äºä»£ç ç¼ºé™·æ£€æµ‹ã€ä»£ç ç”Ÿæˆã€ä»£ç ç¿»è¯‘ç­‰é¢†åŸŸã€‚é€šè¿‡ç†è§£æ¨¡å‹å†…éƒ¨çš„æ¦‚å¿µè¡¨ç¤ºï¼Œå¯ä»¥æé«˜æ¨¡å‹åœ¨è¿™äº›ä»»åŠ¡ä¸­çš„æ€§èƒ½å’Œå¯é æ€§ã€‚æ­¤å¤–ï¼ŒCoCoAè¿˜å¯ä»¥ç”¨äºè¯„ä¼°ä»£ç è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§ï¼Œä¾‹å¦‚è¯†åˆ«æ¨¡å‹æ˜¯å¦å­˜åœ¨å¯¹ç‰¹å®šç±»å‹ä»£ç çš„åè§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Interpreting the internal behavior of large language models trained on code remains a critical challenge, particularly for applications demanding trust, transparency, and semantic robustness. We propose Code Concept Analysis (CoCoA): a global post-hoc interpretability framework that uncovers emergent lexical, syntactic, and semantic structures in a code language model's representation space by clustering contextualized token embeddings into human-interpretable concept groups. We propose a hybrid annotation pipeline that combines static analysis tool-based syntactic alignment with prompt-engineered large language models (LLMs), enabling scalable labeling of latent concepts across abstraction levels. We analyse the distribution of concepts across layers and across three finetuning tasks. Emergent concept clusters can help identify unexpected latent interactions and be used to identify trends and biases within the model's learned representations. We further integrate LCA with local attribution methods to produce concept-grounded explanations, improving the coherence and interpretability of token-level saliency. Empirical evaluations across multiple models and tasks show that LCA discovers concepts that remain stable under semantic-preserving perturbations (average Cluster Sensitivity Index, CSI = 0.288) and evolve predictably with fine-tuning. In a user study on the programming-language classification task, concept-augmented explanations disambiguated token roles and improved human-centric explainability by 37 percentage points compared with token-level attributions using Integrated Gradients.

