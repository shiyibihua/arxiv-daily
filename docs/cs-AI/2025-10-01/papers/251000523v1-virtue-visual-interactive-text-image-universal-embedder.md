---
layout: default
title: VIRTUE: Visual-Interactive Text-Image Universal Embedder
---

# VIRTUE: Visual-Interactive Text-Image Universal Embedder

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.00523" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.00523v1</a>
  <a href="https://arxiv.org/pdf/2510.00523.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.00523v1" onclick="toggleFavorite(this, '2510.00523v1', 'VIRTUE: Visual-Interactive Text-Image Universal Embedder')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Wei-Yao Wang, Kazuya Tateishi, Qiyu Wu, Shusuke Takahashi, Yuki Mitsufuji

**åˆ†ç±»**: cs.AI, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-01

**å¤‡æ³¨**: 25 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVIRTUEï¼šä¸€ç§è§†è§‰äº¤äº’å¼æ–‡æœ¬-å›¾åƒé€šç”¨åµŒå…¥æ¨¡å‹ï¼Œæå‡å¤šæ¨¡æ€è¡¨å¾èƒ½åŠ›ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰äº¤äº’` `æ–‡æœ¬å›¾åƒåµŒå…¥` `å¤šæ¨¡æ€å­¦ä¹ ` `åˆ†å‰²æ¨¡å‹` `è§†è§‰è¯­è¨€æ¨¡å‹` `è¡¨å¾å­¦ä¹ ` `äººæœºäº¤äº’`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åµŒå…¥æ¨¡å‹ç¼ºä¹è§†è§‰äº¤äº’èƒ½åŠ›ï¼Œæ— æ³•æ ¹æ®ç”¨æˆ·æŒ‡å®šçš„å›¾åƒåŒºåŸŸè¿›è¡Œç²¾ç¡®åµŒå…¥ï¼Œé™åˆ¶äº†å…¶åœ¨äººæœºäº¤äº’åœºæ™¯ä¸­çš„åº”ç”¨ã€‚
2. VIRTUEé€šè¿‡é›†æˆåˆ†å‰²æ¨¡å‹å’Œè§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œä½¿åµŒå…¥å™¨èƒ½å¤Ÿå¤„ç†è§†è§‰æç¤ºï¼Œä»è€Œç²¾ç¡®å®šä½å›¾åƒä¸­çš„ç‰¹å®šåŒºåŸŸï¼Œæå‡è¡¨å¾å­¦ä¹ èƒ½åŠ›ã€‚
3. åœ¨åŒ…å«100ä¸‡æ ·æœ¬çš„SCaRåŸºå‡†æµ‹è¯•ä¸­ï¼ŒVIRTUEåœ¨å¤šä¸ªä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¯æ˜äº†å…¶è§†è§‰äº¤äº’èƒ½åŠ›çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€è¡¨å¾å­¦ä¹ æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„é›†æˆè¿›ä¸€æ­¥ä½¿åµŒå…¥æ¨¡å‹å…·å¤‡äº†æŒ‡ä»¤è·Ÿéšèƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åµŒå…¥æ¨¡å‹ç¼ºä¹è§†è§‰äº¤äº’èƒ½åŠ›ï¼Œæ— æ³•æŒ‡å®šç”¨æˆ·æ„Ÿå…´è¶£çš„åŒºåŸŸï¼ˆä¾‹å¦‚ï¼Œç‚¹ã€è¾¹ç•Œæ¡†ã€æ©ç ï¼‰ï¼Œè€Œè¿™ç§èƒ½åŠ›å·²ç»åœ¨ç”Ÿæˆæ¨¡å‹ä¸­å¾—åˆ°æ¢ç´¢ï¼Œä»¥æ‰©å±•å…¶äººæœºäº¤äº’é€‚ç”¨æ€§ã€‚ä¸ºåµŒå…¥æ¨¡å‹é…å¤‡è§†è§‰äº¤äº’èƒ½åŠ›ä¸ä»…å¯ä»¥è§£é”æ–°çš„åº”ç”¨ï¼Œå®ç°ç”¨æˆ·æ„å›¾çš„å±€éƒ¨åŒ–å®šä½ï¼Œè¿™ä»ç„¶æ˜¯ä¸€ä¸ªæœªè¢«æ¢ç´¢çš„é¢†åŸŸï¼Œè¿˜å¯ä»¥ä½¿æ¨¡å‹å­¦ä¹ å›¾åƒä¸­çš„å®ä½“çº§åˆ«ä¿¡æ¯ï¼Œä»¥è¡¥å……å…¶ç”¨äºä¼ ç»ŸåµŒå…¥ä»»åŠ¡çš„å…¨å±€è¡¨å¾ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è§†è§‰äº¤äº’å¼æ–‡æœ¬-å›¾åƒé€šç”¨åµŒå…¥å™¨ï¼ˆVIRTUEï¼‰ï¼Œå®ƒå°†åˆ†å‰²æ¨¡å‹å’Œè§†è§‰-è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›æ‰©å±•åˆ°è¡¨å¾å­¦ä¹ é¢†åŸŸã€‚åœ¨VIRTUEä¸­ï¼Œåˆ†å‰²æ¨¡å‹å¯ä»¥å¤„ç†è§†è§‰æç¤ºï¼Œç²¾ç¡®å®šä½å›¾åƒä¸­çš„ç‰¹å®šåŒºåŸŸï¼Œä»è€Œä½¿åµŒå…¥å™¨èƒ½å¤Ÿæ›´ç²¾ç¡®åœ°å¤„ç†å¤æ‚å’Œæ¨¡ç³Šçš„åœºæ™¯ã€‚ä¸ºäº†è¯„ä¼°VIRTUEçš„è§†è§‰äº¤äº’èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå¤§è§„æ¨¡çš„åˆ†å‰²å’Œåœºæ™¯å­—å¹•æ£€ç´¢ï¼ˆSCaRï¼‰åŸºå‡†ï¼ŒåŒ…å«100ä¸‡ä¸ªæ ·æœ¬ï¼Œæ—¨åœ¨é€šè¿‡è”åˆè€ƒè™‘å…·æœ‰ç‰¹å®šå¯¹è±¡å’Œå›¾åƒåœºæ™¯çš„å®ä½“æ¥æ£€ç´¢æ–‡æœ¬å­—å¹•ã€‚VIRTUEåœ¨36ä¸ªé€šç”¨MMEBä»»åŠ¡ï¼ˆ3.1%-8.5%ï¼‰å’Œ5ä¸ªè§†è§‰äº¤äº’å¼SCaRä»»åŠ¡ï¼ˆ15.2%-20.3%ï¼‰ä¸­å§‹ç»ˆå¦‚ä¸€åœ°å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ–‡æœ¬-å›¾åƒåµŒå…¥æ¨¡å‹ä¸»è¦å…³æ³¨å…¨å±€å›¾åƒç‰¹å¾ï¼Œç¼ºä¹å¯¹ç”¨æˆ·æŒ‡å®šå›¾åƒåŒºåŸŸçš„äº¤äº’èƒ½åŠ›ã€‚è¿™é™åˆ¶äº†æ¨¡å‹åœ¨éœ€è¦ç²¾ç»†åŒ–ç†è§£å’Œå®šä½ç”¨æˆ·æ„å›¾çš„åœºæ™¯ä¸‹çš„åº”ç”¨ï¼Œä¾‹å¦‚ï¼Œæ ¹æ®ç”¨æˆ·ç‚¹å‡»çš„ç‰©ä½“æ£€ç´¢ç›¸å…³æ–‡æœ¬æè¿°ã€‚ç°æœ‰æ–¹æ³•æ— æ³•æœ‰æ•ˆåˆ©ç”¨è§†è§‰æç¤ºä¿¡æ¯ï¼Œå¯¼è‡´åœ¨å¤æ‚åœºæ™¯ä¸‹è¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šVIRTUEçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†åˆ†å‰²æ¨¡å‹ä¸è§†è§‰-è¯­è¨€æ¨¡å‹ç›¸ç»“åˆï¼Œåˆ©ç”¨åˆ†å‰²æ¨¡å‹å¤„ç†è§†è§‰æç¤ºï¼Œä»è€Œç²¾ç¡®å®šä½å›¾åƒä¸­çš„ç‰¹å®šåŒºåŸŸã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹å¯ä»¥å­¦ä¹ åˆ°æ›´ç»†ç²’åº¦çš„å®ä½“çº§åˆ«ä¿¡æ¯ï¼Œå¹¶å°†å…¶èå…¥åˆ°å…¨å±€è¡¨å¾ä¸­ï¼Œä»è€Œæå‡åµŒå…¥çš„å‡†ç¡®æ€§å’Œäº¤äº’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVIRTUEåŒ…å«ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šè§†è§‰æç¤ºå¤„ç†æ¨¡å—ã€å›¾åƒç¼–ç æ¨¡å—å’Œæ–‡æœ¬ç¼–ç æ¨¡å—ã€‚è§†è§‰æç¤ºå¤„ç†æ¨¡å—åˆ©ç”¨åˆ†å‰²æ¨¡å‹å¤„ç†ç”¨æˆ·æä¾›çš„è§†è§‰æç¤ºï¼ˆå¦‚ç‚¹ã€è¾¹ç•Œæ¡†ã€æ©ç ï¼‰ï¼Œæå–æ„Ÿå…´è¶£åŒºåŸŸçš„ç‰¹å¾ã€‚å›¾åƒç¼–ç æ¨¡å—è´Ÿè´£æå–å…¨å±€å›¾åƒç‰¹å¾ã€‚æ–‡æœ¬ç¼–ç æ¨¡å—è´Ÿè´£æå–æ–‡æœ¬æè¿°çš„ç‰¹å¾ã€‚æœ€ç»ˆï¼Œæ¨¡å‹å°†è§†è§‰æç¤ºç‰¹å¾ã€å…¨å±€å›¾åƒç‰¹å¾å’Œæ–‡æœ¬ç‰¹å¾èåˆï¼Œç”Ÿæˆæœ€ç»ˆçš„åµŒå…¥å‘é‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šVIRTUEçš„å…³é”®åˆ›æ–°åœ¨äºå°†åˆ†å‰²æ¨¡å‹å¼•å…¥åˆ°æ–‡æœ¬-å›¾åƒåµŒå…¥æ¡†æ¶ä¸­ï¼Œä½¿å…¶å…·å¤‡äº†è§†è§‰äº¤äº’èƒ½åŠ›ã€‚è¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ ¹æ®ç”¨æˆ·æŒ‡å®šçš„å›¾åƒåŒºåŸŸè¿›è¡Œç²¾ç¡®åµŒå…¥ï¼Œä»è€Œæ›´å¥½åœ°ç†è§£ç”¨æˆ·æ„å›¾ã€‚æ­¤å¤–ï¼ŒVIRTUEè¿˜å¼•å…¥äº†ä¸€ä¸ªå¤§è§„æ¨¡çš„åˆ†å‰²å’Œåœºæ™¯å­—å¹•æ£€ç´¢ï¼ˆSCaRï¼‰åŸºå‡†ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹çš„è§†è§‰äº¤äº’èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šVIRTUEä½¿ç”¨äº†é¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€æ¨¡å‹ä½œä¸ºåŸºç¡€æ¶æ„ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œäº†å¾®è°ƒã€‚åˆ†å‰²æ¨¡å‹é‡‡ç”¨äº†Mask2Formerã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬å¯¹æ¯”æŸå¤±å’Œäº¤å‰ç†µæŸå¤±ï¼Œç”¨äºä¼˜åŒ–åµŒå…¥å‘é‡çš„ç›¸ä¼¼æ€§å’Œåˆ†ç±»æ€§èƒ½ã€‚è§†è§‰æç¤ºçš„ç¼–ç æ–¹å¼é‡‡ç”¨äº†RoI Alignã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

VIRTUEåœ¨36ä¸ªé€šç”¨MMEBä»»åŠ¡ä¸Šå–å¾—äº†3.1%-8.5%çš„æ€§èƒ½æå‡ï¼Œåœ¨5ä¸ªè§†è§‰äº¤äº’å¼SCaRä»»åŠ¡ä¸Šå–å¾—äº†15.2%-20.3%çš„æ˜¾è‘—æå‡ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒVIRTUEçš„è§†è§‰äº¤äº’èƒ½åŠ›èƒ½å¤Ÿæœ‰æ•ˆæå‡å¤šæ¨¡æ€è¡¨å¾å­¦ä¹ çš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦ç²¾ç»†åŒ–ç†è§£å’Œå®šä½ç”¨æˆ·æ„å›¾çš„åœºæ™¯ä¸‹ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

VIRTUEå¯åº”ç”¨äºå›¾åƒæ£€ç´¢ã€è§†è§‰é—®ç­”ã€äººæœºäº¤äº’ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨ç”µå•†åœºæ™¯ä¸­ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡ç‚¹å‡»å•†å“å›¾ç‰‡ä¸­çš„ç‰¹å®šåŒºåŸŸæ¥æœç´¢ç›¸ä¼¼å•†å“ï¼›åœ¨æ™ºèƒ½å®¢æœåœºæ™¯ä¸­ï¼Œå¯ä»¥é€šè¿‡è§†è§‰æç¤ºå¼•å¯¼æ¨¡å‹ç†è§£ç”¨æˆ·æ„å›¾ï¼Œä»è€Œæä¾›æ›´å‡†ç¡®çš„ç­”æ¡ˆã€‚è¯¥ç ”ç©¶æœ‰æœ›æ¨åŠ¨å¤šæ¨¡æ€äº¤äº’å¼äººå·¥æ™ºèƒ½çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal representation learning models have demonstrated successful operation across complex tasks, and the integration of vision-language models (VLMs) has further enabled embedding models with instruction-following capabilities. However, existing embedding models lack visual-interactive capabilities to specify regions of interest from users (e.g., point, bounding box, mask), which have been explored in generative models to broaden their human-interactive applicability. Equipping embedding models with visual interactions not only would unlock new applications with localized grounding of user intent, which remains unexplored, but also enable the models to learn entity-level information within images to complement their global representations for conventional embedding tasks. In this paper, we propose a novel Visual-InteRactive Text-Image Universal Embedder (VIRTUE) that extends the capabilities of the segmentation model and the vision-language model to the realm of representation learning. In VIRTUE, the segmentation model can process visual prompts that pinpoint specific regions within an image, thereby enabling the embedder to handle complex and ambiguous scenarios more precisely. To evaluate the visual-interaction ability of VIRTUE, we introduce a large-scale Segmentation-and-Scene Caption Retrieval (SCaR) benchmark comprising 1M samples that aims to retrieve the text caption by jointly considering the entity with a specific object and image scene. VIRTUE consistently achieves a state-of-the-art performance with significant improvements across 36 universal MMEB (3.1%-8.5%) and five visual-interactive SCaR (15.2%-20.3%) tasks.

