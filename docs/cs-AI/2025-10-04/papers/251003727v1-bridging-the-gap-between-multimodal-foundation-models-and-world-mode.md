---
layout: default
title: Bridging the Gap Between Multimodal Foundation Models and World Models
---

# Bridging the Gap Between Multimodal Foundation Models and World Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.03727" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.03727v1</a>
  <a href="https://arxiv.org/pdf/2510.03727.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.03727v1" onclick="toggleFavorite(this, '2510.03727v1', 'Bridging the Gap Between Multimodal Foundation Models and World Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xuehai He

**åˆ†ç±»**: cs.AI, cs.CL, cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-10-04

**å¤‡æ³¨**: PhD thesis

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**å¼¥åˆå¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ä¸ä¸–ç•Œæ¨¡å‹ä¹‹é—´çš„å·®è·ï¼Œæå‡æ¨ç†ä¸ç”Ÿæˆèƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `ä¸–ç•Œæ¨¡å‹` `å› æœæ¨ç†` `åäº‹å®æ¨ç†` `æ—¶ç©ºæ¨ç†` `å¯æ§ç”Ÿæˆ` `4Dç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ç¼ºä¹åäº‹å®æ¨ç†ã€åŠ¨æ€æ¨¡æ‹Ÿå’Œæ—¶ç©ºç†è§£ç­‰èƒ½åŠ›ï¼Œéš¾ä»¥èƒœä»»ä¸–ç•Œæ¨¡å‹çš„è§’è‰²ã€‚
2. æœ¬æ–‡é€šè¿‡æå‡æ¨ç†èƒ½åŠ›å’Œå¼•å…¥ç»“æ„åŒ–æ¨ç†æŠ€èƒ½ï¼Œå¢å¼ºå¤šæ¨¡æ€åŸºç¡€æ¨¡å‹å¯¹è§†è§‰å’Œæ–‡æœ¬æ•°æ®æ·±å±‚å…³ç³»çš„ç†è§£ã€‚
3. æœ¬æ–‡æå‡ºäº†æ–°çš„æ¡†æ¶ï¼Œé€šè¿‡åœºæ™¯å›¾ã€å¤šæ¨¡æ€æ¡ä»¶å’Œå¯¹é½ç­–ç•¥ï¼Œå®ç°ç»“æ„åŒ–å’Œå¯æ§çš„å›¾åƒã€è§†é¢‘ä»¥åŠ4Då†…å®¹ç”Ÿæˆã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æ—¨åœ¨å¼¥åˆå¤šæ¨¡æ€åŸºç¡€æ¨¡å‹(MFMs)ä¸ä¸–ç•Œæ¨¡å‹ä¹‹é—´çš„å·®è·ã€‚å½“å‰MFMsåœ¨åäº‹å®æ¨ç†ã€åŠ¨æ€æ¨¡æ‹Ÿã€æ—¶ç©ºä¿¡æ¯ç†è§£ã€å¯æ§è§†è§‰ç»“æœç”Ÿæˆä»¥åŠå¤šæ–¹é¢æ¨ç†ç­‰å…³é”®èƒ½åŠ›ä¸Šå­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥ä½œä¸ºæœ‰æ•ˆçš„ä¸–ç•Œæ¨¡å‹ã€‚æœ¬æ–‡é€šè¿‡åˆ¤åˆ«ä»»åŠ¡æå‡MFMsçš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶èµ‹äºˆå…¶ç»“æ„åŒ–æ¨ç†æŠ€èƒ½ï¼Œå¦‚å› æœæ¨æ–­ã€åäº‹å®æ€è€ƒå’Œæ—¶ç©ºæ¨ç†ï¼Œä½¿å…¶èƒ½å¤Ÿè¶…è¶Šè¡¨é¢ç›¸å…³æ€§ï¼Œç†è§£è§†è§‰å’Œæ–‡æœ¬æ•°æ®ä¸­æ›´æ·±å±‚æ¬¡çš„å…³ç³»ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ¢ç´¢äº†MFMsåœ¨å›¾åƒå’Œè§†é¢‘æ¨¡æ€ä¸­çš„ç”Ÿæˆèƒ½åŠ›ï¼Œå¼•å…¥äº†ç”¨äºç»“æ„åŒ–å’Œå¯æ§ç”Ÿæˆçš„æ–°æ¡†æ¶ï¼Œç»“åˆåœºæ™¯å›¾ã€å¤šæ¨¡æ€æ¡ä»¶å’Œå¤šæ¨¡æ€å¯¹é½ç­–ç•¥æ¥æŒ‡å¯¼ç”Ÿæˆè¿‡ç¨‹ï¼Œç¡®ä¿ä¸é«˜å±‚è¯­ä¹‰å’Œç»†ç²’åº¦ç”¨æˆ·æ„å›¾çš„ä¸€è‡´æ€§ã€‚æœ€åï¼Œå°†è¿™äº›æŠ€æœ¯æ‰©å±•åˆ°å¯æ§çš„4Dç”Ÿæˆï¼Œå®ç°éšæ—¶é—´å’Œç©ºé—´å˜åŒ–çš„äº¤äº’å¼ã€å¯ç¼–è¾‘å’Œå¯å˜å½¢çš„å¯¹è±¡åˆæˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå½“å‰çš„å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ï¼ˆMFMsï¼‰è™½ç„¶åœ¨å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬ç¼ºä¹ä¸–ç•Œæ¨¡å‹æ‰€å¿…éœ€çš„å…³é”®èƒ½åŠ›ï¼Œä¾‹å¦‚è¿›è¡Œåäº‹å®æ¨ç†ã€æ¨¡æ‹ŸåŠ¨æ€è¿‡ç¨‹ã€ç†è§£æ—¶ç©ºä¿¡æ¯ã€æ§åˆ¶ç”Ÿæˆè§†è§‰ç»“æœä»¥åŠæ‰§è¡Œå¤šæ–¹é¢çš„æ¨ç†ã€‚ç°æœ‰çš„MFMsä¸»è¦å…³æ³¨è¡¨é¢ç›¸å…³æ€§ï¼Œéš¾ä»¥æ•æ‰æ•°æ®ä¸­æ›´æ·±å±‚æ¬¡çš„å› æœå…³ç³»å’ŒåŠ¨æ€å˜åŒ–ï¼Œé™åˆ¶äº†å…¶åœ¨å¤æ‚ç¯å¢ƒä¸­çš„åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¢å¼ºMFMsçš„æ¨ç†èƒ½åŠ›å’Œç”Ÿæˆèƒ½åŠ›ï¼Œä½¿å…¶æ›´æ¥è¿‘ä¸–ç•Œæ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œé¦–å…ˆé€šè¿‡åˆ¤åˆ«ä»»åŠ¡å’Œç»“æ„åŒ–æ¨ç†æŠ€èƒ½ï¼ˆå¦‚å› æœæ¨æ–­ã€åäº‹å®æ€è€ƒå’Œæ—¶ç©ºæ¨ç†ï¼‰æ¥æå‡MFMsçš„æ¨ç†èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿç†è§£è§†è§‰å’Œæ–‡æœ¬æ•°æ®ä¸­æ›´æ·±å±‚æ¬¡çš„å…³ç³»ã€‚ç„¶åï¼Œé€šè¿‡å¼•å…¥æ–°çš„æ¡†æ¶ï¼Œç»“åˆåœºæ™¯å›¾ã€å¤šæ¨¡æ€æ¡ä»¶å’Œå¤šæ¨¡æ€å¯¹é½ç­–ç•¥ï¼Œå®ç°ç»“æ„åŒ–å’Œå¯æ§çš„å›¾åƒã€è§†é¢‘ä»¥åŠ4Då†…å®¹ç”Ÿæˆã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæœ¬æ–‡æå‡ºçš„æ¡†æ¶ä¸»è¦åŒ…å«ä¸¤ä¸ªéƒ¨åˆ†ï¼šæ¨ç†èƒ½åŠ›å¢å¼ºå’Œç”Ÿæˆèƒ½åŠ›å¢å¼ºã€‚æ¨ç†èƒ½åŠ›å¢å¼ºéƒ¨åˆ†ä¸»è¦é€šè¿‡è®­ç»ƒMFMsæ‰§è¡Œåˆ¤åˆ«ä»»åŠ¡ï¼Œå¹¶èµ‹äºˆå…¶ç»“æ„åŒ–æ¨ç†æŠ€èƒ½æ¥å®ç°ã€‚ç”Ÿæˆèƒ½åŠ›å¢å¼ºéƒ¨åˆ†åˆ™é€šè¿‡å¼•å…¥æ–°çš„ç”Ÿæˆæ¡†æ¶ï¼Œç»“åˆåœºæ™¯å›¾ã€å¤šæ¨¡æ€æ¡ä»¶å’Œå¤šæ¨¡æ€å¯¹é½ç­–ç•¥æ¥æŒ‡å¯¼ç”Ÿæˆè¿‡ç¨‹ã€‚å¯¹äº4Dç”Ÿæˆï¼Œåˆ™æ˜¯åœ¨æ­¤åŸºç¡€ä¸Šè¿›ä¸€æ­¥æ‰©å±•ï¼Œå®ç°éšæ—¶é—´å’Œç©ºé—´å˜åŒ–çš„äº¤äº’å¼ã€å¯ç¼–è¾‘å’Œå¯å˜å½¢çš„å¯¹è±¡åˆæˆã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†ç»“æ„åŒ–æ¨ç†æŠ€èƒ½å¼•å…¥åˆ°å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ä¸­ï¼Œä½¿å…¶èƒ½å¤Ÿè¶…è¶Šè¡¨é¢ç›¸å…³æ€§ï¼Œç†è§£æ•°æ®ä¸­æ›´æ·±å±‚æ¬¡çš„å› æœå…³ç³»å’ŒåŠ¨æ€å˜åŒ–ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†æ–°çš„ç”Ÿæˆæ¡†æ¶ï¼Œé€šè¿‡ç»“åˆåœºæ™¯å›¾ã€å¤šæ¨¡æ€æ¡ä»¶å’Œå¤šæ¨¡æ€å¯¹é½ç­–ç•¥ï¼Œå®ç°äº†ç»“æ„åŒ–å’Œå¯æ§çš„å›¾åƒã€è§†é¢‘ä»¥åŠ4Då†…å®¹ç”Ÿæˆã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨ç†èƒ½åŠ›å¢å¼ºæ–¹é¢ï¼Œå…³é”®åœ¨äºè®¾è®¡åˆé€‚çš„åˆ¤åˆ«ä»»åŠ¡å’Œç»“æ„åŒ–æ¨ç†æŠ€èƒ½è®­ç»ƒæ–¹æ³•ã€‚åœ¨ç”Ÿæˆèƒ½åŠ›å¢å¼ºæ–¹é¢ï¼Œå…³é”®åœ¨äºå¦‚ä½•æœ‰æ•ˆåœ°ç»“åˆåœºæ™¯å›¾ã€å¤šæ¨¡æ€æ¡ä»¶å’Œå¤šæ¨¡æ€å¯¹é½ç­–ç•¥ï¼Œä»¥åŠå¦‚ä½•å°†è¿™äº›æŠ€æœ¯æ‰©å±•åˆ°4Dç”Ÿæˆã€‚å…·ä½“çš„å‚æ•°è®¾ç½®ã€æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç­‰æŠ€æœ¯ç»†èŠ‚åœ¨è®ºæ–‡ä¸­åº”è¯¥æœ‰æ›´è¯¦ç»†çš„æè¿°ï¼ˆæœªçŸ¥ï¼‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

ç”±äºè®ºæ–‡æ‘˜è¦æœªæä¾›å…·ä½“çš„å®éªŒç»“æœï¼Œå› æ­¤å®éªŒäº®ç‚¹æœªçŸ¥ã€‚ä½†å¯ä»¥æ¨æµ‹ï¼Œå®éªŒéƒ¨åˆ†åº”è¯¥ä¼šå±•ç¤ºåœ¨å„ç§æ¨ç†ä»»åŠ¡å’Œç”Ÿæˆä»»åŠ¡ä¸Šçš„æ€§èƒ½æå‡ï¼Œå¹¶ä¸å…¶ä»–åŸºçº¿æ¨¡å‹è¿›è¡Œå¯¹æ¯”ï¼Œä»¥éªŒè¯æœ¬æ–‡æå‡ºçš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ï¼ˆå…·ä½“æ•°æ®æœªçŸ¥ï¼‰

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºæœºå™¨äººã€è‡ªåŠ¨é©¾é©¶ã€è™šæ‹Ÿç°å®ã€æ¸¸æˆå¼€å‘ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨æœºå™¨äººé¢†åŸŸï¼Œå¢å¼ºåçš„MFMså¯ä»¥å¸®åŠ©æœºå™¨äººæ›´å¥½åœ°ç†è§£å‘¨å›´ç¯å¢ƒï¼Œè¿›è¡Œæ›´æ™ºèƒ½çš„å†³ç­–å’Œæ§åˆ¶ã€‚åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸï¼Œå¯ä»¥æé«˜è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿå¯¹å¤æ‚äº¤é€šåœºæ™¯çš„ç†è§£å’Œé¢„æµ‹èƒ½åŠ›ã€‚åœ¨è™šæ‹Ÿç°å®å’Œæ¸¸æˆå¼€å‘é¢†åŸŸï¼Œå¯ä»¥ç”Ÿæˆæ›´é€¼çœŸã€æ›´å¯æ§çš„è™šæ‹Ÿå†…å®¹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Humans understand the world through the integration of multiple sensory modalities, enabling them to perceive, reason about, and imagine dynamic physical processes. Inspired by this capability, multimodal foundation models (MFMs) have emerged as powerful tools for multimodal understanding and generation. However, today's MFMs fall short of serving as effective world models. They lack the essential ability such as perform counterfactual reasoning, simulate dynamics, understand the spatiotemporal information, control generated visual outcomes, and perform multifaceted reasoning. We investigates what it takes to bridge the gap between multimodal foundation models and world models. We begin by improving the reasoning capabilities of MFMs through discriminative tasks and equipping MFMs with structured reasoning skills, such as causal inference, counterfactual thinking, and spatiotemporal reasoning, enabling them to go beyond surface correlations and understand deeper relationships within visual and textual data. Next, we explore generative capabilities of multimodal foundation models across both image and video modalities, introducing new frameworks for structured and controllable generation. Our approaches incorporate scene graphs, multimodal conditioning, and multimodal alignment strategies to guide the generation process, ensuring consistency with high-level semantics and fine-grained user intent. We further extend these techniques to controllable 4D generation, enabling interactive, editable, and morphable object synthesis over time and space.

