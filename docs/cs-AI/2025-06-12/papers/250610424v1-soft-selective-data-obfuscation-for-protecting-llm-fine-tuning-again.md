---
layout: default
title: SOFT: Selective Data Obfuscation for Protecting LLM Fine-tuning against Membership Inference Attacks
---

# SOFT: Selective Data Obfuscation for Protecting LLM Fine-tuning against Membership Inference Attacks

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.10424" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.10424v1</a>
  <a href="https://arxiv.org/pdf/2506.10424.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.10424v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.10424v1', 'SOFT: Selective Data Obfuscation for Protecting LLM Fine-tuning against Membership Inference Attacks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kaiyuan Zhang, Siyuan Cheng, Hanxi Guo, Yuetian Chen, Zian Su, Shengwei An, Yuntao Du, Charles Fleming, Ashish Kundu, Xiangyu Zhang, Ninghui Li

**åˆ†ç±»**: cs.CR, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-12

**å¤‡æ³¨**: Accepted by the 34th USENIX Security Symposium 2025. Code is available at https://github.com/KaiyuanZh/SOFT

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSOFTä»¥è§£å†³LLMå¾®è°ƒä¸­çš„æˆå‘˜æ¨æ–­æ”»å‡»é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `éšç§ä¿æŠ¤` `æˆå‘˜æ¨æ–­æ”»å‡»` `æ•°æ®æ··æ·†` `å¾®è°ƒæŠ€æœ¯` `æœºå™¨å­¦ä¹ ` `å®‰å…¨æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¾®è°ƒæ–¹æ³•åœ¨å¤„ç†ç§å¯†ä¿¡æ¯æ—¶é¢ä¸´æˆå‘˜æ¨æ–­æ”»å‡»çš„ä¸¥é‡éšç§é£é™©ï¼Œå¯¼è‡´æ•æ„Ÿä¿¡æ¯æ³„éœ²ã€‚
2. æœ¬æ–‡æå‡ºSOFTï¼Œé€šè¿‡é€‰æ‹©æ€§æ•°æ®æ··æ·†æ¥é™ä½éšç§æ³„éœ²é£é™©ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹æ€§èƒ½ï¼Œå…·æœ‰å¯è°ƒå‚æ•°ä»¥ä¼˜åŒ–éšç§ä¸æ•ˆç”¨çš„å¹³è¡¡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSOFTåœ¨å¤šä¸ªé¢†åŸŸå’ŒLLMæ¶æ„ä¸­æœ‰æ•ˆé™ä½äº†éšç§é£é™©ï¼Œä¸”æ¨¡å‹æ€§èƒ½ä¿æŒåœ¨ç«äº‰æ°´å¹³ï¼ŒéªŒè¯äº†å…¶å®ç”¨æ€§å’Œå¯æ‰©å±•æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šä¸ªåº”ç”¨ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†å…¶å¾®è°ƒè¿‡ç¨‹å¸¸æ¶‰åŠç§å¯†ä¿¡æ¯ï¼Œå¸¦æ¥äº†ä¸¥é‡çš„éšç§é—®é¢˜ã€‚æœ¬æ–‡é¦–æ¬¡å…¨é¢è¯„ä¼°äº†å¾®è°ƒLLMså¯¹æˆå‘˜æ¨æ–­æ”»å‡»ï¼ˆMIAsï¼‰çš„è„†å¼±æ€§ï¼Œå‘ç°MIAsé€šè¿‡åˆ©ç”¨å¾®è°ƒè¿‡ç¨‹ä¸­çš„æŸå¤±å‡å°‘ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ­ç¤ºæˆå‘˜ä¿¡æ¯ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†SOFTï¼ˆé€‰æ‹©æ€§æ•°æ®æ··æ·†ï¼‰ï¼Œä¸€ç§æ–°é¢–çš„é˜²å¾¡æŠ€æœ¯ï¼Œé€šè¿‡å½±å“æ•°æ®é€‰æ‹©å¹¶è°ƒæ•´å‚æ•°ï¼Œå¹³è¡¡æ•ˆç”¨ä¿æŒä¸éšç§ä¿æŠ¤ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSOFTåœ¨å…­ä¸ªä¸åŒé¢†åŸŸå’Œå¤šç§LLMæ¶æ„ä¸­æœ‰æ•ˆé™ä½äº†éšç§é£é™©ï¼ŒåŒæ—¶ä¿æŒäº†ç«äº‰åŠ›çš„æ¨¡å‹æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹æ—¶é¢ä¸´çš„æˆå‘˜æ¨æ–­æ”»å‡»é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨ä¿æŠ¤éšç§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå®¹æ˜“è¢«æ”»å‡»è€…åˆ©ç”¨æŸå¤±ä¿¡æ¯è¿›è¡Œæˆå‘˜èº«ä»½æ¨æ–­ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSOFTçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡é€‰æ‹©æ€§æ•°æ®æ··æ·†æ¥é™ä½éšç§æ³„éœ²é£é™©ã€‚è¯¥æ–¹æ³•é€šè¿‡å½±å“æ•°æ®é€‰æ‹©ï¼Œè°ƒæ•´å‚æ•°ä»¥å®ç°éšç§ä¿æŠ¤ä¸æ¨¡å‹æ•ˆç”¨ä¹‹é—´çš„å¹³è¡¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSOFTçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é€‰æ‹©æ¨¡å—ã€æ··æ·†å¤„ç†æ¨¡å—å’Œæ¨¡å‹è®­ç»ƒæ¨¡å—ã€‚æ•°æ®é€‰æ‹©æ¨¡å—æ ¹æ®å½±å“åŠ›é€‰æ‹©æ•°æ®ï¼Œæ··æ·†å¤„ç†æ¨¡å—å¯¹é€‰æ‹©çš„æ•°æ®è¿›è¡Œå¤„ç†ï¼Œæœ€åå°†æ··æ·†åçš„æ•°æ®ç”¨äºæ¨¡å‹çš„å¾®è°ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šSOFTçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶é€‰æ‹©æ€§æ•°æ®æ··æ·†æœºåˆ¶ï¼Œèƒ½å¤ŸåŠ¨æ€è°ƒæ•´æ··æ·†ç¨‹åº¦ï¼Œä»¥é€‚åº”ä¸åŒçš„éšç§ä¿æŠ¤éœ€æ±‚ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•çš„é™æ€å¤„ç†æ–¹å¼å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨SOFTä¸­ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬å½±å“åŠ›é˜ˆå€¼å’Œæ··æ·†å¼ºåº¦ï¼Œè¿™äº›å‚æ•°çš„è®¾ç½®ç›´æ¥å½±å“éšç§ä¿æŠ¤æ•ˆæœä¸æ¨¡å‹æ€§èƒ½çš„å¹³è¡¡ã€‚æ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šä¹Ÿè€ƒè™‘äº†éšç§ä¿æŠ¤çš„éœ€æ±‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSOFTåœ¨å…­ä¸ªä¸åŒé¢†åŸŸçš„å¾®è°ƒä»»åŠ¡ä¸­ï¼Œéšç§æ³„éœ²é£é™©æ˜¾è‘—é™ä½ï¼Œæ¨¡å‹æ€§èƒ½ä¿æŒåœ¨95%ä»¥ä¸Šçš„åŸºçº¿æ°´å¹³ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚ä¸æœªä½¿ç”¨SOFTçš„æ¨¡å‹ç›¸æ¯”ï¼Œéšç§ä¿æŠ¤æ•ˆæœæå‡å¹…åº¦è¾¾åˆ°30%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒ»ç–—ã€é‡‘èå’Œç¤¾äº¤åª’ä½“ç­‰æ¶‰åŠæ•æ„Ÿä¿¡æ¯çš„è¡Œä¸šã€‚é€šè¿‡æœ‰æ•ˆä¿æŠ¤å¾®è°ƒè¿‡ç¨‹ä¸­çš„éšç§ï¼ŒSOFTèƒ½å¤Ÿå¸®åŠ©ä¼ä¸šåœ¨ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ—¶ï¼Œé™ä½æ•°æ®æ³„éœ²é£é™©ï¼Œå¢å¼ºç”¨æˆ·ä¿¡ä»»ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›æ¨å¹¿è‡³æ›´å¤šéœ€è¦éšç§ä¿æŠ¤çš„AIåº”ç”¨åœºæ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have achieved remarkable success and are widely adopted for diverse applications. However, fine-tuning these models often involves private or sensitive information, raising critical privacy concerns. In this work, we conduct the first comprehensive study evaluating the vulnerability of fine-tuned LLMs to membership inference attacks (MIAs). Our empirical analysis demonstrates that MIAs exploit the loss reduction during fine-tuning, making them highly effective in revealing membership information. These findings motivate the development of our defense. We propose SOFT (\textbf{S}elective data \textbf{O}bfuscation in LLM \textbf{F}ine-\textbf{T}uning), a novel defense technique that mitigates privacy leakage by leveraging influential data selection with an adjustable parameter to balance utility preservation and privacy protection. Our extensive experiments span six diverse domains and multiple LLM architectures and scales. Results show that SOFT effectively reduces privacy risks while maintaining competitive model performance, offering a practical and scalable solution to safeguard sensitive information in fine-tuned LLMs.

