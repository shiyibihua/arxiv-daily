---
layout: default
title: RoboGPT-R1: Enhancing Robot Planning with Reinforcement Learning
---

# RoboGPT-R1: Enhancing Robot Planning with Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.14828" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.14828v2</a>
  <a href="https://arxiv.org/pdf/2510.14828.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.14828v2" onclick="toggleFavorite(this, '2510.14828v2', 'RoboGPT-R1: Enhancing Robot Planning with Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jinrui Liu, Bingyan Nie, Boyu Li, Yaran Chen, Yuze Wang, Shunsen He, Haoran Li

**åˆ†ç±»**: cs.AI, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-10-16 (æ›´æ–°: 2025-10-22)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**RoboGPT-R1ï¼šå¼ºåŒ–å­¦ä¹ å¢å¼ºæœºå™¨äººè§„åˆ’èƒ½åŠ›ï¼Œæå‡é•¿æ—¶ç¨‹æ“ä½œä»»åŠ¡æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æœºå™¨äººè§„åˆ’` `å¼ºåŒ–å­¦ä¹ ` `å…·èº«æ™ºèƒ½` `è§†è§‰è¯­è¨€æ¨¡å‹` `é•¿æ—¶ç¨‹æ“ä½œ` `ç›‘ç£å¾®è°ƒ` `å¥–åŠ±å‡½æ•°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºç›‘ç£å¾®è°ƒçš„å¤§è¯­è¨€æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤æ‚ç¯å¢ƒä¸­æ‰§è¡Œé•¿æ—¶ç¨‹æ“ä½œä»»åŠ¡æ—¶ï¼Œç”±äºå¸¸è¯†å’Œæ¨ç†èƒ½åŠ›çš„é™åˆ¶ï¼Œé¢ä¸´æŒ‘æˆ˜ã€‚
2. RoboGPT-R1é‡‡ç”¨ä¸¤é˜¶æ®µå¾®è°ƒæ¡†æ¶ï¼Œå…ˆé€šè¿‡ç›‘ç£å­¦ä¹ è·å–çŸ¥è¯†ï¼Œå†åˆ©ç”¨å¼ºåŒ–å­¦ä¹ æå‡è§†è§‰ç©ºé—´ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼Œå¼¥è¡¥ç›‘ç£å­¦ä¹ çš„ä¸è¶³ã€‚
3. å®éªŒè¡¨æ˜ï¼Œåœ¨Qwen2.5-VL-3Bä¸Šè®­ç»ƒçš„RoboGPT-R1ï¼Œåœ¨EmbodiedBenchä¸Šæ˜¾è‘—ä¼˜äºGPT-4o-miniå’ŒåŸºäºQwen2.5-VL-7Bè®­ç»ƒçš„æ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸ºäº†æé«˜å…·èº«æ™ºèƒ½ä½“åœ¨é•¿æ—¶ç¨‹æ“ä½œä»»åŠ¡ä¸­å®Œæˆå¤æ‚äººç±»æŒ‡ä»¤çš„æ¨ç†èƒ½åŠ›ï¼Œæœ¬æ–‡æå‡ºäº†RoboGPT-R1ï¼Œä¸€ä¸ªç”¨äºå…·èº«è§„åˆ’çš„ä¸¤é˜¶æ®µå¾®è°ƒæ¡†æ¶ã€‚è¯¥æ¡†æ¶é¦–å…ˆé€šè¿‡ç›‘ç£è®­ç»ƒä»ä¸“å®¶åºåˆ—ä¸­è·å–åŸºç¡€çŸ¥è¯†ï¼Œç„¶ååˆ©ç”¨å¼ºåŒ–å­¦ä¹ è§£å†³æ¨¡å‹åœ¨è§†è§‰ç©ºé—´ç†è§£å’Œæ¨ç†æ–¹é¢çš„ä¸è¶³ã€‚ä¸ºäº†åœ¨å¤šæ­¥æ¨ç†ä»»åŠ¡ä¸­å®ç°ç‰©ç†ç†è§£å’ŒåŠ¨ä½œåºåˆ—ä¸€è‡´æ€§ï¼Œè®¾è®¡äº†ä¸€ç§åŸºäºè§„åˆ™çš„å¥–åŠ±å‡½æ•°ï¼ŒåŒæ—¶è€ƒè™‘äº†é•¿æ—¶ç¨‹æ€§èƒ½å’Œç¯å¢ƒä¸­çš„åŠ¨ä½œçº¦æŸã€‚åœ¨Qwen2.5-VL-3Bä¸Šè®­ç»ƒçš„æ¨ç†æ¨¡å‹ï¼Œåœ¨EmbodiedBenchåŸºå‡†æµ‹è¯•ä¸­ï¼Œæ˜¾è‘—ä¼˜äºæ›´å¤§è§„æ¨¡çš„æ¨¡å‹GPT-4o-mini 21.33%ï¼Œå¹¶ä¸”è¶…è¿‡äº†åœ¨Qwen2.5-VL-7Bä¸Šè®­ç»ƒçš„å…¶ä»–å·¥ä½œ 20.33%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ–¹æ³•åœ¨å…·èº«æ™ºèƒ½ä½“çš„é•¿æ—¶ç¨‹æ“ä½œä»»åŠ¡ä¸­ï¼Œç”±äºç¼ºä¹è¶³å¤Ÿçš„å¸¸è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œéš¾ä»¥å¾ˆå¥½åœ°ç†è§£ç¯å¢ƒå¹¶è§„åˆ’å‡ºåˆç†çš„åŠ¨ä½œåºåˆ—ï¼Œå¯¼è‡´ä»»åŠ¡å®Œæˆæ•ˆæœä¸ä½³ã€‚ç›‘ç£å¾®è°ƒè™½ç„¶å¯ä»¥å­¦ä¹ åˆ°ä¸€äº›çŸ¥è¯†ï¼Œä½†æ³›åŒ–èƒ½åŠ›å’Œç‰©ç†ç†è§£èƒ½åŠ›ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šRoboGPT-R1çš„æ ¸å¿ƒæ€è·¯æ˜¯ç»“åˆç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ çš„ä¼˜åŠ¿ã€‚é¦–å…ˆé€šè¿‡ç›‘ç£å­¦ä¹ è®©æ¨¡å‹å­¦ä¹ åˆ°åŸºç¡€çŸ¥è¯†ï¼Œç„¶ååˆ©ç”¨å¼ºåŒ–å­¦ä¹ æ¥å¼¥è¡¥æ¨¡å‹åœ¨è§†è§‰ç©ºé—´ç†è§£å’Œæ¨ç†æ–¹é¢çš„ä¸è¶³ï¼Œä»è€Œæé«˜æ¨¡å‹åœ¨å¤æ‚ç¯å¢ƒä¸­çš„è§„åˆ’èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRoboGPT-R1æ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„å¾®è°ƒæ¡†æ¶ã€‚ç¬¬ä¸€é˜¶æ®µæ˜¯ç›‘ç£å­¦ä¹ ï¼Œä½¿ç”¨ä¸“å®¶åºåˆ—å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶å­¦ä¹ åˆ°åŸºç¡€çŸ¥è¯†ã€‚ç¬¬äºŒé˜¶æ®µæ˜¯å¼ºåŒ–å­¦ä¹ ï¼Œä½¿ç”¨åŸºäºè§„åˆ™çš„å¥–åŠ±å‡½æ•°æ¥è®­ç»ƒæ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£ç¯å¢ƒå¹¶è§„åˆ’å‡ºåˆç†çš„åŠ¨ä½œåºåˆ—ã€‚å¥–åŠ±å‡½æ•°åŒæ—¶è€ƒè™‘äº†é•¿æ—¶ç¨‹æ€§èƒ½å’Œç¯å¢ƒä¸­çš„åŠ¨ä½œçº¦æŸã€‚

**å…³é”®åˆ›æ–°**ï¼šRoboGPT-R1çš„å…³é”®åˆ›æ–°åœ¨äºç»“åˆäº†ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§åŸºäºè§„åˆ™çš„å¥–åŠ±å‡½æ•°ã€‚è¿™ç§ç»“åˆå¯ä»¥æœ‰æ•ˆåœ°æé«˜æ¨¡å‹åœ¨å¤æ‚ç¯å¢ƒä¸­çš„è§„åˆ’èƒ½åŠ›ã€‚åŸºäºè§„åˆ™çš„å¥–åŠ±å‡½æ•°èƒ½å¤ŸåŒæ—¶è€ƒè™‘é•¿æ—¶ç¨‹æ€§èƒ½å’Œç¯å¢ƒä¸­çš„åŠ¨ä½œçº¦æŸï¼Œä»è€Œä¿è¯äº†åŠ¨ä½œåºåˆ—çš„åˆç†æ€§å’Œæœ‰æ•ˆæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šå¥–åŠ±å‡½æ•°çš„è®¾è®¡æ˜¯å…³é”®ã€‚å®ƒç”±ä¸¤éƒ¨åˆ†ç»„æˆï¼šä¸€éƒ¨åˆ†æ˜¯é•¿æ—¶ç¨‹æ€§èƒ½å¥–åŠ±ï¼Œç”¨äºé¼“åŠ±æ¨¡å‹å®Œæˆä»»åŠ¡ï¼›å¦ä¸€éƒ¨åˆ†æ˜¯åŠ¨ä½œçº¦æŸå¥–åŠ±ï¼Œç”¨äºæƒ©ç½šè¿åç¯å¢ƒçº¦æŸçš„åŠ¨ä½œã€‚å…·ä½“è§„åˆ™å’Œæƒé‡è®¾ç½®æœªçŸ¥ï¼Œä½†ç›®æ ‡æ˜¯å¼•å¯¼æ¨¡å‹å­¦ä¹ åˆ°æ—¢èƒ½å®Œæˆä»»åŠ¡åˆèƒ½éµå®ˆç¯å¢ƒè§„åˆ™çš„åŠ¨ä½œåºåˆ—ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

RoboGPT-R1åœ¨EmbodiedBenchåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒæ¯”GPT-4o-miniçš„æ€§èƒ½é«˜å‡º21.33%ï¼Œå¹¶ä¸”æ¯”åœ¨Qwen2.5-VL-7Bä¸Šè®­ç»ƒçš„å…¶ä»–å·¥ä½œé«˜å‡º20.33%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒRoboGPT-R1èƒ½å¤Ÿæœ‰æ•ˆåœ°æé«˜æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„è§„åˆ’èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

RoboGPT-R1å¯åº”ç”¨äºå„ç§éœ€è¦æœºå™¨äººè¿›è¡Œé•¿æ—¶ç¨‹æ“ä½œä»»åŠ¡çš„åœºæ™¯ï¼Œä¾‹å¦‚å®¶åº­æœåŠ¡ã€å·¥ä¸šè‡ªåŠ¨åŒ–ã€åŒ»ç–—è¾…åŠ©ç­‰ã€‚é€šè¿‡æå‡æœºå™¨äººçš„æ¨ç†å’Œè§„åˆ’èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£äººç±»æŒ‡ä»¤ï¼Œå¹¶åœ¨å¤æ‚ç¯å¢ƒä¸­å®Œæˆä»»åŠ¡ï¼Œä»è€Œæé«˜å·¥ä½œæ•ˆç‡å’Œç”Ÿæ´»è´¨é‡ã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°æ›´å¤æ‚çš„ç¯å¢ƒå’Œä»»åŠ¡ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Improving the reasoning capabilities of embodied agents is crucial for robots to complete complex human instructions in long-view manipulation tasks successfully. Despite the success of large language models and vision language models based on Supervised Fine-Tuning (SFT) in planning tasks, they continue facing challenges in performing long-horizon manipulation tasks in complex real-world environments, owing to their restricted common sense and reasoning capabilities. Considering that aligning general-purpose vision language models to robotic planning tasks via supervised fine-tuning suffers from poor generalization and insufficient physical understanding, we propose RoboGPT-R1, a two-stage fine-tuning framework for embodied planning. In this framework, supervised training acquires foundational knowledge through expert sequences, followed by RL to address the model's shortcomings in visual-spatial understanding and reasoning. To achieve physical understanding and action sequence consistency in multi-step reasoning tasks, we design a rule-based reward function that simultaneously considers long-horizon performance and action constraint in the environment. The reasoning model, trained on Qwen2.5-VL-3B, significantly outperforms the larger-scale model, GPT-4o-mini, by 21.33% and surpasses other work trained on Qwen2.5-VL-7B by 20.33% on the EmbodiedBench benchmark.

