---
layout: default
title: VStyle: A Benchmark for Voice Style Adaptation with Spoken Instructions
---

# VStyle: A Benchmark for Voice Style Adaptation with Spoken Instructions

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.09716" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.09716v2</a>
  <a href="https://arxiv.org/pdf/2509.09716.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.09716v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.09716v2', 'VStyle: A Benchmark for Voice Style Adaptation with Spoken Instructions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jun Zhan, Mingyang Han, Yuxuan Xie, Chen Wang, Dong Zhang, Kexin Huang, Haoxiang Shi, DongXiao Wang, Tengtao Song, Qinyuan Cheng, Shimin Li, Jun Song, Xipeng Qiu, Bo Zheng

**åˆ†ç±»**: cs.SD, cs.AI, cs.CL, eess.AS

**å‘å¸ƒæ—¥æœŸ**: 2025-09-09 (æ›´æ–°: 2025-09-22)

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://junzhan2000.github.io/VStyle.github.io/)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**VStyleï¼šä¸€ä¸ªåŸºäºå£è¯­æŒ‡ä»¤çš„è¯­éŸ³é£æ ¼è¿ç§»è¯„æµ‹åŸºå‡†**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­éŸ³é£æ ¼è¿ç§»` `å£è¯­è¯­è¨€æ¨¡å‹` `è¯„æµ‹åŸºå‡†` `è¯­éŸ³ç”Ÿæˆ` `äººæœºäº¤äº’`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å£è¯­è¯­è¨€æ¨¡å‹åœ¨è¯­éŸ³é£æ ¼è¿ç§»æ–¹é¢èƒ½åŠ›ä¸è¶³ï¼Œéš¾ä»¥æ ¹æ®å£è¯­æŒ‡ä»¤è°ƒæ•´éŸ³è‰²ã€éŸµå¾‹ç­‰ã€‚
2. è®ºæ–‡æå‡ºVStyleåŸºå‡†ï¼ŒåŒ…å«å£°å­¦å±æ€§ã€è‡ªç„¶è¯­è¨€æŒ‡ä»¤ç­‰å››ç±»ä»»åŠ¡ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹é£æ ¼è¿ç§»èƒ½åŠ›ã€‚
3. å¼•å…¥LALMä½œä¸ºè¯„åˆ¤å™¨æ¡†æ¶ï¼Œä»æ–‡æœ¬å¿ å®åº¦ã€é£æ ¼ä¸€è‡´æ€§å’Œè‡ªç„¶åº¦ä¸‰æ–¹é¢å®¢è§‚è¯„ä¼°æ¨¡å‹è¾“å‡ºã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å£è¯­è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰å·²æˆä¸ºè¯­éŸ³ç†è§£å’Œç”Ÿæˆçš„ä¸€ç§ç»Ÿä¸€èŒƒå¼ï¼Œå®ç°äº†è‡ªç„¶çš„äººæœºäº¤äº’ã€‚ç„¶è€Œï¼Œè™½ç„¶å¤§å¤šæ•°è¿›å±•éƒ½é›†ä¸­åœ¨è¯­ä¹‰å‡†ç¡®æ€§å’ŒæŒ‡ä»¤éµå¾ªä¸Šï¼Œä½†SLMåŸºäºå£è¯­æŒ‡ä»¤è°ƒæ•´å…¶è¯´è¯é£æ ¼çš„èƒ½åŠ›å—åˆ°çš„å…³æ³¨æœ‰é™ã€‚æˆ‘ä»¬å¼•å…¥äº†è¯­éŸ³é£æ ¼è¿ç§»ï¼ˆVSAï¼‰è¿™ä¸€æ–°ä»»åŠ¡ï¼Œæ—¨åœ¨è€ƒå¯ŸSLMæ˜¯å¦èƒ½å¤Ÿæ ¹æ®è‡ªç„¶è¯­è¨€å£è¯­å‘½ä»¤ä¿®æ”¹å…¶è¯´è¯é£æ ¼ï¼Œä¾‹å¦‚éŸ³è‰²ã€éŸµå¾‹æˆ–è§’è‰²ã€‚ä¸ºäº†ç ”ç©¶è¿™ä¸ªä»»åŠ¡ï¼Œæˆ‘ä»¬æå‡ºäº†VStyleï¼Œä¸€ä¸ªåŒè¯­ï¼ˆä¸­æ–‡å’Œè‹±æ–‡ï¼‰è¯„æµ‹åŸºå‡†ï¼Œæ¶µç›–äº†å››ç±»è¯­éŸ³ç”Ÿæˆï¼šå£°å­¦å±æ€§ã€è‡ªç„¶è¯­è¨€æŒ‡ä»¤ã€è§’è‰²æ‰®æ¼”å’Œéšå¼å…±æƒ…ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ä½œä¸ºè¯„åˆ¤å™¨ï¼ˆLALM as a Judgeï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€æ­¥è¯„ä¼°è¾“å‡ºçš„æ–‡æœ¬å¿ å®åº¦ã€é£æ ¼ä¸€è‡´æ€§å’Œè‡ªç„¶åº¦ï¼Œç¡®ä¿å¯é‡å¤å’Œå®¢è§‚çš„è¯„ä¼°ã€‚å¯¹å•†ä¸šç³»ç»Ÿå’Œå¼€æºSLMçš„å®éªŒè¡¨æ˜ï¼Œå½“å‰æ¨¡å‹åœ¨å¯æ§é£æ ¼è¿ç§»æ–¹é¢é¢ä¸´æ˜æ˜¾çš„å±€é™æ€§ï¼Œçªå‡ºäº†è¿™é¡¹ä»»åŠ¡çš„æ–°é¢–æ€§å’ŒæŒ‘æˆ˜æ€§ã€‚é€šè¿‡å‘å¸ƒVStyleåŠå…¶è¯„ä¼°å·¥å…·åŒ…ï¼Œæˆ‘ä»¬æ—¨åœ¨ä¸ºç¤¾åŒºæä¾›ä¸€ä¸ªæ¨è¿›ä»¥äººä¸ºä¸­å¿ƒçš„å£è¯­äº¤äº’çš„åŸºç¡€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å£è¯­è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰åœ¨è¯­éŸ³é£æ ¼è¿ç§»ï¼ˆVSAï¼‰æ–¹é¢çš„ä¸è¶³ã€‚ç°æœ‰SLMä¸»è¦å…³æ³¨è¯­ä¹‰å‡†ç¡®æ€§å’ŒæŒ‡ä»¤éµå¾ªï¼Œè€Œå¿½ç•¥äº†æ ¹æ®å£è¯­æŒ‡ä»¤è°ƒæ•´è¯­éŸ³é£æ ¼ï¼ˆå¦‚éŸ³è‰²ã€éŸµå¾‹ã€è§’è‰²ï¼‰çš„èƒ½åŠ›ã€‚è¿™é™åˆ¶äº†äººæœºäº¤äº’çš„è‡ªç„¶æ€§å’Œè¡¨ç°åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªå…¨é¢çš„è¯„æµ‹åŸºå‡†VStyleï¼Œç”¨äºç³»ç»Ÿæ€§åœ°è¯„ä¼°SLMåœ¨VSAä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚åŒæ—¶ï¼Œå¼•å…¥LALMä½œä¸ºè¯„åˆ¤å™¨ï¼Œä»¥å®¢è§‚ã€å¯é‡å¤çš„æ–¹å¼è¯„ä¼°æ¨¡å‹çš„è¾“å‡ºè´¨é‡ã€‚é€šè¿‡åŸºå‡†æµ‹è¯•å’Œè¯„ä¼°ï¼Œå¯ä»¥ä¿ƒè¿›SLMåœ¨è¯­éŸ³é£æ ¼æ§åˆ¶æ–¹é¢çš„ç ”ç©¶å’Œå‘å±•ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVStyleåŸºå‡†åŒ…å«å››ä¸ªä¸»è¦ç±»åˆ«ï¼š1) å£°å­¦å±æ€§æ§åˆ¶ï¼Œè¦æ±‚æ¨¡å‹æ ¹æ®æŒ‡ä»¤è°ƒæ•´è¯­éŸ³çš„å£°å­¦ç‰¹å¾ï¼›2) è‡ªç„¶è¯­è¨€æŒ‡ä»¤æ§åˆ¶ï¼Œè¦æ±‚æ¨¡å‹æ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤ç”Ÿæˆç‰¹å®šé£æ ¼çš„è¯­éŸ³ï¼›3) è§’è‰²æ‰®æ¼”ï¼Œè¦æ±‚æ¨¡å‹æ¨¡æ‹Ÿç‰¹å®šè§’è‰²çš„è¯´è¯é£æ ¼ï¼›4) éšå¼å…±æƒ…ï¼Œè¦æ±‚æ¨¡å‹åœ¨è¯­éŸ³ä¸­ä½“ç°å‡ºç‰¹å®šçš„æƒ…æ„Ÿã€‚LALMè¯„åˆ¤å™¨æ¡†æ¶åŒ…å«ä¸‰ä¸ªè¯„ä¼°ç»´åº¦ï¼šæ–‡æœ¬å¿ å®åº¦ï¼ˆè¾“å‡ºæ˜¯å¦ç¬¦åˆæŒ‡ä»¤çš„è¯­ä¹‰ï¼‰ã€é£æ ¼ä¸€è‡´æ€§ï¼ˆè¾“å‡ºæ˜¯å¦ç¬¦åˆæŒ‡ä»¤çš„é£æ ¼è¦æ±‚ï¼‰å’Œè‡ªç„¶åº¦ï¼ˆè¾“å‡ºå¬èµ·æ¥æ˜¯å¦è‡ªç„¶æµç•…ï¼‰ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†VSAä»»åŠ¡ï¼Œå¡«è¡¥äº†SLMç ”ç©¶ä¸­å¯¹è¯­éŸ³é£æ ¼æ§åˆ¶å…³æ³¨ä¸è¶³çš„ç©ºç™½ï¼›2) æ„å»ºäº†VStyleåŸºå‡†ï¼Œä¸ºVSAä»»åŠ¡æä¾›äº†ä¸€ä¸ªæ ‡å‡†åŒ–çš„è¯„æµ‹å¹³å°ï¼›3) å¼•å…¥äº†LALMè¯„åˆ¤å™¨æ¡†æ¶ï¼Œæä¾›äº†ä¸€ç§å®¢è§‚ã€å¯é‡å¤çš„è¯„ä¼°æ–¹æ³•ã€‚

**å…³é”®è®¾è®¡**ï¼šVStyleåŸºå‡†åŒ…å«åŒè¯­ï¼ˆä¸­æ–‡å’Œè‹±æ–‡ï¼‰æ•°æ®ï¼Œæ¶µç›–å¤šç§è¯­éŸ³é£æ ¼å’ŒæŒ‡ä»¤ç±»å‹ã€‚LALMè¯„åˆ¤å™¨æ¡†æ¶ä½¿ç”¨å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ä½œä¸ºè¯„ä¼°å™¨ï¼Œé€šè¿‡è®­ç»ƒä½¿å…¶èƒ½å¤Ÿå‡†ç¡®è¯„ä¼°æ–‡æœ¬å¿ å®åº¦ã€é£æ ¼ä¸€è‡´æ€§å’Œè‡ªç„¶åº¦ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç»†èŠ‚æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†è¯´æ˜ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰çš„å•†ä¸šç³»ç»Ÿå’Œå¼€æºSLMåœ¨VStyleåŸºå‡†ä¸Šè¡¨ç°å‡ºæ˜æ˜¾çš„å±€é™æ€§ï¼Œå°¤å…¶æ˜¯åœ¨é£æ ¼ä¸€è‡´æ€§æ–¹é¢ã€‚è¿™éªŒè¯äº†VStyleåŸºå‡†çš„æŒ‘æˆ˜æ€§å’Œä»·å€¼ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æŒ‡æ˜äº†æ–¹å‘ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œæå‡å¹…åº¦æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†è¯´æ˜ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ™ºèƒ½åŠ©æ‰‹ã€è¯­éŸ³åˆæˆã€æ¸¸æˆè§’è‰²é…éŸ³ç­‰é¢†åŸŸã€‚é€šè¿‡æå‡è¯­éŸ³é£æ ¼è¿ç§»èƒ½åŠ›ï¼Œå¯ä»¥ä½¿äººæœºäº¤äº’æ›´åŠ è‡ªç„¶ã€ä¸ªæ€§åŒ–ï¼Œå¢å¼ºç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶æœ‰æœ›æ¨åŠ¨å£è¯­è¯­è¨€æ¨¡å‹åœ¨æƒ…æ„Ÿè¡¨è¾¾ã€è§’è‰²æ‰®æ¼”ç­‰æ–¹é¢çš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Spoken language models (SLMs) have emerged as a unified paradigm for speech understanding and generation, enabling natural human machine interaction. However, while most progress has focused on semantic accuracy and instruction following, the ability of SLMs to adapt their speaking style based on spoken instructions has received limited attention. We introduce Voice Style Adaptation (VSA), a new task that examines whether SLMs can modify their speaking style, such as timbre, prosody, or persona following natural language spoken commands. To study this task, we present VStyle, a bilingual (Chinese & English) benchmark covering four categories of speech generation: acoustic attributes, natural language instruction, role play, and implicit empathy. We also introduce the Large Audio Language Model as a Judge (LALM as a Judge) framework, which progressively evaluates outputs along textual faithfulness, style adherence, and naturalness, ensuring reproducible and objective assessment. Experiments on commercial systems and open source SLMs demonstrate that current models face clear limitations in controllable style adaptation, highlighting both the novelty and challenge of this task. By releasing VStyle and its evaluation toolkit, we aim to provide the community with a foundation for advancing human centered spoken interaction. The dataset and code are publicly available at \href{https://junzhan2000.github.io/VStyle.github.io/}{project's homepage}.

