---
layout: default
title: Visual Backdoor Attacks on MLLM Embodied Decision Making via Contrastive Trigger Learning
---

# Visual Backdoor Attacks on MLLM Embodied Decision Making via Contrastive Trigger Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.27623" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.27623v1</a>
  <a href="https://arxiv.org/pdf/2510.27623.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.27623v1" onclick="toggleFavorite(this, '2510.27623v1', 'Visual Backdoor Attacks on MLLM Embodied Decision Making via Contrastive Trigger Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Qiusi Zhan, Hyeonjeong Ha, Rui Yang, Sirui Xu, Hanyang Chen, Liang-Yan Gui, Yu-Xiong Wang, Huan Zhang, Heng Ji, Daniel Kang

**åˆ†ç±»**: cs.AI, cs.CL, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-31

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºBEATæ¡†æ¶ï¼Œé€šè¿‡å¯¹æ¯”è§¦å‘å­¦ä¹ å®ç°MLLMå…·èº«æ™ºèƒ½ä½“çš„è§†è§‰åé—¨æ”»å‡»**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰åé—¨æ”»å‡»` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `å…·èº«æ™ºèƒ½ä½“` `å¯¹æ¯”å­¦ä¹ ` `è§¦å‘å™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. MLLMé©±åŠ¨çš„å…·èº«æ™ºèƒ½ä½“é¢ä¸´è§†è§‰åé—¨æ”»å‡»çš„å®‰å…¨é£é™©ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥åº”å¯¹è§†è§’å’Œå…‰ç…§å˜åŒ–ä¸‹çš„ç‰©ä½“è§¦å‘å™¨ã€‚
2. BEATæ¡†æ¶é€šè¿‡å¯¹æ¯”è§¦å‘å­¦ä¹ (CTL)ï¼Œæ˜¾å¼åœ°å­¦ä¹ è§¦å‘å™¨å­˜åœ¨ä¸å¦çš„åå¥½ï¼Œä»è€Œé”åŒ–å†³ç­–è¾¹ç•Œï¼Œæå‡åé—¨æ¿€æ´»çš„å‡†ç¡®æ€§ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒBEATåœ¨ä¿æŒè‰¯å¥½è‰¯æ€§ä»»åŠ¡æ€§èƒ½çš„åŒæ—¶ï¼Œå®ç°äº†é«˜è¾¾80%çš„æ”»å‡»æˆåŠŸç‡ï¼Œå¹¶ä¸”åœ¨æœ‰é™åé—¨æ•°æ®ä¸‹ï¼ŒCTLå°†åé—¨æ¿€æ´»å‡†ç¡®ç‡æå‡äº†39%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºBEATæ¡†æ¶ï¼Œæ—¨åœ¨ä¸ºåŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLM)çš„å…·èº«æ™ºèƒ½ä½“æ³¨å…¥è§†è§‰åé—¨ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ç¯å¢ƒä¸­çš„ç‰©ä½“ä½œä¸ºè§¦å‘å™¨ï¼Œå½“è§¦å‘å™¨å‡ºç°åœ¨åœºæ™¯ä¸­æ—¶ï¼Œæ™ºèƒ½ä½“ä¼šæŒç»­æ‰§è¡Œæ”»å‡»è€…é¢„å…ˆè®¾å®šçš„å¤šæ­¥ç­–ç•¥ã€‚BEATé€šè¿‡æ„å»ºåŒ…å«å¤šæ ·åœºæ™¯ã€ä»»åŠ¡å’Œè§¦å‘å™¨ä½ç½®çš„è®­ç»ƒé›†ï¼Œä½¿æ™ºèƒ½ä½“æš´éœ²äºè§¦å‘å™¨çš„å˜åŒ–ä¸­ï¼Œå¹¶é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ¡ˆï¼šé¦–å…ˆè¿›è¡Œç›‘ç£å¾®è°ƒ(SFT)ï¼Œç„¶åè¿›è¡Œå¯¹æ¯”è§¦å‘å­¦ä¹ (CTL)ã€‚CTLå°†è§¦å‘å™¨åˆ¤åˆ«è½¬åŒ–ä¸ºè§¦å‘å™¨å­˜åœ¨ä¸ä¸å­˜åœ¨è¾“å…¥ä¹‹é—´çš„åå¥½å­¦ä¹ ï¼Œä»è€Œæ˜ç¡®åœ°é”åŒ–å†³ç­–è¾¹ç•Œï¼Œç¡®ä¿ç²¾ç¡®çš„åé—¨æ¿€æ´»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBEATåœ¨å„ç§å…·èº«æ™ºèƒ½ä½“åŸºå‡†å’ŒMLLMä¸Šå®ç°äº†é«˜è¾¾80%çš„æ”»å‡»æˆåŠŸç‡ï¼ŒåŒæ—¶ä¿æŒäº†è‰¯å¥½çš„è‰¯æ€§ä»»åŠ¡æ€§èƒ½ï¼Œå¹¶å¯é åœ°æ¨å¹¿åˆ°åˆ†å¸ƒå¤–çš„è§¦å‘å™¨ä½ç½®ã€‚ä¸æœ´ç´ çš„SFTç›¸æ¯”ï¼ŒCTLåœ¨æœ‰é™çš„åé—¨æ•°æ®ä¸‹ï¼Œåé—¨æ¿€æ´»å‡†ç¡®ç‡æé«˜äº†39%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³MLLMå…·èº«æ™ºèƒ½ä½“ä¸­å­˜åœ¨çš„è§†è§‰åé—¨æ”»å‡»é—®é¢˜ã€‚ç°æœ‰çš„åŸºäºæ–‡æœ¬è§¦å‘å™¨çš„åé—¨æ”»å‡»æ–¹æ³•æ— æ³•ç›´æ¥åº”ç”¨äºè§†è§‰åœºæ™¯ï¼Œå› ä¸ºè§†è§‰è§¦å‘å™¨ï¼ˆä¾‹å¦‚ç‰©ä½“ï¼‰åœ¨ä¸åŒè§†è§’å’Œå…‰ç…§æ¡ä»¶ä¸‹ä¼šå‘ç”Ÿæ˜¾è‘—å˜åŒ–ï¼Œå¯¼è‡´è§¦å‘å™¨éš¾ä»¥å¯é åœ°æ¤å…¥ï¼Œä»è€Œå½±å“æ”»å‡»çš„æˆåŠŸç‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¯¹æ¯”å­¦ä¹ æ¥å¢å¼ºæ™ºèƒ½ä½“å¯¹è§†è§‰è§¦å‘å™¨çš„è¯†åˆ«èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡æ„å»ºåŒ…å«å¤šæ ·åœºæ™¯ã€ä»»åŠ¡å’Œè§¦å‘å™¨ä½ç½®çš„è®­ç»ƒé›†ï¼Œä½¿æ™ºèƒ½ä½“æš´éœ²äºè§¦å‘å™¨çš„å„ç§å˜åŒ–ä¸­ã€‚ç„¶åï¼Œåˆ©ç”¨å¯¹æ¯”è§¦å‘å­¦ä¹ (CTL)æ¥æ˜¾å¼åœ°å­¦ä¹ è§¦å‘å™¨å­˜åœ¨ä¸å¦çš„åå¥½ï¼Œä»è€Œé”åŒ–å†³ç­–è¾¹ç•Œï¼Œæé«˜åé—¨æ¿€æ´»çš„å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šBEATæ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šç›‘ç£å¾®è°ƒ(SFT)å’Œå¯¹æ¯”è§¦å‘å­¦ä¹ (CTL)ã€‚é¦–å…ˆï¼Œä½¿ç”¨SFTå¯¹MLLMè¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶èƒ½å¤Ÿæ‰§è¡Œæ­£å¸¸çš„å…·èº«æ™ºèƒ½ä½“ä»»åŠ¡ã€‚ç„¶åï¼Œä½¿ç”¨CTLè¿›ä¸€æ­¥å¾®è°ƒMLLMï¼Œä½¿å…¶èƒ½å¤Ÿå‡†ç¡®åœ°è¯†åˆ«è§†è§‰è§¦å‘å™¨å¹¶æ¿€æ´»åé—¨ã€‚CTLé€šè¿‡æœ€å°åŒ–å¯¹æ¯”æŸå¤±å‡½æ•°æ¥å®ç°ï¼Œè¯¥æŸå¤±å‡½æ•°é¼“åŠ±æ™ºèƒ½ä½“åŒºåˆ†åŒ…å«è§¦å‘å™¨çš„è¾“å…¥å’Œä¸åŒ…å«è§¦å‘å™¨çš„è¾“å…¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†å¯¹æ¯”è§¦å‘å­¦ä¹ (CTL)æ–¹æ³•ã€‚ä¸ä¼ ç»Ÿçš„ç›‘ç£å­¦ä¹ æ–¹æ³•ä¸åŒï¼ŒCTLå°†è§¦å‘å™¨åˆ¤åˆ«è½¬åŒ–ä¸ºè§¦å‘å™¨å­˜åœ¨ä¸ä¸å­˜åœ¨è¾“å…¥ä¹‹é—´çš„åå¥½å­¦ä¹ ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨æœ‰é™çš„åé—¨æ•°æ®ï¼Œå¹¶æé«˜åé—¨æ¿€æ´»çš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼ŒBEATæ¡†æ¶è¿˜é€šè¿‡æ„å»ºåŒ…å«å¤šæ ·åœºæ™¯ã€ä»»åŠ¡å’Œè§¦å‘å™¨ä½ç½®çš„è®­ç»ƒé›†ï¼Œå¢å¼ºäº†æ™ºèƒ½ä½“å¯¹è§¦å‘å™¨å˜åŒ–çš„é²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šCTLä½¿ç”¨å¯¹æ¯”æŸå¤±å‡½æ•°æ¥å­¦ä¹ è§¦å‘å™¨åå¥½ã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹äºæ¯ä¸ªåŒ…å«è§¦å‘å™¨çš„è¾“å…¥ï¼ŒCTLä¼šç”Ÿæˆä¸€ä¸ªå¯¹åº”çš„è´Ÿæ ·æœ¬ï¼ˆä¸åŒ…å«è§¦å‘å™¨çš„è¾“å…¥ï¼‰ã€‚ç„¶åï¼ŒCTLä¼šæœ€å°åŒ–ä»¥ä¸‹å¯¹æ¯”æŸå¤±å‡½æ•°ï¼šL = max(0, margin - s(x_pos) + s(x_neg))ï¼Œå…¶ä¸­s(x)æ˜¯æ™ºèƒ½ä½“å¯¹è¾“å…¥xçš„è¯„åˆ†ï¼Œx_posæ˜¯åŒ…å«è§¦å‘å™¨çš„è¾“å…¥ï¼Œx_negæ˜¯ä¸åŒ…å«è§¦å‘å™¨çš„è¾“å…¥ï¼Œmarginæ˜¯ä¸€ä¸ªè¶…å‚æ•°ã€‚è¯¥æŸå¤±å‡½æ•°é¼“åŠ±æ™ºèƒ½ä½“å¯¹åŒ…å«è§¦å‘å™¨çš„è¾“å…¥ç»™å‡ºæ›´é«˜çš„è¯„åˆ†ï¼Œå¯¹ä¸åŒ…å«è§¦å‘å™¨çš„è¾“å…¥ç»™å‡ºæ›´ä½çš„è¯„åˆ†ï¼Œä»è€Œé”åŒ–å†³ç­–è¾¹ç•Œã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒBEATæ¡†æ¶åœ¨å„ç§å…·èº«æ™ºèƒ½ä½“åŸºå‡†å’ŒMLLMä¸Šå®ç°äº†é«˜è¾¾80%çš„æ”»å‡»æˆåŠŸç‡ï¼ŒåŒæ—¶ä¿æŒäº†è‰¯å¥½çš„è‰¯æ€§ä»»åŠ¡æ€§èƒ½ã€‚ä¸æœ´ç´ çš„SFTç›¸æ¯”ï¼ŒCTLåœ¨æœ‰é™çš„åé—¨æ•°æ®ä¸‹ï¼Œåé—¨æ¿€æ´»å‡†ç¡®ç‡æé«˜äº†39%ã€‚æ­¤å¤–ï¼ŒBEATæ¡†æ¶è¿˜èƒ½å¤Ÿå¯é åœ°æ¨å¹¿åˆ°åˆ†å¸ƒå¤–çš„è§¦å‘å™¨ä½ç½®ï¼Œè¡¨æ˜å…¶å…·æœ‰è¾ƒå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè¯„ä¼°å’Œæå‡MLLMå…·èº«æ™ºèƒ½ä½“çš„å®‰å…¨æ€§ï¼Œå°¤å…¶æ˜¯åœ¨æœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ç­‰å®‰å…¨æ”¸å…³çš„åº”ç”¨åœºæ™¯ä¸­ã€‚é€šè¿‡æ¨¡æ‹Ÿå’Œåˆ†æè§†è§‰åé—¨æ”»å‡»ï¼Œå¯ä»¥å¸®åŠ©ç ”ç©¶äººå‘˜å¼€å‘æ›´é²æ£’çš„é˜²å¾¡æœºåˆ¶ï¼Œç¡®ä¿æ™ºèƒ½ä½“åœ¨çœŸå®ä¸–ç•Œéƒ¨ç½²ä¸­çš„å¯é æ€§å’Œå®‰å…¨æ€§ã€‚æœªæ¥çš„ç ”ç©¶å¯ä»¥æ¢ç´¢æ›´å¤æ‚çš„æ”»å‡»ç­–ç•¥å’Œæ›´æœ‰æ•ˆçš„é˜²å¾¡æ–¹æ³•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal large language models (MLLMs) have advanced embodied agents by enabling direct perception, reasoning, and planning task-oriented actions from visual inputs. However, such vision driven embodied agents open a new attack surface: visual backdoor attacks, where the agent behaves normally until a visual trigger appears in the scene, then persistently executes an attacker-specified multi-step policy. We introduce BEAT, the first framework to inject such visual backdoors into MLLM-based embodied agents using objects in the environments as triggers. Unlike textual triggers, object triggers exhibit wide variation across viewpoints and lighting, making them difficult to implant reliably. BEAT addresses this challenge by (1) constructing a training set that spans diverse scenes, tasks, and trigger placements to expose agents to trigger variability, and (2) introducing a two-stage training scheme that first applies supervised fine-tuning (SFT) and then our novel Contrastive Trigger Learning (CTL). CTL formulates trigger discrimination as preference learning between trigger-present and trigger-free inputs, explicitly sharpening the decision boundaries to ensure precise backdoor activation. Across various embodied agent benchmarks and MLLMs, BEAT achieves attack success rates up to 80%, while maintaining strong benign task performance, and generalizes reliably to out-of-distribution trigger placements. Notably, compared to naive SFT, CTL boosts backdoor activation accuracy up to 39% under limited backdoor data. These findings expose a critical yet unexplored security risk in MLLM-based embodied agents, underscoring the need for robust defenses before real-world deployment.

