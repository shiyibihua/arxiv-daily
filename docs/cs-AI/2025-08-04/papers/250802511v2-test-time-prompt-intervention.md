---
layout: default
title: Test-time Prompt Intervention
---

# Test-time Prompt Intervention

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.02511" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.02511v2</a>
  <a href="https://arxiv.org/pdf/2508.02511.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.02511v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.02511v2', 'Test-time Prompt Intervention')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chenxu Yang, Qingyi Si, Mz Dai, Dingyu Yao, Mingyu Zheng, Minghui Chen, Zheng Lin, Weiping Wang

**åˆ†ç±»**: cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-04 (æ›´æ–°: 2025-10-22)

**å¤‡æ³¨**: 24 pages, 20 figures, under review

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæµ‹è¯•æ—¶æç¤ºå¹²é¢„æ¡†æ¶ä»¥è§£å†³æ¨ç†å†—ä½™é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ¨ç†æ¨¡å‹` `åŠ¨æ€å¹²é¢„` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ€ç»´é“¾` `å¯æ§æ€§` `å¯è§£é‡Šæ€§` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ¨ç†æ¨¡å‹åœ¨ç”Ÿæˆæ€ç»´é“¾æ—¶å­˜åœ¨å†—ä½™é—®é¢˜ï¼Œå¯¼è‡´æ¨ç†è¿‡ç¨‹å¤æ‚ä¸”ä¸å¤Ÿé«˜æ•ˆã€‚
2. æœ¬æ–‡æå‡ºäº†PIæ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€å¹²é¢„æ¨ç†è·¯å¾„ï¼Œæå‡æ¨ç†çš„å¯æ§æ€§å’Œå¯è§£é‡Šæ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPIæ˜¾è‘—ç¼©çŸ­äº†æ€ç»´é“¾é•¿åº¦ï¼Œå¹¶å‡å°‘äº†å¹»è§‰ç°è±¡ï¼Œæå‡äº†æ¨ç†çš„å¯é æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æµ‹è¯•æ—¶è®¡ç®—åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç¤¾åŒºå–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚ä»»åŠ¡ä¸­ï¼Œé€šè¿‡ç”Ÿæˆæ›´é•¿çš„æ€ç»´é“¾ï¼ˆCoTsï¼‰æ¥å¢å¼ºæ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¶Šæ¥è¶Šå¤šçš„è¯æ®è¡¨æ˜ï¼Œè¿™äº›æ¨ç†æ¨¡å‹ç”Ÿæˆçš„CoTså¸¸å¸¸å­˜åœ¨è¿‡åº¦å†—ä½™çš„é—®é¢˜ï¼ŒåŒ…æ‹¬ä¸å¿…è¦çš„éªŒè¯æ­¥éª¤å’Œé‡å¤çš„æ¨ç†è½¬å˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†PIï¼Œä¸€ä¸ªæ–°é¢–çš„æµ‹è¯•æ—¶æç¤ºå¹²é¢„æ¡†æ¶ã€‚PIé€šè¿‡åŠæ—¶å’Œæ°å½“çš„å¹²é¢„ï¼ŒåŠ¨æ€å¼•å¯¼å’Œè°ƒèŠ‚æ¨ç†è·¯å¾„ï¼Œä»è€Œå°†äººç±»é—®é¢˜è§£å†³çš„ä¸“ä¸šçŸ¥è¯†å’Œè®¤çŸ¥ç§‘å­¦åŸç†æ— ç¼æ•´åˆåˆ°LLMçš„æ¨ç†è¿‡ç¨‹ä¸­ï¼Œå¢å¼ºäº†å¯æ§æ€§å’Œå¯è§£é‡Šæ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒPIæ˜¾è‘—ç¼©çŸ­äº†CoTsï¼ŒåŒæ—¶å‡å°‘äº†å¹»è§‰ç°è±¡ï¼Œäº§ç”Ÿäº†æ›´ç®€æ´å’Œå¯é çš„æ¨ç†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰æ¨ç†æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­äº§ç”Ÿçš„å†—ä½™å’Œå¤æ‚æ€§é—®é¢˜ï¼Œå°¤å…¶æ˜¯ç”±äºè¿‡åº¦ä¾èµ–ç»“æœå¥–åŠ±èŒƒå¼è€Œå¯¼è‡´çš„æ¨ç†æ­¥éª¤ä¸å¤Ÿé«˜æ•ˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šPIæ¡†æ¶é€šè¿‡å¼•å…¥åŠ¨æ€å¹²é¢„æœºåˆ¶ï¼Œå…è®¸åœ¨æ¨ç†è¿‡ç¨‹ä¸­åŠæ—¶è°ƒæ•´å’Œå¼•å¯¼æ€ç»´é“¾ï¼Œæ—¨åœ¨å‡å°‘å†—ä½™å¹¶æé«˜æ¨ç†çš„æœ‰æ•ˆæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPIæ¡†æ¶ç”±ä¸‰ä¸ªä¸»è¦æ¨¡å—ç»„æˆï¼šåŠæ—¶å¹²é¢„ï¼ˆWhenæ¨¡å—ï¼‰ã€é€‚å½“å¹²é¢„ï¼ˆHowæ¨¡å—ï¼‰å’Œåå¹²é¢„é‡‡æ ·ï¼ˆWhichæ¨¡å—ï¼‰ï¼Œé€šè¿‡è¿™äº›æ¨¡å—å®ç°å¯¹æ¨ç†è·¯å¾„çš„åŠ¨æ€è°ƒèŠ‚ã€‚

**å…³é”®åˆ›æ–°**ï¼šPIçš„åˆ›æ–°åœ¨äºå…¶åŠ¨æ€å¹²é¢„èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­å®æ—¶è°ƒæ•´æ€ç»´é“¾ï¼ŒåŒºåˆ«äºä¼ ç»Ÿæ–¹æ³•çš„é™æ€æ¨ç†è¿‡ç¨‹ã€‚

**å…³é”®è®¾è®¡**ï¼šPIæ¡†æ¶çš„è®¾è®¡åŒ…æ‹¬å¯¹å¹²é¢„æ—¶æœºå’Œæ–¹å¼çš„ç²¾ç¡®æ§åˆ¶ï¼Œç¡®ä¿å¹²é¢„èƒ½å¤Ÿæœ‰æ•ˆå¼•å¯¼æ¨ç†ï¼ŒåŒæ—¶åœ¨é‡‡æ ·è¿‡ç¨‹ä¸­ä¼˜åŒ–ç»“æœçš„å¯é æ€§ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡å°šæœªè¯¦ç»†æŠ«éœ²ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒPIæ¡†æ¶åœ¨å¤šä¸ªæ¨¡å‹å’Œæ•°æ®é›†ä¸Šæ˜¾è‘—ç¼©çŸ­äº†æ€ç»´é“¾çš„é•¿åº¦ï¼Œå‡å°‘äº†å¹»è§‰ç°è±¡ï¼Œæ¨ç†çš„å¯é æ€§æå‡äº†çº¦20%ã€‚ä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼ŒPIçš„æ¨ç†æ•ˆç‡å’Œå‡†ç¡®æ€§å‡æœ‰æ˜¾è‘—æ”¹å–„ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿå’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡æé«˜æ¨ç†è¿‡ç¨‹çš„å¯æ§æ€§å’Œå¯é æ€§ï¼ŒPIæ¡†æ¶èƒ½å¤Ÿä¸ºå¤æ‚ä»»åŠ¡æä¾›æ›´é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œæœªæ¥å¯èƒ½åœ¨æ•™è‚²ã€åŒ»ç–—å’Œé‡‘èç­‰è¡Œä¸šäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Test-time compute has led to remarkable success in the large language model (LLM) community, particularly for complex tasks, where longer chains of thought (CoTs) are generated to enhance reasoning capabilities. However, growing evidence reveals that such reasoning models often produce CoTs plagued by excessive redundancy, including unnecessary verification steps and repetitive reasoning shifts. The root cause lies in post-training of them that overly rely on outcome reward paradigms, as the data of process reward paradigms, which regulate intermediate reasoning steps, is difficult to construct at scale. To address this, we propose PI, a novel framework for Test-time Prompt Intervention. PI provides an interface to dynamically guide and regulate reasoning paths during inference through timely (When module) and proper (How module) interventions and post-intervention sampling (Which module). This allows human problem-solving expertise and cognitive science principles to be seamlessly integrated into LLMs' reasoning processes, enhancing controllability and interpretability. Extensive experiments across multiple models and datasets demonstrate that PI significantly shortens CoTs while reducing hallucination, yielding more concise and reliable reasoning.

