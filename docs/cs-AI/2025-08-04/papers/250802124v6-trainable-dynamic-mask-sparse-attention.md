---
layout: default
title: Trainable Dynamic Mask Sparse Attention
---

# Trainable Dynamic Mask Sparse Attention

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.02124" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.02124v6</a>
  <a href="https://arxiv.org/pdf/2508.02124.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.02124v6" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.02124v6', 'Trainable Dynamic Mask Sparse Attention')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jingze Shi, Yifan Wu, Yiran Peng, Bingheng Wu, Liangdong Wang, Guang Liu, Yuyu Luo

**åˆ†ç±»**: cs.AI, cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-04 (æ›´æ–°: 2025-11-16)

**å¤‡æ³¨**: 26 pages

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/SmallDoges/flash-dmattn)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¯è®­ç»ƒåŠ¨æ€æ©ç ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ä»¥è§£å†³é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡` `ç¨€ç–æ³¨æ„åŠ›` `åŠ¨æ€æ©ç ` `æ·±åº¦å­¦ä¹ ` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç¨€ç–æ³¨æ„åŠ›æ–¹æ³•åœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡æ—¶å­˜åœ¨é€‚åº”æ€§ä¸è¶³å’Œå¯å¾®æ€§å·®çš„é—®é¢˜ã€‚
2. æœ¬æ–‡æå‡ºçš„åŠ¨æ€æ©ç æ³¨æ„åŠ›æœºåˆ¶é€šè¿‡ç”Ÿæˆå†…å®¹æ„ŸçŸ¥çš„åŠ¨æ€æ©ç å’Œç¡¬ä»¶å‹å¥½çš„ç¨€ç–æƒé‡æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒDMAåœ¨å¤šä¸ªåŸºå‡†ä»»åŠ¡ä¸Šå‡ä¼˜äºç°æœ‰ç¨€ç–æ³¨æ„åŠ›åŸºçº¿ï¼Œå¹¶å®ç°äº†é«˜è¾¾10å€çš„é€Ÿåº¦æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡éœ€æ±‚çš„å¢åŠ ï¼Œæ ‡å‡†è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„å¹³æ–¹å¤æ‚åº¦æˆä¸ºç“¶é¢ˆã€‚è™½ç„¶ç¨€ç–æ³¨æ„åŠ›æ–¹æ³•è¢«æå‡ºä»¥ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œä½†ç°æœ‰çš„åŸºäºä½ç½®çš„ç¨€ç–æ³¨æ„åŠ›æ–¹æ³•ç¼ºä¹å¯¹å¤šæ ·åŒ–æŸ¥è¯¢ä¸Šä¸‹æ–‡çš„é€‚åº”æ€§ï¼Œè€ŒåŸºäºå†…å®¹çš„ç¨€ç–æ³¨æ„åŠ›æ–¹æ³•ä¾èµ–å¯å‘å¼çš„é”®å€¼é€‰æ‹©ï¼Œé™åˆ¶äº†å®Œå…¨å¯å¾®æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å¯è®­ç»ƒçš„åŠ¨æ€æ©ç ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ï¼ˆDMAï¼‰ï¼Œé€šè¿‡ä¸‰é¡¹å…³é”®åˆ›æ–°å®ç°äº†ä½ç½®æ„ŸçŸ¥å’Œå†…å®¹æ„ŸçŸ¥æ–¹æ³•çš„ä¼˜åŠ¿èåˆã€‚DMAé€šè¿‡ç”Ÿæˆå†…å®¹æ„ŸçŸ¥çš„åŠ¨æ€æ©ç ï¼Œè®¡ç®—ç¡¬ä»¶å‹å¥½çš„ä½ç½®æ„ŸçŸ¥ç¨€ç–æƒé‡ï¼Œå¹¶æ”¯æŒç«¯åˆ°ç«¯è®­ç»ƒï¼Œå±•ç¤ºäº†å…¶åœ¨å¤šé¡¹ä»»åŠ¡ä¸­çš„ä¼˜è¶Šæ€§èƒ½å’Œé«˜è¾¾10å€çš„é€Ÿåº¦æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡ä¸­é¢ä¸´çš„è®¡ç®—å¤æ‚åº¦é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨é€‚åº”æ€§å’Œå¯å¾®æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºçš„åŠ¨æ€æ©ç æ³¨æ„åŠ›æœºåˆ¶ï¼ˆDMAï¼‰ç»“åˆäº†ä½ç½®æ„ŸçŸ¥å’Œå†…å®¹æ„ŸçŸ¥çš„ä¼˜ç‚¹ï¼Œé€šè¿‡åŠ¨æ€ç”Ÿæˆæ©ç æ¥é€‚åº”ä¸åŒçš„æŸ¥è¯¢ä¸Šä¸‹æ–‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDMAçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šå†…å®¹æ„ŸçŸ¥åŠ¨æ€æ©ç ç”Ÿæˆã€ç¡¬ä»¶å‹å¥½çš„ä½ç½®æ„ŸçŸ¥ç¨€ç–æƒé‡è®¡ç®—å’Œæ”¯æŒç«¯åˆ°ç«¯è®­ç»ƒçš„æ¢¯åº¦æµåŠ¨è®¾è®¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šDMAçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶åŠ¨æ€æ©ç çš„ç”Ÿæˆæ–¹å¼å’Œç¨€ç–æƒé‡çš„è®¡ç®—æ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè·³è¿‡ä¸å¿…è¦çš„è®¡ç®—åŒºåŸŸï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„å¯å¾®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒDMAä½¿ç”¨å€¼å‘é‡è¡¨ç¤ºç”ŸæˆåŠ¨æ€æ©ç ï¼Œå¹¶é€šè¿‡ä¼˜åŒ–çš„ç¨€ç–æƒé‡è®¡ç®—æ¥æé«˜è®¡ç®—æ•ˆç‡ï¼Œç¡®ä¿åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ¢¯åº¦ä¸è¢«é˜»å¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDMAåœ¨å¤šä¸ªä»»åŠ¡ä¸­å‡ä¼˜äºæœ€å…ˆè¿›çš„ç¨€ç–æ³¨æ„åŠ›åŸºçº¿ï¼Œå°¤å…¶åœ¨å¤šæŸ¥è¯¢å…³è”è®°å¿†å’Œæ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°çªå‡ºï¼Œæ•´ä½“é€Ÿåº¦æå‡é«˜è¾¾10å€ã€‚è¿™äº›ç»“æœè¡¨æ˜DMAåœ¨æ•ˆç‡ä¸é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡èƒ½åŠ›ä¹‹é—´å®ç°äº†æœ‰æ•ˆå¹³è¡¡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„åŠ¨æ€æ©ç ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨ç¿»è¯‘å’Œé•¿æ–‡æœ¬ç†è§£ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æé«˜æ¨¡å‹çš„è®¡ç®—æ•ˆç‡å’Œé•¿ä¸Šä¸‹æ–‡å¤„ç†èƒ½åŠ›ï¼ŒDMAèƒ½å¤Ÿä¸ºå®é™…åº”ç”¨æä¾›æ›´å¿«çš„å“åº”æ—¶é—´å’Œæ›´é«˜çš„å‡†ç¡®æ€§ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•å’Œåº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The increasing demand for long-context modeling in large language models (LLMs) is bottlenecked by the quadratic complexity of the standard self-attention mechanism. The community has proposed sparse attention to mitigate this issue. However, position-aware sparse attention methods rely on static sparse structures that lack adaptability to diverse query contexts, while content-aware sparse attention methods depend on heuristic key-value selection, hindering full differentiability. We introduce a trainable dynamic mask sparse attention mechanism, a method that merges the advantages of both position-aware and content-aware approaches. Dynamic Mask Attention (DMA) achieves this through three key innovations: First, it leverages value vector representations to generate content-aware dynamic masks, enabling the model to adaptively identify and attend to critical information. Second, it computes position-aware sparse weights in a hardware-friendly manner, efficiently skipping unnecessary computational regions. Finally, we demonstrate that the introduced dynamic mask and sparse weights do not obstruct gradients, supporting end-to-end training. We have validated the performance of DMA through comprehensive experiments. A large body of experimental evidence shows that DMA consistently holds a Pareto advantage over state-of-the-art sparse attention baselines in tasks including scaling laws, multi-query associative recall, standard benchmarks, and needle in a haystack tests, while also delivering up to a 10x overall speedup. These results highlight its ability to effectively balance model efficiency with long-context modeling capabilities. Our computational kernel code is now open-source at https://github.com/SmallDoges/flash-dmattn to encourage further research and application by the community.

