---
layout: default
title: Communicating Plans, Not Percepts: Scalable Multi-Agent Coordination with Embodied World Models
---

# Communicating Plans, Not Percepts: Scalable Multi-Agent Coordination with Embodied World Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.02912" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.02912v4</a>
  <a href="https://arxiv.org/pdf/2508.02912.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.02912v4" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.02912v4', 'Communicating Plans, Not Percepts: Scalable Multi-Agent Coordination with Embodied World Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Brennen A. Hill, Mant Koh En Wei, Thangavel Jishnuanandh

**åˆ†ç±»**: cs.MA, cs.AI, cs.LG, eess.SY

**å‘å¸ƒæ—¥æœŸ**: 2025-08-04 (æ›´æ–°: 2025-11-24)

**å¤‡æ³¨**: Published in the Proceedings of the 39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: Scaling Environments for Agents (SEA). Additionally accepted for presentation in the NeurIPS 2025 Workshop: Embodied World Models for Decision Making (EWM) and the NeurIPS 2025 Workshop: Optimization for Machine Learning (OPT)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºä¸–ç•Œæ¨¡å‹çš„é€šä¿¡ç­–ç•¥ä»¥è§£å†³å¤šæ™ºèƒ½ä½“åè°ƒé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ` `å¼ºåŒ–å­¦ä¹ ` `é€šä¿¡ç­–ç•¥` `ä¸–ç•Œæ¨¡å‹` `æœºå™¨äººåä½œ` `ä»»åŠ¡åˆ†é…` `æ ·æœ¬æ•ˆç‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæ™ºèƒ½ä½“åè°ƒæ–¹æ³•åœ¨éƒ¨åˆ†å¯è§‚æµ‹ç¯å¢ƒä¸­é¢ä¸´é€šä¿¡æ•ˆç‡ä½å’Œå†³ç­–è´¨é‡å·®çš„æŒ‘æˆ˜ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸¤ç§é€šä¿¡ç­–ç•¥ï¼Œåˆ†åˆ«ä¸ºå­¦ä¹ ç›´æ¥é€šä¿¡ï¼ˆLDCï¼‰å’Œæ„å›¾é€šä¿¡ï¼Œåè€…åŸºäºå­¦ä¹ çš„ä¸–ç•Œæ¨¡å‹è¿›è¡Œä¿¡æ¯ä¼ é€’ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ„å›¾é€šä¿¡åœ¨å¤æ‚ç¯å¢ƒä¸­è¡¨ç°å‡ºæ›´é«˜çš„æ ·æœ¬æ•ˆç‡å’Œåè°ƒæ€§èƒ½ï¼Œä¼˜äºè‡ªå‘é€šä¿¡ç­–ç•¥ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­ï¼Œç¨³å¥çš„åè°ƒå¯¹äºæœ‰æ•ˆå†³ç­–è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨éƒ¨åˆ†å¯è§‚æµ‹çš„æƒ…å†µä¸‹ã€‚æœ¬æ–‡æ¢è®¨äº†åœ¨å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰ä¸­ï¼Œé€šä¿¡åè®®çš„å·¥ç¨‹è®¾è®¡ä¸ç«¯åˆ°ç«¯å­¦ä¹ ä¹‹é—´çš„æƒè¡¡ã€‚æˆ‘ä»¬æå‡ºå¹¶æ¯”è¾ƒäº†ä¸¤ç§é€šä¿¡ç­–ç•¥ï¼šå­¦ä¹ ç›´æ¥é€šä¿¡ï¼ˆLDCï¼‰å’Œæ„å›¾é€šä¿¡ã€‚æ„å›¾é€šä¿¡åˆ©ç”¨ç´§å‡‘çš„å­¦ä¹ ä¸–ç•Œæ¨¡å‹ï¼Œé€šè¿‡ä»£ç†çš„ç­–ç•¥æ¨¡æ‹Ÿæœªæ¥çŠ¶æ€ï¼Œå¹¶å°†è®¡åˆ’å‹ç¼©ä¸ºæ¶ˆæ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡åœ¨ç®€å•ç¯å¢ƒä¸­è‡ªå‘é€šä¿¡æ˜¯å¯è¡Œçš„ï¼Œä½†åŸºäºä¸–ç•Œæ¨¡å‹çš„å·¥ç¨‹æ–¹æ³•åœ¨å¤æ‚æ€§å¢åŠ æ—¶è¡¨ç°å‡ºæ›´ä¼˜çš„æ€§èƒ½å’Œæ ·æœ¬æ•ˆç‡ã€‚è¿™äº›å‘ç°æ”¯æŒå°†ç»“æ„åŒ–çš„é¢„æµ‹æ¨¡å‹æ•´åˆåˆ°MARLä»£ç†ä¸­ï¼Œä»¥å®ç°ä¸»åŠ¨çš„ç›®æ ‡é©±åŠ¨åè°ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­åœ¨éƒ¨åˆ†å¯è§‚æµ‹ç¯å¢ƒä¸‹çš„æœ‰æ•ˆåè°ƒé—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºè‡ªå‘é€šä¿¡ï¼Œå¯¼è‡´åœ¨å¤æ‚ä»»åŠ¡ä¸­æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºçš„æ„å›¾é€šä¿¡ç­–ç•¥é€šè¿‡åˆ©ç”¨ç´§å‡‘çš„ä¸–ç•Œæ¨¡å‹æ¥ç”Ÿæˆä»£ç†çš„æœªæ¥çŠ¶æ€ï¼Œä»è€Œä¼˜åŒ–ä¿¡æ¯ä¼ é€’è¿‡ç¨‹ã€‚è¿™ç§æ–¹æ³•ç»“åˆäº†å·¥ç¨‹è®¾è®¡ä¸å­¦ä¹ çš„ä¼˜åŠ¿ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šæƒ³è±¡è½¨è¿¹ç”Ÿæˆæ¨¡å—ï¼ˆITGMï¼‰ï¼Œç”¨äºæ¨¡æ‹Ÿæœªæ¥çŠ¶æ€ï¼›æ¶ˆæ¯ç”Ÿæˆç½‘ç»œï¼ˆMGNï¼‰ï¼Œç”¨äºå°†ç”Ÿæˆçš„è®¡åˆ’å‹ç¼©æˆæ¶ˆæ¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†ä¸–ç•Œæ¨¡å‹ä¸é€šä¿¡ç­–ç•¥ç»“åˆï¼Œå½¢æˆäº†ä¸€ç§æ–°çš„é€šä¿¡æ–¹å¼ï¼Œæ˜¾è‘—æé«˜äº†åœ¨å¤æ‚ç¯å¢ƒä¸­çš„åè°ƒèƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ç´§å‡‘çš„å­¦ä¹ æ¨¡å‹ï¼Œå¹¶é€šè¿‡ä¼˜åŒ–æŸå¤±å‡½æ•°æ¥æå‡æ¶ˆæ¯ç”Ÿæˆçš„è´¨é‡ï¼Œç¡®ä¿ä¿¡æ¯åœ¨å¤šæ™ºèƒ½ä½“é—´çš„æœ‰æ•ˆä¼ é€’ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„åœ¨å®éªŒéƒ¨åˆ†è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ„å›¾é€šä¿¡ç­–ç•¥åœ¨å¤æ‚ç¯å¢ƒä¸­çš„æ ·æœ¬æ•ˆç‡å’Œåè°ƒæ€§èƒ½æ˜¾è‘—ä¼˜äºå­¦ä¹ ç›´æ¥é€šä¿¡ï¼ˆLDCï¼‰ï¼Œåœ¨æŸäº›ä»»åŠ¡ä¸­æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°äº†30%ä»¥ä¸Šã€‚è¿™ä¸€å‘ç°å¼ºè°ƒäº†ç»“æ„åŒ–æ¨¡å‹åœ¨å¤šæ™ºèƒ½ä½“åè°ƒä¸­çš„é‡è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººåä½œã€è‡ªåŠ¨é©¾é©¶è½¦è¾†çš„åè°ƒæ§åˆ¶ä»¥åŠæ™ºèƒ½åˆ¶é€ ç³»ç»Ÿç­‰ã€‚é€šè¿‡æå‡å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„åè°ƒèƒ½åŠ›ï¼Œå¯ä»¥æ˜¾è‘—æé«˜ä»»åŠ¡æ‰§è¡Œçš„æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼Œæœªæ¥å¯èƒ½å¯¹æ™ºèƒ½ç³»ç»Ÿçš„è‡ªä¸»å†³ç­–èƒ½åŠ›äº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Robust coordination is critical for effective decision-making in multi-agent systems, especially under partial observability. A central question in Multi-Agent Reinforcement Learning (MARL) is whether to engineer communication protocols or learn them end-to-end. We investigate this dichotomy using embodied world models. We propose and compare two communication strategies for a cooperative task-allocation problem. The first, Learned Direct Communication (LDC), learns a protocol end-to-end. The second, Intention Communication, uses an engineered inductive bias: a compact, learned world model, the Imagined Trajectory Generation Module (ITGM), which uses the agent's own policy to simulate future states. A Message Generation Network (MGN) then compresses this plan into a message. We evaluate these approaches on goal-directed interaction in a grid world, a canonical abstraction for embodied AI problems, while scaling environmental complexity. Our experiments reveal that while emergent communication is viable in simple settings, the engineered, world model-based approach shows superior performance, sample efficiency, and scalability as complexity increases. These findings advocate for integrating structured, predictive models into MARL agents to enable active, goal-driven coordination.

