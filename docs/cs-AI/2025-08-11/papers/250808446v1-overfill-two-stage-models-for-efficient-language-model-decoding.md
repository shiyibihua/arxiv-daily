---
layout: default
title: OverFill: Two-Stage Models for Efficient Language Model Decoding
---

# OverFill: Two-Stage Models for Efficient Language Model Decoding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.08446" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.08446v1</a>
  <a href="https://arxiv.org/pdf/2508.08446.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.08446v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.08446v1', 'OverFill: Two-Stage Models for Efficient Language Model Decoding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Woojeong Kim, Junxiong Wang, Jing Nathan Yan, Mohamed Abdelfattah, Alexander M. Rush

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-11

**å¤‡æ³¨**: Accepted to COLM 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/friendshipkim/overfill)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOverFillä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹è§£ç æ•ˆç‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­è¨€æ¨¡å‹` `æ¨ç†æ•ˆç‡` `è§£ç ä¼˜åŒ–` `ç¨ å¯†å‰ªæ` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ¨¡å‹è®­ç»ƒ` `ç”Ÿæˆè´¨é‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§£ç å™¨æ¨¡å‹æœªèƒ½æœ‰æ•ˆåŒºåˆ†é¢„å¡«å……å’Œè§£ç é˜¶æ®µçš„è®¡ç®—ç‰¹æ€§ï¼Œå¯¼è‡´æ¨ç†æ•ˆç‡ä½ä¸‹ã€‚
2. OverFillé€šè¿‡è§£è€¦é¢„å¡«å……å’Œè§£ç é˜¶æ®µï¼Œä½¿ç”¨å®Œæ•´æ¨¡å‹è¿›è¡Œé¢„å¡«å……ï¼Œéšååˆ‡æ¢åˆ°ç¨ å¯†å‰ªææ¨¡å‹ä»¥æé«˜ç”Ÿæˆè´¨é‡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒOverFillåœ¨å¤šä¸ªæ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—è¶…è¶Šäº†åŒè§„æ¨¡çš„å‰ªææ¨¡å‹ï¼Œä¸”ä½¿ç”¨çš„è®­ç»ƒæ•°æ®æ˜¾è‘—å‡å°‘ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ç”±äºé«˜æ¨ç†æˆæœ¬é¢ä¸´æ˜¾è‘—çš„éƒ¨ç½²æŒ‘æˆ˜ã€‚LLMæ¨ç†åŒ…æ‹¬é¢„å¡«å……ï¼ˆè®¡ç®—å¯†é›†å‹ï¼‰å’Œè§£ç ï¼ˆå†…å­˜å¯†é›†å‹ï¼‰ä¸¤ä¸ªé˜¶æ®µï¼Œå…¶ä¸­è§£ç é˜¶æ®µåœ¨é•¿åºåˆ—ä¸­å ä¸»å¯¼åœ°ä½ã€‚å½“å‰çš„è§£ç å™¨æ¨¡å‹å¯¹è¿™ä¸¤ä¸ªé˜¶æ®µçš„å¤„ç†æ–¹å¼ç›¸åŒï¼Œæœªèƒ½è€ƒè™‘å…¶ä¸åŒçš„è®¡ç®—ç‰¹æ€§ã€‚æœ¬æ–‡æå‡ºçš„OverFillé€šè¿‡è§£è€¦è¿™ä¸¤ä¸ªé˜¶æ®µæ¥ä¼˜åŒ–å‡†ç¡®æ€§ä¸æ•ˆç‡çš„æƒè¡¡ã€‚OverFillé¦–å…ˆä½¿ç”¨å®Œæ•´æ¨¡å‹è¿›è¡Œé¢„å¡«å……ï¼Œå¹¶å¹¶è¡Œå¤„ç†ç³»ç»Ÿå’Œç”¨æˆ·è¾“å…¥ï¼Œç„¶ååœ¨ç”Ÿæˆä»¤ç‰Œæ—¶åˆ‡æ¢åˆ°ç¨ å¯†å‰ªææ¨¡å‹ã€‚é€šè¿‡åœ¨é¢„å¡«å……é˜¶æ®µåˆ©ç”¨æ›´å¤šè®¡ç®—èµ„æºï¼ŒOverFillåœ¨ä¿æŒæœ€ä½å»¶è¿Ÿå¼€é”€çš„åŒæ—¶æé«˜äº†ç”Ÿæˆè´¨é‡ã€‚æˆ‘ä»¬çš„3B-to-1Bé…ç½®åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­æ¯”1Bå‰ªææ¨¡å‹æå‡äº†83.2%ï¼Œè€Œ8B-to-3Bé…ç½®åˆ™åœ¨3Bå‰ªææ¨¡å‹ä¸Šå¹³å‡æå‡äº†79.2%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­ç”±äºè§£ç é˜¶æ®µçš„å†…å­˜å¯†é›†å‹ç‰¹æ€§å¯¼è‡´çš„æ•ˆç‡ä½ä¸‹é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆåŒºåˆ†é¢„å¡«å……å’Œè§£ç é˜¶æ®µçš„è®¡ç®—éœ€æ±‚ï¼Œé€ æˆå»¶è¿Ÿå’Œèµ„æºæµªè´¹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šOverFillçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†æ¨ç†è¿‡ç¨‹ä¸­çš„é¢„å¡«å……å’Œè§£ç é˜¶æ®µè§£è€¦ï¼Œåˆ©ç”¨å®Œæ•´æ¨¡å‹è¿›è¡Œé¢„å¡«å……ä»¥æé«˜ç”Ÿæˆè´¨é‡ï¼Œç„¶ååœ¨è§£ç é˜¶æ®µåˆ‡æ¢åˆ°ç¨ å¯†å‰ªææ¨¡å‹ï¼Œä»è€Œä¼˜åŒ–å‡†ç¡®æ€§ä¸æ•ˆç‡çš„å¹³è¡¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šOverFillçš„æ•´ä½“æ¶æ„åˆ†ä¸ºä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆæ˜¯é¢„å¡«å……é˜¶æ®µï¼Œä½¿ç”¨å®Œæ•´æ¨¡å‹å¹¶è¡Œå¤„ç†è¾“å…¥ï¼›å…¶æ¬¡æ˜¯è§£ç é˜¶æ®µï¼Œé‡‡ç”¨ç¨ å¯†å‰ªææ¨¡å‹æŒ‰é¡ºåºç”Ÿæˆä»¤ç‰Œã€‚

**å…³é”®åˆ›æ–°**ï¼šOverFillçš„åˆ›æ–°åœ¨äºå…¶è§£è€¦çš„æ¨ç†è¿‡ç¨‹ï¼Œå……åˆ†åˆ©ç”¨è®¡ç®—èµ„æºè¿›è¡Œé¢„å¡«å……ï¼Œä»è€Œåœ¨ä¿æŒä½å»¶è¿Ÿçš„åŒæ—¶æ˜¾è‘—æé«˜ç”Ÿæˆè´¨é‡ã€‚è¿™ä¸ç°æœ‰çš„ç»Ÿä¸€å¤„ç†æ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒOverFillé‡‡ç”¨äº†3B-to-1Bå’Œ8B-to-3Bä¸¤ç§é…ç½®ï¼Œåˆ†åˆ«åœ¨ä¸åŒçš„åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œä¸”åœ¨è®­ç»ƒæ•°æ®ä½¿ç”¨ä¸Šæ˜¾è‘—å‡å°‘ï¼Œä¼˜åŒ–äº†æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å®éªŒä¸­ï¼ŒOverFillçš„3B-to-1Bé…ç½®åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­æ¯”1Bå‰ªææ¨¡å‹æå‡äº†83.2%ï¼Œè€Œ8B-to-3Bé…ç½®åˆ™åœ¨3Bå‰ªææ¨¡å‹ä¸Šå¹³å‡æå‡äº†79.2%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒOverFillä¸ä»…åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†åŒè§„æ¨¡çš„æ¨¡å‹ï¼Œè¿˜æ˜¾è‘—å‡å°‘äº†è®­ç»ƒæ•°æ®çš„éœ€æ±‚ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

OverFillçš„ç ”ç©¶æˆæœåœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é«˜æ•ˆæ¨ç†çš„åœºæ™¯ä¸­ï¼Œå¦‚å¯¹è¯ç³»ç»Ÿã€æ–‡æœ¬ç”Ÿæˆå’Œæœºå™¨ç¿»è¯‘ç­‰ã€‚å…¶ä¼˜åŒ–çš„æ¨ç†æ•ˆç‡å’Œç”Ÿæˆè´¨é‡å°†æ¨åŠ¨æ›´å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„å®é™…åº”ç”¨ï¼Œé™ä½éƒ¨ç½²æˆæœ¬ï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) excel across diverse tasks but face significant deployment challenges due to high inference costs. LLM inference comprises prefill (compute-bound) and decode (memory-bound) stages, with decode dominating latency particularly for long sequences. Current decoder-only models handle both stages uniformly, despite their distinct computational profiles. We propose OverFill, which decouples these stages to optimize accuracy-efficiency tradeoffs. OverFill begins with a full model for prefill, processing system and user inputs in parallel. It then switches to a dense pruned model, while generating tokens sequentially. Leveraging more compute during prefill, OverFill improves generation quality with minimal latency overhead. Our 3B-to-1B OverFill configuration outperforms 1B pruned models by 83.2%, while the 8B-to-3B configuration improves over 3B pruned models by 79.2% on average across standard benchmarks. OverFill matches the performance of same-sized models trained from scratch, while using significantly less training data. Our code is available at https://github.com/friendshipkim/overfill.

