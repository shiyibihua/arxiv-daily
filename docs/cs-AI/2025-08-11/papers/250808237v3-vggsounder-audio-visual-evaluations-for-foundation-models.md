---
layout: default
title: VGGSounder: Audio-Visual Evaluations for Foundation Models
---

# VGGSounder: Audio-Visual Evaluations for Foundation Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.08237" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.08237v3</a>
  <a href="https://arxiv.org/pdf/2508.08237.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.08237v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.08237v3', 'VGGSounder: Audio-Visual Evaluations for Foundation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Daniil Zverev, ThaddÃ¤us Wiedemer, Ameya Prabhu, Matthias Bethge, Wieland Brendel, A. Sophia Koepke

**åˆ†ç±»**: cs.MM, cs.AI, cs.CV, cs.SD, eess.AS

**å‘å¸ƒæ—¥æœŸ**: 2025-08-11 (æ›´æ–°: 2025-10-18)

**å¤‡æ³¨**: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVGGSounderä»¥è§£å†³VGGSoundæ•°æ®é›†çš„è¯„ä¼°å±€é™æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€ç†è§£` `éŸ³é¢‘-è§†è§‰æ¨¡å‹` `æ•°æ®é›†é‡æ ‡æ³¨` `æ€§èƒ½è¯„ä¼°` `æ¨¡æ€æ··æ·†æŒ‡æ ‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„VGGSoundæ•°æ®é›†åœ¨æ ‡ç­¾å®Œæ•´æ€§å’Œæ¨¡æ€å¯¹é½æ–¹é¢å­˜åœ¨æ˜æ˜¾ä¸è¶³ï¼Œå½±å“äº†å¤šæ¨¡æ€æ¨¡å‹çš„è¯„ä¼°å‡†ç¡®æ€§ã€‚
2. VGGSounderé€šè¿‡å…¨é¢é‡æ–°æ ‡æ³¨ï¼Œæä¾›äº†å¤šæ ‡ç­¾æµ‹è¯•é›†ï¼Œä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°éŸ³é¢‘-è§†è§‰åŸºç¡€æ¨¡å‹çš„æ€§èƒ½ã€‚
3. é€šè¿‡å¼•å…¥æ¨¡æ€æ··æ·†æŒ‡æ ‡ï¼Œç ”ç©¶æ­ç¤ºäº†æ¨¡å‹åœ¨å¤šæ¨¡æ€è¾“å…¥ä¸‹çš„æ€§èƒ½ä¸‹é™ï¼Œæä¾›äº†æ›´æ·±å…¥çš„åˆ†æå·¥å…·ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éŸ³é¢‘-è§†è§‰åŸºç¡€æ¨¡å‹çš„å‡ºç°å¼ºè°ƒäº†å¯é è¯„ä¼°å…¶å¤šæ¨¡æ€ç†è§£çš„é‡è¦æ€§ã€‚VGGSoundæ•°æ®é›†é€šå¸¸ç”¨ä½œéŸ³é¢‘-è§†è§‰åˆ†ç±»çš„åŸºå‡†ï¼Œä½†æˆ‘ä»¬çš„åˆ†æå‘ç°VGGSoundå­˜åœ¨å¤šé¡¹å±€é™æ€§ï¼ŒåŒ…æ‹¬æ ‡ç­¾ä¸å®Œæ•´ã€ç±»éƒ¨åˆ†é‡å ä»¥åŠæ¨¡æ€ä¸å¯¹é½ã€‚è¿™äº›é—®é¢˜å¯¼è‡´å¯¹å¬è§‰å’Œè§†è§‰èƒ½åŠ›çš„è¯„ä¼°å¤±çœŸã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†VGGSounderï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢é‡æ–°æ ‡æ³¨çš„å¤šæ ‡ç­¾æµ‹è¯•é›†ï¼Œæ—¨åœ¨è¯„ä¼°éŸ³é¢‘-è§†è§‰åŸºç¡€æ¨¡å‹ã€‚VGGSounderå…·æœ‰è¯¦ç»†çš„æ¨¡æ€æ³¨é‡Šï¼Œèƒ½å¤Ÿç²¾ç¡®åˆ†ææ¨¡æ€ç‰¹å®šçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡å¼•å…¥æ–°çš„æ¨¡æ€æ··æ·†æŒ‡æ ‡ï¼Œæ­ç¤ºäº†æ¨¡å‹åœ¨æ·»åŠ å…¶ä»–è¾“å…¥æ¨¡æ€æ—¶çš„æ€§èƒ½ä¸‹é™ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³VGGSoundæ•°æ®é›†åœ¨è¯„ä¼°éŸ³é¢‘-è§†è§‰åŸºç¡€æ¨¡å‹æ—¶å­˜åœ¨çš„æ ‡ç­¾ä¸å®Œæ•´ã€ç±»é‡å å’Œæ¨¡æ€ä¸å¯¹é½ç­‰é—®é¢˜ï¼Œè¿™äº›é—®é¢˜å¯¼è‡´äº†è¯„ä¼°ç»“æœçš„å¤±çœŸã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºVGGSounderä½œä¸ºä¸€ä¸ªå…¨é¢é‡æ–°æ ‡æ³¨çš„å¤šæ ‡ç­¾æµ‹è¯•é›†ï¼Œæ—¨åœ¨æä¾›æ›´å‡†ç¡®çš„è¯„ä¼°å·¥å…·ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹éŸ³é¢‘-è§†è§‰æ¨¡å‹çš„å¤šæ¨¡æ€ç†è§£èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVGGSounderçš„æ„å»ºåŒ…æ‹¬è¯¦ç»†çš„æ¨¡æ€æ³¨é‡Šï¼Œå…è®¸å¯¹ä¸åŒæ¨¡æ€çš„æ€§èƒ½è¿›è¡Œç²¾ç¡®åˆ†æã€‚æ•´ä½“æµç¨‹åŒ…æ‹¬æ•°æ®æ”¶é›†ã€æ ‡æ³¨ã€éªŒè¯å’Œæ€§èƒ½è¯„ä¼°ç­‰å¤šä¸ªé˜¶æ®µã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†æ¨¡æ€æ··æ·†æŒ‡æ ‡ï¼Œèƒ½å¤Ÿæ­ç¤ºæ¨¡å‹åœ¨å¢åŠ è¾“å…¥æ¨¡æ€æ—¶çš„æ€§èƒ½ä¸‹é™ï¼Œè¿™åœ¨ç°æœ‰æ–¹æ³•ä¸­å°šæœªå¾—åˆ°å……åˆ†æ¢è®¨ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°æ®æ ‡æ³¨è¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†å¤šæ ‡ç­¾æ ‡æ³¨ç­–ç•¥ï¼Œç¡®ä¿æ¯ä¸ªæ ·æœ¬èƒ½å¤Ÿå‡†ç¡®åæ˜ å…¶å¤šæ¨¡æ€ç‰¹æ€§ï¼ŒåŒæ—¶åœ¨æ€§èƒ½è¯„ä¼°ä¸­å¼•å…¥äº†æ–°çš„æŸå¤±å‡½æ•°ä»¥é€‚åº”å¤šæ¨¡æ€åˆ†æã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

é€šè¿‡ä½¿ç”¨VGGSounderè¿›è¡Œè¯„ä¼°ï¼Œæ¨¡å‹åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸Šçš„æ€§èƒ½å¾—åˆ°äº†æ˜¾è‘—æå‡ï¼Œå°¤å…¶æ˜¯åœ¨æ¨¡æ€å¯¹é½å’Œæ ‡ç­¾å®Œæ•´æ€§æ–¹é¢ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œç›¸è¾ƒäºä¼ ç»Ÿçš„VGGSoundæ•°æ®é›†ï¼Œæä¾›äº†æ›´ä¸ºå¯é çš„è¯„ä¼°ç»“æœã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

VGGSounderçš„ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºå¤šæ¨¡æ€å­¦ä¹ ã€éŸ³é¢‘-è§†è§‰ç†è§£ç­‰é¢†åŸŸï¼Œå°¤å…¶æ˜¯åœ¨æ™ºèƒ½ç›‘æ§ã€è‡ªåŠ¨é©¾é©¶å’Œäººæœºäº¤äº’ç­‰å®é™…åº”ç”¨ä¸­ï¼Œèƒ½å¤Ÿæå‡æ¨¡å‹çš„å¤šæ¨¡æ€ç†è§£èƒ½åŠ›ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The emergence of audio-visual foundation models underscores the importance of reliably assessing their multi-modal understanding. The VGGSound dataset is commonly used as a benchmark for evaluation audio-visual classification. However, our analysis identifies several limitations of VGGSound, including incomplete labelling, partially overlapping classes, and misaligned modalities. These lead to distorted evaluations of auditory and visual capabilities. To address these limitations, we introduce VGGSounder, a comprehensively re-annotated, multi-label test set that extends VGGSound and is specifically designed to evaluate audio-visual foundation models. VGGSounder features detailed modality annotations, enabling precise analyses of modality-specific performance. Furthermore, we reveal model limitations by analysing performance degradation when adding another input modality with our new modality confusion metric.

