---
layout: default
title: A Data-driven ML Approach for Maximizing Performance in LLM-Adapter Serving
---

# A Data-driven ML Approach for Maximizing Performance in LLM-Adapter Serving

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.08343" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.08343v3</a>
  <a href="https://arxiv.org/pdf/2508.08343.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.08343v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.08343v3', 'A Data-driven ML Approach for Maximizing Performance in LLM-Adapter Serving')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ferran Agullo, Joan Oliveras, Chen Wang, Alberto Gutierrez-Torre, Olivier Tardieu, Alaa Youssef, Jordi Torres, Josep Ll. Berral

**åˆ†ç±»**: cs.PF, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-11 (æ›´æ–°: 2025-11-19)

**å¤‡æ³¨**: Accepted in a computer science workshop

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/FerranAgulloLopez/GPULLMAdapterOptimization)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ•°æ®é©±åŠ¨çš„æœºå™¨å­¦ä¹ æ–¹æ³•ä»¥ä¼˜åŒ–LLMé€‚é…å™¨æœåŠ¡æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `é€‚é…å™¨ä¼˜åŒ–` `æœºå™¨å­¦ä¹ ` `æ•°å­—åŒèƒèƒ` `GPUæ€§èƒ½` `è¯·æ±‚èšåˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨æœåŠ¡å¤šä¸ªLLMé€‚é…å™¨æ—¶å®¹æ˜“å¯¼è‡´GPUå†…å­˜è¶…é™ï¼Œä»è€Œå¼•å‘è¯·æ±‚é¥¥é¥¿ï¼Œå½±å“ç³»ç»Ÿæ€§èƒ½ã€‚
2. æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ•°æ®é©±åŠ¨çš„æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡å¯è§£é‡Šæ¨¡å‹ä¼˜åŒ–å¹¶å‘å’Œå¹¶è¡Œé€‚é…å™¨çš„é…ç½®ï¼Œä»¥æé«˜GPUååé‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ•°å­—åŒèƒèƒæ¨¡å‹çš„ååé‡ä¸çœŸå®ç»“æœçš„è¯¯å·®ä»…ä¸º5.1%ï¼Œè€Œæœºå™¨å­¦ä¹ æ–¹æ³•çš„é¢„æµ‹è¯¯å·®ä¸è¶…è¿‡7.2%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿæ™®åŠï¼ŒLLMé€‚é…å™¨é€æ¸æˆä¸ºä¸€ç§å¸¸è§çš„è½»é‡çº§æ¨¡å‹ä¸“é—¨åŒ–å·¥å…·ã€‚åœ¨å•ä¸ªGPUä¸ŠåŒæ—¶æœåŠ¡æ•°ç™¾æˆ–æ•°åƒä¸ªé€‚é…å™¨å¯ä»¥å®ç°è¯·æ±‚èšåˆï¼Œæé«˜ååé‡ï¼Œä½†å¦‚æœè¶…å‡ºGPUå†…å­˜é™åˆ¶ï¼Œå¯èƒ½å¯¼è‡´è¯·æ±‚é¥¥é¥¿ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬ç ”ç©¶ä¸“æ³¨äºç¡®å®šå¹¶å‘å’Œå¹¶è¡Œé€‚é…å™¨çš„è”åˆé…ç½®ï¼Œä»¥æœ€å¤§åŒ–GPUååé‡è€Œä¸å¼•å‘é¥¥é¥¿ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ•°æ®é©±åŠ¨çš„æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œåˆ©ç”¨å¯è§£é‡Šæ¨¡å‹æ¥è§£å†³è¿™ä¸€ç¼“å­˜é—®é¢˜ï¼Œå¹¶å¼•å…¥äº†é¦–ä¸ªèƒ½å¤Ÿé‡ç°LLMé€‚é…å™¨æœåŠ¡ç³»ç»Ÿçš„æ•°å­—åŒèƒèƒï¼Œä»è€Œå®ç°é«˜æ•ˆçš„è®­ç»ƒæ•°æ®ç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ•°å­—åŒèƒèƒçš„ååé‡ä¸çœŸå®ç»“æœçš„è¯¯å·®åœ¨5.1%ä»¥å†…ï¼Œè€Œæœºå™¨å­¦ä¹ æ–¹æ³•åœ¨å¼‚æ„çœŸå®å·¥ä½œè´Ÿè½½ä¸‹é¢„æµ‹çš„å¹¶å‘å’Œå¹¶è¡Œé€‚é…å™¨çš„æœ€ä¼˜æ•°é‡è¯¯å·®æœ€å¤šä¸º7.2%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³åœ¨å•ä¸ªGPUä¸ŠåŒæ—¶æœåŠ¡å¤šä¸ªLLMé€‚é…å™¨æ—¶ï¼Œç”±äºå†…å­˜é™åˆ¶å¯¼è‡´çš„è¯·æ±‚é¥¥é¥¿é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¼‚æ„é€‚é…å™¨å’Œæµé‡ç‰¹æ€§æ—¶ï¼Œå¾€å¾€æ— æ³•æœ‰æ•ˆé…ç½®é€‚é…å™¨ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§æ•°æ®é©±åŠ¨çš„æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œåˆ©ç”¨å¯è§£é‡Šæ¨¡å‹æ¥ä¼˜åŒ–é€‚é…å™¨çš„å¹¶å‘å’Œå¹¶è¡Œé…ç½®ï¼Œä»è€Œæœ€å¤§åŒ–GPUçš„ååé‡ï¼Œé¿å…è¯·æ±‚é¥¥é¥¿ã€‚é€šè¿‡å¼•å…¥æ•°å­—åŒèƒèƒæŠ€æœ¯ï¼Œèƒ½å¤Ÿé«˜æ•ˆç”Ÿæˆè®­ç»ƒæ•°æ®ï¼Œæå‡æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€æ¨¡å‹è®­ç»ƒå’Œæ€§èƒ½è¯„ä¼°ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡æ•°å­—åŒèƒèƒæ¨¡æ‹ŸLLMé€‚é…å™¨æœåŠ¡ç³»ç»Ÿï¼Œç”Ÿæˆè®­ç»ƒæ•°æ®ï¼›ç„¶åï¼Œåˆ©ç”¨æœºå™¨å­¦ä¹ æ¨¡å‹è¿›è¡Œé€‚é…å™¨é…ç½®çš„ä¼˜åŒ–ï¼›æœ€åï¼Œé€šè¿‡å®éªŒéªŒè¯æ¨¡å‹çš„é¢„æµ‹æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†æ•°å­—åŒèƒèƒæŠ€æœ¯ï¼Œèƒ½å¤Ÿå‡†ç¡®é‡ç°LLMé€‚é…å™¨æœåŠ¡ç³»ç»Ÿï¼Œå¹¶ç»“åˆå¯è§£é‡Šæœºå™¨å­¦ä¹ æ¨¡å‹è¿›è¡Œä¼˜åŒ–ã€‚è¿™ä¸€æ–¹æ³•åœ¨å¤„ç†å¼‚æ„å·¥ä½œè´Ÿè½½æ—¶è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥å¹³è¡¡ååé‡å’Œè¯·æ±‚é¥¥é¥¿çš„é£é™©ï¼ŒåŒæ—¶å¯¹é€‚é…å™¨çš„å¹¶å‘å’Œå¹¶è¡Œæ•°é‡è¿›è¡Œäº†ç³»ç»Ÿçš„å‚æ•°è°ƒä¼˜ï¼Œä»¥ç¡®ä¿æ¨¡å‹çš„é¢„æµ‹ç²¾åº¦å’Œå®ç”¨æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ•°å­—åŒèƒèƒæ¨¡å‹çš„ååé‡ä¸çœŸå®ç»“æœçš„è¯¯å·®ä»…ä¸º5.1%ï¼Œè€Œæœºå™¨å­¦ä¹ æ–¹æ³•åœ¨å¼‚æ„çœŸå®å·¥ä½œè´Ÿè½½ä¸‹é¢„æµ‹çš„å¹¶å‘å’Œå¹¶è¡Œé€‚é…å™¨çš„æœ€ä¼˜æ•°é‡è¯¯å·®ä¸è¶…è¿‡7.2%ã€‚è¿™äº›ç»“æœæ˜¾ç¤ºäº†è¯¥æ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§å’Œå¯é æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬äº‘è®¡ç®—æœåŠ¡ã€æ™ºèƒ½åŠ©æ‰‹å’Œè‡ªç„¶è¯­è¨€å¤„ç†ç­‰åœºæ™¯ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„æœåŠ¡æ€§èƒ½ã€‚é€šè¿‡ä¼˜åŒ–é€‚é…å™¨é…ç½®ï¼Œå¯ä»¥åœ¨èµ„æºæœ‰é™çš„æƒ…å†µä¸‹å®ç°æ›´é«˜çš„ååé‡ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> With the rapid adoption of Large Language Models (LLMs), LLM-adapters have become increasingly common, providing lightweight specialization of large-scale models. Serving hundreds or thousands of these adapters on a single GPU allows request aggregation, increasing throughput, but may also cause request starvation if GPU memory limits are exceeded. To address this issue, this study focuses on determining the joint configuration of concurrent and parallel adapters that maximizes GPU throughput without inducing starvation, given heterogeneous adapter and traffic properties. We propose a data-driven ML approach leveraging interpretable models to tackle this caching problem and introduce the first Digital Twin capable of reproducing an LLM-adapter serving system, enabling efficient training data generation. Experiments with the vLLM framework and LoRA adapters show that the Digital Twin reproduces throughput within 5.1% of real results, while the ML approach predicts optimal numbers of concurrent and parallel adapters with an error of at most 7.2% under heterogeneous, real-world workloads. The code is publicly available at https://github.com/FerranAgulloLopez/GPULLMAdapterOptimization.

