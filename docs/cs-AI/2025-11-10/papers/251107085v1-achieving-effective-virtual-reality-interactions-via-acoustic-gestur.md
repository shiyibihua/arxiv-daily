---
layout: default
title: Achieving Effective Virtual Reality Interactions via Acoustic Gesture Recognition based on Large Language Models
---

# Achieving Effective Virtual Reality Interactions via Acoustic Gesture Recognition based on Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.07085" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.07085v1</a>
  <a href="https://arxiv.org/pdf/2511.07085.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.07085v1" onclick="toggleFavorite(this, '2511.07085v1', 'Achieving Effective Virtual Reality Interactions via Acoustic Gesture Recognition based on Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xijie Zhang, Fengliang He, Hong-Ning Dai

**åˆ†ç±»**: cs.HC, cs.AI, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-10

**å¤‡æ³¨**: 5 pages, 4 figures, 1 table, under review at ICASSP 2026

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„å£°å­¦æ‰‹åŠ¿è¯†åˆ«æ¡†æ¶ï¼Œç”¨äºé«˜æ•ˆè™šæ‹Ÿç°å®äº¤äº’**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation)**

**å…³é”®è¯**: `å£°å­¦æ‰‹åŠ¿è¯†åˆ«` `å¤§è¯­è¨€æ¨¡å‹` `è™šæ‹Ÿç°å®` `å¢å¼ºç°å®` `ä¿¡é“å†²æ¿€å“åº”` `å°‘æ ·æœ¬å­¦ä¹ ` `äººæœºäº¤äº’`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è§†è§‰æ‰‹åŠ¿è¯†åˆ«åœ¨VR/ARä¸­å­˜åœ¨è®¡ç®—é‡å¤§ã€å…‰ç…§æ•æ„Ÿå’Œéšç§æ³„éœ²ç­‰é—®é¢˜ï¼Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚
2. åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹å¤„ç†å£°å­¦ä¿¡å·ï¼ˆCIRï¼‰è¿›è¡Œæ‰‹åŠ¿è¯†åˆ«ï¼Œæ— éœ€å¤§é‡è®­ç»ƒæ•°æ®ï¼Œé€‚ç”¨äºå°‘æ ·æœ¬åœºæ™¯ã€‚
3. é€šè¿‡æ”¶é›†å·®åˆ†CIRæ•°æ®å¹¶ç»“åˆLLMï¼Œå®ç°äº†ä¸ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•ç›¸å½“çš„è¯†åˆ«ç²¾åº¦ï¼Œæ— éœ€é¢†åŸŸç‰¹å®šé‡è®­ç»ƒã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è‡ªç„¶é«˜æ•ˆçš„äº¤äº’ä»ç„¶æ˜¯è™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®ï¼ˆVR/ARï¼‰ç³»ç»Ÿé¢ä¸´çš„å…³é”®æŒ‘æˆ˜ã€‚åŸºäºè§†è§‰çš„æ‰‹åŠ¿è¯†åˆ«è®¡ç®—æˆæœ¬é«˜ï¼Œå¯¹å…‰ç…§æ¡ä»¶æ•æ„Ÿï¼Œå¹¶å­˜åœ¨éšç§æ³„éœ²çš„æ‹…å¿§ã€‚å£°å­¦ä¼ æ„Ÿæä¾›äº†ä¸€ç§æœ‰å¸å¼•åŠ›çš„æ›¿ä»£æ–¹æ¡ˆï¼šé€šè¿‡å‘å°„ä¸å¯å¬çš„é«˜é¢‘ä¿¡å·å¹¶æ•è·å…¶åå°„ï¼Œä¿¡é“å†²æ¿€å“åº”ï¼ˆCIRï¼‰ä»¥ä½æˆæœ¬å’Œç”¨æˆ·é€æ˜çš„æ–¹å¼ç¼–ç æ‰‹åŠ¿å¦‚ä½•æ‰°åŠ¨å£°åœºã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºCIRçš„æ‰‹åŠ¿è¯†åˆ«æ–¹æ³•é€šå¸¸ä¾èµ–äºåœ¨å¤§å‹æ ‡è®°æ•°æ®é›†ä¸Šå¯¹æ¨¡å‹è¿›è¡Œå¤§é‡è®­ç»ƒï¼Œè¿™ä½¿å¾—å®ƒä»¬ä¸é€‚åˆå°‘æ ·æœ¬VRåœºæ™¯ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ç¬¬ä¸€ä¸ªåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡ŒVR/ARç³»ç»Ÿä¸­åŸºäºCIRçš„æ‰‹åŠ¿è¯†åˆ«çš„æ¡†æ¶ã€‚å°½ç®¡LLMå…·æœ‰ä¼˜åŠ¿ï¼Œä½†ç”±äºCIRæ‰‹åŠ¿çš„ä¸æ˜¾çœ¼ç‰¹å¾ï¼Œå®ç°CIRæ‰‹åŠ¿çš„å°‘æ ·æœ¬å’Œé›¶æ ·æœ¬å­¦ä¹ å¹¶éæ˜“äº‹ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ”¶é›†å·®åˆ†CIRæ•°æ®è€Œä¸æ˜¯åŸå§‹CIRæ•°æ®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªçœŸå®ä¸–ç•Œçš„æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†ç”±10åå‚ä¸è€…æ‰§è¡Œ15ä¸ªæ‰‹åŠ¿ï¼ˆè·¨è¶Šæ•°å­—ã€å­—æ¯å’Œå½¢çŠ¶ä¸‰ä¸ªç±»åˆ«ï¼‰ï¼Œæ¯ä¸ªæ‰‹åŠ¿é‡å¤10æ¬¡ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨é‡‡ç”¨LLMçš„åˆ†ç±»å™¨å¯¹è¯¥æ•°æ®é›†è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬åŸºäºLLMçš„æ¡†æ¶å®ç°äº†ä¸ç»å…¸æœºå™¨å­¦ä¹ åŸºçº¿ç›¸å½“çš„å‡†ç¡®ç‡ï¼ŒåŒæ—¶ä¸éœ€è¦ç‰¹å®šé¢†åŸŸçš„é‡æ–°è®­ç»ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºè§†è§‰çš„æ‰‹åŠ¿è¯†åˆ«æ–¹æ³•åœ¨VR/ARç¯å¢ƒä¸­å­˜åœ¨è®¡ç®—æˆæœ¬é«˜ã€å¯¹å…‰ç…§æ¡ä»¶æ•æ„Ÿä»¥åŠæ½œåœ¨çš„éšç§æ³„éœ²é—®é¢˜ã€‚è€ŒåŸºäºä¿¡é“å†²æ¿€å“åº”(CIR)çš„å£°å­¦æ‰‹åŠ¿è¯†åˆ«æ–¹æ³•è™½ç„¶å…·æœ‰ä½æˆæœ¬å’Œç”¨æˆ·é€æ˜çš„ä¼˜ç‚¹ï¼Œä½†é€šå¸¸éœ€è¦å¤§é‡çš„æ ‡æ³¨æ•°æ®è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œè¿™åœ¨å®é™…åº”ç”¨ä¸­éš¾ä»¥æ»¡è¶³ã€‚å› æ­¤ï¼Œå¦‚ä½•åˆ©ç”¨å°‘é‡æ•°æ®å®ç°é«˜æ•ˆçš„å£°å­¦æ‰‹åŠ¿è¯†åˆ«æ˜¯æœ¬æ–‡è¦è§£å†³çš„æ ¸å¿ƒé—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLM)å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›å’ŒçŸ¥è¯†è¿ç§»èƒ½åŠ›ï¼Œå°†LLMåº”ç”¨äºåŸºäºCIRçš„å£°å­¦æ‰‹åŠ¿è¯†åˆ«ä»»åŠ¡ä¸­ã€‚é€šè¿‡å°†CIRæ•°æ®è½¬æ¢ä¸ºLLMå¯ä»¥ç†è§£çš„è¾“å…¥å½¢å¼ï¼Œå¹¶åˆ©ç”¨LLMè¿›è¡Œç‰¹å¾æå–å’Œåˆ†ç±»ï¼Œä»è€Œå®ç°å°‘æ ·æœ¬ç”šè‡³é›¶æ ·æœ¬çš„æ‰‹åŠ¿è¯†åˆ«ã€‚è¿™æ ·å¯ä»¥é¿å…ä¼ ç»Ÿæ–¹æ³•å¯¹å¤§é‡æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œé€‚åº”æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶ä¸»è¦åŒ…å«æ•°æ®é‡‡é›†ã€æ•°æ®é¢„å¤„ç†å’ŒLLMåˆ†ç±»å™¨ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡å£°å­¦ä¼ æ„Ÿå™¨é‡‡é›†ç”¨æˆ·è¿›è¡Œæ‰‹åŠ¿æ“ä½œæ—¶çš„CIRæ•°æ®ã€‚ç„¶åï¼Œå¯¹CIRæ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼ŒåŒ…æ‹¬å·®åˆ†CIRè®¡ç®—ï¼Œä»¥å¢å¼ºæ‰‹åŠ¿ç‰¹å¾ã€‚æœ€åï¼Œå°†é¢„å¤„ç†åçš„æ•°æ®è¾“å…¥åˆ°åŸºäºLLMçš„åˆ†ç±»å™¨ä¸­è¿›è¡Œæ‰‹åŠ¿è¯†åˆ«ã€‚åˆ†ç±»å™¨åˆ©ç”¨LLMå¼ºå¤§çš„ç‰¹å¾æå–å’Œåˆ†ç±»èƒ½åŠ›ï¼Œå®ç°é«˜æ•ˆçš„æ‰‹åŠ¿è¯†åˆ«ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå°†å¤§è¯­è¨€æ¨¡å‹åº”ç”¨äºåŸºäºCIRçš„å£°å­¦æ‰‹åŠ¿è¯†åˆ«ä»»åŠ¡ä¸­ã€‚ä¸ä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•ä¸éœ€è¦å¤§é‡çš„æ ‡æ³¨æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå¹¶ä¸”å…·æœ‰æ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›å’Œé€‚åº”æ€§ã€‚æ­¤å¤–ï¼Œé€šè¿‡æ”¶é›†å·®åˆ†CIRæ•°æ®ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å¢å¼ºæ‰‹åŠ¿ç‰¹å¾ï¼Œæé«˜è¯†åˆ«ç²¾åº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­å…³é”®çš„è®¾è®¡åŒ…æ‹¬ï¼š1) é‡‡ç”¨å·®åˆ†CIRæ•°æ®ï¼Œè€ŒéåŸå§‹CIRæ•°æ®ï¼Œä»¥çªå‡ºæ‰‹åŠ¿å˜åŒ–å¸¦æ¥çš„å½±å“ã€‚2) æ„å»ºäº†ä¸€ä¸ªåŒ…å«10åå‚ä¸è€…ã€15ç§æ‰‹åŠ¿çš„æ•°æ®é›†ï¼Œæ¶µç›–æ•°å­—ã€å­—æ¯å’Œå½¢çŠ¶ä¸‰ç§ç±»åˆ«ã€‚3) ä½¿ç”¨LLMä½œä¸ºåˆ†ç±»å™¨ï¼Œå¹¶é’ˆå¯¹CIRæ•°æ®çš„ç‰¹ç‚¹è¿›è¡Œäº†é€‚å½“çš„è°ƒæ•´å’Œä¼˜åŒ–ã€‚å…·ä½“çš„LLMå‚æ•°è®¾ç½®å’Œè®­ç»ƒç­–ç•¥åœ¨è®ºæ–‡ä¸­æœªè¯¦ç»†è¯´æ˜ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥åŸºäºLLMçš„å£°å­¦æ‰‹åŠ¿è¯†åˆ«æ¡†æ¶åœ¨è‡ªå»ºæ•°æ®é›†ä¸Šå–å¾—äº†ä¸ä¼ ç»Ÿæœºå™¨å­¦ä¹ åŸºçº¿æ–¹æ³•ç›¸å½“çš„å‡†ç¡®ç‡ï¼ŒåŒæ—¶æ— éœ€é¢†åŸŸç‰¹å®šçš„é‡æ–°è®­ç»ƒã€‚è¿™è¡¨æ˜LLMåœ¨å£°å­¦æ‰‹åŠ¿è¯†åˆ«é¢†åŸŸå…·æœ‰å·¨å¤§çš„æ½œåŠ›ï¼Œå¯ä»¥åœ¨å°‘æ ·æœ¬ç”šè‡³é›¶æ ·æœ¬çš„æƒ…å†µä¸‹å®ç°é«˜æ•ˆçš„æ‰‹åŠ¿è¯†åˆ«ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œæå‡å¹…åº¦åœ¨è®ºæ–‡ä¸­æœªè¯¦ç»†è¯´æ˜ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºè™šæ‹Ÿç°å®ã€å¢å¼ºç°å®ã€æ™ºèƒ½å®¶å±…ã€å¯ç©¿æˆ´è®¾å¤‡ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡ç®€å•çš„æ‰‹åŠ¿ä¸VR/ARç¯å¢ƒè¿›è¡Œäº¤äº’ï¼Œæ— éœ€ä½©æˆ´å¤æ‚çš„æ‰‹å¥—æˆ–ä½¿ç”¨æ§åˆ¶å™¨ã€‚åœ¨æ™ºèƒ½å®¶å±…ä¸­ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡æ‰‹åŠ¿æ§åˆ¶å®¶ç”µè®¾å¤‡ã€‚åœ¨å¯ç©¿æˆ´è®¾å¤‡ä¸­ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡æ‰‹åŠ¿è¿›è¡Œå¿«æ·æ“ä½œï¼Œæé«˜è®¾å¤‡çš„æ˜“ç”¨æ€§ã€‚è¯¥æŠ€æœ¯å…·æœ‰ä½æˆæœ¬ã€é«˜æ•ˆç‡å’Œè‰¯å¥½çš„ç”¨æˆ·ä½“éªŒç­‰ä¼˜ç‚¹ï¼Œå…·æœ‰å¹¿é˜”çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Natural and efficient interaction remains a critical challenge for virtual reality and augmented reality (VR/AR) systems. Vision-based gesture recognition suffers from high computational cost, sensitivity to lighting conditions, and privacy leakage concerns. Acoustic sensing provides an attractive alternative: by emitting inaudible high-frequency signals and capturing their reflections, channel impulse response (CIR) encodes how gestures perturb the acoustic field in a low-cost and user-transparent manner. However, existing CIR-based gesture recognition methods often rely on extensive training of models on large labeled datasets, making them unsuitable for few-shot VR scenarios. In this work, we propose the first framework that leverages large language models (LLMs) for CIR-based gesture recognition in VR/AR systems. Despite LLMs' strengths, it is non-trivial to achieve few-shot and zero-shot learning of CIR gestures due to their inconspicuous features. To tackle this challenge, we collect differential CIR rather than original CIR data. Moreover, we construct a real-world dataset collected from 10 participants performing 15 gestures across three categories (digits, letters, and shapes), with 10 repetitions each. We then conduct extensive experiments on this dataset using an LLM-adopted classifier. Results show that our LLM-based framework achieves accuracy comparable to classical machine learning baselines, while requiring no domain-specific retraining.

