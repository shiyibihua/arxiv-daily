---
layout: default
title: Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens
---

# Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.01191" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.01191v3</a>
  <a href="https://arxiv.org/pdf/2508.01191.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.01191v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.01191v3', 'Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chengshuai Zhao, Zhen Tan, Pingchuan Ma, Dawei Li, Bohan Jiang, Yancheng Wang, Yingzhen Yang, Huan Liu

**åˆ†ç±»**: cs.AI, cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-02 (æ›´æ–°: 2025-08-13)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é€šè¿‡æ•°æ®åˆ†å¸ƒè§†è§’æ¢è®¨å¤§è¯­è¨€æ¨¡å‹çš„æ€ç»´é“¾æ¨ç†å±€é™æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é“¾å¼æ€ç»´` `å¤§è¯­è¨€æ¨¡å‹` `æ•°æ®åˆ†å¸ƒ` `æ¨ç†èƒ½åŠ›` `å®éªŒç ”ç©¶` `ç»“æ„æ€§å½’çº³åå·®` `æ€§èƒ½è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é“¾å¼æ€ç»´æ¨ç†æ–¹æ³•åœ¨é¢å¯¹ä¸è®­ç»ƒæ•°æ®åˆ†å¸ƒä¸ä¸€è‡´çš„æµ‹è¯•æŸ¥è¯¢æ—¶è¡¨ç°å‡ºè„†å¼±æ€§ï¼Œç¼ºä¹çœŸæ­£çš„æ¨ç†èƒ½åŠ›ã€‚
2. è®ºæ–‡é€šè¿‡æ•°æ®åˆ†å¸ƒè§†è§’æ¢è®¨CoTæ¨ç†ï¼Œè®¾è®¡äº†DataAlchemyç¯å¢ƒä»¥ç³»ç»Ÿæ€§åœ°åˆ†æä¸åŒåˆ†å¸ƒæ¡ä»¶ä¸‹çš„LLMè¡¨ç°ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒCoTæ¨ç†åœ¨è¶…å‡ºè®­ç»ƒåˆ†å¸ƒæ—¶æ•ˆæœæ˜¾è‘—ä¸‹é™ï¼Œæ­ç¤ºäº†å…¶å±€é™æ€§å¹¶å¼ºè°ƒäº†æ¨ç†èƒ½åŠ›çš„å¯æ¨å¹¿æ€§é—®é¢˜ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºå·²è¢«è¯æ˜èƒ½æå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šç§ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚è¯¥æ–¹æ³•ä½¿LLMåœ¨ç»™å‡ºç­”æ¡ˆå‰ä¼¼ä¹èƒ½å¤Ÿäº§ç”Ÿç±»ä¼¼äººç±»çš„æ¨ç†æ­¥éª¤ã€‚ç„¶è€Œï¼Œåˆæ­¥ç ”ç©¶è¡¨æ˜ï¼ŒCoTæ¨ç†å¯èƒ½æ¯”è¡¨é¢ä¸Šçœ‹èµ·æ¥æ›´ä¸ºè‚¤æµ…ã€‚æœ¬æ–‡é€šè¿‡æ•°æ®åˆ†å¸ƒçš„è§†è§’ç ”ç©¶CoTæ¨ç†ï¼Œæ¢è®¨å…¶æ˜¯å¦åæ˜ äº†ä»è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ åˆ°çš„ç»“æ„æ€§å½’çº³åå·®ã€‚æˆ‘ä»¬è®¾è®¡äº†DataAlchemyï¼Œä¸€ä¸ªéš”ç¦»ä¸”å¯æ§çš„ç¯å¢ƒï¼Œä»¥ä»å¤´è®­ç»ƒLLMå¹¶åœ¨ä¸åŒåˆ†å¸ƒæ¡ä»¶ä¸‹ç³»ç»Ÿæ€§åœ°æ¢æµ‹å…¶æ€§èƒ½ã€‚ç»“æœæ˜¾ç¤ºï¼ŒCoTæ¨ç†åœ¨è¶…å‡ºè®­ç»ƒåˆ†å¸ƒæ—¶è¡¨ç°è„†å¼±ï¼Œå¼ºè°ƒäº†å®ç°çœŸæ­£å¯æ¨å¹¿æ¨ç†çš„æŒç»­æŒ‘æˆ˜ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨æ¢è®¨é“¾å¼æ€ç»´æ¨ç†åœ¨å¤§è¯­è¨€æ¨¡å‹ä¸­çš„æœ‰æ•ˆæ€§ï¼Œå°¤å…¶æ˜¯åœ¨è®­ç»ƒæ•°æ®ä¸æµ‹è¯•æŸ¥è¯¢åˆ†å¸ƒä¸ä¸€è‡´æ—¶çš„è¡¨ç°ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†æ­ç¤ºCoTæ¨ç†çš„æ·±å±‚æ¬¡æœºåˆ¶å’Œå±€é™æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æ•°æ®åˆ†å¸ƒçš„è§†è§’åˆ†æCoTæ¨ç†ï¼Œç ”ç©¶å…¶æ˜¯å¦åæ˜ äº†ä»è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ åˆ°çš„ç»“æ„æ€§å½’çº³åå·®ï¼Œè¿›è€Œå½±å“æ¨ç†è·¯å¾„çš„ç”Ÿæˆã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼š1) æ•°æ®åˆ†å¸ƒåˆ†æï¼Œ2) CoTæ¨ç†è·¯å¾„ç”Ÿæˆï¼Œ3) æ€§èƒ½è¯„ä¼°ã€‚é€šè¿‡åœ¨DataAlchemyç¯å¢ƒä¸­è®­ç»ƒLLMï¼Œç³»ç»Ÿæ€§æ¢æµ‹ä¸åŒåˆ†å¸ƒæ¡ä»¶ä¸‹çš„æ¨ç†èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°åœ¨äºé€šè¿‡æ•°æ®åˆ†å¸ƒçš„è§†è§’æ·±å…¥åˆ†æCoTæ¨ç†çš„æœ‰æ•ˆæ€§ï¼Œæ­ç¤ºäº†å…¶åœ¨è¶…å‡ºè®­ç»ƒåˆ†å¸ƒæ—¶çš„è„†å¼±æ€§ï¼Œä¸ç°æœ‰æ–¹æ³•çš„è¡¨é¢æ¨ç†èƒ½åŠ›å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œè®¾ç½®äº†ä¸åŒçš„è®­ç»ƒå’Œæµ‹è¯•æ•°æ®åˆ†å¸ƒï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ï¼Œä»¥ç¡®ä¿åœ¨ä¸åŒæ¡ä»¶ä¸‹å¯¹LLMçš„æ¨ç†èƒ½åŠ›è¿›è¡Œå…¨é¢è¯„ä¼°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“æµ‹è¯•æŸ¥è¯¢è¶…å‡ºè®­ç»ƒæ•°æ®åˆ†å¸ƒæ—¶ï¼ŒCoTæ¨ç†çš„æ•ˆæœæ˜¾è‘—ä¸‹é™ï¼Œè¡¨æ˜å…¶æ¨ç†èƒ½åŠ›çš„è„†å¼±æ€§ã€‚å…·ä½“è€Œè¨€ï¼Œåœ¨ä¸åŒåˆ†å¸ƒæ¡ä»¶ä¸‹ï¼Œæ¨¡å‹çš„æ¨ç†å‡†ç¡®ç‡ä¸‹é™å¹…åº¦è¶…è¿‡30%ï¼Œå¼ºè°ƒäº†æ¨ç†èƒ½åŠ›çš„å¯æ¨å¹¿æ€§é—®é¢˜ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶ä¸ºç†è§£å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æä¾›äº†æ–°çš„è§†è§’ï¼Œå°¤å…¶æ˜¯åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¸®åŠ©å¼€å‘æ›´å…·é²æ£’æ€§çš„æ¨ç†ç³»ç»Ÿã€‚æœªæ¥å¯åœ¨æ•™è‚²ã€è‡ªåŠ¨é—®ç­”å’Œæ™ºèƒ½åŠ©æ‰‹ç­‰é¢†åŸŸåº”ç”¨ï¼Œä»¥æå‡æ¨¡å‹çš„æ¨ç†å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Chain-of-Thought (CoT) prompting has been shown to improve Large Language Model (LLM) performance on various tasks. With this approach, LLMs appear to produce human-like reasoning steps before providing answers (a.k.a., CoT reasoning), which often leads to the perception that they engage in deliberate inferential processes. However, some initial findings suggest that CoT reasoning may be more superficial than it appears, motivating us to explore further. In this paper, we study CoT reasoning via a data distribution lens and investigate if CoT reasoning reflects a structured inductive bias learned from in-distribution data, allowing the model to conditionally generate reasoning paths that approximate those seen during training. Thus, its effectiveness is fundamentally bounded by the degree of distribution discrepancy between the training data and the test queries. With this lens, we dissect CoT reasoning via three dimensions: task, length, and format. To investigate each dimension, we design DataAlchemy, an isolated and controlled environment to train LLMs from scratch and systematically probe them under various distribution conditions. Our results reveal that CoT reasoning is a brittle mirage that vanishes when it is pushed beyond training distributions. This work offers a deeper understanding of why and when CoT reasoning fails, emphasizing the ongoing challenge of achieving genuine and generalizable reasoning.

