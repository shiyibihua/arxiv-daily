---
layout: default
title: The Landscape of Agentic Reinforcement Learning for LLMs: A Survey
---

# The Landscape of Agentic Reinforcement Learning for LLMs: A Survey

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.02547" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.02547v3</a>
  <a href="https://arxiv.org/pdf/2509.02547.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.02547v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.02547v3', 'The Landscape of Agentic Reinforcement Learning for LLMs: A Survey')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Guibin Zhang, Hejia Geng, Xiaohang Yu, Zhenfei Yin, Zaibin Zhang, Zelin Tan, Heng Zhou, Zhongzhi Li, Xiangyuan Xue, Yijiang Li, Yifan Zhou, Yang Chen, Chen Zhang, Yutao Fan, Zihu Wang, Songtao Huang, Francisco Piedrahita-Velez, Yue Liao, Hongru Wang, Mengyue Yang, Heng Ji, Jun Wang, Shuicheng Yan, Philip Torr, Lei Bai

**åˆ†ç±»**: cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-02 (æ›´æ–°: 2025-11-08)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAgentic RLæ¡†æ¶ï¼Œå°†LLMä»åºåˆ—ç”Ÿæˆå™¨è½¬å˜ä¸ºè‡ªä¸»å†³ç­–æ™ºèƒ½ä½“ï¼Œå¹¶å…¨é¢ç»¼è¿°å…¶èƒ½åŠ›ã€åº”ç”¨ä¸æœªæ¥æ–¹å‘ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `Agenticå¼ºåŒ–å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `è‡ªä¸»æ™ºèƒ½ä½“` `å¼ºåŒ–å­¦ä¹ ` `å†³ç­–æ™ºèƒ½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»ŸLLM-RLå°†LLMè§†ä¸ºè¢«åŠ¨åºåˆ—ç”Ÿæˆå™¨ï¼Œç¼ºä¹åœ¨å¤æ‚åŠ¨æ€ç¯å¢ƒä¸­è‡ªä¸»å†³ç­–çš„èƒ½åŠ›ã€‚
2. Agentic RLå°†LLMé‡å¡‘ä¸ºè‡ªä¸»æ™ºèƒ½ä½“ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ èµ‹äºˆå…¶è§„åˆ’ã€å·¥å…·ä½¿ç”¨ã€è®°å¿†ç­‰æ ¸å¿ƒèƒ½åŠ›ã€‚
3. è®ºæ–‡å…¨é¢ç»¼è¿°Agentic RLï¼Œæ•´ç†å¼€æºç¯å¢ƒã€åŸºå‡†å’Œæ¡†æ¶ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›æŒ‡å¯¼ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

Agenticå¼ºåŒ–å­¦ä¹ ï¼ˆAgentic RLï¼‰çš„å‡ºç°æ ‡å¿—ç€ä»åº”ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLM RLï¼‰çš„ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ çš„èŒƒå¼è½¬å˜ï¼Œå®ƒå°†LLMä»è¢«åŠ¨çš„åºåˆ—ç”Ÿæˆå™¨é‡æ–°å®šä¹‰ä¸ºåµŒå…¥åœ¨å¤æ‚ã€åŠ¨æ€ä¸–ç•Œä¸­çš„è‡ªä¸»å†³ç­–æ™ºèƒ½ä½“ã€‚æœ¬ç»¼è¿°é€šè¿‡å¯¹æ¯”LLM-RLçš„é€€åŒ–å•æ­¥é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ä¸å®šä¹‰Agentic RLçš„æ—¶åºæ‰©å±•ã€éƒ¨åˆ†å¯è§‚å¯Ÿé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆPOMDPï¼‰ï¼Œæ­£å¼ç¡®å®šäº†è¿™ç§æ¦‚å¿µè½¬å˜ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå…¨é¢çš„åŒé‡åˆ†ç±»æ³•ï¼šä¸€ä¸ªå›´ç»•æ ¸å¿ƒæ™ºèƒ½ä½“èƒ½åŠ›ç»„ç»‡ï¼ŒåŒ…æ‹¬è§„åˆ’ã€å·¥å…·ä½¿ç”¨ã€è®°å¿†ã€æ¨ç†ã€è‡ªæˆ‘æå‡å’Œæ„ŸçŸ¥ï¼Œå¦ä¸€ä¸ªå›´ç»•å®ƒä»¬åœ¨ä¸åŒä»»åŠ¡é¢†åŸŸçš„åº”ç”¨ã€‚æˆ‘ä»¬çš„æ ¸å¿ƒè®ºç‚¹æ˜¯ï¼Œå¼ºåŒ–å­¦ä¹ æ˜¯ä½¿è¿™äº›èƒ½åŠ›ä»é™æ€çš„ã€å¯å‘å¼æ¨¡å—è½¬å˜ä¸ºè‡ªé€‚åº”ã€é²æ£’çš„æ™ºèƒ½ä½“è¡Œä¸ºçš„å…³é”®æœºåˆ¶ã€‚ä¸ºäº†æ”¯æŒå’ŒåŠ é€Ÿæœªæ¥çš„ç ”ç©¶ï¼Œæˆ‘ä»¬å°†å¼€æºç¯å¢ƒã€åŸºå‡†å’Œæ¡†æ¶æ•´åˆåˆ°ä¸€ä¸ªå®ç”¨çš„æ¦‚è¦ä¸­ã€‚é€šè¿‡ç»¼åˆäº”ç™¾å¤šç¯‡æœ€æ–°è‘—ä½œï¼Œæœ¬ç»¼è¿°æç»˜äº†è¿™ä¸ªå¿«é€Ÿå‘å±•é¢†åŸŸçš„è½®å»“ï¼Œå¹¶å¼ºè°ƒäº†å°†å¡‘é€ å¯æ‰©å±•çš„é€šç”¨äººå·¥æ™ºèƒ½æ™ºèƒ½ä½“å‘å±•çš„æœºé‡å’ŒæŒ‘æˆ˜ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ ï¼ˆLLM-RLï¼‰æ–¹æ³•é€šå¸¸å°†LLMè§†ä¸ºè¢«åŠ¨çš„åºåˆ—ç”Ÿæˆå™¨ï¼Œé‡‡ç”¨å•æ­¥é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰è¿›è¡Œå»ºæ¨¡ã€‚è¿™ç§æ–¹æ³•æ— æ³•å……åˆ†åˆ©ç”¨LLMåœ¨å¤æ‚ã€åŠ¨æ€ç¯å¢ƒä¸­è¿›è¡Œé•¿æœŸè§„åˆ’å’Œå†³ç­–çš„èƒ½åŠ›ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿå°†LLMè½¬å˜ä¸ºè‡ªä¸»çš„ã€å…·æœ‰æ™ºèƒ½ä½“è¡Œä¸ºçš„å†³ç­–è€…ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†LLMè§†ä¸ºæ™ºèƒ½ä½“ï¼Œå¹¶é‡‡ç”¨Agenticå¼ºåŒ–å­¦ä¹ ï¼ˆAgentic RLï¼‰æ¡†æ¶å¯¹å…¶è¿›è¡Œè®­ç»ƒã€‚Agentic RLä½¿ç”¨æ—¶åºæ‰©å±•ã€éƒ¨åˆ†å¯è§‚å¯Ÿé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆPOMDPï¼‰æ¥å»ºæ¨¡LLMä¸ç¯å¢ƒçš„äº¤äº’ï¼Œä»è€Œä½¿LLMèƒ½å¤Ÿè¿›è¡Œé•¿æœŸè§„åˆ’ã€å·¥å…·ä½¿ç”¨ã€è®°å¿†ã€æ¨ç†ã€è‡ªæˆ‘æå‡å’Œæ„ŸçŸ¥ç­‰æ“ä½œã€‚å¼ºåŒ–å­¦ä¹ ä½œä¸ºå…³é”®æœºåˆ¶ï¼Œå°†è¿™äº›èƒ½åŠ›ä»é™æ€æ¨¡å—è½¬å˜ä¸ºè‡ªé€‚åº”ã€é²æ£’çš„æ™ºèƒ½ä½“è¡Œä¸ºã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šAgentic RLæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) ç¯å¢ƒï¼šå®šä¹‰æ™ºèƒ½ä½“æ‰€å¤„çš„å¤–éƒ¨ä¸–ç•Œï¼Œæä¾›çŠ¶æ€ä¿¡æ¯å’Œå¥–åŠ±ä¿¡å·ã€‚2) æ™ºèƒ½ä½“ï¼šç”±LLMé©±åŠ¨ï¼Œè´Ÿè´£è§‚å¯Ÿç¯å¢ƒã€åˆ¶å®šè¡ŒåŠ¨ç­–ç•¥å¹¶æ‰§è¡Œè¡ŒåŠ¨ã€‚3) å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼šç”¨äºè®­ç»ƒæ™ºèƒ½ä½“ï¼Œä½¿å…¶èƒ½å¤Ÿæ ¹æ®ç¯å¢ƒåé¦ˆä¼˜åŒ–è¡ŒåŠ¨ç­–ç•¥ã€‚4) æ ¸å¿ƒèƒ½åŠ›æ¨¡å—ï¼šåŒ…æ‹¬è§„åˆ’ã€å·¥å…·ä½¿ç”¨ã€è®°å¿†ã€æ¨ç†ã€è‡ªæˆ‘æå‡å’Œæ„ŸçŸ¥ç­‰ï¼Œè¿™äº›æ¨¡å—å¯ä»¥å¢å¼ºæ™ºèƒ½ä½“çš„å†³ç­–èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†Agentic RLçš„æ¦‚å¿µï¼Œå¹¶å°†å…¶åº”ç”¨äºLLMã€‚ä¸ä¼ ç»Ÿçš„LLM-RLæ–¹æ³•ç›¸æ¯”ï¼ŒAgentic RLèƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨LLMçš„æ½œåŠ›ï¼Œä½¿å…¶èƒ½å¤ŸåƒçœŸæ­£çš„æ™ºèƒ½ä½“ä¸€æ ·åœ¨å¤æ‚ç¯å¢ƒä¸­è¿›è¡Œè‡ªä¸»å†³ç­–ã€‚æ­¤å¤–ï¼Œè¯¥è®ºæ–‡è¿˜å¯¹Agentic RLçš„æ ¸å¿ƒèƒ½åŠ›è¿›è¡Œäº†åˆ†ç±»ï¼Œå¹¶æ¢è®¨äº†å¦‚ä½•ä½¿ç”¨å¼ºåŒ–å­¦ä¹ æ¥æå‡è¿™äº›èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­æ²¡æœ‰å…·ä½“æ¶‰åŠå…³é”®å‚æ•°è®¾ç½®ã€æŸå¤±å‡½æ•°æˆ–ç½‘ç»œç»“æ„ç­‰æŠ€æœ¯ç»†èŠ‚ï¼Œè€Œæ˜¯ä¾§é‡äºå¯¹Agentic RLæ¡†æ¶çš„æ•´ä½“æ¦‚å¿µå’Œæ¶æ„è¿›è¡Œé˜è¿°ã€‚æœªæ¥çš„ç ”ç©¶å¯ä»¥é’ˆå¯¹ä¸åŒçš„æ ¸å¿ƒèƒ½åŠ›æ¨¡å—ï¼Œè®¾è®¡ç‰¹å®šçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•å’Œç½‘ç»œç»“æ„ï¼Œä»¥è¿›ä¸€æ­¥æå‡Agentic RLçš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥è®ºæ–‡æ˜¯ä¸€ç¯‡ç»¼è¿°æ€§æ–‡ç« ï¼Œä¸»è¦è´¡çŒ®åœ¨äºå¯¹Agentic RLé¢†åŸŸè¿›è¡Œäº†å…¨é¢çš„æ¢³ç†å’Œæ€»ç»“ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªåŒé‡åˆ†ç±»æ³•ï¼Œå¯¹Agentic RLçš„æ ¸å¿ƒèƒ½åŠ›å’Œåº”ç”¨è¿›è¡Œäº†åˆ†ç±»ã€‚è®ºæ–‡è¿˜æ•´ç†äº†å¼€æºç¯å¢ƒã€åŸºå‡†å’Œæ¡†æ¶ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„èµ„æºã€‚ç”±äºæ˜¯ç»¼è¿°ï¼Œæ²¡æœ‰å…·ä½“çš„å®éªŒç»“æœå’Œæ€§èƒ½æ•°æ®ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Agentic RLåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼ŒåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€æ¸¸æˆAIã€å¯¹è¯ç³»ç»Ÿã€è‡ªåŠ¨åŒ–ä»»åŠ¡æ‰§è¡Œç­‰ã€‚é€šè¿‡èµ‹äºˆLLMè‡ªä¸»å†³ç­–èƒ½åŠ›ï¼Œå¯ä»¥ä½¿å…¶åœ¨å¤æ‚ç¯å¢ƒä¸­å®Œæˆæ›´å…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œä¾‹å¦‚è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½å®¶å±…æ§åˆ¶ã€å®¢æˆ·æœåŠ¡ç­‰ã€‚Agentic RLæœ‰æœ›æ¨åŠ¨é€šç”¨äººå·¥æ™ºèƒ½çš„å‘å±•ï¼Œå¹¶ä¸ºäººç±»å¸¦æ¥æ›´æ™ºèƒ½ã€æ›´ä¾¿æ·çš„ç”Ÿæ´»ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The emergence of agentic reinforcement learning (Agentic RL) marks a paradigm shift from conventional reinforcement learning applied to large language models (LLM RL), reframing LLMs from passive sequence generators into autonomous, decision-making agents embedded in complex, dynamic worlds. This survey formalizes this conceptual shift by contrasting the degenerate single-step Markov Decision Processes (MDPs) of LLM-RL with the temporally extended, partially observable Markov decision processes (POMDPs) that define Agentic RL. Building on this foundation, we propose a comprehensive twofold taxonomy: one organized around core agentic capabilities, including planning, tool use, memory, reasoning, self-improvement, and perception, and the other around their applications across diverse task domains. Central to our thesis is that reinforcement learning serves as the critical mechanism for transforming these capabilities from static, heuristic modules into adaptive, robust agentic behavior. To support and accelerate future research, we consolidate the landscape of open-source environments, benchmarks, and frameworks into a practical compendium. By synthesizing over five hundred recent works, this survey charts the contours of this rapidly evolving field and highlights the opportunities and challenges that will shape the development of scalable, general-purpose AI agents.

