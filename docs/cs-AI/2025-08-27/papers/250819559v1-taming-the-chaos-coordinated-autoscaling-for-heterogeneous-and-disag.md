---
layout: default
title: Taming the Chaos: Coordinated Autoscaling for Heterogeneous and Disaggregated LLM Inference
---

# Taming the Chaos: Coordinated Autoscaling for Heterogeneous and Disaggregated LLM Inference

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.19559" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.19559v1</a>
  <a href="https://arxiv.org/pdf/2508.19559.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.19559v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.19559v1', 'Taming the Chaos: Coordinated Autoscaling for Heterogeneous and Disaggregated LLM Inference')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Rongzhi Li, Ruogu Du, Zefang Chu, Sida Zhao, Chunlei Han, Zuocheng Shi, Yiwen Shao, Huanle Han, Long Huang, Zherui Liu, Shufan Liu

**åˆ†ç±»**: cs.DC, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-27

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHeteroScaleä»¥è§£å†³å¼‚æ„å’Œåˆ†ç¦»LLMæ¨ç†çš„è‡ªåŠ¨æ‰©å±•é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `è‡ªåŠ¨æ‰©å±•` `å¼‚æ„è®¡ç®—` `èµ„æºç®¡ç†` `Prefill-Decodeæ¶æ„`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è‡ªåŠ¨æ‰©å±•æ–¹æ³•åœ¨å¤„ç†ç°ä»£åˆ†ç¦»æ¶æ„æ—¶é¢ä¸´å¼‚æ„ç¡¬ä»¶åˆ©ç”¨ä¸å‡ã€ç½‘ç»œç“¶é¢ˆåŠé¢„å¡«å……ä¸è§£ç é˜¶æ®µä¸å¹³è¡¡ç­‰æŒ‘æˆ˜ã€‚
2. HeteroScaleæ¡†æ¶é€šè¿‡æ‹“æ‰‘æ„ŸçŸ¥è°ƒåº¦å™¨å’ŒåŸºäºåº¦é‡çš„ç­–ç•¥ï¼Œåè°ƒæ‰©å±•é¢„å¡«å……å’Œè§£ç æ± ï¼Œä¼˜åŒ–èµ„æºç®¡ç†ã€‚
3. åœ¨å¤§è§„æ¨¡ç”Ÿäº§ç¯å¢ƒä¸­ï¼ŒHeteroScaleæ˜¾è‘—æé«˜äº†GPUåˆ©ç”¨ç‡26.6ä¸ªç™¾åˆ†ç‚¹ï¼ŒèŠ‚çœäº†å¤§é‡GPUå°æ—¶ï¼Œç¡®ä¿äº†æœåŠ¡è´¨é‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æœåŠ¡æ˜¯ä¸€é¡¹GPUå¯†é›†å‹ä»»åŠ¡ï¼Œä¼ ç»Ÿçš„è‡ªåŠ¨æ‰©å±•æ–¹æ³•åœ¨ç°ä»£çš„Prefill-Decodeï¼ˆP/Dï¼‰åˆ†ç¦»æ¶æ„ä¸­è¡¨ç°ä¸ä½³ã€‚æ­¤æ¶æ„è½¬å˜è™½ç„¶å¼ºå¤§ï¼Œä½†å¸¦æ¥äº†æ˜¾è‘—çš„æ“ä½œæŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å¼‚æ„ç¡¬ä»¶çš„ä½æ•ˆåˆ©ç”¨ã€ç½‘ç»œç“¶é¢ˆä»¥åŠé¢„å¡«å……å’Œè§£ç é˜¶æ®µä¹‹é—´çš„å…³é”®ä¸å¹³è¡¡ã€‚æˆ‘ä»¬æå‡ºäº†HeteroScaleï¼Œä¸€ä¸ªåè°ƒçš„è‡ªåŠ¨æ‰©å±•æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³P/Dåˆ†ç¦»æœåŠ¡çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚HeteroScaleç»“åˆäº†ä¸€ä¸ªæ‹“æ‰‘æ„ŸçŸ¥è°ƒåº¦å™¨ï¼Œèƒ½å¤Ÿé€‚åº”å¼‚æ„ç¡¬ä»¶å’Œç½‘ç»œçº¦æŸï¼Œä»¥åŠä¸€ç§åŸºäºå¤§è§„æ¨¡å®è¯ç ”ç©¶çš„åˆ›æ–°åº¦é‡é©±åŠ¨ç­–ç•¥ã€‚é€šè¿‡åˆ©ç”¨å•ä¸€çš„å¼ºå¥åº¦é‡æ¥å…±åŒæ‰©å±•é¢„å¡«å……å’Œè§£ç æ± ï¼ŒHeteroScaleä¿æŒäº†æ¶æ„å¹³è¡¡ï¼ŒåŒæ—¶ç¡®ä¿äº†é«˜æ•ˆçš„è‡ªé€‚åº”èµ„æºç®¡ç†ã€‚åœ¨æ•°ä¸‡GPUçš„å¤§è§„æ¨¡ç”Ÿäº§ç¯å¢ƒä¸­éƒ¨ç½²åï¼ŒHeteroScaleè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ï¼Œå¹³å‡GPUåˆ©ç”¨ç‡æé«˜äº†26.6ä¸ªç™¾åˆ†ç‚¹ï¼Œæ¯æ—¥èŠ‚çœæ•°åä¸‡GPUå°æ—¶ï¼ŒåŒæ—¶ä¿æŒä¸¥æ ¼çš„æœåŠ¡æ°´å¹³ç›®æ ‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³åœ¨ç°ä»£Prefill-Decodeï¼ˆP/Dï¼‰åˆ†ç¦»æ¶æ„ä¸­ï¼Œä¼ ç»Ÿè‡ªåŠ¨æ‰©å±•æ–¹æ³•åœ¨å¼‚æ„ç¡¬ä»¶åˆ©ç”¨ã€ç½‘ç»œç“¶é¢ˆåŠé˜¶æ®µä¸å¹³è¡¡ç­‰æ–¹é¢çš„ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šHeteroScaleé€šè¿‡ç»“åˆæ‹“æ‰‘æ„ŸçŸ¥è°ƒåº¦å™¨å’Œåº¦é‡é©±åŠ¨ç­–ç•¥ï¼Œåè°ƒæ‰©å±•é¢„å¡«å……å’Œè§£ç èµ„æºï¼Œä»¥å®ç°é«˜æ•ˆçš„èµ„æºç®¡ç†å’Œæ¶æ„å¹³è¡¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šHeteroScaleçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ‹“æ‰‘æ„ŸçŸ¥è°ƒåº¦å™¨ã€åº¦é‡é©±åŠ¨ç­–ç•¥å’Œèµ„æºç®¡ç†æ¨¡å—ï¼Œèƒ½å¤Ÿæ ¹æ®ç¡¬ä»¶å’Œç½‘ç»œæ¡ä»¶åŠ¨æ€è°ƒæ•´èµ„æºåˆ†é…ã€‚

**å…³é”®åˆ›æ–°**ï¼šHeteroScaleçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶åŸºäºå¤§è§„æ¨¡å®è¯ç ”ç©¶çš„åº¦é‡é©±åŠ¨ç­–ç•¥ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°åè°ƒå¼‚æ„èµ„æºçš„ä½¿ç”¨ï¼Œè§£å†³ä¼ ç»Ÿæ–¹æ³•çš„å±€é™æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒHeteroScaleä½¿ç”¨äº†å•ä¸€çš„å¼ºå¥åº¦é‡æ¥å…±åŒæ‰©å±•é¢„å¡«å……å’Œè§£ç æ± ï¼Œç¡®ä¿äº†èµ„æºçš„é«˜æ•ˆåˆ©ç”¨å’ŒæœåŠ¡è´¨é‡çš„ç¨³å®šæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

HeteroScaleåœ¨å¤§è§„æ¨¡ç”Ÿäº§ç¯å¢ƒä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹³å‡GPUåˆ©ç”¨ç‡æé«˜äº†26.6ä¸ªç™¾åˆ†ç‚¹ï¼Œæ˜¾è‘—èŠ‚çœäº†æ•°åä¸‡GPUå°æ—¶ï¼Œä¸”åœ¨ä¿æŒä¸¥æ ¼æœåŠ¡æ°´å¹³ç›®æ ‡çš„åŒæ—¶ï¼Œä¼˜åŒ–äº†èµ„æºç®¡ç†ã€‚è¿™äº›ç»“æœè¡¨æ˜HeteroScaleåœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§å’Œé‡è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

HeteroScaleçš„ç ”ç©¶æˆæœåœ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æœåŠ¡é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é«˜æ•ˆèµ„æºç®¡ç†å’Œå®æ—¶å“åº”çš„åœºæ™¯ä¸­ï¼Œå¦‚åœ¨çº¿å®¢æœã€æ™ºèƒ½åŠ©æ‰‹å’Œå†…å®¹ç”Ÿæˆç­‰ã€‚å…¶åˆ›æ–°çš„è‡ªåŠ¨æ‰©å±•æ–¹æ³•å¯ä»¥æ˜¾è‘—æå‡ç³»ç»Ÿçš„æ€§èƒ½å’Œç”¨æˆ·ä½“éªŒï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨æ›´å¤šæ™ºèƒ½åº”ç”¨çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Serving Large Language Models (LLMs) is a GPU-intensive task where traditional autoscalers fall short, particularly for modern Prefill-Decode (P/D) disaggregated architectures. This architectural shift, while powerful, introduces significant operational challenges, including inefficient use of heterogeneous hardware, network bottlenecks, and critical imbalances between prefill and decode stages. We introduce HeteroScale, a coordinated autoscaling framework that addresses the core challenges of P/D disaggregated serving. HeteroScale combines a topology-aware scheduler that adapts to heterogeneous hardware and network constraints with a novel metric-driven policy derived from the first large-scale empirical study of autoscaling signals in production. By leveraging a single, robust metric to jointly scale prefill and decode pools, HeteroScale maintains architectural balance while ensuring efficient, adaptive resource management. Deployed in a massive production environment on tens of thousands of GPUs, HeteroScale has proven its effectiveness, increasing average GPU utilization by a significant 26.6 percentage points and saving hundreds of thousands of GPU-hours daily, all while upholding stringent service level objectives.

