---
layout: default
title: ReST-RL: Achieving Accurate Code Reasoning of LLMs with Optimized Self-Training and Decoding
---

# ReST-RL: Achieving Accurate Code Reasoning of LLMs with Optimized Self-Training and Decoding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.19576" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.19576v2</a>
  <a href="https://arxiv.org/pdf/2508.19576.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.19576v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.19576v2', 'ReST-RL: Achieving Accurate Code Reasoning of LLMs with Optimized Self-Training and Decoding')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Sining Zhoubian, Dan Zhang, Jie Tang

**ÂàÜÁ±ª**: cs.AI, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-27 (Êõ¥Êñ∞: 2025-09-08)

**Â§áÊ≥®**: 21 pages, 4 figures

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/THUDM/ReST-RL)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫ReST-RL‰ª•Ëß£ÂÜ≥LLMsÊé®ÁêÜÂáÜÁ°ÆÊÄß‰∏çË∂≥ÁöÑÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `Âº∫ÂåñÂ≠¶‰π†` `‰ª£Á†ÅÊé®ÁêÜ` `ËíôÁâπÂç°Ê¥õÊ†ëÊêúÁ¥¢` `ËÆ≠ÁªÉÊï∞ÊçÆ‰ºòÂåñ` `Êó†Ê≥®ÈáäÂ≠¶‰π†` `Êé®ÁêÜÂáÜÁ°ÆÊÄß`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑGRPOÊñπÊ≥ïÂú®Êé®ÁêÜÂáÜÁ°ÆÊÄß‰∏äÂ≠òÂú®Â•ñÂä±ÊñπÂ∑Æ‰∏çË∂≥ÁöÑÈóÆÈ¢òÔºåÂØºËá¥ÂÖ∂ÊïàÊûú‰∏çÁêÜÊÉ≥„ÄÇ
2. Êú¨ÊñáÊèêÂá∫ÁöÑReST-RLÁªìÂêà‰∫ÜÊîπËøõÁöÑGRPOÁÆóÊ≥ïÂíåÊµãËØïÊó∂Èó¥Ëß£Á†ÅÊñπÊ≥ïÔºå‰ºòÂåñ‰∫ÜËÆ≠ÁªÉÊï∞ÊçÆÁöÑÈÄâÊã©Âíå‰ΩøÁî®„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåReST-RLÂú®Â§ö‰∏™ÁºñÁ†ÅÂü∫ÂáÜ‰∏äÊòæËëóË∂ÖË∂ä‰∫ÜÂÖ∂‰ªñÂº∫ÂåñËÆ≠ÁªÉÂíåËß£Á†ÅÈ™åËØÅÂü∫Á∫øÔºåÊèêÂçá‰∫ÜÊé®ÁêÜËÉΩÂäõ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÈíàÂØπÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÊé®ÁêÜÂáÜÁ°ÆÊÄßÊèêÂçáÁöÑÈóÆÈ¢òÔºåÁé∞ÊúâÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïGRPOÂõ†Â•ñÂä±ÊñπÂ∑Æ‰∏çË∂≥ËÄåÂ§±Ë¥•ÔºåËÄåÂü∫‰∫éËøáÁ®ãÂ•ñÂä±Ê®°ÂûãÔºàPRMsÔºâÁöÑÈ™åËØÅÊñπÊ≥ïÂú®ËÆ≠ÁªÉÊï∞ÊçÆËé∑ÂèñÂíåÈ™åËØÅÊúâÊïàÊÄß‰∏äÂ≠òÂú®Âõ∞Èöæ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜReST-RLÔºå‰∏Ä‰∏™Áªü‰∏ÄÁöÑLLMÂº∫ÂåñÂ≠¶‰π†ËåÉÂºèÔºåÈÄöËøáÁªìÂêàÊîπËøõÁöÑGRPOÁÆóÊ≥ï‰∏éÁ≤æÂøÉËÆæËÆ°ÁöÑÊµãËØïÊó∂Èó¥Ëß£Á†ÅÊñπÊ≥ïÔºåÊòæËëóÊèêÂçá‰∫ÜLLMsÁöÑ‰ª£Á†ÅÊé®ÁêÜËÉΩÂäõ„ÄÇReST-GRPOÈááÁî®‰ºòÂåñÁöÑReSTÁÆóÊ≥ïÊù•Á≠õÈÄâÂíåÁªÑË£ÖÈ´ò‰ª∑ÂÄºËÆ≠ÁªÉÊï∞ÊçÆÔºå‰ªéËÄåÊèêÈ´òGRPOÈááÊ†∑ÁöÑÂ•ñÂä±ÊñπÂ∑ÆÔºåÂ¢ûÂº∫ËÆ≠ÁªÉÁöÑÊúâÊïàÊÄßÂíåÊïàÁéá„ÄÇÈÄöËøáËíôÁâπÂç°Ê¥õÊ†ëÊêúÁ¥¢ÔºàMCTSÔºâÔºåÊàë‰ª¨Âú®Êó†Ê≥®ÈáäÁöÑÊÉÖÂÜµ‰∏ãÊî∂ÈõÜÂáÜÁ°ÆÁöÑ‰ª∑ÂÄºÁõÆÊ†áÔºåËøõ‰∏ÄÊ≠•‰ºòÂåñ‰∫ÜÊµãËØïÊó∂Èó¥Ëß£Á†ÅÊñπÊ≥ïVM-MCTSÔºåÊúÄÁªàÂú®Â§ö‰∏™Áü•ÂêçÁºñÁ†ÅÂü∫ÂáÜ‰∏äÈ™åËØÅ‰∫ÜËØ•ÊñπÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®‰ª£Á†ÅÊé®ÁêÜ‰∏≠ÁöÑÂáÜÁ°ÆÊÄß‰∏çË∂≥ÈóÆÈ¢ò„ÄÇÁé∞ÊúâÁöÑGRPOÊñπÊ≥ïÁî±‰∫éÂ•ñÂä±ÊñπÂ∑ÆÂ∞èÔºåÂØºËá¥Êé®ÁêÜÊïàÊûú‰∏ç‰Ω≥ÔºåËÄåÂü∫‰∫éËøáÁ®ãÂ•ñÂä±Ê®°ÂûãÁöÑÈ™åËØÅÊñπÊ≥ïÂú®Êï∞ÊçÆËé∑ÂèñÂíåÈ™åËØÅÊúâÊïàÊÄß‰∏äÂ≠òÂú®ÊåëÊàò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöReST-RLÈÄöËøáÁªìÂêàÊîπËøõÁöÑGRPOÁÆóÊ≥ï‰∏éÊµãËØïÊó∂Èó¥Ëß£Á†ÅÊñπÊ≥ïÔºå‰ºòÂåñ‰∫ÜËÆ≠ÁªÉÊï∞ÊçÆÁöÑÈÄâÊã©ÔºåÂ¢ûÂº∫‰∫ÜÂ•ñÂä±ÊñπÂ∑ÆÔºå‰ªéËÄåÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇËØ•ÊñπÊ≥ïÁöÑËÆæËÆ°Êó®Âú®ÈÄöËøáÈ´òÊïàÁöÑÊï∞ÊçÆÂà©Áî®ÂíåÊó†Ê≥®ÈáäÁöÑ‰ª∑ÂÄºÁõÆÊ†áÊî∂ÈõÜÊù•ÊèêÈ´òËÆ≠ÁªÉÊïàÊûú„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöReST-RLÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏§‰∏™‰∏ªË¶ÅÈò∂ÊÆµÔºöÈ¶ñÂÖàÊòØReST-GRPOÈò∂ÊÆµÔºåÈÄöËøá‰ºòÂåñÁöÑReSTÁÆóÊ≥ïÁ≠õÈÄâÈ´ò‰ª∑ÂÄºËÆ≠ÁªÉÊï∞ÊçÆÔºõÂÖ∂Ê¨°ÊòØVM-MCTSÈò∂ÊÆµÔºåÈÄöËøáËíôÁâπÂç°Ê¥õÊ†ëÊêúÁ¥¢Êî∂ÈõÜÂáÜÁ°ÆÁöÑ‰ª∑ÂÄºÁõÆÊ†áÂπ∂ËøõË°åËß£Á†Å‰ºòÂåñ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöReST-RLÁöÑÊ†∏ÂøÉÂàõÊñ∞Âú®‰∫éÁªìÂêà‰∫Ü‰ºòÂåñÁöÑGRPOÁÆóÊ≥ï‰∏éÊó†Ê≥®ÈáäÁöÑ‰ª∑ÂÄºÊ®°ÂûãËÆ≠ÁªÉÔºåÊòæËëóÊèêÈ´ò‰∫ÜÂ•ñÂä±ÊñπÂ∑ÆÂíåÊé®ÁêÜÂáÜÁ°ÆÊÄß„ÄÇËøô‰∏ÄÊñπÊ≥ï‰∏é‰º†ÁªüÁöÑÂº∫ÂåñÂ≠¶‰π†ÂíåÈ™åËØÅÊñπÊ≥ïÁõ∏ÊØîÔºåÂÖ∑ÊúâÊõ¥È´òÁöÑÊïàÁéáÂíåÊúâÊïàÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ReST-GRPOÈò∂ÊÆµÔºåÈááÁî®‰∫Ü‰ºòÂåñÁöÑReSTÁÆóÊ≥ïËøõË°åÊï∞ÊçÆÁ≠õÈÄâÔºåÁ°Æ‰øùËÆ≠ÁªÉÊï∞ÊçÆÁöÑÈ´ò‰ª∑ÂÄºÔºõÂú®VM-MCTSÈò∂ÊÆµÔºåÂà©Áî®ËíôÁâπÂç°Ê¥õÊ†ëÊêúÁ¥¢Êî∂ÈõÜ‰ª∑ÂÄºÁõÆÊ†áÔºåËÆæËÆ°‰∫ÜÈÄÇÂ∫îÊÄßMCTSÁÆóÊ≥ï‰ª•Êèê‰æõÁ≤æÁ°ÆÁöÑËøáÁ®ã‰ø°Âè∑ÂíåÈ™åËØÅÂàÜÊï∞„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåReST-RLÂú®Â§ö‰∏™Áü•ÂêçÁºñÁ†ÅÂü∫ÂáÜÔºàÂ¶ÇAPPS„ÄÅBigCodeBenchÂíåHumanEvalÔºâ‰∏äÊòæËëó‰ºò‰∫éÂÖ∂‰ªñÂº∫ÂåñËÆ≠ÁªÉÂü∫Á∫øÔºàÂ¶Çnaive GRPOÂíåReST-DPOÔºâÔºå‰ª•ÂèäËß£Á†ÅÂíåÈ™åËØÅÂü∫Á∫øÔºàÂ¶ÇPRM-BoNÂíåORM-MCTSÔºâÔºåÊèêÂçáÂπÖÂ∫¶ËææÂà∞XX%ÔºåË°®ÊòéÂÖ∂Âú®Â¢ûÂº∫LLMÊé®ÁêÜËÉΩÂäõÊñπÈù¢ÁöÑÂº∫Â§ßÊΩúÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨ËΩØ‰ª∂ÂºÄÂèë„ÄÅËá™Âä®‰ª£Á†ÅÁîüÊàêÂíåÊô∫ËÉΩÁºñÁ®ãÂä©ÊâãÁ≠â„ÄÇÈÄöËøáÊèêÂçáLLMsÁöÑÊé®ÁêÜËÉΩÂäõÔºåReST-RLËÉΩÂ§üÂú®ÂÆûÈôÖÁºñÁ®ã‰ªªÂä°‰∏≠Êèê‰æõÊõ¥È´òÁöÑÂáÜÁ°ÆÊÄßÂíåÊïàÁéáÔºåÂÖ∑ÊúâÂπøÊ≥õÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÊú™Êù•ÂΩ±Âìç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> With respect to improving the reasoning accuracy of LLMs, the representative reinforcement learning (RL) method GRPO faces failure due to insignificant reward variance, while verification methods based on process reward models (PRMs) suffer from difficulties with training data acquisition and verification effectiveness. To tackle these problems, this paper introduces ReST-RL, a unified LLM RL paradigm that significantly improves LLM's code reasoning ability by combining an improved GRPO algorithm with a meticulously designed test time decoding method assisted by a value model (VM). As the first stage of policy reinforcement, ReST-GRPO adopts an optimized ReST algorithm to filter and assemble high-value training data, increasing the reward variance of GRPO sampling, thus improving the effectiveness and efficiency of training. After the basic reasoning ability of LLM policy has been improved, we further propose a test time decoding optimization method called VM-MCTS. Through Monte-Carlo Tree Search (MCTS), we collect accurate value targets with no annotation required, on which VM training is based. When decoding, the VM is deployed by an adapted MCTS algorithm to provide precise process signals as well as verification scores, assisting the LLM policy to achieve high reasoning accuracy. We conduct extensive experiments on coding problems to verify the validity of the proposed RL paradigm. Upon comparison, our approach significantly outperforms other reinforcement training baselines (e.g., naive GRPO and ReST-DPO), as well as decoding and verification baselines (e.g., PRM-BoN and ORM-MCTS) on well-known coding benchmarks of various levels (e.g., APPS, BigCodeBench, and HumanEval), indicating its power to strengthen the reasoning ability of LLM policies. Codes for our project can be found at https://github.com/THUDM/ReST-RL.

