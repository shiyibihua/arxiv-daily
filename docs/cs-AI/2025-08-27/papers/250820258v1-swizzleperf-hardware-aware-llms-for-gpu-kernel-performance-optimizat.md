---
layout: default
title: SwizzlePerf: Hardware-Aware LLMs for GPU Kernel Performance Optimization
---

# SwizzlePerf: Hardware-Aware LLMs for GPU Kernel Performance Optimization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.20258" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.20258v1</a>
  <a href="https://arxiv.org/pdf/2508.20258.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.20258v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.20258v1', 'SwizzlePerf: Hardware-Aware LLMs for GPU Kernel Performance Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Arya Tschand, Muhammad Awad, Ryan Swann, Kesavan Ramakrishnan, Jeffrey Ma, Keith Lowery, Ganesh Dasika, Vijay Janapa Reddi

**åˆ†ç±»**: cs.DC, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-27

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSwizzlePerfä»¥è§£å†³GPUå†…æ ¸æ€§èƒ½ä¼˜åŒ–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `GPUå†…æ ¸ä¼˜åŒ–` `ç¡¬ä»¶æ„ŸçŸ¥` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ€§èƒ½å·¥ç¨‹` `æœºå™¨å­¦ä¹ ` `ç§‘å­¦è®¡ç®—` `æ€§èƒ½æå‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨GPUå†…æ ¸æ€§èƒ½ä¼˜åŒ–ä¸­ç¼ºä¹ç¡¬ä»¶æ„ŸçŸ¥ï¼Œå¯¼è‡´ä¼˜åŒ–æ•ˆæœä¸ç†æƒ³ã€‚
2. SwizzlePerfé€šè¿‡æ˜¾å¼ç¡¬ä»¶æ„ŸçŸ¥ï¼Œè‡ªåŠ¨ç”Ÿæˆé’ˆå¯¹GPUå†…æ ¸çš„ç©ºé—´ä¼˜åŒ–ï¼Œæé«˜äº†æ€§èƒ½ã€‚
3. åœ¨å®éªŒä¸­ï¼ŒSwizzlePerfä¸º9ä¸ªå†…æ ¸ç”Ÿæˆçš„ä¼˜åŒ–æ¨¡å¼å®ç°äº†æœ€é«˜2.06å€çš„åŠ é€Ÿå’Œ70%çš„L2å‘½ä¸­ç‡æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨GPUå†…æ ¸æ€§èƒ½å·¥ç¨‹æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•ä¾èµ–äºä½æ•ˆçš„åŸºäºæœç´¢çš„ä¼˜åŒ–ï¼Œç¼ºä¹ç¡¬ä»¶æ„ŸçŸ¥ç‰¹æ€§ã€‚SwizzlePerfé€šè¿‡åˆ©ç”¨ç‰¹å®šçš„å†…å­˜è®¿é—®æ¨¡å¼ã€æ¶æ„è§„æ ¼å’Œå†å²æ€§èƒ½åæ€ï¼Œè‡ªåŠ¨ç”Ÿæˆé’ˆå¯¹åˆ†ç¦»æ¶æ„çš„GPUå†…æ ¸çš„ç©ºé—´ä¼˜åŒ–ã€‚å¯¹äºGEMMå†…æ ¸ï¼ŒSwizzlePerfåœ¨ä¸åˆ°5åˆ†é’Ÿå†…ç”Ÿæˆäº†ä¸ä¸“å®¶æ€§èƒ½å·¥ç¨‹å¸ˆèŠ±è´¹2å‘¨æ—¶é—´æ‰¾åˆ°çš„ç¡¬ä»¶ç‰¹å®šä¼˜åŒ–æ¨¡å¼ç›¸åŒçš„ç»“æœã€‚åœ¨10ä¸ªå¤šæ ·åŒ–çš„æœºå™¨å­¦ä¹ å’Œç§‘å­¦å†…æ ¸çš„æµ‹è¯•ä¸­ï¼ŒSwizzlePerfä¸º9ä¸ªå†…æ ¸ç”Ÿæˆçš„ä¼˜åŒ–æ¨¡å¼å®ç°äº†æœ€é«˜2.06å€çš„åŠ é€Ÿå’Œ70%çš„L2å‘½ä¸­ç‡æå‡ã€‚è¿™é¡¹å·¥ä½œæ˜¯ç³»ç»Ÿæ€§åˆ›å»ºç¡¬ä»¶æ„ŸçŸ¥LLMæ€§èƒ½å·¥ç¨‹ä»£ç†çš„ç¬¬ä¸€æ­¥ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰GPUå†…æ ¸æ€§èƒ½ä¼˜åŒ–æ–¹æ³•ç¼ºä¹ç¡¬ä»¶æ„ŸçŸ¥çš„é—®é¢˜ï¼Œå¯¼è‡´ä¼˜åŒ–æ•ˆç‡ä½ä¸‹ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºæœç´¢ç­–ç•¥ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨ç¡¬ä»¶ç‰¹æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSwizzlePerfçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡åˆ©ç”¨ç‰¹å®šçš„å†…å­˜è®¿é—®æ¨¡å¼å’Œæ¶æ„è§„æ ¼ï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹å…·å¤‡ç¡¬ä»¶æ„ŸçŸ¥èƒ½åŠ›ï¼Œä»è€Œç”Ÿæˆæ›´é«˜æ•ˆçš„ä¼˜åŒ–ç­–ç•¥ã€‚è¿™æ ·çš„è®¾è®¡å¯ä»¥æ˜¾è‘—ç¼©çŸ­ä¼˜åŒ–æ—¶é—´å¹¶æé«˜æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSwizzlePerfçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€ç‰¹å¾æå–ã€æ¨¡å‹è®­ç»ƒå’Œä¼˜åŒ–ç”Ÿæˆå››ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œæ”¶é›†å·¥ä½œè´Ÿè½½çš„å†…å­˜è®¿é—®æ¨¡å¼å’Œæ¶æ„ä¿¡æ¯ï¼Œç„¶åé€šè¿‡LLMè¿›è¡Œåˆ†æï¼Œæœ€åç”Ÿæˆé’ˆå¯¹ç‰¹å®šç¡¬ä»¶çš„ä¼˜åŒ–ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šSwizzlePerfçš„æœ€å¤§åˆ›æ–°åœ¨äºå°†ç¡¬ä»¶æ„ŸçŸ¥å¼•å…¥LLMæ€§èƒ½ä¼˜åŒ–ä¸­ï¼Œä½¿å¾—ç”Ÿæˆçš„ä¼˜åŒ–ç­–ç•¥èƒ½å¤Ÿé’ˆå¯¹ç‰¹å®šç¡¬ä»¶è¿›è¡Œå®šåˆ¶ï¼Œæ˜¾è‘—æå‡äº†ä¼˜åŒ–æ•ˆæœã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œè¿™ç§æ–¹æ³•èƒ½å¤Ÿæ›´å¿«åœ°æ‰¾åˆ°æœ€ä½³ä¼˜åŒ–æ–¹æ¡ˆã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒSwizzlePerfé‡‡ç”¨äº†ç‰¹å®šçš„å‚æ•°è®¾ç½®æ¥ä¼˜åŒ–ç”Ÿæˆè¿‡ç¨‹ï¼Œä½¿ç”¨äº†è¿‡æ»¤çš„æ€§èƒ½æ—¥å¿—å’Œå†å²æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œä»¥ç¡®ä¿ç”Ÿæˆçš„ä¼˜åŒ–ç­–ç•¥èƒ½å¤Ÿæœ‰æ•ˆæå‡æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSwizzlePerfåœ¨10ä¸ªå¤šæ ·åŒ–çš„æœºå™¨å­¦ä¹ å’Œç§‘å­¦å†…æ ¸ä¸­æˆåŠŸç”Ÿæˆäº†9ä¸ªå†…æ ¸çš„ä¼˜åŒ–æ¨¡å¼ï¼Œæœ€é«˜å®ç°äº†2.06å€çš„é€Ÿåº¦æå‡å’Œ70%çš„L2å‘½ä¸­ç‡æ”¹å–„ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„ä¼˜åŒ–æ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SwizzlePerfçš„ç ”ç©¶æˆæœåœ¨é«˜æ€§èƒ½è®¡ç®—ã€æ·±åº¦å­¦ä¹ è®­ç»ƒå’Œç§‘å­¦è®¡ç®—ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æé«˜GPUå†…æ ¸çš„æ€§èƒ½ï¼Œèƒ½å¤Ÿæ˜¾è‘—ç¼©çŸ­è®¡ç®—æ—¶é—´ï¼Œé™ä½èƒ½è€—ï¼Œæå‡æ•´ä½“ç³»ç»Ÿæ•ˆç‡ï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„æŠ€æœ¯è¿›æ­¥å’Œåº”ç”¨è½åœ°ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have shown progress in GPU kernel performance engineering using inefficient search-based methods that optimize around runtime. Any existing approach lacks a key characteristic that human performance engineers rely on for near-optimal utilization -- hardware-awareness. By leveraging the workload's specific memory access patterns, architecture specifications, filtered profiling logs, and reflections on historical performance, we can make software-level optimizations that are tailored to the underlying hardware. SwizzlePerf automatically generates spatial optimizations for GPU kernels on disaggregated architectures by giving LLMs explicit hardware-awareness.
>   For a GEMM kernel, SwizzlePerf takes less than 5 minutes to generate the same hardware-specific optimal swizzling pattern that took expert performance engineers 2 weeks to find. On a suite of 10 diverse ML and Science kernels, SwizzlePerf can generate swizzling patterns for 9 of the kernels that achieve up to a 2.06x speedup and 70% improvement in L2 hit rate. This work is the first of many steps toward systematically creating hardware-aware LLM performance engineering agents.

