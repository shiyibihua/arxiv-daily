---
layout: default
title: SLIM: Subtrajectory-Level Elimination for More Effective Reasoning
---

# SLIM: Subtrajectory-Level Elimination for More Effective Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.19502" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.19502v1</a>
  <a href="https://arxiv.org/pdf/2508.19502.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.19502v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.19502v1', 'SLIM: Subtrajectory-Level Elimination for More Effective Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xifeng Yao, Chengyuan Ma, Dongyu Lang, Yinhao Ni, Zhiwei Xu, Huarui Xie, Zihao Chen, Guang Shen, Dandan Tu, Yi Bai, Changzheng Zhang

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-27

**å¤‡æ³¨**: EMNLP 2025 Findings

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSLIMæ¡†æ¶ä»¥ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `å¤æ‚æ¨ç†` `æ¨ç†è½¨è¿¹` `å­è½¨è¿¹ä¼˜åŒ–` `5+2æ¡†æ¶` `æ•°å­¦åŸºå‡†æµ‹è¯•` `æ€§èƒ½æå‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä¸­ç”Ÿæˆçš„æ¨ç†è½¨è¿¹å¹¶ä¸æ€»æ˜¯æœ€ä¼˜ï¼ŒæŸäº›éƒ¨åˆ†å¯èƒ½å¯¹æ•´ä½“æ€§èƒ½äº§ç”Ÿè´Ÿé¢å½±å“ã€‚
2. æœ¬ç ”ç©¶æå‡ºäº†â€œ5+2â€æ¡†æ¶ï¼Œé€šè¿‡ç³»ç»Ÿè¯†åˆ«å’Œè¯„ä¼°ä¸ç†æƒ³çš„å­è½¨è¿¹ï¼Œä¼˜åŒ–æ¨ç†è¿‡ç¨‹çš„æµç•…æ€§å’Œä¸€è‡´æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨æ¨ç†è¿‡ç¨‹ä¸­å‡å°‘äº†25.9%çš„ä¸ç†æƒ³å­è½¨è¿¹ï¼Œå¹¶åœ¨æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤§è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ¨ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå°¤å…¶æ˜¯åœ¨æµ‹è¯•æ—¶æ‰©å±•çš„åº”ç”¨ä¸Šã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ç”Ÿæˆçš„æ¨ç†è½¨è¿¹å¹¶ä¸æ€»æ˜¯æœ€ä¼˜çš„ã€‚æœ¬ç ”ç©¶å°†æ¨ç†è½¨è¿¹åˆ’åˆ†ä¸ºå­è½¨è¿¹ï¼Œå¹¶æå‡ºâ€œ5+2â€æ¡†æ¶ï¼Œç³»ç»Ÿè¯†åˆ«å’Œè¯„ä¼°ä¸ç†æƒ³çš„å­è½¨è¿¹ï¼Œç¡®ä¿å…¶æ¶ˆé™¤ä¸ä¼šå½±å“æ¨ç†è¿‡ç¨‹çš„æ•´ä½“æµç•…æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ¨ç†è¿‡ç¨‹ä¸­å‡å°‘äº†25.9%çš„ä¸ç†æƒ³å­è½¨è¿¹ï¼Œå¹¶åœ¨ä»…ä½¿ç”¨ä¸‰åˆ†ä¹‹äºŒçš„è®­ç»ƒæ•°æ®æ—¶ï¼Œåœ¨å›°éš¾çš„æ•°å­¦åŸºå‡†ä¸Šå®ç°äº†58.92%çš„å¹³å‡å‡†ç¡®ç‡ï¼Œè¶…è¶Šäº†ä½¿ç”¨å…¨éƒ¨æ•°æ®çš„58.06%çš„å‡†ç¡®ç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹æ¨ç†è½¨è¿¹ä¸­å­˜åœ¨çš„ä¸ç†æƒ³å­è½¨è¿¹é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆè¯†åˆ«å’Œæ¶ˆé™¤è¿™äº›å¯¹æ¨ç†æ€§èƒ½æœ‰è´Ÿé¢å½±å“çš„éƒ¨åˆ†ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å°†æ¨ç†è½¨è¿¹åˆ’åˆ†ä¸ºå¤šä¸ªå­è½¨è¿¹ï¼Œåˆ©ç”¨â€œ5+2â€æ¡†æ¶ç³»ç»Ÿè¯†åˆ«ä¸ç†æƒ³å­è½¨è¿¹ï¼Œå¹¶ç¡®ä¿å…¶æ¶ˆé™¤ä¸ä¼šå½±å“æ¨ç†çš„æ•´ä½“æµç•…æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µæ˜¯åŸºäºäº”ä¸ªæ ‡å‡†è¯†åˆ«ä¸ç†æƒ³å­è½¨è¿¹ï¼Œç¬¬äºŒé˜¶æ®µæ˜¯è¯„ä¼°è¿™äº›å­è½¨è¿¹ä¸åç»­å†…å®¹çš„ç‹¬ç«‹æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†â€œ5+2â€æ¡†æ¶ï¼Œç³»ç»ŸåŒ–åœ°è¯†åˆ«å’Œè¯„ä¼°æ¨ç†è½¨è¿¹ä¸­çš„ä¸ç†æƒ³å­è½¨è¿¹ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æé«˜äº†æ¨ç†çš„æœ‰æ•ˆæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¡†æ¶ä¸­ï¼Œè®¾è®¡äº†ç‰¹å®šçš„è¯„ä¼°æ ‡å‡†å’Œé‡‡æ ·ç®—æ³•ï¼Œä»¥ç¡®ä¿é€‰æ‹©çš„æ¨ç†è¿‡ç¨‹å°½å¯èƒ½ä¸åŒ…å«ä¸ç†æƒ³å­è½¨è¿¹ï¼Œä¼˜åŒ–äº†æ•°æ®çš„ä½¿ç”¨æ•ˆç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSLIMæ–¹æ³•åœ¨æ¨ç†è¿‡ç¨‹ä¸­æˆåŠŸå‡å°‘äº†25.9%çš„ä¸ç†æƒ³å­è½¨è¿¹ï¼Œå¹¶åœ¨ä»…ä½¿ç”¨ä¸‰åˆ†ä¹‹äºŒçš„è®­ç»ƒæ•°æ®æ—¶ï¼Œåœ¨å›°éš¾çš„æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†58.92%çš„å¹³å‡å‡†ç¡®ç‡ï¼Œè¶…è¶Šäº†ä½¿ç”¨å…¨éƒ¨æ•°æ®çš„58.06%çš„å‡†ç¡®ç‡ï¼Œå±•ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€é‡‘èå’Œç§‘å­¦ç ”ç©¶ç­‰éœ€è¦å¤æ‚æ¨ç†çš„åœºæ™¯ã€‚é€šè¿‡ä¼˜åŒ–æ¨ç†è¿‡ç¨‹ï¼Œèƒ½å¤Ÿæé«˜å¤§è¯­è¨€æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In recent months, substantial progress has been made in complex reasoning of Large Language Models, particularly through the application of test-time scaling. Notable examples include o1/o3/o4 series and DeepSeek-R1. When responding to a query, these models generate an extended reasoning trajectory, during which the model explores, reflects, backtracks, and self-verifies before arriving at a conclusion. However, fine-tuning models with such reasoning trajectories may not always be optimal. Our findings indicate that not all components within these reasoning trajectories contribute positively to the reasoning process; in fact, some components may affect the overall performance negatively. In this study, we divide a reasoning trajectory into individual subtrajectories and develop a "5+2" framework to: (1) systematically identify suboptimal subtrajectories within the reasoning trajectory based on five human-established criteria; (2) assess the independence of the suboptimal subtrajectories identified in (1) from the subsequent content, ensuring that their elimination does not compromise overall flow and coherence of the reasoning process. Additionally, a sampling algorithm, built upon the "5+2" framework, is employed to select data whose reasoning process is free from suboptimal subtrajectories to the highest degree. Experimental results demonstrate that our method can reduce the number of suboptimal subtrajectories by 25.9\% during the inference. Furthermore, our method achieves an average accuracy of 58.92\% on highly challenging math benchmarks with only two thirds of training data, surpassing the average accuracy of 58.06\% achieved with the entire data, and outperforming open-source datasets, when fine-tuning Qwen2.5-Math-7B. Finally, We validated our method under resource constraints and observed improved performance across various inference token limits.

