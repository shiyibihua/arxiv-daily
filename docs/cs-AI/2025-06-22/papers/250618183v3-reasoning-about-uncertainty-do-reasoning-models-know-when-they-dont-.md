---
layout: default
title: Reasoning about Uncertainty: Do Reasoning Models Know When They Don't Know?
---

# Reasoning about Uncertainty: Do Reasoning Models Know When They Don't Know?

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.18183" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.18183v3</a>
  <a href="https://arxiv.org/pdf/2506.18183.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.18183v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.18183v3', 'Reasoning about Uncertainty: Do Reasoning Models Know When They Don\'t Know?')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Zhiting Mei, Christina Zhang, Tenny Yin, Justin Lidard, Ola Shorinwa, Anirudha Majumdar

**ÂàÜÁ±ª**: cs.AI, cs.CL

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-22 (Êõ¥Êñ∞: 2025-07-18)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫‰∏çÁ°ÆÂÆöÊÄßÈáèÂåñÊñπÊ≥ï‰ª•ÊèêÂçáÊé®ÁêÜÊ®°ÂûãÁöÑÂèØ‰ø°Â∫¶**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Êé®ÁêÜÊ®°Âûã` `‰∏çÁ°ÆÂÆöÊÄßÈáèÂåñ` `Ê®°ÂûãÊ†°ÂáÜ` `ÂÜÖÁúÅÊú∫Âà∂` `Ëá™‰ø°Â∫¶ËØÑ‰º∞`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Êé®ÁêÜÊ®°ÂûãÂú®ÁîüÊàêÂõûÁ≠îÊó∂Â∏∏Â∏∏Ë°®Áé∞Âá∫ËøáÂ∫¶Ëá™‰ø°ÔºåÂ∞§ÂÖ∂ÊòØÂú®ÈîôËØØÂõûÁ≠îÁöÑÊÉÖÂÜµ‰∏ãÔºåÂØºËá¥‰ø°‰ªªÂ∫¶‰∏çË∂≥„ÄÇ
2. ÊèêÂá∫ÂÜÖÁúÅ‰∏çÁ°ÆÂÆöÊÄßÈáèÂåñÊñπÊ≥ïÔºåÈÄöËøáÂàÜÊûêÊé®ÁêÜËøáÁ®ã‰∏≠ÁöÑÊÄùÁª¥ÈìæÊù°Êù•ÊîπÂñÑÊ®°ÂûãÁöÑÊ†°ÂáÜËÉΩÂäõ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÈÉ®ÂàÜÊé®ÁêÜÊ®°ÂûãÂú®ÂÜÖÁúÅÂêéÊ†°ÂáÜÂæóÂà∞‰∫ÜÊîπÂñÑÔºå‰ΩÜ‰πüÊúâÊ®°ÂûãË°®Áé∞Âá∫Êõ¥Â∑ÆÁöÑÊ†°ÂáÜÊÉÖÂÜµ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êé®ÁêÜËØ≠Ë®ÄÊ®°ÂûãÂú®Â§öÈ°πÊåëÊàòÊÄßÂü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫ÜÊúÄÂÖàËøõÁöÑÊàêÁª©Ôºå‰ΩÜ‰ªçÁÑ∂ÂÆπÊòìÁîüÊàêËá™‰ø°‰∏î‰ººÊòØËÄåÈùûÁöÑÈîôËØØÂõûÁ≠î„ÄÇ‰∫ÜËß£‰ΩïÊó∂‰ª•ÂèäÂú®Â§öÂ§ßÁ®ãÂ∫¶‰∏ä‰ø°‰ªªËøô‰∫õÊ®°ÂûãÂØπ‰∫éÂÖ∂Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÂÆâÂÖ®ÈÉ®ÁΩ≤Ëá≥ÂÖ≥ÈáçË¶Å„ÄÇÊú¨ÊñáÊé¢ËÆ®‰∫ÜÊé®ÁêÜÊ®°ÂûãÁöÑ‰∏çÁ°ÆÂÆöÊÄßÈáèÂåñÔºåÊèêÂá∫‰∫ÜÂÜÖÁúÅ‰∏çÁ°ÆÂÆöÊÄßÈáèÂåñÊñπÊ≥ïÔºåËØÑ‰º∞Êé®ÁêÜÊ®°ÂûãÁöÑÊ†°ÂáÜÊÉÖÂÜµÔºåÂπ∂ÂèëÁé∞Êé®ÁêÜÊ®°ÂûãÈÄöÂ∏∏Ëøá‰∫éËá™‰ø°Ôºå‰∏îÊõ¥Ê∑±Â±ÇÊ¨°ÁöÑÊé®ÁêÜ‰ºöÂØºËá¥Êõ¥È´òÁöÑËá™‰ø°Â∫¶„ÄÇÈÄöËøáÂÜÖÁúÅÔºåÈÉ®ÂàÜÊ®°ÂûãÁöÑÊ†°ÂáÜÂæóÂà∞‰∫ÜÊîπÂñÑÔºå‰ΩÜÂπ∂ÈùûÊâÄÊúâÊ®°ÂûãÂùáÂ¶ÇÊ≠§„ÄÇÊúÄÂêéÔºåÊèêÂá∫‰∫ÜËÆæËÆ°ÂøÖË¶ÅÁöÑ‰∏çÁ°ÆÂÆöÊÄßÈáèÂåñÂü∫ÂáÜÂíåÊîπÂñÑÊé®ÁêÜÊ®°ÂûãÊ†°ÂáÜÁöÑÈáçË¶ÅÁ†îÁ©∂ÊñπÂêë„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Êé®ÁêÜÊ®°ÂûãÂú®ÁîüÊàêÂõûÁ≠îÊó∂ÁöÑËøáÂ∫¶Ëá™‰ø°ÈóÆÈ¢òÔºåÁé∞ÊúâÊñπÊ≥ïÊú™ËÉΩÊúâÊïàÈáèÂåñÊ®°ÂûãÁöÑ‰∏çÁ°ÆÂÆöÊÄßÔºåÂØºËá¥ÈîôËØØÂõûÁ≠îÁöÑ‰ø°‰ªªÂ∫¶‰∏çË∂≥„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊèêÂá∫ÂÜÖÁúÅ‰∏çÁ°ÆÂÆöÊÄßÈáèÂåñÊñπÊ≥ïÔºåÈºìÂä±Ê®°ÂûãÂú®ÁîüÊàêÂõûÁ≠îÊó∂ÂèçÊÄùÂÖ∂Êé®ÁêÜËøáÁ®ãÔºå‰ªéËÄåÊèêÈ´òÊ†°ÂáÜËÉΩÂäõ„ÄÇÈÄöËøáËøôÁßçÊñπÂºèÔºåÊ®°ÂûãÂèØ‰ª•Êõ¥Â•ΩÂú∞ËØÜÂà´ÂÖ∂Ëá™‰ø°Â∫¶‰∏éÂÆûÈôÖÊ≠£Á°ÆÊÄß‰πãÈó¥ÁöÑÂ∑ÆË∑ù„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏â‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöÊé®ÁêÜÊ®°ÂûãÁîüÊàêÂõûÁ≠î„ÄÅÂÜÖÁúÅÊú∫Âà∂ÂàÜÊûêÊé®ÁêÜÈìæÊù°„ÄÅ‰ª•Âèä‰∏çÁ°ÆÂÆöÊÄßÈáèÂåñËØÑ‰º∞„ÄÇÊ®°ÂûãÈ¶ñÂÖàÁîüÊàêÂõûÁ≠îÔºåÁÑ∂ÂêéÈÄöËøáÂÜÖÁúÅÊú∫Âà∂ÂØπÊé®ÁêÜËøáÁ®ãËøõË°åËá™ÊàëÊ£ÄÊü•ÔºåÊúÄÂêéËØÑ‰º∞ÂÖ∂Ëá™‰ø°Â∫¶‰∏éÂÆûÈôÖÊ≠£Á°ÆÊÄß„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞Âú®‰∫éÂºïÂÖ•ÂÜÖÁúÅÊú∫Âà∂Ôºå‰ΩøÊé®ÁêÜÊ®°ÂûãËÉΩÂ§üËá™ÊàëÂèçÊÄùÂÖ∂Êé®ÁêÜËøáÁ®ãÔºå‰ªéËÄåÊîπÂñÑÂÖ∂Ê†°ÂáÜËÉΩÂäõ„ÄÇËøô‰∏ÄÊñπÊ≥ï‰∏é‰º†ÁªüÁöÑÂçï‰∏ÄËæìÂá∫Ê®°ÂûãÊòæËëó‰∏çÂêå„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Ê®°ÂûãËÆæËÆ°‰∏≠ÔºåÈááÁî®‰∫ÜËá™ÊàëËØÑ‰º∞ÁöÑÊú∫Âà∂ÔºåËÆæÁΩÆ‰∫ÜÁõ∏Â∫îÁöÑÊçüÂ§±ÂáΩÊï∞‰ª•ÈºìÂä±Ê®°ÂûãÂú®ÂÜÖÁúÅÊó∂Êõ¥ÂáÜÁ°ÆÂú∞ÂèçÊò†ÂÖ∂Ëá™‰ø°Â∫¶ÔºåÂêåÊó∂ÂØπÁΩëÁªúÁªìÊûÑËøõË°å‰∫Ü‰ºòÂåñÔºå‰ª•ÊîØÊåÅÂÜÖÁúÅËøáÁ®ãÁöÑÈ´òÊïàÊâßË°å„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÊé®ÁêÜÊ®°ÂûãÂú®Ëá™‰ø°Â∫¶ËØÑ‰º∞‰∏≠ÈÄöÂ∏∏Ë∂ÖËøá85%ÔºåÂ∞§ÂÖ∂ÊòØÂú®ÈîôËØØÂõûÁ≠îÊó∂Ë°®Áé∞Âá∫Êõ¥È´òÁöÑËá™‰ø°„ÄÇÈÄöËøáÂÜÖÁúÅÔºåÈÉ®ÂàÜÊ®°ÂûãÂ¶Ço3-MiniÂíåDeepSeek R1ÁöÑÊ†°ÂáÜÂæóÂà∞‰∫ÜÊòæËëóÊîπÂñÑÔºåËÄåClaude 3.7 SonnetÂàôË°®Áé∞Âá∫Ê†°ÂáÜËÉΩÂäõ‰∏ãÈôç„ÄÇËøôË°®ÊòéÂÜÖÁúÅÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÂπ∂ÈùûÂØπÊâÄÊúâÊ®°ÂûãÂùáÈÄÇÁî®„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Ëá™Âä®ÈóÆÁ≠îÁ≥ªÁªü„ÄÅÊô∫ËÉΩÂä©ÊâãÂíåÂÜ≥Á≠ñÊîØÊåÅÁ≥ªÁªüÁ≠â„ÄÇÈÄöËøáÊèêÂçáÊé®ÁêÜÊ®°ÂûãÁöÑÂèØ‰ø°Â∫¶ÔºåÂèØ‰ª•Âú®ÂåªÁñó„ÄÅÈáëËûçÂíåÊ≥ïÂæãÁ≠âÈ´òÈ£éÈô©È¢ÜÂüü‰∏≠Êõ¥ÂÆâÂÖ®Âú∞ÈÉ®ÁΩ≤Ëøô‰∫õÊäÄÊúØÔºåÂáèÂ∞ëÈîôËØØÂÜ≥Á≠ñÁöÑÈ£éÈô©ÔºåÂ¢ûÂº∫Áî®Êà∑‰ø°‰ªª„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Reasoning language models have set state-of-the-art (SOTA) records on many challenging benchmarks, enabled by multi-step reasoning induced using reinforcement learning. However, like previous language models, reasoning models are prone to generating confident, plausible responses that are incorrect (hallucinations). Knowing when and how much to trust these models is critical to the safe deployment of reasoning models in real-world applications. To this end, we explore uncertainty quantification of reasoning models in this work. Specifically, we ask three fundamental questions: First, are reasoning models well-calibrated? Second, does deeper reasoning improve model calibration? Finally, inspired by humans' innate ability to double-check their thought processes to verify the validity of their answers and their confidence, we ask: can reasoning models improve their calibration by explicitly reasoning about their chain-of-thought traces? We introduce introspective uncertainty quantification (UQ) to explore this direction. In extensive evaluations on SOTA reasoning models across a broad range of benchmarks, we find that reasoning models: (i) are typically overconfident, with self-verbalized confidence estimates often greater than 85% particularly for incorrect responses, (ii) become even more overconfident with deeper reasoning, and (iii) can become better calibrated through introspection (e.g., o3-Mini and DeepSeek R1) but not uniformly (e.g., Claude 3.7 Sonnet becomes more poorly calibrated). Lastly, we conclude with important research directions to design necessary UQ benchmarks and improve the calibration of reasoning models.

