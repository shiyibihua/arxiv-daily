---
layout: default
title: TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via Autoregressive Diffusion Models
---

# TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via Autoregressive Diffusion Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.03099" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.03099v1</a>
  <a href="https://arxiv.org/pdf/2506.03099.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.03099v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.03099v1', 'TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via Autoregressive Diffusion Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chetwin Low, Weimin Wang

**åˆ†ç±»**: cs.SD, cs.AI, cs.GR

**å‘å¸ƒæ—¥æœŸ**: 2025-06-03

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://aaxwaz.github.io/TalkingMachines/)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTalkingMachinesä»¥å®ç°å®æ—¶éŸ³é¢‘é©±åŠ¨çš„è§†é¢‘ç”Ÿæˆ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `éŸ³é¢‘é©±åŠ¨ç”Ÿæˆ` `å®æ—¶è§†é¢‘ç”Ÿæˆ` `çŸ¥è¯†è’¸é¦` `å¤šæ¨¡æ€èåˆ` `è§’è‰²åŠ¨ç”»`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†é¢‘ç”Ÿæˆæ–¹æ³•åœ¨å®æ—¶æ€§å’ŒéŸ³é¢‘é©±åŠ¨çš„è§’è‰²åŠ¨ç”»æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œéš¾ä»¥å®ç°è‡ªç„¶çš„å¯¹è¯ä½“éªŒã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ç§å°†éŸ³é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ä¸è§†é¢‘ç”Ÿæˆæ¨¡å‹ç»“åˆçš„æ–¹æ³•ï¼Œåˆ©ç”¨çŸ¥è¯†è’¸é¦æŠ€æœ¯æå‡ç”Ÿæˆæ•ˆç‡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒTalkingMachinesåœ¨è§†é¢‘ç”Ÿæˆçš„å»¶è¿Ÿå’Œååé‡ä¸Šæœ‰æ˜¾è‘—æå‡ï¼Œèƒ½å¤Ÿå®ç°é«˜æ•ˆçš„å®æ—¶è§†é¢‘æµä¼ è¾“ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†TalkingMachinesï¼Œä¸€ä¸ªé«˜æ•ˆæ¡†æ¶ï¼Œå°†é¢„è®­ç»ƒçš„è§†é¢‘ç”Ÿæˆæ¨¡å‹è½¬åŒ–ä¸ºå®æ—¶éŸ³é¢‘é©±åŠ¨çš„è§’è‰²åŠ¨ç”»ç”Ÿæˆå™¨ã€‚é€šè¿‡å°†éŸ³é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ä¸è§†é¢‘ç”ŸæˆåŸºç¡€æ¨¡å‹ç›¸ç»“åˆï¼ŒTalkingMachineså®ç°äº†è‡ªç„¶çš„å¯¹è¯ä½“éªŒã€‚ä¸»è¦è´¡çŒ®åŒ…æ‹¬ï¼šå°†é¢„è®­ç»ƒçš„æœ€å…ˆè¿›å›¾åƒåˆ°è§†é¢‘æ¨¡å‹DiTé€‚é…ä¸ºå…·æœ‰180äº¿å‚æ•°çš„éŸ³é¢‘é©±åŠ¨å¤´åƒç”Ÿæˆæ¨¡å‹ï¼›é€šè¿‡ä»åŒå‘æ•™å¸ˆæ¨¡å‹åˆ°ç¨€ç–å› æœè‡ªå›å½’å­¦ç”Ÿæ¨¡å‹çš„ä¸å¯¹ç§°çŸ¥è¯†è’¸é¦ï¼Œå®ç°æ— é™è§†é¢‘æµä¼ è¾“è€Œä¸å‘ç”Ÿé”™è¯¯ç´¯ç§¯ï¼›è®¾è®¡äº†é«˜ååé‡ã€ä½å»¶è¿Ÿçš„æ¨ç†ç®¡é“ï¼ŒåŒ…å«å¤šä¸ªå…³é”®å·¥ç¨‹ä¼˜åŒ–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰éŸ³é¢‘é©±åŠ¨è§†é¢‘ç”Ÿæˆæ–¹æ³•åœ¨å®æ—¶æ€§å’Œç”Ÿæˆè´¨é‡ä¸Šçš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨è‡ªç„¶å¯¹è¯åœºæ™¯ä¸­çš„åº”ç”¨æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å°†é¢„è®­ç»ƒçš„å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆæ¨¡å‹DiTè¿›è¡Œé€‚é…ï¼Œå¹¶ç»“åˆéŸ³é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œè®¾è®¡å‡ºé«˜æ•ˆçš„å®æ—¶ç”Ÿæˆæ¡†æ¶ã€‚é‡‡ç”¨ä¸å¯¹ç§°çŸ¥è¯†è’¸é¦æŠ€æœ¯ï¼Œé¿å…äº†é”™è¯¯ç´¯ç§¯é—®é¢˜ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬éŸ³é¢‘è¾“å…¥å¤„ç†æ¨¡å—ã€è§†é¢‘ç”Ÿæˆæ¨¡å‹å’Œæ¨ç†ç®¡é“ã€‚éŸ³é¢‘è¾“å…¥é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹å¤„ç†åï¼Œé©±åŠ¨è§†é¢‘ç”Ÿæˆæ¨¡å‹ç”Ÿæˆç›¸åº”çš„è§’è‰²åŠ¨ç”»ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†çŸ¥è¯†è’¸é¦åº”ç”¨äºéŸ³é¢‘é©±åŠ¨è§†é¢‘ç”Ÿæˆï¼Œåˆ©ç”¨åŒå‘æ•™å¸ˆæ¨¡å‹ä¸ç¨€ç–å› æœè‡ªå›å½’å­¦ç”Ÿæ¨¡å‹çš„ç»“åˆï¼Œå®ç°äº†é«˜æ•ˆçš„å®æ—¶ç”Ÿæˆã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†åˆ†ç¦»çš„DiTå’ŒVAEè§£ç å™¨ï¼Œä¼˜åŒ–äº†è®¾å¤‡é—´çš„é€šä¿¡ä¸è®¡ç®—é‡å ï¼Œæ¶ˆé™¤äº†å†—ä½™çš„é‡è®¡ç®—ï¼Œä»¥æœ€å¤§åŒ–å¸§ç”Ÿæˆçš„ååé‡ã€‚å…·ä½“å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°çš„é€‰æ‹©ä¹Ÿç»è¿‡ç²¾å¿ƒè°ƒæ•´ï¼Œä»¥ç¡®ä¿ç”Ÿæˆè´¨é‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTalkingMachinesåœ¨è§†é¢‘ç”Ÿæˆçš„å»¶è¿Ÿä¸Šæ˜¾è‘—é™ä½ï¼Œååé‡æå‡è‡³æ¯ç§’ç”Ÿæˆå¤šä¸ªé«˜è´¨é‡å¸§ï¼Œä¸”åœ¨ä¸ç°æœ‰åŸºçº¿æ¨¡å‹çš„å¯¹æ¯”ä¸­ï¼Œç”Ÿæˆè´¨é‡å’Œå®æ—¶æ€§å‡æœ‰æ˜æ˜¾æ”¹å–„ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„ä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è™šæ‹Ÿç°å®ã€åœ¨çº¿æ•™è‚²ã€æ¸¸æˆå¼€å‘åŠç¤¾äº¤åª’ä½“ç­‰ï¼Œèƒ½å¤Ÿä¸ºç”¨æˆ·æä¾›æ›´ä¸ºè‡ªç„¶å’Œæ²‰æµ¸çš„äº¤äº’ä½“éªŒã€‚æœªæ¥ï¼Œéšç€æŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ï¼Œå¯èƒ½ä¼šåœ¨è¿œç¨‹æ²Ÿé€šå’Œåœ¨çº¿å¨±ä¹ç­‰é¢†åŸŸäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In this paper, we present TalkingMachines -- an efficient framework that transforms pretrained video generation models into real-time, audio-driven character animators. TalkingMachines enables natural conversational experiences by integrating an audio large language model (LLM) with our video generation foundation model. Our primary contributions include: (1) We adapt a pretrained SOTA image-to-video DiT into an audio-driven avatar generation model of 18 billion parameters; (2) We enable infinite video streaming without error accumulation through asymmetric knowledge distillation from a bidirectional teacher model into a sparse causal, autoregressive student model; (3) We design a high-throughput, low-latency inference pipeline incorporating several key engineering optimizations such as: (a) disaggregation of the DiT and VAE decoder across separate devices, (b) efficient overlap of inter-device communication and computation using CUDA streams, (c) elimination of redundant recomputations to maximize frame-generation throughput. Please see demo videos here - https://aaxwaz.github.io/TalkingMachines/

