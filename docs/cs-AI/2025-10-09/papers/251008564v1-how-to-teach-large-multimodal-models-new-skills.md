---
layout: default
title: How to Teach Large Multimodal Models New Skills
---

# How to Teach Large Multimodal Models New Skills

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.08564" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.08564v1</a>
  <a href="https://arxiv.org/pdf/2510.08564.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.08564v1" onclick="toggleFavorite(this, '2510.08564v1', 'How to Teach Large Multimodal Models New Skills')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhen Zhu, Yiming Gong, Yao Xiao, Yaoyao Liu, Derek Hoiem

**åˆ†ç±»**: cs.AI, cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-10-09

**å¤‡æ³¨**: In submission. Code is available at https://github.com/jessemelpolio/LMM_CL

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/jessemelpolio/LMM_CL)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸¤ç§é«˜æ•ˆå¾®è°ƒç­–ç•¥ï¼Œæå‡å¤§å‹å¤šæ¨¡æ€æ¨¡å‹æ–°æŠ€èƒ½å­¦ä¹ èƒ½åŠ›å¹¶ç¼“è§£ç¾éš¾æ€§é—å¿˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `æŒç»­å­¦ä¹ ` `ç¾éš¾æ€§é—å¿˜` `å‚æ•°é«˜æ•ˆå¾®è°ƒ` `å¤§å‹è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨å­¦ä¹ æ–°æŠ€èƒ½æ—¶ï¼Œå®¹æ˜“é—å¿˜å·²æŒæ¡çš„çŸ¥è¯†ï¼Œå³å‘ç”Ÿç¾éš¾æ€§é—å¿˜ï¼Œè¿™æ˜¯ä¸€ä¸ªé‡è¦æŒ‘æˆ˜ã€‚
2. è®ºæ–‡æ ¸å¿ƒæ€æƒ³æ˜¯ï¼Œé€šè¿‡åˆ†ææ¨¡å‹è¾“å‡ºtokenåˆ†å¸ƒçš„å˜åŒ–ï¼Œæ‰¾åˆ°å¯¹æ¨¡å‹æ€§èƒ½å½±å“æœ€å°çš„å…³é”®å‚æ•°è¿›è¡Œå¾®è°ƒã€‚
3. å®éªŒè¡¨æ˜ï¼Œä»…æ›´æ–°è‡ªæ³¨æ„åŠ›æŠ•å½±å±‚æˆ–MLPçš„Gate&Upå±‚ï¼Œèƒ½åœ¨å­¦ä¹ æ–°æŠ€èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½ç¾éš¾æ€§é—å¿˜ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç ”ç©¶å¦‚ä½•åœ¨ä¸æŸå¤±åŸæœ‰èƒ½åŠ›çš„å‰æä¸‹ï¼Œä½¿å¤§å‹å¤šæ¨¡æ€æ¨¡å‹(LMMs)å­¦ä¹ æ–°çš„æŠ€èƒ½ã€‚é€šè¿‡åœ¨äº”ä¸ªç›®æ ‡æŠ€èƒ½ä¸Šè¿›è¡Œåºåˆ—å¾®è°ƒï¼Œå¹¶ç›‘æµ‹ä¸‰ä¸ªæ¨¡å‹å®¶æ—åœ¨å…«ä¸ªé¢„ç•™åŸºå‡†ä¸Šçš„é€šç”¨èƒ½åŠ›ï¼Œæˆ‘ä»¬å‘ç°ï¼Œåœ¨çª„åŸŸå¾®è°ƒåï¼Œé¢„ç•™ä»»åŠ¡ä¸Šå‡ºç°çš„â€œé—å¿˜â€ç°è±¡å¯ä»¥åœ¨åç»­é˜¶æ®µéƒ¨åˆ†æ¢å¤ã€‚æˆ‘ä»¬é€šè¿‡ä¸€ç§ç®€å•çš„è®¡æ•°åå·®æ¢é’ˆæ¥è¿½è¸ªè¿™ç§è¡Œä¸ºï¼Œè¯¥æ¢é’ˆä¸é—å¿˜ç°è±¡å…±åŒå˜åŒ–ï¼Œå¹¶ä½“ç°ä¸ºè¾“å‡ºtokenåˆ†å¸ƒçš„å¯æµ‹é‡å˜åŒ–ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬ç¡®å®šäº†ä¸¤ç§ç®€å•è€Œç¨³å¥çš„å¾®è°ƒæ–¹æ³•ï¼Œå¯ä»¥åœ¨å­¦ä¹ æ–°æŠ€èƒ½çš„åŒæ—¶é™åˆ¶æ¨¡å‹æ¼‚ç§»ï¼šï¼ˆiï¼‰ä»…æ›´æ–°è‡ªæ³¨æ„åŠ›æŠ•å½±å±‚ï¼Œä»¥åŠï¼ˆiiï¼‰ä»…æ›´æ–°MLPçš„Gate&Upå±‚ï¼ŒåŒæ—¶å†»ç»“DownæŠ•å½±å±‚ã€‚åœ¨ä¸åŒçš„æ¨¡å‹å’Œä»»åŠ¡ä¸­ï¼Œè¿™äº›é€‰æ‹©åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¿ç•™äº†é¢„ç•™æ€§èƒ½çš„åŒæ—¶ï¼Œå®ç°äº†å¼ºå¤§çš„ç›®æ ‡å¢ç›Šã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨å­¦ä¹ æ–°ä»»åŠ¡æ—¶ï¼Œå®¹æ˜“é—å¿˜ä¹‹å‰å­¦ä¹ åˆ°çš„çŸ¥è¯†ï¼Œå³ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚ç°æœ‰çš„å¾®è°ƒæ–¹æ³•é€šå¸¸ä¼šæ›´æ–°æ‰€æœ‰å‚æ•°ï¼Œå¯¼è‡´æ¨¡å‹åœ¨å­¦ä¹ æ–°æŠ€èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½åœ¨åŸæœ‰ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚å› æ­¤ï¼Œå¦‚ä½•åœ¨å­¦ä¹ æ–°æŠ€èƒ½çš„åŒæ—¶ï¼Œå°½å¯èƒ½ä¿ç•™æ¨¡å‹çš„åŸæœ‰èƒ½åŠ›æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡åˆ†ææ¨¡å‹åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­è¾“å‡ºtokenåˆ†å¸ƒçš„å˜åŒ–ï¼Œæ‰¾åˆ°å¯¹æ¨¡å‹æ€§èƒ½å½±å“æœ€å°çš„å…³é”®å‚æ•°å­é›†è¿›è¡Œæ›´æ–°ã€‚ä½œè€…å‘ç°ï¼Œæ¨¡å‹åœ¨å¾®è°ƒåå‡ºç°çš„â€œé—å¿˜â€ç°è±¡ä¸è¾“å‡ºtokenåˆ†å¸ƒçš„ç‰¹å®šå˜åŒ–ï¼ˆè®¡æ•°åå·®ï¼‰ç›¸å…³ã€‚åŸºäºæ­¤ï¼Œä½œè€…æå‡ºåªæ›´æ–°æ¨¡å‹ä¸­å¯¹è¿™ç§åˆ†å¸ƒå˜åŒ–å½±å“è¾ƒå°çš„å‚æ•°ï¼Œä»è€Œåœ¨å­¦ä¹ æ–°æŠ€èƒ½çš„åŒæ—¶ï¼Œå°½å¯èƒ½ä¿ç•™åŸæœ‰èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡é‡‡ç”¨åºåˆ—å¾®è°ƒçš„æ–¹å¼ï¼Œå³ä¾æ¬¡åœ¨å¤šä¸ªç›®æ ‡æŠ€èƒ½ä¸Šå¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œä½œè€…ç›‘æµ‹æ¨¡å‹åœ¨é¢„ç•™çš„é€šç”¨èƒ½åŠ›åŸºå‡†ä¸Šçš„è¡¨ç°ï¼Œä»¥è¯„ä¼°ç¾éš¾æ€§é—å¿˜çš„ç¨‹åº¦ã€‚åŒæ—¶ï¼Œä½œè€…ä½¿ç”¨è®¡æ•°åå·®æ¢é’ˆæ¥åˆ†ææ¨¡å‹è¾“å‡ºtokenåˆ†å¸ƒçš„å˜åŒ–ã€‚åŸºäºåˆ†æç»“æœï¼Œä½œè€…æå‡ºäº†ä¸¤ç§å¾®è°ƒç­–ç•¥ï¼š(1)ä»…æ›´æ–°è‡ªæ³¨æ„åŠ›æŠ•å½±å±‚ï¼›(2)ä»…æ›´æ–°MLPçš„Gate&Upå±‚ï¼ŒåŒæ—¶å†»ç»“DownæŠ•å½±å±‚ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå‘ç°äº†æ¨¡å‹â€œé—å¿˜â€ç°è±¡ä¸è¾“å‡ºtokenåˆ†å¸ƒå˜åŒ–ä¹‹é—´çš„å…³è”ï¼Œå¹¶åŸºäºæ­¤æå‡ºäº†ä¸¤ç§å‚æ•°é«˜æ•ˆçš„å¾®è°ƒç­–ç•¥ã€‚ä¸ä¼ ç»Ÿçš„å…¨å‚æ•°å¾®è°ƒç›¸æ¯”ï¼Œè¿™ä¸¤ç§ç­–ç•¥èƒ½å¤Ÿåœ¨å­¦ä¹ æ–°æŠ€èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½ç¾éš¾æ€§é—å¿˜çš„ç¨‹åº¦ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†ä¸€ç§ç®€å•çš„è®¡æ•°åå·®æ¢é’ˆï¼Œç”¨äºé‡åŒ–æ¨¡å‹è¾“å‡ºtokenåˆ†å¸ƒçš„å˜åŒ–ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š(1)é€‰æ‹©åˆé€‚çš„è‡ªæ³¨æ„åŠ›æŠ•å½±å±‚å’ŒMLP Gate&Upå±‚ä½œä¸ºå¾®è°ƒå¯¹è±¡ï¼Œè¿™äº›å±‚å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ç›¸å¯¹è¾ƒå°ï¼Œèƒ½å¤Ÿæœ‰æ•ˆé™ä½ç¾éš¾æ€§é—å¿˜ï¼›(2)ä½¿ç”¨åºåˆ—å¾®è°ƒçš„æ–¹å¼ï¼Œé€æ­¥å­¦ä¹ å¤šä¸ªç›®æ ‡æŠ€èƒ½ï¼Œå¹¶ç›‘æµ‹æ¨¡å‹åœ¨é¢„ç•™åŸºå‡†ä¸Šçš„è¡¨ç°ï¼Œä»¥è¯„ä¼°å¾®è°ƒç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼›(3)è®¾è®¡è®¡æ•°åå·®æ¢é’ˆï¼Œç”¨äºé‡åŒ–æ¨¡å‹è¾“å‡ºtokenåˆ†å¸ƒçš„å˜åŒ–ï¼Œä»è€Œæ›´å¥½åœ°ç†è§£æ¨¡å‹â€œé—å¿˜â€ç°è±¡çš„æœ¬è´¨ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä»…æ›´æ–°è‡ªæ³¨æ„åŠ›æŠ•å½±å±‚æˆ–MLPçš„Gate&Upå±‚ï¼Œèƒ½å¤Ÿåœ¨å­¦ä¹ æ–°æŠ€èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½ç¾éš¾æ€§é—å¿˜ã€‚å…·ä½“æ¥è¯´ï¼Œä¸å…¨å‚æ•°å¾®è°ƒç›¸æ¯”ï¼Œè¿™ä¸¤ç§ç­–ç•¥åœ¨é¢„ç•™åŸºå‡†ä¸Šçš„æ€§èƒ½ä¸‹é™å¹…åº¦æ›´å°ï¼ŒåŒæ—¶åœ¨ç›®æ ‡æŠ€èƒ½ä¸Šçš„æ€§èƒ½æå‡ç›¸å½“ã€‚è¿™è¡¨æ˜ï¼Œé€šè¿‡é€‰æ‹©æ€§åœ°æ›´æ–°æ¨¡å‹å‚æ•°ï¼Œå¯ä»¥åœ¨å­¦ä¹ æ–°æŠ€èƒ½çš„åŒæ—¶ï¼Œæ›´å¥½åœ°ä¿ç•™æ¨¡å‹çš„åŸæœ‰èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦æŒç»­å­¦ä¹ å’Œé€‚åº”æ–°ç¯å¢ƒçš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼Œä¾‹å¦‚æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿã€åŒ»ç–—è¯Šæ–­ç³»ç»Ÿç­‰ã€‚é€šè¿‡é€‰æ‹©æ€§åœ°æ›´æ–°æ¨¡å‹å‚æ•°ï¼Œå¯ä»¥åœ¨å­¦ä¹ æ–°æŠ€èƒ½çš„åŒæ—¶ï¼Œä¿æŒæ¨¡å‹åœ¨åŸæœ‰ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ã€‚è¿™å¯¹äºæ„å»ºå¯é ä¸”é€‚åº”æ€§å¼ºçš„AIç³»ç»Ÿè‡³å…³é‡è¦ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> How can we teach large multimodal models (LMMs) new skills without erasing prior abilities? We study sequential fine-tuning on five target skills while monitoring general ability on eight held-out benchmarks across three model families. We observe that apparent "forgetting" on held-out tasks after narrow fine-tuning can partly recover at later stages. We trace this behavior to a measurable shift in the output token distribution, manifested through a simple counting-bias probe that co-varies with forgetting. Guided by this picture, we identify two simple, robust tuning recipes that learn strongly while limiting drift: (i) updating only the self-attention projection layers, and (ii) updating only the MLP Gate&Up while freezing the Down projection. Across models and tasks, these choices deliver strong target gains while largely preserving held-out performance. Code is available at https://github.com/jessemelpolio/LMM_CL

