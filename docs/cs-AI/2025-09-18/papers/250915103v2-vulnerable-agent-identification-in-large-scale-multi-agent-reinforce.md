---
layout: default
title: Vulnerable Agent Identification in Large-Scale Multi-Agent Reinforcement Learning
---

# Vulnerable Agent Identification in Large-Scale Multi-Agent Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.15103" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.15103v2</a>
  <a href="https://arxiv.org/pdf/2509.15103.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.15103v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.15103v2', 'Vulnerable Agent Identification in Large-Scale Multi-Agent Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Simin Li, Zheng Yuwei, Zihao Mao, Linhao Wang, Ruixiao Xu, Chengdong Ma, Xin Yu, Yuqing Ma, Qi Dou, Xin Wang, Jie Luo, Bo An, Yaodong Yang, Weifeng Lv, Xianglong Liu

**åˆ†ç±»**: cs.MA, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-18 (æ›´æ–°: 2025-09-19)

**å¤‡æ³¨**: submitted to NIPS 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHAD-MFCæ¡†æ¶ï¼Œç”¨äºå¤§è§„æ¨¡å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ä¸­è„†å¼±æ™ºèƒ½ä½“çš„è¯†åˆ«ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ` `è„†å¼±æ™ºèƒ½ä½“è¯†åˆ«` `å‡åœºæ§åˆ¶` `åˆ†å±‚ä¼˜åŒ–` `å¯¹æŠ—å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤§è§„æ¨¡å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­éƒ¨åˆ†æ™ºèƒ½ä½“å¤±æ•ˆä¸å¯é¿å…ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆè¯†åˆ«å¯¼è‡´ç³»ç»Ÿæ€§èƒ½ä¸¥é‡ä¸‹é™çš„è„†å¼±æ™ºèƒ½ä½“ã€‚
2. è®ºæ–‡æå‡ºåˆ†å±‚å¯¹æŠ—å»ä¸­å¿ƒåŒ–å‡åœºæ§åˆ¶(HAD-MFC)æ¡†æ¶ï¼Œé€šè¿‡è§£è€¦ä¸Šä¸‹å±‚ä¼˜åŒ–é—®é¢˜ï¼Œå®ç°é«˜æ•ˆçš„è„†å¼±æ™ºèƒ½ä½“è¯†åˆ«ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆè¯†åˆ«å¤§è§„æ¨¡MARLä¸­çš„è„†å¼±æ™ºèƒ½ä½“ï¼Œè¯±å¯¼ç³»ç»Ÿäº§ç”Ÿæ›´ä¸¥é‡çš„æ•…éšœï¼Œå¹¶å­¦ä¹ è„†å¼±æ€§ä»·å€¼å‡½æ•°ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç ”ç©¶äº†å¤§è§„æ¨¡å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ (MARL)ä¸­çš„è„†å¼±æ™ºèƒ½ä½“è¯†åˆ«(VAI)é—®é¢˜ï¼Œå³è¯†åˆ«å‡ºé‚£äº›ä¸€æ—¦è¢«æ”»å‡»å°±ä¼šä¸¥é‡é™ä½ç³»ç»Ÿæ•´ä½“æ€§èƒ½çš„æ™ºèƒ½ä½“å­é›†ã€‚æˆ‘ä»¬å°†VAIå»ºæ¨¡ä¸ºä¸€ä¸ªåˆ†å±‚å¯¹æŠ—å»ä¸­å¿ƒåŒ–å‡åœºæ§åˆ¶(HAD-MFC)é—®é¢˜ï¼Œå…¶ä¸­ä¸Šå±‚æ¶‰åŠé€‰æ‹©æœ€è„†å¼±æ™ºèƒ½ä½“çš„NP-hardç»„åˆä¼˜åŒ–ä»»åŠ¡ï¼Œä¸‹å±‚ä½¿ç”¨å‡åœºMARLä¸ºè¿™äº›æ™ºèƒ½ä½“å­¦ä¹ æœ€åæƒ…å†µä¸‹çš„å¯¹æŠ—ç­–ç•¥ã€‚ä¸ºäº†è§£å†³ä¸Šä¸‹å±‚è€¦åˆçš„é—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆé€šè¿‡Fenchel-Rockafellarå˜æ¢è§£è€¦åˆ†å±‚è¿‡ç¨‹ï¼Œä»è€Œä¸ºä¸Šå±‚äº§ç”Ÿä¸€ä¸ªæ­£åˆ™åŒ–çš„å‡åœºè´å°”æ›¼ç®—å­ï¼Œå®ç°æ¯å±‚ç‹¬ç«‹å­¦ä¹ ï¼Œé™ä½è®¡ç®—å¤æ‚åº¦ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†ä¸Šå±‚ç»„åˆä¼˜åŒ–é—®é¢˜é‡æ–°è¡¨è¿°ä¸ºä¸€ä¸ªMDPï¼Œå…¶å¥–åŠ±æ¥è‡ªæ­£åˆ™åŒ–çš„å‡åœºè´å°”æ›¼ç®—å­ï¼Œä»è€Œå¯ä»¥é€šè¿‡è´ªå©ªå’Œå¼ºåŒ–å­¦ä¹ ç®—æ³•ä¾æ¬¡è¯†åˆ«æœ€è„†å¼±çš„æ™ºèƒ½ä½“ã€‚è¿™ç§åˆ†è§£å¯ä»¥è¯æ˜ä¿ç•™äº†åŸå§‹HAD-MFCçš„æœ€ä¼˜è§£ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°è¯†åˆ«å¤§è§„æ¨¡MARLå’ŒåŸºäºè§„åˆ™çš„ç³»ç»Ÿä¸­çš„æ›´è„†å¼±çš„æ™ºèƒ½ä½“ï¼Œè¯±å¯¼ç³»ç»Ÿäº§ç”Ÿæ›´ä¸¥é‡çš„æ•…éšœï¼Œå¹¶å­¦ä¹ ä¸€ä¸ªèƒ½å¤Ÿæ­ç¤ºæ¯ä¸ªæ™ºèƒ½ä½“è„†å¼±æ€§çš„ä»·å€¼å‡½æ•°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§è§„æ¨¡å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ (MARL)ä¸­è„†å¼±æ™ºèƒ½ä½“è¯†åˆ«(VAI)çš„é—®é¢˜ã€‚å½“ç³»ç»Ÿè§„æ¨¡å¢å¤§æ—¶ï¼Œéƒ¨åˆ†æ™ºèƒ½ä½“å¤±æ•ˆå˜å¾—ä¸å¯é¿å…ï¼Œè€Œè¯†åˆ«å‡ºå“ªäº›æ™ºèƒ½ä½“çš„å¤±æ•ˆä¼šå¯¹ç³»ç»Ÿæ•´ä½“æ€§èƒ½é€ æˆæœ€ä¸¥é‡çš„å½±å“è‡³å…³é‡è¦ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥åœ¨å¤§è§„æ¨¡åœºæ™¯ä¸‹æœ‰æ•ˆåœ°è¯†åˆ«è¿™äº›è„†å¼±æ™ºèƒ½ä½“ï¼Œé¢ä¸´è®¡ç®—å¤æ‚åº¦å’Œç­–ç•¥æ¢ç´¢çš„æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†VAIé—®é¢˜å»ºæ¨¡ä¸ºåˆ†å±‚å¯¹æŠ—ä¼˜åŒ–é—®é¢˜ï¼Œå³HAD-MFCã€‚ä¸Šå±‚è´Ÿè´£é€‰æ‹©æœ€è„†å¼±çš„æ™ºèƒ½ä½“ï¼Œä¸‹å±‚åˆ™å­¦ä¹ é’ˆå¯¹è¿™äº›æ™ºèƒ½ä½“çš„æœ€åæƒ…å†µä¸‹çš„å¯¹æŠ—ç­–ç•¥ã€‚é€šè¿‡Fenchel-Rockafellarå˜æ¢è§£è€¦ä¸Šä¸‹å±‚ï¼Œä½¿å¾—æ¯å±‚å¯ä»¥ç‹¬ç«‹å­¦ä¹ ï¼Œä»è€Œé™ä½è®¡ç®—å¤æ‚åº¦ã€‚åŒæ—¶ï¼Œå°†ä¸Šå±‚ç»„åˆä¼˜åŒ–é—®é¢˜è½¬åŒ–ä¸ºMDPï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•è¿›è¡Œæ±‚è§£ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šHAD-MFCæ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦å±‚æ¬¡ï¼šä¸Šå±‚è„†å¼±æ™ºèƒ½ä½“é€‰æ‹©å’Œä¸‹å±‚å¯¹æŠ—ç­–ç•¥å­¦ä¹ ã€‚é¦–å…ˆï¼Œä½¿ç”¨Fenchel-Rockafellarå˜æ¢å°†åŸå§‹é—®é¢˜è§£è€¦ï¼Œå¾—åˆ°ä¸€ä¸ªæ­£åˆ™åŒ–çš„å‡åœºè´å°”æ›¼ç®—å­ã€‚ç„¶åï¼Œä¸Šå±‚å°†æ™ºèƒ½ä½“é€‰æ‹©é—®é¢˜å»ºæ¨¡ä¸ºMDPï¼Œä½¿ç”¨è´ªå©ªç®—æ³•æˆ–å¼ºåŒ–å­¦ä¹ ç®—æ³•é€‰æ‹©æœ€è„†å¼±çš„æ™ºèƒ½ä½“ã€‚ä¸‹å±‚ä½¿ç”¨å‡åœºMARLç®—æ³•å­¦ä¹ é’ˆå¯¹è¿™äº›æ™ºèƒ½ä½“çš„å¯¹æŠ—ç­–ç•¥ã€‚ä¸Šä¸‹å±‚äº¤æ›¿è¿­ä»£ï¼Œç›´è‡³æ”¶æ•›ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) å°†VAIé—®é¢˜å»ºæ¨¡ä¸ºåˆ†å±‚å¯¹æŠ—ä¼˜åŒ–é—®é¢˜ï¼Œå¹¶æå‡ºHAD-MFCæ¡†æ¶ï¼›2) ä½¿ç”¨Fenchel-Rockafellarå˜æ¢è§£è€¦ä¸Šä¸‹å±‚ä¼˜åŒ–é—®é¢˜ï¼Œé™ä½è®¡ç®—å¤æ‚åº¦ï¼›3) å°†ä¸Šå±‚ç»„åˆä¼˜åŒ–é—®é¢˜è½¬åŒ–ä¸ºMDPï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•è¿›è¡Œæ±‚è§£ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°è¯†åˆ«å¤§è§„æ¨¡MARLä¸­çš„è„†å¼±æ™ºèƒ½ä½“ã€‚

**å…³é”®è®¾è®¡**ï¼šä¸Šå±‚MDPçš„çŠ¶æ€ç©ºé—´ä¸ºå·²é€‰æ‹©çš„è„†å¼±æ™ºèƒ½ä½“é›†åˆï¼ŒåŠ¨ä½œç©ºé—´ä¸ºå‰©ä½™æœªé€‰æ‹©çš„æ™ºèƒ½ä½“é›†åˆï¼Œå¥–åŠ±å‡½æ•°ä¸ºæ­£åˆ™åŒ–çš„å‡åœºè´å°”æ›¼ç®—å­è¾“å‡ºçš„ä»·å€¼å‡½æ•°å¢ç›Šã€‚ä¸‹å±‚ä½¿ç”¨å‡åœºMARLç®—æ³•ï¼Œä¾‹å¦‚Mean Field Actor-Critic (MFAC)ï¼Œå­¦ä¹ å¯¹æŠ—ç­–ç•¥ã€‚æ­£åˆ™åŒ–é¡¹çš„è®¾è®¡æ—¨åœ¨å¹³è¡¡æ¢ç´¢å’Œåˆ©ç”¨ï¼Œé¿å…è¿‡æ—©æ”¶æ•›åˆ°å±€éƒ¨æœ€ä¼˜è§£ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªå¤§è§„æ¨¡MARLç¯å¢ƒä¸­èƒ½å¤Ÿæœ‰æ•ˆåœ°è¯†åˆ«å‡ºæ›´è„†å¼±çš„æ™ºèƒ½ä½“ï¼Œå¹¶è¯±å¯¼ç³»ç»Ÿäº§ç”Ÿæ›´ä¸¥é‡çš„æ•…éšœã€‚ä¾‹å¦‚ï¼Œåœ¨æ˜Ÿé™…äº‰éœ¸IIç¯å¢ƒä¸­ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿè¯†åˆ«å‡ºå…³é”®çš„å…µç§ç»„åˆï¼Œä½¿å¾—æ•Œæ–¹æ›´å®¹æ˜“çªç ´é˜²çº¿ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•å­¦ä¹ åˆ°çš„ä»·å€¼å‡½æ•°èƒ½å¤Ÿå‡†ç¡®åœ°åæ˜ æ¯ä¸ªæ™ºèƒ½ä½“çš„è„†å¼±æ€§ï¼Œä¸ºç³»ç»Ÿè®¾è®¡å’Œä¼˜åŒ–æä¾›äº†æœ‰ä»·å€¼çš„ä¿¡æ¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå¤§è§„æ¨¡æœºå™¨äººé›†ç¾¤ã€æ™ºèƒ½äº¤é€šç³»ç»Ÿã€åˆ†å¸ƒå¼è®¡ç®—ç³»ç»Ÿç­‰é¢†åŸŸã€‚é€šè¿‡è¯†åˆ«ç³»ç»Ÿä¸­çš„è„†å¼±èŠ‚ç‚¹ï¼Œå¯ä»¥æå‰é‡‡å–é˜²å¾¡æªæ–½ï¼Œæé«˜ç³»ç»Ÿçš„é²æ£’æ€§å’Œå¯é æ€§ï¼Œé™ä½ç³»ç»Ÿå´©æºƒçš„é£é™©ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºè¯„ä¼°ä¸åŒæ™ºèƒ½ä½“çš„ä»·å€¼ï¼Œä¸ºèµ„æºåˆ†é…å’Œç³»ç»Ÿä¼˜åŒ–æä¾›æŒ‡å¯¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Partial agent failure becomes inevitable when systems scale up, making it crucial to identify the subset of agents whose compromise would most severely degrade overall performance. In this paper, we study this Vulnerable Agent Identification (VAI) problem in large-scale multi-agent reinforcement learning (MARL). We frame VAI as a Hierarchical Adversarial Decentralized Mean Field Control (HAD-MFC), where the upper level involves an NP-hard combinatorial task of selecting the most vulnerable agents, and the lower level learns worst-case adversarial policies for these agents using mean-field MARL. The two problems are coupled together, making HAD-MFC difficult to solve. To solve this, we first decouple the hierarchical process by Fenchel-Rockafellar transform, resulting a regularized mean-field Bellman operator for upper level that enables independent learning at each level, thus reducing computational complexity. We then reformulate the upper-level combinatorial problem as a MDP with dense rewards from our regularized mean-field Bellman operator, enabling us to sequentially identify the most vulnerable agents by greedy and RL algorithms. This decomposition provably preserves the optimal solution of the original HAD-MFC. Experiments show our method effectively identifies more vulnerable agents in large-scale MARL and the rule-based system, fooling system into worse failures, and learns a value function that reveals the vulnerability of each agent.

