---
layout: default
title: Towards Human-like Multimodal Conversational Agent by Generating Engaging Speech
---

# Towards Human-like Multimodal Conversational Agent by Generating Engaging Speech

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.14627" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.14627v1</a>
  <a href="https://arxiv.org/pdf/2509.14627.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.14627v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.14627v1', 'Towards Human-like Multimodal Conversational Agent by Generating Engaging Speech')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Taesoo Kim, Yongsik Jo, Hyunmin Song, Taehwan Kim

**åˆ†ç±»**: cs.HC, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-18

**å¤‡æ³¨**: Published in Interspeech 2025

**DOI**: [10.21437/Interspeech.2025-1075](https://doi.org/10.21437/Interspeech.2025-1075)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/kimtaesu24/MSenC)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå¤šæ¨¡æ€LLMçš„å¯¹è¯Agentï¼Œé€šè¿‡ç”Ÿæˆæ›´å…·è¡¨ç°åŠ›çš„è¯­éŸ³æå‡äººæœºäº¤äº’ä½“éªŒã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¯¹è¯` `è¯­éŸ³ç”Ÿæˆ` `äººæœºäº¤äº’` `æƒ…æ„Ÿè®¡ç®—` `å‰¯è¯­è¨€ä¿¡æ¯`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¯¹è¯Agentä¸»è¦å…³æ³¨æ–‡æœ¬ç”Ÿæˆï¼Œå¿½ç•¥äº†è¯­éŸ³ä¸­è•´å«çš„æƒ…æ„Ÿå’Œé£æ ¼ä¿¡æ¯ï¼Œå¯¼è‡´äººæœºäº¤äº’ä¸å¤Ÿè‡ªç„¶ã€‚
2. è®ºæ–‡æå‡ºä¸€ç§å¤šæ¨¡æ€LLMæ¡†æ¶ï¼Œç»“åˆè§†è§‰å’Œå¬è§‰ä¿¡æ¯ï¼Œç”ŸæˆåŒ…å«å‰¯è¯­è¨€ä¿¡æ¯çš„è¯­éŸ³ï¼Œä»è€Œæå‡Agentçš„è¡¨è¾¾èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨è§†è§‰å’ŒéŸ³é¢‘æ¨¡æ€ï¼Œç”Ÿæˆæ›´å…·å¸å¼•åŠ›çš„è¯­éŸ³ï¼Œæ”¹å–„äººæœºäº¤äº’ä½“éªŒã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

äººç±»å¯¹è¯æ¶‰åŠè¯­è¨€ã€è¯­éŸ³å’Œè§†è§‰çº¿ç´¢ï¼Œæ¯ç§åª’ä»‹æä¾›äº’è¡¥ä¿¡æ¯ã€‚ä¾‹å¦‚ï¼Œè¯­éŸ³ä¼ è¾¾äº†ä¸€ç§ä»…é æ–‡æœ¬æ— æ³•å®Œå…¨æ•æ‰åˆ°çš„æ°›å›´æˆ–è¯­è°ƒã€‚è™½ç„¶å¤šæ¨¡æ€LLMä¸“æ³¨äºä»å„ç§è¾“å…¥ä¸­ç”Ÿæˆæ–‡æœ¬å“åº”ï¼Œä½†å¯¹ç”Ÿæˆè‡ªç„¶ä¸”å¼•äººå…¥èƒœçš„è¯­éŸ³çš„å…³æ³¨è¾ƒå°‘ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç±»äººAgentï¼Œå®ƒåŸºäºå¯¹è¯æƒ…ç»ªå’Œå“åº”é£æ ¼ä¿¡æ¯ç”Ÿæˆè¯­éŸ³å“åº”ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªæ–°é¢–çš„MultiSensory Conversationæ•°æ®é›†ï¼Œä¸“æ³¨äºè¯­éŸ³ï¼Œä»¥ä½¿Agentèƒ½å¤Ÿç”Ÿæˆè‡ªç„¶è¯­éŸ³ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå¤šæ¨¡æ€LLMçš„æ¨¡å‹ï¼Œç”¨äºç”Ÿæˆæ–‡æœ¬å“åº”å’Œè¯­éŸ³æè¿°ï¼Œè¿™äº›æè¿°ç”¨äºç”ŸæˆåŒ…å«å‰¯è¯­è¨€ä¿¡æ¯çš„è¯­éŸ³ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å¯¹è¯ä¸­åˆ©ç”¨è§†è§‰å’ŒéŸ³é¢‘æ¨¡æ€æ¥ç”Ÿæˆå¼•äººå…¥èƒœçš„è¯­éŸ³æ˜¯æœ‰æ•ˆçš„ã€‚æºä»£ç å¯åœ¨https://github.com/kimtaesu24/MSenC è·å–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰å¯¹è¯Agentåœ¨ç”Ÿæˆå›å¤æ—¶ï¼Œä¸»è¦å…³æ³¨æ–‡æœ¬å†…å®¹çš„å‡†ç¡®æ€§å’Œæµç•…æ€§ï¼Œå¿½ç•¥äº†è¯­éŸ³æœ¬èº«æ‰€æºå¸¦çš„æƒ…æ„Ÿã€è¯­è°ƒç­‰å‰¯è¯­è¨€ä¿¡æ¯ã€‚è¿™å¯¼è‡´ç”Ÿæˆçš„è¯­éŸ³ç¼ºä¹äººæƒ…å‘³ï¼Œä½¿å¾—äººæœºäº¤äº’ä½“éªŒä¸å¤Ÿè‡ªç„¶å’ŒçœŸå®ã€‚å› æ­¤ï¼Œå¦‚ä½•è®©Agentç”Ÿæˆæ›´å…·è¡¨ç°åŠ›çš„è¯­éŸ³ï¼Œæ˜¯æœ¬æ–‡è¦è§£å†³çš„æ ¸å¿ƒé—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤šæ¨¡æ€ä¿¡æ¯ï¼ˆåŒ…æ‹¬è§†è§‰å’Œå¬è§‰ä¿¡æ¯ï¼‰æ¥æŒ‡å¯¼è¯­éŸ³ç”Ÿæˆã€‚é€šè¿‡åˆ†æå¯¹è¯çš„ä¸Šä¸‹æ–‡ï¼Œæå–å¯¹è¯çš„æƒ…ç»ªå’Œå“åº”é£æ ¼ï¼Œå¹¶å°†è¿™äº›ä¿¡æ¯èå…¥åˆ°è¯­éŸ³ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œä»è€Œä½¿ç”Ÿæˆçš„è¯­éŸ³æ›´ç¬¦åˆäººç±»çš„è¡¨è¾¾ä¹ æƒ¯ï¼Œæ›´å…·å¸å¼•åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) MultiSensory Conversationæ•°æ®é›†ï¼šç”¨äºè®­ç»ƒæ¨¡å‹ï¼ŒåŒ…å«è¯­éŸ³ã€æ–‡æœ¬å’Œè§†è§‰ä¿¡æ¯ã€‚2) å¤šæ¨¡æ€LLMï¼šç”¨äºç”Ÿæˆæ–‡æœ¬å“åº”å’Œè¯­éŸ³æè¿°ï¼Œè¾“å…¥åŒ…æ‹¬å¯¹è¯å†å²ã€è§†è§‰ä¿¡æ¯ç­‰ã€‚3) è¯­éŸ³åˆæˆæ¨¡å—ï¼šæ ¹æ®æ–‡æœ¬å“åº”å’Œè¯­éŸ³æè¿°ç”Ÿæˆæœ€ç»ˆçš„è¯­éŸ³ã€‚æ•´ä¸ªæµç¨‹æ˜¯ï¼Œé¦–å…ˆåˆ©ç”¨å¤šæ¨¡æ€LLMåˆ†æå¯¹è¯ä¸Šä¸‹æ–‡ï¼Œç”Ÿæˆæ–‡æœ¬å›å¤å’Œè¯­éŸ³æè¿°ï¼ˆåŒ…å«æƒ…æ„Ÿã€è¯­è°ƒç­‰ä¿¡æ¯ï¼‰ï¼Œç„¶ååˆ©ç”¨è¯­éŸ³åˆæˆæ¨¡å—å°†æ–‡æœ¬å’Œè¯­éŸ³æè¿°åˆæˆä¸ºæœ€ç»ˆçš„è¯­éŸ³è¾“å‡ºã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†å¤šæ¨¡æ€ä¿¡æ¯èå…¥åˆ°è¯­éŸ³ç”Ÿæˆè¿‡ç¨‹ä¸­ã€‚ä»¥å¾€çš„è¯­éŸ³ç”Ÿæˆæ–¹æ³•ä¸»è¦ä¾èµ–äºæ–‡æœ¬ä¿¡æ¯ï¼Œè€Œå¿½ç•¥äº†è§†è§‰å’Œå¬è§‰ä¿¡æ¯ã€‚æœ¬æ–‡é€šè¿‡åˆ©ç”¨è§†è§‰ä¿¡æ¯ï¼ˆä¾‹å¦‚é¢éƒ¨è¡¨æƒ…ï¼‰å’Œå¬è§‰ä¿¡æ¯ï¼ˆä¾‹å¦‚è¯­éŸ³è¯­è°ƒï¼‰ï¼Œå¯ä»¥æ›´å‡†ç¡®åœ°æ•æ‰å¯¹è¯çš„æƒ…ç»ªå’Œé£æ ¼ï¼Œä»è€Œç”Ÿæˆæ›´å…·è¡¨ç°åŠ›çš„è¯­éŸ³ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡æ„å»ºäº†ä¸€ä¸ªæ–°çš„MultiSensory Conversationæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«è¯­éŸ³ã€æ–‡æœ¬å’Œè§†è§‰ä¿¡æ¯ï¼Œä¸ºæ¨¡å‹çš„è®­ç»ƒæä¾›äº†æ•°æ®åŸºç¡€ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜è®¾è®¡äº†ä¸€ä¸ªå¤šæ¨¡æ€LLMï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°èåˆä¸åŒæ¨¡æ€çš„ä¿¡æ¯ï¼Œå¹¶ç”Ÿæˆé«˜è´¨é‡çš„æ–‡æœ¬å“åº”å’Œè¯­éŸ³æè¿°ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç­‰æŠ€æœ¯ç»†èŠ‚åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨è§†è§‰å’ŒéŸ³é¢‘æ¨¡æ€ï¼Œç”Ÿæˆæ›´å…·å¸å¼•åŠ›çš„è¯­éŸ³ã€‚ç›¸è¾ƒäºä¼ ç»Ÿçš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„è¯­éŸ³åœ¨è‡ªç„¶åº¦å’Œæƒ…æ„Ÿè¡¨è¾¾æ–¹é¢æœ‰æ˜¾è‘—æå‡ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œå¯¹æ¯”åŸºçº¿åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†å±•ç¤ºã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ™ºèƒ½å®¢æœã€è™šæ‹ŸåŠ©æ‰‹ã€æ¸¸æˆè§’è‰²ç­‰é¢†åŸŸï¼Œæå‡äººæœºäº¤äº’çš„è‡ªç„¶æ€§å’Œè¶£å‘³æ€§ã€‚é€šè¿‡ç”Ÿæˆæ›´å…·è¡¨ç°åŠ›çš„è¯­éŸ³ï¼Œå¯ä»¥å¢å¼ºç”¨æˆ·çš„æƒ…æ„Ÿè¿æ¥ï¼Œæé«˜ç”¨æˆ·æ»¡æ„åº¦ï¼Œå¹¶ä¸ºæœªæ¥çš„æƒ…æ„Ÿè®¡ç®—å’Œäººæœºåä½œæä¾›æ–°çš„æ€è·¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Human conversation involves language, speech, and visual cues, with each medium providing complementary information. For instance, speech conveys a vibe or tone not fully captured by text alone. While multimodal LLMs focus on generating text responses from diverse inputs, less attention has been paid to generating natural and engaging speech. We propose a human-like agent that generates speech responses based on conversation mood and responsive style information. To achieve this, we build a novel MultiSensory Conversation dataset focused on speech to enable agents to generate natural speech. We then propose a multimodal LLM-based model for generating text responses and voice descriptions, which are used to generate speech covering paralinguistic information. Experimental results demonstrate the effectiveness of utilizing both visual and audio modalities in conversation to generate engaging speech. The source code is available in https://github.com/kimtaesu24/MSenC

