---
layout: default
title: VisCoder: Fine-Tuning LLMs for Executable Python Visualization Code Generation
---

# VisCoder: Fine-Tuning LLMs for Executable Python Visualization Code Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.03930" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.03930v2</a>
  <a href="https://arxiv.org/pdf/2506.03930.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.03930v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.03930v2', 'VisCoder: Fine-Tuning LLMs for Executable Python Visualization Code Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuansheng Ni, Ping Nie, Kai Zou, Xiang Yue, Wenhu Chen

**åˆ†ç±»**: cs.SE, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-04 (æ›´æ–°: 2025-09-29)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVisCoderä»¥è§£å†³å¯æ‰§è¡ŒPythonå¯è§†åŒ–ä»£ç ç”Ÿæˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¯è§†åŒ–ä»£ç ç”Ÿæˆ` `å¤§å‹è¯­è¨€æ¨¡å‹` `Pythonç¼–ç¨‹` `æ•°æ®é›†æ„å»º` `è‡ªæˆ‘ä¿®æ­£æœºåˆ¶` `åé¦ˆé©±åŠ¨å­¦ä¹ ` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¯è§†åŒ–ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ï¼Œç¼ºä¹æœ‰æ•ˆçš„æ‰§è¡Œç›‘ç£å’Œä»£ç ä¿®æ­£æœºåˆ¶ã€‚
2. æœ¬æ–‡æå‡ºVisCode-200Kæ•°æ®é›†ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šå¾®è°ƒæ¨¡å‹ä»¥ç”Ÿæˆå¯æ‰§è¡Œçš„Pythonå¯è§†åŒ–ä»£ç ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒVisCoderåœ¨PandasPlotBenchä¸Šæ˜¾è‘—è¶…è¶Šäº†å¼€æºåŸºçº¿ï¼Œæ¥è¿‘ä¸“æœ‰æ¨¡å‹çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¯è§†åŒ–ä»»åŠ¡ï¼ˆå¦‚ç»˜åˆ¶å›¾è¡¨ï¼‰ä¸­å¸¸å¸¸é¢ä¸´æŒ‘æˆ˜ï¼ŒæˆåŠŸä¾èµ–äºä»£ç çš„æ­£ç¡®æ€§å’Œè§†è§‰è¯­ä¹‰ã€‚ç°æœ‰çš„æŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†ç¼ºä¹æ‰§è¡ŒåŸºç¡€çš„ç›‘ç£ï¼Œä¸”å¯¹è¿­ä»£ä»£ç ä¿®æ­£æ”¯æŒæœ‰é™ï¼Œå¯¼è‡´ç”Ÿæˆçš„å›¾è¡¨è„†å¼±ä¸”ä¸å¯é ã€‚æœ¬æ–‡æå‡ºäº†VisCode-200Kï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„Pythonå¯è§†åŒ–æŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡20ä¸‡ä¸ªç¤ºä¾‹ï¼Œæ¥æºäºå¼€æºä»£ç åº“çš„éªŒè¯ç»˜å›¾ä»£ç åŠè‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œå’Œæ¥è‡ªCode-Feedbackçš„å¤šè½®ä¿®æ­£å¯¹è¯ã€‚æˆ‘ä»¬åœ¨VisCode-200Kä¸Šå¯¹Qwen2.5-Coder-Instructè¿›è¡Œäº†å¾®è°ƒï¼Œåˆ›å»ºäº†VisCoderï¼Œå¹¶åœ¨PandasPlotBenchä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºVisCoderæ˜¾è‘—ä¼˜äºå¼ºå¤§çš„å¼€æºåŸºçº¿ï¼Œæ¥è¿‘GPT-4o-miniç­‰ä¸“æœ‰æ¨¡å‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜é‡‡ç”¨äº†è‡ªæˆ‘è°ƒè¯•è¯„ä¼°åè®®ï¼Œå±•ç¤ºäº†åé¦ˆé©±åŠ¨å­¦ä¹ åœ¨å¯æ‰§è¡Œã€è§†è§‰å‡†ç¡®çš„ä»£ç ç”Ÿæˆä¸­çš„ä¼˜åŠ¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¯è§†åŒ–ä»»åŠ¡ä¸­ç”Ÿæˆå¯æ‰§è¡ŒPythonä»£ç çš„å›°éš¾ï¼Œç°æœ‰æ–¹æ³•ç¼ºä¹æœ‰æ•ˆçš„æ‰§è¡ŒåŸºç¡€ç›‘ç£ï¼Œå¯¼è‡´ç”Ÿæˆçš„å›¾è¡¨è´¨é‡ä¸é«˜ä¸”ä¸å¯é ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æ„å»ºVisCode-200Kæ•°æ®é›†ï¼Œç»“åˆè‡ªç„¶è¯­è¨€æŒ‡ä»¤å’Œå¤šè½®ä»£ç ä¿®æ­£å¯¹è¯ï¼Œæå‡æ¨¡å‹åœ¨å¯è§†åŒ–ä»£ç ç”Ÿæˆä¸­çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†æ„å»ºã€æ¨¡å‹å¾®è°ƒå’Œè¯„ä¼°ä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚æ•°æ®é›†åŒ…å«æ¥è‡ªå¼€æºä»£ç çš„éªŒè¯ç»˜å›¾ä»£ç å’Œä¿®æ­£å¯¹è¯ï¼Œæ¨¡å‹å¾®è°ƒåˆ™åŸºäºQwen2.5-Coder-Instructè¿›è¡Œã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†æ‰§è¡ŒåŸºç¡€çš„ç›‘ç£å’Œåé¦ˆé©±åŠ¨çš„å­¦ä¹ æœºåˆ¶ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­è¿›è¡Œè‡ªæˆ‘ä¿®æ­£ï¼Œä»è€Œæé«˜ä»£ç çš„æ‰§è¡Œå‡†ç¡®æ€§å’Œè§†è§‰æ•ˆæœã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–ç”Ÿæˆä»£ç çš„å¯æ‰§è¡Œæ€§ï¼Œå¹¶è®¾è®¡äº†å¤šè½®å¯¹è¯æœºåˆ¶ä»¥å¢å¼ºæ¨¡å‹çš„è‡ªæˆ‘ä¿®æ­£èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVisCoderåœ¨PandasPlotBenchä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºå¤šä¸ªå¼€æºåŸºçº¿ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œæ¥è¿‘GPT-4o-miniç­‰ä¸“æœ‰æ¨¡å‹çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶åœ¨å¯è§†åŒ–ä»£ç ç”Ÿæˆä¸­çš„å¼ºå¤§èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•°æ®ç§‘å­¦ã€æ•™è‚²å’Œè½¯ä»¶å¼€å‘ç­‰ï¼Œèƒ½å¤Ÿå¸®åŠ©ç”¨æˆ·æ›´é«˜æ•ˆåœ°ç”Ÿæˆå¯è§†åŒ–ä»£ç ï¼Œæå‡æ•°æ®åˆ†æå’Œå±•ç¤ºçš„è´¨é‡ã€‚æœªæ¥ï¼Œéšç€æ¨¡å‹çš„è¿›ä¸€æ­¥ä¼˜åŒ–ï¼Œå¯èƒ½åœ¨æ›´å¹¿æ³›çš„ç¼–ç¨‹è¯­è¨€å’Œå¯è§†åŒ–å·¥å…·ä¸­å¾—åˆ°åº”ç”¨ï¼Œæ¨åŠ¨è‡ªåŠ¨åŒ–ç¼–ç¨‹çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) often struggle with visualization tasks like plotting diagrams, charts, where success depends on both code correctness and visual semantics. Existing instruction-tuning datasets lack execution-grounded supervision and offer limited support for iterative code correction, resulting in fragile and unreliable plot generation. We present VisCode-200K, a large-scale instruction tuning dataset for Python-based visualization and self-correction. It contains over 200K examples from two sources: (1) validated plotting code from open-source repositories, paired with natural language instructions and rendered plots; and (2) 45K multi-turn correction dialogues from Code-Feedback, enabling models to revise faulty code using runtime feedback. We fine-tune Qwen2.5-Coder-Instruct on VisCode-200K to create VisCoder, and evaluate it on PandasPlotBench. VisCoder significantly outperforms strong open-source baselines and approaches the performance of proprietary models like GPT-4o-mini. We further adopt a self-debug evaluation protocol to assess iterative repair, demonstrating the benefits of feedback-driven learning for executable, visually accurate code generation.

