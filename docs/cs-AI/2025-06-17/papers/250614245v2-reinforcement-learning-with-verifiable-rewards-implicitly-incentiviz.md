---
layout: default
title: Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes Correct Reasoning in Base LLMs
---

# Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes Correct Reasoning in Base LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.14245" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.14245v2</a>
  <a href="https://arxiv.org/pdf/2506.14245.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.14245v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.14245v2', 'Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes Correct Reasoning in Base LLMs')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Xumeng Wen, Zihan Liu, Shun Zheng, Shengyu Ye, Zhirong Wu, Yang Wang, Zhijian Xu, Xiao Liang, Junjie Li, Ziming Miao, Jiang Bian, Mao Yang

**ÂàÜÁ±ª**: cs.AI, cs.CL

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-17 (Êõ¥Êñ∞: 2025-10-02)

**Â§áÊ≥®**: Update with more experiments

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫ÂèØÈ™åËØÅÂ•ñÂä±ÁöÑÂº∫ÂåñÂ≠¶‰π†‰ª•ÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Âº∫ÂåñÂ≠¶‰π†` `ÂèØÈ™åËØÅÂ•ñÂä±` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `Êé®ÁêÜËÉΩÂäõ` `ËØÑ‰º∞ÊåáÊ†á` `Âä®ÊÄÅË∞ÉÊï¥` `‰∏≠Èó¥Êé®ÁêÜÊ≠•È™§`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÂú®ÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜËÉΩÂäõÊñπÈù¢Â≠òÂú®‰∫âËÆÆÔºåÂ∞ö‰∏çÊ∏ÖÊ•öÂÖ∂ÊòØÂê¶ÁúüÊ≠£Â¢ûÂº∫‰∫ÜÊé®ÁêÜËÉΩÂäõ„ÄÇ
2. Êú¨ÊñáÊèêÂá∫ÂèØÈ™åËØÅÂ•ñÂä±ÁöÑÂº∫ÂåñÂ≠¶‰π†ÔºàRLVRÔºâÔºåÈÄöËøáËá™Áî±Êé¢Á¥¢Êù•ÊèêÂçáÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõÔºåÂπ∂ÂºïÂÖ•Êñ∞ÁöÑËØÑ‰º∞ÊåáÊ†áCoT-Pass@K„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåRLVRÂú®Êï∞Â≠¶ÂíåÁºñÁ†Å‰ªªÂä°‰∏≠ÊòæËëóÊâ©Â±ï‰∫ÜÊé®ÁêÜËæπÁïåÔºåÊé®ÁêÜË¥®ÈáèÊúâ‰∫ÜÂÆûË¥®ÊÄßÊèêÂçá„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËøëÂπ¥Êù•ÔºåÈïøÈìæÊé®ÁêÜÔºàCoTÔºâÂú®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ‰∏≠ÁöÑÂ∫îÁî®ÂºïËµ∑‰∫ÜÂπøÊ≥õÂÖ≥Ê≥®ÔºåÂ∞§ÂÖ∂ÊòØÈÄöËøáDeepSeek-R1‰ΩøÁî®ÁöÑÁæ§‰ΩìÁõ∏ÂØπÁ≠ñÁï•‰ºòÂåñÁÆóÊ≥ï„ÄÇÊú¨ÊñáÁ≥ªÁªüÊÄßÂú∞Á†îÁ©∂‰∫ÜÂèØÈ™åËØÅÂ•ñÂä±ÁöÑÂº∫ÂåñÂ≠¶‰π†ÔºàRLVRÔºâÂØπLLMÊé®ÁêÜÁöÑÂΩ±Âìç„ÄÇÊàë‰ª¨ÈáçÊñ∞ÂÆ°ËßÜ‰∫ÜPass@KÂÆûÈ™åÔºåËØÅÊòéRLVRËÉΩÂ§üÊâ©Â±ïÊï∞Â≠¶ÂíåÁºñÁ†Å‰ªªÂä°ÁöÑÊé®ÁêÜËæπÁïå„ÄÇÈÄöËøáÂºïÂÖ•Êñ∞ÁöÑËØÑ‰º∞ÊåáÊ†áCoT-Pass@KÔºåÊàë‰ª¨ËÉΩÂ§üÊõ¥ÂÖ®Èù¢Âú∞ÊçïÊçâÊé®ÁêÜÊàêÂäüÁöÑÂõ†Á¥†„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáËøòÊèêÂá∫‰∫ÜÁêÜËÆ∫Ê°ÜÊû∂ÔºåËß£Èáä‰∫ÜRLVRÁöÑÊøÄÂä±Êú∫Âà∂ÔºåË°®ÊòéÂç≥‰ΩøÂ•ñÂä±‰ªÖÂü∫‰∫éÁ≠îÊ°àÁöÑÊ≠£Á°ÆÊÄßÔºåRLVR‰πüËÉΩ‰øÉËøõÊ≠£Á°ÆÊé®ÁêÜ„ÄÇÊàë‰ª¨ÁöÑÂàÜÊûêÊòæÁ§∫ÔºåRLVRÂú®ËÆ≠ÁªÉÂàùÊúüÂ∞±ËÉΩÊøÄÂä±Ê≠£Á°ÆÊé®ÁêÜÔºåÂπ∂ÈÄöËøáÂπøÊ≥õËØÑ‰º∞ËØÅÂÆû‰∫ÜÊé®ÁêÜË¥®ÈáèÁöÑÊòæËëóÊèêÂçá„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÂú®ÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜËÉΩÂäõÊñπÈù¢ÁöÑ‰∏çË∂≥ÔºåÂ∞§ÂÖ∂ÊòØÂØπÊé®ÁêÜËÉΩÂäõÊèêÂçáÁöÑÁúüÂÆûÊÄßÂ≠òÁñë„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÈÄöËøáÂºïÂÖ•ÂèØÈ™åËØÅÂ•ñÂä±ÁöÑÂº∫ÂåñÂ≠¶‰π†ÔºàRLVRÔºâÔºåÂÖÅËÆ∏Ê®°ÂûãÂú®Ëá™Áî±Êé¢Á¥¢‰∏≠Â≠¶‰π†Ôºå‰ªéËÄåÂ¢ûÂº∫Êé®ÁêÜËÉΩÂäõ„ÄÇËÆæËÆ°‰∏äÔºåRLVR‰∏ç‰ªÖÂÖ≥Ê≥®ÊúÄÁªàÁ≠îÊ°àÔºåËøòÈáçËßÜÊé®ÁêÜËøáÁ®ã‰∏≠ÁöÑ‰∏≠Èó¥Ê≠•È™§„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨RLVRÁöÑËÆ≠ÁªÉËøáÁ®ã„ÄÅËØÑ‰º∞ÊåáÊ†áCoT-Pass@KÁöÑËÆ°ÁÆóÔºå‰ª•ÂèäÂØπÊé®ÁêÜÂä®ÊÄÅÁöÑÂàÜÊûê„ÄÇ‰∏ªË¶ÅÊ®°ÂùóÂåÖÊã¨Â•ñÂä±Êú∫Âà∂„ÄÅÊé®ÁêÜËøáÁ®ãË∑üË∏™ÂíåÊÄßËÉΩËØÑ‰º∞„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöRLVRÁöÑÊúÄÂ§ßÂàõÊñ∞Âú®‰∫éÂÖ∂ÊøÄÂä±Êú∫Âà∂ÔºåËÉΩÂ§üÂú®Â•ñÂä±‰ªÖÂü∫‰∫éÁ≠îÊ°àÊ≠£Á°ÆÊÄßÁöÑÊÉÖÂÜµ‰∏ãÔºå‰øÉËøõÊ®°ÂûãÁöÑÊ≠£Á°ÆÊé®ÁêÜ„ÄÇËøô‰∏é‰º†ÁªüÊñπÊ≥ïÁöÑÂçï‰∏ÄÂ•ñÂä±Êú∫Âà∂ÂΩ¢ÊàêÈ≤úÊòéÂØπÊØî„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂèÇÊï∞ËÆæÁΩÆ‰∏äÔºåRLVRÈááÁî®‰∫ÜÂä®ÊÄÅË∞ÉÊï¥ÁöÑÂ•ñÂä±Á≠ñÁï•ÔºåÊçüÂ§±ÂáΩÊï∞ËÆæËÆ°‰∏äÂº∫Ë∞É‰∏≠Èó¥Êé®ÁêÜÊ≠•È™§ÁöÑÈáçË¶ÅÊÄßÔºåÁΩëÁªúÁªìÊûÑÂàôÁªìÂêà‰∫ÜÈïøÁü≠ÊúüËÆ∞ÂøÜÔºàLSTMÔºâÂíåËá™Ê≥®ÊÑèÂäõÊú∫Âà∂‰ª•Â¢ûÂº∫Êé®ÁêÜËÉΩÂäõ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåRLVRÂú®Êï∞Â≠¶ÂíåÁºñÁ†Å‰ªªÂä°‰∏≠ÊòæËëóÊèêÈ´ò‰∫ÜÊé®ÁêÜËÉΩÂäõÔºåPass@KÊåáÊ†áÁöÑÊèêÂçáÂπÖÂ∫¶Ë∂ÖËøá‰∫Ü20%„ÄÇÈÄöËøáÂºïÂÖ•CoT-Pass@KËØÑ‰º∞ÊåáÊ†áÔºåËÉΩÂ§üÊõ¥ÂÖ®Èù¢Âú∞ÂèçÊò†Ê®°ÂûãÁöÑÊé®ÁêÜÊàêÂäüÁéáÔºåÈ™åËØÅ‰∫ÜRLVRÁöÑÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨ÊïôËÇ≤„ÄÅÁºñÁ®ãËæÖÂä©ÂíåËá™Âä®ÂåñÂÜ≥Á≠ñÁ≠â„ÄÇÈÄöËøáÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõÔºåRLVRÂèØ‰ª•Âú®Â§çÊùÇÈóÆÈ¢òÊ±ÇËß£„ÄÅ‰ª£Á†ÅÁîüÊàêÂíåÊô∫ËÉΩÈóÆÁ≠îÁ≠âÂú∫ÊôØ‰∏≠ÂèëÊå•ÈáçË¶Å‰ΩúÁî®ÔºåÊú™Êù•ÂèØËÉΩÊé®Âä®Êõ¥Êô∫ËÉΩÁöÑ‰∫∫Â∑•Êô∫ËÉΩÁ≥ªÁªüÁöÑÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Recent advancements in long chain-of-thought (CoT) reasoning, particularly through the Group Relative Policy Optimization algorithm used by DeepSeek-R1, have led to significant interest in the potential of Reinforcement Learning with Verifiable Rewards (RLVR) for Large Language Models (LLMs). While RLVR promises to improve reasoning by allowing models to learn from free exploration, there remains debate over whether it truly enhances reasoning abilities or simply boosts sampling efficiency. This paper systematically investigates the impact of RLVR on LLM reasoning. We revisit Pass@K experiments and demonstrate that RLVR can extend the reasoning boundary for both mathematical and coding tasks. This is supported by our introduction of a novel evaluation metric, CoT-Pass@K, which captures reasoning success by accounting for both the final answer and intermediate reasoning steps. Furthermore, we present a theoretical framework explaining RLVR's incentive mechanism, demonstrating how it can encourage correct reasoning even when rewards are based solely on answer correctness. Our analysis of RLVR's training dynamics reveals that it incentivizes correct reasoning early in the process, with substantial improvements in reasoning quality confirmed through extensive evaluations. These findings provide strong evidence of RLVR's potential to enhance LLM reasoning, offering valuable insights into its mechanisms and performance improvements.

