---
layout: default
title: Research on Model Parallelism and Data Parallelism Optimization Methods in Large Language Model-Based Recommendation Systems
---

# Research on Model Parallelism and Data Parallelism Optimization Methods in Large Language Model-Based Recommendation Systems

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.17551" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.17551v2</a>
  <a href="https://arxiv.org/pdf/2506.17551.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.17551v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.17551v2', 'Research on Model Parallelism and Data Parallelism Optimization Methods in Large Language Model-Based Recommendation Systems')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Haowei Yang, Yu Tian, Zhongheng Yang, Zhao Wang, Chengrui Zhou, Dannier Li

**åˆ†ç±»**: cs.DC, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-21 (æ›´æ–°: 2025-06-24)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ¨¡å‹å¹¶è¡Œä¸æ•°æ®å¹¶è¡Œä¼˜åŒ–æ–¹æ³•ä»¥è§£å†³æ¨èç³»ç»Ÿä¸­çš„è®¡ç®—ç“¶é¢ˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨èç³»ç»Ÿ` `æ¨¡å‹å¹¶è¡Œ` `æ•°æ®å¹¶è¡Œ` `ä¼˜åŒ–æ–¹æ³•` `åˆ†å¸ƒå¼è®­ç»ƒ` `è´Ÿè½½å‡è¡¡` `æ¢¯åº¦å‹ç¼©`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ¨èç³»ç»Ÿåœ¨ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ—¶é¢ä¸´è®¡ç®—å’Œé€šä¿¡ç“¶é¢ˆï¼Œå½±å“è®­ç»ƒæ•ˆç‡ã€‚
2. æå‡ºæ¨¡å‹å¹¶è¡Œå’Œæ•°æ®å¹¶è¡Œçš„æ··åˆä¼˜åŒ–æ–¹æ¡ˆï¼Œé€šè¿‡è‡ªé€‚åº”è´Ÿè½½å‡è¡¡å’Œé«˜æ•ˆé€šä¿¡æ¡†æ¶æå‡æ€§èƒ½ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ··åˆå¹¶è¡Œæ–¹æ¡ˆè®­ç»ƒååé‡æå‡è¶…è¿‡30%ï¼Œèµ„æºåˆ©ç”¨ç‡æé«˜çº¦20%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨èç³»ç»Ÿä¸­çš„å¿«é€Ÿåº”ç”¨ï¼Œå…¶åºå¤§çš„å‚æ•°è§„æ¨¡å’Œæ•°æ®é‡å¯¼è‡´çš„è®¡ç®—ä¸é€šä¿¡ç“¶é¢ˆæ—¥ç›Šçªå‡ºã€‚æœ¬æ–‡ç³»ç»Ÿç ”ç©¶äº†ä¸¤ç±»ä¼˜åŒ–æ–¹æ³•â€”â€”æ¨¡å‹å¹¶è¡Œå’Œæ•°æ®å¹¶è¡Œâ€”â€”åœ¨æ¨èåœºæ™¯ä¸‹çš„åˆ†å¸ƒå¼è®­ç»ƒã€‚é’ˆå¯¹æ¨¡å‹å¹¶è¡Œï¼Œæˆ‘ä»¬å®ç°äº†å¼ é‡å¹¶è¡Œå’Œæµæ°´çº¿å¹¶è¡Œï¼Œå¹¶å¼•å…¥è‡ªé€‚åº”è´Ÿè½½å‡è¡¡æœºåˆ¶ä»¥å‡å°‘è·¨è®¾å¤‡é€šä¿¡å¼€é”€ã€‚å¯¹äºæ•°æ®å¹¶è¡Œï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†åŒæ­¥ä¸å¼‚æ­¥æ¨¡å¼ï¼Œç»“åˆæ¢¯åº¦å‹ç¼©å’Œç¨€ç–åŒ–æŠ€æœ¯ï¼Œæ„å»ºäº†é«˜æ•ˆçš„èšåˆé€šä¿¡æ¡†æ¶ï¼Œä»¥æ˜¾è‘—æé«˜å¸¦å®½åˆ©ç”¨ç‡ã€‚åœ¨çœŸå®æ¨èæ•°æ®é›†çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬æå‡ºçš„æ··åˆå¹¶è¡Œæ–¹æ¡ˆç›¸æ¯”ä¼ ç»Ÿå•ä¸€æ¨¡å¼å¹¶è¡Œæé«˜äº†30%ä»¥ä¸Šçš„è®­ç»ƒååé‡ï¼Œå¹¶æ”¹å–„äº†çº¦20%çš„èµ„æºåˆ©ç”¨ç‡ï¼ŒåŒæ—¶ä¿æŒäº†è‰¯å¥½çš„å¯æ‰©å±•æ€§å’Œé²æ£’æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†åœ¨çº¿éƒ¨ç½²ä¸­ä¸åŒå¹¶è¡Œç­–ç•¥çš„æƒè¡¡ï¼Œå¹¶æ¦‚è¿°äº†æœªæ¥åœ¨å¼‚æ„ç¡¬ä»¶é›†æˆå’Œè‡ªåŠ¨è°ƒåº¦æŠ€æœ¯æ–¹é¢çš„ç ”ç©¶æ–¹å‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨èç³»ç»Ÿä¸­è®­ç»ƒæ—¶çš„è®¡ç®—ä¸é€šä¿¡ç“¶é¢ˆé—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†åºå¤§å‚æ•°å’Œæ•°æ®æ—¶ï¼Œå¾€å¾€å¯¼è‡´èµ„æºåˆ©ç”¨ç‡ä½ä¸‹ï¼Œè®­ç»ƒæ•ˆç‡ä¸é«˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡ç»“åˆæ¨¡å‹å¹¶è¡Œå’Œæ•°æ®å¹¶è¡Œçš„ä¼˜åŒ–ç­–ç•¥ï¼Œè®¾è®¡å‡ºä¸€ç§æ··åˆå¹¶è¡Œæ–¹æ¡ˆï¼Œä»¥æé«˜è®­ç»ƒæ•ˆç‡å’Œèµ„æºåˆ©ç”¨ç‡ã€‚æ¨¡å‹å¹¶è¡Œé€šè¿‡å¼ é‡å’Œæµæ°´çº¿å¹¶è¡ŒåŒ–æ¥åˆ†æ•£è®¡ç®—è´Ÿè½½ï¼Œè€Œæ•°æ®å¹¶è¡Œåˆ™é€šè¿‡ä¼˜åŒ–é€šä¿¡æ–¹å¼æ¥æå‡å¸¦å®½åˆ©ç”¨ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ¨¡å‹å¹¶è¡Œå’Œæ•°æ®å¹¶è¡Œä¸¤ä¸ªä¸»è¦æ¨¡å—ã€‚æ¨¡å‹å¹¶è¡Œå®ç°å¼ é‡å¹¶è¡Œå’Œæµæ°´çº¿å¹¶è¡Œï¼Œå¹¶å¼•å…¥è‡ªé€‚åº”è´Ÿè½½å‡è¡¡æœºåˆ¶ï¼›æ•°æ®å¹¶è¡Œåˆ™æ¯”è¾ƒåŒæ­¥ä¸å¼‚æ­¥æ¨¡å¼ï¼Œç»“åˆæ¢¯åº¦å‹ç¼©å’Œç¨€ç–åŒ–æŠ€æœ¯ï¼Œæ„å»ºé«˜æ•ˆçš„èšåˆé€šä¿¡æ¡†æ¶ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†è‡ªé€‚åº”è´Ÿè½½å‡è¡¡æœºåˆ¶ï¼Œæ˜¾è‘—å‡å°‘äº†è·¨è®¾å¤‡é€šä¿¡å¼€é”€ï¼ŒåŒæ—¶ç»“åˆäº†æ¢¯åº¦å‹ç¼©ä¸ç¨€ç–åŒ–æŠ€æœ¯ï¼Œæå‡äº†æ•°æ®å¹¶è¡Œçš„æ•ˆç‡ã€‚è¿™äº›åˆ›æ–°ä½¿å¾—æ··åˆå¹¶è¡Œæ–¹æ¡ˆåœ¨æ€§èƒ½ä¸Šä¼˜äºä¼ ç»Ÿå•ä¸€æ¨¡å¼ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹å¹¶è¡Œä¸­ï¼Œé‡‡ç”¨äº†å¼ é‡åˆ†å‰²å’Œæµæ°´çº¿å¤„ç†çš„ç­–ç•¥ï¼›åœ¨æ•°æ®å¹¶è¡Œä¸­ï¼Œè®¾è®¡äº†é«˜æ•ˆçš„èšåˆé€šä¿¡æ¡†æ¶ï¼Œå¹¶ä½¿ç”¨äº†æ¢¯åº¦å‹ç¼©å’Œç¨€ç–åŒ–æŠ€æœ¯æ¥ä¼˜åŒ–æ•°æ®ä¼ è¾“ï¼Œç¡®ä¿äº†è®­ç»ƒè¿‡ç¨‹ä¸­çš„é«˜æ•ˆæ€§å’Œç¨³å®šæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæå‡ºçš„æ··åˆå¹¶è¡Œæ–¹æ¡ˆåœ¨çœŸå®æ¨èæ•°æ®é›†ä¸Šè®­ç»ƒååé‡æé«˜è¶…è¿‡30%ï¼Œèµ„æºåˆ©ç”¨ç‡æå‡çº¦20%ã€‚ä¸ä¼ ç»Ÿå•ä¸€æ¨¡å¼å¹¶è¡Œç›¸æ¯”ï¼Œæ··åˆæ–¹æ¡ˆåœ¨å¯æ‰©å±•æ€§å’Œé²æ£’æ€§æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾ç¤ºå‡ºæ›´å¼ºçš„é€‚åº”æ€§å’Œæ•ˆç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç”µå•†æ¨èã€ç¤¾äº¤åª’ä½“å†…å®¹æ¨èä»¥åŠä¸ªæ€§åŒ–å¹¿å‘ŠæŠ•æ”¾ç­‰ã€‚é€šè¿‡ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡æ¨èç³»ç»Ÿçš„å“åº”é€Ÿåº¦å’Œç”¨æˆ·ä½“éªŒï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚æœªæ¥ï¼Œéšç€å¼‚æ„ç¡¬ä»¶çš„é›†æˆå’Œè‡ªåŠ¨è°ƒåº¦æŠ€æœ¯çš„å‘å±•ï¼Œè¯¥ç ”ç©¶æˆæœæœ‰æœ›è¿›ä¸€æ­¥æ¨åŠ¨æ¨èç³»ç»Ÿçš„æ™ºèƒ½åŒ–è¿›ç¨‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> With the rapid adoption of large language models (LLMs) in recommendation systems, the computational and communication bottlenecks caused by their massive parameter sizes and large data volumes have become increasingly prominent. This paper systematically investigates two classes of optimization methods-model parallelism and data parallelism-for distributed training of LLMs in recommendation scenarios. For model parallelism, we implement both tensor parallelism and pipeline parallelism, and introduce an adaptive load-balancing mechanism to reduce cross-device communication overhead. For data parallelism, we compare synchronous and asynchronous modes, combining gradient compression and sparsification techniques with an efficient aggregation communication framework to significantly improve bandwidth utilization. Experiments conducted on a real-world recommendation dataset in a simulated service environment demonstrate that our proposed hybrid parallelism scheme increases training throughput by over 30% and improves resource utilization by approximately 20% compared to traditional single-mode parallelism, while maintaining strong scalability and robustness. Finally, we discuss trade-offs among different parallel strategies in online deployment and outline future directions involving heterogeneous hardware integration and automated scheduling technologies.

