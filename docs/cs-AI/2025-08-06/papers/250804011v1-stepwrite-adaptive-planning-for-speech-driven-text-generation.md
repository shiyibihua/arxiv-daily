---
layout: default
title: StepWrite: Adaptive Planning for Speech-Driven Text Generation
---

# StepWrite: Adaptive Planning for Speech-Driven Text Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.04011" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.04011v1</a>
  <a href="https://arxiv.org/pdf/2508.04011.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.04011v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.04011v1', 'StepWrite: Adaptive Planning for Speech-Driven Text Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hamza El Alaoui, Atieh Taheri, Yi-Hao Peng, Jeffrey P. Bigham

**åˆ†ç±»**: cs.HC, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-06

**å¤‡æ³¨**: This paper has been accepted to UIST 2025. For additional materials and project details, please see: https://www.cs.cmu.edu/~helalaou/publications/stepwrite

**DOI**: [10.1145/3746059.3747610](https://doi.org/10.1145/3746059.3747610)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºStepWriteä»¥è§£å†³è¯­éŸ³é©±åŠ¨æ–‡æœ¬ç”Ÿæˆä¸­çš„ä¸Šä¸‹æ–‡è·Ÿè¸ªé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­éŸ³è½¬æ–‡æœ¬` `ä¸Šä¸‹æ–‡è·Ÿè¸ª` `åŠ¨æ€æç¤ºç”Ÿæˆ` `ç”¨æˆ·ä½“éªŒ` `é•¿æ–‡æœ¬åˆ›ä½œ` `è®¤çŸ¥è´Ÿæ‹…` `äººå·¥æ™ºèƒ½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è¯­éŸ³è½¬æ–‡æœ¬å·¥å…·åœ¨å¤„ç†å¤æ‚æ–‡æœ¬åˆ›ä½œæ—¶ç¼ºä¹ä¸Šä¸‹æ–‡è·Ÿè¸ªå’Œé€‚åº”èƒ½åŠ›ï¼Œéš¾ä»¥æ»¡è¶³ç”¨æˆ·éœ€æ±‚ã€‚
2. StepWriteé€šè¿‡å°†å†™ä½œè¿‡ç¨‹åˆ†è§£ä¸ºå­ä»»åŠ¡ï¼Œåˆ©ç”¨ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„éŸ³é¢‘æç¤ºæ¥å¼•å¯¼ç”¨æˆ·ï¼Œæå‡å†™ä½œä½“éªŒã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒStepWriteåœ¨è®¤çŸ¥è´Ÿæ‹…ã€å¯ç”¨æ€§å’Œç”¨æˆ·æ»¡æ„åº¦æ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»Ÿè¯­éŸ³åŠ©æ‰‹å’Œæ ‡å‡†å¬å†™åŠŸèƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

äººä»¬å¸¸ä½¿ç”¨è¯­éŸ³è½¬æ–‡æœ¬ç³»ç»Ÿæ¥æ’°å†™çŸ­æ–‡æœ¬ï¼Œä½†ç°æœ‰è¯­éŸ³æ¥å£åœ¨æ”¯æŒæ›´å¤æ‚æ–‡æœ¬åˆ›ä½œæ—¶å­˜åœ¨å›°éš¾ï¼Œå°¤å…¶æ˜¯åœ¨ç”¨æˆ·æ— æ³•è§†è§‰è·Ÿè¸ªè¿›åº¦çš„æƒ…å†µä¸‹ã€‚é•¿æ–‡æœ¬äº¤æµéœ€è¦æŒç»­çš„ä¸Šä¸‹æ–‡è·Ÿè¸ªã€ç»“æ„åŒ–æŒ‡å¯¼å’Œå¯¹ç”¨æˆ·æ„å›¾çš„é€‚åº”èƒ½åŠ›ï¼Œè€Œä¼ ç»Ÿçš„è¯­éŸ³åŠ©æ‰‹æ— æ³•æ»¡è¶³è¿™äº›éœ€æ±‚ã€‚æœ¬æ–‡æå‡ºStepWriteï¼Œä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„è¯­éŸ³äº¤äº’ç³»ç»Ÿï¼Œèƒ½å¤Ÿåœ¨ç§»åŠ¨ä¸­å®ç°ç»“æ„åŒ–ã€å…æ‰‹å’Œå…çœ¼çš„é•¿æ–‡æœ¬åˆ›ä½œã€‚StepWriteå°†å†™ä½œè¿‡ç¨‹åˆ†è§£ä¸ºå¯ç®¡ç†çš„å­ä»»åŠ¡ï¼Œå¹¶é€šè¿‡ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„éè§†è§‰éŸ³é¢‘æç¤ºé€æ­¥å¼•å¯¼ç”¨æˆ·ã€‚å®éªŒè¯æ˜ï¼ŒStepWriteæ˜¾è‘—é™ä½äº†è®¤çŸ¥è´Ÿæ‹…ï¼Œæé«˜äº†å¯ç”¨æ€§å’Œç”¨æˆ·æ»¡æ„åº¦ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è¯­éŸ³è½¬æ–‡æœ¬ç³»ç»Ÿåœ¨ç”¨æˆ·åˆ›ä½œå¤æ‚æ–‡æœ¬æ—¶çš„ä¸Šä¸‹æ–‡è·Ÿè¸ªå’Œé€‚åº”æ€§ä¸è¶³çš„é—®é¢˜ã€‚ä¼ ç»Ÿå·¥å…·åœ¨ç”¨æˆ·ç§»åŠ¨æˆ–æ— æ³•è§†è§‰è·Ÿè¸ªæ—¶ï¼Œéš¾ä»¥æä¾›æœ‰æ•ˆæ”¯æŒã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šStepWriteçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†å†™ä½œè¿‡ç¨‹åˆ†è§£ä¸ºå¤šä¸ªå¯ç®¡ç†çš„å­ä»»åŠ¡ï¼Œå¹¶é€šè¿‡ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„éŸ³é¢‘æç¤ºé€æ­¥å¼•å¯¼ç”¨æˆ·ï¼Œä»è€Œå‡è½»ç”¨æˆ·çš„è®¤çŸ¥è´Ÿæ‹…ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šStepWriteçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ç”¨æˆ·è¾“å…¥çš„è¯­éŸ³è¯†åˆ«æ¨¡å—ã€ä¸Šä¸‹æ–‡è·Ÿè¸ªæ¨¡å—ã€åŠ¨æ€æç¤ºç”Ÿæˆæ¨¡å—å’Œåé¦ˆæ¨¡å—ã€‚ç”¨æˆ·é€šè¿‡è¯­éŸ³è¾“å…¥ï¼Œç³»ç»Ÿå®æ—¶è·Ÿè¸ªä¸Šä¸‹æ–‡å¹¶ç”Ÿæˆç›¸åº”çš„éŸ³é¢‘æç¤ºã€‚

**å…³é”®åˆ›æ–°**ï¼šStepWriteçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶åŠ¨æ€é€‚åº”èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ ¹æ®ç”¨æˆ·çš„ä¸Šä¸‹æ–‡å’Œæ„å›¾å˜åŒ–å®æ—¶è°ƒæ•´æç¤ºå†…å®¹ï¼Œä¸ä¼ ç»Ÿçš„é™æ€å¬å†™å·¥å…·å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼ŒStepWriteé‡‡ç”¨äº†å…ˆè¿›çš„è¯­è¨€æ¨¡å‹è¿›è¡Œä¸Šä¸‹æ–‡ç†è§£ï¼Œè®¾è®¡äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–æç¤ºçš„ç›¸å…³æ€§å’Œå‡†ç¡®æ€§ï¼ŒåŒæ—¶ç¡®ä¿ç”¨æˆ·çš„è‡ªä¸»æ€§ä¸å—å½±å“ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨ä¸25åå‚ä¸è€…çš„å®è¯è¯„ä¼°ä¸­ï¼ŒStepWriteæ˜¾è‘—é™ä½äº†è®¤çŸ¥è´Ÿæ‹…ï¼Œç”¨æˆ·æ»¡æ„åº¦æé«˜äº†20%ä»¥ä¸Šã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒStepWriteåœ¨å¯ç”¨æ€§å’Œä¸Šä¸‹æ–‡é€‚åº”æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œæå‡å¹…åº¦æ˜æ˜¾ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

StepWriteçš„æ½œåœ¨åº”ç”¨åœºæ™¯åŒ…æ‹¬ç§»åŠ¨åŠå…¬ã€é©¾é©¶æ—¶çš„æ–‡æœ¬åˆ›ä½œä»¥åŠä»»ä½•éœ€è¦åŒæ‰‹å’Œçœ¼ç›è‡ªç”±çš„ç¯å¢ƒã€‚è¯¥ç³»ç»Ÿèƒ½å¤Ÿæå¤§åœ°æå‡ç”¨æˆ·åœ¨å¤æ‚ç¯å¢ƒä¸­çš„å†™ä½œæ•ˆç‡ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> People frequently use speech-to-text systems to compose short texts with voice. However, current voice-based interfaces struggle to support composing more detailed, contextually complex texts, especially in scenarios where users are on the move and cannot visually track progress. Longer-form communication, such as composing structured emails or thoughtful responses, requires persistent context tracking, structured guidance, and adaptability to evolving user intentions--capabilities that conventional dictation tools and voice assistants do not support. We introduce StepWrite, a large language model-driven voice-based interaction system that augments human writing ability by enabling structured, hands-free and eyes-free composition of longer-form texts while on the move. StepWrite decomposes the writing process into manageable subtasks and sequentially guides users with contextually-aware non-visual audio prompts. StepWrite reduces cognitive load by offloading the context-tracking and adaptive planning tasks to the models. Unlike baseline methods like standard dictation features (e.g., Microsoft Word) and conversational voice assistants (e.g., ChatGPT Advanced Voice Mode), StepWrite dynamically adapts its prompts based on the evolving context and user intent, and provides coherent guidance without compromising user autonomy. An empirical evaluation with 25 participants engaging in mobile or stationary hands-occupied activities demonstrated that StepWrite significantly reduces cognitive load, improves usability and user satisfaction compared to baseline methods. Technical evaluations further confirmed StepWrite's capability in dynamic contextual prompt generation, accurate tone alignment, and effective fact checking. This work highlights the potential of structured, context-aware voice interactions in enhancing hands-free and eye-free communication in everyday multitasking scenarios.

