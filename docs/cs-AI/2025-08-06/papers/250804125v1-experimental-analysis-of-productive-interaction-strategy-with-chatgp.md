---
layout: default
title: Experimental Analysis of Productive Interaction Strategy with ChatGPT: User Study on Function and Project-level Code Generation Tasks
---

# Experimental Analysis of Productive Interaction Strategy with ChatGPT: User Study on Function and Project-level Code Generation Tasks

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.04125" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.04125v1</a>
  <a href="https://arxiv.org/pdf/2508.04125.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.04125v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.04125v1', 'Experimental Analysis of Productive Interaction Strategy with ChatGPT: User Study on Function and Project-level Code Generation Tasks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sangwon Hyun, Hyunjun Kim, Jinhyuk Jang, Hyojin Choi, M. Ali Babar

**åˆ†ç±»**: cs.SE, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-06

**å¤‡æ³¨**: The benchmark repository has not been publicly released yet due to the IP policy in our institutions. If you would like to use the benchmark or collaborate on extension, please contact "dr.sangwon.hyun@gmail.com"

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæœ‰æ•ˆçš„äº¤äº’ç­–ç•¥ä»¥æå‡ChatGPTåœ¨ä»£ç ç”Ÿæˆä¸­çš„ç”Ÿäº§åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `äººæœºäº¤äº’` `ä»£ç ç”Ÿæˆ` `å¤§å‹è¯­è¨€æ¨¡å‹` `è½¯ä»¶å·¥ç¨‹` `ç”¨æˆ·ç ”ç©¶` `ç”Ÿäº§åŠ›æå‡` `é”™è¯¯åˆ†ç±»`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨åŠŸèƒ½çº§åˆ«çš„æç¤ºæ¨¡å¼ï¼Œå¿½è§†äº†å¤æ‚çš„å¤šç±»ä¾èµ–å’Œäººæœºäº¤äº’ç‰¹å¾å¯¹ä»£ç ç”Ÿæˆçš„å½±å“ã€‚
2. æœ¬æ–‡è®¾è®¡äº†ä¸€é¡¹å®éªŒï¼Œåˆ†æäººæœºäº¤äº’ç‰¹å¾å¯¹ä»£ç ç”Ÿæˆç”Ÿäº§åŠ›çš„å½±å“ï¼Œæå‡ºäº†é¡¹ç›®çº§åŸºå‡†ä»»åŠ¡ã€‚
3. ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæœ‰3ä¸ªäº¤äº’ç‰¹å¾æ˜¾è‘—å½±å“ç”Ÿäº§åŠ›ï¼Œå¹¶æå‡ºäº†5æ¡æå‡äººæœºäº¤äº’è¿‡ç¨‹çš„æŒ‡å¯¼åŸåˆ™ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ä¸­çš„åº”ç”¨æ—¥ç›Šå¢é•¿ã€‚ç„¶è€Œï¼Œç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨åŠŸèƒ½çº§åˆ«çš„æç¤ºæ¨¡å¼ï¼Œå¿½è§†äº†æ›´å¤æ‚çš„çœŸå®å·¥ä½œæµç¨‹ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡è®¾è®¡äº†ä¸€é¡¹å®éªŒï¼Œå…¨é¢åˆ†æäº†äººæœºäº¤äº’ç‰¹å¾å¯¹ä»£ç ç”Ÿæˆç”Ÿäº§åŠ›çš„å½±å“ã€‚ç ”ç©¶ä¸­æå‡ºäº†ä¸¤ä¸ªé¡¹ç›®çº§åŸºå‡†ä»»åŠ¡ï¼Œå¹¶é€šè¿‡ä¸36åæ¥è‡ªä¸åŒèƒŒæ™¯çš„å‚ä¸è€…çš„ç”¨æˆ·ç ”ç©¶ï¼Œæ¢è®¨äº†ç‰¹å®šæç¤ºæ¨¡å¼ä¸‹çš„äº¤äº’ä½“éªŒã€‚ç»“æœæ˜¾ç¤ºï¼Œ15ä¸ªäº¤äº’ç‰¹å¾ä¸­æœ‰3ä¸ªæ˜¾è‘—å½±å“ä»£ç ç”Ÿæˆçš„ç”Ÿäº§åŠ›ï¼Œå¹¶æå‡ºäº†5æ¡æå‡äººæœºäº¤äº’è¿‡ç¨‹ç”Ÿäº§åŠ›çš„æŒ‡å¯¼åŸåˆ™ï¼Œä»¥åŠ29ç§å¯èƒ½å‡ºç°çš„è¿è¡Œæ—¶å’Œé€»è¾‘é”™è¯¯çš„åˆ†ç±»åŠç¼“è§£æ–¹æ¡ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰ç ”ç©¶åœ¨ä»£ç ç”Ÿæˆä¸­å¯¹äººæœºäº¤äº’ç‰¹å¾å…³æ³¨ä¸è¶³çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚çš„å¤šç±»ä¾èµ–åœºæ™¯ä¸‹çš„åº”ç”¨ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­äºåŠŸèƒ½çº§åˆ«ï¼Œç¼ºä¹å¯¹é¡¹ç›®çº§åˆ«å¤æ‚æ€§çš„è€ƒè™‘ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡è®¾è®¡å®éªŒï¼Œå…¨é¢åˆ†æäººæœºäº¤äº’ç‰¹å¾å¯¹ä»£ç ç”Ÿæˆç”Ÿäº§åŠ›çš„å½±å“ï¼Œæ‰©å±•ç ”ç©¶èŒƒå›´è‡³é¡¹ç›®çº§åˆ«ä»»åŠ¡ï¼Œä»¥æ›´å¥½åœ°åæ˜ çœŸå®å·¥ä½œæµç¨‹ä¸­çš„å¤æ‚æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆæ˜¯è®¾è®¡é¡¹ç›®çº§åŸºå‡†ä»»åŠ¡ï¼Œå…¶æ¬¡æ˜¯è¿›è¡Œç”¨æˆ·ç ”ç©¶ï¼Œæ”¶é›†å‚ä¸è€…ä¸GPTåŠ©æ‰‹äº¤äº’çš„å±å¹•å½•åˆ¶å’ŒèŠå¤©è®°å½•è¿›è¡Œåˆ†æã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ä¸ªæ¶µç›–å¤šç±»ä¾èµ–çš„é¡¹ç›®çº§ä»»åŠ¡æ¡†æ¶ï¼Œå¹¶è¯†åˆ«å‡ºå½±å“ç”Ÿäº§åŠ›çš„ç‰¹å®šäººæœºäº¤äº’ç‰¹å¾ï¼Œä¸ä¼ ç»Ÿçš„åŠŸèƒ½çº§åˆ«ç ”ç©¶å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šç ”ç©¶ä¸­åˆ†æäº†15ä¸ªäº¤äº’ç‰¹å¾ï¼Œç¡®å®šäº†3ä¸ªæ˜¾è‘—å½±å“ç”Ÿäº§åŠ›çš„ç‰¹å¾ï¼Œå¹¶æå‡ºäº†5æ¡æŒ‡å¯¼åŸåˆ™å’Œ29ç§é”™è¯¯åˆ†ç±»åŠå…¶ç¼“è§£æ–¹æ¡ˆã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œ15ä¸ªäº¤äº’ç‰¹å¾ä¸­æœ‰3ä¸ªæ˜¾è‘—å½±å“ä»£ç ç”Ÿæˆçš„ç”Ÿäº§åŠ›ï¼Œæä¾›äº†5æ¡æå‡äººæœºäº¤äº’è¿‡ç¨‹çš„æŒ‡å¯¼åŸåˆ™ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜åˆ†ç±»äº†29ç§å¯èƒ½å‡ºç°çš„è¿è¡Œæ—¶å’Œé€»è¾‘é”™è¯¯ï¼Œå¹¶æå‡ºäº†ç›¸åº”çš„ç¼“è§£æ–¹æ¡ˆã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è½¯ä»¶å¼€å‘ã€è‡ªåŠ¨åŒ–ä»£ç ç”Ÿæˆå’Œäººæœºäº¤äº’è®¾è®¡ç­‰ã€‚é€šè¿‡ä¼˜åŒ–äººæœºäº¤äº’ç­–ç•¥ï¼Œå¯ä»¥æå‡å¼€å‘è€…åœ¨ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ—¶çš„ç”Ÿäº§åŠ›ï¼Œè¿›è€Œæ¨åŠ¨è½¯ä»¶å·¥ç¨‹é¢†åŸŸçš„åˆ›æ–°ä¸æ•ˆç‡æå‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The application of Large Language Models (LLMs) is growing in the productive completion of Software Engineering tasks. Yet, studies investigating the productive prompting techniques often employed a limited problem space, primarily focusing on well-known prompting patterns and mainly targeting function-level SE practices. We identify significant gaps in real-world workflows that involve complexities beyond class-level (e.g., multi-class dependencies) and different features that can impact Human-LLM Interactions (HLIs) processes in code generation. To address these issues, we designed an experiment that comprehensively analyzed the HLI features regarding the code generation productivity. Our study presents two project-level benchmark tasks, extending beyond function-level evaluations. We conducted a user study with 36 participants from diverse backgrounds, asking them to solve the assigned tasks by interacting with the GPT assistant using specific prompting patterns. We also examined the participants' experience and their behavioral features during interactions by analyzing screen recordings and GPT chat logs. Our statistical and empirical investigation revealed (1) that three out of 15 HLI features significantly impacted the productivity in code generation; (2) five primary guidelines for enhancing productivity for HLI processes; and (3) a taxonomy of 29 runtime and logic errors that can occur during HLI processes, along with suggested mitigation plans.

