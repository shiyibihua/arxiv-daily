---
layout: default
title: Adversarial Attacks and Defenses on Graph-aware Large Language Models (LLMs)
---

# Adversarial Attacks and Defenses on Graph-aware Large Language Models (LLMs)

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.04894" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.04894v1</a>
  <a href="https://arxiv.org/pdf/2508.04894.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.04894v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.04894v1', 'Adversarial Attacks and Defenses on Graph-aware Large Language Models (LLMs)')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Iyiola E. Olatunji, Franziska Boenisch, Jing Xu, Adam Dziedzic

**åˆ†ç±»**: cs.CR, cs.AI, cs.SI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-06

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé’ˆå¯¹å›¾æ„ŸçŸ¥å¤§è¯­è¨€æ¨¡å‹çš„å¯¹æŠ—æ”»å‡»ä¸é˜²å¾¡æ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¯¹æŠ—æ”»å‡»` `å¤§è¯­è¨€æ¨¡å‹` `å›¾ç¥ç»ç½‘ç»œ` `é²æ£’æ€§` `é˜²å¾¡æ¡†æ¶` `èŠ‚ç‚¹åˆ†ç±»` `ç‰¹å¾ä¿®æ­£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å›¾æ„ŸçŸ¥å¤§è¯­è¨€æ¨¡å‹åœ¨é¢å¯¹å¯¹æŠ—æ”»å‡»æ—¶çš„è„†å¼±æ€§å°šæœªè¢«å……åˆ†ç ”ç©¶ï¼Œå°¤å…¶æ˜¯åœ¨èŠ‚ç‚¹åˆ†ç±»ä»»åŠ¡ä¸­ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„é˜²å¾¡æ¡†æ¶GALGUARDï¼Œç»“åˆäº†ç‰¹å¾ä¿®æ­£å’ŒGNNé˜²å¾¡ç­–ç•¥ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„é²æ£’æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒLLAGAåœ¨èŠ‚ç‚¹åºåˆ—æ¨¡æ¿ä¸‹æ›´æ˜“å—åˆ°æ”»å‡»ï¼Œè€ŒGRAPHPROMPTERåˆ™å±•ç°å‡ºæ›´é«˜çš„æŠ—æ”»å‡»èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸å›¾ç»“æ„æ•°æ®çš„ç»“åˆæ—¥ç›Šå¢å¤šï¼Œå°¤å…¶åœ¨èŠ‚ç‚¹åˆ†ç±»ç­‰ä»»åŠ¡ä¸­ï¼ŒLLMsçš„é²æ£’æ€§å¯¹æŠ—æ”»å‡»çš„ç ”ç©¶å°šæœªæ·±å…¥ã€‚æœ¬æ–‡é¦–æ¬¡æ¢è®¨äº†å›¾æ„ŸçŸ¥LLMsçš„è„†å¼±æ€§ï¼Œåˆ©ç”¨ç°æœ‰é’ˆå¯¹å›¾æ¨¡å‹çš„å¯¹æŠ—æ”»å‡»æ–¹æ³•ï¼ŒåŒ…æ‹¬è®­ç»ƒæ—¶çš„ä¸­æ¯’æ”»å‡»å’Œæµ‹è¯•æ—¶çš„è§„é¿æ”»å‡»ï¼Œåˆ†æäº†LLAGAå’ŒGRAPHPROMPTERä¸¤ç§ä»£è¡¨æ€§æ¨¡å‹ã€‚ç ”ç©¶å‘ç°ï¼ŒLLAGAçš„èŠ‚ç‚¹åºåˆ—æ¨¡æ¿å¢åŠ äº†å…¶è„†å¼±æ€§ï¼Œè€ŒGRAPHPROMPTERçš„GNNç¼–ç å™¨è¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ã€‚æœ€åï¼Œæå‡ºäº†ä¸€ä¸ªåä¸ºGALGUARDçš„ç«¯åˆ°ç«¯é˜²å¾¡æ¡†æ¶ï¼Œç»“åˆäº†LLMç‰¹å¾ä¿®æ­£æ¨¡å—å’Œé€‚åº”æ€§GNNé˜²å¾¡ï¼Œä»¥æŠµå¾¡ç»“æ„æ€§æ”»å‡»ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å›¾æ„ŸçŸ¥å¤§è¯­è¨€æ¨¡å‹åœ¨å¯¹æŠ—æ”»å‡»ä¸‹çš„è„†å¼±æ€§é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆåº”å¯¹è®­ç»ƒæ—¶å’Œæµ‹è¯•æ—¶çš„æ”»å‡»ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡åˆ©ç”¨ç°æœ‰çš„å¯¹æŠ—æ”»å‡»æ–¹æ³•ï¼Œåˆ†æå›¾æ„ŸçŸ¥LLMsçš„è„†å¼±æ€§ï¼Œå¹¶æå‡ºGALGUARDé˜²å¾¡æ¡†æ¶ï¼Œä»¥å¢å¼ºå…¶é²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é¦–å…ˆå¯¹LLAGAå’ŒGRAPHPROMPTERè¿›è¡Œç³»ç»Ÿåˆ†æï¼Œè¯†åˆ«å…¶åœ¨å¯¹æŠ—æ”»å‡»ä¸‹çš„å¼±ç‚¹ï¼Œç„¶åè®¾è®¡GALGUARDæ¡†æ¶ï¼Œç»“åˆç‰¹å¾ä¿®æ­£å’ŒGNNé˜²å¾¡æ¨¡å—ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„ä¸»è¦åˆ›æ–°åœ¨äºé¦–æ¬¡ç³»ç»Ÿæ€§åœ°åˆ†æäº†å›¾æ„ŸçŸ¥LLMsçš„å¯¹æŠ—æ”»å‡»è„†å¼±æ€§ï¼Œå¹¶æå‡ºäº†é’ˆå¯¹æ€§çš„é˜²å¾¡ç­–ç•¥ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æŠ—æ”»å‡»èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨GALGUARDä¸­ï¼Œç‰¹å¾ä¿®æ­£æ¨¡å—ç”¨äºå‡è½»ç‰¹å¾çº§åˆ«çš„æ‰°åŠ¨ï¼Œè€ŒGNNé˜²å¾¡æ¨¡å—åˆ™é’ˆå¯¹ç»“æ„æ€§æ”»å‡»è¿›è¡Œä¿æŠ¤ï¼Œç¡®ä¿æ•´ä½“é˜²å¾¡æ•ˆæœçš„æå‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLLAGAåœ¨èŠ‚ç‚¹åºåˆ—æ¨¡æ¿ä¸‹çš„æ”»å‡»æˆåŠŸç‡æ˜¾è‘—æé«˜ï¼Œè€ŒGRAPHPROMPTERåˆ™å±•ç°å‡ºæ›´å¼ºçš„æŠ—æ”»å‡»èƒ½åŠ›ã€‚GALGUARDæ¡†æ¶æœ‰æ•ˆé™ä½äº†å¯¹æŠ—æ”»å‡»çš„å½±å“ï¼Œæå‡äº†æ¨¡å‹çš„æ•´ä½“é²æ£’æ€§ï¼Œå±•ç¤ºäº†é˜²å¾¡ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨å›¾æ„ŸçŸ¥å¤§è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§å’Œé²æ£’æ€§æ–¹é¢å…·æœ‰é‡è¦çš„åº”ç”¨ä»·å€¼ï¼Œå°¤å…¶é€‚ç”¨äºé‡‘èã€ç¤¾äº¤ç½‘ç»œåˆ†æå’Œç”Ÿç‰©ä¿¡æ¯å­¦ç­‰é¢†åŸŸï¼Œèƒ½å¤Ÿæœ‰æ•ˆé˜²æ­¢æ½œåœ¨çš„å¯¹æŠ—æ”»å‡»ï¼Œæå‡æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„å¯é æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) are increasingly integrated with graph-structured data for tasks like node classification, a domain traditionally dominated by Graph Neural Networks (GNNs). While this integration leverages rich relational information to improve task performance, their robustness against adversarial attacks remains unexplored. We take the first step to explore the vulnerabilities of graph-aware LLMs by leveraging existing adversarial attack methods tailored for graph-based models, including those for poisoning (training-time attacks) and evasion (test-time attacks), on two representative models, LLAGA (Chen et al. 2024) and GRAPHPROMPTER (Liu et al. 2024). Additionally, we discover a new attack surface for LLAGA where an attacker can inject malicious nodes as placeholders into the node sequence template to severely degrade its performance. Our systematic analysis reveals that certain design choices in graph encoding can enhance attack success, with specific findings that: (1) the node sequence template in LLAGA increases its vulnerability; (2) the GNN encoder used in GRAPHPROMPTER demonstrates greater robustness; and (3) both approaches remain susceptible to imperceptible feature perturbation attacks. Finally, we propose an end-to-end defense framework GALGUARD, that combines an LLM-based feature correction module to mitigate feature-level perturbations and adapted GNN defenses to protect against structural attacks.

