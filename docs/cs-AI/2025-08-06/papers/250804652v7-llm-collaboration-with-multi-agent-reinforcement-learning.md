---
layout: default
title: LLM Collaboration With Multi-Agent Reinforcement Learning
---

# LLM Collaboration With Multi-Agent Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.04652" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.04652v7</a>
  <a href="https://arxiv.org/pdf/2508.04652.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.04652v7" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.04652v7', 'LLM Collaboration With Multi-Agent Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shuo Liu, Tianle Chen, Zeyu Liang, Xueguang Lyu, Christopher Amato

**åˆ†ç±»**: cs.AI, cs.SE

**å‘å¸ƒæ—¥æœŸ**: 2025-08-06 (æ›´æ–°: 2025-12-09)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/OpenMLRL/CoMLRL)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMAGRPOä»¥è§£å†³LLMåä½œä¸­çš„å¥–åŠ±è®¾è®¡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ` `å¼ºåŒ–å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `åä½œä¼˜åŒ–` `ç®—æ³•è®¾è®¡` `è‡ªåŠ¨åŒ–å†™ä½œ` `ä»£ç ç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„LLMå¾®è°ƒæ–¹æ³•ä¾èµ–ä¸ªä½“å¥–åŠ±ï¼Œå¯¼è‡´å¤æ‚çš„å¥–åŠ±è®¾è®¡ï¼Œé™åˆ¶äº†æ™ºèƒ½ä½“ä¹‹é—´çš„æœ‰æ•ˆåˆä½œã€‚
2. æœ¬æ–‡æå‡ºå°†LLMåä½œè§†ä¸ºåˆä½œçš„MARLé—®é¢˜ï¼Œå¹¶å¼€å‘äº†MAGRPOç®—æ³•ä»¥ä¼˜åŒ–å¤šæ™ºèƒ½ä½“çš„åä½œèƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMAGRPOæ˜¾è‘—æå‡äº†LLMåœ¨å†™ä½œå’Œç¼–ç åä½œä¸­çš„å“åº”è´¨é‡å’Œæ•ˆç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰ä¸­ï¼Œå·²æœ‰å¤§é‡ç ”ç©¶è‡´åŠ›äºå»ºæ¨¡å’Œè§£å†³å¤šä¸ªäº¤äº’æ™ºèƒ½ä½“çš„é—®é¢˜ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¯ç‹¬ç«‹é¢„è®­ç»ƒçš„ï¼Œæœªé’ˆå¯¹åè°ƒè¿›è¡Œä¼˜åŒ–ã€‚ç°æœ‰çš„LLMå¾®è°ƒæ¡†æ¶ä¾èµ–äºä¸ªä½“å¥–åŠ±ï¼Œè¿™éœ€è¦ä¸ºæ¯ä¸ªæ™ºèƒ½ä½“è®¾è®¡å¤æ‚çš„å¥–åŠ±æœºåˆ¶ä»¥ä¿ƒè¿›åˆä½œã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡å°†LLMåä½œå»ºæ¨¡ä¸ºä¸€ä¸ªåˆä½œçš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§å¤šæ™ºèƒ½ä½“ã€å¤šå›åˆçš„ç®—æ³•â€”â€”å¤šæ™ºèƒ½ä½“ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆMAGRPOï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨MAGRPOå¾®è°ƒçš„MASèƒ½å¤Ÿé€šè¿‡æœ‰æ•ˆåˆä½œé«˜æ•ˆç”Ÿæˆé«˜è´¨é‡çš„å“åº”ã€‚è¯¥æ–¹æ³•ä¸ºLLMçš„å…¶ä»–MARLæ–¹æ³•çš„åº”ç”¨å¼€è¾Ÿäº†æ–°æ–¹å‘ï¼Œå¹¶çªå‡ºäº†ç›¸å…³æŒ‘æˆ˜ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³LLMåœ¨å¤šæ™ºèƒ½ä½“åä½œä¸­çš„å¥–åŠ±è®¾è®¡å¤æ‚æ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æœ‰æ•ˆä¿ƒè¿›æ™ºèƒ½ä½“ä¹‹é—´çš„åä½œï¼Œå¯¼è‡´ç”Ÿæˆè´¨é‡ä¸é«˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†LLMçš„åä½œå»ºæ¨¡ä¸ºä¸€ä¸ªåˆä½œçš„MARLé—®é¢˜ï¼Œé€šè¿‡MAGRPOç®—æ³•ä¼˜åŒ–æ™ºèƒ½ä½“çš„åä½œç­–ç•¥ï¼Œä»è€Œæé«˜ç”Ÿæˆè´¨é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMAGRPOç®—æ³•åŒ…æ‹¬å¤šä¸ªæ¨¡å—ï¼Œé¦–å…ˆæ˜¯æ™ºèƒ½ä½“çš„çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´å®šä¹‰ï¼Œå…¶æ¬¡æ˜¯åŸºäºåˆä½œå¥–åŠ±çš„ç­–ç•¥ä¼˜åŒ–ï¼Œæœ€åæ˜¯å¤šå›åˆçš„è®­ç»ƒæœºåˆ¶ï¼Œä»¥ä¿ƒè¿›æ™ºèƒ½ä½“ä¹‹é—´çš„æœ‰æ•ˆäº’åŠ¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šMAGRPOçš„ä¸»è¦åˆ›æ–°åœ¨äºå°†LLMçš„åä½œé—®é¢˜è½¬åŒ–ä¸ºMARLæ¡†æ¶ä¸‹çš„ä¼˜åŒ–é—®é¢˜ï¼Œçªç ´äº†ä¼ ç»ŸLLMå¾®è°ƒæ–¹æ³•çš„é™åˆ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆä¿ƒè¿›æ™ºèƒ½ä½“é—´çš„åˆä½œã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨MAGRPOä¸­ï¼Œè®¾è®¡äº†åŸºäºç»„ç›¸å¯¹å¥–åŠ±çš„æŸå¤±å‡½æ•°ï¼Œç¡®ä¿æ™ºèƒ½ä½“åœ¨åä½œè¿‡ç¨‹ä¸­èƒ½å¤Ÿè·å¾—å…±åŒçš„ä¼˜åŒ–ç›®æ ‡ï¼ŒåŒæ—¶é‡‡ç”¨äº†å¤šå›åˆè®­ç»ƒç­–ç•¥ä»¥å¢å¼ºæ™ºèƒ½ä½“çš„å­¦ä¹ èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨MAGRPOå¾®è°ƒçš„LLMåœ¨å†™ä½œå’Œç¼–ç ä»»åŠ¡ä¸­ç”Ÿæˆçš„å“åº”è´¨é‡æ˜¾è‘—æé«˜ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•ï¼Œç”Ÿæˆè´¨é‡æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œä¸”å“åº”æ—¶é—´ç¼©çŸ­äº†15%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨åŒ–å†™ä½œã€ä»£ç ç”Ÿæˆã€æ™ºèƒ½å®¢æœç­‰åœºæ™¯ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„åä½œæ•ˆç‡å’Œç”Ÿæˆè´¨é‡ã€‚æœªæ¥ï¼ŒMAGRPOæ–¹æ³•æœ‰æœ›æ¨å¹¿åˆ°æ›´å¤šå¤æ‚çš„å¤šæ™ºèƒ½ä½“ä»»åŠ¡ä¸­ï¼Œæ¨åŠ¨æ™ºèƒ½ä½“åä½œæŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> A large amount of work has been done in Multi-Agent Systems (MAS) for modeling and solving problems with multiple interacting agents. However, most LLMs are pretrained independently and not specifically optimized for coordination. Existing LLM fine-tuning frameworks rely on individual rewards, which require complex reward designs for each agent to encourage collaboration. To address these challenges, we model LLM collaboration as a cooperative Multi-Agent Reinforcement Learning (MARL) problem. We develop a multi-agent, multi-turn algorithm, Multi-Agent Group Relative Policy Optimization (MAGRPO), to solve it, building on current RL approaches for LLMs as well as MARL techniques. Our experiments on LLM writing and coding collaboration demonstrate that fine-tuning MAS with MAGRPO enables agents to generate high-quality responses efficiently through effective cooperation. Our approach opens the door to using other MARL methods for LLMs and highlights the associated challenges. Our code is available at https://github.com/OpenMLRL/CoMLRL.

