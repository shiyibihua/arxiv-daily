---
layout: default
title: GuirlVG: Incentivize GUI Visual Grounding via Empirical Exploration on Reinforcement Learning
---

# GuirlVG: Incentivize GUI Visual Grounding via Empirical Exploration on Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.04389" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.04389v1</a>
  <a href="https://arxiv.org/pdf/2508.04389.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.04389v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.04389v1', 'GuirlVG: Incentivize GUI Visual Grounding via Empirical Exploration on Reinforcement Learning')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Weitai Kang, Bin Lei, Gaowen Liu, Caiwen Ding, Yan Yan

**ÂàÜÁ±ª**: cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-06

**Â§áÊ≥®**: 9 pages

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫GuirlVG‰ª•Ëß£ÂÜ≥GUIËßÜËßâÂÆö‰ΩçÊïàÁéá‰Ωé‰∏ãÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÂõæÂΩ¢Áî®Êà∑ÁïåÈù¢` `ËßÜËßâÂÆö‰Ωç` `Âº∫ÂåñÂ≠¶‰π†` `Â§öÊ®°ÊÄÅÂ≠¶‰π†` `Êï∞ÊçÆÊïàÁéá` `Êô∫ËÉΩÂä©Êâã` `‰∫∫Êú∫‰∫§‰∫í`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑGUI-VGÊñπÊ≥ï‰∏ªË¶Å‰æùËµñ‰∫éÁõëÁù£ÂæÆË∞ÉÔºåÂ≠òÂú®Êï∞ÊçÆÈúÄÊ±ÇÈ´òÂíåËÆ≠ÁªÉÊàêÊú¨Â§ßÁöÑÈóÆÈ¢ò„ÄÇ
2. GuirlVGÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÂíåÊñ∞È¢ñÁöÑÁ®≥ÂÆöÂåñÊäÄÊúØÔºå‰ºòÂåñ‰∫ÜRFTÁöÑÂ∫îÁî®ÔºåÊèêÂçá‰∫ÜËÆ≠ÁªÉÊïàÁéá„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåGuirlVGÂú®Â∞ëÈáèÊ†∑Êú¨‰∏ãÊòæËëóË∂ÖË∂ä‰º†ÁªüSFTÊñπÊ≥ïÔºåÈ™åËØÅ‰∫ÜÂÖ∂ÊúâÊïàÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÂõæÂΩ¢Áî®Êà∑ÁïåÈù¢ËßÜËßâÂÆö‰ΩçÔºàGUI-VGÔºâÊòØGUI‰ª£ÁêÜÁöÑÊ†∏ÂøÉËÉΩÂäõÔºå‰º†Áªü‰∏ä‰æùËµñ‰∫éÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÁöÑÁõëÁù£ÂæÆË∞ÉÔºàSFTÔºâÔºåËøôÈúÄË¶ÅÂ§ßÈáèÊï∞ÊçÆÂíåÈ´òÊòÇÁöÑËÆ≠ÁªÉÊàêÊú¨„ÄÇÈöèÁùÄMLLMsÁöÑËøõÊ≠•ÔºåSFTÁöÑÂøÖË¶ÅÊÄßÈÄêÊ∏êÂèóÂà∞Ë¥®Áñë„ÄÇÊú¨ÊñáÊèêÂá∫GuirlVGÔºå‰∏ÄÁßçÂü∫‰∫éÂº∫ÂåñÂ≠¶‰π†ÁöÑGUI-VGÊñπÊ≥ïÔºåÈÄöËøáÁ≥ªÁªüÁöÑÂÆûËØÅÁ†îÁ©∂ÂíåÊñ∞È¢ñÁöÑÁ®≥ÂÆöÂåñÊäÄÊúØÔºå‰ºòÂåñ‰∫ÜÂº∫ÂåñÂæÆË∞ÉÔºàRFTÔºâÁöÑÂ∫îÁî®„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåGuirlVGÂú®‰ªÖ‰ΩøÁî®5.2KËÆ≠ÁªÉÊ†∑Êú¨ÁöÑÊÉÖÂÜµ‰∏ãÔºåË∂ÖË∂ä‰∫ÜÂü∫‰∫é10MÊ†∑Êú¨ÁöÑSFTÊñπÊ≥ïÔºåÂàÜÂà´Âú®ScreenSpot„ÄÅScreenSpotProÂíåScreenSpotV2‰∏äÂÆûÁé∞‰∫Ü7.7%„ÄÅ17.2%Âíå91.9%ÁöÑÂáÜÁ°ÆÁéáÊèêÂçá„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâGUI-VGÊñπÊ≥ïÂú®Êï∞ÊçÆÈúÄÊ±ÇÂíåËÆ≠ÁªÉÊàêÊú¨‰∏äÁöÑ‰∏çË∂≥ÔºåÂ∞§ÂÖ∂ÊòØ‰º†ÁªüÁöÑÁõëÁù£ÂæÆË∞ÉÊñπÊ≥ïÊïàÁéá‰Ωé‰∏ãÁöÑÈóÆÈ¢ò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöGuirlVGÈÄöËøáÂºïÂÖ•Âº∫ÂåñÂ≠¶‰π†ÂíåÂØπRFTÁöÑÊ∑±ÂÖ•ÂàÜÊûêÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßçÊõ¥È´òÊïàÁöÑËÆ≠ÁªÉÊñπÂºèÔºåÊó®Âú®ÂáèÂ∞ëÂØπÂ§ßÈáèÊ†áÊ≥®Êï∞ÊçÆÁöÑ‰æùËµñ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöGuirlVGÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨Êï∞ÊçÆÈááÈõÜ„ÄÅRFTÁöÑÊ†∏ÂøÉÁªÑ‰ª∂ÂàÜËß£„ÄÅÂä®ÊÄÅÁ®≥ÂÆöÂåñÊú∫Âà∂ÂíåËÆ≠ÁªÉÈÖçÁΩÆ‰ºòÂåñÁ≠âÂ§ö‰∏™Ê®°ÂùóÔºåÂΩ¢Êàê‰∏Ä‰∏™Á≥ªÁªüÁöÑËÆ≠ÁªÉÊµÅÁ®ã„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÊèêÂá∫‰∫ÜÂä®ÊÄÅÁ®≥ÂÆöÂåñÁöÑÂØπÊäóKLÂõ†Â≠êÔºåÊó®Âú®ÂáèËΩªÂ•ñÂä±ËøáÂ∫¶‰ºòÂåñÁöÑÈóÆÈ¢òÔºå‰ªéËÄåÊèêÂçáËÆ≠ÁªÉÁöÑÁ®≥ÂÆöÊÄßÂíåÊïàÊûú„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂèÇÊï∞ËÆæÁΩÆ‰∏äÔºåGuirlVGÈááÁî®‰∫ÜÈíàÂØπRFTÁöÑ‰ºòÂåñÈÖçÁΩÆÔºåËÆæËÆ°‰∫ÜÊñ∞ÁöÑÊçüÂ§±ÂáΩÊï∞‰ª•Âπ≥Ë°°Êé¢Á¥¢‰∏éÂà©Áî®ÔºåÂêåÊó∂Âú®ÁΩëÁªúÁªìÊûÑ‰∏äËøõË°å‰∫ÜÈÄÇÂΩìÁöÑË∞ÉÊï¥Ôºå‰ª•ÈÄÇÂ∫îGUI-VGÁöÑÁâπÂÆöÈúÄÊ±Ç„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

GuirlVGÂú®‰ªÖ‰ΩøÁî®5.2KËÆ≠ÁªÉÊ†∑Êú¨ÁöÑÊÉÖÂÜµ‰∏ãÔºåÂàÜÂà´Âú®ScreenSpot„ÄÅScreenSpotProÂíåScreenSpotV2‰∏äÂÆûÁé∞‰∫Ü7.7%„ÄÅ17.2%Âíå91.9%ÁöÑÂáÜÁ°ÆÁéáÊèêÂçáÔºåÊòæËëóË∂ÖË∂ä‰∫ÜÂü∫‰∫é10MÊ†∑Êú¨ÁöÑSFTÊñπÊ≥ïÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®GUI-VGÈ¢ÜÂüüÁöÑÂº∫Â§ßÊΩúÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

GuirlVGÁöÑÁ†îÁ©∂ÊàêÊûúÂú®ÂõæÂΩ¢Áî®Êà∑ÁïåÈù¢Ëá™Âä®Âåñ„ÄÅÊô∫ËÉΩÂä©ÊâãÂíå‰∫∫Êú∫‰∫§‰∫íÁ≠âÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõ„ÄÇÈÄöËøáÊèêÈ´òGUI-VGÁöÑÊïàÁéáÔºåËÉΩÂ§üÈôç‰ΩéÂºÄÂèëÊàêÊú¨ÔºåÊèêÂçáÁî®Êà∑‰ΩìÈ™åÔºåÊé®Âä®Êô∫ËÉΩÂ∫îÁî®ÁöÑÊôÆÂèä‰∏éÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Graphical user interface visual grounding (GUI-VG), a core capability for GUI agents, has primarily relied on supervised fine-tuning (SFT) of multimodal large language models (MLLMs), which demands extensive data curation and significant training costs. However, as MLLMs continue to advance and even cover GUI domains during pretraining, the necessity of exhaustive SFT post-training becomes increasingly questionable. Meanwhile, recent successes of rule-based reinforcement fine-tuning (RFT) suggest a more efficient alternative. Despite this promise, the optimal manner of applying RFT for GUI-VG remains unexplored. To bridge this gap, we introduce GuirlVG, a reinforcement learning-based GUI-VG method built on a systematic empirical study and a novel stabilization technique. We find that naive application of RFT underperforms the SFT baseline, motivating a deeper exploration. First, we decompose RFT into its core components and analyze the optimal formulation of each. Second, we propose a novel Adversarial KL Factor that dynamically stabilizes training to mitigate reward over-optimization. Third, we further explore the training configurations of RFT to enhance effectiveness. Extensive experiments show that GuirlVG, with only 5.2K training samples, outperforms SFT methods trained on over 10M samples, achieving a 7.7% improvement on ScreenSpot, a 17.2% improvement on ScreenSpotPro, and 91.9% accuracy on ScreenSpotV2.

