---
layout: default
title: Compressing Large Language Models with PCA Without Performance Loss
---

# Compressing Large Language Models with PCA Without Performance Loss

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.04307" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.04307v1</a>
  <a href="https://arxiv.org/pdf/2508.04307.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.04307v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.04307v1', 'Compressing Large Language Models with PCA Without Performance Loss')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Magnus Bengtsson

**åˆ†ç±»**: cs.CE, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-06

**å¤‡æ³¨**: 23 pages. 4 figures, submitted to journal

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é€šè¿‡PCAå‹ç¼©å¤§è¯­è¨€æ¨¡å‹è€Œä¸æŸå¤±æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä¸»æˆåˆ†åˆ†æ` `æ¨¡å‹å‹ç¼©` `ç¥ç»ç½‘ç»œ` `è‡ªç„¶è¯­è¨€å¤„ç†` `è®¡ç®—æœºè§†è§‰` `è½»é‡åŒ–æ¶æ„` `æ€§èƒ½ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨æ¨¡å‹å‹ç¼©æ—¶å¾€å¾€é¢ä¸´æ€§èƒ½æŸå¤±çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤æ‚æ•°æ®æ—¶ã€‚
2. æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰çš„ç»“æ„åŒ–å‹ç¼©æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨ä¸æŸå¤±æ€§èƒ½çš„æƒ…å†µä¸‹æ˜¾è‘—å‡å°‘æ¨¡å‹å‚æ•°ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨PCAå‹ç¼©çš„æ¨¡å‹åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå‡è¡¨ç°å‡ºè‰²ï¼Œå‡†ç¡®ç‡é«˜ä¸”å‚æ•°æ•°é‡å¤§å¹…å‡å°‘ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶å±•ç¤ºäº†ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰åœ¨ç»“æ„åŒ–åº”ç”¨ä¸‹ï¼Œèƒ½å¤Ÿå¯¹ç¥ç»æ¨¡å‹è¿›è¡Œæç«¯å‹ç¼©è€Œä¸ç‰ºç‰²æ€§èƒ½ã€‚é€šè¿‡ä¸‰ä¸ªæ¡ˆä¾‹ç ”ç©¶ï¼Œæˆ‘ä»¬è¡¨æ˜ï¼ŒåŸºäºPCAå‹ç¼©çš„æåæ ‡MNISTæ•°æ®é›†ä¸Šè®­ç»ƒçš„å•å±‚åˆ†ç±»å™¨ä½¿ç”¨ä»…840ä¸ªå‚æ•°å³å¯è¾¾åˆ°98%ä»¥ä¸Šçš„å‡†ç¡®ç‡ã€‚é‡‡ç”¨70ç»´PCAé™ç»´çš„MiniLMåµŒå…¥è®­ç»ƒçš„åŒå±‚å˜æ¢å™¨åœ¨20 Newsgroupsæ•°æ®é›†ä¸Šä»¥81000ä¸ªå‚æ•°è¾¾åˆ°äº†76.62%çš„å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œè§£ç å™¨ä»…ä½¿ç”¨70ç»´PCAåµŒå…¥ç”Ÿæˆè¿è´¯çš„tokenåºåˆ—ï¼ŒåŒæ—¶ä¸å®Œæ•´çš„MiniLMè¡¨ç¤ºä¿æŒè¶…è¿‡97%çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œå‚æ•°æ•°é‡ä¸åˆ°GPT-2çš„17%ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†åŸºäºPCAçš„è¾“å…¥å‹ç¼©ä½œä¸ºä¸€ç§é€šç”¨ä¸”æœ‰æ•ˆçš„ç­–ç•¥ï¼Œèƒ½å¤Ÿåœ¨å¤šä¸ªæ¨¡æ€ä¸­å®ç°æ¨¡å‹å®¹é‡ä¸ä¿¡æ¯å†…å®¹çš„å¯¹é½ï¼Œä»è€Œå®ç°è½»é‡åŒ–æ¶æ„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨å‹ç¼©è¿‡ç¨‹ä¸­å¸¸å¸¸å¯¼è‡´æ€§èƒ½ä¸‹é™çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å‹ç¼©æ¨¡å‹æ—¶ï¼Œå¾€å¾€æ— æ³•æœ‰æ•ˆå¹³è¡¡å‚æ•°æ•°é‡ä¸æ¨¡å‹æ€§èƒ½ï¼Œå¯¼è‡´å®é™…åº”ç”¨å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé€šè¿‡ç»“æ„åŒ–çš„ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰æ–¹æ³•ï¼Œå¯¹æåæ ‡å˜æ¢çš„å›¾åƒæˆ–åˆ†æ®µçš„tokenåºåˆ—è¿›è¡Œå‹ç¼©ï¼Œä»è€Œåœ¨ä¿æŒæ¨¡å‹æ€§èƒ½çš„åŒæ—¶å®ç°æç«¯çš„å‚æ•°å‡å°‘ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€PCAé™ç»´ã€æ¨¡å‹è®­ç»ƒå’Œæ€§èƒ½è¯„ä¼°å››ä¸ªä¸»è¦é˜¶æ®µã€‚é¦–å…ˆå¯¹è¾“å…¥æ•°æ®è¿›è¡Œæåæ ‡å˜æ¢ï¼Œç„¶ååº”ç”¨PCAè¿›è¡Œé™ç»´ï¼Œæœ€åè®­ç»ƒå‹ç¼©åçš„æ¨¡å‹å¹¶è¯„ä¼°å…¶æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå°†PCAåº”ç”¨äºç¥ç»ç½‘ç»œè¾“å…¥çš„ç»“æ„åŒ–å‹ç¼©ï¼Œæ˜¾è‘—é™ä½äº†æ¨¡å‹å‚æ•°æ•°é‡ï¼ŒåŒæ—¶ä¿æŒäº†é«˜æ°´å¹³çš„å‡†ç¡®ç‡ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„æ¨¡å‹å‹ç¼©æŠ€æœ¯ç›¸æ¯”ï¼Œæä¾›äº†æ›´é«˜çš„çµæ´»æ€§å’Œæœ‰æ•ˆæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œä½¿ç”¨äº†840ä¸ªå‚æ•°çš„å•å±‚åˆ†ç±»å™¨å’Œ81000ä¸ªå‚æ•°çš„åŒå±‚å˜æ¢å™¨ï¼Œé‡‡ç”¨70ç»´çš„PCAé™ç»´åµŒå…¥ï¼ŒæŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥ç¡®ä¿åœ¨å‹ç¼©è¿‡ç¨‹ä¸­æ€§èƒ½çš„æœ€å¤§ä¿ç•™ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸºäºPCAå‹ç¼©çš„å•å±‚åˆ†ç±»å™¨åœ¨æåæ ‡MNISTæ•°æ®é›†ä¸Šè¾¾åˆ°äº†98%ä»¥ä¸Šçš„å‡†ç¡®ç‡ï¼Œå‚æ•°ä»…ä¸º840ä¸ªï¼›åŒå±‚å˜æ¢å™¨åœ¨20 Newsgroupsæ•°æ®é›†ä¸Šä»¥81000ä¸ªå‚æ•°å®ç°76.62%çš„å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œè§£ç å™¨ç”Ÿæˆçš„tokenåºåˆ—ä¸å®Œæ•´MiniLMè¡¨ç¤ºçš„ä½™å¼¦ç›¸ä¼¼åº¦è¶…è¿‡97%ï¼Œä¸”å‚æ•°æ•°é‡ä¸åˆ°GPT-2çš„17%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰å’Œå…¶ä»–éœ€è¦é«˜æ•ˆæ¨¡å‹çš„ä»»åŠ¡ã€‚é€šè¿‡PCAå‹ç¼©ï¼Œç ”ç©¶è€…å¯ä»¥åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­éƒ¨ç½²é«˜æ€§èƒ½æ¨¡å‹ï¼Œæ¨åŠ¨è½»é‡åŒ–äººå·¥æ™ºèƒ½æŠ€æœ¯çš„å‘å±•ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„æœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We demonstrate that Principal Component Analysis (PCA), when applied in a structured manner, either to polar-transformed images or segment-wise to token sequences, enables extreme compression of neural models without sacrificing performance. Across three case studies, we show that a one-layer classifier trained on PCA-compressed polar MNIST achieves over 98 percent accuracy using only 840 parameters. A two-layer transformer trained on 70-dimensional PCA-reduced MiniLM embeddings reaches 76.62 percent accuracy on the 20 Newsgroups dataset with just 81000 parameters. A decoder-only transformer generates coherent token sequences from 70-dimensional PCA embeddings while preserving over 97 percent cosine similarity with full MiniLM representations, using less than 17 percent of the parameter count of GPT-2. These results highlight PCA-based input compression as a general and effective strategy for aligning model capacity with information content, enabling lightweight architectures across multiple modalities.

