---
layout: default
title: Chain-of-Agents: End-to-End Agent Foundation Models via Multi-Agent Distillation and Agentic RL
---

# Chain-of-Agents: End-to-End Agent Foundation Models via Multi-Agent Distillation and Agentic RL

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.13167" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.13167v1</a>
  <a href="https://arxiv.org/pdf/2508.13167.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.13167v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.13167v1', 'Chain-of-Agents: End-to-End Agent Foundation Models via Multi-Agent Distillation and Agentic RL')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Weizhen Li, Jianbo Lin, Zhuosong Jiang, Jingyi Cao, Xinpeng Liu, Jiayu Zhang, Zhenqiang Huang, Qianben Chen, Weichen Sun, Qiexiang Wang, Hongxuan Lu, Tianrui Qin, Chenghao Zhu, Yi Yao, Shuying Fan, Xiaowan Li, Tiannan Wang, Pai Liu, King Zhu, He Zhu, Dingfeng Shi, Piaohong Wang, Yeyi Guan, Xiangru Tang, Minghao Liu, Yuchen Eleanor Jiang, Jian Yang, Jiaheng Liu, Ge Zhang, Wangchunshu Zhou

**åˆ†ç±»**: cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-06

**å¤‡æ³¨**: 51 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºChain-of-Agentsä»¥è§£å†³å¤šä»£ç†ç³»ç»Ÿæ•ˆç‡ä½ä¸‹é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šä»£ç†ç³»ç»Ÿ` `å¤§å‹è¯­è¨€æ¨¡å‹` `ç«¯åˆ°ç«¯å­¦ä¹ ` `å¼ºåŒ–å­¦ä¹ ` `è’¸é¦è®­ç»ƒ` `å¤æ‚é—®é¢˜è§£å†³` `æ™ºèƒ½åŠ©æ‰‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šä»£ç†ç³»ç»Ÿä¾èµ–æ‰‹åŠ¨æç¤ºå’Œå¤æ‚æ¡†æ¶ï¼Œå¯¼è‡´è®¡ç®—æ•ˆç‡ä½ä¸‹å’Œèƒ½åŠ›ä¸è¶³ã€‚
2. æå‡ºChain-of-AgentsèŒƒå¼ï¼Œé€šè¿‡åŠ¨æ€æ¿€æ´»ä¸åŒå·¥å…·ä»£ç†å’Œè§’è‰²æ‰®æ¼”ä»£ç†ï¼Œå®ç°ç«¯åˆ°ç«¯çš„å¤æ‚é—®é¢˜è§£å†³ã€‚
3. AFMåœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ï¼Œå±•ç¤ºäº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤šä»£ç†ç³»ç»Ÿåœ¨å¤æ‚é—®é¢˜è§£å†³ä»»åŠ¡ä¸­å±•ç°å‡ºæ˜¾è‘—èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰å¤šä»£ç†ç³»ç»Ÿé€šå¸¸ä¾èµ–äºæ‰‹åŠ¨æç¤ºå’Œå¤æ‚çš„ä»£ç†æ¡†æ¶ï¼Œå¯¼è‡´è®¡ç®—æ•ˆç‡ä½ä¸‹ï¼Œèƒ½åŠ›ä¸è¶³ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨æ•°æ®é©±åŠ¨å­¦ä¹ ã€‚æœ¬æ–‡æå‡ºChain-of-Agentsï¼ˆCoAï¼‰ï¼Œä¸€ç§æ–°é¢–çš„LLMæ¨ç†èŒƒå¼ï¼Œèƒ½å¤Ÿåœ¨å•ä¸€æ¨¡å‹ä¸­å®ç°åŸç”Ÿçš„ç«¯åˆ°ç«¯å¤æ‚é—®é¢˜è§£å†³ï¼Œæ¨¡æ‹Ÿå¤šä»£ç†åä½œã€‚æˆ‘ä»¬å¼•å…¥å¤šä»£ç†è’¸é¦æ¡†æ¶ï¼Œå°†å…ˆè¿›çš„å¤šä»£ç†ç³»ç»Ÿè’¸é¦ä¸ºChain-of-Agentsè½¨è¿¹ï¼Œä»¥è¿›è¡Œä»£ç†ç›‘ç£å¾®è°ƒï¼Œå¹¶é€šè¿‡ä»£ç†å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥æå‡æ¨¡å‹åœ¨Chain-of-Agentsé—®é¢˜è§£å†³ä¸­çš„èƒ½åŠ›ã€‚å®è¯ç ”ç©¶è¡¨æ˜ï¼ŒAgent Foundation Modelsï¼ˆAFMsï¼‰åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å»ºç«‹äº†æ–°çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤šä»£ç†ç³»ç»Ÿåœ¨å¤æ‚é—®é¢˜è§£å†³ä¸­çš„æ•ˆç‡ä½ä¸‹å’Œèƒ½åŠ›ä¸è¶³é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºæ‰‹åŠ¨æç¤ºå’Œå¤æ‚çš„å·¥ä½œæµè®¾è®¡ï¼Œé™åˆ¶äº†å…¶åœ¨æ•°æ®é©±åŠ¨å­¦ä¹ ä¸­çš„åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºChain-of-Agentsï¼ˆCoAï¼‰èŒƒå¼ï¼Œå…è®¸åœ¨å•ä¸€æ¨¡å‹ä¸­å®ç°å¤šè½®é—®é¢˜è§£å†³ï¼ŒåŠ¨æ€æ¿€æ´»ä¸åŒçš„å·¥å…·ä»£ç†å’Œè§’è‰²æ‰®æ¼”ä»£ç†ï¼Œæ¨¡æ‹Ÿå¤šä»£ç†åä½œã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹èƒ½å¤Ÿæ›´é«˜æ•ˆåœ°å¤„ç†å¤æ‚ä»»åŠ¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä»£ç†è’¸é¦æ¡†æ¶å’Œä»£ç†å¼ºåŒ–å­¦ä¹ ä¸¤ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡è’¸é¦å°†å…ˆè¿›çš„å¤šä»£ç†ç³»ç»Ÿè½¬åŒ–ä¸ºChain-of-Agentsè½¨è¿¹ï¼Œç„¶ååœ¨å¯éªŒè¯çš„ä»£ç†ä»»åŠ¡ä¸Šè¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œä»¥æå‡æ¨¡å‹èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†å¤šä»£ç†è’¸é¦å’Œä»£ç†å¼ºåŒ–å­¦ä¹ çš„ç»“åˆï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨ç«¯åˆ°ç«¯çš„æ–¹å¼ä¸‹è¿›è¡Œå¤æ‚é—®é¢˜è§£å†³ã€‚è¿™ä¸€è®¾è®¡ä¸ä¼ ç»Ÿçš„å¤šä»£ç†ç³»ç»Ÿæœ‰æœ¬è´¨åŒºåˆ«ï¼Œåè€…é€šå¸¸ä¾èµ–äºæ‰‹åŠ¨è®¾è®¡çš„å·¥ä½œæµã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œé‡‡ç”¨äº†ä»£ç†ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ç›¸ç»“åˆçš„ç­–ç•¥ï¼Œä½¿ç”¨ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚æ¨¡å‹ç»“æ„ä¸Šï¼Œè®¾è®¡äº†å¤šå±‚æ¬¡çš„ä»£ç†æ¿€æ´»æœºåˆ¶ï¼Œä»¥æ”¯æŒåŠ¨æ€çš„ä»»åŠ¡å¤„ç†ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAgent Foundation Modelsï¼ˆAFMsï¼‰åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æ–°çš„æœ€å…ˆè¿›æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨ç½‘ç»œä»£ç†å’Œä»£ç ä»£ç†è®¾ç½®ä¸‹ï¼Œæ€§èƒ½æå‡å¹…åº¦æ˜¾è‘—ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨åŒ–é—®é¢˜è§£å†³ã€æ™ºèƒ½åŠ©æ‰‹ã€ç¼–ç¨‹è¾…åŠ©ç­‰ã€‚é€šè¿‡æå‡å¤šä»£ç†ç³»ç»Ÿçš„æ•ˆç‡å’Œèƒ½åŠ›ï¼ŒChain-of-Agentsèƒ½å¤Ÿåœ¨å¤æ‚ä»»åŠ¡ä¸­æä¾›æ›´é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in large language models (LLMs) and multi-agent systems have demonstrated remarkable capabilities in complex problem-solving tasks such as deep research, vibe coding, and mathematical reasoning. However, most existing multi-agent systems are built upon manual prompt/workflow engineering with sophisticated agent frameworks, making them computationally inefficient, less capable, and can not benefit from data-centric learning. In this work, we introduce Chain-of-Agents (CoA), a novel paradigm of LLM reasoning that enables native end-to-end complex problem-solving in the same way as a multi-agent system (i.e., multi-turn problem solving with multiple tools and multiple agents) within one model. In chain-of-agents problem-solving, the model dynamically activates different tool agents and role-playing agents to simulate multi-agent collaboration in an end-to-end fashion. To elicit end-to-end chain-of-agents problem-solving abilities in LLMs, we introduce a multi-agent distillation framework to distill state-of-the-art multi-agent systems into chain-of-agents trajectories for agentic supervised fine-tuning. We then use agentic reinforcement learning on verifiable agentic tasks to further improve the models' capabilities on chain-of-agents problem solving. We call the resulting models Agent Foundation Models (AFMs). Our empirical studies demonstrate that AFM establishes new state-of-the-art performance across diverse benchmarks in both web agent and code agent settings. We make the entire research, including the model weights, code for training and evaluation, and the training data, fully open-sourced, which offers a solid starting point for future research on agent models and agentic RL.

