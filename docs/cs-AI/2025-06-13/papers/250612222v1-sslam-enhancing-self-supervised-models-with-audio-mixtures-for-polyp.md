---
layout: default
title: SSLAM: Enhancing Self-Supervised Models with Audio Mixtures for Polyphonic Soundscapes
---

# SSLAM: Enhancing Self-Supervised Models with Audio Mixtures for Polyphonic Soundscapes

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.12222" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.12222v1</a>
  <a href="https://arxiv.org/pdf/2506.12222.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.12222v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.12222v1', 'SSLAM: Enhancing Self-Supervised Models with Audio Mixtures for Polyphonic Soundscapes')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Tony Alex, Sara Ahmed, Armin Mustafa, Muhammad Awais, Philip JB Jackson

**ÂàÜÁ±ª**: cs.SD, cs.AI, cs.LG, eess.AS

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-13

**Â§áÊ≥®**: Accepted at ICLR 2025. Code and pre-trained models are available at \url{https://github.com/ta012/SSLAM}

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫SSLAM‰ª•Ëß£ÂÜ≥Â§öÈü≥È¢ëÂú∫ÊôØ‰∏ãËá™ÁõëÁù£Ê®°ÂûãÁöÑÊÄßËÉΩ‰∏çË∂≥ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Ëá™ÁõëÁù£Â≠¶‰π†` `Èü≥È¢ëÂ§ÑÁêÜ` `Â§öÈü≥È¢ëÂú∫ÊôØ` `Ê®°ÂûãÊ≥õÂåñ` `Èü≥È¢ëÊ∑∑Âêà`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâËá™ÁõëÁù£Èü≥È¢ëÊ®°ÂûãÂú®Â§öÈü≥È¢ëÁéØÂ¢É‰∏≠ÁöÑË°®Áé∞Â∞öÊú™ÂæóÂà∞ÂÖÖÂàÜÈ™åËØÅÔºå‰∏ªË¶ÅÂü∫‰∫éÂçïÈü≥È¢ëÊï∞ÊçÆÈõÜËøõË°åËØÑ‰º∞„ÄÇ
2. Êú¨ÊñáÊèêÂá∫SSLAMÔºåÈÄöËøáÂºïÂÖ•Èü≥È¢ëÊ∑∑ÂêàÂ≠¶‰π†ÔºåÂ¢ûÂº∫Ê®°ÂûãÂú®Â§öÈü≥È¢ëÊï∞ÊçÆ‰∏äÁöÑÂ≠¶‰π†ËÉΩÂäõÔºåÂêåÊó∂‰øùÊåÅÂçïÈü≥È¢ëÊÄßËÉΩ„ÄÇ
3. SSLAMÂú®Ê†áÂáÜÈü≥È¢ëËá™ÁõëÁù£Âü∫ÂáÜ‰∏äË°®Áé∞‰ºòÂºÇÔºåÂú®Â§öÈü≥È¢ëÊï∞ÊçÆÈõÜ‰∏äËÆæÁ´ãÊñ∞SOTAÔºåÊÄßËÉΩÊèêÂçáËææÂà∞9.1%ÔºàmAPÔºâ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Ëá™ÁõëÁù£È¢ÑËÆ≠ÁªÉÈü≥È¢ëÁΩëÁªúÂú®Áé∞ÂÆûÁ≥ªÁªü‰∏≠ÂæóÂà∞‰∫ÜÂπøÊ≥õÂ∫îÁî®ÔºåÂ∞§ÂÖ∂ÊòØÂú®Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã‰∏≠„ÄÇÁÑ∂ËÄåÔºåËøô‰∫õÁΩëÁªúÈÄöÂ∏∏Âú®ÂÜªÁªìÁä∂ÊÄÅ‰∏ã‰ΩøÁî®ÔºåÂÅáËÆæËá™ÁõëÁù£È¢ÑËÆ≠ÁªÉÂ∑≤Ë∂≥Â§üÂ∫îÂØπÂ§çÊùÇÁöÑÂ§öÈü≥È¢ëÁéØÂ¢É„ÄÇÁé∞ÊúâÈü≥È¢ëËá™ÁõëÁù£ÊñπÊ≥ï‰∏ªË¶ÅÂü∫‰∫éÂçïÈü≥È¢ëÊï∞ÊçÆÈõÜÔºåÂØºËá¥ÂÖ∂Âú®Â§öÈü≥È¢ëÂú∫ÊôØ‰∏≠ÁöÑÊ≥õÂåñËÉΩÂäõÂ∞öÊú™ÂæóÂà∞ÂÖÖÂàÜÊé¢Á¥¢„ÄÇ‰∏∫Ê≠§ÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜËá™ÁõëÁù£Â≠¶‰π†Èü≥È¢ëÊ∑∑ÂêàÔºàSSLAMÔºâÔºåÊó®Âú®ÊèêÂçáÊ®°ÂûãÂú®Â§öÈü≥È¢ëÊï∞ÊçÆ‰∏äÁöÑÂ≠¶‰π†ËÉΩÂäõÔºåÂêåÊó∂‰øùÊåÅÂú®ÂçïÈü≥È¢ëÊï∞ÊçÆ‰∏äÁöÑÂº∫ÊÄßËÉΩ„ÄÇÈÄöËøáÂØπÊ†áÂáÜÈü≥È¢ëËá™ÁõëÁù£Âü∫ÂáÜÊï∞ÊçÆÈõÜÁöÑËØÑ‰º∞ÔºåSSLAMÂú®Â§öÈü≥È¢ëÊï∞ÊçÆÈõÜ‰∏äËÆæÁ´ã‰∫ÜÊñ∞ÁöÑSOTAÔºåÂπ∂Âú®AudioSet-2M‰∏äÂÆûÁé∞‰∫ÜÈ´òËææ3.9%ÁöÑÊÄßËÉΩÊèêÂçáÔºåmAPËææÂà∞50.2„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâËá™ÁõëÁù£Èü≥È¢ëÊ®°ÂûãÂú®Â§öÈü≥È¢ëÂú∫ÊôØ‰∏ãÁöÑÊÄßËÉΩ‰∏çË∂≥ÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ï‰∏ªË¶ÅÂü∫‰∫éÂçïÈü≥È¢ëÊï∞ÊçÆÈõÜÔºåÂØºËá¥Ê®°ÂûãÂú®Â§çÊùÇÁöÑÂ§öÈü≥È¢ëÁéØÂ¢É‰∏≠Ê≥õÂåñËÉΩÂäõ‰∏çË∂≥„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÊèêÂá∫SSLAMÔºåÈÄöËøáÈü≥È¢ëÊ∑∑ÂêàÂ≠¶‰π†ÁöÑÊñπÂºèÔºåÂ¢ûÂº∫Ê®°ÂûãÂØπÂ§öÈü≥È¢ëÊï∞ÊçÆÁöÑÂ≠¶‰π†ËÉΩÂäõÔºåÂêåÊó∂Á°Æ‰øùÂú®ÂçïÈü≥È¢ëÊï∞ÊçÆ‰∏äÁöÑÊÄßËÉΩ‰∏ç‰∏ãÈôç„ÄÇËøôÁßçËÆæËÆ°Êó®Âú®ÊèêÂçáÊ®°ÂûãÂú®ÁúüÂÆû‰∏ñÁïåÈü≥È¢ëÂú∫ÊôØ‰∏≠ÁöÑÈÄÇÂ∫îÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöSSLAMÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨Èü≥È¢ëÊ∑∑ÂêàÁîüÊàêÊ®°ÂùóÂíåËá™ÁõëÁù£Â≠¶‰π†Ê®°Âùó„ÄÇÈü≥È¢ëÊ∑∑ÂêàÁîüÊàêÊ®°ÂùóË¥üË¥£ÂàõÂª∫Â§öÈü≥È¢ëÊ∑∑ÂêàÊ†∑Êú¨ÔºåËÄåËá™ÁõëÁù£Â≠¶‰π†Ê®°ÂùóÂàôÂà©Áî®Ëøô‰∫õÊ†∑Êú¨ËøõË°åÊ®°ÂûãËÆ≠ÁªÉ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöSSLAMÁöÑÊ†∏ÂøÉÂàõÊñ∞Âú®‰∫éÂºïÂÖ•Èü≥È¢ëÊ∑∑ÂêàÂ≠¶‰π†Êú∫Âà∂Ôºå‰ΩøÂæóÊ®°ÂûãËÉΩÂ§üÂú®Â§öÈü≥È¢ëÁéØÂ¢É‰∏≠ÊúâÊïàÂ≠¶‰π†„ÄÇËøô‰∏ÄÊñπÊ≥ï‰∏é‰º†ÁªüÁöÑÂçïÈü≥È¢ëËÆ≠ÁªÉÊñπÂºèÊú¨Ë¥®‰∏ä‰∏çÂêåÔºåËÉΩÂ§üÊõ¥Â•ΩÂú∞ÈÄÇÂ∫îÂ§çÊùÇÁöÑÈü≥È¢ëÂú∫ÊôØ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÊäÄÊúØÁªÜËäÇ‰∏äÔºåSSLAMÈááÁî®‰∫ÜÁâπÂÆöÁöÑÊçüÂ§±ÂáΩÊï∞Êù•Âπ≥Ë°°ÂçïÈü≥È¢ëÂíåÂ§öÈü≥È¢ëÊï∞ÊçÆÁöÑÂ≠¶‰π†ÔºåÂêåÊó∂Âú®ÁΩëÁªúÁªìÊûÑ‰∏äËøõË°å‰∫Ü‰ºòÂåñÔºå‰ª•ÊèêÈ´òÊ®°ÂûãÁöÑÂ≠¶‰π†ÊïàÁéáÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

SSLAMÂú®Ê†áÂáÜÈü≥È¢ëËá™ÁõëÁù£Âü∫ÂáÜ‰∏äË°®Áé∞Âá∫Ëâ≤ÔºåÂ∞§ÂÖ∂Âú®AudioSet-2M‰∏äÂÆûÁé∞‰∫Ü3.9%ÁöÑÊÄßËÉΩÊèêÂçáÔºåmAPËææÂà∞50.2„ÄÇÂêåÊó∂ÔºåÂú®Â§öÈü≥È¢ëÊï∞ÊçÆÈõÜ‰∏äÔºåSSLAMÂú®Á∫øÊÄßËØÑ‰º∞ÂíåÂæÆË∞ÉÈò∂ÊÆµËÆæÁ´ã‰∫ÜÊñ∞ÁöÑSOTAÔºåÊÄßËÉΩÊèêÂçáËææÂà∞9.1%ÔºàmAPÔºâ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨ÁéØÂ¢ÉÂ£∞Èü≥ËØÜÂà´„ÄÅÈü≥‰πêÂàÜÊûêÂíåËØ≠Èü≥Â§ÑÁêÜÁ≠â„ÄÇÈÄöËøáÊèêÂçáËá™ÁõëÁù£Ê®°ÂûãÂú®Â§öÈü≥È¢ëÂú∫ÊôØ‰∏ãÁöÑÊÄßËÉΩÔºåSSLAMËÉΩÂ§ü‰∏∫ÂÆûÈôÖÈü≥È¢ëÂ§ÑÁêÜÁ≥ªÁªüÊèê‰æõÊõ¥Âº∫ÁöÑÈ≤ÅÊ£íÊÄßÂíåÂáÜÁ°ÆÊÄßÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÊú™Êù•ÂΩ±Âìç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Self-supervised pre-trained audio networks have seen widespread adoption in real-world systems, particularly in multi-modal large language models. These networks are often employed in a frozen state, under the assumption that the SSL pre-training has sufficiently equipped them to handle real-world audio. However, a critical question remains: how well do these models actually perform in real-world conditions, where audio is typically polyphonic and complex, involving multiple overlapping sound sources? Current audio SSL methods are often benchmarked on datasets predominantly featuring monophonic audio, such as environmental sounds, and speech. As a result, the ability of SSL models to generalize to polyphonic audio, a common characteristic in natural scenarios, remains underexplored. This limitation raises concerns about the practical robustness of SSL models in more realistic audio settings. To address this gap, we introduce Self-Supervised Learning from Audio Mixtures (SSLAM), a novel direction in audio SSL research, designed to improve, designed to improve the model's ability to learn from polyphonic data while maintaining strong performance on monophonic data. We thoroughly evaluate SSLAM on standard audio SSL benchmark datasets which are predominantly monophonic and conduct a comprehensive comparative analysis against SOTA methods using a range of high-quality, publicly available polyphonic datasets. SSLAM not only improves model performance on polyphonic audio, but also maintains or exceeds performance on standard audio SSL benchmarks. Notably, it achieves up to a 3.9\% improvement on the AudioSet-2M (AS-2M), reaching a mean average precision (mAP) of 50.2. For polyphonic datasets, SSLAM sets new SOTA in both linear evaluation and fine-tuning regimes with performance improvements of up to 9.1\% (mAP).

