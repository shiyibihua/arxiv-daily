---
layout: default
title: Identifying Helpful Context for LLM-based Vulnerability Repair: A Preliminary Study
---

# Identifying Helpful Context for LLM-based Vulnerability Repair: A Preliminary Study

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.11561" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.11561v1</a>
  <a href="https://arxiv.org/pdf/2506.11561.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.11561v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.11561v1', 'Identifying Helpful Context for LLM-based Vulnerability Repair: A Preliminary Study')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: GÃ¡bor Antal, Bence BogenfÃ¼rst, Rudolf Ferenc, PÃ©ter HegedÅ±s

**åˆ†ç±»**: cs.SE, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-13

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ¢è®¨GPT-4oåœ¨Javaæ¼æ´ä¿®å¤ä¸­çš„ä¸Šä¸‹æ–‡å½±å“**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `è‡ªåŠ¨åŒ–æ¼æ´ä¿®å¤` `ä¸Šä¸‹æ–‡ä¿¡æ¯` `è½¯ä»¶å®‰å…¨` `æç¤ºè®¾è®¡` `CVE` `Javaæ¼æ´`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è‡ªåŠ¨åŒ–æ¼æ´ä¿®å¤æ–¹æ³•åœ¨å¤„ç†å¤æ‚ä¸Šä¸‹æ–‡ä¿¡æ¯æ—¶è¡¨ç°ä¸ä½³ï¼Œå¯¼è‡´ä¿®å¤æ•ˆæœæœ‰é™ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡ä¸åŒçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¦‚CWEã€CVEï¼‰ä¼˜åŒ–GPT-4oçš„æ¼æ´ä¿®å¤èƒ½åŠ›ï¼Œæ¢ç´¢æç¤ºè®¾è®¡çš„å½±å“ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒGPT-4oåœ¨ä¿®å¤ç‹¬ç‰¹æ¼æ´æ–¹é¢è¡¨ç°ä¼˜äºåŸºçº¿ï¼Œç»“åˆCVEä¿¡æ¯å’Œæ‰‹åŠ¨ä¸Šä¸‹æ–‡çš„ç»„åˆæ•ˆæœæœ€ä½³ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›æ­¥ï¼Œè‡ªåŠ¨åŒ–æ¼æ´æ£€æµ‹å’Œä¿®å¤åœ¨è½¯ä»¶ç³»ç»Ÿä¸­å±•ç°å‡ºæ½œåŠ›ã€‚æœ¬æ–‡ç ”ç©¶äº†GPT-4oåœ¨ä¿®å¤Vul4Jæ•°æ®é›†ä¸­Javaæ¼æ´çš„è¡¨ç°ï¼Œæ¢è®¨äº†ä¸åŒä¸Šä¸‹æ–‡ä¿¡æ¯å¯¹è‡ªåŠ¨åŒ–æ¼æ´ä¿®å¤èƒ½åŠ›çš„å½±å“ã€‚é€šè¿‡ä¸GPT-4çš„å¯¹æ¯”ï¼Œè¯„ä¼°äº†ä¹ç§ä¸åŒçš„æç¤ºï¼Œç»“æœæ˜¾ç¤ºGPT-4oåœ¨ç›¸åŒæç¤ºä¸‹å¹³å‡è¡¨ç°è¾ƒå·®ï¼Œä½†åœ¨ä¿®å¤ç‹¬ç‰¹æ¼æ´æ–¹é¢æœ‰æ‰€æå‡ï¼Œç»“åˆCVEä¿¡æ¯å’Œæ‰‹åŠ¨æå–çš„ä»£ç ä¸Šä¸‹æ–‡çš„ç»„åˆè¡¨ç°æœ€ä½³ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªåŠ¨åŒ–æ¼æ´ä¿®å¤ä¸­å¯¹ä¸Šä¸‹æ–‡ä¿¡æ¯çš„ä¾èµ–ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤æ‚åœºæ™¯ä¸‹çš„ä¿®å¤æ•ˆæœä¸ç†æƒ³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡è®¾è®¡å¤šç§æç¤ºï¼Œç»“åˆCWEå’ŒCVEç­‰ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæå‡GPT-4oåœ¨ä¿®å¤Javaæ¼æ´æ—¶çš„æ€§èƒ½ï¼Œæ¢ç´¢ä¸åŒä¸Šä¸‹æ–‡å¯¹ä¿®å¤æ•ˆæœçš„å½±å“ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨äº†Vul4Jæ•°æ®é›†ï¼Œé’ˆå¯¹42ä¸ªæ¼æ´è®¾è®¡äº†ä¹ç§ä¸åŒçš„æç¤ºï¼Œåˆ†åˆ«æ‰§è¡Œä¸‰æ¬¡ä»¥éªŒè¯ä¿®å¤å€™é€‰çš„æœ‰æ•ˆæ€§ï¼Œä½¿ç”¨è‡ªåŠ¨åŒ–æµ‹è¯•æ¡†æ¶è¿›è¡ŒéªŒè¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºç»“åˆCVEä¿¡æ¯å’Œæ‰‹åŠ¨æå–çš„ä»£ç ä¸Šä¸‹æ–‡ï¼Œå½¢æˆæœ€ä½³çš„æç¤ºç»„åˆï¼Œä»è€Œæ˜¾è‘—æå‡äº†æ¼æ´ä¿®å¤çš„æˆåŠŸç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œæç¤ºçš„è®¾è®¡è€ƒè™‘äº†ä¸Šä¸‹æ–‡ä¿¡æ¯çš„ä¸°å¯Œæ€§å’Œç›¸å…³æ€§ï¼Œä½¿ç”¨äº†å¤šæ¬¡æ‰§è¡Œä»¥ç¡®ä¿ç»“æœçš„å¯é æ€§ï¼Œä¸”å¯¹æ¯”äº†ä¸åŒæç¤ºçš„ä¿®å¤æ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGPT-4oåœ¨ä¿®å¤ç‹¬ç‰¹æ¼æ´æ–¹é¢è¡¨ç°ä¼˜äºåŸºçº¿ï¼Œä½¿ç”¨æœ€ä½³çš„æç¤ºç»„åˆåï¼Œä¿®å¤æˆåŠŸç‡è¾¾åˆ°62%ï¼Œæ˜¾è‘—é«˜äºåŸå§‹åŸºçº¿çš„40%å’Œé‡ç°ç»“æœçš„45%ã€‚CVEä¿¡æ¯çš„å¼•å…¥æ˜¾è‘—æé«˜äº†ä¿®å¤ç‡ï¼Œæç¤ºè®¾è®¡çš„ä¼˜åŒ–æ•ˆæœæ˜æ˜¾ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è½¯ä»¶å®‰å…¨æ€§æå‡ã€è‡ªåŠ¨åŒ–ä»£ç å®¡æŸ¥å’Œæ¼æ´ä¿®å¤å·¥å…·çš„å¼€å‘ã€‚é€šè¿‡ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¼æ´ä¿®å¤ä¸­çš„è¡¨ç°ï¼Œèƒ½å¤Ÿæœ‰æ•ˆé™ä½è½¯ä»¶ç³»ç»Ÿçš„å®‰å…¨é£é™©ï¼Œæå‡å¼€å‘æ•ˆç‡ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advancements in large language models (LLMs) have shown promise for automated vulnerability detection and repair in software systems. This paper investigates the performance of GPT-4o in repairing Java vulnerabilities from a widely used dataset (Vul4J), exploring how different contextual information affects automated vulnerability repair (AVR) capabilities. We compare the latest GPT-4o's performance against previous results with GPT-4 using identical prompts. We evaluated nine additional prompts crafted by us that contain various contextual information such as CWE or CVE information, and manually extracted code contexts. Each prompt was executed three times on 42 vulnerabilities, and the resulting fix candidates were validated using Vul4J's automated testing framework.
>   Our results show that GPT-4o performed 11.9\% worse on average than GPT-4 with the same prompt, but was able to fix 10.5\% more distinct vulnerabilities in the three runs together. CVE information significantly improved repair rates, while the length of the task description had minimal impact. Combining CVE guidance with manually extracted code context resulted in the best performance. Using our \textsc{Top}-3 prompts together, GPT-4o repaired 26 (62\%) vulnerabilities at least once, outperforming both the original baseline (40\%) and its reproduction (45\%), suggesting that ensemble prompt strategies could improve vulnerability repair in zero-shot settings.

