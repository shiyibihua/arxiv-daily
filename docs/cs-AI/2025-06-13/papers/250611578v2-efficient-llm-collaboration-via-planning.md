---
layout: default
title: Efficient LLM Collaboration via Planning
---

# Efficient LLM Collaboration via Planning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.11578" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.11578v2</a>
  <a href="https://arxiv.org/pdf/2506.11578.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.11578v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.11578v2', 'Efficient LLM Collaboration via Planning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Byeongchan Lee, Jonghoon Lee, Dongyoung Kim, Jaehyung Kim, Kyungjoon Park, Dongjun Lee, Jinwoo Shin

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-13 (æ›´æ–°: 2025-09-27)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCOPEæ¡†æ¶ä»¥å®ç°å°å¤§æ¨¡å‹é«˜æ•ˆåä½œ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å°å‹æ¨¡å‹` `æ¨¡å‹åä½œ` `æ¨ç†æ•ˆç‡` `æˆæœ¬æ§åˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ€§èƒ½ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†é«˜æ˜‚çš„ä½¿ç”¨æˆæœ¬é™åˆ¶äº†å…¶å¹¿æ³›åº”ç”¨ã€‚
2. COPEæ¡†æ¶é€šè¿‡è§„åˆ’å’Œæ‰§è¡Œæ¨¡å‹çš„äº¤æ›¿åä½œï¼Œæ—¨åœ¨é«˜æ•ˆæ•´åˆå°å‹å’Œå¤§å‹æ¨¡å‹çš„ä¼˜åŠ¿ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCOPEåœ¨å¤šä¸ªä»»åŠ¡ä¸Šæ€§èƒ½ä¸å¤§å‹æ¨¡å‹ç›¸å½“ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†æ¨ç†æˆæœ¬ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä»ç®€å•åˆ°å¤æ‚çš„ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œå¤§å‹ä¸“æœ‰æ¨¡å‹ï¼ˆå¦‚å‚æ•°è¶…è¿‡100Bçš„æ¨¡å‹ï¼‰è™½ç„¶åœ¨å¤šæ ·åŒ–ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆæœï¼Œä½†å…¶é«˜æ˜‚çš„APIä½¿ç”¨è´¹ç”¨ä½¿å¾—é¢‘ç¹ä½¿ç”¨å˜å¾—ä¸åˆ‡å®é™…ã€‚ç›¸å¯¹è€Œè¨€ï¼Œå°å‹å¼€æºæ¨¡å‹ï¼ˆå¦‚å‚æ•°å°‘äº3Bçš„æ¨¡å‹ï¼‰è™½ç„¶å¯ä»¥è‡ªç”±ä½¿ç”¨ä¸”æ˜“äºæœ¬åœ°éƒ¨ç½²ï¼Œä½†åœ¨å¤æ‚ä»»åŠ¡ä¸Šçš„è¡¨ç°ä»ç„¶æœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æƒè¡¡é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†COPEï¼Œä¸€ä¸ªæµ‹è¯•æ—¶åä½œæ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡è§„åˆ’æ¨¡å‹ç”Ÿæˆä»»åŠ¡çš„é«˜å±‚æ¬¡æŠ½è±¡è®¡åˆ’ï¼Œå¹¶æŒ‡å¯¼ä¸‹æ¸¸æ‰§è¡Œæ¨¡å‹ã€‚å°å‹å’Œå¤§å‹æ¨¡å‹äº¤æ›¿æ‹…ä»»è§„åˆ’è€…å’Œæ‰§è¡Œè€…ï¼Œé€šè¿‡å¤šé˜¶æ®µçº§è”åä½œè§£å†³ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCOPEåœ¨æ•°å­¦æ¨ç†ã€ä»£ç ç”Ÿæˆç­‰åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºä¸å¤§å‹ä¸“æœ‰æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†æ¨ç†APIæˆæœ¬ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å°å‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹åœ¨åä½œæ—¶çš„æ•ˆç‡é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•å……åˆ†åˆ©ç”¨ä¸¤è€…çš„ä¼˜åŠ¿ï¼Œå¯¼è‡´æ€§èƒ½å’Œæˆæœ¬ä¹‹é—´çš„æƒè¡¡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCOPEæ¡†æ¶é€šè¿‡å¼•å…¥ä¸€ä¸ªè§„åˆ’æ¨¡å‹ï¼Œç”Ÿæˆä»»åŠ¡çš„é«˜å±‚æ¬¡è®¡åˆ’ï¼Œå¹¶æŒ‡å¯¼æ‰§è¡Œæ¨¡å‹çš„æ“ä½œï¼Œä»è€Œå®ç°å°å‹å’Œå¤§å‹æ¨¡å‹çš„é«˜æ•ˆåä½œã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCOPEçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸€ä¸ªè§„åˆ’æ¨¡å‹å’Œä¸€ä¸ªæ‰§è¡Œæ¨¡å‹ï¼Œè§„åˆ’æ¨¡å‹è´Ÿè´£ç”Ÿæˆä»»åŠ¡è®¡åˆ’ï¼Œæ‰§è¡Œæ¨¡å‹åˆ™æ ¹æ®è®¡åˆ’æ‰§è¡Œå…·ä½“ä»»åŠ¡ã€‚ä¸¤è€…äº¤æ›¿è¿›è¡Œï¼Œé€šè¿‡å¤šé˜¶æ®µçš„çº§è”æ–¹å¼å…±åŒè§£å†³å¤æ‚ä»»åŠ¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šCOPEçš„åˆ›æ–°åœ¨äºå¼•å…¥äº†è§„åˆ’ä½œä¸ºä¸­ä»‹ï¼Œå…è®¸å°å‹å’Œå¤§å‹æ¨¡å‹åœ¨ä»»åŠ¡æ‰§è¡Œä¸­äº’ç›¸åä½œï¼Œæ˜¾è‘—æé«˜äº†æ¨ç†æ•ˆç‡å’Œæ•ˆæœã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒCOPEåœ¨ä»»åŠ¡æ‰§è¡Œçš„çµæ´»æ€§å’Œæˆæœ¬æ•ˆç›Šä¸Šå…·æœ‰æ˜æ˜¾ä¼˜åŠ¿ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œè§„åˆ’æ¨¡å‹å’Œæ‰§è¡Œæ¨¡å‹çš„äº¤æ›¿è§’è‰²æ˜¯å…³é”®ï¼Œæ­¤å¤–ï¼Œä»»åŠ¡è®¡åˆ’çš„ç”Ÿæˆå’Œæ‰§è¡Œè¿‡ç¨‹ä¸­çš„å‚æ•°è®¾ç½®ä¹Ÿç»è¿‡ç²¾å¿ƒè°ƒæ•´ï¼Œä»¥ç¡®ä¿æ¨¡å‹é—´çš„æœ‰æ•ˆåä½œã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒCOPEåœ¨æ•°å­¦æ¨ç†ã€ä»£ç ç”Ÿæˆå’Œå¼€æ”¾å¼ä»»åŠ¡ç­‰åŸºå‡†æµ‹è¯•ä¸­ï¼Œæ€§èƒ½ä¸å¤§å‹ä¸“æœ‰æ¨¡å‹ç›¸å½“ï¼Œä¸”æ¨ç†APIæˆæœ¬æ˜¾è‘—é™ä½ï¼Œæå‡å¹…åº¦è¾¾åˆ°50%ä»¥ä¸Šã€‚è¿™ä¸€æˆæœå±•ç¤ºäº†è§„åˆ’åœ¨æˆæœ¬æ•ˆç›Šæ¨ç†ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

COPEæ¡†æ¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶é€‚ç”¨äºéœ€è¦é«˜æ•ˆæ¨ç†å’Œæˆæœ¬æ§åˆ¶çš„é¢†åŸŸï¼Œå¦‚æ™ºèƒ½å®¢æœã€ä»£ç ç”Ÿæˆå’Œå¤æ‚é—®é¢˜æ±‚è§£ç­‰ã€‚é€šè¿‡ç»“åˆå°å‹å’Œå¤§å‹æ¨¡å‹çš„ä¼˜åŠ¿ï¼ŒCOPEèƒ½å¤Ÿåœ¨å¤šç§å®é™…åœºæ™¯ä¸­æä¾›é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recently, large language models (LLMs) have demonstrated strong performance, ranging from simple to complex tasks. However, while large proprietary models (e.g., models with over 100B parameters) achieve remarkable results across diverse tasks, they are often accessible through costly APIs, making frequent use too costly for many applications. In contrast, small open-source models (e.g., models with fewer than 3B parameters) are freely available and easy to deploy locally, but their performance on complex tasks remains limited. This trade-off raises a natural question: how can small and large models efficiently collaborate to combine their complementary strengths? To bridge this trade-off, we propose COPE, a test-time collaboration framework. A planner model first generates a plan, a high-level abstraction of the task, and this plan serves as a lightweight intermediate that guides a downstream executor model. Small and large models take turns acting as planner and executor, exchanging plans in a multi-stage cascade to collaboratively solve tasks. Through comprehensive experiments on benchmarks spanning mathematical reasoning, code generation, open-ended tasks, and agent tasks, we demonstrate that COPE achieves performance comparable to large proprietary models, while drastically reducing the inference API cost. These results
>   highlight planning as an effective prior for cost-efficient inference.

