---
layout: default
title: Tracing LLM Reasoning Processes with Strategic Games: A Framework for Planning, Revision, and Resource-Constrained Decision Making
---

# Tracing LLM Reasoning Processes with Strategic Games: A Framework for Planning, Revision, and Resource-Constrained Decision Making

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.12012" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.12012v1</a>
  <a href="https://arxiv.org/pdf/2506.12012.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.12012v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.12012v1', 'Tracing LLM Reasoning Processes with Strategic Games: A Framework for Planning, Revision, and Resource-Constrained Decision Making')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiaopeng Yuan, Xingjian Zhang, Ke Xu, Yifan Xu, Lijun Yu, Jindong Wang, Yushun Dong, Haohan Wang

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-13

**å¤‡æ³¨**: 19 pages, 7 figures. Under review

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæˆ˜ç•¥æ¸¸æˆæ¡†æ¶ä»¥è¯„ä¼°LLMæ¨ç†è¿‡ç¨‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨ç†è¿‡ç¨‹` `æˆ˜ç•¥æ¸¸æˆ` `è¯„ä¼°æ¡†æ¶` `èµ„æºçº¦æŸå†³ç­–` `ä¿®æ­£æˆåŠŸç‡` `æ¨¡å‹æ€§èƒ½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨LLMsçš„æœ€ç»ˆå†³ç­–ï¼Œç¼ºä¹å¯¹ä¸­é—´æ¨ç†è¿‡ç¨‹çš„æ·±å…¥åˆ†æï¼Œå¯¼è‡´å¯¹æ¨¡å‹è¡Œä¸ºçš„ç†è§£ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåŸºäºæˆ˜ç•¥æ¸¸æˆçš„è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡è§„åˆ’ã€ä¿®æ­£å’Œèµ„æºçº¦æŸå†³ç­–ä¸‰ä¸ªç»´åº¦æ¥å…¨é¢è¯„ä¼°LLMsçš„æ¨ç†è¿‡ç¨‹ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒChatGPT-o3-miniåœ¨4320è½®å¯¹æŠ—ä¸­å–å¾—äº†74.7%çš„èƒœç‡å’Œ78.6%çš„ä¿®æ­£æˆåŠŸç‡ï¼ŒéªŒè¯äº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„åº”ç”¨æ—¥ç›Šå¢å¤šï¼Œä½†ç°æœ‰åŸºå‡†ä¸»è¦å…³æ³¨æœ€ç»ˆç»“æœï¼Œå¿½è§†äº†ä¸­é—´æ¨ç†æ­¥éª¤ï¼Œå¦‚è§„åˆ’ã€ä¿®æ­£å’Œèµ„æºçº¦æŸä¸‹çš„å†³ç­–ã€‚æœ¬æ–‡æå‡ºä½¿ç”¨æˆ˜ç•¥æ¸¸æˆä½œä¸ºè¯„ä¼°ç¯å¢ƒï¼Œæ„å»ºäº†ä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼Œæ¶µç›–è§„åˆ’ã€ä¿®æ­£å’Œèµ„æºçº¦æŸå†³ç­–ä¸‰ä¸ªæ ¸å¿ƒç»´åº¦ï¼Œå¹¶å®šä¹‰äº†è¶…è¶Šèƒœç‡çš„è¯„ä¼°æŒ‡æ ‡ã€‚é€šè¿‡4320è½®å¯¹æŠ—å®éªŒï¼ŒChatGPT-o3-miniåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾ç¤ºå‡ºè¯„ä¼°LLMsæ¨ç†è¿‡ç¨‹çš„é‡è¦æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¯¹å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†è¿‡ç¨‹çš„è¯„ä¼°ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½æ·±å…¥åˆ†ææ¨¡å‹åœ¨å†³ç­–è¿‡ç¨‹ä¸­çš„ä¸­é—´æ­¥éª¤å’Œè¡Œä¸ºè¡¨ç°ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥æˆ˜ç•¥æ¸¸æˆä½œä¸ºè¯„ä¼°ç¯å¢ƒï¼Œæ„å»ºä¸€ä¸ªç»¼åˆè¯„ä¼°æ¡†æ¶ï¼Œå…³æ³¨æ¨¡å‹åœ¨è§„åˆ’ã€ä¿®æ­£å’Œèµ„æºçº¦æŸå†³ç­–ä¸­çš„è¡¨ç°ï¼Œå¼ºè°ƒä¸­é—´æ¨ç†è¿‡ç¨‹çš„é‡è¦æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šè§„åˆ’æ¨¡å—ã€ä¿®æ­£æ¨¡å—å’Œèµ„æºçº¦æŸå†³ç­–æ¨¡å—ï¼Œåˆ©ç”¨æ¸…æ™°çš„çŠ¶æ€å’Œè‡ªåŠ¨åé¦ˆæœºåˆ¶æ¥è¯„ä¼°æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šå¼•å…¥äº†è¶…è¶Šèƒœç‡çš„å¤šç»´åº¦è¯„ä¼°æŒ‡æ ‡ï¼Œå¦‚è¿‡åº¦ä¿®æ­£é£é™©ç‡ã€ä¿®æ­£æˆåŠŸç‡ã€æ”¹è¿›æ–œç‡å’Œè¶…é¢„ç®—æ¯”ç‡ï¼Œæä¾›äº†æ›´å…¨é¢çš„æ¨¡å‹æ€§èƒ½è¯„ä¼°è§†è§’ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œè®¾ç½®äº†4320è½®å¯¹æŠ—æµ‹è¯•ï¼Œé‡‡ç”¨äº†å¤šç§æ¨¡å‹è¿›è¡Œæ¯”è¾ƒï¼Œç‰¹åˆ«å…³æ³¨äº†èµ„æºä½¿ç”¨æƒ…å†µä¸å†³ç­–æˆåŠŸç‡ä¹‹é—´çš„å…³ç³»ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒChatGPT-o3-miniåœ¨4320è½®å¯¹æŠ—ä¸­è·å¾—74.7%çš„èƒœç‡å’Œ78.6%çš„ä¿®æ­£æˆåŠŸç‡ï¼Œæ”¹è¿›æ–œç‡ä¸º0.041ï¼Œè¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯Qwen-Plusçš„25.6%èƒœç‡å’Œ81.6%è¿‡åº¦ä¿®æ­£é£é™©ç‡ï¼Œå¼ºè°ƒäº†è¯„ä¼°æ¨ç†è¿‡ç¨‹çš„é‡è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„è¯„ä¼°æ¡†æ¶å¯å¹¿æ³›åº”ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¼€å‘å’Œä¼˜åŒ–ï¼Œå¸®åŠ©ç ”ç©¶äººå‘˜ç†è§£æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ï¼Œä»è€Œæå‡æ¨¡å‹çš„å¯é æ€§å’Œå†³ç­–èƒ½åŠ›ã€‚æœªæ¥ï¼Œè¯¥æ¡†æ¶è¿˜å¯æ‰©å±•åˆ°å…¶ä»–ç±»å‹çš„æ™ºèƒ½ç³»ç»Ÿï¼Œæ¨åŠ¨æ™ºèƒ½å†³ç­–é¢†åŸŸçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) are increasingly used for tasks that require complex reasoning. Most benchmarks focus on final outcomes but overlook the intermediate reasoning steps - such as planning, revision, and decision making under resource constraints. We argue that measuring these internal processes is essential for understanding model behavior and improving reliability. We propose using strategic games as a natural evaluation environment: closed, rule-based systems with clear states, limited resources, and automatic feedback. We introduce a framework that evaluates LLMs along three core dimensions: planning, revision, and resource-constrained decision making. To operationalize this, we define metrics beyond win rate, including overcorrection risk rate, correction success rate, improvement slope, and over-budget ratio. In 4320 adversarial rounds across 12 leading models, ChatGPT-o3-mini achieves the top composite score, with a win rate of 74.7 percent, a correction success rate of 78.6 percent, and an improvement slope of 0.041. By contrast, Qwen-Plus, despite an overcorrection risk rate of 81.6 percent, wins only 25.6 percent of its matches - primarily due to excessive resource use. We also observe a negative correlation between overcorrection risk rate and correction success rate (Pearson r = -0.51, p = 0.093), suggesting that more frequent edits do not always improve outcomes. Our findings highlight the value of assessing not only what LLMs decide but how they arrive at those decisions

