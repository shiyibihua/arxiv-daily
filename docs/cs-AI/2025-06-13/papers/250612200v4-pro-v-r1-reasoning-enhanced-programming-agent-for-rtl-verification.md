---
layout: default
title: PRO-V-R1: Reasoning Enhanced Programming Agent for RTL Verification
---

# PRO-V-R1: Reasoning Enhanced Programming Agent for RTL Verification

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.12200" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.12200v4</a>
  <a href="https://arxiv.org/pdf/2506.12200.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.12200v4" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.12200v4', 'PRO-V-R1: Reasoning Enhanced Programming Agent for RTL Verification')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yujie Zhao, Zhijing Wu, Boqin Yuan, Zhongming Yu, Hejia Zhang, Wentao Ni, Chia-Tung Ho, Haoxing Ren, Jishen Zhao

**åˆ†ç±»**: cs.AI, cs.AR

**å‘å¸ƒæ—¥æœŸ**: 2025-06-13 (æ›´æ–°: 2025-12-08)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPRO-V-R1ä»¥è§£å†³RTLéªŒè¯ä¸­çš„æ•ˆç‡ç“¶é¢ˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `RTLéªŒè¯` `å¤§å‹è¯­è¨€æ¨¡å‹` `å¼€æºæ¡†æ¶` `å¼ºåŒ–å­¦ä¹ ` `åŠŸèƒ½æ­£ç¡®ç‡` `æ•…éšœæ£€æµ‹` `æ¨¡å—åŒ–ç³»ç»Ÿ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„RTLéªŒè¯æ–¹æ³•ä¾èµ–äºå¤§å‹ä¸“æœ‰æ¨¡å‹ï¼Œå¯¼è‡´é«˜æˆæœ¬å’Œæ•°æ®éšç§é£é™©ï¼Œç¼ºä¹å¼€æºè§£å†³æ–¹æ¡ˆã€‚
2. è®ºæ–‡æå‡ºPRO-V-R1ï¼Œä¸€ä¸ªæ¨¡å—åŒ–çš„è‡ªä¸»éªŒè¯æ¡†æ¶ï¼Œç»“åˆLLMæ¨ç†ä¸ç¨‹åºå·¥å…·ä½¿ç”¨ï¼Œæå‡RTLéªŒè¯æ•ˆç‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPRO-V-R1åœ¨åŠŸèƒ½æ­£ç¡®ç‡è¾¾åˆ°57.7%ï¼Œæ•…éšœæ£€æµ‹ç‡ä¸º34.0%ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¯„å­˜å™¨ä¼ è¾“çº§ï¼ˆRTLï¼‰éªŒè¯æ˜¯å¼€å‘è¿‡ç¨‹ä¸­çš„ä¸»è¦ç“¶é¢ˆï¼Œå æ®60-70%çš„å¼€å‘æ—¶é—´ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨RTLè‡ªåŠ¨åŒ–æ–¹é¢å±•ç°å‡ºæ½œåŠ›ï¼Œä½†å…¶ç ”ç©¶é‡ç‚¹ä¸»è¦é›†ä¸­åœ¨RTLç”Ÿæˆè€ŒééªŒè¯ä¸Šã€‚ç›®å‰çš„RTLéªŒè¯æ–¹æ³•ä¾èµ–äºå¤§å‹ä¸“æœ‰æ¨¡å‹ï¼ˆå¦‚GPT-4oï¼‰ç”ŸæˆåŸºäºPythonçš„åŠŸèƒ½å‚è€ƒï¼Œå¯¼è‡´é«˜æˆæœ¬å’Œæ•°æ®éšç§é£é™©ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†PRO-V-R1ï¼Œè¿™æ˜¯é¦–ä¸ªå¯è®­ç»ƒçš„å¼€æºè‡ªä¸»RTLéªŒè¯æ¡†æ¶ã€‚æˆ‘ä»¬çš„è´¡çŒ®åŒ…æ‹¬è®¾è®¡äº†ä¸€ä¸ªæ¨¡å—åŒ–çš„ä»£ç†ç³»ç»Ÿï¼Œå»ºç«‹äº†æ•°æ®æ„å»ºç®¡é“ï¼Œå¹¶å®ç°äº†é«˜æ•ˆçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPRO-V-R1åœ¨åŠŸèƒ½æ­£ç¡®ç‡å’Œæ•…éšœæ£€æµ‹æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰çš„è‡ªåŠ¨éªŒè¯ç³»ç»Ÿã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³RTLéªŒè¯è¿‡ç¨‹ä¸­çš„æ•ˆç‡ç“¶é¢ˆï¼Œç°æœ‰æ–¹æ³•ä¾èµ–äºå¤§å‹ä¸“æœ‰æ¨¡å‹ï¼Œå¯¼è‡´é«˜æˆæœ¬å’Œæ•°æ®éšç§é£é™©ï¼Œç¼ºä¹æœ‰æ•ˆçš„å¼€æºè§£å†³æ–¹æ¡ˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºPRO-V-R1ï¼Œé€šè¿‡ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†ä¸ç¨‹åºåŒ–å·¥å…·ä½¿ç”¨ï¼Œæ„å»ºä¸€ä¸ªæ¨¡å—åŒ–çš„ä»£ç†ç³»ç»Ÿï¼Œä»¥å®ç°è‡ªä¸»çš„RTLéªŒè¯ã€‚è¿™æ ·çš„è®¾è®¡æ—¨åœ¨æé«˜éªŒè¯çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼ŒåŒæ—¶é™ä½å¯¹ä¸“æœ‰æ¨¡å‹çš„ä¾èµ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPRO-V-R1çš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼š1) PRO-V sysæ¨¡å—ï¼Œè´Ÿè´£LLMæ¨ç†ä¸å·¥å…·ä½¿ç”¨çš„ç»“åˆï¼›2) æ•°æ®æ„å»ºç®¡é“ï¼Œåˆ©ç”¨ç°æœ‰RTLæ•°æ®é›†ç”Ÿæˆä¸“å®¶çº§è½¨è¿¹ï¼›3) å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ŒåŸºäºéªŒè¯åé¦ˆä¼˜åŒ–éªŒè¯æµç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„ä¸»è¦åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ä¸ªå¼€æºçš„è‡ªä¸»RTLéªŒè¯æ¡†æ¶ï¼Œé¦–æ¬¡å°†LLMæ¨ç†ä¸ç¨‹åºå·¥å…·ä½¿ç”¨ç›¸ç»“åˆï¼Œæ˜¾è‘—æå‡äº†éªŒè¯çš„åŠŸèƒ½æ­£ç¡®ç‡å’Œæ•…éšœæ£€æµ‹èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ç‰¹å®šäºéªŒè¯çš„å¥–åŠ±æœºåˆ¶ï¼Œä»¥å¼ºåŒ–å­¦ä¹ ç®—æ³•ä¼˜åŒ–æ•´ä¸ªéªŒè¯å·¥ä½œæµï¼Œç¡®ä¿äº†é«˜æ•ˆçš„è®­ç»ƒå’ŒéªŒè¯è¿‡ç¨‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒPRO-V-R1åœ¨åŠŸèƒ½æ­£ç¡®ç‡ä¸Šè¾¾åˆ°äº†57.7%ï¼Œè€Œç°æœ‰åŸºçº¿æ¨¡å‹ä»…ä¸º25.7%ï¼›åœ¨æ•…éšœæ£€æµ‹æ–¹é¢ï¼ŒPRO-V-R1çš„æ£€æµ‹ç‡ä¸º34.0%ï¼Œæ˜¾è‘—é«˜äºåŸºçº¿çš„21.8%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒPRO-V-R1åœ¨æ€§èƒ½ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬é›†æˆç”µè·¯è®¾è®¡ã€ç¡¬ä»¶éªŒè¯å’Œè‡ªåŠ¨åŒ–æµ‹è¯•ç­‰ã€‚é€šè¿‡æä¾›ä¸€ä¸ªå¼€æºçš„RTLéªŒè¯æ¡†æ¶ï¼Œèƒ½å¤Ÿé™ä½å¼€å‘æˆæœ¬ï¼Œæé«˜éªŒè¯æ•ˆç‡ï¼Œä¿ƒè¿›ç¡¬ä»¶è®¾è®¡é¢†åŸŸçš„åˆ›æ–°ä¸å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Register-Transfer Level (RTL) verification is a primary bottleneck, consuming 60-70% of development time. While Large Language Models (LLMs) show promise for RTL automation, their performance and research focus have overwhelmingly centered on RTL generation rather than verification. Current methods for RTL verification rely on large scale proprietary models (e.g., GPT-4o) to generate Python-based functional references, incurring a high cost and raising data-privacy risks. To date, an end-to-end open-source solution for autonomous verification remains absent.
>   We introduce PRO-V-R1, the first trainable open-source agentic framework for autonomous RTL verification. Our contributions are threefold: (1) we design PRO-V sys, a modular agentic system that couples LLM-based reasoning with programmatic tool use for RTL verification; (2) we establish a data construction pipeline that leverages existing RTL datasets to build simulation-validated, expert-level trajectories tailored for supervised fine-tuning (SFT) RTL verification agents; and (3) we implement an efficient reinforcement learning (RL) algorithm that uses verification-specific rewards derived from program-tool feedback to optimize the end-to-end verification workflow. Our empirical evaluation demonstrates PRO-V-R1 achieves a 57.7% functional correctness rate and 34.0% in robust fault detection, significantly outperforming the base model's 25.7% and 21.8% (respectively) from the state-of-the-art (SOTA) automatic verification system. This configuration also outperforms large-scale proprietary LLMs in functional correctness and shows comparable robustness for fault detection.

