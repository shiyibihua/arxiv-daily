---
layout: default
title: Learning to Deliberate: Meta-policy Collaboration for Agentic LLMs with Multi-agent Reinforcement Learning
---

# Learning to Deliberate: Meta-policy Collaboration for Agentic LLMs with Multi-agent Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.03817" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.03817v2</a>
  <a href="https://arxiv.org/pdf/2509.03817.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.03817v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.03817v2', 'Learning to Deliberate: Meta-policy Collaboration for Agentic LLMs with Multi-agent Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Wei Yang, Jesse Thomason

**åˆ†ç±»**: cs.AI, cs.MA

**å‘å¸ƒæ—¥æœŸ**: 2025-09-04 (æ›´æ–°: 2025-12-08)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMPDFæ¡†æ¶ï¼Œé€šè¿‡å…ƒç­–ç•¥åä½œæå‡Agentic LLMåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ` `å¤§å‹è¯­è¨€æ¨¡å‹` `å…ƒç­–ç•¥å­¦ä¹ ` `å¼ºåŒ–å­¦ä¹ ` `å¤æ‚æ¨ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMå¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¾èµ–å›ºå®šåä½œåè®®ï¼Œå¿½ç•¥äº†æ™ºèƒ½ä½“å†…éƒ¨çš„è‡ªé€‚åº”å®¡è®®èƒ½åŠ›ï¼Œé™åˆ¶äº†å¤æ‚æ¨ç†æ€§èƒ½ã€‚
2. MPDFæ¡†æ¶ä½¿æ™ºèƒ½ä½“å­¦ä¹ å…³äºåšæŒã€æ”¹è¿›å’Œè®©æ­¥ç­‰å…ƒè®¤çŸ¥åŠ¨ä½œçš„å»ä¸­å¿ƒåŒ–ç­–ç•¥ï¼Œå®ç°åŠ¨æ€åä½œã€‚
3. SoftRankPOç®—æ³•é€šè¿‡å¥–åŠ±æ’åç¨³å®šè®­ç»ƒè¿‡ç¨‹ï¼ŒMPDFåœ¨å¤šä¸ªæ¨ç†ä»»åŠ¡ä¸Šç›¸æ¯”ç°æœ‰æ–¹æ³•æå‡4-5%å‡†ç¡®ç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨å¤æ‚æ¨ç†æ–¹é¢å±•ç°å‡ºæ½œåŠ›ï¼Œä½†å…¶æœ‰æ•ˆæ€§å¸¸å—é™äºå›ºå®šçš„åä½œåè®®ã€‚è¿™äº›æ¡†æ¶é€šå¸¸ä¾§é‡äºå®è§‚å±‚é¢çš„ç¼–æ’ï¼Œè€Œå¿½ç•¥äº†æ™ºèƒ½ä½“å†…éƒ¨çš„å®¡è®®èƒ½åŠ›ã€‚è¿™ç§å…³é”®çš„å…ƒè®¤çŸ¥ç›²ç‚¹å°†æ™ºèƒ½ä½“è§†ä¸ºè¢«åŠ¨çš„æ‰§è¡Œè€…ï¼Œæ— æ³•æ ¹æ®å†…éƒ¨è®¤çŸ¥çŠ¶æ€ï¼ˆå¦‚ä¸ç¡®å®šæ€§æˆ–ç½®ä¿¡åº¦ï¼‰æ¥è°ƒæ•´å…¶ç­–ç•¥ã€‚æˆ‘ä»¬å¼•å…¥äº†å…ƒç­–ç•¥å®¡è®®æ¡†æ¶ï¼ˆMPDFï¼‰ï¼Œå…¶ä¸­æ™ºèƒ½ä½“å­¦ä¹ å…³äºä¸€ç»„é«˜çº§å…ƒè®¤çŸ¥åŠ¨ä½œï¼ˆåšæŒã€æ”¹è¿›å’Œè®©æ­¥ï¼‰çš„å»ä¸­å¿ƒåŒ–ç­–ç•¥ã€‚ä¸ºäº†å…‹æœä¼ ç»Ÿç­–ç•¥æ¢¯åº¦åœ¨è¿™ç§ç¯å¢ƒä¸­çš„ä¸ç¨³å®šæ€§ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•SoftRankPOã€‚SoftRankPOé€šè¿‡åŸºäºå¹³æ»‘æ­£æ€åˆ†ä½æ•°æ˜ å°„çš„å¥–åŠ±æ’åæ¥å¡‘é€ ä¼˜åŠ¿ï¼Œä»è€Œç¨³å®šè®­ç»ƒï¼Œä½¿å­¦ä¹ è¿‡ç¨‹å¯¹å¥–åŠ±æ–¹å·®å…·æœ‰é²æ£’æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œä¸å…­ç§æœ€å…ˆè¿›çš„å¯å‘å¼å’ŒåŸºäºå­¦ä¹ çš„å¤šæ™ºèƒ½ä½“æ¨ç†ç®—æ³•ç›¸æ¯”ï¼Œå…·æœ‰SoftRankPOçš„MPDFåœ¨äº”ä¸ªæ•°å­¦å’Œä¸€èˆ¬æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†4-5%çš„å¹³å‡å‡†ç¡®ç‡ç»å¯¹æå‡ã€‚æˆ‘ä»¬çš„å·¥ä½œæå‡ºäº†ä¸€ç§ä¸ºå¤šæ™ºèƒ½ä½“LLMç³»ç»Ÿå­¦ä¹ è‡ªé€‚åº”å…ƒè®¤çŸ¥ç­–ç•¥çš„èŒƒä¾‹ï¼Œå°†é‡ç‚¹ä»è®¾è®¡å›ºå®šåè®®è½¬ç§»åˆ°å­¦ä¹ åŠ¨æ€å®¡è®®ç­–ç•¥ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºLLMçš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨è§£å†³å¤æ‚æ¨ç†é—®é¢˜æ—¶ï¼Œé€šå¸¸é‡‡ç”¨é¢„å®šä¹‰çš„ã€é™æ€çš„åä½œåè®®ã€‚è¿™äº›åè®®ç¼ºä¹çµæ´»æ€§ï¼Œæ— æ³•ä½¿æ™ºèƒ½ä½“æ ¹æ®è‡ªèº«çš„çŠ¶æ€ï¼ˆä¾‹å¦‚ï¼Œå¯¹å½“å‰ç­”æ¡ˆçš„ç½®ä¿¡åº¦ï¼‰åŠ¨æ€è°ƒæ•´ç­–ç•¥ã€‚è¿™ç§ç¼ºä¹è‡ªé€‚åº”æ€§çš„é—®é¢˜å¯¼è‡´æ™ºèƒ½ä½“æ— æ³•æœ‰æ•ˆåœ°åˆ©ç”¨å½¼æ­¤çš„çŸ¥è¯†å’Œèƒ½åŠ›ï¼Œä»è€Œé™åˆ¶äº†æ•´ä½“çš„æ¨ç†æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯èµ‹äºˆæ™ºèƒ½ä½“å…ƒè®¤çŸ¥èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿå­¦ä¹ å¦‚ä½•è¿›è¡Œåä½œã€‚å…·ä½“æ¥è¯´ï¼Œæ¯ä¸ªæ™ºèƒ½ä½“å­¦ä¹ ä¸€ä¸ªå…ƒç­–ç•¥ï¼Œè¯¥ç­–ç•¥å†³å®šäº†æ™ºèƒ½ä½“åœ¨æ¯ä¸ªæ—¶é—´æ­¥åº”è¯¥é‡‡å–çš„å…ƒè®¤çŸ¥åŠ¨ä½œï¼Œä¾‹å¦‚â€œåšæŒå½“å‰ç­”æ¡ˆâ€ã€â€œæ”¹è¿›å½“å‰ç­”æ¡ˆâ€æˆ–â€œè®©æ­¥ç»™å…¶ä»–æ™ºèƒ½ä½“â€ã€‚é€šè¿‡å­¦ä¹ è¿™äº›å…ƒç­–ç•¥ï¼Œæ™ºèƒ½ä½“å¯ä»¥æ ¹æ®è‡ªèº«çš„çŠ¶æ€å’Œç¯å¢ƒåŠ¨æ€åœ°è°ƒæ•´å…¶åä½œè¡Œä¸ºï¼Œä»è€Œæé«˜æ•´ä½“çš„æ¨ç†æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMPDFæ¡†æ¶åŒ…å«å¤šä¸ªLLMæ™ºèƒ½ä½“ï¼Œæ¯ä¸ªæ™ºèƒ½ä½“éƒ½é…å¤‡ä¸€ä¸ªå…ƒç­–ç•¥ã€‚åœ¨æ¯ä¸ªæ—¶é—´æ­¥ï¼Œæ¯ä¸ªæ™ºèƒ½ä½“é¦–å…ˆæ ¹æ®å…¶å½“å‰çŠ¶æ€ï¼ˆä¾‹å¦‚ï¼Œå†å²å¯¹è¯ã€å½“å‰ç­”æ¡ˆã€ç½®ä¿¡åº¦ï¼‰é€‰æ‹©ä¸€ä¸ªå…ƒè®¤çŸ¥åŠ¨ä½œã€‚ç„¶åï¼Œæ ¹æ®æ‰€é€‰æ‹©çš„åŠ¨ä½œï¼Œæ™ºèƒ½ä½“æ‰§è¡Œç›¸åº”çš„æ“ä½œï¼Œä¾‹å¦‚ç”Ÿæˆæ–°çš„ç­”æ¡ˆã€ä¿®æ”¹ç°æœ‰ç­”æ¡ˆæˆ–æ¥å—å…¶ä»–æ™ºèƒ½ä½“çš„ç­”æ¡ˆã€‚æ‰€æœ‰æ™ºèƒ½ä½“çš„åŠ¨ä½œå…±åŒå½±å“ç¯å¢ƒçš„çŠ¶æ€ï¼Œå¹¶äº§ç”Ÿä¸€ä¸ªå¥–åŠ±ä¿¡å·ï¼Œç”¨äºè®­ç»ƒå…ƒç­–ç•¥ã€‚ä¸ºäº†ç¨³å®šè®­ç»ƒè¿‡ç¨‹ï¼Œè®ºæ–‡æå‡ºäº†SoftRankPOç®—æ³•ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†MPDFæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å…è®¸æ™ºèƒ½ä½“å­¦ä¹ è‡ªé€‚åº”çš„å…ƒç­–ç•¥ï¼Œä»è€Œå®ç°æ›´æœ‰æ•ˆçš„åä½œã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†SoftRankPOç®—æ³•ï¼Œè¯¥ç®—æ³•é€šè¿‡åŸºäºå¥–åŠ±æ’åçš„ä¼˜åŠ¿å‡½æ•°æ¥ç¨³å®šè®­ç»ƒè¿‡ç¨‹ï¼Œä½¿å…¶å¯¹å¥–åŠ±æ–¹å·®å…·æœ‰é²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šSoftRankPOç®—æ³•çš„å…³é”®è®¾è®¡åœ¨äºä½¿ç”¨å¹³æ»‘æ­£æ€åˆ†ä½æ•°æ¥æ˜ å°„å¥–åŠ±æ’åã€‚å…·ä½“æ¥è¯´ï¼Œç®—æ³•é¦–å…ˆè®¡ç®—æ‰€æœ‰æ™ºèƒ½ä½“çš„å¥–åŠ±æ’åï¼Œç„¶åå°†è¿™äº›æ’åæ˜ å°„åˆ°æ ‡å‡†æ­£æ€åˆ†å¸ƒçš„åˆ†ä½æ•°ã€‚è¿™äº›åˆ†ä½æ•°è¢«ç”¨ä½œä¼˜åŠ¿å‡½æ•°çš„æƒé‡ï¼Œä»è€Œä½¿ç®—æ³•èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨å¥–åŠ±ä¿¡æ¯ï¼Œå¹¶ç¨³å®šè®­ç»ƒè¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œå…ƒç­–ç•¥é€šå¸¸ä½¿ç”¨å°å‹ç¥ç»ç½‘ç»œå®ç°ï¼Œè¾“å…¥åŒ…æ‹¬æ™ºèƒ½ä½“çš„å†…éƒ¨çŠ¶æ€ï¼ˆä¾‹å¦‚ï¼Œç½®ä¿¡åº¦ã€ä¸ç¡®å®šæ€§ï¼‰å’Œå¤–éƒ¨ä¿¡æ¯ï¼ˆä¾‹å¦‚ï¼Œå…¶ä»–æ™ºèƒ½ä½“çš„è¾“å‡ºï¼‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMPDFæ¡†æ¶åœ¨äº”ä¸ªæ•°å­¦å’Œä¸€èˆ¬æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼Œç›¸æ¯”å…­ç§æœ€å…ˆè¿›çš„å¯å‘å¼å’ŒåŸºäºå­¦ä¹ çš„å¤šæ™ºèƒ½ä½“æ¨ç†ç®—æ³•ï¼Œå®ç°äº†4-5%çš„å¹³å‡å‡†ç¡®ç‡ç»å¯¹æå‡ã€‚è¿™è¡¨æ˜MPDFæ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆåœ°å­¦ä¹ è‡ªé€‚åº”çš„åä½œç­–ç•¥ï¼Œå¹¶æ˜¾è‘—æé«˜å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„æ¨ç†æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºéœ€è¦å¤šæ™ºèƒ½ä½“åä½œçš„å¤æ‚æ¨ç†ä»»åŠ¡ï¼Œä¾‹å¦‚ç§‘å­¦å‘ç°ã€è½¯ä»¶å¼€å‘ã€é‡‘èåˆ†æç­‰ã€‚é€šè¿‡å­¦ä¹ è‡ªé€‚åº”çš„åä½œç­–ç•¥ï¼Œæ™ºèƒ½ä½“å¯ä»¥æ›´æœ‰æ•ˆåœ°åˆ©ç”¨å½¼æ­¤çš„çŸ¥è¯†å’Œèƒ½åŠ›ï¼Œä»è€Œæé«˜è§£å†³é—®é¢˜çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥æ‰©å±•åˆ°å…¶ä»–ç±»å‹çš„æ™ºèƒ½ä½“ç³»ç»Ÿï¼Œä¾‹å¦‚æœºå™¨äººå›¢é˜Ÿå’Œè‡ªåŠ¨é©¾é©¶è½¦è¾†ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multi-agent systems of large language models (LLMs) show promise for complex reasoning, but their effectiveness is often limited by fixed collaboration protocols. These frameworks typically focus on macro-level orchestration while overlooking agents' internal deliberative capabilities. This critical meta-cognitive blindspot treats agents as passive executors unable to adapt their strategy based on internal cognitive states like uncertainty or confidence. We introduce the Meta-Policy Deliberation Framework (MPDF), where agents learn a decentralized policy over a set of high-level meta-cognitive actions: Persist, Refine, and Concede. To overcome the instability of traditional policy gradients in this setting, we develop SoftRankPO, a novel reinforcement learning algorithm. SoftRankPO stabilizes training by shaping advantages based on the rank of rewards mapped through smooth normal quantiles, making the learning process robust to reward variance. Experiments show that MPDF with SoftRankPO achieves a a 4-5% absolute gain in average accuracy across five mathematical and general reasoning benchmarks compared to six state-of-the-art heuristic and learning-based multi-agent reasoning algorithms. Our work presents a paradigm for learning adaptive, meta-cognitive policies for multi-agent LLM systems, shifting the focus from designing fixed protocols to learning dynamic, deliberative strategies.

