---
layout: default
title: Meta-Policy Reflexion: Reusable Reflective Memory and Rule Admissibility for Resource-Efficient LLM Agent
---

# Meta-Policy Reflexion: Reusable Reflective Memory and Rule Admissibility for Resource-Efficient LLM Agent

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.03990" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.03990v2</a>
  <a href="https://arxiv.org/pdf/2509.03990.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.03990v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.03990v2', 'Meta-Policy Reflexion: Reusable Reflective Memory and Rule Admissibility for Resource-Efficient LLM Agent')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chunlong Wu, Ye Luo, Zhibo Qu, Min Wang

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-04 (æ›´æ–°: 2025-09-08)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMeta-Policy Reflexionï¼Œæå‡LLM Agentåœ¨å¤æ‚ä»»åŠ¡ä¸­çš„æ•ˆç‡ä¸æ³›åŒ–æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `LLM Agent` `åæ€å­¦ä¹ ` `å…ƒç­–ç•¥` `çŸ¥è¯†é‡ç”¨` `è§„åˆ™çº¦æŸ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLM Agentåœ¨å¤æ‚ä»»åŠ¡ä¸­å­˜åœ¨é‡å¤å¤±è´¥ã€æ¢ç´¢æ•ˆç‡ä½å’Œè·¨ä»»åŠ¡æ³›åŒ–èƒ½åŠ›å¼±ç­‰é—®é¢˜ã€‚
2. Meta-Policy Reflexion (MPR) å°†LLMåæ€æç‚¼ä¸ºå¯é‡ç”¨çš„Meta-Policy Memoryï¼Œé€šè¿‡è½¯è§£ç å¼•å¯¼å’Œç¡¬è§„åˆ™çº¦æŸæå‡Agentæ€§èƒ½ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒMPRåœ¨æ‰§è¡Œå‡†ç¡®æ€§å’Œé²æ£’æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶èƒ½æœ‰æ•ˆæå‡Agentçš„ç¨³å®šæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰Agentåœ¨å•ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†å¸¸å‡ºç°é‡å¤å¤±è´¥ã€æ¢ç´¢æ•ˆç‡ä½å’Œè·¨ä»»åŠ¡é€‚åº”æ€§æœ‰é™ç­‰é—®é¢˜ã€‚ç°æœ‰çš„åæ€ç­–ç•¥ï¼ˆå¦‚Reflexionã€ReActï¼‰è™½ç„¶èƒ½æ”¹å–„å•æ¬¡è¡Œä¸ºï¼Œä½†é€šå¸¸äº§ç”ŸçŸ­æš‚ä¸”ç‰¹å®šäºä»»åŠ¡çš„è½¨è¿¹ï¼Œæ— æ³•è·¨ä»»åŠ¡é‡ç”¨ã€‚åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•è™½ç„¶å¯ä»¥äº§ç”Ÿå¯è¿ç§»çš„ç­–ç•¥ï¼Œä½†éœ€è¦å¤§é‡çš„å‚æ•°æ›´æ–°å’Œè®¡ç®—ã€‚æœ¬æ–‡æå‡ºäº†Meta-Policy Reflexionï¼ˆMPRï¼‰ï¼šä¸€ç§æ··åˆæ¡†æ¶ï¼Œå°†LLMç”Ÿæˆçš„åæ€ç»“æœæ•´åˆåˆ°ç»“æ„åŒ–çš„ã€ç±»ä¼¼è°“è¯çš„Meta-Policy Memoryï¼ˆMPMï¼‰ä¸­ï¼Œå¹¶é€šè¿‡è½¯è®°å¿†å¼•å¯¼è§£ç å’Œç¡¬è§„åˆ™å¯é‡‡çº³æ€§æ£€æŸ¥ï¼ˆHACï¼‰ä¸¤ç§äº’è¡¥æœºåˆ¶åœ¨æ¨ç†æ—¶åº”ç”¨è¯¥è®°å¿†ã€‚MPRï¼ˆiï¼‰æ— éœ€æ¨¡å‹æƒé‡æ›´æ–°å³å¯å¤–éƒ¨åŒ–å¯é‡ç”¨çš„çº æ­£çŸ¥è¯†ï¼Œï¼ˆiiï¼‰å¼ºåˆ¶æ‰§è¡Œé¢†åŸŸçº¦æŸä»¥å‡å°‘ä¸å®‰å…¨æˆ–æ— æ•ˆçš„æ“ä½œï¼Œå¹¶ä¸”ï¼ˆiiiï¼‰ä¿ç•™äº†åŸºäºè¯­è¨€çš„åæ€çš„é€‚åº”æ€§ã€‚æœ¬æ–‡å½¢å¼åŒ–äº†MPMè¡¨ç¤ºï¼Œæå‡ºäº†æ›´æ–°å’Œè§£ç ç®—æ³•ï¼Œå¹¶åœ¨åŸºäºæ–‡æœ¬çš„Agentç¯å¢ƒä¸­éªŒè¯äº†è¯¥æ–¹æ³•ï¼ˆåŸºäºAlfWorldï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ReflexionåŸºçº¿ç›¸æ¯”ï¼Œæ‰§è¡Œå‡†ç¡®æ€§å’Œé²æ£’æ€§å¾—åˆ°äº†æŒç»­æé«˜ï¼›è§„åˆ™å¯é‡‡çº³æ€§è¿›ä¸€æ­¥æé«˜äº†ç¨³å®šæ€§ã€‚æœ¬æ–‡åˆ†æäº†è§£é‡Šè¿™äº›æ”¶ç›Šçš„æœºåˆ¶ï¼Œè®¨è®ºäº†å¯æ‰©å±•æ€§å’Œå¤±è´¥æ¨¡å¼ï¼Œå¹¶æ¦‚è¿°äº†å¤šæ¨¡æ€å’Œå¤šAgentæ‰©å±•çš„æœªæ¥æ–¹å‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰LLM Agentåœ¨å¤æ‚ä»»åŠ¡ä¸­è¡¨ç°å‡ºé‡å¤å¤±è´¥ï¼Œä½æ•ˆæ¢ç´¢ï¼Œä»¥åŠæœ‰é™çš„è·¨ä»»åŠ¡é€‚åº”æ€§ã€‚ç°æœ‰çš„åæ€æ–¹æ³•ï¼ˆå¦‚Reflexionï¼‰äº§ç”Ÿçš„ç»éªŒæ˜¯çŸ­æš‚çš„ï¼Œæ— æ³•è·¨ä»»åŠ¡é‡ç”¨ã€‚å¼ºåŒ–å­¦ä¹ æ–¹æ³•è™½ç„¶å¯ä»¥å­¦ä¹ å¯è¿ç§»çš„ç­–ç•¥ï¼Œä½†éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºå’Œå‚æ•°æ›´æ–°ã€‚å› æ­¤ï¼Œå¦‚ä½•è®©LLM Agentèƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨å†å²ç»éªŒï¼Œé¿å…é‡å¤çŠ¯é”™ï¼Œå¹¶å…·å¤‡æ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMPRçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†LLMç”Ÿæˆçš„åæ€çŸ¥è¯†æç‚¼å¹¶å­˜å‚¨åˆ°Meta-Policy Memory (MPM) ä¸­ï¼Œè¯¥MPMä»¥ç»“æ„åŒ–çš„ã€ç±»ä¼¼è°“è¯çš„å½¢å¼å­˜å‚¨çŸ¥è¯†ã€‚åœ¨æ¨ç†é˜¶æ®µï¼ŒMPRé€šè¿‡ä¸¤ç§æœºåˆ¶åˆ©ç”¨MPMï¼šè½¯è®°å¿†å¼•å¯¼è§£ç å’Œç¡¬è§„åˆ™å¯é‡‡çº³æ€§æ£€æŸ¥(HAC)ã€‚è½¯è®°å¿†å¼•å¯¼è§£ç é€šè¿‡è°ƒæ•´LLMçš„è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒï¼Œé¼“åŠ±Agenté€‰æ‹©æ›´ç¬¦åˆå†å²ç»éªŒçš„è¡Œä¸ºã€‚ç¡¬è§„åˆ™å¯é‡‡çº³æ€§æ£€æŸ¥åˆ™ç›´æ¥ç¦æ­¢Agenté€‰æ‹©è¿åé¢†åŸŸçº¦æŸæˆ–å·²çŸ¥é”™è¯¯çš„è¡Œä¸ºã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMPRæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) LLM Agentï¼šè´Ÿè´£ä¸ç¯å¢ƒäº¤äº’ï¼Œç”ŸæˆåŠ¨ä½œåºåˆ—ã€‚2) Reflection Moduleï¼šåœ¨Agentæ‰§è¡Œå¤±è´¥åï¼Œåˆ©ç”¨LLMç”Ÿæˆåæ€ï¼Œæ€»ç»“å¤±è´¥åŸå› å’Œæ”¹è¿›ç­–ç•¥ã€‚3) Meta-Policy Memory (MPM)ï¼šå­˜å‚¨ä»åæ€ä¸­æå–çš„çŸ¥è¯†ï¼Œä»¥ç»“æ„åŒ–çš„å½¢å¼è¡¨ç¤ºã€‚4) Soft Memory-Guided Decodingï¼šåˆ©ç”¨MPMä¸­çš„çŸ¥è¯†è°ƒæ•´LLMçš„è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒã€‚5) Hard Rule Admissibility Checks (HAC)ï¼šæ ¹æ®MPMä¸­çš„è§„åˆ™ï¼Œè¿‡æ»¤æ‰ä¸åˆæ³•çš„åŠ¨ä½œã€‚æ•´ä¸ªæµç¨‹æ˜¯ï¼šAgentä¸ç¯å¢ƒäº¤äº’ -> å¤±è´¥åè¿›è¡Œåæ€ -> åæ€ç»“æœæ›´æ–°MPM -> ä¸‹æ¬¡äº¤äº’æ—¶ï¼Œåˆ©ç”¨MPMè¿›è¡Œè½¯è§£ç å¼•å¯¼å’Œç¡¬è§„åˆ™æ£€æŸ¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šMPRçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†Meta-Policy Memory (MPM)ï¼Œä¸€ç§ç»“æ„åŒ–çš„çŸ¥è¯†è¡¨ç¤ºæ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å­˜å‚¨å’Œé‡ç”¨LLMç”Ÿæˆçš„åæ€çŸ¥è¯†ã€‚2) ç»“åˆäº†è½¯è®°å¿†å¼•å¯¼è§£ç å’Œç¡¬è§„åˆ™å¯é‡‡çº³æ€§æ£€æŸ¥ä¸¤ç§æœºåˆ¶ï¼Œæ—¢èƒ½åˆ©ç”¨å†å²ç»éªŒæŒ‡å¯¼Agentçš„è¡Œä¸ºï¼Œåˆèƒ½å¼ºåˆ¶æ‰§è¡Œé¢†åŸŸçº¦æŸï¼Œé¿å…AgentçŠ¯é”™ã€‚3) æ— éœ€æ¨¡å‹æƒé‡æ›´æ–°å³å¯å¤–éƒ¨åŒ–å¯é‡ç”¨çš„çº æ­£çŸ¥è¯†ï¼Œé™ä½äº†è®¡ç®—æˆæœ¬ã€‚

**å…³é”®è®¾è®¡**ï¼šMPMé‡‡ç”¨ç±»ä¼¼è°“è¯çš„ç»“æ„åŒ–è¡¨ç¤ºï¼Œä¾‹å¦‚â€œå¦‚æœAgentåœ¨Xæƒ…å†µä¸‹æ‰§è¡Œäº†åŠ¨ä½œYå¯¼è‡´å¤±è´¥ï¼Œé‚£ä¹ˆä¸‹æ¬¡åœ¨Xæƒ…å†µä¸‹ä¸è¦æ‰§è¡ŒåŠ¨ä½œYâ€ã€‚è½¯è®°å¿†å¼•å¯¼è§£ç é€šè¿‡è°ƒæ•´LLMçš„è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒæ¥å®ç°ï¼Œå…·ä½“æ–¹æ³•æ˜¯æ ¹æ®MPMä¸­çŸ¥è¯†çš„ç›¸å…³æ€§ï¼Œå¯¹LLMçš„è¾“å‡ºlogitsè¿›è¡ŒåŠ æƒã€‚ç¡¬è§„åˆ™å¯é‡‡çº³æ€§æ£€æŸ¥åˆ™ç›´æ¥æ ¹æ®MPMä¸­çš„è§„åˆ™ï¼Œè¿‡æ»¤æ‰ä¸åˆæ³•çš„åŠ¨ä½œã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°åœ¨è®ºæ–‡ä¸­æ²¡æœ‰è¯¦ç»†æè¿°ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMPRåœ¨AlfWorldç¯å¢ƒä¸­ï¼Œä¸ReflexionåŸºçº¿ç›¸æ¯”ï¼Œæ‰§è¡Œå‡†ç¡®æ€§å’Œé²æ£’æ€§å¾—åˆ°äº†æŒç»­æé«˜ã€‚è§„åˆ™å¯é‡‡çº³æ€§è¿›ä¸€æ­¥æé«˜äº†Agentçš„ç¨³å®šæ€§ã€‚å…·ä½“çš„æ•°æ®æå‡å¹…åº¦åœ¨è®ºæ–‡ä¸­æ²¡æœ‰æ˜ç¡®ç»™å‡ºï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚ä½†æ€»ä½“è€Œè¨€ï¼Œå®éªŒéªŒè¯äº†MPRçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MPRå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ä»¥åº”ç”¨äºå„ç§éœ€è¦æ™ºèƒ½Agentè¿›è¡Œå†³ç­–çš„ä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚æ¸¸æˆAIã€æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨åŒ–å®¢æœç­‰ã€‚é€šè¿‡å­¦ä¹ å’Œé‡ç”¨å†å²ç»éªŒï¼ŒMPRå¯ä»¥æ˜¾è‘—æé«˜Agentçš„æ•ˆç‡å’Œé²æ£’æ€§ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”å¤æ‚å’ŒåŠ¨æ€çš„ç¯å¢ƒã€‚æœªæ¥çš„ç ”ç©¶å¯ä»¥å°†MPRæ‰©å±•åˆ°å¤šæ¨¡æ€å’Œå¤šAgentåœºæ™¯ï¼Œè¿›ä¸€æ­¥æå‡å…¶åº”ç”¨ä»·å€¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language model (LLM) agents achieve impressive single-task performance but commonly exhibit repeated failures, inefficient exploration, and limited cross-task adaptability. Existing reflective strategies (e.g., Reflexion, ReAct) improve per-episode behavior but typically produce ephemeral, task-specific traces that are not reused across tasks. Reinforcement-learning based alternatives can produce transferable policies but require substantial parameter updates and compute. In this work we introduce Meta-Policy Reflexion (MPR): a hybrid framework that consolidates LLM-generated reflections into a structured, predicate-like Meta-Policy Memory (MPM) and applies that memory at inference time through two complementary mechanisms soft memory-guided decoding and hard rule admissibility checks(HAC). MPR (i) externalizes reusable corrective knowledge without model weight updates, (ii) enforces domain constraints to reduce unsafe or invalid actions, and (iii) retains the adaptability of language-based reflection. We formalize the MPM representation, present algorithms for update and decoding, and validate the approach in a text-based agent environment following the experimental protocol described in the provided implementation (AlfWorld-based). Empirical results reported in the supplied material indicate consistent gains in execution accuracy and robustness when compared to Reflexion baselines; rule admissibility further improves stability. We analyze mechanisms that explain these gains, discuss scalability and failure modes, and outline future directions for multimodal and multi-agent extensions.

