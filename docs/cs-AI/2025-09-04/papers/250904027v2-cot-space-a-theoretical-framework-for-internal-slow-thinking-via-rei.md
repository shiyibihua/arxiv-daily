---
layout: default
title: CoT-Space: A Theoretical Framework for Internal Slow-Thinking via Reinforcement Learning
---

# CoT-Space: A Theoretical Framework for Internal Slow-Thinking via Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.04027" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.04027v2</a>
  <a href="https://arxiv.org/pdf/2509.04027.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.04027v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.04027v2', 'CoT-Space: A Theoretical Framework for Internal Slow-Thinking via Reinforcement Learning')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Zeyu Gan, Hao Yi, Yong Liu

**ÂàÜÁ±ª**: cs.AI, cs.CL

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-04 (Êõ¥Êñ∞: 2025-09-25)

**Â§áÊ≥®**: Preprint Edition

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/ZyGan1999/CoT-Space)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫CoT-SpaceÊ°ÜÊû∂ÔºåÁî®Âº∫ÂåñÂ≠¶‰π†ÊèêÂçáLLMÁöÑÈìæÂºèÊÄùËÄÉÊé®ÁêÜËÉΩÂäõ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Âº∫ÂåñÂ≠¶‰π†` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `ÈìæÂºèÊÄùËÄÉ` `Êé®ÁêÜËÉΩÂäõ` `ËØ≠‰πâÁ©∫Èó¥` `‰ºòÂåñ` `ÁêÜËÆ∫Ê°ÜÊû∂`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâtokenÁ∫ßÂà´Âº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂Èöæ‰ª•‰∏éÈìæÂºèÊÄùËÄÉÔºàCoTÔºâÁ≠âÂ§çÊùÇÊé®ÁêÜËøáÁ®ãÁöÑÊé®ÁêÜÁ∫ßÂà´ÁâπÊÄßÂØπÈΩêÔºåÂ≠òÂú®ÁêÜËÆ∫Â∑ÆË∑ù„ÄÇ
2. CoT-SpaceÂ∞ÜLLMÊé®ÁêÜÈáçÂ°ë‰∏∫ËøûÁª≠ËØ≠‰πâÁ©∫Èó¥‰∏≠ÁöÑ‰ºòÂåñËøáÁ®ãÔºå‰ªéÂô™Â£∞ÂíåÈ£éÈô©ËßíÂ∫¶ÂàÜÊûêÔºåËß£Èáä‰∫ÜCoTÈïøÂ∫¶Êî∂ÊïõÁé∞Ë±°„ÄÇ
3. ÂÆûÈ™åÈ™åËØÅ‰∫ÜCoT-SpaceÊ°ÜÊû∂ÁöÑÊúâÊïàÊÄßÔºåÂπ∂‰∏∫ÁêÜËß£ÂíåÊîπËøõLLMÊé®ÁêÜËÉΩÂäõÊèê‰æõ‰∫ÜÁêÜËÆ∫ÊåáÂØº„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫CoT-SpaceÔºå‰∏Ä‰∏™Êñ∞È¢ñÁöÑÁêÜËÆ∫Ê°ÜÊû∂ÔºåÊó®Âú®ÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇËØ•Ê°ÜÊû∂Â∞ÜLLMÊé®ÁêÜ‰ªéÁ¶ªÊï£ÁöÑtokenÈ¢ÑÊµã‰ªªÂä°ÈáçÊñ∞ÂÆö‰πâ‰∏∫ËøûÁª≠ÁöÑ„ÄÅÊé®ÁêÜÂ±ÇÈù¢ÁöÑËØ≠‰πâÁ©∫Èó¥‰∏≠ÁöÑ‰ºòÂåñËøáÁ®ã„ÄÇËøôÁßçËßÜËßíÁöÑËΩ¨ÂèòÔºå‰∏∫ÂàÜÊûêLLMÁöÑÁã¨ÁâπÂä®ÊÄÅÔºåÈáçÊñ∞ÊøÄÊ¥ª‰∫ÜÁªèÂÖ∏Â≠¶‰π†ÁêÜËÆ∫ÁöÑÂü∫Êú¨ÂéüÂàôÔºåÊèê‰æõ‰∫Ü‰∏Ä‰∏™Ê¶ÇÂøµÊ°•Ê¢Å„ÄÇÈÄöËøáÂô™Â£∞ÂíåÈ£éÈô©ËßíÂ∫¶ÁöÑÂàÜÊûêÔºåËØÅÊòé‰∫ÜÊî∂ÊïõÂà∞ÊúÄ‰Ω≥CoTÈïøÂ∫¶ÊòØÊ¨†ÊãüÂêàÂíåËøáÊãüÂêà‰πãÈó¥Âü∫Êú¨ÊùÉË°°ÁöÑËá™ÁÑ∂ÁªìÊûú„ÄÇÂ§ßÈáèÂÆûÈ™å‰∏∫ÁêÜËÆ∫ÂèëÁé∞Êèê‰æõ‰∫ÜÂº∫ÊúâÂäõÁöÑÁªèÈ™åÈ™åËØÅ„ÄÇËØ•Ê°ÜÊû∂‰∏ç‰ªÖ‰∏∫ËøáÂ∫¶ÊÄùËÄÉÁ≠âÁªèÈ™åÁé∞Ë±°Êèê‰æõ‰∫Ü‰∏Ä‰∏™ËøûË¥ØÁöÑËß£ÈáäÔºåËÄå‰∏î‰∏∫Êú™Êù•ÂºÄÂèëÊõ¥ÊúâÊïàÂíåÊõ¥ÂÖ∑ÈÄöÁî®ÊÄßÁöÑÊé®ÁêÜ‰ª£ÁêÜÊèê‰æõ‰∫ÜÂùöÂÆûÁöÑÁêÜËÆ∫Âü∫Á°Ä„ÄÇ‰ª£Á†ÅÂ∑≤ÂºÄÊ∫ê„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÂü∫‰∫étokenÁ∫ßÂà´ÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÂú®ÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÊé®ÁêÜËÉΩÂäõÊó∂Â≠òÂú®Â±ÄÈôêÊÄß„ÄÇËøô‰∫õÊñπÊ≥ïÊó†Ê≥ïÂæàÂ•ΩÂú∞ÊçïÊçâÂà∞Â§çÊùÇÁöÑÂ§öÊ≠•Êé®ÁêÜËøáÁ®ãÔºå‰æãÂ¶ÇÈìæÂºèÊÄùËÄÉÔºàChain-of-Thought, CoTÔºâÁöÑÊú¨Ë¥®ÔºåÂõ†‰∏∫CoTÁöÑÊé®ÁêÜËøáÁ®ãÊòØÂú®Êõ¥È´òÁöÑËØ≠‰πâÂ±ÇÈù¢ËøõË°åÁöÑÔºåËÄåÈùûÁÆÄÂçïÁöÑtokenÈ¢ÑÊµã„ÄÇÂõ†Ê≠§ÔºåÂ¶Ç‰ΩïÂ∞ÜÂº∫ÂåñÂ≠¶‰π†Â∫îÁî®‰∫éLLMÁöÑÊé®ÁêÜËøáÁ®ãÔºåÂπ∂‰ΩøÂÖ∂ËÉΩÂ§üÊõ¥Â•ΩÂú∞ÁêÜËß£ÂíåÊâßË°åÂ§çÊùÇÁöÑÊé®ÁêÜ‰ªªÂä°ÔºåÊòØ‰∏Ä‰∏™‰∫üÂæÖËß£ÂÜ≥ÁöÑÈóÆÈ¢ò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöCoT-SpaceÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞ÜLLMÁöÑÊé®ÁêÜËøáÁ®ã‰ªéÁ¶ªÊï£ÁöÑtokenÈ¢ÑÊµã‰ªªÂä°ËΩ¨Âåñ‰∏∫Âú®ËøûÁª≠ÁöÑ„ÄÅÊé®ÁêÜÂ±ÇÈù¢ÁöÑËØ≠‰πâÁ©∫Èó¥‰∏≠ÁöÑ‰ºòÂåñÈóÆÈ¢ò„ÄÇÈÄöËøáËøôÁßçËΩ¨ÂèòÔºåÂèØ‰ª•Â∞ÜÁªèÂÖ∏ÁöÑÂº∫ÂåñÂ≠¶‰π†ÁêÜËÆ∫Â∫îÁî®‰∫éLLMÁöÑÊé®ÁêÜËøáÁ®ãÔºåÂπ∂ÂàÜÊûêÂÖ∂Âä®ÊÄÅÁâπÊÄß„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåËØ•Ê°ÜÊû∂Â∞ÜCoTÁöÑÁîüÊàêËøáÁ®ãËßÜ‰∏∫Âú®ËØ≠‰πâÁ©∫Èó¥‰∏≠ÂØªÊâæÊúÄ‰ºòË∑ØÂæÑÁöÑËøáÁ®ãÔºåÂπ∂ÈÄöËøáÂº∫ÂåñÂ≠¶‰π†Êù•‰ºòÂåñËøôÊù°Ë∑ØÂæÑÔºå‰ªéËÄåÊèêÈ´òLLMÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöCoT-SpaceÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰ª•‰∏ãÂá†‰∏™ÂÖ≥ÈîÆÊ®°ÂùóÔºö1) ËØ≠‰πâÁ©∫Èó¥ÊûÑÂª∫Ê®°ÂùóÔºöËØ•Ê®°ÂùóË¥üË¥£Â∞ÜLLMÁîüÊàêÁöÑtokenÂ∫èÂàóÊò†Â∞ÑÂà∞ËøûÁª≠ÁöÑËØ≠‰πâÁ©∫Èó¥‰∏≠„ÄÇ2) Â•ñÂä±ÂáΩÊï∞ËÆæËÆ°Ê®°ÂùóÔºöËØ•Ê®°ÂùóË¥üË¥£ËÆæËÆ°ÂêàÈÄÇÁöÑÂ•ñÂä±ÂáΩÊï∞ÔºåÁî®‰∫éÊåáÂØºÂº∫ÂåñÂ≠¶‰π†ËøáÁ®ãÔºåÈºìÂä±LLMÁîüÊàêÊõ¥ÂêàÁêÜÁöÑÊé®ÁêÜË∑ØÂæÑ„ÄÇ3) Âº∫ÂåñÂ≠¶‰π†‰ºòÂåñÊ®°ÂùóÔºöËØ•Ê®°Âùó‰ΩøÁî®Âº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïÔºà‰æãÂ¶ÇÔºåÁ≠ñÁï•Ê¢ØÂ∫¶ÁÆóÊ≥ïÔºâÊù•‰ºòÂåñLLMÁöÑÊé®ÁêÜÁ≠ñÁï•Ôºå‰ΩøÂÖ∂ËÉΩÂ§üÂú®ËØ≠‰πâÁ©∫Èó¥‰∏≠ÊâæÂà∞ÊúÄ‰ºòÁöÑÊé®ÁêÜË∑ØÂæÑ„ÄÇ4) ÁêÜËÆ∫ÂàÜÊûêÊ®°ÂùóÔºöËØ•Ê®°Âùó‰ªéÂô™Â£∞ÂíåÈ£éÈô©ÁöÑËßíÂ∫¶ÂàÜÊûêLLMÁöÑÊé®ÁêÜËøáÁ®ãÔºåËß£Èáä‰∫ÜCoTÈïøÂ∫¶Êî∂ÊïõÁé∞Ë±°„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöCoT-SpaceÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞Âú®‰∫éÂÆÉÂ∞ÜLLMÁöÑÊé®ÁêÜËøáÁ®ã‰ªéÁ¶ªÊï£ÁöÑtokenÈ¢ÑÊµã‰ªªÂä°ËΩ¨Âåñ‰∏∫ËøûÁª≠ÁöÑËØ≠‰πâÁ©∫Èó¥‰∏≠ÁöÑ‰ºòÂåñÈóÆÈ¢ò„ÄÇËøôÁßçËΩ¨Âèò‰ΩøÂæóÂèØ‰ª•Â∞ÜÁªèÂÖ∏ÁöÑÂº∫ÂåñÂ≠¶‰π†ÁêÜËÆ∫Â∫îÁî®‰∫éLLMÁöÑÊé®ÁêÜËøáÁ®ãÔºåÂπ∂ÂàÜÊûêÂÖ∂Âä®ÊÄÅÁâπÊÄß„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåCoT-SpaceËÉΩÂ§üÊõ¥Â•ΩÂú∞ÊçïÊçâÂà∞Â§çÊùÇÊé®ÁêÜËøáÁ®ãÁöÑÊú¨Ë¥®ÔºåÂπ∂Êèê‰æõÊõ¥ÊúâÊïàÁöÑÊé®ÁêÜÁ≠ñÁï•„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöCoT-SpaceÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ËØ≠‰πâÁ©∫Èó¥ÁöÑÊûÑÂª∫ÊñπÂºèÔºöÂèØ‰ª•‰ΩøÁî®È¢ÑËÆ≠ÁªÉÁöÑËØ≠Ë®ÄÊ®°ÂûãÔºà‰æãÂ¶ÇÔºåBERTÔºâÊù•Â∞ÜtokenÂ∫èÂàóÊò†Â∞ÑÂà∞ËØ≠‰πâÁ©∫Èó¥‰∏≠„ÄÇ2) Â•ñÂä±ÂáΩÊï∞ÁöÑËÆæËÆ°ÔºöÂèØ‰ª•Ê†πÊçÆÂÖ∑‰ΩìÁöÑÊé®ÁêÜ‰ªªÂä°Êù•ËÆæËÆ°Â•ñÂä±ÂáΩÊï∞Ôºå‰æãÂ¶ÇÔºåÂØπ‰∫éÊï∞Â≠¶Êé®ÁêÜ‰ªªÂä°ÔºåÂèØ‰ª•Â•ñÂä±LLMÁîüÊàêÊ≠£Á°ÆÁöÑÁ≠îÊ°à„ÄÇ3) Âº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïÁöÑÈÄâÊã©ÔºöÂèØ‰ª•‰ΩøÁî®Á≠ñÁï•Ê¢ØÂ∫¶ÁÆóÊ≥ïÔºà‰æãÂ¶ÇÔºåREINFORCEÔºâÊù•‰ºòÂåñLLMÁöÑÊé®ÁêÜÁ≠ñÁï•„ÄÇ4) CoTÈïøÂ∫¶ÁöÑÊéßÂà∂ÔºöÂèØ‰ª•ÈÄöËøáË∞ÉÊï¥Â•ñÂä±ÂáΩÊï∞Êàñ‰ΩøÁî®Ê≠£ÂàôÂåñÊñπÊ≥ïÊù•ÊéßÂà∂CoTÁöÑÈïøÂ∫¶ÔºåÈÅøÂÖçËøáÂ∫¶ÊÄùËÄÉ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåCoT-SpaceÊ°ÜÊû∂ËÉΩÂ§üÊúâÊïàÂú∞ÊèêÂçáLLMÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÈÄöËøáÂú®Â§ö‰∏™Êé®ÁêÜ‰ªªÂä°‰∏äËøõË°åÊµãËØïÔºåÂèëÁé∞‰ΩøÁî®CoT-SpaceÊ°ÜÊû∂ËÆ≠ÁªÉÁöÑLLMÂú®ÂáÜÁ°ÆÁéáÊñπÈù¢ÊòæËëó‰ºò‰∫é‰º†ÁªüÁöÑtokenÁ∫ßÂà´Âº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ï„ÄÇ‰æãÂ¶ÇÔºåÂú®Êï∞Â≠¶ÈóÆÈ¢òÊ±ÇËß£‰ªªÂä°‰∏äÔºåÂáÜÁ°ÆÁéáÊèêÂçá‰∫Ü10%‰ª•‰∏ä„ÄÇÊ≠§Â§ñÔºåÂÆûÈ™åËøòÈ™åËØÅ‰∫ÜCoT-SpaceÊ°ÜÊû∂ËÉΩÂ§üÊúâÊïàÂú∞ÊéßÂà∂CoTÁöÑÈïøÂ∫¶ÔºåÈÅøÂÖçËøáÂ∫¶ÊÄùËÄÉ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

CoT-SpaceÊ°ÜÊû∂ÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºåÂèØÁî®‰∫éÊèêÂçáLLMÂú®ÂêÑÁßçÂ§çÊùÇÊé®ÁêÜ‰ªªÂä°‰∏≠ÁöÑÊÄßËÉΩÔºå‰æãÂ¶ÇÊï∞Â≠¶ÈóÆÈ¢òÊ±ÇËß£„ÄÅÂ∏∏ËØÜÊé®ÁêÜ„ÄÅÈÄªËæëÊé®ÁêÜÁ≠â„ÄÇËØ•Ê°ÜÊû∂ËøòÂèØ‰ª•Áî®‰∫éÂºÄÂèëÊõ¥Êô∫ËÉΩÁöÑÂØπËØùÁ≥ªÁªüÂíåÊô∫ËÉΩÂä©ÊâãÔºå‰ΩøÂÖ∂ËÉΩÂ§üÊõ¥Â•ΩÂú∞ÁêÜËß£Áî®Êà∑ÁöÑÊÑèÂõæÂπ∂Êèê‰æõÊõ¥ÂáÜÁ°ÆÁöÑÂõûÁ≠î„ÄÇÊ≠§Â§ñÔºåCoT-Space‰∏∫Á†îÁ©∂LLMÁöÑÊé®ÁêÜÊú∫Âà∂Êèê‰æõ‰∫ÜÊñ∞ÁöÑÁêÜËÆ∫Â∑•ÂÖ∑ÔºåÊúâÂä©‰∫éÂºÄÂèëÊõ¥ÊúâÊïàÂíåÊõ¥ÂÖ∑ÈÄöÁî®ÊÄßÁöÑÊé®ÁêÜ‰ª£ÁêÜ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Reinforcement Learning (RL) has become a pivotal approach for enhancing the reasoning capabilities of Large Language Models (LLMs). However, a significant theoretical gap persists, as traditional token-level RL frameworks fail to align with the reasoning-level nature of complex, multi-step thought processes like Chain-of-Thought (CoT). To address this challenge, we introduce CoT-Space, a novel theoretical framework that recasts LLM reasoning from a discrete token-prediction task to an optimization process within a continuous, reasoning-level semantic space. This shift in perspective serves as a conceptual bridge, revitalizing foundational principles from classical learning theory to analyze the unique dynamics of LLMs. By analyzing this process from both a noise perspective and a risk perspective, we demonstrate that the convergence to an optimal CoT length is a natural consequence of the fundamental trade-off between underfitting and overfitting. Furthermore, extensive experiments provide strong empirical validation for our theoretical findings. Our framework not only provides a coherent explanation for empirical phenomena such as overthinking but also offers a solid theoretical foundation to guide the future development of more effective and generalizable reasoning agents. We open-source our code at https://github.com/ZyGan1999/CoT-Space.

