---
layout: default
title: Hybrid Reinforcement Learning and Search for Flight Trajectory Planning
---

# Hybrid Reinforcement Learning and Search for Flight Trajectory Planning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.04100" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.04100v1</a>
  <a href="https://arxiv.org/pdf/2509.04100.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.04100v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.04100v1', 'Hybrid Reinforcement Learning and Search for Flight Trajectory Planning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Alberto Luise, Michele Lombardi, Florent Teichteil Koenigsbuch

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-04

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ··åˆå¼ºåŒ–å­¦ä¹ ä¸æœç´¢çš„é£è¡Œè½¨è¿¹è§„åˆ’æ–¹æ³•ï¼ŒåŠ é€Ÿç´§æ€¥æƒ…å†µä¸‹çš„èˆªçº¿é‡è§„åˆ’ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `è·¯å¾„è§„åˆ’` `é£è¡Œè½¨è¿¹ä¼˜åŒ–` `æ··åˆç®—æ³•` `æœç´¢ç®—æ³•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰é£è¡Œè½¨è¿¹è§„åˆ’æ–¹æ³•åœ¨ç´§æ€¥æƒ…å†µä¸‹é‡æ–°è§„åˆ’èˆªçº¿æ—¶é€Ÿåº¦è¾ƒæ…¢ï¼Œéš¾ä»¥æ»¡è¶³å®æ—¶æ€§è¦æ±‚ã€‚
2. è¯¥è®ºæ–‡æå‡ºåˆ©ç”¨å¼ºåŒ–å­¦ä¹ é¢„å…ˆè®¡ç®—è¿‘ä¼˜è·¯å¾„ï¼Œå¹¶å°†å…¶ä½œä¸ºçº¦æŸæ¡ä»¶åŠ é€Ÿæœç´¢ç®—æ³•çš„æ±‚è§£è¿‡ç¨‹ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç‡ƒæ–™æ¶ˆè€—å‡ ä¹ä¸å˜çš„æƒ…å†µä¸‹ï¼Œè®¡ç®—é€Ÿåº¦ç›¸æ¯”ä¼ ç»Ÿæ–¹æ³•æå‡é«˜è¾¾50%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æ¢ç´¢äº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸åŸºäºæœç´¢çš„è·¯å¾„è§„åˆ’å™¨ç›¸ç»“åˆçš„æ–¹æ³•ï¼Œæ—¨åœ¨åŠ é€Ÿå®¢æœºé£è¡Œè·¯å¾„çš„ä¼˜åŒ–ï¼Œå°¤å…¶æ˜¯åœ¨ç´§æ€¥æƒ…å†µä¸‹å¿«é€Ÿé‡æ–°è®¡ç®—èˆªçº¿è‡³å…³é‡è¦ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯è®­ç»ƒä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ï¼Œä½¿å…¶åŸºäºä½ç½®å’Œå¤§æ°”æ•°æ®é¢„å…ˆè®¡ç®—å‡ºæ¥è¿‘æœ€ä¼˜çš„è·¯å¾„ï¼Œå¹¶åœ¨è¿è¡Œæ—¶åˆ©ç”¨è¿™äº›è·¯å¾„æ¥çº¦æŸåº•å±‚çš„è·¯å¾„è§„åˆ’æ±‚è§£å™¨ï¼Œä»è€Œåœ¨åˆå§‹çŒœæµ‹çš„ä¸€å®šè·ç¦»å†…æ‰¾åˆ°è§£å†³æ–¹æ¡ˆã€‚è¯¥æ–¹æ³•æœ‰æ•ˆåœ°å‡å°äº†æ±‚è§£å™¨çš„æœç´¢ç©ºé—´ï¼Œæ˜¾è‘—åŠ å¿«äº†èˆªçº¿ä¼˜åŒ–é€Ÿåº¦ã€‚è™½ç„¶ä¸èƒ½ä¿è¯å…¨å±€æœ€ä¼˜æ€§ï¼Œä½†ä½¿ç”¨ç©ºå®¢é£æœºæ€§èƒ½æ¨¡å‹è¿›è¡Œçš„å®éªŒç»“æœè¡¨æ˜ï¼Œç‡ƒæ–™æ¶ˆè€—ä¸æ— çº¦æŸæ±‚è§£å™¨å‡ ä¹ç›¸åŒï¼Œåå·®é€šå¸¸åœ¨1%ä»¥å†…ã€‚åŒæ—¶ï¼Œä¸å•ç‹¬ä½¿ç”¨ä¼ ç»Ÿæ±‚è§£å™¨ç›¸æ¯”ï¼Œè®¡ç®—é€Ÿåº¦æœ€å¤šå¯æé«˜50%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³é£è¡Œè½¨è¿¹è§„åˆ’ä¸­ï¼Œå°¤å…¶æ˜¯åœ¨ç´§æ€¥æƒ…å†µä¸‹å¿«é€Ÿé‡æ–°è§„åˆ’èˆªçº¿çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚ä¼ ç»Ÿçš„è·¯å¾„è§„åˆ’æ±‚è§£å™¨ï¼Œåœ¨é¢å¯¹å¤æ‚ç¯å¢ƒå’Œå®æ—¶æ€§è¦æ±‚æ—¶ï¼Œè®¡ç®—é€Ÿåº¦è¾ƒæ…¢ï¼Œéš¾ä»¥æ»¡è¶³éœ€æ±‚ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§èƒ½å¤Ÿå¿«é€Ÿç”Ÿæˆå¯è¡Œä¸”æ¥è¿‘æœ€ä¼˜èˆªçº¿çš„æ–¹æ³•ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„å¿«é€Ÿå†³ç­–èƒ½åŠ›å’Œæœç´¢ç®—æ³•çš„ç²¾ç¡®ä¼˜åŒ–èƒ½åŠ›ï¼Œå°†ä¸¤è€…ç»“åˆèµ·æ¥ã€‚å…·ä½“è€Œè¨€ï¼Œé¦–å…ˆä½¿ç”¨RLè®­ç»ƒä¸€ä¸ªæ™ºèƒ½ä½“ï¼Œä½¿å…¶èƒ½å¤Ÿæ ¹æ®å½“å‰çš„ä½ç½®å’Œå¤§æ°”æ•°æ®ï¼Œå¿«é€Ÿç”Ÿæˆä¸€ä¸ªæ¥è¿‘æœ€ä¼˜çš„èˆªçº¿ã€‚ç„¶åï¼Œå°†è¯¥èˆªçº¿ä½œä¸ºæœç´¢ç®—æ³•çš„åˆå§‹çŒœæµ‹å’Œçº¦æŸæ¡ä»¶ï¼Œä»è€Œå‡å°æœç´¢ç©ºé—´ï¼ŒåŠ é€Ÿæ±‚è§£è¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šç¦»çº¿è®­ç»ƒé˜¶æ®µå’Œåœ¨çº¿è§„åˆ’é˜¶æ®µã€‚åœ¨ç¦»çº¿è®­ç»ƒé˜¶æ®µï¼Œä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•è®­ç»ƒä¸€ä¸ªæ™ºèƒ½ä½“ï¼Œä½¿å…¶èƒ½å¤Ÿæ ¹æ®ä½ç½®å’Œå¤§æ°”æ•°æ®é¢„æµ‹è¿‘ä¼˜èˆªçº¿ã€‚åœ¨åœ¨çº¿è§„åˆ’é˜¶æ®µï¼Œå½“éœ€è¦é‡æ–°è§„åˆ’èˆªçº¿æ—¶ï¼Œé¦–å…ˆä½¿ç”¨è®­ç»ƒå¥½çš„RLæ™ºèƒ½ä½“ç”Ÿæˆä¸€ä¸ªåˆå§‹èˆªçº¿ã€‚ç„¶åï¼Œå°†è¯¥èˆªçº¿ä½œä¸ºçº¦æŸæ¡ä»¶è¾“å…¥åˆ°ä¼ ç»Ÿçš„è·¯å¾„è§„åˆ’æ±‚è§£å™¨ä¸­ï¼Œæ±‚è§£å™¨åœ¨çº¦æŸçš„èŒƒå›´å†…æœç´¢æœ€ä¼˜èˆªçº¿ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå°†å¼ºåŒ–å­¦ä¹ å’Œæœç´¢ç®—æ³•æœ‰æœºç»“åˆã€‚å¼ºåŒ–å­¦ä¹ ç”¨äºå¿«é€Ÿç”Ÿæˆåˆå§‹è§£ï¼Œæœç´¢ç®—æ³•ç”¨äºåœ¨åˆå§‹è§£é™„è¿‘è¿›è¡Œç²¾ç¡®ä¼˜åŒ–ã€‚è¿™ç§æ··åˆæ–¹æ³•æ—¢èƒ½ä¿è¯è®¡ç®—é€Ÿåº¦ï¼Œåˆèƒ½ä¿è¯è§£çš„è´¨é‡ã€‚ä¸å•ç‹¬ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å¯ä»¥è·å¾—æ›´é«˜çš„ç²¾åº¦ï¼›ä¸å•ç‹¬ä½¿ç”¨æœç´¢ç®—æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å¯ä»¥æ˜¾è‘—æé«˜è®¡ç®—é€Ÿåº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­ä½¿ç”¨äº†ç‰¹å®šçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆå…·ä½“ç®—æ³•æœªçŸ¥ï¼‰ï¼Œå¹¶è®¾è®¡äº†ç›¸åº”çš„å¥–åŠ±å‡½æ•°ï¼Œä»¥é¼“åŠ±æ™ºèƒ½ä½“ç”Ÿæˆç‡ƒæ–™æ¶ˆè€—ä½çš„èˆªçº¿ã€‚æ­¤å¤–ï¼Œè¿˜è®¾è®¡äº†ä¸€ç§çº¦æŸæœºåˆ¶ï¼Œå°†RLç”Ÿæˆçš„èˆªçº¿ä½œä¸ºæœç´¢ç®—æ³•çš„çº¦æŸæ¡ä»¶ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç­‰ç»†èŠ‚ä¿¡æ¯æœªçŸ¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿçš„è·¯å¾„è§„åˆ’æ±‚è§£å™¨ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨ç‡ƒæ–™æ¶ˆè€—å‡ ä¹ä¸å˜ï¼ˆåå·®é€šå¸¸åœ¨1%ä»¥å†…ï¼‰çš„æƒ…å†µä¸‹ï¼Œè®¡ç®—é€Ÿåº¦æœ€å¤šå¯æé«˜50%ã€‚è¿™è¡¨æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨ä¿è¯é£è¡Œæ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—æé«˜èˆªçº¿è§„åˆ’çš„æ•ˆç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ°‘èˆªé£è¡Œæ§åˆ¶ç³»ç»Ÿï¼Œç‰¹åˆ«æ˜¯åœ¨ç´§æ€¥æƒ…å†µä¸‹ï¼Œä¾‹å¦‚é­é‡æ¶åŠ£å¤©æ°”æˆ–æœºæ¢°æ•…éšœæ—¶ï¼Œèƒ½å¤Ÿå¿«é€Ÿç”Ÿæˆå¤‡é€‰èˆªçº¿ï¼Œæé«˜é£è¡Œå®‰å…¨æ€§å’Œæ•ˆç‡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥æ‰©å±•åˆ°å…¶ä»–éœ€è¦å¿«é€Ÿè·¯å¾„è§„åˆ’çš„é¢†åŸŸï¼Œå¦‚æ— äººæœºå¯¼èˆªã€æœºå™¨äººè¿åŠ¨è§„åˆ’ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper explores the combination of Reinforcement Learning (RL) and search-based path planners to speed up the optimization of flight paths for airliners, where in case of emergency a fast route re-calculation can be crucial. The fundamental idea is to train an RL Agent to pre-compute near-optimal paths based on location and atmospheric data and use those at runtime to constrain the underlying path planning solver and find a solution within a certain distance from the initial guess. The approach effectively reduces the size of the solver's search space, significantly speeding up route optimization. Although global optimality is not guaranteed, empirical results conducted with Airbus aircraft's performance models show that fuel consumption remains nearly identical to that of an unconstrained solver, with deviations typically within 1%. At the same time, computation speed can be improved by up to 50% as compared to using a conventional solver alone.

