---
layout: default
title: SoK: Semantic Privacy in Large Language Models
---

# SoK: Semantic Privacy in Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.23603" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.23603v2</a>
  <a href="https://arxiv.org/pdf/2506.23603.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.23603v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.23603v2', 'SoK: Semantic Privacy in Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Baihe Ma, Yanna Jiang, Xu Wang, Guangsheng Yu, Qin Wang, Caijun Sun, Chen Li, Xuelei Qi, Ying He, Wei Ni, Ren Ping Liu

**åˆ†ç±»**: cs.CR, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-30 (æ›´æ–°: 2025-07-16)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç”Ÿå‘½å‘¨æœŸæ¡†æ¶ä»¥è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹çš„è¯­ä¹‰éšç§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­ä¹‰éšç§` `å¤§å‹è¯­è¨€æ¨¡å‹` `å·®åˆ†éšç§` `éšç§ä¿æŠ¤` `ä¸Šä¸‹æ–‡æ¨æ–­` `æ½œåœ¨è¡¨ç¤ºæ³„éœ²` `å¤šæ¨¡æ€è¾“å…¥` `ç”Ÿå‘½å‘¨æœŸæ¡†æ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç°æœ‰çš„æ•°æ®éšç§æªæ–½æ— æ³•æœ‰æ•ˆåº”å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„è¯­ä¹‰éšç§é£é™©ï¼Œå°¤å…¶æ˜¯åœ¨ä¸Šä¸‹æ–‡æ¨æ–­å’Œæ½œåœ¨è¡¨ç¤ºæ³„éœ²æ–¹é¢ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæå‡ºäº†ä¸€ç§ç”Ÿå‘½å‘¨æœŸä¸­å¿ƒçš„æ¡†æ¶ï¼Œåˆ†æè¯­ä¹‰éšç§é£é™©çš„äº§ç”Ÿï¼Œå¹¶è¯„ä¼°ç°æœ‰é˜²å¾¡æªæ–½çš„æœ‰æ•ˆæ€§ã€‚
3. å®éªŒæˆ–æ•ˆæœï¼šé€šè¿‡åˆ†æï¼Œæ­ç¤ºäº†è¯­ä¹‰å±‚é¢ä¿æŠ¤çš„å…³é”®ç¼ºå£ï¼Œå¹¶æå‡ºäº†æœªæ¥ç ”ç©¶çš„å¼€æ”¾æ€§æŒ‘æˆ˜ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•æ„Ÿé¢†åŸŸçš„å¹¿æ³›åº”ç”¨ï¼Œä¼ ç»Ÿçš„æ•°æ®éšç§æªæ–½å·²æ— æ³•æœ‰æ•ˆä¿æŠ¤éšå«ã€ä¸Šä¸‹æ–‡æˆ–å¯æ¨æ–­çš„ä¿¡æ¯ï¼Œå³æˆ‘ä»¬æ‰€å®šä¹‰çš„è¯­ä¹‰éšç§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ä»¥ç”Ÿå‘½å‘¨æœŸä¸ºä¸­å¿ƒçš„æ¡†æ¶ï¼Œåˆ†æè¯­ä¹‰éšç§é£é™©åœ¨è¾“å…¥å¤„ç†ã€é¢„è®­ç»ƒã€å¾®è°ƒå’Œå¯¹é½é˜¶æ®µçš„äº§ç”Ÿã€‚æˆ‘ä»¬å¯¹å…³é”®æ”»å‡»å‘é‡è¿›è¡Œäº†åˆ†ç±»ï¼Œå¹¶è¯„ä¼°äº†å½“å‰é˜²å¾¡æªæ–½ï¼ˆå¦‚å·®åˆ†éšç§ã€åµŒå…¥åŠ å¯†ã€è¾¹ç¼˜è®¡ç®—å’Œé—å¿˜æŠ€æœ¯ï¼‰å¦‚ä½•åº”å¯¹è¿™äº›å¨èƒã€‚åˆ†æç»“æœæ­ç¤ºäº†è¯­ä¹‰å±‚é¢ä¿æŠ¤çš„å…³é”®ç¼ºå£ï¼Œå°¤å…¶æ˜¯åœ¨ä¸Šä¸‹æ–‡æ¨æ–­å’Œæ½œåœ¨è¡¨ç¤ºæ³„éœ²æ–¹é¢ã€‚æœ€åï¼Œæˆ‘ä»¬æ¦‚è¿°äº†å¼€æ”¾æ€§æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬é‡åŒ–è¯­ä¹‰æ³„éœ²ã€ä¿æŠ¤å¤šæ¨¡æ€è¾“å…¥ã€å¹³è¡¡å»æ ‡è¯†åŒ–ä¸ç”Ÿæˆè´¨é‡ï¼Œä»¥åŠç¡®ä¿éšç§æ‰§è¡Œçš„é€æ˜æ€§ã€‚è¯¥ç ”ç©¶æ—¨åœ¨ä¸ºæœªæ¥è®¾è®¡ç¨³å¥çš„ã€è¯­ä¹‰æ„ŸçŸ¥çš„éšç§ä¿æŠ¤æŠ€æœ¯æä¾›å‚è€ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„è¯­ä¹‰éšç§é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨ä¿æŠ¤éšå«å’Œä¸Šä¸‹æ–‡ä¿¡æ¯æ–¹é¢å­˜åœ¨æ˜æ˜¾ä¸è¶³ï¼Œæ— æ³•æœ‰æ•ˆé˜²æ­¢ä¿¡æ¯æ³„éœ²ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§ç”Ÿå‘½å‘¨æœŸä¸­å¿ƒçš„æ¡†æ¶ï¼Œåˆ†æè¯­ä¹‰éšç§é£é™©åœ¨ä¸åŒé˜¶æ®µçš„è¡¨ç°ï¼Œæ—¨åœ¨ç³»ç»Ÿæ€§åœ°è¯†åˆ«å’Œè¯„ä¼°è¿™äº›é£é™©ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬è¾“å…¥å¤„ç†ã€é¢„è®­ç»ƒã€å¾®è°ƒå’Œå¯¹é½å››ä¸ªä¸»è¦é˜¶æ®µï¼Œé’ˆå¯¹æ¯ä¸ªé˜¶æ®µåˆ†ææ½œåœ¨çš„éšç§é£é™©å’Œæ”»å‡»å‘é‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºç³»ç»ŸåŒ–åœ°è¯†åˆ«è¯­ä¹‰éšç§é£é™©ï¼Œå¹¶è¯„ä¼°ç°æœ‰é˜²å¾¡æªæ–½çš„æœ‰æ•ˆæ€§ï¼Œå¡«è¡¥äº†å½“å‰ç ”ç©¶çš„ç©ºç™½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œè€ƒè™‘äº†å·®åˆ†éšç§ã€åµŒå…¥åŠ å¯†ç­‰å¤šç§é˜²å¾¡æŠ€æœ¯ï¼Œå¹¶åˆ†æå…¶åœ¨ä¸åŒé˜¶æ®µçš„é€‚ç”¨æ€§å’Œæ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åˆ†æç»“æœè¡¨æ˜ï¼Œç°æœ‰çš„é˜²å¾¡æªæ–½åœ¨é¢å¯¹ä¸Šä¸‹æ–‡æ¨æ–­å’Œæ½œåœ¨è¡¨ç¤ºæ³„éœ²æ—¶å­˜åœ¨æ˜¾è‘—çš„ä¿æŠ¤ç¼ºå£ï¼Œå°¤å…¶æ˜¯åœ¨å¤šæ¨¡æ€è¾“å…¥çš„æƒ…å†µä¸‹ï¼Œæœªæ¥çš„ç ”ç©¶éœ€è¦é’ˆå¯¹è¿™äº›æŒ‘æˆ˜è¿›è¡Œæ·±å…¥æ¢ç´¢ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒ»ç–—ã€é‡‘èå’Œæ³•å¾‹ç­‰æ•æ„Ÿè¡Œä¸šï¼Œèƒ½å¤Ÿä¸ºè¿™äº›é¢†åŸŸä¸­çš„å¤§å‹è¯­è¨€æ¨¡å‹æä¾›æ›´ä¸ºæœ‰æ•ˆçš„éšç§ä¿æŠ¤æ–¹æ¡ˆã€‚é€šè¿‡æå‡è¯­ä¹‰éšç§ä¿æŠ¤ï¼Œèƒ½å¤Ÿå¢å¼ºç”¨æˆ·ä¿¡ä»»ï¼Œä¿ƒè¿›æŠ€æœ¯çš„å¹¿æ³›åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As Large Language Models (LLMs) are increasingly deployed in sensitive domains, traditional data privacy measures prove inadequate for protecting information that is implicit, contextual, or inferable - what we define as semantic privacy. This Systematization of Knowledge (SoK) introduces a lifecycle-centric framework to analyze how semantic privacy risks emerge across input processing, pretraining, fine-tuning, and alignment stages of LLMs. We categorize key attack vectors and assess how current defenses, such as differential privacy, embedding encryption, edge computing, and unlearning, address these threats. Our analysis reveals critical gaps in semantic-level protection, especially against contextual inference and latent representation leakage. We conclude by outlining open challenges, including quantifying semantic leakage, protecting multimodal inputs, balancing de-identification with generation quality, and ensuring transparency in privacy enforcement. This work aims to inform future research on designing robust, semantically aware privacy-preserving techniques for LLMs.

