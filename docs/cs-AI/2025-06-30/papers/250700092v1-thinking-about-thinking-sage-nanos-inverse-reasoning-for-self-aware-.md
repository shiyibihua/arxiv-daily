---
layout: default
title: Thinking About Thinking: SAGE-nano's Inverse Reasoning for Self-Aware Language Models
---

# Thinking About Thinking: SAGE-nano's Inverse Reasoning for Self-Aware Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2507.00092" class="toolbar-btn" target="_blank">üìÑ arXiv: 2507.00092v1</a>
  <a href="https://arxiv.org/pdf/2507.00092.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2507.00092v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2507.00092v1', 'Thinking About Thinking: SAGE-nano\'s Inverse Reasoning for Self-Aware Language Models')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Basab Jha, Firoj Paudel, Ujjwal Puri, Zhang Yuting, Choi Donghyuk, Wang Junhao

**ÂàÜÁ±ª**: cs.AI, cs.CL, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-30

**Â§áÊ≥®**: 19 pages, 2 figures, 9 tables

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫ÈÄÜÂêëÊé®ÁêÜÊ°ÜÊû∂‰ª•ÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÁöÑËá™ÊàëËß£ÈáäËÉΩÂäõ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÈÄÜÂêëÊé®ÁêÜ` `Ëá™ÊàëËß£Èáä` `ËØ≠Ë®ÄÊ®°Âûã` `Êé®ÁêÜÈÄèÊòéÂ∫¶` `ÂÖÉËÆ§Áü•ÁªìÊûÑ` `‰∫∫Â∑•Êô∫ËÉΩÂÆâÂÖ®` `ÁßëÂ≠¶ÂèëÁé∞`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÊé®ÁêÜÊñπÊ≥ïÂæÄÂæÄÁº∫‰πèÈÄèÊòéÂ∫¶ÔºåÂØºËá¥ËØ≠Ë®ÄÊ®°ÂûãÁöÑÂÜ≥Á≠ñËøáÁ®ãÈöæ‰ª•ÁêÜËß£„ÄÇ
2. Êú¨ÊñáÊèêÂá∫ÈÄÜÂêëÊé®ÁêÜÊ°ÜÊû∂Ôºå‰ΩøËØ≠Ë®ÄÊ®°ÂûãËÉΩÂ§üÂèçÊÄùÂíåËß£ÈáäÂÖ∂Êé®ÁêÜËøáÁ®ãÔºåÊèêÂçáÂèØËß£ÈáäÊÄß„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåSAGE-nanoÂú®Êé®ÁêÜÂáÜÁ°ÆÊÄßÂíåËß£ÈáäË¥®Èáè‰∏äÂùá‰ºò‰∫éÁé∞ÊúâÊ®°ÂûãÔºåÂÖ∑ÊúâÊòæËëóÊèêÂçá„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Â§çÊùÇÊé®ÁêÜ‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂÖ∂ÂÜ≥Á≠ñËøáÁ®ã‰ªçÁÑ∂ËæÉ‰∏∫ÈªëÁÆ±Âåñ„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫ÈÄÜÂêëÊé®ÁêÜÁöÑÊñ∞ËåÉÂºèÔºå‰ΩøLLMsËÉΩÂ§ü‰∫ãÂêéÂàÜËß£Âπ∂Ëß£ÈáäÂÖ∂Êé®ÁêÜÈìæ„ÄÇÊàë‰ª¨Âú®SAGE-nanoÊ®°Âûã‰∏≠Â∫îÁî®‰∫ÜËøô‰∏ÄÊñπÊ≥ïÔºåËØ•Ê®°ÂûãÊã•Êúâ40‰∫øÂèÇÊï∞ÔºåÈááÁî®‰∫ÜÂÖÉËÆ§Áü•ÁªìÊûÑÔºåÈÄöËøáÊ≥®ÊÑèÂäõÊú∫Âà∂ÂèçÊÄù‰∏ªË¶ÅÂÜ≥Á≠ñÁÇπÂπ∂ÁîüÊàêÊé®ÁêÜÈÄâÊã©ÁöÑËß£Èáä„ÄÇÈÄöËøáÂØπÈÄªËæëÊé®ÁêÜÈöæÈ¢ò„ÄÅÊï∞Â≠¶ÈóÆÈ¢òÂíå‰º¶ÁêÜÂõ∞Â¢ÉÁöÑÂÖ®Èù¢ÊµãËØïÔºåSAGE-nanoÂú®Êé®ÁêÜÂáÜÁ°ÆÊÄßÂíåËß£ÈáäË¥®Èáè‰∏äÂùáË°®Áé∞‰ºòÂºÇÔºåÊé•Ëøë‰∫éClaude-3.5 SonnetÂíåGPT-4oÁ≠âÊ®°Âûã„ÄÇÊàë‰ª¨ÁöÑË¥°ÁåÆÂåÖÊã¨ÔºöÈ¶ñ‰∏™ÈíàÂØπLLMËá™ÊàëÂèçÊÄùÁöÑÈÄÜÂêëÊé®ÁêÜÊ°ÜÊû∂„ÄÅÂèçÂêëÊ≥®ÊÑèÂäõÊµÅÁöÑÂÖÉÂ≠¶‰π†Ê°ÜÊû∂„ÄÅÊé®ÁêÜÈÄèÊòéÂ∫¶ÁöÑÁªºÂêàËØÑ‰º∞Ê°ÜÊû∂Ôºå‰ª•ÂèäËØÅÊçÆË°®ÊòéÈÄÜÂêëÊé®ÁêÜÁöÑÂ¢ûÂä†ÊèêÂçá‰∫ÜÂèØËß£ÈáäÊÄßÂíåÊé®ÁêÜÊÄßËÉΩ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®Êé®ÁêÜËøáÁ®ã‰∏≠Áº∫‰πèÈÄèÊòéÊÄßÁöÑÈóÆÈ¢òÔºåÁé∞ÊúâÁöÑÊé®ÁêÜÊñπÊ≥ïÂæÄÂæÄÊó†Ê≥ïÊ∏ÖÊô∞Ëß£ÈáäÊ®°ÂûãÁöÑÂÜ≥Á≠ñËøáÁ®ã„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊèêÂá∫ÈÄÜÂêëÊé®ÁêÜÁöÑÊ¶ÇÂøµÔºå‰ΩøÊ®°ÂûãËÉΩÂ§ü‰∫ãÂêéÂàÜÊûêÂÖ∂Êé®ÁêÜÈìæÔºåËØÜÂà´ÂÖ≥ÈîÆÂÜ≥Á≠ñÁÇπÂπ∂ÁîüÊàêËß£ÈáäÔºå‰ªéËÄåÊèêÈ´òÂèØËß£ÈáäÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöSAGE-nanoÊ®°ÂûãÈááÁî®‰∫Ü‰∏Ä‰∏™ÂÖÉËÆ§Áü•ÁªìÊûÑÔºåÈÄöËøáÊ≥®ÊÑèÂäõÊú∫Âà∂ÂèçÂêëÊµÅÂä®ÔºåËØÜÂà´‰∏ªË¶ÅÂÜ≥Á≠ñÁÇπ„ÄÇÊï¥‰ΩìÊµÅÁ®ãÂåÖÊã¨ËæìÂÖ•ÊñáÊú¨„ÄÅÊé®ÁêÜÈìæÁîüÊàê„ÄÅÈÄÜÂêëÂàÜÊûêÂíåËß£ÈáäÁîüÊàêÂõõ‰∏™‰∏ªË¶ÅÊ®°Âùó„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÈ¶ñÊ¨°Âª∫Á´ã‰∫ÜLLMËá™ÊàëÂèçÊÄùÁöÑÈÄÜÂêëÊé®ÁêÜÊ°ÜÊû∂ÔºåÊòæËëóÂå∫Âà´‰∫é‰º†ÁªüÁöÑÂâçÂêëÊé®ÁêÜÊñπÊ≥ïÔºåÊèê‰æõ‰∫ÜÂØπÊé®ÁêÜËøáÁ®ãÁöÑÊ∑±ÂàªÊ¥ûÂØü„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Ê®°ÂûãËÆæËÆ°‰∏≠ÔºåÈááÁî®‰∫ÜÁâπÂÆöÁöÑÂèÇÊï∞ËÆæÁΩÆÂíåÊçüÂ§±ÂáΩÊï∞Ôºå‰ª•‰ºòÂåñÈÄÜÂêëÊé®ÁêÜÁöÑÊïàÊûúÔºåÂêåÊó∂Á°Æ‰øùÁîüÊàêÁöÑËß£ÈáäÂÖ∑ÊúâÈ´òË¥®ÈáèÂíåÂèØËØªÊÄß„ÄÇÂÖ∑‰ΩìÁöÑÁΩëÁªúÁªìÊûÑÂíåÊ≥®ÊÑèÂäõÊú∫Âà∂ËÆæËÆ°‰πüÁªèËøáÁ≤æÂøÉË∞ÉÊï¥Ôºå‰ª•ÊîØÊåÅÂèçÂêëÊé®ÁêÜÁöÑÈúÄÊ±Ç„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåSAGE-nanoÂú®AQUA-RATÊï∞ÊçÆÈõÜ‰∏äÁöÑÊé®ÁêÜÂáÜÁ°ÆÁéáËææÂà∞74.6%ÔºåËÄåËß£ÈáäË¥®ÈáèÁöÑ‰∫∫Â∑•ÂÅèÂ•ΩËØÑÂàÜÈ´òËææ92.1%„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåSAGE-nanoÂú®Êé®ÁêÜÊÄßËÉΩÂíåËß£ÈáäËÉΩÂäõ‰∏äÂùáÊé•Ëøë‰∫éClaude-3.5 SonnetÂíåGPT-4oÁ≠âÂÖàËøõÊ®°ÂûãÔºåÂ±ïÁ§∫‰∫ÜÊòæËëóÁöÑÊèêÂçá„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨ÊïôËÇ≤„ÄÅAIÂÆâÂÖ®ÂíåÁßëÂ≠¶ÂèëÁé∞Á≠â„ÄÇÈÄöËøáÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÁöÑÂèØËß£ÈáäÊÄßÔºåÁ†îÁ©∂ÂèØ‰ª•Â∏ÆÂä©Áî®Êà∑Êõ¥Â•ΩÂú∞ÁêÜËß£Ê®°ÂûãÁöÑÂÜ≥Á≠ñËøáÁ®ãÔºå‰ªéËÄåÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠Â¢ûÂº∫‰ø°‰ªªÂ∫¶ÂíåÂÆâÂÖ®ÊÄß„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Large Language Models (LLMs) have demonstrated remarkable capabilities at solving complex reasoning tasks with Chain-of-Thought (CoT) prompting, but their decision-making processes remain somewhat blackbox. We introduce textbfinverse reasoning, a novel paradigm enabling LLMs to decompose and explain their own reasoning chains post-hoc. Our approach, used in SAGE-nano, a 4-billion-parameter reasoning model, employs a metacognitive structure that reflects back via attention processes to identify major decision points and generate explanations of reasoning choices. While typical CoT approaches are directed towards forward reasoning generation, inverse reasoning provides insight into why specific reasoning chains were selected over others. Through thorough testing of logical reasoning puzzles, math problems and ethical dilemmas from AQUA-RAT, CommonsenseQA, and customized benchmarks, we demonstrate that SAGE-nano is at the cutting edge both on reasoning accuracy (74.6% on AQUA-RAT) and explanation quality (92.1% human preference score) for its task, and offers performance almost on par with models like Claude-3.5 Sonnet or GPT-4o. Our contributions are: (i) the first rigorous framework for LLM self-reflection via inverse reasoning, (ii) a novel metalearning framework to reverse the attention flow, (iii) comprehensive evaluation frameworks for reasoning transparency, and (iv) evidence that increasing reasoning using inverse reasoning improves interpretability along with reasoning performance. Our work creates new avenues for transparent AI systems and closes significant gaps in AI safety, education, and scientific discovery.

