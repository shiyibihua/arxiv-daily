---
layout: default
title: Scaling Self-Supervised Representation Learning for Symbolic Piano Performance
---

# Scaling Self-Supervised Representation Learning for Symbolic Piano Performance

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.23869" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.23869v1</a>
  <a href="https://arxiv.org/pdf/2506.23869.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.23869v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.23869v1', 'Scaling Self-Supervised Representation Learning for Symbolic Piano Performance')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Louis Bradshaw, Honglu Fan, Alexander Spangher, Stella Biderman, Simon Colton

**åˆ†ç±»**: cs.SD, cs.AI, cs.LG, eess.AS

**å‘å¸ƒæ—¥æœŸ**: 2025-06-30

**å¤‡æ³¨**: ISMIR (2025)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªç›‘ç£è¡¨ç¤ºå­¦ä¹ æ–¹æ³•ä»¥æå‡ç¬¦å·é’¢ç´è¡¨æ¼”ç”Ÿæˆèƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è‡ªç›‘ç£å­¦ä¹ ` `ç¬¦å·éŸ³ä¹ç”Ÿæˆ` `ç”Ÿæˆæ¨¡å‹` `å¯¹æ¯”å­¦ä¹ ` `é’¢ç´è¡¨æ¼”`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç¬¦å·éŸ³ä¹ç”Ÿæˆæ–¹æ³•åœ¨ç”Ÿæˆä¸€è‡´æ€§å’Œå¤šæ ·æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥æ»¡è¶³å®é™…åº”ç”¨éœ€æ±‚ã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºç”Ÿæˆè‡ªå›å½’å˜æ¢å™¨çš„è‡ªç›‘ç£å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡å¤§è§„æ¨¡é¢„è®­ç»ƒå’Œé«˜è´¨é‡å¾®è°ƒæ¥æå‡æ¨¡å‹æ€§èƒ½ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ¨¡å‹åœ¨é’¢ç´å»¶ç»­ç”Ÿæˆå’ŒMIRåˆ†ç±»ä»»åŠ¡ä¸­å‡å–å¾—äº†é¢†å…ˆçš„æ€§èƒ½ï¼Œå±•ç¤ºäº†è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶æ¢è®¨äº†åœ¨å¤§é‡ç¬¦å·é’¢ç´ä¹è°±ä¸Šè®­ç»ƒçš„ç”Ÿæˆè‡ªå›å½’å˜æ¢å™¨æ¨¡å‹çš„èƒ½åŠ›ã€‚é¦–å…ˆåœ¨çº¦60,000å°æ—¶çš„éŸ³ä¹ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶ååˆ©ç”¨è¾ƒå°çš„é«˜è´¨é‡å­é›†å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥ç”ŸæˆéŸ³ä¹å»¶ç»­ã€æ‰§è¡Œç¬¦å·åˆ†ç±»ä»»åŠ¡ï¼Œå¹¶é€šè¿‡è°ƒæ•´SimCLRæ¡†æ¶ä¸ºç¬¦å·éŸ³ä¹ç”Ÿæˆé€šç”¨å¯¹æ¯”MIDIåµŒå…¥ã€‚åœ¨è¯„ä¼°é’¢ç´å»¶ç»­ä¸€è‡´æ€§æ—¶ï¼Œæˆ‘ä»¬çš„ç”Ÿæˆæ¨¡å‹è¶…è¶Šäº†é¢†å…ˆçš„ç¬¦å·ç”ŸæˆæŠ€æœ¯ï¼Œå¹¶ä¸ä¸“æœ‰éŸ³é¢‘ç”Ÿæˆæ¨¡å‹ä¿æŒç«äº‰åŠ›ã€‚åœ¨MIRåˆ†ç±»åŸºå‡†æµ‹è¯•ä¸­ï¼Œæ¥è‡ªå¯¹æ¯”æ¨¡å‹çš„å†»ç»“è¡¨ç¤ºåœ¨çº¿æ€§æ¢æµ‹å®éªŒä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œè€Œç›´æ¥å¾®è°ƒåˆ™å±•ç¤ºäº†é¢„è®­ç»ƒè¡¨ç¤ºçš„å¯æ³›åŒ–æ€§ï¼Œé€šå¸¸åªéœ€å‡ ç™¾ä¸ªæ ‡è®°æ ·æœ¬å³å¯ä¸“é—¨åŒ–åˆ°ä¸‹æ¸¸ä»»åŠ¡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³ç°æœ‰ç¬¦å·éŸ³ä¹ç”ŸæˆæŠ€æœ¯åœ¨ä¸€è‡´æ€§å’Œå¤šæ ·æ€§æ–¹é¢çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨é’¢ç´è¡¨æ¼”çš„ç”Ÿæˆä»»åŠ¡ä¸­ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºæœ‰é™çš„æ ‡è®°æ•°æ®ï¼Œå¯¼è‡´ç”Ÿæˆç»“æœçš„è´¨é‡å’Œå¤šæ ·æ€§å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤§è§„æ¨¡çš„ç¬¦å·é’¢ç´ä¹è°±è¿›è¡Œè‡ªç›‘ç£å­¦ä¹ ï¼Œé€šè¿‡é¢„è®­ç»ƒå’Œå¾®è°ƒç›¸ç»“åˆçš„æ–¹æ³•ï¼Œæå‡æ¨¡å‹åœ¨ç”Ÿæˆå’Œåˆ†ç±»ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚é€šè¿‡å¼•å…¥å¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼Œå¢å¼ºæ¨¡å‹å¯¹éŸ³ä¹ç‰¹å¾çš„ç†è§£å’Œè¡¨ç¤ºèƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆæ˜¯å¯¹çº¦60,000å°æ—¶éŸ³ä¹æ•°æ®çš„é¢„è®­ç»ƒï¼Œæ¥ç€æ˜¯ä½¿ç”¨é«˜è´¨é‡å­é›†è¿›è¡Œå¾®è°ƒã€‚æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹åŒ…æ‹¬ç”ŸæˆéŸ³ä¹å»¶ç»­ã€ç¬¦å·åˆ†ç±»å’Œå¯¹æ¯”åµŒå…¥çš„å­¦ä¹ ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„å…³é”®åˆ›æ–°åœ¨äºå°†SimCLRæ¡†æ¶é€‚é…äºç¬¦å·éŸ³ä¹ï¼Œç”Ÿæˆé€šç”¨çš„å¯¹æ¯”MIDIåµŒå…¥ï¼Œå¹¶åœ¨é’¢ç´å»¶ç»­ç”Ÿæˆä»»åŠ¡ä¸­è¶…è¶Šäº†ä¼ ç»Ÿçš„ç¬¦å·ç”ŸæˆæŠ€æœ¯ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæ¨¡å‹åœ¨ç”Ÿæˆä¸€è‡´æ€§å’Œå¤šæ ·æ€§æ–¹é¢è¡¨ç°æ›´ä½³ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†è‡ªå›å½’å˜æ¢å™¨æ¶æ„ï¼Œç»“åˆäº†é€‚å½“çš„æŸå¤±å‡½æ•°å’Œå‚æ•°è®¾ç½®ï¼Œä»¥ä¼˜åŒ–ç”Ÿæˆè´¨é‡å’Œåˆ†ç±»æ€§èƒ½ã€‚å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹ä»…éœ€å°‘é‡æ ‡è®°æ ·æœ¬å³å¯å®ç°è‰¯å¥½çš„ä¸‹æ¸¸ä»»åŠ¡é€‚åº”æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æç”Ÿæˆæ¨¡å‹åœ¨é’¢ç´å»¶ç»­ä¸€è‡´æ€§è¯„ä¼°ä¸­è¶…è¶Šäº†é¢†å…ˆçš„ç¬¦å·ç”ŸæˆæŠ€æœ¯ï¼Œå¹¶ä¸ä¸“æœ‰éŸ³é¢‘ç”Ÿæˆæ¨¡å‹ä¿æŒç«äº‰åŠ›ã€‚åœ¨MIRåˆ†ç±»åŸºå‡†æµ‹è¯•ä¸­ï¼Œå†»ç»“è¡¨ç¤ºåœ¨çº¿æ€§æ¢æµ‹å®éªŒä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œå±•ç¤ºäº†æ¨¡å‹çš„ä¼˜è¶Šæ€§èƒ½å’Œè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬éŸ³ä¹åˆ›ä½œã€è‡ªåŠ¨ä¼´å¥ç”Ÿæˆå’ŒéŸ³ä¹æ•™è‚²ç­‰ã€‚é€šè¿‡æå‡ç¬¦å·é’¢ç´è¡¨æ¼”çš„ç”Ÿæˆèƒ½åŠ›ï¼Œèƒ½å¤Ÿä¸ºéŸ³ä¹åˆ›ä½œè€…æä¾›æ›´é«˜æ•ˆçš„å·¥å…·ï¼ŒåŒæ—¶ä¹Ÿä¸ºéŸ³ä¹å­¦ä¹ è€…æä¾›ä¸ªæ€§åŒ–çš„å­¦ä¹ ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åœ¨éŸ³ä¹ç”Ÿæˆå’Œåˆ†æé¢†åŸŸäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We study the capabilities of generative autoregressive transformer models trained on large amounts of symbolic solo-piano transcriptions. After first pretraining on approximately 60,000 hours of music, we use a comparatively smaller, high-quality subset, to finetune models to produce musical continuations, perform symbolic classification tasks, and produce general-purpose contrastive MIDI embeddings by adapting the SimCLR framework to symbolic music. When evaluating piano continuation coherence, our generative model outperforms leading symbolic generation techniques and remains competitive with proprietary audio generation models. On MIR classification benchmarks, frozen representations from our contrastive model achieve state-of-the-art results in linear probe experiments, while direct finetuning demonstrates the generalizability of pretrained representations, often requiring only a few hundred labeled examples to specialize to downstream tasks.

