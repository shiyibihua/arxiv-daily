---
layout: default
title: Software Engineering for Large Language Models: Research Status, Challenges and the Road Ahead
---

# Software Engineering for Large Language Models: Research Status, Challenges and the Road Ahead

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.23762" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.23762v1</a>
  <a href="https://arxiv.org/pdf/2506.23762.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.23762v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.23762v1', 'Software Engineering for Large Language Models: Research Status, Challenges and the Road Ahead')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hongzhou Rao, Yanjie Zhao, Xinyi Hou, Shenao Wang, Haoyu Wang

**åˆ†ç±»**: cs.SE, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç³»ç»Ÿåˆ†æå¤§è¯­è¨€æ¨¡å‹å¼€å‘ç”Ÿå‘½å‘¨æœŸä¸­çš„è½¯ä»¶å·¥ç¨‹æŒ‘æˆ˜ä¸è§£å†³æ–¹æ¡ˆ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `è½¯ä»¶å·¥ç¨‹` `éœ€æ±‚å·¥ç¨‹` `æ¨¡å‹å¼€å‘` `æµ‹è¯•ä¸è¯„ä¼°` `æ•°æ®é›†æ„å»º` `è¿ç»´ç®¡ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç ”ç©¶æœªç³»ç»Ÿæ¢è®¨å¤§è¯­è¨€æ¨¡å‹å¼€å‘ç”Ÿå‘½å‘¨æœŸä¸­çš„è½¯ä»¶å·¥ç¨‹æŒ‘æˆ˜ï¼Œå¯¼è‡´å¼€å‘è¿‡ç¨‹ä¸­çš„å¤æ‚æ€§æœªå¾—åˆ°æœ‰æ•ˆè§£å†³ã€‚
2. æœ¬æ–‡é€šè¿‡åˆ†æLLMå¼€å‘çš„å…­ä¸ªé˜¶æ®µï¼Œæå‡ºäº†é’ˆå¯¹æ¯ä¸ªé˜¶æ®µçš„å…³é”®æŒ‘æˆ˜åŠç›¸åº”çš„ç ”ç©¶æ–¹å‘ï¼Œå¡«è¡¥äº†è¿™ä¸€é¢†åŸŸçš„ç ”ç©¶ç©ºç™½ã€‚
3. ç ”ç©¶æä¾›äº†ä»è½¯ä»¶å·¥ç¨‹è§†è§’çš„è§è§£ï¼Œæ—¨åœ¨ä¸ºæœªæ¥LLMçš„å‘å±•æä¾›æŒ‡å¯¼ï¼Œæ¨åŠ¨æŠ€æœ¯çš„è¿›æ­¥ä¸åº”ç”¨ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•é‡æ–°å®šä¹‰äº†äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰ï¼Œæ¨åŠ¨äº†AIç ”ç©¶çš„è¾¹ç•Œï¼Œå¹¶ä¸ºå­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œå¸¦æ¥äº†æ— é™å¯èƒ½ã€‚ç„¶è€Œï¼ŒLLMå¼€å‘åœ¨å…¶ç”Ÿå‘½å‘¨æœŸä¸­é¢ä¸´è¶Šæ¥è¶Šå¤æ‚çš„æŒ‘æˆ˜ï¼Œç°æœ‰ç ”ç©¶å°šæœªä»è½¯ä»¶å·¥ç¨‹ï¼ˆSEï¼‰è§’åº¦ç³»ç»Ÿæ¢è®¨è¿™äº›æŒ‘æˆ˜åŠè§£å†³æ–¹æ¡ˆã€‚ä¸ºå¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæœ¬æ–‡ç³»ç»Ÿåˆ†æäº†LLMå¼€å‘ç”Ÿå‘½å‘¨æœŸçš„ç ”ç©¶ç°çŠ¶ï¼Œåˆ†ä¸ºå…­ä¸ªé˜¶æ®µï¼šéœ€æ±‚å·¥ç¨‹ã€æ•°æ®é›†æ„å»ºã€æ¨¡å‹å¼€å‘ä¸å¢å¼ºã€æµ‹è¯•ä¸è¯„ä¼°ã€éƒ¨ç½²ä¸è¿ç»´ã€ç»´æŠ¤ä¸æ¼”åŒ–ã€‚æœ€åï¼Œæœ¬æ–‡è¯†åˆ«äº†æ¯ä¸ªé˜¶æ®µçš„å…³é”®æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜çš„æ½œåœ¨ç ”ç©¶æ–¹å‘ã€‚æ€»ä½“è€Œè¨€ï¼Œæœ¬æ–‡ä»SEçš„è§’åº¦æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ï¼Œä»¥ä¿ƒè¿›LLMå¼€å‘çš„æœªæ¥è¿›å±•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹å¼€å‘ç”Ÿå‘½å‘¨æœŸä¸­é¢ä¸´çš„å¤æ‚æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•ç¼ºä¹ç³»ç»Ÿæ€§åˆ†æï¼Œå¯¼è‡´å¼€å‘æ•ˆç‡ä½ä¸‹å’Œè´¨é‡ä¸ç¨³å®šã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å°†LLMå¼€å‘è¿‡ç¨‹åˆ’åˆ†ä¸ºå…­ä¸ªé˜¶æ®µï¼Œç³»ç»Ÿåˆ†ææ¯ä¸ªé˜¶æ®µçš„æŒ‘æˆ˜ï¼Œå¹¶æå‡ºé’ˆå¯¹æ€§çš„è§£å†³æ–¹æ¡ˆï¼Œæ—¨åœ¨æå‡å¼€å‘æ•ˆç‡å’Œæ¨¡å‹è´¨é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬éœ€æ±‚å·¥ç¨‹ã€æ•°æ®é›†æ„å»ºã€æ¨¡å‹å¼€å‘ä¸å¢å¼ºã€æµ‹è¯•ä¸è¯„ä¼°ã€éƒ¨ç½²ä¸è¿ç»´ã€ç»´æŠ¤ä¸æ¼”åŒ–å…­ä¸ªä¸»è¦æ¨¡å—ï¼Œå½¢æˆé—­ç¯çš„å¼€å‘æµç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„åˆ›æ–°ç‚¹åœ¨äºé¦–æ¬¡ä»è½¯ä»¶å·¥ç¨‹çš„è§†è§’ç³»ç»Ÿæ€§åœ°åˆ†æLLMå¼€å‘çš„å„ä¸ªé˜¶æ®µï¼Œè¯†åˆ«å‡ºå…·ä½“çš„æŒ‘æˆ˜å¹¶æå‡ºç›¸åº”çš„ç ”ç©¶æ–¹å‘ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæ›´åŠ å…¨é¢å’Œç³»ç»Ÿã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¯ä¸ªé˜¶æ®µï¼Œè®ºæ–‡å¼ºè°ƒäº†éœ€æ±‚åˆ†æã€æ•°æ®è´¨é‡æ§åˆ¶ã€æ¨¡å‹è¯„ä¼°æ ‡å‡†ç­‰å…³é”®è®¾è®¡ï¼Œç¡®ä¿æ¯ä¸ªç¯èŠ‚çš„é«˜æ•ˆæ€§å’Œæœ‰æ•ˆæ€§ã€‚å…·ä½“å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°çš„é€‰æ‹©ä¹Ÿåœ¨æ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†è®¨è®ºã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡ç³»ç»Ÿåˆ†æLLMå¼€å‘çš„å…­ä¸ªé˜¶æ®µï¼Œèƒ½å¤Ÿæ˜¾è‘—æé«˜å¼€å‘æ•ˆç‡å’Œæ¨¡å‹è´¨é‡ã€‚å…·ä½“è€Œè¨€ï¼Œé’ˆå¯¹æ¯ä¸ªé˜¶æ®µçš„æŒ‘æˆ˜æå‡ºçš„è§£å†³æ–¹æ¡ˆï¼Œé¢„è®¡èƒ½æå‡æ¨¡å‹çš„å‡†ç¡®æ€§å’Œç¨³å®šæ€§ï¼Œå…·ä½“æ€§èƒ½æ•°æ®å¾…è¿›ä¸€æ­¥éªŒè¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿã€æ™ºèƒ½å®¢æœç­‰ï¼Œèƒ½å¤Ÿä¸ºå¤§è¯­è¨€æ¨¡å‹çš„å¼€å‘æä¾›ç³»ç»ŸåŒ–çš„æŒ‡å¯¼ï¼Œæå‡æ¨¡å‹çš„å¼€å‘æ•ˆç‡å’Œåº”ç”¨æ•ˆæœã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶å¯èƒ½å¯¹AIé¢†åŸŸçš„æ ‡å‡†åŒ–å’Œè§„èŒƒåŒ–å‘å±•äº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The rapid advancement of large language models (LLMs) has redefined artificial intelligence (AI), pushing the boundaries of AI research and enabling unbounded possibilities for both academia and the industry. However, LLM development faces increasingly complex challenges throughout its lifecycle, yet no existing research systematically explores these challenges and solutions from the perspective of software engineering (SE) approaches. To fill the gap, we systematically analyze research status throughout the LLM development lifecycle, divided into six phases: requirements engineering, dataset construction, model development and enhancement, testing and evaluation, deployment and operations, and maintenance and evolution. We then conclude by identifying the key challenges for each phase and presenting potential research directions to address these challenges. In general, we provide valuable insights from an SE perspective to facilitate future advances in LLM development.

