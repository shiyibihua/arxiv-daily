---
layout: default
title: How large language models judge and influence human cooperation
---

# How large language models judge and influence human cooperation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2507.00088" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2507.00088v1</a>
  <a href="https://arxiv.org/pdf/2507.00088.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2507.00088v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2507.00088v1', 'How large language models judge and influence human cooperation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Alexandre S. Pires, Laurens Samson, Sennay Ghebreab, Fernando P. Santos

**åˆ†ç±»**: physics.soc-ph, cs.AI, cs.SI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹å¯¹äººç±»åˆä½œçš„å½±å“ä¸åˆ¤æ–­**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `äººç±»åˆä½œ` `ç¤¾ä¼šå†³ç­–` `é“å¾·åˆ¤æ–­` `è¿›åŒ–åšå¼ˆ` `åˆä½œåŠ¨æ€` `å£°èª‰å½±å“`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å½“å‰å¯¹å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç¤¾ä¼šå†³ç­–ä¸­çš„é•¿æœŸå½±å“å°šä¸æ˜ç¡®ï¼Œå°¤å…¶æ˜¯å¦‚ä½•å½±å“äººç±»åˆä½œè¡Œä¸ºã€‚
2. æœ¬æ–‡é€šè¿‡æä¾›å¤šç§ç¤¾äº¤æƒ…å¢ƒç¤ºä¾‹ï¼Œè¯„ä¼°LLMså¯¹åˆä½œè¡Œä¸ºçš„åˆ¤æ–­ï¼Œå¹¶ç»“åˆè¿›åŒ–åšå¼ˆç†è®ºåˆ†æåˆä½œåŠ¨æ€ã€‚
3. ç ”ç©¶å‘ç°ï¼ŒLLMsåœ¨è¯„ä¼°è‰¯å¥½å¯¹æ‰‹æ—¶è¡¨ç°å‡ºä¸€è‡´æ€§ï¼Œä½†åœ¨å£°èª‰ä¸ä½³çš„ä¸ªä½“ä¸Šå­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œè¿™å¯èƒ½å½±å“åˆä½œçš„æ™®éæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

äººç±»æ—¥ç›Šä¾èµ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç¤¾ä¼šæƒ…å¢ƒä¸­æ”¯æŒå†³ç­–ã€‚å…ˆå‰ç ”ç©¶è¡¨æ˜ï¼Œè¿™äº›å·¥å…·ä¼šå½±å“äººä»¬çš„é“å¾·å’Œæ”¿æ²»åˆ¤æ–­ã€‚ç„¶è€Œï¼ŒåŸºäºLLMçš„ç¤¾ä¼šå†³ç­–çš„é•¿æœŸå½±å“ä»ç„¶æœªçŸ¥ã€‚æœ¬æ–‡è¯„ä¼°äº†æœ€å…ˆè¿›çš„LLMså¦‚ä½•åˆ¤æ–­åˆä½œè¡Œä¸ºï¼Œæä¾›äº†21ç§ä¸åŒçš„LLMsä¸ä¸€ç³»åˆ—åˆä½œä¸æ‹’ç»åˆä½œçš„ç¤¾äº¤æƒ…å¢ƒç¤ºä¾‹ï¼Œå¹¶æ¢è®¨äº†è¿™äº›åˆ¤æ–­å¯¹äººç±»åˆä½œçš„é•¿æœŸå½±å“ã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨è¯„ä¼°ä¸è‰¯å¥½å¯¹æ‰‹çš„åˆä½œæ—¶ï¼Œæ¨¡å‹ä¹‹é—´çš„åˆ¤æ–­ä¸€è‡´æ€§æ˜¾è‘—ï¼Œä½†åœ¨ä¸å£°èª‰ä¸ä½³çš„ä¸ªä½“åˆä½œæ—¶ï¼Œæ¨¡å‹ä¹‹é—´å­˜åœ¨è¾ƒå¤§å·®å¼‚ã€‚æœ€åï¼Œé€šè¿‡ç›®æ ‡å¯¼å‘çš„æç¤ºï¼Œç ”ç©¶å±•ç¤ºäº†å¦‚ä½•å¼•å¯¼LLMçš„åˆ¤æ–­ï¼Œä»¥ç»´æŠ¤äººç±»åˆä½œã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹å¦‚ä½•å½±å“äººç±»åˆä½œåˆ¤æ–­çš„é—®é¢˜ã€‚ç°æœ‰ç ”ç©¶æœªèƒ½å……åˆ†æ¢è®¨LLMåœ¨ç¤¾ä¼šå†³ç­–ä¸­çš„é•¿æœŸå½±å“ï¼Œå°¤å…¶æ˜¯å¯¹äººç±»åˆä½œçš„æ½œåœ¨å½±å“ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æä¾›å¤šç§ç¤¾äº¤æƒ…å¢ƒä¸‹çš„åˆä½œä¸æ‹’ç»åˆä½œç¤ºä¾‹ï¼Œè¯„ä¼°LLMsçš„åˆ¤æ–­ï¼Œå¹¶åˆ©ç”¨è¿›åŒ–åšå¼ˆç†è®ºåˆ†æè¿™äº›åˆ¤æ–­å¯¹åˆä½œåŠ¨æ€çš„å½±å“ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é¦–å…ˆæ”¶é›†äº†21ç§ä¸åŒçš„LLMsï¼Œå¹¶ä¸ºæ¯ä¸ªæ¨¡å‹æä¾›äº†ä¸°å¯Œçš„ç¤¾äº¤äº’åŠ¨ç¤ºä¾‹ã€‚ç„¶åï¼Œé€šè¿‡åˆ†ææ¨¡å‹çš„åˆ¤æ–­ç»“æœï¼Œç»“åˆè¿›åŒ–åšå¼ˆæ¨¡å‹ï¼Œè¯„ä¼°åˆä½œçš„é•¿æœŸå½±å“ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºç³»ç»Ÿè¯„ä¼°LLMsåœ¨ä¸åŒç¤¾äº¤æƒ…å¢ƒä¸‹çš„åˆ¤æ–­ä¸€è‡´æ€§ä¸å·®å¼‚ï¼Œæ­ç¤ºäº†è¿™äº›å·®å¼‚å¦‚ä½•å½±å“äººç±»åˆä½œçš„æ™®éæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šç ”ç©¶ä¸­ä½¿ç”¨äº†å¤šç§ç¤¾äº¤æƒ…å¢ƒç¤ºä¾‹ï¼Œå¹¶é€šè¿‡ç›®æ ‡å¯¼å‘çš„æç¤ºæ¥å¼•å¯¼LLMsçš„åˆ¤æ–­ï¼Œæ¢ç´¢å¦‚ä½•é€šè¿‡è°ƒæ•´æç¤ºæ¥å½±å“æ¨¡å‹çš„åˆ¤æ–­ç»“æœã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†è¯´æ˜ï¼Œéœ€å‚è€ƒåŸæ–‡è·å–æ›´å¤šæŠ€æœ¯ç»†èŠ‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

ç ”ç©¶å‘ç°ï¼ŒLLMsåœ¨è¯„ä¼°ä¸è‰¯å¥½å¯¹æ‰‹çš„åˆä½œæ—¶è¡¨ç°å‡ºæ˜¾è‘—çš„ä¸€è‡´æ€§ï¼Œè€Œåœ¨ä¸å£°èª‰ä¸ä½³çš„ä¸ªä½“åˆä½œæ—¶åˆ™å­˜åœ¨è¾ƒå¤§å·®å¼‚ã€‚è¿™äº›å·®å¼‚å¯èƒ½æ˜¾è‘—å½±å“åˆä½œçš„æ™®éæ€§ï¼Œæç¤ºæˆ‘ä»¬åœ¨è®¾è®¡LLMæ—¶éœ€è°¨æ…å¯¹å¾…å…¶åˆ¤æ–­æ ‡å‡†ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç¤¾ä¼šå†³ç­–æ”¯æŒç³»ç»Ÿã€é“å¾·ä¸æ”¿æ²»åˆ¤æ–­è¾…åŠ©å·¥å…·ç­‰ã€‚é€šè¿‡ç†è§£LLMså¯¹äººç±»åˆä½œçš„å½±å“ï¼Œå¯ä»¥åœ¨è®¾è®¡ç¤¾äº¤å¹³å°å’Œå†³ç­–æ”¯æŒç³»ç»Ÿæ—¶æ›´å¥½åœ°ç»´æŠ¤äººç±»çš„åˆä½œè¡Œä¸ºï¼Œä¿ƒè¿›ç¤¾ä¼šå’Œè°ä¸ä¿¡ä»»ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Humans increasingly rely on large language models (LLMs) to support decisions in social settings. Previous work suggests that such tools shape people's moral and political judgements. However, the long-term implications of LLM-based social decision-making remain unknown. How will human cooperation be affected when the assessment of social interactions relies on language models? This is a pressing question, as human cooperation is often driven by indirect reciprocity, reputations, and the capacity to judge interactions of others. Here, we assess how state-of-the-art LLMs judge cooperative actions. We provide 21 different LLMs with an extensive set of examples where individuals cooperate -- or refuse cooperating -- in a range of social contexts, and ask how these interactions should be judged. Furthermore, through an evolutionary game-theoretical model, we evaluate cooperation dynamics in populations where the extracted LLM-driven judgements prevail, assessing the long-term impact of LLMs on human prosociality. We observe a remarkable agreement in evaluating cooperation against good opponents. On the other hand, we notice within- and between-model variance when judging cooperation with ill-reputed individuals. We show that the differences revealed between models can significantly impact the prevalence of cooperation. Finally, we test prompts to steer LLM norms, showing that such interventions can shape LLM judgements, particularly through goal-oriented prompts. Our research connects LLM-based advices and long-term social dynamics, and highlights the need to carefully align LLM norms in order to preserve human cooperation.

