---
layout: default
title: Self-correcting Reward Shaping via Language Models for Reinforcement Learning Agents in Games
---

# Self-correcting Reward Shaping via Language Models for Reinforcement Learning Agents in Games

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.23626" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.23626v1</a>
  <a href="https://arxiv.org/pdf/2506.23626.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.23626v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.23626v1', 'Self-correcting Reward Shaping via Language Models for Reinforcement Learning Agents in Games')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: AntÃ³nio Afonso, Iolanda Leite, Alessandro Sestini, Florian Fuchs, Konrad Tollmar, Linus GisslÃ©n

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-30

**å¤‡æ³¨**: 16 pages in total, 10 pages of main paper, 5 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªæ ¡æ­£å¥–åŠ±å¡‘å½¢æ–¹æ³•ä»¥è§£å†³å¼ºåŒ–å­¦ä¹ ä»£ç†çš„å¥–åŠ±è®¾è®¡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `å¥–åŠ±å¡‘å½¢` `è¯­è¨€æ¨¡å‹` `è‡ªåŠ¨åŒ–è°ƒä¼˜` `æ¸¸æˆä»£ç†` `è‡ªæˆ‘æ ¡æ­£` `æ€§èƒ½æå‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ ä»£ç†åœ¨æ¸¸æˆä¸­éƒ¨ç½²æ—¶ï¼Œè®¾è®¡æœ‰æ•ˆçš„å¥–åŠ±å‡½æ•°é€šå¸¸éœ€è¦ä¸“å®¶çš„å‚ä¸ï¼Œä¸”åœ¨æ¸¸æˆå†…å®¹å˜åŒ–åï¼ŒåŸæœ‰çš„å¥–åŠ±æƒé‡å¯èƒ½å¤±æ•ˆã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè¯­è¨€æ¨¡å‹çš„è‡ªåŠ¨åŒ–æ–¹æ³•ï¼Œé€šè¿‡ç”¨æˆ·å®šä¹‰çš„è¡Œä¸ºç›®æ ‡ï¼Œè¿­ä»£è°ƒæ•´RLä»£ç†çš„å¥–åŠ±å‡½æ•°æƒé‡ï¼Œå‡å°‘æ‰‹åŠ¨å¹²é¢„ã€‚
3. åœ¨èµ›è½¦ä»»åŠ¡ä¸­ï¼ŒLMå¼•å¯¼çš„ä»£ç†åœ¨ä¸€æ¬¡è¿­ä»£ä¸­æˆåŠŸç‡æå‡è‡³74%ï¼Œæœ€ç»ˆè¾¾åˆ°80%çš„æˆåŠŸç‡ï¼Œè¡¨ç°ä¸ä¸“å®¶è°ƒä¼˜çš„ä»£ç†ç›¸å½“ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œæ¸¸æˆä¸­çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œèƒ½å¤Ÿåˆ›é€ å‡ºä¸åŒçš„ä»£ç†è¡Œä¸ºï¼Œæ”¹å˜ç©å®¶çš„æ¸¸æˆä½“éªŒã€‚ç„¶è€Œï¼Œåœ¨ç”Ÿäº§ç¯å¢ƒä¸­éƒ¨ç½²RLä»£ç†é¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šè®¾è®¡æœ‰æ•ˆçš„å¥–åŠ±å‡½æ•°é€šå¸¸éœ€è¦RLä¸“å®¶çš„å‚ä¸ï¼Œä»¥åŠå½“æ¸¸æˆå†…å®¹æˆ–æœºåˆ¶å‘ç”Ÿå˜åŒ–æ—¶ï¼Œä¹‹å‰è°ƒä¼˜çš„å¥–åŠ±æƒé‡å¯èƒ½ä¸å†æœ€ä½³ã€‚ä¸ºäº†è§£å†³åä¸€ä¸ªæŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªåŠ¨åŒ–æ–¹æ³•ï¼Œé€šè¿‡ç”¨æˆ·å®šä¹‰çš„åŸºäºè¯­è¨€çš„è¡Œä¸ºç›®æ ‡ï¼Œè¿­ä»£å¾®è°ƒRLä»£ç†çš„å¥–åŠ±å‡½æ•°æƒé‡ã€‚è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰åœ¨æ¯æ¬¡è¿­ä»£ä¸­æ ¹æ®ç›®æ ‡è¡Œä¸ºå’Œå…ˆå‰è®­ç»ƒè½®æ¬¡çš„æ€§èƒ½ç»Ÿè®¡æ‘˜è¦æå‡ºæ›´æ–°çš„æƒé‡ã€‚è¿™ä¸ªé—­ç¯è¿‡ç¨‹ä½¿å¾—LMèƒ½å¤Ÿè‡ªæˆ‘æ ¡æ­£å¹¶éšç€æ—¶é—´æ¨ç§»ä¸æ–­ä¼˜åŒ–è¾“å‡ºï¼Œäº§ç”Ÿè¶Šæ¥è¶Šä¸€è‡´çš„è¡Œä¸ºï¼Œè€Œæ— éœ€æ‰‹åŠ¨å¥–åŠ±å·¥ç¨‹ã€‚æˆ‘ä»¬åœ¨èµ›è½¦ä»»åŠ¡ä¸­è¯„ä¼°äº†è¯¥æ–¹æ³•ï¼Œç»“æœæ˜¾ç¤ºä»£ç†æ€§èƒ½åœ¨è¿­ä»£ä¸­æŒç»­æå‡ã€‚LMå¼•å¯¼çš„ä»£ç†åœ¨ä¸€æ¬¡è¿­ä»£ä¸­æˆåŠŸç‡ä»9%æå‡è‡³74%ã€‚ä¸äººç±»ä¸“å®¶çš„æ‰‹åŠ¨æƒé‡è®¾è®¡ç›¸æ¯”ï¼ŒLMè°ƒä¼˜çš„ä»£ç†åœ¨æœ€ç»ˆè¿­ä»£ä¸­è¾¾åˆ°äº†80%çš„æˆåŠŸç‡ï¼Œå¹³å‡å®Œæˆåœˆæ•°ä¸º855ä¸ªæ—¶é—´æ­¥ï¼Œè¡¨ç°ä¸ä¸“å®¶è°ƒä¼˜ä»£ç†çš„æœ€é«˜94%æˆåŠŸç‡å’Œ850ä¸ªæ—¶é—´æ­¥ç›¸å½“ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¼ºåŒ–å­¦ä¹ ä»£ç†åœ¨æ¸¸æˆä¸­éƒ¨ç½²æ—¶ï¼Œå¥–åŠ±å‡½æ•°è®¾è®¡çš„å¤æ‚æ€§å’Œæ¸¸æˆå†…å®¹å˜åŒ–å¯¼è‡´çš„å¥–åŠ±æƒé‡å¤±æ•ˆé—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºä¸“å®¶çš„æ‰‹åŠ¨è°ƒä¼˜ï¼Œæ•ˆç‡ä½ä¸‹ä¸”ä¸å¤Ÿçµæ´»ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé€šè¿‡è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰è‡ªåŠ¨åŒ–å¥–åŠ±æƒé‡çš„å¾®è°ƒè¿‡ç¨‹ï¼Œåˆ©ç”¨ç”¨æˆ·å®šä¹‰çš„è¡Œä¸ºç›®æ ‡å’Œå†å²æ€§èƒ½æ•°æ®ï¼Œä½¿å¾—ä»£ç†èƒ½å¤Ÿè‡ªæˆ‘æ ¡æ­£ï¼Œé€æ­¥ä¼˜åŒ–å…¶è¡Œä¸ºã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ç”¨æˆ·å®šä¹‰çš„è¡Œä¸ºç›®æ ‡è¾“å…¥ã€LMç”Ÿæˆçš„å¥–åŠ±æƒé‡æ›´æ–°ã€ä»¥åŠåŸºäºæ€§èƒ½ç»Ÿè®¡çš„åé¦ˆå¾ªç¯ã€‚æ¯æ¬¡è¿­ä»£ä¸­ï¼ŒLMæ ¹æ®ç›®æ ‡è¡Œä¸ºå’Œå…ˆå‰è®­ç»ƒç»“æœæå‡ºæ–°çš„æƒé‡ï¼Œå½¢æˆé—­ç¯ä¼˜åŒ–è¿‡ç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†è¯­è¨€æ¨¡å‹åº”ç”¨äºå¥–åŠ±å¡‘å½¢ï¼Œä½¿å¾—RLä»£ç†èƒ½å¤Ÿåœ¨æ²¡æœ‰æ‰‹åŠ¨å¹²é¢„çš„æƒ…å†µä¸‹ï¼Œè‡ªä¸»è°ƒæ•´å’Œä¼˜åŒ–å…¶è¡Œä¸ºç­–ç•¥ã€‚è¿™ç§æ–¹æ³•æ˜¾è‘—æé«˜äº†ä»£ç†çš„é€‚åº”æ€§å’Œæ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒLMçš„è¾“å…¥åŒ…æ‹¬ç”¨æˆ·å®šä¹‰çš„è¡Œä¸ºç›®æ ‡å’Œå†å²è®­ç»ƒæ•°æ®ï¼Œè¾“å‡ºä¸ºæ›´æ–°çš„å¥–åŠ±æƒé‡ã€‚æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„çš„å…·ä½“ç»†èŠ‚æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†è¯´æ˜ï¼Œéœ€å‚è€ƒè®ºæ–‡çš„å®Œæ•´å†…å®¹ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLMå¼•å¯¼çš„ä»£ç†åœ¨ä¸€æ¬¡è¿­ä»£ä¸­æˆåŠŸç‡ä»9%æå‡è‡³74%ï¼Œæœ€ç»ˆæˆåŠŸç‡è¾¾åˆ°80%ã€‚ä¸äººç±»ä¸“å®¶çš„æ‰‹åŠ¨è°ƒä¼˜ç›¸æ¯”ï¼ŒLMè°ƒä¼˜çš„ä»£ç†åœ¨æ€§èƒ½ä¸Šè¡¨ç°å‡ºç«äº‰åŠ›ï¼Œå®Œæˆåœˆæ•°çš„å¹³å‡æ—¶é—´æ­¥ä¸º855ï¼Œæ¥è¿‘ä¸“å®¶è°ƒä¼˜ä»£ç†çš„850æ—¶é—´æ­¥ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ¸¸æˆå¼€å‘ã€æ™ºèƒ½ä»£ç†ç³»ç»Ÿå’Œè‡ªé€‚åº”å­¦ä¹ ç¯å¢ƒã€‚é€šè¿‡è‡ªåŠ¨åŒ–å¥–åŠ±è®¾è®¡ï¼Œå¼€å‘è€…å¯ä»¥æ›´å¿«é€Ÿåœ°è°ƒæ•´ä»£ç†è¡Œä¸ºï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚åŒæ—¶ï¼Œè¯¥æ–¹æ³•ä¹Ÿå¯æ¨å¹¿è‡³å…¶ä»–éœ€è¦åŠ¨æ€è°ƒæ•´ç­–ç•¥çš„å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ä¸­ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement Learning (RL) in games has gained significant momentum in recent years, enabling the creation of different agent behaviors that can transform a player's gaming experience. However, deploying RL agents in production environments presents two key challenges: (1) designing an effective reward function typically requires an RL expert, and (2) when a game's content or mechanics are modified, previously tuned reward weights may no longer be optimal. Towards the latter challenge, we propose an automated approach for iteratively fine-tuning an RL agent's reward function weights, based on a user-defined language based behavioral goal. A Language Model (LM) proposes updated weights at each iteration based on this target behavior and a summary of performance statistics from prior training rounds. This closed-loop process allows the LM to self-correct and refine its output over time, producing increasingly aligned behavior without the need for manual reward engineering. We evaluate our approach in a racing task and show that it consistently improves agent performance across iterations. The LM-guided agents show a significant increase in performance from $9\%$ to $74\%$ success rate in just one iteration. We compare our LM-guided tuning against a human expert's manual weight design in the racing task: by the final iteration, the LM-tuned agent achieved an $80\%$ success rate, and completed laps in an average of $855$ time steps, a competitive performance against the expert-tuned agent's peak $94\%$ success, and $850$ time steps.

