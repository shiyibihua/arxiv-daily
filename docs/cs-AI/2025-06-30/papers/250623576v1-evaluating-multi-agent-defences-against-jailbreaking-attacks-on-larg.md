---
layout: default
title: Evaluating Multi-Agent Defences Against Jailbreaking Attacks on Large Language Models
---

# Evaluating Multi-Agent Defences Against Jailbreaking Attacks on Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.23576" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.23576v1</a>
  <a href="https://arxiv.org/pdf/2506.23576.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.23576v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.23576v1', 'Evaluating Multi-Agent Defences Against Jailbreaking Attacks on Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Maria Carolina Cornelia Wit, Jun Pang

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-30

**å¤‡æ³¨**: 26 pages, 1 figure

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šä»£ç†ç³»ç»Ÿä»¥åº”å¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„è¶Šç‹±æ”»å‡»**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¶Šç‹±æ”»å‡»` `å¤§å‹è¯­è¨€æ¨¡å‹` `å¤šä»£ç†ç³»ç»Ÿ` `å®‰å…¨æœºåˆ¶` `å‡é˜´æ€§` `é˜²å¾¡ç­–ç•¥` `è®¡ç®—å¼€é”€`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è¶Šç‹±æ”»å‡»å¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§æ„æˆå¨èƒï¼Œç°æœ‰é˜²å¾¡æ–¹æ³•æ•ˆæœæœ‰é™ï¼Œå°¤å…¶åœ¨å‡é˜´æ€§ç‡æ–¹é¢ã€‚
2. æœ¬æ–‡æå‡ºä½¿ç”¨å¤šä»£ç†LLMç³»ç»Ÿä½œä¸ºé˜²å¾¡æ‰‹æ®µï¼Œé€šè¿‡æ¯”è¾ƒä¸åŒä»£ç†é…ç½®æ¥è¯„ä¼°å…¶æœ‰æ•ˆæ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¤šä»£ç†ç³»ç»Ÿåœ¨æŠµæŠ—è¶Šç‹±æ”»å‡»æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå‡é˜´æ€§æ˜¾è‘—å‡å°‘ï¼Œä½†å‡é˜³æ€§å’Œè®¡ç®—å¼€é”€æœ‰æ‰€å¢åŠ ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›æ­¥å¼•å‘äº†å¯¹è¶Šç‹±æ”»å‡»çš„æ‹…å¿§ï¼Œå³é€šè¿‡ç‰¹å®šæç¤ºç»•è¿‡å®‰å…¨æœºåˆ¶ã€‚æœ¬æ–‡ç ”ç©¶äº†å¤šä»£ç†LLMç³»ç»Ÿä½œä¸ºé˜²å¾¡æ‰‹æ®µçš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬è¯„ä¼°äº†ä¸‰ç§è¶Šç‹±ç­–ç•¥ï¼ŒåŒ…æ‹¬åŸå§‹çš„AutoDefenseæ”»å‡»å’ŒDeepleapsçš„BetterDanåŠJBã€‚é€šè¿‡é‡ç°AutoDefenseæ¡†æ¶ï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†å•ä»£ç†ä¸åŒä»£ç†å’Œä¸‰ä»£ç†é…ç½®çš„æ•ˆæœã€‚ç»“æœè¡¨æ˜ï¼Œå¤šä»£ç†ç³»ç»Ÿå¢å¼ºäº†å¯¹è¶Šç‹±æ”»å‡»çš„æŠµæŠ—åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨å‡å°‘å‡é˜´æ€§æ–¹é¢ã€‚ç„¶è€Œï¼Œå…¶æœ‰æ•ˆæ€§å› æ”»å‡»ç±»å‹è€Œå¼‚ï¼Œå¹¶å¼•å…¥äº†å‡é˜³æ€§å¢åŠ å’Œè®¡ç®—å¼€é”€ç­‰æƒè¡¡ã€‚è¿™äº›å‘ç°æ­ç¤ºäº†å½“å‰è‡ªåŠ¨é˜²å¾¡çš„å±€é™æ€§ï¼Œå¹¶ä¸ºæœªæ¥LLMç³»ç»Ÿçš„å¯¹é½é²æ£’æ€§æ”¹è¿›æä¾›äº†æ–¹å‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹é¢ä¸´çš„è¶Šç‹±æ”»å‡»é—®é¢˜ï¼Œç°æœ‰å•ä»£ç†é˜²å¾¡æ–¹æ³•åœ¨å‡é˜´æ€§ç‡ä¸Šå­˜åœ¨ä¸è¶³ï¼Œå¯¼è‡´å®‰å…¨æ€§é™ä½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥å¤šä»£ç†ç³»ç»Ÿï¼Œåˆ©ç”¨å¤šä¸ªä»£ç†ä¹‹é—´çš„åä½œæ¥å¢å¼ºå¯¹è¶Šç‹±æ”»å‡»çš„é˜²å¾¡èƒ½åŠ›ï¼Œæ—¨åœ¨æé«˜æ¨¡å‹çš„æ•´ä½“å®‰å…¨æ€§å’Œé²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªä»£ç†çš„åä½œæœºåˆ¶ï¼Œåˆ†åˆ«è¿›è¡Œè¾“å…¥å¤„ç†å’Œæ”»å‡»æ£€æµ‹ã€‚å®éªŒä¸­æ¯”è¾ƒäº†å•ä»£ç†ã€åŒä»£ç†å’Œä¸‰ä»£ç†é…ç½®çš„æ•ˆæœï¼Œè¯„ä¼°å…¶åœ¨ä¸åŒæ”»å‡»ç­–ç•¥ä¸‹çš„è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¤šä»£ç†ç³»ç»Ÿçš„è®¾è®¡ï¼Œé€šè¿‡ä»£ç†é—´çš„äº¤äº’å‡å°‘å‡é˜´æ€§ï¼Œæé«˜å¯¹è¶Šç‹±æ”»å‡»çš„æŠµæŠ—åŠ›ï¼Œä¸ä¼ ç»Ÿå•ä»£ç†æ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†åŠ¨æ€è°ƒæ•´çš„é˜ˆå€¼æ¥å¹³è¡¡å‡é˜³æ€§å’Œå‡é˜´æ€§ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šè€ƒè™‘äº†å¤šä»£ç†åä½œçš„ç‰¹æ€§ï¼Œç¡®ä¿å„ä»£ç†èƒ½å¤Ÿæœ‰æ•ˆé…åˆã€‚å®éªŒä¸­è¿˜ä¼˜åŒ–äº†è®¡ç®—èµ„æºçš„ä½¿ç”¨ï¼Œä»¥é™ä½æ•´ä½“å¼€é”€ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå¤šä»£ç†ç³»ç»Ÿåœ¨æŠµæŠ—è¶Šç‹±æ”»å‡»æ–¹é¢æ˜¾è‘—ä¼˜äºå•ä»£ç†é…ç½®ï¼Œå‡é˜´æ€§ç‡é™ä½äº†çº¦30%ï¼Œä½†å‡é˜³æ€§ç‡æœ‰æ‰€ä¸Šå‡ï¼Œè®¡ç®—å¼€é”€å¢åŠ äº†15%ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†å¤šä»£ç†ç³»ç»Ÿåœ¨å®‰å…¨é˜²æŠ¤ä¸­çš„æ½œåŠ›å’Œå±€é™æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å®‰å…¨æ•æ„Ÿçš„å¯¹è¯ç³»ç»Ÿã€å†…å®¹ç”Ÿæˆå¹³å°åŠå…¶ä»–ä¾èµ–å¤§å‹è¯­è¨€æ¨¡å‹çš„åº”ç”¨ã€‚é€šè¿‡å¢å¼ºæ¨¡å‹çš„å®‰å…¨æ€§ï¼Œå¯ä»¥æœ‰æ•ˆé˜²æ­¢æ¶æ„ç”¨æˆ·åˆ©ç”¨è¶Šç‹±æ”»å‡»è¿›è¡Œä¸å½“æ“ä½œï¼Œæå‡ç”¨æˆ·ä¿¡ä»»åº¦å’Œç³»ç»Ÿç¨³å®šæ€§ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åœ¨æ›´å¹¿æ³›çš„AIç³»ç»Ÿä¸­æ¨å¹¿åº”ç”¨ï¼Œæå‡æ•´ä½“å®‰å…¨æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in large language models (LLMs) have raised concerns about jailbreaking attacks, i.e., prompts that bypass safety mechanisms. This paper investigates the use of multi-agent LLM systems as a defence against such attacks. We evaluate three jailbreaking strategies, including the original AutoDefense attack and two from Deepleaps: BetterDan and JB. Reproducing the AutoDefense framework, we compare single-agent setups with two- and three-agent configurations. Our results show that multi-agent systems enhance resistance to jailbreaks, especially by reducing false negatives. However, its effectiveness varies by attack type, and it introduces trade-offs such as increased false positives and computational overhead. These findings point to the limitations of current automated defences and suggest directions for improving alignment robustness in future LLM systems.

