---
layout: default
title: Towards Building Private LLMs: Exploring Multi-Node Expert Parallelism on Apple Silicon for Mixture-of-Experts Large Language Model
---

# Towards Building Private LLMs: Exploring Multi-Node Expert Parallelism on Apple Silicon for Mixture-of-Experts Large Language Model

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.23635" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.23635v1</a>
  <a href="https://arxiv.org/pdf/2506.23635.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.23635v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.23635v1', 'Towards Building Private LLMs: Exploring Multi-Node Expert Parallelism on Apple Silicon for Mixture-of-Experts Large Language Model')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Mu-Chi Chen, Po-Hsuan Huang, Xiangrui Ke, Chia-Heng Tu, Chun Jason Xue, Shih-Hao Hung

**ÂàÜÁ±ª**: cs.DC, cs.AI, cs.PF

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-30

**Â§áÊ≥®**: International Conference on Research in Adaptive and Convergent Systems (RACS '24), November 5--8, 2024, Pompei, Italy

**DOI**: [10.1145/3649601.3698722](https://doi.org/10.1145/3649601.3698722)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Â§öËäÇÁÇπ‰∏ìÂÆ∂Âπ∂Ë°åÊñπÊ≥ï‰ª•Ëß£ÂÜ≥ÁßÅÊúâÂ§ßËØ≠Ë®ÄÊ®°ÂûãÊûÑÂª∫ÁöÑÊàêÊú¨‰∏éÂèØÊâ©Â±ïÊÄßÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `ÁßÅÊúâLLM` `Ê∑∑Âêà‰∏ìÂÆ∂` `Â§öËäÇÁÇπÂπ∂Ë°å` `Apple Silicon` `Êé®ÁêÜ‰ºòÂåñ` `ÊàêÊú¨ÊïàÁéá`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®ÊûÑÂª∫ÁßÅÊúâÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊó∂Èù¢‰∏¥È´òÊàêÊú¨ÂíåÂèØÊâ©Â±ïÊÄß‰∏çË∂≥ÁöÑÈóÆÈ¢òÔºåÈôêÂà∂‰∫ÜÂÖ∂Âú®‰∏™‰∫∫ÂíåÂ∞èÁªÑÊúçÂä°‰∏≠ÁöÑÂ∫îÁî®„ÄÇ
2. Êú¨ÊñáÊèêÂá∫Âú®Apple M2 UltraËäØÁâáÁöÑMac StudioÈõÜÁæ§‰∏äÂÆûÁé∞Â§öËäÇÁÇπ‰∏ìÂÆ∂Âπ∂Ë°åÔºåÂà©Áî®Ê∑∑Âêà‰∏ìÂÆ∂Êû∂ÊûÑÊù•Âä†ÈÄüÊé®ÁêÜËøáÁ®ã„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÈááÁî®ËØ•ÊñπÊ≥ïÂêéÔºåÊé®ÁêÜÊó∂Èó¥ÊòæËëóÂáèÂ∞ëÔºå‰∏îMac StudioÈõÜÁæ§ÁöÑÊàêÊú¨ÊïàÁéáÊØîÁé∞ÊúâÁöÑAIË∂ÖÁ∫ßËÆ°ÁÆóÊú∫È´òÂá∫1.15ÂÄç„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®‰∫∫Â∑•Êô∫ËÉΩÈ¢ÜÂüüÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ïÔºåÁÑ∂ËÄåÊûÑÂª∫ÁßÅÊúâLLMÁ≥ªÁªüÈù¢‰∏¥ÊàêÊú¨ÂíåÂèØÊâ©Â±ïÊÄßÊåëÊàò„ÄÇÊú¨ÊñáÈÄöËøáÂª∫Á´ãÂü∫‰∫éApple M2 UltraËäØÁâáÁöÑMac StudioÈõÜÁæ§ÔºåÊé¢ËÆ®‰∫ÜÂ¶Ç‰ΩïÈ´òÊïàÊâòÁÆ°ÂíåÂä†ÈÄüÈ¢ÑËÆ≠ÁªÉÁöÑDBRXÊ®°ÂûãÔºåÈááÁî®Ê∑∑Âêà‰∏ìÂÆ∂ÔºàMoEÔºâÊû∂ÊûÑ„ÄÇÊÄßËÉΩÂàÜÊûêË°®ÊòéÔºåÂú®‰∏§Âà∞Âõõ‰∏™Êú∫Âô®ËäÇÁÇπ‰∏äÂπ∂Ë°åÊâßË°åÊ®°Âûã‰∏ìÂÆ∂ÊòæËëóÂáèÂ∞ë‰∫ÜÊé®ÁêÜÊó∂Èó¥ÔºåÂêåÊó∂ÂèëÁé∞‰∏ìÂÆ∂ËÆ°ÁÆóÊó∂Èó¥‰∏éËæìÂá∫‰∫§Êç¢ÁöÑÈÄö‰ø°Êó∂Èó¥Áõ∏ÂΩìÔºåÂº∫Ë∞É‰∫ÜÁΩëÁªúÂª∂ËøüÁöÑÈáçË¶ÅÊÄß„ÄÇÂü∫‰∫éËøô‰∫õÂèëÁé∞ÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰ºòÂåñÊñπÊ°à‰ª•Ê∂àÈô§ÂÜÖÂ≠òÁÆ°ÁêÜÂºÄÈîÄÔºå‰ΩøÂæóMac StudioÈõÜÁæ§Âú®ÊàêÊú¨ÊïàÁéá‰∏äË∂ÖËøá‰∫ÜÂü∫‰∫éNVIDIA H100 GPUÁöÑÊúÄÂÖàËøõAIË∂ÖÁ∫ßËÆ°ÁÆóÊú∫„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥ÊûÑÂª∫ÁßÅÊúâÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊó∂ÁöÑÈ´òÊàêÊú¨ÂíåÂèØÊâ©Â±ïÊÄßÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®Êé®ÁêÜÈÄüÂ∫¶ÂíåËµÑÊ∫êÂà©Áî®Áéá‰∏äÂ≠òÂú®‰∏çË∂≥ÔºåÂ∞§ÂÖ∂ÊòØÂú®Â∞èËßÑÊ®°ÊúçÂä°Âú∫ÊôØ‰∏≠„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÂú®Mac StudioÈõÜÁæ§‰∏äÂÆûÁé∞Â§öËäÇÁÇπ‰∏ìÂÆ∂Âπ∂Ë°åÔºåÂà©Áî®Ê∑∑Âêà‰∏ìÂÆ∂Êû∂ÊûÑÊù•ÊèêÈ´òÊé®ÁêÜÊïàÁéáÔºåÈôç‰ΩéÊàêÊú¨„ÄÇËøôÊ†∑ÁöÑËÆæËÆ°ËÉΩÂ§üÂÖÖÂàÜÂà©Áî®Apple SiliconÁöÑËÆ°ÁÆóËÉΩÂäõÔºåÂêåÊó∂‰ºòÂåñÁΩëÁªúÈÄö‰ø°„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨Â§ö‰∏™ËÆ°ÁÆóËäÇÁÇπÔºåÊØè‰∏™ËäÇÁÇπ‰∏äËøêË°å‰∏çÂêåÁöÑ‰∏ìÂÆ∂Ê®°Âûã„ÄÇÈÄöËøáÂπ∂Ë°åÊâßË°åÔºåËäÇÁÇπÈó¥ËøõË°åÈ´òÊïàÁöÑËæìÂá∫‰∫§Êç¢ÔºåÂáèÂ∞ëÊé®ÁêÜÊó∂Èó¥„ÄÇ‰∏ªË¶ÅÊ®°ÂùóÂåÖÊã¨‰∏ìÂÆ∂Ê®°Âûã„ÄÅÁΩëÁªúÈÄö‰ø°ÁÆ°ÁêÜÂíåÂÜÖÂ≠òÁÆ°ÁêÜ‰ºòÂåñ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞Âú®‰∫éÊèêÂá∫‰∫ÜÂú®Apple M2 UltraËäØÁâá‰∏äÂÆûÁé∞ÁöÑÂ§öËäÇÁÇπ‰∏ìÂÆ∂Âπ∂Ë°åÊñπÊ≥ïÔºåÂº∫Ë∞É‰∫ÜÁΩëÁªúÂª∂ËøüÂØπÊé®ÁêÜÊïàÁéáÁöÑÂΩ±ÂìçÔºåÂπ∂ÊèêÂá∫‰∫ÜÁõ∏Â∫îÁöÑ‰ºòÂåñÊñπÊ°à„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåÊú¨ÊñáÂú®ÊàêÊú¨ÂíåÊÄßËÉΩ‰∏äÂùáÊúâÊòæËëóÊèêÂçá„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂèÇÊï∞ËÆæÁΩÆ‰∏äÔºå‰ºòÂåñ‰∫Ü‰∏ìÂÆ∂Ê®°ÂûãÁöÑÊï∞ÈáèÂíåËäÇÁÇπÈÖçÁΩÆÔºåÈááÁî®‰∫ÜÈÄÇÂ∫îÊÄßÂÜÖÂ≠òÁÆ°ÁêÜÁ≠ñÁï•‰ª•ÂáèÂ∞ëÁÆ°ÁêÜÂºÄÈîÄ„ÄÇÊçüÂ§±ÂáΩÊï∞ÂíåÁΩëÁªúÁªìÊûÑËÆæËÆ°‰∏äÔºåÁªìÂêà‰∫ÜÊ∑∑Âêà‰∏ìÂÆ∂Êû∂ÊûÑÁöÑÁâπÁÇπÔºå‰ª•ÊèêÈ´òÊ®°ÂûãÁöÑÊé®ÁêÜÈÄüÂ∫¶ÂíåÂáÜÁ°ÆÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÈááÁî®Â§öËäÇÁÇπ‰∏ìÂÆ∂Âπ∂Ë°åÊñπÊ≥ïÂêéÔºåÊé®ÁêÜÊó∂Èó¥ÊòæËëóÂáèÂ∞ëÔºå‰∏îMac StudioÈõÜÁæ§Âú®ÊàêÊú¨ÊïàÁéá‰∏äÊØîÂü∫‰∫éNVIDIA H100 GPUÁöÑË∂ÖÁ∫ßËÆ°ÁÆóÊú∫È´òÂá∫1.15ÂÄç„ÄÇËøô‰∏ÄÊàêÊûúÂ±ïÁ§∫‰∫ÜÂú®ÁßÅÊúâLLMÊûÑÂª∫‰∏≠ÁöÑÂÆûÈôÖÂ∫îÁî®ÊΩúÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨‰∏™‰∫∫Âä©ÁêÜ„ÄÅÂÆöÂà∂ÂåñËÅäÂ§©Êú∫Âô®‰∫∫ÂíåÂ∞èÂûãÂõ¢ÈòüÁöÑÊô∫ËÉΩÊúçÂä°Á≥ªÁªü„ÄÇÈÄöËøáÈôç‰ΩéÊûÑÂª∫ÁßÅÊúâLLMÁöÑÊàêÊú¨ÂíåÊèêÈ´òÊé®ÁêÜÊïàÁéáÔºåËÉΩÂ§ü‰ΩøÊõ¥Â§öÁî®Êà∑ÂíåÁªÑÁªáÂèóÁõä‰∫éÂÖàËøõÁöÑ‰∫∫Â∑•Êô∫ËÉΩÊäÄÊúØÔºåÊé®Âä®Êô∫ËÉΩÊúçÂä°ÁöÑÊôÆÂèä‰∏éÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Large Language Models (LLMs) have revolutionized Artificial Intelligence (AI) with significant advancements such as OpenAI's ChatGPT, Meta's Llama, and Databricks' DBRX. This paper addresses the cost and scalability challenges encountered when constructing private LLM systems for personal or small group services, as aimed by Apple Intelligence. A Mac Studio cluster with Apple's M2 Ultra chips is established as a cost-efficient solution to host and accelerate the pretrained DBRX model with the Mixture-of-Experts (MoE) architecture. Our performance analysis reveal that parallel execution of the model's experts across two to four machine nodes significantly reduces inference time. We find that computation time for the experts is comparable to the communication time for exchanging their outputs, emphasizing the importance of network latency over bandwidth. We also observe significant management overhead due to Apple software stack's memory management logic. Based on these findings, we develop optimization schemes to eliminate the memory management overhead. As a result, the Mac Studio cluster is 1.15 times more cost-efficient than the state-of-the-art AI supercomputer with NVIDIA H100 GPUs. In addition, we construct a performance model to estimate system performance under varying configurations, and the model provides valuable insights for designing private LLM systems.

