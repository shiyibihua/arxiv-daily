---
layout: default
title: Edge-Based Multimodal Sensor Data Fusion with Vision Language Models (VLMs) for Real-time Autonomous Vehicle Accident Avoidance
---

# Edge-Based Multimodal Sensor Data Fusion with Vision Language Models (VLMs) for Real-time Autonomous Vehicle Accident Avoidance

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.01057" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.01057v2</a>
  <a href="https://arxiv.org/pdf/2508.01057.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.01057v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.01057v2', 'Edge-Based Multimodal Sensor Data Fusion with Vision Language Models (VLMs) for Real-time Autonomous Vehicle Accident Avoidance')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Fengze Yang, Bo Yu, Yang Zhou, Xuewen Luo, Zhengzhong Tu, Chenxi Liu

**ÂàÜÁ±ª**: cs.AI, cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-01 (Êõ¥Êñ∞: 2025-08-12)

**Â§áÊ≥®**: 24 pages, 6 tables, 7 figures

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫REACTÊ°ÜÊû∂‰ª•Ëß£ÂÜ≥Ëá™Âä®È©æÈ©∂ÂÆûÊó∂Á¢∞ÊíûÈÅøÂÖçÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Ëá™Âä®È©æÈ©∂` `Â§öÊ®°ÊÄÅËûçÂêà` `ËßÜËßâËØ≠Ë®ÄÊ®°Âûã` `ÂÆûÊó∂Êé®ÁêÜ` `ËΩ®Ëøπ‰ºòÂåñ` `ËæπÁºòËÆ°ÁÆó` `‰∫§ÈÄöÂÆâÂÖ®`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâËá™Âä®È©æÈ©∂ÊñπÊ≥ïÂú®Â§çÊùÇ‰∫§ÈÄöÁéØÂ¢É‰∏≠Èöæ‰ª•ÂÆûÁé∞ÊúâÊïàÁöÑÂ§öÊ®°ÊÄÅÊï∞ÊçÆËûçÂêà‰∏éÂÆûÊó∂Êé®ÁêÜÔºåÂØºËá¥Á¢∞ÊíûÈ£éÈô©Â¢ûÂä†„ÄÇ
2. Êú¨ÊñáÊèêÂá∫REACTÊ°ÜÊû∂ÔºåÈÄöËøáÈõÜÊàêÂü∫Á°ÄËÆæÊñΩË≠¶Êä•‰∏éËΩ¶ËΩΩ‰º†ÊÑüÂô®Êï∞ÊçÆÔºåÂà©Áî®ËΩªÈáèÁ∫ßËßÜËßâËØ≠Ë®ÄÊ®°ÂûãËøõË°åËΩ®Ëøπ‰ºòÂåñ„ÄÇ
3. REACTÂú®DeepAccidentÂü∫ÂáÜ‰∏äË°®Áé∞Âá∫Ëâ≤ÔºåÁ¢∞ÊíûÁéáÈôç‰Ωé77%ÔºåËßÜÈ¢ëÂÖ®ÊôØË¥®ÈáèËææÂà∞48.2%ÔºåÊé®ÁêÜÂª∂Ëøü‰ªÖ‰∏∫0.57Áßí„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Ëá™Âä®È©æÈ©∂Á≥ªÁªü‰ªÖ‰æùËµñËΩ¶ËΩΩ‰º†ÊÑüÂô®ÂèØËÉΩÊó†Ê≥ïÊé¢ÊµãËøúÂ§ÑÊàñÈöúÁ¢çÁâ©ÔºåÂØºËá¥ÂèØÈÅøÂÖçÁöÑÁ¢∞Êíû„ÄÇÁé∞ÊúâÁöÑÂü∫‰∫éÂèòÊç¢Âô®ÁöÑËΩ¶ËÅîÁΩëÔºàV2XÔºâÊñπÊ≥ïÂú®Â§öÊ®°ÊÄÅËûçÂêàÂíåÊé®ÁêÜÊñπÈù¢Â≠òÂú®‰∏çË∂≥ÔºåÊàñÂú®Â§çÊùÇÁöÑÈ´òÁª¥‰∫§ÈÄöÊù°‰ª∂‰∏ãÈöæ‰ª•Êª°Ë∂≥ÂÆûÊó∂ÊÄßËÉΩË¶ÅÊ±Ç„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éËΩªÈáèÁ∫ßËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâÁöÑÂÆûÊó∂ËæπÁºòËá™‰∏ªÂâØÈ©æÈ©∂ËΩ®ËøπËßÑÂàíÂô®ÔºàREACTÔºâÔºåËØ•Ê°ÜÊû∂ÈõÜÊàê‰∫ÜÂü∫Á°ÄËÆæÊñΩÊèê‰æõÁöÑÂç±Èô©Ë≠¶Êä•‰∏éËΩ¶ËΩΩ‰º†ÊÑüÂô®Êï∞ÊçÆÔºåÈÄöËøáËßÜËßâÂµåÂÖ•ÊçïÊçâÂë®Âõ¥‰∫§ÈÄöÂä®ÊÄÅÂíåËΩ¶ËæÜÊÑèÂõæÔºåÂà©Áî®‰∏ä‰∏ãÊñáÊé®ÁêÜÁîüÊàê‰ºòÂåñÁöÑÂÆâÂÖ®ËΩ®Ëøπ„ÄÇREACTÂú®DeepAccidentÂü∫ÂáÜ‰∏äËØÑ‰º∞ÔºåÂèñÂæó‰∫Ü77%ÁöÑÁ¢∞ÊíûÁéáÈôç‰ΩéÂíå0.57ÁßíÁöÑÊé®ÁêÜÂª∂ËøüÔºåÈ™åËØÅ‰∫ÜËΩªÈáèÁ∫ßVLMÂú®ËæπÁºòÂπ≥Âè∞‰∏äÂÆûÁé∞ÂÆûÊó∂ÂçèÂêåËßÑÂàíÁöÑÊúâÊïàÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Ëá™Âä®È©æÈ©∂Á≥ªÁªüÂú®Â§çÊùÇ‰∫§ÈÄöÁéØÂ¢É‰∏≠ÂØπËøúÂ§ÑÈöúÁ¢çÁâ©Êé¢Êµã‰∏çË∂≥ÁöÑÈóÆÈ¢òÔºåÁé∞ÊúâÊñπÊ≥ïÂú®Â§öÊ®°ÊÄÅÊï∞ÊçÆËûçÂêàÂíåÂÆûÊó∂Êé®ÁêÜÊñπÈù¢Â≠òÂú®ÊòæËëó‰∏çË∂≥„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöREACTÊ°ÜÊû∂ÈÄöËøáÁªìÂêàÂü∫Á°ÄËÆæÊñΩÊèê‰æõÁöÑÂç±Èô©Ë≠¶Êä•‰∏éËΩ¶ËΩΩ‰º†ÊÑüÂô®Êï∞ÊçÆÔºåÂà©Áî®ËΩªÈáèÁ∫ßËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâËøõË°å‰∏ä‰∏ãÊñáÊé®ÁêÜÂíåËΩ®Ëøπ‰ºòÂåñÔºå‰ª•ÊèêÈ´òÂÆâÂÖ®ÊÄßÂíåÂÆûÊó∂ÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöREACTÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨Êï∞ÊçÆÈááÈõÜÊ®°Âùó„ÄÅËßÜËßâÂµåÂÖ•ÁîüÊàêÊ®°Âùó„ÄÅ‰∏ä‰∏ãÊñáÊé®ÁêÜÊ®°ÂùóÂíåËΩ®Ëøπ‰ºòÂåñÊ®°ÂùóÔºåÁ°Æ‰øùÂú®ËæπÁºòËÆæÂ§á‰∏äÈ´òÊïàËøêË°å„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöREACTÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÈááÁî®‰∫ÜÊÆãÂ∑ÆËΩ®ËøπËûçÂêàÔºàRTFÔºâËÆæËÆ°Âíå‰∏ìÈó®ÁöÑËæπÁºòÈÄÇÂ∫îÁ≠ñÁï•ÔºåÊòæËëóÈôç‰Ωé‰∫ÜÊ®°ÂûãÂ§çÊùÇÊÄßÂπ∂ÊèêÂçá‰∫ÜÊé®ÁêÜÊïàÁéá„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Ê®°ÂûãËÆæËÆ°‰∏≠ÔºåÈááÁî®‰∫ÜËΩªÈáèÁ∫ßÁöÑÁΩëÁªúÁªìÊûÑÔºå‰ºòÂåñ‰∫ÜÊçüÂ§±ÂáΩÊï∞‰ª•Âπ≥Ë°°ÂÆâÂÖ®ÊÄß‰∏éÊïàÁéáÔºåÂêåÊó∂ÈÄöËøáËæπÁºòÈÄÇÂ∫îÁ≠ñÁï•Á°Æ‰øùÊ®°ÂûãÂú®ÂÆûÊó∂ÁéØÂ¢É‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

REACTÂú®DeepAccidentÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåËææÂà∞‰∫Ü77%ÁöÑÁ¢∞ÊíûÁéáÈôç‰ΩéÔºåËßÜÈ¢ëÂÖ®ÊôØË¥®ÈáèÔºàVPQÔºâ‰∏∫48.2%ÔºåÊé®ÁêÜÂª∂Ëøü‰ªÖ‰∏∫0.57ÁßíÔºåÊòæÁ§∫Âá∫ÂÖ∂Âú®ÂÆûÊó∂Ëá™Âä®È©æÈ©∂Â∫îÁî®‰∏≠ÁöÑÂº∫Â§ßÊÄßËÉΩ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Ëá™Âä®È©æÈ©∂ËΩ¶ËæÜÁöÑÂÆûÊó∂Á¢∞ÊíûÈÅøÂÖçÁ≥ªÁªü„ÄÅÊô∫ËÉΩ‰∫§ÈÄöÁÆ°ÁêÜÂíåËΩ¶ËÅîÁΩëÔºàV2XÔºâÁ≥ªÁªü„ÄÇÈÄöËøáÊèêÈ´ò‰∫§ÈÄöÂÆâÂÖ®ÊÄßÂíåÂìçÂ∫îÈÄüÂ∫¶ÔºåREACTÊ°ÜÊû∂ËÉΩÂ§üÊòæËëóÈôç‰Ωé‰∫§ÈÄö‰∫ãÊïÖÂèëÁîüÁéáÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÁ§æ‰ºöÂΩ±Âìç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Autonomous driving (AD) systems relying solely on onboard sensors may fail to detect distant or obstacle hazards, potentially causing preventable collisions; however, existing transformer-based Vehicle-to-Everything (V2X) approaches, which mitigate AD sensing limitations, either lack effective multimodal fusion and reasoning or struggle to meet real-time performance requirements under complex, high-dimensional traffic conditions. This paper proposes the Real-time Edge-based Autonomous Co-pilot Trajectory planner (REACT), a V2X-integrated trajectory optimization framework for AD based on a fine-tuned lightweight Vision-Language Model (VLM). REACT integrates infrastructure-provided hazard alerts with onboard sensor data, capturing intricate surrounding traffic dynamics and vehicle intents through visual embeddings, interpreting precise numerical data from symbolic inputs, and employing contextual reasoning to generate optimized, safety-oriented trajectories. To ensure robust real-time deployment on edge devices, REACT innovatively employs Residual Trajectory Fusion (RTF) design and specialized edge-adaptation strategies to reduce model complexity and improve inference efficiency. Evaluated on the DeepAccident benchmark, REACT achieves state-of-the-art performance, a 77% collision rate reduction, a 48.2% Video Panoptic Quality (VPQ), and a 0.57-second inference latency on the Jetson AGX Orin. Ablation studies validate the contribution of each input, module, and edge adaptation strategy. These results highlight the effectiveness of lightweight VLMs in enabling real-time cooperative planning on edge platforms and underscore the potential of language-guided contextual reasoning for improving traffic safety and responsiveness.

