---
layout: default
title: CoLLM-NAS: Collaborative Large Language Models for Efficient Knowledge-Guided Neural Architecture Search
---

# CoLLM-NAS: Collaborative Large Language Models for Efficient Knowledge-Guided Neural Architecture Search

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.26037" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.26037v1</a>
  <a href="https://arxiv.org/pdf/2509.26037.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.26037v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.26037v1', 'CoLLM-NAS: Collaborative Large Language Models for Efficient Knowledge-Guided Neural Architecture Search')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhe Li, Zhiwei Lin, Yongtao Wang

**åˆ†ç±»**: cs.AI, cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCoLLM-NASï¼Œåˆ©ç”¨ååŒå¤§è¯­è¨€æ¨¡å‹è¿›è¡Œé«˜æ•ˆçš„çŸ¥è¯†å¼•å¯¼ç¥ç»æ¶æ„æœç´¢**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç¥ç»æ¶æ„æœç´¢` `å¤§è¯­è¨€æ¨¡å‹` `è‡ªåŠ¨åŒ–æœºå™¨å­¦ä¹ ` `çŸ¥è¯†å¼•å¯¼` `ååŒå­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºLLMçš„NASæ–¹æ³•å­˜åœ¨æ¶æ„æ— æ•ˆã€è®¡ç®—ä½æ•ˆå’Œæ€§èƒ½ä¸è¶³ç­‰é—®é¢˜ï¼Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚
2. CoLLM-NASåˆ©ç”¨å¯¼èˆªLLMæŒ‡å¯¼æœç´¢æ–¹å‘ï¼Œç”Ÿæˆå™¨LLMåˆæˆé«˜è´¨é‡å€™é€‰æ¶æ„ï¼Œå¹¶ç”±åè°ƒå™¨æ¨¡å—ç®¡ç†äº¤äº’ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒCoLLM-NASåœ¨ImageNetå’ŒNAS-Bench-201ä¸Šè¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨å¤šç§æœç´¢ç©ºé—´ä¸­æå‡äº†æ€§èƒ½å’Œæ•ˆç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºååŒå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç¥ç»æ¶æ„æœç´¢ï¼ˆNASï¼‰æ¡†æ¶CoLLM-NASï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•ä¸­å­˜åœ¨çš„æ¶æ„æ— æ•ˆæ€§ã€è®¡ç®—æ•ˆç‡ä½ä¸‹ä»¥åŠæ€§èƒ½ä¸å¦‚ä¼ ç»ŸNASç­‰é—®é¢˜ã€‚CoLLM-NASæ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„NASæ¡†æ¶ï¼Œé€šè¿‡ä¸¤ä¸ªäº’è¡¥çš„LLMè¿›è¡ŒçŸ¥è¯†å¼•å¯¼æœç´¢ã€‚å…·ä½“æ¥è¯´ï¼Œæå‡ºäº†ä¸€ä¸ªå¯¼èˆªLLMæ¥æŒ‡å¯¼æœç´¢æ–¹å‘ï¼Œä»¥åŠä¸€ä¸ªç”Ÿæˆå™¨LLMæ¥åˆæˆé«˜è´¨é‡çš„å€™é€‰æ¶æ„ï¼Œå¹¶ä½¿ç”¨ä¸€ä¸ªä¸“é—¨çš„åè°ƒå™¨æ¨¡å—æ¥ç®¡ç†å®ƒä»¬çš„äº¤äº’ã€‚CoLLM-NASé€šè¿‡ç»“åˆLLMå›ºæœ‰çš„ç»“æ„åŒ–ç¥ç»æ¶æ„çŸ¥è¯†ä»¥åŠæ¥è‡ªè¿­ä»£åé¦ˆå’Œå†å²è½¨è¿¹çš„æ¸è¿›çŸ¥è¯†ï¼Œæœ‰æ•ˆåœ°æŒ‡å¯¼æœç´¢è¿‡ç¨‹ã€‚åœ¨ImageNetå’ŒNAS-Bench-201ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒCoLLM-NASè¶…è¶Šäº†ç°æœ‰çš„NASæ–¹æ³•å’Œä¼ ç»Ÿæœç´¢ç®—æ³•ï¼Œå–å¾—äº†æ–°çš„state-of-the-artç»“æœã€‚æ­¤å¤–ï¼ŒCoLLM-NASæŒç»­æå‡äº†å„ç§ä¸¤é˜¶æ®µNASæ–¹æ³•ï¼ˆä¾‹å¦‚ï¼ŒOFAã€SPOSå’ŒAutoFormerï¼‰åœ¨ä¸åŒæœç´¢ç©ºé—´ï¼ˆä¾‹å¦‚ï¼ŒMobileNetã€ShuffleNetå’ŒAutoFormerï¼‰ä¸Šçš„æ€§èƒ½å’Œæ•ˆç‡ï¼Œå±•ç¤ºäº†å…¶å‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç¥ç»æ¶æ„æœç´¢ï¼ˆNASï¼‰æ–¹æ³•ï¼Œè™½ç„¶åˆ©ç”¨äº†LLMçš„çŸ¥è¯†ï¼Œä½†å¸¸å¸¸ç”Ÿæˆæ— æ•ˆçš„æ¶æ„ï¼Œè®¡ç®—æ•ˆç‡ä½ä¸‹ï¼Œå¹¶ä¸”æœ€ç»ˆæ€§èƒ½ä¸å¦‚ä¼ ç»Ÿçš„NASæ–¹æ³•ã€‚è¿™äº›é—®é¢˜é˜»ç¢äº†LLMåœ¨NASé¢†åŸŸçš„æœ‰æ•ˆåº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCoLLM-NASçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ä¸¤ä¸ªäº’è¡¥çš„LLMï¼Œä¸€ä¸ªè´Ÿè´£å¯¼èˆªæœç´¢æ–¹å‘ï¼ˆNavigator LLMï¼‰ï¼Œå¦ä¸€ä¸ªè´Ÿè´£ç”Ÿæˆé«˜è´¨é‡çš„å€™é€‰æ¶æ„ï¼ˆGenerator LLMï¼‰ï¼Œå¹¶é€šè¿‡ä¸€ä¸ªåè°ƒå™¨æ¨¡å—ï¼ˆCoordinatorï¼‰æ¥ç®¡ç†å®ƒä»¬çš„äº¤äº’ã€‚è¿™ç§ååŒçš„æ–¹å¼æ—¨åœ¨ç»“åˆLLMçš„çŸ¥è¯†å’Œè¿­ä»£åé¦ˆï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°æŒ‡å¯¼æœç´¢è¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCoLLM-NASæ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„NASæ¡†æ¶ã€‚ç¬¬ä¸€é˜¶æ®µï¼ŒNavigator LLMæ ¹æ®å†å²æœç´¢è½¨è¿¹å’Œåé¦ˆï¼ŒæŒ‡å¯¼Generator LLMç”Ÿæˆå€™é€‰æ¶æ„ã€‚ç¬¬äºŒé˜¶æ®µï¼ŒCoordinatoræ¨¡å—è¯„ä¼°è¿™äº›å€™é€‰æ¶æ„ï¼Œå¹¶å°†ç»“æœåé¦ˆç»™Navigator LLMï¼Œç”¨äºä¸‹ä¸€è½®çš„æœç´¢æŒ‡å¯¼ã€‚è¿™ä¸ªè¿‡ç¨‹è¿­ä»£è¿›è¡Œï¼Œç›´åˆ°æ‰¾åˆ°æœ€ä¼˜æ¶æ„ã€‚

**å…³é”®åˆ›æ–°**ï¼šCoLLM-NASçš„å…³é”®åˆ›æ–°åœ¨äºååŒä½¿ç”¨ä¸¤ä¸ªLLMï¼Œå¹¶å¼•å…¥åè°ƒå™¨æ¨¡å—æ¥ç®¡ç†å®ƒä»¬çš„äº¤äº’ã€‚Navigator LLMåˆ©ç”¨å†å²ä¿¡æ¯å’Œåé¦ˆæ¥æŒ‡å¯¼æœç´¢æ–¹å‘ï¼ŒGenerator LLMåˆ™ä¸“æ³¨äºç”Ÿæˆé«˜è´¨é‡çš„å€™é€‰æ¶æ„ã€‚è¿™ç§åˆ†å·¥åˆä½œçš„æ–¹å¼èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨LLMçš„çŸ¥è¯†ï¼Œå¹¶é¿å…ç”Ÿæˆæ— æ•ˆæˆ–ä½æ•ˆçš„æ¶æ„ã€‚

**å…³é”®è®¾è®¡**ï¼šNavigator LLMå’ŒGenerator LLMçš„å…·ä½“é€‰æ‹©å’Œè®­ç»ƒæ–¹å¼æœªçŸ¥ï¼ŒCoordinatoræ¨¡å—çš„è¯„ä¼°ç­–ç•¥ä¹ŸæœªçŸ¥ã€‚è®ºæ–‡ä¸­å¯èƒ½ä½¿ç”¨äº†ç‰¹å®šçš„promptå·¥ç¨‹æŠ€æœ¯æ¥å¼•å¯¼LLMçš„è¡Œä¸ºã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡å¯èƒ½åŒ…æ‹¬å¯¹æ¶æ„æœ‰æ•ˆæ€§å’Œæ€§èƒ½çš„çº¦æŸã€‚å…·ä½“çš„ç½‘ç»œç»“æ„ç»†èŠ‚å–å†³äºæ‰€æœç´¢çš„ç©ºé—´ï¼ˆä¾‹å¦‚ï¼ŒMobileNetã€ShuffleNetã€AutoFormerï¼‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

CoLLM-NASåœ¨ImageNetå’ŒNAS-Bench-201ä¸Šå–å¾—äº†state-of-the-artçš„ç»“æœï¼Œè¶…è¶Šäº†ç°æœ‰çš„NASæ–¹æ³•å’Œä¼ ç»Ÿæœç´¢ç®—æ³•ã€‚æ­¤å¤–ï¼ŒCoLLM-NASèƒ½å¤ŸæŒç»­æå‡å„ç§ä¸¤é˜¶æ®µNASæ–¹æ³•ï¼ˆä¾‹å¦‚ï¼ŒOFAã€SPOSå’ŒAutoFormerï¼‰åœ¨ä¸åŒæœç´¢ç©ºé—´ï¼ˆä¾‹å¦‚ï¼ŒMobileNetã€ShuffleNetå’ŒAutoFormerï¼‰ä¸Šçš„æ€§èƒ½å’Œæ•ˆç‡ï¼Œå±•ç¤ºäº†å…¶å‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›ã€‚å…·ä½“çš„æ€§èƒ½æå‡å¹…åº¦æœªçŸ¥ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

CoLLM-NASå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ç”¨äºè‡ªåŠ¨åŒ–è®¾è®¡å„ç§ç¥ç»ç½‘ç»œæ¶æ„ï¼Œä¾‹å¦‚å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸçš„æ¨¡å‹ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿé™ä½æ¨¡å‹è®¾è®¡çš„é—¨æ§›ï¼ŒåŠ é€Ÿæ–°æ¨¡å‹çš„å¼€å‘ï¼Œå¹¶æé«˜æ¨¡å‹æ€§èƒ½ã€‚æœªæ¥ï¼ŒCoLLM-NASæœ‰æœ›åº”ç”¨äºæ›´å¤æ‚çš„ä»»åŠ¡å’Œæ›´å¤§çš„æœç´¢ç©ºé—´ï¼Œæ¨åŠ¨äººå·¥æ™ºèƒ½æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The integration of Large Language Models (LLMs) with Neural Architecture Search (NAS) has introduced new possibilities for automating the design of neural architectures. However, most existing methods face critical limitations, including architectural invalidity, computational inefficiency, and inferior performance compared to traditional NAS. In this work, we present Collaborative LLM-based NAS (CoLLM-NAS), a two-stage NAS framework with knowledge-guided search driven by two complementary LLMs. Specifically, we propose a Navigator LLM to guide search direction and a Generator LLM to synthesize high-quality candidates, with a dedicated Coordinator module to manage their interaction. CoLLM-NAS efficiently guides the search process by combining LLMs' inherent knowledge of structured neural architectures with progressive knowledge from iterative feedback and historical trajectory. Experimental results on ImageNet and NAS-Bench-201 show that CoLLM-NAS surpasses existing NAS methods and conventional search algorithms, achieving new state-of-the-art results. Furthermore, CoLLM-NAS consistently enhances the performance and efficiency of various two-stage NAS methods (e.g., OFA, SPOS, and AutoFormer) across diverse search spaces (e.g., MobileNet, ShuffleNet, and AutoFormer), demonstrating its excellent generalization.

