---
layout: default
title: Fine-tuning Behavioral Cloning Policies with Preference-Based Reinforcement Learning
---

# Fine-tuning Behavioral Cloning Policies with Preference-Based Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.26605" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.26605v2</a>
  <a href="https://arxiv.org/pdf/2509.26605.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.26605v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.26605v2', 'Fine-tuning Behavioral Cloning Policies with Preference-Based Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: MaÃ«l Macuglia, Paul Friedrich, Giorgia Ramponi

**åˆ†ç±»**: cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30 (æ›´æ–°: 2025-10-13)

**å¤‡æ³¨**: 85 pages (11 + references and appendix), 9 figures. v2: added acknowledgements

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºBRIDGEç®—æ³•ï¼Œç»“åˆç¦»çº¿ä¸“å®¶æ•°æ®ä¸åœ¨çº¿åå¥½å­¦ä¹ å¾®è°ƒæœºå™¨äººç­–ç•¥**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `è¡Œä¸ºå…‹éš†` `åå¥½å­¦ä¹ ` `æœºå™¨äººæ§åˆ¶` `ç¦»çº¿å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ åœ¨æœºå™¨äººç­‰é¢†åŸŸåº”ç”¨å—é˜»ï¼Œä¸»è¦æŒ‘æˆ˜åœ¨äºå¥–åŠ±å‡½æ•°è®¾è®¡å›°éš¾å’Œæ¢ç´¢è¿‡ç¨‹ä¸­çš„å®‰å…¨é£é™©ã€‚
2. è®ºæ–‡æå‡ºBRIDGEç®—æ³•ï¼Œç»“åˆç¦»çº¿ä¸“å®¶æ•°æ®å­¦ä¹ åˆå§‹ç­–ç•¥ï¼Œå†åˆ©ç”¨åœ¨çº¿åå¥½åé¦ˆè¿›è¡Œå¾®è°ƒï¼Œæå‡ç­–ç•¥æ€§èƒ½ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒBRIDGEç®—æ³•åœ¨MuJoCoç¯å¢ƒä¸­ä¼˜äºå•ç‹¬çš„è¡Œä¸ºå…‹éš†å’Œåœ¨çº¿åå¥½å­¦ä¹ ï¼Œé™ä½äº†åæ‚”å€¼ï¼Œæé«˜äº†æ ·æœ¬æ•ˆç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨æœºå™¨äººã€å·¥ä¸šå’ŒåŒ»ç–—ä¿å¥é¢†åŸŸéƒ¨ç½²å¼ºåŒ–å­¦ä¹ (RL)é¢ä¸´ä¸¤å¤§éšœç¢ï¼šéš¾ä»¥æŒ‡å®šç²¾ç¡®çš„å¥–åŠ±å‡½æ•°ï¼Œä»¥åŠä¸å®‰å…¨ä¸”æ•°æ®éœ€æ±‚é‡å¤§çš„æ¢ç´¢é£é™©ã€‚æœ¬æ–‡æå‡ºä¸€ä¸ªä¸¤é˜¶æ®µæ¡†æ¶æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè¯¥æ¡†æ¶é¦–å…ˆä»æ— å¥–åŠ±çš„ä¸“å®¶æ¼”ç¤ºæ•°æ®é›†ä¸­å­¦ä¹ ä¸€ä¸ªå®‰å…¨çš„åˆå§‹ç­–ç•¥ï¼Œç„¶åä½¿ç”¨åŸºäºåå¥½çš„äººå·¥åé¦ˆåœ¨çº¿å¾®è°ƒè¯¥ç­–ç•¥ã€‚æˆ‘ä»¬å¯¹è¿™ç§ç¦»çº¿åˆ°åœ¨çº¿çš„æ–¹æ³•è¿›è¡Œäº†é¦–æ¬¡åŸç†æ€§åˆ†æï¼Œå¹¶å¼•å…¥äº†BRIDGEï¼Œä¸€ç§é€šè¿‡ä¸ç¡®å®šæ€§åŠ æƒç›®æ ‡æ•´åˆä¸¤ç§ä¿¡å·çš„ç»Ÿä¸€ç®—æ³•ã€‚æˆ‘ä»¬æ¨å¯¼äº†éšç¦»çº¿æ¼”ç¤ºæ•°é‡å‡å°‘çš„åæ‚”ç•Œé™ï¼Œæ˜ç¡®åœ°å°†ç¦»çº¿æ•°æ®çš„æ•°é‡ä¸åœ¨çº¿æ ·æœ¬æ•ˆç‡è”ç³»èµ·æ¥ã€‚æˆ‘ä»¬åœ¨ç¦»æ•£å’Œè¿ç»­æ§åˆ¶çš„MuJoCoç¯å¢ƒä¸­éªŒè¯äº†BRIDGEï¼Œè¡¨æ˜å®ƒæ¯”ç‹¬ç«‹çš„è¡Œä¸ºå…‹éš†å’Œåœ¨çº¿åŸºäºåå¥½çš„RLå®ç°äº†æ›´ä½çš„åæ‚”å€¼ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºè®¾è®¡æ›´å…·æ ·æœ¬æ•ˆç‡çš„äº¤äº’å¼æ™ºèƒ½ä½“å¥ å®šäº†ç†è®ºåŸºç¡€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨æœºå™¨äººæ§åˆ¶ç­‰é¢†åŸŸåº”ç”¨æ—¶ï¼Œé¢ä¸´å¥–åŠ±å‡½æ•°éš¾ä»¥ç²¾ç¡®è®¾è®¡çš„é—®é¢˜ï¼Œå¯¼è‡´å­¦ä¹ åˆ°çš„ç­–ç•¥å¯èƒ½ä¸ç¬¦åˆé¢„æœŸç”šè‡³å­˜åœ¨å®‰å…¨éšæ‚£ã€‚åŒæ—¶ï¼Œä»é›¶å¼€å§‹çš„æ¢ç´¢å¼å­¦ä¹ éœ€è¦å¤§é‡æ•°æ®ï¼Œæ•ˆç‡ä½ä¸‹ï¼Œä¸”å¯èƒ½å­˜åœ¨ä¸å®‰å…¨è¡Œä¸ºã€‚è¡Œä¸ºå…‹éš†è™½ç„¶å¯ä»¥ä»ä¸“å®¶æ•°æ®ä¸­å­¦ä¹ ï¼Œä½†æ— æ³•è¶…è¶Šä¸“å®¶æ°´å¹³ï¼Œä¸”å¯¹æ•°æ®è´¨é‡è¦æ±‚é«˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯ç»“åˆç¦»çº¿è¡Œä¸ºå…‹éš†å’Œåœ¨çº¿åå¥½å­¦ä¹ çš„ä¼˜ç‚¹ã€‚é¦–å…ˆï¼Œåˆ©ç”¨ç¦»çº¿ä¸“å®¶æ•°æ®å­¦ä¹ ä¸€ä¸ªå®‰å…¨çš„åˆå§‹ç­–ç•¥ï¼Œé¿å…ä»é›¶å¼€å§‹æ¢ç´¢å¸¦æ¥çš„é£é™©ã€‚ç„¶åï¼Œé€šè¿‡åœ¨çº¿åå¥½å­¦ä¹ ï¼Œåˆ©ç”¨äººç±»åé¦ˆå¯¹ç­–ç•¥è¿›è¡Œå¾®è°ƒï¼Œå…‹æœè¡Œä¸ºå…‹éš†æ— æ³•è¶…è¶Šä¸“å®¶æ°´å¹³çš„å±€é™æ€§ï¼Œå¹¶è§£å†³å¥–åŠ±å‡½æ•°è®¾è®¡å›°éš¾çš„é—®é¢˜ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šBRIDGEç®—æ³•åŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼š1) ç¦»çº¿è¡Œä¸ºå…‹éš†é˜¶æ®µï¼šåˆ©ç”¨ä¸“å®¶æ¼”ç¤ºæ•°æ®è®­ç»ƒä¸€ä¸ªåˆå§‹ç­–ç•¥ï¼Œè¯¥ç­–ç•¥ä½œä¸ºåç»­åœ¨çº¿å¾®è°ƒçš„åŸºç¡€ã€‚2) åœ¨çº¿åå¥½å­¦ä¹ é˜¶æ®µï¼šæ™ºèƒ½ä½“ä¸ç¯å¢ƒäº¤äº’ï¼Œå¹¶å‘äººç±»å±•ç¤ºä¸åŒçš„è½¨è¿¹ç‰‡æ®µã€‚äººç±»æ ¹æ®åå¥½å¯¹è¿™äº›ç‰‡æ®µè¿›è¡Œæ’åºï¼Œæ™ºèƒ½ä½“æ ¹æ®è¿™äº›åå¥½ä¿¡æ¯æ›´æ–°ç­–ç•¥ã€‚BRIDGEç®—æ³•çš„å…³é”®åœ¨äºå¦‚ä½•å°†ç¦»çº¿è¡Œä¸ºå…‹éš†å’Œåœ¨çº¿åå¥½å­¦ä¹ çš„ä¿¡æ¯æœ‰æ•ˆåœ°ç»“åˆèµ·æ¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šBRIDGEç®—æ³•çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„ç›®æ ‡å‡½æ•°ï¼Œè¯¥å‡½æ•°é€šè¿‡ä¸ç¡®å®šæ€§åŠ æƒçš„æ–¹å¼æ•´åˆäº†ç¦»çº¿è¡Œä¸ºå…‹éš†å’Œåœ¨çº¿åå¥½å­¦ä¹ çš„ä¿¡æ¯ã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹äºç¦»çº¿æ•°æ®ï¼Œä½¿ç”¨è¡Œä¸ºå…‹éš†æŸå¤±å‡½æ•°è¿›è¡Œçº¦æŸï¼›å¯¹äºåœ¨çº¿åå¥½æ•°æ®ï¼Œä½¿ç”¨åŸºäºåå¥½çš„å¼ºåŒ–å­¦ä¹ æŸå¤±å‡½æ•°è¿›è¡Œä¼˜åŒ–ã€‚ä¸ç¡®å®šæ€§åŠ æƒæœºåˆ¶æ ¹æ®ç¦»çº¿æ•°æ®çš„æ•°é‡å’Œè´¨é‡ï¼Œä»¥åŠåœ¨çº¿åå¥½åé¦ˆçš„ç½®ä¿¡åº¦ï¼ŒåŠ¨æ€è°ƒæ•´ä¸¤ç§æŸå¤±å‡½æ•°çš„æƒé‡ï¼Œä»è€Œå®ç°æ›´æœ‰æ•ˆçš„ç­–ç•¥å­¦ä¹ ã€‚

**å…³é”®è®¾è®¡**ï¼šBRIDGEç®—æ³•ä½¿ç”¨äº†ä¸€ç§åŸºäºé«˜æ–¯è¿‡ç¨‹çš„åå¥½æ¨¡å‹æ¥å»ºæ¨¡äººç±»çš„åå¥½ã€‚è¯¥æ¨¡å‹å¯ä»¥ä¼°è®¡æ¯ä¸ªè½¨è¿¹ç‰‡æ®µçš„å¥–åŠ±å€¼ï¼Œå¹¶æä¾›ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚åœ¨ç›®æ ‡å‡½æ•°ä¸­ï¼Œç¦»çº¿è¡Œä¸ºå…‹éš†æŸå¤±å‡½æ•°çš„æƒé‡ä¸ç¦»çº¿æ•°æ®çš„æ•°é‡æˆæ­£æ¯”ï¼Œä¸é«˜æ–¯è¿‡ç¨‹åå¥½æ¨¡å‹çš„ä¸ç¡®å®šæ€§æˆåæ¯”ã€‚åœ¨çº¿åå¥½å­¦ä¹ æŸå¤±å‡½æ•°çš„æƒé‡åˆ™ä¸é«˜æ–¯è¿‡ç¨‹åå¥½æ¨¡å‹çš„ç½®ä¿¡åº¦æˆæ­£æ¯”ã€‚ç­–ç•¥ç½‘ç»œå¯ä»¥ä½¿ç”¨ä»»ä½•æ ‡å‡†çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ æ¶æ„ï¼Œä¾‹å¦‚Actor-Criticç½‘ç»œã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒBRIDGEç®—æ³•åœ¨ç¦»æ•£å’Œè¿ç»­æ§åˆ¶çš„MuJoCoç¯å¢ƒä¸­å‡ä¼˜äºå•ç‹¬çš„è¡Œä¸ºå…‹éš†å’Œåœ¨çº¿åå¥½å­¦ä¹ æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼ŒBRIDGEç®—æ³•å®ç°äº†æ›´ä½çš„åæ‚”å€¼ï¼Œè¡¨æ˜å…¶å­¦ä¹ åˆ°çš„ç­–ç•¥æ›´æ¥è¿‘æœ€ä¼˜ç­–ç•¥ã€‚æ­¤å¤–ï¼ŒBRIDGEç®—æ³•çš„æ ·æœ¬æ•ˆç‡æ›´é«˜ï¼Œå³åœ¨ç›¸åŒæ•°é‡çš„äº¤äº’æ¬¡æ•°ä¸‹ï¼Œèƒ½å¤Ÿå­¦ä¹ åˆ°æ›´å¥½çš„ç­–ç•¥ã€‚è¿™äº›ç»“æœéªŒè¯äº†BRIDGEç®—æ³•çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—è¾…åŠ©ç­‰é¢†åŸŸã€‚é€šè¿‡ç»“åˆç¦»çº¿ä¸“å®¶æ•°æ®å’Œåœ¨çº¿äººç±»åé¦ˆï¼Œå¯ä»¥æ›´å®‰å…¨ã€é«˜æ•ˆåœ°è®­ç»ƒæ™ºèƒ½ä½“ï¼Œè§£å†³å¤æ‚ä»»åŠ¡ä¸­çš„å¥–åŠ±å‡½æ•°è®¾è®¡éš¾é¢˜ã€‚ä¾‹å¦‚ï¼Œåœ¨åŒ»ç–—æœºå™¨äººæ‰‹æœ¯ä¸­ï¼Œå¯ä»¥å…ˆä»åŒ»ç”Ÿæ¼”ç¤ºæ•°æ®ä¸­å­¦ä¹ åŸºæœ¬æ“ä½œï¼Œå†é€šè¿‡åŒ»ç”Ÿåœ¨çº¿åé¦ˆè¿›è¡Œå¾®è°ƒï¼Œæœ€ç»ˆå®ç°æ›´ç²¾å‡†çš„æ‰‹æœ¯æ“ä½œã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Deploying reinforcement learning (RL) in robotics, industry, and health care is blocked by two obstacles: the difficulty of specifying accurate rewards and the risk of unsafe, data-hungry exploration. We address this by proposing a two-stage framework that first learns a safe initial policy from a reward-free dataset of expert demonstrations, then fine-tunes it online using preference-based human feedback. We provide the first principled analysis of this offline-to-online approach and introduce BRIDGE, a unified algorithm that integrates both signals via an uncertainty-weighted objective. We derive regret bounds that shrink with the number of offline demonstrations, explicitly connecting the quantity of offline data to online sample efficiency. We validate BRIDGE in discrete and continuous control MuJoCo environments, showing it achieves lower regret than both standalone behavioral cloning and online preference-based RL. Our work establishes a theoretical foundation for designing more sample-efficient interactive agents.

