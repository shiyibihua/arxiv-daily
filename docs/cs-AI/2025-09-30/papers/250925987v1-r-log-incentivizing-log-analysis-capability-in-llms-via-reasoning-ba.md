---
layout: default
title: R-Log: Incentivizing Log Analysis Capability in LLMs via Reasoning-based Reinforcement Learning
---

# R-Log: Incentivizing Log Analysis Capability in LLMs via Reasoning-based Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25987" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25987v1</a>
  <a href="https://arxiv.org/pdf/2509.25987.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25987v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25987v1', 'R-Log: Incentivizing Log Analysis Capability in LLMs via Reasoning-based Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yilun Liu, Ziang Chen, Song Xu, Minggui He, Shimin Tao, Weibin Meng, Yuming Xie, Tao Han, Chunguang Zhao, Jingzhou Du, Daimeng Wei, Shenglin Zhang, Yongqian Sun

**åˆ†ç±»**: cs.SE, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºR-Logä»¥è§£å†³LLMsåœ¨æ—¥å¿—åˆ†æä¸­çš„èƒ½åŠ›ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ—¥å¿—åˆ†æ` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨ç†å­¦ä¹ ` `å¼ºåŒ–å­¦ä¹ ` `è‡ªåŠ¨åŒ–è¿ç»´` `æ¨¡å‹ä¼˜åŒ–` `æ•°æ®é©±åŠ¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ—¥å¿—åˆ†ææ–¹æ³•ä¾èµ–äºç›´æ¥ç›‘ç£å¾®è°ƒï¼Œå¯¼è‡´é€šç”¨LLMsä¸ä¸“ç”¨æ—¥å¿—æ•°æ®ä¹‹é—´çš„é¢†åŸŸå·®å¼‚ï¼Œé€ æˆè¿‡æ‹Ÿåˆå’Œå¹»è§‰ç°è±¡ã€‚
2. R-Logé€šè¿‡æ¨¡ä»¿äººç±»çš„é€æ­¥åˆ†æè¿‡ç¨‹ï¼Œç»“åˆå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ¨¡å‹ï¼Œæ—¨åœ¨æé«˜æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œæ³›åŒ–èƒ½åŠ›ã€‚
3. å®éªŒè¯æ˜ï¼ŒR-Logåœ¨äº”ä¸ªæ—¥å¿—åˆ†æä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨æœªè§åœºæ™¯ä¸­æå‡å¹…åº¦è¾¾åˆ°228.05%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°ä»£è½¯ä»¶ç³»ç»Ÿä¸­æ—¥å¿—æ•°æ®çš„å¤æ‚æ€§æ—¥ç›Šå¢åŠ ï¼Œä¿ƒä½¿ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œè‡ªåŠ¨åŒ–æ—¥å¿—åˆ†æã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå¯¹æ—¥å¿—æ ‡ç­¾å¯¹çš„ç›´æ¥ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œè¿™åŠ å‰§äº†é€šç”¨LLMsä¸ä¸“ç”¨æ—¥å¿—æ•°æ®ä¹‹é—´çš„é¢†åŸŸå·®å¼‚ï¼Œå¯¼è‡´è¿‡æ‹Ÿåˆã€‚æ­¤å¤–ï¼ŒSFTçš„ä¸å¹³è¡¡æŸå¤±è®¡ç®—å¸¸å¸¸ä½¿å¾—å†—é•¿çš„ä¸Šä¸‹æ–‡å‹å€’æ¨¡å‹ç­”æ¡ˆä¸­çš„å…³é”®ç®€æ´ç»†èŠ‚ï¼Œå¯¼è‡´å¹»è§‰ç°è±¡ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†R-Logï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ¨ç†çš„èŒƒå¼ï¼Œæ¨¡æ‹Ÿäººç±»å·¥ç¨‹å¸ˆçš„ç»“æ„åŒ–é€æ­¥åˆ†æè¿‡ç¨‹ã€‚é€šè¿‡åœ¨æ¨¡æ‹Ÿçš„è¿ç»´ç¯å¢ƒä¸­ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¼˜åŒ–æ¨¡å‹ï¼Œç›´æ¥å¥–åŠ±æ­£ç¡®ç»“æœï¼Œä»è€Œå‡å°‘å¹»è§‰ã€‚å®è¯è¯„ä¼°è¡¨æ˜ï¼ŒR-Logåœ¨äº”ä¸ªæ—¥å¿—åˆ†æä»»åŠ¡ä¸­ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨æœªè§åœºæ™¯ä¸­æå‡è¾¾228.05%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰æ—¥å¿—åˆ†ææ–¹æ³•ä¸­LLMsçš„èƒ½åŠ›ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯è¿‡æ‹Ÿåˆå’Œå¹»è§‰ç°è±¡çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šè¿‡ç›´æ¥ç›‘ç£å¾®è°ƒï¼Œå¯¼è‡´æ¨¡å‹åœ¨å¤„ç†ä¸“ç”¨æ—¥å¿—æ•°æ®æ—¶è¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šR-Logçš„æ ¸å¿ƒæ€è·¯æ˜¯æ¨¡ä»¿äººç±»å·¥ç¨‹å¸ˆçš„æ¨ç†è¿‡ç¨‹ï¼Œé€šè¿‡ç»“æ„åŒ–çš„åˆ†ææ­¥éª¤æ¥æé«˜æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ¨¡å‹ï¼Œç›´æ¥å¥–åŠ±æ­£ç¡®çš„åˆ†æç»“æœï¼Œä»è€Œå‡å°‘å¹»è§‰ç°è±¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šR-Logçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆåœ¨ä¸€ä¸ªç²¾å¿ƒç­–åˆ’çš„æ•°æ®é›†ä¸Šè¿›è¡Œå†·å¯åŠ¨ï¼Œå»ºç«‹åˆå§‹æ¨ç†èƒ½åŠ›ï¼›ç„¶åé€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹ï¼Œä½¿ç”¨è”åˆå¥–åŠ±å‡½æ•°æ¥æå‡æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šR-Logçš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥äº†åŸºäºæ¨ç†çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼ŒåŒºåˆ«äºä¼ ç»Ÿçš„ç›´æ¥ç›‘ç£å¾®è°ƒï¼Œèƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”å¤æ‚çš„æ—¥å¿—æ•°æ®åˆ†æä»»åŠ¡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œä½¿ç”¨äº†2k+çš„æ¨ç†è½¨è¿¹æ•°æ®é›†ï¼Œå¹¶ç»“åˆ13ç§æ‰‹åŠ¨è¿ç»´ç­–ç•¥è¿›è¡ŒæŒ‡å¯¼ã€‚æŸå¤±å‡½æ•°è®¾è®¡ä¸Šï¼Œé‡‡ç”¨äº†è”åˆå¥–åŠ±æœºåˆ¶ï¼Œä»¥å¹³è¡¡æ¨¡å‹å¯¹å…³é”®ç»†èŠ‚çš„å…³æ³¨ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

R-Logåœ¨äº”ä¸ªæ—¥å¿—åˆ†æä»»åŠ¡ä¸­çš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ¨¡å‹åœ¨æœªè§åœºæ™¯ä¸­çš„æ€§èƒ½æå‡è¾¾228.05%ã€‚æ­¤å¤–ï¼ŒR-Log-fastç‰ˆæœ¬å®ç°äº†5å€çš„é€Ÿåº¦æå‡ï¼ŒåŒæ—¶ä¿æŒäº†93%çš„æ•ˆæœï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å®é™…åº”ç”¨ä¸­çš„é«˜æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

R-Logçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨è½¯ä»¶è¿ç»´ã€æ•…éšœæ£€æµ‹å’Œç³»ç»Ÿç›‘æ§ç­‰åœºæ™¯ä¸­ã€‚é€šè¿‡æé«˜æ—¥å¿—åˆ†æçš„è‡ªåŠ¨åŒ–å’Œå‡†ç¡®æ€§ï¼ŒR-Logèƒ½å¤Ÿå¸®åŠ©å·¥ç¨‹å¸ˆæ›´é«˜æ•ˆåœ°å¤„ç†å¤æ‚çš„æ—¥å¿—æ•°æ®ï¼Œä»è€Œæå‡ç³»ç»Ÿçš„å¯é æ€§å’Œç»´æŠ¤æ•ˆç‡ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯å¯èƒ½ä¼šæ‰©å±•åˆ°å…¶ä»–éœ€è¦å¤æ‚æ•°æ®åˆ†æçš„é¢†åŸŸï¼Œå¦‚ç½‘ç»œå®‰å…¨å’Œå¤§æ•°æ®åˆ†æã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The growing complexity of log data in modern software systems has prompted the use of Large Language Models (LLMs) for automated log analysis. Current approaches typically rely on direct supervised fine-tuning (SFT) on log-label pairs. However, this exacerbates the domain discrepancy between general-purpose LLMs and specialized log data, causing overfitting. Furthermore, SFT's imbalanced loss computation often allows lengthy contexts to overwhelm critical, concise details in model answers, leading to hallucinations. To address these limitations, we propose R-Log, a novel reasoning-based paradigm that mirrors the structured, step-by-step analytical process of human engineers. This approach enhances generalizability by learning the underlying rules behind conclusions. We further employ Reinforcement Learning (RL) to optimize the model within a simulated O&M environment, thereby reducing hallucinations by directly rewarding correct outcomes. R-Log is first cold-started on a curated dataset of 2k+ reasoning trajectories, guided by 13 strategies from manual O&M practices, to establish an initial reasoning capability. This ability is then refined via RL using a joint reward function. Empirical evaluations on real-world logs show that R-Log outperforms existing methods across five log analysis tasks, particularly in unseen scenarios (by 228.05%). We also designed R-Log-fast with 5x speedup while keeping 93% of the efficacy.

