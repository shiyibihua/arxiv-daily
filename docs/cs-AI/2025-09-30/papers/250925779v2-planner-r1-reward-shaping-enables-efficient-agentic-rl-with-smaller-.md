---
layout: default
title: Planner-R1: Reward Shaping Enables Efficient Agentic RL with Smaller LLMs
---

# Planner-R1: Reward Shaping Enables Efficient Agentic RL with Smaller LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25779" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25779v2</a>
  <a href="https://arxiv.org/pdf/2509.25779.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25779v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25779v2', 'Planner-R1: Reward Shaping Enables Efficient Agentic RL with Smaller LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Siyu Zhu, Yanbin Jiang, Hejian Sang, Shao Tang, Qingquan Song, Biao He, Rohit Jain, Zhipeng Wang, Alborz Geramifard

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30 (æ›´æ–°: 2025-10-01)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPlanner-R1ä»¥æå‡å°å‹LLMåœ¨Agentic RLä¸­çš„æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¥–åŠ±å¡‘é€ ` `Agentic RL` `å°å‹è¯­è¨€æ¨¡å‹` `è®¡ç®—æ•ˆç‡` `å†…å­˜æ•ˆç‡` `æ™ºèƒ½è§„åˆ’` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨Agentic RLä¸­å¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¾èµ–å¯¼è‡´è®¡ç®—å’Œå†…å­˜æ•ˆç‡ä½ä¸‹ã€‚
2. æå‡ºçš„Planner-R1é€šè¿‡å¥–åŠ±å¡‘é€ æŠ€æœ¯ï¼Œæ˜¾è‘—æå‡äº†å°å‹æ¨¡å‹åœ¨Agentic RLä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œ8Bæ¨¡å‹åœ¨æ•ˆç‡å’Œæ€§èƒ½ä¸Šå‡ä¼˜äº32Bæ¨¡å‹ï¼Œä¸”åœ¨å¤šä¸ªä»»åŠ¡ä¸Šä¿æŒäº†è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬åœ¨TravelPlanneråŸºå‡†ä¸Šç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„Agentic RLã€‚æˆ‘ä»¬çš„æ–¹æ³•Planner-R1åœ¨ä»…180ä¸ªè®­ç»ƒæŸ¥è¯¢çš„æƒ…å†µä¸‹è¾¾åˆ°äº†56.9%çš„æœ€ç»ˆé€šè¿‡ç‡ï¼Œæ¯”GPT-5çš„21.2%åŸºçº¿æé«˜äº†2.7å€ï¼Œå¹¶åœ¨å…¬å…±æ’è¡Œæ¦œä¸Šå–å¾—äº†æœ€å¼ºçš„Agenticç»“æœã€‚ç ”ç©¶å‘ç°ï¼Œå°å‹æ¨¡å‹ï¼ˆ8Bï¼‰å¯¹å¥–åŠ±å¡‘é€ é«˜åº¦æ•æ„Ÿï¼šåœ¨å¯†é›†çš„è¿‡ç¨‹çº§ä¿¡å·ä¸‹ï¼Œå®ƒä»¬çš„æ€§èƒ½å…·æœ‰ç«äº‰åŠ›ï¼ŒåŒæ—¶åœ¨è®¡ç®—æ•ˆç‡ä¸Šæ¯”32Bæ¨¡å‹é«˜å‡º3.5å€ï¼Œå†…å­˜æ•ˆç‡é«˜å‡º1.5å€ã€‚å°½ç®¡è¾ƒå¤§æ¨¡å‹åœ¨ç¨€ç–å¥–åŠ±ä¸‹æ›´ä¸ºç¨³å¥ï¼Œä½†å®ƒä»¬ä»å¥–åŠ±å¡‘é€ ä¸­è·å¾—çš„ç›¸å¯¹å¢ç›Šè¾ƒå°ï¼Œä¸”è¿è¡Œé—´æ–¹å·®è¾ƒé«˜ã€‚é‡è¦çš„æ˜¯ï¼Œè¿™äº›æå‡å¹¶æœªä»¥è¿‡æ‹Ÿåˆä¸ºä»£ä»·ï¼Œå¾®è°ƒåçš„æ¨¡å‹åœ¨å¤šä¸ªé¢†åŸŸå¤–ä»»åŠ¡ä¸Šå¤§å¤šä¿æŒæˆ–è¶…è¶ŠåŸºçº¿æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³åœ¨Agentic RLä¸­ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ—¶çš„è®¡ç®—å’Œå†…å­˜æ•ˆç‡é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºæ›´å¤§çš„æ¨¡å‹ï¼Œå¯¼è‡´èµ„æºæ¶ˆè€—è¿‡é«˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„Planner-R1æ–¹æ³•é€šè¿‡å¥–åŠ±å¡‘é€ æ¥æå‡å°å‹æ¨¡å‹çš„å­¦ä¹ æ•ˆç‡ï¼Œä½¿å…¶åœ¨Agentic RLä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚é€šè¿‡å¯†é›†çš„è¿‡ç¨‹çº§ä¿¡å·ï¼Œå°å‹æ¨¡å‹èƒ½å¤Ÿåœ¨è¾ƒå°‘çš„è®­ç»ƒæŸ¥è¯¢ä¸‹å®ç°ç«äº‰åŠ›çš„æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¥–åŠ±å¡‘é€ æ¨¡å—å’Œæ¨¡å‹è®­ç»ƒæ¨¡å—ã€‚å¥–åŠ±å¡‘é€ æ¨¡å—è´Ÿè´£ç”Ÿæˆè¿‡ç¨‹çº§ä¿¡å·ï¼Œè€Œæ¨¡å‹è®­ç»ƒæ¨¡å—åˆ™åˆ©ç”¨è¿™äº›ä¿¡å·è¿›è¡Œé«˜æ•ˆçš„å­¦ä¹ ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºé€šè¿‡å¥–åŠ±å¡‘é€ æ˜¾è‘—æå‡å°å‹æ¨¡å‹çš„å­¦ä¹ åŠ¨æ€ï¼Œä½¿å…¶åœ¨è®¡ç®—å’Œå†…å­˜ä½¿ç”¨ä¸Šæ›´ä¸ºé«˜æ•ˆã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿä¾èµ–å¤§å‹æ¨¡å‹çš„æ–¹å¼å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œä½¿ç”¨äº†8Bçš„å°å‹æ¨¡å‹ï¼Œå¹¶é€šè¿‡å¯†é›†çš„å¥–åŠ±ä¿¡å·è¿›è¡Œè®­ç»ƒã€‚æŸå¤±å‡½æ•°è®¾è®¡ä¸Šï¼Œå¼ºè°ƒäº†è¿‡ç¨‹çº§ä¿¡å·çš„å¼•å…¥ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPlanner-R1åœ¨TravelPlanneråŸºå‡†ä¸Šè¾¾åˆ°äº†56.9%çš„æœ€ç»ˆé€šè¿‡ç‡ï¼Œæ¯”GPT-5çš„21.2%åŸºçº¿æé«˜äº†2.7å€ã€‚8Bæ¨¡å‹åœ¨è®¡ç®—å’Œå†…å­˜æ•ˆç‡ä¸Šåˆ†åˆ«æ¯”32Bæ¨¡å‹é«˜å‡º3.5å€å’Œ1.5å€ï¼Œä¸”åœ¨å¤šä¸ªé¢†åŸŸå¤–ä»»åŠ¡ä¸Šä¿æŒäº†è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨åŒ–è§„åˆ’å’Œå†³ç­–æ”¯æŒç³»ç»Ÿç­‰ã€‚é€šè¿‡æå‡å°å‹æ¨¡å‹çš„æ•ˆç‡ï¼ŒPlanner-R1å¯ä»¥åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­å®ç°æ›´é«˜æ•ˆçš„Agentic RLåº”ç”¨ï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„æ™®åŠä¸å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We investigated Agentic RL with large language models on the \textsc{TravelPlanner} benchmark. Our approach, \textsc{Planner-R1}, achieved a \textbf{56.9\%} final-pass rate with only 180 training queries, a $2.7\times$ improvement over GPT-5's $21.2\%$ baseline and the strongest agentic result on the public leaderboard. A central finding was that smaller models (8B) were highly responsive to reward shaping: with dense process-level signals, they reached competitive performance while being $3.5\times$ more compute-efficient and $1.5\times$ more memory-efficient than 32B models. Larger models were more robust under sparse rewards but exhibited smaller relative gains from shaping and higher variance across runs. While curriculum learning offered no significant benefit, shaped rewards consistently amplified learning dynamics, making 8B models the most efficient setting for agentic RL. Crucially, these gains did not come at the cost of overfitting: fine-tuned models mostly maintained or exceeded baseline performance on out-of-domain tasks, including \textsc{Multi-IF}, \textsc{NaturalPlan}, and $Ï„$-\textsc{Bench}. These results establish reward shaping as a decisive lever for scaling agentic RL, highlight the competitive strength of smaller models, and demonstrate that efficiency can be achieved without sacrificing generalization.

