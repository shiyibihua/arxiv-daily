---
layout: default
title: The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain
---

# The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.26507" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.26507v1</a>
  <a href="https://arxiv.org/pdf/2509.26507.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.26507v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.26507v1', 'The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Adrian Kosowski, PrzemysÅ‚aw UznaÅ„ski, Jan Chorowski, Zuzanna Stamirowska, MichaÅ‚ Bartoszkiewicz

**åˆ†ç±»**: cs.NE, cs.AI, cs.LG, stat.ML

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

**å¤‡æ³¨**: Code available at: https://github.com/pathwaycom/bdh Accompanying blog: https://pathway.com/research/bdh

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDragon Hatchlingï¼šä¸€ç§å—ç”Ÿç‰©å¯å‘çš„ã€å¯è§£é‡Šçš„ç±»Transformerè¯­è¨€æ¨¡å‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç”Ÿç‰©å¯å‘ç¥ç»ç½‘ç»œ` `å¯è§£é‡Šæ€§AI` `å¤§å‹è¯­è¨€æ¨¡å‹` `èµ«å¸ƒå­¦ä¹ ` `è„‰å†²ç¥ç»å…ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨é€šç”¨æ¨ç†æ–¹é¢å­˜åœ¨ç“¶é¢ˆï¼Œéš¾ä»¥åƒå¤§è„‘ä¸€æ ·è¿›è¡Œæ—¶é—´æ³›åŒ–ã€‚
2. Dragon Hatchling (BDH) æ˜¯ä¸€ç§åŸºäºç”Ÿç‰©å¯å‘ç½‘ç»œçš„è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨å®ç° Transformer çº§åˆ«çš„æ€§èƒ½ï¼ŒåŒæ—¶æä¾›å†…åœ¨çš„å¯è§£é‡Šæ€§ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒBDH åœ¨è¯­è¨€å’Œç¿»è¯‘ä»»åŠ¡ä¸Šä¸ GPT2 æ€§èƒ½ç›¸å½“ï¼Œä¸”å…·æœ‰ç”Ÿç‰©åˆç†æ€§å’Œå¯è§£é‡Šæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹æ¶æ„â€œDragon Hatchlingâ€(BDH)ï¼Œå®ƒåŸºäºä¸€ä¸ªå…·æœ‰å°ºåº¦ä¸å˜æ€§çš„ã€å—ç”Ÿç‰©å­¦å¯å‘çš„ç¥ç»å…ƒç²’å­ç½‘ç»œï¼Œè¿™äº›ç²’å­è¿›è¡Œå±€éƒ¨äº¤äº’ã€‚BDHç»“åˆäº†å¼ºå¤§çš„ç†è®ºåŸºç¡€å’Œå†…åœ¨çš„å¯è§£é‡Šæ€§ï¼ŒåŒæ—¶ä¸ç‰ºç‰²ç±»ä¼¼Transformerçš„æ€§èƒ½ã€‚BDHæ˜¯ä¸€ç§å®ç”¨çš„ã€é«˜æ€§èƒ½çš„ã€åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„çŠ¶æ€ç©ºé—´åºåˆ—å­¦ä¹ æ¶æ„ã€‚é™¤äº†ä½œä¸ºä¸€ä¸ªå›¾æ¨¡å‹ï¼ŒBDHè¿˜å…·æœ‰GPUå‹å¥½çš„å…¬å¼ã€‚å®ƒè¡¨ç°å‡ºç±»ä¼¼Transformerçš„ç¼©æ”¾è§„å¾‹ï¼šç»éªŒè¡¨æ˜ï¼Œå¯¹äºç›¸åŒçš„è®­ç»ƒæ•°æ®ï¼Œåœ¨ç›¸åŒæ•°é‡çš„å‚æ•°ï¼ˆ1000ä¸‡åˆ°10äº¿ï¼‰ä¸‹ï¼ŒBDHåœ¨è¯­è¨€å’Œç¿»è¯‘ä»»åŠ¡ä¸Šä¸GPT2çš„æ€§èƒ½ç›¸å½“ã€‚BDHå¯ä»¥è¢«è¡¨ç¤ºä¸ºä¸€ä¸ªå¤§è„‘æ¨¡å‹ã€‚BDHåœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„å·¥ä½œè®°å¿†å®Œå…¨ä¾èµ–äºå…·æœ‰èµ«å¸ƒå­¦ä¹ çš„çªè§¦å¯å¡‘æ€§ï¼Œä½¿ç”¨è„‰å†²ç¥ç»å…ƒã€‚ç»éªŒè¯å®ï¼Œå½“BDHåœ¨å¤„ç†è¯­è¨€è¾“å…¥æ—¶å¬åˆ°æˆ–æ¨ç†å…³äºç‰¹å®šæ¦‚å¿µæ—¶ï¼Œç‰¹å®šçš„ã€å•ç‹¬çš„çªè§¦ä¼šåŠ å¼ºè¿æ¥ã€‚BDHçš„ç¥ç»å…ƒäº¤äº’ç½‘ç»œæ˜¯ä¸€ä¸ªå…·æœ‰é«˜æ¨¡å—åŒ–å’Œé‡å°¾åº¦åˆ†å¸ƒçš„å›¾ã€‚BDHæ¨¡å‹åœ¨ç”Ÿç‰©å­¦ä¸Šæ˜¯åˆç†çš„ï¼Œè§£é‡Šäº†äººç±»ç¥ç»å…ƒå¯èƒ½ä½¿ç”¨çš„ä¸€ç§å®ç°è¯­éŸ³çš„æœºåˆ¶ã€‚BDHä¸“ä¸ºå¯è§£é‡Šæ€§è€Œè®¾è®¡ã€‚BDHçš„æ¿€æ´»å‘é‡æ˜¯ç¨€ç–ä¸”æ­£çš„ã€‚æˆ‘ä»¬åœ¨è¯­è¨€ä»»åŠ¡ä¸­å±•ç¤ºäº†BDHçš„å•ä¹‰æ€§ã€‚çŠ¶æ€çš„å¯è§£é‡Šæ€§ï¼Œè¶…è¶Šäº†ç¥ç»å…ƒå’Œæ¨¡å‹å‚æ•°çš„å¯è§£é‡Šæ€§ï¼Œæ˜¯BDHæ¶æ„çš„å›ºæœ‰ç‰¹å¾ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹è™½ç„¶åœ¨å„ç§ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æˆæœï¼Œä½†åœ¨å¯è§£é‡Šæ€§å’Œç”Ÿç‰©åˆç†æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚æ­¤å¤–ï¼Œå®ƒä»¬åœ¨å¤„ç†æ—¶é—´åºåˆ—æ•°æ®æ—¶ï¼Œéš¾ä»¥åƒç”Ÿç‰©å¤§è„‘ä¸€æ ·è¿›è¡Œæ³›åŒ–ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§æ—¢èƒ½ä¿æŒé«˜æ€§èƒ½ï¼Œåˆèƒ½æä¾›å†…åœ¨å¯è§£é‡Šæ€§ï¼Œå¹¶ä¸”æ›´æ¥è¿‘ç”Ÿç‰©ç¥ç»æœºåˆ¶çš„æ¨¡å‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDragon Hatchling (BDH) çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªåŸºäºå°ºåº¦ä¸å˜çš„ã€å—ç”Ÿç‰©å­¦å¯å‘çš„ç¥ç»å…ƒç²’å­ç½‘ç»œã€‚è¯¥ç½‘ç»œä¸­çš„ç¥ç»å…ƒè¿›è¡Œå±€éƒ¨äº¤äº’ï¼Œå¹¶é€šè¿‡èµ«å¸ƒå­¦ä¹ è¿›è¡Œçªè§¦å¯å¡‘æ€§è°ƒæ•´ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨æ¨¡æ‹Ÿå¤§è„‘çš„å·¥ä½œæ–¹å¼ï¼Œä»è€Œæé«˜æ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šBDH çš„æ•´ä½“æ¶æ„æ˜¯ä¸€ä¸ªåŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„çŠ¶æ€ç©ºé—´åºåˆ—å­¦ä¹ æ¨¡å‹ã€‚å®ƒç”±å¤šä¸ªç¥ç»å…ƒç²’å­ç»„æˆï¼Œè¿™äº›ç²’å­é€šè¿‡å›¾ç»“æ„è¿æ¥ã€‚æ¨¡å‹åŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼šè¾“å…¥åµŒå…¥å±‚ã€ç¥ç»å…ƒäº¤äº’å±‚ã€è¾“å‡ºå±‚ã€‚ç¥ç»å…ƒäº¤äº’å±‚æ˜¯ BDH çš„æ ¸å¿ƒï¼Œè´Ÿè´£æ¨¡æ‹Ÿç¥ç»å…ƒä¹‹é—´çš„å±€éƒ¨äº¤äº’å’Œçªè§¦å¯å¡‘æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šBDH çš„å…³é”®åˆ›æ–°åœ¨äºå…¶åŸºäºç”Ÿç‰©å¯å‘çš„ç¥ç»å…ƒäº¤äº’ç½‘ç»œå’Œèµ«å¸ƒå­¦ä¹ æœºåˆ¶ã€‚ä¸ä¼ ç»Ÿçš„ Transformer æ¨¡å‹ä¸åŒï¼ŒBDH çš„ç¥ç»å…ƒä¹‹é—´çš„è¿æ¥ä¸æ˜¯å®Œå…¨è¿æ¥çš„ï¼Œè€Œæ˜¯é€šè¿‡ä¸€ä¸ªå…·æœ‰å°ºåº¦ä¸å˜æ€§çš„å›¾ç»“æ„è¿æ¥ã€‚æ­¤å¤–ï¼ŒBDH ä½¿ç”¨èµ«å¸ƒå­¦ä¹ æ¥è°ƒæ•´ç¥ç»å…ƒä¹‹é—´çš„è¿æ¥å¼ºåº¦ï¼Œä»è€Œå®ç°å·¥ä½œè®°å¿†å’Œæ¦‚å¿µå­¦ä¹ ã€‚

**å…³é”®è®¾è®¡**ï¼šBDH çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨è„‰å†²ç¥ç»å…ƒæ¥æ¨¡æ‹Ÿç”Ÿç‰©ç¥ç»å…ƒçš„æ¿€æ´»æœºåˆ¶ï¼›2) ä½¿ç”¨ç¨€ç–ä¸”æ­£çš„æ¿€æ´»å‘é‡æ¥æé«˜æ¨¡å‹çš„å¯è§£é‡Šæ€§ï¼›3) ä½¿ç”¨å…·æœ‰é«˜æ¨¡å—åŒ–å’Œé‡å°¾åº¦åˆ†å¸ƒçš„å›¾ç»“æ„æ¥è¿æ¥ç¥ç»å…ƒï¼›4) ä½¿ç”¨èµ«å¸ƒå­¦ä¹ è§„åˆ™æ¥è°ƒæ•´ç¥ç»å…ƒä¹‹é—´çš„è¿æ¥å¼ºåº¦ã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡ç›®æ ‡æ˜¯æœ€å°åŒ–é¢„æµ‹è¯¯å·®ï¼Œå¹¶é¼“åŠ±ç¥ç»å…ƒä¹‹é—´çš„ç¨€ç–è¿æ¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒBDH åœ¨è¯­è¨€å’Œç¿»è¯‘ä»»åŠ¡ä¸Šä¸ GPT2 å…·æœ‰ç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶å…·æœ‰æ›´é«˜çš„å¯è§£é‡Šæ€§ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨ç›¸åŒçš„å‚æ•°è§„æ¨¡å’Œè®­ç»ƒæ•°æ®ä¸‹ï¼ŒBDH èƒ½å¤Ÿè¾¾åˆ°ä¸ GPT2 ç›¸ä¼¼çš„å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œå®éªŒè¿˜è¯å®äº† BDH çš„ç¥ç»å…ƒäº¤äº’ç½‘ç»œå…·æœ‰é«˜æ¨¡å—åŒ–å’Œé‡å°¾åº¦åˆ†å¸ƒï¼Œå¹¶ä¸”ç‰¹å®šçš„çªè§¦ä¼šåœ¨æ¨¡å‹å¤„ç†ç‰¹å®šæ¦‚å¿µæ—¶åŠ å¼ºè¿æ¥ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

BDH æ¨¡å‹å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼ŒåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨ç¿»è¯‘ã€è¯­éŸ³è¯†åˆ«ç­‰é¢†åŸŸã€‚ç”±äºå…¶å†…åœ¨çš„å¯è§£é‡Šæ€§ï¼ŒBDH è¿˜å¯ä»¥åº”ç”¨äºåŒ»ç–—è¯Šæ–­ã€é‡‘èé£é™©è¯„ä¼°ç­‰éœ€è¦é«˜åº¦é€æ˜åº¦çš„é¢†åŸŸã€‚æ­¤å¤–ï¼ŒBDH çš„ç”Ÿç‰©åˆç†æ€§ä½¿å…¶æˆä¸ºç ”ç©¶å¤§è„‘å·¥ä½œæœºåˆ¶çš„æœ‰åŠ›å·¥å…·ï¼Œæœ‰åŠ©äºæ¨åŠ¨ç¥ç»ç§‘å­¦çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The relationship between computing systems and the brain has served as motivation for pioneering theoreticians since John von Neumann and Alan Turing. Uniform, scale-free biological networks, such as the brain, have powerful properties, including generalizing over time, which is the main barrier for Machine Learning on the path to Universal Reasoning Models.
>   We introduce `Dragon Hatchling' (BDH), a new Large Language Model architecture based on a scale-free biologically inspired network of \$n\$ locally-interacting neuron particles. BDH couples strong theoretical foundations and inherent interpretability without sacrificing Transformer-like performance.
>   BDH is a practical, performant state-of-the-art attention-based state space sequence learning architecture. In addition to being a graph model, BDH admits a GPU-friendly formulation. It exhibits Transformer-like scaling laws: empirically BDH rivals GPT2 performance on language and translation tasks, at the same number of parameters (10M to 1B), for the same training data.
>   BDH can be represented as a brain model. The working memory of BDH during inference entirely relies on synaptic plasticity with Hebbian learning using spiking neurons. We confirm empirically that specific, individual synapses strengthen connection whenever BDH hears or reasons about a specific concept while processing language inputs. The neuron interaction network of BDH is a graph of high modularity with heavy-tailed degree distribution. The BDH model is biologically plausible, explaining one possible mechanism which human neurons could use to achieve speech.
>   BDH is designed for interpretability. Activation vectors of BDH are sparse and positive. We demonstrate monosemanticity in BDH on language tasks. Interpretability of state, which goes beyond interpretability of neurons and model parameters, is an inherent feature of the BDH architecture.

