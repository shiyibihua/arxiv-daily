---
layout: default
title: RoRecomp: Enhancing Reasoning Efficiency via Rollout Response Recomposition in Reinforcement Learning
---

# RoRecomp: Enhancing Reasoning Efficiency via Rollout Response Recomposition in Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25958" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25958v1</a>
  <a href="https://arxiv.org/pdf/2509.25958.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25958v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25958v1', 'RoRecomp: Enhancing Reasoning Efficiency via Rollout Response Recomposition in Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Gang Li, Yulei Qin, Xiaoyu Tan, Dingkang Yang, Yuchen Shi, Zihan Xu, Xiang Li, Xing Sun, Ke Li

**åˆ†ç±»**: cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRoRecompï¼Œé€šè¿‡é‡ç»„Rolloutå“åº”æå‡å¼ºåŒ–å­¦ä¹ ä¸­LLMçš„æ¨ç†æ•ˆç‡ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨ç†æ•ˆç‡` `Rollouté‡ç»„` `å¯éªŒè¯å¥–åŠ±`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»ŸRLVRè®­ç»ƒLLMæ—¶ï¼Œå¥–åŠ±æœºåˆ¶ç¼ºä¹å¯¹æ•ˆç‡çš„æ¿€åŠ±ï¼Œå¯¼è‡´æ¨ç†è¿‡ç¨‹å†—é•¿ï¼Œæ¢ç´¢è½¨è¿¹æ•ˆç‡ä½ä¸‹ã€‚
2. RoRecompé€šè¿‡é‡ç»„è®­ç»ƒæ•°æ®ï¼ŒåŒºåˆ†ä¼˜å…ˆæ‰¹æ¬¡å’Œè¡¥å¿æ‰¹æ¬¡ï¼Œå¼•å¯¼æ¨¡å‹ç”Ÿæˆæ›´ç®€æ´é«˜æ•ˆçš„æ¨ç†è¿‡ç¨‹ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒRoRecompåœ¨å¤šä¸ªä»»åŠ¡ä¸­æ˜¾è‘—å‡å°‘äº†æ¨ç†é•¿åº¦å’Œä¸å¿…è¦çš„å·¥å…·è°ƒç”¨ï¼ŒåŒæ—¶ä¿æŒäº†æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºRollout Response Recomposition (RoRecomp) æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³åŸºäºå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ—¶ï¼Œæ¨ç†è¿‡ç¨‹å†—é•¿å’Œæ¢ç´¢è½¨è¿¹æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚RoRecompé€šè¿‡ç­–ç•¥æ€§åœ°é‡ç»„è®­ç»ƒæ•°æ®ï¼Œå¼•å¯¼æ¨¡å‹ç”Ÿæˆç®€æ´çš„æ¨ç†è¿‡ç¨‹ã€‚è¯¥æ–¹æ³•å°†å“åº”åˆ†ä¸ºä¸¤ç±»æ‰¹æ¬¡ï¼šä¼˜å…ˆæ‰¹æ¬¡ï¼Œç»“åˆåœ¨çº¿æ‰¹æ¬¡ä¸­çš„çŸ­-æ­£ç¡®å’Œé•¿-é”™è¯¯å“åº”ï¼Œä¸ºç®€æ´æ€§æä¾›æ¸…æ™°çš„æ¢¯åº¦ä¿¡å·ï¼›è¡¥å¿æ‰¹æ¬¡ï¼Œåˆ©ç”¨å›æ”¾ç¼“å†²åŒºä¸­çš„å‰©ä½™å“åº”ï¼Œç»´æŒè®­ç»ƒç¨³å®šæ€§å¹¶é˜²æ­¢æ¨¡å‹å´©æºƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRoRecompåœ¨å¤šä¸ªåœºæ™¯ä¸­æ˜¾è‘—æå‡äº†æ•ˆç‡ï¼šåœ¨é›¶æ ·æœ¬å¼ºåŒ–å­¦ä¹ ä¸­å‡å°‘äº†27.7%çš„æ¨ç†é•¿åº¦ï¼Œåœ¨Agenticå¼ºåŒ–å­¦ä¹ ä¸­å‡å°‘äº†46.8%çš„ä¸å¿…è¦å·¥å…·è°ƒç”¨å¹¶æé«˜äº†å‡†ç¡®ç‡ï¼Œåœ¨æ€ç»´å‹ç¼©ä¸­å®ç°äº†é«˜è¾¾52.5%çš„é•¿åº¦ç¼©å‡ï¼Œä¸”å¯¹æ€§èƒ½å½±å“æå°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ—¶ï¼Œç”±äºå¥–åŠ±ä»…å…³æ³¨ç»“æœï¼Œç¼ºä¹å¯¹è¿‡ç¨‹æ•ˆç‡çš„æ¿€åŠ±ï¼Œå¯¼è‡´æ¨¡å‹ç”Ÿæˆçš„æ¨ç†è¿‡ç¨‹è¿‡äºå†—é•¿ï¼Œæ¢ç´¢è½¨è¿¹æ•ˆç‡ä½ä¸‹ã€‚æ­¤å¤–ï¼ŒRolloutä¸­å“åº”é•¿åº¦å·®å¼‚è¾ƒå¤§ï¼Œå¯¼è‡´ä¼˜åŒ–ä¿¡å·å™ªå£°è¾ƒé«˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šRoRecompçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ç­–ç•¥æ€§åœ°é‡ç»„Rolloutå“åº”ï¼Œæ„å»ºæ›´æœ‰æ•ˆçš„è®­ç»ƒæ•°æ®ï¼Œä»è€Œå¼•å¯¼æ¨¡å‹å­¦ä¹ æ›´ç®€æ´çš„æ¨ç†è¿‡ç¨‹ã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥æ–¹æ³•åŒºåˆ†äº†â€œä¼˜å…ˆæ‰¹æ¬¡â€å’Œâ€œè¡¥å¿æ‰¹æ¬¡â€ï¼Œåˆ†åˆ«ç”¨äºæä¾›ç®€æ´æ€§æ¢¯åº¦ä¿¡å·å’Œç»´æŒè®­ç»ƒç¨³å®šæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRoRecompæ˜¯ä¸€ä¸ªå³æ’å³ç”¨çš„æ–¹æ³•ï¼Œå¯ä»¥åº”ç”¨äºç°æœ‰çš„RLVRè®­ç»ƒæ¡†æ¶ã€‚å…¶ä¸»è¦æµç¨‹åŒ…æ‹¬ï¼š1) ä»Rolloutä¸­æ”¶é›†å“åº”ï¼›2) å°†å“åº”åˆ†ä¸ºçŸ­-æ­£ç¡®ã€é•¿-é”™è¯¯å’Œå…¶ä»–å“åº”ï¼›3) æ„å»ºä¼˜å…ˆæ‰¹æ¬¡ï¼ŒåŒ…å«çŸ­-æ­£ç¡®å’Œé•¿-é”™è¯¯å“åº”ï¼Œç”¨äºä¼˜åŒ–ç®€æ´æ€§ï¼›4) æ„å»ºè¡¥å¿æ‰¹æ¬¡ï¼ŒåŒ…å«å‰©ä½™å“åº”ï¼Œç”¨äºç»´æŒè®­ç»ƒç¨³å®šæ€§ï¼›5) ä½¿ç”¨ä¼˜å…ˆæ‰¹æ¬¡å’Œè¡¥å¿æ‰¹æ¬¡æ›´æ–°æ¨¡å‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šRoRecompçš„å…³é”®åˆ›æ–°åœ¨äºå…¶æ•°æ®é‡ç»„ç­–ç•¥ï¼Œé€šè¿‡åŒºåˆ†ä¼˜å…ˆæ‰¹æ¬¡å’Œè¡¥å¿æ‰¹æ¬¡ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å¼•å¯¼æ¨¡å‹å­¦ä¹ ç®€æ´çš„æ¨ç†è¿‡ç¨‹ã€‚ä¸ä¼ ç»Ÿçš„RLVRæ–¹æ³•ç›¸æ¯”ï¼ŒRoRecompèƒ½å¤Ÿæä¾›æ›´æ¸…æ™°çš„ç®€æ´æ€§æ¢¯åº¦ä¿¡å·ï¼Œå¹¶é˜²æ­¢æ¨¡å‹å´©æºƒã€‚

**å…³é”®è®¾è®¡**ï¼šRoRecompçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä¼˜å…ˆæ‰¹æ¬¡çš„æ„å»ºæ–¹å¼ï¼Œé€‰æ‹©çŸ­-æ­£ç¡®å’Œé•¿-é”™è¯¯å“åº”ï¼Œä»¥æä¾›æ¸…æ™°çš„ç®€æ´æ€§æ¢¯åº¦ä¿¡å·ï¼›2) è¡¥å¿æ‰¹æ¬¡çš„æ„å»ºæ–¹å¼ï¼Œä½¿ç”¨å‰©ä½™å“åº”ï¼Œä»¥ç»´æŒè®­ç»ƒç¨³å®šæ€§ï¼›3) æ‰¹æ¬¡å¤§å°çš„è®¾ç½®ï¼Œéœ€è¦å¹³è¡¡ç®€æ´æ€§ä¼˜åŒ–å’Œè®­ç»ƒç¨³å®šæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

RoRecompåœ¨ä¸‰ä¸ªå®éªŒåœºæ™¯ä¸­å‡å–å¾—äº†æ˜¾è‘—çš„æ•ˆç‡æå‡ã€‚åœ¨é›¶æ ·æœ¬å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œæ¨ç†é•¿åº¦å‡å°‘äº†27.7%ã€‚åœ¨Agenticå¼ºåŒ–å­¦ä¹ ä¸­ï¼Œä¸å¿…è¦çš„å·¥å…·è°ƒç”¨å‡å°‘äº†46.8%ï¼ŒåŒæ—¶æé«˜äº†å‡†ç¡®ç‡ã€‚åœ¨æ€ç»´å‹ç¼©ä»»åŠ¡ä¸­ï¼Œé•¿åº¦ç¼©å‡é«˜è¾¾52.5%ã€‚æ‰€æœ‰è¿™äº›æå‡éƒ½ä¼´éšç€æœ€å°çš„æ€§èƒ½å½±å“ï¼Œè¯æ˜äº†RoRecompçš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

RoRecompå¯åº”ç”¨äºå„ç§éœ€è¦é«˜æ•ˆæ¨ç†çš„å¤§å‹è¯­è¨€æ¨¡å‹åº”ç”¨åœºæ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½Agentã€é—®ç­”ç³»ç»Ÿã€ä»£ç ç”Ÿæˆç­‰ã€‚é€šè¿‡å‡å°‘æ¨ç†é•¿åº¦å’Œä¸å¿…è¦çš„å·¥å…·è°ƒç”¨ï¼ŒRoRecompå¯ä»¥æ˜¾è‘—æé«˜è¿™äº›åº”ç”¨çš„æ•ˆç‡å’Œç”¨æˆ·ä½“éªŒï¼Œå¹¶é™ä½è®¡ç®—æˆæœ¬ã€‚è¯¥æ–¹æ³•è¿˜æœ‰åŠ©äºæå‡LLMåœ¨èµ„æºå—é™ç¯å¢ƒä¸‹çš„éƒ¨ç½²èƒ½åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement learning with verifiable rewards (RLVR) has proven effective in eliciting complex reasoning in large language models (LLMs). However, standard RLVR training often leads to excessively verbose processes (in reasoning tasks) and inefficient exploration trajectories (in agentic settings), as outcome-only rewards provide no incentive for efficiency and the high variance in response length within relatively small rollout groups results in noisy optimization signals. To address this, we propose Rollout Response Recomposition (RoRecomp), a plug-and-play method that guides models toward concise reasoning by strategically recomposing the training data. RoRecomp separates responses into two distinct batch types: 1) priority batches, which combine short-correct and long-incorrect responses selected from online batches to provide a clear gradient signal for brevity, and 2) compensation batches, which utilize remaining responses from a replay buffer to maintain stability and prevent model collapse. To comprehensively evaluate effectiveness, we test RoRecomp across three settings where results demonstrate substantial efficiency gains: reducing reasoning length by 27.7% in zero RL training, reducing unnecessary tool calls by 46.8% while improving accuracy in agentic RL, and achieving up to 52.5% length reduction in thinking compression, all with minimal performance impact.

