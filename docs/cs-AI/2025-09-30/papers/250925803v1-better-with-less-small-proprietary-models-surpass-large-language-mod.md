---
layout: default
title: Better with Less: Small Proprietary Models Surpass Large Language Models in Financial Transaction Understanding
---

# Better with Less: Small Proprietary Models Surpass Large Language Models in Financial Transaction Understanding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25803" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25803v1</a>
  <a href="https://arxiv.org/pdf/2509.25803.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25803v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25803v1', 'Better with Less: Small Proprietary Models Surpass Large Language Models in Financial Transaction Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Wanying Ding, Savinay Narendra, Xiran Shi, Adwait Ratnaparkhi, Chengrui Yang, Nikoo Sabzevar, Ziyan Yin

**åˆ†ç±»**: cs.IR, cs.AI, cs.CE, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

**å¤‡æ³¨**: 9 pages, 5 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**å°è§„æ¨¡é‡‘èäº¤æ˜“ä¸“å±æ¨¡å‹è¶…è¶Šå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæå‡äº¤æ˜“ç†è§£èƒ½åŠ›ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é‡‘èäº¤æ˜“ç†è§£` `Transformeræ¨¡å‹` `ä¸“æœ‰æ¨¡å‹` `é¢†åŸŸè‡ªé€‚åº”` `Decoder-Onlyæ¨¡å‹` `æˆæœ¬æ•ˆç›Š` `æ¬ºè¯ˆæ£€æµ‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é‡‘èäº¤æ˜“ç†è§£æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œæ— æ³•æ»¡è¶³å®æ—¶æ€§å’Œæˆæœ¬æ•ˆç›Šçš„éœ€æ±‚ã€‚
2. è®ºæ–‡æå‡ºå®šåˆ¶åŒ–çš„å°è§„æ¨¡ä¸“æœ‰æ¨¡å‹ï¼Œé’ˆå¯¹é‡‘èäº¤æ˜“æ•°æ®çš„ç‰¹æ€§è¿›è¡Œä¼˜åŒ–ï¼Œæå‡æ€§èƒ½ã€‚
3. å®éªŒè¡¨æ˜ï¼Œä¸“æœ‰æ¨¡å‹åœ¨é€Ÿåº¦å’Œæˆæœ¬ä¸Šä¼˜äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¹¶æå‡äº†äº¤æ˜“è¦†ç›–ç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åˆ†æé‡‘èäº¤æ˜“å¯¹äºç¡®ä¿åˆè§„ã€æ£€æµ‹æ¬ºè¯ˆå’Œæ”¯æŒå†³ç­–è‡³å…³é‡è¦ã€‚é‡‘èäº¤æ˜“æ•°æ®çš„å¤æ‚æ€§éœ€è¦å…ˆè¿›çš„æŠ€æœ¯æ¥æå–æœ‰æ„ä¹‰çš„è§è§£å¹¶ç¡®ä¿å‡†ç¡®çš„åˆ†æã€‚ç”±äºåŸºäºTransformerçš„æ¨¡å‹åœ¨å¤šä¸ªé¢†åŸŸè¡¨ç°å‡ºè‰²ï¼Œæœ¬æ–‡æ—¨åœ¨æ¢ç´¢å®ƒä»¬åœ¨ç†è§£é‡‘èäº¤æ˜“æ–¹é¢çš„æ½œåŠ›ã€‚æœ¬æ–‡è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œä»¥è¯„ä¼°ä¸‰ç§ç±»å‹çš„Transformeræ¨¡å‹ï¼šEncoder-Onlyã€Decoder-Onlyå’ŒEncoder-Decoderæ¨¡å‹ã€‚å¯¹äºæ¯ç§ç±»å‹ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä¸‰ä¸ªé€‰é¡¹ï¼šé¢„è®­ç»ƒçš„LLMã€å¾®è°ƒçš„LLMå’Œä»å¤´å¼€å§‹å¼€å‘çš„å°å‹ä¸“æœ‰æ¨¡å‹ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œè™½ç„¶LLMï¼ˆå¦‚LLaMA3-8bã€Flan-T5å’ŒSBERTï¼‰åœ¨å„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œä½†åœ¨é‡‘èäº¤æ˜“ç†è§£çš„ç‰¹å®šèƒŒæ™¯ä¸‹ï¼Œå®ƒä»¬å¹¶æ²¡æœ‰æ˜æ˜¾ä¼˜äºå°å‹ä¸“æœ‰æ¨¡å‹ã€‚è¿™ç§ç°è±¡åœ¨é€Ÿåº¦å’Œæˆæœ¬æ•ˆç‡æ–¹é¢å°¤ä¸ºæ˜æ˜¾ã€‚ä¸“æœ‰æ¨¡å‹é’ˆå¯¹äº¤æ˜“æ•°æ®çš„ç‹¬ç‰¹éœ€æ±‚é‡èº«å®šåˆ¶ï¼Œè¡¨ç°å‡ºæ›´å¿«çš„å¤„ç†é€Ÿåº¦å’Œæ›´ä½çš„è¿è¥æˆæœ¬ï¼Œä½¿å…¶æ›´é€‚åˆé‡‘èé¢†åŸŸçš„å®æ—¶åº”ç”¨ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœå¼ºè°ƒäº†åŸºäºé¢†åŸŸç‰¹å®šéœ€æ±‚é€‰æ‹©æ¨¡å‹çš„é‡è¦æ€§ï¼Œå¹¶å¼ºè°ƒäº†å®šåˆ¶ä¸“æœ‰æ¨¡å‹åœ¨ä¸“é—¨åº”ç”¨ä¸­ç›¸å¯¹äºé€šç”¨LLMçš„æ½œåœ¨ä¼˜åŠ¿ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬é€‰æ‹©å®æ–½ä¸“æœ‰çš„decoder-onlyæ¨¡å‹æ¥å¤„ç†æˆ‘ä»¬ä»¥å‰æ— æ³•ç®¡ç†çš„å¤æ‚äº¤æ˜“ã€‚è¯¥æ¨¡å‹å¯ä»¥å¸®åŠ©æˆ‘ä»¬æé«˜14%çš„äº¤æ˜“è¦†ç›–ç‡ï¼Œå¹¶èŠ‚çœè¶…è¿‡1300ä¸‡ç¾å…ƒçš„å¹´åº¦æˆæœ¬ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³é‡‘èäº¤æ˜“ç†è§£ä¸­ï¼Œç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é€Ÿåº¦ã€æˆæœ¬å’Œé¢†åŸŸé€‚åº”æ€§æ–¹é¢çš„ä¸è¶³ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚ç›´æ¥ä½¿ç”¨æˆ–å¾®è°ƒé€šç”¨LLMï¼Œæ— æ³•å……åˆ†åˆ©ç”¨é‡‘èäº¤æ˜“æ•°æ®çš„ç‰¹æ€§ï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹å’Œæ€§èƒ½ç“¶é¢ˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸“é—¨é’ˆå¯¹é‡‘èäº¤æ˜“æ•°æ®çš„å°è§„æ¨¡ä¸“æœ‰æ¨¡å‹ã€‚é€šè¿‡ä»å¤´å¼€å§‹è®­ç»ƒï¼Œæ¨¡å‹å¯ä»¥æ›´å¥½åœ°å­¦ä¹ å’Œé€‚åº”é‡‘èé¢†åŸŸçš„ç‰¹å®šæ¨¡å¼å’Œè¯­ä¹‰ï¼Œä»è€Œåœ¨é€Ÿåº¦å’Œæˆæœ¬ä¸Šä¼˜äºé€šç”¨LLMã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡è¯„ä¼°äº†ä¸‰ç§Transformeræ¶æ„ï¼ˆEncoder-Only, Decoder-Only, Encoder-Decoderï¼‰ï¼Œå¹¶é’ˆå¯¹æ¯ç§æ¶æ„æ¢ç´¢äº†ä¸‰ç§è®­ç»ƒæ–¹å¼ï¼ˆé¢„è®­ç»ƒLLMã€å¾®è°ƒLLMã€ä»å¤´è®­ç»ƒçš„ä¸“æœ‰æ¨¡å‹ï¼‰ã€‚æœ€ç»ˆé€‰æ‹©Decoder-Onlyæ¶æ„ï¼Œå¹¶ä»å¤´è®­ç»ƒä¸“æœ‰æ¨¡å‹ã€‚æ•´ä½“æµç¨‹åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹è®­ç»ƒã€è¯„ä¼°å’Œéƒ¨ç½²ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºé’ˆå¯¹é‡‘èäº¤æ˜“æ•°æ®å®šåˆ¶åŒ–çš„å°è§„æ¨¡ä¸“æœ‰æ¨¡å‹ã€‚ä¸ç›´æ¥ä½¿ç”¨æˆ–å¾®è°ƒé€šç”¨LLMä¸åŒï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨é¢†åŸŸçŸ¥è¯†ï¼Œå®ç°æ›´é«˜çš„æ•ˆç‡å’Œæ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡æœ€ç»ˆé€‰æ‹©å¹¶å®ç°äº†ä¸€ä¸ªä¸“æœ‰çš„Decoder-Onlyæ¨¡å‹ã€‚å…·ä½“å‚æ•°è®¾ç½®ã€æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç­‰æŠ€æœ¯ç»†èŠ‚æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†è¯´æ˜ï¼Œä½†å¼ºè°ƒäº†æ¨¡å‹æ˜¯æ ¹æ®é‡‘èäº¤æ˜“æ•°æ®çš„ç‰¹æ€§è¿›è¡Œå®šåˆ¶çš„ã€‚è¯¥æ¨¡å‹æœ€ç»ˆæå‡äº†14%çš„äº¤æ˜“è¦†ç›–ç‡ï¼Œå¹¶èŠ‚çœäº†è¶…è¿‡1300ä¸‡ç¾å…ƒçš„å¹´åº¦æˆæœ¬ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå®šåˆ¶åŒ–çš„å°è§„æ¨¡ä¸“æœ‰æ¨¡å‹åœ¨é‡‘èäº¤æ˜“ç†è§£ä»»åŠ¡ä¸­ä¼˜äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚LLaMA3-8bã€Flan-T5å’ŒSBERTï¼‰ã€‚è¯¥ä¸“æœ‰æ¨¡å‹èƒ½å¤Ÿæé«˜14%çš„äº¤æ˜“è¦†ç›–ç‡ï¼Œå¹¶èŠ‚çœè¶…è¿‡1300ä¸‡ç¾å…ƒçš„å¹´åº¦æˆæœ¬ï¼Œçªæ˜¾äº†å…¶åœ¨é€Ÿåº¦å’Œæˆæœ¬æ•ˆç›Šæ–¹é¢çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºé‡‘èè¡Œä¸šçš„å¤šä¸ªé¢†åŸŸï¼ŒåŒ…æ‹¬æ¬ºè¯ˆæ£€æµ‹ã€åˆè§„æ€§æ£€æŸ¥ã€é£é™©è¯„ä¼°å’Œäº¤æ˜“åˆ†æã€‚é€šè¿‡æé«˜é‡‘èäº¤æ˜“ç†è§£çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œå¯ä»¥é™ä½è¿è¥æˆæœ¬ï¼Œæå‡é£é™©ç®¡ç†èƒ½åŠ›ï¼Œå¹¶ä¸ºå†³ç­–æä¾›æ›´å¯é çš„æ”¯æŒã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯ä»¥æ¨å¹¿åˆ°å…¶ä»–å…·æœ‰ç‰¹å®šé¢†åŸŸçŸ¥è¯†éœ€æ±‚çš„åœºæ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Analyzing financial transactions is crucial for ensuring regulatory compliance, detecting fraud, and supporting decisions. The complexity of financial transaction data necessitates advanced techniques to extract meaningful insights and ensure accurate analysis. Since Transformer-based models have shown outstanding performance across multiple domains, this paper seeks to explore their potential in understanding financial transactions. This paper conducts extensive experiments to evaluate three types of Transformer models: Encoder-Only, Decoder-Only, and Encoder-Decoder models. For each type, we explore three options: pretrained LLMs, fine-tuned LLMs, and small proprietary models developed from scratch. Our analysis reveals that while LLMs, such as LLaMA3-8b, Flan-T5, and SBERT, demonstrate impressive capabilities in various natural language processing tasks, they do not significantly outperform small proprietary models in the specific context of financial transaction understanding. This phenomenon is particularly evident in terms of speed and cost efficiency. Proprietary models, tailored to the unique requirements of transaction data, exhibit faster processing times and lower operational costs, making them more suitable for real-time applications in the financial sector. Our findings highlight the importance of model selection based on domain-specific needs and underscore the potential advantages of customized proprietary models over general-purpose LLMs in specialized applications. Ultimately, we chose to implement a proprietary decoder-only model to handle the complex transactions that we previously couldn't manage. This model can help us to improve 14% transaction coverage, and save more than \$13 million annual cost.

