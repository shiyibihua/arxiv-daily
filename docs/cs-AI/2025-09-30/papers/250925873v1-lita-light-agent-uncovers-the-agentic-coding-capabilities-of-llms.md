---
layout: default
title: Lita: Light Agent Uncovers the Agentic Coding Capabilities of LLMs
---

# Lita: Light Agent Uncovers the Agentic Coding Capabilities of LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25873" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25873v1</a>
  <a href="https://arxiv.org/pdf/2509.25873.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25873v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25873v1', 'Lita: Light Agent Uncovers the Agentic Coding Capabilities of LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hankun Dai, Maoquan Wang, Mengnan Qi, Yikai Zhang, Zijian Jin, Yongqiang Yao, Yufan Huang, Shengyu Fu, Elsie Nallipogu

**åˆ†ç±»**: cs.AI, cs.CL, cs.LG, cs.PL, cs.SE

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**Litaï¼šè½»é‡çº§Agentæ­ç¤ºLLMçš„Agenticç¼–ç èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `ä»£ç Agent` `è½»é‡åŒ–è®¾è®¡` `ç¼–ç èƒ½åŠ›è¯„ä¼°` `è‡ªåŠ¨åŒ–ä»£ç ç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ä»£ç Agentè®¾è®¡ä¾èµ–å¤æ‚å·¥ä½œæµå’Œå·¥å…·é›†ï¼Œè¿‡åº¦ä¾èµ–æç¤ºè°ƒä¼˜ï¼Œæ©ç›–æ¨¡å‹çœŸå®èƒ½åŠ›ï¼Œä¸”pipelineæˆæœ¬é«˜æ˜‚ã€‚
2. Litaé€šè¿‡æœ€å°åŒ–æ‰‹åŠ¨è®¾è®¡ï¼Œä¿ç•™è‡ªä¸»AgentåŸºæœ¬è¦ç´ ï¼Œå®ç°è½»é‡åŒ–ï¼Œä»è€Œæ›´çœŸå®ç»Ÿä¸€åœ°è¯„ä¼°LLMçš„ç¼–ç èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒLitaåœ¨Aider Polyglotå’ŒSWE-Benchä¸Šè¾¾åˆ°æˆ–è¶…è¿‡ç°æœ‰æ–¹æ³•æ€§èƒ½ï¼ŒåŒæ—¶æ¶ˆè€—æ›´å°‘tokenï¼Œè®¾è®¡å·¥ä½œé‡æ›´å°‘ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¶Šæ¥è¶Šå¤šåœ°åº”ç”¨äºç¼–ç¨‹ä»»åŠ¡ï¼Œä»å•è½®ä»£ç è¡¥å…¨åˆ°è‡ªä¸»Agentã€‚å½“å‰çš„ä»£ç Agentè®¾è®¡é€šå¸¸ä¾èµ–äºå¤æ‚çš„æ‰‹å·¥å·¥ä½œæµç¨‹å’Œå·¥å…·é›†ã€‚ç„¶è€Œï¼Œè¿™ç§å¯¹ç²¾å¿ƒè®¾è®¡çš„è„šæ‰‹æ¶çš„ä¾èµ–å¸¦æ¥äº†ä¸€äº›æŒ‘æˆ˜ï¼šAgentæ€§èƒ½è¿‡åº¦ä¾èµ–äºæç¤ºè°ƒä¼˜å’Œè‡ªå®šä¹‰è®¾è®¡é€‰æ‹©ï¼Œå¤§é‡çš„äººå·¥å¹²é¢„æ©ç›–äº†æ¨¡å‹çœŸæ­£çš„åº•å±‚èƒ½åŠ›ï¼Œå¹¶ä¸”å¤æ‚çš„pipelineæ„å»ºå’Œç»´æŠ¤æˆæœ¬é«˜æ˜‚ã€‚æ­¤å¤–ï¼Œä¼˜åŒ–å¤æ‚çš„ä»»åŠ¡æç¤ºä¼šå¢åŠ æ•°æ®æ³„éœ²çš„é£é™©ã€‚ç›®å‰ï¼ŒåƒOpenAIå’ŒAnthropicè¿™æ ·çš„LLMæä¾›å•†åœ¨å¼•å…¥æ–°æ¨¡å‹æ—¶ï¼Œé€šå¸¸ä¼šå‘å¸ƒåŸºå‡†åˆ†æ•°æ¥å±•ç¤ºå…¶æ¨¡å‹çš„ç¼–ç èƒ½åŠ›ï¼Œä½†å¯¹å…¶ä¸“æœ‰çš„è¯„ä¼°æ¡†æ¶ä¿å¯†ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†Litaï¼ˆLite Agentï¼‰ï¼Œå®ƒå°†è½»é‡åŒ–åŸåˆ™ä»˜è¯¸å®è·µï¼Œå³åœ¨ä¿ç•™å®Œå…¨è‡ªä¸»Agentçš„åŸºæœ¬è¦ç´ çš„åŒæ—¶ï¼Œæœ€å¤§é™åº¦åœ°å‡å°‘æ‰‹åŠ¨è®¾è®¡ã€‚Litaèƒ½å¤Ÿåœ¨æ²¡æœ‰ç²¾å¿ƒè®¾è®¡çš„è„šæ‰‹æ¶çš„æƒ…å†µä¸‹è¿›è¡Œæ›´çœŸå®å’Œç»Ÿä¸€çš„è¯„ä¼°ã€‚åœ¨å‰æ²¿æ¨¡å‹ä¸Šè¿›è¡Œçš„Aider Polyglotå’ŒSWE-Benchå®éªŒè¡¨æ˜ï¼Œä¸åŸºäºå·¥ä½œæµç¨‹å’ŒAgenticçš„åŸºçº¿ç›¸æ¯”ï¼ŒLitaå®ç°äº†æœ‰ç«äº‰åŠ›æˆ–æ›´ä¼˜è¶Šçš„æ€§èƒ½ã€‚è‡³å…³é‡è¦çš„æ˜¯ï¼ŒLitaè¿˜æ¶ˆè€—æ›´å°‘çš„tokenï¼Œå¹¶ä¸”éœ€è¦æ˜¾è‘—æ›´å°‘çš„è®¾è®¡å·¥ä½œã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒLitaè¶³ä»¥æ­ç¤ºç°ä»£LLMçš„åº•å±‚ç¼–ç èƒ½åŠ›ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†Agentå¤æ‚æ€§å®šå¾‹ï¼šéšç€æ ¸å¿ƒæ¨¡å‹çš„æ”¹è¿›ï¼Œä»ç®€å•åˆ°å¤æ‚è®¾è®¡çš„å„ç§å¤æ‚åº¦çš„Agentä¹‹é—´çš„æ€§èƒ½å·®è·å°†ç¼©å°ï¼Œæœ€ç»ˆæ”¶æ•›åˆ°å¯ä»¥å¿½ç•¥ä¸è®¡çš„å·®å¼‚ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰ä»£ç Agentè®¾è®¡ä¾èµ–å¤æ‚çš„æ‰‹å·¥å·¥ä½œæµç¨‹å’Œå·¥å…·é›†ï¼Œå¯¼è‡´æ€§èƒ½è¿‡åº¦ä¾èµ–æç¤ºå·¥ç¨‹ï¼Œéš¾ä»¥è¯„ä¼°LLMçš„çœŸå®ç¼–ç èƒ½åŠ›ã€‚åŒæ—¶ï¼Œå¤æ‚pipelineçš„æ„å»ºå’Œç»´æŠ¤æˆæœ¬é«˜æ˜‚ï¼Œä¸”å­˜åœ¨æ•°æ®æ³„éœ²é£é™©ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§è½»é‡çº§ã€æ›´çœŸå®çš„è¯„ä¼°æ–¹æ³•æ¥æ­ç¤ºLLMçš„åº•å±‚ç¼–ç èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šLitaçš„æ ¸å¿ƒæ€è·¯æ˜¯æœ€å°åŒ–æ‰‹åŠ¨è®¾è®¡ï¼Œä¿ç•™è‡ªä¸»Agentçš„åŸºæœ¬è¦ç´ ï¼Œå®ç°â€œè½»é‡åŒ–â€ã€‚é€šè¿‡å‡å°‘å¯¹å¤æ‚å·¥ä½œæµç¨‹å’Œå·¥å…·é›†çš„ä¾èµ–ï¼ŒLitaèƒ½å¤Ÿæ›´ç›´æ¥åœ°è¯„ä¼°LLMçš„ç¼–ç èƒ½åŠ›ï¼Œé¿å…äººå·¥å¹²é¢„å¸¦æ¥çš„åå·®ã€‚è¿™ç§è½»é‡åŒ–çš„è®¾è®¡ä¹Ÿé™ä½äº†æ„å»ºå’Œç»´æŠ¤æˆæœ¬ï¼Œå¹¶å‡å°‘äº†æ•°æ®æ³„éœ²çš„é£é™©ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLitaçš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ ¸å¿ƒæ¨¡å—ï¼š(1) ä»»åŠ¡æ¥æ”¶æ¨¡å—ï¼šæ¥æ”¶æ¥è‡ªç”¨æˆ·çš„ç¼–ç¨‹ä»»åŠ¡æè¿°ã€‚(2) ä»£ç ç”Ÿæˆæ¨¡å—ï¼šåˆ©ç”¨LLMç”Ÿæˆä»£ç ã€‚(3) ä»£ç æ‰§è¡Œæ¨¡å—ï¼šæ‰§è¡Œç”Ÿæˆçš„ä»£ç ï¼Œå¹¶è·å–æ‰§è¡Œç»“æœã€‚(4) ç»“æœè¯„ä¼°æ¨¡å—ï¼šè¯„ä¼°ä»£ç çš„æ‰§è¡Œç»“æœï¼Œå¹¶ç”Ÿæˆåé¦ˆã€‚(5) è¿­ä»£ä¼˜åŒ–æ¨¡å—ï¼šæ ¹æ®åé¦ˆä¿¡æ¯ï¼Œè¿­ä»£ä¼˜åŒ–ä»£ç ç”Ÿæˆè¿‡ç¨‹ã€‚æ•´ä¸ªæµç¨‹ç®€æ´é«˜æ•ˆï¼Œé¿å…äº†å¤æ‚å·¥ä½œæµç¨‹å¸¦æ¥çš„é¢å¤–å¼€é”€ã€‚

**å…³é”®åˆ›æ–°**ï¼šLitaæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå…¶â€œè½»é‡åŒ–â€çš„è®¾è®¡ç†å¿µã€‚ä¸ä¼ ç»Ÿçš„ä»£ç Agentç›¸æ¯”ï¼ŒLitaå‡å°‘äº†å¯¹å¤æ‚å·¥ä½œæµç¨‹å’Œå·¥å…·é›†çš„ä¾èµ–ï¼Œæ›´åŠ æ³¨é‡LLMè‡ªèº«çš„ç¼–ç èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒLitaè¿˜æå‡ºäº†â€œAgentå¤æ‚æ€§å®šå¾‹â€ï¼Œå³éšç€æ ¸å¿ƒæ¨¡å‹çš„æ”¹è¿›ï¼Œä¸åŒå¤æ‚åº¦çš„Agentä¹‹é—´çš„æ€§èƒ½å·®è·å°†ç¼©å°ã€‚

**å…³é”®è®¾è®¡**ï¼šLitaçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š(1) ç®€æ´çš„ä»»åŠ¡æç¤ºï¼šé¿å…è¿‡åº¦å¤æ‚çš„æç¤ºå·¥ç¨‹ï¼Œå‡å°‘äººå·¥å¹²é¢„ã€‚(2) ç»Ÿä¸€çš„è¯„ä¼°æŒ‡æ ‡ï¼šé‡‡ç”¨ç»Ÿä¸€çš„è¯„ä¼°æŒ‡æ ‡æ¥è¡¡é‡LLMçš„ç¼–ç èƒ½åŠ›ï¼Œé¿å…å› è¯„ä¼°æ–¹æ³•ä¸åŒè€Œäº§ç”Ÿçš„åå·®ã€‚(3) è¿­ä»£ä¼˜åŒ–ç­–ç•¥ï¼šé€šè¿‡è¿­ä»£ä¼˜åŒ–ä»£ç ç”Ÿæˆè¿‡ç¨‹ï¼Œæé«˜ä»£ç çš„è´¨é‡å’Œæ•ˆç‡ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ç­‰æŠ€æœ¯ç»†èŠ‚æœªåœ¨è®ºæ–‡ä¸­è¯¦ç»†æè¿°ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

Litaåœ¨Aider Polyglotå’ŒSWE-Benchç­‰åŸºå‡†æµ‹è¯•ä¸­ï¼Œä¸åŸºäºå·¥ä½œæµç¨‹å’ŒAgenticçš„åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼Œå®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„ç”šè‡³æ›´ä¼˜è¶Šçš„æ€§èƒ½ã€‚åŒæ—¶ï¼ŒLitaæ¶ˆè€—çš„tokenæ›´å°‘ï¼Œæ‰€éœ€çš„è®¾è®¡å·¥ä½œé‡ä¹Ÿæ˜¾è‘—é™ä½ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒLitaèƒ½å¤Ÿæœ‰æ•ˆåœ°æ­ç¤ºç°ä»£LLMçš„åº•å±‚ç¼–ç èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Litaå¯åº”ç”¨äºLLMçš„ç¼–ç èƒ½åŠ›è¯„ä¼°ã€è‡ªåŠ¨åŒ–ä»£ç ç”Ÿæˆã€è½¯ä»¶å¼€å‘è¾…åŠ©ç­‰é¢†åŸŸã€‚å®ƒèƒ½å¤Ÿå¸®åŠ©å¼€å‘è€…æ›´å‡†ç¡®åœ°äº†è§£LLMçš„ç¼–ç èƒ½åŠ›ï¼Œå¹¶åˆ©ç”¨LLMå¿«é€Ÿç”Ÿæˆé«˜è´¨é‡çš„ä»£ç ã€‚æœªæ¥ï¼ŒLitaæœ‰æœ›æˆä¸ºLLMé©±åŠ¨çš„è½¯ä»¶å¼€å‘çš„é‡è¦å·¥å…·ï¼ŒåŠ é€Ÿè½¯ä»¶å¼€å‘è¿‡ç¨‹ï¼Œæé«˜å¼€å‘æ•ˆç‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) are increasingly being applied to programming tasks, ranging from single-turn code completion to autonomous agents. Current code agent designs frequently depend on complex, hand-crafted workflows and tool sets. However, this reliance on elaborate scaffolding presents several challenges: agent performance becomes overly dependent on prompt tuning and custom design choices, heavy human intervention obscures a model's true underlying capabilities, and intricate pipelines are costly to build and maintain. Furthermore, optimizing complex task prompts increases the risk of data leakage. Currently, when introducing new models, LLM providers like OpenAI and Anthropic often publish benchmark scores to demonstrate their models' coding proficiency, but keep their proprietary evaluation frameworks confidential. To address these limitations, we introduce Lita (Lite Agent), which operationalizes liteness, a principle of minimizing manual design while retaining the essential elements of a fully autonomous agent. Lita enables a more faithful and unified evaluation without elaborate scaffolding. Experiments on the Aider Polyglot and SWE-Bench with frontier models demonstrate that Lita achieves competitive or superior performance compared to workflow-based and agentic baselines. Crucially, Lita also consumes fewer tokens and requires significantly less design effort. Our results suggest that Lita is sufficient to reveal the underlying coding competence of modern LLMs. Finally, we propose the Agent Complexity Law: the performance gap between agents of varying complexity, from simple to sophisticated designs, will shrink as the core model improves, ultimately converging to a negligible difference.

