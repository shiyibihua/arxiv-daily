---
layout: default
title: Thinking Sparks!: Emergent Attention Heads in Reasoning Models During Post Training
---

# Thinking Sparks!: Emergent Attention Heads in Reasoning Models During Post Training

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25758" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25758v1</a>
  <a href="https://arxiv.org/pdf/2509.25758.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25758v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25758v1', 'Thinking Sparks!: Emergent Attention Heads in Reasoning Models During Post Training')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yein Park, Minbyul Jeong, Jaewoo Kang

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ­ç¤ºæ¨ç†æ¨¡å‹åè®­ç»ƒä¸­æ¶Œç°çš„æ³¨æ„åŠ›å¤´åŠå…¶å¯¹å¤æ‚æ¨ç†çš„å½±å“**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `åè®­ç»ƒ` `ç”µè·¯åˆ†æ` `æ³¨æ„åŠ›æœºåˆ¶` `æ¨ç†èƒ½åŠ›` `ç›‘ç£å¾®è°ƒ` `å¼ºåŒ–å­¦ä¹ ` `æ¨¡å‹è’¸é¦`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹æ¨ç†æ¨¡å‹ç¼ºä¹å¯¹å…¶åè®­ç»ƒæ”¹è¿›æœºåˆ¶çš„æ·±å…¥ç†è§£ï¼Œé˜»ç¢äº†æ¨¡å‹ä¼˜åŒ–ã€‚
2. é€šè¿‡ç”µè·¯åˆ†æï¼Œæ­ç¤ºåè®­ç»ƒè¿‡ç¨‹ä¸­æ¶Œç°å‡ºåŠŸèƒ½ä¸“ç”¨çš„æ³¨æ„åŠ›å¤´ï¼Œæ”¯æŒç»“æ„åŒ–æ¨ç†ã€‚
3. ç ”ç©¶å‘ç°ä¸åŒè®­ç»ƒæ–¹æ³•ï¼ˆSFTã€è’¸é¦ã€å¼ºåŒ–å­¦ä¹ ï¼‰å¯¼è‡´æ³¨æ„åŠ›å¤´æ¼”åŒ–æ–¹å¼ä¸åŒï¼Œå¹¶å½±å“æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°ä»£å¤§å‹æ¨ç†æ¨¡å‹çš„èƒ½åŠ›ä¸»è¦é€šè¿‡ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ç­‰åè®­ç»ƒæŠ€æœ¯è§£é”ã€‚ç„¶è€Œï¼Œè¿™äº›æ”¹è¿›èƒŒåçš„æ¶æ„æœºåˆ¶åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä»ç„¶ä¸é€æ˜ã€‚æœ¬æ–‡åˆ©ç”¨ç”µè·¯åˆ†æè¡¨æ˜ï¼Œå¤æ‚æ¨ç†çš„åè®­ç»ƒä¼šæ¿€å‘æ–°å‹ã€åŠŸèƒ½ä¸“ç”¨çš„æ³¨æ„åŠ›å¤´çš„æ¶Œç°ã€‚è¿™äº›æ³¨æ„åŠ›å¤´å…±åŒæ”¯æŒç»“æ„åŒ–æ¨ç†å’Œè®¡ç®—ã€‚é€šè¿‡å¯¹Qwenç³»åˆ—å’ŒDeepSeek-distilledæ¨¡å‹çš„æ¯”è¾ƒåˆ†æï¼Œæ­ç¤ºäº†è¿™äº›æ¶Œç°çš„æ³¨æ„åŠ›å¤´åœ¨ä¸åŒè®­ç»ƒæœºåˆ¶ä¸‹ä»¥ä¸åŒçš„æ–¹å¼æ¼”åŒ–ã€‚è’¸é¦å’ŒSFTä¿ƒè¿›äº†ç¨³å®šæ¨ç†å¤´çš„ç´¯ç§¯å¢åŠ ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ä»¥åŠ¨æ€æœç´¢æ¨¡å¼è¿è¡Œï¼šç›¸å¯¹è¾ƒå°‘çš„æ³¨æ„åŠ›å¤´è¢«è¿­ä»£æ¿€æ´»ã€è¯„ä¼°å’Œä¿®å‰ªï¼Œå®ƒä»¬çš„å­˜æ´»ä¸ä»»åŠ¡å¥–åŠ±ä¿¡å·çš„æ³¢åŠ¨å¯†åˆ‡ç›¸å…³ã€‚æ­¤å¤–ï¼Œå¯æ§çš„think on/offæ¨¡å‹ä¸å…·å¤‡ä¸“ç”¨çš„æ€è€ƒå¤´ã€‚ç›¸åï¼Œå…³é—­æ˜¾å¼æ¨ç†ä¼šè§¦å‘æ›´å¹¿æ³›ä½†æ•ˆç‡è¾ƒä½çš„è¡¥å¿å¤´é›†åˆã€‚é€šè¿‡æ¶ˆèå’Œå®šæ€§åˆ†æï¼Œå°†è¿™äº›ç”µè·¯å±‚é¢çš„åŠ¨æ€ä¸ä¸€ä¸ªå…³é”®çš„æ€§èƒ½æƒè¡¡è”ç³»èµ·æ¥ï¼šå¼ºåŒ–çš„æ³¨æ„åŠ›å¤´èƒ½å¤Ÿä¸ºéš¾é¢˜æä¾›å¤æ‚çš„è§£å†³ç­–ç•¥ï¼Œä½†ä¹Ÿå¯èƒ½å¼•å…¥è¿‡åº¦æ€è€ƒçš„å¤±è´¥æ¨¡å¼ï¼Œä¾‹å¦‚ç®€å•ä»»åŠ¡ä¸Šçš„è®¡ç®—é”™è¯¯æˆ–é€»è¾‘å¾ªç¯ã€‚è¿™äº›å‘ç°å°†ç”µè·¯å±‚é¢çš„åŠ¨æ€ä¸å®è§‚å±‚é¢çš„æ€§èƒ½è”ç³»èµ·æ¥ï¼Œè¯†åˆ«å‡ºä¸€ç§å›ºæœ‰çš„å¼ åŠ›ï¼Œå³å¤æ‚æ¨ç†çš„ä»£ä»·æ˜¯åŸºæœ¬è®¡ç®—ã€‚æ›´å¹¿æ³›åœ°è¯´ï¼Œè¿™é¡¹å·¥ä½œä¸ºè®­ç»ƒç­–ç•¥è®¾è®¡æŒ‡æ˜äº†æœªæ¥çš„æ–¹å‘ï¼Œå¼ºè°ƒéœ€è¦å¹³è¡¡æœ‰æ•ˆæ¨ç†ç­–ç•¥çš„å¼€å‘ä¸å¯é ã€å®Œç¾æ‰§è¡Œçš„ä¿è¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹æ¨ç†æ¨¡å‹åœ¨åè®­ç»ƒé˜¶æ®µèƒ½åŠ›æå‡çš„æœºåˆ¶ä¸æ˜ç¡®çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥è§£é‡Šæ¨¡å‹å†…éƒ¨ç»“æ„å¦‚ä½•æ”¯æŒå¤æ‚æ¨ç†ï¼Œä»¥åŠä¸åŒè®­ç»ƒç­–ç•¥å¦‚ä½•å½±å“æ¨¡å‹æ€§èƒ½ã€‚è¿™ç§ä¸é€æ˜æ€§é˜»ç¢äº†å¯¹æ¨¡å‹è¡Œä¸ºçš„ç†è§£å’Œè¿›ä¸€æ­¥ä¼˜åŒ–ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ç”µè·¯åˆ†ææ–¹æ³•ï¼Œç ”ç©¶æ¨¡å‹å†…éƒ¨æ³¨æ„åŠ›å¤´çš„è¡Œä¸ºï¼Œæ­ç¤ºåœ¨åè®­ç»ƒè¿‡ç¨‹ä¸­æ¶Œç°çš„ã€åŠŸèƒ½ä¸“ç”¨çš„æ³¨æ„åŠ›å¤´ã€‚é€šè¿‡æ¯”è¾ƒä¸åŒè®­ç»ƒç­–ç•¥ï¼ˆå¦‚ç›‘ç£å¾®è°ƒã€è’¸é¦ã€å¼ºåŒ–å­¦ä¹ ï¼‰ä¸‹æ³¨æ„åŠ›å¤´çš„æ¼”åŒ–æ¨¡å¼ï¼Œç†è§£ä¸åŒè®­ç»ƒæ–¹æ³•å¯¹æ¨¡å‹æ¨ç†èƒ½åŠ›çš„å½±å“ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) é€‰æ‹©Qwenç³»åˆ—å’ŒDeepSeek-distilledæ¨¡å‹ä½œä¸ºç ”ç©¶å¯¹è±¡ã€‚2) ä½¿ç”¨ç”µè·¯åˆ†ææŠ€æœ¯ï¼Œè¯†åˆ«æ¨¡å‹ä¸­æ¶Œç°çš„æ³¨æ„åŠ›å¤´ã€‚3) å¯¹æ¯”ä¸åŒè®­ç»ƒç­–ç•¥ï¼ˆSFTã€è’¸é¦ã€å¼ºåŒ–å­¦ä¹ ï¼‰ä¸‹æ³¨æ„åŠ›å¤´çš„æ¼”åŒ–æ¨¡å¼ã€‚4) é€šè¿‡æ¶ˆèå®éªŒå’Œå®šæ€§åˆ†æï¼Œç ”ç©¶æ³¨æ„åŠ›å¤´å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œç‰¹åˆ«æ˜¯å¤æ‚æ¨ç†å’Œç®€å•è®¡ç®—ä¹‹é—´çš„æƒè¡¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºï¼š1) é¦–æ¬¡ä½¿ç”¨ç”µè·¯åˆ†ææ–¹æ³•ç ”ç©¶å¤§å‹æ¨ç†æ¨¡å‹åè®­ç»ƒé˜¶æ®µçš„å†…éƒ¨æœºåˆ¶ã€‚2) æ­ç¤ºäº†åœ¨åè®­ç»ƒè¿‡ç¨‹ä¸­æ¶Œç°çš„åŠŸèƒ½ä¸“ç”¨çš„æ³¨æ„åŠ›å¤´ï¼Œå¹¶é˜æ˜äº†å®ƒä»¬åœ¨ç»“æ„åŒ–æ¨ç†ä¸­çš„ä½œç”¨ã€‚3) å‘ç°äº†ä¸åŒè®­ç»ƒç­–ç•¥å¯¹æ³¨æ„åŠ›å¤´æ¼”åŒ–çš„ä¸åŒå½±å“ï¼Œä»¥åŠç”±æ­¤å¸¦æ¥çš„æ€§èƒ½æƒè¡¡ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) é€‰æ‹©å…·æœ‰ä»£è¡¨æ€§çš„Qwenç³»åˆ—å’ŒDeepSeek-distilledæ¨¡å‹ï¼Œä¿è¯ç ”ç©¶ç»“æœçš„æ³›åŒ–æ€§ã€‚2) ä½¿ç”¨æ¶ˆèå®éªŒï¼ŒéªŒè¯ç‰¹å®šæ³¨æ„åŠ›å¤´å¯¹æ¨¡å‹æ€§èƒ½çš„å…³é”®ä½œç”¨ã€‚3) é€šè¿‡å®šæ€§åˆ†æï¼Œæ·±å…¥ç†è§£æ³¨æ„åŠ›å¤´åœ¨ä¸åŒä»»åŠ¡ä¸­çš„è¡Œä¸ºæ¨¡å¼ã€‚4) å…³æ³¨å¤æ‚æ¨ç†å’Œç®€å•è®¡ç®—ä¹‹é—´çš„æ€§èƒ½æƒè¡¡ï¼Œæ­ç¤ºæ¨¡å‹ä¼˜åŒ–çš„æ½œåœ¨æŒ‘æˆ˜ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

ç ”ç©¶å‘ç°ï¼Œè’¸é¦å’ŒSFTè®­ç»ƒä¿ƒè¿›äº†ç¨³å®šæ¨ç†å¤´çš„ç´¯ç§¯å¢åŠ ï¼Œè€Œç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–åˆ™é‡‡ç”¨åŠ¨æ€æœç´¢æ¨¡å¼ï¼Œè¿­ä»£æ¿€æ´»ã€è¯„ä¼°å’Œä¿®å‰ªæ³¨æ„åŠ›å¤´ã€‚å¯æ§çš„think on/offæ¨¡å‹ä¸å…·å¤‡ä¸“ç”¨çš„æ€è€ƒå¤´ï¼Œå…³é—­æ˜¾å¼æ¨ç†ä¼šè§¦å‘æ›´å¹¿æ³›ä½†æ•ˆç‡è¾ƒä½çš„è¡¥å¿å¤´é›†åˆã€‚å¼ºåŒ–åçš„æ³¨æ„åŠ›å¤´è™½ç„¶èƒ½è§£å†³éš¾é¢˜ï¼Œä½†ä¹Ÿå¯èƒ½å¯¼è‡´ç®€å•ä»»åŠ¡ä¸Šçš„è®¡ç®—é”™è¯¯æˆ–é€»è¾‘å¾ªç¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ”¹è¿›å¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒç­–ç•¥ï¼Œä¾‹å¦‚é€šè¿‡æœ‰é’ˆå¯¹æ€§åœ°è®­ç»ƒç‰¹å®šç±»å‹çš„æ³¨æ„åŠ›å¤´æ¥æå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶æœ‰åŠ©äºå¼€å‘æ›´å¯é ã€æ›´é«˜æ•ˆçš„æ¨ç†æ¨¡å‹ï¼Œåº”ç”¨äºæ™ºèƒ½é—®ç­”ã€è‡ªåŠ¨ç¼–ç¨‹ã€ç§‘å­¦ç ”ç©¶ç­‰é¢†åŸŸã€‚æœªæ¥çš„ç ”ç©¶å¯ä»¥æ¢ç´¢å¦‚ä½•å¹³è¡¡å¤æ‚æ¨ç†å’Œç®€å•è®¡ç®—ï¼Œé¿å…æ¨¡å‹åœ¨ç®€å•ä»»åŠ¡ä¸Šå‡ºç°è¿‡åº¦æ€è€ƒçš„é”™è¯¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The remarkable capabilities of modern large reasoning models are largely unlocked through post-training techniques such as supervised fine-tuning and reinforcement learning. However, the architectural mechanisms behind such improvements remain largely opaque. In this work, we use circuit analysis to demonstrate that post-training for complex reasoning sparks the emergence of novel, functionally specialized attention heads. These heads collectively support structured reasoning and computation. Our comparative analysis across Qwen families and DeepSeek-distilled model reveals that these emergent heads evolve differently under different training regimes. Distillation and SFT foster a cumulative addition of stable reasoning heads. In contrast, group relative policy optimization operates in a dynamic search mode: relatively few attention heads are iteratively activated, evaluated, and pruned, with their survival closely tracking fluctuations in the task reward signal. Furthermore, we find that controllable think on/off models do not possess dedicated thinking heads. Instead, turning off explicit reasoning triggers a broader-but less efficient-set of compensatory heads. Through ablation and qualitative analyses, we connect these circuit-level dynamics to a crucial performance trade-off: strengthened heads enable sophisticated problem-solving strategies for difficult problems but can also introduce over-thinking failure modes, such as calculation errors or logical loops on simpler tasks. These findings connect circuit-level dynamics to macro-level performance, identifying an inherent tension where complex reasoning comes at the cost of elementary computations. More broadly, our work points to future directions for training policy design, emphasizing the need to balance the development of effective reasoning strategies with the assurance of reliable, flawless execution.

