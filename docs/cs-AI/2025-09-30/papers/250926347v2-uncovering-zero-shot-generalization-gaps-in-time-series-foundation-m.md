---
layout: default
title: Uncovering Zero-Shot Generalization Gaps in Time-Series Foundation Models Using Real-World Videos
---

# Uncovering Zero-Shot Generalization Gaps in Time-Series Foundation Models Using Real-World Videos

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.26347" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.26347v2</a>
  <a href="https://arxiv.org/pdf/2509.26347.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.26347v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.26347v2', 'Uncovering Zero-Shot Generalization Gaps in Time-Series Foundation Models Using Real-World Videos')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Lujun Li, Lama Sleem, Yiqun Wang, Yangjie Xu, NiccolÃ² Gentile, Radu State

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30 (æ›´æ–°: 2025-11-28)

**å¤‡æ³¨**: This paper has been accepted by Artificial Intelligence for Time Series Analysis (AI4TS) Workshop @ AAAI 2026: Theory, Algorithms, and Applications

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºREAL-V-TSFMæ•°æ®é›†ï¼Œæ­ç¤ºæ—¶åºåŸºç¡€æ¨¡å‹åœ¨çœŸå®è§†é¢‘æ•°æ®ä¸Šçš„æ³›åŒ–å·®è·**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹` `é›¶æ ·æœ¬å­¦ä¹ ` `æ³›åŒ–èƒ½åŠ›` `çœŸå®ä¸–ç•Œè§†é¢‘` `å…‰æµæ³•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ—¶åºåŸºç¡€æ¨¡å‹æ•°æ®é›†ç¼ºä¹çœŸå®ä¸–ç•Œæ•°æ®ï¼Œè¿‡åº¦ä¾èµ–åˆæˆæ•°æ®ï¼Œå¯¼è‡´æ¨¡å‹æ³›åŒ–èƒ½åŠ›å—é™ã€‚
2. æå‡ºREAL-V-TSFMæ•°æ®é›†ï¼Œåˆ©ç”¨å…‰æµæŠ€æœ¯ä»çœŸå®è§†é¢‘ä¸­æå–æ—¶é—´åºåˆ—ï¼Œæ¨¡æ‹ŸçœŸå®ç‰©ç†ä¸–ç•Œçš„æ—¶åºåŠ¨æ€ã€‚
3. å®éªŒè¡¨æ˜ï¼Œç°æœ‰æ¨¡å‹åœ¨REAL-V-TSFMæ•°æ®é›†ä¸Šæ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œæ­ç¤ºäº†æ¨¡å‹åœ¨çœŸå®åœºæ™¯ä¸‹çš„æ³›åŒ–å·®è·ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æ—¨åœ¨è§£å†³æ—¶åºåŸºç¡€æ¨¡å‹ï¼ˆTSFMsï¼‰åœ¨çœŸå®ä¸–ç•Œæ•°æ®ä¸Šæ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ•°æ®é›†é€šå¸¸ä¾èµ–åˆæˆæ•°æ®ï¼Œå…¶æ³›åŒ–æ€§å¤‡å—äº‰è®®ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åŸºå‡†æµ‹è¯•æ–¹æ³•ï¼Œæ„å»ºäº†ä¸€ä¸ªåä¸ºREAL-V-TSFMçš„æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†é€šè¿‡å…‰æµæŠ€æœ¯ä»çœŸå®ä¸–ç•Œè§†é¢‘ä¸­æå–æ—¶é—´åºåˆ—ä¿¡å·ï¼Œä»è€Œåæ˜ çœŸå®çš„ç‰©ç†æ—¶é—´åŠ¨æ€ã€‚åœ¨é›¶æ ·æœ¬é¢„æµ‹çš„å®éªŒä¸­ï¼Œæœ€å…ˆè¿›çš„TSFMsåœ¨ä¼ ç»ŸåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æˆ‘ä»¬æå‡ºçš„æ•°æ®é›†ä¸Šæ€§èƒ½ä¸‹é™ï¼Œè¡¨æ˜å…¶æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†è·å–æ—¶é—´åºåˆ—æ•°æ®çš„æ–°æ–¹æ³•çš„éœ€æ±‚ï¼Œå¹¶çªå‡ºäº†ç°æœ‰TSFMsçš„å±€é™æ€§ï¼ŒåŒæ—¶ä¹ŸéªŒè¯äº†æˆ‘ä»¬åŸºäºè§†é¢‘çš„æ—¶é—´åºåˆ—æ•°æ®æå–æµç¨‹çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è¯„ä¼°ç°æœ‰æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹ï¼ˆTSFMsï¼‰åœ¨çœŸå®ä¸–ç•Œæ•°æ®ä¸Šçš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚ç°æœ‰TSFMæ•°æ®é›†é€šå¸¸åŒ…å«å¤§é‡åˆæˆæ•°æ®ï¼Œè¿™ä½¿å¾—æ¨¡å‹åœ¨è¿™äº›æ•°æ®é›†ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†æ— æ³•ä¿è¯å…¶åœ¨çœŸå®åœºæ™¯ä¸­çš„æ€§èƒ½ã€‚å› æ­¤ï¼Œå¦‚ä½•è¯„ä¼°TSFMåœ¨çœŸå®ä¸–ç•Œæ•°æ®ä¸Šçš„æ³›åŒ–èƒ½åŠ›æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ„å»ºä¸€ä¸ªåŸºäºçœŸå®ä¸–ç•Œè§†é¢‘çš„æ—¶é—´åºåˆ—æ•°æ®é›†ï¼Œæ¥è¯„ä¼°TSFMçš„æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ•°æ®é›†é€šè¿‡ä»çœŸå®è§†é¢‘ä¸­æå–æ—¶é—´åºåˆ—ä¿¡å·ï¼Œä»è€Œåæ˜ çœŸå®çš„ç‰©ç†æ—¶é—´åŠ¨æ€ã€‚é€šè¿‡åœ¨è¯¥æ•°æ®é›†ä¸Šæµ‹è¯•TSFMçš„æ€§èƒ½ï¼Œå¯ä»¥æ›´å‡†ç¡®åœ°è¯„ä¼°å…¶åœ¨çœŸå®åœºæ™¯ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) æ”¶é›†çœŸå®ä¸–ç•Œè§†é¢‘æ•°æ®ï¼›2) ä½¿ç”¨å…‰æµæŠ€æœ¯ä»è§†é¢‘ä¸­æå–æ—¶é—´åºåˆ—ä¿¡å·ï¼›3) æ„å»ºREAL-V-TSFMæ•°æ®é›†ï¼›4) åœ¨REAL-V-TSFMæ•°æ®é›†ä¸Šæµ‹è¯•ç°æœ‰TSFMçš„é›¶æ ·æœ¬é¢„æµ‹æ€§èƒ½ï¼›5) åˆ†æå®éªŒç»“æœï¼Œè¯„ä¼°TSFMçš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†REAL-V-TSFMæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ˜¯ç¬¬ä¸€ä¸ªåŸºäºçœŸå®ä¸–ç•Œè§†é¢‘çš„æ—¶é—´åºåˆ—æ•°æ®é›†ã€‚ä¸ç°æœ‰æ•°æ®é›†ç›¸æ¯”ï¼ŒREAL-V-TSFMæ•°æ®é›†æ›´èƒ½åæ˜ çœŸå®çš„ç‰©ç†æ—¶é—´åŠ¨æ€ï¼Œå› æ­¤å¯ä»¥æ›´å‡†ç¡®åœ°è¯„ä¼°TSFMçš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥è®ºæ–‡è¿˜æå‡ºäº†ä¸€ç§åŸºäºå…‰æµæŠ€æœ¯çš„æ—¶é—´åºåˆ—æå–æµç¨‹ï¼Œè¯¥æµç¨‹å¯ä»¥æœ‰æ•ˆåœ°ä»è§†é¢‘ä¸­æå–æ—¶é—´åºåˆ—ä¿¡å·ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°æ®æå–æ–¹é¢ï¼Œè®ºæ–‡ä½¿ç”¨äº†å…‰æµæ³•æ¥ä¼°è®¡è§†é¢‘ä¸­åƒç´ çš„è¿åŠ¨ï¼Œå¹¶å°†è¿™äº›è¿åŠ¨ä¿¡æ¯è½¬æ¢ä¸ºæ—¶é—´åºåˆ—ä¿¡å·ã€‚å…·ä½“æ¥è¯´ï¼Œè®ºæ–‡ä½¿ç”¨äº†Dense Optical Flowç®—æ³•ï¼Œè¯¥ç®—æ³•å¯ä»¥è®¡ç®—è§†é¢‘ä¸­æ¯ä¸ªåƒç´ çš„è¿åŠ¨çŸ¢é‡ã€‚ç„¶åï¼Œè®ºæ–‡å°†è¿™äº›è¿åŠ¨çŸ¢é‡è¿›è¡Œèšåˆï¼Œå¾—åˆ°ä¸€ä¸ªä»£è¡¨æ•´ä¸ªè§†é¢‘çš„æ—¶é—´åºåˆ—ä¿¡å·ã€‚åœ¨å®éªŒæ–¹é¢ï¼Œè®ºæ–‡é€‰æ‹©äº†å¤šä¸ªæœ€å…ˆè¿›çš„TSFMä½œä¸ºåŸºçº¿æ¨¡å‹ï¼Œå¹¶åœ¨REAL-V-TSFMæ•°æ®é›†ä¸Šè¿›è¡Œäº†é›¶æ ·æœ¬é¢„æµ‹å®éªŒã€‚è®ºæ–‡ä½¿ç”¨äº†å¸¸ç”¨çš„æ—¶é—´åºåˆ—é¢„æµ‹æŒ‡æ ‡ï¼Œå¦‚å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰å’Œå¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰ï¼Œæ¥è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡ç°æœ‰TSFMåœ¨ä¼ ç»Ÿæ•°æ®é›†ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨REAL-V-TSFMæ•°æ®é›†ä¸Šæ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚ä¾‹å¦‚ï¼ŒæŸäº›æ¨¡å‹åœ¨REAL-V-TSFMä¸Šçš„é¢„æµ‹è¯¯å·®æ¯”åœ¨ä¼ ç»Ÿæ•°æ®é›†ä¸Šé«˜å‡º50%ä»¥ä¸Šï¼Œè¿™è¡¨æ˜ç°æœ‰æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œæ•°æ®ä¸Šçš„æ³›åŒ–èƒ½åŠ›å­˜åœ¨æ˜æ˜¾å·®è·ã€‚è¿™äº›ç»“æœçªå‡ºäº†REAL-V-TSFMæ•°æ®é›†çš„ä»·å€¼ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ–°çš„æ–¹å‘ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè¯„ä¼°å’Œæ”¹è¿›æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹åœ¨çœŸå®åœºæ™¯ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¾‹å¦‚ï¼Œåœ¨æ™ºèƒ½äº¤é€šã€é‡‘èé¢„æµ‹ã€å·¥ä¸šç›‘æ§ç­‰é¢†åŸŸï¼Œå¯ä»¥åˆ©ç”¨è¯¥æ•°æ®é›†å’Œè¯„ä¼°æ–¹æ³•æ¥é€‰æ‹©å’Œä¼˜åŒ–é€‚ç”¨äºç‰¹å®šåœºæ™¯çš„TSFMï¼Œä»è€Œæé«˜é¢„æµ‹ç²¾åº¦å’Œå¯é æ€§ã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶å¯ä»¥ä¿ƒè¿›æ›´é€šç”¨ã€æ›´é²æ£’çš„æ—¶åºæ¨¡å‹çš„å¼€å‘ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent research on time-series foundation models (TSFMs) has underscored the scarcity of real-world data, often supplemented with synthetic sources in existing datasets, whose generalizability remains however debated. As such, in this work, we propose a novel benchmarking approach: in particular, we aim at building a curated dataset reflecting real world physical temporal dynamics, extracting temporal signals from real-world videos using optical flow. As such, we introduce REAL-V-TSFM, a novel dataset designed to capture rich and diverse time series derived from real-world videos. Experimental results on state-of-the-art TSFMs under zero-shot forecasting show that, despite strong performance on conventional benchmarks, these models exhibit performance degradation on the proposed dataset, suggesting limited generalizability to novel datasets. These findings underscore the need for novel approaches to acquiring time series data and highlight the lack of universality in recent TSFMs, while further validating the effectiveness of our video-based time series data extraction pipeline.

