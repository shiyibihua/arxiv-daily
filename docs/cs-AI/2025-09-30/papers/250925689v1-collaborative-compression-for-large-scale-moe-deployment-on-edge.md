---
layout: default
title: Collaborative Compression for Large-Scale MoE Deployment on Edge
---

# Collaborative Compression for Large-Scale MoE Deployment on Edge

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25689" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25689v1</a>
  <a href="https://arxiv.org/pdf/2509.25689.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25689v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25689v1', 'Collaborative Compression for Large-Scale MoE Deployment on Edge')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yixiao Chen, Yanyue Xie, Ruining Yang, Wei Jiang, Wei Wang, Yong He, Yue Chen, Pu Zhao, Yanzhi Wang

**åˆ†ç±»**: cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºååŒå‹ç¼©æ¡†æ¶ï¼Œå®ç°è¶…å¤§MoEæ¨¡å‹åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šçš„é«˜æ•ˆéƒ¨ç½²**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ··åˆä¸“å®¶æ¨¡å‹` `æ¨¡å‹å‹ç¼©` `è¾¹ç¼˜éƒ¨ç½²` `ä¸“å®¶å‰ªæ` `æ··åˆç²¾åº¦é‡åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è¶…å¤§MoEæ¨¡å‹å‚æ•°é‡å·¨å¤§ï¼Œéš¾ä»¥ç›´æ¥éƒ¨ç½²åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šï¼Œå•çº¯çš„å‰ªææˆ–é‡åŒ–éš¾ä»¥æ»¡è¶³éœ€æ±‚ã€‚
2. æå‡ºååŒå‹ç¼©æ¡†æ¶ï¼Œç»“åˆä¸“å®¶å‰ªæã€æ··åˆç²¾åº¦é‡åŒ–å’Œæ¿€æ´»ä¼˜åŒ–ï¼Œåœ¨å‹ç¼©æ¨¡å‹çš„åŒæ—¶ä¿æŒç²¾åº¦ã€‚
3. æˆåŠŸå°†DeepSeek-V3æ¨¡å‹å‹ç¼©è‡³103GBï¼Œå¹¶åœ¨128GBå†…å­˜é™åˆ¶çš„è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²ï¼Œç²¾åº¦ä¼˜äºä¼ ç»Ÿé‡åŒ–æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§ååŒå‹ç¼©æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è¶…å¤§è§„æ¨¡æ··åˆä¸“å®¶æ¨¡å‹ï¼ˆMoEï¼‰åœ¨èµ„æºå—é™çš„è¾¹ç¼˜å¹³å°ä¸Šçš„éƒ¨ç½²éš¾é¢˜ã€‚è¯¥æ¡†æ¶ç»“åˆäº†ä¸“å®¶å‰ªæã€æ··åˆç²¾åº¦é‡åŒ–å’Œæ¿€æ´»ä¼˜åŒ–ç­‰æŠ€æœ¯ï¼Œèƒ½å¤Ÿæœ‰æ•ˆé™ä½æ¨¡å‹å­˜å‚¨ç©ºé—´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æˆåŠŸå°†DeepSeek-V3æ¨¡å‹çš„ä½“ç§¯ä»1.3TBå‹ç¼©åˆ°103GBï¼Œå¹¶åœ¨ä¿æŒé«˜è¾“å‡ºè´¨é‡çš„åŒæ—¶ï¼Œå®ç°äº†æ¯”ä¼ ç»Ÿå‡åŒ€ä½æ¯”ç‰¹é‡åŒ–æ–¹æ³•æ›´é«˜çš„ç²¾åº¦ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡åœ¨æ€»å†…å­˜é™åˆ¶ä¸º128GBçš„å¹³å°ä¸Šéƒ¨ç½²æ¥è‡ªè¶…å¤§è§„æ¨¡DeepSeek-V3çš„å‹ç¼©æ¨¡å‹ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•å’Œå„ç§å†…å­˜çº¦æŸä¸‹çš„ç»¼åˆå®éªŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜å…¶åœ¨æ›´å°çš„æ¨¡å‹å°ºå¯¸ä¸‹èƒ½å¤Ÿå®ç°æ¯”å‡åŒ€ä½æ¯”ç‰¹é‡åŒ–æ–¹æ³•æ›´é«˜çš„ç²¾åº¦ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè¶…å¤§è§„æ¨¡æ··åˆä¸“å®¶æ¨¡å‹ï¼ˆMoEï¼‰æ‹¥æœ‰æ•°åƒäº¿å‚æ•°ï¼Œå¯¼è‡´å…¶å­˜å‚¨éœ€æ±‚å·¨å¤§ï¼Œéš¾ä»¥éƒ¨ç½²åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šã€‚ç°æœ‰çš„å‰ªææˆ–é‡åŒ–æ–¹æ³•åœ¨é¢å¯¹å¦‚æ­¤é«˜çš„å‹ç¼©ç‡éœ€æ±‚æ—¶ï¼Œå¾€å¾€ä¼šå¯¼è‡´æ˜¾è‘—çš„ç²¾åº¦ä¸‹é™ï¼Œæ— æ³•æ»¡è¶³å®é™…åº”ç”¨çš„éœ€æ±‚ã€‚å› æ­¤ï¼Œå¦‚ä½•åœ¨ä¿è¯æ¨¡å‹ç²¾åº¦çš„å‰æä¸‹ï¼Œå¤§å¹…åº¦é™ä½MoEæ¨¡å‹çš„å­˜å‚¨ç©ºé—´ï¼Œæ˜¯æœ¬æ–‡è¦è§£å†³çš„æ ¸å¿ƒé—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é‡‡ç”¨ååŒå‹ç¼©çš„æ–¹æ³•ï¼Œå³ç»“åˆå¤šç§å‹ç¼©æŠ€æœ¯ï¼Œå……åˆ†åˆ©ç”¨å„ç§å‹ç¼©æ–¹æ³•çš„ä¼˜åŠ¿ï¼Œå¼¥è¡¥å•ä¸€æ–¹æ³•çš„ä¸è¶³ã€‚é€šè¿‡ä¸“å®¶å‰ªæå‡å°‘æ¨¡å‹å‚æ•°é‡ï¼Œæ··åˆç²¾åº¦é‡åŒ–é™ä½å‚æ•°å­˜å‚¨ä½æ•°ï¼Œæ¿€æ´»ä¼˜åŒ–è¿›ä¸€æ­¥å‡å°‘è®¡ç®—è¿‡ç¨‹ä¸­çš„å†…å­˜å ç”¨ï¼Œä»è€Œåœ¨æ•´ä½“ä¸Šå®ç°æ›´é«˜çš„å‹ç¼©ç‡å’Œæ›´å¥½çš„ç²¾åº¦ä¿æŒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥ååŒå‹ç¼©æ¡†æ¶ä¸»è¦åŒ…å«ä¸‰ä¸ªé˜¶æ®µï¼šä¸“å®¶å‰ªæã€æ··åˆç²¾åº¦é‡åŒ–å’Œæ¿€æ´»ä¼˜åŒ–ã€‚é¦–å…ˆï¼Œé€šè¿‡ä¸“å®¶å‰ªæï¼Œç§»é™¤å¯¹æ¨¡å‹æ€§èƒ½å½±å“è¾ƒå°çš„ä¸“å®¶ï¼Œå‡å°‘æ¨¡å‹å‚æ•°é‡ã€‚ç„¶åï¼Œå¯¹æ¨¡å‹å‚æ•°è¿›è¡Œæ··åˆç²¾åº¦é‡åŒ–ï¼Œå¯¹ä¸åŒå±‚æˆ–ä¸åŒå‚æ•°é‡‡ç”¨ä¸åŒçš„é‡åŒ–æ¯”ç‰¹æ•°ï¼Œä»¥åœ¨ç²¾åº¦å’Œå­˜å‚¨ç©ºé—´ä¹‹é—´å–å¾—å¹³è¡¡ã€‚æœ€åï¼Œé€šè¿‡æ¿€æ´»ä¼˜åŒ–ï¼Œå‡å°‘æ¨¡å‹æ¨ç†è¿‡ç¨‹ä¸­çš„å†…å­˜å ç”¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºååŒå‹ç¼©çš„æ€æƒ³ï¼Œå³å°†å¤šç§å‹ç¼©æŠ€æœ¯æœ‰æœºç»“åˆï¼Œè€Œä¸æ˜¯å­¤ç«‹åœ°ä½¿ç”¨å•ä¸€æ–¹æ³•ã€‚è¿™ç§ååŒçš„æ–¹å¼èƒ½å¤Ÿå……åˆ†åˆ©ç”¨å„ç§å‹ç¼©æŠ€æœ¯çš„ä¼˜åŠ¿ï¼Œå®ç°æ›´é«˜çš„å‹ç¼©ç‡å’Œæ›´å¥½çš„ç²¾åº¦ä¿æŒã€‚æ­¤å¤–ï¼Œé’ˆå¯¹MoEæ¨¡å‹çš„ç‰¹ç‚¹ï¼Œå¯¹ä¸“å®¶å‰ªæç­–ç•¥è¿›è¡Œäº†ä¼˜åŒ–ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å‡å°‘æ¨¡å‹å‚æ•°é‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ä¸“å®¶å‰ªææ–¹é¢ï¼Œé‡‡ç”¨äº†åŸºäºé‡è¦æ€§çš„å‰ªæç­–ç•¥ï¼Œå³æ ¹æ®ä¸“å®¶å¯¹æ¨¡å‹æ€§èƒ½çš„è´¡çŒ®ç¨‹åº¦æ¥å†³å®šæ˜¯å¦å‰ªæã€‚åœ¨æ··åˆç²¾åº¦é‡åŒ–æ–¹é¢ï¼Œé‡‡ç”¨äº†åŠ¨æ€é‡åŒ–ç­–ç•¥ï¼Œå³æ ¹æ®ä¸åŒå±‚æˆ–ä¸åŒå‚æ•°çš„æ•æ„Ÿç¨‹åº¦æ¥é€‰æ‹©ä¸åŒçš„é‡åŒ–æ¯”ç‰¹æ•°ã€‚åœ¨æ¿€æ´»ä¼˜åŒ–æ–¹é¢ï¼Œé‡‡ç”¨äº†å†…å­˜å¤ç”¨æŠ€æœ¯ï¼Œå³åœ¨è®¡ç®—è¿‡ç¨‹ä¸­å°½å¯èƒ½åœ°å¤ç”¨å†…å­˜ç©ºé—´ï¼Œå‡å°‘å†…å­˜å ç”¨ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æˆåŠŸå°†DeepSeek-V3æ¨¡å‹çš„ä½“ç§¯ä»1.3TBå‹ç¼©åˆ°103GBï¼Œå‹ç¼©ç‡è¶…è¿‡90%ã€‚åœ¨ä¿æŒé«˜è¾“å‡ºè´¨é‡çš„åŒæ—¶ï¼Œè¯¥æ–¹æ³•å®ç°äº†æ¯”ä¼ ç»Ÿå‡åŒ€ä½æ¯”ç‰¹é‡åŒ–æ–¹æ³•æ›´é«˜çš„ç²¾åº¦ã€‚åœ¨128GBå†…å­˜é™åˆ¶çš„å¹³å°ä¸ŠæˆåŠŸéƒ¨ç½²äº†å‹ç¼©åçš„DeepSeek-V3æ¨¡å‹ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­çš„å¯è¡Œæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½å¯¹è¯ã€æœºå™¨ç¿»è¯‘ç­‰é¢†åŸŸï¼Œå°¤å…¶é€‚ç”¨äºéœ€è¦åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„åœºæ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½æ‰‹æœºã€åµŒå…¥å¼è®¾å¤‡ã€ç‰©è”ç½‘è®¾å¤‡ç­‰ã€‚é€šè¿‡è¯¥æ–¹æ³•ï¼Œå¯ä»¥åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šå®ç°æ›´å¼ºå¤§çš„AIèƒ½åŠ›ï¼Œæå‡ç”¨æˆ·ä½“éªŒï¼Œå¹¶é™ä½äº‘è®¡ç®—æˆæœ¬ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The Mixture of Experts (MoE) architecture is an important method for scaling Large Language Models (LLMs). It increases model capacity while keeping computation cost low. However, the ultra-large MoE models still have hundreds of billions of parameters, requiring massive memory/storage and leading to difficulties for deployment on resource-constrained edge platforms. Pruning or quantization alone can hardly address the issue, because of the super-aggressive compression ratio with significantly degraded accuracy and output quality. To facilitate the deployment of ultra-large MoEs on edge platforms, we propose a collaborative compression framework by combining expert pruning, mixed-precision quantization, and activation optimization. It can effectively reduce the storage footprint of the ultra-large MoE DeepSeek-V3 from 1.3TB to 103GB, while preserving high output quality with better accuracy than traditional uniform low-bit quantization methods. To the best of our knowledge, we are the first to deploy a compressed model from the ultra-large DeepSeek-V3 on the platform with a strict 128GB total memory limit. Our comprehensive experiments on multiple benchmarks under various memory constraints demonstrate the effectiveness of our method with smaller model sizes and higher accuracy than uniform low-bit quantization methods.

