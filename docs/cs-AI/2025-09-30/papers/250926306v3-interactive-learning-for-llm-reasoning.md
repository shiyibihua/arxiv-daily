---
layout: default
title: Interactive Learning for LLM Reasoning
---

# Interactive Learning for LLM Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.26306" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.26306v3</a>
  <a href="https://arxiv.org/pdf/2509.26306.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.26306v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.26306v3', 'Interactive Learning for LLM Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hehai Lin, Shilei Cao, Sudong Wang, Haotian Wu, Minzhi Li, Linyi Yang, Juepeng Zheng, Chengwei Qin

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30 (æ›´æ–°: 2025-10-02)

**å¤‡æ³¨**: The code is available at https://github.com/linhh29/Interactive-Learning-for-LLM-Reasoning

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºILRæ¡†æ¶ï¼Œé€šè¿‡äº¤äº’å¼å­¦ä¹ æå‡LLMç‹¬ç«‹æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å¤šæ™ºèƒ½ä½“å­¦ä¹ ` `äº¤äº’å¼å­¦ä¹ ` `æ¨ç†èƒ½åŠ›` `åŠ¨æ€äº¤äº’`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨æ¨ç†æ—¶éœ€é‡æ–°æ‰§è¡Œå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œä¸äººç±»ç‹¬ç«‹æ¨ç†çš„è®¤çŸ¥ä¸ç¬¦ï¼Œé™åˆ¶äº†LLMèƒ½åŠ›çš„è¿ç§»ã€‚
2. ILRæ¡†æ¶é€šè¿‡åŠ¨æ€äº¤äº’å’Œæ„ŸçŸ¥æ ¡å‡†ï¼Œä½¿LLMåœ¨äº¤äº’å¼å­¦ä¹ åï¼Œèƒ½å¤Ÿç‹¬ç«‹è§£å†³é—®é¢˜ï¼Œæå‡æ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒILRåœ¨æ•°å­¦å’Œç¼–ç åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºå•æ™ºèƒ½ä½“å­¦ä¹ ï¼Œæœ€é«˜æå‡5%ï¼Œå¹¶å¢å¼ºäº†LLMçš„é²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°æœ‰çš„å¤šæ™ºèƒ½ä½“å­¦ä¹ æ–¹æ³•é€šè¿‡äº¤äº’å¼è®­ç»ƒç¯å¢ƒæ¥ä¿ƒè¿›å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¹‹é—´çš„åä½œï¼Œä»è€Œæ„å»ºæ›´å¼ºå¤§çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰ã€‚ç„¶è€Œï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œå®ƒä»¬éœ€è¦é‡æ–°æ‰§è¡ŒMASæ‰èƒ½è·å¾—æœ€ç»ˆè§£å†³æ–¹æ¡ˆï¼Œè¿™ä¸äººç±»é€šè¿‡ä¸ä»–äººäº’åŠ¨æ¥å¢å¼ºæ¨ç†èƒ½åŠ›å¹¶åœ¨æœªæ¥ç‹¬ç«‹è§£å†³é—®é¢˜çš„è®¤çŸ¥ä¸åŒã€‚ä¸ºäº†ç ”ç©¶å¤šæ™ºèƒ½ä½“äº¤äº’æ˜¯å¦å¯ä»¥å¢å¼ºLLMçš„ç‹¬ç«‹é—®é¢˜è§£å†³èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†ILRï¼Œä¸€ç§ç”¨äºMASçš„æ–°å‹ååŒå­¦ä¹ æ¡†æ¶ï¼Œå®ƒé›†æˆäº†ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šåŠ¨æ€äº¤äº’å’Œæ„ŸçŸ¥æ ¡å‡†ã€‚å…·ä½“æ¥è¯´ï¼ŒåŠ¨æ€äº¤äº’é¦–å…ˆæ ¹æ®é—®é¢˜çš„éš¾åº¦å’Œæ¨¡å‹çš„èƒ½åŠ›è‡ªé€‚åº”åœ°é€‰æ‹©åˆä½œæˆ–ç«äº‰ç­–ç•¥ã€‚ç„¶åï¼ŒLLMé€šè¿‡Idea3ï¼ˆæ€æƒ³å…±äº«ã€æ€æƒ³åˆ†æå’Œæ€æƒ³èåˆï¼‰äº¤æ¢ä¿¡æ¯ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨æ¨¡ä»¿äººç±»è®¨è®ºçš„åˆ›æ–°äº¤äº’èŒƒå¼ï¼Œç„¶ååœ¨å¾—å‡ºå„è‡ªçš„æœ€ç»ˆç­”æ¡ˆã€‚åœ¨æ„ŸçŸ¥æ ¡å‡†ä¸­ï¼ŒILRé‡‡ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æ¥è®­ç»ƒLLMï¼ŒåŒæ—¶å°†ä¸€ä¸ªLLMçš„å¥–åŠ±åˆ†å¸ƒç‰¹å¾æ•´åˆåˆ°å¦ä¸€ä¸ªLLMçš„å¥–åŠ±å‡½æ•°ä¸­ï¼Œä»è€Œå¢å¼ºå¤šæ™ºèƒ½ä½“äº¤äº’çš„å‡èšåŠ›ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªä¸åŒè§„æ¨¡çš„æ¨¡å‹ç³»åˆ—çš„ä¸‰ä¸ªLLMä¸ŠéªŒè¯äº†ILRï¼Œè¯„ä¼°äº†äº”ä¸ªæ•°å­¦åŸºå‡†å’Œä¸€ä¸ªç¼–ç åŸºå‡†çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒILRå§‹ç»ˆä¼˜äºå•æ™ºèƒ½ä½“å­¦ä¹ ï¼Œä¸æœ€å¼ºçš„åŸºçº¿ç›¸æ¯”ï¼Œæ€§èƒ½æå‡é«˜è¾¾5%ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å‘ç°ï¼ŒIdea3å¯ä»¥å¢å¼ºæ›´å¼ºå¤§çš„LLMåœ¨å¤šæ™ºèƒ½ä½“æ¨ç†è¿‡ç¨‹ä¸­çš„é²æ£’æ€§ï¼Œå¹¶ä¸”ä¸çº¯ç²¹çš„åˆä½œæˆ–ç«äº‰ç­–ç•¥ç›¸æ¯”ï¼ŒåŠ¨æ€äº¤äº’ç±»å‹å¯ä»¥ä¿ƒè¿›å¤šæ™ºèƒ½ä½“å­¦ä¹ ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯å¤šæ™ºèƒ½ä½“å­¦ä¹ æ–¹æ³•ï¼Œåœ¨æå‡LLMæ¨ç†èƒ½åŠ›æ—¶ï¼Œä¾èµ–äºæ¨ç†é˜¶æ®µçš„å¤šæ™ºèƒ½ä½“åä½œã€‚è¿™æ„å‘³ç€åœ¨å®é™…åº”ç”¨ä¸­ï¼Œæ¯æ¬¡éœ€è¦è§£å†³é—®é¢˜æ—¶ï¼Œéƒ½éœ€è¦é‡æ–°æ„å»ºå’Œæ‰§è¡Œæ•´ä¸ªå¤šæ™ºèƒ½ä½“ç³»ç»Ÿã€‚è¿™ä¸äººç±»çš„å­¦ä¹ æ–¹å¼ä¸åŒï¼Œäººç±»å¯ä»¥é€šè¿‡ä¸ä»–äººäº¤æµå­¦ä¹ ï¼Œæœ€ç»ˆç‹¬ç«‹è§£å†³é—®é¢˜ã€‚å› æ­¤ï¼Œå¦‚ä½•è®©LLMé€šè¿‡äº¤äº’å¼å­¦ä¹ ï¼Œæå‡å…¶ç‹¬ç«‹æ¨ç†èƒ½åŠ›ï¼Œæ˜¯æœ¬æ–‡è¦è§£å†³çš„æ ¸å¿ƒé—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šILRçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ¨¡æ‹Ÿäººç±»çš„è®¨è®ºå’Œå­¦ä¹ è¿‡ç¨‹ï¼Œè®©LLMåœ¨äº¤äº’å¼ç¯å¢ƒä¸­å­¦ä¹ ï¼Œå¹¶æœ€ç»ˆå…·å¤‡ç‹¬ç«‹è§£å†³é—®é¢˜çš„èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼ŒILRåŒ…å«åŠ¨æ€äº¤äº’å’Œæ„ŸçŸ¥æ ¡å‡†ä¸¤ä¸ªå…³é”®ç»„ä»¶ã€‚åŠ¨æ€äº¤äº’å…è®¸LLMæ ¹æ®é—®é¢˜éš¾åº¦å’Œè‡ªèº«èƒ½åŠ›é€‰æ‹©åˆä½œæˆ–ç«äº‰ç­–ç•¥ï¼Œè€Œæ„ŸçŸ¥æ ¡å‡†åˆ™é€šè¿‡ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼Œå¢å¼ºå¤šæ™ºèƒ½ä½“ä¹‹é—´çš„åä½œå‡èšåŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šILRæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) **åŠ¨æ€äº¤äº’é€‰æ‹©**ï¼šæ ¹æ®é—®é¢˜éš¾åº¦å’Œæ¨¡å‹èƒ½åŠ›ï¼Œè‡ªé€‚åº”é€‰æ‹©åˆä½œæˆ–ç«äº‰ç­–ç•¥ã€‚2) **Idea3äº¤äº’**ï¼šLLMä¹‹é—´é€šè¿‡Idea3ï¼ˆæ€æƒ³å…±äº«ã€æ€æƒ³åˆ†æå’Œæ€æƒ³èåˆï¼‰è¿›è¡Œä¿¡æ¯äº¤æ¢ï¼Œæ¨¡æ‹Ÿäººç±»è®¨è®ºè¿‡ç¨‹ã€‚3) **ç‹¬ç«‹æ¨ç†**ï¼šLLMåŸºäºäº¤äº’ä¿¡æ¯ï¼Œç‹¬ç«‹å¾—å‡ºæœ€ç»ˆç­”æ¡ˆã€‚4) **æ„ŸçŸ¥æ ¡å‡†**ï¼šä½¿ç”¨Group Relative Policy Optimization (GRPO) è®­ç»ƒLLMï¼Œå°†ä¸€ä¸ªLLMçš„å¥–åŠ±åˆ†å¸ƒç‰¹å¾æ•´åˆåˆ°å¦ä¸€ä¸ªLLMçš„å¥–åŠ±å‡½æ•°ä¸­ã€‚

**å…³é”®åˆ›æ–°**ï¼šILRçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) **åŠ¨æ€äº¤äº’**ï¼šæ ¹æ®é—®é¢˜éš¾åº¦å’Œæ¨¡å‹èƒ½åŠ›è‡ªé€‚åº”é€‰æ‹©äº¤äº’ç­–ç•¥ï¼Œæ›´çµæ´»æœ‰æ•ˆã€‚2) **Idea3äº¤äº’èŒƒå¼**ï¼šæ¨¡æ‹Ÿäººç±»è®¨è®ºè¿‡ç¨‹ï¼Œä¿ƒè¿›LLMä¹‹é—´çš„ä¿¡æ¯äº¤æ¢å’ŒçŸ¥è¯†èåˆã€‚3) **æ„ŸçŸ¥æ ¡å‡†**ï¼šé€šè¿‡GRPOå¢å¼ºå¤šæ™ºèƒ½ä½“äº¤äº’çš„å‡èšåŠ›ï¼Œæå‡å­¦ä¹ æ•ˆæœã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒILRæ›´æ³¨é‡æå‡LLMçš„ç‹¬ç«‹æ¨ç†èƒ½åŠ›ï¼Œè€Œéä»…ä»…ä¾èµ–äºæ¨ç†é˜¶æ®µçš„å¤šæ™ºèƒ½ä½“åä½œã€‚

**å…³é”®è®¾è®¡**ï¼šIdea3äº¤äº’èŒƒå¼åŒ…å«ä¸‰ä¸ªé˜¶æ®µï¼šæ€æƒ³å…±äº«ï¼ˆIdea Sharingï¼‰ã€æ€æƒ³åˆ†æï¼ˆIdea Analysisï¼‰å’Œæ€æƒ³èåˆï¼ˆIdea Fusionï¼‰ã€‚åœ¨æ€æƒ³å…±äº«é˜¶æ®µï¼ŒLLMåˆ†äº«å„è‡ªçš„åˆæ­¥æƒ³æ³•ã€‚åœ¨æ€æƒ³åˆ†æé˜¶æ®µï¼ŒLLMåˆ†æå½¼æ­¤çš„æƒ³æ³•ï¼Œæ‰¾å‡ºä¼˜ç‚¹å’Œä¸è¶³ã€‚åœ¨æ€æƒ³èåˆé˜¶æ®µï¼ŒLLMå°†å½¼æ­¤çš„æƒ³æ³•èåˆï¼Œå½¢æˆæ›´å®Œå–„çš„è§£å†³æ–¹æ¡ˆã€‚GRPOé€šè¿‡è°ƒæ•´å¥–åŠ±å‡½æ•°ï¼Œä½¿å¾—LLMåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ›´åŠ å…³æ³¨ç¾¤ä½“è¡¨ç°ï¼Œä»è€Œå¢å¼ºå¤šæ™ºèƒ½ä½“ä¹‹é—´çš„åä½œã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒILRåœ¨äº”ä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•å’Œä¸€ä¸ªç¼–ç åŸºå‡†æµ‹è¯•ä¸­ï¼Œå‡ä¼˜äºå•æ™ºèƒ½ä½“å­¦ä¹ æ–¹æ³•ï¼Œæœ€é«˜æå‡è¾¾5%ã€‚æ­¤å¤–ï¼Œå®éªŒè¿˜å‘ç°ï¼ŒIdea3äº¤äº’èŒƒå¼å¯ä»¥å¢å¼ºæ›´å¼ºå¤§çš„LLMåœ¨å¤šæ™ºèƒ½ä½“æ¨ç†è¿‡ç¨‹ä¸­çš„é²æ£’æ€§ï¼Œå¹¶ä¸”åŠ¨æ€äº¤äº’ç±»å‹å¯ä»¥ä¿ƒè¿›å¤šæ™ºèƒ½ä½“å­¦ä¹ ï¼Œæ•ˆæœä¼˜äºçº¯ç²¹çš„åˆä½œæˆ–ç«äº‰ç­–ç•¥ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

ILRæ¡†æ¶å¯åº”ç”¨äºå„ç§éœ€è¦LLMè¿›è¡Œå¤æ‚æ¨ç†å’Œå†³ç­–çš„åœºæ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½å®¢æœã€é‡‘èåˆ†æã€åŒ»ç–—è¯Šæ–­ç­‰ã€‚é€šè¿‡äº¤äº’å¼å­¦ä¹ ï¼ŒLLMå¯ä»¥æ›´å¥½åœ°ç†è§£é—®é¢˜ï¼Œå¹¶ç»™å‡ºæ›´å‡†ç¡®ã€æ›´å¯é çš„ç­”æ¡ˆã€‚æ­¤å¤–ï¼ŒILRè¿˜å¯ä»¥ç”¨äºæå‡LLMåœ¨ç‰¹å®šé¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†å’ŒæŠ€èƒ½ï¼Œä½¿å…¶æ›´å¥½åœ°æœåŠ¡äºå„è¡Œå„ä¸šã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Existing multi-agent learning approaches have developed interactive training environments to explicitly promote collaboration among multiple Large Language Models (LLMs), thereby constructing stronger multi-agent systems (MAS). However, during inference, they require re-executing the MAS to obtain final solutions, which diverges from human cognition that individuals can enhance their reasoning capabilities through interactions with others and resolve questions independently in the future. To investigate whether multi-agent interaction can enhance LLMs' independent problem-solving ability, we introduce ILR, a novel co-learning framework for MAS that integrates two key components: Dynamic Interaction and Perception Calibration. Specifically, Dynamic Interaction first adaptively selects either cooperative or competitive strategies depending on question difficulty and model ability. LLMs then exchange information through Idea3 (Idea Sharing, Idea Analysis, and Idea Fusion), an innovative interaction paradigm designed to mimic human discussion, before deriving their respective final answers. In Perception Calibration, ILR employs Group Relative Policy Optimization (GRPO) to train LLMs while integrating one LLM's reward distribution characteristics into another's reward function, thereby enhancing the cohesion of multi-agent interactions. We validate ILR on three LLMs across two model families of varying scales, evaluating performance on five mathematical benchmarks and one coding benchmark. Experimental results show that ILR consistently outperforms single-agent learning, yielding an improvement of up to 5% over the strongest baseline. We further discover that Idea3 can enhance the robustness of stronger LLMs during multi-agent inference, and dynamic interaction types can boost multi-agent learning compared to pure cooperative or competitive strategies.

