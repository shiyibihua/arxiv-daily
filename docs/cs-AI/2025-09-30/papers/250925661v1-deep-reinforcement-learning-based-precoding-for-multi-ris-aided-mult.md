---
layout: default
title: Deep Reinforcement Learning-Based Precoding for Multi-RIS-Aided Multiuser Downlink Systems with Practical Phase Shift
---

# Deep Reinforcement Learning-Based Precoding for Multi-RIS-Aided Multiuser Downlink Systems with Practical Phase Shift

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25661" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25661v1</a>
  <a href="https://arxiv.org/pdf/2509.25661.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25661v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25661v1', 'Deep Reinforcement Learning-Based Precoding for Multi-RIS-Aided Multiuser Downlink Systems with Practical Phase Shift')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Po-Heng Chou, Bo-Ren Zheng, Wan-Jen Huang, Walid Saad, Yu Tsao, Ronald Y. Chang

**åˆ†ç±»**: cs.IT, cs.AI, cs.LG, cs.NI, eess.SP

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

**å¤‡æ³¨**: 5 pages, 5 figures, and published in IEEE Wireless Communications Letters

**æœŸåˆŠ**: IEEE Wireless Communications Letters, vol. 14, no. 1, pp. 1-5, Jan. 2025

**DOI**: [10.1109/LWC.2024.3482720](https://doi.org/10.1109/LWC.2024.3482720)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é’ˆå¯¹å¤šRISè¾…åŠ©å¤šç”¨æˆ·ä¸‹è¡Œé“¾è·¯ï¼Œæå‡ºåŸºäºDDPGçš„é¢„ç¼–ç æ–¹æ¡ˆï¼Œä¼˜åŒ–é¢‘è°±æ•ˆç‡ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `RISè¾…åŠ©é€šä¿¡` `æ·±åº¦å¼ºåŒ–å­¦ä¹ ` `DDPG` `é¢„ç¼–ç ` `ç›¸ä½åç§»` `é¢‘è°±æ•ˆç‡` `æ¯«ç±³æ³¢é€šä¿¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰RISè¾…åŠ©é€šä¿¡ç ”ç©¶é€šå¸¸å‡è®¾ç†æƒ³åå°„ï¼Œå¿½ç•¥äº†å®é™…RISå…ƒä»¶ä¸­åå°„å¹…åº¦å’Œç›¸ä½åç§»çš„è€¦åˆæ•ˆåº”ï¼Œå¯¼è‡´ä¼˜åŒ–é—®é¢˜å¤æ‚åŒ–ã€‚
2. æœ¬æ–‡æå‡ºåŸºäºDDPGçš„æ·±åº¦å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œè”åˆä¼˜åŒ–å‘å°„æœºé¢„ç¼–ç å’ŒRISç›¸ä½åç§»ï¼Œä»¥åº”å¯¹éå‡¸ä¼˜åŒ–é—®é¢˜å¹¶æœ€å¤§åŒ–é¢‘è°±æ•ˆç‡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å®é™…æ¯«ç±³æ³¢ä¿¡é“å’Œéšæœºç”¨æˆ·åˆ†å¸ƒåœºæ™¯ä¸‹ï¼Œæ‰€æDDPGæ–¹æ³•æ˜¾è‘—ä¼˜äºä¼ ç»Ÿä¼˜åŒ–ç®—æ³•å’ŒåŒé‡æ·±åº¦Qå­¦ä¹ æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶è€ƒè™‘äº†å¤šé‡å¯é‡æ„æ™ºèƒ½è¡¨é¢ï¼ˆRISï¼‰è¾…åŠ©çš„å¤šç”¨æˆ·ä¸‹è¡Œé“¾è·¯ç³»ç»Ÿï¼Œç›®æ ‡æ˜¯è”åˆä¼˜åŒ–å‘å°„æœºé¢„ç¼–ç å’ŒRISç›¸ä½åç§»çŸ©é˜µï¼Œä»¥æœ€å¤§åŒ–é¢‘è°±æ•ˆç‡ã€‚ä¸å‡è®¾ç†æƒ³RISåå°„ç‡çš„å…ˆå‰å·¥ä½œä¸åŒï¼Œæœ¬æ–‡è€ƒè™‘äº†RISå…ƒä»¶åå°„å¹…åº¦å’Œç›¸ä½åç§»ä¹‹é—´çš„å®é™…è€¦åˆæ•ˆåº”ï¼Œè¿™ä½¿å¾—ä¼˜åŒ–é—®é¢˜å˜ä¸ºéå‡¸ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªæŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦ç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦ï¼ˆDDPGï¼‰çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰æ¡†æ¶ã€‚æ‰€æå‡ºçš„æ¨¡å‹åœ¨å®é™…æ¯«ç±³æ³¢ä¿¡é“è®¾ç½®ä¸‹ï¼Œé’ˆå¯¹å›ºå®šå’Œéšæœºæ•°é‡çš„ç”¨æˆ·è¿›è¡Œäº†è¯„ä¼°ã€‚ä»¿çœŸç»“æœè¡¨æ˜ï¼Œå°½ç®¡å…¶å¤æ‚æ€§ï¼Œæ‰€æå‡ºçš„DDPGæ–¹æ³•æ˜æ˜¾ä¼˜äºåŸºäºä¼˜åŒ–çš„ç®—æ³•å’ŒåŒé‡æ·±åº¦Qå­¦ä¹ ï¼Œç‰¹åˆ«æ˜¯åœ¨å…·æœ‰éšæœºç”¨æˆ·åˆ†å¸ƒçš„åœºæ™¯ä¸­ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤šRISè¾…åŠ©å¤šç”¨æˆ·ä¸‹è¡Œé“¾è·¯ç³»ç»Ÿä¸­ï¼Œåœ¨è€ƒè™‘å®é™…RISå…ƒä»¶çš„å¹…ç›¸è€¦åˆæ•ˆåº”ä¸‹ï¼Œå¦‚ä½•è”åˆä¼˜åŒ–å‘å°„æœºé¢„ç¼–ç å’ŒRISç›¸ä½åç§»çŸ©é˜µï¼Œä»¥æœ€å¤§åŒ–é¢‘è°±æ•ˆç‡çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸å‡è®¾ç†æƒ³çš„RISåå°„ï¼Œå¿½ç•¥äº†å®é™…çš„å¹…ç›¸è€¦åˆï¼Œå¯¼è‡´ä¼˜åŒ–ç»“æœä¸å®é™…æ€§èƒ½å­˜åœ¨å·®è·ï¼Œå¹¶ä¸”ä¼˜åŒ–é—®é¢˜æœ¬èº«æ˜¯éå‡¸çš„ï¼Œéš¾ä»¥æ±‚è§£ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯æ·±åº¦ç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦ï¼ˆDDPGï¼‰ç®—æ³•ï¼Œç›´æ¥å­¦ä¹ æœ€ä¼˜çš„é¢„ç¼–ç å’Œç›¸ä½åç§»ç­–ç•¥ã€‚DDPGèƒ½å¤Ÿå¤„ç†è¿ç»­åŠ¨ä½œç©ºé—´ï¼Œé€‚åˆä¼˜åŒ–é¢„ç¼–ç å’Œç›¸ä½åç§»è¿™äº›è¿ç»­å˜é‡ã€‚é€šè¿‡ä¸ç¯å¢ƒçš„äº¤äº’ï¼Œæ™ºèƒ½ä½“èƒ½å¤Ÿå­¦ä¹ åˆ°åœ¨å®é™…å¹…ç›¸è€¦åˆæ•ˆåº”ä¸‹çš„æœ€ä¼˜ç­–ç•¥ï¼Œä»è€Œæœ€å¤§åŒ–é¢‘è°±æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦éƒ¨åˆ†ï¼šç”¨æˆ·ã€åŸºç«™ï¼ˆBSï¼‰å’Œå¤šä¸ªRISã€‚åŸºç«™å‘å¤šä¸ªç”¨æˆ·å‘é€æ•°æ®ï¼ŒRISé€šè¿‡è°ƒæ•´ç›¸ä½åç§»æ¥å¢å¼ºä¿¡å·è´¨é‡ã€‚DDPGæ™ºèƒ½ä½“ä½äºåŸºç«™ï¼Œæ¥æ”¶ç¯å¢ƒçŠ¶æ€ï¼ˆä¾‹å¦‚ä¿¡é“çŠ¶æ€ä¿¡æ¯ã€ç”¨æˆ·ä½ç½®ç­‰ï¼‰ï¼Œè¾“å‡ºé¢„ç¼–ç çŸ©é˜µå’ŒRISç›¸ä½åç§»å‘é‡ã€‚ç¯å¢ƒæ ¹æ®æ™ºèƒ½ä½“çš„åŠ¨ä½œæ›´æ–°çŠ¶æ€ï¼Œå¹¶è¿”å›å¥–åŠ±ï¼ˆé¢‘è°±æ•ˆç‡ï¼‰ã€‚DDPGæ™ºèƒ½ä½“é€šè¿‡ä¸æ–­ä¸ç¯å¢ƒäº¤äº’ï¼Œå­¦ä¹ æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±çš„ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†DDPGåº”ç”¨äºè§£å†³è€ƒè™‘å®é™…RISå¹…ç›¸è€¦åˆæ•ˆåº”çš„é¢„ç¼–ç å’Œç›¸ä½åç§»è”åˆä¼˜åŒ–é—®é¢˜ã€‚ä¸ä¼ ç»Ÿçš„ä¼˜åŒ–ç®—æ³•ç›¸æ¯”ï¼ŒDDPGä¸éœ€è¦å¯¹é—®é¢˜è¿›è¡Œç®€åŒ–æˆ–å‡¸ä¼˜åŒ–ï¼Œå¯ä»¥ç›´æ¥å¤„ç†éå‡¸é—®é¢˜ã€‚æ­¤å¤–ï¼ŒDDPGèƒ½å¤Ÿé€‚åº”åŠ¨æ€å˜åŒ–çš„ç¯å¢ƒï¼Œä¾‹å¦‚ç”¨æˆ·ä½ç½®çš„å˜åŒ–ã€‚

**å…³é”®è®¾è®¡**ï¼šDDPGæ™ºèƒ½ä½“åŒ…æ‹¬Actorç½‘ç»œå’ŒCriticç½‘ç»œã€‚Actorç½‘ç»œç”¨äºç”Ÿæˆé¢„ç¼–ç çŸ©é˜µå’ŒRISç›¸ä½åç§»å‘é‡ï¼ŒCriticç½‘ç»œç”¨äºè¯„ä¼°Actorç½‘ç»œçš„åŠ¨ä½œã€‚å¥–åŠ±å‡½æ•°è¢«è®¾è®¡ä¸ºé¢‘è°±æ•ˆç‡ã€‚çŠ¶æ€ç©ºé—´åŒ…æ‹¬ä¿¡é“çŠ¶æ€ä¿¡æ¯ã€ç”¨æˆ·ä½ç½®ç­‰ã€‚åŠ¨ä½œç©ºé—´åŒ…æ‹¬é¢„ç¼–ç çŸ©é˜µå’ŒRISç›¸ä½åç§»å‘é‡ã€‚ä¸ºäº†æé«˜è®­ç»ƒæ•ˆç‡ï¼Œä½¿ç”¨äº†ç»éªŒå›æ”¾å’Œç›®æ ‡ç½‘ç»œç­‰æŠ€æœ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

ä»¿çœŸç»“æœè¡¨æ˜ï¼Œåœ¨å®é™…æ¯«ç±³æ³¢ä¿¡é“ç¯å¢ƒä¸‹ï¼Œæ‰€æå‡ºçš„DDPGæ–¹æ³•åœ¨é¢‘è°±æ•ˆç‡æ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„ä¼˜åŒ–ç®—æ³•å’ŒåŒé‡æ·±åº¦Qå­¦ä¹ ï¼ˆDouble DQNï¼‰æ–¹æ³•ã€‚ç‰¹åˆ«æ˜¯åœ¨éšæœºç”¨æˆ·åˆ†å¸ƒçš„åœºæ™¯ä¸‹ï¼ŒDDPGçš„æ€§èƒ½æå‡æ›´ä¸ºæ˜æ˜¾ï¼ŒéªŒè¯äº†å…¶åœ¨å¤æ‚ç¯å¢ƒä¸‹çš„é€‚åº”æ€§å’Œä¼˜è¶Šæ€§ã€‚å…·ä½“æ€§èƒ½æå‡å¹…åº¦æœªçŸ¥ï¼ŒåŸæ–‡æœªç»™å‡ºæ˜ç¡®æ•°æ®ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœªæ¥çš„æ— çº¿é€šä¿¡ç³»ç»Ÿï¼Œå°¤å…¶æ˜¯åœ¨æ¯«ç±³æ³¢é€šä¿¡å’Œå¤§è§„æ¨¡MIMOç³»ç»Ÿä¸­ï¼Œé€šè¿‡éƒ¨ç½²RISæ¥å¢å¼ºä¿¡å·è¦†ç›–å’Œæé«˜é¢‘è°±æ•ˆç‡ã€‚å®é™…åº”ç”¨åŒ…æ‹¬å®¤å†…è¦†ç›–å¢å¼ºã€çƒ­ç‚¹åŒºåŸŸå®¹é‡æå‡ä»¥åŠåè¿œåœ°åŒºé€šä¿¡ä¿éšœç­‰ã€‚è¯¥æŠ€æœ¯æœ‰åŠ©äºå®ç°æ›´é«˜æ•ˆã€æ›´æ™ºèƒ½çš„æ— çº¿é€šä¿¡ç½‘ç»œã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This study considers multiple reconfigurable intelligent surfaces (RISs)-aided multiuser downlink systems with the goal of jointly optimizing the transmitter precoding and RIS phase shift matrix to maximize spectrum efficiency. Unlike prior work that assumed ideal RIS reflectivity, a practical coupling effect is considered between reflecting amplitude and phase shift for the RIS elements. This makes the optimization problem non-convex. To address this challenge, we propose a deep deterministic policy gradient (DDPG)-based deep reinforcement learning (DRL) framework. The proposed model is evaluated under both fixed and random numbers of users in practical mmWave channel settings. Simulation results demonstrate that, despite its complexity, the proposed DDPG approach significantly outperforms optimization-based algorithms and double deep Q-learning, particularly in scenarios with random user distributions.

