---
layout: default
title: OWL: Geometry-Aware Spatial Reasoning for Audio Large Language Models
---

# OWL: Geometry-Aware Spatial Reasoning for Audio Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.26140" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.26140v1</a>
  <a href="https://arxiv.org/pdf/2509.26140.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.26140v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.26140v1', 'OWL: Geometry-Aware Spatial Reasoning for Audio Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Subrata Biswas, Mohammad Nur Hossain Khan, Bashima Islam

**åˆ†ç±»**: cs.SD, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOWLæ¨¡å‹ï¼Œé€šè¿‡å‡ ä½•æ„ŸçŸ¥ç©ºé—´æ¨ç†æå‡éŸ³é¢‘å¤§è¯­è¨€æ¨¡å‹å¯¹å£°éŸ³æ–¹ä½å’Œè·ç¦»çš„æ„ŸçŸ¥ç²¾åº¦ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `éŸ³é¢‘å¤§è¯­è¨€æ¨¡å‹` `ç©ºé—´æ¨ç†` `å‡ ä½•æ„ŸçŸ¥` `åŒè€³éŸ³é¢‘` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰éŸ³é¢‘å¤§è¯­è¨€æ¨¡å‹ä¾èµ–éç»“æ„åŒ–åŒè€³çº¿ç´¢å’Œå•æ­¥æ¨ç†ï¼Œåœ¨æ–¹å‘å’Œè·ç¦»ä¼°è®¡ä¸Šå­˜åœ¨ç²¾åº¦å’Œå¯è§£é‡Šæ€§ä¸è¶³çš„é—®é¢˜ã€‚
2. æå‡ºç©ºé—´å£°å­¦å‡ ä½•ç¼–ç å™¨ï¼ˆSAGEï¼‰å°†åŒè€³å£°å­¦ç‰¹å¾ä¸3Dç©ºé—´ç»“æ„å¯¹é½ï¼Œå¹¶ç»“åˆç©ºé—´æ¥åœ°çš„æ€ç»´é“¾è¿›è¡Œæ¨ç†ã€‚
3. æ„å»ºBiDepthæ•°æ®é›†ç”¨äºå¤§è§„æ¨¡è®­ç»ƒå’Œè¯„ä¼°ï¼Œå®éªŒç»“æœè¡¨æ˜OWLæ¨¡å‹åœ¨DoAè¯¯å·®å’Œç©ºé—´æ¨ç†QAå‡†ç¡®ç‡ä¸Šå‡æœ‰æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç©ºé—´æ¨ç†æ˜¯å¬è§‰æ„ŸçŸ¥çš„åŸºæœ¬èƒ½åŠ›ï¼Œä½†ç›®å‰çš„éŸ³é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼ˆALLMsï¼‰ä¸»è¦ä¾èµ–äºéç»“æ„åŒ–çš„åŒè€³çº¿ç´¢å’Œå•æ­¥æ¨ç†ï¼Œè¿™é™åˆ¶äº†æ–¹å‘å’Œè·ç¦»ä¼°è®¡çš„æ„ŸçŸ¥ç²¾åº¦å’Œå¯è§£é‡Šçš„æ¨ç†èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºäº†ç©ºé—´å£°å­¦å‡ ä½•ç¼–ç å™¨ï¼ˆSAGEï¼‰ï¼Œå®ƒæ˜¯ä¸€ç§å‡ ä½•æ„ŸçŸ¥çš„éŸ³é¢‘ç¼–ç å™¨ï¼Œåœ¨è®­ç»ƒæ—¶ä½¿ç”¨å…¨æ™¯æ·±åº¦å›¾åƒå’Œæˆ¿é—´è„‰å†²å“åº”å°†åŒè€³å£°å­¦ç‰¹å¾ä¸3Dç©ºé—´ç»“æ„å¯¹é½ï¼Œè€Œåœ¨æ¨ç†æ—¶åªéœ€è¦éŸ³é¢‘ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæå‡ºäº†OWLï¼Œä¸€ä¸ªALLMï¼Œå®ƒå°†SAGEä¸ç©ºé—´æ¥åœ°çš„æ€ç»´é“¾ç›¸ç»“åˆï¼Œä»¥åˆç†åŒ–åˆ°è¾¾æ–¹å‘ï¼ˆDoAï¼‰å’Œè·ç¦»ä¼°è®¡ã€‚é€šè¿‡ä»æ„ŸçŸ¥QAåˆ°å¤šæ­¥æ¨ç†çš„è¯¾ç¨‹å­¦ä¹ ï¼ŒOWLæ”¯æŒæ—¶é’Ÿçº§åˆ«çš„æ–¹ä½è§’å’ŒDoAä¼°è®¡ã€‚ä¸ºäº†å®ç°å¤§è§„æ¨¡çš„è®­ç»ƒå’Œè¯„ä¼°ï¼Œæ„å»ºå¹¶å‘å¸ƒäº†BiDepthæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«è¶…è¿‡ä¸€ç™¾ä¸‡ä¸ªQAå¯¹ï¼Œç»“åˆäº†åŒè€³éŸ³é¢‘ä¸å…¨æ™¯æ·±åº¦å›¾åƒå’Œå®¤å†…å¤–åœºæ™¯çš„æˆ¿é—´è„‰å†²å“åº”ã€‚åœ¨BiDepthå’Œå…¬å¼€çš„SpatialSoundQAä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šï¼ŒOWLé€šè¿‡SAGEå°†å¹³å‡DoAè¯¯å·®é™ä½äº†11åº¦ï¼Œå¹¶å°†ç©ºé—´æ¨ç†QAçš„å‡†ç¡®ç‡æé«˜äº†25%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰éŸ³é¢‘å¤§è¯­è¨€æ¨¡å‹åœ¨å¤„ç†ç©ºé—´éŸ³é¢‘æ—¶ï¼Œä¸»è¦ä¾èµ–éç»“æ„åŒ–çš„åŒè€³çº¿ç´¢ï¼Œç¼ºä¹å¯¹åœºæ™¯å‡ ä½•ä¿¡æ¯çš„æœ‰æ•ˆåˆ©ç”¨ï¼Œå¯¼è‡´åœ¨å£°éŸ³æ–¹ä½å’Œè·ç¦»ä¼°è®¡æ–¹é¢ç²¾åº¦è¾ƒä½ï¼Œå¹¶ä¸”æ¨ç†è¿‡ç¨‹ç¼ºä¹å¯è§£é‡Šæ€§ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚BATï¼Œè™½ç„¶ä½¿ç”¨äº†ç©ºé—´QAï¼Œä½†ä¾èµ–äºç²—ç³™çš„ç±»åˆ«æ ‡ç­¾ï¼Œç¼ºä¹æ˜¾å¼çš„å‡ ä½•ç›‘ç£ï¼Œé™åˆ¶äº†åˆ†è¾¨ç‡å’Œé²æ£’æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†éŸ³é¢‘ç‰¹å¾ä¸3Dç©ºé—´å‡ ä½•ä¿¡æ¯è¿›è¡Œå¯¹é½ï¼Œä»è€Œä½¿æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å£°éŸ³çš„ç©ºé—´å…³ç³»ã€‚é€šè¿‡å¼•å…¥å…¨æ™¯æ·±åº¦å›¾åƒå’Œæˆ¿é—´è„‰å†²å“åº”ä½œä¸ºè¾…åŠ©ä¿¡æ¯ï¼Œè®­ç»ƒä¸€ä¸ªå‡ ä½•æ„ŸçŸ¥çš„éŸ³é¢‘ç¼–ç å™¨ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ åˆ°å£°éŸ³åœ¨ç©ºé—´ä¸­çš„ä½ç½®å’Œä¼ æ’­ç‰¹æ€§ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œå³ä½¿æ²¡æœ‰æ·±åº¦å›¾åƒï¼Œæ¨¡å‹ä¹Ÿèƒ½åˆ©ç”¨å­¦åˆ°çš„ç©ºé—´çŸ¥è¯†è¿›è¡Œæ›´å‡†ç¡®çš„æ–¹ä½å’Œè·ç¦»ä¼°è®¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šOWLæ¨¡å‹çš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) **ç©ºé—´å£°å­¦å‡ ä½•ç¼–ç å™¨ (SAGE)**ï¼šå°†åŒè€³éŸ³é¢‘ç‰¹å¾ä¸3Dç©ºé—´ç»“æ„å¯¹é½ã€‚2) **ç©ºé—´æ¥åœ°çš„æ€ç»´é“¾**ï¼šç”¨äºè¿›è¡Œå¤šæ­¥æ¨ç†ï¼Œä»DoAå’Œè·ç¦»ä¼°è®¡ä¸­è¿›è¡Œåˆç†åŒ–æ¨å¯¼ã€‚3) **éŸ³é¢‘å¤§è¯­è¨€æ¨¡å‹ (ALLM)**ï¼šå°†SAGEçš„è¾“å‡ºä¸æ–‡æœ¬ä¿¡æ¯ç»“åˆï¼Œè¿›è¡Œç©ºé—´æ¨ç†QAã€‚è®­ç»ƒè¿‡ç¨‹é‡‡ç”¨è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œä»ç®€å•çš„æ„ŸçŸ¥QAä»»åŠ¡é€æ¸è¿‡æ¸¡åˆ°å¤æ‚çš„å¤šæ­¥æ¨ç†ä»»åŠ¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºSAGEï¼Œå®ƒæ˜¯ä¸€ç§å‡ ä½•æ„ŸçŸ¥çš„éŸ³é¢‘ç¼–ç å™¨ï¼Œèƒ½å¤Ÿå°†åŒè€³éŸ³é¢‘ç‰¹å¾ä¸3Dç©ºé—´ç»“æ„è¿›è¡Œå¯¹é½ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒSAGEåˆ©ç”¨å…¨æ™¯æ·±åº¦å›¾åƒå’Œæˆ¿é—´è„‰å†²å“åº”ä½œä¸ºå‡ ä½•ç›‘ç£ä¿¡å·ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ åˆ°æ›´ä¸°å¯Œçš„ç©ºé—´ä¿¡æ¯ã€‚æ­¤å¤–ï¼ŒOWLæ¨¡å‹è¿˜å¼•å…¥äº†ç©ºé—´æ¥åœ°çš„æ€ç»´é“¾ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè¿›è¡Œæ›´å¯è§£é‡Šçš„ç©ºé—´æ¨ç†ã€‚

**å…³é”®è®¾è®¡**ï¼šSAGEçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œæå–åŒè€³éŸ³é¢‘ç‰¹å¾ã€‚2) ä½¿ç”¨å…¨æ™¯æ·±åº¦å›¾åƒå’Œæˆ¿é—´è„‰å†²å“åº”ä½œä¸ºå‡ ä½•ç›‘ç£ä¿¡å·ï¼Œè®­ç»ƒæ¨¡å‹å­¦ä¹ ç©ºé—´ä¿¡æ¯ã€‚3) è®¾è®¡æŸå¤±å‡½æ•°ï¼Œé¼“åŠ±æ¨¡å‹å°†éŸ³é¢‘ç‰¹å¾ä¸å¯¹åº”çš„ç©ºé—´ä½ç½®è¿›è¡Œå¯¹é½ã€‚OWLçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨Transformeræ¶æ„æ„å»ºALLMã€‚2) å¼•å…¥ç©ºé—´æ¥åœ°çš„æ€ç»´é“¾ï¼Œè¿›è¡Œå¤šæ­¥æ¨ç†ã€‚3) é‡‡ç”¨è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œé€æ­¥æå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒOWLæ¨¡å‹åœ¨BiDepthå’ŒSpatialSoundQAä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡SAGEï¼ŒOWLæ¨¡å‹å°†å¹³å‡DoAè¯¯å·®é™ä½äº†11åº¦ï¼Œå¹¶å°†ç©ºé—´æ¨ç†QAçš„å‡†ç¡®ç‡æé«˜äº†25%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„å‡ ä½•æ„ŸçŸ¥ç©ºé—´æ¨ç†æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°æå‡éŸ³é¢‘å¤§è¯­è¨€æ¨¡å‹å¯¹å£°éŸ³æ–¹ä½å’Œè·ç¦»çš„æ„ŸçŸ¥ç²¾åº¦ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ™ºèƒ½å®¶å±…ã€æœºå™¨äººå¯¼èˆªã€è™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨æ™ºèƒ½å®¶å±…ä¸­ï¼Œæ¨¡å‹å¯ä»¥æ ¹æ®å£°éŸ³åˆ¤æ–­äº‹ä»¶å‘ç”Ÿçš„æ–¹ä½å’Œè·ç¦»ï¼Œä»è€Œå®ç°æ›´æ™ºèƒ½çš„æ§åˆ¶ã€‚åœ¨æœºå™¨äººå¯¼èˆªä¸­ï¼Œæ¨¡å‹å¯ä»¥å¸®åŠ©æœºå™¨äººç†è§£å‘¨å›´ç¯å¢ƒçš„å£°éŸ³ä¿¡æ¯ï¼Œä»è€Œæ›´å¥½åœ°è¿›è¡Œå®šä½å’Œé¿éšœã€‚åœ¨VR/ARä¸­ï¼Œå¯ä»¥æä¾›æ›´çœŸå®çš„ç©ºé—´éŸ³é¢‘ä½“éªŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Spatial reasoning is fundamental to auditory perception, yet current audio large language models (ALLMs) largely rely on unstructured binaural cues and single step inference. This limits both perceptual accuracy in direction and distance estimation and the capacity for interpretable reasoning. Recent work such as BAT demonstrates spatial QA with binaural audio, but its reliance on coarse categorical labels (left, right, up, down) and the absence of explicit geometric supervision constrain resolution and robustness. We introduce the $\textbf{Spatial-Acoustic Geometry Encoder (SAGE}$), a geometry-aware audio encoder that aligns binaural acoustic features with 3D spatial structure using panoramic depth images and room-impulse responses at training time, while requiring only audio at inference. Building on this representation, we present $\textbf{OWL}$, an ALLM that integrates $\textbf{SAGE}$ with a spatially grounded chain-of-thought to rationalize over direction-of-arrivals (DoA) and distance estimates. Through curriculum learning from perceptual QA to multi-step reasoning, $\textbf{OWL}$ supports o'clock-level azimuth and DoA estimation. To enable large-scale training and evaluation, we construct and release $\textbf{BiDepth}$, a dataset of over one million QA pairs combining binaural audio with panoramic depth images and room impulse responses across both in-room and out-of-room scenarios. Across two benchmark datasets, our new $\textbf{BiDepth}$ and the public SpatialSoundQA, $\textbf{OWL}$ reduces mean DoA error by $\textbf{11$^{\circ}$}$ through $\textbf{SAGE}$ and improves spatial reasoning QA accuracy by up to $\textbf{25}$\% over BAT.

