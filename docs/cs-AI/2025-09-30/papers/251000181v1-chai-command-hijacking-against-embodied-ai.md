---
layout: default
title: CHAI: Command Hijacking against embodied AI
---

# CHAI: Command Hijacking against embodied AI

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.00181" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.00181v1</a>
  <a href="https://arxiv.org/pdf/2510.00181.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.00181v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.00181v1', 'CHAI: Command Hijacking against embodied AI')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Luis Burbano, Diego Ortiz, Qi Sun, Siwei Yang, Haoqin Tu, Cihang Xie, Yinzhi Cao, Alvaro A Cardenas

**åˆ†ç±»**: cs.CR, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCHAIä»¥è§£å†³å¯¹å…·èº«AIçš„å‘½ä»¤åŠ«æŒé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å…·èº«äººå·¥æ™ºèƒ½` `å‘½ä»¤åŠ«æŒ` `å¤šæ¨¡æ€æ”»å‡»` `è§†è§‰è¯­è¨€æ¨¡å‹` `å®‰å…¨æ€§ç ”ç©¶` `å¯¹æŠ—æ”»å‡»` `æœºå™¨äººç³»ç»Ÿ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å…·èº«AIç³»ç»Ÿåœ¨å¤„ç†è¾¹ç¼˜æ¡ˆä¾‹æ—¶å­˜åœ¨å®‰å…¨é£é™©ï¼Œå°¤å…¶æ˜¯åœ¨å¤šæ¨¡æ€ç†è§£æ–¹é¢å®¹æ˜“å—åˆ°æ”»å‡»ã€‚
2. CHAIé€šè¿‡åµŒå…¥è¯¯å¯¼æ€§æŒ‡ä»¤å¹¶ç³»ç»Ÿæ€§åœ°æœç´¢ä»¤ç‰Œç©ºé—´ï¼Œåˆ©ç”¨LVLMçš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›æ¥è¿›è¡Œå‘½ä»¤åŠ«æŒã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCHAIåœ¨æ— äººæœºã€è‡ªåŠ¨é©¾é©¶å’Œç‰©ä½“è·Ÿè¸ªä»»åŠ¡ä¸­å‡ä¼˜äºç°æœ‰æ”»å‡»æ–¹æ³•ï¼Œå±•ç°å‡ºæ›´å¼ºçš„æ”»å‡»èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å…·èº«äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰åœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ï¼Œé€šè¿‡åŸºäºæ„ŸçŸ¥å’Œè¡ŒåŠ¨çš„å¸¸è¯†æ¨ç†æ¥å¤„ç†æœºå™¨äººç³»ç»Ÿä¸­çš„è¾¹ç¼˜æ¡ˆä¾‹ã€‚ç„¶è€Œï¼Œè¿™äº›èƒ½åŠ›ä¹Ÿå¸¦æ¥äº†æ–°çš„å®‰å…¨é£é™©ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºæç¤ºçš„æ”»å‡»æ–¹æ³•CHAIï¼ˆCommand Hijacking against embodied AIï¼‰ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„å¤šæ¨¡æ€è¯­è¨€ç†è§£èƒ½åŠ›ï¼Œé€šè¿‡åœ¨è§†è§‰è¾“å…¥ä¸­åµŒå…¥è¯¯å¯¼æ€§è‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œç³»ç»Ÿæ€§åœ°æœç´¢ä»¤ç‰Œç©ºé—´ï¼Œæ„å»ºæç¤ºå­—å…¸ï¼Œå¹¶å¼•å¯¼æ”»å‡»æ¨¡å‹ç”Ÿæˆè§†è§‰æ”»å‡»æç¤ºã€‚æˆ‘ä»¬åœ¨å››ä¸ªLVLMä»£ç†ï¼ˆæ— äººæœºç´§æ€¥ç€é™†ã€è‡ªåŠ¨é©¾é©¶å’Œç©ºä¸­ç‰©ä½“è·Ÿè¸ªï¼‰åŠä¸€ä¸ªçœŸå®æœºå™¨äººä¸Šè¯„ä¼°äº†CHAIï¼Œå®éªŒç»“æœè¡¨æ˜ï¼ŒCHAIåœ¨æ€§èƒ½ä¸Šå§‹ç»ˆä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ”»å‡»æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å…·èº«AIç³»ç»Ÿä¸­çš„å‘½ä»¤åŠ«æŒé—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤šæ¨¡æ€ç†è§£å’Œå®‰å…¨æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå®¹æ˜“è¢«è¯¯å¯¼æ€§è¾“å…¥æ”»å‡»ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCHAIçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„å¤šæ¨¡æ€è¯­è¨€ç†è§£èƒ½åŠ›ï¼Œé€šè¿‡åœ¨è§†è§‰è¾“å…¥ä¸­åµŒå…¥è¯¯å¯¼æ€§è‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œç³»ç»Ÿæ€§åœ°ç”Ÿæˆæ”»å‡»æç¤ºï¼Œä»è€ŒåŠ«æŒå‘½ä»¤ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCHAIçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®è¾“å…¥æ¨¡å—ã€ä»¤ç‰Œç©ºé—´æœç´¢æ¨¡å—ã€æç¤ºå­—å…¸æ„å»ºæ¨¡å—å’Œæ”»å‡»æ¨¡å‹ç”Ÿæˆæ¨¡å—ï¼Œå½¢æˆä¸€ä¸ªå®Œæ•´çš„æ”»å‡»æµç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šCHAIçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶åŸºäºæç¤ºçš„æ”»å‡»æ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨LVLMçš„è¯­ä¹‰å’Œå¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ï¼Œè¶…è¶Šä¼ ç»Ÿå¯¹æŠ—æ”»å‡»çš„å±€é™ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒCHAIä½¿ç”¨äº†ç‰¹å®šçš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ï¼Œä»¥ä¼˜åŒ–ç”Ÿæˆçš„è§†è§‰æ”»å‡»æç¤ºï¼Œå¹¶ç¡®ä¿å…¶åœ¨ä¸åŒä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’Œè®­ç»ƒç»†èŠ‚åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒCHAIåœ¨æ— äººæœºç´§æ€¥ç€é™†ã€è‡ªåŠ¨é©¾é©¶å’Œç©ºä¸­ç‰©ä½“è·Ÿè¸ªä»»åŠ¡ä¸­ï¼Œæ”»å‡»æˆåŠŸç‡æ˜¾è‘—é«˜äºç°æœ‰æœ€å…ˆè¿›çš„æ”»å‡»æ–¹æ³•ï¼Œæå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œå±•ç¤ºäº†å…¶å¼ºå¤§çš„æ”»å‡»èƒ½åŠ›å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ— äººé©¾é©¶æ±½è½¦ã€æ— äººæœºæ§åˆ¶å’Œæœºå™¨äººå¯¼èˆªç­‰ï¼Œèƒ½å¤Ÿå¸®åŠ©å¼€å‘æ›´å®‰å…¨çš„å…·èº«AIç³»ç»Ÿã€‚é€šè¿‡è¯†åˆ«å’Œé˜²èŒƒå‘½ä»¤åŠ«æŒæ”»å‡»ï¼Œæå‡ç³»ç»Ÿçš„å®‰å…¨æ€§å’Œå¯é æ€§ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Embodied Artificial Intelligence (AI) promises to handle edge cases in robotic vehicle systems where data is scarce by using common-sense reasoning grounded in perception and action to generalize beyond training distributions and adapt to novel real-world situations. These capabilities, however, also create new security risks. In this paper, we introduce CHAI (Command Hijacking against embodied AI), a new class of prompt-based attacks that exploit the multimodal language interpretation abilities of Large Visual-Language Models (LVLMs). CHAI embeds deceptive natural language instructions, such as misleading signs, in visual input, systematically searches the token space, builds a dictionary of prompts, and guides an attacker model to generate Visual Attack Prompts. We evaluate CHAI on four LVLM agents; drone emergency landing, autonomous driving, and aerial object tracking, and on a real robotic vehicle. Our experiments show that CHAI consistently outperforms state-of-the-art attacks. By exploiting the semantic and multimodal reasoning strengths of next-generation embodied AI systems, CHAI underscores the urgent need for defenses that extend beyond traditional adversarial robustness.

