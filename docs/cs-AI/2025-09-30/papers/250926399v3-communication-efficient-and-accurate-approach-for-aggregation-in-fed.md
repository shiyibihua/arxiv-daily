---
layout: default
title: Communication-Efficient and Accurate Approach for Aggregation in Federated Low-Rank Adaptation
---

# Communication-Efficient and Accurate Approach for Aggregation in Federated Low-Rank Adaptation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.26399" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.26399v3</a>
  <a href="https://arxiv.org/pdf/2509.26399.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.26399v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.26399v3', 'Communication-Efficient and Accurate Approach for Aggregation in Federated Low-Rank Adaptation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Le-Tuan Nguyen, Minh-Duong Nguyen, Seon-Geun Jeong, Dung D. Le, Quoc-Viet Pham

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30 (æ›´æ–°: 2025-10-02)

**å¤‡æ³¨**: 34 pages, 4 figures, 11 tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFLoRA-NAä»¥è§£å†³è”é‚¦ä½ç§©é€‚åº”ä¸­çš„é€šä¿¡æ•ˆç‡ä¸å‡†ç¡®æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è”é‚¦å­¦ä¹ ` `ä½ç§©é€‚åº”` `é€šä¿¡æ•ˆç‡` `æ¨¡å‹èšåˆ` `ä¸ªæ€§åŒ–å­¦ä¹ ` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ•°å­¦æ¨ç†` `ä»£ç è§£å†³`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰FedLoRAæ–¹æ³•åœ¨æ›´æ–°ä¸å‡†ç¡®çš„æƒ…å†µä¸‹ï¼Œå¯¼è‡´å±€éƒ¨ä¸å…¨å±€æ³›åŒ–ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®è·ï¼Œå¹¶ä¸”é€šä¿¡å¼€é”€è¾ƒå¤§ï¼Œå½±å“äº†å¯æ‰©å±•æ€§å’Œæœ‰æ•ˆæ€§ã€‚
2. FLoRA-NAé€šè¿‡åœ¨æœåŠ¡å™¨ä¸Šåˆ©ç”¨å±€éƒ¨LoRAçŸ©é˜µæ¥ä¼°è®¡èšåˆçŸ©é˜µï¼Œè¿›è€Œåœ¨ä¸å¢åŠ é€šä¿¡æˆæœ¬çš„æƒ…å†µä¸‹å®ç°æ›´å‡†ç¡®çš„æ›´æ–°ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFLoRA-NAåœ¨è‡ªç„¶è¯­è¨€ç†è§£ã€æ•°å­¦æ¨ç†å’Œä»£ç è§£å†³èƒ½åŠ›ç­‰å¤šé¡¹ä»»åŠ¡ä¸Šå‡å–å¾—äº†æœ€å…ˆè¿›çš„å…¨å±€æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†ä½é€šä¿¡å¼€é”€ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€åŸºç¡€æ¨¡å‹çš„å¿«é€Ÿå‘å±•å’Œåœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­å¾®è°ƒçš„éœ€æ±‚å¢åŠ ï¼Œè”é‚¦ä½ç§©é€‚åº”ï¼ˆFedLoRAï¼‰å—åˆ°å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰FedLoRAæ–¹æ³•é¢ä¸´ä¸å‡†ç¡®æ›´æ–°çš„æŒ‘æˆ˜ï¼Œå¯¼è‡´å±€éƒ¨ä¸å…¨å±€æ³›åŒ–ä¹‹é—´å­˜åœ¨å·®è·ï¼Œå¹¶å¢åŠ äº†é€šä¿¡å¼€é”€ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†FLoRA-NAæ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨æœåŠ¡å™¨ä¸Šçš„å±€éƒ¨LoRAçŸ©é˜µæ¥ä¼°è®¡èšåˆçŸ©é˜µï¼Œä»è€Œåœ¨ä¸å¢åŠ é€šä¿¡æˆæœ¬çš„æƒ…å†µä¸‹å®ç°é€šä¿¡æ•ˆç‡ï¼Œç¼©å°å±€éƒ¨ä¸ªæ€§åŒ–ä¸å…¨å±€æ³›åŒ–ä¹‹é—´çš„å·®è·ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFLoRA-NAåœ¨å¤šä¸ªä»»åŠ¡ä¸Šå‡è¡¨ç°å‡ºè‰²ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„å…¨å±€æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†ä½é€šä¿¡å¼€é”€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰FedLoRAæ–¹æ³•ä¸­ç”±äºä¸å‡†ç¡®æ›´æ–°å¯¼è‡´çš„å±€éƒ¨ä¸å…¨å±€æ³›åŒ–å·®è·ï¼Œä»¥åŠç”±æ­¤äº§ç”Ÿçš„é«˜é€šä¿¡å¼€é”€é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šFLoRA-NAçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æœåŠ¡å™¨ä¸Šçš„å±€éƒ¨LoRAçŸ©é˜µæ¥ä¼°è®¡èšåˆçŸ©é˜µï¼Œä»è€Œåœ¨ä¸å¢åŠ é¢å¤–é€šä¿¡æˆæœ¬çš„æƒ…å†µä¸‹ï¼Œå‡å°‘ç†æƒ³æ›´æ–°ä¸å®é™…æ›´æ–°ä¹‹é—´çš„å·®å¼‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šFLoRA-NAçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼ŒæœåŠ¡å™¨æ”¶é›†å„å®¢æˆ·ç«¯çš„å±€éƒ¨LoRAçŸ©é˜µï¼›å…¶æ¬¡ï¼ŒæœåŠ¡å™¨åˆ©ç”¨è¿™äº›çŸ©é˜µä¼°è®¡èšåˆçŸ©é˜µï¼›æœ€åï¼Œå°†ä¼°è®¡çš„èšåˆçŸ©é˜µåˆ†å‘ç»™å®¢æˆ·ç«¯è¿›è¡Œæœ¬åœ°æ›´æ–°ã€‚

**å…³é”®åˆ›æ–°**ï¼šFLoRA-NAçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶é€šè¿‡è¿‘ä¼¼èšåˆçŸ©é˜µçš„æ–¹å¼ï¼ŒæˆåŠŸåœ°å‡å°‘äº†å±€éƒ¨ä¸ªæ€§åŒ–ä¸å…¨å±€æ³›åŒ–ä¹‹é—´çš„å·®è·ï¼ŒåŒæ—¶ä¿æŒäº†é€šä¿¡æ•ˆç‡ï¼Œè¿™åœ¨ç°æœ‰æ–¹æ³•ä¸­å°šæœªå®ç°ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒFLoRA-NAé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–èšåˆçŸ©é˜µçš„ä¼°è®¡ï¼Œå¹¶ä¸”åœ¨ç½‘ç»œç»“æ„ä¸Šè¿›è¡Œäº†é€‚å½“çš„è°ƒæ•´ï¼Œä»¥ç¡®ä¿åœ¨å¤šç§åŸºç¡€æ¨¡å‹ä¸Šå‡èƒ½æœ‰æ•ˆè¿è¡Œã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨å®éªŒéƒ¨åˆ†è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒFLoRA-NAåœ¨è‡ªç„¶è¯­è¨€ç†è§£ã€æ•°å­¦æ¨ç†å’Œä»£ç è§£å†³èƒ½åŠ›ç­‰ä»»åŠ¡ä¸Šå‡å®ç°äº†æœ€å…ˆè¿›çš„å…¨å±€æ€§èƒ½ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•ï¼Œé€šä¿¡å¼€é”€æ˜¾è‘—é™ä½ï¼ŒåŒæ—¶å‡†ç¡®æ€§æå‡ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å·¨å¤§æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

FLoRA-NAçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼ŒåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€åˆ†å¸ƒå¼æœºå™¨å­¦ä¹ å’Œæ™ºèƒ½æœºå™¨äººç­‰ã€‚å…¶é«˜æ•ˆçš„é€šä¿¡æœºåˆ¶å’Œå‡†ç¡®çš„æ¨¡å‹æ›´æ–°ç­–ç•¥ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡è¿™äº›é¢†åŸŸä¸­çš„æ¨¡å‹æ€§èƒ½å’Œé€‚åº”æ€§ï¼Œä¿ƒè¿›æ›´æ™ºèƒ½çš„åº”ç”¨åœºæ™¯å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> With the rapid emergence of foundation models and the increasing need for fine-tuning across distributed environments, Federated Low-Rank Adaptation (FedLoRA) has recently gained significant attention. Despite enormous potential, current FedLoRA methods face notable challenges due to inexact updates. Existing approaches have attempted to mitigate this issue, but they often introduce a \emph{local-global generalization gap} and incur \emph{substantial communication overhead}, limiting their scalability and effectiveness. To address these limitations, we propose \textbf{F}ederated \textbf{Lo}w-\textbf{R}ank \textbf{A}ggregation with \textbf{N}early \textbf{A}ccurate Estimation (FLoRA-NA). FLoRA-NA leverages the local LoRA matrices on the server to estimate the aggregated matrices $\hat{A}$ and $\hat{B}$, which are then distributed to clients for local updates. This surrogated aggregated matrices minimizes the divergence between ideal $\nabla \Bar{W} = \sum^{U}_{u=1}B_u A_u$ and practical updates $\nabla \hat{W} = \hat{B}\hat{A}$ without adding communication cost beyond vanilla FedLoRA. By doing so, FLoRA-NA achieves communication efficiency and bridges the gap between local personalization and global generalization, addressing a key limitation of prior personalized FedLoRA approaches. We conduct extensive evaluations across diverse tasks, including natural language understanding, mathematical reasoning, and code-solving ability using various foundation models. Experimental results consistently demonstrate that FLoRA-NA achieves state-of-the-art global performance while maintaining low communication overhead.

