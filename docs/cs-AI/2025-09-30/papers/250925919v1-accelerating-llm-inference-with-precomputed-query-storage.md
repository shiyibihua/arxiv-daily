---
layout: default
title: Accelerating LLM Inference with Precomputed Query Storage
---

# Accelerating LLM Inference with Precomputed Query Storage

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25919" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25919v1</a>
  <a href="https://arxiv.org/pdf/2509.25919.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25919v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25919v1', 'Accelerating LLM Inference with Precomputed Query Storage')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jay H. Park, Youngju Cho, Choungsol Lee, Moonwook Oh, Euiseong Seo

**åˆ†ç±»**: cs.DC, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**StorInferï¼šåˆ©ç”¨é¢„è®¡ç®—æŸ¥è¯¢å­˜å‚¨åŠ é€ŸLLMæ¨ç†ï¼Œå°¤å…¶é€‚ç”¨äºèµ„æºå—é™ç¯å¢ƒ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `LLMæ¨ç†åŠ é€Ÿ` `é¢„è®¡ç®—` `å­˜å‚¨è¾…åŠ©æ¨ç†` `å‘é‡æ•°æ®åº“` `ä½å»¶è¿Ÿ` `èµ„æºå—é™ç¯å¢ƒ` `è‡ªé€‚åº”æŸ¥è¯¢ç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. LLMæ¨ç†åœ¨èµ„æºå—é™è®¾å¤‡ä¸Šå»¶è¿Ÿé«˜æ˜‚ï¼Œä¼ ç»Ÿæ–¹æ³•ä¾èµ–æ˜‚è´µçš„GPUè®¡ç®—ã€‚
2. StorInferé¢„è®¡ç®—å¹¶å­˜å‚¨æŸ¥è¯¢-å“åº”å¯¹ï¼Œé€šè¿‡è¯­ä¹‰åŒ¹é…ç›´æ¥è¿”å›ç»“æœï¼Œé¿å…å®æ—¶æ¨ç†ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒStorInferåœ¨ä¿è¯è´¨é‡çš„å‰æä¸‹ï¼Œæ˜¾è‘—é™ä½äº†æ¨ç†å»¶è¿Ÿï¼Œæå‡äº†æ•ˆç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†é€šå¸¸é¢ä¸´é«˜å»¶è¿Ÿé—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨è®¾å¤‡ç«¯æˆ–è¾¹ç¼˜éƒ¨ç½²ç­‰èµ„æºå—é™ç¯å¢ƒä¸­ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†StorInferï¼Œä¸€ç§æ–°é¢–çš„å­˜å‚¨è¾…åŠ©LLMæ¨ç†ç³»ç»Ÿï¼Œé€šè¿‡ç¦»çº¿é¢„è®¡ç®—å’Œå­˜å‚¨å¯é¢„æµ‹çš„æŸ¥è¯¢-å“åº”å¯¹æ¥åŠ é€Ÿå“åº”æ—¶é—´ã€‚å½“ç”¨æˆ·æŸ¥è¯¢åœ¨è¯­ä¹‰ä¸Šä¸é¢„è®¡ç®—æŸ¥è¯¢åŒ¹é…æ—¶ï¼ŒStorInferç»•è¿‡æ˜‚è´µçš„GPUæ¨ç†ï¼Œç«‹å³è¿”å›å­˜å‚¨çš„å“åº”ï¼Œä»è€Œæ˜¾è‘—é™ä½å»¶è¿Ÿå’Œè®¡ç®—æˆæœ¬ã€‚ä¸ºäº†æœ€å¤§åŒ–è¦†ç›–ç‡å’Œæœ‰æ•ˆæ€§ï¼ŒStorInferé‡‡ç”¨LLMé©±åŠ¨çš„ç”Ÿæˆå™¨ï¼Œè¯¥ç”Ÿæˆå™¨åŸºäºç»™å®šçš„çŸ¥è¯†åº“è‡ªé€‚åº”åœ°ç”Ÿæˆå¤šæ ·åŒ–å’Œå»é‡çš„æŸ¥è¯¢ã€‚è¿™é€šè¿‡ä¸¤ç§æŠ€æœ¯å®ç°ï¼šè‡ªé€‚åº”æŸ¥è¯¢æ©ç ï¼Œé˜²æ­¢é‡æ–°ç”Ÿæˆç›¸ä¼¼æŸ¥è¯¢ï¼›è‡ªé€‚åº”é‡‡æ ·ï¼ŒåŠ¨æ€è°ƒæ•´ç”Ÿæˆå‚æ•°ä»¥ä¿ƒè¿›è¯­ä¹‰å¤šæ ·æ€§ã€‚ç”Ÿæˆçš„æŸ¥è¯¢-å“åº”å¯¹è¢«åµŒå…¥å¹¶ä½¿ç”¨ç£ç›˜æ”¯æŒçš„å‘é‡æ•°æ®åº“è¿›è¡Œç´¢å¼•ï¼Œä»¥å®ç°è¿è¡Œæ—¶å¿«é€Ÿçš„åŸºäºç›¸ä¼¼æ€§çš„æ£€ç´¢ã€‚ä½¿ç”¨è¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬ç”Ÿæˆäº†15ä¸‡ä¸ªç‹¬ç‰¹çš„é¢„è®¡ç®—å¯¹ï¼ˆå ç”¨é«˜è¾¾830 MBçš„å­˜å‚¨ç©ºé—´ï¼‰ï¼Œå®ç°äº†é«˜è¾¾17.3%çš„å»¶è¿Ÿé™ä½ï¼Œä¸”ä¸æŸå¤±å“åº”è´¨é‡ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªQAæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¯æ˜äº†å­˜å‚¨è¾…åŠ©æ¨ç†çš„å®ç”¨æ€§å’Œå¯æ‰©å±•æ€§ï¼Œå°¤å…¶æ˜¯åœ¨å…·æœ‰å¯é¢„æµ‹æŸ¥è¯¢åˆ†å¸ƒçš„åœºæ™¯ä¸­ã€‚StorInferçªå‡ºäº†åˆ©ç”¨å­˜å‚¨ä½œä¸ºé«˜æ•ˆã€ä½å»¶è¿ŸLLMéƒ¨ç½²çš„ä¸»è¦æ¨åŠ¨å› ç´ çš„æœ‰å¸Œæœ›çš„æ–¹å‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨èµ„æºå—é™è®¾å¤‡ä¸Šæ¨ç†å»¶è¿Ÿé«˜çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºå®æ—¶è®¡ç®—ï¼Œè®¡ç®—é‡å¤§ï¼Œå»¶è¿Ÿé«˜ï¼Œéš¾ä»¥æ»¡è¶³å®æ—¶æ€§è¦æ±‚ã€‚å°¤å…¶æ˜¯åœ¨æŸ¥è¯¢åˆ†å¸ƒç›¸å¯¹å›ºå®šçš„åœºæ™¯ä¸‹ï¼Œé‡å¤è®¡ç®—é€ æˆäº†èµ„æºæµªè´¹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨é¢„è®¡ç®—å’Œå­˜å‚¨æ¥åŠ é€ŸLLMæ¨ç†ã€‚é€šè¿‡ç¦»çº¿é¢„å…ˆè®¡ç®—å¹¶å­˜å‚¨å¸¸è§çš„æŸ¥è¯¢åŠå…¶å¯¹åº”çš„å“åº”ï¼Œåœ¨æ¨ç†æ—¶ï¼Œç›´æ¥æ£€ç´¢é¢„è®¡ç®—ç»“æœï¼Œé¿å…é‡å¤è®¡ç®—ï¼Œä»è€Œé™ä½å»¶è¿Ÿå’Œè®¡ç®—æˆæœ¬ã€‚è¿™ç§æ–¹æ³•å°¤å…¶é€‚ç”¨äºæŸ¥è¯¢åˆ†å¸ƒç›¸å¯¹å›ºå®šçš„åœºæ™¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šStorInferç³»ç»ŸåŒ…å«ç¦»çº¿é¢„è®¡ç®—å’Œåœ¨çº¿æ¨ç†ä¸¤ä¸ªä¸»è¦é˜¶æ®µã€‚ç¦»çº¿é¢„è®¡ç®—é˜¶æ®µï¼Œåˆ©ç”¨LLMé©±åŠ¨çš„ç”Ÿæˆå™¨ï¼ŒåŸºäºçŸ¥è¯†åº“ç”Ÿæˆå¤šæ ·åŒ–çš„æŸ¥è¯¢ï¼Œå¹¶è®¡ç®—å¯¹åº”çš„å“åº”ï¼Œå°†æŸ¥è¯¢-å“åº”å¯¹å­˜å‚¨åœ¨å‘é‡æ•°æ®åº“ä¸­ã€‚åœ¨çº¿æ¨ç†é˜¶æ®µï¼Œæ¥æ”¶ç”¨æˆ·æŸ¥è¯¢ï¼Œé€šè¿‡å‘é‡æ•°æ®åº“æ£€ç´¢ç›¸ä¼¼çš„é¢„è®¡ç®—æŸ¥è¯¢ï¼Œå¦‚æœåŒ¹é…æˆåŠŸï¼Œåˆ™ç›´æ¥è¿”å›é¢„è®¡ç®—çš„å“åº”ï¼Œå¦åˆ™è¿›è¡Œå®æ—¶æ¨ç†ã€‚

**å…³é”®åˆ›æ–°**ï¼šStorInferçš„å…³é”®åˆ›æ–°åœ¨äºåˆ©ç”¨LLMé©±åŠ¨çš„ç”Ÿæˆå™¨è‡ªé€‚åº”åœ°ç”Ÿæˆå¤šæ ·åŒ–å’Œå»é‡çš„æŸ¥è¯¢ï¼Œä»¥åŠä½¿ç”¨å‘é‡æ•°æ®åº“è¿›è¡Œå¿«é€Ÿç›¸ä¼¼æ€§æ£€ç´¢ã€‚è‡ªé€‚åº”æŸ¥è¯¢æ©ç é˜²æ­¢ç”Ÿæˆç›¸ä¼¼æŸ¥è¯¢ï¼Œè‡ªé€‚åº”é‡‡æ ·åŠ¨æ€è°ƒæ•´ç”Ÿæˆå‚æ•°ä»¥ä¿ƒè¿›è¯­ä¹‰å¤šæ ·æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŸ¥è¯¢ç”Ÿæˆé˜¶æ®µï¼Œé‡‡ç”¨äº†è‡ªé€‚åº”æŸ¥è¯¢æ©ç å’Œè‡ªé€‚åº”é‡‡æ ·ä¸¤ç§æŠ€æœ¯ã€‚è‡ªé€‚åº”æŸ¥è¯¢æ©ç é€šè¿‡è®°å½•å·²ç”Ÿæˆçš„æŸ¥è¯¢ï¼Œé¿å…é‡å¤ç”Ÿæˆç›¸ä¼¼æŸ¥è¯¢ã€‚è‡ªé€‚åº”é‡‡æ ·é€šè¿‡åŠ¨æ€è°ƒæ•´ç”Ÿæˆå‚æ•°ï¼ˆå¦‚temperatureï¼‰ï¼Œæ§åˆ¶ç”ŸæˆæŸ¥è¯¢çš„å¤šæ ·æ€§ã€‚å‘é‡æ•°æ®åº“é‡‡ç”¨ç£ç›˜å­˜å‚¨ï¼Œä»¥æ”¯æŒå¤§è§„æ¨¡çš„æŸ¥è¯¢-å“åº”å¯¹å­˜å‚¨ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒStorInferåœ¨å¤šä¸ªQAæ•°æ®é›†ä¸Šå®ç°äº†æ˜¾è‘—çš„å»¶è¿Ÿé™ä½ï¼Œæœ€é«˜å¯è¾¾17.3%ï¼Œä¸”ä¸æŸå¤±å“åº”è´¨é‡ã€‚é€šè¿‡ç”Ÿæˆ15ä¸‡ä¸ªç‹¬ç‰¹çš„é¢„è®¡ç®—å¯¹ï¼Œå ç”¨830MBå­˜å‚¨ç©ºé—´ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­çš„å¯è¡Œæ€§å’Œæœ‰æ•ˆæ€§ã€‚è¯¥æ–¹æ³•å°¤å…¶é€‚ç”¨äºå…·æœ‰å¯é¢„æµ‹æŸ¥è¯¢åˆ†å¸ƒçš„åœºæ™¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

StorInferé€‚ç”¨äºèµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡å’Œç§»åŠ¨è®¾å¤‡ä¸Šçš„LLMåº”ç”¨ï¼Œä¾‹å¦‚æ™ºèƒ½å®¢æœã€æ™ºèƒ½å®¶å±…ã€è½¦è½½åŠ©æ‰‹ç­‰ã€‚é€šè¿‡é™ä½æ¨ç†å»¶è¿Ÿå’Œè®¡ç®—æˆæœ¬ï¼Œå¯ä»¥æå‡ç”¨æˆ·ä½“éªŒï¼Œå¹¶é™ä½éƒ¨ç½²æˆæœ¬ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯ä»¥æ‰©å±•åˆ°æ›´å¹¿æ³›çš„LLMåº”ç”¨åœºæ™¯ï¼Œä¾‹å¦‚æ–‡æ¡£æ£€ç´¢ã€çŸ¥è¯†é—®ç­”ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language model (LLM) inference often suffers from high latency, particularly in resource-constrained environments such as on-device or edge deployments. To address this challenge, we present StorInfer, a novel storage-assisted LLM inference system that accelerates response time by precomputing and storing predictable query-response pairs offline. When a user query semantically matches a precomputed query, StorInfer bypasses expensive GPU inference and instantly returns the stored response, significantly reducing latency and compute costs. To maximize coverage and effectiveness, StorInfer employs an LLM-driven generator that adaptively produces diverse and deduplicated queries based on a given knowledge base. This is achieved via two techniques: adaptive query masking, which prevents regeneration of similar queries, and adaptive sampling, which dynamically tunes generation parameters to promote semantic diversity. The resulting query-response pairs are embedded and indexed using a disk-backed vector database to enable fast, similarity-based retrieval at runtime. Using this approach, we generated 150K unique precomputed pairs (taking up to 830 MB of storage space), achieving up to 17.3% latency reduction with no loss in response quality. Our evaluation across multiple QA datasets demonstrates the practicality and scalability of storage-assisted inference, especially in scenarios with predictable query distributions. StorInfer highlights a promising direction in leveraging storage as a primary enabler for efficient, low-latency LLM deployment.

