---
layout: default
title: Toward an Unbiased Collective Memory for Efficient LLM-Based Agentic 6G Cross-Domain Management
---

# Toward an Unbiased Collective Memory for Efficient LLM-Based Agentic 6G Cross-Domain Management

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.26200" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.26200v1</a>
  <a href="https://arxiv.org/pdf/2509.26200.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.26200v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.26200v1', 'Toward an Unbiased Collective Memory for Efficient LLM-Based Agentic 6G Cross-Domain Management')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hatim Chergui, Miguel Catalan Cid, Pouria Sayyad Khodashenas, Daniel Camps Mur, Christos Verikoukis

**åˆ†ç±»**: cs.NI, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-30

**å¤‡æ³¨**: 12 pages, 8 figures

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/HatimChergui/unbiased-collective-memory)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§æ— åé›†ä½“è®°å¿†æ¡†æ¶ï¼Œç”¨äºé«˜æ•ˆçš„åŸºäºLLMçš„Agent 6Gè·¨åŸŸç®¡ç†**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `6Gç½‘ç»œç®¡ç†` `LLM Agent` `è·¨åŸŸèµ„æºç¼–æ’` `è®¤çŸ¥åå·®` `æ— åé›†ä½“è®°å¿†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºLLMçš„Agentåœ¨6Gè·¨åŸŸèµ„æºç®¡ç†ä¸­å­˜åœ¨è®¤çŸ¥åå·®ï¼Œå½±å“å†³ç­–è´¨é‡å’Œæ•ˆç‡ã€‚
2. æå‡ºæ— åé›†ä½“è®°å¿†æ¡†æ¶ï¼Œé€šè¿‡è¯­ä¹‰æ£€ç´¢ã€å¤±è´¥å­¦ä¹ ã€å¤šæ ·æ€§å¢å¼ºå’Œæ—¶é—´åŠ æƒç¼“è§£è®¤çŸ¥åå·®ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶æ˜¾è‘—å‡å°‘æœªè§£å†³åå•†ï¼Œå®Œå…¨æ¶ˆé™¤SLAè¿è§„ï¼Œå¹¶æ”¹å–„å»¶è¿Ÿå’ŒèŠ‚èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œç”¨äº6G RAN-Edgeç½‘ç»œä¸­åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¢å¼ºAgentçš„ä¸»åŠ¨è·¨åŸŸèµ„æºç¼–æ’ã€‚è¯¥ç³»ç»ŸåŒ…å«ä¸“é—¨çš„RANï¼ˆèƒ½æºæ•ˆç‡ï¼‰å’ŒEdgeï¼ˆå»¶è¿Ÿä¿è¯ï¼‰Agentï¼Œå®ƒä»¬è¿›è¡Œè¿­ä»£åå•†ï¼Œå¹¶ç”±é«˜çº§æ¨ç†å’Œè§„åˆ’èƒ½åŠ›æä¾›æ”¯æŒã€‚Agentä¸æ•°å­—å­ªç”Ÿï¼ˆDTï¼‰åŠ¨æ€äº¤äº’ä»¥æµ‹è¯•å…¶ææ¡ˆï¼Œå¹¶åˆ©ç”¨é•¿æœŸé›†ä½“è®°å¿†ï¼Œå…¶ä¸­ä»–ä»¬è”åˆæˆåŠŸå’Œå¤±è´¥çš„åè®®ä»¥åŠç›¸å…³çš„ç½‘ç»œä¸Šä¸‹æ–‡è¢«æç‚¼æˆç­–ç•¥ï¼Œä»¥éµå¾ªæˆ–é¿å…ï¼Œå¹¶éšåå­˜å‚¨ã€‚è€ƒè™‘åˆ°Agentåœ¨æ£€ç´¢è¿™äº›è¿‡å»çš„ç»éªŒæ—¶ä¼šå—åˆ°å¤§é‡çš„è®¤çŸ¥åå·®çš„å½±å“â€”â€”ä¾‹å¦‚é¦–å› æ•ˆåº”ã€è¿‘å› æ•ˆåº”ã€ç¡®è®¤åå·®å’Œå¯å¾—æ€§åå·®â€”â€”æˆ‘ä»¬åœ¨æœ¬æ–‡ä¸­æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ— åè®°å¿†è®¾è®¡ï¼Œå…¶ç‰¹ç‚¹æ˜¯ï¼šï¼ˆiï¼‰é€šè¿‡Jaccardç›¸ä¼¼æ€§è¿›è¡Œè¿‡å»ç­–ç•¥çš„è¯­ä¹‰æ£€ç´¢ï¼›ï¼ˆiiï¼‰é€šè¿‡æ”¾å¤§SLAè¿è§„çš„æƒé‡å’Œå¼ºåˆ¶åŒ…å«å¤±è´¥çš„åå•†æ¡ˆä¾‹æ¥å‡è½»ç¡®è®¤åå·®ï¼Œä»è€Œä»å¤±è´¥ä¸­å­¦ä¹ ï¼›ï¼ˆiiiï¼‰å¼ºåˆ¶å¤šæ ·æ€§ä»¥æœ€å°åŒ–å¯å¾—æ€§åå·®ï¼›ä»¥åŠï¼ˆivï¼‰å…·æœ‰ç¼“æ…¢è¡°å‡çš„è¿‘å› å’Œé¦–å› åŠ æƒä»¥æŠµæ¶ˆæ—¶é—´åå·®ã€‚è¯„ä¼°ç»“æœå±•ç¤ºäº†ç°æœ‰åå·®çš„å½±å“ï¼Œä»¥åŠæ— åè®°å¿†å¦‚ä½•é€šè¿‡å­¦ä¹ æˆåŠŸå’Œå¤±è´¥çš„ç­–ç•¥æ¥è§£å†³è¿™äº›åå·®ï¼Œä»è€Œåˆ†åˆ«å‡å°‘äº†$	imes 4.5$å’Œ$	imes 3.5$å€çš„æœªè§£å†³åå•†ï¼Œä¸éè®°å¿†å’ŒåŸå§‹è®°å¿†åŸºçº¿ç›¸æ¯”ï¼ŒåŒæ—¶å®Œå…¨å‡è½»äº†SLAè¿è§„ï¼Œå¹¶æ”¹å–„äº†å»¶è¿Ÿå’ŒèŠ‚èƒ½åˆ†å¸ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³6G RAN-Edgeç½‘ç»œä¸­ï¼ŒåŸºäºLLMçš„Agentåœ¨è·¨åŸŸèµ„æºç¼–æ’æ—¶ï¼Œç”±äºè®¤çŸ¥åå·®ï¼ˆå¦‚é¦–å› æ•ˆåº”ã€è¿‘å› æ•ˆåº”ã€ç¡®è®¤åå·®å’Œå¯å¾—æ€§åå·®ï¼‰å¯¼è‡´å†³ç­–å¤±è¯¯å’Œæ•ˆç‡é™ä½çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆæ¶ˆé™¤è¿™äº›åå·®ï¼Œå¯¼è‡´Agentæ— æ³•å……åˆ†åˆ©ç”¨å†å²ç»éªŒï¼Œåšå‡ºæœ€ä¼˜å†³ç­–ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªâ€œæ— åé›†ä½“è®°å¿†â€ï¼Œè¯¥è®°å¿†èƒ½å¤Ÿå­˜å‚¨å’Œæ£€ç´¢Agentçš„æˆåŠŸå’Œå¤±è´¥ç»éªŒï¼Œå¹¶é‡‡å–æªæ–½å‡è½»è®¤çŸ¥åå·®çš„å½±å“ã€‚é€šè¿‡è®©Agentä»æ›´å…¨é¢ã€æ›´å®¢è§‚çš„å†å²æ•°æ®ä¸­å­¦ä¹ ï¼Œæé«˜å…¶å†³ç­–çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) RAN Agentå’ŒEdge Agentï¼Œè´Ÿè´£èƒ½æºæ•ˆç‡å’Œå»¶è¿Ÿä¿è¯çš„åå•†ï¼›2) æ•°å­—å­ªç”Ÿ(DT)ï¼Œç”¨äºæµ‹è¯•Agentçš„ææ¡ˆï¼›3) æ— åé›†ä½“è®°å¿†ï¼Œå­˜å‚¨Agentçš„åå•†å†å²ï¼Œå¹¶æä¾›ç­–ç•¥æ£€ç´¢åŠŸèƒ½ã€‚Agenté¦–å…ˆä¸DTäº¤äº’è¿›è¡Œæ–¹æ¡ˆæµ‹è¯•ï¼Œç„¶ååˆ©ç”¨æ— åé›†ä½“è®°å¿†æ£€ç´¢ç›¸ä¼¼çš„å†å²ç­–ç•¥ï¼Œå¹¶æ ¹æ®æ£€ç´¢ç»“æœè¿›è¡Œå†³ç­–ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæ— åé›†ä½“è®°å¿†çš„è®¾è®¡ï¼Œå®ƒé€šè¿‡ä»¥ä¸‹æœºåˆ¶æ¥å‡è½»è®¤çŸ¥åå·®ï¼š1) è¯­ä¹‰æ£€ç´¢ï¼šä½¿ç”¨Jaccardç›¸ä¼¼æ€§è¿›è¡Œç­–ç•¥æ£€ç´¢ï¼Œé¿å…ç®€å•åŒ¹é…å¸¦æ¥çš„åå·®ï¼›2) å¤±è´¥å­¦ä¹ ï¼šæ”¾å¤§SLAè¿è§„çš„æƒé‡ï¼Œå¼ºåˆ¶åŒ…å«å¤±è´¥æ¡ˆä¾‹ï¼Œå…‹æœç¡®è®¤åå·®ï¼›3) å¤šæ ·æ€§å¢å¼ºï¼šé¼“åŠ±æ£€ç´¢å¤šæ ·åŒ–çš„ç­–ç•¥ï¼Œå‡å°‘å¯å¾—æ€§åå·®ï¼›4) æ—¶é—´åŠ æƒï¼šä½¿ç”¨ç¼“æ…¢è¡°å‡çš„è¿‘å› å’Œé¦–å› åŠ æƒï¼Œå¹³è¡¡æ–°æ—§ç»éªŒçš„å½±å“ã€‚

**å…³é”®è®¾è®¡**ï¼šæ— åé›†ä½“è®°å¿†çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼šJaccardç›¸ä¼¼æ€§é˜ˆå€¼çš„è®¾ç½®ï¼Œç”¨äºæ§åˆ¶è¯­ä¹‰æ£€ç´¢çš„èŒƒå›´ï¼›SLAè¿è§„æƒé‡çš„å…·ä½“æ•°å€¼ï¼Œç”¨äºæ”¾å¤§å¤±è´¥æ¡ˆä¾‹çš„å½±å“ï¼›å¤šæ ·æ€§å¢å¼ºçš„å…·ä½“ç®—æ³•ï¼Œä¾‹å¦‚ä½¿ç”¨èšç±»æˆ–é‡‡æ ·æ–¹æ³•é€‰æ‹©ä»£è¡¨æ€§ç­–ç•¥ï¼›ä»¥åŠè¿‘å› å’Œé¦–å› åŠ æƒçš„è¡°å‡ç³»æ•°ï¼Œç”¨äºæ§åˆ¶æ—¶é—´å› ç´ çš„å½±å“ã€‚è®ºæ–‡ä¸­å¼€æºäº†æ— åè®°å¿†çš„mockupç‰ˆæœ¬ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä¸éè®°å¿†å’ŒåŸå§‹è®°å¿†åŸºçº¿ç›¸æ¯”ï¼Œè¯¥æ— åé›†ä½“è®°å¿†æ¡†æ¶åˆ†åˆ«å‡å°‘äº†$	imes 4.5$å’Œ$	imes 3.5$å€çš„æœªè§£å†³åå•†ï¼ŒåŒæ—¶å®Œå…¨æ¶ˆé™¤äº†SLAè¿è§„ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜æ”¹å–„äº†å»¶è¿Ÿå’ŒèŠ‚èƒ½çš„åˆ†å¸ƒï¼Œè¡¨æ˜å…¶åœ¨æé«˜ç½‘ç»œæ€§èƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœªæ¥çš„6Gç½‘ç»œç®¡ç†ï¼Œå®ç°æ›´æ™ºèƒ½ã€é«˜æ•ˆçš„èµ„æºåˆ†é…å’Œä¼˜åŒ–ã€‚é€šè¿‡æ¶ˆé™¤è®¤çŸ¥åå·®ï¼ŒAgentèƒ½å¤Ÿåšå‡ºæ›´åˆç†çš„å†³ç­–ï¼Œæé«˜ç½‘ç»œæ€§èƒ½ï¼Œé™ä½è¿è¥æˆæœ¬ï¼Œå¹¶ä¸ºç”¨æˆ·æä¾›æ›´å¥½çš„æœåŠ¡ä½“éªŒã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•ä¹Ÿå¯æ¨å¹¿åˆ°å…¶ä»–å¤šAgentåä½œçš„å¤æ‚ç³»ç»Ÿä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper introduces a novel framework for proactive cross-domain resource orchestration in 6G RAN-Edge networks, featuring large language model (LLM)-augmented agents. The system comprises specialized RAN (energy efficiency) and Edge (latency assurance) agents that engage in iterative negotiation, supported by advanced reasoning and planning capabilities. Agents dynamically interact with a digital twin (DT) to test their proposals and leverage a long-term collective memory where their joint successful and failed agreements along with the related network contexts are distilled into strategies to either follow or avoid and subsequently stored. Given that agents are subject to a plethora of cognitive distortions when retrieving those past experiences -- such as primacy, recency, confirmation and availability biases -- we propose in this work a novel unbiased memory design (A reusable mockup version of the unbiased memory source code is available for non-commercial use at https://github.com/HatimChergui/unbiased-collective-memory). featuring (i) semantic retrieval of past strategies via Jaccard similarity; (ii) learning from failures through amplified weighting of SLA violations and mandatory inclusion of failed negotiation cases to mitigate confirmation bias; (iii) diversity enforcement to minimize availability bias and (iv) recency and primacy weighting with slow decay to counteract temporal biases. Evaluation results showcase the impact of existing biases and how the unbiased memory allows to tackle them by learning from both successful and failed strategies, either present or old, resulting in $\times 4.5$ and $\times 3.5$ reductions of unresolved negotiations compared to non-memory and vanilla memory baselines, respectively, while totally mitigating SLA violations as well as improving latency and energy saving distributions.

