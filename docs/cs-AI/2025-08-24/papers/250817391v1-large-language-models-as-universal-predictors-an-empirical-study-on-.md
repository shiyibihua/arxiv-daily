---
layout: default
title: Large Language Models as Universal Predictors? An Empirical Study on Small Tabular Datasets
---

# Large Language Models as Universal Predictors? An Empirical Study on Small Tabular Datasets

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.17391" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.17391v1</a>
  <a href="https://arxiv.org/pdf/2508.17391.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.17391v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.17391v1', 'Large Language Models as Universal Predictors? An Empirical Study on Small Tabular Datasets')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Nikolaos Pavlidis, Vasilis Perifanis, Symeon Symeonidis, Pavlos S. Efraimidis

**åˆ†ç±»**: cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-24

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ¢è®¨å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å°å‹è¡¨æ ¼æ•°æ®é›†ä¸Šçš„é¢„æµ‹èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å‡½æ•°é€¼è¿‘` `å°å‹æ•°æ®é›†` `åˆ†ç±»ä»»åŠ¡` `å›å½’ä»»åŠ¡` `èšç±»ä»»åŠ¡` `ä¸Šä¸‹æ–‡å­¦ä¹ ` `å•†ä¸šæ™ºèƒ½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æœºå™¨å­¦ä¹ æ–¹æ³•åœ¨å°å‹ç»“æ„åŒ–æ•°æ®é›†ä¸Šçš„è¡¨ç°å—é™ï¼Œå°¤å…¶åœ¨æ•°æ®ç¨€ç¼ºæƒ…å†µä¸‹ã€‚
2. æœ¬æ–‡æå‡ºåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼Œè¿›è¡Œå°å‹è¡¨æ ¼æ•°æ®çš„é¢„æµ‹ä»»åŠ¡ï¼Œé¿å…æ˜¾å¼å¾®è°ƒã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLLMsåœ¨åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°å¼ºåŠ²ï¼Œè€Œåœ¨å›å½’å’Œèšç±»ä»»åŠ¡ä¸­å­˜åœ¨æ˜æ˜¾å±€é™æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æœ€åˆä¸ºè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰è€Œå¼€å‘ï¼Œå·²æ˜¾ç¤ºå‡ºè·¨æ¨¡æ€å’Œé¢†åŸŸçš„æ³›åŒ–æ½œåŠ›ã€‚æœ¬æ–‡ç ”ç©¶äº†LLMsåœ¨å°è§„æ¨¡ç»“æ„åŒ–æ•°æ®é›†ä¸Šçš„å‡½æ•°é€¼è¿‘èƒ½åŠ›ï¼Œæ¶µç›–åˆ†ç±»ã€å›å½’å’Œèšç±»ä»»åŠ¡ã€‚é€šè¿‡å¯¹æ¯”å¤šç§å…ˆè¿›LLMsï¼ˆå¦‚GPT-5ã€GPT-4oç­‰ï¼‰ä¸ä¼ ç»Ÿæœºå™¨å­¦ä¹ åŸºçº¿æ¨¡å‹ï¼Œç»“æœè¡¨æ˜LLMsåœ¨æœ‰é™æ•°æ®ä¸‹çš„åˆ†ç±»ä»»åŠ¡è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨å›å½’å’Œèšç±»ä»»åŠ¡ä¸­è¡¨ç°è¾ƒå·®ã€‚è¯¥ç ”ç©¶ä¸ºå•†ä¸šæ™ºèƒ½å’Œæ¢ç´¢æ€§åˆ†ææä¾›äº†ä½å¼€é”€çš„æ•°æ®æ¢ç´¢æ›¿ä»£æ–¹æ¡ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å°å‹ç»“æ„åŒ–æ•°æ®é›†ä¸Šçš„é¢„æµ‹èƒ½åŠ›ï¼Œç°æœ‰æ–¹æ³•åœ¨æ•°æ®ç¨€ç¼ºæƒ…å†µä¸‹è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶åœ¨å›å½’å’Œèšç±»ä»»åŠ¡ä¸­ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡åˆ©ç”¨LLMsçš„ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰èƒ½åŠ›ï¼Œè¿›è¡Œåˆ†ç±»ã€å›å½’å’Œèšç±»ä»»åŠ¡çš„é¢„æµ‹ï¼Œé¿å…äº†ä¼ ç»Ÿæ–¹æ³•çš„æ˜¾å¼å¾®è°ƒè¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨äº†å¤šç§å…ˆè¿›çš„LLMsï¼ˆå¦‚GPT-5ã€Gemini-2.5-Flashç­‰ï¼‰è¿›è¡Œå®éªŒï¼Œå¹¶ä¸çº¿æ€§æ¨¡å‹ã€é›†æˆæ–¹æ³•å’Œè¡¨æ ¼åŸºç¡€æ¨¡å‹ï¼ˆTFMsï¼‰è¿›è¡Œå¯¹æ¯”ï¼Œè¯„ä¼°å…¶åœ¨å°‘é‡æ ·æœ¬æç¤ºä¸‹çš„è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„åˆ›æ–°åœ¨äºå°†LLMsåº”ç”¨äºå°å‹ç»“æ„åŒ–æ•°æ®é›†çš„é¢„æµ‹ä»»åŠ¡ï¼Œå±•ç¤ºäº†å…¶åœ¨åˆ†ç±»ä»»åŠ¡ä¸­çš„å¼ºå¤§èƒ½åŠ›ï¼ŒåŒæ—¶æ­ç¤ºäº†å…¶åœ¨å›å½’å’Œèšç±»ä»»åŠ¡ä¸­çš„å±€é™æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šå®éªŒä¸­å…³æ³¨ä¸Šä¸‹æ–‡å¤§å°å’Œæç¤ºç»“æ„å¯¹é¢„æµ‹è´¨é‡çš„å½±å“ï¼Œè¯†åˆ«å‡ºå½±å“é¢„æµ‹æ€§èƒ½çš„æƒè¡¡å› ç´ ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨åˆ†ç±»ä»»åŠ¡ä¸­ï¼ŒLLMsåœ¨æœ‰é™æ•°æ®ä¸‹è¡¨ç°ä¼˜å¼‚ï¼Œå»ºç«‹äº†å®é™…çš„é›¶è®­ç»ƒåŸºçº¿ï¼›è€Œåœ¨å›å½’ä»»åŠ¡ä¸­ï¼ŒLLMsçš„è¡¨ç°æ˜æ˜¾ä½äºä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨ä¸åŒä»»åŠ¡ä¸Šçš„æ€§èƒ½å·®å¼‚ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶ä¸ºå•†ä¸šæ™ºèƒ½å’Œæ¢ç´¢æ€§åˆ†ææä¾›äº†æ–°çš„æ€è·¯ï¼Œå°¤å…¶åœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ï¼ŒLLMså¯ä»¥ä½œä¸ºå¿«é€Ÿã€ä½å¼€é”€çš„æ•°æ®æ¢ç´¢å·¥å…·ï¼Œå¸®åŠ©ä¼ä¸šåœ¨å†³ç­–è¿‡ç¨‹ä¸­æ›´æœ‰æ•ˆåœ°åˆ©ç”¨æœ‰é™çš„æ•°æ®èµ„æºã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs), originally developed for natural language processing (NLP), have demonstrated the potential to generalize across modalities and domains. With their in-context learning (ICL) capabilities, LLMs can perform predictive tasks over structured inputs without explicit fine-tuning on downstream tasks. In this work, we investigate the empirical function approximation capability of LLMs on small-scale structured datasets for classification, regression and clustering tasks. We evaluate the performance of state-of-the-art LLMs (GPT-5, GPT-4o, GPT-o3, Gemini-2.5-Flash, DeepSeek-R1) under few-shot prompting and compare them against established machine learning (ML) baselines, including linear models, ensemble methods and tabular foundation models (TFMs). Our results show that LLMs achieve strong performance in classification tasks under limited data availability, establishing practical zero-training baselines. In contrast, the performance in regression with continuous-valued outputs is poor compared to ML models, likely because regression demands outputs in a large (often infinite) space, and clustering results are similarly limited, which we attribute to the absence of genuine ICL in this setting. Nonetheless, this approach enables rapid, low-overhead data exploration and offers a viable alternative to traditional ML pipelines in business intelligence and exploratory analytics contexts. We further analyze the influence of context size and prompt structure on approximation quality, identifying trade-offs that affect predictive performance. Our findings suggest that LLMs can serve as general-purpose predictive engines for structured data, with clear strengths in classification and significant limitations in regression and clustering.

