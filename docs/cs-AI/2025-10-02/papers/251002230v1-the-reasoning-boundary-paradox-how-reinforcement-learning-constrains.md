---
layout: default
title: The Reasoning Boundary Paradox: How Reinforcement Learning Constrains Language Models
---

# The Reasoning Boundary Paradox: How Reinforcement Learning Constrains Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.02230" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.02230v1</a>
  <a href="https://arxiv.org/pdf/2510.02230.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.02230v1" onclick="toggleFavorite(this, '2510.02230v1', 'The Reasoning Boundary Paradox: How Reinforcement Learning Constrains Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Phuc Minh Nguyen, Chinh D. La, Duy M. H. Nguyen, Nitesh V. Chawla, Binh T. Nguyen, Khoa D. Doan

**åˆ†ç±»**: cs.AI, cs.CL, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-02

**å¤‡æ³¨**: 23 pages, 15 figures

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/mail-research/SELF-llm-interference)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ­ç¤ºRLVRçº¦æŸè¯­è¨€æ¨¡å‹æ¨ç†è¾¹ç•Œçš„æ‚–è®ºï¼Œå¹¶æå‡ºæ•°æ®ç­–å±•ç®—æ³•æå‡æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨ç†èƒ½åŠ›` `æ•°æ®ç­–å±•` `è´Ÿå¹²æ‰°` `èµ¢è€…é€šåƒ` `æ¨ç†è¾¹ç•Œ` `æ•°å­¦æ¨ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰RLVRæ–¹æ³•åœ¨æå‡LLMæ¨ç†èƒ½åŠ›æ—¶å­˜åœ¨æ¨ç†è¾¹ç•Œæ”¶ç¼©çš„æŒ‘æˆ˜ï¼Œå¯¼è‡´æ¨¡å‹æ³›åŒ–èƒ½åŠ›å—é™ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡åˆ†æRLVRå­¦ä¹ åŠ¨æ€ï¼Œæ­ç¤ºè´Ÿå¹²æ‰°å’Œèµ¢è€…é€šåƒç°è±¡ï¼Œå¹¶è®¾è®¡æ•°æ®ç­–å±•ç®—æ³•è§£å†³ã€‚
3. å®éªŒè¡¨æ˜ï¼Œæ‰€ææ•°æ®ç­–å±•ç®—æ³•èƒ½å¤Ÿæœ‰æ•ˆæå‡Pass@$k$æ€§èƒ½ï¼ŒéªŒè¯äº†å…¶åœ¨è§£å†³æ¨ç†è¾¹ç•Œæ”¶ç¼©é—®é¢˜ä¸Šçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç ”ç©¶äº†ä½¿ç”¨å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ (RLVR)åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹(LLM)æ¨ç†èƒ½åŠ›æ—¶å‡ºç°çš„æ¨ç†è¾¹ç•Œæ”¶ç¼©é—®é¢˜ã€‚é€šè¿‡åˆ†æRLVRçš„å­¦ä¹ åŠ¨æ€ï¼Œæ­ç¤ºäº†å¯¼è‡´è¯¥é—®é¢˜çš„ä¸¤ä¸ªå…³é”®ç°è±¡ã€‚é¦–å…ˆï¼Œå‘ç°äº†RLVRä¸­çš„è´Ÿå¹²æ‰°ï¼Œå³å­¦ä¹ è§£å†³æŸäº›è®­ç»ƒé—®é¢˜ä¼šé™ä½å…¶ä»–é—®é¢˜æ­£ç¡®è§£çš„å¯èƒ½æ€§ï¼Œå¯¼è‡´Pass@$k$æ€§èƒ½ä¸‹é™ã€‚å…¶æ¬¡ï¼Œæ­ç¤ºäº†èµ¢è€…é€šåƒç°è±¡ï¼šRLVRä¸æˆæ¯”ä¾‹åœ°å¼ºåŒ–åŸºç¡€æ¨¡å‹ä¸‹é«˜æ¦‚ç‡çš„æ­£ç¡®è§£é—®é¢˜ï¼ŒåŒæ—¶æŠ‘åˆ¶å…¶ä»–åˆå§‹æ¦‚ç‡è¾ƒä½çš„é—®é¢˜ã€‚é€šè¿‡åœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†ä¸Šçš„ç†è®ºå’Œå®è¯åˆ†æï¼Œè¡¨æ˜è¿™ç§æ•ˆåº”æºäºæ ‡å‡†RLç›®æ ‡ä¸­å›ºæœ‰çš„on-policyé‡‡æ ·ï¼Œå¯¼è‡´æ¨¡å‹æ”¶æ•›åˆ°ç‹­çª„çš„è§£å†³æ–¹æ¡ˆç­–ç•¥ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæå‡ºäº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ•°æ®ç­–å±•ç®—æ³•ï¼Œå°†RLVRå­¦ä¹ é›†ä¸­åœ¨ä½æ¦‚ç‡é—®é¢˜ä¸Šï¼Œä»è€Œæ˜¾è‘—æé«˜äº†Pass@$k$æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºå¼ºåŒ–å­¦ä¹ çš„è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›æå‡æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯ä½¿ç”¨å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ (RLVR)ï¼Œè™½ç„¶åœ¨æŸäº›æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†å­˜åœ¨ä¸€ä¸ªæ‚–è®ºï¼šå®ƒä»¬å¯èƒ½ä¼šç¼©å°æ¨¡å‹çš„æ¨ç†è¾¹ç•Œï¼Œå³æ¨¡å‹åªèƒ½è§£å†³ç‰¹å®šç±»å‹çš„é—®é¢˜ï¼Œè€Œå¯¹å…¶ä»–é—®é¢˜çš„è§£å†³èƒ½åŠ›ä¸‹é™ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†è€ƒè™‘ä¸åŒé—®é¢˜ä¹‹é—´çš„ç›¸äº’å½±å“ï¼Œä»¥åŠå¼ºåŒ–å­¦ä¹ çš„é‡‡æ ·åå·®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ·±å…¥åˆ†æRLVRçš„å­¦ä¹ åŠ¨æ€ï¼Œæ­ç¤ºå¯¼è‡´æ¨ç†è¾¹ç•Œæ”¶ç¼©çš„æ ¹æœ¬åŸå› ã€‚é€šè¿‡ç†è®ºåˆ†æå’Œå®éªŒéªŒè¯ï¼Œå‘ç°äº†è´Ÿå¹²æ‰°å’Œèµ¢è€…é€šåƒä¸¤ç§ç°è±¡ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæå‡ºä¸€ç§æ•°æ®ç­–å±•ç®—æ³•ï¼Œæœ‰é€‰æ‹©æ€§åœ°å…³æ³¨é‚£äº›åœ¨åˆå§‹é˜¶æ®µè¡¨ç°ä¸ä½³çš„é—®é¢˜ï¼Œä»è€Œå¹³è¡¡æ¨¡å‹çš„å­¦ä¹ è¿‡ç¨‹ï¼Œæ‰©å¤§æ¨ç†è¾¹ç•Œã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦éƒ¨åˆ†ï¼š1) ä½¿ç”¨RLVRè®­ç»ƒLLMï¼Œ2) åˆ†æRLVRçš„å­¦ä¹ åŠ¨æ€ï¼Œæ­ç¤ºè´Ÿå¹²æ‰°å’Œèµ¢è€…é€šåƒç°è±¡ï¼Œ3) è®¾è®¡å¹¶åº”ç”¨æ•°æ®ç­–å±•ç®—æ³•ï¼Œé‡æ–°åˆ†é…è®­ç»ƒæ ·æœ¬çš„æƒé‡ï¼Œä½¿æ¨¡å‹æ›´å¤šåœ°å…³æ³¨ä½æ¦‚ç‡é—®é¢˜ã€‚æ•°æ®ç­–å±•ç®—æ³•çš„å…·ä½“æµç¨‹æ˜¯ï¼šé¦–å…ˆè¯„ä¼°æ¯ä¸ªé—®é¢˜åœ¨åŸºç¡€æ¨¡å‹ä¸‹çš„è¡¨ç°ï¼ˆä¾‹å¦‚ï¼ŒPass@$k$ï¼‰ï¼Œç„¶åæ ¹æ®è¡¨ç°å¯¹é—®é¢˜è¿›è¡Œæ’åºï¼Œæœ€åå¢åŠ è¡¨ç°è¾ƒå·®çš„é—®é¢˜åœ¨è®­ç»ƒé›†ä¸­çš„æƒé‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) é¦–æ¬¡æ­ç¤ºäº†RLVRåœ¨LLMæ¨ç†èƒ½åŠ›æå‡ä¸­å­˜åœ¨çš„æ¨ç†è¾¹ç•Œæ”¶ç¼©æ‚–è®ºã€‚2) æ·±å…¥åˆ†æäº†å¯¼è‡´è¯¥æ‚–è®ºçš„è´Ÿå¹²æ‰°å’Œèµ¢è€…é€šåƒç°è±¡ã€‚3) æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ•°æ®ç­–å±•ç®—æ³•ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æ›´åŠ å…³æ³¨é—®é¢˜çš„å¤šæ ·æ€§å’Œæ¨¡å‹çš„å­¦ä¹ å¹³è¡¡ã€‚

**å…³é”®è®¾è®¡**ï¼šæ•°æ®ç­–å±•ç®—æ³•çš„å…³é”®è®¾è®¡åœ¨äºå¦‚ä½•é€‰æ‹©éœ€è¦é‡ç‚¹å…³æ³¨çš„ä½æ¦‚ç‡é—®é¢˜ã€‚è®ºæ–‡é‡‡ç”¨äº†ä¸€ç§åŸºäºPass@$k$çš„ç®€å•ç­–ç•¥ï¼šè®¡ç®—æ¯ä¸ªé—®é¢˜åœ¨åŸºç¡€æ¨¡å‹ä¸‹çš„Pass@$k$å€¼ï¼Œç„¶åé€‰æ‹©Pass@$k$å€¼è¾ƒä½çš„é—®é¢˜ï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¢åŠ è¿™äº›é—®é¢˜çš„æƒé‡ã€‚å…·ä½“çš„æƒé‡è°ƒæ•´ç­–ç•¥å¯ä»¥æ ¹æ®å®é™…æƒ…å†µè¿›è¡Œè°ƒæ•´ï¼Œä¾‹å¦‚ï¼Œå¯ä»¥ä½¿ç”¨ä¸€ä¸ªç®€å•çš„çº¿æ€§å‡½æ•°ï¼Œå°†Pass@$k$å€¼æ˜ å°„åˆ°æƒé‡å€¼ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ•°æ®ç­–å±•ç®—æ³•èƒ½å¤Ÿæ˜¾è‘—æå‡LLMåœ¨æ•°å­¦æ¨ç†åŸºå‡†ä¸Šçš„Pass@$k$æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸäº›åŸºå‡†ä¸Šï¼ŒPass@$k$å€¼æå‡äº†è¶…è¿‡10ä¸ªç™¾åˆ†ç‚¹ã€‚æ­¤å¤–ï¼Œå®éªŒè¿˜éªŒè¯äº†è´Ÿå¹²æ‰°å’Œèµ¢è€…é€šåƒç°è±¡çš„å­˜åœ¨ï¼Œä¸ºç†è§£RLVRçš„å­¦ä¹ åŠ¨æ€æä¾›äº†é‡è¦çš„ insightsã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæå‡å„ç§LLMçš„æ¨ç†èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨æ•°å­¦ã€é€»è¾‘ç­‰éœ€è¦å¤æ‚æ¨ç†çš„é¢†åŸŸã€‚é€šè¿‡è§£å†³æ¨ç†è¾¹ç•Œæ”¶ç¼©é—®é¢˜ï¼Œå¯ä»¥æé«˜LLMåœ¨å®é™…åº”ç”¨ä¸­çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ï¼Œä¾‹å¦‚åœ¨æ™ºèƒ½å®¢æœã€è‡ªåŠ¨ç¼–ç¨‹ã€ç§‘å­¦ç ”ç©¶ç­‰é¢†åŸŸã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key method for improving Large Language Models' reasoning capabilities, yet recent evidence suggests it may paradoxically shrink the reasoning boundary rather than expand it. This paper investigates the shrinkage issue of RLVR by analyzing its learning dynamics and reveals two critical phenomena that explain this failure. First, we expose negative interference in RLVR, where learning to solve certain training problems actively reduces the likelihood of correct solutions for others, leading to the decline of Pass@$k$ performance, or the probability of generating a correct solution within $k$ attempts. Second, we uncover the winner-take-all phenomenon: RLVR disproportionately reinforces problems with high likelihood, correct solutions, under the base model, while suppressing other initially low-likelihood ones. Through extensive theoretical and empirical analysis on multiple mathematical reasoning benchmarks, we show that this effect arises from the inherent on-policy sampling in standard RL objectives, causing the model to converge toward narrow solution strategies. Based on these insights, we propose a simple yet effective data curation algorithm that focuses RLVR learning on low-likelihood problems, achieving notable improvement in Pass@$k$ performance. Our code is available at https://github.com/mail-research/SELF-llm-interference.

