---
layout: default
title: VaPR -- Vision-language Preference alignment for Reasoning
---

# VaPR -- Vision-language Preference alignment for Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.01700" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.01700v1</a>
  <a href="https://arxiv.org/pdf/2510.01700.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.01700v1" onclick="toggleFavorite(this, '2510.01700v1', 'VaPR -- Vision-language Preference alignment for Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Rohan Wadhawan, Fabrice Y Harel-Canada, Zi-Yi Dou, Suhaila Shakiah, Robinson Piramuthu, Nanyun Peng

**åˆ†ç±»**: cs.AI, cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-10-02

**æœŸåˆŠ**: COLM 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**VaPRï¼šé€šè¿‡è§†è§‰-è¯­è¨€åå¥½å¯¹é½æå‡å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `åå¥½å¯¹é½` `ç¡¬è´Ÿä¾‹æŒ–æ˜` `æ¨ç†èƒ½åŠ›` `å¤§å‹è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åå¥½å¾®è°ƒæ–¹æ³•å¿½ç•¥äº†åˆæˆåå¥½æ ‡æ³¨ä¸­å­˜åœ¨çš„å™ªå£°ï¼Œå¦‚é£æ ¼å’Œé•¿åº¦åå·®ï¼Œå½±å“äº†LVLMçš„æ€§èƒ½ã€‚
2. è®ºæ–‡æå‡ºåŸºäºLLMå¼•å¯¼çš„å“åº”ç¼–è¾‘çš„ç¡¬è´Ÿä¾‹ç”Ÿæˆæ¡†æ¶ï¼Œç”Ÿæˆå…·æœ‰é’ˆå¯¹æ€§é”™è¯¯ä½†é£æ ¼å’Œé•¿åº¦ç›¸ä¼¼çš„æ‹’ç»å“åº”ã€‚
3. VaPRæ•°æ®é›†å’Œå¾®è°ƒæ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†LVLMçš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶é™ä½äº†å›ç­”â€œæ˜¯â€çš„å€¾å‘ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè§†è§‰-è¯­è¨€åå¥½å¯¹é½çš„æ¨ç†æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰ä¸­å­˜åœ¨çš„åˆæˆåå¥½æ ‡æ³¨å™ªå£°é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯é£æ ¼å’Œé•¿åº¦åå·®ã€‚ä¸ºæ­¤ï¼Œä½œè€…è®¾è®¡äº†ä¸€ä¸ªåŸºäºLLMå¼•å¯¼çš„å“åº”ç¼–è¾‘çš„ç¡¬è´Ÿä¾‹å“åº”ç”Ÿæˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿç”Ÿæˆå…·æœ‰ç›®æ ‡æ€§é”™è¯¯çš„æ‹’ç»å“åº”ï¼ŒåŒæ—¶ä¿æŒä¸æ¥å—å“åº”åœ¨é£æ ¼å’Œé•¿åº¦ä¸Šçš„ç›¸ä¼¼æ€§ã€‚åˆ©ç”¨è¯¥æ¡†æ¶ï¼Œæ„å»ºäº†åŒ…å«3ä¸‡ä¸ªé«˜è´¨é‡æ ·æœ¬çš„VaPRæ•°æ®é›†ï¼Œå¹¶å¯¹LLaVA-V1.5ã€Qwen2VLå’ŒQwen2.5VLï¼ˆ2B-13Bå¤§å°ï¼‰ä¸‰ä¸ªLVLMå®¶æ—è¿›è¡Œäº†å¾®è°ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVaPRæ¨¡å‹åœ¨åä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå¹³å‡å¢ç›Šåˆ†åˆ«ä¸º6.5%ï¼ˆLLaVAï¼‰ã€4.0%ï¼ˆQwen2VLï¼‰å’Œ1.5%ï¼ˆQwen2.5VLï¼‰ï¼Œå°¤å…¶åœ¨æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°çªå‡ºã€‚æ­¤å¤–ï¼ŒVaPRè¿˜é™ä½äº†LVLMï¼ˆå¦‚LLaVAï¼‰åœ¨äºŒå…ƒé—®é¢˜ä¸­å›ç­”â€œæ˜¯â€çš„å€¾å‘ã€‚è¯¥æ¡†æ¶è¿˜å¯æ¨å¹¿åˆ°å¼€æºLLMä½œä¸ºç¼–è¾‘å™¨ï¼Œä½¿ç”¨GPT-4oåˆæˆçš„æ•°æ®è®­ç»ƒçš„æ¨¡å‹æ€§èƒ½æ¥è¿‘ä½¿ç”¨GPT-4oè®­ç»ƒçš„æ¨¡å‹ã€‚æ•°æ®ã€æ¨¡å‹å’Œä»£ç å·²å…¬å¼€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰åœ¨è¿›è¡Œåå¥½å¾®è°ƒæ—¶ï¼Œä¾èµ–äºAIç”Ÿæˆçš„åé¦ˆã€‚ç„¶è€Œï¼Œè¿™äº›åé¦ˆä¸­å­˜åœ¨å¤§é‡çš„å™ªå£°ï¼Œç‰¹åˆ«æ˜¯é£æ ¼å’Œé•¿åº¦ä¸Šçš„åå·®ã€‚è¿™äº›åå·®ä¼šå½±å“æ¨¡å‹å­¦ä¹ åˆ°çœŸæ­£çš„äººç±»åå¥½ï¼Œä»è€Œé™åˆ¶äº†æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚ç°æœ‰æ–¹æ³•æ²¡æœ‰å……åˆ†è€ƒè™‘åˆ°è¿™äº›å™ªå£°ï¼Œå¯¼è‡´æ¨¡å‹å®¹æ˜“å—åˆ°è¿™äº›åå·®çš„å½±å“ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ç”Ÿæˆé«˜è´¨é‡çš„ç¡¬è´Ÿä¾‹æ¥æé«˜åå¥½å¾®è°ƒçš„è´¨é‡ã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡LLMå¼•å¯¼çš„å“åº”ç¼–è¾‘ï¼Œç”Ÿæˆä¸æ¥å—å“åº”åœ¨é£æ ¼å’Œé•¿åº¦ä¸Šç›¸ä¼¼ï¼Œä½†åŒ…å«ç‰¹å®šé”™è¯¯çš„æ‹’ç»å“åº”ã€‚è¿™æ ·å¯ä»¥è¿«ä½¿æ¨¡å‹æ›´åŠ å…³æ³¨è¯­ä¹‰ä¸Šçš„å·®å¼‚ï¼Œè€Œä¸æ˜¯ä»…ä»…ä¾èµ–é£æ ¼å’Œé•¿åº¦ç­‰è¡¨é¢ç‰¹å¾è¿›è¡Œåˆ¤æ–­ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVaPRæ¡†æ¶ä¸»è¦åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼š1) ç¡¬è´Ÿä¾‹å“åº”ç”Ÿæˆé˜¶æ®µï¼šä½¿ç”¨LLMï¼ˆå¦‚GPT-4oï¼‰ä½œä¸ºç¼–è¾‘å™¨ï¼Œå¯¹åŸå§‹å“åº”è¿›è¡Œç¼–è¾‘ï¼Œå¼•å…¥ç‰¹å®šçš„é”™è¯¯ï¼Œç”Ÿæˆæ‹’ç»å“åº”ã€‚2) åå¥½å¾®è°ƒé˜¶æ®µï¼šä½¿ç”¨ç”Ÿæˆçš„VaPRæ•°æ®é›†ï¼Œé‡‡ç”¨Direct Preference Optimization (DPO)ç­‰åå¥½ä¼˜åŒ–ç®—æ³•ï¼Œå¯¹LVLMè¿›è¡Œå¾®è°ƒã€‚æ•´ä¸ªæµç¨‹æ—¨åœ¨å¯¹é½æ¨¡å‹çš„åå¥½ä¸äººç±»çš„åå¥½ï¼Œæå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ä¸ªç¡¬è´Ÿä¾‹å“åº”ç”Ÿæˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿç”Ÿæˆå…·æœ‰é’ˆå¯¹æ€§é”™è¯¯çš„æ‹’ç»å“åº”ï¼ŒåŒæ—¶ä¿æŒä¸æ¥å—å“åº”åœ¨é£æ ¼å’Œé•¿åº¦ä¸Šçš„ç›¸ä¼¼æ€§ã€‚è¿™ç§æ–¹æ³•æœ‰æ•ˆåœ°è§£å†³äº†ç°æœ‰åå¥½å¾®è°ƒæ–¹æ³•ä¸­å­˜åœ¨çš„åˆæˆåå¥½æ ‡æ³¨å™ªå£°é—®é¢˜ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶å…·æœ‰é€šç”¨æ€§ï¼Œå¯ä»¥æ¨å¹¿åˆ°ä¸åŒçš„LLMä½œä¸ºç¼–è¾‘å™¨ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç¡¬è´Ÿä¾‹å“åº”ç”Ÿæˆé˜¶æ®µï¼Œä½¿ç”¨LLMä½œä¸ºç¼–è¾‘å™¨ï¼Œé€šè¿‡ç‰¹å®šçš„promptå¼•å¯¼LLMå¼•å…¥ä¸åŒç±»å‹çš„é”™è¯¯ï¼Œä¾‹å¦‚äº‹å®é”™è¯¯ã€é€»è¾‘é”™è¯¯ç­‰ã€‚åŒæ—¶ï¼Œé€šè¿‡æ§åˆ¶LLMçš„ç”Ÿæˆè¿‡ç¨‹ï¼Œä¿è¯æ‹’ç»å“åº”ä¸æ¥å—å“åº”åœ¨é£æ ¼å’Œé•¿åº¦ä¸Šä¿æŒç›¸ä¼¼ã€‚åœ¨åå¥½å¾®è°ƒé˜¶æ®µï¼Œé‡‡ç”¨DPOç®—æ³•ï¼Œå¹¶è°ƒæ•´äº†DPOçš„è¶…å‚æ•°ï¼Œä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚å…·ä½“å‚æ•°è®¾ç½®æœªçŸ¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

VaPRæ¨¡å‹åœ¨åä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒLLaVAã€Qwen2VLå’ŒQwen2.5VLçš„å¹³å‡å¢ç›Šåˆ†åˆ«ä¸º6.5%ã€4.0%å’Œ1.5%ã€‚å°¤å…¶åœ¨æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°çªå‡ºã€‚æ­¤å¤–ï¼ŒVaPRè¿˜é™ä½äº†LVLMï¼ˆå¦‚LLaVAï¼‰åœ¨äºŒå…ƒé—®é¢˜ä¸­å›ç­”â€œæ˜¯â€çš„å€¾å‘ã€‚ä½¿ç”¨å¼€æºLLMä½œä¸ºç¼–è¾‘å™¨è®­ç»ƒçš„æ¨¡å‹æ€§èƒ½æ¥è¿‘ä½¿ç”¨GPT-4oè®­ç»ƒçš„æ¨¡å‹ï¼Œè¡¨æ˜è¯¥æ¡†æ¶å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

VaPRçš„ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦è§†è§‰å’Œè¯­è¨€ç†è§£çš„åœºæ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½é—®ç­”ã€è§†è§‰æ¨ç†ã€æœºå™¨äººå¯¼èˆªç­‰ã€‚é€šè¿‡æé«˜LVLMçš„æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥æå‡è¿™äº›åº”ç”¨çš„ç”¨æˆ·ä½“éªŒå’Œæ™ºèƒ½åŒ–æ°´å¹³ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶æå‡ºçš„ç¡¬è´Ÿä¾‹ç”Ÿæˆæ¡†æ¶ä¹Ÿå¯ä»¥æ¨å¹¿åˆ°å…¶ä»–æ¨¡æ€ï¼Œä¾‹å¦‚éŸ³é¢‘å’Œæ–‡æœ¬ï¼Œä»è€Œæå‡å¤šæ¨¡æ€æ¨¡å‹çš„æ€§èƒ½ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Preference finetuning methods like Direct Preference Optimization (DPO) with AI-generated feedback have shown promise in aligning Large Vision-Language Models (LVLMs) with human preferences. However, existing techniques overlook the prevalence of noise in synthetic preference annotations in the form of stylistic and length biases. To this end, we introduce a hard-negative response generation framework based on LLM-guided response editing, that produces rejected responses with targeted errors, maintaining stylistic and length similarity to the accepted ones. Using this framework, we develop the VaPR dataset, comprising 30K high-quality samples, to finetune three LVLM families: LLaVA-V1.5, Qwen2VL & Qwen2.5VL (2B-13B sizes). Our VaPR models deliver significant performance improvements across ten benchmarks, achieving average gains of 6.5% (LLaVA), 4.0% (Qwen2VL), and 1.5% (Qwen2.5VL), with notable improvements on reasoning tasks. A scaling analysis shows that performance consistently improves with data size, with LLaVA models benefiting even at smaller scales. Moreover, VaPR reduces the tendency to answer "Yes" in binary questions - addressing a common failure mode in LVLMs like LLaVA. Lastly, we show that the framework generalizes to open-source LLMs as editors, with models trained on VaPR-OS achieving ~99% of the performance of models trained on \name, which is synthesized using GPT-4o. Our data, models, and code can be found on the project page https://vap-r.github.io

