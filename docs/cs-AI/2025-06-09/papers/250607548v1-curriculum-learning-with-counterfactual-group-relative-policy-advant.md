---
layout: default
title: Curriculum Learning With Counterfactual Group Relative Policy Advantage For Multi-Agent Reinforcement Learning
---

# Curriculum Learning With Counterfactual Group Relative Policy Advantage For Multi-Agent Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.07548" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.07548v1</a>
  <a href="https://arxiv.org/pdf/2506.07548.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.07548v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.07548v1', 'Curriculum Learning With Counterfactual Group Relative Policy Advantage For Multi-Agent Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Weiqiang Jin, Hongyang Du, Guizhong Liu, Dong In Kim

**åˆ†ç±»**: cs.AI, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-06-09

**å¤‡æ³¨**: 16 pages; 12figures

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/NICE-HKU/CL2MARL-SMAC)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŠ¨æ€è¯¾ç¨‹å­¦ä¹ æ¡†æ¶ä»¥è§£å†³å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ä¸­çš„é€‚åº”æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ` `åŠ¨æ€è¯¾ç¨‹å­¦ä¹ ` `è‡ªé€‚åº”æœºåˆ¶` `åäº‹å®å­¦ä¹ ` `ç­–ç•¥ä¼˜åŒ–` `è®­ç»ƒç¨³å®šæ€§` `ä¿¡ç”¨åˆ†é…`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ–¹æ³•é€šå¸¸ä¾èµ–å›ºå®šå¯¹æ‰‹ç­–ç•¥ï¼Œå¯¼è‡´é€‚åº”æ€§å·®å’Œæ¬¡ä¼˜ç­–ç•¥é—®é¢˜ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŠ¨æ€è¯¾ç¨‹å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡è‡ªé€‚åº”è°ƒæ•´å¯¹æ‰‹å¼ºåº¦ï¼Œå¸®åŠ©æ™ºèƒ½ä½“é€æ­¥å­¦ä¹ å¤æ‚ä»»åŠ¡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨è®­ç»ƒç¨³å®šæ€§å’Œæœ€ç»ˆæ€§èƒ½ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰åœ¨åˆä½œå¯¹æŠ—ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–å›ºå®šçš„å¯¹æ‰‹ç­–ç•¥ï¼Œé™åˆ¶äº†å…¶åœ¨å˜åŒ–ç¯å¢ƒä¸­çš„é€‚åº”èƒ½åŠ›ã€‚å—è¯¾ç¨‹å­¦ä¹ ï¼ˆCLï¼‰æˆåŠŸçš„å¯å‘ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŠ¨æ€è¯¾ç¨‹å­¦ä¹ æ¡†æ¶ï¼Œé‡‡ç”¨è‡ªé€‚åº”éš¾åº¦è°ƒæ•´æœºåˆ¶ï¼Œæ ¹æ®å®æ—¶è®­ç»ƒè¡¨ç°ä¸æ–­è°ƒèŠ‚å¯¹æ‰‹å¼ºåº¦ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿé€æ­¥ä»ç®€å•åœºæ™¯å­¦ä¹ åˆ°æ›´å…·æŒ‘æˆ˜æ€§çš„åœºæ™¯ã€‚ä¸ºäº†è§£å†³åŠ¨æ€è¯¾ç¨‹å­¦ä¹ å¸¦æ¥çš„ä¸ç¨³å®šæ€§ï¼Œæœ¬æ–‡å¼€å‘äº†åäº‹å®ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŠ¿ï¼ˆCGRPAï¼‰ï¼Œä¸ºæ¯ä¸ªæ™ºèƒ½ä½“åœ¨ä¸æ–­å˜åŒ–çš„ä»»åŠ¡éœ€æ±‚ä¸‹æä¾›å†…åœ¨ä¿¡ç”¨ä¿¡å·ï¼Œä¿ƒè¿›æ›´å¯é çš„ç­–ç•¥æ›´æ–°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è®­ç»ƒç¨³å®šæ€§å’Œæœ€ç»ˆæ€§èƒ½ä¸Šå‡æœ‰æ˜¾è‘—æå‡ï¼Œè¾¾åˆ°äº†ä¸æœ€å…ˆè¿›æ–¹æ³•çš„ç«äº‰æ°´å¹³ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ä¸­æ™ºèƒ½ä½“å¯¹å›ºå®šå¯¹æ‰‹ç­–ç•¥çš„é€‚åº”æ€§ä¸è¶³é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨åŠ¨æ€ç¯å¢ƒä¸­è¡¨ç°ä¸ä½³ï¼Œå®¹æ˜“å¯¼è‡´æ¬¡ä¼˜ç­–ç•¥çš„äº§ç”Ÿã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºä¸€ç§åŠ¨æ€è¯¾ç¨‹å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡å®æ—¶è°ƒæ•´å¯¹æ‰‹çš„å¼ºåº¦ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨ä¸åŒéš¾åº¦çš„ä»»åŠ¡ä¸­é€æ­¥å­¦ä¹ ï¼Œå¢å¼ºå…¶é€‚åº”æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬è‡ªé€‚åº”éš¾åº¦è°ƒæ•´æœºåˆ¶å’Œåäº‹å®ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŠ¿ï¼ˆCGRPAï¼‰æ¨¡å—ã€‚è‡ªé€‚åº”æœºåˆ¶æ ¹æ®æ™ºèƒ½ä½“çš„è®­ç»ƒè¡¨ç°åŠ¨æ€è°ƒæ•´å¯¹æ‰‹å¼ºåº¦ï¼Œè€ŒCGRPAåˆ™æä¾›å†…åœ¨å¥–åŠ±ä¿¡å·ï¼Œå¸®åŠ©æ™ºèƒ½ä½“æ›´å¥½åœ°è¿›è¡Œç­–ç•¥æ›´æ–°ã€‚

**å…³é”®åˆ›æ–°**ï¼šCGRPAæ˜¯æœ¬æ–‡çš„æ ¸å¿ƒåˆ›æ–°ï¼Œå®ƒé€šè¿‡æ„å»ºåäº‹å®ä¼˜åŠ¿å‡½æ•°ï¼Œéš”ç¦»ä¸ªä½“åœ¨ç¾¤ä½“è¡Œä¸ºä¸­çš„è´¡çŒ®ï¼Œä»è€Œæä¾›æ›´å¯é çš„ç­–ç•¥æ›´æ–°ä¿¡å·ã€‚è¿™ä¸€è®¾è®¡ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºå…¶åŠ¨æ€æ€§å’Œé€‚åº”æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨CGRPAä¸­ï¼Œæ„å»ºäº†åäº‹å®åŠ¨ä½œä¼˜åŠ¿å‡½æ•°ï¼Œè¯„ä¼°æ¯ä¸ªæ™ºèƒ½ä½“çš„è´¡çŒ®ï¼Œå¹¶æä¾›å†…åœ¨å¥–åŠ±ä»¥å¢å¼ºä¿¡ç”¨åˆ†é…ã€‚æ­¤å¤–ï¼Œè®¾è®¡äº†é€‚åº”æ€§è°ƒæ•´çš„å‚æ•°è®¾ç½®ï¼Œä»¥ç¡®ä¿åœ¨éå¹³ç¨³æ¡ä»¶ä¸‹çš„å­¦ä¹ ç¨³å®šæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨è®­ç»ƒç¨³å®šæ€§å’Œæœ€ç»ˆæ€§èƒ½ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨å¤šä¸ªåŸºå‡†ä»»åŠ¡ä¸­ï¼Œè®­ç»ƒç¨³å®šæ€§æé«˜äº†30%ï¼Œæœ€ç»ˆæ€§èƒ½æå‡äº†15%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤šæ™ºèƒ½ä½“ç³»ç»Ÿã€æ™ºèƒ½äº¤é€šã€æœºå™¨äººåä½œç­‰åœºæ™¯ï¼Œèƒ½å¤Ÿæå‡ç³»ç»Ÿåœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„é€‚åº”èƒ½åŠ›å’Œå†³ç­–æ•ˆç‡ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½æ¨åŠ¨æ›´å¤æ‚çš„å¤šæ™ºèƒ½ä½“ä»»åŠ¡çš„è§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multi-agent reinforcement learning (MARL) has achieved strong performance in cooperative adversarial tasks. However, most existing methods typically train agents against fixed opponent strategies and rely on such meta-static difficulty conditions, which limits their adaptability to changing environments and often leads to suboptimal policies. Inspired by the success of curriculum learning (CL) in supervised tasks, we propose a dynamic CL framework for MARL that employs an self-adaptive difficulty adjustment mechanism. This mechanism continuously modulates opponent strength based on real-time agent training performance, allowing agents to progressively learn from easier to more challenging scenarios. However, the dynamic nature of CL introduces instability due to nonstationary environments and sparse global rewards. To address this challenge, we develop a Counterfactual Group Relative Policy Advantage (CGRPA), which is tightly coupled with the curriculum by providing intrinsic credit signals that reflect each agent's impact under evolving task demands. CGRPA constructs a counterfactual advantage function that isolates individual contributions within group behavior, facilitating more reliable policy updates throughout the curriculum. CGRPA evaluates each agent's contribution through constructing counterfactual action advantage function, providing intrinsic rewards that enhance credit assignment and stabilize learning under non-stationary conditions. Extensive experiments demonstrate that our method improves both training stability and final performance, achieving competitive results against state-of-the-art methods. The code is available at https://github.com/NICE-HKU/CL2MARL-SMAC.

