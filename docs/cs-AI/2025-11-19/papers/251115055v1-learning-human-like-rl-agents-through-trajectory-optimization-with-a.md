---
layout: default
title: Learning Human-Like RL Agents Through Trajectory Optimization With Action Quantization
---

# Learning Human-Like RL Agents Through Trajectory Optimization With Action Quantization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.15055" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.15055v1</a>
  <a href="https://arxiv.org/pdf/2511.15055.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.15055v1" onclick="toggleFavorite(this, '2511.15055v1', 'Learning Human-Like RL Agents Through Trajectory Optimization With Action Quantization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jian-Ting Guo, Yu-Cheng Chen, Ping-Chun Hsieh, Kuo-Hao Ho, Po-Wei Huang, Ti-Rong Wu, I-Chen Wu

**åˆ†ç±»**: cs.AI, cs.LG, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-11-19

**å¤‡æ³¨**: Accepted by the Thirty-Ninth Annual Conference on Neural Information Processing Systems (NeurIPS 2025)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºè½¨è¿¹ä¼˜åŒ–çš„åŠ¨ä½œé‡åŒ–æ–¹æ³•MAQï¼Œæå‡å¼ºåŒ–å­¦ä¹ Agentçš„äººç±»ç›¸ä¼¼åº¦**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `ç±»äººAgent` `è½¨è¿¹ä¼˜åŒ–` `åŠ¨ä½œé‡åŒ–` `å‘é‡é‡åŒ–VAE`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¼ºåŒ–å­¦ä¹ Agenté€šå¸¸è¡¨ç°å‡ºä¸äººç±»ä¸åŒçš„è¡Œä¸ºï¼Œç¼ºä¹å¯è§£é‡Šæ€§å’Œå¯ä¿¡åº¦ï¼Œéš¾ä»¥ç›´æ¥åº”ç”¨ã€‚
2. è®ºæ–‡æå‡ºå®åŠ¨ä½œé‡åŒ–(MAQ)æ¡†æ¶ï¼Œé€šè¿‡è½¨è¿¹ä¼˜åŒ–å’Œå‘é‡é‡åŒ–VAEå­¦ä¹ ç±»äººè¡Œä¸ºï¼Œå…¼é¡¾å¥–åŠ±æœ€å¤§åŒ–å’Œè¡Œä¸ºç›¸ä¼¼æ€§ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒMAQæ˜¾è‘—æé«˜äº†Agentçš„ç±»äººåº¦ï¼Œåœ¨D4RL AdroitåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½å’Œäººç±»è¯„ä¼°æ’åã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç±»äººAgentä¸€ç›´æ˜¯äººå·¥æ™ºèƒ½è¿½æ±‚çš„ç›®æ ‡ä¹‹ä¸€ã€‚å°½ç®¡å¼ºåŒ–å­¦ä¹ (RL)åœ¨è®¸å¤šé¢†åŸŸå–å¾—äº†è¶…è¶Šäººç±»çš„è¡¨ç°ï¼Œä½†ç›¸å¯¹è¾ƒå°‘å…³æ³¨äºè®¾è®¡ç±»äººRL Agentã€‚å› æ­¤ï¼Œè®¸å¤šå¥–åŠ±é©±åŠ¨çš„RL Agentå¸¸å¸¸è¡¨ç°å‡ºä¸äººç±»ç›¸æ¯”ä¸è‡ªç„¶çš„åŠ¨ä½œï¼Œå¼•å‘äº†å¯¹å¯è§£é‡Šæ€§å’Œå¯ä¿¡åº¦çš„æ‹…å¿§ã€‚ä¸ºäº†åœ¨RLä¸­å®ç°ç±»äººè¡Œä¸ºï¼Œæœ¬æ–‡é¦–å…ˆå°†ç±»äººåº¦å½¢å¼åŒ–ä¸ºè½¨è¿¹ä¼˜åŒ–é—®é¢˜ï¼Œç›®æ ‡æ˜¯æ‰¾åˆ°ä¸€ä¸ªä¸äººç±»è¡Œä¸ºç´§å¯†å¯¹é½åŒæ—¶æœ€å¤§åŒ–å¥–åŠ±çš„åŠ¨ä½œåºåˆ—ï¼Œå¹¶å°†ç»å…¸çš„åé€€è§†é‡æ§åˆ¶åº”ç”¨äºç±»äººå­¦ä¹ ï¼Œä½œä¸ºä¸€ä¸ªæ˜“äºå¤„ç†å’Œé«˜æ•ˆçš„å®ç°ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†å®åŠ¨ä½œé‡åŒ–(MAQ)ï¼Œè¿™æ˜¯ä¸€ä¸ªç±»äººRLæ¡†æ¶ï¼Œé€šè¿‡å‘é‡é‡åŒ–VAEå°†äººç±»æ¼”ç¤ºæç‚¼æˆå®åŠ¨ä½œã€‚åœ¨D4RL AdroitåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMAQæ˜¾è‘—æé«˜äº†ç±»äººåº¦ï¼Œå¢åŠ äº†è½¨è¿¹ç›¸ä¼¼åº¦å¾—åˆ†ï¼Œå¹¶åœ¨äººç±»è¯„ä¼°ç ”ç©¶ä¸­è·å¾—äº†æ‰€æœ‰RL Agentä¸­æœ€é«˜çš„äººç±»ç›¸ä¼¼åº¦æ’åã€‚æˆ‘ä»¬çš„ç»“æœè¿˜è¡¨æ˜ï¼ŒMAQå¯ä»¥å¾ˆå®¹æ˜“åœ°é›†æˆåˆ°å„ç§ç°æˆçš„RLç®—æ³•ä¸­ï¼Œä¸ºå­¦ä¹ ç±»äººRL Agentå¼€è¾Ÿäº†ä¸€ä¸ªæœ‰å¸Œæœ›çš„æ–¹å‘ã€‚ä»£ç å¯åœ¨https://rlg.iis.sinica.edu.tw/papers/MAQè·å¾—ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¼ºåŒ–å­¦ä¹ Agentè¡Œä¸ºä¸äººç±»è¡Œä¸ºå·®å¼‚å¤§çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸åªå…³æ³¨å¥–åŠ±æœ€å¤§åŒ–ï¼Œå¿½ç•¥äº†Agentè¡Œä¸ºçš„è‡ªç„¶æ€§å’Œå¯è§£é‡Šæ€§ï¼Œå¯¼è‡´Agentåœ¨æŸäº›åœºæ™¯ä¸‹è¡¨ç°å‡ºä¸ç¬¦åˆäººç±»ä¹ æƒ¯çš„åŠ¨ä½œï¼Œé™ä½äº†å…¶å¯ä¿¡åº¦ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†ç±»äººè¡Œä¸ºå»ºæ¨¡ä¸ºè½¨è¿¹ä¼˜åŒ–é—®é¢˜ï¼Œå³å¯»æ‰¾ä¸€ä¸ªæ—¢èƒ½è·å¾—é«˜å¥–åŠ±åˆèƒ½ä¸äººç±»è¡Œä¸ºè½¨è¿¹ç›¸ä¼¼çš„åŠ¨ä½œåºåˆ—ã€‚é€šè¿‡å­¦ä¹ äººç±»çš„å®åŠ¨ä½œï¼Œå¹¶å°†å…¶ä½œä¸ºAgentçš„åŠ¨ä½œç©ºé—´ï¼Œå¯ä»¥çº¦æŸAgentçš„è¡Œä¸ºï¼Œä½¿å…¶æ›´æ¥è¿‘äººç±»ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMAQæ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) äººç±»æ¼”ç¤ºæ•°æ®æ”¶é›†ï¼›2) ä½¿ç”¨å‘é‡é‡åŒ–VAE (VQ-VAE) ä»äººç±»æ¼”ç¤ºæ•°æ®ä¸­å­¦ä¹ å®åŠ¨ä½œï¼›3) å°†å­¦ä¹ åˆ°çš„å®åŠ¨ä½œä½œä¸ºå¼ºåŒ–å­¦ä¹ Agentçš„åŠ¨ä½œç©ºé—´ï¼›4) ä½¿ç”¨åé€€è§†é‡æ§åˆ¶(Receding Horizon Control)è¿›è¡Œè½¨è¿¹ä¼˜åŒ–ï¼Œå¹³è¡¡å¥–åŠ±æœ€å¤§åŒ–å’Œä¸äººç±»è¡Œä¸ºçš„ç›¸ä¼¼æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†å®åŠ¨ä½œé‡åŒ–(MAQ)çš„æ¦‚å¿µï¼Œå¹¶å°†å…¶åº”ç”¨äºå¼ºåŒ–å­¦ä¹ ä¸­ã€‚é€šè¿‡VQ-VAEå­¦ä¹ äººç±»çš„å®åŠ¨ä½œï¼Œæœ‰æ•ˆåœ°çº¦æŸäº†Agentçš„åŠ¨ä½œç©ºé—´ï¼Œä½¿å…¶æ›´ç¬¦åˆäººç±»çš„è¡Œä¸ºæ¨¡å¼ã€‚æ­¤å¤–ï¼Œå°†ç±»äººåº¦å½¢å¼åŒ–ä¸ºè½¨è¿¹ä¼˜åŒ–é—®é¢˜ï¼Œå¹¶ä½¿ç”¨åé€€è§†é‡æ§åˆ¶è¿›è¡Œæ±‚è§£ï¼Œä¸ºå­¦ä¹ ç±»äººAgentæä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚

**å…³é”®è®¾è®¡**ï¼šVQ-VAEç”¨äºå­¦ä¹ å®åŠ¨ä½œï¼Œå…¶æŸå¤±å‡½æ•°åŒ…æ‹¬é‡æ„æŸå¤±å’Œé‡åŒ–æŸå¤±ï¼Œç”¨äºä¿è¯å®åŠ¨ä½œçš„è¡¨è¾¾èƒ½åŠ›å’Œç¦»æ•£æ€§ã€‚åé€€è§†é‡æ§åˆ¶çš„ä¼˜åŒ–ç›®æ ‡æ˜¯å¥–åŠ±å’Œè½¨è¿¹ç›¸ä¼¼åº¦çš„åŠ æƒå’Œï¼Œæƒé‡å‚æ•°éœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡è¿›è¡Œè°ƒæ•´ã€‚Agentçš„ç½‘ç»œç»“æ„å¯ä»¥é‡‡ç”¨å„ç§ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œå¦‚DDPGã€SACç­‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMAQåœ¨D4RL AdroitåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†Agentçš„ç±»äººåº¦ï¼Œè½¨è¿¹ç›¸ä¼¼åº¦å¾—åˆ†é«˜äºå…¶ä»–åŸºçº¿æ–¹æ³•ã€‚åœ¨äººç±»è¯„ä¼°ç ”ç©¶ä¸­ï¼ŒMAQè·å¾—äº†æœ€é«˜çš„äººç±»ç›¸ä¼¼åº¦æ’åï¼Œè¡¨æ˜å…¶ç”Ÿæˆçš„è¡Œä¸ºæ›´ç¬¦åˆäººç±»çš„è®¤çŸ¥ã€‚æ­¤å¤–ï¼ŒMAQå¯ä»¥å¾ˆå®¹æ˜“åœ°é›†æˆåˆ°å„ç§ç°æˆçš„RLç®—æ³•ä¸­ï¼Œå…·æœ‰è‰¯å¥½çš„é€šç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººæ§åˆ¶ã€æ¸¸æˆAIã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸï¼Œä½¿Agentçš„è¡Œä¸ºæ›´è‡ªç„¶ã€å¯é¢„æµ‹ï¼Œæé«˜äººæœºäº¤äº’çš„æ•ˆç‡å’Œä¿¡ä»»åº¦ã€‚ä¾‹å¦‚ï¼Œåœ¨åŒ»ç–—æœºå™¨äººé¢†åŸŸï¼Œç±»äººAgentå¯ä»¥æ›´å¥½åœ°è¾…åŠ©åŒ»ç”Ÿè¿›è¡Œæ‰‹æœ¯æ“ä½œï¼Œå‡å°‘æ‚£è€…çš„ææƒ§æ„Ÿã€‚åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸï¼Œç±»äººAgentå¯ä»¥æ›´å¥½åœ°ç†è§£äººç±»é©¾é©¶å‘˜çš„æ„å›¾ï¼Œæé«˜é©¾é©¶å®‰å…¨æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Human-like agents have long been one of the goals in pursuing artificial intelligence. Although reinforcement learning (RL) has achieved superhuman performance in many domains, relatively little attention has been focused on designing human-like RL agents. As a result, many reward-driven RL agents often exhibit unnatural behaviors compared to humans, raising concerns for both interpretability and trustworthiness. To achieve human-like behavior in RL, this paper first formulates human-likeness as trajectory optimization, where the objective is to find an action sequence that closely aligns with human behavior while also maximizing rewards, and adapts the classic receding-horizon control to human-like learning as a tractable and efficient implementation. To achieve this, we introduce Macro Action Quantization (MAQ), a human-like RL framework that distills human demonstrations into macro actions via Vector-Quantized VAE. Experiments on D4RL Adroit benchmarks show that MAQ significantly improves human-likeness, increasing trajectory similarity scores, and achieving the highest human-likeness rankings among all RL agents in the human evaluation study. Our results also demonstrate that MAQ can be easily integrated into various off-the-shelf RL algorithms, opening a promising direction for learning human-like RL agents. Our code is available at https://rlg.iis.sinica.edu.tw/papers/MAQ.

