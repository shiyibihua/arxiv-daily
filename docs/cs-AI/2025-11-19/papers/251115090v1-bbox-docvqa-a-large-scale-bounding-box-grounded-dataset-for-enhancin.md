---
layout: default
title: BBox DocVQA: A Large Scale Bounding Box Grounded Dataset for Enhancing Reasoning in Document Visual Question Answer
---

# BBox DocVQA: A Large Scale Bounding Box Grounded Dataset for Enhancing Reasoning in Document Visual Question Answer

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.15090" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.15090v1</a>
  <a href="https://arxiv.org/pdf/2511.15090.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.15090v1" onclick="toggleFavorite(this, '2511.15090v1', 'BBox DocVQA: A Large Scale Bounding Box Grounded Dataset for Enhancing Reasoning in Document Visual Question Answer')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Wenhan Yu, Wang Chen, Guanqiang Qi, Weikang Li, Yang Li, Lei Sha, Deguo Xia, Jizhou Huang

**åˆ†ç±»**: cs.DB, cs.AI, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-19

**å¤‡æ³¨**: 22 pages, 4 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºBBox DocVQAæ•°æ®é›†ï¼Œå¢å¼ºæ–‡æ¡£è§†è§‰é—®ç­”ä¸­ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `æ–‡æ¡£è§†è§‰é—®ç­”` `è§†è§‰è¯­è¨€æ¨ç†` `ç©ºé—´æ¨ç†` `è¾¹ç•Œæ¡†æ ‡æ³¨` `å¤šæ¨¡æ€å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰DocVQAæ•°æ®é›†ç¼ºä¹ç»†ç²’åº¦çš„ç©ºé—´å®šä½ï¼Œé™åˆ¶äº†è§†è§‰è¯­è¨€æ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œç©ºé—´æ¨ç†èƒ½åŠ›ã€‚
2. æå‡ºBBox DocVQAæ•°æ®é›†ï¼Œé€šè¿‡æ˜¾å¼è¾¹ç•Œæ¡†æ ‡æ³¨QAå¯¹ï¼Œå¢å¼ºæ¨¡å‹åœ¨è§†è§‰æ–‡æ¡£ä¸­çš„ç©ºé—´æ¨ç†å’Œè¯æ®å®šä½èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼Œåœ¨BBox DocVQAä¸Šå¾®è°ƒå¯ä»¥æ˜¾è‘—æé«˜è¾¹ç•Œæ¡†å®šä½å’Œç­”æ¡ˆç”Ÿæˆï¼ŒéªŒè¯äº†æ•°æ®é›†çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ–‡æ¡£è§†è§‰é—®ç­”(DocVQA)æ˜¯å¤šæ¨¡æ€æ–‡æ¡£ç†è§£çš„åŸºç¡€ä»»åŠ¡ï¼Œä¹Ÿæ˜¯è§†è§‰è¯­è¨€æ¨ç†çš„å…³é”®æµ‹è¯•å¹³å°ã€‚ç„¶è€Œï¼Œç°æœ‰DocVQAæ•°æ®é›†å¤§å¤šå±€é™äºé¡µé¢çº§åˆ«ï¼Œç¼ºä¹ç»†ç²’åº¦çš„ç©ºé—´å®šä½ï¼Œé™åˆ¶äº†è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)çš„å¯è§£é‡Šæ€§å’Œæ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†BBox DocVQAï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„ã€åŸºäºè¾¹ç•Œæ¡†çš„æ•°æ®é›†ï¼Œæ—¨åœ¨å¢å¼ºè§†è§‰æ–‡æ¡£ä¸­çš„ç©ºé—´æ¨ç†å’Œè¯æ®å®šä½ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸€ä¸ªè‡ªåŠ¨æ„å»ºæµç¨‹ï¼Œå³â€œåˆ†å‰²ã€åˆ¤æ–­å’Œç”Ÿæˆâ€ï¼Œè¯¥æµç¨‹é›†æˆäº†ç”¨äºåŒºåŸŸåˆ†å‰²çš„åˆ†å‰²æ¨¡å‹ã€ç”¨äºè¯­ä¹‰åˆ¤æ–­çš„VLMå’Œç”¨äºé—®é¢˜ç­”æ¡ˆç”Ÿæˆçš„å¦ä¸€ä¸ªé«˜çº§VLMï¼Œç„¶åè¿›è¡Œäººå·¥éªŒè¯ä»¥ç¡®ä¿è´¨é‡ã€‚æœ€ç»ˆæ•°æ®é›†åŒ…å«3.6Kä¸ªä¸åŒçš„æ–‡æ¡£å’Œ32Kä¸ªQAå¯¹ï¼Œæ¶µç›–å•åŒºåŸŸå’Œå¤šåŒºåŸŸä»¥åŠå•é¡µå’Œå¤šé¡µåœºæ™¯ã€‚æ¯ä¸ªQAå®ä¾‹éƒ½åŸºäºæ˜¾å¼çš„è¾¹ç•Œæ¡†ï¼Œä»è€Œèƒ½å¤Ÿå¯¹ç©ºé—´è¯­ä¹‰å¯¹é½è¿›è¡Œç»†ç²’åº¦è¯„ä¼°ã€‚åœ¨BBox DocVQAä¸Šå¯¹å¤šä¸ªæœ€å…ˆè¿›çš„VLMï¼ˆä¾‹å¦‚ï¼ŒGPT 5ã€Qwen2.5 VLå’ŒInternVLï¼‰è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œæ­ç¤ºäº†ç©ºé—´å®šä½å’Œæ¨ç†å‡†ç¡®æ€§æ–¹é¢æŒç»­å­˜åœ¨çš„æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œåœ¨BBox DocVQAä¸Šè¿›è¡Œå¾®è°ƒå¯ä»¥æ˜¾è‘—æé«˜è¾¹ç•Œæ¡†å®šä½å’Œç­”æ¡ˆç”Ÿæˆï¼Œä»è€ŒéªŒè¯äº†å…¶å¢å¼ºVLMæ¨ç†èƒ½åŠ›çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç å°†å…¬å¼€å‘å¸ƒï¼Œä»¥æ¨è¿›å¯è§£é‡Šå’Œç©ºé—´å®šä½çš„è§†è§‰è¯­è¨€æ¨ç†ç ”ç©¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰DocVQAæ•°æ®é›†ä¸»è¦å…³æ³¨é¡µé¢çº§åˆ«çš„é—®ç­”ï¼Œç¼ºä¹ç»†ç²’åº¦çš„ç©ºé—´ä¿¡æ¯ï¼Œå¯¼è‡´æ¨¡å‹éš¾ä»¥è¿›è¡Œç²¾ç¡®çš„ç©ºé—´æ¨ç†å’Œå®šä½ã€‚è¿™é™åˆ¶äº†æ¨¡å‹åœ¨å¤æ‚æ–‡æ¡£åœºæ™¯ä¸‹çš„åº”ç”¨ï¼Œä¾‹å¦‚éœ€è¦å®šä½ç‰¹å®šè¡¨æ ¼å•å…ƒæ ¼æˆ–å›¾åƒåŒºåŸŸæ‰èƒ½å›ç­”é—®é¢˜çš„æƒ…å†µã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥æä¾›å¯è§£é‡Šçš„æ¨ç†è¿‡ç¨‹ï¼Œæ— æ³•æ˜ç¡®æŒ‡å‡ºç­”æ¡ˆçš„ä¾æ®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šBBox DocVQAçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥è¾¹ç•Œæ¡†æ ‡æ³¨ï¼Œå°†QAå¯¹ä¸æ–‡æ¡£ä¸­çš„ç‰¹å®šåŒºåŸŸè¿›è¡Œå…³è”ï¼Œä»è€Œæ˜¾å¼åœ°æä¾›ç©ºé—´ä¿¡æ¯ã€‚è¿™ç§æ–¹å¼èƒ½å¤Ÿè¿«ä½¿æ¨¡å‹å­¦ä¹ ç©ºé—´è¯­ä¹‰å¯¹é½ï¼Œå¹¶æé«˜æ¨¡å‹åœ¨ç©ºé—´æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚é€šè¿‡ç»†ç²’åº¦çš„æ ‡æ³¨ï¼Œæ¨¡å‹å¯ä»¥æ›´å¥½åœ°ç†è§£é—®é¢˜ä¸æ–‡æ¡£åŒºåŸŸä¹‹é—´çš„å…³ç³»ï¼Œä»è€Œæé«˜ç­”æ¡ˆçš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šBBox DocVQAçš„æ„å»ºæµç¨‹ä¸»è¦åŒ…å«ä¸‰ä¸ªé˜¶æ®µï¼šåˆ†å‰²ï¼ˆSegmentï¼‰ã€åˆ¤æ–­ï¼ˆJudgeï¼‰å’Œç”Ÿæˆï¼ˆGenerateï¼‰ã€‚é¦–å…ˆï¼Œä½¿ç”¨åˆ†å‰²æ¨¡å‹å°†æ–‡æ¡£åˆ†å‰²æˆä¸åŒçš„åŒºåŸŸã€‚ç„¶åï¼Œä½¿ç”¨VLMå¯¹åˆ†å‰²åçš„åŒºåŸŸè¿›è¡Œè¯­ä¹‰åˆ¤æ–­ï¼Œç­›é€‰å‡ºæœ‰æ„ä¹‰çš„åŒºåŸŸã€‚æœ€åï¼Œä½¿ç”¨å¦ä¸€ä¸ªVLMåŸºäºè¿™äº›åŒºåŸŸç”Ÿæˆé—®é¢˜å’Œç­”æ¡ˆï¼Œå¹¶ä½¿ç”¨äººå·¥è¿›è¡ŒéªŒè¯å’Œè´¨é‡ä¿è¯ã€‚è¿™ä¸ªæµç¨‹æ—¨åœ¨è‡ªåŠ¨åŒ–åœ°ç”Ÿæˆé«˜è´¨é‡çš„ã€å¸¦æœ‰è¾¹ç•Œæ¡†æ ‡æ³¨çš„QAå¯¹ã€‚

**å…³é”®åˆ›æ–°**ï¼šBBox DocVQAçš„å…³é”®åˆ›æ–°åœ¨äºå…¶å¤§è§„æ¨¡çš„ã€åŸºäºè¾¹ç•Œæ¡†çš„æ ‡æ³¨æ–¹å¼ã€‚ä¸ç°æœ‰çš„DocVQAæ•°æ®é›†ç›¸æ¯”ï¼ŒBBox DocVQAæä¾›äº†æ›´ç»†ç²’åº¦çš„ç©ºé—´ä¿¡æ¯ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°å­¦ä¹ ç©ºé—´è¯­ä¹‰å¯¹é½ã€‚æ­¤å¤–ï¼Œè‡ªåŠ¨æ„å»ºæµç¨‹ä¹Ÿé™ä½äº†æ•°æ®é›†æ„å»ºçš„æˆæœ¬ï¼Œä½¿å¾—å¯ä»¥æ„å»ºæ›´å¤§è§„æ¨¡çš„æ•°æ®é›†ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è‡ªåŠ¨æ„å»ºæµç¨‹ä¸­ï¼Œé€‰æ‹©åˆé€‚çš„åˆ†å‰²æ¨¡å‹ã€è¯­ä¹‰åˆ¤æ–­VLMå’Œé—®é¢˜ç­”æ¡ˆç”ŸæˆVLMè‡³å…³é‡è¦ã€‚è®ºæ–‡ä¸­ä½¿ç”¨äº†å…ˆè¿›çš„VLMæ¨¡å‹ï¼Œå¹¶è¿›è¡Œäº†äººå·¥éªŒè¯ä»¥ç¡®ä¿æ•°æ®é›†çš„è´¨é‡ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚å¯èƒ½å› æ‰€ä½¿ç”¨çš„VLMæ¨¡å‹è€Œå¼‚ï¼Œä½†æ€»ä½“ç›®æ ‡æ˜¯ç”Ÿæˆé«˜è´¨é‡çš„ã€ä¸æ–‡æ¡£åŒºåŸŸç›¸å…³çš„QAå¯¹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨BBox DocVQAä¸Šå¯¹GPT 5ã€Qwen2.5 VLå’ŒInternVLç­‰å¤šä¸ªæœ€å…ˆè¿›çš„VLMè¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œæ­ç¤ºäº†æ¨¡å‹åœ¨ç©ºé—´å®šä½å’Œæ¨ç†å‡†ç¡®æ€§æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚åœ¨BBox DocVQAä¸Šè¿›è¡Œå¾®è°ƒåï¼Œæ¨¡å‹çš„è¾¹ç•Œæ¡†å®šä½å’Œç­”æ¡ˆç”Ÿæˆèƒ½åŠ›å‡å¾—åˆ°æ˜¾è‘—æå‡ï¼ŒéªŒè¯äº†è¯¥æ•°æ®é›†å¯¹äºå¢å¼ºVLMæ¨ç†èƒ½åŠ›çš„æœ‰æ•ˆæ€§ã€‚å…·ä½“æå‡å¹…åº¦æœªçŸ¥ï¼Œéœ€è¦åœ¨è®ºæ–‡ä¸­æŸ¥æ‰¾ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

BBox DocVQAæ•°æ®é›†å¯åº”ç”¨äºå„ç§æ–‡æ¡£ç†è§£å’Œè§†è§‰è¯­è¨€æ¨ç†ä»»åŠ¡ï¼Œä¾‹å¦‚æ™ºèƒ½æ–‡æ¡£å¤„ç†ã€ä¿¡æ¯æŠ½å–ã€æ™ºèƒ½å®¢æœç­‰ã€‚é€šè¿‡å¢å¼ºæ¨¡å‹åœ¨ç©ºé—´æ¨ç†æ–¹é¢çš„èƒ½åŠ›ï¼Œå¯ä»¥æé«˜æ–‡æ¡£å¤„ç†çš„è‡ªåŠ¨åŒ–ç¨‹åº¦å’Œå‡†ç¡®æ€§ï¼Œä»è€Œæé«˜å·¥ä½œæ•ˆç‡å¹¶é™ä½æˆæœ¬ã€‚è¯¥æ•°æ®é›†è¿˜æœ‰åŠ©äºå¼€å‘æ›´å¯è§£é‡Šçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œæå‡ç”¨æˆ·å¯¹æ¨¡å‹å†³ç­–çš„ä¿¡ä»»åº¦ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Document Visual Question Answering (DocVQA) is a fundamental task for multimodal document understanding and a key testbed for vision language reasoning. However, most existing DocVQA datasets are limited to the page level and lack fine grained spatial grounding, constraining the interpretability and reasoning capability of Vision Language Models (VLMs). To address this gap, we introduce BBox DocVQA a large scale, bounding box grounded dataset designed to enhance spatial reasoning and evidence localization in visual documents. We further present an automated construction pipeline, Segment Judge and Generate, which integrates a segment model for region segmentation, a VLM for semantic judgment, and another advanced VLM for question answer generation, followed by human verification for quality assurance. The resulting dataset contains 3.6 K diverse documents and 32 K QA pairs, encompassing single and multi region as well as single and multi page scenarios. Each QA instance is grounded on explicit bounding boxes, enabling fine grained evaluation of spatial semantic alignment. Benchmarking multiple state of the art VLMs (e.g., GPT 5, Qwen2.5 VL, and InternVL) on BBox DocVQA reveals persistent challenges in spatial grounding and reasoning accuracy. Furthermore, fine tuning on BBox DocVQA substantially improves both bounding box localization and answer generation, validating its effectiveness for enhancing the reasoning ability of VLMs. Our dataset and code will be publicly released to advance research on interpretable and spatially grounded vision language reasoning.

