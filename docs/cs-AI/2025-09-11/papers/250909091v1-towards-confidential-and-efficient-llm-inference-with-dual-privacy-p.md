---
layout: default
title: Towards Confidential and Efficient LLM Inference with Dual Privacy Protection
---

# Towards Confidential and Efficient LLM Inference with Dual Privacy Protection

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.09091" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.09091v1</a>
  <a href="https://arxiv.org/pdf/2509.09091.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.09091v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.09091v1', 'Towards Confidential and Efficient LLM Inference with Dual Privacy Protection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Honglan Yu, Yibin Wang, Feifei Dai, Dong Liu, Haihui Fan, Xiaoyan Gu

**åˆ†ç±»**: cs.CR, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-11

**å¤‡æ³¨**: Accepted by DASFAA2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**CMIFï¼šé¢å‘LLMæ¨ç†çš„åŒé‡éšç§ä¿æŠ¤æ¡†æ¶ï¼Œå…¼é¡¾æ•ˆç‡ä¸å®‰å…¨æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `éšç§ä¿æŠ¤` `å¯ä¿¡æ‰§è¡Œç¯å¢ƒ` `å·®åˆ†éšç§` `å®‰å…¨æ¨ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºTEEçš„LLMæ¨ç†æ–¹æ¡ˆå­˜åœ¨é«˜å»¶è¿Ÿé—®é¢˜ï¼Œè€Œå¸è½½éƒ¨åˆ†è®¡ç®—åˆ°GPUåˆå¼•å…¥äº†å·¨å¤§çš„é€šä¿¡å¼€é”€ã€‚
2. CMIFæ¡†æ¶çš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†LLMçš„åµŒå…¥å±‚éƒ¨ç½²åœ¨å®¢æˆ·ç«¯TEEä¸­ï¼Œåç»­å±‚éƒ¨ç½²åœ¨GPUæœåŠ¡å™¨ä¸Šï¼Œä»è€Œå¹³è¡¡éšç§ä¿æŠ¤å’Œæ¨ç†æ•ˆç‡ã€‚
3. CMIFè¿˜ä¼˜åŒ–äº†Report-Noisy-Maxæœºåˆ¶ï¼Œåœ¨ä¿æŠ¤æ•æ„Ÿè¾“å…¥çš„åŒæ—¶ï¼Œå°½å¯èƒ½å‡å°å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œå®éªŒè¡¨æ˜æœ‰æ•ˆã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºCMIFçš„ä¿å¯†ä¸”é«˜æ•ˆçš„æ¨¡å‹æ¨ç†æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³åŸºäºCPUçš„å¯ä¿¡æ‰§è¡Œç¯å¢ƒï¼ˆTEEï¼‰å’Œå·®åˆ†éšç§ï¼ˆDPï¼‰åœ¨ç§æœ‰æ¨ç†ä¸­é¢ä¸´çš„æŒ‘æˆ˜ã€‚TEEè™½ç„¶èƒ½ä¿æŠ¤éšç§ï¼Œä½†æ¨ç†å»¶è¿Ÿé«˜ï¼Œè€Œå°†çº¿æ€§æ¨¡å‹ç»„ä»¶å¸è½½åˆ°GPUçš„æ–¹æ¡ˆåˆä¼šå› å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¯†é›†éçº¿æ€§å±‚å¯¼è‡´TEEå’ŒGPUä¹‹é—´äº§ç”Ÿæ˜¾è‘—çš„é€šä¿¡å¼€é”€ã€‚åŸºäºDPçš„æ–¹æ³•è™½ç„¶èƒ½é€šè¿‡æ·»åŠ éšæœºå™ªå£°æ¥ä¿æŠ¤æ•°æ®éšç§ï¼Œä½†ä¼šç‰ºç‰²LLMçš„æ€§èƒ½å’Œè¯­ä¹‰ç†è§£èƒ½åŠ›ã€‚CMIFé€šè¿‡åœ¨å®¢æˆ·ç«¯TEEä¸­å®‰å…¨åœ°éƒ¨ç½²åµŒå…¥å±‚ï¼Œå¹¶åœ¨GPUæœåŠ¡å™¨ä¸Šéƒ¨ç½²åç»­å±‚æ¥å…‹æœä¸Šè¿°ç¼ºç‚¹ã€‚åŒæ—¶ï¼ŒCMIFä¼˜åŒ–äº†Report-Noisy-Maxæœºåˆ¶ï¼Œä»¥ä¿æŠ¤æ•æ„Ÿè¾“å…¥ï¼Œä¸”æ¨¡å‹æ€§èƒ½ä»…ç•¥æœ‰ä¸‹é™ã€‚å¯¹Llamaç³»åˆ—æ¨¡å‹çš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒCMIFåœ¨ä¿æŠ¤ç”¨æˆ·æ•°æ®éšç§çš„åŒæ—¶ï¼Œå‡å°‘äº†TEEä¸­çš„é¢å¤–æ¨ç†å¼€é”€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºTEEçš„LLMç§æœ‰æ¨ç†æ–¹æ¡ˆï¼Œç”±äºTEEå†…éƒ¨è®¡ç®—å¼€é”€å¤§ï¼Œå¯¼è‡´æ¨ç†å»¶è¿Ÿè¿‡é«˜ã€‚å°†éƒ¨åˆ†è®¡ç®—ï¼ˆå¦‚çº¿æ€§å±‚ï¼‰å¸è½½åˆ°GPUè™½ç„¶å¯ä»¥åŠ é€Ÿæ¨ç†ï¼Œä½†LLMä¸­å¤§é‡çš„éçº¿æ€§å±‚å¯¼è‡´TEEå’ŒGPUä¹‹é—´é¢‘ç¹çš„æ•°æ®ä¼ è¾“ï¼Œå¼•å…¥äº†æ˜¾è‘—çš„é€šä¿¡å¼€é”€ï¼Œæˆä¸ºæ–°çš„æ€§èƒ½ç“¶é¢ˆã€‚æ­¤å¤–ï¼ŒåŸºäºå·®åˆ†éšç§çš„æ–¹æ¡ˆè™½ç„¶èƒ½ä¿æŠ¤éšç§ï¼Œä½†ä¼šä¸¥é‡å½±å“LLMçš„æ€§èƒ½å’Œè¯­ä¹‰ç†è§£èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCMIFçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆç†åˆ’åˆ†LLMçš„è®¡ç®—ä»»åŠ¡ï¼Œå°†å¯¹éšç§è¦æ±‚æœ€é«˜çš„åµŒå…¥å±‚éƒ¨ç½²åœ¨å®¢æˆ·ç«¯çš„TEEä¸­ï¼Œåˆ©ç”¨TEEçš„å®‰å…¨æ€§ä¿æŠ¤ç”¨æˆ·è¾“å…¥ã€‚è€Œå°†è®¡ç®—é‡å¤§çš„åç»­å±‚éƒ¨ç½²åœ¨GPUæœåŠ¡å™¨ä¸Šï¼Œåˆ©ç”¨GPUçš„å¹¶è¡Œè®¡ç®—èƒ½åŠ›åŠ é€Ÿæ¨ç†ã€‚åŒæ—¶ï¼Œé’ˆå¯¹æ•æ„Ÿè¾“å…¥ï¼Œä¼˜åŒ–Report-Noisy-Maxæœºåˆ¶ï¼Œåœ¨ä¿è¯éšç§çš„å‰æä¸‹ï¼Œå°½å¯èƒ½å‡å°‘å™ªå£°å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCMIFæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) å®¢æˆ·ç«¯TEEï¼šè´Ÿè´£å­˜å‚¨å’Œæ‰§è¡ŒLLMçš„åµŒå…¥å±‚ï¼Œæ¥æ”¶ç”¨æˆ·è¾“å…¥ï¼Œå¹¶å¯¹è¾“å…¥è¿›è¡ŒåµŒå…¥æ“ä½œã€‚2) GPUæœåŠ¡å™¨ï¼šè´Ÿè´£æ‰§è¡ŒLLMçš„åç»­å±‚ï¼Œæ¥æ”¶æ¥è‡ªTEEçš„åµŒå…¥å‘é‡ï¼Œè¿›è¡Œæ¨ç†è®¡ç®—ï¼Œå¹¶å°†ç»“æœè¿”å›ç»™å®¢æˆ·ç«¯ã€‚3) Report-Noisy-Maxæœºåˆ¶ï¼šç”¨äºä¿æŠ¤æ•æ„Ÿè¾“å…¥ï¼Œåœ¨åµŒå…¥å‘é‡ä¸­æ·»åŠ å™ªå£°ï¼Œé˜²æ­¢æ”»å‡»è€…é€šè¿‡åˆ†æåµŒå…¥å‘é‡æ¨æ–­å‡ºç”¨æˆ·çš„æ•æ„Ÿä¿¡æ¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šCMIFçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†TEEå’ŒGPUæ··åˆéƒ¨ç½²çš„LLMæ¨ç†æ¡†æ¶ï¼Œå¹³è¡¡äº†éšç§ä¿æŠ¤å’Œæ¨ç†æ•ˆç‡ã€‚2) é’ˆå¯¹LLMçš„ç‰¹ç‚¹ï¼Œä¼˜åŒ–äº†Report-Noisy-Maxæœºåˆ¶ï¼Œåœ¨ä¿è¯éšç§çš„å‰æä¸‹ï¼Œå°½å¯èƒ½å‡å°‘å™ªå£°å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚3) é€šè¿‡å°†åµŒå…¥å±‚éƒ¨ç½²åœ¨å®¢æˆ·ç«¯TEEä¸­ï¼Œæœ‰æ•ˆå‡å°‘äº†TEEå’ŒGPUä¹‹é—´çš„æ•°æ®ä¼ è¾“é‡ï¼Œé™ä½äº†é€šä¿¡å¼€é”€ã€‚

**å…³é”®è®¾è®¡**ï¼šCMIFçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) åµŒå…¥å±‚çš„ä½ç½®é€‰æ‹©ï¼šå°†åµŒå…¥å±‚éƒ¨ç½²åœ¨å®¢æˆ·ç«¯TEEä¸­ï¼Œå¯ä»¥æœ‰æ•ˆä¿æŠ¤ç”¨æˆ·è¾“å…¥ï¼Œé˜²æ­¢æ•æ„Ÿä¿¡æ¯æ³„éœ²ã€‚2) Report-Noisy-Maxæœºåˆ¶çš„ä¼˜åŒ–ï¼šé€šè¿‡è°ƒæ•´å™ªå£°çš„å°ºåº¦å’Œæ·»åŠ æ–¹å¼ï¼Œå¯ä»¥åœ¨ä¿è¯éšç§çš„å‰æä¸‹ï¼Œå°½å¯èƒ½å‡å°‘å™ªå£°å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚å…·ä½“çš„å™ªå£°æ·»åŠ ç­–ç•¥å’Œå‚æ•°è®¾ç½®éœ€è¦æ ¹æ®å…·ä½“çš„LLMå’Œåº”ç”¨åœºæ™¯è¿›è¡Œè°ƒæ•´ã€‚3) TEEå’ŒGPUä¹‹é—´çš„æ•°æ®ä¼ è¾“ä¼˜åŒ–ï¼šé‡‡ç”¨é«˜æ•ˆçš„æ•°æ®ä¼ è¾“åè®®å’Œå‹ç¼©ç®—æ³•ï¼Œå‡å°‘æ•°æ®ä¼ è¾“é‡ï¼Œé™ä½é€šä¿¡å¼€é”€ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒCMIFæ¡†æ¶åœ¨Llamaç³»åˆ—æ¨¡å‹ä¸Šèƒ½å¤Ÿæœ‰æ•ˆé™ä½TEEä¸­çš„é¢å¤–æ¨ç†å¼€é”€ï¼ŒåŒæ—¶ä¿æŠ¤ç”¨æˆ·æ•°æ®éšç§ã€‚å…·ä½“è€Œè¨€ï¼Œä¸å®Œå…¨åœ¨TEEä¸­è¿›è¡Œæ¨ç†ç›¸æ¯”ï¼ŒCMIFèƒ½å¤Ÿæ˜¾è‘—é™ä½æ¨ç†å»¶è¿Ÿï¼ŒåŒæ—¶æ¨¡å‹æ€§èƒ½ä»…æœ‰è½»å¾®ä¸‹é™ã€‚åœ¨ä¿è¯ä¸€å®šéšç§é¢„ç®—ä¸‹ï¼ŒCMIFçš„æ€§èƒ½ä¼˜äºç›´æ¥åº”ç”¨å·®åˆ†éšç§çš„æ–¹æ¡ˆã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

CMIFæ¡†æ¶å¯åº”ç”¨äºå„ç§éœ€è¦ä¿æŠ¤ç”¨æˆ·éšç§çš„LLMåº”ç”¨åœºæ™¯ï¼Œä¾‹å¦‚ï¼šåŒ»ç–—è¯Šæ–­ã€é‡‘èé£æ§ã€æ³•å¾‹å’¨è¯¢ç­‰ã€‚é€šè¿‡CMIFï¼Œç”¨æˆ·å¯ä»¥åœ¨äº«å—LLMå¼ºå¤§åŠŸèƒ½çš„åŒæ—¶ï¼Œä¸å¿…æ‹…å¿ƒä¸ªäººæ•æ„Ÿä¿¡æ¯æ³„éœ²çš„é£é™©ã€‚è¯¥ç ”ç©¶æœ‰åŠ©äºæ¨åŠ¨LLMåœ¨éšç§æ•æ„Ÿé¢†åŸŸçš„åº”ç”¨ï¼Œä¿ƒè¿›äººå·¥æ™ºèƒ½æŠ€æœ¯çš„å¯ä¿¡å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> CPU-based trusted execution environments (TEEs) and differential privacy (DP) have gained wide applications for private inference. Due to high inference latency in TEEs, researchers use partition-based approaches that offload linear model components to GPUs. However, dense nonlinear layers of large language models (LLMs) result in significant communication overhead between TEEs and GPUs. DP-based approaches apply random noise to protect data privacy, but this compromises LLM performance and semantic understanding. To overcome the above drawbacks, this paper proposes CMIF, a Confidential and efficient Model Inference Framework. CMIF confidentially deploys the embedding layer in the client-side TEE and subsequent layers on GPU servers. Meanwhile, it optimizes the Report-Noisy-Max mechanism to protect sensitive inputs with a slight decrease in model performance. Extensive experiments on Llama-series models demonstrate that CMIF reduces additional inference overhead in TEEs while preserving user data privacy.

