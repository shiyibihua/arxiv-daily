---
layout: default
title: Tree-OPO: Off-policy Monte Carlo Tree-Guided Advantage Optimization for Multistep Reasoning
---

# Tree-OPO: Off-policy Monte Carlo Tree-Guided Advantage Optimization for Multistep Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.09284" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.09284v2</a>
  <a href="https://arxiv.org/pdf/2509.09284.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.09284v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.09284v2', 'Tree-OPO: Off-policy Monte Carlo Tree-Guided Advantage Optimization for Multistep Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Bingning Huang, Tu Nguyen, Matthieu Zimmer

**åˆ†ç±»**: cs.AI, cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-11 (æ›´æ–°: 2025-11-11)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**Tree-OPOï¼šåˆ©ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢å¼•å¯¼çš„ä¼˜åŠ¿ä¼˜åŒ–å¤šæ­¥æ¨ç†**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è’™ç‰¹å¡æ´›æ ‘æœç´¢` `å¼ºåŒ–å­¦ä¹ ` `ç­–ç•¥ä¼˜åŒ–` `ä¼˜åŠ¿ä¼°è®¡` `å¤šæ­¥æ¨ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨åˆ©ç”¨MCTSè½¨è¿¹è¿›è¡Œç­–ç•¥ä¼˜åŒ–æ—¶ï¼Œéš¾ä»¥å¤„ç†ä¸åŒå‰ç¼€å¸¦æ¥çš„é¢„æœŸå›æŠ¥å·®å¼‚ï¼Œå¯¼è‡´ä¼˜åŠ¿ä¼°è®¡åå·®ã€‚
2. è®ºæ–‡æå‡ºåˆ†é˜¶æ®µä¼˜åŠ¿ä¼°è®¡ï¼ˆSAEï¼‰æ¡†æ¶ï¼Œé€šè¿‡çº¦æŸä¼˜åŒ–ï¼Œè®¡ç®—ä½æ–¹å·®ä¸”å‰ç¼€æ„ŸçŸ¥çš„ä¼˜åŠ¿ï¼Œä»è€Œæå‡ç­–ç•¥å­¦ä¹ æ•ˆæœã€‚
3. å®éªŒè¡¨æ˜ï¼ŒSAEåœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­æ˜¾è‘—æå‡äº†ç­–ç•¥çš„å‡†ç¡®ç‡ï¼Œå¹¶ä»ç†è®ºä¸Šè¯æ˜äº†å…¶é™ä½æ¢¯åº¦æ–¹å·®çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æ¢ç´¢äº†å¦‚ä½•å°†è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ç”Ÿæˆçš„é«˜è´¨é‡ä¸­é—´è½¨è¿¹ç”¨äºæ”¹è¿›åŸºäºåå¥½çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­çš„ç­–ç•¥ä¼˜åŒ–ï¼Œè¿™äº›è½¨è¿¹ä¼ ç»Ÿä¸Šç”¨äºè®­ç»ƒä»·å€¼æˆ–å¥–åŠ±æ¨¡å‹ã€‚å…·ä½“è€Œè¨€ï¼Œä½œè€…èšç„¦äºGroup Relative Policy Optimization (GRPO)ç®—æ³•ï¼Œè¯¥ç®—æ³•æ— éœ€ä»·å€¼ç½‘ç»œå³å¯å®ç°åå¥½ä¸€è‡´çš„ç­–ç•¥å­¦ä¹ ã€‚è®ºæ–‡å°†GRPOé‡æ„ä¸ºåˆ†é˜¶æ®µè®­ç»ƒèŒƒå¼ï¼Œåˆ©ç”¨æ•™å¸ˆæ¨¡å‹çš„MCTS rolloutæ„å»ºæ ‘çŠ¶ç»“æ„çš„è¯¾ç¨‹å‰ç¼€ã€‚ç”±æ­¤å¼•å…¥äº†ä¸ºæºè‡ªä¸åŒå‰ç¼€çš„è®­ç»ƒæ ·æœ¬è®¡ç®—ä¼˜åŠ¿çš„æ–°æŒ‘æˆ˜ï¼Œå› ä¸ºæ¯ä¸ªå‰ç¼€éƒ½æœ‰ä¸åŒçš„é¢„æœŸå›æŠ¥ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½œè€…æå‡ºäº†åˆ†é˜¶æ®µä¼˜åŠ¿ä¼°è®¡ï¼ˆSAEï¼‰æ¡†æ¶ï¼Œé€šè¿‡å°†å¥–åŠ±æŠ•å½±åˆ°å°Šé‡æ ‘ç»“æ„çš„çº¦æŸé›†ä¸Šæ¥è®¡ç®—ä½æ–¹å·®ã€å‰ç¼€æ„ŸçŸ¥çš„ä¼˜åŠ¿ã€‚åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒSAEæé«˜äº†æœ€ç»ˆå‡†ç¡®ç‡ï¼Œä¼˜äºæ ‡å‡†GRPOã€‚ç†è®ºåˆ†æè¯å®SAEé™ä½äº†æ¢¯åº¦æ–¹å·®ï¼Œä»è€Œæé«˜äº†æ ·æœ¬æ•ˆç‡ã€‚ä½œè€…é€šè¿‡SAEçš„å®é™…å®ç°è¯æ˜äº†è¿™ä¸€ç‚¹ï¼Œæ¯”è¾ƒäº†é«˜æ•ˆçš„å¯å‘å¼æ–¹æ³•å’Œå½¢å¼åŒ–çš„äºŒæ¬¡è§„åˆ’ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³åœ¨åŸºäºåå¥½çš„å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œå¦‚ä½•æœ‰æ•ˆåˆ©ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ç”Ÿæˆçš„è½¨è¿¹æ¥ä¼˜åŒ–ç­–ç•¥ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šæ­¥æ¨ç†ä»»åŠ¡ä¸­ã€‚ç°æœ‰æ–¹æ³•ï¼Œä¾‹å¦‚ç›´æ¥ä½¿ç”¨MCTSè½¨è¿¹è®­ç»ƒä»·å€¼å‡½æ•°æˆ–å¥–åŠ±æ¨¡å‹ï¼Œç„¶åç”¨äºç­–ç•¥ä¼˜åŒ–ï¼Œå¿½ç•¥äº†MCTSè½¨è¿¹ä¸­ä¸åŒå‰ç¼€ï¼ˆprefixï¼‰å¸¦æ¥çš„é¢„æœŸå›æŠ¥å·®å¼‚ï¼Œå¯¼è‡´ä¼˜åŠ¿å‡½æ•°ä¼°è®¡ä¸å‡†ç¡®ï¼Œè¿›è€Œå½±å“ç­–ç•¥å­¦ä¹ çš„æ•ˆç‡å’Œæ•ˆæœã€‚GRPOè™½ç„¶é¿å…äº†ä»·å€¼ç½‘ç»œçš„è®­ç»ƒï¼Œä½†ä»ç„¶é¢ä¸´å¦‚ä½•æœ‰æ•ˆåˆ©ç”¨MCTSè½¨è¿¹è¿›è¡Œç­–ç•¥ä¼˜åŒ–çš„æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†MCTSç”Ÿæˆçš„è½¨è¿¹è§†ä¸ºä¸€ç§è¯¾ç¨‹ï¼ˆcurriculumï¼‰ï¼Œå¹¶å°†å…¶ç»„ç»‡æˆæ ‘çŠ¶ç»“æ„ï¼Œæ¯ä¸ªèŠ‚ç‚¹ä»£è¡¨ä¸€ä¸ªå‰ç¼€ã€‚ç„¶åï¼Œé€šè¿‡ä¸€ç§æ–°çš„ä¼˜åŠ¿ä¼°è®¡æ–¹æ³•â€”â€”åˆ†é˜¶æ®µä¼˜åŠ¿ä¼°è®¡ï¼ˆSAEï¼‰ï¼Œæ¥è§£å†³ä¸åŒå‰ç¼€å¸¦æ¥çš„é¢„æœŸå›æŠ¥å·®å¼‚é—®é¢˜ã€‚SAEçš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†å¥–åŠ±æŠ•å½±åˆ°ä¸€ä¸ªçº¦æŸé›†ä¸Šï¼Œè¯¥çº¦æŸé›†å°Šé‡æ ‘çš„å±‚æ¬¡ç»“æ„ï¼Œä»è€Œä¿è¯ä¼˜åŠ¿ä¼°è®¡çš„ä¸€è‡´æ€§å’Œä½æ–¹å·®ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦é˜¶æ®µï¼š
1. **MCTS Rollout**ï¼šä½¿ç”¨MCTSç”Ÿæˆé«˜è´¨é‡çš„ä¸­é—´è½¨è¿¹ï¼Œä½œä¸ºæ•™å¸ˆä¿¡å·ã€‚
2. **æ ‘ç»“æ„æ„å»º**ï¼šå°†MCTSè½¨è¿¹ç»„ç»‡æˆæ ‘çŠ¶ç»“æ„ï¼Œæ¯ä¸ªèŠ‚ç‚¹ä»£è¡¨ä¸€ä¸ªå‰ç¼€ã€‚
3. **åˆ†é˜¶æ®µä¼˜åŠ¿ä¼°è®¡ï¼ˆSAEï¼‰**ï¼šä½¿ç”¨SAEæ¡†æ¶è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„ä¼˜åŠ¿ï¼Œè¯¥æ¡†æ¶è€ƒè™‘äº†æ ·æœ¬æ‰€å±å‰ç¼€çš„é¢„æœŸå›æŠ¥ã€‚
4. **ç­–ç•¥ä¼˜åŒ–**ï¼šä½¿ç”¨è®¡ç®—å¾—åˆ°çš„ä¼˜åŠ¿ï¼Œé€šè¿‡GRPOç®—æ³•ä¼˜åŒ–ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹æ˜¯åˆ†é˜¶æ®µä¼˜åŠ¿ä¼°è®¡ï¼ˆSAEï¼‰æ¡†æ¶ã€‚SAEé€šè¿‡å°†å¥–åŠ±æŠ•å½±åˆ°å°Šé‡æ ‘ç»“æ„çš„çº¦æŸé›†ä¸Šï¼Œæœ‰æ•ˆåœ°è§£å†³äº†ä¸åŒå‰ç¼€å¸¦æ¥çš„é¢„æœŸå›æŠ¥å·®å¼‚é—®é¢˜ï¼Œä»è€Œé™ä½äº†ä¼˜åŠ¿ä¼°è®¡çš„æ–¹å·®ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒSAEèƒ½å¤Ÿæ›´å‡†ç¡®åœ°ä¼°è®¡ä¼˜åŠ¿ï¼Œä»è€Œæé«˜ç­–ç•¥å­¦ä¹ çš„æ•ˆç‡å’Œæ•ˆæœã€‚

**å…³é”®è®¾è®¡**ï¼šSAEæ¡†æ¶çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š
1. **çº¦æŸé›†çš„æ„å»º**ï¼šçº¦æŸé›†çš„è®¾è®¡éœ€è¦ä¿è¯ä¼˜åŠ¿ä¼°è®¡çš„ä¸€è‡´æ€§ï¼Œå³åŒä¸€å‰ç¼€ä¸‹çš„æ ·æœ¬åº”è¯¥å…·æœ‰ç›¸ä¼¼çš„ä¼˜åŠ¿ã€‚
2. **æŠ•å½±æ–¹æ³•**ï¼šè®ºæ–‡æ¯”è¾ƒäº†ä¸¤ç§æŠ•å½±æ–¹æ³•ï¼šä¸€ç§æ˜¯é«˜æ•ˆçš„å¯å‘å¼æ–¹æ³•ï¼Œå¦ä¸€ç§æ˜¯å½¢å¼åŒ–çš„äºŒæ¬¡è§„åˆ’æ–¹æ³•ã€‚å¯å‘å¼æ–¹æ³•è®¡ç®—æ•ˆç‡é«˜ï¼Œä½†å¯èƒ½ä¸å¤Ÿç²¾ç¡®ï¼›äºŒæ¬¡è§„åˆ’æ–¹æ³•èƒ½å¤Ÿä¿è¯æœ€ä¼˜æ€§ï¼Œä½†è®¡ç®—å¤æ‚åº¦è¾ƒé«˜ã€‚
3. **æŸå¤±å‡½æ•°**ï¼šä½¿ç”¨GRPOçš„æŸå¤±å‡½æ•°è¿›è¡Œç­–ç•¥ä¼˜åŒ–ï¼Œè¯¥æŸå¤±å‡½æ•°é¼“åŠ±ç­–ç•¥ç”Ÿæˆä¸MCTSè½¨è¿¹ç›¸ä¼¼çš„è¡Œä¸ºã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­ï¼Œä½¿ç”¨SAEçš„GRPOç®—æ³•æ˜¾è‘—ä¼˜äºæ ‡å‡†GRPOç®—æ³•ï¼Œæœ€ç»ˆå‡†ç¡®ç‡å¾—åˆ°æå‡ã€‚ç†è®ºåˆ†æè¡¨æ˜ï¼ŒSAEèƒ½å¤Ÿé™ä½æ¢¯åº¦æ–¹å·®ï¼Œä»è€Œæé«˜æ ·æœ¬æ•ˆç‡ã€‚è®ºæ–‡è¿˜æ¯”è¾ƒäº†é«˜æ•ˆçš„å¯å‘å¼æ–¹æ³•å’Œå½¢å¼åŒ–çš„äºŒæ¬¡è§„åˆ’æ–¹æ³•åœ¨SAEä¸­çš„åº”ç”¨ï¼ŒéªŒè¯äº†SAEçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦å¤šæ­¥æ¨ç†çš„åœºæ™¯ï¼Œä¾‹å¦‚æ•°å­¦é—®é¢˜æ±‚è§£ã€ä»£ç ç”Ÿæˆã€æ¸¸æˆAIç­‰ã€‚é€šè¿‡åˆ©ç”¨MCTSç”Ÿæˆçš„é«˜è´¨é‡è½¨è¿¹ï¼Œå¯ä»¥æœ‰æ•ˆåœ°æŒ‡å¯¼ç­–ç•¥å­¦ä¹ ï¼Œæé«˜æ™ºèƒ½ä½“åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚è¯¥æ–¹æ³•åœ¨æ•™è‚²é¢†åŸŸä¹Ÿæœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ï¼Œä¾‹å¦‚å¯ä»¥ç”¨äºæ„å»ºæ™ºèƒ½è¾…å¯¼ç³»ç»Ÿï¼Œå¸®åŠ©å­¦ç”Ÿè§£å†³å¤æ‚çš„æ•°å­¦é—®é¢˜ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in reasoning with large language models (LLMs) have shown the effectiveness of Monte Carlo Tree Search (MCTS) for generating high-quality intermediate trajectories, particularly in math and symbolic domains. Inspired by this, we explore how MCTS-derived trajectories-traditionally used for training value or reward models-can be repurposed to improve policy optimization in preference-based reinforcement learning (RL). Specifically, we focus on Group Relative Policy Optimization (GRPO), a recent algorithm that enables preference-consistent policy learning without value networks. We reframe GRPO into a staged training paradigm, leveraging a teacher's MCTS rollouts to construct a tree-structured curriculum of prefixes. This introduces the novel challenge of computing advantages for training samples that originate from different prefixes, each with a distinct expected return. To address this, we propose Staged Advantage Estimation (SAE), a framework for computing low-variance, prefix-aware advantages by projecting rewards onto a constraint set that respects the tree's hierarchy. Our empirical results on mathematical reasoning tasks show that SAE improves final accuracy over standard GRPO. This outcome is grounded in our theoretical analysis, which confirms that SAE reduces gradient variance-a principled path to improved sample efficiency. We demonstrate this through practical SAE implementations, comparing efficient heuristics against a formal quadratic program.

