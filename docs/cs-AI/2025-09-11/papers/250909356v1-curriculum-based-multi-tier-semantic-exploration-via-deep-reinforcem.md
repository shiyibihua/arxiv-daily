---
layout: default
title: Curriculum-Based Multi-Tier Semantic Exploration via Deep Reinforcement Learning
---

# Curriculum-Based Multi-Tier Semantic Exploration via Deep Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.09356" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.09356v1</a>
  <a href="https://arxiv.org/pdf/2509.09356.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.09356v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.09356v1', 'Curriculum-Based Multi-Tier Semantic Exploration via Deep Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Abdel Hakim Drid, Vincenzo Suriani, Daniele Nardi, Abderrezzak Debilou

**åˆ†ç±»**: cs.AI, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-09-11

**å¤‡æ³¨**: The 19th International Conference on Intelligent Autonomous Systems (IAS 19), 2025, Genoa

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºè¯¾ç¨‹å­¦ä¹ çš„å¤šå±‚è¯­ä¹‰æ¢ç´¢æ·±åº¦å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œæå‡å…·èº«æ™ºèƒ½ä½“åœ¨æœªçŸ¥ç¯å¢ƒä¸­çš„æ¢ç´¢æ•ˆç‡ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å…·èº«æ™ºèƒ½` `æ·±åº¦å¼ºåŒ–å­¦ä¹ ` `è¯­ä¹‰æ¢ç´¢` `è§†è§‰-è¯­è¨€æ¨¡å‹` `è¯¾ç¨‹å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨æ¢ç´¢æ•ˆç‡å’Œè¯­ä¹‰ç†è§£ä¹‹é—´éš¾ä»¥å¹³è¡¡ï¼Œå› ä¸ºæ™ºèƒ½ä½“çš„ç­–ç•¥è®¤çŸ¥èƒ½åŠ›æœ‰é™ï¼Œå¯¼è‡´è¯­ä¹‰æ¢ç´¢é€šå¸¸éœ€è¦äººå·¥å¹²é¢„ã€‚
2. è®ºæ–‡æå‡ºä¸€ç§åŸºäºè¯¾ç¨‹å­¦ä¹ çš„å¤šå±‚è¯­ä¹‰æ¢ç´¢æ·±åº¦å¼ºåŒ–å­¦ä¹ æ¶æ„ï¼Œé€šè¿‡è§†è§‰-è¯­è¨€æ¨¡å‹å’Œåˆ†å±‚å¥–åŠ±å‡½æ•°ï¼Œä½¿æ™ºèƒ½ä½“å…·å¤‡å¸¸è¯†æ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ™ºèƒ½ä½“æ˜¾è‘—æé«˜äº†å¯¹è±¡å‘ç°ç‡ï¼Œå¹¶å­¦ä¼šäº†ç­–ç•¥æ€§åœ°åˆ©ç”¨å¤–éƒ¨ç¯å¢ƒä¿¡æ¯ï¼Œæœ‰æ•ˆå¯¼èˆªåˆ°è¯­ä¹‰ä¸°å¯Œçš„åŒºåŸŸã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰æ¶æ„ï¼Œä¸“ä¸ºèµ„æºé«˜æ•ˆçš„è¯­ä¹‰æ¢ç´¢è€Œè®¾è®¡ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ†å±‚å¥–åŠ±å‡½æ•°æ•´åˆäº†è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„å¸¸è¯†çŸ¥è¯†ã€‚VLMæŸ¥è¯¢è¢«å»ºæ¨¡ä¸ºä¸€ä¸ªä¸“ç”¨åŠ¨ä½œï¼Œå…è®¸æ™ºèƒ½ä½“ä»…åœ¨è®¤ä¸ºéœ€è¦å¤–éƒ¨æŒ‡å¯¼æ—¶æ‰ç­–ç•¥æ€§åœ°æŸ¥è¯¢VLMï¼Œä»è€ŒèŠ‚çœèµ„æºã€‚è¯¥æœºåˆ¶ä¸è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ç›¸ç»“åˆï¼Œæ—¨åœ¨æŒ‡å¯¼ä¸åŒå¤æ‚ç¨‹åº¦çš„å­¦ä¹ ï¼Œä»¥ç¡®ä¿ç¨³å¥å’Œç¨³å®šçš„å­¦ä¹ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ™ºèƒ½ä½“æ˜¾è‘—æé«˜äº†å¯¹è±¡å‘ç°ç‡ï¼Œå¹¶å‘å±•å‡ºæœ‰æ•ˆå¯¼èˆªåˆ°è¯­ä¹‰ä¸°å¯ŒåŒºåŸŸçš„å­¦ä¹ èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜å±•ç¤ºäº†å¯¹ä½•æ—¶æç¤ºå¤–éƒ¨ç¯å¢ƒä¿¡æ¯çš„ç­–ç•¥æ€§æŒæ¡ã€‚é€šè¿‡å±•ç¤ºä¸€ç§å°†å¸¸è¯†è¯­ä¹‰æ¨ç†åµŒå…¥è‡ªä¸»æ™ºèƒ½ä½“çš„å®ç”¨ä¸”å¯æ‰©å±•çš„æ–¹æ³•ï¼Œè¯¥ç ”ç©¶ä¸ºåœ¨æœºå™¨äººæŠ€æœ¯ä¸­è¿½æ±‚å®Œå…¨æ™ºèƒ½å’Œè‡ªæˆ‘å¼•å¯¼çš„æ¢ç´¢æä¾›äº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰å…·èº«æ™ºèƒ½ä½“åœ¨å¤æ‚æœªçŸ¥ç¯å¢ƒä¸­è¿›è¡Œè‡ªä¸»æ¢ç´¢æ—¶ï¼Œé¢ä¸´ç€æ¢ç´¢æ•ˆç‡å’Œè¯­ä¹‰ç†è§£éš¾ä»¥å…¼é¡¾çš„é—®é¢˜ã€‚ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ æ–¹æ³•ç”±äºæ™ºèƒ½ä½“ç­–ç•¥çš„è®¤çŸ¥èƒ½åŠ›æœ‰é™ï¼Œéš¾ä»¥æœ‰æ•ˆåœ°åˆ©ç”¨ç¯å¢ƒä¸­çš„è¯­ä¹‰ä¿¡æ¯ï¼Œå¯¼è‡´æ¢ç´¢è¿‡ç¨‹æ•ˆç‡ä½ä¸‹ï¼Œç”šè‡³éœ€è¦äººå·¥å¹²é¢„ã€‚å› æ­¤ï¼Œå¦‚ä½•è®©æ™ºèƒ½ä½“åœ¨èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ï¼Œé«˜æ•ˆåœ°è¿›è¡Œè¯­ä¹‰æ¢ç´¢æ˜¯ä¸€ä¸ªé‡è¦çš„æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„å¸¸è¯†çŸ¥è¯†ï¼Œå¹¶é€šè¿‡æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰è®­ç»ƒæ™ºèƒ½ä½“ï¼Œä½¿å…¶èƒ½å¤Ÿç­–ç•¥æ€§åœ°æŸ¥è¯¢VLMï¼Œä»è€Œè·å¾—å¤–éƒ¨æŒ‡å¯¼ï¼Œæå‡æ¢ç´¢æ•ˆç‡ã€‚åŒæ—¶ï¼Œé‡‡ç”¨è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œé€æ­¥å¼•å¯¼æ™ºèƒ½ä½“å­¦ä¹ ä¸åŒå¤æ‚ç¨‹åº¦çš„ä»»åŠ¡ï¼Œä»¥æé«˜å­¦ä¹ çš„ç¨³å®šæ€§å’Œé²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) æ™ºèƒ½ä½“ï¼ˆAgentï¼‰ï¼šè´Ÿè´£ä¸ç¯å¢ƒäº¤äº’ï¼Œæ‰§è¡ŒåŠ¨ä½œï¼Œå¹¶æ¥æ”¶å¥–åŠ±ï¼›2) ç¯å¢ƒï¼ˆEnvironmentï¼‰ï¼šæä¾›æ™ºèƒ½ä½“æ¢ç´¢çš„åœºæ™¯ï¼Œå¹¶æ ¹æ®æ™ºèƒ½ä½“çš„åŠ¨ä½œç»™å‡ºåé¦ˆï¼›3) è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼šæä¾›å¸¸è¯†çŸ¥è¯†ï¼Œæ™ºèƒ½ä½“å¯ä»¥é€šè¿‡æŸ¥è¯¢VLMè·å¾—å¤–éƒ¨æŒ‡å¯¼ï¼›4) å¥–åŠ±å‡½æ•°ï¼ˆReward Functionï¼‰ï¼šç”¨äºè¯„ä¼°æ™ºèƒ½ä½“çš„è¡Œä¸ºï¼Œå¹¶æŒ‡å¯¼æ™ºèƒ½ä½“çš„å­¦ä¹ ï¼›5) è¯¾ç¨‹å­¦ä¹ æ¨¡å—ï¼ˆCurriculum Learningï¼‰ï¼šç”¨äºé€æ­¥å¼•å¯¼æ™ºèƒ½ä½“å­¦ä¹ ä¸åŒå¤æ‚ç¨‹åº¦çš„ä»»åŠ¡ã€‚æ™ºèƒ½ä½“é€šè¿‡æ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•è¿›è¡Œè®­ç»ƒï¼Œå­¦ä¹ å¦‚ä½•åœ¨æ¢ç´¢è¿‡ç¨‹ä¸­ç­–ç•¥æ€§åœ°æŸ¥è¯¢VLMï¼Œå¹¶æ ¹æ®VLMçš„åé¦ˆè°ƒæ•´æ¢ç´¢ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†VLMçš„æŸ¥è¯¢å»ºæ¨¡ä¸ºä¸€ä¸ªä¸“ç”¨åŠ¨ä½œï¼Œå…è®¸æ™ºèƒ½ä½“ä»…åœ¨è®¤ä¸ºå¿…è¦æ—¶æ‰æŸ¥è¯¢VLMï¼Œä»è€ŒèŠ‚çœèµ„æºã€‚è¿™ç§ç­–ç•¥æ€§æŸ¥è¯¢æœºåˆ¶ä½¿å¾—æ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨æ¢ç´¢æ•ˆç‡å’Œè¯­ä¹‰ç†è§£ä¹‹é—´å–å¾—å¹³è¡¡ã€‚æ­¤å¤–ï¼Œè¯¾ç¨‹å­¦ä¹ ç­–ç•¥çš„å¼•å…¥ä¹Ÿæé«˜äº†å­¦ä¹ çš„ç¨³å®šæ€§å’Œé²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šå¥–åŠ±å‡½æ•°çš„è®¾è®¡æ˜¯å…³é”®ã€‚å®ƒåŒ…å«å¤šä¸ªå±‚æ¬¡ï¼ŒåŒ…æ‹¬æ¢ç´¢å¥–åŠ±ã€è¯­ä¹‰å¥–åŠ±å’ŒVLMæŸ¥è¯¢æƒ©ç½šã€‚æ¢ç´¢å¥–åŠ±é¼“åŠ±æ™ºèƒ½ä½“æ¢ç´¢æœªçŸ¥åŒºåŸŸï¼›è¯­ä¹‰å¥–åŠ±é¼“åŠ±æ™ºèƒ½ä½“å‘ç°è¯­ä¹‰ä¸°å¯Œçš„åŒºåŸŸï¼›VLMæŸ¥è¯¢æƒ©ç½šé™åˆ¶æ™ºèƒ½ä½“è¿‡åº¦æŸ¥è¯¢VLMã€‚è¯¾ç¨‹å­¦ä¹ ç­–ç•¥é€šè¿‡é€æ­¥å¢åŠ ä»»åŠ¡çš„å¤æ‚ç¨‹åº¦ï¼Œå¼•å¯¼æ™ºèƒ½ä½“å­¦ä¹ ã€‚ä¾‹å¦‚ï¼Œå…ˆè®©æ™ºèƒ½ä½“åœ¨ç®€å•çš„ç¯å¢ƒä¸­å­¦ä¹ åŸºæœ¬çš„æ¢ç´¢æŠ€èƒ½ï¼Œç„¶åå†è®©æ™ºèƒ½ä½“åœ¨å¤æ‚çš„ç¯å¢ƒä¸­å­¦ä¹ åˆ©ç”¨VLMè¿›è¡Œè¯­ä¹‰æ¢ç´¢ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†å¯¹è±¡å‘ç°ç‡ï¼Œå¹¶å‘å±•å‡ºæœ‰æ•ˆå¯¼èˆªåˆ°è¯­ä¹‰ä¸°å¯ŒåŒºåŸŸçš„å­¦ä¹ èƒ½åŠ›ã€‚ä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨å¯¹è±¡å‘ç°ç‡æ–¹é¢å–å¾—äº†æ˜¾è‘—æå‡ï¼Œå¹¶ä¸”èƒ½å¤Ÿç­–ç•¥æ€§åœ°åˆ©ç”¨å¤–éƒ¨ç¯å¢ƒä¿¡æ¯ã€‚æ­¤å¤–ï¼Œå®éªŒè¿˜éªŒè¯äº†è¯¾ç¨‹å­¦ä¹ ç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼Œè¡¨æ˜è¯¥ç­–ç•¥èƒ½å¤Ÿæé«˜å­¦ä¹ çš„ç¨³å®šæ€§å’Œé²æ£’æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººè‡ªä¸»å¯¼èˆªã€æ™ºèƒ½å®¶å±…ã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨æœºå™¨äººè‡ªä¸»å¯¼èˆªä¸­ï¼Œæ™ºèƒ½ä½“å¯ä»¥åˆ©ç”¨è¯¥æ–¹æ³•åœ¨æœªçŸ¥ç¯å¢ƒä¸­é«˜æ•ˆåœ°æ¢ç´¢ï¼Œå¹¶å‘ç°é‡è¦çš„ç›®æ ‡ç‰©ä½“ã€‚åœ¨æ™ºèƒ½å®¶å±…ä¸­ï¼Œæ™ºèƒ½ä½“å¯ä»¥åˆ©ç”¨è¯¥æ–¹æ³•ç†è§£ç”¨æˆ·çš„æŒ‡ä»¤ï¼Œå¹¶æ‰§è¡Œç›¸åº”çš„ä»»åŠ¡ã€‚åœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼Œæ™ºèƒ½ä½“å¯ä»¥åˆ©ç”¨è¯¥æ–¹æ³•ç†è§£äº¤é€šåœºæ™¯ï¼Œå¹¶åšå‡ºåˆç†çš„å†³ç­–ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Navigating and understanding complex and unknown environments autonomously demands more than just basic perception and movement from embodied agents. Truly effective exploration requires agents to possess higher-level cognitive abilities, the ability to reason about their surroundings, and make more informed decisions regarding exploration strategies. However, traditional RL approaches struggle to balance efficient exploration and semantic understanding due to limited cognitive capabilities embedded in the small policies for the agents, leading often to human drivers when dealing with semantic exploration. In this paper, we address this challenge by presenting a novel Deep Reinforcement Learning (DRL) architecture that is specifically designed for resource efficient semantic exploration. A key methodological contribution is the integration of a Vision-Language Model (VLM) common-sense through a layered reward function. The VLM query is modeled as a dedicated action, allowing the agent to strategically query the VLM only when deemed necessary for gaining external guidance, thereby conserving resources. This mechanism is combined with a curriculum learning strategy designed to guide learning at different levels of complexity to ensure robust and stable learning. Our experimental evaluation results convincingly demonstrate that our agent achieves significantly enhanced object discovery rates and develops a learned capability to effectively navigate towards semantically rich regions. Furthermore, it also shows a strategic mastery of when to prompt for external environmental information. By demonstrating a practical and scalable method for embedding common-sense semantic reasoning with autonomous agents, this research provides a novel approach to pursuing a fully intelligent and self-guided exploration in robotics.

