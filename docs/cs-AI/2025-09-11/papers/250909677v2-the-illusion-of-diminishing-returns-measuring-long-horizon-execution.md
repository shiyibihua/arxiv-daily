---
layout: default
title: The Illusion of Diminishing Returns: Measuring Long Horizon Execution in LLMs
---

# The Illusion of Diminishing Returns: Measuring Long Horizon Execution in LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.09677" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.09677v2</a>
  <a href="https://arxiv.org/pdf/2509.09677.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.09677v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.09677v2', 'The Illusion of Diminishing Returns: Measuring Long Horizon Execution in LLMs')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Akshit Sinha, Arvindh Arun, Shashwat Goel, Steffen Staab, Jonas Geiping

**ÂàÜÁ±ª**: cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-11 (Êõ¥Êñ∞: 2025-09-28)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**Êè≠Á§∫LLMÈïøÁ®ãÊâßË°åËÉΩÂäõÔºöÂçïÊ≠•Á≤æÂ∫¶ÊèêÂçáÂ∏¶Êù•‰ªªÂä°ÂÆåÊàêÈïøÂ∫¶ÁöÑÊåáÊï∞Á∫ßÂ¢ûÈïø**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `ÈïøÁ®ãÊâßË°å` `ÊâßË°åËÉΩÂäõ` `Ëá™ÊàëË∞ÉËäÇÊïàÂ∫î` `ÊÄùÁª¥ËøáÁ®ã` `Êé®ÁêÜ‰∏éÊâßË°å` `‰ªªÂä°ËßÑÂàí`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁü≠‰ªªÂä°Âü∫ÂáÜÊµãËØïÂèØËÉΩ‰Ωé‰º∞‰∫ÜLLMÁöÑÊΩúÂäõÔºåÂõ†‰∏∫ÂçïÊ≠•Á≤æÂ∫¶ÁöÑÂæÆÂ∞èÊèêÂçáËÉΩÊòæËëóÊèêÈ´òÈïø‰ªªÂä°ÁöÑÂÆåÊàêÂ∫¶„ÄÇ
2. ËÆ∫ÊñáÊ†∏ÂøÉÂú®‰∫éÈöîÁ¶ªLLMÁöÑÊâßË°åËÉΩÂäõÔºåÈÄöËøáÊèê‰æõÁü•ËØÜÂíåËÆ°ÂàíÔºå‰∏ìÊ≥®‰∫éÊ®°ÂûãÂú®Â∑≤Áü•Êù°‰ª∂‰∏ãÁöÑÊâßË°åÊïàÊûú„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåÊõ¥Â§ßÊ®°ÂûãÂú®ÈïøÁ®ã‰ªªÂä°‰∏≠Ë°®Áé∞Êõ¥Â•ΩÔºå‰ΩÜÂ≠òÂú®Ëá™ÊàëË∞ÉËäÇÊïàÂ∫îÔºåÊÄùÁª¥ËøáÁ®ãËÉΩÊúâÊïàÁºìËß£Ê≠§ÈóÆÈ¢ò„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÁ†îÁ©∂‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÊåÅÁª≠Êâ©Â±ïÊòØÂê¶‰ºö‰∫ßÁîüÊî∂ÁõäÈÄíÂáèÁöÑÁé∞Ë±°„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÁü≠‰ªªÂä°Âü∫ÂáÜÊµãËØïÂèØËÉΩ‰ºö‰∫ßÁîüËøõÂ±ïÊîæÁºìÁöÑÈîôËßâÔºåÂõ†‰∏∫ÂçïÊ≠•ÂáÜÁ°ÆÁéáÁöÑËæπÈôÖÊî∂ÁõäÂèØ‰ª•ËΩ¨Âåñ‰∏∫Ê®°ÂûãÊàêÂäüÂÆåÊàê‰ªªÂä°ÈïøÂ∫¶ÁöÑÊåáÊï∞Á∫ßÊîπËøõ„ÄÇËÆ∫ÊñáËÆ§‰∏∫ÔºåÂΩìÁÆÄÂçï‰ªªÂä°ÂèòÈïøÊó∂ÔºåLLMÁöÑÂ§±Ë¥•Ê∫ê‰∫éÊâßË°å‰∏≠ÁöÑÈîôËØØÔºåËÄåÈùûÊé®ÁêÜËÉΩÂäõ‰∏çË∂≥„ÄÇÂõ†Ê≠§ÔºåËÆ∫ÊñáÈÄöËøáÊòæÂºèÊèê‰æõËß£ÂÜ≥ÈïøÁ®ã‰ªªÂä°ÊâÄÈúÄÁöÑÁü•ËØÜÂíåËÆ°ÂàíÊù•ÈöîÁ¶ªÊâßË°åËÉΩÂäõ„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÂç≥‰ΩøÂ∞èÂûãÊ®°ÂûãÂÖ∑ÊúâÊé•ËøëÂÆåÁæéÁöÑÂçïÊ≠•ÂáÜÁ°ÆÁéáÔºåÊõ¥Â§ßÁöÑÊ®°Âûã‰πüËÉΩÊ≠£Á°ÆÊâßË°åÊõ¥Â§öÁöÑËΩÆÊ¨°„ÄÇÂêåÊó∂ËßÇÂØüÂà∞ÔºåÊ®°ÂûãÁöÑÊØèÊ≠•ÂáÜÁ°ÆÁéáÈöèÁùÄÊ≠•Êï∞ÁöÑÂ¢ûÂä†ËÄåÈôç‰ΩéÔºåËøô‰∏ç‰ªÖÊòØÁî±‰∫éÈïø‰∏ä‰∏ãÊñáÈôêÂà∂ÔºåËøòÂ≠òÂú®‰∏ÄÁßçËá™ÊàëË∞ÉËäÇÊïàÂ∫î‚Äî‚ÄîÂΩì‰∏ä‰∏ãÊñá‰∏≠ÂåÖÂê´ÂÖàÂâçËΩÆÊ¨°ÁöÑÈîôËØØÊó∂ÔºåÊ®°ÂûãÊõ¥ÂÆπÊòìÂá∫Èîô„ÄÇ‰ªÖ‰ªÖÊâ©Â±ïÊ®°ÂûãÂ§ßÂ∞èÂπ∂‰∏çËÉΩÂáèÂ∞ëËá™ÊàëË∞ÉËäÇ„ÄÇ‰ΩÜÊÄùÁª¥ËøáÁ®ãÂèØ‰ª•ÁºìËß£Ëá™ÊàëË∞ÉËäÇÔºåÂπ∂ÊîØÊåÅÂú®ÂçïËΩÆ‰∏≠ÊâßË°åÊõ¥ÈïøÁöÑ‰ªªÂä°„ÄÇÊúÄÂêéÔºåËÆ∫ÊñáÂØπÂâçÊ≤øÊÄùÁª¥Ê®°ÂûãÂú®ÂçïËΩÆ‰∏≠ÂèØ‰ª•ÊâßË°åÁöÑ‰ªªÂä°ÈïøÂ∫¶ËøõË°å‰∫ÜÂü∫ÂáÜÊµãËØï„ÄÇÊÄªÁöÑÊù•ËØ¥ÔºåÈÄöËøáÂÖ≥Ê≥®ÊâßË°åËÉΩÂäõÔºåÂ∏åÊúõËÉΩÂ§üË∞ÉÂíåLLMÂ¶Ç‰ΩïËß£ÂÜ≥Â§çÊùÇÁöÑÊé®ÁêÜÈóÆÈ¢òÔºå‰ΩÜÂú®‰ªªÂä°ÂèòÈïøÊó∂Âç¥Â§±Ë¥•ÁöÑ‰∫âËÆ∫ÔºåÂπ∂Âº∫Ë∞É‰∫ÜÊâ©Â±ïÊ®°ÂûãÂ§ßÂ∞èÂíåÈ°∫Â∫èÊµãËØïÊó∂ËÆ°ÁÆóÂØπ‰∫éÈïøÁ®ã‰ªªÂä°ÁöÑÂ∑®Â§ßÂ•ΩÂ§Ñ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥LLMÂú®ÈïøÁ®ã‰ªªÂä°‰∏≠Ë°®Áé∞‰∏ç‰Ω≥ÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÁ†îÁ©∂ÂæÄÂæÄ‰æßÈáç‰∫éÊé®ÁêÜËÉΩÂäõÔºåËÄåÂøΩÁï•‰∫ÜÊâßË°åËÉΩÂäõ„ÄÇÂΩìÁÆÄÂçï‰ªªÂä°Ë¢´ÊãâÈïøÊó∂ÔºåÂç≥‰ΩøLLMÂÖ∑Â§áËß£ÂÜ≥ÂçïÊ≠•ÈóÆÈ¢òÁöÑËÉΩÂäõÔºå‰πüÂèØËÉΩÂõ†‰∏∫ÊâßË°åËøáÁ®ã‰∏≠ÁöÑÈîôËØØËÄåÂØºËá¥Êï¥‰ΩìÂ§±Ë¥•„ÄÇÁé∞ÊúâÊñπÊ≥ïÈöæ‰ª•Âå∫ÂàÜLLMÊòØÁº∫‰πèÊé®ÁêÜËÉΩÂäõËøòÊòØÊâßË°åËÉΩÂäõ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞ÜLLMÁöÑÊé®ÁêÜÂíåÊâßË°åËÉΩÂäõËß£ËÄ¶„ÄÇÈÄöËøáÊòæÂºèÂú∞‰∏∫LLMÊèê‰æõÂÆåÊàêÈïøÁ®ã‰ªªÂä°ÊâÄÈúÄÁöÑÁü•ËØÜÂíåËÆ°ÂàíÔºå‰ªéËÄåÂ∞ÜÈóÆÈ¢òÁÆÄÂåñ‰∏∫ÊâßË°åÈóÆÈ¢ò„ÄÇËøôÊ†∑ÂèØ‰ª•Êõ¥Ê∏ÖÊô∞Âú∞ËØÑ‰º∞LLMÁöÑÊâßË°åËÉΩÂäõÔºåÂπ∂ÂàÜÊûêÂÖ∂Âú®ÈïøÁ®ã‰ªªÂä°‰∏≠Â§±Ë¥•ÁöÑÂéüÂõ†„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËÆ∫ÊñáÁöÑÊäÄÊúØÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÊã¨‰ª•‰∏ãÂá†‰∏™Ê≠•È™§Ôºö
1. **‰ªªÂä°ËÆæËÆ°**ÔºöËÆæËÆ°‰∏ÄÁ≥ªÂàóÈïøÁ®ã‰ªªÂä°ÔºåËøô‰∫õ‰ªªÂä°ÈúÄË¶ÅÂ§ö‰∏™Ê≠•È™§ÊâçËÉΩÂÆåÊàê„ÄÇ
2. **Áü•ËØÜÂíåËÆ°ÂàíÊèê‰æõ**Ôºö‰∏∫LLMÊèê‰æõÂÆåÊàê‰ªªÂä°ÊâÄÈúÄÁöÑÂÖ®ÈÉ®Áü•ËØÜÂíåËØ¶ÁªÜÁöÑÊâßË°åËÆ°Âàí„ÄÇ
3. **ÊâßË°åËØÑ‰º∞**ÔºöËØÑ‰º∞LLMÂú®ÁªôÂÆöÁü•ËØÜÂíåËÆ°ÂàíÁöÑÊÉÖÂÜµ‰∏ãÔºåÊâßË°å‰ªªÂä°ÁöÑÂáÜÁ°ÆÁéáÂíåÂÆåÊàêÂ∫¶„ÄÇ
4. **ÈîôËØØÂàÜÊûê**ÔºöÂàÜÊûêLLMÂú®ÊâßË°åËøáÁ®ã‰∏≠Âá∫Áé∞ÁöÑÈîôËØØÔºåÂπ∂Êé¢Á©∂ÂÖ∂ÂéüÂõ†„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÔºö
1. **ÊâßË°åËÉΩÂäõÈöîÁ¶ª**ÔºöÈÄöËøáÊèê‰æõÁü•ËØÜÂíåËÆ°ÂàíÔºåÂ∞ÜLLMÁöÑÊâßË°åËÉΩÂäõ‰ªéÊé®ÁêÜËÉΩÂäõ‰∏≠ÈöîÁ¶ªÂá∫Êù•Ôºå‰ªéËÄåÂèØ‰ª•Êõ¥ÂáÜÁ°ÆÂú∞ËØÑ‰º∞ÂÖ∂ÊâßË°åËÉΩÂäõ„ÄÇ
2. **Ëá™ÊàëË∞ÉËäÇÊïàÂ∫î**ÔºöÂèëÁé∞LLMÂ≠òÂú®Ëá™ÊàëË∞ÉËäÇÊïàÂ∫îÔºåÂç≥Ê®°ÂûãÊõ¥ÂÆπÊòìÂèóÂà∞ÂÖàÂâçÈîôËØØÁöÑÂΩ±ÂìçÔºå‰ªéËÄåÂØºËá¥ÂêéÁª≠Ê≠•È™§ÁöÑÈîôËØØ„ÄÇ
3. **ÊÄùÁª¥ËøáÁ®ãÁºìËß£**ÔºöÂèëÁé∞ÊÄùÁª¥ËøáÁ®ãÂèØ‰ª•ÊúâÊïàÁºìËß£Ëá™ÊàëË∞ÉËäÇÊïàÂ∫îÔºåÂπ∂ÊèêÈ´òLLMÂú®ÈïøÁ®ã‰ªªÂä°‰∏≠ÁöÑË°®Áé∞„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö
1. **‰ªªÂä°ÈöæÂ∫¶ÊéßÂà∂**Ôºö‰ªªÂä°ËÆæËÆ°ÈúÄË¶Å‰øùËØÅÂçïÊ≠•ÈöæÂ∫¶ËæÉ‰ΩéÔºåÁ°Æ‰øùLLMÂÖ∑Â§áËß£ÂÜ≥ÂçïÊ≠•ÈóÆÈ¢òÁöÑËÉΩÂäõÔºå‰ªéËÄåÁ™ÅÂá∫ÊâßË°åËÉΩÂäõÁöÑÂΩ±Âìç„ÄÇ
2. **Áü•ËØÜÂíåËÆ°ÂàíÁöÑËØ¶Â∞ΩÁ®ãÂ∫¶**ÔºöÊèê‰æõÁöÑÁü•ËØÜÂíåËÆ°ÂàíÈúÄË¶ÅË∂≥Â§üËØ¶Â∞ΩÔºå‰ª•Ê∂àÈô§LLMÂú®Êé®ÁêÜÊñπÈù¢ÁöÑË¥üÊãÖ„ÄÇ
3. **ËØÑ‰º∞ÊåáÊ†á**ÔºöÈááÁî®ÊØèÊ≠•ÂáÜÁ°ÆÁéáÂíå‰ªªÂä°ÂÆåÊàêÂ∫¶‰Ωú‰∏∫ËØÑ‰º∞ÊåáÊ†áÔºå‰ª•ÂÖ®Èù¢ËØÑ‰º∞LLMÁöÑÊâßË°åËÉΩÂäõ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊõ¥Â§ßÁöÑÊ®°ÂûãÂú®ÈïøÁ®ãÊâßË°å‰ªªÂä°‰∏≠Ë°®Áé∞ÊòæËëó‰ºò‰∫éÂ∞èÂûãÊ®°ÂûãÔºåÂç≥‰ΩøÂ∞èÂûãÊ®°ÂûãÂú®ÂçïÊ≠•‰ªªÂä°‰∏≠Ë°®Áé∞Êé•ËøëÂÆåÁæé„ÄÇËÆ∫ÊñáËøòÂèëÁé∞‰∫Ü‰∏ÄÁßçËá™ÊàëË∞ÉËäÇÊïàÂ∫îÔºåÂç≥Ê®°ÂûãÊõ¥ÂÆπÊòìÂèóÂà∞ÂÖàÂâçÈîôËØØÁöÑÂΩ±Âìç„ÄÇÈÄöËøáÂºïÂÖ•ÊÄùÁª¥ËøáÁ®ãÔºåÂèØ‰ª•ÊúâÊïàÁºìËß£Ëá™ÊàëË∞ÉËäÇÊïàÂ∫îÔºåÂπ∂ÊòæËëóÊèêÈ´òÊ®°ÂûãÂú®ÈïøÁ®ã‰ªªÂä°‰∏≠ÁöÑË°®Áé∞„ÄÇ‰æãÂ¶ÇÔºåÊÄùÁª¥ËøáÁ®ã‰ΩøÂæóÊ®°ÂûãËÉΩÂ§üÊâßË°åÊõ¥ÈïøÁöÑ‰ªªÂä°ÔºåÂçïËΩÆÊâßË°åÈïøÂ∫¶ÊòæËëóÊèêÂçá„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÊèêÂçáLLMÂú®ÈúÄË¶ÅÈïøÊúüËßÑÂàíÂíåÊâßË°åÁöÑ‰ªªÂä°‰∏≠ÁöÑË°®Áé∞Ôºå‰æãÂ¶ÇÔºöÊú∫Âô®‰∫∫ÊéßÂà∂„ÄÅÂØπËØùÁ≥ªÁªü„ÄÅ‰ª£Á†ÅÁîüÊàêÁ≠â„ÄÇÈÄöËøáÂÖ≥Ê≥®Âíå‰ºòÂåñLLMÁöÑÊâßË°åËÉΩÂäõÔºåÂèØ‰ª•‰ΩøÂÖ∂Êõ¥Â•ΩÂú∞ÂÆåÊàêÂ§çÊùÇÁöÑ„ÄÅÂ§öÊ≠•È™§ÁöÑ‰ªªÂä°Ôºå‰ªéËÄåÊãìÂ±ïLLMÁöÑÂ∫îÁî®ËåÉÂõ¥ÂíåÂÆûÈôÖ‰ª∑ÂÄº„ÄÇÊú™Êù•ÁöÑÁ†îÁ©∂ÂèØ‰ª•Ëøõ‰∏ÄÊ≠•Êé¢Á¥¢Â¶Ç‰ΩïÊúâÊïàÂú∞ÁºìËß£Ëá™ÊàëË∞ÉËäÇÊïàÂ∫îÔºåÂπ∂ËÆæËÆ°Êõ¥ÊúâÊïàÁöÑÊÄùÁª¥ËøáÁ®ã„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Does continued scaling of large language models (LLMs) yield diminishing returns? In this work, we show that short-task benchmarks may give an illusion of slowing progress, as even marginal gains in single-step accuracy can compound into exponential improvements in the length of tasks a model can successfully complete. Then, we argue that failures of LLMs when simple tasks are made longer arise from mistakes in execution, rather than an inability to reason. So, we propose isolating execution capability, by explicitly providing the knowledge and plan needed to solve a long-horizon task. First, we find that larger models can correctly execute significantly more turns even when small models have near-perfect single-turn accuracy. We then observe that the per-step accuracy of models degrades as the number of steps increases. This is not just due to long-context limitations -- curiously, we observe a self-conditioning effect -- models become more likely to make mistakes when the context contains their errors from prior turns. Self-conditioning does not reduce by just scaling the model size. But, we find that thinking mitigates self-conditioning, and also enables execution of much longer tasks in a single turn. We conclude by benchmarking frontier thinking models on the length of tasks they can execute in a single turn. Overall, by focusing on the ability to execute, we hope to reconcile debates on how LLMs can solve complex reasoning problems yet fail at simple tasks when made longer, and highlight the massive benefits of scaling model size and sequential test-time compute for long-horizon tasks.

