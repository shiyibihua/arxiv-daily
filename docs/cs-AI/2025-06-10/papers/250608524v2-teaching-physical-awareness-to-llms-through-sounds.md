---
layout: default
title: Teaching Physical Awareness to LLMs through Sounds
---

# Teaching Physical Awareness to LLMs through Sounds

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.08524" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.08524v2</a>
  <a href="https://arxiv.org/pdf/2506.08524.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.08524v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.08524v2', 'Teaching Physical Awareness to LLMs through Sounds')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Weiguo Wang, Andy Nie, Wenrui Zhou, Yi Kai, Chengchen Hu

**åˆ†ç±»**: cs.SD, cs.AI, cs.MM, cs.RO, eess.AS

**å‘å¸ƒæ—¥æœŸ**: 2025-06-10 (æ›´æ–°: 2025-06-11)

**å¤‡æ³¨**: ICML 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºACORNæ¡†æ¶ä»¥è§£å†³LLMsç¼ºä¹ç‰©ç†æ„è¯†çš„é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `ç‰©ç†æ„è¯†` `éŸ³é¢‘å¤„ç†` `å¤šæ¨¡æ€å­¦ä¹ ` `æ•°æ®ç”Ÿæˆ` `ç‰©ç†æ¨¡æ‹Ÿ` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†æ–‡æœ¬å’Œå¤šæ¨¡æ€ä¿¡æ¯æ—¶è¡¨ç°ä¼˜å¼‚ï¼Œä½†ç¼ºä¹å¯¹ç°å®ç‰©ç†ç°è±¡çš„ç†è§£ï¼Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºACORNæ¡†æ¶ï¼Œé€šè¿‡å£°éŸ³æ¥æ•™ä¼šLLMsç‰©ç†æ„è¯†ï¼Œåˆ©ç”¨ç‰©ç†æ¨¡æ‹Ÿå™¨ç”Ÿæˆå¤šæ ·åŒ–çš„è®­ç»ƒæ•°æ®ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œç»“åˆéŸ³é¢‘ç¼–ç å™¨çš„LLMsåœ¨å¤šé¡¹ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ï¼Œå±•ç¤ºäº†åœ¨ç‰©ç†ç†è§£æ–¹é¢çš„æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ–‡æœ¬å’Œå¤šæ¨¡æ€å¤„ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç†è§£ç°å®ä¸–ç•Œçš„ç‰©ç†ç°è±¡æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚æœ¬æ–‡æå‡ºäº†ACORNæ¡†æ¶ï¼Œé€šè¿‡å£°éŸ³æ•™ä¼šLLMsç‰©ç†æ„è¯†ï¼Œé‡ç‚¹å…³æ³¨å¤šæ™®å‹’æ•ˆåº”ã€è·¯å¾„æ•ˆåº”å’Œç©ºé—´å…³ç³»ç­‰åŸºæœ¬ç‰©ç†ç°è±¡ã€‚ä¸ºäº†è§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼ŒACORNå¼•å…¥äº†ä¸€ç§ç‰©ç†åŸºç¡€çš„æ¨¡æ‹Ÿå™¨ï¼Œå°†çœŸå®ä¸–ç•Œçš„å£°éŸ³æºä¸å—æ§çš„ç‰©ç†é€šé“ç»“åˆï¼Œä»¥ç”Ÿæˆå¤šæ ·åŒ–çš„è®­ç»ƒæ•°æ®ã€‚åˆ©ç”¨è¯¥æ¨¡æ‹Ÿå™¨ï¼Œæˆ‘ä»¬æ„å»ºäº†AQA-PHYï¼Œä¸€ä¸ªå…¨é¢çš„éŸ³é¢‘é—®ç­”æ•°æ®é›†ï¼Œå¹¶æå‡ºäº†ä¸€ç§å¤„ç†å¹…åº¦å’Œç›¸ä½ä¿¡æ¯çš„éŸ³é¢‘ç¼–ç å™¨ã€‚é€šè¿‡å°†æˆ‘ä»¬çš„éŸ³é¢‘ç¼–ç å™¨ä¸æœ€å…ˆè¿›çš„LLMsè¿æ¥ï¼Œæˆ‘ä»¬åœ¨æ¨¡æ‹Ÿå’Œç°å®ä»»åŠ¡ä¸­å±•ç¤ºäº†åˆç†çš„ç»“æœï¼Œå¦‚è§†çº¿æ£€æµ‹ã€å¤šæ™®å‹’æ•ˆåº”ä¼°è®¡å’Œåˆ°è¾¾æ–¹å‘ä¼°è®¡ï¼Œä¸ºLLMsç†è§£ç‰©ç†ä¸–ç•Œé“ºå¹³äº†é“è·¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç†è§£ç‰©ç†ç°è±¡æ–¹é¢çš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•ç¼ºä¹æœ‰æ•ˆçš„è®­ç»ƒæ•°æ®å’Œç‰©ç†æ„è¯†çš„å¼•å¯¼ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å£°éŸ³ä½œä¸ºåª’ä»‹ï¼Œåˆ©ç”¨ç‰©ç†æ¨¡æ‹Ÿå™¨ç”Ÿæˆä¸ç‰©ç†ç°è±¡ç›¸å…³çš„éŸ³é¢‘æ•°æ®ï¼Œä»è€Œæ•™ä¼šLLMsç†è§£ç‰©ç†ä¸–ç•Œçš„åŸºæœ¬è§„å¾‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šACORNæ¡†æ¶åŒ…æ‹¬ç‰©ç†åŸºç¡€çš„æ¨¡æ‹Ÿå™¨ã€éŸ³é¢‘æ•°æ®ç”Ÿæˆæ¨¡å—å’ŒéŸ³é¢‘ç¼–ç å™¨ã€‚æ¨¡æ‹Ÿå™¨ç”Ÿæˆå¤šæ ·åŒ–çš„éŸ³é¢‘æ•°æ®ï¼ŒéŸ³é¢‘ç¼–ç å™¨å¤„ç†å¹…åº¦å’Œç›¸ä½ä¿¡æ¯ï¼Œå¹¶ä¸LLMsè¿æ¥è¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†ç‰©ç†æ¨¡æ‹Ÿä¸éŸ³é¢‘æ•°æ®ç”Ÿæˆç›¸ç»“åˆï¼Œåˆ›é€ å‡ºä¸€ç§æ–°çš„è®­ç»ƒæ–¹å¼ï¼Œä½¿LLMsèƒ½å¤Ÿé€šè¿‡å£°éŸ³ç†è§£ç‰©ç†ç°è±¡ï¼Œä¸ä¼ ç»Ÿçš„æ–‡æœ¬è®­ç»ƒæ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šéŸ³é¢‘ç¼–ç å™¨è®¾è®¡ä¸ºèƒ½å¤ŸåŒæ—¶å¤„ç†å¹…åº¦å’Œç›¸ä½ä¿¡æ¯ï¼ŒæŸå¤±å‡½æ•°é‡‡ç”¨äº†é’ˆå¯¹éŸ³é¢‘ç‰¹å¾çš„ä¼˜åŒ–ç­–ç•¥ï¼Œä»¥æå‡æ¨¡å‹åœ¨ç‰©ç†ç°è±¡ç†è§£ä¸Šçš„è¡¨ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œç»“åˆéŸ³é¢‘ç¼–ç å™¨çš„LLMsåœ¨è§†çº¿æ£€æµ‹ã€å¤šæ™®å‹’æ•ˆåº”ä¼°è®¡å’Œåˆ°è¾¾æ–¹å‘ä¼°è®¡ç­‰ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼ŒéªŒè¯äº†ACORNæ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€æœºå™¨äººå¯¼èˆªå’Œè‡ªåŠ¨é©¾é©¶ç­‰ï¼Œéœ€è¦ç†è§£ç‰©ç†ç¯å¢ƒçš„åœºæ™¯ã€‚é€šè¿‡æå‡LLMsçš„ç‰©ç†æ„è¯†ï¼Œå¯ä»¥å¢å¼ºå…¶åœ¨å¤æ‚ç¯å¢ƒä¸­çš„å†³ç­–èƒ½åŠ›ï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have shown remarkable capabilities in text and multimodal processing, yet they fundamentally lack physical awareness--understanding of real-world physical phenomena. In this work, we present ACORN, a framework that teaches LLMs physical awareness through sound, focusing on fundamental physical phenomena like the Doppler effect, multipath effect, and spatial relationships. To overcome data scarcity, ACORN introduce a physics-based simulator combining real-world sound sources with controlled physical channels to generate diverse training data. Using this simulator, we build AQA-PHY, a comprehensive Audio Question-Answer dataset, and propose an audio encoder that processes both magnitude and phase information. By connecting our audio encoder to state-of-the-art LLMs, we demonstrate reasonable results in both simulated and real-world tasks, such as line-of-sight detection, Doppler effect estimation, and Direction-of-Arrival estimation, paving the way for enabling LLMs to understand physical world.

