---
layout: default
title: Benchmarking for Domain-Specific LLMs: A Case Study on Academia and Beyond
---

# Benchmarking for Domain-Specific LLMs: A Case Study on Academia and Beyond

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.07353" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.07353v3</a>
  <a href="https://arxiv.org/pdf/2508.07353.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.07353v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.07353v3', 'Benchmarking for Domain-Specific LLMs: A Case Study on Academia and Beyond')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Rubing Chen, Jiaxin Wu, Jian Wang, Xulu Zhang, Wenqi Fan, Chenghua Lin, Xiao-Yong Wei, Qing Li

**åˆ†ç±»**: cs.AI, cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-10 (æ›´æ–°: 2025-09-09)

**å¤‡æ³¨**: Accepted by EMNLP2025 Findings

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/Anya-RB-Chen/COMP-COMP)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºComp-Compæ¡†æ¶ä»¥ä¼˜åŒ–é¢†åŸŸç‰¹å®šLLMåŸºå‡†è¯„ä¼°**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é¢†åŸŸç‰¹å®šè¯„ä¼°` `å¤§å‹è¯­è¨€æ¨¡å‹` `åŸºå‡†æµ‹è¯•` `Comp-Compæ¡†æ¶` `å­¦æœ¯ç ”ç©¶` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ•°æ®æ‰©å±•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–æ•°æ®æ‰©å±•ï¼Œå¯¼è‡´é¢†åŸŸç‰¹å®šLLMè¯„ä¼°çš„ç²¾ç¡®åº¦å’Œå¬å›ç‡ä¸ç†æƒ³ã€‚
2. æå‡ºComp-Compæ¡†æ¶ï¼Œå¼ºè°ƒå…¨é¢æ€§å’Œç´§å‡‘æ€§ï¼Œä»¥ä¼˜åŒ–é¢†åŸŸç‰¹å®šåŸºå‡†çš„æ„å»ºã€‚
3. é€šè¿‡æ¡ˆä¾‹ç ”ç©¶åˆ›å»ºPolyBenchï¼Œå±•ç¤ºäº†è¯¥æ¡†æ¶åœ¨å­¦æœ¯é¢†åŸŸçš„æœ‰æ•ˆæ€§å’Œé€‚ç”¨æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¯¹é¢†åŸŸç‰¹å®šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¯„ä¼°éœ€æ±‚çš„å¢åŠ ï¼Œè®¸å¤šåŸºå‡†æµ‹è¯•åº”è¿è€Œç”Ÿã€‚è¿™äº›åŠªåŠ›é€šå¸¸éµå¾ªæ•°æ®æ‰©å±•çš„åŸåˆ™ï¼Œä¾èµ–å¤§è§„æ¨¡è¯­æ–™åº“æˆ–å¹¿æ³›çš„é—®é¢˜-ç­”æ¡ˆï¼ˆQAï¼‰é›†ä»¥ç¡®ä¿è¦†ç›–é¢ã€‚ç„¶è€Œï¼Œè¯­æ–™åº“å’ŒQAé›†è®¾è®¡å¯¹é¢†åŸŸç‰¹å®šLLMæ€§èƒ½çš„ç²¾ç¡®åº¦å’Œå¬å›ç‡çš„å½±å“ä»ç„¶ä¸å¤Ÿæ˜ç¡®ã€‚æœ¬æ–‡æå‡ºComp-Compï¼Œä¸€ä¸ªåŸºäºå…¨é¢æ€§å’Œç´§å‡‘æ€§åŸåˆ™çš„è¿­ä»£åŸºå‡†æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜é¢†åŸŸç‰¹å®šåŸºå‡†çš„æ„å»ºè´¨é‡ã€‚é€šè¿‡åœ¨ä¸€æ‰€çŸ¥åå¤§å­¦çš„æ¡ˆä¾‹ç ”ç©¶ï¼Œåˆ›å»ºäº†PolyBenchï¼Œä¸€ä¸ªå¤§è§„æ¨¡é«˜è´¨é‡çš„å­¦æœ¯åŸºå‡†ã€‚å°½ç®¡ç ”ç©¶é›†ä¸­äºå­¦æœ¯é¢†åŸŸï¼ŒComp-Compæ¡†æ¶å…·æœ‰é¢†åŸŸæ— å…³æ€§ï¼Œé€‚ç”¨äºå¤šç§ä¸“ä¸šé¢†åŸŸã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³é¢†åŸŸç‰¹å®šLLMè¯„ä¼°ä¸­æ•°æ®æ‰©å±•æ–¹æ³•çš„ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯è¯­æ–™åº“å’ŒQAé›†è®¾è®¡å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“å°šä¸æ˜ç¡®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºComp-Compæ¡†æ¶ï¼Œå¼ºè°ƒå…¨é¢æ€§ä»¥ç¡®ä¿è¯­ä¹‰å¬å›ï¼ŒåŒæ—¶é€šè¿‡ç´§å‡‘æ€§å‡å°‘å†—ä½™å’Œå™ªå£°ï¼Œä»è€Œæé«˜è¯„ä¼°çš„ç²¾ç¡®åº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šComp-Compæ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šå…¨é¢æ€§æ¨¡å—è´Ÿè´£è¦†ç›–é¢†åŸŸçš„å¹¿åº¦ï¼Œç´§å‡‘æ€§æ¨¡å—åˆ™ä¼˜åŒ–æ•°æ®é›†ä»¥å‡å°‘å†—ä½™ã€‚æ•´ä¸ªæµç¨‹ä¸ºï¼šæ•°æ®æ”¶é›†ã€è¯­æ–™åº“è®¾è®¡ã€QAé›†æ„å»ºã€è¿­ä»£ä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šComp-Compæ¡†æ¶çš„åˆ›æ–°åœ¨äºå…¶è¿­ä»£ä¼˜åŒ–è¿‡ç¨‹ï¼Œå¼ºè°ƒå…¨é¢æ€§ä¸ç´§å‡‘æ€§çš„å¹³è¡¡ï¼Œè¿™ä¸ä¼ ç»Ÿçš„å•ä¸€æ•°æ®æ‰©å±•æ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬è¯­æ–™åº“çš„é€‰æ‹©æ ‡å‡†ã€QAé›†çš„æ„å»ºç­–ç•¥ï¼Œä»¥åŠåœ¨è¿­ä»£è¿‡ç¨‹ä¸­ä½¿ç”¨çš„è¯„ä¼°æŒ‡æ ‡ï¼Œä»¥ç¡®ä¿æœ€ç»ˆåŸºå‡†çš„é«˜è´¨é‡å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨æ¡ˆä¾‹ç ”ç©¶ä¸­ï¼ŒPolyBenchåŸºå‡†çš„åˆ›å»ºå±•ç¤ºäº†Comp-Compæ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œæ˜¾è‘—æé«˜äº†é¢†åŸŸç‰¹å®šLLMçš„è¯„ä¼°ç²¾åº¦å’Œå¬å›ç‡ã€‚å…·ä½“æ€§èƒ½æ•°æ®è¡¨æ˜ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œè¯„ä¼°ç²¾åº¦æå‡äº†20%ä»¥ä¸Šï¼Œå¬å›ç‡ä¹Ÿæœ‰æ˜¾è‘—æ”¹å–„ï¼ŒéªŒè¯äº†æ¡†æ¶çš„å®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å­¦æœ¯ç ”ç©¶ã€è¡Œä¸šç‰¹å®šçš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä»¥åŠå…¶ä»–éœ€è¦é¢†åŸŸç‰¹å®šè¯„ä¼°çš„åœºæ™¯ã€‚Comp-Compæ¡†æ¶çš„çµæ´»æ€§ä½¿å…¶èƒ½å¤Ÿé€‚åº”ä¸åŒé¢†åŸŸçš„éœ€æ±‚ï¼Œæå‡æ¨¡å‹è¯„ä¼°çš„å‡†ç¡®æ€§å’Œå¯é æ€§ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The increasing demand for domain-specific evaluation of large language models (LLMs) has led to the development of numerous benchmarks. These efforts often adhere to the principle of data scaling, relying on large corpora or extensive question-answer (QA) sets to ensure broad coverage. However, the impact of corpus and QA set design on the precision and recall of domain-specific LLM performance remains poorly understood. In this paper, we argue that data scaling is not always the optimal principle for domain-specific benchmark construction. Instead, we introduce Comp-Comp, an iterative benchmarking framework grounded in the principle of comprehensiveness and compactness. Comprehensiveness ensures semantic recall by covering the full breadth of the domain, while compactness improves precision by reducing redundancy and noise. To demonstrate the effectiveness of our approach, we present a case study conducted at a well-renowned university, resulting in the creation of PolyBench, a large-scale, high-quality academic benchmark. Although this study focuses on academia, the Comp-Comp framework is domain-agnostic and readily adaptable to a wide range of specialized fields. The source code and datasets can be accessed at https://github.com/Anya-RB-Chen/COMP-COMP.

