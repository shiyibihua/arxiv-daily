---
layout: default
title: Seeing but Not Believing: Probing the Disconnect Between Visual Attention and Answer Correctness in VLMs
---

# Seeing but Not Believing: Probing the Disconnect Between Visual Attention and Answer Correctness in VLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.17771" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.17771v1</a>
  <a href="https://arxiv.org/pdf/2510.17771.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.17771v1" onclick="toggleFavorite(this, '2510.17771v1', 'Seeing but Not Believing: Probing the Disconnect Between Visual Attention and Answer Correctness in VLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhining Liu, Ziyi Chen, Hui Liu, Chen Luo, Xianfeng Tang, Suhang Wang, Joy Zeng, Zhenwei Dai, Zhan Shi, Tianxin Wei, Benoit Dumoulin, Hanghang Tong

**åˆ†ç±»**: cs.AI, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-20

**å¤‡æ³¨**: 21 pages, 10 figures, 6 tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ­ç¤ºè§†è§‰è¯­è¨€æ¨¡å‹â€œè§†è€Œä¸ä¿¡â€ç°è±¡ï¼Œæå‡ºæ— éœ€è®­ç»ƒçš„æ³¨æ„åŠ›å¹²é¢„æ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `è§†è§‰é—®ç­”` `æ³¨æ„åŠ›æœºåˆ¶` `æ¨ç†æ—¶å¹²é¢„` `è§†è€Œä¸ä¿¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è§†è§‰é—®ç­”ä»»åŠ¡ä¸­ï¼Œå³ä½¿å­˜åœ¨æ­£ç¡®è§†è§‰è¯æ®ï¼Œä»ä¼šç»™å‡ºé”™è¯¯ç­”æ¡ˆï¼ŒåŸå› å°šä¸æ˜ç¡®ã€‚
2. è¯¥ç ”ç©¶å‘ç°æ·±å±‚æ³¨æ„åŠ›æœºåˆ¶èƒ½å¤Ÿå®šä½åˆ°å…³é”®è§†è§‰è¯æ®ï¼Œä½†æ¨¡å‹æœªèƒ½æœ‰æ•ˆåˆ©ç”¨è¿™äº›ä¿¡æ¯è¿›è¡Œæ¨ç†ã€‚
3. æå‡ºä¸€ç§æ— éœ€è®­ç»ƒçš„æ¨ç†æ—¶å¹²é¢„æ–¹æ³•ï¼Œé€šè¿‡çªå‡ºæ·±å±‚æ³¨æ„åŠ›åŒºåŸŸï¼Œæ˜¾è‘—æå‡äº†å¤šç§VLMsçš„å‡†ç¡®æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨è§†è§‰é—®ç­”ç­‰å¤šæ¨¡æ€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å³ä½¿å­˜åœ¨æ­£ç¡®çš„è§†è§‰è¯æ®ï¼Œä»ç„¶å¯èƒ½å¤±è´¥ã€‚æœ¬æ–‡ç³»ç»Ÿåœ°ç ”ç©¶äº†è¿™äº›å¤±è´¥æ˜¯æºäºæœªèƒ½æ„ŸçŸ¥åˆ°è¯æ®ï¼Œè¿˜æ˜¯æœªèƒ½æœ‰æ•ˆåœ°åˆ©ç”¨è¯æ®ã€‚é€šè¿‡æ£€æŸ¥é€å±‚æ³¨æ„åŠ›åŠ¨æ€ï¼Œå‘ç°æµ…å±‚ä¸»è¦å…³æ³¨æ–‡æœ¬ï¼Œè€Œæ·±å±‚ç¨€ç–ä½†å¯é åœ°å…³æ³¨å±€éƒ¨è¯æ®åŒºåŸŸã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼ŒVLMsåœ¨è¾“å‡ºé”™è¯¯ç­”æ¡ˆæ—¶é€šå¸¸æ„ŸçŸ¥åˆ°è§†è§‰è¯æ®ï¼Œè¿™ç§ç°è±¡è¢«ç§°ä¸ºâ€œè§†è€Œä¸ä¿¡â€ï¼Œå¹¿æ³›å­˜åœ¨äºä¸»è¦çš„VLMå®¶æ—ä¸­ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ¨ç†æ—¶å¹²é¢„æ–¹æ³•ï¼Œé€šè¿‡åŸºäºé€‰æ‹©æ€§æ³¨æ„åŠ›çš„æ©ç çªå‡ºæ˜¾ç¤ºæ·±å±‚è¯æ®åŒºåŸŸã€‚è¯¥æ–¹æ³•æ— éœ€è®­ç»ƒï¼Œå¹¶èƒ½æŒç»­æé«˜å¤šä¸ªVLMå®¶æ—ï¼ˆåŒ…æ‹¬LLaVAã€Qwenã€Gemmaå’ŒInternVLï¼‰çš„å‡†ç¡®æ€§ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒVLMsåœ¨å†…éƒ¨ç¼–ç äº†å¯é çš„è¯æ®ï¼Œä½†æœªèƒ½å……åˆ†åˆ©ç”¨å®ƒï¼Œä½¿è¿™äº›ä¿¡å·æ˜¾å¼åŒ–å¯ä»¥å¼¥åˆæ„ŸçŸ¥å’Œæ¨ç†ä¹‹é—´çš„å·®è·ï¼Œä»è€Œæé«˜VLMsçš„è¯Šæ–­ç†è§£èƒ½åŠ›å’Œå¯é æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè§†è§‰è¯­è¨€æ¨¡å‹åœ¨è§†è§‰é—®ç­”ä»»åŠ¡ä¸­ï¼Œå³ä½¿èƒ½å¤Ÿâ€œçœ‹åˆ°â€æ­£ç¡®çš„è§†è§‰è¯æ®ï¼Œä»ç„¶ä¼šç»™å‡ºé”™è¯¯çš„ç­”æ¡ˆã€‚ç°æœ‰çš„æ–¹æ³•ç¼ºä¹å¯¹è¿™ç§â€œè§†è€Œä¸ä¿¡â€ç°è±¡çš„æ·±å…¥ç†è§£ï¼Œæ— æ³•æœ‰æ•ˆåˆ©ç”¨æ¨¡å‹å†…éƒ¨å·²ç»å­˜åœ¨çš„è§†è§‰ä¿¡æ¯ï¼Œå¯¼è‡´æ€§èƒ½ç“¶é¢ˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯ï¼Œè™½ç„¶VLMsçš„æ·±å±‚ç½‘ç»œèƒ½å¤Ÿå®šä½åˆ°å…³é”®çš„è§†è§‰è¯æ®ï¼Œä½†è¿™äº›è¯æ®å¹¶æ²¡æœ‰è¢«å……åˆ†åˆ©ç”¨ã€‚é€šè¿‡åœ¨æ¨ç†æ—¶å¯¹æ·±å±‚ç½‘ç»œçš„æ³¨æ„åŠ›è¿›è¡Œå¹²é¢„ï¼Œçªå‡ºæ˜¾ç¤ºè¿™äº›å…³é”®åŒºåŸŸï¼Œå¯ä»¥è¿«ä½¿æ¨¡å‹æ›´åŠ å…³æ³¨è¿™äº›è¯æ®ï¼Œä»è€Œæé«˜ç­”æ¡ˆçš„æ­£ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•ä¸»è¦åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆï¼Œåˆ†æVLMsçš„é€å±‚æ³¨æ„åŠ›åŠ¨æ€ï¼Œç¡®å®šæ·±å±‚ç½‘ç»œä¸­èƒ½å¤Ÿæœ‰æ•ˆå®šä½è§†è§‰è¯æ®çš„å±‚ã€‚ç„¶åï¼Œåœ¨æ¨ç†æ—¶ï¼ŒåŸºäºè¿™äº›æ·±å±‚ç½‘ç»œçš„æ³¨æ„åŠ›æƒé‡ï¼Œå¯¹è¾“å…¥å›¾åƒè¿›è¡Œé€‰æ‹©æ€§æ©ç ï¼Œçªå‡ºæ˜¾ç¤ºæ³¨æ„åŠ›é›†ä¸­çš„åŒºåŸŸã€‚ä¿®æ”¹åçš„å›¾åƒå’ŒåŸå§‹é—®é¢˜ä¸€èµ·è¾“å…¥åˆ°VLMä¸­ï¼Œå¾—åˆ°æœ€ç»ˆçš„ç­”æ¡ˆã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•çš„å…³é”®åˆ›æ–°åœ¨äºå‘ç°äº†VLMsçš„â€œè§†è€Œä¸ä¿¡â€ç°è±¡ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„æ¨ç†æ—¶å¹²é¢„æ–¹æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ä¸éœ€è¦é‡æ–°è®­ç»ƒæ¨¡å‹æˆ–ä¿®æ”¹æ¨¡å‹ç»“æ„çš„ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼Œè¯¥æ–¹æ³•å¯ä»¥åœ¨ç°æœ‰æ¨¡å‹çš„åŸºç¡€ä¸Šç›´æ¥åº”ç”¨ï¼Œå…·æœ‰å¾ˆå¼ºçš„é€šç”¨æ€§å’Œå®ç”¨æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šè¯¥æ–¹æ³•çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) é€‰æ‹©åˆé€‚çš„æ·±å±‚ç½‘ç»œå±‚è¿›è¡Œæ³¨æ„åŠ›åˆ†æå’Œå¹²é¢„ï¼›2) è®¾è®¡æœ‰æ•ˆçš„é€‰æ‹©æ€§æ©ç ç­–ç•¥ï¼Œä»¥çªå‡ºæ˜¾ç¤ºæ³¨æ„åŠ›é›†ä¸­çš„åŒºåŸŸï¼ŒåŒæ—¶é¿å…è¿‡åº¦å¹²æ‰°åŸå§‹å›¾åƒçš„ä¿¡æ¯ï¼›3) ç¡®å®šåˆé€‚çš„å¹²é¢„å¼ºåº¦ï¼Œä»¥å¹³è¡¡è§†è§‰è¯æ®çš„çªå‡ºå’ŒåŸå§‹ä¿¡æ¯çš„ä¿ç•™ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªVLMå®¶æ—ï¼ˆåŒ…æ‹¬LLaVAã€Qwenã€Gemmaå’ŒInternVLï¼‰ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨è§†è§‰é—®ç­”ä»»åŠ¡ä¸­ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨ä¸è¿›è¡Œä»»ä½•è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œæé«˜æ¨¡å‹çš„å‡†ç¡®ç‡ã€‚è¿™ä¸€ç»“æœéªŒè¯äº†VLMså†…éƒ¨ç¼–ç äº†å¯é çš„è§†è§‰è¯æ®ï¼Œä½†æœªèƒ½å……åˆ†åˆ©ç”¨è¿™äº›è¯æ®ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæå‡ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹çš„å¯é æ€§å’Œå‡†ç¡®æ€§ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦ç²¾ç¡®è§†è§‰æ¨ç†çš„åœºæ™¯ï¼Œå¦‚åŒ»ç–—å½±åƒè¯Šæ–­ã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½å®¢æœç­‰é¢†åŸŸã€‚é€šè¿‡æé«˜æ¨¡å‹å¯¹è§†è§‰è¯æ®çš„åˆ©ç”¨ç‡ï¼Œå¯ä»¥å‡å°‘é”™è¯¯ç­”æ¡ˆçš„äº§ç”Ÿï¼Œå¢å¼ºç”¨æˆ·å¯¹æ¨¡å‹çš„ä¿¡ä»»åº¦ï¼Œå¹¶ä¸ºæœªæ¥çš„VLMç ”ç©¶æä¾›æ–°çš„æ–¹å‘ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-Language Models (VLMs) achieve strong results on multimodal tasks such as visual question answering, yet they can still fail even when the correct visual evidence is present. In this work, we systematically investigate whether these failures arise from not perceiving the evidence or from not leveraging it effectively. By examining layer-wise attention dynamics, we find that shallow layers focus primarily on text, while deeper layers sparsely but reliably attend to localized evidence regions. Surprisingly, VLMs often perceive the visual evidence when outputting incorrect answers, a phenomenon we term ``seeing but not believing'' that widely exists in major VLM families. Building on this, we introduce an inference-time intervention that highlights deep-layer evidence regions through selective attention-based masking. It requires no training and consistently improves accuracy across multiple families, including LLaVA, Qwen, Gemma, and InternVL. These results show that VLMs encode reliable evidence internally but under-utilize it, making such signals explicit can bridge the gap between perception and reasoning, advancing the diagnostic understanding and reliability of VLMs.

