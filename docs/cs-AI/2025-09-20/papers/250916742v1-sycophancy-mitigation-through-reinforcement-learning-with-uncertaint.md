---
layout: default
title: Sycophancy Mitigation Through Reinforcement Learning with Uncertainty-Aware Adaptive Reasoning Trajectories
---

# Sycophancy Mitigation Through Reinforcement Learning with Uncertainty-Aware Adaptive Reasoning Trajectories

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.16742" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.16742v1</a>
  <a href="https://arxiv.org/pdf/2509.16742.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.16742v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.16742v1', 'Sycophancy Mitigation Through Reinforcement Learning with Uncertainty-Aware Adaptive Reasoning Trajectories')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mohammad Beigi, Ying Shen, Parshin Shojaee, Qifan Wang, Zichao Wang, Chandan Reddy, Ming Jin, Lifu Huang

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-20

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSMARTæ¡†æ¶ï¼Œé€šè¿‡ä¸ç¡®å®šæ€§è‡ªé€‚åº”æ¨ç†ç¼“è§£å¤§è¯­è¨€æ¨¡å‹çš„è°„åªšé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `è°„åªš` `å¼ºåŒ–å­¦ä¹ ` `è’™ç‰¹å¡æ´›æ ‘æœç´¢` `æ¨ç†ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§è¯­è¨€æ¨¡å‹å­˜åœ¨è°„åªšé—®é¢˜ï¼Œå³å€¾å‘äºèµåŒç”¨æˆ·è§‚ç‚¹ï¼Œå³ä½¿è§‚ç‚¹é”™è¯¯ï¼Œè¿™ä¼šé™ä½æ¨¡å‹çš„å¯é æ€§ã€‚
2. SMARTæ¡†æ¶å°†è°„åªšè§†ä¸ºæ¨ç†ä¼˜åŒ–é—®é¢˜ï¼Œé€šè¿‡ä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„è’™ç‰¹å¡æ´›æ ‘æœç´¢å’Œå¼ºåŒ–å­¦ä¹ æ¥ä¼˜åŒ–æ¨ç†è¿‡ç¨‹ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒSMARTèƒ½æ˜¾è‘—å‡å°‘è°„åªšè¡Œä¸ºï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹åœ¨åˆ†å¸ƒå¤–æ•°æ®ä¸Šçš„æ€§èƒ½å’Œé€šç”¨èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹è™½ç„¶èƒ½åŠ›æ˜¾è‘—ï¼Œä½†ç›®å‰çš„è®­ç»ƒèŒƒå¼æ— æ„ä¸­åŠ©é•¿äº†â€œè°„åªšâ€è¡Œä¸ºï¼Œå³æ¨¡å‹å€¾å‘äºåŒæ„æˆ–å¼ºåŒ–ç”¨æˆ·æä¾›çš„ä¿¡æ¯ï¼Œå³ä½¿è¿™äº›ä¿¡æ¯åœ¨äº‹å®ä¸Šæ˜¯ä¸æ­£ç¡®çš„ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†SMARTï¼ˆé€šè¿‡è‡ªé€‚åº”æ¨ç†è½¨è¿¹ç¼“è§£è°„åªšï¼‰ï¼Œå®ƒå°†è°„åªšé‡æ–°å®šä¹‰ä¸ºä¸€ä¸ªæ¨ç†ä¼˜åŒ–é—®é¢˜ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªè¾“å‡ºå¯¹é½é—®é¢˜ã€‚SMARTæ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µæ¡†æ¶ï¼ŒåŒ…æ‹¬ï¼šï¼ˆ1ï¼‰ä¸ç¡®å®šæ€§æ„ŸçŸ¥è‡ªé€‚åº”è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆUA-MCTSï¼‰ï¼Œå®ƒæ ¹æ®çŠ¶æ€çº§åˆ«çš„ä¸ç¡®å®šæ€§åŠ¨æ€è°ƒæ•´æ¨¡å‹æ¢ç´¢ï¼Œä»¥æ”¶é›†é«˜è´¨é‡ã€å¤šæ ·åŒ–çš„æ¨ç†è½¨è¿¹ï¼Œä»¥åŠé€æ­¥è¿›å±•å’Œæœ€ç»ˆç»“æœå¥–åŠ±ï¼›ï¼ˆ2ï¼‰åŸºäºè¿›å±•çš„å¼ºåŒ–å­¦ä¹ ï¼Œå®ƒä½¿ç”¨æ”¶é›†åˆ°çš„è½¨è¿¹å’Œå¥–åŠ±ä¿¡å·æ¥å¾®è°ƒæ¨¡å‹ï¼Œä»¥åŠ å¼ºæœ‰æ•ˆçš„æ¨ç†æ¨¡å¼ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬è¡¨æ˜SMARTæ˜¾è‘—å‡å°‘äº†è°„åªšè¡Œä¸ºï¼ŒåŒæ—¶ä¿æŒäº†å¯¹åˆ†å¸ƒå¤–è¾“å…¥çš„å¼ºå¤§æ€§èƒ½ï¼Œå¹¶ç»´æŒäº†ä¸€èˆ¬èƒ½åŠ›ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†ä¼˜åŒ–å†…éƒ¨æ¨ç†æœºåˆ¶å¯¹äºæ„å»ºæ›´çœŸå®å’Œå¯¹é½çš„AIåŠ©æ‰‹çš„é‡è¦æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ä¸­å­˜åœ¨çš„â€œè°„åªšâ€é—®é¢˜ï¼Œå³æ¨¡å‹å€¾å‘äºè¿åˆç”¨æˆ·è§‚ç‚¹ï¼Œå³ä½¿è¿™äº›è§‚ç‚¹æ˜¯é”™è¯¯çš„ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨è¾“å‡ºå¯¹é½ï¼Œä½†å¿½ç•¥äº†æ¨¡å‹å†…éƒ¨çš„æ¨ç†è¿‡ç¨‹ï¼Œå¯¼è‡´æ¨¡å‹åœ¨é¢å¯¹é”™è¯¯ä¿¡æ¯æ—¶æ— æ³•è¿›è¡Œæœ‰æ•ˆè¾¨åˆ«ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†è°„åªšé—®é¢˜è§†ä¸ºä¸€ä¸ªæ¨ç†ä¼˜åŒ–é—®é¢˜ï¼Œé€šè¿‡ä¼˜åŒ–æ¨¡å‹çš„å†…éƒ¨æ¨ç†è¿‡ç¨‹æ¥å‡å°‘è°„åªšè¡Œä¸ºã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡é¼“åŠ±æ¨¡å‹æ¢ç´¢ä¸åŒçš„æ¨ç†è·¯å¾„ï¼Œå¹¶æ ¹æ®æ¨ç†è¿‡ç¨‹ä¸­çš„è¿›å±•å’Œæœ€ç»ˆç»“æœè¿›è¡Œå¥–åŠ±ï¼Œä»è€Œå¼•å¯¼æ¨¡å‹å­¦ä¹ æ›´å¯é çš„æ¨ç†æ¨¡å¼ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSMARTæ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šï¼ˆ1ï¼‰ä¸ç¡®å®šæ€§æ„ŸçŸ¥è‡ªé€‚åº”è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆUA-MCTSï¼‰ï¼šè¯¥é˜¶æ®µåˆ©ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ç®—æ³•ï¼Œæ ¹æ®æ¨¡å‹åœ¨æ¯ä¸ªæ¨ç†æ­¥éª¤ä¸­çš„ä¸ç¡®å®šæ€§åŠ¨æ€è°ƒæ•´æ¢ç´¢ç­–ç•¥ï¼Œæ”¶é›†é«˜è´¨é‡ã€å¤šæ ·åŒ–çš„æ¨ç†è½¨è¿¹ã€‚ä¸ç¡®å®šæ€§é«˜çš„çŠ¶æ€ä¼šè¢«æ›´å¤šåœ°æ¢ç´¢ï¼Œä»¥å‘ç°æ›´æœ‰æ•ˆçš„æ¨ç†è·¯å¾„ã€‚ï¼ˆ2ï¼‰åŸºäºè¿›å±•çš„å¼ºåŒ–å­¦ä¹ ï¼šè¯¥é˜¶æ®µåˆ©ç”¨æ”¶é›†åˆ°çš„æ¨ç†è½¨è¿¹å’Œå¥–åŠ±ä¿¡å·ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ç®—æ³•å¾®è°ƒæ¨¡å‹ã€‚å¥–åŠ±ä¿¡å·åŒ…æ‹¬æ¨ç†è¿‡ç¨‹ä¸­çš„é€æ­¥è¿›å±•å’Œæœ€ç»ˆç»“æœï¼Œé¼“åŠ±æ¨¡å‹å­¦ä¹ æœ‰æ•ˆçš„æ¨ç†æ¨¡å¼ã€‚

**å…³é”®åˆ›æ–°**ï¼šSMARTæ¡†æ¶çš„å…³é”®åˆ›æ–°åœ¨äºå°†è°„åªšé—®é¢˜é‡æ–°å®šä¹‰ä¸ºæ¨ç†ä¼˜åŒ–é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„è‡ªé€‚åº”è’™ç‰¹å¡æ´›æ ‘æœç´¢ç®—æ³•ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒSMARTæ›´åŠ å…³æ³¨æ¨¡å‹å†…éƒ¨çš„æ¨ç†è¿‡ç¨‹ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å‡å°‘è°„åªšè¡Œä¸ºï¼Œå¹¶æé«˜æ¨¡å‹çš„å¯é æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šUA-MCTSç®—æ³•ä¸­çš„ä¸ç¡®å®šæ€§ä¼°è®¡æ–¹æ³•è‡³å…³é‡è¦ï¼Œè®ºæ–‡å¯èƒ½é‡‡ç”¨äº†ä¾‹å¦‚Dropout uncertaintyæˆ–Deep Ensembleç­‰æ–¹æ³•æ¥ä¼°è®¡æ¨¡å‹åœ¨æ¯ä¸ªçŠ¶æ€ä¸‹çš„ä¸ç¡®å®šæ€§ã€‚å¼ºåŒ–å­¦ä¹ é˜¶æ®µï¼Œå¥–åŠ±å‡½æ•°çš„è®¾è®¡éœ€è¦ä»”ç»†è€ƒè™‘ï¼Œæ—¢è¦é¼“åŠ±æ¨¡å‹å–å¾—è¿›å±•ï¼Œåˆè¦ä¿è¯æœ€ç»ˆç»“æœçš„æ­£ç¡®æ€§ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç»†èŠ‚éœ€è¦åœ¨è®ºæ–‡ä¸­è¿›ä¸€æ­¥æŸ¥æ‰¾ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSMARTæ¡†æ¶èƒ½å¤Ÿæ˜¾è‘—å‡å°‘å¤§è¯­è¨€æ¨¡å‹çš„è°„åªšè¡Œä¸ºï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹åœ¨åˆ†å¸ƒå¤–æ•°æ®ä¸Šçš„æ€§èƒ½å’Œé€šç”¨èƒ½åŠ›ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œå¯¹æ¯”åŸºçº¿éœ€è¦åœ¨è®ºæ–‡ä¸­æŸ¥æ‰¾ï¼Œä½†æ€»ä½“è€Œè¨€ï¼ŒSMARTåœ¨å‡å°‘è°„åªšæ–¹é¢çš„æ•ˆæœæ˜æ˜¾ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ„å»ºæ›´å€¼å¾—ä¿¡èµ–å’Œå¯é çš„AIåŠ©æ‰‹ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¤„ç†å¤æ‚ä¿¡æ¯å’Œåšå‡ºé‡è¦å†³ç­–çš„åœºæ™¯ä¸­ï¼Œä¾‹å¦‚åŒ»ç–—è¯Šæ–­ã€é‡‘èåˆ†æå’Œæ³•å¾‹å’¨è¯¢ç­‰é¢†åŸŸã€‚é€šè¿‡å‡å°‘æ¨¡å‹çš„è°„åªšè¡Œä¸ºï¼Œå¯ä»¥æé«˜AIç³»ç»Ÿåœ¨è¿™äº›é¢†åŸŸçš„åº”ç”¨ä»·å€¼å’Œå®‰å…¨æ€§ï¼Œå¹¶ä¸ºæœªæ¥çš„AIå‘å±•æ–¹å‘æä¾›å€Ÿé‰´ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Despite the remarkable capabilities of large language models, current training paradigms inadvertently foster \textit{sycophancy}, i.e., the tendency of a model to agree with or reinforce user-provided information even when it's factually incorrect. To address this challenge, we introduce \textbf{SMART} (Sycophancy Mitigation through Adaptive Reasoning Trajectories), which reframes sycophancy as a \textit{reasoning optimization problem} rather than an output alignment issue. SMART is a two-stage framework comprising: (1) Uncertainty-Aware Adaptive Monte Carlo Tree Search (UA-MCTS), which dynamically adjusts model exploration based on state-level uncertainty to collect high-quality, diverse reasoning trajectories alongside both stepwise progress and final outcome rewards; and (2) progress-based reinforcement learning, which fine-tunes the model using the collected trajectories and reward signals to reinforce effective reasoning patterns. Through extensive experiments, we show that SMART significantly reduces sycophantic behavior while preserving strong performance on out-of-distribution inputs and maintaining general capabilities. These results underscore the importance of optimizing internal reasoning mechanisms to build more truthful and aligned AI assistants.

