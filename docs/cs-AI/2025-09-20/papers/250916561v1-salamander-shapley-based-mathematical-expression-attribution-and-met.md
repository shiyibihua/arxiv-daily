---
layout: default
title: SalaMAnder: Shapley-based Mathematical Expression Attribution and Metric for Chain-of-Thought Reasoning
---

# SalaMAnder: Shapley-based Mathematical Expression Attribution and Metric for Chain-of-Thought Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.16561" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.16561v1</a>
  <a href="https://arxiv.org/pdf/2509.16561.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.16561v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.16561v1', 'SalaMAnder: Shapley-based Mathematical Expression Attribution and Metric for Chain-of-Thought Reasoning')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Yue Xin, Chen Shen, Shaotian Yan, Xiaosong Yuan, Yaoming Wang, Xiaofeng Zhang, Chenxi Huang, Jieping Ye

**ÂàÜÁ±ª**: cs.AI, cs.CL

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-20

**Â§áÊ≥®**: accpeted by EMNLP 2025

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫SalaMAnderÔºåÂü∫‰∫éShapleyÂÄºËØÑ‰º∞CoTÊé®ÁêÜ‰∏≠Êï∞Â≠¶Ë°®ËææÂºèÁöÑË¥°ÁåÆÂ∫¶ÔºåÂπ∂‰ºòÂåñÊèêÁ§∫ÊûÑÂª∫„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Chain-of-Thought` `ShapleyÂÄº` `Êï∞Â≠¶Ë°®ËææÂºèÂΩíÂõ†` `ÂèØËß£ÈáäÊÄß` `ÊèêÁ§∫Â∑•Á®ã`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑCoTÊé®ÁêÜËÉΩÂäõÊòæËëóÊèêÂçáÔºå‰ΩÜÂÖ∂ÂÜÖÂú®Êú∫Âà∂Â∞ö‰∏çÊòéÁ°ÆÔºåÈúÄË¶ÅÊ∑±ÂÖ•Á†îÁ©∂„ÄÇ
2. Âà©Áî®ShapleyÂÄºËøõË°åÊï∞Â≠¶Ë°®ËææÂºèÂΩíÂõ†ÔºåÂπ∂ËÆæËÆ°ÂàÜÂ±ÇÊäΩÊ†∑ÁÆóÊ≥ïÈôç‰ΩéËÆ°ÁÆóÂ§çÊùÇÂ∫¶ÔºåÊèêÂá∫SalaMAnderÊ°ÜÊû∂„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåCoSPÊåáÊ†á‰∏éÊ®°ÂûãÊÄßËÉΩÂÖ∑ÊúâÂçïË∞ÉÁõ∏ÂÖ≥ÊÄßÔºåÂèØÁî®‰∫éËß£ÈáäCoTÁöÑÊàêÂäüÂπ∂‰ºòÂåñÊèêÁ§∫ÊûÑÂª∫„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫SalaMAnderÔºàÂü∫‰∫éShapleyÂÄºÁöÑÊï∞Â≠¶Ë°®ËææÂºèÂΩíÂõ†ÂíåÂ∫¶ÈáèÔºâÔºåËøôÊòØ‰∏ÄÁßçÁêÜËÆ∫‰∏äÂèØÈù†ÁöÑÊñπÊ≥ï‰ª•ÂèäÊï∞Â≠¶‰∏ä‰∏•Ë∞®ÁöÑËØÑ‰º∞ÊåáÊ†áÔºåÁî®‰∫éÈáèÂåñÂ∞ëÊ†∑Êú¨CoTÊé®ÁêÜ‰∏≠ÁªÑ‰ª∂Á∫ßÂà´ÁöÑË¥°ÁåÆ„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåÊàë‰ª¨Âà©Áî®ShapleyÂÄºËøõË°åÊï∞Â≠¶Ë°®ËææÂºèÂΩíÂõ†ÔºåÂπ∂ÂºÄÂèë‰∫Ü‰∏ÄÁßçÈ´òÊïàÁöÑÂàÜÂ±ÇÊäΩÊ†∑ÁÆóÊ≥ïÔºåÊòæËëóÈôç‰Ωé‰∫ÜËÆ°ÁÆóÂ§çÊùÇÂ∫¶„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÈÄöËøáÂçèÊñπÂ∑ÆÂàÜÊûêÂºÄÂèë‰∫ÜCoSPÔºàShapleyÊ≠£ÂÄºÁöÑÂü∫Êï∞ÔºâÊåáÊ†á„ÄÇÂú®ÊµÅË°åÁöÑLLMÊ®°ÂûãÂíå‰∏çÂêåÁöÑÊï∞Â≠¶Âü∫ÂáÜ‰∏äÁöÑÂÖ®Èù¢È™åËØÅË°®ÊòéÔºåÊàë‰ª¨ÁöÑSalaMAnderÊ°ÜÊû∂‰∏≠ÁöÑCoSPÊåáÊ†á‰∏éÊ®°ÂûãÊÄßËÉΩË°®Áé∞Âá∫Á®≥ÂÅ•ÁöÑÂçïË∞ÉÁõ∏ÂÖ≥ÊÄßÔºå‰∏ç‰ªÖ‰∏∫Áé∞ÊúâÂ∞ëÊ†∑Êú¨CoTÁöÑÁªèÈ™åÊàêÂäüÊèê‰æõ‰∫ÜÁêÜËÆ∫Ëß£ÈáäÔºåËÄå‰∏î‰∏∫ÊèêÁ§∫ÊûÑÂª∫‰ºòÂåñÂª∫Á´ã‰∫ÜÊï∞Â≠¶‰∏ä‰∏•Ë∞®ÁöÑÂéüÂàô„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨È™åËØÅ‰∫ÜËß£ÈáäÁöÑÂèØÈù†ÊÄßÔºåÂπ∂Âú®Ê≠§Âü∫Á°Ä‰∏äÁªü‰∏Ä‰∫ÜÂÖàÂâçÂ∑•‰ΩúÁöÑËßÅËß£„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÊñπÊ≥ïÁº∫‰πèÂØπCoTÊé®ÁêÜËøáÁ®ã‰∏≠ÂêÑ‰∏™Êï∞Â≠¶Ë°®ËææÂºèË¥°ÁåÆÂ∫¶ÁöÑÈáèÂåñËØÑ‰º∞ÔºåÊó†Ê≥ïÊúâÊïàËß£ÈáäCoTÊèêÂçáLLMÊï∞Â≠¶Êé®ÁêÜËÉΩÂäõÁöÑÂÜÖÂú®Êú∫Âà∂ÔºåÈòªÁ¢ç‰∫ÜÊèêÁ§∫Â∑•Á®ãÁöÑ‰ºòÂåñ„ÄÇÂõ†Ê≠§ÔºåÈúÄË¶Å‰∏ÄÁßçËÉΩÂ§üÂáÜÁ°ÆËØÑ‰º∞ÊØè‰∏™Êï∞Â≠¶Ë°®ËææÂºèÂØπÊúÄÁªàÁªìÊûúÂΩ±ÂìçÁöÑÊñπÊ≥ïÔºåÂπ∂Âü∫‰∫éÊ≠§ÊåáÂØºÊèêÁ§∫ÊûÑÂª∫„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®ShapleyÂÄºÊù•ÈáèÂåñÊØè‰∏™Êï∞Â≠¶Ë°®ËææÂºèÂú®CoTÊé®ÁêÜËøáÁ®ã‰∏≠ÁöÑË¥°ÁåÆ„ÄÇShapleyÂÄºÊòØ‰∏ÄÁßçÂêà‰ΩúÂçöÂºàËÆ∫‰∏≠ÁöÑÊ¶ÇÂøµÔºåÂèØ‰ª•ÂÖ¨Âπ≥Âú∞ÂàÜÈÖçÂêà‰Ωú‰∫ßÁîüÁöÑÊî∂Áõä„ÄÇÂú®ËøôÈáåÔºåÊØè‰∏™Êï∞Â≠¶Ë°®ËææÂºèË¢´ËßÜ‰∏∫‰∏Ä‰∏™ÂèÇ‰∏éËÄÖÔºåCoTÊé®ÁêÜÁöÑÊ≠£Á°ÆÊÄßË¢´ËßÜ‰∏∫Âêà‰Ωú‰∫ßÁîüÁöÑÊî∂Áõä„ÄÇÈÄöËøáËÆ°ÁÆóÊØè‰∏™Ë°®ËææÂºèÁöÑShapleyÂÄºÔºåÂèØ‰ª•ËØÑ‰º∞ÂÖ∂ÂØπÊúÄÁªàÁªìÊûúÁöÑÂΩ±Âìç„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöSalaMAnderÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰∏§‰∏™Ê†∏ÂøÉÊ®°ÂùóÔºö1) Âü∫‰∫éShapleyÂÄºÁöÑÊï∞Â≠¶Ë°®ËææÂºèÂΩíÂõ†Ê®°ÂùóÔºöËØ•Ê®°ÂùóË¥üË¥£ËÆ°ÁÆóÊØè‰∏™Êï∞Â≠¶Ë°®ËææÂºèÁöÑShapleyÂÄºÔºåËØÑ‰º∞ÂÖ∂ÂØπÊúÄÁªàÊé®ÁêÜÁªìÊûúÁöÑË¥°ÁåÆ„ÄÇ‰∏∫‰∫ÜÈôç‰ΩéËÆ°ÁÆóÂ§çÊùÇÂ∫¶ÔºåÈááÁî®‰∫ÜÂàÜÂ±ÇÊäΩÊ†∑ÁÆóÊ≥ï„ÄÇ2) CoSPÊåáÊ†áËÆ°ÁÆóÊ®°ÂùóÔºöËØ•Ê®°ÂùóÂü∫‰∫éShapleyÂÄºËÆ°ÁÆóCoSPÊåáÊ†áÔºåCoSPÊåáÊ†áÂèçÊò†‰∫ÜÂØπÊúÄÁªàÁªìÊûúÊúâÁßØÊûÅË¥°ÁåÆÁöÑÊï∞Â≠¶Ë°®ËææÂºèÁöÑÊï∞Èáè„ÄÇÈÄöËøáÂçèÊñπÂ∑ÆÂàÜÊûêÔºåÈ™åËØÅCoSPÊåáÊ†á‰∏éÊ®°ÂûãÊÄßËÉΩ‰πãÈó¥ÁöÑÁõ∏ÂÖ≥ÊÄß„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂ∞ÜShapleyÂÄºÂ∫îÁî®‰∫éCoTÊé®ÁêÜËøáÁ®ã‰∏≠ÁöÑÊï∞Â≠¶Ë°®ËææÂºèÂΩíÂõ†ÔºåÂπ∂ÊèêÂá∫‰∫ÜCoSPÊåáÊ†á„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåËØ•ÊñπÊ≥ïËÉΩÂ§üÊõ¥ÂáÜÁ°ÆÂú∞ÈáèÂåñÊØè‰∏™Êï∞Â≠¶Ë°®ËææÂºèÁöÑË¥°ÁåÆÔºåÂπ∂Êèê‰æõÂèØËß£ÈáäÊÄß„ÄÇÊ≠§Â§ñÔºåÂàÜÂ±ÇÊäΩÊ†∑ÁÆóÊ≥ïÊòæËëóÈôç‰Ωé‰∫ÜShapleyÂÄºËÆ°ÁÆóÁöÑÂ§çÊùÇÂ∫¶„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ShapleyÂÄºËÆ°ÁÆó‰∏≠ÔºåÈááÁî®‰∫ÜÂàÜÂ±ÇÊäΩÊ†∑ÁÆóÊ≥ïÔºåÊ†πÊçÆÊï∞Â≠¶Ë°®ËææÂºèÁöÑÁ±ªÂûãÔºà‰æãÂ¶ÇÔºåÂä†Ê≥ï„ÄÅ‰πòÊ≥ïÁ≠âÔºâËøõË°åÂàÜÂ±ÇÔºå‰ª•ÊèêÈ´òÊäΩÊ†∑ÊïàÁéá„ÄÇCoSPÊåáÊ†áÁöÑËÆ°ÁÆóÂü∫‰∫éShapleyÂÄºÁöÑÊ≠£Ë¥üÊÄßÔºåÁªüËÆ°ÂØπÊúÄÁªàÁªìÊûúÊúâÁßØÊûÅË¥°ÁåÆÁöÑË°®ËææÂºèÊï∞Èáè„ÄÇËÆ∫ÊñáËøòËØ¶ÁªÜÊèèËø∞‰∫ÜÂÆûÈ™åËÆæÁΩÆÔºåÂåÖÊã¨‰ΩøÁî®ÁöÑLLMÊ®°Âûã„ÄÅÊï∞Â≠¶Âü∫ÂáÜÊï∞ÊçÆÈõÜ‰ª•ÂèäËØÑ‰º∞ÊåáÊ†á„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåSalaMAnderÊ°ÜÊû∂‰∏≠ÁöÑCoSPÊåáÊ†á‰∏éÊ®°ÂûãÊÄßËÉΩË°®Áé∞Âá∫Á®≥ÂÅ•ÁöÑÂçïË∞ÉÁõ∏ÂÖ≥ÊÄß„ÄÇÂú®Â§ö‰∏™ÊµÅË°åÁöÑLLMÊ®°ÂûãÂíåÊï∞Â≠¶Âü∫ÂáÜÊï∞ÊçÆÈõÜ‰∏äËøõË°å‰∫ÜÈ™åËØÅÔºåËØÅÊòé‰∫ÜËØ•ÊñπÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇ‰æãÂ¶ÇÔºåÂú®Êüê‰∫õÊï∞ÊçÆÈõÜ‰∏äÔºåÈÄöËøá‰ºòÂåñÊèêÁ§∫ÊûÑÂª∫ÔºåÊ®°ÂûãÁöÑÂáÜÁ°ÆÁéáÊèêÂçá‰∫ÜÊòæËëóÂπÖÂ∫¶ÔºàÂÖ∑‰ΩìÊï∞ÂÄºÊú™Áü•Ôºâ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®Êï∞Â≠¶Êé®ÁêÜ„ÄÅÁßëÂ≠¶ËÆ°ÁÆóÁ≠âÈ¢ÜÂüüÁöÑÊÄßËÉΩ„ÄÇÈÄöËøáÂàÜÊûêCoTÊé®ÁêÜËøáÁ®ã‰∏≠ÂÖ≥ÈîÆÁöÑÊï∞Â≠¶Ë°®ËææÂºèÔºåÂèØ‰ª•ÊåáÂØºÊèêÁ§∫Â∑•Á®ãÔºå‰ºòÂåñÊ®°ÂûãÊé®ÁêÜË∑ØÂæÑÔºåÊèêÈ´òÊ®°ÂûãÁöÑÂèØËß£ÈáäÊÄßÂíåÂèØÈù†ÊÄß„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ïËøòÂèØ‰ª•Â∫îÁî®‰∫éÂÖ∂‰ªñÁ±ªÂûãÁöÑÊé®ÁêÜ‰ªªÂä°Ôºå‰æãÂ¶Ç‰ª£Á†ÅÁîüÊàê„ÄÅÈÄªËæëÊé®ÁêÜÁ≠â„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Chain-of-Thought (CoT) prompting enhances the math reasoning capability of large language models (LLMs) to a large margin. However, the mechanism underlying such improvements remains unexplored. In this paper, we present \textbf{SalaMAnder} (\textbf{S}h\textbf{a}p\textbf{l}ey-b\textbf{a}sed \textbf{M}athematical Expression \textbf{A}ttribution a\textbf{nd} M\textbf{e}t\textbf{r}ic), a theoretically grounded methodology as well as a mathematically rigorous evaluation metric for quantifying component-level contributions in few-shot CoT reasoning. Concretely, we leverage the Shapley value for mathematical expression attribution and develop an efficient stratified sampling algorithm that significantly reduces the computational complexity. Besides, we develop the \textbf{CoSP} (\textbf{C}ardinality \textbf{o}f \textbf{S}hapley \textbf{P}ositives) metric through covariance analysis. Comprehensive validation across popular LLM models and diverse mathematical benchmarks demonstrates that the CoSP metric within our SalaMAnder framework exhibits a robust monotonic correlation with model performance, not only providing theoretical explanations for the empirical success of existing few-shot CoT but also establishing mathematically rigorous principles for prompt construction optimization. Furthermore, we verify the reliability of the explanation, based on which we unify the insights of previous work.

