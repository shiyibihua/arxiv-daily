---
layout: default
title: USF-MAE: Ultrasound Self-Supervised Foundation Model with Masked Autoencoding
---

# USF-MAE: Ultrasound Self-Supervised Foundation Model with Masked Autoencoding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.22990" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.22990v2</a>
  <a href="https://arxiv.org/pdf/2510.22990.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.22990v2" onclick="toggleFavorite(this, '2510.22990v2', 'USF-MAE: Ultrasound Self-Supervised Foundation Model with Masked Autoencoding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Youssef Megahed, Robin Ducharme, Aylin Erman, Mark Walker, Steven Hawken, Adrian D. C. Chan

**åˆ†ç±»**: eess.IV, cs.AI, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-27 (æ›´æ–°: 2025-11-07)

**å¤‡æ³¨**: 18 pages, 8 figures, 2 tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**USF-MAEï¼šåŸºäºæ©ç è‡ªç¼–ç å™¨çš„è¶…å£°è‡ªç›‘ç£é¢„è®­ç»ƒæ¨¡å‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¶…å£°å›¾åƒ` `è‡ªç›‘ç£å­¦ä¹ ` `æ©ç è‡ªç¼–ç å™¨` `Vision Transformer` `é¢„è®­ç»ƒæ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ·±åº¦å­¦ä¹ æ–¹æ³•å—é™äºç¼ºä¹å¤§è§„æ¨¡æ ‡æ³¨è¶…å£°æ•°æ®é›†ï¼Œä¸”é€šç”¨å›¾åƒé¢„è®­ç»ƒæ¨¡å‹åœ¨è¶…å£°å›¾åƒä¸Šçš„è¿ç§»èƒ½åŠ›æœ‰é™ã€‚
2. USF-MAEé€šè¿‡æ©ç è‡ªç¼–ç å™¨åœ¨å¤§é‡æ— æ ‡æ³¨è¶…å£°æ•°æ®ä¸Šè¿›è¡Œè‡ªç›‘ç£é¢„è®­ç»ƒï¼Œå­¦ä¹ ç‰¹å®šäºè¶…å£°æ¨¡æ€çš„ç‰¹å¾è¡¨ç¤ºã€‚
3. å®éªŒè¡¨æ˜ï¼ŒUSF-MAEåœ¨å¤šä¸ªè¶…å£°å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºä¼ ç»ŸCNNå’ŒViTæ¨¡å‹ï¼Œå¹¶å±•ç°å‡ºè‰¯å¥½çš„è·¨è§£å‰–åŒºåŸŸæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†è¶…å£°è‡ªç›‘ç£åŸºç¡€æ¨¡å‹USF-MAEï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„è‡ªç›‘ç£æ©ç è‡ªç¼–ç æ¡†æ¶ï¼Œä¸“é—¨ç”¨äºè¶…å£°æ•°æ®ã€‚è¯¥æ¨¡å‹åœ¨åŒ…å«æ¥è‡ª46ä¸ªå¼€æºæ•°æ®é›†çš„37ä¸‡å¼ 2Då’Œ3Dè¶…å£°å›¾åƒï¼ˆOpenUS-46ï¼‰ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼Œæ¶µç›–äº†20å¤šä¸ªè§£å‰–åŒºåŸŸã€‚OpenUS-46æ•°æ®é›†å·²å…¬å¼€ï¼Œä»¥ä¿ƒè¿›è¿›ä¸€æ­¥çš„ç ”ç©¶å’Œå¯é‡å¤æ€§ã€‚USF-MAEé‡‡ç”¨Vision Transformerç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œé€šè¿‡é‡å»ºè¢«æ©ç›–çš„å›¾åƒå—ï¼Œç›´æ¥ä»æ— æ ‡ç­¾æ•°æ®ä¸­å­¦ä¹ ä¸°å¯Œçš„ã€ç‰¹å®šäºæ¨¡æ€çš„è¡¨ç¤ºã€‚é¢„è®­ç»ƒçš„ç¼–ç å™¨åœ¨ä¸‰ä¸ªå…¬å…±ä¸‹æ¸¸åˆ†ç±»åŸºå‡†ï¼ˆBUS-BRAã€MMOTU-2Då’ŒGIST514-DBï¼‰ä¸Šè¿›è¡Œäº†å¾®è°ƒã€‚åœ¨æ‰€æœ‰ä»»åŠ¡ä¸­ï¼ŒUSF-MAEå§‹ç»ˆä¼˜äºä¼ ç»Ÿçš„CNNå’ŒViTåŸºçº¿ï¼Œåˆ†åˆ«å®ç°äº†81.6%ã€79.6%å’Œ82.4%çš„F1åˆ†æ•°ã€‚å°½ç®¡åœ¨é¢„è®­ç»ƒæœŸé—´æœªä½¿ç”¨æ ‡ç­¾ï¼Œä½†USF-MAEåœ¨ä¹³è…ºç™Œåˆ†ç±»æ–¹é¢æ¥è¿‘äº†æœ‰ç›‘ç£åŸºç¡€æ¨¡å‹UltraSamçš„æ€§èƒ½ï¼Œå¹¶åœ¨å…¶ä»–ä»»åŠ¡ä¸­è¶…è¿‡äº†å®ƒï¼Œå±•ç¤ºäº†å¼ºå¤§çš„è·¨è§£å‰–æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè¶…å£°å›¾åƒè¯Šæ–­é¢ä¸´å™ªå£°å¤§ã€æ“ä½œè€…ä¾èµ–æ€§å¼ºã€è§†é‡æœ‰é™ç­‰æŒ‘æˆ˜ï¼Œå¯¼è‡´è§‚å¯Ÿè€…é—´å·®å¼‚å¤§ã€‚ç°æœ‰æ·±åº¦å­¦ä¹ æ–¹æ³•ä¾èµ–å¤§é‡æ ‡æ³¨æ•°æ®ï¼Œè€Œè¶…å£°å›¾åƒæ ‡æ³¨æˆæœ¬é«˜æ˜‚ã€‚é€šç”¨å›¾åƒé¢„è®­ç»ƒæ¨¡å‹åœ¨è¶…å£°å›¾åƒä¸Šçš„è¿ç§»æ•ˆæœä¸ä½³ï¼Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šåˆ©ç”¨æ©ç è‡ªç¼–ç å™¨ï¼ˆMAEï¼‰è¿›è¡Œè‡ªç›‘ç£å­¦ä¹ ã€‚é€šè¿‡éšæœºæ©ç›–éƒ¨åˆ†è¶…å£°å›¾åƒå—ï¼Œå¹¶è®­ç»ƒæ¨¡å‹é‡å»ºè¿™äº›è¢«æ©ç›–çš„åŒºåŸŸï¼Œè¿«ä½¿æ¨¡å‹å­¦ä¹ è¶…å£°å›¾åƒçš„å†…åœ¨ç»“æ„å’Œç‰¹å¾è¡¨ç¤ºã€‚è¿™ç§æ–¹æ³•æ— éœ€äººå·¥æ ‡æ³¨ï¼Œå¯ä»¥æœ‰æ•ˆåˆ©ç”¨å¤§é‡æœªæ ‡æ³¨çš„è¶…å£°æ•°æ®ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šUSF-MAEé‡‡ç”¨Vision Transformerï¼ˆViTï¼‰ä½œä¸ºåŸºç¡€æ¶æ„ï¼ŒåŒ…å«ä¸€ä¸ªç¼–ç å™¨å’Œä¸€ä¸ªè§£ç å™¨ã€‚ç¼–ç å™¨å¤„ç†æœªè¢«æ©ç›–çš„å›¾åƒå—ï¼Œæå–ç‰¹å¾è¡¨ç¤ºã€‚è§£ç å™¨æ¥æ”¶ç¼–ç å™¨çš„è¾“å‡ºä»¥åŠè¢«æ©ç›–çš„å›¾åƒå—çš„ä½ç½®ä¿¡æ¯ï¼Œé‡å»ºåŸå§‹å›¾åƒã€‚æ•´ä¸ªæµç¨‹åŒ…æ‹¬ï¼š1ï¼‰å›¾åƒå—åˆ’åˆ†ä¸æ©ç ï¼›2ï¼‰ç¼–ç å™¨ç‰¹å¾æå–ï¼›3ï¼‰è§£ç å™¨å›¾åƒé‡å»ºï¼›4ï¼‰æŸå¤±è®¡ç®—ä¸æ¨¡å‹ä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šUSF-MAEæ˜¯é¦–ä¸ªä¸“é—¨é’ˆå¯¹è¶…å£°æ•°æ®çš„ã€å¤§è§„æ¨¡è‡ªç›‘ç£MAEæ¡†æ¶ã€‚å®ƒæ„å»ºäº†ä¸€ä¸ªåŒ…å«46ä¸ªå¼€æºè¶…å£°æ•°æ®é›†çš„OpenUS-46æ•°æ®é›†ï¼Œä¸ºè¶…å£°é¢†åŸŸçš„è‡ªç›‘ç£å­¦ä¹ æä¾›äº†æ•°æ®åŸºç¡€ã€‚ä¸ä¼ ç»Ÿçš„æœ‰ç›‘ç£å­¦ä¹ æˆ–é€šç”¨å›¾åƒé¢„è®­ç»ƒæ¨¡å‹ç›¸æ¯”ï¼ŒUSF-MAEèƒ½å¤Ÿæ›´å¥½åœ°å­¦ä¹ è¶…å£°å›¾åƒçš„ç‰¹å®šæ¨¡æ€ç‰¹å¾ã€‚

**å…³é”®è®¾è®¡**ï¼šUSF-MAEä½¿ç”¨ViT-Baseä½œä¸ºç¼–ç å™¨å’Œè§£ç å™¨çš„ä¸»å¹²ç½‘ç»œã€‚æ©ç æ¯”ä¾‹è®¾ç½®ä¸º75%ï¼Œå³éšæœºæ©ç›–75%çš„å›¾åƒå—ã€‚é‡å»ºç›®æ ‡ä¸ºåƒç´ å€¼ã€‚æŸå¤±å‡½æ•°é‡‡ç”¨å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰æŸå¤±ï¼Œè¡¡é‡é‡å»ºå›¾åƒä¸åŸå§‹å›¾åƒä¹‹é—´çš„å·®å¼‚ã€‚é¢„è®­ç»ƒåœ¨OpenUS-46æ•°æ®é›†ä¸Šè¿›è¡Œï¼Œå¾®è°ƒåœ¨ä¸‹æ¸¸åˆ†ç±»ä»»åŠ¡ä¸­ä½¿ç”¨äº¤å‰ç†µæŸå¤±ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

USF-MAEåœ¨ä¸‰ä¸ªä¸‹æ¸¸åˆ†ç±»ä»»åŠ¡ï¼ˆBUS-BRAã€MMOTU-2Då’ŒGIST514-DBï¼‰ä¸Šå‡å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ã€‚åœ¨BUS-BRAæ•°æ®é›†ä¸Šï¼ŒUSF-MAEçš„F1åˆ†æ•°è¾¾åˆ°81.6%ï¼Œæ¥è¿‘æœ‰ç›‘ç£æ¨¡å‹UltraSamçš„æ€§èƒ½ã€‚åœ¨MMOTU-2Då’ŒGIST514-DBæ•°æ®é›†ä¸Šï¼ŒUSF-MAEçš„F1åˆ†æ•°åˆ†åˆ«ä¸º79.6%å’Œ82.4%ï¼Œè¶…è¿‡äº†UltraSamã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒUSF-MAEå…·æœ‰å¼ºå¤§çš„ç‰¹å¾å­¦ä¹ èƒ½åŠ›å’Œè·¨è§£å‰–åŒºåŸŸæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

USF-MAEå¯åº”ç”¨äºå¤šç§è¶…å£°å›¾åƒåˆ†æä»»åŠ¡ï¼Œå¦‚ç—…ç¶æ£€æµ‹ã€å™¨å®˜åˆ†å‰²ã€ç–¾ç—…è¯Šæ–­ç­‰ã€‚é€šè¿‡è‡ªç›‘ç£é¢„è®­ç»ƒï¼Œå¯ä»¥æœ‰æ•ˆé™ä½å¯¹æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼ŒåŠ é€Ÿè¶…å£°AIæ¨¡å‹çš„å¼€å‘å’Œéƒ¨ç½²ã€‚è¯¥ç ”ç©¶æœ‰æœ›æé«˜è¶…å£°è¯Šæ–­çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œå¹¶æ¨åŠ¨è¶…å£°æŠ€æœ¯åœ¨åŒ»ç–—é¢†åŸŸçš„æ›´å¹¿æ³›åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Ultrasound imaging is one of the most widely used diagnostic modalities, offering real-time, radiation-free assessment across diverse clinical domains. However, interpretation of ultrasound images remains challenging due to high noise levels, operator dependence, and limited field of view, resulting in substantial inter-observer variability. Current Deep Learning approaches are hindered by the scarcity of large labeled datasets and the domain gap between general and sonographic images, which limits the transferability of models pretrained on non-medical data. To address these challenges, we introduce the Ultrasound Self-Supervised Foundation Model with Masked Autoencoding (USF-MAE), the first large-scale self-supervised MAE framework pretrained exclusively on ultrasound data. The model was pre-trained on 370,000 2D and 3D ultrasound images curated from 46 open-source datasets, collectively termed OpenUS-46, spanning over twenty anatomical regions. This curated dataset has been made publicly available to facilitate further research and reproducibility. Using a Vision Transformer encoder-decoder architecture, USF-MAE reconstructs masked image patches, enabling it to learn rich, modality-specific representations directly from unlabeled data. The pretrained encoder was fine-tuned on three public downstream classification benchmarks: BUS-BRA (breast cancer), MMOTU-2D (ovarian tumors), and GIST514-DB (gastrointestinal stromal tumors). Across all tasks, USF-MAE consistently outperformed conventional CNN and ViT baselines, achieving F1-scores of 81.6%, 79.6%, and 82.4%, respectively. Despite not using labels during pretraining, USF-MAE approached the performance of the supervised foundation model UltraSam on breast cancer classification and surpassed it on the other tasks, demonstrating strong cross-anatomical generalization.

