---
layout: default
title: VFLAIR-LLM: A Comprehensive Framework and Benchmark for Split Learning of LLMs
---

# VFLAIR-LLM: A Comprehensive Framework and Benchmark for Split Learning of LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.03097" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.03097v1</a>
  <a href="https://arxiv.org/pdf/2508.03097.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.03097v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.03097v1', 'VFLAIR-LLM: A Comprehensive Framework and Benchmark for Split Learning of LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zixuan Gu, Qiufeng Fan, Long Sun, Yang Liu, Xiaojun Ye

**åˆ†ç±»**: cs.CR, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-05

**å¤‡æ³¨**: 12 pages, 10 figures, published in KDD2025

**æœŸåˆŠ**: In Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2 (KDD'25), August 3-7, 2025, Toronto, ON, Canada. ACM, New York, NY, USA, 12 pages

**DOI**: [10.1145/3711896.3737411](https://doi.org/10.1145/3711896.3737411)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/FLAIR-THU/VFLAIR-LLM)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVFLAIR-LLMä»¥è§£å†³æ•°æ®éšç§ä¸èµ„æºé™åˆ¶ä¸‹çš„LLMé€‚åº”é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `åˆ†å‰²å­¦ä¹ ` `éšç§ä¿æŠ¤` `è®¡ç®—èµ„æº` `æ”»å‡»ä¸é˜²å¾¡` `å¾®è°ƒ` `æ¡†æ¶è®¾è®¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨æ•°æ®éšç§å’Œè®¡ç®—èµ„æºå—é™çš„æƒ…å†µä¸‹ï¼Œéš¾ä»¥æœ‰æ•ˆé€‚åº”å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€‚
2. æœ¬æ–‡æå‡ºVFLAIR-LLMæ¡†æ¶ï¼Œé€šè¿‡åˆ†å‰²å­¦ä¹ å®ç°èµ„æºé«˜æ•ˆä¸”éšç§ä¿æŠ¤çš„LLMæ¨ç†ä¸å¾®è°ƒã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVFLAIR-LLMåœ¨å¤šç§æ”»å‡»å’Œé˜²å¾¡è®¾ç½®ä¸‹è¡¨ç°ä¼˜å¼‚ï¼Œä¸ºå®é™…åº”ç”¨æä¾›äº†å…·ä½“çš„é…ç½®å»ºè®®ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å‘å±•ï¼Œå…¶åº”ç”¨é¢†åŸŸä¸æ–­æ‰©å¤§ã€‚ç„¶è€Œï¼Œç”¨æˆ·åœ¨æ•°æ®éšç§æ–¹é¢çš„é¡¾è™‘ä½¿å¾—ç›´æ¥ä½¿ç”¨LLM APIå—åˆ°é™åˆ¶ï¼Œè€Œç§æœ‰éƒ¨ç½²åˆ™éœ€è¦å¤§é‡è®¡ç®—èµ„æºã€‚è¿™ç»™åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­å®ç°å®‰å…¨çš„LLMé€‚åº”å¸¦æ¥äº†æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºVFLAIR-LLMçš„è½»é‡çº§åˆ†å‰²å­¦ä¹ æ¡†æ¶ï¼Œæ”¯æŒåœ¨éšç§ä¿æŠ¤çš„å‰æä¸‹è¿›è¡ŒLLMæ¨ç†å’Œå¾®è°ƒã€‚è¯¥æ¡†æ¶æä¾›äº†ä¸¤ç§LLMåˆ†åŒºè®¾ç½®ï¼Œæ”¯æŒä¸‰ç§ä»»åŠ¡ç±»å‹å’Œ18ä¸ªæ•°æ®é›†ï¼Œå¹¶åŒ…å«æ ‡å‡†æ¨¡å—ç”¨äºæ”»å‡»å’Œé˜²å¾¡çš„å®ç°ä¸è¯„ä¼°ã€‚æˆ‘ä»¬åœ¨å¤šç§åˆ†å‰²å­¦ä¹ è®¾ç½®ä¸‹å¯¹5ç§æ”»å‡»å’Œ9ç§é˜²å¾¡è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œæä¾›äº†å…³äºæ¨¡å‹åˆ†åŒºé…ç½®ã€é˜²å¾¡ç­–ç•¥å’Œè¶…å‚æ•°é€‰æ‹©çš„å…·ä½“è§è§£å’Œå»ºè®®ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨æ•°æ®éšç§å’Œè®¡ç®—èµ„æºå—é™çš„ç¯å¢ƒä¸­ï¼Œå¦‚ä½•æœ‰æ•ˆé€‚åº”å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•å…¼é¡¾éšç§ä¿æŠ¤ä¸è®¡ç®—æ•ˆç‡ï¼Œå¯¼è‡´ç”¨æˆ·æ— æ³•å®‰å…¨åœ°åˆ©ç”¨LLMã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šVFLAIR-LLMæ¡†æ¶é‡‡ç”¨åˆ†å‰²å­¦ä¹ ï¼ˆSplit Learningï¼‰æ–¹æ³•ï¼Œé€šè¿‡å°†æ¨¡å‹åˆ†ä¸ºå¤šä¸ªéƒ¨åˆ†ï¼Œå…è®¸åœ¨æœ¬åœ°è¿›è¡Œéšç§ä¿æŠ¤çš„æ¨ç†ä¸å¾®è°ƒï¼Œä»è€Œé™ä½è®¡ç®—èµ„æºçš„éœ€æ±‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šLLMåˆ†åŒºè®¾ç½®å’Œæ”»å‡»é˜²å¾¡æ¨¡å—ã€‚ç”¨æˆ·å¯ä»¥æ ¹æ®éœ€æ±‚é€‰æ‹©ä¸åŒçš„åˆ†åŒºè®¾ç½®ï¼Œå¹¶åˆ©ç”¨æ ‡å‡†æ¨¡å—è¯„ä¼°æ¨¡å‹çš„å®‰å…¨æ€§å’Œé²æ£’æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šVFLAIR-LLMçš„åˆ›æ–°åœ¨äºå…¶è½»é‡çº§è®¾è®¡å’Œå¯æ‰©å±•æ€§ï¼Œæ”¯æŒå¤šç§ä»»åŠ¡ç±»å‹å’Œæ•°æ®é›†ï¼ŒåŒæ—¶æä¾›äº†ç³»ç»ŸåŒ–çš„æ”»å‡»ä¸é˜²å¾¡è¯„ä¼°æœºåˆ¶ï¼Œæ˜¾è‘—æå‡äº†LLMåœ¨éšç§ä¿æŠ¤åœºæ™¯ä¸‹çš„é€‚ç”¨æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¡†æ¶ä¸­åŒ…å«äº†å¤šç§å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°çš„è®¾è®¡ï¼Œæ”¯æŒä¸åŒçš„æ¨¡å‹åˆ†åŒºé…ç½®ï¼Œç”¨æˆ·å¯ä»¥æ ¹æ®å…·ä½“åº”ç”¨åœºæ™¯çµæ´»è°ƒæ•´è¶…å‚æ•°ï¼Œä»¥è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚å®éªŒä¸­ä½¿ç”¨äº†5ç§æ”»å‡»å’Œ9ç§é˜²å¾¡ç­–ç•¥ï¼Œç¡®ä¿äº†æ¨¡å‹çš„å®‰å…¨æ€§ä¸æœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒVFLAIR-LLMåœ¨å¤šç§åˆ†å‰²å­¦ä¹ è®¾ç½®ä¸‹ï¼ŒæˆåŠŸå®ç°äº†å¯¹5ç§æ”»å‡»çš„æœ‰æ•ˆé˜²å¾¡ï¼Œå¹¶åœ¨9ç§é˜²å¾¡ç­–ç•¥ä¸­è¡¨ç°å‡ºè‰²ã€‚å…·ä½“è€Œè¨€ï¼Œæ¨¡å‹åœ¨éšç§ä¿æŠ¤å’Œè®¡ç®—æ•ˆç‡æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ï¼Œä¸ºå®é™…åº”ç”¨æä¾›äº†å¯é çš„å‚è€ƒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

VFLAIR-LLMæ¡†æ¶åœ¨åŒ»ç–—ã€é‡‘èç­‰å¯¹æ•°æ®éšç§è¦æ±‚é«˜çš„é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­å®ç°å®‰å…¨çš„LLMé€‚åº”ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿå¸®åŠ©ç”¨æˆ·åœ¨ä¿æŠ¤éšç§çš„åŒæ—¶ï¼Œå……åˆ†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„å¼ºå¤§èƒ½åŠ›ï¼Œæ¨åŠ¨ç›¸å…³è¡Œä¸šçš„æ™ºèƒ½åŒ–å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> With the advancement of Large Language Models (LLMs), LLM applications have expanded into a growing number of fields. However, users with data privacy concerns face limitations in directly utilizing LLM APIs, while private deployments incur significant computational demands. This creates a substantial challenge in achieving secure LLM adaptation under constrained local resources. To address this issue, collaborative learning methods, such as Split Learning (SL), offer a resource-efficient and privacy-preserving solution for adapting LLMs to private domains. In this study, we introduce VFLAIR-LLM (available at https://github.com/FLAIR-THU/VFLAIR-LLM), an extensible and lightweight split learning framework for LLMs, enabling privacy-preserving LLM inference and fine-tuning in resource-constrained environments. Our library provides two LLM partition settings, supporting three task types and 18 datasets. In addition, we provide standard modules for implementing and evaluating attacks and defenses. We benchmark 5 attacks and 9 defenses under various Split Learning for LLM(SL-LLM) settings, offering concrete insights and recommendations on the choice of model partition configurations, defense strategies, and relevant hyperparameters for real-world applications.

