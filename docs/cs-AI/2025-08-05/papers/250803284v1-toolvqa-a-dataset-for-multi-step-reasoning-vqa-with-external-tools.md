---
layout: default
title: ToolVQA: A Dataset for Multi-step Reasoning VQA with External Tools
---

# ToolVQA: A Dataset for Multi-step Reasoning VQA with External Tools

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.03284" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.03284v1</a>
  <a href="https://arxiv.org/pdf/2508.03284.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.03284v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.03284v1', 'ToolVQA: A Dataset for Multi-step Reasoning VQA with External Tools')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shaofeng Yin, Ting Lei, Yang Liu

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-05

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºToolVQAä»¥è§£å†³å¤šæ­¥éª¤æ¨ç†VQAä¸­çš„å·¥å…·ä½¿ç”¨èƒ½åŠ›ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ­¥éª¤æ¨ç†` `è§†è§‰é—®ç­”` `å¤–éƒ¨å·¥å…·` `å¤šæ¨¡æ€æ•°æ®é›†` `æ·±åº¦ä¼˜å…ˆæœç´¢` `äººç±»æ¨ç†æ¨¡æ‹Ÿ` `æ™ºèƒ½åŠ©æ‰‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å·¥å…·å¢å¼ºè§†è§‰é—®ç­”æ–¹æ³•åœ¨çœŸå®ä¸–ç•Œçš„å¤šæ¨¡æ€åœºæ™¯ä¸­è¡¨ç°ä¸ä½³ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šæ­¥éª¤æ¨ç†ä»»åŠ¡ä¸Šå­˜åœ¨æ˜¾è‘—ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºToolVQAæ•°æ®é›†ï¼Œé‡‡ç”¨çœŸå®è§†è§‰ä¸Šä¸‹æ–‡å’Œå¤æ‚çš„æ¨ç†ä»»åŠ¡ï¼Œç»“åˆToolEngineç”Ÿæˆæ•°æ®ä»¥æ¨¡æ‹Ÿäººç±»çš„å·¥å…·ä½¿ç”¨æ¨ç†ã€‚
3. åœ¨ToolVQAä¸Šå¾®è°ƒçš„7B LFMåœ¨æµ‹è¯•é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†å¤§å‹å°é—­æ¨¡å‹GPT-3.5-turboï¼Œå±•ç¤ºäº†è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°†å¤–éƒ¨å·¥å…·æ•´åˆè¿›å¤§å‹åŸºç¡€æ¨¡å‹ï¼ˆLFMï¼‰å·²æˆä¸ºæå‡å…¶é—®é¢˜è§£å†³èƒ½åŠ›çš„æœ‰æ•ˆæ–¹æ³•ã€‚å°½ç®¡ç°æœ‰ç ”ç©¶åœ¨å·¥å…·å¢å¼ºçš„è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†æœ€è¿‘çš„åŸºå‡†æµ‹è¯•æ˜¾ç¤ºï¼Œåœ¨éœ€è¦å¤šæ­¥éª¤æ¨ç†çš„å¤šæ¨¡æ€åœºæ™¯ä¸­ï¼Œå®é™…å·¥å…·ä½¿ç”¨èƒ½åŠ›å­˜åœ¨æ˜¾è‘—å·®è·ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ToolVQAï¼Œä¸€ä¸ªåŒ…å«23Kå®ä¾‹çš„å¤§è§„æ¨¡å¤šæ¨¡æ€æ•°æ®é›†ï¼Œæ—¨åœ¨å¼¥è¡¥è¿™ä¸€å·®è·ã€‚ToolVQAä¸åŒäºä»¥å¾€ä¾èµ–äºåˆæˆåœºæ™¯å’Œç®€åŒ–æŸ¥è¯¢çš„æ•°æ®é›†ï¼Œé‡‡ç”¨çœŸå®ä¸–ç•Œçš„è§†è§‰ä¸Šä¸‹æ–‡å’Œå…·æœ‰æŒ‘æˆ˜æ€§çš„éšå¼å¤šæ­¥éª¤æ¨ç†ä»»åŠ¡ï¼Œæ›´å¥½åœ°ä¸çœŸå®ç”¨æˆ·äº¤äº’å¯¹é½ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ToolEngineï¼Œä¸€ä¸ªæ–°é¢–çš„æ•°æ®ç”Ÿæˆç®¡é“ï¼Œåˆ©ç”¨æ·±åº¦ä¼˜å…ˆæœç´¢ï¼ˆDFSï¼‰å’ŒåŠ¨æ€ä¸Šä¸‹æ–‡ç¤ºä¾‹åŒ¹é…æœºåˆ¶æ¥æ¨¡æ‹Ÿç±»äººå·¥å…·ä½¿ç”¨æ¨ç†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å·¥å…·å¢å¼ºè§†è§‰é—®ç­”ï¼ˆVQAï¼‰åœ¨çœŸå®ä¸–ç•Œå¤šæ¨¡æ€åœºæ™¯ä¸­å¤šæ­¥éª¤æ¨ç†èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¤šä¾èµ–äºåˆæˆæ•°æ®ï¼Œæ— æ³•æœ‰æ•ˆåº”å¯¹å¤æ‚çš„å®é™…åº”ç”¨åœºæ™¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºToolVQAæ•°æ®é›†ï¼ŒåŒ…å«çœŸå®è§†è§‰ä¸Šä¸‹æ–‡å’Œéšå¼å¤šæ­¥éª¤æ¨ç†ä»»åŠ¡ï¼Œåˆ©ç”¨ToolEngineç”Ÿæˆæ•°æ®ä»¥æ¨¡æ‹Ÿäººç±»å·¥å…·ä½¿ç”¨çš„æ¨ç†è¿‡ç¨‹ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¢å¼ºäº†æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„è¡¨ç°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šToolVQAçš„æ•°æ®ç”Ÿæˆæµç¨‹åŒ…æ‹¬æ•°æ®æ”¶é›†ã€æ·±åº¦ä¼˜å…ˆæœç´¢ï¼ˆDFSï¼‰ç®—æ³•å’ŒåŠ¨æ€ä¸Šä¸‹æ–‡ç¤ºä¾‹åŒ¹é…æœºåˆ¶ã€‚ToolEngineä½œä¸ºæ ¸å¿ƒæ¨¡å—ï¼Œè´Ÿè´£ç”Ÿæˆç¬¦åˆçœŸå®åœºæ™¯çš„å¤šæ¨¡æ€æ•°æ®ã€‚

**å…³é”®åˆ›æ–°**ï¼šToolVQAçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶æ•°æ®ç”Ÿæˆæ–¹æ³•å’ŒçœŸå®åœºæ™¯çš„åº”ç”¨ï¼ŒåŒºåˆ«äºä»¥å¾€ä¾èµ–åˆæˆæ•°æ®çš„ç ”ç©¶ï¼Œæä¾›äº†æ›´å…·æŒ‘æˆ˜æ€§çš„æ¨ç†ä»»åŠ¡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ToolEngineä¸­ï¼Œé‡‡ç”¨DFSç®—æ³•è¿›è¡Œæ•°æ®ç”Ÿæˆï¼Œå¹¶ç»“åˆåŠ¨æ€ç¤ºä¾‹åŒ¹é…æœºåˆ¶ï¼Œä»¥ç¡®ä¿ç”Ÿæˆçš„æ•°æ®èƒ½å¤Ÿæœ‰æ•ˆæ¨¡æ‹Ÿäººç±»çš„æ¨ç†è¿‡ç¨‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨ToolVQAæ•°æ®é›†ä¸Šå¾®è°ƒçš„7B LFMåœ¨æµ‹è¯•é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†GPT-3.5-turboç­‰å¤§å‹å°é—­æ¨¡å‹ï¼Œå±•ç¤ºäº†åœ¨å¤šä¸ªåˆ†å¸ƒå¤–æ•°æ®é›†ä¸Šçš„å¼ºæ³›åŒ–èƒ½åŠ›ï¼Œæå‡å¹…åº¦æ˜¾è‘—ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

ToolVQAçš„ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºæ™ºèƒ½åŠ©æ‰‹ã€æœºå™¨äººå¯¼èˆªã€æ•™è‚²å’ŒåŒ»ç–—ç­‰é¢†åŸŸï¼Œæå‡ç³»ç»Ÿåœ¨å¤æ‚ä»»åŠ¡ä¸­çš„å†³ç­–èƒ½åŠ›å’Œç”¨æˆ·äº¤äº’ä½“éªŒã€‚æœªæ¥ï¼Œéšç€æ•°æ®é›†çš„ä¸æ–­å®Œå–„ï¼Œå¯èƒ½ä¼šæ¨åŠ¨å¤šæ¨¡æ€AIç³»ç»Ÿçš„è¿›ä¸€æ­¥å‘å±•ä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Integrating external tools into Large Foundation Models (LFMs) has emerged as a promising approach to enhance their problem-solving capabilities. While existing studies have demonstrated strong performance in tool-augmented Visual Question Answering (VQA), recent benchmarks reveal significant gaps in real-world tool-use proficiency, particularly in functionally diverse multimodal settings requiring multi-step reasoning. In this work, we introduce ToolVQA, a large-scale multimodal dataset comprising 23K instances, designed to bridge this gap. Unlike previous datasets that rely on synthetic scenarios and simplified queries, ToolVQA features real-world visual contexts and challenging implicit multi-step reasoning tasks, better aligning with real user interactions. To construct this dataset, we propose ToolEngine, a novel data generation pipeline that employs Depth-First Search (DFS) with a dynamic in-context example matching mechanism to simulate human-like tool-use reasoning. ToolVQA encompasses 10 multimodal tools across 7 diverse task domains, with an average inference length of 2.78 reasoning steps per instance. The fine-tuned 7B LFMs on ToolVQA not only achieve impressive performance on our test set but also surpass the large close-sourced model GPT-3.5-turbo on various out-of-distribution (OOD) datasets, demonstrating strong generalizability to real-world tool-use scenarios.

