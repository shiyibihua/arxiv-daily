---
layout: default
title: Fine-Tuning Text-to-Speech Diffusion Models Using Reinforcement Learning with Human Feedback
---

# Fine-Tuning Text-to-Speech Diffusion Models Using Reinforcement Learning with Human Feedback

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.03123" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.03123v1</a>
  <a href="https://arxiv.org/pdf/2508.03123.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.03123v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.03123v1', 'Fine-Tuning Text-to-Speech Diffusion Models Using Reinforcement Learning with Human Feedback')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jingyi Chen, Ju Seung Byun, Micha Elsner, Pichao Wang, Andrew Perrault

**åˆ†ç±»**: cs.SD, cs.AI, eess.AS

**å‘å¸ƒæ—¥æœŸ**: 2025-08-05

**å¤‡æ³¨**: 4 pages, 1 figure, INTERSPEECH 2025. arXiv admin note: text overlap with arXiv:2405.14632

**æœŸåˆŠ**: INTERSPEECH 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDLPOæ¡†æ¶ä»¥æå‡TTSæ‰©æ•£æ¨¡å‹çš„å®æ—¶æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ‰©æ•£æ¨¡å‹` `è¯­éŸ³åˆæˆ` `å¼ºåŒ–å­¦ä¹ ` `äººç±»åé¦ˆ` `è‡ªç„¶åº¦è¯„åˆ†` `å®æ—¶åº”ç”¨` `ä¼˜åŒ–ç­–ç•¥`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡è¯­éŸ³æ—¶æ•ˆç‡ä½ä¸‹ï¼Œå°¤å…¶æ˜¯åœ¨å®æ—¶åº”ç”¨ä¸­ï¼Œå»å™ªæ­¥éª¤è¿‡é•¿ä¸”éš¾ä»¥å»ºæ¨¡å£°è°ƒå’ŒèŠ‚å¥ã€‚
2. æœ¬æ–‡æå‡ºçš„DLPOæ¡†æ¶é€šè¿‡å°†è®­ç»ƒæŸå¤±çº³å…¥å¥–åŠ±å‡½æ•°ï¼Œç»“åˆå¼ºåŒ–å­¦ä¹ ä¸äººç±»åé¦ˆï¼Œä¼˜åŒ–äº†TTSæ‰©æ•£æ¨¡å‹çš„æ€§èƒ½ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDLPOåœ¨WaveGrad 2æ¨¡å‹ä¸Šæ˜¾è‘—æå‡äº†è¯­éŸ³è´¨é‡ï¼Œå®¢è§‚æŒ‡æ ‡å’Œä¸»è§‚è¯„ä¼°å‡æœ‰æ˜¾è‘—æ”¹å–„ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆé«˜ä¿çœŸè¯­éŸ³ï¼Œä½†ç”±äºå»å™ªæ­¥éª¤é•¿å’Œå£°è°ƒã€èŠ‚å¥å»ºæ¨¡çš„æŒ‘æˆ˜ï¼Œå®æ—¶ä½¿ç”¨æ•ˆç‡ä½ä¸‹ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†Diffusion Loss-Guided Policy Optimizationï¼ˆDLPOï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨ä¼˜åŒ–TTSæ‰©æ•£æ¨¡å‹ã€‚DLPOå°†åŸå§‹è®­ç»ƒæŸå¤±æ•´åˆè¿›å¥–åŠ±å‡½æ•°ä¸­ï¼Œä¿ç•™ç”Ÿæˆèƒ½åŠ›çš„åŒæ—¶å‡å°‘ä½æ•ˆã€‚é€šè¿‡è‡ªç„¶åº¦è¯„åˆ†ä½œä¸ºåé¦ˆï¼ŒDLPOä½¿å¥–åŠ±ä¼˜åŒ–ä¸æ‰©æ•£æ¨¡å‹ç»“æ„å¯¹é½ï¼Œä»è€Œæå‡è¯­éŸ³è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDLPOåœ¨WaveGrad 2æ¨¡å‹ä¸Šæ˜¾è‘—æé«˜äº†å®¢è§‚æŒ‡æ ‡ï¼ˆUTMOS 3.65ï¼ŒNISQA 4.02ï¼‰å’Œä¸»è§‚è¯„ä¼°ï¼ŒDLPOç”Ÿæˆçš„éŸ³é¢‘åœ¨67%çš„æ—¶é—´å†…è¢«åå¥½ã€‚è¿™äº›å‘ç°å±•ç¤ºäº†DLPOåœ¨å®æ—¶ã€èµ„æºæœ‰é™ç¯å¢ƒä¸­å®ç°é«˜æ•ˆé«˜è´¨é‡æ‰©æ•£TTSçš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ‰©æ•£æ¨¡å‹åœ¨å®æ—¶è¯­éŸ³åˆæˆä¸­çš„ä½æ•ˆé—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å»å™ªæ­¥éª¤å’Œå£°è°ƒã€èŠ‚å¥å»ºæ¨¡æ–¹é¢å­˜åœ¨æ˜¾è‘—æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDLPOæ¡†æ¶é€šè¿‡å°†åŸå§‹è®­ç»ƒæŸå¤±æ•´åˆåˆ°å¥–åŠ±å‡½æ•°ä¸­ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ ä¸äººç±»åé¦ˆç›¸ç»“åˆçš„æ–¹å¼ï¼Œä¼˜åŒ–ç”Ÿæˆè¿‡ç¨‹ï¼Œæå‡è¯­éŸ³åˆæˆçš„è‡ªç„¶åº¦å’Œæ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDLPOçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¥–åŠ±å‡½æ•°è®¾è®¡ã€åé¦ˆæœºåˆ¶å’Œä¼˜åŒ–ç­–ç•¥ã€‚é¦–å…ˆï¼Œé€šè¿‡è‡ªç„¶åº¦è¯„åˆ†è·å–äººç±»åé¦ˆï¼Œç„¶åå°†å…¶ä¸è®­ç»ƒæŸå¤±ç»“åˆï¼Œå½¢æˆæ–°çš„å¥–åŠ±ä¿¡å·ï¼Œæœ€åé€šè¿‡å¼ºåŒ–å­¦ä¹ ç®—æ³•è¿›è¡Œä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šDLPOçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†ä¼ ç»Ÿçš„è®­ç»ƒæŸå¤±ä¸å¥–åŠ±ä¿¡å·ç»“åˆï¼Œå½¢æˆäº†ä¸€ç§æ–°çš„ä¼˜åŒ–ç­–ç•¥ï¼Œä½¿å¾—æ¨¡å‹åœ¨ä¿æŒç”Ÿæˆèƒ½åŠ›çš„åŒæ—¶ï¼Œæ˜¾è‘—æé«˜äº†æ•ˆç‡ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒDLPOæ›´å¥½åœ°é€‚åº”äº†æ‰©æ•£æ¨¡å‹çš„ç»“æ„ç‰¹æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨DLPOä¸­ï¼Œå¥–åŠ±å‡½æ•°çš„è®¾è®¡è‡³å…³é‡è¦ï¼Œé‡‡ç”¨è‡ªç„¶åº¦è¯„åˆ†ä½œä¸ºåé¦ˆï¼Œç¡®ä¿ä¼˜åŒ–è¿‡ç¨‹ä¸äººç±»æ„ŸçŸ¥ä¸€è‡´ã€‚æ­¤å¤–ï¼Œæ¨¡å‹çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥é€‚åº”å¼ºåŒ–å­¦ä¹ çš„éœ€æ±‚ã€‚å…·ä½“ç»†èŠ‚åŒ…æ‹¬å¯¹æŸå¤±å‡½æ•°çš„è°ƒæ•´å’Œå¯¹æ¨¡å‹è®­ç»ƒè¿‡ç¨‹çš„åŠ¨æ€è°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒDLPOåœ¨WaveGrad 2æ¨¡å‹ä¸Šçš„å®¢è§‚æŒ‡æ ‡æ˜¾è‘—æå‡ï¼ŒUTMOSè¾¾åˆ°3.65ï¼ŒNISQAè¾¾åˆ°4.02ã€‚åŒæ—¶ï¼Œä¸»è§‚è¯„ä¼°æ˜¾ç¤ºï¼ŒDLPOç”Ÿæˆçš„éŸ³é¢‘åœ¨67%çš„æ—¶é—´å†…è¢«ç”¨æˆ·åå¥½ï¼Œå±•ç¤ºäº†å…¶åœ¨è¯­éŸ³åˆæˆè´¨é‡ä¸Šçš„æ˜¾è‘—æ”¹è¿›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å®æ—¶è¯­éŸ³åˆæˆã€è™šæ‹ŸåŠ©æ‰‹ã€è¯­éŸ³ç¿»è¯‘ç­‰ã€‚é€šè¿‡æå‡æ‰©æ•£æ¨¡å‹çš„æ•ˆç‡ï¼ŒDLPOèƒ½å¤Ÿåœ¨èµ„æºæœ‰é™çš„ç¯å¢ƒä¸­å®ç°é«˜è´¨é‡çš„è¯­éŸ³ç”Ÿæˆï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œåº”ç”¨å‰æ™¯ã€‚æœªæ¥ï¼Œéšç€æŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ï¼ŒDLPOå¯èƒ½ä¼šåœ¨æ›´å¤šçš„è¯­éŸ³äº¤äº’åœºæ™¯ä¸­å¾—åˆ°åº”ç”¨ï¼Œæ¨åŠ¨æ™ºèƒ½è¯­éŸ³æŠ€æœ¯çš„è¿›æ­¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Diffusion models produce high-fidelity speech but are inefficient for real-time use due to long denoising steps and challenges in modeling intonation and rhythm. To improve this, we propose Diffusion Loss-Guided Policy Optimization (DLPO), an RLHF framework for TTS diffusion models. DLPO integrates the original training loss into the reward function, preserving generative capabilities while reducing inefficiencies. Using naturalness scores as feedback, DLPO aligns reward optimization with the diffusion model's structure, improving speech quality. We evaluate DLPO on WaveGrad 2, a non-autoregressive diffusion-based TTS model. Results show significant improvements in objective metrics (UTMOS 3.65, NISQA 4.02) and subjective evaluations, with DLPO audio preferred 67\% of the time. These findings demonstrate DLPO's potential for efficient, high-quality diffusion TTS in real-time, resource-limited settings.

