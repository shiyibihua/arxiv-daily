---
layout: default
title: Principle-Guided Verilog Optimization: IP-Safe Knowledge Transfer via Local-Cloud Collaboration
---

# Principle-Guided Verilog Optimization: IP-Safe Knowledge Transfer via Local-Cloud Collaboration

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.05675" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.05675v1</a>
  <a href="https://arxiv.org/pdf/2508.05675.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.05675v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.05675v1', 'Principle-Guided Verilog Optimization: IP-Safe Knowledge Transfer via Local-Cloud Collaboration')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Jing Wang, Zheng Li, Lei Li, Fan He, Liyu Lin, Yao Lai, Yan Li, Xiaoyang Zeng, Yufeng Guo

**ÂàÜÁ±ª**: cs.CR, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-05

**Â§áÊ≥®**: Our code and dataset are available at https://github.com/friyawang/VeriOptim

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫IP‰øùÊä§ÁöÑVerilog‰ºòÂåñÊ°ÜÊû∂‰ª•Ëß£ÂÜ≥Á°¨‰ª∂ËÆæËÆ°ÂÆâÂÖ®ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Verilog‰ºòÂåñ` `Áü•ËØÜ‰∫ßÊùÉ‰øùÊä§` `ËæπÁºòËÆ°ÁÆó` `‰∫ëËÆ°ÁÆó` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `Á°¨‰ª∂ËÆæËÆ°` `ÂÆâÂÖ®ÊÄß` `Âçè‰ΩúÊ°ÜÊû∂`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®‰ΩøÁî®‰∫ëÁ´ØLLMsËøõË°åRTL‰ª£Á†Å‰ºòÂåñÊó∂ÔºåÈù¢‰∏¥‰∏•ÈáçÁöÑÁü•ËØÜ‰∫ßÊùÉÊ≥ÑÈú≤È£éÈô©ÔºåÈôêÂà∂‰∫ÜÂÖ∂Â∫îÁî®„ÄÇ
2. Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçËæπÁºò-‰∫ëÂçè‰ΩúÊ°ÜÊû∂ÔºåÂà©Áî®Êú¨Âú∞Â∞èÂûãLLMsËøõË°åÂÆâÂÖ®ÁöÑÊØîËæÉÂàÜÊûêÔºåÂπ∂ÈÄöËøá‰∫ëÁ´ØLLMsËøõË°åÈíàÂØπÊÄß‰ª£Á†ÅÊîπËøõ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÁªìÂêàQwen-2.5-Coder-7B‰∏éDeepseek-V3ÁöÑ‰ºòÂåñÊàêÂäüÁéáËææÂà∞66.67%ÔºåÊòæËëó‰ºò‰∫éÂçïÁã¨‰ΩøÁî®Deepseek-V3ÂíåÂïÜ‰∏öÊ®°ÂûãGPT-4o„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËøëÂπ¥Êù•ÔºåË∂äÊù•Ë∂äÂ§öÁöÑÁ†îÁ©∂ÂÖ≥Ê≥®‰∫éÂà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâËøõË°åÂØÑÂ≠òÂô®‰º†ËæìÁ∫ßÔºàRTLÔºâ‰ª£Á†Å‰ºòÂåñ„ÄÇÁÑ∂ËÄåÔºåÂü∫‰∫é‰∫ëÁöÑLLMsÂú®Â§ÑÁêÜ‰∏ìÊúâÁ°¨‰ª∂ËÆæËÆ°Êó∂Â≠òÂú®‰∏çÂèØÊé•ÂèóÁöÑÁü•ËØÜ‰∫ßÊùÉÔºàIPÔºâÊ≥ÑÈú≤È£éÈô©„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂú∫ÊôØÔºåË¶ÅÊ±ÇÂú®‰∏çÊ≥ÑÈú≤ÊïèÊÑüIP‰ø°ÊÅØÁöÑÊÉÖÂÜµ‰∏ã‰ºòÂåñVerilog‰ª£Á†Å„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫ÜÈ¶ñ‰∏™IP‰øùÊä§ÁöÑËæπÁºò-‰∫ëÂçè‰ΩúÊ°ÜÊû∂ÔºåÁªìÂêà‰∫ÜÊú¨Âú∞Â∞èÂûãLLMs‰∏éÂº∫Â§ßÁöÑ‰∫ëLLMsÁöÑ‰ºòÂäø„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•Ê°ÜÊû∂Âú®‰ºòÂåñÊàêÂäüÁéá‰∏äÊòæËëóÈ´ò‰∫éÂü∫Á∫øÊñπÊ≥ïÔºåÂ±ïÁ§∫‰∫ÜÂú®ÂÆâÂÖ®Á°¨‰ª∂ËÆæËÆ°‰ºòÂåñ‰∏≠Âπ≥Ë°°ÊÄßËÉΩÊèêÂçá‰∏éIP‰øùÊä§ÁöÑÊñ∞ËåÉÂºè„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Âú®‰ºòÂåñVerilog‰ª£Á†ÅÊó∂ÔºåÂ¶Ç‰ΩïÂú®‰∏çÊ≥ÑÈú≤Áü•ËØÜ‰∫ßÊùÉÁöÑÂâçÊèê‰∏ãËøõË°åÊúâÊïàÁöÑ‰ª£Á†ÅÊîπËøõ„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®Â§ÑÁêÜ‰∏ìÊúâÁ°¨‰ª∂ËÆæËÆ°Êó∂ÔºåÂÆπÊòìÂØºËá¥IP‰ø°ÊÅØÁöÑÊ≥ÑÈú≤ÔºåÈôêÂà∂‰∫ÜÂÖ∂Â∫îÁî®ËåÉÂõ¥„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊàë‰ª¨ÊèêÂá∫ÁöÑÊ°ÜÊû∂ÁªìÂêà‰∫ÜÊú¨Âú∞Â∞èÂûãLLMs‰∏é‰∫ëÁ´ØÂº∫Â§ßLLMsÁöÑ‰ºòÂäøÔºåÈÄöËøáÊú¨Âú∞Ê®°ÂûãËøõË°åÂÆâÂÖ®ÁöÑËÆæËÆ°ÂéüÂàôÊèêÂèñÔºåÂÜçÂà©Áî®‰∫ëÁ´ØÊ®°ÂûãËøõË°åÈíàÂØπÊÄß‰ºòÂåñÔºå‰ªéËÄåÁ°Æ‰øùIPÂÆâÂÖ®„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏§‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöÊú¨Âú∞Â∞èÂûãLLMsÁî®‰∫éÊØîËæÉÂàÜÊûêÂíåËÆæËÆ°ÂéüÂàôÊèêÂèñÔºå‰∫ëÁ´ØLLMsÁî®‰∫éÂü∫‰∫éÊèêÂèñÁöÑÂéüÂàôËøõË°å‰ª£Á†Å‰ºòÂåñ„ÄÇÊµÅÁ®ã‰∏∫ÔºöÈ¶ñÂÖàËøõË°åÊú¨Âú∞ÂàÜÊûêÔºåÊèêÂèñËÆæËÆ°ÂéüÂàôÔºåÁÑ∂ÂêéÂ∞ÜËøô‰∫õÂéüÂàôÁî®‰∫éÊü•ËØ¢‰∫ëÁ´ØLLMsËøõË°å‰ºòÂåñ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊú¨Á†îÁ©∂ÁöÑÂàõÊñ∞ÁÇπÂú®‰∫éÈ¶ñÊ¨°ÊèêÂá∫‰∫ÜIP‰øùÊä§ÁöÑËæπÁºò-‰∫ëÂçè‰ΩúÊ°ÜÊû∂ÔºåÁ°Æ‰øù‰∫ÜÂú®‰ºòÂåñËøáÁ®ã‰∏≠‰∏çÊ≥ÑÈú≤ÊïèÊÑü‰ø°ÊÅØÔºå‰∏é‰º†ÁªüÊñπÊ≥ïÁõ∏ÊØîÔºåÊèê‰æõ‰∫ÜÊõ¥È´òÁöÑÂÆâÂÖ®ÊÄßÂíå‰ºòÂåñÊïàÊûú„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Ê®°ÂûãÈÄâÊã©‰∏äÔºå‰ΩøÁî®‰∫ÜQwen-2.5-Coder-7BËøõË°åÊú¨Âú∞ÂàÜÊûêÔºåDeepseek-V3ËøõË°å‰∫ëÁ´Ø‰ºòÂåñ„ÄÇÈÄöËøáÂØπÊØî‰∏çÂêåÊ®°ÂûãÁªÑÂêàÁöÑÊÄßËÉΩÔºåÂèëÁé∞‰∏çÂêåÁöÑÊ®°ÂûãÈÖçÂØπÂú®ÁâπÂÆö‰ºòÂåñÁõÆÊ†á‰∏äË°®Áé∞Âá∫‰∏çÂêåÁöÑ‰ºòÂäø„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÁªìÂêàQwen-2.5-Coder-7B‰∏éDeepseek-V3ÁöÑ‰ºòÂåñÊàêÂäüÁéáËææÂà∞66.67%ÔºåÊòæËëóÈ´ò‰∫éDeepseek-V3ÁöÑ49.81%ÂíåÂïÜ‰∏öÊ®°ÂûãGPT-4oÁöÑ55.81%„ÄÇ‰∏çÂêåÊ®°ÂûãÁªÑÂêàÂú®ÁâπÂÆö‰ºòÂåñÁõÆÊ†á‰∏äÂ±ïÁé∞Âá∫‰∏çÂêåÁöÑ‰ºòÂäøÔºåÊèê‰æõ‰∫ÜÊñ∞ÁöÑÁ†îÁ©∂ÊñπÂêë„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Á°¨‰ª∂ËÆæËÆ°„ÄÅÈõÜÊàêÁîµË∑Ø‰ºòÂåñÂíåÂµåÂÖ•ÂºèÁ≥ªÁªüÂºÄÂèëÁ≠â„ÄÇÈÄöËøáÊèê‰æõÂÆâÂÖ®ÁöÑ‰ºòÂåñÊñπÊ°àÔºåËÉΩÂ§üÊúâÊïà‰øùÊä§‰ºÅ‰∏öÁöÑÁü•ËØÜ‰∫ßÊùÉÔºå‰øÉËøõÁ°¨‰ª∂ËÆæËÆ°ÁöÑÂàõÊñ∞‰∏éÂèëÂ±ïÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÈïøËøúÂΩ±Âìç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Recent years have witnessed growing interest in adopting large language models (LLMs) for Register Transfer Level (RTL) code optimization. While powerful cloud-based LLMs offer superior optimization capabilities, they pose unacceptable intellectual property (IP) leakage risks when processing proprietary hardware designs. In this paper, we propose a new scenario where Verilog code must be optimized for specific attributes without leaking sensitive IP information. We introduce the first IP-preserving edge-cloud collaborative framework that leverages the benefits of both paradigms. Our approach employs local small LLMs (e.g., Qwen-2.5-Coder-7B) to perform secure comparative analysis between paired high-quality target designs and novice draft codes, yielding general design principles that summarize key insights for improvements. These principles are then used to query stronger cloud LLMs (e.g., Deepseek-V3) for targeted code improvement, ensuring that only abstracted and IP-safe guidance reaches external services. Our experimental results demonstrate that the framework achieves significantly higher optimization success rates compared to baseline methods. For example, combining Qwen-2.5-Coder-7B and Deepseek-V3 achieves a 66.67\% optimization success rate for power utilization, outperforming Deepseek-V3 alone (49.81\%) and even commercial models like GPT-4o (55.81\%). Further investigation of local and cloud LLM combinations reveals that different model pairings exhibit varying strengths for specific optimization objectives, with interesting trends emerging when varying the number of comparative code pairs. Our work establishes a new paradigm for secure hardware design optimization that balances performance gains with IP protection.

