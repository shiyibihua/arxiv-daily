---
layout: default
title: Beyond Policy Optimization: A Data Curation Flywheel for Sparse-Reward Long-Horizon Planning
---

# Beyond Policy Optimization: A Data Curation Flywheel for Sparse-Reward Long-Horizon Planning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.03018" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.03018v1</a>
  <a href="https://arxiv.org/pdf/2508.03018.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.03018v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.03018v1', 'Beyond Policy Optimization: A Data Curation Flywheel for Sparse-Reward Long-Horizon Planning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yutong Wang, Pengliang Ji, Kaixin Li, Baolong Bi, Tao Feng, Guillaume Sartoretti

**åˆ†ç±»**: cs.AI, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-08-05

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºBPOæ¡†æ¶ä»¥è§£å†³ç¨€ç–å¥–åŠ±é•¿æ—¶é—´è§„åˆ’é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿æ—¶é—´è§„åˆ’` `ç¨€ç–å¥–åŠ±` `è‡ªæˆ‘æ”¹è¿›` `æ•°æ®é£è½®` `å¤æ‚æ€§åˆ†å±‚å­¦ä¹ ` `å¼ºåŒ–å­¦ä¹ ` `æ¨ç†æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨ç¨€ç–å¥–åŠ±ç¯å¢ƒä¸­é¢ä¸´ä¿¡ç”¨åˆ†é…å›°éš¾ï¼Œå¯¼è‡´å¼ºåŒ–å­¦ä¹ æ•ˆæœä¸ä½³ã€‚
2. æå‡ºBPOæ¡†æ¶ï¼Œé€šè¿‡å¼•å¯¼ã€å¤–æ¨å’Œç²¾ç‚¼ä¸‰ä¸ªé˜¶æ®µï¼Œå»ºç«‹è‡ªæˆ‘æ”¹è¿›çš„æ•°æ®é£è½®æ¥æå‡æ¨ç†æ¨¡å‹çš„æ•ˆç‡ã€‚
3. åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒBPOæ¡†æ¶æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„çŠ¶æ€ï¼Œå¹¶æå‡äº†ä»¤ç‰Œä½¿ç”¨æ•ˆç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨ç†æ¨¡å‹åœ¨é™æ€ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†åœ¨äº¤äº’ç¯å¢ƒä¸­çš„å¤šè½®æ™ºèƒ½è§„åˆ’é¢ä¸´ä¸¤ä¸ªåŸºæœ¬æŒ‘æˆ˜ï¼šä¸€æ˜¯ç¨€ç–å¥–åŠ±ç¯å¢ƒä¸­çš„ä¿¡ç”¨åˆ†é…é—®é¢˜ä½¿å¾—ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ æ•ˆæœä¸ä½³ï¼›äºŒæ˜¯é€æ­¥æ¨ç†å†å²çš„è®¡ç®—å¼€é”€è¿‡å¤§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†BPOæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡å¼•å¯¼ã€å¤–æ¨å’Œç²¾ç‚¼ä¸‰ä¸ªé˜¶æ®µå»ºç«‹è‡ªæˆ‘æ”¹è¿›çš„æ•°æ®é£è½®ï¼Œä»¥å¼€å‘é€‚ç”¨äºé•¿æ—¶é—´ã€ç¨€ç–å¥–åŠ±ç¯å¢ƒçš„é²æ£’æ¨ç†æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ALFWorldã€ScienceWorldå’ŒWebShopä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾è‘—æé«˜äº†ä»¤ç‰Œæ•ˆç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨ç¨€ç–å¥–åŠ±ç¯å¢ƒä¸­è¿›è¡Œé•¿æ—¶é—´è§„åˆ’æ—¶ï¼Œä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ æ–¹æ³•é¢ä¸´çš„ä¿¡ç”¨åˆ†é…é—®é¢˜å’Œè®¡ç®—å¼€é”€è¿‡å¤§çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤šè½®æ¨ç†æ—¶æ•ˆç‡ä½ä¸‹ï¼Œéš¾ä»¥é€‚åº”å¤æ‚çš„äº¤äº’ç¯å¢ƒã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šBPOæ¡†æ¶é€šè¿‡å¼•å¯¼ã€å¤–æ¨å’Œç²¾ç‚¼ä¸‰ä¸ªé˜¶æ®µï¼Œåˆ©ç”¨é•¿çŸ­é“¾æ€ç»´èåˆçš„è§„åˆ’å››å…ƒç»„æ¥é«˜æ•ˆå¼•å¯¼æ¨ç†ï¼Œè¿›è€Œå®ç°è‡ªæˆ‘æ”¹è¿›ã€‚è¯¥è®¾è®¡æ—¨åœ¨é€šè¿‡é€æ­¥ä¼˜åŒ–æ¨ç†è¿‡ç¨‹ï¼Œå…‹æœç¨€ç–å¥–åŠ±å¸¦æ¥çš„å›°éš¾ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šBPOæ¡†æ¶åˆ†ä¸ºä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼šå¼•å¯¼é˜¶æ®µä½¿ç”¨è§„åˆ’å››å…ƒç»„è¿›è¡Œé«˜æ•ˆæ¨ç†ï¼›å¤–æ¨é˜¶æ®µé€šè¿‡å¤æ‚æ€§åˆ†å±‚è¯¾ç¨‹å­¦ä¹ æ‰©å±•åˆ°åˆ†å¸ƒå¤–ä»»åŠ¡ï¼›ç²¾ç‚¼é˜¶æ®µåˆ™é€šè¿‡å¥–åŠ±é—¨æ§æ‹’ç»é‡‡æ ·é€‰æ‹©ç»éªŒè¿›è¡Œè‡ªæˆ‘ä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šBPOæ¡†æ¶çš„åˆ›æ–°åœ¨äºå»ºç«‹äº†ä¸€ä¸ªè‡ªæˆ‘æ”¹è¿›çš„æ•°æ®é£è½®ï¼Œèƒ½å¤Ÿåœ¨ç¨€ç–å¥–åŠ±ç¯å¢ƒä¸­æœ‰æ•ˆæå‡æ¨ç†æ¨¡å‹çš„æ€§èƒ½ï¼ŒåŒºåˆ«äºä¼ ç»Ÿæ–¹æ³•çš„å•ä¸€ä¼˜åŒ–ç­–ç•¥ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å¼•å¯¼é˜¶æ®µï¼Œé‡‡ç”¨é•¿çŸ­é“¾æ€ç»´èåˆçš„æŠ€æœ¯ï¼›åœ¨å¤–æ¨é˜¶æ®µï¼Œå®æ–½å¤æ‚æ€§åˆ†å±‚è¯¾ç¨‹å­¦ä¹ ï¼›åœ¨ç²¾ç‚¼é˜¶æ®µï¼Œä½¿ç”¨å¥–åŠ±é—¨æ§æ‹’ç»é‡‡æ ·æ¥é€‰æ‹©è®­ç»ƒç»éªŒï¼Œç¡®ä¿æ¨¡å‹çš„æŒç»­æ”¹è¿›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒBPOæ¡†æ¶åœ¨ALFWorldã€ScienceWorldå’ŒWebShopç­‰åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾è‘—æé«˜äº†ä»¤ç‰Œæ•ˆç‡ï¼Œç›¸æ¯”äºä¼ ç»Ÿæ–¹æ³•ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°XX%ï¼ˆå…·ä½“æ•°æ®å¾…è¡¥å……ï¼‰ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººå¯¼èˆªã€æ¸¸æˆAIã€æ™ºèƒ½åŠ©æ‰‹ç­‰éœ€è¦è¿›è¡Œå¤æ‚å†³ç­–çš„äº¤äº’å¼ç¯å¢ƒã€‚é€šè¿‡æå‡æ¨¡å‹åœ¨ç¨€ç–å¥–åŠ±ç¯å¢ƒä¸­çš„æ¨ç†èƒ½åŠ›ï¼ŒBPOæ¡†æ¶èƒ½å¤Ÿä¸ºè¿™äº›é¢†åŸŸæä¾›æ›´é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œæ¨åŠ¨æ™ºèƒ½ä½“è§„åˆ’æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Reasoning Models have demonstrated remarkable success on static tasks, yet their application to multi-round agentic planning in interactive environments faces two fundamental challenges. First, the intractable credit assignment problem renders conventional reinforcement learning ineffective in sparse-reward settings. Second, the computational overhead of verbose, step-by-step reasoning histories is prohibitive. To address these challenges, we propose BPO, a three-stage framework (bootstrapping, extrapolation, and refinement) that establishes a self-improving data flywheel to develop robust reasoning models for long-horizon, sparse-reward environments. Our framework first bootstraps efficient reasoning using the proposed planning quaternions with long-short chain-of-thought fusion. It then extrapolates to out-of-distribution tasks through complexity-stratified curriculum learning. Finally, the model iteratively refines itself by learning exclusively on experiences selected via reward-gated rejection sampling. Experiments on ALFWorld, ScienceWorld, and WebShop demonstrate that our approach achieves state-of-the-art with significant token efficiency, providing a new recipe for reasoning models in agentic planning.

