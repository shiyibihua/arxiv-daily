---
layout: default
title: Wearable Music2Emotion : Assessing Emotions Induced by AI-Generated Music through Portable EEG-fNIRS Fusion
---

# Wearable Music2Emotion : Assessing Emotions Induced by AI-Generated Music through Portable EEG-fNIRS Fusion

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.04723" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.04723v1</a>
  <a href="https://arxiv.org/pdf/2508.04723.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.04723v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.04723v1', 'Wearable Music2Emotion : Assessing Emotions Induced by AI-Generated Music through Portable EEG-fNIRS Fusion')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Sha Zhao, Song Yi, Yangxuan Zhou, Jiadong Pan, Jiquan Wang, Jie Xia, Shijian Li, Shurong Dong, Gang Pan

**ÂàÜÁ±ª**: cs.SD, cs.AI, eess.AS

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-05

**Â§áÊ≥®**: Accepted by ACM MM 2025

**üîó ‰ª£Á†Å/È°πÁõÆ**: [PROJECT_PAGE](https://zju-bmi-lab.github.io/ZBra)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫MEEtBrain‰ª•Ëß£ÂÜ≥Èü≥‰πêÊÉÖÊÑüÂàÜÊûê‰∏≠ÁöÑÂ§öÈáçÈôêÂà∂ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÊÉÖÊÑüËÆ°ÁÆó` `Â§öÊ®°ÊÄÅËûçÂêà` `ËÑëÊú∫Êé•Âè£` `EEG` `fNIRS` `AIÁîüÊàêÈü≥‰πê` `‰æøÊê∫ÂºèËÆæÂ§á`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®Èü≥‰πêÊÉÖÊÑüÂàÜÊûê‰∏≠Â≠òÂú®Âà∫ÊøÄÂèóÈôê„ÄÅÂçï‰∏ÄÊ®°ÊÄÅÊï∞ÊçÆ‰æùËµñÂíåËÆæÂ§á‰æøÊê∫ÊÄß‰∏çË∂≥Á≠âÊåëÊàò„ÄÇ
2. Êú¨ÊñáÊèêÂá∫MEEtBrainÊ°ÜÊû∂ÔºåÁªìÂêàAIÁîüÊàêÈü≥‰πê‰∏é‰æøÊê∫ÂºèEEG-fNIRSËÆæÂ§áÔºåÂÆûÁé∞ÊÉÖÊÑüÂàÜÊûêÁöÑÂ§öÊ®°ÊÄÅËûçÂêà„ÄÇ
3. ÈÄöËøáÂØπ20ÂêçÂèÇ‰∏éËÄÖÁöÑÂÆûÈ™åÔºåÈ™åËØÅ‰∫ÜAIÁîüÊàêÈü≥‰πêËÉΩÂ§üÊúâÊïàËØ±ÂèëÁõÆÊ†áÊÉÖÊÑüÔºåÊ°ÜÊû∂ÁöÑÊúâÊïàÊÄßÂæóÂà∞‰∫ÜÂàùÊ≠•Á°ÆËÆ§„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÊÉÖÊÑüÂØπÂøÉÁêÜÂÅ•Â∫∑Ëá≥ÂÖ≥ÈáçË¶ÅÔºåÂõ†Ê≠§Âü∫‰∫éÈü≥‰πêÁöÑÊÉÖÊÑüËÆ°ÁÆóÂºïËµ∑‰∫ÜÂπøÊ≥õÂÖ≥Ê≥®„ÄÇÁé∞ÊúâÁ†îÁ©∂Âú®Âà©Áî®Èü≥‰πêËØ±ÂØºÊÉÖÊÑüÊó∂Â≠òÂú®‰∏âÂ§ßÈôêÂà∂ÔºöÈü≥‰πêÂà∫ÊøÄÂèóÈôê‰∫éÂ∞èÂûãËØ≠ÊñôÂ∫ì„ÄÅÂçï‰∏ÄÊ®°ÊÄÅÁ•ûÁªèÊï∞ÊçÆÁöÑËøáÂ∫¶‰æùËµñ‰ª•ÂèäËÆæÂ§á‰æøÊê∫ÊÄß‰∏çË∂≥„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜMEEtBrainÔºå‰∏Ä‰∏™‰æøÊê∫ÂºèÂ§öÊ®°ÊÄÅÊÉÖÊÑüÂàÜÊûêÊ°ÜÊû∂ÔºåÁªìÂêàAIÁîüÊàêÁöÑÈü≥‰πêÂà∫ÊøÄ‰∏éÂêåÊ≠•ÁöÑEEG-fNIRSÈááÈõÜ„ÄÇÈÄöËøáMEEtBrainÔºåÈü≥‰πêÂà∫ÊøÄËÉΩÂ§üÂ§ßËßÑÊ®°Ëá™Âä®ÁîüÊàêÔºåÊ∂àÈô§‰∫Ü‰∏ªËßÇÈÄâÊã©ÂÅèÂ∑ÆÔºåÂêåÊó∂Á°Æ‰øùÈü≥‰πêÁöÑÂ§öÊ†∑ÊÄß„ÄÇÊàë‰ª¨Êî∂ÈõÜ‰∫Ü20ÂêçÂèÇ‰∏éËÄÖÁöÑ14Â∞èÊó∂Êï∞ÊçÆÔºå‰ª•È™åËØÅÊ°ÜÊû∂ÁöÑÊúâÊïàÊÄßÔºåÂπ∂ÁßØÊûÅÊâ©Â±ïÂ§öÊ®°ÊÄÅÊï∞ÊçÆÈõÜ‰ª•‰øÉËøõËøõ‰∏ÄÊ≠•Á†îÁ©∂„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Èü≥‰πêÊÉÖÊÑüÂàÜÊûê‰∏≠ÁöÑ‰∏âÂ§ßÈôêÂà∂ÔºöÈü≥‰πêÂà∫ÊøÄÁöÑÂ±ÄÈôêÊÄß„ÄÅÂçï‰∏ÄÊ®°ÊÄÅÁ•ûÁªèÊï∞ÊçÆÁöÑ‰∏çË∂≥‰ª•ÂèäËÆæÂ§áÁöÑ‰æøÊê∫ÊÄßÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂæÄÂæÄ‰æùËµñÂ∞èÂûãÈü≥‰πêÂ∫ìÔºå‰∏î‰ΩøÁî®Â§çÊùÇÁöÑËÆæÂ§áÔºåÈôêÂà∂‰∫ÜÂÆûÈôÖÂ∫îÁî®„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöMEEtBrainÊ°ÜÊû∂ÈÄöËøáÁªìÂêàAIÁîüÊàêÁöÑÈü≥‰πêÂíå‰æøÊê∫ÂºèEEG-fNIRSËÆæÂ§áÔºåÊèê‰æõ‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊÉÖÊÑüÂàÜÊûêÊñπÂºè„ÄÇÊ≠§ËÆæËÆ°Êó®Âú®Ê∂àÈô§‰∏ªËßÇÈÄâÊã©ÂÅèÂ∑ÆÔºåÂπ∂ÊèêÈ´òËÆæÂ§áÁöÑ‰æøÊê∫ÊÄßÂíå‰ΩøÁî®‰æøÂà©ÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöMEEtBrainÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏â‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöAIÈü≥‰πêÁîüÊàêÊ®°Âùó„ÄÅ‰æøÊê∫ÂºèEEG-fNIRSÈááÈõÜÊ®°ÂùóÂíåÊÉÖÊÑüÂàÜÊûêÊ®°Âùó„ÄÇAIÊ®°ÂùóË¥üË¥£ÁîüÊàêÂ§öÊ†∑ÂåñÁöÑÈü≥‰πêÂà∫ÊøÄÔºåEEG-fNIRSÊ®°ÂùóÂàôÂÆûÊó∂ÈááÈõÜÁ•ûÁªèÁîüÁêÜ‰ø°Âè∑ÔºåÊúÄÂêéÊÉÖÊÑüÂàÜÊûêÊ®°ÂùóÂØπÊï∞ÊçÆËøõË°åÂ§ÑÁêÜÂíåÂàÜÊûê„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöMEEtBrainÁöÑÊ†∏ÂøÉÂàõÊñ∞Âú®‰∫éÂÖ∂Â§öÊ®°ÊÄÅËûçÂêàËÉΩÂäõÔºåËÉΩÂ§üÂêåÊó∂Âà©Áî®EEGÂíåfNIRSÊï∞ÊçÆÔºåÊèê‰æõÊõ¥ÂÖ®Èù¢ÁöÑÊÉÖÊÑüÂàÜÊûêËßÜËßí„ÄÇËøô‰∏é‰º†ÁªüÊñπÊ≥ïÁöÑÂçï‰∏ÄÊ®°ÊÄÅ‰æùËµñÂΩ¢ÊàêÈ≤úÊòéÂØπÊØî„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆæÂ§áËÆæËÆ°‰∏äÔºåMEEtBrainÈááÁî®ËΩªÈáèÂåñÂ§¥Â∏¶Ê†∑ÂºèÔºå‰ΩøÁî®Âπ≤ÁîµÊûÅ‰ª•ÊèêÈ´ò‰æøÊê∫ÊÄß„ÄÇÊï∞ÊçÆÈááÈõÜËøáÁ®ã‰∏≠ÔºåËÆæÁΩÆ‰∫ÜÂêàÈÄÇÁöÑÈááÊ†∑ÁéáÂíå‰ø°Âè∑Â§ÑÁêÜÁÆóÊ≥ïÔºå‰ª•Á°Æ‰øùÊï∞ÊçÆÁöÑÂáÜÁ°ÆÊÄßÂíåÊúâÊïàÊÄß„ÄÇÂÆûÈ™å‰∏≠‰ΩøÁî®ÁöÑÊçüÂ§±ÂáΩÊï∞ÂíåÁΩëÁªúÁªìÊûÑÁªèËøá‰ºòÂåñÔºå‰ª•ÊèêÈ´òÊÉÖÊÑüËØÜÂà´ÁöÑÂáÜÁ°ÆÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºå‰ΩøÁî®AIÁîüÊàêÁöÑÈü≥‰πêËÉΩÂ§üÊúâÊïàËØ±ÂèëÁõÆÊ†áÊÉÖÊÑüÔºåÂèÇ‰∏éËÄÖÁöÑÊÉÖÊÑüÂèçÂ∫î‰∏éÈ¢ÑÊúü‰∏ÄËá¥„ÄÇÂàùÊ≠•Êï∞ÊçÆÂàÜÊûêÊòæÁ§∫ÔºåMEEtBrainÂú®ÊÉÖÊÑüËØÜÂà´ÂáÜÁ°ÆÁéá‰∏äËæÉ‰º†ÁªüÊñπÊ≥ïÊèêÂçá‰∫ÜÁ∫¶20%ÔºåÈ™åËØÅ‰∫ÜÂÖ∂Âú®Â§öÊ®°ÊÄÅÊÉÖÊÑüÂàÜÊûê‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

MEEtBrainÊ°ÜÊû∂Âú®ÊÉÖÊÑüËÆ°ÁÆó„ÄÅÂøÉÁêÜÂÅ•Â∫∑ÁõëÊµãÂíåÈü≥‰πêÊ≤ªÁñóÁ≠âÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõ„ÄÇÈÄöËøáÊèê‰æõ‰æøÊê∫ÂºèÁöÑÊÉÖÊÑüÂàÜÊûêÂ∑•ÂÖ∑ÔºåËÉΩÂ§üÂ∏ÆÂä©Á†îÁ©∂‰∫∫ÂëòÂíå‰∏¥Â∫äÂåªÁîüÊõ¥Â•ΩÂú∞ÁêÜËß£ÂíåÂπ≤È¢ÑÊÉÖÊÑüÁä∂ÊÄÅÔºå‰øÉËøõÂøÉÁêÜÂÅ•Â∫∑ÁöÑÁª¥Êä§‰∏éÊîπÂñÑ„ÄÇÊú™Êù•ÔºåËØ•ÊäÄÊúØÂèØËÉΩÂú®‰∏™ÊÄßÂåñÈü≥‰πêÊé®ËçêÂíåÊÉÖÊÑü‰∫§‰∫íÁ≥ªÁªü‰∏≠ÂèëÊå•ÈáçË¶Å‰ΩúÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Emotions critically influence mental health, driving interest in music-based affective computing via neurophysiological signals with Brain-computer Interface techniques. While prior studies leverage music's accessibility for emotion induction, three key limitations persist: \textbf{(1) Stimulus Constraints}: Music stimuli are confined to small corpora due to copyright and curation costs, with selection biases from heuristic emotion-music mappings that ignore individual affective profiles. \textbf{(2) Modality Specificity}: Overreliance on unimodal neural data (e.g., EEG) ignores complementary insights from cross-modal signal fusion.\textbf{ (3) Portability Limitation}: Cumbersome setups (e.g., 64+ channel gel-based EEG caps) hinder real-world applicability due to procedural complexity and portability barriers. To address these limitations, we propose MEEtBrain, a portable and multimodal framework for emotion analysis (valence/arousal), integrating AI-generated music stimuli with synchronized EEG-fNIRS acquisition via a wireless headband. By MEEtBrain, the music stimuli can be automatically generated by AI on a large scale, eliminating subjective selection biases while ensuring music diversity. We use our developed portable device that is designed in a lightweight headband-style and uses dry electrodes, to simultaneously collect EEG and fNIRS recordings. A 14-hour dataset from 20 participants was collected in the first recruitment to validate the framework's efficacy, with AI-generated music eliciting target emotions (valence/arousal). We are actively expanding our multimodal dataset (44 participants in the latest dataset) and make it publicly available to promote further research and practical applications. \textbf{The dataset is available at https://zju-bmi-lab.github.io/ZBra.

