---
layout: default
title: Evo-MARL: Co-Evolutionary Multi-Agent Reinforcement Learning for Internalized Safety
---

# Evo-MARL: Co-Evolutionary Multi-Agent Reinforcement Learning for Internalized Safety

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.03864" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.03864v2</a>
  <a href="https://arxiv.org/pdf/2508.03864.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.03864v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.03864v2', 'Evo-MARL: Co-Evolutionary Multi-Agent Reinforcement Learning for Internalized Safety')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhenyu Pan, Yiting Zhang, Yutong Zhang, Jianshu Zhang, Haozheng Luo, Yuwei Han, Dennis Wu, Hong-Yu Chen, Philip S. Yu, Manling Li, Han Liu

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-05 (æ›´æ–°: 2025-09-06)

**å¤‡æ³¨**: accepted by the Trustworthy FMs workshop in ICCV 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEvo-MARLä»¥è§£å†³å¤šæ™ºèƒ½ä½“ç³»ç»Ÿå®‰å…¨æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ` `å¼ºåŒ–å­¦ä¹ ` `å®‰å…¨æ€§` `å¯¹æŠ—æ”»å‡»` `æ¼”åŒ–æœç´¢` `é²æ£’æ€§` `å†…åŒ–æœºåˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„é˜²å¾¡æ–¹æ³•ä¾èµ–å¤–éƒ¨å®‰å…¨ä»£ç†ï¼Œé¢ä¸´ä¿æŠ¤èƒ½åŠ›æœ‰é™å’Œå•ç‚¹æ•…éšœçš„æŒ‘æˆ˜ã€‚
2. Evo-MARLé€šè¿‡è®­ç»ƒæ¯ä¸ªæ™ºèƒ½ä½“åŒæ—¶æ‰§è¡Œä¸»è¦åŠŸèƒ½å’ŒæŠµå¾¡å¯¹æŠ—å¨èƒï¼Œå†…åŒ–å®‰å…¨æœºåˆ¶ï¼Œæå‡ç³»ç»Ÿé²æ£’æ€§ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒEvo-MARLåœ¨æ”»å‡»æˆåŠŸç‡ä¸Šé™ä½äº†22%ï¼Œåœ¨æ¨ç†ä»»åŠ¡ä¸Šå‡†ç¡®ç‡æé«˜äº†5%ï¼Œå®ç°å®‰å…¨æ€§ä¸æ•ˆç”¨çš„å…±åŒæå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰å±•ç°å‡ºå¼ºå¤§çš„åä½œèƒ½åŠ›å’Œæ€§èƒ½ã€‚ç„¶è€Œï¼Œéšç€å¼€æ”¾æ€§å’Œäº¤äº’å¤æ‚æ€§çš„å¢åŠ ï¼Œç³»ç»Ÿé¢ä¸´ä¸¥é‡çš„å®‰å…¨é£é™©ï¼Œå¦‚è¶Šç‹±å’Œå¯¹æŠ—æ”»å‡»ã€‚ç°æœ‰é˜²å¾¡æ–¹æ³•é€šå¸¸ä¾èµ–å¤–éƒ¨å®‰å…¨æ¨¡å—ï¼Œå­˜åœ¨ä¿æŠ¤èƒ½åŠ›æœ‰é™å’Œå•ç‚¹æ•…éšœçš„æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºEvo-MARLï¼Œä¸€ä¸ªæ–°é¢–çš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œä½¿æ‰€æœ‰ä»»åŠ¡æ™ºèƒ½ä½“å…±åŒè·å¾—é˜²å¾¡èƒ½åŠ›ï¼Œå†…åŒ–å®‰å…¨æœºåˆ¶ï¼Œå¹¶åœ¨å…±åŒæ¼”åŒ–çš„å¨èƒä¸‹æŒç»­æå‡MASæ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEvo-MARLå°†æ”»å‡»æˆåŠŸç‡é™ä½äº†22%ï¼ŒåŒæ—¶åœ¨æ¨ç†ä»»åŠ¡ä¸Šæé«˜äº†5%çš„å‡†ç¡®ç‡ï¼Œè¯æ˜äº†å®‰å…¨æ€§å’Œæ•ˆç”¨çš„å…±åŒæå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨å¼€æ”¾æ€§å’Œå¤æ‚äº¤äº’ä¸‹çš„å®‰å…¨æ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–å¤–éƒ¨å®‰å…¨ä»£ç†ï¼Œå¯¼è‡´ä¿æŠ¤èƒ½åŠ›æœ‰é™ä¸”å­˜åœ¨å•ç‚¹æ•…éšœé£é™©ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šEvo-MARLçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼Œä½¿æ¯ä¸ªæ™ºèƒ½ä½“åœ¨æ‰§è¡Œä»»åŠ¡çš„åŒæ—¶å…·å¤‡é˜²å¾¡èƒ½åŠ›ï¼Œä»è€Œå†…åŒ–å®‰å…¨æœºåˆ¶ï¼Œé¿å…ä¾èµ–å¤–éƒ¨æ¨¡å—ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šEvo-MARLæ¡†æ¶åŒ…æ‹¬ä»»åŠ¡æ™ºèƒ½ä½“å’Œé˜²å¾¡æ™ºèƒ½ä½“çš„å…±åŒè®­ç»ƒï¼Œç»“åˆæ¼”åŒ–æœç´¢ä¸å‚æ•°å…±äº«çš„å¼ºåŒ–å­¦ä¹ ï¼Œå½¢æˆæ”»å‡»è€…ä¸é˜²å¾¡è€…çš„å…±æ¼”åŒ–è¿‡ç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šEvo-MARLçš„ä¸»è¦åˆ›æ–°åœ¨äºå°†é˜²å¾¡èƒ½åŠ›å†…åŒ–åˆ°æ¯ä¸ªæ™ºèƒ½ä½“ä¸­ï¼Œé¿å…äº†å¤–éƒ¨å®‰å…¨æ¨¡å—çš„å±€é™æ€§ï¼Œæå‡äº†ç³»ç»Ÿçš„æ•´ä½“é²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†å‚æ•°å…±äº«çš„å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼ŒæŸå¤±å‡½æ•°ç»“åˆäº†ä»»åŠ¡æ€§èƒ½ä¸é˜²å¾¡èƒ½åŠ›çš„æƒè¡¡ï¼Œç¡®ä¿æ™ºèƒ½ä½“åœ¨é¢å¯¹å¯¹æŠ—å¨èƒæ—¶çš„æœ‰æ•ˆæ€§ã€‚æ•´ä½“æ¶æ„æ”¯æŒåŠ¨æ€è°ƒæ•´ï¼Œä»¥é€‚åº”ä¸æ–­å˜åŒ–çš„æ”»å‡»æ¨¡å¼ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒEvo-MARLåœ¨å¯¹æŠ—æ”»å‡»ä¸‹çš„æˆåŠŸç‡é™ä½äº†22%ï¼ŒåŒæ—¶åœ¨æ¨ç†ä»»åŠ¡çš„å‡†ç¡®ç‡ä¸Šæé«˜äº†5%ã€‚è¿™ä¸€ç»“æœè¡¨æ˜ï¼ŒEvo-MARLæœ‰æ•ˆåœ°å®ç°äº†å®‰å…¨æ€§ä¸æ•ˆç”¨çš„åŒé‡æå‡ï¼Œè¶…è¶Šäº†ç°æœ‰çš„é˜²å¾¡æ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Evo-MARLçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼ŒåŒ…æ‹¬æ™ºèƒ½äº¤é€šç³»ç»Ÿã€é‡‘èäº¤æ˜“ç›‘æ§å’Œç½‘ç»œå®‰å…¨ç­‰ã€‚é€šè¿‡æå‡å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„å®‰å…¨æ€§å’Œé²æ£’æ€§ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆåº”å¯¹å¤æ‚ç¯å¢ƒä¸­çš„å¯¹æŠ—æ€§å¨èƒï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multi-agent systems (MAS) built on multimodal large language models exhibit strong collaboration and performance. However, their growing openness and interaction complexity pose serious risks, notably jailbreak and adversarial attacks. Existing defenses typically rely on external guard modules, such as dedicated safety agents, to handle unsafe behaviors. Unfortunately, this paradigm faces two challenges: (1) standalone agents offer limited protection, and (2) their independence leads to single-point failure-if compromised, system-wide safety collapses. Naively increasing the number of guard agents further raises cost and complexity. To address these challenges, we propose Evo-MARL, a novel multi-agent reinforcement learning (MARL) framework that enables all task agents to jointly acquire defensive capabilities. Rather than relying on external safety modules, Evo-MARL trains each agent to simultaneously perform its primary function and resist adversarial threats, ensuring robustness without increasing system overhead or single-node failure. Furthermore, Evo-MARL integrates evolutionary search with parameter-sharing reinforcement learning to co-evolve attackers and defenders. This adversarial training paradigm internalizes safety mechanisms and continually enhances MAS performance under co-evolving threats. Experiments show that Evo-MARL reduces attack success rates by up to 22% while boosting accuracy by up to 5% on reasoning tasks-demonstrating that safety and utility can be jointly improved.

