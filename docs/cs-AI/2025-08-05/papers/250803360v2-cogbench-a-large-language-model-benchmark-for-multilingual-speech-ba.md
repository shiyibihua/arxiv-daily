---
layout: default
title: CogBench: A Large Language Model Benchmark for Multilingual Speech-Based Cognitive Impairment Assessment
---

# CogBench: A Large Language Model Benchmark for Multilingual Speech-Based Cognitive Impairment Assessment

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.03360" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.03360v2</a>
  <a href="https://arxiv.org/pdf/2508.03360.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.03360v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.03360v2', 'CogBench: A Large Language Model Benchmark for Multilingual Speech-Based Cognitive Impairment Assessment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Rui Feng, Zhiyao Luo, Wei Wang, Yuting Song, Yong Liu, Tingting Zhu, Jianqing Li, Xingyao Wang

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-05 (æ›´æ–°: 2025-10-17)

**å¤‡æ³¨**: 19 pages, 9 figures, 12 tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCogBenchä»¥è§£å†³å¤šè¯­è¨€è®¤çŸ¥éšœç¢è¯„ä¼°çš„é€šç”¨æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è®¤çŸ¥éšœç¢è¯„ä¼°` `å¤šè¯­è¨€å¤„ç†` `å¤§å‹è¯­è¨€æ¨¡å‹` `è·¨é¢†åŸŸé€‚åº”` `ä½ç§©é€‚åº”` `æ·±åº¦å­¦ä¹ ` `ä¸´åºŠåº”ç”¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è®¤çŸ¥éšœç¢è¯„ä¼°æ–¹æ³•åœ¨ä¸åŒè¯­è¨€å’Œä¸´åºŠç¯å¢ƒä¸­çš„é€šç”¨æ€§ä¸è¶³ï¼Œé™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚
2. æœ¬ç ”ç©¶æå‡ºCogBenchåŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šè¯­è¨€è¯­éŸ³è®¤çŸ¥éšœç¢è¯„ä¼°ä¸­çš„è¡¨ç°ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¼ ç»Ÿæ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨è·¨é¢†åŸŸè½¬ç§»æ—¶æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œè€ŒLLMsé€šè¿‡LoRAå¾®è°ƒåè¡¨ç°å‡ºæ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è‡ªåŠ¨è¯„ä¼°è®¤çŸ¥éšœç¢çš„è‡ªå‘è¯­éŸ³æä¾›äº†ä¸€ç§æœ‰å‰æ™¯çš„éä¾µå…¥æ€§æ—©æœŸç­›æŸ¥æ–¹æ³•ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨ä¸åŒè¯­è¨€å’Œä¸´åºŠç¯å¢ƒä¸­çš„é€šç”¨æ€§ä¸è¶³ï¼Œé™åˆ¶äº†å…¶å®ç”¨æ€§ã€‚æœ¬ç ”ç©¶æå‡ºäº†CogBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¯­éŸ³åŸºç¡€è®¤çŸ¥éšœç¢è¯„ä¼°ä¸­çš„è·¨è¯­è¨€å’Œè·¨åœºæ‰€é€šç”¨æ€§çš„åŸºå‡†ã€‚é€šè¿‡ç»Ÿä¸€çš„å¤šæ¨¡æ€ç®¡é“ï¼Œæˆ‘ä»¬åœ¨æ¶µç›–è‹±è¯­å’Œæ™®é€šè¯çš„ä¸‰ä¸ªè¯­éŸ³æ•°æ®é›†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼Œä¼ ç»Ÿæ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨è·¨é¢†åŸŸè½¬ç§»æ—¶æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œè€Œé…å¤‡é“¾å¼æ€ç»´æç¤ºçš„LLMsè¡¨ç°å‡ºæ›´å¥½çš„é€‚åº”æ€§ï¼Œå°½ç®¡å…¶æ€§èƒ½å¯¹æç¤ºè®¾è®¡æ•æ„Ÿã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ¢ç´¢äº†é€šè¿‡ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰å¯¹LLMsè¿›è¡Œè½»é‡çº§å¾®è°ƒï¼Œè¿™æ˜¾è‘—æé«˜äº†ç›®æ ‡é¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™äº›å‘ç°ä¸ºæ„å»ºä¸´åºŠå®ç”¨ä¸”è¯­è¨€ä¸Šç¨³å¥çš„è¯­éŸ³åŸºç¡€è®¤çŸ¥è¯„ä¼°å·¥å…·è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³ç°æœ‰è®¤çŸ¥éšœç¢è¯„ä¼°æ–¹æ³•åœ¨ä¸åŒè¯­è¨€å’Œä¸´åºŠç¯å¢ƒä¸­çš„é€šç”¨æ€§ä¸è¶³é—®é¢˜ã€‚ä¼ ç»Ÿæ–¹æ³•åœ¨è·¨è¯­è¨€å’Œè·¨åœºæ‰€çš„åº”ç”¨ä¸­è¡¨ç°ä¸ä½³ï¼Œé™åˆ¶äº†å…¶ä¸´åºŠå®ç”¨æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCogBenchåŸºå‡†çš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡ç»Ÿä¸€çš„å¤šæ¨¡æ€ç®¡é“è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¯­éŸ³åŸºç¡€è®¤çŸ¥éšœç¢è¯„ä¼°ä¸­çš„è·¨è¯­è¨€å’Œè·¨åœºæ‰€çš„é€‚åº”èƒ½åŠ›ï¼Œç‰¹åˆ«å…³æ³¨é“¾å¼æ€ç»´æç¤ºå’Œè½»é‡çº§å¾®è°ƒæŠ€æœ¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†æ”¶é›†ã€æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°ä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚ä½¿ç”¨ADReSSoã€NCMMSC2021-ADå’Œæ–°æ”¶é›†çš„CIR-Eæ•°æ®é›†è¿›è¡Œæ¨¡å‹æ€§èƒ½è¯„ä¼°ï¼Œæ¶µç›–è‹±è¯­å’Œæ™®é€šè¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†CogBenchåŸºå‡†ï¼Œé¦–æ¬¡ç³»ç»Ÿæ€§è¯„ä¼°LLMsåœ¨å¤šè¯­è¨€è®¤çŸ¥éšœç¢è¯„ä¼°ä¸­çš„è¡¨ç°ï¼Œå¹¶é€šè¿‡LoRAæŠ€æœ¯æ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨ç›®æ ‡é¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œé‡‡ç”¨é“¾å¼æ€ç»´æç¤ºè®¾è®¡ä»¥æé«˜LLMsçš„é€‚åº”æ€§ï¼ŒåŒæ—¶ä½¿ç”¨ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰è¿›è¡Œè½»é‡çº§å¾®è°ƒï¼Œä»¥ä¼˜åŒ–æ¨¡å‹åœ¨æ–°é¢†åŸŸçš„è¡¨ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä¼ ç»Ÿæ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨è·¨é¢†åŸŸè½¬ç§»æ—¶æ€§èƒ½ä¸‹é™è¶…è¿‡30%ï¼Œè€Œé€šè¿‡é“¾å¼æ€ç»´æç¤ºçš„LLMsè¡¨ç°å‡ºæ›´å¥½çš„é€‚åº”æ€§ï¼Œä¸”åœ¨LoRAå¾®è°ƒåï¼Œç›®æ ‡é¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›æé«˜äº†çº¦25%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒ»ç–—å¥åº·ã€è€å¹´äººæŠ¤ç†å’Œå¿ƒç†å¥åº·è¯„ä¼°ç­‰ã€‚CogBenchåŸºå‡†ä¸ºå¼€å‘å¤šè¯­è¨€ã€è·¨æ–‡åŒ–çš„è®¤çŸ¥éšœç¢è¯„ä¼°å·¥å…·æä¾›äº†é‡è¦å‚è€ƒï¼Œæœªæ¥å¯ç”¨äºä¸´åºŠç­›æŸ¥å’Œæ—©æœŸå¹²é¢„ï¼Œæå‡è®¤çŸ¥éšœç¢çš„æ£€æµ‹æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Automatic assessment of cognitive impairment from spontaneous speech offers a promising, non-invasive avenue for early cognitive screening. However, current approaches often lack generalizability when deployed across different languages and clinical settings, limiting their practical utility. In this study, we propose CogBench, the first benchmark designed to evaluate the cross-lingual and cross-site generalizability of large language models (LLMs) for speech-based cognitive impairment assessment. Using a unified multimodal pipeline, we evaluate model performance on three speech datasets spanning English and Mandarin: ADReSSo, NCMMSC2021-AD, and a newly collected test set, CIR-E. Our results show that conventional deep learning models degrade substantially when transferred across domains. In contrast, LLMs equipped with chain-of-thought prompting demonstrate better adaptability, though their performance remains sensitive to prompt design. Furthermore, we explore lightweight fine-tuning of LLMs via Low-Rank Adaptation (LoRA), which significantly improves generalization in target domains. These findings offer a critical step toward building clinically useful and linguistically robust speech-based cognitive assessment tools.

