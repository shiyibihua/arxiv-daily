---
layout: default
title: Do GNN-based QEC Decoders Require Classical Knowledge? Evaluating the Efficacy of Knowledge Distillation from MWPM
---

# Do GNN-based QEC Decoders Require Classical Knowledge? Evaluating the Efficacy of Knowledge Distillation from MWPM

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.03782" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.03782v1</a>
  <a href="https://arxiv.org/pdf/2508.03782.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.03782v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.03782v1', 'Do GNN-based QEC Decoders Require Classical Knowledge? Evaluating the Efficacy of Knowledge Distillation from MWPM')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Ryota Ikeda

**ÂàÜÁ±ª**: quant-ph, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-05

**Â§áÊ≥®**: 5 pages, 1 figure, 1 table. Affiliation updated to match user registration

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ËØÑ‰º∞GNNÂü∫Á°ÄÁöÑÈáèÂ≠êÁ∫†ÈîôËß£Á†ÅÂô®ÊòØÂê¶ÈúÄË¶ÅÁªèÂÖ∏Áü•ËØÜ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `ÈáèÂ≠êÁ∫†Èîô` `ÂõæÁ•ûÁªèÁΩëÁªú` `Áü•ËØÜËí∏È¶è` `ÊúÄÂ∞èÊùÉÈáçÂÆåÁæéÂåπÈÖç` `ÂõæÊ≥®ÊÑèÂäõÁΩëÁªú` `ÈáèÂ≠êËÆ°ÁÆó` `ÈîôËØØÁõ∏ÂÖ≥ÊÄß`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÈáèÂ≠êÁ∫†ÈîôËß£Á†ÅÂô®ËÆ≠ÁªÉÊñπÊ≥ïÂ∞ö‰∏çÊàêÁÜüÔºåÂØºËá¥ÊÄßËÉΩÊèêÂçáÊúâÈôê„ÄÇ
2. Êú¨ÊñáÊèêÂá∫ÈÄöËøáÁü•ËØÜËí∏È¶èÂ∞ÜÁªèÂÖ∏ÁÆóÊ≥ïÁöÑÁêÜËÆ∫Áü•ËØÜËΩ¨ÁßªÂà∞GNN‰∏≠Ôºå‰ª•ÊèêÈ´òËß£Á†ÅÂô®ÊÄßËÉΩ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÁü•ËØÜËí∏È¶èÊ®°ÂûãÁöÑËÆ≠ÁªÉÊçüÂ§±Êî∂ÊïõËæÉÊÖ¢Ôºå‰ΩÜÊúÄÁªàÂáÜÁ°ÆÁéá‰∏éÂü∫Á∫øÊ®°ÂûãÁõ∏ÂΩìÔºåËÆ≠ÁªÉÊó∂Èó¥ÊòæËëóÂ¢ûÂä†„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÈáèÂ≠êÁ∫†ÈîôËß£Á†ÅÂô®ÁöÑÊÄßËÉΩÂØπÂÆûÁé∞ÂÆûÁî®ÈáèÂ≠êËÆ°ÁÆóÊú∫Ëá≥ÂÖ≥ÈáçË¶Å„ÄÇËøëÂπ¥Êù•ÔºåÂõæÁ•ûÁªèÁΩëÁªúÔºàGNNÔºâ‰Ωú‰∏∫‰∏ÄÁßçÊúâÂâçÊôØÁöÑÊñπÊ≥ïÈÄêÊ∏êÂèóÂà∞ÂÖ≥Ê≥®Ôºå‰ΩÜÂÖ∂ËÆ≠ÁªÉÊñπÊ≥ïÂ∞öÊú™ÊàêÁÜü„ÄÇÊú¨ÊñáÈÄöËøáÊØîËæÉÂü∫‰∫éÂõæÊ≥®ÊÑèÂäõÁΩëÁªúÔºàGATÔºâÊû∂ÊûÑÁöÑ‰∏§ÁßçÊ®°ÂûãÔºåÈ™åËØÅ‰∫Ü‰ªéÁªèÂÖ∏ÁÆóÊ≥ïÔºàÂ¶ÇÊúÄÂ∞èÊùÉÈáçÂÆåÁæéÂåπÈÖçÔºåMWPMÔºâÂêëGNNËΩ¨ÁßªÁêÜËÆ∫Áü•ËØÜÁöÑÊúâÊïàÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÁü•ËØÜËí∏È¶èÊ®°ÂûãÁöÑÊúÄÁªàÊµãËØïÂáÜÁ°ÆÁéá‰∏éÂü∫Á∫øÊ®°ÂûãÁõ∏ËøëÔºå‰ΩÜËÆ≠ÁªÉÊçüÂ§±Êî∂ÊïõËæÉÊÖ¢ÔºåËÆ≠ÁªÉÊó∂Èó¥Â¢ûÂä†Á∫¶‰∫îÂÄç„ÄÇËøôË°®ÊòéÁé∞‰ª£GNNÊû∂ÊûÑËÉΩÂ§üÈ´òÊïàÂú∞‰ªéÁúüÂÆûÁ°¨‰ª∂Êï∞ÊçÆ‰∏≠Â≠¶‰π†Â§çÊùÇÁöÑÈîôËØØÁõ∏ÂÖ≥ÊÄßÔºåËÄåÊó†ÈúÄ‰æùËµñËøë‰ººÁêÜËÆ∫Ê®°ÂûãÁöÑÊåáÂØº„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥ÈáèÂ≠êÁ∫†ÈîôËß£Á†ÅÂô®ËÆ≠ÁªÉÊñπÊ≥ï‰∏çÊàêÁÜüÁöÑÈóÆÈ¢òÔºåÁé∞ÊúâÊñπÊ≥ïÂú®ÊÄßËÉΩÊèêÂçáÊñπÈù¢Â≠òÂú®Â±ÄÈôêÊÄßÔºåÂ∞§ÂÖ∂ÊòØÂú®Â§çÊùÇÈîôËØØÁõ∏ÂÖ≥ÊÄßÁöÑÂ≠¶‰π†‰∏ä„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÊèêÂá∫ÈÄöËøáÁü•ËØÜËí∏È¶èÁöÑÊñπÂºèÔºåÂ∞ÜÁªèÂÖ∏ÁÆóÊ≥ïÔºàMWPMÔºâÁöÑÁêÜËÆ∫Áü•ËØÜËûçÂÖ•GNNÊ®°Âûã‰∏≠Ôºå‰ª•ÊúüÊèêÈ´òËß£Á†ÅÂô®ÁöÑÂ≠¶‰π†ÊïàÁéáÂíåÊÄßËÉΩ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÁ†îÁ©∂‰∏≠‰ΩøÁî®‰∫ÜÂü∫‰∫éÂõæÊ≥®ÊÑèÂäõÁΩëÁªúÔºàGATÔºâÁöÑ‰∏§ÁßçÊ®°ÂûãÔºö‰∏ÄÁßçÊòØÁ∫ØÊï∞ÊçÆÈ©±Âä®ÁöÑÂü∫Á∫øÊ®°ÂûãÔºå‰ªÖ‰ΩøÁî®ÁúüÂÆûÊ†áÁ≠æËøõË°åËÆ≠ÁªÉÔºõÂè¶‰∏ÄÁßçÂàôÂú®Ê≠§Âü∫Á°Ä‰∏äÂ¢ûÂä†‰∫ÜÁü•ËØÜËí∏È¶èÊçüÂ§±ÔºåÂà©Áî®MWPMÁöÑÁêÜËÆ∫ÈîôËØØÊ¶ÇÁéá‰Ωú‰∏∫ÊåáÂØº„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞Âú®‰∫éÈ™åËØÅ‰∫ÜÁé∞‰ª£GNNÊû∂ÊûÑÂú®Êó†ÈúÄÁªèÂÖ∏Áü•ËØÜÊåáÂØºÁöÑÊÉÖÂÜµ‰∏ãÔºåËÉΩÂ§üÊúâÊïàÂ≠¶‰π†Â§çÊùÇÁöÑÈîôËØØÁõ∏ÂÖ≥ÊÄß„ÄÇËøô‰∏é‰º†Áªü‰æùËµñÁªèÂÖ∏ÁÆóÊ≥ïÁöÑËß£Á†ÅÂô®ÊñπÊ≥ïÂΩ¢ÊàêÈ≤úÊòéÂØπÊØî„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Ê®°ÂûãËÆæËÆ°‰∏≠ÔºåÁü•ËØÜËí∏È¶èÊçüÂ§±ÁöÑÂºïÂÖ•ÊòØÂÖ≥ÈîÆÔºåÂ∞ΩÁÆ°ÂÖ∂ÂØºËá¥ËÆ≠ÁªÉÊó∂Èó¥Â¢ûÂä†Á∫¶‰∫îÂÄçÔºå‰ΩÜÊúÄÁªàÁöÑÊµãËØïÂáÜÁ°ÆÁéá‰∏éÂü∫Á∫øÊ®°ÂûãÁõ∏ËøëÔºåÊòæÁ§∫Âá∫GNNÁöÑÈ´òÊïàÂ≠¶‰π†ËÉΩÂäõ„ÄÇÂÖ∑‰ΩìÁöÑÊçüÂ§±ÂáΩÊï∞ËÆæËÆ°ÂíåÁΩëÁªúÁªìÊûÑÁªÜËäÇÂú®ËÆ∫Êñá‰∏≠ÊúâËØ¶ÁªÜÈòêËø∞„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÁü•ËØÜËí∏È¶èÊ®°ÂûãÁöÑÊúÄÁªàÊµãËØïÂáÜÁ°ÆÁéá‰∏éÂü∫Á∫øÊ®°ÂûãÂá†‰πéÁõ∏ÂêåÔºå‰ΩÜËÆ≠ÁªÉÊçüÂ§±Êî∂ÊïõÈÄüÂ∫¶ËæÉÊÖ¢ÔºåËÆ≠ÁªÉÊó∂Èó¥Â¢ûÂä†‰∫ÜÁ∫¶‰∫îÂÄç„ÄÇËøôË°®ÊòéGNNËÉΩÂ§üÈ´òÊïàÂ≠¶‰π†Â§çÊùÇÁöÑÈîôËØØÁõ∏ÂÖ≥ÊÄßÔºåÂÖ∑ÊúâËæÉÂº∫ÁöÑÂÆûÁî®‰ª∑ÂÄº„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨ÈáèÂ≠êËÆ°ÁÆó„ÄÅÈáèÂ≠êÈÄö‰ø°ÂíåÈáèÂ≠êÁΩëÁªúÁ≠â„ÄÇÈÄöËøáÊèêÂçáÈáèÂ≠êÁ∫†ÈîôËß£Á†ÅÂô®ÁöÑÊÄßËÉΩÔºåËÉΩÂ§üÊé®Âä®ÈáèÂ≠êËÆ°ÁÆóÊú∫ÁöÑÂÆûÁî®ÂåñËøõÁ®ãÔºåËøõËÄåÂú®‰ø°ÊÅØÂ§ÑÁêÜ„ÄÅÂä†ÂØÜÈÄö‰ø°Á≠âÈ¢ÜÂüü‰∫ßÁîüÊ∑±ËøúÂΩ±Âìç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> The performance of decoders in Quantum Error Correction (QEC) is key to realizing practical quantum computers. In recent years, Graph Neural Networks (GNNs) have emerged as a promising approach, but their training methodologies are not yet well-established. It is generally expected that transferring theoretical knowledge from classical algorithms like Minimum Weight Perfect Matching (MWPM) to GNNs, a technique known as knowledge distillation, can effectively improve performance. In this work, we test this hypothesis by rigorously comparing two models based on a Graph Attention Network (GAT) architecture that incorporates temporal information as node features. The first is a purely data-driven model (baseline) trained only on ground-truth labels, while the second incorporates a knowledge distillation loss based on the theoretical error probabilities from MWPM. Using public experimental data from Google, our evaluation reveals that while the final test accuracy of the knowledge distillation model was nearly identical to the baseline, its training loss converged more slowly, and the training time increased by a factor of approximately five. This result suggests that modern GNN architectures possess a high capacity to efficiently learn complex error correlations directly from real hardware data, without guidance from approximate theoretical models.

