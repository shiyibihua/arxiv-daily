---
layout: default
title: Audio-Aware Large Language Models as Judges for Speaking Styles
---

# Audio-Aware Large Language Models as Judges for Speaking Styles

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05984" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05984v1</a>
  <a href="https://arxiv.org/pdf/2506.05984.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05984v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05984v1', 'Audio-Aware Large Language Models as Judges for Speaking Styles')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Cheng-Han Chiang, Xiaofei Wang, Chung-Ching Lin, Kevin Lin, Linjie Li, Radu Kopetz, Yao Qian, Zhendong Wang, Zhengyuan Yang, Hung-yi Lee, Lijuan Wang

**åˆ†ç±»**: eess.AS, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-06

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºéŸ³é¢‘æ„ŸçŸ¥å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºæ¼”è®²é£æ ¼è¯„ä¼°å·¥å…·**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `éŸ³é¢‘æ„ŸçŸ¥` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¼”è®²é£æ ¼è¯„ä¼°` `å£è¯­æ¨¡å‹` `å¤šæ¨¡æ€è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å£è¯­æ¨¡å‹åœ¨æ¼”è®²é£æ ¼çš„æ§åˆ¶å’Œè‡ªç„¶å¯¹è¯ç”Ÿæˆæ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥æ»¡è¶³å¤šæ ·åŒ–çš„åº”ç”¨éœ€æ±‚ã€‚
2. æœ¬æ–‡æå‡ºä½¿ç”¨éŸ³é¢‘æ„ŸçŸ¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆALLMsï¼‰ä½œä¸ºè‡ªåŠ¨è¯„ä¼°å·¥å…·ï¼Œèƒ½å¤Ÿç»¼åˆè¯„ä¼°æ¼”è®²çš„å¤šç§é£æ ¼ç‰¹å¾ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGeminiä¸äººç±»è¯„ä¼°è€…çš„è¯„ä¼°ä¸€è‡´æ€§è‰¯å¥½ï¼Œè¡¨æ˜ALLMsåœ¨è¯„ä¼°SLMsæ–¹é¢å…·æœ‰æ½œåŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éŸ³é¢‘æ„ŸçŸ¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆALLMsï¼‰èƒ½å¤Ÿç†è§£éŸ³é¢‘è¾“å…¥ä¸­çš„æ–‡æœ¬å’Œéæ–‡æœ¬ä¿¡æ¯ã€‚æœ¬æ–‡æ¢è®¨äº†ä½¿ç”¨ALLMsä½œä¸ºè‡ªåŠ¨è¯„ä¼°å·¥å…·æ¥è¯„ä¼°æ¼”è®²çš„é£æ ¼ã€‚æˆ‘ä»¬åˆ©ç”¨ALLMè¯„ä¼°è€…å¯¹ç”±å£è¯­æ¨¡å‹ï¼ˆSLMsï¼‰ç”Ÿæˆçš„æ¼”è®²è¿›è¡Œè¯„ä¼°ï¼Œä»»åŠ¡åŒ…æ‹¬å£°éŸ³é£æ ¼æŒ‡ä»¤è·Ÿéšå’Œè§’è‰²æ‰®æ¼”ã€‚è€ƒè™‘çš„æ¼”è®²é£æ ¼åŒ…æ‹¬æƒ…æ„Ÿã€éŸ³é‡ã€è¯­é€Ÿã€è¯è¯­é‡éŸ³ã€éŸ³è°ƒæ§åˆ¶å’Œéè¯­è¨€å…ƒç´ ã€‚é€šè¿‡ä¸äººç±»è¯„ä¼°è€…çš„æ¯”è¾ƒï¼Œç»“æœè¡¨æ˜ï¼ŒGeminiä¸äººç±»è¯„ä¼°è€…ä¹‹é—´çš„åè®®ç¨‹åº¦ç›¸å½“äºäººç±»è¯„ä¼°è€…ä¹‹é—´çš„åè®®ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒALLMså¯ä»¥ä½œä¸ºè¯„ä¼°SLMsçš„å·¥å…·ï¼ŒåŒæ—¶ä¹Ÿæ­ç¤ºäº†å½“å‰SLMsåœ¨æ§åˆ¶æ¼”è®²é£æ ¼å’Œç”Ÿæˆè‡ªç„¶å¯¹è¯æ–¹é¢ä»æœ‰æ”¹è¿›ç©ºé—´ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å£è¯­æ¨¡å‹åœ¨æ¼”è®²é£æ ¼è¯„ä¼°ä¸­çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨æƒ…æ„Ÿã€è¯­é€Ÿç­‰å¤šæ ·åŒ–é£æ ¼æ§åˆ¶æ–¹é¢çš„æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥éŸ³é¢‘æ„ŸçŸ¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆALLMsï¼‰ï¼Œå®ç°å¯¹æ¼”è®²é£æ ¼çš„è‡ªåŠ¨è¯„ä¼°ï¼Œæå‡è¯„ä¼°çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å››ä¸ªå£è¯­æ¨¡å‹ï¼ˆSLMsï¼‰ç”Ÿæˆæ¼”è®²å†…å®¹ï¼ŒALLMsä½œä¸ºè¯„ä¼°è€…å¯¹ç”Ÿæˆå†…å®¹è¿›è¡Œé£æ ¼è¯„ä¼°ï¼Œå®éªŒä¸­è¿˜å¼•å…¥äººç±»è¯„ä¼°è€…ä½œä¸ºå¯¹æ¯”ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºå°†ALLMsåº”ç”¨äºæ¼”è®²é£æ ¼è¯„ä¼°ï¼Œæä¾›äº†ä¸€ç§æ–°çš„è¯„ä¼°æ–¹å¼ï¼Œä¸”è¯„ä¼°ç»“æœä¸äººç±»è¯„ä¼°è€…çš„ç»“æœé«˜åº¦ä¸€è‡´ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œä½¿ç”¨äº†å¤šç§è¯„ä¼°æŒ‡æ ‡æ¥è¡¡é‡æ¼”è®²é£æ ¼çš„å„ä¸ªæ–¹é¢ï¼ŒåŒ…æ‹¬æƒ…æ„Ÿã€éŸ³é‡ã€è¯­é€Ÿç­‰ï¼Œç¡®ä¿è¯„ä¼°çš„å…¨é¢æ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒGeminiä¸äººç±»è¯„ä¼°è€…ä¹‹é—´çš„åè®®ç¨‹åº¦ç›¸å½“ï¼Œæ˜¾ç¤ºå‡ºALLMsåœ¨è¯„ä¼°å£è¯­æ¨¡å‹ç”Ÿæˆå†…å®¹æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œå½“å‰çš„SLMsåœ¨æ¼”è®²é£æ ¼æ§åˆ¶æ–¹é¢ä»æœ‰æ”¹è¿›ç©ºé—´ï¼Œæç¤ºæœªæ¥çš„ç ”ç©¶æ–¹å‘ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€å…¬å…±æ¼”è®²åŸ¹è®­ã€è¯­éŸ³åŠ©æ‰‹ç­‰ï¼Œèƒ½å¤Ÿä¸ºæ¼”è®²è€…æä¾›å®æ—¶åé¦ˆï¼Œå¸®åŠ©å…¶æå‡æ¼”è®²æŠ€å·§ã€‚æœªæ¥ï¼Œéšç€æŠ€æœ¯çš„è¿›æ­¥ï¼ŒALLMsåœ¨å¤šæ¨¡æ€è¯„ä¼°ä¸­çš„åº”ç”¨å°†æ›´åŠ å¹¿æ³›ï¼Œæ¨åŠ¨äººæœºäº¤äº’çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Audio-aware large language models (ALLMs) can understand the textual and non-textual information in the audio input. In this paper, we explore using ALLMs as an automatic judge to assess the speaking styles of speeches. We use ALLM judges to evaluate the speeches generated by SLMs on two tasks: voice style instruction following and role-playing. The speaking style we consider includes emotion, volume, speaking pace, word emphasis, pitch control, and non-verbal elements. We use four spoken language models (SLMs) to complete the two tasks and use humans and ALLMs to judge the SLMs' responses. We compare two ALLM judges, GPT-4o-audio and Gemini-2.5-pro, with human evaluation results and show that the agreement between Gemini and human judges is comparable to the agreement between human evaluators. These promising results show that ALLMs can be used as a judge to evaluate SLMs. Our results also reveal that current SLMs, even GPT-4o-audio, still have room for improvement in controlling the speaking style and generating natural dialogues.

