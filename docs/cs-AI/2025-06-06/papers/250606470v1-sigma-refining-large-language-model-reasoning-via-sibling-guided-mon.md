---
layout: default
title: SIGMA: Refining Large Language Model Reasoning via Sibling-Guided Monte Carlo Augmentation
---

# SIGMA: Refining Large Language Model Reasoning via Sibling-Guided Monte Carlo Augmentation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.06470" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.06470v1</a>
  <a href="https://arxiv.org/pdf/2506.06470.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.06470v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.06470v1', 'SIGMA: Refining Large Language Model Reasoning via Sibling-Guided Monte Carlo Augmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yanwei Ren, Haotian Zhang, Fuxiang Wu, Jiayan Qiu, Jiaxing Huang, Baosheng Yu, Liu Liu

**åˆ†ç±»**: cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-06

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSIGMAæ¡†æ¶ä»¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨ç†èƒ½åŠ›` `è’™ç‰¹å¡ç½—æ ‘æœç´¢` `æ•°æ®è´¨é‡` `å…„å¼ŸèŠ‚ç‚¹` `ä¼˜åŒ–ç®—æ³•` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›æ—¶ï¼Œç®€å•æ‰©å±•æ•°æ®é›†å·²å¼€å§‹å‡ºç°æ”¶ç›Šé€’å‡ï¼Œæ•°æ®è´¨é‡æˆä¸ºå…³é”®æŒ‘æˆ˜ã€‚
2. SIGMAæ¡†æ¶é€šè¿‡é‡æ–°æ•´åˆè¢«ä¸¢å¼ƒçš„å…„å¼ŸèŠ‚ç‚¹ï¼Œåˆ©ç”¨è¿™äº›èŠ‚ç‚¹ä¸­çš„ä¿¡æ¯æ¥ä¼˜åŒ–æ¨ç†è¿‡ç¨‹ï¼Œæå‡æ¨¡å‹æ€§èƒ½ã€‚
3. åœ¨MATHåŸºå‡†æµ‹è¯•ä¸­ï¼ŒSIGMAè°ƒä¼˜çš„7Bæ¨¡å‹ä»…ä½¿ç”¨30Kæ ·æœ¬ä¾¿è¾¾åˆ°äº†54.92%çš„å‡†ç¡®ç‡ï¼Œæ˜¾è‘—ä¼˜äº590Kæ ·æœ¬è®­ç»ƒçš„æ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒæ•°æ®è§„æ¨¡ä¸æ–­æ‰©å¤§ï¼Œæ•°æ®è´¨é‡çš„é‡è¦æ€§æ„ˆå‘å‡¸æ˜¾ã€‚ä¼ ç»Ÿçš„è’™ç‰¹å¡ç½—æ ‘æœç´¢ï¼ˆMCTSï¼‰æ–¹æ³•é€šå¸¸åªä¿ç•™æœç´¢æ ‘ä¸­çš„æœ€ä½³è½¨è¿¹ï¼Œå¿½ç•¥äº†åŒ…å«æœ‰ä»·å€¼éƒ¨åˆ†è§è§£çš„å…„å¼ŸèŠ‚ç‚¹ã€‚æœ¬æ–‡æå‡ºSIGMAï¼ˆSibling Guided Monte Carlo Augmentationï¼‰æ¡†æ¶ï¼Œé€šè¿‡é‡æ–°æ•´åˆè¿™äº›è¢«ä¸¢å¼ƒçš„å…„å¼ŸèŠ‚ç‚¹ï¼Œæå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚SIGMAé€šè¿‡åœ¨æ¯æ¡æœç´¢è·¯å¾„ä¸Šå»ºç«‹å…„å¼ŸèŠ‚ç‚¹ä¹‹é—´çš„è¯­ä¹‰è”ç³»ï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µçš„ç²¾ç‚¼è¿‡ç¨‹ï¼Œé¦–å…ˆç”±æ‰¹è¯„æ¨¡å‹è¯†åˆ«å…„å¼ŸèŠ‚ç‚¹é›†ä¸­çš„ä¼˜ç¼ºç‚¹ï¼Œç„¶åé€šè¿‡æ–‡æœ¬åå‘ä¼ æ’­æ¨¡å‹å¯¹æœ€ä½³è½¨è¿¹è¿›è¡Œä¿®æ­£ã€‚å®éªŒè¡¨æ˜ï¼ŒSIGMAæ˜¾è‘—æé«˜äº†æ¨ç†è½¨è¿¹çš„è´¨é‡ï¼Œåœ¨MATHåŸºå‡†æµ‹è¯•ä¸­ï¼Œä½¿ç”¨ä»…30Kæ ·æœ¬çš„7Bæ¨¡å‹è¾¾åˆ°äº†54.92%çš„å‡†ç¡®ç‡ï¼Œè¶…è¶Šäº†åœ¨590Kæ ·æœ¬ä¸Šè®­ç»ƒçš„æœ€å…ˆè¿›æ¨¡å‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä¼ ç»Ÿè’™ç‰¹å¡ç½—æ ‘æœç´¢æ–¹æ³•ä¸­ï¼Œä»…ä¿ç•™æœ€ä½³è½¨è¿¹è€Œå¿½ç•¥å…„å¼ŸèŠ‚ç‚¹çš„é—®é¢˜ã€‚è¿™ç§åšæ³•å¯¼è‡´æœ‰ä»·å€¼çš„ä¿¡æ¯è¢«æµªè´¹ï¼Œå½±å“äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSIGMAæ¡†æ¶çš„æ ¸å¿ƒæ€æƒ³æ˜¯é‡æ–°æ•´åˆè¢«ä¸¢å¼ƒçš„å…„å¼ŸèŠ‚ç‚¹ï¼Œé€šè¿‡å»ºç«‹å…„å¼ŸèŠ‚ç‚¹ä¹‹é—´çš„è¯­ä¹‰è”ç³»ï¼Œåˆ©ç”¨è¿™äº›èŠ‚ç‚¹ä¸­çš„ä¿¡æ¯æ¥ä¼˜åŒ–æ¨ç†è¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSIGMAæ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šæ‰¹è¯„æ¨¡å‹å’Œä¿®æ­£æ¨¡å‹ã€‚æ‰¹è¯„æ¨¡å‹è´Ÿè´£è¯†åˆ«å…„å¼ŸèŠ‚ç‚¹é›†ä¸­çš„ä¼˜ç¼ºç‚¹ï¼Œè€Œä¿®æ­£æ¨¡å‹åˆ™é€šè¿‡æ–‡æœ¬åå‘ä¼ æ’­å¯¹æœ€ä½³è½¨è¿¹è¿›è¡Œä¿®æ­£ã€‚

**å…³é”®åˆ›æ–°**ï¼šSIGMAçš„åˆ›æ–°åœ¨äºå…¶å…„å¼ŸèŠ‚ç‚¹çš„å†åˆ©ç”¨ç­–ç•¥ï¼ŒåŒºåˆ«äºä¼ ç»Ÿæ–¹æ³•ä»…å…³æ³¨æœ€ä½³è½¨è¿¹ï¼Œå……åˆ†æŒ–æ˜äº†éæœ€ä¼˜æ¨ç†åˆ†æ”¯ä¸­çš„ä¿¡æ¯ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ä¸¤é˜¶æ®µçš„ç²¾ç‚¼è¿‡ç¨‹ï¼Œæ‰¹è¯„æ¨¡å‹å’Œä¿®æ­£æ¨¡å‹çš„å…·ä½“ç»“æ„å’ŒæŸå¤±å‡½æ•°è®¾ç½®å°šæœªè¯¦ç»†æŠ«éœ²ï¼Œå¯èƒ½éœ€è¦è¿›ä¸€æ­¥çš„ç ”ç©¶æ¥ä¼˜åŒ–è¿™äº›å‚æ•°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨MATHåŸºå‡†æµ‹è¯•ä¸­ï¼ŒSIGMAè°ƒä¼˜çš„7Bæ¨¡å‹ä»…ä½¿ç”¨30Kæ ·æœ¬ä¾¿è¾¾åˆ°äº†54.92%çš„å‡†ç¡®ç‡ï¼Œæ˜¾è‘—è¶…è¶Šäº†åœ¨590Kæ ·æœ¬ä¸Šè®­ç»ƒçš„æœ€å…ˆè¿›æ¨¡å‹ï¼Œå±•ç¤ºäº†å…„å¼Ÿå¼•å¯¼ä¼˜åŒ–çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SIGMAæ¡†æ¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿå’Œæ•™è‚²è¾…åŠ©å·¥å…·ç­‰ã€‚é€šè¿‡æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼ŒSIGMAèƒ½å¤Ÿåœ¨æ›´å¤æ‚çš„ä»»åŠ¡ä¸­æä¾›æ›´å‡†ç¡®çš„ç»“æœï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Enhancing large language models by simply scaling up datasets has begun to yield diminishing returns, shifting the spotlight to data quality. Monte Carlo Tree Search (MCTS) has emerged as a powerful technique for generating high-quality chain-of-thought data, yet conventional approaches typically retain only the top-scoring trajectory from the search tree, discarding sibling nodes that often contain valuable partial insights, recurrent error patterns, and alternative reasoning strategies. This unconditional rejection of non-optimal reasoning branches may waste vast amounts of informative data in the whole search tree. We propose SIGMA (Sibling Guided Monte Carlo Augmentation), a novel framework that reintegrates these discarded sibling nodes to refine LLM reasoning. SIGMA forges semantic links among sibling nodes along each search path and applies a two-stage refinement: a critique model identifies overlooked strengths and weaknesses across the sibling set, and a revision model conducts text-based backpropagation to refine the top-scoring trajectory in light of this comparative feedback. By recovering and amplifying the underutilized but valuable signals from non-optimal reasoning branches, SIGMA substantially improves reasoning trajectories. On the challenging MATH benchmark, our SIGMA-tuned 7B model achieves 54.92% accuracy using only 30K samples, outperforming state-of-the-art models trained on 590K samples. This result highlights that our sibling-guided optimization not only significantly reduces data usage but also significantly boosts LLM reasoning.

