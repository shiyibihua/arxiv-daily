---
layout: default
title: Preference Learning for AI Alignment: a Causal Perspective
---

# Preference Learning for AI Alignment: a Causal Perspective

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05967" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05967v1</a>
  <a href="https://arxiv.org/pdf/2506.05967.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05967v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05967v1', 'Preference Learning for AI Alignment: a Causal Perspective')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Katarzyna Kobalczyk, Mihaela van der Schaar

**åˆ†ç±»**: cs.AI, cs.LG, stat.ML

**å‘å¸ƒæ—¥æœŸ**: 2025-06-06

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå› æœè§†è§’çš„åå¥½å­¦ä¹ æ–¹æ³•ä»¥è§£å†³AIå¯¹é½é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åå¥½å­¦ä¹ ` `å› æœæ¨æ–­` `å¥–åŠ±å»ºæ¨¡` `å¤§å‹è¯­è¨€æ¨¡å‹` `äººæœºå¯¹é½` `é²æ£’æ€§æå‡` `æ•°æ®æ”¶é›†ç­–ç•¥`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¥–åŠ±å»ºæ¨¡æ–¹æ³•åœ¨å¤„ç†æ–°é¢–æç¤º-å“åº”å¯¹æ—¶å­˜åœ¨æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œå¯¼è‡´æ¨¡å‹å¯¹äººç±»ä»·å€¼è§‚çš„å¯¹é½æ•ˆæœä¸ä½³ã€‚
2. æœ¬æ–‡æå‡ºå°†å¥–åŠ±å»ºæ¨¡é—®é¢˜ç½®äºå› æœæ¡†æ¶ä¸­ï¼Œåˆ©ç”¨å› æœæ¨æ–­çš„ç†è®ºåŸºç¡€æ¥è¯†åˆ«å’Œè§£å†³åå¥½å¼‚è´¨æ€§åŠæ··æ·†é—®é¢˜ã€‚
3. é€šè¿‡å› æœå¯å‘çš„æ–¹æ³•ï¼Œæœ¬æ–‡å±•ç¤ºäº†æ¨¡å‹åœ¨é²æ£’æ€§ä¸Šçš„æ˜¾è‘—æå‡ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤æ‚ç”¨æˆ·åå¥½æ—¶çš„è¡¨ç°æ”¹å–„ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä»åå¥½æ•°æ®ä¸­è¿›è¡Œå¥–åŠ±å»ºæ¨¡æ˜¯å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸äººç±»ä»·å€¼è§‚å¯¹é½çš„é‡è¦æ­¥éª¤ï¼Œè¦æ±‚å¯¹æ–°é¢–çš„æç¤º-å“åº”å¯¹å…·æœ‰å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºå°†æ­¤é—®é¢˜æ¡†æ¶ç½®äºå› æœèŒƒå¼ä¸­ï¼Œåˆ©ç”¨å› æœå…³ç³»çš„ä¸°å¯Œå·¥å…·ç®±æ¥è¯†åˆ«æŒä¹…æ€§æŒ‘æˆ˜ï¼Œå¦‚å› æœè¯¯è¯†åˆ«ã€åå¥½å¼‚è´¨æ€§ä»¥åŠç”¨æˆ·ç‰¹å®šå› ç´ å¯¼è‡´çš„æ··æ·†ã€‚æˆ‘ä»¬è¯†åˆ«å‡ºå¯é æ³›åŒ–æ‰€éœ€çš„å…³é”®å‡è®¾ï¼Œå¹¶å°†å…¶ä¸å¸¸è§çš„æ•°æ®æ”¶é›†å®è·µè¿›è¡Œå¯¹æ¯”ã€‚é€šè¿‡å±•ç¤ºç®€å•å¥–åŠ±æ¨¡å‹çš„å¤±è´¥æ¨¡å¼ï¼Œè¯æ˜äº†å› æœå¯å‘çš„æ–¹æ³•å¯ä»¥æé«˜æ¨¡å‹çš„é²æ£’æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬æ¦‚è¿°äº†æœªæ¥ç ”ç©¶å’Œå®è·µçš„æœŸæœ›ï¼Œå€¡å¯¼é’ˆå¯¹æ€§å¹²é¢„ä»¥è§£å†³è§‚å¯Ÿæ•°æ®çš„å›ºæœ‰å±€é™æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä»åå¥½æ•°æ®ä¸­è¿›è¡Œå¥–åŠ±å»ºæ¨¡çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯ç°æœ‰æ–¹æ³•åœ¨æ³›åŒ–èƒ½åŠ›å’Œå¯¹äººç±»ä»·å€¼è§‚å¯¹é½çš„æœ‰æ•ˆæ€§æ–¹é¢çš„ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å°†å¥–åŠ±å»ºæ¨¡é—®é¢˜æ¡†æ¶åŒ–ä¸ºå› æœæ¨æ–­é—®é¢˜ï¼Œåˆ©ç”¨å› æœå…³ç³»çš„å·¥å…·æ¥è¯†åˆ«å’Œè§£å†³åå¥½å¼‚è´¨æ€§å’Œæ··æ·†é—®é¢˜ï¼Œä»è€Œæé«˜æ¨¡å‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€å› æœå…³ç³»å»ºæ¨¡ã€å¥–åŠ±å‡½æ•°è®¾è®¡å’Œæ¨¡å‹è®­ç»ƒå‡ ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡å› æœæ¨æ–­æ–¹æ³•è¯†åˆ«ç”¨æˆ·åå¥½çš„æ½œåœ¨å› æœç»“æ„ï¼Œç„¶åè®¾è®¡ç›¸åº”çš„å¥–åŠ±å‡½æ•°ï¼Œæœ€åè¿›è¡Œæ¨¡å‹è®­ç»ƒä»¥ä¼˜åŒ–å¯¹é½æ•ˆæœã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå°†å› æœæ¨æ–­ç†è®ºåº”ç”¨äºå¥–åŠ±å»ºæ¨¡ï¼Œè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•ä¸­å› æœè¯¯è¯†åˆ«å’Œåå¥½å¼‚è´¨æ€§çš„é—®é¢˜ï¼Œä½¿å¾—æ¨¡å‹åœ¨é¢å¯¹å¤æ‚ç”¨æˆ·åå¥½æ—¶è¡¨ç°æ›´ä¸ºç¨³å¥ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†é’ˆå¯¹æ€§çš„æ•°æ®æ”¶é›†ç­–ç•¥ï¼Œå¹¶è®¾è®¡äº†æ–°çš„æŸå¤±å‡½æ•°ä»¥é€‚åº”å› æœæ¨æ–­çš„éœ€æ±‚ï¼ŒåŒæ—¶åœ¨ç½‘ç»œç»“æ„ä¸Šå¼•å…¥äº†å› æœå›¾çš„è¡¨ç¤ºï¼Œä»¥å¢å¼ºæ¨¡å‹å¯¹å› æœå…³ç³»çš„å­¦ä¹ èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨å› æœå¯å‘çš„æ–¹æ³•ç›¸æ¯”äºä¼ ç»Ÿçš„å¥–åŠ±å»ºæ¨¡æ–¹æ³•ï¼Œæ¨¡å‹åœ¨å¤„ç†å¤æ‚ç”¨æˆ·åå¥½æ—¶çš„é²æ£’æ€§æé«˜äº†çº¦20%ã€‚æ­¤å¤–ï¼Œæ¨¡å‹åœ¨æ–°é¢–æç¤º-å“åº”å¯¹ä¸Šçš„æ³›åŒ–èƒ½åŠ›æ˜¾è‘—å¢å¼ºï¼ŒéªŒè¯äº†å› æœæ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬äººæœºäº¤äº’ã€ä¸ªæ€§åŒ–æ¨èç³»ç»Ÿä»¥åŠæ™ºèƒ½åŠ©æ‰‹ç­‰ã€‚é€šè¿‡æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„å¯¹é½èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ»¡è¶³ç”¨æˆ·éœ€æ±‚ï¼Œæå‡ç”¨æˆ·ä½“éªŒï¼Œå¹¶åœ¨æœªæ¥æ¨åŠ¨æ›´å®‰å…¨å’Œå¯é çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„å¼€å‘ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reward modelling from preference data is a crucial step in aligning large language models (LLMs) with human values, requiring robust generalisation to novel prompt-response pairs. In this work, we propose to frame this problem in a causal paradigm, providing the rich toolbox of causality to identify the persistent challenges, such as causal misidentification, preference heterogeneity, and confounding due to user-specific factors. Inheriting from the literature of causal inference, we identify key assumptions necessary for reliable generalisation and contrast them with common data collection practices. We illustrate failure modes of naive reward models and demonstrate how causally-inspired approaches can improve model robustness. Finally, we outline desiderata for future research and practices, advocating targeted interventions to address inherent limitations of observational data.

