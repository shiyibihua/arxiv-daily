---
layout: default
title: A Synthetic Pseudo-Autoencoder Invites Examination of Tacit Assumptions in Neural Network Design
---

# A Synthetic Pseudo-Autoencoder Invites Examination of Tacit Assumptions in Neural Network Design

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.12076" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.12076v1</a>
  <a href="https://arxiv.org/pdf/2506.12076.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.12076v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.12076v1', 'A Synthetic Pseudo-Autoencoder Invites Examination of Tacit Assumptions in Neural Network Design')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Assaf Marron

**åˆ†ç±»**: cs.NE, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-06

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåˆæˆä¼ªè‡ªç¼–ç å™¨ä»¥æŒ‘æˆ˜ç¥ç»ç½‘ç»œè®¾è®¡å‡è®¾**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `è‡ªç¼–ç å™¨` `ç¥ç»ç½‘ç»œè®¾è®¡` `æ•´æ•°ç¼–ç ` `ç”Ÿç‰©å­¦è§†è§’` `æ‰‹å·¥è®¾è®¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨æ•´æ•°ç¼–ç ä¸æ¢å¤é—®é¢˜ä¸Šé¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨è®¾è®¡ä¸å­¦ä¹ çš„å‡è®¾ä¸Šå­˜åœ¨å±€é™æ€§ã€‚
2. æœ¬æ–‡æå‡ºçš„ä¼ªè‡ªç¼–ç å™¨é€šè¿‡æ‰‹å·¥è®¾è®¡ï¼Œåˆ©ç”¨ç®€å•çš„æ•°å­—è¿æ¥ä¸ä½æ“ä½œæœºåˆ¶ï¼Œè§£å†³äº†æ•´æ•°é›†åˆç¼–ç é—®é¢˜ã€‚
3. è¯¥ç ”ç©¶å¹¶æœªè¿›è¡Œå®é™…åº”ç”¨æµ‹è¯•ï¼Œè€Œæ˜¯é€šè¿‡å¯¹æ¯”æ ‡å‡†è‡ªç¼–ç å™¨ï¼Œæ¢è®¨äº†è®¾è®¡å‡è®¾å¯¹æ¨¡å‹å‘å±•çš„å½±å“ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ‰‹å·¥è®¾è®¡çš„ç¥ç»ç½‘ç»œï¼Œè¯¥ç½‘ç»œåœ¨æœªç»è¿‡è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿè§£å†³å°†ä»»æ„æ•´æ•°é›†åˆç¼–ç ä¸ºå•ä¸€æ•°å€¼å¹¶æ¢å¤åŸå§‹å…ƒç´ çš„éš¾é¢˜ã€‚é€šè¿‡ä»…ä½¿ç”¨æ ‡å‡†ç¥ç»ç½‘ç»œæ“ä½œï¼ˆåŠ æƒå’Œã€åç½®å’Œæ’ç­‰æ¿€æ´»ï¼‰ï¼Œæˆ‘ä»¬åœ¨è®¾è®¡é€‰æ‹©ä¸ŠæŒ‘æˆ˜äº†è¯¥é¢†åŸŸå¸¸è§çš„å‡è®¾ï¼ŒåŒ…æ‹¬è¡¨ç¤ºã€åŸŸçš„è¿ç»­æ€§ã€è®¡ç®—å’Œå¯å­¦ä¹ æ€§ç­‰ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬çš„æ„é€ æ˜¯è®¾è®¡è€Œéå­¦ä¹ çš„ï¼Œé€šè¿‡ç®€å•åœ°è¿æ¥æ•°å­—è€Œä¸è¿›è¡Œå‹ç¼©æ¥è¡¨ç¤ºå¤šä¸ªå€¼ï¼Œå¹¶ä¾èµ–äºç¡¬ä»¶çº§åˆ«çš„å³ä¾§æ•°å­—æˆªæ–­ä½œä¸ºä½æ“ä½œæœºåˆ¶ã€‚è¯¥ç¥ç»ç½‘ç»œå¹¶ä¸æ—¨åœ¨å®é™…åº”ç”¨ï¼Œè€Œæ˜¯é‚€è¯·ç ”ç©¶è€…å®¡è§†å¯èƒ½ä¸å¿…è¦é™åˆ¶è‡ªç¼–ç å’Œæœºå™¨å­¦ä¹ ç³»ç»Ÿä¸æ¨¡å‹å‘å±•çš„å‡è®¾ã€‚æœ€åï¼Œæˆ‘ä»¬ç»“åˆç”Ÿç‰©å­¦è§†è§’å¯¹è®¨è®ºè¿›è¡Œäº†æ·±åŒ–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å°†ä»»æ„æ•´æ•°é›†åˆç¼–ç ä¸ºå•ä¸€æ•°å€¼å¹¶æ¢å¤åŸå§‹å…ƒç´ çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå¤æ‚çš„å­¦ä¹ è¿‡ç¨‹ï¼Œé™åˆ¶äº†è®¾è®¡çš„çµæ´»æ€§å’Œæ•ˆç‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯åœ¨äºé€šè¿‡æ‰‹å·¥è®¾è®¡çš„ç½‘ç»œï¼Œåˆ©ç”¨ç®€å•çš„æ•°å­—è¿æ¥å’Œç¡¬ä»¶çº§åˆ«çš„ä½æ“ä½œæ¥å®ç°ç¼–ç ä¸è§£ç ï¼Œè€Œéä¾èµ–äºè®­ç»ƒè¿‡ç¨‹ã€‚è¿™æ ·çš„è®¾è®¡æŒ‘æˆ˜äº†ä¼ ç»Ÿçš„ç¥ç»ç½‘ç»œè®¾è®¡å‡è®¾ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬è¾“å…¥æ•´æ•°é›†åˆçš„ç¼–ç æ¨¡å—å’Œè¾“å‡ºæ¢å¤åŸå§‹é›†åˆçš„è§£ç æ¨¡å—ã€‚ç¼–ç æ¨¡å—é€šè¿‡è¿æ¥æ•°å­—å®ç°è¡¨ç¤ºï¼Œè§£ç æ¨¡å—åˆ™é€šè¿‡æˆªæ–­æ“ä½œæ¢å¤åŸå§‹å€¼ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºè¯¥ç½‘ç»œçš„è®¾è®¡æ˜¯å®Œå…¨æ‰‹å·¥çš„ï¼Œä¸”é€šè¿‡ç®€å•çš„ä½æ“ä½œå®ç°äº†å¤šå€¼è¡¨ç¤ºï¼ŒåŒºåˆ«äºä¼ ç»Ÿè‡ªç¼–ç å™¨çš„å­¦ä¹ è¿‡ç¨‹ã€‚

**å…³é”®è®¾è®¡**ï¼šç½‘ç»œç»“æ„é‡‡ç”¨æ ‡å‡†çš„åŠ æƒå’Œä¸åç½®ï¼Œæ¿€æ´»å‡½æ•°ä¸ºæ’ç­‰å‡½æ•°ï¼Œè®¾è®¡ä¸­æ²¡æœ‰ä½¿ç”¨å¤æ‚çš„æŸå¤±å‡½æ•°æˆ–è®­ç»ƒæœºåˆ¶ï¼Œå¼ºè°ƒäº†è®¾è®¡çš„ç®€æ´æ€§ä¸æœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥ç ”ç©¶å±•ç¤ºäº†æ‰‹å·¥è®¾è®¡çš„ä¼ªè‡ªç¼–ç å™¨åœ¨ç¼–ç ä¸è§£ç æ•´æ•°é›†åˆæ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œå°½ç®¡æœªè¿›è¡Œå®é™…åº”ç”¨æµ‹è¯•ï¼Œä½†å…¶è®¾è®¡ç†å¿µä¸ä¼ ç»Ÿè‡ªç¼–ç å™¨ç›¸æ¯”ï¼Œæä¾›äº†æ–°çš„è§†è§’ï¼Œå¯èƒ½å½±å“æœªæ¥çš„æ¨¡å‹è®¾è®¡ä¸å‡è®¾æ£€éªŒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç¥ç»ç½‘ç»œè®¾è®¡ã€æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ„å»ºä»¥åŠç”Ÿç‰©å­¦ç‰¹å¾çš„è‡ªç„¶è‡ªç¼–ç ç ”ç©¶ã€‚é€šè¿‡æŒ‘æˆ˜ä¼ ç»Ÿå‡è®¾ï¼Œå¯èƒ½ä¸ºæ–°å‹è‡ªç¼–ç å™¨çš„è®¾è®¡æä¾›æ–°çš„æ€è·¯ä¸æ–¹æ³•ï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We present a handcrafted neural network that, without training, solves the seemingly difficult problem of encoding an arbitrary set of integers into a single numerical variable, and then recovering the original elements. While using only standard neural network operations -- weighted sums with biases and identity activation -- we make design choices that challenge common notions in this area around representation, continuity of domains, computation, learnability and more. For example, our construction is designed, not learned; it represents multiple values using a single one by simply concatenating digits without compression, and it relies on hardware-level truncation of rightmost digits as a bit-manipulation mechanism. This neural net is not intended for practical application. Instead, we see its resemblance to -- and deviation from -- standard trained autoencoders as an invitation to examine assumptions that may unnecessarily constrain the development of systems and models based on autoencoding and machine learning. Motivated in part by our research on a theory of biological evolution centered around natural autoencoding of species characteristics, we conclude by refining the discussion with a biological perspective.

