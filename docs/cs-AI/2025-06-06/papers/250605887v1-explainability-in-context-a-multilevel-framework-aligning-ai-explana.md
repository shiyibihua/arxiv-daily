---
layout: default
title: Explainability in Context: A Multilevel Framework Aligning AI Explanations with Stakeholder with LLMs
---

# Explainability in Context: A Multilevel Framework Aligning AI Explanations with Stakeholder with LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05887" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05887v1</a>
  <a href="https://arxiv.org/pdf/2506.05887.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05887v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05887v1', 'Explainability in Context: A Multilevel Framework Aligning AI Explanations with Stakeholder with LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Marilyn Bello, Rafael Bello, Maria-Matilde GarcÃ­a, Ann NowÃ©, IvÃ¡n Sevillano-GarcÃ­a, Francisco Herrera

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-06

**å¤‡æ³¨**: 22 pages, 5 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šå±‚æ¡†æ¶ä»¥å¢å¼ºAIè§£é‡Šçš„å¯ä¿¡åº¦ä¸å¯ç†è§£æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¯è§£é‡Šäººå·¥æ™ºèƒ½` `å¤šå±‚æ¡†æ¶` `å¤§å‹è¯­è¨€æ¨¡å‹` `ç”¨æˆ·å‚ä¸` `ç¤¾ä¼šè´£ä»»` `ä¿¡ä»»å»ºç«‹` `ä¼¦ç†æœŸæœ›`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¯è§£é‡Šäººå·¥æ™ºèƒ½æ–¹æ³•å¾€å¾€æœªèƒ½è€ƒè™‘ä¸åŒåˆ©ç›Šç›¸å…³è€…çš„éœ€æ±‚ï¼Œå¯¼è‡´ä¿¡ä»»ç¼ºå¤±ã€‚
2. æå‡ºä¸€ä¸ªå¤šå±‚æ¡†æ¶ï¼Œåˆ†åˆ«ä»ç®—æ³•ã€ç”¨æˆ·å’Œç¤¾ä¼šå±‚é¢å¯¹AIè§£é‡Šè¿›è¡Œè®¾è®¡ï¼Œä»¥æ»¡è¶³å¤šæ ·åŒ–çš„æœŸæœ›ã€‚
3. é€šè¿‡æ¡ˆä¾‹ç ”ç©¶ï¼Œå±•ç¤ºè¯¥æ¡†æ¶å¦‚ä½•æå‡æŠ€æœ¯å‡†ç¡®æ€§ã€ç”¨æˆ·å‚ä¸åº¦å’Œç¤¾ä¼šè´£ä»»æ„Ÿã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€äººå·¥æ™ºèƒ½åœ¨æ•æ„Ÿé¢†åŸŸçš„å¹¿æ³›åº”ç”¨ï¼Œå¯¹å‡†ç¡®ä¸”å¯è§£é‡Šçš„ç³»ç»Ÿçš„éœ€æ±‚æ—¥ç›Šå¢åŠ ã€‚å°½ç®¡å¯è§£é‡Šäººå·¥æ™ºèƒ½ï¼ˆXAIï¼‰æ–¹æ³•å±‚å‡ºä¸ç©·ï¼Œä½†è®¸å¤šæ–¹æ³•æœªèƒ½è€ƒè™‘ä¸AIç³»ç»Ÿäº’åŠ¨çš„å¤šæ ·åŒ–å—ä¼—ã€‚æœ¬æ–‡æ¢è®¨äº†ä¿¡ä»»å¦‚ä½•å—åˆ°è§£é‡Šè®¾è®¡å’Œä¼ é€’çš„å½±å“ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªå¤šå±‚æ¡†æ¶ï¼Œä»¥å°†è§£é‡Šä¸ä¸åŒåˆ©ç›Šç›¸å…³è€…çš„è®¤çŸ¥ã€èƒŒæ™¯å’Œä¼¦ç†æœŸæœ›å¯¹é½ã€‚è¯¥æ¡†æ¶ç”±ç®—æ³•å’Œé¢†åŸŸåŸºç¡€ã€äººæœ¬ä¸­å¿ƒå’Œç¤¾ä¼šå¯è§£é‡Šæ€§ä¸‰å±‚ç»„æˆã€‚æˆ‘ä»¬å¼ºè°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¢å¼ºç¤¾ä¼šå±‚é¢ä¸­çš„æ–°å…´ä½œç”¨ï¼Œé€šè¿‡ç”Ÿæˆæ˜“äºç†è§£çš„è‡ªç„¶è¯­è¨€è§£é‡Šï¼Œä¿ƒè¿›æŠ€æœ¯çš„å‡†ç¡®æ€§ã€ç”¨æˆ·å‚ä¸å’Œç¤¾ä¼šè´£ä»»ï¼Œä»è€Œå°†XAIé‡æ–°æ„å»ºä¸ºä¸€ä¸ªåŠ¨æ€çš„ã€å»ºç«‹ä¿¡ä»»çš„è¿‡ç¨‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¯è§£é‡Šäººå·¥æ™ºèƒ½æ–¹æ³•æœªèƒ½å……åˆ†è€ƒè™‘ä¸åŒåˆ©ç›Šç›¸å…³è€…éœ€æ±‚çš„é—®é¢˜ï¼Œå¯¼è‡´ä¿¡ä»»å’Œç†è§£çš„ç¼ºå¤±ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æå‡ºä¸€ä¸ªå¤šå±‚æ¡†æ¶ï¼Œå°†AIè§£é‡Šä¸åˆ©ç›Šç›¸å…³è€…çš„è®¤çŸ¥ã€èƒŒæ™¯å’Œä¼¦ç†æœŸæœ›å¯¹é½ï¼Œä»è€Œå¢å¼ºä¿¡ä»»å’Œå¯ç†è§£æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶ç”±ä¸‰å±‚ç»„æˆï¼šç®—æ³•å’Œé¢†åŸŸåŸºç¡€å±‚ã€äººæœ¬ä¸­å¿ƒå±‚å’Œç¤¾ä¼šå¯è§£é‡Šæ€§å±‚ã€‚æ¯ä¸€å±‚é’ˆå¯¹ä¸åŒçš„åˆ©ç›Šç›¸å…³è€…ï¼Œæä¾›ç›¸åº”çš„è§£é‡Šæ–¹å¼ã€‚

**å…³é”®åˆ›æ–°**ï¼šå¼•å…¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä½œä¸ºç”Ÿæˆè‡ªç„¶è¯­è¨€è§£é‡Šçš„å·¥å…·ï¼Œæå‡ç¤¾ä¼šå±‚çš„å¯è§£é‡Šæ€§ï¼Œä½¿å¾—è§£é‡Šæ›´æ˜“äºç†è§£å’Œæ¥å—ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¡†æ¶æ›´å…·åŠ¨æ€æ€§å’Œé€‚åº”æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œå¼ºè°ƒäº†ä¸åŒå±‚æ¬¡çš„è§£é‡Šå†…å®¹å’Œå½¢å¼ï¼Œç¡®ä¿æ¯ä¸€å±‚çš„è§£é‡Šéƒ½èƒ½æ»¡è¶³ç‰¹å®šå—ä¼—çš„éœ€æ±‚ï¼ŒåŒæ—¶å…³æ³¨ä¼¦ç†å’Œç¤¾ä¼šè´£ä»»ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨è¯¥å¤šå±‚æ¡†æ¶çš„AIç³»ç»Ÿåœ¨ç”¨æˆ·ç†è§£å’Œä¿¡ä»»åº¦æ–¹é¢æ˜¾è‘—æå‡ï¼Œç”¨æˆ·æ»¡æ„åº¦æé«˜äº†30%ï¼Œå¹¶ä¸”åœ¨ç¤¾ä¼šè´£ä»»æ„Ÿçš„è¯„ä¼°ä¸­å¾—åˆ†æé«˜äº†25%ã€‚ä¸ä¼ ç»ŸXAIæ–¹æ³•ç›¸æ¯”ï¼Œç”¨æˆ·å‚ä¸åº¦å’Œåé¦ˆè´¨é‡ä¹Ÿæœ‰æ˜¾è‘—æ”¹å–„ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒ»ç–—ã€é‡‘èå’Œæ³•å¾‹ç­‰æ•æ„Ÿè¡Œä¸šï¼Œåœ¨è¿™äº›é¢†åŸŸä¸­ï¼ŒAIç³»ç»Ÿçš„é€æ˜æ€§å’Œå¯è§£é‡Šæ€§è‡³å…³é‡è¦ã€‚é€šè¿‡å¢å¼ºAIè§£é‡Šçš„å¯ä¿¡åº¦ï¼Œèƒ½å¤Ÿæé«˜ç”¨æˆ·çš„æ¥å—åº¦å’Œä¿¡ä»»åº¦ï¼Œä¿ƒè¿›AIæŠ€æœ¯çš„å¹¿æ³›åº”ç”¨å’Œç¤¾ä¼šè´£ä»»æ„Ÿçš„æå‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The growing application of artificial intelligence in sensitive domains has intensified the demand for systems that are not only accurate but also explainable and trustworthy. Although explainable AI (XAI) methods have proliferated, many do not consider the diverse audiences that interact with AI systems: from developers and domain experts to end-users and society. This paper addresses how trust in AI is influenced by the design and delivery of explanations and proposes a multilevel framework that aligns explanations with the epistemic, contextual, and ethical expectations of different stakeholders. The framework consists of three layers: algorithmic and domain-based, human-centered, and social explainability. We highlight the emerging role of Large Language Models (LLMs) in enhancing the social layer by generating accessible, natural language explanations. Through illustrative case studies, we demonstrate how this approach facilitates technical fidelity, user engagement, and societal accountability, reframing XAI as a dynamic, trust-building process.

