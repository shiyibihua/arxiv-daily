---
layout: default
title: Cost-Efficient LLM Training with Lifetime-Aware Tensor Offloading via GPUDirect Storage
---

# Cost-Efficient LLM Training with Lifetime-Aware Tensor Offloading via GPUDirect Storage

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.06472" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.06472v1</a>
  <a href="https://arxiv.org/pdf/2506.06472.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.06472v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.06472v1', 'Cost-Efficient LLM Training with Lifetime-Aware Tensor Offloading via GPUDirect Storage')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Ziqi Yuan, Haoyang Zhang, Yirui Eric Zhou, Apoorve Mohan, I-Hsin Chung, Seetharami Seelam, Jian Huang

**ÂàÜÁ±ª**: cs.DC, cs.AI, cs.LG, cs.PF

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-06

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫TERAIO‰ª•Ëß£ÂÜ≥GPUÂÜÖÂ≠òÊâ©Â±ïÁöÑÊàêÊú¨ÊïàÁéáÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `GPUÂÜÖÂ≠ò‰ºòÂåñ` `Âº†ÈáèÂç∏ËΩΩ` `Âõ∫ÊÄÅÁ°¨Áõò` `ÊÄßËÉΩÊèêÂçá`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãËÆ≠ÁªÉ‰∏≠ÔºåGPUÂÜÖÂ≠òÂà©Áî®Áéá‰ΩéÔºåÂØºËá¥ËµÑÊ∫êÊµ™Ë¥πÂíåÊÄßËÉΩÁì∂È¢à„ÄÇ
2. TERAIOÈÄöËøáÁîüÂëΩÂë®ÊúüÊÑüÁü•ÁöÑÂº†ÈáèÂç∏ËΩΩÁ≠ñÁï•Ôºå‰ºòÂåñ‰∫ÜGPUÂÜÖÂ≠òÁöÑ‰ΩøÁî®ÔºåÊèêÂçá‰∫ÜËÆ≠ÁªÉÊïàÁéá„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåTERAIOÂú®Â§öÁßçLLMËÆ≠ÁªÉ‰∏≠Âπ≥ÂùáÊèêÂçá‰∫Ü1.47ÂÄçÁöÑÊÄßËÉΩÔºåÊé•ËøëÁêÜÊÉ≥ÊÄßËÉΩÁöÑ80.7%„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÁîüÂëΩÂë®ÊúüÊÑüÁü•Âº†ÈáèÂç∏ËΩΩÊ°ÜÊû∂TERAIOÔºåÊó®Âú®Âà©Áî®‰ΩéÊàêÊú¨ÁöÑÂü∫‰∫éPCIeÁöÑÂõ∫ÊÄÅÁ°¨ÁõòÔºàSSDÔºâÊâ©Â±ïGPUÂÜÖÂ≠ò„ÄÇËØ•Ê°ÜÊû∂‰∏ì‰∏∫Â§öGPUÂíåÂ§öSSDÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâËÆ≠ÁªÉËÄåËÆæËÆ°„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂú®ÊØèÊ¨°LLMËÆ≠ÁªÉËø≠‰ª£‰∏≠ÔºåÊ¥ªË∑ÉÂº†Èáè‰ªÖÂç†ÂàÜÈÖçGPUÂÜÖÂ≠òÁöÑ1.7%ÔºåËÄå‰∏çÊ¥ªË∑ÉÂº†ÈáèÈÄöÂ∏∏ËæÉÂ§ß‰∏îÈïøÊó∂Èó¥‰∏ç‰ΩøÁî®ÔºåËøô‰∏∫Â∞ÜÂº†ÈáèÂç∏ËΩΩÂà∞ÊÖ¢ÈÄüSSDÊèê‰æõ‰∫ÜÊú∫‰ºö„ÄÇTERAIOÈÄöËøáÂàÜÊûêËÆ≠ÁªÉËøáÁ®ã‰∏≠ÁöÑÂâçÂá†Ê¨°Ëø≠‰ª£ÔºåÂáÜÁ°Æ‰º∞ËÆ°ÊØè‰∏™Âº†ÈáèÁöÑÁîüÂëΩÂë®ÊúüÔºåÂπ∂ÁîüÊàê‰ºòÂåñÁöÑÂç∏ËΩΩ/È¢ÑÂèñËÆ°ÂàíÔºåÈõÜÊàêÂà∞PyTorchÁºñËØëÁöÑLLMÁ®ãÂ∫è‰∏≠„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáGPUDirectÂ≠òÂÇ®ÂÆûÁé∞Âº†ÈáèÁöÑÁõ¥Êé•ËøÅÁßªÔºåÁºìËß£CPUÁì∂È¢àÔºåÊúÄÂ§ßÂåñSSDÂ∏¶ÂÆΩÂà©Áî®Áéá„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåTERAIOÂú®Â§öÁßçLLMËÆ≠ÁªÉ‰∏≠Âπ≥ÂùáÊèêÂçá‰∫Ü1.47ÂÄçÁöÑÊÄßËÉΩÔºåËææÂà∞‰∫ÜÁêÜÊÉ≥ÊÄßËÉΩÁöÑ80.7%„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãËÆ≠ÁªÉ‰∏≠GPUÂÜÖÂ≠òÂà©Áî®Áéá‰ΩéÁöÑÈóÆÈ¢òÔºåÁé∞ÊúâÊñπÊ≥ïÊú™ËÉΩÊúâÊïàÂà©Áî®‰∏çÊ¥ªË∑ÉÂº†ÈáèÁöÑÂÜÖÂ≠òÔºåÂØºËá¥ËµÑÊ∫êÊµ™Ë¥πÂíåÊÄßËÉΩÁì∂È¢à„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöTERAIOÈÄöËøáÂàÜÊûêÂº†ÈáèÁöÑÁîüÂëΩÂë®ÊúüÔºåËØÜÂà´Ê¥ªË∑É‰∏é‰∏çÊ¥ªË∑ÉÂº†ÈáèÔºåÂà∂ÂÆöÂç∏ËΩΩÂíåÈ¢ÑÂèñÁ≠ñÁï•Ôºå‰ªéËÄå‰ºòÂåñGPUÂÜÖÂ≠òÁöÑ‰ΩøÁî®ÊïàÁéá„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöTERAIOÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨Âº†ÈáèÁîüÂëΩÂë®ÊúüÂàÜÊûêÊ®°Âùó„ÄÅÂç∏ËΩΩ/È¢ÑÂèñËÆ°ÂàíÁîüÊàêÊ®°ÂùóÂíåËøêË°åÊó∂Âº†ÈáèËøÅÁßªÂºïÊìéÔºåÊîØÊåÅÈÄöËøáGPUDirectÂ≠òÂÇ®ÂÆûÁé∞GPU‰∏éSSD‰πãÈó¥ÁöÑÁõ¥Êé•Êï∞ÊçÆËøÅÁßª„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöTERAIOÁöÑÊ†∏ÂøÉÂàõÊñ∞Âú®‰∫éÁîüÂëΩÂë®ÊúüÊÑüÁü•ÁöÑÂº†ÈáèÂç∏ËΩΩÁ≠ñÁï•ÔºåËÉΩÂ§üÂä®ÊÄÅË∞ÉÊï¥Âº†ÈáèÂú®GPUÂíåSSD‰πãÈó¥ÁöÑÂ≠òÂÇ®‰ΩçÁΩÆÔºåÊòæËëóÊèêÈ´ò‰∫ÜÂÜÖÂ≠òÂà©Áî®ÁéáÂíåËÆ≠ÁªÉÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆæËÆ°‰∏≠ÔºåTERAIOÈÄöËøáÂØπËÆ≠ÁªÉÂàùÊúüÁöÑÂº†Èáè‰ΩøÁî®ÊÉÖÂÜµËøõË°åÂàÜÊûêÔºåÂáÜÁ°Æ‰º∞ËÆ°ÊØè‰∏™Âº†ÈáèÁöÑÊ¥ªË∑ÉÊó∂Èó¥ÔºåÂπ∂Âü∫‰∫éÊ≠§ÁîüÊàê‰ºòÂåñÁöÑÂç∏ËΩΩËÆ°ÂàíÔºåÁ°Æ‰øùGPUËÆ≠ÁªÉËøáÁ®ã‰∏çË¢´ÈòªÂ°û„ÄÇÂÖ∑‰ΩìÁöÑÂèÇÊï∞ËÆæÁΩÆÂíåÊçüÂ§±ÂáΩÊï∞Âú®ËÆ∫Êñá‰∏≠ËøõË°å‰∫ÜËØ¶ÁªÜÊèèËø∞„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåTERAIOÂú®Â§öÁßçÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãËÆ≠ÁªÉ‰∏≠Âπ≥ÂùáÊèêÂçá‰∫Ü1.47ÂÄçÁöÑÊÄßËÉΩÔºåÁõ∏ËæÉ‰∫éÁé∞ÊúâÁöÑZeRO-OffloadÂíåZeRO-InfinityÊñπÊ≥ïÔºåÂ±ïÁé∞‰∫ÜÊòæËëóÁöÑ‰ºòÂäø„ÄÇÊ≠§Â§ñÔºåTERAIOËææÂà∞‰∫ÜÁêÜÊÉ≥ÊÄßËÉΩÁöÑ80.7%ÔºåÊòæÁ§∫Âá∫ÂÖ∂Âú®GPUÂÜÖÂ≠òÊâ©Â±ïÊñπÈù¢ÁöÑÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑËÆ≠ÁªÉÂíåÊé®ÁêÜÔºåÂ∞§ÂÖ∂ÊòØÂú®ËµÑÊ∫êÂèóÈôêÁöÑÁéØÂ¢É‰∏≠„ÄÇÈÄöËøá‰ºòÂåñGPUÂÜÖÂ≠òÁöÑ‰ΩøÁî®ÔºåTERAIOËÉΩÂ§üÈôç‰ΩéËÆ≠ÁªÉÊàêÊú¨ÔºåÊèêÈ´òËÆ≠ÁªÉÊïàÁéáÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> We present the design and implementation of a new lifetime-aware tensor offloading framework for GPU memory expansion using low-cost PCIe-based solid-state drives (SSDs). Our framework, TERAIO, is developed explicitly for large language model (LLM) training with multiple GPUs and multiple SSDs. Its design is driven by our observation that the active tensors take only a small fraction (1.7% on average) of allocated GPU memory in each LLM training iteration, the inactive tensors are usually large and will not be used for a long period of time, creating ample opportunities for offloading/prefetching tensors to/from slow SSDs without stalling the GPU training process. TERAIO accurately estimates the lifetime (active period of time in GPU memory) of each tensor with the profiling of the first few iterations in the training process. With the tensor lifetime analysis, TERAIO will generate an optimized tensor offloading/prefetching plan and integrate it into the compiled LLM program via PyTorch. TERAIO has a runtime tensor migration engine to execute the offloading/prefetching plan via GPUDirect storage, which allows direct tensor migration between GPUs and SSDs for alleviating the CPU bottleneck and maximizing the SSD bandwidth utilization. In comparison with state-of-the-art studies such as ZeRO-Offload and ZeRO-Infinity, we show that TERAIO improves the training performance of various LLMs by 1.47x on average, and achieves 80.7% of the ideal performance assuming unlimited GPU memory.

