---
layout: default
title: PersonaAgent: When Large Language Model Agents Meet Personalization at Test Time
---

# PersonaAgent: When Large Language Model Agents Meet Personalization at Test Time

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.06254" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.06254v1</a>
  <a href="https://arxiv.org/pdf/2506.06254.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.06254v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.06254v1', 'PersonaAgent: When Large Language Model Agents Meet Personalization at Test Time')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Weizhi Zhang, Xinyang Zhang, Chenwei Zhang, Liangwei Yang, Jingbo Shang, Zhepei Wei, Henry Peng Zou, Zijie Huang, Zhengyang Wang, Yifan Gao, Xiaoman Pan, Lian Xiong, Jingguo Liu, Philip S. Yu, Xian Li

**åˆ†ç±»**: cs.AI, cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-06

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPersonaAgentä»¥è§£å†³ä¸ªæ€§åŒ–å“åº”ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä¸ªæ€§åŒ–æ™ºèƒ½ä½“` `å¤§å‹è¯­è¨€æ¨¡å‹` `ç”¨æˆ·åå¥½å¯¹é½` `ä¸ªæ€§åŒ–è®°å¿†` `åŠ¨æ€å“åº”` `æ™ºèƒ½å®¢æœ` `æ¨èç³»ç»Ÿ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„LLMæ™ºèƒ½ä½“é€šå¸¸é‡‡ç”¨ä¸€åˆ€åˆ‡çš„æ–¹æ³•ï¼Œæ— æ³•çµæ´»å“åº”ç”¨æˆ·çš„ä¸ªæ€§åŒ–éœ€æ±‚ã€‚
2. æœ¬æ–‡æå‡ºçš„PersonaAgentæ¡†æ¶é€šè¿‡ä¸ªæ€§åŒ–è®°å¿†å’Œè¡ŒåŠ¨æ¨¡å—ï¼Œé’ˆå¯¹ç”¨æˆ·çš„ç‹¬ç‰¹éœ€æ±‚è¿›è¡ŒåŠ¨æ€è°ƒæ•´ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPersonaAgentåœ¨ä¸ªæ€§åŒ–æ•ˆæœå’Œå®é™…åº”ç”¨ä¸­å‡æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œæå‡äº†ç”¨æˆ·ä½“éªŒã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ™ºèƒ½ä½“ä½œä¸ºå…ˆè¿›çš„èŒƒå¼ï¼Œåœ¨å¤šä¸ªé¢†åŸŸå’Œä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„LLMæ™ºèƒ½ä½“é€šå¸¸é‡‡ç”¨ä¸€åˆ€åˆ‡çš„æ–¹æ³•ï¼Œç¼ºä¹çµæ´»æ€§ä»¥åº”å¯¹ç”¨æˆ·çš„ä¸åŒéœ€æ±‚å’Œåå¥½ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†PersonaAgentï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæ—¨åœ¨è§£å†³å¤šæ ·åŒ–ä¸ªæ€§åŒ–ä»»åŠ¡çš„ä¸ªæ€§åŒ–LLMæ™ºèƒ½ä½“æ¡†æ¶ã€‚PersonaAgenté›†æˆäº†ä¸¤ä¸ªäº’è¡¥ç»„ä»¶ï¼šä¸ªæ€§åŒ–è®°å¿†æ¨¡å—å’Œä¸ªæ€§åŒ–è¡ŒåŠ¨æ¨¡å—ã€‚æ ¸å¿ƒçš„ä¸ªæ€§åŒ–ç³»ç»Ÿæç¤ºä½œä¸ºä¸­ä»‹ï¼Œåˆ©ç”¨ä¸ªæ€§åŒ–è®°å¿†çš„æ´å¯Ÿæ¥æ§åˆ¶æ™ºèƒ½ä½“çš„è¡ŒåŠ¨ï¼Œå¹¶é€šè¿‡è¡ŒåŠ¨ç»“æœä¸æ–­ä¼˜åŒ–è®°å¿†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPersonaAgentåœ¨ä¸ªæ€§åŒ–è¡ŒåŠ¨ç©ºé—´å’Œå®é™…åº”ç”¨ä¸­çš„æ‰©å±•æ€§ä¸Šæ˜¾è‘—ä¼˜äºå…¶ä»–åŸºçº¿æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„LLMæ™ºèƒ½ä½“ç¼ºä¹ä¸ªæ€§åŒ–èƒ½åŠ›ï¼Œæ— æ³•æ ¹æ®ç”¨æˆ·çš„ä¸åŒéœ€æ±‚å’Œåå¥½è¿›è¡Œçµæ´»å“åº”ï¼Œå¯¼è‡´ç”¨æˆ·ä½“éªŒä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡æå‡ºçš„PersonaAgentæ¡†æ¶é€šè¿‡å¼•å…¥ä¸ªæ€§åŒ–è®°å¿†æ¨¡å—å’Œä¸ªæ€§åŒ–è¡ŒåŠ¨æ¨¡å—ï¼Œåˆ©ç”¨ç”¨æˆ·çš„ä¸ªæ€§åŒ–ä¿¡æ¯æ¥ä¼˜åŒ–æ™ºèƒ½ä½“çš„å“åº”å’Œè¡Œä¸ºã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPersonaAgentçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸ªæ€§åŒ–è®°å¿†æ¨¡å—ï¼ˆåŒ…å«æƒ…èŠ‚è®°å¿†å’Œè¯­ä¹‰è®°å¿†æœºåˆ¶ï¼‰å’Œä¸ªæ€§åŒ–è¡ŒåŠ¨æ¨¡å—ï¼Œæ ¸å¿ƒæ˜¯ä¸ºæ¯ä¸ªç”¨æˆ·å®šä¹‰ç‹¬ç‰¹çš„ç³»ç»Ÿæç¤ºï¼ˆpersonaï¼‰ï¼Œä»¥æ­¤ä¸ºåŸºç¡€è¿›è¡ŒåŠ¨æ€è°ƒæ•´å’Œä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†ä¸ªæ€§åŒ–è®°å¿†å’Œè¡ŒåŠ¨æ¨¡å—çš„ç»“åˆï¼Œå…è®¸æ™ºèƒ½ä½“åœ¨æµ‹è¯•é˜¶æ®µå®æ—¶è°ƒæ•´å“åº”ï¼Œæ˜¾è‘—æå‡äº†ä¸ªæ€§åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œä¸ªæ€§åŒ–è®°å¿†æ¨¡å—é€šè¿‡æ¨¡æ‹Ÿæœ€è¿‘çš„äº¤äº’æ¥ä¼˜åŒ–ç³»ç»Ÿæç¤ºï¼Œå¹¶ä½¿ç”¨æ–‡æœ¬æŸå¤±åé¦ˆæœºåˆ¶æ¥ç¡®ä¿ç”¨æˆ·åå¥½çš„å®æ—¶å¯¹é½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒPersonaAgentåœ¨ä¸ªæ€§åŒ–å“åº”çš„æœ‰æ•ˆæ€§ä¸Šæ˜¾è‘—ä¼˜äºå…¶ä»–åŸºçº¿æ–¹æ³•ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨æµ‹è¯•é˜¶æ®µçš„ç”¨æˆ·åå¥½å¯¹é½ä¸Šæå‡äº†20%ä»¥ä¸Šï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å¯è¡Œæ€§å’Œæ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

PersonaAgentçš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½å®¢æœã€ä¸ªæ€§åŒ–æ¨èç³»ç»Ÿå’Œæ•™è‚²è¾…å¯¼ç­‰ã€‚é€šè¿‡æä¾›åŠ¨æ€çš„ä¸ªæ€§åŒ–å“åº”ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæ˜¾è‘—æå‡ç”¨æˆ·ä½“éªŒï¼Œæ»¡è¶³ä¸åŒç”¨æˆ·çš„ç‹¬ç‰¹éœ€æ±‚ï¼Œæœªæ¥å¯èƒ½åœ¨å¤šä¸ªè¡Œä¸šä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Model (LLM) empowered agents have recently emerged as advanced paradigms that exhibit impressive capabilities in a wide range of domains and tasks. Despite their potential, current LLM agents often adopt a one-size-fits-all approach, lacking the flexibility to respond to users' varying needs and preferences. This limitation motivates us to develop PersonaAgent, the first personalized LLM agent framework designed to address versatile personalization tasks. Specifically, PersonaAgent integrates two complementary components - a personalized memory module that includes episodic and semantic memory mechanisms; a personalized action module that enables the agent to perform tool actions tailored to the user. At the core, the persona (defined as unique system prompt for each user) functions as an intermediary: it leverages insights from personalized memory to control agent actions, while the outcomes of these actions in turn refine the memory. Based on the framework, we propose a test-time user-preference alignment strategy that simulate the latest n interactions to optimize the persona prompt, ensuring real-time user preference alignment through textual loss feedback between simulated and ground-truth responses. Experimental evaluations demonstrate that PersonaAgent significantly outperforms other baseline methods by not only personalizing the action space effectively but also scaling during test-time real-world applications. These results underscore the feasibility and potential of our approach in delivering tailored, dynamic user experiences.

