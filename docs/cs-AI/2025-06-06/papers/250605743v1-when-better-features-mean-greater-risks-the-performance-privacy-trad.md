---
layout: default
title: When Better Features Mean Greater Risks: The Performance-Privacy Trade-Off in Contrastive Learning
---

# When Better Features Mean Greater Risks: The Performance-Privacy Trade-Off in Contrastive Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05743" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05743v1</a>
  <a href="https://arxiv.org/pdf/2506.05743.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05743v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05743v1', 'When Better Features Mean Greater Risks: The Performance-Privacy Trade-Off in Contrastive Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ruining Sun, Hongsheng Hu, Wei Luo, Zhaoxi Zhang, Yanjun Zhang, Haizhuan Yuan, Leo Yu Zhang

**åˆ†ç±»**: cs.CR, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-06

**å¤‡æ³¨**: Accepted In ACM ASIA Conference on Computer and Communications Security (ASIA CCS '25), August 25-29, 2025, Ha Noi, Vietnam. For Code, see https://github.com/SeroneySun/LpLA_code

**DOI**: [10.1145/3708821.3733915](https://doi.org/10.1145/3708821.3733915)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/SeroneySun/LpLA_code)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLpLAæ–¹æ³•ä»¥è§£å†³å¯¹æ¯”å­¦ä¹ ä¸­çš„éšç§æ³„éœ²é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¯¹æ¯”å­¦ä¹ ` `éšç§ä¿æŠ¤` `æˆå‘˜æ¨æ–­æ”»å‡»` `æ·±åº¦å­¦ä¹ ` `ç‰¹å¾æå–` `è‡ªç›‘ç£å­¦ä¹ ` `å®‰å…¨æ€§ç ”ç©¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¯¹æ¯”å­¦ä¹ æ¡†æ¶åœ¨æå‡ç‰¹å¾æå–æ€§èƒ½çš„åŒæ—¶ï¼Œå­˜åœ¨æ˜¾è‘—çš„éšç§æ³„éœ²é£é™©ï¼Œå°¤å…¶æ˜¯æˆå‘˜æ¨æ–­æ”»å‡»çš„å¨èƒã€‚
2. æå‡ºäº†ä¸€ç§æ–°å‹çš„æˆå‘˜æ¨æ–­æ”»å‡»æ–¹æ³•LpLAï¼Œåˆ©ç”¨ç‰¹å¾å‘é‡çš„pèŒƒæ•°ç»Ÿè®¡ç‰¹æ€§æ¥æ¨æ–­è®­ç»ƒæ•°æ®çš„æˆå‘˜èº«ä»½ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLpLAåœ¨å¤šä¸ªæ•°æ®é›†å’Œæ¨¡å‹æ¶æ„ä¸Šå‡ä¼˜äºç°æœ‰æ”»å‡»æ–¹æ³•ï¼Œå°¤å…¶åœ¨æ”»å‡»çŸ¥è¯†å’ŒæŸ¥è¯¢é‡æœ‰é™çš„æƒ…å†µä¸‹è¡¨ç°çªå‡ºã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€æ·±åº¦å­¦ä¹ æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œé¢„è®­ç»ƒç¼–ç å™¨æ¨¡å‹åœ¨ç‰¹å¾æå–æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶å¹¿æ³›ä½¿ç”¨å¼•å‘äº†è®­ç»ƒæ•°æ®éšç§æ³„éœ²çš„é‡å¤§æ‹…å¿§ã€‚æœ¬æ–‡ç³»ç»Ÿç ”ç©¶äº†é’ˆå¯¹ç¼–ç å™¨æ¨¡å‹çš„æˆå‘˜æ¨æ–­æ”»å‡»ï¼ˆMIAï¼‰æ‰€å¸¦æ¥çš„éšç§å¨èƒï¼Œé‡ç‚¹å…³æ³¨å¯¹æ¯”å­¦ä¹ æ¡†æ¶ã€‚é€šè¿‡å®éªŒåˆ†æï¼Œæˆ‘ä»¬æ­ç¤ºäº†æ¨¡å‹æ¶æ„å¤æ‚æ€§å¯¹æˆå‘˜éšç§æ³„éœ²çš„æ˜¾è‘—å½±å“ï¼šæ›´å…ˆè¿›çš„ç¼–ç å™¨æ¡†æ¶è™½ç„¶æå‡äº†ç‰¹å¾æå–æ€§èƒ½ï¼Œä½†åŒæ—¶åŠ å‰§äº†éšç§æ³„éœ²é£é™©ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç‰¹å¾å‘é‡pèŒƒæ•°çš„æ–°å‹æˆå‘˜æ¨æ–­æ”»å‡»æ–¹æ³•ï¼Œç§°ä¸ºåµŒå…¥LpèŒƒæ•°ä¼¼ç„¶æ”»å‡»ï¼ˆLpLAï¼‰ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ç‰¹å¾å‘é‡pèŒƒæ•°çš„ç»Ÿè®¡åˆ†å¸ƒç‰¹å¾æ¨æ–­æˆå‘˜çŠ¶æ€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLpLAåœ¨æ”»å‡»æ€§èƒ½å’Œé²æ£’æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå°¤å…¶æ˜¯åœ¨æœ‰é™çš„æ”»å‡»çŸ¥è¯†å’ŒæŸ¥è¯¢é‡ä¸‹ã€‚æ­¤ç ”ç©¶ä¸ä»…æ­ç¤ºäº†å¯¹æ¯”å­¦ä¹ æ¡†æ¶ä¸­éšç§æ³„éœ²çš„æ½œåœ¨é£é™©ï¼Œä¹Ÿä¸ºç¼–ç å™¨æ¨¡å‹çš„éšç§ä¿æŠ¤ç ”ç©¶æä¾›äº†å®è·µåŸºç¡€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¯¹æ¯”å­¦ä¹ æ¡†æ¶ä¸­å­˜åœ¨çš„æˆå‘˜æ¨æ–­æ”»å‡»ï¼ˆMIAï¼‰å¯¼è‡´çš„éšç§æ³„éœ²é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨é¢å¯¹å¤æ‚æ¨¡å‹æ¶æ„æ—¶ï¼Œæ— æ³•æœ‰æ•ˆå¹³è¡¡ç‰¹å¾æå–æ€§èƒ½ä¸éšç§ä¿æŠ¤ä¹‹é—´çš„çŸ›ç›¾ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„LpLAæ–¹æ³•é€šè¿‡åˆ†æç‰¹å¾å‘é‡çš„pèŒƒæ•°çš„ç»Ÿè®¡åˆ†å¸ƒç‰¹å¾ï¼Œæ¨æ–­æ ·æœ¬æ˜¯å¦å±äºè®­ç»ƒé›†ï¼Œä»è€Œå®ç°å¯¹æˆå‘˜èº«ä»½çš„æ¨æ–­ã€‚è¿™ç§è®¾è®¡èƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨æ¨¡å‹è¾“å‡ºçš„ç‰¹å¾ä¿¡æ¯ï¼Œå¢å¼ºæ”»å‡»çš„å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLpLAæ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬ç‰¹å¾æå–ã€pèŒƒæ•°è®¡ç®—å’Œæˆå‘˜èº«ä»½æ¨æ–­ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œä»æ¨¡å‹ä¸­æå–ç‰¹å¾å‘é‡ï¼›å…¶æ¬¡ï¼Œè®¡ç®—è¿™äº›ç‰¹å¾å‘é‡çš„pèŒƒæ•°ï¼›æœ€åï¼ŒåŸºäºpèŒƒæ•°çš„ç»Ÿè®¡ç‰¹æ€§è¿›è¡Œæˆå‘˜èº«ä»½æ¨æ–­ã€‚

**å…³é”®åˆ›æ–°**ï¼šLpLAçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºåˆ©ç”¨ç‰¹å¾å‘é‡çš„pèŒƒæ•°è¿›è¡Œæˆå‘˜æ¨æ–­ï¼Œè¿™ä¸€æ–¹æ³•åœ¨æ”»å‡»æ€§èƒ½å’Œé²æ£’æ€§ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„æˆå‘˜æ¨æ–­æ”»å‡»æ–¹æ³•ï¼Œå°¤å…¶æ˜¯åœ¨ä¿¡æ¯æœ‰é™çš„æƒ…å†µä¸‹ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨LpLAä¸­ï¼Œé€‰æ‹©åˆé€‚çš„på€¼å¯¹æ”»å‡»æ•ˆæœè‡³å…³é‡è¦ï¼ŒæŸå¤±å‡½æ•°çš„è®¾è®¡ä¹Ÿç»è¿‡ç²¾å¿ƒè°ƒæ•´ï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨æ¨æ–­æ—¶çš„å‡†ç¡®æ€§å’Œç¨³å®šæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒLpLAåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå‡è¡¨ç°å‡ºè‰²ï¼Œç›¸è¾ƒäºç°æœ‰æ–¹æ³•ï¼Œå…¶æ”»å‡»æ€§èƒ½æå‡äº†çº¦20%ï¼Œåœ¨æœ‰é™çš„æ”»å‡»çŸ¥è¯†å’ŒæŸ¥è¯¢é‡ä¸‹ï¼Œé²æ£’æ€§æ˜¾è‘—å¢å¼ºï¼Œå±•ç¤ºäº†å…¶åœ¨éšç§ä¿æŠ¤ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ·±åº¦å­¦ä¹ æ¨¡å‹çš„éšç§ä¿æŠ¤ã€æ•°æ®å®‰å…¨å’Œè‡ªç›‘ç£å­¦ä¹ ç­‰ã€‚é€šè¿‡æé«˜å¯¹æ¯”å­¦ä¹ æ¡†æ¶çš„éšç§ä¿æŠ¤èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨ä¿è¯æ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œé™ä½æ•°æ®æ³„éœ²é£é™©ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œç¤¾ä¼šå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> With the rapid advancement of deep learning technology, pre-trained encoder models have demonstrated exceptional feature extraction capabilities, playing a pivotal role in the research and application of deep learning. However, their widespread use has raised significant concerns about the risk of training data privacy leakage. This paper systematically investigates the privacy threats posed by membership inference attacks (MIAs) targeting encoder models, focusing on contrastive learning frameworks. Through experimental analysis, we reveal the significant impact of model architecture complexity on membership privacy leakage: As more advanced encoder frameworks improve feature-extraction performance, they simultaneously exacerbate privacy-leakage risks. Furthermore, this paper proposes a novel membership inference attack method based on the p-norm of feature vectors, termed the Embedding Lp-Norm Likelihood Attack (LpLA). This method infers membership status, by leveraging the statistical distribution characteristics of the p-norm of feature vectors. Experimental results across multiple datasets and model architectures demonstrate that LpLA outperforms existing methods in attack performance and robustness, particularly under limited attack knowledge and query volumes. This study not only uncovers the potential risks of privacy leakage in contrastive learning frameworks, but also provides a practical basis for privacy protection research in encoder models. We hope that this work will draw greater attention to the privacy risks associated with self-supervised learning models and shed light on the importance of a balance between model utility and training data privacy. Our code is publicly available at: https://github.com/SeroneySun/LpLA_code.

