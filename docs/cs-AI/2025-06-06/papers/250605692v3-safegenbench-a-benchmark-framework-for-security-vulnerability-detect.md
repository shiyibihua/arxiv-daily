---
layout: default
title: SafeGenBench: A Benchmark Framework for Security Vulnerability Detection in LLM-Generated Code
---

# SafeGenBench: A Benchmark Framework for Security Vulnerability Detection in LLM-Generated Code

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05692" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05692v3</a>
  <a href="https://arxiv.org/pdf/2506.05692.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05692v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05692v3', 'SafeGenBench: A Benchmark Framework for Security Vulnerability Detection in LLM-Generated Code')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xinghang Li, Jingzhe Ding, Chao Peng, Bing Zhao, Xiang Gao, Hongwan Gao, Xinchen Gu

**åˆ†ç±»**: cs.CR, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-06 (æ›´æ–°: 2025-06-20)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSafeGenBenchä»¥è§£å†³LLMç”Ÿæˆä»£ç çš„å®‰å…¨æ¼æ´æ£€æµ‹é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å®‰å…¨æ¼æ´æ£€æµ‹` `å¤§å‹è¯­è¨€æ¨¡å‹` `ä»£ç ç”Ÿæˆ` `é™æ€åº”ç”¨å®‰å…¨æµ‹è¯•` `åŸºå‡†æ¡†æ¶` `è‡ªåŠ¨è¯„ä¼°` `è½¯ä»¶å¼€å‘` `å®‰å…¨æ€§è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç ”ç©¶æœªèƒ½å……åˆ†è€ƒè™‘LLMç”Ÿæˆä»£ç çš„å®‰å…¨é£é™©ï¼Œå¯¼è‡´å®‰å…¨æ¼æ´çš„æ½œåœ¨å¨èƒã€‚
2. æå‡ºSafeGenBenchåŸºå‡†æ¡†æ¶ï¼Œç»“åˆé™æ€åº”ç”¨å®‰å…¨æµ‹è¯•å’ŒLLMåˆ¤æ–­ï¼Œè‡ªåŠ¨è¯„ä¼°ç”Ÿæˆä»£ç çš„å®‰å…¨æ€§ã€‚
3. å®è¯è¯„ä¼°æ˜¾ç¤ºï¼Œå½“å‰LLMåœ¨ç”Ÿæˆæ— æ¼æ´ä»£ç æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œæ­ç¤ºäº†æœªæ¥æ”¹è¿›çš„æ–¹å‘ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ä»£ç ç”Ÿæˆèƒ½åŠ›å·²æˆä¸ºè¯„ä¼°å…¶æ•´ä½“æ€§èƒ½çš„é‡è¦ç»´åº¦ã€‚ç„¶è€Œï¼Œå…ˆå‰çš„ç ”ç©¶åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå¿½è§†äº†ç”Ÿæˆä»£ç ä¸­å›ºæœ‰çš„å®‰å…¨é£é™©ã€‚æœ¬æ–‡ä»‹ç»äº†SafeGenBenchï¼Œä¸€ä¸ªä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°LLMç”Ÿæˆä»£ç å®‰å…¨æ€§çš„åŸºå‡†æ¡†æ¶ã€‚è¯¥æ•°æ®é›†æ¶µç›–äº†å¤šç§å¸¸è§è½¯ä»¶å¼€å‘åœºæ™¯å’Œæ¼æ´ç±»å‹ã€‚åŸºäºæ­¤åŸºå‡†ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªè‡ªåŠ¨è¯„ä¼°æ¡†æ¶ï¼Œç»“åˆé™æ€åº”ç”¨å®‰å…¨æµ‹è¯•ï¼ˆSASTï¼‰å’ŒåŸºäºLLMçš„åˆ¤æ–­ï¼Œè¯„ä¼°æ¨¡å‹ç”Ÿæˆä»£ç ä¸­çš„å®‰å…¨æ¼æ´ã€‚é€šè¿‡å¯¹æœ€å…ˆè¿›çš„LLMåœ¨SafeGenBenchä¸Šçš„å®è¯è¯„ä¼°ï¼Œæˆ‘ä»¬æ­ç¤ºäº†å®ƒä»¬åœ¨ç”Ÿæˆæ— æ¼æ´ä»£ç æ–¹é¢çš„æ˜¾è‘—ä¸è¶³ã€‚æˆ‘ä»¬çš„å‘ç°çªæ˜¾äº†ç´§è¿«çš„æŒ‘æˆ˜ï¼Œå¹¶ä¸ºæœªæ¥LLMå®‰å…¨ä»£ç ç”Ÿæˆæ€§èƒ½çš„æå‡æä¾›äº†å¯è¡Œçš„è§è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆä»£ç ä¸­çš„å®‰å…¨æ¼æ´æ£€æµ‹é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆè¯†åˆ«å’Œè¯„ä¼°ç”Ÿæˆä»£ç çš„å®‰å…¨æ€§ï¼Œå¯¼è‡´æ½œåœ¨çš„å®‰å…¨é£é™©æœªè¢«é‡è§†ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºSafeGenBenchåŸºå‡†æ¡†æ¶ï¼Œä¸“æ³¨äºè¯„ä¼°LLMç”Ÿæˆä»£ç çš„å®‰å…¨æ€§ï¼Œç»“åˆé™æ€åº”ç”¨å®‰å…¨æµ‹è¯•ï¼ˆSASTï¼‰å’ŒLLMåˆ¤æ–­ï¼Œæä¾›è‡ªåŠ¨åŒ–çš„æ¼æ´æ£€æµ‹èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†æ„å»ºã€é™æ€åˆ†ææ¨¡å—å’ŒLLMè¯„ä¼°æ¨¡å—ã€‚æ•°æ®é›†æ¶µç›–å¤šç§è½¯ä»¶å¼€å‘åœºæ™¯å’Œæ¼æ´ç±»å‹ï¼Œé™æ€åˆ†ææ¨¡å—è´Ÿè´£è¯†åˆ«æ½œåœ¨æ¼æ´ï¼ŒLLMè¯„ä¼°æ¨¡å—åˆ™é€šè¿‡æ¨¡å‹ç”Ÿæˆçš„ä»£ç è¿›è¡Œåˆ¤æ–­ã€‚

**å…³é”®åˆ›æ–°**ï¼šSafeGenBenchæ˜¯é¦–ä¸ªä¸“é—¨é’ˆå¯¹LLMç”Ÿæˆä»£ç å®‰å…¨æ€§çš„åŸºå‡†æ¡†æ¶ï¼Œå¡«è¡¥äº†ç°æœ‰ç ”ç©¶çš„ç©ºç™½ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œå®ƒæä¾›äº†æ›´å…¨é¢çš„è¯„ä¼°æ‰‹æ®µï¼Œèƒ½å¤Ÿæ›´å¥½åœ°è¯†åˆ«å®‰å…¨æ¼æ´ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œæ•°æ®é›†åŒ…å«å¤šç§å¸¸è§æ¼æ´ç±»å‹ï¼Œé™æ€åˆ†æä½¿ç”¨äº†å…ˆè¿›çš„å®‰å…¨æ£€æµ‹æŠ€æœ¯ï¼ŒLLMè¯„ä¼°æ¨¡å—åˆ™é‡‡ç”¨äº†æœ€æ–°çš„æ¨¡å‹æ¶æ„ï¼Œä»¥ç¡®ä¿è¯„ä¼°çš„å‡†ç¡®æ€§å’Œå…¨é¢æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå½“å‰ä¸»æµçš„LLMåœ¨ç”Ÿæˆæ— æ¼æ´ä»£ç æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨SafeGenBenchä¸Šè¯„ä¼°æ—¶ï¼Œå¹³å‡æ¼æ´æ£€æµ‹ç‡ä½äº30%ã€‚è¿™ä¸€å‘ç°å¼ºè°ƒäº†åœ¨LLMä»£ç ç”Ÿæˆä¸­åŠ å¼ºå®‰å…¨æ€§çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ˜ç¡®çš„æ”¹è¿›æ–¹å‘ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è½¯ä»¶å¼€å‘ã€ä»£ç å®¡è®¡å’Œå®‰å…¨æ€§è¯„ä¼°ç­‰ã€‚é€šè¿‡æä¾›ä¸€ä¸ªç³»ç»ŸåŒ–çš„å®‰å…¨è¯„ä¼°æ¡†æ¶ï¼Œå¼€å‘è€…å¯ä»¥åœ¨ä»£ç ç”Ÿæˆè¿‡ç¨‹ä¸­åŠæ—¶è¯†åˆ«å’Œä¿®å¤å®‰å…¨æ¼æ´ï¼Œä»è€Œæå‡è½¯ä»¶çš„æ•´ä½“å®‰å…¨æ€§ã€‚æœªæ¥ï¼Œè¯¥æ¡†æ¶è¿˜å¯èƒ½æ¨åŠ¨LLMåœ¨å®‰å…¨ä»£ç ç”Ÿæˆæ–¹é¢çš„è¿›ä¸€æ­¥ç ”ç©¶ä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The code generation capabilities of large language models(LLMs) have emerged as a critical dimension in evaluating their overall performance. However, prior research has largely overlooked the security risks inherent in the generated code. In this work, we introduce SafeGenBench, a benchmark specifically designed to assess the security of LLM-generated code. The dataset encompasses a wide range of common software development scenarios and vulnerability types. Building upon this benchmark, we develop an automatic evaluation framework that leverages both static application security testing(SAST) and LLM-based judging to assess the presence of security vulnerabilities in model-generated code. Through the empirical evaluation of state-of-the-art LLMs on SafeGenBench, we reveal notable deficiencies in their ability to produce vulnerability-free code. Our findings highlight pressing challenges and offer actionable insights for future advancements in the secure code generation performance of LLMs. The data and code will be released soon.

