---
layout: default
title: Human-Aligned Procedural Level Generation Reinforcement Learning via Text-Level-Sketch Shared Representation
---

# Human-Aligned Procedural Level Generation Reinforcement Learning via Text-Level-Sketch Shared Representation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.09860" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.09860v1</a>
  <a href="https://arxiv.org/pdf/2508.09860.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.09860v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.09860v1', 'Human-Aligned Procedural Level Generation Reinforcement Learning via Text-Level-Sketch Shared Representation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: In-Chang Baek, Seoyoung Lee, Sung-Hyun Kim, Geumhwan Hwang, KyungJoong Kim

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-13

**å¤‡æ³¨**: 9 pages, 6 tables, 3 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVIPCGRLä»¥è§£å†³äººç±»ä¸­å¿ƒçš„ç¨‹åºå†…å®¹ç”Ÿæˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `ç¨‹åºå†…å®¹ç”Ÿæˆ` `æ·±åº¦å¼ºåŒ–å­¦ä¹ ` `å¤šæ¨¡æ€å­¦ä¹ ` `äººç±»ä¸­å¿ƒè®¾è®¡` `å…±äº«åµŒå…¥ç©ºé—´` `å¯¹æ¯”å­¦ä¹ ` `å¯æ§ç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç¨‹åºå†…å®¹ç”Ÿæˆç³»ç»Ÿå¾€å¾€æ— æ³•æœ‰æ•ˆä½“ç°äººç±»ä¸­å¿ƒçš„è¡Œä¸ºï¼Œé™åˆ¶äº†å…¶åœ¨å®é™…è®¾è®¡ä¸­çš„åº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºVIPCGRLæ¡†æ¶ï¼Œé€šè¿‡ç»“åˆæ–‡æœ¬ã€å…³å¡å’Œè‰å›¾ä¸‰ç§æ¨¡æ€ï¼Œå¢å¼ºç”Ÿæˆè¿‡ç¨‹ä¸­çš„äººç±»ç›¸ä¼¼æ€§å’Œæ§åˆ¶èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVIPCGRLåœ¨ä¸äººç±»ç›¸ä¼¼æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

äººç±»å¯¹é½çš„äººå·¥æ™ºèƒ½æ˜¯å…±åŒåˆ›é€ çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œå®ƒä½¿æ¨¡å‹èƒ½å¤Ÿå‡†ç¡®ç†è§£äººç±»æ„å›¾å¹¶ç”Ÿæˆç¬¦åˆè®¾è®¡ç›®æ ‡çš„å¯æ§è¾“å‡ºã€‚ç°æœ‰çš„ç¨‹åºå†…å®¹ç”Ÿæˆç³»ç»Ÿå¾€å¾€ç¼ºä¹äººç±»ä¸­å¿ƒçš„è¡Œä¸ºï¼Œé™åˆ¶äº†AIé©±åŠ¨ç”Ÿæˆå·¥å…·åœ¨å®é™…è®¾è®¡å·¥ä½œæµä¸­çš„åº”ç”¨ã€‚æœ¬æ–‡æå‡ºäº†VIPCGRLï¼ˆè§†è§‰-æŒ‡ä»¤ç¨‹åºå†…å®¹ç”Ÿæˆå¼ºåŒ–å­¦ä¹ ï¼‰ï¼Œä¸€ä¸ªæ–°é¢–çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç»“åˆæ–‡æœ¬ã€å…³å¡å’Œè‰å›¾ä¸‰ç§æ¨¡æ€ï¼Œæ‰©å±•æ§åˆ¶æ¨¡æ€å¹¶å¢å¼ºäººç±»ç›¸ä¼¼æ€§ã€‚é€šè¿‡è·¨æ¨¡æ€å’Œäººæœºé£æ ¼çš„å››é‡å¯¹æ¯”å­¦ä¹ è®­ç»ƒå…±äº«åµŒå…¥ç©ºé—´ï¼Œå¹¶åˆ©ç”¨åŸºäºåµŒå…¥ç›¸ä¼¼æ€§çš„è¾…åŠ©å¥–åŠ±æ¥å¯¹é½ç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVIPCGRLåœ¨ä¸äººç±»ç›¸ä¼¼æ€§æ–¹é¢ä¼˜äºç°æœ‰åŸºçº¿ï¼Œå¾—åˆ°äº†å®šé‡æŒ‡æ ‡å’Œäººç±»è¯„ä¼°çš„éªŒè¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰ç¨‹åºå†…å®¹ç”Ÿæˆç³»ç»Ÿç¼ºä¹äººç±»ä¸­å¿ƒè¡Œä¸ºçš„é—®é¢˜ï¼Œè¿™é™åˆ¶äº†AIåœ¨å®é™…è®¾è®¡å·¥ä½œæµä¸­çš„æœ‰æ•ˆåº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šVIPCGRLæ¡†æ¶é€šè¿‡å¼•å…¥æ–‡æœ¬ã€å…³å¡å’Œè‰å›¾ä¸‰ç§æ¨¡æ€ï¼Œåˆ©ç”¨å…±äº«åµŒå…¥ç©ºé—´å’Œè¾…åŠ©å¥–åŠ±æœºåˆ¶ï¼Œå¢å¼ºç”Ÿæˆå†…å®¹çš„å¯æ§æ€§å’Œäººç±»ç›¸ä¼¼æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šæ¨¡æ€è¾“å…¥æ¨¡å—ï¼ˆå¤„ç†æ–‡æœ¬ã€å…³å¡å’Œè‰å›¾ï¼‰ã€å…±äº«åµŒå…¥ç©ºé—´ï¼ˆé€šè¿‡å››é‡å¯¹æ¯”å­¦ä¹ è¿›è¡Œè®­ç»ƒï¼‰å’Œç­–ç•¥å¯¹é½æ¨¡å—ï¼ˆåˆ©ç”¨åµŒå…¥ç›¸ä¼¼æ€§è¿›è¡Œå¥–åŠ±è°ƒæ•´ï¼‰ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†å››é‡å¯¹æ¯”å­¦ä¹ æœºåˆ¶ï¼Œå…è®¸æ¨¡å‹åœ¨å¤šæ¨¡æ€ä¹‹é—´å…±äº«ä¿¡æ¯ï¼Œä»è€Œæå‡ç”Ÿæˆå†…å®¹çš„è´¨é‡å’Œäººç±»ç›¸ä¼¼æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–åµŒå…¥ç©ºé—´ï¼Œå¹¶é€šè¿‡è°ƒæ•´å¥–åŠ±æœºåˆ¶æ¥å¼•å¯¼ç­–ç•¥å­¦ä¹ ï¼Œç¡®ä¿ç”Ÿæˆå†…å®¹ç¬¦åˆäººç±»è®¾è®¡ç›®æ ‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒVIPCGRLåœ¨ä¸äººç±»ç›¸ä¼¼æ€§æ–¹é¢çš„è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼Œå…·ä½“æŒ‡æ ‡æ˜¾ç¤ºæå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œä¸”åœ¨ç”¨æˆ·è¯„ä¼°ä¸­è·å¾—äº†æ›´é«˜çš„æ»¡æ„åº¦è¯„åˆ†ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨åœºæ™¯åŒ…æ‹¬æ¸¸æˆè®¾è®¡ã€è™šæ‹Ÿç¯å¢ƒæ„å»ºå’Œäº¤äº’å¼å†…å®¹ç”Ÿæˆç­‰é¢†åŸŸã€‚é€šè¿‡æä¾›æ›´ç¬¦åˆäººç±»è®¾è®¡æ„å›¾çš„ç”Ÿæˆå·¥å…·ï¼ŒVIPCGRLèƒ½å¤Ÿæå‡è®¾è®¡å¸ˆçš„åˆ›ä½œæ•ˆç‡å’Œå†…å®¹è´¨é‡ï¼Œæœªæ¥å¯èƒ½å¯¹åˆ›æ„äº§ä¸šäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Human-aligned AI is a critical component of co-creativity, as it enables models to accurately interpret human intent and generate controllable outputs that align with design goals in collaborative content creation. This direction is especially relevant in procedural content generation via reinforcement learning (PCGRL), which is intended to serve as a tool for human designers. However, existing systems often fall short of exhibiting human-centered behavior, limiting the practical utility of AI-driven generation tools in real-world design workflows. In this paper, we propose VIPCGRL (Vision-Instruction PCGRL), a novel deep reinforcement learning framework that incorporates three modalities-text, level, and sketches-to extend control modality and enhance human-likeness. We introduce a shared embedding space trained via quadruple contrastive learning across modalities and human-AI styles, and align the policy using an auxiliary reward based on embedding similarity. Experimental results show that VIPCGRL outperforms existing baselines in human-likeness, as validated by both quantitative metrics and human evaluations. The code and dataset will be available upon publication.

