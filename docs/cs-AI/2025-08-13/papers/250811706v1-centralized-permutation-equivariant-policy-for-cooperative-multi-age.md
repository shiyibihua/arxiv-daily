---
layout: default
title: Centralized Permutation Equivariant Policy for Cooperative Multi-Agent Reinforcement Learning
---

# Centralized Permutation Equivariant Policy for Cooperative Multi-Agent Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.11706" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.11706v1</a>
  <a href="https://arxiv.org/pdf/2508.11706.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.11706v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.11706v1', 'Centralized Permutation Equivariant Policy for Cooperative Multi-Agent Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhuofan Xu, Benedikt Bollig, Matthias FÃ¼gger, Thomas Nowak, Vincent Le DrÃ©au

**åˆ†ç±»**: cs.MA, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-13

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé›†ä¸­ç½®æ¢ç­‰å˜ç­–ç•¥ä»¥è§£å†³å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ä¸­çš„æ€§èƒ½ç“¶é¢ˆ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ` `é›†ä¸­è®­ç»ƒ` `ç½®æ¢ç­‰å˜ç½‘ç»œ` `å…¨å±€-å±€éƒ¨æ¶æ„` `æ€§èƒ½æå‡` `åˆä½œåŸºå‡†æµ‹è¯•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨éƒ¨åˆ†å¯è§‚æµ‹æ€§ä¸‹è¡¨ç°ä¸ä½³ï¼Œä¸”å®Œå…¨é›†ä¸­æ–¹æ³•åœ¨æ™ºèƒ½ä½“æ•°é‡å¢åŠ æ—¶é¢ä¸´å¯æ‰©å±•æ€§é—®é¢˜ã€‚
2. æœ¬æ–‡æå‡ºé›†ä¸­ç½®æ¢ç­‰å˜å­¦ä¹ æ¡†æ¶ï¼Œåˆ©ç”¨å…¨å±€-å±€éƒ¨ç½®æ¢ç­‰å˜ç½‘ç»œæ¶æ„ï¼Œæ—¨åœ¨æé«˜å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„æ€§èƒ½ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCPEæ˜¾è‘—æå‡äº†æ ‡å‡†CTDEç®—æ³•çš„è¡¨ç°ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¸æœ€å…ˆè¿›çš„å®ç°ç›¸åŒ¹é…ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é›†ä¸­è®­ç»ƒä¸åˆ†æ•£æ‰§è¡Œï¼ˆCTDEï¼‰èŒƒå¼åœ¨å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰ä¸­å—åˆ°å¹¿æ³›å…³æ³¨ï¼Œç„¶è€Œï¼Œåˆ†æ•£ç­–ç•¥åœ¨éƒ¨åˆ†å¯è§‚æµ‹æ€§ä¸‹å¸¸å¸¸è¡¨ç°ä¸ä½³ï¼Œä¸”å®Œå…¨é›†ä¸­æ–¹æ³•åœ¨æ™ºèƒ½ä½“æ•°é‡å¢åŠ æ—¶é¢ä¸´å¯æ‰©å±•æ€§æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºé›†ä¸­ç½®æ¢ç­‰å˜ï¼ˆCPEï¼‰å­¦ä¹ æ¡†æ¶ï¼Œé‡‡ç”¨å®Œå…¨é›†ä¸­ç­–ç•¥ä»¥å…‹æœè¿™äº›é™åˆ¶ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§è½»é‡çº§ã€å¯æ‰©å±•ä¸”æ˜“äºå®ç°çš„å…¨å±€-å±€éƒ¨ç½®æ¢ç­‰å˜ï¼ˆGLPEï¼‰ç½‘ç»œæ¶æ„ã€‚å®éªŒè¡¨æ˜ï¼ŒCPEä¸ä»·å€¼åˆ†è§£å’Œæ¼”å‘˜-è¯„è®ºå®¶æ–¹æ³•æ— ç¼é›†æˆï¼Œæ˜¾è‘—æå‡äº†æ ‡å‡†CTDEç®—æ³•åœ¨å¤šä¸ªåˆä½œåŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ï¼ŒåŒ…æ‹¬MPEã€SMACå’ŒRWAREï¼Œå¹¶ä¸æœ€å…ˆè¿›çš„RWAREå®ç°ç›¸åŒ¹é…ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ä¸­åˆ†æ•£ç­–ç•¥åœ¨éƒ¨åˆ†å¯è§‚æµ‹æ€§ä¸‹çš„æ€§èƒ½ä¸è¶³ï¼Œä»¥åŠå®Œå…¨é›†ä¸­æ–¹æ³•åœ¨æ™ºèƒ½ä½“æ•°é‡å¢åŠ æ—¶çš„å¯æ‰©å±•æ€§æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºé›†ä¸­ç½®æ¢ç­‰å˜å­¦ä¹ æ¡†æ¶ï¼Œé‡‡ç”¨å…¨å±€-å±€éƒ¨ç½®æ¢ç­‰å˜ç½‘ç»œæ¶æ„ï¼Œåˆ©ç”¨é›†ä¸­ç­–ç•¥æ¥æå‡æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒç³»ç»Ÿçš„å¯æ‰©å±•æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬é›†ä¸­è®­ç»ƒé˜¶æ®µå’Œåˆ†æ•£æ‰§è¡Œé˜¶æ®µï¼Œä¸»è¦æ¨¡å—ä¸ºå…¨å±€-å±€éƒ¨ç½®æ¢ç­‰å˜ç½‘ç»œï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å¤šæ™ºèƒ½ä½“é—´çš„äº¤äº’å’Œä¿¡æ¯å…±äº«ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥äº†ç½®æ¢ç­‰å˜ç½‘ç»œæ¶æ„ï¼Œä½¿å¾—æ¨¡å‹åœ¨å¤„ç†ä¸åŒæ•°é‡çš„æ™ºèƒ½ä½“æ—¶ä»èƒ½ä¿æŒä¸€è‡´çš„æ€§èƒ½ï¼Œè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•çš„å±€é™æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šç½‘ç»œç»“æ„è®¾è®¡ä¸Šï¼ŒGLPEç½‘ç»œé‡‡ç”¨è½»é‡çº§æ¶æ„ï¼Œå…·å¤‡è‰¯å¥½çš„å¯æ‰©å±•æ€§ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šè€ƒè™‘äº†å¤šæ™ºèƒ½ä½“é—´çš„åä½œä¸ç«äº‰å…³ç³»ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œè®­ç»ƒç­–ç•¥åœ¨å®éªŒéƒ¨åˆ†è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒCPEå­¦ä¹ æ¡†æ¶åœ¨å¤šä¸ªåˆä½œåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨MPEã€SMACå’ŒRWAREç¯å¢ƒä¸‹ï¼Œç›¸è¾ƒäºæ ‡å‡†CTDEç®—æ³•ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œå¹¶ä¸æœ€å…ˆè¿›çš„RWAREå®ç°ç›¸åŒ¹é…ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½äº¤é€šç³»ç»Ÿã€æ— äººæœºç¼–é˜Ÿã€æœºå™¨äººåä½œç­‰å¤šæ™ºèƒ½ä½“ç³»ç»Ÿã€‚é€šè¿‡æå‡å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„åä½œèƒ½åŠ›ï¼ŒCPEå­¦ä¹ æ¡†æ¶èƒ½å¤Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­å®ç°æ›´é«˜æ•ˆçš„å†³ç­–ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The Centralized Training with Decentralized Execution (CTDE) paradigm has gained significant attention in multi-agent reinforcement learning (MARL) and is the foundation of many recent algorithms. However, decentralized policies operate under partial observability and often yield suboptimal performance compared to centralized policies, while fully centralized approaches typically face scalability challenges as the number of agents increases.
>   We propose Centralized Permutation Equivariant (CPE) learning, a centralized training and execution framework that employs a fully centralized policy to overcome these limitations. Our approach leverages a novel permutation equivariant architecture, Global-Local Permutation Equivariant (GLPE) networks, that is lightweight, scalable, and easy to implement. Experiments show that CPE integrates seamlessly with both value decomposition and actor-critic methods, substantially improving the performance of standard CTDE algorithms across cooperative benchmarks including MPE, SMAC, and RWARE, and matching the performance of state-of-the-art RWARE implementations.

