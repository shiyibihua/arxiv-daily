---
layout: default
title: UDA: Unsupervised Debiasing Alignment for Pair-wise LLM-as-a-Judge
---

# UDA: Unsupervised Debiasing Alignment for Pair-wise LLM-as-a-Judge

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.09724" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.09724v2</a>
  <a href="https://arxiv.org/pdf/2508.09724.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.09724v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.09724v2', 'UDA: Unsupervised Debiasing Alignment for Pair-wise LLM-as-a-Judge')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yang Zhang, Cunxiang Wang, Lindong Wu, Wenbo Yu, Yidong Wang, Guangsheng Bao, Jie Tang

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-13 (æ›´æ–°: 2025-11-16)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºUDAæ¡†æ¶ä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹è¯„ä¼°ä¸­çš„åè§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ— ç›‘ç£å­¦ä¹ ` `åè§å»é™¤` `å¤§è¯­è¨€æ¨¡å‹` `Eloè¯„åˆ†ç³»ç»Ÿ` `æ¨¡å‹è¯„ä¼°` `æˆå¯¹æ¯”è¾ƒ` `ä¸€è‡´æ€§å¯¹é½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æˆå¯¹è¯„ä¼°æ–¹æ³•å®¹æ˜“å—åˆ°è¯„å®¡åè§çš„å½±å“ï¼Œå¯¼è‡´ä¸åŒè¯„å®¡ä¹‹é—´çš„ç»“æœä¸ä¸€è‡´å’Œåæ–œã€‚
2. æœ¬æ–‡æå‡ºUDAæ¡†æ¶ï¼Œé€šè¿‡æ— ç›‘ç£æ–¹å¼åŠ¨æ€è°ƒæ•´Eloè¯„åˆ†ç³»ç»Ÿï¼Œå‡å°‘è¯„å®¡ä¹‹é—´çš„åˆ†æ­§ï¼Œä¿ƒè¿›ä¸€è‡´æ€§ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒUDAå°†è¯„å®¡è¯„åˆ†æ ‡å‡†å·®é™ä½äº†63.4%ï¼Œå¹¶å°†ä¸äººç±»åˆ¤æ–­çš„å¹³å‡ç›¸å…³æ€§æå‡äº†24.7%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æˆå¯¹è¯„ä¼°æ˜¯ä¸€ç§å¸¸è§çš„èŒƒå¼ï¼Œä½†å®¹æ˜“å—åˆ°åå¥½åè§çš„å½±å“ï¼Œå¯¼è‡´ä¸åŒè¯„å®¡ä¹‹é—´çš„æ’åä¸ä¸€è‡´ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡é¦–å…ˆå®è¯å±•ç¤ºäº†è·¨æ¨¡å‹è¯„ä¼°ä¸­çš„æ˜¾è‘—ä¸”å¼‚è´¨çš„åè§ã€‚æ¥ç€ï¼Œæå‡ºäº†UDAï¼ˆæ— ç›‘ç£å»åå¯¹é½ï¼‰æ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´Eloè¯„åˆ†ç³»ç»Ÿï¼Œå‡å°‘è¯„å®¡ä¹‹é—´çš„åˆ†æ­§ã€‚UDAåœ¨å®Œå…¨æ— ç›‘ç£çš„æƒ…å†µä¸‹è¿è¡Œï¼Œç›®æ ‡æ˜¯æœ€å°åŒ–æ‰€æœ‰è¯„å®¡çš„Eloè½¨è¿¹ä¹‹é—´çš„ç¦»æ•£åº¦ï¼Œä»è€Œä¿ƒä½¿å¯¹é›†ä½“å…±è¯†çš„å¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUDAæ˜¾è‘—é™ä½äº†è¯„å®¡è¯„åˆ†çš„æ ‡å‡†å·®ï¼Œæå‡äº†ä¸äººç±»åˆ¤æ–­çš„ç›¸å…³æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹è¯„ä¼°ä¸­çš„åè§é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨è·¨æ¨¡å‹è¯„ä¼°ä¸­å­˜åœ¨æ˜¾è‘—çš„åè§ï¼Œå¯¼è‡´è¯„å®¡ä¹‹é—´çš„ç»“æœä¸ä¸€è‡´ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šUDAæ¡†æ¶é€šè¿‡æ— ç›‘ç£æ–¹å¼åŠ¨æ€è°ƒæ•´Eloè¯„åˆ†ç³»ç»Ÿï¼Œå­¦ä¹ é€‚åº”æ€§åœ°è®¾ç½®Kå› å­å’Œä¼˜åŒ–èƒœç‡ï¼Œä»è€Œå‡å°‘è¯„å®¡ä¹‹é—´çš„åˆ†æ­§ï¼Œä¿ƒè¿›å¯¹é›†ä½“å…±è¯†çš„å¯¹é½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šUDAçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸€ä¸ªç´§å‡‘çš„ç¥ç»ç½‘ç»œæ¨¡å—ï¼Œè¯¥æ¨¡å—è´Ÿè´£åœ¨æ¯æ¬¡æˆå¯¹æ¯”è¾ƒä¸­è°ƒæ•´Kå› å­ï¼Œå¹¶è®¡ç®—èƒœç‡ã€‚æ•´ä¸ªè¿‡ç¨‹ä¸ä¾èµ–äºäººå·¥æ ‡ç­¾ï¼Œå®Œå…¨åŸºäºè¯„å®¡çš„è¯„åˆ†è½¨è¿¹ã€‚

**å…³é”®åˆ›æ–°**ï¼šUDAçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶æ— ç›‘ç£çš„å¯¹é½æœºåˆ¶ï¼Œé€šè¿‡æœ€å°åŒ–Eloè½¨è¿¹çš„ç¦»æ•£åº¦æ¥å®ç°è¯„å®¡ä¹‹é—´çš„å…±è¯†ï¼Œè¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„æœ‰ç›‘ç£å­¦ä¹ æ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒUDAé‡‡ç”¨äº†åŠ¨æ€è°ƒæ•´çš„Kå› å­å’Œèƒœç‡è®¡ç®—ï¼ŒæŸå¤±å‡½æ•°æ—¨åœ¨æœ€å°åŒ–è¯„å®¡è¯„åˆ†çš„ç¦»æ•£åº¦ï¼Œç½‘ç»œç»“æ„ä¸ºç´§å‡‘å‹ç¥ç»ç½‘ç»œï¼Œä»¥æé«˜è®¡ç®—æ•ˆç‡å’Œé€‚åº”æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒUDAæ˜¾è‘—é™ä½äº†è¯„å®¡è¯„åˆ†çš„æ ‡å‡†å·®ï¼Œæœ€é«˜å¯è¾¾63.4%çš„é™ä½ï¼ŒåŒæ—¶ä¸äººç±»åˆ¤æ–­çš„å¹³å‡ç›¸å…³æ€§æå‡äº†24.7%ã€‚æ­¤å¤–ï¼ŒUDAè¿˜æå‡äº†è¡¨ç°è¾ƒå·®çš„è¯„å®¡çš„è¯„åˆ†æ°´å¹³ï¼Œä½¿å…¶ä¸é«˜è´¨é‡è¯„å®¡çš„è¡¨ç°ç›¸å½“ï¼Œå¢å¼ºäº†è¯„ä¼°ç³»ç»Ÿçš„æ•´ä½“å¯é æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤§è¯­è¨€æ¨¡å‹çš„è¯„ä¼°ã€æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ¯”è¾ƒä»¥åŠäººæœºäº¤äº’ç³»ç»Ÿçš„ä¼˜åŒ–ã€‚é€šè¿‡å‡å°‘è¯„å®¡åè§ï¼ŒUDAèƒ½å¤Ÿæå‡æ¨¡å‹è¯„ä¼°çš„ç¨³å®šæ€§å’Œå¯é‡å¤æ€§ï¼Œè¿›è€Œæé«˜æ¨¡å‹çš„å®é™…åº”ç”¨ä»·å€¼ã€‚æœªæ¥ï¼ŒUDAå¯èƒ½åœ¨å¤šç§è¯„ä¼°åœºæ™¯ä¸­å¾—åˆ°å¹¿æ³›åº”ç”¨ï¼Œä¿ƒè¿›æ›´å…¬å¹³å’Œå¯é çš„è¯„ä¼°ä½“ç³»çš„å»ºç«‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Pairwise evaluation of Large Language Models (LLMs) is a common paradigm, but it is prone to preference bias, where judges systematically favor certain outputs, such as their own. This bias leads to inconsistent and skewed rankings across different judges. To address this, we first empirically demonstrate significant and heterogeneous biases in cross-model evaluations. We then propose UDA (Unsupervised Debiasing Alignment), a framework that reduces inter-judge disagreement by dynamically adjusting the Elo rating system. For each pairwise comparison, a compact neural network learns to adaptively set the K-factor and refine win probabilities. Crucially, UDA operates in a fully unsupervised manner, guided solely by the objective of minimizing the dispersion among the Elo trajectories of all judges. This forces an alignment towards a collective consensus, which serves as an unsupervised proxy for a more stable and reproducible evaluation. In addition, we provide theoretical motivation demonstrating how alignment towards a consensus can reduce aggregate system bias. Experiments show that UDA significantly reduces the inter-judge rating standard deviation by up to 63.4% and improves the average correlation with human judgments by 24.7%. Notably, UDA elevates the performance of poorly performing judges to achieve parity with high-quality ones, fostering a more robust and reliable evaluation ecosystem. Code and data are available at https://anonymous.4open.science/r/62AB93CD-23B4.

