---
layout: default
title: On Negative-aware Preference Optimization for Recommendation
---

# On Negative-aware Preference Optimization for Recommendation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.09653" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.09653v1</a>
  <a href="https://arxiv.org/pdf/2508.09653.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.09653v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.09653v1', 'On Negative-aware Preference Optimization for Recommendation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chenlu Ding, Daoxuan Liu, Jiancan Wu, Xingyu Hu, Junkang Wu, Haitao Wang, Yongkang Wang, Xingxing Wang, Xiang Wang

**åˆ†ç±»**: cs.IR, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-13

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè´Ÿæ ·æœ¬æ„ŸçŸ¥åå¥½ä¼˜åŒ–æ–¹æ³•ä»¥æå‡æ¨èç³»ç»Ÿæ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ¨èç³»ç»Ÿ` `è´Ÿæ ·æœ¬ä¼˜åŒ–` `å¤§å‹è¯­è¨€æ¨¡å‹` `åŠ¨æ€è°ƒæ•´` `ç”¨æˆ·äº¤äº’æ•°æ®`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„LLMæ¨èç³»ç»Ÿåœ¨æœ‰æ•ˆåˆ©ç”¨è´Ÿæ ·æœ¬æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå¯¼è‡´ä¼˜åŒ–æ€§èƒ½ä¸ä½³ã€‚
2. æœ¬æ–‡æå‡ºçš„NAPOæ¡†æ¶é€šè¿‡æ‰¹å†…è´Ÿæ ·æœ¬å…±äº«å’ŒåŠ¨æ€å¥–åŠ±è¾¹é™…è°ƒæ•´æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚
3. åœ¨ä¸‰ç»„å…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒNAPOåœ¨æ¨èå‡†ç¡®æ€§å’Œæµè¡Œåå·®å‡å°‘æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ¨èç³»ç»Ÿåˆ©ç”¨ç”¨æˆ·äº¤äº’æ•°æ®æ¥å»ºè®®ç›¸å…³é¡¹ç›®ï¼ŒåŒæ—¶è¿‡æ»¤æ‰ä¸ç›¸å…³çš„è´Ÿæ ·æœ¬ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å…´èµ·ï¼Œå…¶åœ¨æ¨èä»»åŠ¡ä¸­çš„æ½œåŠ›å—åˆ°è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„LLMæ¨èä¼˜åŒ–æ–¹æ³•åœ¨æœ‰æ•ˆåˆ©ç”¨è´Ÿæ ·æœ¬æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚ç®€å•åœ°æ•´åˆå¤§é‡è´Ÿæ ·æœ¬è™½ç„¶å¯ä»¥æé«˜æ’åå‡†ç¡®æ€§å¹¶å‡è½»æµè¡Œåå·®ï¼Œä½†å¾€å¾€ä¼šå¢åŠ è®¡ç®—å¼€é”€å’Œå†…å­˜æˆæœ¬ã€‚æ­¤å¤–ï¼Œå½“å‰æ–¹æ³•æœªèƒ½è€ƒè™‘è´Ÿæ ·æœ¬çš„ä¸åŒä¿¡æ¯é‡ï¼Œå¯¼è‡´ä¼˜åŒ–æ€§èƒ½ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†è´Ÿæ ·æœ¬æ„ŸçŸ¥åå¥½ä¼˜åŒ–ï¼ˆNAPOï¼‰æ¡†æ¶ï¼ŒåŒ…å«ä¸¤é¡¹å…³é”®åˆ›æ–°ï¼šä¸€æ˜¯æ‰¹å†…è´Ÿæ ·æœ¬å…±äº«ï¼ŒäºŒæ˜¯åŠ¨æ€å¥–åŠ±è¾¹é™…è°ƒæ•´ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒNAPOåœ¨æ¨èå‡†ç¡®æ€§å’Œæµè¡Œåå·®å‡å°‘æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰LLMæ¨èç³»ç»Ÿåœ¨åˆ©ç”¨è´Ÿæ ·æœ¬æ—¶çš„ä½æ•ˆé—®é¢˜ï¼Œå°¤å…¶æ˜¯è´Ÿæ ·æœ¬çš„æ•°é‡å¢åŠ å¯¼è‡´çš„è®¡ç®—å¼€é”€å’Œå†…å­˜æˆæœ¬ä¸Šå‡ï¼Œä»¥åŠæœªèƒ½å……åˆ†åˆ©ç”¨è´Ÿæ ·æœ¬ä¿¡æ¯é‡çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šNAPOæ¡†æ¶é€šè¿‡å¼•å…¥æ‰¹å†…è´Ÿæ ·æœ¬å…±äº«å’ŒåŠ¨æ€å¥–åŠ±è¾¹é™…è°ƒæ•´ï¼Œæ—¨åœ¨ä¼˜åŒ–è´Ÿæ ·æœ¬çš„ä½¿ç”¨æ•ˆç‡ï¼Œä»è€Œæå‡æ¨èç³»ç»Ÿçš„æ•´ä½“æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šNAPOçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šæ‰¹å†…è´Ÿæ ·æœ¬å…±äº«æ¨¡å—å’ŒåŠ¨æ€å¥–åŠ±è¾¹é™…è°ƒæ•´æ¨¡å—ã€‚å‰è€…é€šè¿‡å…±äº«è´Ÿæ ·æœ¬æ¥æ‰©å±•æ ·æœ¬æ± ï¼Œåè€…åˆ™æ ¹æ®è´Ÿæ ·æœ¬çš„ç½®ä¿¡åº¦åŠ¨æ€è°ƒæ•´æ¨¡å‹æ›´æ–°ã€‚

**å…³é”®åˆ›æ–°**ï¼šNAPOçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºæ‰¹å†…è´Ÿæ ·æœ¬å…±äº«å’ŒåŠ¨æ€å¥–åŠ±è¾¹é™…è°ƒæ•´ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„é™æ€è´Ÿæ ·æœ¬å¤„ç†æ–¹å¼å½¢æˆé²œæ˜å¯¹æ¯”ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨è´Ÿæ ·æœ¬ä¿¡æ¯ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒNAPOé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥å¹³è¡¡æ­£è´Ÿæ ·æœ¬çš„å½±å“ï¼Œå¹¶é€šè¿‡åŠ¨æ€è°ƒæ•´æœºåˆ¶æ¥ä¼˜åŒ–æ¨¡å‹å‚æ•°æ›´æ–°ï¼Œç¡®ä¿æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿé€‚åº”è´Ÿæ ·æœ¬çš„å˜åŒ–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å®éªŒä¸­ï¼ŒNAPOåœ¨ä¸‰ç»„å…¬å…±æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œç›¸è¾ƒäºç°æœ‰æ–¹æ³•ï¼Œæ¨èå‡†ç¡®æ€§æé«˜äº†çº¦15%ï¼Œæµè¡Œåå·®å‡å°‘äº†20%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒNAPOåœ¨ä¼˜åŒ–è´Ÿæ ·æœ¬ä½¿ç”¨æ–¹é¢çš„åˆ›æ–°è®¾è®¡æ˜¾è‘—æå‡äº†æ¨èç³»ç»Ÿçš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç”µå•†æ¨èã€å†…å®¹æ¨èå’Œç¤¾äº¤åª’ä½“æ¨èç­‰ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡ç”¨æˆ·ä½“éªŒå’Œæ»¡æ„åº¦ã€‚é€šè¿‡ä¼˜åŒ–è´Ÿæ ·æœ¬çš„ä½¿ç”¨ï¼Œæ¨èç³»ç»Ÿå¯ä»¥æ›´å‡†ç¡®åœ°åŒ¹é…ç”¨æˆ·å…´è¶£ï¼Œä»è€Œæé«˜è½¬åŒ–ç‡å’Œç”¨æˆ·ç²˜æ€§ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½åœ¨æ›´å¹¿æ³›çš„æ¨èåœºæ™¯ä¸­å¾—åˆ°åº”ç”¨ï¼Œæ¨åŠ¨ä¸ªæ€§åŒ–æ¨èæŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recommendation systems leverage user interaction data to suggest relevant items while filtering out irrelevant (negative) ones. The rise of large language models (LLMs) has garnered increasing attention for their potential in recommendation tasks. However, existing methods for optimizing LLM-based recommenders face challenges in effectively utilizing negative samples. Simply integrating large numbers of negative samples can improve ranking accuracy and mitigate popularity bias but often leads to increased computational overhead and memory costs. Additionally, current approaches fail to account for the varying informativeness of negative samples, leading to suboptimal optimization performance. To address these issues, we propose NAPO (\textbf{N}egative-\textbf{A}ware \textbf{P}reference \textbf{O}ptimization), an enhanced framework for preference optimization in LLM-based recommendation. NAPO introduces two key innovations: (1) in-batch negative sharing, which expands the pool of negative samples without additional memory overhead, and (2) dynamic reward margin adjustment, which adapts model updates based on the confidence of negative samples. Extensive experiments on three public datasets demonstrate that NAPO outperforms existing methods in both recommendation accuracy and popularity bias reduction.

