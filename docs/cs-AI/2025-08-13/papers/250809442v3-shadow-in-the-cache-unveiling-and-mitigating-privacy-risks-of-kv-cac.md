---
layout: default
title: Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache in LLM Inference
---

# Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache in LLM Inference

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.09442" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.09442v3</a>
  <a href="https://arxiv.org/pdf/2508.09442.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.09442v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.09442v3', 'Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache in LLM Inference')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhifan Luo, Shuo Shao, Su Zhang, Lijing Zhou, Yuke Hu, Chenxu Zhao, Zhihao Liu, Zhan Qin

**åˆ†ç±»**: cs.CR, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-13 (æ›´æ–°: 2025-12-08)

**å¤‡æ³¨**: This paper is accepted by Network and Distributed System Security Symposium (NDSS) 2026

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºKV-Cloakä»¥è§£å†³LLMæ¨ç†ä¸­çš„KVç¼“å­˜éšç§é£é™©é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `éšç§ä¿æŠ¤` `å¤§å‹è¯­è¨€æ¨¡å‹` `KVç¼“å­˜` `æ”»å‡»é˜²å¾¡` `æ•°æ®å®‰å…¨` `æœºå™¨å­¦ä¹ ` `æ¨¡å‹æ¨ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„KVç¼“å­˜æœºåˆ¶åœ¨åŠ é€ŸLLMæ¨ç†çš„åŒæ—¶ï¼Œå­˜åœ¨ä¸¥é‡çš„éšç§æ³„éœ²é£é™©ï¼Œæ”»å‡»è€…å¯é‡æ„ç”¨æˆ·è¾“å…¥ã€‚
2. æœ¬æ–‡æå‡ºKV-Cloakï¼Œé€šè¿‡å¯é€†çŸ©é˜µæ··æ·†å’Œæ“ä½œèåˆæŠ€æœ¯ï¼Œæä¾›äº†ä¸€ç§æœ‰æ•ˆçš„KVç¼“å­˜ä¿æŠ¤æ–¹æ¡ˆã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒKV-Cloakèƒ½å¤Ÿæœ‰æ•ˆæŠµå¾¡æ‰€æœ‰æ”»å‡»ï¼Œé‡æ„è´¨é‡é™è‡³éšæœºå™ªå£°ï¼Œä¸”å¯¹æ¨¡å‹æ€§èƒ½å½±å“æå°ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å…³é”®å€¼ï¼ˆKVï¼‰ç¼“å­˜æ˜¯åŠ é€Ÿå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†çš„åŸºç¡€æœºåˆ¶ï¼Œä½†å…¶æ•ˆç‡ä¼˜åŒ–å¼•å…¥äº†æ˜¾è‘—çš„éšç§é£é™©ã€‚æœ¬æ–‡é¦–æ¬¡å…¨é¢åˆ†æäº†è¿™äº›æ¼æ´ï¼Œè¡¨æ˜æ”»å‡»è€…å¯ä»¥ç›´æ¥ä»KVç¼“å­˜ä¸­é‡æ„æ•æ„Ÿç”¨æˆ·è¾“å…¥ã€‚æˆ‘ä»¬è®¾è®¡å¹¶å®ç°äº†ä¸‰ç§æ”»å‡»æ–¹å¼ï¼šç›´æ¥åæ¼”æ”»å‡»ã€å¹¿æ³›é€‚ç”¨çš„ç¢°æ’æ”»å‡»å’ŒåŸºäºè¯­ä¹‰çš„æ³¨å…¥æ”»å‡»ï¼Œå±•ç¤ºäº†KVç¼“å­˜éšç§æ³„éœ²é—®é¢˜çš„å®ç”¨æ€§å’Œä¸¥é‡æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†KV-Cloakï¼Œä¸€ç§æ–°é¢–ã€è½»é‡ä¸”é«˜æ•ˆçš„é˜²å¾¡æœºåˆ¶ï¼Œåˆ©ç”¨å¯é€†çŸ©é˜µæ··æ·†æ–¹æ¡ˆå’Œæ“ä½œèåˆæ¥ä¿æŠ¤KVç¼“å­˜ã€‚å®éªŒè¡¨æ˜ï¼ŒKV-Cloakæœ‰æ•ˆé˜»æ­¢æ‰€æœ‰æå‡ºçš„æ”»å‡»ï¼Œå°†é‡æ„è´¨é‡é™ä½åˆ°éšæœºå™ªå£°ï¼ŒåŒæ—¶å‡ ä¹ä¸å½±å“æ¨¡å‹å‡†ç¡®æ€§å’Œæ€§èƒ½å¼€é”€ï¼Œä¸ºå¯ä¿¡çš„LLMéƒ¨ç½²æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³KVç¼“å­˜ä¸­å­˜åœ¨çš„éšç§æ³„éœ²é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆé˜²æ­¢æ”»å‡»è€…é‡æ„ç”¨æˆ·è¾“å…¥ï¼Œå¯¼è‡´ç”¨æˆ·æ•æ„Ÿä¿¡æ¯çš„æ³„éœ²ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºKV-Cloakä½œä¸ºé˜²å¾¡æœºåˆ¶ï¼Œé€šè¿‡å¯é€†çŸ©é˜µæ··æ·†æŠ€æœ¯å’Œæ“ä½œèåˆï¼Œå¢å¼ºKVç¼“å­˜çš„å®‰å…¨æ€§ï¼Œç¡®ä¿ç”¨æˆ·éšç§ä¸è¢«æ³„éœ²ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šKV-Cloakçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®è¾“å…¥ã€KVç¼“å­˜æ··æ·†ã€æ“ä½œèåˆå’Œè¾“å‡ºé˜¶æ®µï¼Œç¡®ä¿åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¯¹ç¼“å­˜æ•°æ®è¿›è¡Œæœ‰æ•ˆä¿æŠ¤ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†å¯é€†çŸ©é˜µæ··æ·†æ–¹æ¡ˆï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒKV-Cloakåœ¨ä¿æŠ¤éšç§çš„åŒæ—¶ï¼Œå‡ ä¹ä¸å½±å“æ¨¡å‹çš„æ¨ç†å‡†ç¡®æ€§å’Œæ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æ··æ·†å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ï¼Œä»¥ç¡®ä¿æ··æ·†è¿‡ç¨‹çš„å¯é€†æ€§å’Œæœ‰æ•ˆæ€§ï¼ŒåŒæ—¶ä¼˜åŒ–äº†æ“ä½œèåˆçš„å®ç°ï¼Œä»¥é™ä½æ€§èƒ½å¼€é”€ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒKV-Cloakèƒ½å¤Ÿæœ‰æ•ˆé˜»æ­¢æ‰€æœ‰ä¸‰ç§æ”»å‡»æ–¹å¼ï¼Œé‡æ„è´¨é‡é™ä½è‡³éšæœºå™ªå£°ï¼Œä¸”åœ¨æ¨¡å‹å‡†ç¡®æ€§æ–¹é¢å‡ ä¹æ²¡æœ‰ä¸‹é™ï¼Œæ€§èƒ½å¼€é”€ä¹Ÿä¿æŒåœ¨æœ€ä½æ°´å¹³ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å¯è¡Œæ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½å®¢æœå’Œä»»ä½•ä¾èµ–äºå¤§å‹è¯­è¨€æ¨¡å‹çš„ç³»ç»Ÿã€‚é€šè¿‡æœ‰æ•ˆä¿æŠ¤ç”¨æˆ·éšç§ï¼ŒKV-Cloakä¸ºå¯ä¿¡èµ–çš„AIåº”ç”¨æä¾›äº†ä¿éšœï¼Œæœªæ¥å¯èƒ½åœ¨æ•°æ®éšç§æ³•è§„æ—¥ç›Šä¸¥æ ¼çš„èƒŒæ™¯ä¸‹ï¼Œæˆä¸ºè¡Œä¸šæ ‡å‡†ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The Key-Value (KV) cache, which stores intermediate attention computations (Key and Value pairs) to avoid redundant calculations, is a fundamental mechanism for accelerating Large Language Model (LLM) inference. However, this efficiency optimization introduces significant yet underexplored privacy risks. This paper provides the first comprehensive analysis of these vulnerabilities, demonstrating that an attacker can reconstruct sensitive user inputs directly from the KV-cache. We design and implement three distinct attack vectors: a direct Inversion Attack, a more broadly applicable and potent Collision Attack, and a semantic-based Injection Attack. These methods demonstrate the practicality and severity of KV-cache privacy leakage issues. To mitigate this, we propose KV-Cloak, a novel, lightweight, and efficient defense mechanism. KV-Cloak uses a reversible matrix-based obfuscation scheme, combined with operator fusion, to secure the KV-cache. Our extensive experiments show that KV-Cloak effectively thwarts all proposed attacks, reducing reconstruction quality to random noise. Crucially, it achieves this robust security with virtually no degradation in model accuracy and minimal performance overhead, offering a practical solution for trustworthy LLM deployment.

