---
layout: default
title: MEML-GRPO: Heterogeneous Multi-Expert Mutual Learning for RLVR Advancement
---

# MEML-GRPO: Heterogeneous Multi-Expert Mutual Learning for RLVR Advancement

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.09670" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.09670v2</a>
  <a href="https://arxiv.org/pdf/2508.09670.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.09670v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.09670v2', 'MEML-GRPO: Heterogeneous Multi-Expert Mutual Learning for RLVR Advancement')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Weitao Jia, Jinghui Lu, Haiyang Yu, Siqi Wang, Guozhi Tang, An-Lan Wang, Weijie Yin, Dingkang Yang, Yuxiang Nie, Bin Shan, Hao Feng, Irene Li, Kun Yang, Han Wang, Jingqun Tang, Teng Fu, Changhong Jin, Chao Feng, Xiaohui Lv, Can Huang

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-13 (æ›´æ–°: 2025-12-18)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMEML-GRPOä»¥è§£å†³RLVRä¸­çš„å¥–åŠ±ç¨€ç–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `å¯éªŒè¯å¥–åŠ±` `å¤šä¸“å®¶å­¦ä¹ ` `çŸ¥è¯†å…±äº«` `æ¨ç†èƒ½åŠ›æå‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ ‡å‡†RLVRæ–¹æ³•åœ¨é¢å¯¹å¥–åŠ±ç¨€ç–é—®é¢˜æ—¶è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨é”™è¯¯å€™é€‰ç­”æ¡ˆé¢‘ç¹å‡ºç°çš„æƒ…å†µä¸‹ï¼Œæ— æ³•æä¾›æœ‰æ•ˆçš„å­¦ä¹ ä¿¡å·ã€‚
2. æœ¬æ–‡æå‡ºçš„MEML-GRPOæ¡†æ¶é€šè¿‡å¤šä¸“å®¶äº’å­¦ä¹ æœºåˆ¶ï¼Œåˆ©ç”¨å¤šæ ·åŒ–çš„ä¸“å®¶æç¤ºç”Ÿæˆæ›´å¹¿æ³›çš„å“åº”ï¼Œä»è€Œæé«˜æ­£ç¡®ç­”æ¡ˆçš„è¯†åˆ«ç‡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒMEML-GRPOåœ¨å¤šä¸ªæ¨ç†åŸºå‡†ä¸Šå–å¾—äº†æ˜¾è‘—æå‡ï¼ŒQwençš„å¹³å‡æ€§èƒ½æå‡ä¸º4.89%ï¼ŒLlamaçš„æå‡å¹…åº¦è¾¾åˆ°11.33%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥çš„ç ”ç©¶è¡¨æ˜ï¼Œå…·æœ‰å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰æ˜¾è‘—æå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œæ ‡å‡†RLVRé¢ä¸´å¥–åŠ±ç¨€ç–çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨å›°éš¾ä»»åŠ¡ä¸­ï¼Œé”™è¯¯å€™é€‰ç­”æ¡ˆçš„é›¶å¥–åŠ±æ— æ³•æä¾›å­¦ä¹ ä¿¡å·ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†å¤šä¸“å®¶äº’å­¦ä¹ GRPOï¼ˆMEML-GRPOï¼‰æ¡†æ¶ï¼Œåˆ©ç”¨å¤šæ ·åŒ–çš„ä¸“å®¶æç¤ºç”Ÿæˆæ›´å¹¿æ³›çš„å“åº”ï¼Œä»è€Œæ˜¾è‘—æé«˜è¯†åˆ«æ­£ç¡®è§£å†³æ–¹æ¡ˆçš„å¯èƒ½æ€§ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸“å®¶é—´çš„äº’å­¦ä¹ æœºåˆ¶ï¼Œä¿ƒè¿›çŸ¥è¯†å…±äº«ä¸è½¬ç§»ï¼Œè¿›ä¸€æ­¥æå‡æ¨¡å‹åœ¨RLVRä¸­çš„è¡¨ç°ã€‚é€šè¿‡åœ¨å¤šä¸ªæ¨ç†åŸºå‡†ä¸Šçš„å¹¿æ³›å®éªŒï¼ŒMEML-GRPOæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ”¹è¿›ï¼ŒQwençš„å¹³å‡æ€§èƒ½æå‡ä¸º4.89%ï¼ŒLlamaä¸º11.33%ï¼Œæœ‰æ•ˆå…‹æœäº†ä¼ ç»ŸRLVRæ–¹æ³•çš„æ ¸å¿ƒå±€é™æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ ‡å‡†RLVRæ–¹æ³•åœ¨å¥–åŠ±ç¨€ç–æƒ…å†µä¸‹çš„å­¦ä¹ ä¿¡å·ç¼ºå¤±é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚ä»»åŠ¡ä¸­ï¼Œé”™è¯¯å€™é€‰ç­”æ¡ˆå¯¼è‡´çš„é›¶å¥–åŠ±æ— æ³•æœ‰æ•ˆæŒ‡å¯¼å­¦ä¹ ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMEML-GRPOæ¡†æ¶çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¤šä¸“å®¶äº’å­¦ä¹ æœºåˆ¶ï¼Œåˆ©ç”¨å¤šæ ·åŒ–çš„ä¸“å®¶æç¤ºç”Ÿæˆæ›´å¹¿æ³›çš„å“åº”ï¼Œä»è€Œå¢åŠ è¯†åˆ«æ­£ç¡®ç­”æ¡ˆçš„æœºä¼šã€‚è¿™æ ·çš„è®¾è®¡æ—¨åœ¨é€šè¿‡çŸ¥è¯†å…±äº«å’Œè½¬ç§»ï¼Œæå‡æ¨¡å‹çš„æ•´ä½“æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMEML-GRPOçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªä¸“å®¶æ¨¡å‹ï¼Œæ¯ä¸ªä¸“å®¶ç”Ÿæˆä¸åŒçš„å“åº”ï¼Œå¹¶é€šè¿‡äº’å­¦ä¹ æœºåˆ¶è¿›è¡ŒçŸ¥è¯†å…±äº«ã€‚è¯¥æ¡†æ¶åˆ†ä¸ºä¸“å®¶æç¤ºç”Ÿæˆã€å“åº”ç”Ÿæˆå’Œäº’å­¦ä¹ ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥äº†å¤šä¸“å®¶äº’å­¦ä¹ æœºåˆ¶ï¼Œä½¿å¾—ä¸åŒä¸“å®¶ä¹‹é—´èƒ½å¤Ÿæœ‰æ•ˆå…±äº«çŸ¥è¯†ï¼Œå…‹æœäº†ä¼ ç»ŸRLVRæ–¹æ³•çš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¥–åŠ±ç¨€ç–çš„æƒ…å†µä¸‹ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼Œæœ¬æ–‡è®¾ç½®äº†å¤šä¸ªä¸“å®¶æ¨¡å‹ï¼Œæ¯ä¸ªæ¨¡å‹ä½¿ç”¨ä¸åŒçš„æç¤ºç”Ÿæˆç­–ç•¥ï¼Œå¹¶è®¾è®¡äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¿ƒè¿›äº’å­¦ä¹ è¿‡ç¨‹ä¸­çš„çŸ¥è¯†å…±äº«ä¸è½¬ç§»ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMEML-GRPOåœ¨å¤šä¸ªæ¨ç†åŸºå‡†ä¸Šæ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ï¼ŒQwençš„å¹³å‡æ€§èƒ½æå‡ä¸º4.89%ï¼Œè€ŒLlamaçš„æå‡å¹…åº¦æ›´æ˜¯è¾¾åˆ°11.33%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆå…‹æœäº†ä¼ ç»ŸRLVRæ–¹æ³•çš„æ ¸å¿ƒå±€é™æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿå’Œè‡ªåŠ¨åŒ–æ¨ç†ç­‰ã€‚é€šè¿‡æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼ŒMEML-GRPOèƒ½å¤Ÿåœ¨å¤æ‚ä»»åŠ¡ä¸­æä¾›æ›´å‡†ç¡®çš„ç­”æ¡ˆï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances demonstrate that reinforcement learning with verifiable rewards (RLVR) significantly enhances the reasoning capabilities of large language models (LLMs). However, standard RLVR faces challenges with reward sparsity, where zero rewards from consistently incorrect candidate answers provide no learning signal, particularly in challenging tasks. To address this, we propose Multi-Expert Mutual Learning GRPO (MEML-GRPO), an innovative framework that utilizes diverse expert prompts as system prompts to generate a broader range of responses, substantially increasing the likelihood of identifying correct solutions. Additionally, we introduce an inter-expert mutual learning mechanism that facilitates knowledge sharing and transfer among experts, further boosting the model's performance through RLVR. Extensive experiments across multiple reasoning benchmarks show that MEML-GRPO delivers significant improvements, achieving an average performance gain of 4.89% with Qwen and 11.33% with Llama, effectively overcoming the core limitations of traditional RLVR methods.

