---
layout: default
title: Mathematical Computation and Reasoning Errors by Large Language Models
---

# Mathematical Computation and Reasoning Errors by Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.09932" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.09932v2</a>
  <a href="https://arxiv.org/pdf/2508.09932.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.09932v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.09932v2', 'Mathematical Computation and Reasoning Errors by Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Liang Zhang, Edith Aurora Graf

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-13 (æ›´æ–°: 2025-08-14)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦è®¡ç®—ä¸­çš„é”™è¯¯ä»¥æå‡æ•™è‚²æ•ˆæœ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æ•°å­¦æ•™è‚²` `æ¨ç†é”™è¯¯` `æ•™è‚²æŠ€æœ¯` `æ€§èƒ½è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„LLMsåœ¨æ•°å­¦é—®é¢˜è§£å†³ä¸­å­˜åœ¨å‡†ç¡®æ€§ä¸è¶³å’Œæ¨ç†é”™è¯¯çš„é—®é¢˜ï¼Œå½±å“æ•™è‚²æ•ˆæœã€‚
2. æœ¬æ–‡é€šè¿‡æ„å»ºå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦ä»»åŠ¡ï¼Œè¯„ä¼°LLMsçš„è¡¨ç°ï¼Œå¹¶åˆ†æå…¶é”™è¯¯ç±»å‹ï¼Œæ—¨åœ¨æå‡å…¶åœ¨æ•™è‚²ä¸­çš„åº”ç”¨ã€‚
3. ç ”ç©¶å‘ç°ï¼Œå¢å¼ºæ¨ç†çš„OpenAI o1æ¨¡å‹åœ¨æ•°å­¦ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒåŒä»£ç†é…ç½®æ˜¾è‘—æé«˜äº†æ•´ä½“æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨AIé©±åŠ¨çš„æ•™è‚²ä¸­ï¼Œå°¤å…¶æ˜¯æ•°å­¦æ•™è‚²ä¸­ï¼Œè¶Šæ¥è¶Šå¤šåœ°è¢«åº”ç”¨äºæ•™å­¦å’Œè¯„ä¼°ã€‚æœ¬æ–‡ç ”ç©¶äº†å››ç§LLMsï¼ˆOpenAI GPT-4oå’Œo1ï¼ŒDeepSeek-V3å’ŒDeepSeek-R1ï¼‰åœ¨è§£å†³ç®—æœ¯ã€ä»£æ•°å’Œæ•°è®ºç­‰ä¸‰ç±»æ•°å­¦ä»»åŠ¡æ—¶çš„å‡†ç¡®æ€§ï¼Œå¹¶è¯†åˆ«äº†è§£å†³æ–¹æ¡ˆä¸­çš„é€æ­¥æ¨ç†é”™è¯¯ã€‚ç ”ç©¶æ„å»ºäº†å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦ä»»åŠ¡ï¼Œç³»ç»Ÿåˆ†æäº†æœ€ç»ˆç­”æ¡ˆçš„å‡†ç¡®æ€§åŠä¸ªåˆ«è§£é¢˜æ­¥éª¤ä¸­çš„é”™è¯¯ã€‚ç»“æœè¡¨æ˜ï¼Œå¢å¼ºæ¨ç†èƒ½åŠ›çš„OpenAI o1æ¨¡å‹åœ¨æ‰€æœ‰ä¸‰ç±»ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ›´é«˜çš„å‡†ç¡®æ€§ï¼Œè€Œç¨‹åºæ€§å¤±è¯¯æ˜¯æœ€å¸¸è§çš„é”™è¯¯ç±»å‹ã€‚åŒä»£ç†é…ç½®æ˜¾è‘—æå‡äº†æ•´ä½“è¡¨ç°ã€‚è¿™äº›å‘ç°ä¸ºæå‡LLMsæ€§èƒ½æä¾›äº†å¯è¡Œçš„è§è§£ï¼Œå¹¶å¼ºè°ƒäº†å°†LLMsæœ‰æ•ˆæ•´åˆåˆ°æ•°å­¦æ•™è‚²ä¸­çš„ç­–ç•¥ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦è®¡ç®—ä¸­å¸¸è§çš„æ¨ç†é”™è¯¯å’Œå‡†ç¡®æ€§ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚æ•°å­¦ä»»åŠ¡æ—¶å®¹æ˜“å‡ºç°é”™è¯¯ï¼Œå½±å“æ•™è‚²åé¦ˆçš„å¯é æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æ„å»ºå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦ä»»åŠ¡ï¼Œç³»ç»Ÿè¯„ä¼°ä¸åŒLLMsçš„è¡¨ç°ï¼Œç‰¹åˆ«å…³æ³¨é€æ­¥æ¨ç†ä¸­çš„é”™è¯¯ï¼Œæ—¨åœ¨è¯†åˆ«å¹¶æ”¹è¿›è¿™äº›æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶è®¾è®¡äº†å¤šç§æ•°å­¦ä»»åŠ¡ï¼Œåˆ†ä¸ºç®—æœ¯ã€ä»£æ•°å’Œæ•°è®ºä¸‰ç±»ï¼Œé‡‡ç”¨å•ä»£ç†å’ŒåŒä»£ç†é…ç½®è¿›è¡Œæµ‹è¯•ã€‚æ¯ä¸ªæ¨¡å‹çš„ç­”æ¡ˆå‡†ç¡®æ€§å’Œè§£é¢˜æ­¥éª¤ä¸­çš„é”™è¯¯è¢«ç³»ç»Ÿåˆ†æå’Œç¼–ç ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæ„å»ºäº†é’ˆå¯¹LLMsçš„æŒ‘æˆ˜æ€§æ•°å­¦ä»»åŠ¡ï¼Œå¹¶é€šè¿‡ç³»ç»Ÿåˆ†æé”™è¯¯ç±»å‹ï¼Œæä¾›äº†æ”¹è¿›æ¨¡å‹æ€§èƒ½çš„å®è¯ä¾æ®ã€‚è¿™ä¸ä¼ ç»ŸåŸºå‡†æµ‹è¯•æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºæ›´å…·é’ˆå¯¹æ€§å’Œå®ç”¨æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†ä¸åŒçš„æ¨¡å‹é…ç½®å’Œä»»åŠ¡è®¾è®¡ï¼Œç‰¹åˆ«å…³æ³¨ç¨‹åºæ€§å¤±è¯¯å’Œæ¦‚å¿µæ€§è¯¯è§£çš„åˆ†æï¼ŒåŒä»£ç†é…ç½®çš„å¼•å…¥æ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ•´ä½“è¡¨ç°ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†è¯´æ˜ï¼Œéœ€å‚è€ƒåŸæ–‡è·å–æ›´å¤šæŠ€æœ¯ç»†èŠ‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¢å¼ºæ¨ç†èƒ½åŠ›çš„OpenAI o1æ¨¡å‹åœ¨æ‰€æœ‰ä¸‰ç±»æ•°å­¦ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ›´é«˜æˆ–æ¥è¿‘å®Œç¾çš„å‡†ç¡®æ€§ã€‚ç¨‹åºæ€§å¤±è¯¯æ˜¯æœ€å¸¸è§çš„é”™è¯¯ç±»å‹ï¼ŒåŒä»£ç†é…ç½®çš„å¼•å…¥æ˜¾è‘—æå‡äº†æ•´ä½“æ€§èƒ½ï¼Œè¡¨æ˜è¯¥æ–¹æ³•åœ¨æ•™è‚²åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²æŠ€æœ¯ã€æ™ºèƒ½è¾…å¯¼ç³»ç»Ÿå’Œä¸ªæ€§åŒ–å­¦ä¹ å¹³å°ã€‚é€šè¿‡æå‡LLMsåœ¨æ•°å­¦æ•™è‚²ä¸­çš„è¡¨ç°ï¼Œå¯ä»¥ä¸ºå­¦ç”Ÿæä¾›æ›´å‡†ç¡®çš„åé¦ˆå’ŒæŒ‡å¯¼ï¼Œè¿›è€Œæ”¹å–„å­¦ä¹ æ•ˆæœã€‚æœªæ¥ï¼Œç ”ç©¶æˆæœå¯èƒ½æ¨åŠ¨AIåœ¨æ•™è‚²é¢†åŸŸçš„æ›´å¹¿æ³›åº”ç”¨ï¼Œæå‡æ•™å­¦è´¨é‡å’Œæ•ˆç‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) are increasingly utilized in AI-driven educational instruction and assessment, particularly within mathematics education. The capability of LLMs to generate accurate answers and detailed solutions for math problem-solving tasks is foundational for ensuring reliable and precise feedback and assessment in math education practices. Our study focuses on evaluating the accuracy of four LLMs (OpenAI GPT-4o and o1, DeepSeek-V3 and DeepSeek-R1) solving three categories of math tasks, including arithmetic, algebra, and number theory, and identifies step-level reasoning errors within their solutions. Instead of relying on standard benchmarks, we intentionally build math tasks (via item models) that are challenging for LLMs and prone to errors. The accuracy of final answers and the presence of errors in individual solution steps were systematically analyzed and coded. Both single-agent and dual-agent configurations were tested. It is observed that the reasoning-enhanced OpenAI o1 model consistently achieved higher or nearly perfect accuracy across all three math task categories. Analysis of errors revealed that procedural slips were the most frequent and significantly impacted overall performance, while conceptual misunderstandings were less frequent. Deploying dual-agent configurations substantially improved overall performance. These findings offer actionable insights into enhancing LLM performance and underscore effective strategies for integrating LLMs into mathematics education, thereby advancing AI-driven instructional practices and assessment precision.

