---
layout: default
title: Enabling MoE on the Edge via Importance-Driven Expert Scheduling
---

# Enabling MoE on the Edge via Importance-Driven Expert Scheduling

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.18983" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.18983v2</a>
  <a href="https://arxiv.org/pdf/2508.18983.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.18983v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.18983v2', 'Enabling MoE on the Edge via Importance-Driven Expert Scheduling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Guoying Zhu, Meng Li, Haipeng Dai, Xuechen Liu, Weijun Wang, Keran Li, Jun xiao, Ligeng Chen, Wei Wang

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-26 (æ›´æ–°: 2025-11-19)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é€šè¿‡é‡è¦æ€§é©±åŠ¨çš„ä¸“å®¶è°ƒåº¦å®ç°è¾¹ç¼˜è®¾å¤‡ä¸Šçš„MoE**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ··åˆä¸“å®¶` `è¾¹ç¼˜è®¡ç®—` `åŠ¨æ€è°ƒåº¦` `GPUç¼“å­˜` `æ¨¡å‹ä¼˜åŒ–` `è§£ç å»¶è¿Ÿ` `æ™ºèƒ½è®¾å¤‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„MoEæ¶æ„åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²æ—¶é¢ä¸´å†…å­˜é™åˆ¶ï¼Œå¯¼è‡´åŠ¨æ€ä¸“å®¶å¸è½½æˆä¸ºå¿…è¦ï¼Œä½†ä¼ ç»Ÿæ–¹æ³•æœªèƒ½æœ‰æ•ˆè§£å†³è¿™ä¸€é—®é¢˜ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºä¸“å®¶é‡è¦æ€§çš„è°ƒåº¦ç­–ç•¥ï¼Œé€šè¿‡æ›¿æ¢ä½é‡è¦æ€§ä¸“å®¶æ¥ä¼˜åŒ–å†…å­˜ä½¿ç”¨å’Œæ•°æ®ä¼ è¾“ï¼Œæå‡äº†ç³»ç»Ÿæ•ˆç‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•å®ç°äº†48%çš„è§£ç å»¶è¿Ÿé™ä½å’Œè¶…è¿‡60%çš„ä¸“å®¶ç¼“å­˜å‘½ä¸­ç‡ï¼ŒåŒæ—¶ä¿æŒäº†å‡ ä¹æ— æŸçš„å‡†ç¡®æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„å·²æˆä¸ºæ‰©å±•å¤§å‹è¯­è¨€æ¨¡å‹çš„å…³é”®æŠ€æœ¯ï¼Œé€šè¿‡æ¯ä¸ªæŸ¥è¯¢æ¿€æ´»ä»…ä¸€éƒ¨åˆ†ä¸“å®¶ã€‚ç„¶è€Œï¼Œåœ¨æ¶ˆè´¹çº§è¾¹ç¼˜ç¡¬ä»¶ä¸Šéƒ¨ç½²MoEå—åˆ°è®¾å¤‡å†…å­˜é™åˆ¶çš„çº¦æŸï¼Œå› æ­¤åŠ¨æ€ä¸“å®¶å¸è½½å˜å¾—è‡³å…³é‡è¦ã€‚ä¸ä»¥å¾€å°†å¸è½½è§†ä¸ºè°ƒåº¦é—®é¢˜çš„å·¥ä½œä¸åŒï¼Œæˆ‘ä»¬åˆ©ç”¨ä¸“å®¶çš„é‡è¦æ€§æ¥æŒ‡å¯¼å†³ç­–ï¼Œç”¨åŠŸèƒ½ç›¸ä¼¼ä¸”å·²ç¼“å­˜äºGPUå†…å­˜ä¸­çš„ä½é‡è¦æ€§æ¿€æ´»ä¸“å®¶è¿›è¡Œæ›¿æ¢ï¼Œä»è€Œä¿æŒå‡†ç¡®æ€§ã€‚è¯¥è®¾è®¡å‡å°‘äº†å†…å­˜ä½¿ç”¨å’Œæ•°æ®ä¼ è¾“ï¼ŒåŒæ—¶å¤§å¹…æ¶ˆé™¤PCIeå¼€é”€ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è°ƒåº¦ç­–ç•¥ï¼Œä»¥æœ€å¤§åŒ–GPUç¼“å­˜ä¸“å®¶çš„é‡ç”¨ç‡ï¼Œè¿›ä¸€æ­¥æå‡æ•ˆç‡ã€‚å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¿æŒå‡ ä¹æ— æŸå‡†ç¡®æ€§çš„åŒæ—¶ï¼Œè§£ç å»¶è¿Ÿé™ä½äº†48%ï¼Œä¸“å®¶ç¼“å­˜å‘½ä¸­ç‡è¶…è¿‡60%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨æ¶ˆè´¹çº§è¾¹ç¼˜ç¡¬ä»¶ä¸Šéƒ¨ç½²æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„æ—¶ï¼Œç”±äºå†…å­˜é™åˆ¶å¯¼è‡´çš„åŠ¨æ€ä¸“å®¶å¸è½½é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨ä¸“å®¶çš„é‡è¦æ€§ï¼Œå¯¼è‡´èµ„æºæµªè´¹å’Œæ€§èƒ½ç“¶é¢ˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨ä¸“å®¶é‡è¦æ€§è¿›è¡Œè°ƒåº¦çš„ç­–ç•¥ï¼Œé€šè¿‡ç”¨åŠŸèƒ½ç›¸ä¼¼ä¸”å·²ç¼“å­˜çš„ä¸“å®¶æ›¿æ¢ä½é‡è¦æ€§ä¸“å®¶ï¼Œä»è€Œä¼˜åŒ–å†…å­˜ä½¿ç”¨å’Œæ•°æ®ä¼ è¾“ï¼Œä¿æŒæ¨¡å‹çš„å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸“å®¶é€‰æ‹©æ¨¡å—ã€é‡è¦æ€§è¯„ä¼°æ¨¡å—å’Œè°ƒåº¦ç­–ç•¥æ¨¡å—ã€‚ä¸“å®¶é€‰æ‹©æ¨¡å—æ ¹æ®è¾“å…¥æŸ¥è¯¢çš„ç‰¹å¾æ¿€æ´»ç›¸åº”çš„ä¸“å®¶ï¼Œé‡è¦æ€§è¯„ä¼°æ¨¡å—åˆ™è®¡ç®—æ¯ä¸ªä¸“å®¶çš„é‡è¦æ€§ï¼Œè°ƒåº¦ç­–ç•¥æ¨¡å—è´Ÿè´£åŠ¨æ€æ›¿æ¢ä½é‡è¦æ€§ä¸“å®¶ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥äº†ä¸“å®¶é‡è¦æ€§é©±åŠ¨çš„è°ƒåº¦ç­–ç•¥ï¼Œä¸ä¼ ç»Ÿçš„è°ƒåº¦æ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨GPUç¼“å­˜ï¼Œå‡å°‘å†…å­˜å ç”¨å’Œæ•°æ®ä¼ è¾“å¼€é”€ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œæˆ‘ä»¬è®¾ç½®äº†ä¸“å®¶çš„é‡è¦æ€§è¯„ä¼°æ ‡å‡†ï¼Œå¹¶ä¼˜åŒ–äº†è°ƒåº¦ç­–ç•¥ä»¥æœ€å¤§åŒ–GPUç¼“å­˜çš„é‡ç”¨ç‡ã€‚æ­¤å¤–ï¼Œé‡‡ç”¨äº†é€‚åº”æ€§è°ƒæ•´çš„å‚æ•°è®¾ç½®ï¼Œä»¥ç¡®ä¿åœ¨ä¸åŒåœºæ™¯ä¸‹çš„æ€§èƒ½è¡¨ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•åœ¨è§£ç å»¶è¿Ÿä¸Šå®ç°äº†48%çš„é™ä½ï¼Œä¸“å®¶ç¼“å­˜å‘½ä¸­ç‡è¶…è¿‡60%ï¼ŒåŒæ—¶ä¿æŒäº†å‡ ä¹æ— æŸçš„å‡†ç¡®æ€§ã€‚è¿™äº›ç»“æœæ˜¾è‘—ä¼˜äºç°æœ‰çš„MoEè°ƒåº¦æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šçš„å®é™…åº”ç”¨æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è¾¹ç¼˜è®¡ç®—ã€æ™ºèƒ½è®¾å¤‡å’Œç§»åŠ¨ç«¯AIåº”ç”¨ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨èµ„æºå—é™ç¯å¢ƒä¸‹çš„è¿è¡Œæ•ˆç‡ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½æ¨åŠ¨æ›´å¤šAIæ¨¡å‹åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šçš„åº”ç”¨ï¼Œä¿ƒè¿›æ™ºèƒ½åŒ–æœåŠ¡çš„æ™®åŠã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The Mixture of Experts (MoE) architecture has emerged as a key technique for scaling Large Language Models by activating only a subset of experts per query. Deploying MoE on consumer-grade edge hardware, however, is constrained by limited device memory, making dynamic expert offloading essential. Unlike prior work that treats offloading purely as a scheduling problem, we leverage expert importance to guide decisions, substituting low-importance activated experts with functionally similar ones already cached in GPU memory, thereby preserving accuracy. As a result, this design reduces memory usage and data transfer, while largely eliminating PCIe overhead. In addition, we introduce a scheduling policy that maximizes the reuse ratio of GPU-cached experts, further boosting efficiency. Extensive evaluations show that our approach delivers 48% lower decoding latency with over 60% expert cache hit rate, while maintaining nearly lossless accuracy.

