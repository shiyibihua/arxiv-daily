---
layout: default
title: Beyond Memorization: Reasoning-Driven Synthesis as a Mitigation Strategy Against Benchmark Contamination
---

# Beyond Memorization: Reasoning-Driven Synthesis as a Mitigation Strategy Against Benchmark Contamination

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.00072" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.00072v2</a>
  <a href="https://arxiv.org/pdf/2509.00072.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.00072v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.00072v2', 'Beyond Memorization: Reasoning-Driven Synthesis as a Mitigation Strategy Against Benchmark Contamination')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Terry Jingchen Zhang, Gopal Dev, Ning Wang, Nicole Ni, Wenyuan Jiang, Yinya Huang, Bernhard SchÃ¶lkopf, Mrinmaya Sachan, Zhijing Jin

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-26 (æ›´æ–°: 2025-10-06)

**å¤‡æ³¨**: The authors choose to withdraw this manuscript as it constitutes incomplete work

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºæ¨ç†é©±åŠ¨åˆæˆçš„ç­–ç•¥ä»¥åº”å¯¹åŸºå‡†æ±¡æŸ“é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æ•°æ®æ±¡æŸ“` `æ¨ç†èƒ½åŠ›` `åˆæˆé—®é¢˜` `åŸºå‡†è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹æ—¶é¢ä¸´æ•°æ®æ±¡æŸ“çš„æŒ‘æˆ˜ï¼Œå¯¼è‡´å¯¹æ¨ç†èƒ½åŠ›çš„æµ‹é‡ä¸å‡†ç¡®ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ¨ç†é©±åŠ¨åˆæˆçš„æ¡†æ¶ï¼Œç›´æ¥ä»arXivè®ºæ–‡ä¸­ç”Ÿæˆå¤šæ­¥æ¨ç†é—®é¢˜ï¼Œä»¥æé«˜åŸºå‡†çš„æœ‰æ•ˆæ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œä¸åŒæ¨¡å‹åœ¨çŸ¥è¯†æˆªæ­¢æ—¥æœŸé™„è¿‘çš„æ€§èƒ½æ²¡æœ‰æ˜¾è‘—è¡°é€€ï¼ŒéªŒè¯äº†åˆæˆæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½åŠ›è¯„ä¼°çš„å…³æ³¨å¢åŠ ï¼Œæ•°æ®æ±¡æŸ“é—®é¢˜å¼•å‘äº†å¯¹é™æ€åŸºå‡†æ˜¯å¦çœŸæ­£æµ‹é‡æ¨ç†èƒ½åŠ›çš„è´¨ç–‘ã€‚æœ¬æ–‡é€šè¿‡ä¸€ä¸ªæ— é™å¯æ‰©å±•çš„æ¡†æ¶ï¼Œç›´æ¥ä»arXivè®ºæ–‡åˆæˆç ”ç©¶çº§é—®ç­”ï¼Œåˆ©ç”¨ç ”ç©¶å‡ºç‰ˆç‰©çš„è‡ªç„¶æ—¶é—´ç»“æ„ï¼Œè¯„ä¼°äº†çŸ¥è¯†æˆªæ­¢æ—¥æœŸå‰åæ€§èƒ½è¡°é€€çš„æ½œåœ¨æ±¡æŸ“ã€‚æˆ‘ä»¬å¯¹4ä¸ªå‰æ²¿æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºä¸åŒè§„æ¨¡å’Œå¼€å‘è€…çš„æ¨¡å‹åœ¨çŸ¥è¯†æˆªæ­¢æ—¥æœŸé™„è¿‘æ²¡æœ‰æ˜¾è‘—çš„æ€§èƒ½è¡°é€€ã€‚æˆ‘ä»¬å‡è®¾å¤šæ­¥æ¨ç†çš„åˆæˆç®¡é“æä¾›äº†æ¯”æµ…å±‚è®°å¿†æ›´æ·±å±‚æ¬¡çš„å¤æ‚æ€§ï¼Œæœ‰æ•ˆåœ°ä½œä¸ºåŸºå‡†æ±¡æŸ“çš„ç¼“è§£ç­–ç•¥ã€‚æˆ‘ä»¬å…¨é¢å¼€æºäº†ä»£ç å’Œæ•°æ®é›†ï¼Œä»¥ä¿ƒè¿›å¯é‡å¤æ€§ï¼Œå¹¶å€¡å¯¼ä¼˜å…ˆè€ƒè™‘æ¨ç†é©±åŠ¨çš„åˆæˆæ¥æ„å»ºåŸºå‡†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹èƒ½åŠ›è¯„ä¼°ä¸­çš„æ•°æ®æ±¡æŸ“é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºé™æ€åŸºå‡†ï¼Œå¯èƒ½å¯¼è‡´å¯¹æ¨¡å‹æ¨ç†èƒ½åŠ›çš„è¯¯åˆ¤ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡åˆæˆå¤šæ­¥æ¨ç†é—®é¢˜ï¼Œåˆ©ç”¨ç ”ç©¶æ–‡çŒ®çš„æ—¶é—´ç»“æ„ï¼Œæä¾›æ›´å¤æ‚çš„è¯„ä¼°ä»»åŠ¡ï¼Œä»è€Œå‡å°‘æ¨¡å‹çš„è®°å¿†ä¾èµ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€é—®é¢˜åˆæˆå’Œæ€§èƒ½è¯„ä¼°ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œä»arXivè®ºæ–‡ä¸­æå–ä¿¡æ¯ï¼Œç„¶åç”Ÿæˆå¤šæ­¥æ¨ç†é—®é¢˜ï¼Œæœ€åè¯„ä¼°æ¨¡å‹åœ¨è¿™äº›é—®é¢˜ä¸Šçš„è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§æ¨ç†é©±åŠ¨çš„åˆæˆæ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåŒºåˆ†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ä¸è®°å¿†èƒ½åŠ›ï¼Œä¸ä¼ ç»Ÿçš„åŸºå‡†è¯„ä¼°æ–¹æ³•æœ¬è´¨ä¸Šä¸åŒã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨åˆæˆè¿‡ç¨‹ä¸­ï¼Œè®¾ç½®äº†å¤šæ­¥æ¨ç†çš„å¤æ‚æ€§ï¼Œå¹¶è®¾è®¡äº†ç›¸åº”çš„è¯„ä¼°æ ‡å‡†ï¼Œä»¥ç¡®ä¿ç”Ÿæˆé—®é¢˜çš„æœ‰æ•ˆæ€§å’ŒæŒ‘æˆ˜æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯„ä¼°çš„å››ä¸ªå‰æ²¿æ¨¡å‹åœ¨çŸ¥è¯†æˆªæ­¢æ—¥æœŸé™„è¿‘çš„æ€§èƒ½æ²¡æœ‰æ˜¾è‘—è¡°é€€ï¼Œè¡¨æ˜åˆæˆçš„å¤šæ­¥æ¨ç†é—®é¢˜æœ‰æ•ˆåœ°å‡å°‘äº†æ¨¡å‹çš„è®°å¿†ä¾èµ–ã€‚è¿™ä¸€å‘ç°ä¸ä»¥å¾€ç ”ç©¶çš„ç»“æœå½¢æˆé²œæ˜å¯¹æ¯”ï¼Œå¼ºè°ƒäº†æ¨ç†é©±åŠ¨åˆæˆçš„é‡è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ•™è‚²è¯„ä¼°å’Œäººå·¥æ™ºèƒ½æ¨¡å‹çš„èƒ½åŠ›è¯„ä¼°ã€‚é€šè¿‡æä¾›æ›´æœ‰æ•ˆçš„åŸºå‡†æ„å»ºæ–¹æ³•ï¼Œå¯ä»¥å¸®åŠ©ç ”ç©¶äººå‘˜æ›´å‡†ç¡®åœ°è¯„ä¼°å’Œæ”¹è¿›è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Capability evaluation of large language models (LLMs) is increasingly shadowed by rising concerns of data contamination that cast doubts on whether static benchmarks measure genuine reasoning or mere memorization. We present an empirical study using an infinitely scalable framework to synthesize research-level QA directly from arXiv papers, harnessing the natural temporal structure of research publications where performance decay after knowledge cutoffs may indicate potential contamination. We evaluated 4 frontier model represented by 2 models of different knowledge cutoff dates per family on 1,643 multi-step reasoning questions synthesized from 20,277 arXiv papers stratified over 26 months, covering at least 6 months before and after all cutoff dates. Our results consistently showed a lack of significant performance decay near knowledge cutoff dates for models of various sizes, developers, and release dates. We further performed a comparative analysis with previous longitudinal studies that reported significant post-cutoff performance decay using directly retrieved questions based on public data. we hypothesize that the multi-step reasoning required by our synthesis pipeline offered additional complexity that goes deeper than shallow memorization, which effectively serves a mitigation strategy against benchmark contamination. We fully open source our code and dataset to aid reproducibility and advocate for a paradigm shift that prioritize reasoning-driven synthesis to construct benchmarks over simply collecting newly released questions periodically.

