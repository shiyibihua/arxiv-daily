---
layout: default
title: Interactive Evaluation of Large Language Models for Multi-Requirement Software Engineering Tasks
---

# Interactive Evaluation of Large Language Models for Multi-Requirement Software Engineering Tasks

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.18905" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.18905v1</a>
  <a href="https://arxiv.org/pdf/2508.18905.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.18905v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.18905v1', 'Interactive Evaluation of Large Language Models for Multi-Requirement Software Engineering Tasks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Dimitrios Rontogiannis, Maxime Peyrard, Nicolas Baldwin, Martin Josifoski, Robert West, Dimitrios Gunopulos

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºäº¤äº’å¼è¯„ä¼°æ¡†æ¶ä»¥æå‡å¤§è¯­è¨€æ¨¡å‹åœ¨è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ä¸­çš„è¡¨ç°**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `è½¯ä»¶å·¥ç¨‹` `äº¤äº’å¼è¯„ä¼°` `åŠ¨æ€åé¦ˆ` `ç¼–ç¨‹ä»»åŠ¡` `éœ€æ±‚ä¾èµ–å›¾` `åä½œä»£ç ç”Ÿæˆ` `DevAIåŸºå‡†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å•è½®é™æ€åŸºå‡†æµ‹è¯•æ— æ³•å…¨é¢è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹åœ¨å¤æ‚è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ä¸­çš„èƒ½åŠ›ï¼Œå¯¼è‡´è¯„ä¼°ç»“æœä¸å¤Ÿå‡†ç¡®ã€‚
2. æå‡ºäº†ä¸€ç§äº¤äº’å¼è¯„ä¼°æ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€å¯¹è¯å’Œåé¦ˆæœºåˆ¶ï¼Œåˆ©ç”¨éœ€æ±‚ä¾èµ–å›¾æ¥è¯„ä¼°å¤šéœ€æ±‚ç¼–ç¨‹ä»»åŠ¡ä¸­çš„LLMsè¡¨ç°ã€‚
3. é€šè¿‡åœ¨DevAIåŸºå‡†ä¸Šå¢åŠ çœŸå®è§£å†³æ–¹æ¡ˆå¹¶è¿›è¡Œä¸“å®¶æ³¨é‡Šï¼ŒéªŒè¯äº†é¢è¯•å®˜æç¤ºçš„æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºäº†åŠ¨æ€è¯„ä¼°çš„ä¼˜åŠ¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¼ ç»Ÿçš„å•è½®é™æ€åŸºå‡†æµ‹è¯•æ— æ³•æœ‰æ•ˆè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ä¸­çš„ç»†å¾®èƒ½åŠ›ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°é¢–çš„äº¤äº’å¼è¯„ä¼°æ¡†æ¶ï¼Œé€šè¿‡ç»“æ„åŒ–çš„åé¦ˆé©±åŠ¨å¯¹è¯ï¼Œè¯„ä¼°LLMsåœ¨å¤šéœ€æ±‚ç¼–ç¨‹ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚æ¯ä¸ªä»»åŠ¡è¢«å»ºæ¨¡ä¸ºéœ€æ±‚ä¾èµ–å›¾ï¼Œå…·å¤‡çœŸå®è§£å†³æ–¹æ¡ˆçš„â€œé¢è¯•å®˜â€LLMå‘â€œé¢è¯•è€…â€æ¨¡å‹æä¾›æœ€å°åŒ–çš„ã€é’ˆå¯¹æ€§çš„æç¤ºï¼Œä»¥å¸®åŠ©çº æ­£é”™è¯¯å¹¶æ»¡è¶³ç›®æ ‡çº¦æŸã€‚è¿™ç§åŠ¨æ€åè®®èƒ½å¤Ÿæ·±å…¥è¯Šæ–­æ¨¡å‹è¡Œä¸ºï¼Œæ­ç¤ºé™æ€åŸºå‡†æ— æ³•æµ‹é‡çš„ä¼˜åŠ¿å’Œç³»ç»Ÿæ€§å¼±ç‚¹ã€‚æˆ‘ä»¬åœ¨DevAIåŸºå‡†ä¸Šè¿›è¡Œäº†æ‰©å±•ï¼Œå¢åŠ äº†çœŸå®è§£å†³æ–¹æ¡ˆï¼Œå¹¶é€šè¿‡ä¸“å®¶æ³¨é‡Šè¯„ä¼°äº†é¢è¯•å®˜æç¤ºçš„ç›¸å…³æ€§å’Œå®ç”¨æ€§ã€‚ç ”ç©¶ç»“æœå¼ºè°ƒäº†åŠ¨æ€è¯„ä¼°åœ¨æ¨åŠ¨åä½œä»£ç ç”Ÿæˆä»£ç†å‘å±•ä¸­çš„é‡è¦æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³ä¼ ç»Ÿé™æ€åŸºå‡†æµ‹è¯•æ— æ³•å‡†ç¡®è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹åœ¨å¤æ‚è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ä¸­çš„èƒ½åŠ›çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹å¯¹æ¨¡å‹ç»†å¾®è¡Œä¸ºçš„æ·±å…¥åˆ†æï¼Œå¯¼è‡´è¯„ä¼°ç»“æœçš„å±€é™æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„äº¤äº’å¼è¯„ä¼°æ¡†æ¶é€šè¿‡ç»“æ„åŒ–çš„åé¦ˆé©±åŠ¨å¯¹è¯ï¼Œåˆ©ç”¨éœ€æ±‚ä¾èµ–å›¾æ¥åŠ¨æ€è¯„ä¼°LLMsåœ¨å¤šéœ€æ±‚ç¼–ç¨‹ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚è¿™ç§è®¾è®¡å…è®¸æ¨¡å‹åœ¨äº¤äº’ä¸­è·å¾—å®æ—¶åé¦ˆï¼Œä»è€Œæ›´å¥½åœ°çº æ­£é”™è¯¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¢è¯•å®˜LLMå’Œé¢è¯•è€…æ¨¡å‹ã€‚é¢è¯•å®˜LLMè´Ÿè´£æä¾›é’ˆå¯¹æ€§çš„æç¤ºï¼Œè€Œé¢è¯•è€…æ¨¡å‹åˆ™æ ¹æ®æç¤ºè¿›è¡Œè°ƒæ•´å’Œæ”¹è¿›ã€‚è¯„ä¼°è¿‡ç¨‹é€šè¿‡å¤šè½®å¯¹è¯è¿›è¡Œï¼Œå½¢æˆä¸€ä¸ªåŠ¨æ€çš„åé¦ˆå¾ªç¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥äº†åŠ¨æ€äº¤äº’è¯„ä¼°æœºåˆ¶ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨å®æ—¶åé¦ˆä¸­ä¸æ–­ä¼˜åŒ–è¡¨ç°ã€‚è¿™ä¸ä¼ ç»Ÿé™æ€è¯„ä¼°æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºï¼Œåè€…æ— æ³•æä¾›å®æ—¶çš„é”™è¯¯çº æ­£å’Œèƒ½åŠ›æå‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé¢è¯•å®˜LLMçš„æç¤ºæ˜¯æ ¹æ®çœŸå®è§£å†³æ–¹æ¡ˆç”Ÿæˆçš„ï¼Œç¡®ä¿äº†æç¤ºçš„ç›¸å…³æ€§å’Œæœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œä¸“å®¶æ³¨é‡Šç”¨äºè¯„ä¼°æç¤ºçš„å®ç”¨æ€§ï¼Œç¡®ä¿äº†è¯„ä¼°ç»“æœçš„å¯é æ€§ã€‚æ•´ä½“æµç¨‹å¼ºè°ƒäº†åé¦ˆçš„åŠæ—¶æ€§å’Œé’ˆå¯¹æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œäº¤äº’å¼è¯„ä¼°æ¡†æ¶æ˜¾è‘—æå‡äº†å¤§è¯­è¨€æ¨¡å‹åœ¨å¤šéœ€æ±‚ç¼–ç¨‹ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ä¸ä¼ ç»ŸåŸºå‡†ç›¸æ¯”ï¼Œæ¨¡å‹åœ¨é”™è¯¯çº æ­£å’Œä»»åŠ¡å®Œæˆåº¦ä¸Šæœ‰æ˜æ˜¾æ”¹å–„ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼ŒéªŒè¯äº†åŠ¨æ€è¯„ä¼°çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è½¯ä»¶å¼€å‘ã€è‡ªåŠ¨åŒ–ä»£ç ç”Ÿæˆå’Œæ™ºèƒ½ç¼–ç¨‹åŠ©æ‰‹ç­‰ã€‚é€šè¿‡æå‡å¤§è¯­è¨€æ¨¡å‹åœ¨å¤æ‚ç¼–ç¨‹ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæé«˜è½¯ä»¶å·¥ç¨‹å¸ˆçš„å·¥ä½œæ•ˆç‡ï¼Œå‡å°‘é”™è¯¯ç‡ï¼Œæ¨åŠ¨åä½œä»£ç ç”Ÿæˆä»£ç†çš„å‘å±•ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Standard single-turn, static benchmarks fall short in evaluating the nuanced capabilities of Large Language Models (LLMs) on complex tasks such as software engineering. In this work, we propose a novel interactive evaluation framework that assesses LLMs on multi-requirement programming tasks through structured, feedback-driven dialogue. Each task is modeled as a requirement dependency graph, and an ``interviewer'' LLM, aware of the ground-truth solution, provides minimal, targeted hints to an ``interviewee'' model to help correct errors and fulfill target constraints. This dynamic protocol enables fine-grained diagnostic insights into model behavior, uncovering strengths and systematic weaknesses that static benchmarks fail to measure. We build on DevAI, a benchmark of 55 curated programming tasks, by adding ground-truth solutions and evaluating the relevance and utility of interviewer hints through expert annotation. Our results highlight the importance of dynamic evaluation in advancing the development of collaborative code-generating agents.

