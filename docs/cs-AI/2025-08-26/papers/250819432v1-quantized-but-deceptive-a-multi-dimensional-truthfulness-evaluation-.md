---
layout: default
title: Quantized but Deceptive? A Multi-Dimensional Truthfulness Evaluation of Quantized LLMs
---

# Quantized but Deceptive? A Multi-Dimensional Truthfulness Evaluation of Quantized LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.19432" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.19432v1</a>
  <a href="https://arxiv.org/pdf/2508.19432.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.19432v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.19432v1', 'Quantized but Deceptive? A Multi-Dimensional Truthfulness Evaluation of Quantized LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yao Fu, Xianxuan Long, Runchao Li, Haotian Yu, Mu Sheng, Xiaotian Han, Yu Yin, Pan Li

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-26

**å¤‡æ³¨**: Accepted to EMNLP2025 main conference (poster)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTruthfulnessEvalæ¡†æ¶ä»¥è¯„ä¼°é‡åŒ–LLMçš„çœŸå®æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é‡åŒ–æ¨¡å‹` `çœŸå®æ€§è¯„ä¼°` `é€»è¾‘æ¨ç†` `å¸¸è¯†æ¨ç†` `è™šå‡ä¿¡æ¯æ£€æµ‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰é‡åŒ–LLMåœ¨ç”ŸæˆçœŸå®å“åº”æ–¹é¢çš„è¡¨ç°å°šæœªå¾—åˆ°å……åˆ†è¯„ä¼°ï¼Œç‰¹åˆ«æ˜¯åœ¨é¢å¯¹è¯¯å¯¼æ€§æç¤ºæ—¶çš„è„†å¼±æ€§ã€‚
2. æœ¬æ–‡æå‡ºTruthfulnessEvalæ¡†æ¶ï¼Œç»¼åˆè¯„ä¼°é‡åŒ–LLMåœ¨é€»è¾‘æ¨ç†ã€å¸¸è¯†å’Œæ¨¡ä»¿è™šå‡æ–¹é¢çš„çœŸå®æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡åŒ–æ¨¡å‹åœ¨é¢å¯¹â€œæ¬ºéª—æ€§â€æç¤ºæ—¶æ›´æ˜“äº§ç”Ÿé”™è¯¯è¾“å‡ºï¼Œæ­ç¤ºäº†å…¶å†…éƒ¨çœŸå®è¡¨å¾ä¸è¾“å‡ºä¹‹é—´çš„çŸ›ç›¾ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é‡åŒ–æŠ€æœ¯ä½¿å¾—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨èµ„æºå—é™ç¯å¢ƒä¸­é«˜æ•ˆéƒ¨ç½²ï¼Œé€šè¿‡æ˜¾è‘—é™ä½å†…å­˜å’Œè®¡ç®—æˆæœ¬ã€‚ç„¶è€Œï¼Œé‡åŒ–LLMåœ¨ç”ŸæˆçœŸå®æˆ–æ¬ºéª—æ€§å“åº”æ–¹é¢çš„å½±å“å°šæœªå¾—åˆ°å……åˆ†æ¢è®¨ã€‚æœ¬æ–‡æå‡ºäº†TruthfulnessEvalï¼Œä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºä»é€»è¾‘æ¨ç†ã€å¸¸è¯†å’Œæ¨¡ä»¿è™šå‡ä¸‰ç»´åº¦è¯„ä¼°é‡åŒ–LLMçš„çœŸå®æ€§ã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡é‡åŒ–æ¨¡å‹å†…éƒ¨ä¿æŒçœŸå®çš„è¡¨å¾ï¼Œä½†åœ¨è¯¯å¯¼æ€§æç¤ºä¸‹æ›´æ˜“äº§ç”Ÿé”™è¯¯è¾“å‡ºã€‚é€šè¿‡å¯¹15ç§ä¸åŒæç¤ºçš„æµ‹è¯•ï¼Œå‘ç°â€œæ¬ºéª—æ€§â€æç¤ºèƒ½å¤Ÿè¦†ç›–çœŸå®ä¸€è‡´çš„è¡Œä¸ºï¼Œè€Œâ€œè¯šå®â€å’Œâ€œä¸­ç«‹â€æç¤ºåˆ™ä¿æŒç¨³å®šè¾“å‡ºã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºæœªæ¥é‡åŒ–æ„ŸçŸ¥çš„å¯¹é½å’ŒçœŸå®æ€§å¹²é¢„è®¾è®¡æä¾›äº†é‡è¦è§è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³é‡åŒ–LLMåœ¨ç”ŸæˆçœŸå®å“åº”æ—¶çš„è„†å¼±æ€§ï¼Œå°¤å…¶æ˜¯åœ¨è¯¯å¯¼æ€§æç¤ºä¸‹çš„è¡¨ç°ä¸è¶³ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†æ¢è®¨é‡åŒ–å¯¹æ¨¡å‹çœŸå®æ€§çš„å½±å“ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºTruthfulnessEvalæ¡†æ¶ï¼Œé€šè¿‡ä¸‰ä¸ªç»´åº¦è¯„ä¼°é‡åŒ–LLMçš„çœŸå®æ€§ï¼Œæ—¨åœ¨æ­ç¤ºé‡åŒ–æ¨¡å‹åœ¨ä¸åŒæç¤ºä¸‹çš„è¾“å‡ºç‰¹æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ¡†æ¶åŒ…æ‹¬é€»è¾‘æ¨ç†ã€å¸¸è¯†å’Œæ¨¡ä»¿è™šå‡ä¸‰ä¸ªè¯„ä¼°æ¨¡å—ï¼Œé‡‡ç”¨å¤šç§é‡åŒ–æŠ€æœ¯ï¼ˆ4-bitåˆ°2-bitï¼‰å¯¹å¤šä¸ªå¼€æºLLMè¿›è¡Œæµ‹è¯•ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºè¯†åˆ«é‡åŒ–æ¨¡å‹åœ¨é¢å¯¹ä¸åŒç±»å‹æç¤ºæ—¶çš„è¡Œä¸ºå·®å¼‚ï¼Œå°¤å…¶æ˜¯â€œæ¬ºéª—æ€§â€æç¤ºå¯¹è¾“å‡ºçš„å½±å“ï¼Œè¿™åœ¨ç°æœ‰æ–‡çŒ®ä¸­å°šæœªè¢«æ·±å…¥æ¢è®¨ã€‚

**å…³é”®è®¾è®¡**ï¼šé€šè¿‡å±‚çº§æ¢æµ‹å’Œä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰å¯è§†åŒ–ï¼Œæ­ç¤ºé‡åŒ–æ¨¡å‹å†…éƒ¨çš„çœŸå®è¡¨å¾ä¸è¾“å‡ºä¹‹é—´çš„å…³ç³»ï¼Œè®¾è®¡äº†å¤šç§æç¤ºä»¥æµ‹è¯•æ¨¡å‹çš„å“åº”ç¨³å®šæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé‡åŒ–æ¨¡å‹åœ¨é¢å¯¹â€œæ¬ºéª—æ€§â€æç¤ºæ—¶ï¼Œå…¶è¾“å‡ºçš„é”™è¯¯ç‡æ˜¾è‘—é«˜äºâ€œè¯šå®â€å’Œâ€œä¸­ç«‹â€æç¤ºï¼Œæ­ç¤ºäº†æ¨¡å‹åœ¨çœŸå®è¡¨å¾ä¸è¾“å‡ºä¹‹é—´çš„çŸ›ç›¾ã€‚è¿™ä¸€å‘ç°ä¸ºé‡åŒ–æ¨¡å‹çš„çœŸå®æ€§è¯„ä¼°æä¾›äº†æ–°çš„è§†è§’ï¼Œå¼ºè°ƒäº†æç¤ºè®¾è®¡çš„é‡è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½å¯¹è¯ç³»ç»Ÿå’Œè™šå‡ä¿¡æ¯æ£€æµ‹ç­‰ã€‚é€šè¿‡æ”¹è¿›é‡åŒ–LLMçš„çœŸå®æ€§è¯„ä¼°ï¼Œå¯ä»¥æå‡æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„å¯é æ€§ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦ç”ŸæˆçœŸå®ä¿¡æ¯çš„åœºæ™¯ä¸­ã€‚æœªæ¥ï¼Œè¯¥æ¡†æ¶å¯ä¸ºé‡åŒ–æ¨¡å‹çš„è®¾è®¡å’Œä¼˜åŒ–æä¾›æŒ‡å¯¼ï¼Œæ¨åŠ¨æ›´é«˜æ•ˆçš„AIç³»ç»Ÿå‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Quantization enables efficient deployment of large language models (LLMs) in resource-constrained environments by significantly reducing memory and computation costs. While quantized LLMs often maintain performance on perplexity and zero-shot tasks, their impact on truthfulness-whether generating truthful or deceptive responses-remains largely unexplored. In this work, we introduce TruthfulnessEval, a comprehensive evaluation framework for assessing the truthfulness of quantized LLMs across three dimensions: (1) Truthfulness on Logical Reasoning; (2) Truthfulness on Common Sense; and (3) Truthfulness on Imitative Falsehoods. Using this framework, we examine mainstream quantization techniques (ranging from 4-bit to extreme 2-bit) across several open-source LLMs. Surprisingly, we find that while quantized models retain internally truthful representations, they are more susceptible to producing false outputs under misleading prompts. To probe this vulnerability, we test 15 rephrased variants of "honest", "neutral" and "deceptive" prompts and observe that "deceptive" prompts can override truth-consistent behavior, whereas "honest" and "neutral" prompts maintain stable outputs. Further, we reveal that quantized models "know" the truth internally yet still produce false outputs when guided by "deceptive" prompts via layer-wise probing and PCA visualizations. Our findings provide insights into future designs of quantization-aware alignment and truthfulness interventions.

