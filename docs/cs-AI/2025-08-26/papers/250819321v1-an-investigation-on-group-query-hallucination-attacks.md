---
layout: default
title: An Investigation on Group Query Hallucination Attacks
---

# An Investigation on Group Query Hallucination Attacks

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.19321" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.19321v1</a>
  <a href="https://arxiv.org/pdf/2508.19321.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.19321v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.19321v1', 'An Investigation on Group Query Hallucination Attacks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kehao Miao, Xiaolong Jin

**åˆ†ç±»**: cs.CR, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç¾¤æŸ¥è¯¢æ”»å‡»ä»¥æ­ç¤ºå¤§è¯­è¨€æ¨¡å‹çš„æ½œåœ¨ç¼ºé™·**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `ç¾¤æŸ¥è¯¢æ”»å‡»` `æ¨¡å‹å®‰å…¨æ€§` `æ¨ç†ä»»åŠ¡` `æ€§èƒ½è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†è€ƒè™‘ç”¨æˆ·åœ¨ä¸å¤§è¯­è¨€æ¨¡å‹äº¤äº’æ—¶åŒæ—¶æå‡ºå¤šä¸ªé—®é¢˜çš„æƒ…å†µï¼Œå¯¼è‡´æ¨¡å‹è¾“å‡ºçš„ä¸ç¨³å®šæ€§ã€‚
2. è®ºæ–‡æå‡ºçš„ç¾¤æŸ¥è¯¢æ”»å‡»é€šè¿‡åŒæ—¶è¾“å…¥å¤šä¸ªæŸ¥è¯¢ï¼Œæ¨¡æ‹Ÿç”¨æˆ·çš„çœŸå®äº¤äº’åœºæ™¯ï¼Œç ”ç©¶å…¶å¯¹æ¨¡å‹è¾“å‡ºçš„å½±å“ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œç¾¤æŸ¥è¯¢æ”»å‡»æ˜¾è‘—é™ä½äº†æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œå¹¶åœ¨æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºæœ‰æ•ˆæ€§ï¼Œæ­ç¤ºäº†æ¨¡å‹æ½œåœ¨çš„å®‰å…¨é£é™©ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¹¿æ³›åº”ç”¨ï¼Œç†è§£å…¶åœ¨ç”¨æˆ·äº¤äº’ä¸­çš„æ½œåœ¨å¤±è´¥æ¨¡å¼å˜å¾—è‡³å…³é‡è¦ã€‚ç”¨æˆ·åœ¨ä¸LLMsçš„å¯¹è¯ä¸­ï¼Œå¸¸å¸¸ä¼šåŒæ—¶æå‡ºå¤šä¸ªé—®é¢˜ã€‚æœ¬ç ”ç©¶æå‡ºäº†ç¾¤æŸ¥è¯¢æ”»å‡»æŠ€æœ¯ï¼Œé€šè¿‡åŒæ—¶å‘LLMså‘ˆç°å¤šä¸ªæŸ¥è¯¢ï¼Œæ¢è®¨è¿ç»­æç¤ºçš„ç´¯ç§¯ä¸Šä¸‹æ–‡å¦‚ä½•å½±å“æ¨¡å‹è¾“å‡ºã€‚ç ”ç©¶è¡¨æ˜ï¼Œç¾¤æŸ¥è¯¢æ”»å‡»æ˜¾è‘—é™ä½äº†åœ¨ç‰¹å®šä»»åŠ¡ä¸Šå¾®è°ƒæ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶å¯èƒ½è§¦å‘LLMsçš„æ½œåœ¨åé—¨ã€‚æ­¤å¤–ï¼Œè¯¥æ”»å‡»åœ¨æ¶‰åŠæ¨ç†çš„ä»»åŠ¡ä¸­ï¼Œå¦‚æ•°å­¦æ¨ç†å’Œä»£ç ç”Ÿæˆç­‰ï¼Œäº¦è¡¨ç°å‡ºæœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨ç”¨æˆ·åŒæ—¶æå‡ºå¤šä¸ªæŸ¥è¯¢æ—¶çš„è¾“å‡ºä¸ç¨³å®šæ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆåº”å¯¹è¿™ç§äº¤äº’æ¨¡å¼ï¼Œå¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸‹é™å’Œæ½œåœ¨çš„å®‰å…¨éšæ‚£ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ç¾¤æŸ¥è¯¢æ”»å‡»æŠ€æœ¯ï¼Œæ¨¡æ‹Ÿç”¨æˆ·çš„çœŸå®äº¤äº’åœºæ™¯ï¼Œç ”ç©¶è¿ç»­æç¤ºçš„ä¸Šä¸‹æ–‡å¦‚ä½•å½±å“æ¨¡å‹è¾“å‡ºã€‚è¿™ç§è®¾è®¡æ—¨åœ¨æ­ç¤ºæ¨¡å‹åœ¨å¤„ç†å¤æ‚æŸ¥è¯¢æ—¶çš„è„†å¼±æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬è¾“å…¥å¤šä¸ªæŸ¥è¯¢çš„æ¨¡å—ã€ä¸Šä¸‹æ–‡ç´¯ç§¯åˆ†ææ¨¡å—å’Œè¾“å‡ºç»“æœè¯„ä¼°æ¨¡å—ã€‚é€šè¿‡å¯¹æ¯”æ¨¡å‹åœ¨å•ä¸€æŸ¥è¯¢å’Œç¾¤æŸ¥è¯¢ä¸‹çš„è¡¨ç°ï¼Œè¯„ä¼°å…¶æ€§èƒ½å˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†ç¾¤æŸ¥è¯¢æ”»å‡»è¿™ä¸€æ–°æ¦‚å¿µï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ­ç¤ºå¤§è¯­è¨€æ¨¡å‹åœ¨å¤šæŸ¥è¯¢åœºæ™¯ä¸‹çš„æ€§èƒ½ä¸‹é™å’Œæ½œåœ¨åé—¨é£é™©ã€‚è¿™ä¸ç°æœ‰å•ä¸€æŸ¥è¯¢çš„ç ”ç©¶æ–¹æ³•æœ¬è´¨ä¸Šä¸åŒã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œè®¾ç½®äº†ä¸åŒçš„æŸ¥è¯¢ç»„åˆå’Œä¸Šä¸‹æ–‡é•¿åº¦ï¼Œä»¥è¯„ä¼°å…¶å¯¹æ¨¡å‹è¾“å‡ºçš„å½±å“ã€‚æŸå¤±å‡½æ•°å’Œè¯„ä¼°æŒ‡æ ‡åˆ™é€‰å–äº†ä¸ç‰¹å®šä»»åŠ¡ç›¸å…³çš„æ ‡å‡†ï¼Œä»¥ç¡®ä¿ç»“æœçš„æœ‰æ•ˆæ€§å’Œå¯æ¯”æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œç¾¤æŸ¥è¯¢æ”»å‡»æ˜¾è‘—é™ä½äº†åœ¨ç‰¹å®šä»»åŠ¡ä¸Šå¾®è°ƒæ¨¡å‹çš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨æ¨ç†ä»»åŠ¡ä¸­ï¼Œæ¨¡å‹è¾“å‡ºçš„å‡†ç¡®ç‡ä¸‹é™å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šã€‚è¿™ä¸€å‘ç°æ­ç¤ºäº†å¤§è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤æ‚ç”¨æˆ·æŸ¥è¯¢æ—¶çš„æ½œåœ¨é£é™©ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤§è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§è¯„ä¼°ã€ç”¨æˆ·äº¤äº’ä¼˜åŒ–å’Œæ¨¡å‹é²æ£’æ€§æå‡ã€‚é€šè¿‡è¯†åˆ«å’Œä¿®å¤æ¨¡å‹åœ¨å¤šæŸ¥è¯¢åœºæ™¯ä¸‹çš„è„†å¼±æ€§ï¼Œå¯ä»¥æé«˜å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å¯é æ€§å’Œå®‰å…¨æ€§ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> With the widespread use of large language models (LLMs), understanding their potential failure modes during user interactions is essential. In practice, users often pose multiple questions in a single conversation with LLMs. Therefore, in this study, we propose Group Query Attack, a technique that simulates this scenario by presenting groups of queries to LLMs simultaneously. We investigate how the accumulated context from consecutive prompts influences the outputs of LLMs. Specifically, we observe that Group Query Attack significantly degrades the performance of models fine-tuned on specific tasks. Moreover, we demonstrate that Group Query Attack induces a risk of triggering potential backdoors of LLMs. Besides, Group Query Attack is also effective in tasks involving reasoning, such as mathematical reasoning and code generation for pre-trained and aligned models.

