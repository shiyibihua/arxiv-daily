---
layout: default
title: MedVQA-TREE: A Multimodal Reasoning and Retrieval Framework for Sarcopenia Prediction
---

# MedVQA-TREE: A Multimodal Reasoning and Retrieval Framework for Sarcopenia Prediction

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.19319" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.19319v1</a>
  <a href="https://arxiv.org/pdf/2508.19319.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.19319v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.19319v1', 'MedVQA-TREE: A Multimodal Reasoning and Retrieval Framework for Sarcopenia Prediction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Pardis Moradbeiki, Nasser Ghadiri, Sayed Jalal Zahabi, Uffe Kock Wiil, Kristoffer Kittelmann Brockhattingen, Ali Ebrahimi

**åˆ†ç±»**: eess.IV, cs.AI, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMedVQA-TREEæ¡†æ¶ä»¥è§£å†³è‚Œè‚‰å‡å°‘ç—‡é¢„æµ‹é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è‚Œè‚‰å‡å°‘ç—‡` `è¶…å£°è¯Šæ–­` `å¤šæ¨¡æ€èåˆ` `çŸ¥è¯†æ£€ç´¢` `æ·±åº¦å­¦ä¹ ` `åŒ»ç–—å½±åƒ` `AIè¾…åŠ©è¯Šæ–­`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨è‚Œè‚‰å‡å°‘ç—‡çš„è¶…å£°è¯Šæ–­ä¸­é¢ä¸´å½±åƒçº¿ç´¢å¾®å¦™ã€æ ‡æ³¨æ•°æ®ä¸è¶³å’Œç¼ºä¹ä¸´åºŠèƒŒæ™¯ç­‰æŒ‘æˆ˜ã€‚
2. è®ºæ–‡æå‡ºçš„MedVQA-TREEæ¡†æ¶é€šè¿‡åˆ†å±‚å›¾åƒè§£é‡Šã€é—¨æ§ç‰¹å¾èåˆå’Œå¤šè·³æ£€ç´¢ç­–ç•¥æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¾¾åˆ°äº†99%çš„è¯Šæ–­å‡†ç¡®ç‡ï¼Œè¶…è¶Šäº†ç°æœ‰æ–¹æ³•è¶…è¿‡10%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å‡†ç¡®çš„è‚Œè‚‰å‡å°‘ç—‡è¯Šæ–­é€šè¿‡è¶…å£°æˆåƒä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºå½±åƒçº¿ç´¢å¾®å¦™ã€æ ‡æ³¨æ•°æ®æœ‰é™ä»¥åŠå¤§å¤šæ•°æ¨¡å‹ç¼ºä¹ä¸´åºŠèƒŒæ™¯ã€‚æˆ‘ä»¬æå‡ºäº†MedVQA-TREEï¼Œä¸€ä¸ªå¤šæ¨¡æ€æ¡†æ¶ï¼Œé›†æˆäº†åˆ†å±‚å›¾åƒè§£é‡Šæ¨¡å—ã€é—¨æ§ç‰¹å¾çº§èåˆæœºåˆ¶å’Œæ–°é¢–çš„å¤šè·³å¤šæŸ¥è¯¢æ£€ç´¢ç­–ç•¥ã€‚è§†è§‰æ¨¡å—åŒ…æ‹¬è§£å‰–åˆ†ç±»ã€åŒºåŸŸåˆ†å‰²å’ŒåŸºäºå›¾çš„ç©ºé—´æ¨ç†ï¼Œä»¥æ•æ‰ç²—ã€ä¸­ã€ç»†ç²’åº¦ç»“æ„ã€‚é—¨æ§èåˆæœºåˆ¶é€‰æ‹©æ€§åœ°å°†è§†è§‰ç‰¹å¾ä¸æ–‡æœ¬æŸ¥è¯¢é›†æˆï¼ŒåŒæ—¶é€šè¿‡UMLSå¼•å¯¼çš„ç®¡é“è®¿é—®PubMedå’Œç‰¹å®šäºè‚Œè‚‰å‡å°‘ç—‡çš„å¤–éƒ¨çŸ¥è¯†åº“æ¥æ£€ç´¢ä¸´åºŠçŸ¥è¯†ã€‚MedVQA-TREEåœ¨ä¸¤ä¸ªå…¬å…±MedVQAæ•°æ®é›†ï¼ˆVQA-RADå’ŒPathVQAï¼‰åŠä¸€ä¸ªè‡ªå®šä¹‰è‚Œè‚‰å‡å°‘ç—‡è¶…å£°æ•°æ®é›†ä¸Šè¿›è¡Œäº†è®­ç»ƒå’Œè¯„ä¼°ï¼Œæ¨¡å‹è¾¾åˆ°äº†99%çš„è¯Šæ–­å‡†ç¡®ç‡ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„æœ€å…ˆè¿›æ–¹æ³•è¶…è¿‡10%ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†å°†ç»“æ„åŒ–è§†è§‰ç†è§£ä¸å¼•å¯¼çŸ¥è¯†æ£€ç´¢ç›¸ç»“åˆåœ¨è‚Œè‚‰å‡å°‘ç—‡æœ‰æ•ˆAIè¾…åŠ©è¯Šæ–­ä¸­çš„ç›Šå¤„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³è‚Œè‚‰å‡å°‘ç—‡çš„è¶…å£°è¯Šæ–­é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å½±åƒçº¿ç´¢å¾®å¦™ã€æ ‡æ³¨æ•°æ®æœ‰é™ä»¥åŠç¼ºä¹ä¸´åºŠèƒŒæ™¯ç­‰æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMedVQA-TREEæ¡†æ¶é€šè¿‡ç»“åˆåˆ†å±‚å›¾åƒè§£é‡Šå’Œå¤šæ¨¡æ€ä¿¡æ¯æ£€ç´¢ï¼Œæ—¨åœ¨æé«˜è‚Œè‚‰å‡å°‘ç—‡çš„è¯Šæ–­å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚è®¾è®¡ä¸Šå¼ºè°ƒäº†è§†è§‰ç‰¹å¾ä¸æ–‡æœ¬æŸ¥è¯¢çš„æœ‰æ•ˆèåˆã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä¸‰ä¸ªæ¨¡å—ï¼šåˆ†å±‚å›¾åƒè§£é‡Šæ¨¡å—è´Ÿè´£è§£å‰–åˆ†ç±»å’ŒåŒºåŸŸåˆ†å‰²ï¼Œé—¨æ§ç‰¹å¾èåˆæœºåˆ¶ç”¨äºæ•´åˆè§†è§‰ä¸æ–‡æœ¬ä¿¡æ¯ï¼Œå¤šè·³å¤šæŸ¥è¯¢æ£€ç´¢ç­–ç•¥åˆ™ç”¨äºä¸´åºŠçŸ¥è¯†çš„è·å–ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†é—¨æ§ç‰¹å¾èåˆæœºåˆ¶å’Œå¤šè·³å¤šæŸ¥è¯¢æ£€ç´¢ç­–ç•¥ï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•çš„å•ä¸€ç‰¹å¾æå–å’Œæ£€ç´¢æ–¹å¼æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¨¡å‹çš„å…³é”®è®¾è®¡åŒ…æ‹¬ä½¿ç”¨UMLSå¼•å¯¼çš„çŸ¥è¯†æ£€ç´¢ç®¡é“ï¼Œç»“åˆPubMedå’Œç‰¹å®šçŸ¥è¯†åº“ï¼Œç¡®ä¿ä¸´åºŠçŸ¥è¯†çš„å‡†ç¡®è·å–ï¼ŒåŒæ—¶åœ¨ç½‘ç»œç»“æ„ä¸­å®ç°äº†å¤šå±‚æ¬¡çš„ç‰¹å¾æå–ä¸èåˆã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMedVQA-TREEæ¨¡å‹åœ¨VQA-RADå’ŒPathVQAç­‰å¤šä¸ªæ•°æ®é›†ä¸Šè¾¾åˆ°äº†99%çš„è¯Šæ–­å‡†ç¡®ç‡ï¼Œè¾ƒä¹‹å‰çš„æœ€å…ˆè¿›æ–¹æ³•æå‡è¶…è¿‡10%ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨è‚Œè‚‰å‡å°‘ç—‡è¯Šæ–­ä¸­çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒ»ç–—å½±åƒåˆ†æã€ä¸´åºŠè¾…åŠ©è¯Šæ–­å’Œæ™ºèƒ½å¥åº·ç®¡ç†ç­‰ã€‚é€šè¿‡æé«˜è‚Œè‚‰å‡å°‘ç—‡çš„è¯Šæ–­å‡†ç¡®æ€§ï¼ŒMedVQA-TREEæ¡†æ¶èƒ½å¤Ÿä¸ºä¸´åºŠåŒ»ç”Ÿæä¾›æ›´å¯é çš„å†³ç­–æ”¯æŒï¼Œæ¨åŠ¨ä¸ªæ€§åŒ–åŒ»ç–—çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Accurate sarcopenia diagnosis via ultrasound remains challenging due to subtle imaging cues, limited labeled data, and the absence of clinical context in most models. We propose MedVQA-TREE, a multimodal framework that integrates a hierarchical image interpretation module, a gated feature-level fusion mechanism, and a novel multi-hop, multi-query retrieval strategy. The vision module includes anatomical classification, region segmentation, and graph-based spatial reasoning to capture coarse, mid-level, and fine-grained structures. A gated fusion mechanism selectively integrates visual features with textual queries, while clinical knowledge is retrieved through a UMLS-guided pipeline accessing PubMed and a sarcopenia-specific external knowledge base. MedVQA-TREE was trained and evaluated on two public MedVQA datasets (VQA-RAD and PathVQA) and a custom sarcopenia ultrasound dataset. The model achieved up to 99% diagnostic accuracy and outperformed previous state-of-the-art methods by over 10%. These results underscore the benefit of combining structured visual understanding with guided knowledge retrieval for effective AI-assisted diagnosis in sarcopenia.

