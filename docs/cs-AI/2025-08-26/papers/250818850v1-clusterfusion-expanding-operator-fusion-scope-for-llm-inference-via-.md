---
layout: default
title: ClusterFusion: Expanding Operator Fusion Scope for LLM Inference via Cluster-Level Collective Primitive
---

# ClusterFusion: Expanding Operator Fusion Scope for LLM Inference via Cluster-Level Collective Primitive

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.18850" class="toolbar-btn" target="_blank">üìÑ arXiv: 2508.18850v1</a>
  <a href="https://arxiv.org/pdf/2508.18850.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.18850v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.18850v1', 'ClusterFusion: Expanding Operator Fusion Scope for LLM Inference via Cluster-Level Collective Primitive')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Xinhao Luo, Zihan Liu, Yangjie Zhou, Shihan Fang, Ziyu Huang, Yu Feng, Chen Zhang, Shixuan Sun, Zhenzhe Zheng, Jingwen Leng, Minyi Guo

**ÂàÜÁ±ª**: cs.DC, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-26

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/xinhao-luo/ClusterFusion)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫ClusterFusion‰ª•Ëß£ÂÜ≥LLMÊé®ÁêÜ‰∏≠ÁöÑÂª∂ËøüÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `Êé®ÁêÜ‰ºòÂåñ` `ÈõÜÁæ§ÈÄö‰ø°` `Êìç‰ΩúÁ¨¶ËûçÂêà` `Ê∑±Â∫¶Â≠¶‰π†Ê°ÜÊû∂` `È´òÊïàËÆ°ÁÆó` `ÂÜÖÂ≠òÁÆ°ÁêÜ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑLLMÊé®ÁêÜÊñπÊ≥ïÁî±‰∫éÊìç‰ΩúÁ¨¶ÊâßË°åÁ¢éÁâáÂåñÂíåÂØπÂ§ñÈÉ®ÂÜÖÂ≠òÁöÑ‰æùËµñÔºåÂØºËá¥È´òÂª∂ËøüÂíåÊÄßËÉΩÁì∂È¢à„ÄÇ
2. Êú¨ÊñáÊèêÂá∫ClusterReduceÂíåClusterGather‰∏§ÁßçÈõÜÁæ§Á∫ßÈÄö‰ø°ÂéüËØ≠Ôºå‰ºòÂåñÈõÜÁæ§ÂÜÖÁöÑÊï∞ÊçÆ‰∫§Êç¢ÂíåÂΩíÁ∫¶ÔºåËøõËÄåËÆæËÆ°ClusterFusionÊ°ÜÊû∂ÂÆûÁé∞Êìç‰ΩúÁ¨¶ËûçÂêà„ÄÇ
3. Âú®H100 GPU‰∏äÁöÑÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåClusterFusionÂú®‰∏çÂêåÊ®°ÂûãÂíåÈÖçÁΩÆ‰∏ãÁöÑÁ´ØÂà∞Á´ØÂª∂ËøüÂπ≥ÂùáÊèêÂçá‰∫Ü1.61ÂÄçÔºåÊòæËëó‰ºò‰∫éÁé∞ÊúâÊé®ÁêÜÊ°ÜÊû∂„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâËß£Á†ÅÁî±‰∫éÊìç‰ΩúÁ¨¶‰πãÈó¥ÁöÑÁ¢éÁâáÂåñÊâßË°åÂíåÂØπÂ§ñÈÉ®ÂÜÖÂ≠òÁöÑÈáçÂ∫¶‰æùËµñÔºåÂØºËá¥È´òÂª∂Ëøü„ÄÇËøôÁßçÊâßË°åÊ®°ÂûãÈôêÂà∂‰∫ÜËûçÂêàÁöÑÊú∫‰ºöÔºåÂπ∂ÈÄ†ÊàêÊòæËëóÁöÑÂÜÖÂ≠òÊµÅÈáèÂíåÂÜÖÊ†∏ÂêØÂä®ÂºÄÈîÄ„ÄÇÂ∞ΩÁÆ°Áé∞‰ª£Êû∂ÊûÑÂ¶ÇNVIDIA HopperÊèê‰æõ‰∫ÜÂàÜÂ∏ÉÂºèÂÖ±‰∫´ÂÜÖÂ≠òÂíå‰ΩéÂª∂ËøüÁöÑÈõÜÁæ§ÂÜÖÈÉ®‰∫íËøûÔºå‰ΩÜÂÆÉ‰ª¨‰ªÖÊö¥Èú≤‰ΩéÁ∫ßÊï∞ÊçÆÁßªÂä®Êåá‰ª§ÔºåÁº∫‰πèÁªìÊûÑÂåñÁöÑÈõÜÊàêÈÄö‰ø°ÊäΩË±°„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄËΩØ‰ª∂‰∏éÁ°¨‰ª∂‰πãÈó¥ÁöÑÂ∑ÆË∑ùÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏§ÁßçÈõÜÁæ§Á∫ßÈÄö‰ø°ÂéüËØ≠ClusterReduceÂíåClusterGatherÔºåËÉΩÂ§üÊäΩË±°Â∏∏ËßÅÁöÑÈÄö‰ø°Ê®°ÂºèÔºåÂÆûÁé∞ÈõÜÁæ§ÂÜÖÁ∫øÁ®ãÂùó‰πãÈó¥ÁöÑÈ´òÊïàÊï∞ÊçÆ‰∫§Êç¢ÂíåÂΩíÁ∫¶„ÄÇÂü∫‰∫éËøô‰∫õÊäΩË±°ÔºåËÆæËÆ°‰∫ÜClusterFusionÊâßË°åÊ°ÜÊû∂ÔºåÈÄöËøáÂ∞ÜËß£Á†ÅÈò∂ÊÆµÂ¶ÇQKVÊäïÂΩ±„ÄÅÊ≥®ÊÑèÂäõÊú∫Âà∂ÂíåËæìÂá∫ÊäïÂΩ±ÁªÑÂêàÊàêÂçï‰∏™ËûçÂêàÂÜÖÊ†∏ÔºåÊâ©Â±ï‰∫ÜÊìç‰ΩúÁ¨¶ËûçÂêàÁöÑËåÉÂõ¥„ÄÇH100 GPU‰∏äÁöÑËØÑ‰º∞ÊòæÁ§∫ÔºåClusterFusionÂú®‰∏çÂêåÊ®°ÂûãÂíåÈÖçÁΩÆ‰∏ãÁöÑÁ´ØÂà∞Á´ØÂª∂ËøüÂπ≥ÂùáÊèêÂçá‰∫Ü1.61ÂÄç„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜ‰∏≠ÁöÑÈ´òÂª∂ËøüÈóÆÈ¢òÔºåÁé∞ÊúâÊñπÊ≥ïÂõ†Êìç‰ΩúÁ¨¶ÊâßË°åÁöÑÁ¢éÁâáÂåñÂíåÂØπÂ§ñÈÉ®ÂÜÖÂ≠òÁöÑÈáçÂ∫¶‰æùËµñÔºåÂØºËá¥ÊÄßËÉΩ‰∏ãÈôçÂíåÂÜÖÂ≠òÂºÄÈîÄÂ¢ûÂä†„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÈÄöËøáÂºïÂÖ•ClusterReduceÂíåClusterGather‰∏§ÁßçÈõÜÁæ§Á∫ßÈÄö‰ø°ÂéüËØ≠ÔºåÊú¨ÊñáÂÆûÁé∞‰∫ÜÈõÜÁæ§ÂÜÖÁ∫øÁ®ãÂùó‰πãÈó¥ÁöÑÈ´òÊïàÊï∞ÊçÆ‰∫§Êç¢ÂíåÂΩíÁ∫¶Ôºå‰ªéËÄåÂáèÂ∞ëÂØπÂ§ñÈÉ®ÂÜÖÂ≠òÁöÑ‰æùËµñÔºåÊèêÂçá‰∫ÜÊï∞ÊçÆÂ§ÑÁêÜÊïàÁéá„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöClusterFusionÊ°ÜÊû∂Â∞ÜÈÄö‰ø°‰∏éËÆ°ÁÆóË∞ÉÂ∫¶ÁªìÂêàÔºåÂÖÅËÆ∏Â∞ÜÂ§ö‰∏™Ëß£Á†ÅÈò∂ÊÆµÔºàÂ¶ÇQKVÊäïÂΩ±„ÄÅÊ≥®ÊÑèÂäõÊú∫Âà∂ÂíåËæìÂá∫ÊäïÂΩ±ÔºâÂêàÂπ∂‰∏∫Âçï‰∏™ËûçÂêàÂÜÖÊ†∏Ôºå‰ºòÂåñ‰∫ÜÊâßË°åÊµÅÁ®ã„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöClusterReduceÂíåClusterGatherÁöÑÊèêÂá∫ÊòØÊú¨ÊñáÁöÑÊ†∏ÂøÉÂàõÊñ∞ÔºåÂÆÉ‰ª¨Êèê‰æõ‰∫ÜÈ´òÊïàÁöÑÈõÜÁæ§ÂÜÖÈÄö‰ø°ÊäΩË±°ÔºåÊòæËëóÊèêÂçá‰∫ÜÊï∞ÊçÆÂ§ÑÁêÜÈÄüÂ∫¶ÔºåÂπ∂Êâ©Â±ï‰∫ÜÊìç‰ΩúÁ¨¶ËûçÂêàÁöÑËåÉÂõ¥„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆæËÆ°‰∏≠ÔºåClusterFusionÊ°ÜÊû∂ÈÄöËøá‰ºòÂåñÂÜÖÊ†∏Ë∞ÉÂ∫¶ÂíåÊï∞ÊçÆÊµÅÂä®ÔºåÂáèÂ∞ë‰∫ÜÂÜÖÂ≠òÊµÅÈáèÂíåÂÜÖÊ†∏ÂêØÂä®ÂºÄÈîÄÔºåÁ°Æ‰øù‰∫Ü‰∏≠Èó¥ÁªìÊûúËÉΩÂ§üÂú®ËäØÁâáÂÜÖÂ§ÑÁêÜÔºåÈÅøÂÖç‰∫ÜÂ§ñÈÉ®ÂÜÖÂ≠òÁöÑÂπ≤Êâ∞„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Âú®H100 GPU‰∏äÁöÑÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåClusterFusionÂú®‰∏çÂêåÊ®°ÂûãÂíåÈÖçÁΩÆ‰∏ãÁöÑÁ´ØÂà∞Á´ØÂª∂ËøüÂπ≥ÂùáÊèêÂçá‰∫Ü1.61ÂÄçÔºåÊòæËëó‰ºò‰∫éÁé∞ÊúâÁöÑÊé®ÁêÜÊ°ÜÊû∂ÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÂº∫Â§ßÊÄßËÉΩÂíå‰ºòÂäø„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ClusterFusionÁöÑÁ†îÁ©∂ÊàêÊûúÂú®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜ„ÄÅÊ∑±Â∫¶Â≠¶‰π†ËÆ≠ÁªÉÂíåÂÆûÊó∂Êï∞ÊçÆÂ§ÑÁêÜÁ≠âÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõ„ÄÇÈÄöËøáÊèêÂçáÊé®ÁêÜÊïàÁéáÔºåËØ•ÊäÄÊúØÂèØ‰ª•Âä†ÈÄüËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ„ÄÅÊú∫Âô®ÁøªËØëÂíåÂØπËØùÁ≥ªÁªüÁ≠âÂ∫îÁî®ÁöÑÂìçÂ∫îÈÄüÂ∫¶ÔºåËøõËÄåÊèêÂçáÁî®Êà∑‰ΩìÈ™åÂíåÁ≥ªÁªüÊÄßËÉΩ„ÄÇÊú™Êù•ÔºåËØ•Ê°ÜÊû∂ÂèØËÉΩ‰ºöÊé®Âä®Êõ¥Â§öÈ´òÊïàÁöÑÊ∑±Â∫¶Â≠¶‰π†Ê®°ÂûãÁöÑÂºÄÂèë‰∏éÂ∫îÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Large language model (LLM) decoding suffers from high latency due to fragmented execution across operators and heavy reliance on off-chip memory for data exchange and reduction. This execution model limits opportunities for fusion and incurs significant memory traffic and kernel launch overhead. While modern architectures such as NVIDIA Hopper provide distributed shared memory and low-latency intra-cluster interconnects, they expose only low-level data movement instructions, lacking structured abstractions for collective on-chip communication. To bridge this software-hardware gap, we introduce two cluster-level communication primitives, ClusterReduce and ClusterGather, which abstract common communication patterns and enable structured, high-speed data exchange and reduction between thread blocks within a cluster, allowing intermediate results to be on-chip without involving off-chip memory. Building on these abstractions, we design ClusterFusion, an execution framework that schedules communication and computation jointly to expand operator fusion scope by composing decoding stages such as QKV Projection, Attention, and Output Projection into a single fused kernels. Evaluations on H100 GPUs show that ClusterFusion outperforms state-of-the-art inference frameworks by 1.61x on average in end-to-end latency across different models and configurations. The source code is available at https://github.com/xinhao-luo/ClusterFusion.

