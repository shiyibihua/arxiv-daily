---
layout: default
title: MUA-RL: Multi-turn User-interacting Agent Reinforcement Learning for agentic tool use
---

# MUA-RL: Multi-turn User-interacting Agent Reinforcement Learning for agentic tool use

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.18669" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.18669v1</a>
  <a href="https://arxiv.org/pdf/2508.18669.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.18669v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.18669v1', 'MUA-RL: Multi-turn User-interacting Agent Reinforcement Learning for agentic tool use')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Weikang Zhao, Xili Wang, Chengdi Ma, Lingbin Kong, Zhaohua Yang, Mingxiang Tuo, Xiaowei Shi, Yitao Zhai, Xunliang Cai

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMUA-RLä»¥è§£å†³å¤šè½®ç”¨æˆ·äº¤äº’ä¸­çš„å·¥å…·ä½¿ç”¨é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `ä»£ç†æ™ºèƒ½` `å¼ºåŒ–å­¦ä¹ ` `å¤šè½®äº¤äº’` `å·¥å…·ä½¿ç”¨` `ç”¨æˆ·æ¨¡æ‹Ÿ` `åŠ¨æ€ç¯å¢ƒ` `æ¨¡å‹ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å·¥å…·ä½¿ç”¨ä¸­æœªèƒ½æœ‰æ•ˆæ•´åˆåŠ¨æ€ç”¨æˆ·ï¼Œå¯¼è‡´ä»£ç†åœ¨å¤šè½®äº¤äº’ä¸­éš¾ä»¥æ»¡è¶³ç”¨æˆ·éœ€æ±‚ã€‚
2. MUA-RLæ¡†æ¶é€šè¿‡å°†LLMæ¨¡æ‹Ÿç”¨æˆ·çº³å…¥å¼ºåŒ–å­¦ä¹ å¾ªç¯ï¼Œæ—¨åœ¨å®ç°æ¨¡å‹çš„è‡ªä¸»å­¦ä¹ ä¸é«˜æ•ˆæ²Ÿé€šã€‚
3. MUA-RL-32Båœ¨å¤šä¸ªå¤šè½®å·¥å…·ä½¿ç”¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šæˆ–åŒ¹é…äº†å¦‚DeepSeek-V3-0324ç­‰æ›´å¤§æ¨¡å‹çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€ä»£ç†æ™ºèƒ½çš„å¿«é€Ÿå‘å±•ï¼ŒLLMä¸­çš„ä»£ç†å·¥å…·ä½¿ç”¨å˜å¾—æ„ˆåŠ é‡è¦ã€‚åœ¨å¤šè½®äº¤äº’ä¸­ï¼Œç”¨æˆ·éœ€æ±‚çš„åŠ¨æ€æ€§å’Œä¸ç¡®å®šæ€§å¯¹ä»£ç†çš„å·¥å…·è°ƒç”¨èƒ½åŠ›æå‡ºäº†é‡å¤§æŒ‘æˆ˜ã€‚ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•æœªèƒ½åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æœ‰æ•ˆæ•´åˆåŠ¨æ€ç”¨æˆ·ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†MUA-RLæ¡†æ¶ï¼Œé¦–æ¬¡å°†LLMæ¨¡æ‹Ÿç”¨æˆ·çº³å…¥å¼ºåŒ–å­¦ä¹ å¾ªç¯ï¼Œæ—¨åœ¨ä½¿æ¨¡å‹èƒ½å¤Ÿé«˜æ•ˆåœ°ä¸ç”¨æˆ·æ²Ÿé€šå¹¶ä½¿ç”¨å„ç§å·¥å…·è§£å†³å®é™…é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMUA-RL-32Båœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šæˆ–åŒ¹é…äº†æ›´å¤§å¼€æºæ¨¡å‹çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨å¤šè½®ç”¨æˆ·äº¤äº’ä¸­ï¼Œä»£ç†å·¥å…·ä½¿ç”¨çš„åŠ¨æ€æ€§å’Œä¸ç¡®å®šæ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆæ•´åˆåŠ¨æ€ç”¨æˆ·ï¼Œå¯¼è‡´ä»£ç†æ— æ³•å‡†ç¡®ç†è§£å’Œæ»¡è¶³ç”¨æˆ·éœ€æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMUA-RLé€šè¿‡å°†LLMæ¨¡æ‹Ÿç”¨æˆ·å¼•å…¥å¼ºåŒ–å­¦ä¹ å¾ªç¯ï¼Œä½¿ä»£ç†èƒ½å¤Ÿåœ¨ä¸ç”¨æˆ·çš„äº¤äº’ä¸­ä¸æ–­è¿­ä»£å’Œä¼˜åŒ–ç†è§£ï¼Œä»è€Œæé«˜å·¥å…·è°ƒç”¨çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMUA-RLçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ç”¨æˆ·æ¨¡æ‹Ÿæ¨¡å—ã€å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¨¡å—å’Œå·¥å…·è°ƒç”¨æ¨¡å—ã€‚ç”¨æˆ·æ¨¡æ‹Ÿæ¨¡å—ç”ŸæˆåŠ¨æ€ç”¨æˆ·éœ€æ±‚ï¼Œå¼ºåŒ–å­¦ä¹ æ¨¡å—é€šè¿‡ä¸ç”¨æˆ·çš„äº¤äº’ä¸æ–­ä¼˜åŒ–ç­–ç•¥ï¼Œå·¥å…·è°ƒç”¨æ¨¡å—åˆ™æ‰§è¡Œå…·ä½“çš„å·¥å…·æ“ä½œã€‚

**å…³é”®åˆ›æ–°**ï¼šMUA-RLçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºé¦–æ¬¡å°†LLMæ¨¡æ‹Ÿç”¨æˆ·é›†æˆåˆ°å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ä¸­ï¼Œä½¿å¾—ä»£ç†èƒ½å¤Ÿåœ¨åŠ¨æ€ç¯å¢ƒä¸­è¿›è¡Œæœ‰æ•ˆå­¦ä¹ ã€‚è¿™ä¸€è®¾è®¡ä¸ä¼ ç»Ÿæ–¹æ³•çš„é™æ€ç”¨æˆ·æ¨¡å‹å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼ŒMUA-RLé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–ç”¨æˆ·äº¤äº’çš„æ•ˆæœï¼Œå¹¶è®¾è®¡äº†é€‚åº”åŠ¨æ€ç”¨æˆ·éœ€æ±‚çš„ç½‘ç»œç»“æ„ï¼Œä»¥æé«˜æ¨¡å‹çš„çµæ´»æ€§å’Œå“åº”é€Ÿåº¦ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„è®¾è®¡åœ¨å®éªŒéƒ¨åˆ†è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

MUA-RL-32Båœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå…·ä½“ç»“æœä¸ºï¼šåœ¨TAU2 Retailä¸Šå¾—åˆ†67.3ï¼ŒTAU2 Airlineä¸Šå¾—åˆ†45.4ï¼ŒTAU2 Telecomä¸Šå¾—åˆ†28.3ï¼ŒBFCL-V3 Multi Turnä¸Šå¾—åˆ†28.4ï¼Œä»¥åŠACEBench Agentä¸Šå¾—åˆ†82.5ã€‚è¿™äº›ç»“æœè¶…è¶Šæˆ–åŒ¹é…äº†å¦‚DeepSeek-V3-0324å’ŒQwen3-235B-A22Bç­‰æ›´å¤§æ¨¡å‹çš„æ€§èƒ½ï¼Œæ˜¾ç¤ºäº†MUA-RLçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MUA-RLçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼ŒåŒ…æ‹¬æ™ºèƒ½å®¢æœã€åœ¨çº¿æ•™è‚²å’Œäººæœºäº¤äº’ç­‰åœºæ™¯ã€‚é€šè¿‡æé«˜ä»£ç†åœ¨å¤šè½®äº¤äº’ä¸­çš„å·¥å…·ä½¿ç”¨èƒ½åŠ›ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæ˜¾è‘—æå‡ç”¨æˆ·ä½“éªŒå’Œé—®é¢˜è§£å†³æ•ˆç‡ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨æ›´æ™ºèƒ½çš„äº¤äº’ç³»ç»Ÿçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> With the recent rapid advancement of Agentic Intelligence, agentic tool use in LLMs has become increasingly important. During multi-turn interactions between agents and users, the dynamic, uncertain, and stochastic nature of user demands poses significant challenges to the agent's tool invocation capabilities. Agents are no longer expected to simply call tools to deliver a result; rather, they must iteratively refine their understanding of user needs through communication while simultaneously invoking tools to resolve user queries. Existing reinforcement learning (RL) approaches for tool use lack the integration of genuinely dynamic users during the RL training process. To bridge this gap, we introduce MUA-RL (Multi-turn User-interacting Agent Reinforcement Learning for agentic tool use), a novel reinforcement learning framework that, for the first time in the field of agentic tool use, integrates LLM-simulated users into the reinforcement learning loop. MUA-RL aims to enable autonomous learning of models to communicate with users efficiently and use various tools to solve practical problems in dynamic multi-turn interactions. Evaluations are done on several multi-turn tool-using benchmarks (see Figure 1). Specifically, MUA-RL-32B achieves 67.3 on TAU2 Retail, 45.4 on TAU2 Airline, 28.3 on TAU2 Telecom, 28.4 on BFCL-V3 Multi Turn, and 82.5 on ACEBench Agent -- outperforming or matching the performance of larger open-source models such as DeepSeek-V3-0324 and Qwen3-235B-A22B in non-thinking settings.

