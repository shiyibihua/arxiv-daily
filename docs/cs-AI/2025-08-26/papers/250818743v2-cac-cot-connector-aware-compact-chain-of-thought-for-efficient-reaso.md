---
layout: default
title: CAC-CoT: Connector-Aware Compact Chain-of-Thought for Efficient Reasoning Data Synthesis Across Dual-System Cognitive Tasks
---

# CAC-CoT: Connector-Aware Compact Chain-of-Thought for Efficient Reasoning Data Synthesis Across Dual-System Cognitive Tasks

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.18743" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.18743v2</a>
  <a href="https://arxiv.org/pdf/2508.18743.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.18743v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.18743v2', 'CAC-CoT: Connector-Aware Compact Chain-of-Thought for Efficient Reasoning Data Synthesis Across Dual-System Cognitive Tasks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sunguk Choi, Yonghoon Kwon, Heondeuk Lee

**åˆ†ç±»**: cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-26 (æ›´æ–°: 2025-09-15)

**å¤‡æ³¨**: Accepted at EMNLP 2025 findings

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCAC-CoTä»¥æå‡åŒç³»ç»Ÿè®¤çŸ¥ä»»åŠ¡ä¸­çš„æ¨ç†æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿é“¾æ¨ç†` `è¿æ¥å™¨æ„ŸçŸ¥` `æ¨ç†æ•ˆç‡` `å¤§å‹è¯­è¨€æ¨¡å‹` `ç³»ç»Ÿ-1ä»»åŠ¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é•¿é“¾æ¨ç†æ–¹æ³•åœ¨å¤„ç†å¿«é€Ÿç›´è§‚çš„ä»»åŠ¡æ—¶ï¼Œå¾€å¾€å¯¼è‡´æ€§èƒ½ä¸‹é™å’Œæ•ˆç‡ä½ä¸‹ã€‚
2. CAC-CoTæ–¹æ³•é€šè¿‡é™åˆ¶æ¨ç†è¿‡ç¨‹ä¸­çš„è¿æ¥çŸ­è¯­æ•°é‡ï¼Œæ—¨åœ¨å¼•å¯¼æ¨¡å‹ç”Ÿæˆæ›´ç®€æ´çš„æ¨ç†é“¾ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCAC-CoTåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨ç³»ç»Ÿ-1ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é•¿é“¾æ¨ç†ï¼ˆCoTï¼‰æç¤ºæœ‰åŠ©äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è§£å†³å¤æ‚é—®é¢˜ï¼Œä½†è¿‡é•¿çš„æ¨ç†é“¾ä¼šåœ¨å¿«é€Ÿç›´è§‚çš„â€œç³»ç»Ÿ-1â€ä»»åŠ¡ä¸­é™ä½æ€§èƒ½ã€‚æœ¬æ–‡æå‡ºäº†è¿æ¥å™¨æ„ŸçŸ¥ç´§å‡‘é“¾æ¨ç†ï¼ˆCAC-CoTï¼‰æ–¹æ³•ï¼Œé™åˆ¶æ¨ç†ä½¿ç”¨å›ºå®šçš„è¿æ¥çŸ­è¯­ï¼Œä»è€Œå¼•å¯¼æ¨¡å‹ç”Ÿæˆç®€æ´ä¸”ç»“æ„è‰¯å¥½çš„è§£é‡Šã€‚å°½ç®¡æ–¹æ³•ç®€å•ï¼Œä½†åœ¨é€šç”¨LLMsä¸Šå®ç°äº†é«˜è´¨é‡çš„è®­ç»ƒæ•ˆæœã€‚CAC-CoTåœ¨GSM8Kä¸Šè¾¾åˆ°äº†çº¦85%çš„å‡†ç¡®ç‡ï¼Œåœ¨GPQAï¼ˆç³»ç»Ÿ-2ï¼‰ä¸Šçº¦ä¸º40%ï¼ŒåŒæ—¶åœ¨S1-Benchï¼ˆç³»ç»Ÿ-1ï¼‰ä¸Šä¹Ÿè¾¾åˆ°äº†çº¦85%ï¼Œè¶…å‡ºåŸºçº¿20%ä»¥ä¸Šã€‚å…¶æ¨ç†é“¾å¹³å‡çº¦300ä¸ªæ ‡è®°ï¼Œçº¦ä¸ºåŸºçº¿é•¿åº¦çš„ä¸‰åˆ†ä¹‹ä¸€ï¼Œåœ¨ä¸æŸå¤±å‡†ç¡®æ€§çš„æƒ…å†µä¸‹æé«˜äº†æ•ˆç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³é•¿é“¾æ¨ç†åœ¨å¿«é€Ÿç›´è§‚ä»»åŠ¡ä¸­æ€§èƒ½ä¸‹é™çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†æ­¤ç±»ä»»åŠ¡æ—¶ï¼Œæ¨ç†é“¾è¿‡é•¿å¯¼è‡´æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCAC-CoTæ–¹æ³•é€šè¿‡é™åˆ¶æ¨ç†ä¸­ä½¿ç”¨çš„è¿æ¥çŸ­è¯­æ•°é‡ï¼Œä¿ƒä½¿æ¨¡å‹ç”Ÿæˆç®€æ´ä¸”ç»“æ„åŒ–çš„æ¨ç†é“¾ï¼Œä»è€Œæé«˜æ¨ç†æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCAC-CoTçš„æ•´ä½“æ¶æ„åŒ…æ‹¬è¾“å…¥å¤„ç†ã€è¿æ¥çŸ­è¯­é€‰æ‹©ã€æ¨ç†ç”Ÿæˆå’Œè¾“å‡ºä¼˜åŒ–å››ä¸ªä¸»è¦æ¨¡å—ã€‚æ¨¡å‹é¦–å…ˆæ¥æ”¶è¾“å…¥ï¼Œç„¶åé€‰æ‹©å›ºå®šçš„è¿æ¥çŸ­è¯­ï¼Œæœ€åç”Ÿæˆæ¨ç†ç»“æœå¹¶è¿›è¡Œä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šCAC-CoTçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶è¿æ¥å™¨æ„ŸçŸ¥çš„è®¾è®¡ï¼Œé€šè¿‡é™åˆ¶æ¨ç†é“¾çš„é•¿åº¦ï¼Œæ˜¾è‘—æé«˜äº†åœ¨ç³»ç»Ÿ-1ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæä¾›äº†æ›´é«˜çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼ŒCAC-CoTä½¿ç”¨äº†å›ºå®šçš„è¿æ¥çŸ­è¯­é›†ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸ºä¼˜åŒ–æ¨ç†é“¾çš„ç®€æ´æ€§å’Œå‡†ç¡®æ€§ï¼Œç½‘ç»œç»“æ„ä¸Šåˆ™é‡‡ç”¨äº†é€šç”¨çš„LLMæ¶æ„ï¼Œç¡®ä¿äº†æ–¹æ³•çš„å¹¿æ³›é€‚ç”¨æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

CAC-CoTåœ¨GSM8Kä¸Šè¾¾åˆ°äº†çº¦85%çš„å‡†ç¡®ç‡ï¼Œåœ¨GPQAä¸Šçº¦ä¸º40%ï¼Œåœ¨S1-Benchä¸Šä¹Ÿè¾¾åˆ°äº†çº¦85%ï¼Œè¶…å‡ºåŸºçº¿20%ä»¥ä¸Šã€‚å…¶æ¨ç†é“¾å¹³å‡é•¿åº¦çº¦ä¸º300ä¸ªæ ‡è®°ï¼Œæ˜¾è‘—æé«˜äº†æ¨ç†æ•ˆç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

CAC-CoTæ–¹æ³•åœ¨æ•™è‚²ã€æ™ºèƒ½é—®ç­”å’Œäººæœºäº¤äº’ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æé«˜æ¨ç†æ•ˆç‡ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå¸®åŠ©ç”¨æˆ·æ›´å¿«é€Ÿåœ°è·å–ä¿¡æ¯ï¼Œæå‡å†³ç­–è´¨é‡ï¼Œæœªæ¥å¯èƒ½åœ¨æ™ºèƒ½åŠ©æ‰‹å’Œè‡ªåŠ¨åŒ–ç³»ç»Ÿä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Long chain-of-thought (CoT) prompting helps Large Language Models (LLMs) solve difficult problems, but very long traces often slow or even degrade performance on fast, intuitive "System-1" tasks. We introduce Connector-Aware Compact CoT (CAC-CoT) -- a method that deliberately restricts reasoning to a small, fixed set of connector phrases, steering the model toward concise and well -- structured explanations. Despite its simplicity, our synthetic method with general-purpose LLMs yields a high-quality training quality. CAC-CoT achieves approximately 85% on GSM8K and approximately 40% on GPQA (System-2) while also achieving approximately 85% on S1-Bench (System-1), surpassing the baseline by over 20%. Its reasoning traces average approximately 300 tokens(ART), about one-third the length of baseline traces, delivering higher efficiency without loss of accuracy.

