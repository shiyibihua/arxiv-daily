---
layout: default
title: Federated Fine-Tuning of Sparsely-Activated Large Language Models on Resource-Constrained Devices
---

# Federated Fine-Tuning of Sparsely-Activated Large Language Models on Resource-Constrained Devices

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.19078" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.19078v2</a>
  <a href="https://arxiv.org/pdf/2508.19078.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.19078v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.19078v2', 'Federated Fine-Tuning of Sparsely-Activated Large Language Models on Resource-Constrained Devices')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Fahao Chen, Jie Wan, Peng Li, Zhou Su, Dongxiao Yu

**åˆ†ç±»**: cs.DC, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-26 (æ›´æ–°: 2025-10-10)

**å¤‡æ³¨**: Accepted by EuroSys 2026

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFLUXä»¥è§£å†³èµ„æºå—é™è®¾å¤‡ä¸ŠMoEæ¨¡å‹çš„è”é‚¦å¾®è°ƒé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è”é‚¦å­¦ä¹ ` `å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹` `Mixture-of-Experts` `æ¨¡å‹å¾®è°ƒ` `èµ„æºä¼˜åŒ–` `åŠ¨æ€åˆ†é…` `é‡åŒ–åˆ†æ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨è”é‚¦å¾®è°ƒMoEæ¨¡å‹æ—¶é¢ä¸´è®¡ç®—èµ„æºä¸è¶³çš„é—®é¢˜ï¼Œå¯¼è‡´æ€§èƒ½æ— æ³•æ»¡è¶³éœ€æ±‚ã€‚
2. FLUXé€šè¿‡é‡åŒ–æœ¬åœ°åˆ†æã€é€‚åº”æ€§ä¸“å®¶åˆå¹¶å’ŒåŠ¨æ€è§’è‰²åˆ†é…ç­‰åˆ›æ–°æ–¹æ³•ï¼Œä¼˜åŒ–äº†èµ„æºä½¿ç”¨æ•ˆç‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFLUXåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†æ—¶é—´åˆ°è¾¾å‡†ç¡®ç‡ï¼Œé€Ÿåº¦æå‡å¯è¾¾4.75å€ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è”é‚¦å¾®è°ƒåŸºäºMixture-of-Experts (MoE)çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é¢ä¸´å·¨å¤§çš„è®¡ç®—éœ€æ±‚å’Œå‚ä¸è€…çš„èµ„æºé™åˆ¶ã€‚ç°æœ‰æ–¹æ³•é€šè¿‡æ¨¡å‹é‡åŒ–ã€è®¡ç®—å¸è½½æˆ–ä¸“å®¶å‰ªææ¥å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œä½†ç”±äºä¸åˆ‡å®é™…çš„ç³»ç»Ÿå‡è®¾å’Œå¯¹MoEç‰¹æ€§ç¼ºä¹è€ƒè™‘ï¼Œæ— æ³•è¾¾åˆ°é¢„æœŸæ€§èƒ½ã€‚æœ¬æ–‡æå‡ºFLUXï¼Œä¸€ä¸ªæ—¨åœ¨å®ç°èµ„æºå—é™è®¾å¤‡ä¸ŠMoEæ¨¡å‹çš„è”é‚¦å¾®è°ƒçš„ç³»ç»Ÿï¼Œæ—¨åœ¨æœ€å°åŒ–æ—¶é—´åˆ°è¾¾å‡†ç¡®ç‡ã€‚FLUXå¼•å…¥äº†ä¸‰é¡¹å…³é”®åˆ›æ–°ï¼šé‡åŒ–åŸºç¡€çš„æœ¬åœ°åˆ†æã€é€‚åº”æ€§å±‚æ„ŸçŸ¥çš„ä¸“å®¶åˆå¹¶å’ŒåŠ¨æ€ä¸“å®¶è§’è‰²åˆ†é…ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒFLUXåœ¨æ—¶é—´åˆ°è¾¾å‡†ç¡®ç‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œé€Ÿåº¦æå‡å¯è¾¾4.75å€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨èµ„æºå—é™è®¾å¤‡ä¸Šè¿›è¡ŒMoEæ¨¡å‹çš„è”é‚¦å¾®è°ƒæ—¶çš„è®¡ç®—éœ€æ±‚è¿‡é«˜å’Œæ€§èƒ½ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†è€ƒè™‘MoEæ¨¡å‹çš„ç‰¹æ€§ï¼Œå¯¼è‡´æ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šFLUXçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡é‡åŒ–å’ŒåŠ¨æ€è°ƒæ•´ä¸“å®¶è§’è‰²æ¥ä¼˜åŒ–èµ„æºä½¿ç”¨ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„å‡†ç¡®æ€§ã€‚è¿™æ ·çš„è®¾è®¡æ—¨åœ¨åœ¨æœ‰é™çš„è®¡ç®—èµ„æºä¸‹å®ç°é«˜æ•ˆçš„æ¨¡å‹å¾®è°ƒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šFLUXçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé‡åŒ–åŸºç¡€çš„æœ¬åœ°åˆ†ææ¨¡å—ã€é€‚åº”æ€§å±‚æ„ŸçŸ¥çš„ä¸“å®¶åˆå¹¶æ¨¡å—å’ŒåŠ¨æ€ä¸“å®¶è§’è‰²åˆ†é…æ¨¡å—ã€‚è¿™äº›æ¨¡å—ååŒå·¥ä½œï¼Œä»¥å®ç°é«˜æ•ˆçš„è”é‚¦å¾®è°ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šFLUXçš„å…³é”®åˆ›æ–°åœ¨äºå…¶é‡åŒ–æœ¬åœ°åˆ†æå’ŒåŠ¨æ€è§’è‰²åˆ†é…ç­–ç•¥ï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•çš„é™æ€ä¸“å®¶é€‰æ‹©å’Œç®€å•é‡åŒ–æ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”MoEæ¨¡å‹çš„ç‰¹æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨FLUXä¸­ï¼Œé‡åŒ–åˆ†æç”¨äºä¼°è®¡ä¸“å®¶æ¿€æ´»ï¼Œå‡å°‘è®¡ç®—å¼€é”€ï¼›é€‚åº”æ€§ä¸“å®¶åˆå¹¶é€šè¿‡åˆ†æå±‚çº§ç‰¹å¾æ¥ä¼˜åŒ–èµ„æºæ¶ˆè€—ï¼›åŠ¨æ€è§’è‰²åˆ†é…åˆ™é‡‡ç”¨æ¢ç´¢-åˆ©ç”¨ç­–ç•¥ï¼Œä»¥å¹³è¡¡è°ƒä¼˜å’Œéè°ƒä¼˜ä¸“å®¶çš„è§’è‰²ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡åœ¨å®éªŒä¸­ç»è¿‡è°ƒä¼˜ï¼Œä»¥ç¡®ä¿æœ€ä½³æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒFLUXåœ¨LLaMA-MoEå’ŒDeepSeek-MoEæ¨¡å‹ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºç°æœ‰æ–¹æ³•ï¼Œæ—¶é—´åˆ°è¾¾å‡†ç¡®ç‡æå‡å¯è¾¾4.75å€ï¼Œæ˜¾è‘—æé«˜äº†è”é‚¦å¾®è°ƒçš„æ•ˆç‡å’Œæ•ˆæœã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

FLUXçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é«˜æ•ˆå¤„ç†å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„åœºæ™¯ä¸­ï¼Œå¦‚æ™ºèƒ½åŠ©æ‰‹ã€åœ¨çº¿æ•™è‚²å’Œä¸ªæ€§åŒ–æ¨èç³»ç»Ÿç­‰ã€‚é€šè¿‡åœ¨èµ„æºå—é™è®¾å¤‡ä¸Šå®ç°é«˜æ•ˆçš„æ¨¡å‹å¾®è°ƒï¼ŒFLUXèƒ½å¤Ÿæ¨åŠ¨è¿™äº›åº”ç”¨çš„æ™®åŠå’Œå‘å±•ï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Federated fine-tuning of Mixture-of-Experts (MoE)-based large language models (LLMs) is challenging due to their massive computational requirements and the resource constraints of participants. Existing working attempts to fill this gap through model quantization, computation offloading, or expert pruning. However, they cannot achieve desired performance due to impractical system assumptions and a lack of consideration for MoE-specific characteristics. In this paper, we propose FLUX, a system designed to enable federated fine-tuning of MoE-based LLMs across participants with constrained computing resources (e.g., consumer-grade GPUs), aiming to minimize time-to-accuracy. FLUX introduces three key innovations: (1) quantization-based local profiling to estimate expert activation with minimal overhead, (2) adaptive layer-aware expert merging to reduce resource consumption while preserving accuracy, and (3) dynamic expert role assignment using an exploration-exploitation strategy to balance tuning and non-tuning experts. Extensive experiments on LLaMA-MoE and DeepSeek-MoE with multiple benchmark datasets demonstrate that FLUX significantly outperforms existing methods, achieving up to 4.75X speedup in time-to-accuracy.

