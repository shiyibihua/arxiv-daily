---
layout: default
title: Self-Exploring Language Models for Explainable Link Forecasting on Temporal Graphs via Reinforcement Learning
---

# Self-Exploring Language Models for Explainable Link Forecasting on Temporal Graphs via Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.00975" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.00975v2</a>
  <a href="https://arxiv.org/pdf/2509.00975.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.00975v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.00975v2', 'Self-Exploring Language Models for Explainable Link Forecasting on Temporal Graphs via Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zifeng Ding, Shenyang Huang, Zeyu Cao, Emma Kondrup, Zachary Yang, Xingyue Huang, Yuan Sui, Zhangdie Yuan, Yuqicheng Zhu, Xianglong Hu, Yuan He, Farimah Poursafaei, Michael Bronstein, Andreas Vlachos

**åˆ†ç±»**: cs.AI, cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-31 (æ›´æ–°: 2025-10-13)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºReaL-TGæ¡†æ¶ä»¥å®ç°å¯è§£é‡Šçš„æ—¶é—´å›¾é“¾æ¥é¢„æµ‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ—¶é—´å›¾` `é“¾æ¥é¢„æµ‹` `å¯è§£é‡Šæ€§` `å¼ºåŒ–å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ—¶é—´å›¾é“¾æ¥é¢„æµ‹æ–¹æ³•ç¼ºä¹å¯è§£é‡Šæ€§ï¼Œä¸”åœ¨æœªè§å›¾ä¸Šæ— æ³•ç›´æ¥åº”ç”¨ï¼Œé™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºçš„ReaL-TGæ¡†æ¶é€šè¿‡å¼ºåŒ–å­¦ä¹ å¾®è°ƒLLMsï¼Œæ—¨åœ¨å®ç°å¯è§£é‡Šçš„é“¾æ¥é¢„æµ‹ï¼Œå¹¶é¼“åŠ±æ¨¡å‹è‡ªæˆ‘æ¢ç´¢æ¨ç†ç­–ç•¥ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒReaL-TG-4Båœ¨æ’åæŒ‡æ ‡ä¸Šè¶…è¶Šäº†æ›´å¤§çš„LLMsï¼Œå¹¶ç”Ÿæˆäº†é«˜è´¨é‡çš„è§£é‡Šï¼Œå¾—åˆ°äº†è¯„åˆ¤ç³»ç»Ÿå’Œäººç±»è¯„ä¼°çš„è®¤å¯ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é“¾æ¥é¢„æµ‹æ˜¯æ—¶é—´å›¾æ¨ç†ä¸­çš„æ ¸å¿ƒä»»åŠ¡ï¼Œéœ€è¦æ¨¡å‹åˆ©ç”¨å†å²äº¤äº’æ¥é¢„æµ‹å³å°†å‘ç”Ÿçš„é“¾æ¥ã€‚ä¼ ç»Ÿçš„ç¥ç»ç½‘ç»œæ–¹æ³•ï¼Œå¦‚æ—¶é—´å›¾ç¥ç»ç½‘ç»œï¼Œè™½ç„¶è¡¨ç°è‰¯å¥½ï¼Œä½†ç¼ºä¹å¯è§£é‡Šæ€§ï¼Œå¹¶ä¸”åœ¨æœªè§å›¾ä¸Šæ— æ³•åº”ç”¨è€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚è¿‘æœŸç ”ç©¶å¼€å§‹æ¢ç´¢ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œå›¾æ¨ç†ï¼Œä½†å¤§å¤šæ•°ç ”ç©¶å±€é™äºé™æ€å›¾æˆ–å°å‹åˆæˆæ—¶é—´å›¾ï¼Œä¸”ç¼ºä¹å¯¹LLMç”Ÿæˆçš„æ¨ç†è½¨è¿¹è´¨é‡çš„è¯„ä¼°ã€‚æœ¬æ–‡æå‡ºäº†åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ—¶é—´å›¾æ¨ç†å¢å¼ºå­¦ä¹ æ¡†æ¶ï¼ˆReaL-TGï¼‰ï¼Œè¯¥æ¡†æ¶å¾®è°ƒLLMsä»¥åœ¨çœŸå®ä¸–ç•Œçš„æ—¶é—´å›¾ä¸Šæ‰§è¡Œå¯è§£é‡Šçš„é“¾æ¥é¢„æµ‹ã€‚ReaL-TGä½¿ç”¨åŸºäºç»“æœçš„å¥–åŠ±ï¼Œé¼“åŠ±æ¨¡å‹è‡ªæˆ‘æ¢ç´¢å›¾ç»“æ„ä¸­çš„æ¨ç†ç­–ç•¥ï¼Œå¹¶ç”Ÿæˆç›´æ¥è¯æ˜å…¶é¢„æµ‹çš„è§£é‡Šã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°åè®®ï¼Œç»“åˆæ’åæŒ‡æ ‡ä¸LLMè¯„åˆ¤ç³»ç»Ÿï¼Œè¯„ä¼°æ¨ç†è´¨é‡åŠå¹»è§‰å½±å“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒReaL-TG-4Båœ¨æ’åæŒ‡æ ‡ä¸Šä¼˜äºæ›´å¤§çš„å‰æ²¿LLMsï¼Œå¹¶ç”Ÿæˆé«˜è´¨é‡çš„è§£é‡Šï¼Œå¾—åˆ°äº†LLMè¯„åˆ¤å’Œäººå·¥è¯„ä¼°çš„ç¡®è®¤ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡è§£å†³çš„å…·ä½“é—®é¢˜æ˜¯å¦‚ä½•åœ¨æ—¶é—´å›¾ä¸Šè¿›è¡Œå¯è§£é‡Šçš„é“¾æ¥é¢„æµ‹ã€‚ç°æœ‰æ–¹æ³•å¦‚æ—¶é—´å›¾ç¥ç»ç½‘ç»œè™½ç„¶è¡¨ç°è‰¯å¥½ï¼Œä½†ç¼ºä¹å¯è§£é‡Šæ€§ï¼Œä¸”åœ¨æœªè§å›¾ä¸Šæ— æ³•åº”ç”¨ï¼Œå¯¼è‡´å…¶åœ¨å®é™…åœºæ™¯ä¸­çš„å±€é™æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒè§£å†³æ€è·¯æ˜¯é€šè¿‡å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼ˆReaL-TGï¼‰å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä»¥å®ç°å¯è§£é‡Šçš„é“¾æ¥é¢„æµ‹ã€‚é€šè¿‡ä½¿ç”¨åŸºäºç»“æœçš„å¥–åŠ±æœºåˆ¶ï¼Œæ¨¡å‹èƒ½å¤Ÿè‡ªæˆ‘æ¢ç´¢æ¨ç†ç­–ç•¥ï¼Œå¹¶ç”Ÿæˆèƒ½å¤Ÿè§£é‡Šå…¶é¢„æµ‹çš„ç†ç”±ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šReaL-TGçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®è¾“å…¥æ¨¡å—ã€LLMå¾®è°ƒæ¨¡å—å’Œè¯„ä¼°æ¨¡å—ã€‚æ•°æ®è¾“å…¥æ¨¡å—è´Ÿè´£å¤„ç†æ—¶é—´å›¾æ•°æ®ï¼ŒLLMå¾®è°ƒæ¨¡å—åˆ™é€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œè¯„ä¼°æ¨¡å—åˆ™ç”¨äºè¯„ä¼°ç”Ÿæˆçš„æ¨ç†è½¨è¿¹çš„è´¨é‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºç»“åˆäº†å¼ºåŒ–å­¦ä¹ ä¸å¤§å‹è¯­è¨€æ¨¡å‹çš„å¾®è°ƒï¼Œèƒ½å¤Ÿåœ¨çœŸå®ä¸–ç•Œçš„æ—¶é—´å›¾ä¸Šè¿›è¡Œå¯è§£é‡Šçš„é“¾æ¥é¢„æµ‹ã€‚è¿™ä¸ä¼ ç»Ÿæ–¹æ³•çš„é™æ€æ€§å’Œç¼ºä¹è‡ªæˆ‘è§£é‡Šèƒ½åŠ›å½¢æˆäº†é²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å…³é”®è®¾è®¡æ–¹é¢ï¼Œä½¿ç”¨äº†åŸºäºç»“æœçš„å¥–åŠ±æœºåˆ¶æ¥å¼•å¯¼æ¨¡å‹æ¢ç´¢æ¨ç†ç­–ç•¥ï¼ŒåŒæ—¶è®¾è®¡äº†æ–°çš„è¯„ä¼°åè®®ï¼Œç»“åˆæ’åæŒ‡æ ‡ä¸LLMè¯„åˆ¤ç³»ç»Ÿï¼Œä»¥å…¨é¢è¯„ä¼°æ¨ç†è´¨é‡å’Œå¹»è§‰å½±å“ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒReaL-TG-4Båœ¨æ’åæŒ‡æ ‡ä¸Šè¶…è¶Šäº†æ›´å¤§çš„å‰æ²¿LLMsï¼Œå¦‚GPT-5 miniï¼Œä¸”åœ¨ç”Ÿæˆçš„è§£é‡Šè´¨é‡ä¸Šå¾—åˆ°äº†LLMè¯„åˆ¤å’Œäººç±»è¯„ä¼°çš„é«˜åº¦è®¤å¯ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç¤¾äº¤ç½‘ç»œåˆ†æã€äº¤é€šæµé‡é¢„æµ‹å’Œé‡‘èäº¤æ˜“ç½‘ç»œç­‰ã€‚é€šè¿‡å®ç°å¯è§£é‡Šçš„é“¾æ¥é¢„æµ‹ï¼ŒReaL-TGèƒ½å¤Ÿå¸®åŠ©å†³ç­–è€…ç†è§£æ¨¡å‹çš„é¢„æµ‹ä¾æ®ï¼Œä»è€Œåœ¨å®é™…åº”ç”¨ä¸­æé«˜å†³ç­–çš„é€æ˜åº¦å’Œå¯é æ€§ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½ä¼šæ¨åŠ¨æ›´å¤šé¢†åŸŸçš„å›¾æ¨ç†ç ”ç©¶ï¼Œä¿ƒè¿›æ™ºèƒ½ç³»ç»Ÿçš„å¯è§£é‡Šæ€§å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Forecasting future links is a central task in temporal graph (TG) reasoning, requiring models to leverage historical interactions to predict upcoming ones. Traditional neural approaches, such as temporal graph neural networks, achieve strong performance but lack explainability and cannot be applied to unseen graphs without retraining. Recent studies have begun to explore using large language models (LLMs) for graph reasoning, but most of them are constrained to static graphs or small synthetic TGs and lack the evaluation of the quality of reasoning traces generated by LLMs. In this work, we present Reasoning-Enhanced Learning for Temporal Graphs (ReaL-TG), a reinforcement learning framework that fine-tunes LLMs to perform explainable link forecasting on real-world TGs. ReaL-TG uses outcome-based reward to encourage models to self-explore reasoning strategies from graph structure and to produce explanations that directly justify their predictions. To enable evaluation on LLM-generated reasoning traces, we propose a new evaluation protocol combining ranking metrics with an LLM-as-a-Judge system that assesses both the quality of reasoning and the impact of hallucinations. Experiments with ReaL-TG-4B, obtained by fine-tuning Qwen3-4B under our framework, show that it outperforms much larger frontier LLMs, including GPT-5 mini, on ranking metrics, while producing high-quality explanations confirmed by both the LLM judge and human evaluation.

