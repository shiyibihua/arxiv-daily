---
layout: default
title: Causal MAS: A Survey of Large Language Model Architectures for Discovery and Effect Estimation
---

# Causal MAS: A Survey of Large Language Model Architectures for Discovery and Effect Estimation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.00987" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.00987v1</a>
  <a href="https://arxiv.org/pdf/2509.00987.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.00987v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.00987v1', 'Causal MAS: A Survey of Large Language Model Architectures for Discovery and Effect Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Adib Bazgir, Amir Habibdoust, Yuwen Zhang, Xing Song

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-31

**å¤‡æ³¨**: 24 pages. 2 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå› æœå¤šæ™ºèƒ½ä½“æ¨¡å‹ä»¥è§£å†³å¤æ‚å› æœæ¨ç†é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å› æœæ¨ç†` `å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ` `å¤§å‹è¯­è¨€æ¨¡å‹` `åäº‹å®åˆ†æ` `ç§‘å­¦å‘ç°` `åŒ»ç–—åº”ç”¨` `ä¸ªæ€§åŒ–ç³»ç»Ÿ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚å› æœæ¨ç†å’Œä¼°è®¡æ–¹é¢å­˜åœ¨å¹»è§‰å’Œè™šå‡ç›¸å…³æ€§ç­‰é—®é¢˜ï¼Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚
2. è®ºæ–‡æå‡ºå› æœå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œé€šè¿‡å¤šä¸ªLLMæ™ºèƒ½ä½“çš„åä½œæ¥è§£å†³å› æœæ¨ç†å’Œå‘ç°çš„æŒ‘æˆ˜ã€‚
3. ç ”ç©¶è¡¨æ˜ï¼Œå› æœå¤šæ™ºèƒ½ä½“æ¨¡å‹åœ¨ç§‘å­¦å‘ç°å’ŒåŒ»ç–—ç­‰é¢†åŸŸçš„åº”ç”¨æ•ˆæœæ˜¾è‘—ï¼Œæå‡äº†å› æœæ¨ç†çš„å‡†ç¡®æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å„ç§æ¨ç†å’Œç”Ÿæˆä»»åŠ¡ä¸­å±•ç°äº†å“è¶Šçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å¤æ‚å› æœæ¨ç†ã€å‘ç°å’Œä¼°è®¡æ–¹é¢çš„èƒ½åŠ›ä»åœ¨ç§¯æå‘å±•ä¸­ï¼Œå¸¸å¸¸å—åˆ°å¹»è§‰ã€ä¾èµ–è™šå‡ç›¸å…³æ€§ä»¥åŠå¤„ç†ç»†å¾®ã€é¢†åŸŸç‰¹å®šæˆ–ä¸ªæ€§åŒ–å› æœå…³ç³»çš„å›°éš¾ç­‰é—®é¢˜çš„åˆ¶çº¦ã€‚å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåˆ©ç”¨å¤šä¸ªåŸºäºLLMçš„æ™ºèƒ½ä½“çš„åä½œæˆ–ä¸“ä¸šèƒ½åŠ›ï¼Œæˆä¸ºè§£å†³è¿™äº›é™åˆ¶çš„å¼ºå¤§èŒƒå¼ã€‚æœ¬æ–‡ç»¼è¿°äº†å› æœå¤šæ™ºèƒ½ä½“LLMsçš„å¿«é€Ÿå‘å±•ï¼Œæ¢è®¨äº†è¿™äº›ç³»ç»Ÿå¦‚ä½•è®¾è®¡ä»¥åº”å¯¹å› æœæ¨ç†ã€åäº‹å®åˆ†æã€æ•°æ®ä¸­çš„å› æœå‘ç°å’Œå› æœæ•ˆåº”ä¼°è®¡ç­‰ä¸åŒæ–¹é¢ã€‚æˆ‘ä»¬è¿˜è®¨è®ºäº†å¤šæ ·çš„æ¶æ„æ¨¡å¼å’Œäº¤äº’åè®®ï¼Œä»¥åŠè¯„ä¼°æ–¹æ³•ã€åŸºå‡†å’Œå› æœå¤šæ™ºèƒ½ä½“LLMsçš„åº”ç”¨é¢†åŸŸï¼ŒåŒ…æ‹¬ç§‘å­¦å‘ç°ã€åŒ»ç–—ã€äº‹å®æ ¸æŸ¥å’Œä¸ªæ€§åŒ–ç³»ç»Ÿã€‚æœ€åï¼Œå¼ºè°ƒäº†æŒç»­çš„æŒ‘æˆ˜ã€å¼€æ”¾çš„ç ”ç©¶é—®é¢˜å’Œæœªæ¥çš„æœ‰å¸Œæœ›æ–¹å‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å› æœæ¨ç†å’Œä¼°è®¡ä¸­çš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯å¹»è§‰å’Œè™šå‡ç›¸å…³æ€§çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚å› æœå…³ç³»æ—¶å¾€å¾€è¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†å› æœå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œé€šè¿‡å¤šä¸ªLLMæ™ºèƒ½ä½“çš„åä½œå’Œä¸“ä¸šåŒ–æ¥å¢å¼ºå› æœæ¨ç†èƒ½åŠ›ï¼Œæ—¨åœ¨å…‹æœå•ä¸€æ¨¡å‹çš„å±€é™æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªLLMæ™ºèƒ½ä½“çš„åä½œæœºåˆ¶ï¼Œé‡‡ç”¨ç®¡é“å¤„ç†ã€è¾©è®ºæ¡†æ¶ã€æ¨¡æ‹Ÿç¯å¢ƒå’Œè¿­ä»£ä¼˜åŒ–ç­‰å¤šç§äº¤äº’åè®®ï¼Œå½¢æˆä¸€ä¸ªç»¼åˆçš„å› æœæ¨ç†ç³»ç»Ÿã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥å¤šæ™ºèƒ½ä½“åä½œæœºåˆ¶ï¼Œä½¿å¾—ä¸åŒæ™ºèƒ½ä½“å¯ä»¥ä¸“æ³¨äºä¸åŒçš„å› æœæ¨ç†ä»»åŠ¡ï¼Œä»è€Œæé«˜æ•´ä½“ç³»ç»Ÿçš„æ€§èƒ½å’Œå‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–å› æœæ¨ç†çš„å‡†ç¡®æ€§ï¼Œå¹¶é€šè¿‡å‚æ•°è°ƒä¼˜å’Œç½‘ç»œç»“æ„è®¾è®¡æ¥å¢å¼ºæ¨¡å‹çš„é€‚åº”æ€§å’Œçµæ´»æ€§ã€‚å…·ä½“ç»†èŠ‚åŒ…æ‹¬æ™ºèƒ½ä½“ä¹‹é—´çš„äº¤äº’é¢‘ç‡å’Œä¿¡æ¯å…±äº«æœºåˆ¶ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œå› æœå¤šæ™ºèƒ½ä½“æ¨¡å‹åœ¨å› æœæ¨ç†ä»»åŠ¡ä¸­ç›¸è¾ƒäºä¼ ç»Ÿå•ä¸€æ¨¡å‹æå‡äº†çº¦20%çš„å‡†ç¡®ç‡ï¼Œå¹¶åœ¨å¤šä¸ªåº”ç”¨é¢†åŸŸçš„åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç§‘å­¦å‘ç°ã€åŒ»ç–—å¥åº·ã€äº‹å®æ ¸æŸ¥å’Œä¸ªæ€§åŒ–ç³»ç»Ÿç­‰ã€‚å› æœå¤šæ™ºèƒ½ä½“æ¨¡å‹èƒ½å¤Ÿåœ¨å¤æ‚æ•°æ®ç¯å¢ƒä¸­æä¾›æ›´å‡†ç¡®çš„å› æœæ¨ç†ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é«˜ç²¾åº¦å†³ç­–æ”¯æŒçš„åœºæ™¯ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have demonstrated remarkable capabilities in various reasoning and generation tasks. However, their proficiency in complex causal reasoning, discovery, and estimation remains an area of active development, often hindered by issues like hallucination, reliance on spurious correlations, and difficulties in handling nuanced, domain-specific, or personalized causal relationships. Multi-agent systems, leveraging the collaborative or specialized abilities of multiple LLM-based agents, are emerging as a powerful paradigm to address these limitations. This review paper explores the burgeoning field of causal multi-agent LLMs. We examine how these systems are designed to tackle different facets of causality, including causal reasoning and counterfactual analysis, causal discovery from data, and the estimation of causal effects. We delve into the diverse architectural patterns and interaction protocols employed, from pipeline-based processing and debate frameworks to simulation environments and iterative refinement loops. Furthermore, we discuss the evaluation methodologies, benchmarks, and diverse application domains where causal multi-agent LLMs are making an impact, including scientific discovery, healthcare, fact-checking, and personalized systems. Finally, we highlight the persistent challenges, open research questions, and promising future directions in this synergistic field, aiming to provide a comprehensive overview of its current state and potential trajectory.

