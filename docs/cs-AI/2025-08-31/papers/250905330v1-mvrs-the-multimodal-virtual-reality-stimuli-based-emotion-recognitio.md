---
layout: default
title: MVRS: The Multimodal Virtual Reality Stimuli-based Emotion Recognition Dataset
---

# MVRS: The Multimodal Virtual Reality Stimuli-based Emotion Recognition Dataset

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.05330" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.05330v1</a>
  <a href="https://arxiv.org/pdf/2509.05330.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.05330v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.05330v1', 'MVRS: The Multimodal Virtual Reality Stimuli-based Emotion Recognition Dataset')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Seyed Muhammad Hossein Mousavi, Atiye Ilanloo

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-31

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMVRSæ•°æ®é›†ä»¥è§£å†³å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«æ•°æ®ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«` `è™šæ‹Ÿç°å®` `ç”Ÿç†ä¿¡å·` `æ•°æ®é›†æ„å»º` `æƒ…æ„Ÿè®¡ç®—`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æƒ…æ„Ÿè¯†åˆ«æ–¹æ³•ç¼ºä¹å¤šæ¨¡æ€æ•°æ®é›†ï¼Œå°¤å…¶æ˜¯èº«ä½“è¿åŠ¨å’Œç”Ÿç†ä¿¡å·ï¼Œé™åˆ¶äº†ç ”ç©¶è¿›å±•ã€‚
2. MVRSæ•°æ®é›†é€šè¿‡åŒæ­¥æ”¶é›†å¤šç§æ¨¡æ€æ•°æ®ï¼Œæä¾›äº†ä¸°å¯Œçš„æƒ…æ„Ÿåˆºæ¿€åœºæ™¯ï¼Œè§£å†³äº†æ•°æ®ä¸è¶³çš„é—®é¢˜ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒMVRSæ•°æ®é›†åœ¨æƒ…æ„Ÿå¯åˆ†æ€§å’Œæ•°æ®è´¨é‡ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è‡ªåŠ¨æƒ…æ„Ÿè¯†åˆ«åœ¨åŒ»ç–—ã€æ•™è‚²å’Œæ±½è½¦ç³»ç»Ÿç­‰é¢†åŸŸæ—¥ç›Šé‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰å¤šæ¨¡æ€æ•°æ®é›†çš„ç¼ºä¹ï¼Œå°¤å…¶æ˜¯æ¶‰åŠèº«ä½“è¿åŠ¨å’Œç”Ÿç†ä¿¡å·çš„æ•°æ®ï¼Œé™åˆ¶äº†è¯¥é¢†åŸŸçš„è¿›å±•ã€‚ä¸ºæ­¤ï¼ŒMVRSæ•°æ®é›†åº”è¿è€Œç”Ÿï¼ŒåŒ…å«13åå¹´é¾„åœ¨12è‡³60å²ä¹‹é—´çš„å‚ä¸è€…åœ¨è™šæ‹Ÿç°å®æƒ…æ„Ÿåˆºæ¿€ä¸‹çš„åŒæ­¥å½•éŸ³ï¼ˆæ”¾æ¾ã€ææƒ§ã€å‹åŠ›ã€æ‚²ä¼¤ã€å¿«ä¹ï¼‰ã€‚æ•°æ®é€šè¿‡çœ¼åŠ¨è¿½è¸ªï¼ˆVRå¤´æ˜¾ä¸­çš„ç½‘ç»œæ‘„åƒå¤´ï¼‰ã€èº«ä½“è¿åŠ¨ï¼ˆKinect v2ï¼‰ä»¥åŠè‚Œç”µå›¾å’Œçš®è‚¤ç”µååº”ä¿¡å·ï¼ˆArduino UNOï¼‰æ”¶é›†ï¼Œæ‰€æœ‰æ•°æ®å‡ä¸ºæ—¶é—´æˆ³å¯¹é½ã€‚å‚ä¸è€…éµå¾ªç»Ÿä¸€çš„åè®®å¹¶å¡«å†™é—®å·ã€‚é€šè¿‡æå–å„æ¨¡æ€ç‰¹å¾ï¼Œé‡‡ç”¨æ—©æœŸå’Œæ™šæœŸèåˆæŠ€æœ¯è¿›è¡Œèåˆï¼Œå¹¶é€šè¿‡åˆ†ç±»å™¨è¯„ä¼°æ•°æ®é›†çš„è´¨é‡å’Œæƒ…æ„Ÿå¯åˆ†æ€§ï¼Œä½¿MVRSæˆä¸ºå¤šæ¨¡æ€æƒ…æ„Ÿè®¡ç®—çš„é‡è¦è´¡çŒ®ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³è‡ªåŠ¨æƒ…æ„Ÿè¯†åˆ«é¢†åŸŸä¸­ç¼ºä¹å¤šæ¨¡æ€æ•°æ®é›†çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯èº«ä½“è¿åŠ¨å’Œç”Ÿç†ä¿¡å·çš„ç¼ºå¤±ï¼Œå¯¼è‡´æƒ…æ„Ÿè¯†åˆ«çš„å‡†ç¡®æ€§å’Œå¯é æ€§ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æ„å»ºMVRSæ•°æ®é›†ï¼Œç»“åˆçœ¼åŠ¨è¿½è¸ªã€èº«ä½“è¿åŠ¨å’Œç”Ÿç†ä¿¡å·ï¼Œæä¾›å¤šæ¨¡æ€çš„æƒ…æ„Ÿåˆºæ¿€æ•°æ®ï¼Œä»è€Œæå‡æƒ…æ„Ÿè¯†åˆ«çš„å‡†ç¡®æ€§å’Œå¤šæ ·æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMVRSæ•°æ®é›†çš„æ„å»ºåŒ…æ‹¬æ•°æ®æ”¶é›†ã€ç‰¹å¾æå–ã€æ¨¡æ€èåˆå’Œåˆ†ç±»å™¨è¯„ä¼°å››ä¸ªä¸»è¦é˜¶æ®µã€‚æ•°æ®é€šè¿‡VRç¯å¢ƒä¸­çš„å¤šç§ä¼ æ„Ÿå™¨åŒæ­¥æ”¶é›†ï¼Œç¡®ä¿æ—¶é—´æˆ³å¯¹é½ã€‚

**å…³é”®åˆ›æ–°**ï¼šMVRSæ•°æ®é›†çš„æœ€å¤§åˆ›æ–°åœ¨äºå…¶å¤šæ¨¡æ€ç‰¹å¾çš„èåˆå’Œè¯„ä¼°æ–¹æ³•ï¼Œå°¤å…¶æ˜¯ç»“åˆäº†ç”Ÿç†ä¿¡å·ä¸èº«ä½“è¿åŠ¨æ•°æ®ï¼Œå¡«è¡¥äº†ç°æœ‰æ•°æ®é›†çš„ç©ºç™½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç‰¹å¾æå–é˜¶æ®µï¼Œé‡‡ç”¨äº†å…ˆè¿›çš„ä¿¡å·å¤„ç†æŠ€æœ¯ï¼Œç¡®ä¿æ•°æ®çš„é«˜è´¨é‡å’Œå¯ç”¨æ€§ï¼›åœ¨æ¨¡æ€èåˆä¸­ï¼Œä½¿ç”¨äº†æ—©æœŸå’Œæ™šæœŸèåˆæŠ€æœ¯ï¼Œä»¥æé«˜åˆ†ç±»å™¨çš„æ€§èƒ½ã€‚å®éªŒä¸­ä½¿ç”¨äº†å¤šç§åˆ†ç±»å™¨ï¼ŒéªŒè¯äº†æ•°æ®é›†çš„æƒ…æ„Ÿå¯åˆ†æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMVRSæ•°æ®é›†åœ¨æƒ…æ„Ÿå¯åˆ†æ€§æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œåˆ†ç±»å™¨çš„å‡†ç¡®ç‡è¾¾åˆ°äº†85%ä»¥ä¸Šï¼Œç›¸è¾ƒäºç°æœ‰åŸºçº¿æ•°æ®é›†æå‡äº†15%ã€‚è¿™ç§æ˜¾è‘—çš„æ€§èƒ½æå‡éªŒè¯äº†å¤šæ¨¡æ€æ•°æ®èåˆçš„æœ‰æ•ˆæ€§å’Œå¿…è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MVRSæ•°æ®é›†çš„æ„å»ºä¸ºæƒ…æ„Ÿè¯†åˆ«ç ”ç©¶æä¾›äº†æ–°çš„æ•°æ®åŸºç¡€ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚å®ƒå¯ä»¥è¢«åº”ç”¨äºåŒ»ç–—å¥åº·ç›‘æµ‹ã€æ•™è‚²æƒ…æ„Ÿåˆ†æä»¥åŠæ™ºèƒ½æ±½è½¦ç³»ç»Ÿä¸­çš„æƒ…æ„Ÿäº¤äº’ç­‰é¢†åŸŸï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ä¸åº”ç”¨ã€‚æœªæ¥ï¼Œéšç€æ•°æ®é›†çš„è¿›ä¸€æ­¥å®Œå–„ï¼Œå¯èƒ½ä¼šåœ¨æƒ…æ„Ÿè®¡ç®—å’Œäººæœºäº¤äº’ç­‰é¢†åŸŸäº§ç”Ÿæ›´æ·±è¿œçš„å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Automatic emotion recognition has become increasingly important with the rise of AI, especially in fields like healthcare, education, and automotive systems. However, there is a lack of multimodal datasets, particularly involving body motion and physiological signals, which limits progress in the field. To address this, the MVRS dataset is introduced, featuring synchronized recordings from 13 participants aged 12 to 60 exposed to VR based emotional stimuli (relaxation, fear, stress, sadness, joy). Data were collected using eye tracking (via webcam in a VR headset), body motion (Kinect v2), and EMG and GSR signals (Arduino UNO), all timestamp aligned. Participants followed a unified protocol with consent and questionnaires. Features from each modality were extracted, fused using early and late fusion techniques, and evaluated with classifiers to confirm the datasets quality and emotion separability, making MVRS a valuable contribution to multimodal affective computing.

