---
layout: default
title: LLM-Guided Semantic Relational Reasoning for Multimodal Intent Recognition
---

# LLM-Guided Semantic Relational Reasoning for Multimodal Intent Recognition

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.01337" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.01337v1</a>
  <a href="https://arxiv.org/pdf/2509.01337.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.01337v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.01337v1', 'LLM-Guided Semantic Relational Reasoning for Multimodal Intent Recognition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Qianrui Zhou, Hua Xu, Yifan Wang, Xinzhi Dong, Hanlei Zhang

**åˆ†ç±»**: cs.MM, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-01

**å¤‡æ³¨**: Accepted by EMNLP 2025 (Main Track, Long Paper)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/thuiar/LGSRR)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLLMå¼•å¯¼çš„è¯­ä¹‰å…³ç³»æ¨ç†æ–¹æ³•ï¼Œæå‡å¤šæ¨¡æ€æ„å›¾è¯†åˆ«æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ„å›¾è¯†åˆ«` `è¯­ä¹‰å…³ç³»æ¨ç†` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ€ç»´é“¾` `äººæœºäº¤äº’`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨æ¨¡æ€å±‚é¢çš„ä¾èµ–æ€§é™åˆ¶äº†å¯¹ç»†ç²’åº¦è¯­ä¹‰çš„å…³ç³»æ¨ç†ï¼Œé˜»ç¢äº†å¤æ‚æ„å›¾çš„ç†è§£ã€‚
2. åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„çŸ¥è¯†ï¼Œæå–ç»†ç²’åº¦è¯­ä¹‰ä½œä¸ºæŒ‡å¯¼ï¼Œæå‡å°å‹æ¨¡å‹çš„å…³ç³»æ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒLGSRRåœ¨å¤šæ¨¡æ€æ„å›¾å’Œå¯¹è¯è¡Œä¸ºè¯†åˆ«ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæ€§èƒ½å¾—åˆ°æŒç»­æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„LLMå¼•å¯¼çš„è¯­ä¹‰å…³ç³»æ¨ç†ï¼ˆLGSRRï¼‰æ–¹æ³•ï¼Œæ—¨åœ¨åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¹¿æ³›çŸ¥è¯†æ¥å»ºç«‹è¯­ä¹‰åŸºç¡€ï¼Œä»è€Œæå‡å°å‹æ¨¡å‹çš„å…³ç³»æ¨ç†æ€§èƒ½ã€‚è¯¥æ–¹æ³•é€šè¿‡åŸºäºLLMçš„ç­–ç•¥æå–ç»†ç²’åº¦çš„è¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶é‡‡ç”¨ç”±æµ…å…¥æ·±çš„æ€ç»´é“¾ï¼ˆCoTï¼‰è‡ªä¸»åœ°å‘ç°ã€æè¿°å’Œæ’åºè¯­ä¹‰çº¿ç´¢çš„é‡è¦æ€§ï¼Œæ— éœ€æ‰‹åŠ¨å®šä¹‰å…ˆéªŒçŸ¥è¯†ã€‚æ­¤å¤–ï¼Œè®ºæ–‡æ­£å¼åœ°å°†ä¸‰ç§åŸºäºé€»è¾‘åŸåˆ™çš„åŸºæœ¬è¯­ä¹‰å…³ç³»å»ºæ¨¡ï¼Œå¹¶åˆ†æå®ƒä»¬ä¹‹é—´çš„ç»†å¾®ç›¸äº’ä½œç”¨ï¼Œä»¥å®ç°æ›´æœ‰æ•ˆçš„å…³ç³»æ¨ç†ã€‚åœ¨å¤šæ¨¡æ€æ„å›¾å’Œå¯¹è¯è¡Œä¸ºè¯†åˆ«ä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒLGSRRä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå¹¶åœ¨å„ç§è¯­ä¹‰ç†è§£åœºæ™¯ä¸­å®ç°äº†æŒç»­çš„æ€§èƒ½æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå¤šæ¨¡æ€æ„å›¾è¯†åˆ«æ—¨åœ¨ä»å¤šç§æ¨¡æ€çš„ä¿¡å·ä¸­ç†è§£äººç±»æ„å›¾ã€‚ç°æœ‰æ–¹æ³•è¿‡åº¦ä¾èµ–äºç‰¹å®šæ¨¡æ€çš„ä¿¡æ¯ï¼Œç¼ºä¹å¯¹ç»†ç²’åº¦è¯­ä¹‰çš„å…³ç³»æ¨ç†èƒ½åŠ›ï¼Œéš¾ä»¥å¤„ç†å¤æ‚çš„æ„å›¾ç†è§£åœºæ™¯ã€‚è¿™äº›æ–¹æ³•é€šå¸¸éœ€è¦æ‰‹åŠ¨å®šä¹‰å…ˆéªŒçŸ¥è¯†ï¼Œé™åˆ¶äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¼ºå¤§è¯­ä¹‰ç†è§£èƒ½åŠ›ï¼Œæå–ç»†ç²’åº¦çš„è¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶å°†å…¶ä½œä¸ºæŒ‡å¯¼ï¼Œè¾…åŠ©å°å‹æ¨¡å‹è¿›è¡Œå…³ç³»æ¨ç†ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¯ä»¥å…‹æœå°å‹æ¨¡å‹åœ¨çŸ¥è¯†å‚¨å¤‡å’Œæ¨ç†èƒ½åŠ›ä¸Šçš„ä¸è¶³ï¼Œæå‡å…¶åœ¨å¤æ‚åœºæ™¯ä¸‹çš„æ„å›¾è¯†åˆ«æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLGSRRæ–¹æ³•ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) LLMå¼•å¯¼çš„è¯­ä¹‰æå–ï¼šåˆ©ç”¨LLMæå–ç»†ç²’åº¦çš„è¯­ä¹‰ä¿¡æ¯ã€‚2) åŸºäºæ€ç»´é“¾ï¼ˆCoTï¼‰çš„è¯­ä¹‰çº¿ç´¢å‘ç°ã€æè¿°å’Œæ’åºï¼šé‡‡ç”¨ç”±æµ…å…¥æ·±çš„CoTæ–¹æ³•ï¼Œè‡ªä¸»åœ°å‘ç°ã€æè¿°å’Œæ’åºè¯­ä¹‰çº¿ç´¢çš„é‡è¦æ€§ã€‚3) è¯­ä¹‰å…³ç³»å»ºæ¨¡ï¼šå¯¹ä¸‰ç§åŸºæœ¬è¯­ä¹‰å…³ç³»è¿›è¡Œå»ºæ¨¡ï¼Œå¹¶åˆ†æå®ƒä»¬ä¹‹é—´çš„ç›¸äº’ä½œç”¨ã€‚4) å…³ç³»æ¨ç†ï¼šåŸºäºå»ºæ¨¡çš„è¯­ä¹‰å…³ç³»è¿›è¡Œæ¨ç†ï¼Œæœ€ç»ˆå®ç°æ„å›¾è¯†åˆ«ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºåˆ©ç”¨LLMæ¥å¼•å¯¼è¯­ä¹‰å…³ç³»æ¨ç†ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒLGSRRæ— éœ€æ‰‹åŠ¨å®šä¹‰å…ˆéªŒçŸ¥è¯†ï¼Œè€Œæ˜¯é€šè¿‡LLMè‡ªä¸»åœ°å­¦ä¹ å’Œæå–è¯­ä¹‰ä¿¡æ¯ï¼Œä»è€Œæé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œé€‚åº”æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯¹è¯­ä¹‰å…³ç³»è¿›è¡Œäº†å½¢å¼åŒ–å»ºæ¨¡ï¼Œå¹¶åˆ†æäº†å®ƒä»¬ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼Œä»è€Œå®ç°äº†æ›´æœ‰æ•ˆçš„å…³ç³»æ¨ç†ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨LLMå¼•å¯¼çš„è¯­ä¹‰æå–é˜¶æ®µï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„promptå·¥ç¨‹æ¥æŒ‡å¯¼LLMç”Ÿæˆé«˜è´¨é‡çš„è¯­ä¹‰æè¿°ã€‚åœ¨CoTé˜¶æ®µï¼Œè®¾è®¡äº†ç”±æµ…å…¥æ·±çš„æ¨ç†é“¾ï¼Œé€æ­¥æŒ–æ˜è¯­ä¹‰çº¿ç´¢ã€‚åœ¨è¯­ä¹‰å…³ç³»å»ºæ¨¡é˜¶æ®µï¼Œå®šä¹‰äº†ä¸‰ç§åŸºæœ¬è¯­ä¹‰å…³ç³»ï¼Œå¹¶è®¾è®¡äº†ç›¸åº”çš„è®¡ç®—æ–¹æ³•ã€‚æŸå¤±å‡½æ•°æ–¹é¢ï¼Œé‡‡ç”¨äº†äº¤å‰ç†µæŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ¨¡å‹å‚æ•°ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„ç»†èŠ‚å’Œå‚æ•°è®¾ç½®åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒLGSRRåœ¨å¤šæ¨¡æ€æ„å›¾å’Œå¯¹è¯è¡Œä¸ºè¯†åˆ«ä»»åŠ¡ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨æŸä¸ªæ•°æ®é›†ä¸Šï¼ŒLGSRRçš„å‡†ç¡®ç‡æ¯”æœ€å…ˆè¿›çš„æ–¹æ³•æé«˜äº†è¶…è¿‡5%ã€‚æ­¤å¤–ï¼Œæ¶ˆèå®éªŒéªŒè¯äº†LLMå¼•å¯¼å’Œè¯­ä¹‰å…³ç³»å»ºæ¨¡çš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„å„ä¸ªç»„æˆéƒ¨åˆ†éƒ½å¯¹æœ€ç»ˆæ€§èƒ½åšå‡ºäº†è´¡çŒ®ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºäººæœºäº¤äº’ã€æ™ºèƒ½å®¢æœã€æ™ºèƒ½å®¶å±…ã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸã€‚é€šè¿‡æå‡æœºå™¨å¯¹äººç±»æ„å›¾çš„ç†è§£èƒ½åŠ›ï¼Œå¯ä»¥æ”¹å–„ç”¨æˆ·ä½“éªŒï¼Œæé«˜å·¥ä½œæ•ˆç‡ï¼Œå¹¶ä¸ºæœªæ¥çš„æ™ºèƒ½åŒ–åº”ç”¨å¥ å®šåŸºç¡€ã€‚ä¾‹å¦‚ï¼Œåœ¨æ™ºèƒ½å®¢æœä¸­ï¼Œå¯ä»¥æ›´å‡†ç¡®åœ°ç†è§£ç”¨æˆ·çš„é—®é¢˜ï¼Œæä¾›æ›´æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Understanding human intents from multimodal signals is critical for analyzing human behaviors and enhancing human-machine interactions in real-world scenarios. However, existing methods exhibit limitations in their modality-level reliance, constraining relational reasoning over fine-grained semantics for complex intent understanding. This paper proposes a novel LLM-Guided Semantic Relational Reasoning (LGSRR) method, which harnesses the expansive knowledge of large language models (LLMs) to establish semantic foundations that boost smaller models' relational reasoning performance. Specifically, an LLM-based strategy is proposed to extract fine-grained semantics as guidance for subsequent reasoning, driven by a shallow-to-deep Chain-of-Thought (CoT) that autonomously uncovers, describes, and ranks semantic cues by their importance without relying on manually defined priors. Besides, we formally model three fundamental types of semantic relations grounded in logical principles and analyze their nuanced interplay to enable more effective relational reasoning. Extensive experiments on multimodal intent and dialogue act recognition tasks demonstrate LGSRR's superiority over state-of-the-art methods, with consistent performance gains across diverse semantic understanding scenarios. The complete data and code are available at https://github.com/thuiar/LGSRR.

