---
layout: default
title: Large Language Models as End-to-end Combinatorial Optimization Solvers
---

# Large Language Models as End-to-end Combinatorial Optimization Solvers

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.16865" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.16865v1</a>
  <a href="https://arxiv.org/pdf/2509.16865.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.16865v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.16865v1', 'Large Language Models as End-to-end Combinatorial Optimization Solvers')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xia Jiang, Yaoxin Wu, Minshuo Li, Zhiguang Cao, Yingqian Zhang

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-21

**æœŸåˆŠ**: The 39th Annual Conference on Neural Information Processing Systems (NeurIPS 2025)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºLLMçš„ç«¯åˆ°ç«¯ç»„åˆä¼˜åŒ–æ±‚è§£å™¨ï¼Œæ— éœ€ä»£ç ç”Ÿæˆæˆ–è°ƒç”¨å¤–éƒ¨æ±‚è§£å™¨ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç»„åˆä¼˜åŒ–` `å¤§å‹è¯­è¨€æ¨¡å‹` `ç«¯åˆ°ç«¯æ±‚è§£å™¨` `å¼ºåŒ–å­¦ä¹ ` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»Ÿç»„åˆä¼˜åŒ–æ–¹æ³•ä¾èµ–é¢†åŸŸä¸“å®¶çŸ¥è¯†å’Œç‰¹å®šç®—æ³•ï¼Œé€šç”¨æ€§ä¸å¯è®¿é—®æ€§å—é™ã€‚
2. æå‡ºä¸€ç§ç«¯åˆ°ç«¯æ¡†æ¶ï¼Œåˆ©ç”¨LLMç›´æ¥å°†è‡ªç„¶è¯­è¨€é—®é¢˜æè¿°æ˜ å°„åˆ°è§£å†³æ–¹æ¡ˆã€‚
3. é€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒï¼Œæ˜¾è‘—æå‡LLMåœ¨NP-hardé—®é¢˜ä¸Šçš„å¯è¡Œæ€§å’Œæœ€ä¼˜æ€§ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿä½œä¸ºç«¯åˆ°ç«¯çš„ç»„åˆä¼˜åŒ–ï¼ˆCOï¼‰æ±‚è§£å™¨ï¼Œç›´æ¥å°†è‡ªç„¶è¯­è¨€é—®é¢˜æè¿°æ˜ å°„åˆ°è§£å†³æ–¹æ¡ˆã€‚ä¼ ç»Ÿä¸Šï¼Œç»„åˆä¼˜åŒ–é—®é¢˜ï¼ˆå¦‚ç‰©æµå’Œåˆ¶é€ ä¸­çš„å†³ç­–é—®é¢˜ï¼‰ä½¿ç”¨éœ€è¦å¤§é‡é¢†åŸŸçŸ¥è¯†çš„ç‰¹å®šç®—æ³•æ¥è§£å†³ã€‚è™½ç„¶LLMåœ¨è‡ªåŠ¨åŒ–COé—®é¢˜æ±‚è§£æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†ç°æœ‰æ–¹æ³•ä¾èµ–äºä»£ç ç”Ÿæˆæˆ–æ±‚è§£å™¨è°ƒç”¨ç­‰ä¸­é—´æ­¥éª¤ï¼Œé™åˆ¶äº†å…¶é€šç”¨æ€§å’Œå¯è®¿é—®æ€§ã€‚è¯¥æ–¹æ³•é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼šç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä½¿LLMæŒæ¡æ¥è‡ªé¢†åŸŸç‰¹å®šæ±‚è§£å™¨çš„è§£å†³æ–¹æ¡ˆç”Ÿæˆæ¨¡å¼ï¼Œè€Œå¯è¡Œæ€§å’Œæœ€ä¼˜æ€§æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ ï¼ˆFOARLï¼‰è¿‡ç¨‹æ˜¾å¼åœ°ç¼“è§£äº†çº¦æŸè¿åå¹¶ä¼˜åŒ–äº†è§£å†³æ–¹æ¡ˆè´¨é‡ã€‚åœ¨ä¸ƒä¸ªNP-hard COé—®é¢˜ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œé€šè¿‡è°ƒæ•´ä¸€ä¸ª70äº¿å‚æ•°çš„LLMï¼Œè¯¥æ–¹æ³•å®ç°äº†é«˜å¯è¡Œç‡ï¼Œå¹¶å°†å¹³å‡æœ€ä¼˜æ€§å·®è·é™ä½åˆ°1.03-8.20%ï¼Œè¶…è¿‡äº†é€šç”¨LLMï¼ˆå¦‚GPT-4oï¼‰ã€æ¨ç†æ¨¡å‹ï¼ˆå¦‚DeepSeek-R1ï¼‰å’Œé¢†åŸŸç‰¹å®šçš„å¯å‘å¼æ–¹æ³•ã€‚è¯¥æ–¹æ³•å»ºç«‹äº†ä¸€ä¸ªç»Ÿä¸€çš„ã€åŸºäºè¯­è¨€çš„COæµç¨‹ï¼Œæ— éœ€å¤§é‡ä»£ç æ‰§è¡Œæˆ–é’ˆå¯¹ä¸åŒé—®é¢˜è¿›è¡Œæ‰‹åŠ¨æ¶æ„è°ƒæ•´ï¼Œä¸ºä¼ ç»Ÿæ±‚è§£å™¨è®¾è®¡æä¾›äº†ä¸€ç§é€šç”¨çš„ã€è¯­è¨€é©±åŠ¨çš„æ›¿ä»£æ–¹æ¡ˆï¼ŒåŒæ—¶ä¿æŒäº†ç›¸å¯¹å¯è¡Œæ€§ä¿è¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ç»„åˆä¼˜åŒ–ï¼ˆCOï¼‰é—®é¢˜ï¼Œä¾‹å¦‚æ—…è¡Œå•†é—®é¢˜ã€è½¦è¾†è·¯å¾„é—®é¢˜ç­‰ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦äººå·¥è®¾è®¡ç‰¹å®šé¢†åŸŸçš„ç®—æ³•ï¼Œæˆ–è€…ä¾èµ–äºä¸­é—´æ­¥éª¤ï¼Œå¦‚ç”Ÿæˆä»£ç æˆ–è°ƒç”¨å¤–éƒ¨æ±‚è§£å™¨ï¼Œè¿™é™åˆ¶äº†å…¶é€šç”¨æ€§å’Œæ˜“ç”¨æ€§ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•å¾€å¾€éœ€è¦å¤§é‡çš„é¢†åŸŸçŸ¥è¯†å’Œæ‰‹åŠ¨è°ƒæ•´ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯ç›´æ¥åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å­¦ä¹ ä»è‡ªç„¶è¯­è¨€é—®é¢˜æè¿°åˆ°è§£å†³æ–¹æ¡ˆçš„æ˜ å°„ã€‚é€šè¿‡å°†COé—®é¢˜è½¬åŒ–ä¸ºè¯­è¨€ç†è§£å’Œç”Ÿæˆä»»åŠ¡ï¼Œé¿å…äº†å¯¹ç‰¹å®šé¢†åŸŸç®—æ³•çš„ä¾èµ–ï¼Œå¹¶æé«˜äº†æ¨¡å‹çš„é€šç”¨æ€§å’Œå¯è®¿é—®æ€§ã€‚è¿™ç§æ–¹æ³•çš„æ ¸å¿ƒåœ¨äºå¦‚ä½•å¼•å¯¼LLMç”Ÿæˆæ—¢å¯è¡Œåˆé«˜è´¨é‡çš„è§£å†³æ–¹æ¡ˆã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¯è¡Œæ€§å’Œæœ€ä¼˜æ€§æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ ï¼ˆFOARLï¼‰ã€‚åœ¨SFTé˜¶æ®µï¼Œä½¿ç”¨é¢†åŸŸç‰¹å®šæ±‚è§£å™¨ç”Ÿæˆçš„æ•°æ®å¯¹LLMè¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶å­¦ä¹ åˆ°è§£å†³æ–¹æ¡ˆçš„ç”Ÿæˆæ¨¡å¼ã€‚åœ¨FOARLé˜¶æ®µï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥ä¼˜åŒ–LLMï¼Œä½¿å…¶èƒ½å¤Ÿç”Ÿæˆæ»¡è¶³çº¦æŸæ¡ä»¶ä¸”å…·æœ‰è¾ƒé«˜è´¨é‡çš„è§£å†³æ–¹æ¡ˆã€‚è¯¥æ¡†æ¶ç›´æ¥å°†è‡ªç„¶è¯­è¨€é—®é¢˜æè¿°è¾“å…¥LLMï¼Œè¾“å‡ºè§£å†³æ–¹æ¡ˆï¼Œæ— éœ€ä¸­é—´æ­¥éª¤ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå°†LLMä½œä¸ºç«¯åˆ°ç«¯çš„COæ±‚è§£å™¨ï¼Œé¿å…äº†å¯¹ä¼ ç»Ÿæ±‚è§£å™¨æˆ–ä»£ç ç”Ÿæˆçš„ä¾èµ–ã€‚æ­¤å¤–ï¼ŒFOARLè¿‡ç¨‹æ˜¾å¼åœ°è€ƒè™‘äº†å¯è¡Œæ€§å’Œæœ€ä¼˜æ€§ï¼Œé€šè¿‡è®¾è®¡ç‰¹å®šçš„å¥–åŠ±å‡½æ•°ï¼Œå¼•å¯¼LLMç”Ÿæˆé«˜è´¨é‡çš„è§£å†³æ–¹æ¡ˆã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æ›´åŠ é€šç”¨ã€æ˜“ç”¨ï¼Œå¹¶ä¸”èƒ½å¤Ÿå¤„ç†å¤šç§ä¸åŒçš„COé—®é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼šSFTé˜¶æ®µä½¿ç”¨é¢†åŸŸç‰¹å®šæ±‚è§£å™¨ç”Ÿæˆçš„æ•°æ®è¿›è¡Œå¾®è°ƒï¼ŒæŸå¤±å‡½æ•°ä¸ºäº¤å‰ç†µæŸå¤±ã€‚FOARLé˜¶æ®µçš„å…³é”®åœ¨äºå¥–åŠ±å‡½æ•°çš„è®¾è®¡ï¼Œè¯¥å¥–åŠ±å‡½æ•°ç»¼åˆè€ƒè™‘äº†å¯è¡Œæ€§å’Œæœ€ä¼˜æ€§ï¼Œä¾‹å¦‚ï¼Œå¯¹äºè¿åçº¦æŸçš„è§£å†³æ–¹æ¡ˆç»™äºˆè´Ÿå¥–åŠ±ï¼Œå¯¹äºé«˜è´¨é‡çš„è§£å†³æ–¹æ¡ˆç»™äºˆæ­£å¥–åŠ±ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†è¯´æ˜ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸ƒä¸ªNP-hard COé—®é¢˜ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚é€šè¿‡è°ƒæ•´ä¸€ä¸ª70äº¿å‚æ•°çš„LLMï¼Œå®ç°äº†é«˜å¯è¡Œç‡ï¼Œå¹¶å°†å¹³å‡æœ€ä¼˜æ€§å·®è·é™ä½åˆ°1.03-8.20%ï¼Œä¼˜äºGPT-4oã€DeepSeek-R1ç­‰é€šç”¨LLMå’Œé¢†åŸŸç‰¹å®šçš„å¯å‘å¼æ–¹æ³•ã€‚è¿™è¡¨æ˜è¯¥æ–¹æ³•å…·æœ‰å¾ˆå¼ºçš„ç«äº‰åŠ›å’Œæ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºç‰©æµã€ä¾›åº”é“¾ç®¡ç†ã€ç”Ÿäº§è°ƒåº¦ã€èµ„æºåˆ†é…ç­‰é¢†åŸŸã€‚é€šè¿‡è‡ªç„¶è¯­è¨€æè¿°é—®é¢˜ï¼Œå³å¯åˆ©ç”¨LLMå¿«é€Ÿç”Ÿæˆè§£å†³æ–¹æ¡ˆï¼Œé™ä½äº†å¯¹ä¸“ä¸šç®—æ³•å·¥ç¨‹å¸ˆçš„ä¾èµ–ï¼Œæé«˜äº†å†³ç­–æ•ˆç‡ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›æ‰©å±•åˆ°æ›´å¤æ‚çš„ä¼˜åŒ–é—®é¢˜ï¼Œå¹¶ä¸å…¶ä»–AIæŠ€æœ¯ç›¸ç»“åˆï¼Œå®ç°æ›´æ™ºèƒ½åŒ–çš„å†³ç­–æ”¯æŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Combinatorial optimization (CO) problems, central to decision-making scenarios like logistics and manufacturing, are traditionally solved using problem-specific algorithms requiring significant domain expertise. While large language models (LLMs) have shown promise in automating CO problem solving, existing approaches rely on intermediate steps such as code generation or solver invocation, limiting their generality and accessibility. This paper introduces a novel framework that empowers LLMs to serve as end-to-end CO solvers by directly mapping natural language problem descriptions to solutions. We propose a two-stage training strategy: supervised fine-tuning (SFT) imparts LLMs with solution generation patterns from domain-specific solvers, while a feasibility-and-optimality-aware reinforcement learning (FOARL) process explicitly mitigates constraint violations and refines solution quality. Evaluation across seven NP-hard CO problems shows that our method achieves a high feasibility rate and reduces the average optimality gap to 1.03-8.20% by tuning a 7B-parameter LLM, surpassing both general-purpose LLMs (e.g., GPT-4o), reasoning models (e.g., DeepSeek-R1), and domain-specific heuristics. Our method establishes a unified language-based pipeline for CO without extensive code execution or manual architectural adjustments for different problems, offering a general and language-driven alternative to traditional solver design while maintaining relative feasibility guarantees.

