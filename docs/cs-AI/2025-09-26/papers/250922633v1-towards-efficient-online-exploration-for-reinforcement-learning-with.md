---
layout: default
title: Towards Efficient Online Exploration for Reinforcement Learning with Human Feedback
---

# Towards Efficient Online Exploration for Reinforcement Learning with Human Feedback

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.22633" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.22633v1</a>
  <a href="https://arxiv.org/pdf/2509.22633.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.22633v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.22633v1', 'Towards Efficient Online Exploration for Reinforcement Learning with Human Feedback')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Gen Li, Yuling Yan

**åˆ†ç±»**: stat.ML, cs.AI, cs.CL, cs.LG, math.ST

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåœ¨çº¿RLHFé«˜æ•ˆæ¢ç´¢ç®—æ³•ï¼Œè§£å†³å¥–åŠ±æ¨¡å‹ä¸ç¡®å®šæ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ` `åœ¨çº¿å­¦ä¹ ` `æ¢ç´¢ç­–ç•¥` `å¥–åŠ±æ¨¡å‹` `å¤šè‡‚è€è™æœº`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºä¹è§‚ä¸»ä¹‰çš„RLHFæ¢ç´¢ç®—æ³•ï¼Œåœ¨å‡å°‘å¥–åŠ±å·®å¼‚çš„ä¸ç¡®å®šæ€§æ–¹é¢æ•ˆç‡è¾ƒä½ï¼Œå¯¼è‡´æ¬¡ä¼˜çš„æ¢ç´¢ç­–ç•¥ã€‚
2. æå‡ºä¸€ç§æ–°çš„æ¢ç´¢æ–¹æ¡ˆï¼Œé€šè¿‡å°†åå¥½æŸ¥è¯¢å¯¼å‘äºå‡å°‘ä¸ç­–ç•¥æ”¹è¿›æœ€ç›¸å…³çš„å¥–åŠ±å·®å¼‚çš„ä¸ç¡®å®šæ€§ï¼Œæ¥æå‡æ¢ç´¢æ•ˆç‡ã€‚
3. åœ¨å¤šè‡‚è€è™æœºRLHFæ¨¡å‹ä¸‹ï¼Œè¯æ˜äº†æ‰€æç®—æ³•çš„é—æ†¾ç•Œä¸º$T^{(Î²+1)/(Î²+2)}$ï¼Œå®ç°äº†å¤šé¡¹å¼çº§åˆ«çš„é—æ†¾ç¼©æ”¾ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç ”ç©¶äº†åœ¨çº¿äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ (RLHF)çš„æ¢ç´¢åŸåˆ™ï¼Œæ—¨åœ¨è‡ªé€‚åº”åœ°æ”¶é›†æ–°çš„åå¥½æ•°æ®ï¼Œä»¥æ•°æ®é«˜æ•ˆçš„æ–¹å¼æ”¹è¿›å¥–åŠ±æ¨¡å‹å’Œç­–ç•¥ã€‚é€šè¿‡åˆ†æç°æœ‰çš„åŸºäºä¹è§‚ä¸»ä¹‰çš„æ¢ç´¢ç®—æ³•ï¼Œæˆ‘ä»¬å‘ç°å®ƒä»¬åœ¨é‡‡æ ·åè®®ä¸­å­˜åœ¨ç¼ºé™·ï¼šå€¾å‘äºæ”¶é›†æ— æ³•æœ‰æ•ˆå‡å°‘å¥–åŠ±å·®å¼‚ä¸­ä¿¡æ¯é‡æœ€å¤§çš„ä¸ç¡®å®šæ€§çš„æ¯”è¾ƒæ•°æ®ã€‚æˆ‘ä»¬è¯æ˜äº†è¿™äº›æ–¹æ³•å¯èƒ½å¯¼è‡´åœ¨æŒ‡æ•°çº§é•¿çš„æ—¶é—´èŒƒå›´å†…äº§ç”Ÿçº¿æ€§é—æ†¾ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¢ç´¢æ–¹æ¡ˆï¼Œå°†åå¥½æŸ¥è¯¢å¯¼å‘äºå‡å°‘ä¸ç­–ç•¥æ”¹è¿›æœ€ç›¸å…³çš„å¥–åŠ±å·®å¼‚çš„ä¸ç¡®å®šæ€§ã€‚åœ¨RLHFçš„å¤šè‡‚è€è™æœºæ¨¡å‹ä¸‹ï¼Œæˆ‘ä»¬å»ºç«‹äº†é˜¶æ•°ä¸º$T^{(Î²+1)/(Î²+2)}$çš„é—æ†¾ç•Œï¼Œå…¶ä¸­$Î²>0$æ˜¯ä¸€ä¸ªå¹³è¡¡å¥–åŠ±æœ€å¤§åŒ–å’Œå‡è½»åˆ†å¸ƒåç§»çš„è¶…å‚æ•°ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåœ¨çº¿RLHFç®—æ³•ï¼Œå…¶é—æ†¾ç•Œä»¥æ‰€æœ‰æ¨¡å‹å‚æ•°çš„å¤šé¡¹å¼å½¢å¼ç¼©æ”¾ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³åœ¨çº¿äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ä¸­ï¼Œå¦‚ä½•é«˜æ•ˆæ¢ç´¢ä»¥æ”¹è¿›å¥–åŠ±æ¨¡å‹å’Œç­–ç•¥çš„é—®é¢˜ã€‚ç°æœ‰åŸºäºä¹è§‚ä¸»ä¹‰çš„æ¢ç´¢ç®—æ³•ï¼Œä¾‹å¦‚UCBï¼ˆUpper Confidence Boundï¼‰ç­‰ï¼Œåœ¨RLHFåœºæ™¯ä¸‹ï¼Œå€¾å‘äºæ”¶é›†å¯¹å‡å°‘å¥–åŠ±å·®å¼‚ä¸ç¡®å®šæ€§è´¡çŒ®è¾ƒå°çš„æ•°æ®ï¼Œå¯¼è‡´æ¢ç´¢æ•ˆç‡ä½ä¸‹ï¼Œæœ€ç»ˆå½±å“ç­–ç•¥çš„ä¼˜åŒ–æ•ˆæœã€‚è¿™äº›æ–¹æ³•å¯èƒ½å¯¼è‡´åœ¨é•¿æ—¶é—´èŒƒå›´å†…äº§ç”Ÿçº¿æ€§é—æ†¾ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯ï¼Œå¹¶éæ‰€æœ‰å¥–åŠ±å·®å¼‚çš„ä¸ç¡®å®šæ€§éƒ½å¯¹ç­–ç•¥æ”¹è¿›æœ‰åŒç­‰ä»·å€¼ã€‚åº”è¯¥ä¼˜å…ˆæ¢ç´¢é‚£äº›ä¸ç­–ç•¥æ”¹è¿›æœ€ç›¸å…³çš„å¥–åŠ±å·®å¼‚çš„ä¸ç¡®å®šæ€§ã€‚é€šè¿‡ä¸»åŠ¨é€‰æ‹©èƒ½å¤Ÿæœ€å¤§ç¨‹åº¦å‡å°‘è¿™äº›å…³é”®ä¸ç¡®å®šæ€§çš„åå¥½æŸ¥è¯¢ï¼Œå¯ä»¥æ›´æœ‰æ•ˆåœ°æ”¹è¿›å¥–åŠ±æ¨¡å‹ï¼Œä»è€Œæå‡ç­–ç•¥æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡å°†RLHFé—®é¢˜å»ºæ¨¡ä¸ºå¤šè‡‚è€è™æœºé—®é¢˜ã€‚æ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼š1ï¼‰æ ¹æ®å½“å‰å¥–åŠ±æ¨¡å‹çš„ä¸ç¡®å®šæ€§ï¼Œé€‰æ‹©ä¸€ç»„å€™é€‰çš„åŠ¨ä½œå¯¹ï¼ˆå³è€è™æœºè‡‚ï¼‰ï¼›2ï¼‰å‘äººç±»è¯·æ±‚å¯¹è¿™äº›åŠ¨ä½œå¯¹çš„åå¥½åé¦ˆï¼›3ï¼‰åˆ©ç”¨äººç±»åé¦ˆæ›´æ–°å¥–åŠ±æ¨¡å‹ï¼›4ï¼‰åŸºäºæ›´æ–°åçš„å¥–åŠ±æ¨¡å‹ï¼Œä¼˜åŒ–ç­–ç•¥ã€‚å…³é”®åœ¨äºå¦‚ä½•é€‰æ‹©åŠ¨ä½œå¯¹ï¼Œå³å¦‚ä½•è®¾è®¡æ¢ç´¢ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§æ–°çš„æ¢ç´¢ç­–ç•¥ï¼Œè¯¥ç­–ç•¥å¹¶éç›²ç›®åœ°è¿½æ±‚å‡å°‘æ‰€æœ‰å¥–åŠ±å·®å¼‚çš„ä¸ç¡®å®šæ€§ï¼Œè€Œæ˜¯æœ‰é€‰æ‹©æ€§åœ°å…³æ³¨é‚£äº›å¯¹ç­–ç•¥æ”¹è¿›å½±å“æœ€å¤§çš„ä¸ç¡®å®šæ€§ã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥ç­–ç•¥ä¼šè¯„ä¼°æ¯ä¸ªå€™é€‰åŠ¨ä½œå¯¹çš„åå¥½æŸ¥è¯¢ï¼Œå¯¹å‡å°‘ç­–ç•¥æ”¹è¿›æœ€ç›¸å…³çš„å¥–åŠ±å·®å¼‚ä¸ç¡®å®šæ€§çš„è´¡çŒ®ï¼Œå¹¶é€‰æ‹©è´¡çŒ®æœ€å¤§çš„åŠ¨ä½œå¯¹è¿›è¡ŒæŸ¥è¯¢ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡è®¾è®¡äº†ä¸€ä¸ªè¶…å‚æ•°$Î²>0$ï¼Œç”¨äºå¹³è¡¡å¥–åŠ±æœ€å¤§åŒ–å’Œå‡è½»åˆ†å¸ƒåç§»ã€‚$Î²$è¶Šå¤§ï¼Œåˆ™è¶Šå€¾å‘äºæ¢ç´¢ï¼Œä»¥å‡å°‘å¥–åŠ±æ¨¡å‹çš„ä¸ç¡®å®šæ€§ï¼›$Î²$è¶Šå°ï¼Œåˆ™è¶Šå€¾å‘äºåˆ©ç”¨å½“å‰å¥–åŠ±æ¨¡å‹ï¼Œä»¥æœ€å¤§åŒ–å¥–åŠ±ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æ¨å¯¼å‡ºäº†é—æ†¾ç•Œï¼Œå¹¶è¯æ˜äº†æ‰€æç®—æ³•çš„é—æ†¾ç•Œä»¥æ‰€æœ‰æ¨¡å‹å‚æ•°çš„å¤šé¡¹å¼å½¢å¼ç¼©æ”¾ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡åœ¨å¤šè‡‚è€è™æœºRLHFæ¨¡å‹ä¸‹ï¼Œè¯æ˜äº†æ‰€æå‡ºçš„æ¢ç´¢ç®—æ³•çš„é—æ†¾ç•Œä¸º$T^{(Î²+1)/(Î²+2)}$ï¼Œå…¶ä¸­$Î²>0$æ˜¯ä¸€ä¸ªè¶…å‚æ•°ã€‚è¿™æ˜¯ç¬¬ä¸€ä¸ªåœ¨çº¿RLHFç®—æ³•ï¼Œå…¶é—æ†¾ç•Œä»¥æ‰€æœ‰æ¨¡å‹å‚æ•°çš„å¤šé¡¹å¼å½¢å¼ç¼©æ”¾ï¼Œè¡¨æ˜äº†è¯¥ç®—æ³•åœ¨ç†è®ºä¸Šçš„ä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦äººç±»åé¦ˆæ¥ä¼˜åŒ–ç­–ç•¥çš„å¼ºåŒ–å­¦ä¹ åœºæ™¯ï¼Œä¾‹å¦‚å¤§å‹è¯­è¨€æ¨¡å‹çš„å¯¹é½ã€æœºå™¨äººæŠ€èƒ½å­¦ä¹ ã€æ¨èç³»ç»Ÿç­‰ã€‚é€šè¿‡æ›´æœ‰æ•ˆåœ°åˆ©ç”¨äººç±»åé¦ˆï¼Œå¯ä»¥é™ä½è®­ç»ƒæˆæœ¬ï¼Œæå‡æ¨¡å‹æ€§èƒ½ï¼Œå¹¶ä½¿æ¨¡å‹æ›´å¥½åœ°ç¬¦åˆäººç±»çš„åå¥½å’Œä»·å€¼è§‚ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement learning with human feedback (RLHF), which learns a reward model from human preference data and then optimizes a policy to favor preferred responses, has emerged as a central paradigm for aligning large language models (LLMs) with human preferences. In this paper, we investigate exploration principles for online RLHF, where one seeks to adaptively collect new preference data to refine both the reward model and the policy in a data-efficient manner. By examining existing optimism-based exploration algorithms, we identify a drawback in their sampling protocol: they tend to gather comparisons that fail to reduce the most informative uncertainties in reward differences, and we prove lower bounds showing that such methods can incur linear regret over exponentially long horizons. Motivated by this insight, we propose a new exploration scheme that directs preference queries toward reducing uncertainty in reward differences most relevant to policy improvement. Under a multi-armed bandit model of RLHF, we establish regret bounds of order $T^{(Î²+1)/(Î²+2)}$, where $Î²>0$ is a hyperparameter that balances reward maximization against mitigating distribution shift. To our knowledge, this is the first online RLHF algorithm with regret scaling polynomially in all model parameters.

