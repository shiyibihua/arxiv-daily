---
layout: default
title: UniMIC: Token-Based Multimodal Interactive Coding for Human-AI Collaboration
---

# UniMIC: Token-Based Multimodal Interactive Coding for Human-AI Collaboration

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.22570" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.22570v1</a>
  <a href="https://arxiv.org/pdf/2509.22570.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.22570v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.22570v1', 'UniMIC: Token-Based Multimodal Interactive Coding for Human-AI Collaboration')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Qi Mao, Tinghan Yang, Jiahao Li, Bin Li, Libiao Jin, Yan Lu

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**UniMICï¼šé¢å‘äººæœºåä½œçš„TokenåŒ–å¤šæ¨¡æ€äº¤äº’ç¼–ç æ¡†æ¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€äº¤äº’` `äººæœºåä½œ` `TokenåŒ–ç¼–ç ` `ä½æ¯”ç‰¹ç‡ä¼ è¾“` `Transformer` `ç†µæ¨¡å‹` `å›¾åƒå‹ç¼©` `AIä»£ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç¼–è§£ç å™¨åœ¨å¤šæ¨¡æ€äººæœºäº¤äº’ä¸­å­˜åœ¨å•å‘é€šä¿¡å’Œæ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„èƒ½åŠ›ã€‚
2. UniMICé€šè¿‡å¼•å…¥TokenåŒ–çš„å¤šæ¨¡æ€è¡¨ç¤ºï¼Œå®ç°äº†è¾¹ç¼˜è®¾å¤‡å’Œäº‘ç«¯AIä»£ç†ä¹‹é—´çš„é«˜æ•ˆä½æ¯”ç‰¹ç‡é€šä¿¡ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒUniMICåœ¨å¤šç§è§†è§‰ä»»åŠ¡ä¸­ï¼Œå³ä½¿åœ¨æä½æ¯”ç‰¹ç‡ä¸‹ä¹Ÿèƒ½ä¿æŒè‰¯å¥½çš„æ€§èƒ½ï¼Œæ˜¾è‘—èŠ‚çœäº†æ¯”ç‰¹ç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰å’Œäº‘ç«¯AIä»£ç†çš„å¿«é€Ÿå‘å±•æ­£å°†äººæœºåä½œè½¬å˜ä¸ºåŒå‘ã€å¤šæ¨¡æ€çš„äº¤äº’ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç¼–è§£ç å™¨ä»ç„¶é’ˆå¯¹å•æ¨¡æ€ã€å•å‘é€šä¿¡è¿›è¡Œäº†ä¼˜åŒ–ï¼Œå¯¼è‡´åœ¨ä¼ ç»Ÿçš„å‹ç¼©-ä¼ è¾“-é‡å»ºæµç¨‹ä¸‹æ€§èƒ½é‡å¤ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†UniMICï¼Œä¸€ä¸ªç»Ÿä¸€çš„åŸºäºTokençš„å¤šæ¨¡æ€äº¤äº’ç¼–ç æ¡†æ¶ï¼Œå®ƒè¿æ¥äº†è¾¹ç¼˜è®¾å¤‡å’Œäº‘ç«¯AIä»£ç†ã€‚UniMICä¸ä¼ è¾“åŸå§‹åƒç´ æˆ–çº¯æ–‡æœ¬ï¼Œè€Œæ˜¯é‡‡ç”¨ç´§å‡‘çš„TokenåŒ–è¡¨ç¤ºä½œä¸ºé€šä¿¡åª’ä»‹ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„ä½æ¯”ç‰¹ç‡ä¼ è¾“ï¼ŒåŒæ—¶ä¿æŒä¸LMMçš„å…¼å®¹æ€§ã€‚ä¸ºäº†è¿›ä¸€æ­¥å¢å¼ºå‹ç¼©ï¼Œè½»é‡çº§çš„åŸºäºTransformerçš„ç†µæ¨¡å‹ï¼Œé€šè¿‡åœºæ™¯ç‰¹å®šçš„è®¾è®¡â€”â€”é€šç”¨ã€æ©ç å’Œæ–‡æœ¬æ¡ä»¶â€”â€”æœ‰æ•ˆåœ°æœ€å°åŒ–äº†Tokené—´çš„å†—ä½™ã€‚åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€æ–‡æœ¬å¼•å¯¼çš„å›¾åƒä¿®å¤ã€å›¾åƒæ‰©å±•å’Œè§†è§‰é—®ç­”ç­‰ä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒUniMICå®ç°äº†æ˜¾è‘—çš„æ¯”ç‰¹ç‡èŠ‚çœï¼Œå³ä½¿åœ¨è¶…ä½æ¯”ç‰¹ç‡ï¼ˆ<0.05bppï¼‰ä¸‹ä¹Ÿèƒ½ä¿æŒé²æ£’æ€§ï¼Œè€Œä¸ä¼šå½±å“ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚è¿™äº›ç»“æœç¡®ç«‹äº†UniMICä½œä¸ºä¸‹ä¸€ä»£å¤šæ¨¡æ€äº¤äº’é€šä¿¡çš„å®ç”¨ä¸”å…·æœ‰å‰ç»æ€§çš„èŒƒä¾‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„äººå·¥æ™ºèƒ½åä½œç³»ç»Ÿé€šå¸¸ä¾èµ–äºä¼ è¾“åŸå§‹åƒç´ æˆ–æ–‡æœ¬æ•°æ®ï¼Œè¿™åœ¨å¸¦å®½å—é™çš„ç¯å¢ƒä¸‹æ•ˆç‡ä½ä¸‹ã€‚ä¼ ç»Ÿçš„ç¼–è§£ç å™¨é’ˆå¯¹å•æ¨¡æ€æ•°æ®è®¾è®¡ï¼Œæ— æ³•æœ‰æ•ˆå¤„ç†å¤šæ¨¡æ€äº¤äº’åœºæ™¯ï¼Œå¹¶ä¸”åœ¨å‹ç¼©ã€ä¼ è¾“å’Œé‡å»ºè¿‡ç¨‹ä¸­ä¼šé€ æˆä¿¡æ¯æŸå¤±ï¼Œå½±å“ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šUniMICçš„æ ¸å¿ƒæ€æƒ³æ˜¯ä½¿ç”¨TokenåŒ–çš„è¡¨ç¤ºä½œä¸ºå¤šæ¨¡æ€ä¿¡æ¯çš„ä¸­é—´åª’ä»‹ã€‚é€šè¿‡å°†å›¾åƒå’Œæ–‡æœ¬ç­‰ä¿¡æ¯è½¬æ¢ä¸ºç¦»æ•£çš„Tokenåºåˆ—ï¼Œå¯ä»¥å®ç°æ›´é«˜æ•ˆçš„å‹ç¼©å’Œä¼ è¾“ï¼ŒåŒæ—¶ä¿æŒä¸å¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„å…¼å®¹æ€§ã€‚è¿™ç§æ–¹æ³•é¿å…äº†ç›´æ¥ä¼ è¾“åŸå§‹æ•°æ®ï¼Œå‡å°‘äº†å¸¦å®½éœ€æ±‚ï¼Œå¹¶é™ä½äº†ä¿¡æ¯æŸå¤±çš„é£é™©ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šUniMICæ¡†æ¶åŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) å¤šæ¨¡æ€æ•°æ®TokenåŒ–ï¼šå°†å›¾åƒå’Œæ–‡æœ¬ç­‰è¾“å…¥æ•°æ®è½¬æ¢ä¸ºTokenåºåˆ—ã€‚2) ç†µæ¨¡å‹ï¼šåˆ©ç”¨Transformerç»“æ„ï¼Œå¯¹Tokenåºåˆ—è¿›è¡Œå‹ç¼©ï¼Œå»é™¤å†—ä½™ä¿¡æ¯ã€‚UniMICè®¾è®¡äº†ä¸‰ç§åœºæ™¯ç‰¹å®šçš„ç†µæ¨¡å‹ï¼šé€šç”¨æ¨¡å‹ã€æ©ç æ¨¡å‹å’Œæ–‡æœ¬æ¡ä»¶æ¨¡å‹ï¼Œä»¥é€‚åº”ä¸åŒçš„ä»»åŠ¡éœ€æ±‚ã€‚3) Tokenåºåˆ—ä¼ è¾“ï¼šå°†å‹ç¼©åçš„Tokenåºåˆ—é€šè¿‡ç½‘ç»œä¼ è¾“åˆ°äº‘ç«¯AIä»£ç†ã€‚4) å¤šæ¨¡æ€æ•°æ®é‡å»ºï¼šäº‘ç«¯AIä»£ç†æ¥æ”¶åˆ°Tokenåºåˆ—åï¼Œå°†å…¶è§£ç ä¸ºåŸå§‹çš„å¤šæ¨¡æ€æ•°æ®ï¼Œç”¨äºä¸‹æ¸¸ä»»åŠ¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šUniMICçš„å…³é”®åˆ›æ–°åœ¨äºå…¶ç»Ÿä¸€çš„TokenåŒ–å¤šæ¨¡æ€äº¤äº’ç¼–ç æ¡†æ¶ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºåƒç´ æˆ–æ–‡æœ¬çš„ç¼–è§£ç å™¨ä¸åŒï¼ŒUniMICä½¿ç”¨TokenåŒ–çš„è¡¨ç¤ºä½œä¸ºé€šä¿¡åª’ä»‹ï¼Œå®ç°äº†é«˜æ•ˆçš„ä½æ¯”ç‰¹ç‡ä¼ è¾“ï¼ŒåŒæ—¶ä¿æŒäº†ä¸å¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„å…¼å®¹æ€§ã€‚æ­¤å¤–ï¼ŒUniMICè¿˜è®¾è®¡äº†åœºæ™¯ç‰¹å®šçš„ç†µæ¨¡å‹ï¼Œè¿›ä¸€æ­¥æé«˜äº†å‹ç¼©æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šUniMICçš„ç†µæ¨¡å‹åŸºäºTransformerç»“æ„ï¼Œåˆ©ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶æ•æ‰Tokenä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚ä¸ºäº†é€‚åº”ä¸åŒçš„ä»»åŠ¡éœ€æ±‚ï¼ŒUniMICè®¾è®¡äº†ä¸‰ç§ç†µæ¨¡å‹ï¼šé€šç”¨æ¨¡å‹ç”¨äºå¤„ç†ä¸€èˆ¬çš„å¤šæ¨¡æ€æ•°æ®ï¼Œæ©ç æ¨¡å‹ç”¨äºå¤„ç†å›¾åƒä¿®å¤ç­‰ä»»åŠ¡ï¼Œæ–‡æœ¬æ¡ä»¶æ¨¡å‹ç”¨äºå¤„ç†æ–‡æœ¬å¼•å¯¼çš„å›¾åƒç”Ÿæˆç­‰ä»»åŠ¡ã€‚æŸå¤±å‡½æ•°é‡‡ç”¨äº¤å‰ç†µæŸå¤±ï¼Œä¼˜åŒ–ç›®æ ‡æ˜¯æœ€å°åŒ–Tokenåºåˆ—çš„ç¼–ç é•¿åº¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

UniMICåœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€æ–‡æœ¬å¼•å¯¼çš„å›¾åƒä¿®å¤ã€å›¾åƒæ‰©å±•å’Œè§†è§‰é—®ç­”ç­‰ä»»åŠ¡ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒã€‚ç»“æœè¡¨æ˜ï¼ŒUniMICåœ¨æä½çš„æ¯”ç‰¹ç‡ï¼ˆ<0.05bppï¼‰ä¸‹ä¹Ÿèƒ½ä¿æŒè‰¯å¥½çš„æ€§èƒ½ï¼Œå¹¶ä¸”å®ç°äº†æ˜¾è‘—çš„æ¯”ç‰¹ç‡èŠ‚çœã€‚ä¾‹å¦‚ï¼Œåœ¨æŸäº›ä»»åŠ¡ä¸­ï¼ŒUniMICå¯ä»¥å°†æ¯”ç‰¹ç‡é™ä½åˆ°ä¼ ç»Ÿæ–¹æ³•çš„ååˆ†ä¹‹ä¸€ï¼Œè€Œä¸ä¼šå½±å“ä¸‹æ¸¸ä»»åŠ¡çš„å‡†ç¡®ç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

UniMICé€‚ç”¨äºå„ç§éœ€è¦ä½å»¶è¿Ÿã€é«˜æ•ˆç‡å¤šæ¨¡æ€äººæœºåä½œçš„åœºæ™¯ï¼Œä¾‹å¦‚è¿œç¨‹æœºå™¨äººæ§åˆ¶ã€äº‘æ¸¸æˆã€å¢å¼ºç°å®/è™šæ‹Ÿç°å®ï¼ˆAR/VRï¼‰åº”ç”¨ã€ä»¥åŠæ™ºèƒ½äº¤é€šç³»ç»Ÿã€‚é€šè¿‡é™ä½å¸¦å®½éœ€æ±‚å’Œæé«˜é€šä¿¡æ•ˆç‡ï¼ŒUniMICèƒ½å¤Ÿä¿ƒè¿›è¾¹ç¼˜è®¾å¤‡ä¸äº‘ç«¯AIä»£ç†ä¹‹é—´çš„æ— ç¼äº¤äº’ï¼Œä¸ºç”¨æˆ·æä¾›æ›´æµç•…ã€æ›´è‡ªç„¶çš„ä½“éªŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The rapid progress of Large Multimodal Models (LMMs) and cloud-based AI agents is transforming human-AI collaboration into bidirectional, multimodal interaction. However, existing codecs remain optimized for unimodal, one-way communication, resulting in repeated degradation under conventional compress-transmit-reconstruct pipelines. To address this limitation, we propose UniMIC, a Unified token-based Multimodal Interactive Coding framework that bridges edge devices and cloud AI agents. Instead of transmitting raw pixels or plain text, UniMIC employs compact tokenized representations as the communication medium, enabling efficient low-bitrate transmission while maintaining compatibility with LMMs. To further enhance compression, lightweight Transformer-based entropy models with scenario-specific designs-generic, masked, and text-conditioned-effectively minimize inter-token redundancy. Extensive experiments on text-to-image generation, text-guided inpainting, outpainting, and visual question answering show that UniMIC achieves substantial bitrate savings and remains robust even at ultra-low bitrates (<0.05bpp), without compromising downstream task performance. These results establish UniMIC as a practical and forward-looking paradigm for next-generation multimodal interactive communication.

