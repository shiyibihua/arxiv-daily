---
layout: default
title: You Can't Steal Nothing: Mitigating Prompt Leakages in LLMs via System Vectors
---

# You Can't Steal Nothing: Mitigating Prompt Leakages in LLMs via System Vectors

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.21884" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.21884v1</a>
  <a href="https://arxiv.org/pdf/2509.21884.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.21884v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.21884v1', 'You Can\'t Steal Nothing: Mitigating Prompt Leakages in LLMs via System Vectors')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Bochuan Cao, Changjiang Li, Yuanpu Cao, Yameng Ge, Ting Wang, Jinghui Chen

**åˆ†ç±»**: cs.CR, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26

**å¤‡æ³¨**: 29 pages, 10 tables, 6figures, accepted by CCS 25

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSysVecï¼Œé€šè¿‡ç³»ç»Ÿå‘é‡ç¼–ç ç¼“è§£å¤§è¯­è¨€æ¨¡å‹ä¸­çš„æç¤ºè¯æ³„éœ²é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `æç¤ºè¯æ³„éœ²` `ç³»ç»Ÿæç¤ºè¯` `å‘é‡ç¼–ç ` `å®‰å…¨` `æŒ‡ä»¤éµå¾ª` `é•¿æ–‡æœ¬é—å¿˜`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§è¯­è¨€æ¨¡å‹å®¹æ˜“é­å—æç¤ºè¯æ³„éœ²æ”»å‡»ï¼Œæ”»å‡»è€…å¯ä»¥é€šè¿‡æ„é€ ç‰¹å®šè¾“å…¥ï¼Œä»è€Œè·å–æ¨¡å‹çš„ç³»ç»Ÿæç¤ºè¯ï¼Œå¨èƒæ¨¡å‹å®‰å…¨ã€‚
2. è®ºæ–‡æå‡ºSysVecæ–¹æ³•ï¼Œå°†ç³»ç»Ÿæç¤ºè¯ç¼–ç ä¸ºå†…éƒ¨å‘é‡è¡¨ç¤ºï¼Œè€Œéç›´æ¥ä½¿ç”¨æ–‡æœ¬ï¼Œä»è€Œé¿å…æç¤ºè¯ç›´æ¥æš´éœ²åœ¨ä¸Šä¸‹æ–‡ä¸­ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒSysVecæœ‰æ•ˆç¼“è§£äº†æç¤ºè¯æ³„éœ²æ”»å‡»ï¼ŒåŒæ—¶ä¿æŒäº†æ¨¡å‹çš„åŠŸèƒ½å®Œæ•´æ€§ï¼Œå¹¶æ”¹å–„äº†é•¿æ–‡æœ¬åœºæ™¯ä¸‹çš„é—å¿˜é—®é¢˜ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å·²è¢«å¹¿æ³›åº”ç”¨äºå„ç§åº”ç”¨ä¸­ï¼Œåˆ©ç”¨å®šåˆ¶çš„ç³»ç»Ÿæç¤ºè¯æ¥æ‰§è¡Œä¸åŒçš„ä»»åŠ¡ã€‚é¢å¯¹æ½œåœ¨çš„ç³»ç»Ÿæç¤ºè¯æ³„éœ²é£é™©ï¼Œæ¨¡å‹å¼€å‘è€…å·²ç»å®æ–½äº†ä¸€äº›ç­–ç•¥æ¥é˜²æ­¢æ³„éœ²ï¼Œä¸»è¦æ˜¯é€šè¿‡ç¦æ­¢LLMsåœ¨é‡åˆ°å·²çŸ¥çš„æ”»å‡»æ¨¡å¼æ—¶é‡å¤å…¶ä¸Šä¸‹æ–‡ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä»ç„¶å®¹æ˜“å—åˆ°æ–°çš„å’Œæœªé¢„è§çš„æç¤ºè¯æ³„éœ²æŠ€æœ¯çš„æ”»å‡»ã€‚æœ¬æ–‡é¦–å…ˆä»‹ç»äº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æç¤ºè¯æ³„éœ²æ”»å‡»ï¼Œä»¥æ­ç¤ºæ­¤ç±»é£é™©ã€‚æˆ‘ä»¬çš„æ”»å‡»èƒ½å¤Ÿä»å„ç§åŸºäºLLMçš„åº”ç”¨ä¸­æå–ç³»ç»Ÿæç¤ºè¯ï¼Œç”šè‡³åŒ…æ‹¬GPT-4oæˆ–Claude 3.5 Sonnetç­‰SOTA LLMæ¨¡å‹ã€‚æˆ‘ä»¬çš„å‘ç°è¿›ä¸€æ­¥å¯å‘æˆ‘ä»¬é€šè¿‡åœ¨ä¸Šä¸‹æ–‡ä¸­ä¸åŒ…å«ç³»ç»Ÿæç¤ºè¯æ¥å¯»æ‰¾è§£å†³é—®é¢˜çš„æ ¹æœ¬æ–¹æ¡ˆã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºSysVecï¼Œä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œå°†ç³»ç»Ÿæç¤ºè¯ç¼–ç ä¸ºå†…éƒ¨è¡¨ç¤ºå‘é‡ï¼Œè€Œä¸æ˜¯åŸå§‹æ–‡æœ¬ã€‚é€šè¿‡è¿™æ ·åšï¼ŒSysVecæœ€å¤§é™åº¦åœ°é™ä½äº†æœªç»æˆæƒçš„æ³„éœ²é£é™©ï¼ŒåŒæ—¶ä¿ç•™äº†LLMçš„æ ¸å¿ƒè¯­è¨€èƒ½åŠ›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™ç§æ–¹æ³•ä¸ä»…å¢å¼ºäº†å®‰å…¨æ€§ï¼Œè¿˜æé«˜äº†æ¨¡å‹çš„ä¸€èˆ¬æŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSysVecæœ‰æ•ˆåœ°ç¼“è§£äº†æç¤ºè¯æ³„éœ²æ”»å‡»ï¼Œä¿æŒäº†LLMçš„åŠŸèƒ½å®Œæ•´æ€§ï¼Œå¹¶æœ‰åŠ©äºç¼“è§£é•¿ä¸Šä¸‹æ–‡åœºæ™¯ä¸­çš„é—å¿˜é—®é¢˜ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ä¸­ç³»ç»Ÿæç¤ºè¯æ³„éœ²çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é€šè¿‡æ£€æµ‹å·²çŸ¥çš„æ”»å‡»æ¨¡å¼æ¥é˜»æ­¢æ³„éœ²ï¼Œä½†æ— æ³•é˜²å¾¡æ–°å‹æ”»å‡»ï¼Œä¸”ä¾èµ–äºå¯¹ä¸Šä¸‹æ–‡çš„è¿‡æ»¤ï¼Œå¯èƒ½å½±å“æ¨¡å‹æ€§èƒ½ã€‚ç›´æ¥å°†ç³»ç»Ÿæç¤ºè¯ä»¥æ–‡æœ¬å½¢å¼åŒ…å«åœ¨ä¸Šä¸‹æ–‡ä¸­ï¼Œå­˜åœ¨è¢«æ¶æ„ç”¨æˆ·æå–çš„é£é™©ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†ç³»ç»Ÿæç¤ºè¯ç¼–ç æˆä¸€ä¸ªå‘é‡è¡¨ç¤ºï¼ˆSystem Vectorï¼‰ï¼Œå¹¶å°†å…¶ä½œä¸ºæ¨¡å‹çš„å†…éƒ¨çŠ¶æ€ã€‚è¿™æ ·ï¼Œç³»ç»Ÿæç¤ºè¯ä¸å†ä»¥æ–‡æœ¬å½¢å¼å­˜åœ¨äºä¸Šä¸‹æ–‡ä¸­ï¼Œä»è€Œé¿å…äº†ç›´æ¥æ³„éœ²çš„é£é™©ã€‚åŒæ—¶ï¼Œæ¨¡å‹å¯ä»¥é€šè¿‡è¿™ä¸ªå‘é‡æ¥ç†è§£å’Œæ‰§è¡Œç³»ç»ŸæŒ‡ä»¤ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSysVecæ–¹æ³•ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) **ç³»ç»Ÿæç¤ºè¯ç¼–ç **ï¼šä½¿ç”¨ä¸€ä¸ªç¼–ç å™¨ï¼ˆä¾‹å¦‚ï¼Œä¸€ä¸ªå°å‹Transformeræ¨¡å‹ï¼‰å°†ç³»ç»Ÿæç¤ºè¯ç¼–ç æˆä¸€ä¸ªå‘é‡è¡¨ç¤ºã€‚2) **å‘é‡æ³¨å…¥**ï¼šå°†ç¼–ç åçš„å‘é‡æ³¨å…¥åˆ°LLMçš„å†…éƒ¨çŠ¶æ€ä¸­ã€‚å…·ä½“å®ç°æ–¹å¼æœªçŸ¥ï¼Œå¯èƒ½é€šè¿‡ä¿®æ”¹LLMçš„æ³¨æ„åŠ›æœºåˆ¶æˆ–ç›´æ¥ä¿®æ”¹éšè—å±‚çŠ¶æ€ã€‚3) **æ¨ç†**ï¼šLLMåœ¨æ¨ç†æ—¶ï¼Œåˆ©ç”¨æ³¨å…¥çš„ç³»ç»Ÿå‘é‡æ¥æŒ‡å¯¼ç”Ÿæˆè¿‡ç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå°†ç³»ç»Ÿæç¤ºè¯ä»æ–‡æœ¬å½¢å¼è½¬æ¢ä¸ºå‘é‡å½¢å¼ï¼Œä»è€Œä»æ ¹æœ¬ä¸Šé¿å…äº†æç¤ºè¯æ³„éœ²çš„é£é™©ã€‚è¿™ç§æ–¹æ³•ä¸éœ€è¦ä¾èµ–äºå¯¹ä¸Šä¸‹æ–‡çš„è¿‡æ»¤æˆ–æ£€æµ‹ï¼Œå› æ­¤æ›´åŠ å®‰å…¨å’Œçµæ´»ã€‚æ­¤å¤–ï¼ŒSysVecè¿˜å¯ä»¥æé«˜æ¨¡å‹çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›å’Œç¼“è§£é•¿æ–‡æœ¬é—å¿˜é—®é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­å…³äºç¼–ç å™¨å’Œå‘é‡æ³¨å…¥çš„å…·ä½“æŠ€æœ¯ç»†èŠ‚æè¿°è¾ƒå°‘ï¼Œä¾‹å¦‚ï¼šç¼–ç å™¨çš„å…·ä½“ç»“æ„ã€è®­ç»ƒæ–¹å¼ï¼Œä»¥åŠå‘é‡å¦‚ä½•æ³¨å…¥åˆ°LLMçš„å†…éƒ¨çŠ¶æ€ã€‚è¿™äº›ç»†èŠ‚å¯¹äºSysVecçš„å®é™…åº”ç”¨è‡³å…³é‡è¦ï¼Œä½†ç›®å‰æœªçŸ¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡é€šè¿‡å®éªŒè¯æ˜ï¼ŒSysVecèƒ½å¤Ÿæœ‰æ•ˆç¼“è§£æç¤ºè¯æ³„éœ²æ”»å‡»ï¼Œå³ä½¿é¢å¯¹GPT-4oå’ŒClaude 3.5 Sonnetç­‰å…ˆè¿›æ¨¡å‹ä¹Ÿèƒ½æˆåŠŸé˜²å¾¡ã€‚åŒæ—¶ï¼ŒSysVecåœ¨ä¿æŒæ¨¡å‹åŠŸèƒ½å®Œæ•´æ€§çš„å‰æä¸‹ï¼Œè¿˜æå‡äº†æ¨¡å‹çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›ï¼Œå¹¶ç¼“è§£äº†é•¿æ–‡æœ¬åœºæ™¯ä¸‹çš„é—å¿˜é—®é¢˜ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œæå‡å¹…åº¦æœªçŸ¥ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SysVecæ–¹æ³•å¯ä»¥åº”ç”¨äºå„ç§éœ€è¦å®šåˆ¶ç³»ç»Ÿæç¤ºè¯çš„å¤§è¯­è¨€æ¨¡å‹åº”ç”¨åœºæ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½åŠ©æ‰‹ã€å¯¹è¯æœºå™¨äººã€å†…å®¹ç”Ÿæˆç­‰ã€‚é€šè¿‡ä½¿ç”¨SysVecï¼Œå¯ä»¥æœ‰æ•ˆä¿æŠ¤ç³»ç»Ÿæç¤ºè¯ä¸è¢«æ³„éœ²ï¼Œæé«˜åº”ç”¨çš„å®‰å…¨æ€§ã€‚æ­¤å¤–ï¼ŒSysVecè¿˜æœ‰åŠ©äºæé«˜æ¨¡å‹çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›å’Œç¼“è§£é•¿æ–‡æœ¬é—å¿˜é—®é¢˜ï¼Œä»è€Œæå‡ç”¨æˆ·ä½“éªŒã€‚è¯¥æ–¹æ³•å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå°¤å…¶æ˜¯åœ¨å¯¹å®‰å…¨æ€§è¦æ±‚è¾ƒé«˜çš„åœºæ™¯ä¸‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have been widely adopted across various applications, leveraging customized system prompts for diverse tasks. Facing potential system prompt leakage risks, model developers have implemented strategies to prevent leakage, primarily by disabling LLMs from repeating their context when encountering known attack patterns. However, it remains vulnerable to new and unforeseen prompt-leaking techniques. In this paper, we first introduce a simple yet effective prompt leaking attack to reveal such risks. Our attack is capable of extracting system prompts from various LLM-based application, even from SOTA LLM models such as GPT-4o or Claude 3.5 Sonnet. Our findings further inspire us to search for a fundamental solution to the problems by having no system prompt in the context. To this end, we propose SysVec, a novel method that encodes system prompts as internal representation vectors rather than raw text. By doing so, SysVec minimizes the risk of unauthorized disclosure while preserving the LLM's core language capabilities. Remarkably, this approach not only enhances security but also improves the model's general instruction-following abilities. Experimental results demonstrate that SysVec effectively mitigates prompt leakage attacks, preserves the LLM's functional integrity, and helps alleviate the forgetting issue in long-context scenarios.

