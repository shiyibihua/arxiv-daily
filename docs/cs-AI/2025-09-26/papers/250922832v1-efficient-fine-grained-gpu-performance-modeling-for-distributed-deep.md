---
layout: default
title: Efficient Fine-Grained GPU Performance Modeling for Distributed Deep Learning of LLM
---

# Efficient Fine-Grained GPU Performance Modeling for Distributed Deep Learning of LLM

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.22832" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.22832v1</a>
  <a href="https://arxiv.org/pdf/2509.22832.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.22832v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.22832v1', 'Efficient Fine-Grained GPU Performance Modeling for Distributed Deep Learning of LLM')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Biyao Zhang, Mingkai Zheng, Debargha Ganguly, Xuecen Zhang, Vikash Singh, Vipin Chaudhary, Zhao Zhang

**åˆ†ç±»**: cs.DC, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§é«˜æ•ˆç»†ç²’åº¦çš„GPUæ€§èƒ½å»ºæ¨¡æ–¹æ³•ï¼Œç”¨äºé¢„æµ‹LLMåˆ†å¸ƒå¼è®­ç»ƒæ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `åˆ†å¸ƒå¼è®­ç»ƒ` `æ€§èƒ½å»ºæ¨¡` `GPUæ€§èƒ½é¢„æµ‹` `ç¡¬ä»¶æ„ŸçŸ¥` `ç®—å­çº§åˆ†è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨é¢„æµ‹LLMåˆ†å¸ƒå¼è®­ç»ƒæ—¶é—´æ—¶ï¼Œé¢ä¸´ç€é‡‡æ ·æˆæœ¬é«˜æ˜‚å’Œéš¾ä»¥å¤„ç†å®é™…ç½‘ç»œç¡¬ä»¶å¤æ‚æ€§çš„æŒ‘æˆ˜ã€‚
2. è¯¥è®ºæ–‡æå‡ºä¸€ç§åŸºäºç®—å­çº§åˆ†è§£å’Œè½»é‡çº§é‡‡æ ·çš„ç¡¬ä»¶æ„ŸçŸ¥é¢„æµ‹æ¨¡å‹ï¼Œç”¨äºå‡†ç¡®é¢„æµ‹LLMçš„è®­ç»ƒæ€§èƒ½ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨Perlmutterå’ŒVistaç³»ç»Ÿä¸Šï¼Œå¯¹é«˜è¾¾200äº¿å‚æ•°çš„æ¨¡å‹ï¼Œé¢„æµ‹è¯¯å·®åˆ†åˆ«ä½è‡³4.98%å’Œ9.38%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¯é«˜æ€§èƒ½è®¡ç®—ä¸­æœ€æ¶ˆè€—è®¡ç®—èµ„æºçš„ä»»åŠ¡ä¹‹ä¸€ã€‚ç”±äºTransformerç»„ä»¶ã€å¹¶è¡Œç­–ç•¥ï¼ˆæ•°æ®ã€æ¨¡å‹ã€æµæ°´çº¿ã€å¼ é‡ï¼‰å’Œå¤šå±‚é€šä¿¡ä¹‹é—´å¤æ‚çš„äº¤äº’ï¼Œé¢„æµ‹è·¨æ•°ç™¾ä¸ªGPUåˆ†å¸ƒçš„æ•°åäº¿å‚æ•°æ¨¡å‹çš„ç«¯åˆ°ç«¯è®­ç»ƒæ—¶é—´ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚å­¦ä¹ æ¨¡å‹éœ€è¦æ˜‚è´µçš„é‡‡æ ·ï¼Œè€Œåˆ†ææ¨¡å‹é€šå¸¸éš¾ä»¥åº”å¯¹å®é™…çš„ç½‘ç»œå’Œç¡¬ä»¶å¤æ‚æ€§ã€‚æˆ‘ä»¬é€šè¿‡å°†LLMåˆ†è§£ä¸ºæ ¸å¿ƒè®¡ç®—åŸè¯­å¹¶å¯¹å…¶è¿›è¡Œå»ºæ¨¡æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼šï¼ˆ1ï¼‰ç”¨äºç»†ç²’åº¦åˆ†æçš„ç®—å­çº§åˆ†è§£ï¼›ï¼ˆ2ï¼‰åŸºäºè½»é‡çº§é‡‡æ ·çš„ç¡¬ä»¶æ„ŸçŸ¥é¢„æµ‹æ¨¡å‹ï¼Œç”¨äºå…³é”®æ“ä½œï¼›ï¼ˆ3ï¼‰ä¸€ä¸ªç«¯åˆ°ç«¯é¢„æµ‹ç³»ç»Ÿï¼Œå°†è¿™äº›ç»„ä»¶é›†æˆåˆ°å¤æ‚çš„å¹¶è¡ŒåŒ–ç­–ç•¥ä¸­ã€‚è‡³å…³é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å·²ç»åœ¨ä¸¤ä¸ªå¤§å‹HPCç³»ç»Ÿä¸Šå¾—åˆ°äº†éªŒè¯ã€‚æˆ‘ä»¬çš„æ¡†æ¶åœ¨Perlmutter(A100)ä¸Šå®ç°äº†4.98%çš„ä½å¹³å‡é¢„æµ‹è¯¯å·®ï¼Œåœ¨Vista(GH200)ä¸Šå®ç°äº†9.38%çš„ä½å¹³å‡é¢„æµ‹è¯¯å·®ï¼Œé€‚ç”¨äºé«˜è¾¾200äº¿å‚æ•°çš„æ¨¡å‹ï¼Œè·¨è¶Š128ä¸ªGPUã€‚é‡è¦çš„æ˜¯ï¼Œå®ƒå®Œå…¨åœ¨CPUä¸Šè¿è¡Œï¼Œæ— éœ€æ˜‚è´µçš„é›†ç¾¤å®éªŒï¼Œå³å¯å¿«é€Ÿè¿­ä»£ç¡¬ä»¶é…ç½®å’Œè®­ç»ƒç­–ç•¥ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸‹è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ—¶ï¼Œå‡†ç¡®é¢„æµ‹è®­ç»ƒæ—¶é—´çš„é—®é¢˜ã€‚ç°æœ‰çš„æ–¹æ³•ï¼Œå¦‚åŸºäºå­¦ä¹ çš„æ¨¡å‹ï¼Œéœ€è¦å¤§é‡çš„é‡‡æ ·æ•°æ®ï¼Œæˆæœ¬é«˜æ˜‚ï¼›è€Œåˆ†ææ¨¡å‹åˆ™éš¾ä»¥æ•æ‰å®é™…ç½‘ç»œå’Œç¡¬ä»¶çš„å¤æ‚æ€§ï¼Œå¯¼è‡´é¢„æµ‹ç²¾åº¦ä¸è¶³ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§é«˜æ•ˆä¸”å‡†ç¡®çš„æ€§èƒ½é¢„æµ‹æ–¹æ³•ï¼Œä»¥ä¾¿åœ¨å®é™…è®­ç»ƒå‰è¯„ä¼°ä¸åŒç¡¬ä»¶é…ç½®å’Œå¹¶è¡Œç­–ç•¥çš„æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†LLMçš„è®­ç»ƒè¿‡ç¨‹åˆ†è§£ä¸ºä¸€ç³»åˆ—æ ¸å¿ƒè®¡ç®—åŸè¯­ï¼ˆç®—å­ï¼‰ï¼Œç„¶åé’ˆå¯¹è¿™äº›ç®—å­æ„å»ºç¡¬ä»¶æ„ŸçŸ¥çš„æ€§èƒ½é¢„æµ‹æ¨¡å‹ã€‚é€šè¿‡å¯¹å°‘é‡æ ·æœ¬è¿›è¡Œé‡‡æ ·ï¼Œå­¦ä¹ ç®—å­çš„æ€§èƒ½ç‰¹å¾ï¼Œå¹¶å°†å…¶é›†æˆåˆ°ä¸€ä¸ªç«¯åˆ°ç«¯çš„é¢„æµ‹ç³»ç»Ÿä¸­ã€‚è¿™ç§æ–¹æ³•é¿å…äº†å¯¹æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹è¿›è¡Œæ˜‚è´µçš„é‡‡æ ·ï¼ŒåŒæ—¶èƒ½å¤Ÿæ•æ‰ç¡¬ä»¶çš„ç‰¹æ€§ï¼Œä»è€Œæé«˜é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥è®ºæ–‡æå‡ºçš„æ€§èƒ½é¢„æµ‹æ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š
1. **ç®—å­çº§åˆ†è§£**ï¼šå°†LLMçš„è®­ç»ƒè¿‡ç¨‹åˆ†è§£ä¸ºä¸€ç³»åˆ—åŸºæœ¬çš„è®¡ç®—ç®—å­ï¼Œä¾‹å¦‚çŸ©é˜µä¹˜æ³•ã€æ¿€æ´»å‡½æ•°ç­‰ã€‚
2. **è½»é‡çº§é‡‡æ ·**ï¼šé’ˆå¯¹æ¯ä¸ªç®—å­ï¼Œåœ¨ä¸åŒçš„ç¡¬ä»¶é…ç½®ä¸‹è¿›è¡Œå°‘é‡é‡‡æ ·ï¼Œæ”¶é›†æ€§èƒ½æ•°æ®ã€‚
3. **ç¡¬ä»¶æ„ŸçŸ¥é¢„æµ‹æ¨¡å‹**ï¼šåŸºäºé‡‡æ ·æ•°æ®ï¼Œæ„å»ºç¡¬ä»¶æ„ŸçŸ¥çš„é¢„æµ‹æ¨¡å‹ï¼Œç”¨äºé¢„æµ‹æ¯ä¸ªç®—å­çš„æ‰§è¡Œæ—¶é—´ã€‚
4. **ç«¯åˆ°ç«¯é¢„æµ‹ç³»ç»Ÿ**ï¼šå°†å„ä¸ªç®—å­çš„é¢„æµ‹ç»“æœé›†æˆèµ·æ¥ï¼Œé¢„æµ‹æ•´ä¸ªLLMè®­ç»ƒè¿‡ç¨‹çš„ç«¯åˆ°ç«¯æ—¶é—´ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š
1. **ç»†ç²’åº¦çš„ç®—å­çº§åˆ†è§£**ï¼šé€šè¿‡å°†LLMåˆ†è§£ä¸ºæ›´å°çš„è®¡ç®—å•å…ƒï¼Œå¯ä»¥æ›´ç²¾ç¡®åœ°å»ºæ¨¡ç¡¬ä»¶å¯¹ä¸åŒç®—å­çš„å½±å“ã€‚
2. **è½»é‡çº§é‡‡æ ·æ–¹æ³•**ï¼šé€šè¿‡å°‘é‡é‡‡æ ·å³å¯æ„å»ºå‡†ç¡®çš„é¢„æµ‹æ¨¡å‹ï¼Œå¤§å¤§é™ä½äº†é‡‡æ ·æˆæœ¬ã€‚
3. **ç¡¬ä»¶æ„ŸçŸ¥çš„é¢„æµ‹æ¨¡å‹**ï¼šè€ƒè™‘äº†ç¡¬ä»¶çš„ç‰¹æ€§ï¼Œæé«˜äº†é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­å…³é”®çš„è®¾è®¡åŒ…æ‹¬ï¼š
1. **ç®—å­çš„é€‰æ‹©**ï¼šé€‰æ‹©å“ªäº›ç®—å­è¿›è¡Œå»ºæ¨¡ï¼Œéœ€è¦æƒè¡¡å»ºæ¨¡çš„å¤æ‚åº¦å’Œé¢„æµ‹çš„å‡†ç¡®æ€§ã€‚
2. **é‡‡æ ·ç­–ç•¥**ï¼šå¦‚ä½•é€‰æ‹©é‡‡æ ·ç‚¹ï¼Œä»¥ä¿è¯é‡‡æ ·æ•°æ®çš„ä»£è¡¨æ€§ã€‚
3. **é¢„æµ‹æ¨¡å‹çš„é€‰æ‹©**ï¼šé€‰æ‹©åˆé€‚çš„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œç”¨äºé¢„æµ‹ç®—å­çš„æ‰§è¡Œæ—¶é—´ã€‚è®ºæ–‡ä¸­ä½¿ç”¨äº†ç¡¬ä»¶æ„ŸçŸ¥çš„æ¨¡å‹ï¼Œå…·ä½“æ¨¡å‹ç»†èŠ‚æœªçŸ¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥è®ºæ–‡æå‡ºçš„æ¡†æ¶åœ¨Perlmutter(A100)å’ŒVista(GH200)ä¸¤ä¸ªå¤§è§„æ¨¡HPCç³»ç»Ÿä¸Šè¿›è¡Œäº†éªŒè¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¯¹äºé«˜è¾¾200äº¿å‚æ•°çš„æ¨¡å‹ï¼Œè¯¥æ¡†æ¶åœ¨Perlmutterä¸Šçš„å¹³å‡é¢„æµ‹è¯¯å·®ä¸º4.98%ï¼Œåœ¨Vistaä¸Šçš„å¹³å‡é¢„æµ‹è¯¯å·®ä¸º9.38%ã€‚é‡è¦çš„æ˜¯ï¼Œè¯¥æ¡†æ¶å®Œå…¨åœ¨CPUä¸Šè¿è¡Œï¼Œæ— éœ€æ˜‚è´µçš„GPUèµ„æºã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå¤§è§„æ¨¡åˆ†å¸ƒå¼æ·±åº¦å­¦ä¹ ç³»ç»Ÿçš„æ€§èƒ½ä¼˜åŒ–å’Œèµ„æºè°ƒåº¦ã€‚é€šè¿‡å‡†ç¡®é¢„æµ‹ä¸åŒç¡¬ä»¶é…ç½®å’Œè®­ç»ƒç­–ç•¥ä¸‹çš„æ€§èƒ½ï¼Œå¯ä»¥å¸®åŠ©ç ”ç©¶äººå‘˜å’Œå·¥ç¨‹å¸ˆé€‰æ‹©æœ€ä¼˜çš„é…ç½®ï¼Œæé«˜è®­ç»ƒæ•ˆç‡ï¼Œé™ä½è®­ç»ƒæˆæœ¬ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºæŒ‡å¯¼ç¡¬ä»¶è®¾è®¡ï¼Œä¼˜åŒ–ç¡¬ä»¶æ¶æ„ä»¥æ›´å¥½åœ°æ”¯æŒLLMçš„è®­ç»ƒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Training Large Language Models(LLMs) is one of the most compute-intensive tasks in high-performance computing. Predicting end-to-end training time for multi-billion parameter models distributed across hundreds of GPUs remains challenging due to complex interactions between transformer components, parallelism strategies(data, model, pipeline, tensor), and multi-tier communication. Learned models require costly sampling, while analytical models often struggle with real-world network and hardware complexities. We address this by decomposing LLMs into core computational primitives and modeling them with: (1) operator-level decomposition for fine-grained analysis; (2) lightweight sampling based hardware-aware prediction models for key operations; (3) an end-to-end prediction system integrating these components across complex parallelization strategies. Crucially, our methodology has been validated on two large-scale HPC systems. Our framework achieves low average prediction errors-4.98\% on Perlmutter(A100) and 9.38\% on Vista(GH200)-for models up to 20B parameters across 128 GPUs. Importantly, it runs entirely on CPUs, enabling rapid iteration over hardware configurations and training strategies without costly on-cluster experimentation.

