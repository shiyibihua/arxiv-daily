---
layout: default
title: Toward a Theory of Generalizability in LLM Mechanistic Interpretability Research
---

# Toward a Theory of Generalizability in LLM Mechanistic Interpretability Research

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.22831" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.22831v1</a>
  <a href="https://arxiv.org/pdf/2509.22831.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.22831v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.22831v1', 'Toward a Theory of Generalizability in LLM Mechanistic Interpretability Research')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sean Trott

**åˆ†ç±»**: cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLLMå¯è§£é‡Šæ€§ç ”ç©¶ä¸­çš„æ³›åŒ–æ€§ç†è®ºæ¡†æ¶ï¼Œå¹¶éªŒè¯1-backæ³¨æ„åŠ›å¤´çš„æ³›åŒ–èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `LLMå¯è§£é‡Šæ€§` `æ³›åŒ–æ€§` `æœºåˆ¶è§£é‡Š` `æ³¨æ„åŠ›å¤´` `é¢„è®­ç»ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å½“å‰LLMå¯è§£é‡Šæ€§ç ”ç©¶ç¼ºä¹æ˜ç¡®çš„æ³›åŒ–æ€§åŸåˆ™ï¼Œéš¾ä»¥ç¡®å®šä»ä¸€ä¸ªæ¨¡å‹è·å¾—çš„ç»“è®ºèƒ½å¦æ¨å¹¿åˆ°å…¶ä»–æ¨¡å‹ã€‚
2. è®ºæ–‡æå‡ºäº”ä¸ªæ³›åŒ–è½´ï¼šåŠŸèƒ½æ€§ã€å‘å±•æ€§ã€ä½ç½®æ€§ã€å…³ç³»æ€§å’Œé…ç½®æ€§ï¼Œç”¨äºè¯„ä¼°æœºåˆ¶æ€§å£°æ˜åœ¨ä¸åŒLLMé—´çš„æ³›åŒ–èƒ½åŠ›ã€‚
3. é€šè¿‡å¯¹Pythiaæ¨¡å‹ä¸­1-backæ³¨æ„åŠ›å¤´çš„åˆ†æï¼ŒéªŒè¯äº†å‘å±•æ€§æ³›åŒ–å…·æœ‰è¾ƒå¼ºä¸€è‡´æ€§ï¼Œè€Œä½ç½®æ€§æ³›åŒ–åˆ™ç›¸å¯¹æœ‰é™ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç ”ç©¶è¶Šæ¥è¶Šå…³æ³¨äºè¯†åˆ«å…¶è¡Œä¸ºçš„æœºåˆ¶æ€§è§£é‡Šï¼Œä½†è¯¥é¢†åŸŸç¼ºä¹æ˜ç¡®çš„åŸåˆ™æ¥ç¡®å®šä»ä¸€ä¸ªæ¨¡å‹å®ä¾‹ä¸­è·å¾—çš„å‘ç°ä½•æ—¶ï¼ˆä»¥åŠå¦‚ä½•ï¼‰æ¨å¹¿åˆ°å¦ä¸€ä¸ªæ¨¡å‹ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³ä¸€ä¸ªæ ¹æœ¬æ€§çš„è®¤è¯†è®ºæŒ‘æˆ˜ï¼šç»™å®šå…³äºç‰¹å®šæ¨¡å‹çš„æœºåˆ¶æ€§å£°æ˜ï¼Œæœ‰ä»€ä¹ˆç†ç”±å°†è¿™ä¸€å‘ç°å¤–æ¨åˆ°å…¶ä»–LLMï¼Ÿä»¥åŠè¿™ç§æ³›åŒ–å¯èƒ½æ²¿ç€å“ªäº›ç»´åº¦è¿›è¡Œï¼Ÿæˆ‘æå‡ºäº†äº”ä¸ªæ½œåœ¨çš„å¯¹åº”è½´ï¼Œæœºåˆ¶æ€§å£°æ˜å¯èƒ½æ²¿ç€è¿™äº›è½´æ³›åŒ–ï¼ŒåŒ…æ‹¬ï¼šåŠŸèƒ½æ€§ï¼ˆæ˜¯å¦æ»¡è¶³ç›¸åŒçš„åŠŸèƒ½æ ‡å‡†ï¼‰ï¼Œå‘å±•æ€§ï¼ˆæ˜¯å¦åœ¨é¢„è®­ç»ƒçš„ç›¸ä¼¼æ—¶é—´ç‚¹å‘å±•ï¼‰ï¼Œä½ç½®æ€§ï¼ˆæ˜¯å¦å æ®ç›¸ä¼¼çš„ç»å¯¹æˆ–ç›¸å¯¹ä½ç½®ï¼‰ï¼Œå…³ç³»æ€§ï¼ˆæ˜¯å¦ä»¥ç±»ä¼¼çš„æ–¹å¼ä¸å…¶ä»–æ¨¡å‹ç»„ä»¶äº¤äº’ï¼‰å’Œé…ç½®æ€§ï¼ˆæ˜¯å¦å¯¹åº”äºæƒé‡ç©ºé—´ä¸­çš„ç‰¹å®šåŒºåŸŸæˆ–ç»“æ„ï¼‰ã€‚ä¸ºäº†å®è¯éªŒè¯è¿™ä¸ªæ¡†æ¶ï¼Œæˆ‘åˆ†æäº†Pythiaæ¨¡å‹ï¼ˆ14Mï¼Œ70Mï¼Œ160Mï¼Œ410Mï¼‰çš„éšæœºç§å­åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­çš„â€œ1-backæ³¨æ„åŠ›å¤´â€ï¼ˆå…³æ³¨å…ˆå‰tokençš„ç»„ä»¶ï¼‰ã€‚ç»“æœè¡¨æ˜ï¼Œæ¨¡å‹é—´1-backæ³¨æ„åŠ›çš„å‘å±•è½¨è¿¹å…·æœ‰æ˜¾è‘—çš„ä¸€è‡´æ€§ï¼Œè€Œä½ç½®ä¸€è‡´æ€§åˆ™è¾ƒä¸ºæœ‰é™ã€‚æ­¤å¤–ï¼Œè¾ƒå¤§æ¨¡å‹çš„ç§å­ç³»ç»Ÿåœ°æ˜¾ç¤ºå‡º1-backæ³¨æ„åŠ›çš„æ›´æ—©å‡ºç°ã€æ›´é™¡å³­çš„æ–œç‡å’Œæ›´é«˜çš„å³°å€¼ã€‚æˆ‘è¿˜è®¨è®ºäº†å¯¹æœ¬æ–‡æå‡ºçš„è®ºç‚¹å’Œå»ºè®®çš„å¯èƒ½å¼‚è®®ã€‚æœ€åï¼Œæˆ‘æ€»ç»“è®¤ä¸ºï¼Œåœ¨æœºåˆ¶å¯è§£é‡Šæ€§ç ”ç©¶çš„æ³›åŒ–æ€§æ–¹é¢å–å¾—è¿›å±•å°†åŒ…æ‹¬å°†LLMçš„æ„æˆæ€§è®¾è®¡å±æ€§æ˜ å°„åˆ°å…¶æ¶Œç°è¡Œä¸ºå’Œæœºåˆ¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå½“å‰LLMå¯è§£é‡Šæ€§ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ç‰¹å®šæ¨¡å‹çš„æœºåˆ¶è§£é‡Šï¼Œç¼ºä¹ä¸€å¥—ç³»ç»Ÿæ€§çš„ç†è®ºæ¡†æ¶æ¥è¯„ä¼°è¿™äº›è§£é‡Šåœ¨ä¸åŒæ¨¡å‹é—´çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™å¯¼è‡´ç ”ç©¶ç»“æœéš¾ä»¥æ¨å¹¿ï¼Œé™åˆ¶äº†æˆ‘ä»¬å¯¹LLMé€šç”¨è¡Œä¸ºçš„ç†è§£ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹å¯¹æ³›åŒ–ç»´åº¦çš„æ˜ç¡®å®šä¹‰å’Œè¯„ä¼°æ ‡å‡†ï¼Œä½¿å¾—ç ”ç©¶ç»“è®ºçš„å¯é æ€§å’Œé€‚ç”¨èŒƒå›´éš¾ä»¥ç¡®å®šã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†LLMçš„æœºåˆ¶æ€§è§£é‡Šçš„æ³›åŒ–é—®é¢˜åˆ†è§£ä¸ºå¤šä¸ªå¯è¯„ä¼°çš„ç»´åº¦ï¼Œå¹¶æå‡ºäº”ä¸ªæ½œåœ¨çš„æ³›åŒ–è½´ï¼šåŠŸèƒ½æ€§ã€å‘å±•æ€§ã€ä½ç½®æ€§ã€å…³ç³»æ€§å’Œé…ç½®æ€§ã€‚é€šè¿‡åˆ†æè¿™äº›è½´ä¸Šçš„å¯¹åº”å…³ç³»ï¼Œå¯ä»¥æ›´ç³»ç»Ÿåœ°è¯„ä¼°ä¸€ä¸ªæ¨¡å‹ä¸­å‘ç°çš„æœºåˆ¶æ˜¯å¦èƒ½åœ¨å…¶ä»–æ¨¡å‹ä¸­æ‰¾åˆ°ç±»ä¼¼çš„å¯¹åº”ã€‚è¿™ç§åˆ†è§£æœ‰åŠ©äºæ›´ç²¾ç¡®åœ°ç†è§£æ³›åŒ–çš„è¾¹ç•Œå’Œæ¡ä»¶ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡é¦–å…ˆæå‡ºäº†äº”ä¸ªæ³›åŒ–è½´çš„æ¦‚å¿µæ¡†æ¶ã€‚ç„¶åï¼Œé€‰æ‹©1-backæ³¨æ„åŠ›å¤´ä½œä¸ºæ¡ˆä¾‹ç ”ç©¶å¯¹è±¡ï¼Œåœ¨ä¸åŒå¤§å°çš„Pythiaæ¨¡å‹ä¸Šè¿›è¡Œå®éªŒã€‚å®éªŒæµç¨‹åŒ…æ‹¬ï¼š1) è®­ç»ƒå¤šä¸ªéšæœºç§å­çš„Pythiaæ¨¡å‹ï¼›2) ç›‘æµ‹é¢„è®­ç»ƒè¿‡ç¨‹ä¸­1-backæ³¨æ„åŠ›å¤´çš„æ¿€æ´»æƒ…å†µï¼›3) åˆ†æä¸åŒæ¨¡å‹å’Œç§å­ä¹‹é—´1-backæ³¨æ„åŠ›å¤´çš„å‘å±•è½¨è¿¹ã€ä½ç½®å’ŒåŠŸèƒ½ï¼›4) è¯„ä¼°ä¸åŒæ³›åŒ–è½´ä¸Šçš„ä¸€è‡´æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡æœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ä¸ªå¤šç»´åº¦çš„æ³›åŒ–æ€§è¯„ä¼°æ¡†æ¶ï¼Œå°†LLMæœºåˆ¶æ€§è§£é‡Šçš„æ³›åŒ–é—®é¢˜åˆ†è§£ä¸ºäº”ä¸ªå¯æ“ä½œçš„è½´ã€‚è¿™ä¸ºæœªæ¥çš„å¯è§£é‡Šæ€§ç ”ç©¶æä¾›äº†ä¸€ä¸ªæ›´ç³»ç»Ÿã€æ›´ä¸¥è°¨çš„è¯„ä¼°æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜é€šè¿‡å®è¯ç ”ç©¶éªŒè¯äº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œå¹¶æ­ç¤ºäº†ä¸åŒæ³›åŒ–è½´ä¸Šçš„å·®å¼‚ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) é€‰æ‹©1-backæ³¨æ„åŠ›å¤´ä½œä¸ºæ¡ˆä¾‹ç ”ç©¶å¯¹è±¡ï¼Œå› ä¸ºå®ƒæ˜¯ä¸€ä¸ªç›¸å¯¹ç®€å•ä½†é‡è¦çš„æœºåˆ¶ï¼›2) ä½¿ç”¨Pythiaæ¨¡å‹ï¼Œå› ä¸ºå®ƒæ˜¯ä¸€ä¸ªå¼€æºçš„ã€æ˜“äºå¤ç°çš„æ¨¡å‹ç³»åˆ—ï¼›3) é‡‡ç”¨å¤šä¸ªéšæœºç§å­ï¼Œä»¥å‡å°‘å¶ç„¶å› ç´ çš„å½±å“ï¼›4) é‡‡ç”¨å®šé‡æŒ‡æ ‡æ¥è¯„ä¼°ä¸åŒæ³›åŒ–è½´ä¸Šçš„ä¸€è‡´æ€§ï¼Œä¾‹å¦‚ï¼Œä½¿ç”¨æ¿€æ´»å³°å€¼ã€å‡ºç°æ—¶é—´å’Œæ–œç‡æ¥è¡¡é‡å‘å±•æ€§æ³›åŒ–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œ1-backæ³¨æ„åŠ›å¤´åœ¨ä¸åŒå¤§å°çš„Pythiaæ¨¡å‹ä¸­å±•ç°å‡ºæ˜¾è‘—çš„å‘å±•æ€§ä¸€è‡´æ€§ï¼Œå³å®ƒä»¬åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­ä»¥ç›¸ä¼¼çš„æ¨¡å¼å‡ºç°å’Œæ¼”åŒ–ã€‚ç„¶è€Œï¼Œä½ç½®ä¸€è‡´æ€§ç›¸å¯¹è¾ƒå¼±ï¼Œè¡¨æ˜ç›¸åŒåŠŸèƒ½çš„æœºåˆ¶å¯èƒ½ä½äºæ¨¡å‹ä¸­çš„ä¸åŒä½ç½®ã€‚æ­¤å¤–ï¼Œè¾ƒå¤§æ¨¡å‹çš„ç§å­é€šå¸¸è¡¨ç°å‡ºæ›´æ—©çš„æ¿€æ´»ã€æ›´é™¡å³­çš„æ–œç‡å’Œæ›´é«˜çš„å³°å€¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæŒ‡å¯¼LLMçš„æ¶æ„è®¾è®¡å’Œè®­ç»ƒç­–ç•¥ï¼Œæå‡æ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡ç†è§£ä¸åŒæ¨¡å‹é—´æœºåˆ¶çš„å¯¹åº”å…³ç³»ï¼Œå¯ä»¥æ›´å¥½åœ°è¿ç§»çŸ¥è¯†å’Œä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶ä¹Ÿæœ‰åŠ©äºè¯„ä¼°ä¸åŒå¯è§£é‡Šæ€§æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œå¯é æ€§ï¼Œæ¨åŠ¨å¯è§£é‡Šæ€§ç ”ç©¶çš„æ ‡å‡†åŒ–å’Œè§„èŒƒåŒ–ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Research on Large Language Models (LLMs) increasingly focuses on identifying mechanistic explanations for their behaviors, yet the field lacks clear principles for determining when (and how) findings from one model instance generalize to another. This paper addresses a fundamental epistemological challenge: given a mechanistic claim about a particular model, what justifies extrapolating this finding to other LLMs -- and along which dimensions might such generalizations hold? I propose five potential axes of correspondence along which mechanistic claims might generalize, including: functional (whether they satisfy the same functional criteria), developmental (whether they develop at similar points during pretraining), positional (whether they occupy similar absolute or relative positions), relational (whether they interact with other model components in similar ways), and configurational (whether they correspond to particular regions or structures in weight-space). To empirically validate this framework, I analyze "1-back attention heads" (components attending to previous tokens) across pretraining in random seeds of the Pythia models (14M, 70M, 160M, 410M). The results reveal striking consistency in the developmental trajectories of 1-back attention across models, while positional consistency is more limited. Moreover, seeds of larger models systematically show earlier onsets, steeper slopes, and higher peaks of 1-back attention. I also address possible objections to the arguments and proposals outlined here. Finally, I conclude by arguing that progress on the generalizability of mechanistic interpretability research will consist in mapping constitutive design properties of LLMs to their emergent behaviors and mechanisms.

