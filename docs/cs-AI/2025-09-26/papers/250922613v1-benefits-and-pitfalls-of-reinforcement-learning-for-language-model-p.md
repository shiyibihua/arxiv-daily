---
layout: default
title: Benefits and Pitfalls of Reinforcement Learning for Language Model Planning: A Theoretical Perspective
---

# Benefits and Pitfalls of Reinforcement Learning for Language Model Planning: A Theoretical Perspective

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.22613" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.22613v1</a>
  <a href="https://arxiv.org/pdf/2509.22613.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.22613v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.22613v1', 'Benefits and Pitfalls of Reinforcement Learning for Language Model Planning: A Theoretical Perspective')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Siwei Wang, Yifei Shen, Haoran Sun, Shi Feng, Shang-Hua Teng, Li Dong, Yaru Hao, Wei Chen

**åˆ†ç±»**: cs.AI, cs.CL, cs.LG, stat.ML

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç†è®ºåˆ†æå¼ºåŒ–å­¦ä¹ åœ¨è¯­è¨€æ¨¡å‹è§„åˆ’ä¸­çš„ä¼˜åŠ£ï¼Œæ­ç¤ºæ¢ç´¢ä¸å¤šæ ·æ€§çš„é‡è¦æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `è¯­è¨€æ¨¡å‹` `è§„åˆ’` `ç­–ç•¥æ¢¯åº¦` `Q-learning` `æ¢ç´¢` `å¤šæ ·æ€§` `ç†è®ºåˆ†æ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç›‘ç£å¾®è°ƒæ–¹æ³•åœ¨è¯­è¨€æ¨¡å‹è§„åˆ’ä¸­æ˜“å—å…±ç°å…³ç³»å½±å“ï¼Œäº§ç”Ÿè™šå‡è§£ï¼Œæ³›åŒ–èƒ½åŠ›å—é™ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼Œç‰¹åˆ«æ˜¯ç­–ç•¥æ¢¯åº¦å’ŒQ-learningï¼Œåˆ©ç”¨æ¢ç´¢æœºåˆ¶æ¥å…‹æœç›‘ç£å­¦ä¹ çš„å±€é™æ€§ï¼Œå®ç°æ›´å‡†ç¡®çš„è§„åˆ’ã€‚
3. ç†è®ºåˆ†æå’Œå®éªŒç»“æœè¡¨æ˜ï¼Œå¼ºåŒ–å­¦ä¹ èƒ½æœ‰æ•ˆæå‡è§„åˆ’èƒ½åŠ›ï¼Œä½†ç­–ç•¥æ¢¯åº¦å­˜åœ¨å¤šæ ·æ€§å´©æºƒé—®é¢˜ï¼ŒQ-learningåœ¨å¤šæ ·æ€§ä¿æŒæ–¹é¢æ›´å…·ä¼˜åŠ¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ä»ç†è®ºè§’åº¦ç ”ç©¶äº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è§„åˆ’èƒ½åŠ›çš„æœ‰æ•ˆæ€§ï¼Œé‡ç‚¹å…³æ³¨ç­–ç•¥æ¢¯åº¦ï¼ˆPGï¼‰å’ŒQ-learningæ–¹æ³•ã€‚é€šè¿‡å¯å¤„ç†çš„åŸºäºå›¾çš„æŠ½è±¡ï¼Œåˆ†æè¡¨æ˜ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å¯èƒ½å¼•å…¥åŸºäºå…±ç°çš„è™šå‡è§£ï¼Œè€ŒRLä¸»è¦é€šè¿‡æ¢ç´¢å®ç°æ­£ç¡®çš„è§„åˆ’ï¼Œå¼ºè°ƒäº†æ¢ç´¢åœ¨å®ç°æ›´å¥½æ³›åŒ–ä¸­çš„ä½œç”¨ã€‚ç„¶è€Œï¼ŒPGå­˜åœ¨å¤šæ ·æ€§å´©æºƒé—®é¢˜ï¼Œå³ä½¿åœ¨è¾¾åˆ°å®Œç¾å‡†ç¡®ç‡åï¼Œè¾“å‡ºå¤šæ ·æ€§ä¹Ÿä¼šé™ä½ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒQ-learningæä¾›äº†ç¦»ç­–ç•¥å­¦ä¹ å’Œæ”¶æ•›æ—¶ä¿æŒå¤šæ ·æ€§è¿™ä¸¤ä¸ªå…³é”®ä¼˜åŠ¿ã€‚è¿›ä¸€æ­¥è¯æ˜ï¼Œéœ€è¦ä»”ç»†è®¾è®¡å¥–åŠ±ä»¥é˜²æ­¢Q-learningä¸­çš„å¥–åŠ±ç ´è§£ã€‚æœ€åï¼Œå°†è¯¥æ¡†æ¶åº”ç”¨äºç°å®ä¸–ç•Œçš„è§„åˆ’åŸºå‡†Blocksworldï¼Œè¯å®äº†è¿™äº›è¡Œä¸ºåœ¨å®è·µä¸­ç¡®å®å­˜åœ¨ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¿›è¡Œè§„åˆ’ä»»åŠ¡æ—¶ï¼Œé€šå¸¸é‡‡ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ–¹æ³•ã€‚ç„¶è€Œï¼ŒSFTå®¹æ˜“å—åˆ°è®­ç»ƒæ•°æ®ä¸­è™šå‡ç›¸å…³æ€§çš„å½±å“ï¼Œä¾‹å¦‚ï¼ŒæŸäº›è¯è¯­ç»å¸¸ä¸€èµ·å‡ºç°ï¼Œå¯¼è‡´æ¨¡å‹å­¦ä¹ åˆ°é”™è¯¯çš„è§„åˆ’ç­–ç•¥ã€‚è¿™ç§åŸºäºå…±ç°çš„è™šå‡è§£ä¼šé™ä½æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½¿å…¶åœ¨æ–°çš„ã€æœªè§è¿‡çš„æƒ…å¢ƒä¸‹è¡¨ç°ä¸ä½³ã€‚å› æ­¤ï¼Œå¦‚ä½•æå‡LLMåœ¨è§„åˆ’ä»»åŠ¡ä¸­çš„æ³›åŒ–èƒ½åŠ›ï¼Œé¿å…é™·å…¥è™šå‡è§£ï¼Œæ˜¯ä¸€ä¸ªé‡è¦çš„ç ”ç©¶é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­çš„æ¢ç´¢æœºåˆ¶æ¥å…‹æœSFTçš„å±€é™æ€§ã€‚RLé€šè¿‡ä¸ç¯å¢ƒçš„äº¤äº’ï¼Œä¸æ–­å°è¯•ä¸åŒçš„è¡ŒåŠ¨ï¼Œå¹¶æ ¹æ®è·å¾—çš„å¥–åŠ±æ¥è°ƒæ•´ç­–ç•¥ã€‚è¿™ç§æ¢ç´¢è¿‡ç¨‹æœ‰åŠ©äºæ¨¡å‹å‘ç°æ›´ä¼˜çš„è§„åˆ’è·¯å¾„ï¼Œé¿å…è¿‡åº¦ä¾èµ–è®­ç»ƒæ•°æ®ä¸­çš„è™šå‡ç›¸å…³æ€§ã€‚åŒæ—¶ï¼Œè®ºæ–‡å¯¹æ¯”äº†ç­–ç•¥æ¢¯åº¦ï¼ˆPGï¼‰å’ŒQ-learningä¸¤ç§RLæ–¹æ³•ï¼Œåˆ†æäº†å®ƒä»¬åœ¨è§„åˆ’ä»»åŠ¡ä¸­çš„ä¼˜ç¼ºç‚¹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡æ„å»ºäº†ä¸€ä¸ªåŸºäºå›¾çš„æŠ½è±¡æ¡†æ¶ï¼Œç”¨äºç†è®ºåˆ†æRLåœ¨LLMè§„åˆ’ä¸­çš„è¡Œä¸ºã€‚è¯¥æ¡†æ¶å°†è§„åˆ’ä»»åŠ¡è¡¨ç¤ºä¸ºä¸€ä¸ªå›¾ï¼Œå…¶ä¸­èŠ‚ç‚¹è¡¨ç¤ºçŠ¶æ€ï¼Œè¾¹è¡¨ç¤ºè¡ŒåŠ¨ã€‚ç„¶åï¼Œåˆ†åˆ«ä½¿ç”¨PGå’ŒQ-learningç®—æ³•åœ¨è¯¥å›¾ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶åˆ†æå®ƒä»¬çš„æ”¶æ•›æ€§è´¨å’Œæ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜è®¾è®¡äº†ä¸€ä¸ªBlocksworldç¯å¢ƒï¼Œç”¨äºéªŒè¯ç†è®ºåˆ†æçš„ç»“è®ºã€‚æ•´ä½“æµç¨‹åŒ…æ‹¬ï¼š1ï¼‰æ„å»ºå›¾æŠ½è±¡ï¼›2ï¼‰åˆ†åˆ«ä½¿ç”¨PGå’ŒQ-learningè¿›è¡Œè®­ç»ƒï¼›3ï¼‰ç†è®ºåˆ†ææ”¶æ•›æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼›4ï¼‰åœ¨Blocksworldç¯å¢ƒä¸­è¿›è¡Œå®éªŒéªŒè¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºä»ç†è®ºä¸Šæ­ç¤ºäº†RLåœ¨LLMè§„åˆ’ä¸­çš„ä¼˜åŠ¿å’Œå±€é™æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œè®ºæ–‡è¯æ˜äº†SFTå¯èƒ½å¼•å…¥åŸºäºå…±ç°çš„è™šå‡è§£ï¼Œè€ŒRLä¸»è¦é€šè¿‡æ¢ç´¢å®ç°æ­£ç¡®çš„è§„åˆ’ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜å‘ç°äº†PGå­˜åœ¨å¤šæ ·æ€§å´©æºƒé—®é¢˜ï¼Œè€ŒQ-learningåœ¨å¤šæ ·æ€§ä¿æŒæ–¹é¢æ›´å…·ä¼˜åŠ¿ã€‚è¿™äº›ç†è®ºå‘ç°ä¸ºRLåœ¨LLMè§„åˆ’ä¸­çš„åº”ç”¨æä¾›äº†é‡è¦çš„æŒ‡å¯¼ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨Q-learningä¸­ï¼Œå¥–åŠ±å‡½æ•°çš„è®¾è®¡è‡³å…³é‡è¦ã€‚ä¸ºäº†é˜²æ­¢å¥–åŠ±ç ´è§£ï¼Œéœ€è¦ä»”ç»†è®¾è®¡å¥–åŠ±å‡½æ•°ï¼Œä½¿å…¶èƒ½å¤Ÿå‡†ç¡®åæ˜ è§„åˆ’ä»»åŠ¡çš„ç›®æ ‡ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥é‡‡ç”¨ç¨€ç–å¥–åŠ±ï¼Œåªåœ¨å®Œæˆè§„åˆ’ç›®æ ‡æ—¶ç»™äºˆå¥–åŠ±ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜åˆ†æäº†å­¦ä¹ ç‡ã€æ¢ç´¢ç‡ç­‰è¶…å‚æ•°å¯¹RLæ€§èƒ½çš„å½±å“ï¼Œå¹¶ç»™å‡ºäº†åˆç†çš„è®¾ç½®å»ºè®®ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡é€šè¿‡ç†è®ºåˆ†æå’Œå®éªŒéªŒè¯ï¼Œæ­ç¤ºäº†å¼ºåŒ–å­¦ä¹ åœ¨è¯­è¨€æ¨¡å‹è§„åˆ’ä¸­çš„ä¼˜åŠ¿å’Œå±€é™æ€§ã€‚åœ¨Blocksworldå®éªŒä¸­ï¼ŒéªŒè¯äº†ç­–ç•¥æ¢¯åº¦å­˜åœ¨å¤šæ ·æ€§å´©æºƒé—®é¢˜ï¼Œè€ŒQ-learningåœ¨å¤šæ ·æ€§ä¿æŒæ–¹é¢æ›´å…·ä¼˜åŠ¿ã€‚è¿™äº›ç»“æœä¸ºå®é™…åº”ç”¨ä¸­é€‰æ‹©åˆé€‚çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•æä¾›äº†é‡è¦çš„å‚è€ƒä¾æ®ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦è¯­è¨€æ¨¡å‹è¿›è¡Œè§„åˆ’çš„åœºæ™¯ï¼Œä¾‹å¦‚æœºå™¨äººå¯¼èˆªã€ä»»åŠ¡è°ƒåº¦ã€å¯¹è¯ç³»ç»Ÿç­‰ã€‚é€šè¿‡åˆ©ç”¨å¼ºåŒ–å­¦ä¹ çš„æ¢ç´¢èƒ½åŠ›å’Œå¤šæ ·æ€§ä¿æŒç‰¹æ€§ï¼Œå¯ä»¥æå‡è¯­è¨€æ¨¡å‹åœ¨å¤æ‚ç¯å¢ƒä¸‹çš„è§„åˆ’èƒ½åŠ›å’Œæ³›åŒ–æ€§èƒ½ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°å®Œæˆå„ç§å®é™…ä»»åŠ¡ã€‚æœªæ¥çš„ç ”ç©¶å¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢æ›´æœ‰æ•ˆçš„å¥–åŠ±å‡½æ•°è®¾è®¡å’Œæ¢ç´¢ç­–ç•¥ï¼Œä»¥æå‡å¼ºåŒ–å­¦ä¹ åœ¨è¯­è¨€æ¨¡å‹è§„åˆ’ä¸­çš„åº”ç”¨æ•ˆæœã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent reinforcement learning (RL) methods have substantially enhanced the planning capabilities of Large Language Models (LLMs), yet the theoretical basis for their effectiveness remains elusive. In this work, we investigate RL's benefits and limitations through a tractable graph-based abstraction, focusing on policy gradient (PG) and Q-learning methods. Our theoretical analyses reveal that supervised fine-tuning (SFT) may introduce co-occurrence-based spurious solutions, whereas RL achieves correct planning primarily through exploration, underscoring exploration's role in enabling better generalization. However, we also show that PG suffers from diversity collapse, where output diversity decreases during training and persists even after perfect accuracy is attained. By contrast, Q-learning provides two key advantages: off-policy learning and diversity preservation at convergence. We further demonstrate that careful reward design is necessary to prevent reward hacking in Q-learning. Finally, applying our framework to the real-world planning benchmark Blocksworld, we confirm that these behaviors manifest in practice.

