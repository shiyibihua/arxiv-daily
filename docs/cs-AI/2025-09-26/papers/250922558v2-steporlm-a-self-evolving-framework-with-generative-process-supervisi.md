---
layout: default
title: StepORLM: A Self-Evolving Framework With Generative Process Supervision For Operations Research Language Models
---

# StepORLM: A Self-Evolving Framework With Generative Process Supervision For Operations Research Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.22558" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.22558v2</a>
  <a href="https://arxiv.org/pdf/2509.22558.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.22558v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.22558v2', 'StepORLM: A Self-Evolving Framework With Generative Process Supervision For Operations Research Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chenyu Zhou, Tianyi Xu, Jianghao Lin, Dongdong Ge

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-26 (æ›´æ–°: 2025-10-01)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**StepORLMï¼šä¸€ä¸ªè‡ªè¿›åŒ–æ¡†æ¶ï¼Œé€šè¿‡ç”Ÿæˆè¿‡ç¨‹ç›‘ç£æå‡è¿ç­¹å­¦è¯­è¨€æ¨¡å‹æ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¿ç­¹å­¦` `è¯­è¨€æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `è¿‡ç¨‹ç›‘ç£` `ååŒè¿›åŒ–` `å¥–åŠ±æ¨¡å‹` `ç›´æ¥åå¥½ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è¿ç­¹å­¦è¯­è¨€æ¨¡å‹è®­ç»ƒé¢ä¸´ä¿¡ç”¨åˆ†é…é—®é¢˜å’Œè¿‡ç¨‹ç›‘ç£çš„çŸ­è§†æ€§ï¼Œå¯¼è‡´æ¨¡å‹æ¨ç†è¿‡ç¨‹å­˜åœ¨ç¼ºé™·ã€‚
2. StepORLMé€šè¿‡ååŒè¿›åŒ–ç­–ç•¥æ¨¡å‹å’Œç”Ÿæˆè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼Œåˆ©ç”¨åŒé‡åé¦ˆæœºåˆ¶è¿›è¡Œä¼˜åŒ–ï¼Œè§£å†³ä¸Šè¿°é—®é¢˜ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒStepORLMåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œå¹¶èƒ½æœ‰æ•ˆæå‡å…¶ä»–LLMçš„æ¨ç†æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨è§£å†³è¿ç­¹å­¦(OR)é—®é¢˜æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰åŸºäºå¼ºåŒ–å­¦ä¹ çš„LLMè®­ç»ƒæ–¹æ³•é€šå¸¸é¢ä¸´ä¸¤ä¸ªå…³é”®é™åˆ¶ï¼šä¸€æ˜¯ç»“æœå¥–åŠ±å­˜åœ¨ä¿¡ç”¨åˆ†é…é—®é¢˜ï¼Œæ­£ç¡®çš„æœ€ç»ˆç­”æ¡ˆå¯èƒ½ä¼šå¼ºåŒ–é”™è¯¯çš„æ¨ç†è¿‡ç¨‹ï¼›äºŒæ˜¯ä¼ ç»Ÿçš„åˆ¤åˆ«å¼è¿‡ç¨‹ç›‘ç£æ˜¯çŸ­è§†çš„ï¼Œæ— æ³•å…¨é¢è¯„ä¼°ORå»ºæ¨¡ä¸­ç›¸äº’ä¾èµ–çš„æ­¥éª¤ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†StepORLMï¼Œä¸€ç§å…·æœ‰ç”Ÿæˆè¿‡ç¨‹ç›‘ç£çš„æ–°å‹è‡ªè¿›åŒ–æ¡†æ¶ã€‚StepORLMçš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªååŒè¿›åŒ–å¾ªç¯ï¼Œå…¶ä¸­ç­–ç•¥æ¨¡å‹å’Œç”Ÿæˆè¿‡ç¨‹å¥–åŠ±æ¨¡å‹(GenPRM)è¿­ä»£åœ°ç›¸äº’æ”¹è¿›ã€‚è¯¥å¾ªç¯ç”±åŒé‡åé¦ˆæœºåˆ¶é©±åŠ¨ï¼šæ¥è‡ªå¤–éƒ¨æ±‚è§£å™¨çš„æ˜ç¡®çš„ã€åŸºäºç»“æœçš„éªŒè¯ï¼Œä»¥åŠæ¥è‡ªGenPRMçš„ç»†è‡´çš„ã€å…¨é¢çš„è¿‡ç¨‹è¯„ä¼°ã€‚ç»“åˆåçš„ä¿¡å·ç”¨äºé€šè¿‡åŠ æƒç›´æ¥åå¥½ä¼˜åŒ–(W-DPO)æ¥å¯¹é½ç­–ç•¥ï¼Œå¹¶åŒæ—¶æ”¹è¿›GenPRMã€‚æˆ‘ä»¬å¾—åˆ°çš„80äº¿å‚æ•°StepORLMåœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸­å»ºç«‹äº†æ–°çš„æœ€å…ˆè¿›æ°´å¹³ï¼Œæ˜¾è‘—ä¼˜äºæ›´å¤§çš„é€šç”¨æ¨¡å‹ã€æ™ºèƒ½ä½“æ–¹æ³•å’Œä¸“é—¨çš„åŸºçº¿ã€‚æ­¤å¤–ï¼ŒååŒè¿›åŒ–çš„GenPRMèƒ½å¤Ÿå……å½“å¼ºå¤§ä¸”æ™®éé€‚ç”¨çš„è¿‡ç¨‹éªŒè¯å™¨ï¼Œä»è€Œæ˜¾è‘—æé«˜æˆ‘ä»¬è‡ªå·±çš„æ¨¡å‹å’Œå…¶ä»–ç°æœ‰LLMçš„æ¨ç†æ‰©å±•æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è¿ç­¹å­¦è¯­è¨€æ¨¡å‹åœ¨è§£å†³é—®é¢˜æ—¶ï¼Œä»…ä»…ä¾èµ–æœ€ç»ˆç»“æœè¿›è¡Œå¥–åŠ±ï¼Œæ— æ³•åŒºåˆ†æ­£ç¡®çš„æ¨ç†è¿‡ç¨‹å’Œé”™è¯¯çš„æ¨ç†è¿‡ç¨‹ç¢°å·§å¾—åˆ°æ­£ç¡®ç­”æ¡ˆçš„æƒ…å†µï¼Œå¯¼è‡´æ¨¡å‹å­¦ä¹ åˆ°é”™è¯¯çš„æ¨ç†é€»è¾‘ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„è¿‡ç¨‹ç›‘ç£æ–¹æ³•é€šå¸¸æ˜¯åˆ¤åˆ«å¼çš„ï¼Œåªå…³æ³¨å½“å‰æ­¥éª¤çš„æ­£ç¡®æ€§ï¼Œå¿½ç•¥äº†æ­¥éª¤ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œæ— æ³•è¿›è¡Œå…¨å±€ä¼˜åŒ–ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šStepORLMçš„æ ¸å¿ƒæ€è·¯æ˜¯å¼•å…¥ä¸€ä¸ªç”Ÿæˆè¿‡ç¨‹å¥–åŠ±æ¨¡å‹(GenPRM)ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿè¯„ä¼°æ•´ä¸ªæ¨ç†è¿‡ç¨‹çš„è´¨é‡ï¼Œè€Œä¸ä»…ä»…æ˜¯æœ€ç»ˆç»“æœã€‚é€šè¿‡è®©ç­–ç•¥æ¨¡å‹å’ŒGenPRMååŒè¿›åŒ–ï¼Œç­–ç•¥æ¨¡å‹å¯ä»¥å­¦ä¹ åˆ°æ›´åˆç†çš„æ¨ç†è¿‡ç¨‹ï¼Œè€ŒGenPRMä¹Ÿå¯ä»¥ä¸æ–­æå‡è¯„ä¼°è¿‡ç¨‹çš„èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šStepORLMåŒ…å«ä¸€ä¸ªç­–ç•¥æ¨¡å‹å’Œä¸€ä¸ªç”Ÿæˆè¿‡ç¨‹å¥–åŠ±æ¨¡å‹(GenPRM)ã€‚ç­–ç•¥æ¨¡å‹è´Ÿè´£ç”Ÿæˆè§£å†³è¿ç­¹å­¦é—®é¢˜çš„æ­¥éª¤ï¼ŒGenPRMè´Ÿè´£è¯„ä¼°ç­–ç•¥æ¨¡å‹ç”Ÿæˆçš„æ­¥éª¤çš„è´¨é‡ã€‚æ•´ä¸ªæ¡†æ¶é€šè¿‡ä¸€ä¸ªååŒè¿›åŒ–å¾ªç¯è¿›è¡Œè®­ç»ƒã€‚åœ¨æ¯ä¸ªå¾ªç¯ä¸­ï¼Œç­–ç•¥æ¨¡å‹é¦–å…ˆç”Ÿæˆä¸€ç³»åˆ—æ­¥éª¤ï¼Œç„¶åGenPRMå¯¹è¿™äº›æ­¥éª¤è¿›è¡Œè¯„ä¼°ï¼Œå¹¶ç»™å‡ºå¥–åŠ±ã€‚ç­–ç•¥æ¨¡å‹æ ¹æ®å¥–åŠ±è°ƒæ•´è‡ªèº«çš„å‚æ•°ï¼Œä»¥ç”Ÿæˆæ›´å¥½çš„æ­¥éª¤ã€‚åŒæ—¶ï¼ŒGenPRMä¹Ÿæ ¹æ®ç­–ç•¥æ¨¡å‹çš„è¡¨ç°è°ƒæ•´è‡ªèº«çš„å‚æ•°ï¼Œä»¥æ›´å‡†ç¡®åœ°è¯„ä¼°æ­¥éª¤çš„è´¨é‡ã€‚æœ€ç»ˆï¼Œé€šè¿‡ä¸æ–­è¿­ä»£ï¼Œç­–ç•¥æ¨¡å‹å’ŒGenPRMéƒ½èƒ½å¤Ÿå¾—åˆ°æå‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šStepORLMçš„å…³é”®åˆ›æ–°åœ¨äºå¼•å…¥äº†ç”Ÿæˆè¿‡ç¨‹å¥–åŠ±æ¨¡å‹(GenPRM)å’ŒååŒè¿›åŒ–æœºåˆ¶ã€‚GenPRMèƒ½å¤Ÿå¯¹æ•´ä¸ªæ¨ç†è¿‡ç¨‹è¿›è¡Œè¯„ä¼°ï¼Œä»è€Œé¿å…äº†ä¿¡ç”¨åˆ†é…é—®é¢˜ã€‚ååŒè¿›åŒ–æœºåˆ¶ä½¿å¾—ç­–ç•¥æ¨¡å‹å’ŒGenPRMèƒ½å¤Ÿç›¸äº’ä¿ƒè¿›ï¼Œå…±åŒæå‡æ€§èƒ½ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒStepORLMèƒ½å¤Ÿå­¦ä¹ åˆ°æ›´åˆç†çš„æ¨ç†è¿‡ç¨‹ï¼Œå¹¶å–å¾—æ›´å¥½çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šStepORLMä½¿ç”¨åŠ æƒç›´æ¥åå¥½ä¼˜åŒ–(W-DPO)æ¥å¯¹é½ç­–ç•¥æ¨¡å‹ã€‚W-DPOæ˜¯ä¸€ç§åŸºäºåå¥½å­¦ä¹ çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œå®ƒèƒ½å¤Ÿæ ¹æ®GenPRMç»™å‡ºçš„å¥–åŠ±æ¥è°ƒæ•´ç­–ç•¥æ¨¡å‹çš„å‚æ•°ã€‚GenPRMçš„è®­ç»ƒç›®æ ‡æ˜¯æœ€å¤§åŒ–ç­–ç•¥æ¨¡å‹ç”Ÿæˆçš„æ­£ç¡®æ­¥éª¤çš„å¥–åŠ±ï¼ŒåŒæ—¶æœ€å°åŒ–ç­–ç•¥æ¨¡å‹ç”Ÿæˆçš„é”™è¯¯æ­¥éª¤çš„å¥–åŠ±ã€‚GenPRMå¯ä»¥ä½¿ç”¨å„ç§ç¥ç»ç½‘ç»œç»“æ„ï¼Œä¾‹å¦‚Transformerã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

StepORLMåœ¨å…­ä¸ªè¿ç­¹å­¦åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„é€šç”¨æ¨¡å‹ã€æ™ºèƒ½ä½“æ–¹æ³•å’Œä¸“é—¨çš„åŸºçº¿æ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸäº›åŸºå‡†æµ‹è¯•ä¸­ï¼ŒStepORLMçš„æ€§èƒ½æå‡è¶…è¿‡äº†10%ã€‚æ­¤å¤–ï¼ŒååŒè¿›åŒ–çš„GenPRMèƒ½å¤Ÿæœ‰æ•ˆæå‡å…¶ä»–LLMçš„æ¨ç†æ‰©å±•æ€§èƒ½ï¼Œè¡¨æ˜å…¶å…·æœ‰å¾ˆå¼ºçš„é€šç”¨æ€§å’Œå®ç”¨ä»·å€¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

StepORLMå¯åº”ç”¨äºå„ç§è¿ç­¹å­¦é—®é¢˜çš„æ±‚è§£ï¼Œä¾‹å¦‚ä¼˜åŒ–è°ƒåº¦ã€èµ„æºåˆ†é…ã€è·¯å¾„è§„åˆ’ç­‰ã€‚è¯¥ç ”ç©¶æˆæœæœ‰åŠ©äºæå‡äººå·¥æ™ºèƒ½åœ¨è§£å†³å¤æ‚ä¼˜åŒ–é—®é¢˜æ–¹é¢çš„èƒ½åŠ›ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½ç‰©æµã€æ™ºèƒ½åˆ¶é€ ã€æ™ºèƒ½äº¤é€šç­‰é¢†åŸŸã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯ä»¥æ‰©å±•åˆ°å…¶ä»–éœ€è¦å¤æ‚æ¨ç†è¿‡ç¨‹çš„ä»»åŠ¡ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have shown promising capabilities for solving Operations Research (OR) problems. While reinforcement learning serves as a powerful paradigm for LLM training on OR problems, existing works generally face two key limitations. First, outcome reward suffers from the credit assignment problem, where correct final answers can reinforce flawed reasoning. Second, conventional discriminative process supervision is myopic, failing to evaluate the interdependent steps of OR modeling holistically. To this end, we introduce StepORLM, a novel self-evolving framework with generative process supervision. At its core, StepORLM features a co-evolutionary loop where a policy model and a generative process reward model (GenPRM) iteratively improve on each other. This loop is driven by a dual-feedback mechanism: definitive, outcome-based verification from an external solver, and nuanced, holistic process evaluation from the GenPRM. The combined signal is used to align the policy via Weighted Direct Preference Optimization (W-DPO) and simultaneously refine the GenPRM. Our resulting 8B-parameter StepORLM establishes a new state-of-the-art across six benchmarks, significantly outperforming vastly larger generalist models, agentic methods, and specialized baselines. Moreover, the co-evolved GenPRM is able to act as a powerful and universally applicable process verifier, substantially boosting the inference scaling performance of both our own model and other existing LLMs.

