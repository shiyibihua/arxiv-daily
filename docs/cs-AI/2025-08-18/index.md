---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.AI - 2025-08-18
---

# cs.AIï¼ˆ2025-08-18ï¼‰

ğŸ“Š å…± **24** ç¯‡è®ºæ–‡
 | ğŸ”— **4** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (15 ğŸ”—1)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (4 ğŸ”—1)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1)</a>
<a href="#æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction" class="interest-badge">æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1)</a>
<a href="#æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting" class="interest-badge">æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (1 ğŸ”—1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (15 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250812854v1-e3rg-building-explicit-emotion-driven-empathetic-response-generation.html">E3RG: Building Explicit Emotion-driven Empathetic Response Generation System with Multimodal Large Language Model</a></td>
  <td>æå‡ºE3RGä»¥è§£å†³å¤šæ¨¡æ€æƒ…æ„Ÿé©±åŠ¨çš„åŒç†å¿ƒå“åº”ç”Ÿæˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.12854v1" data-paper-url="./papers/250812854v1-e3rg-building-explicit-emotion-driven-empathetic-response-generation.html" onclick="toggleFavorite(this, '2508.12854v1', 'E3RG: Building Explicit Emotion-driven Empathetic Response Generation System with Multimodal Large Language Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250813387v1-spaner-shared-prompt-aligner-for-multimodal-semantic-representation.html">SPANER: Shared Prompt Aligner for Multimodal Semantic Representation</a></td>
  <td>æå‡ºSPANERä»¥è§£å†³å¤šæ¨¡æ€è¯­ä¹‰è¡¨ç¤ºçš„å­¤ç«‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.13387v1" data-paper-url="./papers/250813387v1-spaner-shared-prompt-aligner-for-multimodal-semantic-representation.html" onclick="toggleFavorite(this, '2508.13387v1', 'SPANER: Shared Prompt Aligner for Multimodal Semantic Representation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250813327v1-towards-unified-multimodal-financial-forecasting-integrating-sentime.html">Towards Unified Multimodal Financial Forecasting: Integrating Sentiment Embeddings and Market Indicators via Cross-Modal Attention</a></td>
  <td>æå‡ºSTONKæ¡†æ¶ä»¥è§£å†³å¤šæ¨¡æ€é‡‘èé¢„æµ‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.13327v1" data-paper-url="./papers/250813327v1-towards-unified-multimodal-financial-forecasting-integrating-sentime.html" onclick="toggleFavorite(this, '2508.13327v1', 'Towards Unified Multimodal Financial Forecasting: Integrating Sentiment Embeddings and Market Indicators via Cross-Modal Attention')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250813072v1-a-language-signal-vision-multimodal-framework-for-multitask-cardiac-.html">A Language-Signal-Vision Multimodal Framework for Multitask Cardiac Analysis</a></td>
  <td>æå‡ºTGMMæ¡†æ¶ä»¥è§£å†³å¤šæ¨¡æ€å¿ƒè„åˆ†æé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.13072v1" data-paper-url="./papers/250813072v1-a-language-signal-vision-multimodal-framework-for-multitask-cardiac-.html" onclick="toggleFavorite(this, '2508.13072v1', 'A Language-Signal-Vision Multimodal Framework for Multitask Cardiac Analysis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250813257v1-vitad-timing-violation-aware-debugging-of-rtl-code-using-large-langu.html">ViTAD: Timing Violation-Aware Debugging of RTL Code using Large Language Models</a></td>
  <td>æå‡ºViTADä»¥è§£å†³RTLä»£ç ä¸­çš„æ—¶åºè¿è§„è°ƒè¯•é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.13257v1" data-paper-url="./papers/250813257v1-vitad-timing-violation-aware-debugging-of-rtl-code-using-large-langu.html" onclick="toggleFavorite(this, '2508.13257v1', 'ViTAD: Timing Violation-Aware Debugging of RTL Code using Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250812920v1-do-large-language-model-agents-exhibit-a-survival-instinct-an-empiri.html">Do Large Language Model Agents Exhibit a Survival Instinct? An Empirical Study in a Sugarscape-Style Simulation</a></td>
  <td>ç ”ç©¶å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†çš„ç”Ÿå­˜æœ¬èƒ½ä»¥æå‡AIå®‰å…¨æ€§</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.12920v1" data-paper-url="./papers/250812920v1-do-large-language-model-agents-exhibit-a-survival-instinct-an-empiri.html" onclick="toggleFavorite(this, '2508.12920v1', 'Do Large Language Model Agents Exhibit a Survival Instinct? An Empirical Study in a Sugarscape-Style Simulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250813251v2-dive-into-hydrogen-storage-materials-discovery-with-ai-agents.html">"DIVE" into Hydrogen Storage Materials Discovery with AI Agents</a></td>
  <td>æå‡ºDIVEä»¥è§£å†³æ°¢å‚¨å­˜ææ–™å‘ç°ä¸­çš„æ•°æ®æå–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.13251v2" data-paper-url="./papers/250813251v2-dive-into-hydrogen-storage-materials-discovery-with-ai-agents.html" onclick="toggleFavorite(this, '2508.13251v2', '&quot;DIVE&quot; into Hydrogen Storage Materials Discovery with AI Agents')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250814123v1-ai-agents-for-photonic-integrated-circuit-design-automation.html">AI Agents for Photonic Integrated Circuit Design Automation</a></td>
  <td>æå‡ºPhIDOæ¡†æ¶ä»¥å®ç°å…‰å­é›†æˆç”µè·¯è®¾è®¡è‡ªåŠ¨åŒ–</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.14123v1" data-paper-url="./papers/250814123v1-ai-agents-for-photonic-integrated-circuit-design-automation.html" onclick="toggleFavorite(this, '2508.14123v1', 'AI Agents for Photonic Integrated Circuit Design Automation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250813092v3-veriloglavd-llm-aided-rule-generation-for-vulnerability-detection-in.html">VerilogLAVD: LLM-Aided Rule Generation for Vulnerability Detection in Verilog</a></td>
  <td>æå‡ºVerilogLAVDä»¥è§£å†³Verilogç¡¬ä»¶æ¼æ´æ£€æµ‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.13092v3" data-paper-url="./papers/250813092v3-veriloglavd-llm-aided-rule-generation-for-vulnerability-detection-in.html" onclick="toggleFavorite(this, '2508.13092v3', 'VerilogLAVD: LLM-Aided Rule Generation for Vulnerability Detection in Verilog')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250813047v1-using-ai-for-user-representation-an-analysis-of-83-persona-prompts.html">Using AI for User Representation: An Analysis of 83 Persona Prompts</a></td>
  <td>åˆ†æ83ä¸ªç”¨æˆ·è§’è‰²æç¤ºä»¥ä¼˜åŒ–AIç”¨æˆ·è¡¨ç¤º</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.13047v1" data-paper-url="./papers/250813047v1-using-ai-for-user-representation-an-analysis-of-83-persona-prompts.html" onclick="toggleFavorite(this, '2508.13047v1', 'Using AI for User Representation: An Analysis of 83 Persona Prompts')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250813003v2-evolmatheval-towards-evolvable-benchmarks-for-mathematical-reasoning.html">EvolMathEval: Towards Evolvable Benchmarks for Mathematical Reasoning via Evolutionary Testing</a></td>
  <td>æå‡ºEvolMathEvalä»¥è§£å†³æ•°å­¦æ¨ç†åŸºå‡†è¯„ä¼°çš„æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.13003v2" data-paper-url="./papers/250813003v2-evolmatheval-towards-evolvable-benchmarks-for-mathematical-reasoning.html" onclick="toggleFavorite(this, '2508.13003v2', 'EvolMathEval: Towards Evolvable Benchmarks for Mathematical Reasoning via Evolutionary Testing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250909688v1-ai-powered-assistant-for-long-term-access-to-rhic-knowledge.html">AI-Powered Assistant for Long-Term Access to RHIC Knowledge</a></td>
  <td>æå‡ºAIåŠ©æ‰‹ä»¥å®ç°RHICçŸ¥è¯†çš„é•¿æœŸè®¿é—®ä¸ä¿å­˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.09688v1" data-paper-url="./papers/250909688v1-ai-powered-assistant-for-long-term-access-to-rhic-knowledge.html" onclick="toggleFavorite(this, '2509.09688v1', 'AI-Powered Assistant for Long-Term Access to RHIC Knowledge')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250812910v2-secfsm-knowledge-graph-guided-verilog-code-generation-for-secure-fin.html">SecFSM: Knowledge Graph-Guided Verilog Code Generation for Secure Finite State Machines in Systems-on-Chip</a></td>
  <td>æå‡ºSecFSMä»¥è§£å†³å®‰å…¨æ•æ„ŸFSMçš„Verilogä»£ç ç”Ÿæˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.12910v2" data-paper-url="./papers/250812910v2-secfsm-knowledge-graph-guided-verilog-code-generation-for-secure-fin.html" onclick="toggleFavorite(this, '2508.12910v2', 'SecFSM: Knowledge Graph-Guided Verilog Code Generation for Secure Finite State Machines in Systems-on-Chip')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250813246v2-involuntary-jailbreak.html">Involuntary Jailbreak</a></td>
  <td>æ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ–°å‹è„†å¼±æ€§ï¼šéè‡ªæ„¿è¶Šç‹±</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.13246v2" data-paper-url="./papers/250813246v2-involuntary-jailbreak.html" onclick="toggleFavorite(this, '2508.13246v2', 'Involuntary Jailbreak')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250812611v2-an-llm-asp-workflow-for-joint-entity-relation-extraction.html">An LLM + ASP Workflow for Joint Entity-Relation Extraction</a></td>
  <td>æå‡ºLLMä¸ASPç»“åˆçš„å·¥ä½œæµç¨‹ä»¥è§£å†³è”åˆå®ä½“å…³ç³»æŠ½å–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.12611v2" data-paper-url="./papers/250812611v2-an-llm-asp-workflow-for-joint-entity-relation-extraction.html" onclick="toggleFavorite(this, '2508.12611v2', 'An LLM + ASP Workflow for Joint Entity-Relation Extraction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (4 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>16</td>
  <td><a href="./papers/250812790v1-reinforcement-learning-with-rubric-anchors.html">Reinforcement Learning with Rubric Anchors</a></td>
  <td>æå‡ºåŸºäºè¯„åˆ†æ ‡å‡†çš„å¼ºåŒ–å­¦ä¹ ä»¥è§£å†³å¼€æ”¾ä»»åŠ¡çš„è¯„ä¼°é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.12790v1" data-paper-url="./papers/250812790v1-reinforcement-learning-with-rubric-anchors.html" onclick="toggleFavorite(this, '2508.12790v1', 'Reinforcement Learning with Rubric Anchors')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/250812935v1-towards-open-ended-emotional-support-conversations-in-llms-via-reinf.html">Towards Open-Ended Emotional Support Conversations in LLMs via Reinforcement Learning with Future-Oriented Rewards</a></td>
  <td>æå‡ºRLFF-ESCæ¡†æ¶ä»¥è§£å†³æƒ…æ„Ÿæ”¯æŒå¯¹è¯ç³»ç»Ÿçš„çµæ´»æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.12935v1" data-paper-url="./papers/250812935v1-towards-open-ended-emotional-support-conversations-in-llms-via-reinf.html" onclick="toggleFavorite(this, '2508.12935v1', 'Towards Open-Ended Emotional Support Conversations in LLMs via Reinforcement Learning with Future-Oriented Rewards')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/250812943v2-optic-er-a-reinforcement-learning-framework-for-real-time-emergency-.html">OPTIC-ER: A Reinforcement Learning Framework for Real-Time Emergency Response and Equitable Resource Allocation in Underserved African Communities</a></td>
  <td>æå‡ºOPTIC-ERæ¡†æ¶ä»¥è§£å†³éæ´²ç¤¾åŒºç´§æ€¥å“åº”ä¸èµ„æºåˆ†é…ä¸å‡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.12943v2" data-paper-url="./papers/250812943v2-optic-er-a-reinforcement-learning-framework-for-real-time-emergency-.html" onclick="toggleFavorite(this, '2508.12943v2', 'OPTIC-ER: A Reinforcement Learning Framework for Real-Time Emergency Response and Equitable Resource Allocation in Underserved African Communities')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/250813023v1-g2rpo-a-guided-group-relative-policy-optimization-with-adaptive-guid.html">G$^2$RPO-A: Guided Group Relative Policy Optimization with Adaptive Guidance</a></td>
  <td>æå‡ºG$^2$RPO-Aä»¥è§£å†³å°å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›ä¸è¶³çš„é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">large language model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.13023v1" data-paper-url="./papers/250813023v1-g2rpo-a-guided-group-relative-policy-optimization-with-adaptive-guid.html" onclick="toggleFavorite(this, '2508.13023v1', 'G$^2$RPO-A: Guided Group Relative Policy Optimization with Adaptive Guidance')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>20</td>
  <td><a href="./papers/250812687v2-egoillusion-benchmarking-hallucinations-in-egocentric-video-understa.html">EGOILLUSION: Benchmarking Hallucinations in Egocentric Video Understanding</a></td>
  <td>æå‡ºEgoIllusionä»¥è¯„ä¼°è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘ç†è§£ä¸­çš„å¹»è§‰é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">egocentric</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.12687v2" data-paper-url="./papers/250812687v2-egoillusion-benchmarking-hallucinations-in-egocentric-video-understa.html" onclick="toggleFavorite(this, '2508.12687v2', 'EGOILLUSION: Benchmarking Hallucinations in Egocentric Video Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction">ğŸ”¬ æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>21</td>
  <td><a href="./papers/250813256v2-cardaic-agents-a-multimodal-framework-with-hierarchical-adaptation-f.html">CardAIc-Agents: A Multimodal Framework with Hierarchical Adaptation for Cardiac Care Support</a></td>
  <td>æå‡ºCardAIc-Agentsä»¥è§£å†³å¿ƒè„æŠ¤ç†æ”¯æŒä¸­çš„é€‚åº”æ€§ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">ReMoS</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.13256v2" data-paper-url="./papers/250813256v2-cardaic-agents-a-multimodal-framework-with-hierarchical-adaptation-f.html" onclick="toggleFavorite(this, '2508.13256v2', 'CardAIc-Agents: A Multimodal Framework with Hierarchical Adaptation for Cardiac Care Support')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting">ğŸ”¬ æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>22</td>
  <td><a href="./papers/250813371v1-loop-a-plug-and-play-neuro-symbolic-framework-for-enhancing-planning.html">LOOP: A Plug-and-Play Neuro-Symbolic Framework for Enhancing Planning in Autonomous Systems</a></td>
  <td>æå‡ºLOOPæ¡†æ¶ä»¥è§£å†³è‡ªä¸»ç³»ç»Ÿè§„åˆ’ä¸­çš„ç¥ç»ç¬¦å·é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">spatial relationship</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.13371v1" data-paper-url="./papers/250813371v1-loop-a-plug-and-play-neuro-symbolic-framework-for-enhancing-planning.html" onclick="toggleFavorite(this, '2508.13371v1', 'LOOP: A Plug-and-Play Neuro-Symbolic Framework for Enhancing Planning in Autonomous Systems')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>23</td>
  <td><a href="./papers/250813021v2-pc-sampler-position-aware-calibration-of-decoding-bias-in-masked-dif.html">PC-Sampler: Position-Aware Calibration of Decoding Bias in Masked Diffusion Models</a></td>
  <td>æå‡ºPC-Samplerä»¥è§£å†³MDMsè§£ç ç­–ç•¥çš„å±€é™æ€§</td>
  <td class="tags-cell"><span class="paper-tag">MDM</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.13021v2" data-paper-url="./papers/250813021v2-pc-sampler-position-aware-calibration-of-decoding-bias-in-masked-dif.html" onclick="toggleFavorite(this, '2508.13021v2', 'PC-Sampler: Position-Aware Calibration of Decoding Bias in Masked Diffusion Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>24</td>
  <td><a href="./papers/250813049v1-xr-npe-high-throughput-mixed-precision-simd-neural-processing-engine.html">XR-NPE: High-Throughput Mixed-precision SIMD Neural Processing Engine for Extended Reality Perception Workloads</a></td>
  <td>æå‡ºXR-NPEä»¥è§£å†³æ‰©å±•ç°å®æ„ŸçŸ¥å·¥ä½œè´Ÿè½½çš„é«˜æ•ˆè®¡ç®—é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">VIO</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2508.13049v1" data-paper-url="./papers/250813049v1-xr-npe-high-throughput-mixed-precision-simd-neural-processing-engine.html" onclick="toggleFavorite(this, '2508.13049v1', 'XR-NPE: High-Throughput Mixed-precision SIMD Neural Processing Engine for Extended Reality Perception Workloads')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.AI é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)