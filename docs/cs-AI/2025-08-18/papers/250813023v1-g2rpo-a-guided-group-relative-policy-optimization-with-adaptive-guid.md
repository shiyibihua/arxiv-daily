---
layout: default
title: G$^2$RPO-A: Guided Group Relative Policy Optimization with Adaptive Guidance
---

# G$^2$RPO-A: Guided Group Relative Policy Optimization with Adaptive Guidance

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.13023" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.13023v1</a>
  <a href="https://arxiv.org/pdf/2508.13023.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.13023v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.13023v1', 'G$^2$RPO-A: Guided Group Relative Policy Optimization with Adaptive Guidance')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yongxin Guo, Wenbo Deng, Zhenglin Cheng, Xiaoying Tang

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-18

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/T-Lab-CUHKSZ/G2RPO-A)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºG$^2$RPO-Aä»¥è§£å†³å°å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›ä¸è¶³çš„é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `å°å‹è¯­è¨€æ¨¡å‹` `æ¨ç†èƒ½åŠ›` `è‡ªé€‚åº”ç®—æ³•` `ä»£ç ç”Ÿæˆ` `æ•°å­¦æ¨ç†` `å¼•å¯¼ç­–ç•¥`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å°å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ä¸Šè¡¨ç°æœ‰é™ï¼Œä¸»è¦ä¾èµ–äºå¼ºå¤§çš„åŸºç¡€æ¨¡å‹ã€‚
2. æœ¬æ–‡æå‡ºçš„G$^2$RPO-Aé€šè¿‡è‡ªé€‚åº”è°ƒæ•´å¼•å¯¼å¼ºåº¦ï¼Œå¢å¼ºå°å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒG$^2$RPO-Aåœ¨æ•°å­¦æ¨ç†å’Œä»£ç ç”Ÿæˆä»»åŠ¡ä¸Šæ˜¾è‘—è¶…è¶Šäº†ä¼ ç»Ÿçš„GRPOæ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æ˜¾è‘—æå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å…¶æˆåŠŸä¾èµ–äºå¼ºå¤§çš„åŸºç¡€æ¨¡å‹ï¼Œå¯¼è‡´å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰ä»…è·å¾—æœ‰é™æ”¹è¿›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é™åˆ¶ï¼Œæœ¬æ–‡æå‡ºäº†å¼•å¯¼å¼GRPOï¼ˆGuided GRPOï¼‰ï¼Œé€šè¿‡å°†çœŸå®æ¨ç†æ­¥éª¤æ³¨å…¥åˆ°å›æ»šè½¨è¿¹ä¸­æ¥å¼¥è¡¥SLMsçš„å›ºæœ‰å¼±ç‚¹ã€‚ç»è¿‡å¯¹å¤šç§å¼•å¯¼é…ç½®çš„ç»¼åˆç ”ç©¶ï¼Œå‘ç°ç®€å•åœ°æ·»åŠ å¼•å¯¼æ•ˆæœæœ‰é™ã€‚è¿™ä¿ƒä½¿æˆ‘ä»¬å¼€å‘äº†G$^2$RPO-Aï¼Œè¿™æ˜¯ä¸€ç§è‡ªé€‚åº”ç®—æ³•ï¼Œèƒ½å¤Ÿæ ¹æ®æ¨¡å‹çš„è®­ç»ƒåŠ¨æ€è‡ªåŠ¨è°ƒæ•´å¼•å¯¼å¼ºåº¦ã€‚åœ¨æ•°å­¦æ¨ç†å’Œä»£ç ç”ŸæˆåŸºå‡†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒG$^2$RPO-Aæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„GRPOã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å°å‹è¯­è¨€æ¨¡å‹åœ¨æ¨ç†èƒ½åŠ›ä¸Šçš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•åœ¨å¼•å¯¼å°å‹æ¨¡å‹æ—¶æ•ˆæœæœ‰é™ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨çœŸå®æ¨ç†æ­¥éª¤çš„ä¼˜åŠ¿ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šG$^2$RPO-Aé€šè¿‡å°†çœŸå®æ¨ç†æ­¥éª¤å¼•å…¥å›æ»šè½¨è¿¹ï¼Œå¢å¼ºå°å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶ä¸”èƒ½å¤Ÿæ ¹æ®è®­ç»ƒåŠ¨æ€è‡ªé€‚åº”è°ƒæ•´å¼•å¯¼å¼ºåº¦ï¼Œä»¥å®ç°æ›´å¥½çš„å­¦ä¹ æ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¼•å¯¼ç­–ç•¥ç”Ÿæˆã€åŠ¨æ€è°ƒæ•´æœºåˆ¶å’Œæ¨¡å‹è®­ç»ƒä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚å¼•å¯¼ç­–ç•¥ç”Ÿæˆè´Ÿè´£åˆ›å»ºæ¨ç†æ­¥éª¤ï¼ŒåŠ¨æ€è°ƒæ•´æœºåˆ¶æ ¹æ®æ¨¡å‹åé¦ˆè°ƒæ•´å¼•å¯¼å¼ºåº¦ï¼Œæ¨¡å‹è®­ç»ƒåˆ™åˆ©ç”¨è¿™äº›å¼•å¯¼ä¿¡æ¯è¿›è¡Œä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šG$^2$RPO-Açš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶è‡ªé€‚åº”å¼•å¯¼æœºåˆ¶ï¼Œèƒ½å¤Ÿæ ¹æ®æ¨¡å‹çš„è®­ç»ƒçŠ¶æ€å®æ—¶è°ƒæ•´å¼•å¯¼å¼ºåº¦ï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•çš„é™æ€å¼•å¯¼ç­–ç•¥å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒG$^2$RPO-Aé‡‡ç”¨äº†åŠ¨æ€æŸå¤±å‡½æ•°ï¼Œç»“åˆäº†æ¨¡å‹çš„åé¦ˆä¿¡æ¯ï¼Œç¡®ä¿å¼•å¯¼å¼ºåº¦çš„è°ƒæ•´èƒ½å¤Ÿæœ‰æ•ˆä¿ƒè¿›æ¨¡å‹çš„å­¦ä¹ ã€‚åŒæ—¶ï¼Œç½‘ç»œç»“æ„ä¸Šä¹Ÿè¿›è¡Œäº†ä¼˜åŒ–ï¼Œä»¥æ›´å¥½åœ°å¤„ç†å¼•å¯¼ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒG$^2$RPO-Aåœ¨æ•°å­¦æ¨ç†å’Œä»£ç ç”Ÿæˆä»»åŠ¡ä¸Šç›¸è¾ƒäºä¼ ç»ŸGRPOæ–¹æ³•æœ‰æ˜¾è‘—æå‡ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ€§èƒ½æå‡å¹…åº¦è¶…è¿‡20%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€ç¼–ç¨‹è¾…åŠ©å’Œæ™ºèƒ½é—®ç­”ç³»ç»Ÿç­‰ã€‚é€šè¿‡æå‡å°å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼ŒG$^2$RPO-Aå¯ä»¥åœ¨è¿™äº›é¢†åŸŸä¸­æä¾›æ›´å‡†ç¡®çš„å›ç­”å’Œè§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement Learning with Verifiable Rewards (RLVR) has markedly enhanced the reasoning abilities of large language models (LLMs). Its success, however, largely depends on strong base models with rich world knowledge, yielding only modest improvements for small-size language models (SLMs). To address this limitation, we investigate Guided GRPO, which injects ground-truth reasoning steps into roll-out trajectories to compensate for SLMs' inherent weaknesses. Through a comprehensive study of various guidance configurations, we find that naively adding guidance delivers limited gains. These insights motivate G$^2$RPO-A, an adaptive algorithm that automatically adjusts guidance strength in response to the model's evolving training dynamics. Experiments on mathematical reasoning and code-generation benchmarks confirm that G$^2$RPO-A substantially outperforms vanilla GRPO. Our code and models are available at https://github.com/T-Lab-CUHKSZ/G2RPO-A.

