---
layout: default
title: Reinforcement Learning with Rubric Anchors
---

# Reinforcement Learning with Rubric Anchors

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.12790" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.12790v1</a>
  <a href="https://arxiv.org/pdf/2508.12790.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.12790v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.12790v1', 'Reinforcement Learning with Rubric Anchors')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zenan Huang, Yihong Zhuang, Guoshan Lu, Zeyu Qin, Haokai Xu, Tianyu Zhao, Ru Peng, Jiaqi Hu, Zhanming Shen, Xiaomeng Hu, Xijun Gu, Peiyi Tu, Jiaxin Liu, Wenyu Chen, Yuzhuo Fu, Zhiting Fan, Yanmei Gu, Yuanyuan Wang, Zhengkai Yang, Jianguo Li, Junbo Zhao

**åˆ†ç±»**: cs.AI, cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-18

**å¤‡æ³¨**: technical report

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºè¯„åˆ†æ ‡å‡†çš„å¼ºåŒ–å­¦ä¹ ä»¥è§£å†³å¼€æ”¾ä»»åŠ¡çš„è¯„ä¼°é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `è¯„åˆ†æ ‡å‡†` `å¤§å‹è¯­è¨€æ¨¡å‹` `è‡ªåŠ¨è¯„åˆ†` `å¼€æ”¾ä»»åŠ¡` `æ¨¡å‹è®­ç»ƒ` `äººæœºäº¤äº’`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„RLVRæ–¹æ³•ä¸»è¦ä¾èµ–äºå¯éªŒè¯çš„å¥–åŠ±ä¿¡å·ï¼Œé™åˆ¶äº†å…¶åœ¨å¼€æ”¾ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚
2. æœ¬æ–‡é€šè¿‡å¼•å…¥åŸºäºè¯„åˆ†æ ‡å‡†çš„å¥–åŠ±ï¼Œæ„å»ºäº†ä¸€ä¸ªåŒ…å«è¶…è¿‡10,000ä¸ªè¯„åˆ†æ ‡å‡†çš„ç³»ç»Ÿï¼Œä»¥æ”¯æŒä¸»è§‚è¾“å‡ºçš„è‡ªåŠ¨è¯„åˆ†ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒQwen-30B-A3Bæ¨¡å‹åœ¨å¼€æ”¾ä»»åŠ¡åŸºå‡†ä¸Šæå‡äº†5.2%ï¼Œè¶…è¶Šäº†671Bçš„DeepSeek-V3æ¨¡å‹ï¼Œä¸”ä¿æŒäº†æ¨ç†èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰å·²æˆä¸ºå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¼ºå¤§èŒƒå¼ï¼Œä½†å…¶ä¸»è¦å±€é™äºå…·æœ‰è‡ªåŠ¨å¯æ£€æŸ¥ç»“æœçš„é¢†åŸŸã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡é€šè¿‡å¼•å…¥åŸºäºè¯„åˆ†æ ‡å‡†çš„å¥–åŠ±ï¼Œæ‰©å±•äº†RLVRçš„åº”ç”¨èŒƒå›´ï¼Œæ„å»ºäº†è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„è¯„åˆ†æ ‡å‡†å¥–åŠ±ç³»ç»Ÿï¼ŒåŒ…å«è¶…è¿‡10,000ä¸ªè¯„åˆ†æ ‡å‡†ã€‚æˆ‘ä»¬æå‡ºçš„Qwen-30B-A3Bæ¨¡å‹åœ¨å¼€æ”¾ä»»åŠ¡åŸºå‡†ä¸Šå–å¾—äº†æ˜¾è‘—æå‡ï¼Œå°¤å…¶æ˜¯åœ¨æ–‡ç§‘é¢†åŸŸï¼Œä¸”åœ¨ä¿æŒä¸€èˆ¬æ€§å’Œæ¨ç†èƒ½åŠ›çš„åŒæ—¶ï¼Œæä¾›äº†æ›´ç»†è‡´çš„é£æ ¼æ§åˆ¶ã€‚æˆ‘ä»¬åˆ†äº«äº†è¯„åˆ†æ ‡å‡†æ„å»ºå’Œè®­ç»ƒçš„å…³é”®ç»éªŒï¼Œå¹¶è®¨è®ºäº†å±€é™æ€§å’Œæœªæ¥çš„å‘å¸ƒè®¡åˆ’ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰RLVRæ–¹æ³•åœ¨å¼€æ”¾ä»»åŠ¡ä¸­æ— æ³•æœ‰æ•ˆè¯„ä¼°ä¸»è§‚è¾“å‡ºçš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•ä¾èµ–äºè‡ªåŠ¨å¯æ£€æŸ¥çš„ç»“æœï¼Œé™åˆ¶äº†å…¶åº”ç”¨èŒƒå›´ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥åŸºäºè¯„åˆ†æ ‡å‡†çš„å¥–åŠ±æœºåˆ¶ï¼Œæ„å»ºä¸€ä¸ªç»“æ„åŒ–ä¸”å¯è§£é‡Šçš„è¯„åˆ†ç³»ç»Ÿï¼Œä»¥æ”¯æŒå¯¹ä¸»è§‚è¾“å‡ºçš„è‡ªåŠ¨åŒ–è¯„ä¼°ï¼Œä»è€Œæ‰©å±•RLVRçš„åº”ç”¨åœºæ™¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬è¯„åˆ†æ ‡å‡†çš„è®¾è®¡ã€æ•°æ®é€‰æ‹©ã€æ¨¡å‹è®­ç»ƒç­‰å¤šä¸ªé˜¶æ®µï¼Œé‡ç‚¹åœ¨äºå¦‚ä½•å°†è¯„åˆ†æ ‡å‡†æœ‰æ•ˆæ•´åˆåˆ°å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ä¸­ã€‚

**å…³é”®åˆ›æ–°**ï¼šæ„å»ºäº†è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„è¯„åˆ†æ ‡å‡†å¥–åŠ±ç³»ç»Ÿï¼ŒåŒ…å«æ¥è‡ªäººç±»ã€LLMsåŠå…¶æ··åˆçš„è¯„åˆ†æ ‡å‡†ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨å¼€æ”¾ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ï¼Œä»¥ç¡®ä¿è¯„åˆ†æ ‡å‡†çš„æœ‰æ•ˆæ€§å’Œæ¨¡å‹çš„å“åº”é£æ ¼æ§åˆ¶ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒQwen-30B-A3Bæ¨¡å‹åœ¨å¼€æ”¾ä»»åŠ¡åŸºå‡†ä¸Šæå‡äº†5.2%ï¼Œå°¤å…¶åœ¨æ–‡ç§‘é¢†åŸŸè¡¨ç°çªå‡ºï¼Œè¶…è¶Šäº†671Bçš„DeepSeek-V3æ¨¡å‹2.4%ã€‚è¯¥æ–¹æ³•è¿˜æä¾›äº†æ›´ç»†è‡´çš„é£æ ¼æ§åˆ¶ï¼Œå‡å°‘äº†â€œAIå¼â€è¯­è°ƒï¼Œä½¿è¾“å‡ºæ›´å…·äººç±»è¡¨è¾¾ç‰¹å¾ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€å†…å®¹ç”Ÿæˆå’Œäººæœºäº¤äº’ç­‰ï¼Œèƒ½å¤Ÿä¸ºä¸»è§‚æ€§è¾ƒå¼ºçš„ä»»åŠ¡æä¾›æ›´ä¸ºå¯é çš„è‡ªåŠ¨è¯„åˆ†æœºåˆ¶ï¼Œæå‡ç”¨æˆ·ä½“éªŒå’Œæ¨¡å‹çš„å®ç”¨æ€§ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½åœ¨æ›´å¤šå¼€æ”¾ä»»åŠ¡ä¸­å¾—åˆ°åº”ç”¨ï¼Œæ¨åŠ¨äººå·¥æ™ºèƒ½åœ¨å¤æ‚åœºæ™¯ä¸­çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a powerful paradigm for enhancing Large Language Models (LLMs), exemplified by the success of OpenAI's o-series. In RLVR, rewards are derived from verifiable signals-such as passing unit tests in code generation or matching correct answers in mathematical reasoning. While effective, this requirement largely confines RLVR to domains with automatically checkable outcomes. To overcome this, we extend the RLVR paradigm to open-ended tasks by integrating rubric-based rewards, where carefully designed rubrics serve as structured, model-interpretable criteria for automatic scoring of subjective outputs. We construct, to our knowledge, the largest rubric reward system to date, with over 10,000 rubrics from humans, LLMs, or a hybrid human-LLM collaboration. Implementing rubric-based RL is challenging; we tackle these issues with a clear framework and present an open-sourced Qwen-30B-A3B model with notable gains: 1) With only 5K+ samples, our system improves by +5.2% on open-ended benchmarks (especially humanities), outperforming a 671B DeepSeek-V3 model by +2.4%, while preserving general and reasoning abilities. 2) Our method provides fine-grained stylistic control, using rubrics as anchors to mitigate the "AI-like" tone and produce more human-like, expressive responses. We share key lessons in rubric construction, data selection, and training, and discuss limitations and future releases.

