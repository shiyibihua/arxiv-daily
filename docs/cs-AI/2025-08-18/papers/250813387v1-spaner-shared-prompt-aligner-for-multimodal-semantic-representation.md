---
layout: default
title: SPANER: Shared Prompt Aligner for Multimodal Semantic Representation
---

# SPANER: Shared Prompt Aligner for Multimodal Semantic Representation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.13387" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.13387v1</a>
  <a href="https://arxiv.org/pdf/2508.13387.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.13387v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.13387v1', 'SPANER: Shared Prompt Aligner for Multimodal Semantic Representation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Thye Shan Ng, Caren Soyeon Han, Eun-Jung Holden

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSPANERä»¥è§£å†³å¤šæ¨¡æ€è¯­ä¹‰è¡¨ç¤ºçš„å­¤ç«‹é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `å‚æ•°é«˜æ•ˆå¾®è°ƒ` `å…±äº«æç¤ºæœºåˆ¶` `è¯­ä¹‰è¡¨ç¤º` `å°‘æ ·æœ¬æ£€ç´¢` `è·¨æ¨¡æ€å¯¹é½` `åµŒå…¥ç©ºé—´`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ¨¡æ€å­¦ä¹ æ–¹æ³•å¤šèšç„¦äºä»»åŠ¡ç‰¹å®šçš„æ€§èƒ½æå‡ï¼Œå¿½è§†äº†æ¨¡æ€é—´çš„åµŒå…¥ç»“æ„ï¼Œå¯¼è‡´æ¨¡æ€è¡¨ç¤ºå­¤ç«‹ã€‚
2. æœ¬æ–‡æå‡ºSPANERæ¡†æ¶ï¼Œé€šè¿‡å…±äº«æç¤ºæœºåˆ¶å°†ä¸åŒæ¨¡æ€çš„è¾“å…¥åµŒå…¥åˆ°ç»Ÿä¸€çš„è¯­ä¹‰ç©ºé—´ï¼Œå¢å¼ºè·¨æ¨¡æ€çš„è¯­ä¹‰å…³è”ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒSPANERåœ¨è§†è§‰-è¯­è¨€å’ŒéŸ³é¢‘-è§†è§‰ä»»åŠ¡ä¸­å®ç°äº†ç«äº‰åŠ›çš„å°‘æ ·æœ¬æ£€ç´¢æ€§èƒ½ï¼Œä¸”ä¿æŒäº†é«˜è¯­ä¹‰ä¸€è‡´æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤šæ¨¡æ€å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰çš„è¿›å±•æ˜¾è‘—æå‡äº†ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ï¼Œå¦‚å°‘æ ·æœ¬æ£€ç´¢ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¤§å¤šå…³æ³¨ä»»åŠ¡ç‰¹å®šçš„æå‡ï¼Œå¿½è§†äº†å¤šæ¨¡æ€åµŒå…¥ç©ºé—´çš„ç»“æ„ï¼Œå¯¼è‡´æ¨¡æ€ç‰¹å®šçš„è¡¨ç¤ºå¾€å¾€ç›¸äº’å­¤ç«‹ï¼Œé™åˆ¶äº†è·¨æ¨¡æ€çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†å…±äº«æç¤ºå¯¹é½å™¨ï¼ˆSPANERï¼‰ï¼Œä¸€ä¸ªæ¨¡æ€æ— å…³çš„PEFTæ¡†æ¶ï¼Œæ—¨åœ¨å°†æ¥è‡ªä¸åŒæ¨¡æ€çš„è¾“å…¥åµŒå…¥åˆ°ç»Ÿä¸€çš„è¯­ä¹‰ç©ºé—´ã€‚SPANERçš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªå…±äº«æç¤ºæœºåˆ¶ï¼Œä½œä¸ºæ¦‚å¿µé”šç‚¹ï¼Œä½¿å¾—è¯­ä¹‰ç›¸å…³çš„å®ä¾‹èƒ½å¤Ÿåœ¨ç©ºé—´ä¸Šèšåˆï¼Œæ— è®ºå…¶æ¨¡æ€å¦‚ä½•ã€‚é€šè¿‡åœ¨è§†è§‰-è¯­è¨€å’ŒéŸ³é¢‘-è§†è§‰åŸºå‡†ä¸Šçš„å…¨é¢å®éªŒï¼ŒSPANERå±•ç¤ºäº†ç«äº‰åŠ›çš„å°‘æ ·æœ¬æ£€ç´¢æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†å­¦ä¹ åµŒå…¥ç©ºé—´çš„é«˜è¯­ä¹‰ä¸€è‡´æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤šæ¨¡æ€å­¦ä¹ æ–¹æ³•ä¸­æ¨¡æ€ç‰¹å®šè¡¨ç¤ºç›¸äº’å­¤ç«‹çš„é—®é¢˜ï¼Œè¿™é™åˆ¶äº†è·¨æ¨¡æ€çš„æ³›åŒ–èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSPANERé€šè¿‡å¼•å…¥å…±äº«æç¤ºæœºåˆ¶ï¼Œä½œä¸ºæ¦‚å¿µé”šç‚¹ï¼Œä½¿å¾—ä¸åŒæ¨¡æ€çš„è¯­ä¹‰ç›¸å…³å®ä¾‹èƒ½å¤Ÿåœ¨åµŒå…¥ç©ºé—´ä¸­èšåˆï¼Œä»è€Œå®ç°æ¨¡æ€æ— å…³çš„è¯­ä¹‰è¡¨ç¤ºã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSPANERçš„æ•´ä½“æ¶æ„åŒ…æ‹¬è¾“å…¥æ¨¡å—ã€å…±äº«æç¤ºç”Ÿæˆæ¨¡å—å’ŒåµŒå…¥ç©ºé—´å¯¹é½æ¨¡å—ã€‚è¾“å…¥æ¨¡å—è´Ÿè´£æ¥æ”¶ä¸åŒæ¨¡æ€çš„æ•°æ®ï¼Œæç¤ºç”Ÿæˆæ¨¡å—ç”Ÿæˆå…±äº«æç¤ºï¼ŒåµŒå…¥ç©ºé—´å¯¹é½æ¨¡å—åˆ™ç¡®ä¿ä¸åŒæ¨¡æ€çš„è¡¨ç¤ºåœ¨è¯­ä¹‰ä¸Šç›¸äº’å…³è”ã€‚

**å…³é”®åˆ›æ–°**ï¼šSPANERçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶å…±äº«æç¤ºæœºåˆ¶ï¼Œä½¿å¾—æ¨¡æ€é—´çš„è¡¨ç¤ºèƒ½å¤Ÿåœ¨åŒä¸€è¯­ä¹‰ç©ºé—´ä¸­å¯¹é½ï¼Œè¿™ä¸€è®¾è®¡ä¸ä¼ ç»Ÿæ–¹æ³•ä»…ä¾èµ–äºè°ƒä¼˜é€‚é…å™¨æƒé‡çš„æ–¹å¼æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼ŒSPANERé‡‡ç”¨äº†çµæ´»çš„æç¤ºç”Ÿæˆç­–ç•¥ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šåˆ™å¼ºè°ƒäº†è¯­ä¹‰ä¸€è‡´æ€§ï¼Œç½‘ç»œç»“æ„ä¸Šä¿æŒäº†æ ¸å¿ƒæ¶æ„çš„å¯æ‰©å±•æ€§ï¼Œä»¥ä¾¿äºæœªæ¥é›†æˆæ›´å¤šæ¨¡æ€ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å…¨é¢çš„å®éªŒä¸­ï¼ŒSPANERåœ¨è§†è§‰-è¯­è¨€å’ŒéŸ³é¢‘-è§†è§‰åŸºå‡†ä¸Šå±•ç°äº†ä¼˜è¶Šçš„å°‘æ ·æœ¬æ£€ç´¢æ€§èƒ½ï¼Œç›¸è¾ƒäºç°æœ‰åŸºçº¿æ–¹æ³•ï¼Œæ€§èƒ½æå‡å¹…åº¦æ˜¾è‘—ï¼Œå…·ä½“æ•°æ®æœªæä¾›ï¼Œä½†å®éªŒç»“æœè¡¨æ˜å…¶åœ¨ä¿æŒè¯­ä¹‰ä¸€è‡´æ€§æ–¹é¢è¡¨ç°çªå‡ºã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SPANERçš„ç ”ç©¶æˆæœåœ¨å¤šæ¨¡æ€å­¦ä¹ é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦èåˆè§†è§‰ã€è¯­è¨€å’ŒéŸ³é¢‘ä¿¡æ¯çš„ä»»åŠ¡ä¸­ï¼Œå¦‚æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨å†…å®¹ç”Ÿæˆå’Œè·¨æ¨¡æ€æ£€ç´¢ç­‰ã€‚å…¶é«˜æ•ˆçš„åµŒå…¥å¯¹é½èƒ½åŠ›å°†æ¨åŠ¨å¤šæ¨¡æ€ç³»ç»Ÿçš„æ™ºèƒ½åŒ–å‘å±•ï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in multimodal Parameter-Efficient Fine-Tuning (PEFT) have significantly improved performance on downstream tasks such as few-shot retrieval. However, most existing approaches focus on task-specific gains while neglecting the structure of the multimodal embedding space. As a result, modality-specific representations often remain isolated, limiting cross-modal generalisation. In this work, we introduce Shared Prompt AligNER (SPANER), a modality-agnostic PEFT framework designed to embed inputs from diverse modalities into a unified semantic space. At its core, SPANER employs a shared prompt mechanism that acts as a conceptual anchor, enabling semantically related instances to converge spatially regardless of modality. This shared prompt design is inherently extensible, supporting the seamless integration of additional modalities, such as audio, without altering the core architecture. Through comprehensive experiments across vision-language and audio-visual benchmarks, SPANER demonstrates competitive few-shot retrieval performance while preserving high semantic coherence in the learned embedding space. Our results highlight the importance of aligning embedding structures, rather than merely tuning adapter weights, for scalable multimodal learning.

