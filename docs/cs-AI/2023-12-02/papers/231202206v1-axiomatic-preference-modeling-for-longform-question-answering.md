---
layout: default
title: Axiomatic Preference Modeling for Longform Question Answering
---

# Axiomatic Preference Modeling for Longform Question Answering

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2312.02206" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2312.02206v1</a>
  <a href="https://arxiv.org/pdf/2312.02206.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2312.02206v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2312.02206v1', 'Axiomatic Preference Modeling for Longform Question Answering')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Corby Rosset, Guoqing Zheng, Victor Dibia, Ahmed Awadallah, Paul Bennett

**åˆ†ç±»**: cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2023-12-02

**å¤‡æ³¨**: Accepted to EMNLP 2023

**ğŸ”— ä»£ç /é¡¹ç›®**: [HUGGINGFACE](https://huggingface.co/corbyrosset/axiomatic_preference_model)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå…¬ç†åŒ–åå¥½å»ºæ¨¡çš„é•¿æ–‡æœ¬é—®ç­”æ–¹æ³•ï¼Œå°æ¨¡å‹æ€§èƒ½è¶…è¶ŠGPT-4ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åå¥½å»ºæ¨¡` `å…¬ç†åŒ–æ–¹æ³•` `é•¿æ–‡æœ¬é—®ç­”` `å¥–åŠ±æ¨¡å‹` `äººç±»åé¦ˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¥–åŠ±æ¨¡å‹ç¼ºä¹å¯¹äººç±»åå¥½èƒŒååŸåˆ™çš„ç›´æ¥ç†è§£ï¼Œé™åˆ¶äº†å…¶å¯¹é½äººç±»æ„å›¾çš„èƒ½åŠ›ã€‚
2. æå‡ºä¸€ç§å…¬ç†åŒ–æ¡†æ¶ï¼Œé€šè¿‡ç”Ÿæˆç¬¦åˆç‰¹å®šåŸåˆ™çš„åå¥½ä¿¡å·ï¼Œè®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼Œä½¿å…¶æ›´å¥½åœ°å¯¹é½äººç±»åå¥½ã€‚
3. å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨å…¬ç†åŒ–ä¿¡å·è®­ç»ƒçš„å°å‹åå¥½æ¨¡å‹åœ¨é•¿æ–‡æœ¬é—®ç­”åå¥½è¯„åˆ†ä»»åŠ¡ä¸Šä¼˜äºGPT-4ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¦‚GPT-4çš„å“è¶Šèƒ½åŠ›éƒ¨åˆ†æºäºåè®­ç»ƒè¿‡ç¨‹ï¼Œä¾‹å¦‚åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ï¼Œå…¶ä¸­äººç±»åå¥½è¢«ç¼–ç åˆ°å¥–åŠ±æ¨¡å‹ä¸­ã€‚ç„¶è€Œï¼Œè¿™äº›å¥–åŠ±æ¨¡å‹ï¼ˆRMsï¼‰é€šå¸¸ç¼ºä¹å…³äºåå¥½æ ‡æ³¨åŸå› æˆ–ä¾æ®åŸåˆ™çš„ç›´æ¥çŸ¥è¯†ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è¯†åˆ«æŒ‡å¯¼RMsæ›´å¥½å¯¹é½äººç±»åå¥½çš„åŸåˆ™ï¼Œå¹¶å¼€å‘ä¸€ä¸ªå…¬ç†åŒ–æ¡†æ¶æ¥ç”Ÿæˆä¸°å¯Œçš„åå¥½ä¿¡å·ä»¥æ”¯æŒè¿™äº›åŸåˆ™ã€‚æˆ‘ä»¬ä½¿ç”¨è¿™äº›å…¬ç†åŒ–ä¿¡å·æ¥è®­ç»ƒä¸€ä¸ªæ¨¡å‹ï¼Œç”¨äºå¯¹é•¿æ–‡æœ¬é—®é¢˜çš„ç­”æ¡ˆè¿›è¡Œè¯„åˆ†ã€‚æˆ‘ä»¬çš„æ–¹æ³•äº§ç”Ÿäº†ä¸€ä¸ªä»…çº¦2.2äº¿å‚æ•°çš„åå¥½æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨äººç±»æ ‡æ³¨çš„åå¥½æ ‡ç­¾ä¸Šæ¯”GPT-4æ›´ä¸€è‡´ã€‚æœ¬ç ”ç©¶çš„è´¡çŒ®åŒ…æ‹¬ï¼šè®­ç»ƒä¸€ä¸ªç‹¬ç«‹çš„åå¥½æ¨¡å‹ï¼Œå¯ä»¥åœ¨åŒä¸€å°ºåº¦ä¸Šå¯¹äººç±»å’ŒLLMç”Ÿæˆçš„ç­”æ¡ˆè¿›è¡Œè¯„åˆ†ï¼›å¼€å‘ä¸€ä¸ªå…¬ç†åŒ–æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆé’ˆå¯¹ç‰¹å®šåŸåˆ™å®šåˆ¶çš„è®­ç»ƒæ•°æ®å¯¹ï¼›ä»¥åŠè¡¨æ˜å°‘é‡çš„å…¬ç†åŒ–ä¿¡å·å¯ä»¥å¸®åŠ©å°æ¨¡å‹åœ¨åå¥½è¯„åˆ†æ–¹é¢ä¼˜äºGPT-4ã€‚æˆ‘ä»¬åœ¨Hugging Faceä¸Šå‘å¸ƒäº†æˆ‘ä»¬çš„æ¨¡å‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³é•¿æ–‡æœ¬é—®ç­”ä¸­ï¼Œå¥–åŠ±æ¨¡å‹éš¾ä»¥å‡†ç¡®æ•æ‰äººç±»åå¥½çš„é—®é¢˜ã€‚ç°æœ‰å¥–åŠ±æ¨¡å‹é€šå¸¸ç¼ºä¹å¯¹äººç±»åå¥½èƒŒååŸåˆ™çš„ç†è§£ï¼Œå¯¼è‡´æ¨¡å‹è®­ç»ƒæ•ˆç‡ä½ä¸‹ï¼Œä¸”éš¾ä»¥æ³›åŒ–åˆ°æœªè§è¿‡çš„åœºæ™¯ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–å¤§é‡äººå·¥æ ‡æ³¨æ•°æ®ï¼Œæˆæœ¬é«˜æ˜‚ï¼Œä¸”æ ‡æ³¨è´¨é‡éš¾ä»¥ä¿è¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å…¬ç†åŒ–æ–¹æ³•ï¼Œå°†äººç±»åå¥½åˆ†è§£ä¸ºä¸€ç³»åˆ—å¯è§£é‡Šçš„åŸåˆ™ï¼ˆå…¬ç†ï¼‰ï¼Œå¹¶åŸºäºè¿™äº›å…¬ç†ç”Ÿæˆè®­ç»ƒæ•°æ®ã€‚é€šè¿‡åœ¨è¿™äº›å…¬ç†åŒ–æ•°æ®ä¸Šè®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿå­¦ä¹ åˆ°æ›´é²æ£’ã€æ›´ç¬¦åˆäººç±»ç›´è§‰çš„åå¥½è¡¨ç¤ºã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨å‡å°‘å¯¹å¤§é‡äººå·¥æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼Œå¹¶æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼š1) å®šä¹‰ä¸€ç»„åæ˜ äººç±»åå¥½çš„å…¬ç†ï¼›2) åŸºäºè¿™äº›å…¬ç†ï¼Œè‡ªåŠ¨ç”Ÿæˆå¤§é‡çš„è®­ç»ƒæ•°æ®å¯¹ï¼Œæ¯ä¸ªæ•°æ®å¯¹åŒ…å«ä¸¤ä¸ªç­”æ¡ˆï¼Œä»¥åŠå®ƒä»¬ä¹‹é—´çš„åå¥½å…³ç³»ï¼›3) ä½¿ç”¨ç”Ÿæˆçš„è®­ç»ƒæ•°æ®è®­ç»ƒä¸€ä¸ªåå¥½æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿå¯¹ç»™å®šçš„ç­”æ¡ˆè¿›è¡Œè¯„åˆ†ï¼Œå¹¶é¢„æµ‹å®ƒä»¬ä¹‹é—´çš„åå¥½å…³ç³»ï¼›4) ä½¿ç”¨è®­ç»ƒå¥½çš„åå¥½æ¨¡å‹å¯¹é•¿æ–‡æœ¬é—®ç­”ç³»ç»Ÿçš„è¾“å‡ºè¿›è¡Œæ’åºï¼Œé€‰æ‹©æœ€ç¬¦åˆäººç±»åå¥½çš„ç­”æ¡ˆã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†ä¸€ä¸ªå…¬ç†åŒ–æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆè®­ç»ƒæ•°æ®ã€‚ä¸ä¼ ç»Ÿçš„ä¾èµ–äººå·¥æ ‡æ³¨æ•°æ®çš„æ–¹æ³•ä¸åŒï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆå¤§é‡çš„ã€ç¬¦åˆç‰¹å®šåŸåˆ™çš„è®­ç»ƒæ•°æ®ï¼Œä»è€Œé™ä½äº†å¯¹äººå·¥æ ‡æ³¨çš„ä¾èµ–ï¼Œå¹¶æé«˜äº†æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜èƒ½å¤Ÿå¸®åŠ©æ¨¡å‹å­¦ä¹ åˆ°æ›´é²æ£’ã€æ›´ç¬¦åˆäººç±»ç›´è§‰çš„åå¥½è¡¨ç¤ºã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­ï¼Œå…¬ç†çš„é€‰æ‹©è‡³å…³é‡è¦ï¼Œéœ€è¦ä»”ç»†è€ƒè™‘å“ªäº›åŸåˆ™èƒ½å¤Ÿæœ‰æ•ˆåœ°åæ˜ äººç±»çš„åå¥½ã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡ä¹Ÿéœ€è¦ç‰¹åˆ«æ³¨æ„ï¼Œéœ€è¦ç¡®ä¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ åˆ°æ­£ç¡®çš„åå¥½å…³ç³»ã€‚æ­¤å¤–ï¼Œæ¨¡å‹ç»“æ„çš„é€‰æ‹©ä¹Ÿéœ€è¦æ ¹æ®å…·ä½“çš„ä»»åŠ¡è¿›è¡Œè°ƒæ•´ã€‚è®ºæ–‡ä¸­ä½¿ç”¨çš„åå¥½æ¨¡å‹æ˜¯ä¸€ä¸ªç›¸å¯¹è¾ƒå°çš„æ¨¡å‹ï¼ˆçº¦2.2äº¿å‚æ•°ï¼‰ï¼Œè¿™è¡¨æ˜å³ä½¿æ˜¯å°å‹æ¨¡å‹ï¼Œä¹Ÿå¯ä»¥é€šè¿‡æœ‰æ•ˆçš„è®­ç»ƒæ–¹æ³•ï¼Œè¾¾åˆ°ç”šè‡³è¶…è¿‡å¤§å‹æ¨¡å‹çš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨å…¬ç†åŒ–ä¿¡å·è®­ç»ƒçš„åå¥½æ¨¡å‹ï¼Œåœ¨é•¿æ–‡æœ¬é—®ç­”åå¥½è¯„åˆ†ä»»åŠ¡ä¸Šï¼Œä¸äººç±»æ ‡æ³¨çš„åå¥½æ ‡ç­¾çš„ä¸€è‡´æ€§è¶…è¿‡äº†GPT-4ã€‚è¯¥æ¨¡å‹ä»…åŒ…å«çº¦2.2äº¿å‚æ•°ï¼Œè¿œå°äºGPT-4ï¼Œè¿™è¡¨æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨å°‘é‡æ•°æ®è®­ç»ƒå‡ºé«˜æ€§èƒ½çš„æ¨¡å‹ã€‚è¿™ä¸€ç»“æœçªå‡ºäº†å…¬ç†åŒ–å»ºæ¨¡åœ¨åå¥½å­¦ä¹ ä¸­çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦å¯¹æ–‡æœ¬è¿›è¡Œæ’åºå’Œé€‰æ‹©çš„åœºæ™¯ï¼Œä¾‹å¦‚æœç´¢å¼•æ“ã€æ¨èç³»ç»Ÿã€å¯¹è¯ç³»ç»Ÿç­‰ã€‚é€šè¿‡ä½¿ç”¨å…¬ç†åŒ–åå¥½æ¨¡å‹ï¼Œå¯ä»¥æ›´å‡†ç¡®åœ°æ•æ‰ç”¨æˆ·çš„æ„å›¾ï¼Œä»è€Œæä¾›æ›´ä¸ªæ€§åŒ–ã€æ›´ç¬¦åˆç”¨æˆ·éœ€æ±‚çš„ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥æ‰©å±•åˆ°å…¶ä»–é¢†åŸŸï¼Œä¾‹å¦‚å›¾åƒã€è§†é¢‘ç­‰ï¼Œä»¥å®ç°æ›´å¹¿æ³›çš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The remarkable abilities of large language models (LLMs) like GPT-4 partially stem from post-training processes like Reinforcement Learning from Human Feedback (RLHF) involving human preferences encoded in a reward model. However, these reward models (RMs) often lack direct knowledge of why, or under what principles, the preferences annotations were made. In this study, we identify principles that guide RMs to better align with human preferences, and then develop an axiomatic framework to generate a rich variety of preference signals to uphold them. We use these axiomatic signals to train a model for scoring answers to longform questions. Our approach yields a Preference Model with only about 220M parameters that agrees with gold human-annotated preference labels more often than GPT-4. The contributions of this work include: training a standalone preference model that can score human- and LLM-generated answers on the same scale; developing an axiomatic framework for generating training data pairs tailored to certain principles; and showing that a small amount of axiomatic signals can help small models outperform GPT-4 in preference scoring. We release our model on huggingface: https://huggingface.co/corbyrosset/axiomatic_preference_model

