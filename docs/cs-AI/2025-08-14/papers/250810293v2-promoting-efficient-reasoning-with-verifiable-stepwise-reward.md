---
layout: default
title: Promoting Efficient Reasoning with Verifiable Stepwise Reward
---

# Promoting Efficient Reasoning with Verifiable Stepwise Reward

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.10293" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.10293v2</a>
  <a href="https://arxiv.org/pdf/2508.10293.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.10293v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.10293v2', 'Promoting Efficient Reasoning with Verifiable Stepwise Reward')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chuhuai Yue, Chengqi Dong, Yinan Gao, Hang He, Jiajun Chai, Guojun Yin, Wei Lin

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-14 (æ›´æ–°: 2025-08-16)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¯éªŒè¯çš„é€æ­¥å¥–åŠ±æœºåˆ¶ä»¥è§£å†³æ¨ç†æ¨¡å‹è¿‡åº¦æ€è€ƒé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ¨ç†æ¨¡å‹` `é€æ­¥å¥–åŠ±` `å¼ºåŒ–å­¦ä¹ ` `æ•ˆç‡æå‡` `æ•°å­¦æ¨ç†` `å¯éªŒè¯æœºåˆ¶` `è®¡ç®—èµ„æº`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ¨ç†æ¨¡å‹åœ¨ç®€å•é—®é¢˜ä¸Šå¸¸å¸¸è¿‡åº¦æ€è€ƒï¼Œå¯¼è‡´è®¡ç®—æ•ˆç‡ä½ä¸‹ã€‚
2. æå‡ºäº†ä¸€ç§åŸºäºè§„åˆ™çš„å¯éªŒè¯é€æ­¥å¥–åŠ±æœºåˆ¶ï¼Œé€šè¿‡å¯¹ä¸­é—´çŠ¶æ€çš„è¡¨ç°è¿›è¡Œå¥–åŠ±ï¼Œé¼“åŠ±æœ‰æ•ˆæ¨ç†æ­¥éª¤ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒæ¨ç†æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—å‡å°‘äº†è¾“å‡ºé•¿åº¦ï¼Œæå‡äº†æ•ˆç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå¾—ç›Šäºå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ã€‚ç„¶è€Œï¼ŒLRMså¸¸å¸¸é¢ä¸´è¿‡åº¦æ€è€ƒçš„é—®é¢˜ï¼Œåœ¨ç®€å•é—®é¢˜ä¸Šæ¶ˆè€—è¿‡å¤šè®¡ç®—èµ„æºï¼Œé™ä½äº†æ•ˆç‡ã€‚ç°æœ‰çš„é«˜æ•ˆæ¨ç†æ–¹æ³•é€šå¸¸éœ€è¦å‡†ç¡®çš„ä»»åŠ¡è¯„ä¼°æ¥é¢„è®¾ä»¤ç‰Œé¢„ç®—æˆ–é€‰æ‹©æ¨ç†æ¨¡å¼ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬çš„çµæ´»æ€§å’Œå¯é æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŸºäºè§„åˆ™çš„å¯éªŒè¯é€æ­¥å¥–åŠ±æœºåˆ¶ï¼ˆVSRMï¼‰ï¼Œæ ¹æ®æ¨ç†è½¨è¿¹ä¸­é—´çŠ¶æ€çš„è¡¨ç°åˆ†é…å¥–åŠ±ï¼Œä»è€Œæœ‰æ•ˆæŠ‘åˆ¶æ— æ•ˆæ­¥éª¤ï¼Œé¼“åŠ±æœ‰æ•ˆæ¨ç†ï¼Œæ˜¾è‘—æé«˜äº†æ•ˆç‡ä¸å‡†ç¡®æ€§çš„å¹³è¡¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ ‡å‡†æ•°å­¦æ¨ç†åŸºå‡†ä¸Šå®ç°äº†è¾“å‡ºé•¿åº¦çš„æ˜¾è‘—å‡å°‘ï¼ŒåŒæ—¶ä¿æŒäº†åŸæœ‰çš„æ¨ç†æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹æ¨ç†æ¨¡å‹åœ¨ç®€å•é—®é¢˜ä¸Šè¿‡åº¦æ€è€ƒçš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•ä¾èµ–äºå‡†ç¡®çš„ä»»åŠ¡è¯„ä¼°ï¼Œå¯¼è‡´çµæ´»æ€§ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºçš„å¯éªŒè¯é€æ­¥å¥–åŠ±æœºåˆ¶ï¼ˆVSRMï¼‰é€šè¿‡å¯¹æ¨ç†è¿‡ç¨‹ä¸­çš„ä¸­é—´çŠ¶æ€è¿›è¡Œå¥–åŠ±ï¼Œé¼“åŠ±æœ‰æ•ˆæ­¥éª¤ï¼ŒæŠ‘åˆ¶æ— æ•ˆæ­¥éª¤ï¼Œä»è€Œæé«˜æ¨ç†æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•é›†æˆäº†VSRMä¸PPOå’ŒReinforce++ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­åŠ¨æ€è¯„ä¼°ä¸­é—´çŠ¶æ€çš„è¡¨ç°ï¼Œå½¢æˆä¸€ä¸ªåé¦ˆå¾ªç¯ä»¥ä¼˜åŒ–æ¨ç†ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šVSRMæ˜¯æœ¬ç ”ç©¶çš„æ ¸å¿ƒåˆ›æ–°ç‚¹ï¼Œå®ƒé€šè¿‡é€æ­¥å¥–åŠ±æœºåˆ¶ç›´æ¥é’ˆå¯¹æ¨ç†è¿‡ç¨‹ä¸­çš„æœ‰æ•ˆæ€§è¿›è¡Œä¼˜åŒ–ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œå…·æœ‰æ›´é«˜çš„çµæ´»æ€§å’Œé€‚åº”æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®ç°ä¸­ï¼ŒVSRMè®¾è®¡äº†ç‰¹å®šçš„å¥–åŠ±å‡½æ•°ï¼Œç»“åˆäº†ä¸­é—´çŠ¶æ€çš„è¡¨ç°è¯„ä¼°ï¼Œç¡®ä¿äº†å¥–åŠ±çš„åŠæ—¶æ€§å’Œå‡†ç¡®æ€§ï¼ŒåŒæ—¶ä¼˜åŒ–äº†æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨VSRMåï¼Œæ¨¡å‹åœ¨æ ‡å‡†æ•°å­¦æ¨ç†åŸºå‡†AIME24å’ŒAIME25ä¸Šçš„è¾“å‡ºé•¿åº¦æ˜¾è‘—å‡å°‘ï¼ŒåŒæ—¶ä¿æŒäº†æ¨ç†æ€§èƒ½ï¼Œè¾¾åˆ°äº†æ•ˆç‡ä¸å‡†ç¡®æ€§çš„æœ€ä½³å¹³è¡¡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€è‡ªåŠ¨åŒ–æ¨ç†ç³»ç»Ÿå’Œæ™ºèƒ½é—®ç­”ç­‰ã€‚é€šè¿‡æé«˜æ¨ç†æ•ˆç‡ï¼Œèƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­èŠ‚çœè®¡ç®—èµ„æºï¼Œæå‡ç”¨æˆ·ä½“éªŒï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large reasoning models (LRMs) have recently achieved significant progress in complex reasoning tasks, aided by reinforcement learning with verifiable rewards. However, LRMs often suffer from overthinking, expending excessive computation on simple problems and reducing efficiency. Existing efficient reasoning methods typically require accurate task assessment to preset token budgets or select reasoning modes, which limits their flexibility and reliability. In this work, we revisit the essence of overthinking and identify that encouraging effective steps while penalizing ineffective ones is key to its solution. To this end, we propose a novel rule-based verifiable stepwise reward mechanism (VSRM), which assigns rewards based on the performance of intermediate states in the reasoning trajectory. This approach is intuitive and naturally fits the step-by-step nature of reasoning tasks. We conduct extensive experiments on standard mathematical reasoning benchmarks, including AIME24 and AIME25, by integrating VSRM with PPO and Reinforce++. Results show that our method achieves substantial output length reduction while maintaining original reasoning performance, striking an optimal balance between efficiency and accuracy. Further analysis of overthinking frequency and pass@k score before and after training demonstrates that our approach in deed effectively suppresses ineffective steps and encourages effective reasoning, fundamentally alleviating the overthinking problem. All code will be released upon acceptance.

