---
layout: default
title: Value Function Initialization for Knowledge Transfer and Jump-start in Deep Reinforcement Learning
---

# Value Function Initialization for Knowledge Transfer and Jump-start in Deep Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.09277" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.09277v1</a>
  <a href="https://arxiv.org/pdf/2508.09277.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.09277v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.09277v1', 'Value Function Initialization for Knowledge Transfer and Jump-start in Deep Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Soumia Mehimeh

**åˆ†ç±»**: cs.AI, cs.LG, cs.LO

**å‘å¸ƒæ—¥æœŸ**: 2025-08-12

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDQInitä»¥è§£å†³æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸­çš„ä»·å€¼å‡½æ•°åˆå§‹åŒ–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ä»·å€¼å‡½æ•°åˆå§‹åŒ–æ–¹æ³•åœ¨æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸­é¢ä¸´æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬çŠ¶æ€-åŠ¨ä½œç©ºé—´çš„è¿ç»­æ€§å’Œç¥ç»ç½‘ç»œçš„å™ªå£°è¿‘ä¼¼ã€‚
2. æœ¬æ–‡æå‡ºDQInitï¼Œé€šè¿‡é‡ç”¨ç´§å‡‘çš„è¡¨æ ¼Qå€¼ï¼Œç»“åˆå·²çŸ¥åº¦æœºåˆ¶ï¼Œå°†è½¬ç§»å€¼æŸ”å’Œæ•´åˆåˆ°å­¦ä¹ è¿‡ç¨‹ä¸­ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDQInitåœ¨å¤šä¸ªä»»åŠ¡ä¸­æå‡äº†å­¦ä¹ æ•ˆç‡å’Œç¨³å®šæ€§ï¼Œç›¸è¾ƒäºæ ‡å‡†åˆå§‹åŒ–å’Œç°æœ‰è½¬ç§»æŠ€æœ¯è¡¨ç°æ›´ä½³ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä»·å€¼å‡½æ•°åˆå§‹åŒ–ï¼ˆVFIï¼‰æ˜¯ä¸€ç§æœ‰æ•ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è·³å¯åŠ¨æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨å…ˆå‰ä»»åŠ¡çš„ä»·å€¼ä¼°è®¡æ¥åŠ é€Ÿå­¦ä¹ ã€‚å°½ç®¡åœ¨è¡¨æ ¼è®¾ç½®ä¸­è¯¥æ–¹æ³•å·²å¾—åˆ°å¹¿æ³›åº”ç”¨ï¼Œä½†åœ¨æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰ä¸­ï¼Œç”±äºçŠ¶æ€-åŠ¨ä½œç©ºé—´çš„è¿ç»­æ€§ã€ç¥ç»ç½‘ç»œçš„å™ªå£°è¿‘ä¼¼ä»¥åŠå­˜å‚¨æ‰€æœ‰è¿‡å»æ¨¡å‹çš„å®é™…å›°éš¾ï¼Œæ‰©å±•æ­¤æ–¹æ³•é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºDQInitï¼Œé€‚åº”æ€§åœ°å°†ä»·å€¼å‡½æ•°åˆå§‹åŒ–åº”ç”¨äºDRLï¼Œé‡ç”¨ä»å·²è§£å†³ä»»åŠ¡ä¸­æå–çš„ç´§å‡‘è¡¨æ ¼Qå€¼ä½œä¸ºå¯è½¬ç§»çŸ¥è¯†åº“ã€‚è¯¥æ–¹æ³•é‡‡ç”¨åŸºäºå·²çŸ¥åº¦çš„æœºåˆ¶ï¼Œå°†è¿™äº›è½¬ç§»å€¼æŸ”å’Œåœ°æ•´åˆåˆ°æœªå……åˆ†æ¢ç´¢çš„åŒºåŸŸï¼Œå¹¶é€æ¸å‘æ™ºèƒ½ä½“çš„å­¦ä¹ ä¼°è®¡è½¬ç§»ï¼Œé¿å…äº†å›ºå®šæ—¶é—´è¡°å‡çš„å±€é™æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDQInitåœ¨å¤šä¸ªè¿ç»­æ§åˆ¶ä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†æ—©æœŸå­¦ä¹ æ•ˆç‡ã€ç¨³å®šæ€§å’Œæ•´ä½“æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸­ä»·å€¼å‡½æ•°åˆå§‹åŒ–çš„æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†è¿ç»­çŠ¶æ€-åŠ¨ä½œç©ºé—´æ—¶æ•ˆæœä¸ä½³ï¼Œä¸”æ— æ³•æœ‰æ•ˆåˆ©ç”¨è¿‡å»çš„çŸ¥è¯†ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDQInité€šè¿‡é‡ç”¨å·²è§£å†³ä»»åŠ¡çš„ç´§å‡‘è¡¨æ ¼Qå€¼ï¼Œæ„å»ºå¯è½¬ç§»çš„çŸ¥è¯†åº“ï¼Œå¹¶é‡‡ç”¨åŸºäºå·²çŸ¥åº¦çš„æœºåˆ¶å°†è¿™äº›å€¼æ•´åˆåˆ°æ™ºèƒ½ä½“çš„å­¦ä¹ è¿‡ç¨‹ä¸­ï¼Œé€æ­¥å‘æ™ºèƒ½ä½“çš„å­¦ä¹ ä¼°è®¡è¿‡æ¸¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDQInitçš„æ•´ä½“æ¶æ„åŒ…æ‹¬çŸ¥è¯†åº“çš„æ„å»ºã€å·²çŸ¥åº¦æœºåˆ¶çš„åº”ç”¨ä»¥åŠä»·å€¼å‡½æ•°çš„åŠ¨æ€è°ƒæ•´ã€‚é¦–å…ˆæå–å·²è§£å†³ä»»åŠ¡çš„Qå€¼ï¼Œç„¶åæ ¹æ®æ™ºèƒ½ä½“çš„æ¢ç´¢æƒ…å†µåŠ¨æ€è°ƒæ•´è¿™äº›å€¼ã€‚

**å…³é”®åˆ›æ–°**ï¼šDQInitçš„ä¸»è¦åˆ›æ–°åœ¨äºä»…ä¾èµ–ä»·å€¼ä¼°è®¡è€Œéç­–ç•¥æˆ–ç¤ºèŒƒè¿›è¡ŒçŸ¥è¯†è½¬ç§»ï¼Œè¿™ç§æ–¹æ³•æœ‰æ•ˆç»“åˆäº†è·³å¯åŠ¨å¼ºåŒ–å­¦ä¹ å’Œç­–ç•¥è’¸é¦çš„ä¼˜ç‚¹ï¼ŒåŒæ—¶å…‹æœäº†å®ƒä»¬çš„ç¼ºé™·ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨DQInitä¸­ï¼Œå…³é”®è®¾è®¡åŒ…æ‹¬å¦‚ä½•æå–å’Œå­˜å‚¨Qå€¼ã€å·²çŸ¥åº¦æœºåˆ¶çš„å…·ä½“å®ç°ï¼Œä»¥åŠå¦‚ä½•åŠ¨æ€è°ƒæ•´è½¬ç§»å€¼ä»¥é€‚åº”æ™ºèƒ½ä½“çš„å­¦ä¹ è¿›ç¨‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Value function initialization (VFI) is an effective way to achieve a jumpstart in reinforcement learning (RL) by leveraging value estimates from prior tasks. While this approach is well established in tabular settings, extending it to deep reinforcement learning (DRL) poses challenges due to the continuous nature of the state-action space, the noisy approximations of neural networks, and the impracticality of storing all past models for reuse. In this work, we address these challenges and introduce DQInit, a method that adapts value function initialization to DRL. DQInit reuses compact tabular Q-values extracted from previously solved tasks as a transferable knowledge base. It employs a knownness-based mechanism to softly integrate these transferred values into underexplored regions and gradually shift toward the agent's learned estimates, avoiding the limitations of fixed time decay. Our approach offers a novel perspective on knowledge transfer in DRL by relying solely on value estimates rather than policies or demonstrations, effectively combining the strengths of jumpstart RL and policy distillation while mitigating their drawbacks. Experiments across multiple continuous control tasks demonstrate that DQInit consistently improves early learning efficiency, stability, and overall performance compared to standard initialization and existing transfer techniques.

