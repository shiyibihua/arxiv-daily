---
layout: default
title: Reducing Cognitive Overhead in Tool Use via Multi-Small-Agent Reinforcement Learning
---

# Reducing Cognitive Overhead in Tool Use via Multi-Small-Agent Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.08882" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.08882v4</a>
  <a href="https://arxiv.org/pdf/2508.08882.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.08882v4" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.08882v4', 'Reducing Cognitive Overhead in Tool Use via Multi-Small-Agent Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Dayu Wang, Jiaye Yang, Weikang Li, Jiahui Liang, Yang Li

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-12 (æ›´æ–°: 2025-10-11)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMSARLæ¡†æ¶ä»¥é™ä½å·¥å…·ä½¿ç”¨ä¸­çš„è®¤çŸ¥è´Ÿæ‹…**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ` `å¼ºåŒ–å­¦ä¹ ` `å·¥å…·ä½¿ç”¨` `è®¤çŸ¥è´Ÿæ‹…` `æ¨ç†è§£è€¦` `æ¨¡ä»¿å­¦ä¹ ` `è‡ªåŠ¨åŒ–ç¼–ç¨‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å·¥å…·é›†æˆæ¨ç†ç³»ç»Ÿå¾€å¾€ä¾èµ–å•ä¸€å¤§å‹æ¨¡å‹ï¼Œå¯¼è‡´è®¤çŸ¥è´Ÿæ‹…å¹²æ‰°å’Œåè°ƒä¸ç¨³å®šã€‚
2. MSARLæ¡†æ¶é€šè¿‡å°†æ¨ç†ä¸å·¥å…·ä½¿ç”¨è§£è€¦ï¼Œé‡‡ç”¨å¤šä¸ªå°å‹æ™ºèƒ½ä½“åä½œï¼Œæå‡äº†ç³»ç»Ÿçš„ç¨³å®šæ€§å’Œå‡†ç¡®æ€§ã€‚
3. åœ¨æ•°å­¦é—®é¢˜è§£å†³ä»»åŠ¡ä¸­ï¼ŒMSARLæ˜¾è‘—æé«˜äº†æ¨ç†ç¨³å®šæ€§å’Œæœ€ç»ˆç­”æ¡ˆçš„å‡†ç¡®ç‡ï¼Œç›¸è¾ƒäºå•æ™ºèƒ½ä½“åŸºçº¿æœ‰æ˜æ˜¾æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„è¿›å±•çªæ˜¾äº†é€šè¿‡åˆ†å·¥åˆä½œçš„ä¸“é—¨å°å‹æ™ºèƒ½ä½“çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å·¥å…·é›†æˆæ¨ç†ç³»ç»Ÿé€šå¸¸éµå¾ªå•æ™ºèƒ½ä½“èŒƒå¼ï¼Œè¿™å¯¼è‡´è®¤çŸ¥è´Ÿæ‹…å¹²æ‰°å’Œåè°ƒä¸ç¨³å®šã€‚æœ¬æ–‡æå‡ºäº†MSARLï¼Œä¸€ä¸ªå¤šå°å‹æ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ˜ç¡®å°†æ¨ç†ä¸å·¥å…·ä½¿ç”¨è§£è€¦ã€‚åœ¨MSARLä¸­ï¼Œæ¨ç†æ™ºèƒ½ä½“åˆ†è§£é—®é¢˜å¹¶è§„åˆ’å·¥å…·è°ƒç”¨ï¼Œè€Œå¤šä¸ªå·¥å…·æ™ºèƒ½ä½“ä¸“æ³¨äºç‰¹å®šå¤–éƒ¨å·¥å…·ï¼Œé€šè¿‡æ¨¡ä»¿å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ çš„ç»„åˆè¿›è¡Œè®­ç»ƒï¼Œé‡‡ç”¨è§’è‰²ç‰¹å®šçš„å¥–åŠ±ã€‚åœ¨æ•°å­¦é—®é¢˜è§£å†³ä¸ä»£ç æ‰§è¡Œçš„ä»»åŠ¡ä¸­ï¼ŒMSARLæ˜¾è‘—æé«˜äº†æ¨ç†çš„ç¨³å®šæ€§å’Œæœ€ç»ˆç­”æ¡ˆçš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ¶æ„å¯æ¨å¹¿åˆ°å¤šç§å·¥å…·ä½¿ç”¨ä»»åŠ¡ï¼Œå±•ç¤ºäº†å°å‹æ™ºèƒ½ä½“çš„è®¤çŸ¥è§’è‰²è§£è€¦æ˜¯å¤šæ™ºèƒ½ä½“AIè®¾è®¡çš„å¯æ‰©å±•è“å›¾ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å·¥å…·é›†æˆæ¨ç†ç³»ç»Ÿä¸­å› å•ä¸€å¤§å‹æ¨¡å‹å¯¼è‡´çš„è®¤çŸ¥è´Ÿæ‹…å¹²æ‰°å’Œåè°ƒä¸ç¨³å®šçš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨é•¿æ—¶é—´æ¨ç†å’Œç²¾ç¡®å·¥å…·æ“ä½œä¹‹é—´çš„äº¤æ›¿ä½¿ç”¨ä¸Šå­˜åœ¨æ˜¾è‘—æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMSARLæ¡†æ¶çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å°†æ¨ç†ä¸å·¥å…·ä½¿ç”¨è§£è€¦ï¼Œé‡‡ç”¨å¤šä¸ªå°å‹æ™ºèƒ½ä½“è¿›è¡Œåä½œã€‚æ¨ç†æ™ºèƒ½ä½“è´Ÿè´£é—®é¢˜åˆ†è§£å’Œå·¥å…·è°ƒç”¨è§„åˆ’ï¼Œè€Œå·¥å…·æ™ºèƒ½ä½“åˆ™ä¸“æ³¨äºç‰¹å®šå·¥å…·çš„æ“ä½œã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMSARLçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸€ä¸ªæ¨ç†æ™ºèƒ½ä½“å’Œå¤šä¸ªå·¥å…·æ™ºèƒ½ä½“ã€‚æ¨ç†æ™ºèƒ½ä½“è´Ÿè´£åˆ†æé—®é¢˜å¹¶åˆ¶å®šå·¥å…·è°ƒç”¨è®¡åˆ’ï¼Œå·¥å…·æ™ºèƒ½ä½“åˆ™é€šè¿‡æ¨¡ä»¿å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒï¼Œä¸“æ³¨äºå„è‡ªçš„å·¥å…·ã€‚

**å…³é”®åˆ›æ–°**ï¼šMSARLçš„ä¸»è¦åˆ›æ–°åœ¨äºé€šè¿‡å°å‹æ™ºèƒ½ä½“çš„è®¤çŸ¥è§’è‰²è§£è€¦ï¼Œæå‡äº†ç³»ç»Ÿçš„ç¨³å®šæ€§å’Œå‡†ç¡®æ€§ã€‚è¿™ä¸€è®¾è®¡ä¸ä¼ ç»Ÿçš„å•æ™ºèƒ½ä½“æ–¹æ³•æœ¬è´¨ä¸Šä¸åŒï¼Œåè€…å¾€å¾€åœ¨æ¨ç†å’Œå·¥å…·æ“ä½œä¹‹é—´éš¾ä»¥æœ‰æ•ˆåè°ƒã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨MSARLä¸­ï¼Œå·¥å…·æ™ºèƒ½ä½“çš„è®­ç»ƒç»“åˆäº†æ¨¡ä»¿å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ï¼Œé‡‡ç”¨è§’è‰²ç‰¹å®šçš„å¥–åŠ±æœºåˆ¶ï¼Œä»¥ç¡®ä¿æ¯ä¸ªæ™ºèƒ½ä½“èƒ½å¤Ÿä¸“æ³¨äºå…¶ç‰¹å®šçš„å·¥å…·æ“ä½œã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨æ•°å­¦é—®é¢˜è§£å†³ä¸ä»£ç æ‰§è¡Œçš„å®éªŒä¸­ï¼ŒMSARLæ¡†æ¶æ˜¾è‘—æé«˜äº†æ¨ç†çš„ç¨³å®šæ€§å’Œæœ€ç»ˆç­”æ¡ˆçš„å‡†ç¡®æ€§ï¼Œç›¸è¾ƒäºå•æ™ºèƒ½ä½“åŸºçº¿ï¼Œå‡†ç¡®ç‡æå‡å¹…åº¦è¾¾åˆ°XX%ï¼ˆå…·ä½“æ•°æ®æœªçŸ¥ï¼‰ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨åŒ–ç¼–ç¨‹ã€æ™ºèƒ½åŠ©æ‰‹å’Œå¤æ‚å†³ç­–æ”¯æŒç³»ç»Ÿç­‰ã€‚é€šè¿‡é™ä½å·¥å…·ä½¿ç”¨ä¸­çš„è®¤çŸ¥è´Ÿæ‹…ï¼ŒMSARLèƒ½å¤Ÿæå‡å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨å®é™…ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in multi-agent systems highlight the potential of specialized small agents that collaborate via division of labor. Existing tool-integrated reasoning systems, however, often follow a single-agent paradigm in which one large model interleaves long-horizon reasoning with precise tool operations, leading to cognitive-load interference and unstable coordination. We present MSARL, a Multi-Small-Agent Reinforcement Learning framework that explicitly decouples reasoning from tool use. In MSARL, a Reasoning Agent decomposes problems and plans tool invocations, while multiple Tool Agents specialize in specific external tools, each trained via a combination of imitation learning and reinforcement learning with role-specific rewards. On mathematical problem solving with code execution, MSARL significantly improves reasoning stability and final-answer accuracy over single-agent baselines. Moreover, the architecture generalizes to diverse tool-use tasks, demonstrating that cognitive-role decoupling with small agents is a scalable blueprint for multi-agent AI design.

