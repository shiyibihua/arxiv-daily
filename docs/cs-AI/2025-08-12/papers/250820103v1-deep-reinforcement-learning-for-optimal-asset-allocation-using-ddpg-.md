---
layout: default
title: Deep Reinforcement Learning for Optimal Asset Allocation Using DDPG with TiDE
---

# Deep Reinforcement Learning for Optimal Asset Allocation Using DDPG with TiDE

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.20103" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.20103v1</a>
  <a href="https://arxiv.org/pdf/2508.20103.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.20103v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.20103v1', 'Deep Reinforcement Learning for Optimal Asset Allocation Using DDPG with TiDE')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Rongwei Liu, Jin Zheng, John Cartlidge

**åˆ†ç±»**: q-fin.PM, cs.AI, cs.LG, q-fin.RM

**å‘å¸ƒæ—¥æœŸ**: 2025-08-12

**å¤‡æ³¨**: 10 pages, 3 figures, authors accepted manuscript, to appear in 24th International Conference on Modelling and Applied Simulation (MAS), Sep. 2025, Fes, Morocco

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºDDPGä¸TiDEçš„æ·±åº¦å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä»¥ä¼˜åŒ–èµ„äº§é…ç½®**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ·±åº¦å¼ºåŒ–å­¦ä¹ ` `èµ„äº§é…ç½®` `é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹` `æ—¶é—´åºåˆ—åˆ†æ` `é£é™©è°ƒæ•´æ”¶ç›Š` `åŠ¨æ€ç­–ç•¥` `å‡¯åˆ©å‡†åˆ™`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰èµ„äº§é…ç½®æ–¹æ³•ä¾èµ–äºä¸¥æ ¼çš„åˆ†å¸ƒå‡è®¾ï¼Œç¼ºä¹çµæ´»æ€§å’Œç¨³å¥æ€§ï¼Œéš¾ä»¥é€‚åº”é‡‘èå¸‚åœºçš„æ³¢åŠ¨æ€§ã€‚
2. æœ¬æ–‡æå‡ºå°†èµ„äº§é…ç½®é—®é¢˜è½¬åŒ–ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œåˆ©ç”¨æ·±åº¦å¼ºåŒ–å­¦ä¹ å’ŒTiDEå®ç°åŠ¨æ€å†³ç­–ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDDPG-TiDEåœ¨é£é™©è°ƒæ•´æ”¶ç›Šæ–¹é¢ä¼˜äºQå­¦ä¹ å’Œä¹°å…¥æŒæœ‰ç­–ç•¥ï¼Œå±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨é‡‘èå¸‚åœºä¸­ï¼Œé£é™©èµ„äº§ä¸æ— é£é™©èµ„äº§çš„æœ€ä½³é…ç½®ä¸€ç›´æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºä¸¥æ ¼çš„åˆ†å¸ƒå‡è®¾æˆ–éåŠ æ€§å¥–åŠ±æ¯”ç‡ï¼Œé™åˆ¶äº†å…¶ç¨³å¥æ€§å’Œé€‚ç”¨æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡å°†æœ€ä½³åŒèµ„äº§é…ç½®é—®é¢˜è¡¨è¿°ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ä¸­çš„åºåˆ—å†³ç­–ä»»åŠ¡ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ æœºåˆ¶åœ¨æ¨¡æ‹Ÿé‡‘èåœºæ™¯ä¸­å¼€å‘åŠ¨æ€ç­–ç•¥ã€‚æˆ‘ä»¬é‡‡ç”¨å‡¯åˆ©å‡†åˆ™å¹³è¡¡å³æ—¶å¥–åŠ±ä¿¡å·ä¸é•¿æœŸæŠ•èµ„ç›®æ ‡ï¼Œå¹¶å°†æ—¶é—´åºåˆ—å¯†é›†ç¼–ç å™¨ï¼ˆTiDEï¼‰é›†æˆåˆ°æ·±åº¦ç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦ï¼ˆDDPGï¼‰æ¡†æ¶ä¸­ã€‚å®è¯ç»“æœè¡¨æ˜ï¼ŒDDPG-TiDEä¼˜äºç®€å•çš„ç¦»æ•£åŠ¨ä½œQå­¦ä¹ æ¡†æ¶ï¼Œå¹¶ä¸”ç”Ÿæˆçš„é£é™©è°ƒæ•´æ”¶ç›Šé«˜äºè¢«åŠ¨çš„ä¹°å…¥æŒæœ‰ç­–ç•¥ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³é£é™©èµ„äº§ä¸æ— é£é™©èµ„äº§çš„æœ€ä½³é…ç½®é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•çš„ç—›ç‚¹åœ¨äºå…¶ä¾èµ–äºä¸¥æ ¼çš„åˆ†å¸ƒå‡è®¾ï¼Œé™åˆ¶äº†å…¶åœ¨åŠ¨æ€å¸‚åœºä¸­çš„åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å°†èµ„äº§é…ç½®é—®é¢˜å»ºæ¨¡ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œåˆ©ç”¨æ·±åº¦å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯DDPGæ¡†æ¶ï¼Œç»“åˆTiDEç¼–ç å™¨ï¼Œå½¢æˆåŠ¨æ€å†³ç­–ç­–ç•¥ï¼Œå…‹æœä¼ ç»Ÿæ–¹æ³•çš„å±€é™æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬çŠ¶æ€ç©ºé—´çš„å®šä¹‰ã€åŠ¨ä½œç©ºé—´çš„è®¾è®¡ã€å¥–åŠ±ä¿¡å·çš„æ„å»ºä»¥åŠç­–ç•¥ä¼˜åŒ–è¿‡ç¨‹ã€‚TiDEç”¨äºå¤„ç†æ—¶é—´åºåˆ—æ•°æ®ï¼ŒDDPGè´Ÿè´£ç­–ç•¥çš„å­¦ä¹ ä¸ä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šå°†TiDEé›†æˆåˆ°DDPGæ¡†æ¶ä¸­æ˜¯æœ¬æ–‡çš„ä¸»è¦åˆ›æ–°ç‚¹ï¼Œè¿™ä¸€è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰æ—¶é—´åºåˆ—æ•°æ®çš„ç‰¹å¾ï¼Œä»è€Œæå‡å†³ç­–è´¨é‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨å‡¯åˆ©å‡†åˆ™ä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œè®¾ç½®äº†é€‚å½“çš„è¶…å‚æ•°ä»¥å¹³è¡¡çŸ­æœŸä¸é•¿æœŸæ”¶ç›Šï¼Œç½‘ç»œç»“æ„é‡‡ç”¨äº†æ·±åº¦ç¥ç»ç½‘ç»œä»¥å¢å¼ºå­¦ä¹ èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒDDPG-TiDEåœ¨é£é™©è°ƒæ•´æ”¶ç›Šæ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„Qå­¦ä¹ æ¡†æ¶ï¼Œä¸”å…¶æ”¶ç›Šç‡é«˜äºè¢«åŠ¨çš„ä¹°å…¥æŒæœ‰ç­–ç•¥ï¼Œå±•ç¤ºäº†åœ¨åŠ¨æ€å¸‚åœºä¸­ä¼˜åŒ–èµ„äº§é…ç½®çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬é‡‘èæŠ•èµ„ã€èµ„äº§ç®¡ç†å’Œé£é™©æ§åˆ¶ç­‰ã€‚é€šè¿‡ä¼˜åŒ–èµ„äº§é…ç½®ç­–ç•¥ï¼ŒæŠ•èµ„è€…å¯ä»¥åœ¨æ³¢åŠ¨çš„å¸‚åœºä¸­å®ç°æ›´é«˜çš„é£é™©è°ƒæ•´æ”¶ç›Šï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The optimal asset allocation between risky and risk-free assets is a persistent challenge due to the inherent volatility in financial markets. Conventional methods rely on strict distributional assumptions or non-additive reward ratios, which limit their robustness and applicability to investment goals. To overcome these constraints, this study formulates the optimal two-asset allocation problem as a sequential decision-making task within a Markov Decision Process (MDP). This framework enables the application of reinforcement learning (RL) mechanisms to develop dynamic policies based on simulated financial scenarios, regardless of prerequisites. We use the Kelly criterion to balance immediate reward signals against long-term investment objectives, and we take the novel step of integrating the Time-series Dense Encoder (TiDE) into the Deep Deterministic Policy Gradient (DDPG) RL framework for continuous decision-making. We compare DDPG-TiDE with a simple discrete-action Q-learning RL framework and a passive buy-and-hold investment strategy. Empirical results show that DDPG-TiDE outperforms Q-learning and generates higher risk adjusted returns than buy-and-hold. These findings suggest that tackling the optimal asset allocation problem by integrating TiDE within a DDPG reinforcement learning framework is a fruitful avenue for further exploration.

