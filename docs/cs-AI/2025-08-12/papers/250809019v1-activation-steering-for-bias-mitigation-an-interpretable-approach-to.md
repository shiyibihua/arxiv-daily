---
layout: default
title: Activation Steering for Bias Mitigation: An Interpretable Approach to Safer LLMs
---

# Activation Steering for Bias Mitigation: An Interpretable Approach to Safer LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.09019" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.09019v1</a>
  <a href="https://arxiv.org/pdf/2508.09019.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.09019v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.09019v1', 'Activation Steering for Bias Mitigation: An Interpretable Approach to Safer LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shivam Dubey

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-12

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ¿€æ´»å¼•å¯¼æŠ€æœ¯ä»¥ç¼“è§£å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„åè§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `åè§ç¼“è§£` `æœºæ¢°è§£é‡Šå­¦` `æ¿€æ´»å¼•å¯¼` `å®æ—¶è°ƒæ•´` `ç¤¾äº¤åª’ä½“` `è‡ªåŠ¨åŒ–å®¢æœ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åè§ç¼“è§£æ–¹æ³•å¤šä¾èµ–æ•°æ®è¿‡æ»¤æˆ–åæœŸè°ƒèŠ‚ï¼Œæ— æ³•æ·±å…¥ç†è§£æ¨¡å‹å†…éƒ¨çš„åè§è¡¨ç°ã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡è®­ç»ƒçº¿æ€§æ¢é’ˆè¯†åˆ«æ¨¡å‹å†…éƒ¨æ¿€æ´»ä¸­çš„åè§ï¼Œå¹¶åˆ©ç”¨å¼•å¯¼å‘é‡å®æ—¶è°ƒæ•´ç”Ÿæˆå†…å®¹ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨åè§è¯†åˆ«ä¸Šæ¥è¿‘å®Œç¾å‡†ç¡®ç‡ï¼Œå¹¶æˆåŠŸå°†åè§ç”Ÿæˆå†…å®¹è½¬å‘æ›´ä¸­ç«‹çš„æ›¿ä»£æ–¹æ¡ˆã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç¤¾ä¼šç³»ç»Ÿä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œå®ƒä»¬å¯èƒ½ä¼šå»¶ç»­å’ŒåŠ å‰§æœ‰å®³åè§çš„é£é™©æˆä¸ºä¸€ä¸ªé‡è¦çš„å®‰å…¨é—®é¢˜ã€‚ä¼ ç»Ÿçš„åè§ç¼“è§£æ–¹æ³•é€šå¸¸ä¾èµ–äºæ•°æ®è¿‡æ»¤æˆ–åæœŸè¾“å‡ºè°ƒèŠ‚ï¼Œè¿™äº›æ–¹æ³•å°†æ¨¡å‹è§†ä¸ºä¸€ä¸ªä¸é€æ˜çš„é»‘ç®±ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å®Œæ•´çš„ç«¯åˆ°ç«¯ç³»ç»Ÿï¼Œåˆ©ç”¨æœºæ¢°è§£é‡Šå­¦æŠ€æœ¯ç›´æ¥è¯†åˆ«å’Œä¸»åŠ¨ç¼“è§£æ¨¡å‹å†…éƒ¨çš„åè§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆï¼Œè®­ç»ƒçº¿æ€§â€œæ¢é’ˆâ€ä»¥æ£€æµ‹æ¨¡å‹å†…éƒ¨æ¿€æ´»ä¸­çš„æ½œåœ¨åè§è¡¨ç¤ºï¼›å…¶æ¬¡ï¼Œé€šè¿‡å¯¹æ¯”åè§å’Œä¸­æ€§é™ˆè¿°çš„æ¿€æ´»æ¨¡å¼è®¡ç®—â€œå¼•å¯¼å‘é‡â€ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­å®æ—¶å¼•å¯¼æ¨¡å‹ç”Ÿæˆæ›´ä¸­ç«‹çš„å†…å®¹ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè¯¥æŠ€æœ¯æœ‰æ•ˆåœ°æ”¹å˜äº†åè§ç”Ÿæˆç»“æœï¼Œæœå‘æ›´ä¸­ç«‹çš„æ›¿ä»£æ–¹æ¡ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ä¸­åè§çš„è¯†åˆ«ä¸ç¼“è§£é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€å°†æ¨¡å‹è§†ä¸ºé»‘ç®±ï¼Œæ— æ³•æ·±å…¥åˆ†æå…¶å†…éƒ¨åè§è¡¨ç°ï¼Œå¯¼è‡´åè§é—®é¢˜éš¾ä»¥æœ‰æ•ˆè§£å†³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æœºæ¢°è§£é‡Šå­¦æŠ€æœ¯ï¼Œé€šè¿‡è®­ç»ƒçº¿æ€§æ¢é’ˆè¯†åˆ«æ¨¡å‹å†…éƒ¨çš„åè§è¡¨ç¤ºï¼Œå¹¶è®¡ç®—å¼•å¯¼å‘é‡ä»¥å®æ—¶è°ƒæ•´ç”Ÿæˆå†…å®¹ï¼Œä»è€Œä¸»åŠ¨ç¼“è§£åè§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åˆ†ä¸ºä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µï¼Œè®­ç»ƒçº¿æ€§æ¢é’ˆä»¥æ£€æµ‹æ¨¡å‹å†…éƒ¨æ¿€æ´»ä¸­çš„åè§è¡¨ç¤ºï¼›ç¬¬äºŒé˜¶æ®µï¼Œè®¡ç®—å¼•å¯¼å‘é‡å¹¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­åº”ç”¨ï¼Œä»¥å¼•å¯¼æ¨¡å‹ç”Ÿæˆæ›´ä¸­ç«‹çš„å†…å®¹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºé€šè¿‡å†…éƒ¨æ¿€æ´»çš„åˆ†æï¼Œç›´æ¥è¯†åˆ«å’Œç¼“è§£åè§ï¼Œè€Œä¸æ˜¯ä¾èµ–äºå¤–éƒ¨æ•°æ®è¿‡æ»¤æˆ–åæœŸè°ƒèŠ‚ã€‚è¿™ç§æ–¹æ³•æä¾›äº†æ›´ç›´æ¥å’Œå¯è§£é‡Šçš„åè§ç¼“è§£æ–¹æ¡ˆã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼Œä½¿ç”¨çº¿æ€§æ¢é’ˆæ¥æ•æ‰åè§è¡¨ç¤ºï¼Œå¹¶é€šè¿‡å¯¹æ¯”åè§ä¸ä¸­æ€§é™ˆè¿°çš„æ¿€æ´»æ¨¡å¼æ¥è®¡ç®—å¼•å¯¼å‘é‡ï¼Œç¡®ä¿åœ¨æ¨ç†æ—¶èƒ½å¤Ÿæœ‰æ•ˆå¼•å¯¼æ¨¡å‹çš„ç”Ÿæˆè¿‡ç¨‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè®­ç»ƒçš„çº¿æ€§æ¢é’ˆåœ¨è¯†åˆ«åè§å†…å®¹æ–¹é¢è¾¾åˆ°äº†è¿‘ä¹å®Œç¾çš„å‡†ç¡®ç‡ï¼Œä¸”é€šè¿‡å¼•å¯¼å‘é‡çš„åº”ç”¨ï¼ŒæˆåŠŸå°†åè§ç”Ÿæˆçš„å†…å®¹è½¬å‘æ›´ä¸­ç«‹çš„æ›¿ä»£æ–¹æ¡ˆã€‚è¿™ä¸€æ–¹æ³•åœ¨åè§ç¼“è§£ä¸Šå±•ç¤ºäº†æ˜¾è‘—çš„æ•ˆæœï¼Œå…·æœ‰è¾ƒå¼ºçš„å®ç”¨æ€§å’Œå¯æ‰©å±•æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç¤¾äº¤åª’ä½“å†…å®¹ç”Ÿæˆã€è‡ªåŠ¨åŒ–å®¢æœç³»ç»Ÿä»¥åŠä»»ä½•ä¾èµ–å¤§å‹è¯­è¨€æ¨¡å‹çš„åº”ç”¨åœºæ™¯ã€‚é€šè¿‡ä¸»åŠ¨ç¼“è§£åè§ï¼Œè¯¥æŠ€æœ¯èƒ½å¤Ÿæé«˜æ¨¡å‹çš„å®‰å…¨æ€§å’Œç¤¾ä¼šè´£ä»»æ„Ÿï¼Œå‡å°‘å¯¹ç”¨æˆ·çš„æ½œåœ¨ä¼¤å®³ï¼Œä¿ƒè¿›æ›´å…¬å¹³çš„æŠ€æœ¯åº”ç”¨ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½ä¼šå½±å“å¤§å‹è¯­è¨€æ¨¡å‹çš„è®¾è®¡å’Œä½¿ç”¨æ ‡å‡†ï¼Œæ¨åŠ¨æ›´è´Ÿè´£ä»»çš„äººå·¥æ™ºèƒ½å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As large language models (LLMs) become more integrated into societal systems, the risk of them perpetuating and amplifying harmful biases becomes a critical safety concern. Traditional methods for mitigating bias often rely on data filtering or post-hoc output moderation, which treat the model as an opaque black box. In this work, we introduce a complete, end-to-end system that uses techniques from mechanistic interpretability to both identify and actively mitigate bias directly within a model's internal workings. Our method involves two primary stages. First, we train linear "probes" on the internal activations of a model to detect the latent representations of various biases (e.g., gender, race, age). Our experiments on \texttt{gpt2-large} demonstrate that these probes can identify biased content with near-perfect accuracy, revealing that bias representations become most salient in the model's later layers. Second, we leverage these findings to compute "steering vectors" by contrasting the model's activation patterns for biased and neutral statements. By adding these vectors during inference, we can actively steer the model's generative process away from producing harmful, stereotypical, or biased content in real-time. We demonstrate the efficacy of this activation steering technique, showing that it successfully alters biased completions toward more neutral alternatives. We present our work as a robust and reproducible system that offers a more direct and interpretable approach to building safer and more accountable LLMs.

