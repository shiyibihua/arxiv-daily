---
layout: default
title: STELAR-VISION: Self-Topology-Aware Efficient Learning for Aligned Reasoning in Vision
---

# STELAR-VISION: Self-Topology-Aware Efficient Learning for Aligned Reasoning in Vision

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.08688" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.08688v1</a>
  <a href="https://arxiv.org/pdf/2508.08688.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.08688v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.08688v1', 'STELAR-VISION: Self-Topology-Aware Efficient Learning for Aligned Reasoning in Vision')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chen Li, Han Zhang, Zhantao Yang, Fangyi Chen, Zihan Wang, Anudeepsekhar Bolimera, Marios Savvides

**åˆ†ç±»**: cs.AI, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-08-12

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSTELAR-VISIONä»¥è§£å†³å¤šæ¨¡æ€æ¨ç†ä¸­çš„æ‹“æ‰‘ä¾èµ–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `å¤šæ¨¡æ€æ¨ç†` `æ‹“æ‰‘æ„ŸçŸ¥` `åˆæˆæ•°æ®` `å¼ºåŒ–å­¦ä¹ ` `å‡†ç¡®æ€§æå‡` `è¾“å‡ºä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤æ‚å¤šæ¨¡æ€ä»»åŠ¡æ—¶ï¼Œå¸¸å¸¸ç”Ÿæˆå†—é•¿çš„è¾“å‡ºï¼Œä¸”ä¾èµ–é“¾å¼æ¨ç†ï¼Œé™åˆ¶äº†å…¶è¡¨ç°ã€‚
2. STELAR-Visioné€šè¿‡å¼•å…¥æ‹“æ‰‘æ„ŸçŸ¥çš„è®­ç»ƒæ¡†æ¶å’Œåˆæˆæ•°æ®ç®¡é“TopoAugï¼Œå¢å¼ºäº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚
3. åœ¨MATH-Vå’ŒVLM-S2Hä¸Šï¼ŒSTELAR-Visionç›¸æ¯”åŸºç¡€æ¨¡å‹æé«˜äº†9.7%çš„å‡†ç¡®ç‡ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨æ¨ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨å¤æ‚çš„å¤šæ¨¡æ€ä»»åŠ¡ä¸­ä»é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯ç”Ÿæˆå†—é•¿è¾“å‡ºçš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–é“¾å¼æ¨ç†ï¼ˆCoTï¼‰ï¼Œè€Œè®¸å¤šä»»åŠ¡æ›´é€‚åˆä½¿ç”¨æ ‘æˆ–å›¾ç­‰æ›¿ä»£æ‹“æ‰‘ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†STELAR-Visionï¼Œä¸€ä¸ªæ‹“æ‰‘æ„ŸçŸ¥çš„æ¨ç†è®­ç»ƒæ¡†æ¶ã€‚å…¶æ ¸å¿ƒæ˜¯TopoAugï¼Œä¸€ä¸ªåˆæˆæ•°æ®ç®¡é“ï¼Œé€šè¿‡å¤šæ ·çš„æ‹“æ‰‘ç»“æ„ä¸°å¯Œè®­ç»ƒè¿‡ç¨‹ã€‚é€šè¿‡ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ï¼Œæˆ‘ä»¬å¯¹Qwen2VLæ¨¡å‹è¿›è¡Œäº†åè®­ç»ƒï¼Œå…¼é¡¾å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†Frugal Learningï¼Œæ—¨åœ¨ä»¥æœ€å°çš„å‡†ç¡®æ€§æŸå¤±å‡å°‘è¾“å‡ºé•¿åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSTELAR-Visionåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†æ¨¡å‹çš„å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤æ‚å¤šæ¨¡æ€ä»»åŠ¡ä¸­ç”Ÿæˆå†—é•¿è¾“å‡ºå’Œæ¨ç†èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–é“¾å¼æ¨ç†ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨å…¶ä»–æ‹“æ‰‘ç»“æ„çš„ä¼˜åŠ¿ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSTELAR-Visioné€šè¿‡å¼•å…¥æ‹“æ‰‘æ„ŸçŸ¥çš„è®­ç»ƒæ¡†æ¶ï¼Œåˆ©ç”¨å¤šæ ·çš„æ‹“æ‰‘ç»“æ„æ¥å¢å¼ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œä»è€Œæé«˜å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…æ‹¬TopoAugåˆæˆæ•°æ®ç®¡é“ã€ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚TopoAugè´Ÿè´£ç”Ÿæˆå¤šæ ·çš„æ‹“æ‰‘ç»“æ„æ•°æ®ï¼Œç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ åˆ™ç”¨äºä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šSTELAR-Visionçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶æ‹“æ‰‘æ„ŸçŸ¥çš„è®­ç»ƒæ–¹æ³•ï¼ŒåŒºåˆ«äºä¼ ç»Ÿçš„é“¾å¼æ¨ç†ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”å¤šæ¨¡æ€ä»»åŠ¡çš„éœ€æ±‚ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†Frugal Learningç­–ç•¥ä»¥å‡å°‘è¾“å‡ºé•¿åº¦ï¼ŒåŒæ—¶ä¿æŒè¾ƒé«˜çš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œæ¨¡å‹çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡ä¹Ÿç»è¿‡ç²¾å¿ƒè°ƒæ•´ï¼Œä»¥ç¡®ä¿æœ€ä½³çš„è®­ç»ƒæ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

STELAR-VISIONåœ¨MATH-Vå’ŒVLM-S2Hä¸Šåˆ†åˆ«æé«˜äº†9.7%å’Œ7.3%çš„å‡†ç¡®ç‡ï¼Œä¸”åœ¨äº”ä¸ªè¶…å‡ºåˆ†å¸ƒçš„åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¡¨ç°ä¼˜äºPhi-4-Multimodal-Instructé«˜è¾¾28.4%ï¼Œæ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç›¸æ¯”äºä»…ä½¿ç”¨é“¾å¼è®­ç»ƒï¼Œæ•´ä½“å‡†ç¡®ç‡æå‡äº†4.3%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

STELAR-VISIONçš„ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºå¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ï¼Œå¦‚å›¾åƒæè¿°ç”Ÿæˆã€è§†è§‰é—®ç­”å’Œäººæœºäº¤äº’ç­‰é¢†åŸŸã€‚å…¶æ‹“æ‰‘æ„ŸçŸ¥çš„è®­ç»ƒæ–¹æ³•èƒ½å¤Ÿæå‡æ¨¡å‹åœ¨å¤æ‚åœºæ™¯ä¸‹çš„è¡¨ç°ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-language models (VLMs) have made significant strides in reasoning, yet they often struggle with complex multimodal tasks and tend to generate overly verbose outputs. A key limitation is their reliance on chain-of-thought (CoT) reasoning, despite many tasks benefiting from alternative topologies like trees or graphs. To address this, we introduce STELAR-Vision, a training framework for topology-aware reasoning. At its core is TopoAug, a synthetic data pipeline that enriches training with diverse topological structures. Using supervised fine-tuning and reinforcement learning, we post-train Qwen2VL models with both accuracy and efficiency in mind. Additionally, we propose Frugal Learning, which reduces output length with minimal accuracy loss. On MATH-V and VLM-S2H, STELAR-Vision improves accuracy by 9.7% over its base model and surpasses the larger Qwen2VL-72B-Instruct by 7.3%. On five out-of-distribution benchmarks, it outperforms Phi-4-Multimodal-Instruct by up to 28.4% and LLaMA-3.2-11B-Vision-Instruct by up to 13.2%, demonstrating strong generalization. Compared to Chain-Only training, our approach achieves 4.3% higher overall accuracy on in-distribution datasets and consistently outperforms across all OOD benchmarks. We have released datasets, and code will be available.

