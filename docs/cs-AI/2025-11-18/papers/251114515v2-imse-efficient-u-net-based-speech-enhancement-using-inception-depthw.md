---
layout: default
title: IMSE: Efficient U-Net-based Speech Enhancement using Inception Depthwise Convolution and Amplitude-Aware Linear Attention
---

# IMSE: Efficient U-Net-based Speech Enhancement using Inception Depthwise Convolution and Amplitude-Aware Linear Attention

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.14515" class="toolbar-btn" target="_blank">üìÑ arXiv: 2511.14515v2</a>
  <a href="https://arxiv.org/pdf/2511.14515.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.14515v2" onclick="toggleFavorite(this, '2511.14515v2', 'IMSE: Efficient U-Net-based Speech Enhancement using Inception Depthwise Convolution and Amplitude-Aware Linear Attention')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Xinxin Tang, Bin Qin, Yufang Li

**ÂàÜÁ±ª**: cs.SD, cs.AI, cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-11-18 (Êõ¥Êñ∞: 2025-12-01)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**IMSEÔºöÂà©Áî®InceptionÊ∑±Â∫¶ÂèØÂàÜÁ¶ªÂç∑ÁßØÂíåÂπÖÂ∫¶ÊÑüÁü•Á∫øÊÄßÊ≥®ÊÑèÂäõÁöÑÈ´òÊïàU-NetËØ≠Èü≥Â¢ûÂº∫**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `ËØ≠Èü≥Â¢ûÂº∫` `ËΩªÈáèÂåñÊ®°Âûã` `U-Net` `Á∫øÊÄßÊ≥®ÊÑèÂäõ` `Ê∑±Â∫¶ÂèØÂàÜÁ¶ªÂç∑ÁßØ` `ÂπÖÂ∫¶ÊÑüÁü•` `ËµÑÊ∫êÂèóÈôêËÆæÂ§á`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâËØ≠Èü≥Â¢ûÂº∫ÊñπÊ≥ïÂú®ËµÑÊ∫êÂèóÈôêËÆæÂ§á‰∏äÈöæ‰ª•ÂÖºÈ°æËΩªÈáèÂåñÂíåÈ´òÊÄßËÉΩÔºåÂ≠òÂú®ÊïàÁéáÁì∂È¢à„ÄÇ
2. IMSEÈÄöËøáÂπÖÂ∫¶ÊÑüÁü•Á∫øÊÄßÊ≥®ÊÑèÂäõÂíåInceptionÊ∑±Â∫¶ÂèØÂàÜÁ¶ªÂç∑ÁßØÔºåÂÆûÁé∞È´òÊïàÂÖ®Â±ÄÂª∫Ê®°ÂíåÁâπÂæÅÊèêÂèñ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåIMSEÂú®ÂèÇÊï∞ÈáèÊòæËëóÈôç‰ΩéÁöÑÂêåÊó∂Ôºå‰øùÊåÅ‰∫Ü‰∏éÁé∞ÊúâÊúÄ‰Ω≥ÊñπÊ≥ïÁõ∏ÂΩìÁöÑËØ≠Èü≥Â¢ûÂº∫ÊÄßËÉΩ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫IMSEÔºå‰∏Ä‰∏™Á≥ªÁªü‰ºòÂåñ‰∏îË∂ÖËΩªÈáèÁ∫ßÁöÑËØ≠Èü≥Â¢ûÂº∫ÁΩëÁªúÔºåÊó®Âú®Âπ≥Ë°°ËΩªÈáèÂåñËÆæËÆ°ÂíåÈ´òÊÄßËÉΩ„ÄÇÁé∞ÊúâÊñπÊ≥ïÂ¶ÇMUSEËôΩÁÑ∂ÂèÇÊï∞Èáè‰ªÖ‰∏∫0.51MÔºå‰ΩÜÊïàÁéá‰ªçÊúâÁì∂È¢à„ÄÇMUSE‰∏≠ÁöÑMETÊ®°Âùó‰æùËµñÂ§çÊùÇÁöÑ‚ÄúËøë‰ºº-Ë°•ÂÅø‚ÄùÊú∫Âà∂Êù•ÁºìËß£Ê≥∞ÂãíÂ±ïÂºÄÊ≥®ÊÑèÂäõÁöÑÂ±ÄÈôêÊÄßÔºåËÄåÂèØÂèòÂΩ¢ÂµåÂÖ•ÁöÑÂÅèÁßªËÆ°ÁÆóÂºïÂÖ•‰∫ÜÈ¢ùÂ§ñÁöÑËÆ°ÁÆóË¥üÊãÖ„ÄÇIMSEÂºïÂÖ•‰∫Ü‰∏§‰∏™Ê†∏ÂøÉÂàõÊñ∞Ôºö1) ‰ΩøÁî®ÂπÖÂ∫¶ÊÑüÁü•Á∫øÊÄßÊ≥®ÊÑèÂäõÔºàMALAÔºâÊõøÊç¢METÊ®°ÂùóÔºåÈÄöËøáÊòæÂºè‰øùÁïôÊü•ËØ¢ÂêëÈáèÁöÑËåÉÊï∞‰ø°ÊÅØÔºå‰ªéÊ†πÊú¨‰∏äÁ∫†Ê≠£Á∫øÊÄßÊ≥®ÊÑèÂäõ‰∏≠‚ÄúÂøΩÁï•ÂπÖÂ∫¶‚ÄùÁöÑÈóÆÈ¢òÔºåÂÆûÁé∞È´òÊïàÁöÑÂÖ®Â±ÄÂª∫Ê®°„ÄÇ2) ‰ΩøÁî®InceptionÊ∑±Â∫¶ÂèØÂàÜÁ¶ªÂç∑ÁßØÔºàIDConvÔºâÊõøÊç¢DEÊ®°ÂùóÔºåÂ∞ÜÂ§ßÊ†∏Êìç‰ΩúÂàÜËß£‰∏∫È´òÊïàÁöÑÂπ∂Ë°åÂàÜÊîØÔºàÊ≠£ÊñπÂΩ¢„ÄÅÊ∞¥Âπ≥ÂíåÂûÇÁõ¥Êù°ÔºâÔºå‰ª•ÊûÅ‰ΩéÁöÑÂèÇÊï∞ÂÜó‰ΩôÊçïËé∑È¢ëË∞±ÂõæÁâπÂæÅ„ÄÇÂú®VoiceBank+DEMANDÊï∞ÊçÆÈõÜ‰∏äÁöÑÂÆûÈ™åË°®ÊòéÔºå‰∏éMUSEÁõ∏ÊØîÔºåIMSEÂú®ÂèÇÊï∞ÈáèÂáèÂ∞ë16.8%Ôºà‰ªé0.513MÂà∞0.427MÔºâÁöÑÂêåÊó∂ÔºåÂú®PESQÊåáÊ†á‰∏äÂÆûÁé∞‰∫Ü‰∏éÊúÄÂÖàËøõÊ∞¥Âπ≥Áõ∏ÂΩìÁöÑÊÄßËÉΩÔºà3.373Ôºâ„ÄÇËøôÈ°πÁ†îÁ©∂‰∏∫Ë∂ÖËΩªÈáèÁ∫ßËØ≠Èü≥Â¢ûÂº∫‰∏≠Ê®°ÂûãÂ§ßÂ∞èÂíåËØ≠Èü≥Ë¥®Èáè‰πãÈó¥ÁöÑÊùÉË°°ËÆæÂÆö‰∫ÜÊñ∞ÁöÑÂü∫ÂáÜ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥ËØ≠Èü≥Â¢ûÂº∫‰ªªÂä°‰∏≠ÔºåÂ¶Ç‰ΩïÂú®ËµÑÊ∫êÂèóÈôêÁöÑËÆæÂ§á‰∏äÂÆûÁé∞È´òÊÄßËÉΩÂíå‰ΩéËÆ°ÁÆóÂ§çÊùÇÂ∫¶ÁöÑÂπ≥Ë°°ÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÔºåÂ¶ÇMUSEÔºåËôΩÁÑ∂ÂèÇÊï∞ÈáèËæÉÂ∞èÔºå‰ΩÜÂÖ∂METÊ®°ÂùóÂíåDEÊ®°Âùó‰ªçÁÑ∂Â≠òÂú®ÊïàÁéáÁì∂È¢àÔºå‰æãÂ¶ÇMETÊ®°ÂùóÈúÄË¶ÅÂ§çÊùÇÁöÑËøë‰ººË°•ÂÅøÊú∫Âà∂ÔºåDEÊ®°ÂùóÈúÄË¶ÅÈ¢ùÂ§ñÁöÑÂÅèÁßªËÆ°ÁÆóÔºåÂØºËá¥ËÆ°ÁÆóË¥üÊãÖÂ¢ûÂä†„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÊîπËøõÊ≥®ÊÑèÂäõÊú∫Âà∂ÂíåÂç∑ÁßØÊìç‰ΩúÔºåÂú®‰∏çÊçüÂ§±ÊÄßËÉΩÁöÑÂâçÊèê‰∏ãÔºåÊòæËëóÈôç‰ΩéÊ®°ÂûãÁöÑÂèÇÊï∞ÈáèÂíåËÆ°ÁÆóÂ§çÊùÇÂ∫¶„ÄÇÂÖ∑‰ΩìÊù•ËØ¥Ôºå‰ΩøÁî®ÂπÖÂ∫¶ÊÑüÁü•Á∫øÊÄßÊ≥®ÊÑèÂäõÔºàMALAÔºâÊõø‰ª£Â§çÊùÇÁöÑMETÊ®°ÂùóÔºåËß£ÂÜ≥Á∫øÊÄßÊ≥®ÊÑèÂäõÂøΩÁï•ÂπÖÂ∫¶‰ø°ÊÅØÁöÑÈóÆÈ¢òÔºõ‰ΩøÁî®InceptionÊ∑±Â∫¶ÂèØÂàÜÁ¶ªÂç∑ÁßØÔºàIDConvÔºâÊõø‰ª£DEÊ®°ÂùóÔºå‰ª•Êõ¥È´òÊïàÁöÑÊñπÂºèÊèêÂèñÈ¢ëË∞±ÂõæÁâπÂæÅ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöIMSEÈááÁî®U-NetÁªìÊûÑ‰Ωú‰∏∫Êï¥‰ΩìÊ°ÜÊû∂ÔºåÁºñÁ†ÅÂô®ÊèêÂèñËæìÂÖ•Âô™Â£∞ËØ≠Èü≥ÁöÑÁâπÂæÅÔºåËß£Á†ÅÂô®ÈáçÂª∫Â¢ûÂº∫ÂêéÁöÑËØ≠Èü≥„ÄÇMALAÊ®°ÂùóË¢´ÈõÜÊàêÂà∞U-NetÁöÑÁì∂È¢àÂ±ÇÔºåÁî®‰∫éÂÖ®Â±ÄÂª∫Ê®°„ÄÇIDConvÊ®°ÂùóË¢´Áî®‰∫éÁºñÁ†ÅÂô®ÂíåËß£Á†ÅÂô®ÁöÑÂç∑ÁßØÂ±ÇÔºåÁî®‰∫éÈ´òÊïàÁöÑÁâπÂæÅÊèêÂèñ„ÄÇÊï¥‰∏™ÁΩëÁªúÁªìÊûÑÁÆÄÊ¥ÅÈ´òÊïàÔºåÊòì‰∫éÈÉ®ÁΩ≤Âà∞ËµÑÊ∫êÂèóÈôêÁöÑËÆæÂ§á‰∏ä„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÊèêÂá∫‰∫ÜÂπÖÂ∫¶ÊÑüÁü•Á∫øÊÄßÊ≥®ÊÑèÂäõÔºàMALAÔºâÂíåInceptionÊ∑±Â∫¶ÂèØÂàÜÁ¶ªÂç∑ÁßØÔºàIDConvÔºâ„ÄÇMALAÈÄöËøáÊòæÂºè‰øùÁïôÊü•ËØ¢ÂêëÈáèÁöÑËåÉÊï∞‰ø°ÊÅØÔºåËß£ÂÜ≥‰∫ÜÁ∫øÊÄßÊ≥®ÊÑèÂäõÂøΩÁï•ÂπÖÂ∫¶‰ø°ÊÅØÁöÑÈóÆÈ¢òÔºåÂÆûÁé∞‰∫ÜÈ´òÊïàÁöÑÂÖ®Â±ÄÂª∫Ê®°„ÄÇIDConvÂ∞ÜÂ§ßÊ†∏Âç∑ÁßØÂàÜËß£‰∏∫Â§ö‰∏™Âπ∂Ë°åÁöÑÊ∑±Â∫¶ÂèØÂàÜÁ¶ªÂç∑ÁßØÂàÜÊîØÔºåÂáèÂ∞ë‰∫ÜÂèÇÊï∞ÂÜó‰ΩôÔºåÊèêÈ´ò‰∫ÜËÆ°ÁÆóÊïàÁéá„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöMALAÊ®°ÂùóÁöÑÂÖ≥ÈîÆËÆæËÆ°Âú®‰∫éÂú®ËÆ°ÁÆóÊ≥®ÊÑèÂäõÊùÉÈáçÊó∂ÔºåÊòæÂºèÂú∞ËÄÉËôë‰∫ÜÊü•ËØ¢ÂêëÈáèÁöÑËåÉÊï∞‰ø°ÊÅØÔºå‰ªéËÄåÈÅøÂÖç‰∫Ü‰ø°ÊÅØÊçüÂ§±„ÄÇIDConvÊ®°ÂùóÁöÑÂÖ≥ÈîÆËÆæËÆ°Âú®‰∫éÂ∞ÜÂ§ßÊ†∏Âç∑ÁßØÂàÜËß£‰∏∫Ê≠£ÊñπÂΩ¢„ÄÅÊ∞¥Âπ≥ÂíåÂûÇÁõ¥Êù°Áä∂ÁöÑÊ∑±Â∫¶ÂèØÂàÜÁ¶ªÂç∑ÁßØÔºå‰ªéËÄåÂú®‰∏çÂêåÊñπÂêë‰∏äÊèêÂèñÁâπÂæÅÔºåÂπ∂ÂáèÂ∞ëÂèÇÊï∞Èáè„ÄÇÊçüÂ§±ÂáΩÊï∞ÈááÁî®Â∏∏Áî®ÁöÑÊó∂ÂüüÊàñÈ¢ëÂüüÊçüÂ§±ÂáΩÊï∞Ôºå‰æãÂ¶ÇL1ÊçüÂ§±ÊàñÂùáÊñπËØØÂ∑Æ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåIMSEÂú®VoiceBank+DEMANDÊï∞ÊçÆÈõÜ‰∏äÔºåÁõ∏ÊØî‰∫éMUSEÂü∫Á∫øÔºåÂèÇÊï∞ÈáèÂáèÂ∞ë‰∫Ü16.8%Ôºà‰ªé0.513MÂà∞0.427MÔºâÔºåÂêåÊó∂Âú®PESQÊåáÊ†á‰∏äËææÂà∞‰∫Ü3.373Ôºå‰∏éÊúÄÂÖàËøõÊ∞¥Âπ≥Áõ∏ÂΩì„ÄÇËøôË°®ÊòéIMSEÂú®Ê®°ÂûãÂ§ßÂ∞èÂíåËØ≠Èü≥Ë¥®Èáè‰πãÈó¥ÂèñÂæó‰∫ÜÊõ¥Â•ΩÁöÑÂπ≥Ë°°Ôºå‰∏∫Ë∂ÖËΩªÈáèÁ∫ßËØ≠Èü≥Â¢ûÂº∫ËÆæÂÆö‰∫ÜÊñ∞ÁöÑÂü∫ÂáÜ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂêÑÁßçËµÑÊ∫êÂèóÈôêÁöÑËØ≠Èü≥Â¢ûÂº∫Âú∫ÊôØÔºå‰æãÂ¶ÇÁßªÂä®ËÆæÂ§á„ÄÅÂµåÂÖ•ÂºèÁ≥ªÁªü„ÄÅÊô∫ËÉΩÂÆ∂Â±ÖËÆæÂ§áÁ≠â„ÄÇÈÄöËøáÈôç‰ΩéÊ®°ÂûãÂ§çÊùÇÂ∫¶ÂíåËÆ°ÁÆóÈáèÔºåIMSEËÉΩÂ§üÂú®Ëøô‰∫õËÆæÂ§á‰∏äÂÆûÁé∞ÂÆûÊó∂ÁöÑËØ≠Èü≥Â¢ûÂº∫ÔºåÊèêÈ´òËØ≠Èü≥ÈÄö‰ø°Ë¥®ÈáèÂíåÁî®Êà∑‰ΩìÈ™å„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ïËøòÂèØ‰ª•Â∫îÁî®‰∫éÂä©Âê¨Âô®Á≠âËæÖÂä©ËÆæÂ§áÔºåÂ∏ÆÂä©Âê¨ÂäõÂèóÊçü‰∫∫Â£´Êõ¥Â•ΩÂú∞ÁêÜËß£ËØ≠Èü≥„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Achieving a balance between lightweight design and high performance remains a significant challenge for speech enhancement (SE) tasks on resource-constrained devices. Existing state-of-the-art methods, such as MUSE, have established a strong baseline with only 0.51M parameters by introducing a Multi-path Enhanced Taylor (MET) transformer and Deformable Embedding (DE). However, an in-depth analysis reveals that MUSE still suffers from efficiency bottlenecks: the MET module relies on a complex "approximate-compensate" mechanism to mitigate the limitations of Taylor-expansion-based attention, while the offset calculation for deformable embedding introduces additional computational burden. This paper proposes IMSE, a systematically optimized and ultra-lightweight network. We introduce two core innovations: 1) Replacing the MET module with Amplitude-Aware Linear Attention (MALA). MALA fundamentally rectifies the "amplitude-ignoring" problem in linear attention by explicitly preserving the norm information of query vectors in the attention calculation, achieving efficient global modeling without an auxiliary compensation branch. 2) Replacing the DE module with Inception Depthwise Convolution (IDConv). IDConv borrows the Inception concept, decomposing large-kernel operations into efficient parallel branches (square, horizontal, and vertical strips), thereby capturing spectrogram features with extremely low parameter redundancy. Extensive experiments on the VoiceBank+DEMAND dataset demonstrate that, compared to the MUSE baseline, IMSE significantly reduces the parameter count by 16.8\% (from 0.513M to 0.427M) while achieving competitive performance comparable to the state-of-the-art on the PESQ metric (3.373). This study sets a new benchmark for the trade-off between model size and speech quality in ultra-lightweight speech enhancement.

