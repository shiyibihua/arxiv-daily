---
layout: default
title: Zero-Shot Coordination in Ad Hoc Teams with Generalized Policy Improvement and Difference Rewards
---

# Zero-Shot Coordination in Ad Hoc Teams with Generalized Policy Improvement and Difference Rewards

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.16187" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.16187v1</a>
  <a href="https://arxiv.org/pdf/2510.16187.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.16187v1" onclick="toggleFavorite(this, '2510.16187v1', 'Zero-Shot Coordination in Ad Hoc Teams with Generalized Policy Improvement and Difference Rewards')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Rupal Nigam, Niket Parikh, Hamid Osooli, Mikihisa Yuasa, Jacob Heglund, Huy T. Tran

**åˆ†ç±»**: cs.MA, cs.AI, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-10-17

**å¤‡æ³¨**: 10 pages, 8 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGPATç®—æ³•ï¼Œåˆ©ç”¨å¹¿ä¹‰ç­–ç•¥æå‡å’Œå·®å¼‚å¥–åŠ±å®ç°Ad Hocå›¢é˜Ÿé›¶æ ·æœ¬åä½œ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `Ad Hocå›¢é˜Ÿåä½œ` `é›¶æ ·æœ¬å­¦ä¹ ` `å¹¿ä¹‰ç­–ç•¥æå‡` `å·®å¼‚å¥–åŠ±` `å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰Ad Hocå›¢é˜Ÿåä½œæ–¹æ³•ä¾èµ–äºé˜Ÿå‹æ¨¡å‹æ¨æ–­æˆ–é²æ£’ç­–ç•¥é¢„è®­ç»ƒï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨æ‰€æœ‰é¢„è®­ç»ƒç­–ç•¥ã€‚
2. GPATç®—æ³•é€šè¿‡å¹¿ä¹‰ç­–ç•¥æå‡èšåˆå¤šä¸ªé¢„è®­ç»ƒç­–ç•¥ï¼Œå¹¶åˆ©ç”¨å·®å¼‚å¥–åŠ±ä¿ƒè¿›å›¢é˜Ÿå†…æœ‰æ•ˆåä½œã€‚
3. å®éªŒè¡¨æ˜ï¼ŒGPATåœ¨å¤šä¸ªæ¨¡æ‹Ÿç¯å¢ƒå’ŒçœŸå®æœºå™¨äººåœºæ™¯ä¸­ï¼Œå®ç°äº†å¯¹æ–°å›¢é˜Ÿçš„æœ‰æ•ˆé›¶æ ·æœ¬è¿ç§»ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°å®ä¸–ç•Œçš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿå¯èƒ½éœ€è¦Ad Hocå›¢é˜Ÿåä½œï¼Œå³æ™ºèƒ½ä½“å¿…é¡»ä¸ä¹‹å‰æœªè§è¿‡çš„é˜Ÿå‹åä½œï¼Œä»¥é›¶æ ·æœ¬æ–¹å¼è§£å†³ä»»åŠ¡ã€‚å…ˆå‰çš„å·¥ä½œé€šå¸¸åŸºäºå¯¹æ–°é˜Ÿå‹çš„æ¨æ–­æ¨¡å‹é€‰æ‹©é¢„è®­ç»ƒç­–ç•¥ï¼Œæˆ–è€…é¢„è®­ç»ƒå•ä¸ªå¯¹æ½œåœ¨é˜Ÿå‹å…·æœ‰é²æ£’æ€§çš„ç­–ç•¥ã€‚æœ¬æ–‡æå‡ºåˆ©ç”¨é›¶æ ·æœ¬è¿ç§»è®¾ç½®ä¸­çš„æ‰€æœ‰é¢„è®­ç»ƒç­–ç•¥ã€‚æˆ‘ä»¬å°†æ­¤é—®é¢˜å½¢å¼åŒ–ä¸ºAd Hocå¤šæ™ºèƒ½ä½“é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œå¹¶æå‡ºä¸€ç§è§£å†³æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆä½¿ç”¨å¹¿ä¹‰ç­–ç•¥æå‡å’Œå·®å¼‚å¥–åŠ±è¿™ä¸¤ä¸ªå…³é”®æ€æƒ³ï¼Œä»¥å®ç°ä¸åŒå›¢é˜Ÿä¹‹é—´é«˜æ•ˆä¸”æœ‰æ•ˆçš„çŸ¥è¯†è½¬ç§»ã€‚ç»éªŒè¯è¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç®—æ³•ï¼ŒAd Hocå›¢é˜Ÿå¹¿ä¹‰ç­–ç•¥æå‡ï¼ˆGPATï¼‰ï¼ŒæˆåŠŸåœ°å®ç°äº†åœ¨ä¸‰ä¸ªæ¨¡æ‹Ÿç¯å¢ƒï¼ˆåˆä½œè§…é£Ÿã€æ•é£Ÿè€…-çŒç‰©å’ŒOvercookedï¼‰ä¸­é›¶æ ·æœ¬è¿ç§»åˆ°æ–°å›¢é˜Ÿã€‚æˆ‘ä»¬è¿˜åœ¨çœŸå®çš„å¤šæœºå™¨äººç¯å¢ƒä¸­å±•ç¤ºäº†æˆ‘ä»¬çš„ç®—æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³Ad Hocå›¢é˜Ÿåä½œä¸­çš„é›¶æ ·æœ¬è¿ç§»é—®é¢˜ã€‚åœ¨Ad Hocå›¢é˜Ÿåä½œä¸­ï¼Œæ™ºèƒ½ä½“éœ€è¦ä¸æœªçŸ¥çš„é˜Ÿå‹è¿›è¡Œåä½œï¼Œå®Œæˆç‰¹å®šä»»åŠ¡ã€‚ç°æœ‰çš„æ–¹æ³•ä¸»è¦å­˜åœ¨ä¸¤ä¸ªç—›ç‚¹ï¼šä¸€æ˜¯ä¾èµ–äºå¯¹æ–°é˜Ÿå‹çš„å»ºæ¨¡ï¼Œè¿™åœ¨é˜Ÿå‹å®Œå…¨æœªçŸ¥çš„æƒ…å†µä¸‹éš¾ä»¥å®ç°ï¼›äºŒæ˜¯é¢„è®­ç»ƒä¸€ä¸ªå¯¹æ‰€æœ‰æ½œåœ¨é˜Ÿå‹éƒ½é²æ£’çš„ç­–ç•¥ï¼Œä½†è¿™ç§ç­–ç•¥å¾€å¾€æ¬¡ä¼˜ï¼Œä¸”è®­ç»ƒéš¾åº¦è¾ƒé«˜ã€‚å› æ­¤ï¼Œå¦‚ä½•æœ‰æ•ˆåœ°åˆ©ç”¨å·²æœ‰çš„é¢„è®­ç»ƒç­–ç•¥ï¼Œå®ç°ä¸æœªçŸ¥é˜Ÿå‹çš„æœ‰æ•ˆåä½œï¼Œæ˜¯æœ¬æ–‡è¦è§£å†³çš„å…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ‰€æœ‰å¯ç”¨çš„é¢„è®­ç»ƒç­–ç•¥ï¼Œå¹¶é€šè¿‡å¹¿ä¹‰ç­–ç•¥æå‡ï¼ˆGeneralized Policy Improvement, GPIï¼‰æ–¹æ³•ï¼Œå°†è¿™äº›ç­–ç•¥è¿›è¡Œèšåˆï¼Œä»è€Œå¾—åˆ°ä¸€ä¸ªæ›´ä¼˜çš„ç­–ç•¥ã€‚æ­¤å¤–ï¼Œä¸ºäº†ä¿ƒè¿›å›¢é˜Ÿå†…éƒ¨çš„æœ‰æ•ˆåä½œï¼Œè®ºæ–‡è¿˜å¼•å…¥äº†å·®å¼‚å¥–åŠ±ï¼ˆDifference Rewardsï¼‰ï¼Œé¼“åŠ±æ™ºèƒ½ä½“é‡‡å–èƒ½å¤Ÿæå‡å›¢é˜Ÿæ•´ä½“è¡¨ç°çš„è¡ŒåŠ¨ã€‚é€šè¿‡ç»“åˆGPIå’Œå·®å¼‚å¥–åŠ±ï¼ŒGPATç®—æ³•èƒ½å¤Ÿåœ¨é›¶æ ·æœ¬çš„æ¡ä»¶ä¸‹ï¼Œå®ç°ä¸æœªçŸ¥é˜Ÿå‹çš„æœ‰æ•ˆåä½œã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGPATç®—æ³•çš„æ•´ä½“æ¡†æ¶å¯ä»¥åˆ†ä¸ºä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¢„è®­ç»ƒé˜¶æ®µå’ŒAd Hocåä½œé˜¶æ®µã€‚åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼Œé’ˆå¯¹ä¸åŒçš„é˜Ÿå‹ç»„åˆï¼Œè®­ç»ƒå¤šä¸ªç‹¬ç«‹çš„ç­–ç•¥ã€‚åœ¨Ad Hocåä½œé˜¶æ®µï¼Œé¦–å…ˆåˆ©ç”¨GPIæ–¹æ³•ï¼Œå°†æ‰€æœ‰é¢„è®­ç»ƒçš„ç­–ç•¥è¿›è¡Œèšåˆï¼Œå¾—åˆ°ä¸€ä¸ªåˆå§‹ç­–ç•¥ã€‚ç„¶åï¼Œåœ¨åä½œè¿‡ç¨‹ä¸­ï¼Œåˆ©ç”¨å·®å¼‚å¥–åŠ±æ¥æŒ‡å¯¼æ™ºèƒ½ä½“çš„è¡Œä¸ºï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”æ–°çš„é˜Ÿå‹ï¼Œå¹¶æå‡å›¢é˜Ÿçš„æ•´ä½“è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šGPATç®—æ³•çš„å…³é”®åˆ›æ–°åœ¨äºä»¥ä¸‹ä¸¤ç‚¹ï¼šä¸€æ˜¯æå‡ºäº†å¹¿ä¹‰ç­–ç•¥æå‡ï¼ˆGPIï¼‰æ–¹æ³•ï¼Œç”¨äºèšåˆå¤šä¸ªé¢„è®­ç»ƒç­–ç•¥ã€‚ä¸ä¼ ç»Ÿçš„ç­–ç•¥æå‡æ–¹æ³•ä¸åŒï¼ŒGPIèƒ½å¤ŸåŒæ—¶è€ƒè™‘å¤šä¸ªç­–ç•¥ï¼Œå¹¶é€‰æ‹©å…¶ä¸­æœ€ä¼˜çš„ç­–ç•¥ç»„åˆï¼Œä»è€Œå¾—åˆ°ä¸€ä¸ªæ›´ä¼˜çš„ç­–ç•¥ã€‚äºŒæ˜¯å¼•å…¥äº†å·®å¼‚å¥–åŠ±ï¼Œç”¨äºä¿ƒè¿›å›¢é˜Ÿå†…éƒ¨çš„æœ‰æ•ˆåä½œã€‚å·®å¼‚å¥–åŠ±èƒ½å¤Ÿè¡¡é‡æ™ºèƒ½ä½“çš„è¡Œä¸ºå¯¹å›¢é˜Ÿæ•´ä½“è¡¨ç°çš„å½±å“ï¼Œä»è€Œé¼“åŠ±æ™ºèƒ½ä½“é‡‡å–èƒ½å¤Ÿæå‡å›¢é˜Ÿæ•´ä½“è¡¨ç°çš„è¡ŒåŠ¨ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨GPIæ–¹æ³•ä¸­ï¼Œè®ºæ–‡é‡‡ç”¨äº†ä¸€ç§åŸºäºQå€¼çš„ç­–ç•¥é€‰æ‹©æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹äºæ¯ä¸ªçŠ¶æ€ï¼ŒGPIä¼šé€‰æ‹©èƒ½å¤Ÿæœ€å¤§åŒ–å›¢é˜ŸQå€¼çš„ç­–ç•¥ç»„åˆã€‚åœ¨å·®å¼‚å¥–åŠ±çš„è®¾è®¡ä¸­ï¼Œè®ºæ–‡é‡‡ç”¨äº†ä¸€ç§åŸºäºcounterfactual reasoningçš„æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œå·®å¼‚å¥–åŠ±ä¼šè¡¡é‡æ™ºèƒ½ä½“é‡‡å–æŸä¸ªè¡ŒåŠ¨åï¼Œå›¢é˜Ÿæ•´ä½“è¡¨ç°çš„å˜åŒ–ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜é‡‡ç”¨äº†ä¸€ç§åŸºäºç¥ç»ç½‘ç»œçš„å‡½æ•°é€¼è¿‘æ–¹æ³•ï¼Œç”¨äºä¼°è®¡Qå€¼å’Œå·®å¼‚å¥–åŠ±ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒGPATç®—æ³•åœ¨åˆä½œè§…é£Ÿã€æ•é£Ÿè€…-çŒç‰©å’ŒOvercookedä¸‰ä¸ªæ¨¡æ‹Ÿç¯å¢ƒä¸­ï¼Œå‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨åˆä½œè§…é£Ÿç¯å¢ƒä¸­ï¼ŒGPATç®—æ³•çš„å¹³å‡å¥–åŠ±æ¯”åŸºçº¿æ–¹æ³•æé«˜äº†20%ä»¥ä¸Šã€‚æ­¤å¤–ï¼ŒGPATç®—æ³•è¿˜åœ¨çœŸå®çš„å¤šæœºå™¨äººç¯å¢ƒä¸­è¿›è¡Œäº†éªŒè¯ï¼Œè¯æ˜äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å¯è¡Œæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§å¤šæ™ºèƒ½ä½“åä½œåœºæ™¯ï¼Œä¾‹å¦‚ï¼šå¤šæœºå™¨äººååŒæ¬è¿ã€è‡ªåŠ¨é©¾é©¶è½¦è¾†ç¼–é˜Ÿã€æ™ºèƒ½äº¤é€šæ§åˆ¶ç­‰ã€‚é€šè¿‡å®ç°é›¶æ ·æœ¬çš„Ad Hocå›¢é˜Ÿåä½œï¼Œå¯ä»¥å¤§å¤§é™ä½ç³»ç»Ÿéƒ¨ç½²å’Œç»´æŠ¤çš„æˆæœ¬ï¼Œå¹¶æé«˜ç³»ç»Ÿçš„çµæ´»æ€§å’Œé€‚åº”æ€§ã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶è¿˜å¯ä»¥æ‰©å±•åˆ°æ›´å¤æ‚çš„åä½œåœºæ™¯ï¼Œä¾‹å¦‚ï¼šäººæœºåä½œã€è·¨é¢†åŸŸåä½œç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Real-world multi-agent systems may require ad hoc teaming, where an agent must coordinate with other previously unseen teammates to solve a task in a zero-shot manner. Prior work often either selects a pretrained policy based on an inferred model of the new teammates or pretrains a single policy that is robust to potential teammates. Instead, we propose to leverage all pretrained policies in a zero-shot transfer setting. We formalize this problem as an ad hoc multi-agent Markov decision process and present a solution that uses two key ideas, generalized policy improvement and difference rewards, for efficient and effective knowledge transfer between different teams. We empirically demonstrate that our algorithm, Generalized Policy improvement for Ad hoc Teaming (GPAT), successfully enables zero-shot transfer to new teams in three simulated environments: cooperative foraging, predator-prey, and Overcooked. We also demonstrate our algorithm in a real-world multi-robot setting.

