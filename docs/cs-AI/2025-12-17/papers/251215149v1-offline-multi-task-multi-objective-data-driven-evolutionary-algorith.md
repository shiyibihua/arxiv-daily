---
layout: default
title: Offline Multi-Task Multi-Objective Data-Driven Evolutionary Algorithm with Language Surrogate Model and Implicit Q-Learning
---

# Offline Multi-Task Multi-Objective Data-Driven Evolutionary Algorithm with Language Surrogate Model and Implicit Q-Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.15149" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.15149v1</a>
  <a href="https://arxiv.org/pdf/2512.15149.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.15149v1" onclick="toggleFavorite(this, '2512.15149v1', 'Offline Multi-Task Multi-Objective Data-Driven Evolutionary Algorithm with Language Surrogate Model and Implicit Q-Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xian-Rong Zhang, Yue-Jiao Gong, Zeyuan Ma, Jun Zhang

**åˆ†ç±»**: cs.NE, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-17

**å¤‡æ³¨**: 16 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºQ-MetaSurï¼Œåˆ©ç”¨è¯­è¨€æ¨¡å‹å’Œå¼ºåŒ–å­¦ä¹ è§£å†³ç¦»çº¿å¤šä»»åŠ¡å¤šç›®æ ‡ä¼˜åŒ–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šä»»åŠ¡ä¼˜åŒ–` `å¤šç›®æ ‡ä¼˜åŒ–` `ä»£ç†æ¨¡å‹` `å¤§å‹è¯­è¨€æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `ç¦»çº¿å­¦ä¹ ` `è¿›åŒ–ç®—æ³•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ä»£ç†å»ºæ¨¡æ–¹æ³•åœ¨å¤æ‚çš„å¤šå­ç›®æ ‡ä¼˜åŒ–é—®é¢˜ä¸­å­˜åœ¨å±€é™æ€§ï¼Œéœ€è¦é‡å¤ä¸”ç¹ççš„è¿‘ä¼¼ã€‚
2. Q-MetaSurå°†å¤šç›®æ ‡ä¼˜åŒ–é—®é¢˜è½¬åŒ–ä¸ºåºåˆ—åˆ°åºåˆ—å»ºæ¨¡ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œç¼–ç å’Œè§£ç ï¼Œå®ç°ç»Ÿä¸€çš„ä»£ç†å­¦ä¹ ã€‚
3. é€šè¿‡ä¸¤é˜¶æ®µç¦»çº¿è®­ç»ƒï¼Œç»“åˆç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ï¼ŒQ-MetaSuråœ¨CEC2019åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºQ-MetaSurçš„å³æ’å³ç”¨ä»£ç†å»ºæ¨¡æ–¹æ¡ˆï¼Œæ—¨åœ¨ä¸ºå¤šä»»åŠ¡å¤šç›®æ ‡ä¼˜åŒ–ï¼ˆMTMOOï¼‰æä¾›ç»Ÿä¸€å’Œé€šç”¨çš„ä»£ç†å­¦ä¹ ã€‚è¯¥æ–¹æ³•å°†ç›®æ ‡è¿‘ä¼¼è½¬åŒ–ä¸ºåºåˆ—åˆ°åºåˆ—çš„å»ºæ¨¡ï¼Œå…¶ä¸­MTMOOé—®é¢˜é€šè¿‡æ–‡æœ¬æ ‡è®°åŒ–è¡¨ç¤ºã€‚ä¸ºäº†åœ¨è¿™ç§è‡ªå›å½’å»ºæ¨¡ä¸‹è¿è¡Œï¼Œå¼•å…¥äº†ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„ä»£ç†æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é¦–å…ˆç¼–ç MTMOOå®ä¾‹ï¼Œç„¶åè§£ç æœªè§è¿‡çš„å†³ç­–å˜é‡çš„ç›®æ ‡å€¼ã€‚ä¸ºäº†ç¡®ä¿æ¨¡å‹è®­ç»ƒçš„ç¨³å®šæ€§ï¼Œæå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µç¦»çº¿è®­ç»ƒç­–ç•¥ï¼Œè¯¥ç­–ç•¥ç»“åˆäº†ç›‘ç£è°ƒä¼˜å’Œå¼ºåŒ–å­¦ä¹ å¾®è°ƒï¼Œé¦–å…ˆåˆ©ç”¨ç¦»çº¿æ•°æ®é›†æ¥æ‹Ÿåˆç°æœ‰çŸ¥è¯†ï¼Œç„¶ååˆ©ç”¨å¼ºåŒ–å­¦ä¹ æ¥å¢å¼ºæ¨¡å‹çš„æ³›åŒ–æ€§èƒ½ã€‚åœ¨CEC2019åŸºå‡†ä¸Šçš„å¤§é‡å®éªŒç»“æœè¡¨æ˜ï¼ŒQ-MetaSurä¸ä»…åœ¨ç›®æ ‡è¿‘ä¼¼ç²¾åº¦æ–¹é¢ä¼˜äºä»£è¡¨æ€§çš„ä»£ç†åŸºçº¿ï¼Œè€Œä¸”è¿˜æœ‰åŠ©äºåº•å±‚è¿›åŒ–ç®—æ³•å®ç°æœŸæœ›çš„ä¼˜åŒ–æ”¶æ•›å’Œæ”¹è¿›çš„å¸•ç´¯æ‰˜æœ€ä¼˜æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ç¦»çº¿å¤šä»»åŠ¡å¤šç›®æ ‡ä¼˜åŒ–ï¼ˆMTMOOï¼‰é—®é¢˜ã€‚ç°æœ‰ä»£ç†å»ºæ¨¡æ–¹æ³•åœ¨å¤„ç†å…·æœ‰å¤šä¸ªå­ç›®æ ‡çš„å¤æ‚ä¼˜åŒ–é—®é¢˜æ—¶ï¼Œéœ€è¦è¿›è¡Œé‡å¤ä¸”ç¹ççš„è¿‘ä¼¼ï¼Œæ•ˆç‡è¾ƒä½ï¼Œä¸”æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†MTMOOé—®é¢˜è½¬åŒ–ä¸ºåºåˆ—åˆ°åºåˆ—çš„å»ºæ¨¡é—®é¢˜ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å­¦ä¹ MTMOOé—®é¢˜çš„é€šç”¨è¡¨ç¤ºï¼Œå¹¶é¢„æµ‹ç›®æ ‡å€¼ã€‚é€šè¿‡æ–‡æœ¬æ ‡è®°åŒ–å°†MTMOOé—®é¢˜è½¬åŒ–ä¸ºLLMå¯ä»¥å¤„ç†çš„åºåˆ—æ•°æ®ï¼Œä»è€Œå®ç°ç»Ÿä¸€çš„ä»£ç†å­¦ä¹ ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šQ-MetaSuråŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) MTMOOé—®é¢˜æ–‡æœ¬æ ‡è®°åŒ–æ¨¡å—ï¼Œå°†MTMOOå®ä¾‹è½¬åŒ–ä¸ºæ–‡æœ¬åºåˆ—ï¼›2) åŸºäºLLMçš„ä»£ç†æ¨¡å‹ï¼Œç”¨äºç¼–ç MTMOOå®ä¾‹å¹¶è§£ç ç›®æ ‡å€¼ï¼›3) ä¸¤é˜¶æ®µç¦»çº¿è®­ç»ƒç­–ç•¥ï¼ŒåŒ…æ‹¬ç›‘ç£è°ƒä¼˜å’Œå¼ºåŒ–å­¦ä¹ å¾®è°ƒã€‚æ•´ä½“æµç¨‹ä¸ºï¼šé¦–å…ˆä½¿ç”¨ç¦»çº¿æ•°æ®é›†è¿›è¡Œç›‘ç£å­¦ä¹ ï¼Œæ‹Ÿåˆç°æœ‰çŸ¥è¯†ï¼›ç„¶ååˆ©ç”¨å¼ºåŒ–å­¦ä¹ å¾®è°ƒæ¨¡å‹ï¼Œæå‡æ³›åŒ–æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå°†å¤šç›®æ ‡ä¼˜åŒ–é—®é¢˜è½¬åŒ–ä¸ºåºåˆ—åˆ°åºåˆ—å»ºæ¨¡ï¼Œå¹¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å­¦ä¹ é€šç”¨è¡¨ç¤ºã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒQ-MetaSuræ— éœ€é’ˆå¯¹æ¯ä¸ªMTMOOé—®é¢˜å•ç‹¬è®¾è®¡ä»£ç†æ¨¡å‹ï¼Œè€Œæ˜¯é€šè¿‡å­¦ä¹ é€šç”¨è¡¨ç¤ºï¼Œå®ç°æ›´é«˜æ•ˆã€æ›´é€šç”¨çš„ä»£ç†å­¦ä¹ ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨æ–‡æœ¬æ ‡è®°åŒ–è¡¨ç¤ºMTMOOé—®é¢˜ï¼Œä¾‹å¦‚å°†å†³ç­–å˜é‡ã€ç›®æ ‡å‡½æ•°ç­‰ä¿¡æ¯è½¬åŒ–ä¸ºæ–‡æœ¬tokenï¼›2) ä½¿ç”¨Transformeræ¶æ„çš„LLMä½œä¸ºä»£ç†æ¨¡å‹ï¼Œä¾‹å¦‚BERTæˆ–GPTï¼›3) ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œç›‘ç£å­¦ä¹ é˜¶æ®µä½¿ç”¨å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ç­‰æŸå¤±å‡½æ•°ï¼Œå¼ºåŒ–å­¦ä¹ é˜¶æ®µä½¿ç”¨Implicit Q-Learning (IQL) ç®—æ³•ï¼Œä¼˜åŒ–æ¨¡å‹çš„æ³›åŒ–æ€§èƒ½ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.15149v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.15149v1/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.15149v1/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨CEC2019åŸºå‡†æµ‹è¯•ä¸­ï¼ŒQ-MetaSuråœ¨ç›®æ ‡è¿‘ä¼¼ç²¾åº¦æ–¹é¢ä¼˜äºä»£è¡¨æ€§çš„ä»£ç†åŸºçº¿ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒQ-MetaSurä¸ä»…èƒ½å¤Ÿæ›´å‡†ç¡®åœ°é¢„æµ‹ç›®æ ‡å€¼ï¼Œè€Œä¸”èƒ½å¤Ÿå¸®åŠ©åº•å±‚è¿›åŒ–ç®—æ³•å®ç°æ›´å¥½çš„ä¼˜åŒ–æ”¶æ•›æ€§å’Œå¸•ç´¯æ‰˜æœ€ä¼˜æ€§ï¼Œæ˜¾è‘—æå‡äº†ä¼˜åŒ–æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Q-MetaSurå¯åº”ç”¨äºå„ç§éœ€è¦è¿›è¡Œæ˜‚è´µä¼˜åŒ–çš„é¢†åŸŸï¼Œä¾‹å¦‚ææ–™è®¾è®¡ã€è¯ç‰©å‘ç°ã€å·¥ç¨‹ä¼˜åŒ–ç­‰ã€‚é€šè¿‡é™ä½ä¼˜åŒ–è¿‡ç¨‹çš„è®¡ç®—æˆæœ¬ï¼Œå¯ä»¥åŠ é€Ÿæ–°ææ–™å’Œæ–°äº§å“çš„ç ”å‘ï¼Œæé«˜å·¥ç¨‹è®¾è®¡çš„æ•ˆç‡ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼å’Œæ½œåœ¨çš„ç»æµæ•ˆç›Šã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Data-driven evolutionary algorithms has shown surprising results in addressing expensive optimization problems through robust surrogate modeling. Though promising, existing surrogate modeling schemes may encounter limitations in complex optimization problems with many sub-objectives, which rely on repeated and tedious approximation. To address such technical gap, we propose Q-MetaSur as a plug-and-play surrogate modeling scheme capable of providing unified and generalized surrogate learning. Specifically, we consider multi-task-multi-objective optimization~(MTMOO) in offline setting. Several key designs are proposed: 1) we transform objective approximation into sequence-to-sequence modeling where MTMOO problem can be represented by tenxual tokenization. To operate under such auto-regressive modeling, we introduce a Large Language Model-based surrogate model that first encodes a MTMOO instance and then decodes objective values of unseen decision variables. To ensure stability in training the proposed model, we propose a two-stage offline training strategy that operates as a synergy of supervised tuning and RL fine-tuning, which first exploits offline dataset to fit existing knowledge and then leverages RL to enhance model's generalization performance. Extensive empirical results on the CEC2019 benchmark demonstrate that Q-MetaSur not only outperforms representative surrogate baselines in objective approximation accuracy, but also helps underlying evolutionary algorithms achieve both desired optimization convergence and improved pareto optimality.

