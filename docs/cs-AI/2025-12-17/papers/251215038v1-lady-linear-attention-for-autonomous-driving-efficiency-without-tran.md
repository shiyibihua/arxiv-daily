---
layout: default
title: LADY: Linear Attention for Autonomous Driving Efficiency without Transformers
---

# LADY: Linear Attention for Autonomous Driving Efficiency without Transformers

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.15038" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.15038v1</a>
  <a href="https://arxiv.org/pdf/2512.15038.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.15038v1" onclick="toggleFavorite(this, '2512.15038v1', 'LADY: Linear Attention for Autonomous Driving Efficiency without Transformers')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jihao Huang, Xi Xia, Zhiyuan Li, Tianle Liu, Jingke Wang, Junbo Chen, Tengju Ye

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-17

**å¤‡æ³¨**: Under review

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLADYï¼šä¸€ç§åŸºäºçº¿æ€§æ³¨æ„åŠ›çš„é«˜æ•ˆè‡ªåŠ¨é©¾é©¶æ¨¡å‹ï¼Œæ— éœ€Transformerã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation)**

**å…³é”®è¯**: `è‡ªåŠ¨é©¾é©¶` `çº¿æ€§æ³¨æ„åŠ›` `ç«¯åˆ°ç«¯å­¦ä¹ ` `è·¨æ¨¡æ€èåˆ` `è¾¹ç¼˜è®¡ç®—`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. Transformeråœ¨è‡ªåŠ¨é©¾é©¶ä¸­çš„åº”ç”¨å—é™äºå…¶äºŒæ¬¡æ–¹çº§åˆ«çš„è®¡ç®—å¤æ‚åº¦ï¼Œéš¾ä»¥å¤„ç†é•¿æ—¶åºæ•°æ®ï¼Œå°¤å…¶æ˜¯åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šã€‚
2. LADYé€šè¿‡å®Œå…¨çº¿æ€§æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ç°äº†å¯¹é•¿ç¨‹æ—¶åºä¸Šä¸‹æ–‡çš„æœ‰æ•ˆå»ºæ¨¡ï¼ŒåŒæ—¶ä¿æŒäº†æ’å®šçš„è®¡ç®—å’Œå†…å­˜æˆæœ¬ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒLADYåœ¨è‡ªåŠ¨é©¾é©¶åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨è¾¹ç¼˜è®¾å¤‡ä¸ŠæˆåŠŸéƒ¨ç½²ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç«¯åˆ°ç«¯èŒƒå¼åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸå±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¤§å¤šåŸºäºTransformeræ¶æ„ï¼Œå…¶äºŒæ¬¡æ–¹çº§åˆ«çš„æ³¨æ„åŠ›è®¡ç®—æˆæœ¬é™åˆ¶äº†å¯¹é•¿æ—¶ç©ºåºåˆ—çš„å»ºæ¨¡èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºå—é™çš„è¾¹ç¼˜å¹³å°ä¸Šã€‚ç”±äºè‡ªåŠ¨é©¾é©¶æœ¬è´¨ä¸Šéœ€è¦é«˜æ•ˆçš„æ—¶åºå»ºæ¨¡ï¼Œè¿™ä¸€æŒ‘æˆ˜ä¸¥é‡é™åˆ¶äº†å…¶éƒ¨ç½²å’Œå®æ—¶æ€§èƒ½ã€‚è¿‘å¹´æ¥ï¼Œçº¿æ€§æ³¨æ„åŠ›æœºåˆ¶å› å…¶ä¼˜è¶Šçš„æ—¶ç©ºå¤æ‚åº¦è€Œå¤‡å—å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„çº¿æ€§æ³¨æ„åŠ›æ¶æ„ä»…é™äºè‡ªæ³¨æ„åŠ›ï¼Œç¼ºä¹å¯¹è·¨æ¨¡æ€å’Œè·¨æ—¶åºäº¤äº’çš„æ”¯æŒï¼Œè€Œè¿™å¯¹äºè‡ªåŠ¨é©¾é©¶è‡³å…³é‡è¦ã€‚æœ¬æ–‡æå‡ºäº†LADYï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå®Œå…¨åŸºäºçº¿æ€§æ³¨æ„åŠ›çš„ç”Ÿæˆå¼ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶æ¨¡å‹ã€‚LADYèƒ½å¤Ÿåœ¨æ¨ç†æ—¶èåˆé•¿ç¨‹æ—¶åºä¸Šä¸‹æ–‡ï¼Œä¸”è®¡ç®—å’Œå†…å­˜æˆæœ¬æ’å®šï¼Œä¸å—ç›¸æœºå’Œæ¿€å…‰é›·è¾¾ç‰¹å¾å†å²é•¿åº¦çš„å½±å“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è½»é‡çº§çš„çº¿æ€§äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½å¤Ÿå®ç°æœ‰æ•ˆçš„è·¨æ¨¡æ€ä¿¡æ¯äº¤æ¢ã€‚åœ¨NAVSIMå’ŒBench2DriveåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒLADYä»¥æ’å®šçš„æ—¶é—´å’Œå†…å­˜å¤æ‚åº¦å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæä¾›äº†æ”¹è¿›çš„è§„åˆ’æ€§èƒ½å¹¶æ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ã€‚è¯¥æ¨¡å‹å·²åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²å’ŒéªŒè¯ï¼Œè¯æ˜äº†å…¶åœ¨èµ„æºå—é™åœºæ™¯ä¸­çš„å®ç”¨æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºTransformerçš„è‡ªåŠ¨é©¾é©¶æ¨¡å‹è®¡ç®—å¤æ‚åº¦é«˜ï¼Œéš¾ä»¥å¤„ç†é•¿æ—¶åºæ•°æ®ï¼Œé™åˆ¶äº†å…¶åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šçš„éƒ¨ç½²å’Œå®æ—¶æ€§èƒ½ã€‚ç°æœ‰çº¿æ€§æ³¨æ„åŠ›æ–¹æ³•ç¼ºä¹å¯¹è·¨æ¨¡æ€ï¼ˆå¦‚ç›¸æœºå’Œæ¿€å…‰é›·è¾¾ï¼‰å’Œè·¨æ—¶åºä¿¡æ¯çš„æœ‰æ•ˆèåˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šLADYçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨çº¿æ€§æ³¨æ„åŠ›æœºåˆ¶æ›¿ä»£Transformerä¸­çš„ä¼ ç»Ÿæ³¨æ„åŠ›æœºåˆ¶ï¼Œä»è€Œå°†è®¡ç®—å¤æ‚åº¦ä»äºŒæ¬¡æ–¹é™ä½åˆ°çº¿æ€§çº§åˆ«ã€‚é€šè¿‡è®¾è®¡æ–°çš„çº¿æ€§äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ç°è·¨æ¨¡æ€ä¿¡æ¯çš„æœ‰æ•ˆèåˆï¼Œä»è€Œæå‡è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„æ„ŸçŸ¥å’Œå†³ç­–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLADYæ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„ç”Ÿæˆå¼æ¨¡å‹ï¼Œå…¶æ•´ä½“æ¶æ„åŒ…æ‹¬ç‰¹å¾æå–æ¨¡å—ã€çº¿æ€§æ³¨æ„åŠ›æ¨¡å—å’Œæ§åˆ¶é¢„æµ‹æ¨¡å—ã€‚ç‰¹å¾æå–æ¨¡å—è´Ÿè´£ä»ç›¸æœºå’Œæ¿€å…‰é›·è¾¾æ•°æ®ä¸­æå–ç‰¹å¾ã€‚çº¿æ€§æ³¨æ„åŠ›æ¨¡å—è´Ÿè´£èåˆé•¿ç¨‹æ—¶åºä¸Šä¸‹æ–‡å’Œè·¨æ¨¡æ€ä¿¡æ¯ã€‚æ§åˆ¶é¢„æµ‹æ¨¡å—æ ¹æ®èåˆåçš„ç‰¹å¾é¢„æµ‹è½¦è¾†çš„æ§åˆ¶æŒ‡ä»¤ã€‚

**å…³é”®åˆ›æ–°**ï¼šLADYçš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ç¬¬ä¸€ä¸ªå®Œå…¨åŸºäºçº¿æ€§æ³¨æ„åŠ›çš„ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶æ¨¡å‹ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§è½»é‡çº§çš„çº¿æ€§äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ã€‚è¿™ç§çº¿æ€§äº¤å‰æ³¨æ„åŠ›æœºåˆ¶èƒ½å¤Ÿæœ‰æ•ˆåœ°èåˆæ¥è‡ªä¸åŒæ¨¡æ€ï¼ˆå¦‚ç›¸æœºå’Œæ¿€å…‰é›·è¾¾ï¼‰çš„ä¿¡æ¯ï¼ŒåŒæ—¶ä¿æŒè¾ƒä½çš„è®¡ç®—å¤æ‚åº¦ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒLADYèƒ½å¤Ÿåœ¨å¤„ç†é•¿æ—¶åºæ•°æ®æ—¶ä¿æŒæ’å®šçš„è®¡ç®—å’Œå†…å­˜æˆæœ¬ã€‚

**å…³é”®è®¾è®¡**ï¼šLADYä½¿ç”¨äº†çº¿æ€§åŒ–çš„æ³¨æ„åŠ›è®¡ç®—æ–¹æ³•ï¼Œä¾‹å¦‚ä½¿ç”¨æ ¸å‡½æ•°å°†queryå’Œkeyçš„ä¹˜ç§¯è½¬åŒ–ä¸ºç‰¹å¾çš„çº¿æ€§ç»„åˆã€‚çº¿æ€§äº¤å‰æ³¨æ„åŠ›æ¨¡å—çš„è®¾è®¡è€ƒè™‘äº†ä¸åŒæ¨¡æ€ç‰¹å¾çš„å·®å¼‚æ€§ï¼Œé‡‡ç”¨äº†ç‹¬ç«‹çš„çº¿æ€§å˜æ¢æ¥å¤„ç†ä¸åŒæ¨¡æ€çš„ç‰¹å¾ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬æ§åˆ¶æŒ‡ä»¤é¢„æµ‹çš„æŸå¤±å’Œè½¨è¿¹é¢„æµ‹çš„æŸå¤±ï¼Œç”¨äºä¼˜åŒ–æ¨¡å‹çš„æ€§èƒ½ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.15038v1/figures/cover_photo.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.15038v1/figures/new_whole.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.15038v1/figures/rwkv.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

LADYåœ¨NAVSIMå’ŒBench2DriveåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶åœ¨è‡ªåŠ¨é©¾é©¶ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚ä¸åŸºäºTransformerçš„æ¨¡å‹ç›¸æ¯”ï¼ŒLADYåœ¨ä¿æŒç”šè‡³æå‡è§„åˆ’æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ï¼Œå®ç°äº†æ’å®šçš„æ—¶é—´å’Œå†…å­˜å¤æ‚åº¦ã€‚æ­¤å¤–ï¼ŒLADYå·²æˆåŠŸéƒ¨ç½²åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šï¼ŒéªŒè¯äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å¯è¡Œæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

LADYé€‚ç”¨äºå„ç§è‡ªåŠ¨é©¾é©¶åº”ç”¨åœºæ™¯ï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¡ç®—å¹³å°ä¸Šã€‚å®ƒå¯ä»¥åº”ç”¨äºä½åŠŸè€—è‡ªåŠ¨é©¾é©¶è½¦è¾†ã€æœºå™¨äººä»¥åŠå…¶ä»–éœ€è¦å®æ—¶æ„ŸçŸ¥å’Œå†³ç­–çš„åµŒå…¥å¼ç³»ç»Ÿã€‚LADYçš„ä½è®¡ç®—æˆæœ¬å’Œé«˜æ•ˆç‡ä½¿å…¶èƒ½å¤Ÿéƒ¨ç½²åœ¨ç®—åŠ›æœ‰é™çš„è®¾å¤‡ä¸Šï¼Œä»è€Œæ¨åŠ¨è‡ªåŠ¨é©¾é©¶æŠ€æœ¯çš„æ™®åŠå’Œåº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> End-to-end paradigms have demonstrated great potential for autonomous driving. Additionally, most existing methods are built upon Transformer architectures. However, transformers incur a quadratic attention cost, limiting their ability to model long spatial and temporal sequences-particularly on resource-constrained edge platforms. As autonomous driving inherently demands efficient temporal modeling, this challenge severely limits their deployment and real-time performance. Recently, linear attention mechanisms have gained increasing attention due to their superior spatiotemporal complexity. However, existing linear attention architectures are limited to self-attention, lacking support for cross-modal and cross-temporal interactions-both crucial for autonomous driving. In this work, we propose LADY, the first fully linear attention-based generative model for end-to-end autonomous driving. LADY enables fusion of long-range temporal context at inference with constant computational and memory costs, regardless of the history length of camera and LiDAR features. Additionally, we introduce a lightweight linear cross-attention mechanism that enables effective cross-modal information exchange. Experiments on the NAVSIM and Bench2Drive benchmarks demonstrate that LADY achieves state-of-the-art performance with constant-time and memory complexity, offering improved planning performance and significantly reduced computational cost. Additionally, the model has been deployed and validated on edge devices, demonstrating its practicality in resource-limited scenarios.

