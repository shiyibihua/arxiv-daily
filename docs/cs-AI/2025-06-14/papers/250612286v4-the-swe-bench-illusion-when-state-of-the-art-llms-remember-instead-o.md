---
layout: default
title: The SWE-Bench Illusion: When State-of-the-Art LLMs Remember Instead of Reason
---

# The SWE-Bench Illusion: When State-of-the-Art LLMs Remember Instead of Reason

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.12286" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.12286v4</a>
  <a href="https://arxiv.org/pdf/2506.12286.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.12286v4" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.12286v4', 'The SWE-Bench Illusion: When State-of-the-Art LLMs Remember Instead of Reason')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Shanchao Liang, Spandan Garg, Roshanak Zilouchian Moghaddam

**ÂàÜÁ±ª**: cs.AI, cs.SE

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-14 (Êõ¥Êñ∞: 2025-12-01)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Êñ∞ËØäÊñ≠‰ªªÂä°‰ª•Êè≠Á§∫LLMsÂú®ÁºñÁ†ÅËÉΩÂäõËØÑ‰º∞‰∏≠ÁöÑËÆ∞ÂøÜÂÅèÂ∑Æ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `ÁºñÁ†ÅËÉΩÂäõ` `Âü∫ÂáÜÊµãËØï` `ËÆ∞ÂøÜÂÅèÂ∑Æ` `ËΩØ‰ª∂Â∑•Á®ã` `ËØäÊñ≠‰ªªÂä°`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑËØÑ‰º∞ÊñπÊ≥ïÂèØËÉΩÂ§∏Â§ß‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®ÁºñÁ†Å‰ªªÂä°‰∏≠ÁöÑÁúüÂÆûËÉΩÂäõÔºåÂ∞§ÂÖ∂ÊòØÂÖ∂ËÆ∞ÂøÜ‰∏éÊé®ÁêÜÁöÑÂå∫ÂàÜ‰∏çÊòéÁ°Æ„ÄÇ
2. Êú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏§‰∏™Êñ∞ÁöÑËØäÊñ≠‰ªªÂä°ÔºåÊó®Âú®ÈÄöËøá‰ªÖ‰æùËµñÈóÆÈ¢òÊèèËø∞ÂíåÂΩìÂâçÊñá‰ª∂‰∏ä‰∏ãÊñáÊù•Êé¢ÊµãÊ®°ÂûãÁöÑÂü∫Á°ÄÁü•ËØÜ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÂΩìÂâçÊúÄÂÖàËøõÁöÑÊ®°ÂûãÂú®SWE-Bench-Verified‰∏äË°®Áé∞Âá∫È´òËææ76%ÁöÑÂáÜÁ°ÆÁéáÔºå‰ΩÜÂú®Êú™ÂåÖÂê´ÁöÑ‰ªªÂä°‰∏ä‰ªÖ‰∏∫53%ÔºåË°®ÊòéÂèØËÉΩÂ≠òÂú®Êï∞ÊçÆÊ±°ÊüìÊàñËÆ∞ÂøÜÁé∞Ë±°„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÈöèÁùÄÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâËÉΩÂäõÁöÑÊèêÂçáÂíåÂπøÊ≥õÂ∫îÁî®ÔºåÂü∫ÂáÜÊµãËØïÂú®ËØÑ‰º∞ÂÖ∂ÂÆûÈôÖÊïàÁî®ÊñπÈù¢ÂèëÊå•‰∫ÜÈáçË¶Å‰ΩúÁî®„ÄÇSWE-Bench Verified‰Ωú‰∏∫ËØÑ‰º∞LLMsËΩØ‰ª∂Â∑•Á®ãËÉΩÂäõÁöÑÈáçË¶ÅÂü∫ÂáÜÔºåÊòæÁ§∫Âá∫Ëøô‰∫õÊ®°ÂûãÂú®Â§çÊùÇÁºñÁ†Å‰ªªÂä°‰∏äÁöÑËâØÂ•ΩË°®Áé∞„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâËØÑ‰º∞ÂçèËÆÆÂèØËÉΩÂ§∏Â§ß‰∫ÜËøô‰∫õÊ®°ÂûãÁöÑÁúüÂÆûËÉΩÂäõ„ÄÇÊú¨Á†îÁ©∂ÂºïÂÖ•‰∫Ü‰∏§‰∏™ËØäÊñ≠‰ªªÂä°Ôºå‰ª•Êé¢ÊµãÊ®°ÂûãÁöÑÂü∫Á°ÄÁü•ËØÜÔºåÂπ∂Êèê‰æõÂÆûËØÅËØÅÊçÆË°®ÊòéÔºåSWE-Bench-Verified‰∏äÁöÑÊÄßËÉΩÊèêÂçáÂèØËÉΩÈÉ®ÂàÜÊ∫ê‰∫éËÆ∞ÂøÜËÄåÈùûÁúüÊ≠£ÁöÑÈóÆÈ¢òËß£ÂÜ≥ËÉΩÂäõ„ÄÇËøô‰∫õÂèëÁé∞ÂºïÂèë‰∫ÜÂØπÁé∞ÊúâÁªìÊûúÊúâÊïàÊÄßÁöÑÊãÖÂøßÔºåÂπ∂Âº∫Ë∞É‰∫ÜÈúÄË¶ÅÊõ¥Á®≥ÂÅ•„ÄÅÊäóÊ±°ÊüìÁöÑÂü∫ÂáÜÊù•ÂèØÈù†ËØÑ‰º∞LLMsÁöÑÁºñÁ†ÅËÉΩÂäõ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨Á†îÁ©∂Êó®Âú®Ëß£ÂÜ≥Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®ÁºñÁ†ÅËÉΩÂäõËØÑ‰º∞‰∏≠Â≠òÂú®ÁöÑËÆ∞ÂøÜÂÅèÂ∑ÆÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÊú™ËÉΩÊúâÊïàÂå∫ÂàÜÊ®°ÂûãÁöÑÁúüÊ≠£ÈóÆÈ¢òËß£ÂÜ≥ËÉΩÂäõ‰∏éÂÖ∂ËÆ∞ÂøÜËÉΩÂäõÔºåÂØºËá¥ËØÑ‰º∞ÁªìÊûúÁöÑÂèØÈù†ÊÄßÂèóÂà∞Ë¥®Áñë„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÈÄöËøáÂºïÂÖ•‰∏§‰∏™Êñ∞ÁöÑËØäÊñ≠‰ªªÂä°ÔºåÂàÜÂà´ÊòØ‰ªéÈóÆÈ¢òÊèèËø∞‰∏≠ËØÜÂà´Êñá‰ª∂Ë∑ØÂæÑÂíåÂú®‰ªÖÊúâÂΩìÂâçÊñá‰ª∂‰∏ä‰∏ãÊñáÁöÑÊÉÖÂÜµ‰∏ãÈáçÁé∞ÁúüÂÆûÂáΩÊï∞ÔºåÊù•Êé¢ÊµãÊ®°ÂûãÁöÑÂü∫Á°ÄÁü•ËØÜ„ÄÇËøôÁßçËÆæËÆ°Êó®Âú®Êè≠Á§∫Ê®°ÂûãÂú®ÁºñÁ†Å‰ªªÂä°‰∏≠ÁöÑÁúüÂÆûËÉΩÂäõÔºåËÄåÈùû‰ªÖ‰ªÖ‰æùËµñ‰∫éËÆ∞ÂøÜ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏§‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöÊñá‰ª∂Ë∑ØÂæÑËØÜÂà´ÂíåÂáΩÊï∞ÈáçÁé∞„ÄÇÊØè‰∏™Ê®°ÂùóÈÄöËøáÂàÜÊûêÈóÆÈ¢òÊèèËø∞Âíå‰∏ä‰∏ãÊñá‰ø°ÊÅØÔºåËØÑ‰º∞Ê®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõÂíåÁü•ËØÜÊéåÊè°ÊÉÖÂÜµ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞ÁÇπÂú®‰∫éÈÄöËøáÊñ∞ÁöÑËØäÊñ≠‰ªªÂä°Êè≠Á§∫‰∫ÜÊ®°ÂûãÂú®ÁºñÁ†ÅËÉΩÂäõËØÑ‰º∞‰∏≠ÁöÑËÆ∞ÂøÜÂÅèÂ∑Æ„ÄÇËøô‰∏éÁé∞ÊúâÊñπÊ≥ïÁöÑÊú¨Ë¥®Âå∫Âà´Âú®‰∫éÔºåÁé∞ÊúâÊñπÊ≥ïÂæÄÂæÄÊú™ËÉΩËÄÉËôëÊ®°ÂûãÁöÑËÆ∞ÂøÜÊïàÂ∫î„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂÆûÈ™å‰∏≠ÔºåÊ®°ÂûãÁöÑËØÑ‰º∞‰æùËµñ‰∫éÂáÜÁ°ÆÁöÑ‰∏ä‰∏ãÊñá‰ø°ÊÅØÂíåÈóÆÈ¢òÊèèËø∞ÔºåÈááÁî®‰∫ÜÁâπÂÆöÁöÑÂáÜÁ°ÆÁéáÊåáÊ†áÊù•Ë°°ÈáèÊ®°ÂûãÂú®‰∏çÂêåÂü∫ÂáÜ‰∏äÁöÑË°®Áé∞ÔºåÁ°Æ‰øù‰∫ÜËØÑ‰º∞ÁöÑ‰∏•Ë∞®ÊÄß„ÄÇÂÖ∑‰ΩìÁöÑÂèÇÊï∞ËÆæÁΩÆÂíåÊçüÂ§±ÂáΩÊï∞ËÆæËÆ°Êú™Âú®ÊëòË¶Å‰∏≠ËØ¶ÁªÜËØ¥ÊòéÔºåÈúÄÂèÇËÄÉÂÆåÊï¥ËÆ∫Êñá„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÂΩìÂâçÊúÄÂÖàËøõÁöÑÊ®°ÂûãÂú®SWE-Bench-Verified‰∏äËØÜÂà´ÈîôËØØÊñá‰ª∂Ë∑ØÂæÑÁöÑÂáÜÁ°ÆÁéáÈ´òËææ76%ÔºåËÄåÂú®Êú™ÂåÖÂê´ÁöÑ‰ªªÂä°‰∏ä‰ªÖ‰∏∫53%„ÄÇÊ≠§Â§ñÔºåÂáΩÊï∞ÈáçÁé∞‰ªªÂä°ÁöÑËøûÁª≠5-gramÂáÜÁ°ÆÁéáÂú®SWE-Bench Verified‰∏äËææÂà∞35%ÔºåËÄåÂú®ÂÖ∂‰ªñÂü∫ÂáÜ‰∏ä‰ªÖ‰∏∫18%ÔºåÊòæÁ§∫Âá∫ÊòæËëóÁöÑÊÄßËÉΩÂ∑ÆÂºÇ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨ËΩØ‰ª∂Â∑•Á®ã„ÄÅËá™Âä®ÂåñÁºñÁ†ÅÂíåÊô∫ËÉΩÂä©ÊâãÁ≠â„ÄÇÈÄöËøáÊõ¥ÂáÜÁ°ÆÂú∞ËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÁºñÁ†ÅËÉΩÂäõÔºåËÉΩÂ§üÊèêÂçáËøô‰∫õÊ®°ÂûãÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÂèØÈù†ÊÄßÂíåÊúâÊïàÊÄßÔºåËøõËÄåÊé®Âä®Êô∫ËÉΩÁºñÁ®ãÂ∑•ÂÖ∑ÁöÑÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> As large language models (LLMs) become increasingly capable and widely adopted, benchmarks play a central role in assessing their practical utility. For example, SWE-Bench Verified has emerged as a critical benchmark for evaluating LLMs' software engineering abilities, particularly their aptitude for resolving real-world GitHub issues. Recent LLMs show impressive performance on SWE-Bench, leading to optimism about their capacity for complex coding tasks. However, current evaluation protocols may overstate these models' true capabilities. It is crucial to distinguish LLMs' generalizable problem-solving ability and other learned artifacts. In this work, we introduce two diagnostic tasks: file path identification from issue descriptions alone and ground truth function reproduction with only the current file context and issue description to probe models' underlying knowledge. We present empirical evidence that performance gains on SWE-Bench-Verified may be partially driven by memorization rather than genuine problem-solving. We show that state-of-the-art models achieve up to 76% accuracy in identifying buggy file paths using only issue descriptions, without access to repository structure. This performance is merely up to 53% on tasks from repositories not included in SWE-Bench, pointing to possible data contamination or memorization. Similar patterns are also observed for the function reproduction task, where the verbatim similarity is much higher on SWE-Bench Verified than on other similar coding benchmarks (up to 35% consecutive 5-gram accuracy on SWE-Bench Verified and Full, but only up to 18% for tasks in other benchmarks). These findings raise concerns about the validity of existing results and underscore the need for more robust, contamination-resistant benchmarks to reliably evaluate LLMs' coding abilities.

