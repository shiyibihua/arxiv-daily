---
layout: default
title: Language Models Do Not Follow Occam's Razor: A Benchmark for Inductive and Abductive Reasoning
---

# Language Models Do Not Follow Occam's Razor: A Benchmark for Inductive and Abductive Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.03345" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.03345v1</a>
  <a href="https://arxiv.org/pdf/2509.03345.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.03345v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.03345v1', 'Language Models Do Not Follow Occam\'s Razor: A Benchmark for Inductive and Abductive Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yunxin Sun, Abulhair Saparov

**åˆ†ç±»**: cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-03

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºInAbHyDåŸºå‡†æµ‹è¯•LLMçš„å½’çº³å’Œæº¯å› æ¨ç†èƒ½åŠ›ï¼Œå‘ç°å…¶ä¸éµå¾ªå¥¥å¡å§†å‰ƒåˆ€åŸåˆ™**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å½’çº³æ¨ç†` `æº¯å› æ¨ç†` `å¥¥å¡å§†å‰ƒåˆ€` `åŸºå‡†æ•°æ®é›†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å·¥ä½œä¸»è¦å…³æ³¨LLMçš„æ¼”ç»æ¨ç†ï¼Œå¿½ç•¥äº†åœ¨ç°å®ä¸–ç•Œé—®é¢˜ä¸­åŒæ ·é‡è¦çš„å½’çº³å’Œæº¯å› æ¨ç†ã€‚
2. è®ºæ–‡æå‡ºInAbHyDæ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°LLMåœ¨ä¸å®Œæ•´ä¸–ç•Œæ¨¡å‹ä¸‹åŸºäºè§‚å¯Ÿç»“æœç”Ÿæˆå‡è®¾çš„èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒLLMåœ¨ç®€å•åœºæ™¯ä¸‹è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¤æ‚åœºæ™¯å’Œç”Ÿæˆé«˜è´¨é‡å‡è®¾æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡è¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å½’çº³å’Œæº¯å› æ¨ç†èƒ½åŠ›ï¼Œè¿™äº›èƒ½åŠ›åœ¨è§£å†³ç°å®ä¸–ç•Œé—®é¢˜ä¸­è‡³å…³é‡è¦ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªå¯ç¼–ç¨‹çš„åˆæˆæ•°æ®é›†InAbHyDï¼Œå…¶ä¸­æ¯ä¸ªæ¨ç†ç¤ºä¾‹éƒ½åŒ…å«ä¸€ä¸ªä¸å®Œæ•´çš„ä¸–ç•Œæ¨¡å‹å’Œä¸€ç»„è§‚å¯Ÿç»“æœã€‚æ™ºèƒ½ä½“çš„ä»»åŠ¡æ˜¯åœ¨ä¸å®Œæ•´çš„ä¸–ç•Œæ¨¡å‹ä¸‹ç”Ÿæˆå‡è®¾æ¥è§£é‡Šè§‚å¯Ÿç»“æœï¼Œä»è€Œè§£å†³æ¯ä¸ªæ¨ç†ç¤ºä¾‹ã€‚ä½œè€…è¿˜æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°å‡è®¾è´¨é‡çš„æŒ‡æ ‡ï¼Œè¯¥æŒ‡æ ‡åŸºäºå¥¥å¡å§†å‰ƒåˆ€åŸåˆ™ã€‚å¯¹ä¸€äº›æœ€å…ˆè¿›çš„LLMè¿›è¡Œäº†è¯„ä¼°å’Œåˆ†æï¼Œç»“æœè¡¨æ˜ï¼ŒLLMå¯ä»¥åœ¨ç®€å•åœºæ™¯ä¸­æ‰§è¡Œå½’çº³å’Œæº¯å› æ¨ç†ï¼Œä½†åœ¨å¤æ‚çš„ä¸–ç•Œæ¨¡å‹å’Œç”Ÿæˆé«˜è´¨é‡å‡è®¾æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå³ä½¿ä½¿ç”¨è¯¸å¦‚ä¸Šä¸‹æ–‡å­¦ä¹ å’ŒRLVRç­‰æµè¡Œçš„æ¨ç†å¢å¼ºæŠ€æœ¯ä¹Ÿæ˜¯å¦‚æ­¤ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å½’çº³å’Œæº¯å› æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨æ¼”ç»æ¨ç†ä¸Šï¼Œå¿½ç•¥äº†å½’çº³å’Œæº¯å› æ¨ç†åœ¨è§£å†³ç°å®ä¸–ç•Œé—®é¢˜ä¸­çš„é‡è¦æ€§ã€‚æ­¤å¤–ï¼Œç¼ºä¹ä¸“é—¨ç”¨äºè¯„ä¼°LLMè¿™ä¸¤ç§æ¨ç†èƒ½åŠ›çš„åŸºå‡†æ•°æ®é›†ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªå¯æ§çš„åˆæˆæ•°æ®é›†ï¼Œå…è®¸ç ”ç©¶äººå‘˜ç³»ç»Ÿåœ°è¯„ä¼°LLMåœ¨ä¸åŒå¤æ‚ç¨‹åº¦çš„ä¸–ç•Œæ¨¡å‹å’Œè§‚å¯Ÿæ¡ä»¶ä¸‹çš„æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡è®¾è®¡InAbHyDæ•°æ®é›†ï¼Œå¯ä»¥ç²¾ç¡®æ§åˆ¶ä¸–ç•Œæ¨¡å‹çš„å¤æ‚æ€§ã€è§‚å¯Ÿç»“æœçš„ç±»å‹å’Œæ•°é‡ï¼Œä»è€Œæ›´æ·±å…¥åœ°äº†è§£LLMçš„æ¨ç†å±€é™æ€§ã€‚åŒæ—¶ï¼Œå¼•å…¥åŸºäºå¥¥å¡å§†å‰ƒåˆ€åŸåˆ™çš„è¯„ä¼°æŒ‡æ ‡ï¼Œé¼“åŠ±æ¨¡å‹ç”Ÿæˆæ›´ç®€æ´ã€æ›´åˆç†çš„å‡è®¾ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šInAbHyDæ•°æ®é›†çš„ç”Ÿæˆè¿‡ç¨‹æ˜¯å¯ç¼–ç¨‹çš„ï¼Œå…è®¸ç”¨æˆ·è‡ªå®šä¹‰ä¸–ç•Œæ¨¡å‹çš„è§„åˆ™å’Œè§‚å¯Ÿç»“æœçš„ç”Ÿæˆæ–¹å¼ã€‚æ¯ä¸ªæ¨ç†ç¤ºä¾‹åŒ…å«ä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼šä¸€ä¸ªä¸å®Œæ•´çš„ä¸–ç•Œæ¨¡å‹ï¼ˆå®šä¹‰äº†å®ä½“ã€å…³ç³»å’Œè§„åˆ™ï¼‰ï¼Œä¸€ç»„è§‚å¯Ÿç»“æœï¼ˆæè¿°äº†ä¸–ç•Œçš„çŠ¶æ€ï¼‰ï¼Œä»¥åŠä¸€ä¸ªéœ€è¦ç”Ÿæˆçš„å‡è®¾ï¼ˆè§£é‡Šè§‚å¯Ÿç»“æœï¼‰ã€‚è¯„ä¼°è¿‡ç¨‹ä½¿ç”¨åŸºäºå¥¥å¡å§†å‰ƒåˆ€åŸåˆ™çš„æŒ‡æ ‡æ¥è¡¡é‡å‡è®¾çš„è´¨é‡ï¼Œè¯¥æŒ‡æ ‡å€¾å‘äºé€‰æ‹©æœ€ç®€å•çš„è§£é‡Šã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†InAbHyDæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°LLMå½’çº³å’Œæº¯å› æ¨ç†èƒ½åŠ›çš„åˆæˆæ•°æ®é›†ã€‚2) å¼•å…¥äº†åŸºäºå¥¥å¡å§†å‰ƒåˆ€åŸåˆ™çš„è¯„ä¼°æŒ‡æ ‡ï¼Œç”¨äºè¡¡é‡å‡è®¾çš„è´¨é‡ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒInAbHyDæ•°æ®é›†å…è®¸æ›´ç²¾ç»†çš„æ§åˆ¶å’Œè¯„ä¼°ï¼Œè€Œå¥¥å¡å§†å‰ƒåˆ€æŒ‡æ ‡åˆ™é¼“åŠ±æ¨¡å‹ç”Ÿæˆæ›´åˆç†çš„å‡è®¾ã€‚

**å…³é”®è®¾è®¡**ï¼šInAbHyDæ•°æ®é›†ä¸­çš„ä¸–ç•Œæ¨¡å‹ç”±ä¸€ç»„è§„åˆ™å®šä¹‰ï¼Œè¿™äº›è§„åˆ™æè¿°äº†å®ä½“ä¹‹é—´çš„å…³ç³»ã€‚è§‚å¯Ÿç»“æœæ˜¯ä¸–ç•ŒçŠ¶æ€çš„å¿«ç…§ï¼Œå¯èƒ½åŒ…å«å™ªå£°æˆ–ä¸ç¡®å®šæ€§ã€‚å‡è®¾æ˜¯è§£é‡Šè§‚å¯Ÿç»“æœçš„ä¸€ç»„è§„åˆ™æˆ–äº‹å®ã€‚å¥¥å¡å§†å‰ƒåˆ€æŒ‡æ ‡é€šè¿‡æƒ©ç½šå‡è®¾çš„å¤æ‚æ€§ï¼ˆä¾‹å¦‚ï¼Œè§„åˆ™çš„æ•°é‡ï¼‰æ¥è¡¡é‡å‡è®¾çš„è´¨é‡ã€‚å®éªŒä¸­ä½¿ç”¨äº†ä¸åŒçš„LLMï¼Œå¹¶é‡‡ç”¨äº†ä¸Šä¸‹æ–‡å­¦ä¹ å’ŒRLVRç­‰æŠ€æœ¯æ¥æé«˜æ¨ç†æ€§èƒ½ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„å–å†³äºæ‰€ä½¿ç”¨çš„LLMã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMåœ¨ç®€å•çš„InAbHyDåœºæ™¯ä¸­è¡¨ç°å‡ºä¸€å®šçš„å½’çº³å’Œæº¯å› æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨å¤æ‚åœºæ™¯ä¸‹æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚å³ä½¿ä½¿ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ å’ŒRLVRç­‰æŠ€æœ¯ï¼ŒLLMåœ¨ç”Ÿæˆé«˜è´¨é‡å‡è®¾æ–¹é¢ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚è¿™è¡¨æ˜LLMåœ¨ç†è§£å¤æ‚ä¸–ç•Œæ¨¡å‹å’Œéµå¾ªå¥¥å¡å§†å‰ƒåˆ€åŸåˆ™æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œéœ€è¦è¿›ä¸€æ­¥çš„ç ”ç©¶å’Œæ”¹è¿›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå¼€å‘æ›´æ™ºèƒ½ã€æ›´å¯é çš„AIç³»ç»Ÿï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦è¿›è¡Œå‡è®¾ç”Ÿæˆå’Œè§£é‡Šçš„é¢†åŸŸï¼Œä¾‹å¦‚ç§‘å­¦å‘ç°ã€æ•…éšœè¯Šæ–­ã€å®‰å…¨åˆ†æç­‰ã€‚é€šè¿‡æé«˜LLMçš„å½’çº³å’Œæº¯å› æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥ä½¿å…¶æ›´å¥½åœ°ç†è§£å¤æ‚çš„ä¸–ç•Œæ¨¡å‹ï¼Œå¹¶åšå‡ºæ›´åˆç†çš„å†³ç­–ã€‚æœªæ¥çš„ç ”ç©¶å¯ä»¥æ¢ç´¢å¦‚ä½•å°†è¿™äº›æŠ€æœ¯åº”ç”¨äºå®é™…é—®é¢˜ï¼Œå¹¶è¿›ä¸€æ­¥æé«˜LLMçš„æ¨ç†æ€§èƒ½ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reasoning is a core capability in artificial intelligence systems, for which large language models (LLMs) have recently shown remarkable progress. However, most work focuses exclusively on deductive reasoning, which is problematic since other types of reasoning are also essential in solving real-world problems, and they are less explored. This work focuses on evaluating LLMs' inductive and abductive reasoning capabilities. We introduce a programmable and synthetic dataset, InAbHyD (pronounced in-a-bid), where each reasoning example consists of an incomplete world model and a set of observations. The task for the intelligent agent is to produce hypotheses to explain observations under the incomplete world model to solve each reasoning example. We propose a new metric to evaluate the quality of hypotheses based on Occam's Razor. We evaluate and analyze some state-of-the-art LLMs. Our analysis shows that LLMs can perform inductive and abductive reasoning in simple scenarios, but struggle with complex world models and producing high-quality hypotheses, even with popular reasoning-enhancing techniques such as in-context learning and RLVR.

