---
layout: default
title: Emergent Hierarchical Reasoning in LLMs through Reinforcement Learning
---

# Emergent Hierarchical Reasoning in LLMs through Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.03646" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.03646v3</a>
  <a href="https://arxiv.org/pdf/2509.03646.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.03646v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.03646v3', 'Emergent Hierarchical Reasoning in LLMs through Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Haozhe Wang, Qixin Xu, Che Liu, Junhong Wu, Fangzhen Lin, Wenhu Chen

**åˆ†ç±»**: cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-03 (æ›´æ–°: 2025-09-27)

**å¤‡æ³¨**: Preprint

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHICRAç®—æ³•ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ æå‡LLMçš„å±‚çº§æ¨ç†èƒ½åŠ›ï¼Œä¼˜åŒ–ç­–ç•¥è§„åˆ’ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `å±‚çº§æ¨ç†` `ä¿¡ç”¨åˆ†é…` `æˆ˜ç•¥è§„åˆ’` `æ¶Œç°èƒ½åŠ›` `HICRAç®—æ³•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨æå‡LLMæ¨ç†èƒ½åŠ›æ—¶ï¼Œä¼˜åŒ–å‹åŠ›åˆ†é…ä¸å‡ï¼Œå¯¼è‡´å­¦ä¹ ä¿¡å·è¢«ç¨€é‡Šã€‚
2. è®ºæ–‡æå‡ºå±‚çº§æ„ŸçŸ¥ä¿¡ç”¨åˆ†é…ï¼ˆHICRAï¼‰ç®—æ³•ï¼Œä¸“æ³¨äºä¼˜åŒ–é«˜å½±å“åŠ›çš„è§„åˆ’tokenï¼Œæå‡å­¦ä¹ æ•ˆç‡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒHICRAç®—æ³•æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ï¼Œå¹¶æä¾›äº†å…³äºæˆ˜ç•¥æ¢ç´¢å¦‚ä½•æå‡æ¨ç†èƒ½åŠ›çš„æ·±åˆ»è§è§£ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç ”ç©¶è¡¨æ˜ï¼Œå¼ºåŒ–å­¦ä¹ èƒ½æœ‰æ•ˆæå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤æ‚æ¨ç†èƒ½åŠ›ï¼Œå¹¶æ­ç¤ºäº†å…¶èƒŒåçš„æœºåˆ¶ã€‚ç ”ç©¶å‘ç°ï¼Œâ€œé¡¿æ‚Ÿæ—¶åˆ»â€ã€â€œé•¿åº¦ç¼©æ”¾â€å’Œç†µåŠ¨åŠ›å­¦ç­‰ç°è±¡å¹¶éå­¤ç«‹å­˜åœ¨ï¼Œè€Œæ˜¯æ¶Œç°æ¨ç†å±‚çº§çš„æ ‡å¿—ï¼Œç±»ä¼¼äºäººç±»è®¤çŸ¥ä¸­é«˜å±‚æˆ˜ç•¥è§„åˆ’ä¸ä½å±‚ç¨‹åºæ‰§è¡Œçš„åˆ†ç¦»ã€‚æ¨¡å‹å­¦ä¹ å‘ˆç°å‡ºä¸¤é˜¶æ®µåŠ¨æ€ï¼šåˆå§‹é˜¶æ®µå—é™äºç¨‹åºæ­£ç¡®æ€§ï¼Œå¿…é¡»æå‡ä½å±‚æŠ€èƒ½ï¼›éšåï¼Œå­¦ä¹ ç“¶é¢ˆè½¬ç§»åˆ°é«˜å±‚æˆ˜ç•¥è§„åˆ’çš„æ¢ç´¢å’ŒæŒæ¡ã€‚é’ˆå¯¹ç°æœ‰å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆå¦‚GRPOï¼‰ä¼˜åŒ–å‹åŠ›åˆ†é…ä¸å‡çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºå±‚çº§æ„ŸçŸ¥ä¿¡ç”¨åˆ†é…ï¼ˆHICRAï¼‰ç®—æ³•ï¼Œå°†ä¼˜åŒ–é‡ç‚¹æ”¾åœ¨é«˜å½±å“åŠ›çš„è§„åˆ’tokenä¸Šã€‚å®éªŒéªŒè¯äº†HICRAç®—æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶æ·±å…¥åˆ†æäº†æˆ˜ç•¥æ¢ç´¢è§†è§’ä¸‹çš„æ¨ç†èƒ½åŠ›æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸­ï¼Œç”±äºä¼˜åŒ–å‹åŠ›åˆ†é…ä¸å‡å¯¼è‡´çš„æ¨ç†èƒ½åŠ›æå‡ç“¶é¢ˆé—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚GRPOï¼Œå¯¹æ‰€æœ‰tokenæ–½åŠ ç›¸åŒçš„ä¼˜åŒ–å‹åŠ›ï¼Œå¿½ç•¥äº†ä¸åŒtokenåœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„é‡è¦æ€§å·®å¼‚ï¼Œå¯¼è‡´å­¦ä¹ ä¿¡å·è¢«ç¨€é‡Šï¼Œè®­ç»ƒæ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯è¯†åˆ«å¹¶ä¸“æ³¨äºä¼˜åŒ–å¯¹æ¨ç†ç»“æœå½±å“æœ€å¤§çš„â€œè§„åˆ’tokenâ€ã€‚é€šè¿‡å°†ä¼˜åŒ–é‡ç‚¹æ”¾åœ¨è¿™äº›é«˜å½±å“åŠ›çš„tokenä¸Šï¼Œå¯ä»¥æ›´æœ‰æ•ˆåœ°æå‡LLMçš„æˆ˜ç•¥è§„åˆ’èƒ½åŠ›ï¼Œä»è€Œæé«˜æ•´ä½“çš„æ¨ç†æ€§èƒ½ã€‚è¿™ç§æ–¹æ³•æ¨¡æ‹Ÿäº†äººç±»è®¤çŸ¥ä¸­é«˜å±‚æˆ˜ç•¥è§„åˆ’ä¸ä½å±‚ç¨‹åºæ‰§è¡Œçš„åˆ†ç¦»ï¼Œæ—¨åœ¨æ›´æœ‰æ•ˆåœ°åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ä¿¡å·ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šHICRAç®—æ³•çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒLLMï¼›2) åˆ†ææ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„è¡Œä¸ºï¼Œè¯†åˆ«å¯¹æœ€ç»ˆç»“æœå½±å“æœ€å¤§çš„tokenï¼ˆå³â€œè§„åˆ’tokenâ€ï¼‰ï¼›3) è®¾è®¡ä¸€ç§ä¿¡ç”¨åˆ†é…æœºåˆ¶ï¼Œå°†æ›´å¤šçš„ä¼˜åŒ–å‹åŠ›åˆ†é…ç»™è¿™äº›è§„åˆ’tokenï¼›4) ä½¿ç”¨è°ƒæ•´åçš„ä¼˜åŒ–ç­–ç•¥æ›´æ–°æ¨¡å‹å‚æ•°ã€‚æ•´ä½“æµç¨‹æ—¨åœ¨å¼•å¯¼æ¨¡å‹æ›´å¤šåœ°å…³æ³¨æˆ˜ç•¥è§„åˆ’ï¼Œè€Œéä»…ä»…æ˜¯ç¨‹åºæ‰§è¡Œã€‚

**å…³é”®åˆ›æ–°**ï¼šHICRAç®—æ³•çš„æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå…¶å±‚çº§æ„ŸçŸ¥çš„ä¿¡ç”¨åˆ†é…æœºåˆ¶ã€‚ä¸ç°æœ‰æ–¹æ³•å¯¹æ‰€æœ‰tokenä¸€è§†åŒä»ä¸åŒï¼ŒHICRAèƒ½å¤Ÿæ ¹æ®tokenå¯¹æ¨ç†ç»“æœçš„å½±å“ç¨‹åº¦ï¼ŒåŠ¨æ€åœ°è°ƒæ•´ä¼˜åŒ–å‹åŠ›ã€‚è¿™ç§å·®å¼‚åŒ–çš„ä¼˜åŒ–ç­–ç•¥èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ä¿¡å·ï¼ŒåŠ é€Ÿæ¨¡å‹çš„å­¦ä¹ è¿‡ç¨‹ï¼Œå¹¶æå‡å…¶æˆ˜ç•¥è§„åˆ’èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šHICRAç®—æ³•çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å¦‚ä½•è¯†åˆ«â€œè§„åˆ’tokenâ€ï¼šè®ºæ–‡å¯èƒ½é‡‡ç”¨æ³¨æ„åŠ›æœºåˆ¶æˆ–å…¶ä»–æ–¹æ³•æ¥è¯„ä¼°tokençš„é‡è¦æ€§ï¼›2) ä¿¡ç”¨åˆ†é…ç­–ç•¥ï¼šå¦‚ä½•æ ¹æ®tokençš„é‡è¦æ€§æ¥è°ƒæ•´ä¼˜åŒ–å‹åŠ›ï¼Œä¾‹å¦‚ï¼Œå¯ä»¥é‡‡ç”¨åŠ æƒæŸå¤±å‡½æ•°æˆ–æ¢¯åº¦è£å‰ªç­‰æŠ€æœ¯ï¼›3) æŸå¤±å‡½æ•°çš„è®¾è®¡ï¼šå¯èƒ½éœ€è¦è®¾è®¡ä¸€ç§æ–°çš„æŸå¤±å‡½æ•°ï¼Œä»¥é¼“åŠ±æ¨¡å‹æ›´å¤šåœ°å…³æ³¨æˆ˜ç•¥è§„åˆ’ï¼Œè€Œéä»…ä»…æ˜¯ç¨‹åºæ‰§è¡Œã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚éœ€è¦åœ¨è®ºæ–‡ä¸­æŸ¥æ‰¾ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒHICRAç®—æ³•åœ¨æå‡LLMçš„æ¨ç†èƒ½åŠ›æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®ï¼ˆä¾‹å¦‚ï¼Œåœ¨ç‰¹å®šæ¨ç†ä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡æå‡ï¼‰éœ€è¦åœ¨è®ºæ–‡ä¸­æŸ¥æ‰¾ã€‚é€šè¿‡ä¸“æ³¨äºä¼˜åŒ–é«˜å½±å“åŠ›çš„è§„åˆ’tokenï¼ŒHICRAèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ä¿¡å·ï¼ŒåŠ é€Ÿæ¨¡å‹çš„å­¦ä¹ è¿‡ç¨‹ï¼Œå¹¶æå‡å…¶æˆ˜ç•¥è§„åˆ’èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦å¤æ‚æ¨ç†èƒ½åŠ›çš„åœºæ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½é—®ç­”ã€æ¸¸æˆAIã€è‡ªåŠ¨é©¾é©¶å†³ç­–ç­‰ã€‚é€šè¿‡æå‡LLMçš„æˆ˜ç•¥è§„åˆ’èƒ½åŠ›ï¼Œå¯ä»¥ä½¿å…¶åœ¨å¤æ‚ç¯å¢ƒä¸­åšå‡ºæ›´æ˜æ™ºçš„å†³ç­–ï¼Œæé«˜ä»»åŠ¡å®Œæˆçš„æ•ˆç‡å’Œè´¨é‡ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›æ¨åŠ¨é€šç”¨äººå·¥æ™ºèƒ½çš„å‘å±•ï¼Œä½¿å…¶åœ¨æ›´å¹¿æ³›çš„é¢†åŸŸå‘æŒ¥ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement Learning (RL) has proven highly effective at enhancing the complex reasoning abilities of Large Language Models (LLMs), yet underlying mechanisms driving this success remain largely opaque. Our analysis reveals that puzzling phenomena like ``aha moments", ``length-scaling'' and entropy dynamics are not disparate occurrences but hallmarks of an emergent reasoning hierarchy, akin to the separation of high-level strategic planning from low-level procedural execution in human cognition. We uncover a compelling two-phase dynamic: initially, a model is constrained by procedural correctness and must improve its low-level skills. The learning bottleneck then decisively shifts, with performance gains being driven by the exploration and mastery of high-level strategic planning. This insight exposes a core inefficiency in prevailing RL algorithms like GRPO, which apply optimization pressure agnostically and dilute the learning signal across all tokens. To address this, we propose Hierarchy-Aware Credit Assignment (HICRA), an algorithm that concentrates optimization efforts on high-impact planning tokens. Our extensive experiments validate that HICRA significantly outperforms strong baselines, and offer deep insights into how reasoning advances through the lens of strategic exploration.

