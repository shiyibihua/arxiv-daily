---
layout: default
title: Programming by Backprop: LLMs Acquire Reusable Algorithmic Abstractions During Code Training
---

# Programming by Backprop: LLMs Acquire Reusable Algorithmic Abstractions During Code Training

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.18777" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.18777v1</a>
  <a href="https://arxiv.org/pdf/2506.18777.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.18777v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.18777v1', 'Programming by Backprop: LLMs Acquire Reusable Algorithmic Abstractions During Code Training')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Jonathan Cook, Silvia Sapora, Arash Ahmadian, Akbir Khan, Tim Rocktaschel, Jakob Foerster, Laura Ruis

**ÂàÜÁ±ª**: cs.AI, cs.CL, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-23

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫ÁºñÁ®ãÂèçÂêë‰º†Êí≠ÊñπÊ≥ï‰ª•ÊèêÂçáLLMsÁöÑÁÆóÊ≥ïÊäΩË±°ËÉΩÂäõ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `ÁºñÁ®ãÂèçÂêë‰º†Êí≠` `ÁÆóÊ≥ïÊäΩË±°` `Ê∫ê‰ª£Á†ÅËÆ≠ÁªÉ` `Á®ãÂ∫èËØÑ‰º∞`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂØπLLMsÂú®Ê∫ê‰ª£Á†ÅËÆ≠ÁªÉ‰∏≠Êé®ÁêÜËÉΩÂäõÁöÑÊèêÂçáÊú∫Âà∂ÁêÜËß£‰∏çË∂≥ÔºåÁº∫‰πèÊúâÊïàÁöÑËß£Èáä„ÄÇ
2. Êú¨ÊñáÊèêÂá∫ÁºñÁ®ãÂèçÂêë‰º†Êí≠ÔºàPBBÔºâ‰Ωú‰∏∫‰∏ÄÁßçÊñ∞ÊñπÊ≥ïÔºåÈÄöËøá‰ªÖ‰ΩøÁî®Ê∫ê‰ª£Á†ÅËÆ≠ÁªÉÊ®°ÂûãÊù•ËØÑ‰º∞Á®ãÂ∫è„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåLLMsÂú®Ê≤°ÊúâËæìÂÖ•/ËæìÂá∫Á§∫‰æãÁöÑÊÉÖÂÜµ‰∏ãËÉΩÂ§üÊúâÊïàËØÑ‰º∞Á®ãÂ∫èÔºå‰∏î‰ΩøÁî®‰ª£Á†ÅÁöÑÊïàÊûú‰ºò‰∫éËØ≠Ë®ÄÊèèËø∞„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËÆ≠ÁªÉÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Ê∫ê‰ª£Á†Å‰∏äÊòæËëóÂ¢ûÂº∫‰∫ÜÂÖ∂ÈÄöÁî®Êé®ÁêÜËÉΩÂäõÔºå‰ΩÜÂÖ∂ËÉåÂêéÁöÑÊú∫Âà∂Â∞ö‰∏çÊ∏ÖÊ•ö„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜÁºñÁ®ãÂèçÂêë‰º†Êí≠ÔºàPBBÔºâ‰Ωú‰∏∫Ëøô‰∏ÄÊïàÂ∫îÁöÑÊΩúÂú®È©±Âä®Âõ†Á¥†ÔºåÊó®Âú®ÈÄöËøá‰ªÖËÆ≠ÁªÉÊ∫ê‰ª£Á†ÅÊù•Êïô‰ºöÊ®°ÂûãËØÑ‰º∞Á®ãÂ∫è„ÄÇÊàë‰ª¨ÂØπLLMsËøõË°å‰∫ÜÂæÆË∞ÉÔºå‰ΩøÁî®‰∫ÜÂåÖÂê´ËæìÂÖ•/ËæìÂá∫Á§∫‰æãÂíå‰ªÖÂåÖÂê´Ê∫ê‰ª£Á†ÅÁöÑ‰∏§ÁªÑÁ®ãÂ∫è„ÄÇÁªìÊûúË°®ÊòéÔºåLLMsÂú®Ê≤°ÊúâËæìÂÖ•/ËæìÂá∫Á§∫‰æãÁöÑÊÉÖÂÜµ‰∏ãËÉΩÂ§üËØÑ‰º∞Á®ãÂ∫èÔºåÂπ∂‰∏îÂú®‰ΩøÁî®‰ª£Á†ÅËÄåÈùûËØ≠‰πâÁ≠â‰ª∑ÁöÑËØ≠Ë®ÄÊèèËø∞Êó∂ÊïàÊûúÊõ¥‰Ω≥„ÄÇPBBÊñπÊ≥ï‰ΩøÂæóLLMsÂú®‰∏çÂêåËæìÂÖ•‰∏ãÁöÑÁ®ãÂ∫èËØÑ‰º∞Êõ¥Âä†Á®≥ÂÅ•ÔºåË°®Êòé‰ª£Á†ÅËÆ≠ÁªÉËÉΩÂ§üÂ∏ÆÂä©LLMsÂÜÖÂåñÂèØÈáçÁî®ÁöÑÁÆóÊ≥ïÊäΩË±°„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®Ê∫ê‰ª£Á†ÅËÆ≠ÁªÉ‰∏≠Êé®ÁêÜËÉΩÂäõÊèêÂçáÁöÑÊú∫Âà∂‰∏çÊòéÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÊú™ËÉΩÊúâÊïàÂà©Áî®Ê∫ê‰ª£Á†ÅËøõË°åÊ®°ÂûãËÆ≠ÁªÉÔºåÂØºËá¥Êé®ÁêÜËÉΩÂäõÁöÑÊèêÂçáÁº∫‰πèËß£Èáä„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÊèêÂá∫ÁºñÁ®ãÂèçÂêë‰º†Êí≠ÔºàPBBÔºâÊñπÊ≥ïÔºåÈÄöËøá‰ªÖ‰ΩøÁî®Ê∫ê‰ª£Á†ÅÊù•ËÆ≠ÁªÉÊ®°ÂûãÔºå‰ΩøÂÖ∂ËÉΩÂ§üÂú®Ê≤°ÊúâËæìÂÖ•/ËæìÂá∫Á§∫‰æãÁöÑÊÉÖÂÜµ‰∏ãËØÑ‰º∞Á®ãÂ∫è„ÄÇËøô‰∏ÄËÆæËÆ°Êó®Âú®ËÆ©Ê®°ÂûãÂÜÖÂåñÁÆóÊ≥ïÊäΩË±°Ôºå‰ªéËÄåÊèêÂçáÊé®ÁêÜËÉΩÂäõ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏§‰∏™‰∏ªË¶ÅÈò∂ÊÆµÔºöÈ¶ñÂÖàÔºåÂæÆË∞ÉLLMsÔºåÂàÜÂà´‰ΩøÁî®ÂåÖÂê´ËæìÂÖ•/ËæìÂá∫Á§∫‰æãÂíå‰ªÖÂåÖÂê´Ê∫ê‰ª£Á†ÅÁöÑÁ®ãÂ∫èÈõÜÔºõÂÖ∂Ê¨°ÔºåËØÑ‰º∞Ê®°ÂûãÂú®‰∏çÂêåÂÆûÈ™åËÆæÁΩÆ‰∏ãÁöÑË°®Áé∞ÔºåÂàÜÊûêÂÖ∂ÂØπÊó†ËæìÂÖ•/ËæìÂá∫Á®ãÂ∫èÁöÑËØÑ‰º∞ËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöPBBÊñπÊ≥ïÁöÑÊ†∏ÂøÉÂàõÊñ∞Âú®‰∫éÈÄöËøáÊ∫ê‰ª£Á†ÅËÆ≠ÁªÉÊ®°ÂûãÔºå‰ΩøÂÖ∂ËÉΩÂ§üÂú®Ê≤°ÊúâÁ§∫‰æãÁöÑÊÉÖÂÜµ‰∏ãËøõË°åÁ®ãÂ∫èËØÑ‰º∞„ÄÇËøô‰∏é‰º†ÁªüÊñπÊ≥ï‰æùËµñËæìÂÖ•/ËæìÂá∫ÂØπÁöÑËÆ≠ÁªÉÊñπÂºèÊúâÊú¨Ë¥®Âå∫Âà´„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂÆûÈ™å‰∏≠ÔºåÊ®°ÂûãÁöÑÊçüÂ§±ÂáΩÊï∞ËÆæËÆ°‰∏∫ÂÖ≥Ê≥®Á®ãÂ∫èËØÑ‰º∞ÁöÑÂáÜÁ°ÆÊÄßÔºåÁΩëÁªúÁªìÊûÑÂàôÈááÁî®‰∫ÜÈÄÇÂêàÂ§ÑÁêÜÊ∫ê‰ª£Á†ÅÁöÑTransformerÊû∂ÊûÑ„ÄÇÊ®°ÂûãÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÈÄöËøáÈìæÂºèÊÄùÁª¥ÈÄêÊ≠•ËØÑ‰º∞Á®ãÂ∫èÔºåÊèêÈ´ò‰∫ÜËæìÂá∫ÁöÑÂèØÈù†ÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºå‰ΩøÁî®PBBÊñπÊ≥ïÁöÑÊ®°ÂûãÂú®Ê≤°ÊúâËæìÂÖ•/ËæìÂá∫Á§∫‰æãÁöÑÊÉÖÂÜµ‰∏ãÔºåËÉΩÂ§üÊúâÊïàËØÑ‰º∞Á®ãÂ∫èÔºå‰∏îÂú®‰ΩøÁî®Ê∫ê‰ª£Á†ÅÊó∂ÁöÑË°®Áé∞ÊòæËëó‰ºò‰∫é‰ΩøÁî®ËØ≠‰πâÊèèËø∞ÁöÑÊ®°Âûã„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåPBBÊñπÊ≥ïÂú®Â§öÁßçÂÆûÈ™åËÆæÁΩÆ‰∏≠Â±ïÁé∞Âá∫Êõ¥È´òÁöÑËØÑ‰º∞ÂáÜÁ°ÆÊÄßÂíåÈ≤ÅÊ£íÊÄßÔºåË°®ÊòéÂÖ∂Âú®ÁÆóÊ≥ïÊäΩË±°ÂÜÖÂåñÊñπÈù¢ÁöÑÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Ëá™Âä®‰ª£Á†ÅÁîüÊàê„ÄÅÁ®ãÂ∫èÂàÜÊûêÂíåÊô∫ËÉΩÁºñÁ®ãÂä©ÊâãÁ≠â„ÄÇÈÄöËøáÊèêÂçáLLMsÂØπÁÆóÊ≥ïÊäΩË±°ÁöÑÁêÜËß£ËÉΩÂäõÔºåÂèØ‰ª•Âú®ËΩØ‰ª∂ÂºÄÂèë„ÄÅÊïôËÇ≤ÂíåÁ†îÁ©∂Á≠âÂ§ö‰∏™È¢ÜÂüü‰∫ßÁîüÊ∑±ËøúÂΩ±ÂìçÔºå‰øÉËøõ‰∫∫Êú∫Âçè‰ΩúÁöÑÊïàÁéáÂíåË¥®Èáè„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Training large language models (LLMs) on source code significantly enhances their general-purpose reasoning abilities, but the mechanisms underlying this generalisation are poorly understood. In this paper, we propose Programming by Backprop (PBB) as a potential driver of this effect - teaching a model to evaluate a program for inputs by training on its source code alone, without ever seeing I/O examples. To explore this idea, we finetune LLMs on two sets of programs representing simple maths problems and algorithms: one with source code and I/O examples (w/ IO), the other with source code only (w/o IO). We find evidence that LLMs have some ability to evaluate w/o IO programs for inputs in a range of experimental settings, and make several observations. Firstly, PBB works significantly better when programs are provided as code rather than semantically equivalent language descriptions. Secondly, LLMs can produce outputs for w/o IO programs directly, by implicitly evaluating the program within the forward pass, and more reliably when stepping through the program in-context via chain-of-thought. We further show that PBB leads to more robust evaluation of programs across inputs than training on I/O pairs drawn from a distribution that mirrors naturally occurring data. Our findings suggest a mechanism for enhanced reasoning through code training: it allows LLMs to internalise reusable algorithmic abstractions. Significant scope remains for future work to enable LLMs to more effectively learn from symbolic procedures, and progress in this direction opens other avenues like model alignment by training on formal constitutional principles.

