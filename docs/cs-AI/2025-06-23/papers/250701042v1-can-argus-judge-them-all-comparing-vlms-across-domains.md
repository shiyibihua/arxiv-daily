---
layout: default
title: Can Argus Judge Them All? Comparing VLMs Across Domains
---

# Can Argus Judge Them All? Comparing VLMs Across Domains

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2507.01042" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2507.01042v1</a>
  <a href="https://arxiv.org/pdf/2507.01042.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2507.01042v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2507.01042v1', 'Can Argus Judge Them All? Comparing VLMs Across Domains')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Harsh Joshi, Gautam Siddharth Kashyap, Rafiq Ali, Ebad Shabbir, Niharika Jain, Sarthak Jain, Jiechao Gao, Usman Naseem

**åˆ†ç±»**: cs.IR, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-23

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ¯”è¾ƒå¤šæ¨¡æ€æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸­çš„è¡¨ç°ä»¥æå‡åº”ç”¨æ•ˆæœ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€æ¨¡å‹` `å¤šæ¨¡æ€AI` `æ€§èƒ½è¯„ä¼°` `è·¨æ•°æ®é›†ä¸€è‡´æ€§` `æ¨¡å‹æ¯”è¾ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸­çš„æ€§èƒ½ä¸€è‡´æ€§ç ”ç©¶ä¸è¶³ï¼Œå¯¼è‡´å…¶åº”ç”¨æ•ˆæœä¸ç¨³å®šã€‚
2. æœ¬æ–‡é€šè¿‡åŸºå‡†æµ‹è¯•æ¯”è¾ƒäº†ä¸‰ç§ä¸»æµVLMsï¼Œæå‡ºäº†ä¸€ç§æ–°çš„è·¨æ•°æ®é›†ä¸€è‡´æ€§æŒ‡æ ‡ï¼Œä»¥è¯„ä¼°æ¨¡å‹åœ¨å¤šä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒCLIPåœ¨æ³›åŒ–èƒ½åŠ›ä¸Šè¡¨ç°æœ€ä½³ï¼ŒBLIPåœ¨ç‰¹å®šæ•°æ®é›†ä¸Šè¡¨ç°çªå‡ºï¼Œè€ŒLXMERTåœ¨æ¨ç†ä»»åŠ¡ä¸­å…·æœ‰ä¼˜åŠ¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æ­£åœ¨æ¨åŠ¨å¤šæ¨¡æ€äººå·¥æ™ºèƒ½çš„å‘å±•ï¼Œä½†å…¶åœ¨ä¸åŒä»»åŠ¡ä¸­çš„æ€§èƒ½ä¸€è‡´æ€§å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚æœ¬æ–‡å¯¹CLIPã€BLIPå’ŒLXMERTè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–äº†æ£€ç´¢ã€æè¿°ç”Ÿæˆå’Œæ¨ç†ç­‰å¤šç§æ•°æ®é›†ã€‚æˆ‘ä»¬çš„è¯„ä¼°åŒ…æ‹¬ä»»åŠ¡å‡†ç¡®æ€§ã€ç”Ÿæˆè´¨é‡ã€æ•ˆç‡ä»¥åŠä¸€ç§æ–°é¢–çš„è·¨æ•°æ®é›†ä¸€è‡´æ€§ï¼ˆCDCï¼‰æŒ‡æ ‡ã€‚ç»“æœæ˜¾ç¤ºï¼ŒCLIPåœ¨æ³›åŒ–èƒ½åŠ›ä¸Šè¡¨ç°æœ€ä½³ï¼ˆCDC: 0.92ï¼‰ï¼ŒBLIPåœ¨ç²¾å¿ƒç­–åˆ’çš„æ•°æ®ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè€ŒLXMERTåœ¨ç»“æ„åŒ–æ¨ç†ä¸­é¢†å…ˆã€‚è¿™äº›ç»“æœæ­ç¤ºäº†æ³›åŒ–ä¸ä¸“ä¸šåŒ–ä¹‹é—´çš„æƒè¡¡ï¼Œä¸ºVLMsçš„å·¥ä¸šéƒ¨ç½²æä¾›äº†æŒ‡å¯¼ï¼Œå¹¶æ¨åŠ¨äº†é¢å‘å¼ºå¥ã€ä»»åŠ¡çµæ´»æ¶æ„çš„å¼€å‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸­çš„æ€§èƒ½ä¸€è‡´æ€§ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„è¡¨ç°å·®å¼‚è¾ƒå¤§ï¼Œå½±å“äº†å…¶å®é™…åº”ç”¨æ•ˆæœã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¯¹CLIPã€BLIPå’ŒLXMERTè¿›è¡Œç³»ç»Ÿçš„åŸºå‡†æµ‹è¯•ï¼Œè¯„ä¼°å…¶åœ¨æ£€ç´¢ã€æè¿°ç”Ÿæˆå’Œæ¨ç†ç­‰ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œæå‡ºè·¨æ•°æ®é›†ä¸€è‡´æ€§ï¼ˆCDCï¼‰æŒ‡æ ‡æ¥è¡¡é‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶è®¾è®¡äº†ä¸€ä¸ªç»¼åˆè¯„ä¼°æ¡†æ¶ï¼ŒåŒ…å«ä»»åŠ¡å‡†ç¡®æ€§ã€ç”Ÿæˆè´¨é‡ã€æ•ˆç‡ç­‰å¤šä¸ªç»´åº¦çš„è¯„ä¼°ï¼Œç»“åˆCDCæŒ‡æ ‡è¿›è¡Œå…¨é¢æ¯”è¾ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæå‡ºäº†è·¨æ•°æ®é›†ä¸€è‡´æ€§ï¼ˆCDCï¼‰è¿™ä¸€æ–°é¢–çš„è¯„ä¼°æŒ‡æ ‡ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ­ç¤ºæ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸­çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæä¾›äº†æ›´å…¨é¢çš„æ€§èƒ½è¯„ä¼°è§†è§’ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œè®¾ç½®äº†å¤šç§ä»»åŠ¡åœºæ™¯ï¼Œé‡‡ç”¨æ ‡å‡†åŒ–çš„æ•°æ®é›†è¿›è¡Œè¯„ä¼°ï¼Œç¡®ä¿äº†ç»“æœçš„å¯æ¯”æ€§å’Œå¯é æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCLIPåœ¨è·¨æ•°æ®é›†ä¸€è‡´æ€§ï¼ˆCDCï¼‰æŒ‡æ ‡ä¸Šè¾¾åˆ°äº†0.92ï¼Œå±•ç°å‡ºæœ€ä½³çš„æ³›åŒ–èƒ½åŠ›ï¼›BLIPåœ¨ç‰¹å®šæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œé€‚åˆäºç²¾å¿ƒç­–åˆ’çš„ä»»åŠ¡ï¼›è€ŒLXMERTåœ¨ç»“æ„åŒ–æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°çªå‡ºã€‚è¿™äº›ç»“æœæ­ç¤ºäº†ä¸åŒæ¨¡å‹åœ¨æ³›åŒ–ä¸ä¸“ä¸šåŒ–ä¹‹é—´çš„æƒè¡¡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½æœç´¢å¼•æ“ã€è‡ªåŠ¨å†…å®¹ç”Ÿæˆã€ä»¥åŠäººæœºäº¤äº’ç³»ç»Ÿç­‰ã€‚é€šè¿‡æå‡è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨å¤šä»»åŠ¡ä¸­çš„ä¸€è‡´æ€§è¡¨ç°ï¼Œå¯ä»¥æ˜¾è‘—å¢å¼ºè¿™äº›åº”ç”¨çš„æ™ºèƒ½åŒ–æ°´å¹³å’Œç”¨æˆ·ä½“éªŒï¼Œæ¨åŠ¨å¤šæ¨¡æ€äººå·¥æ™ºèƒ½çš„å®é™…è½åœ°ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-Language Models (VLMs) are advancing multimodal AI, yet their performance consistency across tasks is underexamined. We benchmark CLIP, BLIP, and LXMERT across diverse datasets spanning retrieval, captioning, and reasoning. Our evaluation includes task accuracy, generation quality, efficiency, and a novel Cross-Dataset Consistency (CDC) metric. CLIP shows strongest generalization (CDC: 0.92), BLIP excels on curated data, and LXMERT leads in structured reasoning. These results expose trade-offs between generalization and specialization, informing industrial deployment of VLMs and guiding development toward robust, task-flexible architectures.

