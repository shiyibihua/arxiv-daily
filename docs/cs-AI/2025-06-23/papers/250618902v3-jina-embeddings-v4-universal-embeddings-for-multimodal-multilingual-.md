---
layout: default
title: jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval
---

# jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.18902" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.18902v3</a>
  <a href="https://arxiv.org/pdf/2506.18902.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.18902v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.18902v3', 'jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Michael GÃ¼nther, Saba Sturua, Mohammad Kalim Akram, Isabelle Mohr, Andrei Ungureanu, Bo Wang, Sedigheh Eslami, Scott Martens, Maximilian Werk, Nan Wang, Han Xiao

**åˆ†ç±»**: cs.AI, cs.CL, cs.IR

**å‘å¸ƒæ—¥æœŸ**: 2025-06-23 (æ›´æ–°: 2025-07-07)

**å¤‡æ³¨**: 22 pages, 1-10 main, 14-22 experimental results, benchmark tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºjina-embeddings-v4ä»¥è§£å†³å¤šæ¨¡æ€å¤šè¯­è¨€æ£€ç´¢é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ£€ç´¢` `ä½ç§©é€‚é…` `åµŒå…¥æ¨¡å‹` `è§†è§‰å†…å®¹å¤„ç†` `ä¿¡æ¯æ£€ç´¢` `è¯­ä¹‰ç›¸ä¼¼æ€§` `å›¾åƒæœç´¢`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæ¨¡æ€æ£€ç´¢æ–¹æ³•åœ¨å¤„ç†è§†è§‰ä¸°å¯Œå†…å®¹æ—¶è¡¨ç°ä¸ä½³ï¼Œéš¾ä»¥ç»Ÿä¸€æ–‡æœ¬å’Œå›¾åƒçš„è¡¨ç¤ºã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€åµŒå…¥æ¨¡å‹ï¼Œé€šè¿‡ä½ç§©é€‚é…å™¨ä¼˜åŒ–ä¸åŒæ£€ç´¢ä»»åŠ¡çš„æ€§èƒ½ï¼Œæ”¯æŒå¤šç§åµŒå…¥å½¢å¼ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œjina-embeddings-v4åœ¨å¤šæ¨¡æ€æ£€ç´¢ä»»åŠ¡ä¸Šè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œå°¤å…¶åœ¨è§†è§‰å†…å®¹å¤„ç†ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬ä»‹ç»äº†jina-embeddings-v4ï¼Œè¿™æ˜¯ä¸€ä¸ªæ‹¥æœ‰38äº¿å‚æ•°çš„å¤šæ¨¡æ€åµŒå…¥æ¨¡å‹ï¼Œé€šè¿‡ä¸€ç§æ–°é¢–çš„æ¶æ„ç»Ÿä¸€æ–‡æœ¬å’Œå›¾åƒè¡¨ç¤ºï¼Œæ”¯æŒå•å‘é‡å’Œå¤šå‘é‡åµŒå…¥çš„åæœŸäº¤äº’é£æ ¼ã€‚è¯¥æ¨¡å‹ç»“åˆäº†ä»»åŠ¡ç‰¹å®šçš„ä½ç§©é€‚é…ï¼ˆLoRAï¼‰é€‚é…å™¨ï¼Œä»¥ä¼˜åŒ–åœ¨æŸ¥è¯¢-æ–‡æ¡£æ£€ç´¢ã€è¯­ä¹‰æ–‡æœ¬ç›¸ä¼¼æ€§å’Œä»£ç æœç´¢ç­‰å¤šç§æ£€ç´¢åœºæ™¯ä¸­çš„æ€§èƒ½ã€‚å…¨é¢è¯„ä¼°è¡¨æ˜ï¼Œjina-embeddings-v4åœ¨å•æ¨¡æ€å’Œè·¨æ¨¡æ€æ£€ç´¢ä»»åŠ¡ä¸Šå‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå°¤å…¶åœ¨å¤„ç†è¡¨æ ¼ã€å›¾è¡¨ã€å›¾ç¤ºå’Œæ··åˆåª’ä½“æ ¼å¼ç­‰è§†è§‰ä¸°å¯Œå†…å®¹æ–¹é¢è¡¨ç°çªå‡ºã€‚ä¸ºè¯„ä¼°è¯¥èƒ½åŠ›ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†Jina-VDRï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºè§†è§‰ä¸°å¯Œå›¾åƒæ£€ç´¢è®¾è®¡çš„æ–°åŸºå‡†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€æ£€ç´¢ä¸­ç»Ÿä¸€æ–‡æœ¬ä¸å›¾åƒè¡¨ç¤ºçš„æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†è§†è§‰ä¸°å¯Œå†…å®¹æ—¶æ•ˆæœä¸ä½³ï¼Œéš¾ä»¥æ»¡è¶³å®é™…åº”ç”¨éœ€æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„jina-embeddings-v4æ¨¡å‹é€šè¿‡åˆ›æ–°çš„æ¶æ„è®¾è®¡ï¼Œç»“åˆä½ç§©é€‚é…å™¨ï¼ˆLoRAï¼‰ï¼Œå®ç°äº†å¯¹å¤šç§æ£€ç´¢ä»»åŠ¡çš„ä¼˜åŒ–ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†æ–‡æœ¬å’Œå›¾åƒçš„å¤šæ¨¡æ€è¡¨ç¤ºã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¨¡å‹çš„æ•´ä½“æ¶æ„åŒ…æ‹¬åµŒå…¥å±‚ã€LoRAé€‚é…å™¨å’ŒåæœŸäº¤äº’æ¨¡å—ï¼Œæ”¯æŒå•å‘é‡å’Œå¤šå‘é‡çš„åµŒå…¥æ–¹å¼ï¼Œèƒ½å¤Ÿçµæ´»åº”å¯¹ä¸åŒçš„æ£€ç´¢éœ€æ±‚ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥äº†ä½ç§©é€‚é…å™¨ï¼Œä½¿å¾—æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸­èƒ½å¤Ÿå¿«é€Ÿé€‚åº”ï¼Œæ˜¾è‘—æå‡äº†æ£€ç´¢æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨è§†è§‰å†…å®¹çš„å¤„ç†ä¸Šã€‚

**å…³é”®è®¾è®¡**ï¼šæ¨¡å‹çš„å‚æ•°è®¾ç½®åŒ…æ‹¬38äº¿ä¸ªå‚æ•°ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–æ£€ç´¢æ•ˆæœï¼Œç½‘ç»œç»“æ„è®¾è®¡ä¸Šæ³¨é‡äº†å¤šæ¨¡æ€ä¿¡æ¯çš„èåˆä¸äº¤äº’ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œjina-embeddings-v4åœ¨å•æ¨¡æ€å’Œè·¨æ¨¡æ€æ£€ç´¢ä»»åŠ¡ä¸Šå‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå°¤å…¶åœ¨å¤„ç†è§†è§‰ä¸°å¯Œå†…å®¹æ—¶ï¼Œæ€§èƒ½æå‡å¹…åº¦è¶…è¿‡äº†ç°æœ‰åŸºçº¿ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤æ‚æ£€ç´¢åœºæ™¯ä¸­çš„ä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ä¿¡æ¯æ£€ç´¢ã€å›¾åƒæœç´¢ã€è¯­ä¹‰åˆ†æç­‰ï¼Œèƒ½å¤Ÿä¸ºå¤šæ¨¡æ€æ•°æ®å¤„ç†æä¾›å¼ºå¤§çš„æ”¯æŒã€‚å…¶å®é™…ä»·å€¼åœ¨äºæå‡äº†å¤šè¯­è¨€å’Œå¤šæ¨¡æ€æ£€ç´¢çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œæœªæ¥å¯èƒ½åœ¨æ•™è‚²ã€åŒ»ç–—ã€é‡‘èç­‰è¡Œä¸šäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We introduce jina-embeddings-v4, a 3.8 billion parameter multimodal embedding model that unifies text and image representations through a novel architecture supporting both single-vector and multi-vector embeddings in the late interaction style. The model incorporates task-specific Low-Rank Adaptation (LoRA) adapters to optimize performance across diverse retrieval scenarios, including query-document retrieval, semantic text similarity, and code search. Comprehensive evaluations demonstrate that jina-embeddings-v4 achieves state-of-the-art performance on both single-modal and cross-modal retrieval tasks, with particular strength in processing visually rich content such as tables, charts, diagrams, and mixed-media formats. To facilitate evaluation of this capability, we also introduce Jina-VDR, a novel benchmark specifically designed for visually rich image retrieval.

