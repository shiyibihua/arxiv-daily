---
layout: default
title: LibEMER: A novel benchmark and algorithms library for EEG-based Multimodal Emotion Recognition
---

# LibEMER: A novel benchmark and algorithms library for EEG-based Multimodal Emotion Recognition

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.19330" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.19330v2</a>
  <a href="https://arxiv.org/pdf/2509.19330.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.19330v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.19330v2', 'LibEMER: A novel benchmark and algorithms library for EEG-based Multimodal Emotion Recognition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zejun Liu, Yunshan Chen, Chengxi Xie, Yugui Xie, Huan Liu

**åˆ†ç±»**: eess.SP, cs.AI, cs.HC, cs.LG, cs.MM

**å‘å¸ƒæ—¥æœŸ**: 2025-09-14 (æ›´æ–°: 2025-10-15)

**å¤‡æ³¨**: 5 pages, 2 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**LibEMERï¼šç”¨äºè„‘ç”µå¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«çš„åŸºå‡†æµ‹è¯•ä¸ç®—æ³•åº“**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è„‘ç”µä¿¡å·` `å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«` `æ·±åº¦å­¦ä¹ ` `åŸºå‡†æµ‹è¯•` `å¼€æºåº“`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è„‘ç”µå¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«ç ”ç©¶ç¼ºä¹å¼€æºå®ç°å’Œç»Ÿä¸€çš„è¯„ä¼°åŸºå‡†ï¼Œé˜»ç¢äº†å…¬å¹³çš„æ€§èƒ½æ¯”è¾ƒå’Œå¤ç°ã€‚
2. LibEMERæä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„è¯„ä¼°æ¡†æ¶ï¼ŒåŒ…å«å¯å¤ç°çš„PyTorchå®ç°å’Œæ ‡å‡†åŒ–çš„æ•°æ®å¤„ç†ã€æ¨¡å‹å®ç°å’Œå®éªŒè®¾ç½®ã€‚
3. LibEMERåœ¨ä¸‰ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šè¿›è¡Œäº†æ— åçš„æ€§èƒ½è¯„ä¼°ï¼Œä¸ºè¯¥é¢†åŸŸçš„ç ”ç©¶æä¾›äº†ä¸€ä¸ªå¯é çš„åŸºå‡†ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºäºè„‘ç”µçš„å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«(EMER)å—åˆ°äº†å¹¿æ³›å…³æ³¨å¹¶å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç”±äºäººç±»ç¥ç»ç³»ç»Ÿå›ºæœ‰çš„å¤æ‚æ€§ï¼Œå¤šæ¨¡æ€æ–¹æ³•çš„ç ”ç©¶æŠ•å…¥äº†å¤§é‡ç²¾åŠ›ã€‚ç„¶è€Œï¼Œè¯¥é¢†åŸŸç›®å‰å­˜åœ¨ä¸‰ä¸ªå…³é”®é™åˆ¶ï¼šï¼ˆiï¼‰ç¼ºä¹å¼€æºå®ç°ï¼›ï¼ˆiiï¼‰ç¼ºä¹ç”¨äºå…¬å¹³æ€§èƒ½åˆ†æçš„æ ‡å‡†åŒ–å’Œé€æ˜çš„åŸºå‡†ï¼›ï¼ˆiiiï¼‰å¯¹ä¸»è¦æŒ‘æˆ˜å’Œæœ‰å‰æ™¯çš„ç ”ç©¶æ–¹å‘çš„æ·±å…¥è®¨è®ºæ˜æ˜¾ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†LibEMERï¼Œä¸€ä¸ªç»Ÿä¸€çš„è¯„ä¼°æ¡†æ¶ï¼Œå®ƒæä¾›äº†ç²¾é€‰æ·±åº¦å­¦ä¹ æ–¹æ³•çš„å¯å®Œå…¨å¤ç°çš„PyTorchå®ç°ï¼Œä»¥åŠç”¨äºæ•°æ®é¢„å¤„ç†ã€æ¨¡å‹å®ç°å’Œå®éªŒè®¾ç½®çš„æ ‡å‡†åŒ–åè®®ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿåœ¨ä¸¤ä¸ªå­¦ä¹ ä»»åŠ¡ä¸­çš„ä¸‰ä¸ªå¹¿æ³›ä½¿ç”¨çš„å…¬å…±æ•°æ®é›†ä¸Šè¿›è¡Œæ— åçš„æ€§èƒ½è¯„ä¼°ã€‚è¯¥å¼€æºåº“å¯åœ¨ä»¥ä¸‹ç½‘å€å…¬å¼€è®¿é—®ï¼šhttps://anonymous.4open.science/r/2025ULUIUBUEUMUEUR485384

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè„‘ç”µå¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«æ—¨åœ¨åˆ©ç”¨è„‘ç”µä¿¡å·å’Œå…¶ä»–æ¨¡æ€æ•°æ®å‡†ç¡®è¯†åˆ«ä¸ªä½“çš„æƒ…æ„ŸçŠ¶æ€ã€‚ç°æœ‰æ–¹æ³•çš„ç—›ç‚¹åœ¨äºç¼ºä¹ç»Ÿä¸€çš„è¯„ä¼°æ ‡å‡†å’Œå¼€æºå®ç°ï¼Œå¯¼è‡´ç ”ç©¶ç»“æœéš¾ä»¥å¤ç°å’Œæ¯”è¾ƒï¼Œé˜»ç¢äº†è¯¥é¢†åŸŸçš„å¥åº·å‘å±•ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šLibEMERçš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªæ ‡å‡†åŒ–çš„ã€å¯å¤ç°çš„è¯„ä¼°æ¡†æ¶ï¼ŒåŒ…å«å¸¸ç”¨çš„æ•°æ®é›†ã€é¢„å¤„ç†æµç¨‹ã€æ¨¡å‹å®ç°å’Œè¯„ä¼°æŒ‡æ ‡ã€‚é€šè¿‡æä¾›å¼€æºçš„ä»£ç å’Œç»Ÿä¸€çš„å®éªŒè®¾ç½®ï¼ŒLibEMERæ—¨åœ¨ä¿ƒè¿›å…¬å¹³çš„æ€§èƒ½æ¯”è¾ƒå’Œç®—æ³•çš„æ”¹è¿›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLibEMERæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) æ•°æ®é›†ç®¡ç†ï¼šæ”¯æŒå¸¸ç”¨çš„è„‘ç”µæƒ…æ„Ÿæ•°æ®é›†ï¼Œå¹¶æä¾›æ ‡å‡†åŒ–çš„æ•°æ®åŠ è½½å’Œé¢„å¤„ç†æ¥å£ã€‚2) æ¨¡å‹åº“ï¼šåŒ…å«å¤šç§æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå¦‚å·ç§¯ç¥ç»ç½‘ç»œ(CNN)ã€å¾ªç¯ç¥ç»ç½‘ç»œ(RNN)å’ŒTransformerç­‰ï¼Œå¹¶æä¾›å¯å¤ç°çš„PyTorchå®ç°ã€‚3) è¯„ä¼°æ¨¡å—ï¼šæä¾›å¸¸ç”¨çš„æƒ…æ„Ÿè¯†åˆ«è¯„ä¼°æŒ‡æ ‡ï¼Œå¦‚å‡†ç¡®ç‡ã€F1å€¼ç­‰ï¼Œå¹¶æ”¯æŒäº¤å‰éªŒè¯ç­‰å®éªŒè®¾ç½®ã€‚4) åŸºå‡†æµ‹è¯•ï¼šåœ¨æ ‡å‡†æ•°æ®é›†ä¸Šè¯„ä¼°ä¸åŒæ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶æä¾›è¯¦ç»†çš„å®éªŒæŠ¥å‘Šã€‚

**å…³é”®åˆ›æ–°**ï¼šLibEMERçš„å…³é”®åˆ›æ–°åœ¨äºå…¶ç»Ÿä¸€æ€§å’Œå¯å¤ç°æ€§ã€‚å®ƒä¸ä»…æä¾›äº†ä¸€ç³»åˆ—å¸¸ç”¨çš„æ¨¡å‹å®ç°ï¼Œè¿˜æ ‡å‡†åŒ–äº†æ•°æ®é¢„å¤„ç†å’Œå®éªŒè®¾ç½®ï¼Œä½¿å¾—ç ”ç©¶è€…å¯ä»¥æ–¹ä¾¿åœ°æ¯”è¾ƒä¸åŒç®—æ³•çš„æ€§èƒ½ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œæ”¹è¿›ã€‚æ­¤å¤–ï¼ŒLibEMERçš„å¼€æºç‰¹æ€§ä¹Ÿä¿ƒè¿›äº†ç¤¾åŒºçš„åˆä½œå’ŒçŸ¥è¯†å…±äº«ã€‚

**å…³é”®è®¾è®¡**ï¼šLibEMERåœ¨æ¨¡å‹å®ç°æ–¹é¢ï¼Œé‡‡ç”¨äº†æ¨¡å—åŒ–çš„è®¾è®¡ï¼Œæ–¹ä¾¿ç”¨æˆ·è‡ªå®šä¹‰æ¨¡å‹ç»“æ„å’Œè®­ç»ƒå‚æ•°ã€‚åœ¨æ•°æ®é¢„å¤„ç†æ–¹é¢ï¼Œæä¾›äº†å¸¸ç”¨çš„æ»¤æ³¢ã€é™é‡‡æ ·ç­‰æ–¹æ³•ï¼Œå¹¶æ”¯æŒç”¨æˆ·è‡ªå®šä¹‰é¢„å¤„ç†æµç¨‹ã€‚åœ¨è¯„ä¼°æŒ‡æ ‡æ–¹é¢ï¼Œé™¤äº†å¸¸ç”¨çš„å‡†ç¡®ç‡å’ŒF1å€¼å¤–ï¼Œè¿˜æä¾›äº†æ··æ·†çŸ©é˜µç­‰å¯è§†åŒ–å·¥å…·ï¼Œæ–¹ä¾¿ç”¨æˆ·åˆ†ææ¨¡å‹çš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

LibEMERåœ¨ä¸‰ä¸ªå¹¿æ³›ä½¿ç”¨çš„å…¬å…±æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæä¾›å¯é çš„æ€§èƒ½è¯„ä¼°ï¼Œå¹¶ä¿ƒè¿›ä¸åŒç®—æ³•ä¹‹é—´çš„å…¬å¹³æ¯”è¾ƒã€‚é€šè¿‡LibEMERï¼Œç ”ç©¶äººå‘˜å¯ä»¥æ›´å®¹æ˜“åœ°å¤ç°ç°æœ‰ç ”ç©¶ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œæ”¹è¿›ï¼Œä»è€Œæ¨åŠ¨è„‘ç”µå¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«é¢†åŸŸçš„å‘å±•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

LibEMERåœ¨æƒ…æ„Ÿè®¡ç®—ã€äººæœºäº¤äº’ã€åŒ»ç–—å¥åº·ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥ç”¨äºå¼€å‘æƒ…æ„Ÿæ™ºèƒ½åŠ©æ‰‹ï¼Œæ ¹æ®ç”¨æˆ·çš„è„‘ç”µä¿¡å·è¯†åˆ«å…¶æƒ…æ„ŸçŠ¶æ€ï¼Œå¹¶æä¾›ä¸ªæ€§åŒ–çš„æœåŠ¡ã€‚æ­¤å¤–ï¼Œè¿˜å¯ä»¥ç”¨äºè¾…åŠ©è¯Šæ–­ç²¾ç¥ç–¾ç—…ï¼Œå¦‚æŠ‘éƒç—‡å’Œç„¦è™‘ç—‡ï¼Œé€šè¿‡åˆ†æè„‘ç”µä¿¡å·çš„å˜åŒ–æ¥è¯„ä¼°æ‚£è€…çš„æƒ…æ„ŸçŠ¶æ€ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> EEG-based multimodal emotion recognition(EMER) has gained significant attention and witnessed notable advancements, the inherent complexity of human neural systems has motivated substantial efforts toward multimodal approaches. However, this field currently suffers from three critical limitations: (i) the absence of open-source implementations. (ii) the lack of standardized and transparent benchmarks for fair performance analysis. (iii) in-depth discussion regarding main challenges and promising research directions is a notable scarcity. To address these challenges, we introduce LibEMER, a unified evaluation framework that provides fully reproducible PyTorch implementations of curated deep learning methods alongside standardized protocols for data preprocessing, model realization, and experimental setups. This framework enables unbiased performance assessment on three widely-used public datasets across two learning tasks. The open-source library is publicly accessible at: https://anonymous.4open.science/r/2025ULUIUBUEUMUEUR485384

