---
layout: default
title: Are Large Language Models Capable of Deep Relational Reasoning? Insights from DeepSeek-R1 and Benchmark Comparisons
---

# Are Large Language Models Capable of Deep Relational Reasoning? Insights from DeepSeek-R1 and Benchmark Comparisons

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.23128" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.23128v1</a>
  <a href="https://arxiv.org/pdf/2506.23128.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.23128v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.23128v1', 'Are Large Language Models Capable of Deep Relational Reasoning? Insights from DeepSeek-R1 and Benchmark Comparisons')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chi Chiu So, Yueyue Sun, Jun-Min Wang, Siu Pang Yung, Anthony Wai Keung Loh, Chun Pong Chau

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-29

**å¤‡æ³¨**: 10 pages, 0 figures, accepted by 2025 IEEE international conference on artificial intelligence testing (AITest)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/kelvinhkcs/Deep-Relational-Reasoning)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ·±å±‚å…³ç³»æ¨ç†ä¸­çš„èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æ·±å±‚å…³ç³»æ¨ç†` `é€»è¾‘æ¨ç†` `åŸºå‡†ä»»åŠ¡` `æ¨ç†èƒ½åŠ›`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡æ—¶é¢ä¸´æ˜¾è‘—æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨é—®é¢˜å¤æ‚æ€§å¢åŠ æ—¶è¡¨ç°ä¸ä½³ã€‚
2. è®ºæ–‡é€šè¿‡è®¾è®¡ä¸€ç³»åˆ—åŸºå‡†ä»»åŠ¡ï¼Œè¯„ä¼°å¹¶æ¯”è¾ƒäº†ä¸‰ç§å‰æ²¿å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œæå‡ºäº†DeepSeek-R1çš„ç‹¬ç‰¹ç­–ç•¥ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒDeepSeek-R1åœ¨å¤šä¸ªä»»åŠ¡ä¸­å–å¾—äº†æœ€é«˜çš„F1åˆ†æ•°ï¼Œä½†åœ¨å¤æ‚é—®é¢˜ä¸Šä»å­˜åœ¨æ¨ç†ä¸å®Œæ•´çš„é—®é¢˜ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æ¢è®¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ·±å±‚å…³ç³»æ¨ç†ä¸­çš„è¡¨ç°ï¼Œè¯„ä¼°äº†DeepSeek-R1ã€DeepSeek-V3å’ŒGPT-4oä¸‰ç§å‰æ²¿æ¨¡å‹åœ¨å®¶è°±å’Œä¸€èˆ¬å›¾æ¨ç†ä»»åŠ¡ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDeepSeek-R1åœ¨å¤šä¸ªä»»åŠ¡å’Œé—®é¢˜è§„æ¨¡ä¸­å‡å–å¾—äº†æœ€é«˜çš„F1åˆ†æ•°ï¼Œå±•ç°å‡ºå¼ºå¤§çš„é€»è¾‘æ¨ç†å’Œå…³ç³»æ¨æ–­èƒ½åŠ›ã€‚ç„¶è€Œï¼Œæ‰€æœ‰è¯„ä¼°æ¨¡å‹åœ¨é—®é¢˜å¤æ‚æ€§å¢åŠ æ—¶å‡è¡¨ç°ä¸ä½³ï¼Œä¸»è¦å—åˆ°ä»¤ç‰Œé•¿åº¦é™åˆ¶å’Œè¾“å‡ºç»“æ„ä¸å®Œæ•´çš„å½±å“ã€‚å¯¹DeepSeek-R1çš„é•¿é“¾æ€ç»´å“åº”çš„è¯¦ç»†åˆ†ææ­ç¤ºäº†å…¶ç‹¬ç‰¹çš„è§„åˆ’å’ŒéªŒè¯ç­–ç•¥ï¼Œä½†ä¹Ÿçªæ˜¾äº†æ¨ç†ä¸è¿è´¯æˆ–ä¸å®Œæ•´çš„å®ä¾‹ï¼Œå¼ºè°ƒäº†å¯¹LLMså†…éƒ¨æ¨ç†åŠ¨æ€çš„æ·±å…¥å®¡æŸ¥çš„å¿…è¦æ€§ã€‚æœ¬æ–‡è¿˜è®¨è®ºäº†æœªæ¥å·¥ä½œçš„å…³é”®æ–¹å‘ï¼ŒåŒ…æ‹¬å¤šæ¨¡æ€æ¨ç†çš„è§’è‰²å’Œæ¨ç†å¤±è´¥çš„ç³»ç»Ÿæ€§æ£€éªŒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ·±å±‚å…³ç³»æ¨ç†ä¸­çš„èƒ½åŠ›ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨é¢å¯¹å¤æ‚æ¨ç†ä»»åŠ¡æ—¶çš„è¡¨ç°ä¸ä½³ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚é—®é¢˜æ—¶å¸¸å¸¸å—åˆ°ä»¤ç‰Œé•¿åº¦é™åˆ¶å’Œè¾“å‡ºç»“æ„ä¸å®Œæ•´çš„å½±å“ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé€šè¿‡è®¾è®¡ä¸€ç³»åˆ—åŸºå‡†ä»»åŠ¡æ¥è¯„ä¼°å’Œæ¯”è¾ƒä¸åŒå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œç‰¹åˆ«å…³æ³¨DeepSeek-R1çš„é•¿é“¾æ€ç»´ç­–ç•¥ï¼Œä»¥æ­ç¤ºå…¶æ¨ç†è¿‡ç¨‹ä¸­çš„ä¼˜ç¼ºç‚¹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä»»åŠ¡è®¾è®¡ã€æ¨¡å‹è¯„ä¼°å’Œç»“æœåˆ†æä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œè®¾è®¡äº†ä¸€ç³»åˆ—å®¶è°±å’Œå›¾æ¨ç†ä»»åŠ¡ï¼›å…¶æ¬¡ï¼Œè¯„ä¼°ä¸‰ç§æ¨¡å‹åœ¨è¿™äº›ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼›æœ€åï¼Œåˆ†ææ¨¡å‹çš„æ¨ç†è¿‡ç¨‹å’Œç»“æœã€‚

**å…³é”®åˆ›æ–°**ï¼šDeepSeek-R1åœ¨é€»è¾‘æ¨ç†å’Œå…³ç³»æ¨æ–­æ–¹é¢å±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨å¤šä¸ªä»»åŠ¡ä¸­å–å¾—äº†æœ€é«˜çš„F1åˆ†æ•°ã€‚å…¶ç‹¬ç‰¹çš„è§„åˆ’å’ŒéªŒè¯ç­–ç•¥æ˜¯ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼ŒDeepSeek-R1é‡‡ç”¨äº†é•¿é“¾æ€ç»´çš„å“åº”æœºåˆ¶ï¼Œå°½ç®¡åœ¨å¤æ‚é—®é¢˜ä¸Šä»å­˜åœ¨æ¨ç†ä¸è¿è´¯çš„æƒ…å†µï¼Œä½†å…¶æ•´ä½“ç»“æ„å’Œå‚æ•°è®¾ç½®ä¸ºæ¨ç†æä¾›äº†æ–°çš„è§†è§’ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç»†èŠ‚å°šæœªæ˜ç¡®ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDeepSeek-R1åœ¨å¤šä¸ªä»»åŠ¡ä¸­å‡å–å¾—äº†æœ€é«˜çš„F1åˆ†æ•°ï¼Œè¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œæ‰€æœ‰æ¨¡å‹åœ¨é¢å¯¹å¤æ‚é—®é¢˜æ—¶å‡æ˜¾è‘—ä¸‹é™ï¼Œå¼ºè°ƒäº†æ¨ç†èƒ½åŠ›çš„å±€é™æ€§ã€‚å…·ä½“è€Œè¨€ï¼ŒDeepSeek-R1åœ¨æŸäº›ä»»åŠ¡ä¸­F1åˆ†æ•°æå‡å¹…åº¦è¾¾åˆ°äº†XX%ï¼Œä½†åœ¨å¤æ‚ä»»åŠ¡ä¸­ä»å­˜åœ¨æ¨ç†ä¸å®Œæ•´çš„æƒ…å†µã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½é—®ç­”ç³»ç»Ÿã€çŸ¥è¯†å›¾è°±æ„å»ºå’Œå¤æ‚å†³ç­–æ”¯æŒç­‰ã€‚é€šè¿‡æå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ·±å±‚å…³ç³»æ¨ç†ä¸­çš„èƒ½åŠ›ï¼Œå¯ä»¥ä¸ºå¤šç§å®é™…åº”ç”¨æä¾›æ›´ä¸ºå‡†ç¡®å’Œå¯é çš„æ¨ç†æ”¯æŒï¼Œæ¨åŠ¨äººå·¥æ™ºèƒ½åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„åº”ç”¨ä»·å€¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> How far are Large Language Models (LLMs) in performing deep relational reasoning? In this paper, we evaluate and compare the reasoning capabilities of three cutting-edge LLMs, namely, DeepSeek-R1, DeepSeek-V3 and GPT-4o, through a suite of carefully designed benchmark tasks in family tree and general graph reasoning. Our experiments reveal that DeepSeek-R1 consistently achieves the highest F1-scores across multiple tasks and problem sizes, demonstrating strong aptitude in logical deduction and relational inference. However, all evaluated models, including DeepSeek-R1, struggle significantly as problem complexity increases, largely due to token length limitations and incomplete output structures. A detailed analysis of DeepSeek-R1's long Chain-of-Thought responses uncovers its unique planning and verification strategies, but also highlights instances of incoherent or incomplete reasoning, calling attention to the need for deeper scrutiny into LLMs' internal inference dynamics. We further discuss key directions for future work, including the role of multimodal reasoning and the systematic examination of reasoning failures. Our findings provide both empirical insights and theoretical implications for advancing LLMs' reasoning abilities, particularly in tasks that demand structured, multi-step logical inference. Our code repository will be publicly available at https://github.com/kelvinhkcs/Deep-Relational-Reasoning.

