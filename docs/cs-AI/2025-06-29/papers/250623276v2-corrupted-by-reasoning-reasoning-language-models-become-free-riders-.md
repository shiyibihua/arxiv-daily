---
layout: default
title: Corrupted by Reasoning: Reasoning Language Models Become Free-Riders in Public Goods Games
---

# Corrupted by Reasoning: Reasoning Language Models Become Free-Riders in Public Goods Games

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.23276" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.23276v2</a>
  <a href="https://arxiv.org/pdf/2506.23276.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.23276v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.23276v2', 'Corrupted by Reasoning: Reasoning Language Models Become Free-Riders in Public Goods Games')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: David Guzman Piedrahita, Yongjin Yang, Mrinmaya Sachan, Giorgia Ramponi, Bernhard SchÃ¶lkopf, Zhijing Jin

**åˆ†ç±»**: cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-29 (æ›´æ–°: 2025-07-24)

**å¤‡æ³¨**: Published at COLM 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/davidguzmanp/SanctSim)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ¢è®¨è¯­è¨€æ¨¡å‹åœ¨å…¬å…±ç‰©å“åšå¼ˆä¸­çš„åˆä½œæœºåˆ¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­è¨€æ¨¡å‹` `å…¬å…±ç‰©å“åšå¼ˆ` `å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ` `åˆä½œæœºåˆ¶` `æ¨ç†èƒ½åŠ›` `ç¤¾ä¼šå›°å¢ƒ` `è¡Œä¸ºç»æµå­¦`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å½“å‰LLMåœ¨åˆä½œä¸è‡ªæˆ‘åˆ©ç›Šä¹‹é—´çš„å¹³è¡¡é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œå½±å“å…¶å¯¹é½æ€§å’Œå®‰å…¨éƒ¨ç½²ã€‚
2. æœ¬æ–‡é€šè¿‡é€‚åº”å…¬å…±ç‰©å“åšå¼ˆï¼Œç ”ç©¶LLMåœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„åˆä½œå†³ç­–æœºåˆ¶ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ¨ç†èƒ½åŠ›å¼ºçš„LLMåœ¨åˆä½œä¸Šè¡¨ç°ä¸ä½³ï¼Œè€Œä¼ ç»ŸLLMåˆ™èƒ½ç»´æŒé«˜æ°´å¹³çš„åˆä½œã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä½œä¸ºè‡ªä¸»ä»£ç†çš„å¹¿æ³›åº”ç”¨ï¼Œç†è§£å®ƒä»¬çš„åˆä½œä¸ç¤¾ä¼šæœºåˆ¶å˜å¾—æ„ˆå‘é‡è¦ã€‚æœ¬æ–‡ç ”ç©¶äº†å¤šæ™ºèƒ½ä½“LLMç³»ç»Ÿä¸­ï¼Œä»£ç†åœ¨æ¿€åŠ±åˆä½œæˆ–æƒ©ç½šèƒŒå›æ—¶çš„èµ„æºæŠ•èµ„å†³ç­–ã€‚é€šè¿‡é€‚åº”è¡Œä¸ºç»æµå­¦ä¸­çš„å…¬å…±ç‰©å“åšå¼ˆï¼Œè§‚å¯Ÿä¸åŒLLMåœ¨é‡å¤äº’åŠ¨ä¸­çš„ç¤¾ä¼šå›°å¢ƒè¡¨ç°ï¼Œå‘ç°å››ç§ä¸åŒçš„è¡Œä¸ºæ¨¡å¼ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæ¨ç†èƒ½åŠ›å¼ºçš„LLMåœ¨åˆä½œä¸Šè¡¨ç°ä¸ä½³ï¼Œè€Œä¸€äº›ä¼ ç»ŸLLMåˆ™èƒ½æŒç»­å®ç°é«˜æ°´å¹³çš„åˆä½œã€‚è¿™ä¸€å‘ç°ä¸ºåœ¨éœ€è¦æŒç»­åä½œçš„ç¯å¢ƒä¸­éƒ¨ç½²LLMä»£ç†æä¾›äº†é‡è¦çš„è§è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ™ºèƒ½ä½“LLMç³»ç»Ÿä¸­ï¼Œä»£ç†åœ¨æ¿€åŠ±åˆä½œä¸æƒ©ç½šèƒŒå›æ—¶çš„èµ„æºæŠ•èµ„å†³ç­–é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆä¿ƒè¿›LLMä¹‹é—´çš„åˆä½œï¼Œå¯¼è‡´åˆä½œæ°´å¹³ä¸ç¨³å®šã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡é€‚åº”è¡Œä¸ºç»æµå­¦ä¸­çš„å…¬å…±ç‰©å“åšå¼ˆï¼Œè§‚å¯Ÿä¸åŒLLMåœ¨é‡å¤äº’åŠ¨ä¸­çš„è¡Œä¸ºæ¨¡å¼ï¼Œåˆ†æå…¶åœ¨ç¤¾ä¼šå›°å¢ƒä¸­çš„è¡¨ç°ã€‚è¿™æ ·çš„è®¾è®¡ä½¿å¾—ç ”ç©¶è€…èƒ½å¤Ÿæ·±å…¥ç†è§£LLMçš„åˆä½œæœºåˆ¶ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å…¬å…±ç‰©å“åšå¼ˆçš„è®¾ç½®ã€LLMçš„è¡Œä¸ºè§‚å¯Ÿä¸è®°å½•ã€ä»¥åŠå¯¹ä¸åŒæ¨¡å‹çš„æ¯”è¾ƒåˆ†æã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬ä»£ç†å†³ç­–æ¨¡å—ã€åˆä½œè¡Œä¸ºè¯„ä¼°æ¨¡å—å’Œç»“æœåˆ†ææ¨¡å—ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå°†å…¬å…±ç‰©å“åšå¼ˆå¼•å…¥LLMçš„åˆä½œç ”ç©¶ï¼Œæ­ç¤ºäº†æ¨ç†èƒ½åŠ›ä¸åˆä½œèƒ½åŠ›ä¹‹é—´çš„åå‘å…³ç³»ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„å‡è®¾å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œè®¾ç½®äº†ä¸åŒçš„èµ„æºæŠ•èµ„å‚æ•°å’Œæƒ©ç½šæœºåˆ¶ï¼Œä½¿ç”¨äº†å¤šç§LLMæ¨¡å‹è¿›è¡Œå¯¹æ¯”ï¼Œç¡®ä¿äº†å®éªŒç»“æœçš„å¯é æ€§ä¸æœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ¨ç†èƒ½åŠ›å¼ºçš„LLMåœ¨åˆä½œæ–¹é¢çš„è¡¨ç°æ˜¾è‘—ä½äºä¸€äº›ä¼ ç»ŸLLMï¼Œåè€…èƒ½å¤ŸæŒç»­ç»´æŒé«˜æ°´å¹³çš„åˆä½œã€‚è¿™ä¸€å‘ç°æŒ‘æˆ˜äº†å½“å‰å¯¹LLMæ¨ç†èƒ½åŠ›æå‡çš„æ™®éçœ‹æ³•ï¼Œå¼ºè°ƒäº†åˆä½œæœºåˆ¶çš„é‡è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤šæ™ºèƒ½ä½“ç³»ç»Ÿã€è‡ªåŠ¨åŒ–åä½œå¹³å°å’Œç¤¾ä¼šç½‘ç»œåˆ†æç­‰ã€‚é€šè¿‡ç†è§£LLMåœ¨åˆä½œä¸­çš„è¡¨ç°ï¼Œå¯ä»¥ä¸ºè®¾è®¡æ›´é«˜æ•ˆçš„æ™ºèƒ½ä½“ç³»ç»Ÿæä¾›ç†è®ºæ”¯æŒï¼Œä¿ƒè¿›å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å®‰å…¨ä¸æœ‰æ•ˆæ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As large language models (LLMs) are increasingly deployed as autonomous agents, understanding their cooperation and social mechanisms is becoming increasingly important. In particular, how LLMs balance self-interest and collective well-being is a critical challenge for ensuring alignment, robustness, and safe deployment. In this paper, we examine the challenge of costly sanctioning in multi-agent LLM systems, where an agent must decide whether to invest its own resources to incentivize cooperation or penalize defection. To study this, we adapt a public goods game with institutional choice from behavioral economics, allowing us to observe how different LLMs navigate social dilemmas over repeated interactions. Our analysis reveals four distinct behavioral patterns among models: some consistently establish and sustain high levels of cooperation, others fluctuate between engagement and disengagement, some gradually decline in cooperative behavior over time, and others rigidly follow fixed strategies regardless of outcomes. Surprisingly, we find that reasoning LLMs, such as the o1 series, struggle significantly with cooperation, whereas some traditional LLMs consistently achieve high levels of cooperation. These findings suggest that the current approach to improving LLMs, which focuses on enhancing their reasoning capabilities, does not necessarily lead to cooperation, providing valuable insights for deploying LLM agents in environments that require sustained collaboration. Our code is available at https://github.com/davidguzmanp/SanctSim

