---
layout: default
title: Beyond the Strongest LLM: Multi-Turn Multi-Agent Orchestration vs. Single LLMs on Benchmarks
---

# Beyond the Strongest LLM: Multi-Turn Multi-Agent Orchestration vs. Single LLMs on Benchmarks

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.23537" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.23537v2</a>
  <a href="https://arxiv.org/pdf/2509.23537.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.23537v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.23537v2', 'Beyond the Strongest LLM: Multi-Turn Multi-Agent Orchestration vs. Single LLMs on Benchmarks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Aaron Xuxiang Tian, Ruofan Zhang, Jiayao Tang, Young Min Cho, Xueqian Li, Qiang Yi, Ji Wang, Zhunping Zhang, Danrui Qi, Zekun Li, Xingyu Xiang, Sharath Chandra Guntuku, Lyle Ungar, Tianyu Shi, Chi Wang

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-28 (æ›´æ–°: 2025-10-01)

**å¤‡æ³¨**: 9 pages, 3 tables, 1 figure

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**å¤šæ™ºèƒ½ä½“ååŒè¶…è¶Šæœ€å¼ºLLMï¼šå¤šè½®äº¤äº’åœ¨åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºå•ä¸€å¤§æ¨¡å‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ` `å¤§å‹è¯­è¨€æ¨¡å‹` `ååŒæ¨ç†` `å…±è¯†æœºåˆ¶` `åŸºå‡†æµ‹è¯•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­å­˜åœ¨å±€é™æ€§ï¼Œéš¾ä»¥å……åˆ†åˆ©ç”¨æ¨¡å‹è‡ªèº«èƒ½åŠ›ã€‚
2. è®ºæ–‡æå‡ºå¤šæ™ºèƒ½ä½“ååŒæ¡†æ¶ï¼Œé€šè¿‡å¤šè½®äº¤äº’å’ŒæŠ•ç¥¨æœºåˆ¶ï¼Œæå‡é—®é¢˜è§£å†³èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ååŒæ¡†æ¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†æœ€å¼ºçš„å•ä¸€å¤§æ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç ”ç©¶äº†å¤šè½®å¤šæ™ºèƒ½ä½“ååŒï¼Œå…¶ä¸­å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ™ºèƒ½ä½“é€šè¿‡è¿­ä»£æå‡ºç­”æ¡ˆæˆ–æŠ•ç¥¨è¿›è¡Œå¤šè½®äº¤äº’ï¼Œç›´åˆ°è¾¾æˆå…±è¯†ã€‚ä½œè€…ä½¿ç”¨å››ä¸ªLLMï¼ˆGemini 2.5 Proã€GPT-5ã€Grok 4 å’Œ Claude Sonnet 4ï¼‰åœ¨ GPQA-Diamondã€IFEval å’Œ MuSR ä¸Šè¿›è¡Œäº†ä¸¤é¡¹å®éªŒï¼šï¼ˆiï¼‰å°†ååŒä¸å•LLMåŸºçº¿è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼›ï¼ˆiiï¼‰åœ¨ GPQA-Diamond ä¸Šè¿›è¡Œæ¶ˆèç ”ç©¶ï¼Œæ”¹å˜æ™ºèƒ½ä½“æ˜¯å¦èƒ½çœ‹åˆ°ç­”æ¡ˆçš„ä½œè€…ä»¥åŠæ˜¯å¦èƒ½è§‚å¯Ÿåˆ°æ­£åœ¨è¿›è¡Œçš„æŠ•ç¥¨ã€‚ååŒåŒ¹é…æˆ–è¶…è¿‡äº†æœ€å¼ºçš„å•ä¸€æ¨¡å‹ï¼Œå¹¶ä¸”å§‹ç»ˆä¼˜äºå…¶ä»–æ¨¡å‹ã€‚å¯¹æœ€ä½³å¯å®ç°ååŒæ€§èƒ½çš„åˆ†æè¡¨æ˜ï¼Œå­˜åœ¨è¿›ä¸€æ­¥æå‡çš„æ½œåŠ›ã€‚æ¶ˆèç ”ç©¶è¡¨æ˜ï¼Œæ­ç¤ºä½œè€…èº«ä»½ä¼šå¢åŠ è‡ªæˆ‘æŠ•ç¥¨å’Œå¹¶åˆ—ï¼Œè€Œæ˜¾ç¤ºæ­£åœ¨è¿›è¡Œçš„æŠ•ç¥¨ä¼šæ”¾å¤§ç¾Šç¾¤æ•ˆåº”ï¼Œè¿™ä¼šåŠ é€Ÿæ”¶æ•›ï¼Œä½†æœ‰æ—¶ä¼šå¯¼è‡´è¿‡æ—©è¾¾æˆå…±è¯†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤æ‚é—®é¢˜æ—¶ï¼Œå¾€å¾€å—é™äºè‡ªèº«çš„çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œéš¾ä»¥è¾¾åˆ°æœ€ä¼˜æ€§èƒ½ã€‚å•ä¸€å¤§æ¨¡å‹å®¹æ˜“å‡ºç°å¹»è§‰ã€åè§ç­‰é—®é¢˜ï¼Œä¸”éš¾ä»¥æœ‰æ•ˆåˆ©ç”¨å¤šä¸ªæ¨¡å‹çš„ä¼˜åŠ¿ã€‚å› æ­¤ï¼Œå¦‚ä½•æœ‰æ•ˆåœ°æ•´åˆå¤šä¸ªLLMçš„èƒ½åŠ›ï¼Œæå‡é—®é¢˜è§£å†³çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§æ˜¯ä¸€ä¸ªé‡è¦çš„ç ”ç©¶é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤šæ™ºèƒ½ä½“ååŒæœºåˆ¶ï¼Œé€šè¿‡è®©å¤šä¸ªLLMæ™ºèƒ½ä½“è¿›è¡Œå¤šè½®äº¤äº’ï¼Œå…±åŒè§£å†³é—®é¢˜ã€‚æ¯ä¸ªæ™ºèƒ½ä½“ç‹¬ç«‹ç”Ÿæˆç­”æ¡ˆæˆ–è¿›è¡ŒæŠ•ç¥¨ï¼Œå¹¶é€šè¿‡è¿­ä»£è¿‡ç¨‹é€æ­¥è¾¾æˆå…±è¯†ã€‚è¿™ç§æ–¹æ³•å¯ä»¥æœ‰æ•ˆåˆ©ç”¨ä¸åŒæ¨¡å‹çš„ä¼˜åŠ¿ï¼Œå‡å°‘å•ä¸€æ¨¡å‹çš„å±€é™æ€§ï¼Œå¹¶æé«˜é—®é¢˜è§£å†³çš„å¯é æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥ç ”ç©¶çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) åˆå§‹åŒ–ï¼šä¸ºæ¯ä¸ªLLMåˆ›å»ºä¸€ä¸ªæ™ºèƒ½ä½“ï¼›2) ç­”æ¡ˆç”Ÿæˆï¼šæ¯ä¸ªæ™ºèƒ½ä½“ç‹¬ç«‹ç”Ÿæˆç­”æ¡ˆï¼›3) æŠ•ç¥¨ï¼šæ¯ä¸ªæ™ºèƒ½ä½“å¯¹æ‰€æœ‰ç­”æ¡ˆè¿›è¡ŒæŠ•ç¥¨ï¼›4) å…±è¯†è¾¾æˆï¼šæ ¹æ®æŠ•ç¥¨ç»“æœï¼Œåˆ¤æ–­æ˜¯å¦è¾¾æˆå…±è¯†ï¼›5) è¿­ä»£ï¼šå¦‚æœæœªè¾¾æˆå…±è¯†ï¼Œåˆ™é‡å¤ç­”æ¡ˆç”Ÿæˆå’ŒæŠ•ç¥¨è¿‡ç¨‹ï¼Œç›´åˆ°è¾¾æˆå…±è¯†æˆ–è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ã€‚åœ¨æ¶ˆèå®éªŒä¸­ï¼Œä½œè€…è¿˜ç ”ç©¶äº†æ™ºèƒ½ä½“æ˜¯å¦èƒ½çœ‹åˆ°ç­”æ¡ˆçš„ä½œè€…ä»¥åŠæ˜¯å¦èƒ½è§‚å¯Ÿåˆ°æ­£åœ¨è¿›è¡Œçš„æŠ•ç¥¨å¯¹ç»“æœçš„å½±å“ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥ç ”ç©¶çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†å¤šè½®å¤šæ™ºèƒ½ä½“ååŒæ¡†æ¶ï¼Œå¹¶å°†å…¶åº”ç”¨äºè§£å†³å¤æ‚é—®é¢˜ã€‚ä¸ä¼ ç»Ÿçš„å•ä¸€å¤§æ¨¡å‹æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨å¤šä¸ªæ¨¡å‹çš„ä¼˜åŠ¿ï¼Œæé«˜é—®é¢˜è§£å†³çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜é€šè¿‡æ¶ˆèå®éªŒï¼Œæ·±å…¥åˆ†æäº†ä¸åŒå› ç´ å¯¹ååŒæ•ˆæœçš„å½±å“ï¼Œä¸ºè¿›ä¸€æ­¥ä¼˜åŒ–ååŒç­–ç•¥æä¾›äº†æŒ‡å¯¼ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œä½œè€…ä½¿ç”¨äº†Gemini 2.5 Proã€GPT-5ã€Grok 4 å’Œ Claude Sonnet 4 å››ä¸ªLLMä½œä¸ºæ™ºèƒ½ä½“ã€‚æŠ•ç¥¨æœºåˆ¶é‡‡ç”¨ç®€å•çš„å¤šæ•°æŠ•ç¥¨è§„åˆ™ã€‚è¿­ä»£æ¬¡æ•°è®¾ç½®ä¸ºä¸€ä¸ªä¸Šé™ï¼Œä»¥é˜²æ­¢æ— é™å¾ªç¯ã€‚æ¶ˆèå®éªŒä¸­ï¼Œä½œè€…æ§åˆ¶äº†æ™ºèƒ½ä½“æ˜¯å¦èƒ½çœ‹åˆ°ç­”æ¡ˆçš„ä½œè€…ä»¥åŠæ˜¯å¦èƒ½è§‚å¯Ÿåˆ°æ­£åœ¨è¿›è¡Œçš„æŠ•ç¥¨ï¼Œä»¥ç ”ç©¶è¿™äº›å› ç´ å¯¹ååŒæ•ˆæœçš„å½±å“ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå¤šæ™ºèƒ½ä½“ååŒæ¡†æ¶åœ¨ GPQA-Diamondã€IFEval å’Œ MuSR ä¸‰ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒåŒ¹é…æˆ–è¶…è¿‡äº†æœ€å¼ºçš„å•ä¸€æ¨¡å‹ï¼Œå¹¶ä¸”å§‹ç»ˆä¼˜äºå…¶ä»–æ¨¡å‹ã€‚æ¶ˆèå®éªŒè¡¨æ˜ï¼Œæ­ç¤ºä½œè€…èº«ä»½ä¼šå¢åŠ è‡ªæˆ‘æŠ•ç¥¨å’Œå¹¶åˆ—ï¼Œè€Œæ˜¾ç¤ºæ­£åœ¨è¿›è¡Œçš„æŠ•ç¥¨ä¼šæ”¾å¤§ç¾Šç¾¤æ•ˆåº”ï¼Œè¿™ä¼šåŠ é€Ÿæ”¶æ•›ï¼Œä½†æœ‰æ—¶ä¼šå¯¼è‡´è¿‡æ—©è¾¾æˆå…±è¯†ã€‚æœ€ä½³å¯å®ç°ååŒæ€§èƒ½çš„åˆ†æè¡¨æ˜ï¼Œå­˜åœ¨è¿›ä¸€æ­¥æå‡çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºéœ€è¦é«˜å‡†ç¡®æ€§å’Œå¯é æ€§çš„å¤æ‚é—®é¢˜è§£å†³åœºæ™¯ï¼Œä¾‹å¦‚åŒ»ç–—è¯Šæ–­ã€é‡‘èé£é™©è¯„ä¼°ã€æ³•å¾‹å’¨è¯¢ç­‰ã€‚é€šè¿‡å¤šæ™ºèƒ½ä½“ååŒï¼Œå¯ä»¥æœ‰æ•ˆå‡å°‘å•ä¸€æ¨¡å‹çš„é”™è¯¯ï¼Œæé«˜å†³ç­–çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åº”ç”¨äºæ›´å¹¿æ³›çš„é¢†åŸŸï¼Œå¹¶ä¸å…¶ä»–æŠ€æœ¯ï¼ˆå¦‚çŸ¥è¯†å›¾è°±ã€å¼ºåŒ–å­¦ä¹ ï¼‰ç›¸ç»“åˆï¼Œè¿›ä¸€æ­¥æå‡é—®é¢˜è§£å†³èƒ½åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We study multi-turn multi-agent orchestration, where multiple large language model (LLM) agents interact over multiple turns by iteratively proposing answers or casting votes until reaching consensus. Using four LLMs (Gemini 2.5 Pro, GPT-5, Grok 4, and Claude Sonnet 4) on GPQA-Diamond, IFEval, and MuSR, we conduct two experiments: (i) benchmarking orchestration against single-LLM baselines; and (ii) ablations on GPQA-Diamond that vary whether agents see who authored answers and whether they can observe ongoing votes. Orchestration matches or exceeds the strongest single model and consistently outperforms the others. Analysis of best-achievable orchestration performance shows potential for further gains. The ablations show that revealing authorship increases self-voting and ties, and that showing ongoing votes amplifies herding, which speeds convergence but can sometimes yield premature consensus.

