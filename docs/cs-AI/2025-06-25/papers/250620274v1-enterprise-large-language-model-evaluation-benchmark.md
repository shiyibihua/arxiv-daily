---
layout: default
title: Enterprise Large Language Model Evaluation Benchmark
---

# Enterprise Large Language Model Evaluation Benchmark

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.20274" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.20274v1</a>
  <a href="https://arxiv.org/pdf/2506.20274.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.20274v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.20274v1', 'Enterprise Large Language Model Evaluation Benchmark')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Liya Wang, David Yi, Damien Jose, John Passarelli, James Gao, Jordan Leventis, Kang Li

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-25

**å¤‡æ³¨**: Submitted to MLNLP 2025 at https://csity2025.org/mlnlp/index

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¼ä¸šå¤§å‹è¯­è¨€æ¨¡å‹è¯„ä¼°åŸºå‡†ä»¥è§£å†³ç°æœ‰è¯„ä¼°ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `ä¼ä¸šè¯„ä¼°` `å¸ƒé²å§†åˆ†ç±»æ³•` `æ•°æ®å¢å¼º` `æ¨¡å‹ä¼˜åŒ–` `æ€§èƒ½è¯„ä¼°` `å¼€æºæ¨¡å‹` `ä»»åŠ¡å¤æ‚æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è¯„ä¼°åŸºå‡†æœªèƒ½å……åˆ†è€ƒè™‘ä¼ä¸šç‰¹å®šä»»åŠ¡çš„å¤æ‚æ€§ï¼Œå¯¼è‡´å¯¹LLMèƒ½åŠ›çš„è¯„ä¼°ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåŸºäºå¸ƒé²å§†åˆ†ç±»æ³•çš„14ä»»åŠ¡æ¡†æ¶ï¼Œç»“åˆLLMä½œä¸ºæ ‡æ³¨è€…å’Œè¯„åˆ¤è€…çš„ç­–ç•¥ï¼Œä»¥å…¨é¢è¯„ä¼°LLMåœ¨ä¼ä¸šä¸­çš„åº”ç”¨èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œå¼€æºæ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨åˆ¤æ–­ä»»åŠ¡ä¸­å­˜åœ¨æ˜æ˜¾çŸ­æ¿ï¼Œæ­ç¤ºäº†ä¼ä¸šåº”ç”¨ä¸­çš„æ€§èƒ½å·®è·ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æå‡AIå·¥å…·ç”Ÿäº§åŠ›æ–¹é¢å±•ç°å‡ºæ½œåŠ›ï¼Œä½†ç°æœ‰åŸºå‡†å¦‚å¤§è§„æ¨¡å¤šä»»åŠ¡è¯­è¨€ç†è§£ï¼ˆMMLUï¼‰æœªèƒ½å……åˆ†è¯„ä¼°ä¼ä¸šç‰¹å®šä»»åŠ¡çš„å¤æ‚æ€§ã€‚æœ¬æ–‡æå‡ºä¸€ä¸ªåŸºäºå¸ƒé²å§†åˆ†ç±»æ³•çš„14ä»»åŠ¡æ¡†æ¶ï¼Œä»¥å…¨é¢è¯„ä¼°LLMåœ¨ä¼ä¸šç¯å¢ƒä¸­çš„èƒ½åŠ›ã€‚ä¸ºåº”å¯¹å™ªå£°æ•°æ®å’Œé«˜æ˜‚æ ‡æ³¨æˆæœ¬çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå¯æ‰©å±•çš„æµç¨‹ï¼Œç»“åˆäº†LLMä½œä¸ºæ ‡æ³¨è€…ã€LLMä½œä¸ºè¯„åˆ¤è€…å’Œçº æ­£æ€§æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆCRAGï¼‰ï¼Œç­–åˆ’äº†ä¸€ä¸ªåŒ…å«9700ä¸ªæ ·æœ¬çš„ç¨³å¥åŸºå‡†ã€‚å¯¹å…­ä¸ªé¢†å…ˆæ¨¡å‹çš„è¯„ä¼°æ˜¾ç¤ºï¼Œå¼€æºæ¨¡å‹å¦‚DeepSeek R1åœ¨æ¨ç†ä»»åŠ¡ä¸­ä¸ä¸“æœ‰æ¨¡å‹ç›¸å½“ï¼Œä½†åœ¨åŸºäºåˆ¤æ–­çš„åœºæ™¯ä¸­è¡¨ç°ä¸ä½³ï¼Œå¯èƒ½ç”±äºè¿‡åº¦æ€è€ƒã€‚æˆ‘ä»¬çš„åŸºå‡†æ­ç¤ºäº†å…³é”®çš„ä¼ä¸šæ€§èƒ½å·®è·ï¼Œå¹¶æä¾›äº†æ¨¡å‹ä¼˜åŒ–çš„å¯è¡Œè§è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è¯„ä¼°åŸºå‡†æ— æ³•æœ‰æ•ˆè¯„ä¼°ä¼ä¸šç‰¹å®šä»»åŠ¡å¤æ‚æ€§çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†å™ªå£°æ•°æ®å’Œæ ‡æ³¨æˆæœ¬æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºä¸€ä¸ª14ä»»åŠ¡çš„è¯„ä¼°æ¡†æ¶ï¼ŒåŸºäºå¸ƒé²å§†åˆ†ç±»æ³•ï¼Œç»“åˆLLMä½œä¸ºæ ‡æ³¨è€…å’Œè¯„åˆ¤è€…çš„ç­–ç•¥ï¼Œä»¥å®ç°å…¨é¢çš„èƒ½åŠ›è¯„ä¼°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šLLMä½œä¸ºæ ‡æ³¨è€…ç”Ÿæˆæ ‡ç­¾ï¼ŒLLMä½œä¸ºè¯„åˆ¤è€…è¿›è¡Œè¯„ä¼°ï¼Œä»¥åŠçº æ­£æ€§æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆCRAGï¼‰ç”¨äºæ•°æ®ä¿®æ­£ï¼Œå½¢æˆä¸€ä¸ª9700æ ·æœ¬çš„åŸºå‡†æ•°æ®é›†ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºç»“åˆäº†å¤šç§LLMè§’è‰²ï¼ˆæ ‡æ³¨è€…å’Œè¯„åˆ¤è€…ï¼‰ï¼Œå¹¶é€šè¿‡CRAGæŠ€æœ¯æå‡æ•°æ®è´¨é‡ï¼Œä»è€Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•çš„å±€é™æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†é€‚åº”æ€§æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ï¼Œç½‘ç»œç»“æ„ä¸Šåˆ™ç»“åˆäº†å¤šå±‚æ¬¡çš„è¯„ä¼°æœºåˆ¶ï¼Œç¡®ä¿è¯„ä¼°çš„å…¨é¢æ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¼€æºæ¨¡å‹DeepSeek R1åœ¨æ¨ç†ä»»åŠ¡ä¸­ä¸ä¸“æœ‰æ¨¡å‹è¡¨ç°ç›¸å½“ï¼Œä½†åœ¨åˆ¤æ–­ä»»åŠ¡ä¸­å­˜åœ¨æ˜æ˜¾åŠ£åŠ¿ï¼Œæ­ç¤ºäº†ä¼ä¸šåº”ç”¨ä¸­çš„å…³é”®æ€§èƒ½å·®è·ã€‚è¿™ä¸€å‘ç°ä¸ºæ¨¡å‹ä¼˜åŒ–æä¾›äº†é‡è¦çš„æ–¹å‘ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶ä¸ºä¼ä¸šæä¾›äº†ä¸€ç§é‡èº«å®šåˆ¶çš„è¯„ä¼°æ¡†æ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«å’Œä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚æœªæ¥ï¼Œéšç€ä¼ä¸šå¯¹AIå·¥å…·çš„ä¾èµ–åŠ æ·±ï¼Œè¯¥æ¡†æ¶å°†æœ‰åŠ©äºæ¨åŠ¨LLMçš„å®é™…åº”ç”¨å’Œéƒ¨ç½²ï¼Œæå‡å·¥ä½œæ•ˆç‡å’Œå†³ç­–è´¨é‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) ) have demonstrated promise in boosting productivity across AI-powered tools, yet existing benchmarks like Massive Multitask Language Understanding (MMLU) inadequately assess enterprise-specific task complexities. We propose a 14-task framework grounded in Bloom's Taxonomy to holistically evaluate LLM capabilities in enterprise contexts. To address challenges of noisy data and costly annotation, we develop a scalable pipeline combining LLM-as-a-Labeler, LLM-as-a-Judge, and corrective retrieval-augmented generation (CRAG), curating a robust 9,700-sample benchmark. Evaluation of six leading models shows open-source contenders like DeepSeek R1 rival proprietary models in reasoning tasks but lag in judgment-based scenarios, likely due to overthinking. Our benchmark reveals critical enterprise performance gaps and offers actionable insights for model optimization. This work provides enterprises a blueprint for tailored evaluations and advances practical LLM deployment.

