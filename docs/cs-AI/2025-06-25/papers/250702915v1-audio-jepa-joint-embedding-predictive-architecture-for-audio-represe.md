---
layout: default
title: Audio-JEPA: Joint-Embedding Predictive Architecture for Audio Representation Learning
---

# Audio-JEPA: Joint-Embedding Predictive Architecture for Audio Representation Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2507.02915" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2507.02915v1</a>
  <a href="https://arxiv.org/pdf/2507.02915.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2507.02915v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2507.02915v1', 'Audio-JEPA: Joint-Embedding Predictive Architecture for Audio Representation Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ludovic Tuncay, Etienne LabbÃ©, Emmanouil Benetos, Thomas Pellegrini

**åˆ†ç±»**: cs.SD, cs.AI, cs.LG, eess.AS, eess.SP

**å‘å¸ƒæ—¥æœŸ**: 2025-06-25

**æœŸåˆŠ**: ICME 2025, Jun 2025, Nantes, France

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAudio-JEPAä»¥è§£å†³éŸ³é¢‘è¡¨ç¤ºå­¦ä¹ é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `éŸ³é¢‘è¡¨ç¤ºå­¦ä¹ ` `è‡ªç›‘ç£å­¦ä¹ ` `è”åˆåµŒå…¥` `è§†è§‰å˜æ¢å™¨` `å£°è°±å›¾`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰éŸ³é¢‘è¡¨ç¤ºå­¦ä¹ æ–¹æ³•åœ¨æ•°æ®åˆ©ç”¨æ•ˆç‡å’Œæ¨¡å‹æ€§èƒ½ä¸Šå­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨ç¼ºä¹æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹ã€‚
2. Audio-JEPAé€šè¿‡é¢„æµ‹æ©è”½çš„å£°è°±å›¾å—çš„æ½œåœ¨è¡¨ç¤ºï¼Œé‡‡ç”¨ç®€å•çš„è§†è§‰å˜æ¢å™¨æ¶æ„ï¼Œé¿å…äº†å¯¹åŸå§‹éŸ³é¢‘çš„é‡å»ºã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒAudio-JEPAåœ¨å¤šä¸ªéŸ³é¢‘ä»»åŠ¡ä¸Šè¡¨ç°å‡ºä¸å…ˆè¿›æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ï¼Œä¸”è®­ç»ƒæ•°æ®éœ€æ±‚æ˜¾è‘—é™ä½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºäºè”åˆåµŒå…¥é¢„æµ‹æ¶æ„ï¼ˆJEPAï¼‰èŒƒå¼ï¼Œæœ¬æ–‡æå‡ºäº†ä¸“é—¨é’ˆå¯¹éŸ³é¢‘æ•°æ®çš„Audio-JEPAã€‚è¯¥æ–¹æ³•åˆ©ç”¨ç®€å•çš„è§†è§‰å˜æ¢å™¨éª¨å¹²ç½‘ç»œï¼Œé¢„æµ‹æ©è”½çš„å£°è°±å›¾å—çš„æ½œåœ¨è¡¨ç¤ºï¼Œè€Œä¸æ˜¯é‡å»ºåŸå§‹éŸ³é¢‘ã€‚æˆ‘ä»¬åœ¨æœªæ ‡è®°çš„AudioSetç‰‡æ®µä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶åœ¨X-ARESå¥—ä»¶ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œæ¶µç›–è¯­éŸ³ã€éŸ³ä¹å’Œç¯å¢ƒå£°éŸ³ä»»åŠ¡ã€‚å°½ç®¡å®ç°æ˜¯å¯¹åŸå§‹æ¨¡å‹çš„ç›´æ¥ç¿»è¯‘ï¼Œç»“æœä»æ˜¾ç¤ºå‡ºä¸wav2vec 2.0å’Œdata2vecç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶ä½¿ç”¨çš„è®­ç»ƒæ•°æ®ä¸åˆ°å…¶äº”åˆ†ä¹‹ä¸€ï¼Œä¸”æ— éœ€è¶…å‚æ•°è°ƒä¼˜ã€‚æ‰€æœ‰ä»£ç å’Œé¢„è®­ç»ƒæ£€æŸ¥ç‚¹å°†å‘å¸ƒåœ¨GitHubä¸Šã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³éŸ³é¢‘è¡¨ç¤ºå­¦ä¹ ä¸­çš„æ•°æ®åˆ©ç”¨æ•ˆç‡ä½å’Œæ¨¡å‹æ€§èƒ½ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¦‚wav2vec 2.0å’Œdata2vecåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­éœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®ï¼Œä¸”è¶…å‚æ•°è°ƒä¼˜å¤æ‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šAudio-JEPAçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è”åˆåµŒå…¥é¢„æµ‹æ¶æ„ï¼Œé¢„æµ‹æ©è”½çš„å£°è°±å›¾å—çš„æ½œåœ¨è¡¨ç¤ºï¼Œè€Œéç›´æ¥é‡å»ºåŸå§‹éŸ³é¢‘ä¿¡å·ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨æœªæ ‡è®°æ•°æ®ï¼Œæå‡æ¨¡å‹çš„å­¦ä¹ æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šAudio-JEPAçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸€ä¸ªè§†è§‰å˜æ¢å™¨éª¨å¹²ç½‘ç»œï¼Œè´Ÿè´£å¤„ç†å’Œé¢„æµ‹å£°è°±å›¾çš„æ©è”½å—ã€‚é¢„è®­ç»ƒé˜¶æ®µä½¿ç”¨éšæœºæ©è”½çš„melå£°è°±å›¾è¿›è¡Œè®­ç»ƒï¼Œéšååœ¨å¤šä¸ªéŸ³é¢‘ä»»åŠ¡ä¸Šè¿›è¡Œè¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå°†JEPAæ¡†æ¶æˆåŠŸåº”ç”¨äºéŸ³é¢‘æ•°æ®ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„éŸ³é¢‘è¡¨ç¤ºå­¦ä¹ æ–¹æ³•ï¼Œæ˜¾è‘—é™ä½äº†å¯¹æ ‡æ³¨æ•°æ®çš„ä¾èµ–ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒAudio-JEPAä½¿ç”¨äº†ç®€å•çš„è§†è§‰å˜æ¢å™¨æ¶æ„ï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é‡‡ç”¨äº†éšæœºæ©è”½ç­–ç•¥ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸ºé¢„æµ‹æ©è”½å—çš„æ½œåœ¨è¡¨ç¤ºï¼Œç¡®ä¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆå­¦ä¹ éŸ³é¢‘ç‰¹å¾ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å®éªŒä¸­ï¼ŒAudio-JEPAåœ¨X-ARESå¥—ä»¶ä¸Šè¡¨ç°å‡ºä¸wav2vec 2.0å’Œdata2vecç›¸å½“çš„æ€§èƒ½ï¼Œä¸”è®­ç»ƒæ•°æ®éœ€æ±‚ä¸åˆ°å…¶äº”åˆ†ä¹‹ä¸€ï¼Œä¸”æ— éœ€è¿›è¡Œè¶…å‚æ•°è°ƒä¼˜ï¼Œè¿™è¡¨æ˜å…¶åœ¨éŸ³é¢‘è¡¨ç¤ºå­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§å’Œé«˜æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Audio-JEPAçš„æ½œåœ¨åº”ç”¨åœºæ™¯åŒ…æ‹¬è¯­éŸ³è¯†åˆ«ã€éŸ³ä¹æ¨èå’Œç¯å¢ƒå£°éŸ³åˆ†ç±»ç­‰é¢†åŸŸã€‚å…¶é«˜æ•ˆçš„éŸ³é¢‘è¡¨ç¤ºå­¦ä¹ èƒ½åŠ›èƒ½å¤Ÿä¸ºç›¸å…³åº”ç”¨æä¾›æ›´å¼ºçš„æ”¯æŒï¼Œå°¤å…¶æ˜¯åœ¨æ ‡æ³¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Building on the Joint-Embedding Predictive Architecture (JEPA) paradigm, a recent self-supervised learning framework that predicts latent representations of masked regions in high-level feature spaces, we propose Audio-JEPA (Audio Joint-Embedding Predictive Architecture), tailored specifically for audio data. Audio-JEPA uses a simple Vision Transformer backbone to predict latent representations of masked spectrogram patches rather than reconstructing raw audio. We pre-train on unlabeled AudioSet clips (10s, 32kHz) with random patch masking on mel-spectrograms. We evaluate on the X-ARES suite covering speech, music, and environmental sound tasks. Although our implementation is a straightforward translation of the original model to audio, the results still show comparable performance to wav2vec 2.0 and data2vec while using less than one-fifth of their training data and with no hyper-parameter tuning. All code and pretrained checkpoints will be released on GitHub.

