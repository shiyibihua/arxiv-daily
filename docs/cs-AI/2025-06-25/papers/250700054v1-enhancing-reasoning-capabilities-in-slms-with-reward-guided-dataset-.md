---
layout: default
title: Enhancing Reasoning Capabilities in SLMs with Reward Guided Dataset Distillation
---

# Enhancing Reasoning Capabilities in SLMs with Reward Guided Dataset Distillation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2507.00054" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2507.00054v1</a>
  <a href="https://arxiv.org/pdf/2507.00054.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2507.00054v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2507.00054v1', 'Enhancing Reasoning Capabilities in SLMs with Reward Guided Dataset Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shreyansh Padarha

**åˆ†ç±»**: cs.AI, cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-25

**å¤‡æ³¨**: 17 Pages, 7 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAdvDistillä»¥è§£å†³å°å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å°å‹è¯­è¨€æ¨¡å‹` `çŸ¥è¯†è’¸é¦` `æ¨ç†èƒ½åŠ›` `å¥–åŠ±æœºåˆ¶` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„çŸ¥è¯†è’¸é¦æ–¹æ³•å¾€å¾€ä½¿å­¦ç”Ÿæ¨¡å‹ä»…å¤åˆ¶æ•™å¸ˆæ¨¡å‹çš„å“åº”ï¼Œé™åˆ¶äº†å…¶åœ¨æ¨ç†ä»»åŠ¡ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚
2. æœ¬æ–‡æå‡ºçš„AdvDistillæ¡†æ¶åˆ©ç”¨æ•™å¸ˆæ¨¡å‹çš„å¤šé‡å“åº”ï¼Œå¹¶é€šè¿‡è§„åˆ™éªŒè¯å™¨ä¸ºæ¯ä¸ªå“åº”åˆ†é…å¥–åŠ±ï¼Œå¢å¼ºäº†å­¦ç”Ÿæ¨¡å‹çš„å­¦ä¹ æ•ˆæœã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAdvDistillåœ¨æ•°å­¦å’Œå¤æ‚æ¨ç†ä»»åŠ¡ä¸Šæ˜¾è‘—æå‡äº†å­¦ç”Ÿæ¨¡å‹çš„æ€§èƒ½ï¼ŒéªŒè¯äº†å¥–åŠ±æœºåˆ¶çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¯¹å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å‹ç¼©å¹¶è½¬åŒ–ä¸ºæ›´é«˜æ•ˆçš„å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰çš„éœ€æ±‚å¢åŠ ï¼ŒçŸ¥è¯†è’¸é¦ï¼ˆKDï¼‰æŠ€æœ¯çš„æ”¹è¿›æ˜¾å¾—å°¤ä¸ºé‡è¦ã€‚ç°æœ‰çš„è’¸é¦æ–¹æ³•é€šå¸¸ä»…ä½¿å­¦ç”Ÿæ¨¡å‹å¤åˆ¶æ•™å¸ˆæ¨¡å‹çš„å“åº”ï¼Œé™åˆ¶äº†å…¶æ³›åŒ–èƒ½åŠ›ï¼Œå°¤å…¶åœ¨æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å¾—å°¤ä¸ºæ˜æ˜¾ä¸”è®¡ç®—æˆæœ¬é«˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å¥–åŠ±å¼•å¯¼çš„æ•°æ®è’¸é¦æ¡†æ¶AdvDistillï¼Œé€šè¿‡å¯¹æ¯ä¸ªæç¤ºç”Ÿæˆå¤šä¸ªæ•™å¸ˆå“åº”ï¼Œå¹¶åŸºäºè§„åˆ™éªŒè¯å™¨åˆ†é…å¥–åŠ±ã€‚è¿™äº›ä¸åŒä¸”å‘ˆæ­£æ€åˆ†å¸ƒçš„å¥–åŠ±åœ¨è®­ç»ƒå­¦ç”Ÿæ¨¡å‹æ—¶ä½œä¸ºæƒé‡ä½¿ç”¨ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ•°å­¦å’Œå¤æ‚æ¨ç†ä»»åŠ¡ä¸Šæ˜¾è‘—æå‡äº†å­¦ç”Ÿæ¨¡å‹çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†åœ¨æ•°æ®è’¸é¦è¿‡ç¨‹ä¸­å¼•å…¥å¥–åŠ±æœºåˆ¶çš„æœ‰æ•ˆæ€§å’Œä¼˜åŠ¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å°å‹è¯­è¨€æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸­çš„æ€§èƒ½ä¸è¶³ï¼Œç°æœ‰çŸ¥è¯†è’¸é¦æ–¹æ³•ä¸»è¦ä¾èµ–äºå­¦ç”Ÿæ¨¡å‹å¤åˆ¶æ•™å¸ˆæ¨¡å‹çš„å“åº”ï¼Œå¯¼è‡´æ³›åŒ–èƒ½åŠ›å·®å’Œè®¡ç®—æˆæœ¬é«˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºAdvDistillæ¡†æ¶ï¼Œé€šè¿‡å¯¹æ¯ä¸ªæç¤ºç”Ÿæˆå¤šä¸ªæ•™å¸ˆå“åº”ï¼Œå¹¶åŸºäºè§„åˆ™éªŒè¯å™¨åˆ†é…å¥–åŠ±ï¼Œåˆ©ç”¨è¿™äº›å¥–åŠ±ä½œä¸ºè®­ç»ƒå­¦ç”Ÿæ¨¡å‹çš„æƒé‡ï¼Œä»è€Œæå‡å…¶æ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šAdvDistillçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªé˜¶æ®µï¼šé¦–å…ˆç”Ÿæˆæ•™å¸ˆæ¨¡å‹çš„å¤šé‡å“åº”ï¼›å…¶æ¬¡ï¼Œä½¿ç”¨è§„åˆ™éªŒè¯å™¨è¯„ä¼°è¿™äº›å“åº”å¹¶åˆ†é…å¥–åŠ±ï¼›æœ€åï¼Œå°†å¥–åŠ±ä½œä¸ºæƒé‡ç”¨äºå­¦ç”Ÿæ¨¡å‹çš„è®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†å¥–åŠ±æœºåˆ¶ï¼Œä½¿å¾—å­¦ç”Ÿæ¨¡å‹ä¸ä»…ä»…æ˜¯å¤åˆ¶æ•™å¸ˆæ¨¡å‹çš„å“åº”ï¼Œè€Œæ˜¯é€šè¿‡å¥–åŠ±å¼•å¯¼å­¦ä¹ ï¼Œæ˜¾è‘—æå‡äº†æ¨ç†ä»»åŠ¡çš„è¡¨ç°ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œå¥–åŠ±çš„åˆ†é…åŸºäºè§„åˆ™éªŒè¯å™¨çš„è¾“å‡ºï¼Œç¡®ä¿å¥–åŠ±çš„å¤šæ ·æ€§å’Œæ­£æ€åˆ†å¸ƒï¼Œä»¥ä¾¿åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æœ‰æ•ˆå¼•å¯¼å­¦ç”Ÿæ¨¡å‹çš„å­¦ä¹ ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨AdvDistillæ¡†æ¶çš„å­¦ç”Ÿæ¨¡å‹åœ¨æ•°å­¦å’Œå¤æ‚æ¨ç†ä»»åŠ¡ä¸Šæ€§èƒ½æå‡æ˜¾è‘—ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨åŸºå‡†æµ‹è¯•ä¸­ç›¸è¾ƒäºä¼ ç»Ÿè’¸é¦æ–¹æ³•æé«˜äº†çº¦20%çš„å‡†ç¡®ç‡ï¼ŒéªŒè¯äº†å¥–åŠ±æœºåˆ¶çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿå’Œæ•™è‚²æŠ€æœ¯ç­‰ã€‚é€šè¿‡æå‡å°å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼ŒAdvDistillå¯ä»¥åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­å®ç°æ›´é«˜æ•ˆçš„æ¨¡å‹éƒ¨ç½²ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The push to compress and impart the proficiency of Large Language Models (LLMs) into more deployable and efficient Small Language Models (SLMs) has benefited from improvements in knowledge distillation (KD) techniques. These techniques allow a smaller student model to learn from a more capable and larger teacher model's responses. However, distillation often revolves around the student model merely copying the teacher's in-distribution responses, limiting its generalisability. This limitation is amplified on reasoning tasks and can be computationally expensive. In this study, we propose AdvDistill, a reward-guided dataset distillation framework. We utilise multiple generations (responses) from a teacher for each prompt and assign rewards based on rule-based verifiers. These varying and normally distributed rewards serve as weights when training student models. Our methods and their subsequent behavioural analysis demonstrate a significant improvement in student model performance for mathematical and complex reasoning tasks, showcasing the efficacy and benefits of incorporating a rewarding mechanism in dataset distillation processes.

