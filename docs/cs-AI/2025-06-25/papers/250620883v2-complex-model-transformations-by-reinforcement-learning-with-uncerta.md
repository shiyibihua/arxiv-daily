---
layout: default
title: Complex Model Transformations by Reinforcement Learning with Uncertain Human Guidance
---

# Complex Model Transformations by Reinforcement Learning with Uncertain Human Guidance

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.20883" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.20883v2</a>
  <a href="https://arxiv.org/pdf/2506.20883.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.20883v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.20883v2', 'Complex Model Transformations by Reinforcement Learning with Uncertain Human Guidance')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kyanna Dagenais, Istvan David

**åˆ†ç±»**: cs.SE, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-25 (æ›´æ–°: 2025-08-06)

**å¤‡æ³¨**: Accepted for ACM/IEEE MODELS'25

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå¼ºåŒ–å­¦ä¹ çš„å¤æ‚æ¨¡å‹è½¬æ¢æ–¹æ³•ä»¥åº”å¯¹ä¸ç¡®å®šäººç±»æŒ‡å¯¼é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ¨¡å‹è½¬æ¢` `å¼ºåŒ–å­¦ä¹ ` `äººç±»æŒ‡å¯¼` `è‡ªåŠ¨åŒ–å·¥ç¨‹` `å¤æ‚ç³»ç»Ÿ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ¨¡å‹è½¬æ¢æ–¹æ³•åœ¨å¤„ç†å¤æ‚æ¨¡å‹è½¬æ¢æ—¶å®¹æ˜“å‡ºé”™ï¼Œä¸”æ‰‹åŠ¨å¼€å‘è¿‡ç¨‹å¾€å¾€ä¸å¯è¡Œã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡å¼ºåŒ–å­¦ä¹ ç»“åˆä¸ç¡®å®šäººç±»æŒ‡å¯¼çš„æ¡†æ¶ï¼Œä»¥è‡ªåŠ¨åŒ–å¤æ‚æ¨¡å‹è½¬æ¢çš„å¼€å‘è¿‡ç¨‹ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨äººç±»æŒ‡å¯¼çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•æ˜¾è‘—æé«˜äº†æ¨¡å‹è½¬æ¢çš„æ•ˆç‡ï¼Œä¼˜åŒ–äº†å¼€å‘æµç¨‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ¨¡å‹é©±åŠ¨å·¥ç¨‹é—®é¢˜å¸¸å¸¸éœ€è¦å¤æ‚çš„æ¨¡å‹è½¬æ¢ï¼Œè¿™äº›è½¬æ¢é€šå¸¸ä»¥å¹¿æ³›çš„åºåˆ—é“¾å¼è¿æ¥ã€‚æ‰‹åŠ¨å¼€å‘å¤æ‚çš„æ¨¡å‹è½¬æ¢è¿‡ç¨‹å®¹æ˜“å‡ºé”™ä¸”å¾€å¾€ä¸å¯è¡Œã€‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ˜¯ä¸€ç§æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œä½†åœ¨å¤æ‚é—®é¢˜ä¸­è¡¨ç°ä¸ä½³ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡RLå¼€å‘å¤æ‚æ¨¡å‹è½¬æ¢åºåˆ—çš„æ–¹æ³•æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å…è®¸å°†ç”¨æˆ·å®šä¹‰çš„æ¨¡å‹è½¬æ¢æ˜ å°„åˆ°RLåŸè¯­ï¼Œå¹¶ä½œä¸ºRLç¨‹åºæ‰§è¡Œï¼Œä»¥å¯»æ‰¾æœ€ä¼˜çš„æ¨¡å‹è½¬æ¢åºåˆ—ã€‚æˆ‘ä»¬çš„è¯„ä¼°è¡¨æ˜ï¼Œå³ä½¿äººç±»æŒ‡å¯¼ä¸ç¡®å®šï¼Œä¹Ÿèƒ½æ˜¾è‘—æå‡RLæ€§èƒ½ï¼Œä»è€Œæ›´é«˜æ•ˆåœ°å¼€å‘å¤æ‚æ¨¡å‹è½¬æ¢ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤æ‚æ¨¡å‹è½¬æ¢ï¼ˆMTï¼‰å¼€å‘ä¸­çš„æ•ˆç‡ä½ä¸‹å’Œé”™è¯¯é¢‘å‘é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚åºåˆ—æ—¶è¡¨ç°ä¸ä½³ï¼Œä¸”æ‰‹åŠ¨å¼€å‘éš¾åº¦å¤§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸ä¸ç¡®å®šçš„äººç±»æŒ‡å¯¼ï¼Œæ„å»ºä¸€ä¸ªæ¡†æ¶ï¼Œä½¿å¾—ç”¨æˆ·å®šä¹‰çš„æ¨¡å‹è½¬æ¢èƒ½å¤Ÿæ˜ å°„åˆ°RLåŸè¯­ï¼Œä»è€Œè‡ªåŠ¨å¯»æ‰¾æœ€ä¼˜çš„æ¨¡å‹è½¬æ¢åºåˆ—ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…æ‹¬å‡ ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ˜¯ç”¨æˆ·å®šä¹‰çš„æ¨¡å‹è½¬æ¢æ˜ å°„æ¨¡å—ï¼Œå…¶æ¬¡æ˜¯RLç®—æ³•æ¨¡å—ï¼Œæœ€åæ˜¯æ‰§è¡Œå’Œä¼˜åŒ–æ¨¡å—ï¼Œæ•´ä½“æµç¨‹ä¸ºï¼šç”¨æˆ·å®šä¹‰MT â†’ æ˜ å°„åˆ°RLåŸè¯­ â†’ æ‰§è¡ŒRLç¨‹åº â†’ ä¼˜åŒ–MTåºåˆ—ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†ä¸ç¡®å®šçš„äººç±»æŒ‡å¯¼æœ‰æ•ˆæ•´åˆè¿›RLæ¡†æ¶ä¸­ï¼Œæå‡äº†RLåœ¨å¤æ‚é—®é¢˜ä¸Šçš„è¡¨ç°ï¼Œä¸ä¼ ç»ŸRLæ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨äººç±»çŸ¥è¯†ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬RLç®—æ³•çš„é€‰æ‹©ã€å¥–åŠ±å‡½æ•°çš„è®¾è®¡ï¼Œä»¥åŠå¦‚ä½•å¤„ç†äººç±»æŒ‡å¯¼çš„ä¸ç¡®å®šæ€§ï¼Œè¿™äº›è®¾è®¡å†³å®šäº†æ¨¡å‹è½¬æ¢çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨äººç±»æŒ‡å¯¼çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”äºä¼ ç»Ÿæ–¹æ³•ï¼Œæ¨¡å‹è½¬æ¢æ•ˆç‡æå‡äº†æ˜¾è‘—ï¼Œå…·ä½“æ€§èƒ½æ•°æ®è¡¨æ˜ï¼ŒRLæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°30%ä»¥ä¸Šï¼Œæ˜¾ç¤ºå‡ºäººç±»æŒ‡å¯¼åœ¨å¤æ‚æ¨¡å‹è½¬æ¢ä¸­çš„é‡è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ¨¡å‹åŒæ­¥ã€è‡ªåŠ¨åŒ–æ¨¡å‹ä¿®å¤å’Œè®¾è®¡ç©ºé—´æ¢ç´¢ç­‰ã€‚é€šè¿‡æé«˜å¤æ‚æ¨¡å‹è½¬æ¢çš„æ•ˆç‡ï¼Œèƒ½å¤Ÿåœ¨è½¯ä»¶å·¥ç¨‹ã€ç³»ç»Ÿè®¾è®¡ç­‰é¢†åŸŸå¸¦æ¥æ˜¾è‘—çš„å®é™…ä»·å€¼ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨äººæœºåä½œå·¥ç¨‹æ–¹æ³•çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Model-driven engineering problems often require complex model transformations (MTs), i.e., MTs that are chained in extensive sequences. Pertinent examples of such problems include model synchronization, automated model repair, and design space exploration. Manually developing complex MTs is an error-prone and often infeasible process. Reinforcement learning (RL) is an apt way to alleviate these issues. In RL, an autonomous agent explores the state space through trial and error to identify beneficial sequences of actions, such as MTs. However, RL methods exhibit performance issues in complex problems. In these situations, human guidance can be of high utility. In this paper, we present an approach and technical framework for developing complex MT sequences through RL, guided by potentially uncertain human advice. Our framework allows user-defined MTs to be mapped onto RL primitives, and executes them as RL programs to find optimal MT sequences. Our evaluation shows that human guidance, even if uncertain, substantially improves RL performance, and results in more efficient development of complex MTs. Through a trade-off between the certainty and timeliness of human advice, our method takes a step towards RL-driven human-in-the-loop engineering methods.

