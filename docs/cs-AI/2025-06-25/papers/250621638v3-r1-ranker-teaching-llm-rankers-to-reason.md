---
layout: default
title: R1-Ranker: Teaching LLM Rankers to Reason
---

# R1-Ranker: Teaching LLM Rankers to Reason

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21638" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.21638v3</a>
  <a href="https://arxiv.org/pdf/2506.21638.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21638v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21638v3', 'R1-Ranker: Teaching LLM Rankers to Reason')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tao Feng, Zhigang Hua, Zijie Lei, Yan Xie, Shuang Yang, Bo Long, Jiaxuan You

**åˆ†ç±»**: cs.IR, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-25 (æ›´æ–°: 2025-10-16)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºR1-Rankerä»¥è§£å†³LLMæ’åä»»åŠ¡ä¸­çš„æ¨ç†ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æ’åä»»åŠ¡` `æ¨ç†èƒ½åŠ›` `å¼ºåŒ–å­¦ä¹ ` `æ¨èç³»ç»Ÿ` `ä¿¡æ¯æ£€ç´¢` `è¿­ä»£ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„LLMæ’åå™¨å¾€å¾€æ˜¯ç‰¹å®šé¢†åŸŸçš„ï¼Œç¼ºä¹çµæ´»æ€§å’Œè¿­ä»£ä¼˜åŒ–ï¼Œé™åˆ¶äº†æ¨ç†èƒ½åŠ›çš„å‘æŒ¥ã€‚
2. æœ¬æ–‡æå‡ºR1-Rankeræ¡†æ¶ï¼Œç»“åˆDRankerä¸€æ¬¡æ€§ç”Ÿæˆå®Œæ•´æ’åå’ŒIRankeré€šè¿‡è¿­ä»£æ¶ˆé™¤è¿‡ç¨‹è¿›è¡Œæ·±åº¦æ¨ç†ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒIRanker-3Båœ¨å¤šä¸ªä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨é›¶-shotä»»åŠ¡ä¸Šæå‡è¶…è¿‡9%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•°å­¦ã€ç¼–ç¨‹å’Œç§‘å­¦é—®é¢˜è§£å†³ç­‰é¢†åŸŸå±•ç°äº†å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨æ’åä»»åŠ¡ä¸­çš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ç°æœ‰çš„LLMæ’åå™¨å¾€å¾€æ˜¯ç‰¹å®šé¢†åŸŸçš„ï¼Œä¾èµ–å›ºå®šçš„éª¨å¹²ç½‘ç»œï¼Œç¼ºä¹è¿­ä»£ä¼˜åŒ–ï¼Œé™åˆ¶äº†å…¶æ¨ç†èƒ½åŠ›çš„å‘æŒ¥ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†R1-Rankerï¼Œä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¨ç†æ¿€åŠ±æ¡†æ¶ï¼ŒåŒ…å«DRankerå’ŒIRankerä¸¤ä¸ªäº’è¡¥è®¾è®¡ã€‚é€šè¿‡åœ¨ä¹ä¸ªæ•°æ®é›†ä¸Šçš„è¯„ä¼°ï¼ŒIRanker-3Båœ¨æŸäº›ä»»åŠ¡ä¸Šè¶…è¶Šäº†æ›´å¤§çš„7Bæ¨¡å‹ï¼Œå¹¶å®ç°äº†15.7%çš„å¹³å‡ç›¸å¯¹æå‡ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œç»Ÿä¸€å¤šæ ·çš„æ’åä»»åŠ¡ä¸å•ä¸€çš„æ¨ç†é©±åŠ¨åŸºç¡€æ¨¡å‹æ˜¯æ¨åŠ¨LLMåœ¨æ’ååœºæ™¯ä¸­æ¨ç†èƒ½åŠ›å‘å±•çš„æœ‰æ•ˆé€”å¾„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³LLMåœ¨æ’åä»»åŠ¡ä¸­çš„æ¨ç†ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€å±€é™äºç‰¹å®šé¢†åŸŸï¼Œç¼ºä¹çµæ´»æ€§å’Œè¿­ä»£ä¼˜åŒ–ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨LLMçš„æ¨ç†æ½œåŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºR1-Rankeræ¡†æ¶ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ æ¿€åŠ±æ¨ç†è¿‡ç¨‹ï¼Œè®¾è®¡DRankerå’ŒIRankerä¸¤ä¸ªæ¨¡å—ï¼Œå‰è€…ä¸€æ¬¡æ€§ç”Ÿæˆå®Œæ•´æ’åï¼Œåè€…åˆ™é€šè¿‡è¿­ä»£æ¶ˆé™¤è¿‡ç¨‹ä¿ƒè¿›æ›´æ·±å±‚æ¬¡çš„æ¨ç†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šR1-Rankeræ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šDRankerè´Ÿè´£å¿«é€Ÿç”Ÿæˆåˆæ­¥æ’åï¼ŒIRankeråˆ™é€šè¿‡é€æ­¥æ¶ˆé™¤å€™é€‰é¡¹å¹¶ç»™äºˆé˜¶æ®µæ€§å¥–åŠ±æ¥ä¼˜åŒ–æ’åç»“æœã€‚

**å…³é”®åˆ›æ–°**ï¼šR1-Rankerçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†å¼ºåŒ–å­¦ä¹ ä¸æ¨ç†è¿‡ç¨‹ç»“åˆï¼ŒIRankerçš„è¿­ä»£æ¶ˆé™¤æœºåˆ¶ä¸ä¼ ç»Ÿçš„å›ºå®šæ’åæ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”ä¸åŒä»»åŠ¡çš„éœ€æ±‚ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒIRankerä½¿ç”¨äº†é€æ­¥å¥–åŠ±æœºåˆ¶ï¼Œå¼ºåŒ–å­¦ä¹ çš„æŸå¤±å‡½æ•°è¢«ä¼˜åŒ–ä»¥é¼“åŠ±æ›´æ·±å±‚æ¬¡çš„æ¨ç†ï¼Œç½‘ç»œç»“æ„åˆ™é‡‡ç”¨äº†é€‚åº”æ€§è°ƒæ•´ä»¥é€‚åº”ä¸åŒè§„æ¨¡çš„æ¨¡å‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒIRanker-3Båœ¨ä¹ä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†æ›´å¤§çš„7Bæ¨¡å‹ï¼Œå¹¶å®ç°äº†15.7%çš„å¹³å‡ç›¸å¯¹æå‡ã€‚æ­¤å¤–ï¼ŒIRanker-3Båœ¨é›¶-shotä»»åŠ¡ä¸Šæå‡è¶…è¿‡9%ï¼Œè€Œæ¨ç†è½¨è¿¹å¯¹å…¶ä»–LLMçš„æå‡å¹…åº¦å¯è¾¾22.87%ã€‚è¿™äº›ç»“æœéªŒè¯äº†å¼ºåŒ–å­¦ä¹ å’Œè¿­ä»£æ¨ç†åœ¨æ’åä»»åŠ¡ä¸­çš„é‡è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

R1-Rankerçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼ŒåŒ…æ‹¬ä¿¡æ¯æ£€ç´¢ã€æ¨èç³»ç»Ÿå’Œè‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„ä»»åŠ¡æ’åºç­‰ã€‚é€šè¿‡æå‡LLMåœ¨æ’åä»»åŠ¡ä¸­çš„æ¨ç†èƒ½åŠ›ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿä¸ºç”¨æˆ·æä¾›æ›´ç²¾å‡†çš„æ¨èå’Œä¿¡æ¯æ£€ç´¢ç»“æœï¼Œè¿›è€Œæå‡ç”¨æˆ·ä½“éªŒå’Œæ»¡æ„åº¦ã€‚æœªæ¥ï¼ŒR1-Rankerå¯èƒ½æ¨åŠ¨æ›´å¤šæ™ºèƒ½ç³»ç»Ÿçš„å¼€å‘ï¼Œä½¿å…¶åœ¨å¤æ‚å†³ç­–åœºæ™¯ä¸­è¡¨ç°æ›´ä¸ºå‡ºè‰²ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have recently shown strong reasoning abilities in domains like mathematics, coding, and scientific problem-solving, yet their potential for ranking tasks, where prime examples include retrieval, recommender systems, and LLM routing, remains underexplored. Ranking requires complex reasoning across heterogeneous candidates, but existing LLM-based rankers are often domain-specific, tied to fixed backbones, and lack iterative refinement, limiting their ability to fully exploit LLMs' reasoning potential. To address these challenges, we propose R1-Ranker, a reasoning-incentive framework built on reinforcement learning, with two complementary designs: DRanker, which generates full rankings in one shot, and IRanker, which decomposes ranking into an iterative elimination process with step-wise rewards to encourage deeper reasoning. We evaluate unified R1-Rankers on nine datasets spanning recommendation, routing, and passage ranking, showing that IRanker-3B consistently achieves state-of-the-art performance, surpasses larger 7B models on some tasks, and yields a 15.7% average relative improvement. Ablation and generalization experiments further confirm the critical role of reinforcement learning and iterative reasoning, with IRanker-3B improving zero-shot performance by over 9% on out-of-domain tasks and reasoning traces boosting other LLMs by up to 22.87%. These results demonstrate that unifying diverse ranking tasks with a single reasoning-driven foundation model is both effective and essential for advancing LLM reasoning in ranking scenarios.

