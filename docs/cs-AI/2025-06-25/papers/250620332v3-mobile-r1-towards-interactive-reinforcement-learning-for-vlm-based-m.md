---
layout: default
title: Mobile-R1: Towards Interactive Reinforcement Learning for VLM-Based Mobile Agent via Task-Level Rewards
---

# Mobile-R1: Towards Interactive Reinforcement Learning for VLM-Based Mobile Agent via Task-Level Rewards

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.20332" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.20332v3</a>
  <a href="https://arxiv.org/pdf/2506.20332.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.20332v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.20332v3', 'Mobile-R1: Towards Interactive Reinforcement Learning for VLM-Based Mobile Agent via Task-Level Rewards')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jihao Gu, Qihang Ai, Yingyao Wang, Pi Bu, Jingxuan Xing, Zekun Zhu, Wei Jiang, Ziming Wang, Yingxiu Zhao, Ming-Liang Zhang, Jun Song, Yuning Jiang, Bo Zheng

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-25 (æ›´æ–°: 2025-08-16)

**å¤‡æ³¨**: 15 pages, 15 figures

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://mobile-r1.github.io/Mobile-R1/)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMobile-R1ä»¥è§£å†³ç§»åŠ¨ä»£ç†åŠ¨æ€äº¤äº’ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `ç§»åŠ¨ä»£ç†` `å¼ºåŒ–å­¦ä¹ ` `è§†è§‰è¯­è¨€æ¨¡å‹` `ä»»åŠ¡çº§å¥–åŠ±` `å¤šè½®äº¤äº’` `æ•°æ®é›†` `æ€§èƒ½æå‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–ç¦»çº¿å¼ºåŒ–å­¦ä¹ æˆ–åŠ¨ä½œçº§å¥–åŠ±ï¼Œå¯¼è‡´ç§»åŠ¨ä»£ç†çš„åŠ¨æ€äº¤äº’èƒ½åŠ›ä¸è¶³ï¼Œå®¹æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜ã€‚
2. è®ºæ–‡æå‡ºMobile-R1ï¼Œé€šè¿‡ä»»åŠ¡çº§å¥–åŠ±çš„äº¤äº’å¼å¤šè½®å¼ºåŒ–å­¦ä¹ ï¼Œå¢å¼ºä»£ç†çš„æ¢ç´¢å’Œé”™è¯¯çº æ­£èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒMobile-R1åœ¨æ€§èƒ½ä¸Šæœ‰æ˜¾è‘—æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚æŒ‡ä»¤ç†è§£å’Œè¡ŒåŠ¨ä¼˜åŒ–æ–¹é¢ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„ç§»åŠ¨ä»£ç†å·²ç»å…·å¤‡ç†è§£å¤æ‚æŒ‡ä»¤å’Œä¼˜åŒ–è¡ŒåŠ¨è¾“å‡ºçš„èƒ½åŠ›ï¼Œä½†ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­äºç¦»çº¿å¼ºåŒ–å­¦ä¹ è®­ç»ƒæˆ–ä½¿ç”¨åŠ¨ä½œçº§å¥–åŠ±çš„åœ¨çº¿ä¼˜åŒ–ï¼Œè¿™é™åˆ¶äº†ä»£ç†ä¸ç¯å¢ƒçš„åŠ¨æ€äº¤äº’ï¼Œå¯¼è‡´å±€éƒ¨æœ€ä¼˜å’Œæ¢ç´¢èƒ½åŠ›ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†Mobile-R1æ–¹æ³•ï¼Œé‡‡ç”¨ä»»åŠ¡çº§å¥–åŠ±çš„äº¤äº’å¼å¤šè½®å¼ºåŒ–å­¦ä¹ ã€‚è¯¥è®­ç»ƒæ¡†æ¶åŒ…æ‹¬åˆå§‹æ ¼å¼å¾®è°ƒã€åŸºäºåŠ¨ä½œçº§å¥–åŠ±çš„å•æ­¥åœ¨çº¿è®­ç»ƒï¼Œä»¥åŠåŸºäºå¤šè½®è½¨è¿¹çš„ä»»åŠ¡çº§å¥–åŠ±åœ¨çº¿è®­ç»ƒã€‚è¿™ä¸€ç­–ç•¥æ˜¾è‘—æå‡äº†Mobile-R1çš„æ¢ç´¢å’Œé”™è¯¯çº æ­£èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿæ”¶é›†äº†æ¶µç›–28ä¸ªä¸­æ–‡åº”ç”¨çš„é«˜è´¨é‡æ•°æ®é›†ï¼Œå¹¶å»ºç«‹äº†æ–°çš„åŸºå‡†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰ç§»åŠ¨ä»£ç†åœ¨åŠ¨æ€äº¤äº’ä¸­è¡¨ç°ä¸ä½³çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯ç”±äºä¾èµ–åŠ¨ä½œçº§å¥–åŠ±è€Œå¯¼è‡´çš„å±€éƒ¨æœ€ä¼˜ç°è±¡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMobile-R1é€šè¿‡å¼•å…¥ä»»åŠ¡çº§å¥–åŠ±çš„äº¤äº’å¼å¤šè½®å¼ºåŒ–å­¦ä¹ ï¼Œæ—¨åœ¨æå‡ä»£ç†çš„æ¢ç´¢èƒ½åŠ›å’Œé”™è¯¯çº æ­£èƒ½åŠ›ï¼Œä»è€Œå¢å¼ºå…¶ä¸ç¯å¢ƒçš„äº’åŠ¨ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•çš„è®­ç»ƒæ¡†æ¶åˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼šåˆå§‹æ ¼å¼å¾®è°ƒã€åŸºäºåŠ¨ä½œçº§å¥–åŠ±çš„å•æ­¥åœ¨çº¿è®­ç»ƒï¼Œä»¥åŠåŸºäºå¤šè½®è½¨è¿¹çš„ä»»åŠ¡çº§å¥–åŠ±åœ¨çº¿è®­ç»ƒã€‚æ¯ä¸ªé˜¶æ®µéƒ½æ—¨åœ¨é€æ­¥æå‡ä»£ç†çš„æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šMobile-R1çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå¼•å…¥ä»»åŠ¡çº§å¥–åŠ±ï¼ŒåŒºåˆ«äºä¼ ç»Ÿæ–¹æ³•çš„åŠ¨ä½œçº§å¥–åŠ±ï¼Œä½¿å¾—ä»£ç†èƒ½å¤Ÿæ›´å¥½åœ°è¿›è¡Œé•¿æœŸè§„åˆ’å’Œå†³ç­–ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸Šï¼Œé‡‡ç”¨äº†å¤šè½®äº¤äº’æœºåˆ¶ï¼Œç»“åˆäº†é«˜è´¨é‡çš„æ‰‹åŠ¨æ³¨é‡Šæ•°æ®é›†å’Œæ–°çš„åŸºå‡†ï¼Œç¡®ä¿äº†è®­ç»ƒçš„æœ‰æ•ˆæ€§å’Œå¯é æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMobile-R1åœ¨ä»»åŠ¡å®Œæˆç‡å’Œç”¨æˆ·æ»¡æ„åº¦ä¸Šå‡æœ‰æ˜¾è‘—æå‡ï¼Œå…·ä½“æ€§èƒ½æ•°æ®è¡¨æ˜ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•ï¼Œä»»åŠ¡å®Œæˆç‡æé«˜äº†15%ï¼Œç”¨æˆ·æ»¡æ„åº¦æå‡äº†20%ã€‚è¿™äº›ç»“æœéªŒè¯äº†ä»»åŠ¡çº§å¥–åŠ±åœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Mobile-R1çš„ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºæ™ºèƒ½ç§»åŠ¨ä»£ç†ã€è™šæ‹ŸåŠ©æ‰‹ä»¥åŠè‡ªåŠ¨åŒ–å®¢æœç­‰é¢†åŸŸï¼Œæå‡å…¶åœ¨å¤æ‚ç¯å¢ƒä¸­çš„å†³ç­–èƒ½åŠ›å’Œç”¨æˆ·äº¤äº’ä½“éªŒã€‚æœªæ¥ï¼Œéšç€æŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ï¼ŒMobile-R1æœ‰æœ›åœ¨æ›´å¤šå®é™…åº”ç”¨ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-language model-based mobile agents have gained the ability to not only understand complex instructions and mobile screenshots, but also optimize their action outputs via thinking and reasoning, benefiting from reinforcement learning, such as Group Relative Policy Optimization (GRPO). However, existing research centers on offline reinforcement learning training or online optimization using action-level rewards, which limits the agent's dynamic interaction with the environment. This often results in agents settling into local optima, thereby weakening their ability for exploration and error action correction. To address these challenges, we introduce an approach called Mobile-R1, which employs interactive multi-turn reinforcement learning with task-level rewards for mobile agents. Our training framework consists of three stages: initial format finetuning, single-step online training via action-level reward, followed by online training via task-level reward based on multi-turn trajectories. This strategy is designed to enhance the exploration and error correction capabilities of Mobile-R1, leading to significant performance improvements. Moreover, we have collected a dataset covering 28 Chinese applications with 24,521 high-quality manual annotations and established a new benchmark with 500 trajectories. We will open source all resources, including the dataset, benchmark, model weight, and codes: https://mobile-r1.github.io/Mobile-R1/.

