---
layout: default
title: General agents contain world models
---

# General agents contain world models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.01622" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.01622v5</a>
  <a href="https://arxiv.org/pdf/2506.01622.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.01622v5" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.01622v5', 'General agents contain world models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jonathan Richens, David Abel, Alexis Bellot, Tom Everitt

**åˆ†ç±»**: cs.AI, cs.LG, cs.RO, stat.ML

**å‘å¸ƒæ—¥æœŸ**: 2025-06-02 (æ›´æ–°: 2025-10-20)

**å¤‡æ³¨**: Accepted ICML 2025. Typos corrected

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸–ç•Œæ¨¡å‹ä»¥æå‡æ™ºèƒ½ä½“çš„ç›®æ ‡å¯¼å‘è¡Œä¸º**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `ä¸–ç•Œæ¨¡å‹` `ç›®æ ‡å¯¼å‘è¡Œä¸º` `æ™ºèƒ½ä½“å­¦ä¹ ` `å¤šæ­¥ä»»åŠ¡` `é¢„æµ‹æ¨¡å‹` `å®‰å…¨æ™ºèƒ½ä½“` `å¤æ‚ç¯å¢ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç°æœ‰çš„æ— æ¨¡å‹å­¦ä¹ æ–¹æ³•åœ¨å¤„ç†å¤æ‚ç›®æ ‡å¯¼å‘ä»»åŠ¡æ—¶å­˜åœ¨å±€é™æ€§ï¼Œæ— æ³•æœ‰æ•ˆæ³›åŒ–ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šè®ºæ–‡æå‡ºé€šè¿‡æå–æ™ºèƒ½ä½“çš„ç­–ç•¥æ¥å­¦ä¹ ç¯å¢ƒçš„é¢„æµ‹æ¨¡å‹ï¼Œä»è€Œå®ç°æ›´çµæ´»çš„ç›®æ ‡å¯¼å‘è¡Œä¸ºã€‚
3. å®éªŒæˆ–æ•ˆæœï¼šç ”ç©¶è¡¨æ˜ï¼Œæå‡æ™ºèƒ½ä½“çš„æ€§èƒ½å’Œç›®æ ‡å¤æ‚æ€§éœ€è¦æ›´å‡†ç¡®çš„ä¸–ç•Œæ¨¡å‹ï¼Œå…·æœ‰æ˜¾è‘—çš„å®é™…åº”ç”¨ä»·å€¼ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æ¢è®¨äº†ä¸–ç•Œæ¨¡å‹æ˜¯å¦æ˜¯å®ç°çµæ´»ç›®æ ‡å¯¼å‘è¡Œä¸ºçš„å¿…è¦æ¡ä»¶ï¼Œè¿˜æ˜¯æ— æ¨¡å‹å­¦ä¹ å°±è¶³å¤Ÿã€‚æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªæ­£å¼çš„ç­”æ¡ˆï¼Œè¡¨æ˜ä»»ä½•èƒ½å¤Ÿæ³›åŒ–åˆ°å¤šæ­¥ç›®æ ‡å¯¼å‘ä»»åŠ¡çš„æ™ºèƒ½ä½“ï¼Œå¿…é¡»å­¦ä¹ å…¶ç¯å¢ƒçš„é¢„æµ‹æ¨¡å‹ã€‚æˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•ä»æ™ºèƒ½ä½“çš„ç­–ç•¥ä¸­æå–è¯¥æ¨¡å‹ï¼Œå¹¶æŒ‡å‡ºæå‡æ™ºèƒ½ä½“æ€§èƒ½æˆ–å¤æ‚ç›®æ ‡çš„èƒ½åŠ›éœ€è¦å­¦ä¹ æ›´å‡†ç¡®çš„ä¸–ç•Œæ¨¡å‹ã€‚è¿™ä¸€å‘ç°å¯¹å¼€å‘å®‰å…¨ä¸”é€šç”¨çš„æ™ºèƒ½ä½“ã€ç•Œå®šæ™ºèƒ½ä½“åœ¨å¤æ‚ç¯å¢ƒä¸­çš„èƒ½åŠ›ä»¥åŠæä¾›ä»æ™ºèƒ½ä½“ä¸­å¼•å‡ºä¸–ç•Œæ¨¡å‹çš„æ–°ç®—æ³•å…·æœ‰é‡è¦æ„ä¹‰ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ™ºèƒ½ä½“åœ¨å¤æ‚ç¯å¢ƒä¸­å®ç°çµæ´»ç›®æ ‡å¯¼å‘è¡Œä¸ºçš„èƒ½åŠ›é—®é¢˜ã€‚ç°æœ‰çš„æ— æ¨¡å‹å­¦ä¹ æ–¹æ³•åœ¨å¤šæ­¥ä»»åŠ¡ä¸­è¡¨ç°ä¸è¶³ï¼Œæ— æ³•æœ‰æ•ˆæ³›åŒ–åˆ°æ–°æƒ…å¢ƒã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯è¯æ˜ä»»ä½•èƒ½å¤ŸæˆåŠŸæ‰§è¡Œå¤šæ­¥ç›®æ ‡å¯¼å‘ä»»åŠ¡çš„æ™ºèƒ½ä½“ï¼Œå¿…é¡»å…·å¤‡å¯¹ç¯å¢ƒçš„é¢„æµ‹æ¨¡å‹ã€‚é€šè¿‡ä»æ™ºèƒ½ä½“çš„ç­–ç•¥ä¸­æå–è¯¥æ¨¡å‹ï¼Œèƒ½å¤Ÿæå‡å…¶æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ™ºèƒ½ä½“çš„ç­–ç•¥å­¦ä¹ æ¨¡å—å’Œä¸–ç•Œæ¨¡å‹æå–æ¨¡å—ã€‚æ™ºèƒ½ä½“é€šè¿‡ä¸ç¯å¢ƒäº¤äº’å­¦ä¹ ç­–ç•¥ï¼ŒåŒæ—¶æå–å¹¶æ›´æ–°å…¶ç¯å¢ƒæ¨¡å‹ï¼Œä»¥é€‚åº”å¤æ‚ç›®æ ‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†ä»æ™ºèƒ½ä½“ç­–ç•¥ä¸­æå–ä¸–ç•Œæ¨¡å‹çš„æ–¹æ³•ï¼Œè¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„æ— æ¨¡å‹å­¦ä¹ æ–¹æ³•æœ¬è´¨ä¸Šä¸åŒï¼Œå¼ºè°ƒäº†æ¨¡å‹å­¦ä¹ çš„é‡è¦æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ¨¡å‹çš„é¢„æµ‹å‡†ç¡®æ€§ï¼Œå¹¶é€šè¿‡å¤šå±‚ç¥ç»ç½‘ç»œç»“æ„æ¥å¢å¼ºæ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ï¼Œç¡®ä¿æ™ºèƒ½ä½“èƒ½å¤Ÿæœ‰æ•ˆå­¦ä¹ å’Œæ›´æ–°å…¶ä¸–ç•Œæ¨¡å‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨ä¸–ç•Œæ¨¡å‹çš„æ™ºèƒ½ä½“åœ¨å¤æ‚ç›®æ ‡å¯¼å‘ä»»åŠ¡ä¸­çš„æˆåŠŸç‡æ˜¾è‘—æé«˜ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨å¤šæ­¥ä»»åŠ¡ä¸­æˆåŠŸç‡æå‡äº†20%ä»¥ä¸Šï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ— æ¨¡å‹å­¦ä¹ æ–¹æ³•å…·æœ‰æ˜æ˜¾ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªä¸»æœºå™¨äººã€æ™ºèƒ½ä»£ç†å’Œå¤æ‚ç³»ç»Ÿæ§åˆ¶ç­‰ã€‚é€šè¿‡å¼•å…¥ä¸–ç•Œæ¨¡å‹ï¼Œæ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨åŠ¨æ€ç¯å¢ƒä¸­æ›´çµæ´»åœ°é€‚åº”å’Œæ‰§è¡Œä»»åŠ¡ï¼Œæå‡å…¶å®‰å…¨æ€§å’Œæ•ˆç‡ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Are world models a necessary ingredient for flexible, goal-directed behaviour, or is model-free learning sufficient? We provide a formal answer to this question, showing that any agent capable of generalizing to multi-step goal-directed tasks must have learned a predictive model of its environment. We show that this model can be extracted from the agent's policy, and that increasing the agents performance or the complexity of the goals it can achieve requires learning increasingly accurate world models. This has a number of consequences: from developing safe and general agents, to bounding agent capabilities in complex environments, and providing new algorithms for eliciting world models from agents.

