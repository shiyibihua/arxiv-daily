---
layout: default
title: SHERPA: A Model-Driven Framework for Large Language Model Execution
---

# SHERPA: A Model-Driven Framework for Large Language Model Execution

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.00272" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.00272v1</a>
  <a href="https://arxiv.org/pdf/2509.00272.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.00272v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.00272v1', 'SHERPA: A Model-Driven Framework for Large Language Model Execution')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Boqi Chen, Kua Chen, Jos√© Antonio Hern√°ndez L√≥pez, Gunter Mussbacher, D√°niel Varr√≥, Amir Feizpour

**ÂàÜÁ±ª**: cs.AI, cs.SE

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-29

**Â§áÊ≥®**: MODELS 2025

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫SHERPAÊ°ÜÊû∂‰ª•ÊèêÂçáÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®Â§çÊùÇ‰ªªÂä°‰∏≠ÁöÑË°®Áé∞**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§ßËØ≠Ë®ÄÊ®°Âûã` `Â±ÇÊ¨°Áä∂ÊÄÅÊú∫` `ÁªìÊûÑÂåñÊé®ÁêÜ` `È¢ÜÂüüÊúÄ‰Ω≥ÂÆûË∑µ` `Êú∫Âô®Â≠¶‰π†ÊéßÂà∂`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®Â§ÑÁêÜÂ§çÊùÇ‰ªªÂä°Êó∂ÔºåÁº∫‰πèÁªìÊûÑÂåñÊé®ÁêÜËÉΩÂäõÔºåÂ∞§ÂÖ∂ÊòØÈ¢ÜÂüüÁâπÂÆöÁöÑÊúÄ‰Ω≥ÂÆûË∑µÂ∏∏Â∏∏Êú™Ë¢´Á∫≥ÂÖ•„ÄÇ
2. SHERPAÊ°ÜÊû∂ÈÄöËøáÂ∞ÜÈ¢ÜÂüüÊúÄ‰Ω≥ÂÆûË∑µÊï¥ÂêàËøõÂ±ÇÊ¨°Áä∂ÊÄÅÊú∫ÔºåÊèê‰æõ‰∫ÜÂØπLLMË°å‰∏∫ÁöÑÁªÜÁ≤íÂ∫¶ÊéßÂà∂„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåSHERPAÂú®‰ª£Á†ÅÁîüÊàê„ÄÅÁ±ªÂêçÁîüÊàêÂíåÈóÆÁ≠î‰ªªÂä°‰∏≠ÊòæËëóÊèêÂçá‰∫ÜLLMÁöÑËæìÂá∫Ë¥®Èáè„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËøëÂπ¥Êù•ÔºåÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Â§ö‰∏™È¢ÜÂüüÂæóÂà∞‰∫ÜÂπøÊ≥õÂ∫îÁî®„ÄÇÂ∞ΩÁÆ°ÂÖ∂ËÉΩÂäõ‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÔºå‰ΩÜLLMsÂú®ÁªìÊûÑÂåñÊé®ÁêÜËÉΩÂäõÊñπÈù¢Â≠òÂú®‰∏çË∂≥ÔºåÂ∞§ÂÖ∂ÊòØÂú®ÈúÄË¶ÅÈ¢ÜÂüüÁâπÂÆöÊúÄ‰Ω≥ÂÆûË∑µÁöÑÂ§çÊùÇ‰ªªÂä°‰∏≠ÔºåËøô‰∫õÊúÄ‰Ω≥ÂÆûË∑µÂæÄÂæÄÂú®ËÆ≠ÁªÉÊï∞ÊçÆ‰∏≠Áº∫Â§±„ÄÇËôΩÁÑ∂ÁªìÂêà‰∫∫Á±ªÊúÄ‰Ω≥ÂÆûË∑µÁöÑÂ§öÊ≠•ÊèêÁ§∫ÊñπÊ≥ïÔºàÂ¶ÇÊÄùÁª¥ÈìæÂíåÊÄùÁª¥Ê†ëÔºâÈÄêÊ∏êÊµÅË°åÔºå‰ΩÜÁº∫‰πèÊéßÂà∂LLMË°å‰∏∫ÁöÑÈÄöÁî®Êú∫Âà∂„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜSHERPAÔºå‰∏Ä‰∏™Ê®°ÂûãÈ©±Âä®ÁöÑÊ°ÜÊû∂ÔºåÈÄöËøáÂ∞ÜÈ¢ÜÂüüÁâπÂÆöÊúÄ‰Ω≥ÂÆûË∑µÊòéÁ°ÆÁ∫≥ÂÖ•Â±ÇÊ¨°Áä∂ÊÄÅÊú∫ÔºåÊù•ÊèêÈ´òLLMÂú®Â§çÊùÇ‰ªªÂä°‰∏≠ÁöÑË°®Áé∞„ÄÇSHERPAÈÄöËøá‰ΩøÁî®Áä∂ÊÄÅÊú∫ÁªìÊûÑÂåñLLMÊâßË°åËøáÁ®ãÔºå‰ΩøÂæóÈÄöËøáËßÑÂàôÊàñÂü∫‰∫éÊú∫Âô®Â≠¶‰π†ÁöÑÊñπÊ≥ïÔºàÂåÖÊã¨LLMsÔºâÂØπÂÖ∂Ë°å‰∏∫ËøõË°åÊõ¥ÁªÜÁ≤íÂ∫¶ÁöÑÊéßÂà∂Êàê‰∏∫ÂèØËÉΩ„ÄÇÊàë‰ª¨Â±ïÁ§∫‰∫ÜSHERPAÂú®‰ª£Á†ÅÁîüÊàê„ÄÅÁ±ªÂêçÁîüÊàêÂíåÈóÆÁ≠îÁ≠âÂ§öÁßç‰ªªÂä°‰∏≠ÁöÑÊúâÊïàÊÄßÔºåÂπ∂ÈÄöËøáÁ≥ªÁªüËØÑ‰º∞ÊØîËæÉ‰∫Ü‰∏çÂêåÁä∂ÊÄÅÊú∫ÈÖçÁΩÆ‰∏éÊó†Áä∂ÊÄÅÊú∫Âü∫Á∫øÊñπÊ≥ïÁöÑË°®Áé∞ÔºåÁªìÊûúË°®ÊòéÔºåÂêàÁêÜËÆæËÆ°ÁöÑÁä∂ÊÄÅÊú∫ÊòæËëóÊèêÂçá‰∫ÜLLMËæìÂá∫ÁöÑË¥®ÈáèÔºåÂ∞§ÂÖ∂ÊòØÂú®Áº∫‰πèËÆ≠ÁªÉÊï∞ÊçÆÁöÑÂ§çÊùÇ‰ªªÂä°‰∏≠„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Â§ßËØ≠Ë®ÄÊ®°ÂûãÂú®Â§çÊùÇ‰ªªÂä°‰∏≠Áº∫‰πèÁªìÊûÑÂåñÊé®ÁêÜËÉΩÂäõÁöÑÈóÆÈ¢òÔºåÁé∞ÊúâÊñπÊ≥ïÊú™ËÉΩÊúâÊïàÊï¥ÂêàÈ¢ÜÂüüÁâπÂÆöÁöÑÊúÄ‰Ω≥ÂÆûË∑µ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöSHERPAÊ°ÜÊû∂ÈÄöËøáÂ∞ÜÈ¢ÜÂüüÊúÄ‰Ω≥ÂÆûË∑µÁ∫≥ÂÖ•Â±ÇÊ¨°Áä∂ÊÄÅÊú∫ÔºåÊèê‰æõ‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊéßÂà∂Êú∫Âà∂Ôºå‰ΩøÂæóLLMÂú®ÊâßË°åÂ§çÊùÇ‰ªªÂä°Êó∂ËÉΩÂ§üÈÅµÂæ™Êõ¥ÊòéÁ°ÆÁöÑËßÑÂàôÂíåÂÜ≥Á≠ñ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöSHERPAÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨Áä∂ÊÄÅÊú∫ÁöÑËÆæËÆ°‰∏éÂÆûÁé∞ÔºåÂÖ∑‰ΩìÊ®°ÂùóÂåÖÊã¨Áä∂ÊÄÅÂÆö‰πâ„ÄÅËΩ¨Êç¢ËßÑÂàôÂíåÂü∫‰∫éÊú∫Âô®Â≠¶‰π†ÁöÑÂÜ≥Á≠ñÊú∫Âà∂„ÄÇÈÄöËøáËøô‰∫õÊ®°ÂùóÔºåSHERPAËÉΩÂ§üÂú®ÊâßË°åËøáÁ®ã‰∏≠Âä®ÊÄÅË∞ÉÊï¥LLMÁöÑË°å‰∏∫„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöSHERPAÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÂ∞ÜÂ±ÇÊ¨°Áä∂ÊÄÅÊú∫‰∏éLLMÁªìÂêàÔºåÂΩ¢Êàê‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊéßÂà∂Êú∫Âà∂Ôºå‰∏é‰º†ÁªüÁöÑÂ§öÊ≠•ÊèêÁ§∫ÊñπÊ≥ïÁõ∏ÊØîÔºåÊèê‰æõ‰∫ÜÊõ¥È´òÁöÑÁÅµÊ¥ªÊÄßÂíåÂèØÊéßÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆæËÆ°ËøáÁ®ã‰∏≠ÔºåSHERPAÂÖ≥Ê≥®Áä∂ÊÄÅÊú∫ÁöÑÈÖçÁΩÆ„ÄÅËΩ¨Êç¢ËßÑÂàôÁöÑËÆæËÆ°‰ª•ÂèäÂ¶Ç‰ΩïÊúâÊïàÂú∞Â∞ÜÊú∫Âô®Â≠¶‰π†Ê®°ÂûãÂµåÂÖ•Áä∂ÊÄÅÊú∫‰∏≠Ôºå‰ª•ÂÆûÁé∞ÂØπLLMË°å‰∏∫ÁöÑÁ≤æÁªÜÊéßÂà∂„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåSHERPAÂú®Â§ö‰∏™‰ªªÂä°‰∏äÊòæËëóÊèêÂçá‰∫ÜLLMÁöÑËæìÂá∫Ë¥®ÈáèÔºå‰∏éÊó†Áä∂ÊÄÅÊú∫ÁöÑÂü∫Á∫øÊñπÊ≥ïÁõ∏ÊØîÔºåÊÄßËÉΩÊèêÂçáÂπÖÂ∫¶ËææÂà∞20%‰ª•‰∏äÔºåÂ∞§ÂÖ∂Âú®Â§çÊùÇ‰ªªÂä°‰∏≠Ë°®Áé∞Â∞§‰∏∫Á™ÅÂá∫„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

SHERPAÊ°ÜÊû∂ÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõÔºåÂ∞§ÂÖ∂Âú®ÈúÄË¶ÅÈ¢ÜÂüüÁâπÂÆöÁü•ËØÜÁöÑÂ§çÊùÇ‰ªªÂä°‰∏≠ÔºåÂ¶ÇËΩØ‰ª∂ÂºÄÂèë‰∏≠ÁöÑ‰ª£Á†ÅÁîüÊàê„ÄÅÂëΩÂêçÁîüÊàêÂíåÊô∫ËÉΩÈóÆÁ≠îÁ≥ªÁªüÁ≠â„ÄÇÈÄöËøáÊèêÂçáLLMÁöÑÊâßË°åËÉΩÂäõÔºåSHERPAËÉΩÂ§ü‰∏∫Áõ∏ÂÖ≥Ë°å‰∏öÊèê‰æõÊõ¥È´òÊïàÁöÑËß£ÂÜ≥ÊñπÊ°àÔºåÊé®Âä®Êô∫ËÉΩÁ≥ªÁªüÁöÑËøõ‰∏ÄÊ≠•ÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Recently, large language models (LLMs) have achieved widespread application across various fields. Despite their impressive capabilities, LLMs suffer from a lack of structured reasoning ability, particularly for complex tasks requiring domain-specific best practices, which are often unavailable in the training data. Although multi-step prompting methods incorporating human best practices, such as chain-of-thought and tree-of-thought, have gained popularity, they lack a general mechanism to control LLM behavior. In this paper, we propose SHERPA, a model-driven framework to improve the LLM performance on complex tasks by explicitly incorporating domain-specific best practices into hierarchical state machines. By structuring the LLM execution processes using state machines, SHERPA enables more fine-grained control over their behavior via rules or decisions driven by machine learning-based approaches, including LLMs. We show that SHERPA is applicable to a wide variety of tasks-specifically, code generation, class name generation, and question answering-replicating previously proposed approaches while further improving the performance. We demonstrate the effectiveness of SHERPA for the aforementioned tasks using various LLMs. Our systematic evaluation compares different state machine configurations against baseline approaches without state machines. Results show that integrating well-designed state machines significantly improves the quality of LLM outputs, and is particularly beneficial for complex tasks with well-established human best practices but lacking data used for training LLMs.

