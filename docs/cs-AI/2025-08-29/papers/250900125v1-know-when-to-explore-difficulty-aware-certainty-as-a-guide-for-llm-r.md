---
layout: default
title: Know When to Explore: Difficulty-Aware Certainty as a Guide for LLM Reinforcement Learning
---

# Know When to Explore: Difficulty-Aware Certainty as a Guide for LLM Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.00125" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.00125v1</a>
  <a href="https://arxiv.org/pdf/2509.00125.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.00125v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.00125v1', 'Know When to Explore: Difficulty-Aware Certainty as a Guide for LLM Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ang Li, Zhihang Yuan, Yang Zhang, Shouda Liu, Yisen Wang

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-29

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå›°éš¾æ„ŸçŸ¥è‡ªä¿¡å¼•å¯¼æ¢ç´¢ä»¥ä¼˜åŒ–LLMå¼ºåŒ–å­¦ä¹ **

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨ç†èƒ½åŠ›` `å›°éš¾æ„ŸçŸ¥` `è‡ªä¿¡å¼•å¯¼` `æ¢ç´¢ä¸åˆ©ç”¨` `æ•°å­¦æ¨ç†` `åŠ¨æ€è°ƒæ•´`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¾èµ–ç¨€ç–çš„å¥–åŠ±ä¿¡å·ï¼Œæ— æ³•æœ‰æ•ˆæŒ‡å¯¼æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ï¼Œå¯¼è‡´å­¦ä¹ æ•ˆç‡ä½ä¸‹ã€‚
2. æœ¬æ–‡æå‡ºDACEç®—æ³•ï¼Œé€šè¿‡åœ¨çº¿è¯„ä¼°ä»»åŠ¡éš¾åº¦ï¼ŒåŠ¨æ€è°ƒæ•´æ¢ç´¢ä¸åˆ©ç”¨çš„å¹³è¡¡ï¼Œä¼˜åŒ–å­¦ä¹ è¿‡ç¨‹ã€‚
3. åœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒDACEæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºäºå¯éªŒè¯åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVFï¼‰å·²æˆä¸ºå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¨ç†èƒ½åŠ›çš„å…³é”®æŠ€æœ¯ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¾èµ–ç¨€ç–çš„åŸºäºç»“æœçš„å¥–åŠ±ï¼Œæ— æ³•æä¾›å¯¹æ¨ç†è¿‡ç¨‹çš„ç»†ç²’åº¦æŒ‡å¯¼ï¼Œé™åˆ¶äº†å­¦ä¹ æ•ˆç‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†å›°éš¾æ„ŸçŸ¥è‡ªä¿¡å¼•å¯¼æ¢ç´¢ï¼ˆDACEï¼‰ï¼Œé€šè¿‡åœ¨çº¿è¯„ä¼°ä»»åŠ¡éš¾åº¦æ¥åŠ¨æ€å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨çš„æƒè¡¡ã€‚DACEåœ¨å›°éš¾ä»»åŠ¡ä¸­æƒ©ç½šé«˜è‡ªä¿¡ä»¥é¼“åŠ±æ¢ç´¢ï¼Œè€Œåœ¨ç®€å•ä»»åŠ¡ä¸­åˆ™å¥–åŠ±é«˜è‡ªä¿¡ä»¥æé«˜å­¦ä¹ æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDACEåœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºå¼ºåŸºçº¿ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨æœ‰æ•ˆæ¢ç´¢ä¸ç²¾åº¦ä¹‹é—´çš„è‰¯å¥½å¹³è¡¡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„æŒ‡å¯¼ä¸è¶³ï¼Œå¯¼è‡´æ¨¡å‹æ— æ³•æœ‰æ•ˆåŒºåˆ†é«˜è´¨é‡ä¸ä½æ•ˆç‡çš„è§£å†³æ–¹æ¡ˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDACEç®—æ³•åˆ©ç”¨LLMçš„è‡ªä¿¡åº¦ä¸ä»»åŠ¡éš¾åº¦ä¹‹é—´çš„å…³è”ï¼ŒåŠ¨æ€è°ƒæ•´æ¢ç´¢ä¸åˆ©ç”¨çš„æƒè¡¡ï¼Œä»¥æé«˜å­¦ä¹ æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDACEçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä»»åŠ¡éš¾åº¦è¯„ä¼°æ¨¡å—å’Œå†…åœ¨å¥–åŠ±è°ƒèŠ‚æ¨¡å—ã€‚å‰è€…æ ¹æ®ç­–ç•¥çš„æˆåŠŸç‡åœ¨çº¿è¯„ä¼°ä»»åŠ¡éš¾åº¦ï¼Œåè€…æ ¹æ®éš¾åº¦è°ƒæ•´å¥–åŠ±ä¿¡å·ã€‚

**å…³é”®åˆ›æ–°**ï¼šDACEçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå¼•å…¥äº†å›°éš¾æ„ŸçŸ¥è‡ªä¿¡çš„æ¦‚å¿µï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´å¥–åŠ±æœºåˆ¶æ¥ä¼˜åŒ–æ¢ç´¢è¿‡ç¨‹ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å¼•å¯¼æ¨¡å‹å­¦ä¹ ã€‚

**å…³é”®è®¾è®¡**ï¼šDACEè®¾è®¡äº†ç‰¹å®šçš„å¥–åŠ±å‡½æ•°ï¼Œåœ¨å›°éš¾ä»»åŠ¡ä¸­æƒ©ç½šé«˜è‡ªä¿¡ï¼Œåœ¨ç®€å•ä»»åŠ¡ä¸­å¥–åŠ±é«˜è‡ªä¿¡ï¼Œä»è€Œå®ç°äº†å¯¹å­¦ä¹ è¿‡ç¨‹çš„æœ‰æ•ˆè°ƒæ§ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨å®éªŒéƒ¨åˆ†è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ï¼ˆAIME, MATHï¼‰ä¸­ï¼ŒDACEæ˜¾è‘—è¶…è¶Šäº†å¤šä¸ªå¼ºåŸºçº¿ï¼Œæ¨¡å‹å‡†ç¡®æ€§æå‡å¹…åº¦è¾¾åˆ°XX%ï¼Œå¹¶åœ¨è®¡ç®—èµ„æºæ‰©å±•æ—¶è¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ï¼ŒéªŒè¯äº†å…¶åœ¨æ¢ç´¢ä¸ç²¾åº¦ä¹‹é—´çš„ä¼˜è‰¯å¹³è¡¡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€é‡‘èå’Œç§‘å­¦ç ”ç©¶ç­‰éœ€è¦é«˜æ•ˆæ¨ç†çš„åœºæ™¯ã€‚é€šè¿‡ä¼˜åŒ–LLMçš„å­¦ä¹ è¿‡ç¨‹ï¼ŒDACEèƒ½å¤Ÿåœ¨å„ç§å¤æ‚ä»»åŠ¡ä¸­æä¾›æ›´é«˜çš„å‡†ç¡®æ€§å’Œå¯é æ€§ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨æ™ºèƒ½åŠ©æ‰‹å’Œè‡ªåŠ¨åŒ–å†³ç­–ç³»ç»Ÿçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement Learning with Verifiable Feedback (RLVF) has become a key technique for enhancing the reasoning abilities of Large Language Models (LLMs). However, its reliance on sparse, outcome based rewards, which only indicate if a final answer is correct or not, fails to provide granular guidance on the reasoning process itself. This limitation hinders efficient learning, as the model cannot distinguish between high quality and inefficient solutions, nor can it learn effectively from different types of failures. To address this, we observe that an LLMs self-certainty often correlates with task difficulty and solution quality. We introduce Difficulty Aware Certainty guided Exploration (DACE), a novel RL algorithm that leverages this insight to dynamically balance the exploration exploitation trade-off. DACE assesses task difficulty online based on the policys success rate. It then uses this signal to modulate an intrinsic reward: for difficult tasks where the model is struggling, DACE encourages exploration by penalizing high certainty; for easier tasks, it encourages learning efficiency by rewarding high certainty. Experiments on challenging mathematical reasoning benchmarks (AIME, MATH) show that DACE significantly outperforms strong baselines. The DACE-trained models not only achieve higher accuracy but also demonstrate more robust performance when scaling test-time compute, validating that our adaptive approach fosters effective exploration without sacrificing precision.

