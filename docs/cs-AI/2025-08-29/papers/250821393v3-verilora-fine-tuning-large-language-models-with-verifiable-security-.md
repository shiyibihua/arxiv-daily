---
layout: default
title: VeriLoRA: Fine-Tuning Large Language Models with Verifiable Security via Zero-Knowledge Proofs
---

# VeriLoRA: Fine-Tuning Large Language Models with Verifiable Security via Zero-Knowledge Proofs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.21393" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.21393v3</a>
  <a href="https://arxiv.org/pdf/2508.21393.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.21393v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.21393v3', 'VeriLoRA: Fine-Tuning Large Language Models with Verifiable Security via Zero-Knowledge Proofs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Guofu Liao, Taotao Wang, Shengli Zhang, Jiqun Zhang, Shi Long, Dacheng Tao

**åˆ†ç±»**: cs.CR, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-29 (æ›´æ–°: 2025-12-02)

**å¤‡æ³¨**: This paper has been accepted for publication at the Network and Distributed System Security Symposium (NDSS) 2026

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVeriLoRAä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹å®‰å…¨æ€§ä¸å¯éªŒè¯æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `å¾®è°ƒ` `é›¶çŸ¥è¯†è¯æ˜` `å®‰å…¨æ€§` `å¯éªŒè¯æ€§` `å¯†ç å­¦æŠ€æœ¯` `ä½ç§©é€‚åº”` `Transformeræ¶æ„`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¾®è°ƒæ–¹æ³•åœ¨è®¡ç®—èµ„æºå’Œéšç§ä¿æŠ¤æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨ä¸å¯ä¿¡ç¯å¢ƒä¸­ã€‚
2. VeriLoRAæ¡†æ¶å°†LoRAå¾®è°ƒä¸é›¶çŸ¥è¯†è¯æ˜ç»“åˆï¼Œç¡®ä¿å¾®è°ƒè¿‡ç¨‹çš„å®‰å…¨æ€§å’Œå¯éªŒè¯æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒVeriLoRAåœ¨å¼€æºLLMsä¸Šå®ç°äº†é«˜æ•ˆçš„å¾®è°ƒï¼Œæ”¯æŒå¤§è§„æ¨¡æ¨¡å‹çš„å®‰å…¨éƒ¨ç½²ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œå¾®è°ƒå¯¹äºé€‚åº”ç‰¹å®šä»»åŠ¡è‡³å…³é‡è¦ï¼Œä½†åœ¨è®¡ç®—èµ„æºå’Œéšç§å®‰å…¨æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚å°½ç®¡ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰ç­‰å‚æ•°é«˜æ•ˆæ–¹æ³•æ˜¾è‘—é™ä½äº†èµ„æºéœ€æ±‚ï¼Œä½†åœ¨é›¶çŸ¥è¯†çº¦æŸä¸‹ç¡®ä¿å¾®è°ƒçš„å®‰å…¨æ€§å’Œå¯éªŒè¯æ€§ä»ç„¶æ˜¯ä¸€ä¸ªæœªè§£å†³çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†VeriLoRAï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå°†LoRAå¾®è°ƒä¸é›¶çŸ¥è¯†è¯æ˜ï¼ˆZKPsï¼‰ç›¸ç»“åˆçš„æ¡†æ¶ï¼Œå®ç°äº†å¯è¯æ˜çš„å®‰å…¨æ€§å’Œæ­£ç¡®æ€§ã€‚VeriLoRAåˆ©ç”¨å…ˆè¿›çš„å¯†ç å­¦æŠ€æœ¯ï¼Œå¦‚æŸ¥æ‰¾è®ºè¯ã€å’Œæ£€æŸ¥åè®®å’Œå¤šé¡¹å¼æ‰¿è¯ºï¼Œæ¥éªŒè¯åŸºäºTransformeræ¶æ„çš„ç®—æœ¯å’Œéç®—æœ¯æ“ä½œã€‚è¯¥æ¡†æ¶ä¸ºLoRAå¾®è°ƒè¿‡ç¨‹ä¸­çš„å‰å‘ä¼ æ’­ã€åå‘ä¼ æ’­å’Œå‚æ•°æ›´æ–°æä¾›äº†ç«¯åˆ°ç«¯çš„å¯éªŒè¯æ€§ï¼ŒåŒæ—¶ä¿æŠ¤æ¨¡å‹å‚æ•°å’Œè®­ç»ƒæ•°æ®çš„éšç§ã€‚é€šè¿‡GPUå®ç°ï¼ŒVeriLoRAåœ¨å¼€æºLLMsï¼ˆå¦‚LLaMAï¼‰ä¸Šçš„å®éªŒéªŒè¯æ˜¾ç¤ºäº†å…¶å®ç”¨æ€§å’Œæ•ˆç‡ï¼Œæ”¯æŒé«˜è¾¾130äº¿å‚æ•°çš„æ¨¡å‹ã€‚ç»“åˆå‚æ•°é«˜æ•ˆçš„å¾®è°ƒä¸ZKPsï¼ŒVeriLoRAå¡«è¡¥äº†åœ¨æ•æ„Ÿæˆ–ä¸å¯ä¿¡ç¯å¢ƒä¸­å®‰å…¨å¯ä¿¡éƒ¨ç½²LLMsçš„å…³é”®ç©ºç™½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³åœ¨ä¸å¯ä¿¡ç¯å¢ƒä¸­å¯¹å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒæ—¶çš„å®‰å…¨æ€§å’Œå¯éªŒè¯æ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨éšç§ä¿æŠ¤å’Œè®¡ç®—èµ„æºéœ€æ±‚ä¸Šå­˜åœ¨æ˜¾è‘—ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šVeriLoRAé€šè¿‡ç»“åˆLoRAå¾®è°ƒä¸é›¶çŸ¥è¯†è¯æ˜ï¼Œç¡®ä¿å¾®è°ƒè¿‡ç¨‹çš„å®‰å…¨æ€§å’Œæ­£ç¡®æ€§ï¼Œåˆ©ç”¨å¯†ç å­¦æŠ€æœ¯å®ç°å¯éªŒè¯æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVeriLoRAçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å‰å‘ä¼ æ’­ã€åå‘ä¼ æ’­å’Œå‚æ•°æ›´æ–°ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼Œé‡‡ç”¨é›¶çŸ¥è¯†è¯æ˜æŠ€æœ¯å¯¹æ¯ä¸ªæ¨¡å—è¿›è¡ŒéªŒè¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ¡†æ¶çš„æœ€å¤§åˆ›æ–°åœ¨äºå°†LoRAå¾®è°ƒä¸é›¶çŸ¥è¯†è¯æ˜ç›¸ç»“åˆï¼Œæä¾›äº†ç«¯åˆ°ç«¯çš„å¯éªŒè¯æ€§ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•æ— æ³•åœ¨ä¸å¯ä¿¡ç¯å¢ƒä¸­å®‰å…¨å¾®è°ƒçš„é—®é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼ŒVeriLoRAä½¿ç”¨æŸ¥æ‰¾è®ºè¯ã€å’Œæ£€æŸ¥åè®®å’Œå¤šé¡¹å¼æ‰¿è¯ºç­‰å¯†ç å­¦æŠ€æœ¯ï¼Œç¡®ä¿ç®—æœ¯å’Œéç®—æœ¯æ“ä½œçš„æ­£ç¡®æ€§ï¼ŒåŒæ—¶ä¿æŠ¤æ¨¡å‹å‚æ•°å’Œè®­ç»ƒæ•°æ®çš„éšç§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVeriLoRAåœ¨å¼€æºLLMsï¼ˆå¦‚LLaMAï¼‰ä¸Šå®ç°äº†é«˜è¾¾130äº¿å‚æ•°çš„å¾®è°ƒï¼Œå±•ç°å‡ºè‰¯å¥½çš„å®ç”¨æ€§å’Œæ•ˆç‡ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒVeriLoRAåœ¨å®‰å…¨æ€§å’Œå¯éªŒè¯æ€§æ–¹é¢æä¾›äº†æ˜¾è‘—æå‡ï¼Œç¡®ä¿äº†æ¨¡å‹åœ¨æ•æ„Ÿç¯å¢ƒä¸­çš„å¯é æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

VeriLoRAçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦ä¿æŠ¤æ•æ„Ÿæ•°æ®çš„åœºæ™¯ï¼Œå¦‚åŒ»ç–—ã€é‡‘èå’Œæ³•å¾‹ç­‰è¡Œä¸šã€‚é€šè¿‡ç¡®ä¿å¤§å‹è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§å’Œå¯éªŒè¯æ€§ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿä¿ƒè¿›è¿™äº›æ¨¡å‹åœ¨ä¸å¯ä¿¡ç¯å¢ƒä¸­çš„å¯ä¿¡éƒ¨ç½²ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å¹¿æ³›åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Fine-tuning large language models (LLMs) is crucial for adapting them to specific tasks, yet it remains computationally demanding and raises concerns about correctness and privacy, particularly in untrusted environments. Although parameter-efficient methods like Low-Rank Adaptation (LoRA) significantly reduce resource requirements, ensuring the security and verifiability of fine-tuning under zero-knowledge constraints remains an unresolved challenge. To address this, we introduce VeriLoRA, the first framework to integrate LoRA fine-tuning with zero-knowledge proofs (ZKPs), achieving provable security and correctness. VeriLoRA employs advanced cryptographic techniques -- such as lookup arguments, sumcheck protocols, and polynomial commitments -- to verify both arithmetic and non-arithmetic operations in Transformer-based architectures. The framework provides end-to-end verifiability for forward propagation, backward propagation, and parameter updates during LoRA fine-tuning, while safeguarding the privacy of model parameters and training data. Leveraging GPU-based implementations, VeriLoRA demonstrates practicality and efficiency through experimental validation on open-source LLMs like LLaMA, scaling up to 13 billion parameters. By combining parameter-efficient fine-tuning with ZKPs, VeriLoRA bridges a critical gap, enabling secure and trustworthy deployment of LLMs in sensitive or untrusted environments.

