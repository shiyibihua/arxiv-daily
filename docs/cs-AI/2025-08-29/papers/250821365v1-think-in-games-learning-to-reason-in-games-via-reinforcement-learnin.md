---
layout: default
title: Think in Games: Learning to Reason in Games via Reinforcement Learning with Large Language Models
---

# Think in Games: Learning to Reason in Games via Reinforcement Learning with Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.21365" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.21365v1</a>
  <a href="https://arxiv.org/pdf/2508.21365.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.21365v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.21365v1', 'Think in Games: Learning to Reason in Games via Reinforcement Learning with Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yi Liao, Yu Gu, Yuan Sui, Zining Zhu, Yifan Lu, Guohua Tang, Zhongqian Sun, Wei Yang

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-29

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºThink in Gamesæ¡†æ¶ä»¥è§£å†³è¯­è¨€æ¨¡å‹åœ¨æ¸¸æˆä¸­çš„å†³ç­–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `ç¨‹åºæ€§çŸ¥è¯†` `æ¸¸æˆç¯å¢ƒ` `å†³ç­–æ”¯æŒ` `å¯è§£é‡Šæ€§` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤„ç†ç®€å•çš„äº’åŠ¨ä»»åŠ¡æ—¶è¡¨ç°ä¸ä½³ï¼Œä¸”é€šå¸¸éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®ï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹ã€‚
2. æœ¬æ–‡æå‡ºçš„TiGæ¡†æ¶é€šè¿‡å°†RLå†³ç­–è¿‡ç¨‹è½¬åŒ–ä¸ºè¯­è¨€å»ºæ¨¡ä»»åŠ¡ï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿåœ¨æ¸¸æˆç¯å¢ƒä¸­ç›´æ¥å­¦ä¹ ç¨‹åºæ€§çŸ¥è¯†ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTiGåœ¨æ€§èƒ½ä¸Šä¸ä¼ ç»ŸRLæ–¹æ³•ç›¸å½“ï¼Œä½†åœ¨æ•°æ®å’Œè®¡ç®—éœ€æ±‚ä¸Šæ˜¾è‘—é™ä½ï¼Œä¸”æä¾›äº†å¯è§£é‡Šçš„å†³ç­–è¿‡ç¨‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨ç®€å•çš„äº’åŠ¨ä»»åŠ¡ä¸­å´å¸¸å¸¸è¡¨ç°ä¸ä½³ã€‚è¿™æ­ç¤ºäº†å£°æ˜æ€§çŸ¥è¯†ä¸ç¨‹åºæ€§çŸ¥è¯†ä¹‹é—´çš„å…³é”®å·®è·ã€‚ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä»£ç†é€šè¿‡ç¯å¢ƒäº¤äº’è·å¾—ç¨‹åºæ€§çŸ¥è¯†ï¼Œä½†é€šå¸¸éœ€è¦å¤§é‡è®­ç»ƒæ•°æ®ä¸”è¿ä½œå¦‚é»‘ç®±ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†Think in Gamesï¼ˆTiGï¼‰æ¡†æ¶ï¼Œä½¿LLMsèƒ½å¤Ÿé€šè¿‡ç›´æ¥ä¸æ¸¸æˆç¯å¢ƒäº’åŠ¨æ¥å‘å±•ç¨‹åºæ€§ç†è§£ï¼ŒåŒæ—¶ä¿ç•™å…¶æ¨ç†å’Œè§£é‡Šèƒ½åŠ›ã€‚TiGå°†åŸºäºRLçš„å†³ç­–åˆ¶å®šé‡æ–°æ„å»ºä¸ºè¯­è¨€å»ºæ¨¡ä»»åŠ¡ï¼ŒLLMsç”Ÿæˆè¯­è¨€å¼•å¯¼çš„ç­–ç•¥ï¼Œå¹¶é€šè¿‡åœ¨çº¿å¼ºåŒ–å­¦ä¹ æ ¹æ®ç¯å¢ƒåé¦ˆè¿›è¡Œè¿­ä»£ä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTiGæˆåŠŸå¼¥åˆäº†å£°æ˜æ€§ä¸ç¨‹åºæ€§çŸ¥è¯†ä¹‹é—´çš„å·®è·ï¼Œä¸”åœ¨æ•°æ®å’Œè®¡ç®—éœ€æ±‚ä¸Šæ˜¾è‘—ä½äºä¼ ç»ŸRLæ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨äº’åŠ¨ä»»åŠ¡ä¸­çš„å†³ç­–èƒ½åŠ›ä¸è¶³é—®é¢˜ï¼Œç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡æ•°æ®ä¸”ç¼ºä¹é€æ˜æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šTiGæ¡†æ¶é€šè¿‡å°†å¼ºåŒ–å­¦ä¹ çš„å†³ç­–è¿‡ç¨‹è½¬åŒ–ä¸ºè¯­è¨€å»ºæ¨¡ä»»åŠ¡ï¼Œä½¿å¾—LLMsèƒ½å¤Ÿåœ¨æ¸¸æˆç¯å¢ƒä¸­è¿›è¡Œç›´æ¥çš„ç¨‹åºæ€§å­¦ä¹ ï¼ŒåŒæ—¶ä¿ç•™å…¶æ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTiGçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šè¯­è¨€å¼•å¯¼ç­–ç•¥ç”Ÿæˆå’Œåœ¨çº¿å¼ºåŒ–å­¦ä¹ ã€‚é¦–å…ˆï¼ŒLLMsç”ŸæˆåŸºäºè¯­è¨€çš„ç­–ç•¥ï¼Œç„¶åé€šè¿‡ç¯å¢ƒåé¦ˆè¿›è¡Œè¿­ä»£ä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šTiGçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†RLå†³ç­–è¿‡ç¨‹ä¸è¯­è¨€æ¨¡å‹ç»“åˆï¼Œä½¿å¾—æ¨¡å‹ä¸ä»…èƒ½å¤Ÿè¿›è¡Œå†³ç­–ï¼Œè¿˜èƒ½æä¾›è‡ªç„¶è¯­è¨€çš„è§£é‡Šï¼Œä»è€Œæé«˜é€æ˜æ€§å’Œå¯è§£é‡Šæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒTiGé‡‡ç”¨äº†è¿­ä»£ä¼˜åŒ–çš„ç­–ç•¥ç”Ÿæˆæœºåˆ¶ï¼Œå¹¶ç»“åˆäº†ç¯å¢ƒåé¦ˆæ¥è°ƒæ•´ç­–ç•¥ï¼Œç¡®ä¿æ¨¡å‹åœ¨åŠ¨æ€ç¯å¢ƒä¸­èƒ½å¤Ÿæœ‰æ•ˆå­¦ä¹ ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒTiGåœ¨å¤šé¡¹ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¾¾åˆ°äº†ä¸ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨æ•°æ®å’Œè®¡ç®—éœ€æ±‚ä¸Šé™ä½äº†çº¦70%ã€‚æ­¤å¤–ï¼ŒTiGèƒ½å¤Ÿæä¾›é€æ­¥çš„è‡ªç„¶è¯­è¨€è§£é‡Šï¼Œæ˜¾è‘—æå‡äº†å†³ç­–è¿‡ç¨‹çš„é€æ˜æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²æ¸¸æˆã€æ™ºèƒ½åŠ©æ‰‹å’Œå¤æ‚å†³ç­–æ”¯æŒç³»ç»Ÿã€‚é€šè¿‡æé«˜è¯­è¨€æ¨¡å‹åœ¨äº’åŠ¨ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼ŒTiGå¯ä»¥åœ¨å¤šç§åœºæ™¯ä¸‹æä¾›æ›´æ™ºèƒ½çš„å†³ç­–æ”¯æŒï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨äººæœºäº¤äº’çš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) excel at complex reasoning tasks such as mathematics and coding, yet they frequently struggle with simple interactive tasks that young children perform effortlessly. This discrepancy highlights a critical gap between declarative knowledge (knowing about something) and procedural knowledge (knowing how to do something). Although traditional reinforcement learning (RL) agents can acquire procedural knowledge through environmental interaction, they often operate as black boxes and require substantial training data. In contrast, LLMs possess extensive world knowledge and reasoning capabilities, but are unable to effectively convert this static knowledge into dynamic decision-making in interactive settings. To address this challenge, we propose Think in Games (TiG), a novel framework that empowers LLMs to develop procedural understanding through direct interaction with game environments, while retaining their inherent reasoning and explanatory abilities. Specifically, TiG reformulates RL-based decision-making as a language modeling task: LLMs generate language-guided policies, which are refined iteratively through online reinforcement learning based on environmental feedback. Our experimental results show that TiG successfully bridges the gap between declarative and procedural knowledge, achieving competitive performance with dramatically lower data and computational demands compared to conventional RL methods. Moreover, TiG provides step-by-step natural language explanations for its decisions, greatly improving transparency and interpretability in complex interactive tasks.

