---
layout: default
title: MMSearch-Plus: Benchmarking Provenance-Aware Search for Multimodal Browsing Agents
---

# MMSearch-Plus: Benchmarking Provenance-Aware Search for Multimodal Browsing Agents

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.21475" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.21475v2</a>
  <a href="https://arxiv.org/pdf/2508.21475.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.21475v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.21475v2', 'MMSearch-Plus: Benchmarking Provenance-Aware Search for Multimodal Browsing Agents')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xijia Tao, Yihua Teng, Xinxing Su, Xinyu Fu, Jihao Wu, Chaofan Tao, Ziru Liu, Haoli Bai, Rui Liu, Lingpeng Kong

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-08-29 (æ›´æ–°: 2025-09-26)

**å¤‡æ³¨**: Project Page: https://mmsearch-plus.github.io

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMMSearch-Plusä»¥è§£å†³å¤šæ¨¡æ€æœç´¢ä¸­çš„æ¨ç†ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æœç´¢` `æ¨ç†èƒ½åŠ›` `è§†è§‰ä¿¡æ¯` `æ¨¡å‹æ¡†æ¶` `æ ‡è®°é›†æ¨¡å—`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ¨¡æ€æµè§ˆåŸºå‡†ç¼ºä¹çœŸæ­£çš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ï¼Œè®¸å¤šä»»åŠ¡å¯ç”¨æ–‡æœ¬å¯å‘å¼è§£å†³ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨è§†è§‰ä¿¡æ¯ã€‚
2. è®ºæ–‡æå‡ºMMSearch-PlusåŸºå‡†ï¼Œé€šè¿‡è¦æ±‚ç»†ç²’åº¦è§†è§‰çº¿ç´¢çš„æå–ä¸ä¼ æ’­ï¼Œå¼ºåŒ–å¤šæ¨¡æ€ç†è§£ï¼Œæå‡æ£€ç´¢çš„å‡†ç¡®æ€§ã€‚
3. åœ¨è¯„ä¼°ä¸­ï¼Œæœ€å¼ºç³»ç»Ÿçš„ç«¯åˆ°ç«¯å‡†ç¡®ç‡è¾¾åˆ°36.0%ï¼Œé›†æˆSoMæ¨¡å—åœ¨å¤šä¸ªè®¾ç½®ä¸­å‡æœ‰æ˜¾è‘—æå‡ï¼Œæœ€é«˜æé«˜3.9ä¸ªç™¾åˆ†ç‚¹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°æœ‰çš„å¤šæ¨¡æ€æµè§ˆåŸºå‡†å¾€å¾€æœªèƒ½çœŸæ­£è¦æ±‚å¤šæ¨¡æ€æ¨ç†ï¼Œå› ä¸ºè®¸å¤šä»»åŠ¡å¯ä»¥ä»…é€šè¿‡æ–‡æœ¬å¯å‘å¼è§£å†³ï¼Œè€Œæ— éœ€è§†è§‰éªŒè¯ã€‚æˆ‘ä»¬å¼•å…¥MMSearch-Plusï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«311ä¸ªä»»åŠ¡çš„åŸºå‡†ï¼Œå¼ºåˆ¶è¦æ±‚é€šè¿‡è¿­ä»£çš„å›¾åƒ-æ–‡æœ¬æ£€ç´¢å’Œäº¤å‰éªŒè¯åœ¨æ£€ç´¢å™ªå£°ä¸‹æå–å’Œä¼ æ’­ç»†ç²’åº¦çš„è§†è§‰çº¿ç´¢ã€‚æˆ‘ä»¬çš„ç­–åˆ’ç¨‹åºæå‡ºçš„é—®é¢˜è¦æ±‚ä»ç©ºé—´çº¿ç´¢å’Œæ—¶é—´è½¨è¿¹æ¨æ–­å‡ºå›¾åƒå¤–çš„äº‹å®ï¼Œå¦‚äº‹ä»¶ã€æ—¥æœŸå’Œåœºæ‰€ã€‚é™¤äº†æ•°æ®é›†ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†ä¸€ä¸ªä¸æ¨¡å‹æ— å…³çš„ä»£ç†æ¡†æ¶ï¼Œé…å¤‡æ ‡å‡†æµè§ˆå·¥å…·å’Œä¸€ä¸ªæ ‡è®°é›†æ¨¡å—ï¼ˆSoMï¼‰ï¼Œä½¿ä»£ç†èƒ½å¤Ÿæ”¾ç½®æ ‡è®°ã€è£å‰ªå­åŒºåŸŸå¹¶å‘èµ·é’ˆå¯¹æ€§çš„å›¾åƒ/æ–‡æœ¬æœç´¢ã€‚SoMå®ç°äº†åŸºäºæ¥æºçš„ç¼©æ”¾å’Œæ£€ç´¢ï¼Œå¹¶æé«˜äº†å¤šæ­¥éª¤æ¨ç†çš„é²æ£’æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡è¦è§£å†³çš„é—®é¢˜æ˜¯ç°æœ‰å¤šæ¨¡æ€æœç´¢åŸºå‡†æœªèƒ½æœ‰æ•ˆåˆ©ç”¨è§†è§‰ä¿¡æ¯ï¼Œå¯¼è‡´æ¨ç†èƒ½åŠ›ä¸è¶³ï¼Œæ— æ³•åº”å¯¹çœŸå®ä¸–ç•Œçš„å¤æ‚æ£€ç´¢ä»»åŠ¡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡MMSearch-PlusåŸºå‡†ï¼Œè¦æ±‚åœ¨æ£€ç´¢è¿‡ç¨‹ä¸­æå–å’Œä¼ æ’­ç»†ç²’åº¦çš„è§†è§‰çº¿ç´¢ï¼Œä»è€Œå¢å¼ºå¤šæ¨¡æ€ç†è§£èƒ½åŠ›ã€‚è®¾è®¡ä¸Šå¼ºè°ƒäº†å›¾åƒä¸æ–‡æœ¬çš„äº¤äº’ä½œç”¨ï¼Œç¡®ä¿æ¨ç†è¿‡ç¨‹çš„å…¨é¢æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†æ„å»ºã€æ¨¡å‹æ¡†æ¶å’ŒSoMæ¨¡å—ã€‚æ•°æ®é›†åŒ…å«311ä¸ªä»»åŠ¡ï¼Œæ¨¡å‹æ¡†æ¶æ”¯æŒå¤šç§MLLMï¼ŒSoMæ¨¡å—ç”¨äºæ ‡è®°ã€è£å‰ªå’Œæ£€ç´¢ï¼Œå½¢æˆé—­ç¯åé¦ˆæœºåˆ¶ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥äº†SoMæ¨¡å—ï¼Œä½¿å¾—ä»£ç†èƒ½å¤Ÿè¿›è¡ŒåŸºäºæ¥æºçš„ç¼©æ”¾å’Œæ£€ç´¢ï¼Œæ˜¾è‘—æé«˜äº†å¤šæ­¥éª¤æ¨ç†çš„é²æ£’æ€§ã€‚è¿™ä¸€è®¾è®¡ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œå¢å¼ºäº†å¯¹è§†è§‰ä¿¡æ¯çš„åˆ©ç”¨ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬SoMæ¨¡å—çš„å®ç°ï¼Œæ”¯æŒæ ‡è®°å’Œè£å‰ªåŠŸèƒ½ï¼ŒæŸå¤±å‡½æ•°çš„é€‰æ‹©ç¡®ä¿äº†æ¨¡å‹åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œç½‘ç»œç»“æ„åˆ™é‡‡ç”¨äº†é€‚åº”æ€§å¼ºçš„æ¶æ„ä»¥æ”¯æŒä¸åŒç±»å‹çš„è¾“å…¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæœ€å¼ºç³»ç»Ÿåœ¨MMSearch-PlusåŸºå‡†ä¸Šçš„ç«¯åˆ°ç«¯å‡†ç¡®ç‡è¾¾åˆ°36.0%ï¼Œé›†æˆSoMæ¨¡å—ååœ¨å¤šä¸ªè®¾ç½®ä¸­å‡æœ‰æ˜¾è‘—æå‡ï¼Œæœ€é«˜æé«˜3.9ä¸ªç™¾åˆ†ç‚¹ã€‚è¿™è¡¨æ˜SoMæ¨¡å—åœ¨å¤šæ­¥éª¤æ¨ç†ä¸­çš„æœ‰æ•ˆæ€§å’Œé‡è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½æœç´¢å¼•æ“ã€è™šæ‹ŸåŠ©æ‰‹å’Œå¤šæ¨¡æ€ä¿¡æ¯æ£€ç´¢ç³»ç»Ÿã€‚é€šè¿‡æå‡å¤šæ¨¡æ€ç†è§£èƒ½åŠ›ï¼ŒMMSearch-Pluså¯ä¸ºç”¨æˆ·æä¾›æ›´å‡†ç¡®çš„æœç´¢ç»“æœï¼Œæ”¹å–„äººæœºäº¤äº’ä½“éªŒï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨ç›¸å…³æŠ€æœ¯åœ¨æ•™è‚²ã€åŒ»ç–—å’Œå¨±ä¹ç­‰é¢†åŸŸçš„å¹¿æ³›åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Existing multimodal browsing benchmarks often fail to require genuine multimodal reasoning, as many tasks can be solved with text-only heuristics without vision-in-the-loop verification. We introduce MMSearch-Plus, a 311-task benchmark that enforces multimodal understanding by requiring extraction and propagation of fine-grained visual cues through iterative image-text retrieval and cross-validation under retrieval noise. Our curation procedure seeds questions whose answers require extrapolating from spatial cues and temporal traces to out-of-image facts such as events, dates, and venues. Beyond the dataset, we provide a model-agnostic agent framework with standard browsing tools and a set-of-mark (SoM) module, which lets the agent place marks, crop subregions, and launch targeted image/text searches. SoM enables provenance-aware zoom-and-retrieve and improves robustness in multi-step reasoning. We evaluated closed- and open-source MLLMs in this framework. The strongest system achieves an end-to-end accuracy of 36.0%, and integrating SoM produces consistent gains in multiple settings, with improvements up to +3.9 points. From failure analysis, we observe recurring errors in locating relevant webpages and distinguishing between visually similar events. These results underscore the challenges of real-world multimodal search and establish MMSearch-Plus as a rigorous benchmark for advancing agentic MLLMs.

