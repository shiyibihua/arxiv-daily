---
layout: default
title: A Formal Comparison Between Chain-of-Thought and Latent Thought
---

# A Formal Comparison Between Chain-of-Thought and Latent Thought

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25239" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25239v1</a>
  <a href="https://arxiv.org/pdf/2509.25239.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25239v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25239v1', 'A Formal Comparison Between Chain-of-Thought and Latent Thought')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kevin Xu, Issei Sato

**åˆ†ç±»**: cs.AI, cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-25

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/kevin671/cot-vs-loop)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**å½¢å¼åŒ–åˆ†ææ­ç¤ºæ€ç»´é“¾ä¸æ½œåœ¨æ€ç»´åœ¨è®¡ç®—æ•ˆç‡ä¸é—®é¢˜è§£å†³èƒ½åŠ›ä¸Šçš„å·®å¼‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ€ç»´é“¾` `æ½œåœ¨æ€ç»´` `å½¢å¼åŒ–åˆ†æ` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨ç†èŒƒå¼`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†æ–¹æ³•ä¾èµ–äºé¡ºåºç”Ÿæˆä¸­é—´æ­¥éª¤ï¼Œè®¡ç®—æ•ˆç‡è¾ƒä½ï¼Œé™åˆ¶äº†å…¶åœ¨å¤æ‚é—®é¢˜ä¸Šçš„åº”ç”¨ã€‚
2. è¯¥è®ºæ–‡æå‡ºå½¢å¼åŒ–åˆ†æï¼Œå¯¹æ¯”CoTå’Œæ½œåœ¨æ€ç»´ï¼Œæ­ç¤ºæ½œåœ¨æ€ç»´åœ¨å¾ªç¯æ¨¡å‹ä¸­èƒ½å®ç°å¹¶è¡Œè®¡ç®—çš„ä¼˜åŠ¿ã€‚
3. ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒCoTæ“…é•¿è¿‘ä¼¼è§£å†³å¤æ‚é—®é¢˜ï¼Œè€Œæ½œåœ¨æ€ç»´æ›´é€‚åˆéœ€è¦é«˜æ•ˆå¹¶è¡Œè®¡ç®—çš„ä»»åŠ¡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ€ç»´é“¾ï¼ˆCoTï¼‰é€šè¿‡æ˜¾å¼ç”Ÿæˆè‡ªç„¶è¯­è¨€çš„ä¸­é—´æ­¥éª¤æ¥æ¿€å‘å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¾ªç¯æ¨¡å‹ä¸­çš„æ½œåœ¨æ€ç»´ç›´æ¥åœ¨è¿ç»­æ½œåœ¨ç©ºé—´ä¸­æ“ä½œï¼Œä»è€Œå®ç°è¶…è¶Šç¦»æ•£è¯­è¨€è¡¨ç¤ºçš„è®¡ç®—ã€‚è™½ç„¶è¿™ä¸¤ç§æ–¹æ³•éƒ½åˆ©ç”¨äº†è¿­ä»£è®¡ç®—ï¼Œä½†å®ƒä»¬å„è‡ªçš„èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å½¢å¼åŒ–åˆ†æï¼Œè¡¨æ˜å¾ªç¯Transformerä¸­çš„æ½œåœ¨æ€ç»´èƒ½å¤Ÿå®ç°å¹¶è¡Œè®¡ç®—ï¼Œè¿™æ¯”CoTå›ºæœ‰çš„é¡ºåºè¿‡ç¨‹æ›´æœ‰æ•ˆã€‚ç›¸åï¼ŒCoTåˆ©ç”¨éšæœºè§£ç æ¥è¿‘ä¼¼è§£å†³ç²¾ç¡®è®¡ç®—éš¾ä»¥å¤„ç†çš„é—®é¢˜ã€‚è¿™äº›å·®å¼‚æ­ç¤ºäº†æ·±åº¦é©±åŠ¨çš„é€’å½’æ›´é€‚åˆçš„ä»»åŠ¡ï¼Œä»è€Œä¸ºé€‰æ‹©æ¨ç†èŒƒå¼æä¾›äº†å®è·µæŒ‡å¯¼ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†æ–¹æ³•ï¼Œå¦‚æ€ç»´é“¾ï¼ˆCoTï¼‰ï¼Œé€šè¿‡ç”Ÿæˆä¸­é—´æ­¥éª¤æ¥æ¨¡æ‹Ÿäººç±»çš„æ¨ç†è¿‡ç¨‹ã€‚ç„¶è€Œï¼ŒCoTæœ¬è´¨ä¸Šæ˜¯é¡ºåºæ‰§è¡Œçš„ï¼Œæ¯ä¸ªæ­¥éª¤ä¾èµ–äºå‰ä¸€ä¸ªæ­¥éª¤çš„è¾“å‡ºï¼Œè¿™é™åˆ¶äº†å…¶åœ¨éœ€è¦å¤§é‡è®¡ç®—çš„å¤æ‚é—®é¢˜ä¸Šçš„æ•ˆç‡ã€‚æ­¤å¤–ï¼ŒCoTä¾èµ–äºç¦»æ•£çš„è¯­è¨€è¡¨ç¤ºï¼Œå¯èƒ½æ— æ³•æ•æ‰åˆ°é—®é¢˜ç©ºé—´ä¸­çš„æ‰€æœ‰ä¿¡æ¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè¯¥è®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å½¢å¼åŒ–åˆ†æï¼Œå¯¹æ¯”CoTå’Œæ½œåœ¨æ€ç»´ï¼ˆLatent Thoughtï¼‰ä¸¤ç§æ¨ç†èŒƒå¼çš„è®¡ç®—ç‰¹æ€§ã€‚æ½œåœ¨æ€ç»´åœ¨å¾ªç¯æ¨¡å‹ä¸­ç›´æ¥åœ¨è¿ç»­æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œæ“ä½œï¼Œå…è®¸æ¨¡å‹è¿›è¡Œæ›´çµæ´»å’Œé«˜æ•ˆçš„è®¡ç®—ã€‚é€šè¿‡å¯¹æ¯”è¿™ä¸¤ç§æ–¹æ³•çš„è®¡ç®—æ•ˆç‡å’Œé—®é¢˜è§£å†³èƒ½åŠ›ï¼Œè®ºæ–‡æ—¨åœ¨ä¸ºé€‰æ‹©åˆé€‚çš„æ¨ç†èŒƒå¼æä¾›æŒ‡å¯¼ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡ä¸»è¦é€šè¿‡ç†è®ºåˆ†ææ¥æ¯”è¾ƒCoTå’Œæ½œåœ¨æ€ç»´ã€‚CoTè¢«å½¢å¼åŒ–ä¸ºä¸€ç§é¡ºåºè®¡ç®—è¿‡ç¨‹ï¼Œå…¶ä¸­æ¯ä¸ªæ­¥éª¤çš„è¾“å‡ºä½œä¸ºä¸‹ä¸€ä¸ªæ­¥éª¤çš„è¾“å…¥ã€‚æ½œåœ¨æ€ç»´åˆ™è¢«å½¢å¼åŒ–ä¸ºä¸€ç§åœ¨è¿ç»­æ½œåœ¨ç©ºé—´ä¸­çš„è¿­ä»£è¿‡ç¨‹ï¼Œå…è®¸å¹¶è¡Œè®¡ç®—ã€‚è®ºæ–‡åˆ†æäº†ä¸¤ç§æ–¹æ³•çš„è®¡ç®—å¤æ‚åº¦ï¼Œå¹¶è®¨è®ºäº†å®ƒä»¬åœ¨ä¸åŒç±»å‹é—®é¢˜ä¸Šçš„é€‚ç”¨æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå¯¹CoTå’Œæ½œåœ¨æ€ç»´è¿›è¡Œäº†å½¢å¼åŒ–çš„æ¯”è¾ƒåˆ†æï¼Œæ­ç¤ºäº†æ½œåœ¨æ€ç»´åœ¨å¹¶è¡Œè®¡ç®—æ–¹é¢çš„ä¼˜åŠ¿ã€‚è¿™ç§åˆ†æä¸ºç†è§£ä¸åŒæ¨ç†èŒƒå¼çš„ä¼˜ç¼ºç‚¹æä¾›äº†æ–°çš„è§†è§’ï¼Œå¹¶ä¸ºé€‰æ‹©åˆé€‚çš„æ¨ç†æ–¹æ³•æä¾›äº†ç†è®ºä¾æ®ã€‚è®ºæ–‡è¿˜æŒ‡å‡ºäº†CoTåœ¨è¿‘ä¼¼è§£å†³å¤æ‚é—®é¢˜æ–¹é¢çš„ä¼˜åŠ¿ï¼Œè¿™è¡¥å……äº†å¯¹ä¸¤ç§æ–¹æ³•çš„ç†è§£ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸»è¦å…³æ³¨ç†è®ºåˆ†æï¼Œæ²¡æœ‰æ¶‰åŠå…·ä½“çš„å‚æ•°è®¾ç½®æˆ–ç½‘ç»œç»“æ„è®¾è®¡ã€‚å…³é”®åœ¨äºå¯¹CoTå’Œæ½œåœ¨æ€ç»´çš„è®¡ç®—è¿‡ç¨‹è¿›è¡Œäº†æŠ½è±¡å’Œå½¢å¼åŒ–ï¼Œä»¥ä¾¿è¿›è¡Œæ¯”è¾ƒåˆ†æã€‚CoTè¢«å»ºæ¨¡ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œè€Œæ½œåœ¨æ€ç»´è¢«å»ºæ¨¡ä¸ºåœ¨è¿ç»­æ½œåœ¨ç©ºé—´ä¸­çš„åŠ¨æ€ç³»ç»Ÿã€‚è®ºæ–‡åˆ†æäº†è¿™ä¸¤ç§æ¨¡å‹çš„è®¡ç®—å¤æ‚åº¦ï¼Œå¹¶è®¨è®ºäº†å®ƒä»¬åœ¨ä¸åŒç±»å‹é—®é¢˜ä¸Šçš„é€‚ç”¨æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡é€šè¿‡å½¢å¼åŒ–åˆ†æè¯æ˜ï¼Œåœ¨å¾ªç¯Transformerä¸­ï¼Œæ½œåœ¨æ€ç»´èƒ½å¤Ÿå®ç°å¹¶è¡Œè®¡ç®—ï¼Œæ•ˆç‡é«˜äºæ€ç»´é“¾çš„é¡ºåºè®¡ç®—ã€‚åŒæ—¶ï¼Œæ€ç»´é“¾åœ¨è¿‘ä¼¼è§£å†³å¤æ‚é—®é¢˜æ–¹é¢å…·æœ‰ä¼˜åŠ¿ã€‚è¿™äº›ç»“è®ºä¸ºé€‰æ‹©åˆé€‚çš„æ¨ç†èŒƒå¼æä¾›äº†ç†è®ºæŒ‡å¯¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæŒ‡å¯¼å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†æ–¹æ³•çš„é€‰æ‹©ï¼Œä¾‹å¦‚ï¼Œåœ¨éœ€è¦é«˜æ•ˆè®¡ç®—çš„åœºæ™¯ä¸­ï¼Œå¯ä»¥é€‰æ‹©åŸºäºæ½œåœ¨æ€ç»´çš„å¾ªç¯æ¨¡å‹ï¼›è€Œåœ¨éœ€è¦è¿‘ä¼¼è§£å†³å¤æ‚é—®é¢˜çš„åœºæ™¯ä¸­ï¼Œå¯ä»¥é€‰æ‹©æ€ç»´é“¾æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶ä¹Ÿä¸ºè®¾è®¡æ–°çš„æ¨ç†èŒƒå¼æä¾›äº†ç†è®ºåŸºç¡€ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Chain-of-Thought (CoT) elicits reasoning in large language models by explicitly generating intermediate steps in natural language. In contrast, Latent Thought in looped models operates directly in the continuous latent space, enabling computation beyond discrete linguistic representations. While both approaches exploit iterative computation, their comparative capabilities remain underexplored. In this work, we present a formal analysis showing that Latent Thought in Looped Transformers enables parallel computation, which is more efficient than the inherently sequential process of CoT. In contrast, CoT leverages stochastic decoding to approximate solutions to problems where exact computation is intractable. These separations suggest the tasks for which depth-driven recursion is more suitable, thereby offering practical guidance for choosing between reasoning paradigms. Code is available at https://github.com/kevin671/cot-vs-loop.

