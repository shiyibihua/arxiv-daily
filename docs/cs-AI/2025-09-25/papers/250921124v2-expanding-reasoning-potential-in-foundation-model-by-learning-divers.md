---
layout: default
title: Expanding Reasoning Potential in Foundation Model by Learning Diverse Chains of Thought Patterns
---

# Expanding Reasoning Potential in Foundation Model by Learning Diverse Chains of Thought Patterns

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.21124" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.21124v2</a>
  <a href="https://arxiv.org/pdf/2509.21124.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.21124v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.21124v2', 'Expanding Reasoning Potential in Foundation Model by Learning Diverse Chains of Thought Patterns')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xuemiao Zhang, Can Ren, Chengying Tu, Rongxiang Weng, Shuo Wang, Hongfei Yan, Jingang Wang, Xunliang Cai

**åˆ†ç±»**: cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-25 (æ›´æ–°: 2025-09-26)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é€šè¿‡å­¦ä¹ å¤šæ ·åŒ–æ€ç»´é“¾æ¨¡å¼ï¼Œæå‡åŸºç¡€æ¨¡å‹æ¨ç†æ½œåŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ€ç»´é“¾` `æ¨ç†æ½œåŠ›` `åŸå­æ¨ç†æ¨¡å¼` `æ··åˆä¸“å®¶æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `æ•°æ®é€‰æ‹©` `åŒç²’åº¦ç®—æ³•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨åˆ©ç”¨æ€ç»´é“¾æ•°æ®æ—¶ç¼ºä¹é€‰æ‹©æ€§ï¼Œæœªèƒ½æ˜ç¡®å“ªäº›æ•°æ®ç±»å‹èƒ½æœ€æœ‰æ•ˆåœ°æå‡æ¨¡å‹æ¨ç†èƒ½åŠ›ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡å­¦ä¹ å¤šæ ·åŒ–çš„ã€å¯Œå«é«˜ä»·å€¼æ¨ç†æ¨¡å¼çš„æ€ç»´é“¾æ•°æ®æ¥æ‰©å±•åŸºç¡€æ¨¡å‹çš„æ¨ç†æ½œåŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œä»…ä½¿ç”¨10B tokençš„ç²¾é€‰æ€ç»´é“¾æ•°æ®ï¼ŒMoEæ¨¡å‹åœ¨AIMEæ•°æ®é›†ä¸Šæå‡äº†9.58%ï¼ŒRLæ€§èƒ½ä¸Šé™æå‡äº†7.81%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹æ¨ç†æ¨¡å‹åœ¨å¤æ‚æ•°å­¦æ¨ç†æ–¹é¢çš„æœ€æ–°è¿›å±•å¾—ç›Šäºå¼ºåŒ–å­¦ä¹ (RL)ã€‚åœ¨ä¸­æœŸè®­ç»ƒä¸­åŠ å…¥é•¿æ€ç»´é“¾(CoT)æ•°æ®ä¹Ÿè¢«è¯æ˜èƒ½æ˜¾è‘—æé«˜æ¨ç†æ·±åº¦ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•é€šå¸¸ä¸åŠ åŒºåˆ†åœ°ä½¿ç”¨CoTæ•°æ®ï¼Œç•™ä¸‹äº†ä¸€ä¸ªå…³é”®é—®é¢˜ï¼šå“ªç§æ•°æ®ç±»å‹æœ€æœ‰æ•ˆåœ°å¢å¼ºæ¨¡å‹æ¨ç†èƒ½åŠ›ï¼Ÿæœ¬æ–‡é¦–æ¬¡å°†åŸºç¡€æ¨¡å‹çš„æ¨ç†æ½œåŠ›å®šä¹‰ä¸ºæ­£ç¡®å›ç­”é—®é¢˜æ‰€éœ€çš„ç‹¬ç«‹å°è¯•æ¬¡æ•°çš„å€’æ•°ï¼Œè¿™ä¸æœ€ç»ˆæ¨¡å‹æ€§èƒ½å¯†åˆ‡ç›¸å…³ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºåˆ©ç”¨å¯Œå«é«˜ä»·å€¼æ¨ç†æ¨¡å¼çš„å¤šæ ·åŒ–æ•°æ®æ¥æ‰©å±•æ¨ç†æ½œåŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä»CoTåºåˆ—ä¸­æå–åŸå­æ¨ç†æ¨¡å¼ï¼Œè¿™äº›æ¨¡å¼å…·æœ‰å…±æ€§å’Œå½’çº³èƒ½åŠ›ï¼Œå¹¶ä½¿ç”¨å®ƒä»¬æ„å»ºä¸€ä¸ªå¯Œå«å®è´µæ¨ç†æ¨¡å¼çš„æ ¸å¿ƒå‚è€ƒé›†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŒç²’åº¦ç®—æ³•ï¼Œæ¶‰åŠæ¨ç†æ¨¡å¼é“¾å’Œtokenç†µï¼Œæœ‰æ•ˆåœ°ä»æ•°æ®æ± ä¸­é€‰æ‹©ä¸æ ¸å¿ƒé›†å¯¹é½çš„é«˜ä»·å€¼CoTæ•°æ®(CoTP)ï¼Œä»è€Œè®­ç»ƒæ¨¡å‹æœ‰æ•ˆåœ°æŒæ¡æ¨ç†ã€‚ä»…10B tokençš„CoTPæ•°æ®å°±ä½¿85A6Bæ··åˆä¸“å®¶(MoE)æ¨¡å‹åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„AIME 2024å’Œ2025ä¸Šæé«˜äº†9.58%ï¼Œå¹¶å°†ä¸‹æ¸¸RLæ€§èƒ½çš„ä¸Šé™æé«˜äº†7.81%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¦‚ä½•æ›´æœ‰æ•ˆåœ°åˆ©ç”¨æ€ç»´é“¾(CoT)æ•°æ®æ¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹(LLM)çš„æ¨ç†èƒ½åŠ›ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¸åŠ åŒºåˆ†åœ°ä½¿ç”¨CoTæ•°æ®ï¼Œå¯¼è‡´è®­ç»ƒæ•ˆç‡ä½ä¸‹ï¼Œä¸”æœªèƒ½å……åˆ†æŒ–æ˜CoTæ•°æ®ä¸­è•´å«çš„æ¨ç†æ¨¡å¼ã€‚å› æ­¤ï¼Œå¦‚ä½•é€‰æ‹©å’Œåˆ©ç”¨é«˜ä»·å€¼çš„CoTæ•°æ®æˆä¸ºä¸€ä¸ªå…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å­¦ä¹ å¤šæ ·åŒ–çš„æ¨ç†æ¨¡å¼æ¥æ‰©å±•åŸºç¡€æ¨¡å‹çš„æ¨ç†æ½œåŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œé¦–å…ˆä»CoTæ•°æ®ä¸­æå–åŸå­æ¨ç†æ¨¡å¼ï¼Œæ„å»ºä¸€ä¸ªæ ¸å¿ƒå‚è€ƒé›†ã€‚ç„¶åï¼Œåˆ©ç”¨åŒç²’åº¦ç®—æ³•ï¼ŒåŸºäºæ¨ç†æ¨¡å¼é“¾å’Œtokenç†µï¼Œä»æ•°æ®æ± ä¸­é€‰æ‹©ä¸æ ¸å¿ƒé›†å¯¹é½çš„é«˜ä»·å€¼CoTæ•°æ®(CoTP)è¿›è¡Œè®­ç»ƒã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨è®©æ¨¡å‹å­¦ä¹ åˆ°æ›´æœ‰æ•ˆã€æ›´é€šç”¨çš„æ¨ç†ç­–ç•¥ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼š1) ä»CoTæ•°æ®ä¸­æŠ½è±¡åŸå­æ¨ç†æ¨¡å¼ï¼›2) æ„å»ºåŒ…å«é«˜ä»·å€¼æ¨ç†æ¨¡å¼çš„æ ¸å¿ƒå‚è€ƒé›†ï¼›3) ä½¿ç”¨åŒç²’åº¦ç®—æ³•ï¼ˆæ¨ç†æ¨¡å¼é“¾å’Œtokenç†µï¼‰ä»æ•°æ®æ± ä¸­é€‰æ‹©é«˜ä»·å€¼CoTPæ•°æ®ï¼›4) ä½¿ç”¨é€‰æ‹©çš„CoTPæ•°æ®è®­ç»ƒæ¨¡å‹ã€‚åŒç²’åº¦ç®—æ³•åŒæ—¶è€ƒè™‘äº†æ¨ç†æ¨¡å¼çš„åŒ¹é…ç¨‹åº¦å’Œtokençº§åˆ«çš„ä¿¡æ¯é‡ï¼Œä»¥æ›´ç²¾ç¡®åœ°é€‰æ‹©æœ‰ä»·å€¼çš„CoTæ•°æ®ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) é¦–æ¬¡æå‡ºäº†â€œæ¨ç†æ½œåŠ›â€çš„æ¦‚å¿µï¼Œå¹¶å°†å…¶å®šä¹‰ä¸ºæ­£ç¡®å›ç­”é—®é¢˜æ‰€éœ€çš„ç‹¬ç«‹å°è¯•æ¬¡æ•°çš„å€’æ•°ï¼›2) æå‡ºäº†åŸºäºåŸå­æ¨ç†æ¨¡å¼çš„CoTæ•°æ®é€‰æ‹©æ–¹æ³•ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°é€‰æ‹©é«˜ä»·å€¼çš„CoTæ•°æ®ï¼›3) æå‡ºäº†åŒç²’åº¦ç®—æ³•ï¼Œç»“åˆäº†æ¨ç†æ¨¡å¼é“¾å’Œtokenç†µï¼Œæé«˜äº†CoTæ•°æ®é€‰æ‹©çš„å‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨åŸå­æ¨ç†æ¨¡å¼çš„æå–ä¸Šï¼Œè®ºæ–‡å¯èƒ½é‡‡ç”¨äº†ä¸€äº›å¯å‘å¼è§„åˆ™æˆ–èšç±»ç®—æ³•æ¥è¯†åˆ«CoTåºåˆ—ä¸­çš„å¸¸è§æ¨¡å¼ã€‚åœ¨åŒç²’åº¦ç®—æ³•ä¸­ï¼Œæ¨ç†æ¨¡å¼é“¾çš„åŒ¹é…ç¨‹åº¦å¯èƒ½é€šè¿‡è®¡ç®—ç›¸ä¼¼åº¦æˆ–ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œç¼–ç æ¥å®ç°ã€‚Tokenç†µåˆ™ç”¨äºè¡¡é‡CoTåºåˆ—ä¸­æ¯ä¸ªtokençš„ä¿¡æ¯é‡ï¼Œé«˜ç†µçš„tokenå¯èƒ½åŒ…å«æ›´é‡è¦çš„æ¨ç†ä¿¡æ¯ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ç»†èŠ‚åœ¨è®ºæ–‡ä¸­å¯èƒ½æœ‰æ‰€æè¿°ï¼Œä½†æ‘˜è¦ä¸­æœªæåŠã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œä»…ä½¿ç”¨10B tokençš„ç²¾é€‰CoTPæ•°æ®ï¼Œ85A6B MoEæ¨¡å‹åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„AIME 2024å’Œ2025æ•°æ®é›†ä¸Šå–å¾—äº†9.58%çš„æ€§èƒ½æå‡ï¼Œå¹¶ä¸”å°†ä¸‹æ¸¸å¼ºåŒ–å­¦ä¹ æ€§èƒ½çš„ä¸Šé™æé«˜äº†7.81%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°æå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶ä¸ºåç»­ç ”ç©¶æä¾›äº†æœ‰åŠ›çš„æ”¯æŒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦å¤æ‚æ¨ç†èƒ½åŠ›çš„åœºæ™¯ï¼Œä¾‹å¦‚æ•°å­¦é—®é¢˜æ±‚è§£ã€ä»£ç ç”Ÿæˆã€çŸ¥è¯†å›¾è°±æ¨ç†ç­‰ã€‚é€šè¿‡æå‡åŸºç¡€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥æé«˜è¿™äº›åº”ç”¨åœºæ™¯çš„æ€§èƒ½å’Œæ•ˆç‡ï¼Œå¹¶ä¸ºå¼€å‘æ›´æ™ºèƒ½çš„AIç³»ç»Ÿå¥ å®šåŸºç¡€ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºæå‡å…¶ä»–ç±»å‹ä»»åŠ¡çš„æ¨¡å‹æ€§èƒ½ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent progress in large reasoning models for challenging mathematical reasoning has been driven by reinforcement learning (RL). Incorporating long chain-of-thought (CoT) data during mid-training has also been shown to substantially improve reasoning depth. However, current approaches often utilize CoT data indiscriminately, leaving open the critical question of which data types most effectively enhance model reasoning capabilities. In this paper, we define the foundation model's reasoning potential for the first time as the inverse of the number of independent attempts required to correctly answer the question, which is strongly correlated with the final model performance. We then propose utilizing diverse data enriched with high-value reasoning patterns to expand the reasoning potential. Specifically, we abstract atomic reasoning patterns from CoT sequences, characterized by commonality and inductive capabilities, and use them to construct a core reference set enriched with valuable reasoning patterns. Furthermore, we propose a dual-granularity algorithm involving chains of reasoning patterns and token entropy, efficiently selecting high-value CoT data (CoTP) from the data pool that aligns with the core set, thereby training models to master reasoning effectively. Only 10B-token CoTP data enables the 85A6B Mixture-of-Experts (MoE) model to improve by 9.58% on the challenging AIME 2024 and 2025, and to raise the upper bound of downstream RL performance by 7.81%.

