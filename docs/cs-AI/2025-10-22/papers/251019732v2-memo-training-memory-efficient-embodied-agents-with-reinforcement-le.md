---
layout: default
title: Memo: Training Memory-Efficient Embodied Agents with Reinforcement Learning
---

# Memo: Training Memory-Efficient Embodied Agents with Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.19732" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.19732v2</a>
  <a href="https://arxiv.org/pdf/2510.19732.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.19732v2" onclick="toggleFavorite(this, '2510.19732v2', 'Memo: Training Memory-Efficient Embodied Agents with Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Gunshi Gupta, Karmesh Yadav, Zsolt Kira, Yarin Gal, Rahaf Aljundi

**åˆ†ç±»**: cs.AI, cs.CV, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-10-22 (æ›´æ–°: 2025-11-27)

**å¤‡æ³¨**: Accepted for Spotlight Presentation at NeurIPS 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/gunshi/memo)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMemoï¼šä¸€ç§å†…å­˜é«˜æ•ˆçš„å¼ºåŒ–å­¦ä¹ å…·èº«æ™ºèƒ½ä½“è®­ç»ƒæ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å…·èº«æ™ºèƒ½` `å¼ºåŒ–å­¦ä¹ ` `Transformer` `è®°å¿†ç½‘ç»œ` `é•¿æ—¶ç¨‹ä»»åŠ¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºTransformerçš„å…·èº«æ™ºèƒ½ä½“ç­–ç•¥è®­ç»ƒï¼Œé¢ä¸´è§†è§‰è¾“å…¥è¶…å‡ºTransformerä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶çš„æŒ‘æˆ˜ã€‚
2. Memoé€šè¿‡å‘¨æœŸæ€§åœ°ç”Ÿæˆå’Œæ£€ç´¢æ‘˜è¦tokenï¼Œå°†è¾“å…¥ä¿¡æ¯å‹ç¼©æˆè®°å¿†ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒMemoåœ¨å†…å­˜æ•ˆç‡ã€è®¡ç®—æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ä¸Šä¼˜äºä¼ ç»ŸTransformerï¼Œå°¤å…¶æ˜¯åœ¨é•¿æ—¶ç¨‹ä»»åŠ¡ä¸­ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸ºäº†ä½¿å…·èº«æ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨è¾ƒé•¿æ—¶é—´èŒƒå›´å†…æœ‰æ•ˆè¿è¡Œï¼Œå¼€å‘èƒ½å¤Ÿå½¢æˆå’Œè®¿é—®è®°å¿†çš„æ¨¡å‹è‡³å…³é‡è¦ï¼Œä»¥ä¾¿åœ¨ç¯å¢ƒä¸­ä¿æŒä¸Šä¸‹æ–‡æ„ŸçŸ¥ã€‚åœ¨å½“å‰åŸºäºTransformerçš„ç­–ç•¥è®­ç»ƒèŒƒå¼ä¸­ï¼Œè§†è§‰è¾“å…¥é€šå¸¸ä¼šè¶…å‡ºTransformerçš„ä¸Šä¸‹æ–‡é™åˆ¶ï¼Œè€Œäººç±»å¯ä»¥ç»´æŠ¤å’Œåˆ©ç”¨å‹ç¼©ä¸ºè®°å¿†çš„ç»ˆèº«ç»éªŒã€‚åŸåˆ™ä¸Šå¯ä»¥è¿›è¡Œæ˜¾è‘—çš„å‹ç¼©ï¼Œå› ä¸ºå¤§éƒ¨åˆ†è¾“å…¥æ˜¯ä¸ç›¸å…³çš„å¹¶ä¸”å¯ä»¥è¢«æŠ½è±¡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å…·æœ‰å›ºå®šå¤§å°å†…å­˜çš„å¾ªç¯æ¨¡å‹æˆ–å®Œå…¨ä¾èµ–ä¸Šä¸‹æ–‡çš„Transformerã€‚æœ¬æ–‡æå‡ºMemoï¼Œä¸€ç§åŸºäºTransformerçš„æ¶æ„å’Œè®­ç»ƒæ–¹æ³•ï¼Œç”¨äºåœ¨å†…å­˜å¯†é›†å‹ã€é•¿æ—¶ç¨‹ä»»åŠ¡ä¸Šè¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ã€‚Memoé€šè¿‡åœ¨è®­ç»ƒæœŸé—´å°†å‘¨æœŸæ€§çš„æ‘˜è¦tokenä¸æ¨¡å‹çš„è¾“å…¥äº¤é”™ï¼Œä»è€Œå®ç°è®°å¿†çš„åˆ›å»ºå’Œæ£€ç´¢ã€‚æˆ‘ä»¬åœ¨ç½‘æ ¼ä¸–ç•Œå…ƒå¼ºåŒ–å­¦ä¹ åŸºå‡†æµ‹è¯•å’Œé€¼çœŸå®¤å†…ç¯å¢ƒä¸­çš„å¤šå¯¹è±¡å¯¼èˆªä»»åŠ¡ä¸­è¯æ˜äº†Memoçš„æœ‰æ•ˆæ€§ã€‚Memoä¼˜äºæœ´ç´ çš„é•¿ä¸Šä¸‹æ–‡TransformeråŸºçº¿ï¼ŒåŒæ—¶æ›´å…·è®¡ç®—å’Œå­˜å‚¨æ•ˆç‡ã€‚æ­¤å¤–ï¼ŒMemoåœ¨æ¨ç†æ—¶æ›´å¥½åœ°æ³›åŒ–åˆ°æ›´é•¿çš„ä¸Šä¸‹æ–‡ï¼Œå¹¶ä¸”åœ¨æµå¼è®¾ç½®ä¸­ä¿æŒé²æ£’æ€§ï¼Œåœ¨è¿™ç§è®¾ç½®ä¸­ï¼Œå†å²ä¸Šä¸‹æ–‡å¿…é¡»è¢«æˆªæ–­ä»¥é€‚åº”æ¨ç†çº¦æŸã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºTransformerçš„å…·èº«æ™ºèƒ½ä½“åœ¨é•¿æ—¶ç¨‹ä»»åŠ¡ä¸­é¢ä¸´ä¸Šä¸‹æ–‡ä¿¡æ¯çˆ†ç‚¸çš„é—®é¢˜ã€‚Transformerçš„ä¸Šä¸‹æ–‡çª—å£æœ‰é™ï¼Œæ— æ³•å¤„ç†é•¿æ—¶é—´åºåˆ—çš„è§†è§‰è¾“å…¥ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚æ­¤å¤–ï¼Œå­˜å‚¨å’Œè®¡ç®—æˆæœ¬ä¹Ÿéšç€ä¸Šä¸‹æ–‡é•¿åº¦çš„å¢åŠ è€Œæ˜¾è‘—å¢åŠ ã€‚ç°æœ‰æ–¹æ³•è¦ä¹ˆä¾èµ–å›ºå®šå¤§å°çš„å¾ªç¯è®°å¿†ï¼Œè¦ä¹ˆå®Œå…¨ä¾èµ–Transformerçš„ä¸Šä¸‹æ–‡ï¼Œæ— æ³•æœ‰æ•ˆè§£å†³é•¿æ—¶ç¨‹ä»»åŠ¡ä¸­çš„è®°å¿†é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMemoçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å‘¨æœŸæ€§åœ°ç”Ÿæˆå’Œæ£€ç´¢æ‘˜è¦tokenæ¥å‹ç¼©å’Œåˆ©ç”¨å†å²ä¿¡æ¯ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹å­¦ä¹ å°†å…³é”®ä¿¡æ¯æç‚¼æˆæ‘˜è¦tokenï¼Œå¹¶å°†å…¶å­˜å‚¨åœ¨è®°å¿†ä¸­ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹å¯ä»¥æ ¹æ®å½“å‰çŠ¶æ€æ£€ç´¢ç›¸å…³çš„æ‘˜è¦tokenï¼Œä»è€Œåœ¨æœ‰é™çš„ä¸Šä¸‹æ–‡çª—å£å†…è®¿é—®æ›´é•¿çš„å†å²ä¿¡æ¯ã€‚è¿™ç§æ–¹æ³•ç±»ä¼¼äºäººç±»çš„è®°å¿†æœºåˆ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†é•¿æ—¶ç¨‹ä»»åŠ¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMemoçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸€ä¸ªTransformerç¼–ç å™¨ã€ä¸€ä¸ªè®°å¿†æ¨¡å—å’Œä¸€ä¸ªç­–ç•¥ç½‘ç»œã€‚Transformerç¼–ç å™¨è´Ÿè´£å°†è§†è§‰è¾“å…¥ç¼–ç æˆç‰¹å¾å‘é‡ã€‚è®°å¿†æ¨¡å—è´Ÿè´£å­˜å‚¨å’Œæ£€ç´¢æ‘˜è¦tokenã€‚ç­–ç•¥ç½‘ç»œæ ¹æ®å½“å‰çŠ¶æ€å’Œæ£€ç´¢åˆ°çš„æ‘˜è¦tokenç”ŸæˆåŠ¨ä½œã€‚è®­ç»ƒè¿‡ç¨‹åŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µï¼šè®°å¿†åˆ›å»ºé˜¶æ®µå’Œç­–ç•¥å­¦ä¹ é˜¶æ®µã€‚åœ¨è®°å¿†åˆ›å»ºé˜¶æ®µï¼Œæ¨¡å‹å­¦ä¹ ç”Ÿæˆèƒ½å¤Ÿä»£è¡¨å†å²ä¿¡æ¯çš„æ‘˜è¦tokenã€‚åœ¨ç­–ç•¥å­¦ä¹ é˜¶æ®µï¼Œæ¨¡å‹å­¦ä¹ æ ¹æ®å½“å‰çŠ¶æ€å’Œæ£€ç´¢åˆ°çš„æ‘˜è¦tokenç”Ÿæˆæœ€ä¼˜åŠ¨ä½œã€‚

**å…³é”®åˆ›æ–°**ï¼šMemoçš„å…³é”®åˆ›æ–°åœ¨äºå°†è®°å¿†æœºåˆ¶èå…¥åˆ°Transformeræ¶æ„ä¸­ï¼Œå¹¶æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„è®­ç»ƒæ–¹æ³•æ¥å­¦ä¹ ç”Ÿæˆå’Œæ£€ç´¢æ‘˜è¦tokenã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒMemoèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨å†å²ä¿¡æ¯ï¼Œå¹¶åœ¨é•¿æ—¶ç¨‹ä»»åŠ¡ä¸­å–å¾—æ›´å¥½çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒMemoè¿˜å…·æœ‰æ›´é«˜çš„è®¡ç®—å’Œå­˜å‚¨æ•ˆç‡ï¼Œä½¿å…¶èƒ½å¤Ÿåº”ç”¨äºæ›´å¤æ‚çš„å…·èº«æ™ºèƒ½ä½“ä»»åŠ¡ã€‚

**å…³é”®è®¾è®¡**ï¼šMemoçš„å…³é”®è®¾è®¡åŒ…æ‹¬æ‘˜è¦tokençš„ç”Ÿæˆæ–¹å¼ã€è®°å¿†æ¨¡å—çš„ç»“æ„å’Œæ£€ç´¢æœºåˆ¶ã€‚æ‘˜è¦tokené€šè¿‡ä¸€ä¸ªç‹¬ç«‹çš„Transformerå±‚ç”Ÿæˆï¼Œè¯¥å±‚æ¥æ”¶Transformerç¼–ç å™¨çš„è¾“å‡ºä½œä¸ºè¾“å…¥ã€‚è®°å¿†æ¨¡å—é‡‡ç”¨é”®å€¼å¯¹å­˜å‚¨ç»“æ„ï¼Œå…¶ä¸­é”®æ˜¯æ‘˜è¦tokenï¼Œå€¼æ˜¯å¯¹åº”çš„å†å²ä¿¡æ¯ã€‚æ£€ç´¢æœºåˆ¶é‡‡ç”¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ ¹æ®å½“å‰çŠ¶æ€è®¡ç®—æ¯ä¸ªæ‘˜è¦tokençš„æƒé‡ï¼Œå¹¶é€‰æ‹©æƒé‡æœ€é«˜çš„æ‘˜è¦tokenã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬ç­–ç•¥æ¢¯åº¦æŸå¤±å’Œè®°å¿†æŸå¤±ï¼Œå…¶ä¸­ç­–ç•¥æ¢¯åº¦æŸå¤±ç”¨äºä¼˜åŒ–ç­–ç•¥ç½‘ç»œï¼Œè®°å¿†æŸå¤±ç”¨äºä¼˜åŒ–æ‘˜è¦tokençš„ç”Ÿæˆã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

Memoåœ¨ç½‘æ ¼ä¸–ç•Œå…ƒå¼ºåŒ–å­¦ä¹ åŸºå‡†æµ‹è¯•å’Œé€¼çœŸå®¤å†…ç¯å¢ƒä¸­çš„å¤šå¯¹è±¡å¯¼èˆªä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆæœã€‚åœ¨ç½‘æ ¼ä¸–ç•Œä»»åŠ¡ä¸­ï¼ŒMemoçš„æ€§èƒ½ä¼˜äºæœ´ç´ çš„é•¿ä¸Šä¸‹æ–‡TransformeråŸºçº¿ã€‚åœ¨å¤šå¯¹è±¡å¯¼èˆªä»»åŠ¡ä¸­ï¼ŒMemoåœ¨å¯¼èˆªæˆåŠŸç‡å’Œè·¯å¾„é•¿åº¦æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒMemoè¿˜è¡¨ç°å‡ºæ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨æ›´é•¿çš„ä¸Šä¸‹æ–‡é•¿åº¦ä¸‹ä¿æŒé²æ£’æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMemoæ˜¯ä¸€ç§æœ‰æ•ˆçš„å†…å­˜é«˜æ•ˆçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Memoå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚æœºå™¨äººå¯¼èˆªã€é•¿æœŸå¯¹è¯ã€æ¸¸æˆAIç­‰ã€‚åœ¨æœºå™¨äººå¯¼èˆªé¢†åŸŸï¼ŒMemoå¯ä»¥å¸®åŠ©æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­è¿›è¡Œé•¿æœŸæ¢ç´¢å’Œå®šä½ã€‚åœ¨é•¿æœŸå¯¹è¯é¢†åŸŸï¼ŒMemoå¯ä»¥å¸®åŠ©æ¨¡å‹è®°ä½å¯¹è¯å†å²ï¼Œä»è€Œç”Ÿæˆæ›´è¿è´¯å’Œè‡ªç„¶çš„å›å¤ã€‚åœ¨æ¸¸æˆAIé¢†åŸŸï¼ŒMemoå¯ä»¥å¸®åŠ©æ™ºèƒ½ä½“æ›´å¥½åœ°ç†è§£æ¸¸æˆç¯å¢ƒï¼Œå¹¶åˆ¶å®šæ›´æœ‰æ•ˆçš„ç­–ç•¥ã€‚Memoçš„å†…å­˜æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ä½¿å…¶èƒ½å¤Ÿåº”ç”¨äºå„ç§éœ€è¦é•¿æœŸè®°å¿†çš„ä»»åŠ¡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> To enable embodied agents to operate effectively over extended timeframes, it is crucial to develop models that form and access memories to stay contextualized in their environment. In the current paradigm of training transformer-based policies for embodied sequential decision-making tasks, visual inputs often overwhelm the context limits of transformers, while humans can maintain and utilize a lifetime of experience compressed as memories. Significant compression is possible in principle, as much of the input is irrelevant and can be abstracted. However, existing approaches predominantly focus on either recurrent models with fixed-size memory or transformers with full-context reliance. In this work, we propose Memo, a transformer-based architecture and training recipe for reinforcement learning (RL) on memory-intensive, long-horizon tasks. Memo incorporates the creation and retrieval of memory by interleaving periodic summarization tokens with the inputs of a model during training. We demonstrate Memo's effectiveness on a gridworld meta-RL benchmark and a multi-object navigation task in photo-realistic indoor settings. Memo outperforms naive long-context transformer baselines while being more compute and storage efficient. Additionally, Memo generalizes better to longer contexts at inference time and remains robust in streaming settings, where historical context must be truncated to fit inference constraints. Our code is available at: https://github.com/gunshi/memo.

