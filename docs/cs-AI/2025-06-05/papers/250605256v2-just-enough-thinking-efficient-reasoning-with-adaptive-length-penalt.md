---
layout: default
title: Just Enough Thinking: Efficient Reasoning with Adaptive Length Penalties Reinforcement Learning
---

# Just Enough Thinking: Efficient Reasoning with Adaptive Length Penalties Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05256" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05256v2</a>
  <a href="https://arxiv.org/pdf/2506.05256.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05256v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05256v2', 'Just Enough Thinking: Efficient Reasoning with Adaptive Length Penalties Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Violet Xiang, Chase Blagden, Rafael Rafailov, Nathan Lile, Sang Truong, Chelsea Finn, Nick Haber

**åˆ†ç±»**: cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05 (æ›´æ–°: 2025-06-06)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªé€‚åº”é•¿åº¦æƒ©ç½šä»¥æé«˜æ¨ç†æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è‡ªé€‚åº”é•¿åº¦æƒ©ç½š` `æ¨ç†æ•ˆç‡` `å¼ºåŒ–å­¦ä¹ ` `è®¡ç®—èµ„æºä¼˜åŒ–` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤„ç†ç®€å•é—®é¢˜æ—¶ï¼Œå¾€å¾€å› ç”Ÿæˆå†—é•¿çš„æ ‡è®°è€Œæµªè´¹è®¡ç®—èµ„æºï¼Œæ— æ³•æœ‰æ•ˆåŒºåˆ†é—®é¢˜çš„éš¾æ˜“ç¨‹åº¦ã€‚
2. æœ¬æ–‡æå‡ºè‡ªé€‚åº”é•¿åº¦æƒ©ç½šï¼ˆALPï¼‰ï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´ç”Ÿæˆé•¿åº¦æ¥ä¼˜åŒ–æ¨ç†æ•ˆç‡ï¼Œé’ˆå¯¹ä¸åŒéš¾åº¦çš„æç¤ºæ–½åŠ ä¸åŒçš„æƒ©ç½šã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨ALPçš„DeepScaleR-1.5Bæ¨¡å‹åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶ï¼Œå¹³å‡æ ‡è®°ä½¿ç”¨é‡å‡å°‘äº†50%ï¼Œå¹¶åœ¨æœ€éš¾é—®é¢˜ä¸Šå–å¾—äº†æ›´é«˜çš„å‡†ç¡®æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­é€šè¿‡ç”Ÿæˆæ›´å¤šçš„æ ‡è®°æ¥æé«˜æ€§èƒ½ï¼Œä½†è¿™ç§å†—é•¿çš„ç”Ÿæˆåœ¨ç®€å•é—®é¢˜ä¸Šå¾€å¾€æµªè´¹è®¡ç®—èµ„æºã€‚ç°æœ‰è§£å†³æ–¹æ¡ˆå¦‚ç›‘ç£å¾®è°ƒã€ç”¨æˆ·æ§åˆ¶é¢„ç®—æˆ–å‡åŒ€æƒ©ç½šçš„å¼ºåŒ–å­¦ä¹ ï¼Œå‡éœ€æ•°æ®æ•´ç†ã€æ‰‹åŠ¨é…ç½®æˆ–å¯¹æ‰€æœ‰é—®é¢˜ä¸€è§†åŒä»ã€‚æœ¬æ–‡æå‡ºè‡ªé€‚åº”é•¿åº¦æƒ©ç½šï¼ˆALPï¼‰ï¼Œé€šè¿‡ç›‘æ§æ¯ä¸ªæç¤ºçš„åœ¨çº¿è§£å†³ç‡ï¼ŒåŠ¨æ€è°ƒæ•´ç”Ÿæˆé•¿åº¦ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒALPä¸ºè‡ªä¿¡ï¼ˆç®€å•ï¼‰æç¤ºå¢åŠ é«˜æƒ©ç½šï¼Œè€Œå¯¹å›°éš¾æç¤ºåˆ™ä¸å—å½±å“ã€‚ç»è¿‡è®­ç»ƒçš„DeepScaleR-1.5Båœ¨ä¸æ˜¾è‘—é™ä½æ€§èƒ½çš„æƒ…å†µä¸‹ï¼Œå°†å¹³å‡æ ‡è®°ä½¿ç”¨é‡å‡å°‘äº†50%ã€‚ä¸å›ºå®šé¢„ç®—å’Œå‡åŒ€æƒ©ç½šåŸºçº¿ç›¸æ¯”ï¼ŒALPæ›´æ™ºèƒ½åœ°é‡æ–°åˆ†é…é¢„ç®—ï¼Œæå‡äº†å¯¹æœ€éš¾é—®é¢˜çš„å‡†ç¡®æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹æ¨ç†æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­å› ç”Ÿæˆå†—é•¿æ ‡è®°è€Œå¯¼è‡´çš„è®¡ç®—èµ„æºæµªè´¹é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æ— æ³•æœ‰æ•ˆåŒºåˆ†é—®é¢˜çš„éš¾æ˜“ç¨‹åº¦ï¼Œå¯¼è‡´åœ¨ç®€å•é—®é¢˜ä¸Šä¹Ÿæ¶ˆè€—è¿‡å¤šè®¡ç®—èµ„æºã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºè‡ªé€‚åº”é•¿åº¦æƒ©ç½šï¼ˆALPï¼‰ï¼Œé€šè¿‡ç›‘æ§æ¯ä¸ªæç¤ºçš„åœ¨çº¿è§£å†³ç‡ï¼ŒåŠ¨æ€è°ƒæ•´ç”Ÿæˆé•¿åº¦ã€‚ç®€å•é—®é¢˜çš„ç”Ÿæˆä¼šå—åˆ°æ›´é«˜çš„æƒ©ç½šï¼Œè€Œå›°éš¾é—®é¢˜åˆ™ä¸å—å½±å“ï¼Œä»è€Œä¼˜åŒ–è®¡ç®—èµ„æºçš„ä½¿ç”¨ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šALPçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªé˜¶æ®µï¼šé¦–å…ˆï¼Œé€šè¿‡å¤šæ¬¡å›æ»šç›‘æ§æ¯ä¸ªæç¤ºçš„è§£å†³ç‡ï¼›ç„¶åï¼Œæ ¹æ®è§£å†³ç‡åŠ¨æ€è°ƒæ•´æƒ©ç½šçš„å¤§å°ï¼›æœ€åï¼Œä¼˜åŒ–ç”Ÿæˆè¿‡ç¨‹ä»¥å‡å°‘ç®€å•é—®é¢˜çš„æ ‡è®°ç”Ÿæˆã€‚

**å…³é”®åˆ›æ–°**ï¼šALPçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶åŠ¨æ€è°ƒæ•´æƒ©ç½šæœºåˆ¶ï¼Œèƒ½å¤Ÿæ ¹æ®æ¯ä¸ªæç¤ºçš„è§£å†³ç‡çµæ´»è°ƒæ•´ç”Ÿæˆé•¿åº¦ï¼Œä¸ç°æœ‰çš„å›ºå®šé¢„ç®—å’Œå‡åŒ€æƒ©ç½šæ–¹æ³•ç›¸æ¯”ï¼Œå…·æœ‰æ›´é«˜çš„æ™ºèƒ½æ€§å’Œé€‚åº”æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ALPä¸­ï¼Œæƒ©ç½šçš„å¤§å°ä¸è§£å†³ç‡æˆåæ¯”ï¼Œç®€å•é—®é¢˜çš„é¢å¤–æ ‡è®°ç”Ÿæˆä¼šäº§ç”Ÿè¾ƒé«˜çš„æƒ©ç½šï¼Œè€Œå›°éš¾é—®é¢˜åˆ™ä¸å—æ­¤é™åˆ¶ã€‚è¯¥è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æœ‰æ•ˆåœ°å­¦ä¹ åˆ°å¦‚ä½•åˆ†é…è®¡ç®—èµ„æºã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨è‡ªé€‚åº”é•¿åº¦æƒ©ç½šçš„DeepScaleR-1.5Bæ¨¡å‹åœ¨ä¸æ˜¾è‘—é™ä½æ€§èƒ½çš„æƒ…å†µä¸‹ï¼Œå¹³å‡æ ‡è®°ä½¿ç”¨é‡å‡å°‘äº†50%ã€‚ä¸å›ºå®šé¢„ç®—å’Œå‡åŒ€æƒ©ç½šåŸºçº¿ç›¸æ¯”ï¼ŒALPåœ¨æœ€éš¾é—®é¢˜ä¸Šçš„å‡†ç¡®æ€§å¾—åˆ°äº†æ˜¾è‘—æå‡ï¼Œå±•ç¤ºäº†å…¶åœ¨æ¨ç†æ•ˆç‡ä¸Šçš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿå’Œè‡ªåŠ¨åŒ–æ¨ç†ç­‰ã€‚é€šè¿‡ä¼˜åŒ–æ¨ç†è¿‡ç¨‹ï¼ŒALPèƒ½å¤Ÿæé«˜æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„æ•ˆç‡ï¼Œé™ä½è®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒæˆ–æå‡æ¨¡å‹çš„æ€§èƒ½ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large reasoning models (LRMs) achieve higher performance on challenging reasoning tasks by generating more tokens at inference time, but this verbosity often wastes computation on easy problems. Existing solutions, including supervised finetuning on shorter traces, user-controlled budgets, or RL with uniform penalties, either require data curation, manual configuration, or treat all problems alike regardless of difficulty. We introduce Adaptive Length Penalty (ALP), a reinforcement learning objective tailoring generation length to per-prompt solve rate. During training, ALP monitors each prompt's online solve rate through multiple rollouts and adds a differentiable penalty whose magnitude scales inversely with that rate, so confident (easy) prompts incur a high cost for extra tokens while hard prompts remain unhindered. Posttraining DeepScaleR-1.5B with ALP cuts average token usage by 50\% without significantly dropping performance. Relative to fixed-budget and uniform penalty baselines, ALP redistributes its reduced budget more intelligently by cutting compute on easy prompts and reallocating saved tokens to difficult ones, delivering higher accuracy on the hardest problems with higher cost.

