---
layout: default
title: Beyond Accuracy: Dissecting Mathematical Reasoning for LLMs Under Reinforcement Learning
---

# Beyond Accuracy: Dissecting Mathematical Reasoning for LLMs Under Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.04723" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.04723v2</a>
  <a href="https://arxiv.org/pdf/2506.04723.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.04723v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.04723v2', 'Beyond Accuracy: Dissecting Mathematical Reasoning for LLMs Under Reinforcement Learning')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Jiayu Wang, Yifei Ming, Zixuan Ke, Caiming Xiong, Shafiq Joty, Aws Albarghouthi, Frederic Sala

**ÂàÜÁ±ª**: cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-05 (Êõ¥Êñ∞: 2025-10-24)

**Â§áÊ≥®**: Accepted to NeurIPS 2025

**üîó ‰ª£Á†Å/È°πÁõÆ**: [PROJECT_PAGE](https://sparkle-reasoning.github.io/)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫SPARKLEÊ°ÜÊû∂‰ª•Ê∑±ÂÖ•ÁêÜËß£RLÂØπLLMsÊé®ÁêÜËÉΩÂäõÁöÑÂΩ±Âìç**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Âº∫ÂåñÂ≠¶‰π†` `ËØ≠Ë®ÄÊ®°Âûã` `Êé®ÁêÜËÉΩÂäõ` `Áü•ËØÜÊï¥Âêà` `SPARKLEÊ°ÜÊû∂` `Ê®°ÂûãÈ≤ÅÊ£íÊÄß` `Â§öÈò∂ÊÆµÁÆ°ÈÅì`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâRLÊñπÊ≥ïÂú®ÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜËÉΩÂäõÊñπÈù¢Áº∫‰πèÁªÜËá¥ÁöÑÁêÜËß£ÔºåÂ∞§ÂÖ∂ÊòØÂ¶Ç‰ΩïÂΩ±ÂìçÊ®°ÂûãÁöÑÊâßË°åÂíåÁü•ËØÜÊï¥Âêà„ÄÇ
2. Êú¨ÊñáÊèêÂá∫SPARKLEÊ°ÜÊû∂ÔºåÈÄöËøáÂàÜÊûêËÆ°ÂàíÊâßË°å„ÄÅÁü•ËØÜÊï¥ÂêàÂíåÂ≠êÈóÆÈ¢òÈìæÔºåÊ∑±ÂÖ•Êé¢ËÆ®RLÂØπÊ®°ÂûãÊé®ÁêÜÁöÑÂΩ±Âìç„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåRLË∞É‰ºòÁöÑÊ®°ÂûãÂú®Èù¢ÂØπÂõ∞ÈöæÈóÆÈ¢òÊó∂Ë°®Áé∞Âá∫Êõ¥Â•ΩÁöÑÈ≤ÅÊ£íÊÄßÔºåÂπ∂‰∏îÂú®Áü•ËØÜÊï¥ÂêàÊñπÈù¢ÂèñÂæó‰∫Ü‰∏ÄËá¥ÊÄßÊèêÂçá„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Âº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÂ∑≤Êàê‰∏∫ÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÂú®Â§çÊùÇÊé®ÁêÜ‰ªªÂä°‰∏≠Ë°®Áé∞ÁöÑ‰∏ªË¶ÅÊñπÊ≥ï„ÄÇÂ∞ΩÁÆ°RLËÆ≠ÁªÉÊñπÊ≥ïÂ¶ÇGRPOÂú®ÂÆûËØÅ‰∏äÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ïÔºå‰ΩÜÂØπRLÂ¶Ç‰ΩïÊèêÂçáÊÄßËÉΩÁöÑÁªÜËá¥ÁêÜËß£‰ªçÁÑ∂Áº∫‰πè„ÄÇ‰∏∫Ê≠§ÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜSPARKLEÊ°ÜÊû∂Ôºå‰ªéËÆ°ÂàíÊâßË°å„ÄÅÁü•ËØÜÊï¥ÂêàÂíåÂ≠êÈóÆÈ¢òÈìæ‰∏â‰∏™Áª¥Â∫¶ÂàÜÊûêRLÁöÑÂΩ±Âìç„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÊèê‰æõÊòéÁ°ÆÁöÑÊ≠•È™§ËÆ°ÂàíÂèØËÉΩ‰ºöÈôç‰ΩéÊ®°ÂûãÂú®ÊåëÊàòÊÄßÂü∫ÂáÜ‰∏äÁöÑË°®Áé∞ÔºåËÄåRLË∞É‰ºòÁöÑÊ®°ÂûãÂàôË°®Áé∞Âá∫Êõ¥Âº∫ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇÊ≠§Â§ñÔºåRLÂ¢ûÂº∫‰∫ÜÊ®°ÂûãÂ∞ÜÁü•ËØÜÊï¥ÂêàËøõÊé®ÁêÜËøáÁ®ãÁöÑËÉΩÂäõÔºåÂ∏¶Êù•‰∫ÜË∑®‰ªªÂä°ÁöÑ‰∏ÄËá¥ÊÄßÊèêÂçá„ÄÇÊúÄÂêéÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜSparkleRL-PSSÔºå‰∏Ä‰∏™Â§öÈò∂ÊÆµRLÁÆ°ÈÅìÔºåÂà©Áî®ÈÉ®ÂàÜÊ≠•È™§ÊîØÊû∂ÊúâÊïàÊåáÂØºÊé¢Á¥¢ÔºåÈÅøÂÖç‰∫ÜÈ¢ùÂ§ñÊï∞ÊçÆÁîüÊàê„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥ÂØπÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÂ¶Ç‰ΩïÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÊé®ÁêÜËÉΩÂäõÁöÑÁêÜËß£‰∏çË∂≥ÔºåÁé∞ÊúâÊñπÊ≥ïÊú™ËÉΩÊ∑±ÂÖ•ÂàÜÊûêRLÁöÑÂÖ∑‰ΩìÂΩ±Âìç„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊèêÂá∫SPARKLEÊ°ÜÊû∂Ôºå‰ªéËÆ°ÂàíÊâßË°å„ÄÅÁü•ËØÜÊï¥ÂêàÂíåÂ≠êÈóÆÈ¢òÈìæ‰∏â‰∏™Áª¥Â∫¶ÂàÜÊûêRLÁöÑÂΩ±ÂìçÔºåÂº∫Ë∞ÉRL‰∏ç‰ªÖÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑÊâßË°åËÉΩÂäõÔºåËøòÂ¢ûÂº∫‰∫ÜÂÖ∂Áü•ËØÜÊï¥ÂêàËÉΩÂäõ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöSPARKLEÊ°ÜÊû∂ÂåÖÂê´‰∏â‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöËÆ°ÂàíÊâßË°åÊ®°Âùó„ÄÅÁü•ËØÜÊï¥ÂêàÊ®°ÂùóÂíåÂ≠êÈóÆÈ¢òÈìæÊ®°Âùó„ÄÇÊØè‰∏™Ê®°ÂùóÂàÜÂà´ÂàÜÊûêRLÂú®‰∏çÂêåÊé®ÁêÜÁª¥Â∫¶‰∏äÁöÑ‰ΩúÁî®„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞Âú®‰∫éÈÄöËøáÁªÜËá¥ÁöÑÁª¥Â∫¶ÂàÜÊûêÔºåÊè≠Á§∫‰∫ÜRLÂú®Êé®ÁêÜ‰ªªÂä°‰∏≠ÁöÑÂ§öÈáç‰ΩúÁî®ÔºåÂ∞§ÂÖ∂ÊòØÂÖ∂Âú®Áü•ËØÜÊï¥ÂêàÊñπÈù¢ÁöÑÊòæËëóÊèêÂçáÔºå‰∏é‰º†ÁªüÊñπÊ≥ïÁõ∏ÊØîÂÖ∑ÊúâÊõ¥Ê∑±Â±ÇÊ¨°ÁöÑÁêÜËß£„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂÆûÈ™å‰∏≠ÔºåÈááÁî®‰∫ÜÂ§öÈò∂ÊÆµRLÁÆ°ÈÅìSparkleRL-PSSÔºåÂà©Áî®ÈÉ®ÂàÜÊ≠•È™§ÊîØÊû∂Êù•ÊåáÂØºÊ®°ÂûãÊé¢Á¥¢ÔºåÈÅøÂÖç‰∫ÜÈ¢ùÂ§ñÁöÑÊï∞ÊçÆÁîüÊàêÈúÄÊ±Ç„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåRLË∞É‰ºòÁöÑÊ®°ÂûãÂú®Èù¢ÂØπÂõ∞ÈöæÈóÆÈ¢òÊó∂Ë°®Áé∞Âá∫Êõ¥Â∞èÁöÑÊÄßËÉΩ‰∏ãÈôçÔºåÁõ∏ËæÉ‰∫éÂü∫Á∫øÊ®°ÂûãÂíåSFTÊ®°ÂûãÔºåÈ≤ÅÊ£íÊÄßÊòæËëóÂ¢ûÂº∫„ÄÇÊ≠§Â§ñÔºåÊ®°ÂûãÂú®Áü•ËØÜÊï¥ÂêàÊñπÈù¢ÁöÑË°®Áé∞‰πüÊúâ‰∏ÄËá¥ÊÄßÊèêÂçáÔºåÂ±ïÁ§∫‰∫ÜRLÂú®Â§ö‰ªªÂä°‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ„ÄÅÊô∫ËÉΩÈóÆÁ≠îÁ≥ªÁªüÂíåÊïôËÇ≤ÊäÄÊúØÁ≠â„ÄÇÈÄöËøáÊ∑±ÂÖ•ÁêÜËß£RLÂØπÊ®°ÂûãÊé®ÁêÜËÉΩÂäõÁöÑÂΩ±ÂìçÔºåÂèØ‰ª•‰∏∫ÊûÑÂª∫Êõ¥È´òÊïà„ÄÅÈÄÇÂ∫îÊÄßÂº∫ÁöÑÊé®ÁêÜÊ®°ÂûãÊèê‰æõÁêÜËÆ∫Âü∫Á°ÄÔºåËøõËÄåÊé®Âä®Áõ∏ÂÖ≥ÊäÄÊúØÁöÑÂèëÂ±ï‰∏éÂ∫îÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Reinforcement learning (RL) has become the dominant paradigm for improving the performance of language models on complex reasoning tasks. Despite the substantial empirical gains demonstrated by RL-based training methods like GRPO, a granular understanding of why and how RL enhances performance is still lacking. To bridge this gap, we introduce SPARKLE, a fine-grained analytic framework to dissect the effects of RL across three key dimensions: (1) plan following and execution, (2) knowledge integration, and (3) chain of subproblems. Using this framework, we gain insights beyond mere accuracy. For instance, providing models with explicit human-crafted, step-by-step plans can surprisingly degrade performance on the most challenging benchmarks, yet RL-tuned models exhibit greater robustness, experiencing markedly smaller performance drops than base or SFT models. This suggests that RL may not primarily enhance the execution of external plans but rather empower models to formulate and follow internal strategies better suited to their reasoning processes. Conversely, we observe that RL enhances models' ability to integrate provided knowledge into their reasoning process, yielding consistent gains across diverse tasks. Finally, we study whether difficult problems -- those yielding no RL signals and mixed-quality reasoning traces -- can still be effectively used for training. We introduce SparkleRL-PSS, a multi-stage RL pipeline that reuses hard problems with partial step scaffolding, guiding exploration effectively without additional data generation. Together, our findings provide a principled foundation for understanding how RL shapes model behavior, offering practical insights for building more adaptive, data-efficient, and interpretable RL pipelines for reasoning tasks. Our code, data, and checkpoints are available at: https://sparkle-reasoning.github.io/.

