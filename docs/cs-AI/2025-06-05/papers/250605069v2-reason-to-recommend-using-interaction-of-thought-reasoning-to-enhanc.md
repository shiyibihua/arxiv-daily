---
layout: default
title: Reason-to-Recommend: Using Interaction-of-Thought Reasoning to Enhance LLM Recommendation
---

# Reason-to-Recommend: Using Interaction-of-Thought Reasoning to Enhance LLM Recommendation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05069" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05069v2</a>
  <a href="https://arxiv.org/pdf/2506.05069.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05069v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05069v2', 'Reason-to-Recommend: Using Interaction-of-Thought Reasoning to Enhance LLM Recommendation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Keyu Zhao, Fengli Xu, Yong Li

**åˆ†ç±»**: cs.IR, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05 (æ›´æ–°: 2025-06-09)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºR2Recä»¥è§£å†³æ¨èç³»ç»Ÿä¸­éšå¼åé¦ˆçš„æ¨ç†é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ¨èç³»ç»Ÿ` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨ç†æœºåˆ¶` `éšå¼åé¦ˆ` `å¼ºåŒ–å­¦ä¹ ` `ä¸ªæ€§åŒ–æ¨è` `ç”¨æˆ·äº¤äº’`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ¨èç³»ç»Ÿæ–¹æ³•åœ¨å¤„ç†éšå¼ç”¨æˆ·åé¦ˆæ—¶ç¼ºä¹æœ‰æ•ˆçš„æ¨ç†æœºåˆ¶ï¼Œå¯¼è‡´æ¨èæ•ˆæœä¸ä½³ã€‚
2. æœ¬æ–‡æå‡ºR2Recæ¡†æ¶ï¼Œé€šè¿‡é€æ­¥æ©è”½æç¤ºç­–ç•¥ï¼Œå°†ç”¨æˆ·-ç‰©å“äº¤äº’è½¬åŒ–ä¸ºç»“æ„åŒ–çš„æ€ç»´äº¤äº’ï¼Œå¢å¼ºæ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒR2Recåœ¨å¤šä¸ªçœŸå®æ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†æ¨èæ€§èƒ½ï¼Œå°¤å…¶åœ¨HitRatio@1ä¸Šè¡¨ç°çªå‡ºã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›æ­¥ï¼Œå°†å…¶æ•´åˆåˆ°æ¨èä»»åŠ¡ä¸­å› å…¶å¼ºå¤§çš„è¯­ä¹‰ç†è§£å’Œçµæ´»çš„æç¤ºèƒ½åŠ›è€Œå—åˆ°å…³æ³¨ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é€šè¿‡ç¼–ç ç”¨æˆ·-ç‰©å“äº¤äº’æˆ–å…ƒæ•°æ®æ¥è¿›è¡Œæ¨èã€‚ç„¶è€Œï¼Œç›´æ¥å°†æ¨ç†æ–¹æ³•åº”ç”¨äºæ¨èç³»ç»Ÿæ•ˆæœä¸ä½³ï¼Œå› ä¸ºç”¨æˆ·åé¦ˆæ˜¯éšå¼çš„ä¸”ç¼ºä¹æ¨ç†ç›‘ç£ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†R2Recï¼Œä¸€ä¸ªå¢å¼ºæ¨ç†çš„æ¨èæ¡†æ¶ï¼Œé€šè¿‡é€æ­¥æ©è”½æç¤ºç­–ç•¥ä»ç”¨æˆ·-ç‰©å“å›¾ä¸­é‡‡æ ·äº¤äº’é“¾ï¼Œå°†å…¶è½¬æ¢ä¸ºç»“æ„åŒ–çš„æ€ç»´äº¤äº’ã€‚è¿™ä½¿å¾—LLMsèƒ½å¤ŸåŸºäºéšå¼æ¨¡å¼æ¨¡æ‹Ÿé€æ­¥å†³ç­–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒR2Recåœ¨ä¸‰ä¸ªçœŸå®æ•°æ®é›†ä¸Šè¶…è¶Šäº†ç»å…¸å’ŒåŸºäºLLMçš„åŸºçº¿ï¼ŒHitRatio@1å¹³å‡æå‡10.48%ï¼Œç›¸è¾ƒäºåŸå§‹LLMæå‡131.81%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ¨èç³»ç»Ÿä¸­éšå¼ç”¨æˆ·åé¦ˆçš„æ¨ç†é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆåˆ©ç”¨ç”¨æˆ·åé¦ˆçš„éšå¼æ€§ï¼Œå¯¼è‡´æ¨èæ•ˆæœä¸ç†æƒ³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šR2Recæ¡†æ¶é€šè¿‡é‡‡æ ·ç”¨æˆ·-ç‰©å“äº¤äº’é“¾ï¼Œåˆ©ç”¨é€æ­¥æ©è”½æç¤ºç­–ç•¥å°†å…¶è½¬åŒ–ä¸ºç»“æ„åŒ–çš„æ€ç»´äº¤äº’ï¼Œä»è€Œå¢å¼ºLLMsçš„æ¨ç†èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿæ¨¡æ‹Ÿé€æ­¥å†³ç­–è¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šR2Recçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µä¸ºç›‘ç£å¾®è°ƒï¼Œåˆ©ç”¨é«˜è´¨é‡çš„æ¨ç†è½¨è¿¹æ•™æˆåŸºæœ¬æ¨ç†ï¼›ç¬¬äºŒé˜¶æ®µä¸ºå¼ºåŒ–å­¦ä¹ ï¼Œé€šè¿‡å¥–åŠ±ä¿¡å·ä¼˜åŒ–æ¨ç†è¿‡ç¨‹ï¼Œç¼“è§£ç¨€ç–çš„æ˜¾å¼ç›‘ç£é—®é¢˜ã€‚

**å…³é”®åˆ›æ–°**ï¼šR2Recçš„ä¸»è¦åˆ›æ–°åœ¨äºå°†äº¤äº’é“¾è½¬åŒ–ä¸ºæ€ç»´äº¤äº’ï¼Œå…è®¸LLMsè¿›è¡Œé€æ­¥æ¨ç†ï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•çš„ç›´æ¥åé¦ˆå¤„ç†æ–¹å¼æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†é€æ­¥æ©è”½æç¤ºç­–ç•¥ï¼Œå¹¶ç»“åˆäº†å¼ºåŒ–å­¦ä¹ çš„å¥–åŠ±æœºåˆ¶ï¼Œä»¥æå‡æ¨ç†çš„å‡†ç¡®æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒR2Recåœ¨ä¸‰ä¸ªçœŸå®æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼ŒHitRatio@1å¹³å‡æå‡10.48%ï¼Œç›¸è¾ƒäºåŸå§‹LLMæå‡131.81%ã€‚è¿™äº›ç»“æœä¸ä»…è¯æ˜äº†R2Recçš„æœ‰æ•ˆæ€§ï¼Œä¹Ÿå±•ç¤ºäº†å…¶åœ¨æ¨èç³»ç»Ÿä¸­çš„æ½œåœ¨åº”ç”¨ä»·å€¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨åœºæ™¯åŒ…æ‹¬ç”µå•†æ¨èã€å†…å®¹æ¨èå’Œç¤¾äº¤åª’ä½“æ¨èç­‰é¢†åŸŸã€‚é€šè¿‡å¢å¼ºæ¨èç³»ç»Ÿçš„æ¨ç†èƒ½åŠ›ï¼ŒR2Recå¯ä»¥æä¾›æ›´ç²¾å‡†çš„ä¸ªæ€§åŒ–æ¨èï¼Œæå‡ç”¨æˆ·ä½“éªŒï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Driven by advances in Large Language Models (LLMs), integrating them into recommendation tasks has gained interest due to their strong semantic understanding and prompt flexibility. Prior work encoded user-item interactions or metadata into prompts for recommendations. In parallel, LLM reasoning, boosted by test-time scaling and reinforcement learning, has excelled in fields like mathematics and code, where reasoning traces and correctness signals are clear, enabling high performance and interpretability. However, directly applying these reasoning methods to recommendation is ineffective because user feedback is implicit and lacks reasoning supervision. To address this, we propose $\textbf{R2Rec}$, a reasoning-enhanced recommendation framework that samples interaction chains from the user-item graph and converts them into structured interaction-of-thoughts via a progressive masked prompting strategy, with each thought representing stepwise reasoning grounded in interaction context. This allows LLMs to simulate step-by-step decision-making based on implicit patterns. We design a two-stage training pipeline: supervised fine-tuning teaches basic reasoning from high-quality traces, and reinforcement learning refines reasoning via reward signals, alleviating sparse explicit supervision. Experiments on three real-world datasets show R2Rec outperforms classical and LLM-based baselines with an average $\textbf{10.48%}$ improvement in HitRatio@1 and $\textbf{131.81%}$ gain over the original LLM. Furthermore, the explicit reasoning chains enhance interpretability by revealing the decision process. Our code is available at: https://anonymous.4open.science/r/R2Rec-7C5D.

