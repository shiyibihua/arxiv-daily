---
layout: default
title: Sensory-Motor Control with Large Language Models via Iterative Policy Refinement
---

# Sensory-Motor Control with Large Language Models via Iterative Policy Refinement

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.04867" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.04867v3</a>
  <a href="https://arxiv.org/pdf/2506.04867.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.04867v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.04867v3', 'Sensory-Motor Control with Large Language Models via Iterative Policy Refinement')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: JÃ´nata Tyska Carvalho, Stefano Nolfi

**åˆ†ç±»**: cs.AI, cs.HC, cs.LG, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05 (æ›´æ–°: 2025-11-14)

**å¤‡æ³¨**: Article updated with results from gpt-oss:120b and gpt-oss:20b. 27 pages (13 pages are from appendix), 8 figures, 2 tables, code for experiments replication and supplementary material provided at https://github.com/jtyska/llm-robotics-article/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§æ–¹æ³•ä½¿å¤§å‹è¯­è¨€æ¨¡å‹æ§åˆ¶å…·èº«æ™ºèƒ½ä½“**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æ§åˆ¶ç­–ç•¥` `å…·èº«æ™ºèƒ½ä½“` `è¿­ä»£ä¼˜åŒ–` `æ„ŸçŸ¥-è¿åŠ¨æ•°æ®` `æœºå™¨äººæ§åˆ¶` `è‡ªåŠ¨é©¾é©¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨æ§åˆ¶å…·èº«æ™ºèƒ½ä½“æ—¶ï¼Œå¾€å¾€ä¾èµ–äºå¤æ‚çš„æ‰‹å·¥è®¾è®¡ç­–ç•¥ï¼Œç¼ºä¹çµæ´»æ€§å’Œé€‚åº”æ€§ã€‚
2. æœ¬æ–‡æå‡ºçš„è§£å†³æ–¹æ¡ˆé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆåˆæ­¥æ§åˆ¶ç­–ç•¥ï¼Œå¹¶é€šè¿‡åé¦ˆè¿­ä»£ä¼˜åŒ–ï¼Œæå‡äº†ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªç»å…¸æ§åˆ¶ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œèƒ½å¤Ÿæ‰¾åˆ°æ¥è¿‘æœ€ä¼˜çš„è§£å†³æ–¹æ¡ˆï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å¤Ÿé€šè¿‡ç”Ÿæˆæ§åˆ¶ç­–ç•¥æ¥æ§åˆ¶å…·èº«æ™ºèƒ½ä½“ï¼Œè¿™äº›ç­–ç•¥å°†è¿ç»­è§‚å¯Ÿå‘é‡ç›´æ¥æ˜ å°„åˆ°è¿ç»­åŠ¨ä½œå‘é‡ã€‚æœ€åˆï¼ŒLLMsåŸºäºæ™ºèƒ½ä½“ã€ç¯å¢ƒå’Œé¢„æœŸç›®æ ‡çš„æ–‡æœ¬æè¿°ç”Ÿæˆæ§åˆ¶ç­–ç•¥ã€‚éšåï¼Œé€šè¿‡å­¦ä¹ è¿‡ç¨‹å¯¹è¯¥ç­–ç•¥è¿›è¡Œè¿­ä»£ä¼˜åŒ–ï¼ŒLLMsåœ¨è¯„ä¼°è¿‡ç¨‹ä¸­ä½¿ç”¨æ€§èƒ½åé¦ˆå’Œæ„ŸçŸ¥-è¿åŠ¨æ•°æ®ä¸æ–­æ”¹è¿›å½“å‰ç­–ç•¥ã€‚è¯¥æ–¹æ³•åœ¨Gymnasiumåº“çš„ç»å…¸æ§åˆ¶ä»»åŠ¡å’ŒMuJoCoåº“çš„å€’ç«‹æ‘†ä»»åŠ¡ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œç»“æœè¡¨æ˜ï¼Œä½¿ç”¨ç›¸å¯¹ç´§å‡‘çš„æ¨¡å‹å¦‚GPT-oss:120bå’ŒQwen2.5:72bæ—¶ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«æœ€ä¼˜æˆ–è¿‘ä¼¼æœ€ä¼˜çš„è§£å†³æ–¹æ¡ˆï¼ŒæˆåŠŸåœ°å°†æ¨ç†å¾—å‡ºçš„ç¬¦å·çŸ¥è¯†ä¸æ™ºèƒ½ä½“ä¸ç¯å¢ƒäº¤äº’è¿‡ç¨‹ä¸­æ”¶é›†çš„å­ç¬¦å·æ„ŸçŸ¥-è¿åŠ¨æ•°æ®ç›¸ç»“åˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ§åˆ¶å…·èº«æ™ºèƒ½ä½“æ—¶çš„ç­–ç•¥ç”Ÿæˆå’Œä¼˜åŒ–é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºæ‰‹å·¥è®¾è®¡çš„æ§åˆ¶ç­–ç•¥ï¼Œç¼ºä¹çµæ´»æ€§å’Œé€‚åº”æ€§ï¼Œéš¾ä»¥åº”å¯¹å¤æ‚ç¯å¢ƒä¸­çš„å˜åŒ–ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆåˆæ­¥æ§åˆ¶ç­–ç•¥ï¼Œå¹¶é€šè¿‡è¿­ä»£å­¦ä¹ è¿‡ç¨‹ä¸æ–­ä¼˜åŒ–è¯¥ç­–ç•¥ã€‚é€šè¿‡å°†ç¬¦å·çŸ¥è¯†ä¸æ„ŸçŸ¥-è¿åŠ¨æ•°æ®ç»“åˆï¼Œæå‡äº†ç­–ç•¥çš„é€‚åº”æ€§å’Œæœ‰æ•ˆæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼š1) åˆæ­¥ç­–ç•¥ç”Ÿæˆæ¨¡å—ï¼ŒåŸºäºæ–‡æœ¬æè¿°ç”Ÿæˆæ§åˆ¶ç­–ç•¥ï¼›2) åé¦ˆæ”¶é›†æ¨¡å—ï¼Œåœ¨æ™ºèƒ½ä½“ä¸ç¯å¢ƒäº¤äº’ä¸­æ”¶é›†æ€§èƒ½åé¦ˆå’Œæ„ŸçŸ¥-è¿åŠ¨æ•°æ®ï¼›3) ç­–ç•¥ä¼˜åŒ–æ¨¡å—ï¼Œåˆ©ç”¨æ”¶é›†çš„æ•°æ®è¿­ä»£æ”¹è¿›æ§åˆ¶ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå°†å¤§å‹è¯­è¨€æ¨¡å‹ä¸æ„ŸçŸ¥-è¿åŠ¨æ•°æ®ç»“åˆï¼Œé€šè¿‡è¿­ä»£ä¼˜åŒ–å®ç°äº†ç­–ç•¥çš„åŠ¨æ€è°ƒæ•´ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„æ‰‹å·¥è®¾è®¡ç­–ç•¥æœ‰æœ¬è´¨åŒºåˆ«ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”å¤æ‚ç¯å¢ƒã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç´§å‡‘çš„è¯­è¨€æ¨¡å‹ï¼ˆå¦‚GPT-oss:120bå’ŒQwen2.5:72bï¼‰ï¼Œå¹¶é€šè¿‡ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥å¹³è¡¡ç­–ç•¥ç”Ÿæˆä¸åé¦ˆä¼˜åŒ–çš„è¿‡ç¨‹ï¼Œç¡®ä¿ç­–ç•¥çš„æœ‰æ•ˆæ€§å’Œç¨³å®šæ€§ã€‚å®éªŒä¸­è¿˜å¯¹æ¨¡å‹çš„è¶…å‚æ•°è¿›è¡Œäº†ç»†è‡´è°ƒä¼˜ï¼Œä»¥è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨Gymnasiumå’ŒMuJoCoåº“çš„ç»å…¸æ§åˆ¶ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œèƒ½å¤Ÿåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹æ‰¾åˆ°æœ€ä¼˜æˆ–è¿‘ä¼¼æœ€ä¼˜çš„è§£å†³æ–¹æ¡ˆã€‚ç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼Œä½¿ç”¨GPT-oss:120bå’ŒQwen2.5:72bæ¨¡å‹æ—¶ï¼Œç­–ç•¥çš„æœ‰æ•ˆæ€§æ˜¾è‘—æå‡ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„å®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½å®¶å±…ç­‰å…·èº«æ™ºèƒ½ä½“çš„æ§åˆ¶ä»»åŠ¡ã€‚é€šè¿‡æå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„é€‚åº”èƒ½åŠ›ï¼Œèƒ½å¤Ÿä¸ºå®é™…åº”ç”¨æä¾›æ›´çµæ´»å’Œé«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œæ¨åŠ¨æ™ºèƒ½ä½“æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We propose a method that enables large language models (LLMs) to control embodied agents through the generation of control policies that directly map continuous observation vectors to continuous action vectors. At the outset, the LLMs generate a control strategy based on a textual description of the agent, its environment, and the intended goal. This strategy is then iteratively refined through a learning process in which the LLMs are repeatedly prompted to improve the current strategy, using performance feedback and sensory-motor data collected during its evaluation. The method is validated on classic control tasks from the Gymnasium library and the inverted pendulum task from the MuJoCo library. The approach proves effective with relatively compact models such as GPT-oss:120b and Qwen2.5:72b. In most cases, it successfully identifies optimal or near-optimal solutions by integrating symbolic knowledge derived through reasoning with sub-symbolic sensory-motor data gathered as the agent interacts with its environment.

