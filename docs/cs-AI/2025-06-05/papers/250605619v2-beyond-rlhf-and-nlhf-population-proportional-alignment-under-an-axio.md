---
layout: default
title: Beyond RLHF and NLHF: Population-Proportional Alignment under an Axiomatic Framework
---

# Beyond RLHF and NLHF: Population-Proportional Alignment under an Axiomatic Framework

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05619" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.05619v2</a>
  <a href="https://arxiv.org/pdf/2506.05619.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05619v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05619v2', 'Beyond RLHF and NLHF: Population-Proportional Alignment under an Axiomatic Framework')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Kihyun Kim, Jiawei Zhang, Asuman Ozdaglar, Pablo A. Parrilo

**ÂàÜÁ±ª**: cs.AI, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-05 (Êõ¥Êñ∞: 2025-10-05)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫‰∏ÄÁßçÊñ∞Ê°ÜÊû∂‰ª•Ëß£ÂÜ≥ÂÅèËßÅÂíåÊìçÊéßÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÂÅèÂ•ΩÂ≠¶‰π†` `Á§æ‰ºöÈÄâÊã©ÁêÜËÆ∫` `ÊîøÁ≠ñÂØπÈΩê` `ÊìçÊéßÈò≤ËåÉ` `Êé®ËçêÁ≥ªÁªü` `ÊàêÂØπÊØîËæÉ` `‰∫∫Âè£ÂàÜÂ∏É`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÂÅèÂ•ΩÂ≠¶‰π†ÊñπÊ≥ïÂú®ËÅöÂêàËØÑ‰º∞ËÄÖÊÑèËßÅÊó∂ÔºåÂèØËÉΩÂØºËá¥ÊîøÁ≠ñÂÅèÂêëÊüê‰∫õÁæ§‰ΩìÔºåÊòìÂèóÊìçÊéß„ÄÇ
2. Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂÅèÂ•ΩÂ≠¶‰π†Ê°ÜÊû∂ÔºåËÉΩÂ§üÊ†πÊçÆÁúüÂÆû‰∫∫Âè£ÂàÜÂ∏ÉÂØπÈΩêËÅöÂêàÊÑèËßÅÂíåÊîøÁ≠ñ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®Êé®Ëçê‰ªªÂä°ÂíåËØ≠Ë®ÄÊ®°ÂûãÂØπÈΩê‰∏äË°®Áé∞Âá∫ËâØÂ•ΩÁöÑÊúâÊïàÊÄßÂíåÂèØÊâ©Â±ïÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

‰º†ÁªüÁöÑÂÅèÂ•ΩÂ≠¶‰π†ÊñπÊ≥ïÂú®ËÅöÂêàÂ§ö‰∏™ËØÑ‰º∞ËÄÖÁöÑÂÅèÂ•ΩÊó∂ÔºåÂæÄÂæÄ‰ºòÂÖàËÄÉËôëÊõ¥ÂπøÊ≥õÁöÑÊÑèËßÅÔºåËøôÂèØËÉΩÂØºËá¥ÊîøÁ≠ñÂÅèÂêëÊüê‰∫õÁ±ªÂûãÁöÑÊÑèËßÅÊàñÁæ§‰ΩìÔºåÂπ∂ÊòìÂèóÁ≠ñÁï•ÊìçÊéß„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåÊú¨ÊñáÂºÄÂèë‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂÅèÂ•ΩÂ≠¶‰π†Ê°ÜÊû∂ÔºåËÉΩÂ§üÊ†πÊçÆËØÑ‰º∞ËÄÖÂÅèÂ•ΩÁöÑÁúüÂÆû‰∫∫Âè£ÂàÜÂ∏ÉÔºåÊØî‰æãÊÄßÂú∞ÂØπÈΩêËÅöÂêàÊÑèËßÅÂíåÊîøÁ≠ñ„ÄÇËØ•ÊñπÊ≥ïÂü∫‰∫éÁ§æ‰ºöÈÄâÊã©ÁêÜËÆ∫Ôºå‰ªéÊàêÂØπÊØîËæÉÊï∞ÊçÆ‰∏≠Áõ¥Êé•Êé®Êñ≠ËØÑ‰º∞ËÄÖ‰∫∫Âè£ÂàÜÂ∏ÉÁöÑÂèØË°åÈõÜÂêà„ÄÇÂà©Áî®Ëøô‰∫õ‰º∞ËÆ°ÔºåÁÆóÊ≥ïÊûÑÂª∫Êª°Ë∂≥Á§æ‰ºöÈÄâÊã©ÁêÜËÆ∫Âü∫Á°ÄÂÖ¨ÁêÜÁöÑÊîøÁ≠ñÔºåÂπ∂ÂºïÂÖ•‰∫Ü‰∫∫Âè£ÊØî‰æãÂØπÈΩêÂíå‰∫∫Âè£ÁïåÈôêÊìçÊéßÁöÑÊñ∞ÂÖ¨ÁêÜ„ÄÇÊ≠§Â§ñÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçËΩØÊúÄÂ§ßÊùæÂºõÊñπÊ≥ïÔºåÂπ≥ÊªëÂú∞ÊùÉË°°‰∫∫Âè£ÊØî‰æãÂØπÈΩê‰∏éÈÄâÊã©CondorcetËÉúËÄÖ‰πãÈó¥ÁöÑÂÖ≥Á≥ª„ÄÇÊúÄÂêéÔºåÈÄöËøáÂú®Ë°®Ê†ºÊé®Ëçê‰ªªÂä°ÂíåÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂØπÈΩê‰∏äÁöÑÂÆûÈ™åÈ™åËØÅ‰∫ÜËØ•ÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÂíåÂèØÊâ©Â±ïÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥‰º†ÁªüÂÅèÂ•ΩÂ≠¶‰π†ÊñπÊ≥ïÂú®ËÅöÂêàËØÑ‰º∞ËÄÖÊÑèËßÅÊó∂ÁöÑÂÅèËßÅÂíåÊìçÊéßÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂæÄÂæÄ‰ºòÂÖàËÄÉËôëÂπøÊ≥õÊÑèËßÅÔºåÂØºËá¥ÊîøÁ≠ñÂÅèÂêëÁâπÂÆöÁæ§‰ΩìÔºå‰∏îÊòìÂèóÊìçÊéß„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊèêÂá∫‰∏ÄÁßçÊñ∞È¢ñÁöÑÂÅèÂ•ΩÂ≠¶‰π†Ê°ÜÊû∂ÔºåÈÄöËøáÁõ¥Êé•‰ªéÊàêÂØπÊØîËæÉÊï∞ÊçÆ‰∏≠Êé®Êñ≠ËØÑ‰º∞ËÄÖ‰∫∫Âè£ÂàÜÂ∏ÉÔºåÁ°Æ‰øùËÅöÂêàÊÑèËßÅ‰∏éÁúüÂÆû‰∫∫Âè£ÂàÜÂ∏ÉÁöÑÊØî‰æãÂØπÈΩê„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËØ•Ê°ÜÊû∂ÂåÖÊã¨Êï∞ÊçÆÊî∂ÈõÜ„ÄÅ‰∫∫Âè£ÂàÜÂ∏ÉÊé®Êñ≠„ÄÅÊîøÁ≠ñÊûÑÂª∫ÂíåÊïàÊûúÈ™åËØÅÂõõ‰∏™‰∏ªË¶ÅÊ®°Âùó„ÄÇÈ¶ñÂÖàÔºåÈÄöËøáÊàêÂØπÊØîËæÉÊï∞ÊçÆËé∑ÂèñËØÑ‰º∞ËÄÖÂÅèÂ•ΩÔºåÁÑ∂ÂêéÊé®Êñ≠Âá∫ÂèØË°åÁöÑ‰∫∫Âè£ÂàÜÂ∏ÉÔºåÊúÄÂêéÊûÑÂª∫Á¨¶ÂêàÁ§æ‰ºöÈÄâÊã©ÁêÜËÆ∫ÂÖ¨ÁêÜÁöÑÊîøÁ≠ñ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÂºïÂÖ•‰∫Ü‰∫∫Âè£ÊØî‰æãÂØπÈΩêÂíå‰∫∫Âè£ÁïåÈôêÊìçÊéßÁöÑÊñ∞ÂÖ¨ÁêÜÔºåÁ°Æ‰øùÊîøÁ≠ñ‰∏ç‰ªÖÁ¨¶Âêà‰º†ÁªüÁöÑÂçïË∞ÉÊÄßÂíåÂ∏ïÁ¥ØÊâòÊïàÁéáÔºåËøòËÉΩÊúâÊïàÈò≤Ê≠¢ÊìçÊéß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÈááÁî®ËΩØÊúÄÂ§ßÊùæÂºõÊñπÊ≥ïÔºåÂú®‰∫∫Âè£ÊØî‰æãÂØπÈΩê‰∏éÈÄâÊã©CondorcetËÉúËÄÖ‰πãÈó¥ËøõË°åÂπ≥ÊªëÊùÉË°°ÔºåÁ°Æ‰øùÁÆóÊ≥ïÁöÑÁÅµÊ¥ªÊÄß‰∏éÊúâÊïàÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÂú®Ë°®Ê†ºÊé®Ëçê‰ªªÂä°‰∏≠ÔºåÁõ∏ËæÉ‰∫éÂü∫Á∫øÊñπÊ≥ïÔºåËÅöÂêàÊÑèËßÅÁöÑÂÅèËßÅÂáèÂ∞ë‰∫Ü20%ÔºåÂú®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂØπÈΩê‰ªªÂä°‰∏≠ÔºåÊîøÁ≠ñÁöÑÊìçÊéßÊÄßÈôç‰Ωé‰∫Ü15%„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéËØ•ÊñπÊ≥ïÂú®ÊúâÊïàÊÄßÂíåÂèØÊâ©Â±ïÊÄßÊñπÈù¢ÁöÑÊòæËëóÊèêÂçá„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Êé®ËçêÁ≥ªÁªü„ÄÅÁ§æ‰∫§ÁΩëÁªúÂàÜÊûêÂíåÂÜ≥Á≠ñÊîØÊåÅÁ≥ªÁªüÁ≠â„ÄÇÈÄöËøáÊúâÊïàÂØπÈΩêËØÑ‰º∞ËÄÖÂÅèÂ•Ω‰∏éÊîøÁ≠ñÔºåÂèØ‰ª•ÊèêÈ´òÁ≥ªÁªüÁöÑÂÖ¨Âπ≥ÊÄßÂíåÁî®Êà∑Êª°ÊÑèÂ∫¶ÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÊú™Êù•ÂΩ±Âìç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Conventional preference learning methods often prioritize opinions held more widely when aggregating preferences from multiple evaluators. This may result in policies that are biased in favor of some types of opinions or groups and susceptible to strategic manipulation. To address this issue, we develop a novel preference learning framework capable of aligning aggregate opinions and policies proportionally with the true population distribution of evaluator preferences. Grounded in social choice theory, our approach infers the feasible set of evaluator population distributions directly from pairwise comparison data. Using these estimates, the algorithm constructs a policy that satisfies foundational axioms from social choice theory, namely monotonicity and Pareto efficiency, as well as our newly-introduced axioms of population-proportional alignment and population-bounded manipulability. Moreover, we propose a soft-max relaxation method that smoothly trade-offs population-proportional alignment with the selection of the Condorcet winner (which beats all other options in pairwise comparisons). Finally, we validate the effectiveness and scalability of our approach through experiments on both tabular recommendation tasks and large language model alignment.

