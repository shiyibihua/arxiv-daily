---
layout: default
title: MMTU: A Massive Multi-Task Table Understanding and Reasoning Benchmark
---

# MMTU: A Massive Multi-Task Table Understanding and Reasoning Benchmark

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05587" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05587v3</a>
  <a href="https://arxiv.org/pdf/2506.05587.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05587v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05587v3', 'MMTU: A Massive Multi-Task Table Understanding and Reasoning Benchmark')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Junjie Xing, Yeye He, Mengyu Zhou, Haoyu Dong, Shi Han, Lingjiao Chen, Dongmei Zhang, Surajit Chaudhuri, H. V. Jagadish

**åˆ†ç±»**: cs.AI, cs.CL, cs.DB, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05 (æ›´æ–°: 2025-11-25)

**å¤‡æ³¨**: Accepted at NeurIPS 2025; Code and data available at https://github.com/MMTU-Benchmark/MMTU and https://huggingface.co/datasets/MMTU-benchmark/MMTU

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/MMTU-Benchmark/MMTU) | [HUGGINGFACE](https://huggingface.co/datasets/MMTU-benchmark)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMMTUåŸºå‡†ä»¥è§£å†³è¡¨æ ¼ç†è§£ä¸æ¨ç†çš„è¯„ä¼°é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¡¨æ ¼ç†è§£` `æ¨ç†è¯„ä¼°` `å¤šä»»åŠ¡åŸºå‡†` `æ•°æ®åˆ†æ` `äººå·¥æ™ºèƒ½` `æœºå™¨å­¦ä¹ ` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è¡¨æ ¼ç›¸å…³ä»»åŠ¡è¯„ä¼°ä¸»è¦é›†ä¸­åœ¨NL-to-SQLå’Œè¡¨æ ¼é—®ç­”ï¼Œç¼ºä¹å¯¹ä¸“ä¸šç”¨æˆ·å¹¿æ³›ä»»åŠ¡çš„å…¨é¢ç†è§£ã€‚
2. æœ¬æ–‡æå‡ºMMTUåŸºå‡†ï¼Œæ¶µç›–28,000ä¸ªé—®é¢˜å’Œ25ä¸ªå¤æ‚è¡¨æ ¼ä»»åŠ¡ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹çš„è¡¨æ ¼ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å‰å‰æ²¿æ¨¡å‹åœ¨MMTUä¸Šçš„å¾—åˆ†ä»…ä¸º69%å’Œ57%ï¼Œè¡¨æ˜åœ¨è¡¨æ ¼ç†è§£å’Œæ¨ç†æ–¹é¢ä»æœ‰å¾ˆå¤§æ”¹è¿›ç©ºé—´ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¡¨æ ¼åŠå…¶åº”ç”¨åœ¨ç”µå­è¡¨æ ¼ã€æ•°æ®åº“å’Œè®¡ç®—ç¬”è®°æœ¬ç­‰é‡è¦ç°å®åœºæ™¯ä¸­å‘æŒ¥ç€å…³é”®ä½œç”¨ï¼Œç„¶è€Œç°æœ‰çš„åŸºå‡†è¯„ä¼°ä¸»è¦é›†ä¸­åœ¨NL-to-SQLå’Œè¡¨æ ¼é—®ç­”ç­‰ç‹­çª„ä»»åŠ¡ï¼Œç¼ºä¹å¯¹ä¸“ä¸šç”¨æˆ·é¢ä¸´çš„å¹¿æ³›å®é™…ä»»åŠ¡çš„å…¨é¢è¯„ä¼°ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†MMTUï¼Œä¸€ä¸ªåŒ…å«è¶…è¿‡28,000ä¸ªé—®é¢˜å’Œ25ä¸ªçœŸå®ä¸–ç•Œè¡¨æ ¼ä»»åŠ¡çš„å¤§è§„æ¨¡åŸºå‡†ï¼Œæ—¨åœ¨å…¨é¢è¯„ä¼°æ¨¡å‹åœ¨ç†è§£ã€æ¨ç†å’Œæ“ä½œçœŸå®è¡¨æ ¼æ–¹é¢çš„èƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå½“å‰å‰æ²¿æ¨¡å‹åœ¨MMTUä¸Šçš„è¡¨ç°ä»æœ‰æ˜¾è‘—æå‡ç©ºé—´ï¼ŒæœŸæœ›è¯¥åŸºå‡†æ¨åŠ¨ç»“æ„åŒ–æ•°æ®å¤„ç†å’Œåˆ†æåŸºç¡€æ¨¡å‹çš„å‘å±•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è¡¨æ ¼ç†è§£ä¸æ¨ç†ä»»åŠ¡è¯„ä¼°ä¸è¶³çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯ç¼ºä¹å¯¹ä¸“ä¸šç”¨æˆ·æ‰€é¢ä¸´çš„å¤æ‚ä»»åŠ¡çš„å…¨é¢è¯„ä¼°ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æ„å»ºMMTUåŸºå‡†ï¼Œæä¾›ä¸€ä¸ªåŒ…å«å¤šç§çœŸå®ä¸–ç•Œè¡¨æ ¼ä»»åŠ¡çš„å¤§è§„æ¨¡è¯„ä¼°å¹³å°ï¼Œä»¥å…¨é¢æµ‹è¯•æ¨¡å‹åœ¨è¡¨æ ¼ç†è§£ã€æ¨ç†å’Œæ“ä½œæ–¹é¢çš„èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMMTUåŸºå‡†ç”±28,000ä¸ªé—®é¢˜ç»„æˆï¼Œæ¶µç›–25ä¸ªä»»åŠ¡ï¼Œä»»åŠ¡è®¾è®¡åŸºäºè®¡ç®—æœºç§‘å­¦é¢†åŸŸæ•°åå¹´çš„ç ”ç©¶ï¼Œå¼ºè°ƒå¤æ‚çš„è¡¨æ ¼æ“ä½œã€‚è¯„ä¼°æµç¨‹åŒ…æ‹¬é—®é¢˜ç”Ÿæˆã€æ¨¡å‹æµ‹è¯•å’Œç»“æœåˆ†æç­‰ä¸»è¦æ¨¡å—ã€‚

**å…³é”®åˆ›æ–°**ï¼šMMTUçš„åˆ›æ–°åœ¨äºå…¶å¹¿æ³›çš„ä»»åŠ¡è¦†ç›–å’Œç»¼åˆè¯„ä¼°èƒ½åŠ›ï¼Œå¡«è¡¥äº†ç°æœ‰åŸºå‡†åœ¨è¡¨æ ¼ä»»åŠ¡è¯„ä¼°ä¸Šçš„ç©ºç™½ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹ä¸“ä¸šç”¨æˆ·çš„éœ€æ±‚ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡MMTUæ—¶ï¼Œå…³æ³¨ä»»åŠ¡çš„å¤šæ ·æ€§å’Œå¤æ‚æ€§ï¼Œç¡®ä¿é—®é¢˜èƒ½å¤Ÿæœ‰æ•ˆæµ‹è¯•æ¨¡å‹çš„è¡¨æ ¼ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶é‡‡ç”¨æ ‡å‡†åŒ–çš„è¯„ä¼°æŒ‡æ ‡ä»¥ä¾¿äºä¸åŒæ¨¡å‹é—´çš„æ¯”è¾ƒã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨MMTUåŸºå‡†çš„è¯„ä¼°ä¸­ï¼Œå‰æ²¿æ¨¡å‹å¦‚OpenAI GPT-5å’ŒDeepSeek R1çš„å¾—åˆ†åˆ†åˆ«ä¸º69%å’Œ57%ï¼Œæ˜¾ç¤ºå‡ºåœ¨è¡¨æ ¼ç†è§£å’Œæ¨ç†æ–¹é¢çš„æ˜¾è‘—æŒ‘æˆ˜ã€‚è¿™ä¸€ç»“æœå¼ºè°ƒäº†å½“å‰æ¨¡å‹åœ¨å¤„ç†å¤æ‚è¡¨æ ¼ä»»åŠ¡æ—¶çš„ä¸è¶³ï¼ŒæŒ‡å‡ºäº†æœªæ¥æ”¹è¿›çš„æ–¹å‘ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MMTUåŸºå‡†çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•°æ®åˆ†æã€æ•°æ®åº“ç®¡ç†å’Œæ™ºèƒ½åŠ©æ‰‹ç­‰åœºæ™¯ï¼Œèƒ½å¤Ÿå¸®åŠ©ç ”ç©¶äººå‘˜å’Œå¼€å‘è€…æ›´å¥½åœ°ç†è§£å’Œæå‡æ¨¡å‹åœ¨è¡¨æ ¼æ•°æ®å¤„ç†æ–¹é¢çš„èƒ½åŠ›ã€‚éšç€å¯¹è¡¨æ ¼æ•°æ®éœ€æ±‚çš„å¢åŠ ï¼Œè¯¥åŸºå‡†å°†æ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ï¼Œæå‡ç”¨æˆ·åœ¨å®é™…åº”ç”¨ä¸­çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Tables and table-based use cases play a crucial role in many important real-world applications, such as spreadsheets, databases, and computational notebooks, which traditionally require expert-level users like data engineers, data analysts, and database administrators to operate. Although LLMs have shown remarkable progress in working with tables (e.g., in spreadsheet and database copilot scenarios), comprehensive benchmarking of such capabilities remains limited. In contrast to an extensive and growing list of NLP benchmarks, evaluations of table-related tasks are scarce, and narrowly focus on tasks like NL-to-SQL and Table-QA, overlooking the broader spectrum of real-world tasks that professional users face. This gap limits our understanding and model progress in this important area.
>   In this work, we introduce MMTU, a large-scale benchmark with over 28K questions across 25 real-world table tasks, designed to comprehensively evaluate models ability to understand, reason, and manipulate real tables at the expert-level. These tasks are drawn from decades' worth of computer science research on tabular data, with a focus on complex table tasks faced by professional users. We show that MMTU require a combination of skills -- including table understanding, reasoning, and coding -- that remain challenging for today's frontier models, where even frontier reasoning models like OpenAI GPT-5 and DeepSeek R1 score only around 69\% and 57\% respectively, suggesting significant room for improvement. We highlight key findings in our evaluation using MMTU and hope that this benchmark drives further advances in understanding and developing foundation models for structured data processing and analysis.
>   Our code and data are available at https://github.com/MMTU-Benchmark/MMTU and https://huggingface.co/datasets/MMTU-benchmark/MMTU.

