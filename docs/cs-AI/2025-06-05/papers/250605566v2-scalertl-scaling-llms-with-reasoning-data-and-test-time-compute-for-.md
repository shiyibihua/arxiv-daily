---
layout: default
title: ScaleRTL: Scaling LLMs with Reasoning Data and Test-Time Compute for Accurate RTL Code Generation
---

# ScaleRTL: Scaling LLMs with Reasoning Data and Test-Time Compute for Accurate RTL Code Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05566" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05566v2</a>
  <a href="https://arxiv.org/pdf/2506.05566.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05566v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05566v2', 'ScaleRTL: Scaling LLMs with Reasoning Data and Test-Time Compute for Accurate RTL Code Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chenhui Deng, Yun-Da Tsai, Guan-Ting Liu, Zhongzhi Yu, Haoxing Ren

**åˆ†ç±»**: cs.AR, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-05 (æ›´æ–°: 2025-07-15)

**å¤‡æ³¨**: Accepted to MLCAD 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºScaleRTLä»¥è§£å†³RTLä»£ç ç”Ÿæˆä¸­çš„æ•°æ®ç“¶é¢ˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `RTLä»£ç ç”Ÿæˆ` `æ¨ç†èƒ½åŠ›` `æµ‹è¯•æ—¶è®¡ç®—` `æ•°æ®é›†ç­–åˆ’`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨RTLä»£ç ç”Ÿæˆä¸­é¢ä¸´é«˜è´¨é‡è®­ç»ƒæ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œé™åˆ¶äº†LLMsçš„æœ‰æ•ˆæ€§ã€‚
2. ScaleRTLé€šè¿‡ç­–åˆ’é•¿é“¾æ¨ç†è½¨è¿¹å’Œæµ‹è¯•æ—¶è®¡ç®—æ‰©å±•ï¼Œè§£å†³äº†æ•°æ®ç“¶é¢ˆå’Œæ¨ç†èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒScaleRTLåœ¨VerilogEvalå’ŒRTLLMä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåˆ†åˆ«è¶…è¶Š18ä¸ªåŸºçº¿æ¨¡å‹18.4%å’Œ12.7%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›æ­¥ä½¿å…¶åœ¨è½¯ä»¶ç¼–ç åŸºå‡†æµ‹è¯•ä¸­æ¥è¿‘äººç±»è¡¨ç°ï¼Œä½†åœ¨RTLä»£ç ç”Ÿæˆæ–¹é¢çš„æœ‰æ•ˆæ€§ä»ç„¶æœ‰é™ï¼Œä¸»è¦ç”±äºé«˜è´¨é‡è®­ç»ƒæ•°æ®çš„ç¨€ç¼ºã€‚å°½ç®¡ä¹‹å‰çš„ç ”ç©¶å¯¹LLMsè¿›è¡Œäº†å¾®è°ƒä»¥é€‚åº”RTLä»»åŠ¡ï¼Œä½†æœªèƒ½ä»æ ¹æœ¬ä¸Šå…‹æœæ•°æ®ç“¶é¢ˆï¼Œå¹¶ä¸”ç”±äºç¼ºä¹æ¨ç†èƒ½åŠ›ï¼Œæ— æ³•æ”¯æŒæµ‹è¯•æ—¶çš„æ‰©å±•ã€‚æœ¬æ–‡æå‡ºäº†ScaleRTLï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹RTLç¼–ç çš„æ¨ç†LLMï¼Œèƒ½å¤ŸåŒæ—¶æ‰©å±•é«˜è´¨é‡æ¨ç†æ•°æ®å’Œæµ‹è¯•æ—¶è®¡ç®—ã€‚æˆ‘ä»¬ç‰¹åˆ«ç­–åˆ’äº†ä¸€å¥—å¤šæ ·åŒ–çš„é•¿é“¾æ¨ç†è½¨è¿¹ï¼Œå¹³å‡æ¯ä¸ªè½¨è¿¹åŒ…å«56Kä¸ªæ ‡è®°ï¼Œå½¢æˆäº†ä¸€ä¸ªåŒ…å«35äº¿ä¸ªæ ‡è®°çš„æ•°æ®é›†ï¼Œæ•æ‰äº†ä¸°å¯Œçš„RTLçŸ¥è¯†ã€‚å¯¹è¿™ä¸€è¯­æ–™åº“è¿›è¡Œå¾®è°ƒåï¼ŒScaleRTLèƒ½å¤Ÿè¿›è¡Œæ·±åº¦RTLæ¨ç†ï¼Œå¹¶é€šè¿‡ä¸€ç§æ–°é¢–çš„æµ‹è¯•æ—¶æ‰©å±•ç­–ç•¥è¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨RTLä»£ç ç”Ÿæˆä¸­çš„æ•°æ®ç“¶é¢ˆé—®é¢˜ã€‚ç°æœ‰æ–¹æ³•è™½ç„¶ç»è¿‡å¾®è°ƒï¼Œä½†ä»æ— æ³•æœ‰æ•ˆåˆ©ç”¨é«˜è´¨é‡çš„æ¨ç†æ•°æ®ï¼Œä¸”ç¼ºä¹åœ¨æµ‹è¯•æ—¶çš„æ¨ç†æ‰©å±•èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šScaleRTLçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ç­–åˆ’ä¸°å¯Œçš„æ¨ç†æ•°æ®é›†å’Œå¼•å…¥æµ‹è¯•æ—¶è®¡ç®—æ‰©å±•ï¼Œæ¥æå‡æ¨¡å‹åœ¨RTLç¼–ç ä»»åŠ¡ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚è¿™æ ·çš„è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨ç”Ÿæˆä»£ç æ—¶è¿›è¡Œæ·±åº¦æ¨ç†å’Œè‡ªæˆ‘ä¿®æ­£ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šScaleRTLçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†ç­–åˆ’ã€æ¨¡å‹å¾®è°ƒå’Œæµ‹è¯•æ—¶æ¨ç†æ‰©å±•ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œç­–åˆ’å‡ºåŒ…å«ä¸°å¯Œæ¨ç†ä¿¡æ¯çš„æ•°æ®é›†ï¼›å…¶æ¬¡ï¼Œå¯¹é€šç”¨æ¨ç†æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼›æœ€åï¼Œé€šè¿‡è¿­ä»£åæ€å’Œè‡ªæˆ‘ä¿®æ­£æ¥æ‰©å±•æ¨ç†è¿‡ç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šScaleRTLçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶æ¨ç†èƒ½åŠ›çš„å¼•å…¥å’Œæµ‹è¯•æ—¶è®¡ç®—çš„æ‰©å±•ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„é™æ€æ¨ç†æ–¹å¼å½¢æˆäº†é²œæ˜å¯¹æ¯”ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒScaleRTLèƒ½å¤Ÿåœ¨ç”Ÿæˆä»£ç æ—¶è¿›è¡Œæ›´æ·±å±‚æ¬¡çš„æ¨ç†å’Œè‡ªæˆ‘æ ¡æ­£ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹å¾®è°ƒè¿‡ç¨‹ä¸­ï¼ŒScaleRTLä½¿ç”¨äº†åŒ…å«35äº¿ä¸ªæ ‡è®°çš„å¤šæ ·åŒ–æ•°æ®é›†ï¼Œå¹¶è®¾è®¡äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–æ¨ç†è¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œæ¨¡å‹çš„ç½‘ç»œç»“æ„ç»è¿‡è°ƒæ•´ï¼Œä»¥é€‚åº”é•¿é“¾æ¨ç†çš„éœ€æ±‚ã€‚é€šè¿‡è¿™äº›è®¾è®¡ï¼ŒScaleRTLåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—æå‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

ScaleRTLåœ¨VerilogEvalå’ŒRTLLMä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåˆ†åˆ«è¶…è¶Š18ä¸ªç«äº‰åŸºçº¿æ¨¡å‹18.4%å’Œ12.7%ã€‚è¿™äº›å®éªŒç»“æœè¡¨æ˜ï¼ŒScaleRTLåœ¨å¤„ç†å¤æ‚RTLç¼–ç ä»»åŠ¡æ—¶å…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ï¼ŒéªŒè¯äº†å…¶æ¨ç†èƒ½åŠ›å’Œæµ‹è¯•æ—¶æ‰©å±•ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

ScaleRTLçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨é›†æˆç”µè·¯è®¾è®¡å’Œç¡¬ä»¶æè¿°è¯­è¨€çš„è‡ªåŠ¨ç”Ÿæˆæ–¹é¢ã€‚éšç€å¯¹é«˜æ•ˆRTLä»£ç ç”Ÿæˆéœ€æ±‚çš„å¢åŠ ï¼ŒScaleRTLèƒ½å¤Ÿä¸ºå·¥ç¨‹å¸ˆæä¾›æ›´é«˜æ•ˆçš„å·¥å…·ï¼Œæå‡è®¾è®¡æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯å¯èƒ½ä¼šæ¨åŠ¨è‡ªåŠ¨åŒ–è®¾è®¡å·¥å…·çš„å‘å±•ï¼Œè¿›ä¸€æ­¥é™ä½äººåŠ›æˆæœ¬å’Œé”™è¯¯ç‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in large language models (LLMs) have enabled near-human performance on software coding benchmarks, but their effectiveness in RTL code generation remains limited due to the scarcity of high-quality training data. While prior efforts have fine-tuned LLMs for RTL tasks, they do not fundamentally overcome the data bottleneck and lack support for test-time scaling due to their non-reasoning nature. In this work, we introduce ScaleRTL, the first reasoning LLM for RTL coding that scales up both high-quality reasoning data and test-time compute. Specifically, we curate a diverse set of long chain-of-thought reasoning traces averaging 56K tokens each, resulting in a dataset of 3.5B tokens that captures rich RTL knowledge. Fine-tuning a general-purpose reasoning model on this corpus yields ScaleRTL that is capable of deep RTL reasoning. Subsequently, we further enhance the performance of ScaleRTL through a novel test-time scaling strategy that extends the reasoning process via iteratively reflecting on and self-correcting previous reasoning steps. Experimental results show that ScaleRTL achieves state-of-the-art performance on VerilogEval and RTLLM, outperforming 18 competitive baselines by up to 18.4% on VerilogEval and 12.7% on RTLLM.

