---
layout: default
title: Cognitive Workspace: Active Memory Management for LLMs -- An Empirical Study of Functional Infinite Context
---

# Cognitive Workspace: Active Memory Management for LLMs -- An Empirical Study of Functional Infinite Context

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.13171" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.13171v1</a>
  <a href="https://arxiv.org/pdf/2508.13171.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.13171v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.13171v1', 'Cognitive Workspace: Active Memory Management for LLMs -- An Empirical Study of Functional Infinite Context')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tao An

**åˆ†ç±»**: cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-08

**å¤‡æ³¨**: 13 pages, 1 figure, code available at https://github.com/tao-hpu/cognitive-workspace

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè®¤çŸ¥å·¥ä½œç©ºé—´ä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹çš„è®°å¿†ç®¡ç†é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `è®°å¿†ç®¡ç†` `è®¤çŸ¥ç§‘å­¦` `ä¸»åŠ¨è®°å¿†` `ä»»åŠ¡é©±åŠ¨ä¼˜åŒ–` `å±‚æ¬¡ç¼“å†²åŒº` `ä¿¡æ¯æ£€ç´¢` `è®¤çŸ¥å¢å¼º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è¢«åŠ¨æ£€ç´¢ç³»ç»Ÿæ— æ³•æœ‰æ•ˆç®¡ç†åŠ¨æ€å’Œä»»åŠ¡é©±åŠ¨çš„è®°å¿†ï¼Œå¯¼è‡´å¤§è¯­è¨€æ¨¡å‹åœ¨ä¸Šä¸‹æ–‡ç®¡ç†ä¸Šå­˜åœ¨æ˜¾è‘—ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºçš„è®¤çŸ¥å·¥ä½œç©ºé—´é€šè¿‡ä¸»åŠ¨è®°å¿†ç®¡ç†ã€å±‚æ¬¡è®¤çŸ¥ç¼“å†²åŒºå’Œä»»åŠ¡é©±åŠ¨çš„ä¸Šä¸‹æ–‡ä¼˜åŒ–ï¼Œæ¨¡æ‹Ÿäººç±»çš„è®¤çŸ¥æœºåˆ¶ã€‚
3. å®éªŒè¯æ˜ï¼Œè®¤çŸ¥å·¥ä½œç©ºé—´åœ¨è®°å¿†é‡ç”¨ç‡ä¸Šè¾¾åˆ°äº†58.6%ï¼Œç›¸æ¯”ä¼ ç»Ÿæ–¹æ³•æœ‰17-18%çš„æ•ˆç‡æå‡ï¼Œä¸”å…·æœ‰ç»Ÿè®¡å­¦æ˜¾è‘—æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡æœ€è¿‘åœ¨æ‰©å±•ä¸Šä¸‹æ–‡çª—å£æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸Šä¸‹æ–‡ç®¡ç†ä¸Šä»é¢ä¸´æ ¹æœ¬æ€§é™åˆ¶ã€‚æœ¬æ–‡æå‡ºäº†è®¤çŸ¥å·¥ä½œç©ºé—´è¿™ä¸€æ–°èŒƒå¼ï¼Œé€šè¿‡æ¨¡æ‹Ÿäººç±»å¤–éƒ¨è®°å¿†ä½¿ç”¨çš„è®¤çŸ¥æœºåˆ¶ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•ã€‚åŸºäºè®¤çŸ¥ç§‘å­¦çš„ç†è®ºåŸºç¡€ï¼Œæœ¬æ–‡å±•ç¤ºäº†å½“å‰è¢«åŠ¨æ£€ç´¢ç³»ç»Ÿæ— æ³•æ•æ‰äººç±»è®°å¿†ç®¡ç†çš„åŠ¨æ€å’Œä»»åŠ¡é©±åŠ¨ç‰¹æ€§ã€‚å®è¯éªŒè¯è¡¨æ˜ï¼Œè®¤çŸ¥å·¥ä½œç©ºé—´åœ¨è®°å¿†é‡ç”¨ç‡ä¸Šè¾¾åˆ°äº†58.6%ï¼Œç›¸æ¯”ä¼ ç»ŸRAGçš„0%æœ‰æ˜¾è‘—æå‡ï¼Œä¸”åœ¨å¤šç§ä»»åŠ¡ç±»å‹ä¸­å‡è¡¨ç°å‡ºç»Ÿè®¡å­¦ä¸Šçš„æ˜¾è‘—æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨ä¸Šä¸‹æ–‡ç®¡ç†ä¸­çš„æ ¹æœ¬æ€§é™åˆ¶ï¼Œç°æœ‰çš„è¢«åŠ¨æ£€ç´¢ç³»ç»Ÿæ— æ³•é€‚åº”åŠ¨æ€çš„ä»»åŠ¡éœ€æ±‚ï¼Œå¯¼è‡´è®°å¿†ç®¡ç†æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®¤çŸ¥å·¥ä½œç©ºé—´é€šè¿‡æ¨¡æ‹Ÿäººç±»çš„è®¤çŸ¥æœºåˆ¶ï¼Œé‡‡ç”¨ä¸»åŠ¨è®°å¿†ç®¡ç†å’Œä»»åŠ¡é©±åŠ¨çš„ä¸Šä¸‹æ–‡ä¼˜åŒ–ï¼Œæ—¨åœ¨æå‡è®°å¿†çš„é‡ç”¨ç‡å’Œç®¡ç†æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šä¸»åŠ¨è®°å¿†ç®¡ç†æ¨¡å—ã€å±‚æ¬¡è®¤çŸ¥ç¼“å†²åŒºå’Œä»»åŠ¡é©±åŠ¨çš„ä¸Šä¸‹æ–‡ä¼˜åŒ–æ¨¡å—ï¼Œå½¢æˆä¸€ä¸ªåŠ¨æ€é€‚åº”çš„è®°å¿†ç®¡ç†ç³»ç»Ÿã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºä¸»åŠ¨è®°å¿†ç®¡ç†å’Œå±‚æ¬¡è®¤çŸ¥ç¼“å†²åŒºçš„å¼•å…¥ï¼Œä½¿å¾—ç³»ç»Ÿèƒ½å¤Ÿåœ¨ä¸åŒä»»åŠ¡ä¸­åŠ¨æ€è°ƒæ•´ä¸Šä¸‹æ–‡ï¼Œæ˜¾è‘—æå‡äº†è®°å¿†çš„é‡ç”¨ç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„å‚æ•°è®¾ç½®ä»¥ä¼˜åŒ–ä¿¡æ¯çš„é€‰æ‹©å’Œå­˜å‚¨ï¼ŒåŒæ—¶å¼•å…¥äº†é€‚åº”æ€§æŸå¤±å‡½æ•°ä»¥æå‡æ¨¡å‹çš„å­¦ä¹ æ•ˆç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œè®¤çŸ¥å·¥ä½œç©ºé—´åœ¨è®°å¿†é‡ç”¨ç‡ä¸Šè¾¾åˆ°äº†58.6%ï¼Œç›¸æ¯”ä¼ ç»Ÿçš„æ£€ç´¢å¢å¼ºç”Ÿæˆæ–¹æ³•ï¼ˆ0%ï¼‰æœ‰æ˜¾è‘—æå‡ã€‚æ­¤å¤–ï¼Œå°½ç®¡æ“ä½œæ¬¡æ•°å¢åŠ äº†3.3å€ï¼Œä»å®ç°äº†17-18%çš„å‡€æ•ˆç‡æå‡ï¼Œä¸”åœ¨å¤šç§ä»»åŠ¡ç±»å‹ä¸­å‡è¡¨ç°å‡ºp < 0.001å’ŒCohen's d > 23çš„ç»Ÿè®¡å­¦æ˜¾è‘—æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è®¤çŸ¥å·¥ä½œç©ºé—´çš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ï¼ŒåŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€æ•™è‚²æŠ€æœ¯å’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡æœ‰æ•ˆç®¡ç†å’Œä¼˜åŒ–è®°å¿†ï¼Œèƒ½å¤Ÿæå‡ç³»ç»Ÿçš„å“åº”èƒ½åŠ›å’Œç”¨æˆ·ä½“éªŒï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿå‘æ›´é«˜å±‚æ¬¡çš„è®¤çŸ¥èƒ½åŠ›å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) face fundamental limitations in context management despite recent advances extending context windows to millions of tokens. We propose Cognitive Workspace, a novel paradigm that transcends traditional Retrieval-Augmented Generation (RAG) by emulating human cognitive mechanisms of external memory use. Drawing from cognitive science foundations including Baddeley's working memory model, Clark's extended mind thesis, and Hutchins' distributed cognition framework, we demonstrate that current passive retrieval systems fail to capture the dynamic, task-driven nature of human memory management. Our analysis of 2024-2025 developments reveals that while techniques like Infini-attention and StreamingLLM achieve impressive context lengths, they lack the metacognitive awareness and active planning capabilities essential for true cognitive extension. Cognitive Workspace addresses these limitations through three core innovations: (1) active memory management with deliberate information curation, (2) hierarchical cognitive buffers enabling persistent working states, and (3) task-driven context optimization that dynamically adapts to cognitive demands. Empirical validation demonstrates Cognitive Workspace achieves an average 58.6% memory reuse rate (ranging from 54-60% across different tasks) compared to 0% for traditional RAG, with 17-18% net efficiency gain despite 3.3x higher operation counts. Statistical analysis confirms these advantages with p < 0.001 and Cohen's d > 23 across multiple task types, establishing the first quantitative evidence for active memory superiority in LLM systems. We present a comprehensive theoretical framework synthesizing insights from 50+ recent papers, positioning Cognitive Workspace as a fundamental shift from information retrieval to genuine cognitive augmentation.

