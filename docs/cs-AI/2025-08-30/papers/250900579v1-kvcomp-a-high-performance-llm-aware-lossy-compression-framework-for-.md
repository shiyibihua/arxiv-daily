---
layout: default
title: KVComp: A High-Performance, LLM-Aware, Lossy Compression Framework for KV Cache
---

# KVComp: A High-Performance, LLM-Aware, Lossy Compression Framework for KV Cache

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.00579" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.00579v1</a>
  <a href="https://arxiv.org/pdf/2509.00579.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.00579v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.00579v1', 'KVComp: A High-Performance, LLM-Aware, Lossy Compression Framework for KV Cache')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Bo Jiang, Taolue Yang, Youyuan Liu, Chengming Zhang, Xubin He, Sian Jin

**ÂàÜÁ±ª**: cs.DC, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-08-30

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫KVComp‰ª•Ëß£ÂÜ≥ÈïøÊñáÊú¨ÁîüÊàê‰∏≠ÁöÑKVÁºìÂ≠òÁÆ°ÁêÜÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÈïøÊñáÊú¨ÁîüÊàê` `KVÁºìÂ≠ò` `ÊúâÊçüÂéãÁº©` `ÂÜÖÂ≠òÁÆ°ÁêÜ` `Êé®ÁêÜ‰ºòÂåñ` `TransformerÊ®°Âûã` `Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Èïø‰∏ä‰∏ãÊñáÊé®ÁêÜÁöÑKVÁºìÂ≠òÈúÄÊ±ÇÂ∑®Â§ßÔºåÁé∞ÊúâÊñπÊ≥ïÂú®ÂÜÖÂ≠òÁÆ°ÁêÜ‰∏äÂ≠òÂú®ÊòæËëó‰∏çË∂≥„ÄÇ
2. KVCompÊ°ÜÊû∂ÈÄöËøáÊñ∞ÂûãÊúâÊçüÂéãÁº©ÊäÄÊúØ‰ºòÂåñKVÁºìÂ≠òÔºåÂÖºÈ°æÂª∂ËøüÂíåÂêûÂêêÈáèÈúÄÊ±Ç„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåKVCompÂú®ÂÜÖÂ≠òÂáèÂ∞ëÁéá‰∏äÂπ≥ÂùáÊèêÈ´ò47%ÔºåÂπ∂‰∏îÂú®Êüê‰∫õÊÉÖÂÜµ‰∏ãÂä†ÈÄü‰∫ÜËÆ°ÁÆóÊÄßËÉΩ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Âü∫‰∫éTransformerÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Â§öÁßçÂÆûÈôÖÂ∫îÁî®‰∏≠Â±ïÁé∞Âá∫ÂçìË∂äÁöÑÊΩúÂäõ„ÄÇÁÑ∂ËÄåÔºåÈïø‰∏ä‰∏ãÊñáÊé®ÁêÜÈù¢‰∏¥ÁùÄÂÖ≥ÈîÆÁöÑÊåëÊàòÔºå‰∏ªË¶ÅÊòØÁî±‰∫éÈîÆÂÄºÔºàKVÔºâÁºìÂ≠òÁöÑÂ∑®Â§ßÂÜÖÂ≠òÈúÄÊ±ÇÔºåÈöèÁùÄÂ∫èÂàóÈïøÂ∫¶ÂíåÊâπÈáèÂ§ßÂ∞èÁöÑÂ¢ûÂä†ÔºåÂèØËÉΩËææÂà∞Â§ö‰∏™GB„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜKVCompÔºå‰∏Ä‰∏™ÈÄöÁî®‰∏îÈ´òÊïàÁöÑKVÁºìÂ≠òÁÆ°ÁêÜÊ°ÜÊû∂Ôºå‰∏ì‰∏∫ÈïøÊñáÊú¨ÁîüÊàê‰ºòÂåñÔºåËÉΩÂ§ü‰∏éÂª∂ËøüÊïèÊÑüÂíåÂêûÂêêÈáèÊïèÊÑüÁöÑÊé®ÁêÜÁ≥ªÁªüÂçèÂêåÂ∑•‰Ωú„ÄÇKVCompÈááÁî®‰∫Ü‰∏ìÈó®‰∏∫KVÁºìÂ≠òÊï∞ÊçÆÁâπÊÄßËÆæËÆ°ÁöÑÊñ∞ÂûãÊúâÊçüÂéãÁº©ÊäÄÊúØÔºåÁ≤æÂøÉÂÖ±ÂêåËÆæËÆ°‰∫ÜÂéãÁº©ÁÆóÊ≥ïÂíåÁ≥ªÁªüÊû∂ÊûÑ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåKVCompÂú®ÂÜÖÂ≠òÂáèÂ∞ëÁéá‰∏äÂπ≥ÂùáÊèêÈ´ò‰∫Ü47%ÔºåÊúÄÈ´òÂèØËææ83%Ôºå‰∏îÂá†‰πéÊ≤°ÊúâÊ®°ÂûãÁ≤æÂ∫¶‰∏ãÈôç„ÄÇÊ≠§Â§ñÔºåKVCompÂÆûÁé∞‰∫ÜÊûÅÈ´òÁöÑÊâßË°åÂêûÂêêÈáèÔºåÊúâÊïàÂáèÂ∞ë‰∫ÜËß£ÂéãÁº©ÂºÄÈîÄÔºåÂú®Êüê‰∫õÊÉÖÂÜµ‰∏ãÁîöËá≥Âä†ÈÄü‰∫ÜÁü©Èòµ-ÂêëÈáè‰πòÊ≥ïÊìç‰ΩúÔºåË∂ÖË∂ä‰∫ÜÂü∫‰∫écuBLASÁöÑÊ≥®ÊÑèÂäõÂÜÖÊ†∏ÔºåÂáèÂ∞ë‰∫ÜÊï∞ÊçÆÁßªÂä®„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥ÈïøÊñáÊú¨ÁîüÊàê‰∏≠KVÁºìÂ≠òÁöÑÂÜÖÂ≠òÁÆ°ÁêÜÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®Â§ÑÁêÜÈïø‰∏ä‰∏ãÊñáÊó∂ÔºåKVÁºìÂ≠òÁöÑÂÜÖÂ≠òÈúÄÊ±ÇÊÄ•ÂâßÂ¢ûÂä†ÔºåÂØºËá¥Á≥ªÁªüÊÄßËÉΩ‰∏ãÈôç„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöKVCompÈÄöËøáËÆæËÆ°‰∏ìÈó®ÈíàÂØπKVÁºìÂ≠òÊï∞ÊçÆÁâπÊÄßÁöÑÊúâÊçüÂéãÁº©ÊäÄÊúØÔºå‰ºòÂåñ‰∫ÜÂÜÖÂ≠ò‰ΩøÁî®ÊïàÁéá„ÄÇËØ•ÊñπÊ≥ïÂÖºÈ°æ‰∫ÜÂª∂ËøüÂíåÂêûÂêêÈáèÁöÑÈúÄÊ±ÇÔºåÁ°Æ‰øùÂú®È´òÊïàÊé®ÁêÜÁöÑÂêåÊó∂Ôºå‰øùÊåÅÊ®°ÂûãÁöÑËÆ°ÁÆóÊïàÁéá„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöKVCompÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨ÂéãÁº©ÁÆóÊ≥ïÂíåÁ≥ªÁªüÊû∂ÊûÑÁöÑÂçèÂêåËÆæËÆ°„ÄÇ‰∏ªË¶ÅÊ®°ÂùóÂåÖÊã¨Êï∞ÊçÆÈ¢ÑÂ§ÑÁêÜ„ÄÅÂéãÁº©ÁÆóÊ≥ï„ÄÅËß£ÂéãÁº©Ê®°ÂùóÂíåÊé®ÁêÜÂºïÊìéÔºåÁ°Æ‰øùÊï∞ÊçÆÂú®ÂêÑ‰∏™Èò∂ÊÆµÁöÑÈ´òÊïàÊµÅÂä®„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöKVCompÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÂÖ∂ÊúâÊçüÂéãÁº©ÊäÄÊúØÁöÑËÆæËÆ°ÔºåËÉΩÂ§üÂú®‰øùËØÅËæÉÈ´òÁöÑÂÜÖÂ≠òÂáèÂ∞ëÁéáÁöÑÂêåÊó∂ÔºåÂá†‰πé‰∏çÂΩ±ÂìçÊ®°ÂûãÁöÑÁ≤æÂ∫¶„ÄÇËøô‰∏ÄÂàõÊñ∞‰ΩøÂæóKVÁºìÂ≠òÁöÑÁÆ°ÁêÜÊõ¥Âä†ÁÅµÊ¥ªÂíåÈ´òÊïà„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆæËÆ°‰∏≠ÔºåKVCompÈááÁî®‰∫ÜÁâπÂÆöÁöÑÂéãÁº©ÂèÇÊï∞ÂíåÊçüÂ§±ÂáΩÊï∞Ôºå‰ª•ÈÄÇÂ∫îKVÁºìÂ≠òÁöÑÁâπÊÄß„ÄÇÂêåÊó∂ÔºåÁ≥ªÁªüÊû∂ÊûÑÁªèËøá‰ºòÂåñÔºåÂáèÂ∞ë‰∫ÜÊï∞ÊçÆÁßªÂä®ÔºåÊèêÈ´ò‰∫ÜËÆ°ÁÆóÊïàÁéá„ÄÇÂÖ∑‰ΩìÁöÑÁΩëÁªúÁªìÊûÑÂíåÂèÇÊï∞ËÆæÁΩÆÂú®ÂÆûÈ™å‰∏≠ËøõË°å‰∫ÜÈ™åËØÅ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåKVCompÂú®ÂÜÖÂ≠òÂáèÂ∞ëÁéá‰∏äÂπ≥ÂùáÊèêÈ´ò‰∫Ü47%ÔºåÊúÄÈ´òÂèØËææ83%ÔºåÂá†‰πéÊ≤°ÊúâÊ®°ÂûãÁ≤æÂ∫¶‰∏ãÈôç„ÄÇÊ≠§Â§ñÔºåKVCompÂú®ÊâßË°åÂêûÂêêÈáèÊñπÈù¢Ë°®Áé∞‰ºòÂºÇÔºåÊúâÊïàÈôç‰Ωé‰∫ÜËß£ÂéãÁº©ÂºÄÈîÄÔºåÂπ∂Âú®Êüê‰∫õÊÉÖÂÜµ‰∏ãÂä†ÈÄü‰∫ÜÁü©Èòµ-ÂêëÈáè‰πòÊ≥ïÊìç‰ΩúÔºåË∂ÖË∂ä‰∫Ü‰º†ÁªüÁöÑcuBLASÂü∫‰∫éÁöÑÊ≥®ÊÑèÂäõÂÜÖÊ†∏„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

KVCompÁöÑÁ†îÁ©∂ÊàêÊûúÂú®ÈïøÊñáÊú¨ÁîüÊàê„ÄÅËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÁ≠âÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõ„ÄÇÈÄöËøá‰ºòÂåñKVÁºìÂ≠òÁÆ°ÁêÜÔºåËØ•Ê°ÜÊû∂ËÉΩÂ§üÊòæËëóÊèêÈ´òÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊé®ÁêÜÊïàÁéáÔºåÈôç‰ΩéÂÜÖÂ≠òÈúÄÊ±ÇÔºåÈÄÇÁî®‰∫éÂÆûÊó∂Â∫îÁî®Âú∫ÊôØÔºåÂ¶ÇÂØπËØùÁ≥ªÁªüÂíåËá™Âä®ÊñáÊú¨ÁîüÊàê„ÄÇÊú™Êù•ÔºåKVCompÂèØËÉΩÊé®Âä®Êõ¥Â§öÈ´òÊïàÊé®ÁêÜÁ≥ªÁªüÁöÑÂºÄÂèëÔºåÊèêÂçáAIÊ®°ÂûãÁöÑÂÆûÈôÖÂ∫îÁî®ËÉΩÂäõ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Transformer-based large language models (LLMs) demonstrate impressive potential in various practical applications. However, long context inference poses a significant challenge due to the enormous memory requirements of the key-value (KV) cache, which can scale to multiple gigabytes as sequence length and batch size increase. In this paper, we present KVComp, a generic and efficient KV cache management framework optimized for long-text generation that synergistically works with both latency-critical and throughput-critical inference systems. KVComp employs novel lossy compression techniques specifically designed for KV cache data characteristics, featuring careful co-design of compression algorithms and system architecture. Our approach maintains compatibility with the growing nature of KV cache while preserving high computational efficiency. Experimental results show that KVComp achieves on average 47\% and up to 83\% higher memory reduction rate compared to existing methods with little/no model accuracy degradation. Furthermore, KVComp achieves extremely high execution throughput, effectively reducing decompression overhead and, in some cases, even accelerating the matrix-vector multiplication operation and outperform cuBLAS-based attention kernels with less data movement.

