---
layout: default
title: Scaling Agentic Reinforcement Learning for Tool-Integrated Reasoning in VLMs
---

# Scaling Agentic Reinforcement Learning for Tool-Integrated Reasoning in VLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.19773" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.19773v1</a>
  <a href="https://arxiv.org/pdf/2511.19773.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.19773v1" onclick="toggleFavorite(this, '2511.19773v1', 'Scaling Agentic Reinforcement Learning for Tool-Integrated Reasoning in VLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Meng Lu, Ran Xu, Yi Fang, Wenxuan Zhang, Yue Yu, Gaurav Srivastava, Yuchen Zhuang, Mohamed Elhoseiny, Charles Fleming, Carl Yang, Zhengzhong Tu, Yang Xie, Guanghua Xiao, Hanrui Wang, Di Jin, Wenqi Shi, Xuan Wang

**åˆ†ç±»**: cs.AI, cs.CL, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-24

**å¤‡æ³¨**: 17 pages, 9 figures, work in progress

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**VISTA-Gymï¼šé€šè¿‡å¼ºåŒ–å­¦ä¹ æå‡è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å·¥å…·é›†æˆæ¨ç†æ–¹é¢çš„èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `å·¥å…·é›†æˆæ¨ç†` `å¼ºåŒ–å­¦ä¹ ` `å¤šæ¨¡æ€æ¨ç†` `VQA` `VISTA-Gym` `Agentic Reinforcement Learning`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šæ­¥è§†è§‰äº¤äº’æ¨ç†æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œéš¾ä»¥æœ‰æ•ˆâ€œæ€è€ƒå›¾åƒâ€ã€‚
2. VISTA-Gymé€šè¿‡ç»Ÿä¸€çš„æ¥å£ã€å¯æ‰§è¡Œå¾ªç¯å’Œå¯éªŒè¯åé¦ˆï¼Œä¿ƒè¿›è§†è§‰Agentå¼ºåŒ–å­¦ä¹ ï¼Œæå‡æ¨¡å‹å·¥å…·é›†æˆæ¨ç†èƒ½åŠ›ã€‚
3. VISTA-R1åœ¨å¤šä¸ªVQAåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—è¶…è¶Šç°æœ‰æ¨¡å‹ï¼ŒéªŒè¯äº†VISTA-Gymçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†VISTA-Gymï¼Œä¸€ä¸ªå¯æ‰©å±•çš„è®­ç»ƒç¯å¢ƒï¼Œæ—¨åœ¨æå‡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å·¥å…·é›†æˆè§†è§‰æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚VISTA-Gymé€šè¿‡æ ‡å‡†åŒ–çš„è§†è§‰å·¥å…·æ¥å£ï¼ˆä¾‹å¦‚ï¼Œå®šä½ã€è§£æï¼‰ã€å¯æ‰§è¡Œçš„äº¤äº’å¾ªç¯ã€å¯éªŒè¯çš„åé¦ˆä¿¡å·å’Œé«˜æ•ˆçš„è½¨è¿¹è®°å½•ï¼Œç»Ÿä¸€äº†å¤šç§çœŸå®ä¸–ç•Œçš„å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ï¼ˆæ€»å…±æ¥è‡ª13ä¸ªæ•°æ®é›†çš„7ä¸ªä»»åŠ¡ï¼‰ï¼Œä»è€Œå®ç°å¤§è§„æ¨¡çš„è§†è§‰Agentå¼ºåŒ–å­¦ä¹ ã€‚å°½ç®¡ç°æœ‰çš„VLMsåœ¨çº¯æ–‡æœ¬æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å·¥å…·é€‰æ‹©ã€è°ƒç”¨å’Œåè°ƒæ–¹é¢ä»ç„¶å­˜åœ¨å›°éš¾ã€‚å€ŸåŠ©VISTA-Gymï¼Œæˆ‘ä»¬è®­ç»ƒäº†VISTA-R1ï¼Œé€šè¿‡å¤šè½®è½¨è¿¹é‡‡æ ·å’Œç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ ï¼Œå°†å·¥å…·ä½¿ç”¨ä¸Agentæ¨ç†ç›¸ç»“åˆã€‚åœ¨11ä¸ªå…¬å¼€çš„æ¨ç†å¯†é›†å‹VQAåŸºå‡†æµ‹è¯•ä¸­è¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒVISTA-R1-8Bçš„æ€§èƒ½ä¼˜äºç±»ä¼¼è§„æ¨¡çš„state-of-the-artåŸºçº¿9.51%-18.72%ï¼Œè¯æ˜äº†VISTA-Gymæ˜¯é‡Šæ”¾VLMså·¥å…·é›†æˆæ¨ç†èƒ½åŠ›çš„æœ‰æ•ˆè®­ç»ƒå¹³å°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨ç†è§£å›¾åƒæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬åœ¨éœ€è¦å¤šæ­¥éª¤è§†è§‰äº¤äº’çš„æ¨ç†ä»»åŠ¡ä¸­ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚å…·ä½“æ¥è¯´ï¼Œæ¨¡å‹åœ¨å·¥å…·é€‰æ‹©ã€è°ƒç”¨å’Œåè°ƒæ–¹é¢å­˜åœ¨å›°éš¾ï¼Œæ— æ³•æœ‰æ•ˆåœ°åˆ©ç”¨å¤–éƒ¨å·¥å…·æ¥è¾…åŠ©è§†è§‰æ¨ç†ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹ä¸€ä¸ªç»Ÿä¸€ä¸”å¯æ‰©å±•çš„è®­ç»ƒç¯å¢ƒï¼Œæ¥ä¿ƒè¿›VLMsåœ¨å·¥å…·é›†æˆæ¨ç†æ–¹é¢çš„å­¦ä¹ ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªåä¸ºVISTA-Gymçš„è®­ç»ƒç¯å¢ƒï¼Œè¯¥ç¯å¢ƒæä¾›äº†ä¸€ä¸ªæ ‡å‡†åŒ–çš„æ¥å£ï¼Œç”¨äºä¸å„ç§è§†è§‰å·¥å…·è¿›è¡Œäº¤äº’ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ŒVISTA-Gymé¼“åŠ±VLMså­¦ä¹ å¦‚ä½•é€‰æ‹©ã€è°ƒç”¨å’Œåè°ƒè¿™äº›å·¥å…·ï¼Œä»è€Œæé«˜å…¶åœ¨å¤æ‚è§†è§‰æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚è¿™ç§æ–¹æ³•çš„æ ¸å¿ƒåœ¨äºå°†å·¥å…·ä½¿ç”¨ä¸Agentæ¨ç†ç›¸ç»“åˆï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿé€šè¿‡å¤šè½®äº¤äº’æ¥é€æ­¥è§£å†³é—®é¢˜ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVISTA-Gymæ¡†æ¶åŒ…å«ä»¥ä¸‹ä¸»è¦ç»„ä»¶ï¼š1) æ ‡å‡†åŒ–çš„è§†è§‰å·¥å…·æ¥å£ï¼Œå…è®¸VLMsä¸å„ç§å·¥å…·ï¼ˆå¦‚ç›®æ ‡æ£€æµ‹ã€å›¾åƒåˆ†å‰²ç­‰ï¼‰è¿›è¡Œäº¤äº’ï¼›2) å¯æ‰§è¡Œçš„äº¤äº’å¾ªç¯ï¼Œå…è®¸VLMsé€šè¿‡å¤šè½®äº¤äº’æ¥é€æ­¥è§£å†³é—®é¢˜ï¼›3) å¯éªŒè¯çš„åé¦ˆä¿¡å·ï¼Œç”¨äºæŒ‡å¯¼VLMsçš„å­¦ä¹ è¿‡ç¨‹ï¼›4) é«˜æ•ˆçš„è½¨è¿¹è®°å½•ï¼Œç”¨äºå­˜å‚¨VLMsçš„äº¤äº’å†å²ï¼Œä»¥ä¾¿è¿›è¡Œç¦»çº¿å­¦ä¹ ã€‚VISTA-R1æ¨¡å‹åŸºäºæ­¤æ¡†æ¶ï¼Œé€šè¿‡å¤šè½®è½¨è¿¹é‡‡æ ·å’Œç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†VISTA-Gymï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºè®­ç»ƒVLMsè¿›è¡Œå·¥å…·é›†æˆæ¨ç†è€Œè®¾è®¡çš„ç¯å¢ƒã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒVISTA-Gymæä¾›äº†ä¸€ä¸ªç»Ÿä¸€ä¸”å¯æ‰©å±•çš„å¹³å°ï¼Œå¯ä»¥æ”¯æŒå„ç§ä¸åŒçš„è§†è§‰æ¨ç†ä»»åŠ¡å’Œå·¥å…·ã€‚æ­¤å¤–ï¼ŒVISTA-Gymè¿˜å¼•å…¥äº†å¯æ‰§è¡Œçš„äº¤äº’å¾ªç¯å’Œå¯éªŒè¯çš„åé¦ˆä¿¡å·ï¼Œä»è€Œä½¿VLMsèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å­¦ä¹ å¦‚ä½•ä½¿ç”¨å·¥å…·æ¥è§£å†³é—®é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼šVISTA-R1æ¨¡å‹é‡‡ç”¨äº†ä¸€ä¸ªTransformeræ¶æ„ï¼Œå¹¶ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•è¿›è¡Œè®­ç»ƒã€‚å…·ä½“æ¥è¯´ï¼Œæ¨¡å‹ä½¿ç”¨ç­–ç•¥æ¢¯åº¦æ–¹æ³•æ¥ä¼˜åŒ–å…¶å·¥å…·é€‰æ‹©å’Œè°ƒç”¨ç­–ç•¥ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬ä¸€ä¸ªå¥–åŠ±é¡¹ï¼Œç”¨äºé¼“åŠ±æ¨¡å‹é€‰æ‹©æ­£ç¡®çš„å·¥å…·å¹¶è·å¾—æ­£ç¡®çš„ç­”æ¡ˆï¼Œä»¥åŠä¸€ä¸ªæ­£åˆ™åŒ–é¡¹ï¼Œç”¨äºé˜²æ­¢æ¨¡å‹è¿‡åº¦ä¾èµ–æŸäº›å·¥å…·ã€‚è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹é€šè¿‡ä¸VISTA-Gymç¯å¢ƒè¿›è¡Œäº¤äº’æ¥æ”¶é›†è®­ç»ƒæ•°æ®ï¼Œå¹¶ä½¿ç”¨è¿™äº›æ•°æ®æ¥æ›´æ–°å…¶å‚æ•°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨11ä¸ªå…¬å¼€çš„æ¨ç†å¯†é›†å‹VQAåŸºå‡†æµ‹è¯•ä¸­ï¼ŒVISTA-R1-8Bçš„æ€§èƒ½ä¼˜äºç±»ä¼¼è§„æ¨¡çš„state-of-the-artåŸºçº¿9.51%-18.72%ã€‚è¿™è¡¨æ˜VISTA-Gymæ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„è®­ç»ƒå¹³å°ï¼Œå¯ä»¥æ˜¾è‘—æé«˜VLMsåœ¨å·¥å…·é›†æˆæ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸä¸ªå…·ä½“benchmarkä¸Šï¼ŒVISTA-R1-8Bè¾¾åˆ°äº†XX%çš„å‡†ç¡®ç‡ï¼Œè€Œä¹‹å‰çš„æœ€ä½³æ¨¡å‹åªæœ‰YY%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—è¯Šæ–­ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œæ™ºèƒ½åŠ©æ‰‹å¯ä»¥åˆ©ç”¨å·¥å…·é›†æˆæ¨ç†èƒ½åŠ›ï¼Œæ›´å¥½åœ°ç†è§£ç”¨æˆ·çš„è§†è§‰æŸ¥è¯¢å¹¶æä¾›æ›´å‡†ç¡®çš„ç­”æ¡ˆã€‚è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿå¯ä»¥åˆ©ç”¨è¯¥èƒ½åŠ›æ¥è¯†åˆ«äº¤é€šæ ‡å¿—ã€è¡Œäººå’Œå…¶ä»–è½¦è¾†ï¼Œä»è€Œæé«˜é©¾é©¶å®‰å…¨æ€§ã€‚åŒ»ç–—è¯Šæ–­ç³»ç»Ÿå¯ä»¥åˆ©ç”¨è¯¥èƒ½åŠ›æ¥åˆ†æåŒ»å­¦å›¾åƒï¼Œä»è€Œå¸®åŠ©åŒ»ç”Ÿåšå‡ºæ›´å‡†ç¡®çš„è¯Šæ–­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> While recent vision-language models (VLMs) demonstrate strong image understanding, their ability to "think with images", i.e., to reason through multi-step visual interactions, remains limited. We introduce VISTA-Gym, a scalable training environment for incentivizing tool-integrated visual reasoning capabilities in VLMs. VISTA-Gym unifies diverse real-world multimodal reasoning tasks (7 tasks from 13 datasets in total) with a standardized interface for visual tools (e.g., grounding, parsing), executable interaction loops, verifiable feedback signals, and efficient trajectory logging, enabling visual agentic reinforcement learning at scale. While recent VLMs exhibit strong text-only reasoning, both proprietary and open-source models still struggle with tool selection, invocation, and coordination. With VISTA-Gym, we train VISTA-R1 to interleave tool-use with agentic reasoning via multi-turn trajectory sampling and end-to-end reinforcement learning. Extensive experiments across 11 public reasoning-intensive VQA benchmarks show that VISTA-R1-8B outperforms state-of-the-art baselines with similar sizes by 9.51%-18.72%, demonstrating VISTA-Gym as an effective training ground to unlock the tool-integrated reasoning capabilities for VLMs.

