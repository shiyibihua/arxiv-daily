---
layout: default
title: TICL: Text-Embedding KNN For Speech In-Context Learning Unlocks Speech Recognition Abilities of Large Multimodal Models
---

# TICL: Text-Embedding KNN For Speech In-Context Learning Unlocks Speech Recognition Abilities of Large Multimodal Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.13395" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.13395v1</a>
  <a href="https://arxiv.org/pdf/2509.13395.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.13395v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.13395v1', 'TICL: Text-Embedding KNN For Speech In-Context Learning Unlocks Speech Recognition Abilities of Large Multimodal Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Haolong Zheng, Yekaterina Yegorova, Mark Hasegawa-Johnson

**åˆ†ç±»**: eess.AS, cs.AI, cs.CL, cs.LG, cs.MM

**å‘å¸ƒæ—¥æœŸ**: 2025-09-16

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**TICLï¼šæ–‡æœ¬åµŒå…¥KNNç”¨äºè¯­éŸ³ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œè§£é”å¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„è¯­éŸ³è¯†åˆ«èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­éŸ³ä¸Šä¸‹æ–‡å­¦ä¹ ` `æ–‡æœ¬åµŒå…¥` `KNNç®—æ³•` `å¤šæ¨¡æ€æ¨¡å‹` `è¯­éŸ³è¯†åˆ«` `è¯­ä¹‰ç›¸ä¼¼æ€§` `é›¶æ ·æœ¬å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è¯­éŸ³ä¸Šä¸‹æ–‡å­¦ä¹ (SICL)ä¾èµ–äºæœ‰æ•ˆçš„ä¸Šä¸‹æ–‡ç¤ºä¾‹é€‰æ‹©ï¼Œä½†ç°æœ‰æ–¹æ³•å¯¹æ­¤æ¢ç´¢ä¸è¶³ã€‚
2. TICLåˆ©ç”¨æ–‡æœ¬åµŒå…¥KNNï¼Œé€šè¿‡è¯­ä¹‰ä¸Šä¸‹æ–‡å¢å¼ºå¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„è¯­éŸ³è¯†åˆ«èƒ½åŠ›ï¼Œæ— éœ€å¾®è°ƒã€‚
3. å®éªŒè¡¨æ˜ï¼ŒTICLåœ¨å¤šç§è¯­éŸ³è¯†åˆ«ä»»åŠ¡ä¸­æ˜¾è‘—è¶…è¶Šé›¶æ ·æœ¬æ€§èƒ½ï¼ŒWERé™ä½é«˜è¾¾84.7%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¯­éŸ³åŸºç¡€æ¨¡å‹æœ€è¿‘å±•ç¤ºäº†æ‰§è¡Œè¯­éŸ³ä¸Šä¸‹æ–‡å­¦ä¹ (SICL)çš„èƒ½åŠ›ã€‚é€‰æ‹©æœ‰æ•ˆçš„ä¸Šä¸‹æ–‡ç¤ºä¾‹å¯¹äºSICLæ€§èƒ½è‡³å…³é‡è¦ï¼Œä½†é€‰æ‹©æ–¹æ³•ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡æå‡ºäº†ç”¨äºSICLçš„æ–‡æœ¬åµŒå…¥KNN(TICL)ï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•çš„æµç¨‹ï¼Œå®ƒä½¿ç”¨è¯­ä¹‰ä¸Šä¸‹æ–‡æ¥å¢å¼ºç°æˆçš„(off-the-shelf)å¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„è¯­éŸ³è¯†åˆ«èƒ½åŠ›ï¼Œè€Œæ— éœ€è¿›è¡Œå¾®è°ƒã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ä»»åŠ¡ä¸­ï¼ŒåŒ…æ‹¬å£éŸ³è‹±è¯­ã€å¤šè¯­ç§è¯­éŸ³å’Œå„¿ç«¥è¯­éŸ³ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä½¿æ¨¡å‹èƒ½å¤Ÿè¶…è¶Šé›¶æ ·æœ¬æ€§èƒ½ï¼Œç›¸å¯¹è¯é”™è¯¯ç‡(WER)é™ä½é«˜è¾¾84.7%ã€‚æˆ‘ä»¬è¿›è¡Œäº†æ¶ˆèç ”ç©¶ï¼Œä»¥è¯æ˜æˆ‘ä»¬æ–¹æ³•çš„ç¨³å¥æ€§å’Œæ•ˆç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³è¯­éŸ³ä¸Šä¸‹æ–‡å­¦ä¹ (SICL)ä¸­ä¸Šä¸‹æ–‡ç¤ºä¾‹é€‰æ‹©çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨é€‰æ‹©åˆé€‚çš„ä¸Šä¸‹æ–‡ç¤ºä¾‹æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå¯¼è‡´SICLçš„æ€§èƒ½å—é™ã€‚ç¼ºä¹æœ‰æ•ˆçš„ç¤ºä¾‹é€‰æ‹©ç­–ç•¥æ˜¯ç°æœ‰æ–¹æ³•çš„ç—›ç‚¹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ–‡æœ¬åµŒå…¥KNNæ¥é€‰æ‹©ä¸ç›®æ ‡è¯­éŸ³åœ¨è¯­ä¹‰ä¸Šæœ€ç›¸å…³çš„ä¸Šä¸‹æ–‡ç¤ºä¾‹ã€‚é€šè¿‡å°†è¯­éŸ³è½¬å½•æ–‡æœ¬åµŒå…¥åˆ°è¯­ä¹‰ç©ºé—´ä¸­ï¼Œå¹¶ä½¿ç”¨KNNç®—æ³•æ‰¾åˆ°æœ€ç›¸ä¼¼çš„ç¤ºä¾‹ï¼Œä»è€Œæé«˜SICLçš„æ€§èƒ½ã€‚è¿™ç§æ–¹æ³•åŸºäºè¯­ä¹‰ç›¸ä¼¼æ€§ï¼Œèƒ½å¤Ÿæ›´å‡†ç¡®åœ°é€‰æ‹©åˆé€‚çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTICLçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦é˜¶æ®µï¼š1) å°†ç›®æ ‡è¯­éŸ³å’Œå€™é€‰ä¸Šä¸‹æ–‡è¯­éŸ³è¿›è¡Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«(ASR)ï¼Œå¾—åˆ°å¯¹åº”çš„æ–‡æœ¬è½¬å½•ï¼›2) ä½¿ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åµŒå…¥æ¨¡å‹ï¼ˆå¦‚Sentence-BERTï¼‰å°†æ–‡æœ¬è½¬å½•åµŒå…¥åˆ°è¯­ä¹‰ç©ºé—´ä¸­ï¼›3) ä½¿ç”¨KNNç®—æ³•åœ¨è¯­ä¹‰ç©ºé—´ä¸­æ‰¾åˆ°ä¸ç›®æ ‡è¯­éŸ³æœ€ç›¸ä¼¼çš„Kä¸ªä¸Šä¸‹æ–‡ç¤ºä¾‹ï¼›4) å°†é€‰æ‹©çš„ä¸Šä¸‹æ–‡ç¤ºä¾‹ä¸ç›®æ ‡è¯­éŸ³ä¸€èµ·è¾“å…¥åˆ°å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ä¸­è¿›è¡Œè¯­éŸ³è¯†åˆ«ã€‚

**å…³é”®åˆ›æ–°**ï¼šTICLçš„å…³é”®åˆ›æ–°åœ¨äºä½¿ç”¨æ–‡æœ¬åµŒå…¥KNNæ¥æŒ‡å¯¼ä¸Šä¸‹æ–‡ç¤ºä¾‹çš„é€‰æ‹©ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºè¯­éŸ³ç‰¹å¾æˆ–éšæœºé€‰æ‹©çš„æ–¹æ³•ç›¸æ¯”ï¼ŒTICLèƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨è¯­ä¹‰ä¿¡æ¯ï¼Œä»è€Œé€‰æ‹©æ›´ç›¸å…³çš„ä¸Šä¸‹æ–‡ç¤ºä¾‹ã€‚è¿™ç§æ–¹æ³•æ— éœ€å¯¹å¤§å‹å¤šæ¨¡æ€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå³å¯æ˜¾è‘—æé«˜è¯­éŸ³è¯†åˆ«æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨Sentence-BERTç­‰é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œæ–‡æœ¬åµŒå…¥ï¼Œä»¥è·å¾—é«˜è´¨é‡çš„è¯­ä¹‰è¡¨ç¤ºï¼›2) ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ä½œä¸ºKNNç®—æ³•çš„è·ç¦»åº¦é‡ï¼Œä»¥è¡¡é‡æ–‡æœ¬åµŒå…¥ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼›3) é€šè¿‡æ¶ˆèå®éªŒé€‰æ‹©åˆé€‚çš„Kå€¼ï¼ˆå³é€‰æ‹©çš„ä¸Šä¸‹æ–‡ç¤ºä¾‹æ•°é‡ï¼‰ï¼Œä»¥å¹³è¡¡æ€§èƒ½å’Œè®¡ç®—æˆæœ¬ï¼›4) è¯¥æ–¹æ³•å¯ä»¥ä¸å„ç§å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ç»“åˆä½¿ç”¨ï¼Œå…·æœ‰è‰¯å¥½çš„é€šç”¨æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒTICLåœ¨å¤šç§è¯­éŸ³è¯†åˆ«ä»»åŠ¡ä¸­æ˜¾è‘—è¶…è¶Šäº†é›¶æ ·æœ¬æ€§èƒ½ã€‚åœ¨å£éŸ³è‹±è¯­è¯†åˆ«ä»»åŠ¡ä¸­ï¼ŒTICLå®ç°äº†é«˜è¾¾84.7%çš„ç›¸å¯¹WERé™ä½ã€‚åœ¨å¤šè¯­ç§è¯­éŸ³å’Œå„¿ç«¥è¯­éŸ³è¯†åˆ«ä»»åŠ¡ä¸­ï¼ŒTICLä¹Ÿå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚æ¶ˆèå®éªŒéªŒè¯äº†è¯¥æ–¹æ³•çš„ç¨³å¥æ€§å’Œæ•ˆç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§è¯­éŸ³è¯†åˆ«åœºæ™¯ï¼Œå°¤å…¶æ˜¯åœ¨å£éŸ³è‹±è¯­ã€å¤šè¯­ç§è¯­éŸ³å’Œå„¿ç«¥è¯­éŸ³ç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¸­ã€‚é€šè¿‡é€‰æ‹©åˆé€‚çš„ä¸Šä¸‹æ–‡ç¤ºä¾‹ï¼Œå¯ä»¥æ˜¾è‘—æé«˜è¯­éŸ³è¯†åˆ«çš„å‡†ç¡®ç‡ï¼Œä»è€Œæ”¹å–„äººæœºäº¤äº’ä½“éªŒï¼Œå¹¶ä¸ºè¯­éŸ³åŠ©æ‰‹ã€è¯­éŸ³ç¿»è¯‘ç­‰åº”ç”¨æä¾›æ›´å¥½çš„æ”¯æŒã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥æ‰©å±•åˆ°å…¶ä»–æ¨¡æ€ï¼Œä¾‹å¦‚è§†é¢‘ä¸Šä¸‹æ–‡å­¦ä¹ ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Speech foundation models have recently demonstrated the ability to perform Speech In-Context Learning (SICL). Selecting effective in-context examples is crucial for SICL performance, yet selection methodologies remain underexplored. In this work, we propose Text-Embedding KNN for SICL (TICL), a simple pipeline that uses semantic context to enhance off-the-shelf large multimodal models' speech recognition ability without fine-tuning. Across challenging automatic speech recognition tasks, including accented English, multilingual speech, and children's speech, our method enables models to surpass zero-shot performance with up to 84.7% relative WER reduction. We conduct ablation studies to show the robustness and efficiency of our method.

