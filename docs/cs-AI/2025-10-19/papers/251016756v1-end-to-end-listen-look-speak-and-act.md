---
layout: default
title: End-to-end Listen, Look, Speak and Act
---

# End-to-end Listen, Look, Speak and Act

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.16756" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.16756v1</a>
  <a href="https://arxiv.org/pdf/2510.16756.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.16756v1" onclick="toggleFavorite(this, '2510.16756v1', 'End-to-end Listen, Look, Speak and Act')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Siyin Wang, Wenyi Yu, Xianzhao Chen, Xiaohai Tian, Jun Zhang, Lu Lu, Chao Zhang

**åˆ†ç±»**: cs.AI, cs.CL, cs.CV, cs.RO, eess.AS

**å‘å¸ƒæ—¥æœŸ**: 2025-10-19

**å¤‡æ³¨**: 22 pages, 8 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºELLSAï¼Œé¦–ä¸ªç«¯åˆ°ç«¯å…¨åŒå·¥å¤šæ¨¡æ€æ¨¡å‹ï¼Œå®ç°ç±»äººäº¤äº’ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `å…¨åŒå·¥äº¤äº’` `ç«¯åˆ°ç«¯æ¨¡å‹` `æ··åˆä¸“å®¶æ¨¡å‹` `äººæœºäº¤äº’`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ¨¡å‹éš¾ä»¥æ¨¡æ‹Ÿäººç±»è‡ªç„¶çš„å¤šæ¨¡æ€å…¨åŒå·¥äº¤äº’ï¼Œå¦‚è¾¹å¬è¾¹çœ‹ã€è¾¹è¯´è¾¹åšï¼Œä»¥åŠæµç•…çš„è½®æ¢å’Œä¸­æ–­ã€‚
2. ELLSAæ¨¡å‹é‡‡ç”¨SA-MoEæ¶æ„ï¼Œé€šè¿‡è‡ªæ³¨æ„åŠ›æ··åˆä¸“å®¶æœºåˆ¶ï¼Œå®ç°å¤šæ¨¡æ€ä¿¡æ¯çš„æœ‰æ•ˆè·¯ç”±å’Œèåˆï¼Œæ”¯æŒå¹¶å‘ç”Ÿæˆã€‚
3. å®éªŒè¡¨æ˜ï¼ŒELLSAåœ¨è¯­éŸ³äº¤äº’å’Œæœºå™¨äººæ“ä½œä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶èƒ½æ”¯æŒæ›´é«˜çº§çš„å¤šæ¨¡æ€äº¤äº’è¡Œä¸ºã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºELLSAï¼ˆEnd-to-end Listen, Look, Speak and Actï¼‰ï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå…¨åŒå·¥ã€ç«¯åˆ°ç«¯æ¨¡å‹ï¼Œå®ƒåœ¨å•ä¸ªæ¶æ„å†…åŒæ—¶æ„ŸçŸ¥å’Œç”Ÿæˆè§†è§‰ã€æ–‡æœ¬ã€è¯­éŸ³å’ŒåŠ¨ä½œï¼Œä»è€Œå®ç°ä»¥å‰æ— æ³•å®ç°çš„äº¤äº’æ¨¡å¼ï¼Œäº§ç”Ÿæ›´è‡ªç„¶ã€ç±»äººçš„è¡Œä¸ºã€‚å…¶æ ¸å¿ƒæ˜¯ä¸€ç§æ–°é¢–çš„SA-MoEï¼ˆè‡ªæ³¨æ„åŠ›æ··åˆä¸“å®¶ï¼‰æ¶æ„ï¼Œè¯¥æ¶æ„å°†æ¯ä¸ªæ¨¡æ€è·¯ç”±åˆ°ä¸“é—¨çš„ä¸“å®¶ï¼Œå¹¶é€šè¿‡ç»Ÿä¸€çš„æ³¨æ„åŠ›ä¸»å¹²èåˆå®ƒä»¬ã€‚è¿™ä¸ºè”åˆå¤šæ¨¡æ€æ„ŸçŸ¥å’Œå¹¶å‘ç”Ÿæˆæä¾›äº†ä¸€ä¸ªé€šç”¨çš„è§£å†³æ–¹æ¡ˆï¼Œåˆ©ç”¨å¼ºå¤§çš„é¢„è®­ç»ƒç»„ä»¶ï¼ŒåŒæ—¶å®ç°é«˜æ•ˆçš„æ¨¡æ€é›†æˆå¹¶å‡è½»æ¨¡æ€å¹²æ‰°ã€‚åœ¨è¯­éŸ³äº¤äº’å’Œæœºå™¨äººæ“ä½œåŸºå‡†æµ‹è¯•ä¸­ï¼ŒELLSAä¸ç‰¹å®šæ¨¡æ€çš„åŸºçº¿ç›¸åŒ¹é…ï¼ŒåŒæ—¶ç‹¬ç‰¹åœ°æ”¯æŒé«˜çº§å¤šæ¨¡æ€å’Œå…¨åŒå·¥è¡Œä¸ºï¼Œä¾‹å¦‚å¯¹è¯å’ŒåŠ¨ä½œè½®æ¢ã€ç¼ºé™·æŒ‡ä»¤æ‹’ç»ã€è¾¹è¯´è¾¹åšã€åŸºäºä¸Šä¸‹æ–‡çš„è§†è§‰é—®ç­”å’ŒåŠ¨ä½œæŠ¢æ–­ã€‚æˆ‘ä»¬è®¤ä¸ºELLSAä»£è¡¨äº†è¿ˆå‘æ›´è‡ªç„¶å’Œé€šç”¨äº¤äº’æ™ºèƒ½çš„ä¸€æ­¥ï¼Œæœ‰åŠ©äºæ›´å¹¿æ³›çš„äººå·¥é€šç”¨æ™ºèƒ½çš„è¿½æ±‚ã€‚æ‰€æœ‰æ•°æ®ã€ä»£ç å’Œæ¨¡å‹æ£€æŸ¥ç‚¹å°†åœ¨æ¥å—åå‘å¸ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤šæ¨¡æ€äº¤äº’æ—¶ï¼Œé€šå¸¸æ˜¯å•å‘æˆ–åŠåŒå·¥çš„ï¼Œæ— æ³•æ¨¡æ‹Ÿäººç±»è‡ªç„¶æµç•…çš„äº¤äº’æ–¹å¼ã€‚ä¾‹å¦‚ï¼Œæ¨¡å‹å¯èƒ½åªèƒ½åœ¨æ¥æ”¶åˆ°å®Œæ•´çš„æŒ‡ä»¤åæ‰èƒ½å¼€å§‹è¡ŒåŠ¨ï¼Œè€Œæ— æ³•åœ¨è¡ŒåŠ¨è¿‡ç¨‹ä¸­æ ¹æ®æ–°çš„ä¿¡æ¯è¿›è¡Œè°ƒæ•´ã€‚æ­¤å¤–ï¼Œä¸åŒæ¨¡æ€çš„ä¿¡æ¯èåˆä¹Ÿå­˜åœ¨æŒ‘æˆ˜ï¼Œå®¹æ˜“å‡ºç°æ¨¡æ€å¹²æ‰°ï¼Œå½±å“æ¨¡å‹çš„æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šELLSAçš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªç«¯åˆ°ç«¯ã€å…¨åŒå·¥çš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œèƒ½å¤ŸåŒæ—¶æ„ŸçŸ¥å’Œç”Ÿæˆè§†è§‰ã€æ–‡æœ¬ã€è¯­éŸ³å’ŒåŠ¨ä½œä¿¡æ¯ã€‚é€šè¿‡SA-MoEæ¶æ„ï¼Œå°†ä¸åŒæ¨¡æ€çš„ä¿¡æ¯è·¯ç”±åˆ°ä¸“é—¨çš„ä¸“å®¶è¿›è¡Œå¤„ç†ï¼Œç„¶åé€šè¿‡ç»Ÿä¸€çš„æ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œèåˆï¼Œä»è€Œå®ç°é«˜æ•ˆçš„æ¨¡æ€é›†æˆï¼Œå¹¶å‡è½»æ¨¡æ€å¹²æ‰°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šELLSAæ¨¡å‹åŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) å¤šæ¨¡æ€è¾“å…¥ç¼–ç å™¨ï¼šç”¨äºå°†è§†è§‰ã€æ–‡æœ¬ã€è¯­éŸ³å’ŒåŠ¨ä½œä¿¡æ¯ç¼–ç æˆç»Ÿä¸€çš„è¡¨ç¤ºï¼›2) SA-MoEæ¶æ„ï¼šåŒ…å«å¤šä¸ªä¸“å®¶ç½‘ç»œï¼Œæ¯ä¸ªä¸“å®¶è´Ÿè´£å¤„ç†ç‰¹å®šæ¨¡æ€çš„ä¿¡æ¯ï¼›3) æ³¨æ„åŠ›èåˆæ¨¡å—ï¼šç”¨äºå°†ä¸åŒä¸“å®¶çš„è¾“å‡ºè¿›è¡Œèåˆï¼Œç”Ÿæˆæœ€ç»ˆçš„è¾“å‡ºï¼›4) å¤šæ¨¡æ€è¾“å‡ºè§£ç å™¨ï¼šç”¨äºå°†èåˆåçš„è¡¨ç¤ºè§£ç æˆè§†è§‰ã€æ–‡æœ¬ã€è¯­éŸ³å’ŒåŠ¨ä½œä¿¡æ¯ã€‚æ•´ä¸ªæµç¨‹æ˜¯ç«¯åˆ°ç«¯å¯è®­ç»ƒçš„ã€‚

**å…³é”®åˆ›æ–°**ï¼šELLSAçš„å…³é”®åˆ›æ–°åœ¨äºSA-MoEæ¶æ„ï¼Œå®ƒèƒ½å¤Ÿæ ¹æ®è¾“å…¥æ¨¡æ€çš„ç‰¹ç‚¹ï¼ŒåŠ¨æ€åœ°é€‰æ‹©åˆé€‚çš„ä¸“å®¶è¿›è¡Œå¤„ç†ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„æ¨¡æ€é›†æˆã€‚ä¸ä¼ ç»Ÿçš„æ³¨æ„åŠ›æœºåˆ¶ç›¸æ¯”ï¼ŒSA-MoEèƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†ä¸åŒæ¨¡æ€ä¹‹é—´çš„å·®å¼‚ï¼Œå¹¶å‡è½»æ¨¡æ€å¹²æ‰°ã€‚æ­¤å¤–ï¼ŒELLSAæ˜¯é¦–ä¸ªå…¨åŒå·¥çš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œèƒ½å¤ŸåŒæ—¶æ„ŸçŸ¥å’Œç”Ÿæˆå¤šç§æ¨¡æ€çš„ä¿¡æ¯ï¼Œä»è€Œå®ç°æ›´è‡ªç„¶ã€ç±»äººçš„äº¤äº’ã€‚

**å…³é”®è®¾è®¡**ï¼šSA-MoEæ¶æ„ä¸­çš„ä¸“å®¶ç½‘ç»œå¯ä»¥æ˜¯ä»»æ„ç±»å‹çš„ç¥ç»ç½‘ç»œï¼Œä¾‹å¦‚Transformerã€CNNæˆ–RNNã€‚æ³¨æ„åŠ›èåˆæ¨¡å—å¯ä»¥ä½¿ç”¨å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥ä¾¿æ›´å¥½åœ°æ•æ‰ä¸åŒæ¨¡æ€ä¹‹é—´çš„å…³ç³»ã€‚æŸå¤±å‡½æ•°å¯ä»¥åŒ…æ‹¬äº¤å‰ç†µæŸå¤±ã€å‡æ–¹è¯¯å·®æŸå¤±ç­‰ï¼Œç”¨äºè¡¡é‡æ¨¡å‹è¾“å‡ºä¸çœŸå®å€¼ä¹‹é—´çš„å·®å¼‚ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®éœ€è¦æ ¹æ®å…·ä½“çš„ä»»åŠ¡è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

ELLSAåœ¨è¯­éŸ³äº¤äº’å’Œæœºå™¨äººæ“ä½œåŸºå‡†æµ‹è¯•ä¸­ï¼Œæ€§èƒ½ä¸ç‰¹å®šæ¨¡æ€çš„åŸºçº¿æ¨¡å‹ç›¸å½“ï¼ŒåŒæ—¶å±•ç°äº†ç‹¬ç‰¹çš„å¤šæ¨¡æ€å…¨åŒå·¥äº¤äº’èƒ½åŠ›ã€‚ä¾‹å¦‚ï¼Œå®ƒèƒ½å¤Ÿè¿›è¡Œå¯¹è¯å’ŒåŠ¨ä½œè½®æ¢ï¼Œæ‹’ç»é”™è¯¯çš„æŒ‡ä»¤ï¼Œè¾¹è¯´è¾¹åšï¼Œè¿›è¡ŒåŸºäºä¸Šä¸‹æ–‡çš„è§†è§‰é—®ç­”ï¼Œä»¥åŠè¿›è¡ŒåŠ¨ä½œæŠ¢æ–­ã€‚è¿™äº›ç»“æœè¡¨æ˜ELLSAåœ¨å¤šæ¨¡æ€äº¤äº’æ–¹é¢å…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

ELLSAæ¨¡å‹å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚äººæœºäº¤äº’ã€æœºå™¨äººæ§åˆ¶ã€æ™ºèƒ½åŠ©æ‰‹ç­‰ã€‚å®ƒå¯ä»¥ç”¨äºæ„å»ºæ›´è‡ªç„¶ã€æ›´æ™ºèƒ½çš„äº¤äº’ç³»ç»Ÿï¼Œä¾‹å¦‚èƒ½å¤Ÿç†è§£äººç±»è¯­éŸ³æŒ‡ä»¤å¹¶æ‰§è¡Œå¤æ‚ä»»åŠ¡çš„æœºå™¨äººï¼Œæˆ–è€…èƒ½å¤Ÿæ ¹æ®ç”¨æˆ·çš„è§†è§‰ä¿¡æ¯æä¾›ä¸ªæ€§åŒ–æœåŠ¡çš„æ™ºèƒ½åŠ©æ‰‹ã€‚æœªæ¥ï¼ŒELLSAæœ‰æœ›æˆä¸ºé€šç”¨äººå·¥æ™ºèƒ½çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Human interaction is inherently multimodal and full-duplex: we listen while watching, speak while acting, and fluidly adapt to turn-taking and interruptions. Realizing these capabilities is essential for building models simulating humans. We present ELLSA (End-to-end Listen, Look, Speak and Act), which, to our knowledge, is the first full-duplex, end-to-end model that simultaneously perceives and generates across vision, text, speech, and action within a single architecture, enabling interaction patterns previously out of reach, yielding more natural, human-like behaviors. At its core is a novel SA-MoE architecture (Self-Attention Mixture-of-Experts) that routes each modality to specialized experts and fuses them through a unified attention backbone. This provides a generalizable solution for joint multimodal perception and concurrent generation, leveraging strong pre-trained components while enabling efficient modality integration and mitigating modality interference. On speech-interaction and robot-manipulation benchmarks, ELLSA matches modality-specific baselines, while uniquely supporting advanced multimodal and full-duplex behaviors such as dialogue and action turn-taking, defective instruction rejection, speaking-while-acting, context-grounded visual question answering, and action barge-ins. We contend that ELLSA represents a step toward more natural and general interactive intelligence, contributing to the broader pursuit of artificial general intelligence. All data, code and model checkpoints will be released upon acceptance.

