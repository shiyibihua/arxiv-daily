---
layout: default
title: Reinforcement Learning Foundations for Deep Research Systems: A Survey
---

# Reinforcement Learning Foundations for Deep Research Systems: A Survey

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.06733" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.06733v2</a>
  <a href="https://arxiv.org/pdf/2509.06733.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.06733v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.06733v2', 'Reinforcement Learning Foundations for Deep Research Systems: A Survey')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Wenjun Li, Zhi Chen, Jingru Lin, Hannan Cao, Wei Han, Sheng Liang, Zhi Zhang, Kuicai Dong, Dexun Li, Chen Zhang, Yong Liu

**åˆ†ç±»**: cs.AI, cs.CL, cs.IR

**å‘å¸ƒæ—¥æœŸ**: 2025-09-08 (æ›´æ–°: 2025-11-05)

**å¤‡æ³¨**: 39 pages, second version

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç»¼è¿°ï¼šåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ·±åº¦ç ”ç©¶ç³»ç»Ÿï¼Œè§£å†³æ™ºèƒ½ä½“å·¥å…·äº¤äº’ä¸é•¿æœŸä¿¡ç”¨åˆ†é…é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `æ·±åº¦ç ”ç©¶ç³»ç»Ÿ` `æ™ºèƒ½ä½“` `å·¥å…·äº¤äº’` `ä¿¡ç”¨åˆ†é…` `å¤šç›®æ ‡ä¼˜åŒ–` `ç»¼è¿°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ·±åº¦ç ”ç©¶ç³»ç»Ÿä¾èµ–SFTå’ŒDPOï¼Œå­˜åœ¨æ¨¡ä»¿åå·®ã€æš´éœ²åå·®ï¼Œä¸”éš¾ä»¥è¿›è¡Œé•¿ç¨‹ä¿¡ç”¨åˆ†é…å’Œå¤šç›®æ ‡æƒè¡¡ã€‚
2. æœ¬ç ”ç©¶å¼ºè°ƒå¼ºåŒ–å­¦ä¹ åœ¨æ·±åº¦ç ”ç©¶ç³»ç»Ÿä¸­çš„åº”ç”¨ï¼Œé€šè¿‡ä¼˜åŒ–è½¨è¿¹çº§ç­–ç•¥ï¼Œå®ç°æ¢ç´¢å’Œæœ‰åŸåˆ™çš„ä¿¡ç”¨åˆ†é…ã€‚
3. è¯¥ç»¼è¿°ç³»ç»Ÿåœ°æ•´ç†äº†æ•°æ®åˆæˆã€å¼ºåŒ–å­¦ä¹ æ–¹æ³•å’Œè®­ç»ƒç³»ç»Ÿï¼Œå¹¶æ¶µç›–äº†æ™ºèƒ½ä½“æ¶æ„ã€è¯„ä¼°å’ŒåŸºå‡†ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ·±åº¦ç ”ç©¶ç³»ç»Ÿï¼Œå³é€šè¿‡åè°ƒæ¨ç†ã€ç½‘ç»œæœç´¢å’Œç”¨æˆ·æ–‡ä»¶è®¿é—®ä»¥åŠå·¥å…·ä½¿ç”¨æ¥è§£å†³å¤æ‚å¤šæ­¥éª¤ä»»åŠ¡çš„æ™ºèƒ½ä½“AIï¼Œæ­£æœç€å…·æœ‰è§„åˆ’å™¨ã€åè°ƒå™¨å’Œæ‰§è¡Œå™¨çš„åˆ†å±‚éƒ¨ç½²å‘å±•ã€‚ç«¯åˆ°ç«¯è®­ç»ƒæ•´ä¸ªå †æ ˆä»ç„¶ä¸åˆ‡å®é™…ï¼Œå› æ­¤å¤§å¤šæ•°å·¥ä½œè®­ç»ƒè¿æ¥åˆ°æœç´¢ã€æµè§ˆå’Œä»£ç ç­‰æ ¸å¿ƒå·¥å…·çš„å•ä¸ªè§„åˆ’å™¨ã€‚è™½ç„¶SFTèµ‹äºˆäº†åè®®ä¿çœŸåº¦ï¼Œä½†å®ƒå­˜åœ¨æ¨¡ä»¿å’Œæš´éœ²åå·®ï¼Œå¹¶ä¸”æœªèƒ½å……åˆ†åˆ©ç”¨ç¯å¢ƒåé¦ˆã€‚åå¥½å¯¹é½æ–¹æ³•ï¼ˆå¦‚DPOï¼‰ä¾èµ–äºæ¨¡å¼å’Œä»£ç†ï¼Œæ˜¯ç¦»çº¿ç­–ç•¥ï¼Œå¹¶ä¸”åœ¨é•¿ç¨‹ä¿¡ç”¨åˆ†é…å’Œå¤šç›®æ ‡æƒè¡¡æ–¹é¢è¡¨ç°ä¸ä½³ã€‚SFTå’ŒDPOçš„å¦ä¸€ä¸ªå±€é™æ€§åœ¨äºå®ƒä»¬ä¾èµ–äºäººç±»å®šä¹‰çš„å†³ç­–ç‚¹å’Œå­æŠ€èƒ½ã€‚å¼ºåŒ–å­¦ä¹ é€šè¿‡ä¼˜åŒ–è½¨è¿¹çº§ç­–ç•¥ï¼Œå®ç°æ¢ç´¢ã€æ¢å¤è¡Œä¸ºå’Œæœ‰åŸåˆ™çš„ä¿¡ç”¨åˆ†é…ï¼Œä»è€Œä¸é—­ç¯ã€å·¥å…·äº¤äº’ç ”ç©¶ä¿æŒä¸€è‡´ï¼Œå¹¶å‡å°‘å¯¹äººç±»å…ˆéªŒå’Œè¯„åˆ†è€…åå·®çš„ä¾èµ–ã€‚æœ¬ç»¼è¿°æ˜¯é¦–ä¸ªä¸“é—¨é’ˆå¯¹æ·±åº¦ç ”ç©¶ç³»ç»Ÿçš„å¼ºåŒ–å­¦ä¹ åŸºç¡€çš„ç»¼è¿°ã€‚å®ƒæ²¿ç€ä¸‰ä¸ªè½´ç³»ç»ŸåŒ–äº†æœ€è¿‘çš„å·¥ä½œï¼šï¼ˆiï¼‰æ•°æ®åˆæˆå’Œç®¡ç†ï¼›ï¼ˆiiï¼‰ç”¨äºæ™ºèƒ½ä½“ç ”ç©¶çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œæ¶µç›–ç¨³å®šæ€§ã€æ ·æœ¬æ•ˆç‡ã€é•¿ä¸Šä¸‹æ–‡å¤„ç†ã€å¥–åŠ±å’Œä¿¡ç”¨è®¾è®¡ã€å¤šç›®æ ‡ä¼˜åŒ–å’Œå¤šæ¨¡æ€é›†æˆï¼›ï¼ˆiiiï¼‰æ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ è®­ç»ƒç³»ç»Ÿå’Œæ¡†æ¶ã€‚æˆ‘ä»¬è¿˜æ¶µç›–äº†æ™ºèƒ½ä½“æ¶æ„å’Œåè°ƒï¼Œä»¥åŠè¯„ä¼°å’ŒåŸºå‡†ï¼ŒåŒ…æ‹¬æœ€è¿‘çš„QAã€VQAã€é•¿æ ¼å¼åˆæˆå’Œé¢†åŸŸç›¸å…³çš„å·¥å…·äº¤äº’ä»»åŠ¡ã€‚æˆ‘ä»¬æç‚¼å‡ºåå¤å‡ºç°çš„æ¨¡å¼ï¼Œæ­ç¤ºåŸºç¡€è®¾æ–½ç“¶é¢ˆï¼Œå¹¶ä¸ºä½¿ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒç¨³å¥ã€é€æ˜çš„æ·±åº¦ç ”ç©¶æ™ºèƒ½ä½“æä¾›å®ç”¨æŒ‡å¯¼ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæ·±åº¦ç ”ç©¶ç³»ç»Ÿæ—¨åœ¨è§£å†³å¤æ‚ã€å¤šæ­¥éª¤çš„ä»»åŠ¡ï¼Œéœ€è¦æ™ºèƒ½ä½“å…·å¤‡æ¨ç†ã€æœç´¢ã€å·¥å…·ä½¿ç”¨ç­‰èƒ½åŠ›ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚SFTå’ŒDPOï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å­˜åœ¨æ¨¡ä»¿åå·®ã€æš´éœ²åå·®ï¼Œéš¾ä»¥è¿›è¡Œé•¿ç¨‹ä¿¡ç”¨åˆ†é…ï¼Œå¹¶ä¸”è¿‡åº¦ä¾èµ–äººå·¥å®šä¹‰çš„å†³ç­–ç‚¹å’Œå­æŠ€èƒ½ã€‚è¿™äº›é—®é¢˜é™åˆ¶äº†æ™ºèƒ½ä½“çš„æ¢ç´¢èƒ½åŠ›å’Œæ³›åŒ–æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬ç»¼è¿°çš„æ ¸å¿ƒæ€è·¯æ˜¯å¼ºè°ƒå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æ·±åº¦ç ”ç©¶ç³»ç»Ÿä¸­çš„é‡è¦æ€§ã€‚RLèƒ½å¤Ÿé€šè¿‡ä¼˜åŒ–è½¨è¿¹çº§åˆ«çš„ç­–ç•¥ï¼Œå®ç°æ™ºèƒ½ä½“ä¸ç¯å¢ƒçš„é—­ç¯äº¤äº’ï¼Œä»è€Œå…‹æœSFTå’ŒDPOçš„å±€é™æ€§ã€‚é€šè¿‡å¥–åŠ±å‡½æ•°çš„è®¾è®¡ï¼ŒRLå¯ä»¥é¼“åŠ±æ™ºèƒ½ä½“è¿›è¡Œæ¢ç´¢ï¼Œå­¦ä¹ æ¢å¤è¡Œä¸ºï¼Œå¹¶å®ç°æœ‰åŸåˆ™çš„ä¿¡ç”¨åˆ†é…ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥ç»¼è¿°å°†æ·±åº¦ç ”ç©¶ç³»ç»Ÿçš„RLæ–¹æ³•åˆ†ä¸ºä¸‰ä¸ªä¸»è¦éƒ¨åˆ†ï¼šæ•°æ®åˆæˆä¸ç®¡ç†ã€æ™ºèƒ½ä½“ç ”ç©¶çš„RLæ–¹æ³•ä»¥åŠæ™ºèƒ½ä½“RLè®­ç»ƒç³»ç»Ÿå’Œæ¡†æ¶ã€‚æ•°æ®åˆæˆä¸ç®¡ç†å…³æ³¨å¦‚ä½•ç”Ÿæˆé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ã€‚æ™ºèƒ½ä½“ç ”ç©¶çš„RLæ–¹æ³•æ¶µç›–äº†ç¨³å®šæ€§ã€æ ·æœ¬æ•ˆç‡ã€é•¿ä¸Šä¸‹æ–‡å¤„ç†ã€å¥–åŠ±å’Œä¿¡ç”¨è®¾è®¡ã€å¤šç›®æ ‡ä¼˜åŒ–å’Œå¤šæ¨¡æ€é›†æˆç­‰å…³é”®æŠ€æœ¯ã€‚è®­ç»ƒç³»ç»Ÿå’Œæ¡†æ¶åˆ™æä¾›äº†å®é™…è®­ç»ƒæ™ºèƒ½ä½“çš„å·¥å…·å’Œå¹³å°ã€‚æ­¤å¤–ï¼Œç»¼è¿°è¿˜è®¨è®ºäº†æ™ºèƒ½ä½“æ¶æ„å’Œåè°ƒï¼Œä»¥åŠè¯„ä¼°å’ŒåŸºå‡†ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥ç»¼è¿°çš„å…³é”®åˆ›æ–°åœ¨äºå®ƒæ˜¯é¦–ä¸ªä¸“é—¨é’ˆå¯¹æ·±åº¦ç ”ç©¶ç³»ç»Ÿçš„å¼ºåŒ–å­¦ä¹ åŸºç¡€çš„ç»¼è¿°ã€‚å®ƒç³»ç»Ÿåœ°æ•´ç†äº†è¯¥é¢†åŸŸçš„ç ”ç©¶è¿›å±•ï¼Œå¹¶æç‚¼å‡ºåå¤å‡ºç°çš„æ¨¡å¼å’ŒåŸºç¡€è®¾æ–½ç“¶é¢ˆã€‚é€šè¿‡å¯¹ç°æœ‰æ–¹æ³•çš„åˆ†æå’Œæ€»ç»“ï¼Œè¯¥ç»¼è¿°ä¸ºä½¿ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒç¨³å¥ã€é€æ˜çš„æ·±åº¦ç ”ç©¶æ™ºèƒ½ä½“æä¾›äº†å®ç”¨æŒ‡å¯¼ã€‚

**å…³é”®è®¾è®¡**ï¼šç»¼è¿°ä¸­è®¨è®ºçš„å…³é”®è®¾è®¡åŒ…æ‹¬å¥–åŠ±å‡½æ•°çš„è®¾è®¡ã€ä¿¡ç”¨åˆ†é…ç­–ç•¥ã€é•¿ä¸Šä¸‹æ–‡å¤„ç†æ–¹æ³•ã€å¤šç›®æ ‡ä¼˜åŒ–ç­–ç•¥ä»¥åŠå¤šæ¨¡æ€é›†æˆæŠ€æœ¯ã€‚å¥–åŠ±å‡½æ•°çš„è®¾è®¡éœ€è¦èƒ½å¤Ÿå‡†ç¡®åœ°åæ˜ ä»»åŠ¡ç›®æ ‡ï¼Œå¹¶é¼“åŠ±æ™ºèƒ½ä½“è¿›è¡Œæ¢ç´¢ã€‚ä¿¡ç”¨åˆ†é…ç­–ç•¥éœ€è¦èƒ½å¤Ÿå°†å¥–åŠ±åˆ†é…ç»™å¯¹æœ€ç»ˆç»“æœæœ‰è´¡çŒ®çš„æ­¥éª¤ã€‚é•¿ä¸Šä¸‹æ–‡å¤„ç†æ–¹æ³•éœ€è¦èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨å†å²ä¿¡æ¯ã€‚å¤šç›®æ ‡ä¼˜åŒ–ç­–ç•¥éœ€è¦èƒ½å¤Ÿå¹³è¡¡ä¸åŒçš„ä»»åŠ¡ç›®æ ‡ã€‚å¤šæ¨¡æ€é›†æˆæŠ€æœ¯éœ€è¦èƒ½å¤Ÿå°†ä¸åŒæ¨¡æ€çš„ä¿¡æ¯èåˆåœ¨ä¸€èµ·ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥ç»¼è¿°ç³»ç»Ÿåœ°æ•´ç†äº†æ·±åº¦ç ”ç©¶ç³»ç»Ÿä¸­å¼ºåŒ–å­¦ä¹ çš„åº”ç”¨ï¼Œæ€»ç»“äº†æ•°æ®åˆæˆã€RLæ–¹æ³•å’Œè®­ç»ƒç³»ç»Ÿä¸‰ä¸ªæ–¹é¢çš„ç ”ç©¶è¿›å±•ã€‚å®ƒå¼ºè°ƒäº†RLåœ¨è§£å†³é•¿ç¨‹ä¿¡ç”¨åˆ†é…ã€å¤šç›®æ ‡ä¼˜åŒ–å’Œå‡å°‘äººå·¥å¹²é¢„æ–¹é¢çš„ä¼˜åŠ¿ï¼Œå¹¶ä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æ–¹å‘ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ™ºèƒ½å®¢æœã€è‡ªåŠ¨åŒ–æŠ¥å‘Šç”Ÿæˆã€ç§‘å­¦ç ”ç©¶è¾…åŠ©ç­‰é¢†åŸŸã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„æ™ºèƒ½ä½“èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£ç”¨æˆ·æ„å›¾ï¼Œè‡ªä¸»è¿›è¡Œä¿¡æ¯æ£€ç´¢å’Œå·¥å…·è°ƒç”¨ï¼Œä»è€Œæé«˜å·¥ä½œæ•ˆç‡å’Œå†³ç­–è´¨é‡ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›æ¨åŠ¨äººå·¥æ™ºèƒ½åœ¨å„ä¸ªé¢†åŸŸçš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Deep research systems, agentic AI that solve complex, multi-step tasks by coordinating reasoning, search across the open web and user files, and tool use, are moving toward hierarchical deployments with a Planner, Coordinator, and Executors. In practice, training entire stacks end-to-end remains impractical, so most work trains a single planner connected to core tools such as search, browsing, and code. While SFT imparts protocol fidelity, it suffers from imitation and exposure biases and underuses environment feedback. Preference alignment methods such as DPO are schema and proxy-dependent, off-policy, and weak for long-horizon credit assignment and multi-objective trade-offs. A further limitation of SFT and DPO is their reliance on human defined decision points and subskills through schema design and labeled comparisons. Reinforcement learning aligns with closed-loop, tool-interaction research by optimizing trajectory-level policies, enabling exploration, recovery behaviors, and principled credit assignment, and it reduces dependence on such human priors and rater biases.
>   This survey is, to our knowledge, the first dedicated to the RL foundations of deep research systems. It systematizes recent work along three axes: (i) data synthesis and curation; (ii) RL methods for agentic research covering stability, sample efficiency, long context handling, reward and credit design, multi-objective optimization, and multimodal integration; and (iii) agentic RL training systems and frameworks. We also cover agent architecture and coordination, as well as evaluation and benchmarks, including recent QA, VQA, long-form synthesis, and domain-grounded, tool-interaction tasks. We distill recurring patterns, surface infrastructure bottlenecks, and offer practical guidance for training robust, transparent deep research agents with RL.

