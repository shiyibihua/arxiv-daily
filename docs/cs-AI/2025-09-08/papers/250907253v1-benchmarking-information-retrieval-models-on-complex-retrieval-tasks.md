---
layout: default
title: Benchmarking Information Retrieval Models on Complex Retrieval Tasks
---

# Benchmarking Information Retrieval Models on Complex Retrieval Tasks

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.07253" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.07253v1</a>
  <a href="https://arxiv.org/pdf/2509.07253.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.07253v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.07253v1', 'Benchmarking Information Retrieval Models on Complex Retrieval Tasks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Julian Killingback, Hamed Zamani

**åˆ†ç±»**: cs.IR, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-08

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ„å»ºå¤æ‚æ£€ç´¢ä»»åŠ¡åŸºå‡†ï¼Œè¯„ä¼°å¹¶æå‡ä¿¡æ¯æ£€ç´¢æ¨¡å‹æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä¿¡æ¯æ£€ç´¢` `å¤æ‚æ£€ç´¢ä»»åŠ¡` `åŸºå‡†æµ‹è¯•` `å¤§å‹è¯­è¨€æ¨¡å‹` `æŸ¥è¯¢æ‰©å±•` `æŸ¥è¯¢é‡å†™` `æ¨¡å‹è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ£€ç´¢æ¨¡å‹éš¾ä»¥å¤„ç†åŒ…å«å¤šæ–¹é¢çº¦æŸçš„å¤æ‚æŸ¥è¯¢ï¼Œé™åˆ¶äº†å…¶åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨ã€‚
2. æ„å»ºäº†å¤šæ ·ä¸”çœŸå®çš„å¤æ‚æ£€ç´¢ä»»åŠ¡æ•°æ®é›†ï¼Œç”¨äºå…¨é¢è¯„ä¼°ç°æœ‰æ£€ç´¢æ¨¡å‹çš„èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼Œç°æœ‰æœ€ä½³æ¨¡å‹åœ¨å¤æ‚æ£€ç´¢ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ï¼ŒLLMå¢å¼ºå¯¹å¼ºæ¨¡å‹æ•ˆæœåè€Œä¸‹é™ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ–‡æœ¬ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå‚¬ç”Ÿäº†æ— æ•°åº”ç”¨ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæ£€ç´¢æ¨¡å‹å°šæœªæ¶Œç°å‡ºåŒæ ·å¼ºå¤§çš„é€šç”¨æ¨¡å‹ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œæ£€ç´¢æ¨¡å‹å¿…é¡»èƒ½å¤Ÿæ‰§è¡Œå¤æ‚çš„æ£€ç´¢ä»»åŠ¡ï¼Œå³æŸ¥è¯¢åŒ…å«å¤šä¸ªéƒ¨åˆ†ã€çº¦æŸæˆ–è‡ªç„¶è¯­è¨€è¦æ±‚ã€‚è¿™äº›ä»»åŠ¡ä»£è¡¨äº†ä»ç®€å•ã€å•æ–¹é¢æŸ¥è¯¢åˆ°æ›´é«˜çº§çš„è‡ªç„¶æ¼”è¿›ï¼Œæ­£å¦‚äººä»¬æœŸæœ›æœç´¢ç³»ç»Ÿå¤„ç†æ›´å…·ä½“å’Œé›„å¿ƒå‹ƒå‹ƒçš„ä¿¡æ¯è¯·æ±‚ä¸€æ ·ï¼ŒLLMä¿¡æ¯ç³»ç»Ÿçš„ä½¿ç”¨ä¹Ÿè¯æ˜äº†è¿™ä¸€ç‚¹ã€‚å°½ç®¡å¯¹æ£€ç´¢æ¨¡å‹åœ¨å¤æ‚æ£€ç´¢ä»»åŠ¡ä¸­æ‰©å±•èƒ½åŠ›çš„éœ€æ±‚æ—¥ç›Šå¢é•¿ï¼Œä½†è¯„ä¼°æ£€ç´¢æ¨¡å‹åœ¨å„ç§å¤æ‚ä»»åŠ¡ä¸Šçš„èƒ½åŠ›çš„èµ„æºæœ‰é™ã€‚ç°æœ‰çš„å°‘æ•°èµ„æºèŒƒå›´æœ‰é™ï¼Œä¸”ç¼ºä¹ç°å®è®¾ç½®ï¼Œéš¾ä»¥äº†è§£æ£€ç´¢æ¨¡å‹åœ¨å¤æ‚çœŸå®æ£€ç´¢ä»»åŠ¡ä¸­çš„çœŸæ­£èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€ä¸è¶³ï¼Œå¹¶æ¿€å‘ä¸‹ä¸€ä»£æ£€ç´¢æ¨¡å‹çš„åˆ›æ–°ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ç»„å¤šæ ·ä¸”çœŸå®çš„å¤æ‚æ£€ç´¢ä»»åŠ¡ï¼Œå¹¶å¯¹ä¸€ç»„å…·æœ‰ä»£è¡¨æ€§çš„æœ€å…ˆè¿›çš„æ£€ç´¢æ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¢è®¨äº†åŸºäºLLMçš„æŸ¥è¯¢æ‰©å±•å’Œé‡å†™å¯¹æ£€ç´¢è´¨é‡çš„å½±å“ã€‚ç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯æœ€å¥½çš„æ¨¡å‹ä¹Ÿå¾ˆéš¾äº§ç”Ÿé«˜è´¨é‡çš„æ£€ç´¢ç»“æœï¼Œæ‰€æœ‰ä»»åŠ¡çš„å¹³å‡nDCG@10æœ€é«˜ä»…ä¸º0.346ï¼ŒR@100æœ€é«˜ä»…ä¸º0.587ã€‚è™½ç„¶LLMå¢å¼ºå¯ä»¥å¸®åŠ©è¾ƒå¼±çš„æ¨¡å‹ï¼Œä½†æœ€å¼ºæ¨¡å‹åœ¨æ‰€æœ‰é‡å†™æŠ€æœ¯ä¸‹çš„æ‰€æœ‰æŒ‡æ ‡ä¸Šçš„æ€§èƒ½éƒ½æœ‰æ‰€ä¸‹é™ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰ä¿¡æ¯æ£€ç´¢æ¨¡å‹åœ¨å¤„ç†å¤æ‚æ£€ç´¢ä»»åŠ¡æ—¶è¡¨ç°ä¸ä½³çš„é—®é¢˜ã€‚å¤æ‚æ£€ç´¢ä»»åŠ¡æŒ‡çš„æ˜¯é‚£äº›åŒ…å«å¤šä¸ªçº¦æŸæ¡ä»¶ã€éœ€è¦ç†è§£è‡ªç„¶è¯­è¨€æè¿°çš„æŸ¥è¯¢ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸é’ˆå¯¹ç®€å•æŸ¥è¯¢è®¾è®¡ï¼Œæ— æ³•æœ‰æ•ˆå¤„ç†å¤æ‚æŸ¥è¯¢ï¼Œå¯¼è‡´æ£€ç´¢ç»“æœè´¨é‡ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªåŒ…å«å¤šæ ·åŒ–å¤æ‚æ£€ç´¢ä»»åŠ¡çš„åŸºå‡†æ•°æ®é›†ï¼Œå¹¶åœ¨æ­¤æ•°æ®é›†ä¸Šè¯„ä¼°ç°æœ‰æ£€ç´¢æ¨¡å‹çš„æ€§èƒ½ã€‚é€šè¿‡åˆ†ææ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œå¯ä»¥å‘ç°æ¨¡å‹çš„ä¼˜åŠ¿å’Œä¸è¶³ï¼Œä»è€ŒæŒ‡å¯¼æ¨¡å‹æ”¹è¿›ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æ¢ç´¢äº†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡ŒæŸ¥è¯¢æ‰©å±•å’Œé‡å†™ï¼Œä»¥æå‡æ£€ç´¢æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼š1) æ„å»ºå¤æ‚æ£€ç´¢ä»»åŠ¡æ•°æ®é›†ï¼›2) é€‰æ‹©å…·æœ‰ä»£è¡¨æ€§çš„æœ€å…ˆè¿›æ£€ç´¢æ¨¡å‹ï¼›3) åœ¨æ•°æ®é›†ä¸Šè¯„ä¼°è¿™äº›æ¨¡å‹çš„æ€§èƒ½ï¼›4) æ¢ç´¢LLMå¢å¼ºæ–¹æ³•ï¼Œå¦‚æŸ¥è¯¢æ‰©å±•å’Œé‡å†™ï¼›5) åˆ†æå®éªŒç»“æœï¼Œæ€»ç»“æ¨¡å‹çš„ä¼˜ç¼ºç‚¹ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæ„å»ºäº†ä¸€ä¸ªå¤šæ ·ä¸”çœŸå®çš„å¤æ‚æ£€ç´¢ä»»åŠ¡æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†æ¶µç›–äº†å„ç§ç±»å‹çš„å¤æ‚æŸ¥è¯¢ï¼Œèƒ½å¤Ÿæ›´å…¨é¢åœ°è¯„ä¼°æ£€ç´¢æ¨¡å‹çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜å¯¹LLMå¢å¼ºæ–¹æ³•è¿›è¡Œäº†æ·±å…¥ç ”ç©¶ï¼Œå‘ç°LLMå¢å¼ºå¯¹å¼±æ¨¡å‹æœ‰å¸®åŠ©ï¼Œä½†å¯¹å¼ºæ¨¡å‹å¯èƒ½ä¼šé™ä½æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡åœ¨æ„å»ºæ•°æ®é›†æ—¶ï¼Œè€ƒè™‘äº†ä»»åŠ¡çš„å¤šæ ·æ€§å’ŒçœŸå®æ€§ã€‚ä»»åŠ¡ç±»å‹åŒ…æ‹¬å¤šæ–¹é¢çº¦æŸã€è‡ªç„¶è¯­è¨€æè¿°ç­‰ã€‚åœ¨è¯„ä¼°æ¨¡å‹æ€§èƒ½æ—¶ï¼Œä½¿ç”¨äº†nDCG@10å’ŒR@100ç­‰å¸¸ç”¨æŒ‡æ ‡ã€‚åœ¨æ¢ç´¢LLMå¢å¼ºæ–¹æ³•æ—¶ï¼Œä½¿ç”¨äº†ä¸åŒçš„LLMæ¨¡å‹å’Œé‡å†™ç­–ç•¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰æœ€ä½³æ¨¡å‹åœ¨å¤æ‚æ£€ç´¢ä»»åŠ¡ä¸Šçš„å¹³å‡nDCG@10ä»…ä¸º0.346ï¼ŒR@100ä»…ä¸º0.587ï¼Œè¡¨æ˜æ€§èƒ½ä»æœ‰å¾ˆå¤§æå‡ç©ºé—´ã€‚LLMå¢å¼ºæ–¹æ³•å¯¹å¼±æ¨¡å‹æœ‰ä¸€å®šå¸®åŠ©ï¼Œä½†å¯¹æœ€å¼ºæ¨¡å‹ï¼Œæ‰€æœ‰é‡å†™æŠ€æœ¯éƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œæç¤ºéœ€è¦æ›´ç²¾ç»†çš„LLMé›†æˆç­–ç•¥ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ™ºèƒ½æœç´¢å¼•æ“ã€é—®ç­”ç³»ç»Ÿã€æ¨èç³»ç»Ÿç­‰é¢†åŸŸã€‚é€šè¿‡æå‡æ¨¡å‹å¤„ç†å¤æ‚æŸ¥è¯¢çš„èƒ½åŠ›ï¼Œå¯ä»¥æ”¹å–„ç”¨æˆ·ä½“éªŒï¼Œæé«˜ä¿¡æ¯æ£€ç´¢çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶å¯ä»¥æ¨åŠ¨ä¸‹ä¸€ä»£æ£€ç´¢æ¨¡å‹çš„å‘å±•ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°æ»¡è¶³ç”¨æˆ·æ—¥ç›Šå¢é•¿çš„å¤æ‚ä¿¡æ¯éœ€æ±‚ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) are incredible and versatile tools for text-based tasks that have enabled countless, previously unimaginable, applications. Retrieval models, in contrast, have not yet seen such capable general-purpose models emerge. To achieve this goal, retrieval models must be able to perform complex retrieval tasks, where queries contain multiple parts, constraints, or requirements in natural language. These tasks represent a natural progression from the simple, single-aspect queries that are used in the vast majority of existing, commonly used evaluation sets. Complex queries naturally arise as people expect search systems to handle more specific and often ambitious information requests, as is demonstrated by how people use LLM-based information systems. Despite the growing desire for retrieval models to expand their capabilities in complex retrieval tasks, there exist limited resources to assess the ability of retrieval models on a comprehensive set of diverse complex tasks. The few resources that do exist feature a limited scope and often lack realistic settings making it hard to know the true capabilities of retrieval models on complex real-world retrieval tasks. To address this shortcoming and spur innovation in next-generation retrieval models, we construct a diverse and realistic set of complex retrieval tasks and benchmark a representative set of state-of-the-art retrieval models. Additionally, we explore the impact of LLM-based query expansion and rewriting on retrieval quality. Our results show that even the best models struggle to produce high-quality retrieval results with the highest average nDCG@10 of only 0.346 and R@100 of only 0.587 across all tasks. Although LLM augmentation can help weaker models, the strongest model has decreased performance across all metrics with all rewriting techniques.

