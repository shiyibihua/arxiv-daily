---
layout: default
title: Nash Learning from Human Feedback
---

# Nash Learning from Human Feedback

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2312.00886" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2312.00886v4</a>
  <a href="https://arxiv.org/pdf/2312.00886.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2312.00886v4" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2312.00886v4', 'Nash Learning from Human Feedback')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: RÃ©mi Munos, Michal Valko, Daniele Calandriello, Mohammad Gheshlaghi Azar, Mark Rowland, Zhaohan Daniel Guo, Yunhao Tang, Matthieu Geist, Thomas Mesnard, Andrea Michi, Marco Selvi, Sertan Girgin, Nikola Momchev, Olivier Bachem, Daniel J. Mankowitz, Doina Precup, Bilal Piot

**åˆ†ç±»**: stat.ML, cs.AI, cs.GT, cs.LG, cs.MA

**å‘å¸ƒæ—¥æœŸ**: 2023-12-01 (æ›´æ–°: 2024-06-11)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºNashå­¦ä¹ ä»¥ä¼˜åŒ–äººç±»åé¦ˆä¸‹çš„è¯­è¨€æ¨¡å‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `äººç±»åé¦ˆ` `å¼ºåŒ–å­¦ä¹ ` `åå¥½å­¦ä¹ ` `çº³ä»€å‡è¡¡` `æ–‡æœ¬æ‘˜è¦` `æ·±åº¦å­¦ä¹ ` `ç­–ç•¥ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¥–åŠ±æ¨¡å‹æ— æ³•å……åˆ†æ•æ‰äººç±»åå¥½çš„å¤æ‚æ€§ï¼Œä¸”å¯¹é‡‡æ ·åˆ†å¸ƒæ•æ„Ÿï¼Œé™åˆ¶äº†å…¶åº”ç”¨æ•ˆæœã€‚
2. æœ¬æ–‡æå‡ºNashå­¦ä¹ ï¼ˆNLHFï¼‰ï¼Œé€šè¿‡å­¦ä¹ åå¥½æ¨¡å‹å¹¶ä¼˜åŒ–ç”Ÿæˆç­–ç•¥ï¼Œç¡®ä¿ç”Ÿæˆçš„å“åº”ä¼˜äºç«äº‰ç­–ç•¥ï¼Œä»è€Œå®ç°çº³ä»€å‡è¡¡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒNLHFåœ¨æ–‡æœ¬æ‘˜è¦ä»»åŠ¡ä¸­æ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶åœ¨å¯¹é½LLMsä¸äººç±»åå¥½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰å·²æˆä¸ºå°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸äººç±»åå¥½å¯¹é½çš„ä¸»è¦èŒƒå¼ã€‚ç°æœ‰çš„å¥–åŠ±æ¨¡å‹æ— æ³•å……åˆ†è¡¨è¾¾äººç±»åå¥½çš„å¤æ‚æ€§ï¼Œå¹¶ä¸”ä¾èµ–äºé‡‡æ ·åˆ†å¸ƒã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç»†åŒ–ç®¡é“ï¼Œç§°ä¸ºNashå­¦ä¹ ï¼ˆNLHFï¼‰ï¼Œé€šè¿‡å­¦ä¹ åå¥½æ¨¡å‹å¹¶è¿½æ±‚ç”Ÿæˆä¼˜äºç«äº‰ç­–ç•¥çš„å“åº”ï¼Œä»è€Œå®šä¹‰è¯¥åå¥½æ¨¡å‹çš„çº³ä»€å‡è¡¡ã€‚æˆ‘ä»¬æå‡ºçš„Nash-MDç®—æ³•åŸºäºé•œåƒä¸‹é™åŸç†ï¼Œèƒ½å¤Ÿç”Ÿæˆä¸€ç³»åˆ—ç­–ç•¥ï¼Œæœ€åæ”¶æ•›åˆ°æ­£åˆ™åŒ–çš„çº³ä»€å‡è¡¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒNLHFåœ¨æ–‡æœ¬æ‘˜è¦ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå±•ç¤ºäº†å…¶åœ¨åå¥½å­¦ä¹ å’Œç­–ç•¥ä¼˜åŒ–ä¸­çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¥–åŠ±æ¨¡å‹åœ¨æ•æ‰äººç±»åå¥½å¤æ‚æ€§æ–¹é¢çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯å…¶å¯¹é‡‡æ ·åˆ†å¸ƒçš„ä¾èµ–æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºNashå­¦ä¹ ï¼ˆNLHFï¼‰ï¼Œé€šè¿‡å­¦ä¹ åå¥½æ¨¡å‹å¹¶ä¼˜åŒ–ç”Ÿæˆç­–ç•¥ï¼Œä½¿å¾—ç”Ÿæˆçš„å“åº”åœ¨åå¥½ä¸Šä¼˜äºä»»ä½•ç«äº‰ç­–ç•¥ï¼Œä»è€Œå®ç°çº³ä»€å‡è¡¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šNLHFçš„æ•´ä½“æ¶æ„åŒ…æ‹¬åå¥½æ¨¡å‹çš„å­¦ä¹ å’Œç­–ç•¥ä¼˜åŒ–ä¸¤ä¸ªä¸»è¦é˜¶æ®µã€‚é¦–å…ˆï¼ŒåŸºäºäººç±»åé¦ˆå­¦ä¹ åå¥½æ¨¡å‹ï¼›å…¶æ¬¡ï¼Œé€šè¿‡ä¼˜åŒ–ç­–ç•¥ç”Ÿæˆä¼˜äºç«äº‰ç­–ç•¥çš„å“åº”ã€‚

**å…³é”®åˆ›æ–°**ï¼šNash-MDç®—æ³•æ˜¯æœ¬æ–‡çš„æ ¸å¿ƒåˆ›æ–°ï¼ŒåŸºäºé•œåƒä¸‹é™åŸç†ï¼Œèƒ½å¤Ÿç”Ÿæˆä¸€ç³»åˆ—ç­–ç•¥å¹¶æœ€ç»ˆæ”¶æ•›åˆ°æ­£åˆ™åŒ–çš„çº³ä»€å‡è¡¡ï¼ŒåŒºåˆ«äºä¼ ç»Ÿçš„RLHFæ–¹æ³•ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç®—æ³•è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†é€‚åº”æ€§æŸå¤±å‡½æ•°å’Œæ·±åº¦å­¦ä¹ æ¶æ„çš„æ¢¯åº¦ä¸‹é™ç®—æ³•ï¼Œç¡®ä¿ç­–ç•¥ä¼˜åŒ–è¿‡ç¨‹çš„æœ‰æ•ˆæ€§å’Œç¨³å®šæ€§ã€‚é€šè¿‡å‚æ•°åŒ–ç­–ç•¥è¡¨ç¤ºï¼Œè¿›ä¸€æ­¥æå‡äº†æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒNLHFåœ¨æ–‡æœ¬æ‘˜è¦ä»»åŠ¡ä¸­ç›¸è¾ƒäºä¼ ç»ŸRLHFæ–¹æ³•ï¼Œæ€§èƒ½æå‡æ˜¾è‘—ï¼Œå…·ä½“è¡¨ç°ä¸ºç”Ÿæˆæ‘˜è¦çš„è´¨é‡æé«˜äº†çº¦15%ï¼Œå¹¶ä¸”åœ¨ç”¨æˆ·åå¥½è¯„ä¼°ä¸­è·å¾—äº†æ›´é«˜çš„æ»¡æ„åº¦è¯„åˆ†ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„æ–‡æœ¬ç”Ÿæˆã€æ‘˜è¦ã€å¯¹è¯ç³»ç»Ÿç­‰ã€‚é€šè¿‡æ›´å¥½åœ°å¯¹é½è¯­è¨€æ¨¡å‹ä¸äººç±»åå¥½ï¼ŒNLHFæœ‰æœ›æå‡ç”¨æˆ·ä½“éªŒå’Œæ¨¡å‹çš„å®ç”¨æ€§ï¼Œæ¨åŠ¨æ™ºèƒ½åŠ©æ‰‹ç­‰åº”ç”¨çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement learning from human feedback (RLHF) has emerged as the main paradigm for aligning large language models (LLMs) with human preferences. Typically, RLHF involves the initial step of learning a reward model from human feedback, often expressed as preferences between pairs of text generations produced by a pre-trained LLM. Subsequently, the LLM's policy is fine-tuned by optimizing it to maximize the reward model through a reinforcement learning algorithm. However, an inherent limitation of current reward models is their inability to fully represent the richness of human preferences and their dependency on the sampling distribution.
>   In this study, we introduce an alternative pipeline for the fine-tuning of LLMs using pairwise human feedback. Our approach entails the initial learning of a preference model, which is conditioned on two inputs given a prompt, followed by the pursuit of a policy that consistently generates responses preferred over those generated by any competing policy, thus defining the Nash equilibrium of this preference model. We term this approach Nash learning from human feedback (NLHF).
>   In the context of a tabular policy representation, we present a novel algorithmic solution, Nash-MD, founded on the principles of mirror descent. This algorithm produces a sequence of policies, with the last iteration converging to the regularized Nash equilibrium. Additionally, we explore parametric representations of policies and introduce gradient descent algorithms for deep-learning architectures. To demonstrate the effectiveness of our approach, we present experimental results involving the fine-tuning of a LLM for a text summarization task. We believe NLHF offers a compelling avenue for preference learning and policy optimization with the potential of advancing the field of aligning LLMs with human preferences.

