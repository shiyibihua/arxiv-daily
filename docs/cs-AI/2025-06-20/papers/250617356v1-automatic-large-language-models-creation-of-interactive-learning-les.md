---
layout: default
title: Automatic Large Language Models Creation of Interactive Learning Lessons
---

# Automatic Large Language Models Creation of Interactive Learning Lessons

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.17356" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.17356v1</a>
  <a href="https://arxiv.org/pdf/2506.17356.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.17356v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.17356v1', 'Automatic Large Language Models Creation of Interactive Learning Lessons')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jionghao Lin, Jiarui Rao, Yiyang Zhao, Yuting Wang, Ashish Gurung, Amanda Barany, Jaclyn Ocumpaugh, Ryan S. Baker, Kenneth R. Koedinger

**åˆ†ç±»**: cs.CY, cs.AI, cs.HC

**å‘å¸ƒæ—¥æœŸ**: 2025-06-20

**å¤‡æ³¨**: Full Research Paper, 15 pages, In Proceedings of 20th European Conference on Technology Enhanced Learning (ECTEL2025)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„äº’åŠ¨å­¦ä¹ è¯¾ç¨‹è‡ªåŠ¨ç”Ÿæˆæ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `åœ¨çº¿æ•™è‚²` `è¯¾ç¨‹ç”Ÿæˆ` `ä»»åŠ¡åˆ†è§£` `æ•™è‚²æŠ€æœ¯` `äººæœºåä½œ` `åŸ¹è®­ç³»ç»Ÿ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åœ¨çº¿å¯¼å¸ˆåŸ¹è®­è¯¾ç¨‹ç”Ÿæˆæ–¹æ³•ç¼ºä¹ç»“æ„æ€§ï¼Œå¯¼è‡´è¯¾ç¨‹è´¨é‡å‚å·®ä¸é½ï¼Œéš¾ä»¥æ»¡è¶³æ•™å­¦éœ€æ±‚ã€‚
2. æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºä»»åŠ¡åˆ†è§£çš„æç¤ºç­–ç•¥ï¼Œåˆ©ç”¨GPT-4oæ¨¡å‹è‡ªåŠ¨ç”Ÿæˆç»“æ„åŒ–çš„å¯¼å¸ˆåŸ¹è®­è¯¾ç¨‹ï¼Œæå‡è¯¾ç¨‹è´¨é‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨ä»»åŠ¡åˆ†è§£ç­–ç•¥ç”Ÿæˆçš„è¯¾ç¨‹åœ¨è¯„ä¼°ä¸­è·å¾—äº†æ›´é«˜çš„è¯„åˆ†ï¼Œè¡¨æ˜è¯¥æ–¹æ³•åœ¨è¯¾ç¨‹è®¾è®¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶æ¢è®¨äº†è‡ªåŠ¨ç”Ÿæˆäº’åŠ¨åœºæ™¯è¯¾ç¨‹çš„æ–¹æ³•ï¼Œæ—¨åœ¨åŸ¹è®­åœ¨çº¿æ•™æˆä¸­å­¦æ•°å­¦çš„åˆå­¦è€…äººç±»å¯¼å¸ˆã€‚é€šè¿‡ä½¿ç”¨åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆçš„æç¤ºå·¥ç¨‹ï¼Œç»“åˆGPT-4oï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªèƒ½å¤Ÿåˆ›å»ºç»“æ„åŒ–å¯¼å¸ˆåŸ¹è®­è¯¾ç¨‹çš„ç³»ç»Ÿã€‚ç ”ç©¶ç”Ÿæˆäº†å…³äºé¼“åŠ±å­¦ç”Ÿç‹¬ç«‹æ€§ã€é¼“åŠ±å¯»æ±‚å¸®åŠ©è¡Œä¸ºå’Œå¼€å¯æ‘„åƒå¤´ä¸‰ä¸ªå…³é”®ä¸»é¢˜çš„è¯¾ç¨‹ï¼Œé‡‡ç”¨ä»»åŠ¡åˆ†è§£çš„æç¤ºç­–ç•¥å°†è¯¾ç¨‹ç”Ÿæˆåˆ†è§£ä¸ºå­ä»»åŠ¡ã€‚ä¸¤ä½äººç±»è¯„ä¼°è€…å¯¹ç”Ÿæˆçš„è¯¾ç¨‹è¿›è¡Œäº†å®šé‡å’Œå®šæ€§è¯„ä¼°ï¼Œç»“æœè¡¨æ˜ï¼Œä»»åŠ¡åˆ†è§£ç­–ç•¥ç”Ÿæˆçš„è¯¾ç¨‹è¯„åˆ†é«˜äºå•æ­¥ç”Ÿæˆçš„è¯¾ç¨‹ã€‚è¯„ä¼°è€…æŒ‡å‡ºäº†LLMç”Ÿæˆè¯¾ç¨‹çš„å‡ ä¸ªä¼˜ç‚¹ï¼ŒåŒ…æ‹¬å†…å®¹ç»“æ„è‰¯å¥½å’ŒèŠ‚çœæ—¶é—´çš„æ½œåŠ›ï¼ŒåŒæ—¶ä¹Ÿæåˆ°äº†ä¸€äº›å±€é™æ€§ï¼Œå¦‚åé¦ˆè¿‡äºé€šç”¨å’ŒæŸäº›æ•™å­¦éƒ¨åˆ†ç¼ºä¹æ¸…æ™°æ€§ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†äººæœºæ··åˆæ–¹æ³•åœ¨ç”Ÿæˆæœ‰æ•ˆå¯¼å¸ˆåŸ¹è®­è¯¾ç¨‹ä¸­çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³åœ¨çº¿æ•°å­¦å¯¼å¸ˆåŸ¹è®­è¯¾ç¨‹ç”Ÿæˆçš„ç»“æ„æ€§ä¸è¶³å’Œè´¨é‡ä¸å‡çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æœ‰æ•ˆæ»¡è¶³æ•™å­¦éœ€æ±‚ï¼Œå¯¼è‡´è¯¾ç¨‹å†…å®¹ç¼ºä¹æ·±åº¦å’Œé’ˆå¯¹æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆçš„æç¤ºå·¥ç¨‹æ–¹æ³•ï¼Œé€šè¿‡å°†è¯¾ç¨‹ç”Ÿæˆä»»åŠ¡åˆ†è§£ä¸ºå¤šä¸ªå­ä»»åŠ¡ï¼Œåˆ©ç”¨GPT-4oæ¨¡å‹ç”Ÿæˆæ›´ä¸ºç»“æ„åŒ–å’Œé«˜è´¨é‡çš„è¯¾ç¨‹å†…å®¹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ£€ç´¢æ¨¡å—ã€ä»»åŠ¡åˆ†è§£æ¨¡å—å’Œè¯¾ç¨‹ç”Ÿæˆæ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡æ£€ç´¢ç›¸å…³æ•™è‚²èµ„æºï¼Œç„¶åå°†è¯¾ç¨‹ç”Ÿæˆä»»åŠ¡åˆ†è§£ä¸ºå¤šä¸ªå­ä»»åŠ¡ï¼Œæœ€ååˆ©ç”¨GPT-4oç”Ÿæˆå®Œæ•´çš„è¯¾ç¨‹å†…å®¹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºä»»åŠ¡åˆ†è§£ç­–ç•¥çš„å¼•å…¥ï¼Œä½¿å¾—è¯¾ç¨‹ç”Ÿæˆè¿‡ç¨‹æ›´åŠ ç³»ç»ŸåŒ–å’Œé«˜æ•ˆï¼Œä¸ä¼ ç»Ÿçš„å•æ­¥ç”Ÿæˆæ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æå‡äº†è¯¾ç¨‹çš„è´¨é‡å’Œç»“æ„æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–ç”Ÿæˆå†…å®¹çš„ç»“æ„æ€§ï¼ŒåŒæ—¶è®¾ç½®äº†å¤šç§å‚æ•°ä»¥ç¡®ä¿ç”Ÿæˆå†…å®¹çš„å¤šæ ·æ€§å’Œé’ˆå¯¹æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨ä»»åŠ¡åˆ†è§£ç­–ç•¥ç”Ÿæˆçš„è¯¾ç¨‹åœ¨è¯„ä¼°ä¸­è·å¾—äº†æ›´é«˜çš„è¯„åˆ†ï¼Œå…·ä½“è€Œè¨€ï¼Œè¯¾ç¨‹è¯„åˆ†æ¯”å•æ­¥ç”Ÿæˆçš„è¯¾ç¨‹æé«˜äº†æ˜¾è‘—çš„ç™¾åˆ†æ¯”ã€‚è¿™è¡¨æ˜è¯¥æ–¹æ³•åœ¨è¯¾ç¨‹è®¾è®¡ä¸­çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åœ¨çº¿æ•™è‚²å¹³å°å’Œæ•™å¸ˆåŸ¹è®­é¡¹ç›®ï¼Œèƒ½å¤Ÿä¸ºæ•™è‚²å·¥ä½œè€…æä¾›é«˜è´¨é‡çš„åŸ¹è®­è¯¾ç¨‹ï¼Œæå‡æ•™å­¦æ•ˆæœã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½åœ¨å…¶ä»–å­¦ç§‘çš„è¯¾ç¨‹ç”Ÿæˆä¸­å¾—åˆ°æ¨å¹¿ï¼Œè¿›ä¸€æ­¥æ¨åŠ¨æ•™è‚²æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We explore the automatic generation of interactive, scenario-based lessons designed to train novice human tutors who teach middle school mathematics online. Employing prompt engineering through a Retrieval-Augmented Generation approach with GPT-4o, we developed a system capable of creating structured tutor training lessons. Our study generated lessons in English for three key topics: Encouraging Students' Independence, Encouraging Help-Seeking Behavior, and Turning on Cameras, using a task decomposition prompting strategy that breaks lesson generation into sub-tasks. The generated lessons were evaluated by two human evaluators, who provided both quantitative and qualitative evaluations using a comprehensive rubric informed by lesson design research. Results demonstrate that the task decomposition strategy led to higher-rated lessons compared to single-step generation. Human evaluators identified several strengths in the LLM-generated lessons, including well-structured content and time-saving potential, while also noting limitations such as generic feedback and a lack of clarity in some instructional sections. These findings underscore the potential of hybrid human-AI approaches for generating effective lessons in tutor training.

