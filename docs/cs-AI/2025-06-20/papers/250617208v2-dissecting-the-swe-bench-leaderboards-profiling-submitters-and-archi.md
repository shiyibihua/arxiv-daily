---
layout: default
title: Dissecting the SWE-Bench Leaderboards: Profiling Submitters and Architectures of LLM- and Agent-Based Repair Systems
---

# Dissecting the SWE-Bench Leaderboards: Profiling Submitters and Architectures of LLM- and Agent-Based Repair Systems

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.17208" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.17208v2</a>
  <a href="https://arxiv.org/pdf/2506.17208.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.17208v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.17208v2', 'Dissecting the SWE-Bench Leaderboards: Profiling Submitters and Architectures of LLM- and Agent-Based Repair Systems')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Matias Martinez, Xavier Franch

**åˆ†ç±»**: cs.SE, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-20 (æ›´æ–°: 2025-08-18)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åˆ†æSWE-Benchæ’è¡Œæ¦œï¼Œæ­ç¤ºLLMä¸ä»£ç†ä¿®å¤ç³»ç»Ÿçš„æäº¤è€…ä¸æ¶æ„ç‰¹å¾**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è‡ªåŠ¨åŒ–ç¨‹åºä¿®å¤` `å¤§å‹è¯­è¨€æ¨¡å‹` `ä»£ç†ç³»ç»Ÿ` `SWE-Bench` `å¼€æºè½¯ä»¶` `æŠ€æœ¯åˆ†æ` `ç³»ç»Ÿæ¶æ„`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è‡ªåŠ¨åŒ–ç¨‹åºä¿®å¤æ–¹æ³•ç¼ºä¹å¯¹æäº¤è€…å’Œæ¶æ„çš„è¯¦ç»†æ–‡æ¡£ï¼Œå¯¼è‡´è®¸å¤šè§£å†³æ–¹æ¡ˆçš„è®¾è®¡å’Œæ¥æºä¸æ˜ç¡®ã€‚
2. æœ¬æ–‡é€šè¿‡åˆ†æSWE-Benchæ’è¡Œæ¦œçš„79ä¸ªLiteå’Œ99ä¸ªVerifiedæäº¤ï¼Œæ­ç¤ºäº†LLMå’Œä»£ç†ç³»ç»Ÿçš„ä½¿ç”¨æƒ…å†µåŠå…¶è®¾è®¡ç‰¹å¾ã€‚
3. ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œä¸“æœ‰LLMçš„ä½¿ç”¨å ä¸»å¯¼åœ°ä½ï¼Œä¸”è´¡çŒ®è€…çš„èƒŒæ™¯å¤šæ ·ï¼Œä»ä¸ªäººå¼€å‘è€…åˆ°å¤§å‹ç§‘æŠ€å…¬å¸å‡æœ‰æ¶‰åŠã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€è‡ªåŠ¨åŒ–ç¨‹åºä¿®å¤ï¼ˆAPRï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œç‰¹åˆ«æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’ŒåŸºäºä»£ç†çš„ç³»ç»Ÿçš„è¿›æ­¥ï¼ŒSWE-Benchæˆä¸ºè¯„ä¼°LLMä¿®å¤ç³»ç»Ÿçš„é‡è¦åŸºå‡†ã€‚æœ¬æ–‡é¦–æ¬¡å…¨é¢ç ”ç©¶äº†SWE-Bench Liteå’ŒVerifiedæ’è¡Œæ¦œçš„æ‰€æœ‰æäº¤ï¼Œåˆ†æäº†80ç§ç‹¬ç‰¹çš„æ–¹æ³•ï¼Œæ¶µç›–æäº¤è€…ç±»å‹ã€äº§å“å¯ç”¨æ€§ã€LLMä½¿ç”¨æƒ…å†µå’Œç³»ç»Ÿæ¶æ„ç­‰ç»´åº¦ã€‚ç ”ç©¶å‘ç°ï¼Œä¸“æœ‰LLMï¼ˆå°¤å…¶æ˜¯Claude 3.5ï¼‰å ä¸»å¯¼åœ°ä½ï¼Œä¸”å­˜åœ¨ä»£ç†å’Œéä»£ç†è®¾è®¡ï¼Œè´¡çŒ®è€…åŒ…æ‹¬ä¸ªäººå¼€å‘è€…å’Œå¤§å‹ç§‘æŠ€å…¬å¸ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³SWE-Benchæ’è¡Œæ¦œæäº¤è€…å’Œæ¶æ„ç¼ºä¹é€æ˜åº¦çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½è¯¦ç»†è®°å½•è§£å†³æ–¹æ¡ˆçš„è®¾è®¡å’Œæ¥æºã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¯¹SWE-Bench Liteå’ŒVerifiedæ’è¡Œæ¦œçš„æäº¤è¿›è¡Œå…¨é¢åˆ†æï¼Œæ­ç¤ºä¸åŒæäº¤è€…çš„ç±»å‹ã€ä½¿ç”¨çš„LLMåŠå…¶ç³»ç»Ÿæ¶æ„çš„ç‰¹å¾ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨å®šé‡å’Œå®šæ€§åˆ†æç›¸ç»“åˆçš„æ–¹æ³•ï¼Œé¦–å…ˆæ”¶é›†æ‰€æœ‰æäº¤æ•°æ®ï¼Œç„¶åä»å¤šä¸ªç»´åº¦è¿›è¡Œåˆ†ç±»å’Œæ¯”è¾ƒï¼Œæœ€ç»ˆæ€»ç»“å‡ºä¸»è¦å‘ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶é¦–æ¬¡ç³»ç»Ÿæ€§åœ°åˆ†æäº†SWE-Benchæ’è¡Œæ¦œçš„æäº¤ï¼Œå¡«è¡¥äº†ç°æœ‰æ–‡çŒ®ä¸­å¯¹LLMå’Œä»£ç†ä¿®å¤ç³»ç»Ÿæ¶æ„ç†è§£çš„ç©ºç™½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨åˆ†æè¿‡ç¨‹ä¸­ï¼Œé‡ç‚¹å…³æ³¨äº†æäº¤è€…çš„èƒŒæ™¯ã€äº§å“çš„å¯ç”¨æ€§ã€LLMçš„å…·ä½“ä½¿ç”¨æƒ…å†µä»¥åŠç³»ç»Ÿçš„è®¾è®¡æ¶æ„ï¼Œç¡®ä¿äº†åˆ†æçš„å…¨é¢æ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä¸“æœ‰LLMï¼ˆå¦‚Claude 3.5ï¼‰åœ¨æäº¤ä¸­å æ®ä¸»å¯¼åœ°ä½ï¼Œä¸”ä»£ç†å’Œéä»£ç†è®¾è®¡çš„å­˜åœ¨æ˜¾ç¤ºå‡ºå¤šæ ·åŒ–çš„è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡å¯¹79ä¸ªLiteå’Œ99ä¸ªVerifiedæäº¤çš„åˆ†æï¼Œæ­ç¤ºäº†è´¡çŒ®è€…çš„å¹¿æ³›èƒŒæ™¯ï¼Œä¿ƒè¿›äº†å¯¹å½“å‰APRæŠ€æœ¯çš„æ·±å…¥ç†è§£ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶ä¸ºè‡ªåŠ¨åŒ–ç¨‹åºä¿®å¤é¢†åŸŸæä¾›äº†é‡è¦çš„åŸºå‡†å’Œå‚è€ƒï¼Œå¸®åŠ©ç ”ç©¶äººå‘˜å’Œå¼€å‘è€…ç†è§£å½“å‰æŠ€æœ¯çš„è¶‹åŠ¿ä¸å±€é™æ€§ã€‚æœªæ¥ï¼ŒåŸºäºæ­¤ç ”ç©¶çš„å‘ç°ï¼Œå¯ä»¥æ¨åŠ¨æ›´é«˜æ•ˆçš„ä¿®å¤ç³»ç»Ÿçš„è®¾è®¡ä¸å¼€å‘ï¼Œä¿ƒè¿›å¼€æºç¤¾åŒºçš„æŠ€æœ¯è¿›æ­¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The rapid progress in Automated Program Repair (APR) has been driven by advances in AI, particularly large language models (LLMs) and agent-based systems. SWE-Bench is a recent benchmark designed to evaluate LLM-based repair systems using real issues and pull requests mined from 12 popular open-source Python repositories. Its public leaderboards -- SWE-Bench Lite and SWE-Bench Verified -- have become central platforms for tracking progress and comparing solutions. However, because the submission process does not require detailed documentation, the architectural design and origin of many solutions remain unclear. In this paper, we present the first comprehensive study of all submissions to the SWE-Bench Lite (79 entries) and Verified (99 entries) leaderboards, analyzing 80 unique approaches across dimensions such as submitter type, product availability, LLM usage, and system architecture. Our findings reveal the dominance of proprietary LLMs (especially Claude 3.5), the presence of both agentic and non-agentic designs, and a contributor base spanning from individual developers to large tech companies.

