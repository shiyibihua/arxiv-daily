---
layout: default
title: Re-Evaluating Code LLM Benchmarks Under Semantic Mutation
---

# Re-Evaluating Code LLM Benchmarks Under Semantic Mutation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.17369" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.17369v1</a>
  <a href="https://arxiv.org/pdf/2506.17369.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.17369v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.17369v1', 'Re-Evaluating Code LLM Benchmarks Under Semantic Mutation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhiyuan Pan, Xing Hu, Xin Xia, Xiaohu Yang

**åˆ†ç±»**: cs.SE, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-20

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé€šç”¨æ¡†æ¶ä»¥è§£å†³ä»£ç åŸºå‡†æµ‹è¯•ä¸­çš„æç¤ºæ•æ„Ÿæ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä»£ç åŸºå‡†æµ‹è¯•` `æç¤ºæ•æ„Ÿæ€§` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ€§èƒ½è¯„ä¼°` `è½¯ä»¶å·¥ç¨‹` `å®éªŒè®¾è®¡` `è¯­ä¹‰ç›¸ä¼¼æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ä»£ç åŸºå‡†æµ‹è¯•é€šå¸¸ä¾èµ–å•ä¸€æç¤ºæ¨¡æ¿ï¼Œå¯¼è‡´æç¤ºæ•æ„Ÿæ€§é—®é¢˜ï¼Œå½±å“æ¨¡å‹æ€§èƒ½è¯„ä¼°çš„å¯é æ€§ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šç”¨æ¡†æ¶ï¼Œé€šè¿‡ä¿®æ”¹æç¤ºæ¨¡æ¿æ¥ä¿æŒå…¶è¯­ä¹‰å’Œç»“æ„ï¼Œè§£å†³æç¤ºæ•æ„Ÿæ€§é—®é¢˜ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæç¤ºçš„å¾®å°å˜åŒ–ä¼šæ˜¾è‘—å½±å“æ¨¡å‹æ€§èƒ½ï¼Œå¹¶å¯¼è‡´ä¸åŒæ¨¡å‹çš„æ€§èƒ½æ’åä¸ä¸€è‡´ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ—¶ä»£ï¼Œä»£ç åŸºå‡†æµ‹è¯•æˆä¸ºè½¯ä»¶å·¥ç¨‹ç ”ç©¶çš„é‡è¦é¢†åŸŸï¼Œå¹¿æ³›åº”ç”¨äºå®è·µä¸­ã€‚è¿™äº›åŸºå‡†æµ‹è¯•è¯„ä¼°LLMsåœ¨ç‰¹å®šä»£ç ç›¸å…³ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œå¦‚ä»£ç ç†è§£å’Œç”Ÿæˆã€‚æ„å»ºä»£ç åŸºå‡†æµ‹è¯•çš„å…³é”®æ­¥éª¤æ˜¯æç¤ºè®¾è®¡ã€‚ç„¶è€Œï¼Œç°æœ‰åŸºå‡†æµ‹è¯•é€šå¸¸ä¾èµ–äºæ¯ä¸ªä»»åŠ¡çš„å•ä¸€æç¤ºæ¨¡æ¿ï¼Œå®¹æ˜“å‡ºç°æç¤ºæ•æ„Ÿæ€§é—®é¢˜ï¼Œå³å¾®å°çš„æç¤ºå˜åŒ–å¯èƒ½å¯¼è‡´æ€§èƒ½çš„æ˜¾è‘—æ³¢åŠ¨ï¼Œä»è€Œå½±å“æ¨¡å‹èƒ½åŠ›çš„è¯„ä¼°ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šç”¨æ¡†æ¶ï¼Œé€šè¿‡ä¿®æ”¹æç¤ºæ¨¡æ¿æ¥å°½å¯èƒ½ä¿ç•™å…¶è¯­ä¹‰å’Œç»“æ„ï¼Œå¹¶åœ¨åä¸ªå¼€æºLLMsä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼Œç»“æœè¡¨æ˜æç¤ºçš„ç»†å¾®å˜åŒ–ä¼šå¯¼è‡´æ˜¾è‘—çš„æ€§èƒ½å˜åŒ–ï¼Œå¼ºè°ƒäº†åœ¨è®¾è®¡æœªæ¥ä»£ç åŸºå‡†æµ‹è¯•æ—¶è€ƒè™‘æç¤ºæ•æ„Ÿæ€§çš„é‡è¦æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰ä»£ç åŸºå‡†æµ‹è¯•ä¸­æç¤ºæ•æ„Ÿæ€§çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–å•ä¸€çš„æç¤ºæ¨¡æ¿ï¼Œå¯¼è‡´å¾®å°å˜åŒ–å¼•èµ·çš„æ€§èƒ½æ³¢åŠ¨ï¼Œå½±å“æ¨¡å‹èƒ½åŠ›çš„å¯é è¯„ä¼°ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§é€šç”¨æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ä¿®æ”¹æç¤ºæ¨¡æ¿çš„æ–¹å¼ï¼Œå°½å¯èƒ½ä¿ç•™å…¶è¯­ä¹‰å’Œç»“æ„ï¼Œä»è€Œå‡è½»æç¤ºæ•æ„Ÿæ€§å¯¹è¯„ä¼°ç»“æœçš„å½±å“ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æç¤ºæ¨¡æ¿çš„è®¾è®¡ä¸ä¿®æ”¹ã€å®éªŒè®¾è®¡ã€æ€§èƒ½è¯„ä¼°ç­‰ä¸»è¦æ¨¡å—ã€‚æ¡†æ¶é¦–å…ˆç”Ÿæˆ100ä¸ªè¯­ä¹‰ç›¸ä¼¼çš„æç¤ºæ¨¡æ¿ï¼Œç„¶ååœ¨å…«ä¸ªä»£ç åŸºå‡†ä»»åŠ¡ä¸Šè¿›è¡Œå®éªŒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†ä¸€ä¸ªç³»ç»ŸåŒ–çš„æ–¹æ³•æ¥ç”Ÿæˆå’Œè¯„ä¼°å¤šä¸ªæç¤ºæ¨¡æ¿ï¼Œæ˜¾è‘—æé«˜äº†å¯¹æ¨¡å‹æ€§èƒ½è¯„ä¼°çš„å¯é æ€§ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´å…¨é¢åœ°åæ˜ æ¨¡å‹èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œä½¿ç”¨äº†å¤šç§ç»Ÿè®¡æŒ‡æ ‡æ¥åˆ†ææ¨¡å‹æ€§èƒ½ï¼ŒåŒ…æ‹¬ç»å¯¹æ€§èƒ½å’Œç›¸å¯¹æ€§èƒ½çš„æ¯”è¾ƒï¼Œç¡®ä¿è¯„ä¼°ç»“æœçš„å…¨é¢æ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæç¤ºçš„å¾®å°å˜åŒ–å¯å¯¼è‡´æ¨¡å‹æ€§èƒ½çš„æ˜¾è‘—æ³¢åŠ¨ï¼Œç”šè‡³å½±å“ä¸åŒæ¨¡å‹çš„æ€§èƒ½æ’åã€‚å…·ä½“è€Œè¨€ï¼Œåœ¨åä¸ªå¼€æºLLMsçš„å®éªŒä¸­ï¼Œæ€§èƒ½å˜åŒ–å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œå¼ºè°ƒäº†åœ¨è®¾è®¡åŸºå‡†æµ‹è¯•æ—¶è€ƒè™‘æç¤ºæ•æ„Ÿæ€§çš„å¿…è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è½¯ä»¶å·¥ç¨‹ä¸­çš„ä»£ç ç”Ÿæˆä¸ç†è§£ã€è‡ªåŠ¨åŒ–æµ‹è¯•å’Œä»£ç å®¡æŸ¥ç­‰ã€‚é€šè¿‡æé«˜ä»£ç åŸºå‡†æµ‹è¯•çš„å¯é æ€§ï¼Œèƒ½å¤Ÿä¸ºå¼€å‘è€…æä¾›æ›´å‡†ç¡®çš„æ¨¡å‹æ€§èƒ½è¯„ä¼°ï¼Œè¿›è€Œæ¨åŠ¨LLMsåœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§å’Œå¯é æ€§ã€‚æœªæ¥ï¼Œè¯¥æ¡†æ¶è¿˜å¯èƒ½æ‰©å±•åˆ°å…¶ä»–é¢†åŸŸçš„åŸºå‡†æµ‹è¯•è®¾è®¡ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In the era of large language models (LLMs), code benchmarks have become an important research area in software engineering and are widely used by practitioners. These benchmarks evaluate the performance of LLMs on specific code-related tasks, such as code understanding and generation. A critical step in constructing code benchmarks is the design of prompts. However, as existing code benchmarks typically rely on a single prompt template per task, they are prone to the issue of prompt sensitivity, where minor prompt variations could result in substantial performance variations, leading to unreliable evaluations of model capabilities.
>   While previous studies have explored prompt sensitivity, their experimental designs and findings are limited to traditional natural language processing (NLP) tasks. In this paper, we present an empirical study to investigate prompt sensitivity in code benchmarks. We first propose a general framework that modifies prompt templates in a manner that preserves both their semantics and their structure as much as possible. Based on the framework, we conduct extensive experiments across eight code benchmark tasks on 10 representative open-source LLMs, with each task featuring 100 semantically similar prompt templates. We then analyze the evaluation results using various statistical metrics, focusing on both absolute and relative model performance. Our findings suggest that even slight prompt variations can lead to significant shifts in performance. Additionally, we observe that such variations can introduce inconsistencies in the performance rankings across different models. These insights highlight the need for considering prompt sensitivity when designing future code benchmarks, to ensure more reliable and accurate evaluation of LLM capabilities.

