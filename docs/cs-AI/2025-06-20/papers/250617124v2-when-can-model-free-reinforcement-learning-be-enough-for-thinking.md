---
layout: default
title: When Can Model-Free Reinforcement Learning be Enough for Thinking?
---

# When Can Model-Free Reinforcement Learning be Enough for Thinking?

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.17124" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.17124v2</a>
  <a href="https://arxiv.org/pdf/2506.17124.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.17124v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.17124v2', 'When Can Model-Free Reinforcement Learning be Enough for Thinking?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Josiah P. Hanna, Nicholas E. Corrado

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-20 (æ›´æ–°: 2025-10-25)

**å¤‡æ³¨**: 26 pages, 4 figures, Accepted to NeurIPS 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ€ç»´é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ä»¥æ¨åŠ¨æ— æ¨¡å‹å¼ºåŒ–å­¦ä¹ çš„æ€ç»´èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ— æ¨¡å‹å¼ºåŒ–å­¦ä¹ ` `æ€ç»´èƒ½åŠ›` `é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹` `ç­–ç•¥åˆå§‹åŒ–` `å¤šä»»åŠ¡é¢„è®­ç»ƒ` `æ™ºèƒ½ä»£ç†` `è‡ªåŠ¨åŒ–å†³ç­–` `äººæœºäº¤äº’`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç°æœ‰çš„æ— æ¨¡å‹å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨è®­ç»ƒæ€ç»´èƒ½åŠ›æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œæ€ç»´è¡Œä¸ºæ— æ³•ç›´æ¥è·å¾—å¥–åŠ±æˆ–æ”¹å˜ç¯å¢ƒçŠ¶æ€ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šè®ºæ–‡æå‡ºæ€ç»´é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ï¼Œæ‰©å±•ç»å…¸MDPä»¥åŒ…å«æ€ç»´çŠ¶æ€å’Œæ€ç»´è¡Œä¸ºï¼Œä»ç†è®ºä¸Šæ¢è®¨æ€ç»´çš„äº§ç”Ÿæ¡ä»¶ã€‚
3. å®éªŒæˆ–æ•ˆæœï¼šç ”ç©¶è¡¨æ˜ï¼Œå¼€æºå¤§å‹è¯­è¨€æ¨¡å‹æ»¡è¶³ç†è®ºé¢„æµ‹çš„æ¡ä»¶ï¼Œèƒ½å¤Ÿäº§ç”Ÿç±»ä¼¼æ€ç»´çš„è¡Œä¸ºï¼Œå¹¶åœ¨ç‰¹å®šç©å…·é¢†åŸŸä¸­å®ç°æ›´é«˜çš„æ•°æ®æ•ˆç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘æœŸå…³äºå¤§å‹è¯­è¨€æ¨¡å‹çš„ç ”ç©¶è¡¨æ˜ï¼Œæ— æ¨¡å‹å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ä»¥ç”¨äºè®­ç»ƒç±»ä¼¼æ€ç»´çš„èƒ½åŠ›ã€‚æ€ç»´è¡Œä¸ºçš„å‡ºç°ä»¤äººå…³æ³¨ï¼Œå› ä¸ºè¿™äº›è¡Œä¸ºæ—¢ä¸äº§ç”Ÿå¥–åŠ±ï¼Œä¹Ÿä¸æ”¹å˜å¤–éƒ¨ä¸–ç•ŒçŠ¶æ€ä»¥æé«˜è·å¾—å¥–åŠ±çš„å¯èƒ½æ€§ã€‚æœ¬æ–‡æ—¨åœ¨å»ºç«‹ä¸€ä¸ªä¸é¢†åŸŸæ— å…³çš„ç†è§£ï¼Œæ¢è®¨ä½•æ—¶æ— æ¨¡å‹RLèƒ½å¤Ÿä½œä¸ºå¥–åŠ±æœ€å¤§åŒ–çš„ç­–ç•¥æ¥å¼•å‘æ€ç»´ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é¦–å…ˆå¼•å…¥äº†ä¸€ä¸ªç†è®ºæ¨¡å‹ï¼Œç§°ä¸ºæ€ç»´é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ï¼Œè¯¥æ¨¡å‹åœ¨ç»å…¸MDPæ¨¡å‹çš„åŸºç¡€ä¸Šï¼Œæœ€å°åŒ–åœ°æ‰©å±•äº†æŠ½è±¡çš„æ€ç»´çŠ¶æ€å’Œæ€ç»´è¡Œä¸ºã€‚é€šè¿‡æ€ç»´MDPæ¨¡å‹ï¼Œæˆ‘ä»¬è¯æ˜äº†ç­–ç•¥åˆå§‹åŒ–åœ¨æ€ç»´æ˜¯å¦å‡ºç°ä¸­çš„é‡è¦æ€§ï¼Œå¹¶æ­£å¼å±•ç¤ºäº†æ€ç»´è¡Œä¸ºç­‰åŒäºä»£ç†åœ¨ç»§ç»­è¡ŒåŠ¨å‰é€‰æ‹©è¿›è¡Œç­–ç•¥æ”¹è¿›çš„ä¸€æ­¥ã€‚æœ€åï¼Œæˆ‘ä»¬å‡è®¾äº†èƒ½å¤Ÿä½¿æ€ç»´åœ¨è¯­è¨€ç”Ÿæˆä¹‹å¤–å­¦ä¹ çš„å……åˆ†æ¡ä»¶ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªç©å…·é¢†åŸŸï¼Œé€šè¿‡å¤šä»»åŠ¡é¢„è®­ç»ƒå’ŒæŒ‡å®šæ€ç»´è¡Œä¸ºçš„ç»“åˆï¼Œèƒ½å¤Ÿå®ç°æ¯”éæ€ç»´ä»£ç†æ›´é«˜çš„æ•°æ®æ•ˆç‡çš„RLã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡è§£å†³çš„é—®é¢˜æ˜¯å¦‚ä½•åœ¨æ— æ¨¡å‹å¼ºåŒ–å­¦ä¹ ä¸­å¼•å‘æ€ç»´è¡Œä¸ºï¼Œç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆåˆ©ç”¨æ€ç»´è¡Œä¸ºè¿›è¡Œå¥–åŠ±æœ€å¤§åŒ–ï¼Œå¯¼è‡´æ€ç»´èƒ½åŠ›çš„ç¼ºå¤±ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å¼•å…¥æ€ç»´é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ï¼Œé€šè¿‡æ‰©å±•ç»å…¸MDPæ¨¡å‹ï¼Œå®šä¹‰æ€ç»´çŠ¶æ€å’Œæ€ç»´è¡Œä¸ºï¼Œä»¥ç†è®ºä¸Šåˆ†ææ€ç»´çš„äº§ç”Ÿæ¡ä»¶ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ€ç»´MDPæ¨¡å‹çš„æ„å»ºã€ç­–ç•¥åˆå§‹åŒ–çš„é‡è¦æ€§åˆ†æä»¥åŠæ€ç»´è¡Œä¸ºçš„å®šä¹‰ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬ç†è®ºæ¨¡å‹æ„å»ºã€ç­–ç•¥æ”¹è¿›æ­¥éª¤çš„åˆ†æå’Œå®éªŒéªŒè¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†æ€ç»´MDPè¿™ä¸€æ–°æ¨¡å‹ï¼Œæ˜ç¡®äº†æ€ç»´è¡Œä¸ºä¸ç­–ç•¥æ”¹è¿›ä¹‹é—´çš„ç­‰ä»·å…³ç³»ï¼Œæ­ç¤ºäº†ç­–ç•¥åˆå§‹åŒ–å¯¹æ€ç»´äº§ç”Ÿçš„å½±å“ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬æ€ç»´çŠ¶æ€å’Œæ€ç»´è¡Œä¸ºçš„å®šä¹‰ï¼Œç­–ç•¥åˆå§‹åŒ–çš„å…·ä½“æ–¹æ³•ï¼Œä»¥åŠåœ¨ç©å…·é¢†åŸŸä¸­å¤šä»»åŠ¡é¢„è®­ç»ƒä¸æ€ç»´è¡Œä¸ºç»“åˆçš„å®ç°ç»†èŠ‚ã€‚é€šè¿‡è¿™äº›è®¾è®¡ï¼Œè®ºæ–‡å±•ç¤ºäº†å¦‚ä½•åœ¨æ— æ¨¡å‹RLä¸­æœ‰æ•ˆå¼•å…¥æ€ç»´èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå¼€æºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ»¡è¶³ç†è®ºæ¡ä»¶ä¸‹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆäº§ç”Ÿæ€ç»´è¡Œä¸ºã€‚é€šè¿‡åœ¨ç‰¹å®šç©å…·é¢†åŸŸä¸­è¿›è¡Œå¤šä»»åŠ¡é¢„è®­ç»ƒï¼Œç ”ç©¶å®ç°äº†æ¯”éæ€ç»´ä»£ç†æ›´é«˜çš„æ•°æ®æ•ˆç‡ï¼Œå±•ç¤ºäº†æ€ç»´èƒ½åŠ›åœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„é‡è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½ä»£ç†ã€è‡ªåŠ¨åŒ–å†³ç­–ç³»ç»Ÿå’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡æå‡æ— æ¨¡å‹å¼ºåŒ–å­¦ä¹ çš„æ€ç»´èƒ½åŠ›ï¼Œå¯ä»¥ä½¿æ™ºèƒ½ç³»ç»Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­æ›´æœ‰æ•ˆåœ°è¿›è¡Œå†³ç­–ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent work on large language models has demonstrated the use of model-free reinforcement learning (RL) to train reasoning-like capabilities. The emergence of "thinking" through model-free RL is interesting as thinking actions neither produce reward nor change the external world state to one where the agent is more likely to get reward. This paper seeks to build a domain-independent understanding of when model-free RL will lead to such "thinking" as a strategy for reward maximization. To build this understanding, we first introduce a theoretical model which we call a thought Markov decision process (MDP). Thought MDPs minimally extend the classical MDP model to include an abstract notion of thought state and thought action. Using the thought MDP model, we prove the importance of policy initialization in determining whether or not thinking emerges and show formally that thought actions are equivalent to the agent choosing to perform a step of policy improvement before continuing to act. We then show that open-source LLMs satisfy the conditions that our theory predicts are necessary for model-free RL to produce thinking-like behavior. Finally, we hypothesize sufficient conditions that would enable thinking to be learned outside of language generation and introduce a toy domain where a combination of multi-task pre-training and designated thought actions enable more data-efficient RL compared to non-thinking agents.

