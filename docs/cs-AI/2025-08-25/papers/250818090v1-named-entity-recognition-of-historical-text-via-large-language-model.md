---
layout: default
title: Named Entity Recognition of Historical Text via Large Language Model
---

# Named Entity Recognition of Historical Text via Large Language Model

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.18090" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.18090v1</a>
  <a href="https://arxiv.org/pdf/2508.18090.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.18090v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.18090v1', 'Named Entity Recognition of Historical Text via Large Language Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shibingfeng Zhang, Giovanni Colavizza

**åˆ†ç±»**: cs.DL, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-25

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è§£å†³å†å²æ–‡æœ¬å‘½åå®ä½“è¯†åˆ«é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å‘½åå®ä½“è¯†åˆ«` `å¤§å‹è¯­è¨€æ¨¡å‹` `å†å²æ–‡æœ¬` `ä¿¡æ¯æå–` `é›¶æ ·æœ¬å­¦ä¹ ` `å°‘æ ·æœ¬å­¦ä¹ ` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»Ÿçš„å‘½åå®ä½“è¯†åˆ«æ–¹æ³•ä¾èµ–äºå¤§é‡æ ‡æ³¨æ•°æ®ï¼Œè€Œå†å²æ–‡æœ¬çš„æ ‡æ³¨æ•°æ®ç¨€ç¼ºï¼Œå¯¼è‡´NERç³»ç»Ÿçš„å¼€å‘é¢ä¸´æŒ‘æˆ˜ã€‚
2. æœ¬æ–‡æå‡ºåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œå†å²æ–‡æœ¬çš„NERï¼Œé‡‡ç”¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æç¤ºç­–ç•¥ï¼Œå‡å°‘å¯¹ä»»åŠ¡ç‰¹å®šè®­ç»ƒæ•°æ®çš„ä¾èµ–ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLLMsåœ¨HIPE-2022æ•°æ®é›†ä¸Šçš„NERä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ï¼Œå°½ç®¡æœªè¾¾åˆ°å®Œå…¨ç›‘ç£æ¨¡å‹çš„æ€§èƒ½ï¼Œä½†ç»“æœä»ç„¶å…·æœ‰å‰æ™¯ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šçš„å¤šåŠŸèƒ½æ€§ï¼Œå…¶ä¸­å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰æ˜¯è¯†åˆ«å’Œåˆ†ç±»æ–‡æœ¬ä¸­ä¸“æœ‰åè¯çš„é‡è¦ä»»åŠ¡ã€‚ä¼ ç»Ÿçš„NERæ–¹æ³•ä¾èµ–äºå¤§é‡æ ‡æ³¨æ•°æ®ï¼Œä½†å†å²æ–‡æœ¬çš„æ ‡æ³¨æ•°æ®ç¨€ç¼ºä¸”æˆæœ¬é«˜æ˜‚ã€‚æœ¬æ–‡æ¢è®¨äº†åœ¨å†å²æ–‡çŒ®ä¸­åº”ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡ŒNERçš„å¯è¡Œæ€§ï¼Œé‡‡ç”¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æç¤ºç­–ç•¥ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡æ€§èƒ½ä¸åŠå®Œå…¨ç›‘ç£æ¨¡å‹ï¼Œä½†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä½èµ„æºç¯å¢ƒä¸‹çš„ä¿¡æ¯æå–ä¸­å±•ç°å‡ºè‰¯å¥½çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å†å²æ–‡æœ¬ä¸­çš„å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•é¢ä¸´æ ‡æ³¨æ•°æ®ç¨€ç¼ºå’Œå†å²è¯­è¨€å˜å¼‚æ€§ç­‰æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡åº”ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¹¶é‡‡ç”¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æç¤ºç­–ç•¥ï¼Œå‡å°‘å¯¹å¤§é‡æ ‡æ³¨æ•°æ®çš„éœ€æ±‚ï¼Œä»è€Œæé«˜å†å²æ–‡æœ¬çš„NERæ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ–¹æ³•åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹é€‰æ‹©ã€æç¤ºè®¾è®¡å’Œç»“æœè¯„ä¼°ç­‰ä¸»è¦æ¨¡å—ï¼Œåˆ©ç”¨LLMsè¿›è¡Œå®ä½“è¯†åˆ«ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°åœ¨äºå°†å¤§å‹è¯­è¨€æ¨¡å‹åº”ç”¨äºä½èµ„æºçš„å†å²æ–‡æœ¬NERä»»åŠ¡ï¼Œæä¾›äº†ä¸€ç§æ–°çš„ä¿¡æ¯æå–æ–¹å¼ï¼ŒåŒºåˆ«äºä¼ ç»Ÿçš„ç›‘ç£å­¦ä¹ æ–¹æ³•ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œé‡‡ç”¨äº†é€‚åº”å†å²æ–‡æœ¬ç‰¹å¾çš„æç¤ºè®¾è®¡ï¼Œç¡®ä¿æ¨¡å‹èƒ½å¤Ÿç†è§£å’Œå¤„ç†å¤è€è¯­è¨€çš„å˜å¼‚æ€§ï¼ŒåŒæ—¶ä¼˜åŒ–äº†æ¨¡å‹çš„å‚æ•°è®¾ç½®ä»¥æé«˜è¯†åˆ«å‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„NERæ–¹æ³•åœ¨HIPE-2022æ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½ï¼Œå°½ç®¡æœªè¾¾åˆ°å®Œå…¨ç›‘ç£æ¨¡å‹çš„æ°´å¹³ï¼Œä½†åœ¨ä½èµ„æºç¯å¢ƒä¸‹çš„è¡¨ç°ä»ç„¶ä»¤äººé¼“èˆï¼Œå±•ç¤ºäº†LLMsåœ¨å†å²æ–‡æœ¬å¤„ç†ä¸­çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å†å²æ–‡çŒ®çš„æ•°å­—åŒ–å¤„ç†ã€ä¿¡æ¯æ£€ç´¢ç³»ç»Ÿä»¥åŠè€ƒå¤å­¦å’Œå†å²ç ”ç©¶ä¸­çš„æ•°æ®æŒ–æ˜ã€‚é€šè¿‡æé«˜å†å²æ–‡æœ¬çš„NERèƒ½åŠ›ï¼Œå¯ä»¥æ›´æœ‰æ•ˆåœ°æå–å’Œç»„ç»‡ä¿¡æ¯ï¼Œä¿ƒè¿›å¯¹å†å²æ•°æ®çš„æ·±å…¥åˆ†æä¸ç†è§£ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models have demonstrated remarkable versatility across a wide range of natural language processing tasks and domains. One such task is Named Entity Recognition (NER), which involves identifying and classifying proper names in text, such as people, organizations, locations, dates, and other specific entities. NER plays a crucial role in extracting information from unstructured textual data, enabling downstream applications such as information retrieval from unstructured text.
>   Traditionally, NER is addressed using supervised machine learning approaches, which require large amounts of annotated training data. However, historical texts present a unique challenge, as the annotated datasets are often scarce or nonexistent, due to the high cost and expertise required for manual labeling. In addition, the variability and noise inherent in historical language, such as inconsistent spelling and archaic vocabulary, further complicate the development of reliable NER systems for these sources.
>   In this study, we explore the feasibility of applying LLMs to NER in historical documents using zero-shot and few-shot prompting strategies, which require little to no task-specific training data. Our experiments, conducted on the HIPE-2022 (Identifying Historical People, Places and other Entities) dataset, show that LLMs can achieve reasonably strong performance on NER tasks in this setting. While their performance falls short of fully supervised models trained on domain-specific annotations, the results are nevertheless promising. These findings suggest that LLMs offer a viable and efficient alternative for information extraction in low-resource or historically significant corpora, where traditional supervised methods are infeasible.

