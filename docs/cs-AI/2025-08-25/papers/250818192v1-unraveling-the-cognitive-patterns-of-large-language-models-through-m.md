---
layout: default
title: Unraveling the cognitive patterns of Large Language Models through module communities
---

# Unraveling the cognitive patterns of Large Language Models through module communities

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2508.18192" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2508.18192v1</a>
  <a href="https://arxiv.org/pdf/2508.18192.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2508.18192v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2508.18192v1', 'Unraveling the cognitive patterns of Large Language Models through module communities')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kushal Raj Bhandari, Pin-Yu Chen, Jianxi Gao

**åˆ†ç±»**: cs.AI, cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-08-25

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºæ¨¡å—ç¤¾åŒºçš„æ¡†æ¶ä»¥æ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹çš„è®¤çŸ¥æ¨¡å¼**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `è®¤çŸ¥æ¨¡å¼` `æ¨¡å—ç¤¾åŒº` `ç½‘ç»œåˆ†æ` `ç”Ÿç‰©å­¦è®¤çŸ¥` `æŠ€èƒ½è·å–` `åŠ¨æ€äº¤äº’` `ç¥ç»å¯å¡‘æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹çš„å†…éƒ¨æœºåˆ¶å¤æ‚ï¼Œéš¾ä»¥ç†è§£å…¶è®¤çŸ¥è¿‡ç¨‹å’Œæ¶æ„ï¼Œé™åˆ¶äº†å…¶è¿›ä¸€æ­¥åº”ç”¨å’Œä¼˜åŒ–ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§ç½‘ç»œåŸºç¡€æ¡†æ¶ï¼Œç»“åˆç”Ÿç‰©å­¦è®¤çŸ¥åŸç†ï¼Œåˆ†æLLMçš„æ¨¡å—ç¤¾åŒºåŠå…¶æŠ€èƒ½åˆ†å¸ƒï¼Œæ­ç¤ºå…¶è®¤çŸ¥æ¨¡å¼ã€‚
3. ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼ŒLLMsçš„æŠ€èƒ½è·å–å—ç›ŠäºåŠ¨æ€äº¤äº’å’Œç¥ç»å¯å¡‘æ€§ï¼Œæä¾›äº†æ–°çš„å¾®è°ƒç­–ç•¥å»ºè®®ï¼Œæå‡äº†æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç§‘å­¦ã€å·¥ç¨‹å’Œç¤¾ä¼šç­‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œç„¶è€Œå…¶å†…éƒ¨æœºåˆ¶ä»ç„¶éšè—åœ¨æ•°åäº¿ä¸ªå‚æ•°å’Œå¤æ‚ç»“æ„ä¸­ï¼Œéš¾ä»¥ç†è§£ã€‚æœ¬æ–‡é€šè¿‡å€Ÿé‰´ç”Ÿç‰©å­¦ä¸­çš„è®¤çŸ¥ç†è§£æ–¹æ³•ï¼Œæå‡ºäº†ä¸€ç§ç½‘ç»œåŸºç¡€æ¡†æ¶ï¼Œå°†è®¤çŸ¥æŠ€èƒ½ã€LLMæ¶æ„å’Œæ•°æ®é›†è”ç³»èµ·æ¥ï¼Œæ¨åŠ¨äº†åŸºç¡€æ¨¡å‹åˆ†æçš„èŒƒå¼è½¬å˜ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒLLMsçš„æ¨¡å—ç¤¾åŒºå±•ç°å‡ºç‹¬ç‰¹çš„æŠ€èƒ½æ¨¡å¼ï¼Œéƒ¨åˆ†åæ˜ äº†é¸Ÿç±»å’Œå°å‹å“ºä¹³åŠ¨ç‰©å¤§è„‘çš„åˆ†å¸ƒå¼è®¤çŸ¥ç»„ç»‡ï¼ŒåŒæ—¶å¼ºè°ƒäº†åŠ¨æ€è·¨åŒºåŸŸäº¤äº’å’Œç¥ç»å¯å¡‘æ€§åœ¨æŠ€èƒ½è·å–ä¸­çš„é‡è¦æ€§ã€‚è¯¥æ¡†æ¶ä¸ºLLMçš„å¯è§£é‡Šæ€§æä¾›äº†æ–°è§†è§’ï¼Œå¹¶å»ºè®®æœ‰æ•ˆçš„å¾®è°ƒç­–ç•¥åº”åˆ©ç”¨åˆ†å¸ƒå¼å­¦ä¹ åŠ¨æ€ï¼Œè€ŒéåƒµåŒ–çš„æ¨¡å—å¹²é¢„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å†…éƒ¨æœºåˆ¶ä¸é€æ˜çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•æ— æ³•æœ‰æ•ˆæ­ç¤ºå…¶è®¤çŸ¥è¿‡ç¨‹å’Œæ¶æ„çš„å¤æ‚æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å€Ÿé‰´ç”Ÿç‰©å­¦ä¸­çš„è®¤çŸ¥ç†è§£æ–¹æ³•ï¼Œæ„å»ºä¸€ä¸ªç½‘ç»œåŸºç¡€æ¡†æ¶ï¼Œå°†è®¤çŸ¥æŠ€èƒ½ã€LLMæ¶æ„å’Œæ•°æ®é›†è¿›è¡Œå…³è”ï¼Œæ­ç¤ºæ¨¡å—ç¤¾åŒºçš„æŠ€èƒ½åˆ†å¸ƒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…æ‹¬å¤šä¸ªæ¨¡å—ï¼Œé¦–å…ˆåˆ†æLLMsçš„æ¶æ„å’Œæ•°æ®é›†ï¼Œç„¶åé€šè¿‡ç½‘ç»œåˆ†ææ–¹æ³•è¯†åˆ«æ¨¡å—ç¤¾åŒºï¼Œæœ€åè¯„ä¼°å…¶æŠ€èƒ½æ¨¡å¼ä¸ç”Ÿç‰©ç³»ç»Ÿçš„ç›¸ä¼¼æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†ç”Ÿç‰©å­¦çš„è®¤çŸ¥æ¨¡å¼ä¸æœºå™¨å­¦ä¹ ç›¸ç»“åˆï¼Œæå‡ºäº†åŠ¨æ€äº¤äº’å’Œç¥ç»å¯å¡‘æ€§åœ¨æŠ€èƒ½è·å–ä¸­çš„é‡è¦æ€§ï¼ŒåŒºåˆ«äºä¼ ç»Ÿçš„é™æ€æ¨¡å—åˆ†ææ–¹æ³•ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„ç½‘ç»œç»“æ„å’ŒæŸå¤±å‡½æ•°ï¼Œä»¥ä¼˜åŒ–æ¨¡å—é—´çš„äº¤äº’ï¼Œç¡®ä¿æŠ€èƒ½æ¨¡å¼çš„æœ‰æ•ˆè¯†åˆ«å’Œåˆ†æã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMsçš„æŠ€èƒ½è·å–åœ¨åŠ¨æ€äº¤äº’å’Œç¥ç»å¯å¡‘æ€§æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œä¸ä¼ ç»Ÿç”Ÿç‰©ç³»ç»Ÿç›¸æ¯”ï¼ŒæŠ€èƒ½æ¨¡å¼çš„åˆ†å¸ƒå¼ç‰¹å¾å¾—åˆ°äº†æœ‰æ•ˆéªŒè¯ã€‚å…·ä½“æ€§èƒ½æ•°æ®å’Œå¯¹æ¯”åŸºçº¿çš„æå‡å¹…åº¦å°šæœªæ˜ç¡®ï¼Œä½†ç ”ç©¶æä¾›äº†æ–°çš„åˆ†æè§†è§’å’Œå¾®è°ƒç­–ç•¥å»ºè®®ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½å¯¹è¯ç³»ç»Ÿå’Œè‡ªåŠ¨åŒ–åŒ»ç–—è¯Šæ–­ç­‰ã€‚é€šè¿‡æ·±å…¥ç†è§£å¤§å‹è¯­è¨€æ¨¡å‹çš„è®¤çŸ¥æ¨¡å¼ï¼Œå¯ä»¥ä¼˜åŒ–æ¨¡å‹çš„è®­ç»ƒå’Œå¾®è°ƒç­–ç•¥ï¼Œæé«˜å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„è¡¨ç°å’Œå¯é æ€§ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have reshaped our world with significant advancements in science, engineering, and society through applications ranging from scientific discoveries and medical diagnostics to Chatbots. Despite their ubiquity and utility, the underlying mechanisms of LLM remain concealed within billions of parameters and complex structures, making their inner architecture and cognitive processes challenging to comprehend. We address this gap by adopting approaches to understanding emerging cognition in biology and developing a network-based framework that links cognitive skills, LLM architectures, and datasets, ushering in a paradigm shift in foundation model analysis. The skill distribution in the module communities demonstrates that while LLMs do not strictly parallel the focalized specialization observed in specific biological systems, they exhibit unique communities of modules whose emergent skill patterns partially mirror the distributed yet interconnected cognitive organization seen in avian and small mammalian brains. Our numerical results highlight a key divergence from biological systems to LLMs, where skill acquisition benefits substantially from dynamic, cross-regional interactions and neural plasticity. By integrating cognitive science principles with machine learning, our framework provides new insights into LLM interpretability and suggests that effective fine-tuning strategies should leverage distributed learning dynamics rather than rigid modular interventions.

