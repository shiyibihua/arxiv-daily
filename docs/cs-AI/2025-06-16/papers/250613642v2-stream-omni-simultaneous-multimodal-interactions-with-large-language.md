---
layout: default
title: Stream-Omni: Simultaneous Multimodal Interactions with Large Language-Vision-Speech Model
---

# Stream-Omni: Simultaneous Multimodal Interactions with Large Language-Vision-Speech Model

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.13642" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.13642v2</a>
  <a href="https://arxiv.org/pdf/2506.13642.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.13642v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.13642v2', 'Stream-Omni: Simultaneous Multimodal Interactions with Large Language-Vision-Speech Model')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Shaolei Zhang, Shoutao Guo, Qingkai Fang, Yan Zhou, Yang Feng

**ÂàÜÁ±ª**: cs.AI, cs.CL, cs.CV, cs.SD, eess.AS

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-16 (Êõ¥Êñ∞: 2025-06-22)

**Â§áÊ≥®**: Code: https://github.com/ictnlp/Stream-Omni , Model: https://huggingface.co/ICTNLP/stream-omni-8b

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Stream-Omni‰ª•ÂÆûÁé∞Â§öÊ®°ÊÄÅÈ´òÊïà‰∫§‰∫í**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊ®°ÊÄÅÊ®°Âûã` `ËØ≠Ë®Ä-ËßÜËßâ-ËØ≠Èü≥` `Ê®°ÊÄÅÂØπÈΩê` `È´òÊïà‰∫§‰∫í` `Êô∫ËÉΩÂä©Êâã` `Ëá™Âä®ËØ≠Èü≥ËØÜÂà´` `ËßÜËßâÈóÆÁ≠î`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂ§öÊ®°ÊÄÅÊ®°ÂûãÂú®Ê®°ÊÄÅÂØπÈΩê‰∏ä‰æùËµñ‰∫éÂ§ßËßÑÊ®°Êï∞ÊçÆÔºåÂØºËá¥ÊïàÁéá‰Ωé‰∏ãÔºåÂ∞§ÂÖ∂ÊòØÂú®ËØ≠Èü≥Ê®°ÊÄÅ‰∏ä„ÄÇ
2. Êú¨ÊñáÊèêÂá∫Stream-OmniÔºåÈÄöËøáÊõ¥ÊúâÁõÆÁöÑÂú∞Âª∫Ê®°Ê®°ÊÄÅ‰πãÈó¥ÁöÑÂÖ≥Á≥ªÔºåÂÆûÁé∞È´òÊïàÁöÑÊ®°ÊÄÅÂØπÈΩê„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåStream-OmniÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåÂ∞§ÂÖ∂Âú®ËßÜËßâÁêÜËß£ÂíåËØ≠Èü≥‰∫§‰∫í‰ªªÂä°‰∏äÊúâÊòæËëóÊèêÂçá„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÈöèÁùÄÁ±ª‰ººGPT-4oÁöÑÂ§ßÂûãÂ§öÊ®°ÊÄÅÊ®°ÂûãÔºàLMMÔºâÁöÑÂá∫Áé∞ÔºåÊé¢Á¥¢ÊñáÊú¨„ÄÅËßÜËßâÂíåËØ≠Èü≥Ê®°ÊÄÅÁöÑÊï¥Âêà‰ª•ÊîØÊåÅÊõ¥ÁÅµÊ¥ªÁöÑÂ§öÊ®°ÊÄÅ‰∫§‰∫íÂèòÂæóÂ∞§‰∏∫ÈáçË¶Å„ÄÇÁé∞ÊúâLMMÈÄöÂ∏∏Ê≤øÂ∫èÂàóÁª¥Â∫¶ËøûÊé•Ê®°ÊÄÅË°®Á§∫ÔºåÂπ∂Â∞ÜÂÖ∂ËæìÂÖ•Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâ‰∏ªÂπ≤„ÄÇÂ∞ΩÁÆ°Â∫èÂàóÁª¥Â∫¶ËøûÊé•Âú®Ê®°ÊÄÅÊï¥Âêà‰∏äËæÉ‰∏∫Áõ¥Êé•Ôºå‰ΩÜÂæÄÂæÄ‰æùËµñ‰∫éÂ§ßËßÑÊ®°Êï∞ÊçÆÊù•Â≠¶‰π†Ê®°ÊÄÅÂØπÈΩê„ÄÇÊú¨ÊñáÊèêÂá∫Stream-OmniÔºå‰∏Ä‰∏™ÂÖ∑ÊúâÈ´òÊïàÊ®°ÊÄÅÂØπÈΩêÁöÑÂ§ßÂûãËØ≠Ë®Ä-ËßÜËßâ-ËØ≠Èü≥Ê®°ÂûãÔºåËÉΩÂ§üÂêåÊó∂ÊîØÊåÅÂ§öÁßçÊ®°ÊÄÅÁªÑÂêà‰∏ãÁöÑ‰∫§‰∫í„ÄÇStream-OmniÂà©Áî®LLM‰Ωú‰∏∫‰∏ªÂπ≤ÔºåÂπ∂Âü∫‰∫éÊ®°ÊÄÅ‰πãÈó¥ÁöÑÂÖ≥Á≥ªÂØπËßÜËßâÂíåËØ≠Èü≥ËøõË°åÂØπÈΩê„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåStream-OmniÂú®ËßÜËßâÁêÜËß£„ÄÅËØ≠Èü≥‰∫§‰∫íÂíåËßÜËßâÂºïÂØºÁöÑËØ≠Èü≥‰∫§‰∫í‰ªªÂä°‰∏äË°®Áé∞Âá∫Ëâ≤„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâÂ§öÊ®°ÊÄÅÊ®°ÂûãÂú®Ê®°ÊÄÅÂØπÈΩê‰∏ä‰æùËµñÂ§ßËßÑÊ®°Êï∞ÊçÆÁöÑÈóÆÈ¢òÔºåÂ∞§ÂÖ∂ÊòØÂú®ËØ≠Èü≥Ê®°ÊÄÅÁöÑÊïàÁéá‰Ωé‰∏ã„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöStream-OmniÈÄöËøáÊõ¥ÊúâÁõÆÁöÑÂú∞Âª∫Ê®°Ê®°ÊÄÅ‰πãÈó¥ÁöÑÂÖ≥Á≥ªÔºåÈááÁî®‰∏çÂêåÁöÑÂØπÈΩêÁ≠ñÁï•Êù•ÂÆûÁé∞È´òÊïàÁöÑÊ®°ÊÄÅÊï¥ÂêàÔºåÂáèÂ∞ëÂØπÊï∞ÊçÆÁöÑ‰æùËµñ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöStream-OmniÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏Ä‰∏™Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã‰Ωú‰∏∫‰∏ªÂπ≤ÔºåËßÜËßâÊ®°ÊÄÅÈÄöËøáÂ∫èÂàóÁª¥Â∫¶ËøûÊé•‰∏éÊñáÊú¨ÂØπÈΩêÔºåËÄåËØ≠Èü≥Ê®°ÊÄÅÂàôÈÄöËøáCTCÔºàConnectionist Temporal ClassificationÔºâÂ±ÇÁª¥Â∫¶Êò†Â∞Ñ‰∏éÊñáÊú¨ÂØπÈΩê„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöStream-OmniÁöÑÊ†∏ÂøÉÂàõÊñ∞Âú®‰∫éÂºïÂÖ•‰∫ÜÂ±ÇÁª¥Â∫¶Êò†Â∞ÑÁöÑÊú∫Âà∂Ôºå‰ΩøÂæóËØ≠Èü≥Ê®°ÊÄÅËÉΩÂ§üÂú®ËæÉÂ∞ëÁöÑÊï∞ÊçÆ‰∏ãÂÆûÁé∞‰∏éÊñáÊú¨ÁöÑÊúâÊïàÂØπÈΩêÔºåËøô‰∏éÁé∞ÊúâÊñπÊ≥ïÁöÑÁÆÄÂçïÂ∫èÂàóËøûÊé•ÊñπÂºèÊúâÊú¨Ë¥®Âå∫Âà´„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆæËÆ°‰∏äÔºåStream-OmniÈááÁî®‰∫ÜCTC-basedÂ±ÇÁª¥Â∫¶Êò†Â∞ÑÊäÄÊúØÔºåÂπ∂Âú®ÊçüÂ§±ÂáΩÊï∞‰∏≠ËÄÉËôë‰∫ÜÊ®°ÊÄÅ‰πãÈó¥ÁöÑÂÖ≥Á≥ªÔºå‰ª•‰ºòÂåñÊ®°ÊÄÅÂØπÈΩêÁöÑÊïàÊûú„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Âú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÔºåStream-OmniÂú®ËßÜËßâÁêÜËß£„ÄÅËØ≠Èü≥‰∫§‰∫íÂíåËßÜËßâÂºïÂØºÁöÑËØ≠Èü≥‰∫§‰∫í‰ªªÂä°‰∏äÂùáË°®Áé∞Âá∫Ëâ≤ÔºåÂ∞§ÂÖ∂Âú®ËØ≠Èü≥Ê®°ÊÄÅÁöÑÂØπÈΩêÊïàÁéá‰∏äÊòæËëóÊèêÂçáÔºåÂáèÂ∞ë‰∫ÜÂØπÂ§ßËßÑÊ®°Êï∞ÊçÆÁöÑ‰æùËµñÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âº∫Â§ßÁöÑÂ§öÊ®°ÊÄÅÂ§ÑÁêÜËÉΩÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

Stream-OmniÁöÑÁ†îÁ©∂ÊàêÊûúÂú®Êô∫ËÉΩÂä©Êâã„ÄÅËá™Âä®ËØ≠Èü≥ËØÜÂà´„ÄÅËßÜËßâÈóÆÁ≠îÁ≠âÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõ„ÄÇÈÄöËøáÈ´òÊïàÁöÑÊ®°ÊÄÅÂØπÈΩêÔºåÁî®Êà∑ÂèØ‰ª•Ëé∑ÂæóÊõ¥ÊµÅÁïÖÁöÑÂ§öÊ®°ÊÄÅ‰∫§‰∫í‰ΩìÈ™åÔºåÊèêÂçá‰∫∫Êú∫‰∫§‰∫íÁöÑËá™ÁÑ∂ÊÄßÂíåÊô∫ËÉΩÊÄß„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> The emergence of GPT-4o-like large multimodal models (LMMs) has raised the exploration of integrating text, vision, and speech modalities to support more flexible multimodal interaction. Existing LMMs typically concatenate representation of modalities along the sequence dimension and feed them into a large language model (LLM) backbone. While sequence-dimension concatenation is straightforward for modality integration, it often relies heavily on large-scale data to learn modality alignments. In this paper, we aim to model the relationships between modalities more purposefully, thereby achieving more efficient and flexible modality alignments. To this end, we propose Stream-Omni, a large language-vision-speech model with efficient modality alignments, which can simultaneously support interactions under various modality combinations. Stream-Omni employs LLM as the backbone and aligns the vision and speech to the text based on their relationships. For vision that is semantically complementary to text, Stream-Omni uses sequence-dimension concatenation to achieve vision-text alignment. For speech that is semantically consistent with text, Stream-Omni introduces a CTC-based layer-dimension mapping to achieve speech-text alignment. In this way, Stream-Omni can achieve modality alignments with less data (especially speech), enabling the transfer of text capabilities to other modalities. Experiments on various benchmarks demonstrate that Stream-Omni achieves strong performance on visual understanding, speech interaction, and vision-grounded speech interaction tasks. Owing to the layer-dimensional mapping, Stream-Omni can simultaneously provide intermediate text outputs (such as ASR transcriptions and model responses) during speech interaction, offering users a comprehensive multimodal experience.

