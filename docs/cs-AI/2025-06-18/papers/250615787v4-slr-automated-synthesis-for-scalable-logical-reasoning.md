---
layout: default
title: SLR: Automated Synthesis for Scalable Logical Reasoning
---

# SLR: Automated Synthesis for Scalable Logical Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.15787" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.15787v4</a>
  <a href="https://arxiv.org/pdf/2506.15787.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.15787v4" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.15787v4', 'SLR: Automated Synthesis for Scalable Logical Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Lukas Helff, Ahmad Omar, Felix Friedrich, Antonia WÃ¼st, Hikaru Shindo, Rupert Mitchell, Tim Woydt, Patrick Schramowski, Wolfgang Stammer, Kristian Kersting

**åˆ†ç±»**: cs.AI, cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-18 (æ›´æ–°: 2025-08-06)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSLRæ¡†æ¶ä»¥å®ç°å¯æ‰©å±•çš„é€»è¾‘æ¨ç†è‡ªåŠ¨åˆæˆ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é€»è¾‘æ¨ç†` `è‡ªåŠ¨åˆæˆ` `å¤§å‹è¯­è¨€æ¨¡å‹` `è¯¾ç¨‹å­¦ä¹ ` `éªŒè¯ç¨‹åº` `æ¨ç†èƒ½åŠ›` `æ— ç›‘ç£å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨é€»è¾‘æ¨ç†ä»»åŠ¡ä¸­å¸¸å¸¸ä¾èµ–äººå·¥æ³¨é‡Šï¼Œç¼ºä¹è‡ªåŠ¨åŒ–å’Œå¯æ‰©å±•æ€§ï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹ã€‚
2. SLRæ¡†æ¶é€šè¿‡è‡ªåŠ¨åˆæˆä»»åŠ¡æç¤ºã€éªŒè¯ç¨‹åºå’ŒçœŸå®è§„åˆ™ï¼Œæä¾›äº†ä¸€ç§æ— éœ€äººå·¥å¹²é¢„çš„é€»è¾‘æ¨ç†è®­ç»ƒæ–¹æ³•ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒSLRæ˜¾è‘—æå‡äº†Llama-3-8Bçš„æ¨ç†å‡†ç¡®ç‡ï¼Œå¹¶åœ¨è®¡ç®—æˆæœ¬ä¸Šä¼˜äºå…¶ä»–æ¨ç†æ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬ä»‹ç»äº†SLRï¼Œä¸€ä¸ªç«¯åˆ°ç«¯æ¡†æ¶ï¼Œç”¨äºé€šè¿‡å¯æ‰©å±•çš„é€»è¾‘æ¨ç†ç³»ç»Ÿè¯„ä¼°å’Œè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€‚SLRæ ¹æ®ç”¨æˆ·çš„ä»»åŠ¡è§„èŒƒï¼Œè‡ªåŠ¨åˆæˆï¼ˆiï¼‰å½’çº³æ¨ç†ä»»åŠ¡çš„æŒ‡ä»¤æç¤ºï¼Œï¼ˆiiï¼‰å¯åœ¨æ¨¡å‹è¾“å‡ºä¸Šæ‰§è¡Œçš„éªŒè¯ç¨‹åºï¼Œä»¥æä¾›å¯éªŒè¯çš„å¥–åŠ±ï¼Œä»¥åŠï¼ˆiiiï¼‰æ½œåœ¨çš„çœŸå®è§„åˆ™ã€‚è¯¥è¿‡ç¨‹å®Œå…¨è‡ªåŠ¨åŒ–ã€å¯æ‰©å±•ï¼Œæ— éœ€äººå·¥æ³¨é‡Šï¼Œå¹¶æä¾›å¯¹ä»»åŠ¡éš¾åº¦çš„ç²¾ç¡®æ§åˆ¶ã€‚ä½¿ç”¨SLRï¼Œæˆ‘ä»¬åˆ›å»ºäº†SLR-Benchï¼Œä¸€ä¸ªåŒ…å«19,000ä¸ªæç¤ºçš„åŸºå‡†ï¼Œåˆ†ä¸º20ä¸ªé€æ­¥å¢åŠ å…³ç³»ã€ç®—æœ¯å’Œé€’å½’å¤æ‚åº¦çš„è¯¾ç¨‹çº§åˆ«ã€‚å¤§è§„æ¨¡è¯„ä¼°æ˜¾ç¤ºï¼Œå½“å‰çš„LLMsèƒ½å¤Ÿç”Ÿæˆè¯­æ³•ä¸Šæœ‰æ•ˆçš„è§„åˆ™ï¼Œä½†åœ¨æ­£ç¡®çš„é€»è¾‘æ¨ç†æ–¹é¢å¸¸å¸¸å¤±è´¥ã€‚æœ€è¿‘çš„æ¨ç†LLMsè¡¨ç°æœ‰æ‰€æ”¹å–„ï¼Œä½†æµ‹è¯•æ—¶è®¡ç®—æˆæœ¬æé«˜ï¼Œè¾¾åˆ°æ¯1,000ä¸ªæç¤ºè¶…è¿‡300ç¾å…ƒã€‚æœ€åï¼Œé€šè¿‡SLRçš„è¯¾ç¨‹å­¦ä¹ ï¼ŒLlama-3-8Båœ¨SLR-Benchä¸Šçš„å‡†ç¡®ç‡ç¿»å€ï¼Œä»¥è¾ƒä½çš„è®¡ç®—æˆæœ¬è¾¾åˆ°äº†ä¸Gemini-Flash-Thinkingç›¸å½“çš„æ°´å¹³ã€‚æ­¤å¤–ï¼Œè¿™äº›æ¨ç†èƒ½åŠ›åœ¨å¹¿æ³›çš„å·²å»ºç«‹åŸºå‡†ä¸Šå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œçªæ˜¾äº†SLRåœ¨ä¸‹æ¸¸æ¨ç†ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰é€»è¾‘æ¨ç†æ¨¡å‹åœ¨ä»»åŠ¡åˆæˆå’Œè¯„ä¼°ä¸­çš„ä½æ•ˆç‡å’Œé«˜æˆæœ¬é—®é¢˜ï¼Œå°¤å…¶æ˜¯ä¾èµ–äººå·¥æ³¨é‡Šçš„å±€é™æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSLRæ¡†æ¶é€šè¿‡è‡ªåŠ¨åŒ–åˆæˆæŒ‡ä»¤æç¤ºã€éªŒè¯ç¨‹åºå’Œæ½œåœ¨è§„åˆ™ï¼Œæä¾›äº†ä¸€ç§é«˜æ•ˆçš„é€»è¾‘æ¨ç†è®­ç»ƒæ–¹å¼ï¼Œé¿å…äº†äººå·¥å¹²é¢„ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSLRçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šä»»åŠ¡æç¤ºåˆæˆæ¨¡å—ã€éªŒè¯ç¨‹åºç”Ÿæˆæ¨¡å—å’ŒçœŸå®è§„åˆ™æå–æ¨¡å—ã€‚ç”¨æˆ·è¾“å…¥ä»»åŠ¡è§„èŒƒåï¼Œç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆç›¸åº”çš„å†…å®¹ã€‚

**å…³é”®åˆ›æ–°**ï¼šSLRçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶å®Œå…¨è‡ªåŠ¨åŒ–çš„åˆæˆè¿‡ç¨‹ï¼Œèƒ½å¤Ÿåœ¨æ— äººå·¥æ³¨é‡Šçš„æƒ…å†µä¸‹ï¼Œç²¾ç¡®æ§åˆ¶ä»»åŠ¡çš„éš¾åº¦ï¼Œå¹¶ç”Ÿæˆå¤šæ ·åŒ–çš„æ¨ç†ä»»åŠ¡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒSLRé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ¨¡å‹è¾“å‡ºçš„å‡†ç¡®æ€§ï¼Œå¹¶é€šè¿‡è¯¾ç¨‹å­¦ä¹ ç­–ç•¥é€æ­¥å¢åŠ ä»»åŠ¡å¤æ‚åº¦ï¼Œä»¥æé«˜æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé€šè¿‡SLRæ¡†æ¶ï¼ŒLlama-3-8Båœ¨SLR-Benchä¸Šçš„å‡†ç¡®ç‡ç¿»å€ï¼Œè¾¾åˆ°äº†ä¸Gemini-Flash-Thinkingç›¸å½“çš„æ°´å¹³ï¼Œä¸”è®¡ç®—æˆæœ¬æ˜¾è‘—é™ä½ï¼Œå±•ç¤ºäº†SLRåœ¨é€»è¾‘æ¨ç†ä»»åŠ¡ä¸­çš„ä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€äººå·¥æ™ºèƒ½åŠ©æ‰‹å’Œè‡ªåŠ¨åŒ–å†³ç­–ç³»ç»Ÿã€‚SLRæ¡†æ¶èƒ½å¤Ÿä¸ºå„ç§é€»è¾‘æ¨ç†ä»»åŠ¡æä¾›é«˜æ•ˆçš„è®­ç»ƒå’Œè¯„ä¼°æ–¹æ³•ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é«˜æ•ˆæ¨ç†çš„åœºæ™¯ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We introduce SLR, an end-to-end framework for systematic evaluation and training of Large Language Models (LLMs) via Scalable Logical Reasoning. Given a user's task specification, SLR automatically synthesizes (i) an instruction prompt for an inductive reasoning task, (ii) a validation program, executable on model outputs to provide verifiable rewards, and (iii) the latent ground-truth rule. This process is fully automated, scalable, requires no human annotations, and offers precise control over task difficulty. Using SLR, we create SLR-Bench, a benchmark comprising 19k prompts organized into 20 curriculum levels that progressively increase in relational, arithmetic, and recursive complexity. Large-scale evaluation reveals that contemporary LLMs readily produce syntactically valid rules, yet often fail at correct logical inference. Recent reasoning LLMs demonstrate improved performance but incur very high test-time computation, with costs exceeding $300 for just 1,000 prompts. Finally, curriculum learning via SLR doubles Llama-3-8B accuracy on SLR-Bench, achieving parity with Gemini-Flash-Thinking at a fraction of computational cost. Moreover, these reasoning capabilities generalize to a wide range of established benchmarks, underscoring the effectiveness of SLR for downstream reasoning.

