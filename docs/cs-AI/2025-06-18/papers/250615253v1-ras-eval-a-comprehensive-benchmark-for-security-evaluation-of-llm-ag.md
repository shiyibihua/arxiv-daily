---
layout: default
title: RAS-Eval: A Comprehensive Benchmark for Security Evaluation of LLM Agents in Real-World Environments
---

# RAS-Eval: A Comprehensive Benchmark for Security Evaluation of LLM Agents in Real-World Environments

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.15253" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.15253v1</a>
  <a href="https://arxiv.org/pdf/2506.15253.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.15253v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.15253v1', 'RAS-Eval: A Comprehensive Benchmark for Security Evaluation of LLM Agents in Real-World Environments')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuchuan Fu, Xiaohan Yuan, Dongxia Wang

**åˆ†ç±»**: cs.CR, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-18

**å¤‡æ³¨**: 12 pages, 8 figures

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/lanzer-tree/RAS-Eval)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRAS-Evalä»¥è§£å†³LLMä»£ç†å®‰å…¨è¯„ä¼°é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å®‰å…¨è¯„ä¼°` `å¤§å‹è¯­è¨€æ¨¡å‹` `åŠ¨æ€ç¯å¢ƒ` `æ”»å‡»ä»»åŠ¡` `ä»»åŠ¡å®Œæˆç‡` `å¸¸è§å¼±ç‚¹æšä¸¾` `è¯„ä¼°åŸºå‡†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ç¼ºä¹é’ˆå¯¹LLMä»£ç†åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„æ ‡å‡†åŒ–å®‰å…¨è¯„ä¼°åŸºå‡†ï¼Œå¯¼è‡´å®‰å…¨æ¼æ´éš¾ä»¥è¯†åˆ«å’Œä¿®å¤ã€‚
2. æœ¬æ–‡æå‡ºRAS-Evalï¼Œé€šè¿‡æ„å»ºå…¨é¢çš„å®‰å…¨åŸºå‡†ï¼Œæ”¯æŒå¤šç§ç¯å¢ƒä¸‹çš„è¯„ä¼°ï¼Œå¡«è¡¥äº†è¿™ä¸€ç©ºç™½ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ”»å‡»æ˜¾è‘—é™ä½äº†ä»£ç†çš„ä»»åŠ¡å®Œæˆç‡ï¼Œä¸”è¾ƒå¤§æ¨¡å‹åœ¨å®‰å…¨èƒ½åŠ›ä¸Šè¡¨ç°ä¼˜äºå°æ¨¡å‹ï¼Œæ­ç¤ºäº†æ½œåœ¨é£é™©ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†åœ¨åŒ»ç–—å’Œé‡‘èç­‰å…³é”®é¢†åŸŸçš„å¿«é€Ÿéƒ¨ç½²ï¼Œå»ºç«‹å¥å…¨çš„å®‰å…¨æ¡†æ¶æ˜¾å¾—å°¤ä¸ºé‡è¦ã€‚ä¸ºäº†è§£å†³åŠ¨æ€ç¯å¢ƒä¸­ç¼ºä¹æ ‡å‡†åŒ–è¯„ä¼°åŸºå‡†çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†RAS-Evalï¼Œä¸€ä¸ªå…¨é¢çš„å®‰å…¨åŸºå‡†ï¼Œæ”¯æŒæ¨¡æ‹Ÿå’ŒçœŸå®ç¯å¢ƒä¸­çš„å·¥å…·æ‰§è¡Œã€‚RAS-EvalåŒ…å«80ä¸ªæµ‹è¯•ç”¨ä¾‹å’Œ3,802ä¸ªæ”»å‡»ä»»åŠ¡ï¼Œæ˜ å°„åˆ°11ä¸ªå¸¸è§å¼±ç‚¹æšä¸¾ï¼ˆCWEï¼‰ç±»åˆ«ï¼Œå·¥å…·å®ç°é‡‡ç”¨JSONã€LangGraphå’Œæ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼ˆMCPï¼‰æ ¼å¼ã€‚å¯¹6ç§æœ€å…ˆè¿›çš„LLMè¿›è¡Œè¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºæ”»å‡»å¹³å‡é™ä½äº†ä»£ç†ä»»åŠ¡å®Œæˆç‡ï¼ˆTCRï¼‰36.78%ï¼Œåœ¨å­¦æœ¯ç¯å¢ƒä¸­æˆåŠŸç‡è¾¾åˆ°85.65%ã€‚ç ”ç©¶æ­ç¤ºäº†ç°å®ä¸–ç•Œä»£ç†éƒ¨ç½²ä¸­çš„å…³é”®é£é™©ï¼Œå¹¶ä¸ºæœªæ¥çš„å®‰å…¨ç ”ç©¶æä¾›äº†åŸºç¡€æ¡†æ¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„å®‰å…¨è¯„ä¼°ç¼ºä¹æ ‡å‡†åŒ–åŸºå‡†çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆè¯†åˆ«å’Œè¯„ä¼°è¿™äº›ä»£ç†çš„å®‰å…¨æ¼æ´ï¼Œå¯¼è‡´åœ¨å®é™…åº”ç”¨ä¸­å­˜åœ¨è¾ƒå¤§é£é™©ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºRAS-Evalï¼Œæ„å»ºäº†ä¸€ä¸ªå…¨é¢çš„å®‰å…¨è¯„ä¼°åŸºå‡†ï¼ŒåŒ…å«å¤šç§æµ‹è¯•ç”¨ä¾‹å’Œæ”»å‡»ä»»åŠ¡ï¼Œæ—¨åœ¨ä¸ºLLMä»£ç†çš„å®‰å…¨æ€§æä¾›ç³»ç»ŸåŒ–çš„è¯„ä¼°æ¡†æ¶ã€‚é€šè¿‡æ”¯æŒæ¨¡æ‹Ÿå’ŒçœŸå®ç¯å¢ƒçš„å·¥å…·æ‰§è¡Œï¼Œå¢å¼ºäº†è¯„ä¼°çš„å®ç”¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRAS-Evalçš„æ•´ä½“æ¶æ„åŒ…æ‹¬80ä¸ªæµ‹è¯•ç”¨ä¾‹å’Œ3,802ä¸ªæ”»å‡»ä»»åŠ¡ï¼Œè¦†ç›–11ä¸ªCWEç±»åˆ«ã€‚å·¥å…·å®ç°é‡‡ç”¨JSONã€LangGraphå’ŒMCPæ ¼å¼ï¼Œè¯„ä¼°è¿‡ç¨‹åˆ†ä¸ºæµ‹è¯•ç”¨ä¾‹è®¾è®¡ã€æ”»å‡»ä»»åŠ¡æ‰§è¡Œå’Œç»“æœåˆ†æä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæ„å»ºäº†ä¸€ä¸ªå…¨é¢çš„å®‰å…¨åŸºå‡†ï¼Œæ”¯æŒå¤šç§ç¯å¢ƒä¸‹çš„è¯„ä¼°ï¼Œä¸”é¦–æ¬¡ç³»ç»Ÿæ€§åœ°æ­ç¤ºäº†LLMä»£ç†åœ¨å®é™…åº”ç”¨ä¸­çš„å®‰å…¨æ¼æ´åŠå…¶å½±å“ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒRAS-Evalæä¾›äº†æ›´ä¸ºç»†è‡´å’Œå…¨é¢çš„è¯„ä¼°æ¡†æ¶ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬æ”»å‡»ä»»åŠ¡çš„å¤šæ ·æ€§å’Œå¤æ‚æ€§ï¼ŒæŸå¤±å‡½æ•°ç”¨äºè¯„ä¼°ä»£ç†çš„ä»»åŠ¡å®Œæˆç‡ï¼ˆTCRï¼‰ï¼Œå¹¶é€šè¿‡å¯¹æ¯”ä¸åŒè§„æ¨¡æ¨¡å‹çš„è¡¨ç°ï¼Œæ­ç¤ºå®‰å…¨èƒ½åŠ›çš„è§„æ¨¡æ•ˆåº”ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ”»å‡»å¹³å‡é™ä½äº†ä»£ç†ä»»åŠ¡å®Œæˆç‡ï¼ˆTCRï¼‰36.78%ï¼Œåœ¨å­¦æœ¯ç¯å¢ƒä¸­æ”»å‡»æˆåŠŸç‡è¾¾åˆ°85.65%ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°è¾ƒå¤§æ¨¡å‹åœ¨å®‰å…¨èƒ½åŠ›ä¸Šæ˜¾è‘—ä¼˜äºå°æ¨¡å‹ï¼ŒéªŒè¯äº†è§„æ¨¡æ•ˆåº”çš„å­˜åœ¨ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒ»ç–—ã€é‡‘èç­‰å…³é”®è¡Œä¸šï¼Œèƒ½å¤Ÿä¸ºLLMä»£ç†çš„å®‰å…¨æ€§æä¾›ç³»ç»ŸåŒ–çš„è¯„ä¼°å’Œæ”¹è¿›æ–¹æ¡ˆã€‚é€šè¿‡è¯†åˆ«å’Œä¿®å¤å®‰å…¨æ¼æ´ï¼Œæå‡ä»£ç†åœ¨å®é™…åº”ç”¨ä¸­çš„å¯é æ€§å’Œå®‰å…¨æ€§ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The rapid deployment of Large language model (LLM) agents in critical domains like healthcare and finance necessitates robust security frameworks. To address the absence of standardized evaluation benchmarks for these agents in dynamic environments, we introduce RAS-Eval, a comprehensive security benchmark supporting both simulated and real-world tool execution. RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats. We evaluate 6 state-of-the-art LLMs across diverse scenarios, revealing significant vulnerabilities: attacks reduced agent task completion rates (TCR) by 36.78% on average and achieved an 85.65% success rate in academic settings. Notably, scaling laws held for security capabilities, with larger models outperforming smaller counterparts. Our findings expose critical risks in real-world agent deployments and provide a foundational framework for future security research. Code and data are available at https://github.com/lanzer-tree/RAS-Eval.

