---
layout: default
title: Multi-Agent Reinforcement Learning for Autonomous Multi-Satellite Earth Observation: A Realistic Case Study
---

# Multi-Agent Reinforcement Learning for Autonomous Multi-Satellite Earth Observation: A Realistic Case Study

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.15207" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.15207v2</a>
  <a href="https://arxiv.org/pdf/2506.15207.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.15207v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.15207v2', 'Multi-Agent Reinforcement Learning for Autonomous Multi-Satellite Earth Observation: A Realistic Case Study')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mohamad A. Hady, Siyi Hu, Mahardhika Pratama, Jimmy Cao, Ryszard Kowalczyk

**åˆ†ç±»**: cs.AI, cs.MA, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-06-18 (æ›´æ–°: 2025-11-05)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ä»¥è§£å†³è‡ªä¸»å¤šå«æ˜Ÿåœ°çƒè§‚æµ‹é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ` `åœ°çƒè§‚æµ‹` `ä½åœ°çƒè½¨é“å«æ˜Ÿ` `è‡ªä¸»åè°ƒ` `èµ„æºç®¡ç†` `å®æ—¶å†³ç­–` `æ°”å€™ç›‘æµ‹` `ç¾å®³ç®¡ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤šå«æ˜Ÿç³»ç»Ÿä¸­é¢ä¸´å®æ—¶å†³ç­–å’Œè‡ªä¸»åè°ƒçš„æŒ‘æˆ˜ï¼Œä¼ ç»Ÿä¼˜åŒ–æ–¹æ³•éš¾ä»¥é€‚åº”åŠ¨æ€å˜åŒ–çš„ç¯å¢ƒã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰æ¡†æ¶æ¥å®ç°è‡ªä¸»çš„åœ°çƒè§‚æµ‹ä»»åŠ¡è§„åˆ’ï¼Œè§£å†³èƒ½é‡å’Œæ•°æ®å­˜å‚¨é™åˆ¶ç­‰é—®é¢˜ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMARLç®—æ³•åœ¨æˆåƒä¸èµ„æºç®¡ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåº”å¯¹å¤šå«æ˜Ÿåè°ƒä¸­çš„å¤æ‚æ€§å’Œä¸ç¡®å®šæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä½åœ°çƒè½¨é“ï¼ˆLEOï¼‰å«æ˜Ÿçš„å¿«é€Ÿå¢é•¿å½»åº•æ”¹å˜äº†åœ°çƒè§‚æµ‹ï¼ˆEOï¼‰ä»»åŠ¡ï¼Œè§£å†³äº†æ°”å€™ç›‘æµ‹å’Œç¾å®³ç®¡ç†ç­‰æŒ‘æˆ˜ã€‚ç„¶è€Œï¼Œå¤šå«æ˜Ÿç³»ç»Ÿä¸­çš„è‡ªä¸»åè°ƒä»ç„¶æ˜¯ä¸€ä¸ªåŸºæœ¬æŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„ä¼˜åŒ–æ–¹æ³•éš¾ä»¥æ»¡è¶³åŠ¨æ€EOä»»åŠ¡çš„å®æ—¶å†³ç­–éœ€æ±‚ï¼Œå› æ­¤éœ€è¦ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰ã€‚æœ¬æ–‡é€šè¿‡å»ºæ¨¡å•å«æ˜Ÿæ“ä½œå¹¶æ‰©å±•åˆ°å¤šå«æ˜Ÿæ˜Ÿåº§ï¼Œç ”ç©¶åŸºäºRLçš„è‡ªä¸»EOä»»åŠ¡è§„åˆ’ã€‚æˆ‘ä»¬è§£å†³äº†èƒ½é‡å’Œæ•°æ®å­˜å‚¨é™åˆ¶ã€å«æ˜Ÿè§‚æµ‹çš„ä¸ç¡®å®šæ€§ä»¥åŠåœ¨éƒ¨åˆ†å¯è§‚æµ‹æ¡ä»¶ä¸‹çš„å»ä¸­å¿ƒåŒ–åè°ƒå¤æ‚æ€§ç­‰å…³é”®æŒ‘æˆ˜ã€‚é€šè¿‡åˆ©ç”¨è¿‘ä¹çœŸå®çš„å«æ˜Ÿä»¿çœŸç¯å¢ƒï¼Œæˆ‘ä»¬è¯„ä¼°äº†æœ€å…ˆè¿›çš„MARLç®—æ³•ï¼ˆåŒ…æ‹¬PPOã€IPPOã€MAPPOå’ŒHAPPOï¼‰çš„è®­ç»ƒç¨³å®šæ€§å’Œæ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼ŒMARLèƒ½å¤Ÿæœ‰æ•ˆå¹³è¡¡æˆåƒå’Œèµ„æºç®¡ç†ï¼ŒåŒæ—¶åº”å¯¹å¤šå«æ˜Ÿåè°ƒä¸­çš„éå¹³ç¨³æ€§å’Œå¥–åŠ±ç›¸äº’ä¾èµ–æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šå«æ˜Ÿç³»ç»Ÿä¸­çš„è‡ªä¸»åè°ƒé—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨åŠ¨æ€åœ°çƒè§‚æµ‹ä»»åŠ¡ä¸­éš¾ä»¥è¿›è¡Œå®æ—¶å†³ç­–ï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰æ¡†æ¶ï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨åŠ¨æ€ç¯å¢ƒä¸­è‡ªä¸»è§„åˆ’ä»»åŠ¡ï¼Œä¼˜åŒ–èµ„æºä½¿ç”¨å’Œæˆåƒè°ƒåº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å•å«æ˜Ÿæ“ä½œæ¨¡å‹å’Œå¤šå«æ˜Ÿæ˜Ÿåº§çš„æ‰©å±•ï¼Œåˆ©ç”¨è¿‘ä¹çœŸå®çš„å«æ˜Ÿä»¿çœŸç¯å¢ƒè¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ï¼Œä¸»è¦æ¨¡å—åŒ…æ‹¬çŠ¶æ€è¡¨ç¤ºã€åŠ¨ä½œé€‰æ‹©å’Œå¥–åŠ±æœºåˆ¶ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„ä¸»è¦åˆ›æ–°åœ¨äºå°†MARLåº”ç”¨äºå¤šå«æ˜Ÿåè°ƒï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†éå¹³ç¨³æ€§å’Œå¥–åŠ±ç›¸äº’ä¾èµ–æ€§ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æå‡äº†è‡ªä¸»å†³ç­–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç®—æ³•è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†PPOã€IPPOã€MAPPOå’ŒHAPPOç­‰å…ˆè¿›MARLç®—æ³•ï¼Œè®¾ç½®äº†é€‚åº”æ€§çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ï¼Œä»¥ç¡®ä¿è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨MARLç®—æ³•çš„å¤šå«æ˜Ÿç³»ç»Ÿåœ¨æˆåƒå’Œèµ„æºç®¡ç†æ–¹é¢çš„è¡¨ç°ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°20%-30%ã€‚æ­¤å¤–ï¼Œç®—æ³•åœ¨å¤„ç†éå¹³ç¨³æ€§å’Œå¥–åŠ±ç›¸äº’ä¾èµ–æ€§æ–¹é¢å±•ç°å‡ºè‰¯å¥½çš„ç¨³å®šæ€§å’Œé€‚åº”æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ°”å€™ç›‘æµ‹ã€ç¾å®³ç®¡ç†å’Œç¯å¢ƒä¿æŠ¤ç­‰ï¼Œèƒ½å¤Ÿä¸ºè‡ªä¸»å«æ˜Ÿæ“ä½œæä¾›å®ç”¨æŒ‡å¯¼ï¼Œæå‡å¤šå«æ˜Ÿç³»ç»Ÿçš„æ•ˆç‡å’Œå“åº”èƒ½åŠ›ã€‚æœªæ¥ï¼Œéšç€å«æ˜ŸæŠ€æœ¯çš„å‘å±•ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åœ¨æ›´å¹¿æ³›çš„é¢†åŸŸä¸­å¾—åˆ°åº”ç”¨ï¼Œæ¨åŠ¨æ™ºèƒ½åŒ–åœ°çƒè§‚æµ‹çš„è¿›ç¨‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The exponential growth of Low Earth Orbit (LEO) satellites has revolutionised Earth Observation (EO) missions, addressing challenges in climate monitoring, disaster management, and more. However, autonomous coordination in multi-satellite systems remains a fundamental challenge. Traditional optimisation approaches struggle to handle the real-time decision-making demands of dynamic EO missions, necessitating the use of Reinforcement Learning (RL) and Multi-Agent Reinforcement Learning (MARL). In this paper, we investigate RL-based autonomous EO mission planning by modelling single-satellite operations and extending to multi-satellite constellations using MARL frameworks. We address key challenges, including energy and data storage limitations, uncertainties in satellite observations, and the complexities of decentralised coordination under partial observability. By leveraging a near-realistic satellite simulation environment, we evaluate the training stability and performance of state-of-the-art MARL algorithms, including PPO, IPPO, MAPPO, and HAPPO. Our results demonstrate that MARL can effectively balance imaging and resource management while addressing non-stationarity and reward interdependency in multi-satellite coordination. The insights gained from this study provide a foundation for autonomous satellite operations, offering practical guidelines for improving policy learning in decentralised EO missions.

