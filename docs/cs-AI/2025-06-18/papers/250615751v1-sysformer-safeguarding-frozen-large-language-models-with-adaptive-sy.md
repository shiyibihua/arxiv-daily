---
layout: default
title: Sysformer: Safeguarding Frozen Large Language Models with Adaptive System Prompts
---

# Sysformer: Safeguarding Frozen Large Language Models with Adaptive System Prompts

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.15751" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.15751v1</a>
  <a href="https://arxiv.org/pdf/2506.15751.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.15751v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.15751v1', 'Sysformer: Safeguarding Frozen Large Language Models with Adaptive System Prompts')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kartik Sharma, Yiqiao Jin, Vineeth Rakesh, Yingtong Dou, Menghai Pan, Mahashweta Das, Srijan Kumar

**åˆ†ç±»**: cs.AI, cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSysformerä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹å®‰å…¨æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `å®‰å…¨æ€§` `ç³»ç»Ÿæç¤º` `é²æ£’æ€§` `é€‚åº”æ€§å­¦ä¹ ` `æ¨¡å‹è®­ç»ƒ` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨ç¡®ä¿å¤§è¯­è¨€æ¨¡å‹å®‰å…¨æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå¸¸ä¾èµ–æ˜‚è´µçš„å¾®è°ƒæˆ–æ¬¡ä¼˜çš„å¯å‘å¼æŠ€æœ¯ã€‚
2. æœ¬æ–‡æå‡ºSysformerï¼Œé€šè¿‡åœ¨ä¿æŒæ¨¡å‹å‚æ•°ä¸å˜çš„æƒ…å†µä¸‹ï¼Œé€‚åº”æ€§åœ°è°ƒæ•´ç³»ç»Ÿæç¤ºä»¥æé«˜å®‰å…¨æ€§ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒSysformeråœ¨æ‹’ç»æœ‰å®³æç¤ºçš„èƒ½åŠ›ä¸Šæå‡äº†80%ï¼Œåœ¨éµå¾ªå®‰å…¨æç¤ºçš„åˆè§„æ€§ä¸Šæå‡äº†90%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å®‰å…¨å…³é”®ç¯å¢ƒä¸­çš„åº”ç”¨ï¼Œç¡®ä¿å…¶å“åº”ç¬¦åˆå®‰å…¨æ ‡å‡†å˜å¾—è‡³å…³é‡è¦ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒLLMså¸¸å¸¸æ— æ³•ç†è§£å®‰å…¨è¡Œä¸ºï¼Œå¯¼è‡´å¯¹æ— å®³æç¤ºçš„æ— ç«¯æ‹’ç»æˆ–ç”Ÿæˆæœ‰å®³å†…å®¹ã€‚ç°æœ‰é˜²å¾¡æ–¹æ³•é€šå¸¸ä¾èµ–äºæ˜‚è´µçš„æ¨¡å‹å‚æ•°å¾®è°ƒæˆ–ä½¿ç”¨æ¬¡ä¼˜çš„å¯å‘å¼æŠ€æœ¯ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡å­¦ä¹ é€‚åº”ç³»ç»Ÿæç¤ºæ¥ä¿æŠ¤LLMsã€‚æˆ‘ä»¬æå‡ºçš„Sysformeræ¨¡å‹åœ¨ä¿æŒLLMå‚æ•°ä¸å˜çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿæ ¹æ®ç”¨æˆ·è¾“å…¥è°ƒæ•´ç³»ç»Ÿæç¤ºï¼Œä»è€Œæ˜¾è‘—æé«˜LLMsçš„é²æ£’æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSysformeråœ¨æ‹’ç»æœ‰å®³æç¤ºçš„èƒ½åŠ›ä¸Šæå‡äº†80%ï¼Œåœ¨éµå¾ªå®‰å…¨æç¤ºçš„åˆè§„æ€§ä¸Šæå‡äº†90%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨å®‰å…¨æ€§æ–¹é¢çš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–æ˜‚è´µçš„å¾®è°ƒæˆ–å¯å‘å¼æŠ€æœ¯ï¼Œéš¾ä»¥æœ‰æ•ˆåº”å¯¹æœ‰å®³æç¤ºã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSysformeré€šè¿‡åœ¨ä¿æŒLLMå‚æ•°ä¸å˜çš„æƒ…å†µä¸‹ï¼Œå­¦ä¹ é€‚åº”æ€§åœ°è°ƒæ•´ç³»ç»Ÿæç¤ºï¼Œä»¥æé«˜æ¨¡å‹å¯¹ç”¨æˆ·è¾“å…¥çš„å“åº”å®‰å…¨æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSysformeræ¨¡å‹çš„æ•´ä½“æ¶æ„åŒ…æ‹¬è¾“å…¥åµŒå…¥ç©ºé—´ä¸­çš„åˆå§‹ç³»ç»Ÿæç¤ºå’Œç»è¿‡è°ƒæ•´çš„ç³»ç»Ÿæç¤ºï¼Œæ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å…³æ³¨ç”¨æˆ·æç¤ºï¼Œç¡®ä¿å¯¹æœ‰å®³æç¤ºçš„æ‹’ç»å’Œå¯¹å®‰å…¨æç¤ºçš„å“åº”ã€‚

**å…³é”®åˆ›æ–°**ï¼šSysformerçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶èƒ½å¤Ÿåœ¨ä¸æ”¹å˜LLMå‚æ•°çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´ç³»ç»Ÿæç¤ºæ¥å¢å¼ºæ¨¡å‹çš„å®‰å…¨æ€§ï¼Œè¿™ä¸ä¼ ç»Ÿçš„å¾®è°ƒæ–¹æ³•æœ¬è´¨ä¸Šä¸åŒã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼ŒSysformerä½¿ç”¨ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–å¯¹æœ‰å®³æç¤ºçš„æ‹’ç»ç‡ï¼ŒåŒæ—¶ç¡®ä¿å¯¹å®‰å…¨æç¤ºçš„é«˜åˆè§„æ€§ï¼Œè®¾è®¡äº†æœ‰æ•ˆçš„ç½‘ç»œç»“æ„ä»¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSysformeråœ¨æ‹’ç»æœ‰å®³æç¤ºçš„èƒ½åŠ›ä¸Šæå‡äº†80%ï¼Œåœ¨éµå¾ªå®‰å…¨æç¤ºçš„åˆè§„æ€§ä¸Šæå‡äº†90%ã€‚æ­¤å¤–ï¼ŒSysformerå¯¹å¤æ‚çš„è¶Šç‹±æ”»å‡»è¡¨ç°å‡ºè‰¯å¥½çš„é²æ£’æ€§ï¼Œä½¿å¾—æ¨¡å‹åœ¨ä¸åŒæ”»å‡»ç­–ç•¥ä¸‹çš„å®‰å…¨æ€§æå‡è¾¾åˆ°100%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒ»ç–—ã€é‡‘èå’Œè‡ªåŠ¨é©¾é©¶ç­‰å®‰å…¨å…³é”®åœºæ™¯ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡å¤§è¯­è¨€æ¨¡å‹åœ¨è¿™äº›é¢†åŸŸçš„å®‰å…¨æ€§å’Œå¯é æ€§ã€‚æœªæ¥ï¼ŒSysformerçš„è®¾è®¡ç†å¿µå¯èƒ½æ¿€åŠ±æ›´å¤šå…³äºå¯å˜ç³»ç»Ÿæç¤ºçš„ç ”ç©¶ï¼Œæ¨åŠ¨å¤§è¯­è¨€æ¨¡å‹çš„å®‰å…¨åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As large language models (LLMs) are deployed in safety-critical settings, it is essential to ensure that their responses comply with safety standards. Prior research has revealed that LLMs often fail to grasp the notion of safe behaviors, resulting in either unjustified refusals to harmless prompts or the generation of harmful content. While substantial efforts have been made to improve their robustness, existing defenses often rely on costly fine-tuning of model parameters or employ suboptimal heuristic techniques. In this work, we take a novel approach to safeguard LLMs by learning to adapt the system prompts in instruction-tuned LLMs. While LLMs are typically pre-trained to follow a fixed system prompt, we investigate the impact of tailoring the system prompt to each specific user input on the safety of the responses. To this end, we propose $\textbf{Sysformer}$, a trans$\textbf{former}$ model that updates an initial $\textbf{sys}$tem prompt to a more robust system prompt in the LLM input embedding space while attending to the user prompt. While keeping the LLM parameters frozen, the Sysformer is trained to refuse to respond to a set of harmful prompts while responding ideally to a set of safe ones. Through extensive experiments on $5$ LLMs from different families and $2$ recent benchmarks, we demonstrate that Sysformer can significantly enhance the robustness of LLMs, leading to upto $80\%$ gain in the refusal rate on harmful prompts while enhancing the compliance with the safe prompts by upto $90\%$. Results also generalize well to sophisticated jailbreaking attacks, making LLMs upto $100\%$ more robust against different attack strategies. We hope our findings lead to cheaper safeguarding of LLMs and motivate future investigations into designing variable system prompts.

