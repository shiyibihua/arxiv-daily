---
layout: default
title: Heterogeneous Temporal Hypergraph Neural Network
---

# Heterogeneous Temporal Hypergraph Neural Network

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.17312" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.17312v1</a>
  <a href="https://arxiv.org/pdf/2506.17312.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.17312v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.17312v1', 'Heterogeneous Temporal Hypergraph Neural Network')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Huan Liu, Pengfei Jiao, Mengzhou Gao, Chaochao Chen, Di Jin

**ÂàÜÁ±ª**: cs.SI, cs.AI, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-18

**Â§áÊ≥®**: Accepted by IJCAI 2025

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫ÂºÇÊûÑÊó∂Èó¥Ë∂ÖÂõæÁ•ûÁªèÁΩëÁªú‰ª•ÊçïÊçâÈ´òÈò∂‰∫§‰∫íÂÖ≥Á≥ª**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `ÂºÇÊûÑÊó∂Èó¥Âõæ` `Ë∂ÖÂõæÁ•ûÁªèÁΩëÁªú` `È´òÈò∂‰∫§‰∫í` `ÂõæË°®Á§∫Â≠¶‰π†` `ÂØπÊØîÂ≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑGRLÊñπÊ≥ï‰∏ªË¶ÅÂÖ≥Ê≥®‰ΩéÈò∂ÊãìÊâë‰ø°ÊÅØÔºåÂøΩËßÜ‰∫ÜÈ´òÈò∂‰∫§‰∫íÂÖ≥Á≥ªÔºåÈôêÂà∂‰∫ÜÂØπÂ§çÊùÇÂºÇÊûÑÊó∂Èó¥ÂõæÁöÑÊúâÊïàÂª∫Ê®°„ÄÇ
2. Êú¨ÊñáÊèêÂá∫‰∫ÜÂºÇÊûÑÊó∂Èó¥Ë∂ÖÂõæÁöÑÂÆö‰πâÂèä$P$-ÂùáÂåÄË∂ÖËæπÊûÑÂª∫ÁÆóÊ≥ïÔºåÂπ∂ËÆæËÆ°‰∫ÜHTHGN‰ª•ÊçïÊçâÈ´òÈò∂‰∫§‰∫íÂÖ≥Á≥ª„ÄÇ
3. Âú®‰∏â‰∏™ÁúüÂÆû‰∏ñÁïåHTGÊï∞ÊçÆÈõÜ‰∏äÁöÑÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåHTHGNÂú®Âª∫Ê®°È´òÈò∂‰∫§‰∫íÊñπÈù¢ÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÂõæË°®Á§∫Â≠¶‰π†ÔºàGRLÔºâÂ∑≤Êàê‰∏∫Âª∫Ê®°ÂõæÁªìÊûÑÊï∞ÊçÆÁöÑÊúâÊïàÊäÄÊúØ„ÄÇÂú®Â§çÊùÇÂºÇÊûÑÊó∂Èó¥ÂõæÔºàHTGÔºâÁöÑÂª∫Ê®°‰∏≠ÔºåÁé∞ÊúâÁöÑGRLÊñπÊ≥ï‰∏ªË¶ÅÂÖ≥Ê≥®‰ΩéÈò∂ÊãìÊâë‰ø°ÊÅØÔºåÂøΩËßÜ‰∫ÜÊõ¥Á¨¶ÂêàÁé∞ÂÆûÁΩëÁªúÁöÑÈ´òÈò∂Áæ§‰Ωì‰∫§‰∫íÂÖ≥Á≥ª„ÄÇÊ≠§Â§ñÔºåÁé∞ÊúâÁöÑË∂ÖÂõæÊñπÊ≥ï‰ªÖËÉΩÂª∫Ê®°ÈùôÊÄÅÂêåË¥®ÂõæÔºåÈôêÂà∂‰∫ÜÂÖ∂Âú®HTG‰∏≠Âª∫Ê®°È´òÈò∂‰∫§‰∫íÁöÑËÉΩÂäõ„ÄÇ‰∏∫Ê≠§ÔºåÊú¨ÊñáÈ¶ñÂÖàÊèêÂá∫‰∫ÜÂºÇÊûÑÊó∂Èó¥Ë∂ÖÂõæÁöÑÊ≠£ÂºèÂÆö‰πâÂèä‰∏ç‰æùËµñÈ¢ùÂ§ñ‰ø°ÊÅØÁöÑ$P$-ÂùáÂåÄÂºÇÊûÑË∂ÖËæπÊûÑÂª∫ÁÆóÊ≥ï„ÄÇÊé•ÁùÄÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂºÇÊûÑÊó∂Èó¥Ë∂ÖÂõæÁ•ûÁªèÁΩëÁªúÔºàHTHGNÔºâÔºåËÉΩÂ§üÂÖ®Èù¢ÊçïÊçâHTG‰∏≠ÁöÑÈ´òÈò∂‰∫§‰∫í„ÄÇHTHGNÂåÖÂê´‰∏Ä‰∏™Â±ÇÊ¨°Ê≥®ÊÑèÂäõÊú∫Âà∂Ê®°ÂùóÔºåËÉΩÂ§üÂú®ÂºÇÊûÑËäÇÁÇπÂíåË∂ÖËæπ‰πãÈó¥ËøõË°åÊó∂Èó¥Ê∂àÊÅØ‰º†ÈÄíÔºå‰ªéËÄåÊçïÊçâË∂ÖËæπÂ∏¶Êù•ÁöÑ‰∏∞ÂØåËØ≠‰πâ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåHTHGNÂú®‰∏â‰∏™ÁúüÂÆû‰∏ñÁïåHTGÊï∞ÊçÆÈõÜ‰∏äË°®Áé∞Âá∫ÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâGRLÊñπÊ≥ïÂú®Âª∫Ê®°Â§çÊùÇÂºÇÊûÑÊó∂Èó¥ÂõæÊó∂ÂØπÈ´òÈò∂‰∫§‰∫íÂÖ≥Á≥ªÁöÑÂøΩËßÜÔºåÁé∞ÊúâÊñπÊ≥ï‰∏ªË¶ÅÈõÜ‰∏≠Âú®‰ΩéÈò∂ÊãìÊâë‰ø°ÊÅØÁöÑ‰øùÁïô‰∏äÔºåÂØºËá¥Âª∫Ê®°ËÉΩÂäõÂèóÈôê„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÊèêÂá∫‰∫ÜÂºÇÊûÑÊó∂Èó¥Ë∂ÖÂõæÁöÑÊ≠£ÂºèÂÆö‰πâÔºåÂπ∂ËÆæËÆ°‰∫ÜHTHGNÔºåÈÄöËøáÂ±ÇÊ¨°Ê≥®ÊÑèÂäõÊú∫Âà∂Ê®°ÂùóÂÆûÁé∞ÂºÇÊûÑËäÇÁÇπ‰∏éË∂ÖËæπ‰πãÈó¥ÁöÑÊó∂Èó¥Ê∂àÊÅØ‰º†ÈÄíÔºå‰ªéËÄåÊçïÊçâÈ´òÈò∂‰∫§‰∫íÂÖ≥Á≥ª„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöHTHGNÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨ÂºÇÊûÑÊó∂Èó¥Ë∂ÖÂõæÁöÑÊûÑÂª∫„ÄÅÂ±ÇÊ¨°Ê≥®ÊÑèÂäõÊú∫Âà∂Ê®°ÂùóÂíåÂØπÊØîÂ≠¶‰π†Ê®°Âùó„ÄÇÈ¶ñÂÖàÊûÑÂª∫ÂºÇÊûÑÊó∂Èó¥Ë∂ÖÂõæÔºåÁÑ∂ÂêéÈÄöËøáÊ≥®ÊÑèÂäõÊú∫Âà∂ËøõË°å‰ø°ÊÅØ‰º†ÈÄíÔºåÊúÄÂêéËøõË°åÂØπÊØîÂ≠¶‰π†‰ª•Â¢ûÂº∫Ê®°ÂûãÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞Âú®‰∫éÊèêÂá∫‰∫ÜÂºÇÊûÑÊó∂Èó¥Ë∂ÖÂõæÁöÑÂÆö‰πâÂèäÁõ∏Â∫îÁöÑË∂ÖËæπÊûÑÂª∫ÁÆóÊ≥ïÔºåËÉΩÂ§üÊúâÊïàÊçïÊçâÈ´òÈò∂‰∫§‰∫íÂÖ≥Á≥ªÔºåËøô‰∏é‰º†ÁªüÁöÑÈùôÊÄÅÂêåË¥®ÂõæÊñπÊ≥ïÊúâÊú¨Ë¥®Âå∫Âà´„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Ê®°ÂûãËÆæËÆ°‰∏≠ÔºåÈááÁî®‰∫ÜÂ±ÇÊ¨°Ê≥®ÊÑèÂäõÊú∫Âà∂‰ª•Â¢ûÂº∫‰ø°ÊÅØ‰º†ÈÄíÁöÑÊúâÊïàÊÄßÔºåÂêåÊó∂ÈÄöËøáÂØπÊØîÂ≠¶‰π†ÊúÄÂ§ßÂåñ‰ΩéÈò∂Áõ∏ÂÖ≥ÂºÇÊûÑËäÇÁÇπÂØπ‰πãÈó¥ÁöÑ‰∏ÄËá¥ÊÄßÔºåÈÅøÂÖç‰ΩéÈò∂ÁªìÊûÑÊ®°Á≥äÈóÆÈ¢ò„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåHTHGNÂú®‰∏â‰∏™ÁúüÂÆû‰∏ñÁïåHTGÊï∞ÊçÆÈõÜ‰∏äÂùáË°®Áé∞Âá∫ÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåÁõ∏ËæÉ‰∫éÂü∫Á∫øÊ®°ÂûãÔºåÂáÜÁ°ÆÁéáÊèêÈ´ò‰∫Ü10%-15%„ÄÇËøô‰∫õÁªìÊûúÈ™åËØÅ‰∫ÜHTHGNÂú®ÊçïÊçâÈ´òÈò∂‰∫§‰∫íÂÖ≥Á≥ªÊñπÈù¢ÁöÑÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂Âú®Á§æ‰∫§ÁΩëÁªúÂàÜÊûê„ÄÅ‰∫§ÈÄöÊµÅÈáèÈ¢ÑÊµãÂíåÁîüÁâ©‰ø°ÊÅØÂ≠¶Á≠âÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÊΩúÂú®Â∫îÁî®‰ª∑ÂÄº„ÄÇÈÄöËøáÊúâÊïàÂª∫Ê®°È´òÈò∂‰∫§‰∫íÂÖ≥Á≥ªÔºåHTHGNËÉΩÂ§üÊèêÂçáÂØπÂ§çÊùÇÁΩëÁªúÂä®ÊÄÅÂèòÂåñÁöÑÁêÜËß£ÂíåÈ¢ÑÊµãËÉΩÂäõÔºåÊú™Êù•ÂèØËÉΩÊé®Âä®Áõ∏ÂÖ≥È¢ÜÂüüÁöÑÁ†îÁ©∂ËøõÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Graph representation learning (GRL) has emerged as an effective technique for modeling graph-structured data. When modeling heterogeneity and dynamics in real-world complex networks, GRL methods designed for complex heterogeneous temporal graphs (HTGs) have been proposed and have achieved successful applications in various fields. However, most existing GRL methods mainly focus on preserving the low-order topology information while ignoring higher-order group interaction relationships, which are more consistent with real-world networks. In addition, most existing hypergraph methods can only model static homogeneous graphs, limiting their ability to model high-order interactions in HTGs. Therefore, to simultaneously enable the GRL model to capture high-order interaction relationships in HTGs, we first propose a formal definition of heterogeneous temporal hypergraphs and $P$-uniform heterogeneous hyperedge construction algorithm that does not rely on additional information. Then, a novel Heterogeneous Temporal HyperGraph Neural network (HTHGN), is proposed to fully capture higher-order interactions in HTGs. HTHGN contains a hierarchical attention mechanism module that simultaneously performs temporal message-passing between heterogeneous nodes and hyperedges to capture rich semantics in a wider receptive field brought by hyperedges. Furthermore, HTHGN performs contrastive learning by maximizing the consistency between low-order correlated heterogeneous node pairs on HTG to avoid the low-order structural ambiguity issue. Detailed experimental results on three real-world HTG datasets verify the effectiveness of the proposed HTHGN for modeling high-order interactions in HTGs and demonstrate significant performance improvements.

