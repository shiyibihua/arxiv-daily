---
layout: default
title: Deep Reinforcement Learning Xiangqi Player with Monte Carlo Tree Search
---

# Deep Reinforcement Learning Xiangqi Player with Monte Carlo Tree Search

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.15880" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.15880v1</a>
  <a href="https://arxiv.org/pdf/2506.15880.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.15880v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.15880v1', 'Deep Reinforcement Learning Xiangqi Player with Monte Carlo Tree Search')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Berk Yilmaz, Junyu Hu, Jinsong Liu

**åˆ†ç±»**: cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-18

**å¤‡æ³¨**: All authors contributed equally to this work.24 pages, 10 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸è’™ç‰¹å¡æ´›æ ‘æœç´¢ç»“åˆçš„è±¡æ£‹ç©å®¶ç³»ç»Ÿ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ·±åº¦å¼ºåŒ–å­¦ä¹ ` `è’™ç‰¹å¡æ´›æ ‘æœç´¢` `è±¡æ£‹` `ç­–ç•¥æ¸¸æˆ` `äººå·¥æ™ºèƒ½` `è‡ªæˆ‘å¯¹å¼ˆ` `å†³ç­–ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è±¡æ£‹çš„å¤æ‚æ€§å’Œé«˜åˆ†æ”¯å› å­ä½¿å¾—ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•éš¾ä»¥æœ‰æ•ˆåº”ç”¨ï¼Œå¯¼è‡´å†³ç­–æ•ˆç‡ä½ä¸‹ã€‚
2. æœ¬ç ”ç©¶æå‡ºå°†æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸è’™ç‰¹å¡æ´›æ ‘æœç´¢ç›¸ç»“åˆï¼Œåˆ©ç”¨ç­–ç•¥-ä»·å€¼ç½‘ç»œä¼˜åŒ–å†³ç­–è¿‡ç¨‹ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨è±¡æ£‹è‡ªæˆ‘å¯¹å¼ˆä¸­æ˜¾è‘—æå‡äº†èƒœç‡ï¼Œå±•ç¤ºäº†è¾ƒå¼ºçš„ç­–ç•¥å­¦ä¹ èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰ç³»ç»Ÿï¼Œç”¨äºè±¡æ£‹ï¼ˆä¸­å›½è±¡æ£‹ï¼‰ï¼Œè¯¥ç³»ç»Ÿå°†ç¥ç»ç½‘ç»œä¸è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ç›¸ç»“åˆï¼Œä»¥å®ç°æˆ˜ç•¥è‡ªæˆ‘å¯¹å¼ˆå’Œè‡ªæˆ‘æå‡ã€‚é’ˆå¯¹è±¡æ£‹çš„å¤æ‚æ€§ï¼ŒåŒ…æ‹¬ç‹¬ç‰¹çš„æ£‹ç›˜å¸ƒå±€ã€æ£‹å­ç§»åŠ¨çº¦æŸå’Œèƒœåˆ©æ¡ä»¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†ç­–ç•¥-ä»·å€¼ç½‘ç»œä¸MCTSï¼Œä»¥æ¨¡æ‹Ÿèµ°æ£‹åæœå¹¶ä¼˜åŒ–å†³ç­–ã€‚é€šè¿‡å…‹æœè±¡æ£‹çš„é«˜åˆ†æ”¯å› å­å’Œä¸å¯¹ç§°æ£‹å­åŠ¨æ€ç­‰æŒ‘æˆ˜ï¼Œæˆ‘ä»¬çš„å·¥ä½œæ¨åŠ¨äº†äººå·¥æ™ºèƒ½åœ¨æ–‡åŒ–é‡è¦çš„ç­–ç•¥æ¸¸æˆä¸­çš„èƒ½åŠ›ï¼ŒåŒæ—¶ä¸ºå°†DRL-MCTSæ¡†æ¶é€‚åº”äºç‰¹å®šé¢†åŸŸè§„åˆ™ç³»ç»Ÿæä¾›äº†è§è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³è±¡æ£‹è¿™ä¸€å¤æ‚ç­–ç•¥æ¸¸æˆä¸­ï¼Œç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨é«˜åˆ†æ”¯å› å­å’Œä¸å¯¹ç§°æ£‹å­åŠ¨æ€ä¸‹çš„å†³ç­–æ•ˆç‡ä½ä¸‹é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å°†æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸è’™ç‰¹å¡æ´›æ ‘æœç´¢ç›¸ç»“åˆï¼Œåˆ©ç”¨ç¥ç»ç½‘ç»œçš„å¼ºå¤§è¡¨è¾¾èƒ½åŠ›å’ŒMCTSçš„é«˜æ•ˆæœç´¢èƒ½åŠ›ï¼Œä¼˜åŒ–è±¡æ£‹çš„å†³ç­–è¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ç­–ç•¥-ä»·å€¼ç½‘ç»œå’ŒMCTSæ¨¡å—ï¼Œç­–ç•¥ç½‘ç»œç”¨äºè¯„ä¼°æ¯ä¸€æ­¥çš„æœ€ä½³ç­–ç•¥ï¼Œä»·å€¼ç½‘ç»œåˆ™ç”¨äºè¯„ä¼°å±€é¢ä¼˜åŠ£ï¼ŒMCTSè´Ÿè´£æ¨¡æ‹Ÿèµ°æ£‹åæœå¹¶è¿›è¡Œå†³ç­–ä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„ä¸»è¦åˆ›æ–°åœ¨äºå°†DRLä¸MCTSæœ‰æ•ˆç»“åˆï¼Œå…‹æœäº†è±¡æ£‹ç‰¹æœ‰çš„å¤æ‚æ€§ï¼Œæå‡äº†AIåœ¨æ–‡åŒ–ç­–ç•¥æ¸¸æˆä¸­çš„è¡¨ç°ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç½‘ç»œç»“æ„ä¸Šï¼Œé‡‡ç”¨äº†æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸ºç»“åˆç­–ç•¥æŸå¤±å’Œä»·å€¼æŸå¤±ï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­å…¼é¡¾ç­–ç•¥å’Œå±€é¢è¯„ä¼°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„DRL-MCTSç³»ç»Ÿåœ¨è‡ªæˆ‘å¯¹å¼ˆä¸­èƒœç‡è¾¾åˆ°äº†85%ï¼Œç›¸æ¯”äºä¼ ç»Ÿæ–¹æ³•æå‡äº†20%ã€‚è¿™ä¸€æ˜¾è‘—çš„æ€§èƒ½æå‡è¡¨æ˜äº†è¯¥ç³»ç»Ÿåœ¨å¤æ‚ç­–ç•¥æ¸¸æˆä¸­çš„æœ‰æ•ˆæ€§å’Œæ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½æ¸¸æˆå¼€å‘ã€æ•™è‚²å·¥å…·ä»¥åŠAIå¯¹å¼ˆç³»ç»Ÿç­‰ã€‚é€šè¿‡æå‡è±¡æ£‹AIçš„å†³ç­–èƒ½åŠ›ï¼Œå¯ä»¥ä¸ºç©å®¶æä¾›æ›´å…·æŒ‘æˆ˜æ€§çš„å¯¹æ‰‹ï¼ŒåŒæ—¶ä¹Ÿä¸ºAIåœ¨å…¶ä»–å¤æ‚ç­–ç•¥æ¸¸æˆä¸­çš„åº”ç”¨æä¾›äº†å€Ÿé‰´ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper presents a Deep Reinforcement Learning (DRL) system for Xiangqi (Chinese Chess) that integrates neural networks with Monte Carlo Tree Search (MCTS) to enable strategic self-play and self-improvement. Addressing the underexplored complexity of Xiangqi, including its unique board layout, piece movement constraints, and victory conditions, our approach combines policy-value networks with MCTS to simulate move consequences and refine decision-making. By overcoming challenges such as Xiangqi's high branching factor and asymmetrical piece dynamics, our work advances AI capabilities in culturally significant strategy games while providing insights for adapting DRL-MCTS frameworks to domain-specific rule systems.

