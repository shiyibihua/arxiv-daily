---
layout: default
title: Adaptive Test-Time Reasoning via Reward-Guided Dual-Phase Search
---

# Adaptive Test-Time Reasoning via Reward-Guided Dual-Phase Search

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25420" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25420v1</a>
  <a href="https://arxiv.org/pdf/2509.25420.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25420v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25420v1', 'Adaptive Test-Time Reasoning via Reward-Guided Dual-Phase Search')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yingqian Cui, Zhenwei Dai, Pengfei He, Bing He, Hui Liu, Xianfeng Tang, Jingying Zeng, Suhang Wang, Yue Xing, Jiliang Tang, Benoit Dumoulin

**åˆ†ç±»**: cs.AI, cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå¥–åŠ±å¼•å¯¼çš„åŒé˜¶æ®µæœç´¢ï¼Œæå‡LLMåœ¨æ¨ç†ä»»åŠ¡ä¸­çš„æ•ˆç‡ä¸å‡†ç¡®æ€§ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `æ¨ç†ä»»åŠ¡` `åŒé˜¶æ®µæœç´¢` `å¥–åŠ±æ¨¡å‹` `è§„åˆ’æ‰§è¡Œ` `åŠ¨æ€é¢„ç®—åˆ†é…` `æ•°å­¦æ¨ç†` `ä»£ç ç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºæ ‘æœç´¢çš„LLMæ¨ç†æ–¹æ³•ï¼Œå¯¹æ¨ç†è¿‡ç¨‹åˆ†è§£ç®€å•ï¼Œå¿½ç•¥äº†è§„åˆ’-æ‰§è¡Œçš„æœ¬è´¨ï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹ã€‚
2. è®ºæ–‡æå‡ºåŒé˜¶æ®µæœç´¢æ¡†æ¶ï¼Œå°†æ¨ç†è¿‡ç¨‹æ˜¾å¼åœ°åˆ†ä¸ºè§„åˆ’å’Œæ‰§è¡Œé˜¶æ®µï¼Œå¹¶åˆ†åˆ«è¿›è¡Œæœç´¢å’Œä¼˜åŒ–ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ•°å­¦æ¨ç†å’Œä»£ç ç”Ÿæˆä»»åŠ¡ä¸Šï¼Œèƒ½æé«˜å‡†ç¡®ç‡å¹¶å‡å°‘å†—ä½™è®¡ç®—ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ä¸€ç§å…³é”®æ–¹æ³•æ˜¯åŸºäºæ ‘çš„æœç´¢ä¸éªŒè¯å™¨ï¼Œå®ƒæ‰©å±•å€™é€‰æ¨ç†è·¯å¾„å¹¶ä½¿ç”¨å¥–åŠ±æ¨¡å‹æ¥æŒ‡å¯¼å‰ªæå’Œé€‰æ‹©ã€‚å°½ç®¡è¿™äº›æ–¹æ³•åœ¨æé«˜å‡†ç¡®æ€§æ–¹é¢æœ‰æ•ˆï¼Œä½†åœ¨æ•ˆç‡æ–¹é¢å¹¶éæœ€ä½³ï¼šå®ƒä»¬å¯¹æ¨ç†è¿‡ç¨‹æ‰§è¡Œç®€å•çš„åˆ†è§£ï¼Œä½†å¿½ç•¥äº†æ•°å­¦æ¨ç†æˆ–ä»£ç ç”Ÿæˆç­‰ä»»åŠ¡çš„è§„åˆ’-æ‰§è¡Œæ€§è´¨ã€‚è¿™å¯¼è‡´æ¨ç†è¿‡ç¨‹çš„ä½æ•ˆæ¢ç´¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŒé˜¶æ®µæµ‹è¯•æ—¶ç¼©æ”¾æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ˜ç¡®åœ°å°†æ¨ç†åˆ†ä¸ºè§„åˆ’å’Œæ‰§è¡Œï¼Œå¹¶åˆ†åˆ«å¯¹è¿™ä¸¤ä¸ªé˜¶æ®µæ‰§è¡Œæœç´¢ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åˆ†è§£æ¨ç†è½¨è¿¹ï¼Œå¹¶ä¸ºæ¯ä¸ªé˜¶æ®µå¼€å‘å¥–åŠ±æ¨¡å‹ï¼Œä½¿æœç´¢èƒ½å¤Ÿåˆ†åˆ«æ¢ç´¢å’Œå‰ªæè®¡åˆ’å’Œæ‰§è¡Œã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†ä¸€ç§åŠ¨æ€é¢„ç®—åˆ†é…æœºåˆ¶ï¼Œè¯¥æœºåˆ¶æ ¹æ®å¥–åŠ±åé¦ˆè‡ªé€‚åº”åœ°é‡æ–°åˆ†é…é‡‡æ ·å·¥ä½œï¼Œä»è€Œå…è®¸åœ¨æœ‰ä¿¡å¿ƒçš„æ­¥éª¤ä¸Šæå‰åœæ­¢ï¼Œå¹¶å°†è®¡ç®—é‡æ–°åˆ†é…ç»™æ¨ç†è¿‡ç¨‹ä¸­æ›´å…·æŒ‘æˆ˜æ€§çš„éƒ¨åˆ†ã€‚åœ¨æ•°å­¦æ¨ç†å’Œä»£ç ç”ŸæˆåŸºå‡†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å‡å°‘å†—ä½™è®¡ç®—çš„åŒæ—¶ï¼Œå§‹ç»ˆå¦‚ä¸€åœ°æé«˜äº†å‡†ç¡®æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºæ ‘æœç´¢çš„LLMæ¨ç†æ–¹æ³•ï¼Œä¾‹å¦‚åœ¨æ•°å­¦æ¨ç†å’Œä»£ç ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œé€šå¸¸é‡‡ç”¨ç®€å•çš„åˆ†è§£æ–¹å¼ï¼Œæ²¡æœ‰å……åˆ†è€ƒè™‘ä»»åŠ¡çš„è§„åˆ’-æ‰§è¡Œç‰¹æ€§ã€‚è¿™ç§æ–¹å¼å¯¼è‡´æœç´¢ç©ºé—´åºå¤§ï¼Œæ•ˆç‡ä½ä¸‹ï¼Œæµªè´¹è®¡ç®—èµ„æºã€‚å› æ­¤ï¼Œå¦‚ä½•æ›´æœ‰æ•ˆåœ°æ¢ç´¢æ¨ç†è¿‡ç¨‹ï¼Œæé«˜æ¨ç†æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼Œæ˜¯æœ¬æ–‡è¦è§£å†³çš„æ ¸å¿ƒé—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†æ¨ç†è¿‡ç¨‹åˆ†è§£ä¸ºè§„åˆ’å’Œæ‰§è¡Œä¸¤ä¸ªé˜¶æ®µï¼Œå¹¶åˆ†åˆ«å¯¹è¿™ä¸¤ä¸ªé˜¶æ®µè¿›è¡Œæœç´¢å’Œä¼˜åŒ–ã€‚è§„åˆ’é˜¶æ®µè´Ÿè´£åˆ¶å®šæ¨ç†æ–¹æ¡ˆï¼Œæ‰§è¡Œé˜¶æ®µè´Ÿè´£æ ¹æ®æ–¹æ¡ˆæ‰§è¡Œå…·ä½“æ­¥éª¤ã€‚é€šè¿‡è¿™ç§åˆ†è§£ï¼Œå¯ä»¥æ›´æœ‰æ•ˆåœ°åˆ©ç”¨å¥–åŠ±æ¨¡å‹ï¼Œåˆ†åˆ«å¯¹è§„åˆ’å’Œæ‰§è¡Œè¿›è¡Œè¯„ä¼°å’Œå‰ªæï¼Œä»è€Œå‡å°‘æœç´¢ç©ºé—´ï¼Œæé«˜æ¨ç†æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…å«è§„åˆ’é˜¶æ®µå’Œæ‰§è¡Œé˜¶æ®µã€‚åœ¨è§„åˆ’é˜¶æ®µï¼Œæ¨¡å‹ç”Ÿæˆå¤šä¸ªæ¨ç†è®¡åˆ’ï¼Œå¹¶ä½¿ç”¨å¥–åŠ±æ¨¡å‹å¯¹è¿™äº›è®¡åˆ’è¿›è¡Œè¯„ä¼°ã€‚ç„¶åï¼Œé€‰æ‹©å¥–åŠ±æœ€é«˜çš„è®¡åˆ’è¿›å…¥æ‰§è¡Œé˜¶æ®µã€‚åœ¨æ‰§è¡Œé˜¶æ®µï¼Œæ¨¡å‹æ ¹æ®é€‰å®šçš„è®¡åˆ’æ‰§è¡Œå…·ä½“æ­¥éª¤ï¼Œå¹¶ä½¿ç”¨å¦ä¸€ä¸ªå¥–åŠ±æ¨¡å‹å¯¹æ‰§è¡Œç»“æœè¿›è¡Œè¯„ä¼°ã€‚æ¡†æ¶è¿˜åŒ…å«ä¸€ä¸ªåŠ¨æ€é¢„ç®—åˆ†é…æœºåˆ¶ï¼Œè¯¥æœºåˆ¶æ ¹æ®å¥–åŠ±åé¦ˆè‡ªé€‚åº”åœ°è°ƒæ•´è§„åˆ’å’Œæ‰§è¡Œé˜¶æ®µçš„è®¡ç®—èµ„æºåˆ†é…ã€‚å¦‚æœæŸä¸ªé˜¶æ®µçš„å¥–åŠ±è¾ƒé«˜ï¼Œåˆ™åˆ†é…æ›´å¤šçš„è®¡ç®—èµ„æºç»™è¯¥é˜¶æ®µï¼Œåä¹‹åˆ™å‡å°‘åˆ†é…ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†åŒé˜¶æ®µæœç´¢æ¡†æ¶ï¼Œå°†æ¨ç†è¿‡ç¨‹åˆ†è§£ä¸ºè§„åˆ’å’Œæ‰§è¡Œé˜¶æ®µï¼Œå¹¶åˆ†åˆ«è¿›è¡Œæœç´¢å’Œä¼˜åŒ–ã€‚è¿™ç§åˆ†è§£æ–¹å¼æ›´ç¬¦åˆäººç±»çš„æ¨ç†è¿‡ç¨‹ï¼Œå¯ä»¥æ›´æœ‰æ•ˆåœ°åˆ©ç”¨å¥–åŠ±æ¨¡å‹ï¼Œå‡å°‘æœç´¢ç©ºé—´ï¼Œæé«˜æ¨ç†æ•ˆç‡ã€‚æ­¤å¤–ï¼ŒåŠ¨æ€é¢„ç®—åˆ†é…æœºåˆ¶å¯ä»¥æ ¹æ®å¥–åŠ±åé¦ˆè‡ªé€‚åº”åœ°è°ƒæ•´è®¡ç®—èµ„æºåˆ†é…ï¼Œè¿›ä¸€æ­¥æé«˜æ¨ç†æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸ºè§„åˆ’å’Œæ‰§è¡Œé˜¶æ®µåˆ†åˆ«è®¾è®¡äº†å¥–åŠ±æ¨¡å‹ã€‚è§„åˆ’é˜¶æ®µçš„å¥–åŠ±æ¨¡å‹ç”¨äºè¯„ä¼°æ¨ç†è®¡åˆ’çš„è´¨é‡ï¼Œæ‰§è¡Œé˜¶æ®µçš„å¥–åŠ±æ¨¡å‹ç”¨äºè¯„ä¼°æ‰§è¡Œç»“æœçš„è´¨é‡ã€‚å¥–åŠ±æ¨¡å‹å¯ä»¥ä½¿ç”¨å„ç§æœºå™¨å­¦ä¹ æ–¹æ³•è¿›è¡Œè®­ç»ƒï¼Œä¾‹å¦‚ç›‘ç£å­¦ä¹ æˆ–å¼ºåŒ–å­¦ä¹ ã€‚åŠ¨æ€é¢„ç®—åˆ†é…æœºåˆ¶å¯ä»¥ä½¿ç”¨å„ç§ä¼˜åŒ–ç®—æ³•è¿›è¡Œå®ç°ï¼Œä¾‹å¦‚æ¢¯åº¦ä¸‹é™æˆ–è¿›åŒ–ç®—æ³•ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„å–å†³äºå…·ä½“çš„ä»»åŠ¡å’Œæ•°æ®é›†ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ•°å­¦æ¨ç†å’Œä»£ç ç”Ÿæˆä»»åŠ¡ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šï¼Œè¯¥æ–¹æ³•åœ¨å‡†ç¡®ç‡æå‡çš„åŒæ—¶ï¼Œå‡å°‘äº†å†—ä½™è®¡ç®—ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨è®¡ç®—èµ„æºï¼Œæé«˜æ¨ç†æ•ˆç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦å¤æ‚æ¨ç†èƒ½åŠ›çš„åœºæ™¯ï¼Œä¾‹å¦‚æ•°å­¦é—®é¢˜æ±‚è§£ã€ä»£ç ç”Ÿæˆã€è‡ªç„¶è¯­è¨€ç†è§£å’Œå¯¹è¯ç³»ç»Ÿã€‚é€šè¿‡æé«˜LLMçš„æ¨ç†æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼Œå¯ä»¥ä½¿è¿™äº›åº”ç”¨æ›´åŠ æ™ºèƒ½å’Œå¯é ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥åº”ç”¨äºæœºå™¨äººæ§åˆ¶ç­‰é¢†åŸŸï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿæ›´å¥½åœ°è§„åˆ’å’Œæ‰§è¡Œä»»åŠ¡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have achieved significant advances in reasoning tasks. A key approach is tree-based search with verifiers, which expand candidate reasoning paths and use reward models to guide pruning and selection. Although effective in improving accuracy, these methods are not optimal in terms of efficiency: they perform simple decomposition on the reasoning process, but ignore the planning-execution nature of tasks such as math reasoning or code generation. This results in inefficient exploration of reasoning process. To address this, we propose a dual-phase test-time scaling framework that explicitly separates reasoning into planning and execution, and performs search over the two phases individually. Specifically, we decompose reasoning trajectories and develop reward models for each phase, enabling the search to explore and prune plans and executions separately. We further introduce a dynamic budget allocation mechanism that adaptively redistributes sampling effort based on reward feedback, allowing early stopping on confident steps and reallocation of computation to more challenging parts of the reasoning process. Experiments on both mathematical reasoning and code generation benchmarks demonstrate that our approach consistently improves accuracy while reducing redundant computation.

