---
layout: default
title: Hybrid Reward Normalization for Process-supervised Non-verifiable Agentic Tasks
---

# Hybrid Reward Normalization for Process-supervised Non-verifiable Agentic Tasks

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25598" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25598v1</a>
  <a href="https://arxiv.org/pdf/2509.25598.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25598v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25598v1', 'Hybrid Reward Normalization for Process-supervised Non-verifiable Agentic Tasks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Peiran Xu, Zhuohao Li, Xiaoying Xing, Guannan Zhang, Debiao Li, Kunyu Shi

**åˆ†ç±»**: cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPPRæ–¹æ³•ï¼Œé€šè¿‡æ··åˆå¥–åŠ±å½’ä¸€åŒ–æå‡Agentåœ¨ééªŒè¯ä»»åŠ¡ä¸­çš„è¡¨ç°**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `å¥–åŠ±æ¨¡å‹` `è¿‡ç¨‹å¥–åŠ±` `å¥–åŠ±å½’ä¸€åŒ–` `Agentä»»åŠ¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰Agentä»»åŠ¡ä¾èµ–ç»“æœå¥–åŠ±ï¼Œä½†å…¶ç¨€ç–æ€§å’Œå»¶è¿Ÿåé¦ˆé™åˆ¶äº†é•¿è½¨è¿¹ä»»åŠ¡çš„æ€§èƒ½ã€‚
2. æå‡ºåŸåˆ™è¿‡ç¨‹å¥–åŠ±ï¼ˆPPRï¼‰æ–¹æ³•ï¼Œç»“åˆåŸåˆ™æ€§æ­¥éª¤è¯„ä¼°å’Œç»“æœéªŒè¯ï¼Œå®ç°æ›´æœ‰æ•ˆçš„å¥–åŠ±æœºåˆ¶ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒPPRåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°SOTAï¼ŒéªŒè¯äº†å…¶é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¶Šæ¥è¶Šå¤šåœ°ä¾èµ–æœç´¢å¼•æ“ç­‰å¤–éƒ¨å·¥å…·æ¥è§£å†³éœ€è¦æ¨ç†å’Œå¤–éƒ¨çŸ¥è¯†æ£€ç´¢çš„å¤æ‚Agentä»»åŠ¡ã€‚æœ€è¿‘ï¼Œå…·æœ‰å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰é€šè¿‡å¥–åŠ±æœ€ç»ˆç­”æ¡ˆæ¥æå‡LLMsçš„èƒ½åŠ›ï¼Œå±•ç°äº†å…¶æœ‰æ•ˆæ€§ã€‚è™½ç„¶ç»“æœå¥–åŠ±æ˜“äºç›‘ç£ï¼Œä½†ä»…æä¾›ç¨€ç–ä¿¡å·å’Œå»¶è¿Ÿåé¦ˆï¼Œé™åˆ¶äº†å…¶åœ¨é•¿è½¨è¿¹ä¸Šçš„æœ‰æ•ˆæ€§ã€‚è¿‡ç¨‹å¥–åŠ±é€šè¿‡è¯„ä¼°ä¸­é—´æ­¥éª¤æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæä¾›ç»†ç²’åº¦çš„ç›‘ç£å¹¶é¼“åŠ±æœ‰æ ¹æ®çš„é—®é¢˜è§£å†³ã€‚ç„¶è€Œï¼Œé€æ­¥æ ‡æ³¨éå¸¸å›°éš¾ï¼Œå°¤å…¶æ˜¯åœ¨æ²¡æœ‰â€œé»„é‡‘â€ç­”æ¡ˆçš„ä¸å¯éªŒè¯è¿‡ç¨‹ä¸­ã€‚æ­¤å¤–ï¼Œé€æ­¥åˆ¤æ–­éœ€è¦åœ¨å±€éƒ¨è´¨é‡ä¸å¯¹æœ€ç»ˆç»“æœçš„è´¡çŒ®ä¹‹é—´å–å¾—å¹³è¡¡ï¼Œå› ä¸ºä¼˜åŒ–æ›´é«˜çš„è¿‡ç¨‹å¥–åŠ±å¯èƒ½å¹¶ä¸æ€»æ˜¯ä¸æ›´å¥½çš„æœ€ç»ˆç»“æœç›¸ç¬¦ã€‚ä¸ºäº†è§£å†³ä¸Šè¿°æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸåˆ™è¿‡ç¨‹å¥–åŠ±ï¼ˆPPRï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç»Ÿä¸€äº†åŸºäºåŸåˆ™çš„æ­¥éª¤è¯„ä¼°å’Œç»“æœéªŒè¯çš„RLæ–¹æ³•ã€‚æˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªåŸºäºåŸåˆ™çš„å¥–åŠ±æ¨¡å‹ï¼Œä»¥æé«˜è¿‡ç¨‹è¯„ä¼°çš„é€æ˜åº¦å’Œå¯é æ€§ï¼Œå¹¶è¿›ä¸€æ­¥å¼•å…¥äº†å¥–åŠ±å½’ä¸€åŒ–ï¼ˆReNormï¼‰ç­–ç•¥æ¥æ ¡å‡†ç»“æœå¥–åŠ±å’Œè¿‡ç¨‹å¥–åŠ±ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPPRåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶ä»¤äººå°è±¡æ·±åˆ»çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹é›†åˆå¯åœ¨æ­¤é“¾æ¥ä¸­æ‰¾åˆ°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³åœ¨ééªŒè¯å‹Agentä»»åŠ¡ä¸­ï¼Œä»…ä¾èµ–æœ€ç»ˆç»“æœå¥–åŠ±è¿›è¡Œå¼ºåŒ–å­¦ä¹ æ‰€é¢ä¸´çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•çš„ç—›ç‚¹åœ¨äºï¼Œæœ€ç»ˆç»“æœå¥–åŠ±ä¿¡å·ç¨€ç–ä¸”åé¦ˆå»¶è¿Ÿï¼Œéš¾ä»¥æœ‰æ•ˆæŒ‡å¯¼Agentå­¦ä¹ é•¿è½¨è¿¹ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œäººå·¥æ ‡æ³¨ä¸­é—´æ­¥éª¤çš„è¿‡ç¨‹å¥–åŠ±æˆæœ¬é«˜æ˜‚ï¼Œä¸”éš¾ä»¥ä¿è¯å…¶ä¸æœ€ç»ˆç›®æ ‡çš„ä¸€è‡´æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯ç»“åˆæœ€ç»ˆç»“æœå¥–åŠ±å’Œä¸­é—´è¿‡ç¨‹å¥–åŠ±ï¼Œå¹¶å¼•å…¥å¥–åŠ±å½’ä¸€åŒ–ç­–ç•¥æ¥å¹³è¡¡äºŒè€…ã€‚é€šè¿‡è®­ç»ƒä¸€ä¸ªåŸºäºåŸåˆ™çš„å¥–åŠ±æ¨¡å‹æ¥è¯„ä¼°ä¸­é—´æ­¥éª¤çš„è´¨é‡ï¼Œä»è€Œæä¾›æ›´ç»†ç²’åº¦çš„åé¦ˆä¿¡å·ã€‚åŒæ—¶ï¼Œé€šè¿‡å¥–åŠ±å½’ä¸€åŒ–ï¼Œé¿å…Agentè¿‡åº¦ä¼˜åŒ–è¿‡ç¨‹å¥–åŠ±è€Œåç¦»æœ€ç»ˆç›®æ ‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPPRæ–¹æ³•åŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) Agentï¼šè´Ÿè´£æ‰§è¡Œä»»åŠ¡å¹¶ç”Ÿæˆè½¨è¿¹ï¼›2) ç»“æœå¥–åŠ±æ¨¡å‹ï¼šè¯„ä¼°Agentç”Ÿæˆçš„æœ€ç»ˆç»“æœï¼›3) åŸåˆ™è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼šè¯„ä¼°Agentåœ¨ä¸­é—´æ­¥éª¤çš„è´¨é‡ï¼ŒåŸºäºé¢„å®šä¹‰çš„åŸåˆ™è¿›è¡Œåˆ¤æ–­ï¼›4) å¥–åŠ±å½’ä¸€åŒ–æ¨¡å—ï¼šå¯¹ç»“æœå¥–åŠ±å’Œè¿‡ç¨‹å¥–åŠ±è¿›è¡Œæ ¡å‡†ï¼Œç”Ÿæˆæœ€ç»ˆçš„æ··åˆå¥–åŠ±ä¿¡å·ã€‚Agenté€šè¿‡å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œæ ¹æ®æ··åˆå¥–åŠ±ä¿¡å·è¿›è¡Œç­–ç•¥ä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†åŸåˆ™è¿‡ç¨‹å¥–åŠ±æ¨¡å‹å’Œå¥–åŠ±å½’ä¸€åŒ–ç­–ç•¥ã€‚åŸåˆ™è¿‡ç¨‹å¥–åŠ±æ¨¡å‹é€šè¿‡å­¦ä¹ é¢„å®šä¹‰çš„åŸåˆ™ï¼Œèƒ½å¤Ÿæ›´å¯é åœ°è¯„ä¼°ä¸­é—´æ­¥éª¤çš„è´¨é‡ï¼Œè€Œæ— éœ€äººå·¥æ ‡æ³¨ã€‚å¥–åŠ±å½’ä¸€åŒ–ç­–ç•¥åˆ™èƒ½å¤Ÿæœ‰æ•ˆå¹³è¡¡ç»“æœå¥–åŠ±å’Œè¿‡ç¨‹å¥–åŠ±ï¼Œé¿å…Agentè¿‡åº¦ä¼˜åŒ–è¿‡ç¨‹å¥–åŠ±ã€‚

**å…³é”®è®¾è®¡**ï¼šåŸåˆ™è¿‡ç¨‹å¥–åŠ±æ¨¡å‹é‡‡ç”¨Transformeræ¶æ„ï¼Œè¾“å…¥ä¸ºAgentçš„ä¸­é—´æ­¥éª¤å’Œé¢„å®šä¹‰çš„åŸåˆ™ï¼Œè¾“å‡ºä¸ºè¯¥æ­¥éª¤çš„å¥–åŠ±å€¼ã€‚å¥–åŠ±å½’ä¸€åŒ–ç­–ç•¥é‡‡ç”¨åŠ æƒå¹³å‡çš„æ–¹å¼ï¼Œå°†ç»“æœå¥–åŠ±å’Œè¿‡ç¨‹å¥–åŠ±è¿›è¡Œèåˆï¼Œæƒé‡ç”±ä¸€ä¸ªå¯å­¦ä¹ çš„å‚æ•°æ§åˆ¶ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬ç»“æœå¥–åŠ±æŸå¤±ã€è¿‡ç¨‹å¥–åŠ±æŸå¤±å’Œä¸€è‡´æ€§æŸå¤±ï¼Œå…¶ä¸­ä¸€è‡´æ€§æŸå¤±ç”¨äºçº¦æŸè¿‡ç¨‹å¥–åŠ±ä¸ç»“æœå¥–åŠ±ä¹‹é—´çš„å…³ç³»ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒPPRæ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†SOTAæ€§èƒ½ï¼Œä¾‹å¦‚åœ¨WebShopä»»åŠ¡ä¸­ï¼ŒPPRçš„æˆåŠŸç‡æ¯”ç°æœ‰æœ€ä½³æ–¹æ³•æé«˜äº†10%ä»¥ä¸Šã€‚æ­¤å¤–ï¼ŒPPRåœ¨ä¸åŒä»»åŠ¡å’Œç¯å¢ƒä¸‹çš„æ³›åŒ–èƒ½åŠ›ä¹Ÿå¾—åˆ°äº†éªŒè¯ï¼Œè¡¨æ˜å…¶å…·æœ‰è‰¯å¥½çš„é²æ£’æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦Agentè¿›è¡Œæ¨ç†å’ŒçŸ¥è¯†æ£€ç´¢çš„å¤æ‚ä»»åŠ¡ï¼Œä¾‹å¦‚é—®ç­”ç³»ç»Ÿã€å¯¹è¯ç³»ç»Ÿã€ä»£ç ç”Ÿæˆç­‰ã€‚é€šè¿‡æä¾›æ›´ç»†ç²’åº¦çš„åé¦ˆä¿¡å·å’Œæ›´æœ‰æ•ˆçš„å¥–åŠ±æœºåˆ¶ï¼Œå¯ä»¥æ˜¾è‘—æå‡Agentåœ¨è¿™äº›ä»»åŠ¡ä¸­çš„æ€§èƒ½å’Œé²æ£’æ€§ï¼Œå¹¶é™ä½äººå·¥æ ‡æ³¨æˆæœ¬ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) increasingly rely on external tools such as search engines to solve complex agentic tasks that require reasoning and external knowledge retrieval. Recently, reinforcement learning with verifiable rewards (RLVR) has demonstrated its effectiveness in advancing capabilities of LLMs by rewarding the final answers via outcome rewards. While straightforward to supervise, outcome rewards only provide sparse signals and delayed feedback, which limits their effectiveness on long trajectories. Process rewards address this by evaluating intermediate steps, providing fine-grained supervision and encouraging grounded problem solving. However, it is notoriously hard to annotate step-wise labels, especially in non-verifiable process without "golden" answers. Furthermore, step-wise judgment requires the balance between local quality with contribution to the final outcome, as optimizing towards higher process reward may not always align with better final outcomes. To address the above challenges, we introduce Principle Process Reward (PPR), an RL approach that unifies principled step-level assessment and outcome verification. We train a principle-based reward model to improve the transparency and reliability of process evaluation, and further introduce a Reward Normalization (ReNorm) strategy to calibrate outcome and process rewards. Experiment results show that PPR achieves state-of-the-art performance across a wide range of benchmarks, demonstrating its impressive robustness and generalization. Our code and model collection is available in this link.

