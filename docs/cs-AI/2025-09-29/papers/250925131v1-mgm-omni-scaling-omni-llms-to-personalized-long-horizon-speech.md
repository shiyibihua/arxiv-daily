---
layout: default
title: MGM-Omni: Scaling Omni LLMs to Personalized Long-Horizon Speech
---

# MGM-Omni: Scaling Omni LLMs to Personalized Long-Horizon Speech

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25131" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25131v1</a>
  <a href="https://arxiv.org/pdf/2509.25131.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25131v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25131v1', 'MGM-Omni: Scaling Omni LLMs to Personalized Long-Horizon Speech')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chengyao Wang, Zhisheng Zhong, Bohao Peng, Senqiao Yang, Yuqi Liu, Haokun Gui, Bin Xia, Jingyao Li, Bei Yu, Jiaya Jia

**åˆ†ç±»**: cs.SD, cs.AI, cs.CL, cs.CV, cs.MM

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29

**å¤‡æ³¨**: Code is available at https://github.com/dvlab-research/MGM-Omni

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**MGM-Omniï¼šé¢å‘ä¸ªæ€§åŒ–é•¿æ—¶ç¨‹è¯­éŸ³çš„é€šç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `è¯­éŸ³ç”Ÿæˆ` `é•¿æ—¶ç¨‹è¯­éŸ³` `é›¶æ ·æœ¬è¯­éŸ³å…‹éš†` `è·¨æ¨¡æ€ç†è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è¯­éŸ³ç”Ÿæˆæ–¹æ³•é€šå¸¸é‡‡ç”¨çº§è”æµç¨‹ï¼Œç¼ºä¹è·¨æ¨¡æ€ä¿¡æ¯çš„æœ‰æ•ˆèåˆå’Œå®æ—¶æ€§ã€‚
2. MGM-Omnié‡‡ç”¨â€œè„‘-å£â€åŒè½¨æ¶æ„ï¼Œè§£è€¦å¤šæ¨¡æ€æ¨ç†å’Œè¯­éŸ³ç”Ÿæˆï¼Œå®ç°é«˜æ•ˆè·¨æ¨¡æ€äº¤äº’ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒMGM-Omniåœ¨éŸ³è‰²ä¿æŒã€è¯­éŸ³è‡ªç„¶åº¦å’Œé•¿éŸ³é¢‘ç†è§£æ–¹é¢ä¼˜äºç°æœ‰å¼€æºæ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†MGM-Omniï¼Œä¸€ä¸ªç»Ÿä¸€çš„é€šç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œç”¨äºå¤šæ¨¡æ€ç†è§£å’Œå¯Œæœ‰è¡¨ç°åŠ›çš„é•¿æ—¶ç¨‹è¯­éŸ³ç”Ÿæˆã€‚ä¸å­¤ç«‹è¯­éŸ³åˆæˆçš„çº§è”æµç¨‹ä¸åŒï¼ŒMGM-Omnié‡‡ç”¨â€œè„‘-å£â€è®¾è®¡ï¼Œå…·æœ‰åŒè½¨ã€åŸºäºtokençš„æ¶æ„ï¼Œå°†å¤šæ¨¡æ€æ¨ç†ä¸å®æ—¶è¯­éŸ³ç”Ÿæˆå¹²å‡€åœ°è§£è€¦ã€‚è¿™ç§è®¾è®¡å®ç°äº†é«˜æ•ˆçš„è·¨æ¨¡æ€äº¤äº’å’Œä½å»¶è¿Ÿçš„æµå¼è¯­éŸ³ç”Ÿæˆã€‚åœ¨ç†è§£æ–¹é¢ï¼Œç»Ÿä¸€çš„è®­ç»ƒç­–ç•¥ä¸åŒéŸ³é¢‘ç¼–ç å™¨è®¾è®¡ç›¸ç»“åˆï¼Œå®ç°äº†è·¨ä¸åŒå£°å­¦æ¡ä»¶çš„é•¿æ ¼å¼éŸ³é¢‘æ„ŸçŸ¥ã€‚åœ¨ç”Ÿæˆæ–¹é¢ï¼ŒåŸºäºå—çš„å¹¶è¡Œè§£ç æ–¹æ¡ˆç¼©å°äº†æ–‡æœ¬-è¯­éŸ³tokené€Ÿç‡å·®è·ï¼ŒåŠ é€Ÿäº†æ¨ç†ï¼Œå¹¶æ”¯æŒå…·æœ‰ç¨³å®šéŸ³è‰²çš„æ‰©å±•æŒç»­æ—¶é—´çš„æµå¼é›¶æ ·æœ¬è¯­éŸ³å…‹éš†ã€‚ä¸åŒæœŸçš„å·¥ä½œç›¸æ¯”ï¼ŒMGM-Omniä»¥æ˜¾è‘—çš„æ•°æ®é«˜æ•ˆè®­ç»ƒå®ç°äº†è¿™äº›èƒ½åŠ›ã€‚å¤§é‡çš„å®éªŒè¡¨æ˜ï¼ŒMGM-Omniåœ¨è·¨æ‰©å±•åºåˆ—ä¿æŒéŸ³è‰²èº«ä»½ã€ç”Ÿæˆè‡ªç„¶å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è¯­éŸ³ä»¥åŠå®ç°å“è¶Šçš„é•¿æ ¼å¼éŸ³é¢‘å’Œå…¨æ¨¡æ€ç†è§£æ–¹é¢ä¼˜äºç°æœ‰çš„å¼€æºæ¨¡å‹ã€‚MGM-Omniä¸ºå…¨æ¨¡æ€ç†è§£å’Œå¯æ§çš„ä¸ªæ€§åŒ–é•¿æ—¶ç¨‹è¯­éŸ³ç”Ÿæˆå»ºç«‹äº†ä¸€ä¸ªé«˜æ•ˆçš„ç«¯åˆ°ç«¯èŒƒä¾‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è¯­éŸ³ç”Ÿæˆç³»ç»Ÿé€šå¸¸é‡‡ç”¨çº§è”pipelineï¼Œå°†è¯­éŸ³åˆæˆä¸å¤šæ¨¡æ€ç†è§£åˆ†ç¦»ï¼Œå¯¼è‡´è·¨æ¨¡æ€ä¿¡æ¯äº¤äº’å—é™ï¼Œéš¾ä»¥å®ç°ä½å»¶è¿Ÿçš„æµå¼è¯­éŸ³ç”Ÿæˆï¼Œå¹¶ä¸”åœ¨é•¿æ—¶ç¨‹è¯­éŸ³ç”Ÿæˆä¸­éŸ³è‰²å®¹æ˜“æ¼‚ç§»ã€‚æ­¤å¤–ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†ä¸åŒå£°å­¦æ¡ä»¶ä¸‹çš„é•¿éŸ³é¢‘æ—¶ï¼Œé²æ£’æ€§å’Œç†è§£èƒ½åŠ›æœ‰å¾…æé«˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMGM-Omniçš„æ ¸å¿ƒåœ¨äºé‡‡ç”¨ä¸€ä¸ªç»Ÿä¸€çš„Omni LLMæ¡†æ¶ï¼Œé€šè¿‡â€œè„‘-å£â€åŒè½¨æ¶æ„ï¼Œå°†å¤šæ¨¡æ€ç†è§£ï¼ˆâ€œè„‘â€ï¼‰å’Œè¯­éŸ³ç”Ÿæˆï¼ˆâ€œå£â€ï¼‰è§£è€¦ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„è·¨æ¨¡æ€äº¤äº’å’Œä½å»¶è¿Ÿçš„æµå¼è¯­éŸ³ç”Ÿæˆã€‚è¿™ç§è®¾è®¡å…è®¸æ¨¡å‹åœ¨ç†è§£å¤šæ¨¡æ€è¾“å…¥çš„åŒæ—¶ï¼Œå¹¶è¡Œåœ°ç”Ÿæˆè¯­éŸ³ï¼Œä»è€Œæ˜¾è‘—æå‡äº†ç”Ÿæˆæ•ˆç‡å’Œå®æ—¶æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMGM-Omniçš„æ•´ä½“æ¶æ„åŒ…å«ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ï¼šå¤šæ¨¡æ€ç†è§£æ¨¡å—å’Œè¯­éŸ³ç”Ÿæˆæ¨¡å—ã€‚å¤šæ¨¡æ€ç†è§£æ¨¡å—è´Ÿè´£å¤„ç†åŒ…æ‹¬æ–‡æœ¬ã€éŸ³é¢‘ç­‰å¤šç§æ¨¡æ€çš„è¾“å…¥ï¼Œå¹¶å°†å…¶ç¼–ç æˆç»Ÿä¸€çš„tokenè¡¨ç¤ºã€‚è¯¥æ¨¡å—é‡‡ç”¨äº†åŒéŸ³é¢‘ç¼–ç å™¨è®¾è®¡ï¼Œä»¥å¢å¼ºå¯¹ä¸åŒå£°å­¦æ¡ä»¶çš„é²æ£’æ€§ã€‚è¯­éŸ³ç”Ÿæˆæ¨¡å—åˆ™åŸºäºè¿™äº›tokenè¡¨ç¤ºç”Ÿæˆè¯­éŸ³ï¼Œé‡‡ç”¨chunk-basedå¹¶è¡Œè§£ç æ–¹æ¡ˆï¼ŒåŠ é€Ÿæ¨ç†è¿‡ç¨‹ã€‚æ•´ä¸ªæ¡†æ¶é‡‡ç”¨ç«¯åˆ°ç«¯çš„æ–¹å¼è¿›è¡Œè®­ç»ƒï¼Œä»¥ä¼˜åŒ–è·¨æ¨¡æ€ä¿¡æ¯çš„æµåŠ¨å’Œè¯­éŸ³ç”Ÿæˆçš„è´¨é‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šMGM-Omniçš„å…³é”®åˆ›æ–°åœ¨äºå…¶â€œè„‘-å£â€åŒè½¨æ¶æ„å’Œç»Ÿä¸€çš„Omni LLMæ¡†æ¶ã€‚è¿™ç§æ¶æ„ä¸ä»…å®ç°äº†å¤šæ¨¡æ€ç†è§£å’Œè¯­éŸ³ç”Ÿæˆçš„è§£è€¦ï¼Œè¿˜é€šè¿‡chunk-basedå¹¶è¡Œè§£ç æ–¹æ¡ˆæ˜¾è‘—æå‡äº†è¯­éŸ³ç”Ÿæˆçš„æ•ˆç‡å’Œå®æ—¶æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹é‡‡ç”¨ç»Ÿä¸€çš„è®­ç»ƒç­–ç•¥ï¼Œå®ç°äº†æ•°æ®é«˜æ•ˆçš„è®­ç»ƒï¼Œå¹¶åœ¨é•¿æ—¶ç¨‹è¯­éŸ³ç”Ÿæˆä¸­ä¿æŒäº†ç¨³å®šçš„éŸ³è‰²ã€‚

**å…³é”®è®¾è®¡**ï¼šMGM-Omnié‡‡ç”¨äº†åŒéŸ³é¢‘ç¼–ç å™¨è®¾è®¡ï¼Œä»¥å¢å¼ºå¯¹ä¸åŒå£°å­¦æ¡ä»¶çš„é²æ£’æ€§ã€‚åœ¨è¯­éŸ³ç”Ÿæˆæ¨¡å—ï¼Œé‡‡ç”¨äº†chunk-basedå¹¶è¡Œè§£ç æ–¹æ¡ˆï¼Œå°†é•¿åºåˆ—åˆ†å‰²æˆå¤šä¸ªchunkå¹¶è¡Œè§£ç ï¼Œä»è€ŒåŠ é€Ÿæ¨ç†è¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œæ¨¡å‹è¿˜é‡‡ç”¨äº†ç‰¹æ®Šçš„æŸå¤±å‡½æ•°ï¼Œä»¥ä¼˜åŒ–éŸ³è‰²ä¿æŒå’Œè¯­éŸ³è‡ªç„¶åº¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

MGM-Omniåœ¨é•¿æ—¶ç¨‹è¯­éŸ³ç”Ÿæˆä¸­è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿä¿æŒç¨³å®šçš„éŸ³è‰²èº«ä»½ï¼Œå¹¶åœ¨è¯­éŸ³è‡ªç„¶åº¦å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥æ–¹é¢ä¼˜äºç°æœ‰å¼€æºæ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMGM-Omniåœ¨é•¿æ ¼å¼éŸ³é¢‘å’Œå…¨æ¨¡æ€ç†è§£æ–¹é¢ä¹Ÿå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå¹¶ä¸”ä»¥æ›´å°‘çš„æ•°æ®å®ç°äº†è¿™äº›èƒ½åŠ›ï¼Œä½“ç°äº†å…¶æ•°æ®é«˜æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MGM-Omniå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼ŒåŒ…æ‹¬ä¸ªæ€§åŒ–è¯­éŸ³åŠ©æ‰‹ã€æ™ºèƒ½å®¢æœã€æ¸¸æˆè§’è‰²é…éŸ³ã€æœ‰å£°è¯»ç‰©ç”Ÿæˆç­‰ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿç†è§£å¤šæ¨¡æ€è¾“å…¥ï¼Œç”Ÿæˆè‡ªç„¶ã€æµç•…ä¸”å…·æœ‰ä¸ªæ€§åŒ–éŸ³è‰²çš„è¯­éŸ³ï¼Œä»è€Œæå‡ç”¨æˆ·ä½“éªŒã€‚æ­¤å¤–ï¼ŒMGM-Omniçš„ä½å»¶è¿Ÿç‰¹æ€§ä½¿å…¶é€‚ç”¨äºå®æ—¶è¯­éŸ³äº¤äº’åœºæ™¯ï¼Œä¾‹å¦‚åœ¨çº¿ä¼šè®®å’Œè¿œç¨‹æ•™è‚²ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We present MGM-Omni, a unified Omni LLM for omni-modal understanding and expressive, long-horizon speech generation. Unlike cascaded pipelines that isolate speech synthesis, MGM-Omni adopts a "brain-mouth" design with a dual-track, token-based architecture that cleanly decouples multimodal reasoning from real-time speech generation. This design enables efficient cross-modal interaction and low-latency, streaming speech generation. For understanding, a unified training strategy coupled with a dual audio encoder design enables long-form audio perception across diverse acoustic conditions. For generation, a chunk-based parallel decoding scheme narrows the text speech token-rate gap, accelerating inference and supporting streaming zero-shot voice cloning with stable timbre over extended durations. Compared to concurrent work, MGM-Omni achieves these capabilities with markedly data-efficient training. Extensive experiments demonstrate that MGM-Omni outperforms existing open source models in preserving timbre identity across extended sequences, producing natural and context-aware speech, and achieving superior long-form audio and omnimodal understanding. MGM-Omni establishes an efficient, end-to-end paradigm for omnimodal understanding and controllable, personalised long-horizon speech generation.

