---
layout: default
title: Bridging the behavior-neural gap: A multimodal AI reveals the brain's geometry of emotion more accurately than human self-reports
---

# Bridging the behavior-neural gap: A multimodal AI reveals the brain's geometry of emotion more accurately than human self-reports

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.24298" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.24298v1</a>
  <a href="https://arxiv.org/pdf/2509.24298.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.24298v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.24298v1', 'Bridging the behavior-neural gap: A multimodal AI reveals the brain\'s geometry of emotion more accurately than human self-reports')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Changde Du, Yizhuo Lu, Zhongyu Huang, Yi Sun, Zisen Zhou, Shaozheng Qin, Huiguang He

**åˆ†ç±»**: cs.HC, cs.AI, cs.CL, cs.CY, cs.MM

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://reedonepeck.github.io/ai-emotion.github.io/)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**å¤šæ¨¡æ€AIè¶…è¶Šäººç±»è‡ªæŠ¥å‘Šï¼Œæ›´å‡†ç¡®æ­ç¤ºå¤§è„‘æƒ…æ„Ÿå‡ ä½•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æƒ…æ„Ÿè¡¨å¾` `å¤šæ¨¡æ€å­¦ä¹ ` `å¤§è¯­è¨€æ¨¡å‹` `ç¥ç»æ´»åŠ¨é¢„æµ‹` `è¡Œä¸º-ç¥ç»å·®è·` `æƒ…æ„Ÿè®¡ç®—` `è®¤çŸ¥ä»£ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æƒ…æ„Ÿç ”ç©¶ä¾èµ–äººç±»è‡ªæŠ¥å‘Šï¼Œä½†å…¶é¢„æµ‹å¤§è„‘æ´»åŠ¨çš„èƒ½åŠ›æœ‰é™ï¼Œå­˜åœ¨â€œè¡Œä¸º-ç¥ç»å·®è·â€ã€‚
2. åˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰å’Œçº¯è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œå¤§è§„æ¨¡ç›¸ä¼¼æ€§åˆ¤æ–­ï¼Œæ„å»ºæƒ…æ„Ÿè¡¨å¾ã€‚
3. MLLMçš„æƒ…æ„Ÿè¡¨å¾èƒ½æ›´å‡†ç¡®åœ°é¢„æµ‹äººç±»æƒ…æ„Ÿå¤„ç†ç½‘ç»œä¸­çš„ç¥ç»æ´»åŠ¨ï¼Œä¼˜äºLLMå’Œäººç±»è‡ªæŠ¥å‘Šã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æƒ…æ„Ÿè¡¨å¾åœ¨äººç±»è®¤çŸ¥å’Œç¤¾ä¼šäº’åŠ¨ä¸­è‡³å…³é‡è¦ï¼Œä½†æƒ…æ„Ÿç©ºé—´çš„é«˜ç»´å‡ ä½•ç»“æ„åŠå…¶ç¥ç»åŸºç¡€ä»å­˜åœ¨äº‰è®®ã€‚ä¸€ä¸ªå…³é”®æŒ‘æˆ˜æ˜¯â€œè¡Œä¸º-ç¥ç»å·®è·â€ï¼Œå³äººç±»è‡ªæŠ¥å‘Šé¢„æµ‹å¤§è„‘æ´»åŠ¨çš„èƒ½åŠ›æœ‰é™ã€‚æœ¬æ–‡éªŒè¯äº†è¿™ä¸€å·®è·æºäºä¼ ç»Ÿè¯„çº§é‡è¡¨çš„çº¦æŸï¼Œè€Œå¤§è§„æ¨¡ç›¸ä¼¼æ€§åˆ¤æ–­èƒ½æ›´çœŸå®åœ°æ•æ‰å¤§è„‘çš„æƒ…æ„Ÿå‡ ä½•ã€‚ç ”ç©¶ä½¿ç”¨AIæ¨¡å‹ä½œä¸ºâ€œè®¤çŸ¥ä»£ç†â€ï¼Œä»å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹(MLLM)å’Œçº¯è¯­è¨€æ¨¡å‹(LLM)æ”¶é›†äº†æ•°ç™¾ä¸‡ä¸ªä¸‰å…ƒç»„å¥‡å¶åˆ¤æ–­ï¼Œä»¥å“åº”2180ä¸ªæƒ…æ„Ÿè§†é¢‘ã€‚ç»“æœè¡¨æ˜ï¼Œè¿™äº›æ¨¡å‹äº§ç”Ÿçš„30ç»´åµŒå…¥å…·æœ‰é«˜åº¦å¯è§£é‡Šæ€§ï¼Œå¹¶ä¸»è¦æ²¿ç±»åˆ«çº¿ç»„ç»‡æƒ…æ„Ÿï¼ŒåŒæ—¶ä»¥èåˆçš„æ–¹å¼ç»“åˆäº†ç»´åº¦å±æ€§ã€‚æœ€é‡è¦çš„æ˜¯ï¼ŒMLLMçš„è¡¨å¾ä»¥æœ€é«˜çš„å‡†ç¡®åº¦é¢„æµ‹äº†äººç±»æƒ…æ„Ÿå¤„ç†ç½‘ç»œä¸­çš„ç¥ç»æ´»åŠ¨ï¼Œä¸ä»…ä¼˜äºLLMï¼Œè€Œä¸”å‡ºä¹æ„æ–™åœ°ä¼˜äºç›´æ¥ä»äººç±»è¡Œä¸ºè¯„çº§ä¸­è·å¾—çš„è¡¨å¾ã€‚è¯¥ç»“æœæ”¯æŒäº†ä¸»è¦å‡è®¾ï¼Œå¹¶è¡¨æ˜æ„Ÿå®˜åŸºç¡€â€”â€”ä»ä¸°å¯Œçš„è§†è§‰æ•°æ®ä¸­å­¦ä¹ â€”â€”å¯¹äºå¼€å‘çœŸæ­£ç¥ç»å¯¹é½çš„æƒ…æ„Ÿæ¦‚å¿µæ¡†æ¶è‡³å…³é‡è¦ã€‚ç ”ç©¶ç»“æœæä¾›äº†ä»¤äººä¿¡æœçš„è¯æ®ï¼Œè¡¨æ˜MLLMå¯ä»¥è‡ªä¸»å¼€å‘ä¸°å¯Œçš„ã€ç¥ç»å¯¹é½çš„æƒ…æ„Ÿè¡¨å¾ï¼Œä¸ºå¼¥åˆä¸»è§‚ä½“éªŒä¸å…¶ç¥ç»åŸºè´¨ä¹‹é—´çš„å·®è·æä¾›äº†ä¸€ä¸ªå¼ºå¤§çš„èŒƒä¾‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æƒ…æ„Ÿç ”ç©¶ä¸»è¦ä¾èµ–äººç±»çš„è‡ªæˆ‘æŠ¥å‘Šï¼Œä¾‹å¦‚ä½¿ç”¨è¯„çº§é‡è¡¨ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œæ— æ³•å‡†ç¡®åæ˜ å¤§è„‘ä¸­æƒ…æ„Ÿçš„çœŸå®è¡¨å¾ï¼Œå¯¼è‡´â€œè¡Œä¸º-ç¥ç»å·®è·â€ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§æ–°çš„æ–¹æ³•æ¥æ›´æœ‰æ•ˆåœ°æ•æ‰å¤§è„‘çš„æƒ…æ„Ÿå‡ ä½•ç»“æ„ï¼Œå¹¶å¼¥åˆä¸»è§‚ä½“éªŒå’Œç¥ç»æ´»åŠ¨ä¹‹é—´çš„å·®è·ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨AIæ¨¡å‹ï¼ˆç‰¹åˆ«æ˜¯å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹MLLMï¼‰ä½œä¸ºâ€œè®¤çŸ¥ä»£ç†â€ï¼Œé€šè¿‡å¤§è§„æ¨¡çš„ä¸‰å…ƒç»„å¥‡å¶åˆ¤æ–­ä»»åŠ¡æ¥å­¦ä¹ æƒ…æ„Ÿè¡¨å¾ã€‚è¿™ç§æ–¹æ³•é¿å…äº†ä¼ ç»Ÿè¯„çº§é‡è¡¨çš„é™åˆ¶ï¼Œèƒ½å¤Ÿæ›´å…¨é¢åœ°æ•æ‰æƒ…æ„Ÿçš„å¤æ‚æ€§å’Œç»†å¾®å·®åˆ«ã€‚é€šè¿‡æ¯”è¾ƒMLLMå’ŒLLMçš„è¡¨ç°ï¼Œä»¥åŠå®ƒä»¬ä¸äººç±»ç¥ç»æ´»åŠ¨çš„ç›¸å…³æ€§ï¼Œæ¥éªŒè¯æ„Ÿå®˜åŸºç¡€å¯¹äºæ„å»ºç¥ç»å¯¹é½çš„æƒ…æ„Ÿè¡¨å¾çš„é‡è¦æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦é˜¶æ®µï¼š1) æ”¶é›†æƒ…æ„Ÿè§†é¢‘æ•°æ®é›†ï¼›2) ä½¿ç”¨MLLMå’ŒLLMå¯¹è§†é¢‘è¿›è¡Œä¸‰å…ƒç»„å¥‡å¶åˆ¤æ–­ï¼Œç”Ÿæˆæƒ…æ„Ÿç›¸ä¼¼æ€§æ•°æ®ï¼›3) åˆ©ç”¨è¿™äº›æ•°æ®è®­ç»ƒæƒ…æ„Ÿè¡¨å¾æ¨¡å‹ï¼Œå¾—åˆ°æƒ…æ„ŸåµŒå…¥ï¼›4) å°†æ¨¡å‹ç”Ÿæˆçš„æƒ…æ„Ÿè¡¨å¾ä¸äººç±»å¤§è„‘çš„ç¥ç»æ´»åŠ¨æ•°æ®è¿›è¡Œæ¯”è¾ƒï¼Œè¯„ä¼°å…¶é¢„æµ‹ç¥ç»æ´»åŠ¨çš„èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºä½¿ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ¥å­¦ä¹ æƒ…æ„Ÿè¡¨å¾ï¼Œå¹¶è¯æ˜å…¶èƒ½å¤Ÿè¶…è¶Šäººç±»è‡ªæŠ¥å‘Šï¼Œæ›´å‡†ç¡®åœ°é¢„æµ‹å¤§è„‘çš„ç¥ç»æ´»åŠ¨ã€‚è¿™ç§æ–¹æ³•çªç ´äº†ä¼ ç»Ÿæƒ…æ„Ÿç ”ç©¶çš„å±€é™æ€§ï¼Œä¸ºç†è§£æƒ…æ„Ÿçš„ç¥ç»åŸºç¡€æä¾›äº†ä¸€ç§æ–°çš„è§†è§’ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä½¿ç”¨äº†2180ä¸ªæƒ…æ„Ÿè§†é¢‘ä½œä¸ºè¾“å…¥ï¼Œå¹¶è®¾è®¡äº†ä¸‰å…ƒç»„å¥‡å¶åˆ¤æ–­ä»»åŠ¡ï¼Œè¦æ±‚æ¨¡å‹åˆ¤æ–­å“ªä¸ªè§†é¢‘ä¸å…¶ä»–ä¸¤ä¸ªè§†é¢‘çš„æƒ…æ„Ÿå·®å¼‚æœ€å¤§ã€‚é€šè¿‡æ”¶é›†æ•°ç™¾ä¸‡ä¸ªè¿™æ ·çš„åˆ¤æ–­ï¼Œæ„å»ºäº†å¤§è§„æ¨¡çš„æƒ…æ„Ÿç›¸ä¼¼æ€§æ•°æ®é›†ã€‚æ¨¡å‹æœ€ç»ˆç”Ÿæˆ30ç»´çš„æƒ…æ„ŸåµŒå…¥ï¼Œç”¨äºä¸äººç±»å¤§è„‘çš„ç¥ç»æ´»åŠ¨æ•°æ®è¿›è¡Œæ¯”è¾ƒã€‚æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç­‰å…·ä½“æŠ€æœ¯ç»†èŠ‚åœ¨è®ºæ–‡ä¸­å¯èƒ½æœªè¯¦ç»†æè¿°ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMLLMç”Ÿæˆçš„æƒ…æ„Ÿè¡¨å¾èƒ½å¤Ÿä»¥æœ€é«˜çš„å‡†ç¡®åº¦é¢„æµ‹äººç±»æƒ…æ„Ÿå¤„ç†ç½‘ç»œä¸­çš„ç¥ç»æ´»åŠ¨ï¼Œä¼˜äºçº¯è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œäººç±»è‡ªæŠ¥å‘Šã€‚è¿™è¡¨æ˜å¤šæ¨¡æ€ä¿¡æ¯å¯¹äºæ„å»ºç¥ç»å¯¹é½çš„æƒ…æ„Ÿè¡¨å¾è‡³å…³é‡è¦ï¼Œå¹¶éªŒè¯äº†è®ºæ–‡çš„ä¸»è¦å‡è®¾ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæƒ…æ„Ÿè®¡ç®—ã€äººæœºäº¤äº’ã€å¿ƒç†å¥åº·è¯„ä¼°ç­‰é¢†åŸŸã€‚é€šè¿‡æ›´å‡†ç¡®åœ°ç†è§£å’Œé¢„æµ‹äººç±»æƒ…æ„Ÿï¼Œå¯ä»¥å¼€å‘æ›´æ™ºèƒ½ã€æ›´äººæ€§åŒ–çš„AIç³»ç»Ÿï¼Œä¾‹å¦‚æƒ…æ„Ÿè¯†åˆ«åŠ©æ‰‹ã€ä¸ªæ€§åŒ–æ¨èç³»ç»Ÿå’Œå¿ƒç†å¥åº·å¹²é¢„å·¥å…·ã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶æœ‰æœ›ä¿ƒè¿›å¯¹æƒ…æ„Ÿéšœç¢çš„è¯Šæ–­å’Œæ²»ç–—ï¼Œå¹¶æå‡äººä¸æœºå™¨ä¹‹é—´çš„æƒ…æ„Ÿäº¤æµã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The ability to represent emotion plays a significant role in human cognition and social interaction, yet the high-dimensional geometry of this affective space and its neural underpinnings remain debated. A key challenge, the `behavior-neural gap,' is the limited ability of human self-reports to predict brain activity. Here we test the hypothesis that this gap arises from the constraints of traditional rating scales and that large-scale similarity judgments can more faithfully capture the brain's affective geometry. Using AI models as `cognitive agents,' we collected millions of triplet odd-one-out judgments from a multimodal large language model (MLLM) and a language-only model (LLM) in response to 2,180 emotionally evocative videos. We found that the emergent 30-dimensional embeddings from these models are highly interpretable and organize emotion primarily along categorical lines, yet in a blended fashion that incorporates dimensional properties. Most remarkably, the MLLM's representation predicted neural activity in human emotion-processing networks with the highest accuracy, outperforming not only the LLM but also, counterintuitively, representations derived directly from human behavioral ratings. This result supports our primary hypothesis and suggests that sensory grounding--learning from rich visual data--is critical for developing a truly neurally-aligned conceptual framework for emotion. Our findings provide compelling evidence that MLLMs can autonomously develop rich, neurally-aligned affective representations, offering a powerful paradigm to bridge the gap between subjective experience and its neural substrates. Project page: https://reedonepeck.github.io/ai-emotion.github.io/.

