---
layout: default
title: Radiology's Last Exam (RadLE): Benchmarking Frontier Multimodal AI Against Human Experts and a Taxonomy of Visual Reasoning Errors in Radiology
---

# Radiology's Last Exam (RadLE): Benchmarking Frontier Multimodal AI Against Human Experts and a Taxonomy of Visual Reasoning Errors in Radiology

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25559" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.25559v1</a>
  <a href="https://arxiv.org/pdf/2509.25559.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25559v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25559v1', 'Radiology\'s Last Exam (RadLE): Benchmarking Frontier Multimodal AI Against Human Experts and a Taxonomy of Visual Reasoning Errors in Radiology')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Suvrankar Datta, Divya Buchireddygari, Lakshmi Vennela Chowdary Kaza, Mrudula Bhalke, Kautik Singh, Ayush Pandey, Sonit Sai Vasipalli, Upasana Karnwal, Hakikat Bir Singh Bhatti, Bhavya Ratan Maroo, Sanjana Hebbar, Rahul Joseph, Gurkawal Kaur, Devyani Singh, Akhil V, Dheeksha Devasya Shama Prasad, Nishtha Mahajan, Ayinaparthi Arisha, Rajesh Vanagundi, Reet Nandy, Kartik Vuthoo, Snigdhaa Rajvanshi, Nikhileswar Kondaveeti, Suyash Gunjal, Rishabh Jain, Rajat Jain, Anurag Agrawal

**ÂàÜÁ±ª**: cs.AI, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-29

**Â§áÊ≥®**: 29 pages, 7 figures, 7 tables, includes Annexure (1). Part of the work accepted at RSNA 2025 (Cutting Edge Oral Presentation)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**RadLEÔºöÊîæÂ∞ÑÂ≠¶‰∏ìÂÆ∂Á∫ßËØäÊñ≠Âü∫ÂáÜÔºåËØÑ‰º∞Â§öÊ®°ÊÄÅAIÂπ∂ÂàÜÊûêËßÜËßâÊé®ÁêÜÈîôËØØ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÊîæÂ∞ÑÂ≠¶` `Â§öÊ®°ÊÄÅAI` `ËßÜËßâÊé®ÁêÜ` `ËØäÊñ≠Âü∫ÂáÜ` `ÂåªÂ≠¶ÂΩ±ÂÉè` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `‰∏ìÂÆ∂Á≥ªÁªü`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâAIÂåªÂ≠¶ÂΩ±ÂÉèËØÑ‰º∞Â§öÈõÜ‰∏≠‰∫éÂ∏∏ËßÅÁóÖÁêÜÁöÑÂÖ¨ÂÖ±Êï∞ÊçÆÈõÜÔºåÁº∫‰πèÂØπÂõ∞ÈöæËØäÊñ≠Ê°à‰æãÁöÑ‰∏•Ê†ºËØÑ‰º∞„ÄÇ
2. ÊûÑÂª∫RadLEÂü∫ÂáÜÔºåÈÄöËøáÊ®°ÊãüÁúüÂÆû‰ΩøÁî®Âú∫ÊôØÔºåÂØπÊØîÂâçÊ≤øAIÊ®°Âûã‰∏éÊîæÂ∞ÑÁßëÂåªÁîüÂú®‰∏ìÂÆ∂Á∫ßËØäÊñ≠‰∏≠ÁöÑË°®Áé∞„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåÂâçÊ≤øAIÊ®°ÂûãÂú®Â§çÊùÇËØäÊñ≠‰∏≠ËøúÈÄä‰∫éÊîæÂ∞ÑÁßëÂåªÁîüÔºåÂπ∂ÂàÜÊûê‰∫ÜAIËßÜËßâÊé®ÁêÜÁöÑÈîôËØØÁ±ªÂûã„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨Á†îÁ©∂ÂºÄÂèë‰∫Ü‰∏Ä‰∏™ÂåÖÂê´50‰∏™‰∏ìÂÆ∂Á∫ß‚ÄúÂç≥Êó∂ËØäÊñ≠‚ÄùÊ°à‰æãÁöÑÊîæÂ∞ÑÂ≠¶Âü∫ÂáÜ(RadLE)ÔºåÊ∂µÁõñÂ§öÁßçÂΩ±ÂÉèÊ®°ÊÄÅÔºåÊó®Âú®ËØÑ‰º∞ÂâçÊ≤øAIÊ®°Âûã‰∏éËÆ§ËØÅÊîæÂ∞ÑÁßëÂåªÁîüÂíåÊîæÂ∞ÑÂ≠¶Â≠¶ÂëòÁöÑËØäÊñ≠ËÉΩÂäõ„ÄÇÈÄöËøáÂéüÁîüWebÁïåÈù¢ÊµãËØï‰∫Ü‰∫îÁßçÊµÅË°åÁöÑÂâçÊ≤øAIÊ®°ÂûãÔºàOpenAI o3„ÄÅOpenAI GPT-5„ÄÅGemini 2.5 Pro„ÄÅGrok-4ÂíåClaude Opus 4.1ÔºâÁöÑÊé®ÁêÜÊ®°ÂºèÔºåÊ®°ÊãüÁúüÂÆû‰∏ñÁïåÁöÑ‰ΩøÁî®Âú∫ÊôØ„ÄÇÁî±Áõ≤Ê≥ï‰∏ìÂÆ∂ÂØπÂáÜÁ°ÆÊÄßËøõË°åËØÑÂàÜÔºåÂπ∂ÈÄöËøá‰∏âÊ¨°Áã¨Á´ãËøêË°åËØÑ‰º∞ÂèØÈáçÂ§çÊÄß„ÄÇGPT-5ËøòÊé•Âèó‰∫ÜÂêÑÁßçÊé®ÁêÜÊ®°ÂºèÁöÑËØÑ‰º∞„ÄÇÁ†îÁ©∂ÂÆö‰πâ‰∫ÜËßÜËßâÊé®ÁêÜÈîôËØØÁöÑÂàÜÁ±ªÔºåÂπ∂ËØÑ‰º∞‰∫ÜÊé®ÁêÜË¥®Èáè„ÄÇÁªìÊûúË°®ÊòéÔºåËÆ§ËØÅÊîæÂ∞ÑÁßëÂåªÁîüÁöÑËØäÊñ≠ÂáÜÁ°ÆÁéáÊúÄÈ´òÔºà83%ÔºâÔºå‰ºò‰∫éÂ≠¶ÂëòÔºà45%ÔºâÂíåÊâÄÊúâAIÊ®°ÂûãÔºàGPT-5ÁöÑÊúÄ‰Ω≥ÊÄßËÉΩ‰∏∫30%Ôºâ„ÄÇGPT-5Âíåo3ÁöÑÂèØÈù†ÊÄßËæÉÈ´òÔºåGemini 2.5 ProÂíåGrok-4ÁöÑÂèØÈù†ÊÄß‰∏≠Á≠âÔºåClaude Opus 4.1ÁöÑÂèØÈù†ÊÄßËæÉÂ∑Æ„ÄÇÁ†îÁ©∂ÁªìÊûúË°®ÊòéÔºåÂú®ÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑËØäÊñ≠Ê°à‰æã‰∏≠ÔºåÂÖàËøõÁöÑÂâçÊ≤øÊ®°Âûã‰∏éÊîæÂ∞ÑÁßëÂåªÁîüÁõ∏ÊØî‰ªçÊúâÂæàÂ§ßÂ∑ÆË∑ù„ÄÇËØ•Âü∫ÂáÜÁ™ÅÂá∫‰∫ÜÈÄöÁî®AIÂú®ÂåªÂ≠¶ÂΩ±ÂÉèÊñπÈù¢ÁöÑÂ±ÄÈôêÊÄßÔºåÂπ∂Ë≠¶Âëä‰∏çË¶ÅÂú®Êó†ÁõëÁù£ÁöÑÊÉÖÂÜµ‰∏ãËøõË°å‰∏¥Â∫ä‰ΩøÁî®„ÄÇÊ≠§Â§ñÔºåËøòÂØπÊé®ÁêÜËΩ®ËøπËøõË°å‰∫ÜÂÆöÊÄßÂàÜÊûêÔºåÂπ∂ÊèêÂá∫‰∫ÜAIÊ®°ÂûãËßÜËßâÊé®ÁêÜÈîôËØØÁöÑÂÆûÁî®ÂàÜÁ±ªÔºå‰ª•‰æøÊõ¥Â•ΩÂú∞ÁêÜËß£ÂÖ∂Â§±ÊïàÊ®°ÂºèÔºå‰∏∫ËØÑ‰º∞Ê†áÂáÜÊèê‰æõ‰ø°ÊÅØÔºåÂπ∂ÊåáÂØºÊõ¥Âº∫Â§ßÁöÑÊ®°ÂûãÂºÄÂèë„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥ÈÄöÁî®Â§öÊ®°ÊÄÅAIÊ®°ÂûãÂú®Â§çÊùÇÊîæÂ∞ÑÂ≠¶ËØäÊñ≠‰ªªÂä°‰∏≠Ë°®Áé∞‰∏çË∂≥ÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ï‰∏ªË¶Å‰æùËµñ‰∫éÂÖ¨ÂÖ±Êï∞ÊçÆÈõÜÔºåËøô‰∫õÊï∞ÊçÆÈõÜÈÄöÂ∏∏ÂåÖÂê´Â∏∏ËßÅÁóÖÁêÜÔºåÊó†Ê≥ïÂÖÖÂàÜËØÑ‰º∞Ê®°ÂûãÂú®Â§ÑÁêÜÁΩïËßÅÊàñÂ§çÊùÇÁóÖ‰æãÊó∂ÁöÑËÉΩÂäõ„ÄÇÊ≠§Â§ñÔºåÁé∞ÊúâËØÑ‰º∞ÊñπÊ≥ïÁº∫‰πèÂØπAIÊ®°ÂûãÊé®ÁêÜËøáÁ®ãÁöÑÊ∑±ÂÖ•ÂàÜÊûêÔºåÈöæ‰ª•‰∫ÜËß£ÂÖ∂Â§±ÊïàÊ®°Âºè„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÊûÑÂª∫‰∏Ä‰∏™Êõ¥ÂÖ∑ÊåëÊàòÊÄßÁöÑÊîæÂ∞ÑÂ≠¶ËØäÊñ≠Âü∫ÂáÜ(RadLE)ÔºåËØ•Âü∫ÂáÜÂåÖÂê´‰∏ìÂÆ∂Á∫ßÁöÑ‚ÄúÂç≥Êó∂ËØäÊñ≠‚ÄùÊ°à‰æãÔºåÊ∂µÁõñÂ§öÁßçÂΩ±ÂÉèÊ®°ÊÄÅ„ÄÇÈÄöËøáÂ∞ÜÂâçÊ≤øAIÊ®°Âûã‰∏éÊîæÂ∞ÑÁßëÂåªÁîüÂíåÂ≠¶ÂëòÁöÑËØäÊñ≠ÁªìÊûúËøõË°åÂØπÊØîÔºåÂèØ‰ª•Êõ¥ÂáÜÁ°ÆÂú∞ËØÑ‰º∞AIÊ®°ÂûãÂú®ÂÆûÈôÖ‰∏¥Â∫äÂ∫îÁî®‰∏≠ÁöÑÊΩúÂäõ„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáËøòÂØπAIÊ®°ÂûãÁöÑÊé®ÁêÜËøáÁ®ãËøõË°å‰∫ÜÂÆöÊÄßÂàÜÊûêÔºåÂπ∂ÊèêÂá∫‰∫Ü‰∏Ä‰∏™ËßÜËßâÊé®ÁêÜÈîôËØØÁöÑÂàÜÁ±ªÔºå‰ª•‰æøÊõ¥Â•ΩÂú∞ÁêÜËß£ÂÖ∂Â§±ÊïàÊ®°Âºè„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËØ•Á†îÁ©∂ÁöÑÊäÄÊúØÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÊã¨‰ª•‰∏ãÂá†‰∏™ÈÉ®ÂàÜÔºö1)ÊûÑÂª∫RadLEÂü∫ÂáÜÔºöÊî∂ÈõÜ50‰∏™‰∏ìÂÆ∂Á∫ßÁöÑ‚ÄúÂç≥Êó∂ËØäÊñ≠‚ÄùÊ°à‰æãÔºåÊ∂µÁõñÂ§öÁßçÂΩ±ÂÉèÊ®°ÊÄÅ„ÄÇ2)ÈÄâÊã©ÂâçÊ≤øAIÊ®°ÂûãÔºöÈÄâÊã©‰∫îÁßçÊµÅË°åÁöÑÂâçÊ≤øAIÊ®°ÂûãÔºàOpenAI o3„ÄÅOpenAI GPT-5„ÄÅGemini 2.5 Pro„ÄÅGrok-4ÂíåClaude Opus 4.1ÔºâËøõË°åËØÑ‰º∞„ÄÇ3)ËØÑ‰º∞ËØäÊñ≠ÂáÜÁ°ÆÊÄßÔºöÁî±Áõ≤Ê≥ï‰∏ìÂÆ∂ÂØπAIÊ®°ÂûãÂíå‰∫∫Á±ª‰∏ìÂÆ∂ÁöÑËØäÊñ≠ÁªìÊûúËøõË°åËØÑÂàÜ„ÄÇ4)ËØÑ‰º∞ÂèØÈù†ÊÄßÔºöÈÄöËøá‰∏âÊ¨°Áã¨Á´ãËøêË°åËØÑ‰º∞AIÊ®°ÂûãËØäÊñ≠ÁªìÊûúÁöÑÂèØÈáçÂ§çÊÄß„ÄÇ5)ÂàÜÊûêÊé®ÁêÜËøáÁ®ãÔºöÂØπAIÊ®°ÂûãÁöÑÊé®ÁêÜËΩ®ËøπËøõË°åÂÆöÊÄßÂàÜÊûêÔºåÂπ∂ÊèêÂá∫‰∏Ä‰∏™ËßÜËßâÊé®ÁêÜÈîôËØØÁöÑÂàÜÁ±ª„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•ËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞ÁÇπÂú®‰∫éÔºö1)ÊûÑÂª∫‰∫Ü‰∏Ä‰∏™Êõ¥ÂÖ∑ÊåëÊàòÊÄßÁöÑÊîæÂ∞ÑÂ≠¶ËØäÊñ≠Âü∫ÂáÜ(RadLE)ÔºåËØ•Âü∫ÂáÜÂåÖÂê´‰∏ìÂÆ∂Á∫ßÁöÑ‚ÄúÂç≥Êó∂ËØäÊñ≠‚ÄùÊ°à‰æãÔºåÊ∂µÁõñÂ§öÁßçÂΩ±ÂÉèÊ®°ÊÄÅ„ÄÇ2)ÂØπAIÊ®°ÂûãÁöÑÊé®ÁêÜËøáÁ®ãËøõË°å‰∫ÜÂÆöÊÄßÂàÜÊûêÔºåÂπ∂ÊèêÂá∫‰∫Ü‰∏Ä‰∏™ËßÜËßâÊé®ÁêÜÈîôËØØÁöÑÂàÜÁ±ªÔºå‰ª•‰æøÊõ¥Â•ΩÂú∞ÁêÜËß£ÂÖ∂Â§±ÊïàÊ®°Âºè„ÄÇ3)ÈÄöËøáÊ®°ÊãüÁúüÂÆû‰∏ñÁïåÁöÑ‰ΩøÁî®Âú∫ÊôØÔºåÊõ¥ÂáÜÁ°ÆÂú∞ËØÑ‰º∞‰∫ÜÂâçÊ≤øAIÊ®°ÂûãÂú®ÂÆûÈôÖ‰∏¥Â∫äÂ∫îÁî®‰∏≠ÁöÑÊΩúÂäõ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂÆûÈ™åËÆæËÆ°ÊñπÈù¢ÔºåËÆ∫ÊñáÈááÁî®‰∫Ü‰ª•‰∏ãÂÖ≥ÈîÆËÆæËÆ°Ôºö1)Áõ≤Ê≥ïËØÑ‰º∞ÔºöÁî±Áõ≤Ê≥ï‰∏ìÂÆ∂ÂØπAIÊ®°ÂûãÂíå‰∫∫Á±ª‰∏ìÂÆ∂ÁöÑËØäÊñ≠ÁªìÊûúËøõË°åËØÑÂàÜÔºå‰ª•ÈÅøÂÖç‰∏ªËßÇÂÅèÂ∑Æ„ÄÇ2)Áã¨Á´ãËøêË°åÔºöÈÄöËøá‰∏âÊ¨°Áã¨Á´ãËøêË°åËØÑ‰º∞AIÊ®°ÂûãËØäÊñ≠ÁªìÊûúÁöÑÂèØÈáçÂ§çÊÄßÔºå‰ª•Á°Æ‰øùÁªìÊûúÁöÑÂèØÈù†ÊÄß„ÄÇ3)Êé®ÁêÜÊ®°ÂºèÊµãËØïÔºöÈÄöËøáÂéüÁîüWebÁïåÈù¢ÊµãËØïAIÊ®°ÂûãÁöÑÊé®ÁêÜÊ®°ÂºèÔºå‰ª•Ê®°ÊãüÁúüÂÆû‰∏ñÁïåÁöÑ‰ΩøÁî®Âú∫ÊôØ„ÄÇ4)ÈîôËØØÂàÜÁ±ªÔºöÊèêÂá∫‰∫Ü‰∏Ä‰∏™ËßÜËßâÊé®ÁêÜÈîôËØØÁöÑÂàÜÁ±ªÔºåÂåÖÊã¨ÂπªËßâ„ÄÅÂøΩÁï•Áõ∏ÂÖ≥‰ø°ÊÅØ„ÄÅ‰∏çÊ≠£Á°ÆÁöÑÁ©∫Èó¥Êé®ÁêÜÁ≠â„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËÆ§ËØÅÊîæÂ∞ÑÁßëÂåªÁîüÁöÑËØäÊñ≠ÂáÜÁ°ÆÁéáÊúÄÈ´òÔºà83%ÔºâÔºåÊòæËëó‰ºò‰∫éÊîæÂ∞ÑÂ≠¶Â≠¶ÂëòÔºà45%ÔºâÂíåÊâÄÊúâAIÊ®°ÂûãÔºàGPT-5ÁöÑÊúÄ‰Ω≥ÊÄßËÉΩ‰∏∫30%Ôºâ„ÄÇGPT-5Âíåo3ÁöÑÂèØÈù†ÊÄßËæÉÈ´òÔºåGemini 2.5 ProÂíåGrok-4ÁöÑÂèØÈù†ÊÄß‰∏≠Á≠âÔºåClaude Opus 4.1ÁöÑÂèØÈù†ÊÄßËæÉÂ∑Æ„ÄÇËøô‰∫õÊï∞ÊçÆÊ∏ÖÊô∞Âú∞Â±ïÁ§∫‰∫ÜÂΩìÂâçÂâçÊ≤øAIÊ®°ÂûãÂú®Â§çÊùÇÊîæÂ∞ÑÂ≠¶ËØäÊñ≠‰ªªÂä°‰∏≠‰∏é‰∫∫Á±ª‰∏ìÂÆ∂‰πãÈó¥ÁöÑÂ∑ÆË∑ù„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂåªÂ≠¶ÂΩ±ÂÉèAIÊ®°ÂûãÁöÑËØÑ‰º∞ÂíåÊîπËøõÔºåÊåáÂØºÊõ¥ÂèØÈù†ÁöÑ‰∏¥Â∫äËæÖÂä©ËØäÊñ≠Â∑•ÂÖ∑ÁöÑÂºÄÂèë„ÄÇÈÄöËøáÂàÜÊûêAIÁöÑËßÜËßâÊé®ÁêÜÈîôËØØÔºåÂèØ‰ª•ÈíàÂØπÊÄßÂú∞ÊîπËøõÊ®°ÂûãÔºåÊèêÈ´òËØäÊñ≠ÂáÜÁ°ÆÁéáÔºåÂáèÂ∞ëËØØËØäÊºèËØäÔºåÊúÄÁªàÊèêÂçáÂåªÁñóÊúçÂä°Ë¥®Èáè„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Generalist multimodal AI systems such as large language models (LLMs) and vision language models (VLMs) are increasingly accessed by clinicians and patients alike for medical image interpretation through widely available consumer-facing chatbots. Most evaluations claiming expert level performance are on public datasets containing common pathologies. Rigorous evaluation of frontier models on difficult diagnostic cases remains limited. We developed a pilot benchmark of 50 expert-level "spot diagnosis" cases across multiple imaging modalities to evaluate the performance of frontier AI models against board-certified radiologists and radiology trainees. To mirror real-world usage, the reasoning modes of five popular frontier AI models were tested through their native web interfaces, viz. OpenAI o3, OpenAI GPT-5, Gemini 2.5 Pro, Grok-4, and Claude Opus 4.1. Accuracy was scored by blinded experts, and reproducibility was assessed across three independent runs. GPT-5 was additionally evaluated across various reasoning modes. Reasoning quality errors were assessed and a taxonomy of visual reasoning errors was defined. Board-certified radiologists achieved the highest diagnostic accuracy (83%), outperforming trainees (45%) and all AI models (best performance shown by GPT-5: 30%). Reliability was substantial for GPT-5 and o3, moderate for Gemini 2.5 Pro and Grok-4, and poor for Claude Opus 4.1. These findings demonstrate that advanced frontier models fall far short of radiologists in challenging diagnostic cases. Our benchmark highlights the present limitations of generalist AI in medical imaging and cautions against unsupervised clinical use. We also provide a qualitative analysis of reasoning traces and propose a practical taxonomy of visual reasoning errors by AI models for better understanding their failure modes, informing evaluation standards and guiding more robust model development.

