---
layout: default
title: The Era of Real-World Human Interaction: RL from User Conversations
---

# The Era of Real-World Human Interaction: RL from User Conversations

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25137" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25137v1</a>
  <a href="https://arxiv.org/pdf/2509.25137.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25137v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25137v1', 'The Era of Real-World Human Interaction: RL from User Conversations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chuanyang Jin, Jing Xu, Bo Liu, Leitian Tao, Olga Golovneva, Tianmin Shu, Wenting Zhao, Xian Li, Jason Weston

**åˆ†ç±»**: cs.AI, cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºç”¨æˆ·å¯¹è¯çš„å¼ºåŒ–å­¦ä¹ (RLHI)ï¼Œå®ç°ä¸ªæ€§åŒ–å¯¹é½å’ŒæŒç»­æ¨¡å‹æ”¹è¿›ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `äººæœºäº¤äº’` `å¼ºåŒ–å­¦ä¹ ` `å¯¹è¯ç³»ç»Ÿ` `ä¸ªæ€§åŒ–` `ç”¨æˆ·å»ºæ¨¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¯¹è¯æ¨¡å‹ä¾èµ–ä¸“å®¶æ ‡æ³¨åé¦ˆï¼Œæˆæœ¬é«˜ä¸”éš¾ä»¥æŒç»­æ”¹è¿›å’Œä¸ªæ€§åŒ–å¯¹é½ã€‚
2. æå‡ºRLHIæ¡†æ¶ï¼Œç›´æ¥ä»çœŸå®ç”¨æˆ·å¯¹è¯ä¸­å­¦ä¹ ï¼Œé€šè¿‡ç”¨æˆ·å¼•å¯¼é‡å†™å’Œç”¨æˆ·å¥–åŠ±å»ºæ¨¡å®ç°ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒRLHIåœ¨ä¸ªæ€§åŒ–ã€æŒ‡ä»¤éµå¾ªå’Œæ¨ç†åŸºå‡†ä¸Šä¼˜äºåŸºçº¿ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸ºäº†å®ç°æŒç»­çš„æ¨¡å‹æ”¹è¿›å’Œå¤šæ–¹é¢çš„å¯¹é½ï¼Œæœ¬æ–‡æå‡ºæœªæ¥çš„æ¨¡å‹å¿…é¡»ä»è‡ªç„¶çš„äººæœºäº¤äº’ä¸­å­¦ä¹ ã€‚ç°æœ‰çš„å¯¹è¯æ¨¡å‹é€šå¸¸ä½¿ç”¨é¢„å…ˆæ ‡æ³¨çš„ã€ä¸“å®¶ç”Ÿæˆçš„äººå·¥åé¦ˆè¿›è¡Œå¯¹é½ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºâ€œåŸºäºäººæœºäº¤äº’çš„å¼ºåŒ–å­¦ä¹ â€ï¼ˆRLHIï¼‰çš„èŒƒå¼ï¼Œè¯¥èŒƒå¼ç›´æ¥ä»çœŸå®çš„ã€ç”¨æˆ·å‚ä¸çš„å¯¹è¯ä¸­å­¦ä¹ ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸¤ç§äº’è¡¥çš„æ–¹æ³•ï¼šï¼ˆ1ï¼‰å¸¦æœ‰ç”¨æˆ·å¼•å¯¼é‡å†™çš„RLHIï¼Œå®ƒåŸºäºç”¨æˆ·çš„è‡ªç„¶è¯­è¨€åç»­å›å¤æ¥ä¿®æ”¹ä¸ä»¤äººæ»¡æ„çš„æ¨¡å‹è¾“å‡ºï¼›ï¼ˆ2ï¼‰å¸¦æœ‰åŸºäºç”¨æˆ·çš„å¥–åŠ±çš„RLHIï¼Œå®ƒé€šè¿‡ä¸€ä¸ªå¥–åŠ±æ¨¡å‹å­¦ä¹ ï¼Œè¯¥æ¨¡å‹ä»¥ç”¨æˆ·çš„é•¿æœŸäº¤äº’å†å²ï¼ˆç§°ä¸ºè§’è‰² personaï¼‰ä¸ºæ¡ä»¶ã€‚è¿™äº›æ–¹æ³•å…±åŒé€šè¿‡è§’è‰²æ¡ä»¶åå¥½ä¼˜åŒ–å°†é•¿æœŸç”¨æˆ·è§’è‰²ä¸turnçº§åˆ«çš„åå¥½è”ç³»èµ·æ¥ã€‚åœ¨æºè‡ªWildChatçš„å¯¹è¯ä¸Šè®­ç»ƒåï¼Œä¸¤ç§RLHIå˜ä½“åœ¨ä¸ªæ€§åŒ–å’ŒæŒ‡ä»¤éµå¾ªæ–¹é¢å‡ä¼˜äºå¼ºå¤§çš„åŸºçº¿ï¼Œå¹¶ä¸”ç±»ä¼¼çš„åé¦ˆå¢å¼ºäº†æ¨ç†åŸºå‡†çš„æ€§èƒ½ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œæœ‰æœºçš„äººæœºäº¤äº’ä¸ºä¸ªæ€§åŒ–å¯¹é½æä¾›äº†å¯æ‰©å±•çš„ã€æœ‰æ•ˆçš„ç›‘ç£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰å¯¹è¯æ¨¡å‹ä¾èµ–äºé¢„å…ˆæ ‡æ³¨çš„ã€ä¸“å®¶ç”Ÿæˆçš„äººå·¥åé¦ˆè¿›è¡Œå¯¹é½ï¼Œè¿™ç§æ–¹å¼æˆæœ¬é«˜æ˜‚ï¼Œéš¾ä»¥æ‰©å±•åˆ°å¤§è§„æ¨¡çœŸå®ç”¨æˆ·äº¤äº’åœºæ™¯ï¼Œå¹¶ä¸”éš¾ä»¥æ•æ‰ç”¨æˆ·çš„ä¸ªæ€§åŒ–åå¥½ï¼Œä»è€Œé™åˆ¶äº†æ¨¡å‹çš„æŒç»­æ”¹è¿›å’Œå¤šæ–¹é¢å¯¹é½èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šRLHIçš„æ ¸å¿ƒæ€è·¯æ˜¯ç›´æ¥ä»çœŸå®çš„ç”¨æˆ·å¯¹è¯ä¸­å­¦ä¹ ï¼Œå°†ç”¨æˆ·ä¸æ¨¡å‹çš„äº¤äº’è§†ä¸ºå¼ºåŒ–å­¦ä¹ çš„ç¯å¢ƒï¼Œç”¨æˆ·çš„åé¦ˆï¼ˆåŒ…æ‹¬åç»­å›å¤å’Œé•¿æœŸäº¤äº’å†å²ï¼‰ä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œä»è€Œä¼˜åŒ–æ¨¡å‹ç­–ç•¥ï¼Œä½¿å…¶æ›´å¥½åœ°æ»¡è¶³ç”¨æˆ·çš„ä¸ªæ€§åŒ–éœ€æ±‚ã€‚è¿™æ ·å¯ä»¥é¿å…å¯¹å¤§é‡äººå·¥æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼Œå¹¶å®ç°æ¨¡å‹çš„æŒç»­æ”¹è¿›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRLHIåŒ…å«ä¸¤ä¸ªä¸»è¦æ–¹æ³•ï¼šï¼ˆ1ï¼‰RLHI with User-Guided Rewritesï¼šå½“æ¨¡å‹è¾“å‡ºä¸ä»¤äººæ»¡æ„æ—¶ï¼Œåˆ©ç”¨ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€åç»­å›å¤æ¥ä¿®æ”¹æ¨¡å‹è¾“å‡ºï¼Œä»è€Œç›´æ¥å­¦ä¹ ç”¨æˆ·çš„åå¥½ã€‚ï¼ˆ2ï¼‰RLHI with User-Based Rewardsï¼šæ„å»ºä¸€ä¸ªå¥–åŠ±æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä»¥ç”¨æˆ·çš„é•¿æœŸäº¤äº’å†å²ï¼ˆpersonaï¼‰ä¸ºæ¡ä»¶ï¼Œä»è€Œå°†é•¿æœŸç”¨æˆ·è§’è‰²ä¸turnçº§åˆ«çš„åå¥½è”ç³»èµ·æ¥ï¼Œå®ç°ä¸ªæ€§åŒ–çš„åå¥½ä¼˜åŒ–ã€‚æ•´ä½“æµç¨‹æ˜¯ï¼Œæ¨¡å‹ç”Ÿæˆå›å¤ï¼Œç”¨æˆ·è¿›è¡Œäº¤äº’ï¼ŒRLHIåˆ©ç”¨ç”¨æˆ·çš„äº¤äº’ä¿¡æ¯æ›´æ–°æ¨¡å‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šRLHIçš„å…³é”®åˆ›æ–°åœ¨äºå®ƒå°†å¼ºåŒ–å­¦ä¹ åº”ç”¨äºçœŸå®çš„ç”¨æˆ·å¯¹è¯åœºæ™¯ï¼Œå¹¶åˆ©ç”¨ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€åé¦ˆä½œä¸ºå¥–åŠ±ä¿¡å·ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºä¸“å®¶æ ‡æ³¨çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¸åŒï¼ŒRLHIèƒ½å¤Ÿç›´æ¥ä»ç”¨æˆ·çš„çœŸå®äº¤äº’ä¸­å­¦ä¹ ï¼Œä»è€Œæ›´å¥½åœ°æ•æ‰ç”¨æˆ·çš„ä¸ªæ€§åŒ–åå¥½ï¼Œå¹¶å®ç°æ¨¡å‹çš„æŒç»­æ”¹è¿›ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¼•å…¥ç”¨æˆ·personaï¼ŒRLHIèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£ç”¨æˆ·çš„é•¿æœŸéœ€æ±‚ï¼Œä»è€Œç”Ÿæˆæ›´ç¬¦åˆç”¨æˆ·æœŸæœ›çš„å›å¤ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨RLHI with User-Guided Rewritesä¸­ï¼Œå…³é”®åœ¨äºå¦‚ä½•æœ‰æ•ˆåœ°åˆ©ç”¨ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€å›å¤æ¥ä¿®æ”¹æ¨¡å‹è¾“å‡ºã€‚ä¸€ç§å¯èƒ½çš„è®¾è®¡æ˜¯ä½¿ç”¨åºåˆ—åˆ°åºåˆ—æ¨¡å‹ï¼Œå°†åŸå§‹æ¨¡å‹è¾“å‡ºå’Œç”¨æˆ·å›å¤ä½œä¸ºè¾“å…¥ï¼Œç”Ÿæˆä¿®æ”¹åçš„æ¨¡å‹è¾“å‡ºã€‚åœ¨RLHI with User-Based Rewardsä¸­ï¼Œå…³é”®åœ¨äºå¦‚ä½•æ„å»ºä¸€ä¸ªèƒ½å¤Ÿå‡†ç¡®é¢„æµ‹ç”¨æˆ·åå¥½çš„å¥–åŠ±æ¨¡å‹ã€‚ä¸€ç§å¯èƒ½çš„è®¾è®¡æ˜¯ä½¿ç”¨Transformeræ¨¡å‹ï¼Œå°†ç”¨æˆ·personaå’Œæ¨¡å‹è¾“å‡ºä½œä¸ºè¾“å…¥ï¼Œé¢„æµ‹ç”¨æˆ·å¯¹è¯¥è¾“å‡ºçš„åå¥½å¾—åˆ†ã€‚æŸå¤±å‡½æ•°å¯ä»¥ä½¿ç”¨pairwise ranking lossï¼Œé¼“åŠ±æ¨¡å‹ç”Ÿæˆæ›´ç¬¦åˆç”¨æˆ·åå¥½çš„å›å¤ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨WildChatæ•°æ®é›†ä¸Šè®­ç»ƒçš„RLHIå˜ä½“åœ¨ä¸ªæ€§åŒ–å’ŒæŒ‡ä»¤éµå¾ªæ–¹é¢å‡ä¼˜äºå¼ºå¤§çš„åŸºçº¿ã€‚æ­¤å¤–ï¼Œç±»ä¼¼çš„åé¦ˆå¢å¼ºäº†æ¨¡å‹åœ¨æ¨ç†åŸºå‡†ä¸Šçš„æ€§èƒ½ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œæœ‰æœºçš„äººæœºäº¤äº’ä¸ºä¸ªæ€§åŒ–å¯¹é½æä¾›äº†å¯æ‰©å±•çš„ã€æœ‰æ•ˆçš„ç›‘ç£ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

RLHIå¯åº”ç”¨äºå„ç§å¯¹è¯ç³»ç»Ÿï¼Œå¦‚èŠå¤©æœºå™¨äººã€æ™ºèƒ½å®¢æœã€è™šæ‹ŸåŠ©æ‰‹ç­‰ï¼Œä»¥æå‡ç”¨æˆ·ä½“éªŒã€‚é€šè¿‡å­¦ä¹ ç”¨æˆ·çš„ä¸ªæ€§åŒ–åå¥½ï¼ŒRLHIèƒ½å¤Ÿä½¿å¯¹è¯ç³»ç»Ÿç”Ÿæˆæ›´ç¬¦åˆç”¨æˆ·æœŸæœ›çš„å›å¤ï¼Œä»è€Œæé«˜ç”¨æˆ·çš„æ»¡æ„åº¦å’Œå‚ä¸åº¦ã€‚æ­¤å¤–ï¼ŒRLHIè¿˜å¯ä»¥ç”¨äºæ•™è‚²ã€åŒ»ç–—ç­‰é¢†åŸŸï¼Œä¸ºç”¨æˆ·æä¾›ä¸ªæ€§åŒ–çš„å­¦ä¹ å’Œå¥åº·å»ºè®®ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We posit that to achieve continual model improvement and multifaceted alignment, future models must learn from natural human interaction. Current conversational models are aligned using pre-annotated, expert-generated human feedback. In this work, we introduce Reinforcement Learning from Human Interaction (RLHI), a paradigm that learns directly from in-the-wild user conversations. We develop two complementary methods: (1) RLHI with User-Guided Rewrites, which revises unsatisfactory model outputs based on users' natural-language follow-up responses, (2) RLHI with User-Based Rewards, which learns via a reward model conditioned on knowledge of the user's long-term interaction history (termed persona). Together, these methods link long-term user personas to turn-level preferences via persona-conditioned preference optimization. Trained on conversations derived from WildChat, both RLHI variants outperform strong baselines in personalization and instruction-following, and similar feedback enhances performance on reasoning benchmarks. These results suggest organic human interaction offers scalable, effective supervision for personalized alignment.

