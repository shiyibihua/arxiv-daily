---
layout: default
title: Towards Safe Reasoning in Large Reasoning Models via Corrective Intervention
---

# Towards Safe Reasoning in Large Reasoning Models via Corrective Intervention

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.24393" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.24393v1</a>
  <a href="https://arxiv.org/pdf/2509.24393.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.24393v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.24393v1', 'Towards Safe Reasoning in Large Reasoning Models via Corrective Intervention')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yichi Zhang, Yue Ding, Jingwen Yang, Tianwei Luo, Dongbai Li, Ranjie Duan, Qiang Liu, Hang Su, Yinpeng Dong, Jun Zhu

**åˆ†ç±»**: cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºIntervened Preference Optimizationä»¥æå‡å¤§å‹æ¨ç†æ¨¡å‹å®‰å…¨æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹æ¨ç†æ¨¡å‹` `å®‰å…¨æ€§` `æ€ç»´é“¾` `è¿‡ç¨‹ç›‘ç£` `åå¥½ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹æ¨ç†æ¨¡å‹åœ¨å¤æ‚æ¨ç†è¿‡ç¨‹ä¸­å¯èƒ½äº§ç”Ÿæœ‰å®³å†…å®¹ï¼Œå³ä½¿æœ€ç»ˆç»“æœçœ‹ä¼¼å®‰å…¨ï¼Œè¿™å¸¦æ¥äº†å®‰å…¨é£é™©ã€‚
2. è®ºæ–‡æå‡ºIntervened Preference Optimization (IPO)æ–¹æ³•ï¼Œé€šè¿‡å¹²é¢„ä¸å®‰å…¨æ¨ç†æ­¥éª¤ï¼Œå¼•å¯¼æ¨¡å‹ç”Ÿæˆæ›´å®‰å…¨çš„æ¨ç†è¿‡ç¨‹ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒIPOåœ¨jailbreakå’Œå¯¹æŠ—æ€§å®‰å…¨åŸºå‡†æµ‹è¯•ä¸­ï¼Œæ˜¾è‘—é™ä½äº†æœ‰å®³æ€§ï¼Œä¼˜äºç°æœ‰SFTå’ŒRLæ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹æ¨ç†æ¨¡å‹(LRMs)åœ¨è§£å†³å¤æ‚é—®é¢˜æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†å…¶æ€ç»´é“¾(CoT)æ¨ç†å¸¸åŒ…å«æœ‰å®³å†…å®¹ï¼Œå³ä½¿æœ€ç»ˆå›å¤çœ‹èµ·æ¥å®‰å…¨ä¹Ÿå¯èƒ½æŒç»­å­˜åœ¨ã€‚ç°æœ‰æ–¹æ³•å¿½ç•¥äº†å®‰å…¨æ¨ç†çš„ç‹¬ç‰¹é‡è¦æ€§ï¼ŒæŸå®³äº†å…¶å¯ä¿¡åº¦ï¼Œå¹¶åœ¨æ¶æ„ç”¨æˆ·åˆ©ç”¨ä¸å®‰å…¨æ¨ç†æ—¶æ„æˆæ½œåœ¨é£é™©ã€‚æœ¬æ–‡å°†é‡ç‚¹è½¬ç§»åˆ°å¯¹é½æ¨ç†æœ¬èº«çš„å®‰å…¨æ€§ï¼Œå¹¶æ¢ç´¢è¿‡ç¨‹ç›‘ç£ä½œä¸ºè§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œç®€å•åœ°å¥–åŠ±å®‰å…¨æ¨ç†æ˜¯ä¸å¤Ÿçš„ï¼Œå› ä¸ºå…¶rolloutå¤šæ ·æ€§ä½ä¸”è®­ç»ƒä¿¡å·æœ‰é™ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ·±å…¥ç ”ç©¶äº†å®‰å…¨æ¨ç†çš„ç‰¹å¾ï¼Œå¹¶æ­ç¤ºäº†å‡ ä¸ªå…³é”®è§è§£ï¼š1)å®‰å…¨æ¨ç†é€šå¸¸ç”±å‡ ä¸ªå…³é”®çš„å®‰å…¨è§¦å‘æ­¥éª¤å·©å›ºï¼›2)åˆè§„çº¿ç´¢ä¸ä¸å®‰å…¨çš„å»¶ç»­å¯†åˆ‡ç›¸å…³ï¼›3)çº æ­£æ€§å¹²é¢„å¯é åœ°å¼•å¯¼ä¸å®‰å…¨è½¨è¿¹èµ°å‘æ›´å®‰å…¨çš„è½¨è¿¹ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬æå‡ºIntervened Preference Optimization (IPO)ï¼Œä¸€ç§é€šè¿‡ç”¨å®‰å…¨è§¦å‘å™¨æ›¿æ¢åˆè§„æ­¥éª¤å¹¶æ„å»ºå…·æœ‰å¼ºä¿¡å·çš„åå¥½å­¦ä¹ å¯¹æ¥å¼ºåˆ¶æ‰§è¡Œå®‰å…¨æ¨ç†çš„å¯¹é½æ–¹æ³•ã€‚åœ¨jailbreakå’Œå¯¹æŠ—æ€§å®‰å…¨åŸºå‡†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒIPOæ˜¾è‘—æé«˜äº†æ¨ç†å’Œå“åº”çš„æ•´ä½“å®‰å…¨æ€§ï¼Œä¼˜äºåŸºäºSFTå’ŒåŸºäºRLçš„åŸºçº¿ï¼Œæœ‰å®³æ€§ç›¸å¯¹é™ä½äº†30%ä»¥ä¸Šï¼ŒåŒæ—¶ä¿æŒäº†åœ¨å„ç§æ¨ç†ä»»åŠ¡ä¸­çš„å‡ºè‰²æ€§èƒ½ã€‚ç»“æœçªå‡ºäº†æ˜¾å¼å¯¹é½æ¨ç†çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºæ›´å®‰å…¨çš„LRMæä¾›äº†ä¸€æ¡å®ç”¨é€”å¾„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå¤§å‹æ¨ç†æ¨¡å‹è™½ç„¶åœ¨è§£å†³å¤æ‚é—®é¢˜ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†å…¶æ¨ç†è¿‡ç¨‹ï¼ˆchain-of-thought, CoTï¼‰å¯èƒ½åŒ…å«æœ‰å®³ä¿¡æ¯ï¼Œå³ä½¿æœ€ç»ˆè¾“å‡ºæ˜¯å®‰å…¨çš„ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€å¿½ç•¥äº†æ¨ç†è¿‡ç¨‹æœ¬èº«çš„å®‰å…¨ï¼Œå¯¼è‡´æ¨¡å‹å®¹æ˜“å—åˆ°æ¶æ„æ”»å‡»ï¼Œäº§ç”Ÿä¸å®‰å…¨æˆ–æœ‰å®³çš„æ¨ç†è·¯å¾„ã€‚å› æ­¤ï¼Œå¦‚ä½•ä¿è¯æ¨ç†è¿‡ç¨‹çš„å®‰å…¨æ€§æ˜¯ä¸€ä¸ªäºŸå¾…è§£å†³çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¯¹ä¸å®‰å…¨çš„æ¨ç†è¿‡ç¨‹è¿›è¡Œå¹²é¢„ï¼Œå¼•å¯¼æ¨¡å‹ç”Ÿæˆæ›´å®‰å…¨çš„æ¨ç†è½¨è¿¹ã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡è¯†åˆ«ä¸å®‰å…¨æ¨ç†æ­¥éª¤ä¸­çš„â€œåˆè§„çº¿ç´¢â€ï¼Œå¹¶å°†å…¶æ›¿æ¢ä¸ºâ€œå®‰å…¨è§¦å‘å™¨â€ï¼Œä»è€Œæ”¹å˜æ¨ç†æ–¹å‘ï¼Œé¿å…äº§ç”Ÿæœ‰å®³å†…å®¹ã€‚è¿™ç§å¹²é¢„ç­–ç•¥æ—¨åœ¨ä»æ ¹æœ¬ä¸Šæå‡æ¨ç†è¿‡ç¨‹çš„å®‰å…¨æ€§ï¼Œè€Œä¸ä»…ä»…æ˜¯å…³æ³¨æœ€ç»ˆè¾“å‡ºã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šIntervened Preference Optimization (IPO) æ–¹æ³•ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) åˆ†æå®‰å…¨æ¨ç†çš„ç‰¹å¾ï¼Œè¯†åˆ«å®‰å…¨è§¦å‘å™¨å’Œåˆè§„çº¿ç´¢ï¼›2) åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæ£€æµ‹åˆ°åˆè§„çº¿ç´¢æ—¶ï¼Œç”¨å®‰å…¨è§¦å‘å™¨è¿›è¡Œæ›¿æ¢ï¼Œç”Ÿæˆå¹²é¢„åçš„æ¨ç†è½¨è¿¹ï¼›3) ä½¿ç”¨å¹²é¢„å‰åçš„æ¨ç†è½¨è¿¹æ„å»ºåå¥½å­¦ä¹ å¯¹ï¼Œå¹¶åˆ©ç”¨åå¥½ä¼˜åŒ–ç®—æ³•è®­ç»ƒæ¨¡å‹ï¼Œä½¿å…¶å€¾å‘äºç”Ÿæˆæ›´å®‰å…¨çš„æ¨ç†è¿‡ç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šIPO çš„å…³é”®åˆ›æ–°åœ¨äºå…¶å¯¹æ¨ç†è¿‡ç¨‹çš„æ˜¾å¼å¹²é¢„ã€‚ä¸ä»¥å¾€åªå…³æ³¨æœ€ç»ˆè¾“å‡ºå®‰å…¨æ€§çš„æ–¹æ³•ä¸åŒï¼ŒIPO æ·±å…¥åˆ°æ¨ç†çš„ä¸­é—´æ­¥éª¤ï¼Œé€šè¿‡æ›¿æ¢ä¸å®‰å…¨çº¿ç´¢æ¥æ”¹å˜æ¨ç†è½¨è¿¹ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ§åˆ¶æ¨ç†è¿‡ç¨‹ï¼Œä»è€Œæé«˜æ•´ä½“å®‰å…¨æ€§ã€‚æ­¤å¤–ï¼ŒIPO è¿˜åˆ©ç”¨åå¥½å­¦ä¹ ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ åˆ°å®‰å…¨æ¨ç†çš„æ¨¡å¼ï¼Œå¹¶å°†å…¶æ³›åŒ–åˆ°æ–°çš„åœºæ™¯ä¸­ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ IPO ä¸­ï¼Œå…³é”®çš„è®¾è®¡åŒ…æ‹¬ï¼š1) å®‰å…¨è§¦å‘å™¨å’Œåˆè§„çº¿ç´¢çš„å®šä¹‰å’Œè¯†åˆ«æ–¹æ³•ï¼›2) å¹²é¢„ç­–ç•¥çš„é€‰æ‹©ï¼Œä¾‹å¦‚å¦‚ä½•é€‰æ‹©åˆé€‚çš„å®‰å…¨è§¦å‘å™¨æ¥æ›¿æ¢åˆè§„çº¿ç´¢ï¼›3) åå¥½å­¦ä¹ å¯¹çš„æ„å»ºæ–¹å¼ï¼Œå¦‚ä½•ç¡®ä¿åå¥½ä¿¡å·çš„å¼ºåº¦å’Œå‡†ç¡®æ€§ï¼›4) åå¥½ä¼˜åŒ–ç®—æ³•çš„é€‰æ‹©ï¼Œä¾‹å¦‚ä½¿ç”¨ PPO æˆ–å…¶ä»–åˆé€‚çš„ç®—æ³•æ¥è®­ç»ƒæ¨¡å‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒIPO æ–¹æ³•åœ¨ jailbreak å’Œå¯¹æŠ—æ€§å®‰å…¨åŸºå‡†æµ‹è¯•ä¸­ï¼Œç›¸å¯¹äºåŸºäº SFT å’ŒåŸºäº RL çš„åŸºçº¿æ–¹æ³•ï¼Œæœ‰å®³æ€§ç›¸å¯¹é™ä½äº† 30% ä»¥ä¸Šã€‚åŒæ—¶ï¼ŒIPO è¿˜èƒ½ä¿æŒæ¨¡å‹åœ¨å„ç§æ¨ç†ä»»åŠ¡ä¸­çš„å‡ºè‰²æ€§èƒ½ï¼Œè¡¨æ˜è¯¥æ–¹æ³•åœ¨æå‡å®‰å…¨æ€§çš„åŒæ—¶ï¼Œä¸ä¼šæ˜¾è‘—é™ä½æ¨¡å‹çš„é€šç”¨èƒ½åŠ›ã€‚è¿™äº›ç»“æœå……åˆ†è¯æ˜äº† IPO æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦å®‰å…¨æ¨ç†çš„å¤§å‹è¯­è¨€æ¨¡å‹åº”ç”¨åœºæ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½å®¢æœã€å†…å®¹ç”Ÿæˆã€ä»£ç ç”Ÿæˆç­‰ã€‚é€šè¿‡æå‡æ¨ç†è¿‡ç¨‹çš„å®‰å…¨æ€§ï¼Œå¯ä»¥æœ‰æ•ˆé˜²æ­¢æ¨¡å‹ç”Ÿæˆæœ‰å®³ã€ä¸å‡†ç¡®æˆ–å…·æœ‰åè§çš„å†…å®¹ï¼Œä»è€Œæé«˜ç”¨æˆ·ä½“éªŒå’Œé™ä½æ½œåœ¨é£é™©ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›æ¨å¹¿åˆ°æ›´å¹¿æ³›çš„AIç³»ç»Ÿä¸­ï¼Œæ„å»ºæ›´å®‰å…¨ã€å¯é çš„äººå·¥æ™ºèƒ½æœåŠ¡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Although Large Reasoning Models (LRMs) have progressed in solving complex problems, their chain-of-thought (CoT) reasoning often contains harmful content that can persist even when the final responses appear safe. We show that this issue still remains in existing methods which overlook the unique significance of safe reasoning, undermining their trustworthiness and posing potential risks in applications if unsafe reasoning is accessible for and exploited by malicious users. We therefore shift our focus to aligning the safety of reasoning itself in this paper and explore process supervision as the solution. However, simply rewarding safe reasoning proves inadequate due to low rollout diversity and limited training signals. To tackle this challenge, we first delve into the characteristics of safe reasoning and uncover several critical insights that 1) safe reasoning is often consolidated by a few critical steps of safety triggers; 2) compliance cues strongly correlate with unsafe continuations; and 3) corrective interventions reliably steer unsafe trajectories towards safer traces. Motivated by these, we propose Intervened Preference Optimization (IPO), an alignment method that enforces safe reasoning by substituting compliance steps with safety triggers and constructing pairs for preference learning with strong signals. Experiments on jailbreak and adversarial safety benchmarks demonstrate that IPO remarkably improves overall safety regarding both reasoning and responses, outperforming SFT-based and RL-based baselines with a relative reduction of over 30% in harmfulness, while preserving excellent performance across diverse reasoning tasks. The results highlight the importance of explicit alignment for reasoning and provide a practical path to safer LRMs.

