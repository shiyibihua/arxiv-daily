---
layout: default
title: Vision-and-Language Navigation with Analogical Textual Descriptions in LLMs
---

# Vision-and-Language Navigation with Analogical Textual Descriptions in LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25139" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25139v1</a>
  <a href="https://arxiv.org/pdf/2509.25139.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25139v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25139v1', 'Vision-and-Language Navigation with Analogical Textual Descriptions in LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yue Zhang, Tianyi Ma, Zun Wang, Yanyuan Qiao, Parisa Kordjamshidi

**åˆ†ç±»**: cs.AI, cs.CV, cs.MM

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºLLMä¸­ç±»æ¯”æ–‡æœ¬æè¿°çš„è§†è§‰-è¯­è¨€å¯¼èˆªæ–¹æ³•ï¼Œæå‡å¯¼èˆªæ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€å¯¼èˆª` `å¤§å‹è¯­è¨€æ¨¡å‹` `ç±»æ¯”æ¨ç†` `æœºå™¨äººå¯¼èˆª` `ä¸Šä¸‹æ–‡ç†è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºLLMçš„VLNæ™ºèƒ½ä½“åœ¨å¤„ç†è§†è§‰ä¿¡æ¯æ—¶å­˜åœ¨å±€é™æ€§ï¼Œè¦ä¹ˆæŸå¤±è§†è§‰ç»†èŠ‚ï¼Œè¦ä¹ˆæ— æ³•è¿›è¡Œé«˜å±‚æ¬¡è¯­ä¹‰æ¨ç†ã€‚
2. è¯¥è®ºæ–‡çš„æ ¸å¿ƒæ€æƒ³æ˜¯åˆ©ç”¨å¤šè§†è§’çš„æ–‡æœ¬æè¿°ï¼Œé€šè¿‡ç±»æ¯”æ¨ç†å¢å¼ºæ™ºèƒ½ä½“çš„åœºæ™¯ç†è§£å’Œç©ºé—´æ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨R2Ræ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†å¯¼èˆªæ€§èƒ½ï¼ŒéªŒè¯äº†ç±»æ¯”æ¨ç†åœ¨VLNä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ”¹è¿›çš„åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è§†è§‰-è¯­è¨€å¯¼èˆªï¼ˆVLNï¼‰æ™ºèƒ½ä½“ï¼Œæ—¨åœ¨æå‡å…¶ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›ã€‚ç°æœ‰åŸºäºLLMçš„VLNæ–¹æ³•è¦ä¹ˆå°†å›¾åƒç¼–ç ä¸ºæ–‡æœ¬åœºæ™¯æè¿°ï¼Œå¯èƒ½è¿‡åº¦ç®€åŒ–è§†è§‰ç»†èŠ‚ï¼Œè¦ä¹ˆç›´æ¥å¤„ç†åŸå§‹å›¾åƒè¾“å…¥ï¼Œéš¾ä»¥æ•æ‰é«˜å±‚æ¬¡æ¨ç†æ‰€éœ€çš„æŠ½è±¡è¯­ä¹‰ã€‚æœ¬æ–‡é€šè¿‡æ•´åˆæ¥è‡ªå¤šä¸ªè§†è§’çš„æ–‡æœ¬æè¿°ï¼Œä¿ƒè¿›å›¾åƒé—´çš„ç±»æ¯”æ¨ç†ï¼Œä»è€Œå¢å¼ºæ™ºèƒ½ä½“çš„å…¨å±€åœºæ™¯ç†è§£å’Œç©ºé—´æ¨ç†ï¼Œæœ€ç»ˆæé«˜åŠ¨ä½œå†³ç­–çš„å‡†ç¡®æ€§ã€‚åœ¨R2Ræ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¯¼èˆªæ€§èƒ½æ–¹é¢å–å¾—äº†æ˜¾è‘—æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„è§†è§‰-è¯­è¨€å¯¼èˆªï¼ˆVLNï¼‰æ–¹æ³•åœ¨å¤„ç†è§†è§‰ä¿¡æ¯æ—¶å­˜åœ¨ä¸è¶³ã€‚ä¸€äº›æ–¹æ³•å°†å›¾åƒè½¬æ¢ä¸ºæ–‡æœ¬æè¿°ï¼Œè¿™å¯èƒ½ä¼šä¸¢å¤±é‡è¦çš„è§†è§‰ç»†èŠ‚ã€‚å¦ä¸€äº›æ–¹æ³•ç›´æ¥å¤„ç†åŸå§‹å›¾åƒï¼Œä½†ç¼ºä¹æ•æ‰æŠ½è±¡è¯­ä¹‰çš„èƒ½åŠ›ï¼Œå¯¼è‡´éš¾ä»¥è¿›è¡Œé«˜å±‚æ¬¡çš„æ¨ç†ï¼Œä»è€Œå½±å“å¯¼èˆªæ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥å¤šè§†è§’çš„æ–‡æœ¬æè¿°ï¼Œä¿ƒè¿›å›¾åƒä¹‹é—´çš„ç±»æ¯”æ¨ç†ã€‚é€šè¿‡ç±»æ¯”æ¨ç†ï¼Œæ™ºèƒ½ä½“å¯ä»¥æ›´å¥½åœ°ç†è§£å…¨å±€åœºæ™¯å’Œç©ºé—´å…³ç³»ï¼Œä»è€Œåšå‡ºæ›´å‡†ç¡®çš„å¯¼èˆªå†³ç­–ã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨å¼¥åˆè§†è§‰ç»†èŠ‚å’ŒæŠ½è±¡è¯­ä¹‰ä¹‹é—´çš„å·®è·ï¼Œæå‡æ™ºèƒ½ä½“çš„ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•é¦–å…ˆåˆ©ç”¨LLMç”Ÿæˆå¤šè§†è§’çš„æ–‡æœ¬æè¿°ï¼Œç„¶åå°†è¿™äº›æè¿°ç”¨äºç±»æ¯”æ¨ç†ã€‚å…·ä½“æ¥è¯´ï¼Œæ™ºèƒ½ä½“å°†å½“å‰è§‚å¯Ÿåˆ°çš„å›¾åƒä¸å†å²å›¾åƒè¿›è¡Œæ¯”è¾ƒï¼Œå¹¶åˆ©ç”¨æ–‡æœ¬æè¿°æ¥è¯†åˆ«ç›¸ä¼¼ä¹‹å¤„å’Œå·®å¼‚ã€‚åŸºäºè¿™äº›ç±»æ¯”ï¼Œæ™ºèƒ½ä½“å¯ä»¥æ¨æ–­å‡ºä¸‹ä¸€æ­¥åº”è¯¥é‡‡å–çš„è¡ŒåŠ¨ã€‚æ•´ä½“æ¡†æ¶åŒ…å«å›¾åƒç¼–ç ã€æ–‡æœ¬æè¿°ç”Ÿæˆã€ç±»æ¯”æ¨ç†å’ŒåŠ¨ä½œå†³ç­–ç­‰æ¨¡å—ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå°†ç±»æ¯”æ¨ç†å¼•å…¥åˆ°åŸºäºLLMçš„VLNä»»åŠ¡ä¸­ã€‚é€šè¿‡åˆ©ç”¨æ–‡æœ¬æè¿°è¿›è¡Œç±»æ¯”æ¨ç†ï¼Œæ™ºèƒ½ä½“å¯ä»¥æ›´å¥½åœ°ç†è§£åœºæ™¯çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»è€Œåšå‡ºæ›´æ˜æ™ºçš„å¯¼èˆªå†³ç­–ã€‚è¿™ä¸ç›´æ¥ä½¿ç”¨å›¾åƒæˆ–ç®€å•æ–‡æœ¬æè¿°çš„æ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šå…·ä½“çš„å‚æ•°è®¾ç½®ã€æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç­‰æŠ€æœ¯ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æ²¡æœ‰è¯¦ç»†è¯´æ˜ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚ä½†å¯ä»¥æ¨æµ‹ï¼Œæ–‡æœ¬æè¿°çš„è´¨é‡å’Œç±»æ¯”æ¨ç†çš„ç®—æ³•æ˜¯å½±å“æ€§èƒ½çš„å…³é”®å› ç´ ã€‚æœªæ¥çš„ç ”ç©¶å¯ä»¥å…³æ³¨å¦‚ä½•ä¼˜åŒ–æ–‡æœ¬æè¿°çš„ç”Ÿæˆï¼Œä»¥åŠå¦‚ä½•è®¾è®¡æ›´æœ‰æ•ˆçš„ç±»æ¯”æ¨ç†ç®—æ³•ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥æ–¹æ³•åœ¨R2Ræ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¯¼èˆªæ€§èƒ½æ–¹é¢å–å¾—äº†æ˜¾è‘—æå‡ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œå¯¹æ¯”åŸºçº¿åœ¨æ‘˜è¦ä¸­æ²¡æœ‰ç»™å‡ºï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚ä½†å¯ä»¥ç¡®å®šçš„æ˜¯ï¼Œé€šè¿‡å¼•å…¥ç±»æ¯”æ¨ç†ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°æå‡VLNæ™ºèƒ½ä½“çš„å¯¼èˆªèƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ã€è™šæ‹Ÿç°å®ç­‰é¢†åŸŸã€‚é€šè¿‡æå‡æ™ºèƒ½ä½“å¯¹ç¯å¢ƒçš„ç†è§£èƒ½åŠ›ï¼Œå¯ä»¥ä½¿å…¶åœ¨å¤æ‚ç¯å¢ƒä¸­æ›´æœ‰æ•ˆåœ°å®Œæˆå¯¼èˆªä»»åŠ¡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥æ‰©å±•åˆ°å…¶ä»–éœ€è¦ä¸Šä¸‹æ–‡ç†è§£å’Œæ¨ç†çš„ä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚æ™ºèƒ½å®¶å±…ã€æ™ºèƒ½åŠ©æ‰‹ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Integrating large language models (LLMs) into embodied AI models is becoming increasingly prevalent. However, existing zero-shot LLM-based Vision-and-Language Navigation (VLN) agents either encode images as textual scene descriptions, potentially oversimplifying visual details, or process raw image inputs, which can fail to capture abstract semantics required for high-level reasoning. In this paper, we improve the navigation agent's contextual understanding by incorporating textual descriptions from multiple perspectives that facilitate analogical reasoning across images. By leveraging text-based analogical reasoning, the agent enhances its global scene understanding and spatial reasoning, leading to more accurate action decisions. We evaluate our approach on the R2R dataset, where our experiments demonstrate significant improvements in navigation performance.

