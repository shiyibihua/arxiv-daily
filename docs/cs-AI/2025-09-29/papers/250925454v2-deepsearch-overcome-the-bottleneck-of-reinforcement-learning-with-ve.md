---
layout: default
title: DeepSearch: Overcome the Bottleneck of Reinforcement Learning with Verifiable Rewards via Monte Carlo Tree Search
---

# DeepSearch: Overcome the Bottleneck of Reinforcement Learning with Verifiable Rewards via Monte Carlo Tree Search

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.25454" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.25454v2</a>
  <a href="https://arxiv.org/pdf/2509.25454.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.25454v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.25454v2', 'DeepSearch: Overcome the Bottleneck of Reinforcement Learning with Verifiable Rewards via Monte Carlo Tree Search')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Fang Wu, Weihao Xuan, Heli Qi, Ximing Lu, Aaron Tu, Li Erran Li, Yejin Choi

**åˆ†ç±»**: cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-29 (æ›´æ–°: 2025-10-01)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**DeepSearchï¼šé€šè¿‡è’™ç‰¹å¡æ´›æ ‘æœç´¢å’Œå¯éªŒè¯å¥–åŠ±å…‹æœå¼ºåŒ–å­¦ä¹ ç“¶é¢ˆ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `è’™ç‰¹å¡æ´›æ ‘æœç´¢` `å¯éªŒè¯å¥–åŠ±` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨ç†` `æ¢ç´¢` `ä¿¡ç”¨åˆ†é…`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰RLVRæ–¹æ³•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å­˜åœ¨æ¢ç´¢ä¸è¶³çš„é—®é¢˜ï¼Œå¯¼è‡´æ€§èƒ½æå‡åœæ»ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨è®¡ç®—èµ„æºã€‚
2. DeepSearchå°†è’™ç‰¹å¡æ´›æ ‘æœç´¢é›†æˆåˆ°RLVRè®­ç»ƒä¸­ï¼Œé€šè¿‡ç»“æ„åŒ–æœç´¢å®ç°ç³»ç»Ÿæ¢ç´¢å’Œç»†ç²’åº¦ä¿¡ç”¨åˆ†é…ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒDeepSearchåœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå¹¶å¤§å¹…å‡å°‘äº†è®­ç»ƒæ‰€éœ€çš„è®¡ç®—èµ„æºã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡RLVRå·²æˆä¸ºå¼€å‘LLMä¸­é«˜çº§æ¨ç†èƒ½åŠ›çš„å…³é”®ç»„æˆéƒ¨åˆ†ï¼Œä½†ç°æœ‰ç ”ç©¶è¡¨æ˜ï¼Œç»è¿‡æ•°åƒæ¬¡ä¼˜åŒ–æ­¥éª¤åä¼šå‡ºç°è®­ç»ƒåœæ»ï¼Œå°½ç®¡è®¡ç®—æŠ•å…¥å¢åŠ ï¼Œæ€§èƒ½æå‡å´æ˜¾è‘—ä¸‹é™ã€‚è¿™ç§é™åˆ¶æºäºå½“å‰RLVRå®è·µä¸­å›ºæœ‰çš„ç¨€ç–æ¢ç´¢æ¨¡å¼ï¼Œæ¨¡å‹ä¾èµ–äºæœ‰é™çš„rolloutï¼Œè¿™é€šå¸¸ä¼šé”™è¿‡å…³é”®çš„æ¨ç†è·¯å¾„ï¼Œå¹¶ä¸”æ— æ³•ç³»ç»Ÿåœ°è¦†ç›–è§£å†³æ–¹æ¡ˆç©ºé—´ã€‚æˆ‘ä»¬æå‡ºäº†DeepSearchï¼Œä¸€ä¸ªå°†è’™ç‰¹å¡æ´›æ ‘æœç´¢ç›´æ¥é›†æˆåˆ°RLVRè®­ç»ƒä¸­çš„æ¡†æ¶ã€‚ä¸ä»…åœ¨æ¨ç†æ—¶ä¾èµ–æ ‘æœç´¢çš„ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒDeepSearchå°†ç»“æ„åŒ–æœç´¢åµŒå…¥åˆ°è®­ç»ƒå¾ªç¯ä¸­ï¼Œä»è€Œå®ç°ç³»ç»Ÿçš„æ¢ç´¢å’Œè·¨æ¨ç†æ­¥éª¤çš„ç»†ç²’åº¦ä¿¡ç”¨åˆ†é…ã€‚é€šè¿‡è®­ç»ƒæ—¶æ¢ç´¢ï¼ŒDeepSearchè§£å†³äº†æ¢ç´¢ä¸è¶³çš„æ ¹æœ¬ç“¶é¢ˆï¼Œä»è€Œå¯¼è‡´é•¿æ—¶é—´è®­ç»ƒæ­¥éª¤ä¸­æ€§èƒ½æå‡çš„é™ä½ã€‚æˆ‘ä»¬çš„è´¡çŒ®åŒ…æ‹¬ï¼šï¼ˆ1ï¼‰ä¸€ç§å…¨å±€å‰æ²¿é€‰æ‹©ç­–ç•¥ï¼Œä¼˜å…ˆè€ƒè™‘æœç´¢æ ‘ä¸­æœ‰å¸Œæœ›çš„èŠ‚ç‚¹ï¼Œï¼ˆ2ï¼‰åŸºäºç†µå¼•å¯¼çš„é€‰æ‹©ï¼Œç”¨äºè¯†åˆ«æœ‰ä¿¡å¿ƒçš„è·¯å¾„ä»¥è¿›è¡Œç›‘ç£ï¼Œä»¥åŠï¼ˆ3ï¼‰å…·æœ‰è§£å†³æ–¹æ¡ˆç¼“å­˜çš„è‡ªé€‚åº”é‡æ”¾ç¼“å†²åŒºè®­ç»ƒä»¥æé«˜æ•ˆç‡ã€‚åœ¨æ•°å­¦æ¨ç†åŸºå‡†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDeepSearchå®ç°äº†62.95%çš„å¹³å‡å‡†ç¡®ç‡ï¼Œå¹¶ä¸º1.5Bæ¨ç†æ¨¡å‹å»ºç«‹äº†æ–°çš„state-of-the-artï¼Œä½¿ç”¨çš„GPUæ—¶é—´æ¯”æ‰©å±•è®­ç»ƒæ–¹æ³•å°‘5.7å€ã€‚è¿™äº›ç»“æœçªå‡ºäº†æˆ˜ç•¥æ¢ç´¢ç›¸å¯¹äºè›®åŠ›æ‰©å±•çš„é‡è¦æ€§ï¼Œå¹¶è¯æ˜äº†ç®—æ³•åˆ›æ–°åœ¨æ¨è¿›RLVRæ–¹æ³•è®ºæ–¹é¢çš„å¸Œæœ›ã€‚DeepSearchä¸ºé€šè¿‡ç³»ç»Ÿæœç´¢è€Œä¸æ˜¯é•¿æ—¶é—´è®¡ç®—æ¥æ‰©å±•æ¨ç†èƒ½åŠ›å»ºç«‹äº†ä¸€ä¸ªæ–°çš„æ–¹å‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºå¼ºåŒ–å­¦ä¹ çš„å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æ–¹æ³•åœ¨è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¤æ‚æ¨ç†æ—¶ï¼Œé¢ä¸´æ¢ç´¢ç©ºé—´ä¸è¶³çš„é—®é¢˜ã€‚æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®¹æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜ï¼Œéš¾ä»¥å‘ç°æœ‰æ•ˆçš„æ¨ç†è·¯å¾„ï¼Œå¯¼è‡´è®­ç»ƒæ•ˆç‡ä½ä¸‹ï¼Œæ€§èƒ½æå‡ç¼“æ…¢ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºæœ‰é™çš„rolloutï¼Œæ— æ³•ç³»ç»Ÿåœ°è¦†ç›–è§£å†³æ–¹æ¡ˆç©ºé—´ï¼Œé€ æˆäº†è®­ç»ƒç“¶é¢ˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDeepSearchçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ç›´æ¥åµŒå…¥åˆ°RLVRçš„è®­ç»ƒå¾ªç¯ä¸­ã€‚é€šè¿‡åœ¨è®­ç»ƒæ—¶è¿›è¡Œç»“æ„åŒ–æœç´¢ï¼ŒDeepSearchèƒ½å¤Ÿæ›´å…¨é¢åœ°æ¢ç´¢æ¨ç†ç©ºé—´ï¼Œå¹¶ä¸ºæ¯ä¸ªæ¨ç†æ­¥éª¤è¿›è¡Œç»†ç²’åº¦çš„ä¿¡ç”¨åˆ†é…ã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨è§£å†³ç°æœ‰RLVRæ–¹æ³•ä¸­æ¢ç´¢ä¸è¶³çš„é—®é¢˜ï¼Œä»è€Œæé«˜è®­ç»ƒæ•ˆç‡å’Œæ¨¡å‹æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDeepSearchçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼šç”¨äºåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¿›è¡Œç»“æ„åŒ–æ¢ç´¢ï¼›2) å…¨å±€å‰æ²¿é€‰æ‹©ç­–ç•¥ï¼šç”¨äºä¼˜å…ˆé€‰æ‹©æœç´¢æ ‘ä¸­æœ‰å¸Œæœ›çš„èŠ‚ç‚¹è¿›è¡Œæ‰©å±•ï¼›3) åŸºäºç†µå¼•å¯¼çš„é€‰æ‹©ï¼šç”¨äºè¯†åˆ«ç½®ä¿¡åº¦é«˜çš„æ¨ç†è·¯å¾„è¿›è¡Œç›‘ç£å­¦ä¹ ï¼›4) è‡ªé€‚åº”é‡æ”¾ç¼“å†²åŒºï¼šç”¨äºå­˜å‚¨å’Œé‡ç”¨æœ‰æ•ˆçš„æ¨ç†è·¯å¾„ï¼Œæé«˜è®­ç»ƒæ•ˆç‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šDeepSearchçš„å…³é”®åˆ›æ–°åœ¨äºå°†MCTSé›†æˆåˆ°RLVRçš„è®­ç»ƒå¾ªç¯ä¸­ã€‚ä¸ç°æœ‰æ–¹æ³•ä»…åœ¨æ¨ç†æ—¶ä½¿ç”¨æ ‘æœç´¢ä¸åŒï¼ŒDeepSearchåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åˆ©ç”¨MCTSè¿›è¡Œç³»ç»Ÿæ¢ç´¢ï¼Œä»è€Œè§£å†³äº†æ¢ç´¢ä¸è¶³çš„æ ¹æœ¬é—®é¢˜ã€‚æ­¤å¤–ï¼Œå…¨å±€å‰æ²¿é€‰æ‹©ç­–ç•¥å’ŒåŸºäºç†µå¼•å¯¼çš„é€‰æ‹©è¿›ä¸€æ­¥æé«˜äº†æœç´¢æ•ˆç‡å’Œè®­ç»ƒæ•ˆæœã€‚

**å…³é”®è®¾è®¡**ï¼šDeepSearchçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å…¨å±€å‰æ²¿é€‰æ‹©ç­–ç•¥ï¼Œè¯¥ç­–ç•¥åŸºäºèŠ‚ç‚¹çš„åˆ†æ•°å’Œè®¿é—®æ¬¡æ•°æ¥é€‰æ‹©ä¸‹ä¸€ä¸ªè¦æ‰©å±•çš„èŠ‚ç‚¹ï¼›2) åŸºäºç†µå¼•å¯¼çš„é€‰æ‹©ï¼Œè¯¥ç­–ç•¥åˆ©ç”¨æ¨¡å‹å¯¹æ¯ä¸ªæ¨ç†æ­¥éª¤çš„ç½®ä¿¡åº¦æ¥æŒ‡å¯¼æœç´¢æ–¹å‘ï¼›3) è‡ªé€‚åº”é‡æ”¾ç¼“å†²åŒºï¼Œè¯¥ç¼“å†²åŒºæ ¹æ®æ¨ç†è·¯å¾„çš„è´¨é‡åŠ¨æ€è°ƒæ•´å­˜å‚¨æ¦‚ç‡ï¼Œä»è€Œä¼˜å…ˆå­˜å‚¨å’Œé‡ç”¨æœ‰æ•ˆçš„æ¨ç†è·¯å¾„ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

DeepSearchåœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—æˆæœï¼Œå¹³å‡å‡†ç¡®ç‡è¾¾åˆ°62.95%ï¼Œä¸º1.5Bå‚æ•°çš„æ¨ç†æ¨¡å‹å»ºç«‹äº†æ–°çš„state-of-the-artã€‚ä¸ä¼ ç»Ÿçš„æ‰©å±•è®­ç»ƒæ–¹æ³•ç›¸æ¯”ï¼ŒDeepSearchä½¿ç”¨çš„GPUæ—¶é—´å‡å°‘äº†5.7å€ï¼Œè¯æ˜äº†å…¶åœ¨æé«˜è®­ç»ƒæ•ˆç‡æ–¹é¢çš„ä¼˜åŠ¿ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒDeepSearchèƒ½å¤Ÿæœ‰æ•ˆåœ°è§£å†³RLVRè®­ç»ƒä¸­çš„æ¢ç´¢ç“¶é¢ˆï¼Œå¹¶æ˜¾è‘—æå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DeepSearchå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ç”¨äºæå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å„ç§å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œä¾‹å¦‚æ•°å­¦æ¨ç†ã€é€»è¾‘æ¨ç†ã€å¸¸è¯†æ¨ç†ç­‰ã€‚è¯¥æ–¹æ³•è¿˜å¯ä»¥åº”ç”¨äºæœºå™¨äººæ§åˆ¶ã€æ¸¸æˆAIç­‰é¢†åŸŸï¼Œé€šè¿‡ç³»ç»Ÿæ¢ç´¢å’Œç»†ç²’åº¦ä¿¡ç”¨åˆ†é…ï¼Œæé«˜æ™ºèƒ½ä½“çš„å­¦ä¹ æ•ˆç‡å’Œå†³ç­–èƒ½åŠ›ã€‚DeepSearchçš„æˆåŠŸè¡¨æ˜ï¼Œç®—æ³•åˆ›æ–°åœ¨æå‡AIç³»ç»Ÿèƒ½åŠ›æ–¹é¢å…·æœ‰é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Although RLVR has become an essential component for developing advanced reasoning skills in LLMs, contemporary studies have documented training plateaus that emerge following thousands of optimization steps, demonstrating notable decreases in performance gains despite increased computational investment. This limitation stems from the sparse exploration patterns inherent in current RLVR practices, where models rely on limited rollouts that often miss critical reasoning paths and fail to provide systematic coverage of the solution space. We present DeepSearch, a framework that integrates Monte Carlo Tree Search directly into RLVR training. In contrast to existing methods that rely on tree search only at inference, DeepSearch embeds structured search into the training loop, enabling systematic exploration and fine-grained credit assignment across reasoning steps. Through training-time exploration, DeepSearch addresses the fundamental bottleneck of insufficient exploration, which leads to diminishing performance improvements over prolonged training steps. Our contributions include: (1) a global frontier selection strategy that prioritizes promising nodes across the search tree, (2) selection with entropy-based guidance that identifies confident paths for supervision, and (3) adaptive replay buffer training with solution caching for efficiency. Experiments on mathematical reasoning benchmarks show that DeepSearch achieves 62.95% average accuracy and establishes a new state-of-the-art for 1.5B reasoning models - using 5.7x fewer GPU hours than extended training approaches. These results highlight the importance of strategic exploration over brute-force scaling and demonstrate the promise of algorithmic innovation for advancing RLVR methodologies. DeepSearch establishes a new direction for scaling reasoning capabilities through systematic search rather than prolonged computation.

