---
layout: default
title: Refine-POI: Reinforcement Fine-Tuned Large Language Models for Next Point-of-Interest Recommendation
---

# Refine-POI: Reinforcement Fine-Tuned Large Language Models for Next Point-of-Interest Recommendation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21599" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.21599v2</a>
  <a href="https://arxiv.org/pdf/2506.21599.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21599v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21599v2', 'Refine-POI: Reinforcement Fine-Tuned Large Language Models for Next Point-of-Interest Recommendation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Peibo Li, Shuang Ao, Hao Xue, Yang Song, Maarten de Rijke, Johan BarthÃ©lemy, Tomasz Bednarz, Flora D. Salim

**åˆ†ç±»**: cs.IR, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-19 (æ›´æ–°: 2025-06-30)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRefine-POIä»¥è§£å†³POIæ¨èä¸­çš„æ•°æ®ä¸åŒ¹é…é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å…´è¶£ç‚¹æ¨è` `å¼ºåŒ–å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `ç›‘ç£å¾®è°ƒ` `æ¨èç³»ç»Ÿ` `æ•°æ®åŒ¹é…` `ç”¨æˆ·ä½“éªŒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„POIæ¨èæ–¹æ³•åœ¨æ•°æ®åŒ¹é…ä¸Šå­˜åœ¨æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯SFTæ¨¡å‹æ— æ³•æœ‰æ•ˆå¤„ç†å•ä¸€ç›®æ ‡POIçš„æƒ…å†µã€‚
2. Refine-POIé€šè¿‡å¼•å…¥æ¨èé©±åŠ¨çš„å¥–åŠ±æœºåˆ¶ï¼Œå…è®¸æ¨¡å‹åœ¨ä»…æœ‰ä¸€ä¸ªçœŸå®POIçš„æƒ…å†µä¸‹ç”Ÿæˆtop-kæ¨èåˆ—è¡¨ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRefine-POIåœ¨å¤šä¸ªçœŸå®æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„top-kæ¨èæ€§èƒ½ï¼Œè¶…è¶Šäº†ç°æœ‰åŸºçº¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å·²è¢«åº”ç”¨äºä¸‹ä¸€ä¸ªå…´è¶£ç‚¹ï¼ˆPOIï¼‰æ¨èä»»åŠ¡ã€‚ç°æœ‰çš„åŸºäºLLMçš„æ¨èç³»ç»Ÿé€šå¸¸åˆ†ä¸ºåŸºäºæç¤ºå’Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä¸¤ç§æ¨¡å‹ã€‚åŸºäºæç¤ºçš„æ¨¡å‹æä¾›æ›´å¤§çš„è¾“å‡ºçµæ´»æ€§ï¼Œä½†å‡†ç¡®æ€§è¾ƒä½ï¼›è€ŒSFTæ¨¡å‹è™½ç„¶æ€§èƒ½æ›´é«˜ï¼Œä½†é¢ä¸´æ ¹æœ¬æ€§ä¸åŒ¹é…é—®é¢˜ï¼šPOIæ¨èæ•°æ®å¹¶ä¸é€‚åˆç›‘ç£å¾®è°ƒã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†Refine-POIï¼Œä¸€ä¸ªç”¨äºä¸‹ä¸€ä¸ªPOIæ¨èçš„å¼ºåŒ–å¾®è°ƒæ¡†æ¶ã€‚æˆ‘ä»¬å¼•å…¥äº†æ¨èé©±åŠ¨çš„å¥–åŠ±æœºåˆ¶ï¼Œä½¿LLMsèƒ½å¤Ÿä»…ä½¿ç”¨ä¸€ä¸ªçœŸå®POIç”Ÿæˆtop-kæ¨èåˆ—è¡¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRefine-POIåœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„top-kæ¨èæ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä¸‹ä¸€ä¸ªå…´è¶£ç‚¹ï¼ˆPOIï¼‰æ¨èä»»åŠ¡ä¸­ï¼Œç°æœ‰ç›‘ç£å¾®è°ƒæ–¹æ³•ä¸æ•°æ®ä¸åŒ¹é…çš„é—®é¢˜ã€‚ä¼ ç»ŸSFTæ¨¡å‹è¦æ±‚æ¯ä¸ªè®­ç»ƒæ ·æœ¬éƒ½æœ‰å¤šä¸ªç›®æ ‡POIï¼Œä½†å®é™…æ•°æ®ä¸­æ¯ä¸ªæ ·æœ¬ä»…æœ‰ä¸€ä¸ªçœŸå®POIï¼Œå¯¼è‡´æ¨¡å‹æ— æ³•æœ‰æ•ˆå­¦ä¹ top-kæ¨èã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šRefine-POIæå‡ºäº†ä¸€ç§å¼ºåŒ–å¾®è°ƒæ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥æ¨èé©±åŠ¨çš„å¥–åŠ±æœºåˆ¶ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨å•ä¸€çœŸå®POIçš„åŸºç¡€ä¸Šï¼Œå­¦ä¹ ç”Ÿæˆtop-kæ¨èåˆ—è¡¨ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨å…‹æœä¼ ç»ŸSFTæ–¹æ³•çš„å±€é™æ€§ï¼Œæå‡æ¨èçš„å‡†ç¡®æ€§å’Œçµæ´»æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹è®­ç»ƒå’Œæ¨èç”Ÿæˆä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚åœ¨æ¨¡å‹è®­ç»ƒé˜¶æ®µï¼Œä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•æ¥ä¼˜åŒ–ç”Ÿæˆçš„æ¨èåˆ—è¡¨ï¼Œä½¿å…¶æ›´ç¬¦åˆç”¨æˆ·çš„å®é™…éœ€æ±‚ã€‚

**å…³é”®åˆ›æ–°**ï¼šRefine-POIçš„æœ€å¤§åˆ›æ–°åœ¨äºå¼•å…¥äº†æ¨èé©±åŠ¨çš„å¥–åŠ±æœºåˆ¶ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨ç¼ºä¹å¤šç›®æ ‡POIçš„æƒ…å†µä¸‹ï¼Œä¾ç„¶æœ‰æ•ˆåœ°ç”Ÿæˆé«˜è´¨é‡çš„æ¨èåˆ—è¡¨ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„SFTæ–¹æ³•æœ¬è´¨ä¸Šä¸åŒï¼Œåè€…æ— æ³•å¤„ç†å•ä¸€ç›®æ ‡çš„æ¨èä»»åŠ¡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥å¹³è¡¡æ¨èçš„å‡†ç¡®æ€§ä¸å¤šæ ·æ€§ï¼ŒåŒæ—¶ä¼˜åŒ–äº†ç½‘ç»œç»“æ„ä»¥é€‚åº”å¼ºåŒ–å­¦ä¹ çš„éœ€æ±‚ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œè®­ç»ƒç­–ç•¥ä¹Ÿç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥ç¡®ä¿æ¨¡å‹çš„æ”¶æ•›æ€§å’Œæ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¤šä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒRefine-POIåœ¨top-kæ¨èæ€§èƒ½ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ï¼Œç›¸è¾ƒäºä¼ ç»ŸSFTæ¨¡å‹ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°äº†XX%ï¼ˆå…·ä½“æ•°æ®æœªçŸ¥ï¼‰ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨POIæ¨èä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å’Œä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Refine-POIçš„ç ”ç©¶æˆæœå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æ—…æ¸¸ã€é¤é¥®å’Œç¤¾äº¤ç½‘ç»œç­‰é¢†åŸŸã€‚é€šè¿‡æä¾›æ›´å‡†ç¡®çš„POIæ¨èï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡ç”¨æˆ·ä½“éªŒå’Œæ»¡æ„åº¦ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ä¹Ÿä¸ºå…¶ä»–æ¨èç³»ç»Ÿçš„ä¼˜åŒ–æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•è®ºã€‚æœªæ¥ï¼Œéšç€æ•°æ®é‡çš„å¢åŠ å’Œæ¨¡å‹çš„è¿›ä¸€æ­¥ä¼˜åŒ–ï¼ŒRefine-POIæœ‰æœ›åœ¨æ›´å¤šå®é™…åœºæ™¯ä¸­å¾—åˆ°åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have been adopted for next point-of-interest (POI) recommendation tasks. Typical LLM-based recommenders fall into two categories: prompt-based and supervised fine-tuning (SFT)-based models. Prompt-based models generally offer greater output flexibility but deliver lower accuracy, whereas SFT-based models achieve higher performance yet face a fundamental mismatch: next POI recommendation data does not naturally suit supervised fine-tuning. In SFT, the model is trained to reproduce the exact ground truth, but each training example provides only a single target POI, so there is no ground truth for producing a top-k list.
>   To address this, we propose Refine-POI, a reinforcement fine-tuning framework for next POI recommendation. We introduce recommendation-driven rewards that enable LLMs to learn to generate top-k recommendation lists using only one ground-truth POI per example. Experiments on real-world datasets demonstrate that Refine-POI achieves state-of-the-art top-k recommendation performance.

