---
layout: default
title: LMR-BENCH: Evaluating LLM Agent's Ability on Reproducing Language Modeling Research
---

# LMR-BENCH: Evaluating LLM Agent's Ability on Reproducing Language Modeling Research

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.17335" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.17335v1</a>
  <a href="https://arxiv.org/pdf/2506.17335.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.17335v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.17335v1', 'LMR-BENCH: Evaluating LLM Agent\'s Ability on Reproducing Language Modeling Research')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shuo Yan, Ruochen Li, Ziming Luo, Zimu Wang, Daoyang Li, Liqiang Jing, Kaiyu He, Peilin Wu, George Michalopoulos, Yue Zhang, Ziyang Zhang, Mian Zhang, Zhiyu Chen, Xinya Du

**åˆ†ç±»**: cs.SE, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-19

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLMR-BENCHä»¥è¯„ä¼°LLMä»£ç†åœ¨è¯­è¨€å»ºæ¨¡ç ”ç©¶ä¸­çš„ä»£ç é‡ç°èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `ä»£ç é‡ç°` `è‡ªç„¶è¯­è¨€å¤„ç†` `ç§‘å­¦æ¨ç†` `åŸºå‡†è¯„ä¼°` `æœºå™¨å­¦ä¹ ` `è‡ªåŠ¨åŒ–ç ”ç©¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨é‡ç°ç ”ç©¶è®ºæ–‡ä¸­çš„ä»£ç èƒ½åŠ›ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚çš„æ¨ç†å’Œä»£ç ç†è§£æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚
2. è®ºæ–‡æå‡ºLMR-BENCHåŸºå‡†ï¼Œé€šè¿‡28ä¸ªä»»åŠ¡ç³»ç»Ÿè¯„ä¼°LLMä»£ç†åœ¨ä»£ç é‡ç°ä¸­çš„è¡¨ç°ï¼Œå¡«è¡¥äº†è¿™ä¸€ç ”ç©¶ç©ºç™½ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å‰æœ€å…ˆè¿›çš„LLMæ¨¡å‹åœ¨ç§‘å­¦æ¨ç†å’Œä»£ç ç»¼åˆæ–¹é¢ä»æœ‰æ˜¾è‘—å±€é™ï¼ŒäºŸéœ€æ”¹è¿›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†åœ¨æ¨åŠ¨ç§‘å­¦å‘ç°æ–¹é¢å±•ç°å‡ºæ˜¾è‘—æ½œåŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨é‡ç°ç ”ç©¶è®ºæ–‡ä¸­çš„ä»£ç ï¼Œå°¤å…¶æ˜¯åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸçš„èƒ½åŠ›ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æ­¤ä»»åŠ¡æ¶‰åŠå¤æ‚çš„æ¨ç†æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æŠ½è±¡æ¦‚å¿µçš„æ™ºåŠ›ç»¼åˆå’Œå¯¹ç›¸äº’ä¾èµ–æ–‡ä»¶çš„ä»£ç åº“çš„ç†è§£ã€‚ä¸ºå¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæœ¬æ–‡æå‡ºäº†LMR-BENCHï¼Œä¸€ä¸ªæ—¨åœ¨ç³»ç»Ÿè¯„ä¼°LLMä»£ç†åœ¨è¯­è¨€å»ºæ¨¡ç ”ç©¶ä¸­ä»£ç é‡ç°èƒ½åŠ›çš„åŸºå‡†ã€‚è¯¥åŸºå‡†åŒ…å«28ä¸ªä»£ç é‡ç°ä»»åŠ¡ï¼Œæ¥æºäºè¿‡å»äº”å¹´å†…23ç¯‡å‘è¡¨åœ¨é¡¶çº§NLPä¼šè®®ä¸Šçš„ç ”ç©¶è®ºæ–‡ï¼Œæ¶µç›–ä¹ä¸ªåŸºæœ¬ç±»åˆ«ã€‚æ¨¡å‹æ¥æ”¶ç ”ç©¶è®ºæ–‡ã€åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªè¢«é®è”½å‡½æ•°çš„ä»£ç åº“ä»¥åŠå®ç°è¿™äº›å‡½æ•°çš„æŒ‡ä»¤ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨ç§‘å­¦æ¨ç†å’Œä»£ç ç»¼åˆæ–¹é¢ä»å­˜åœ¨æŒç»­çš„å±€é™æ€§ï¼Œçªæ˜¾äº†LLMä»£ç†åœ¨è‡ªä¸»é‡ç°ç§‘å­¦ç ”ç©¶ä¸­çš„å…³é”®ç¼ºå£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³LLMä»£ç†åœ¨é‡ç°è¯­è¨€å»ºæ¨¡ç ”ç©¶ä¸­ä»£ç çš„èƒ½åŠ›ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚æ¨ç†å’Œä»£ç åº“ç†è§£æ–¹é¢å­˜åœ¨æ˜¾è‘—ç—›ç‚¹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šLMR-BENCHåŸºå‡†é€šè¿‡è®¾è®¡ä¸€ç³»åˆ—ä»£ç é‡ç°ä»»åŠ¡ï¼Œç³»ç»Ÿæ€§åœ°è¯„ä¼°LLMä»£ç†çš„èƒ½åŠ›ï¼Œæ—¨åœ¨æ­ç¤ºå…¶åœ¨ç§‘å­¦ç ”ç©¶é‡ç°ä¸­çš„å±€é™æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä»»åŠ¡è®¾è®¡ã€æ¨¡å‹è¾“å…¥ï¼ˆç ”ç©¶è®ºæ–‡å’Œä»£ç åº“ï¼‰ã€ä»¥åŠè¯„ä¼°æ ‡å‡†ï¼ˆå•å…ƒæµ‹è¯•å‡†ç¡®æ€§å’Œä»£ç æ­£ç¡®æ€§è¯„ä¼°ï¼‰ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬ä»»åŠ¡ç”Ÿæˆã€æ¨¡å‹æ¨ç†å’Œç»“æœè¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šLMR-BENCHçš„è®¾è®¡æ˜¯å…¶æ ¸å¿ƒåˆ›æ–°ç‚¹ï¼Œé€šè¿‡ç³»ç»ŸåŒ–çš„ä»»åŠ¡è¯„ä¼°å¡«è¡¥äº†LLMåœ¨ç§‘å­¦ç ”ç©¶é‡ç°èƒ½åŠ›è¯„ä¼°çš„ç©ºç™½ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæ›´åŠ æ³¨é‡å¤æ‚æ¨ç†å’Œä»£ç ç»¼åˆèƒ½åŠ›çš„è¯„ä¼°ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ä»»åŠ¡è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†å¤šæ ·åŒ–çš„ä»£ç é‡ç°ä»»åŠ¡ï¼Œç¡®ä¿æ¶µç›–ä¸åŒçš„ç ”ç©¶é¢†åŸŸå’Œå¤æ‚åº¦ï¼ŒåŒæ—¶åœ¨è¯„ä¼°ä¸­å¼•å…¥äº†æ ‡å‡†åŒ–çš„å•å…ƒæµ‹è¯•ï¼Œä»¥é‡åŒ–æ¨¡å‹çš„è¡¨ç°ã€‚å®éªŒä¸­ä½¿ç”¨äº†æœ€æ–°çš„LLMæ¨¡å‹è¿›è¡Œå¯¹æ¯”ï¼Œç¡®ä¿ç»“æœçš„å¯é æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å‰æœ€å…ˆè¿›çš„LLMæ¨¡å‹åœ¨28ä¸ªä»£ç é‡ç°ä»»åŠ¡ä¸­çš„è¡¨ç°ä»ç„¶æœ‰é™ï¼Œå°¤å…¶åœ¨ç§‘å­¦æ¨ç†å’Œä»£ç ç»¼åˆæ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ã€‚å…·ä½“è€Œè¨€ï¼Œæ¨¡å‹åœ¨å•å…ƒæµ‹è¯•ä¸­çš„å‡†ç¡®ç‡æœªè¾¾åˆ°é¢„æœŸï¼Œåæ˜ å‡ºåœ¨è‡ªä¸»é‡ç°ç§‘å­¦ç ”ç©¶æ–¹é¢çš„å…³é”®ç¼ºå£ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç§‘å­¦ç ”ç©¶è‡ªåŠ¨åŒ–ã€ä»£ç ç”Ÿæˆå·¥å…·ä»¥åŠæ•™è‚²é¢†åŸŸçš„ç¼–ç¨‹æ•™å­¦ã€‚é€šè¿‡æé«˜LLMåœ¨ä»£ç é‡ç°æ–¹é¢çš„èƒ½åŠ›ï¼Œå¯ä»¥ä¿ƒè¿›ç§‘å­¦å‘ç°çš„æ•ˆç‡ï¼Œå¹¶ä¸ºå¼€å‘æ›´æ™ºèƒ½çš„ç¼–ç¨‹åŠ©æ‰‹å¥ å®šåŸºç¡€ã€‚æœªæ¥ï¼ŒLMR-BENCHå¯èƒ½æˆä¸ºè¯„ä¼°LLMèƒ½åŠ›çš„é‡è¦æ ‡å‡†ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„è¿›æ­¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language model (LLM) agents have demonstrated remarkable potential in advancing scientific discovery. However, their capability in the fundamental yet crucial task of reproducing code from research papers, especially in the NLP domain, remains underexplored. This task includes unique complex reasoning challenges in the intellectual synthesis of abstract concepts and the comprehension of code repositories with interdependent files. Motivated by this gap, we present LMR-BENCH, a benchmark designed to systematically evaluate the capability of LLM agents on code reproduction from Language Modeling Research. It consists of 28 code reproduction tasks derived from 23 research papers published in top-tier NLP venues over the past five years, spanning nine fundamental categories. Models are provided with a research paper, a code repository containing one or more masked functions, and instructions for implementing these functions. We conduct extensive experiments in standard prompting and LLM agent settings with state-of-the-art LLMs, evaluating the accuracy of unit tests and performing LLM-based evaluation of code correctness. Experimental results reveal that even the most advanced models still exhibit persistent limitations in scientific reasoning and code synthesis, highlighting critical gaps in LLM agents' ability to autonomously reproduce scientific research

