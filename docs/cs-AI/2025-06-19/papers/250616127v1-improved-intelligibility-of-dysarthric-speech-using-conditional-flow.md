---
layout: default
title: Improved Intelligibility of Dysarthric Speech using Conditional Flow Matching
---

# Improved Intelligibility of Dysarthric Speech using Conditional Flow Matching

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.16127" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.16127v1</a>
  <a href="https://arxiv.org/pdf/2506.16127.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.16127v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.16127v1', 'Improved Intelligibility of Dysarthric Speech using Conditional Flow Matching')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shoutrik Das, Nishant Singh, Arjun Gangwar, S Umesh

**åˆ†ç±»**: cs.SD, cs.AI, eess.AS

**å‘å¸ƒæ—¥æœŸ**: 2025-06-19

**å¤‡æ³¨**: Accepted at Interspeech 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ¡ä»¶æµåŒ¹é…ä»¥æ”¹å–„æ„éŸ³éšœç¢è¯­éŸ³çš„å¯æ‡‚æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ„éŸ³éšœç¢` `è¯­éŸ³è½¬æ¢` `æ¡ä»¶æµåŒ¹é…` `è‡ªç›‘ç£å­¦ä¹ ` `æ‰©æ•£å˜æ¢å™¨` `ç¦»æ•£å£°å­¦å•å…ƒ` `è¯­éŸ³å¯æ‡‚æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ„éŸ³éšœç¢æ‚£è€…çš„è¯­éŸ³å¯æ‡‚æ€§æ˜¾è‘—é™ä½ï¼Œç°æœ‰çš„è½¬æ¢æŠ€æœ¯åœ¨å¤„ç†è¿™ç§è¯­éŸ³æ—¶å­˜åœ¨å±€é™æ€§ã€‚
2. æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ¡ä»¶æµåŒ¹é…çš„éè‡ªå›å½’æ–¹æ³•ï¼Œæ—¨åœ¨ç›´æ¥å°†æ„éŸ³éšœç¢è¯­éŸ³è½¬æ¢ä¸ºæ¸…æ™°è¯­éŸ³ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨ç¦»æ•£å£°å­¦å•å…ƒå¯ä»¥æœ‰æ•ˆæé«˜è¯­éŸ³å¯æ‡‚æ€§ï¼Œå¹¶ä¸”æ”¶æ•›é€Ÿåº¦ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ„éŸ³éšœç¢æ˜¯ä¸€ç§æ˜¾è‘—å½±å“è¯­éŸ³å¯æ‡‚æ€§çš„ç¥ç»ç³»ç»Ÿç–¾ç—…ï¼Œå¸¸å¯¼è‡´æ‚£è€…æ— æ³•æœ‰æ•ˆæ²Ÿé€šã€‚å› æ­¤ï¼Œå¼€å‘ç¨³å¥çš„æ„éŸ³éšœç¢åˆ°æ­£å¸¸è¯­éŸ³çš„è½¬æ¢æŠ€æœ¯æ˜¾å¾—å°¤ä¸ºé‡è¦ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰ç‰¹å¾åŠå…¶é‡åŒ–è¡¨ç¤ºä½œä¸ºè¯­éŸ³ç”Ÿæˆçš„æ›¿ä»£æ–¹æ¡ˆï¼Œå¹¶æå‡ºäº†ä¸€ç§å®Œå…¨éè‡ªå›å½’çš„æ–¹æ³•ï¼Œåˆ©ç”¨æ¡ä»¶æµåŒ¹é…ï¼ˆCFMï¼‰ä¸æ‰©æ•£å˜æ¢å™¨ç›´æ¥æ˜ å°„æ„éŸ³éšœç¢è¯­éŸ³åˆ°æ¸…æ™°è¯­éŸ³ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç¦»æ•£å£°å­¦å•å…ƒåœ¨æé«˜å¯æ‡‚æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶ä¸ä¼ ç»Ÿçš„æ¢…å°”é¢‘è°±æ³•ç›¸æ¯”ï¼Œæ”¶æ•›é€Ÿåº¦æ›´å¿«ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³æ„éŸ³éšœç¢è¯­éŸ³çš„å¯æ‡‚æ€§é—®é¢˜ï¼Œç°æœ‰çš„è¯­éŸ³è½¬æ¢æ–¹æ³•åœ¨å¤„ç†æ­¤ç±»è¯­éŸ³æ—¶å¸¸å¸¸æ•ˆæœä¸ä½³ï¼Œéš¾ä»¥æ»¡è¶³å®é™…æ²Ÿé€šéœ€æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºæ¡ä»¶æµåŒ¹é…ï¼ˆCFMï¼‰çš„å®Œå…¨éè‡ªå›å½’æ–¹æ³•ï¼Œé€šè¿‡ç›´æ¥æ˜ å°„æ„éŸ³éšœç¢è¯­éŸ³åˆ°æ¸…æ™°è¯­éŸ³ï¼Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•çš„å±€é™æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ç‰¹å¾æå–ã€æ¡ä»¶æµåŒ¹é…æ¨¡å—å’Œæ‰©æ•£å˜æ¢å™¨ã€‚ç‰¹å¾æå–ä½¿ç”¨WavLMæå–å•ä¸€è¯´è¯è€…çš„å¹²å‡€è¯­éŸ³ç‰¹å¾ï¼Œéšåé€šè¿‡CFMè¿›è¡Œæ˜ å°„ï¼Œæœ€åç”Ÿæˆæ¸…æ™°è¯­éŸ³ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†æ¡ä»¶æµåŒ¹é…ä¸æ‰©æ•£å˜æ¢å™¨çš„ç»“åˆï¼Œåˆ©ç”¨ç¦»æ•£å£°å­¦å•å…ƒæ¥æå‡è¯­éŸ³çš„å¯æ‡‚æ€§ï¼Œæ˜¾è‘—æé«˜äº†è½¬æ¢çš„æ•ˆç‡ä¸æ•ˆæœã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–è¯­éŸ³ç”Ÿæˆè´¨é‡ï¼ŒåŒæ—¶ç½‘ç»œç»“æ„è®¾è®¡ä¸Šæ³¨é‡äº†å¯¹ç¦»æ•£å£°å­¦å•å…ƒçš„æœ‰æ•ˆåˆ©ç”¨ï¼Œä»¥åŠ é€Ÿæ”¶æ•›è¿‡ç¨‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨æ¡ä»¶æµåŒ¹é…æ–¹æ³•çš„æ¨¡å‹åœ¨è¯­éŸ³å¯æ‡‚æ€§ä¸Šè¾ƒä¼ ç»Ÿæ¢…å°”é¢‘è°±æ–¹æ³•æœ‰æ˜¾è‘—æå‡ï¼Œå…·ä½“æ€§èƒ½æ•°æ®æœªæä¾›ï¼Œä½†æ”¶æ•›é€Ÿåº¦æ˜æ˜¾åŠ å¿«ï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è¾…åŠ©æ²Ÿé€šè®¾å¤‡ã€è¯­éŸ³æ²»ç–—å·¥å…·ä»¥åŠæ™ºèƒ½è¯­éŸ³åŠ©æ‰‹ç­‰ã€‚é€šè¿‡æ”¹å–„æ„éŸ³éšœç¢æ‚£è€…çš„è¯­éŸ³å¯æ‡‚æ€§ï¼Œèƒ½å¤Ÿæå¤§æå‡ä»–ä»¬çš„æ²Ÿé€šèƒ½åŠ›å’Œç”Ÿæ´»è´¨é‡ï¼Œå…·æœ‰é‡è¦çš„ç¤¾ä¼šä»·å€¼å’Œå®é™…æ„ä¹‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Dysarthria is a neurological disorder that significantly impairs speech intelligibility, often rendering affected individuals unable to communicate effectively. This necessitates the development of robust dysarthric-to-regular speech conversion techniques. In this work, we investigate the utility and limitations of self-supervised learning (SSL) features and their quantized representations as an alternative to mel-spectrograms for speech generation. Additionally, we explore methods to mitigate speaker variability by generating clean speech in a single-speaker voice using features extracted from WavLM. To this end, we propose a fully non-autoregressive approach that leverages Conditional Flow Matching (CFM) with Diffusion Transformers to learn a direct mapping from dysarthric to clean speech. Our findings highlight the effectiveness of discrete acoustic units in improving intelligibility while achieving faster convergence compared to traditional mel-spectrogram-based approaches.

