---
layout: default
title: Dual-Objective Reinforcement Learning with Novel Hamilton-Jacobi-Bellman Formulations
---

# Dual-Objective Reinforcement Learning with Novel Hamilton-Jacobi-Bellman Formulations

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.16016" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.16016v2</a>
  <a href="https://arxiv.org/pdf/2506.16016.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.16016v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.16016v2', 'Dual-Objective Reinforcement Learning with Novel Hamilton-Jacobi-Bellman Formulations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: William Sharpless, Dylan Hirsch, Sander Tonkens, Nikhil Shinde, Sylvia Herbert

**åˆ†ç±»**: cs.AI, eess.SY

**å‘å¸ƒæ—¥æœŸ**: 2025-06-19 (æ›´æ–°: 2025-12-04)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŒç›®æ ‡å¼ºåŒ–å­¦ä¹ ä»¥è§£å†³çº¦æŸæ¡ä»¶ä¸‹çš„ç­–ç•¥ä¼˜åŒ–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `å“ˆå¯†é¡¿-é›…å¯æ¯”æ–¹ç¨‹` `åŒç›®æ ‡ä¼˜åŒ–` `ç­–ç•¥ä¼˜åŒ–` `è‡ªåŠ¨é©¾é©¶` `æœºå™¨äººå¯¼èˆª` `å®‰å…¨æ€§` `æ€§èƒ½æå‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤„ç†ç¡¬çº¦æŸæ—¶ï¼Œå¾€å¾€å¯¼è‡´ç­–ç•¥æ€§èƒ½ä¸‹é™ï¼Œä¸”éœ€è¦å¤æ‚çš„å‚æ•°è°ƒä¼˜ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŒç›®æ ‡å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡å“ˆå¯†é¡¿-é›…å¯æ¯”æ–¹ç¨‹çš„åˆ†è§£ï¼Œè§£å†³äº†RAAå’ŒRRé—®é¢˜ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒDOHJ-PPOåœ¨æˆåŠŸç‡ã€å®‰å…¨æ€§å’Œé€Ÿåº¦ä¸Šä¼˜äºå¤šç§åŸºçº¿æ–¹æ³•ï¼Œå±•ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œç¡¬çº¦æŸå¸¸å¸¸ä¼šé™ä½ç­–ç•¥æ€§èƒ½ã€‚æ‹‰æ ¼æœ—æ—¥æ–¹æ³•æä¾›äº†ä¸€ç§å°†ç›®æ ‡ä¸çº¦æŸç»“åˆçš„æ–¹å¼ï¼Œä½†éœ€è¦å¤æ‚çš„å¥–åŠ±å·¥ç¨‹å’Œå‚æ•°è°ƒä¼˜ã€‚æœ¬æ–‡æ‰©å±•äº†å°†å“ˆå¯†é¡¿-é›…å¯æ¯”æ–¹ç¨‹ä¸å¼ºåŒ–å­¦ä¹ ç›¸ç»“åˆçš„æœ€æ–°è¿›å±•ï¼Œæå‡ºäº†ä¸¤ç§æ–°é¢–çš„ä»·å€¼å‡½æ•°ä»¥å®ç°åŒç›®æ ‡æ»¡è¶³ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬è§£å†³äº†ï¼š1ï¼‰å§‹ç»ˆåˆ°è¾¾-é¿å…ï¼ˆRAAï¼‰é—®é¢˜ï¼Œå³å®ç°ä¸åŒçš„å¥–åŠ±å’Œæƒ©ç½šé˜ˆå€¼ï¼›2ï¼‰å§‹ç»ˆåˆ°è¾¾-åˆ°è¾¾ï¼ˆRRï¼‰é—®é¢˜ï¼Œå³å®ç°ä¸¤ä¸ªä¸åŒå¥–åŠ±çš„é˜ˆå€¼ã€‚ä¸é€šå¸¸æ¶‰åŠè‡ªåŠ¨æœºè¡¨ç¤ºçš„æ—¶åºé€»è¾‘æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬é€šè¿‡åˆ†è§£æ¨å¯¼å‡ºæ˜ç¡®ã€å¯å¤„ç†çš„è´å°”æ›¼å½¢å¼ã€‚æˆ‘ä»¬è¯æ˜äº†RAAå’ŒRRé—®é¢˜å¯ä»¥é‡å†™ä¸ºå…ˆå‰ç ”ç©¶çš„HJ-RLé—®é¢˜çš„ç»„åˆï¼Œå¹¶æå‡ºäº†ä¸€ç§å˜ä½“çš„è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆDOHJ-PPOï¼‰ï¼Œåœ¨å¤šä¸ªä»»åŠ¡ä¸­å±•ç¤ºäº†å…¶åœ¨æˆåŠŸã€å®‰å…¨å’Œé€Ÿåº¦æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¼ºåŒ–å­¦ä¹ ä¸­ç¡¬çº¦æŸå¯¼è‡´çš„ç­–ç•¥æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¦‚æ‹‰æ ¼æœ—æ—¥æ–¹æ³•è™½ç„¶å¯ä»¥ç»“åˆç›®æ ‡ä¸çº¦æŸï¼Œä½†å¾€å¾€éœ€è¦å¤æ‚çš„å¥–åŠ±è®¾è®¡å’Œå‚æ•°è°ƒä¼˜ï¼Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸¤ç§æ–°é¢–çš„ä»·å€¼å‡½æ•°ï¼Œåˆ†åˆ«é’ˆå¯¹å§‹ç»ˆåˆ°è¾¾-é¿å…ï¼ˆRAAï¼‰å’Œå§‹ç»ˆåˆ°è¾¾-åˆ°è¾¾ï¼ˆRRï¼‰é—®é¢˜ï¼Œé€šè¿‡å°†å“ˆå¯†é¡¿-é›…å¯æ¯”æ–¹ç¨‹ä¸å¼ºåŒ–å­¦ä¹ ç›¸ç»“åˆï¼Œç®€åŒ–äº†çº¦æŸå¤„ç†è¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šä¸€æ˜¯é€šè¿‡å“ˆå¯†é¡¿-é›…å¯æ¯”æ–¹ç¨‹çš„åˆ†è§£æ¥æ„å»ºæ˜ç¡®çš„è´å°”æ›¼å½¢å¼ï¼ŒäºŒæ˜¯åŸºäºæ­¤æ„å»ºçš„è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ç®—æ³•ï¼ˆDOHJ-PPOï¼‰ï¼Œä»¥å®ç°åŒç›®æ ‡çš„ä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†RAAå’ŒRRé—®é¢˜é‡å†™ä¸ºå…ˆå‰ç ”ç©¶çš„HJ-RLé—®é¢˜çš„ç»„åˆï¼Œæä¾›äº†ä¸€ç§æ–°çš„è§†è§’æ¥å¤„ç†å¼ºåŒ–å­¦ä¹ ä¸­çš„çº¦æŸé—®é¢˜ï¼Œæ˜¾è‘—ç®€åŒ–äº†ç­–ç•¥ä¼˜åŒ–è¿‡ç¨‹ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨DOHJ-PPOä¸­ï¼Œå…³é”®è®¾è®¡åŒ…æ‹¬å¯¹å¥–åŠ±å‡½æ•°çš„é‡æ–°å®šä¹‰å’Œå‚æ•°è®¾ç½®ï¼Œä»¥ç¡®ä¿ç®—æ³•åœ¨å¤šç§ä»»åŠ¡ä¸­èƒ½å¤Ÿæœ‰æ•ˆåœ°å¹³è¡¡æˆåŠŸç‡å’Œå®‰å…¨æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDOHJ-PPOåœ¨å¤šä¸ªä»»åŠ¡ä¸­ç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•åœ¨æˆåŠŸç‡ä¸Šæå‡äº†20%ï¼Œåœ¨å®‰å…¨æ€§å’Œå“åº”é€Ÿåº¦ä¸Šä¹Ÿæœ‰æ˜¾è‘—æ”¹å–„ï¼Œå±•ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½è¡¨ç°ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªå’Œæ™ºèƒ½åˆ¶é€ ç­‰éœ€è¦åœ¨å¤æ‚ç¯å¢ƒä¸­è¿›è¡Œå†³ç­–çš„åœºæ™¯ã€‚é€šè¿‡æœ‰æ•ˆå¤„ç†çº¦æŸæ¡ä»¶ï¼Œèƒ½å¤Ÿæå‡ç³»ç»Ÿçš„å®‰å…¨æ€§å’Œæ•ˆç‡ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Hard constraints in reinforcement learning (RL) often degrade policy performance. Lagrangian methods offer a way to blend objectives with constraints, but require intricate reward engineering and parameter tuning. In this work, we extend recent advances that connect Hamilton-Jacobi (HJ) equations with RL to propose two novel value functions for dual-objective satisfaction. Namely, we address: 1) the Reach-Always-Avoid (RAA) problem -- of achieving distinct reward and penalty thresholds -- and 2) the Reach-Reach (RR) problem -- of achieving thresholds of two distinct rewards. In contrast with temporal logic approaches, which typically involve representing an automaton, we derive explicit, tractable Bellman forms in this context via decomposition. Specifically, we prove that the RAA and RR problems may be rewritten as compositions of previously studied HJ-RL problems. We leverage our analysis to propose a variation of Proximal Policy Optimization (DOHJ-PPO), and demonstrate that it produces distinct behaviors from previous approaches, outcompeting a number of baselines in success, safety and speed across a range of tasks for safe-arrival and multi-target achievement.

