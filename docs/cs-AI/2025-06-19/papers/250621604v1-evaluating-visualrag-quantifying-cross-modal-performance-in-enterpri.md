---
layout: default
title: Evaluating VisualRAG: Quantifying Cross-Modal Performance in Enterprise Document Understanding
---

# Evaluating VisualRAG: Quantifying Cross-Modal Performance in Enterprise Document Understanding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21604" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.21604v1</a>
  <a href="https://arxiv.org/pdf/2506.21604.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21604v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21604v1', 'Evaluating VisualRAG: Quantifying Cross-Modal Performance in Enterprise Document Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Varun Mannam, Fang Wang, Xin Chen

**åˆ†ç±»**: cs.IR, cs.AI, cs.CV, cs.HC, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-19

**å¤‡æ³¨**: Conference: KDD conference workshop: https://kdd-eval-workshop.github.io/genai-evaluation-kdd2025/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé‡åŒ–æ¡†æ¶ä»¥æå‡ä¼ä¸šæ–‡æ¡£ç†è§£ä¸­çš„è·¨æ¨¡æ€æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€ç”Ÿæˆ` `ä¼ä¸šæ–‡æ¡£ç†è§£` `ä¿¡ä»»åº¦è¯„ä¼°` `VisualRAG` `è·¨æ¨¡æ€è¾“å…¥` `æ€§èƒ½æå‡` `åŸºç¡€æ¨¡å‹æ¯”è¾ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ¨¡æ€ç”ŸæˆAIè¯„ä¼°æ¡†æ¶éš¾ä»¥å»ºç«‹ä¿¡ä»»åº¦ï¼Œé™åˆ¶äº†å…¶åœ¨ä¼ä¸šä¸­çš„å¯é åº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§ç³»ç»Ÿçš„å®šé‡åŸºå‡†æ¡†æ¶ï¼Œæ—¨åœ¨é‡åŒ–è·¨æ¨¡æ€è¾“å…¥çš„å¯ä¿¡åº¦ï¼Œæå‡ä¼ä¸šæ–‡æ¡£æ™ºèƒ½åŒ–æ°´å¹³ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæœ€ä½³æ¨¡æ€æƒé‡é…ç½®ä½¿æ€§èƒ½æå‡57.3%ï¼Œå¹¶ä¸”ä¸åŸºç¡€æ¨¡å‹çš„æ¯”è¾ƒæ­ç¤ºäº†ä¸åŒæ¨¡å‹å¯¹ä¿¡ä»»åº¦çš„å½±å“ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å½“å‰å¤šæ¨¡æ€ç”ŸæˆAIçš„è¯„ä¼°æ¡†æ¶åœ¨å»ºç«‹å¯ä¿¡åº¦æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œé™åˆ¶äº†å…¶åœ¨ä¼ä¸šä¸­çš„åº”ç”¨ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç³»ç»Ÿçš„å®šé‡åŸºå‡†æ¡†æ¶ï¼Œç”¨äºè¡¡é‡åœ¨VisualRAGç³»ç»Ÿä¸­é€æ­¥é›†æˆæ–‡æœ¬ã€å›¾åƒã€æ ‡é¢˜å’ŒOCRç­‰è·¨æ¨¡æ€è¾“å…¥çš„å¯ä¿¡åº¦ã€‚æˆ‘ä»¬çš„ç ”ç©¶å»ºç«‹äº†æŠ€æœ¯æŒ‡æ ‡ä¸ç”¨æˆ·ä¸­å¿ƒä¿¡ä»»åº¦ä¹‹é—´çš„å®šé‡å…³ç³»ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œæœ€ä½³æ¨¡æ€æƒé‡ï¼ˆæ–‡æœ¬30%ã€å›¾åƒ15%ã€æ ‡é¢˜25%ã€OCR30%ï¼‰ç›¸æ¯”äºä»…ä½¿ç”¨æ–‡æœ¬çš„åŸºçº¿æ€§èƒ½æå‡äº†57.3%ï¼ŒåŒæ—¶ä¿æŒäº†è®¡ç®—æ•ˆç‡ã€‚æˆ‘ä»¬è¿˜å¯¹åŸºç¡€æ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒè¯„ä¼°ï¼Œå±•ç¤ºäº†å®ƒä»¬åœ¨æ ‡é¢˜ç”Ÿæˆå’ŒOCRæå–ä¸­çš„å·®å¼‚æ€§å½±å“ï¼Œè¿™å¯¹å¯é çš„ä¼ä¸šAIè‡³å…³é‡è¦ã€‚æ­¤ç ”ç©¶ä¸ºè´Ÿè´£ä»»çš„AIéƒ¨ç½²æä¾›äº†é‡åŒ–å’Œå¢å¼ºå¤šæ¨¡æ€RAGå¯ä¿¡åº¦çš„ä¸¥æ ¼æ¡†æ¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤šæ¨¡æ€ç”ŸæˆAIè¯„ä¼°æ¡†æ¶åœ¨å¯ä¿¡åº¦å»ºç«‹ä¸Šçš„ä¸è¶³ï¼Œå¯¼è‡´ä¼ä¸šåœ¨é‡‡ç”¨è¿™äº›æŠ€æœ¯æ—¶é¢ä¸´ä¿¡ä»»é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºäº†ä¸€ç§ç³»ç»Ÿçš„å®šé‡åŸºå‡†æ¡†æ¶ï¼Œé€šè¿‡é‡åŒ–æŠ€æœ¯æŒ‡æ ‡ä¸ç”¨æˆ·ä¿¡ä»»åº¦ä¹‹é—´çš„å…³ç³»ï¼Œæ¥æå‡è·¨æ¨¡æ€è¾“å…¥çš„é›†æˆæ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®è¾“å…¥æ¨¡å—ï¼ˆæ–‡æœ¬ã€å›¾åƒã€æ ‡é¢˜ã€OCRï¼‰ã€ä¿¡ä»»åº¦è¯„ä¼°æ¨¡å—å’Œæ€§èƒ½è¯„ä¼°æ¨¡å—ï¼Œé€æ­¥é›†æˆä¸åŒæ¨¡æ€ä»¥ä¼˜åŒ–è¾“å‡ºã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†æœ€ä½³æ¨¡æ€æƒé‡é…ç½®ï¼ˆæ–‡æœ¬30%ã€å›¾åƒ15%ã€æ ‡é¢˜25%ã€OCR30%ï¼‰ï¼Œæ˜¾è‘—æå‡äº†ç³»ç»Ÿæ€§èƒ½ï¼Œå¹¶ä¿æŒäº†è®¡ç®—æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œæƒé‡è®¾ç½®ï¼Œä»¥ç¡®ä¿ä¸åŒæ¨¡æ€çš„æœ‰æ•ˆèåˆå’Œæ€§èƒ½æå‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨æœ€ä½³æ¨¡æ€æƒé‡é…ç½®åï¼Œç³»ç»Ÿæ€§èƒ½ç›¸æ¯”äºä»…ä½¿ç”¨æ–‡æœ¬çš„åŸºçº¿æå‡äº†57.3%ã€‚æ­¤å¤–ï¼ŒåŸºç¡€æ¨¡å‹çš„æ¯”è¾ƒè¯„ä¼°æ­ç¤ºäº†ä¸åŒæ¨¡å‹åœ¨æ ‡é¢˜ç”Ÿæˆå’ŒOCRæå–ä¸­çš„å·®å¼‚æ€§å½±å“ï¼Œä¸ºä¼ä¸šAIçš„å¯é æ€§æä¾›äº†é‡è¦å‚è€ƒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ä¼ä¸šæ–‡æ¡£æ™ºèƒ½åŒ–å¤„ç†ã€ä¿¡æ¯æå–å’Œè‡ªåŠ¨åŒ–æŠ¥å‘Šç”Ÿæˆç­‰ã€‚é€šè¿‡æå‡å¤šæ¨¡æ€AIç³»ç»Ÿçš„å¯ä¿¡åº¦ï¼Œèƒ½å¤Ÿä¿ƒè¿›ä¼ä¸šåœ¨å…³é”®å†³ç­–ä¸­çš„åº”ç”¨ï¼Œå¢å¼ºæ•°æ®é©±åŠ¨çš„å†³ç­–èƒ½åŠ›ï¼Œæœªæ¥å¯èƒ½å¯¹ä¼ä¸šè¿è¥æ•ˆç‡äº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Current evaluation frameworks for multimodal generative AI struggle to establish trustworthiness, hindering enterprise adoption where reliability is paramount. We introduce a systematic, quantitative benchmarking framework to measure the trustworthiness of progressively integrating cross-modal inputs such as text, images, captions, and OCR within VisualRAG systems for enterprise document intelligence. Our approach establishes quantitative relationships between technical metrics and user-centric trust measures. Evaluation reveals that optimal modality weighting with weights of 30% text, 15% image, 25% caption, and 30% OCR improves performance by 57.3% over text-only baselines while maintaining computational efficiency. We provide comparative assessments of foundation models, demonstrating their differential impact on trustworthiness in caption generation and OCR extraction-a vital consideration for reliable enterprise AI. This work advances responsible AI deployment by providing a rigorous framework for quantifying and enhancing trustworthiness in multimodal RAG for critical enterprise applications.

