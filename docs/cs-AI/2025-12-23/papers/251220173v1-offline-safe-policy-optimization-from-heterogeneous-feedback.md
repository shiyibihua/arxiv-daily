---
layout: default
title: Offline Safe Policy Optimization From Heterogeneous Feedback
---

# Offline Safe Policy Optimization From Heterogeneous Feedback

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.20173" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.20173v1</a>
  <a href="https://arxiv.org/pdf/2512.20173.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.20173v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.20173v1', 'Offline Safe Policy Optimization From Heterogeneous Feedback')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ze Gong, Pradeep Varakantham, Akshat Kumar

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-23

**å¤‡æ³¨**: Accepted at AAMAS 2026 (Extended Abstract)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPreSaæ¡†æ¶ï¼Œé€šè¿‡å¼‚æ„åé¦ˆç›´æ¥ä¼˜åŒ–å®‰å…¨ç­–ç•¥ï¼Œè§£å†³ç¦»çº¿å®‰å…¨ç­–ç•¥ä¼˜åŒ–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `ç¦»çº¿å¼ºåŒ–å­¦ä¹ ` `åå¥½å­¦ä¹ ` `å®‰å…¨å¼ºåŒ–å­¦ä¹ ` `äººç±»åé¦ˆ` `çº¦æŸä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å®‰å…¨å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨é•¿æ—¶ç¨‹ä»»åŠ¡ä¸­ï¼Œå¥–åŠ±å’Œæˆæœ¬æ¨¡å‹è¯¯å·®ç´¯ç§¯å¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œæ˜¯æ ¸å¿ƒæŒ‘æˆ˜ã€‚
2. PreSaæ¡†æ¶ç›´æ¥åŸºäºäººç±»åå¥½å’Œå®‰å…¨æ ‡ç­¾å­¦ä¹ ç­–ç•¥ï¼Œé¿å…äº†æ˜¾å¼å¥–åŠ±å’Œæˆæœ¬æ¨¡å‹çš„å­¦ä¹ ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒPreSaåœ¨è¿ç»­æ§åˆ¶ä»»åŠ¡ä¸­ä¼˜äºç°æœ‰åŸºçº¿ï¼ŒæˆåŠŸå­¦ä¹ äº†é«˜å¥–åŠ±çš„å®‰å…¨ç­–ç•¥ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§ç¦»çº¿åå¥½å¼ºåŒ–å­¦ä¹ (PbRL)æ¡†æ¶ï¼Œæ—¨åœ¨æ— éœ€å¤§é‡å¥–åŠ±å·¥ç¨‹å’Œä¸äººå·¥æ ‡æ³¨è€…ç›´æ¥äº¤äº’çš„æƒ…å†µä¸‹ï¼Œå­¦ä¹ ä¸äººç±»åå¥½å¯¹é½çš„å¥–åŠ±å’Œç­–ç•¥ï¼ŒåŒæ—¶ç¡®ä¿å®‰å…¨æ€§ã€‚é’ˆå¯¹ç°æœ‰åŸºäºäººç±»åé¦ˆçš„å®‰å…¨å¼ºåŒ–å­¦ä¹ (RLHF)æ–¹æ³•åœ¨é•¿æ—¶ç¨‹è¿ç»­æ§åˆ¶ä»»åŠ¡ä¸­ï¼Œç”±äºå¥–åŠ±å’Œæˆæœ¬æ¨¡å‹è¯¯å·®ç´¯ç§¯å¯¼è‡´æ€§èƒ½ä¸‹é™çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºPreSa (Preference and Safety Alignment)æ–¹æ³•ã€‚è¯¥æ–¹æ³•ä¸é—´æ¥å­¦ä¹ ç­–ç•¥ï¼ˆä»å¥–åŠ±å’Œæˆæœ¬ï¼‰ï¼Œè€Œæ˜¯ç›´æ¥åŸºäºè½¨è¿¹ç‰‡æ®µçš„å¥–åŠ±åå¥½å’Œå®‰å…¨äºŒå…ƒæ ‡ç­¾å­¦ä¹ ç­–ç•¥ï¼Œé¿å…äº†æ˜¾å¼å­¦ä¹ å¥–åŠ±å’Œæˆæœ¬æ¨¡å‹ï¼Œä¹Ÿæ— éœ€çº¦æŸå¼ºåŒ–å­¦ä¹ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åˆæˆå’ŒçœŸå®äººç±»åé¦ˆä¸‹ï¼ŒæˆåŠŸå­¦ä¹ äº†é«˜å¥–åŠ±çš„å®‰å…¨ç­–ç•¥ï¼Œä¼˜äºç°æœ‰æŠ€æœ¯æ°´å¹³çš„åŸºçº¿å’Œå…·æœ‰çœŸå®å¥–åŠ±å’Œæˆæœ¬çš„ç¦»çº¿å®‰å…¨å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ç¦»çº¿åå¥½å¼ºåŒ–å­¦ä¹ ä¸­çš„å®‰å…¨ç­–ç•¥ä¼˜åŒ–é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚å…ˆå­¦ä¹ å¥–åŠ±å’Œæˆæœ¬æ¨¡å‹ï¼Œå†ä½¿ç”¨çº¦æŸå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ç­–ç•¥ï¼Œåœ¨é•¿æ—¶ç¨‹è¿ç»­æ§åˆ¶ä»»åŠ¡ä¸­ä¼šå› ä¸ºå¥–åŠ±å’Œæˆæœ¬æ¨¡å‹è¯¯å·®çš„ç´¯ç§¯è€Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚è¿™äº›æ–¹æ³•ä¾èµ–äºå‡†ç¡®çš„å¥–åŠ±å’Œæˆæœ¬å»ºæ¨¡ï¼Œè€Œè¿™åœ¨å®é™…åº”ç”¨ä¸­å¾€å¾€æ˜¯å›°éš¾çš„ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é¿å…æ˜¾å¼åœ°å­¦ä¹ å¥–åŠ±å’Œæˆæœ¬æ¨¡å‹ï¼Œè€Œæ˜¯ç›´æ¥ä»äººç±»çš„åå¥½åé¦ˆï¼ˆå…³äºå¥–åŠ±ï¼‰å’Œå®‰å…¨æ ‡ç­¾ä¸­å­¦ä¹ ç­–ç•¥ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¯ä»¥ç»•è¿‡å¥–åŠ±å’Œæˆæœ¬æ¨¡å‹å¸¦æ¥çš„è¯¯å·®ç´¯ç§¯é—®é¢˜ï¼Œç›´æ¥ä¼˜åŒ–ç­–ç•¥ï¼Œä½¿å…¶æ—¢èƒ½æ»¡è¶³äººç±»çš„åå¥½ï¼Œåˆèƒ½ä¿è¯å®‰å…¨æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPreSaæ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šåå¥½å­¦ä¹ æ¨¡å—å’Œå®‰å…¨å¯¹é½æ¨¡å—ã€‚æ•´ä½“æµç¨‹å¦‚ä¸‹ï¼š1) æ”¶é›†åŒ…å«è½¨è¿¹ç‰‡æ®µçš„åå¥½å’Œå®‰å…¨æ ‡ç­¾çš„ç¦»çº¿æ•°æ®é›†ï¼›2) ä½¿ç”¨åå¥½å­¦ä¹ æ¨¡å—å­¦ä¹ ä¸€ä¸ªç­–ç•¥ï¼Œä½¿å…¶ç”Ÿæˆçš„è½¨è¿¹ç‰‡æ®µä¸äººç±»åå¥½å¯¹é½ï¼›3) ä½¿ç”¨å®‰å…¨å¯¹é½æ¨¡å—ï¼Œé€šè¿‡çº¦æŸä¼˜åŒ–é—®é¢˜ï¼Œç¡®ä¿å­¦ä¹ åˆ°çš„ç­–ç•¥ç”Ÿæˆçš„è½¨è¿¹ç‰‡æ®µæ˜¯å®‰å…¨çš„ã€‚æ•´ä¸ªä¼˜åŒ–é—®é¢˜åœ¨ä¸€ä¸ªæ‹‰æ ¼æœ—æ—¥æ¡†æ¶å†…è§£å†³ï¼Œç›´æ¥å­¦ä¹ å¥–åŠ±æœ€å¤§åŒ–çš„å®‰å…¨ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºç›´æ¥ä»åå¥½å’Œå®‰å…¨æ ‡ç­¾å­¦ä¹ ç­–ç•¥ï¼Œé¿å…äº†æ˜¾å¼å¥–åŠ±å’Œæˆæœ¬æ¨¡å‹çš„å­¦ä¹ ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºï¼Œå®ƒç»•è¿‡äº†å¥–åŠ±å’Œæˆæœ¬æ¨¡å‹å¸¦æ¥çš„è¯¯å·®ç´¯ç§¯é—®é¢˜ï¼Œä»è€Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å­¦ä¹ å®‰å…¨ç­–ç•¥ã€‚æ­¤å¤–ï¼Œä½¿ç”¨æ‹‰æ ¼æœ—æ—¥æ¡†æ¶è§£å†³çº¦æŸä¼˜åŒ–é—®é¢˜ï¼Œä½¿å¾—å¯ä»¥ç›´æ¥å­¦ä¹ å®‰å…¨ç­–ç•¥ï¼Œè€Œæ— éœ€ä½¿ç”¨çº¦æŸå¼ºåŒ–å­¦ä¹ ã€‚

**å…³é”®è®¾è®¡**ï¼šPreSaæ¡†æ¶çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨pairwise ranking lossæ¥å­¦ä¹ åå¥½ï¼Œé¼“åŠ±æ¨¡å‹ç”Ÿæˆæ›´ç¬¦åˆäººç±»åå¥½çš„è½¨è¿¹ç‰‡æ®µï¼›2) ä½¿ç”¨å®‰å…¨çº¦æŸæ¥ç¡®ä¿å­¦ä¹ åˆ°çš„ç­–ç•¥ç”Ÿæˆçš„è½¨è¿¹ç‰‡æ®µæ˜¯å®‰å…¨çš„ï¼Œå®‰å…¨çº¦æŸå¯ä»¥æ˜¯åŸºäºäºŒå…ƒå®‰å…¨æ ‡ç­¾çš„çº¦æŸï¼›3) ä½¿ç”¨æ‹‰æ ¼æœ—æ—¥ä¹˜å­æ³•æ¥è§£å†³çº¦æŸä¼˜åŒ–é—®é¢˜ï¼Œå°†å®‰å…¨çº¦æŸè½¬åŒ–ä¸ºæ‹‰æ ¼æœ—æ—¥å‡½æ•°çš„ä¸€éƒ¨åˆ†ï¼Œä»è€Œå¯ä»¥ç›´æ¥å­¦ä¹ å®‰å…¨ç­–ç•¥ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.20173v1/figures/problem_setting.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.20173v1/figures/safety_ratio.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.20173v1/figures/seglen_reward.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒPreSaåœ¨è¿ç»­æ§åˆ¶ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸä¸ªä»»åŠ¡ä¸­ï¼ŒPreSaèƒ½å¤Ÿè¾¾åˆ°æ¯”ç°æœ‰æœ€ä½³åŸºçº¿é«˜å‡º15%çš„å¥–åŠ±ï¼ŒåŒæ—¶ä¿æŒè¾ƒé«˜çš„å®‰å…¨æ€§ã€‚æ­¤å¤–ï¼ŒPreSaè¿˜ä¼˜äºä½¿ç”¨çœŸå®å¥–åŠ±å’Œæˆæœ¬çš„ç¦»çº¿å®‰å…¨å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶åœ¨é¿å…å¥–åŠ±å’Œæˆæœ¬æ¨¡å‹è¯¯å·®æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººå®‰å…¨æ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—å†³ç­–ç­‰é¢†åŸŸã€‚åœ¨è¿™äº›é¢†åŸŸä¸­ï¼Œå®‰å…¨æ€§è‡³å…³é‡è¦ï¼Œå¹¶ä¸”éš¾ä»¥ç²¾ç¡®å»ºæ¨¡å¥–åŠ±å‡½æ•°ã€‚é€šè¿‡åˆ©ç”¨äººç±»çš„åå¥½å’Œå®‰å…¨åé¦ˆï¼Œå¯ä»¥è®­ç»ƒå‡ºæ›´å®‰å…¨ã€æ›´ç¬¦åˆäººç±»æœŸæœ›çš„æ™ºèƒ½ç³»ç»Ÿï¼Œä»è€Œæé«˜ç³»ç»Ÿçš„å¯é æ€§å’Œå®ç”¨æ€§ï¼Œå¹¶é™ä½æ½œåœ¨é£é™©ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Offline Preference-based Reinforcement Learning (PbRL) learns rewards and policies aligned with human preferences without the need for extensive reward engineering and direct interaction with human annotators. However, ensuring safety remains a critical challenge across many domains and tasks. Previous works on safe RL from human feedback (RLHF) first learn reward and cost models from offline data, then use constrained RL to optimize a safe policy. While such an approach works in the contextual bandits settings (LLMs), in long horizon continuous control tasks, errors in rewards and costs accumulate, leading to impairment in performance when used with constrained RL methods. To address these challenges, (a) instead of indirectly learning policies (from rewards and costs), we introduce a framework that learns a policy directly based on pairwise preferences regarding the agent's behavior in terms of rewards, as well as binary labels indicating the safety of trajectory segments; (b) we propose \textsc{PreSa} (Preference and Safety Alignment), a method that combines preference learning module with safety alignment in a constrained optimization problem. This optimization problem is solved within a Lagrangian paradigm that directly learns reward-maximizing safe policy \textit{without explicitly learning reward and cost models}, avoiding the need for constrained RL; (c) we evaluate our approach on continuous control tasks with both synthetic and real human feedback. Empirically, our method successfully learns safe policies with high rewards, outperforming state-of-the-art baselines, and offline safe RL approaches with ground-truth reward and cost.

