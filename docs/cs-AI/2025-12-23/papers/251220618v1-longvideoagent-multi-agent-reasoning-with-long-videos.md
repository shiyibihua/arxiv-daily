---
layout: default
title: LongVideoAgent: Multi-Agent Reasoning with Long Videos
---

# LongVideoAgent: Multi-Agent Reasoning with Long Videos

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.20618" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.20618v1</a>
  <a href="https://arxiv.org/pdf/2512.20618.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.20618v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.20618v1', 'LongVideoAgent: Multi-Agent Reasoning with Long Videos')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Runtao Liu, Ziyi Liu, Jiaqi Tang, Yue Ma, Renjie Pi, Jipeng Zhang, Qifeng Chen

**åˆ†ç±»**: cs.AI, cs.CV, cs.LG, cs.MA

**å‘å¸ƒæ—¥æœŸ**: 2025-12-23

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://longvideoagent.github.io/)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**LongVideoAgentï¼šæå‡ºä¸€ç§åŸºäºå¤šæ™ºèƒ½ä½“æ¨ç†çš„é•¿è§†é¢‘é—®ç­”æ¡†æ¶ï¼Œæå‡æ—¶åºå®šä½å’Œç»†èŠ‚æ•æ‰èƒ½åŠ›ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿è§†é¢‘é—®ç­”` `å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ` `å¼ºåŒ–å­¦ä¹ ` `æ—¶åºå®šä½` `è§†è§‰ä¿¡æ¯æå–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰é•¿è§†é¢‘é—®ç­”æ–¹æ³•ä¾èµ–æœ‰æŸå‹ç¼©æˆ–æœ‰é™çš„å·¥å…·é›†ï¼Œå‰Šå¼±äº†æ—¶åºå®šä½èƒ½åŠ›ï¼Œå¹¶å¯èƒ½é—æ¼ç»†ç²’åº¦çš„çº¿ç´¢ã€‚
2. æå‡ºä¸€ç§å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œåˆ©ç”¨ grounding agent å®šä½ç›¸å…³ç‰‡æ®µï¼Œvision agent æå–è§†è§‰ç»†èŠ‚ï¼Œä¸» LLM åè°ƒæ¨ç†ã€‚
3. åœ¨ LongTVQA å’Œ LongTVQA+ æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºéæ™ºèƒ½ä½“åŸºçº¿ï¼Œä¸”å¼ºåŒ–å­¦ä¹ èƒ½è¿›ä¸€æ­¥æå‡æ¨ç†å’Œè§„åˆ’èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œç”¨äºå¤„ç†é•¿è§†é¢‘é—®ç­”ä»»åŠ¡ã€‚è¯¥æ¡†æ¶ç”±ä¸€ä¸ªä¸»LLMåè°ƒï¼Œä¸€ä¸ªè´Ÿè´£å®šä½é—®é¢˜ç›¸å…³ç‰‡æ®µçš„ grounding agent å’Œä¸€ä¸ªæå–ç›®æ ‡æ–‡æœ¬è§‚æµ‹çš„ vision agentã€‚ä¸»æ™ºèƒ½ä½“åœ¨æ­¥æ•°é™åˆ¶ä¸‹è¿›è¡Œè§„åˆ’ï¼Œå¹¶é€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒï¼Œä»¥é¼“åŠ±ç®€æ´ã€æ­£ç¡®å’Œé«˜æ•ˆçš„å¤šæ™ºèƒ½ä½“åä½œã€‚è¿™ç§è®¾è®¡é€šè¿‡ grounding å¸®åŠ©ä¸»æ™ºèƒ½ä½“ä¸“æ³¨äºç›¸å…³ç‰‡æ®µï¼Œåˆ©ç”¨è§†è§‰ç»†èŠ‚è¡¥å……å­—å¹•ä¿¡æ¯ï¼Œå¹¶äº§ç”Ÿå¯è§£é‡Šçš„æ¨ç†è½¨è¿¹ã€‚åœ¨ä» TVQA/TVQA+ èšåˆè€Œæ¥çš„ episode-level æ•°æ®é›† LongTVQA å’Œ LongTVQA+ ä¸Šï¼Œæˆ‘ä»¬çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæ˜¾è‘—ä¼˜äºå¼ºå¤§çš„éæ™ºèƒ½ä½“åŸºçº¿ã€‚å®éªŒè¿˜è¡¨æ˜ï¼Œå¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥åŠ å¼ºäº†è®­ç»ƒåæ™ºèƒ½ä½“çš„æ¨ç†å’Œè§„åˆ’èƒ½åŠ›ã€‚ä»£ç å’Œæ•°æ®å°†åœ¨ https://longvideoagent.github.io/ ä¸Šå…±äº«ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šé•¿è§†é¢‘é—®ç­”ä»»åŠ¡éœ€è¦æ¨¡å‹å…·å¤‡åœ¨é•¿æ—¶é—´è·¨åº¦å†…è¿›è¡Œæ¨ç†çš„èƒ½åŠ›ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸é‡‡ç”¨æœ‰æŸçš„è§†é¢‘æ‘˜è¦æˆ–ä¾èµ–æœ‰é™çš„å·¥å…·é›†ï¼Œå¯¼è‡´æ—¶åºå®šä½ç²¾åº¦ä¸‹é™ï¼Œæ— æ³•æ•æ‰è§†é¢‘ä¸­çš„ç»†ç²’åº¦ä¿¡æ¯ï¼Œä»è€Œå½±å“é—®ç­”æ•ˆæœã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†é•¿è§†é¢‘é—®ç­”ä»»åŠ¡åˆ†è§£ä¸ºå¤šä¸ªæ™ºèƒ½ä½“åä½œå®Œæˆã€‚é€šè¿‡å¼•å…¥ grounding agent å’Œ vision agentï¼Œåˆ†åˆ«è´Ÿè´£å®šä½ç›¸å…³è§†é¢‘ç‰‡æ®µå’Œæå–è§†è§‰ä¿¡æ¯ï¼Œä»è€Œå‡è½»ä¸» LLM çš„è´Ÿæ‹…ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´ä¸“æ³¨äºæ¨ç†å’Œè§„åˆ’ã€‚è¿™ç§åˆ†å·¥åä½œçš„æ–¹å¼èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨è§†é¢‘ä¿¡æ¯ï¼Œæé«˜é—®ç­”çš„å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLongVideoAgent æ¡†æ¶åŒ…å«ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šä¸» LLMã€grounding agent å’Œ vision agentã€‚ä¸» LLM è´Ÿè´£æ¥æ”¶é—®é¢˜ï¼Œåè°ƒå…¶ä»–æ™ºèƒ½ä½“çš„å·¥ä½œï¼Œå¹¶æœ€ç»ˆç”Ÿæˆç­”æ¡ˆã€‚grounding agent è´Ÿè´£æ ¹æ®é—®é¢˜å®šä½è§†é¢‘ä¸­ç›¸å…³çš„ç‰‡æ®µã€‚vision agent è´Ÿè´£ä»å®šä½åˆ°çš„è§†é¢‘ç‰‡æ®µä¸­æå–è§†è§‰ä¿¡æ¯ï¼Œä¾‹å¦‚åœºæ™¯æè¿°ã€ç‰©ä½“è¯†åˆ«ç­‰ã€‚æ•´ä¸ªæµç¨‹å¦‚ä¸‹ï¼šä¸» LLM æ¥æ”¶é—®é¢˜ -> ä¸» LLM è°ƒç”¨ grounding agent å®šä½ç›¸å…³ç‰‡æ®µ -> ä¸» LLM è°ƒç”¨ vision agent æå–è§†è§‰ä¿¡æ¯ -> ä¸» LLM ç»“åˆé—®é¢˜ã€å®šä½ç‰‡æ®µå’Œè§†è§‰ä¿¡æ¯è¿›è¡Œæ¨ç† -> ä¸» LLM ç”Ÿæˆç­”æ¡ˆã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†ä¸€ä¸ªå¤šæ™ºèƒ½ä½“åä½œçš„æ¡†æ¶ï¼Œå°†é•¿è§†é¢‘é—®ç­”ä»»åŠ¡åˆ†è§£ä¸ºå¤šä¸ªå­ä»»åŠ¡ï¼Œå¹¶ç”±ä¸åŒçš„æ™ºèƒ½ä½“è´Ÿè´£å®Œæˆã€‚è¿™ç§åˆ†å·¥åä½œçš„æ–¹å¼èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨è§†é¢‘ä¿¡æ¯ï¼Œæé«˜é—®ç­”çš„å‡†ç¡®æ€§ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•ä¸éœ€è¦å¯¹è§†é¢‘è¿›è¡Œæœ‰æŸå‹ç¼©ï¼Œèƒ½å¤Ÿä¿ç•™æ›´å¤šçš„ç»†èŠ‚ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸» LLMï¼Œèƒ½å¤Ÿè¿›ä¸€æ­¥æé«˜å…¶æ¨ç†å’Œè§„åˆ’èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šä¸» LLM ä½¿ç”¨é¢„è®­ç»ƒçš„ LLMï¼Œå¹¶è¿›è¡Œå¾®è°ƒä»¥é€‚åº”é•¿è§†é¢‘é—®ç­”ä»»åŠ¡ã€‚Grounding agent å’Œ vision agent å¯ä»¥ä½¿ç”¨ç°æœ‰çš„ç›®æ ‡æ£€æµ‹ã€è§†é¢‘åˆ†å‰²ç­‰æ¨¡å‹ã€‚å¼ºåŒ–å­¦ä¹ çš„ç›®æ ‡æ˜¯é¼“åŠ±ä¸» LLM è¿›è¡Œç®€æ´ã€æ­£ç¡®å’Œé«˜æ•ˆçš„å¤šæ™ºèƒ½ä½“åä½œã€‚å¥–åŠ±å‡½æ•°å¯ä»¥è®¾è®¡ä¸ºåŸºäºç­”æ¡ˆçš„å‡†ç¡®æ€§ã€ä½¿ç”¨çš„æ­¥æ•°ç­‰å› ç´ ã€‚æ­¥æ•°é™åˆ¶æ˜¯ä¸ºäº†é˜²æ­¢ä¸» LLM æ— é™å¾ªç¯è°ƒç”¨å…¶ä»–æ™ºèƒ½ä½“ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.20618v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.20618v1/x2.png" alt="fig_1" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ LongTVQA å’Œ LongTVQA+ æ•°æ®é›†ä¸Šï¼ŒLongVideoAgent æ˜¾è‘—ä¼˜äºéæ™ºèƒ½ä½“åŸºçº¿ã€‚å…·ä½“è€Œè¨€ï¼ŒLongVideoAgent åœ¨ LongTVQA æ•°æ®é›†ä¸Šå–å¾—äº† X% çš„æå‡ï¼Œåœ¨ LongTVQA+ æ•°æ®é›†ä¸Šå–å¾—äº† Y% çš„æå‡ï¼ˆå…·ä½“æ•°å€¼æœªçŸ¥ï¼‰ã€‚æ­¤å¤–ï¼Œå®éªŒè¿˜è¡¨æ˜ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸» LLMï¼Œèƒ½å¤Ÿè¿›ä¸€æ­¥æé«˜å…¶æ¨ç†å’Œè§„åˆ’èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ™ºèƒ½å®¢æœã€è§†é¢‘å†…å®¹ç†è§£ã€æ™ºèƒ½å®‰é˜²ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨æ™ºèƒ½å®¢æœä¸­ï¼Œå¯ä»¥åˆ©ç”¨è¯¥æ–¹æ³•å¯¹ç”¨æˆ·ä¸Šä¼ çš„è§†é¢‘è¿›è¡Œåˆ†æï¼Œå¿«é€Ÿå®šä½é—®é¢˜å¹¶ç»™å‡ºè§£ç­”ã€‚åœ¨è§†é¢‘å†…å®¹ç†è§£ä¸­ï¼Œå¯ä»¥ç”¨äºè‡ªåŠ¨ç”Ÿæˆè§†é¢‘æ‘˜è¦ã€è§†é¢‘æ ‡ç­¾ç­‰ã€‚åœ¨æ™ºèƒ½å®‰é˜²ä¸­ï¼Œå¯ä»¥ç”¨äºç›‘æ§è§†é¢‘åˆ†æï¼Œè‡ªåŠ¨è¯†åˆ«å¼‚å¸¸è¡Œä¸ºã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in multimodal LLMs and systems that use tools for long-video QA point to the promise of reasoning over hour-long episodes. However, many methods still compress content into lossy summaries or rely on limited toolsets, weakening temporal grounding and missing fine-grained cues. We propose a multi-agent framework in which a master LLM coordinates a grounding agent to localize question-relevant segments and a vision agent to extract targeted textual observations. The master agent plans with a step limit, and is trained with reinforcement learning to encourage concise, correct, and efficient multi-agent cooperation. This design helps the master agent focus on relevant clips via grounding, complements subtitles with visual detail, and yields interpretable trajectories. On our proposed LongTVQA and LongTVQA+ which are episode-level datasets aggregated from TVQA/TVQA+, our multi-agent system significantly outperforms strong non-agent baselines. Experiments also show reinforcement learning further strengthens reasoning and planning for the trained agent. Code and data will be shared at https://longvideoagent.github.io/.

