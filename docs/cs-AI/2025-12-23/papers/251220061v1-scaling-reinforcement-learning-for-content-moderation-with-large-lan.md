---
layout: default
title: Scaling Reinforcement Learning for Content Moderation with Large Language Models
---

# Scaling Reinforcement Learning for Content Moderation with Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.20061" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.20061v1</a>
  <a href="https://arxiv.org/pdf/2512.20061.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.20061v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.20061v1', 'Scaling Reinforcement Learning for Content Moderation with Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hamed Firooz, Rui Liu, Yuchen Lu, Zhenyu Hou, Fangzhou Xiong, Xiaoyang Zhang, Changshu Jian, Zhicheng Zhu, Jiayuan Ma, Jacob Tao, Chaitali Gupta, Xiaochang Peng, Shike Mei, Hang Cui, Yang Qin, Shuo Tang, Jason Gaedtke, Arpit Mittal

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-23

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åˆ©ç”¨å¼ºåŒ–å­¦ä¹ å’Œå¤§å‹è¯­è¨€æ¨¡å‹æå‡å¤§è§„æ¨¡å†…å®¹å®¡æ ¸çš„æ•ˆç‡ä¸å‡†ç¡®æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å†…å®¹å®¡æ ¸` `å¼ºåŒ–å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `å¥–åŠ±å¡‘é€ ` `ç­–ç•¥å¯¹é½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å†…å®¹å®¡æ ¸æ–¹æ³•éš¾ä»¥åº”å¯¹æ ‡ç­¾ç¨€ç–ã€ç­–ç•¥æ¼”å˜å’Œå¤æ‚æ¨ç†ç­‰æŒ‘æˆ˜ï¼Œå¯¼è‡´å®¡æ ¸æ•ˆç‡å’Œå‡†ç¡®æ€§ä¸è¶³ã€‚
2. è®ºæ–‡æå‡ºåˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œé€šè¿‡å¥–åŠ±å¡‘é€ ç­–ç•¥ä½¿å…¶æˆä¸ºä¸“ä¸šçš„ã€ç¬¦åˆç­–ç•¥çš„åˆ†ç±»å™¨ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒRLæ–¹æ³•åœ¨å†…å®¹å®¡æ ¸ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰¯å¥½çš„æ‰©å±•æ€§ï¼Œæ•°æ®æ•ˆç‡æ¯”ç›‘ç£å¾®è°ƒé«˜å‡º100å€ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è§„æ¨¡å†…å®¹å®¡æ ¸æ˜¯å½“å‰æ•°å­—ç”Ÿæ€ç³»ç»Ÿé¢ä¸´çš„æœ€ç´§è¿«æŒ‘æˆ˜ä¹‹ä¸€ï¼Œéœ€è¦æŒç»­è¯„ä¼°æ•°åäº¿ç”¨æˆ·å’ŒAIç”Ÿæˆçš„å†…å®¹ï¼Œä»¥ç¡®å®šå…¶æ˜¯å¦è¿åæ”¿ç­–ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åŸºäºç­–ç•¥çš„å†…å®¹å®¡æ ¸æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†åœ¨å®é™…ç¯å¢ƒä¸­è®­ç»ƒè¿™äº›ç³»ç»Ÿä»¥è¾¾åˆ°ä¸“å®¶çº§å‡†ç¡®ç‡ä»ç„¶é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨æ ‡ç­¾ç¨€ç–ã€æ”¿ç­–å®šä¹‰ä¸æ–­æ¼”å˜ä»¥åŠéœ€è¦è¶…è¶Šæµ…å±‚æ¨¡å¼åŒ¹é…çš„ç»†è‡´æ¨ç†çš„æƒ…å†µä¸‹ã€‚æœ¬æ–‡å¯¹ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡Œå†…å®¹åˆ†ç±»çš„æ‰©å±•æ€§è¿›è¡Œäº†å…¨é¢çš„å®è¯ç ”ç©¶ï¼Œç³»ç»Ÿåœ°è¯„ä¼°äº†å¤šç§RLè®­ç»ƒæ–¹æ³•å’Œå¥–åŠ±å¡‘é€ ç­–ç•¥ï¼ˆåŒ…æ‹¬å¯éªŒè¯å¥–åŠ±å’ŒLLMä½œä¸ºè¯„åˆ¤æ¡†æ¶ï¼‰ï¼Œä»¥å°†é€šç”¨è¯­è¨€æ¨¡å‹è½¬åŒ–ä¸ºåœ¨ä¸‰ä¸ªçœŸå®å†…å®¹å®¡æ ¸ä»»åŠ¡ä¸­ä¸“é—¨çš„ã€ç¬¦åˆç­–ç•¥çš„åˆ†ç±»å™¨ã€‚ç ”ç©¶ç»“æœä¸ºå·¥ä¸šçº§å®¡æ ¸ç³»ç»Ÿæä¾›äº†å¯æ“ä½œçš„è§è§£ï¼Œè¡¨æ˜RLè¡¨ç°å‡ºç±»ä¼¼Sigmoidçš„æ‰©å±•è¡Œä¸ºï¼Œå³æ€§èƒ½éšç€è®­ç»ƒæ•°æ®ã€rolloutå’Œä¼˜åŒ–æ­¥éª¤çš„å¢åŠ è€Œå¹³ç¨³æé«˜ï¼Œç„¶åé€æ¸é¥±å’Œã€‚æ­¤å¤–ï¼Œç ”ç©¶è¡¨æ˜ï¼ŒRLåœ¨éœ€è¦å¤æ‚ç­–ç•¥æ¨ç†çš„ä»»åŠ¡ä¸Šæ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼ŒåŒæ—¶å®ç°äº†æ¯”ç›‘ç£å¾®è°ƒé«˜å‡º100å€çš„æ•°æ®æ•ˆç‡ï¼Œä½¿å…¶åœ¨ä¸“å®¶æ ‡æ³¨ç¨€ç¼ºæˆ–æˆæœ¬é«˜æ˜‚çš„é¢†åŸŸç‰¹åˆ«æœ‰æ•ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§è§„æ¨¡å†…å®¹å®¡æ ¸ä¸­ï¼Œç°æœ‰æ–¹æ³•åœ¨æ ‡ç­¾ç¨€ç–ã€ç­–ç•¥å®šä¹‰ä¸æ–­æ¼”å˜ä»¥åŠéœ€è¦å¤æ‚æ¨ç†çš„æƒ…å†µä¸‹ï¼Œéš¾ä»¥è¾¾åˆ°ä¸“å®¶çº§å‡†ç¡®ç‡çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚ç›‘ç£å­¦ä¹ ï¼Œéœ€è¦å¤§é‡çš„æ ‡æ³¨æ•°æ®ï¼Œæˆæœ¬é«˜æ˜‚ï¼Œä¸”éš¾ä»¥é€‚åº”ç­–ç•¥çš„å¿«é€Ÿå˜åŒ–ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œä½¿å…¶èƒ½å¤Ÿæ ¹æ®å†…å®¹å®¡æ ¸ç­–ç•¥è¿›è¡Œå†³ç­–ã€‚é€šè¿‡å¥–åŠ±å¡‘é€ ç­–ç•¥ï¼Œå¼•å¯¼LLMå­¦ä¹ ç¬¦åˆç­–ç•¥çš„åˆ†ç±»è¡Œä¸ºï¼Œä»è€Œåœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ä¹Ÿèƒ½è¾¾åˆ°è¾ƒé«˜çš„å‡†ç¡®ç‡ã€‚è¿™æ ·è®¾è®¡çš„ç›®çš„æ˜¯ä¸ºäº†æé«˜æ•°æ®æ•ˆç‡ï¼Œå¹¶ä½¿æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”ä¸æ–­å˜åŒ–çš„ç­–ç•¥ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1ï¼‰LLMä½œä¸ºAgentï¼Œè´Ÿè´£å¯¹å†…å®¹è¿›è¡Œåˆ†ç±»ï¼›2ï¼‰ç¯å¢ƒï¼Œæ¨¡æ‹Ÿå†…å®¹å®¡æ ¸åœºæ™¯ï¼Œæä¾›å†…å®¹æ ·æœ¬ï¼›3ï¼‰å¥–åŠ±å‡½æ•°ï¼Œæ ¹æ®LLMçš„åˆ†ç±»ç»“æœå’Œå®¡æ ¸ç­–ç•¥ï¼Œç»™å‡ºå¥–åŠ±ä¿¡å·ï¼›4ï¼‰RLç®—æ³•ï¼Œç”¨äºæ›´æ–°LLMçš„ç­–ç•¥ï¼Œä½¿å…¶èƒ½å¤Ÿè·å¾—æ›´é«˜çš„å¥–åŠ±ã€‚å…¶ä¸­ï¼Œå¥–åŠ±å‡½æ•°çš„è®¾è®¡è‡³å…³é‡è¦ï¼Œè®ºæ–‡æ¢ç´¢äº†å¤šç§å¥–åŠ±å¡‘é€ ç­–ç•¥ï¼ŒåŒ…æ‹¬å¯éªŒè¯å¥–åŠ±å’ŒLLMä½œä¸ºè¯„åˆ¤æ¡†æ¶ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå°†å¼ºåŒ–å­¦ä¹ åº”ç”¨äºå†…å®¹å®¡æ ¸ä»»åŠ¡ï¼Œå¹¶æ¢ç´¢äº†å¤šç§æœ‰æ•ˆçš„å¥–åŠ±å¡‘é€ ç­–ç•¥ã€‚ä¸ä¼ ç»Ÿçš„ç›‘ç£å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼ŒRLæ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨æœªæ ‡æ³¨æ•°æ®ï¼Œæé«˜æ•°æ®æ•ˆç‡ï¼Œå¹¶é€‚åº”ç­–ç•¥çš„å˜åŒ–ã€‚æ­¤å¤–ï¼Œä½¿ç”¨LLMä½œä¸ºè¯„åˆ¤æ¡†æ¶ï¼Œå¯ä»¥å‡å°‘å¯¹äººå·¥æ ‡æ³¨çš„ä¾èµ–ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­ï¼Œå¥–åŠ±å‡½æ•°çš„è®¾è®¡æ˜¯å…³é”®ã€‚å¯éªŒè¯å¥–åŠ±æ˜¯æŒ‡æ ¹æ®ä¸€äº›æ˜ç¡®çš„è§„åˆ™æˆ–æ ‡å‡†æ¥åˆ¤æ–­LLMçš„åˆ†ç±»ç»“æœæ˜¯å¦æ­£ç¡®ï¼Œå¹¶ç»™å‡ºç›¸åº”çš„å¥–åŠ±ã€‚LLMä½œä¸ºè¯„åˆ¤æ¡†æ¶æ˜¯æŒ‡ä½¿ç”¨å¦ä¸€ä¸ªLLMæ¥è¯„ä¼°LLMçš„åˆ†ç±»ç»“æœï¼Œå¹¶ç»™å‡ºå¥–åŠ±ä¿¡å·ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æ¢ç´¢äº†ä¸åŒçš„RLç®—æ³•ï¼Œå¦‚PPOç­‰ï¼Œä»¥åŠä¸åŒçš„è®­ç»ƒå‚æ•°è®¾ç½®ï¼Œä»¥ä¼˜åŒ–æ¨¡å‹çš„æ€§èƒ½ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.20061v1/img/reward_hacking.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.20061v1/img/instruction_halu.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.20061v1/img/factuality_halu.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒRLæ–¹æ³•åœ¨å†…å®¹å®¡æ ¸ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰¯å¥½çš„æ‰©å±•æ€§ï¼Œæ€§èƒ½éšç€è®­ç»ƒæ•°æ®ã€rolloutå’Œä¼˜åŒ–æ­¥éª¤çš„å¢åŠ è€Œå¹³ç¨³æé«˜ã€‚ä¸ç›‘ç£å¾®è°ƒç›¸æ¯”ï¼ŒRLæ–¹æ³•åœ¨éœ€è¦å¤æ‚ç­–ç•¥æ¨ç†çš„ä»»åŠ¡ä¸Šæ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼ŒåŒæ—¶å®ç°äº†é«˜è¾¾100å€çš„æ•°æ®æ•ˆç‡ã€‚è¿™è¡¨æ˜RLæ–¹æ³•åœ¨æ•°æ®ç¨€ç¼ºæˆ–æ ‡æ³¨æˆæœ¬é«˜æ˜‚çš„åœºæ™¯ä¸‹å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§åœ¨çº¿å¹³å°çš„å†…å®¹å®¡æ ¸ç³»ç»Ÿï¼Œä¾‹å¦‚ç¤¾äº¤åª’ä½“ã€è®ºå›ã€ç”µå•†å¹³å°ç­‰ã€‚é€šè¿‡ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„LLMï¼Œå¯ä»¥è‡ªåŠ¨è¯†åˆ«å’Œè¿‡æ»¤è¿åæ”¿ç­–çš„å†…å®¹ï¼Œå‡å°‘äººå·¥å®¡æ ¸çš„å·¥ä½œé‡ï¼Œæé«˜å®¡æ ¸æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼Œä»è€Œç»´æŠ¤å¥åº·çš„åœ¨çº¿ç¯å¢ƒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Content moderation at scale remains one of the most pressing challenges in today's digital ecosystem, where billions of user- and AI-generated artifacts must be continuously evaluated for policy violations. Although recent advances in large language models (LLMs) have demonstrated strong potential for policy-grounded moderation, the practical challenges of training these systems to achieve expert-level accuracy in real-world settings remain largely unexplored, particularly in regimes characterized by label sparsity, evolving policy definitions, and the need for nuanced reasoning beyond shallow pattern matching. In this work, we present a comprehensive empirical investigation of scaling reinforcement learning (RL) for content classification, systematically evaluating multiple RL training recipes and reward-shaping strategies-including verifiable rewards and LLM-as-judge frameworks-to transform general-purpose language models into specialized, policy-aligned classifiers across three real-world content moderation tasks. Our findings provide actionable insights for industrial-scale moderation systems, demonstrating that RL exhibits sigmoid-like scaling behavior in which performance improves smoothly with increased training data, rollouts, and optimization steps before gradually saturating. Moreover, we show that RL substantially improves performance on tasks requiring complex policy-grounded reasoning while achieving up to 100x higher data efficiency than supervised fine-tuning, making it particularly effective in domains where expert annotations are scarce or costly.

