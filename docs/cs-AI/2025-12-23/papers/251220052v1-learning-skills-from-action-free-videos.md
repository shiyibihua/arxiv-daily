---
layout: default
title: Learning Skills from Action-Free Videos
---

# Learning Skills from Action-Free Videos

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.20052" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.20052v1</a>
  <a href="https://arxiv.org/pdf/2512.20052.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.20052v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.20052v1', 'Learning Skills from Action-Free Videos')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hung-Chieh Fang, Kuo-Han Hung, Chu-Rong Chen, Po-Jung Chou, Chun-Kai Yang, Po-Chen Ko, Yu-Chiang Wang, Yueh-Hua Wu, Min-Hung Chen, Shao-Hua Sun

**åˆ†ç±»**: cs.AI, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-12-23

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå…‰æµçš„æŠ€èƒ½æŠ½è±¡æ¡†æ¶SOFï¼Œä»æ— åŠ¨ä½œè§†é¢‘ä¸­å­¦ä¹ æœºå™¨äººæŠ€èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `æœºå™¨äººæŠ€èƒ½å­¦ä¹ ` `æ— åŠ¨ä½œè§†é¢‘` `å…‰æµ` `æ½œåœ¨ç©ºé—´` `æŠ€èƒ½è§„åˆ’` `æœºå™¨äººæ§åˆ¶` `è§†è§‰å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†é¢‘ç”Ÿæˆæ¨¡å‹éš¾ä»¥è½¬åŒ–ä¸ºæœºå™¨äººå¯æ‰§è¡Œçš„ä½çº§åŠ¨ä½œï¼Œè€Œå•æ­¥æ½œåœ¨åŠ¨ä½œæ¨¡å‹ç¼ºä¹é«˜çº§è§„åˆ’èƒ½åŠ›ã€‚
2. SOFæ¡†æ¶é€šè¿‡å…‰æµä¸­é—´è¡¨ç¤ºå­¦ä¹ æ½œåœ¨æŠ€èƒ½ç©ºé—´ï¼Œè¯¥ç©ºé—´å¯¹é½è§†é¢‘åŠ¨æ€å’Œæœºå™¨äººåŠ¨ä½œï¼Œå®ç°æŠ€èƒ½çš„é«˜çº§è§„åˆ’ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒSOFåœ¨å¤šä»»åŠ¡å’Œé•¿æ—¶ç¨‹ä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºæ€§èƒ½æå‡ï¼ŒéªŒè¯äº†å…¶ä»åŸå§‹è§†é¢‘æ•°æ®å­¦ä¹ æŠ€èƒ½çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºâ€œåŸºäºå…‰æµçš„æŠ€èƒ½æŠ½è±¡â€ï¼ˆSOFï¼‰çš„æ¡†æ¶ï¼Œæ—¨åœ¨ä»å¤§é‡æ— åŠ¨ä½œè§†é¢‘ä¸­å­¦ä¹ æ½œåœ¨æŠ€èƒ½ï¼Œä»è€Œå¼¥åˆè§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸æœºå™¨äººåŠ¨ä½œæ‰§è¡Œä¹‹é—´çš„å·®è·ã€‚ç°æœ‰è§†é¢‘ç”Ÿæˆæ¨¡å‹è™½ç„¶èƒ½äº§ç”Ÿå‡ºè‰²çš„è§†è§‰é¢„æµ‹ï¼Œä½†éš¾ä»¥è½¬åŒ–ä¸ºä½çº§åŠ¨ä½œã€‚è€Œæ½œåœ¨åŠ¨ä½œæ¨¡å‹è™½ç„¶èƒ½æ›´å¥½åœ°å°†è§†é¢‘ä¸åŠ¨ä½œå¯¹é½ï¼Œä½†é€šå¸¸ä»…é™äºå•æ­¥æ“ä½œï¼Œç¼ºä¹é«˜çº§è§„åˆ’èƒ½åŠ›ã€‚SOFé€šè¿‡å­¦ä¹ åŸºäºå…‰æµçš„ä¸­é—´è¡¨ç¤ºæ¥å­¦ä¹ æ½œåœ¨æŠ€èƒ½ç©ºé—´ï¼Œå…‰æµèƒ½å¤Ÿæ•æ‰ä¸è§†é¢‘åŠ¨æ€å’Œæœºå™¨äººåŠ¨ä½œå¯¹é½çš„è¿åŠ¨ä¿¡æ¯ã€‚é€šè¿‡åœ¨åŸºäºæµçš„æ½œåœ¨ç©ºé—´ä¸­å­¦ä¹ æŠ€èƒ½ï¼ŒSOFèƒ½å¤Ÿå¯¹è§†é¢‘å¯¼å‡ºçš„æŠ€èƒ½è¿›è¡Œé«˜çº§è§„åˆ’ï¼Œå¹¶æ›´å®¹æ˜“åœ°å°†è¿™äº›æŠ€èƒ½è½¬åŒ–ä¸ºåŠ¨ä½œã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä»»åŠ¡å’Œé•¿æ—¶ç¨‹è®¾ç½®ä¸­å‡èƒ½æŒç»­æé«˜æ€§èƒ½ï¼Œè¯æ˜äº†å…¶ç›´æ¥ä»åŸå§‹è§†è§‰æ•°æ®ä¸­è·å–å’Œç»„åˆæŠ€èƒ½çš„èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ–¹æ³•åœ¨åˆ©ç”¨è§†é¢‘æ•°æ®å­¦ä¹ æœºå™¨äººæŠ€èƒ½æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚è§†é¢‘ç”Ÿæˆæ¨¡å‹æ“…é•¿è§†è§‰é¢„æµ‹ï¼Œä½†éš¾ä»¥è½¬åŒ–ä¸ºå…·ä½“çš„æœºå™¨äººåŠ¨ä½œæŒ‡ä»¤ã€‚å¦ä¸€æ–¹é¢ï¼ŒåŸºäºæ½œåœ¨åŠ¨ä½œçš„æ¨¡å‹è™½ç„¶èƒ½å°†è§†é¢‘ä¸åŠ¨ä½œå¯¹é½ï¼Œä½†é€šå¸¸åªèƒ½å¤„ç†å•æ­¥åŠ¨ä½œï¼Œç¼ºä¹é•¿æœŸè§„åˆ’èƒ½åŠ›ã€‚å› æ­¤ï¼Œå¦‚ä½•ä»æ— åŠ¨ä½œè§†é¢‘ä¸­å­¦ä¹ å¯ç”¨äºæœºå™¨äººé•¿æœŸè§„åˆ’çš„æŠ€èƒ½æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å…‰æµä½œä¸ºä¸­é—´è¡¨ç¤ºï¼Œå­¦ä¹ ä¸€ä¸ªæ½œåœ¨çš„æŠ€èƒ½ç©ºé—´ã€‚å…‰æµèƒ½å¤Ÿæ•æ‰è§†é¢‘ä¸­çš„è¿åŠ¨ä¿¡æ¯ï¼Œå¹¶ä¸”ä¸æœºå™¨äººåŠ¨ä½œå…·æœ‰ä¸€å®šçš„ç›¸å…³æ€§ã€‚é€šè¿‡åœ¨å…‰æµè¡¨ç¤ºçš„æ½œåœ¨ç©ºé—´ä¸­å­¦ä¹ æŠ€èƒ½ï¼Œå¯ä»¥æ›´å®¹æ˜“åœ°å°†è§†é¢‘ä¸­çš„è¿åŠ¨æ¨¡å¼è½¬åŒ–ä¸ºæœºå™¨äººå¯ä»¥æ‰§è¡Œçš„åŠ¨ä½œåºåˆ—ã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨å¼¥åˆè§†é¢‘ç”Ÿæˆæ¨¡å‹å’Œæœºå™¨äººåŠ¨ä½œæ‰§è¡Œä¹‹é—´çš„å·®è·ï¼Œå®ç°ä»è§†é¢‘æ•°æ®ä¸­å­¦ä¹ å¯å¤ç”¨çš„æŠ€èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSOFæ¡†æ¶åŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) å…‰æµä¼°è®¡æ¨¡å—ï¼Œç”¨äºä»è§†é¢‘å¸§ä¸­æå–å…‰æµä¿¡æ¯ï¼›2) æŠ€èƒ½ç¼–ç å™¨ï¼Œå°†å…‰æµä¿¡æ¯ç¼–ç åˆ°æ½œåœ¨æŠ€èƒ½ç©ºé—´ä¸­ï¼›3) æŠ€èƒ½è§£ç å™¨ï¼Œå°†æ½œåœ¨æŠ€èƒ½è§£ç ä¸ºå…‰æµåºåˆ—ï¼›4) æŠ€èƒ½è§„åˆ’å™¨ï¼Œç”¨äºåœ¨æ½œåœ¨æŠ€èƒ½ç©ºé—´ä¸­è¿›è¡Œé•¿æœŸè§„åˆ’ï¼Œç”ŸæˆæŠ€èƒ½åºåˆ—ï¼›5) åŠ¨ä½œæ‰§è¡Œå™¨ï¼Œå°†æŠ€èƒ½åºåˆ—è½¬åŒ–ä¸ºæœºå™¨äººåŠ¨ä½œæŒ‡ä»¤ã€‚æ•´ä¸ªæµç¨‹é¦–å…ˆä»è§†é¢‘ä¸­æå–å…‰æµï¼Œç„¶åé€šè¿‡ç¼–ç å™¨å­¦ä¹ æ½œåœ¨æŠ€èƒ½ï¼Œå†åˆ©ç”¨è§„åˆ’å™¨ç”ŸæˆæŠ€èƒ½åºåˆ—ï¼Œæœ€åé€šè¿‡æ‰§è¡Œå™¨è½¬åŒ–ä¸ºæœºå™¨äººåŠ¨ä½œã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºä½¿ç”¨å…‰æµä½œä¸ºä¸­é—´è¡¨ç¤ºæ¥å­¦ä¹ æ½œåœ¨æŠ€èƒ½ç©ºé—´ã€‚ä¸ç›´æ¥ä»åƒç´ ç©ºé—´å­¦ä¹ æŠ€èƒ½ç›¸æ¯”ï¼Œå…‰æµèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰è§†é¢‘ä¸­çš„è¿åŠ¨ä¿¡æ¯ï¼Œå¹¶ä¸”ä¸æœºå™¨äººåŠ¨ä½œå…·æœ‰æ›´å¼ºçš„ç›¸å…³æ€§ã€‚æ­¤å¤–ï¼ŒSOFæ¡†æ¶è¿˜å¼•å…¥äº†æŠ€èƒ½è§„åˆ’å™¨ï¼Œèƒ½å¤Ÿåœ¨æ½œåœ¨æŠ€èƒ½ç©ºé—´ä¸­è¿›è¡Œé•¿æœŸè§„åˆ’ï¼Œä»è€Œå®ç°æ›´å¤æ‚çš„ä»»åŠ¡ã€‚

**å…³é”®è®¾è®¡**ï¼šSOFæ¡†æ¶çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨é¢„è®­ç»ƒçš„å…‰æµä¼°è®¡ç½‘ç»œæå–å…‰æµä¿¡æ¯ï¼›2) ä½¿ç”¨å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰å­¦ä¹ æ½œåœ¨æŠ€èƒ½ç©ºé—´ï¼Œå¹¶å¼•å…¥KLæ•£åº¦æŸå¤±æ¥çº¦æŸæ½œåœ¨ç©ºé—´çš„åˆ†å¸ƒï¼›3) ä½¿ç”¨å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰ä½œä¸ºæŠ€èƒ½è§£ç å™¨ï¼Œç”Ÿæˆå…‰æµåºåˆ—ï¼›4) ä½¿ç”¨åŸºäºæ¨¡å‹çš„è§„åˆ’ç®—æ³•ï¼Œå¦‚æ¨¡å‹é¢„æµ‹æ§åˆ¶ï¼ˆMPCï¼‰ï¼Œåœ¨æ½œåœ¨æŠ€èƒ½ç©ºé—´ä¸­è¿›è¡Œé•¿æœŸè§„åˆ’ï¼›5) ä½¿ç”¨é€†è¿åŠ¨å­¦æ–¹æ³•å°†å…‰æµåºåˆ—è½¬åŒ–ä¸ºæœºå™¨äººåŠ¨ä½œæŒ‡ä»¤ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSOFæ¡†æ¶åœ¨å¤šä»»åŠ¡å’Œé•¿æ—¶ç¨‹ä»»åŠ¡ä¸­å‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨ç‰©ä½“æ“ä½œä»»åŠ¡ä¸­ï¼ŒSOFæ¡†æ¶èƒ½å¤ŸæˆåŠŸåœ°ä»è§†é¢‘ä¸­å­¦ä¹ åˆ°æŠ“å–ã€æ”¾ç½®ç­‰æŠ€èƒ½ï¼Œå¹¶å°†å…¶åº”ç”¨äºæ–°çš„ç‰©ä½“æ“ä½œä»»åŠ¡ä¸­ã€‚ä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒSOFæ¡†æ¶åœ¨ä»»åŠ¡æˆåŠŸç‡å’Œæ‰§è¡Œæ•ˆç‡æ–¹é¢å‡æœ‰æ˜æ˜¾æé«˜ã€‚å…·ä½“æå‡å¹…åº¦æœªçŸ¥ï¼Œéœ€æŸ¥é˜…è®ºæ–‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§æœºå™¨äººä»»åŠ¡ï¼Œä¾‹å¦‚å®¶åº­æœåŠ¡æœºå™¨äººã€å·¥ä¸šæœºå™¨äººå’Œè‡ªåŠ¨é©¾é©¶æ±½è½¦ã€‚é€šè¿‡ä»å¤§é‡æ— åŠ¨ä½œè§†é¢‘ä¸­å­¦ä¹ æŠ€èƒ½ï¼Œæœºå™¨äººå¯ä»¥æ›´å¥½åœ°ç†è§£å‘¨å›´ç¯å¢ƒï¼Œå¹¶æ‰§è¡Œå¤æ‚çš„ä»»åŠ¡ï¼Œä¾‹å¦‚ç‰©ä½“æ“ä½œã€å¯¼èˆªå’Œäººæœºäº¤äº’ã€‚è¯¥æŠ€æœ¯è¿˜å¯ä»¥ç”¨äºæœºå™¨äººæŠ€èƒ½çš„è¿ç§»å­¦ä¹ ï¼Œå°†ä»ä¸€ä¸ªä»»åŠ¡ä¸­å­¦åˆ°çš„æŠ€èƒ½åº”ç”¨åˆ°æ–°çš„ä»»åŠ¡ä¸­ï¼Œä»è€Œæé«˜æœºå™¨äººçš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Learning from videos offers a promising path toward generalist robots by providing rich visual and temporal priors beyond what real robot datasets contain. While existing video generative models produce impressive visual predictions, they are difficult to translate into low-level actions. Conversely, latent-action models better align videos with actions, but they typically operate at the single-step level and lack high-level planning capabilities. We bridge this gap by introducing Skill Abstraction from Optical Flow (SOF), a framework that learns latent skills from large collections of action-free videos. Our key idea is to learn a latent skill space through an intermediate representation based on optical flow that captures motion information aligned with both video dynamics and robot actions. By learning skills in this flow-based latent space, SOF enables high-level planning over video-derived skills and allows for easier translation of these skills into actions. Experiments show that our approach consistently improves performance in both multitask and long-horizon settings, demonstrating the ability to acquire and compose skills directly from raw visual data.

