---
layout: default
title: Advancing Multimodal Teacher Sentiment Analysis:The Large-Scale T-MED Dataset & The Effective AAM-TSA Model
---

# Advancing Multimodal Teacher Sentiment Analysis:The Large-Scale T-MED Dataset & The Effective AAM-TSA Model

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.20548" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.20548v1</a>
  <a href="https://arxiv.org/pdf/2512.20548.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.20548v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.20548v1', 'Advancing Multimodal Teacher Sentiment Analysis:The Large-Scale T-MED Dataset & The Effective AAM-TSA Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhiyi Duan, Xiangren Wang, Hongyu Yuan, Qianli Xing

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-23

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ„å»ºT-MEDæ•°æ®é›†ä¸AAM-TSAæ¨¡å‹ä»¥æå‡æ•™å¸ˆæƒ…æ„Ÿåˆ†æå‡†ç¡®æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ•™å¸ˆæƒ…æ„Ÿåˆ†æ` `å¤šæ¨¡æ€æ•°æ®` `éå¯¹ç§°æ³¨æ„åŠ›` `æƒ…æ„Ÿè®¡ç®—` `æ•™è‚²æŠ€æœ¯` `æ•°æ®é›†æ„å»º` `æ¨¡å‹åˆ›æ–°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨æ•æ‰æ•™å¸ˆæƒ…æ„Ÿæ—¶å­˜åœ¨ä¸è¶³ï¼Œæœªèƒ½å……åˆ†è€ƒè™‘æ•™å­¦ä¿¡æ¯å¯¹æƒ…æ„Ÿè¡¨è¾¾çš„å½±å“ã€‚
2. è®ºæ–‡æå‡ºäº†T-MEDæ•°æ®é›†å’ŒAAM-TSAæ¨¡å‹ï¼Œä»¥å®ç°æ•™å¸ˆæƒ…æ„Ÿçš„å¤šæ¨¡æ€åˆ†æå’Œç²¾å‡†åˆ†ç±»ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAAM-TSAåœ¨å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ•™å¸ˆçš„æƒ…æ„ŸçŠ¶æ€åœ¨æ•™è‚²åœºæ™¯ä¸­è‡³å…³é‡è¦ï¼Œç›´æ¥å½±å“æ•™å­¦æ•ˆæœã€å­¦ç”Ÿå‚ä¸åº¦å’Œå­¦ä¹ æˆå°±ã€‚ç„¶è€Œï¼Œç°æœ‰ç ”ç©¶å¸¸å¸¸æ— æ³•å‡†ç¡®æ•æ‰æ•™å¸ˆæƒ…æ„Ÿï¼Œå¿½è§†äº†æ•™å­¦ä¿¡æ¯å¯¹æƒ…æ„Ÿè¡¨è¾¾çš„å½±å“ã€‚æœ¬æ–‡ç³»ç»Ÿæ€§åœ°ç ”ç©¶äº†æ•™å¸ˆæƒ…æ„Ÿåˆ†æï¼Œæ„å»ºäº†é¦–ä¸ªå¤§è§„æ¨¡æ•™å¸ˆå¤šæ¨¡æ€æƒ…æ„Ÿåˆ†ææ•°æ®é›†T-MEDã€‚ä¸ºç¡®ä¿æ ‡æ³¨çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œé‡‡ç”¨äº†äººæœºåä½œçš„æ ‡æ³¨è¿‡ç¨‹ã€‚T-MEDæ•°æ®é›†åŒ…å«æ¥è‡ª250ä¸ªçœŸå®è¯¾å ‚çš„14,938ä¸ªæ•™å¸ˆæƒ…æ„Ÿæ•°æ®å®ä¾‹ï¼Œæ¶µç›–K-12è‡³é«˜ç­‰æ•™è‚²çš„11ä¸ªå­¦ç§‘ï¼Œæ•´åˆäº†å¤šæ¨¡æ€æ–‡æœ¬ã€éŸ³é¢‘ã€è§†é¢‘å’Œæ•™å­¦ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œæå‡ºäº†ä¸€ç§æ–°é¢–çš„åŸºäºéå¯¹ç§°æ³¨æ„åŠ›çš„å¤šæ¨¡æ€æ•™å¸ˆæƒ…æ„Ÿåˆ†ææ¨¡å‹AAM-TSAï¼Œå®éªŒç»“æœè¡¨æ˜AAM-TSAåœ¨T-MEDæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ•™å¸ˆæƒ…æ„Ÿåˆ†æä¸­çš„å‡†ç¡®æ€§é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆæ•æ‰æ•™å¸ˆæƒ…æ„Ÿï¼Œå°¤å…¶æ˜¯åœ¨å¤šæ¨¡æ€ä¿¡æ¯çš„æ•´åˆä¸Šå­˜åœ¨ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºäº†T-MEDæ•°æ®é›†å’ŒAAM-TSAæ¨¡å‹ï¼Œåˆ©ç”¨éå¯¹ç§°æ³¨æ„åŠ›æœºåˆ¶å’Œåˆ†å±‚é—¨æ§å•å…ƒï¼Œå®ç°è·¨æ¨¡æ€ç‰¹å¾çš„å·®å¼‚åŒ–èåˆå’Œç²¾ç¡®æƒ…æ„Ÿåˆ†ç±»ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šAAM-TSAæ¨¡å‹çš„æ•´ä½“æ¶æ„åŒ…æ‹¬è¾“å…¥å±‚ï¼ˆå¤šæ¨¡æ€æ•°æ®ï¼‰ã€ç‰¹å¾æå–å±‚ï¼ˆéŸ³é¢‘ã€è§†é¢‘ã€æ–‡æœ¬ç‰¹å¾æå–ï¼‰ã€éå¯¹ç§°æ³¨æ„åŠ›æœºåˆ¶å±‚å’Œæƒ…æ„Ÿåˆ†ç±»å±‚ï¼Œå½¢æˆä¸€ä¸ªå®Œæ•´çš„æƒ…æ„Ÿåˆ†ææµç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šAAM-TSAæ¨¡å‹çš„éå¯¹ç§°æ³¨æ„åŠ›æœºåˆ¶æ˜¯å…¶æ ¸å¿ƒåˆ›æ–°ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†å¤šæ¨¡æ€ä¿¡æ¯çš„å·®å¼‚æ€§ï¼Œæå‡æƒ…æ„Ÿåˆ†ç±»çš„å‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¨¡å‹è®¾è®¡ä¸­é‡‡ç”¨äº†åˆ†å±‚é—¨æ§å•å…ƒï¼Œä»¥å®ç°ä¸åŒæ¨¡æ€ç‰¹å¾çš„æœ‰æ•ˆèåˆï¼ŒæŸå¤±å‡½æ•°åˆ™é’ˆå¯¹å¤šæ¨¡æ€ç‰¹å¾çš„ç‰¹æ€§è¿›è¡Œäº†ä¼˜åŒ–ï¼Œç¡®ä¿æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„ç¨³å®šæ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.20548v1/w001-7.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.20548v1/w002-5.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.20548v1/w003-7.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒAAM-TSAæ¨¡å‹åœ¨T-MEDæ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡æ˜¾è‘—é«˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°XX%ï¼ŒåŒæ—¶åœ¨æƒ…æ„Ÿåˆ†ç±»çš„å¯è§£é‡Šæ€§æ–¹é¢ä¹Ÿæœ‰æ˜¾è‘—æ”¹å–„ï¼Œå±•ç¤ºäº†æ¨¡å‹çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²æŠ€æœ¯ã€æ•™å¸ˆåŸ¹è®­å’Œæƒ…æ„Ÿè®¡ç®—ç­‰ã€‚é€šè¿‡å‡†ç¡®åˆ†ææ•™å¸ˆæƒ…æ„Ÿï¼Œèƒ½å¤Ÿä¸ºæ•™è‚²å†³ç­–æä¾›æ•°æ®æ”¯æŒï¼Œæå‡æ•™å­¦è´¨é‡å’Œå­¦ç”Ÿå­¦ä¹ ä½“éªŒï¼Œæœªæ¥å¯èƒ½åœ¨æ™ºèƒ½æ•™è‚²ç³»ç»Ÿä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Teachers' emotional states are critical in educational scenarios, profoundly impacting teaching efficacy, student engagement, and learning achievements. However, existing studies often fail to accurately capture teachers' emotions due to the performative nature and overlook the critical impact of instructional information on emotional expression.In this paper, we systematically investigate teacher sentiment analysis by building both the dataset and the model accordingly. We construct the first large-scale teacher multimodal sentiment analysis dataset, T-MED.To ensure labeling accuracy and efficiency, we employ a human-machine collaborative labeling process.The T-MED dataset includes 14,938 instances of teacher emotional data from 250 real classrooms across 11 subjects ranging from K-12 to higher education, integrating multimodal text, audio, video, and instructional information.Furthermore, we propose a novel asymmetric attention-based multimodal teacher sentiment analysis model, AAM-TSA.AAM-TSA introduces an asymmetric attention mechanism and hierarchical gating unit to enable differentiated cross-modal feature fusion and precise emotional classification. Experimental results demonstrate that AAM-TSA significantly outperforms existing state-of-the-art methods in terms of accuracy and interpretability on the T-MED dataset.

