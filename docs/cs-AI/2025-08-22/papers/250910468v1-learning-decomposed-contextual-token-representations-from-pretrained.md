---
layout: default
title: Learning Decomposed Contextual Token Representations from Pretrained and Collaborative Signals for Generative Recommendation
---

# Learning Decomposed Contextual Token Representations from Pretrained and Collaborative Signals for Generative Recommendation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.10468" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.10468v1</a>
  <a href="https://arxiv.org/pdf/2509.10468.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.10468v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.10468v1', 'Learning Decomposed Contextual Token Representations from Pretrained and Collaborative Signals for Generative Recommendation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yifan Liu, Yaokun Liu, Zelin Li, Zhenrui Yue, Gyuseok Lee, Ruichen Yao, Yang Zhang, Dong Wang

**åˆ†ç±»**: cs.IR, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-08-22

**å¤‡æ³¨**: preprint under review

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDECORæ¡†æ¶ä»¥è§£å†³æ¨èç³»ç»Ÿä¸­çš„è¯­ä¹‰é‡å»ºä¸ç”¨æˆ·äº¤äº’å»ºæ¨¡ä¸ä¸€è‡´é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç”Ÿæˆæ¨èç³»ç»Ÿ` `ä¸Šä¸‹æ–‡åŒ–æ ‡è®°` `åˆ†è§£åµŒå…¥èåˆ` `é¢„è®­ç»ƒè¯­ä¹‰` `ç”¨æˆ·äº¤äº’å»ºæ¨¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç”Ÿæˆæ¨èç³»ç»Ÿåœ¨æ ‡è®°åŒ–å’Œæ¨èè®­ç»ƒé˜¶æ®µçš„ç›®æ ‡ä¸ä¸€è‡´ï¼Œå¯¼è‡´é™æ€æ ‡è®°æ— æ³•é€‚åº”å¤šæ ·åŒ–çš„ä½¿ç”¨åœºæ™¯ã€‚
2. æå‡ºDECORæ¡†æ¶ï¼Œé€šè¿‡ä¸Šä¸‹æ–‡åŒ–çš„æ ‡è®°ç»„åˆå’Œåˆ†è§£åµŒå…¥èåˆï¼Œä¿ç•™é¢„è®­ç»ƒçŸ¥è¯†å¹¶å¢å¼ºæ ‡è®°åµŒå…¥çš„é€‚åº”æ€§ã€‚
3. åœ¨ä¸‰ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDECORåœ¨æ¨èæ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºçº¿æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œç”Ÿæˆæ¨èç³»ç»Ÿé‡‡ç”¨ä¸¤é˜¶æ®µèŒƒå¼ï¼šé¦–å…ˆé€šè¿‡é¢„è®­ç»ƒçš„åˆ†è¯å™¨å°†é¡¹ç›®æ ‡è®°ä¸ºè¯­ä¹‰IDï¼Œç„¶åä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆä¸‹ä¸€ä¸ªé¡¹ç›®ã€‚ç„¶è€Œï¼Œè¿™ä¸¤ä¸ªé˜¶æ®µçš„ä¼˜åŒ–ç›®æ ‡ä¸åŒï¼Œå¯¼è‡´äº†é™æ€æ ‡è®°ä¸ä½³å’Œé¢„è®­ç»ƒè¯­ä¹‰ä¸¢å¤±ç­‰é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†DEcomposed COntextual Token Representationsï¼ˆDECORï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨ä¿ç•™é¢„è®­ç»ƒè¯­ä¹‰çš„åŒæ—¶å¢å¼ºæ ‡è®°åµŒå…¥çš„é€‚åº”æ€§ã€‚DECORå¼•å…¥äº†ä¸Šä¸‹æ–‡åŒ–çš„æ ‡è®°ç»„åˆï¼Œä»¥æ ¹æ®ç”¨æˆ·äº¤äº’ä¸Šä¸‹æ–‡æ¥ç»†åŒ–æ ‡è®°åµŒå…¥ï¼Œå¹¶é€šè¿‡åˆ†è§£åµŒå…¥èåˆå°†é¢„è®­ç»ƒçš„ä»£ç æœ¬åµŒå…¥ä¸æ–°å­¦ä¹ çš„åä½œåµŒå…¥ç›¸ç»“åˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDECORåœ¨æ¨èæ€§èƒ½ä¸Šå§‹ç»ˆä¼˜äºæœ€å…ˆè¿›çš„åŸºçº¿æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç”Ÿæˆæ¨èç³»ç»Ÿä¸­æ ‡è®°åŒ–ä¸æ¨èè®­ç»ƒé˜¶æ®µç›®æ ‡ä¸ä¸€è‡´çš„é—®é¢˜ï¼Œå¯¼è‡´é™æ€æ ‡è®°æ— æ³•åæ˜ å¤šæ ·åŒ–çš„ä½¿ç”¨ä¸Šä¸‹æ–‡ï¼Œä»¥åŠé¢„è®­ç»ƒè¯­ä¹‰åœ¨ç”¨æˆ·äº¤äº’è®­ç»ƒä¸­è¢«è¦†ç›–çš„ç°è±¡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºDECORæ¡†æ¶ï¼Œé€šè¿‡ä¸Šä¸‹æ–‡åŒ–çš„æ ‡è®°ç»„åˆæ¥ç»†åŒ–æ ‡è®°åµŒå…¥ï¼Œå¹¶é€šè¿‡åˆ†è§£åµŒå…¥èåˆå°†é¢„è®­ç»ƒçš„çŸ¥è¯†ä¸æ–°å­¦ä¹ çš„åä½œåµŒå…¥ç»“åˆï¼Œä»¥æå‡æ¨èç³»ç»Ÿçš„æ€§èƒ½å’Œé€‚åº”æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDECORæ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šä¸Šä¸‹æ–‡åŒ–æ ‡è®°ç»„åˆæ¨¡å—å’Œåˆ†è§£åµŒå…¥èåˆæ¨¡å—ã€‚å‰è€…æ ¹æ®ç”¨æˆ·äº¤äº’ä¸Šä¸‹æ–‡åŠ¨æ€è°ƒæ•´æ ‡è®°åµŒå…¥ï¼Œåè€…åˆ™å°†é¢„è®­ç»ƒçš„ä»£ç æœ¬åµŒå…¥ä¸æ–°å­¦ä¹ çš„åä½œåµŒå…¥è¿›è¡Œæœ‰æ•ˆæ•´åˆã€‚

**å…³é”®åˆ›æ–°**ï¼šDECORçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå¼•å…¥äº†ä¸Šä¸‹æ–‡åŒ–çš„æ ‡è®°ç»„åˆå’Œåˆ†è§£åµŒå…¥èåˆæœºåˆ¶ï¼Œèƒ½å¤ŸåŒæ—¶ä¿ç•™é¢„è®­ç»ƒçš„è¯­ä¹‰ä¿¡æ¯å¹¶å¢å¼ºæ ‡è®°çš„é€‚åº”æ€§ï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•çš„é™æ€æ ‡è®°åŒ–æ–¹å¼å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒDECORé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥å¹³è¡¡é¢„è®­ç»ƒè¯­ä¹‰ä¸ç”¨æˆ·äº¤äº’ä¿¡æ¯çš„èåˆï¼ŒåŒæ—¶åœ¨ç½‘ç»œç»“æ„ä¸Šå®ç°äº†çµæ´»çš„åµŒå…¥è°ƒæ•´æœºåˆ¶ï¼Œä»¥é€‚åº”ä¸åŒçš„ç”¨æˆ·è¡Œä¸ºæ¨¡å¼ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒDECORåœ¨ä¸‰ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šå‡æ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„åŸºçº¿æ–¹æ³•ï¼Œæ¨èæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°10%ä»¥ä¸Šï¼ŒéªŒè¯äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§å’Œä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç”µå­å•†åŠ¡ã€ç¤¾äº¤åª’ä½“å’Œå†…å®¹æ¨èç­‰åœºæ™¯ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡ä¸ªæ€§åŒ–æ¨èçš„å‡†ç¡®æ€§å’Œç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼ŒDECORæ¡†æ¶å¯æ‰©å±•è‡³æ›´å¤šé¢†åŸŸï¼Œæ¨åŠ¨æ¨èç³»ç»Ÿçš„æ™ºèƒ½åŒ–å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in generative recommenders adopt a two-stage paradigm: items are first tokenized into semantic IDs using a pretrained tokenizer, and then large language models (LLMs) are trained to generate the next item via sequence-to-sequence modeling. However, these two stages are optimized for different objectives: semantic reconstruction during tokenizer pretraining versus user interaction modeling during recommender training. This objective misalignment leads to two key limitations: (i) suboptimal static tokenization, where fixed token assignments fail to reflect diverse usage contexts; and (ii) discarded pretrained semantics, where pretrained knowledge - typically from language model embeddings - is overwritten during recommender training on user interactions. To address these limitations, we propose to learn DEcomposed COntextual Token Representations (DECOR), a unified framework that preserves pretrained semantics while enhancing the adaptability of token embeddings. DECOR introduces contextualized token composition to refine token embeddings based on user interaction context, and decomposed embedding fusion that integrates pretrained codebook embeddings with newly learned collaborative embeddings. Experiments on three real-world datasets demonstrate that DECOR consistently outperforms state-of-the-art baselines in recommendation performance. Our code will be made available upon publication.

