---
layout: default
title: Learning General Policies with Policy Gradient Methods
---

# Learning General Policies with Policy Gradient Methods

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.19366" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.19366v1</a>
  <a href="https://arxiv.org/pdf/2512.19366.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.19366v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.19366v1', 'Learning General Policies with Policy Gradient Methods')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Simon StÃ¥hlberg, Blai Bonet, Hector Geffner

**åˆ†ç±»**: cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-12-22

**å¤‡æ³¨**: In Proceedings of the 20th International Conference on Principles of Knowledge Representation and Reasoning (KR 2023)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå›¾ç¥ç»ç½‘ç»œçš„ç­–ç•¥æ¢¯åº¦æ–¹æ³•ï¼Œå­¦ä¹ å¯æ³›åŒ–çš„é€šç”¨ç­–ç•¥**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `ç­–ç•¥æ¢¯åº¦` `å›¾ç¥ç»ç½‘ç»œ` `æ³›åŒ–` `è§„åˆ’`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨æ³›åŒ–èƒ½åŠ›ä¸Šå­˜åœ¨æŒ‘æˆ˜ï¼Œéš¾ä»¥ç”Ÿæˆå¯é ä¸”ç³»ç»Ÿæ€§çš„é€šç”¨ç­–ç•¥ã€‚
2. è®ºæ–‡æå‡ºå°†ç­–ç•¥å»ºæ¨¡ä¸ºçŠ¶æ€è½¬ç§»åˆ†ç±»å™¨ï¼Œå¹¶åˆ©ç”¨å›¾ç¥ç»ç½‘ç»œå¤„ç†å…³ç³»ç»“æ„ï¼Œå­¦ä¹ é€šç”¨ç­–ç•¥ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ³›åŒ–èƒ½åŠ›ä¸Šæ¥è¿‘ç»„åˆæ–¹æ³•ï¼ŒåŒæ—¶é¿å…äº†å¯æ‰©å±•æ€§ç“¶é¢ˆï¼Œå¹¶é€šè¿‡ä¼˜åŒ–æˆæœ¬ç»“æ„è§£å†³äº†GNNçš„å±€é™æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨è®¸å¤šé¢†åŸŸå–å¾—äº†æ˜¾è‘—æˆæœï¼Œä½†æ³›åŒ–èƒ½åŠ›ï¼Œå³ä»¥å¯é å’Œç³»ç»Ÿçš„æ–¹å¼ç”Ÿæˆå¯æ³›åŒ–ç­–ç•¥çš„èƒ½åŠ›ï¼Œä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æ³›åŒ–é—®é¢˜åœ¨ç»å…¸è§„åˆ’ä¸­å¾—åˆ°äº†æ­£å¼è§£å†³ï¼Œå…¶ä¸­ä½¿ç”¨ç»„åˆæ–¹æ³•å­¦ä¹ äº†å¯åœ¨ç»™å®šé¢†åŸŸçš„æ‰€æœ‰å®ä¾‹ä¸­æ³›åŒ–çš„å¯è¯æ˜æ­£ç¡®çš„ç­–ç•¥ã€‚æœ¬ç ”ç©¶æ—¨åœ¨å°†è¿™ä¸¤ä¸ªç ”ç©¶æ–¹å‘ç»“åˆèµ·æ¥ï¼Œé˜æ˜åœ¨ä½•ç§æ¡ä»¶ä¸‹ï¼Œï¼ˆæ·±åº¦ï¼‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ï¼Œå¯ä»¥åƒç»„åˆæ–¹æ³•ä¸€æ ·ç”¨äºå­¦ä¹ å¯æ³›åŒ–çš„ç­–ç•¥ã€‚æˆ‘ä»¬å€Ÿé‰´äº†å…ˆå‰ç»„åˆå’Œæ·±åº¦å­¦ä¹ æ–¹æ³•çš„ç»éªŒï¼Œå¹¶ä»¥ä¸€ç§æ–¹ä¾¿çš„æ–¹å¼å¯¹å…¶è¿›è¡Œäº†æ‰©å±•ã€‚ä¸å‰è€…ç±»ä¼¼ï¼Œæˆ‘ä»¬å°†ç­–ç•¥å»ºæ¨¡ä¸ºçŠ¶æ€è½¬ç§»åˆ†ç±»å™¨ï¼Œå› ä¸ºï¼ˆgroundï¼‰åŠ¨ä½œä¸æ˜¯é€šç”¨çš„ï¼Œå¹¶ä¸”å› å®ä¾‹è€Œå¼‚ã€‚ä¸åè€…ç±»ä¼¼ï¼Œæˆ‘ä»¬ä½¿ç”¨é€‚ç”¨äºå¤„ç†å…³ç³»ç»“æ„çš„å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰æ¥è¡¨ç¤ºè§„åˆ’çŠ¶æ€ä¸Šçš„ä»·å€¼å‡½æ•°ï¼Œåœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œæ˜¯ç­–ç•¥ã€‚æœ‰äº†è¿™äº›è¦ç´ ï¼Œæˆ‘ä»¬å‘ç°actor-criticæ–¹æ³•å¯ä»¥ç”¨äºå­¦ä¹ å‡ ä¹ä¸ä½¿ç”¨ç»„åˆæ–¹æ³•è·å¾—çš„ç­–ç•¥ä¸€æ ·å¥½çš„ç­–ç•¥ï¼ŒåŒæ—¶é¿å…äº†å¯æ‰©å±•æ€§ç“¶é¢ˆå’Œç‰¹å¾æ± çš„ä½¿ç”¨ã€‚æ­¤å¤–ï¼ŒDRLæ–¹æ³•åœ¨æ‰€è€ƒè™‘çš„åŸºå‡†æµ‹è¯•ä¸­çš„å±€é™æ€§ä¸æ·±åº¦å­¦ä¹ æˆ–å¼ºåŒ–å­¦ä¹ ç®—æ³•æ— å…³ï¼Œè€Œæ˜¯æºäºGNNçš„è¡¨è¾¾èƒ½åŠ›é™åˆ¶ï¼Œä»¥åŠæœ€ä¼˜æ€§å’Œæ³›åŒ–ä¹‹é—´çš„æƒè¡¡ï¼ˆé€šç”¨ç­–ç•¥åœ¨æŸäº›é¢†åŸŸå¯èƒ½ä¸æ˜¯æœ€ä¼˜çš„ï¼‰ã€‚é€šè¿‡æ·»åŠ æ´¾ç”Ÿè°“è¯å’Œä¼˜åŒ–æ›¿ä»£æˆæœ¬ç»“æ„ï¼Œå¯ä»¥åœ¨ä¸æ”¹å˜åŸºæœ¬DRLæ–¹æ³•çš„æƒ…å†µä¸‹è§£å†³è¿™ä¸¤ä¸ªé™åˆ¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨è§„åˆ’é—®é¢˜ä¸­éš¾ä»¥æ³›åŒ–åˆ°ä¸åŒçš„å®ä¾‹ã€‚ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•é€šå¸¸å­¦ä¹ é’ˆå¯¹ç‰¹å®šå®ä¾‹çš„ç­–ç•¥ï¼Œè€Œæ— æ³•æ¨å¹¿åˆ°å…·æœ‰ä¸åŒåˆå§‹çŠ¶æ€æˆ–ç›®æ ‡çŠ¶æ€çš„ç›¸åŒé¢†åŸŸçš„æ–°å®ä¾‹ã€‚æ­¤å¤–ï¼Œç»„åˆè§„åˆ’æ–¹æ³•è™½ç„¶å¯ä»¥ç”Ÿæˆå¯è¯æ˜æ­£ç¡®çš„é€šç”¨ç­–ç•¥ï¼Œä½†åœ¨å¯æ‰©å±•æ€§æ–¹é¢å­˜åœ¨ç“¶é¢ˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†ç­–ç•¥è¡¨ç¤ºä¸ºçŠ¶æ€è½¬ç§»åˆ†ç±»å™¨ï¼Œè€Œä¸æ˜¯ç›´æ¥å­¦ä¹ åŠ¨ä½œã€‚è¿™ç§è¡¨ç¤ºæ–¹å¼å…è®¸ç­–ç•¥åœ¨ä¸åŒçš„å®ä¾‹ä¹‹é—´å…±äº«çŸ¥è¯†ï¼Œå› ä¸ºçŠ¶æ€è½¬ç§»è§„åˆ™åœ¨åŒä¸€é¢†åŸŸå†…æ˜¯é€šç”¨çš„ã€‚æ­¤å¤–ï¼Œè®ºæ–‡åˆ©ç”¨å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰æ¥å¤„ç†è§„åˆ’çŠ¶æ€çš„å…³ç³»ç»“æ„ï¼Œä»è€Œæ›´å¥½åœ°æ•æ‰çŠ¶æ€ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•é‡‡ç”¨Actor-Criticæ¡†æ¶ï¼Œå…¶ä¸­Actorä½¿ç”¨GNNæ¥è¡¨ç¤ºç­–ç•¥ï¼ŒCriticä½¿ç”¨GNNæ¥è¯„ä¼°ç­–ç•¥çš„ä»·å€¼ã€‚Actorçš„ç›®æ ‡æ˜¯å­¦ä¹ ä¸€ä¸ªç­–ç•¥ï¼Œè¯¥ç­–ç•¥å¯ä»¥é¢„æµ‹ç»™å®šçŠ¶æ€ä¸‹é‡‡å–å“ªä¸ªåŠ¨ä½œä¼šå¯¼è‡´æœŸæœ›çš„çŠ¶æ€è½¬ç§»ã€‚Criticçš„ç›®æ ‡æ˜¯å­¦ä¹ ä¸€ä¸ªä»·å€¼å‡½æ•°ï¼Œè¯¥å‡½æ•°å¯ä»¥é¢„æµ‹ç»™å®šçŠ¶æ€ä¸‹éµå¾ªç­–ç•¥çš„é¢„æœŸå›æŠ¥ã€‚è®­ç»ƒè¿‡ç¨‹ä½¿ç”¨ç­–ç•¥æ¢¯åº¦æ–¹æ³•ï¼Œé€šè¿‡æœ€å¤§åŒ–é¢„æœŸå›æŠ¥æ¥æ›´æ–°Actorå’ŒCriticçš„å‚æ•°ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•çš„å…³é”®åˆ›æ–°åœ¨äºå°†ç­–ç•¥å»ºæ¨¡ä¸ºçŠ¶æ€è½¬ç§»åˆ†ç±»å™¨ï¼Œå¹¶ä½¿ç”¨GNNæ¥å¤„ç†è§„åˆ’çŠ¶æ€çš„å…³ç³»ç»“æ„ã€‚è¿™ç§ç»„åˆä½¿å¾—è¯¥æ–¹æ³•èƒ½å¤Ÿå­¦ä¹ å¯æ³›åŒ–çš„é€šç”¨ç­–ç•¥ï¼ŒåŒæ—¶é¿å…äº†ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ æ–¹æ³•çš„å¯æ‰©å±•æ€§é—®é¢˜ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†ä¸€ç§ä¼˜åŒ–æˆæœ¬ç»“æ„çš„æ–¹æ³•ï¼Œä»¥è§£å†³GNNçš„è¡¨è¾¾èƒ½åŠ›é™åˆ¶ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä½¿ç”¨æ¶ˆæ¯ä¼ é€’ç¥ç»ç½‘ç»œï¼ˆMPNNï¼‰ä½œä¸ºGNNçš„éª¨å¹²ç½‘ç»œã€‚çŠ¶æ€è¡¨ç¤ºä¸ºå›¾ä¸­çš„èŠ‚ç‚¹ï¼ŒèŠ‚ç‚¹ä¹‹é—´çš„è¾¹è¡¨ç¤ºçŠ¶æ€ä¹‹é—´çš„å…³ç³»ã€‚æ¶ˆæ¯ä¼ é€’è¿‡ç¨‹ç”¨äºèšåˆæ¥è‡ªç›¸é‚»èŠ‚ç‚¹çš„ä¿¡æ¯ï¼Œä»è€Œæ›´æ–°èŠ‚ç‚¹çš„çŠ¶æ€è¡¨ç¤ºã€‚ç­–ç•¥ç½‘ç»œä½¿ç”¨æ›´æ–°åçš„èŠ‚ç‚¹è¡¨ç¤ºæ¥é¢„æµ‹çŠ¶æ€è½¬ç§»æ¦‚ç‡ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬ç­–ç•¥æ¢¯åº¦æŸå¤±å’Œä»·å€¼å‡½æ•°æŸå¤±ã€‚ä¸ºäº†è§£å†³GNNçš„è¡¨è¾¾èƒ½åŠ›é™åˆ¶ï¼Œè®ºæ–‡æ·»åŠ äº†æ´¾ç”Ÿè°“è¯ï¼Œè¿™äº›è°“è¯æ˜¯åŸºäºåŸå§‹çŠ¶æ€å˜é‡è®¡ç®—å¾—åˆ°çš„ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜ä½¿ç”¨äº†ä¸€ç§æ›¿ä»£æˆæœ¬ç»“æ„ï¼Œè¯¥ç»“æ„é¼“åŠ±ç­–ç•¥å­¦ä¹ æ›´æœ‰æ•ˆçš„çŠ¶æ€è½¬ç§»ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªè§„åˆ’åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†ä¸ç»„åˆæ–¹æ³•ç›¸å½“çš„æ³›åŒ–æ€§èƒ½ï¼ŒåŒæ—¶é¿å…äº†ç»„åˆæ–¹æ³•çš„å¯æ‰©å±•æ€§ç“¶é¢ˆã€‚ä¾‹å¦‚ï¼Œåœ¨æŸäº›é¢†åŸŸï¼Œè¯¥æ–¹æ³•å¯ä»¥å­¦ä¹ åœ¨æ‰€æœ‰å®ä¾‹ä¸­éƒ½è¡¨ç°è‰¯å¥½çš„ç­–ç•¥ï¼Œè€Œä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ æ–¹æ³•åªèƒ½å­¦ä¹ é’ˆå¯¹ç‰¹å®šå®ä¾‹çš„ç­–ç•¥ã€‚æ­¤å¤–ï¼Œé€šè¿‡æ·»åŠ æ´¾ç”Ÿè°“è¯å’Œä¼˜åŒ–æˆæœ¬ç»“æ„ï¼Œè¯¥æ–¹æ³•å¯ä»¥è§£å†³GNNçš„è¡¨è¾¾èƒ½åŠ›é™åˆ¶ï¼Œè¿›ä¸€æ­¥æé«˜æ³›åŒ–æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§è§„åˆ’å’Œå†³ç­–é—®é¢˜ï¼Œä¾‹å¦‚æœºå™¨äººå¯¼èˆªã€æ¸¸æˆAIå’Œèµ„æºè°ƒåº¦ã€‚é€šè¿‡å­¦ä¹ å¯æ³›åŒ–çš„é€šç”¨ç­–ç•¥ï¼Œå¯ä»¥ä½¿æ™ºèƒ½ä½“åœ¨é¢å¯¹æ–°çš„ç¯å¢ƒå’Œä»»åŠ¡æ—¶æ›´åŠ çµæ´»å’Œé«˜æ•ˆã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºè§£å†³ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ æ–¹æ³•éš¾ä»¥å¤„ç†çš„å¤æ‚è§„åˆ’é—®é¢˜ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> While reinforcement learning methods have delivered remarkable results in a number of settings, generalization, i.e., the ability to produce policies that generalize in a reliable and systematic way, has remained a challenge. The problem of generalization has been addressed formally in classical planning where provable correct policies that generalize over all instances of a given domain have been learned using combinatorial methods. The aim of this work is to bring these two research threads together to illuminate the conditions under which (deep) reinforcement learning approaches, and in particular, policy optimization methods, can be used to learn policies that generalize like combinatorial methods do. We draw on lessons learned from previous combinatorial and deep learning approaches, and extend them in a convenient way. From the former, we model policies as state transition classifiers, as (ground) actions are not general and change from instance to instance. From the latter, we use graph neural networks (GNNs) adapted to deal with relational structures for representing value functions over planning states, and in our case, policies. With these ingredients in place, we find that actor-critic methods can be used to learn policies that generalize almost as well as those obtained using combinatorial approaches while avoiding the scalability bottleneck and the use of feature pools. Moreover, the limitations of the DRL methods on the benchmarks considered have little to do with deep learning or reinforcement learning algorithms, and result from the well-understood expressive limitations of GNNs, and the tradeoff between optimality and generalization (general policies cannot be optimal in some domains). Both of these limitations are addressed without changing the basic DRL methods by adding derived predicates and an alternative cost structure to optimize.

