---
layout: default
title: Training Multimodal Large Reasoning Models Needs Better Thoughts: A Three-Stage Framework for Long Chain-of-Thought Synthesis and Selection
---

# Training Multimodal Large Reasoning Models Needs Better Thoughts: A Three-Stage Framework for Long Chain-of-Thought Synthesis and Selection

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.18956" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.18956v1</a>
  <a href="https://arxiv.org/pdf/2512.18956.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.18956v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.18956v1', 'Training Multimodal Large Reasoning Models Needs Better Thoughts: A Three-Stage Framework for Long Chain-of-Thought Synthesis and Selection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yizhi Wang, Linan Yue, Min-Ling Zhang

**åˆ†ç±»**: cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-12-22

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSynSelectæ¡†æ¶ï¼Œä¸ºå¤šæ¨¡æ€å¤§æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡é•¿é“¾æ¨ç†è®­ç»ƒæ•°æ®ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ¨ç†` `é•¿é“¾æ€è€ƒ` `æ•°æ®åˆæˆ` `æ•°æ®é€‰æ‹©` `å¤§å‹æ¨ç†æ¨¡å‹` `è§†è§‰é—®ç­”` `æ¨¡å‹å¾®è°ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤šæ¨¡æ€æ¨ç†é¢ä¸´é«˜è´¨é‡é•¿é“¾æ€è€ƒæ•°æ®ç¨€ç¼ºçš„æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•æ¨ç†æ·±åº¦æœ‰é™ä¸”æ˜“å‡ºé”™ã€‚
2. SynSelectæ¡†æ¶é€šè¿‡å¤šé˜¶æ®µåˆæˆä¸é€‰æ‹©ï¼Œç”Ÿæˆé«˜è´¨é‡ã€å¤šæ ·åŒ–çš„å¤šæ¨¡æ€é•¿é“¾æ€è€ƒæ•°æ®ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒåŸºäºSynSelectè®­ç»ƒçš„æ¨¡å‹åœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹æ¨ç†æ¨¡å‹(LRMs)åœ¨å¤æ‚çš„æ¨ç†ä»»åŠ¡ä¸­é€šè¿‡é•¿é“¾æ€è€ƒ(CoT)æ¨ç†è¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç”±äºæ•´åˆä¸åŒè¾“å…¥æ¨¡æ€çš„å¤æ‚æ€§å¢åŠ ä»¥åŠé«˜è´¨é‡é•¿CoTè®­ç»ƒæ•°æ®çš„ç¨€ç¼ºï¼Œå°†è¿™äº›æˆåŠŸæ‰©å±•åˆ°å¤šæ¨¡æ€æ¨ç†ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç°æœ‰çš„å¤šæ¨¡æ€æ•°æ®é›†å’ŒCoTåˆæˆæ–¹æ³•ä»ç„¶å­˜åœ¨æ¨ç†æ·±åº¦æœ‰é™ã€æ¨¡æ€è½¬æ¢é”™è¯¯å’Œç”Ÿæˆæµç¨‹åƒµåŒ–ç­‰é—®é¢˜ï¼Œä»è€Œé˜»ç¢äº†æ¨¡å‹çš„æ€§èƒ½å’Œç¨³å®šæ€§ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä¸‰é˜¶æ®µåˆæˆ-é€‰æ‹©æ¡†æ¶SynSelectï¼Œç”¨äºç”Ÿæˆé’ˆå¯¹å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡é‡èº«å®šåˆ¶çš„é«˜è´¨é‡é•¿CoTæ•°æ®ã€‚å…·ä½“æ¥è¯´ï¼ŒSynSelecté¦–å…ˆåˆ©ç”¨å¤šä¸ªå¼‚æ„å¤šæ¨¡æ€LRMæ¥ç”Ÿæˆå¤šæ ·åŒ–çš„å€™é€‰CoTï¼Œç„¶ååº”ç”¨å®ä¾‹å’Œæ‰¹æ¬¡çº§åˆ«çš„é€‰æ‹©æ¥è¿‡æ»¤èƒ½å¤Ÿæœ‰æ•ˆå¢å¼ºæ¨¡å‹æ¨ç†èƒ½åŠ›çš„é«˜è´¨é‡CoTã€‚åœ¨å¤šä¸ªå¤šæ¨¡æ€åŸºå‡†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œåœ¨SynSelectç”Ÿæˆçš„æ•°æ®ä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒçš„æ¨¡å‹æ˜¾è‘—ä¼˜äºåŸºçº¿ï¼Œå¹¶åœ¨å¼ºåŒ–å­¦ä¹ åè®­ç»ƒåå–å¾—äº†è¿›ä¸€æ­¥çš„æ”¹è¿›ã€‚æˆ‘ä»¬çš„ç»“æœéªŒè¯äº†SynSelectæ˜¯æé«˜å¤šæ¨¡æ€LRMæ¨ç†èƒ½åŠ›çš„æœ‰æ•ˆæ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå¤šæ¨¡æ€å¤§å‹æ¨ç†æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ç¼ºä¹é«˜è´¨é‡çš„é•¿é“¾æ€è€ƒ(CoT)è®­ç»ƒæ•°æ®ã€‚ç°æœ‰æ•°æ®é›†å’ŒCoTç”Ÿæˆæ–¹æ³•å­˜åœ¨æ¨ç†æ·±åº¦ä¸è¶³ã€æ¨¡æ€è½¬æ¢é”™è¯¯ä»¥åŠç”Ÿæˆæµç¨‹åƒµåŒ–ç­‰é—®é¢˜ï¼Œé™åˆ¶äº†æ¨¡å‹æ€§èƒ½å’Œç¨³å®šæ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSynSelectçš„æ ¸å¿ƒåœ¨äºé€šè¿‡ä¸€ä¸ªä¸‰é˜¶æ®µçš„åˆæˆ-é€‰æ‹©æ¡†æ¶ï¼Œç”Ÿæˆé«˜è´¨é‡ã€å¤šæ ·åŒ–çš„é•¿CoTæ•°æ®ã€‚å®ƒåˆ©ç”¨å¤šä¸ªå¼‚æ„çš„å¤šæ¨¡æ€LRMç”Ÿæˆå€™é€‰CoTï¼Œå¹¶é€šè¿‡å®ä¾‹å’Œæ‰¹æ¬¡çº§åˆ«çš„é€‰æ‹©æœºåˆ¶ï¼Œç­›é€‰å‡ºèƒ½å¤Ÿæœ‰æ•ˆæå‡æ¨¡å‹æ¨ç†èƒ½åŠ›çš„CoTã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨å…‹æœç°æœ‰æ–¹æ³•åœ¨æ•°æ®è´¨é‡å’Œå¤šæ ·æ€§æ–¹é¢çš„å±€é™æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSynSelectæ¡†æ¶åŒ…å«ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼š
1. **CoT Synthesis (CoTåˆæˆ)**ï¼šåˆ©ç”¨å¤šä¸ªå¼‚æ„çš„å¤šæ¨¡æ€LRMç”Ÿæˆå¤šæ ·åŒ–çš„å€™é€‰CoTã€‚
2. **Instance-level Selection (å®ä¾‹çº§åˆ«é€‰æ‹©)**ï¼šå¯¹æ¯ä¸ªCoTè¿›è¡Œè´¨é‡è¯„ä¼°ï¼Œé€‰æ‹©é«˜è´¨é‡çš„CoTã€‚
3. **Batch-level Selection (æ‰¹æ¬¡çº§åˆ«é€‰æ‹©)**ï¼šåœ¨æ‰¹æ¬¡å±‚é¢è¿›ä¸€æ­¥ç­›é€‰ï¼Œç¡®ä¿é€‰å‡ºçš„CoTèƒ½å¤Ÿæœ‰æ•ˆæå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šSynSelectçš„å…³é”®åˆ›æ–°åœ¨äºå…¶ä¸‰é˜¶æ®µçš„åˆæˆ-é€‰æ‹©æ¡†æ¶ï¼Œç‰¹åˆ«æ˜¯æ‰¹æ¬¡çº§åˆ«çš„é€‰æ‹©æœºåˆ¶ã€‚ä¸ä¼ ç»Ÿçš„åªå…³æ³¨å•ä¸ªCoTè´¨é‡çš„æ–¹æ³•ä¸åŒï¼ŒSynSelectåœ¨æ‰¹æ¬¡å±‚é¢è€ƒè™‘äº†CoTä¹‹é—´çš„å…³ç³»ï¼Œä»è€Œèƒ½å¤Ÿé€‰æ‹©å‡ºæ›´å…·ä¿¡æ¯é‡å’Œäº’è¡¥æ€§çš„CoTé›†åˆï¼Œæ›´æœ‰æ•ˆåœ°æå‡æ¨¡å‹æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨CoTåˆæˆé˜¶æ®µï¼Œä½¿ç”¨ä¸åŒçš„å¤šæ¨¡æ€LRMä»¥å¢åŠ CoTçš„å¤šæ ·æ€§ã€‚å®ä¾‹çº§åˆ«é€‰æ‹©å¯èƒ½ä½¿ç”¨å¥–åŠ±æ¨¡å‹æˆ–åŸºäºè§„åˆ™çš„è¿‡æ»¤ã€‚æ‰¹æ¬¡çº§åˆ«é€‰æ‹©å¯èƒ½æ¶‰åŠèšç±»æˆ–åŸºäºè¦†ç›–ç‡çš„ç­–ç•¥ï¼Œä»¥ç¡®ä¿é€‰æ‹©çš„CoTé›†åˆå…·æœ‰ä»£è¡¨æ€§ï¼Œå¹¶èƒ½è¦†ç›–ä¸åŒçš„æ¨ç†è·¯å¾„ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„å–å†³äºæ‰€ä½¿ç”¨çš„å¤šæ¨¡æ€LRMã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.18956v1/x2.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.18956v1/x3.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.18956v1/x4.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨SynSelectç”Ÿæˆçš„æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒçš„æ¨¡å‹ï¼Œåœ¨å¤šä¸ªå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ åè®­ç»ƒï¼Œæ¨¡å‹æ€§èƒ½å¾—åˆ°è¿›ä¸€æ­¥æå‡ï¼ŒéªŒè¯äº†SynSelectåœ¨æé«˜å¤šæ¨¡æ€LRMæ¨ç†èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚å…·ä½“æ€§èƒ½æå‡å¹…åº¦æœªçŸ¥ï¼Œä½†æ‘˜è¦å¼ºè°ƒäº†â€œæ˜¾è‘—ä¼˜äºåŸºçº¿â€ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºéœ€è¦å¤æ‚æ¨ç†çš„å¤šæ¨¡æ€ä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚è§†è§‰é—®ç­”ã€å›¾åƒæè¿°ç”Ÿæˆã€æœºå™¨äººå¯¼èˆªç­‰ã€‚é€šè¿‡æä¾›é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ï¼Œå¯ä»¥æ˜¾è‘—æå‡å¤šæ¨¡æ€å¤§æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œä½¿å…¶åœ¨å®é™…åº”ç”¨ä¸­æ›´åŠ å¯é å’Œæœ‰æ•ˆã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°å…¶ä»–æ¨¡æ€å’Œä»»åŠ¡ï¼Œæ¨åŠ¨å¤šæ¨¡æ€äººå·¥æ™ºèƒ½çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex reasoning tasks through long Chain-of-Thought (CoT) reasoning. Extending these successes to multimodal reasoning remains challenging due to the increased complexity of integrating diverse input modalities and the scarcity of high-quality long CoT training data. Existing multimodal datasets and CoT synthesis methods still suffer from limited reasoning depth, modality conversion errors, and rigid generation pipelines, hindering model performance and stability. To this end, in this paper, we propose SynSelect, a novel three-stage Synthesis-Selection framework for generating high-quality long CoT data tailored to multimodal reasoning tasks. Specifically, SynSelect first leverages multiple heterogeneous multimodal LRMs to produce diverse candidate CoTs, and then applies both instance and batch level selection to filter high-quality CoTs that can effectively enhance the model's reasoning capabilities. Extensive experiments on multiple multimodal benchmarks demonstrate that models supervised fine-tuned on SynSelect-generated data significantly outperform baselines and achieve further improvements after reinforcement learning post-training. Our results validate SynSelect as an effective approach for advancing multimodal LRMs reasoning capabilities.

