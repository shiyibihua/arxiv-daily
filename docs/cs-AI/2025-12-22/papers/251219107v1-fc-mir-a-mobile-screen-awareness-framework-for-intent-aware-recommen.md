---
layout: default
title: FC-MIR: A Mobile Screen Awareness Framework for Intent-Aware Recommendation based on Frame-Compressed Multimodal Trajectory Reasoning
---

# FC-MIR: A Mobile Screen Awareness Framework for Intent-Aware Recommendation based on Frame-Compressed Multimodal Trajectory Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.19107" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.19107v1</a>
  <a href="https://arxiv.org/pdf/2512.19107.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.19107v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.19107v1', 'FC-MIR: A Mobile Screen Awareness Framework for Intent-Aware Recommendation based on Frame-Compressed Multimodal Trajectory Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhe Yang, Xiaoshuang Sheng, Zhengnan Zhang, Jidong Wu, Zexing Wang, Xin He, Shenghua Xu, Guanjing Xiong

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-22

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFC-MIRæ¡†æ¶ï¼Œé€šè¿‡å¸§å‹ç¼©å¤šæ¨¡æ€è½¨è¿¹æ¨ç†å®ç°æ„å›¾æ„ŸçŸ¥çš„ç§»åŠ¨å±å¹•æ¨èã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç§»åŠ¨UIç†è§£` `æ„å›¾è¯†åˆ«` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `å…³é”®å¸§é‡‡æ ·` `å¸§å‹ç¼©` `UIè‡ªåŠ¨åŒ–` `äººæœºäº¤äº’`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šéƒ¨ç½²å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ—¶ï¼Œé¢ä¸´è®¡ç®—æˆæœ¬é«˜æ˜‚å’Œå†—ä½™å¸§å¤„ç†æ•ˆç‡ä½ä¸‹çš„æŒ‘æˆ˜ã€‚
2. FC-MIRæ¡†æ¶é€šè¿‡å…³é”®å¸§é‡‡æ ·å’Œè‡ªé€‚åº”è¿æ¥ï¼Œæœ‰æ•ˆé™ä½è§†è§‰å†—ä½™ï¼Œæå‡MLLMåœ¨ç§»åŠ¨è®¾å¤‡ä¸Šçš„æ¨ç†æ•ˆç‡ï¼ŒåŒæ—¶è¿›è¡Œæ„å›¾é¢„æµ‹ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶å®ç°äº†è¾ƒé«˜çš„å‹ç¼©ç‡ï¼Œå¹¶éªŒè¯äº†MLLMåœ¨æ„å›¾æ€»ç»“æ–¹é¢çš„èƒ½åŠ›ï¼Œä¸ºè½»é‡çº§éƒ¨ç½²å¥ å®šåŸºç¡€ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºFC-MIRæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç§»åŠ¨UIæ“ä½œè½¨è¿¹ä¸­ç”¨æˆ·æ„å›¾è¯†åˆ«çš„é—®é¢˜ï¼Œä»è€Œä¿ƒè¿›UIç†è§£å’Œä»»åŠ¡è‡ªåŠ¨åŒ–ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å…³é”®å¸§é‡‡æ ·å’Œè‡ªé€‚åº”è¿æ¥ï¼Œå‡å°‘è§†è§‰å†—ä½™ï¼Œæé«˜æ¨ç†æ•ˆç‡ã€‚åŒæ—¶ï¼Œé›†æˆå…ˆè¿›çš„é—­æºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æˆ–å¾®è°ƒæ¨¡å‹ï¼ˆå¦‚Qwen3-VLï¼‰ï¼Œç”¨äºè½¨è¿¹æ€»ç»“å’Œæ„å›¾é¢„æµ‹ã€‚ç ”ç©¶è¿›ä¸€æ­¥æ‰©å±•åˆ°é¢„æµ‹åæ“ä½œå’Œæœç´¢å»ºè®®ç”Ÿæˆï¼Œå¹¶å¼•å…¥ç»†ç²’åº¦æŒ‡æ ‡è¯„ä¼°æ€»ç»“ã€é¢„æµ‹å’Œå»ºè®®çš„å®ç”¨æ€§ã€‚é€šè¿‡UI-Agentså’ŒçœŸå®ç”¨æˆ·äº¤äº’æ„å»ºçš„UIè½¨è¿¹æ•°æ®é›†è¿›è¡Œè¯„ä¼°ï¼Œç»“æœè¡¨æ˜å‹ç¼©æ–¹æ³•åœ¨50%-60%å‹ç¼©ç‡ä¸‹ä¿æŒæ€§èƒ½ï¼›é—­æºå’Œå¾®è°ƒMLLMå‡è¡¨ç°å‡ºå¼ºå¤§çš„æ„å›¾æ€»ç»“èƒ½åŠ›ï¼Œæ”¯æŒè½»é‡çº§è®¾å¤‡éƒ¨ç½²ã€‚ç„¶è€Œï¼ŒMLLMåœ¨ç”Ÿæˆæœ‰ç”¨å’Œâ€œä»¤äººæƒŠè®¶â€çš„å»ºè®®æ–¹é¢ä»æœ‰æ”¹è¿›ç©ºé—´ã€‚æœ€åï¼Œè¯¥æ¡†æ¶å·²éƒ¨ç½²åœ¨å®é™…ç¯å¢ƒä¸­ï¼Œé›†æˆäº†UIæ„ŸçŸ¥å’ŒUI-Agentä»£ç†ï¼Œä¸ºè¯¥é¢†åŸŸçš„æœªæ¥å‘å±•å¥ å®šåŸºç¡€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ä»ç§»åŠ¨UIæ“ä½œè½¨è¿¹ä¸­è¯†åˆ«ç”¨æˆ·æ„å›¾çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯ç›´æ¥åº”ç”¨MLLMçš„æ–¹æ³•ï¼Œåœ¨ç§»åŠ¨è®¾å¤‡ä¸Šéƒ¨ç½²æ—¶é¢ä¸´è®¡ç®—é‡å¤§ã€æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œä¸»è¦æ˜¯å› ä¸ºè§†é¢‘å¸§å­˜åœ¨å¤§é‡å†—ä½™ä¿¡æ¯ï¼Œå¯¼è‡´ä¸å¿…è¦çš„è®¡ç®—å¼€é”€ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å‡å°‘è§†è§‰å†—ä½™æ¥æé«˜æ¨ç†æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒæ„å›¾è¯†åˆ«çš„å‡†ç¡®æ€§ã€‚å…·ä½“è€Œè¨€ï¼Œé‡‡ç”¨å…³é”®å¸§é‡‡æ ·å’Œè‡ªé€‚åº”è¿æ¥çš„æ–¹æ³•ï¼Œä»UIæ“ä½œè½¨è¿¹ä¸­æå–æœ€å…·ä»£è¡¨æ€§çš„å¸§ï¼Œå¹¶å°†å…¶è¾“å…¥åˆ°MLLMä¸­è¿›è¡Œå¤„ç†ã€‚è¿™æ ·å¯ä»¥åœ¨æ˜¾è‘—å‡å°‘è®¡ç®—é‡çš„åŒæ—¶ï¼Œä¿ç•™è¶³å¤Ÿçš„ä¿¡æ¯ç”¨äºæ„å›¾ç†è§£ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šFC-MIRæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) **UIè½¨è¿¹æ•°æ®é‡‡é›†**ï¼šæ”¶é›†UI-Agentå’ŒçœŸå®ç”¨æˆ·çš„UIæ“ä½œè½¨è¿¹æ•°æ®ã€‚2) **å…³é”®å¸§é‡‡æ ·**ï¼šä½¿ç”¨ç®—æ³•ä»UIè½¨è¿¹ä¸­é€‰æ‹©å…³é”®å¸§ï¼Œå‡å°‘è§†è§‰å†—ä½™ã€‚3) **è‡ªé€‚åº”è¿æ¥**ï¼šå°†å…³é”®å¸§è¿›è¡Œè¿æ¥ï¼Œå½¢æˆMLLMçš„è¾“å…¥ã€‚4) **MLLMæ¨ç†**ï¼šä½¿ç”¨é—­æºMLLMæˆ–å¾®è°ƒçš„å¼€æºMLLMï¼ˆå¦‚Qwen3-VLï¼‰è¿›è¡Œè½¨è¿¹æ€»ç»“ã€æ„å›¾é¢„æµ‹ã€åæ“ä½œç”Ÿæˆå’Œæœç´¢å»ºè®®ç”Ÿæˆã€‚5) **è¯„ä¼°**ï¼šä½¿ç”¨ç»†ç²’åº¦æŒ‡æ ‡è¯„ä¼°æ€»ç»“ã€é¢„æµ‹å’Œå»ºè®®çš„å®ç”¨æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†å¸§å‹ç¼©æ–¹æ³•ï¼Œé€šè¿‡å…³é”®å¸§é‡‡æ ·å’Œè‡ªé€‚åº”è¿æ¥ï¼Œæ˜¾è‘—é™ä½äº†MLLMå¤„ç†ç§»åŠ¨UIæ“ä½œè½¨è¿¹æ—¶çš„è®¡ç®—é‡ï¼Œä½¿å…¶æ›´é€‚åˆåœ¨ç§»åŠ¨è®¾å¤‡ä¸Šéƒ¨ç½²ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ç»†ç²’åº¦çš„è¯„ä¼°æŒ‡æ ‡ï¼Œç”¨äºè¯„ä¼°è½¨è¿¹æ€»ç»“ã€æ„å›¾é¢„æµ‹å’Œå»ºè®®çš„å®ç”¨æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®å¸§é‡‡æ ·ç®—æ³•çš„å…·ä½“å®ç°ç»†èŠ‚æœªçŸ¥ï¼Œä½†å…¶ç›®æ ‡æ˜¯é€‰æ‹©æœ€å…·ä»£è¡¨æ€§çš„å¸§ï¼Œä»¥æœ€å¤§ç¨‹åº¦åœ°ä¿ç•™ä¿¡æ¯å¹¶å‡å°‘å†—ä½™ã€‚è‡ªé€‚åº”è¿æ¥æ–¹æ³•çš„å…·ä½“å®ç°ç»†èŠ‚ä¹ŸæœªçŸ¥ï¼Œä½†å…¶ç›®æ ‡æ˜¯æœ‰æ•ˆåœ°å°†å…³é”®å¸§è¿æ¥èµ·æ¥ï¼Œä»¥ä¾¿MLLMèƒ½å¤Ÿç†è§£UIæ“ä½œçš„ä¸Šä¸‹æ–‡ã€‚è®ºæ–‡ä¸­ä½¿ç”¨äº†é—­æºMLLMå’Œå¾®è°ƒçš„Qwen3-VLæ¨¡å‹ï¼Œå…·ä½“å¾®è°ƒç­–ç•¥æœªçŸ¥ã€‚è¯„ä¼°æŒ‡æ ‡çš„è®¾è®¡è€ƒè™‘äº†æ€»ç»“ã€é¢„æµ‹å’Œå»ºè®®çš„å®ç”¨æ€§å’Œâ€œä»¤äººæƒŠè®¶â€ç¨‹åº¦ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.19107v1/lc_e.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.19107v1/l.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.19107v1/guanjian.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒFC-MIRæ¡†æ¶åœ¨50%-60%çš„å‹ç¼©ç‡ä¸‹ä»èƒ½ä¿æŒæ„å›¾è¯†åˆ«çš„æ€§èƒ½ã€‚é—­æºå’Œå¾®è°ƒçš„MLLMåœ¨è½¨è¿¹æ€»ç»“æ–¹é¢è¡¨ç°å‡ºè‰²ï¼ŒéªŒè¯äº†å…¶åœ¨è½»é‡çº§è®¾å¤‡ä¸Šéƒ¨ç½²çš„æ½œåŠ›ã€‚ç„¶è€Œï¼ŒMLLMåœ¨ç”Ÿæˆæœ‰ç”¨å’Œâ€œä»¤äººæƒŠè®¶â€çš„å»ºè®®æ–¹é¢ä»æœ‰æå‡ç©ºé—´ï¼Œè¡¨æ˜æœªæ¥ç ”ç©¶æ–¹å‘ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨åŒ–æµ‹è¯•ã€ç”¨æˆ·è¡Œä¸ºåˆ†æç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œå¯ä»¥åˆ©ç”¨è¯¥æ¡†æ¶æ„å»ºæ›´æ™ºèƒ½çš„UIè‡ªåŠ¨åŒ–æµ‹è¯•å·¥å…·ï¼Œæˆ–è€…ä¸ºç”¨æˆ·æä¾›æ›´ä¸ªæ€§åŒ–çš„åº”ç”¨æ¨èå’Œæ“ä½œå»ºè®®ã€‚é€šè¿‡ç†è§£ç”¨æˆ·åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šçš„æ“ä½œæ„å›¾ï¼Œå¯ä»¥å®ç°æ›´é«˜æ•ˆã€æ›´ä¾¿æ·çš„äººæœºäº¤äº’ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Identifying user intent from mobile UI operation trajectories is critical for advancing UI understanding and enabling task automation agents. While Multimodal Large Language Models (MLLMs) excel at video understanding tasks, their real-time mobile deployment is constrained by heavy computational costs and inefficient redundant frame processing. To address these issues, we propose the FC-MIR framework: leveraging keyframe sampling and adaptive concatenation, it cuts visual redundancy to boost inference efficiency, while integrating state-of-the-art closed-source MLLMs or fine-tuned models (e.g., Qwen3-VL) for trajectory summarization and intent prediction. We further expand task scope to explore generating post-prediction operations and search suggestions, and introduce a fine-grained metric to evaluate the practical utility of summaries, predictions, and suggestions. For rigorous assessment, we construct a UI trajectory dataset covering scenarios from UI-Agents (Agent-I) and real user interactions (Person-I). Experimental results show our compression method retains performance at 50%-60% compression rates; both closed-source and fine-tuned MLLMs demonstrate strong intent summarization, supporting potential lightweight on-device deployment. However, MLLMs still struggle with useful and "surprising" suggestions, leaving room for improvement. Finally, we deploy the framework in a real-world setting, integrating UI perception and UI-Agent proxies to lay a foundation for future progress in this field.

