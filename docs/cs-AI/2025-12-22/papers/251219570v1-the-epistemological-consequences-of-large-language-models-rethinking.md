---
layout: default
title: The Epistemological Consequences of Large Language Models: Rethinking collective intelligence and institutional knowledge
---

# The Epistemological Consequences of Large Language Models: Rethinking collective intelligence and institutional knowledge

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.19570" class="toolbar-btn" target="_blank">📄 arXiv: 2512.19570v1</a>
  <a href="https://arxiv.org/pdf/2512.19570.pdf" class="toolbar-btn" target="_blank">📥 PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.19570v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.19570v1', 'The Epistemological Consequences of Large Language Models: Rethinking collective intelligence and institutional knowledge')" title="添加到收藏夹">☆ 收藏</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">🔗 分享</button>
</div>


**作者**: Angjelin Hila

**分类**: cs.HC, cs.AI

**发布日期**: 2025-12-22

**备注**: AI & Soc (2025)

**DOI**: [10.1007/s00146-025-02426-3](https://doi.org/10.1007/s00146-025-02426-3)

---

## 💡 一句话要点

**大型语言模型对认知的影响：重新思考集体智能与机构知识**

🎯 **匹配领域**: **支柱九：具身大模型 (Embodied Foundation Models)**

**关键词**: `大型语言模型` `集体认知` `机构知识` `认知风险` `认知义务`

## 📋 核心要点

1. 现有方法未能充分考虑大型语言模型（LLM）与人类交互对集体认知和机构知识的潜在威胁。
2. 论文提出集体认知理论，强调内在论证（反思性理解）和外在论证（可靠传递）在集体理性中的作用。
3. 论文构建三层规范方案，旨在减轻LLM可能带来的认知风险，维护反思性知识和认知义务。

## 📝 摘要（中文）

本文探讨了人与大型语言模型（LLM）交互所带来的认知风险。我们构建了集体认知理论，将认知保证分布在人类集体中，并以有限理性和双重过程理论为背景。我们区分了内在论证（对命题为何为真的反思性理解）和外在论证（真理的可靠传递）。两者对于集体理性都是必要的，但只有内在论证才能产生反思性知识。我们将反思性知识定义如下：主体理解主张的评估基础，当该基础不可用时，主体持续评估真理来源的可靠性，并且主体有义务在其能力范围内应用这些标准。我们认为，LLM近似于外在可靠性，因为它们可以可靠地传递其论证基础已在其他地方确立的信息，但它们本身并不具备反思性论证。将反思性工作广泛外包给可靠的LLM输出可能会削弱反思性论证标准，降低理解的积极性，并降低主体履行专业和公民认知义务的能力。为了减轻这些风险，我们提出了一个三层规范方案，包括个人使用的认知交互模型、用于播种和执行认知最优结果规范的机构和组织框架，以及在组织和立法层面实例化话语规范并遏制认知恶习的义务约束。

## 🔬 方法详解

**问题定义**：论文旨在解决人与大型语言模型（LLM）交互对集体认知和机构知识产生的潜在威胁。现有方法未能充分考虑LLM在知识生产和传播中扮演的角色，以及这种角色可能对人类认知能力和社会认知规范带来的影响。特别关注的是，过度依赖LLM可能导致反思性知识的弱化和认知义务的缺失。

**核心思路**：论文的核心思路是区分内在论证和外在论证，并强调内在论证（即对知识的理解和反思）在集体认知中的重要性。论文认为，LLM虽然可以可靠地传递信息，但缺乏内在论证能力，过度依赖LLM可能导致人类认知能力的退化。因此，需要建立一套规范体系，以维护反思性知识和认知义务。

**技术框架**：论文构建了一个三层规范方案：第一层是个人使用的认知交互模型，旨在指导个体如何有效地与LLM交互，避免过度依赖；第二层是机构和组织框架，旨在在组织内部播种和执行认知最优结果的规范；第三层是在组织和立法层面建立义务约束，旨在实例化话语规范并遏制认知恶习。这个框架旨在从个体、组织和社会三个层面，共同维护反思性知识和认知义务。

**关键创新**：论文的关键创新在于提出了集体认知理论，并将其应用于分析LLM对人类认知的影响。论文区分了内在论证和外在论证，并强调了内在论证在集体认知中的重要性。此外，论文提出的三层规范方案也为应对LLM带来的认知风险提供了一个可行的框架。

**关键设计**：论文的关键设计在于三层规范方案的具体内容。例如，认知交互模型可能包括一些指导原则，如在使用LLM时要保持批判性思维，要验证LLM提供的信息，要尽可能理解LLM的推理过程等。机构和组织框架可能包括一些制度安排，如建立知识管理系统，鼓励员工进行知识分享和讨论，定期进行认知能力培训等。义务约束可能包括一些法律法规，如禁止使用LLM进行欺骗或误导，保护公民的知情权等。这些具体的设计旨在从不同层面维护反思性知识和认知义务。

## 🖼️ 关键图片

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.19570v1/human-llm-interaction_model.png" alt="fig_0" loading="lazy">
</figure>
</div>

## 📊 实验亮点

该论文侧重于理论分析和框架构建，并未提供具体的实验数据。其亮点在于提出了集体认知理论，并将其应用于分析LLM对人类认知的影响，为应对LLM带来的认知风险提供了一个新的视角和框架。

## 🎯 应用场景

该研究成果可应用于教育、新闻传播、科研等领域，帮助人们更理性地使用大型语言模型，避免过度依赖，维护批判性思维和认知能力。通过构建更完善的认知规范和社会制度，可以促进人与AI的协同发展，提升集体智能。

## 📄 摘要（原文）

> We examine epistemological threats posed by human and LLM interaction. We develop collective epistemology as a theory of epistemic warrant distributed across human collectives, using bounded rationality and dual process theory as background. We distinguish internalist justification, defined as reflective understanding of why a proposition is true, from externalist justification, defined as reliable transmission of truths. Both are necessary for collective rationality, but only internalist justification produces reflective knowledge. We specify reflective knowledge as follows: agents understand the evaluative basis of a claim, when that basis is unavailable agents consistently assess the reliability of truth sources, and agents have a duty to apply these standards within their domains of competence. We argue that LLMs approximate externalist reliabilism because they can reliably transmit information whose justificatory basis is established elsewhere, but they do not themselves possess reflective justification. Widespread outsourcing of reflective work to reliable LLM outputs can weaken reflective standards of justification, disincentivize comprehension, and reduce agents' capacity to meet professional and civic epistemic duties. To mitigate these risks, we propose a three tier norm program that includes an epistemic interaction model for individual use, institutional and organizational frameworks that seed and enforce norms for epistemically optimal outcomes, and deontic constraints at organizational and or legislative levels that instantiate discursive norms and curb epistemic vices.

