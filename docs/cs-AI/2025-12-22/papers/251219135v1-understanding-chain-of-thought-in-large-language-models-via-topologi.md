---
layout: default
title: Understanding Chain-of-Thought in Large Language Models via Topological Data Analysis
---

# Understanding Chain-of-Thought in Large Language Models via Topological Data Analysis

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.19135" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.19135v1</a>
  <a href="https://arxiv.org/pdf/2512.19135.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.19135v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.19135v1', 'Understanding Chain-of-Thought in Large Language Models via Topological Data Analysis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chenghao Li, Chaoning Zhang, Yi Lu, Shuxu Chen, Xudong Wang, Jiaquan Zhang, Zhicheng Wang, Zhengxun Jin, Kuien Liu, Sung-Ho Bae, Guoqing Wang, Yang Yang, Hen Tao Shen

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-22

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åˆ©ç”¨æ‹“æ‰‘æ•°æ®åˆ†æç†è§£å¤§è¯­è¨€æ¨¡å‹ä¸­çš„æ€ç»´é“¾**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æ€ç»´é“¾` `æ‹“æ‰‘æ•°æ®åˆ†æ` `æŒä¹…åŒè°ƒ` `æ¨ç†èƒ½åŠ›`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç ”ç©¶ä¸»è¦ä»åŠŸèƒ½è§’åº¦è¯„ä¼°LLMæ¨ç†é“¾ï¼Œç¼ºä¹å¯¹å…¶å†…åœ¨ç»“æ„æœºåˆ¶çš„æ·±å…¥ç†è§£ã€‚
2. è¯¥è®ºæ–‡åˆ©ç”¨æ‹“æ‰‘æ•°æ®åˆ†æï¼ˆTDAï¼‰ä¸­çš„æŒä¹…åŒè°ƒï¼Œå°†æ¨ç†æ­¥éª¤æ˜ å°„åˆ°è¯­ä¹‰ç©ºé—´ï¼Œå¹¶æå–æ‹“æ‰‘ç‰¹å¾ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ¨ç†é“¾çš„æ‹“æ‰‘ç»“æ„å¤æ‚æ€§ä¸å‡†ç¡®æ€§æ­£ç›¸å…³ï¼ŒæˆåŠŸçš„æ¨ç†é“¾å…·æœ‰æ›´ç®€å•çš„æ‹“æ‰‘ç»“æ„ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å‘å±•ï¼Œç‰¹åˆ«æ˜¯é•¿æ¨ç†é“¾æŠ€æœ¯çš„å¼•å…¥ï¼ŒLLMsåœ¨å¤æ‚é—®é¢˜è§£å†³ä¸­çš„æ¨ç†èƒ½åŠ›å¾—åˆ°äº†æ˜¾è‘—å¢å¼ºã€‚è™½ç„¶é•¿æ¨ç†é“¾çš„èƒ½åŠ›å¾ˆå¼ºï¼Œä½†æˆ‘ä»¬ä¸ç¦è¦é—®ï¼šä¸ºä»€ä¹ˆä¸åŒçš„æ¨ç†é“¾åœ¨æ¨ç†ä¸­çš„è¡¨ç°ä¸åŒï¼Ÿæ¨ç†é“¾çš„å“ªäº›ç»„æˆéƒ¨åˆ†èµ·ç€å…³é”®ä½œç”¨ï¼Ÿç°æœ‰çš„ç ”ç©¶ä¸»è¦ä»åŠŸèƒ½è§’åº¦è¯„ä¼°æ¨ç†é“¾ï¼Œè€Œå¾ˆå°‘å…³æ³¨å…¶ç»“æ„æœºåˆ¶ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæœ¬ç ”ç©¶é¦–æ¬¡ä»ç»“æ„è§’åº¦åˆ†æå’Œè¯„ä¼°æ¨ç†é“¾çš„è´¨é‡ã€‚æˆ‘ä»¬åº”ç”¨æ‹“æ‰‘æ•°æ®åˆ†æï¼ˆTDAï¼‰ä¸­çš„æŒä¹…åŒè°ƒï¼Œå°†æ¨ç†æ­¥éª¤æ˜ å°„åˆ°è¯­ä¹‰ç©ºé—´ï¼Œæå–æ‹“æ‰‘ç‰¹å¾ï¼Œå¹¶åˆ†æç»“æ„å˜åŒ–ã€‚è¿™äº›å˜åŒ–æ­ç¤ºäº†è¯­ä¹‰è¿è´¯æ€§ã€é€»è¾‘å†—ä½™ï¼Œå¹¶è¯†åˆ«é€»è¾‘ä¸­æ–­å’Œå·®è·ã€‚é€šè¿‡è®¡ç®—åŒè°ƒç¾¤ï¼Œæˆ‘ä»¬è¯„ä¼°äº†ä¸åŒå°ºåº¦çš„è¿é€šæ€§å’Œå†—ä½™æ€§ï¼Œä½¿ç”¨æ¡å½¢ç å’ŒæŒä¹…æ€§å›¾æ¥é‡åŒ–ç¨³å®šæ€§å’Œä¸€è‡´æ€§ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæ¨ç†é“¾çš„æ‹“æ‰‘ç»“æ„å¤æ‚æ€§ä¸å‡†ç¡®æ€§å‘ˆæ­£ç›¸å…³ã€‚æ›´å¤æ‚çš„é“¾èƒ½æ›´å¿«åœ°è¯†åˆ«å‡ºæ­£ç¡®çš„ç­”æ¡ˆï¼Œè€ŒæˆåŠŸçš„æ¨ç†è¡¨ç°å‡ºæ›´ç®€å•çš„æ‹“æ‰‘ç»“æ„ï¼Œå‡å°‘å†—ä½™å’Œå¾ªç¯ï¼Œä»è€Œæé«˜æ•ˆç‡å’Œå¯è§£é‡Šæ€§ã€‚è¿™é¡¹å·¥ä½œä¸ºæ¨ç†é“¾è´¨é‡è¯„ä¼°æä¾›äº†ä¸€ä¸ªæ–°çš„è§†è§’ï¼Œå¹¶ä¸ºæœªæ¥çš„ä¼˜åŒ–æä¾›äº†æŒ‡å¯¼ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†é“¾çš„åŠŸèƒ½æ€§è¯„ä¼°ï¼Œç¼ºä¹å¯¹å…¶ç»“æ„æ€§æœºåˆ¶çš„ç†è§£ã€‚ä¸åŒçš„æ¨ç†é“¾è¡¨ç°å·®å¼‚å¾ˆå¤§ï¼Œä½†æˆ‘ä»¬å¹¶ä¸æ¸…æ¥šå“ªäº›ç»„æˆéƒ¨åˆ†èµ·å…³é”®ä½œç”¨ï¼Œä»¥åŠå¦‚ä½•ä»ç»“æ„ä¸Šè¯„ä¼°æ¨ç†é“¾çš„è´¨é‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè¯¥è®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ‹“æ‰‘æ•°æ®åˆ†æï¼ˆTDAï¼‰ä¸­çš„æŒä¹…åŒè°ƒï¼Œå°†æ¨ç†é“¾ä¸­çš„æ¯ä¸ªæ­¥éª¤æ˜ å°„åˆ°è¯­ä¹‰ç©ºé—´ï¼Œç„¶ååˆ†æè¿™äº›æ­¥éª¤åœ¨è¯­ä¹‰ç©ºé—´ä¸­çš„æ‹“æ‰‘ç»“æ„ã€‚é€šè¿‡åˆ†ææ‹“æ‰‘ç»“æ„ï¼Œå¯ä»¥æ­ç¤ºæ¨ç†é“¾çš„è¯­ä¹‰è¿è´¯æ€§ã€é€»è¾‘å†—ä½™ä»¥åŠé€»è¾‘æ–­è£‚ç‚¹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1. å°†æ¨ç†é“¾ä¸­çš„æ¯ä¸ªæ­¥éª¤åµŒå…¥åˆ°è¯­ä¹‰ç©ºé—´ä¸­ï¼ˆä¾‹å¦‚ï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„è¯åµŒå…¥æˆ–å¥å­åµŒå…¥æ¨¡å‹ï¼‰ã€‚2. ä½¿ç”¨æŒä¹…åŒè°ƒåˆ†æè¯­ä¹‰ç©ºé—´ä¸­ç‚¹é›†çš„æ‹“æ‰‘ç»“æ„ï¼Œè®¡ç®—åŒè°ƒç¾¤ï¼Œå¹¶ç”Ÿæˆæ¡å½¢ç å’ŒæŒä¹…æ€§å›¾ã€‚3. åˆ†ææ¡å½¢ç å’ŒæŒä¹…æ€§å›¾ï¼Œæå–æ‹“æ‰‘ç‰¹å¾ï¼Œä¾‹å¦‚å¾ªç¯çš„æ•°é‡ã€æŒä¹…æ€§ç­‰ã€‚4. å°†æ‹“æ‰‘ç‰¹å¾ä¸æ¨ç†é“¾çš„å‡†ç¡®æ€§è¿›è¡Œå…³è”åˆ†æï¼Œè¯„ä¼°æ¨ç†é“¾çš„è´¨é‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†æ‹“æ‰‘æ•°æ®åˆ†æåº”ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†é“¾åˆ†æã€‚è¿™æ˜¯é¦–æ¬¡ä»ç»“æ„è§’åº¦è¯„ä¼°æ¨ç†é“¾çš„è´¨é‡ï¼Œå¹¶æä¾›äº†ä¸€ç§æ–°çš„è§†è§’æ¥ç†è§£LLMçš„æ¨ç†è¿‡ç¨‹ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•ä¸ä»…å…³æ³¨æ¨ç†é“¾çš„åŠŸèƒ½ï¼Œè¿˜å…³æ³¨å…¶å†…åœ¨çš„ç»“æ„ç‰¹å¾ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­å…³é”®çš„è®¾è®¡åŒ…æ‹¬ï¼š1. é€‰æ‹©åˆé€‚çš„è¯­ä¹‰åµŒå…¥æ¨¡å‹ï¼Œå°†æ¨ç†æ­¥éª¤æ˜ å°„åˆ°è¯­ä¹‰ç©ºé—´ã€‚2. é€‰æ‹©åˆé€‚çš„è·ç¦»åº¦é‡ï¼Œç”¨äºè®¡ç®—è¯­ä¹‰ç©ºé—´ä¸­ç‚¹ä¹‹é—´çš„è·ç¦»ã€‚3. é€‰æ‹©åˆé€‚çš„æŒä¹…åŒè°ƒç®—æ³•ï¼Œè®¡ç®—åŒè°ƒç¾¤å’Œç”Ÿæˆæ¡å½¢ç /æŒä¹…æ€§å›¾ã€‚4. è®¾è®¡åˆé€‚çš„æ‹“æ‰‘ç‰¹å¾ï¼Œç”¨äºè¯„ä¼°æ¨ç†é“¾çš„è´¨é‡ï¼Œä¾‹å¦‚å¾ªç¯çš„æ•°é‡ã€æŒä¹…æ€§ç­‰ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç­‰æŠ€æœ¯ç»†èŠ‚åœ¨è®ºæ–‡ä¸­å¯èƒ½æ²¡æœ‰è¯¦ç»†æè¿°ï¼Œéœ€è¦è¿›ä¸€æ­¥æŸ¥é˜…è®ºæ–‡åŸæ–‡ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.19135v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.19135v1/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.19135v1/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ¨ç†é“¾çš„æ‹“æ‰‘ç»“æ„å¤æ‚æ€§ä¸å‡†ç¡®æ€§å‘ˆæ­£ç›¸å…³ã€‚æ›´å¤æ‚çš„é“¾èƒ½æ›´å¿«åœ°è¯†åˆ«å‡ºæ­£ç¡®çš„ç­”æ¡ˆï¼Œè€ŒæˆåŠŸçš„æ¨ç†è¡¨ç°å‡ºæ›´ç®€å•çš„æ‹“æ‰‘ç»“æ„ï¼Œå‡å°‘å†—ä½™å’Œå¾ªç¯ã€‚è¿™è¡¨æ˜æ‹“æ‰‘ç»“æ„åˆ†æå¯ä»¥æœ‰æ•ˆåœ°è¯„ä¼°æ¨ç†é“¾çš„è´¨é‡ï¼Œå¹¶ä¸ºä¼˜åŒ–æ¨ç†è¿‡ç¨‹æä¾›æŒ‡å¯¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œå¯è§£é‡Šæ€§ã€‚é€šè¿‡åˆ†ææ¨ç†é“¾çš„æ‹“æ‰‘ç»“æ„ï¼Œå¯ä»¥è¯†åˆ«å’Œä¼˜åŒ–æ¨ç†è¿‡ç¨‹ä¸­çš„é€»è¾‘é”™è¯¯å’Œå†—ä½™æ­¥éª¤ï¼Œä»è€Œæé«˜æ¨ç†çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºè¯„ä¼°ä¸åŒæ¨ç†ç­–ç•¥çš„ä¼˜åŠ£ï¼ŒæŒ‡å¯¼LLMçš„è®­ç»ƒå’Œä¼˜åŒ–ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> With the development of large language models (LLMs), particularly with the introduction of the long reasoning chain technique, the reasoning ability of LLMs in complex problem-solving has been significantly enhanced. While acknowledging the power of long reasoning chains, we cannot help but wonder: Why do different reasoning chains perform differently in reasoning? What components of the reasoning chains play a key role? Existing studies mainly focus on evaluating reasoning chains from a functional perspective, with little attention paid to their structural mechanisms. To address this gap, this work is the first to analyze and evaluate the quality of the reasoning chain from a structural perspective. We apply persistent homology from Topological Data Analysis (TDA) to map reasoning steps into semantic space, extract topological features, and analyze structural changes. These changes reveal semantic coherence, logical redundancy, and identify logical breaks and gaps. By calculating homology groups, we assess connectivity and redundancy at various scales, using barcode and persistence diagrams to quantify stability and consistency. Our results show that the topological structural complexity of reasoning chains correlates positively with accuracy. More complex chains identify correct answers sooner, while successful reasoning exhibits simpler topologies, reducing redundancy and cycles, enhancing efficiency and interpretability. This work provides a new perspective on reasoning chain quality assessment and offers guidance for future optimization.

