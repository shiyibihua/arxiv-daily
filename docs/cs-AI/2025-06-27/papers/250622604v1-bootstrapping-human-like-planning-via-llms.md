---
layout: default
title: Bootstrapping Human-Like Planning via LLMs
---

# Bootstrapping Human-Like Planning via LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.22604" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.22604v1</a>
  <a href="https://arxiv.org/pdf/2506.22604.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.22604v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.22604v1', 'Bootstrapping Human-Like Planning via LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: David Porfirio, Vincent Hsiao, Morgan Fine-Morris, Leslie Smith, Laura M. Hiatt

**åˆ†ç±»**: cs.AI, cs.HC, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-06-27

**å¤‡æ³¨**: Accepted by the 2025 34th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ä»»åŠ¡è§„åˆ’æ–¹æ³•ä»¥æå‡æœºå™¨äººä»»åŠ¡æŒ‡å®šèƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æœºå™¨äººä»»åŠ¡è§„åˆ’` `è‡ªç„¶è¯­è¨€å¤„ç†` `å¤§è¯­è¨€æ¨¡å‹` `äººç±»åŠ¨ä½œåºåˆ—` `ç”¨æˆ·å‹å¥½æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æœºå™¨äººä»»åŠ¡æŒ‡å®šæ–¹æ³•å¦‚è‡ªç„¶è¯­è¨€ç¼–ç¨‹å’Œæ‹–æ”¾æ¥å£å„æœ‰ä¼˜ç¼ºç‚¹ï¼Œéš¾ä»¥æ»¡è¶³ç”¨æˆ·çš„ç²¾ç¡®éœ€æ±‚ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆè‡ªç„¶è¯­è¨€è¾“å…¥ä¸å¤§è¯­è¨€æ¨¡å‹çš„ä»»åŠ¡è§„åˆ’æ–¹æ³•ï¼Œæ—¨åœ¨ç”Ÿæˆç»†ç²’åº¦çš„äººç±»åŠ¨ä½œåºåˆ—ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¾ƒå¤§çš„æ¨¡å‹åœ¨ç”Ÿæˆè´¨é‡ä¸Šä¼˜äºå°æ¨¡å‹ï¼Œä½†å°æ¨¡å‹ä»èƒ½è¾¾åˆ°å¯æ¥å—çš„æ€§èƒ½æ°´å¹³ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€æœºå™¨äººç»ˆç«¯ç”¨æˆ·å¯¹ä»»åŠ¡æŒ‡å®šæ–¹å¼çš„éœ€æ±‚æ—¥ç›Šå¢åŠ ï¼Œæœ¬æ–‡æ¢è®¨äº†è‡ªç„¶è¯­è¨€ç¼–ç¨‹ä¸æ‹–æ”¾æ¥å£çš„ç»“åˆã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ç®¡é“ï¼Œæ¥å—è‡ªç„¶è¯­è¨€è¾“å…¥å¹¶ç”Ÿæˆç±»ä¼¼äººç±»çš„åŠ¨ä½œåºåˆ—ï¼Œè¾¾åˆ°äººç±»æŒ‡å®šçš„ç»†ç²’åº¦æ°´å¹³ã€‚é€šè¿‡ä¸æ‰‹åŠ¨æŒ‡å®šçš„åŠ¨ä½œåºåˆ—æ•°æ®é›†è¿›è¡Œæ¯”è¾ƒï¼Œç»“æœè¡¨æ˜è¾ƒå¤§çš„æ¨¡å‹åœ¨ç”Ÿæˆç±»ä¼¼äººç±»çš„åŠ¨ä½œåºåˆ—æ–¹é¢è¡¨ç°æ›´ä½³ï¼Œå°½ç®¡è¾ƒå°çš„æ¨¡å‹ä¹Ÿèƒ½å–å¾—ä»¤äººæ»¡æ„çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æœºå™¨äººä»»åŠ¡æŒ‡å®šçš„ç²¾ç¡®æ€§ä¸ç”¨æˆ·å‹å¥½æ€§ä¹‹é—´çš„çŸ›ç›¾ã€‚ç°æœ‰æ–¹æ³•åœ¨ç”¨æˆ·äº¤äº’å’Œä»»åŠ¡ç»†èŠ‚è¡¨è¾¾ä¸Šå­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥æ»¡è¶³å¤æ‚ä»»åŠ¡çš„éœ€æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æ„å»ºä¸€ä¸ªåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ç®¡é“ï¼Œæ¥å—è‡ªç„¶è¯­è¨€è¾“å…¥å¹¶ç”Ÿæˆä¸äººç±»ç›¸ä¼¼çš„åŠ¨ä½œåºåˆ—ï¼Œä»è€Œæé«˜ä»»åŠ¡æŒ‡å®šçš„çµæ´»æ€§å’Œå‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å—ã€åŠ¨ä½œåºåˆ—ç”Ÿæˆæ¨¡å—å’Œè¾“å‡ºè¯„ä¼°æ¨¡å—ã€‚é¦–å…ˆè§£æç”¨æˆ·è¾“å…¥çš„è‡ªç„¶è¯­è¨€ï¼Œç„¶åç”Ÿæˆç›¸åº”çš„åŠ¨ä½œåºåˆ—ï¼Œæœ€åä¸æ‰‹åŠ¨æŒ‡å®šçš„åºåˆ—è¿›è¡Œå¯¹æ¯”è¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºå°†è‡ªç„¶è¯­è¨€å¤„ç†ä¸æœºå™¨äººä»»åŠ¡è§„åˆ’ç›¸ç»“åˆï¼Œåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆç»†ç²’åº¦çš„åŠ¨ä½œåºåˆ—ï¼Œæ˜¾è‘—æå‡äº†æœºå™¨äººä»»åŠ¡æŒ‡å®šçš„è‡ªç„¶æ€§å’Œå‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸Šï¼Œé‡‡ç”¨äº†å¤šå±‚Transformerç»“æ„ï¼Œä¼˜åŒ–äº†æŸå¤±å‡½æ•°ä»¥æé«˜ç”Ÿæˆåºåˆ—çš„è´¨é‡ï¼Œå¹¶è¿›è¡Œäº†è¶…å‚æ•°è°ƒä¼˜ä»¥é€‚åº”ä¸åŒè§„æ¨¡çš„æ¨¡å‹ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œè®­ç»ƒç»†èŠ‚åœ¨å®éªŒéƒ¨åˆ†è¿›è¡Œäº†è¯¦ç»†è¯´æ˜ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨è¾ƒå¤§æ¨¡å‹ç”Ÿæˆçš„åŠ¨ä½œåºåˆ—åœ¨è´¨é‡ä¸Šæ˜¾è‘—ä¼˜äºå°æ¨¡å‹ï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œä¸”åœ¨ç”¨æˆ·æ»¡æ„åº¦è°ƒæŸ¥ä¸­è·å¾—äº†è¾ƒé«˜çš„è¯„ä»·ï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœåŠ¡æœºå™¨äººã€å·¥ä¸šè‡ªåŠ¨åŒ–å’Œå®¶åº­åŠ©ç†ç­‰åœºæ™¯ã€‚é€šè¿‡æå‡æœºå™¨äººå¯¹è‡ªç„¶è¯­è¨€æŒ‡ä»¤çš„ç†è§£å’Œæ‰§è¡Œèƒ½åŠ›ï¼Œå¯ä»¥å¤§å¹…åº¦æé«˜ç”¨æˆ·ä½“éªŒï¼Œé™ä½æ“ä½œé—¨æ§›ï¼Œæ¨åŠ¨æœºå™¨äººæŠ€æœ¯çš„æ™®åŠä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Robot end users increasingly require accessible means of specifying tasks for robots to perform. Two common end-user programming paradigms include drag-and-drop interfaces and natural language programming. Although natural language interfaces harness an intuitive form of human communication, drag-and-drop interfaces enable users to meticulously and precisely dictate the key actions of the robot's task. In this paper, we investigate the degree to which both approaches can be combined. Specifically, we construct a large language model (LLM)-based pipeline that accepts natural language as input and produces human-like action sequences as output, specified at a level of granularity that a human would produce. We then compare these generated action sequences to another dataset of hand-specified action sequences. Although our results reveal that larger models tend to outperform smaller ones in the production of human-like action sequences, smaller models nonetheless achieve satisfactory performance.

