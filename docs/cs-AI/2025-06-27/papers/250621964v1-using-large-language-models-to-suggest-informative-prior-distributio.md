---
layout: default
title: Using Large Language Models to Suggest Informative Prior Distributions in Bayesian Statistics
---

# Using Large Language Models to Suggest Informative Prior Distributions in Bayesian Statistics

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21964" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.21964v1</a>
  <a href="https://arxiv.org/pdf/2506.21964.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21964v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21964v1', 'Using Large Language Models to Suggest Informative Prior Distributions in Bayesian Statistics')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Michael A. Riegler, Kristoffer Herland Hellton, Vajira Thambawita, Hugo L. Hammer

**åˆ†ç±»**: stat.ME, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-27

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å»ºè®®è´å¶æ–¯ç»Ÿè®¡ä¸­çš„ä¿¡æ¯æ€§å…ˆéªŒåˆ†å¸ƒ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è´å¶æ–¯ç»Ÿè®¡` `å¤§å‹è¯­è¨€æ¨¡å‹` `ä¿¡æ¯æ€§å…ˆéªŒ` `Kullback-Leibleræ•£åº¦` `è‡ªåŠ¨åŒ–é€‰æ‹©` `ç»Ÿè®¡å»ºæ¨¡` `æ•°æ®åˆ†æ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è´å¶æ–¯ç»Ÿè®¡æ–¹æ³•åœ¨é€‰æ‹©å…ˆéªŒåˆ†å¸ƒæ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå¸¸å¸¸ä¾èµ–ä¸»è§‚åˆ¤æ–­ï¼Œä¸”èµ„æºæ¶ˆè€—å¤§ã€‚
2. æœ¬æ–‡æå‡ºåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¥è‡ªåŠ¨å»ºè®®ä¿¡æ¯æ€§å…ˆéªŒåˆ†å¸ƒï¼Œæ—¨åœ¨æé«˜é€‰æ‹©çš„å®¢è§‚æ€§å’Œæ•ˆç‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒClaudeå’ŒGeminiåœ¨æä¾›å…ˆéªŒæ–¹é¢ä¼˜äºChatGPTï¼Œå°¤å…¶åœ¨å¼±ä¿¡æ¯æ€§å…ˆéªŒçš„è¡¨ç°ä¸Šï¼ŒClaudeå±•ç°å‡ºæ›´å¥½çš„æ ¡å‡†èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é€‰æ‹©è´å¶æ–¯ç»Ÿè®¡ä¸­çš„å…ˆéªŒåˆ†å¸ƒæ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§ã€èµ„æºå¯†é›†ä¸”ä¸»è§‚çš„ä»»åŠ¡ã€‚æœ¬æ–‡åˆ†æäº†ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¥å»ºè®®åˆé€‚çš„çŸ¥è¯†åŸºç¡€ä¿¡æ¯æ€§å…ˆéªŒã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå¹¿æ³›çš„æç¤ºï¼Œè¦æ±‚LLMsä¸ä»…å»ºè®®å…ˆéªŒï¼Œè¿˜è¦éªŒè¯å’Œåæ€å…¶é€‰æ‹©ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªçœŸå®æ•°æ®é›†ï¼ˆå¿ƒè„ç—…é£é™©å’Œæ··å‡åœŸå¼ºåº¦ï¼‰ä¸Šè¯„ä¼°äº†Claude Opusã€Gemini 2.5 Proå’ŒChatGPT-4o-miniã€‚æ‰€æœ‰LLMséƒ½æ­£ç¡®è¯†åˆ«äº†æ‰€æœ‰å…³è”çš„æ–¹å‘ã€‚å»ºè®®å…ˆéªŒçš„è´¨é‡é€šè¿‡å…¶ä¸æœ€å¤§ä¼¼ç„¶ä¼°è®¡åˆ†å¸ƒçš„Kullback-Leibleræ•£åº¦æ¥è¡¡é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒClaudeå’ŒGeminiæä¾›çš„å…ˆéªŒä¼˜äºChatGPTï¼Œå°¤å…¶æ˜¯åœ¨å¼±ä¿¡æ¯æ€§å…ˆéªŒæ–¹é¢ï¼ŒClaudeè¡¨ç°å‡ºæ˜æ˜¾ä¼˜åŠ¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è´å¶æ–¯ç»Ÿè®¡ä¸­å…ˆéªŒåˆ†å¸ƒé€‰æ‹©çš„ä¸»è§‚æ€§å’Œèµ„æºå¯†é›†æ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºä¸“å®¶çŸ¥è¯†ï¼Œå¯¼è‡´é€‰æ‹©è¿‡ç¨‹ä¸å¤Ÿé«˜æ•ˆå’Œå®¢è§‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•æ¥è‡ªåŠ¨ç”Ÿæˆå’ŒéªŒè¯ä¿¡æ¯æ€§å…ˆéªŒåˆ†å¸ƒï¼Œä»è€Œå‡å°‘äººä¸ºåå·®å¹¶æé«˜é€‰æ‹©çš„å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶ä¸­ä½¿ç”¨çš„æŠ€æœ¯æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼š1) æç¤ºç”Ÿæˆæ¨¡å—ï¼Œè®¾è®¡ç”¨äºå¼•å¯¼LLMsç”Ÿæˆå…ˆéªŒï¼›2) éªŒè¯æ¨¡å—ï¼Œè¯„ä¼°ç”Ÿæˆå…ˆéªŒçš„åˆç†æ€§ï¼›3) åé¦ˆæ¨¡å—ï¼Œå…è®¸LLMsåæ€å…¶é€‰æ‹©å¹¶è¿›è¡Œè°ƒæ•´ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„ä¸»è¦åˆ›æ–°åœ¨äºç»“åˆLLMsçš„è‡ªç„¶è¯­è¨€å¤„ç†èƒ½åŠ›ä¸è´å¶æ–¯ç»Ÿè®¡çš„éœ€æ±‚ï¼Œé¦–æ¬¡ç³»ç»Ÿæ€§åœ°å°†LLMsåº”ç”¨äºå…ˆéªŒåˆ†å¸ƒçš„é€‰æ‹©ä¸­ï¼Œæ˜¾è‘—æé«˜äº†é€‰æ‹©çš„å®¢è§‚æ€§å’Œæ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œä½¿ç”¨Kullback-Leibleræ•£åº¦ä½œä¸ºè¡¡é‡å»ºè®®å…ˆéªŒè´¨é‡çš„æ ‡å‡†ï¼ŒLLMsçš„æç¤ºè®¾è®¡ä¹Ÿç»è¿‡ç²¾å¿ƒè°ƒæ•´ï¼Œä»¥ç¡®ä¿å…¶ç”Ÿæˆçš„å…ˆéªŒå…·æœ‰é€‚å½“çš„ä¿¡æ¯æ€§å’Œæ ¡å‡†æ€§ã€‚å®éªŒä¸­è¿˜æ¯”è¾ƒäº†ä¸åŒLLMsçš„è¡¨ç°ï¼Œå‘ç°Claudeåœ¨å¼±ä¿¡æ¯æ€§å…ˆéªŒçš„ç”Ÿæˆä¸Šå…·æœ‰æ˜æ˜¾ä¼˜åŠ¿ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒClaudeå’ŒGeminiåœ¨æä¾›ä¿¡æ¯æ€§å…ˆéªŒæ–¹é¢çš„è¡¨ç°ä¼˜äºChatGPTï¼Œå°¤å…¶åœ¨å¼±ä¿¡æ¯æ€§å…ˆéªŒçš„ç”Ÿæˆä¸­ï¼ŒClaudeæœªé»˜è®¤ä½¿ç”¨æ¨¡ç³Šçš„å‡å€¼0ï¼Œæ˜¾ç¤ºå‡ºæ›´å¥½çš„æ ¡å‡†èƒ½åŠ›ã€‚è¿™äº›å‘ç°ä¸ºLLMsåœ¨è´å¶æ–¯ç»Ÿè®¡ä¸­çš„åº”ç”¨æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒ»å­¦ç»Ÿè®¡ã€å·¥ç¨‹è´¨é‡æ§åˆ¶å’Œç¤¾ä¼šç§‘å­¦ç ”ç©¶ç­‰é¢†åŸŸï¼Œèƒ½å¤Ÿä¸ºç ”ç©¶äººå‘˜æä¾›æ›´ä¸ºå®¢è§‚å’Œé«˜æ•ˆçš„å…ˆéªŒé€‰æ‹©å·¥å…·ã€‚æœªæ¥ï¼Œéšç€LLMsæŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ï¼Œå¯èƒ½ä¼šåœ¨æ›´å¤šå¤æ‚çš„ç»Ÿè®¡å»ºæ¨¡ä»»åŠ¡ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Selecting prior distributions in Bayesian statistics is challenging, resource-intensive, and subjective. We analyze using large-language models (LLMs) to suggest suitable, knowledge-based informative priors. We developed an extensive prompt asking LLMs not only to suggest priors but also to verify and reflect on their choices.
>   We evaluated Claude Opus, Gemini 2.5 Pro, and ChatGPT-4o-mini on two real datasets: heart disease risk and concrete strength. All LLMs correctly identified the direction for all associations (e.g., that heart disease risk is higher for males). The quality of suggested priors was measured by their Kullback-Leibler divergence from the maximum likelihood estimator's distribution.
>   The LLMs suggested both moderately and weakly informative priors. The moderate priors were often overconfident, resulting in distributions misaligned with the data. In our experiments, Claude and Gemini provided better priors than ChatGPT. For weakly informative priors, a key performance difference emerged: ChatGPT and Gemini defaulted to an "unnecessarily vague" mean of 0, while Claude did not, demonstrating a significant advantage.
>   The ability of LLMs to identify correct associations shows their great potential as an efficient, objective method for developing informative priors. However, the primary challenge remains in calibrating the width of these priors to avoid over- and under-confidence.

