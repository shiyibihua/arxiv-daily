---
layout: default
title: A Different Approach to AI Safety: Proceedings from the Columbia Convening on Openness in Artificial Intelligence and AI Safety
---

# A Different Approach to AI Safety: Proceedings from the Columbia Convening on Openness in Artificial Intelligence and AI Safety

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.22183" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.22183v1</a>
  <a href="https://arxiv.org/pdf/2506.22183.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.22183v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.22183v1', 'A Different Approach to AI Safety: Proceedings from the Columbia Convening on Openness in Artificial Intelligence and AI Safety')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Camille FranÃ§ois, Ludovic PÃ©ran, Ayah Bdeir, Nouha Dziri, Will Hawkins, Yacine Jernite, Sayash Kapoor, Juliet Shen, Heidy Khlaaf, Kevin Klyman, Nik Marda, Marie Pellat, Deb Raji, Divya Siddarth, Aviya Skowron, Joseph Spisak, Madhulika Srikumar, Victor Storchan, Audrey Tang, Jen Weedon

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-27

**å¤‡æ³¨**: Proceedings from the Columbia Convening on Openness in Artificial Intelligence and AI Safety

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¼€æ”¾æ€§ä¸å®‰å…¨æ€§ç»“åˆçš„AIç ”ç©¶è®®ç¨‹ä»¥åº”å¯¹AIå®‰å…¨æŒ‘æˆ˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `AIå®‰å…¨` `å¼€æºæ¨¡å‹` `å‚ä¸å¼æœºåˆ¶` `æŠ€æœ¯å¹²é¢„` `å†…å®¹è¿‡æ»¤` `å¤šæ¨¡æ€åŸºå‡†` `å…¬å…±æ²»ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å½“å‰AIå®‰å…¨é¢†åŸŸé¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜æ˜¯ç¼ºä¹æœ‰æ•ˆçš„å¤šæ¨¡æ€å’Œå¤šè¯­è¨€åŸºå‡†ï¼Œä»¥åŠå¯¹æŠ—æ€§æ”»å‡»çš„é˜²å¾¡ä¸è¶³ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡å¼€æ”¾æ€§ï¼ˆé€æ˜æƒé‡ã€å¯äº’æ“ä½œå·¥å…·å’Œå…¬å…±æ²»ç†ï¼‰æ¥å¢å¼ºAIç³»ç»Ÿçš„å®‰å…¨æ€§ï¼Œå¼ºè°ƒå‚ä¸å¼è¾“å…¥å’Œå¤šå…ƒç›‘ç£çš„é‡è¦æ€§ã€‚
3. ç ”ç©¶ç»“æœä¸ºæœªæ¥çš„AIå®‰å…¨ç ”ç©¶æä¾›äº†äº”ä¸ªä¼˜å…ˆæ–¹å‘ï¼Œæ—¨åœ¨å»ºç«‹ä¸€ä¸ªå¼€æ”¾ã€å¯é—®è´£çš„AIå®‰å…¨å­¦ç§‘åŸºç¡€ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¼€æ”¾æƒé‡å’Œå¼€æºåŸºç¡€æ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œç¡®ä¿AIç³»ç»Ÿå®‰å…¨çš„è´£ä»»å’Œæœºä¼šæ­£åœ¨é‡æ–°å¡‘é€ ã€‚æœ¬æ–‡æŠ¥å‘Šäº†2024å¹´11æœˆ19æ—¥åœ¨æ—§é‡‘å±±ä¸¾è¡Œçš„å“¥ä¼¦æ¯”äºšAIå¼€æ”¾æ€§ä¸å®‰å…¨ä¼šè®®çš„æˆæœï¼Œä»¥åŠä¸ºæœŸå…­å‘¨çš„å‡†å¤‡é¡¹ç›®ï¼Œå‚ä¸è€…åŒ…æ‹¬æ¥è‡ªå­¦æœ¯ç•Œã€å·¥ä¸šç•Œã€æ°‘é—´ç¤¾ä¼šå’Œæ”¿åºœçš„å››åäº”ä½ç ”ç©¶äººå‘˜ã€å·¥ç¨‹å¸ˆå’Œæ”¿ç­–é¢†å¯¼è€…ã€‚é€šè¿‡å‚ä¸å¼çš„è§£å†³æ–¹æ¡ˆå¯¼å‘è¿‡ç¨‹ï¼Œå·¥ä½œç»„åˆ¶å®šäº†å®‰å…¨ä¸å¼€æºAIäº¤å‰é¢†åŸŸçš„ç ”ç©¶è®®ç¨‹ï¼Œç°æœ‰ä¸æ‰€éœ€æŠ€æœ¯å¹²é¢„å’Œå¼€æºå·¥å…·çš„æ˜ å°„ï¼Œä»¥åŠå†…å®¹å®‰å…¨è¿‡æ»¤ç”Ÿæ€ç³»ç»Ÿçš„æ˜ å°„å’Œæœªæ¥ç ”ç©¶ä¸å¼€å‘çš„è·¯çº¿å›¾ã€‚ç ”ç©¶å‘ç°ï¼Œå¼€æ”¾æ€§å¯ä»¥é€šè¿‡ä¿ƒè¿›ç‹¬ç«‹å®¡æŸ¥ã€å»ä¸­å¿ƒåŒ–ç¼“è§£å’Œæ–‡åŒ–å¤šå…ƒç›‘ç£æ¥å¢å¼ºå®‰å…¨æ€§ï¼Œä½†åœ¨å¤šæ¨¡æ€å’Œå¤šè¯­è¨€åŸºå‡†ã€å¯¹æŠ—æ€§æ”»å‡»çš„é˜²å¾¡ä»¥åŠå‚ä¸æœºåˆ¶æ–¹é¢ä»å­˜åœ¨æ˜¾è‘—å·®è·ã€‚æœ€åï¼Œè®ºæ–‡æå‡ºäº†äº”ä¸ªä¼˜å…ˆç ”ç©¶æ–¹å‘çš„è·¯çº¿å›¾ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³AIç³»ç»Ÿå®‰å…¨æ€§ä¸è¶³çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨å¼€æ”¾æºä»£ç å’Œå¼€æ”¾æƒé‡æ¨¡å‹å¿«é€Ÿå‘å±•çš„èƒŒæ™¯ä¸‹ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤šæ¨¡æ€å’Œå¤šè¯­è¨€åŸºå‡†ã€å¯¹æŠ—æ€§æ”»å‡»é˜²å¾¡ç­‰æ–¹é¢å­˜åœ¨æ˜¾è‘—ç¼ºé™·ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¢å¼ºå¼€æ”¾æ€§æ¥æå‡AIå®‰å…¨æ€§ï¼Œå…·ä½“åŒ…æ‹¬é€æ˜çš„æ¨¡å‹æƒé‡ã€å¯äº’æ“ä½œçš„å·¥å…·å’Œå…¬å…±æ²»ç†ç»“æ„ï¼Œä»¥ä¿ƒè¿›ç‹¬ç«‹å®¡æŸ¥å’Œå»ä¸­å¿ƒåŒ–çš„å®‰å…¨æªæ–½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šç ”ç©¶è®®ç¨‹çš„åˆ¶å®šã€ç°æœ‰ä¸æ‰€éœ€æŠ€æœ¯å¹²é¢„çš„æ˜ å°„ã€å†…å®¹å®‰å…¨è¿‡æ»¤ç”Ÿæ€ç³»ç»Ÿçš„æ„å»ºã€‚æ¯ä¸ªæ¨¡å—éƒ½é€šè¿‡å‚ä¸å¼çš„æ–¹å¼è¿›è¡Œè®¾è®¡ï¼Œä»¥ç¡®ä¿å¤šæ–¹åˆ©ç›Šç›¸å…³è€…çš„å£°éŸ³è¢«çº³å…¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§æ–°çš„ç ”ç©¶æ¡†æ¶ï¼Œå°†å¼€æ”¾æ€§ä¸å®‰å…¨æ€§ç»“åˆï¼Œå¼ºè°ƒäº†å¤šå…ƒæ–‡åŒ–ç›‘ç£çš„é‡è¦æ€§ï¼Œè¿™ä¸ä¼ ç»Ÿçš„é›†ä¸­å¼å®‰å…¨æ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼Œè®ºæ–‡å¼ºè°ƒäº†å‚ä¸å¼æœºåˆ¶çš„å»ºç«‹ï¼Œæå‡ºäº†æœªæ¥å†…å®¹è¿‡æ»¤å™¨çš„è®¾è®¡æ–¹å‘ï¼Œå¹¶å»ºè®®å»ºç«‹ç”Ÿæ€ç³»ç»Ÿçº§çš„å®‰å…¨åŸºç¡€è®¾æ–½ï¼Œä»¥åº”å¯¹AIå¸¦æ¥çš„æ½œåœ¨å±å®³ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŠ€æœ¯ç»†èŠ‚å°šæœªæ˜ç¡®ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé€šè¿‡å¼€æ”¾æ€§æªæ–½ï¼ŒAIç³»ç»Ÿçš„å®‰å…¨æ€§å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚å…·ä½“è€Œè¨€ï¼Œæå‡ºçš„ç ”ç©¶è®®ç¨‹å’ŒæŠ€æœ¯å¹²é¢„æ˜ å°„ä¸ºæœªæ¥çš„AIå®‰å…¨ç ”ç©¶æä¾›äº†æ¸…æ™°çš„æ–¹å‘ï¼Œå°¤å…¶æ˜¯åœ¨å¤šæ¨¡æ€å’Œå¤šè¯­è¨€ç¯å¢ƒä¸‹çš„åº”ç”¨ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬AIæ¨¡å‹çš„å¼€å‘ä¸éƒ¨ç½²ã€æ”¿ç­–åˆ¶å®šã€ä»¥åŠAIå®‰å…¨æ ‡å‡†çš„å»ºç«‹ã€‚é€šè¿‡æä¾›å¼€æ”¾æ€§å’Œå®‰å…¨æ€§çš„ç»“åˆï¼Œèƒ½å¤Ÿä¸ºå„ç±»AIåº”ç”¨æä¾›æ›´ä¸ºå®‰å…¨çš„ç¯å¢ƒï¼Œå‡å°‘AIæŠ€æœ¯å¯¹ç¤¾ä¼šçš„æ½œåœ¨å±å®³ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The rapid rise of open-weight and open-source foundation models is intensifying the obligation and reshaping the opportunity to make AI systems safe. This paper reports outcomes from the Columbia Convening on AI Openness and Safety (San Francisco, 19 Nov 2024) and its six-week preparatory programme involving more than forty-five researchers, engineers, and policy leaders from academia, industry, civil society, and government. Using a participatory, solutions-oriented process, the working groups produced (i) a research agenda at the intersection of safety and open source AI; (ii) a mapping of existing and needed technical interventions and open source tools to safely and responsibly deploy open foundation models across the AI development workflow; and (iii) a mapping of the content safety filter ecosystem with a proposed roadmap for future research and development. We find that openness -- understood as transparent weights, interoperable tooling, and public governance -- can enhance safety by enabling independent scrutiny, decentralized mitigation, and culturally plural oversight. However, significant gaps persist: scarce multimodal and multilingual benchmarks, limited defenses against prompt-injection and compositional attacks in agentic systems, and insufficient participatory mechanisms for communities most affected by AI harms. The paper concludes with a roadmap of five priority research directions, emphasizing participatory inputs, future-proof content filters, ecosystem-wide safety infrastructure, rigorous agentic safeguards, and expanded harm taxonomies. These recommendations informed the February 2025 French AI Action Summit and lay groundwork for an open, plural, and accountable AI safety discipline.

