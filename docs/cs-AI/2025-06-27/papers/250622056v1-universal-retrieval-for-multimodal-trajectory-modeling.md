---
layout: default
title: Universal Retrieval for Multimodal Trajectory Modeling
---

# Universal Retrieval for Multimodal Trajectory Modeling

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.22056" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.22056v1</a>
  <a href="https://arxiv.org/pdf/2506.22056.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.22056v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.22056v1', 'Universal Retrieval for Multimodal Trajectory Modeling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xuan Zhang, Ziyan Jiang, Rui Meng, Yifei Leng, Zhenbang Xiao, Zora Zhiruo Wang, Yanyi Shang, Dehan Kong

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-27

**å¤‡æ³¨**: 18 pages, 3 figures, accepted by Workshop on Computer-use Agents @ ICML 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šæ¨¡æ€è½¨è¿¹æ£€ç´¢ä»¥è§£å†³è½¨è¿¹æ•°æ®å»ºæ¨¡æŒ‘æˆ˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€è½¨è¿¹æ£€ç´¢` `è§†è§‰-è¯­è¨€æ¨¡å‹` `å¯¹æ¯”å­¦ä¹ ` `è½¨è¿¹æ•°æ®å»ºæ¨¡` `æ™ºèƒ½ä»£ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤šæ¨¡æ€è½¨è¿¹æ•°æ®æ—¶ç¼ºä¹ç³»ç»Ÿæ€§ï¼Œéš¾ä»¥æœ‰æ•ˆå»ºæ¨¡è½¨è¿¹çº§æ•°æ®çš„è¡¨ç¤ºã€‚
2. æœ¬æ–‡æå‡ºäº†GAE-Retrieveræ¡†æ¶ï¼Œç»“åˆè§†è§‰-è¯­è¨€æ¨¡å‹ä¸ä¼˜åŒ–çš„å¯¹æ¯”å­¦ä¹ ï¼Œæå‡å¤šæ¨¡æ€è½¨è¿¹æ£€ç´¢èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒGAE-Retrieveråœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰å¼ºåŸºçº¿ï¼Œæ£€ç´¢å¬å›ç‡æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è½¨è¿¹æ•°æ®æ•æ‰äº†äººç±»è¡Œä¸ºå’Œç¯å¢ƒçŠ¶æ€ï¼Œå…·æœ‰æå‡AIä»£ç†èƒ½åŠ›çš„æ½œåŠ›ï¼Œå°¤å…¶åœ¨GUIç¯å¢ƒä¸­ã€‚ç„¶è€Œï¼Œå¦‚ä½•æœ‰æ•ˆå»ºæ¨¡è½¨è¿¹çº§æ•°æ®çš„è¡¨ç¤ºä»ç„¶æ˜¯ä¸€ä¸ªæœªè¢«ç³»ç»Ÿæ€§è§£å†³çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†å¤šæ¨¡æ€è½¨è¿¹æ£€ç´¢ï¼Œå¼¥è¡¥äº†é€šç”¨æ£€ç´¢ä¸ä»£ç†ä¸­å¿ƒè½¨è¿¹å»ºæ¨¡ä¹‹é—´çš„å·®è·ã€‚æˆ‘ä»¬æ„å»ºäº†ç»Ÿä¸€ä»£ç†è½¨è¿¹æ•°æ®é›†ï¼ˆUATDï¼‰ï¼Œå¹¶æå‡ºäº†GAE-BenchåŸºå‡†ï¼ŒåŒ…å«å¤§é‡åŸºäºè½¨è¿¹çš„æ£€ç´¢å¯¹ã€‚æ­¤å¤–ï¼Œæå‡ºçš„GAE-Retrieveræ¡†æ¶ç»“åˆäº†è§†è§‰-è¯­è¨€æ¨¡å‹å’Œä¼˜åŒ–çš„å¯¹æ¯”å­¦ä¹ æœºåˆ¶ã€‚ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒGAE-Retrieveråœ¨æ£€ç´¢å¬å›ç‡ä¸ŠæŒç»­è¶…è¶Šå¼ºåŸºçº¿ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤šæ¨¡æ€è½¨è¿¹æ£€ç´¢ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€è½¨è¿¹æ•°æ®å»ºæ¨¡ä¸­çš„æœ‰æ•ˆæ€§é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨é¢å¯¹æµ·é‡è½¨è¿¹æ•°æ®æ—¶è¡¨ç°ä¸è¶³ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨å¤šæ¨¡æ€ä¿¡æ¯è¿›è¡Œæœ‰æ•ˆæ£€ç´¢ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„GAE-Retrieveræ¡†æ¶é€šè¿‡ç»“åˆè§†è§‰-è¯­è¨€æ¨¡å‹ä¸ä¼˜åŒ–çš„å¯¹æ¯”å­¦ä¹ æœºåˆ¶ï¼Œæ—¨åœ¨æå‡å¤šæ¨¡æ€è½¨è¿¹æ£€ç´¢çš„å‡†ç¡®æ€§ä¸æ•ˆç‡ã€‚æ­¤è®¾è®¡èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰ä¸åŒæ¨¡æ€ä¹‹é—´çš„å…³è”æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGAE-Retrieverçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€ç‰¹å¾æå–ã€å¯¹æ¯”å­¦ä¹ å’Œæ£€ç´¢æ¨¡å—ã€‚é¦–å…ˆï¼Œä»ç»Ÿä¸€ä»£ç†è½¨è¿¹æ•°æ®é›†ä¸­æå–å¤šæ¨¡æ€ç‰¹å¾ï¼Œç„¶åé€šè¿‡å¯¹æ¯”å­¦ä¹ ä¼˜åŒ–ç‰¹å¾è¡¨ç¤ºï¼Œæœ€åè¿›è¡Œæ£€ç´¢ã€‚

**å…³é”®åˆ›æ–°**ï¼šGAE-Retrieverçš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥GradCacheæœºåˆ¶å’ŒåŸºäºtokené€‰æ‹©çš„ä¼˜åŒ–å¯¹æ¯”å­¦ä¹ ï¼Œæ˜¾è‘—æå‡äº†å¤šæ¨¡æ€ä¿¡æ¯çš„åˆ©ç”¨æ•ˆç‡ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”å…·æœ‰æ›´é«˜çš„æ£€ç´¢å¬å›ç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨GAE-Retrieverä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–å¯¹æ¯”å­¦ä¹ æ•ˆæœï¼ŒåŒæ—¶åœ¨ç½‘ç»œç»“æ„ä¸Šè®¾è®¡äº†å¤šæ¨¡æ€ç‰¹å¾èåˆæ¨¡å—ï¼Œä»¥ç¡®ä¿ä¸åŒæ¨¡æ€ä¿¡æ¯çš„æœ‰æ•ˆæ•´åˆã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨å®éªŒéƒ¨åˆ†è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGAE-Retrieveråœ¨æ£€ç´¢å¬å›ç‡ä¸Šè¶…è¶Šäº†å¤šä¸ªå¼ºåŸºçº¿ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°XX%ã€‚è¿™ä¸€ç»“æœè¡¨æ˜ï¼ŒGAE-Retrieveråœ¨å¤šæ¨¡æ€è½¨è¿¹æ£€ç´¢ä»»åŠ¡ä¸­å…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨æ™ºèƒ½ä»£ç†ã€æœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æå‡å¤šæ¨¡æ€è½¨è¿¹æ£€ç´¢çš„èƒ½åŠ›ï¼Œèƒ½å¤Ÿå¸®åŠ©AIç³»ç»Ÿæ›´å¥½åœ°ç†è§£å’Œé¢„æµ‹äººç±»è¡Œä¸ºï¼Œä»è€Œåœ¨å¤æ‚ç¯å¢ƒä¸­åšå‡ºæ›´æ™ºèƒ½çš„å†³ç­–ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›æ¨åŠ¨äººæœºäº¤äº’å’Œæ™ºèƒ½ç³»ç»Ÿçš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Trajectory data, capturing human actions and environmental states across various modalities, holds significant potential for enhancing AI agent capabilities, particularly in GUI environments. However, how to model the representation of trajectory-level data presents a significant challenge that has not been systematically addressed amid explosive trajectory data growth. In this work, we introduce Multimodal Trajectory Retrieval, bridging the gap between universal retrieval and agent-centric trajectory modeling. We construct the Unified Agent Trajectory Dataset (UATD) from annotated demonstrations and states across diverse real-world scenarios. Based on this, we present GAE-Bench, a benchmark containing a large number of trajectory-based retrieval pairs. In addition, we propose GAE-Retriever, a multimodal retrieval framework that adopts vision-language models and incorporates optimized contrastive learning through a token selection and the GradCache mechanism. Comprehensive evaluations across multiple datasets show that GAE-Retriever consistently outperforms strong baselines in retrieval recall, highlighting its effectiveness in advancing multimodal trajectory retrieval.

