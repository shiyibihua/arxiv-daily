---
layout: default
title: Mapping Overlaps in Benchmarks through Perplexity in the Wild
---

# Mapping Overlaps in Benchmarks through Perplexity in the Wild

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.23488" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.23488v3</a>
  <a href="https://arxiv.org/pdf/2509.23488.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.23488v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.23488v3', 'Mapping Overlaps in Benchmarks through Perplexity in the Wild')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Siyang Wu, Honglin Bao, Sida Li, Ari Holtzman, James A. Evans

**åˆ†ç±»**: cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-27 (æ›´æ–°: 2025-11-03)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é€šè¿‡å›°æƒ‘åº¦åˆ†æåŸºå‡†æµ‹è¯•é›†çš„é‡å åº¦ï¼Œæ­ç¤ºLLMèƒ½åŠ›é—´çš„å…³è”**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `åŸºå‡†æµ‹è¯•` `å›°æƒ‘åº¦` `èƒ½åŠ›è¯„ä¼°` `é‡å åˆ†æ` `æ¨¡å‹æ³›åŒ–` `çŸ¥è¯†è¡¨ç¤º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMåŸºå‡†æµ‹è¯•é›†ä¹‹é—´å­˜åœ¨é‡å ï¼Œå¯¼è‡´å¯¹æ¨¡å‹èƒ½åŠ›çš„è¯„ä¼°ä¸å¤Ÿå‡†ç¡®ï¼Œéš¾ä»¥åŒºåˆ†æ¨¡å‹çœŸæ­£æ“…é•¿çš„é¢†åŸŸã€‚
2. è®ºæ–‡æå‡ºåŸºäºå›°æƒ‘åº¦çš„åŸºå‡†æµ‹è¯•ç­¾åæ–¹æ³•ï¼Œé€šè¿‡åˆ†æLLMåœ¨ç‰¹å®štokenä¸Šçš„å›°æƒ‘åº¦æ¥è¯†åˆ«åŸºå‡†æµ‹è¯•é›†ä¹‹é—´çš„é‡å å’Œå·®å¼‚ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæ•æ‰ä¸åŒåŸºå‡†æµ‹è¯•é›†ä¹‹é—´çš„é‡å å…³ç³»ï¼Œå¹¶æ­ç¤ºLLMåœ¨ä¸åŒèƒ½åŠ›ä¸Šçš„è¡¨ç°å·®å¼‚ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå®¹é‡ç†Ÿæ‚‰åº¦ç‰¹å¾çš„æ–¹æ³•ï¼Œç”¨äºåˆ»ç”»å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åŸºå‡†æµ‹è¯•é›†åŠå…¶æœ‰æ„ä¹‰çš„é‡å ã€‚åŸºå‡†æµ‹è¯•ç­¾åæ¢ç©¶äº†åŸºå‡†æµ‹è¯•æ€§èƒ½æ‰€éœ€çš„èƒ½åŠ›ï¼Œå¹¶å°†å…¶æ­£å¼å®šä¹‰ä¸ºä¸€ç»„æ¥è‡ªçœŸå®ä¸–ç•Œè¯­æ–™åº“çš„æ˜¾è‘—tokenï¼Œå…¶ä¸­LLM tokenå›°æƒ‘åº¦ï¼ˆåæ˜ äº†é¢„è®­ç»ƒçš„æš´éœ²ç¨‹åº¦ï¼‰èƒ½å¤Ÿé«˜åº¦é¢„æµ‹LLMåŸºå‡†æµ‹è¯•æ€§èƒ½ã€‚é€šè¿‡å¤§è§„æ¨¡çš„å…ƒè¯„ä¼°ï¼Œæˆ‘ä»¬åˆ©ç”¨é€æ­¥å‰å‘é€‰æ‹©å’Œçº¿æ€§å›å½’ï¼Œåœ¨32ä¸ªLLMå’Œ88ä¸ªåŸºå‡†æµ‹è¯•é›†ï¼ˆæ¶µç›–çŸ¥è¯†ã€ç¼–ç ã€é€»è¾‘ã€æŒ‡ä»¤è·Ÿéšã€æ•°å­¦ã€è¯­è¨€ã€æ¨ç†å’Œä¸–ç•Œå»ºæ¨¡ç­‰é¢†åŸŸï¼‰ä¸­æå–åŸºå‡†æµ‹è¯•ç­¾åã€‚æˆ‘ä»¬çš„åˆ†æå°†ç­¾åä¸åŸºå‡†æµ‹è¯•é—®é¢˜çš„è¯­ä¹‰ç›¸ä¼¼æ€§å’Œæ¨¡å‹æ€§èƒ½çš„ç›¸å…³æ€§è”ç³»èµ·æ¥ã€‚è™½ç„¶æ€§èƒ½é‡å æ™®éè¾ƒé«˜ï¼Œè¯­ä¹‰é‡å ä»…é™äºç‹­çª„çš„ä¸­é—´èŒƒå›´ï¼Œä½†åŸºå‡†æµ‹è¯•ç­¾ååœ¨æ•è·å˜åŒ–ã€é‡å å’Œå·®å¼‚æ–¹é¢éå¸¸æœ‰æ•ˆã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°çŸ¥è¯†å’Œæ¨ç†å­ä»»åŠ¡å­˜åœ¨é‡å ï¼Œè€Œå¤šè¯­è¨€å’Œæ–‡åŒ–åŸºå‡†æµ‹è¯•çš„ç›¸ä¼¼æ€§è¾ƒä½ï¼Œç”šè‡³ä½äºè·¨ä»»åŠ¡é‡å ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ€§èƒ½å±‚é¢çš„ç»“æœå—åˆ°åŸºå‡†æµ‹è¯•æ­£äº¤å› ç´ ï¼ˆå¦‚é—®é¢˜æ ¼å¼ï¼‰çš„å¼ºçƒˆå½±å“ï¼Œçªå‡ºäº†LLMæ³›åŒ–çš„å±€é™æ€§ï¼Œæ€§èƒ½ä¸èƒ½åŠ›çš„æ··æ·†ï¼Œä»¥åŠå½“å‰ä¸»æµåŸºå‡†æµ‹è¯•ä¸€è‡´æ€§ç ”ç©¶ä¸­å›ºæœ‰çš„é—®é¢˜ã€‚ç„¶è€Œï¼ŒåŸºå‡†æµ‹è¯•ç­¾åå¯¹è¿™äº›å½±å“å…·æœ‰é²æ£’æ€§ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬è¯†åˆ«å‡ºé€»è¾‘ã€æ•°å­¦ã€è¯­è¨€ã€æŒ‡ä»¤è·Ÿéšå’Œä¸–ç•Œå»ºæ¨¡ä¹‹é—´çš„è·¨åŠŸèƒ½é‡å ï¼Œè€Œç¼–ç æ˜¯æœ€ä¸é‡å çš„é¢†åŸŸã€‚æ€»ä¹‹ï¼Œè¿™äº›å‘ç°ä¸ºåŸºå‡†æµ‹è¯•çš„æœ‰æ•ˆæ€§å’ŒLLMçš„æ•æ„Ÿæ€§æä¾›äº†æœºåˆ¶æ€§çš„è§è§£ï¼Œå¹¶å‹¾å‹’å‡ºç›¸äº’å…³è”çš„LLMèƒ½åŠ›çš„åŸºç¡€å›¾æ™¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åŸºå‡†æµ‹è¯•é›†å­˜åœ¨æ˜¾è‘—çš„é‡å ï¼Œè¿™æ„å‘³ç€æ¨¡å‹åœ¨æŸäº›åŸºå‡†æµ‹è¯•ä¸Šçš„é«˜åˆ†å¯èƒ½å¹¶éæºäºå…¶åœ¨è¯¥ç‰¹å®šé¢†åŸŸçš„å“è¶Šèƒ½åŠ›ï¼Œè€Œæ˜¯å› ä¸ºå…¶åœ¨å…¶ä»–ç›¸å…³é¢†åŸŸçš„çŸ¥è¯†æˆ–æŠ€èƒ½ã€‚è¿™ç§é‡å ä½¿å¾—æˆ‘ä»¬éš¾ä»¥å‡†ç¡®è¯„ä¼°LLMçš„çœŸå®èƒ½åŠ›ï¼Œå¹¶å¯èƒ½å¯¼è‡´å¯¹æ¨¡å‹æ€§èƒ½çš„è¯¯åˆ¤ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆè¯†åˆ«å’Œé‡åŒ–è¿™äº›é‡å ï¼Œä»è€Œé™åˆ¶äº†æˆ‘ä»¬å¯¹LLMèƒ½åŠ›è¾¹ç•Œçš„ç†è§£ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨LLMåœ¨ä¸åŒtokenä¸Šçš„å›°æƒ‘åº¦ï¼ˆPerplexityï¼‰ä½œä¸ºä¸€ç§â€œç­¾åâ€ï¼Œæ¥è¡¨å¾ä¸åŒåŸºå‡†æµ‹è¯•é›†æ‰€è€ƒå¯Ÿçš„èƒ½åŠ›ã€‚å›°æƒ‘åº¦åæ˜ äº†LLMå¯¹ç‰¹å®štokençš„ç†Ÿæ‚‰ç¨‹åº¦ï¼Œå¯ä»¥é—´æ¥åæ˜ LLMåœ¨é¢„è®­ç»ƒé˜¶æ®µæ¥è§¦åˆ°çš„ç›¸å…³çŸ¥è¯†æˆ–æŠ€èƒ½ã€‚å¦‚æœä¸¤ä¸ªåŸºå‡†æµ‹è¯•é›†çš„ç­¾åç›¸ä¼¼ï¼Œåˆ™è¡¨æ˜å®ƒä»¬è€ƒå¯Ÿçš„èƒ½åŠ›å­˜åœ¨é‡å ã€‚è¿™ç§æ–¹æ³•çš„æ ¸å¿ƒåœ¨äºå‡è®¾LLMçš„é¢„è®­ç»ƒæš´éœ²ç¨‹åº¦ä¸åŸºå‡†æµ‹è¯•æ€§èƒ½ä¹‹é—´å­˜åœ¨å…³è”ï¼Œå¹¶åˆ©ç”¨å›°æƒ‘åº¦æ¥é‡åŒ–è¿™ç§å…³è”ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦é˜¶æ®µï¼š1) **åŸºå‡†æµ‹è¯•é›†é€‰æ‹©**ï¼šé€‰æ‹©æ¶µç›–ä¸åŒé¢†åŸŸï¼ˆçŸ¥è¯†ã€ç¼–ç ã€é€»è¾‘ç­‰ï¼‰çš„LLMåŸºå‡†æµ‹è¯•é›†ã€‚2) **LLMé€‰æ‹©**ï¼šé€‰æ‹©å¤šä¸ªå…·æœ‰ä»£è¡¨æ€§çš„LLMè¿›è¡Œè¯„ä¼°ã€‚3) **ç­¾åæå–**ï¼šå¯¹äºæ¯ä¸ªåŸºå‡†æµ‹è¯•é›†ï¼Œæå–ä¸€ç»„æ˜¾è‘—çš„tokenä½œä¸ºå…¶ç­¾åï¼Œè¿™äº›tokençš„å›°æƒ‘åº¦ä¸LLMåœ¨è¯¥åŸºå‡†æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½å…·æœ‰é«˜åº¦ç›¸å…³æ€§ã€‚4) **é‡å åˆ†æ**ï¼šåˆ†æä¸åŒåŸºå‡†æµ‹è¯•é›†ç­¾åä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œä»è€Œç¡®å®šå®ƒä»¬ä¹‹é—´çš„é‡å ç¨‹åº¦ã€‚5) **æ€§èƒ½åˆ†æ**ï¼šå°†ç­¾åé‡å ä¸æ¨¡å‹æ€§èƒ½ç›¸å…³è”ï¼Œä»¥éªŒè¯ç­¾åçš„æœ‰æ•ˆæ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•çš„å…³é”®åˆ›æ–°åœ¨äºå°†å›°æƒ‘åº¦ä½œä¸ºä¸€ç§è¡¨å¾åŸºå‡†æµ‹è¯•é›†æ‰€éœ€èƒ½åŠ›çš„â€œç­¾åâ€ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºè¯­ä¹‰ç›¸ä¼¼æ€§çš„æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ•æ‰åŸºå‡†æµ‹è¯•é›†ä¹‹é—´çš„æ½œåœ¨é‡å ï¼Œå¹¶èƒ½å¤Ÿè¯†åˆ«é‚£äº›è¡¨é¢ä¸Šçœ‹èµ·æ¥ä¸åŒä½†å®é™…ä¸Šè€ƒå¯Ÿç›¸ä¼¼èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•é›†ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜èƒ½å¤Ÿæ­ç¤ºLLMåœ¨ä¸åŒèƒ½åŠ›ä¸Šçš„è¡¨ç°å·®å¼‚ï¼Œå¹¶ä¸ºåŸºå‡†æµ‹è¯•é›†çš„è®¾è®¡æä¾›æŒ‡å¯¼ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç­¾åæå–é˜¶æ®µï¼Œè®ºæ–‡é‡‡ç”¨äº†é€æ­¥å‰å‘é€‰æ‹©ï¼ˆStepwise Forward Selectionï¼‰å’Œçº¿æ€§å›å½’çš„æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œé¦–å…ˆé€‰æ‹©ä¸€ä¸ªä¸åŸºå‡†æµ‹è¯•æ€§èƒ½ç›¸å…³æ€§æœ€é«˜çš„tokenä½œä¸ºåˆå§‹ç­¾åï¼Œç„¶åé€æ­¥æ·»åŠ å…¶ä»–tokenï¼Œç›´åˆ°ç­¾åçš„é¢„æµ‹æ€§èƒ½ä¸å†æ˜¾è‘—æé«˜ã€‚çº¿æ€§å›å½’ç”¨äºé‡åŒ–tokenå›°æƒ‘åº¦ä¸åŸºå‡†æµ‹è¯•æ€§èƒ½ä¹‹é—´çš„å…³ç³»ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜è€ƒè™‘äº†åŸºå‡†æµ‹è¯•é›†çš„é—®é¢˜æ ¼å¼ç­‰å› ç´ å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œå¹¶è®¾è®¡äº†ç›¸åº”çš„å®éªŒæ¥éªŒè¯ç­¾åçš„é²æ£’æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæ•æ‰ä¸åŒåŸºå‡†æµ‹è¯•é›†ä¹‹é—´çš„é‡å å…³ç³»ï¼Œå¹¶æ­ç¤ºLLMåœ¨ä¸åŒèƒ½åŠ›ä¸Šçš„è¡¨ç°å·®å¼‚ã€‚ä¾‹å¦‚ï¼ŒçŸ¥è¯†å’Œæ¨ç†å­ä»»åŠ¡ä¹‹é—´å­˜åœ¨æ˜¾è‘—é‡å ï¼Œè€Œå¤šè¯­è¨€å’Œæ–‡åŒ–åŸºå‡†æµ‹è¯•çš„ç›¸ä¼¼æ€§è¾ƒä½ã€‚æ­¤å¤–ï¼Œå®éªŒè¿˜å‘ç°ï¼ŒåŸºå‡†æµ‹è¯•é›†çš„é—®é¢˜æ ¼å¼ç­‰å› ç´ ä¼šå¯¹æ¨¡å‹æ€§èƒ½äº§ç”Ÿæ˜¾è‘—å½±å“ï¼Œä½†è¯¥æ–¹æ³•æå–çš„ç­¾åå¯¹è¿™äº›å› ç´ å…·æœ‰é²æ£’æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºLLMåŸºå‡†æµ‹è¯•é›†çš„æ”¹è¿›å’Œè®¾è®¡ï¼Œå¸®åŠ©æ„å»ºæ›´å…·åŒºåˆ†åº¦å’Œä»£è¡¨æ€§çš„åŸºå‡†æµ‹è¯•é›†ï¼Œä»è€Œæ›´å‡†ç¡®åœ°è¯„ä¼°LLMçš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºåˆ†æLLMåœ¨ä¸åŒé¢†åŸŸçš„çŸ¥è¯†å’ŒæŠ€èƒ½ï¼Œä¸ºLLMçš„è®­ç»ƒå’Œä¼˜åŒ–æä¾›æŒ‡å¯¼ã€‚è¯¥ç ”ç©¶è¿˜æœ‰åŠ©äºç†è§£LLMèƒ½åŠ›ä¹‹é—´çš„ç›¸äº’å…³ç³»ï¼Œå¹¶ä¸ºå¼€å‘æ›´é€šç”¨å’Œå¼ºå¤§çš„LLMå¥ å®šåŸºç¡€ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We develop signatures of capacity familiarity to characterize large language model (LLM) benchmarks and their meaningful overlaps. Benchmark signatures probe the capacity required for benchmark performance. We formally define them as a set of salient tokens drawn from in-the-wild, naturally authored corpora, where LLM token perplexity, reflecting more or less pre-training exposure, becomes highly predictive of LLM benchmark performance. Through a large-scale meta-evaluation, we extract benchmark signatures via stepwise forward selection with linear regressions across 32 LLMs and 88 benchmarks spanning diverse knowledge, coding, logic, instruction following, math, language, reasoning, and world modeling. Our analysis situates signatures in relation to both the semantic similarity of benchmark questions and the correlation of model performance. While performance overlaps are universally high and semantic overlaps remain confined to a narrow mid-range, benchmark signatures prove highly informative in capturing variation, overlap, and divergence. We observe overlap in knowledge and reasoning subtasks, whereas multilingual and cultural benchmarks exhibit less similarity, even compared to cross-task overlap. Notably, performance-level results are strongly influenced by benchmark-orthogonal factors such as question format, highlighting limitations in LLM generalization, the conflation of performance with ability, and issues inherent in current mainstream benchmark agreement studies. Benchmark signatures, however, remain robust to such effects. Ultimately, we identify cross-functional overlaps across logic, math, language, instruction following, and world modeling, with coding emerging as the least overlapping domain. Together, these findings provide mechanistic insights into benchmark validity and LLM sensitivities, and sketch the underlying landscape of interconnected LLM capabilities.

