---
layout: default
title: Training Vision-Language Process Reward Models for Test-Time Scaling in Multimodal Reasoning: Key Insights and Lessons Learned
---

# Training Vision-Language Process Reward Models for Test-Time Scaling in Multimodal Reasoning: Key Insights and Lessons Learned

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.23250" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.23250v3</a>
  <a href="https://arxiv.org/pdf/2509.23250.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.23250v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.23250v3', 'Training Vision-Language Process Reward Models for Test-Time Scaling in Multimodal Reasoning: Key Insights and Lessons Learned')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Brandon Ong, Tej Deep Pala, Vernon Toh, William Chandra Tjhi, Soujanya Poria

**ÂàÜÁ±ª**: cs.AI, cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-27 (Êõ¥Êñ∞: 2025-10-07)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Ê∑∑ÂêàÊï∞ÊçÆÂêàÊàêÊ°ÜÊû∂ÂíåÊÑüÁü•ËÅöÁÑ¶ÁõëÁù£ÔºåÊèêÂçáËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÂ§öÊ®°ÊÄÅÊé®ÁêÜËÉΩÂäõ„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâËØ≠Ë®ÄÊ®°Âûã` `ËøáÁ®ãÂ•ñÂä±Ê®°Âûã` `Â§öÊ®°ÊÄÅÊé®ÁêÜ` `ÊµãËØïÊó∂Áº©Êîæ` `Êï∞ÊçÆÂêàÊàê` `ÊÑüÁü•ËÅöÁÑ¶ÁõëÁù£`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâËßÜËßâËØ≠Ë®ÄËøáÁ®ãÂ•ñÂä±Ê®°Âûã‰æùËµñMCTSÔºåÊòì‰∫ßÁîüÂô™Â£∞ÁõëÁù£‰ø°Âè∑ÔºåÈôêÂà∂‰∫ÜÊ®°ÂûãÂú®‰∏çÂêå‰ªªÂä°‰∏äÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ
2. ÊèêÂá∫Ê∑∑ÂêàÊï∞ÊçÆÂêàÊàêÊ°ÜÊû∂ÔºåÁªìÂêàMCTSÂíåVLMÂà§Êñ≠ÔºåÁîüÊàêÊõ¥ÂáÜÁ°ÆÁöÑÊ≠•È™§Á∫ßÊ†áÁ≠æÔºåÂπ∂ÂºïÂÖ•ÊÑüÁü•ËÅöÁÑ¶ÁõëÁù£„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®Â§ö‰∏™Â§öÊ®°ÊÄÅÂü∫ÂáÜÊµãËØï‰∏≠ÔºåËÉΩÂ§üÊúâÊïàÊèêÂçáËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõÔºåÂπ∂Êè≠Á§∫‰∫ÜÊΩúÂú®ÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊó®Âú®Êé¢Á¥¢ËßÜËßâËØ≠Ë®ÄËøáÁ®ãÂ•ñÂä±Ê®°ÂûãÔºàVL-PRMÔºâÁöÑËÆæËÆ°Á©∫Èó¥Ôºå‰ª•ÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã‰∏≠Êé®ÁêÜÁöÑÂèØÈù†ÊÄß„ÄÇÁé∞ÊúâVL-PRM‰æùËµñËíôÁâπÂç°Ê¥õÊ†ëÊêúÁ¥¢ÔºàMCTSÔºâÊûÑÂª∫Êï∞ÊçÆÔºå‰∫ßÁîüÂô™Â£∞ÁõëÁù£‰ø°Âè∑Âπ∂ÈôêÂà∂Ë∑®‰ªªÂä°Ê≥õÂåñ„ÄÇÊú¨ÊñáÊèêÂá∫‰∏ÄÁßçÊ∑∑ÂêàÊï∞ÊçÆÂêàÊàêÊ°ÜÊû∂ÔºåÁªìÂêàMCTSÂíåÂº∫Â§ßVLMÁöÑÂà§Êñ≠ÔºåÁîüÊàêÊõ¥ÂáÜÁ°ÆÁöÑÊ≠•È™§Á∫ßÊ†áÁ≠æ„ÄÇÊ≠§Â§ñÔºåÊèêÂá∫ÊÑüÁü•ËÅöÁÑ¶ÁõëÁù£Ôºå‰ΩøPRMËÉΩÂ§üÊòæÂºèÊ£ÄÊµãËßÜËßâÂü∫Á°ÄÈò∂ÊÆµÁöÑÈîôËØØ„ÄÇÁ≥ªÁªüËØÑ‰º∞Â§öÁßçÊµãËØïÊó∂Áº©ÊîæÁ≠ñÁï•ÔºåË°®ÊòéPRMËÉΩÂ§üÂèØÈù†Âú∞ÂºïÂØºVLMËé∑ÂæóÊõ¥ÂáÜÁ°ÆÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇÂÆûÈ™åÊ∂µÁõñ‰∫î‰∏™Â§öÊ®°ÊÄÅÂü∫ÂáÜÔºàMMMU„ÄÅPuzzleVQA„ÄÅAlgoPuzzleVQA„ÄÅMathVistaÂíåMathVisionÔºâÔºåÊè≠Á§∫‰∫ÜVL-PRM‰Ωú‰∏∫ÁªìÊûúÂ•ñÂä±Ê®°ÂûãÔºàORMÔºâÂú®ÊµãËØïÊó∂Áº©ÊîæÔºàTTSÔºâ‰∏≠‰ºò‰∫éVL-PRMÂºïÂØºÁöÑËøáÁ®ãÊ≠•È™§ÈÄâÊã©ÔºåËæÉÂ∞èÁöÑVL-PRMÂú®Ê£ÄÊµãËøáÁ®ãÈîôËØØÊñπÈù¢ÂèØ‰ª•ÂåπÈÖçÁîöËá≥Ë∂ÖËøáËæÉÂ§ßÁöÑVL-PRMÔºåVL-PRMÊè≠Á§∫‰∫ÜÊõ¥Âº∫VLMÈ™®Âπ≤ÁΩëÁªú‰∏≠ÁöÑÊΩúÂú®Êé®ÁêÜËÉΩÂäõÔºåÊÑüÁü•Á∫ßÂà´ÁõëÁù£ÊòæËëóÊèêÈ´ò‰∫ÜÊµãËØïÊó∂Áº©ÊîæÊïàÊûúÔºå‰ª•Âèä‰∏çÂêåÁ≠ñÁï•ÁöÑTTSÊÄßËÉΩÂú®È´òÁ∫ßÊï∞Â≠¶Êé®ÁêÜÊï∞ÊçÆÈõÜ‰∏äÊúâÊâÄÊèêÈ´òÔºåÂ∞ΩÁÆ°Ê≤°ÊúâÂú®Ëøô‰∫õÊï∞ÊçÆÈõÜ‰∏äËÆ≠ÁªÉVL-PRM„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâËßÜËßâËØ≠Ë®ÄËøáÁ®ãÂ•ñÂä±Ê®°ÂûãÔºàVL-PRMÔºâ‰æùËµñËíôÁâπÂç°Ê¥õÊ†ëÊêúÁ¥¢ÔºàMCTSÔºâËøõË°åÊï∞ÊçÆÊûÑÂª∫ÔºåËøô‰ºöÂØºËá¥‰∫ßÁîüÂ∏¶ÊúâÂô™Â£∞ÁöÑÁõëÁù£‰ø°Âè∑ÔºåÂπ∂‰∏îÈôêÂà∂‰∫ÜÊ®°ÂûãÂú®‰∏çÂêå‰ªªÂä°‰∏äÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇÂõ†Ê≠§ÔºåÂ¶Ç‰ΩïÊûÑÂª∫Êõ¥ÂáÜÁ°Æ„ÄÅÊõ¥ÊúâÊïàÁöÑVL-PRMËÆ≠ÁªÉÊï∞ÊçÆÔºåÂπ∂ÊèêÂçáÂÖ∂Âú®ÊµãËØïÊó∂ÁöÑÊé®ÁêÜËÉΩÂäõÔºåÊòØÊú¨ÊñáË¶ÅËß£ÂÜ≥ÁöÑÊ†∏ÂøÉÈóÆÈ¢ò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊú¨ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÁªìÂêàMCTSÂíåÂº∫Â§ßÁöÑËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâÁöÑÂà§Êñ≠ÔºåÊù•ÁîüÊàêÊõ¥ÂáÜÁ°ÆÁöÑÊ≠•È™§Á∫ßÊ†áÁ≠æÔºå‰ªéËÄåÊîπËøõVL-PRMÁöÑËÆ≠ÁªÉÊï∞ÊçÆË¥®Èáè„ÄÇÊ≠§Â§ñÔºåÂºïÂÖ•ÊÑüÁü•ËÅöÁÑ¶ÁõëÁù£Ôºå‰ΩøPRMËÉΩÂ§üÊòæÂºèÂú∞Ê£ÄÊµãËßÜËßâÂü∫Á°ÄÈò∂ÊÆµÁöÑÈîôËØØÔºå‰ªéËÄåÊèêÂçáÊ®°ÂûãÂØπËßÜËßâ‰ø°ÊÅØÁöÑÁêÜËß£ÂíåÂà©Áî®ËÉΩÂäõ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÂê´Êï∞ÊçÆÂêàÊàê„ÄÅÊ®°ÂûãËÆ≠ÁªÉÂíåÊµãËØïÊó∂Áº©Êîæ‰∏â‰∏™‰∏ªË¶ÅÈò∂ÊÆµ„ÄÇÊï∞ÊçÆÂêàÊàêÈò∂ÊÆµÔºåÈááÁî®Ê∑∑ÂêàÁ≠ñÁï•ÔºåÁªìÂêàMCTSÂíåVLMÁöÑÂà§Êñ≠ÔºåÁîüÊàêÊ≠•È™§Á∫ßÁöÑÂ•ñÂä±‰ø°Âè∑„ÄÇÊ®°ÂûãËÆ≠ÁªÉÈò∂ÊÆµÔºå‰ΩøÁî®ÂêàÊàêÁöÑÊï∞ÊçÆËÆ≠ÁªÉVL-PRMÔºåÂπ∂ÂºïÂÖ•ÊÑüÁü•ËÅöÁÑ¶ÁõëÁù£„ÄÇÊµãËØïÊó∂Áº©ÊîæÈò∂ÊÆµÔºåËØÑ‰º∞‰∏çÂêåÁöÑÁ≠ñÁï•ÔºåÂà©Áî®ËÆ≠ÁªÉÂ•ΩÁöÑVL-PRMÂºïÂØºVLMËøõË°åÊé®ÁêÜ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊú¨ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÊ∑∑ÂêàÊï∞ÊçÆÂêàÊàêÊ°ÜÊû∂ÂíåÊÑüÁü•ËÅöÁÑ¶ÁõëÁù£„ÄÇÊ∑∑ÂêàÊï∞ÊçÆÂêàÊàêÊ°ÜÊû∂ÈÄöËøáÁªìÂêàMCTSÂíåVLMÁöÑÂà§Êñ≠ÔºåÊúâÊïàÈôç‰Ωé‰∫ÜÊï∞ÊçÆ‰∏≠ÁöÑÂô™Â£∞ÔºåÊèêÂçá‰∫ÜÊï∞ÊçÆË¥®Èáè„ÄÇÊÑüÁü•ËÅöÁÑ¶ÁõëÁù£Âàô‰ΩøPRMËÉΩÂ§üÊòæÂºèÂú∞Ê£ÄÊµãËßÜËßâÂü∫Á°ÄÈò∂ÊÆµÁöÑÈîôËØØÔºå‰ªéËÄåÊèêÂçá‰∫ÜÊ®°ÂûãÂØπËßÜËßâ‰ø°ÊÅØÁöÑÁêÜËß£ÂíåÂà©Áî®ËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Êï∞ÊçÆÂêàÊàêÈò∂ÊÆµÔºåMCTSÁî®‰∫éÊé¢Á¥¢ÂèØËÉΩÁöÑÊé®ÁêÜË∑ØÂæÑÔºåËÄåVLMÂàôÁî®‰∫éËØÑ‰º∞Ëøô‰∫õË∑ØÂæÑÁöÑË¥®ÈáèÔºåÂπ∂Êèê‰æõÊõ¥ÂáÜÁ°ÆÁöÑÂ•ñÂä±‰ø°Âè∑„ÄÇÊÑüÁü•ËÅöÁÑ¶ÁõëÁù£ÈÄöËøáÂºïÂÖ•È¢ùÂ§ñÁöÑÊçüÂ§±ÂáΩÊï∞Ôºå‰øÉ‰ΩøPRMÂÖ≥Ê≥®ËßÜËßâÂü∫Á°ÄÈò∂ÊÆµÁöÑÈîôËØØ„ÄÇÂú®ÊµãËØïÊó∂Áº©ÊîæÈò∂ÊÆµÔºåËØÑ‰º∞‰∫ÜÂ§öÁßçÁ≠ñÁï•ÔºåÂåÖÊã¨‰ΩøÁî®VL-PRM‰Ωú‰∏∫ÁªìÊûúÂ•ñÂä±Ê®°ÂûãÔºàORMÔºâÂíå‰ΩøÁî®VL-PRMÂºïÂØºÁöÑËøáÁ®ãÊ≠•È™§ÈÄâÊã©„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåVL-PRM‰Ωú‰∏∫ÁªìÊûúÂ•ñÂä±Ê®°ÂûãÔºàORMÔºâÂú®ÊµãËØïÊó∂Áº©ÊîæÔºàTTSÔºâ‰∏≠‰ºò‰∫éVL-PRMÂºïÂØºÁöÑËøáÁ®ãÊ≠•È™§ÈÄâÊã©„ÄÇËæÉÂ∞èÁöÑVL-PRMÂú®Ê£ÄÊµãËøáÁ®ãÈîôËØØÊñπÈù¢ÂèØ‰ª•ÂåπÈÖçÁîöËá≥Ë∂ÖËøáËæÉÂ§ßÁöÑVL-PRM„ÄÇÊÑüÁü•Á∫ßÂà´ÁõëÁù£ÊòæËëóÊèêÈ´ò‰∫ÜÊµãËØïÊó∂Áº©ÊîæÊïàÊûú„ÄÇÂú®È´òÁ∫ßÊï∞Â≠¶Êé®ÁêÜÊï∞ÊçÆÈõÜ‰∏äÔºå‰∏çÂêåÁ≠ñÁï•ÁöÑTTSÊÄßËÉΩÊúâÊâÄÊèêÈ´òÔºåÂç≥‰ΩøÊ≤°ÊúâÂú®Ëøô‰∫õÊï∞ÊçÆÈõÜ‰∏äËÆ≠ÁªÉVL-PRM„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÈúÄË¶ÅÂ§öÊ®°ÊÄÅÊé®ÁêÜÁöÑÂú∫ÊôØÔºå‰æãÂ¶ÇÊô∫ËÉΩÈóÆÁ≠î„ÄÅËßÜËßâÂØºËà™„ÄÅÊú∫Âô®‰∫∫Êìç‰ΩúÁ≠â„ÄÇÈÄöËøáÊèêÂçáËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõÔºåÂèØ‰ª•ÊèêÈ´òËøô‰∫õÂ∫îÁî®Âú®Â§çÊùÇÁéØÂ¢É‰∏ãÁöÑÂèØÈù†ÊÄßÂíåÂáÜÁ°ÆÊÄßÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖÂ∫îÁî®‰ª∑ÂÄºÂíåÊΩúÂú®ÁöÑÂïÜ‰∏öÂâçÊôØ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Process Reward Models (PRMs) provide step-level supervision that improves the reliability of reasoning in large language models. While PRMs have been extensively studied in text-based domains, their extension to Vision Language Models (VLMs) remains limited. Existing Vision-Language PRMs (VL-PRMs) rely on Monte Carlo Tree Search (MCTS) for data construction, which can often produce noisy supervision signals and limit generalization across tasks. In this work, we aim to elucidate the design space of VL-PRMs by exploring diverse strategies for dataset construction, training, and test-time scaling. First, we introduce a hybrid data synthesis framework that combines MCTS with judgments from a strong VLM, producing more accurate step-level labels. Second, we propose perception-focused supervision, enabling our PRM to explicitly detect errors at the visual grounding stage of reasoning. Third, we systematically evaluate multiple test-time scaling strategies, showing that our PRMs can reliably guide VLMs toward more accurate solutions. Our experiments covering five diverse multimodal benchmarks (MMMU, PuzzleVQA, AlgoPuzzleVQA, MathVista, and MathVision) reveal several key insights: (i) VL-PRMs when used as Outcome Reward Models (ORMs) during test-time scaling (TTS) can outperform VL-PRM guided process step selection, (ii) smaller VL-PRMs can match or even surpass larger ones in detecting process errors, (iii) VL-PRMs uncover latent reasoning abilities in stronger VLM backbones, (iv) perception-level supervision leads to significant gains in test-time scaling, and (v) TTS performance of different policies improve on advanced math reasoning datasets despite not training VL-PRMs on such datasets. We hope our work will motivate further research and support the advancement of VLMs.

