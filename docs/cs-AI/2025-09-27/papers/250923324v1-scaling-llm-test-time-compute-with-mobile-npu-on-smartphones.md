---
layout: default
title: Scaling LLM Test-Time Compute with Mobile NPU on Smartphones
---

# Scaling LLM Test-Time Compute with Mobile NPU on Smartphones

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.23324" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.23324v1</a>
  <a href="https://arxiv.org/pdf/2509.23324.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.23324v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.23324v1', 'Scaling LLM Test-Time Compute with Mobile NPU on Smartphones')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Zixu Hao, Jianyu Wei, Tuowei Wang, Minxing Huang, Huiqiang Jiang, Shiqi Jiang, Ting Cao, Ju Ren

**ÂàÜÁ±ª**: cs.DC, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-27

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Èù¢ÂêëÁßªÂä®NPUÁöÑLLMÊµãËØïÊó∂Âπ∂Ë°åÊâ©Â±ïÊñπÊ≥ïÔºåÊèêÂçáÂ∞èÊ®°ÂûãÊÄßËÉΩ„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÁßªÂä®NPU` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `ÊµãËØïÊó∂Êâ©Â±ï` `Ê®°ÂûãÈáèÂåñ` `Á°¨‰ª∂ÊÑüÁü•` `Êé®ÁêÜÂä†ÈÄü` `‰ΩéËµÑÊ∫êÈÉ®ÁΩ≤`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁßªÂä®Á´ØLLMÈÉ®ÁΩ≤Èù¢‰∏¥Â∞èÊ®°ÂûãÊÄßËÉΩ‰∏çË∂≥„ÄÅÂ§ßÊ®°ÂûãËµÑÊ∫êÊ∂àËÄóËøáÈ´òÁöÑÈöæÈ¢òÔºåNPUËÆ°ÁÆóËµÑÊ∫êÊú™Ë¢´ÂÖÖÂàÜÂà©Áî®„ÄÇ
2. ÊèêÂá∫Âπ∂Ë°åÊµãËØïÊó∂Êâ©Â±ïÊäÄÊúØÔºåÁªìÂêàÁ°¨‰ª∂ÊÑüÁü•ÂàÜÂùóÈáèÂåñÂíåLUT‰ºòÂåñÔºåÂÖÖÂàÜÂà©Áî®NPUÁü©Èòµ‰πòÊ≥ïÂçïÂÖÉ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåËØ•ÊñπÊ≥ïÊòæËëóÂä†ÈÄüÊé®ÁêÜËøáÁ®ãÔºå‰ΩøÂ∞èÊ®°ÂûãÂú®Á≤æÂ∫¶‰∏äÂèØÂ™≤ÁæéÁîöËá≥Ë∂ÖË∂äÂ§ßÊ®°Âûã„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂú®ÁßªÂä®ËÆæÂ§á‰∏äÈÉ®ÁΩ≤Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÊâÄÈù¢‰∏¥ÁöÑÊåëÊàòÔºåÂç≥Â∞èÊ®°ÂûãÊÄßËÉΩ‰∏çË∂≥ÂíåÂ§ßÊ®°ÂûãËµÑÊ∫êÊ∂àËÄóËøáÈ´ò„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÂú®ÂÖ∏ÂûãÁöÑLLMÊé®ÁêÜËøáÁ®ã‰∏≠ÔºåÁßªÂä®Á•ûÁªèÂ§ÑÁêÜÂçïÂÖÉÔºàNPUÔºâÁöÑËÆ°ÁÆóËµÑÊ∫êÔºåÁâπÂà´ÊòØÂÖ∂Áü©Èòµ‰πòÊ≥ïÂçïÂÖÉÔºåÂπ∂Êú™ÂæóÂà∞ÂÖÖÂàÜÂà©Áî®„ÄÇ‰∏∫‰∫ÜÂà©Áî®Ëøô‰∫õÊú™Ë¢´ÂÖÖÂàÜÂà©Áî®ÁöÑËÆ°ÁÆóËÉΩÂäõÔºåÊú¨ÊñáÊèêÂá∫Âú®ÁßªÂä®NPU‰∏äÂ∫îÁî®Âπ∂Ë°åÊµãËØïÊó∂Êâ©Â±ïÊäÄÊúØÔºå‰ª•ÊèêÈ´òËæÉÂ∞èLLMÁöÑÊÄßËÉΩ„ÄÇÁÑ∂ËÄåÔºåËøôÁßçÊñπÊ≥ïÈù¢‰∏¥NPUÂõ∫ÊúâÁöÑÊåëÊàòÔºåÂåÖÊã¨ÂØπÁªÜÁ≤íÂ∫¶ÈáèÂåñÁöÑÁ°¨‰ª∂ÊîØÊåÅ‰∏çË∂≥‰ª•ÂèäÈÄöÁî®ËÆ°ÁÆóÊïàÁéá‰Ωé‰∏ã„ÄÇ‰∏∫‰∫ÜÂÖãÊúçËøô‰∫õÈóÆÈ¢òÔºåÊú¨ÊñáÂºïÂÖ•‰∫Ü‰∏§È°πÂÖ≥ÈîÆÊäÄÊúØÔºö‰∏ÄÁßçÁ°¨‰ª∂ÊÑüÁü•ÁöÑÂàÜÂùóÈáèÂåñÊñπÊ°àÔºå‰ΩøÁªÑÈáèÂåñ‰∏éNPUÂÜÖÂ≠òËÆøÈóÆÊ®°ÂºèÂØπÈΩêÔºõ‰ª•ÂèäÂü∫‰∫éLUTÁöÑÈ´òÊïàÊõøÊç¢ÊñπÊ°àÔºåÁî®‰∫éÊõø‰ª£ËØ∏Â¶ÇSoftmaxÂíåÂèçÈáèÂåñÁ≠âÂ§çÊùÇÊìç‰Ωú„ÄÇËÆæËÆ°Âπ∂ÂÆûÁé∞‰∫Ü‰∏Ä‰∏™Á´ØÂà∞Á´ØÁöÑÊé®ÁêÜÁ≥ªÁªüÔºåËØ•Á≥ªÁªüÂà©Áî®NPUÁöÑËÆ°ÁÆóËÉΩÂäõÊù•ÊîØÊåÅÈ´òÈÄöÈ™ÅÈæôÂπ≥Âè∞‰∏äÁöÑÊµãËØïÊó∂Êâ©Â±ï„ÄÇÂÆûÈ™åË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂ∏¶Êù•‰∫ÜÊòæËëóÁöÑÂä†ÈÄüÔºöÊ∑∑ÂêàÁ≤æÂ∫¶GEMMÈ´òËææ19.0ÂÄçÔºåSoftmaxÈ´òËææ2.2ÂÄç„ÄÇÊõ¥ÈáçË¶ÅÁöÑÊòØÔºåËØÅÊòé‰∫Ü‰ΩøÁî®ÊµãËØïÊó∂Êâ©Â±ïÁöÑËæÉÂ∞èÊ®°ÂûãÂèØ‰ª•ÂåπÈÖçÊàñË∂ÖËøáËæÉÂ§ßÊ®°ÂûãÁöÑÂáÜÁ°ÆÊÄßÔºå‰ªéËÄåÂÆûÁé∞‰∫ÜÊñ∞ÁöÑÊÄßËÉΩ-ÊàêÊú¨Â∏ïÁ¥ØÊâòÂâçÊ≤ø„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Âú®ÁßªÂä®ËÆæÂ§á‰∏äÈÉ®ÁΩ≤LLMÊó∂ÔºåÂ∞èÊ®°ÂûãÊÄßËÉΩ‰∏çË∂≥ËÄåÂ§ßÊ®°ÂûãËµÑÊ∫êÊ∂àËÄóËøáÈ´òÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÁöÑÁßªÂä®Á´ØLLMÊé®ÁêÜÊñπÊ≥ïÈöæ‰ª•ÂÖÖÂàÜÂà©Áî®NPUÁöÑËÆ°ÁÆóËÉΩÂäõÔºåÁâπÂà´ÊòØÁü©Èòµ‰πòÊ≥ïÂçïÂÖÉÁöÑÁÆóÂäõÔºåÂØºËá¥Êé®ÁêÜÊïàÁéá‰Ωé‰∏ã„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®ÊµãËØïÊó∂Êâ©Â±ïÊäÄÊúØÔºåÈÄöËøáÂπ∂Ë°åËÆ°ÁÆóÊù•ÊèêÂçáÂ∞èÊ®°ÂûãÁöÑÊÄßËÉΩÔºå‰ΩøÂÖ∂ËææÂà∞ÁîöËá≥Ë∂ÖËøáÂ§ßÊ®°ÂûãÁöÑÁ≤æÂ∫¶„ÄÇÂÖ≥ÈîÆÂú®‰∫éÂÖÖÂàÜÊåñÊéòÂíåÂà©Áî®ÁßªÂä®NPU‰∏≠Êú™Ë¢´ÂÖÖÂàÜÂà©Áî®ÁöÑËÆ°ÁÆóËµÑÊ∫êÔºåÂπ∂ÂÖãÊúçNPUÁ°¨‰ª∂ÁöÑÈôêÂà∂„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËØ•ËÆ∫ÊñáÊûÑÂª∫‰∫Ü‰∏Ä‰∏™Á´ØÂà∞Á´ØÁöÑÊé®ÁêÜÁ≥ªÁªüÔºå‰∏ªË¶ÅÂåÖÂê´‰ª•‰∏ãÂá†‰∏™Èò∂ÊÆµÔºö1) Ê®°ÂûãÈáèÂåñÔºöÈááÁî®Á°¨‰ª∂ÊÑüÁü•ÁöÑÂàÜÂùóÈáèÂåñÊñπÊ°àÔºåÂ∞ÜÊ®°ÂûãÂèÇÊï∞ÈáèÂåñ‰∏∫ËæÉ‰ΩéÁ≤æÂ∫¶Ôºå‰ª•ÈÄÇÂ∫îNPUÁöÑÁ°¨‰ª∂ÁâπÊÄß„ÄÇ2) Âπ∂Ë°åËÆ°ÁÆóÔºöÂà©Áî®NPUÁöÑÂπ∂Ë°åËÆ°ÁÆóËÉΩÂäõÔºåÂØπÈáèÂåñÂêéÁöÑÊ®°ÂûãËøõË°åÂπ∂Ë°åÊé®ÁêÜ„ÄÇ3) Êìç‰Ωú‰ºòÂåñÔºö‰ΩøÁî®Âü∫‰∫éLUTÁöÑÊõøÊç¢ÊñπÊ°àÔºå‰ºòÂåñSoftmaxÂíåÂèçÈáèÂåñÁ≠âÂ§çÊùÇÊìç‰ΩúÔºåÊèêÈ´òËÆ°ÁÆóÊïàÁéá„ÄÇ4) ÈÉ®ÁΩ≤‰∏éÊé®ÁêÜÔºöÂ∞Ü‰ºòÂåñÂêéÁöÑÊ®°ÂûãÈÉ®ÁΩ≤Âà∞È´òÈÄöÈ™ÅÈæôÂπ≥Âè∞ÁöÑNPU‰∏äËøõË°åÊé®ÁêÜ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÔºö1) ÊèêÂá∫‰∫ÜÁ°¨‰ª∂ÊÑüÁü•ÁöÑÂàÜÂùóÈáèÂåñÊñπÊ°àÔºåËØ•ÊñπÊ°à‰∏éNPUÁöÑÂÜÖÂ≠òËÆøÈóÆÊ®°ÂºèÂØπÈΩêÔºåÊèêÈ´ò‰∫ÜÈáèÂåñÊïàÁéá„ÄÇ2) ‰ΩøÁî®Âü∫‰∫éLUTÁöÑÊõøÊç¢ÊñπÊ°àÔºå‰ºòÂåñ‰∫ÜSoftmaxÂíåÂèçÈáèÂåñÁ≠âÂ§çÊùÇÊìç‰ΩúÔºåÊòæËëóÊèêÈ´ò‰∫ÜËÆ°ÁÆóÈÄüÂ∫¶„ÄÇ3) Â∞ÜÊµãËØïÊó∂Êâ©Â±ïÊäÄÊúØÂ∫îÁî®‰∫éÁßªÂä®NPUÔºåÂÖÖÂàÜÂà©Áî®‰∫ÜNPUÁöÑÂπ∂Ë°åËÆ°ÁÆóËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÁ°¨‰ª∂ÊÑüÁü•ÁöÑÂàÜÂùóÈáèÂåñÊñπÊ°àÁöÑÂÖ≥ÈîÆÂú®‰∫éÊ†πÊçÆNPUÁöÑÂÜÖÂ≠òËÆøÈóÆÊ®°ÂºèÔºåÂ∞ÜÊ®°ÂûãÂèÇÊï∞ÂàíÂàÜ‰∏∫‰∏çÂêåÁöÑÂùóÔºåÂπ∂ÂØπÊØè‰∏™ÂùóËøõË°åÈáèÂåñ„ÄÇÂü∫‰∫éLUTÁöÑÊõøÊç¢ÊñπÊ°àÁöÑÂÖ≥ÈîÆÂú®‰∫éÈ¢ÑÂÖàËÆ°ÁÆóSoftmaxÂíåÂèçÈáèÂåñÁöÑÁªìÊûúÔºåÂπ∂Â∞ÜÂÖ∂Â≠òÂÇ®Âú®LUT‰∏≠ÔºåÂú®Êé®ÁêÜÊó∂Áõ¥Êé•Êü•Ë°®Ëé∑ÂèñÁªìÊûúÔºåÈÅøÂÖç‰∫ÜÂ§çÊùÇÁöÑËÆ°ÁÆóËøáÁ®ã„ÄÇÂÖ∑‰ΩìÂèÇÊï∞ËÆæÁΩÆÂíåÁΩëÁªúÁªìÊûÑÁªÜËäÇÊú™Âú®ÊëòË¶Å‰∏≠ËØ¶ÁªÜËØ¥ÊòéÔºåÂ±û‰∫éÊú™Áü•‰ø°ÊÅØ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®Ê∑∑ÂêàÁ≤æÂ∫¶GEMM‰∏äÂÆûÁé∞‰∫ÜÈ´òËææ19.0ÂÄçÁöÑÂä†ÈÄüÔºåÂú®SoftmaxÊìç‰Ωú‰∏äÂÆûÁé∞‰∫ÜÈ´òËææ2.2ÂÄçÁöÑÂä†ÈÄü„ÄÇÊõ¥ÈáçË¶ÅÁöÑÊòØÔºå‰ΩøÁî®ÊµãËØïÊó∂Êâ©Â±ïÁöÑËæÉÂ∞èÊ®°ÂûãÂèØ‰ª•ÂåπÈÖçÊàñË∂ÖËøáËæÉÂ§ßÊ®°ÂûãÁöÑÂáÜÁ°ÆÊÄßÔºåÂÆûÁé∞‰∫ÜÊñ∞ÁöÑÊÄßËÉΩ-ÊàêÊú¨Â∏ïÁ¥ØÊâòÂâçÊ≤ø„ÄÇËøô‰∫õÁªìÊûúÈ™åËØÅ‰∫ÜËØ•ÊñπÊ≥ïÂú®ÊèêÂçáÁßªÂä®Á´ØLLMÊé®ÁêÜÊïàÁéáÊñπÈù¢ÁöÑÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂπøÊ≥õÂ∫îÁî®‰∫éÁßªÂä®ËÆæÂ§á‰∏äÁöÑLLMÈÉ®ÁΩ≤Ôºå‰æãÂ¶ÇÊô∫ËÉΩÂä©Êâã„ÄÅÊú∫Âô®ÁøªËØë„ÄÅÊñáÊú¨ÊëòË¶ÅÁ≠â„ÄÇÈÄöËøáÊèêÂçáÂ∞èÊ®°ÂûãÁöÑÊÄßËÉΩÔºåÈôç‰ΩéËµÑÊ∫êÊ∂àËÄóÔºå‰ΩøÂæóÂú®ËµÑÊ∫êÂèóÈôêÁöÑÁßªÂä®ËÆæÂ§á‰∏äËøêË°åÂ§çÊùÇÁöÑLLMÊàê‰∏∫ÂèØËÉΩÔºå‰ªéËÄåÊîπÂñÑÁî®Êà∑‰ΩìÈ™åÔºåÂπ∂Êé®Âä®ÁßªÂä®‰∫∫Â∑•Êô∫ËÉΩÁöÑÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Deploying Large Language Models (LLMs) on mobile devices faces the challenge of insufficient performance in smaller models and excessive resource consumption in larger ones. This paper highlights that mobile Neural Processing Units (NPUs) have underutilized computational resources, particularly their matrix multiplication units, during typical LLM inference. To leverage this wasted compute capacity, we propose applying parallel test-time scaling techniques on mobile NPUs to enhance the performance of smaller LLMs. However, this approach confronts inherent NPU challenges, including inadequate hardware support for fine-grained quantization and low efficiency in general-purpose computations. To overcome these, we introduce two key techniques: a hardware-aware tile quantization scheme that aligns group quantization with NPU memory access patterns, and efficient LUT-based replacements for complex operations such as Softmax and dequantization. We design and implement an end-to-end inference system that leverages the NPU's compute capability to support test-time scaling on Qualcomm Snapdragon platforms. Experiments show our approach brings significant speedups: up to 19.0 for mixed-precision GEMM and 2.2 for Softmax. More importantly, we demonstrate that smaller models using test-time scaling can match or exceed the accuracy of larger models, achieving a new performance-cost Pareto frontier.

