---
layout: default
title: Toward Effective Tool-Integrated Reasoning via Self-Evolved Preference Learning
---

# Toward Effective Tool-Integrated Reasoning via Self-Evolved Preference Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.23285" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.23285v2</a>
  <a href="https://arxiv.org/pdf/2509.23285.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.23285v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.23285v2', 'Toward Effective Tool-Integrated Reasoning via Self-Evolved Preference Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yifei Chen, Guanting Dong, Zhicheng Dou

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-27 (æ›´æ–°: 2025-09-30)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTool-Lightæ¡†æ¶ï¼Œé€šè¿‡è‡ªè¿›åŒ–åå¥½å­¦ä¹ æå‡LLMå·¥å…·é›†æˆæ¨ç†çš„æ•ˆç‡ä¸å‡†ç¡®æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å·¥å…·é›†æˆæ¨ç†` `å¤§å‹è¯­è¨€æ¨¡å‹` `ä¿¡æ¯ç†µ` `è‡ªè¿›åŒ–å­¦ä¹ ` `åå¥½ä¼˜åŒ–` `ç›‘ç£å¾®è°ƒ` `ç›´æ¥åå¥½ä¼˜åŒ–` `LLM`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å·¥å…·é›†æˆæ¨ç†æ–¹æ³•å­˜åœ¨å·¥å…·ä½¿ç”¨æ•ˆç‡ä½ã€å‡†ç¡®æ€§å·®ç­‰é—®é¢˜ï¼Œéš¾ä»¥å……åˆ†å‘æŒ¥LLMçš„æ¨ç†èƒ½åŠ›ã€‚
2. Tool-Lightæ¡†æ¶é€šè¿‡åˆ†æå·¥å…·è°ƒç”¨å¯¹ä¿¡æ¯ç†µçš„å½±å“ï¼ŒæŒ‡å¯¼æ¨¡å‹æ›´æœ‰æ•ˆåœ°åˆ©ç”¨å·¥å…·ï¼Œé¿å…è¿‡åº¦æˆ–ä¸è¶³çš„ä½¿ç”¨ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒTool-Lightåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†LLMåœ¨å·¥å…·é›†æˆæ¨ç†ä»»åŠ¡ä¸­çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç ”ç©¶äº†å·¥å…·é›†æˆæ¨ç†(TIR)ä¸­ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹(LLM)åœ¨å·¥å…·ä½¿ç”¨ä¸Šå­˜åœ¨çš„æ•ˆç‡å’Œå‡†ç¡®æ€§é—®é¢˜ï¼Œä¾‹å¦‚å·¥å…·ä½¿ç”¨ä¸è¶³æˆ–è¿‡åº¦ï¼Œä»¥åŠå·¥å…·è°ƒç”¨åçš„è¿‡åº¦æ€è€ƒã€‚é€šè¿‡åˆ†æå·¥å…·è°ƒç”¨å¯¹æ¨¡å‹æ¨ç†è¿‡ç¨‹ä¿¡æ¯ç†µçš„å½±å“ï¼Œå‘ç°å·¥å…·è°ƒç”¨ç»“æœæ˜¾è‘—æ”¹å˜åç»­æ¨ç†çš„ä¿¡æ¯ç†µï¼Œä¸”æ¨ç†é“¾çš„æ•´ä½“ç†µå€¼éšå·¥å…·è°ƒç”¨æ¬¡æ•°å˜åŒ–ã€‚åŸºäºæ­¤ï¼Œæå‡ºäº†Tool-Lightæ¡†æ¶ï¼Œæ—¨åœ¨é¼“åŠ±LLMé«˜æ•ˆå‡†ç¡®åœ°æ‰§è¡ŒTIRä»»åŠ¡ã€‚è¯¥æ¡†æ¶åŒ…å«æ•°æ®é›†æ„å»ºå’Œå¤šé˜¶æ®µå¾®è°ƒï¼Œæ•°æ®é›†æ„å»ºé‡‡ç”¨åŸºäºå¾®è°ƒæ¨¡å‹çš„è¿ç»­è‡ªè¿›åŒ–é‡‡æ ·ï¼Œèåˆäº†æ™®é€šé‡‡æ ·å’Œç†µå¼•å¯¼é‡‡æ ·ï¼Œå¹¶å»ºç«‹äº†ä¸¥æ ¼çš„æ­£è´Ÿæ ·æœ¬å¯¹é€‰æ‹©æ ‡å‡†ã€‚è®­ç»ƒè¿‡ç¨‹é‡‡ç”¨ä¸¤é˜¶æ®µæ–¹æ³•ï¼ŒåŒ…æ‹¬ç›‘ç£å¾®è°ƒ(SFT)å’Œè‡ªè¿›åŒ–ç›´æ¥åå¥½ä¼˜åŒ–(DPO)ã€‚åœ¨10ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒTool-Lightèƒ½æ˜¾è‘—æé«˜æ¨¡å‹æ‰§è¡ŒTIRä»»åŠ¡çš„æ•ˆç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¿›è¡Œå·¥å…·é›†æˆæ¨ç†æ—¶ï¼Œå¸¸å¸¸è¡¨ç°å‡ºæ¬¡ä¼˜è¡Œä¸ºã€‚å…·ä½“è¡¨ç°ä¸ºï¼šå·¥å…·ä½¿ç”¨ä¸è¶³ï¼Œå¯¼è‡´æ— æ³•å……åˆ†åˆ©ç”¨å¤–éƒ¨ä¿¡æ¯ï¼›å·¥å…·ä½¿ç”¨è¿‡åº¦ï¼Œé€ æˆä¸å¿…è¦çš„è®¡ç®—å¼€é”€ï¼›åœ¨å·¥å…·è°ƒç”¨åå‡ºç°è¿‡åº¦æ€è€ƒï¼Œåè€Œé™ä½äº†æ¨ç†æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚è¿™äº›é—®é¢˜é˜»ç¢äº†LLMåœ¨å¤æ‚ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ä¿¡æ¯ç†µæ¥æŒ‡å¯¼LLMçš„å·¥å…·ä½¿ç”¨ã€‚ç ”ç©¶å‘ç°ï¼Œå·¥å…·è°ƒç”¨ä¼šæ˜¾è‘—æ”¹å˜åç»­æ¨ç†è¿‡ç¨‹çš„ä¿¡æ¯ç†µï¼Œå› æ­¤å¯ä»¥é€šè¿‡æ§åˆ¶ä¿¡æ¯ç†µçš„å˜åŒ–æ¥ä¼˜åŒ–å·¥å…·çš„ä½¿ç”¨ã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡é¼“åŠ±æ¨¡å‹åœ¨ä¿¡æ¯ç†µè¾ƒä½æ—¶æ›´å¤šåœ°åˆ©ç”¨å·¥å…·ï¼Œè€Œåœ¨ä¿¡æ¯ç†µè¾ƒé«˜æ—¶å‡å°‘å·¥å…·çš„ä½¿ç”¨ï¼Œä»è€Œæé«˜æ¨ç†çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTool-Lightæ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ï¼šæ•°æ®é›†æ„å»ºå’Œå¤šé˜¶æ®µå¾®è°ƒã€‚æ•°æ®é›†æ„å»ºé˜¶æ®µï¼Œé¦–å…ˆä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹è¿›è¡Œè¿ç»­è‡ªè¿›åŒ–é‡‡æ ·ï¼ŒåŒ…æ‹¬æ™®é€šé‡‡æ ·å’Œç†µå¼•å¯¼é‡‡æ ·ã€‚ç†µå¼•å¯¼é‡‡æ ·æ ¹æ®æ¨¡å‹æ¨ç†è¿‡ç¨‹ä¸­çš„ä¿¡æ¯ç†µæ¥è°ƒæ•´é‡‡æ ·ç­–ç•¥ã€‚åŒæ—¶ï¼Œå»ºç«‹ä¸¥æ ¼çš„æ­£è´Ÿæ ·æœ¬å¯¹é€‰æ‹©æ ‡å‡†ï¼Œç¡®ä¿è®­ç»ƒæ•°æ®çš„è´¨é‡ã€‚å¤šé˜¶æ®µå¾®è°ƒåŒ…æ‹¬ç›‘ç£å¾®è°ƒ(SFT)å’Œè‡ªè¿›åŒ–ç›´æ¥åå¥½ä¼˜åŒ–(DPO)ã€‚SFTç”¨äºåˆæ­¥æå‡æ¨¡å‹çš„èƒ½åŠ›ï¼ŒDPOåˆ™ç”¨äºä¼˜åŒ–æ¨¡å‹çš„åå¥½ï¼Œä½¿å…¶æ›´å€¾å‘äºé«˜æ•ˆå‡†ç¡®çš„å·¥å…·ä½¿ç”¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šTool-Lightçš„å…³é”®åˆ›æ–°åœ¨äºåˆ©ç”¨ä¿¡æ¯ç†µæ¥æŒ‡å¯¼å·¥å…·é›†æˆæ¨ç†ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒTool-Lightä¸æ˜¯ç®€å•åœ°å¢åŠ æˆ–å‡å°‘å·¥å…·çš„ä½¿ç”¨ï¼Œè€Œæ˜¯æ ¹æ®æ¨¡å‹æ¨ç†è¿‡ç¨‹ä¸­çš„ä¿¡æ¯ç†µåŠ¨æ€åœ°è°ƒæ•´å·¥å…·çš„ä½¿ç”¨ç­–ç•¥ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨å·¥å…·ï¼Œé¿å…è¿‡åº¦æˆ–ä¸è¶³çš„ä½¿ç”¨ï¼Œä»è€Œæé«˜æ¨ç†çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œè‡ªè¿›åŒ–é‡‡æ ·å’ŒDPOçš„ç»“åˆä¹Ÿèƒ½å¤Ÿæ›´å¥½åœ°ä¼˜åŒ–æ¨¡å‹çš„åå¥½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°æ®é›†æ„å»ºé˜¶æ®µï¼Œé‡‡ç”¨äº†è¿ç»­è‡ªè¿›åŒ–é‡‡æ ·ï¼Œå¹¶èåˆäº†æ™®é€šé‡‡æ ·å’Œç†µå¼•å¯¼é‡‡æ ·ã€‚ç†µå¼•å¯¼é‡‡æ ·çš„å…·ä½“å®ç°æ–¹å¼æœªçŸ¥ï¼Œä½†å¯ä»¥æ¨æµ‹æ˜¯æ ¹æ®æ¨¡å‹æ¨ç†è¿‡ç¨‹ä¸­æ¯ä¸€æ­¥çš„ä¿¡æ¯ç†µæ¥è°ƒæ•´é‡‡æ ·æ¦‚ç‡ã€‚åœ¨è®­ç»ƒé˜¶æ®µï¼Œé‡‡ç”¨äº†ä¸¤é˜¶æ®µçš„å¾®è°ƒç­–ç•¥ï¼ŒåŒ…æ‹¬SFTå’ŒDPOã€‚DPOçš„å…·ä½“å®ç°æ–¹å¼æœªçŸ¥ï¼Œä½†å¯ä»¥æ¨æµ‹æ˜¯æ ¹æ®æ­£è´Ÿæ ·æœ¬å¯¹æ¥ä¼˜åŒ–æ¨¡å‹çš„åå¥½ï¼Œä½¿å…¶æ›´å€¾å‘äºé«˜æ•ˆå‡†ç¡®çš„å·¥å…·ä½¿ç”¨ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒTool-Lightåœ¨10ä¸ªæ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜äº†LLMæ‰§è¡ŒTIRä»»åŠ¡çš„æ•ˆç‡ã€‚å…·ä½“æ€§èƒ½æ•°æ®æœªçŸ¥ï¼Œä½†è®ºæ–‡å¼ºè°ƒäº†Tool-Lightåœ¨æé«˜æ•ˆç‡æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚ä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒTool-Lightèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨å·¥å…·ï¼Œé¿å…è¿‡åº¦æˆ–ä¸è¶³çš„ä½¿ç”¨ï¼Œä»è€Œæé«˜æ¨ç†çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Tool-Lightæ¡†æ¶å¯åº”ç”¨äºå„ç§éœ€è¦å·¥å…·é›†æˆæ¨ç†çš„åœºæ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½å®¢æœã€ä»£ç ç”Ÿæˆã€ç§‘å­¦ç ”ç©¶ç­‰ã€‚é€šè¿‡æé«˜LLMçš„å·¥å…·ä½¿ç”¨æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼Œå¯ä»¥æ˜¾è‘—æå‡è¿™äº›åº”ç”¨çš„æ€§èƒ½å’Œç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°æ›´å¤æ‚çš„ä»»åŠ¡å’Œæ›´å¤šçš„å·¥å…·ç±»å‹ï¼Œæ¨åŠ¨LLMåœ¨å®é™…åº”ç”¨ä¸­çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Tool-Integrated Reasoning (TIR) enables large language models (LLMs) to improve their internal reasoning ability by integrating external tools. However, models employing TIR often display suboptimal behaviors, such as insufficient or excessive tool usage and overthinking after tool calls. The challenge of incentivizing LLMs to perform TIR efficiently and accurately, while stabilizing the reasoning process, remains an open question. In this paper, we start by exploring the impact of tool calls on model reasoning from the perspective of information entropy. Our findings indicate that tool call results lead to a distinct change in the information entropy of subsequent reasoning, with the overall entropy of the reasoning chain varying based on the number of tool calls. Building on these insights, we propose Tool-Light, a framework designed to encourage LLMs to perform TIR efficiently and accurately. Our framework includes dataset construction and multi-stage fine-tuning. For dataset construction, we employ continuous self-evolved sampling using the fine-tuned model, integrating both vanilla sampling and entropy-guided sampling. Besides, we establish strict criteria for selecting positive-negative pairs during sampling. The training process involves a two-stage approach, comprising Supervised Fine-Tuning (SFT) and Self-Evolved Direct Preference Optimization (DPO). Experimental results on 10 datasets demonstrate the effectiveness of Tool-Light, significantly improving the model's efficiency in executing TIR tasks.

