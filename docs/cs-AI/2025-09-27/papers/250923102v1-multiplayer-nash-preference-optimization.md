---
layout: default
title: Multiplayer Nash Preference Optimization
---

# Multiplayer Nash Preference Optimization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.23102" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.23102v1</a>
  <a href="https://arxiv.org/pdf/2509.23102.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.23102v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.23102v1', 'Multiplayer Nash Preference Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Fang Wu, Xu Huang, Weihao Xuan, Zhiwei Zhang, Yijia Xiao, Guancheng Wan, Xiaomin Li, Bing Hu, Peng Xia, Jure Leskovec, Yejin Choi

**åˆ†ç±»**: cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-09-27

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/smiles724/MNPO)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šç©å®¶çº³ä»€åå¥½ä¼˜åŒ–ï¼ˆMNPOï¼‰ï¼Œæå‡LLMåœ¨å¤æ‚åå¥½ä¸‹çš„å¯¹é½æ•ˆæœã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹å¯¹é½` `äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ` `çº³ä»€å‡è¡¡` `å¤šç©å®¶åšå¼ˆ` `åå¥½ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºå¥–åŠ±çš„RLHFæ–¹æ³•éš¾ä»¥æ•æ‰äººç±»åå¥½çš„éä¼ é€’æ€§å’Œå¼‚è´¨æ€§ï¼Œå¯¼è‡´å¯¹é½æ•ˆæœä¸ä½³ã€‚
2. MNPOå°†å¯¹é½é—®é¢˜å»ºæ¨¡ä¸ºnäººåšå¼ˆï¼Œæ¯ä¸ªç­–ç•¥ä¸å¤šä¸ªå¯¹æ‰‹ç«äº‰ï¼Œå¹¶æ­£åˆ™åŒ–åˆ°å‚è€ƒæ¨¡å‹ï¼Œä»è€Œæ•æ‰æ›´ä¸°å¯Œçš„åå¥½ç»“æ„ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒMNPOåœ¨æŒ‡ä»¤è·Ÿéšä»»åŠ¡ä¸Šä¼˜äºç°æœ‰NLHFæ–¹æ³•ï¼Œå°¤å…¶åœ¨å¼‚æ„æ ‡æ³¨å’Œæ··åˆç­–ç•¥è¯„ä¼°ä¸­è¡¨ç°æ›´ä½³ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä»äººç±»åé¦ˆä¸­è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰å·²æˆä¸ºä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»åå¥½å¯¹é½çš„æ ‡å‡†èŒƒå¼ã€‚ç„¶è€Œï¼ŒåŸºäºBradley-Terryå‡è®¾çš„åŸºäºå¥–åŠ±çš„æ–¹æ³•éš¾ä»¥æ•æ‰çœŸå®ä¸–ç•Œåå¥½çš„éä¼ é€’æ€§å’Œå¼‚è´¨æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ€è¿‘çš„ç ”ç©¶å°†å¯¹é½é‡æ–°å®šä¹‰ä¸ºä¸€ä¸ªåŒäººçº³ä»€åšå¼ˆï¼Œä»è€Œäº§ç”Ÿäº†åŸºäºäººç±»åé¦ˆçš„çº³ä»€å­¦ä¹ ï¼ˆNLHFï¼‰ã€‚è™½ç„¶è¿™ç§è§†è§’æ¿€å‘äº†è¯¸å¦‚INPOã€ONPOå’ŒEGPOç­‰å…·æœ‰å¼ºå¤§ç†è®ºå’Œç»éªŒä¿è¯çš„ç®—æ³•ï¼Œä½†å®ƒä»¬ä»ç„¶ä»æ ¹æœ¬ä¸Šå±€é™äºåŒäººäº¤äº’ï¼Œäº§ç”Ÿäº†ä¸€ç§å•ä¸€å¯¹æ‰‹åå·®ï¼Œæ— æ³•æ•æ‰ç°å®åå¥½ç»“æ„çš„å…¨éƒ¨å¤æ‚æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤šç©å®¶çº³ä»€åå¥½ä¼˜åŒ–ï¼ˆMNPOï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå°†NLHFæ¨å¹¿åˆ°å¤šç©å®¶æœºåˆ¶çš„æ–°æ¡†æ¶ã€‚å®ƒå°†å¯¹é½å½¢å¼åŒ–ä¸ºä¸€ä¸ªnäººåšå¼ˆï¼Œå…¶ä¸­æ¯ä¸ªç­–ç•¥ä¸ä¸€ä¸ªå¯¹æ‰‹ç¾¤ä½“ç«äº‰ï¼ŒåŒæ—¶è¢«æ­£åˆ™åŒ–åˆ°ä¸€ä¸ªå‚è€ƒæ¨¡å‹ã€‚æˆ‘ä»¬çš„æ¡†æ¶åœ¨å¤šç©å®¶ç¯å¢ƒä¸­å»ºç«‹äº†æ˜ç¡®å®šä¹‰çš„çº³ä»€å‡è¡¡ï¼Œå¹¶å°†å¯¹å¶é—´éš™çš„æ¦‚å¿µæ‰©å±•åˆ°é‡åŒ–è¿‘ä¼¼è´¨é‡ã€‚æˆ‘ä»¬è¯æ˜MNPOç»§æ‰¿äº†åŒäººæ–¹æ³•çš„å‡è¡¡ä¿è¯ï¼ŒåŒæ—¶å®ç°äº†æ›´ä¸°å¯Œçš„ç«äº‰åŠ¨æ€å’Œå¯¹å¤šæ ·åŒ–åå¥½ç»“æ„çš„æ›´å¥½è¦†ç›–ã€‚é€šè¿‡å…¨é¢çš„ç»éªŒè¯„ä¼°ï¼Œæˆ‘ä»¬è¡¨æ˜MNPOåœ¨æŒ‡ä»¤è·ŸéšåŸºå‡†æµ‹è¯•ä¸­å§‹ç»ˆä¼˜äºç°æœ‰çš„NLHFåŸºçº¿ï¼Œåœ¨å¼‚æ„æ ‡æ³¨è€…æ¡ä»¶å’Œæ··åˆç­–ç•¥è¯„ä¼°åœºæ™¯ä¸‹å®ç°äº†å“è¶Šçš„å¯¹é½è´¨é‡ã€‚æ€»ä¹‹ï¼Œè¿™äº›ç»“æœç¡®ç«‹äº†MNPOä½œä¸ºä¸€ä¸ªåŸåˆ™æ€§å’Œå¯æ‰©å±•çš„æ¡†æ¶ï¼Œç”¨äºä½¿LLMä¸å¤æ‚çš„ã€éä¼ é€’çš„äººç±»åå¥½å¯¹é½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åŸºäºBradley-Terryæ¨¡å‹çš„å¥–åŠ±å­¦ä¹ æ–¹æ³•ï¼Œåœ¨å¤„ç†å¤æ‚çš„ã€éä¼ é€’æ€§çš„äººç±»åå¥½æ—¶è¡¨ç°ä¸è¶³ã€‚è¿™äº›æ–¹æ³•é€šå¸¸å‡è®¾åå¥½æ˜¯ä¼ é€’çš„ï¼Œå¹¶ä¸”å¿½ç•¥äº†ä¸åŒæ ‡æ³¨è€…ä¹‹é—´çš„å¼‚è´¨æ€§ï¼Œå¯¼è‡´å­¦ä¹ åˆ°çš„å¥–åŠ±æ¨¡å‹æ— æ³•å‡†ç¡®åæ˜ çœŸå®çš„äººç±»åå¥½ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„çº³ä»€å­¦ä¹ æ–¹æ³•ï¼ˆNLHFï¼‰ä¸»è¦å±€é™äºåŒäººåšå¼ˆï¼Œæ— æ³•æ•æ‰å¤šæ–¹äº¤äº’çš„å¤æ‚æ€§ï¼Œäº§ç”Ÿå•ä¸€å¯¹æ‰‹åå·®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMNPOçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†LLMçš„å¯¹é½é—®é¢˜å»ºæ¨¡ä¸ºä¸€ä¸ªå¤šç©å®¶çº³ä»€åšå¼ˆã€‚åœ¨è¿™ä¸ªåšå¼ˆä¸­ï¼Œæ¯ä¸ªç­–ç•¥ï¼ˆä¾‹å¦‚ï¼ŒLLMçš„ä¸åŒç‰ˆæœ¬æˆ–ç­–ç•¥ï¼‰éƒ½ä¸å…¶ä»–ç­–ç•¥ç¾¤ä½“è¿›è¡Œç«äº‰ã€‚é€šè¿‡å¼•å…¥å¤šä¸ªå¯¹æ‰‹ï¼ŒMNPOèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰äººç±»åå¥½çš„å¤šæ ·æ€§å’Œéä¼ é€’æ€§ã€‚åŒæ—¶ï¼ŒMNPOé€šè¿‡å°†æ¯ä¸ªç­–ç•¥æ­£åˆ™åŒ–åˆ°ä¸€ä¸ªå‚è€ƒæ¨¡å‹ï¼Œä¿è¯äº†è®­ç»ƒçš„ç¨³å®šæ€§ï¼Œé¿å…äº†ç­–ç•¥å´©æºƒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMNPOçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦ç»„æˆéƒ¨åˆ†ï¼š1ï¼‰**ç­–ç•¥ç½‘ç»œ**ï¼šç”¨äºç”ŸæˆLLMçš„è¾“å‡ºã€‚2ï¼‰**å‚è€ƒæ¨¡å‹**ï¼šç”¨äºæä¾›ç­–ç•¥æ­£åˆ™åŒ–ï¼Œé˜²æ­¢ç­–ç•¥åç¦»è¿‡è¿œã€‚3ï¼‰**åå¥½æ•°æ®**ï¼šç”±äººç±»æ ‡æ³¨è€…æä¾›çš„åå¥½æ¯”è¾ƒæ•°æ®ã€‚4ï¼‰**çº³ä»€å‡è¡¡æ±‚è§£å™¨**ï¼šç”¨äºæ‰¾åˆ°å¤šç©å®¶åšå¼ˆçš„çº³ä»€å‡è¡¡ç­–ç•¥ã€‚MNPOçš„è®­ç»ƒæµç¨‹å¦‚ä¸‹ï¼šé¦–å…ˆï¼Œä»ç­–ç•¥ç½‘ç»œå’Œå‚è€ƒæ¨¡å‹ä¸­é‡‡æ ·ç”Ÿæˆå¤šä¸ªè¾“å‡ºã€‚ç„¶åï¼Œå°†è¿™äº›è¾“å‡ºå‘ˆç°ç»™äººç±»æ ‡æ³¨è€…ï¼Œæ”¶é›†åå¥½æ•°æ®ã€‚æ¥ä¸‹æ¥ï¼Œä½¿ç”¨åå¥½æ•°æ®è®­ç»ƒç­–ç•¥ç½‘ç»œï¼Œä½¿å…¶èƒ½å¤Ÿç”Ÿæˆæ›´ç¬¦åˆäººç±»åå¥½çš„è¾“å‡ºã€‚åŒæ—¶ï¼Œä½¿ç”¨å‚è€ƒæ¨¡å‹å¯¹ç­–ç•¥ç½‘ç»œè¿›è¡Œæ­£åˆ™åŒ–ã€‚æœ€åï¼Œä½¿ç”¨çº³ä»€å‡è¡¡æ±‚è§£å™¨æ‰¾åˆ°å¤šç©å®¶åšå¼ˆçš„çº³ä»€å‡è¡¡ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šMNPOçš„å…³é”®åˆ›æ–°åœ¨äºå°†NLHFæ‰©å±•åˆ°å¤šç©å®¶åšå¼ˆã€‚ä¸ä¼ ç»Ÿçš„åŒäººåšå¼ˆæ–¹æ³•ç›¸æ¯”ï¼ŒMNPOèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰äººç±»åå¥½çš„å¤šæ ·æ€§å’Œéä¼ é€’æ€§ã€‚æ­¤å¤–ï¼ŒMNPOè¿˜å¼•å…¥äº†å¯¹å¶é—´éš™çš„æ¦‚å¿µï¼Œç”¨äºé‡åŒ–å¤šç©å®¶åšå¼ˆçš„è¿‘ä¼¼è´¨é‡ï¼Œä¸ºç®—æ³•çš„æ”¶æ•›æ€§æä¾›äº†ç†è®ºä¿è¯ã€‚

**å…³é”®è®¾è®¡**ï¼šMNPOçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1ï¼‰**å¤šç©å®¶åšå¼ˆçš„å»ºæ¨¡**ï¼šMNPOå°†LLMçš„å¯¹é½é—®é¢˜å»ºæ¨¡ä¸ºä¸€ä¸ªnäººåšå¼ˆï¼Œå…¶ä¸­æ¯ä¸ªç­–ç•¥ä¸å…¶ä»–ç­–ç•¥ç¾¤ä½“è¿›è¡Œç«äº‰ã€‚2ï¼‰**ç­–ç•¥æ­£åˆ™åŒ–**ï¼šMNPOä½¿ç”¨å‚è€ƒæ¨¡å‹å¯¹ç­–ç•¥ç½‘ç»œè¿›è¡Œæ­£åˆ™åŒ–ï¼Œé˜²æ­¢ç­–ç•¥åç¦»è¿‡è¿œã€‚3ï¼‰**å¯¹å¶é—´éš™çš„é‡åŒ–**ï¼šMNPOå¼•å…¥äº†å¯¹å¶é—´éš™çš„æ¦‚å¿µï¼Œç”¨äºé‡åŒ–å¤šç©å®¶åšå¼ˆçš„è¿‘ä¼¼è´¨é‡ã€‚4ï¼‰**æŸå¤±å‡½æ•°**ï¼šMNPOä½¿ç”¨åŸºäºåå¥½æ•°æ®çš„æŸå¤±å‡½æ•°æ¥è®­ç»ƒç­–ç•¥ç½‘ç»œï¼Œä½¿å…¶èƒ½å¤Ÿç”Ÿæˆæ›´ç¬¦åˆäººç±»åå¥½çš„è¾“å‡ºã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å½¢å¼å¯èƒ½æ ¹æ®å…·ä½“çš„åå¥½æ¨¡å‹ï¼ˆä¾‹å¦‚ï¼ŒBradley-Terryæ¨¡å‹æˆ–Hinge Lossï¼‰è€Œæœ‰æ‰€ä¸åŒã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMNPOåœ¨æŒ‡ä»¤è·ŸéšåŸºå‡†æµ‹è¯•ä¸­å§‹ç»ˆä¼˜äºç°æœ‰çš„NLHFåŸºçº¿ã€‚åœ¨å¼‚æ„æ ‡æ³¨è€…æ¡ä»¶ä¸‹ï¼ŒMNPOèƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”ä¸åŒæ ‡æ³¨è€…çš„åå¥½å·®å¼‚ï¼Œå®ç°æ›´é«˜çš„å¯¹é½è´¨é‡ã€‚åœ¨æ··åˆç­–ç•¥è¯„ä¼°åœºæ™¯ä¸‹ï¼ŒMNPOèƒ½å¤Ÿæ›´å¥½åœ°åº”å¯¹æ¥è‡ªä¸åŒç­–ç•¥çš„æŒ‘æˆ˜ï¼Œè¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ã€‚å…·ä½“æ€§èƒ½æå‡æ•°æ®åœ¨è®ºæ–‡ä¸­ç»™å‡ºï¼Œè¯æ˜äº†MNPOåœ¨å¤æ‚åå¥½ç¯å¢ƒä¸‹çš„ä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MNPOå¯åº”ç”¨äºå„ç§éœ€è¦ä¸äººç±»åå¥½å¯¹é½çš„LLMåº”ç”¨åœºæ™¯ï¼Œä¾‹å¦‚å¯¹è¯ç³»ç»Ÿã€æ–‡æœ¬ç”Ÿæˆã€ä»£ç ç”Ÿæˆç­‰ã€‚é€šè¿‡æ›´å¥½åœ°æ•æ‰å¤æ‚çš„äººç±»åå¥½ï¼ŒMNPOå¯ä»¥æé«˜LLMçš„å®ç”¨æ€§å’Œç”¨æˆ·æ»¡æ„åº¦ã€‚æ­¤å¤–ï¼ŒMNPOè¿˜å¯ä»¥ç”¨äºä¸ªæ€§åŒ–æ¨èç³»ç»Ÿï¼Œæ ¹æ®ç”¨æˆ·çš„ä¸ªäººåå¥½æä¾›æ›´ç²¾å‡†çš„æ¨èç»“æœã€‚æœªæ¥ï¼ŒMNPOæœ‰æœ›æˆä¸ºLLMå¯¹é½çš„æ ‡å‡†æ¡†æ¶ï¼Œæ¨åŠ¨LLMåœ¨å„ä¸ªé¢†åŸŸçš„å¹¿æ³›åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement learning from human feedback (RLHF) has emerged as the standard paradigm for aligning large language models (LLMs) with human preferences. However, reward-based methods built on the Bradley-Terry assumption struggle to capture the non-transitive and heterogeneous nature of real-world preferences. To address this, recent studies have reframed alignment as a two-player Nash game, giving rise to Nash learning from human feedback (NLHF). While this perspective has inspired algorithms such as INPO, ONPO, and EGPO with strong theoretical and empirical guarantees, they remain fundamentally restricted to two-player interactions, creating a single-opponent bias that fails to capture the full complexity of realistic preference structures. In this work, we introduce Multiplayer Nash Preference Optimization (MNPO), a novel framework that generalizes NLHF to the multiplayer regime. It formulates alignment as an $n$-player game, where each policy competes against a population of opponents while being regularized toward a reference model. Our framework establishes well-defined Nash equilibria in multiplayer settings and extends the concept of duality gap to quantify approximation quality. We demonstrate that MNPO inherits the equilibrium guarantees of two-player methods while enabling richer competitive dynamics and improved coverage of diverse preference structures. Through comprehensive empirical evaluation, we show that MNPO consistently outperforms existing NLHF baselines on instruction-following benchmarks, achieving superior alignment quality under heterogeneous annotator conditions and mixed-policy evaluation scenarios. Together, these results establish MNPO as a principled and scalable framework for aligning LLMs with complex, non-transitive human preferences. Code is available at https://github.com/smiles724/MNPO.

