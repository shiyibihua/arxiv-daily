---
layout: default
title: Understanding and Enhancing the Planning Capability of Language Models via Multi-Token Prediction
---

# Understanding and Enhancing the Planning Capability of Language Models via Multi-Token Prediction

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.23186" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.23186v1</a>
  <a href="https://arxiv.org/pdf/2509.23186.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.23186v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.23186v1', 'Understanding and Enhancing the Planning Capability of Language Models via Multi-Token Prediction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Qimin Zhong, Hao Liao, Siwei Wang, Mingyang Zhou, Xiaoqun Wu, Rui Mao, Wei Chen

**åˆ†ç±»**: cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-09-27

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é€šè¿‡å¤šTokené¢„æµ‹å¢å¼ºè¯­è¨€æ¨¡å‹åœ¨å¤æ‚è§„åˆ’ä¸­çš„æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­è¨€æ¨¡å‹` `å¤æ‚è§„åˆ’` `ä¼ é€’å…³ç³»å­¦ä¹ ` `å¤šTokené¢„æµ‹` `Transformer` `è·¯å¾„è§„åˆ’` `Next-Token Injection`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚è§„åˆ’ä»»åŠ¡ä¸­ï¼Œéš¾ä»¥æœ‰æ•ˆå­¦ä¹ ä¼ é€’å…³ç³»ï¼Œé™åˆ¶äº†å…¶è§„åˆ’èƒ½åŠ›ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡å¤šTokené¢„æµ‹èŒƒå¼ï¼Œåˆ©ç”¨Transformeræ¶æ„ä¸­çš„ä¼ é€’å±‚å­¦ä¹ å¤šæ­¥é‚»æ¥ä¿¡æ¯ï¼Œä»è€Œæå‡æ¨¡å‹å¯¹ä¼ é€’å…³ç³»çš„æ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„Next-Token Injectionå’ŒTransformer-basedä¼ é€’å±‚èƒ½å¤Ÿæ˜¾è‘—å¢å¼ºæ¨¡å‹åœ¨è·¯å¾„è§„åˆ’ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å­¦ä¹ ä¼ é€’å…³ç³»æ–¹é¢ä»ç„¶å­˜åœ¨å›°éš¾ï¼Œè€Œä¼ é€’å…³ç³»æ˜¯å¤æ‚è§„åˆ’çš„åŸºçŸ³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å¤šTokené¢„æµ‹(MTP)èŒƒå¼åŠå…¶å¯¹ä¼ é€’å…³ç³»å­¦ä¹ çš„å½±å“ã€‚æˆ‘ä»¬ä½¿ç”¨ç”±å…±äº«è¾“å‡ºå¤´å’Œä¼ é€’å±‚ç»„æˆçš„Transformeræ¶æ„ï¼Œä»ç†è®ºä¸Šåˆ†æäº†MTPèŒƒå¼ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œä¼ é€’å±‚é€æ¸å­¦ä¹ å¤šæ­¥é‚»æ¥ä¿¡æ¯ï¼Œä»è€Œä½¿éª¨å¹²æ¨¡å‹èƒ½å¤Ÿæ•è·æœªè§‚å¯Ÿåˆ°çš„ä¼ é€’å¯è¾¾å…³ç³»ï¼Œå³ä½¿è¿™äº›å…³ç³»å¹¶éç›´æ¥å­˜åœ¨äºè®­ç»ƒæ•°æ®ä¸­ï¼Œä½†é‚»æ¥ä¼°è®¡ä¸­å­˜åœ¨ä¸€äº›ä¸å¯é¿å…çš„å™ªå£°ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§ç­–ç•¥æ¥å¢å¼ºä¼ é€’å±‚å’Œæ•´ä½“å­¦ä¹ è´¨é‡ï¼šNext-Token Injection (NTI)å’ŒåŸºäºTransformerçš„ä¼ é€’å±‚ã€‚æˆ‘ä»¬åœ¨åˆæˆå›¾å’ŒBlocksworldè§„åˆ’åŸºå‡†ä¸Šçš„å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„ç†è®ºå‘ç°ï¼Œå¹¶è¡¨æ˜è¿™äº›æ”¹è¿›æ˜¾è‘—å¢å¼ºäº†æ¨¡å‹çš„è·¯å¾„è§„åˆ’èƒ½åŠ›ã€‚è¿™äº›å‘ç°åŠ æ·±äº†æˆ‘ä»¬å¯¹å…·æœ‰MTPçš„Transformerå¦‚ä½•åœ¨å¤æ‚è§„åˆ’ä»»åŠ¡ä¸­å­¦ä¹ çš„ç†è§£ï¼Œå¹¶æä¾›äº†å…‹æœä¼ é€’ç“¶é¢ˆçš„å®ç”¨ç­–ç•¥ï¼Œä¸ºç»“æ„æ„ŸçŸ¥å’Œé€šç”¨è§„åˆ’æ¨¡å‹é“ºå¹³äº†é“è·¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†éœ€è¦ä¼ é€’æ¨ç†çš„å¤æ‚è§„åˆ’ä»»åŠ¡æ—¶ï¼Œä¾‹å¦‚è·¯å¾„è§„åˆ’ï¼Œè¡¨ç°ä¸ä½³ã€‚ç°æœ‰çš„è¯­è¨€æ¨¡å‹éš¾ä»¥ä»æœ‰é™çš„è®­ç»ƒæ•°æ®ä¸­æ³›åŒ–åˆ°æœªè§è¿‡çš„ä¼ é€’å…³ç³»ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºå®ƒä»¬éš¾ä»¥å­¦ä¹ å’Œåˆ©ç”¨æ•°æ®ä¸­éšå«çš„ä¼ é€’æ€§ä¿¡æ¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤šTokené¢„æµ‹ï¼ˆMTPï¼‰èŒƒå¼ï¼Œé€šè¿‡é¢„æµ‹å¤šä¸ªåç»­tokenï¼Œè¿«ä½¿æ¨¡å‹å­¦ä¹ æ•°æ®ä¸­çš„å¤šæ­¥é‚»æ¥å…³ç³»ã€‚å…·ä½“æ¥è¯´ï¼Œè®ºæ–‡è®¤ä¸ºTransformeræ¶æ„ä¸­çš„ä¼ é€’å±‚èƒ½å¤Ÿé€æ­¥å­¦ä¹ å¤šæ­¥é‚»æ¥ä¿¡æ¯ï¼Œä»è€Œä½¿æ¨¡å‹èƒ½å¤Ÿæ¨æ–­å‡ºè®­ç»ƒæ•°æ®ä¸­æœªç›´æ¥è§‚å¯Ÿåˆ°çš„ä¼ é€’å¯è¾¾å…³ç³»ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡ä½¿ç”¨ä¸€ä¸ªTransformeræ¶æ„ï¼Œè¯¥æ¶æ„åŒ…å«ä¸€ä¸ªå…±äº«è¾“å‡ºå¤´å’Œä¸€ä¸ªä¼ é€’å±‚ã€‚ä¼ é€’å±‚è´Ÿè´£å­¦ä¹ å¤šæ­¥é‚»æ¥ä¿¡æ¯ï¼Œè€Œå…±äº«è¾“å‡ºå¤´åˆ™è´Ÿè´£é¢„æµ‹ä¸‹ä¸€ä¸ªtokenã€‚è®ºæ–‡æå‡ºäº†ä¸¤ç§å¢å¼ºä¼ é€’å±‚å’Œæ•´ä½“å­¦ä¹ è´¨é‡çš„ç­–ç•¥ï¼šNext-Token Injection (NTI) å’Œ Transformer-based ä¼ é€’å±‚ã€‚NTIé€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ³¨å…¥ä¸‹ä¸€ä¸ªtokençš„ä¿¡æ¯æ¥å¸®åŠ©ä¼ é€’å±‚æ›´å¥½åœ°å­¦ä¹ é‚»æ¥å…³ç³»ã€‚Transformer-basedä¼ é€’å±‚åˆ™ä½¿ç”¨ä¸€ä¸ªTransformerç»“æ„æ¥å»ºæ¨¡ä¼ é€’å±‚ï¼Œä»è€Œæé«˜å…¶å­¦ä¹ èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºç†è®ºåˆ†æäº†MTPèŒƒå¼åœ¨ä¼ é€’å…³ç³»å­¦ä¹ ä¸­çš„ä½œç”¨ï¼Œå¹¶æå‡ºäº†ä¸¤ç§å¢å¼ºä¼ é€’å±‚å­¦ä¹ èƒ½åŠ›çš„ç­–ç•¥ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè®ºæ–‡çš„æ–¹æ³•æ›´åŠ å…³æ³¨å¦‚ä½•åˆ©ç”¨Transformeræ¶æ„çš„ç‰¹æ€§æ¥å­¦ä¹ å’Œåˆ©ç”¨æ•°æ®ä¸­çš„ä¼ é€’æ€§ä¿¡æ¯ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨Transformeræ¶æ„ä½œä¸ºåŸºç¡€æ¨¡å‹ï¼›2) è®¾è®¡ä¼ é€’å±‚æ¥å­¦ä¹ å¤šæ­¥é‚»æ¥ä¿¡æ¯ï¼›3) æå‡ºNext-Token Injection (NTI)ç­–ç•¥ï¼Œé€šè¿‡æ³¨å…¥ä¸‹ä¸€ä¸ªtokençš„ä¿¡æ¯æ¥å¸®åŠ©ä¼ é€’å±‚å­¦ä¹ ï¼›4) ä½¿ç”¨Transformer-basedä¼ é€’å±‚æ¥æé«˜ä¼ é€’å±‚çš„å­¦ä¹ èƒ½åŠ›ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ç­‰ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡åœ¨åˆæˆå›¾å’ŒBlocksworldè§„åˆ’åŸºå‡†ä¸Šè¿›è¡Œäº†å®éªŒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„Next-Token Injection (NTI)å’ŒTransformer-basedä¼ é€’å±‚èƒ½å¤Ÿæ˜¾è‘—å¢å¼ºæ¨¡å‹çš„è·¯å¾„è§„åˆ’èƒ½åŠ›ã€‚ä¾‹å¦‚ï¼Œåœ¨Blocksworldæ•°æ®é›†ä¸Šï¼Œæ¨¡å‹çš„è§„åˆ’æˆåŠŸç‡å¾—åˆ°äº†æ˜¾è‘—æå‡ï¼ŒéªŒè¯äº†ç†è®ºåˆ†æçš„æ­£ç¡®æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦å¤æ‚è§„åˆ’å’Œæ¨ç†çš„é¢†åŸŸï¼Œä¾‹å¦‚æœºå™¨äººå¯¼èˆªã€ä»»åŠ¡è°ƒåº¦ã€ä¾›åº”é“¾ç®¡ç†å’Œæ¸¸æˆAIã€‚é€šè¿‡æå‡è¯­è¨€æ¨¡å‹åœ¨ä¼ é€’å…³ç³»å­¦ä¹ æ–¹é¢çš„èƒ½åŠ›ï¼Œå¯ä»¥æ„å»ºæ›´æ™ºèƒ½ã€æ›´é€šç”¨çš„è§„åˆ’æ¨¡å‹ï¼Œä»è€Œåœ¨å®é™…åº”ç”¨ä¸­å®ç°æ›´é«˜æ•ˆã€æ›´å¯é çš„å†³ç­–ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have achieved impressive performance across diverse tasks but continue to struggle with learning transitive relations, a cornerstone for complex planning. To address this issue, we investigate the Multi-Token Prediction (MTP) paradigm and its impact to transitive relation learning. We theoretically analyze the MTP paradigm using a Transformer architecture composed of a shared output head and a transfer layer. Our analysis reveals that the transfer layer gradually learns the multi-step adjacency information, which in turn enables the backbone model to capture unobserved transitive reachability relations beyond those directly present in the training data, albeit with some inevitable noise in adjacency estimation. Building on this foundation, we propose two strategies to enhance the transfer layer and overall learning quality: Next-Token Injection (NTI) and a Transformer-based transfer layer. Our experiments on both synthetic graphs and the Blocksworld planning benchmark validate our theoretical findings and demonstrate that the improvements significantly enhance the model's path-planning capability. These findings deepen our understanding of how Transformers with MTP learn in complex planning tasks, and provide practical strategies to overcome the transitivity bottleneck, paving the way toward structurally aware and general-purpose planning models.

