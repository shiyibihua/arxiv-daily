---
layout: default
title: ZKPROV: A Zero-Knowledge Approach to Dataset Provenance for Large Language Models
---

# ZKPROV: A Zero-Knowledge Approach to Dataset Provenance for Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.20915" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.20915v2</a>
  <a href="https://arxiv.org/pdf/2506.20915.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.20915v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.20915v2', 'ZKPROV: A Zero-Knowledge Approach to Dataset Provenance for Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mina Namazi, Alexander Nemecek, Erman Ayday

**åˆ†ç±»**: cs.CR, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-26 (æ›´æ–°: 2025-12-18)

**å¤‡æ³¨**: 16 pages, 3 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºZKPROVä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹æ•°æ®æ¥æºéªŒè¯é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `æ•°æ®æ¥æºéªŒè¯` `é›¶çŸ¥è¯†è¯æ˜` `æ•°æ®éšç§` `å¯†ç å­¦æ¡†æ¶` `æ•æ„Ÿé¢†åŸŸåº”ç”¨` `åˆè§„æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨éªŒè¯å¤§è¯­è¨€æ¨¡å‹çš„è®¡ç®—æ¥æºæ—¶é¢ä¸´é«˜æ˜‚çš„è®¡ç®—æˆæœ¬æˆ–ä¿¡æ¯æ³„éœ²çš„é£é™©ã€‚
2. ZKPROVé€šè¿‡é›¶çŸ¥è¯†è¯æ˜æŠ€æœ¯ï¼Œå…è®¸ç”¨æˆ·éªŒè¯æ¨¡å‹å“åº”çš„è®­ç»ƒæ•°æ®é›†æ¥æºï¼ŒåŒæ—¶ä¿æŠ¤æ•°æ®éšç§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒZKPROVåœ¨ç”Ÿæˆå’ŒéªŒè¯è¯æ˜æ—¶å…·æœ‰äºšçº¿æ€§æ‰©å±•æ€§ï¼Œå¤„ç†æ—¶é—´ä½äº3.3ç§’ï¼Œé€‚ç”¨äºå®é™…åº”ç”¨ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•æ„Ÿé¢†åŸŸçš„åº”ç”¨ï¼Œå‡†ç¡®éªŒè¯å…¶è®¡ç®—æ¥æºè€Œä¸æ³„éœ²è®­ç»ƒæ•°æ®é›†æˆä¸ºä¸€é¡¹é‡å¤§æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨åŒ»ç–—ç­‰å—ç›‘ç®¡è¡Œä¸šã€‚ä¼ ç»Ÿæ–¹æ³•è¦ä¹ˆéœ€è¦é«˜æ˜‚çš„è®¡ç®—æˆæœ¬æ¥å®Œå…¨éªŒè¯æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ï¼Œè¦ä¹ˆä¼šå‘éªŒè¯è€…æ³„éœ²æœªæˆæƒçš„ä¿¡æ¯ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ZKPROVï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„å¯†ç å­¦æ¡†æ¶ï¼Œå…è®¸ç”¨æˆ·éªŒè¯LLMå¯¹å…¶æç¤ºçš„å“åº”æ˜¯åŸºäºç”±æ•°æ®é›†æ‰€æœ‰è€…è®¤è¯çš„æ•°æ®é›†è¿›è¡Œè®­ç»ƒçš„ã€‚æ­¤å¤–ï¼Œå®ƒç¡®ä¿æ•°æ®é›†å†…å®¹ä¸ç”¨æˆ·æŸ¥è¯¢ç›¸å…³ï¼ŒåŒæ—¶ä¸æ³„éœ²æ•æ„Ÿä¿¡æ¯ã€‚ZKPROVåœ¨éšç§ä¸æ•ˆç‡ä¹‹é—´æä¾›äº†ç‹¬ç‰¹çš„å¹³è¡¡ï¼Œé€šè¿‡å°†è®­ç»ƒæ•°æ®é›†ã€æ¨¡å‹å‚æ•°å’Œå“åº”ç»‘å®šåœ¨ä¸€èµ·ï¼Œå¹¶é™„åŠ é›¶çŸ¥è¯†è¯æ˜æ¥éªŒè¯è¿™äº›å£°æ˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¯¹äºæœ€å¤§å‚æ•°é‡ä¸º8Bçš„æ¨¡å‹ï¼Œç”Ÿæˆå’ŒéªŒè¯è¿™äº›è¯æ˜çš„ç«¯åˆ°ç«¯å¼€é”€ä½äº3.3ç§’ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å¯è¡Œæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨æ•æ„Ÿé¢†åŸŸä¸­éªŒè¯å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è®¡ç®—æ¥æºçš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€éœ€è¦é«˜æ˜‚çš„è®¡ç®—æˆæœ¬æˆ–ä¼šæ³„éœ²æ•æ„Ÿä¿¡æ¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šZKPROVé€šè¿‡å¼•å…¥é›¶çŸ¥è¯†è¯æ˜æŠ€æœ¯ï¼Œå…è®¸ç”¨æˆ·åœ¨ä¸æ³„éœ²è®­ç»ƒæ•°æ®é›†å†…å®¹çš„æƒ…å†µä¸‹ï¼ŒéªŒè¯æ¨¡å‹å“åº”çš„æ¥æºå’Œç›¸å…³æ€§ã€‚è¿™æ ·çš„è®¾è®¡ç¡®ä¿äº†æ•°æ®éšç§ï¼ŒåŒæ—¶æ»¡è¶³äº†åˆè§„æ€§è¦æ±‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šZKPROVçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†è®¤è¯ã€æ¨¡å‹å‚æ•°ç»‘å®šå’Œå“åº”ç”Ÿæˆä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚ç”¨æˆ·æäº¤æŸ¥è¯¢åï¼Œç³»ç»Ÿç”Ÿæˆç›¸åº”çš„é›¶çŸ¥è¯†è¯æ˜ï¼Œç¡®ä¿æ•°æ®é›†çš„åˆæ³•æ€§å’Œç›¸å…³æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šZKPROVçš„ä¸»è¦åˆ›æ–°åœ¨äºå°†è®­ç»ƒæ•°æ®é›†ã€æ¨¡å‹å‚æ•°å’Œç”Ÿæˆå“åº”ç»‘å®šåœ¨ä¸€èµ·ï¼Œå¹¶é€šè¿‡é›¶çŸ¥è¯†è¯æ˜éªŒè¯è¿™äº›ç»‘å®šçš„æœ‰æ•ˆæ€§ã€‚è¿™ä¸ä¼ ç»Ÿæ–¹æ³•çš„æ˜¾è‘—åŒºåˆ«åœ¨äºï¼Œå®ƒä¸éœ€è¦æ³„éœ²ä»»ä½•æ•æ„Ÿä¿¡æ¯ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®ç°ä¸­ï¼ŒZKPROVé‡‡ç”¨äº†é«˜æ•ˆçš„é›¶çŸ¥è¯†è¯æ˜ç®—æ³•ï¼Œç¡®ä¿ç”Ÿæˆå’ŒéªŒè¯è¿‡ç¨‹çš„æ—¶é—´å¤æ‚åº¦ä¿æŒåœ¨äºšçº¿æ€§æ°´å¹³ã€‚æ­¤å¤–ï¼Œç³»ç»Ÿè®¾è®¡äº†åˆç†çš„å‚æ•°è®¾ç½®ï¼Œä»¥ä¼˜åŒ–æ€§èƒ½å’Œå®‰å…¨æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒZKPROVåœ¨ç”Ÿæˆå’ŒéªŒè¯é›¶çŸ¥è¯†è¯æ˜æ—¶å…·æœ‰äºšçº¿æ€§æ‰©å±•æ€§ï¼Œå¤„ç†æ—¶é—´ä½äº3.3ç§’ï¼Œé€‚ç”¨äºæœ€å¤§å‚æ•°é‡ä¸º8Bçš„æ¨¡å‹ã€‚è¿™ä¸€æ€§èƒ½æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å¯è¡Œæ€§å’Œæ•ˆç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

ZKPROVçš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒ»ç–—ã€é‡‘èå’Œæ³•å¾‹ç­‰å—ç›‘ç®¡è¡Œä¸šï¼Œè¿™äº›é¢†åŸŸå¯¹æ•°æ®æ¥æºçš„éªŒè¯æœ‰ä¸¥æ ¼è¦æ±‚ã€‚é€šè¿‡ç¡®ä¿æ•°æ®é›†çš„åˆæ³•æ€§å’Œç›¸å…³æ€§ï¼ŒZKPROVå¯ä»¥å¸®åŠ©ä¼ä¸šåœ¨åˆè§„çš„åŒæ—¶ï¼Œåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œåˆ›æ–°å’Œå†³ç­–ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯å¯èƒ½æ¨åŠ¨æ›´å¤šè¡Œä¸šåœ¨æ•°æ®éšç§å’Œå®‰å…¨æ€§æ–¹é¢çš„è¿›æ­¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As large language models (LLMs) are used in sensitive fields, accurately verifying their computational provenance without disclosing their training datasets poses a significant challenge, particularly in regulated sectors such as healthcare, which have strict requirements for dataset use. Traditional approaches either incur substantial computational cost to fully verify the entire training process or leak unauthorized information to the verifier. Therefore, we introduce ZKPROV, a novel cryptographic framework allowing users to verify that the LLM's responses to their prompts are trained on datasets certified by the authorities that own them. Additionally, it ensures that the dataset's content is relevant to the users' queries without revealing sensitive information about the datasets or the model parameters. ZKPROV offers a unique balance between privacy and efficiency by binding training datasets, model parameters, and responses, while also attaching zero-knowledge proofs to the responses generated by the LLM to validate these claims. Our experimental results demonstrate sublinear scaling for generating and verifying these proofs, with end-to-end overhead under 3.3 seconds for models up to 8B parameters, presenting a practical solution for real-world applications. We also provide formal security guarantees, proving that our approach preserves dataset confidentiality while ensuring trustworthy dataset provenance.

