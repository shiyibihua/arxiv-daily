---
layout: default
title: TableMoE: Neuro-Symbolic Routing for Structured Expert Reasoning in Multimodal Table Understanding
---

# TableMoE: Neuro-Symbolic Routing for Structured Expert Reasoning in Multimodal Table Understanding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21393" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.21393v1</a>
  <a href="https://arxiv.org/pdf/2506.21393.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21393v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21393v1', 'TableMoE: Neuro-Symbolic Routing for Structured Expert Reasoning in Multimodal Table Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Junwen Zhang, Pu Chen, Yin Zhang

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-06-26

**å¤‡æ³¨**: 43 pages and 11 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTableMoEä»¥è§£å†³å¤šæ¨¡æ€è¡¨æ ¼ç†è§£ä¸­çš„å¤æ‚æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€ç†è§£` `ç¥ç»ç¬¦å·æ¨ç†` `è¡¨æ ¼æ•°æ®` `ç»“æ„åŒ–æ¨ç†` `ä¸“å®¶æ¨¡å‹` `æ•°æ®é›†æ„å»º` `æ€§èƒ½è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤æ‚è¡¨æ ¼æ—¶è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨ç»“æ„å¤æ‚å’Œè§†è§‰é€€åŒ–çš„æƒ…å†µä¸‹ã€‚
2. æœ¬æ–‡æå‡ºTableMoEï¼Œé€šè¿‡ç¥ç»ç¬¦å·è·¯ç”±æœºåˆ¶åŠ¨æ€è·¯ç”±è¡¨æ ¼å…ƒç´ åˆ°ä¸“é—¨çš„ä¸“å®¶ï¼Œä»¥å®ç°æ›´ç¨³å¥çš„æ¨ç†ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTableMoEåœ¨å¤šä¸ªWildStructåŸºå‡†ä¸Šæ˜¾è‘—è¶…è¶Šç°æœ‰æ¨¡å‹ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨ç°å®åœºæ™¯ä¸­ï¼Œå¤šæ¨¡æ€è¡¨æ ¼ç†è§£é¢ä¸´ç»“æ„å¤æ‚æ€§ã€ç¬¦å·å¯†åº¦å’Œè§†è§‰é€€åŒ–ç­‰æŒ‘æˆ˜ã€‚ç°æœ‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¿™äº›WildStructæ¡ä»¶ä¸‹è¡¨ç°ä¸ä½³ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†TableMoEï¼Œä¸€ç§ä¸“ä¸ºå¤šæ¨¡æ€è¡¨æ ¼æ•°æ®è®¾è®¡çš„ç¥ç»ç¬¦å·æ··åˆè¿æ¥ä¸“å®¶æ¶æ„ã€‚TableMoEå¼•å…¥äº†åˆ›æ–°çš„ç¥ç»ç¬¦å·è·¯ç”±æœºåˆ¶ï¼Œé€šè¿‡é¢„æµ‹æ½œåœ¨è¯­ä¹‰ä»¤ç‰Œè§’è‰²å¹¶åŠ¨æ€è·¯ç”±è¡¨æ ¼å…ƒç´ åˆ°ä¸“é—¨çš„ä¸“å®¶ï¼Œä»è€Œå®ç°ç¨³å¥çš„ç»“æ„åŒ–æ¨ç†ã€‚æˆ‘ä»¬è¿˜æ„å»ºäº†å¤§è§„æ¨¡çš„TableMoE-Alignæ•°æ®é›†ï¼Œå¹¶å‘å¸ƒäº†å››ä¸ªWildStructåŸºå‡†è¿›è¡Œè¯„ä¼°ï¼Œå®éªŒç»“æœè¡¨æ˜TableMoEæ˜¾è‘—è¶…è¶Šç°æœ‰æœ€å…ˆè¿›æ¨¡å‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€è¡¨æ ¼ç†è§£ä¸­çš„ç»“æ„å¤æ‚æ€§å’Œè§†è§‰é€€åŒ–é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨è¿™äº›WildStructæ¡ä»¶ä¸‹è¡¨ç°ä¸ä½³ï¼Œå¯¼è‡´æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šTableMoEçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ç¥ç»ç¬¦å·è·¯ç”±æœºåˆ¶ï¼Œé¢„æµ‹è¡¨æ ¼å…ƒç´ çš„æ½œåœ¨è¯­ä¹‰è§’è‰²ï¼Œå¹¶å°†å…¶åŠ¨æ€è·¯ç”±åˆ°ä¸“é—¨çš„ä¸“å®¶ï¼Œä»¥å®ç°æ›´é«˜æ•ˆçš„ç»“æ„åŒ–æ¨ç†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTableMoEé‡‡ç”¨æ··åˆè¿æ¥ä¸“å®¶æ¶æ„ï¼Œä¸»è¦æ¨¡å—åŒ…æ‹¬ç¥ç»ç¬¦å·è·¯ç”±æœºåˆ¶ã€ä¸“å®¶æ¨¡å‹ï¼ˆå¦‚Table-to-HTMLã€Table-to-JSONã€Table-to-Codeï¼‰ä»¥åŠåŸºäºä¿¡å¿ƒçš„é—¨æ§ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°æ˜¯ç¥ç»ç¬¦å·è·¯ç”±æœºåˆ¶ï¼Œå®ƒé€šè¿‡ç¬¦å·æ¨ç†å›¾æ¥æŒ‡å¯¼è·¯ç”±å†³ç­–ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨å¤æ‚è¡¨æ ¼ç†è§£ä¸­çš„è¡¨ç°ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒTableMoEä½¿ç”¨äº†ä¿¡å¿ƒæ„ŸçŸ¥çš„é—¨æ§ç­–ç•¥ï¼Œç¡®ä¿è¡¨æ ¼å…ƒç´ èƒ½å¤Ÿè¢«æœ‰æ•ˆåœ°åˆ†é…ç»™åˆé€‚çš„ä¸“å®¶ï¼ŒåŒæ—¶å¼•å…¥äº†å¤§è§„æ¨¡çš„TableMoE-Alignæ•°æ®é›†è¿›è¡Œé¢„è®­ç»ƒã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒTableMoEåœ¨å››ä¸ªWildStructåŸºå‡†ä¸Šå‡æ˜¾è‘—è¶…è¶Šç°æœ‰æœ€å…ˆè¿›æ¨¡å‹ï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°XX%ï¼ˆå…·ä½“æ•°æ®æœªçŸ¥ï¼‰ï¼ŒéªŒè¯äº†å…¶åœ¨å¤æ‚è¡¨æ ¼ç†è§£ä¸­çš„æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

TableMoEçš„æ½œåœ¨åº”ç”¨åœºæ™¯åŒ…æ‹¬é‡‘èã€ç§‘å­¦ã€ç”Ÿç‰©åŒ»å­¦å’Œå·¥ä¸šç­‰é¢†åŸŸï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å¤æ‚çš„è¡¨æ ¼æ•°æ®ï¼Œæå‡æ•°æ®åˆ†æå’Œå†³ç­–æ”¯æŒçš„èƒ½åŠ›ã€‚æœªæ¥ï¼Œè¯¥æ¨¡å‹æœ‰æœ›åœ¨å¤šæ¨¡æ€æ•°æ®ç†è§£å’Œæ™ºèƒ½é—®ç­”ç³»ç»Ÿä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal understanding of tables in real-world contexts is challenging due to the complexity of structure, symbolic density, and visual degradation (blur, skew, watermarking, incomplete structures or fonts, multi-span or hierarchically nested layouts). Existing multimodal large language models (MLLMs) struggle with such WildStruct conditions, resulting in limited performance and poor generalization. To address these challenges, we propose TableMoE, a neuro-symbolic Mixture-of-Connector-Experts (MoCE) architecture specifically designed for robust, structured reasoning over multimodal table data. TableMoE features an innovative Neuro-Symbolic Routing mechanism, which predicts latent semantic token roles (e.g., header, data cell, axis, formula) and dynamically routes table elements to specialized experts (Table-to-HTML, Table-to-JSON, Table-to-Code) using a confidence-aware gating strategy informed by symbolic reasoning graphs. To facilitate effective alignment-driven pretraining, we introduce the large-scale TableMoE-Align dataset, consisting of 1.2M table-HTML-JSON-code quadruples across finance, science, biomedicine and industry, utilized exclusively for model pretraining. For evaluation, we curate and release four challenging WildStruct benchmarks: WMMFinQA, WMMTatQA, WMMTabDialog, and WMMFinanceMath, designed specifically to stress-test models under real-world multimodal degradation and structural complexity. Experimental results demonstrate that TableMoE significantly surpasses existing state-of-the-art models. Extensive ablation studies validate each core component, emphasizing the critical role of Neuro-Symbolic Routing and structured expert alignment. Through qualitative analyses, we further showcase TableMoE's interpretability and enhanced robustness, underscoring the effectiveness of integrating neuro-symbolic reasoning for multimodal table understanding.

