---
layout: default
title: World-aware Planning Narratives Enhance Large Vision-Language Model Planner
---

# World-aware Planning Narratives Enhance Large Vision-Language Model Planner

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.21230" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.21230v2</a>
  <a href="https://arxiv.org/pdf/2506.21230.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.21230v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.21230v2', 'World-aware Planning Narratives Enhance Large Vision-Language Model Planner')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Junhao Shi, Zhaoye Fei, Siyin Wang, Qipeng Guo, Jingjing Gong, Xipeng Qiu

**åˆ†ç±»**: cs.AI, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-06-26 (æ›´æ–°: 2025-07-02)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸–ç•Œæ„ŸçŸ¥è§„åˆ’å™äº‹å¢å¼ºæ¡†æ¶ä»¥è§£å†³å¤æ‚ç¯å¢ƒä¸­çš„è§„åˆ’é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `è§„åˆ’ä»»åŠ¡` `ç¯å¢ƒç†è§£` `è®¤çŸ¥èƒ½åŠ›` `é•¿æ—¶é—´è§„åˆ’` `å¸¸è¯†æ¨ç†` `è¯¾ç¨‹å­¦ä¹ ` `å¤šæ¨¡æ€èåˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤æ‚ç¯å¢ƒå’Œå¤šæ­¥éª¤ç›®æ ‡çš„è§„åˆ’ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ï¼Œç¼ºä¹ç¯å¢ƒä¸Šä¸‹æ–‡çš„ç†è§£ã€‚
2. æœ¬æ–‡æå‡ºWAPæ¡†æ¶ï¼Œé€šè¿‡å››ç§è®¤çŸ¥èƒ½åŠ›å¢å¼ºLVLMsçš„ç¯å¢ƒç†è§£ï¼Œæå‡æ¨¡å‹çš„è§„åˆ’èƒ½åŠ›ã€‚
3. åœ¨EB-ALFREDåŸºå‡†æµ‹è¯•ä¸­ï¼ŒQwen2.5-VLåœ¨ä»»åŠ¡æˆåŠŸç‡ä¸Šæå‡60.7ï¼Œå°¤å…¶åœ¨å¸¸è¯†æ¨ç†å’Œé•¿æ—¶é—´è§„åˆ’æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨å…·èº«è§„åˆ’ä»»åŠ¡ä¸­å±•ç°å‡ºæ½œåŠ›ï¼Œä½†åœ¨å¤„ç†å¤æ‚åœºæ™¯å’Œå¤šæ­¥éª¤ç›®æ ‡æ—¶å­˜åœ¨å›°éš¾ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºç¯å¢ƒæ— å…³çš„æ¨¡ä»¿å­¦ä¹ ï¼Œå¯¼è‡´æ¨¡å‹åœ¨ä¸Šä¸‹æ–‡æ•æ„ŸæŒ‡ä»¤ä¸Šè¡¨ç°ä¸ä½³ã€‚æœ¬æ–‡æå‡ºä¸–ç•Œæ„ŸçŸ¥è§„åˆ’å™äº‹å¢å¼ºï¼ˆWAPï¼‰æ¡†æ¶ï¼Œé€šè¿‡è§†è§‰å¤–è§‚å»ºæ¨¡ã€ç©ºé—´æ¨ç†ã€åŠŸèƒ½æŠ½è±¡å’Œå¥æ³•åŸºç¡€ç­‰å››ç§è®¤çŸ¥èƒ½åŠ›ï¼Œæå‡LVLMsçš„ç¯å¢ƒç†è§£èƒ½åŠ›ã€‚é€šè¿‡è¯¾ç¨‹å­¦ä¹ ï¼Œä»…ä½¿ç”¨åŸå§‹è§†è§‰è§‚å¯Ÿè¿›è¡Œæ¨¡å‹å¼€å‘å’Œè¯„ä¼°ã€‚EB-ALFREDåŸºå‡†æµ‹è¯•ç»“æœæ˜¾ç¤ºï¼ŒQwen2.5-VLåœ¨ä»»åŠ¡æˆåŠŸç‡ä¸Šå®ç°äº†60.7çš„ç»å¯¹æå‡ï¼Œå°¤å…¶åœ¨å¸¸è¯†æ¨ç†å’Œé•¿æ—¶é—´è§„åˆ’æ–¹é¢è¡¨ç°çªå‡ºã€‚æˆ‘ä»¬çš„å¼€æºæ¨¡å‹æ˜¾è‘—ä¼˜äºGPT-4oå’ŒClaude-3.5-Sonnetç­‰ä¸“æœ‰ç³»ç»Ÿã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤æ‚ç¯å¢ƒå’Œå¤šæ­¥éª¤ç›®æ ‡è§„åˆ’ä¸­çš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€å¿½è§†ç¯å¢ƒä¸Šä¸‹æ–‡ï¼Œå¯¼è‡´æ¨¡å‹åœ¨å¤„ç†ä¸Šä¸‹æ–‡æ•æ„ŸæŒ‡ä»¤æ—¶è¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºä¸–ç•Œæ„ŸçŸ¥è§„åˆ’å™äº‹å¢å¼ºï¼ˆWAPï¼‰æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥è§†è§‰å¤–è§‚å»ºæ¨¡ã€ç©ºé—´æ¨ç†ã€åŠŸèƒ½æŠ½è±¡å’Œå¥æ³•åŸºç¡€ç­‰è®¤çŸ¥èƒ½åŠ›ï¼Œæå‡æ¨¡å‹å¯¹ç¯å¢ƒçš„ç†è§£ï¼Œä»è€Œæ”¹å–„è§„åˆ’æ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šWAPæ¡†æ¶åŒ…æ‹¬å››ä¸ªä¸»è¦æ¨¡å—ï¼šè§†è§‰å¤–è§‚å»ºæ¨¡ç”¨äºç†è§£ç¯å¢ƒçš„è§†è§‰ç‰¹å¾ï¼Œç©ºé—´æ¨ç†ç”¨äºå¤„ç†ç©ºé—´å…³ç³»ï¼ŒåŠŸèƒ½æŠ½è±¡ç”¨äºç†è§£ç‰©ä½“çš„åŠŸèƒ½ï¼Œå¥æ³•åŸºç¡€ç”¨äºè§£ææŒ‡ä»¤çš„è¯­æ³•ç»“æ„ã€‚æ¨¡å‹é€šè¿‡è¯¾ç¨‹å­¦ä¹ é€æ­¥æå‡èƒ½åŠ›ï¼Œä»…ä¾èµ–åŸå§‹è§†è§‰è§‚å¯Ÿè¿›è¡Œè®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºå°†å››ç§è®¤çŸ¥èƒ½åŠ›ç³»ç»Ÿæ€§åœ°æ•´åˆåˆ°LVLMsä¸­ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­è¿›è¡Œæ›´æœ‰æ•ˆçš„è§„åˆ’ï¼Œä¸ç°æœ‰çš„ç¯å¢ƒæ— å…³æ¨¡ä»¿å­¦ä¹ æ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥å¹³è¡¡å„æ¨¡å—çš„å­¦ä¹ ç›®æ ‡ï¼Œå¹¶é€šè¿‡è°ƒæ•´ç½‘ç»œç»“æ„æ¥ä¼˜åŒ–æ¨¡å‹çš„æ€§èƒ½ï¼Œç¡®ä¿åœ¨é•¿æ—¶é—´è§„åˆ’å’Œå¸¸è¯†æ¨ç†ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒQwen2.5-VLåœ¨EB-ALFREDåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†60.7çš„ä»»åŠ¡æˆåŠŸç‡æå‡ï¼Œå¸¸è¯†æ¨ç†å’Œé•¿æ—¶é—´è§„åˆ’çš„æå‡å¹…åº¦åˆ†åˆ«è¾¾åˆ°60.0å’Œ70.0ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„ä¸“æœ‰ç³»ç»Ÿå¦‚GPT-4oå’ŒClaude-3.5-Sonnetã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººå¯¼èˆªã€æ™ºèƒ½åŠ©æ‰‹å’Œè‡ªåŠ¨åŒ–å†³ç­–ç³»ç»Ÿç­‰ã€‚é€šè¿‡å¢å¼ºæ¨¡å‹çš„ç¯å¢ƒç†è§£èƒ½åŠ›ï¼Œå¯ä»¥åœ¨å¤æ‚å’ŒåŠ¨æ€çš„ç¯å¢ƒä¸­å®ç°æ›´é«˜æ•ˆçš„è§„åˆ’å’Œå†³ç­–ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Vision-Language Models (LVLMs) show promise for embodied planning tasks but struggle with complex scenarios involving unfamiliar environments and multi-step goals. Current approaches rely on environment-agnostic imitation learning that disconnects instructions from environmental contexts, causing models to struggle with context-sensitive instructions and rely on supplementary cues rather than visual reasoning during long-horizon interactions. In this work, we propose World-Aware Planning Narrative Enhancement (WAP), a framework that infuses LVLMs with comprehensive environmental understanding through four cognitive capabilities (visual appearance modeling, spatial reasoning, functional abstraction, and syntactic grounding) while developing and evaluating models using only raw visual observations through curriculum learning. Evaluations on the EB-ALFRED benchmark demonstrate substantial improvements, with Qwen2.5-VL achieving a 60.7 absolute improvement in task success rates, particularly in commonsense reasoning (+60.0) and long-horizon planning (+70.0). Notably, our enhanced open-source models outperform proprietary systems like GPT-4o and Claude-3.5-Sonnet by a large margin.

