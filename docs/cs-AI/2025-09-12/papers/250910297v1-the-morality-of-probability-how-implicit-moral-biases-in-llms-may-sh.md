---
layout: default
title: The Morality of Probability: How Implicit Moral Biases in LLMs May Shape the Future of Human-AI Symbiosis
---

# The Morality of Probability: How Implicit Moral Biases in LLMs May Shape the Future of Human-AI Symbiosis

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.10297" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.10297v1</a>
  <a href="https://arxiv.org/pdf/2509.10297.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.10297v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.10297v1', 'The Morality of Probability: How Implicit Moral Biases in LLMs May Shape the Future of Human-AI Symbiosis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Eoin O'Doherty, Nicole Weinrauch, Andrew Talone, Uri Klempner, Xiaoyuan Yi, Xing Xie, Yi Zeng

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-09-12

**å¤‡æ³¨**: Work in progress

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ­ç¤ºLLMä¸­éšå«çš„é“å¾·åè§ï¼Œæ¢ç´¢äººæœºå…±ç”Ÿçš„æœªæ¥**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `é“å¾·åè§` `äººæœºå…±ç”Ÿ` `é“å¾·æ¨ç†` `å¯è§£é‡Šæ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰AIç³»ç»Ÿåœ¨é“å¾·å†³ç­–æ–¹é¢ç¼ºä¹é€æ˜åº¦ï¼Œéš¾ä»¥ä¸äººç±»é“å¾·ä»·å€¼è§‚å¯¹é½ï¼Œé˜»ç¢äº†äººæœºå…±ç”Ÿã€‚
2. é€šè¿‡å®šé‡å®éªŒè¯„ä¼°LLMåœ¨é“å¾·å›°å¢ƒä¸­çš„é€‰æ‹©ï¼Œæ­ç¤ºå…¶éšå«çš„é“å¾·åè§ï¼Œå¹¶åˆ†æä¸åŒæ¨¡å‹æ¶æ„å’Œæ–‡åŒ–èƒŒæ™¯çš„å½±å“ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒLLMæ™®éåå‘å…³æ€€å’Œç¾å¾·ä»·å€¼è§‚ï¼Œè€Œå¿½è§†è‡ªç”±ä¸»ä¹‰é€‰æ‹©ï¼Œæ¨ç†èƒ½åŠ›å¼ºçš„æ¨¡å‹å¯¹ä¸Šä¸‹æ–‡æ›´æ•æ„Ÿã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æ¢è®¨äº†äººå·¥æ™ºèƒ½ç³»ç»Ÿåœ¨é“å¾·å†³ç­–ä¸­å¦‚ä½•ä¼˜å…ˆè€ƒè™‘é“å¾·ç»“æœï¼Œä»¥åŠè¿™å¯¹äºäººæœºå…±ç”Ÿçš„å‰æ™¯æœ‰ä½•å¯ç¤ºã€‚ç ”ç©¶å…³æ³¨ä¸¤ä¸ªæ ¸å¿ƒé—®é¢˜ï¼š(1) æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹(LLM)åœ¨é¢å¯¹é“å¾·å›°å¢ƒæ—¶ï¼Œéšå«åœ°åå‘å“ªäº›é“å¾·ä»·å€¼è§‚ï¼Ÿ(2) æ¨¡å‹æ¶æ„ã€æ–‡åŒ–èƒŒæ™¯å’Œå¯è§£é‡Šæ€§çš„å·®å¼‚å¦‚ä½•å½±å“è¿™äº›é“å¾·åå¥½ï¼Ÿé€šè¿‡å¯¹å…­ä¸ªLLMè¿›è¡Œå®šé‡å®éªŒï¼Œå¯¹18ä¸ªä»£è¡¨äº”ç§é“å¾·æ¡†æ¶çš„å›°å¢ƒä¸­çš„ç»“æœè¿›è¡Œæ’åºå’Œè¯„åˆ†ã€‚ç ”ç©¶å‘ç°äº†ä¸€è‡´çš„ä»·å€¼åè§ï¼šå…³æ€€å’Œç¾å¾·ä»·å€¼è§‚çš„ç»“æœè¢«è¯„ä¸ºæœ€å…·é“å¾·æ€§ï¼Œè€Œè‡ªç”±ä¸»ä¹‰é€‰æ‹©åˆ™å—åˆ°ä¸€è‡´æƒ©ç½šã€‚å…·æœ‰æ¨ç†èƒ½åŠ›çš„æ¨¡å‹å¯¹ä¸Šä¸‹æ–‡è¡¨ç°å‡ºæ›´é«˜çš„æ•æ„Ÿæ€§ï¼Œå¹¶æä¾›äº†æ›´ä¸°å¯Œçš„è§£é‡Šï¼Œè€Œéæ¨ç†æ¨¡å‹åˆ™äº§ç”Ÿäº†æ›´ç»Ÿä¸€ä½†æ¨¡ç³Šçš„åˆ¤æ–­ã€‚è¯¥ç ”ç©¶åœ¨ç»éªŒä¸Šã€ç†è®ºä¸Šå’Œå®è·µä¸Šéƒ½åšå‡ºäº†è´¡çŒ®ï¼Œå¼ºè°ƒäº†å¯è§£é‡Šæ€§å’Œæ–‡åŒ–æ„è¯†ä½œä¸ºæŒ‡å¯¼äººå·¥æ™ºèƒ½èµ°å‘é€æ˜ã€ä¸€è‡´å’Œå…±ç”Ÿæœªæ¥çš„å…³é”®è®¾è®¡åŸåˆ™ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é¢å¯¹é“å¾·å›°å¢ƒæ—¶ï¼Œå…¶å†³ç­–è¿‡ç¨‹ä¸­éšå«çš„é“å¾·åè§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹å¯¹LLMé“å¾·ä»·å€¼è§‚çš„ç³»ç»Ÿæ€§è¯„ä¼°ï¼Œéš¾ä»¥ç†è§£å…¶å†³ç­–ä¾æ®ï¼Œå¯¼è‡´æ½œåœ¨çš„ä¼¦ç†é£é™©å’Œäººæœºåä½œéšœç¢ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è®¾è®¡ä¸€ç³»åˆ—é“å¾·å›°å¢ƒï¼Œè®©LLMåœ¨ä¸åŒçš„é“å¾·æ¡†æ¶ä¸‹è¿›è¡Œé€‰æ‹©ï¼Œå¹¶å¯¹é€‰æ‹©ç»“æœè¿›è¡Œæ’åºå’Œè¯„åˆ†ï¼Œä»è€Œé‡åŒ–LLMçš„é“å¾·åå¥½ã€‚é€šè¿‡æ¯”è¾ƒä¸åŒæ¶æ„ã€æ–‡åŒ–èƒŒæ™¯å’Œå¯è§£é‡Šæ€§çš„LLMï¼Œåˆ†æè¿™äº›å› ç´ å¯¹é“å¾·åè§çš„å½±å“ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨å®šé‡å®éªŒæ–¹æ³•ï¼Œä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š
1. **é“å¾·å›°å¢ƒè®¾è®¡**ï¼šæ„å»ºåŒ…å«18ä¸ªé“å¾·å›°å¢ƒçš„æ•°æ®é›†ï¼Œè¿™äº›å›°å¢ƒä»£è¡¨äº†äº”ç§ä¸åŒçš„é“å¾·æ¡†æ¶ï¼ˆä¾‹å¦‚ï¼Œå…³æ€€ã€ç¾å¾·ã€è‡ªç”±ä¸»ä¹‰ç­‰ï¼‰ã€‚
2. **LLMé€‰æ‹©ä¸æ’åº**ï¼šé€‰å–å…­ä¸ªå…·æœ‰ä»£è¡¨æ€§çš„LLMï¼Œè¦æ±‚å®ƒä»¬å¯¹æ¯ä¸ªå›°å¢ƒä¸­çš„ä¸åŒç»“æœè¿›è¡Œæ’åºå’Œè¯„åˆ†ã€‚
3. **ç»“æœåˆ†æ**ï¼šåˆ†æLLMçš„é€‰æ‹©ç»“æœï¼Œé‡åŒ–å…¶å¯¹ä¸åŒé“å¾·ä»·å€¼è§‚çš„åå¥½ï¼Œå¹¶æ¯”è¾ƒä¸åŒLLMä¹‹é—´çš„å·®å¼‚ã€‚
4. **å¯è§£é‡Šæ€§åˆ†æ**ï¼šå¯¹äºå…·æœ‰æ¨ç†èƒ½åŠ›çš„LLMï¼Œåˆ†æå…¶æä¾›çš„è§£é‡Šï¼Œç†è§£å…¶å†³ç­–ä¾æ®ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥ç ”ç©¶çš„å…³é”®åˆ›æ–°åœ¨äºï¼š
1. **å¤§è§„æ¨¡çš„é“å¾·åè§è¯„ä¼°**ï¼šé¦–æ¬¡å¯¹å¤šä¸ªå…·æœ‰ä¸åŒæ¶æ„å’Œæ–‡åŒ–èƒŒæ™¯çš„LLMè¿›è¡Œäº†å¤§è§„æ¨¡çš„é“å¾·åè§è¯„ä¼°ã€‚
2. **é‡åŒ–çš„é“å¾·åå¥½åˆ†æ**ï¼šé€šè¿‡æ’åºå’Œè¯„åˆ†çš„æ–¹å¼ï¼Œé‡åŒ–äº†LLMå¯¹ä¸åŒé“å¾·ä»·å€¼è§‚çš„åå¥½ã€‚
3. **å¯è§£é‡Šæ€§ä¸é“å¾·åè§å…³è”**ï¼šåˆ†æäº†å¯è§£é‡Šæ€§å¯¹LLMé“å¾·å†³ç­–çš„å½±å“ï¼Œæ­ç¤ºäº†å¯è§£é‡Šæ€§åœ¨å‡å°‘é“å¾·åè§ä¸­çš„ä½œç”¨ã€‚

**å…³é”®è®¾è®¡**ï¼š
1. **é“å¾·å›°å¢ƒçš„å¤šæ ·æ€§**ï¼šç¡®ä¿é“å¾·å›°å¢ƒæ¶µç›–ä¸åŒçš„é“å¾·æ¡†æ¶ï¼Œä»¥å…¨é¢è¯„ä¼°LLMçš„é“å¾·åå¥½ã€‚
2. **LLMçš„é€‰æ‹©**ï¼šé€‰æ‹©å…·æœ‰ä»£è¡¨æ€§çš„LLMï¼ŒåŒ…æ‹¬ä¸åŒæ¶æ„ï¼ˆä¾‹å¦‚ï¼ŒTransformerã€æ¨ç†å¢å¼ºå‹æ¨¡å‹ï¼‰å’Œæ–‡åŒ–èƒŒæ™¯ï¼ˆä¾‹å¦‚ï¼Œè¥¿æ–¹ã€ä¸œæ–¹ï¼‰çš„æ¨¡å‹ã€‚
3. **è¯„åˆ†æ ‡å‡†**ï¼šè®¾è®¡åˆç†çš„è¯„åˆ†æ ‡å‡†ï¼Œä»¥å‡†ç¡®é‡åŒ–LLMå¯¹ä¸åŒç»“æœçš„åå¥½ã€‚
4. **ç»Ÿè®¡åˆ†ææ–¹æ³•**ï¼šé‡‡ç”¨åˆé€‚çš„ç»Ÿè®¡åˆ†ææ–¹æ³•ï¼Œä¾‹å¦‚æ–¹å·®åˆ†æã€ç›¸å…³æ€§åˆ†æï¼Œä»¥åˆ†æLLMçš„é€‰æ‹©ç»“æœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æœ‰æ¨¡å‹éƒ½è¡¨ç°å‡ºå¯¹å…³æ€€å’Œç¾å¾·ä»·å€¼è§‚çš„åå¥½ï¼Œè€Œå¯¹è‡ªç”±ä¸»ä¹‰é€‰æ‹©çš„æƒ©ç½šã€‚å…·æœ‰æ¨ç†èƒ½åŠ›çš„æ¨¡å‹å¯¹ä¸Šä¸‹æ–‡æ›´æ•æ„Ÿï¼Œèƒ½æä¾›æ›´ä¸°å¯Œçš„è§£é‡Šã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†LLMä¸­æ™®éå­˜åœ¨çš„é“å¾·åè§ï¼Œå¹¶å¼ºè°ƒäº†å¯è§£é‡Šæ€§åœ¨å‡å°‘åè§ä¸­çš„ä½œç”¨ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå¼€å‘æ›´ç¬¦åˆäººç±»é“å¾·ä»·å€¼è§‚çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿï¼Œä¾‹å¦‚è‡ªåŠ¨é©¾é©¶æ±½è½¦ã€åŒ»ç–—è¯Šæ–­ç³»ç»Ÿå’Œé‡‘èé£é™©è¯„ä¼°ç³»ç»Ÿã€‚é€šè¿‡ç†è§£å’Œçº æ­£LLMä¸­çš„é“å¾·åè§ï¼Œå¯ä»¥æé«˜AIç³»ç»Ÿçš„å…¬å¹³æ€§ã€é€æ˜åº¦å’Œå¯é æ€§ï¼Œä¿ƒè¿›äººæœºåä½œã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Artificial intelligence (AI) is advancing at a pace that raises urgent questions about how to align machine decision-making with human moral values. This working paper investigates how leading AI systems prioritize moral outcomes and what this reveals about the prospects for human-AI symbiosis. We address two central questions: (1) What moral values do state-of-the-art large language models (LLMs) implicitly favour when confronted with dilemmas? (2) How do differences in model architecture, cultural origin, and explainability affect these moral preferences? To explore these questions, we conduct a quantitative experiment with six LLMs, ranking and scoring outcomes across 18 dilemmas representing five moral frameworks. Our findings uncover strikingly consistent value biases. Across all models, Care and Virtue values outcomes were rated most moral, while libertarian choices were consistently penalized. Reasoning-enabled models exhibited greater sensitivity to context and provided richer explanations, whereas non-reasoning models produced more uniform but opaque judgments. This research makes three contributions: (i) Empirically, it delivers a large-scale comparison of moral reasoning across culturally distinct LLMs; (ii) Theoretically, it links probabilistic model behaviour with underlying value encodings; (iii) Practically, it highlights the need for explainability and cultural awareness as critical design principles to guide AI toward a transparent, aligned, and symbiotic future.

