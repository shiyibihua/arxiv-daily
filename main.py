#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
arXiv è®ºæ–‡æ—¥æŠ¥æŠ“å–å·¥å…·

ç”¨æ³•:
    python main.py                           # åŸºç¡€æŠ“å–ï¼ˆä½¿ç”¨å…´è¶£ç­›é€‰ï¼‰
    python main.py --no-filter               # ä¸ä½¿ç”¨å…´è¶£ç­›é€‰
    python main.py --thumbnails              # æŠ“å–å¹¶è·å–ç¼©ç•¥å›¾
    python main.py --max-results 500         # é™åˆ¶æœ€å¤§è®ºæ–‡æ•°
    python main.py --concurrency 16          # æé«˜AIåˆ†æå¹¶å‘æ•°
"""

from utils.scrapy import load_tags, get_today_arxiv, filter_by_interests
from utils.analyser import update_ai_summary_async
from utils.image_extractor import batch_extract_images

import argparse
import asyncio
import json
import os


def main():
    parser = argparse.ArgumentParser(description="arXiv è®ºæ–‡æ—¥æŠ¥æŠ“å–å·¥å…·")
    parser.add_argument("--max-results", type=int, default=1000, 
                        help="æœ€å¤§æŠ“å–è®ºæ–‡æ•° (é»˜è®¤: 1000)")
    parser.add_argument("--thumbnails", action="store_true",
                        help="æŠ“å–è®ºæ–‡é¢„è§ˆç¼©ç•¥å›¾ (è¾ƒæ…¢)")
    parser.add_argument("--concurrency", type=int, default=8,
                        help="AIåˆ†æå¹¶å‘æ•° (é»˜è®¤: 8)")
    parser.add_argument("--temperature", type=float, default=0.2,
                        help="AIç”Ÿæˆæ¸©åº¦ (é»˜è®¤: 0.2)")
    parser.add_argument("--tags-file", default="tags.json",
                        help="åˆ†ç±»é…ç½®æ–‡ä»¶ (é»˜è®¤: tags.json)")
    parser.add_argument("--interests-file", default="interests.json",
                        help="å…´è¶£é…ç½®æ–‡ä»¶ (é»˜è®¤: interests.json)")
    parser.add_argument("--no-filter", action="store_true",
                        help="ä¸ä½¿ç”¨å…´è¶£ç­›é€‰ï¼ŒæŠ“å–æ‰€æœ‰è®ºæ–‡")
    parser.add_argument("--skip-ai", action="store_true",
                        help="è·³è¿‡AIåˆ†æï¼Œä»…æŠ“å–è®ºæ–‡")
    parser.add_argument("--no-images", action="store_true",
                        help="ä¸æå–è®ºæ–‡å›¾ç‰‡ï¼ˆé»˜è®¤ä¼šæå–ï¼‰")
    parser.add_argument("--max-images", type=int, default=3,
                        help="æ¯ç¯‡è®ºæ–‡æœ€å¤šæå–å›¾ç‰‡æ•° (é»˜è®¤: 3)")
    args = parser.parse_args()

    # åŠ è½½åˆ†ç±»æ ‡ç­¾
    tags = load_tags(args.tags_file)
    print(f"[INFO] ç›®æ ‡åˆ†ç±»: {tags}")

    # æŠ“å–è®ºæ–‡
    print(f"\nğŸ“¥ æ­£åœ¨æŠ“å– arXiv è®ºæ–‡...")
    result, label_date = get_today_arxiv(
        tags, 
        max_results=args.max_results,
        fetch_thumbnails=args.thumbnails
    )
    
    metas = result.get("papers", [])
    print(f"[INFO] å…±æŠ“å– {len(metas)} ç¯‡è®ºæ–‡")
    
    # å…´è¶£ç­›é€‰
    if not args.no_filter:
        print(f"\nğŸ¯ æ­£åœ¨æ ¹æ®å…´è¶£ç­›é€‰è®ºæ–‡...")
        metas = filter_by_interests(metas, args.interests_file)
        result["papers"] = metas
        result["count"] = len(metas)
        result["filtered"] = True
    
    # ä¿å­˜åŸå§‹æ•°æ®
    os.makedirs(f'data/{label_date}', exist_ok=True)
    arxiv_path = f'data/{label_date}/arxiv.json'
    with open(arxiv_path, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    print(f"[OK] ä¿å­˜åŸå§‹æ•°æ®: {arxiv_path}")
    
    # ç»Ÿè®¡ä»£ç é“¾æ¥
    papers_with_code = sum(1 for p in metas if p.get("code_links"))
    if papers_with_code:
        print(f"[INFO] å…¶ä¸­ {papers_with_code} ç¯‡è®ºæ–‡åŒ…å«ä»£ç é“¾æ¥")
    
    if args.thumbnails:
        papers_with_thumb = sum(1 for p in metas if p.get("thumbnail"))
        print(f"[INFO] æˆåŠŸè·å– {papers_with_thumb} ç¯‡è®ºæ–‡çš„é¢„è§ˆå›¾")
    
    if args.skip_ai:
        print("[INFO] è·³è¿‡AIåˆ†æ")
        return
    
    if len(metas) == 0:
        print("[WARN] æ²¡æœ‰åŒ¹é…å…´è¶£çš„è®ºæ–‡ï¼Œè·³è¿‡AIåˆ†æ")
        return
    
    # AI åˆ†æï¼ˆè‡ªåŠ¨é€‰æ‹© APIï¼Œæ”¯æŒæ•…éšœè½¬ç§»ï¼‰
    print(f"\nğŸ¤– æ­£åœ¨è¿›è¡Œ AI åˆ†æ...")
    print(f"[INFO] å¹¶å‘æ•°: {args.concurrency}, æ¸©åº¦: {args.temperature}")
    
    results = asyncio.run(update_ai_summary_async(
        metas=metas, 
        concurrency=args.concurrency, 
        temperature=args.temperature
    ))
    
    # å›¾ç‰‡æå–ï¼ˆé»˜è®¤å¯ç”¨ï¼‰
    if not args.no_images:
        print(f"\nğŸ–¼ï¸ æ­£åœ¨æå–è®ºæ–‡å›¾ç‰‡...")
        image_results = asyncio.run(batch_extract_images(
            papers=results,
            max_images_per_paper=args.max_images,
            concurrency=5
        ))
        # å°†å›¾ç‰‡ä¿¡æ¯åˆå¹¶åˆ°ç»“æœä¸­
        for paper in results:
            arxiv_id = paper.get("arxiv_id", "")
            if arxiv_id in image_results:
                paper["figures"] = image_results[arxiv_id]
        
        img_count = sum(len(v) for v in image_results.values())
        print(f"[OK] æå–å›¾ç‰‡: {img_count} å¼  (æ¥è‡ª {len(image_results)} ç¯‡è®ºæ–‡)")
    
    # ä¿å­˜AIåˆ†æç»“æœ
    ai_path = f'data/{label_date}/ai_summary.json'
    with open(ai_path, 'w', encoding='utf-8') as f:
        json.dump({"papers": results}, f, ensure_ascii=False, indent=4)
    print(f"[OK] ä¿å­˜AIåˆ†æ: {ai_path}")
    
    # ç»Ÿè®¡ç»“æœ
    ok_count = sum(1 for r in results if "_model_error" not in r and "_parse_error" not in r)
    err_count = len(results) - ok_count
    
    print(f"\nâœ… å®Œæˆï¼")
    print(f"   ğŸ“„ è®ºæ–‡æ•°: {len(results)}")
    print(f"   âœ“ æˆåŠŸåˆ†æ: {ok_count}")
    if err_count:
        print(f"   âœ— åˆ†æå¤±è´¥: {err_count}")
    print(f"\nğŸ“ æ•°æ®ç›®å½•: data/{label_date}/")
    print(f"   - arxiv.json       (åŸå§‹æ•°æ®)")
    print(f"   - ai_summary.json  (AIåˆ†æç»“æœ)")
    print(f"\nğŸ‘‰ è¿è¡Œ 'python build_page.py' ç”Ÿæˆç½‘ç«™é¡µé¢")


if __name__ == '__main__':
    main()
