#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
arXiv è®ºæ–‡æ—¥æŠ¥æŠ“å–å·¥å…·

ç”¨æ³•:
    python main.py                           # åŸºç¡€æŠ“å–
    python main.py --thumbnails              # æŠ“å–å¹¶è·å–ç¼©ç•¥å›¾
    python main.py --max-results 500         # é™åˆ¶æœ€å¤§è®ºæ–‡æ•°
    python main.py --concurrency 16          # æé«˜AIåˆ†æå¹¶å‘æ•°
"""

from utils.scrapy import load_tags, get_today_arxiv
from utils.analyser import get_client, update_ai_summary_async, get_model

import argparse
import asyncio
import json
import os


def main():
    parser = argparse.ArgumentParser(description="arXiv è®ºæ–‡æ—¥æŠ¥æŠ“å–å·¥å…·")
    parser.add_argument("--max-results", type=int, default=1000, 
                        help="æœ€å¤§æŠ“å–è®ºæ–‡æ•° (é»˜è®¤: 1000)")
    parser.add_argument("--thumbnails", action="store_true",
                        help="æŠ“å–è®ºæ–‡é¢„è§ˆç¼©ç•¥å›¾ (è¾ƒæ…¢)")
    parser.add_argument("--concurrency", type=int, default=8,
                        help="AIåˆ†æå¹¶å‘æ•° (é»˜è®¤: 8)")
    parser.add_argument("--temperature", type=float, default=0.2,
                        help="AIç”Ÿæˆæ¸©åº¦ (é»˜è®¤: 0.2)")
    parser.add_argument("--tags-file", default="tags.json",
                        help="åˆ†ç±»é…ç½®æ–‡ä»¶ (é»˜è®¤: tags.json)")
    parser.add_argument("--skip-ai", action="store_true",
                        help="è·³è¿‡AIåˆ†æï¼Œä»…æŠ“å–è®ºæ–‡")
    args = parser.parse_args()

    # åŠ è½½åˆ†ç±»æ ‡ç­¾
    tags = load_tags(args.tags_file)
    print(f"[INFO] ç›®æ ‡åˆ†ç±»: {tags}")

    # æŠ“å–è®ºæ–‡
    print(f"\nğŸ“¥ æ­£åœ¨æŠ“å– arXiv è®ºæ–‡...")
    result, label_date = get_today_arxiv(
        tags, 
        max_results=args.max_results,
        fetch_thumbnails=args.thumbnails
    )
    
    # ä¿å­˜åŸå§‹æ•°æ®
    os.makedirs(f'data/{label_date}', exist_ok=True)
    arxiv_path = f'data/{label_date}/arxiv.json'
    with open(arxiv_path, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    print(f"[OK] ä¿å­˜åŸå§‹æ•°æ®: {arxiv_path}")
    
    metas = result.get("papers", [])
    print(f"[INFO] å…±æŠ“å– {len(metas)} ç¯‡è®ºæ–‡")
    
    # ç»Ÿè®¡ä»£ç é“¾æ¥
    papers_with_code = sum(1 for p in metas if p.get("code_links"))
    if papers_with_code:
        print(f"[INFO] å…¶ä¸­ {papers_with_code} ç¯‡è®ºæ–‡åŒ…å«ä»£ç é“¾æ¥")
    
    if args.thumbnails:
        papers_with_thumb = sum(1 for p in metas if p.get("thumbnail"))
        print(f"[INFO] æˆåŠŸè·å– {papers_with_thumb} ç¯‡è®ºæ–‡çš„é¢„è§ˆå›¾")
    
    if args.skip_ai:
        print("[INFO] è·³è¿‡AIåˆ†æ")
        return
    
    # AI åˆ†æ
    print(f"\nğŸ¤– æ­£åœ¨è¿›è¡Œ AI åˆ†æ...")
    client = get_client()
    model = get_model()
    print(f"[INFO] ä½¿ç”¨æ¨¡å‹: {model}")
    print(f"[INFO] å¹¶å‘æ•°: {args.concurrency}, æ¸©åº¦: {args.temperature}")
    
    results = asyncio.run(update_ai_summary_async(
        client, 
        metas, 
        concurrency=args.concurrency, 
        temperature=args.temperature
    ))
    
    # ä¿å­˜AIåˆ†æç»“æœ
    ai_path = f'data/{label_date}/ai_summary.json'
    with open(ai_path, 'w', encoding='utf-8') as f:
        json.dump({"papers": results}, f, ensure_ascii=False, indent=4)
    print(f"[OK] ä¿å­˜AIåˆ†æ: {ai_path}")
    
    # ç»Ÿè®¡ç»“æœ
    ok_count = sum(1 for r in results if "_model_error" not in r and "_parse_error" not in r)
    err_count = len(results) - ok_count
    
    print(f"\nâœ… å®Œæˆï¼")
    print(f"   ğŸ“„ è®ºæ–‡æ•°: {len(results)}")
    print(f"   âœ“ æˆåŠŸåˆ†æ: {ok_count}")
    if err_count:
        print(f"   âœ— åˆ†æå¤±è´¥: {err_count}")
    print(f"\nğŸ“ æ•°æ®ç›®å½•: data/{label_date}/")
    print(f"   - arxiv.json       (åŸå§‹æ•°æ®)")
    print(f"   - ai_summary.json  (AIåˆ†æç»“æœ)")
    print(f"\nğŸ‘‰ è¿è¡Œ 'python build_page.py' ç”Ÿæˆç½‘ç«™é¡µé¢")


if __name__ == '__main__':
    main()
