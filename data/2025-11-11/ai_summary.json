{
    "papers": [
        {
            "title": "SONIC: Supersizing Motion Tracking for Natural Humanoid Whole-Body Control",
            "authors": [
                "Zhengyi Luo",
                "Ye Yuan",
                "Tingwu Wang",
                "Chenran Li",
                "Sirui Chen",
                "Fernando Castañeda",
                "Zi-Ang Cao",
                "Jiefeng Li",
                "David Minor",
                "Qingwei Ben",
                "Xingye Da",
                "Runyu Ding",
                "Cyrus Hogg",
                "Lina Song",
                "Edy Lim",
                "Eugene Jeong",
                "Tairan He",
                "Haoru Xue",
                "Wenli Xiao",
                "Zi Wang",
                "Simon Yuen",
                "Jan Kautz",
                "Yan Chang",
                "Umar Iqbal",
                "Linxi \"Jim\" Fan",
                "Yuke Zhu"
            ],
            "arxiv_id": "2511.07820v1",
            "summary": "Despite the rise of billion-parameter foundation models trained across thousands of GPUs, similar scaling gains have not been shown for humanoid control. Current neural controllers for humanoids remain modest in size, target a limited behavior set, and are trained on a handful of GPUs over several days. We show that scaling up model capacity, data, and compute yields a generalist humanoid controller capable of creating natural and robust whole-body movements. Specifically, we posit motion tracking as a natural and scalable task for humanoid control, leverageing dense supervision from diverse motion-capture data to acquire human motion priors without manual reward engineering. We build a foundation model for motion tracking by scaling along three axes: network size (from 1.2M to 42M parameters), dataset volume (over 100M frames, 700 hours of high-quality motion data), and compute (9k GPU hours). Beyond demonstrating the benefits of scale, we show the practical utility of our model through two mechanisms: (1) a real-time universal kinematic planner that bridges motion tracking to downstream task execution, enabling natural and interactive control, and (2) a unified token space that supports various motion input interfaces, such as VR teleoperation devices, human videos, and vision-language-action (VLA) models, all using the same policy. Scaling motion tracking exhibits favorable properties: performance improves steadily with increased compute and data diversity, and learned representations generalize to unseen motions, establishing motion tracking at scale as a practical foundation for humanoid control.",
            "headline_zh": "提出大规模运动跟踪模型以解决人形机器人全身控制问题",
            "intro_zh": [
                "当前人形机器人控制器规模小、行为有限，训练资源不足",
                "通过扩展模型参数、数据集和计算量，构建通用运动跟踪基础模型",
                "模型支持实时运动规划和多输入接口，提升控制自然性和鲁棒性"
            ],
            "tags_zh": [
                "人形机器人控制",
                "运动跟踪",
                "基础模型",
                "大规模训练",
                "全身运动",
                "多模态输入"
            ],
            "_index": 0
        },
        {
            "title": "An Image-Based Path Planning Algorithm Using a UAV Equipped with Stereo Vision",
            "authors": [
                "Selim Ahmet Iz",
                "Mustafa Unel"
            ],
            "arxiv_id": "2511.07928v1",
            "summary": "This paper presents a novel image-based path planning algorithm that was developed using computer vision techniques, as well as its comparative analysis with well-known deterministic and probabilistic algorithms, namely A* and Probabilistic Road Map algorithm (PRM). The terrain depth has a significant impact on the calculated path safety. The craters and hills on the surface cannot be distinguished in a two-dimensional image. The proposed method uses a disparity map of the terrain that is generated by using a UAV. Several computer vision techniques, including edge, line and corner detection methods, as well as the stereo depth reconstruction technique, are applied to the captured images and the found disparity map is used to define candidate way-points of the trajectory. The initial and desired points are detected automatically using ArUco marker pose estimation and circle detection techniques. After presenting the mathematical model and vision techniques, the developed algorithm is compared with well-known algorithms on different virtual scenes created in the V-REP simulation program and a physical setup created in a laboratory environment. Results are promising and demonstrate effectiveness of the proposed algorithm.",
            "headline_zh": "提出基于立体视觉的无人机图像路径规划算法，以提升复杂地形安全性。",
            "intro_zh": [
                "核心问题：二维图像无法区分地形坑洼与山丘，影响路径安全。",
                "方法要点：利用立体视觉生成视差图，结合边缘、线角检测定义候选路径点。",
                "实验或效果：在虚拟和物理环境中对比A*和PRM算法，结果有效。"
            ],
            "tags_zh": [
                "路径规划",
                "立体视觉",
                "视差图",
                "无人机导航",
                "计算机视觉",
                "仿真实验"
            ],
            "_index": 1
        },
        {
            "title": "ViPRA: Video Prediction for Robot Actions",
            "authors": [
                "Sandeep Routray",
                "Hengkai Pan",
                "Unnat Jain",
                "Shikhar Bahl",
                "Deepak Pathak"
            ],
            "arxiv_id": "2511.07732v1",
            "summary": "Can we turn a video prediction model into a robot policy? Videos, including those of humans or teleoperated robots, capture rich physical interactions. However, most of them lack labeled actions, which limits their use in robot learning. We present Video Prediction for Robot Actions (ViPRA), a simple pretraining-finetuning framework that learns continuous robot control from these actionless videos. Instead of directly predicting actions, we train a video-language model to predict both future visual observations and motion-centric latent actions, which serve as intermediate representations of scene dynamics. We train these latent actions using perceptual losses and optical flow consistency to ensure they reflect physically grounded behavior. For downstream control, we introduce a chunked flow matching decoder that maps latent actions to robot-specific continuous action sequences, using only 100 to 200 teleoperated demonstrations. This approach avoids expensive action annotation, supports generalization across embodiments, and enables smooth, high-frequency continuous control upto 22 Hz via chunked action decoding. Unlike prior latent action works that treat pretraining as autoregressive policy learning, explicitly models both what changes and how. Our method outperforms strong baselines, with a 16% gain on the SIMPLER benchmark and a 13% improvement across real world manipulation tasks. We will release models and code at https://vipra-project.github.io",
            "headline_zh": "提出ViPRA框架，从无动作视频学习机器人连续控制，提升泛化能力。",
            "intro_zh": [
                "问题：无标签动作视频难以直接用于机器人学习，限制物理交互利用。",
                "方法：训练视频-语言模型预测未来观测和运动中心潜在动作，使用感知损失和光流一致性。",
                "效果：在SIMPLER基准提升16%，真实世界任务提升13%，支持22Hz高频控制。"
            ],
            "tags_zh": [
                "视频预测",
                "机器人控制",
                "潜在动作学习",
                "光流一致性",
                "连续动作解码"
            ],
            "_index": 2
        },
        {
            "title": "Intuitive control of supernumerary robotic limbs through a tactile-encoded neural interface",
            "authors": [
                "Tianyu Jia",
                "Xingchen Yang",
                "Ciaran McGeady",
                "Yifeng Li",
                "Jinzhi Lin",
                "Kit San Ho",
                "Feiyu Pan",
                "Linhong Ji",
                "Chong Li",
                "Dario Farina"
            ],
            "arxiv_id": "2511.08454v1",
            "summary": "Brain-computer interfaces (BCIs) promise to extend human movement capabilities by enabling direct neural control of supernumerary effectors, yet integrating augmented commands with multiple degrees of freedom without disrupting natural movement remains a key challenge. Here, we propose a tactile-encoded BCI that leverages sensory afferents through a novel tactile-evoked P300 paradigm, allowing intuitive and reliable decoding of supernumerary motor intentions even when superimposed with voluntary actions. The interface was evaluated in a multi-day experiment comprising of a single motor recognition task to validate baseline BCI performance and a dual task paradigm to assess the potential influence between the BCI and natural human movement. The brain interface achieved real-time and reliable decoding of four supernumerary degrees of freedom, with significant performance improvements after only three days of training. Importantly, after training, performance did not differ significantly between the single- and dual-BCI task conditions, and natural movement remained unimpaired during concurrent supernumerary control. Lastly, the interface was deployed in a movement augmentation task, demonstrating its ability to command two supernumerary robotic arms for functional assistance during bimanual tasks. These results establish a new neural interface paradigm for movement augmentation through stimulation of sensory afferents, expanding motor degrees of freedom without impairing natural movement.",
            "headline_zh": "提出触觉编码神经接口以直观控制多余机器人肢体而不干扰自然运动",
            "intro_zh": [
                "核心问题：脑机接口在多自由度控制中易干扰自然运动，难以集成增强命令。",
                "方法要点：利用触觉诱发P300范式，通过感觉传入实现直觉式多余运动意图解码。",
                "实验或效果：经训练后，实时解码四自由度，双任务下性能无差异，自然运动未受损。"
            ],
            "tags_zh": [
                "脑机接口",
                "多余机器人肢体",
                "触觉编码",
                "P300范式",
                "运动增强",
                "感觉传入"
            ],
            "_index": 3
        },
        {
            "title": "A Circular Argument : Does RoPE need to be Equivariant for Vision?",
            "authors": [
                "Chase van de Geijn",
                "Timo Lüddecke",
                "Polina Turishcheva",
                "Alexander S. Ecker"
            ],
            "arxiv_id": "2511.08368v1",
            "summary": "Rotary Positional Encodings (RoPE) have emerged as a highly effective technique for one-dimensional sequences in Natural Language Processing spurring recent progress towards generalizing RoPE to higher-dimensional data such as images and videos. The success of RoPE has been thought to be due to its positional equivariance, i.e. its status as a relative positional encoding. In this paper, we mathematically show RoPE to be one of the most general solutions for equivariant positional embedding in one-dimensional data. Moreover, we show Mixed RoPE to be the analogously general solution for M-dimensional data, if we require commutative generators -- a property necessary for RoPE's equivariance. However, we question whether strict equivariance plays a large role in RoPE's performance. We propose Spherical RoPE, a method analogous to Mixed RoPE, but assumes non-commutative generators. Empirically, we find Spherical RoPE to have the equivalent or better learning behavior compared to its equivariant analogues. This suggests that relative positional embeddings are not as important as is commonly believed, at least within computer vision. We expect this discovery to facilitate future work in positional encodings for vision that can be faster and generalize better by removing the preconception that they must be relative.",
            "headline_zh": "提出Spherical RoPE以质疑位置等变性在视觉任务中的必要性",
            "intro_zh": [
                "核心问题：RoPE的成功是否依赖于位置等变性，尤其在视觉数据中。",
                "方法要点：引入Spherical RoPE，使用非交换生成器，不强制等变性。",
                "实验或效果：Spherical RoPE在视觉任务中表现等同或优于等变方法。"
            ],
            "tags_zh": [
                "旋转位置编码",
                "位置等变性",
                "视觉任务",
                "相对位置嵌入",
                "非交换生成器"
            ],
            "_index": 4
        },
        {
            "title": "Retrospective motion correction in MRI using disentangled embeddings",
            "authors": [
                "Qi Wang",
                "Veronika Ecker",
                "Marcel Früh",
                "Sergios Gatidis",
                "Thomas Küstner"
            ],
            "arxiv_id": "2511.08365v1",
            "summary": "Physiological motion can affect the diagnostic quality of magnetic resonance imaging (MRI). While various retrospective motion correction methods exist, many struggle to generalize across different motion types and body regions. In particular, machine learning (ML)-based corrections are often tailored to specific applications and datasets. We hypothesize that motion artifacts, though diverse, share underlying patterns that can be disentangled and exploited. To address this, we propose a hierarchical vector-quantized (VQ) variational auto-encoder that learns a disentangled embedding of motion-to-clean image features. A codebook is deployed to capture finite collection of motion patterns at multiple resolutions, enabling coarse-to-fine correction. An auto-regressive model is trained to learn the prior distribution of motion-free images and is used at inference to guide the correction process. Unlike conventional approaches, our method does not require artifact-specific training and can generalize to unseen motion patterns. We demonstrate the approach on simulated whole-body motion artifacts and observe robust correction across varying motion severity. Our results suggest that the model effectively disentangled physical motion of the simulated motion-effective scans, therefore, improving the generalizability of the ML-based MRI motion correction. Our work of disentangling the motion features shed a light on its potential application across anatomical regions and motion types.",
            "headline_zh": "提出分层VQ-VAE以解决MRI中生理运动伪影的泛化校正问题",
            "intro_zh": [
                "生理运动影响MRI诊断质量，现有方法难以泛化不同运动类型和身体区域",
                "使用分层VQ-VAE学习运动与干净图像特征的解耦嵌入，结合自回归模型引导校正",
                "在模拟全身运动伪影上验证，模型能泛化未见运动模式，提升图像质量"
            ],
            "tags_zh": [
                "MRI运动校正",
                "解耦嵌入",
                "分层VQ-VAE",
                "自回归模型",
                "泛化学习"
            ],
            "_index": 5
        },
        {
            "title": "Towards Open-Set Myoelectric Gesture Recognition via Dual-Perspective Inconsistency Learning",
            "authors": [
                "Chen Liu",
                "Can Han",
                "Weishi Xu",
                "Yaqi Wang",
                "Dahong Qian"
            ],
            "arxiv_id": "2511.08344v1",
            "summary": "Surface electromyography (sEMG)-based gesture recognition plays a critical role in human-machine interaction (HMI), particularly for rehabilitation and prosthetic control. However, sEMG-based systems often suffer from the scarcity of informative training data, leading to overfitting and poor generalization in deep learning models. Data augmentation offers a promising approach to increasing the size and diversity of training data, where faithfulness and diversity are two critical factors to effectiveness. However, promoting untargeted diversity can result in redundant samples with limited utility. To address these challenges, we propose a novel diffusion-based data augmentation approach, Sparse-Aware Semantic-Guided Diffusion Augmentation (SASG-DA). To enhance generation faithfulness, we introduce the Semantic Representation Guidance (SRG) mechanism by leveraging fine-grained, task-aware semantic representations as generation conditions. To enable flexible and diverse sample generation, we propose a Gaussian Modeling Semantic Modeling (GMSS) strategy, which models the semantic representation distribution and allows stochastic sampling to produce both faithful and diverse samples. To enhance targeted diversity, we further introduce a Sparse-Aware Semantic Sampling strategy to explicitly explore underrepresented regions, improving distribution coverage and sample utility. Extensive experiments on benchmark sEMG datasets, Ninapro DB2, DB4, and DB7, demonstrate that SASG-DA significantly outperforms existing augmentation methods. Overall, our proposed data augmentation approach effectively mitigates overfitting and improves recognition performance and generalization by offering both faithful and diverse samples.",
            "headline_zh": "提出SASG-DA数据增强方法以解决sEMG手势识别中的过拟合和泛化问题",
            "intro_zh": [
                "核心问题：sEMG手势识别数据稀缺导致模型过拟合和泛化能力差",
                "方法要点：使用扩散模型结合语义引导和稀疏感知采样生成忠实多样样本",
                "实验或效果：在Ninapro数据集上显著优于现有增强方法，提升识别性能"
            ],
            "tags_zh": [
                "表面肌电手势识别",
                "数据增强",
                "扩散模型",
                "语义引导",
                "稀疏感知采样",
                "泛化性能"
            ],
            "_index": 6
        },
        {
            "title": "Re-coding for Uncertainties: Edge-awareness Semantic Concordance for Resilient Event-RGB Segmentation",
            "authors": [
                "Nan Bao",
                "Yifan Zhao",
                "Lin Zhu",
                "Jia Li"
            ],
            "arxiv_id": "2511.08269v1",
            "summary": "Semantic segmentation has achieved great success in ideal conditions. However, when facing extreme conditions (e.g., insufficient light, fierce camera motion), most existing methods suffer from significant information loss of RGB, severely damaging segmentation results. Several researches exploit the high-speed and high-dynamic event modality as a complement, but event and RGB are naturally heterogeneous, which leads to feature-level mismatch and inferior optimization of existing multi-modality methods. Different from these researches, we delve into the edge secret of both modalities for resilient fusion and propose a novel Edge-awareness Semantic Concordance framework to unify the multi-modality heterogeneous features with latent edge cues. In this framework, we first propose Edge-awareness Latent Re-coding, which obtains uncertainty indicators while realigning event-RGB features into unified semantic space guided by re-coded distribution, and transfers event-RGB distributions into re-coded features by utilizing a pre-established edge dictionary as clues. We then propose Re-coded Consolidation and Uncertainty Optimization, which utilize re-coded edge features and uncertainty indicators to solve the heterogeneous event-RGB fusion issues under extreme conditions. We establish two synthetic and one real-world event-RGB semantic segmentation datasets for extreme scenario comparisons. Experimental results show that our method outperforms the state-of-the-art by a 2.55% mIoU on our proposed DERS-XS, and possesses superior resilience under spatial occlusion. Our code and datasets are publicly available at https://github.com/iCVTEAM/ESC.",
            "headline_zh": "提出边缘感知语义一致性框架以解决极端条件下事件-RGB分割的异构特征融合问题",
            "intro_zh": [
                "核心问题：极端条件下RGB信息丢失，事件与RGB模态异构导致特征不匹配和优化困难。",
                "方法要点：通过边缘感知潜在重编码和不确定性优化，统一异构特征并提升融合鲁棒性。",
                "实验或效果：在合成和真实数据集上优于现有方法，mIoU提升2.55%，具有空间遮挡鲁棒性。"
            ],
            "tags_zh": [
                "事件-RGB分割",
                "异构特征融合",
                "边缘感知",
                "不确定性优化",
                "极端条件语义分割"
            ],
            "_index": 7
        },
        {
            "title": "Hierarchical Direction Perception via Atomic Dot-Product Operators for Rotation-Invariant Point Clouds Learning",
            "authors": [
                "Chenyu Hu",
                "Xiaotong Li",
                "Hao Zhu",
                "Biao Hou"
            ],
            "arxiv_id": "2511.08240v1",
            "summary": "Point cloud processing has become a cornerstone technology in many 3D vision tasks. However, arbitrary rotations introduce variations in point cloud orientations, posing a long-standing challenge for effective representation learning. The core of this issue is the disruption of the point cloud's intrinsic directional characteristics caused by rotational perturbations. Recent methods attempt to implicitly model rotational equivariance and invariance, preserving directional information and propagating it into deep semantic spaces. Yet, they often fall short of fully exploiting the multiscale directional nature of point clouds to enhance feature representations. To address this, we propose the Direction-Perceptive Vector Network (DiPVNet). At its core is an atomic dot-product operator that simultaneously encodes directional selectivity and rotation invariance--endowing the network with both rotational symmetry modeling and adaptive directional perception. At the local level, we introduce a Learnable Local Dot-Product (L2DP) Operator, which enables interactions between a center point and its neighbors to adaptively capture the non-uniform local structures of point clouds. At the global level, we leverage generalized harmonic analysis to prove that the dot-product between point clouds and spherical sampling vectors is equivalent to a direction-aware spherical Fourier transform (DASFT). This leads to the construction of a global directional response spectrum for modeling holistic directional structures. We rigorously prove the rotation invariance of both operators. Extensive experiments on challenging scenarios involving noise and large-angle rotations demonstrate that DiPVNet achieves state-of-the-art performance on point cloud classification and segmentation tasks. Our code is available at https://github.com/wxszreal0/DiPVNet.",
            "headline_zh": "提出DiPVNet以解决点云旋转不变性学习中的方向感知问题",
            "intro_zh": [
                "点云旋转导致方向特征破坏，影响表示学习",
                "使用原子点积算子实现旋转不变性和自适应方向感知",
                "在噪声和大角度旋转场景下实现SOTA分类与分割性能"
            ],
            "tags_zh": [
                "点云处理",
                "旋转不变性",
                "方向感知",
                "原子点积算子",
                "球形傅里叶变换"
            ],
            "_index": 8
        },
        {
            "title": "Remodeling Semantic Relationships in Vision-Language Fine-Tuning",
            "authors": [
                "Xiangyang Wu",
                "Liu Liu",
                "Baosheng Yu",
                "Jiayan Qiu",
                "Zhenwei Shi"
            ],
            "arxiv_id": "2511.08238v1",
            "summary": "Vision-language fine-tuning has emerged as an efficient paradigm for constructing multimodal foundation models. While textual context often highlights semantic relationships within an image, existing fine-tuning methods typically overlook this information when aligning vision and language, thus leading to suboptimal performance. Toward solving this problem, we propose a method that can improve multimodal alignment and fusion based on both semantics and relationships.Specifically, we first extract multilevel semantic features from different vision encoder to capture more visual cues of the relationships. Then, we learn to project the vision features to group related semantics, among which are more likely to have relationships. Finally, we fuse the visual features with the textual by using inheritable cross-attention, where we globally remove the redundant visual relationships by discarding visual-language feature pairs with low correlation. We evaluate our proposed method on eight foundation models and two downstream tasks, visual question answering and image captioning, and show that it outperforms all existing methods.",
            "headline_zh": "提出基于语义关系重塑的方法以改进视觉-语言微调中的多模态对齐",
            "intro_zh": [
                "现有视觉-语言微调方法忽略文本上下文中的语义关系，导致性能不佳",
                "方法包括提取多级视觉语义特征、投影分组相关语义、使用可继承交叉注意力融合特征",
                "在八个基础模型和两个下游任务上评估，优于现有方法"
            ],
            "tags_zh": [
                "视觉-语言微调",
                "语义关系建模",
                "多模态对齐",
                "交叉注意力",
                "视觉问答",
                "图像描述生成"
            ],
            "_index": 9
        },
        {
            "title": "Real-Time Performance Analysis of Multi-Fidelity Residual Physics-Informed Neural Process-Based State Estimation for Robotic Systems",
            "authors": [
                "Devin Hunter",
                "Chinwendu Enyioha"
            ],
            "arxiv_id": "2511.08231v1",
            "summary": "Various neural network architectures are used in many of the state-of-the-art approaches for real-time nonlinear state estimation. With the ever-increasing incorporation of these data-driven models into the estimation domain, model predictions with reliable margins of error are a requirement -- especially for safety-critical applications. This paper discusses the application of a novel real-time, data-driven estimation approach based on the multi-fidelity residual physics-informed neural process (MFR-PINP) toward the real-time state estimation of a robotic system. Specifically, we address the model-mismatch issue of selecting an accurate kinematic model by tasking the MFR-PINP to also learn the residuals between simple, low-fidelity predictions and complex, high-fidelity ground-truth dynamics. To account for model uncertainty present in a physical implementation, robust uncertainty guarantees from the split conformal (SC) prediction framework are modeled in the training and inference paradigms. We provide implementation details of our MFR-PINP-based estimator for a hybrid online learning setting to validate our model's usage in real-time applications. Experimental results of our approach's performance in comparison to the state-of-the-art variants of the Kalman filter (i.e. unscented Kalman filter and deep Kalman filter) in estimation scenarios showed promising results for the MFR-PINP model as a viable option in real-time estimation tasks.",
            "headline_zh": "提出多保真残差物理信息神经过程方法，用于机器人系统实时状态估计",
            "intro_zh": [
                "核心问题：机器人状态估计中模型不匹配和不确定性影响实时性与安全性。",
                "方法要点：结合多保真残差学习和分形预测框架，提升估计精度与鲁棒性。",
                "实验或效果：在实时场景中优于卡尔曼滤波器变体，验证了方法的可行性。"
            ],
            "tags_zh": [
                "状态估计",
                "物理信息神经网络",
                "多保真学习",
                "不确定性建模",
                "机器人系统",
                "实时应用"
            ],
            "_index": 10
        },
        {
            "title": "The Online Patch Redundancy Eliminator (OPRE): A novel approach to online agnostic continual learning using dataset compression",
            "authors": [
                "Raphaël Bayle",
                "Martial Mermillod",
                "Robert M. French"
            ],
            "arxiv_id": "2511.08226v1",
            "summary": "In order to achieve Continual Learning (CL), the problem of catastrophic forgetting, one that has plagued neural networks since their inception, must be overcome. The evaluation of continual learning methods relies on splitting a known homogeneous dataset and learning the associated tasks one after the other. We argue that most CL methods introduce a priori information about the data to come and cannot be considered agnostic. We exemplify this point with the case of methods relying on pretrained feature extractors, which are still used in CL. After showing that pretrained feature extractors imply a loss of generality with respect to the data that can be learned by the model, we then discuss other kinds of a priori information introduced in other CL methods. We then present the Online Patch Redundancy Eliminator (OPRE), an online dataset compression algorithm, which, along with the training of a classifier at test time, yields performance on CIFAR-10 and CIFAR-100 superior to a number of other state-of-the-art online continual learning methods. Additionally, OPRE requires only minimal and interpretable hypothesis on the data to come. We suggest that online dataset compression could well be necessary to achieve fully agnostic CL.",
            "headline_zh": "提出OPRE在线数据集压缩方法，实现无先验信息的在线持续学习。",
            "intro_zh": [
                "核心问题：持续学习中灾难性遗忘，现有方法依赖先验信息，缺乏无先验通用性。",
                "方法要点：OPRE在线压缩数据集，测试时训练分类器，减少数据冗余。",
                "实验或效果：在CIFAR-10和CIFAR-100上性能优于其他在线持续学习方法。"
            ],
            "tags_zh": [
                "持续学习",
                "灾难性遗忘",
                "数据集压缩",
                "在线学习",
                "无先验学习"
            ],
            "_index": 11
        },
        {
            "title": "2D Representation for Unguided Single-View 3D Super-Resolution in Real-Time",
            "authors": [
                "Ignasi Mas",
                "Ivan Huerta",
                "Ramon Morros",
                "Javier Ruiz-Hidalgo"
            ],
            "arxiv_id": "2511.08224v1",
            "summary": "We introduce 2Dto3D-SR, a versatile framework for real-time single-view 3D super-resolution that eliminates the need for high-resolution RGB guidance. Our framework encodes 3D data from a single viewpoint into a structured 2D representation, enabling the direct application of existing 2D image super-resolution architectures. We utilize the Projected Normalized Coordinate Code (PNCC) to represent 3D geometry from a visible surface as a regular image, thereby circumventing the complexities of 3D point-based or RGB-guided methods. This design supports lightweight and fast models adaptable to various deployment environments. We evaluate 2Dto3D-SR with two implementations: one using Swin Transformers for high accuracy, and another using Vision Mamba for high efficiency. Experiments show the Swin Transformer model achieves state-of-the-art accuracy on standard benchmarks, while the Vision Mamba model delivers competitive results at real-time speeds. This establishes our geometry-guided pipeline as a surprisingly simple yet viable and practical solution for real-world scenarios, especially where high-resolution RGB data is inaccessible.",
            "headline_zh": "提出2Dto3D-SR框架，实现无高分辨率RGB引导的实时单视图3D超分辨率。",
            "intro_zh": [
                "核心问题：单视图3D超分辨率需高分辨率RGB引导，复杂且不实用。",
                "方法要点：将3D数据编码为2D表示，直接应用2D超分辨率架构。",
                "实验效果：Swin Transformer版精度领先，Vision Mamba版实时高效。"
            ],
            "tags_zh": [
                "3D超分辨率",
                "单视图重建",
                "2D表示",
                "实时处理",
                "几何编码",
                "轻量模型"
            ],
            "_index": 12
        },
        {
            "title": "Evaluating Gemini LLM in Food Image-Based Recipe and Nutrition Description with EfficientNet-B4 Visual Backbone",
            "authors": [
                "Rizal Khoirul Anam"
            ],
            "arxiv_id": "2511.08215v1",
            "summary": "The proliferation of digital food applications necessitates robust methods for automated nutritional analysis and culinary guidance. This paper presents a comprehensive comparative evaluation of a decoupled, multimodal pipeline for food recognition. We evaluate a system integrating a specialized visual backbone (EfficientNet-B4) with a powerful generative large language model (Google's Gemini LLM). The core objective is to evaluate the trade-offs between visual classification accuracy, model efficiency, and the quality of generative output (nutritional data and recipes). We benchmark this pipeline against alternative vision backbones (VGG-16, ResNet-50, YOLOv8) and a lightweight LLM (Gemma). We introduce a formalization for \"Semantic Error Propagation\" (SEP) to analyze how classification inaccuracies from the visual module cascade into the generative output. Our analysis is grounded in a new Custom Chinese Food Dataset (CCFD) developed to address cultural bias in public datasets. Experimental results demonstrate that while EfficientNet-B4 (89.0\\% Top-1 Acc.) provides the best balance of accuracy and efficiency, and Gemini (9.2/10 Factual Accuracy) provides superior generative quality, the system's overall utility is fundamentally bottlenecked by the visual front-end's perceptive accuracy. We conduct a detailed per-class analysis, identifying high semantic similarity as the most critical failure mode.",
            "headline_zh": "评估基于EfficientNet-B4和Gemini LLM的多模态系统在食物图像识别与生成中的性能平衡",
            "intro_zh": [
                "核心问题：自动化食物营养分析和食谱生成中视觉分类精度与生成质量的权衡。",
                "方法要点：结合EfficientNet-B4视觉骨干和Gemini LLM，分析语义错误传播。",
                "实验或效果：在自定义数据集上，EfficientNet-B4准确率89.0%，Gemini事实准确率9.2/10。"
            ],
            "tags_zh": [
                "食物图像识别",
                "多模态系统",
                "语义错误传播",
                "EfficientNet-B4",
                "Gemini LLM",
                "营养分析"
            ],
            "_index": 13
        },
        {
            "title": "Twist and Compute: The Cost of Pose in 3D Generative Diffusion",
            "authors": [
                "Kyle Fogarty",
                "Jack Foster",
                "Boqiao Zhang",
                "Jing Yang",
                "Cengiz Öztireli"
            ],
            "arxiv_id": "2511.08203v1",
            "summary": "Despite their impressive results, large-scale image-to-3D generative models remain opaque in their inductive biases. We identify a significant limitation in image-conditioned 3D generative models: a strong canonical view bias. Through controlled experiments using simple 2D rotations, we show that the state-of-the-art Hunyuan3D 2.0 model can struggle to generalize across viewpoints, with performance degrading under rotated inputs. We show that this failure can be mitigated by a lightweight CNN that detects and corrects input orientation, restoring model performance without modifying the generative backbone. Our findings raise an important open question: Is scale enough, or should we pursue modular, symmetry-aware designs?",
            "headline_zh": "提出轻量CNN检测输入方向以缓解3D生成模型视角偏差问题",
            "intro_zh": [
                "核心问题：图像到3D生成模型存在强规范视角偏差，影响多视角泛化能力",
                "方法要点：使用轻量CNN检测并校正输入图像方向，不修改生成主干网络",
                "实验或效果：在旋转输入下性能下降，经校正后恢复模型性能"
            ],
            "tags_zh": [
                "3D生成模型",
                "视角偏差",
                "图像到3D生成",
                "轻量CNN",
                "泛化能力",
                "对称感知设计"
            ],
            "_index": 14
        },
        {
            "title": "PEOD: A Pixel-Aligned Event-RGB Benchmark for Object Detection under Challenging Conditions",
            "authors": [
                "Luoping Cui",
                "Hanqing Liu",
                "Mingjie Liu",
                "Endian Lin",
                "Donghong Jiang",
                "Yuhao Wang",
                "Chuang Zhu"
            ],
            "arxiv_id": "2511.08140v1",
            "summary": "Robust object detection for challenging scenarios increasingly relies on event cameras, yet existing Event-RGB datasets remain constrained by sparse coverage of extreme conditions and low spatial resolution (<= 640 x 480), which prevents comprehensive evaluation of detectors under challenging scenarios. To address these limitations, we propose PEOD, the first large-scale, pixel-aligned and high-resolution (1280 x 720) Event-RGB dataset for object detection under challenge conditions. PEOD contains 130+ spatiotemporal-aligned sequences and 340k manual bounding boxes, with 57% of data captured under low-light, overexposure, and high-speed motion. Furthermore, we benchmark 14 methods across three input configurations (Event-based, RGB-based, and Event-RGB fusion) on PEOD. On the full test set and normal subset, fusion-based models achieve the excellent performance. However, in illumination challenge subset, the top event-based model outperforms all fusion models, while fusion models still outperform their RGB-based counterparts, indicating limits of existing fusion methods when the frame modality is severely degraded. PEOD establishes a realistic, high-quality benchmark for multimodal perception and facilitates future research.",
            "headline_zh": "提出PEOD数据集以解决挑战条件下事件-RGB目标检测的基准不足问题",
            "intro_zh": [
                "现有事件-RGB数据集覆盖极端条件稀疏且分辨率低，阻碍鲁棒检测评估",
                "构建首个大规模像素对齐高分辨率事件-RGB数据集，含130+序列和34万标注框",
                "基准测试显示融合模型在正常条件下优，事件模型在光照挑战下领先融合模型"
            ],
            "tags_zh": [
                "事件相机",
                "目标检测",
                "多模态融合",
                "挑战条件",
                "高分辨率数据集"
            ],
            "_index": 15
        },
        {
            "title": "Introducing Nylon Face Mask Attacks: A Dataset for Evaluating Generalised Face Presentation Attack Detection",
            "authors": [
                "Manasa",
                "Sushrut Patwardhan",
                "Narayan Vetrekar",
                "Pavan Kumar",
                "R. S. Gad",
                "Raghavendra Ramachandra"
            ],
            "arxiv_id": "2511.08114v1",
            "summary": "Face recognition systems are increasingly deployed across a wide range of applications, including smartphone authentication, access control, and border security. However, these systems remain vulnerable to presentation attacks (PAs), which can significantly compromise their reliability. In this work, we introduce a new dataset focused on a novel and realistic presentation attack instrument called Nylon Face Masks (NFMs), designed to simulate advanced 3D spoofing scenarios. NFMs are particularly concerning due to their elastic structure and photorealistic appearance, which enable them to closely mimic the victim's facial geometry when worn by an attacker. To reflect real-world smartphone-based usage conditions, we collected the dataset using an iPhone 11 Pro, capturing 3,760 bona fide samples from 100 subjects and 51,281 NFM attack samples across four distinct presentation scenarios involving both humans and mannequins. We benchmark the dataset using five state-of-the-art PAD methods to evaluate their robustness under unseen attack conditions. The results demonstrate significant performance variability across methods, highlighting the challenges posed by NFMs and underscoring the importance of developing PAD techniques that generalise effectively to emerging spoofing threats.",
            "headline_zh": "提出尼龙面罩攻击数据集以评估人脸呈现攻击检测的泛化能力",
            "intro_zh": [
                "人脸识别系统易受呈现攻击，尼龙面罩因其弹性和逼真外观构成新威胁。",
                "使用iPhone 11 Pro收集真实与攻击样本，涵盖人类和人体模型四种场景。",
                "基准测试显示现有PAD方法性能差异大，强调需开发泛化技术应对新攻击。"
            ],
            "tags_zh": [
                "人脸呈现攻击检测",
                "尼龙面罩攻击",
                "数据集构建",
                "泛化评估",
                "智能手机认证",
                "3D欺骗场景"
            ],
            "_index": 16
        },
        {
            "title": "StableMorph: High-Quality Face Morph Generation with Stable Diffusion",
            "authors": [
                "Wassim Kabbani",
                "Kiran Raja",
                "Raghavendra Ramachandra",
                "Christoph Busch"
            ],
            "arxiv_id": "2511.08090v1",
            "summary": "Face morphing attacks threaten the integrity of biometric identity systems by enabling multiple individuals to share a single identity. To develop and evaluate effective morphing attack detection (MAD) systems, we need access to high-quality, realistic morphed images that reflect the challenges posed in real-world scenarios. However, existing morph generation methods often produce images that are blurry, riddled with artifacts, or poorly constructed making them easy to detect and not representative of the most dangerous attacks. In this work, we introduce StableMorph, a novel approach that generates highly realistic, artifact-free morphed face images using modern diffusion-based image synthesis. Unlike prior methods, StableMorph produces full-head images with sharp details, avoids common visual flaws, and offers unmatched control over visual attributes. Through extensive evaluation, we show that StableMorph images not only rival or exceed the quality of genuine face images but also maintain a strong ability to fool face recognition systems posing a greater challenge to existing MAD solutions and setting a new standard for morph quality in research and operational testing. StableMorph improves the evaluation of biometric security by creating more realistic and effective attacks and supports the development of more robust detection systems.",
            "headline_zh": "提出StableMorph方法以生成高质量人脸融合图像，用于评估生物识别安全。",
            "intro_zh": [
                "人脸融合攻击威胁生物识别系统，现有方法图像模糊且易检测。",
                "基于扩散模型生成无伪影、细节清晰的完整头部融合图像。",
                "实验显示图像质量高，能有效欺骗人脸识别系统，提升检测挑战。"
            ],
            "tags_zh": [
                "人脸融合攻击",
                "扩散模型",
                "生物识别安全",
                "图像生成",
                "攻击检测"
            ],
            "_index": 17
        },
        {
            "title": "Beyond the Pixels: VLM-based Evaluation of Identity Preservation in Reference-Guided Synthesis",
            "authors": [
                "Aditi Singhania",
                "Krutik Malani",
                "Riddhi Dhawan",
                "Arushi Jain",
                "Garv Tandon",
                "Nippun Sharma",
                "Souymodip Chakraborty",
                "Vineet Batra",
                "Ankit Phogat"
            ],
            "arxiv_id": "2511.08087v1",
            "summary": "Evaluating identity preservation in generative models remains a critical yet unresolved challenge. Existing metrics rely on global embeddings or coarse VLM prompting, failing to capture fine-grained identity changes and providing limited diagnostic insight. We introduce Beyond the Pixels, a hierarchical evaluation framework that decomposes identity assessment into feature-level transformations. Our approach guides VLMs through structured reasoning by (1) hierarchically decomposing subjects into (type, style) -> attribute -> feature decision tree, and (2) prompting for concrete transformations rather than abstract similarity scores. This decomposition grounds VLM analysis in verifiable visual evidence, reducing hallucinations and improving consistency. We validate our framework across four state-of-the-art generative models, demonstrating strong alignment with human judgments in measuring identity consistency. Additionally, we introduce a new benchmark specifically designed to stress-test generative models. It comprises 1,078 image-prompt pairs spanning diverse subject types, including underrepresented categories such as anthropomorphic and animated characters, and captures an average of six to seven transformation axes per prompt.",
            "headline_zh": "提出基于VLM的分层评估框架以解决生成模型中身份保持的细粒度评估问题",
            "intro_zh": [
                "核心问题：现有指标依赖全局嵌入或粗略提示，无法捕捉细粒度身份变化且诊断能力有限",
                "方法要点：通过分层分解主体为决策树，引导VLM进行结构化推理以评估特征级变换",
                "实验或效果：在四个先进生成模型上验证，与人类判断高度一致，并引入新基准"
            ],
            "tags_zh": [
                "身份保持评估",
                "视觉语言模型",
                "分层评估框架",
                "生成模型基准",
                "细粒度分析"
            ],
            "_index": 18
        },
        {
            "title": "Taming Identity Consistency and Prompt Diversity in Diffusion Models via Latent Concatenation and Masked Conditional Flow Matching",
            "authors": [
                "Aditi Singhania",
                "Arushi Jain",
                "Krutik Malani",
                "Riddhi Dhawan",
                "Souymodip Chakraborty",
                "Vineet Batra",
                "Ankit Phogat"
            ],
            "arxiv_id": "2511.08061v1",
            "summary": "Subject-driven image generation aims to synthesize novel depictions of a specific subject across diverse contexts while preserving its core identity features. Achieving both strong identity consistency and high prompt diversity presents a fundamental trade-off. We propose a LoRA fine-tuned diffusion model employing a latent concatenation strategy, which jointly processes reference and target images, combined with a masked Conditional Flow Matching (CFM) objective. This approach enables robust identity preservation without architectural modifications. To facilitate large-scale training, we introduce a two-stage Distilled Data Curation Framework: the first stage leverages data restoration and VLM-based filtering to create a compact, high-quality seed dataset from diverse sources; the second stage utilizes these curated examples for parameter-efficient fine-tuning, thus scaling the generation capability across various subjects and contexts. Finally, for filtering and quality assessment, we present CHARIS, a fine-grained evaluation framework that performs attribute-level comparisons along five key axes: identity consistency, prompt adherence, region-wise color fidelity, visual quality, and transformation diversity.",
            "headline_zh": "提出潜在连接与掩码条件流匹配方法，以解决主题驱动图像生成中身份一致性与提示多样性的权衡问题。",
            "intro_zh": [
                "核心问题：主题驱动图像生成中，身份一致性与提示多样性存在根本性权衡。",
                "方法要点：使用LoRA微调扩散模型，结合潜在连接策略和掩码条件流匹配目标。",
                "实验或效果：引入蒸馏数据策展框架和CHARIS评估，提升生成质量和多样性。"
            ],
            "tags_zh": [
                "主题驱动图像生成",
                "扩散模型",
                "潜在连接",
                "条件流匹配",
                "数据蒸馏",
                "图像质量评估"
            ],
            "_index": 19
        },
        {
            "title": "Generalized-Scale Object Counting with Gradual Query Aggregation",
            "authors": [
                "Jer Pelhan",
                "Alan Lukezic",
                "Matej Kristan"
            ],
            "arxiv_id": "2511.08048v1",
            "summary": "Few-shot detection-based counters estimate the number of instances in the image specified only by a few test-time exemplars. A common approach to localize objects across multiple sizes is to merge backbone features of different resolutions. Furthermore, to enable small object detection in densely populated regions, the input image is commonly upsampled and tiling is applied to cope with the increased computational and memory requirements. Because of these ad-hoc solutions, existing counters struggle with images containing diverse-sized objects and densely populated regions of small objects. We propose GECO2, an end-to-end few-shot counting and detection method that explicitly addresses the object scale issues. A new dense query representation gradually aggregates exemplar-specific feature information across scales that leads to high-resolution dense queries that enable detection of large as well as small objects. GECO2 surpasses state-of-the-art few-shot counters in counting as well as detection accuracy by 10% while running 3x times faster at smaller GPU memory footprint.",
            "headline_zh": "提出GECO2方法以解决多尺度与密集小物体计数问题",
            "intro_zh": [
                "核心问题：现有计数器难以处理多尺度物体和密集小物体区域",
                "方法要点：使用渐进查询聚合跨尺度特征，生成高分辨率密集查询",
                "实验或效果：计数与检测精度提升10%，速度提高3倍，内存占用更小"
            ],
            "tags_zh": [
                "少样本计数",
                "多尺度检测",
                "渐进查询聚合",
                "密集物体检测",
                "端到端学习"
            ],
            "_index": 20
        },
        {
            "title": "High-Quality Proposal Encoding and Cascade Denoising for Imaginary Supervised Object Detection",
            "authors": [
                "Zhiyuan Chen",
                "Yuelin Guo",
                "Zitong Huang",
                "Haoyu He",
                "Renhao Lu",
                "Weizhe Zhang"
            ],
            "arxiv_id": "2511.08018v1",
            "summary": "Object detection models demand large-scale annotated datasets, which are costly and labor-intensive to create. This motivated Imaginary Supervised Object Detection (ISOD), where models train on synthetic images and test on real images. However, existing methods face three limitations: (1) synthetic datasets suffer from simplistic prompts, poor image quality, and weak supervision; (2) DETR-based detectors, due to their random query initialization, struggle with slow convergence and overfitting to synthetic patterns, hindering real-world generalization; (3) uniform denoising pressure promotes model overfitting to pseudo-label noise. We propose Cascade HQP-DETR to address these limitations. First, we introduce a high-quality data pipeline using LLaMA-3, Flux, and Grounding DINO to generate the FluxVOC and FluxCOCO datasets, advancing ISOD from weak to full supervision. Second, our High-Quality Proposal guided query encoding initializes object queries with image-specific priors from SAM-generated proposals and RoI-pooled features, accelerating convergence while steering the model to learn transferable features instead of overfitting to synthetic patterns. Third, our cascade denoising algorithm dynamically adjusts training weights through progressively increasing IoU thresholds across decoder layers, guiding the model to learn robust boundaries from reliable visual cues rather than overfitting to noisy labels. Trained for just 12 epochs solely on FluxVOC, Cascade HQP-DETR achieves a SOTA 61.04\\% mAP@0.5 on PASCAL VOC 2007, outperforming strong baselines, with its competitive real-data performance confirming the architecture's universal applicability.",
            "headline_zh": "提出Cascade HQP-DETR以解决虚构监督目标检测中的数据质量、收敛慢和噪声过拟合问题",
            "intro_zh": [
                "核心问题：合成数据质量差、DETR收敛慢、伪标签噪声导致过拟合",
                "方法要点：高质量数据生成、基于SAM的查询初始化、级联去噪训练",
                "实验或效果：仅训练12epochs，在PASCAL VOC上达到61.04% mAP@0.5"
            ],
            "tags_zh": [
                "虚构监督目标检测",
                "查询初始化",
                "级联去噪",
                "合成数据生成",
                "DETR架构"
            ],
            "_index": 21
        },
        {
            "title": "Invisible Triggers, Visible Threats! Road-Style Adversarial Creation Attack for Visual 3D Detection in Autonomous Driving",
            "authors": [
                "Jian Wang",
                "Lijun He",
                "Yixing Yong",
                "Haixia Bi",
                "Fan Li"
            ],
            "arxiv_id": "2511.08015v1",
            "summary": "Modern autonomous driving (AD) systems leverage 3D object detection to perceive foreground objects in 3D environments for subsequent prediction and planning. Visual 3D detection based on RGB cameras provides a cost-effective solution compared to the LiDAR paradigm. While achieving promising detection accuracy, current deep neural network-based models remain highly susceptible to adversarial examples. The underlying safety concerns motivate us to investigate realistic adversarial attacks in AD scenarios. Previous work has demonstrated the feasibility of placing adversarial posters on the road surface to induce hallucinations in the detector. However, the unnatural appearance of the posters makes them easily noticeable by humans, and their fixed content can be readily targeted and defended. To address these limitations, we propose the AdvRoad to generate diverse road-style adversarial posters. The adversaries have naturalistic appearances resembling the road surface while compromising the detector to perceive non-existent objects at the attack locations. We employ a two-stage approach, termed Road-Style Adversary Generation and Scenario-Associated Adaptation, to maximize the attack effectiveness on the input scene while ensuring the natural appearance of the poster, allowing the attack to be carried out stealthily without drawing human attention. Extensive experiments show that AdvRoad generalizes well to different detectors, scenes, and spoofing locations. Moreover, physical attacks further demonstrate the practical threats in real-world environments.",
            "headline_zh": "提出AdvRoad方法生成道路风格对抗海报，以隐蔽攻击自动驾驶3D视觉检测系统。",
            "intro_zh": [
                "核心问题：现有对抗海报外观不自然，易被人类察觉且防御性强，威胁自动驾驶安全。",
                "方法要点：采用两阶段方法生成自然道路风格对抗海报，确保隐蔽性并诱导检测器误判。",
                "实验或效果：AdvRoad泛化性强，适用于多种检测器、场景和位置，物理实验验证现实威胁。"
            ],
            "tags_zh": [
                "自动驾驶安全",
                "对抗攻击",
                "3D视觉检测",
                "道路风格生成",
                "隐蔽攻击"
            ],
            "_index": 22
        },
        {
            "title": "Hardware-Aware YOLO Compression for Low-Power Edge AI on STM32U5 for Weeds Detection in Digital Agriculture",
            "authors": [
                "Charalampos S. Kouzinopoulos",
                "Yuri Manna"
            ],
            "arxiv_id": "2511.07990v1",
            "summary": "Weeds significantly reduce crop yields worldwide and pose major challenges to sustainable agriculture. Traditional weed management methods, primarily relying on chemical herbicides, risk environmental contamination and lead to the emergence of herbicide-resistant species. Precision weeding, leveraging computer vision and machine learning methods, offers a promising eco-friendly alternative but is often limited by reliance on high-power computational platforms. This work presents an optimized, low-power edge AI system for weeds detection based on the YOLOv8n object detector deployed on the STM32U575ZI microcontroller. Several compression techniques are applied to the detection model, including structured pruning, integer quantization and input image resolution scaling in order to meet strict hardware constraints. The model is trained and evaluated on the CropAndWeed dataset with 74 plant species, achieving a balanced trade-off between detection accuracy and efficiency. Our system supports real-time, in-situ weeds detection with a minimal energy consumption of 51.8mJ per inference, enabling scalable deployment in power-constrained agricultural environments.",
            "headline_zh": "提出硬件感知YOLO压缩方法，用于STM32U5低功耗杂草检测以解决农业边缘AI能效问题",
            "intro_zh": [
                "杂草降低作物产量，传统除草方法有环境风险，需低功耗精准检测方案",
                "应用结构化剪枝、整数量化和输入缩放压缩YOLOv8n，适配STM32U5微控制器",
                "在CropAndWeed数据集评估，实现51.8mJ/推理的低能耗，平衡检测精度与效率"
            ],
            "tags_zh": [
                "杂草检测",
                "YOLO压缩",
                "边缘AI",
                "低功耗优化",
                "STM32微控制器",
                "数字农业"
            ],
            "_index": 23
        },
        {
            "title": "Morphing Through Time: Diffusion-Based Bridging of Temporal Gaps for Robust Alignment in Change Detection",
            "authors": [
                "Seyedehanita Madani",
                "Vishal M. Patel"
            ],
            "arxiv_id": "2511.07976v1",
            "summary": "Remote sensing change detection is often challenged by spatial misalignment between bi-temporal images, especially when acquisitions are separated by long seasonal or multi-year gaps. While modern convolutional and transformer-based models perform well on aligned data, their reliance on precise co-registration limits their robustness in real-world conditions. Existing joint registration-detection frameworks typically require retraining and transfer poorly across domains. We introduce a modular pipeline that improves spatial and temporal robustness without altering existing change detection networks. The framework integrates diffusion-based semantic morphing, dense registration, and residual flow refinement. A diffusion module synthesizes intermediate morphing frames that bridge large appearance gaps, enabling RoMa to estimate stepwise correspondences between consecutive frames. The composed flow is then refined through a lightweight U-Net to produce a high-fidelity warp that co-registers the original image pair. Extensive experiments on LEVIR-CD, WHU-CD, and DSIFN-CD show consistent gains in both registration accuracy and downstream change detection across multiple backbones, demonstrating the generality and effectiveness of the proposed approach.",
            "headline_zh": "提出扩散式语义变形模块以解决遥感变化检测中的时空错位问题",
            "intro_zh": [
                "核心问题：遥感变化检测中，双时相图像因季节或多年间隔导致空间错位，影响检测鲁棒性。",
                "方法要点：集成扩散模型生成中间变形帧，结合密集配准和残差流优化，实现高保真图像配准。",
                "实验或效果：在多个数据集上验证，提升配准精度和变化检测性能，无需修改现有检测网络。"
            ],
            "tags_zh": [
                "遥感变化检测",
                "扩散模型",
                "图像配准",
                "时空鲁棒性",
                "模块化框架"
            ],
            "_index": 24
        },
        {
            "title": "Is It Truly Necessary to Process and Fit Minutes-Long Reference Videos for Personalized Talking Face Generation?",
            "authors": [
                "Rui-Qing Sun",
                "Ang Li",
                "Zhijing Wu",
                "Tian Lan",
                "Qianyu Lu",
                "Xingshan Yao",
                "Chen Xu",
                "Xian-Ling Mao"
            ],
            "arxiv_id": "2511.07940v1",
            "summary": "Talking Face Generation (TFG) aims to produce realistic and dynamic talking portraits, with broad applications in fields such as digital education, film and television production, e-commerce live streaming, and other related areas. Currently, TFG methods based on Neural Radiated Field (NeRF) or 3D Gaussian sputtering (3DGS) are received widespread attention. They learn and store personalized features from reference videos of each target individual to generate realistic speaking videos. To ensure models can capture sufficient 3D information and successfully learns the lip-audio mapping, previous studies usually require meticulous processing and fitting several minutes of reference video, which always takes hours. The computational burden of processing and fitting long reference videos severely limits the practical application value of these methods.However, is it really necessary to fit such minutes of reference video? Our exploratory case studies show that using some informative reference video segments of just a few seconds can achieve performance comparable to or even better than the full reference video. This indicates that video informative quality is much more important than its length. Inspired by this observation, we propose the ISExplore (short for Informative Segment Explore), a simple-yet-effective segment selection strategy that automatically identifies the informative 5-second reference video segment based on three key data quality dimensions: audio feature diversity, lip movement amplitude, and number of camera views. Extensive experiments demonstrate that our approach increases data processing and training speed by more than 5x for NeRF and 3DGS methods, while maintaining high-fidelity output. Project resources are available at xx.",
            "headline_zh": "提出ISExplore策略，通过选择信息丰富短视频段，提升个性化说话人脸生成效率。",
            "intro_zh": [
                "核心问题：传统方法需处理数分钟参考视频，计算负担大，限制实际应用。",
                "方法要点：基于音频多样性、唇动幅度和相机视角，自动选择5秒信息丰富视频段。",
                "实验或效果：在NeRF和3DGS方法中，数据处理和训练速度提升5倍以上，保持高保真输出。"
            ],
            "tags_zh": [
                "说话人脸生成",
                "参考视频选择",
                "信息丰富段",
                "NeRF方法",
                "3D高斯溅射",
                "效率优化"
            ],
            "_index": 25
        },
        {
            "title": "HD$^2$-SSC: High-Dimension High-Density Semantic Scene Completion for Autonomous Driving",
            "authors": [
                "Zhiwen Yang",
                "Yuxin Peng"
            ],
            "arxiv_id": "2511.07925v1",
            "summary": "Camera-based 3D semantic scene completion (SSC) plays a crucial role in autonomous driving, enabling voxelized 3D scene understanding for effective scene perception and decision-making. Existing SSC methods have shown efficacy in improving 3D scene representations, but suffer from the inherent input-output dimension gap and annotation-reality density gap, where the 2D planner view from input images with sparse annotated labels leads to inferior prediction of real-world dense occupancy with a 3D stereoscopic view. In light of this, we propose the corresponding High-Dimension High-Density Semantic Scene Completion (HD$^2$-SSC) framework with expanded pixel semantics and refined voxel occupancies. To bridge the dimension gap, a High-dimension Semantic Decoupling module is designed to expand 2D image features along a pseudo third dimension, decoupling coarse pixel semantics from occlusions, and then identify focal regions with fine semantics to enrich image features. To mitigate the density gap, a High-density Occupancy Refinement module is devised with a \"detect-and-refine\" architecture to leverage contextual geometric and semantic structures for enhanced semantic density with the completion of missing voxels and correction of erroneous ones. Extensive experiments and analyses on the SemanticKITTI and SSCBench-KITTI-360 datasets validate the effectiveness of our HD$^2$-SSC framework.",
            "headline_zh": "提出HD²-SSC框架以解决自动驾驶中语义场景完成的维度与密度差距问题",
            "intro_zh": [
                "核心问题：现有方法存在2D输入与3D输出维度差距及标注稀疏与真实密集的密度差距",
                "方法要点：设计高维语义解耦模块扩展2D特征，高密度占用细化模块检测并修正体素",
                "实验或效果：在SemanticKITTI和SSCBench-KITTI-360数据集上验证有效性"
            ],
            "tags_zh": [
                "语义场景完成",
                "自动驾驶",
                "3D场景理解",
                "体素化表示",
                "高维特征扩展",
                "密度优化"
            ],
            "_index": 26
        },
        {
            "title": "Exploring the Underwater World Segmentation without Extra Training",
            "authors": [
                "Bingyu Li",
                "Tao Huo",
                "Da Zhang",
                "Zhiyuan Zhao",
                "Junyu Gao",
                "Xuelong Li"
            ],
            "arxiv_id": "2511.07923v1",
            "summary": "Accurate segmentation of marine organisms is vital for biodiversity monitoring and ecological assessment, yet existing datasets and models remain largely limited to terrestrial scenes. To bridge this gap, we introduce \\textbf{AquaOV255}, the first large-scale and fine-grained underwater segmentation dataset containing 255 categories and over 20K images, covering diverse categories for open-vocabulary (OV) evaluation. Furthermore, we establish the first underwater OV segmentation benchmark, \\textbf{UOVSBench}, by integrating AquaOV255 with five additional underwater datasets to enable comprehensive evaluation. Alongside, we present \\textbf{Earth2Ocean}, a training-free OV segmentation framework that transfers terrestrial vision--language models (VLMs) to underwater domains without any additional underwater training. Earth2Ocean consists of two core components: a Geometric-guided Visual Mask Generator (\\textbf{GMG}) that refines visual features via self-similarity geometric priors for local structure perception, and a Category-visual Semantic Alignment (\\textbf{CSA}) module that enhances text embeddings through multimodal large language model reasoning and scene-aware template construction. Extensive experiments on the UOVSBench benchmark demonstrate that Earth2Ocean achieves significant performance improvement on average while maintaining efficient inference.",
            "headline_zh": "提出Earth2Ocean框架，实现无额外训练的水下开放词汇分割。",
            "intro_zh": [
                "核心问题：现有分割模型局限于陆地场景，缺乏水下生物准确分割。",
                "方法要点：通过几何引导视觉掩码生成和类别-视觉语义对齐模块，迁移视觉-语言模型。",
                "实验或效果：在UOVSBench基准上显著提升性能，保持高效推理。"
            ],
            "tags_zh": [
                "水下分割",
                "开放词汇分割",
                "视觉-语言模型",
                "训练免费框架",
                "几何引导",
                "语义对齐"
            ],
            "_index": 27
        },
        {
            "title": "Dual-MPC Footstep Planning for Robust Quadruped Locomotion",
            "authors": [
                "Byeong-Il Ham",
                "Hyun-Bin Kim",
                "Jeonguk Kang",
                "Keun Ha Choi",
                "Kyung-Soo Kim"
            ],
            "arxiv_id": "2511.07921v1",
            "summary": "In this paper, we propose a footstep planning strategy based on model predictive control (MPC) that enables robust regulation of body orientation against undesired body rotations by optimizing footstep placement. Model-based locomotion approaches typically adopt heuristic methods or planning based on the linear inverted pendulum model. These methods account for linear velocity in footstep planning, while excluding angular velocity, which leads to angular momentum being handled exclusively via ground reaction force (GRF). Footstep planning based on MPC that takes angular velocity into account recasts the angular momentum control problem as a dual-input approach that coordinates GRFs and footstep placement, instead of optimizing GRFs alone, thereby improving tracking performance. A mutual-feedback loop couples the footstep planner and the GRF MPC, with each using the other's solution to iteratively update footsteps and GRFs. The use of optimal solutions reduces body oscillation and enables extended stance and swing phases. The method is validated on a quadruped robot, demonstrating robust locomotion with reduced oscillations, longer stance and swing phases across various terrains.",
            "headline_zh": "提出双MPC脚步规划方法，以增强四足机器人对不期望身体旋转的鲁棒性。",
            "intro_zh": [
                "核心问题：传统方法忽略角速度，仅依赖地面反作用力控制角动量，导致性能不足。",
                "方法要点：结合模型预测控制优化脚步位置和地面反作用力，形成双输入协调控制。",
                "实验或效果：在四足机器人上验证，减少振荡，延长支撑和摆动相，适应多种地形。"
            ],
            "tags_zh": [
                "脚步规划",
                "模型预测控制",
                "四足机器人",
                "角动量控制",
                "鲁棒运动"
            ],
            "_index": 28
        },
        {
            "title": "Theoretical Analysis of Power-law Transformation on Images for Text Polarity Detection",
            "authors": [
                "Narendra Singh Yadav",
                "Pavan Kumar Perepu"
            ],
            "arxiv_id": "2511.07916v1",
            "summary": "Several computer vision applications like vehicle license plate recognition, captcha recognition, printed or handwriting character recognition from images etc., text polarity detection and binarization are the important preprocessing tasks. To analyze any image, it has to be converted to a simple binary image. This binarization process requires the knowledge of polarity of text in the images. Text polarity is defined as the contrast of text with respect to background. That means, text is darker than the background (dark text on bright background) or vice-versa. The binarization process uses this polarity information to convert the original colour or gray scale image into a binary image. In the literature, there is an intuitive approach based on power-law transformation on the original images. In this approach, the authors have illustrated an interesting phenomenon from the histogram statistics of the transformed images. Considering text and background as two classes, they have observed that maximum between-class variance between two classes is increasing (decreasing) for dark (bright) text on bright (dark) background. The corresponding empirical results have been presented. In this paper, we present a theoretical analysis of the above phenomenon.",
            "headline_zh": "理论分析幂律变换在图像文本极性检测中的现象，以改进二值化预处理。",
            "intro_zh": [
                "核心问题：图像文本极性检测是二值化预处理的关键，影响字符识别等应用。",
                "方法要点：基于幂律变换分析直方图统计，理论解释类间方差随文本极性变化的现象。",
                "实验或效果：通过实证结果验证理论，但具体性能指标未知。"
            ],
            "tags_zh": [
                "文本极性检测",
                "图像二值化",
                "幂律变换",
                "类间方差",
                "计算机视觉预处理"
            ],
            "_index": 29
        },
        {
            "title": "EquiMus: Energy-Equivalent Dynamic Modeling and Simulation of Musculoskeletal Robots Driven by Linear Elastic Actuators",
            "authors": [
                "Yinglei Zhu",
                "Xuguang Dong",
                "Qiyao Wang",
                "Qi Shao",
                "Fugui Xie",
                "Xinjun Liu",
                "Huichan Zhao"
            ],
            "arxiv_id": "2511.07887v1",
            "summary": "Dynamic modeling and control are critical for unleashing soft robots' potential, yet remain challenging due to their complex constitutive behaviors and real-world operating conditions. Bio-inspired musculoskeletal robots, which integrate rigid skeletons with soft actuators, combine high load-bearing capacity with inherent flexibility. Although actuation dynamics have been studied through experimental methods and surrogate models, accurate and effective modeling and simulation remain a significant challenge, especially for large-scale hybrid rigid--soft robots with continuously distributed mass, kinematic loops, and diverse motion modes. To address these challenges, we propose EquiMus, an energy-equivalent dynamic modeling framework and MuJoCo-based simulation for musculoskeletal rigid--soft hybrid robots with linear elastic actuators. The equivalence and effectiveness of the proposed approach are validated and examined through both simulations and real-world experiments on a bionic robotic leg. EquiMus further demonstrates its utility for downstream tasks, including controller design and learning-based control strategies.",
            "headline_zh": "提出EquiMus框架以解决肌肉骨骼机器人动态建模与仿真的挑战",
            "intro_zh": [
                "核心问题：软机器人动态建模因复杂本构行为和现实条件而困难",
                "方法要点：基于能量等效原理和MuJoCo仿真，处理刚柔混合机器人",
                "实验或效果：通过仿生腿实验验证等效性和有效性，支持控制器设计"
            ],
            "tags_zh": [
                "肌肉骨骼机器人",
                "动态建模",
                "能量等效",
                "刚柔混合系统",
                "线性弹性驱动器",
                "MuJoCo仿真"
            ],
            "_index": 30
        },
        {
            "title": "Occlusion-Aware Ground Target Search by a UAV in an Urban Environment",
            "authors": [
                "Collin Hague",
                "Artur Wolek"
            ],
            "arxiv_id": "2511.07822v1",
            "summary": "This paper considers the problem of searching for a point of interest (POI) moving along an urban road network with an uncrewed aerial vehicle (UAV). The UAV is modeled as a variable-speed Dubins vehicle with a line-of-sight sensor in an urban environment that may occlude the sensor's view of the POI. A search strategy is proposed that exploits a probabilistic visibility volume (VV) to plan its future motion with iterative deepening $A^\\ast$. The probabilistic VV is a time-varying three-dimensional representation of the sensing constraints for a particular distribution of the POI's state. To find the path most likely to view the POI, the planner uses a heuristic to optimistically estimate the probability of viewing the POI over a time horizon. The probabilistic VV is max-pooled to create a variable-timestep planner that reduces the search space and balances long-term and short-term planning. The proposed path planning method is compared to prior work with a Monte-Carlo simulation and is shown to outperform the baseline methods in cluttered environments when the UAV's sensor has a higher false alarm probability.",
            "headline_zh": "提出基于概率可见性体积的无人机路径规划方法，以解决城市环境中遮挡下的地面目标搜索问题。",
            "intro_zh": [
                "核心问题：无人机在城市道路网络中搜索移动兴趣点，传感器视线可能被遮挡。",
                "方法要点：使用概率可见性体积和迭代加深A*规划路径，优化目标发现概率。",
                "实验或效果：蒙特卡洛模拟显示，在高误报概率的杂乱环境中优于基线方法。"
            ],
            "tags_zh": [
                "无人机路径规划",
                "概率可见性体积",
                "迭代加深A*",
                "城市环境搜索",
                "遮挡感知"
            ],
            "_index": 31
        },
        {
            "title": "Human Motion Synthesis in 3D Scenes via Unified Scene Semantic Occupancy",
            "authors": [
                "Gong Jingyu",
                "Tong Kunkun",
                "Chen Zhuoran",
                "Yuan Chuanhan",
                "Chen Mingang",
                "Zhang Zhizhong",
                "Tan Xin",
                "Xie Yuan"
            ],
            "arxiv_id": "2511.07819v1",
            "summary": "Human motion synthesis in 3D scenes relies heavily on scene comprehension, while current methods focus mainly on scene structure but ignore the semantic understanding. In this paper, we propose a human motion synthesis framework that take an unified Scene Semantic Occupancy (SSO) for scene representation, termed SSOMotion. We design a bi-directional tri-plane decomposition to derive a compact version of the SSO, and scene semantics are mapped to an unified feature space via CLIP encoding and shared linear dimensionality reduction. Such strategy can derive the fine-grained scene semantic structures while significantly reduce redundant computations. We further take these scene hints and movement direction derived from instructions for motion control via frame-wise scene query. Extensive experiments and ablation studies conducted on cluttered scenes using ShapeNet furniture, as well as scanned scenes from PROX and Replica datasets, demonstrate its cutting-edge performance while validating its effectiveness and generalization ability. Code will be publicly available at https://github.com/jingyugong/SSOMotion.",
            "headline_zh": "提出SSOMotion框架，利用统一场景语义占据实现3D场景中人体运动合成。",
            "intro_zh": [
                "核心问题：现有方法依赖场景结构，但忽略语义理解，影响运动合成准确性。",
                "方法要点：采用双向三平面分解压缩场景语义占据，结合CLIP编码映射到统一特征空间。",
                "实验效果：在ShapeNet、PROX和Replica数据集上验证，性能领先且泛化能力强。"
            ],
            "tags_zh": [
                "人体运动合成",
                "3D场景理解",
                "场景语义占据",
                "CLIP编码",
                "双向三平面分解"
            ],
            "_index": 32
        },
        {
            "title": "PC-Diffusion: Aligning Diffusion Models with Human Preferences via Preference Classifier",
            "authors": [
                "Shaomeng Wang",
                "He Wang",
                "Xiaolu Wei",
                "Longquan Dai",
                "Jinhui Tang"
            ],
            "arxiv_id": "2511.07806v1",
            "summary": "Diffusion models have achieved remarkable success in conditional image generation, yet their outputs often remain misaligned with human preferences. To address this, recent work has applied Direct Preference Optimization (DPO) to diffusion models, yielding significant improvements.~However, DPO-like methods exhibit two key limitations: 1) High computational cost,due to the entire model fine-tuning; 2) Sensitivity to reference model quality}, due to its tendency to introduce instability and bias. To overcome these limitations, we propose a novel framework for human preference alignment in diffusion models (PC-Diffusion), using a lightweight, trainable Preference Classifier that directly models the relative preference between samples. By restricting preference learning to this classifier, PC-Diffusion decouples preference alignment from the generative model, eliminating the need for entire model fine-tuning and reference model reliance.~We further provide theoretical guarantees for PC-Diffusion:1) PC-Diffusion ensures that the preference-guided distributions are consistently propagated across timesteps. 2)The training objective of the preference classifier is equivalent to DPO, but does not require a reference model.3) The proposed preference-guided correction can progressively steer generation toward preference-aligned regions.~Empirical results show that PC-Diffusion achieves comparable preference consistency to DPO while significantly reducing training costs and enabling efficient and stable preference-guided generation.",
            "headline_zh": "提出PC-Diffusion框架，通过偏好分类器对齐扩散模型与人类偏好",
            "intro_zh": [
                "扩散模型输出常与人类偏好不一致，现有DPO方法计算成本高且依赖参考模型",
                "PC-Diffusion使用轻量偏好分类器直接建模样本偏好，无需全模型微调或参考模型",
                "实验显示PC-Diffusion在保持偏好一致性的同时显著降低训练成本，实现高效稳定生成"
            ],
            "tags_zh": [
                "扩散模型",
                "偏好对齐",
                "轻量分类器",
                "直接偏好优化",
                "图像生成",
                "计算效率"
            ],
            "_index": 33
        },
        {
            "title": "Beyond Randomness: Understand the Order of the Noise in Diffusion",
            "authors": [
                "Song Yan",
                "Min Li",
                "Bi Xinliang",
                "Jian Yang",
                "Yusen Zhang",
                "Guanye Xiong",
                "Yunwei Lan",
                "Tao Zhang",
                "Wei Zhai",
                "Zheng-Jun Zha"
            ],
            "arxiv_id": "2511.07756v1",
            "summary": "In text-driven content generation (T2C) diffusion model, semantic of generated content is mostly attributed to the process of text embedding and attention mechanism interaction. The initial noise of the generation process is typically characterized as a random element that contributes to the diversity of the generated content. Contrary to this view, this paper reveals that beneath the random surface of noise lies strong analyzable patterns. Specifically, this paper first conducts a comprehensive analysis of the impact of random noise on the model's generation. We found that noise not only contains rich semantic information, but also allows for the erasure of unwanted semantics from it in an extremely simple way based on information theory, and using the equivalence between the generation process of diffusion model and semantic injection to inject semantics into the cleaned noise. Then, we mathematically decipher these observations and propose a simple but efficient training-free and universal two-step \"Semantic Erasure-Injection\" process to modulate the initial noise in T2C diffusion model. Experimental results demonstrate that our method is consistently effective across various T2C models based on both DiT and UNet architectures and presents a novel perspective for optimizing the generation of diffusion model, providing a universal tool for consistent generation.",
            "headline_zh": "提出语义擦除-注入方法以优化文本驱动扩散模型的生成过程",
            "intro_zh": [
                "核心问题：初始噪声在扩散模型中通常被视为随机，但实际包含可分析语义模式",
                "方法要点：基于信息理论，通过两步语义擦除与注入过程调制噪声，无需训练",
                "实验或效果：在DiT和UNet架构的多种模型中一致有效，提升生成一致性"
            ],
            "tags_zh": [
                "扩散模型",
                "文本驱动生成",
                "噪声分析",
                "语义调制",
                "训练免费方法"
            ],
            "_index": 34
        },
        {
            "title": "Filtered-ViT: A Robust Defense Against Multiple Adversarial Patch Attacks",
            "authors": [
                "Aja Khanal",
                "Ahmed Faid",
                "Apurva Narayan"
            ],
            "arxiv_id": "2511.07755v1",
            "summary": "Deep learning vision systems are increasingly deployed in safety-critical domains such as healthcare, yet they remain vulnerable to small adversarial patches that can trigger misclassifications. Most existing defenses assume a single patch and fail when multiple localized disruptions occur, the type of scenario adversaries and real-world artifacts often exploit. We propose Filtered-ViT, a new vision transformer architecture that integrates SMART Vector Median Filtering (SMART-VMF), a spatially adaptive, multi-scale, robustness-aware mechanism that enables selective suppression of corrupted regions while preserving semantic detail. On ImageNet with LaVAN multi-patch attacks, Filtered-ViT achieves 79.8% clean accuracy and 46.3% robust accuracy under four simultaneous 1\\% patches, outperforming existing defenses. Beyond synthetic benchmarks, a real-world case study on radiographic medical imagery shows that Filtered-ViT mitigates natural artifacts such as occlusions and scanner noise without degrading diagnostic content. This establishes Filtered-ViT as the first transformer to demonstrate unified robustness against both adversarial and naturally occurring patch-like disruptions, charting a path toward reliable vision systems in truly high-stakes environments.",
            "headline_zh": "提出Filtered-ViT以防御多对抗补丁攻击，提升安全关键领域视觉系统鲁棒性",
            "intro_zh": [
                "深度学习视觉系统易受多对抗补丁攻击，现有防御方法在多重局部干扰下失效",
                "集成SMART-VMF机制，实现空间自适应多尺度过滤，选择性抑制损坏区域并保留语义细节",
                "在ImageNet多补丁攻击下，鲁棒准确率达46.3%，并在医疗影像中有效缓解自然伪影"
            ],
            "tags_zh": [
                "对抗补丁防御",
                "视觉Transformer",
                "多尺度过滤",
                "鲁棒性增强",
                "医疗影像分析"
            ],
            "_index": 35
        },
        {
            "title": "Auto-US: An Ultrasound Video Diagnosis Agent Using Video Classification Framework and LLMs",
            "authors": [
                "Yuezhe Yang",
                "Yiyue Guo",
                "Wenjie Cai",
                "Qingqing Ruan",
                "Siying Wang",
                "Xingbo Dong",
                "Zhe Jin",
                "Yong Dai"
            ],
            "arxiv_id": "2511.07748v1",
            "summary": "AI-assisted ultrasound video diagnosis presents new opportunities to enhance the efficiency and accuracy of medical imaging analysis. However, existing research remains limited in terms of dataset diversity, diagnostic performance, and clinical applicability. In this study, we propose \\textbf{Auto-US}, an intelligent diagnosis agent that integrates ultrasound video data with clinical diagnostic text. To support this, we constructed \\textbf{CUV Dataset} of 495 ultrasound videos spanning five categories and three organs, aggregated from multiple open-access sources. We developed \\textbf{CTU-Net}, which achieves state-of-the-art performance in ultrasound video classification, reaching an accuracy of 86.73\\% Furthermore, by incorporating large language models, Auto-US is capable of generating clinically meaningful diagnostic suggestions. The final diagnostic scores for each case exceeded 3 out of 5 and were validated by professional clinicians. These results demonstrate the effectiveness and clinical potential of Auto-US in real-world ultrasound applications. Code and data are available at: https://github.com/Bean-Young/Auto-US.",
            "headline_zh": "提出Auto-US智能诊断代理，集成超声视频与临床文本以提升医疗影像分析效率。",
            "intro_zh": [
                "核心问题：现有AI辅助超声视频诊断在数据集多样性、性能和临床适用性方面存在局限。",
                "方法要点：开发CTU-Net实现超声视频分类，并整合大语言模型生成诊断建议。",
                "实验或效果：在CUV数据集上准确率达86.73%，诊断评分超3/5，经临床验证有效。"
            ],
            "tags_zh": [
                "超声视频诊断",
                "视频分类框架",
                "大语言模型",
                "临床文本集成",
                "智能诊断代理"
            ],
            "_index": 36
        },
        {
            "title": "UltraGS: Gaussian Splatting for Ultrasound Novel View Synthesis",
            "authors": [
                "Yuezhe Yang",
                "Wenjie Cai",
                "Dexin Yang",
                "Yufang Dong",
                "Xingbo Dong",
                "Zhe Jin"
            ],
            "arxiv_id": "2511.07743v1",
            "summary": "Ultrasound imaging is a cornerstone of non-invasive clinical diagnostics, yet its limited field of view complicates novel view synthesis. We propose \\textbf{UltraGS}, a Gaussian Splatting framework optimized for ultrasound imaging. First, we introduce a depth-aware Gaussian splatting strategy, where each Gaussian is assigned a learnable field of view, enabling accurate depth prediction and precise structural representation. Second, we design SH-DARS, a lightweight rendering function combining low-order spherical harmonics with ultrasound-specific wave physics, including depth attenuation, reflection, and scattering, to model tissue intensity accurately. Third, we contribute the Clinical Ultrasound Examination Dataset, a benchmark capturing diverse anatomical scans under real-world clinical protocols. Extensive experiments on three datasets demonstrate UltraGS's superiority, achieving state-of-the-art results in PSNR (up to 29.55), SSIM (up to 0.89), and MSE (as low as 0.002) while enabling real-time synthesis at 64.69 fps. The code and dataset are open-sourced at: https://github.com/Bean-Young/UltraGS.",
            "headline_zh": "提出UltraGS框架以解决超声成像新视角合成问题",
            "intro_zh": [
                "超声成像视野有限，新视角合成困难",
                "采用深度感知高斯溅射和SH-DARS渲染函数，结合超声物理建模",
                "在多个数据集上实现SOTA性能，PSNR达29.55，实时合成64.69 fps"
            ],
            "tags_zh": [
                "高斯溅射",
                "超声成像",
                "新视角合成",
                "深度预测",
                "实时渲染",
                "临床数据集"
            ],
            "_index": 37
        },
        {
            "title": "SeFA-Policy: Fast and Accurate Visuomotor Policy Learning with Selective Flow Alignment",
            "authors": [
                "Rong Xue",
                "Jiageng Mao",
                "Mingtong Zhang",
                "Yue Wang"
            ],
            "arxiv_id": "2511.08583v1",
            "summary": "Developing efficient and accurate visuomotor policies poses a central challenge in robotic imitation learning. While recent rectified flow approaches have advanced visuomotor policy learning, they suffer from a key limitation: After iterative distillation, generated actions may deviate from the ground-truth actions corresponding to the current visual observation, leading to accumulated error as the reflow process repeats and unstable task execution. We present Selective Flow Alignment (SeFA), an efficient and accurate visuomotor policy learning framework. SeFA resolves this challenge by a selective flow alignment strategy, which leverages expert demonstrations to selectively correct generated actions and restore consistency with observations, while preserving multimodality. This design introduces a consistency correction mechanism that ensures generated actions remain observation-aligned without sacrificing the efficiency of one-step flow inference. Extensive experiments across both simulated and real-world manipulation tasks show that SeFA Policy surpasses state-of-the-art diffusion-based and flow-based policies, achieving superior accuracy and robustness while reducing inference latency by over 98%. By unifying rectified flow efficiency with observation-consistent action generation, SeFA provides a scalable and dependable solution for real-time visuomotor policy learning. Code is available on https://github.com/RongXueZoe/SeFA.",
            "headline_zh": "提出选择性流对齐以解决视觉运动策略学习中的动作偏差问题",
            "intro_zh": [
                "核心问题：整流流方法在迭代蒸馏后，生成动作偏离当前视觉观察，导致累积误差和不稳定执行",
                "方法要点：利用专家演示选择性校正生成动作，保持观察一致性和多模态性，同时支持一步推理",
                "实验或效果：在模拟和真实操作任务中超越现有方法，提高准确性和鲁棒性，推理延迟降低超98%"
            ],
            "tags_zh": [
                "视觉运动策略学习",
                "选择性流对齐",
                "整流流方法",
                "动作一致性校正",
                "机器人模仿学习"
            ],
            "_index": 38
        },
        {
            "title": "Vision Transformer Based User Equipment Positioning",
            "authors": [
                "Parshwa Shah",
                "Dhaval K. Patel",
                "Brijesh Soni",
                "Miguel López-Benítez",
                "Siddhartan Govindasamy"
            ],
            "arxiv_id": "2511.08549v1",
            "summary": "Recently, Deep Learning (DL) techniques have been used for User Equipment (UE) positioning. However, the key shortcomings of such models is that: i) they weigh the same attention to the entire input; ii) they are not well suited for the non-sequential data e.g., when only instantaneous Channel State Information (CSI) is available. In this context, we propose an attention-based Vision Transformer (ViT) architecture that focuses on the Angle Delay Profile (ADP) from CSI matrix. Our approach, validated on the `DeepMIMO' and `ViWi' ray-tracing datasets, achieves an Root Mean Squared Error (RMSE) of 0.55m indoors, 13.59m outdoors in DeepMIMO, and 3.45m in ViWi's outdoor blockage scenario. The proposed scheme outperforms state-of-the-art schemes by $\\sim$ 38\\%. It also performs substantially better than other approaches that we have considered in terms of the distribution of error distance.",
            "headline_zh": "提出基于视觉变换器的用户设备定位方法，利用角度延迟配置文件提升精度。",
            "intro_zh": [
                "现有深度学习模型对输入数据分配相同注意力，且不适用于非序列数据如瞬时信道状态信息。",
                "采用注意力机制的视觉变换器架构，聚焦信道状态信息矩阵中的角度延迟配置文件。",
                "在DeepMIMO和ViWi数据集上验证，室内外定位误差显著降低，优于现有方法约38%。"
            ],
            "tags_zh": [
                "用户设备定位",
                "视觉变换器",
                "信道状态信息",
                "角度延迟配置文件",
                "深度学习",
                "定位误差"
            ],
            "_index": 39
        },
        {
            "title": "LeJEPA: Provable and Scalable Self-Supervised Learning Without the Heuristics",
            "authors": [
                "Randall Balestriero",
                "Yann LeCun"
            ],
            "arxiv_id": "2511.08544v1",
            "summary": "Learning manipulable representations of the world and its dynamics is central to AI. Joint-Embedding Predictive Architectures (JEPAs) offer a promising blueprint, but lack of practical guidance and theory has led to ad-hoc R&D. We present a comprehensive theory of JEPAs and instantiate it in {\\bf LeJEPA}, a lean, scalable, and theoretically grounded training objective. First, we identify the isotropic Gaussian as the optimal distribution that JEPAs' embeddings should follow to minimize downstream prediction risk. Second, we introduce a novel objective--{\\bf Sketched Isotropic Gaussian Regularization} (SIGReg)--to constrain embeddings to reach that ideal distribution. Combining the JEPA predictive loss with SIGReg yields LeJEPA with numerous theoretical and practical benefits: (i) single trade-off hyperparameter, (ii) linear time and memory complexity, (iii) stability across hyper-parameters, architectures (ResNets, ViTs, ConvNets) and domains, (iv) heuristics-free, e.g., no stop-gradient, no teacher-student, no hyper-parameter schedulers, and (v) distributed training-friendly implementation requiring only $\\approx$50 lines of code. Our empirical validation covers 10+ datasets, 60+ architectures, all with varying scales and domains. As an example, using imagenet-1k for pretraining and linear evaluation with frozen backbone, LeJEPA reaches 79\\% with a ViT-H/14. We hope that the simplicity and theory-friendly ecosystem offered by LeJEPA will reestablish self-supervised pre-training as a core pillar of AI research (\\href{git@github.com:rbalestr-lab/lejepa.git}{GitHub repo}).",
            "headline_zh": "提出LeJEPA以提供可证明且可扩展的自监督学习，无需启发式方法。",
            "intro_zh": [
                "核心问题：JEPA缺乏理论指导，导致研发依赖启发式方法。",
                "方法要点：结合预测损失与SIGReg，约束嵌入分布为各向同性高斯。",
                "实验或效果：在ImageNet-1k上，ViT-H/14线性评估准确率达79%。"
            ],
            "tags_zh": [
                "自监督学习",
                "联合嵌入预测架构",
                "各向同性高斯分布",
                "可扩展训练",
                "理论指导",
                "图像识别"
            ],
            "_index": 40
        },
        {
            "title": "CleverBirds: A Multiple-Choice Benchmark for Fine-grained Human Knowledge Tracing",
            "authors": [
                "Leonie Bossemeyer",
                "Samuel Heinrich",
                "Grant Van Horn",
                "Oisin Mac Aodha"
            ],
            "arxiv_id": "2511.08512v1",
            "summary": "Mastering fine-grained visual recognition, essential in many expert domains, can require that specialists undergo years of dedicated training. Modeling the progression of such expertize in humans remains challenging, and accurately inferring a human learner's knowledge state is a key step toward understanding visual learning. We introduce CleverBirds, a large-scale knowledge tracing benchmark for fine-grained bird species recognition. Collected by the citizen-science platform eBird, it offers insight into how individuals acquire expertize in complex fine-grained classification. More than 40,000 participants have engaged in the quiz, answering over 17 million multiple-choice questions spanning over 10,000 bird species, with long-range learning patterns across an average of 400 questions per participant. We release this dataset to support the development and evaluation of new methods for visual knowledge tracing. We show that tracking learners' knowledge is challenging, especially across participant subgroups and question types, with different forms of contextual information offering varying degrees of predictive benefit. CleverBirds is among the largest benchmark of its kind, offering a substantially higher number of learnable concepts. With it, we hope to enable new avenues for studying the development of visual expertize over time and across individuals.",
            "headline_zh": "提出CleverBirds基准以支持细粒度视觉知识追踪研究",
            "intro_zh": [
                "核心问题：建模人类在细粒度视觉识别中的知识状态进展具有挑战性",
                "方法要点：基于eBird平台构建大规模多选问答数据集，覆盖超万种鸟类",
                "实验或效果：追踪学习者知识困难，不同上下文信息预测效果各异"
            ],
            "tags_zh": [
                "知识追踪",
                "细粒度视觉识别",
                "多选问答基准",
                "鸟类分类",
                "公民科学数据"
            ],
            "_index": 41
        },
        {
            "title": "ReIDMamba: Learning Discriminative Features with Visual State Space Model for Person Re-Identification",
            "authors": [
                "Hongyang Gu",
                "Qisong Yang",
                "Lei Pu",
                "Siming Han",
                "Yao Ding"
            ],
            "arxiv_id": "2511.07948v1",
            "summary": "Extracting robust discriminative features is a critical challenge in person re-identification (ReID). While Transformer-based methods have successfully addressed some limitations of convolutional neural networks (CNNs), such as their local processing nature and information loss resulting from convolution and downsampling operations, they still face the scalability issue due to the quadratic increase in memory and computational requirements with the length of the input sequence. To overcome this, we propose a pure Mamba-based person ReID framework named ReIDMamba. Specifically, we have designed a Mamba-based strong baseline that effectively leverages fine-grained, discriminative global features by introducing multiple class tokens. To further enhance robust features learning within Mamba, we have carefully designed two novel techniques. First, the multi-granularity feature extractor (MGFE) module, designed with a multi-branch architecture and class token fusion, effectively forms multi-granularity features, enhancing both discrimination ability and fine-grained coverage. Second, the ranking-aware triplet regularization (RATR) is introduced to reduce redundancy in features from multiple branches, enhancing the diversity of multi-granularity features by incorporating both intra-class and inter-class diversity constraints, thus ensuring the robustness of person features. To our knowledge, this is the pioneering work that integrates a purely Mamba-driven approach into ReID research. Our proposed ReIDMamba model boasts only one-third the parameters of TransReID, along with lower GPU memory usage and faster inference throughput. Experimental results demonstrate ReIDMamba's superior and promising performance, achieving state-of-the-art performance on five person ReID benchmarks. Code is available at https://github.com/GuHY777/ReIDMamba.",
            "headline_zh": "提出ReIDMamba框架，利用Mamba模型解决行人重识别中特征提取的扩展性问题",
            "intro_zh": [
                "核心问题：Transformer在行人重识别中面临内存和计算复杂度随序列长度二次增长的问题",
                "方法要点：设计Mamba基线，引入多类令牌和多粒度特征提取器，增强特征判别性和鲁棒性",
                "实验或效果：在五个基准测试中达到SOTA，参数减少三分之二，推理速度更快"
            ],
            "tags_zh": [
                "行人重识别",
                "Mamba模型",
                "多粒度特征",
                "特征提取",
                "状态空间模型"
            ],
            "_index": 42
        },
        {
            "title": "Libra-MIL: Multimodal Prototypes Stereoscopic Infused with Task-specific Language Priors for Few-shot Whole Slide Image Classification",
            "authors": [
                "Zhenfeng Zhuang",
                "Fangyu Zhou",
                "Liansheng Wang"
            ],
            "arxiv_id": "2511.07941v1",
            "summary": "While Large Language Models (LLMs) are emerging as a promising direction in computational pathology, the substantial computational cost of giga-pixel Whole Slide Images (WSIs) necessitates the use of Multi-Instance Learning (MIL) to enable effective modeling. A key challenge is that pathological tasks typically provide only bag-level labels, while instance-level descriptions generated by LLMs often suffer from bias due to a lack of fine-grained medical knowledge. To address this, we propose that constructing task-specific pathological entity prototypes is crucial for learning generalizable features and enhancing model interpretability. Furthermore, existing vision-language MIL methods often employ unidirectional guidance, limiting cross-modal synergy. In this paper, we introduce a novel approach, Multimodal Prototype-based Multi-Instance Learning, that promotes bidirectional interaction through a balanced information compression scheme. Specifically, we leverage a frozen LLM to generate task-specific pathological entity descriptions, which are learned as text prototypes. Concurrently, the vision branch learns instance-level prototypes to mitigate the model's reliance on redundant data. For the fusion stage, we employ the Stereoscopic Optimal Transport (SOT) algorithm, which is based on a similarity metric, thereby facilitating broader semantic alignment in a higher-dimensional space. We conduct few-shot classification and explainability experiments on three distinct cancer datasets, and the results demonstrate the superior generalization capabilities of our proposed method.",
            "headline_zh": "提出多模态原型多示例学习方法，以解决少样本全切片图像分类中的跨模态交互问题。",
            "intro_zh": [
                "核心问题：全切片图像计算成本高，且病理任务仅有袋级标签，语言模型生成实例描述存在偏差。",
                "方法要点：构建任务特定病理实体原型，通过立体最优传输算法促进视觉与文本双向交互。",
                "实验或效果：在多个癌症数据集上验证，方法在少样本分类和可解释性方面表现优越。"
            ],
            "tags_zh": [
                "多示例学习",
                "多模态原型",
                "立体最优传输",
                "少样本分类",
                "全切片图像",
                "病理实体描述"
            ],
            "_index": 43
        },
        {
            "title": "DiffRegCD: Integrated Registration and Change Detection with Diffusion Features",
            "authors": [
                "Seyedehnanita Madani",
                "Rama Chellappa",
                "Vishal M. Patel"
            ],
            "arxiv_id": "2511.07935v1",
            "summary": "Change detection (CD) is fundamental to computer vision and remote sensing, supporting applications in environmental monitoring, disaster response, and urban development. Most CD models assume co-registered inputs, yet real-world imagery often exhibits parallax, viewpoint shifts, and long temporal gaps that cause severe misalignment. Traditional two stage methods that first register and then detect, as well as recent joint frameworks (e.g., BiFA, ChangeRD), still struggle under large displacements, relying on regression only flow, global homographies, or synthetic perturbations. We present DiffRegCD, an integrated framework that unifies dense registration and change detection in a single model. DiffRegCD reformulates correspondence estimation as a Gaussian smoothed classification task, achieving sub-pixel accuracy and stable training. It leverages frozen multi-scale features from a pretrained denoising diffusion model, ensuring robustness to illumination and viewpoint variation. Supervision is provided through controlled affine perturbations applied to standard CD datasets, yielding paired ground truth for both flow and change detection without pseudo labels. Extensive experiments on aerial (LEVIR-CD, DSIFN-CD, WHU-CD, SYSU-CD) and ground level (VL-CMU-CD) datasets show that DiffRegCD consistently surpasses recent baselines and remains reliable under wide temporal and geometric variation, establishing diffusion features and classification based correspondence as a strong foundation for unified change detection.",
            "headline_zh": "提出DiffRegCD框架，集成密集配准与变化检测，解决大位移图像变化检测问题。",
            "intro_zh": [
                "核心问题：真实图像存在视差和视角偏移，导致配准不准，影响变化检测。",
                "方法要点：将配准建模为高斯平滑分类任务，利用预训练扩散模型特征提升鲁棒性。",
                "实验效果：在多个数据集上超越基线，对广泛时空和几何变化保持可靠。"
            ],
            "tags_zh": [
                "变化检测",
                "图像配准",
                "扩散模型",
                "高斯平滑分类",
                "遥感图像分析",
                "集成学习"
            ],
            "_index": 44
        },
        {
            "title": "Statistically Assuring Safety of Control Systems using Ensembles of Safety Filters and Conformal Prediction",
            "authors": [
                "Ihab Tabbara",
                "Yuxuan Yang",
                "Hussein Sibai"
            ],
            "arxiv_id": "2511.07899v1",
            "summary": "Safety assurance is a fundamental requirement for deploying learning-enabled autonomous systems. Hamilton-Jacobi (HJ) reachability analysis is a fundamental method for formally verifying safety and generating safe controllers. However, computing the HJ value function that characterizes the backward reachable set (BRS) of a set of user-defined failure states is computationally expensive, especially for high-dimensional systems, motivating the use of reinforcement learning approaches to approximate the value function. Unfortunately, a learned value function and its corresponding safe policy are not guaranteed to be correct. The learned value function evaluated at a given state may not be equal to the actual safety return achieved by following the learned safe policy. To address this challenge, we introduce a conformal prediction-based (CP) framework that bounds such uncertainty. We leverage CP to provide probabilistic safety guarantees when using learned HJ value functions and policies to prevent control systems from reaching failure states. Specifically, we use CP to calibrate the switching between the unsafe nominal controller and the learned HJ-based safe policy and to derive safety guarantees under this switched policy. We also investigate using an ensemble of independently trained HJ value functions as a safety filter and compare this ensemble approach to using individual value functions alone.",
            "headline_zh": "提出基于保形预测和集成安全过滤器的框架，为学习型控制系统提供概率安全保证",
            "intro_zh": [
                "核心问题：学习型Hamilton-Jacobi值函数及其安全策略无法保证正确性，存在不确定性。",
                "方法要点：利用保形预测校准不安全控制器与学习安全策略的切换，提供概率安全边界。",
                "实验或效果：比较集成HJ值函数与单一值函数作为安全过滤器的性能，未知具体结果。"
            ],
            "tags_zh": [
                "安全保证",
                "保形预测",
                "Hamilton-Jacobi可达性分析",
                "强化学习",
                "控制系统",
                "集成学习"
            ],
            "_index": 45
        },
        {
            "title": "A Comprehensive Experimental Characterization of Mechanical Layer Jamming Systems",
            "authors": [
                "Jessica Gumowski",
                "Krishna Manaswi Digumarti",
                "David Howard"
            ],
            "arxiv_id": "2511.07882v1",
            "summary": "Organisms in nature, such as Cephalopods and Pachyderms, exploit stiffness modulation to achieve amazing dexterity in the control of their appendages. In this paper, we explore the phenomenon of layer jamming, which is a popular stiffness modulation mechanism that provides an equivalent capability for soft robots. More specifically, we focus on mechanical layer jamming, which we realise through two-layer multi material structure with tooth-like protrusions. We identify key design parameters for mechanical layer jamming systems, including the ability to modulate stiffness, and perform a variety of comprehensive tests placing the specimens under bending and torsional loads to understand the influence of our selected design parameters (mainly tooth geometry) on the performance of the jammed structures. We note the ability of these structures to produce a peak change in stiffness of 5 times in bending and 3.2 times in torsion. We also measure the force required to separate the two jammed layers, an often ignored parameter in the study of jamming-induced stiffness change. This study aims to shed light on the principled design of mechanical layer jammed systems and guide researchers in the selection of appropriate designs for their specific application domains.",
            "headline_zh": "提出机械层卡滞系统设计参数实验表征，以指导软机器人刚度调制应用",
            "intro_zh": [
                "核心问题：软机器人如何通过层卡滞机制实现刚度调制，模仿生物体如头足类动物。",
                "方法要点：采用双层多材料结构，带齿状突起，研究齿几何等关键设计参数。",
                "实验或效果：弯曲刚度峰值提升5倍，扭转刚度提升3.2倍，并测量层分离力。"
            ],
            "tags_zh": [
                "机械层卡滞",
                "刚度调制",
                "软机器人",
                "实验表征",
                "设计参数"
            ],
            "_index": 46
        },
        {
            "title": "CloudMamba: Grouped Selective State Spaces for Point Cloud Analysis",
            "authors": [
                "Kanglin Qu",
                "Pan Gao",
                "Qun Dai",
                "Zhanzhi Ye",
                "Rui Ye",
                "Yuanhao Sun"
            ],
            "arxiv_id": "2511.07823v1",
            "summary": "Due to the long-range modeling ability and linear complexity property, Mamba has attracted considerable attention in point cloud analysis. Despite some interesting progress, related work still suffers from imperfect point cloud serialization, insufficient high-level geometric perception, and overfitting of the selective state space model (S6) at the core of Mamba. To this end, we resort to an SSM-based point cloud network termed CloudMamba to address the above challenges. Specifically, we propose sequence expanding and sequence merging, where the former serializes points along each axis separately and the latter serves to fuse the corresponding higher-order features causally inferred from different sequences, enabling unordered point sets to adapt more stably to the causal nature of Mamba without parameters. Meanwhile, we design chainedMamba that chains the forward and backward processes in the parallel bidirectional Mamba, capturing high-level geometric information during scanning. In addition, we propose a grouped selective state space model (GS6) via parameter sharing on S6, alleviating the overfitting problem caused by the computational mode in S6. Experiments on various point cloud tasks validate CloudMamba's ability to achieve state-of-the-art results with significantly less complexity.",
            "headline_zh": "提出CloudMamba以解决点云分析中的序列化、几何感知和过拟合问题",
            "intro_zh": [
                "核心问题：点云序列化不完善、高层几何感知不足、S6模型过拟合",
                "方法要点：序列扩展与合并、链式Mamba、分组选择性状态空间模型",
                "实验或效果：在多个任务中实现SOTA结果，复杂度显著降低"
            ],
            "tags_zh": [
                "点云分析",
                "选择性状态空间模型",
                "序列化方法",
                "几何感知",
                "过拟合缓解",
                "线性复杂度"
            ],
            "_index": 47
        },
        {
            "title": "Cancer-Net PCa-MultiSeg: Multimodal Enhancement of Prostate Cancer Lesion Segmentation Using Synthetic Correlated Diffusion Imaging",
            "authors": [
                "Jarett Dewbury",
                "Chi-en Amy Tai",
                "Alexander Wong"
            ],
            "arxiv_id": "2511.07816v1",
            "summary": "Current deep learning approaches for prostate cancer lesion segmentation achieve limited performance, with Dice scores of 0.32 or lower in large patient cohorts. To address this limitation, we investigate synthetic correlated diffusion imaging (CDI$^s$) as an enhancement to standard diffusion-based protocols. We conduct a comprehensive evaluation across six state-of-the-art segmentation architectures using 200 patients with co-registered CDI$^s$, diffusion-weighted imaging (DWI) and apparent diffusion coefficient (ADC) sequences. We demonstrate that CDI$^s$ integration reliably enhances or preserves segmentation performance in 94% of evaluated configurations, with individual architectures achieving up to 72.5% statistically significant relative improvement over baseline modalities. CDI$^s$ + DWI emerges as the safest enhancement pathway, achieving significant improvements in half of evaluated architectures with zero instances of degradation. Since CDI$^s$ derives from existing DWI acquisitions without requiring additional scan time or architectural modifications, it enables immediate deployment in clinical workflows. Our results establish validated integration pathways for CDI$^s$ as a practical drop-in enhancement for PCa lesion segmentation tasks across diverse deep learning architectures.",
            "headline_zh": "提出合成相关扩散成像以增强前列腺癌病灶分割性能",
            "intro_zh": [
                "当前深度学习前列腺癌病灶分割性能有限，Dice分数低至0.32或以下",
                "使用合成相关扩散成像增强标准扩散协议，无需额外扫描或架构修改",
                "在200患者数据上评估，94%配置性能提升，相对改进最高达72.5%"
            ],
            "tags_zh": [
                "前列腺癌分割",
                "合成相关扩散成像",
                "多模态增强",
                "深度学习架构",
                "临床部署"
            ],
            "_index": 48
        },
        {
            "title": "Sparse3DPR: Training-Free 3D Hierarchical Scene Parsing and Task-Adaptive Subgraph Reasoning from Sparse RGB Views",
            "authors": [
                "Haida Feng",
                "Hao Wei",
                "Zewen Xu",
                "Haolin Wang",
                "Chade Li",
                "Yihong Wu"
            ],
            "arxiv_id": "2511.07813v1",
            "summary": "Recently, large language models (LLMs) have been explored widely for 3D scene understanding. Among them, training-free approaches are gaining attention for their flexibility and generalization over training-based methods. However, they typically struggle with accuracy and efficiency in practical deployment. To address the problems, we propose Sparse3DPR, a novel training-free framework for open-ended scene understanding, which leverages the reasoning capabilities of pre-trained LLMs and requires only sparse-view RGB inputs. Specifically, we introduce a hierarchical plane-enhanced scene graph that supports open vocabulary and adopts dominant planar structures as spatial anchors, which enables clearer reasoning chains and more reliable high-level inferences. Furthermore, we design a task-adaptive subgraph extraction method to filter query-irrelevant information dynamically, reducing contextual noise and improving 3D scene reasoning efficiency and accuracy. Experimental results demonstrate the superiority of Sparse3DPR, which achieves a 28.7% EM@1 improvement and a 78.2% speedup compared with ConceptGraphs on the Space3D-Bench. Moreover, Sparse3DPR obtains comparable performance to training-based methods on ScanQA, with additional real-world experiments confirming its robustness and generalization capability.",
            "headline_zh": "提出Sparse3DPR训练无关框架，利用稀疏RGB视图实现3D场景解析与推理",
            "intro_zh": [
                "核心问题：训练无关3D场景理解方法在精度和效率上存在不足",
                "方法要点：引入层次化平面增强场景图和任务自适应子图提取",
                "实验或效果：在Space3D-Bench上精度提升28.7%，速度提升78.2%"
            ],
            "tags_zh": [
                "3D场景理解",
                "训练无关方法",
                "场景图推理",
                "稀疏视图输入",
                "层次化解析"
            ],
            "_index": 49
        },
        {
            "title": "Revisiting MLLM Based Image Quality Assessment: Errors and Remedy",
            "authors": [
                "Zhenchen Tang",
                "Songlin Yang",
                "Bo Peng",
                "Zichuan Wang",
                "Jing Dong"
            ],
            "arxiv_id": "2511.07812v1",
            "summary": "The rapid progress of multi-modal large language models (MLLMs) has boosted the task of image quality assessment (IQA). However, a key challenge arises from the inherent mismatch between the discrete token outputs of MLLMs and the continuous nature of quality scores required by IQA tasks. This discrepancy significantly hinders the performance of MLLM-based IQA methods. Previous approaches that convert discrete token predictions into continuous scores often suffer from conversion errors. Moreover, the semantic confusion introduced by level tokens (e.g., ``good'') further constrains the performance of MLLMs on IQA tasks and degrades their original capabilities for related tasks. To tackle these problems, we provide a theoretical analysis of the errors inherent in previous approaches and, motivated by this analysis, propose a simple yet effective framework, Q-Scorer. This framework incorporates a lightweight regression module and IQA-specific score tokens into the MLLM pipeline. Extensive experiments demonstrate that Q-Scorer achieves state-of-the-art performance across multiple IQA benchmarks, generalizes well to mixed datasets, and further improves when combined with other methods.",
            "headline_zh": "提出Q-Scorer框架以解决MLLM在图像质量评估中的离散-连续不匹配问题",
            "intro_zh": [
                "核心问题：MLLM离散输出与IQA连续分数不匹配，导致转换错误和语义混淆。",
                "方法要点：引入轻量回归模块和IQA专用分数令牌，改进MLLM管道。",
                "实验或效果：在多个IQA基准上达到SOTA，泛化性强且可与其他方法结合提升。"
            ],
            "tags_zh": [
                "多模态大语言模型",
                "图像质量评估",
                "离散-连续不匹配",
                "回归模块",
                "分数令牌",
                "基准测试"
            ],
            "_index": 50
        },
        {
            "title": "DI3CL: Contrastive Learning With Dynamic Instances and Contour Consistency for SAR Land-Cover Classification Foundation Model",
            "authors": [
                "Zhongle Ren",
                "Hui Ding",
                "Kai Wang",
                "Biao Hou",
                "Xingyu Luo",
                "Weibin Li",
                "Licheng Jiao"
            ],
            "arxiv_id": "2511.07808v1",
            "summary": "Although significant advances have been achieved in SAR land-cover classification, recent methods remain predominantly focused on supervised learning, which relies heavily on extensive labeled datasets. This dependency not only limits scalability and generalization but also restricts adaptability to diverse application scenarios. In this paper, a general-purpose foundation model for SAR land-cover classification is developed, serving as a robust cornerstone to accelerate the development and deployment of various downstream models. Specifically, a Dynamic Instance and Contour Consistency Contrastive Learning (DI3CL) pre-training framework is presented, which incorporates a Dynamic Instance (DI) module and a Contour Consistency (CC) module. DI module enhances global contextual awareness by enforcing local consistency across different views of the same region. CC module leverages shallow feature maps to guide the model to focus on the geometric contours of SAR land-cover objects, thereby improving structural discrimination. Additionally, to enhance robustness and generalization during pre-training, a large-scale and diverse dataset named SARSense, comprising 460,532 SAR images, is constructed to enable the model to capture comprehensive and representative features. To evaluate the generalization capability of our foundation model, we conducted extensive experiments across a variety of SAR land-cover classification tasks, including SAR land-cover mapping, water body detection, and road extraction. The results consistently demonstrate that the proposed DI3CL outperforms existing methods. Our code and pre-trained weights are publicly available at: https://github.com/SARpre-train/DI3CL.",
            "headline_zh": "提出DI3CL对比学习框架，构建SAR地物分类基础模型以提升泛化能力",
            "intro_zh": [
                "核心问题：监督学习依赖大量标注数据，限制SAR地物分类的泛化与适应性。",
                "方法要点：引入动态实例和轮廓一致性模块，增强全局上下文与结构判别。",
                "实验效果：在多种任务中优于现有方法，使用大规模数据集提升鲁棒性。"
            ],
            "tags_zh": [
                "SAR地物分类",
                "对比学习",
                "基础模型",
                "轮廓一致性",
                "动态实例",
                "泛化能力"
            ],
            "_index": 51
        },
        {
            "title": "Divide-and-Conquer Decoupled Network for Cross-Domain Few-Shot Segmentation",
            "authors": [
                "Runmin Cong",
                "Anpeng Wang",
                "Bin Wan",
                "Cong Zhang",
                "Xiaofei Zhou",
                "Wei Zhang"
            ],
            "arxiv_id": "2511.07798v1",
            "summary": "Cross-domain few-shot segmentation (CD-FSS) aims to tackle the dual challenge of recognizing novel classes and adapting to unseen domains with limited annotations. However, encoder features often entangle domain-relevant and category-relevant information, limiting both generalization and rapid adaptation to new domains. To address this issue, we propose a Divide-and-Conquer Decoupled Network (DCDNet). In the training stage, to tackle feature entanglement that impedes cross-domain generalization and rapid adaptation, we propose the Adversarial-Contrastive Feature Decomposition (ACFD) module. It decouples backbone features into category-relevant private and domain-relevant shared representations via contrastive learning and adversarial learning. Then, to mitigate the potential degradation caused by the disentanglement, the Matrix-Guided Dynamic Fusion (MGDF) module adaptively integrates base, shared, and private features under spatial guidance, maintaining structural coherence. In addition, in the fine-tuning stage, to enhanced model generalization, the Cross-Adaptive Modulation (CAM) module is placed before the MGDF, where shared features guide private features via modulation ensuring effective integration of domain-relevant information. Extensive experiments on four challenging datasets show that DCDNet outperforms existing CD-FSS methods, setting a new state-of-the-art for cross-domain generalization and few-shot adaptation.",
            "headline_zh": "提出DCDNet以解决跨域少样本分割中的特征纠缠问题",
            "intro_zh": [
                "核心问题：编码器特征纠缠域相关和类别相关信息，限制跨域泛化和快速适应",
                "方法要点：使用ACFD模块解耦特征，MGDF模块动态融合，CAM模块在微调中调制特征",
                "实验或效果：在四个数据集上超越现有方法，实现新的最优性能"
            ],
            "tags_zh": [
                "跨域少样本分割",
                "特征解耦",
                "对抗学习",
                "对比学习",
                "动态特征融合",
                "跨域适应"
            ],
            "_index": 52
        },
        {
            "title": "VectorSynth: Fine-Grained Satellite Image Synthesis with Structured Semantics",
            "authors": [
                "Daniel Cher",
                "Brian Wei",
                "Srikumar Sastry",
                "Nathan Jacobs"
            ],
            "arxiv_id": "2511.07744v1",
            "summary": "We introduce VectorSynth, a diffusion-based framework for pixel-accurate satellite image synthesis conditioned on polygonal geographic annotations with semantic attributes. Unlike prior text- or layout-conditioned models, VectorSynth learns dense cross-modal correspondences that align imagery and semantic vector geometry, enabling fine-grained, spatially grounded edits. A vision language alignment module produces pixel-level embeddings from polygon semantics; these embeddings guide a conditional image generation framework to respect both spatial extents and semantic cues. VectorSynth supports interactive workflows that mix language prompts with geometry-aware conditioning, allowing rapid what-if simulations, spatial edits, and map-informed content generation. For training and evaluation, we assemble a collection of satellite scenes paired with pixel-registered polygon annotations spanning diverse urban scenes with both built and natural features. We observe strong improvements over prior methods in semantic fidelity and structural realism, and show that our trained vision language model demonstrates fine-grained spatial grounding. The code and data are available at https://github.com/mvrl/VectorSynth.",
            "headline_zh": "提出VectorSynth框架，基于扩散模型实现多边形标注下的精细卫星图像合成。",
            "intro_zh": [
                "核心问题：现有文本或布局条件模型难以实现像素级精确的卫星图像合成与编辑。",
                "方法要点：通过视觉语言对齐模块生成像素级嵌入，结合语义向量几何指导图像生成。",
                "实验或效果：在语义保真度和结构真实感上优于先前方法，支持交互式空间编辑。"
            ],
            "tags_zh": [
                "卫星图像合成",
                "扩散模型",
                "语义向量几何",
                "视觉语言对齐",
                "空间编辑",
                "像素级嵌入"
            ],
            "_index": 53
        },
        {
            "title": "Cross-pyramid consistency regularization for semi-supervised medical image segmentation",
            "authors": [
                "Matus Bojko",
                "Maros Kollar",
                "Marek Jakab",
                "Wanda Benesova"
            ],
            "arxiv_id": "2511.08435v1",
            "summary": "Semi-supervised learning (SSL) enables training of powerful models with the assumption of limited, carefully labelled data and a large amount of unlabeled data to support the learning. In this paper, we propose a hybrid consistency learning approach to effectively exploit unlabeled data for semi-supervised medical image segmentation by leveraging Cross-Pyramid Consistency Regularization (CPCR) between two decoders. First, we design a hybrid Dual Branch Pyramid Network (DBPNet), consisting of an encoder and two decoders that differ slightly, each producing a pyramid of perturbed auxiliary predictions across multiple resolution scales. Second, we present a learning strategy for this network named CPCR that combines existing consistency learning and uncertainty minimization approaches on the main output predictions of decoders with our novel regularization term. More specifically, in this term, we extend the soft-labeling setting to pyramid predictions across decoders to support knowledge distillation in deep hierarchical features. Experimental results show that DBPNet with CPCR outperforms five state-of-the-art self-supervised learning methods and has comparable performance with recent ones on a public benchmark dataset.",
            "headline_zh": "提出跨金字塔一致性正则化以提升半监督医学图像分割性能",
            "intro_zh": [
                "核心问题：半监督学习在医学图像分割中如何有效利用未标记数据",
                "方法要点：设计双分支金字塔网络，结合跨解码器金字塔预测一致性正则化",
                "实验或效果：在公共数据集上优于五种自监督方法，与最新方法性能相当"
            ],
            "tags_zh": [
                "半监督学习",
                "医学图像分割",
                "一致性正则化",
                "金字塔网络",
                "知识蒸馏"
            ],
            "_index": 54
        },
        {
            "title": "OmniAID: Decoupling Semantic and Artifacts for Universal AI-Generated Image Detection in the Wild",
            "authors": [
                "Yuncheng Guo",
                "Junyan Ye",
                "Chenjue Zhang",
                "Hengrui Kang",
                "Haohuan Fu",
                "Conghui He",
                "Weijia Li"
            ],
            "arxiv_id": "2511.08423v1",
            "summary": "A truly universal AI-Generated Image (AIGI) detector must simultaneously generalize across diverse generative models and varied semantic content. Current state-of-the-art methods learn a single, entangled forgery representation--conflating content-dependent flaws with content-agnostic artifacts--and are further constrained by outdated benchmarks. To overcome these limitations, we propose OmniAID, a novel framework centered on a decoupled Mixture-of-Experts (MoE) architecture. The core of our method is a hybrid expert system engineered to decouple: (1) semantic flaws across distinct content domains, and (2) these content-dependent flaws from content-agnostic universal artifacts. This system employs a set of Routable Specialized Semantic Experts, each for a distinct domain (e.g., human, animal), complemented by a Fixed Universal Artifact Expert. This architecture is trained using a bespoke two-stage strategy: we first train the experts independently with domain-specific hard-sampling to ensure specialization, and subsequently train a lightweight gating network for effective input routing. By explicitly decoupling \"what is generated\" (content-specific flaws) from \"how it is generated\" (universal artifacts), OmniAID achieves robust generalization. To address outdated benchmarks and validate real-world applicability, we introduce Mirage, a new large-scale, contemporary dataset. Extensive experiments, using both traditional benchmarks and our Mirage dataset, demonstrate our model surpasses existing monolithic detectors, establishing a new, robust standard for AIGI authentication against modern, in-the-wild threats.",
            "headline_zh": "提出OmniAID框架，通过解耦语义与伪影实现通用AI生成图像检测",
            "intro_zh": [
                "当前AI生成图像检测方法泛化性差，因语义与伪影特征纠缠且基准过时",
                "采用解耦混合专家架构，分离内容相关缺陷与通用伪影，提升鲁棒性",
                "实验显示在传统基准和新数据集Mirage上优于现有方法，验证通用性"
            ],
            "tags_zh": [
                "AI生成图像检测",
                "混合专家架构",
                "语义解耦",
                "通用伪影",
                "大规模数据集",
                "鲁棒泛化"
            ],
            "_index": 55
        },
        {
            "title": "Probabilistic Safety Guarantee for Stochastic Control Systems Using Average Reward MDPs",
            "authors": [
                "Saber Omidi",
                "Marek Petrik",
                "Se Young Yoon",
                "Momotaz Begum"
            ],
            "arxiv_id": "2511.08419v1",
            "summary": "Safety in stochastic control systems, which are subject to random noise with a known probability distribution, aims to compute policies that satisfy predefined operational constraints with high confidence throughout the uncertain evolution of the state variables. The unpredictable evolution of state variables poses a significant challenge for meeting predefined constraints using various control methods. To address this, we present a new algorithm that computes safe policies to determine the safety level across a finite state set. This algorithm reduces the safety objective to the standard average reward Markov Decision Process (MDP) objective. This reduction enables us to use standard techniques, such as linear programs, to compute and analyze safe policies. We validate the proposed method numerically on the Double Integrator and the Inverted Pendulum systems. Results indicate that the average-reward MDPs solution is more comprehensive, converges faster, and offers higher quality compared to the minimum discounted-reward solution.",
            "headline_zh": "提出基于平均奖励MDP的算法以解决随机控制系统安全策略计算问题",
            "intro_zh": [
                "随机控制系统状态变量不确定演化难以满足预定义约束",
                "将安全目标简化为平均奖励MDP目标，利用线性规划计算策略",
                "在双积分器和倒立摆系统验证，收敛更快、质量更高"
            ],
            "tags_zh": [
                "随机控制系统",
                "安全策略",
                "平均奖励MDP",
                "线性规划",
                "策略验证"
            ],
            "_index": 56
        },
        {
            "title": "Top2Ground: A Height-Aware Dual Conditioning Diffusion Model for Robust Aerial-to-Ground View Generation",
            "authors": [
                "Jae Joong Lee",
                "Bedrich Benes"
            ],
            "arxiv_id": "2511.08258v1",
            "summary": "Generating ground-level images from aerial views is a challenging task due to extreme viewpoint disparity, occlusions, and a limited field of view. We introduce Top2Ground, a novel diffusion-based method that directly generates photorealistic ground-view images from aerial input images without relying on intermediate representations such as depth maps or 3D voxels. Specifically, we condition the denoising process on a joint representation of VAE-encoded spatial features (derived from aerial RGB images and an estimated height map) and CLIP-based semantic embeddings. This design ensures the generation is both geometrically constrained by the scene's 3D structure and semantically consistent with its content. We evaluate Top2Ground on three diverse datasets: CVUSA, CVACT, and the Auto Arborist. Our approach shows 7.3% average improvement in SSIM across three benchmark datasets, showing Top2Ground can robustly handle both wide and narrow fields of view, highlighting its strong generalization capabilities.",
            "headline_zh": "提出Top2Ground扩散模型，从航拍图像生成地面视图，解决视角差异和遮挡问题。",
            "intro_zh": [
                "核心问题：航拍到地面视图生成因视角差异、遮挡和视野限制而困难。",
                "方法要点：使用VAE空间特征和CLIP语义嵌入联合调节扩散过程，无需中间表示。",
                "实验或效果：在三个数据集上平均SSIM提升7.3%，展示强泛化能力。"
            ],
            "tags_zh": [
                "航拍图像生成",
                "扩散模型",
                "视图转换",
                "几何约束",
                "语义一致性"
            ],
            "_index": 57
        },
        {
            "title": "Non-Aligned Reference Image Quality Assessment for Novel View Synthesis",
            "authors": [
                "Abhijay Ghildyal",
                "Rajesh Sureddi",
                "Nabajeet Barman",
                "Saman Zadtootaghaj",
                "Alan Bovik"
            ],
            "arxiv_id": "2511.08155v1",
            "summary": "Evaluating the perceptual quality of Novel View Synthesis (NVS) images remains a key challenge, particularly in the absence of pixel-aligned ground truth references. Full-Reference Image Quality Assessment (FR-IQA) methods fail under misalignment, while No-Reference (NR-IQA) methods struggle with generalization. In this work, we introduce a Non-Aligned Reference (NAR-IQA) framework tailored for NVS, where it is assumed that the reference view shares partial scene content but lacks pixel-level alignment. We constructed a large-scale image dataset containing synthetic distortions targeting Temporal Regions of Interest (TROI) to train our NAR-IQA model. Our model is built on a contrastive learning framework that incorporates LoRA-enhanced DINOv2 embeddings and is guided by supervision from existing IQA methods. We train exclusively on synthetically generated distortions, deliberately avoiding overfitting to specific real NVS samples and thereby enhancing the model's generalization capability. Our model outperforms state-of-the-art FR-IQA, NR-IQA, and NAR-IQA methods, achieving robust performance on both aligned and non-aligned references. We also conducted a novel user study to gather data on human preferences when viewing non-aligned references in NVS. We find strong correlation between our proposed quality prediction model and the collected subjective ratings. For dataset and code, please visit our project page: https://stootaghaj.github.io/nova-project/",
            "headline_zh": "提出非对齐参考图像质量评估框架以解决新视角合成中的质量评估挑战",
            "intro_zh": [
                "核心问题：新视角合成图像在缺乏像素对齐参考时，现有全参考和无参考方法评估效果不佳",
                "方法要点：基于对比学习框架，结合LoRA增强的DINOv2嵌入，使用合成失真数据进行训练",
                "实验或效果：模型在多种参考条件下优于现有方法，并通过用户研究验证与主观评分强相关"
            ],
            "tags_zh": [
                "新视角合成",
                "图像质量评估",
                "非对齐参考",
                "对比学习",
                "DINOv2嵌入",
                "合成失真数据"
            ],
            "_index": 58
        },
        {
            "title": "LatentPrintFormer: A Hybrid CNN-Transformer with Spatial Attention for Latent Fingerprint identification",
            "authors": [
                "Arnab Maity",
                "Manasa",
                "Pavan Kumar C",
                "Raghavendra Ramachandra"
            ],
            "arxiv_id": "2511.08119v1",
            "summary": "Latent fingerprint identification remains a challenging task due to low image quality, background noise, and partial impressions. In this work, we propose a novel identification approach called LatentPrintFormer. The proposed model integrates a CNN backbone (EfficientNet-B0) and a Transformer backbone (Swin Tiny) to extract both local and global features from latent fingerprints. A spatial attention module is employed to emphasize high-quality ridge regions while suppressing background noise. The extracted features are fused and projected into a unified 512-dimensional embedding, and matching is performed using cosine similarity in a closed-set identification setting. Extensive experiments on two publicly available datasets demonstrate that LatentPrintFormer consistently outperforms three state-of-the-art latent fingerprint recognition techniques, achieving higher identification rates across Rank-10.",
            "headline_zh": "提出LatentPrintFormer混合模型以解决潜指纹识别中的图像质量低和噪声问题",
            "intro_zh": [
                "潜指纹识别面临图像质量低、背景噪声和部分印记等挑战",
                "结合CNN和Transformer提取局部与全局特征，并使用空间注意力模块增强脊线区域",
                "在公开数据集上实验，识别率优于现有方法，尤其在Rank-10表现突出"
            ],
            "tags_zh": [
                "潜指纹识别",
                "混合模型",
                "空间注意力",
                "特征融合",
                "余弦相似度匹配"
            ],
            "_index": 59
        },
        {
            "title": "PerspAct: Enhancing LLM Situated Collaboration Skills through Perspective Taking and Active Vision",
            "authors": [
                "Sabrina Patania",
                "Luca Annese",
                "Anita Pellegrini",
                "Silvia Serino",
                "Anna Lambiase",
                "Luca Pallonetto",
                "Silvia Rossi",
                "Simone Colombani",
                "Tom Foulsham",
                "Azzurra Ruggeri",
                "Dimitri Ognibene"
            ],
            "arxiv_id": "2511.08098v1",
            "summary": "Recent advances in Large Language Models (LLMs) and multimodal foundation models have significantly broadened their application in robotics and collaborative systems. However, effective multi-agent interaction necessitates robust perspective-taking capabilities, enabling models to interpret both physical and epistemic viewpoints. Current training paradigms often neglect these interactive contexts, resulting in challenges when models must reason about the subjectivity of individual perspectives or navigate environments with multiple observers. This study evaluates whether explicitly incorporating diverse points of view using the ReAct framework, an approach that integrates reasoning and acting, can enhance an LLM's ability to understand and ground the demands of other agents. We extend the classic Director task by introducing active visual exploration across a suite of seven scenarios of increasing perspective-taking complexity. These scenarios are designed to challenge the agent's capacity to resolve referential ambiguity based on visual access and interaction, under varying state representations and prompting strategies, including ReAct-style reasoning. Our results demonstrate that explicit perspective cues, combined with active exploration strategies, significantly improve the model's interpretative accuracy and collaborative effectiveness. These findings highlight the potential of integrating active perception with perspective-taking mechanisms in advancing LLMs' application in robotics and multi-agent systems, setting a foundation for future research into adaptive and context-aware AI systems.",
            "headline_zh": "提出PerspAct方法，通过视角采择和主动视觉增强LLM在协作系统中的能力",
            "intro_zh": [
                "核心问题：LLM在多智能体交互中缺乏视角采择能力，导致推理主观视角困难",
                "方法要点：扩展ReAct框架，引入主动视觉探索和多样化视角提示策略",
                "实验或效果：在七种复杂场景中，显著提升模型解释准确性和协作有效性"
            ],
            "tags_zh": [
                "视角采择",
                "主动视觉",
                "多智能体系统",
                "ReAct框架",
                "LLM协作"
            ],
            "_index": 60
        },
        {
            "title": "Dynamic Sparsity: Challenging Common Sparsity Assumptions for Learning World Models in Robotic Reinforcement Learning Benchmarks",
            "authors": [
                "Muthukumar Pandaram",
                "Jakob Hollenstein",
                "David Drexel",
                "Samuele Tosatto",
                "Antonio Rodríguez-Sánchez",
                "Justus Piater"
            ],
            "arxiv_id": "2511.08086v1",
            "summary": "The use of learned dynamics models, also known as world models, can improve the sample efficiency of reinforcement learning. Recent work suggests that the underlying causal graphs of such dynamics models are sparsely connected, with each of the future state variables depending only on a small subset of the current state variables, and that learning may therefore benefit from sparsity priors. Similarly, temporal sparsity, i.e. sparsely and abruptly changing local dynamics, has also been proposed as a useful inductive bias.\n  In this work, we critically examine these assumptions by analyzing ground-truth dynamics from a set of robotic reinforcement learning environments in the MuJoCo Playground benchmark suite, aiming to determine whether the proposed notions of state and temporal sparsity actually tend to hold in typical reinforcement learning tasks.\n  We study (i) whether the causal graphs of environment dynamics are sparse, (ii) whether such sparsity is state-dependent, and (iii) whether local system dynamics change sparsely.\n  Our results indicate that global sparsity is rare, but instead the tasks show local, state-dependent sparsity in their dynamics and this sparsity exhibits distinct structures, appearing in temporally localized clusters (e.g., during contact events) and affecting specific subsets of state dimensions. These findings challenge common sparsity prior assumptions in dynamics learning, emphasizing the need for grounded inductive biases that reflect the state-dependent sparsity structure of real-world dynamics.",
            "headline_zh": "分析机器人强化学习基准中动态稀疏性假设，揭示状态依赖稀疏性结构",
            "intro_zh": [
                "核心问题：检验世界模型中状态和时序稀疏性假设在真实机器人任务中的有效性",
                "方法要点：分析MuJoCo Playground基准的真实动态，评估因果图稀疏性和状态依赖性",
                "实验或效果：发现全局稀疏性罕见，但存在局部状态依赖稀疏性，挑战常见先验假设"
            ],
            "tags_zh": [
                "强化学习",
                "世界模型",
                "稀疏性分析",
                "机器人基准",
                "动态建模"
            ],
            "_index": 61
        },
        {
            "title": "Re$^{\\text{2}}$MaP: Macro Placement by Recursively Prototyping and Packing Tree-based Relocating",
            "authors": [
                "Yunqi Shi",
                "Xi Lin",
                "Zhiang Wang",
                "Siyuan Xu",
                "Shixiong Kai",
                "Yao Lai",
                "Chengrui Gao",
                "Ke Xue",
                "Mingxuan Yuan",
                "Chao Qian",
                "Zhi-Hua Zhou"
            ],
            "arxiv_id": "2511.08054v1",
            "summary": "This work introduces the Re$^{\\text{2}}$MaP method, which generates expert-quality macro placements through recursively prototyping and packing tree-based relocating. We first perform multi-level macro grouping and PPA-aware cell clustering to produce a unified connection matrix that captures both wirelength and dataflow among macros and clusters. Next, we use DREAMPlace to build a mixed-size placement prototype and obtain reference positions for each macro and cluster. Based on this prototype, we introduce ABPlace, an angle-based analytical method that optimizes macro positions on an ellipse to distribute macros uniformly near chip periphery, while optimizing wirelength and dataflow. A packing tree-based relocating procedure is then designed to jointly adjust the locations of macro groups and the macros within each group, by optimizing an expertise-inspired cost function that captures various design constraints through evolutionary search. Re$^{\\text{2}}$MaP repeats the above process: Only a subset of macro groups are positioned in each iteration, and the remaining macros are deferred to the next iteration to improve the prototype's accuracy. Using a well-established backend flow with sufficient timing optimizations, Re$^{\\text{2}}$MaP achieves up to 22.22% (average 10.26%) improvement in worst negative slack (WNS) and up to 97.91% (average 33.97%) improvement in total negative slack (TNS) compared to the state-of-the-art academic placer Hier-RTLMP. It also ranks higher on WNS, TNS, power, design rule check (DRC) violations, and runtime than the conference version ReMaP, across seven tested cases. Our code is available at https://github.com/lamda-bbo/Re2MaP.",
            "headline_zh": "提出Re²MaP方法，通过递归原型和树状重定位优化宏布局，提升芯片设计质量",
            "intro_zh": [
                "核心问题：优化宏布局以改善芯片性能、功耗和面积，应对复杂设计约束",
                "方法要点：结合多级分组、原型构建、角度优化和树状重定位，迭代改进布局",
                "实验效果：相比先进方法，WNS和TNS平均提升10.26%和33.97%，多项指标更优"
            ],
            "tags_zh": [
                "宏布局优化",
                "芯片物理设计",
                "递归原型",
                "树状重定位",
                "PPA优化",
                "进化搜索"
            ],
            "_index": 62
        },
        {
            "title": "Multi-modal Deepfake Detection and Localization with FPN-Transformer",
            "authors": [
                "Chende Zheng",
                "Ruiqi Suo",
                "Zhoulin Ji",
                "Jingyi Deng",
                "Fangbin Yi",
                "Chenhao Lin",
                "Chao Shen"
            ],
            "arxiv_id": "2511.08031v1",
            "summary": "The rapid advancement of generative adversarial networks (GANs) and diffusion models has enabled the creation of highly realistic deepfake content, posing significant threats to digital trust across audio-visual domains. While unimodal detection methods have shown progress in identifying synthetic media, their inability to leverage cross-modal correlations and precisely localize forged segments limits their practicality against sophisticated, fine-grained manipulations. To address this, we introduce a multi-modal deepfake detection and localization framework based on a Feature Pyramid-Transformer (FPN-Transformer), addressing critical gaps in cross-modal generalization and temporal boundary regression. The proposed approach utilizes pre-trained self-supervised models (WavLM for audio, CLIP for video) to extract hierarchical temporal features. A multi-scale feature pyramid is constructed through R-TLM blocks with localized attention mechanisms, enabling joint analysis of cross-context temporal dependencies. The dual-branch prediction head simultaneously predicts forgery probabilities and refines temporal offsets of manipulated segments, achieving frame-level localization precision. We evaluate our approach on the test set of the IJCAI'25 DDL-AV benchmark, showing a good performance with a final score of 0.7535 for cross-modal deepfake detection and localization in challenging environments. Experimental results confirm the effectiveness of our approach and provide a novel way for generalized deepfake detection. Our code is available at https://github.com/Zig-HS/MM-DDL",
            "headline_zh": "提出基于FPN-Transformer的多模态深度伪造检测与定位框架，以应对跨模态伪造威胁。",
            "intro_zh": [
                "核心问题：单模态方法难以利用跨模态关联并精确定位伪造片段，限制了对精细伪造的检测。",
                "方法要点：使用WavLM和CLIP提取特征，构建多尺度特征金字塔，通过双分支预测头实现检测与定位。",
                "实验或效果：在IJCAI'25 DDL-AV基准测试中得分0.7535，验证了方法的有效性。"
            ],
            "tags_zh": [
                "多模态深度伪造检测",
                "特征金字塔-Transformer",
                "跨模态定位",
                "自监督特征提取",
                "时间边界回归"
            ],
            "_index": 63
        },
        {
            "title": "AVOID-JACK: Avoidance of Jackknifing for Swarms of Long Heavy Articulated Vehicles",
            "authors": [
                "Adrian Schönnagel",
                "Michael Dubé",
                "Christoph Steup",
                "Felix Keppler",
                "Sanaz Mostaghim"
            ],
            "arxiv_id": "2511.08016v1",
            "summary": "This paper presents a novel approach to avoiding jackknifing and mutual collisions in Heavy Articulated Vehicles (HAVs) by leveraging decentralized swarm intelligence. In contrast to typical swarm robotics research, our robots are elongated and exhibit complex kinematics, introducing unique challenges. Despite its relevance to real-world applications such as logistics automation, remote mining, airport baggage transport, and agricultural operations, this problem has not been addressed in the existing literature.\n  To tackle this new class of swarm robotics problems, we propose a purely reaction-based, decentralized swarm intelligence strategy tailored to automate elongated, articulated vehicles. The method presented in this paper prioritizes jackknifing avoidance and establishes a foundation for mutual collision avoidance. We validate our approach through extensive simulation experiments and provide a comprehensive analysis of its performance. For the experiments with a single HAV, we observe that for 99.8% jackknifing was successfully avoided and that 86.7% and 83.4% reach their first and second goals, respectively. With two HAVs interacting, we observe 98.9%, 79.4%, and 65.1%, respectively, while 99.7% of the HAVs do not experience mutual collisions.",
            "headline_zh": "提出去中心化群智方法以避免长重型铰接车辆的折叠与碰撞",
            "intro_zh": [
                "核心问题：长重型铰接车辆在群控中易折叠和相互碰撞，现有研究未解决",
                "方法要点：采用纯反应式去中心化群智策略，专为长铰接车辆自动化设计",
                "实验效果：单车辆折叠避免率达99.8%，多车辆交互中碰撞避免率达99.7%"
            ],
            "tags_zh": [
                "群机器人学",
                "铰接车辆控制",
                "折叠避免",
                "去中心化控制",
                "反应式策略",
                "仿真验证"
            ],
            "_index": 64
        },
        {
            "title": "From Noise to Latent: Generating Gaussian Latents for INR-Based Image Compression",
            "authors": [
                "Chaoyi Lin",
                "Yaojun Wu",
                "Yue Li",
                "Junru Li",
                "Kai Zhang",
                "Li Zhang"
            ],
            "arxiv_id": "2511.08009v1",
            "summary": "Recent implicit neural representation (INR)-based image compression methods have shown competitive performance by overfitting image-specific latent codes. However, they remain inferior to end-to-end (E2E) compression approaches due to the absence of expressive latent representations. On the other hand, E2E methods rely on transmitting latent codes and requiring complex entropy models, leading to increased decoding complexity. Inspired by the normalization strategy in E2E codecs where latents are transformed into Gaussian noise to demonstrate the removal of spatial redundancy, we explore the inverse direction: generating latents directly from Gaussian noise. In this paper, we propose a novel image compression paradigm that reconstructs image-specific latents from a multi-scale Gaussian noise tensor, deterministically generated using a shared random seed. A Gaussian Parameter Prediction (GPP) module estimates the distribution parameters, enabling one-shot latent generation via reparameterization trick. The predicted latent is then passed through a synthesis network to reconstruct the image. Our method eliminates the need to transmit latent codes while preserving latent-based benefits, achieving competitive rate-distortion performance on Kodak and CLIC dataset. To the best of our knowledge, this is the first work to explore Gaussian latent generation for learned image compression.",
            "headline_zh": "提出从高斯噪声生成隐变量以消除隐码传输的图像压缩方法",
            "intro_zh": [
                "核心问题：INR图像压缩方法因隐码表达能力不足而性能劣于端到端方法",
                "方法要点：使用共享随机种子生成多尺度高斯噪声，通过重参数化预测隐变量",
                "实验或效果：在Kodak和CLIC数据集上实现竞争性率失真性能"
            ],
            "tags_zh": [
                "图像压缩",
                "隐式神经表示",
                "高斯噪声生成",
                "隐变量预测",
                "率失真优化"
            ],
            "_index": 65
        },
        {
            "title": "EAGLE: Episodic Appearance- and Geometry-aware Memory for Unified 2D-3D Visual Query Localization in Egocentric Vision",
            "authors": [
                "Yifei Cao",
                "Yu Liu",
                "Guolong Wang",
                "Zhu Liu",
                "Kai Wang",
                "Xianjie Zhang",
                "Jizhe Yu",
                "Xun Tu"
            ],
            "arxiv_id": "2511.08007v1",
            "summary": "Egocentric visual query localization is vital for embodied AI and VR/AR, yet remains challenging due to camera motion, viewpoint changes, and appearance variations. We present EAGLE, a novel framework that leverages episodic appearance- and geometry-aware memory to achieve unified 2D-3D visual query localization in egocentric vision. Inspired by avian memory consolidation, EAGLE synergistically integrates segmentation guided by an appearance-aware meta-learning memory (AMM), with tracking driven by a geometry-aware localization memory (GLM). This memory consolidation mechanism, through structured appearance and geometry memory banks, stores high-confidence retrieval samples, effectively supporting both long- and short-term modeling of target appearance variations. This enables precise contour delineation with robust spatial discrimination, leading to significantly improved retrieval accuracy. Furthermore, by integrating the VQL-2D output with a visual geometry grounded Transformer (VGGT), we achieve a efficient unification of 2D and 3D tasks, enabling rapid and accurate back-projection into 3D space. Our method achieves state-ofthe-art performance on the Ego4D-VQ benchmark.",
            "headline_zh": "提出EAGLE框架，利用外观和几何感知记忆实现统一2D-3D视觉查询定位，解决自我中心视觉中的挑战。",
            "intro_zh": [
                "核心问题：自我中心视觉查询定位因相机运动、视角变化和外观变化而困难。",
                "方法要点：结合外观感知元学习记忆和几何感知定位记忆，支持目标外观的长期和短期建模。",
                "实验或效果：在Ego4D-VQ基准上实现最先进性能，提升检索精度和3D空间投影效率。"
            ],
            "tags_zh": [
                "自我中心视觉",
                "视觉查询定位",
                "记忆机制",
                "2D-3D统一",
                "外观变化建模",
                "几何感知"
            ],
            "_index": 66
        },
        {
            "title": "A Two-Layer Electrostatic Film Actuator with High Actuation Stress and Integrated Brake",
            "authors": [
                "Huacen Wang",
                "Hongqiang Wang"
            ],
            "arxiv_id": "2511.08005v1",
            "summary": "Robotic systems driven by conventional motors often suffer from challenges such as large mass, complex control algorithms, and the need for additional braking mechanisms, which limit their applications in lightweight and compact robotic platforms. Electrostatic film actuators offer several advantages, including thinness, flexibility, lightweight construction, and high open-loop positioning accuracy. However, the actuation stress exhibited by conventional actuators in air still needs improvement, particularly for the widely used three-phase electrode design. To enhance the output performance of actuators, this paper presents a two-layer electrostatic film actuator with an integrated brake. By alternately distributing electrodes on both the top and bottom layers, a smaller effective electrode pitch is achieved under the same fabrication constraints, resulting in an actuation stress of approximately 241~N/m$^2$, representing a 90.5\\% improvement over previous three-phase actuators operating in air. Furthermore, its integrated electrostatic adhesion mechanism enables load retention under braking mode. Several demonstrations, including a tug-of-war between a conventional single-layer actuator and the proposed two-layer actuator, a payload operation, a one-degree-of-freedom robotic arm, and a dual-mode gripper, were conducted to validate the actuator's advantageous capabilities in both actuation and braking modes.",
            "headline_zh": "提出双层静电薄膜致动器以提升致动应力并集成制动功能",
            "intro_zh": [
                "传统电机驱动机器人存在质量大、控制复杂和需额外制动机制的问题",
                "采用双层电极交替分布设计，减小有效电极间距，提高致动应力",
                "实验验证致动应力达241 N/m²，提升90.5%，并展示集成制动能力"
            ],
            "tags_zh": [
                "静电薄膜致动器",
                "双层电极设计",
                "高致动应力",
                "集成制动",
                "机器人致动",
                "轻量化平台"
            ],
            "_index": 67
        },
        {
            "title": "CSF-Net: Context-Semantic Fusion Network for Large Mask Inpainting",
            "authors": [
                "Chae-Yeon Heo",
                "Yeong-Jun Cho"
            ],
            "arxiv_id": "2511.07987v1",
            "summary": "In this paper, we propose a semantic-guided framework to address the challenging problem of large-mask image inpainting, where essential visual content is missing and contextual cues are limited. To compensate for the limited context, we leverage a pretrained Amodal Completion (AC) model to generate structure-aware candidates that serve as semantic priors for the missing regions. We introduce Context-Semantic Fusion Network (CSF-Net), a transformer-based fusion framework that fuses these candidates with contextual features to produce a semantic guidance image for image inpainting. This guidance improves inpainting quality by promoting structural accuracy and semantic consistency. CSF-Net can be seamlessly integrated into existing inpainting models without architectural changes and consistently enhances performance across diverse masking conditions. Extensive experiments on the Places365 and COCOA datasets demonstrate that CSF-Net effectively reduces object hallucination while enhancing visual realism and semantic alignment. The code for CSF-Net is available at https://github.com/chaeyeonheo/CSF-Net.",
            "headline_zh": "提出CSF-Net以解决大掩码图像修复中语义一致性问题",
            "intro_zh": [
                "核心问题：大掩码图像修复中视觉内容缺失和上下文线索有限。",
                "方法要点：利用预训练AC模型生成语义先验，通过transformer融合上下文特征。",
                "实验或效果：在Places365和COCOA数据集上减少物体幻觉，提升视觉真实感。"
            ],
            "tags_zh": [
                "图像修复",
                "语义引导",
                "transformer融合",
                "大掩码处理",
                "结构一致性"
            ],
            "_index": 68
        },
        {
            "title": "Multi-Modal Assistance for Unsupervised Domain Adaptation on Point Cloud 3D Object Detection",
            "authors": [
                "Shenao Zhao",
                "Pengpeng Liang",
                "Zhoufan Yang"
            ],
            "arxiv_id": "2511.07966v1",
            "summary": "Unsupervised domain adaptation for LiDAR-based 3D object detection (3D UDA) based on the teacher-student architecture with pseudo labels has achieved notable improvements in recent years. Although it is quite popular to collect point clouds and images simultaneously, little attention has been paid to the usefulness of image data in 3D UDA when training the models. In this paper, we propose an approach named MMAssist that improves the performance of 3D UDA with multi-modal assistance. A method is designed to align 3D features between the source domain and the target domain by using image and text features as bridges. More specifically, we project the ground truth labels or pseudo labels to the images to get a set of 2D bounding boxes. For each 2D box, we extract its image feature from a pre-trained vision backbone. A large vision-language model (LVLM) is adopted to extract the box's text description, and a pre-trained text encoder is used to obtain its text feature. During the training of the model in the source domain and the student model in the target domain, we align the 3D features of the predicted boxes with their corresponding image and text features, and the 3D features and the aligned features are fused with learned weights for the final prediction. The features between the student branch and the teacher branch in the target domain are aligned as well. To enhance the pseudo labels, we use an off-the-shelf 2D object detector to generate 2D bounding boxes from images and estimate their corresponding 3D boxes with the aid of point cloud, and these 3D boxes are combined with the pseudo labels generated by the teacher model. Experimental results show that our approach achieves promising performance compared with state-of-the-art methods in three domain adaptation tasks on three popular 3D object detection datasets. The code is available at https://github.com/liangp/MMAssist.",
            "headline_zh": "提出MMAssist方法，利用多模态辅助提升点云3D目标检测的无监督域适应性能",
            "intro_zh": [
                "核心问题：点云3D目标检测在无监督域适应中，图像数据未被充分利用。",
                "方法要点：使用图像和文本特征作为桥梁，对齐源域和目标域的3D特征。",
                "实验或效果：在三个域适应任务中，性能优于现有先进方法。"
            ],
            "tags_zh": [
                "无监督域适应",
                "点云3D目标检测",
                "多模态学习",
                "特征对齐",
                "伪标签增强"
            ],
            "_index": 69
        },
        {
            "title": "Simulating the Visual World with Artificial Intelligence: A Roadmap",
            "authors": [
                "Jingtong Yue",
                "Ziqi Huang",
                "Zhaoxi Chen",
                "Xintao Wang",
                "Pengfei Wan",
                "Ziwei Liu"
            ],
            "arxiv_id": "2511.08585v1",
            "summary": "The landscape of video generation is shifting, from a focus on generating visually appealing clips to building virtual environments that support interaction and maintain physical plausibility. These developments point toward the emergence of video foundation models that function not only as visual generators but also as implicit world models, models that simulate the physical dynamics, agent-environment interactions, and task planning that govern real or imagined worlds. This survey provides a systematic overview of this evolution, conceptualizing modern video foundation models as the combination of two core components: an implicit world model and a video renderer. The world model encodes structured knowledge about the world, including physical laws, interaction dynamics, and agent behavior. It serves as a latent simulation engine that enables coherent visual reasoning, long-term temporal consistency, and goal-driven planning. The video renderer transforms this latent simulation into realistic visual observations, effectively producing videos as a \"window\" into the simulated world. We trace the progression of video generation through four generations, in which the core capabilities advance step by step, ultimately culminating in a world model, built upon a video generation model, that embodies intrinsic physical plausibility, real-time multimodal interaction, and planning capabilities spanning multiple spatiotemporal scales. For each generation, we define its core characteristics, highlight representative works, and examine their application domains such as robotics, autonomous driving, and interactive gaming. Finally, we discuss open challenges and design principles for next-generation world models, including the role of agent intelligence in shaping and evaluating these systems. An up-to-date list of related works is maintained at this link.",
            "headline_zh": "提出视频基础模型作为隐式世界模型，以构建交互式虚拟环境。",
            "intro_zh": [
                "核心问题：视频生成从视觉吸引力转向物理合理性和交互性。",
                "方法要点：结合隐式世界模型和视频渲染器，编码物理知识与动态。",
                "实验或效果：应用于机器人、自动驾驶和游戏等领域。"
            ],
            "tags_zh": [
                "视频基础模型",
                "隐式世界模型",
                "视频渲染器",
                "物理合理性",
                "交互模拟",
                "多尺度规划"
            ],
            "_index": 70
        },
        {
            "title": "SENCA-st: Integrating Spatial Transcriptomics and Histopathology with Cross Attention Shared Encoder for Region Identification in Cancer Pathology",
            "authors": [
                "Shanaka Liyanaarachchi",
                "Chathurya Wijethunga",
                "Shihab Aaquil Ahamed",
                "Akthas Absar",
                "Ranga Rodrigo"
            ],
            "arxiv_id": "2511.08573v1",
            "summary": "Spatial transcriptomics is an emerging field that enables the identification of functional regions based on the spatial distribution of gene expression. Integrating this functional information present in transcriptomic data with structural data from histopathology images is an active research area with applications in identifying tumor substructures associated with cancer drug resistance. Current histopathology-spatial-transcriptomic region segmentation methods suffer due to either making spatial transcriptomics prominent by using histopathology features just to assist processing spatial transcriptomics data or using vanilla contrastive learning that make histopathology images prominent due to only promoting common features losing functional information. In both extremes, the model gets either lost in the noise of spatial transcriptomics or overly smoothed, losing essential information. Thus, we propose our novel architecture SENCA-st (Shared Encoder with Neighborhood Cross Attention) that preserves the features of both modalities. More importantly, it emphasizes regions that are structurally similar in histopathology but functionally different on spatial transcriptomics using cross-attention. We demonstrate the superior performance of our model that surpasses state-of-the-art methods in detecting tumor heterogeneity and tumor micro-environment regions, a clinically crucial aspect.",
            "headline_zh": "提出SENCA-st架构以整合空间转录组学与组织病理学，用于癌症病理区域识别",
            "intro_zh": [
                "现有方法在整合空间转录组学与组织病理学时，易偏向一方导致信息丢失或噪声放大",
                "采用共享编码器与邻域交叉注意力机制，强调结构相似但功能不同的区域",
                "实验显示模型在检测肿瘤异质性和微环境区域方面优于现有方法"
            ],
            "tags_zh": [
                "空间转录组学",
                "组织病理学",
                "交叉注意力",
                "肿瘤异质性",
                "区域识别"
            ],
            "_index": 71
        },
        {
            "title": "RePose-NeRF: Robust Radiance Fields for Mesh Reconstruction under Noisy Camera Poses",
            "authors": [
                "Sriram Srinivasan",
                "Gautam Ramachandra"
            ],
            "arxiv_id": "2511.08545v1",
            "summary": "Accurate 3D reconstruction from multi-view images is essential for downstream robotic tasks such as navigation, manipulation, and environment understanding. However, obtaining precise camera poses in real-world settings remains challenging, even when calibration parameters are known. This limits the practicality of existing NeRF-based methods that rely heavily on accurate extrinsic estimates. Furthermore, their implicit volumetric representations differ significantly from the widely adopted polygonal meshes, making rendering and manipulation inefficient in standard 3D software. In this work, we propose a robust framework that reconstructs high-quality, editable 3D meshes directly from multi-view images with noisy extrinsic parameters. Our approach jointly refines camera poses while learning an implicit scene representation that captures fine geometric detail and photorealistic appearance. The resulting meshes are compatible with common 3D graphics and robotics tools, enabling efficient downstream use. Experiments on standard benchmarks demonstrate that our method achieves accurate and robust 3D reconstruction under pose uncertainty, bridging the gap between neural implicit representations and practical robotic applications.",
            "headline_zh": "提出RePose-NeRF以从噪声相机位姿的多视角图像中重建可编辑3D网格",
            "intro_zh": [
                "核心问题：真实场景中相机位姿不精确，限制NeRF方法实用性，且隐式体积表示与多边形网格不兼容。",
                "方法要点：联合优化相机位姿并学习隐式场景表示，捕获几何细节和真实感外观。",
                "实验或效果：在标准基准测试中，实现位姿不确定性下的准确鲁棒3D重建，提升机器人应用兼容性。"
            ],
            "tags_zh": [
                "神经辐射场",
                "3D网格重建",
                "相机位姿优化",
                "多视角图像",
                "机器人视觉",
                "隐式表示"
            ],
            "_index": 72
        },
        {
            "title": "A Supervised Autonomous Resection and Retraction Framework for Transurethral Enucleation of the Prostatic Median Lobe",
            "authors": [
                "Mariana Smith",
                "Tanner Watts",
                "Susheela Sharma Stern",
                "Brendan Burkhart",
                "Hao Li",
                "Alejandro O. Chara",
                "Nithesh Kumar",
                "James Ferguson",
                "Ayberk Acar",
                "Jesse F. d'Almeida",
                "Lauren Branscombe",
                "Lauren Shepard",
                "Ahmed Ghazi",
                "Ipek Oguz",
                "Jie Ying Wu",
                "Robert J. Webster",
                "Axel Krieger",
                "Alan Kuntz"
            ],
            "arxiv_id": "2511.08490v1",
            "summary": "Concentric tube robots (CTRs) offer dexterous motion at millimeter scales, enabling minimally invasive procedures through natural orifices. This work presents a coordinated model-based resection planner and learning-based retraction network that work together to enable semi-autonomous tissue resection using a dual-arm transurethral concentric tube robot (the Virtuoso). The resection planner operates directly on segmented CT volumes of prostate phantoms, automatically generating tool trajectories for a three-phase median lobe resection workflow: left/median trough resection, right/median trough resection, and median blunt dissection. The retraction network, PushCVAE, trained on surgeon demonstrations, generates retractions according to the procedural phase. The procedure is executed under Level-3 (supervised) autonomy on a prostate phantom composed of hydrogel materials that replicate the mechanical and cutting properties of tissue. As a feasibility study, we demonstrate that our combined autonomous system achieves a 97.1% resection of the targeted volume of the median lobe. Our study establishes a foundation for image-guided autonomy in transurethral robotic surgery and represents a first step toward fully automated minimally-invasive prostate enucleation.",
            "headline_zh": "提出基于模型规划与学习网络的半自主切除框架，用于经尿道前列腺中叶机器人手术",
            "intro_zh": [
                "核心问题：实现经尿道机器人手术中前列腺中叶的精确半自主切除",
                "方法要点：结合模型规划器生成工具轨迹与学习网络执行牵拉操作",
                "实验效果：在前列腺模型上实现97.1%目标体积切除，验证可行性"
            ],
            "tags_zh": [
                "同心管机器人",
                "半自主手术",
                "图像引导规划",
                "学习网络",
                "前列腺切除",
                "机器人控制"
            ],
            "_index": 73
        },
        {
            "title": "Generalizable Blood Cell Detection via Unified Dataset and Faster R-CNN",
            "authors": [
                "Siddharth Sahay"
            ],
            "arxiv_id": "2511.08465v1",
            "summary": "This paper presents a comprehensive methodology and comparative performance analysis for the automated classification and object detection of peripheral blood cells (PBCs) in microscopic images. Addressing the critical challenge of data scarcity and heterogeneity, robust data pipeline was first developed to standardize and merge four public datasets (PBC, BCCD, Chula, Sickle Cell) into a unified resource. Then employed a state-of-the-art Faster R-CNN object detection framework, leveraging a ResNet-50-FPN backbone. Comparative training rigorously evaluated a randomly initialized baseline model (Regimen 1) against a Transfer Learning Regimen (Regimen 2), initialized with weights pre-trained on the Microsoft COCO dataset. The results demonstrate that the Transfer Learning approach achieved significantly faster convergence and superior stability, culminating in a final validation loss of 0.08666, a substantial improvement over the baseline. This validated methodology establishes a robust foundation for building high-accuracy, deployable systems for automated hematological diagnosis.",
            "headline_zh": "提出统一数据集与Faster R-CNN方法以解决血细胞检测中的数据稀缺与异质性问题",
            "intro_zh": [
                "核心问题：血细胞检测中数据稀缺和异质性阻碍模型泛化",
                "方法要点：整合四个公共数据集，采用Faster R-CNN与ResNet-50-FPN骨干网络",
                "实验或效果：迁移学习方案收敛更快，验证损失降至0.08666，优于基线"
            ],
            "tags_zh": [
                "血细胞检测",
                "统一数据集",
                "Faster R-CNN",
                "迁移学习",
                "ResNet-50-FPN",
                "验证损失"
            ],
            "_index": 74
        },
        {
            "title": "Contrastive Integrated Gradients: A Feature Attribution-Based Method for Explaining Whole Slide Image Classification",
            "authors": [
                "Anh Mai Vu",
                "Tuan L. Vo",
                "Ngoc Lam Quang Bui",
                "Nam Nguyen Le Binh",
                "Akash Awasthi",
                "Huy Quoc Vo",
                "Thanh-Huy Nguyen",
                "Zhu Han",
                "Chandra Mohan",
                "Hien Van Nguyen"
            ],
            "arxiv_id": "2511.08464v1",
            "summary": "Interpretability is essential in Whole Slide Image (WSI) analysis for computational pathology, where understanding model predictions helps build trust in AI-assisted diagnostics. While Integrated Gradients (IG) and related attribution methods have shown promise, applying them directly to WSIs introduces challenges due to their high-resolution nature. These methods capture model decision patterns but may overlook class-discriminative signals that are crucial for distinguishing between tumor subtypes. In this work, we introduce Contrastive Integrated Gradients (CIG), a novel attribution method that enhances interpretability by computing contrastive gradients in logit space. First, CIG highlights class-discriminative regions by comparing feature importance relative to a reference class, offering sharper differentiation between tumor and non-tumor areas. Second, CIG satisfies the axioms of integrated attribution, ensuring consistency and theoretical soundness. Third, we propose two attribution quality metrics, MIL-AIC and MIL-SIC, which measure how predictive information and model confidence evolve with access to salient regions, particularly under weak supervision. We validate CIG across three datasets spanning distinct cancer types: CAMELYON16 (breast cancer metastasis in lymph nodes), TCGA-RCC (renal cell carcinoma), and TCGA-Lung (lung cancer). Experimental results demonstrate that CIG yields more informative attributions both quantitatively, using MIL-AIC and MIL-SIC, and qualitatively, through visualizations that align closely with ground truth tumor regions, underscoring its potential for interpretable and trustworthy WSI-based diagnostics",
            "headline_zh": "提出对比集成梯度方法以增强全切片图像分类的可解释性",
            "intro_zh": [
                "全切片图像分析中，现有归因方法可能忽略类间判别信号，影响肿瘤亚型区分。",
                "CIG在logit空间计算对比梯度，突出类判别区域，满足集成归因公理。",
                "在多个癌症数据集上验证，CIG提供更信息化的归因，定量和定性评估均优于基线。"
            ],
            "tags_zh": [
                "全切片图像分析",
                "特征归因方法",
                "对比集成梯度",
                "可解释性",
                "计算病理学",
                "弱监督学习"
            ],
            "_index": 75
        },
        {
            "title": "Virtual Traffic Lights for Multi-Robot Navigation: Decentralized Planning with Centralized Conflict Resolution",
            "authors": [
                "Sagar Gupta",
                "Thanh Vinh Nguyen",
                "Thieu Long Phan",
                "Vidul Attri",
                "Archit Gupta",
                "Niroshinie Fernando",
                "Kevin Lee",
                "Seng W. Loke",
                "Ronny Kutadinata",
                "Benjamin Champion",
                "Akansel Cosgun"
            ],
            "arxiv_id": "2511.07811v1",
            "summary": "We present a hybrid multi-robot coordination framework that combines decentralized path planning with centralized conflict resolution. In our approach, each robot autonomously plans its path and shares this information with a centralized node. The centralized system detects potential conflicts and allows only one of the conflicting robots to proceed at a time, instructing others to stop outside the conflicting area to avoid deadlocks. Unlike traditional centralized planning methods, our system does not dictate robot paths but instead provides stop commands, functioning as a virtual traffic light. In simulation experiments with multiple robots, our approach increased the success rate of robots reaching their goals while reducing deadlocks. Furthermore, we successfully validated the system in real-world experiments with two quadruped robots and separately with wheeled Duckiebots.",
            "headline_zh": "提出混合多机器人协调框架，结合分散路径规划与集中冲突解决以提高导航成功率",
            "intro_zh": [
                "核心问题：多机器人导航中潜在冲突导致死锁和效率低下",
                "方法要点：机器人自主规划路径，集中系统检测冲突并发出停止指令",
                "实验或效果：仿真和真实实验显示成功率高、死锁减少"
            ],
            "tags_zh": [
                "多机器人导航",
                "分散规划",
                "集中冲突解决",
                "虚拟交通灯",
                "死锁避免"
            ],
            "_index": 76
        },
        {
            "title": "USV Obstacles Detection and Tracking in Marine Environments",
            "authors": [
                "Yara AlaaEldin",
                "Enrico Simetti",
                "Francesca Odone"
            ],
            "arxiv_id": "2511.07950v1",
            "summary": "Developing a robust and effective obstacle detection and tracking system for Unmanned Surface Vehicle (USV) at marine environments is a challenging task. Research efforts have been made in this area during the past years by GRAAL lab at the university of Genova that resulted in a methodology for detecting and tracking obstacles on the image plane and, then, locating them in the 3D LiDAR point cloud. In this work, we continue on the developed system by, firstly, evaluating its performance on recently published marine datasets. Then, we integrate the different blocks of the system on ROS platform where we could test it in real-time on synchronized LiDAR and camera data collected in various marine conditions available in the MIT marine datasets. We present a thorough experimental analysis of the results obtained using two approaches; one that uses sensor fusion between the camera and LiDAR to detect and track the obstacles and the other uses only the LiDAR point cloud for the detection and tracking. In the end, we propose a hybrid approach that merges the advantages of both approaches to build an informative obstacles map of the surrounding environment to the USV.",
            "headline_zh": "提出混合方法融合相机与LiDAR，提升USV在海洋环境中的障碍物检测与跟踪",
            "intro_zh": [
                "核心问题：海洋环境中USV障碍物检测与跟踪的鲁棒性挑战。",
                "方法要点：集成相机与LiDAR传感器融合，并在ROS平台上实时测试。",
                "实验或效果：评估两种方法性能，提出混合方法构建环境障碍物地图。"
            ],
            "tags_zh": [
                "障碍物检测",
                "传感器融合",
                "ROS平台",
                "海洋环境",
                "LiDAR点云",
                "实时跟踪"
            ],
            "_index": 77
        },
        {
            "title": "Class-feature Watermark: A Resilient Black-box Watermark Against Model Extraction Attacks",
            "authors": [
                "Yaxin Xiao",
                "Qingqing Ye",
                "Zi Liang",
                "Haoyang Li",
                "RongHua Li",
                "Huadi Zheng",
                "Haibo Hu"
            ],
            "arxiv_id": "2511.07947v1",
            "summary": "Machine learning models constitute valuable intellectual property, yet remain vulnerable to model extraction attacks (MEA), where adversaries replicate their functionality through black-box queries. Model watermarking counters MEAs by embedding forensic markers for ownership verification. Current black-box watermarks prioritize MEA survival through representation entanglement, yet inadequately explore resilience against sequential MEAs and removal attacks. Our study reveals that this risk is underestimated because existing removal methods are weakened by entanglement. To address this gap, we propose Watermark Removal attacK (WRK), which circumvents entanglement constraints by exploiting decision boundaries shaped by prevailing sample-level watermark artifacts. WRK effectively reduces watermark success rates by at least 88.79% across existing watermarking benchmarks.\n  For robust protection, we propose Class-Feature Watermarks (CFW), which improve resilience by leveraging class-level artifacts. CFW constructs a synthetic class using out-of-domain samples, eliminating vulnerable decision boundaries between original domain samples and their artifact-modified counterparts (watermark samples). CFW concurrently optimizes both MEA transferability and post-MEA stability. Experiments across multiple domains show that CFW consistently outperforms prior methods in resilience, maintaining a watermark success rate of at least 70.15% in extracted models even under the combined MEA and WRK distortion, while preserving the utility of protected models.",
            "headline_zh": "提出类特征水印以增强模型提取攻击下的黑盒水印鲁棒性",
            "intro_zh": [
                "核心问题：现有黑盒水印在序列模型提取和移除攻击下鲁棒性不足，风险被低估。",
                "方法要点：CFW利用类级伪影构建合成类，消除易受攻击的决策边界。",
                "实验效果：CFW在多种领域保持至少70.15%水印成功率，优于现有方法。"
            ],
            "tags_zh": [
                "模型水印",
                "模型提取攻击",
                "黑盒水印",
                "鲁棒性",
                "类特征水印",
                "移除攻击"
            ],
            "_index": 78
        },
        {
            "title": "IBMA: An Imputation-Based Mixup Augmentation Using Self-Supervised Learning for Time Series Data",
            "authors": [
                "Dang Nha Nguyen",
                "Hai Dang Nguyen",
                "Khoa Tho Anh Nguyen"
            ],
            "arxiv_id": "2511.07930v1",
            "summary": "Data augmentation in time series forecasting plays a crucial role in enhancing model performance by introducing variability while maintaining the underlying temporal patterns. However, time series data offers fewer augmentation strategies compared to fields such as image or text, with advanced techniques like Mixup rarely being used. In this work, we propose a novel approach, Imputation-Based Mixup Augmentation (IBMA), which combines Imputation-Augmented data with Mixup augmentation to bolster model generalization and improve forecasting performance. We evaluate the effectiveness of this method across several forecasting models, including DLinear (MLP), TimesNet (CNN), and iTrainformer (Transformer), these models represent some of the most recent advances in time series forecasting. Our experiments, conducted on four datasets (ETTh1, ETTh2, ETTm1, ETTm2) and compared against eight other augmentation techniques, demonstrate that IBMA consistently enhances performance, achieving 22 improvements out of 24 instances, with 10 of those being the best performances, particularly with iTrainformer imputation.",
            "headline_zh": "提出基于插值的混合增强方法，用于提升时间序列预测性能。",
            "intro_zh": [
                "时间序列数据增强策略较少，混合增强等先进技术应用不足。",
                "结合插值增强与混合增强，提高模型泛化能力。",
                "在多个数据集和模型上测试，IBMA在24个实例中22次提升性能。"
            ],
            "tags_zh": [
                "时间序列预测",
                "数据增强",
                "混合增强",
                "插值方法",
                "自监督学习"
            ],
            "_index": 79
        },
        {
            "title": "DynaQuant: Dynamic Mixed-Precision Quantization for Learned Image Compression",
            "authors": [
                "Youneng Bao",
                "Yulong Cheng",
                "Yiping Liu",
                "Yichen Yang",
                "Peng Qin",
                "Mu Li",
                "Yongsheng Liang"
            ],
            "arxiv_id": "2511.07903v1",
            "summary": "Prevailing quantization techniques in Learned Image Compression (LIC) typically employ a static, uniform bit-width across all layers, failing to adapt to the highly diverse data distributions and sensitivity characteristics inherent in LIC models. This leads to a suboptimal trade-off between performance and efficiency. In this paper, we introduce DynaQuant, a novel framework for dynamic mixed-precision quantization that operates on two complementary levels. First, we propose content-aware quantization, where learnable scaling and offset parameters dynamically adapt to the statistical variations of latent features. This fine-grained adaptation is trained end-to-end using a novel Distance-aware Gradient Modulator (DGM), which provides a more informative learning signal than the standard Straight-Through Estimator. Second, we introduce a data-driven, dynamic bit-width selector that learns to assign an optimal bit precision to each layer, dynamically reconfiguring the network's precision profile based on the input data. Our fully dynamic approach offers substantial flexibility in balancing rate-distortion (R-D) performance and computational cost. Experiments demonstrate that DynaQuant achieves rd performance comparable to full-precision models while significantly reducing computational and storage requirements, thereby enabling the practical deployment of advanced LIC on diverse hardware platforms.",
            "headline_zh": "提出动态混合精度量化框架以优化学习图像压缩的性能与效率平衡",
            "intro_zh": [
                "静态统一比特宽度无法适应学习图像压缩模型的多样数据分布和敏感性",
                "结合内容感知量化和动态比特宽度选择器，实现端到端训练",
                "实验显示在保持率失真性能的同时显著降低计算和存储需求"
            ],
            "tags_zh": [
                "学习图像压缩",
                "动态量化",
                "混合精度",
                "率失真优化",
                "内容感知",
                "比特宽度选择"
            ],
            "_index": 80
        },
        {
            "title": "High-Altitude Balloon Station-Keeping with First Order Model Predictive Control",
            "authors": [
                "Myles Pasetsky",
                "Jiawei Lin",
                "Bradley Guo",
                "Sarah Dean"
            ],
            "arxiv_id": "2511.07761v1",
            "summary": "High-altitude balloons (HABs) are common in scientific research due to their wide range of applications and low cost. Because of their nonlinear, underactuated dynamics and the partial observability of wind fields, prior work has largely relied on model-free reinforcement learning (RL) methods to design near-optimal control schemes for station-keeping. These methods often compare only against hand-crafted heuristics, dismissing model-based approaches as impractical given the system complexity and uncertain wind forecasts. We revisit this assumption about the efficacy of model-based control for station-keeping by developing First-Order Model Predictive Control (FOMPC). By implementing the wind and balloon dynamics as differentiable functions in JAX, we enable gradient-based trajectory optimization for online planning. FOMPC outperforms a state-of-the-art RL policy, achieving a 24% improvement in time-within-radius (TWR) without requiring offline training, though at the cost of greater online computation per control step. Through systematic ablations of modeling assumptions and control factors, we show that online planning is effective across many configurations, including under simplified wind and dynamics models.",
            "headline_zh": "提出一阶模型预测控制以解决高空气球定点保持问题",
            "intro_zh": [
                "高空气球非线性、欠驱动且风场部分可观测，传统方法依赖无模型强化学习",
                "开发一阶模型预测控制，使用JAX实现可微分风与气球动力学，支持在线梯度优化",
                "实验显示优于强化学习策略，时间在半径内指标提升24%，但在线计算成本较高"
            ],
            "tags_zh": [
                "高空气球控制",
                "模型预测控制",
                "可微分优化",
                "在线规划",
                "风场建模"
            ],
            "_index": 81
        },
        {
            "title": "Navigating the Wild: Pareto-Optimal Visual Decision-Making in Image Space",
            "authors": [
                "Durgakant Pushp",
                "Weizhe Chen",
                "Zheng Chen",
                "Chaomin Luo",
                "Jason M. Gregory",
                "Lantao Liu"
            ],
            "arxiv_id": "2511.07750v1",
            "summary": "Navigating complex real-world environments requires semantic understanding and adaptive decision-making. Traditional reactive methods without maps often fail in cluttered settings, map-based approaches demand heavy mapping effort, and learning-based solutions rely on large datasets with limited generalization. To address these challenges, we present Pareto-Optimal Visual Navigation, a lightweight image-space framework that combines data-driven semantics, Pareto-optimal decision-making, and visual servoing for real-time navigation.",
            "headline_zh": "提出Pareto最优视觉导航框架，结合语义、决策和视觉伺服，实现实时复杂环境导航。",
            "intro_zh": [
                "核心问题：传统导航方法在复杂环境中存在失败、高成本或泛化性差的问题。",
                "方法要点：在图像空间中集成数据驱动语义、Pareto最优决策和视觉伺服。",
                "实验或效果：未知，但声称轻量级且适用于实时导航。"
            ],
            "tags_zh": [
                "视觉导航",
                "Pareto最优决策",
                "图像空间处理",
                "语义理解",
                "实时系统"
            ],
            "_index": 82
        },
        {
            "title": "From Exploration to Exploitation: A Two-Stage Entropy RLVR Approach for Noise-Tolerant MLLM Training",
            "authors": [
                "Donglai Xu",
                "Hongzheng Yang",
                "Yuzhi Zhao",
                "Pingping Zhang",
                "Jinpeng Chen",
                "Wenao Ma",
                "Zhijian Hou",
                "Mengyang Wu",
                "Xiaolei Li",
                "Senkang Hu",
                "Ziyi Guan",
                "Jason Chun Lok Li",
                "Lai Man Po"
            ],
            "arxiv_id": "2511.07738v1",
            "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) for Multimodal Large Language Models (MLLMs) is highly dependent on high-quality labeled data, which is often scarce and prone to substantial annotation noise in real-world scenarios. Existing unsupervised RLVR methods, including pure entropy minimization, can overfit to incorrect labels and limit the crucial reward ranking signal for Group-Relative Policy Optimization (GRPO). To address these challenges and enhance noise tolerance, we propose a novel two-stage, token-level entropy optimization method for RLVR. This approach dynamically guides the model from exploration to exploitation during training. In the initial exploration phase, token-level entropy maximization promotes diverse and stochastic output generation, serving as a strong regularizer that prevents premature convergence to noisy labels and ensures sufficient intra-group variation, which enables more reliable reward gradient estimation in GRPO. As training progresses, the method transitions into the exploitation phase, where token-level entropy minimization encourages the model to produce confident and deterministic outputs, thereby consolidating acquired knowledge and refining prediction accuracy. Empirically, across three MLLM backbones - Qwen2-VL-2B, Qwen2-VL-7B, and Qwen2.5-VL-3B - spanning diverse noise settings and multiple tasks, our phased strategy consistently outperforms prior approaches by unifying and enhancing external, internal, and entropy-based methods, delivering robust and superior performance across the board.",
            "headline_zh": "提出两阶段熵优化方法以增强噪声容忍的MLLM强化学习训练",
            "intro_zh": [
                "核心问题：MLLM强化学习依赖高质量标注数据，但现实数据噪声大易导致过拟合",
                "方法要点：分探索与利用阶段，动态调整词级熵以平衡多样性与确定性输出",
                "实验或效果：在多个MLLM骨干和噪声设置下，性能优于现有方法，提升鲁棒性"
            ],
            "tags_zh": [
                "强化学习",
                "多模态大语言模型",
                "噪声容忍训练",
                "熵优化",
                "两阶段策略",
                "GRPO"
            ],
            "_index": 83
        },
        {
            "title": "Multistep Quasimetric Learning for Scalable Goal-conditioned Reinforcement Learning",
            "authors": [
                "Bill Chunyuan Zheng",
                "Vivek Myers",
                "Benjamin Eysenbach",
                "Sergey Levine"
            ],
            "arxiv_id": "2511.07730v1",
            "summary": "Learning how to reach goals in an environment is a longstanding challenge in AI, yet reasoning over long horizons remains a challenge for modern methods. The key question is how to estimate the temporal distance between pairs of observations. While temporal difference methods leverage local updates to provide optimality guarantees, they often perform worse than Monte Carlo methods that perform global updates (e.g., with multi-step returns), which lack such guarantees. We show how these approaches can be integrated into a practical GCRL method that fits a quasimetric distance using a multistep Monte-Carlo return. We show our method outperforms existing GCRL methods on long-horizon simulated tasks with up to 4000 steps, even with visual observations. We also demonstrate that our method can enable stitching in the real-world robotic manipulation domain (Bridge setup). Our approach is the first end-to-end GCRL method that enables multistep stitching in this real-world manipulation domain from an unlabeled offline dataset of visual observations.",
            "headline_zh": "提出多步拟度量学习以解决长视野目标条件强化学习中的距离估计问题",
            "intro_zh": [
                "核心问题：长视野任务中估计观测对之间的时间距离，现有方法在最优性与性能间存在权衡",
                "方法要点：结合多步蒙特卡洛回报拟合拟度量距离，实现端到端目标条件强化学习",
                "实验或效果：在模拟任务中优于现有方法，并在真实机器人操作中实现多步拼接"
            ],
            "tags_zh": [
                "目标条件强化学习",
                "拟度量学习",
                "多步回报",
                "长视野任务",
                "机器人操作",
                "离线数据集"
            ],
            "_index": 84
        },
        {
            "title": "LLM-GROP: Visually Grounded Robot Task and Motion Planning with Large Language Models",
            "authors": [
                "Xiaohan Zhang",
                "Yan Ding",
                "Yohei Hayamizu",
                "Zainab Altaweel",
                "Yifeng Zhu",
                "Yuke Zhu",
                "Peter Stone",
                "Chris Paxton",
                "Shiqi Zhang"
            ],
            "arxiv_id": "2511.07727v1",
            "summary": "Task planning and motion planning are two of the most important problems in robotics, where task planning methods help robots achieve high-level goals and motion planning methods maintain low-level feasibility. Task and motion planning (TAMP) methods interleave the two processes of task planning and motion planning to ensure goal achievement and motion feasibility. Within the TAMP context, we are concerned with the mobile manipulation (MoMa) of multiple objects, where it is necessary to interleave actions for navigation and manipulation.\n  In particular, we aim to compute where and how each object should be placed given underspecified goals, such as ``set up dinner table with a fork, knife and plate.'' We leverage the rich common sense knowledge from large language models (LLMs), e.g., about how tableware is organized, to facilitate both task-level and motion-level planning. In addition, we use computer vision methods to learn a strategy for selecting base positions to facilitate MoMa behaviors, where the base position corresponds to the robot's ``footprint'' and orientation in its operating space. Altogether, this article provides a principled TAMP framework for MoMa tasks that accounts for common sense about object rearrangement and is adaptive to novel situations that include many objects that need to be moved. We performed quantitative experiments in both real-world settings and simulated environments. We evaluated the success rate and efficiency in completing long-horizon object rearrangement tasks. While the robot completed 84.4\\% real-world object rearrangement trials, subjective human evaluations indicated that the robot's performance is still lower than experienced human waiters.",
            "headline_zh": "提出LLM-GROP框架，结合大语言模型与视觉方法解决移动操作任务规划问题",
            "intro_zh": [
                "核心问题：移动操作中任务与运动规划的结合，处理多物体放置的模糊目标",
                "方法要点：利用大语言模型常识知识指导任务规划，视觉方法选择机器人基位置",
                "实验或效果：真实世界实验成功率84.4%，但性能低于人类服务员"
            ],
            "tags_zh": [
                "任务与运动规划",
                "大语言模型",
                "移动操作",
                "物体重排",
                "视觉基础",
                "机器人导航"
            ],
            "_index": 85
        },
        {
            "title": "KPLM-STA: Physically-Accurate Shadow Synthesis for Human Relighting via Keypoint-Based Light Modeling",
            "authors": [
                "Xinhui Yin",
                "Qifei Li",
                "Yilin Guo",
                "Hongxia Xie",
                "Xiaoli Zhang"
            ],
            "arxiv_id": "2511.08169v1",
            "summary": "Image composition aims to seamlessly integrate a foreground object into a background, where generating realistic and geometrically accurate shadows remains a persistent challenge. While recent diffusion-based methods have outperformed GAN-based approaches, existing techniques, such as the diffusion-based relighting framework IC-Light, still fall short in producing shadows with both high appearance realism and geometric precision, especially in composite images. To address these limitations, we propose a novel shadow generation framework based on a Keypoints Linear Model (KPLM) and a Shadow Triangle Algorithm (STA). KPLM models articulated human bodies using nine keypoints and one bounding block, enabling physically plausible shadow projection and dynamic shading across joints, thereby enhancing visual realism. STA further improves geometric accuracy by computing shadow angles, lengths, and spatial positions through explicit geometric formulations. Extensive experiments demonstrate that our method achieves state-of-the-art performance on shadow realism benchmarks, particularly under complex human poses, and generalizes effectively to multi-directional relighting scenarios such as those supported by IC-Light.",
            "headline_zh": "提出KPLM-STA框架以解决图像合成中人体阴影生成的真实性与几何精度问题",
            "intro_zh": [
                "核心问题：现有方法在图像合成中难以生成外观真实且几何精确的阴影，尤其在复杂人体姿态下。",
                "方法要点：使用关键点线性模型和阴影三角算法，实现物理准确的阴影投影和几何计算。",
                "实验或效果：在阴影真实度基准测试中达到最优性能，并泛化到多方向重光照场景。"
            ],
            "tags_zh": [
                "阴影合成",
                "关键点建模",
                "几何算法",
                "图像重光照",
                "人体姿态处理"
            ],
            "_index": 86
        },
        {
            "title": "Multi-Granularity Mutual Refinement Network for Zero-Shot Learning",
            "authors": [
                "Ning Wang",
                "Long Yu",
                "Cong Hua",
                "Guangming Zhu",
                "Lin Mei",
                "Syed Afaq Ali Shah",
                "Mohammed Bennamoun",
                "Liang Zhang"
            ],
            "arxiv_id": "2511.08163v1",
            "summary": "Zero-shot learning (ZSL) aims to recognize unseen classes with zero samples by transferring semantic knowledge from seen classes. Current approaches typically correlate global visual features with semantic information (i.e., attributes) or align local visual region features with corresponding attributes to enhance visual-semantic interactions. Although effective, these methods often overlook the intrinsic interactions between local region features, which can further improve the acquisition of transferable and explicit visual features. In this paper, we propose a network named Multi-Granularity Mutual Refinement Network (Mg-MRN), which refine discriminative and transferable visual features by learning decoupled multi-granularity features and cross-granularity feature interactions. Specifically, we design a multi-granularity feature extraction module to learn region-level discriminative features through decoupled region feature mining. Then, a cross-granularity feature fusion module strengthens the inherent interactions between region features of varying granularities. This module enhances the discriminability of representations at each granularity level by integrating region representations from adjacent hierarchies, further improving ZSL recognition performance. Extensive experiments on three popular ZSL benchmark datasets demonstrate the superiority and competitiveness of our proposed Mg-MRN method. Our code is available at https://github.com/NingWang2049/Mg-MRN.",
            "headline_zh": "提出多粒度互精炼网络以增强零样本学习中的视觉特征可迁移性",
            "intro_zh": [
                "核心问题：现有零样本学习方法忽视局部区域特征间的内在交互，影响视觉特征可迁移性。",
                "方法要点：通过解耦多粒度特征提取和跨粒度特征融合，强化区域特征的判别性和交互。",
                "实验或效果：在三个基准数据集上验证了方法的优越性和竞争力，代码已开源。"
            ],
            "tags_zh": [
                "零样本学习",
                "多粒度特征",
                "视觉语义交互",
                "特征融合",
                "区域特征挖掘"
            ],
            "_index": 87
        },
        {
            "title": "LandSegmenter: Towards a Flexible Foundation Model for Land Use and Land Cover Mapping",
            "authors": [
                "Chenying Liu",
                "Wei Huang",
                "Xiao Xiang Zhu"
            ],
            "arxiv_id": "2511.08156v1",
            "summary": "Land Use and Land Cover (LULC) mapping is a fundamental task in Earth Observation (EO). However, current LULC models are typically developed for a specific modality and a fixed class taxonomy, limiting their generability and broader applicability. Recent advances in foundation models (FMs) offer promising opportunities for building universal models. Yet, task-agnostic FMs often require fine-tuning for downstream applications, whereas task-specific FMs rely on massive amounts of labeled data for training, which is costly and impractical in the remote sensing (RS) domain. To address these challenges, we propose LandSegmenter, an LULC FM framework that resolves three-stage challenges at the input, model, and output levels. From the input side, to alleviate the heavy demand on labeled data for FM training, we introduce LAnd Segment (LAS), a large-scale, multi-modal, multi-source dataset built primarily with globally sampled weak labels from existing LULC products. LAS provides a scalable, cost-effective alternative to manual annotation, enabling large-scale FM training across diverse LULC domains. For model architecture, LandSegmenter integrates an RS-specific adapter for cross-modal feature extraction and a text encoder for semantic awareness enhancement. At the output stage, we introduce a class-wise confidence-guided fusion strategy to mitigate semantic omissions and further improve LandSegmenter's zero-shot performance. We evaluate LandSegmenter on six precisely annotated LULC datasets spanning diverse modalities and class taxonomies. Extensive transfer learning and zero-shot experiments demonstrate that LandSegmenter achieves competitive or superior performance, particularly in zero-shot settings when transferred to unseen datasets. These results highlight the efficacy of our proposed framework and the utility of weak supervision for building task-specific FMs.",
            "headline_zh": "提出LandSegmenter框架以解决土地利用与覆盖映射中模型泛化性差和数据标注成本高的问题",
            "intro_zh": [
                "核心问题：现有LULC模型依赖特定模态和固定分类体系，泛化性差，且任务无关基础模型需微调，任务特定模型需大量标注数据",
                "方法要点：构建弱标签数据集LAS，集成遥感适配器和文本编码器，采用置信度引导融合策略提升零样本性能",
                "实验或效果：在六个数据集上评估，零样本和迁移学习表现优异，弱监督有效构建任务特定基础模型"
            ],
            "tags_zh": [
                "土地利用与覆盖映射",
                "基础模型",
                "弱监督学习",
                "多模态遥感",
                "零样本学习",
                "语义分割"
            ],
            "_index": 88
        },
        {
            "title": "Boomda: Balanced Multi-objective Optimization for Multimodal Domain Adaptation",
            "authors": [
                "Jun Sun",
                "Xinxin Zhang",
                "Simin Hong",
                "Jian Zhu",
                "Xiang Gao"
            ],
            "arxiv_id": "2511.08152v1",
            "summary": "Multimodal learning, while contributing to numerous success stories across various fields, faces the challenge of prohibitively expensive manual annotation. To address the scarcity of annotated data, a popular solution is unsupervised domain adaptation, which has been extensively studied in unimodal settings yet remains less explored in multimodal settings. In this paper, we investigate heterogeneous multimodal domain adaptation, where the primary challenge is the varying domain shifts of different modalities from the source to the target domain. We first introduce the information bottleneck method to learn representations for each modality independently, and then match the source and target domains in the representation space with correlation alignment. To balance the domain alignment of all modalities, we formulate the problem as a multi-objective task, aiming for a Pareto optimal solution. By exploiting the properties specific to our model, the problem can be simplified to a quadratic programming problem. Further approximation yields a closed-form solution, leading to an efficient modality-balanced multimodal domain adaptation algorithm. The proposed method features \\textbf{B}alanced multi-\\textbf{o}bjective \\textbf{o}ptimization for \\textbf{m}ultimodal \\textbf{d}omain \\textbf{a}daptation, termed \\textbf{Boomda}. Extensive empirical results showcase the effectiveness of the proposed approach and demonstrate that Boomda outperforms the competing schemes. The code is is available at: https://github.com/sunjunaimer/Boomda.git.",
            "headline_zh": "提出Boomda方法以平衡多模态领域适应中的多目标优化问题",
            "intro_zh": [
                "核心问题：多模态学习中不同模态的领域偏移不一致，导致无监督领域适应困难",
                "方法要点：使用信息瓶颈和相关性对齐，将多目标优化简化为二次规划问题求解",
                "实验或效果：在多个数据集上验证，Boomda优于现有方法，代码已开源"
            ],
            "tags_zh": [
                "多模态学习",
                "无监督领域适应",
                "多目标优化",
                "信息瓶颈",
                "相关性对齐"
            ],
            "_index": 89
        },
        {
            "title": "Radar-APLANC: Unsupervised Radar-based Heartbeat Sensing via Augmented Pseudo-Label and Noise Contrast",
            "authors": [
                "Ying Wang",
                "Zhaodong Sun",
                "Xu Cheng",
                "Zuxian He",
                "Xiaobai Li"
            ],
            "arxiv_id": "2511.08071v1",
            "summary": "Frequency Modulated Continuous Wave (FMCW) radars can measure subtle chest wall oscillations to enable non-contact heartbeat sensing. However, traditional radar-based heartbeat sensing methods face performance degradation due to noise. Learning-based radar methods achieve better noise robustness but require costly labeled signals for supervised training. To overcome these limitations, we propose the first unsupervised framework for radar-based heartbeat sensing via Augmented Pseudo-Label and Noise Contrast (Radar-APLANC). We propose to use both the heartbeat range and noise range within the radar range matrix to construct the positive and negative samples, respectively, for improved noise robustness. Our Noise-Contrastive Triplet (NCT) loss only utilizes positive samples, negative samples, and pseudo-label signals generated by the traditional radar method, thereby avoiding dependence on expensive ground-truth physiological signals. We further design a pseudo-label augmentation approach featuring adaptive noise-aware label selection to improve pseudo-label signal quality. Extensive experiments on the Equipleth dataset and our collected radar dataset demonstrate that our unsupervised method achieves performance comparable to state-of-the-art supervised methods. Our code, dataset, and supplementary materials can be accessed from https://github.com/RadarHRSensing/Radar-APLANC.",
            "headline_zh": "提出Radar-APLANC无监督框架，通过增强伪标签和噪声对比实现雷达心跳感知。",
            "intro_zh": [
                "核心问题：雷达心跳感知易受噪声影响，监督方法依赖昂贵标注信号。",
                "方法要点：利用心跳和噪声范围构建正负样本，设计噪声对比三元损失。",
                "实验或效果：在Equipleth和自收集数据集上，性能媲美监督方法。"
            ],
            "tags_zh": [
                "雷达心跳感知",
                "无监督学习",
                "噪声对比",
                "伪标签增强",
                "FMCW雷达"
            ],
            "_index": 90
        },
        {
            "title": "I2E: Real-Time Image-to-Event Conversion for High-Performance Spiking Neural Networks",
            "authors": [
                "Ruichen Ma",
                "Liwei Meng",
                "Guanchao Qiao",
                "Ning Ning",
                "Yang Liu",
                "Shaogang Hu"
            ],
            "arxiv_id": "2511.08065v1",
            "summary": "Spiking neural networks (SNNs) promise highly energy-efficient computing, but their adoption is hindered by a critical scarcity of event-stream data. This work introduces I2E, an algorithmic framework that resolves this bottleneck by converting static images into high-fidelity event streams. By simulating microsaccadic eye movements with a highly parallelized convolution, I2E achieves a conversion speed over 300x faster than prior methods, uniquely enabling on-the-fly data augmentation for SNN training. The framework's effectiveness is demonstrated on large-scale benchmarks. An SNN trained on the generated I2E-ImageNet dataset achieves a state-of-the-art accuracy of 60.50%. Critically, this work establishes a powerful sim-to-real paradigm where pre-training on synthetic I2E data and fine-tuning on the real-world CIFAR10-DVS dataset yields an unprecedented accuracy of 92.5%. This result validates that synthetic event data can serve as a high-fidelity proxy for real sensor data, bridging a long-standing gap in neuromorphic engineering. By providing a scalable solution to the data problem, I2E offers a foundational toolkit for developing high-performance neuromorphic systems. The open-source algorithm and all generated datasets are provided to accelerate research in the field.",
            "headline_zh": "提出I2E框架将静态图像转换为事件流，以解决脉冲神经网络数据稀缺问题。",
            "intro_zh": [
                "核心问题：脉冲神经网络因事件流数据稀缺而应用受限。",
                "方法要点：通过模拟微扫视眼动，使用并行卷积实现高速图像到事件流转换。",
                "实验或效果：在ImageNet上达到60.50%准确率，CIFAR10-DVS上达92.5%，验证合成数据有效性。"
            ],
            "tags_zh": [
                "图像到事件转换",
                "脉冲神经网络",
                "数据增强",
                "神经形态工程",
                "合成事件数据",
                "高通量模拟"
            ],
            "_index": 91
        },
        {
            "title": "ProSona: Prompt-Guided Personalization for Multi-Expert Medical Image Segmentation",
            "authors": [
                "Aya Elgebaly",
                "Nikolaos Delopoulos",
                "Juliane Hörner-Rieber",
                "Carolin Rippke",
                "Sebastian Klüter",
                "Luca Boldrini",
                "Lorenzo Placidi",
                "Riccardo Dal Bello",
                "Nicolaus Andratschke",
                "Michael Baumgartl",
                "Claus Belka",
                "Christopher Kurz",
                "Guillaume Landry",
                "Shadi Albarqouni"
            ],
            "arxiv_id": "2511.08046v1",
            "summary": "Automated medical image segmentation suffers from high inter-observer variability, particularly in tasks such as lung nodule delineation, where experts often disagree. Existing approaches either collapse this variability into a consensus mask or rely on separate model branches for each annotator. We introduce ProSona, a two-stage framework that learns a continuous latent space of annotation styles, enabling controllable personalization via natural language prompts. A probabilistic U-Net backbone captures diverse expert hypotheses, while a prompt-guided projection mechanism navigates this latent space to generate personalized segmentations. A multi-level contrastive objective aligns textual and visual representations, promoting disentangled and interpretable expert styles. Across the LIDC-IDRI lung nodule and multi-institutional prostate MRI datasets, ProSona reduces the Generalized Energy Distance by 17% and improves mean Dice by more than one point compared with DPersona. These results demonstrate that natural-language prompts can provide flexible, accurate, and interpretable control over personalized medical image segmentation. Our implementation is available online 1 .",
            "headline_zh": "提出ProSona框架，通过自然语言提示实现多专家医学图像分割的个性化控制。",
            "intro_zh": [
                "医学图像分割存在高观察者间变异性，如肺结节勾画中专家意见分歧。",
                "方法使用概率U-Net学习注释风格潜空间，结合提示引导投影生成个性化分割。",
                "在LIDC-IDRI和前列腺MRI数据集上，比DPersona降低广义能量距离17%，提升Dice分数。"
            ],
            "tags_zh": [
                "医学图像分割",
                "多专家个性化",
                "自然语言提示",
                "潜空间学习",
                "对比学习目标"
            ],
            "_index": 92
        },
        {
            "title": "DANCE: Density-agnostic and Class-aware Network for Point Cloud Completion",
            "authors": [
                "Da-Yeong Kim",
                "Yeong-Jun Cho"
            ],
            "arxiv_id": "2511.07978v1",
            "summary": "Point cloud completion aims to recover missing geometric structures from incomplete 3D scans, which often suffer from occlusions or limited sensor viewpoints. Existing methods typically assume fixed input/output densities or rely on image-based representations, making them less suitable for real-world scenarios with variable sparsity and limited supervision. In this paper, we introduce Density-agnostic and Class-aware Network (DANCE), a novel framework that completes only the missing regions while preserving the observed geometry. DANCE generates candidate points via ray-based sampling from multiple viewpoints. A transformer decoder then refines their positions and predicts opacity scores, which determine the validity of each point for inclusion in the final surface. To incorporate semantic guidance, a lightweight classification head is trained directly on geometric features, enabling category-consistent completion without external image supervision. Extensive experiments on the PCN and MVP benchmarks show that DANCE outperforms state-of-the-art methods in accuracy and structural consistency, while remaining robust to varying input densities and noise levels.",
            "headline_zh": "提出DANCE网络以解决点云补全中密度变化和语义一致性问题",
            "intro_zh": [
                "核心问题：点云补全在输入密度可变和有限监督下难以保持几何结构。",
                "方法要点：使用射线采样生成候选点，Transformer解码器优化位置和预测不透明度。",
                "实验或效果：在PCN和MVP基准上优于现有方法，对密度变化和噪声鲁棒。"
            ],
            "tags_zh": [
                "点云补全",
                "密度无关网络",
                "语义引导",
                "Transformer解码器",
                "射线采样"
            ],
            "_index": 93
        },
        {
            "title": "Aligning by Misaligning: Boundary-aware Curriculum Learning for Multimodal Alignment",
            "authors": [
                "Hua Ye",
                "Hang Ding",
                "Siyuan Chen",
                "Yiyang Jiang",
                "Changyuan Zhang",
                "Xuan Zhang"
            ],
            "arxiv_id": "2511.08399v1",
            "summary": "Most multimodal models treat every negative pair alike, ignoring the ambiguous negatives that differ from the positive by only a small detail. We propose Boundary-Aware Curriculum with Local Attention (BACL), a lightweight add-on that turns these borderline cases into a curriculum signal. A Boundary-aware Negative Sampler gradually raises difficulty, while a Contrastive Local Attention loss highlights where the mismatch occurs. The two modules are fully differentiable and work with any off-the-shelf dual encoder. Theory predicts a fast O(1/n) error rate; practice shows up to +32% R@1 over CLIP and new SOTA on four large-scale benchmarks, all without extra labels.",
            "headline_zh": "提出边界感知课程学习以改进多模态对齐，通过边界负采样和局部注意力损失提升性能。",
            "intro_zh": [
                "核心问题：多模态模型忽略模糊负样本，导致对齐不精确。",
                "方法要点：使用边界感知负采样器和对比局部注意力损失，构建课程学习信号。",
                "实验或效果：在四个基准测试中实现SOTA，召回率提升高达32%。"
            ],
            "tags_zh": [
                "多模态对齐",
                "课程学习",
                "边界感知",
                "对比学习",
                "负采样",
                "局部注意力"
            ],
            "_index": 94
        },
        {
            "title": "RAPTR: Radar-based 3D Pose Estimation using Transformer",
            "authors": [
                "Sorachi Kato",
                "Ryoma Yataka",
                "Pu Perry Wang",
                "Pedro Miraldo",
                "Takuya Fujihashi",
                "Petros Boufounos"
            ],
            "arxiv_id": "2511.08387v1",
            "summary": "Radar-based indoor 3D human pose estimation typically relied on fine-grained 3D keypoint labels, which are costly to obtain especially in complex indoor settings involving clutter, occlusions, or multiple people. In this paper, we propose \\textbf{RAPTR} (RAdar Pose esTimation using tRansformer) under weak supervision, using only 3D BBox and 2D keypoint labels which are considerably easier and more scalable to collect. Our RAPTR is characterized by a two-stage pose decoder architecture with a pseudo-3D deformable attention to enhance (pose/joint) queries with multi-view radar features: a pose decoder estimates initial 3D poses with a 3D template loss designed to utilize the 3D BBox labels and mitigate depth ambiguities; and a joint decoder refines the initial poses with 2D keypoint labels and a 3D gravity loss. Evaluated on two indoor radar datasets, RAPTR outperforms existing methods, reducing joint position error by $34.3\\%$ on HIBER and $76.9\\%$ on MMVR. Our implementation is available at https://github.com/merlresearch/radar-pose-transformer.",
            "headline_zh": "提出RAPTR以在弱监督下使用雷达进行室内3D人体姿态估计",
            "intro_zh": [
                "核心问题：雷达室内3D姿态估计依赖昂贵3D关键点标注，难以在复杂场景中扩展",
                "方法要点：采用两阶段解码器，结合伪3D可变形注意力，利用3D边界框和2D关键点标签",
                "实验或效果：在两个数据集上优于现有方法，关节位置误差显著降低"
            ],
            "tags_zh": [
                "雷达姿态估计",
                "弱监督学习",
                "3D人体姿态",
                "Transformer架构",
                "室内场景"
            ],
            "_index": 95
        },
        {
            "title": "VideoChain: A Transformer-Based Framework for Multi-hop Video Question Generation",
            "authors": [
                "Arpan Phukan",
                "Anupam Pandey",
                "Deepjyoti Bodo",
                "Asif Ekbal"
            ],
            "arxiv_id": "2511.08348v1",
            "summary": "Multi-hop Question Generation (QG) effectively evaluates reasoning but remains confined to text; Video Question Generation (VideoQG) is limited to zero-hop questions over single segments. To address this, we introduce VideoChain, a novel Multi-hop Video Question Generation (MVQG) framework designed to generate questions that require reasoning across multiple, temporally separated video segments. VideoChain features a modular architecture built on a modified BART backbone enhanced with video embeddings, capturing textual and visual dependencies. Using the TVQA+ dataset, we automatically construct the large-scale MVQ-60 dataset by merging zero-hop QA pairs, ensuring scalability and diversity. Evaluations show VideoChain's strong performance across standard generation metrics: ROUGE-L (0.6454), ROUGE-1 (0.6854), BLEU-1 (0.6711), BERTScore-F1 (0.7967), and semantic similarity (0.8110). These results highlight the model's ability to generate coherent, contextually grounded, and reasoning-intensive questions.",
            "headline_zh": "提出VideoChain框架以解决多跳视频问题生成任务",
            "intro_zh": [
                "核心问题：多跳问题生成局限于文本，视频问题生成仅支持零跳单片段",
                "方法要点：基于改进BART的模块化架构，融合视频嵌入捕获多模态依赖",
                "实验或效果：在MVQ-60数据集上评估，ROUGE-L达0.6454，生成问题连贯且推理密集"
            ],
            "tags_zh": [
                "多跳视频问题生成",
                "Transformer框架",
                "视频嵌入",
                "BART模型",
                "多模态推理",
                "TVQA+数据集"
            ],
            "_index": 96
        },
        {
            "title": "NeuSpring: Neural Spring Fields for Reconstruction and Simulation of Deformable Objects from Videos",
            "authors": [
                "Qingshan Xu",
                "Jiao Liu",
                "Shangshu Yu",
                "Yuxuan Wang",
                "Yuan Zhou",
                "Junbao Zhou",
                "Jiequan Cui",
                "Yew-Soon Ong",
                "Hanwang Zhang"
            ],
            "arxiv_id": "2511.08310v1",
            "summary": "In this paper, we aim to create physical digital twins of deformable objects under interaction. Existing methods focus more on the physical learning of current state modeling, but generalize worse to future prediction. This is because existing methods ignore the intrinsic physical properties of deformable objects, resulting in the limited physical learning in the current state modeling. To address this, we present NeuSpring, a neural spring field for the reconstruction and simulation of deformable objects from videos. Built upon spring-mass models for realistic physical simulation, our method consists of two major innovations: 1) a piecewise topology solution that efficiently models multi-region spring connection topologies using zero-order optimization, which considers the material heterogeneity of real-world objects. 2) a neural spring field that represents spring physical properties across different frames using a canonical coordinate-based neural network, which effectively leverages the spatial associativity of springs for physical learning. Experiments on real-world datasets demonstrate that our NeuSping achieves superior reconstruction and simulation performance for current state modeling and future prediction, with Chamfer distance improved by 20% and 25%, respectively.",
            "headline_zh": "提出NeuSpring神经弹簧场，从视频重建和模拟可变形物体，提升物理学习与预测性能。",
            "intro_zh": [
                "现有方法忽略可变形物体内在物理属性，导致当前状态建模物理学习有限，泛化预测差。",
                "NeuSpring结合弹簧质量模型，引入分段拓扑和神经弹簧场，建模多区域连接和弹簧物理属性。",
                "实验显示，在真实数据集上，当前状态和未来预测的Chamfer距离分别提升20%和25%。"
            ],
            "tags_zh": [
                "可变形物体重建",
                "神经弹簧场",
                "弹簧质量模型",
                "物理模拟",
                "视频分析"
            ],
            "_index": 97
        },
        {
            "title": "Accurate and Efficient Surface Reconstruction from Point Clouds via Geometry-Aware Local Adaptation",
            "authors": [
                "Eito Ogawa",
                "Taiga Hayami",
                "Hiroshi Watanabe"
            ],
            "arxiv_id": "2511.08233v1",
            "summary": "Point cloud surface reconstruction has improved in accuracy with advances in deep learning, enabling applications such as infrastructure inspection. Recent approaches that reconstruct from small local regions rather than entire point clouds have attracted attention for their strong generalization capability. However, prior work typically places local regions uniformly and keeps their size fixed, limiting adaptability to variations in geometric complexity. In this study, we propose a method that improves reconstruction accuracy and efficiency by adaptively modulating the spacing and size of local regions based on the curvature of the input point cloud.",
            "headline_zh": "提出基于曲率自适应调节局部区域的方法，以提高点云表面重建的精度和效率。",
            "intro_zh": [
                "核心问题：现有方法局部区域固定，难以适应几何复杂度变化。",
                "方法要点：根据点云曲率自适应调整局部区域的间距和大小。",
                "实验或效果：未知，但声称改进重建精度和效率。"
            ],
            "tags_zh": [
                "点云表面重建",
                "局部区域自适应",
                "曲率估计",
                "深度学习",
                "几何复杂度"
            ],
            "_index": 98
        },
        {
            "title": "3D4D: An Interactive, Editable, 4D World Model via 3D Video Generation",
            "authors": [
                "Yunhong He",
                "Zhengqing Yuan",
                "Zhengzhong Tu",
                "Yanfang Ye",
                "Lichao Sun"
            ],
            "arxiv_id": "2511.08536v1",
            "summary": "We introduce 3D4D, an interactive 4D visualization framework that integrates WebGL with Supersplat rendering. It transforms static images and text into coherent 4D scenes through four core modules and employs a foveated rendering strategy for efficient, real-time multi-modal interaction. This framework enables adaptive, user-driven exploration of complex 4D environments. The project page and code are available at https://yunhonghe1021.github.io/NOVA/.",
            "headline_zh": "提出3D4D交互式4D可视化框架，通过3D视频生成实现用户驱动的复杂环境探索。",
            "intro_zh": [
                "核心问题：如何从静态图像和文本生成交互式、可编辑的4D世界模型。",
                "方法要点：集成WebGL与Supersplat渲染，采用四模块和注视点渲染策略。",
                "实验或效果：实现高效实时多模态交互，支持自适应用户探索。"
            ],
            "tags_zh": [
                "4D可视化",
                "3D视频生成",
                "交互式渲染",
                "注视点渲染",
                "WebGL集成",
                "多模态交互"
            ],
            "_index": 99
        },
        {
            "title": "Large Sign Language Models: Toward 3D American Sign Language Translation",
            "authors": [
                "Sen Zhang",
                "Xiaoxiao He",
                "Di Liu",
                "Zhaoyang Xia",
                "Mingyu Zhao",
                "Chaowei Tan",
                "Vivian Li",
                "Bo Liu",
                "Dimitris N. Metaxas",
                "Mubbasir Kapadia"
            ],
            "arxiv_id": "2511.08535v1",
            "summary": "We present Large Sign Language Models (LSLM), a novel framework for translating 3D American Sign Language (ASL) by leveraging Large Language Models (LLMs) as the backbone, which can benefit hearing-impaired individuals' virtual communication. Unlike existing sign language recognition methods that rely on 2D video, our approach directly utilizes 3D sign language data to capture rich spatial, gestural, and depth information in 3D scenes. This enables more accurate and resilient translation, enhancing digital communication accessibility for the hearing-impaired community. Beyond the task of ASL translation, our work explores the integration of complex, embodied multimodal languages into the processing capabilities of LLMs, moving beyond purely text-based inputs to broaden their understanding of human communication. We investigate both direct translation from 3D gesture features to text and an instruction-guided setting where translations can be modulated by external prompts, offering greater flexibility. This work provides a foundational step toward inclusive, multimodal intelligent systems capable of understanding diverse forms of language.",
            "headline_zh": "提出大型手语模型框架，利用LLM处理3D美国手语以改善听障者虚拟通信",
            "intro_zh": [
                "核心问题：现有手语识别依赖2D视频，缺乏3D空间信息，影响翻译准确性。",
                "方法要点：直接使用3D手语数据，结合LLM处理空间、姿态和深度信息。",
                "实验或效果：探索从3D特征到文本的翻译，支持指令引导，增强翻译灵活性。"
            ],
            "tags_zh": [
                "3D手语翻译",
                "大型语言模型",
                "多模态处理",
                "听障通信",
                "手势识别"
            ],
            "_index": 100
        },
        {
            "title": "Fast Multi-Organ Fine Segmentation in CT Images with Hierarchical Sparse Sampling and Residual Transformer",
            "authors": [
                "Xueqi Guo",
                "Halid Ziya Yerebakan",
                "Yoshihisa Shinagawa",
                "Kritika Iyer",
                "Gerardo Hermosillo Valadez"
            ],
            "arxiv_id": "2511.08509v1",
            "summary": "Multi-organ segmentation of 3D medical images is fundamental with meaningful applications in various clinical automation pipelines. Although deep learning has achieved superior performance, the time and memory consumption of segmenting the entire 3D volume voxel by voxel using neural networks can be huge. Classifiers have been developed as an alternative in cases with certain points of interest, but the trade-off between speed and accuracy remains an issue. Thus, we propose a novel fast multi-organ segmentation framework with the usage of hierarchical sparse sampling and a Residual Transformer. Compared with whole-volume analysis, the hierarchical sparse sampling strategy could successfully reduce computation time while preserving a meaningful hierarchical context utilizing multiple resolution levels. The architecture of the Residual Transformer segmentation network could extract and combine information from different levels of information in the sparse descriptor while maintaining a low computational cost. In an internal data set containing 10,253 CT images and the public dataset TotalSegmentator, the proposed method successfully improved qualitative and quantitative segmentation performance compared to the current fast organ classifier, with fast speed at the level of ~2.24 seconds on CPU hardware. The potential of achieving real-time fine organ segmentation is suggested.",
            "headline_zh": "提出基于分层稀疏采样和残差Transformer的快速多器官精细分割方法，以解决CT图像分割中速度与精度的权衡问题。",
            "intro_zh": [
                "核心问题：3D医学图像多器官分割中，逐体素分割方法计算成本高，速度与精度难以兼顾。",
                "方法要点：采用分层稀疏采样策略减少计算时间，并利用残差Transformer网络提取多级信息。",
                "实验或效果：在内部和公共数据集上，分割性能优于现有快速分类器，CPU上速度约2.24秒。"
            ],
            "tags_zh": [
                "多器官分割",
                "分层稀疏采样",
                "残差Transformer",
                "CT图像",
                "实时分割",
                "医学图像分析"
            ],
            "_index": 101
        },
        {
            "title": "UI2Code$^\\text{N}$: A Visual Language Model for Test-Time Scalable Interactive UI-to-Code Generation",
            "authors": [
                "Zhen Yang",
                "Wenyi Hong",
                "Mingde Xu",
                "Xinyue Fan",
                "Weihan Wang",
                "Jiele Cheng",
                "Xiaotao Gu",
                "Jie Tang"
            ],
            "arxiv_id": "2511.08195v1",
            "summary": "User interface (UI) programming is a core yet highly complex part of modern software development. Recent advances in visual language models (VLMs) highlight the potential of automatic UI coding, but current approaches face two key limitations: multimodal coding capabilities remain underdeveloped, and single-turn paradigms make little use of iterative visual feedback. We address these challenges with an interactive UI-to-code paradigm that better reflects real-world workflows and raises the upper bound of achievable performance. Under this paradigm, we present UI2Code$^\\text{N}$, a visual language model trained through staged pretraining, fine-tuning, and reinforcement learning to achieve foundational improvements in multimodal coding. The model unifies three key capabilities: UI-to-code generation, UI editing, and UI polishing. We further explore test-time scaling for interactive generation, enabling systematic use of multi-turn feedback. Experiments on UI-to-code and UI polishing benchmarks show that UI2Code$^\\text{N}$ establishes a new state of the art among open-source models and achieves performance comparable to leading closed-source models such as Claude-4-Sonnet and GPT-5. Our code and models are available at https://github.com/zai-org/UI2Code_N.",
            "headline_zh": "提出UI2Code^N视觉语言模型，通过交互式UI到代码生成解决多模态编码不足问题。",
            "intro_zh": [
                "核心问题：UI编程复杂，现有视觉语言模型多模态编码能力弱且缺乏迭代反馈。",
                "方法要点：采用分阶段预训练、微调和强化学习，统一UI生成、编辑和优化能力。",
                "实验效果：在UI到代码和优化基准上达到开源模型最优，性能接近闭源领先模型。"
            ],
            "tags_zh": [
                "视觉语言模型",
                "UI到代码生成",
                "多模态编码",
                "交互式生成",
                "测试时扩展",
                "强化学习"
            ],
            "_index": 102
        },
        {
            "title": "Pixel-level Quality Assessment for Oriented Object Detection",
            "authors": [
                "Yunhui Zhu",
                "Buliao Huang"
            ],
            "arxiv_id": "2511.08186v1",
            "summary": "Modern oriented object detectors typically predict a set of bounding boxes and select the top-ranked ones based on estimated localization quality. Achieving high detection performance requires that the estimated quality closely aligns with the actual localization accuracy. To this end, existing approaches predict the Intersection over Union (IoU) between the predicted and ground-truth (GT) boxes as a proxy for localization quality. However, box-level IoU prediction suffers from a structural coupling issue: since the predicted box is derived from the detector's internal estimation of the GT box, the predicted IoU--based on their similarity--can be overestimated for poorly localized boxes. To overcome this limitation, we propose a novel Pixel-level Quality Assessment (PQA) framework, which replaces box-level IoU prediction with the integration of pixel-level spatial consistency. PQA measures the alignment between each pixel's relative position to the predicted box and its corresponding position to the GT box. By operating at the pixel level, PQA avoids directly comparing the predicted box with the estimated GT box, thereby eliminating the inherent similarity bias in box-level IoU prediction. Furthermore, we introduce a new integration metric that aggregates pixel-level spatial consistency into a unified quality score, yielding a more accurate approximation of the actual localization quality. Extensive experiments on HRSC2016 and DOTA demonstrate that PQA can be seamlessly integrated into various oriented object detectors, consistently improving performance (e.g., +5.96% AP$_{50:95}$ on Rotated RetinaNet and +2.32% on STD).",
            "headline_zh": "提出像素级质量评估框架以解决定向目标检测中框级IoU预测的结构耦合问题",
            "intro_zh": [
                "定向目标检测中框级IoU预测存在结构耦合，导致定位质量被高估",
                "PQA通过像素级空间一致性评估，避免直接比较预测框与估计真值框",
                "实验显示PQA可集成多种检测器，提升性能如Rotated RetinaNet AP增加5.96%"
            ],
            "tags_zh": [
                "定向目标检测",
                "像素级质量评估",
                "空间一致性",
                "IoU预测",
                "检测器集成"
            ],
            "_index": 103
        },
        {
            "title": "WarpGAN: Warping-Guided 3D GAN Inversion with Style-Based Novel View Inpainting",
            "authors": [
                "Kaitao Huang",
                "Yan Yan",
                "Jing-Hao Xue",
                "Hanzi Wang"
            ],
            "arxiv_id": "2511.08178v1",
            "summary": "3D GAN inversion projects a single image into the latent space of a pre-trained 3D GAN to achieve single-shot novel view synthesis, which requires visible regions with high fidelity and occluded regions with realism and multi-view consistency. However, existing methods focus on the reconstruction of visible regions, while the generation of occluded regions relies only on the generative prior of 3D GAN. As a result, the generated occluded regions often exhibit poor quality due to the information loss caused by the low bit-rate latent code. To address this, we introduce the warping-and-inpainting strategy to incorporate image inpainting into 3D GAN inversion and propose a novel 3D GAN inversion method, WarpGAN. Specifically, we first employ a 3D GAN inversion encoder to project the single-view image into a latent code that serves as the input to 3D GAN. Then, we perform warping to a novel view using the depth map generated by 3D GAN. Finally, we develop a novel SVINet, which leverages the symmetry prior and multi-view image correspondence w.r.t. the same latent code to perform inpainting of occluded regions in the warped image. Quantitative and qualitative experiments demonstrate that our method consistently outperforms several state-of-the-art methods.",
            "headline_zh": "提出WarpGAN方法以解决单视图图像3D GAN反演中遮挡区域生成质量差的问题",
            "intro_zh": [
                "核心问题：现有3D GAN反演方法在遮挡区域生成质量差，因低比特率潜在码导致信息丢失",
                "方法要点：结合扭曲与修复策略，使用SVINet基于对称性和多视图对应修复遮挡区域",
                "实验或效果：定量和定性实验显示，WarpGAN在多个指标上优于现有先进方法"
            ],
            "tags_zh": [
                "3D GAN反演",
                "单视图合成",
                "图像修复",
                "多视图一致性",
                "扭曲引导",
                "对称性先验"
            ],
            "_index": 104
        },
        {
            "title": "OTSNet: A Neurocognitive-Inspired Observation-Thinking-Spelling Pipeline for Scene Text Recognition",
            "authors": [
                "Lixu Sun",
                "Nurmemet Yolwas",
                "Wushour Silamu"
            ],
            "arxiv_id": "2511.08133v1",
            "summary": "Scene Text Recognition (STR) remains challenging due to real-world complexities, where decoupled visual-linguistic optimization in existing frameworks amplifies error propagation through cross-modal misalignment. Visual encoders exhibit attention bias toward background distractors, while decoders suffer from spatial misalignment when parsing geometrically deformed text-collectively degrading recognition accuracy for irregular patterns. Inspired by the hierarchical cognitive processes in human visual perception, we propose OTSNet, a novel three-stage network embodying a neurocognitive-inspired Observation-Thinking-Spelling pipeline for unified STR modeling. The architecture comprises three core components: (1) a Dual Attention Macaron Encoder (DAME) that refines visual features through differential attention maps to suppress irrelevant regions and enhance discriminative focus; (2) a Position-Aware Module (PAM) and Semantic Quantizer (SQ) that jointly integrate spatial context with glyph-level semantic abstraction via adaptive sampling; and (3) a Multi-Modal Collaborative Verifier (MMCV) that enforces self-correction through cross-modal fusion of visual, semantic, and character-level features. Extensive experiments demonstrate that OTSNet achieves state-of-the-art performance, attaining 83.5% average accuracy on the challenging Union14M-L benchmark and 79.1% on the heavily occluded OST dataset-establishing new records across 9 out of 14 evaluation scenarios.",
            "headline_zh": "提出OTSNet以解决场景文本识别中视觉-语言跨模态对齐问题",
            "intro_zh": [
                "核心问题：现有方法视觉-语言解耦优化导致错误传播，视觉编码器注意力偏向背景干扰，解码器空间对齐差",
                "方法要点：采用观察-思考-拼写三阶段管道，集成双注意力编码器、位置感知模块和多模态验证器",
                "实验或效果：在Union14M-L和OST数据集上达到83.5%和79.1%准确率，14个场景中9个创纪录"
            ],
            "tags_zh": [
                "场景文本识别",
                "跨模态对齐",
                "神经认知启发",
                "注意力机制",
                "多模态融合",
                "不规则文本处理"
            ],
            "_index": 105
        },
        {
            "title": "Perceptual Quality Assessment of 3D Gaussian Splatting: A Subjective Dataset and Prediction Metric",
            "authors": [
                "Zhaolin Wan",
                "Yining Diao",
                "Jingqi Xu",
                "Hao Wang",
                "Zhiyang Li",
                "Xiaopeng Fan",
                "Wangmeng Zuo",
                "Debin Zhao"
            ],
            "arxiv_id": "2511.08032v1",
            "summary": "With the rapid advancement of 3D visualization, 3D Gaussian Splatting (3DGS) has emerged as a leading technique for real-time, high-fidelity rendering. While prior research has emphasized algorithmic performance and visual fidelity, the perceptual quality of 3DGS-rendered content, especially under varying reconstruction conditions, remains largely underexplored. In practice, factors such as viewpoint sparsity, limited training iterations, point downsampling, noise, and color distortions can significantly degrade visual quality, yet their perceptual impact has not been systematically studied. To bridge this gap, we present 3DGS-QA, the first subjective quality assessment dataset for 3DGS. It comprises 225 degraded reconstructions across 15 object types, enabling a controlled investigation of common distortion factors. Based on this dataset, we introduce a no-reference quality prediction model that directly operates on native 3D Gaussian primitives, without requiring rendered images or ground-truth references. Our model extracts spatial and photometric cues from the Gaussian representation to estimate perceived quality in a structure-aware manner. We further benchmark existing quality assessment methods, spanning both traditional and learning-based approaches. Experimental results show that our method consistently achieves superior performance, highlighting its robustness and effectiveness for 3DGS content evaluation. The dataset and code are made publicly available at https://github.com/diaoyn/3DGSQA to facilitate future research in 3DGS quality assessment.",
            "headline_zh": "提出3DGS-QA数据集与无参考质量预测模型，以评估3D高斯泼溅的感知质量。",
            "intro_zh": [
                "核心问题：3D高斯泼溅渲染内容在多种失真因素下的感知质量未被系统研究。",
                "方法要点：构建主观数据集并开发基于高斯原语的无参考质量预测模型。",
                "实验或效果：模型在基准测试中表现优越，数据集和代码已公开。"
            ],
            "tags_zh": [
                "3D高斯泼溅",
                "感知质量评估",
                "无参考质量预测",
                "主观数据集",
                "3D渲染失真"
            ],
            "_index": 106
        },
        {
            "title": "Sharp Eyes and Memory for VideoLLMs: Information-Aware Visual Token Pruning for Efficient and Reliable VideoLLM Reasoning",
            "authors": [
                "Jialong Qin",
                "Xin Zou",
                "Di Lu",
                "Yibo Yan",
                "Xuming Hu"
            ],
            "arxiv_id": "2511.08003v1",
            "summary": "Current Video Large Language Models (VideoLLMs) suffer from quadratic computational complexity and key-value cache scaling, due to their reliance on processing excessive redundant visual tokens. To address this problem, we propose SharpV, a minimalist and efficient method for adaptive pruning of visual tokens and KV cache. Different from most uniform compression approaches, SharpV dynamically adjusts pruning ratios based on spatial-temporal information. Remarkably, this adaptive mechanism occasionally achieves performance gains over dense models, offering a novel paradigm for adaptive pruning. During the KV cache pruning stage, based on observations of visual information degradation, SharpV prunes degraded visual features via a self-calibration manner, guided by similarity to original visual features. In this way, SharpV achieves hierarchical cache pruning from the perspective of information bottleneck, offering a new insight into VideoLLMs' information flow. Experiments on multiple public benchmarks demonstrate the superiority of SharpV. Moreover, to the best of our knowledge, SharpV is notably the first two-stage pruning framework that operates without requiring access to exposed attention scores, ensuring full compatibility with hardware acceleration techniques like Flash Attention.",
            "headline_zh": "提出SharpV以解决VideoLLMs中视觉令牌冗余导致的效率问题",
            "intro_zh": [
                "核心问题：VideoLLMs因处理冗余视觉令牌导致计算复杂度和KV缓存增长",
                "方法要点：基于时空信息动态调整剪枝比例，并自校准剪枝退化特征",
                "实验或效果：在多个基准测试中表现优越，无需注意力分数即可兼容硬件加速"
            ],
            "tags_zh": [
                "视频大语言模型",
                "视觉令牌剪枝",
                "KV缓存优化",
                "自适应压缩",
                "信息瓶颈",
                "硬件加速兼容"
            ],
            "_index": 107
        },
        {
            "title": "Anatomy-VLM: A Fine-grained Vision-Language Model for Medical Interpretation",
            "authors": [
                "Difei Gu",
                "Yunhe Gao",
                "Mu Zhou",
                "Dimitris Metaxas"
            ],
            "arxiv_id": "2511.08402v1",
            "summary": "Accurate disease interpretation from radiology remains challenging due to imaging heterogeneity. Achieving expert-level diagnostic decisions requires integration of subtle image features with clinical knowledge. Yet major vision-language models (VLMs) treat images as holistic entities and overlook fine-grained image details that are vital for disease diagnosis. Clinicians analyze images by utilizing their prior medical knowledge and identify anatomical structures as important region of interests (ROIs). Inspired from this human-centric workflow, we introduce Anatomy-VLM, a fine-grained, vision-language model that incorporates multi-scale information. First, we design a model encoder to localize key anatomical features from entire medical images. Second, these regions are enriched with structured knowledge for contextually-aware interpretation. Finally, the model encoder aligns multi-scale medical information to generate clinically-interpretable disease prediction. Anatomy-VLM achieves outstanding performance on both in- and out-of-distribution datasets. We also validate the performance of Anatomy-VLM on downstream image segmentation tasks, suggesting that its fine-grained alignment captures anatomical and pathology-related knowledge. Furthermore, the Anatomy-VLM's encoder facilitates zero-shot anatomy-wise interpretation, providing its strong expert-level clinical interpretation capabilities.",
            "headline_zh": "提出Anatomy-VLM以解决医学影像中细粒度特征忽略导致的疾病诊断挑战",
            "intro_zh": [
                "核心问题：医学影像异质性使疾病诊断困难，现有视觉语言模型忽略关键细粒度图像细节",
                "方法要点：设计模型定位关键解剖结构，结合结构化知识进行多尺度信息对齐",
                "实验或效果：在分布内外数据集表现优异，支持零样本解剖解释和下游分割任务"
            ],
            "tags_zh": [
                "细粒度视觉语言模型",
                "医学影像诊断",
                "多尺度信息对齐",
                "解剖结构定位",
                "零样本解释"
            ],
            "_index": 108
        },
        {
            "title": "Human Motion Intent Inferencing in Teleoperation Through a SINDy Paradigm",
            "authors": [
                "Michael Bowman",
                "Xiaoli Zhang"
            ],
            "arxiv_id": "2511.08377v1",
            "summary": "Intent inferencing in teleoperation has been instrumental in aligning operator goals and coordinating actions with robotic partners. However, current intent inference methods often ignore subtle motion that can be strong indicators for a sudden change in intent. Specifically, we aim to tackle 1) if we can detect sudden jumps in operator trajectories, 2) how we appropriately use these sudden jump motions to infer an operator's goal state, and 3) how to incorporate these discontinuous and continuous dynamics to infer operator motion. Our framework, called Psychic, models these small indicative motions through a jump-drift-diffusion stochastic differential equation to cover discontinuous and continuous dynamics. Kramers-Moyal (KM) coefficients allow us to detect jumps with a trajectory which we pair with a statistical outlier detection algorithm to nominate goal transitions. Through identifying jumps, we can perform early detection of existing goals and discover undefined goals in unstructured scenarios. Our framework then applies a Sparse Identification of Nonlinear Dynamics (SINDy) model using KM coefficients with the goal transitions as a control input to infer an operator's motion behavior in unstructured scenarios. We demonstrate Psychic can produce probabilistic reachability sets and compare our strategy to a negative log-likelihood model fit. We perform a retrospective study on 600 operator trajectories in a hands-free teleoperation task to evaluate the efficacy of our opensource package, Psychic, in both offline and online learning.",
            "headline_zh": "提出Psychic框架，通过SINDy范式推断遥操作中的人类运动意图。",
            "intro_zh": [
                "核心问题：现有意图推断方法忽略细微运动，难以检测意图突变。",
                "方法要点：使用跳跃-漂移-扩散SDE建模运动，结合KM系数和异常检测识别目标转换。",
                "实验或效果：在600条轨迹上验证，支持离线和在线学习，生成概率可达集。"
            ],
            "tags_zh": [
                "意图推断",
                "遥操作",
                "SINDy模型",
                "随机微分方程",
                "运动分析",
                "目标检测"
            ],
            "_index": 109
        },
        {
            "title": "Text-based Aerial-Ground Person Retrieval",
            "authors": [
                "Xinyu Zhou",
                "Yu Wu",
                "Jiayao Ma",
                "Wenhao Wang",
                "Min Cao",
                "Mang Ye"
            ],
            "arxiv_id": "2511.08369v1",
            "summary": "This work introduces Text-based Aerial-Ground Person Retrieval (TAG-PR), which aims to retrieve person images from heterogeneous aerial and ground views with textual descriptions. Unlike traditional Text-based Person Retrieval (T-PR), which focuses solely on ground-view images, TAG-PR introduces greater practical significance and presents unique challenges due to the large viewpoint discrepancy across images. To support this task, we contribute: (1) TAG-PEDES dataset, constructed from public benchmarks with automatically generated textual descriptions, enhanced by a diversified text generation paradigm to ensure robustness under view heterogeneity; and (2) TAG-CLIP, a novel retrieval framework that addresses view heterogeneity through a hierarchically-routed mixture of experts module to learn view-specific and view-agnostic features and a viewpoint decoupling strategy to decouple view-specific features for better cross-modal alignment. We evaluate the effectiveness of TAG-CLIP on both the proposed TAG-PEDES dataset and existing T-PR benchmarks. The dataset and code are available at https://github.com/Flame-Chasers/TAG-PR.",
            "headline_zh": "提出TAG-PR任务和TAG-CLIP框架，以解决基于文本的空中-地面行人检索中的视角差异问题。",
            "intro_zh": [
                "核心问题：空中与地面视图间存在大视角差异，增加了文本描述检索行人图像的难度。",
                "方法要点：使用分层路由专家模块和视角解耦策略，学习视图特定和视图无关特征。",
                "实验或效果：在TAG-PEDES数据集和现有基准上评估，框架有效提升检索性能。"
            ],
            "tags_zh": [
                "文本行人检索",
                "多视角图像检索",
                "跨模态对齐",
                "数据集构建",
                "混合专家模型"
            ],
            "_index": 110
        },
        {
            "title": "The Impact of Longitudinal Mammogram Alignment on Breast Cancer Risk Assessment",
            "authors": [
                "Solveig Thrun",
                "Stine Hansen",
                "Zijun Sun",
                "Nele Blum",
                "Suaiba A. Salahuddin",
                "Xin Wang",
                "Kristoffer Wickstrøm",
                "Elisabeth Wetzer",
                "Robert Jenssen",
                "Maik Stille",
                "Michael Kampffmeyer"
            ],
            "arxiv_id": "2511.08328v1",
            "summary": "Regular mammography screening is crucial for early breast cancer detection. By leveraging deep learning-based risk models, screening intervals can be personalized, especially for high-risk individuals. While recent methods increasingly incorporate longitudinal information from prior mammograms, accurate spatial alignment across time points remains a key challenge. Misalignment can obscure meaningful tissue changes and degrade model performance. In this study, we provide insights into various alignment strategies, image-based registration, feature-level (representation space) alignment with and without regularization, and implicit alignment methods, for their effectiveness in longitudinal deep learning-based risk modeling. Using two large-scale mammography datasets, we assess each method across key metrics, including predictive accuracy, precision, recall, and deformation field quality.\n  Our results show that image-based registration consistently outperforms the more recently favored feature-based and implicit approaches across all metrics, enabling more accurate, temporally consistent predictions and generating smooth, anatomically plausible deformation fields. Although regularizing the deformation field improves deformation quality, it reduces the risk prediction performance of feature-level alignment. Applying image-based deformation fields within the feature space yields the best risk prediction performance.\n  These findings underscore the importance of image-based deformation fields for spatial alignment in longitudinal risk modeling, offering improved prediction accuracy and robustness. This approach has strong potential to enhance personalized screening and enable earlier interventions for high-risk individuals. The code is available at https://github.com/sot176/Mammogram_Alignment_Study_Risk_Prediction.git, allowing full reproducibility of the results.",
            "headline_zh": "评估乳腺X线图像纵向对齐策略对深度学习风险模型性能的影响",
            "intro_zh": [
                "核心问题：纵向乳腺X线图像空间未对齐会掩盖组织变化，降低风险预测准确性。",
                "方法要点：比较图像配准、特征对齐和隐式对齐方法，评估其对风险建模的效果。",
                "实验或效果：图像配准在预测精度和变形场质量上优于其他方法，提升模型鲁棒性。"
            ],
            "tags_zh": [
                "乳腺X线图像对齐",
                "纵向深度学习",
                "风险预测模型",
                "图像配准",
                "变形场优化"
            ],
            "_index": 111
        },
        {
            "title": "Mitigating Negative Flips via Margin Preserving Training",
            "authors": [
                "Simone Ricci",
                "Niccolò Biondi",
                "Federico Pernici",
                "Alberto Del Bimbo"
            ],
            "arxiv_id": "2511.08322v1",
            "summary": "Minimizing inconsistencies across successive versions of an AI system is as crucial as reducing the overall error. In image classification, such inconsistencies manifest as negative flips, where an updated model misclassifies test samples that were previously classified correctly. This issue becomes increasingly pronounced as the number of training classes grows over time, since adding new categories reduces the margin of each class and may introduce conflicting patterns that undermine their learning process, thereby degrading performance on the original subset. To mitigate negative flips, we propose a novel approach that preserves the margins of the original model while learning an improved one. Our method encourages a larger relative margin between the previously learned and newly introduced classes by introducing an explicit margin-calibration term on the logits. However, overly constraining the logit margin for the new classes can significantly degrade their accuracy compared to a new independently trained model. To address this, we integrate a double-source focal distillation loss with the previous model and a new independently trained model, learning an appropriate decision margin from both old and new data, even under a logit margin calibration. Extensive experiments on image classification benchmarks demonstrate that our approach consistently reduces the negative flip rate with high overall accuracy.",
            "headline_zh": "提出边界保持训练方法以减少图像分类中的负翻转",
            "intro_zh": [
                "核心问题：模型更新时负翻转增加，新类引入降低原类边界，导致性能下降",
                "方法要点：结合边界校准项和双源焦点蒸馏损失，平衡新旧类学习",
                "实验或效果：在图像分类基准上，显著降低负翻转率并保持高准确率"
            ],
            "tags_zh": [
                "图像分类",
                "负翻转",
                "边界保持",
                "蒸馏训练",
                "模型更新"
            ],
            "_index": 112
        },
        {
            "title": "Work-in-Progress: Function-as-Subtask API Replacing Publish/Subscribe for OS-Native DAG Scheduling",
            "authors": [
                "Takahiro Ishikawa-Aso",
                "Atsushi Yano",
                "Yutaro Kobayashi",
                "Takumi Jin",
                "Yuuki Takano",
                "Shinpei Kato"
            ],
            "arxiv_id": "2511.08297v1",
            "summary": "The Directed Acyclic Graph (DAG) task model for real-time scheduling finds its primary practical target in Robot Operating System 2 (ROS 2). However, ROS 2's publish/subscribe API leaves DAG precedence constraints unenforced: a callback may publish mid-execution, and multi-input callbacks let developers choose topic-matching policies. Thus preserving DAG semantics relies on conventions; once violated, the model collapses. We propose the Function-as-Subtask (FasS) API, which expresses each subtask as a function whose arguments/return values are the subtask's incoming/outgoing edges. By minimizing description freedom, DAG semantics is guaranteed at the API rather than by programmer discipline. We implement a DAG-native scheduler using FasS on a Rust-based experimental kernel and evaluate its semantic fidelity, and we outline design guidelines for applying FasS to Linux Linux sched_ext.",
            "headline_zh": "提出Function-as-Subtask API以解决ROS 2中DAG语义未强制的问题",
            "intro_zh": [
                "ROS 2发布/订阅API未强制DAG优先约束，依赖开发者惯例易导致模型失效",
                "FasS API将子任务表达为函数，参数和返回值对应边，确保DAG语义",
                "在Rust内核实现DAG原生调度器，评估语义保真度并指导Linux应用"
            ],
            "tags_zh": [
                "实时调度",
                "DAG任务模型",
                "ROS 2",
                "API设计",
                "语义保证",
                "操作系统内核"
            ],
            "_index": 113
        },
        {
            "title": "SkelSplat: Robust Multi-view 3D Human Pose Estimation with Differentiable Gaussian Rendering",
            "authors": [
                "Laura Bragagnolo",
                "Leonardo Barcellona",
                "Stefano Ghidoni"
            ],
            "arxiv_id": "2511.08294v1",
            "summary": "Accurate 3D human pose estimation is fundamental for applications such as augmented reality and human-robot interaction. State-of-the-art multi-view methods learn to fuse predictions across views by training on large annotated datasets, leading to poor generalization when the test scenario differs. To overcome these limitations, we propose SkelSplat, a novel framework for multi-view 3D human pose estimation based on differentiable Gaussian rendering. Human pose is modeled as a skeleton of 3D Gaussians, one per joint, optimized via differentiable rendering to enable seamless fusion of arbitrary camera views without 3D ground-truth supervision. Since Gaussian Splatting was originally designed for dense scene reconstruction, we propose a novel one-hot encoding scheme that enables independent optimization of human joints. SkelSplat outperforms approaches that do not rely on 3D ground truth in Human3.6M and CMU, while reducing the cross-dataset error up to 47.8% compared to learning-based methods. Experiments on Human3.6M-Occ and Occlusion-Person demonstrate robustness to occlusions, without scenario-specific fine-tuning. Our project page is available here: https://skelsplat.github.io.",
            "headline_zh": "提出SkelSplat框架，基于可微高斯渲染解决多视角3D人体姿态估计泛化问题",
            "intro_zh": [
                "核心问题：多视角3D人体姿态估计方法依赖标注数据，泛化性差",
                "方法要点：使用3D高斯骨架建模姿态，通过可微渲染融合任意视角，无需3D真值监督",
                "实验效果：在Human3.6M和CMU数据集上优于无3D真值方法，跨数据集误差降低达47.8%"
            ],
            "tags_zh": [
                "多视角3D人体姿态估计",
                "可微高斯渲染",
                "骨架建模",
                "无监督学习",
                "泛化性提升"
            ],
            "_index": 114
        },
        {
            "title": "X-IONet: Cross-Platform Inertial Odometry Network with Dual-Stage Attention",
            "authors": [
                "Dehan Shen",
                "Changhao Chen"
            ],
            "arxiv_id": "2511.08277v1",
            "summary": "Learning-based inertial odometry has achieved remarkable progress in pedestrian navigation. However, extending these methods to quadruped robots remains challenging due to their distinct and highly dynamic motion patterns. Models that perform well on pedestrian data often experience severe degradation when deployed on legged platforms. To tackle this challenge, we introduce X-IONet, a cross-platform inertial odometry framework that operates solely using a single Inertial Measurement Unit (IMU). X-IONet incorporates a rule-based expert selection module to classify motion platforms and route IMU sequences to platform-specific expert networks. The displacement prediction network features a dual-stage attention architecture that jointly models long-range temporal dependencies and inter-axis correlations, enabling accurate motion representation. It outputs both displacement and associated uncertainty, which are further fused through an Extended Kalman Filter (EKF) for robust state estimation. Extensive experiments on public pedestrian datasets and a self-collected quadruped robot dataset demonstrate that X-IONet achieves state-of-the-art performance, reducing Absolute Trajectory Error (ATE) by 14.3% and Relative Trajectory Error (RTE) by 11.4% on pedestrian data, and by 52.8% and 41.3% on quadruped robot data. These results highlight the effectiveness of X-IONet in advancing accurate and robust inertial navigation across both human and legged robot platforms.",
            "headline_zh": "提出X-IONet以解决跨平台惯性里程计在行人和四足机器人上的性能退化问题",
            "intro_zh": [
                "核心问题：基于学习的惯性里程计在四足机器人上性能显著下降，因运动模式差异大",
                "方法要点：采用基于规则的专家选择模块和双阶段注意力网络，结合EKF进行状态估计",
                "实验或效果：在公开和自收集数据集上，ATE和RTE在行人和四足机器人数据上均显著降低"
            ],
            "tags_zh": [
                "惯性里程计",
                "跨平台学习",
                "双阶段注意力",
                "专家网络",
                "状态估计"
            ],
            "_index": 115
        },
        {
            "title": "Prioritizing Perception-Guided Self-Supervision: A New Paradigm for Causal Modeling in End-to-End Autonomous Driving",
            "authors": [
                "Yi Huang",
                "Zhan Qu",
                "Lihui Jiang",
                "Bingbing Liu",
                "Hongbo Zhang"
            ],
            "arxiv_id": "2511.08214v1",
            "summary": "End-to-end autonomous driving systems, predominantly trained through imitation learning, have demonstrated considerable effectiveness in leveraging large-scale expert driving data. Despite their success in open-loop evaluations, these systems often exhibit significant performance degradation in closed-loop scenarios due to causal confusion. This confusion is fundamentally exacerbated by the overreliance of the imitation learning paradigm on expert trajectories, which often contain unattributable noise and interfere with the modeling of causal relationships between environmental contexts and appropriate driving actions.\n  To address this fundamental limitation, we propose Perception-Guided Self-Supervision (PGS) - a simple yet effective training paradigm that leverages perception outputs as the primary supervisory signals, explicitly modeling causal relationships in decision-making. The proposed framework aligns both the inputs and outputs of the decision-making module with perception results, such as lane centerlines and the predicted motions of surrounding agents, by introducing positive and negative self-supervision for the ego trajectory. This alignment is specifically designed to mitigate causal confusion arising from the inherent noise in expert trajectories.\n  Equipped with perception-driven supervision, our method, built on a standard end-to-end architecture, achieves a Driving Score of 78.08 and a mean success rate of 48.64% on the challenging closed-loop Bench2Drive benchmark, significantly outperforming existing state-of-the-art methods, including those employing more complex network architectures and inference pipelines. These results underscore the effectiveness and robustness of the proposed PGS framework and point to a promising direction for addressing causal confusion and enhancing real-world generalization in autonomous driving.",
            "headline_zh": "提出感知引导自监督范式以解决端到端自动驾驶中的因果混淆问题",
            "intro_zh": [
                "核心问题：模仿学习依赖专家轨迹导致因果混淆，影响闭环性能。",
                "方法要点：利用感知输出作为监督信号，建模环境与驾驶动作的因果关系。",
                "实验或效果：在Bench2Drive基准上，驾驶分数达78.08，显著优于现有方法。"
            ],
            "tags_zh": [
                "端到端自动驾驶",
                "因果建模",
                "自监督学习",
                "感知引导",
                "闭环评估",
                "模仿学习"
            ],
            "_index": 116
        },
        {
            "title": "Burst Image Quality Assessment: A New Benchmark and Unified Framework for Multiple Downstream Tasks",
            "authors": [
                "Xiaoye Liang",
                "Lai Jiang",
                "Minglang Qiao",
                "Yichen Guo",
                "Yue Zhang",
                "Xin Deng",
                "Shengxi Li",
                "Yufan Liu",
                "Mai Xu"
            ],
            "arxiv_id": "2511.07958v1",
            "summary": "In recent years, the development of burst imaging technology has improved the capture and processing capabilities of visual data, enabling a wide range of applications. However, the redundancy in burst images leads to the increased storage and transmission demands, as well as reduced efficiency of downstream tasks. To address this, we propose a new task of Burst Image Quality Assessment (BuIQA), to evaluate the task-driven quality of each frame within a burst sequence, providing reasonable cues for burst image selection. Specifically, we establish the first benchmark dataset for BuIQA, consisting of $7,346$ burst sequences with $45,827$ images and $191,572$ annotated quality scores for multiple downstream scenarios. Inspired by the data analysis, a unified BuIQA framework is proposed to achieve an efficient adaption for BuIQA under diverse downstream scenarios. Specifically, a task-driven prompt generation network is developed with heterogeneous knowledge distillation, to learn the priors of the downstream task. Then, the task-aware quality assessment network is introduced to assess the burst image quality based on the task prompt. Extensive experiments across 10 downstream scenarios demonstrate the impressive BuIQA performance of the proposed approach, outperforming the state-of-the-art. Furthermore, it can achieve $0.33$ dB PSNR improvement in the downstream tasks of denoising and super-resolution, by applying our approach to select the high-quality burst frames.",
            "headline_zh": "提出突发图像质量评估任务与统一框架，以优化多下游任务性能。",
            "intro_zh": [
                "突发图像冗余导致存储传输负担增加，下游任务效率降低。",
                "开发任务驱动提示生成网络，结合异构知识蒸馏学习任务先验。",
                "在10个下游场景中验证，提升去噪和超分辨率任务PSNR 0.33 dB。"
            ],
            "tags_zh": [
                "突发图像质量评估",
                "任务驱动提示",
                "异构知识蒸馏",
                "下游任务优化",
                "基准数据集"
            ],
            "_index": 117
        },
        {
            "title": "Laytrol: Preserving Pretrained Knowledge in Layout Control for Multimodal Diffusion Transformers",
            "authors": [
                "Sida Huang",
                "Siqi Huang",
                "Ping Luo",
                "Hongyuan Zhang"
            ],
            "arxiv_id": "2511.07934v1",
            "summary": "With the development of diffusion models, enhancing spatial controllability in text-to-image generation has become a vital challenge. As a representative task for addressing this challenge, layout-to-image generation aims to generate images that are spatially consistent with the given layout condition. Existing layout-to-image methods typically introduce the layout condition by integrating adapter modules into the base generative model. However, the generated images often exhibit low visual quality and stylistic inconsistency with the base model, indicating a loss of pretrained knowledge. To alleviate this issue, we construct the Layout Synthesis (LaySyn) dataset, which leverages images synthesized by the base model itself to mitigate the distribution shift from the pretraining data. Moreover, we propose the Layout Control (Laytrol) Network, in which parameters are inherited from MM-DiT to preserve the pretrained knowledge of the base model. To effectively activate the copied parameters and avoid disturbance from unstable control conditions, we adopt a dedicated initialization scheme for Laytrol. In this scheme, the layout encoder is initialized as a pure text encoder to ensure that its output tokens remain within the data domain of MM-DiT. Meanwhile, the outputs of the layout control network are initialized to zero. In addition, we apply Object-level Rotary Position Embedding to the layout tokens to provide coarse positional information. Qualitative and quantitative experiments demonstrate the effectiveness of our method.",
            "headline_zh": "提出Laytrol网络以解决布局控制中预训练知识丢失问题",
            "intro_zh": [
                "核心问题：现有布局到图像方法导致生成图像质量低且风格不一致，丢失预训练知识。",
                "方法要点：构建LaySyn数据集并使用Laytrol网络继承MM-DiT参数，初始化布局编码器为文本编码器。",
                "实验或效果：定性和定量实验验证方法有效性，提升图像质量和风格一致性。"
            ],
            "tags_zh": [
                "布局到图像生成",
                "扩散模型",
                "预训练知识保留",
                "布局控制网络",
                "数据集构建",
                "位置嵌入"
            ],
            "_index": 118
        },
        {
            "title": "CNN-Based Automated Parameter Extraction Framework for Modeling Memristive Devices",
            "authors": [
                "Akif Hamid",
                "Orchi Hassan"
            ],
            "arxiv_id": "2511.07926v1",
            "summary": "Resistive random access memory (RRAM) is a promising candidate for next-generation nonvolatile memory (NVM) and in-memory computing applications. Compact models are essential for analyzing the circuit and system-level performance of experimental RRAM devices. However, most existing RRAM compact models rely on multiple fitting parameters to reproduce the device I-V characteristics, and in most cases, as the parameters are not directly related to measurable quantities, their extraction requires extensive manual tuning, making the process time-consuming and limiting adaptability across different devices. This work presents an automated framework for extracting the fitting parameters of the widely used Stanford RRAM model directly from the device I-V characteristics. The framework employs a convolutional neural network (CNN) trained on a synthetic dataset to generate initial parameter estimates, which are then refined through three heuristic optimization blocks that minimize errors via adaptive binary search in the parameter space. We evaluated the framework using four key NVM metrics: set voltage, reset voltage, hysteresis loop area, and low resistance state (LRS) slope. Benchmarking against RRAM device characteristics derived from previously reported Stanford model fits, other analytical models, and experimental data shows that the framework achieves low error across diverse device characteristics, offering a fast, reliable, and robust solution for RRAM modeling.",
            "headline_zh": "提出基于CNN的自动化参数提取框架，以解决RRAM建模中参数手动调优耗时问题。",
            "intro_zh": [
                "核心问题：RRAM紧凑模型参数提取依赖手动调优，过程耗时且适应性差。",
                "方法要点：使用CNN生成初始参数估计，并通过启发式优化块进行误差最小化。",
                "实验或效果：在多个NVM指标上实现低误差，验证框架的快速性和鲁棒性。"
            ],
            "tags_zh": [
                "RRAM建模",
                "参数提取",
                "卷积神经网络",
                "启发式优化",
                "非易失性存储器"
            ],
            "_index": 119
        },
        {
            "title": "Visual Bridge: Universal Visual Perception Representations Generating",
            "authors": [
                "Yilin Gao",
                "Shuguang Dou",
                "Junzhou Li",
                "Zhiheng Yu",
                "Yin Li",
                "Dongsheng Jiang",
                "Shugong Xu"
            ],
            "arxiv_id": "2511.07877v1",
            "summary": "Recent advances in diffusion models have achieved remarkable success in isolated computer vision tasks such as text-to-image generation, depth estimation, and optical flow. However, these models are often restricted by a ``single-task-single-model'' paradigm, severely limiting their generalizability and scalability in multi-task scenarios. Motivated by the cross-domain generalization ability of large language models, we propose a universal visual perception framework based on flow matching that can generate diverse visual representations across multiple tasks. Our approach formulates the process as a universal flow-matching problem from image patch tokens to task-specific representations rather than an independent generation or regression problem. By leveraging a strong self-supervised foundation model as the anchor and introducing a multi-scale, circular task embedding mechanism, our method learns a universal velocity field to bridge the gap between heterogeneous tasks, supporting efficient and flexible representation transfer. Extensive experiments on classification, detection, segmentation, depth estimation, and image-text retrieval demonstrate that our model achieves competitive performance in both zero-shot and fine-tuned settings, outperforming prior generalist and several specialist models. Ablation studies further validate the robustness, scalability, and generalization of our framework. Our work marks a significant step towards general-purpose visual perception, providing a solid foundation for future research in universal vision modeling.",
            "headline_zh": "提出基于流匹配的通用视觉感知框架，以解决多任务场景下的泛化与扩展性问题。",
            "intro_zh": [
                "核心问题：现有扩散模型局限于单任务单模型范式，泛化与扩展性不足。",
                "方法要点：使用流匹配从图像块令牌生成任务特定表示，引入多尺度循环任务嵌入。",
                "实验或效果：在分类、检测等任务中，零样本和微调设置下性能优于先前模型。"
            ],
            "tags_zh": [
                "通用视觉感知",
                "流匹配",
                "多任务学习",
                "视觉表示生成",
                "零样本学习"
            ],
            "_index": 120
        },
        {
            "title": "MonoCLUE : Object-Aware Clustering Enhances Monocular 3D Object Detection",
            "authors": [
                "Sunghun Yang",
                "Minhyeok Lee",
                "Jungho Lee",
                "Sangyoun Lee"
            ],
            "arxiv_id": "2511.07862v1",
            "summary": "Monocular 3D object detection offers a cost-effective solution for autonomous driving but suffers from ill-posed depth and limited field of view. These constraints cause a lack of geometric cues and reduced accuracy in occluded or truncated scenes. While recent approaches incorporate additional depth information to address geometric ambiguity, they overlook the visual cues crucial for robust recognition. We propose MonoCLUE, which enhances monocular 3D detection by leveraging both local clustering and generalized scene memory of visual features. First, we perform K-means clustering on visual features to capture distinct object-level appearance parts (e.g., bonnet, car roof), improving detection of partially visible objects. The clustered features are propagated across regions to capture objects with similar appearances. Second, we construct a generalized scene memory by aggregating clustered features across images, providing consistent representations that generalize across scenes. This improves object-level feature consistency, enabling stable detection across varying environments. Lastly, we integrate both local cluster features and generalized scene memory into object queries, guiding attention toward informative regions. Exploiting a unified local clustering and generalized scene memory strategy, MonoCLUE enables robust monocular 3D detection under occlusion and limited visibility, achieving state-of-the-art performance on the KITTI benchmark.",
            "headline_zh": "提出MonoCLUE以增强单目3D目标检测，通过对象感知聚类和场景记忆解决遮挡与视野限制问题。",
            "intro_zh": [
                "核心问题：单目3D检测因深度模糊和视野限制，在遮挡或截断场景中准确性下降。",
                "方法要点：使用K-means聚类视觉特征捕获对象部分，并构建跨图像场景记忆提升特征一致性。",
                "实验或效果：在KITTI基准测试中实现领先性能，提升遮挡和低可见度下的检测鲁棒性。"
            ],
            "tags_zh": [
                "单目3D目标检测",
                "对象感知聚类",
                "场景记忆",
                "视觉特征增强",
                "KITTI基准"
            ],
            "_index": 121
        },
        {
            "title": "Deep Learning Analysis of Prenatal Ultrasound for Identification of Ventriculomegaly",
            "authors": [
                "Youssef Megahed",
                "Inok Lee",
                "Robin Ducharme",
                "Aylin Erman",
                "Olivier X. Miguel",
                "Kevin Dick",
                "Adrian D. C. Chan",
                "Steven Hawken",
                "Mark Walker",
                "Felipe Moretti"
            ],
            "arxiv_id": "2511.07827v1",
            "summary": "The proposed study aimed to develop a deep learning model capable of detecting ventriculomegaly on prenatal ultrasound images. Ventriculomegaly is a prenatal condition characterized by dilated cerebral ventricles of the fetal brain and is important to diagnose early, as it can be associated with an increased risk for fetal aneuploidies and/or underlying genetic syndromes. An Ultrasound Self-Supervised Foundation Model with Masked Autoencoding (USF-MAE), recently developed by our group, was fine-tuned for a binary classification task to distinguish fetal brain ultrasound images as either normal or showing ventriculomegaly. The USF-MAE incorporates a Vision Transformer encoder pretrained on more than 370,000 ultrasound images from the OpenUS-46 corpus. For this study, the pretrained encoder was adapted and fine-tuned on a curated dataset of fetal brain ultrasound images to optimize its performance for ventriculomegaly detection. Model evaluation was conducted using 5-fold cross-validation and an independent test cohort, and performance was quantified using accuracy, precision, recall, specificity, F1-score, and area under the receiver operating characteristic curve (AUC). The proposed USF-MAE model reached an F1-score of 91.76% on the 5-fold cross-validation and 91.78% on the independent test set, with much higher scores than those obtained by the baseline models by 19.37% and 16.15% compared to VGG-19, 2.31% and 2.56% compared to ResNet-50, and 5.03% and 11.93% compared to ViT-B/16, respectively. The model also showed a high mean test precision of 94.47% and an accuracy of 97.24%. The Eigen-CAM (Eigen Class Activation Map) heatmaps showed that the model was focusing on the ventricle area for the diagnosis of ventriculomegaly, which has explainability and clinical plausibility.",
            "headline_zh": "提出基于自监督预训练模型的深度学习方法来检测产前超声中的脑室扩大",
            "intro_zh": [
                "核心问题：产前脑室扩大（脑室扩大）的早期诊断，与胎儿非整倍体和遗传综合征风险相关。",
                "方法要点：使用USF-MAE模型，基于Vision Transformer编码器，在胎儿脑超声图像上进行微调。",
                "实验或效果：模型在独立测试集上F1-score达91.78%，优于基线模型，并具有临床可解释性。"
            ],
            "tags_zh": [
                "产前超声分析",
                "脑室扩大检测",
                "自监督学习",
                "Vision Transformer",
                "医学图像分类",
                "模型可解释性"
            ],
            "_index": 122
        },
        {
            "title": "Learning Sparse Label Couplings for Multilabel Chest X-Ray Diagnosis",
            "authors": [
                "Utkarsh Prakash Srivastava",
                "Kaushik Gupta",
                "Kaushik Nath"
            ],
            "arxiv_id": "2511.07801v1",
            "summary": "We study multilabel classification of chest X-rays and present a simple, strong pipeline built on SE-ResNeXt101 $(32 \\times 4d)$. The backbone is finetuned for 14 thoracic findings with a sigmoid head, trained using Multilabel Iterative Stratification (MIS) for robust cross-validation splits that preserve label co-occurrence. To address extreme class imbalance and asymmetric error costs, we optimize with Asymmetric Loss, employ mixed-precision (AMP), cosine learning-rate decay with warm-up, gradient clipping, and an exponential moving average (EMA) of weights. We propose a lightweight Label-Graph Refinement module placed after the classifier: given per-label probabilities, it learns a sparse, trainable inter-label coupling matrix that refines logits via a single message-passing step while adding only an L1-regularized parameter head. At inference, we apply horizontal flip test-time augmentation (TTA) and average predictions across MIS folds (a compact deep ensemble). Evaluation uses macro AUC averaging classwise ROC-AUC and skipping single-class labels in a fold to reflect balanced performance across conditions. On our dataset, a strong SE-ResNeXt101 baseline attains competitive macro AUC (e.g., 92.64% in our runs). Adding the Label-Graph Refinement consistently improves validation macro AUC across folds with negligible compute. The resulting method is reproducible, hardware-friendly, and requires no extra annotations, offering a practical route to stronger multilabel CXR classifiers.",
            "headline_zh": "提出稀疏标签耦合模块以增强多标签胸部X光诊断性能",
            "intro_zh": [
                "核心问题：多标签胸部X光分类中标签共现与类别不平衡问题",
                "方法要点：使用SE-ResNeXt101骨干，添加轻量级标签图优化模块学习稀疏耦合",
                "实验或效果：在验证集上一致提升宏观AUC，计算开销可忽略"
            ],
            "tags_zh": [
                "多标签分类",
                "胸部X光诊断",
                "标签耦合",
                "类别不平衡",
                "轻量级优化",
                "测试时增强"
            ],
            "_index": 123
        },
        {
            "title": "Benchmarking Resilience and Sensitivity of Polyurethane-Based Vision-Based Tactile Sensors",
            "authors": [
                "Benjamin Davis",
                "Hannah Stuart"
            ],
            "arxiv_id": "2511.07797v1",
            "summary": "Vision-based tactile sensors (VBTSs) are a promising technology for robots, providing them with dense signals that can be translated into an understanding of normal and shear load, contact region, texture classification, and more. However, existing VBTS tactile surfaces make use of silicone gels, which provide high sensitivity but easily deteriorate from loading and surface wear. We propose that polyurethane rubber, used for high-load applications like shoe soles, rubber wheels, and industrial gaskets, may provide improved physical gel resilience, potentially at the cost of sensitivity. To compare the resilience and sensitivity of silicone and polyurethane VBTS gels, we propose a series of standard evaluation benchmarking protocols. Our resilience tests assess sensor durability across normal loading, shear loading, and abrasion. For sensitivity, we introduce model-free assessments of force and spatial sensitivity to directly measure the physical capabilities of each gel without effects introduced from data and model quality. Finally, we include a bottle cap loosening and tightening demonstration as an example where polyurethane gels provide an advantage over their silicone counterparts.",
            "headline_zh": "提出基于聚氨酯的视觉触觉传感器，以提升耐用性并评估其与硅胶的对比性能。",
            "intro_zh": [
                "核心问题：硅胶视觉触觉传感器灵敏度高但易磨损，需改进耐用性。",
                "方法要点：使用聚氨酯材料，设计标准评估协议比较耐用性和灵敏度。",
                "实验或效果：通过加载、磨损测试和瓶盖操作演示聚氨酯的优势。"
            ],
            "tags_zh": [
                "视觉触觉传感器",
                "聚氨酯材料",
                "耐用性评估",
                "灵敏度测试",
                "机器人触觉"
            ],
            "_index": 124
        },
        {
            "title": "UniVA: Universal Video Agent towards Open-Source Next-Generation Video Generalist",
            "authors": [
                "Zhengyang Liang",
                "Daoan Zhang",
                "Huichi Zhou",
                "Rui Huang",
                "Bobo Li",
                "Yuechen Zhang",
                "Shengqiong Wu",
                "Xiaohan Wang",
                "Jiebo Luo",
                "Lizi Liao",
                "Hao Fei"
            ],
            "arxiv_id": "2511.08521v1",
            "summary": "While specialized AI models excel at isolated video tasks like generation or understanding, real-world applications demand complex, iterative workflows that combine these capabilities. To bridge this gap, we introduce UniVA, an open-source, omni-capable multi-agent framework for next-generation video generalists that unifies video understanding, segmentation, editing, and generation into cohesive workflows. UniVA employs a Plan-and-Act dual-agent architecture that drives a highly automated and proactive workflow: a planner agent interprets user intentions and decomposes them into structured video-processing steps, while executor agents execute these through modular, MCP-based tool servers (for analysis, generation, editing, tracking, etc.). Through a hierarchical multi-level memory (global knowledge, task context, and user-specific preferences), UniVA sustains long-horizon reasoning, contextual continuity, and inter-agent communication, enabling interactive and self-reflective video creation with full traceability. This design enables iterative and any-conditioned video workflows (e.g., text/image/video-conditioned generation $\\rightarrow$ multi-round editing $\\rightarrow$ object segmentation $\\rightarrow$ compositional synthesis) that were previously cumbersome to achieve with single-purpose models or monolithic video-language models. We also introduce UniVA-Bench, a benchmark suite of multi-step video tasks spanning understanding, editing, segmentation, and generation, to rigorously evaluate such agentic video systems. Both UniVA and UniVA-Bench are fully open-sourced, aiming to catalyze research on interactive, agentic, and general-purpose video intelligence for the next generation of multimodal AI systems. (https://univa.online/)",
            "headline_zh": "提出UniVA通用视频代理框架，以统一视频任务解决复杂工作流需求。",
            "intro_zh": [
                "核心问题：专业AI模型难以处理视频理解、编辑和生成的复杂迭代工作流。",
                "方法要点：采用计划-执行双代理架构，结合模块化工具和分层内存实现自动化。",
                "实验或效果：引入UniVA-Bench基准评估，并开源框架促进多模态AI研究。"
            ],
            "tags_zh": [
                "视频通用代理",
                "多代理框架",
                "视频工作流",
                "分层内存",
                "开源基准"
            ],
            "_index": 125
        },
        {
            "title": "Safe and Optimal Learning from Preferences via Weighted Temporal Logic with Applications in Robotics and Formula 1",
            "authors": [
                "Ruya Karagulle",
                "Cristian-Ioan Vasile",
                "Necmiye Ozay"
            ],
            "arxiv_id": "2511.08502v1",
            "summary": "Autonomous systems increasingly rely on human feedback to align their behavior, expressed as pairwise comparisons, rankings, or demonstrations. While existing methods can adapt behaviors, they often fail to guarantee safety in safety-critical domains. We propose a safety-guaranteed, optimal, and efficient approach to solve the learning problem from preferences, rankings, or demonstrations using Weighted Signal Temporal Logic (WSTL). WSTL learning problems, when implemented naively, lead to multi-linear constraints in the weights to be learned. By introducing structural pruning and log-transform procedures, we reduce the problem size and recast the problem as a Mixed-Integer Linear Program while preserving safety guarantees. Experiments on robotic navigation and real-world Formula 1 data demonstrate that the method effectively captures nuanced preferences and models complex task objectives.",
            "headline_zh": "提出基于加权信号时序逻辑的方法，以安全学习偏好并应用于机器人和F1赛车",
            "intro_zh": [
                "现有方法在安全关键领域学习人类偏好时，常无法保证安全性。",
                "引入结构剪枝和对数变换，将问题转化为混合整数线性规划，确保安全。",
                "在机器人导航和F1数据实验中，有效捕捉复杂偏好和任务目标。"
            ],
            "tags_zh": [
                "加权信号时序逻辑",
                "偏好学习",
                "安全保证",
                "混合整数线性规划",
                "机器人导航",
                "F1赛车数据"
            ],
            "_index": 126
        },
        {
            "title": "Compression then Matching: An Efficient Pre-training Paradigm for Multimodal Embedding",
            "authors": [
                "Da Li",
                "Yuxiao Luo",
                "Keping Bi",
                "Jiafeng Guo",
                "Wei Yuan",
                "Biao Yang",
                "Yan Wang",
                "Fan Yang",
                "Tingting Gao",
                "Guorui Zhou"
            ],
            "arxiv_id": "2511.08480v1",
            "summary": "Vision-language models advance multimodal representation learning by acquiring transferable semantic embeddings, thereby substantially enhancing performance across a range of vision-language tasks, including cross-modal retrieval, clustering, and classification. An effective embedding is expected to comprehensively preserve the semantic content of the input while simultaneously emphasizing features that are discriminative for downstream tasks. Recent approaches demonstrate that VLMs can be adapted into competitive embedding models via large-scale contrastive learning, enabling the simultaneous optimization of two complementary objectives. We argue that the two aforementioned objectives can be decoupled: a comprehensive understanding of the input facilitates the embedding model in achieving superior performance in downstream tasks via contrastive learning. In this paper, we propose CoMa, a compressed pre-training phase, which serves as a warm-up stage for contrastive learning. Experiments demonstrate that with only a small amount of pre-training data, we can transform a VLM into a competitive embedding model. CoMa achieves new state-of-the-art results among VLMs of comparable size on the MMEB, realizing optimization in both efficiency and effectiveness.",
            "headline_zh": "提出CoMa预训练范式以高效优化多模态嵌入模型",
            "intro_zh": [
                "核心问题：多模态嵌入需平衡语义完整性与下游任务判别性",
                "方法要点：通过压缩预训练阶段解耦语义理解与对比学习优化",
                "实验或效果：在MMEB基准上实现SOTA，提升效率与效果"
            ],
            "tags_zh": [
                "多模态嵌入",
                "预训练范式",
                "对比学习",
                "压缩训练",
                "跨模态检索"
            ],
            "_index": 127
        },
        {
            "title": "NeuCLIP: Efficient Large-Scale CLIP Training with Neural Normalizer Optimization",
            "authors": [
                "Xiyuan Wei",
                "Chih-Jen Lin",
                "Tianbao Yang"
            ],
            "arxiv_id": "2511.08417v1",
            "summary": "Accurately estimating the normalization term (also known as the partition function) in the contrastive loss is a central challenge for training Contrastive Language-Image Pre-training (CLIP) models. Conventional methods rely on large batches for approximation, demanding substantial computational resources. To mitigate this issue, prior works introduced per-sample normalizer estimators, which are updated at each epoch in a blockwise coordinate manner to keep track of updated encoders. However, this scheme incurs optimization error that scales with the ratio of dataset size to batch size, limiting effectiveness for large datasets or small batches. To overcome this limitation, we propose NeuCLIP, a novel and elegant optimization framework based on two key ideas: (i) $\\textbf{reformulating}$ the contrastive loss for each sample $\\textbf{via convex analysis}$ into a minimization problem with an auxiliary variable representing its log-normalizer; and (ii) $\\textbf{transforming}$ the resulting minimization over $n$ auxiliary variables (where $n$ is the dataset size) via $\\textbf{variational analysis}$ into the minimization over a compact neural network that predicts the log-normalizers. We design an alternating optimization algorithm that jointly trains the CLIP model and the auxiliary network. By employing a tailored architecture and acceleration techniques for the auxiliary network, NeuCLIP achieves more accurate normalizer estimation, leading to improved performance compared with previous methods. Extensive experiments on large-scale CLIP training, spanning datasets from millions to billions of samples, demonstrate that NeuCLIP outperforms previous methods.",
            "headline_zh": "提出NeuCLIP以优化CLIP训练中的归一化项估计，提升大规模数据集效率",
            "intro_zh": [
                "核心问题：CLIP对比损失中归一化项估计依赖大批次，计算资源需求高",
                "方法要点：通过凸分析和变分分析，将归一化项估计转化为神经网络优化问题",
                "实验或效果：在大规模数据集上优于先前方法，提升训练性能"
            ],
            "tags_zh": [
                "对比学习",
                "归一化优化",
                "CLIP训练",
                "大规模数据集",
                "神经网络估计"
            ],
            "_index": 128
        },
        {
            "title": "Extreme Model Compression with Structured Sparsity at Low Precision",
            "authors": [
                "Dan Liu",
                "Nikita Dvornik",
                "Xue Liu"
            ],
            "arxiv_id": "2511.08360v1",
            "summary": "Deep neural networks (DNNs) are used in many applications, but their large size and high computational cost make them hard to run on devices with limited resources. Two widely used techniques to address this challenge are weight quantization, which lowers the precision of all weights, and structured sparsity, which removes unimportant weights while retaining the important ones at full precision. Although both are effective individually, they are typically studied in isolation due to their compounded negative impact on model accuracy when combined. In this work, we introduce SLOPE Structured Sparsity at Low Precision), a unified framework, to effectively combine structured sparsity and low-bit quantization in a principled way. We show that naively combining sparsity and quantization severely harms performance due to the compounded impact of both techniques. To address this, we propose a training-time regularization strategy that minimizes the discrepancy between full-precision weights and their sparse, quantized counterparts by promoting angular alignment rather than direct matching. On ResNet-18, SLOPE achieves $\\sim20\\times$ model size reduction while retaining $\\sim$99% of the original accuracy. It consistently outperforms state-of-the-art quantization and structured sparsity methods across classification, detection, and segmentation tasks on models such as ResNet-18, ViT-Small, and Mask R-CNN.",
            "headline_zh": "提出SLOPE框架，结合结构化稀疏与低精度量化以压缩模型",
            "intro_zh": [
                "核心问题：深度神经网络在资源受限设备上部署困难，稀疏与量化结合会严重损害精度",
                "方法要点：通过训练时正则化策略，促进权重角度对齐而非直接匹配",
                "实验或效果：在ResNet-18上实现约20倍模型压缩，保持约99%原始精度"
            ],
            "tags_zh": [
                "模型压缩",
                "结构化稀疏",
                "低精度量化",
                "训练正则化",
                "角度对齐"
            ],
            "_index": 129
        },
        {
            "title": "A CODECO Case Study and Initial Validation for Edge Orchestration of Autonomous Mobile Robots",
            "authors": [
                "H. Zhu",
                "T. Samizadeh",
                "R. C. Sofia"
            ],
            "arxiv_id": "2511.08354v1",
            "summary": "Autonomous Mobile Robots (AMRs) increasingly adopt containerized micro-services across the Edge-Cloud continuum. While Kubernetes is the de-facto orchestrator for such systems, its assumptions of stable networks, homogeneous resources, and ample compute capacity do not fully hold in mobile, resource-constrained robotic environments.\n  This paper describes a case study on smart-manufacturing AMRs and performs an initial comparison between CODECO orchestration and standard Kubernetes using a controlled KinD environment. Metrics include pod deployment and deletion times, CPU and memory usage, and inter-pod data rates. The observed results indicate that CODECO offers reduced CPU consumption and more stable communication patterns, at the cost of modest memory overhead (10-15%) and slightly increased pod lifecycle latency due to secure overlay initialization.",
            "headline_zh": "提出CODECO编排以优化边缘自主移动机器人资源使用与通信稳定性",
            "intro_zh": [
                "Kubernetes在移动机器人环境中假设稳定网络和充足资源不成立",
                "通过案例研究比较CODECO与Kubernetes在KinD环境中的性能",
                "CODECO降低CPU消耗、稳定通信，但增加内存开销和延迟"
            ],
            "tags_zh": [
                "边缘编排",
                "自主移动机器人",
                "容器编排",
                "资源优化",
                "通信稳定性"
            ],
            "_index": 130
        },
        {
            "title": "Empowering DINO Representations for Underwater Instance Segmentation via Aligner and Prompter",
            "authors": [
                "Zhiyang Chen",
                "Chen Zhang",
                "Hao Fang",
                "Runmin Cong"
            ],
            "arxiv_id": "2511.08334v1",
            "summary": "Underwater instance segmentation (UIS), integrating pixel-level understanding and instance-level discrimination, is a pivotal technology in marine resource exploration and ecological protection. In recent years, large-scale pretrained visual foundation models, exemplified by DINO, have advanced rapidly and demonstrated remarkable performance on complex downstream tasks. In this paper, we demonstrate that DINO can serve as an effective feature learner for UIS, and we introduce DiveSeg, a novel framework built upon two insightful components: (1) The AquaStyle Aligner, designed to embed underwater color style features into the DINO fine-tuning process, facilitating better adaptation to the underwater domain. (2) The ObjectPrior Prompter, which incorporates binary segmentation-based prompts to deliver object-level priors, provides essential guidance for instance segmentation task that requires both object- and instance-level reasoning. We conduct thorough experiments on the popular UIIS and USIS10K datasets, and the results show that DiveSeg achieves the state-of-the-art performance. Code: https://github.com/ettof/Diveseg.",
            "headline_zh": "提出DiveSeg框架，通过AquaStyle Aligner和ObjectPrior Prompter增强DINO表示，以解决水下实例分割问题。",
            "intro_zh": [
                "核心问题：水下实例分割需结合像素级理解和实例级区分，应用于海洋资源探索和生态保护。",
                "方法要点：引入AquaStyle Aligner嵌入水下颜色风格特征，ObjectPrior Prompter提供基于二值分割的对象先验。",
                "实验或效果：在UIIS和USIS10K数据集上实现最先进性能，代码已开源。"
            ],
            "tags_zh": [
                "水下实例分割",
                "DINO模型",
                "AquaStyle Aligner",
                "ObjectPrior Prompter",
                "海洋视觉",
                "实例分割框架"
            ],
            "_index": 131
        },
        {
            "title": "Learning Omnidirectional Locomotion for a Salamander-Like Quadruped Robot",
            "authors": [
                "Zhiang Liu",
                "Yang Liu",
                "Yongchun Fang",
                "Xian Guo"
            ],
            "arxiv_id": "2511.08299v1",
            "summary": "Salamander-like quadruped robots are designed inspired by the skeletal structure of their biological counterparts. However, existing controllers cannot fully exploit these morphological features and largely rely on predefined gait patterns or joint trajectories, which prevents the generation of diverse and flexible locomotion and limits their applicability in real-world scenarios. In this paper, we propose a learning framework that enables the robot to acquire a diverse repertoire of omnidirectional gaits without reference motions. Each body part is controlled by a phase variable capable of forward and backward evolution, with a phase coverage reward to promote the exploration of the leg phase space. Additionally, morphological symmetry of the robot is incorporated via data augmentation, improving sample efficiency and enforcing both motion-level and task-level symmetry in learned behaviors. Extensive experiments show that the robot successfully acquires 22 omnidirectional gaits exhibiting both dynamic and symmetric movements, demonstrating the effectiveness of the proposed learning framework.",
            "headline_zh": "提出学习框架使机器人获得全向步态，无需参考运动。",
            "intro_zh": [
                "现有控制器依赖预定义步态，限制机器人灵活性和多样性。",
                "使用相位变量和相位覆盖奖励，探索腿部相位空间。",
                "实验获得22种全向步态，展示动态和对称运动。"
            ],
            "tags_zh": [
                "四足机器人",
                "全向运动",
                "强化学习",
                "相位控制",
                "形态对称"
            ],
            "_index": 132
        },
        {
            "title": "SynWeather: Weather Observation Data Synthesis across Multiple Regions and Variables via a General Diffusion Transformer",
            "authors": [
                "Kaiyi Xu",
                "Junchao Gong",
                "Zhiwang Zhou",
                "Zhangrui Li",
                "Yuandong Pu",
                "Yihao Liu",
                "Ben Fei",
                "Fenghua Ling",
                "Wenlong Zhang",
                "Lei Bei"
            ],
            "arxiv_id": "2511.08291v1",
            "summary": "With the advancement of meteorological instruments, abundant data has become available. Current approaches are typically focus on single-variable, single-region tasks and primarily rely on deterministic modeling. This limits unified synthesis across variables and regions, overlooks cross-variable complementarity and often leads to over-smoothed results. To address above challenges, we introduce SynWeather, the first dataset designed for Unified Multi-region and Multi-variable Weather Observation Data Synthesis. SynWeather covers four representative regions: the Continental United States, Europe, East Asia, and Tropical Cyclone regions, as well as provides high-resolution observations of key weather variables, including Composite Radar Reflectivity, Hourly Precipitation, Visible Light, and Microwave Brightness Temperature. In addition, we introduce SynWeatherDiff, a general and probabilistic weather synthesis model built upon the Diffusion Transformer framework to address the over-smoothed problem. Experiments on the SynWeather dataset demonstrate the effectiveness of our network compared with both task-specific and general models.",
            "headline_zh": "提出SynWeatherDiff扩散变换器模型，解决多区域多变量天气观测数据合成中的过平滑问题。",
            "intro_zh": [
                "核心问题：现有方法局限于单变量单区域，忽略跨变量互补性，导致过平滑结果。",
                "方法要点：基于扩散变换器框架构建概率模型，实现统一多区域多变量天气数据合成。",
                "实验或效果：在SynWeather数据集上验证，优于任务特定和通用模型。"
            ],
            "tags_zh": [
                "天气数据合成",
                "扩散变换器",
                "多变量建模",
                "多区域分析",
                "概率模型",
                "观测数据"
            ],
            "_index": 133
        },
        {
            "title": "MAUGIF: Mechanism-Aware Unsupervised General Image Fusion via Dual Cross-Image Autoencoders",
            "authors": [
                "Kunjing Yang",
                "Zhiwei Wang",
                "Minru Bai"
            ],
            "arxiv_id": "2511.08272v1",
            "summary": "Image fusion aims to integrate structural and complementary information from multi-source images. However, existing fusion methods are often either highly task-specific, or general frameworks that apply uniform strategies across diverse tasks, ignoring their distinct fusion mechanisms. To address this issue, we propose a mechanism-aware unsupervised general image fusion (MAUGIF) method based on dual cross-image autoencoders. Initially, we introduce a classification of additive and multiplicative fusion according to the inherent mechanisms of different fusion tasks. Then, dual encoders map source images into a shared latent space, capturing common content while isolating modality-specific details. During the decoding phase, dual decoders act as feature injectors, selectively reintegrating the unique characteristics of each modality into the shared content for reconstruction. The modality-specific features are injected into the source image in the fusion process, generating the fused image that integrates information from both modalities. The architecture of decoders varies according to their fusion mechanisms, enhancing both performance and interpretability. Extensive experiments are conducted on diverse fusion tasks to validate the effectiveness and generalization ability of our method. The code is available at https://anonymous.4open.science/r/MAUGIF.",
            "headline_zh": "提出机制感知无监督通用图像融合方法，通过双交叉图像自编码器解决任务特定与通用策略的平衡问题。",
            "intro_zh": [
                "现有图像融合方法常忽略不同任务的独特机制，导致任务特定或通用策略不适用。",
                "基于双交叉图像自编码器，分类加性和乘性融合机制，选择性注入模态特征。",
                "在多种融合任务上实验验证方法有效性和泛化能力，代码已开源。"
            ],
            "tags_zh": [
                "图像融合",
                "无监督学习",
                "自编码器",
                "机制感知",
                "多模态融合"
            ],
            "_index": 134
        },
        {
            "title": "SWAN - Enabling Fast and Mobile Histopathology Image Annotation through Swipeable Interfaces",
            "authors": [
                "Sweta Banerjee",
                "Timo Gosch",
                "Sara Hester",
                "Viktoria Weiss",
                "Thomas Conrad",
                "Taryn A. Donovan",
                "Nils Porsche",
                "Jonas Ammeling",
                "Christoph Stroblberger",
                "Robert Klopfleisch",
                "Christopher Kaltenecker",
                "Christof A. Bertram",
                "Katharina Breininger",
                "Marc Aubreville"
            ],
            "arxiv_id": "2511.08271v1",
            "summary": "The annotation of large scale histopathology image datasets remains a major bottleneck in developing robust deep learning models for clinically relevant tasks, such as mitotic figure classification. Folder-based annotation workflows are usually slow, fatiguing, and difficult to scale. To address these challenges, we introduce SWipeable ANnotations (SWAN), an open-source, MIT-licensed web application that enables intuitive image patch classification using a swiping gesture. SWAN supports both desktop and mobile platforms, offers real-time metadata capture, and allows flexible mapping of swipe gestures to class labels. In a pilot study with four pathologists annotating 600 mitotic figure image patches, we compared SWAN against a traditional folder-sorting workflow. SWAN enabled rapid annotations with pairwise percent agreement ranging from 86.52% to 93.68% (Cohen's Kappa = 0.61-0.80), while for the folder-based method, the pairwise percent agreement ranged from 86.98% to 91.32% (Cohen's Kappa = 0.63-0.75) for the task of classifying atypical versus normal mitotic figures, demonstrating high consistency between annotators and comparable performance. Participants rated the tool as highly usable and appreciated the ability to annotate on mobile devices. These results suggest that SWAN can accelerate image annotation while maintaining annotation quality, offering a scalable and user-friendly alternative to conventional workflows.",
            "headline_zh": "提出SWAN滑动界面以加速组织病理学图像标注",
            "intro_zh": [
                "大规模组织病理学图像标注是深度学习模型开发的瓶颈，传统文件夹方法缓慢且易疲劳",
                "SWAN为开源Web应用，支持滑动手势分类图像块，可在桌面和移动端实时捕获元数据",
                "试点研究显示SWAN标注速度快，与文件夹方法一致性相当，用户评价高可用性"
            ],
            "tags_zh": [
                "组织病理学图像标注",
                "滑动界面",
                "深度学习",
                "移动应用",
                "开源工具",
                "用户研究"
            ],
            "_index": 135
        },
        {
            "title": "ImagebindDC: Compressing Multi-modal Data with Imagebind-based Condensation",
            "authors": [
                "Yue Min",
                "Shaobo Wang",
                "Jiaze Li",
                "Tianle Niu",
                "Junxin Fan",
                "Yongliang Miao",
                "Lijin Yang",
                "Linfeng Zhang"
            ],
            "arxiv_id": "2511.08263v1",
            "summary": "Data condensation techniques aim to synthesize a compact dataset from a larger one to enable efficient model training, yet while successful in unimodal settings, they often fail in multimodal scenarios where preserving intricate inter-modal dependencies is crucial. To address this, we introduce ImageBindDC, a novel data condensation framework operating within the unified feature space of ImageBind. Our approach moves beyond conventional distribution-matching by employing a powerful Characteristic Function (CF) loss, which operates in the Fourier domain to facilitate a more precise statistical alignment via exact infinite moment matching. We design our objective to enforce three critical levels of distributional consistency: (i) uni-modal alignment, which matches the statistical properties of synthetic and real data within each modality; (ii) cross-modal alignment, which preserves pairwise semantics by matching the distributions of hybrid real-synthetic data pairs; and (iii) joint-modal alignment, which captures the complete multivariate data structure by aligning the joint distribution of real data pairs with their synthetic counterparts. Extensive experiments highlight the effectiveness of ImageBindDC: on the NYU-v2 dataset, a model trained on just 5 condensed datapoints per class achieves lossless performance comparable to one trained on the full dataset, achieving a new state-of-the-art with an 8.2\\% absolute improvement over the previous best method and more than 4$\\times$ less condensation time.",
            "headline_zh": "提出ImageBindDC以解决多模态数据压缩中模态间依赖保留问题",
            "intro_zh": [
                "核心问题：传统数据压缩方法在多模态场景中难以保持模态间复杂依赖关系",
                "方法要点：在ImageBind统一特征空间使用特征函数损失实现精确统计对齐",
                "实验效果：在NYU-v2数据集上，每类5个压缩数据点实现无损性能，超越现有方法"
            ],
            "tags_zh": [
                "数据压缩",
                "多模态学习",
                "特征函数损失",
                "ImageBind框架",
                "分布对齐"
            ],
            "_index": 136
        },
        {
            "title": "LayerEdit: Disentangled Multi-Object Editing via Conflict-Aware Multi-Layer Learning",
            "authors": [
                "Fengyi Fu",
                "Mengqi Huang",
                "Lei Zhang",
                "Zhendong Mao"
            ],
            "arxiv_id": "2511.08251v1",
            "summary": "Text-driven multi-object image editing which aims to precisely modify multiple objects within an image based on text descriptions, has recently attracted considerable interest. Existing works primarily follow the localize-editing paradigm, focusing on independent object localization and editing while neglecting critical inter-object interactions. However, this work points out that the neglected attention entanglements in inter-object conflict regions, inherently hinder disentangled multi-object editing, leading to either inter-object editing leakage or intra-object editing constraints. We thereby propose a novel multi-layer disentangled editing framework LayerEdit, a training-free method which, for the first time, through precise object-layered decomposition and coherent fusion, enables conflict-free object-layered editing. Specifically, LayerEdit introduces a novel \"decompose-editingfusion\" framework, consisting of: (1) Conflict-aware Layer Decomposition module, which utilizes an attention-aware IoU scheme and time-dependent region removing, to enhance conflict awareness and suppression for layer decomposition. (2) Object-layered Editing module, to establish coordinated intra-layer text guidance and cross-layer geometric mapping, achieving disentangled semantic and structural modifications. (3) Transparency-guided Layer Fusion module, to facilitate structure-coherent inter-object layer fusion through precise transparency guidance learning. Extensive experiments verify the superiority of LayerEdit over existing methods, showing unprecedented intra-object controllability and inter-object coherence in complex multi-object scenarios. Codes are available at: https://github.com/fufy1024/LayerEdit.",
            "headline_zh": "提出LayerEdit框架以解决多对象图像编辑中的注意力纠缠问题",
            "intro_zh": [
                "核心问题：现有方法忽视对象间交互，导致编辑泄漏或约束",
                "方法要点：通过分层分解、编辑和融合实现无冲突多对象编辑",
                "实验或效果：在复杂场景中验证了优越的编辑可控性和一致性"
            ],
            "tags_zh": [
                "多对象图像编辑",
                "注意力解缠",
                "分层学习",
                "文本驱动编辑",
                "冲突感知"
            ],
            "_index": 137
        },
        {
            "title": "NERVE: Neighbourhood & Entropy-guided Random-walk for training free open-Vocabulary sEgmentation",
            "authors": [
                "Kunal Mahatha",
                "Jose Dolz",
                "Christian Desrosiers"
            ],
            "arxiv_id": "2511.08248v1",
            "summary": "Despite recent advances in Open-Vocabulary Semantic Segmentation (OVSS), existing training-free methods face several limitations: use of computationally expensive affinity refinement strategies, ineffective fusion of transformer attention maps due to equal weighting or reliance on fixed-size Gaussian kernels to reinforce local spatial smoothness, enforcing isotropic neighborhoods. We propose a strong baseline for training-free OVSS termed as NERVE (Neighbourhood \\& Entropy-guided Random-walk for open-Vocabulary sEgmentation), which uniquely integrates global and fine-grained local information, exploiting the neighbourhood structure from the self-attention layer of a stable diffusion model. We also introduce a stochastic random walk for refining the affinity rather than relying on fixed-size Gaussian kernels for local context. This spatial diffusion process encourages propagation across connected and semantically related areas, enabling it to effectively delineate objects with arbitrary shapes. Whereas most existing approaches treat self-attention maps from different transformer heads or layers equally, our method uses entropy-based uncertainty to select the most relevant maps. Notably, our method does not require any conventional post-processing techniques like Conditional Random Fields (CRF) or Pixel-Adaptive Mask Refinement (PAMR). Experiments are performed on 7 popular semantic segmentation benchmarks, yielding an overall state-of-the-art zero-shot segmentation performance, providing an effective approach to open-vocabulary semantic segmentation.",
            "headline_zh": "提出NERVE方法以解决开放词汇语义分割中训练免费方法的局限性",
            "intro_zh": [
                "现有方法依赖计算昂贵的亲和力优化和固定高斯核，导致局部平滑性不足",
                "NERVE结合全局与局部信息，利用自注意力图进行随机游走优化亲和力",
                "在7个基准测试中实现零样本分割SOTA性能，无需后处理如CRF或PAMR"
            ],
            "tags_zh": [
                "开放词汇语义分割",
                "训练免费方法",
                "随机游走优化",
                "自注意力图",
                "零样本学习",
                "语义分割基准"
            ],
            "_index": 138
        },
        {
            "title": "UCDSC: Open Set UnCertainty aware Deep Simplex Classifier for Medical Image Datasets",
            "authors": [
                "Arnav Aditya",
                "Nitin Kumar",
                "Saurabh Shigwan"
            ],
            "arxiv_id": "2511.08196v1",
            "summary": "Driven by advancements in deep learning, computer-aided diagnoses have made remarkable progress. However, outside controlled laboratory settings, algorithms may encounter several challenges. In the medical domain, these difficulties often stem from limited data availability due to ethical and legal restrictions, as well as the high cost and time required for expert annotations-especially in the face of emerging or rare diseases. In this context, open-set recognition plays a vital role by identifying whether a sample belongs to one of the known classes seen during training or should be rejected as an unknown. Recent studies have shown that features learned in the later stages of deep neural networks are observed to cluster around their class means, which themselves are arranged as individual vertices of a regular simplex [32]. The proposed method introduces a loss function designed to reject samples of unknown classes effectively by penalizing open space regions using auxiliary datasets. This approach achieves significant performance gain across four MedMNIST datasets-BloodMNIST, OCTMNIST, DermaMNIST, TissueMNIST and a publicly available skin dataset [29] outperforming state-of-the-art techniques.",
            "headline_zh": "提出UCDSC方法以解决医学图像开放集识别问题",
            "intro_zh": [
                "医学图像诊断中数据有限且存在未知类别识别挑战",
                "基于深度神经网络特征聚类于正则单纯形顶点设计损失函数",
                "在多个MedMNIST数据集上性能优于现有技术"
            ],
            "tags_zh": [
                "开放集识别",
                "医学图像分析",
                "深度单纯形分类器",
                "不确定性建模"
            ],
            "_index": 139
        },
        {
            "title": "VLMDiff: Leveraging Vision-Language Models for Multi-Class Anomaly Detection with Diffusion",
            "authors": [
                "Samet Hicsonmez",
                "Abd El Rahman Shabayek",
                "Djamila Aouada"
            ],
            "arxiv_id": "2511.08173v1",
            "summary": "Detecting visual anomalies in diverse, multi-class real-world images is a significant challenge. We introduce \\ours, a novel unsupervised multi-class visual anomaly detection framework. It integrates a Latent Diffusion Model (LDM) with a Vision-Language Model (VLM) for enhanced anomaly localization and detection. Specifically, a pre-trained VLM with a simple prompt extracts detailed image descriptions, serving as additional conditioning for LDM training. Current diffusion-based methods rely on synthetic noise generation, limiting their generalization and requiring per-class model training, which hinders scalability. \\ours, however, leverages VLMs to obtain normal captions without manual annotations or additional training. These descriptions condition the diffusion model, learning a robust normal image feature representation for multi-class anomaly detection. Our method achieves competitive performance, improving the pixel-level Per-Region-Overlap (PRO) metric by up to 25 points on the Real-IAD dataset and 8 points on the COCO-AD dataset, outperforming state-of-the-art diffusion-based approaches. Code is available at https://github.com/giddyyupp/VLMDiff.",
            "headline_zh": "提出VLMDiff框架，利用视觉语言模型和扩散模型解决多类视觉异常检测问题",
            "intro_zh": [
                "多类真实图像中的视觉异常检测是重大挑战，现有方法泛化性差且需逐类训练",
                "集成预训练VLM和LDM，通过提示生成图像描述作为扩散模型条件，无需人工标注",
                "在Real-IAD和COCO-AD数据集上，像素级PRO指标提升高达25点和8点，优于先进方法"
            ],
            "tags_zh": [
                "视觉异常检测",
                "扩散模型",
                "视觉语言模型",
                "多类检测",
                "无监督学习",
                "异常定位"
            ],
            "_index": 140
        },
        {
            "title": "Distributed Zero-Shot Learning for Visual Recognition",
            "authors": [
                "Zhi Chen",
                "Yadan Luo",
                "Zi Huang",
                "Jingjing Li",
                "Sen Wang",
                "Xin Yu"
            ],
            "arxiv_id": "2511.08170v1",
            "summary": "In this paper, we propose a Distributed Zero-Shot Learning (DistZSL) framework that can fully exploit decentralized data to learn an effective model for unseen classes. Considering the data heterogeneity issues across distributed nodes, we introduce two key components to ensure the effective learning of DistZSL: a cross-node attribute regularizer and a global attribute-to-visual consensus. Our proposed cross-node attribute regularizer enforces the distances between attribute features to be similar across different nodes. In this manner, the overall attribute feature space would be stable during learning, and thus facilitate the establishment of visual-to-attribute(V2A) relationships. Then, we introduce the global attribute-tovisual consensus to mitigate biased V2A mappings learned from individual nodes. Specifically, we enforce the bilateral mapping between the attribute and visual feature distributions to be consistent across different nodes. Thus, the learned consistent V2A mapping can significantly enhance zero-shot learning across different nodes. Extensive experiments demonstrate that DistZSL achieves superior performance to the state-of-the-art in learning from distributed data.",
            "headline_zh": "提出分布式零样本学习框架以解决去中心化数据中的异构性问题",
            "intro_zh": [
                "核心问题：分布式节点数据异构性影响零样本学习模型对未见类的泛化能力",
                "方法要点：引入跨节点属性正则器和全局属性-视觉共识以稳定特征空间和映射",
                "实验或效果：在分布式数据学习中优于现有方法，提升零样本学习性能"
            ],
            "tags_zh": [
                "分布式学习",
                "零样本学习",
                "属性正则化",
                "视觉-属性映射",
                "数据异构性"
            ],
            "_index": 141
        },
        {
            "title": "Foam Segmentation in Wastewater Treatment Plants: A Federated Learning Approach with Segment Anything Model 2",
            "authors": [
                "Mehmet Batuhan Duman",
                "Alejandro Carnero",
                "Cristian Martín",
                "Daniel Garrido",
                "Manuel Díaz"
            ],
            "arxiv_id": "2511.08130v1",
            "summary": "Foam formation in Wastewater Treatment Plants (WTPs) is a major challenge that can reduce treatment efficiency and increase costs. The ability to automatically examine changes in real-time with respect to the percentage of foam can be of great benefit to the plant. However, large amounts of labeled data are required to train standard Machine Learning (ML) models. The development of these systems is slow due to the scarcity and heterogeneity of labeled data. Additionally, the development is often hindered by the fact that different WTPs do not share their data due to privacy concerns. This paper proposes a new framework to address these challenges by combining Federated Learning (FL) with the state-of-the-art base model for image segmentation, Segment Anything Model 2 (SAM2). The FL paradigm enables collaborative model training across multiple WTPs without centralizing sensitive operational data, thereby ensuring privacy. The framework accelerates training convergence and improves segmentation performance even with limited local datasets by leveraging SAM2's strong pre-trained weights for initialization. The methodology involves fine-tuning SAM2 on distributed clients (edge nodes) using the Flower framework, where a central Fog server orchestrates the process by aggregating model weights without accessing private data. The model was trained and validated using various data collections, including real-world images captured at a WTPs in Granada, Spain, a synthetically generated foam dataset, and images from publicly available datasets to improve generalization. This research offers a practical, scalable, and privacy-aware solution for automatic foam tracking in WTPs. The findings highlight the significant potential of integrating large-scale foundational models into FL systems to solve real-world industrial challenges characterized by distributed and sensitive data.",
            "headline_zh": "提出联邦学习与SAM2结合框架以解决污水处理厂泡沫分割中的数据隐私与稀缺问题",
            "intro_zh": [
                "污水处理厂泡沫形成降低效率，需实时监测但数据标注困难且隐私受限",
                "采用联邦学习在分布式客户端微调SAM2，保护隐私并加速训练收敛",
                "使用真实与合成数据集验证，提升分割性能并实现泛化应用"
            ],
            "tags_zh": [
                "联邦学习",
                "图像分割",
                "污水处理",
                "隐私保护",
                "模型微调",
                "实时监测"
            ],
            "_index": 142
        },
        {
            "title": "CLIP is All You Need for Human-like Semantic Representations in Stable Diffusion",
            "authors": [
                "Cameron Braunstein",
                "Mariya Toneva",
                "Eddy Ilg"
            ],
            "arxiv_id": "2511.08075v1",
            "summary": "Latent diffusion models such as Stable Diffusion achieve state-of-the-art results on text-to-image generation tasks. However, the extent to which these models have a semantic understanding of the images they generate is not well understood. In this work, we investigate whether the internal representations used by these models during text-to-image generation contain semantic information that is meaningful to humans. To do so, we perform probing on Stable Diffusion with simple regression layers that predict semantic attributes for objects and evaluate these predictions against human annotations. Surprisingly, we find that this success can actually be attributed to the text encoding occurring in CLIP rather than the reverse diffusion process. We demonstrate that groups of specific semantic attributes have markedly different decoding accuracy than the average, and are thus represented to different degrees. Finally, we show that attributes become more difficult to disambiguate from one another during the inverse diffusion process, further demonstrating the strongest semantic representation of object attributes in CLIP. We conclude that the separately trained CLIP vision-language model is what determines the human-like semantic representation, and that the diffusion process instead takes the role of a visual decoder.",
            "headline_zh": "揭示CLIP在Stable Diffusion中主导人类语义表示，而非扩散过程",
            "intro_zh": [
                "研究Stable Diffusion在文本到图像生成中是否具有人类可理解的语义表示",
                "使用回归层探测模型内部表示，预测语义属性并与人类标注比较",
                "发现CLIP文本编码决定语义表示，扩散过程仅作为视觉解码器"
            ],
            "tags_zh": [
                "语义表示",
                "文本到图像生成",
                "CLIP模型",
                "扩散模型",
                "模型探测"
            ],
            "_index": 143
        },
        {
            "title": "WEDepth: Efficient Adaptation of World Knowledge for Monocular Depth Estimation",
            "authors": [
                "Gongshu Wang",
                "Zhirui Wang",
                "Kan Yang"
            ],
            "arxiv_id": "2511.08036v1",
            "summary": "Monocular depth estimation (MDE) has widely applicable but remains highly challenging due to the inherently ill-posed nature of reconstructing 3D scenes from single 2D images. Modern Vision Foundation Models (VFMs), pre-trained on large-scale diverse datasets, exhibit remarkable world understanding capabilities that benefit for various vision tasks. Recent studies have demonstrated significant improvements in MDE through fine-tuning these VFMs. Inspired by these developments, we propose WEDepth, a novel approach that adapts VFMs for MDE without modi-fying their structures and pretrained weights, while effec-tively eliciting and leveraging their inherent priors. Our method employs the VFM as a multi-level feature en-hancer, systematically injecting prior knowledge at differ-ent representation levels. Experiments on NYU-Depth v2 and KITTI datasets show that WEDepth establishes new state-of-the-art (SOTA) performance, achieving competi-tive results compared to both diffusion-based approaches (which require multiple forward passes) and methods pre-trained on relative depth. Furthermore, we demonstrate our method exhibits strong zero-shot transfer capability across diverse scenarios.",
            "headline_zh": "提出WEDepth以高效适配世界知识用于单目深度估计",
            "intro_zh": [
                "单目深度估计因从单张2D图像重建3D场景而具有挑战性",
                "方法利用视觉基础模型作为多级特征增强器，不修改其结构或权重",
                "在NYU-Depth v2和KITTI数据集上实现SOTA，并展示强零样本迁移能力"
            ],
            "tags_zh": [
                "单目深度估计",
                "视觉基础模型",
                "特征增强",
                "零样本迁移",
                "世界知识适配"
            ],
            "_index": 144
        },
        {
            "title": "Model Predictive Control via Probabilistic Inference: A Tutorial",
            "authors": [
                "Kohei Honda"
            ],
            "arxiv_id": "2511.08019v1",
            "summary": "Model Predictive Control (MPC) is a fundamental framework for optimizing robot behavior over a finite future horizon. While conventional numerical optimization methods can efficiently handle simple dynamics and cost structures, they often become intractable for the nonlinear or non-differentiable systems commonly encountered in robotics. This article provides a tutorial on probabilistic inference-based MPC, presenting a unified theoretical foundation and a comprehensive overview of representative methods. Probabilistic inference-based MPC approaches, such as Model Predictive Path Integral (MPPI) control, have gained significant attention by reinterpreting optimal control as a problem of probabilistic inference. Rather than relying on gradient-based numerical optimization, these methods estimate optimal control distributions through sampling-based techniques, accommodating arbitrary cost functions and dynamics. We first derive the optimal control distribution from the standard optimal control problem, elucidating its probabilistic interpretation and key characteristics. The widely used MPPI algorithm is then derived as a practical example, followed by discussions on prior and variational distribution design, tuning principles, and theoretical aspects. This article aims to serve as a systematic guide for researchers and practitioners seeking to understand, implement, and extend these methods in robotics and beyond.",
            "headline_zh": "提出基于概率推理的模型预测控制教程，以处理机器人中的非线性系统优化问题",
            "intro_zh": [
                "核心问题：传统优化方法在非线性或不可微机器人系统中难以处理",
                "方法要点：将最优控制重新解释为概率推理，使用采样技术估计控制分布",
                "实验或效果：未知，但提供MPPI算法推导和调优原则作为实用指南"
            ],
            "tags_zh": [
                "模型预测控制",
                "概率推理",
                "最优控制",
                "机器人行为优化",
                "采样方法"
            ],
            "_index": 145
        },
        {
            "title": "Effective Game-Theoretic Motion Planning via Nested Search",
            "authors": [
                "Avishav Engle",
                "Andrey Zhitnikov",
                "Oren Salzman",
                "Omer Ben-Porat",
                "Kiril Solovey"
            ],
            "arxiv_id": "2511.08001v1",
            "summary": "To facilitate effective, safe deployment in the real world, individual robots must reason about interactions with other agents, which often occur without explicit communication. Recent work has identified game theory, particularly the concept of Nash Equilibrium (NE), as a key enabler for behavior-aware decision-making. Yet, existing work falls short of fully unleashing the power of game-theoretic reasoning. Specifically, popular optimization-based methods require simplified robot dynamics and tend to get trapped in local minima due to convexification. Other works that rely on payoff matrices suffer from poor scalability due to the explicit enumeration of all possible trajectories. To bridge this gap, we introduce Game-Theoretic Nested Search (GTNS), a novel, scalable, and provably correct approach for computing NEs in general dynamical systems. GTNS efficiently searches the action space of all agents involved, while discarding trajectories that violate the NE constraint (no unilateral deviation) through an inner search over a lower-dimensional space. Our algorithm enables explicit selection among equilibria by utilizing a user-specified global objective, thereby capturing a rich set of realistic interactions. We demonstrate the approach on a variety of autonomous driving and racing scenarios where we achieve solutions in mere seconds on commodity hardware.",
            "headline_zh": "提出GTNS方法以高效计算多智能体交互中的纳什均衡",
            "intro_zh": [
                "现有方法因简化动力学或枚举轨迹而难以扩展和避免局部最优",
                "GTNS通过嵌套搜索高效筛选动作空间，确保无单边偏离约束",
                "在自动驾驶场景中实现秒级求解，支持用户指定目标选择均衡"
            ],
            "tags_zh": [
                "博弈论规划",
                "纳什均衡计算",
                "多智能体交互",
                "嵌套搜索算法",
                "自动驾驶决策"
            ],
            "_index": 146
        },
        {
            "title": "ChexFract: From General to Specialized - Enhancing Fracture Description Generation",
            "authors": [
                "Nikolay Nechaev",
                "Evgeniia Przhezdzetskaia",
                "Dmitry Umerenkov",
                "Dmitry V. Dylov"
            ],
            "arxiv_id": "2511.07983v1",
            "summary": "Generating accurate and clinically meaningful radiology reports from chest X-ray images remains a significant challenge in medical AI. While recent vision-language models achieve strong results in general radiology report generation, they often fail to adequately describe rare but clinically important pathologies like fractures. This work addresses this gap by developing specialized models for fracture pathology detection and description. We train fracture-specific vision-language models with encoders from MAIRA-2 and CheXagent, demonstrating significant improvements over general-purpose models in generating accurate fracture descriptions. Analysis of model outputs by fracture type, location, and age reveals distinct strengths and limitations of current vision-language model architectures. We publicly release our best-performing fracture-reporting model, facilitating future research in accurate reporting of rare pathologies.",
            "headline_zh": "提出骨折专用视觉语言模型以增强胸部X光骨折描述生成",
            "intro_zh": [
                "核心问题：通用模型难以准确描述胸部X光中罕见但重要的骨折病理",
                "方法要点：基于MAIRA-2和CheXagent编码器训练骨折专用视觉语言模型",
                "实验或效果：模型在骨折描述准确性上显著优于通用模型，并公开最佳模型"
            ],
            "tags_zh": [
                "骨折检测",
                "视觉语言模型",
                "胸部X光",
                "医学报告生成",
                "专用模型训练"
            ],
            "_index": 147
        },
        {
            "title": "Federated CLIP for Resource-Efficient Heterogeneous Medical Image Classification",
            "authors": [
                "Yihang Wu",
                "Ahmad Chaddad"
            ],
            "arxiv_id": "2511.07929v1",
            "summary": "Despite the remarkable performance of deep models in medical imaging, they still require source data for training, which limits their potential in light of privacy concerns. Federated learning (FL), as a decentralized learning framework that trains a shared model with multiple hospitals (a.k.a., FL clients), provides a feasible solution. However, data heterogeneity and resource costs hinder the deployment of FL models, especially when using vision language models (VLM). To address these challenges, we propose a novel contrastive language-image pre-training (CLIP) based FL approach for medical image classification (FedMedCLIP). Specifically, we introduce a masked feature adaptation module (FAM) as a communication module to reduce the communication load while freezing the CLIP encoders to reduce the computational overhead. Furthermore, we propose a masked multi-layer perceptron (MLP) as a private local classifier to adapt to the client tasks. Moreover, we design an adaptive Kullback-Leibler (KL) divergence-based distillation regularization method to enable mutual learning between FAM and MLP. Finally, we incorporate model compression to transmit the FAM parameters while using ensemble predictions for classification. Extensive experiments on four publicly available medical datasets demonstrate that our model provides feasible performance (e.g., 8\\% higher compared to second best baseline on ISIC2019) with reasonable resource cost (e.g., 120$\\times$ faster than FedAVG).",
            "headline_zh": "提出FedMedCLIP以解决医疗图像分类中联邦学习的异构数据与资源成本问题",
            "intro_zh": [
                "核心问题：医疗图像分类中数据隐私、异构性和高资源成本限制联邦学习部署",
                "方法要点：引入掩码特征适应模块和私有分类器，结合KL蒸馏正则化与模型压缩",
                "实验或效果：在四个公开数据集上性能提升8%，通信速度提升120倍"
            ],
            "tags_zh": [
                "联邦学习",
                "医疗图像分类",
                "CLIP模型",
                "特征适应",
                "模型压缩",
                "KL蒸馏"
            ],
            "_index": 148
        },
        {
            "title": "Local Path Planning with Dynamic Obstacle Avoidance in Unstructured Environments",
            "authors": [
                "Okan Arif Guvenkaya",
                "Selim Ahmet Iz",
                "Mustafa Unel"
            ],
            "arxiv_id": "2511.07927v1",
            "summary": "Obstacle avoidance and path planning are essential for guiding unmanned ground vehicles (UGVs) through environments that are densely populated with dynamic obstacles. This paper develops a novel approach that combines tangentbased path planning and extrapolation methods to create a new decision-making algorithm for local path planning. In the assumed scenario, a UGV has a prior knowledge of its initial and target points within the dynamic environment. A global path has already been computed, and the robot is provided with waypoints along this path. As the UGV travels between these waypoints, the algorithm aims to avoid collisions with dynamic obstacles. These obstacles follow polynomial trajectories, with their initial positions randomized in the local map and velocities randomized between O and the allowable physical velocity limit of the robot, along with some random accelerations. The developed algorithm is tested in several scenarios where many dynamic obstacles move randomly in the environment. Simulation results show the effectiveness of the proposed local path planning strategy by gradually generating a collision free path which allows the robot to navigate safely between initial and the target locations.",
            "headline_zh": "提出基于切线和外推的局部路径规划算法，用于UGV在动态障碍物环境中的避障。",
            "intro_zh": [
                "核心问题：UGV在动态障碍物密集的非结构化环境中需安全导航并避免碰撞。",
                "方法要点：结合切线路径规划和外推方法，开发新决策算法处理动态障碍物。",
                "实验或效果：模拟测试显示算法能逐步生成无碰撞路径，确保机器人安全到达目标。"
            ],
            "tags_zh": [
                "局部路径规划",
                "动态避障",
                "无人地面车辆",
                "非结构化环境",
                "切线规划",
                "外推方法"
            ],
            "_index": 149
        },
        {
            "title": "Generating Sketches in a Hierarchical Auto-Regressive Process for Flexible Sketch Drawing Manipulation at Stroke-Level",
            "authors": [
                "Sicong Zang",
                "Shuhui Gao",
                "Zhijun Fang"
            ],
            "arxiv_id": "2511.07889v1",
            "summary": "Generating sketches with specific patterns as expected, i.e., manipulating sketches in a controllable way, is a popular task. Recent studies control sketch features at stroke-level by editing values of stroke embeddings as conditions. However, in order to provide generator a global view about what a sketch is going to be drawn, all these edited conditions should be collected and fed into generator simultaneously before generation starts, i.e., no further manipulation is allowed during sketch generating process. In order to realize sketch drawing manipulation more flexibly, we propose a hierarchical auto-regressive sketch generating process. Instead of generating an entire sketch at once, each stroke in a sketch is generated in a three-staged hierarchy: 1) predicting a stroke embedding to represent which stroke is going to be drawn, and 2) anchoring the predicted stroke on the canvas, and 3) translating the embedding to a sequence of drawing actions to form the full sketch. Moreover, the stroke prediction, anchoring and translation are proceeded auto-regressively, i.e., both the recently generated strokes and their positions are considered to predict the current one, guiding model to produce an appropriate stroke at a suitable position to benefit the full sketch generation. It is flexible to manipulate stroke-level sketch drawing at any time during generation by adjusting the exposed editable stroke embeddings.",
            "headline_zh": "提出分层自回归过程以实现笔画级灵活草图绘制操控",
            "intro_zh": [
                "现有方法需在生成前固定所有笔画条件，无法在过程中灵活操控",
                "采用三阶段分层自回归过程：预测笔画嵌入、锚定位置、转换为绘制动作",
                "实验表明模型能实时调整笔画嵌入，提升草图生成的灵活性和可控性"
            ],
            "tags_zh": [
                "草图生成",
                "自回归模型",
                "笔画级操控",
                "分层过程",
                "可控生成"
            ],
            "_index": 150
        },
        {
            "title": "Semantic-Consistent Bidirectional Contrastive Hashing for Noisy Multi-Label Cross-Modal Retrieval",
            "authors": [
                "Likang Peng",
                "Chao Su",
                "Wenyuan Wu",
                "Yuan Sun",
                "Dezhong Peng",
                "Xi Peng",
                "Xu Wang"
            ],
            "arxiv_id": "2511.07780v1",
            "summary": "Cross-modal hashing (CMH) facilitates efficient retrieval across different modalities (e.g., image and text) by encoding data into compact binary representations. While recent methods have achieved remarkable performance, they often rely heavily on fully annotated datasets, which are costly and labor-intensive to obtain. In real-world scenarios, particularly in multi-label datasets, label noise is prevalent and severely degrades retrieval performance. Moreover, existing CMH approaches typically overlook the partial semantic overlaps inherent in multi-label data, limiting their robustness and generalization. To tackle these challenges, we propose a novel framework named Semantic-Consistent Bidirectional Contrastive Hashing (SCBCH). The framework comprises two complementary modules: (1) Cross-modal Semantic-Consistent Classification (CSCC), which leverages cross-modal semantic consistency to estimate sample reliability and reduce the impact of noisy labels; (2) Bidirectional Soft Contrastive Hashing (BSCH), which dynamically generates soft contrastive sample pairs based on multi-label semantic overlap, enabling adaptive contrastive learning between semantically similar and dissimilar samples across modalities. Extensive experiments on four widely-used cross-modal retrieval benchmarks validate the effectiveness and robustness of our method, consistently outperforming state-of-the-art approaches under noisy multi-label conditions.",
            "headline_zh": "提出语义一致双向对比哈希以解决噪声多标签跨模态检索问题",
            "intro_zh": [
                "核心问题：多标签数据中标签噪声和语义重叠未被充分处理，影响检索性能。",
                "方法要点：结合跨模态语义一致分类和双向软对比哈希，动态生成样本对。",
                "实验或效果：在四个基准数据集上验证，噪声条件下优于现有方法。"
            ],
            "tags_zh": [
                "跨模态哈希",
                "多标签检索",
                "噪声标签处理",
                "对比学习",
                "语义一致性"
            ],
            "_index": 151
        },
        {
            "title": "Class Incremental Medical Image Segmentation via Prototype-Guided Calibration and Dual-Aligned Distillation",
            "authors": [
                "Shengqian Zhu",
                "Chengrong Yu",
                "Qiang Wang",
                "Ying Song",
                "Guangjun Li",
                "Jiafei Wu",
                "Xiaogang Xu",
                "Zhang Yi",
                "Junjie Hu"
            ],
            "arxiv_id": "2511.07749v1",
            "summary": "Class incremental medical image segmentation (CIMIS) aims to preserve knowledge of previously learned classes while learning new ones without relying on old-class labels. However, existing methods 1) either adopt one-size-fits-all strategies that treat all spatial regions and feature channels equally, which may hinder the preservation of accurate old knowledge, 2) or focus solely on aligning local prototypes with global ones for old classes while overlooking their local representations in new data, leading to knowledge degradation. To mitigate the above issues, we propose Prototype-Guided Calibration Distillation (PGCD) and Dual-Aligned Prototype Distillation (DAPD) for CIMIS in this paper. Specifically, PGCD exploits prototype-to-feature similarity to calibrate class-specific distillation intensity in different spatial regions, effectively reinforcing reliable old knowledge and suppressing misleading information from old classes. Complementarily, DAPD aligns the local prototypes of old classes extracted from the current model with both global prototypes and local prototypes, further enhancing segmentation performance on old categories. Comprehensive evaluations on two widely used multi-organ segmentation benchmarks demonstrate that our method outperforms state-of-the-art methods, highlighting its robustness and generalization capabilities.",
            "headline_zh": "提出原型引导校准与双对齐蒸馏以解决医学图像增量分割中的知识遗忘问题",
            "intro_zh": [
                "核心问题：现有方法对空间区域和特征通道处理均等，或仅对齐原型而忽略局部表示，导致旧知识退化。",
                "方法要点：PGCD利用原型-特征相似性校准蒸馏强度，DAPD对齐局部原型与全局原型以增强旧类分割。",
                "实验或效果：在多个多器官分割基准上评估，方法优于现有技术，展现鲁棒性和泛化能力。"
            ],
            "tags_zh": [
                "医学图像分割",
                "增量学习",
                "原型蒸馏",
                "知识保留",
                "多器官分割"
            ],
            "_index": 152
        }
    ]
}