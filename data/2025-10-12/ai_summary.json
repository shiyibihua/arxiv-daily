{
    "papers": [
        {
            "title": "StreamingVLM: Real-Time Understanding for Infinite Video Streams",
            "authors": [
                "Ruyi Xu",
                "Guangxuan Xiao",
                "Yukang Chen",
                "Liuning He",
                "Kelly Peng",
                "Yao Lu",
                "Song Han"
            ],
            "arxiv_id": "2510.09608v1",
            "summary": "Vision-language models (VLMs) could power real-time assistants and autonomous\nagents, but they face a critical challenge: understanding near-infinite video\nstreams without escalating latency and memory usage. Processing entire videos\nwith full attention leads to quadratic computational costs and poor performance\non long videos. Meanwhile, simple sliding window methods are also flawed, as\nthey either break coherence or suffer from high latency due to redundant\nrecomputation. In this paper, we introduce StreamingVLM, a model designed for\nreal-time, stable understanding of infinite visual input. Our approach is a\nunified framework that aligns training with streaming inference. During\ninference, we maintain a compact KV cache by reusing states of attention sinks,\na short window of recent vision tokens, and a long window of recent text\ntokens. This streaming ability is instilled via a simple supervised fine-tuning\n(SFT) strategy that applies full attention on short, overlapped video chunks,\nwhich effectively mimics the inference-time attention pattern without training\non prohibitively long contexts. For evaluation, we build Inf-Streams-Eval, a\nnew benchmark with videos averaging over two hours that requires dense,\nper-second alignment between frames and text. On Inf-Streams-Eval, StreamingVLM\nachieves a 66.18% win rate against GPT-4O mini and maintains stable, real-time\nperformance at up to 8 FPS on a single NVIDIA H100. Notably, our SFT strategy\nalso enhances general VQA abilities without any VQA-specific fine-tuning,\nimproving performance on LongVideoBench by +4.30 and OVOBench Realtime by\n+5.96. Code is available at https://github.com/mit-han-lab/streaming-vlm.",
            "headline_zh": "提出StreamingVLM以解决无限视频流实时理解中的延迟与内存问题",
            "intro_zh": [
                "核心问题：处理无限视频流时，全注意力机制导致二次计算成本和长视频性能下降。",
                "方法要点：通过复用注意力sink状态、短窗口视觉令牌和长窗口文本令牌，维持紧凑KV缓存。",
                "实验效果：在Inf-Streams-Eval基准上，以66.18%胜率优于GPT-4O mini，实时性能达8 FPS。"
            ],
            "tags_zh": [
                "无限视频流理解",
                "实时视觉语言模型",
                "KV缓存优化",
                "监督微调策略",
                "长视频基准评估"
            ],
            "_index": 0
        },
        {
            "title": "VITA-VLA: Efficiently Teaching Vision-Language Models to Act via Action Expert Distillation",
            "authors": [
                "Shaoqi Dong",
                "Chaoyou Fu",
                "Haihan Gao",
                "Yi-Fan Zhang",
                "Chi Yan",
                "Chu Wu",
                "Xiaoyu Liu",
                "Yunhang Shen",
                "Jing Huo",
                "Deqiang Jiang",
                "Haoyu Cao",
                "Yang Gao",
                "Xing Sun",
                "Ran He",
                "Caifeng Shan"
            ],
            "arxiv_id": "2510.09607v1",
            "summary": "Vision-Language Action (VLA) models significantly advance robotic\nmanipulation by leveraging the strong perception capabilities of pretrained\nvision-language models (VLMs). By integrating action modules into these\npretrained models, VLA methods exhibit improved generalization. However,\ntraining them from scratch is costly. In this work, we propose a simple yet\neffective distillation-based framework that equips VLMs with action-execution\ncapability by transferring knowledge from pretrained small action models. Our\narchitecture retains the original VLM structure, adding only an action token\nand a state encoder to incorporate physical inputs. To distill action\nknowledge, we adopt a two-stage training strategy. First, we perform\nlightweight alignment by mapping VLM hidden states into the action space of the\nsmall action model, enabling effective reuse of its pretrained action decoder\nand avoiding expensive pretraining. Second, we selectively fine-tune the\nlanguage model, state encoder, and action modules, enabling the system to\nintegrate multimodal inputs with precise action generation. Specifically, the\naction token provides the VLM with a direct handle for predicting future\nactions, while the state encoder allows the model to incorporate robot dynamics\nnot captured by vision alone. This design yields substantial efficiency gains\nover training large VLA models from scratch. Compared with previous\nstate-of-the-art methods, our method achieves 97.3% average success rate on\nLIBERO (11.8% improvement) and 93.5% on LIBERO-LONG (24.5% improvement). In\nreal-world experiments across five manipulation tasks, our method consistently\noutperforms the teacher model, achieving 82.0% success rate (17% improvement),\nwhich demonstrate that action distillation effectively enables VLMs to generate\nprecise actions while substantially reducing training costs.",
            "headline_zh": "提出基于动作专家蒸馏的框架，高效赋予视觉语言模型动作执行能力",
            "intro_zh": [
                "核心问题：训练视觉语言动作模型成本高昂，需从零开始。",
                "方法要点：通过两阶段蒸馏，重用预训练小动作模型知识，添加动作令牌和状态编码器。",
                "实验效果：在LIBERO和真实世界任务中，成功率显著提升，训练成本降低。"
            ],
            "tags_zh": [
                "视觉语言动作模型",
                "知识蒸馏",
                "机器人操作",
                "动作预测",
                "高效训练"
            ],
            "_index": 1
        },
        {
            "title": "SpaceVista: All-Scale Visual Spatial Reasoning from mm to km",
            "authors": [
                "Peiwen Sun",
                "Shiqiang Lang",
                "Dongming Wu",
                "Yi Ding",
                "Kaituo Feng",
                "Huadai Liu",
                "Zhen Ye",
                "Rui Liu",
                "Yun-Hui Liu",
                "Jianan Wang",
                "Xiangyu Yue"
            ],
            "arxiv_id": "2510.09606v1",
            "summary": "With the current surge in spatial reasoning explorations, researchers have\nmade significant progress in understanding indoor scenes, but still struggle\nwith diverse applications such as robotics and autonomous driving. This paper\naims to advance all-scale spatial reasoning across diverse scenarios by\ntackling two key challenges: 1) the heavy reliance on indoor 3D scans and\nlabor-intensive manual annotations for dataset curation; 2) the absence of\neffective all-scale scene modeling, which often leads to overfitting to\nindividual scenes. In this paper, we introduce a holistic solution that\nintegrates a structured spatial reasoning knowledge system, scale-aware\nmodeling, and a progressive training paradigm, as the first attempt to broaden\nthe all-scale spatial intelligence of MLLMs to the best of our knowledge. Using\na task-specific, specialist-driven automated pipeline, we curate over 38K video\nscenes across 5 spatial scales to create SpaceVista-1M, a dataset comprising\napproximately 1M spatial QA pairs spanning 19 diverse task types. While\nspecialist models can inject useful domain knowledge, they are not reliable for\nevaluation. We then build an all-scale benchmark with precise annotations by\nmanually recording, retrieving, and assembling video-based data. However, naive\ntraining with SpaceVista-1M often yields suboptimal results due to the\npotential knowledge conflict. Accordingly, we introduce SpaceVista-7B, a\nspatial reasoning model that accepts dense inputs beyond semantics and uses\nscale as an anchor for scale-aware experts and progressive rewards. Finally,\nextensive evaluations across 5 benchmarks, including our SpaceVista-Bench,\ndemonstrate competitive performance, showcasing strong generalization across\nall scales and scenarios. Our dataset, model, and benchmark will be released on\nhttps://peiwensun2000.github.io/mm2km .",
            "headline_zh": "提出SpaceVista以解决全尺度视觉空间推理问题，涵盖毫米到千米场景。",
            "intro_zh": [
                "核心问题：依赖室内3D扫描和手动标注，缺乏有效全尺度场景建模。",
                "方法要点：集成结构化知识系统、尺度感知建模和渐进训练范式。",
                "实验或效果：在多个基准测试中表现竞争性，展示跨尺度和场景的强泛化能力。"
            ],
            "tags_zh": [
                "全尺度空间推理",
                "尺度感知建模",
                "渐进训练",
                "空间问答数据集",
                "多模态大语言模型",
                "视觉基准测试"
            ],
            "_index": 2
        },
        {
            "title": "STaTS: Structure-Aware Temporal Sequence Summarization via Statistical Window Merging",
            "authors": [
                "Disharee Bhowmick",
                "Ranjith Ramanathan",
                "Sathyanarayanan N. Aakur"
            ],
            "arxiv_id": "2510.09593v1",
            "summary": "Time series data often contain latent temporal structure, transitions between\nlocally stationary regimes, repeated motifs, and bursts of variability, that\nare rarely leveraged in standard representation learning pipelines. Existing\nmodels typically operate on raw or fixed-window sequences, treating all time\nsteps as equally informative, which leads to inefficiencies, poor robustness,\nand limited scalability in long or noisy sequences. We propose STaTS, a\nlightweight, unsupervised framework for Structure-Aware Temporal Summarization\nthat adaptively compresses both univariate and multivariate time series into\ncompact, information-preserving token sequences. STaTS detects change points\nacross multiple temporal resolutions using a BIC-based statistical divergence\ncriterion, then summarizes each segment using simple functions like the mean or\ngenerative models such as GMMs. This process achieves up to 30x sequence\ncompression while retaining core temporal dynamics. STaTS operates as a\nmodel-agnostic preprocessor and can be integrated with existing unsupervised\ntime series encoders without retraining. Extensive experiments on 150+\ndatasets, including classification tasks on the UCR-85, UCR-128, and UEA-30\narchives, and forecasting on ETTh1 and ETTh2, ETTm1, and Electricity,\ndemonstrate that STaTS enables 85-90\\% of the full-model performance while\noffering dramatic reductions in computational cost. Moreover, STaTS improves\nrobustness under noise and preserves discriminative structure, outperforming\nuniform and clustering-based compression baselines. These results position\nSTaTS as a principled, general-purpose solution for efficient, structure-aware\ntime series modeling.",
            "headline_zh": "提出STaTS框架以解决时间序列结构感知压缩问题",
            "intro_zh": [
                "时间序列数据常含潜在结构，但现有方法忽略结构导致效率低、鲁棒性差",
                "STaTS使用BIC准则检测变点，自适应压缩序列为紧凑token，保留核心动态",
                "实验显示STaTS在150+数据集上实现高压缩比，保持85-90%性能并提升鲁棒性"
            ],
            "tags_zh": [
                "时间序列压缩",
                "结构感知建模",
                "无监督学习",
                "变点检测",
                "计算效率优化"
            ],
            "_index": 3
        },
        {
            "title": "Vision Language Models: A Survey of 26K Papers",
            "authors": [
                "Fengming Lin"
            ],
            "arxiv_id": "2510.09586v1",
            "summary": "We present a transparent, reproducible measurement of research trends across\n26,104 accepted papers from CVPR, ICLR, and NeurIPS spanning 2023-2025. Titles\nand abstracts are normalized, phrase-protected, and matched against a\nhand-crafted lexicon to assign up to 35 topical labels and mine fine-grained\ncues about tasks, architectures, training regimes, objectives, datasets, and\nco-mentioned modalities. The analysis quantifies three macro shifts: (1) a\nsharp rise of multimodal vision-language-LLM work, which increasingly reframes\nclassic perception as instruction following and multi-step reasoning; (2)\nsteady expansion of generative methods, with diffusion research consolidating\naround controllability, distillation, and speed; and (3) resilient 3D and video\nactivity, with composition moving from NeRFs to Gaussian splatting and a\ngrowing emphasis on human- and agent-centric understanding. Within VLMs,\nparameter-efficient adaptation like prompting/adapters/LoRA and lightweight\nvision-language bridges dominate; training practice shifts from building\nencoders from scratch to instruction tuning and finetuning strong backbones;\ncontrastive objectives recede relative to cross-entropy/ranking and\ndistillation. Cross-venue comparisons show CVPR has a stronger 3D footprint and\nICLR the highest VLM share, while reliability themes such as efficiency or\nrobustness diffuse across areas. We release the lexicon and methodology to\nenable auditing and extension. Limitations include lexicon recall and\nabstract-only scope, but the longitudinal signals are consistent across venues\nand years.",
            "headline_zh": "分析26K篇论文以量化计算机视觉与语言模型的研究趋势",
            "intro_zh": [
                "核心问题：量化CVPR、ICLR和NeurIPS会议中视觉语言模型的研究趋势与宏观变化。",
                "方法要点：使用手工词典对标题和摘要进行标准化处理，分配主题标签并挖掘任务细节。",
                "实验或效果：识别多模态、生成方法和3D视频活动的三大宏观转变，并比较不同会议特点。"
            ],
            "tags_zh": [
                "视觉语言模型",
                "多模态研究",
                "生成方法",
                "3D视频分析",
                "参数高效适应",
                "研究趋势分析"
            ],
            "_index": 4
        },
        {
            "title": "FSP-DETR: Few-Shot Prototypical Parasitic Ova Detection",
            "authors": [
                "Shubham Trehan",
                "Udhav Ramachandran",
                "Akash Rao",
                "Ruth Scimeca",
                "Sathyanarayanan N. Aakur"
            ],
            "arxiv_id": "2510.09583v1",
            "summary": "Object detection in biomedical settings is fundamentally constrained by the\nscarcity of labeled data and the frequent emergence of novel or rare\ncategories. We present FSP-DETR, a unified detection framework that enables\nrobust few-shot detection, open-set recognition, and generalization to unseen\nbiomedical tasks within a single model. Built upon a class-agnostic DETR\nbackbone, our approach constructs class prototypes from original support images\nand learns an embedding space using augmented views and a lightweight\ntransformer decoder. Training jointly optimizes a prototype matching loss, an\nalignment-based separation loss, and a KL divergence regularization to improve\ndiscriminative feature learning and calibration under scarce supervision.\nUnlike prior work that tackles these tasks in isolation, FSP-DETR enables\ninference-time flexibility to support unseen class recognition, background\nrejection, and cross-task adaptation without retraining. We also introduce a\nnew ova species detection benchmark with 20 parasite classes and establish\nstandardized evaluation protocols. Extensive experiments across ova, blood\ncell, and malaria detection tasks demonstrate that FSP-DETR significantly\noutperforms prior few-shot and prototype-based detectors, especially in\nlow-shot and open-set scenarios.",
            "headline_zh": "提出FSP-DETR框架以解决生物医学中少样本检测和开放集识别问题",
            "intro_zh": [
                "核心问题：生物医学目标检测中标注数据稀缺和新类别频繁出现",
                "方法要点：基于类无关DETR构建原型，使用增强视图和轻量解码器学习嵌入空间",
                "实验或效果：在多个任务中显著优于现有方法，尤其在少样本和开放集场景"
            ],
            "tags_zh": [
                "少样本目标检测",
                "原型学习",
                "开放集识别",
                "生物医学图像分析",
                "DETR框架"
            ],
            "_index": 5
        },
        {
            "title": "Dyna-Mind: Learning to Simulate from Experience for Better AI Agents",
            "authors": [
                "Xiao Yu",
                "Baolin Peng",
                "Michel Galley",
                "Hao Cheng",
                "Qianhui Wu",
                "Janardhan Kulkarni",
                "Suman Nath",
                "Zhou Yu",
                "Jianfeng Gao"
            ],
            "arxiv_id": "2510.09577v1",
            "summary": "Reasoning models have recently shown remarkable progress in domains such as\nmath and coding. However, their expert-level abilities in math and coding\ncontrast sharply with their performance in long-horizon, interactive tasks such\nas web navigation and computer/phone-use. Inspired by literature on human\ncognition, we argue that current AI agents need ''vicarious trial and error'' -\nthe capacity to mentally simulate alternative futures before acting - in order\nto enhance their understanding and performance in complex interactive\nenvironments. We introduce Dyna-Mind, a two-stage training framework that\nexplicitly teaches (V)LM agents to integrate such simulation into their\nreasoning. In stage 1, we introduce Reasoning with Simulations (ReSim), which\ntrains the agent to generate structured reasoning traces from expanded search\ntrees built from real experience gathered through environment interactions.\nReSim thus grounds the agent's reasoning in faithful world dynamics and equips\nit with the ability to anticipate future states in its reasoning. In stage 2,\nwe propose Dyna-GRPO, an online reinforcement learning method to further\nstrengthen the agent's simulation and decision-making ability by using both\noutcome rewards and intermediate states as feedback from real rollouts.\nExperiments on two synthetic benchmarks (Sokoban and ALFWorld) and one\nrealistic benchmark (AndroidWorld) demonstrate that (1) ReSim effectively\ninfuses simulation ability into AI agents, and (2) Dyna-GRPO leverages outcome\nand interaction-level signals to learn better policies for long-horizon,\nplanning-intensive tasks. Together, these results highlight the central role of\nsimulation in enabling AI agents to reason, plan, and act more effectively in\nthe ever more challenging environments.",
            "headline_zh": "提出Dyna-Mind框架以提升AI代理在复杂交互任务中的模拟与决策能力",
            "intro_zh": [
                "核心问题：AI代理在数学和编码领域表现优异，但在长视野交互任务中性能不足，缺乏心理模拟能力。",
                "方法要点：采用两阶段训练，ReSim基于真实经验生成结构化推理轨迹，Dyna-GRPO利用在线强化学习优化模拟与决策。",
                "实验或效果：在Sokoban、ALFWorld和AndroidWorld基准测试中，验证了模拟能力提升和策略学习效果。"
            ],
            "tags_zh": [
                "AI代理",
                "模拟学习",
                "强化学习",
                "长视野任务",
                "推理模型",
                "交互环境"
            ],
            "_index": 6
        },
        {
            "title": "Zero-shot Structure Learning and Planning for Autonomous Robot Navigation using Active Inference",
            "authors": [
                "Daria de tinguy",
                "Tim Verbelen",
                "Emilio Gamba",
                "Bart Dhoedt"
            ],
            "arxiv_id": "2510.09574v1",
            "summary": "Autonomous navigation in unfamiliar environments requires robots to\nsimultaneously explore, localise, and plan under uncertainty, without relying\non predefined maps or extensive training. We present a biologically inspired,\nActive Inference-based framework, Active Inference MAPping and Planning\n(AIMAPP). This model unifies mapping, localisation, and decision-making within\na single generative model. Inspired by hippocampal navigation, it uses\ntopological reasoning, place-cell encoding, and episodic memory to guide\nbehaviour. The agent builds and updates a sparse topological map online, learns\nstate transitions dynamically, and plans actions by minimising Expected Free\nEnergy. This allows it to balance goal-directed and exploratory behaviours. We\nimplemented a ROS-compatible navigation system that is sensor and\nrobot-agnostic, capable of integrating with diverse hardware configurations. It\noperates in a fully self-supervised manner, is resilient to drift, and supports\nboth exploration and goal-directed navigation without any pre-training. We\ndemonstrate robust performance in large-scale real and simulated environments\nagainst state-of-the-art planning models, highlighting the system's\nadaptability to ambiguous observations, environmental changes, and sensor\nnoise. The model offers a biologically inspired, modular solution to scalable,\nself-supervised navigation in unstructured settings. AIMAPP is available at\nhttps://github.com/decide-ugent/AIMAPP.",
            "headline_zh": "提出AIMAPP框架以解决未知环境中机器人自主导航问题",
            "intro_zh": [
                "核心问题：未知环境中机器人需同时探索、定位和规划，无预定义地图或训练",
                "方法要点：基于主动推理，统一建图、定位和决策，使用拓扑推理和预期自由能量最小化",
                "实验或效果：在真实和模拟环境中展示鲁棒性能，适应观测模糊和环境变化"
            ],
            "tags_zh": [
                "自主导航",
                "主动推理",
                "拓扑建图",
                "零样本学习",
                "机器人规划"
            ],
            "_index": 7
        },
        {
            "title": "Differential Analysis of Pseudo Haptic Feedback: Novel Comparative Study of Visual and Auditory Cue Integration for Psychophysical Evaluation",
            "authors": [
                "Nishant Gautam",
                "Somya Sharma",
                "Peter Corcoran",
                "Kaspar Althoefer"
            ],
            "arxiv_id": "2510.09570v1",
            "summary": "Pseudo-haptics exploit carefully crafted visual or auditory cues to trick the\nbrain into \"feeling\" forces that are never physically applied, offering a\nlow-cost alternative to traditional haptic hardware. Here, we present a\ncomparative psychophysical study that quantifies how visual and auditory\nstimuli combine to evoke pseudo-haptic pressure sensations on a commodity\ntablet. Using a Unity-based Rollball game, participants (n = 4) guided a\nvirtual ball across three textured terrains while their finger forces were\ncaptured in real time with a Robotous RFT40 force-torque sensor. Each terrain\nwas paired with a distinct rolling-sound profile spanning 440 Hz - 4.7 kHz, 440\nHz - 13.1 kHz, or 440 Hz - 8.9 kHz; crevice collisions triggered additional\n\"knocking\" bursts to heighten realism. Average tactile forces increased\nsystematically with cue intensity: 0.40 N, 0.79 N and 0.88 N for visual-only\ntrials and 0.41 N, 0.81 N and 0.90 N for audio-only trials on Terrains 1-3,\nrespectively. Higher audio frequencies and denser visual textures both elicited\nstronger muscle activation, and their combination further reduced the force\nneeded to perceive surface changes, confirming multisensory integration. These\nresults demonstrate that consumer-grade isometric devices can reliably induce\nand measure graded pseudo-haptic feedback without specialized actuators,\nopening a path toward affordable rehabilitation tools, training simulators and\nassistive interfaces.",
            "headline_zh": "比较视觉与听觉线索整合，评估伪触觉反馈在平板设备上的心理物理效应",
            "intro_zh": [
                "核心问题：如何量化视觉和听觉线索在诱导伪触觉压力感中的整合效果。",
                "方法要点：使用Unity游戏和力传感器，结合不同纹理与声音频率进行实验。",
                "实验效果：多感官集成增强感知，降低所需力，验证消费级设备可行性。"
            ],
            "tags_zh": [
                "伪触觉反馈",
                "多感官集成",
                "心理物理评估",
                "力传感器",
                "Unity游戏",
                "康复工具"
            ],
            "_index": 8
        },
        {
            "title": "TC-LoRA: Temporally Modulated Conditional LoRA for Adaptive Diffusion Control",
            "authors": [
                "Minkyoung Cho",
                "Ruben Ohana",
                "Christian Jacobsen",
                "Adityan Jothi",
                "Min-Hung Chen",
                "Z. Morley Mao",
                "Ethem Can"
            ],
            "arxiv_id": "2510.09561v1",
            "summary": "Current controllable diffusion models typically rely on fixed architectures\nthat modify intermediate activations to inject guidance conditioned on a new\nmodality. This approach uses a static conditioning strategy for a dynamic,\nmulti-stage denoising process, limiting the model's ability to adapt its\nresponse as the generation evolves from coarse structure to fine detail. We\nintroduce TC-LoRA (Temporally Modulated Conditional LoRA), a new paradigm that\nenables dynamic, context-aware control by conditioning the model's weights\ndirectly. Our framework uses a hypernetwork to generate LoRA adapters\non-the-fly, tailoring weight modifications for the frozen backbone at each\ndiffusion step based on time and the user's condition. This mechanism enables\nthe model to learn and execute an explicit, adaptive strategy for applying\nconditional guidance throughout the entire generation process. Through\nexperiments on various data domains, we demonstrate that this dynamic,\nparametric control significantly enhances generative fidelity and adherence to\nspatial conditions compared to static, activation-based methods. TC-LoRA\nestablishes an alternative approach in which the model's conditioning strategy\nis modified through a deeper functional adaptation of its weights, allowing\ncontrol to align with the dynamic demands of the task and generative stage.",
            "headline_zh": "提出TC-LoRA以在扩散模型中实现动态条件控制",
            "intro_zh": [
                "当前可控扩散模型使用静态条件策略，限制生成过程适应性",
                "TC-LoRA通过超网络动态生成LoRA适配器，基于时间和条件调整权重",
                "实验显示该方法提升生成保真度和空间条件遵循能力"
            ],
            "tags_zh": [
                "可控扩散模型",
                "动态权重调整",
                "LoRA适配器",
                "超网络",
                "生成保真度"
            ],
            "_index": 9
        },
        {
            "title": "Guiding Energy-Efficient Locomotion through Impact Mitigation Rewards",
            "authors": [
                "Chenghao Wang",
                "Arjun Viswanathan",
                "Eric Sihite",
                "Alireza Ramezani"
            ],
            "arxiv_id": "2510.09543v1",
            "summary": "Animals achieve energy-efficient locomotion by their implicit passive\ndynamics, a marvel that has captivated roboticists for decades.Recently,\nmethods incorporated Adversarial Motion Prior (AMP) and Reinforcement learning\n(RL) shows promising progress to replicate Animals' naturalistic motion.\nHowever, such imitation learning approaches predominantly capture explicit\nkinematic patterns, so-called gaits, while overlooking the implicit passive\ndynamics. This work bridges this gap by incorporating a reward term guided by\nImpact Mitigation Factor (IMF), a physics-informed metric that quantifies a\nrobot's ability to passively mitigate impacts. By integrating IMF with AMP, our\napproach enables RL policies to learn both explicit motion trajectories from\nanimal reference motion and the implicit passive dynamic. We demonstrate energy\nefficiency improvements of up to 32%, as measured by the Cost of Transport\n(CoT), across both AMP and handcrafted reward structure.",
            "headline_zh": "提出基于冲击缓解因子的奖励项，以提升机器人能量效率。",
            "intro_zh": [
                "模仿学习忽略被动动态，导致机器人运动能量效率不足。",
                "结合冲击缓解因子与对抗运动先验，学习显式轨迹与隐式动态。",
                "实验显示成本运输降低达32%，验证能量效率提升。"
            ],
            "tags_zh": [
                "能量效率优化",
                "强化学习",
                "冲击缓解因子",
                "被动动态",
                "机器人运动控制"
            ],
            "_index": 10
        },
        {
            "title": "FLOWING: Implicit Neural Flows for Structure-Preserving Morphing",
            "authors": [
                "Arthur Bizzi",
                "Matias Grynberg",
                "Vitor Matias",
                "Daniel Perazzo",
                "João Paulo Lima",
                "Luiz Velho",
                "Nuno Gonçalves",
                "João Pereira",
                "Guilherme Schardong",
                "Tiago Novello"
            ],
            "arxiv_id": "2510.09537v1",
            "summary": "Morphing is a long-standing problem in vision and computer graphics,\nrequiring a time-dependent warping for feature alignment and a blending for\nsmooth interpolation. Recently, multilayer perceptrons (MLPs) have been\nexplored as implicit neural representations (INRs) for modeling such\ndeformations, due to their meshlessness and differentiability; however,\nextracting coherent and accurate morphings from standard MLPs typically relies\non costly regularizations, which often lead to unstable training and prevent\neffective feature alignment. To overcome these limitations, we propose FLOWING\n(FLOW morphING), a framework that recasts warping as the construction of a\ndifferential vector flow, naturally ensuring continuity, invertibility, and\ntemporal coherence by encoding structural flow properties directly into the\nnetwork architectures. This flow-centric approach yields principled and stable\ntransformations, enabling accurate and structure-preserving morphing of both 2D\nimages and 3D shapes. Extensive experiments across a range of applications -\nincluding face and image morphing, as well as Gaussian Splatting morphing -\nshow that FLOWING achieves state-of-the-art morphing quality with faster\nconvergence. Code and pretrained models are available at\nhttp://schardong.github.io/flowing.",
            "headline_zh": "提出FLOWING框架，通过微分向量流实现结构保持的2D图像和3D形状变形",
            "intro_zh": [
                "核心问题：传统多层感知机变形方法依赖昂贵正则化，导致训练不稳定和特征对齐困难",
                "方法要点：将变形建模为微分向量流，确保连续性、可逆性和时间一致性",
                "实验或效果：在面部和图像变形等应用中，实现最先进质量并加速收敛"
            ],
            "tags_zh": [
                "隐式神经表示",
                "图像变形",
                "3D形状变形",
                "微分向量流",
                "结构保持变形"
            ],
            "_index": 11
        },
        {
            "title": "PRNet: Original Information Is All You Have",
            "authors": [
                "PeiHuang Zheng",
                "Yunlong Zhao",
                "Zheng Cui",
                "Yang Li"
            ],
            "arxiv_id": "2510.09531v1",
            "summary": "Small object detection in aerial images suffers from severe information\ndegradation during feature extraction due to limited pixel representations,\nwhere shallow spatial details fail to align effectively with semantic\ninformation, leading to frequent misses and false positives. Existing FPN-based\nmethods attempt to mitigate these losses through post-processing enhancements,\nbut the reconstructed details often deviate from the original image\ninformation, impeding their fusion with semantic content. To address this\nlimitation, we propose PRNet, a real-time detection framework that prioritizes\nthe preservation and efficient utilization of primitive shallow spatial\nfeatures to enhance small object representations. PRNet achieves this via two\nmodules:the Progressive Refinement Neck (PRN) for spatial-semantic alignment\nthrough backbone reuse and iterative refinement, and the Enhanced SliceSamp\n(ESSamp) for preserving shallow information during downsampling via optimized\nrearrangement and convolution. Extensive experiments on the VisDrone, AI-TOD,\nand UAVDT datasets demonstrate that PRNet outperforms state-of-the-art methods\nunder comparable computational constraints, achieving superior\naccuracy-efficiency trade-offs.",
            "headline_zh": "提出PRNet以解决航拍图像小目标检测中的信息退化问题",
            "intro_zh": [
                "核心问题：航拍图像小目标检测中，浅层空间细节与语义信息对齐困难，导致漏检和误检",
                "方法要点：通过渐进细化颈部和增强切片采样模块，保留并高效利用原始浅层特征",
                "实验或效果：在VisDrone等数据集上，PRNet在计算约束下优于现有方法，实现高精度与效率平衡"
            ],
            "tags_zh": [
                "小目标检测",
                "航拍图像",
                "特征保留",
                "实时检测",
                "空间语义对齐",
                "渐进细化"
            ],
            "_index": 12
        },
        {
            "title": "Dynamic Quadrupedal Legged and Aerial Locomotion via Structure Repurposing",
            "authors": [
                "Chenghao Wang",
                "Kaushik Venkatesh Krishnamurthy",
                "Shreyansh Pitroda",
                "Adarsh Salagame",
                "Ioannis Mandralis",
                "Eric Sihite",
                "Alireza Ramezani",
                "Morteza Gharib"
            ],
            "arxiv_id": "2510.09526v1",
            "summary": "Multi-modal ground-aerial robots have been extensively studied, with a\nsignificant challenge lying in the integration of conflicting requirements\nacross different modes of operation. The Husky robot family, developed at\nNortheastern University, and specifically the Husky v.2 discussed in this\nstudy, addresses this challenge by incorporating posture manipulation and\nthrust vectoring into multi-modal locomotion through structure repurposing.\nThis quadrupedal robot features leg structures that can be repurposed for\ndynamic legged locomotion and flight. In this paper, we present the hardware\ndesign of the robot and report primary results on dynamic quadrupedal legged\nlocomotion and hovering.",
            "headline_zh": "提出结构复用方法实现四足机器人的动态腿式与空中运动",
            "intro_zh": [
                "核心问题：多模态地面-空中机器人面临不同模式需求冲突的集成挑战",
                "方法要点：通过姿态操纵和推力矢量实现结构复用，将腿部结构用于腿式运动和飞行",
                "实验或效果：报告了动态四足腿式运动和悬停的主要结果"
            ],
            "tags_zh": [
                "多模态机器人",
                "结构复用",
                "四足机器人",
                "动态运动",
                "姿态操纵",
                "推力矢量"
            ],
            "_index": 13
        },
        {
            "title": "Toggling stiffness via multistability",
            "authors": [
                "Hugo de Souza Oliveira",
                "Michele Curatolo",
                "Renate Sachse",
                "Edoardo Milana"
            ],
            "arxiv_id": "2510.09511v1",
            "summary": "Mechanical metamaterials enable unconventional and programmable mechanical\nresponses through structural design rather than material composition. In this\nwork, we introduce a multistable mechanical metamaterial that exhibits a\ntoggleable stiffness effect, where the effective shear stiffness switches\ndiscretely between stable configurations. The mechanical analysis of surrogate\nbeam models of the unit cell reveal that this behavior originates from the\nrotation transmitted by the support beams to the curved beam, which governs the\nbalance between bending and axial deformation. The stiffness ratio between the\ntwo states of the unit cell can be tuned by varying the slenderness of the\nsupport beams or by incorporating localized hinges that modulate rotational\ntransfer. Experiments on 3D-printed prototypes validate the numerical\npredictions, confirming consistent stiffness toggling across different\ngeometries. Finally, we demonstrate a monolithic soft clutch that leverages\nthis effect to achieve programmable, stepwise stiffness modulation. This work\nestablishes a design strategy for toggleable stiffness using multistable\nmetamaterials, paving the way for adaptive, lightweight, and autonomous systems\nin soft robotics and smart structures.",
            "headline_zh": "提出多稳态机械超材料实现可切换刚度，用于软机器人和智能结构。",
            "intro_zh": [
                "核心问题：机械超材料需实现可编程刚度切换，以提升自适应系统性能。",
                "方法要点：通过单元结构设计，利用支撑梁旋转控制弯曲与轴向变形平衡。",
                "实验或效果：3D打印原型验证数值预测，并展示单片软离合器实现步进刚度调制。"
            ],
            "tags_zh": [
                "机械超材料",
                "多稳态结构",
                "可切换刚度",
                "软机器人",
                "智能结构",
                "3D打印原型"
            ],
            "_index": 14
        },
        {
            "title": "Diagonal Artifacts in Samsung Images: PRNU Challenges and Solutions",
            "authors": [
                "David Vázquez-Padín",
                "Fernando Pérez-González",
                "Alejandro Martín-Del-Río"
            ],
            "arxiv_id": "2510.09509v1",
            "summary": "We investigate diagonal artifacts present in images captured by several\nSamsung smartphones and their impact on PRNU-based camera source verification.\nWe first show that certain Galaxy S series models share a common pattern\ncausing fingerprint collisions, with a similar issue also found in some Galaxy\nA models. Next, we demonstrate that reliable PRNU verification remains feasible\nfor devices supporting PRO mode with raw capture, since raw images bypass the\nprocessing pipeline that introduces artifacts. This option, however, is not\navailable for the mid-range A series models or in forensic cases without access\nto raw images. Finally, we outline potential forensic applications of the\ndiagonal artifacts, such as reducing misdetections in HDR images and localizing\nregions affected by synthetic bokeh in portrait-mode images.",
            "headline_zh": "分析三星图像对角线伪影对PRNU验证的影响并提出解决方案",
            "intro_zh": [
                "三星Galaxy S和A系列图像存在对角线伪影，导致PRNU指纹碰撞问题",
                "使用PRO模式原始图像可避免伪影，但中端A系列和法医场景无法应用",
                "伪影可用于减少HDR误检和定位肖像模式合成虚化区域"
            ],
            "tags_zh": [
                "图像伪影分析",
                "PRNU相机验证",
                "原始图像处理",
                "法医图像应用",
                "智能手机成像"
            ],
            "_index": 15
        },
        {
            "title": "PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs",
            "authors": [
                "Zixin Zhang",
                "Kanghao Chen",
                "Xingwang Lin",
                "Lutao Jiang",
                "Xu Zheng",
                "Yuanhuiyi Lyu",
                "Litao Guo",
                "Yinchuan Li",
                "Ying-Cong Chen"
            ],
            "arxiv_id": "2510.09507v1",
            "summary": "The ability to use, understand, and create tools is a hallmark of human\nintelligence, enabling sophisticated interaction with the physical world. For\nany general-purpose intelligent agent to achieve true versatility, it must also\nmaster these fundamental skills. While modern Multimodal Large Language Models\n(MLLMs) leverage their extensive common knowledge for high-level planning in\nembodied AI and in downstream Vision-Language-Action (VLA) models, the extent\nof their true understanding of physical tools remains unquantified. To bridge\nthis gap, we present PhysToolBench, the first benchmark dedicated to evaluating\nthe comprehension of physical tools by MLLMs. Our benchmark is structured as a\nVisual Question Answering (VQA) dataset comprising over 1,000 image-text pairs.\nIt assesses capabilities across three distinct difficulty levels: (1) Tool\nRecognition: Requiring the recognition of a tool's primary function. (2) Tool\nUnderstanding: Testing the ability to grasp the underlying principles of a\ntool's operation. (3) Tool Creation: Challenging the model to fashion a new\ntool from surrounding objects when conventional options are unavailable. Our\ncomprehensive evaluation of 32 MLLMs-spanning proprietary, open-source,\nspecialized embodied, and backbones in VLAs-reveals a significant deficiency in\ntool understanding. Furthermore, we provide an in-depth analysis and propose\npreliminary solutions. Code and dataset are publicly available.",
            "headline_zh": "提出PhysToolBench基准以评估多模态大语言模型对物理工具的理解能力",
            "intro_zh": [
                "核心问题：多模态大语言模型对物理工具的理解能力尚未量化，影响其在具身AI中的应用。",
                "方法要点：构建视觉问答数据集，包含1000+图像-文本对，评估工具识别、理解和创造能力。",
                "实验或效果：评估32个模型，发现工具理解存在显著缺陷，并提出初步解决方案。"
            ],
            "tags_zh": [
                "物理工具理解",
                "多模态大语言模型",
                "视觉问答基准",
                "具身AI",
                "工具创造评估"
            ],
            "_index": 16
        },
        {
            "title": "A methodology for clinically driven interactive segmentation evaluation",
            "authors": [
                "Parhom Esmaeili",
                "Virginia Fernandez",
                "Pedro Borges",
                "Eli Gibson",
                "Sebastien Ourselin",
                "M. Jorge Cardoso"
            ],
            "arxiv_id": "2510.09499v1",
            "summary": "Interactive segmentation is a promising strategy for building robust,\ngeneralisable algorithms for volumetric medical image segmentation. However,\ninconsistent and clinically unrealistic evaluation hinders fair comparison and\nmisrepresents real-world performance. We propose a clinically grounded\nmethodology for defining evaluation tasks and metrics, and built a software\nframework for constructing standardised evaluation pipelines. We evaluate\nstate-of-the-art algorithms across heterogeneous and complex tasks and observe\nthat (i) minimising information loss when processing user interactions is\ncritical for model robustness, (ii) adaptive-zooming mechanisms boost\nrobustness and speed convergence, (iii) performance drops if validation\nprompting behaviour/budgets differ from training, (iv) 2D methods perform well\nwith slab-like images and coarse targets, but 3D context helps with large or\nirregularly shaped targets, (v) performance of non-medical-domain models (e.g.\nSAM2) degrades with poor contrast and complex shapes.",
            "headline_zh": "提出临床驱动的交互式分割评估方法以解决医学图像分割评估不一致问题",
            "intro_zh": [
                "核心问题：交互式分割评估不一致且临床不现实，阻碍公平比较和真实性能评估",
                "方法要点：定义临床基础评估任务与指标，构建标准化评估软件框架",
                "实验或效果：评估先进算法，发现交互处理信息损失最小化、自适应缩放等关键因素"
            ],
            "tags_zh": [
                "交互式分割",
                "医学图像评估",
                "临床驱动方法",
                "标准化框架",
                "模型鲁棒性"
            ],
            "_index": 17
        },
        {
            "title": "Autonomous Soft Robotic Guidewire Navigation via Imitation Learning",
            "authors": [
                "Noah Barnes",
                "Ji Woong Kim",
                "Lingyun Di",
                "Hannah Qu",
                "Anuruddha Bhattacharjee",
                "Miroslaw Janowski",
                "Dheeraj Gandhi",
                "Bailey Felix",
                "Shaopeng Jiang",
                "Olivia Young",
                "Mark Fuge",
                "Ryan D. Sochol",
                "Jeremy D. Brown",
                "Axel Krieger"
            ],
            "arxiv_id": "2510.09497v1",
            "summary": "In endovascular surgery, endovascular interventionists push a thin tube\ncalled a catheter, guided by a thin wire to a treatment site inside the\npatient's blood vessels to treat various conditions such as blood clots,\naneurysms, and malformations. Guidewires with robotic tips can enhance\nmaneuverability, but they present challenges in modeling and control.\nAutomation of soft robotic guidewire navigation has the potential to overcome\nthese challenges, increasing the precision and safety of endovascular\nnavigation. In other surgical domains, end-to-end imitation learning has shown\npromising results. Thus, we develop a transformer-based imitation learning\nframework with goal conditioning, relative action outputs, and automatic\ncontrast dye injections to enable generalizable soft robotic guidewire\nnavigation in an aneurysm targeting task. We train the model on 36 different\nmodular bifurcated geometries, generating 647 total demonstrations under\nsimulated fluoroscopy, and evaluate it on three previously unseen vascular\ngeometries. The model can autonomously drive the tip of the robot to the\naneurysm location with a success rate of 83% on the unseen geometries,\noutperforming several baselines. In addition, we present ablation and baseline\nstudies to evaluate the effectiveness of each design and data collection\nchoice. Project website: https://softrobotnavigation.github.io/",
            "headline_zh": "提出基于Transformer的模仿学习框架，实现软体机器人导丝在血管瘤导航中的自主控制。",
            "intro_zh": [
                "核心问题：软体机器人导丝在血管内导航中建模与控制困难，影响精度与安全。",
                "方法要点：采用目标条件、相对动作输出和自动造影剂注射的Transformer模仿学习框架。",
                "实验或效果：在未见血管几何上，模型导航成功率83%，优于多个基线方法。"
            ],
            "tags_zh": [
                "软体机器人导航",
                "模仿学习",
                "Transformer模型",
                "血管内手术",
                "自主控制"
            ],
            "_index": 18
        },
        {
            "title": "FOGMACHINE -- Leveraging Discrete-Event Simulation and Scene Graphs for Modeling Hierarchical, Interconnected Environments under Partial Observations from Mobile Agents",
            "authors": [
                "Lars Ohnemus",
                "Nils Hantke",
                "Max Weißer",
                "Kai Furmans"
            ],
            "arxiv_id": "2510.09483v1",
            "summary": "Dynamic Scene Graphs (DSGs) provide a structured representation of\nhierarchical, interconnected environments, but current approaches struggle to\ncapture stochastic dynamics, partial observability, and multi-agent activity.\nThese aspects are critical for embodied AI, where agents must act under\nuncertainty and delayed perception. We introduce FOGMACHINE , an open-source\nframework that fuses DSGs with discrete-event simulation to model object\ndynamics, agent observations, and interactions at scale. This setup enables the\nstudy of uncertainty propagation, planning under limited perception, and\nemergent multi-agent behavior. Experiments in urban scenarios illustrate\nrealistic temporal and spatial patterns while revealing the challenges of\nbelief estimation under sparse observations. By combining structured\nrepresentations with efficient simulation, FOGMACHINE establishes an effective\ntool for benchmarking, model training, and advancing embodied AI in complex,\nuncertain environments.",
            "headline_zh": "提出FOGMACHINE框架，融合动态场景图与离散事件仿真，以建模部分观测下的复杂环境",
            "intro_zh": [
                "核心问题：动态场景图方法难以处理随机动态、部分可观测性和多智能体活动",
                "方法要点：结合动态场景图与离散事件仿真，模拟对象动态、智能体观测和交互",
                "实验或效果：在城市场景中验证了时空模式，揭示了稀疏观测下信念估计的挑战"
            ],
            "tags_zh": [
                "动态场景图",
                "离散事件仿真",
                "部分可观测性",
                "多智能体系统",
                "具身人工智能",
                "不确定性建模"
            ],
            "_index": 19
        },
        {
            "title": "Few-shot multi-token DreamBooth with LoRa for style-consistent character generation",
            "authors": [
                "Ruben Pascual",
                "Mikel Sesma-Sara",
                "Aranzazu Jurio",
                "Daniel Paternain",
                "Mikel Galar"
            ],
            "arxiv_id": "2510.09475v1",
            "summary": "The audiovisual industry is undergoing a profound transformation as it is\nintegrating AI developments not only to automate routine tasks but also to\ninspire new forms of art. This paper addresses the problem of producing a\nvirtually unlimited number of novel characters that preserve the artistic style\nand shared visual traits of a small set of human-designed reference characters,\nthus broadening creative possibilities in animation, gaming, and related\ndomains. Our solution builds upon DreamBooth, a well-established fine-tuning\ntechnique for text-to-image diffusion models, and adapts it to tackle two core\nchallenges: capturing intricate character details beyond textual prompts and\nthe few-shot nature of the training data. To achieve this, we propose a\nmulti-token strategy, using clustering to assign separate tokens to individual\ncharacters and their collective style, combined with LoRA-based\nparameter-efficient fine-tuning. By removing the class-specific regularization\nset and introducing random tokens and embeddings during generation, our\napproach allows for unlimited character creation while preserving the learned\nstyle. We evaluate our method on five small specialized datasets, comparing it\nto relevant baselines using both quantitative metrics and a human evaluation\nstudy. Our results demonstrate that our approach produces high-quality, diverse\ncharacters while preserving the distinctive aesthetic features of the reference\ncharacters, with human evaluation further reinforcing its effectiveness and\nhighlighting the potential of our method.",
            "headline_zh": "提出多令牌DreamBooth与LoRA方法，解决少样本风格一致角色生成问题",
            "intro_zh": [
                "核心问题：从少量参考角色生成无限新角色，需保持艺术风格和共享视觉特征。",
                "方法要点：结合多令牌策略和LoRA微调，提升细节捕捉和参数效率。",
                "实验效果：在五个数据集上评估，人类研究证实方法有效且生成高质量角色。"
            ],
            "tags_zh": [
                "少样本学习",
                "角色生成",
                "风格一致性",
                "DreamBooth",
                "LoRA微调",
                "多令牌策略"
            ],
            "_index": 20
        },
        {
            "title": "D-TPT: Dimensional Entropy Maximization for Calibrating Test-Time Prompt Tuning in Vision-Language Models",
            "authors": [
                "Jisu Han",
                "Wonjun Hwang"
            ],
            "arxiv_id": "2510.09473v1",
            "summary": "Test-time adaptation paradigm provides flexibility towards domain shifts by\nperforming immediate adaptation on unlabeled target data from the source model.\nVision-Language Models (VLMs) leverage their generalization capabilities for\ndiverse downstream tasks, and test-time prompt tuning has emerged as a\nprominent solution for adapting VLMs. In this work, we explore contrastive VLMs\nand identify the modality gap caused by a single dominant feature dimension\nacross modalities. We observe that the dominant dimensions in both text and\nimage modalities exhibit high predictive sensitivity, and that constraining\ntheir influence can improve calibration error. Building on this insight, we\npropose dimensional entropy maximization that regularizes the distribution of\ntextual features toward uniformity to mitigate the dependency of dominant\ndimensions. Our method alleviates the degradation of calibration performance in\ntest-time prompt tuning, offering a simple yet effective solution to enhance\nthe reliability of VLMs in real-world deployment scenarios.",
            "headline_zh": "提出维度熵最大化方法以改善视觉语言模型测试时提示调优的校准性能",
            "intro_zh": [
                "核心问题：模态间隙由单一主导特征维度引起，导致校准误差增加",
                "方法要点：通过最大化维度熵正则化文本特征分布，减少主导维度依赖",
                "实验或效果：缓解测试时提示调优中校准性能退化，提升模型可靠性"
            ],
            "tags_zh": [
                "视觉语言模型",
                "测试时适应",
                "提示调优",
                "维度熵最大化",
                "校准误差",
                "模态间隙"
            ],
            "_index": 21
        },
        {
            "title": "Scalable Multi-Agent Path Finding using Collision-Aware Dynamic Alert Mask and a Hybrid Execution Strategy",
            "authors": [
                "Bharath Muppasani",
                "Ritirupa Dey",
                "Biplav Srivastava",
                "Vignesh Narayanan"
            ],
            "arxiv_id": "2510.09469v1",
            "summary": "Multi-agent pathfinding (MAPF) remains a critical problem in robotics and\nautonomous systems, where agents must navigate shared spaces efficiently while\navoiding conflicts. Traditional centralized algorithms that have global\ninformation, such as Conflict-Based Search (CBS), provide high-quality\nsolutions but become computationally expensive in large-scale scenarios due to\nthe combinatorial explosion of conflicts that need resolution. Conversely,\ndistributed approaches that have local information, particularly learning-based\nmethods, offer better scalability by operating with relaxed information\navailability, yet often at the cost of solution quality. To address these\nlimitations, we propose a hybrid framework that combines decentralized path\nplanning with a lightweight centralized coordinator. Our framework leverages\nreinforcement learning (RL) for decentralized planning, enabling agents to\nadapt their planning based on minimal, targeted alerts--such as static\nconflict-cell flags or brief conflict tracks--that are dynamically shared\ninformation from the central coordinator for effective conflict resolution. We\nempirically study the effect of the information available to an agent on its\nplanning performance. Our approach reduces the inter-agent information sharing\ncompared to fully centralized and distributed methods, while still consistently\nfinding feasible, collision-free solutions--even in large-scale scenarios\nhaving higher agent counts.",
            "headline_zh": "提出混合框架结合去中心化路径规划与轻量级中央协调器，以解决大规模多智能体路径规划问题。",
            "intro_zh": [
                "多智能体路径规划在机器人中需高效导航并避免冲突，传统方法计算昂贵，分布式方法牺牲质量。",
                "方法结合强化学习去中心化规划与动态警报共享，减少信息交换，确保无碰撞解决方案。",
                "实验验证在大规模场景中可行，减少冲突并保持高可扩展性。"
            ],
            "tags_zh": [
                "多智能体路径规划",
                "强化学习",
                "混合框架",
                "冲突解决",
                "可扩展性"
            ],
            "_index": 22
        },
        {
            "title": "Failure Prediction at Runtime for Generative Robot Policies",
            "authors": [
                "Ralf Römer",
                "Adrian Kobras",
                "Luca Worbis",
                "Angela P. Schoellig"
            ],
            "arxiv_id": "2510.09459v1",
            "summary": "Imitation learning (IL) with generative models, such as diffusion and flow\nmatching, has enabled robots to perform complex, long-horizon tasks. However,\ndistribution shifts from unseen environments or compounding action errors can\nstill cause unpredictable and unsafe behavior, leading to task failure. Early\nfailure prediction during runtime is therefore essential for deploying robots\nin human-centered and safety-critical environments. We propose FIPER, a general\nframework for Failure Prediction at Runtime for generative IL policies that\ndoes not require failure data. FIPER identifies two key indicators of impending\nfailure: (i) out-of-distribution (OOD) observations detected via random network\ndistillation in the policy's embedding space, and (ii) high uncertainty in\ngenerated actions measured by a novel action-chunk entropy score. Both failure\nprediction scores are calibrated using a small set of successful rollouts via\nconformal prediction. A failure alarm is triggered when both indicators,\naggregated over short time windows, exceed their thresholds. We evaluate FIPER\nacross five simulation and real-world environments involving diverse failure\nmodes. Our results demonstrate that FIPER better distinguishes actual failures\nfrom benign OOD situations and predicts failures more accurately and earlier\nthan existing methods. We thus consider this work an important step towards\nmore interpretable and safer generative robot policies. Code, data and videos\nare available at https://tum-lsy.github.io/fiper_website.",
            "headline_zh": "提出FIPER框架以预测生成式机器人策略的运行时失败",
            "intro_zh": [
                "核心问题：生成式模仿学习在未知环境或动作误差下易导致任务失败，需早期预测",
                "方法要点：通过OOD检测和动作块熵评估失败指标，并利用保形预测校准阈值",
                "实验或效果：在多种仿真和真实环境中，FIPER比现有方法更准确、更早预测失败"
            ],
            "tags_zh": [
                "失败预测",
                "生成式模仿学习",
                "保形预测",
                "动作不确定性",
                "机器人安全"
            ],
            "_index": 23
        },
        {
            "title": "SilvaScenes: Tree Segmentation and Species Classification from Under-Canopy Images in Natural Forests",
            "authors": [
                "David-Alexandre Duclos",
                "William Guimont-Martin",
                "Gabriel Jeanson",
                "Arthur Larochelle-Tremblay",
                "Théo Defosse",
                "Frédéric Moore",
                "Philippe Nolet",
                "François Pomerleau",
                "Philippe Giguère"
            ],
            "arxiv_id": "2510.09458v1",
            "summary": "Interest in robotics for forest management is growing, but perception in\ncomplex, natural environments remains a significant hurdle. Conditions such as\nheavy occlusion, variable lighting, and dense vegetation pose challenges to\nautomated systems, which are essential for precision forestry, biodiversity\nmonitoring, and the automation of forestry equipment. These tasks rely on\nadvanced perceptual capabilities, such as detection and fine-grained species\nclassification of individual trees. Yet, existing datasets are inadequate to\ndevelop such perception systems, as they often focus on urban settings or a\nlimited number of species. To address this, we present SilvaScenes, a new\ndataset for instance segmentation of tree species from under-canopy images.\nCollected across five bioclimatic domains in Quebec, Canada, SilvaScenes\nfeatures 1476 trees from 24 species with annotations from forestry experts. We\ndemonstrate the relevance and challenging nature of our dataset by benchmarking\nmodern deep learning approaches for instance segmentation. Our results show\nthat, while tree segmentation is easy, with a top mean average precision (mAP)\nof 67.65%, species classification remains a significant challenge with an mAP\nof only 35.69%. Our dataset and source code will be available at\nhttps://github.com/norlab-ulaval/SilvaScenes.",
            "headline_zh": "提出SilvaScenes数据集以解决自然林冠下树木分割与物种分类的感知挑战",
            "intro_zh": [
                "核心问题：自然森林中遮挡、光照变化和植被密集导致机器人感知困难，现有数据集不足",
                "方法要点：收集加拿大魁北克五生物气候域图像，标注24种1476棵树，用于实例分割",
                "实验或效果：树分割mAP达67.65%，物种分类mAP仅35.69%，显示分类挑战大"
            ],
            "tags_zh": [
                "实例分割",
                "树木物种分类",
                "森林机器人感知",
                "数据集构建",
                "深度学习基准测试"
            ],
            "_index": 24
        },
        {
            "title": "Dynamic Weight-based Temporal Aggregation for Low-light Video Enhancement",
            "authors": [
                "Ruirui Lin",
                "Guoxi Huang",
                "Nantheera Anantrasirichai"
            ],
            "arxiv_id": "2510.09450v1",
            "summary": "Low-light video enhancement (LLVE) is challenging due to noise, low contrast,\nand color degradations. Learning-based approaches offer fast inference but\nstill struggle with heavy noise in real low-light scenes, primarily due to\nlimitations in effectively leveraging temporal information. In this paper, we\naddress this issue with DWTA-Net, a novel two-stage framework that jointly\nexploits short- and long-term temporal cues. Stage I employs Visual State-Space\nblocks for multi-frame alignment, recovering brightness, color, and structure\nwith local consistency. Stage II introduces a recurrent refinement module with\ndynamic weight-based temporal aggregation guided by optical flow, adaptively\nbalancing static and dynamic regions. A texture-adaptive loss further preserves\nfine details while promoting smoothness in flat areas. Experiments on\nreal-world low-light videos show that DWTA-Net effectively suppresses noise and\nartifacts, delivering superior visual quality compared with state-of-the-art\nmethods.",
            "headline_zh": "提出DWTA-Net框架以解决低光视频增强中的噪声和伪影问题",
            "intro_zh": [
                "核心问题：低光视频存在噪声、低对比度和颜色退化，现有方法难以有效利用时序信息。",
                "方法要点：采用两阶段框架，结合短长期时序线索，通过动态权重聚合和光流引导优化。",
                "实验或效果：在真实低光视频上验证，有效抑制噪声和伪影，视觉质量优于先进方法。"
            ],
            "tags_zh": [
                "低光视频增强",
                "时序聚合",
                "动态权重",
                "光流引导",
                "纹理自适应损失"
            ],
            "_index": 25
        },
        {
            "title": "Mono4DEditor: Text-Driven 4D Scene Editing from Monocular Video via Point-Level Localization of Language-Embedded Gaussians",
            "authors": [
                "Jin-Chuan Shi",
                "Chengye Su",
                "Jiajun Wang",
                "Ariel Shamir",
                "Miao Wang"
            ],
            "arxiv_id": "2510.09438v1",
            "summary": "Editing 4D scenes reconstructed from monocular videos based on text prompts\nis a valuable yet challenging task with broad applications in content creation\nand virtual environments. The key difficulty lies in achieving semantically\nprecise edits in localized regions of complex, dynamic scenes, while preserving\nthe integrity of unedited content. To address this, we introduce Mono4DEditor,\na novel framework for flexible and accurate text-driven 4D scene editing. Our\nmethod augments 3D Gaussians with quantized CLIP features to form a\nlanguage-embedded dynamic representation, enabling efficient semantic querying\nof arbitrary spatial regions. We further propose a two-stage point-level\nlocalization strategy that first selects candidate Gaussians via CLIP\nsimilarity and then refines their spatial extent to improve accuracy. Finally,\ntargeted edits are performed on localized regions using a diffusion-based video\nediting model, with flow and scribble guidance ensuring spatial fidelity and\ntemporal coherence. Extensive experiments demonstrate that Mono4DEditor enables\nhigh-quality, text-driven edits across diverse scenes and object types, while\npreserving the appearance and geometry of unedited areas and surpassing prior\napproaches in both flexibility and visual fidelity.",
            "headline_zh": "提出Mono4DEditor框架，通过语言嵌入高斯实现单目视频4D场景的文本驱动编辑",
            "intro_zh": [
                "核心问题：单目视频4D场景编辑中，实现语义精确的局部编辑并保持未编辑内容完整性。",
                "方法要点：使用量化CLIP特征增强3D高斯，结合点级定位策略和扩散模型进行编辑。",
                "实验或效果：在多样场景中实现高质量编辑，超越现有方法，保持时空一致性。"
            ],
            "tags_zh": [
                "4D场景编辑",
                "文本驱动编辑",
                "3D高斯表示",
                "CLIP特征嵌入",
                "扩散模型编辑",
                "时空一致性"
            ],
            "_index": 26
        },
        {
            "title": "Bridging Research and Practice in Simulation-based Testing of Industrial Robot Navigation Systems",
            "authors": [
                "Sajad Khatiri",
                "Francisco Eli Vina Barrientos",
                "Maximilian Wulf",
                "Paolo Tonella",
                "Sebastiano Panichella"
            ],
            "arxiv_id": "2510.09396v1",
            "summary": "Ensuring robust robotic navigation in dynamic environments is a key\nchallenge, as traditional testing methods often struggle to cover the full\nspectrum of operational requirements. This paper presents the industrial\nadoption of Surrealist, a simulation-based test generation framework originally\nfor UAVs, now applied to the ANYmal quadrupedal robot for industrial\ninspection. Our method uses a search-based algorithm to automatically generate\nchallenging obstacle avoidance scenarios, uncovering failures often missed by\nmanual testing. In a pilot phase, generated test suites revealed critical\nweaknesses in one experimental algorithm (40.3% success rate) and served as an\neffective benchmark to prove the superior robustness of another (71.2% success\nrate). The framework was then integrated into the ANYbotics workflow for a\nsix-month industrial evaluation, where it was used to test five proprietary\nalgorithms. A formal survey confirmed its value, showing it enhances the\ndevelopment process, uncovers critical failures, provides objective benchmarks,\nand strengthens the overall verification pipeline.",
            "headline_zh": "提出仿真测试框架Surrealist，应用于ANYmal四足机器人工业检测，以自动生成障碍规避场景。",
            "intro_zh": [
                "核心问题：动态环境中机器人导航鲁棒性不足，传统测试方法难以覆盖全操作需求。",
                "方法要点：基于搜索算法自动生成挑战性障碍规避测试场景，提升故障发现能力。",
                "实验或效果：在工业评估中测试五款专有算法，通过调查确认增强开发流程和验证。"
            ],
            "tags_zh": [
                "机器人导航测试",
                "仿真测试生成",
                "障碍规避",
                "工业机器人",
                "搜索算法",
                "ANYmal机器人"
            ],
            "_index": 27
        },
        {
            "title": "Identifying & Interactively Refining Ambiguous User Goals for Data Visualization Code Generation",
            "authors": [
                "Mert İnan",
                "Anthony Sicilia",
                "Alex Xie",
                "Saujas Vaduguru",
                "Daniel Fried",
                "Malihe Alikhani"
            ],
            "arxiv_id": "2510.09390v1",
            "summary": "Establishing shared goals is a fundamental step in human-AI communication.\nHowever, ambiguities can lead to outputs that seem correct but fail to reflect\nthe speaker's intent. In this paper, we explore this issue with a focus on the\ndata visualization domain, where ambiguities in natural language impact the\ngeneration of code that visualizes data. The availability of multiple views on\nthe contextual (e.g., the intended plot and the code rendering the plot) allows\nfor a unique and comprehensive analysis of diverse ambiguity types. We develop\na taxonomy of types of ambiguity that arise in this task and propose metrics to\nquantify them. Using Matplotlib problems from the DS-1000 dataset, we\ndemonstrate that our ambiguity metrics better correlate with human annotations\nthan uncertainty baselines. Our work also explores how multi-turn dialogue can\nreduce ambiguity, therefore, improve code accuracy by better matching user\ngoals. We evaluate three pragmatic models to inform our dialogue strategies:\nGricean Cooperativity, Discourse Representation Theory, and Questions under\nDiscussion. A simulated user study reveals how pragmatic dialogues reduce\nambiguity and enhance code accuracy, highlighting the value of multi-turn\nexchanges in code generation.",
            "headline_zh": "提出歧义分类与多轮对话方法，以提升数据可视化代码生成的准确性。",
            "intro_zh": [
                "核心问题：自然语言歧义导致数据可视化代码生成不准确，影响用户意图匹配。",
                "方法要点：构建歧义分类法，并基于语用模型设计多轮对话策略以澄清目标。",
                "实验或效果：在DS-1000数据集上验证歧义指标优于基线，模拟用户研究显示对话提升代码准确率。"
            ],
            "tags_zh": [
                "数据可视化",
                "代码生成",
                "自然语言歧义",
                "多轮对话",
                "语用模型",
                "歧义分类"
            ],
            "_index": 28
        },
        {
            "title": "Utilizing dynamic sparsity on pretrained DETR",
            "authors": [
                "Reza Sedghi",
                "Anand Subramoney",
                "David Kappel"
            ],
            "arxiv_id": "2510.09380v1",
            "summary": "Efficient inference with transformer-based models remains a challenge,\nespecially in vision tasks like object detection. We analyze the inherent\nsparsity in the MLP layers of DETR and introduce two methods to exploit it\nwithout retraining. First, we propose Static Indicator-Based Sparsification\n(SIBS), a heuristic method that predicts neuron inactivity based on fixed\nactivation patterns. While simple, SIBS offers limited gains due to the\ninput-dependent nature of sparsity. To address this, we introduce Micro-Gated\nSparsification (MGS), a lightweight gating mechanism trained on top of a\npretrained DETR. MGS predicts dynamic sparsity using a small linear layer and\nachieves up to 85 to 95% activation sparsity. Experiments on the COCO dataset\nshow that MGS maintains or even improves performance while significantly\nreducing computation. Our method offers a practical, input-adaptive approach to\nsparsification, enabling efficient deployment of pretrained vision transformers\nwithout full model retraining.",
            "headline_zh": "提出动态稀疏化方法以提升预训练DETR在目标检测中的推理效率",
            "intro_zh": [
                "核心问题：Transformer模型在视觉任务中推理效率低，DETR的MLP层存在固有稀疏性。",
                "方法要点：引入MGS轻量门控机制，预测动态稀疏性，无需重新训练整个模型。",
                "实验或效果：在COCO数据集上，MGS实现85-95%激活稀疏，保持或提升性能并减少计算。"
            ],
            "tags_zh": [
                "动态稀疏化",
                "DETR模型",
                "目标检测",
                "推理效率",
                "门控机制",
                "COCO数据集"
            ],
            "_index": 29
        },
        {
            "title": "Minkowski-MambaNet: A Point Cloud Framework with Selective State Space Models for Forest Biomass Quantification",
            "authors": [
                "Jinxiang Tu",
                "Dayong Ren",
                "Fei Shi",
                "Zhenhong Jia",
                "Yahong Ren",
                "Jiwei Qin",
                "Fang He"
            ],
            "arxiv_id": "2510.09367v1",
            "summary": "Accurate forest biomass quantification is vital for carbon cycle monitoring.\nWhile airborne LiDAR excels at capturing 3D forest structure, directly\nestimating woody volume and Aboveground Biomass (AGB) from point clouds is\nchallenging due to difficulties in modeling long-range dependencies needed to\ndistinguish trees.We propose Minkowski-MambaNet, a novel deep learning\nframework that directly estimates volume and AGB from raw LiDAR. Its key\ninnovation is integrating the Mamba model's Selective State Space Model (SSM)\ninto a Minkowski network, enabling effective encoding of global context and\nlong-range dependencies for improved tree differentiation. Skip connections are\nincorporated to enhance features and accelerate convergence.Evaluated on Danish\nNational Forest Inventory LiDAR data, Minkowski-MambaNet significantly\noutperforms state-of-the-art methods, providing more accurate and robust\nestimates. Crucially, it requires no Digital Terrain Model (DTM) and is robust\nto boundary artifacts. This work offers a powerful tool for large-scale forest\nbiomass analysis, advancing LiDAR-based forest inventories.",
            "headline_zh": "提出Minkowski-MambaNet框架，结合选择性状态空间模型，从LiDAR点云直接估计森林生物量",
            "intro_zh": [
                "核心问题：点云中长程依赖建模困难，影响树木区分和生物量估计准确性",
                "方法要点：集成Mamba的选择性状态空间模型到Minkowski网络，增强全局上下文编码",
                "实验或效果：在丹麦国家森林清单数据上，显著优于现有方法，无需DTM且鲁棒"
            ],
            "tags_zh": [
                "点云处理",
                "选择性状态空间模型",
                "森林生物量估计",
                "LiDAR数据分析",
                "深度学习框架"
            ],
            "_index": 30
        },
        {
            "title": "A Biophysically-Conditioned Generative Framework for 3D Brain Tumor MRI Synthesis",
            "authors": [
                "Valentin Biller",
                "Lucas Zimmer",
                "Can Erdur",
                "Sandeep Nagar",
                "Daniel Rückert",
                "Niklas Bubeck",
                "Jonas Weidner"
            ],
            "arxiv_id": "2510.09365v1",
            "summary": "Magnetic resonance imaging (MRI) inpainting supports numerous clinical and\nresearch applications. We introduce the first generative model that conditions\non voxel-level, continuous tumor concentrations to synthesize high-fidelity\nbrain tumor MRIs. For the BraTS 2025 Inpainting Challenge, we adapt this\narchitecture to the complementary task of healthy tissue restoration by setting\nthe tumor concentrations to zero. Our latent diffusion model conditioned on\nboth tissue segmentations and the tumor concentrations generates 3D spatially\ncoherent and anatomically consistent images for both tumor synthesis and\nhealthy tissue inpainting. For healthy inpainting, we achieve a PSNR of 18.5,\nand for tumor inpainting, we achieve 17.4. Our code is available at:\nhttps://github.com/valentin-biller/ldm.git",
            "headline_zh": "提出基于生物物理条件的生成模型以合成3D脑肿瘤MRI和修复健康组织",
            "intro_zh": [
                "核心问题：脑肿瘤MRI合成与健康组织修复需要高保真和空间一致性。",
                "方法要点：使用潜在扩散模型，条件输入组织分割和肿瘤浓度。",
                "实验或效果：健康修复PSNR 18.5，肿瘤合成PSNR 17.4，代码开源。"
            ],
            "tags_zh": [
                "脑肿瘤MRI合成",
                "潜在扩散模型",
                "生物物理条件",
                "图像修复",
                "3D医学影像"
            ],
            "_index": 31
        },
        {
            "title": "Visibility-Aware Densification for 3D Gaussian Splatting in Dynamic Urban Scenes",
            "authors": [
                "Yikang Zhang",
                "Rui Fan"
            ],
            "arxiv_id": "2510.09364v1",
            "summary": "3D Gaussian splatting (3DGS) has demonstrated impressive performance in\nsynthesizing high-fidelity novel views. Nonetheless, its effectiveness\ncritically depends on the quality of the initialized point cloud. Specifically,\nachieving uniform and complete point coverage over the underlying scene\nstructure requires overlapping observation frustums, an assumption that is\noften violated in unbounded, dynamic urban environments. Training Gaussian\nmodels with partially initialized point clouds often leads to distortions and\nartifacts, as camera rays may fail to intersect valid surfaces, resulting in\nincorrect gradient propagation to Gaussian primitives associated with occluded\nor invisible geometry. Additionally, existing densification strategies simply\nclone and split Gaussian primitives from existing ones, incapable of\nreconstructing missing structures. To address these limitations, we propose\nVAD-GS, a 3DGS framework tailored for geometry recovery in challenging urban\nscenes. Our method identifies unreliable geometry structures via voxel-based\nvisibility reasoning, selects informative supporting views through\ndiversity-aware view selection, and recovers missing structures via patch\nmatching-based multi-view stereo reconstruction. This design enables the\ngeneration of new Gaussian primitives guided by reliable geometric priors, even\nin regions lacking initial points. Extensive experiments on the Waymo and\nnuScenes datasets demonstrate that VAD-GS outperforms state-of-the-art 3DGS\napproaches and significantly improves the quality of reconstructed geometry for\nboth static and dynamic objects. Source code will be released upon publication.",
            "headline_zh": "提出VAD-GS框架以解决动态城市场景中3D高斯溅射的几何恢复问题",
            "intro_zh": [
                "核心问题：动态城市场景中初始点云覆盖不均导致3D高斯溅射产生失真和伪影",
                "方法要点：通过体素可见性推理、多样性视图选择和补丁匹配多视图立体重建恢复缺失结构",
                "实验或效果：在Waymo和nuScenes数据集上优于现有方法，提升静态和动态对象重建质量"
            ],
            "tags_zh": [
                "3D高斯溅射",
                "几何恢复",
                "动态城市场景",
                "多视图立体重建",
                "可见性推理"
            ],
            "_index": 32
        },
        {
            "title": "BLINK-Twice: You see, but do you observe? A Reasoning Benchmark on Visual Perception",
            "authors": [
                "Junyan Ye",
                "Dongzhi Jiang",
                "Jun He",
                "Baichuan Zhou",
                "Zilong Huang",
                "Zhiyuan Yan",
                "Hongsheng Li",
                "Conghui He",
                "Weijia Li"
            ],
            "arxiv_id": "2510.09361v1",
            "summary": "Recently, Multimodal Large Language Models (MLLMs) have made rapid progress,\nparticularly in enhancing their reasoning capabilities. However, existing\nreasoning benchmarks still primarily assess language-based reasoning, often\ntreating visual input as replaceable context. To address this gap, we introduce\nBLINK-Twice, a vision-centric reasoning benchmark grounded in challenging\nperceptual tasks. Instead of relying on external knowledge, our tasks require\nmodels to reason from visual content alone, shifting the focus from\nlanguage-based to image-grounded reasoning. Compared to prior perception\nbenchmarks, it moves beyond shallow perception (\"see\") and requires\nfine-grained observation and analytical reasoning (\"observe\"). BLINK-Twice\nintegrates three core components: seven types of visual challenges for testing\nvisual reasoning, natural adversarial image pairs that enforce reliance on\nvisual content, and annotated reasoning chains for fine-grained evaluation of\nthe reasoning process rather than final answers alone. We evaluate 20 leading\nMLLMs, including 12 foundation models and 8 reasoning-enhanced models.\nBLINK-Twice poses a significant challenge to current models. While existing\nreasoning strategies in the language space-such as chain-of-thought or\nself-criticism can improve performance, they often result in unstable and\nredundant reasoning. We observe that repeated image observation improves\nperformance across models, and active visual interaction, as demonstrated by\nmodels like o3, highlights the need for a new paradigm for vision reasoning.\nThe dataset is publicly available at https://github.com/PicoTrex/BLINK-Twice",
            "headline_zh": "提出BLINK-Twice基准以评估多模态大模型在视觉感知推理中的能力",
            "intro_zh": [
                "现有基准多关注语言推理，视觉输入常被忽略，导致视觉中心推理评估不足",
                "基准包含七类视觉挑战、自然对抗图像对和标注推理链，强调从视觉内容推理",
                "评估20个领先模型，发现现有推理策略不稳定，重复观察和主动交互可提升性能"
            ],
            "tags_zh": [
                "多模态大语言模型",
                "视觉推理基准",
                "感知任务",
                "对抗图像对",
                "推理链评估",
                "模型性能分析"
            ],
            "_index": 33
        },
        {
            "title": "Boosting Multi-modal Keyphrase Prediction with Dynamic Chain-of-Thought in Vision-Language Models",
            "authors": [
                "Qihang Ma",
                "Shengyu Li",
                "Jie Tang",
                "Dingkang Yang",
                "Shaodong Chen",
                "Yingyi Zhang",
                "Chao Feng",
                "Jiao Ran"
            ],
            "arxiv_id": "2510.09358v1",
            "summary": "Multi-modal keyphrase prediction (MMKP) aims to advance beyond text-only\nmethods by incorporating multiple modalities of input information to produce a\nset of conclusive phrases. Traditional multi-modal approaches have been proven\nto have significant limitations in handling the challenging absence and unseen\nscenarios. Additionally, we identify shortcomings in existing benchmarks that\noverestimate model capability due to significant overlap in training tests. In\nthis work, we propose leveraging vision-language models (VLMs) for the MMKP\ntask. Firstly, we use two widely-used strategies, e.g., zero-shot and\nsupervised fine-tuning (SFT) to assess the lower bound performance of VLMs.\nNext, to improve the complex reasoning capabilities of VLMs, we adopt\nFine-tune-CoT, which leverages high-quality CoT reasoning data generated by a\nteacher model to finetune smaller models. Finally, to address the\n\"overthinking\" phenomenon, we propose a dynamic CoT strategy which adaptively\ninjects CoT data during training, allowing the model to flexibly leverage its\nreasoning capabilities during the inference stage. We evaluate the proposed\nstrategies on various datasets and the experimental results demonstrate the\neffectiveness of the proposed approaches. The code is available at\nhttps://github.com/bytedance/DynamicCoT.",
            "headline_zh": "提出动态思维链策略以提升视觉语言模型在多模态关键词预测中的推理能力",
            "intro_zh": [
                "核心问题：多模态关键词预测在缺失和未见场景中表现不佳，且现有基准高估模型能力",
                "方法要点：采用零样本和微调评估基线，并引入动态思维链策略优化推理过程",
                "实验或效果：在多个数据集上验证了方法的有效性，代码已开源"
            ],
            "tags_zh": [
                "多模态关键词预测",
                "视觉语言模型",
                "思维链推理",
                "动态训练策略",
                "模型微调"
            ],
            "_index": 34
        },
        {
            "title": "Enhancing Infrared Vision: Progressive Prompt Fusion Network and Benchmark",
            "authors": [
                "Jinyuan Liu",
                "Zihang Chen",
                "Zhu Liu",
                "Zhiying Jiang",
                "Long Ma",
                "Xin Fan",
                "Risheng Liu"
            ],
            "arxiv_id": "2510.09343v1",
            "summary": "We engage in the relatively underexplored task named thermal infrared image\nenhancement. Existing infrared image enhancement methods primarily focus on\ntackling individual degradations, such as noise, contrast, and blurring, making\nit difficult to handle coupled degradations. Meanwhile, all-in-one enhancement\nmethods, commonly applied to RGB sensors, often demonstrate limited\neffectiveness due to the significant differences in imaging models. In sight of\nthis, we first revisit the imaging mechanism and introduce a Progressive Prompt\nFusion Network (PPFN). Specifically, the PPFN initially establishes prompt\npairs based on the thermal imaging process. For each type of degradation, we\nfuse the corresponding prompt pairs to modulate the model's features, providing\nadaptive guidance that enables the model to better address specific\ndegradations under single or multiple conditions. In addition, a Selective\nProgressive Training (SPT) mechanism is introduced to gradually refine the\nmodel's handling of composite cases to align the enhancement process, which not\nonly allows the model to remove camera noise and retain key structural details,\nbut also enhancing the overall contrast of the thermal image. Furthermore, we\nintroduce the most high-quality, multi-scenarios infrared benchmark covering a\nwide range of scenarios. Extensive experiments substantiate that our approach\nnot only delivers promising visual results under specific degradation but also\nsignificantly improves performance on complex degradation scenes, achieving a\nnotable 8.76\\% improvement. Code is available at\nhttps://github.com/Zihang-Chen/HM-TIR.",
            "headline_zh": "提出渐进提示融合网络以解决热红外图像耦合退化增强问题",
            "intro_zh": [
                "核心问题：现有方法难以处理热红外图像中的耦合退化，如噪声、对比度和模糊",
                "方法要点：基于热成像过程建立提示对，通过渐进融合调制特征，自适应处理单或多退化",
                "实验或效果：在复杂退化场景中性能提升8.76%，并引入高质量多场景红外基准数据集"
            ],
            "tags_zh": [
                "热红外图像增强",
                "渐进提示融合网络",
                "耦合退化处理",
                "选择性渐进训练",
                "红外基准数据集"
            ],
            "_index": 35
        },
        {
            "title": "Efficient Bayesian Inference from Noisy Pairwise Comparisons",
            "authors": [
                "Till Aczel",
                "Lucas Theis",
                "Wattenhofer Roger"
            ],
            "arxiv_id": "2510.09333v1",
            "summary": "Evaluating generative models is challenging because standard metrics often\nfail to reflect human preferences. Human evaluations are more reliable but\ncostly and noisy, as participants vary in expertise, attention, and diligence.\nPairwise comparisons improve consistency, yet aggregating them into overall\nquality scores requires careful modeling. Bradley-Terry-based methods update\nitem scores from comparisons, but existing approaches either ignore rater\nvariability or lack convergence guarantees, limiting robustness and\ninterpretability. We introduce BBQ, a Bayesian Bradley-Terry variant that\nexplicitly models rater quality, downweighting or removing unreliable\nparticipants, and provides guaranteed monotonic likelihood convergence through\nan Expectation-Maximization algorithm. Empirical results show that BBQ achieves\nfaster convergence, well-calibrated uncertainty estimates, and more robust,\ninterpretable rankings compared to baseline Bradley-Terry models, even with\nnoisy or crowdsourced raters. This framework enables more reliable and\ncost-effective human evaluation of generative models.",
            "headline_zh": "提出BBQ贝叶斯方法以解决生成模型评估中噪声成对比较的聚合问题",
            "intro_zh": [
                "核心问题：生成模型评估中人类偏好可靠但成本高、噪声大，成对比较聚合需稳健建模",
                "方法要点：BBQ基于Bradley-Terry模型，显式建模评估者质量，使用EM算法保证单调收敛",
                "实验或效果：BBQ收敛更快，提供校准不确定性估计，在噪声评估者下更稳健可解释"
            ],
            "tags_zh": [
                "贝叶斯推断",
                "成对比较",
                "Bradley-Terry模型",
                "评估者质量建模",
                "生成模型评估"
            ],
            "_index": 36
        },
        {
            "title": "Instance-Aware Robust Consistency Regularization for Semi-Supervised Nuclei Instance Segmentation",
            "authors": [
                "Zenan Lin",
                "Wei Li",
                "Jintao Chen",
                "Zihao Wu",
                "Wenxiong Kang",
                "Changxin Gao",
                "Liansheng Wang",
                "Jin-Gang Yu"
            ],
            "arxiv_id": "2510.09329v1",
            "summary": "Nuclei instance segmentation in pathological images is crucial for downstream\ntasks such as tumor microenvironment analysis. However, the high cost and\nscarcity of annotated data limit the applicability of fully supervised methods,\nwhile existing semi-supervised methods fail to adequately regularize\nconsistency at the instance level, lack leverage of the inherent prior\nknowledge of pathological structures, and are prone to introducing noisy\npseudo-labels during training. In this paper, we propose an Instance-Aware\nRobust Consistency Regularization Network (IRCR-Net) for accurate\ninstance-level nuclei segmentation. Specifically, we introduce the\nMatching-Driven Instance-Aware Consistency (MIAC) and Prior-Driven\nInstance-Aware Consistency (PIAC) mechanisms to refine the nuclei instance\nsegmentation result of the teacher and student subnetwork, particularly for\ndensely distributed and overlapping nuclei. We incorporate morphological prior\nknowledge of nuclei in pathological images and utilize these priors to assess\nthe quality of pseudo-labels generated from unlabeled data. Low-quality\npseudo-labels are discarded, while high-quality predictions are enhanced to\nreduce pseudo-label noise and benefit the network's robust training.\nExperimental results demonstrate that the proposed method significantly\nenhances semi-supervised nuclei instance segmentation performance across\nmultiple public datasets compared to existing approaches, even surpassing fully\nsupervised methods in some scenarios.",
            "headline_zh": "提出IRCR-Net以解决半监督细胞核实例分割中的实例级一致性问题",
            "intro_zh": [
                "核心问题：半监督方法在细胞核实例分割中缺乏实例级一致性正则化，易产生噪声伪标签。",
                "方法要点：引入MIAC和PIAC机制，利用形态先验知识优化教师-学生网络的实例分割结果。",
                "实验或效果：在多个公开数据集上显著提升性能，部分场景超越全监督方法。"
            ],
            "tags_zh": [
                "半监督学习",
                "细胞核实例分割",
                "一致性正则化",
                "伪标签优化",
                "病理图像分析"
            ],
            "_index": 37
        },
        {
            "title": "Parametrized Topological Complexity for a Multi-Robot System with Variable Tasks",
            "authors": [
                "Gopal Chandra Dutta",
                "Amit Kumar Paul",
                "Subhankar Sau"
            ],
            "arxiv_id": "2510.09323v1",
            "summary": "We study a generalized motion planning problem involving multiple autonomous\nrobots navigating in a $d$-dimensional Euclidean space in the presence of a set\nof obstacles whose positions are unknown a priori. Each robot is required to\nvisit sequentially a prescribed set of target states, with the number of\ntargets varying between robots. This heterogeneous setting generalizes the\nframework considered in the prior works on sequential parametrized topological\ncomplexity by Farber and the second author of this article. To determine the\ntopological complexity of our problem, we formulate it mathematically by\nconstructing an appropriate fibration. Our main contribution is the\ndetermination of this invariant in the generalized setting, which captures the\nminimal algorithmic instability required for designing collision-free motion\nplanning algorithms under parameter-dependent constraints. We provide a\ndetailed analysis for both odd and even-dimensional ambient spaces, including\nthe essential cohomological computations and explicit constructions of\ncorresponding motion planning algorithms.",
            "headline_zh": "提出广义参数化拓扑复杂度以解决多机器人可变任务运动规划问题",
            "intro_zh": [
                "研究多机器人在未知障碍物环境中的序列目标访问问题，目标数量可变",
                "通过构造纤维化确定拓扑复杂度，捕获算法不稳定性的最小需求",
                "分析奇偶维空间，提供上同调计算和运动规划算法构造"
            ],
            "tags_zh": [
                "多机器人系统",
                "拓扑复杂度",
                "运动规划",
                "参数化问题",
                "上同调计算",
                "算法设计"
            ],
            "_index": 38
        },
        {
            "title": "Hybrid-grained Feature Aggregation with Coarse-to-fine Language Guidance for Self-supervised Monocular Depth Estimation",
            "authors": [
                "Wenyao Zhang",
                "Hongsi Liu",
                "Bohan Li",
                "Jiawei He",
                "Zekun Qi",
                "Yunnan Wang",
                "Shengyang Zhao",
                "Xinqiang Yu",
                "Wenjun Zeng",
                "Xin Jin"
            ],
            "arxiv_id": "2510.09320v1",
            "summary": "Current self-supervised monocular depth estimation (MDE) approaches encounter\nperformance limitations due to insufficient semantic-spatial knowledge\nextraction. To address this challenge, we propose Hybrid-depth, a novel\nframework that systematically integrates foundation models (e.g., CLIP and\nDINO) to extract visual priors and acquire sufficient contextual information\nfor MDE. Our approach introduces a coarse-to-fine progressive learning\nframework: 1) Firstly, we aggregate multi-grained features from CLIP (global\nsemantics) and DINO (local spatial details) under contrastive language\nguidance. A proxy task comparing close-distant image patches is designed to\nenforce depth-aware feature alignment using text prompts; 2) Next, building on\nthe coarse features, we integrate camera pose information and pixel-wise\nlanguage alignment to refine depth predictions. This module seamlessly\nintegrates with existing self-supervised MDE pipelines (e.g., Monodepth2,\nManyDepth) as a plug-and-play depth encoder, enhancing continuous depth\nestimation. By aggregating CLIP's semantic context and DINO's spatial details\nthrough language guidance, our method effectively addresses feature granularity\nmismatches. Extensive experiments on the KITTI benchmark demonstrate that our\nmethod significantly outperforms SOTA methods across all metrics, which also\nindeed benefits downstream tasks like BEV perception. Code is available at\nhttps://github.com/Zhangwenyao1/Hybrid-depth.",
            "headline_zh": "提出混合粒度特征聚合框架，通过粗到细语言引导解决自监督单目深度估计中语义-空间知识不足问题。",
            "intro_zh": [
                "核心问题：自监督单目深度估计中语义-空间知识提取不足，导致性能受限。",
                "方法要点：集成CLIP和DINO模型，通过对比语言引导聚合多粒度特征，并设计代理任务进行深度感知对齐。",
                "实验或效果：在KITTI基准测试中显著优于现有方法，并提升下游任务如BEV感知性能。"
            ],
            "tags_zh": [
                "自监督单目深度估计",
                "混合粒度特征聚合",
                "语言引导学习",
                "CLIP模型",
                "DINO模型",
                "KITTI基准测试"
            ],
            "_index": 39
        },
        {
            "title": "RadioFlow: Efficient Radio Map Construction Framework with Flow Matching",
            "authors": [
                "Haozhe Jia",
                "Wenshuo Chen",
                "Xiucheng Wang",
                "Nan Cheng",
                "Hongbo Zhang",
                "Kuimou Yu",
                "Songning Lai",
                "Nanjian Jia",
                "Bowen Tian",
                "Hongru Xiao",
                "Yutao Yue"
            ],
            "arxiv_id": "2510.09314v1",
            "summary": "Accurate and real-time radio map (RM) generation is crucial for\nnext-generation wireless systems, yet diffusion-based approaches often suffer\nfrom large model sizes, slow iterative denoising, and high inference latency,\nwhich hinder practical deployment. To overcome these limitations, we propose\n\\textbf{RadioFlow}, a novel flow-matching-based generative framework that\nachieves high-fidelity RM generation through single-step efficient sampling.\nUnlike conventional diffusion models, RadioFlow learns continuous transport\ntrajectories between noise and data, enabling both training and inference to be\nsignificantly accelerated while preserving reconstruction accuracy.\nComprehensive experiments demonstrate that RadioFlow achieves state-of-the-art\nperformance with \\textbf{up to 8$\\times$ fewer parameters} and \\textbf{over\n4$\\times$ faster inference} compared to the leading diffusion-based baseline\n(RadioDiff). This advancement provides a promising pathway toward scalable,\nenergy-efficient, and real-time electromagnetic digital twins for future 6G\nnetworks. We release the code at\n\\href{https://github.com/Hxxxz0/RadioFlow}{GitHub}.",
            "headline_zh": "提出RadioFlow框架，通过流匹配实现高效无线电地图构建，用于6G网络",
            "intro_zh": [
                "扩散模型在无线电地图生成中模型大、推理慢，阻碍实际部署",
                "采用流匹配学习噪声到数据的连续轨迹，实现单步高效采样",
                "实验显示参数减少8倍、推理加速4倍，保持高重建精度"
            ],
            "tags_zh": [
                "无线电地图生成",
                "流匹配",
                "高效采样",
                "6G网络",
                "生成模型",
                "推理加速"
            ],
            "_index": 40
        },
        {
            "title": "Rewiring Development in Brain Segmentation: Leveraging Adult Brain Priors for Enhancing Infant MRI Segmentation",
            "authors": [
                "Alemu Sisay Nigru",
                "Michele Svanera",
                "Austin Dibble",
                "Connor Dalby",
                "Mattia Savardi",
                "Sergio Benini"
            ],
            "arxiv_id": "2510.09306v1",
            "summary": "Accurate segmentation of infant brain MRI is critical for studying early\nneurodevelopment and diagnosing neurological disorders. Yet, it remains a\nfundamental challenge due to continuously evolving anatomy of the subjects,\nmotion artifacts, and the scarcity of high-quality labeled data. In this work,\nwe present LODi, a novel framework that utilizes prior knowledge from an adult\nbrain MRI segmentation model to enhance the segmentation performance of infant\nscans. Given the abundance of publicly available adult brain MRI data, we\npre-train a segmentation model on a large adult dataset as a starting point.\nThrough transfer learning and domain adaptation strategies, we progressively\nadapt the model to the 0-2 year-old population, enabling it to account for the\nanatomical and imaging variability typical of infant scans. The adaptation of\nthe adult model is carried out using weakly supervised learning on infant brain\nscans, leveraging silver-standard ground truth labels obtained with FreeSurfer.\nBy introducing a novel training strategy that integrates hierarchical feature\nrefinement and multi-level consistency constraints, our method enables fast,\naccurate, age-adaptive segmentation, while mitigating scanner and site-specific\nbiases. Extensive experiments on both internal and external datasets\ndemonstrate the superiority of our approach over traditional supervised\nlearning and domain-specific models. Our findings highlight the advantage of\nleveraging adult brain priors as a foundation for age-flexible neuroimaging\nanalysis, paving the way for more reliable and generalizable brain MRI\nsegmentation across the lifespan.",
            "headline_zh": "提出LODi框架，利用成人脑MRI先验增强婴儿脑MRI分割性能",
            "intro_zh": [
                "婴儿脑MRI分割面临解剖变化、运动伪影和标注数据稀缺等挑战",
                "通过迁移学习和领域适应，将成人模型逐步适配到0-2岁婴儿数据",
                "实验显示方法优于传统监督学习和领域特定模型，提升分割准确性"
            ],
            "tags_zh": [
                "脑MRI分割",
                "迁移学习",
                "领域适应",
                "婴儿神经影像",
                "弱监督学习"
            ],
            "_index": 41
        },
        {
            "title": "CapGeo: A Caption-Assisted Approach to Geometric Reasoning",
            "authors": [
                "Yuying Li",
                "Siyi Qian",
                "Hao Liang",
                "Leqi Zheng",
                "Ruichuan An",
                "Yongzhen Guo",
                "Wentao Zhang"
            ],
            "arxiv_id": "2510.09302v1",
            "summary": "Geometric reasoning remains a core challenge for Multimodal Large Language\nModels (MLLMs). Even the most advanced closed-source systems, such as GPT-O3\nand Gemini-2.5-Pro, still struggle to solve geometry problems reliably, despite\nexhibiting strong textual reasoning abilities on tasks like the International\nMathematical Olympiad (IMO). This gap suggests that the bottleneck lies in\nunderstanding geometric diagrams rather than reasoning itself. Since geometric\nfigures can often be faithfully described in concise textual form, converting\nvisual content into captions offers a promising direction. Motivated by this\ninsight, we introduce CapGeo, a caption-assisted reasoning framework that\nbridges visual and textual modalities. Experiments show substantial\nimprovements when models are equipped with captions: Qwen2.5-VL-72B improves\nfrom 8.6% (vision-only) to 59.0%, while Claude-Opus-4 rises from 44.8% to\n73.0%. To systematically evaluate and identify high-quality geometric\ncaptioning models, we further propose CapGeo-Bench, a dataset of 4,641 curated\nfigure-caption pairs. Crucially, CapGeo-Bench incorporates a keypoint-based\nevaluation metric that correlates strongly with downstream CapGeo performance,\nenabling reliable assessment of geometric captioning ability. Together, our\nframework and benchmark highlight a new pathway toward advancing geometric\nreasoning in MLLMs.",
            "headline_zh": "提出CapGeo框架，通过字幕辅助解决多模态大语言模型的几何推理瓶颈",
            "intro_zh": [
                "核心问题：多模态大语言模型在几何推理中难以可靠理解几何图形，而非推理能力本身。",
                "方法要点：将几何图形转换为文本字幕，桥接视觉与文本模态以提升推理性能。",
                "实验效果：在基准测试中，模型准确率显著提升，如Qwen2.5-VL-72B从8.6%增至59.0%。"
            ],
            "tags_zh": [
                "几何推理",
                "多模态大语言模型",
                "字幕辅助",
                "基准数据集",
                "关键点评估"
            ],
            "_index": 42
        },
        {
            "title": "Foraging with the Eyes: Dynamics in Human Visual Gaze and Deep Predictive Modeling",
            "authors": [
                "Tejaswi V. Panchagnula"
            ],
            "arxiv_id": "2510.09299v1",
            "summary": "Animals often forage via Levy walks stochastic trajectories with heavy tailed\nstep lengths optimized for sparse resource environments. We show that human\nvisual gaze follows similar dynamics when scanning images. While traditional\nmodels emphasize image based saliency, the underlying spatiotemporal statistics\nof eye movements remain underexplored. Understanding these dynamics has broad\napplications in attention modeling and vision-based interfaces. In this study,\nwe conducted a large scale human subject experiment involving 40 participants\nviewing 50 diverse images under unconstrained conditions, recording over 4\nmillion gaze points using a high speed eye tracker. Analysis of these data\nshows that the gaze trajectory of the human eye also follows a Levy walk akin\nto animal foraging. This suggests that the human eye forages for visual\ninformation in an optimally efficient manner. Further, we trained a\nconvolutional neural network (CNN) to predict fixation heatmaps from image\ninput alone. The model accurately reproduced salient fixation regions across\nnovel images, demonstrating that key components of gaze behavior are learnable\nfrom visual structure alone. Our findings present new evidence that human\nvisual exploration obeys statistical laws analogous to natural foraging and\nopen avenues for modeling gaze through generative and predictive frameworks.",
            "headline_zh": "揭示人类视觉注视遵循莱维行走动态，并训练CNN预测注视热图。",
            "intro_zh": [
                "核心问题：人类视觉注视的时空统计动态是否类似动物觅食的莱维行走。",
                "方法要点：分析大规模眼动数据，并训练卷积神经网络预测注视热图。",
                "实验或效果：40名参与者观看50张图像，模型能准确预测新图像的注视区域。"
            ],
            "tags_zh": [
                "视觉注视动态",
                "莱维行走",
                "眼动追踪",
                "卷积神经网络",
                "注视预测",
                "视觉探索"
            ],
            "_index": 43
        },
        {
            "title": "Spotlight on Token Perception for Multimodal Reinforcement Learning",
            "authors": [
                "Siyuan Huang",
                "Xiaoye Qu",
                "Yafu Li",
                "Yun Luo",
                "Zefeng He",
                "Daizong Liu",
                "Yu Cheng"
            ],
            "arxiv_id": "2510.09285v1",
            "summary": "While Reinforcement Learning with Verifiable Rewards (RLVR) has advanced the\nreasoning capabilities of Large Vision-Language Models (LVLMs), most existing\nmethods in multimodal reasoning neglect the critical role of visual perception\nwithin the RLVR optimization process. In this paper, we undertake a pioneering\nexploration of multimodal RLVR through the novel perspective of token\nperception, which measures the visual dependency of each generated token. With\na granular analysis of Chain-of-Thought (CoT) processes, we uncover two key\ninsights: first, token perception in a rollout trajectory is sparsely\ndistributed, where only a small fraction of tokens have high visual dependency\nfor visually-grounded reasoning; second, different trajectories exhibit\nsignificant divergence in their overall visual dependency. Based on these\nobservations, we propose Visually-Perceptive Policy Optimization (VPPO), a\nnovel policy gradient algorithm that explicitly leverages token perception to\nrefine the learning signal. Specifically, VPPO achieves this through a dual\nmechanism: it reweights a trajectory's advantage by its overall visual\ndependency, and focuses policy updates exclusively on perceptually pivotal\ntokens. On a comprehensive suite of eight perception and reasoning benchmarks,\nVPPO demonstrates substantial gains over leading open-source RL-tuned models,\nwith its effectiveness consistently validated across 7B and 32B model scales.\nOur findings not only establish a new token-level perceptual perspective for\nanalyzing multimodal RLVR but also present a novel and effective optimization\nstrategy to significantly enhance the multimodal reasoning capabilities of\nLVLMs.",
            "headline_zh": "提出视觉感知策略优化以增强多模态强化学习中的视觉推理能力",
            "intro_zh": [
                "现有方法忽视视觉感知在强化学习优化中的作用，导致多模态推理能力受限",
                "引入令牌感知视角，通过重新加权轨迹优势和聚焦关键令牌优化策略",
                "在多个基准测试中显著优于现有模型，验证了方法的有效性和可扩展性"
            ],
            "tags_zh": [
                "多模态强化学习",
                "令牌感知",
                "视觉依赖",
                "策略优化",
                "大型视觉语言模型",
                "推理基准"
            ],
            "_index": 44
        },
        {
            "title": "MomentSeg: Moment-Centric Sampling for Enhanced Video Pixel Understanding",
            "authors": [
                "Ming Dai",
                "Sen Yang",
                "Boqiang Duan",
                "Wankou Yang",
                "Jingdong Wang"
            ],
            "arxiv_id": "2510.09274v1",
            "summary": "Referring Video Object Segmentation (RefVOS) seeks to segment target objects\nin videos guided by natural language descriptions, demanding both temporal\nreasoning and fine-grained visual comprehension. Existing sampling strategies\nfor LLM-based approaches typically rely on either handcrafted heuristics or\nexternal keyframe models. The former often overlooks essential temporal cues,\nwhile the latter increases system complexity. To address this, we propose a\nunified framework that jointly optimizes Temporal Sentence Grounding (TSG) and\nRefVOS, naturally incorporating key moment grounding capability. During\ntraining, we introduce a novel TSG paradigm that employs a dedicated\n\\texttt{[FIND]} token for key moment identification through temporal token\nsimilarity matching, thereby avoiding the need for external timestamp\nencodings. For inference, we design a Moment-Centric Sampling (MCS) strategy\nthat densely samples informative moments while sparsely sampling non-essential\nframes, preserving both motion details and global context. To further enhance\ntracking stability, we develop Bidirectional Anchor-updated Propagation (BAP),\nwhich leverages the most relevant moment as start point for high-quality mask\ninitialization and dynamically updates at sampled points to mitigate\naccumulated errors. Code and model will be available at:\nhttps://github.com/Dmmm1997/MomentSeg",
            "headline_zh": "提出MomentSeg框架，通过关键时刻采样优化视频像素理解，解决RefVOS中的时序推理问题。",
            "intro_zh": [
                "核心问题：现有RefVOS采样策略忽略时序线索或增加系统复杂性。",
                "方法要点：联合优化TSG和RefVOS，使用MCS策略密集采样关键帧。",
                "实验或效果：未知，但代码和模型将开源，可能提升分割精度。"
            ],
            "tags_zh": [
                "视频对象分割",
                "时序句子定位",
                "关键时刻采样",
                "双向传播",
                "语言引导分割"
            ],
            "_index": 45
        },
        {
            "title": "Goal-oriented Backdoor Attack against Vision-Language-Action Models via Physical Objects",
            "authors": [
                "Zirun Zhou",
                "Zhengyang Xiao",
                "Haochuan Xu",
                "Jing Sun",
                "Di Wang",
                "Jingfeng Zhang"
            ],
            "arxiv_id": "2510.09269v1",
            "summary": "Recent advances in vision-language-action (VLA) models have greatly improved\nembodied AI, enabling robots to follow natural language instructions and\nperform diverse tasks. However, their reliance on uncurated training datasets\nraises serious security concerns. Existing backdoor attacks on VLAs mostly\nassume white-box access and result in task failures instead of enforcing\nspecific actions. In this work, we reveal a more practical threat: attackers\ncan manipulate VLAs by simply injecting physical objects as triggers into the\ntraining dataset. We propose goal-oriented backdoor attacks (GoBA), where the\nVLA behaves normally in the absence of physical triggers but executes\npredefined and goal-oriented actions in the presence of physical triggers.\nSpecifically, based on a popular VLA benchmark LIBERO, we introduce BadLIBERO\nthat incorporates diverse physical triggers and goal-oriented backdoor actions.\nIn addition, we propose a three-level evaluation that categorizes the victim\nVLA's actions under GoBA into three states: nothing to do, try to do, and\nsuccess to do. Experiments show that GoBA enables the victim VLA to\nsuccessfully achieve the backdoor goal in 97 percentage of inputs when the\nphysical trigger is present, while causing zero performance degradation on\nclean inputs. Finally, by investigating factors related to GoBA, we find that\nthe action trajectory and trigger color significantly influence attack\nperformance, while trigger size has surprisingly little effect. The code and\nBadLIBERO dataset are accessible via the project page at\nhttps://goba-attack.github.io/.",
            "headline_zh": "提出目标导向后门攻击，通过物理对象操纵视觉-语言-动作模型",
            "intro_zh": [
                "核心问题：视觉-语言-动作模型依赖未筛选数据集，易受物理对象后门攻击威胁。",
                "方法要点：注入物理触发对象，使模型在触发时执行预设目标动作，正常时无影响。",
                "实验效果：攻击成功率97%，清洁输入性能零下降，动作轨迹和颜色影响显著。"
            ],
            "tags_zh": [
                "后门攻击",
                "视觉-语言-动作模型",
                "物理触发",
                "目标导向",
                "安全评估",
                "机器人控制"
            ],
            "_index": 46
        },
        {
            "title": "Placeit! A Framework for Learning Robot Object Placement Skills",
            "authors": [
                "Amina Ferrad",
                "Johann Huber",
                "François Hélénon",
                "Julien Gleyze",
                "Mahdi Khoramshahi",
                "Stéphane Doncieux"
            ],
            "arxiv_id": "2510.09267v1",
            "summary": "Robotics research has made significant strides in learning, yet mastering\nbasic skills like object placement remains a fundamental challenge. A key\nbottleneck is the acquisition of large-scale, high-quality data, which is often\na manual and laborious process. Inspired by Graspit!, a foundational work that\nused simulation to automatically generate dexterous grasp poses, we introduce\nPlaceit!, an evolutionary-computation framework for generating valid placement\npositions for rigid objects. Placeit! is highly versatile, supporting tasks\nfrom placing objects on tables to stacking and inserting them. Our experiments\nshow that by leveraging quality-diversity optimization, Placeit! significantly\noutperforms state-of-the-art methods across all scenarios for generating\ndiverse valid poses. A pick&place pipeline built on our framework achieved a\n90% success rate over 120 real-world deployments. This work positions Placeit!\nas a powerful tool for open-environment pick-and-place tasks and as a valuable\nengine for generating the data needed to train simulation-based foundation\nmodels in robotics.",
            "headline_zh": "提出Placeit!框架，通过进化计算生成机器人物体放置技能，解决数据获取瓶颈。",
            "intro_zh": [
                "核心问题：机器人物体放置技能学习面临大规模高质量数据获取困难。",
                "方法要点：基于进化计算和质量多样性优化，自动生成多样有效放置位姿。",
                "实验或效果：在真实世界部署中，基于框架的拾放管道成功率高达90%。"
            ],
            "tags_zh": [
                "机器人放置技能",
                "进化计算",
                "质量多样性优化",
                "仿真数据生成",
                "拾放任务"
            ],
            "_index": 47
        },
        {
            "title": "Hallucination Filtering in Radiology Vision-Language Models Using Discrete Semantic Entropy",
            "authors": [
                "Patrick Wienholt",
                "Sophie Caselitz",
                "Robert Siepmann",
                "Philipp Bruners",
                "Keno Bressem",
                "Christiane Kuhl",
                "Jakob Nikolas Kather",
                "Sven Nebelung",
                "Daniel Truhn"
            ],
            "arxiv_id": "2510.09256v1",
            "summary": "To determine whether using discrete semantic entropy (DSE) to reject\nquestions likely to generate hallucinations can improve the accuracy of\nblack-box vision-language models (VLMs) in radiologic image based visual\nquestion answering (VQA). This retrospective study evaluated DSE using two\npublicly available, de-identified datasets: (i) the VQA-Med 2019 benchmark (500\nimages with clinical questions and short-text answers) and (ii) a diagnostic\nradiology dataset (206 cases: 60 computed tomography scans, 60 magnetic\nresonance images, 60 radiographs, 26 angiograms) with corresponding\nground-truth diagnoses. GPT-4o and GPT-4.1 answered each question 15 times\nusing a temperature of 1.0. Baseline accuracy was determined using\nlow-temperature answers (temperature 0.1). Meaning-equivalent responses were\ngrouped using bidirectional entailment checks, and DSE was computed from the\nrelative frequencies of the resulting semantic clusters. Accuracy was\nrecalculated after excluding questions with DSE > 0.6 or > 0.3. p-values and\n95% confidence intervals were obtained using bootstrap resampling and a\nBonferroni-corrected threshold of p < .004 for statistical significance. Across\n706 image-question pairs, baseline accuracy was 51.7% for GPT-4o and 54.8% for\nGPT-4.1. After filtering out high-entropy questions (DSE > 0.3), accuracy on\nthe remaining questions was 76.3% (retained questions: 334/706) for GPT-4o and\n63.8% (retained questions: 499/706) for GPT-4.1 (both p < .001). Accuracy gains\nwere observed across both datasets and largely remained statistically\nsignificant after Bonferroni correction. DSE enables reliable hallucination\ndetection in black-box VLMs by quantifying semantic inconsistency. This method\nsignificantly improves diagnostic answer accuracy and offers a filtering\nstrategy for clinical VLM applications.",
            "headline_zh": "提出离散语义熵方法以过滤放射学视觉语言模型中的幻觉问题",
            "intro_zh": [
                "核心问题：黑盒视觉语言模型在放射学视觉问答中易产生幻觉，影响诊断准确性。",
                "方法要点：使用离散语义熵量化语义不一致性，过滤高熵问题以减少幻觉。",
                "实验效果：在706个图像-问题对中，过滤后GPT-4o准确率从51.7%提升至76.3%。"
            ],
            "tags_zh": [
                "视觉语言模型",
                "离散语义熵",
                "放射学视觉问答",
                "幻觉过滤",
                "诊断准确性"
            ],
            "_index": 48
        },
        {
            "title": "Obstacle Avoidance using Dynamic Movement Primitives and Reinforcement Learning",
            "authors": [
                "Dominik Urbaniak",
                "Alejandro Agostini",
                "Pol Ramon",
                "Jan Rosell",
                "Raúl Suárez",
                "Michael Suppa"
            ],
            "arxiv_id": "2510.09254v1",
            "summary": "Learning-based motion planning can quickly generate near-optimal\ntrajectories. However, it often requires either large training datasets or\ncostly collection of human demonstrations. This work proposes an alternative\napproach that quickly generates smooth, near-optimal collision-free 3D\nCartesian trajectories from a single artificial demonstration. The\ndemonstration is encoded as a Dynamic Movement Primitive (DMP) and iteratively\nreshaped using policy-based reinforcement learning to create a diverse\ntrajectory dataset for varying obstacle configurations. This dataset is used to\ntrain a neural network that takes as inputs the task parameters describing the\nobstacle dimensions and location, derived automatically from a point cloud, and\noutputs the DMP parameters that generate the trajectory. The approach is\nvalidated in simulation and real-robot experiments, outperforming a RRT-Connect\nbaseline in terms of computation and execution time, as well as trajectory\nlength, while supporting multi-modal trajectory generation for different\nobstacle geometries and end-effector dimensions. Videos and the implementation\ncode are available at https://github.com/DominikUrbaniak/obst-avoid-dmp-pi2.",
            "headline_zh": "提出结合动态运动基元与强化学习的避障方法，从单演示生成平滑3D轨迹",
            "intro_zh": [
                "核心问题：学习型运动规划需大量数据或昂贵演示，难以快速生成最优轨迹",
                "方法要点：用DMP编码演示，强化学习迭代优化，训练网络输出轨迹参数",
                "实验或效果：仿真与真实实验验证，优于RRT-Connect，支持多模态轨迹生成"
            ],
            "tags_zh": [
                "动态运动基元",
                "强化学习",
                "避障规划",
                "轨迹生成",
                "机器人控制"
            ],
            "_index": 49
        },
        {
            "title": "Zero-shot image privacy classification with Vision-Language Models",
            "authors": [
                "Alina Elena Baia",
                "Alessio Xompero",
                "Andrea Cavallaro"
            ],
            "arxiv_id": "2510.09253v1",
            "summary": "While specialized learning-based models have historically dominated image\nprivacy prediction, the current literature increasingly favours adopting large\nVision-Language Models (VLMs) designed for generic tasks. This trend risks\noverlooking the performance ceiling set by purpose-built models due to a lack\nof systematic evaluation. To address this problem, we establish a zero-shot\nbenchmark for image privacy classification, enabling a fair comparison. We\nevaluate the top-3 open-source VLMs, according to a privacy benchmark, using\ntask-aligned prompts and we contrast their performance, efficiency, and\nrobustness against established vision-only and multi-modal methods.\nCounter-intuitively, our results show that VLMs, despite their\nresource-intensive nature in terms of high parameter count and slower\ninference, currently lag behind specialized, smaller models in privacy\nprediction accuracy. We also find that VLMs exhibit higher robustness to image\nperturbations.",
            "headline_zh": "建立零样本基准以公平比较视觉语言模型在图像隐私分类中的性能。",
            "intro_zh": [
                "核心问题：缺乏系统评估，视觉语言模型在图像隐私预测中可能被高估。",
                "方法要点：使用任务对齐提示评估开源视觉语言模型，对比专业模型。",
                "实验或效果：视觉语言模型精度较低但鲁棒性更高，资源消耗大。"
            ],
            "tags_zh": [
                "图像隐私分类",
                "视觉语言模型",
                "零样本学习",
                "基准评估",
                "鲁棒性分析"
            ],
            "_index": 50
        },
        {
            "title": "Diagnosing Shoulder Disorders Using Multimodal Large Language Models and Consumer-Grade Cameras",
            "authors": [
                "Jindong Hong",
                "Wencheng Zhang",
                "Shiqin Qiao",
                "Jianhai Chen",
                "Jianing Qiu",
                "Chuanyang Zheng",
                "Qian Xu",
                "Yun Ji",
                "Qianyue Wen",
                "Weiwei Sun",
                "Hao Li",
                "Huizhen Li",
                "Huichao Wang",
                "Kai Wu",
                "Meng Li",
                "Yijun He",
                "Lingjie Luo",
                "Jiankai Sun"
            ],
            "arxiv_id": "2510.09230v1",
            "summary": "Shoulder disorders, such as frozen shoulder (a.k.a., adhesive capsulitis),\nare common conditions affecting the health of people worldwide, and have a high\nincidence rate among the elderly and workers engaged in repetitive shoulder\ntasks. In regions with scarce medical resources, achieving early and accurate\ndiagnosis poses significant challenges, and there is an urgent need for\nlow-cost and easily scalable auxiliary diagnostic solutions. This research\nintroduces videos captured by consumer-grade devices as the basis for\ndiagnosis, reducing the cost for users. We focus on the innovative application\nof Multimodal Large Language Models (MLLMs) in the preliminary diagnosis of\nshoulder disorders and propose a Hybrid Motion Video Diagnosis framework\n(HMVDx). This framework divides the two tasks of action understanding and\ndisease diagnosis, which are respectively completed by two MLLMs. In addition\nto traditional evaluation indicators, this work proposes a novel metric called\nUsability Index by the logical process of medical decision-making (action\nrecognition, movement diagnosis, and final diagnosis). This index evaluates the\neffectiveness of MLLMs in the medical field from the perspective of the entire\nmedical diagnostic pathway, revealing the potential value of low-cost MLLMs in\nmedical applications for medical practitioners. In experimental comparisons,\nthe accuracy of HMVDx in diagnosing shoulder joint injuries has increased by\n79.6\\% compared with direct video diagnosis, a significant technical\ncontribution to future research on the application of MLLMs for video\nunderstanding in the medical field.",
            "headline_zh": "提出HMVDx框架，使用消费级摄像头和MLLMs辅助诊断肩部疾病。",
            "intro_zh": [
                "核心问题：医疗资源稀缺地区肩部疾病早期诊断困难，需低成本可扩展方案。",
                "方法要点：HMVDx框架分动作理解和疾病诊断任务，由两个MLLMs分别完成。",
                "实验或效果：HMVDx诊断准确率比直接视频诊断提升79.6%，并引入可用性指数评估。"
            ],
            "tags_zh": [
                "多模态大语言模型",
                "肩部疾病诊断",
                "消费级摄像头",
                "视频理解",
                "医疗辅助诊断",
                "可用性指数"
            ],
            "_index": 51
        },
        {
            "title": "Glovity: Learning Dexterous Contact-Rich Manipulation via Spatial Wrench Feedback Teleoperation System",
            "authors": [
                "Yuyang Gao",
                "Haofei Ma",
                "Pai Zheng"
            ],
            "arxiv_id": "2510.09229v1",
            "summary": "We present Glovity, a novel, low-cost wearable teleoperation system that\nintegrates a spatial wrench (force-torque) feedback device with a haptic glove\nfeaturing fingertip Hall sensor calibration, enabling feedback-rich dexterous\nmanipulation. Glovity addresses key challenges in contact-rich tasks by\nproviding intuitive wrench and tactile feedback, while overcoming embodiment\ngaps through precise retargeting. User studies demonstrate significant\nimprovements: wrench feedback boosts success rates in book-flipping tasks from\n48% to 78% and reduces completion time by 25%, while fingertip calibration\nenhances thin-object grasping success significantly compared to commercial\nglove. Furthermore, incorporating wrench signals into imitation learning (via\nDP-R3M) achieves high success rate in novel contact-rich scenarios, such as\nadaptive page flipping and force-aware handovers. All hardware designs,\nsoftware will be open-sourced. Project website: https://glovity.github.io/",
            "headline_zh": "提出Glovity系统以解决接触丰富灵巧操作中的反馈不足问题",
            "intro_zh": [
                "核心问题：接触丰富任务中缺乏直观力反馈和精确重定向，导致操作成功率低。",
                "方法要点：集成空间力反馈设备和带指尖校准的触觉手套，提供丰富反馈。",
                "实验效果：力反馈使书本翻转成功率从48%提升至78%，模仿学习在新型任务中表现优异。"
            ],
            "tags_zh": [
                "灵巧操作",
                "力反馈系统",
                "触觉手套",
                "模仿学习",
                "开源硬件"
            ],
            "_index": 52
        },
        {
            "title": "Clear Roads, Clear Vision: Advancements in Multi-Weather Restoration for Smart Transportation",
            "authors": [
                "Vijay M. Galshetwar",
                "Praful Hambarde",
                "Prashant W. Patil",
                "Akshay Dudhane",
                "Sachin Chaudhary",
                "Santosh Kumar Vipparathi",
                "Subrahmanyam Murala"
            ],
            "arxiv_id": "2510.09228v1",
            "summary": "Adverse weather conditions such as haze, rain, and snow significantly degrade\nthe quality of images and videos, posing serious challenges to intelligent\ntransportation systems (ITS) that rely on visual input. These degradations\naffect critical applications including autonomous driving, traffic monitoring,\nand surveillance. This survey presents a comprehensive review of image and\nvideo restoration techniques developed to mitigate weather-induced visual\nimpairments. We categorize existing approaches into traditional prior-based\nmethods and modern data-driven models, including CNNs, transformers, diffusion\nmodels, and emerging vision-language models (VLMs). Restoration strategies are\nfurther classified based on their scope: single-task models,\nmulti-task/multi-weather systems, and all-in-one frameworks capable of handling\ndiverse degradations. In addition, we discuss day and night time restoration\nchallenges, benchmark datasets, and evaluation protocols. The survey concludes\nwith an in-depth discussion on limitations in current research and outlines\nfuture directions such as mixed/compound-degradation restoration, real-time\ndeployment, and agentic AI frameworks. This work aims to serve as a valuable\nreference for advancing weather-resilient vision systems in smart\ntransportation environments. Lastly, to stay current with rapid advancements in\nthis field, we will maintain regular updates of the latest relevant papers and\ntheir open-source implementations at\nhttps://github.com/ChaudharyUPES/A-comprehensive-review-on-Multi-weather-restoration",
            "headline_zh": "综述多天气图像视频恢复技术以提升智能交通系统视觉输入质量",
            "intro_zh": [
                "核心问题：雾霾、雨雪等恶劣天气导致图像视频质量下降，影响自动驾驶和交通监控。",
                "方法要点：分类传统先验方法和现代数据驱动模型，包括CNN、Transformer和扩散模型。",
                "实验或效果：讨论基准数据集、评估协议，并指出未来方向如混合退化恢复和实时部署。"
            ],
            "tags_zh": [
                "图像恢复",
                "视频恢复",
                "多天气处理",
                "智能交通系统",
                "数据驱动模型",
                "基准数据集"
            ],
            "_index": 53
        },
        {
            "title": "Tag-Enriched Multi-Attention with Large Language Models for Cross-Domain Sequential Recommendation",
            "authors": [
                "Wangyu Wu",
                "Xuhang Chen",
                "Zhenhong Chen",
                "Jing-En Jiang",
                "Kim-Fung Tsang",
                "Xiaowei Huang",
                "Fei Ma",
                "Jimin Xiao"
            ],
            "arxiv_id": "2510.09224v1",
            "summary": "Cross-Domain Sequential Recommendation (CDSR) plays a crucial role in modern\nconsumer electronics and e-commerce platforms, where users interact with\ndiverse services such as books, movies, and online retail products. These\nsystems must accurately capture both domain-specific and cross-domain\nbehavioral patterns to provide personalized and seamless consumer experiences.\nTo address this challenge, we propose \\textbf{TEMA-LLM} (\\textit{Tag-Enriched\nMulti-Attention with Large Language Models}), a practical and effective\nframework that integrates \\textit{Large Language Models (LLMs)} for semantic\ntag generation and enrichment. Specifically, TEMA-LLM employs LLMs to assign\ndomain-aware prompts and generate descriptive tags from item titles and\ndescriptions. The resulting tag embeddings are fused with item identifiers as\nwell as textual and visual features to construct enhanced item representations.\nA \\textit{Tag-Enriched Multi-Attention} mechanism is then introduced to jointly\nmodel user preferences within and across domains, enabling the system to\ncapture complex and evolving consumer interests. Extensive experiments on four\nlarge-scale e-commerce datasets demonstrate that TEMA-LLM consistently\noutperforms state-of-the-art baselines, underscoring the benefits of LLM-based\nsemantic tagging and multi-attention integration for consumer-facing\nrecommendation systems. The proposed approach highlights the potential of LLMs\nto advance intelligent, user-centric services in the field of consumer\nelectronics.",
            "headline_zh": "提出TEMA-LLM框架，利用大语言模型生成语义标签以提升跨域序列推荐性能",
            "intro_zh": [
                "跨域序列推荐需捕获域内和跨域用户行为模式，以提供个性化体验",
                "集成大语言模型生成语义标签，结合多特征构建增强物品表示",
                "在四个电商数据集上实验，TEMA-LLM优于现有基线方法"
            ],
            "tags_zh": [
                "跨域序列推荐",
                "大语言模型",
                "语义标签生成",
                "多注意力机制",
                "物品表示增强"
            ],
            "_index": 54
        },
        {
            "title": "HANDO: Hierarchical Autonomous Navigation and Dexterous Omni-loco-manipulation",
            "authors": [
                "Jingyuan Sun",
                "Chaoran Wang",
                "Mingyu Zhang",
                "Cui Miao",
                "Hongyu Ji",
                "Zihan Qu",
                "Han Sun",
                "Bing Wang",
                "Qingyi Si"
            ],
            "arxiv_id": "2510.09221v1",
            "summary": "Seamless loco-manipulation in unstructured environments requires robots to\nleverage autonomous exploration alongside whole-body control for physical\ninteraction. In this work, we introduce HANDO (Hierarchical Autonomous\nNavigation and Dexterous Omni-loco-manipulation), a two-layer framework\ndesigned for legged robots equipped with manipulators to perform human-centered\nmobile manipulation tasks. The first layer utilizes a goal-conditioned\nautonomous exploration policy to guide the robot to semantically specified\ntargets, such as a black office chair in a dynamic environment. The second\nlayer employs a unified whole-body loco-manipulation policy to coordinate the\narm and legs for precise interaction tasks-for example, handing a drink to a\nperson seated on the chair. We have conducted an initial deployment of the\nnavigation module, and will continue to pursue finer-grained deployment of\nwhole-body loco-manipulation.",
            "headline_zh": "提出HANDO框架以解决腿式机器人在非结构化环境中的自主导航与全身操作问题",
            "intro_zh": [
                "核心问题：非结构化环境中机器人需结合自主探索与全身控制进行无缝移动操作",
                "方法要点：采用双层框架，包括目标导向自主探索和统一全身移动操作策略",
                "实验或效果：已初步部署导航模块，计划进一步部署精细全身操作"
            ],
            "tags_zh": [
                "腿式机器人",
                "自主导航",
                "全身操作",
                "分层控制",
                "非结构化环境"
            ],
            "_index": 55
        },
        {
            "title": "Stable Video Infinity: Infinite-Length Video Generation with Error Recycling",
            "authors": [
                "Wuyang Li",
                "Wentao Pan",
                "Po-Chien Luan",
                "Yang Gao",
                "Alexandre Alahi"
            ],
            "arxiv_id": "2510.09212v1",
            "summary": "We propose Stable Video Infinity (SVI) that is able to generate\ninfinite-length videos with high temporal consistency, plausible scene\ntransitions, and controllable streaming storylines. While existing long-video\nmethods attempt to mitigate accumulated errors via handcrafted anti-drifting\n(e.g., modified noise scheduler, frame anchoring), they remain limited to\nsingle-prompt extrapolation, producing homogeneous scenes with repetitive\nmotions. We identify that the fundamental challenge extends beyond error\naccumulation to a critical discrepancy between the training assumption (seeing\nclean data) and the test-time autoregressive reality (conditioning on\nself-generated, error-prone outputs). To bridge this hypothesis gap, SVI\nincorporates Error-Recycling Fine-Tuning, a new type of efficient training that\nrecycles the Diffusion Transformer (DiT)'s self-generated errors into\nsupervisory prompts, thereby encouraging DiT to actively identify and correct\nits own errors. This is achieved by injecting, collecting, and banking errors\nthrough closed-loop recycling, autoregressively learning from error-injected\nfeedback. Specifically, we (i) inject historical errors made by DiT to\nintervene on clean inputs, simulating error-accumulated trajectories in flow\nmatching; (ii) efficiently approximate predictions with one-step bidirectional\nintegration and calculate errors with residuals; (iii) dynamically bank errors\ninto replay memory across discretized timesteps, which are resampled for new\ninput. SVI is able to scale videos from seconds to infinite durations with no\nadditional inference cost, while remaining compatible with diverse conditions\n(e.g., audio, skeleton, and text streams). We evaluate SVI on three benchmarks,\nincluding consistent, creative, and conditional settings, thoroughly verifying\nits versatility and state-of-the-art role.",
            "headline_zh": "提出Stable Video Infinity，通过错误回收微调实现无限长度视频生成",
            "intro_zh": [
                "核心问题：训练假设与自回归生成间的误差累积和假设差距，导致场景重复和运动单调。",
                "方法要点：采用错误回收微调，将自生成错误作为监督提示，促进模型主动识别和纠正错误。",
                "实验或效果：在三个基准测试中验证其多功能性和先进性能，支持从秒级到无限时长视频生成。"
            ],
            "tags_zh": [
                "无限视频生成",
                "错误回收微调",
                "扩散变换器",
                "自回归学习",
                "流匹配",
                "条件生成"
            ],
            "_index": 56
        },
        {
            "title": "PLEXUS Hand: Lightweight Four-Motor Prosthetic Hand Enabling Precision-Lateral Dexterous Manipulation",
            "authors": [
                "Yuki Kuroda",
                "Tomoya Takahashi",
                "Cristian C Beltran-Hernandez",
                "Masashi Hamaya",
                "Kazutoshi Tanaka"
            ],
            "arxiv_id": "2510.09209v1",
            "summary": "Electric prosthetic hands should be lightweight to decrease the burden on the\nuser, shaped like human hands for cosmetic purposes, and have motors inside to\nprotect them from damage and dirt. In addition to the ability to perform daily\nactivities, these features are essential for everyday use of the hand. In-hand\nmanipulation is necessary to perform daily activities such as transitioning\nbetween different postures, particularly through rotational movements, such as\nreorienting cards before slot insertion and operating tools such as\nscrewdrivers. However, currently used electric prosthetic hands only achieve\nstatic grasp postures, and existing manipulation approaches require either many\nmotors, which makes the prosthesis heavy for daily use in the hand, or complex\nmechanisms that demand a large internal space and force external motor\nplacement, complicating attachment and exposing the components to damage.\nAlternatively, we combine a single-axis thumb and optimized thumb positioning\nto achieve basic posture and in-hand manipulation, that is, the reorientation\nbetween precision and lateral grasps, using only four motors in a lightweight\n(311 g) prosthetic hand. Experimental validation using primitive objects of\nvarious widths (5-30 mm) and shapes (cylinders and prisms) resulted in success\nrates of 90-100% for reorientation tasks. The hand performed seal stamping and\nUSB device insertion, as well as rotation to operate a screwdriver.",
            "headline_zh": "提出四电机轻量假手，结合单轴拇指实现精确-侧向灵巧操作。",
            "intro_zh": [
                "现有假手仅静态抓握，多电机或复杂机制导致笨重和易损问题。",
                "采用单轴拇指和优化定位，仅四电机实现轻量化和内部电机保护。",
                "实验验证重定向任务成功率90-100%，支持盖章和螺丝刀操作。"
            ],
            "tags_zh": [
                "假肢手设计",
                "灵巧操作",
                "轻量化结构",
                "电机优化",
                "实验验证"
            ],
            "_index": 57
        },
        {
            "title": "3D Reconstruction from Transient Measurements with Time-Resolved Transformer",
            "authors": [
                "Yue Li",
                "Shida Sun",
                "Yu Hong",
                "Feihu Xu",
                "Zhiwei Xiong"
            ],
            "arxiv_id": "2510.09205v1",
            "summary": "Transient measurements, captured by the timeresolved systems, are widely\nemployed in photon-efficient reconstruction tasks, including line-of-sight\n(LOS) and non-line-of-sight (NLOS) imaging. However, challenges persist in\ntheir 3D reconstruction due to the low quantum efficiency of sensors and the\nhigh noise levels, particularly for long-range or complex scenes. To boost the\n3D reconstruction performance in photon-efficient imaging, we propose a generic\nTime-Resolved Transformer (TRT) architecture. Different from existing\ntransformers designed for high-dimensional data, TRT has two elaborate\nattention designs tailored for the spatio-temporal transient measurements.\nSpecifically, the spatio-temporal self-attention encoders explore both local\nand global correlations within transient data by splitting or downsampling\ninput features into different scales. Then, the spatio-temporal cross attention\ndecoders integrate the local and global features in the token space, resulting\nin deep features with high representation capabilities. Building on TRT, we\ndevelop two task-specific embodiments: TRT-LOS for LOS imaging and TRT-NLOS for\nNLOS imaging. Extensive experiments demonstrate that both embodiments\nsignificantly outperform existing methods on synthetic data and real-world data\ncaptured by different imaging systems. In addition, we contribute a\nlarge-scale, high-resolution synthetic LOS dataset with various noise levels\nand capture a set of real-world NLOS measurements using a custom-built imaging\nsystem, enhancing the data diversity in this field. Code and datasets are\navailable at https://github.com/Depth2World/TRT.",
            "headline_zh": "提出时间分辨Transformer以提升光子高效成像中的3D重建性能",
            "intro_zh": [
                "核心问题：瞬态测量中传感器量子效率低、噪声高，影响3D重建。",
                "方法要点：设计时空自注意力和交叉注意力，提取局部与全局特征。",
                "实验或效果：在合成和真实数据上优于现有方法，贡献新数据集。"
            ],
            "tags_zh": [
                "3D重建",
                "瞬态测量",
                "时间分辨Transformer",
                "非视距成像",
                "光子高效成像"
            ],
            "_index": 58
        },
        {
            "title": "Flow-Opt: Scalable Centralized Multi-Robot Trajectory Optimization with Flow Matching and Differentiable Optimization",
            "authors": [
                "Simon Idoko",
                "Arun Kumar Singh"
            ],
            "arxiv_id": "2510.09204v1",
            "summary": "Centralized trajectory optimization in the joint space of multiple robots\nallows access to a larger feasible space that can result in smoother\ntrajectories, especially while planning in tight spaces. Unfortunately, it is\noften computationally intractable beyond a very small swarm size. In this\npaper, we propose Flow-Opt, a learning-based approach towards improving the\ncomputational tractability of centralized multi-robot trajectory optimization.\nSpecifically, we reduce the problem to first learning a generative model to\nsample different candidate trajectories and then using a learned\nSafety-Filter(SF) to ensure fast inference-time constraint satisfaction. We\npropose a flow-matching model with a diffusion transformer (DiT) augmented with\npermutation invariant robot position and map encoders as the generative model.\nWe develop a custom solver for our SF and equip it with a neural network that\npredicts context-specific initialization. The initialization network is trained\nin a self-supervised manner, taking advantage of the differentiability of the\nSF solver. We advance the state-of-the-art in the following respects. First, we\nshow that we can generate trajectories of tens of robots in cluttered\nenvironments in a few tens of milliseconds. This is several times faster than\nexisting centralized optimization approaches. Moreover, our approach also\ngenerates smoother trajectories orders of magnitude faster than competing\nbaselines based on diffusion models. Second, each component of our approach can\nbe batched, allowing us to solve a few tens of problem instances in a fraction\nof a second. We believe this is a first such result; no existing approach\nprovides such capabilities. Finally, our approach can generate a diverse set of\ntrajectories between a given set of start and goal locations, which can capture\ndifferent collision-avoidance behaviors.",
            "headline_zh": "提出Flow-Opt以提升多机器人集中式轨迹优化的计算效率",
            "intro_zh": [
                "核心问题：集中式多机器人轨迹优化在复杂环境中计算不可行",
                "方法要点：结合流匹配生成模型与可微安全过滤器进行快速推理",
                "实验效果：在数十机器人场景中实现毫秒级轨迹生成，速度快于现有方法"
            ],
            "tags_zh": [
                "多机器人轨迹优化",
                "流匹配",
                "可微优化",
                "安全过滤器",
                "扩散变换器"
            ],
            "_index": 59
        },
        {
            "title": "Cattle-CLIP: A Multimodal Framework for Cattle Behaviour Recognition",
            "authors": [
                "Huimin Liu",
                "Jing Gao",
                "Daria Baran",
                "AxelX Montout",
                "Neill W Campbell",
                "Andrew W Dowsey"
            ],
            "arxiv_id": "2510.09203v1",
            "summary": "Cattle behaviour is a crucial indicator of an individual animal health,\nproductivity and overall well-being. Video-based monitoring, combined with deep\nlearning techniques, has become a mainstream approach in animal biometrics, and\nit can offer high accuracy in some behaviour recognition tasks. We present\nCattle-CLIP, a multimodal deep learning framework for cattle behaviour\nrecognition, using semantic cues to improve the performance of video-based\nvisual feature recognition. It is adapted from the large-scale image-language\nmodel CLIP by adding a temporal integration module. To address the domain gap\nbetween web data used for the pre-trained model and real-world cattle\nsurveillance footage, we introduce tailored data augmentation strategies and\nspecialised text prompts. Cattle-CLIP is evaluated under both fully-supervised\nand few-shot learning scenarios, with a particular focus on data-scarce\nbehaviour recognition - an important yet under-explored goal in livestock\nmonitoring. To evaluate the proposed method, we release the CattleBehaviours6\ndataset, which comprises six types of indoor behaviours: feeding, drinking,\nstanding-self-grooming, standing-ruminating, lying-self-grooming and\nlying-ruminating. The dataset consists of 1905 clips collected from our John\nOldacre Centre dairy farm research platform housing 200 Holstein-Friesian cows.\nExperiments show that Cattle-CLIP achieves 96.1% overall accuracy across six\nbehaviours in a supervised setting, with nearly 100% recall for feeding,\ndrinking and standing-ruminating behaviours, and demonstrates robust\ngeneralisation with limited data in few-shot scenarios, highlighting the\npotential of multimodal learning in agricultural and animal behaviour analysis.",
            "headline_zh": "提出Cattle-CLIP多模态框架，利用语义提示提升牛只行为识别性能。",
            "intro_zh": [
                "核心问题：牛只行为识别对健康监测至关重要，但数据稀缺场景下性能受限。",
                "方法要点：基于CLIP模型，添加时间集成模块，并采用数据增强和专用文本提示。",
                "实验或效果：在CattleBehaviours6数据集上，监督学习准确率达96.1%，少样本学习泛化性强。"
            ],
            "tags_zh": [
                "多模态学习",
                "牛只行为识别",
                "CLIP模型",
                "少样本学习",
                "数据增强",
                "时间集成"
            ],
            "_index": 60
        },
        {
            "title": "Towards Safer and Understandable Driver Intention Prediction",
            "authors": [
                "Mukilan Karuppasamy",
                "Shankar Gangisetty",
                "Shyam Nandan Rai",
                "Carlo Masone",
                "C V Jawahar"
            ],
            "arxiv_id": "2510.09200v1",
            "summary": "Autonomous driving (AD) systems are becoming increasingly capable of handling\ncomplex tasks, mainly due to recent advances in deep learning and AI. As\ninteractions between autonomous systems and humans increase, the\ninterpretability of decision-making processes in driving systems becomes\nincreasingly crucial for ensuring safe driving operations. Successful\nhuman-machine interaction requires understanding the underlying representations\nof the environment and the driving task, which remains a significant challenge\nin deep learning-based systems. To address this, we introduce the task of\ninterpretability in maneuver prediction before they occur for driver safety,\ni.e., driver intent prediction (DIP), which plays a critical role in AD\nsystems. To foster research in interpretable DIP, we curate the eXplainable\nDriving Action Anticipation Dataset (DAAD-X), a new multimodal, ego-centric\nvideo dataset to provide hierarchical, high-level textual explanations as\ncausal reasoning for the driver's decisions. These explanations are derived\nfrom both the driver's eye-gaze and the ego-vehicle's perspective. Next, we\npropose Video Concept Bottleneck Model (VCBM), a framework that generates\nspatio-temporally coherent explanations inherently, without relying on post-hoc\ntechniques. Finally, through extensive evaluations of the proposed VCBM on the\nDAAD-X dataset, we demonstrate that transformer-based models exhibit greater\ninterpretability than conventional CNN-based models. Additionally, we introduce\na multilabel t-SNE visualization technique to illustrate the disentanglement\nand causal correlation among multiple explanations. Our data, code and models\nare available at: https://mukil07.github.io/VCBM.github.io/",
            "headline_zh": "提出视频概念瓶颈模型以增强自动驾驶中驾驶员意图预测的可解释性",
            "intro_zh": [
                "核心问题：自动驾驶系统决策过程缺乏可解释性，影响人机交互安全。",
                "方法要点：引入DAAD-X数据集和VCBM框架，生成时空一致的解释。",
                "实验或效果：评估显示基于Transformer的模型比CNN更具可解释性。"
            ],
            "tags_zh": [
                "驾驶员意图预测",
                "可解释人工智能",
                "多模态数据集",
                "视频理解",
                "Transformer模型"
            ],
            "_index": 61
        },
        {
            "title": "Decentralized Multi-Robot Relative Navigation in Unknown, Structurally Constrained Environments under Limited Communication",
            "authors": [
                "Zihao Mao",
                "Yunheng Wang",
                "Yunting Ji",
                "Yi Yang",
                "Wenjie Song"
            ],
            "arxiv_id": "2510.09188v1",
            "summary": "Multi-robot navigation in unknown, structurally constrained, and GPS-denied\nenvironments presents a fundamental trade-off between global strategic\nforesight and local tactical agility, particularly under limited communication.\nCentralized methods achieve global optimality but suffer from high\ncommunication overhead, while distributed methods are efficient but lack the\nbroader awareness to avoid deadlocks and topological traps. To address this, we\npropose a fully decentralized, hierarchical relative navigation framework that\nachieves both strategic foresight and tactical agility without a unified\ncoordinate system. At the strategic layer, robots build and exchange\nlightweight topological maps upon opportunistic encounters. This process\nfosters an emergent global awareness, enabling the planning of efficient,\ntrap-avoiding routes at an abstract level. This high-level plan then inspires\nthe tactical layer, which operates on local metric information. Here, a\nsampling-based escape point strategy resolves dense spatio-temporal conflicts\nby generating dynamically feasible trajectories in real time, concurrently\nsatisfying tight environmental and kinodynamic constraints. Extensive\nsimulations and real-world experiments demonstrate that our system\nsignificantly outperforms in success rate and efficiency, especially in\ncommunication-limited environments with complex topological structures.",
            "headline_zh": "提出去中心化分层相对导航框架，解决未知结构约束环境中多机器人导航问题。",
            "intro_zh": [
                "核心问题：未知GPS缺失环境中多机器人导航，需平衡全局策略与局部敏捷性，通信受限。",
                "方法要点：分层框架，战略层交换拓扑地图，战术层实时生成可行轨迹。",
                "实验效果：仿真与真实实验显示，在通信受限复杂环境中成功率和效率显著提升。"
            ],
            "tags_zh": [
                "多机器人导航",
                "去中心化系统",
                "相对导航",
                "拓扑地图",
                "实时轨迹规划",
                "通信受限环境"
            ],
            "_index": 62
        },
        {
            "title": "Modern Deep Learning Approaches for Cricket Shot Classification: A Comprehensive Baseline Study",
            "authors": [
                "Sungwoo Kang"
            ],
            "arxiv_id": "2510.09187v1",
            "summary": "Cricket shot classification from video sequences remains a challenging\nproblem in sports video analysis, requiring effective modeling of both spatial\nand temporal features. This paper presents the first comprehensive baseline\nstudy comparing seven different deep learning approaches across four distinct\nresearch paradigms for cricket shot classification. We implement and\nsystematically evaluate traditional CNN-LSTM architectures, attention-based\nmodels, vision transformers, transfer learning approaches, and modern\nEfficientNet-GRU combinations on a unified benchmark. A critical finding of our\nstudy is the significant performance gap between claims in academic literature\nand practical implementation results. While previous papers reported accuracies\nof 96\\% (Balaji LRCN), 99.2\\% (IJERCSE), and 93\\% (Sensors), our standardized\nre-implementations achieve 46.0\\%, 55.6\\%, and 57.7\\% respectively. Our modern\nSOTA approach, combining EfficientNet-B0 with a GRU-based temporal model,\nachieves 92.25\\% accuracy, demonstrating that substantial improvements are\npossible with modern architectures and systematic optimization. All\nimplementations follow modern MLOps practices with PyTorch Lightning, providing\na reproducible research platform that exposes the critical importance of\nstandardized evaluation protocols in sports video analysis research.",
            "headline_zh": "提出现代深度学习基线研究，系统比较七种方法以解决板球击球分类问题。",
            "intro_zh": [
                "核心问题：板球击球视频分类需有效建模时空特征，现有文献与实际性能存在显著差距。",
                "方法要点：比较CNN-LSTM、注意力模型、视觉Transformer、迁移学习及EfficientNet-GRU组合。",
                "实验或效果：现代SOTA方法达92.25%准确率，强调标准化评估协议的重要性。"
            ],
            "tags_zh": [
                "板球击球分类",
                "深度学习基线",
                "时空特征建模",
                "标准化评估",
                "视频序列分析",
                "MLOps实践"
            ],
            "_index": 63
        },
        {
            "title": "Online Video Depth Anything: Temporally-Consistent Depth Prediction with Low Memory Consumption",
            "authors": [
                "Johann-Friedrich Feiden",
                "Tim Küchler",
                "Denis Zavadski",
                "Bogdan Savchynskyy",
                "Carsten Rother"
            ],
            "arxiv_id": "2510.09182v1",
            "summary": "Depth estimation from monocular video has become a key component of many\nreal-world computer vision systems. Recently, Video Depth Anything (VDA) has\ndemonstrated strong performance on long video sequences. However, it relies on\nbatch-processing which prohibits its use in an online setting. In this work, we\novercome this limitation and introduce online VDA (oVDA). The key innovation is\nto employ techniques from Large Language Models (LLMs), namely, caching latent\nfeatures during inference and masking frames at training. Our oVDA method\noutperforms all competing online video depth estimation methods in both\naccuracy and VRAM usage. Low VRAM usage is particularly important for\ndeployment on edge devices. We demonstrate that oVDA runs at 42 FPS on an\nNVIDIA A100 and at 20 FPS on an NVIDIA Jetson edge device. We will release\nboth, code and compilation scripts, making oVDA easy to deploy on low-power\nhardware.",
            "headline_zh": "提出在线视频深度预测方法oVDA，实现低内存消耗和时序一致性。",
            "intro_zh": [
                "核心问题：现有视频深度预测方法依赖批处理，无法在线实时应用。",
                "方法要点：借鉴LLM技术，缓存潜在特征并训练时掩码帧。",
                "实验效果：在精度和VRAM使用上优于其他在线方法，边缘设备可达20 FPS。"
            ],
            "tags_zh": [
                "在线视频深度估计",
                "时序一致性",
                "低内存消耗",
                "边缘设备部署",
                "潜在特征缓存"
            ],
            "_index": 64
        },
        {
            "title": "TARO: Toward Semantically Rich Open-World Object Detection",
            "authors": [
                "Yuchen Zhang",
                "Yao Lu",
                "Johannes Betz"
            ],
            "arxiv_id": "2510.09173v1",
            "summary": "Modern object detectors are largely confined to a \"closed-world\" assumption,\nlimiting them to a predefined set of classes and posing risks when encountering\nnovel objects in real-world scenarios. While open-set detection methods aim to\naddress this by identifying such instances as 'Unknown', this is often\ninsufficient. Rather than treating all unknowns as a single class, assigning\nthem more descriptive subcategories can enhance decision-making in\nsafety-critical contexts. For example, identifying an object as an 'Unknown\nAnimal' (requiring an urgent stop) versus 'Unknown Debris' (requiring a safe\nlane change) is far more useful than just 'Unknown' in autonomous driving. To\nbridge this gap, we introduce TARO, a novel detection framework that not only\nidentifies unknown objects but also classifies them into coarse parent\ncategories within a semantic hierarchy. TARO employs a unique architecture with\na sparsemax-based head for modeling objectness, a hierarchy-guided relabeling\ncomponent that provides auxiliary supervision, and a classification module that\nlearns hierarchical relationships. Experiments show TARO can categorize up to\n29.9% of unknowns into meaningful coarse classes, significantly reduce\nconfusion between unknown and known classes, and achieve competitive\nperformance in both unknown recall and known mAP. Code will be made available.",
            "headline_zh": "提出TARO框架以解决开放世界物体检测中未知对象语义分类不足的问题",
            "intro_zh": [
                "核心问题：现有检测器在封闭世界假设下，无法对未知对象进行语义丰富的分类",
                "方法要点：采用稀疏最大头建模物体性，结合层次引导重标签和分类模块学习语义层次",
                "实验或效果：TARO可将29.9%未知对象分类为粗类别，减少未知与已知类混淆"
            ],
            "tags_zh": [
                "开放世界物体检测",
                "语义层次分类",
                "稀疏最大头",
                "未知对象识别",
                "自动驾驶安全"
            ],
            "_index": 65
        },
        {
            "title": "Instance-Level Generation for Representation Learning",
            "authors": [
                "Yankun Wu",
                "Zakaria Laskar",
                "Giorgos Kordopatis-Zilos",
                "Noa Garcia",
                "Giorgos Tolias"
            ],
            "arxiv_id": "2510.09171v1",
            "summary": "Instance-level recognition (ILR) focuses on identifying individual objects\nrather than broad categories, offering the highest granularity in image\nclassification. However, this fine-grained nature makes creating large-scale\nannotated datasets challenging, limiting ILR's real-world applicability across\ndomains. To overcome this, we introduce a novel approach that synthetically\ngenerates diverse object instances from multiple domains under varied\nconditions and backgrounds, forming a large-scale training set. Unlike prior\nwork on automatic data synthesis, our method is the first to address\nILR-specific challenges without relying on any real images. Fine-tuning\nfoundation vision models on the generated data significantly improves retrieval\nperformance across seven ILR benchmarks spanning multiple domains. Our approach\noffers a new, efficient, and effective alternative to extensive data collection\nand curation, introducing a new ILR paradigm where the only input is the names\nof the target domains, unlocking a wide range of real-world applications.",
            "headline_zh": "提出实例级生成方法以解决细粒度识别数据稀缺问题",
            "intro_zh": [
                "实例级识别因细粒度特性导致大规模标注数据获取困难",
                "方法从多领域合成多样实例，不依赖真实图像进行训练",
                "在七个基准测试中显著提升检索性能，解锁广泛应用"
            ],
            "tags_zh": [
                "实例级识别",
                "数据合成",
                "表示学习",
                "图像检索",
                "多领域应用"
            ],
            "_index": 66
        },
        {
            "title": "Online Topological Localization for Navigation Assistance in Bronchoscopy",
            "authors": [
                "Clara Tomasini",
                "Luis Riazuelo",
                "Ana C. Murillo"
            ],
            "arxiv_id": "2510.09144v1",
            "summary": "Video bronchoscopy is a fundamental procedure in respiratory medicine, where\nmedical experts navigate through the bronchial tree of a patient to diagnose or\noperate the patient. Surgeons need to determine the position of the scope as\nthey go through the airway until they reach the area of interest. This task is\nvery challenging for practitioners due to the complex bronchial tree structure\nand varying doctor experience and training. Navigation assistance to locate the\nbronchoscope during the procedure can improve its outcome. Currently used\ntechniques for navigational guidance commonly rely on previous CT scans of the\npatient to obtain a 3D model of the airway, followed by tracking of the scope\nwith additional sensors or image registration. These methods obtain accurate\nlocations but imply additional setup, scans and training. Accurate metric\nlocalization is not always required, and a topological localization with regard\nto a generic airway model can often suffice to assist the surgeon with\nnavigation. We present an image-based bronchoscopy topological localization\npipeline to provide navigation assistance during the procedure, with no need of\npatient CT scan. Our approach is trained only on phantom data, eliminating the\nhigh cost of real data labeling, and presents good generalization capabilities.\nThe results obtained surpass existing methods, particularly on real data test\nsequences.",
            "headline_zh": "提出基于图像的支气管镜拓扑定位方法，用于导航辅助，无需患者CT扫描。",
            "intro_zh": [
                "核心问题：支气管镜导航中，医生难以在复杂气道中定位，现有方法依赖CT扫描和额外传感器。",
                "方法要点：仅使用幻影数据训练，实现图像到通用气道模型的拓扑定位，无需患者特定数据。",
                "实验或效果：在真实数据测试序列上表现优于现有方法，具有良好的泛化能力。"
            ],
            "tags_zh": [
                "支气管镜导航",
                "拓扑定位",
                "图像处理",
                "医学影像分析",
                "无监督学习",
                "泛化能力"
            ],
            "_index": 67
        },
        {
            "title": "Training Feature Attribution for Vision Models",
            "authors": [
                "Aziz Bacha",
                "Thomas George"
            ],
            "arxiv_id": "2510.09135v1",
            "summary": "Deep neural networks are often considered opaque systems, prompting the need\nfor explainability methods to improve trust and accountability. Existing\napproaches typically attribute test-time predictions either to input features\n(e.g., pixels in an image) or to influential training examples. We argue that\nboth perspectives should be studied jointly. This work explores *training\nfeature attribution*, which links test predictions to specific regions of\nspecific training images and thereby provides new insights into the inner\nworkings of deep models. Our experiments on vision datasets show that training\nfeature attribution yields fine-grained, test-specific explanations: it\nidentifies harmful examples that drive misclassifications and reveals spurious\ncorrelations, such as patch-based shortcuts, that conventional attribution\nmethods fail to expose.",
            "headline_zh": "提出训练特征归因方法，联合分析测试预测与训练图像区域，提升视觉模型可解释性。",
            "intro_zh": [
                "核心问题：深度神经网络被视为黑盒，需可解释性方法增强信任与问责。",
                "方法要点：联合归因测试预测到特定训练图像区域，提供细粒度解释。",
                "实验或效果：在视觉数据集上识别有害样本和虚假相关性，优于传统方法。"
            ],
            "tags_zh": [
                "训练特征归因",
                "视觉模型可解释性",
                "深度神经网络",
                "测试预测归因",
                "虚假相关性检测"
            ],
            "_index": 68
        },
        {
            "title": "Polar Separable Transform for Efficient Orthogonal Rotation-Invariant Image Representation",
            "authors": [
                "Satya P. Singh",
                "Rashmi Chaudhry",
                "Anand Srivastava",
                "Jagath C. Rajapakse"
            ],
            "arxiv_id": "2510.09125v1",
            "summary": "Orthogonal moment-based image representations are fundamental in computer\nvision, but classical methods suffer from high computational complexity and\nnumerical instability at large orders. Zernike and pseudo-Zernike moments, for\ninstance, require coupled radial-angular processing that precludes efficient\nfactorization, resulting in $\\mathcal{O}(n^3N^2)$ to $\\mathcal{O}(n^6N^2)$\ncomplexity and $\\mathcal{O}(N^4)$ condition number scaling for the $n$th-order\nmoments on an $N\\times N$ image. We introduce \\textbf{PSepT} (Polar Separable\nTransform), a separable orthogonal transform that overcomes the\nnon-separability barrier in polar coordinates. PSepT achieves complete kernel\nfactorization via tensor-product construction of Discrete Cosine Transform\n(DCT) radial bases and Fourier harmonic angular bases, enabling independent\nradial and angular processing. This separable design reduces computational\ncomplexity to $\\mathcal{O}(N^2 \\log N)$, memory requirements to\n$\\mathcal{O}(N^2)$, and condition number scaling to $\\mathcal{O}(\\sqrt{N})$,\nrepresenting exponential improvements over polynomial approaches. PSepT\nexhibits orthogonality, completeness, energy conservation, and\nrotation-covariance properties. Experimental results demonstrate better\nnumerical stability, computational efficiency, and competitive classification\nperformance on structured datasets, while preserving exact reconstruction. The\nseparable framework enables high-order moment analysis previously infeasible\nwith classical methods, opening new possibilities for robust image analysis\napplications.",
            "headline_zh": "提出PSepT变换以高效实现正交旋转不变图像表示",
            "intro_zh": [
                "经典正交矩方法计算复杂度高且数值不稳定，阻碍高效图像分析",
                "PSepT通过可分离径向-角向基实现独立处理，大幅降低复杂度与内存需求",
                "实验显示PSepT在数值稳定性和分类性能上优于传统方法，支持精确重建"
            ],
            "tags_zh": [
                "正交图像表示",
                "可分离变换",
                "旋转不变性",
                "计算效率",
                "数值稳定性"
            ],
            "_index": 69
        },
        {
            "title": "MSDM: Generating Task-Specific Pathology Images with a Multimodal Conditioned Diffusion Model for Cell and Nuclei Segmentation",
            "authors": [
                "Dominik Winter",
                "Mai Bui",
                "Monica Azqueta Gavaldon",
                "Nicolas Triltsch",
                "Marco Rosati",
                "Nicolas Brieu"
            ],
            "arxiv_id": "2510.09121v1",
            "summary": "Scarcity of annotated data, particularly for rare or atypical morphologies,\npresent significant challenges for cell and nuclei segmentation in\ncomputational pathology. While manual annotation is labor-intensive and costly,\nsynthetic data offers a cost-effective alternative. We introduce a Multimodal\nSemantic Diffusion Model (MSDM) for generating realistic pixel-precise\nimage-mask pairs for cell and nuclei segmentation. By conditioning the\ngenerative process with cellular/nuclear morphologies (using horizontal and\nvertical maps), RGB color characteristics, and BERT-encoded assay/indication\nmetadata, MSDM generates datasests with desired morphological properties. These\nheterogeneous modalities are integrated via multi-head cross-attention,\nenabling fine-grained control over the generated images. Quantitative analysis\ndemonstrates that synthetic images closely match real data, with low\nWasserstein distances between embeddings of generated and real images under\nmatching biological conditions. The incorporation of these synthetic samples,\nexemplified by columnar cells, significantly improves segmentation model\naccuracy on columnar cells. This strategy systematically enriches data sets,\ndirectly targeting model deficiencies. We highlight the effectiveness of\nmultimodal diffusion-based augmentation for advancing the robustness and\ngeneralizability of cell and nuclei segmentation models. Thereby, we pave the\nway for broader application of generative models in computational pathology.",
            "headline_zh": "提出多模态语义扩散模型以生成病理图像-掩码对，提升细胞和核分割性能",
            "intro_zh": [
                "细胞和核分割面临标注数据稀缺问题，尤其对罕见形态",
                "模型融合形态、颜色和元数据，通过多模态条件扩散生成图像-掩码对",
                "合成数据显著改善分割模型准确性，如柱状细胞，并验证与真实数据相似性"
            ],
            "tags_zh": [
                "计算病理学",
                "扩散模型",
                "图像生成",
                "细胞分割",
                "多模态学习",
                "数据增强"
            ],
            "_index": 70
        },
        {
            "title": "SOS: Synthetic Object Segments Improve Detection, Segmentation, and Grounding",
            "authors": [
                "Weikai Huang",
                "Jieyu Zhang",
                "Taoyang Jia",
                "Chenhao Zheng",
                "Ziqi Gao",
                "Jae Sung Park",
                "Ranjay Krishna"
            ],
            "arxiv_id": "2510.09110v1",
            "summary": "Visual grouping -- operationalized via instance segmentation, visual\ngrounding, and object detection -- underpins applications from robotic\nperception to photo editing. Large annotated datasets are costly, biased in\ncoverage, and hard to scale. Synthetic data are promising but often lack\nflexibility, accuracy, and compositional diversity.\n  We present SOS, a simple and scalable data synthesis pipeline based on an\nobject-centric composition strategy. It pastes high-quality synthetic object\nsegments into new images using structured layout priors and generative\nrelighting, producing accurate and diverse masks, boxes, and referring\nexpressions. Models trained on 100000 synthetic images from SOS outperform\nthose trained on larger real-image datasets such as GRIT (20M) and V3Det (200K)\non detection and grounding tasks, achieving +10.9 AP on LVIS detection and +8.4\n$N_{\\text{Acc}}$ on gRefCOCO grounding. SOS enables controllable dataset\nconstruction and improves generalization in both low-data and closed-vocabulary\nsettings. Augmenting LVIS and COCO with synthetic object segments yields strong\nperformance across real-data scales and even larger gains under extremely\nlimited real data (for example, +3.83 $AP_{\\text{rare}}$ on LVIS instance\nsegmentation and +6.59 AP with a 1 percent COCO setup). This controllability\nalso supports targeted data generation for challenging intra-class referring in\nvisual grounding.",
            "headline_zh": "提出SOS合成对象分割数据管道，提升检测、分割和视觉定位性能",
            "intro_zh": [
                "核心问题：真实标注数据成本高、偏差大，合成数据缺乏多样性和准确性。",
                "方法要点：基于对象中心合成策略，结合布局先验和生成重光照，生成高质量合成数据。",
                "实验或效果：在检测和视觉定位任务中，优于大型真实数据集，并增强低数据场景泛化能力。"
            ],
            "tags_zh": [
                "合成数据生成",
                "对象检测",
                "实例分割",
                "视觉定位",
                "数据增强",
                "泛化性能"
            ],
            "_index": 71
        },
        {
            "title": "A Novel Multi-branch ConvNeXt Architecture for Identifying Subtle Pathological Features in CT Scans",
            "authors": [
                "Irash Perera",
                "Uthayasanker Thayasivam"
            ],
            "arxiv_id": "2510.09107v1",
            "summary": "Intelligent analysis of medical imaging plays a crucial role in assisting\nclinical diagnosis, especially for identifying subtle pathological features.\nThis paper introduces a novel multi-branch ConvNeXt architecture designed\nspecifically for the nuanced challenges of medical image analysis. While\napplied here to the specific problem of COVID-19 diagnosis, the methodology\noffers a generalizable framework for classifying a wide range of pathologies\nfrom CT scans. The proposed model incorporates a rigorous end-to-end pipeline,\nfrom meticulous data preprocessing and augmentation to a disciplined two-phase\ntraining strategy that leverages transfer learning effectively. The\narchitecture uniquely integrates features extracted from three parallel\nbranches: Global Average Pooling, Global Max Pooling, and a new\nAttention-weighted Pooling mechanism. The model was trained and validated on a\ncombined dataset of 2,609 CT slices derived from two distinct datasets.\nExperimental results demonstrate a superior performance on the validation set,\nachieving a final ROC-AUC of 0.9937, a validation accuracy of 0.9757, and an\nF1-score of 0.9825 for COVID-19 cases, outperforming all previously reported\nmodels on this dataset. These findings indicate that a modern, multi-branch\narchitecture, coupled with careful data handling, can achieve performance\ncomparable to or exceeding contemporary state-of-the-art models, thereby\nproving the efficacy of advanced deep learning techniques for robust medical\ndiagnostics.",
            "headline_zh": "提出多分支ConvNeXt架构以识别CT扫描中的细微病理特征",
            "intro_zh": [
                "核心问题：医学图像中细微病理特征的识别，应用于COVID-19诊断等场景。",
                "方法要点：集成全局平均池化、全局最大池化和注意力加权池化的多分支架构。",
                "实验或效果：在验证集上ROC-AUC达0.9937，准确率0.9757，优于现有模型。"
            ],
            "tags_zh": [
                "多分支架构",
                "ConvNeXt",
                "CT扫描分析",
                "医学图像分类",
                "注意力机制",
                "COVID-19诊断"
            ],
            "_index": 72
        },
        {
            "title": "When a Robot is More Capable than a Human: Learning from Constrained Demonstrators",
            "authors": [
                "Xinhu Li",
                "Ayush Jain",
                "Zhaojing Yang",
                "Yigit Korkmaz",
                "Erdem Bıyık"
            ],
            "arxiv_id": "2510.09096v1",
            "summary": "Learning from demonstrations enables experts to teach robots complex tasks\nusing interfaces such as kinesthetic teaching, joystick control, and\nsim-to-real transfer. However, these interfaces often constrain the expert's\nability to demonstrate optimal behavior due to indirect control, setup\nrestrictions, and hardware safety. For example, a joystick can move a robotic\narm only in a 2D plane, even though the robot operates in a higher-dimensional\nspace. As a result, the demonstrations collected by constrained experts lead to\nsuboptimal performance of the learned policies. This raises a key question: Can\na robot learn a better policy than the one demonstrated by a constrained\nexpert? We address this by allowing the agent to go beyond direct imitation of\nexpert actions and explore shorter and more efficient trajectories. We use the\ndemonstrations to infer a state-only reward signal that measures task progress,\nand self-label reward for unknown states using temporal interpolation. Our\napproach outperforms common imitation learning in both sample efficiency and\ntask completion time. On a real WidowX robotic arm, it completes the task in 12\nseconds, 10x faster than behavioral cloning, as shown in real-robot videos on\nhttps://sites.google.com/view/constrainedexpert .",
            "headline_zh": "提出从受限专家演示中学习更优策略的方法，通过状态奖励推断和探索提升机器人性能。",
            "intro_zh": [
                "核心问题：专家演示因控制受限导致策略次优，机器人能否学习优于演示的策略。",
                "方法要点：推断状态奖励信号，通过时间插值自标注奖励，探索更高效轨迹。",
                "实验效果：在真实机器人上任务完成时间比行为克隆快10倍，样本效率更高。"
            ],
            "tags_zh": [
                "模仿学习",
                "机器人学习",
                "奖励推断",
                "状态奖励",
                "轨迹优化",
                "受限演示"
            ],
            "_index": 73
        },
        {
            "title": "Dense2MoE: Restructuring Diffusion Transformer to MoE for Efficient Text-to-Image Generation",
            "authors": [
                "Youwei Zheng",
                "Yuxi Ren",
                "Xin Xia",
                "Xuefeng Xiao",
                "Xiaohua Xie"
            ],
            "arxiv_id": "2510.09094v1",
            "summary": "Diffusion Transformer (DiT) has demonstrated remarkable performance in\ntext-to-image generation; however, its large parameter size results in\nsubstantial inference overhead. Existing parameter compression methods\nprimarily focus on pruning, but aggressive pruning often leads to severe\nperformance degradation due to reduced model capacity. To address this\nlimitation, we pioneer the transformation of a dense DiT into a Mixture of\nExperts (MoE) for structured sparsification, reducing the number of activated\nparameters while preserving model capacity. Specifically, we replace the\nFeed-Forward Networks (FFNs) in DiT Blocks with MoE layers, reducing the number\nof activated parameters in the FFNs by 62.5\\%. Furthermore, we propose the\nMixture of Blocks (MoB) to selectively activate DiT blocks, thereby further\nenhancing sparsity. To ensure an effective dense-to-MoE conversion, we design a\nmulti-step distillation pipeline, incorporating Taylor metric-based expert\ninitialization, knowledge distillation with load balancing, and group feature\nloss for MoB optimization. We transform large diffusion transformers (e.g.,\nFLUX.1 [dev]) into an MoE structure, reducing activated parameters by 60\\%\nwhile maintaining original performance and surpassing pruning-based approaches\nin extensive experiments. Overall, Dense2MoE establishes a new paradigm for\nefficient text-to-image generation.",
            "headline_zh": "提出Dense2MoE将扩散Transformer转换为MoE结构以高效文本到图像生成",
            "intro_zh": [
                "核心问题：扩散Transformer参数大导致推理开销高，现有剪枝方法易致性能下降",
                "方法要点：用MoE层替换FFN，引入MoB选择性激活块，多步蒸馏优化转换",
                "实验或效果：激活参数减少60%，保持原性能，优于剪枝方法"
            ],
            "tags_zh": [
                "文本到图像生成",
                "扩散Transformer",
                "专家混合",
                "模型压缩",
                "知识蒸馏"
            ],
            "_index": 74
        },
        {
            "title": "GL-DT: Multi-UAV Detection and Tracking with Global-Local Integration",
            "authors": [
                "Juanqin Liu",
                "Leonardo Plotegher",
                "Eloy Roura",
                "Shaoming He"
            ],
            "arxiv_id": "2510.09092v1",
            "summary": "The extensive application of unmanned aerial vehicles (UAVs) in military\nreconnaissance, environmental monitoring, and related domains has created an\nurgent need for accurate and efficient multi-object tracking (MOT)\ntechnologies, which are also essential for UAV situational awareness. However,\ncomplex backgrounds, small-scale targets, and frequent occlusions and\ninteractions continue to challenge existing methods in terms of detection\naccuracy and trajectory continuity. To address these issues, this paper\nproposes the Global-Local Detection and Tracking (GL-DT) framework. It employs\na Spatio-Temporal Feature Fusion (STFF) module to jointly model motion and\nappearance features, combined with a global-local collaborative detection\nstrategy, effectively enhancing small-target detection. Building upon this, the\nJPTrack tracking algorithm is introduced to mitigate common issues such as ID\nswitches and trajectory fragmentation. Experimental results demonstrate that\nthe proposed approach significantly improves the continuity and stability of\nMOT while maintaining real-time performance, providing strong support for the\nadvancement of UAV detection and tracking technologies.",
            "headline_zh": "提出GL-DT框架以解决无人机多目标检测与跟踪中的小目标和轨迹中断问题",
            "intro_zh": [
                "核心问题：复杂背景、小目标和频繁遮挡导致检测精度低和轨迹不连续",
                "方法要点：采用时空特征融合和全局-局部协作检测，结合JPTrack算法减少ID切换",
                "实验或效果：显著提升跟踪连续性和稳定性，同时保持实时性能"
            ],
            "tags_zh": [
                "多目标跟踪",
                "无人机检测",
                "时空特征融合",
                "小目标检测",
                "轨迹连续性"
            ],
            "_index": 75
        },
        {
            "title": "Robust Visual Teach-and-Repeat Navigation with Flexible Topo-metric Graph Map Representation",
            "authors": [
                "Jikai Wang",
                "Yunqi Cheng",
                "Kezhi Wang",
                "Zonghai Chen"
            ],
            "arxiv_id": "2510.09089v1",
            "summary": "Visual Teach-and-Repeat Navigation is a direct solution for mobile robot to\nbe deployed in unknown environments. However, robust trajectory repeat\nnavigation still remains challenged due to environmental changing and dynamic\nobjects. In this paper, we propose a novel visual teach-and-repeat navigation\nsystem, which consists of a flexible map representation, robust map matching\nand a map-less local navigation module. During the teaching process, the\nrecorded keyframes are formulated as a topo-metric graph and each node can be\nfurther extended to save new observations. Such representation also alleviates\nthe requirement of globally consistent mapping. To enhance the place\nrecognition performance during repeating process, instead of using\nframe-to-frame matching, we firstly implement keyframe clustering to aggregate\nsimilar connected keyframes into local map and perform place recognition based\non visual frame-tolocal map matching strategy. To promote the local goal\npersistent tracking performance, a long-term goal management algorithm is\nconstructed, which can avoid the robot getting lost due to environmental\nchanges or obstacle occlusion. To achieve the goal without map, a local\ntrajectory-control candidate optimization algorithm is proposed. Extensively\nexperiments are conducted on our mobile platform. The results demonstrate that\nour system is superior to the baselines in terms of robustness and\neffectiveness.",
            "headline_zh": "提出灵活拓扑-度量图表示以增强视觉教导-重复导航的鲁棒性",
            "intro_zh": [
                "核心问题：环境变化和动态物体导致视觉教导-重复导航轨迹重复不鲁棒。",
                "方法要点：使用拓扑-度量图表示、关键帧聚类和局部地图匹配提升地点识别。",
                "实验或效果：在移动平台上实验，系统在鲁棒性和有效性上优于基线方法。"
            ],
            "tags_zh": [
                "视觉教导-重复导航",
                "拓扑-度量图表示",
                "地点识别",
                "局部导航",
                "鲁棒性优化"
            ],
            "_index": 76
        },
        {
            "title": "MambaH-Fit: Rethinking Hyper-surface Fitting-based Point Cloud Normal Estimation via State Space Modelling",
            "authors": [
                "Weijia Wang",
                "Yuanzhi Su",
                "Pei-Gen Ye",
                "Yuan-Gen Wang",
                "Xuequan Lu"
            ],
            "arxiv_id": "2510.09088v1",
            "summary": "We present MambaH-Fit, a state space modelling framework tailored for\nhyper-surface fitting-based point cloud normal estimation. Existing normal\nestimation methods often fall short in modelling fine-grained geometric\nstructures, thereby limiting the accuracy of the predicted normals. Recently,\nstate space models (SSMs), particularly Mamba, have demonstrated strong\nmodelling capability by capturing long-range dependencies with linear\ncomplexity and inspired adaptations to point cloud processing. However,\nexisting Mamba-based approaches primarily focus on understanding global shape\nstructures, leaving the modelling of local, fine-grained geometric details\nlargely under-explored. To address the issues above, we first introduce an\nAttention-driven Hierarchical Feature Fusion (AHFF) scheme to adaptively fuse\nmulti-scale point cloud patch features, significantly enhancing geometric\ncontext learning in local point cloud neighbourhoods. Building upon this, we\nfurther propose Patch-wise State Space Model (PSSM) that models point cloud\npatches as implicit hyper-surfaces via state dynamics, enabling effective\nfine-grained geometric understanding for normal prediction. Extensive\nexperiments on benchmark datasets show that our method outperforms existing\nones in terms of accuracy, robustness, and flexibility. Ablation studies\nfurther validate the contribution of the proposed components.",
            "headline_zh": "提出MambaH-Fit框架，通过状态空间建模改进点云法向量估计的精细几何结构建模。",
            "intro_zh": [
                "现有方法在建模点云精细几何结构方面不足，影响法向量估计精度。",
                "引入注意力驱动层次特征融合和补丁状态空间模型，增强局部几何上下文学习。",
                "在基准数据集上实验显示，方法在准确性、鲁棒性和灵活性方面优于现有方法。"
            ],
            "tags_zh": [
                "点云法向量估计",
                "状态空间模型",
                "超曲面拟合",
                "注意力机制",
                "多尺度特征融合"
            ],
            "_index": 77
        },
        {
            "title": "Training Models to Detect Successive Robot Errors from Human Reactions",
            "authors": [
                "Shannon Liu",
                "Maria Teresa Parreira",
                "Wendy Ju"
            ],
            "arxiv_id": "2510.09080v1",
            "summary": "As robots become more integrated into society, detecting robot errors is\nessential for effective human-robot interaction (HRI). When a robot fails\nrepeatedly, how can it know when to change its behavior? Humans naturally\nrespond to robot errors through verbal and nonverbal cues that intensify over\nsuccessive failures-from confusion and subtle speech changes to visible\nfrustration and impatience. While prior work shows that human reactions can\nindicate robot failures, few studies examine how these evolving responses\nreveal successive failures. This research uses machine learning to recognize\nstages of robot failure from human reactions. In a study with 26 participants\ninteracting with a robot that made repeated conversational errors, behavioral\nfeatures were extracted from video data to train models for individual users.\nThe best model achieved 93.5% accuracy for detecting errors and 84.1% for\nclassifying successive failures. Modeling the progression of human reactions\nenhances error detection and understanding of repeated interaction breakdowns\nin HRI.",
            "headline_zh": "提出基于人类反应建模的方法，以检测机器人连续错误，提升人机交互效果。",
            "intro_zh": [
                "核心问题：机器人连续失败时，如何从人类反应中检测错误并调整行为。",
                "方法要点：提取视频行为特征，训练机器学习模型识别人类反应的演变阶段。",
                "实验或效果：在26人实验中，模型检测错误准确率达93.5%，分类连续失败达84.1%。"
            ],
            "tags_zh": [
                "人机交互",
                "错误检测",
                "机器学习",
                "行为分析",
                "连续失败分类"
            ],
            "_index": 78
        },
        {
            "title": "Visual Anomaly Detection for Reliable Robotic Implantation of Flexible Microelectrode Array",
            "authors": [
                "Yitong Chen",
                "Xinyao Xu",
                "Ping Zhu",
                "Xinyong Han",
                "Fangbo Qin",
                "Shan Yu"
            ],
            "arxiv_id": "2510.09071v1",
            "summary": "Flexible microelectrode (FME) implantation into brain cortex is challenging\ndue to the deformable fiber-like structure of FME probe and the interaction\nwith critical bio-tissue. To ensure reliability and safety, the implantation\nprocess should be monitored carefully. This paper develops an image-based\nanomaly detection framework based on the microscopic cameras of the robotic FME\nimplantation system. The unified framework is utilized at four checkpoints to\ncheck the micro-needle, FME probe, hooking result, and implantation point,\nrespectively. Exploiting the existing object localization results, the aligned\nregions of interest (ROIs) are extracted from raw image and input to a\npretrained vision transformer (ViT). Considering the task specifications, we\npropose a progressive granularity patch feature sampling method to address the\nsensitivity-tolerance trade-off issue at different locations. Moreover, we\nselect a part of feature channels with higher signal-to-noise ratios from the\nraw general ViT features, to provide better descriptors for each specific\nscene. The effectiveness of the proposed methods is validated with the image\ndatasets collected from our implantation system.",
            "headline_zh": "提出基于视觉Transformer的异常检测框架，用于机器人柔性微电极植入过程监控",
            "intro_zh": [
                "核心问题：柔性微电极植入脑皮层时，因结构变形和生物组织交互，需可靠监控以确保安全。",
                "方法要点：利用预训练ViT提取对齐ROI特征，采用渐进粒度采样和特征通道选择优化检测。",
                "实验或效果：在植入系统收集的图像数据集上验证了方法的有效性。"
            ],
            "tags_zh": [
                "视觉异常检测",
                "机器人植入",
                "柔性微电极",
                "视觉Transformer",
                "特征采样",
                "脑皮层监控"
            ],
            "_index": 79
        },
        {
            "title": "MMAudioSep: Taming Video-to-Audio Generative Model Towards Video/Text-Queried Sound Separation",
            "authors": [
                "Akira Takahashi",
                "Shusuke Takahashi",
                "Yuki Mitsufuji"
            ],
            "arxiv_id": "2510.09065v1",
            "summary": "We introduce MMAudioSep, a generative model for video/text-queried sound\nseparation that is founded on a pretrained video-to-audio model. By leveraging\nknowledge about the relationship between video/text and audio learned through a\npretrained audio generative model, we can train the model more efficiently,\ni.e., the model does not need to be trained from scratch. We evaluate the\nperformance of MMAudioSep by comparing it to existing separation models,\nincluding models based on both deterministic and generative approaches, and\nfind it is superior to the baseline models. Furthermore, we demonstrate that\neven after acquiring functionality for sound separation via fine-tuning, the\nmodel retains the ability for original video-to-audio generation. This\nhighlights the potential of foundational sound generation models to be adopted\nfor sound-related downstream tasks. Our code is available at\nhttps://github.com/sony/mmaudiosep.",
            "headline_zh": "提出MMAudioSep以基于预训练视频-音频模型实现视频/文本查询的声音分离",
            "intro_zh": [
                "核心问题：视频或文本查询下的声音分离任务，需高效训练模型。",
                "方法要点：利用预训练视频到音频生成模型的知识，通过微调而非从头训练。",
                "实验或效果：在分离性能上优于基线模型，并保留原始视频到音频生成能力。"
            ],
            "tags_zh": [
                "声音分离",
                "视频到音频生成",
                "预训练模型微调",
                "多模态查询",
                "生成模型"
            ],
            "_index": 80
        },
        {
            "title": "OSCAR: Orthogonal Stochastic Control for Alignment-Respecting Diversity in Flow Matching",
            "authors": [
                "Jingxuan Wu",
                "Zhenglin Wan",
                "Xingrui Yu",
                "Yuzhe Yang",
                "Bo An",
                "Ivor Tsang"
            ],
            "arxiv_id": "2510.09060v1",
            "summary": "Flow-based text-to-image models follow deterministic trajectories, forcing\nusers to repeatedly sample to discover diverse modes, which is a costly and\ninefficient process. We present a training-free, inference-time control\nmechanism that makes the flow itself diversity-aware. Our method simultaneously\nencourages lateral spread among trajectories via a feature-space objective and\nreintroduces uncertainty through a time-scheduled stochastic perturbation.\nCrucially, this perturbation is projected to be orthogonal to the generation\nflow, a geometric constraint that allows it to boost variation without\ndegrading image details or prompt fidelity. Our procedure requires no\nretraining or modification to the base sampler and is compatible with common\nflow-matching solvers. Theoretically, our method is shown to monotonically\nincrease a volume surrogate while, due to its geometric constraints,\napproximately preserving the marginal distribution. This provides a principled\nexplanation for why generation quality is robustly maintained. Empirically,\nacross multiple text-to-image settings under fixed sampling budgets, our method\nconsistently improves diversity metrics such as the Vendi Score and Brisque\nover strong baselines, while upholding image quality and alignment.",
            "headline_zh": "提出OSCAR方法以提升基于流的文本到图像模型的多样性，同时保持图像质量。",
            "intro_zh": [
                "核心问题：基于流的文本到图像模型轨迹确定性，导致多样性探索成本高且低效。",
                "方法要点：通过正交随机扰动和特征空间目标，在推理时增强轨迹多样性而不需重训练。",
                "实验或效果：在固定采样预算下，提升Vendi Score等多样性指标，并保持图像质量和提示对齐。"
            ],
            "tags_zh": [
                "文本到图像生成",
                "流匹配",
                "多样性增强",
                "推理时控制",
                "正交扰动",
                "图像质量保持"
            ],
            "_index": 81
        },
        {
            "title": "Lesion-Aware Post-Training of Latent Diffusion Models for Synthesizing Diffusion MRI from CT Perfusion",
            "authors": [
                "Junhyeok Lee",
                "Hyunwoong Kim",
                "Hyungjin Chung",
                "Heeseong Eom",
                "Joon Jang",
                "Chul-Ho Sohn",
                "Kyu Sung Choi"
            ],
            "arxiv_id": "2510.09056v1",
            "summary": "Image-to-Image translation models can help mitigate various challenges\ninherent to medical image acquisition. Latent diffusion models (LDMs) leverage\nefficient learning in compressed latent space and constitute the core of\nstate-of-the-art generative image models. However, this efficiency comes with a\ntrade-off, potentially compromising crucial pixel-level detail essential for\nhigh-fidelity medical images. This limitation becomes particularly critical\nwhen generating clinically significant structures, such as lesions, which often\noccupy only a small portion of the image. Failure to accurately reconstruct\nthese regions can severely impact diagnostic reliability and clinical\ndecision-making. To overcome this limitation, we propose a novel post-training\nframework for LDMs in medical image-to-image translation by incorporating\nlesion-aware medical pixel space objectives. This approach is essential, as it\nnot only enhances overall image quality but also improves the precision of\nlesion delineation. We evaluate our framework on brain CT-to-MRI translation in\nacute ischemic stroke patients, where early and accurate diagnosis is critical\nfor optimal treatment selection and improved patient outcomes. While diffusion\nMRI is the gold standard for stroke diagnosis, its clinical utility is often\nconstrained by high costs and low accessibility. Using a dataset of 817\npatients, we demonstrate that our framework improves overall image quality and\nenhances lesion delineation when synthesizing DWI and ADC images from CT\nperfusion scans, outperforming existing image-to-image translation models.\nFurthermore, our post-training strategy is easily adaptable to pre-trained LDMs\nand exhibits substantial potential for broader applications across diverse\nmedical image translation tasks.",
            "headline_zh": "提出病灶感知后训练框架，以提升CT灌注到扩散MRI合成的病灶描绘精度",
            "intro_zh": [
                "潜在扩散模型在医学图像翻译中可能丢失关键像素细节，影响病灶等小结构重建",
                "引入病灶感知像素空间目标进行后训练，增强整体图像质量和病灶描绘精度",
                "在817名急性缺血性卒中患者数据集上验证，合成DWI和ADC图像优于现有方法"
            ],
            "tags_zh": [
                "潜在扩散模型",
                "医学图像翻译",
                "病灶感知",
                "后训练",
                "CT灌注",
                "扩散MRI"
            ],
            "_index": 82
        },
        {
            "title": "Auto-scaling Continuous Memory for GUI Agent",
            "authors": [
                "Wenyi Wu",
                "Kun Zhou",
                "Ruoxin Yuan",
                "Vivian Yu",
                "Stephen Wang",
                "Zhiting Hu",
                "Biwei Huang"
            ],
            "arxiv_id": "2510.09038v1",
            "summary": "We study how to endow GUI agents with scalable memory that help generalize\nacross unfamiliar interfaces and long-horizon tasks. Prior GUI agents compress\npast trajectories into text tokens, which balloons context length and misses\ndecisive visual cues (e.g., exact widget size and position). We propose a\ncontinuous memory that encodes each GUI trajectory into a fixed-length sequence\nof continuous embeddings using the VLM itself as an encoder; these embeddings\nare plugged directly into the backbone's input layer, sharply reducing context\ncost while preserving fine-grained visual information. As memory size and\nretrieval depth increase, performance improves monotonically, unlike text\nmemories that degrade with long prompts. To grow memory at low cost, we\nintroduce an auto-scaling data flywheel that (i) discovers new environments via\nsearch, (ii) synthesizes tasks with an open-source VLM, (iii) rolls out\ntrajectories with the agent, and (iv) verifies success with the same VLM. Using\nthis pipeline, we collect 100k+ trajectories for about \\$4000 and fine-tune\nonly the memory encoder (LoRA on a Q-Former, 1.2\\% parameters) with 1,500\nsamples. On real-world GUI benchmarks, our memory-augmented agent consistently\nimproves success rates under long horizons and distribution shifts. Notably,\nQwen-2.5-VL-7B + continuous memory achieves performance comparable to\nstate-of-the-art closed-source models (e.g., GPT-4o, Claude-4).",
            "headline_zh": "提出连续记忆方法以增强GUI代理在陌生界面和长任务中的泛化能力",
            "intro_zh": [
                "核心问题：现有GUI代理压缩轨迹为文本，导致上下文过长且丢失关键视觉信息。",
                "方法要点：使用VLM编码轨迹为固定长度连续嵌入，直接输入骨干网络，降低上下文成本。",
                "实验或效果：在真实GUI基准测试中，成功率和泛化能力显著提升，性能接近闭源模型。"
            ],
            "tags_zh": [
                "GUI代理",
                "连续记忆",
                "视觉语言模型",
                "自动扩展",
                "长任务泛化",
                "嵌入编码"
            ],
            "_index": 83
        },
        {
            "title": "iMoWM: Taming Interactive Multi-Modal World Model for Robotic Manipulation",
            "authors": [
                "Chuanrui Zhang",
                "Zhengxian Wu",
                "Guanxing Lu",
                "Yansong Tang",
                "Ziwei Wang"
            ],
            "arxiv_id": "2510.09036v1",
            "summary": "Learned world models hold significant potential for robotic manipulation, as\nthey can serve as simulator for real-world interactions. While extensive\nprogress has been made in 2D video-based world models, these approaches often\nlack geometric and spatial reasoning, which is essential for capturing the\nphysical structure of the 3D world. To address this limitation, we introduce\niMoWM, a novel interactive world model designed to generate color images, depth\nmaps, and robot arm masks in an autoregressive manner conditioned on actions.\nTo overcome the high computational cost associated with three-dimensional\ninformation, we propose MMTokenizer, which unifies multi-modal inputs into a\ncompact token representation. This design enables iMoWM to leverage large-scale\npretrained VideoGPT models while maintaining high efficiency and incorporating\nricher physical information. With its multi-modal representation, iMoWM not\nonly improves the visual quality of future predictions but also serves as an\neffective simulator for model-based reinforcement learning (MBRL) and\nfacilitates real-world imitation learning. Extensive experiments demonstrate\nthe superiority of iMoWM across these tasks, showcasing the advantages of\nmulti-modal world modeling for robotic manipulation. Homepage:\nhttps://xingyoujun.github.io/imowm/",
            "headline_zh": "提出iMoWM交互式多模态世界模型，以增强机器人操作中的物理推理能力。",
            "intro_zh": [
                "核心问题：现有2D视频世界模型缺乏几何和空间推理，难以捕捉3D物理结构。",
                "方法要点：引入MMTokenizer统一多模态输入，实现自回归生成图像、深度图和机器人臂掩码。",
                "实验或效果：在模型强化学习和模仿学习中展示优越性，提升预测质量和仿真效率。"
            ],
            "tags_zh": [
                "机器人操作",
                "多模态世界模型",
                "自回归生成",
                "深度预测",
                "模型强化学习"
            ],
            "_index": 84
        },
        {
            "title": "Exploring Single Domain Generalization of LiDAR-based Semantic Segmentation under Imperfect Labels",
            "authors": [
                "Weitong Kong",
                "Zichao Zeng",
                "Di Wen",
                "Jiale Wei",
                "Kunyu Peng",
                "June Moh Goo",
                "Jan Boehm",
                "Rainer Stiefelhagen"
            ],
            "arxiv_id": "2510.09035v1",
            "summary": "Accurate perception is critical for vehicle safety, with LiDAR as a key\nenabler in autonomous driving. To ensure robust performance across\nenvironments, sensor types, and weather conditions without costly\nre-annotation, domain generalization in LiDAR-based 3D semantic segmentation is\nessential. However, LiDAR annotations are often noisy due to sensor\nimperfections, occlusions, and human errors. Such noise degrades segmentation\naccuracy and is further amplified under domain shifts, threatening system\nreliability. While noisy-label learning is well-studied in images, its\nextension to 3D LiDAR segmentation under domain generalization remains largely\nunexplored, as the sparse and irregular structure of point clouds limits direct\nuse of 2D methods. To address this gap, we introduce the novel task Domain\nGeneralization for LiDAR Semantic Segmentation under Noisy Labels (DGLSS-NL)\nand establish the first benchmark by adapting three representative noisy-label\nlearning strategies from image classification to 3D segmentation. However, we\nfind that existing noisy-label learning approaches adapt poorly to LiDAR data.\nWe therefore propose DuNe, a dual-view framework with strong and weak branches\nthat enforce feature-level consistency and apply cross-entropy loss based on\nconfidence-aware filtering of predictions. Our approach shows state-of-the-art\nperformance by achieving 56.86% mIoU on SemanticKITTI, 42.28% on nuScenes, and\n52.58% on SemanticPOSS under 10% symmetric label noise, with an overall\nArithmetic Mean (AM) of 49.57% and Harmonic Mean (HM) of 48.50%, thereby\ndemonstrating robust domain generalization in DGLSS-NL tasks. The code is\navailable on our project page.",
            "headline_zh": "提出DuNe双视图框架以解决LiDAR语义分割在噪声标签下的单域泛化问题",
            "intro_zh": [
                "核心问题：LiDAR标注噪声在域泛化中加剧，影响3D语义分割的鲁棒性",
                "方法要点：采用强弱分支双视图框架，通过特征一致性和置信度过滤优化损失",
                "实验或效果：在多个数据集上实现SOTA性能，mIoU达49.57%算术平均"
            ],
            "tags_zh": [
                "LiDAR语义分割",
                "域泛化",
                "噪声标签学习",
                "双视图框架",
                "3D点云处理"
            ],
            "_index": 85
        },
        {
            "title": "Trust Modeling and Estimation in Human-Autonomy Interactions",
            "authors": [
                "Daniel A. Williams",
                "Airlie Chapman",
                "Daniel R. Little",
                "Chris Manzie"
            ],
            "arxiv_id": "2510.09013v1",
            "summary": "Advances in the control of autonomous systems have accompanied an expansion\nin the potential applications for autonomous robotic systems. The success of\napplications involving humans depends on the quality of interaction between the\nautonomous system and the human supervisor, which is particularly affected by\nthe degree of trust that the supervisor places in the autonomous system. Absent\nfrom the literature are models of supervisor trust dynamics that can\naccommodate asymmetric responses to autonomous system performance and the\nintermittent nature of supervisor-autonomous system communication. This paper\nfocuses on formulating an estimated model of supervisor trust that incorporates\nboth of these features by employing a switched linear system structure with\nevent-triggered sampling of the model input and output. Trust response data\ncollected in a user study with 51 participants were then used identify\nparameters for a switched linear model-based observer of supervisor trust.",
            "headline_zh": "提出基于切换线性系统的信任估计模型，以解决人机交互中信任动态建模问题。",
            "intro_zh": [
                "核心问题：人机交互中缺乏能处理不对称信任响应和间歇通信的信任动态模型。",
                "方法要点：采用切换线性系统结构，结合事件触发采样来建模信任输入和输出。",
                "实验或效果：基于51名参与者的用户研究数据，识别了信任观测器的模型参数。"
            ],
            "tags_zh": [
                "信任建模",
                "人机交互",
                "切换线性系统",
                "事件触发采样",
                "自主系统控制"
            ],
            "_index": 86
        },
        {
            "title": "Towards Better & Faster Autoregressive Image Generation: From the Perspective of Entropy",
            "authors": [
                "Xiaoxiao Ma",
                "Feng Zhao",
                "Pengyang Ling",
                "Haibo Qiu",
                "Zhixiang Wei",
                "Hu Yu",
                "Jie Huang",
                "Zhixiong Zeng",
                "Lin Ma"
            ],
            "arxiv_id": "2510.09012v1",
            "summary": "In this work, we first revisit the sampling issues in current autoregressive\n(AR) image generation models and identify that image tokens, unlike text\ntokens, exhibit lower information density and non-uniform spatial distribution.\nAccordingly, we present an entropy-informed decoding strategy that facilitates\nhigher autoregressive generation quality with faster synthesis speed.\nSpecifically, the proposed method introduces two main innovations: 1) dynamic\ntemperature control guided by spatial entropy of token distributions, enhancing\nthe balance between content diversity, alignment accuracy, and structural\ncoherence in both mask-based and scale-wise models, without extra computational\noverhead, and 2) entropy-aware acceptance rules in speculative decoding,\nachieving near-lossless generation at about 85\\% of the inference cost of\nconventional acceleration methods. Extensive experiments across multiple\nbenchmarks using diverse AR image generation models demonstrate the\neffectiveness and generalizability of our approach in enhancing both generation\nquality and sampling speed.",
            "headline_zh": "提出熵引导解码策略以提升自回归图像生成的质量与速度",
            "intro_zh": [
                "核心问题：图像令牌信息密度低且空间分布不均，影响自回归生成质量与速度",
                "方法要点：动态温度控制与熵感知接受规则，平衡多样性、准确性和结构连贯性",
                "实验或效果：在多个基准测试中，质量提升且推理成本降至约85%，通用性强"
            ],
            "tags_zh": [
                "自回归图像生成",
                "熵引导解码",
                "动态温度控制",
                "推测解码",
                "图像令牌分布",
                "生成加速"
            ],
            "_index": 87
        },
        {
            "title": "On Epistemic Uncertainty of Visual Tokens for Object Hallucinations in Large Vision-Language Models",
            "authors": [
                "Hoigi Seo",
                "Dong Un Kang",
                "Hyunjin Cho",
                "Joohoon Lee",
                "Se Young Chun"
            ],
            "arxiv_id": "2510.09008v1",
            "summary": "Large vision-language models (LVLMs), which integrate a vision encoder (VE)\nwith a large language model, have achieved remarkable success across various\ntasks. However, there are still crucial challenges in LVLMs such as object\nhallucination, generating descriptions of objects that are not in the input\nimage. Here, we argue that uncertain visual tokens within the VE is a key\nfactor that contributes to object hallucination. Our statistical analysis found\nthat there are positive correlations between visual tokens with high epistemic\nuncertainty and the occurrence of hallucinations. Furthermore, we show\ntheoretically and empirically that visual tokens in early VE layers that\nexhibit large representation deviations under small adversarial perturbations\nindicate high epistemic uncertainty. Based on these findings, we propose a\nsimple yet effective strategy to mitigate object hallucination by modifying the\nVE only. Our method comprises a proxy method with adversarial perturbations for\nidentifying uncertain visual tokens efficiently and a method to mask these\nuncertain visual tokens during the self-attention process in the middle layers\nof the VE, suppressing their influence on visual encoding and thus alleviating\nhallucinations. Extensive experiments show that our method significantly\nreduces object hallucinations in LVLMs and can synergistically work with other\nprior arts.",
            "headline_zh": "提出视觉编码器修改策略以缓解大视觉语言模型中的物体幻觉问题",
            "intro_zh": [
                "核心问题：大视觉语言模型存在物体幻觉，即生成图像中不存在的物体描述。",
                "方法要点：通过对抗扰动识别不确定视觉令牌，并在自注意力过程中掩码它们。",
                "实验或效果：实验显示该方法显著减少物体幻觉，并能与其他方法协同工作。"
            ],
            "tags_zh": [
                "大视觉语言模型",
                "物体幻觉",
                "视觉编码器",
                "对抗扰动",
                "自注意力掩码",
                "不确定性估计"
            ],
            "_index": 88
        },
        {
            "title": "Speculative Jacobi-Denoising Decoding for Accelerating Autoregressive Text-to-image Generation",
            "authors": [
                "Yao Teng",
                "Fuyun Wang",
                "Xian Liu",
                "Zhekai Chen",
                "Han Shi",
                "Yu Wang",
                "Zhenguo Li",
                "Weiyang Liu",
                "Difan Zou",
                "Xihui Liu"
            ],
            "arxiv_id": "2510.08994v1",
            "summary": "As a new paradigm of visual content generation, autoregressive text-to-image\nmodels suffer from slow inference due to their sequential token-by-token\ndecoding process, often requiring thousands of model forward passes to generate\na single image. To address this inefficiency, we propose Speculative\nJacobi-Denoising Decoding (SJD2), a framework that incorporates the denoising\nprocess into Jacobi iterations to enable parallel token generation in\nautoregressive models. Our method introduces a next-clean-token prediction\nparadigm that enables the pre-trained autoregressive models to accept\nnoise-perturbed token embeddings and predict the next clean tokens through\nlow-cost fine-tuning. This denoising paradigm guides the model towards more\nstable Jacobi trajectories. During inference, our method initializes token\nsequences with Gaussian noise and performs iterative\nnext-clean-token-prediction in the embedding space. We employ a probabilistic\ncriterion to verify and accept multiple tokens in parallel, and refine the\nunaccepted tokens for the next iteration with the denoising trajectory.\nExperiments show that our method can accelerate generation by reducing model\nforward passes while maintaining the visual quality of generated images.",
            "headline_zh": "提出推测性雅可比去噪解码以加速自回归文本到图像生成",
            "intro_zh": [
                "自回归文本到图像模型因逐令牌解码导致推理缓慢，需数千次前向传播",
                "引入去噪过程到雅可比迭代，通过预测下一干净令牌实现并行生成",
                "实验显示方法减少前向传播次数，同时保持生成图像视觉质量"
            ],
            "tags_zh": [
                "自回归模型",
                "文本到图像生成",
                "推理加速",
                "雅可比迭代",
                "去噪解码"
            ],
            "_index": 89
        },
        {
            "title": "Uncolorable Examples: Preventing Unauthorized AI Colorization via Perception-Aware Chroma-Restrictive Perturbation",
            "authors": [
                "Yuki Nii",
                "Futa Waseda",
                "Ching-Chun Chang",
                "Isao Echizen"
            ],
            "arxiv_id": "2510.08979v1",
            "summary": "AI-based colorization has shown remarkable capability in generating realistic\ncolor images from grayscale inputs. However, it poses risks of copyright\ninfringement -- for example, the unauthorized colorization and resale of\nmonochrome manga and films. Despite these concerns, no effective method\ncurrently exists to prevent such misuse. To address this, we introduce the\nfirst defensive paradigm, Uncolorable Examples, which embed imperceptible\nperturbations into grayscale images to invalidate unauthorized colorization. To\nensure real-world applicability, we establish four criteria: effectiveness,\nimperceptibility, transferability, and robustness. Our method, Perception-Aware\nChroma-Restrictive Perturbation (PAChroma), generates Uncolorable Examples that\nmeet these four criteria by optimizing imperceptible perturbations with a\nLaplacian filter to preserve perceptual quality, and applying diverse input\ntransformations during optimization to enhance transferability across models\nand robustness against common post-processing (e.g., compression). Experiments\non ImageNet and Danbooru datasets demonstrate that PAChroma effectively\ndegrades colorization quality while maintaining the visual appearance. This\nwork marks the first step toward protecting visual content from illegitimate AI\ncolorization, paving the way for copyright-aware defenses in generative media.",
            "headline_zh": "提出PAChroma方法以防止未经授权的AI着色，保护灰度图像版权。",
            "intro_zh": [
                "核心问题：AI着色技术可能导致版权侵犯，如未经授权为漫画和电影着色。",
                "方法要点：通过优化不可感知扰动，结合拉普拉斯滤波保持视觉质量。",
                "实验或效果：在ImageNet和Danbooru数据集上验证，有效降低着色质量。"
            ],
            "tags_zh": [
                "AI着色防御",
                "不可感知扰动",
                "版权保护",
                "图像处理",
                "鲁棒性优化"
            ],
            "_index": 90
        },
        {
            "title": "HandEval: Taking the First Step Towards Hand Quality Evaluation in Generated Images",
            "authors": [
                "Zichuan Wang",
                "Bo Peng",
                "Songlin Yang",
                "Zhenchen Tang",
                "Jing Dong"
            ],
            "arxiv_id": "2510.08978v1",
            "summary": "Although recent text-to-image (T2I) models have significantly improved the\noverall visual quality of generated images, they still struggle in the\ngeneration of accurate details in complex local regions, especially human\nhands. Generated hands often exhibit structural distortions and unrealistic\ntextures, which can be very noticeable even when the rest of the body is\nwell-generated. However, the quality assessment of hand regions remains largely\nneglected, limiting downstream task performance like human-centric generation\nquality optimization and AIGC detection. To address this, we propose the first\nquality assessment task targeting generated hand regions and showcase its\nabundant downstream applications. We first introduce the HandPair dataset for\ntraining hand quality assessment models. It consists of 48k images formed by\nhigh- and low-quality hand pairs, enabling low-cost, efficient supervision\nwithout manual annotation. Based on it, we develop HandEval, a carefully\ndesigned hand-specific quality assessment model. It leverages the powerful\nvisual understanding capability of Multimodal Large Language Model (MLLM) and\nincorporates prior knowledge of hand keypoints, gaining strong perception of\nhand quality. We further construct a human-annotated test set with hand images\nfrom various state-of-the-art (SOTA) T2I models to validate its quality\nevaluation capability. Results show that HandEval aligns better with human\njudgments than existing SOTA methods. Furthermore, we integrate HandEval into\nimage generation and AIGC detection pipelines, prominently enhancing generated\nhand realism and detection accuracy, respectively, confirming its universal\neffectiveness in downstream applications. Code and dataset will be available.",
            "headline_zh": "提出HandEval以评估生成图像中手部质量，提升下游任务性能",
            "intro_zh": [
                "文本到图像模型在复杂局部区域如手部生成细节不准确，存在结构扭曲和纹理不真实问题",
                "基于HandPair数据集开发HandEval模型，利用多模态大语言模型和手部关键点先验知识",
                "实验显示HandEval与人类判断更一致，集成到生成和检测管道中显著提升手部真实性和检测准确率"
            ],
            "tags_zh": [
                "手部质量评估",
                "文本到图像生成",
                "多模态大语言模型",
                "手部关键点",
                "AIGC检测",
                "数据集构建"
            ],
            "_index": 91
        },
        {
            "title": "Hierarchical Scheduling for Multi-Vector Image Retrieval",
            "authors": [
                "Maoliang Li",
                "Ke Li",
                "Yaoyang Liu",
                "Jiayu Chen",
                "Zihao Zheng",
                "Yinjun Wu",
                "Xiang Chen"
            ],
            "arxiv_id": "2510.08976v1",
            "summary": "To effectively leverage user-specific data, retrieval augmented generation\n(RAG) is employed in multimodal large language model (MLLM) applications.\nHowever, conventional retrieval approaches often suffer from limited retrieval\naccuracy. Recent advances in multi-vector retrieval (MVR) improve accuracy by\ndecomposing queries and matching against segmented images. They still suffer\nfrom sub-optimal accuracy and efficiency, overlooking alignment between the\nquery and varying image objects and redundant fine-grained image segments. In\nthis work, we present an efficient scheduling framework for image retrieval -\nHiMIR. First, we introduce a novel hierarchical paradigm, employing multiple\nintermediate granularities for varying image objects to enhance alignment.\nSecond, we minimize redundancy in retrieval by leveraging cross-hierarchy\nsimilarity consistency and hierarchy sparsity to minimize unnecessary matching\ncomputation. Furthermore, we configure parameters for each dataset\nautomatically for practicality across diverse scenarios. Our empirical study\nshows that, HiMIR not only achieves substantial accuracy improvements but also\nreduces computation by up to 3.5 times over the existing MVR system.",
            "headline_zh": "提出分层调度框架HiMIR以提升多向量图像检索的准确性和效率",
            "intro_zh": [
                "核心问题：传统检索方法准确率低，多向量检索存在对齐不足和冗余计算问题",
                "方法要点：采用分层范式增强查询与图像对象对齐，利用一致性减少冗余匹配",
                "实验效果：在准确率显著提升的同时，计算量减少高达3.5倍"
            ],
            "tags_zh": [
                "多向量图像检索",
                "分层调度",
                "检索增强生成",
                "相似性一致性",
                "计算效率优化"
            ],
            "_index": 92
        },
        {
            "title": "A geometrical approach to solve the proximity of a point to an axisymmetric quadric in space",
            "authors": [
                "Bibekananda Patra",
                "Aditya Mahesh Kolte",
                "Sandipan Bandyopadhyay"
            ],
            "arxiv_id": "2510.08973v1",
            "summary": "This paper presents the classification of a general quadric into an\naxisymmetric quadric (AQ) and the solution to the problem of the proximity of a\ngiven point to an AQ. The problem of proximity in $R^3$ is reduced to the same\nin $R^2$, which is not found in the literature. A new method to solve the\nproblem in $R^2$ is used based on the geometrical properties of the conics,\nsuch as sub-normal, length of the semi-major axis, eccentricity, slope and\nradius. Furthermore, the problem in $R^2$ is categorised into two and three\nmore sub-cases for parabola and ellipse/hyperbola, respectively, depending on\nthe location of the point, which is a novel approach as per the authors'\nknowledge. The proposed method is suitable for implementation in a common\nprogramming language, such as C and proved to be faster than a commercial\nlibrary, namely, Bullet.",
            "headline_zh": "提出几何方法解决空间点与轴对称二次曲面邻近问题，并实现高效计算。",
            "intro_zh": [
                "核心问题：计算三维空间中给定点与轴对称二次曲面的最短距离。",
                "方法要点：将问题降维至二维，利用圆锥曲线几何性质分类求解。",
                "实验或效果：在C语言实现中，速度优于Bullet商业库。"
            ],
            "tags_zh": [
                "轴对称二次曲面",
                "点邻近问题",
                "几何方法",
                "降维求解",
                "圆锥曲线",
                "高效计算"
            ],
            "_index": 93
        },
        {
            "title": "mmJoints: Expanding Joint Representations Beyond (x,y,z) in mmWave-Based 3D Pose Estimation",
            "authors": [
                "Zhenyu Wang",
                "Mahathir Monjur",
                "Shahriar Nirjon"
            ],
            "arxiv_id": "2510.08970v1",
            "summary": "In mmWave-based pose estimation, sparse signals and weak reflections often\ncause models to infer body joints from statistical priors rather than sensor\ndata. While prior knowledge helps in learning meaningful representations,\nover-reliance on it degrades performance in downstream tasks like gesture and\nactivity recognition. In this paper, we introduce mmJoints, a framework that\naugments a pre-trained, black-box mmWave-based 3D pose estimator's output with\nadditional joint descriptors. Rather than mitigating bias, mmJoints makes it\nexplicit by estimating the likelihood of a joint being sensed and the\nreliability of its predicted location. These descriptors enhance\ninterpretability and improve downstream task accuracy. Through extensive\nevaluations using over 115,000 signal frames across 13 pose estimation\nsettings, we show that mmJoints estimates descriptors with an error rate below\n4.2%. mmJoints also improves joint position accuracy by up to 12.5% and boosts\nactivity recognition by up to 16% over state-of-the-art methods.",
            "headline_zh": "提出mmJoints框架，通过增强关节描述符提升毫米波3D姿态估计的可靠性和下游任务性能",
            "intro_zh": [
                "毫米波姿态估计中，稀疏信号和弱反射导致模型过度依赖统计先验，影响下游任务准确性",
                "mmJoints估计关节被感知的可能性和位置可靠性，使偏差显式化，增强可解释性",
                "实验显示，描述符估计错误率低于4.2%，关节位置精度提升达12.5%，活动识别提升达16%"
            ],
            "tags_zh": [
                "毫米波3D姿态估计",
                "关节描述符",
                "偏差显式化",
                "下游任务增强",
                "可解释性提升"
            ],
            "_index": 94
        },
        {
            "title": "SAM2-3dMed: Empowering SAM2 for 3D Medical Image Segmentation",
            "authors": [
                "Yeqing Yang",
                "Le Xu",
                "Lixia Tian"
            ],
            "arxiv_id": "2510.08967v1",
            "summary": "Accurate segmentation of 3D medical images is critical for clinical\napplications like disease assessment and treatment planning. While the Segment\nAnything Model 2 (SAM2) has shown remarkable success in video object\nsegmentation by leveraging temporal cues, its direct application to 3D medical\nimages faces two fundamental domain gaps: 1) the bidirectional anatomical\ncontinuity between slices contrasts sharply with the unidirectional temporal\nflow in videos, and 2) precise boundary delineation, crucial for morphological\nanalysis, is often underexplored in video tasks. To bridge these gaps, we\npropose SAM2-3dMed, an adaptation of SAM2 for 3D medical imaging. Our framework\nintroduces two key innovations: 1) a Slice Relative Position Prediction (SRPP)\nmodule explicitly models bidirectional inter-slice dependencies by guiding SAM2\nto predict the relative positions of different slices in a self-supervised\nmanner; 2) a Boundary Detection (BD) module enhances segmentation accuracy\nalong critical organ and tissue boundaries. Extensive experiments on three\ndiverse medical datasets (the Lung, Spleen, and Pancreas in the Medical\nSegmentation Decathlon (MSD) dataset) demonstrate that SAM2-3dMed significantly\noutperforms state-of-the-art methods, achieving superior performance in\nsegmentation overlap and boundary precision. Our approach not only advances 3D\nmedical image segmentation performance but also offers a general paradigm for\nadapting video-centric foundation models to spatial volumetric data.",
            "headline_zh": "提出SAM2-3dMed以解决3D医学图像分割中的领域差距问题",
            "intro_zh": [
                "核心问题：SAM2直接应用于3D医学图像存在双向解剖连续性与视频单向时间流的差距，以及边界精确性不足。",
                "方法要点：引入切片相对位置预测模块建模双向依赖，边界检测模块增强分割精度。",
                "实验或效果：在多个医学数据集上显著优于现有方法，提升分割重叠和边界精度。"
            ],
            "tags_zh": [
                "3D医学图像分割",
                "切片相对位置预测",
                "边界检测",
                "领域适应",
                "自监督学习"
            ],
            "_index": 95
        },
        {
            "title": "Unleashing Perception-Time Scaling to Multimodal Reasoning Models",
            "authors": [
                "Yifan Li",
                "Zhenghao Chen",
                "Ziheng Wu",
                "Kun Zhou",
                "Ruipu Luo",
                "Can Zhang",
                "Zhentao He",
                "Yufei Zhan",
                "Wayne Xin Zhao",
                "Minghui Qiu"
            ],
            "arxiv_id": "2510.08964v1",
            "summary": "Recent advances in inference-time scaling, particularly those leveraging\nreinforcement learning with verifiable rewards, have substantially enhanced the\nreasoning capabilities of Large Vision-Language Models (LVLMs). Inspired by\nthis success, similar strategies have been applied to multimodal reasoning, yet\ntheir impact on visual perception remains unclear. To investigate this gap, we\nintroduce DisTANCE, a perception-centric benchmark for visual estimation tasks.\nEvaluation results show that LVLMs exhibit limited estimation precision, and\ninference-time scaling offers only marginal gains. We attribute this to the\nfast perception paradigm of current LVLMs, where visual understanding is\ntreated as a one-shot output without modeling the underlying perceptual\nprocess. To address this, we propose Perception-Time Scaling (PTS), a novel\nparadigm that encourages token-rich perception and decomposes complex\nperception problems into intermediate tractable sub-problems, thereby enabling\nperception to align with and benefit from inference-time scaling. Combined with\nreinforcement learning techniques, PTS significantly improves perception\naccuracy, raising high-precision performance on DisTANCE from 8.0% to 64.7%,\nand generalizes well to out-of-domain tasks. Surprisingly, even though PTS data\nare purely synthetic, combining them with math reasoning data yields consistent\ngains in both reasoning and real-world perception benchmarks. Further analysis\nreveals that PTS introduces more perception-related tokens and increases the\nmodel's attention to image tokens. Our code and data will be publicly released.",
            "headline_zh": "提出感知时间缩放以提升多模态模型的视觉感知精度",
            "intro_zh": [
                "核心问题：当前大视觉语言模型在视觉感知任务中精度有限，推理时间缩放效果不佳。",
                "方法要点：引入感知时间缩放，分解复杂感知问题为可处理子问题，结合强化学习。",
                "实验或效果：在DisTANCE基准上高精度性能从8.0%提升至64.7%，泛化能力强。"
            ],
            "tags_zh": [
                "感知时间缩放",
                "多模态推理",
                "视觉估计",
                "强化学习",
                "基准评估",
                "合成数据"
            ],
            "_index": 96
        },
        {
            "title": "Denoised Diffusion for Object-Focused Image Augmentation",
            "authors": [
                "Nisha Pillai",
                "Aditi Virupakshaiah",
                "Harrison W. Smith",
                "Amanda J. Ashworth",
                "Prasanna Gowda",
                "Phillip R. Owens",
                "Adam R. Rivers",
                "Bindu Nanduri",
                "Mahalingam Ramkumar"
            ],
            "arxiv_id": "2510.08955v1",
            "summary": "Modern agricultural operations increasingly rely on integrated monitoring\nsystems that combine multiple data sources for farm optimization. Aerial\ndrone-based animal health monitoring serves as a key component but faces\nlimited data availability, compounded by scene-specific issues such as small,\noccluded, or partially visible animals. Transfer learning approaches often fail\nto address this limitation due to the unavailability of large datasets that\nreflect specific farm conditions, including variations in animal breeds,\nenvironments, and behaviors. Therefore, there is a need for developing a\nproblem-specific, animal-focused data augmentation strategy tailored to these\nunique challenges. To address this gap, we propose an object-focused data\naugmentation framework designed explicitly for animal health monitoring in\nconstrained data settings. Our approach segments animals from backgrounds and\naugments them through transformations and diffusion-based synthesis to create\nrealistic, diverse scenes that enhance animal detection and monitoring\nperformance. Our initial experiments demonstrate that our augmented dataset\nyields superior performance compared to our baseline models on the animal\ndetection task. By generating domain-specific data, our method empowers\nreal-time animal health monitoring solutions even in data-scarce scenarios,\nbridging the gap between limited data and practical applicability.",
            "headline_zh": "提出对象聚焦数据增强框架以解决农业动物健康监测中数据稀缺问题",
            "intro_zh": [
                "核心问题：农业无人机监测中动物数据稀缺，受限于场景特异性如遮挡和品种差异",
                "方法要点：分割动物并应用变换和扩散合成，生成多样化真实场景以增强检测",
                "实验或效果：初步实验显示增强数据集在动物检测任务上优于基线模型"
            ],
            "tags_zh": [
                "数据增强",
                "动物检测",
                "扩散模型",
                "农业监测",
                "对象分割"
            ],
            "_index": 97
        },
        {
            "title": "Direct Data-Driven Predictive Control for a Three-dimensional Cable-Driven Soft Robotic Arm",
            "authors": [
                "Cheng Ouyang",
                "Moeen Ul Islam",
                "Dong Chen",
                "Kaixiang Zhang",
                "Zhaojian Li",
                "Xiaobo Tan"
            ],
            "arxiv_id": "2510.08953v1",
            "summary": "Soft robots offer significant advantages in safety and adaptability, yet\nachieving precise and dynamic control remains a major challenge due to their\ninherently complex and nonlinear dynamics. Recently, Data-enabled Predictive\nControl (DeePC) has emerged as a promising model-free approach that bypasses\nexplicit system identification by directly leveraging input-output data. While\nDeePC has shown success in other domains, its application to soft robots\nremains underexplored, particularly for three-dimensional (3D) soft robotic\nsystems. This paper addresses this gap by developing and experimentally\nvalidating an effective DeePC framework on a 3D, cable-driven soft arm.\nSpecifically, we design and fabricate a soft robotic arm with a thick tubing\nbackbone for stability, a dense silicone body with large cavities for strength\nand flexibility, and rigid endcaps for secure termination. Using this platform,\nwe implement DeePC with singular value decomposition (SVD)-based dimension\nreduction for two key control tasks: fixed-point regulation and trajectory\ntracking in 3D space. Comparative experiments with a baseline model-based\ncontroller demonstrate DeePC's superior accuracy, robustness, and adaptability,\nhighlighting its potential as a practical solution for dynamic control of soft\nrobots.",
            "headline_zh": "提出数据驱动预测控制框架以解决三维软体机器人动态控制难题",
            "intro_zh": [
                "软体机器人因非线性动力学难以实现精确动态控制",
                "采用DeePC方法直接利用输入输出数据，避免显式系统辨识",
                "实验验证在三维软体臂上实现高精度轨迹跟踪和鲁棒性"
            ],
            "tags_zh": [
                "软体机器人控制",
                "数据驱动预测控制",
                "三维运动控制",
                "模型自由控制",
                "轨迹跟踪"
            ],
            "_index": 98
        },
        {
            "title": "FS-RWKV: Leveraging Frequency Spatial-Aware RWKV for 3T-to-7T MRI Translation",
            "authors": [
                "Yingtie Lei",
                "Zimeng Li",
                "Chi-Man Pun",
                "Yupeng Liu",
                "Xuhang Chen"
            ],
            "arxiv_id": "2510.08951v1",
            "summary": "Ultra-high-field 7T MRI offers enhanced spatial resolution and tissue\ncontrast that enables the detection of subtle pathological changes in\nneurological disorders. However, the limited availability of 7T scanners\nrestricts widespread clinical adoption due to substantial infrastructure costs\nand technical demands. Computational approaches for synthesizing 7T-quality\nimages from accessible 3T acquisitions present a viable solution to this\naccessibility challenge. Existing CNN approaches suffer from limited spatial\ncoverage, while Transformer models demand excessive computational overhead.\nRWKV architectures offer an efficient alternative for global feature modeling\nin medical image synthesis, combining linear computational complexity with\nstrong long-range dependency capture. Building on this foundation, we propose\nFrequency Spatial-RWKV (FS-RWKV), an RWKV-based framework for 3T-to-7T MRI\ntranslation. To better address the challenges of anatomical detail preservation\nand global tissue contrast recovery, FS-RWKV incorporates two key modules: (1)\nFrequency-Spatial Omnidirectional Shift (FSO-Shift), which performs discrete\nwavelet decomposition followed by omnidirectional spatial shifting on the\nlow-frequency branch to enhance global contextual representation while\npreserving high-frequency anatomical details; and (2) Structural Fidelity\nEnhancement Block (SFEB), a module that adaptively reinforces anatomical\nstructure through frequency-aware feature fusion. Comprehensive experiments on\nUNC and BNU datasets demonstrate that FS-RWKV consistently outperforms existing\nCNN-, Transformer-, GAN-, and RWKV-based baselines across both T1w and T2w\nmodalities, achieving superior anatomical fidelity and perceptual quality.",
            "headline_zh": "提出FS-RWKV框架，利用频率空间感知RWKV实现3T到7T MRI图像翻译。",
            "intro_zh": [
                "核心问题：7T MRI稀缺，需从3T图像合成高质量7T图像以提升临床可及性。",
                "方法要点：引入FSO-Shift和SFEB模块，增强全局上下文和结构保真度。",
                "实验或效果：在UNC和BNU数据集上优于多种基线，提升解剖保真度和感知质量。"
            ],
            "tags_zh": [
                "医学图像合成",
                "MRI翻译",
                "RWKV架构",
                "频率空间感知",
                "解剖结构保真"
            ],
            "_index": 99
        },
        {
            "title": "Progressive Uncertainty-Guided Evidential U-KAN for Trustworthy Medical Image Segmentation",
            "authors": [
                "Zhen Yang",
                "Yansong Ma",
                "Lei Chen"
            ],
            "arxiv_id": "2510.08949v1",
            "summary": "Trustworthy medical image segmentation aims at deliver accurate and reliable\nresults for clinical decision-making. Most existing methods adopt the evidence\ndeep learning (EDL) paradigm due to its computational efficiency and\ntheoretical robustness. However, the EDL-based methods often neglect leveraging\nuncertainty maps rich in attention cues to refine ambiguous boundary\nsegmentation. To address this, we propose a progressive evidence uncertainty\nguided attention (PEUA) mechanism to guide the model to focus on the feature\nrepresentation learning of hard regions. Unlike conventional approaches, PEUA\nprogressively refines attention using uncertainty maps while employing low-rank\nlearning to denoise attention weights, enhancing feature learning for\nchallenging regions. Concurrently, standard EDL methods suppress evidence of\nincorrect class indiscriminately via Kullback-Leibler (KL) regularization,\nimpairing the uncertainty assessment in ambiguous areas and consequently\ndistorts the corresponding attention guidance. We thus introduce a\nsemantic-preserving evidence learning (SAEL) strategy, integrating a\nsemantic-smooth evidence generator and a fidelity-enhancing regularization term\nto retain critical semantics. Finally, by embedding PEUA and SAEL with the\nstate-of-the-art U-KAN, we proposes Evidential U-KAN, a novel solution for\ntrustworthy medical image segmentation. Extensive experiments on 4 datasets\ndemonstrate superior accuracy and reliability over the competing methods. The\ncode is available at\n\\href{https://anonymous.4open.science/r/Evidence-U-KAN-BBE8}{github}.",
            "headline_zh": "提出渐进不确定性引导的Evidential U-KAN以提升可信医学图像分割",
            "intro_zh": [
                "现有证据深度学习方法忽略不确定性图对模糊边界分割的指导作用",
                "引入渐进证据不确定性引导注意机制和语义保持证据学习策略",
                "在四个数据集上实验显示准确性和可靠性优于竞争方法"
            ],
            "tags_zh": [
                "医学图像分割",
                "证据深度学习",
                "不确定性引导",
                "注意力机制",
                "语义保持学习"
            ],
            "_index": 100
        },
        {
            "title": "Bi-level Meta-Policy Control for Dynamic Uncertainty Calibration in Evidential Deep Learning",
            "authors": [
                "Zhen Yang",
                "Yansong Ma",
                "Lei Chen"
            ],
            "arxiv_id": "2510.08938v1",
            "summary": "Traditional Evidence Deep Learning (EDL) methods rely on static\nhyperparameter for uncertainty calibration, limiting their adaptability in\ndynamic data distributions, which results in poor calibration and\ngeneralization in high-risk decision-making tasks. To address this limitation,\nwe propose the Meta-Policy Controller (MPC), a dynamic meta-learning framework\nthat adjusts the KL divergence coefficient and Dirichlet prior strengths for\noptimal uncertainty modeling. Specifically, MPC employs a bi-level optimization\napproach: in the inner loop, model parameters are updated through a dynamically\nconfigured loss function that adapts to the current training state; in the\nouter loop, a policy network optimizes the KL divergence coefficient and\nclass-specific Dirichlet prior strengths based on multi-objective rewards\nbalancing prediction accuracy and uncertainty quality. Unlike previous methods\nwith fixed priors, our learnable Dirichlet prior enables flexible adaptation to\nclass distributions and training dynamics. Extensive experimental results show\nthat MPC significantly enhances the reliability and calibration of model\npredictions across various tasks, improving uncertainty calibration, prediction\naccuracy, and performance retention after confidence-based sample rejection.",
            "headline_zh": "提出元策略控制器以解决动态数据分布下不确定性校准问题",
            "intro_zh": [
                "传统EDL依赖静态超参数，在动态数据分布中校准和泛化能力差",
                "采用双层优化：内层动态配置损失函数，外层策略网络优化KL系数和Dirichlet先验",
                "实验显示显著提升不确定性校准、预测准确性和置信度样本拒绝后性能"
            ],
            "tags_zh": [
                "元学习",
                "不确定性校准",
                "证据深度学习",
                "双层优化",
                "Dirichlet先验"
            ],
            "_index": 101
        },
        {
            "title": "RO-Bench: Large-scale robustness evaluation of MLLMs with text-driven counterfactual videos",
            "authors": [
                "Zixi Yang",
                "Jiapeng Li",
                "Muxi Diao",
                "Yinuo Jing",
                "Kongming Liang"
            ],
            "arxiv_id": "2510.08936v1",
            "summary": "Recently, Multi-modal Large Language Models (MLLMs) have demonstrated\nsignificant performance across various video understanding tasks. However,\ntheir robustness, particularly when faced with manipulated video content,\nremains largely unexplored. In this paper, we introduce Ro-Bench, the first\nbenchmark for evaluating MLLMs on dynamic out-of-distribution (OOD)\ncounterfactual video test sets. Ro-Bench incorporates high-quality, diverse and\ntemporally relevant video data, by editing Style, Object, Background and their\ncompositions. We evaluated eight recent video MLLMs and found that current\nmodels exhibit substantial performance degradation on Ro-Bench when exposed to\ncounterfactual video content. Furthermore, we demonstrate that fine-tuning\nMLLMs with counterfactual data enhances robustness, achieving a 21.73%\nperformance increase on Ro-Bench and a 12.78% improvement across 20 tasks in\nthe MVBench dataset. These findings underscore the effectiveness of\ncounterfactual data in enhancing the video understanding ability of MLLMs. The\ncode and data will be released shortly.",
            "headline_zh": "提出Ro-Bench基准以评估多模态大语言模型在反事实视频中的鲁棒性",
            "intro_zh": [
                "多模态大语言模型在视频理解任务中表现优异，但面对操纵视频内容时鲁棒性未知",
                "通过编辑风格、对象、背景及其组合，构建动态分布外反事实视频测试集",
                "评估显示模型性能显著下降，反事实数据微调可提升鲁棒性，在Ro-Bench和MVBench上分别提高21.73%和12.78%"
            ],
            "tags_zh": [
                "多模态大语言模型",
                "视频理解",
                "鲁棒性评估",
                "反事实数据",
                "分布外测试",
                "基准构建"
            ],
            "_index": 102
        },
        {
            "title": "Defense against Unauthorized Distillation in Image Restoration via Feature Space Perturbation",
            "authors": [
                "Han Hu",
                "Zhuoran Zheng",
                "Chen Lyu"
            ],
            "arxiv_id": "2510.08925v1",
            "summary": "Knowledge distillation (KD) attacks pose a significant threat to deep model\nintellectual property by enabling adversaries to train student networks using a\nteacher model's outputs. While recent defenses in image classification have\nsuccessfully disrupted KD by perturbing output probabilities, extending these\nmethods to image restoration is difficult. Unlike classification, restoration\nis a generative task with continuous, high-dimensional outputs that depend on\nspatial coherence and fine details. Minor perturbations are often insufficient,\nas students can still learn the underlying mapping.To address this, we propose\nAdaptive Singular Value Perturbation (ASVP), a runtime defense tailored for\nimage restoration models. ASVP operates on internal feature maps of the teacher\nusing singular value decomposition (SVD). It amplifies the topk singular values\nto inject structured, high-frequency perturbations, disrupting the alignment\nneeded for distillation. This hinders student learning while preserving the\nteacher's output quality.We evaluate ASVP across five image restoration tasks:\nsuper-resolution, low-light enhancement, underwater enhancement, dehazing, and\nderaining. Experiments show ASVP reduces student PSNR by up to 4 dB and SSIM by\n60-75%, with negligible impact on the teacher's performance. Compared to prior\nmethods, ASVP offers a stronger and more consistent defense.Our approach\nprovides a practical solution to protect open-source restoration models from\nunauthorized knowledge distillation.",
            "headline_zh": "提出自适应奇异值扰动以防御图像恢复模型的知识蒸馏攻击",
            "intro_zh": [
                "知识蒸馏攻击威胁模型知识产权，图像恢复任务因连续高维输出难以防御",
                "ASVP方法在特征空间扰动奇异值，注入高频噪声破坏蒸馏对齐",
                "实验显示ASVP显著降低学生性能，对教师模型影响可忽略"
            ],
            "tags_zh": [
                "图像恢复",
                "知识蒸馏防御",
                "奇异值分解",
                "特征扰动",
                "模型保护"
            ],
            "_index": 103
        },
        {
            "title": "SegTrans: Transferable Adversarial Examples for Segmentation Models",
            "authors": [
                "Yufei Song",
                "Ziqi Zhou",
                "Qi Lu",
                "Hangtao Zhang",
                "Yifan Hu",
                "Lulu Xue",
                "Shengshan Hu",
                "Minghui Li",
                "Leo Yu Zhang"
            ],
            "arxiv_id": "2510.08922v1",
            "summary": "Segmentation models exhibit significant vulnerability to adversarial examples\nin white-box settings, but existing adversarial attack methods often show poor\ntransferability across different segmentation models. While some researchers\nhave explored transfer-based adversarial attack (i.e., transfer attack) methods\nfor segmentation models, the complex contextual dependencies within these\nmodels and the feature distribution gaps between surrogate and target models\nresult in unsatisfactory transfer success rates. To address these issues, we\npropose SegTrans, a novel transfer attack framework that divides the input\nsample into multiple local regions and remaps their semantic information to\ngenerate diverse enhanced samples. These enhanced samples replace the original\nones for perturbation optimization, thereby improving the transferability of\nadversarial examples across different segmentation models. Unlike existing\nmethods, SegTrans only retains local semantic information from the original\ninput, rather than using global semantic information to optimize perturbations.\nExtensive experiments on two benchmark datasets, PASCAL VOC and Cityscapes,\nfour different segmentation models, and three backbone networks show that\nSegTrans significantly improves adversarial transfer success rates without\nintroducing additional computational overhead. Compared to the current\nstate-of-the-art methods, SegTrans achieves an average increase of 8.55% in\ntransfer attack success rate and improves computational efficiency by more than\n100%.",
            "headline_zh": "提出SegTrans框架以提升分割模型间对抗样本的可迁移性",
            "intro_zh": [
                "分割模型在对抗攻击中可迁移性差，源于上下文依赖和特征分布差异",
                "方法将输入划分为局部区域并重映射语义，生成多样增强样本优化扰动",
                "实验在PASCAL VOC和Cityscapes数据集上，显著提高迁移成功率与效率"
            ],
            "tags_zh": [
                "分割模型",
                "对抗攻击",
                "可迁移性",
                "语义重映射",
                "扰动优化"
            ],
            "_index": 104
        },
        {
            "title": "PHyCLIP: $\\ell_1$-Product of Hyperbolic Factors Unifies Hierarchy and Compositionality in Vision-Language Representation Learning",
            "authors": [
                "Daiki Yoshikawa",
                "Takashi Matsubara"
            ],
            "arxiv_id": "2510.08919v1",
            "summary": "Vision-language models have achieved remarkable success in multi-modal\nrepresentation learning from large-scale pairs of visual scenes and linguistic\ndescriptions. However, they still struggle to simultaneously express two\ndistinct types of semantic structures: the hierarchy within a concept family\n(e.g., dog $\\preceq$ mammal $\\preceq$ animal) and the compositionality across\ndifferent concept families (e.g., \"a dog in a car\" $\\preceq$ dog, car). Recent\nworks have addressed this challenge by employing hyperbolic space, which\nefficiently captures tree-like hierarchy, yet its suitability for representing\ncompositionality remains unclear. To resolve this dilemma, we propose PHyCLIP,\nwhich employs an $\\ell_1$-Product metric on a Cartesian product of Hyperbolic\nfactors. With our design, intra-family hierarchies emerge within individual\nhyperbolic factors, and cross-family composition is captured by the\n$\\ell_1$-product metric, analogous to a Boolean algebra. Experiments on\nzero-shot classification, retrieval, hierarchical classification, and\ncompositional understanding tasks demonstrate that PHyCLIP outperforms existing\nsingle-space approaches and offers more interpretable structures in the\nembedding space.",
            "headline_zh": "提出PHyCLIP以统一视觉语言表示中的层次性和组合性",
            "intro_zh": [
                "视觉语言模型难以同时表达概念族内层次和跨族组合性",
                "使用双曲空间ℓ₁-积度量，分别捕获层次和组合结构",
                "在零样本分类等任务中优于单空间方法，嵌入更可解释"
            ],
            "tags_zh": [
                "视觉语言表示学习",
                "双曲空间",
                "ℓ₁-积度量",
                "层次性建模",
                "组合性建模",
                "零样本学习"
            ],
            "_index": 105
        },
        {
            "title": "Modeling Time-Lapse Trajectories to Characterize Cranberry Growth",
            "authors": [
                "Ronan John",
                "Anis Chihoub",
                "Ryan Meegan",
                "Gina Sidelli",
                "Jeffery Neyhart",
                "Peter Oudemans",
                "Kristin Dana"
            ],
            "arxiv_id": "2510.08901v1",
            "summary": "Change monitoring is an essential task for cranberry farming as it provides\nboth breeders and growers with the ability to analyze growth, predict yield,\nand make treatment decisions. However, this task is often done manually,\nrequiring significant time on the part of a cranberry grower or breeder. Deep\nlearning based change monitoring holds promise, despite the caveat of\nhard-to-interpret high dimensional features and hand-annotations for\nfine-tuning. To address this gap, we introduce a method for modeling crop\ngrowth based on fine-tuning vision transformers (ViTs) using a self-supervised\napproach that avoids tedious image annotations. We use a two-fold pretext task\n(time regression and class prediction) to learn a latent space for the\ntime-lapse evolution of plant and fruit appearance. The resulting 2D temporal\ntracks provide an interpretable time-series model of crop growth that can be\nused to: 1) predict growth over time and 2) distinguish temporal differences of\ncranberry varieties. We also provide a novel time-lapse dataset of cranberry\nfruit featuring eight distinct varieties, observed 52 times over the growing\nseason (span of around four months), annotated with information about fungicide\napplication, yield, and rot. Our approach is general and can be applied to\nother crops and applications (code and dataset can be found at https://github.\ncom/ronan-39/tlt/).",
            "headline_zh": "提出基于自监督ViT的时间轨迹建模方法，以自动化监测蔓越莓生长。",
            "intro_zh": [
                "核心问题：蔓越莓生长监测依赖手动，耗时且需大量标注。",
                "方法要点：使用时间回归和类别预测的自监督ViT，学习植物外观的潜在空间。",
                "实验或效果：构建新数据集，模型可预测生长和区分品种时间差异。"
            ],
            "tags_zh": [
                "时间轨迹建模",
                "自监督学习",
                "视觉变换器",
                "作物监测",
                "时间序列分析"
            ],
            "_index": 106
        },
        {
            "title": "Model-Based Lookahead Reinforcement Learning for in-hand manipulation",
            "authors": [
                "Alexandre Lopes",
                "Catarina Barata",
                "Plinio Moreno"
            ],
            "arxiv_id": "2510.08884v1",
            "summary": "In-Hand Manipulation, as many other dexterous tasks, remains a difficult\nchallenge in robotics by combining complex dynamic systems with the capability\nto control and manoeuvre various objects using its actuators. This work\npresents the application of a previously developed hybrid Reinforcement\nLearning (RL) Framework to In-Hand Manipulation task, verifying that it is\ncapable of improving the performance of the task. The model combines concepts\nof both Model-Free and Model-Based Reinforcement Learning, by guiding a trained\npolicy with the help of a dynamic model and value-function through trajectory\nevaluation, as done in Model Predictive Control. This work evaluates the\nperformance of the model by comparing it with the policy that will be guided.\nTo fully explore this, various tests are performed using both fully-actuated\nand under-actuated simulated robotic hands to manipulate different objects for\na given task. The performance of the model will also be tested for\ngeneralization tests, by changing the properties of the objects in which both\nthe policy and dynamic model were trained, such as density and size, and\nadditionally by guiding a trained policy in a certain object to perform the\nsame task in a different one. The results of this work show that, given a\npolicy with high average reward and an accurate dynamic model, the hybrid\nframework improves the performance of in-hand manipulation tasks for most test\ncases, even when the object properties are changed. However, this improvement\ncomes at the expense of increasing the computational cost, due to the\ncomplexity of trajectory evaluation.",
            "headline_zh": "提出混合强化学习框架以提升机器人灵巧操作性能",
            "intro_zh": [
                "核心问题：机器人灵巧操作任务复杂，涉及动态系统与多对象控制挑战。",
                "方法要点：结合无模型与基于模型的强化学习，通过轨迹评估指导策略。",
                "实验或效果：在模拟手部测试中，多数情况下性能提升，但计算成本增加。"
            ],
            "tags_zh": [
                "强化学习",
                "灵巧操作",
                "模型预测控制",
                "机器人仿真",
                "动态模型"
            ],
            "_index": 107
        },
        {
            "title": "Online IMU-odometer Calibration using GNSS Measurements for Autonomous Ground Vehicle Localization",
            "authors": [
                "Baoshan Song",
                "Xiao Xia",
                "Penggao Yan",
                "Yihan Zhong",
                "Weisong Wen",
                "Li-Ta Hsu"
            ],
            "arxiv_id": "2510.08880v1",
            "summary": "Accurate calibration of intrinsic (odometer scaling factors) and extrinsic\nparameters (IMU-odometer translation and rotation) is essential for autonomous\nground vehicle localization. Existing GNSS-aided approaches often rely on\npositioning results or raw measurements without ambiguity resolution, and their\nobservability properties remain underexplored. This paper proposes a tightly\ncoupled online calibration method that fuses IMU, odometer, and raw GNSS\nmeasurements (pseudo-range, carrier-phase, and Doppler) within an extendable\nfactor graph optimization (FGO) framework, incorporating outlier mitigation and\nambiguity resolution. Observability analysis reveals that two horizontal\ntranslation and three rotation parameters are observable under general motion,\nwhile vertical translation remains unobservable. Simulation and real-world\nexperiments demonstrate superior calibration and localization performance over\nstate-of-the-art loosely coupled methods. Specifically, the IMU-odometer\npositioning using our calibrated parameters achieves the absolute maximum error\nof 17.75 m while the one of LC method is 61.51 m, achieving up to 71.14 percent\nimprovement. To foster further research, we also release the first open-source\ndataset that combines IMU, 2D odometer, and raw GNSS measurements from both\nrover and base stations.",
            "headline_zh": "提出在线IMU-里程计标定方法，融合GNSS测量以提升自主地面车辆定位精度",
            "intro_zh": [
                "核心问题：IMU-里程计内外参标定不准确，影响自主车辆定位精度。",
                "方法要点：在可扩展因子图优化框架中，融合IMU、里程计和原始GNSS测量。",
                "实验效果：标定后定位最大误差降至17.75米，比松散耦合方法提升71.14%。"
            ],
            "tags_zh": [
                "自主车辆定位",
                "传感器标定",
                "因子图优化",
                "GNSS融合",
                "在线校准"
            ],
            "_index": 108
        }
    ]
}