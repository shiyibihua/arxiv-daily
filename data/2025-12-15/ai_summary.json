{
    "papers": [
        {
            "title": "DiffusionBrowser: Interactive Diffusion Previews via Multi-Branch Decoders",
            "authors": [
                "Susung Hong",
                "Chongjian Ge",
                "Zhifei Zhang",
                "Jui-Hsien Wang"
            ],
            "arxiv_id": "2512.13690v1",
            "summary": "Video diffusion models have revolutionized generative video synthesis, but they are imprecise, slow, and can be opaque during generation -- keeping users in the dark for a prolonged period. In this work, we propose DiffusionBrowser, a model-agnostic, lightweight decoder framework that allows users to interactively generate previews at any point (timestep or transformer block) during the denoising process. Our model can generate multi-modal preview representations that include RGB and scene intrinsics at more than 4$\\times$ real-time speed (less than 1 second for a 4-second video) that convey consistent appearance and motion to the final video. With the trained decoder, we show that it is possible to interactively guide the generation at intermediate noise steps via stochasticity reinjection and modal steering, unlocking a new control capability. Moreover, we systematically probe the model using the learned decoders, revealing how scene, object, and other details are composed and assembled during the otherwise black-box denoising process.",
            "headline_zh": "提出DiffusionBrowser，通过多分支解码器实现视频扩散模型交互式预览",
            "intro_zh": [
                "视频扩散模型生成慢且不透明，用户需长时间等待",
                "提出轻量级解码器框架，支持在去噪过程任意点生成RGB和场景内在预览",
                "实验显示预览速度超实时4倍，并可通过随机性重注入和模态引导交互控制"
            ],
            "tags_zh": [
                "视频扩散模型",
                "交互式预览",
                "多分支解码器",
                "去噪过程分析",
                "实时生成",
                "模态控制"
            ],
            "_index": 0
        },
        {
            "title": "LitePT: Lighter Yet Stronger Point Transformer",
            "authors": [
                "Yuanwen Yue",
                "Damien Robert",
                "Jianyuan Wang",
                "Sunghwan Hong",
                "Jan Dirk Wegner",
                "Christian Rupprecht",
                "Konrad Schindler"
            ],
            "arxiv_id": "2512.13689v1",
            "summary": "Modern neural architectures for 3D point cloud processing contain both convolutional layers and attention blocks, but the best way to assemble them remains unclear. We analyse the role of different computational blocks in 3D point cloud networks and find an intuitive behaviour: convolution is adequate to extract low-level geometry at high-resolution in early layers, where attention is expensive without bringing any benefits; attention captures high-level semantics and context in low-resolution, deep layers more efficiently. Guided by this design principle, we propose a new, improved 3D point cloud backbone that employs convolutions in early stages and switches to attention for deeper layers. To avoid the loss of spatial layout information when discarding redundant convolution layers, we introduce a novel, training-free 3D positional encoding, PointROPE. The resulting LitePT model has $3.6\\times$ fewer parameters, runs $2\\times$ faster, and uses $2\\times$ less memory than the state-of-the-art Point Transformer V3, but nonetheless matches or even outperforms it on a range of tasks and datasets. Code and models are available at: https://github.com/prs-eth/LitePT.",
            "headline_zh": "提出LitePT，通过早期卷积与深层注意力结合，优化3D点云网络架构。",
            "intro_zh": [
                "分析卷积与注意力在3D点云网络中的角色，发现早期卷积提取低层几何更高效，深层注意力捕获高层语义更优。",
                "引入训练无关的3D位置编码PointROPE，以保留空间布局信息，避免冗余卷积层丢弃。",
                "LitePT相比Point Transformer V3参数减少3.6倍、速度提升2倍、内存使用减半，性能相当或更优。"
            ],
            "tags_zh": [
                "3D点云处理",
                "卷积注意力结合",
                "位置编码",
                "网络架构优化",
                "轻量化模型"
            ],
            "_index": 1
        },
        {
            "title": "Towards Scalable Pre-training of Visual Tokenizers for Generation",
            "authors": [
                "Jingfeng Yao",
                "Yuda Song",
                "Yucong Zhou",
                "Xinggang Wang"
            ],
            "arxiv_id": "2512.13687v1",
            "summary": "The quality of the latent space in visual tokenizers (e.g., VAEs) is crucial for modern generative models. However, the standard reconstruction-based training paradigm produces a latent space that is biased towards low-level information, leading to a foundation flaw: better pixel-level accuracy does not lead to higher-quality generation. This implies that pouring extensive compute into visual tokenizer pre-training translates poorly to improved performance in generation. We identify this as the ``pre-training scaling problem`` and suggest a necessary shift: to be effective for generation, a latent space must concisely represent high-level semantics. We present VTP, a unified visual tokenizer pre-training framework, pioneering the joint optimization of image-text contrastive, self-supervised, and reconstruction losses. Our large-scale study reveals two principal findings: (1) understanding is a key driver of generation, and (2) much better scaling properties, where generative performance scales effectively with compute, parameters, and data allocated to the pretraining of the visual tokenizer. After large-scale pre-training, our tokenizer delivers a competitive profile (78.2 zero-shot accuracy and 0.36 rFID on ImageNet) and 4.1 times faster convergence on generation compared to advanced distillation methods. More importantly, it scales effectively: without modifying standard DiT training specs, solely investing more FLOPS in pretraining VTP achieves 65.8\\% FID improvement in downstream generation, while conventional autoencoder stagnates very early at 1/10 FLOPS. Our pre-trained models are available at https://github.com/MiniMax-AI/VTP.",
            "headline_zh": "提出VTP框架以解决视觉分词器预训练中的扩展性问题，提升生成质量。",
            "intro_zh": [
                "核心问题：传统基于重建的视觉分词器预训练导致潜在空间偏向低级信息，生成性能随计算投入提升有限。",
                "方法要点：VTP框架联合优化图像-文本对比、自监督和重建损失，强调高层语义表示。",
                "实验或效果：大规模预训练后，VTP在生成任务中实现更快收敛和显著性能提升，扩展性优于传统方法。"
            ],
            "tags_zh": [
                "视觉分词器",
                "预训练扩展性",
                "生成模型",
                "语义表示",
                "联合优化"
            ],
            "_index": 2
        },
        {
            "title": "Recurrent Video Masked Autoencoders",
            "authors": [
                "Daniel Zoran",
                "Nikhil Parthasarathy",
                "Yi Yang",
                "Drew A Hudson",
                "Joao Carreira",
                "Andrew Zisserman"
            ],
            "arxiv_id": "2512.13684v1",
            "summary": "We present Recurrent Video Masked-Autoencoders (RVM): a novel video representation learning approach that uses a transformer-based recurrent neural network to aggregate dense image features over time, effectively capturing the spatio-temporal structure of natural video data. RVM learns via an asymmetric masked prediction task requiring only a standard pixel reconstruction objective. This design yields a highly efficient ``generalist'' encoder: RVM achieves competitive performance with state-of-the-art video models (e.g. VideoMAE, V-JEPA) on video-level tasks like action recognition and point/object tracking, while also performing favorably against image models (e.g. DINOv2) on tasks that test geometric and dense spatial understanding. Notably, RVM achieves strong performance in the small-model regime without requiring knowledge distillation, exhibiting up to 30x greater parameter efficiency than competing video masked autoencoders. Moreover, we demonstrate that RVM's recurrent nature allows for stable feature propagation over long temporal horizons with linear computational cost, overcoming some of the limitations of standard spatio-temporal attention-based architectures. Finally, we use qualitative visualizations to highlight that RVM learns rich representations of scene semantics, structure, and motion.",
            "headline_zh": "提出循环视频掩码自编码器，通过循环聚合图像特征高效学习视频时空表示。",
            "intro_zh": [
                "核心问题：视频表示学习需高效捕获时空结构，传统方法计算成本高或依赖复杂目标。",
                "方法要点：使用基于Transformer的循环神经网络，通过非对称掩码预测任务学习，仅需像素重建目标。",
                "实验或效果：在动作识别等任务上媲美先进视频模型，参数效率提升高达30倍，支持长时稳定特征传播。"
            ],
            "tags_zh": [
                "视频表示学习",
                "循环神经网络",
                "掩码自编码器",
                "时空建模",
                "参数效率",
                "长视频理解"
            ],
            "_index": 3
        },
        {
            "title": "I-Scene: 3D Instance Models are Implicit Generalizable Spatial Learners",
            "authors": [
                "Lu Ling",
                "Yunhao Ge",
                "Yichen Sheng",
                "Aniket Bera"
            ],
            "arxiv_id": "2512.13683v1",
            "summary": "Generalization remains the central challenge for interactive 3D scene generation. Existing learning-based approaches ground spatial understanding in limited scene dataset, restricting generalization to new layouts. We instead reprogram a pre-trained 3D instance generator to act as a scene level learner, replacing dataset-bounded supervision with model-centric spatial supervision. This reprogramming unlocks the generator transferable spatial knowledge, enabling generalization to unseen layouts and novel object compositions. Remarkably, spatial reasoning still emerges even when the training scenes are randomly composed objects. This demonstrates that the generator's transferable scene prior provides a rich learning signal for inferring proximity, support, and symmetry from purely geometric cues. Replacing widely used canonical space, we instantiate this insight with a view-centric formulation of the scene space, yielding a fully feed-forward, generalizable scene generator that learns spatial relations directly from the instance model. Quantitative and qualitative results show that a 3D instance generator is an implicit spatial learner and reasoner, pointing toward foundation models for interactive 3D scene understanding and generation. Project page: https://luling06.github.io/I-Scene-project/",
            "headline_zh": "提出I-Scene方法，利用预训练3D实例生成器作为隐式空间学习器，实现交互式3D场景生成中的泛化问题。",
            "intro_zh": [
                "核心问题：交互式3D场景生成面临泛化挑战，现有方法受限于数据集布局，难以适应新布局和对象组合。",
                "方法要点：通过重新编程预训练3D实例生成器，以模型为中心进行空间监督，解锁其可转移空间知识，实现泛化。",
                "实验或效果：在未见布局和新对象组合上展示泛化能力，空间推理从几何线索中涌现，支持接近、支撑和对称推断。"
            ],
            "tags_zh": [
                "3D场景生成",
                "空间泛化",
                "实例模型",
                "隐式学习",
                "交互式理解"
            ],
            "_index": 4
        },
        {
            "title": "LASER: Layer-wise Scale Alignment for Training-Free Streaming 4D Reconstruction",
            "authors": [
                "Tianye Ding",
                "Yiming Xie",
                "Yiqing Liang",
                "Moitreya Chatterjee",
                "Pedro Miraldo",
                "Huaizu Jiang"
            ],
            "arxiv_id": "2512.13680v1",
            "summary": "Recent feed-forward reconstruction models like VGGT and $π^3$ achieve impressive reconstruction quality but cannot process streaming videos due to quadratic memory complexity, limiting their practical deployment. While existing streaming methods address this through learned memory mechanisms or causal attention, they require extensive retraining and may not fully leverage the strong geometric priors of state-of-the-art offline models. We propose LASER, a training-free framework that converts an offline reconstruction model into a streaming system by aligning predictions across consecutive temporal windows. We observe that simple similarity transformation ($\\mathrm{Sim}(3)$) alignment fails due to layer depth misalignment: monocular scale ambiguity causes relative depth scales of different scene layers to vary inconsistently between windows. To address this, we introduce layer-wise scale alignment, which segments depth predictions into discrete layers, computes per-layer scale factors, and propagates them across both adjacent windows and timestamps. Extensive experiments show that LASER achieves state-of-the-art performance on camera pose estimation and point map reconstruction %quality with offline models while operating at 14 FPS with 6 GB peak memory on a RTX A6000 GPU, enabling practical deployment for kilometer-scale streaming videos. Project website: $\\href{https://neu-vi.github.io/LASER/}{\\texttt{https://neu-vi.github.io/LASER/}}$",
            "headline_zh": "提出LASER训练免费框架，通过层尺度对齐实现离线模型到流式4D重建的转换",
            "intro_zh": [
                "核心问题：现有前馈重建模型因二次内存复杂度无法处理流式视频，而流式方法需重训练且未充分利用离线模型几何先验。",
                "方法要点：引入层尺度对齐，将深度预测分段为离散层，计算每层尺度因子并在相邻窗口和时间戳间传播，解决简单相似变换对齐的层深度错位问题。",
                "实验或效果：在相机姿态估计和点云重建上达到先进性能，在RTX A6000 GPU上以14 FPS和6 GB峰值内存运行，支持千米级流式视频部署。"
            ],
            "tags_zh": [
                "流式4D重建",
                "训练免费框架",
                "层尺度对齐",
                "深度预测",
                "相机姿态估计",
                "点云重建"
            ],
            "_index": 5
        },
        {
            "title": "Feedforward 3D Editing via Text-Steerable Image-to-3D",
            "authors": [
                "Ziqi Ma",
                "Hongqiao Chen",
                "Yisong Yue",
                "Georgia Gkioxari"
            ],
            "arxiv_id": "2512.13678v1",
            "summary": "Recent progress in image-to-3D has opened up immense possibilities for design, AR/VR, and robotics. However, to use AI-generated 3D assets in real applications, a critical requirement is the capability to edit them easily. We present a feedforward method, Steer3D, to add text steerability to image-to-3D models, which enables editing of generated 3D assets with language. Our approach is inspired by ControlNet, which we adapt to image-to-3D generation to enable text steering directly in a forward pass. We build a scalable data engine for automatic data generation, and develop a two-stage training recipe based on flow-matching training and Direct Preference Optimization (DPO). Compared to competing methods, Steer3D more faithfully follows the language instruction and maintains better consistency with the original 3D asset, while being 2.4x to 28.5x faster. Steer3D demonstrates that it is possible to add a new modality (text) to steer the generation of pretrained image-to-3D generative models with 100k data. Project website: https://glab-caltech.github.io/steer3d/",
            "headline_zh": "提出Steer3D方法，通过文本引导实现图像到3D生成模型的快速编辑",
            "intro_zh": [
                "核心问题：AI生成3D资产难以用语言轻松编辑，限制实际应用。",
                "方法要点：基于ControlNet适配图像到3D生成，实现前馈式文本引导，结合流匹配训练和DPO优化。",
                "实验或效果：相比竞品，更忠实遵循语言指令，保持原始3D资产一致性，速度提升2.4至28.5倍。"
            ],
            "tags_zh": [
                "图像到3D生成",
                "文本引导编辑",
                "前馈式方法",
                "流匹配训练",
                "直接偏好优化",
                "数据引擎"
            ],
            "_index": 6
        },
        {
            "title": "JoVA: Unified Multimodal Learning for Joint Video-Audio Generation",
            "authors": [
                "Xiaohu Huang",
                "Hao Zhou",
                "Qiangpeng Yang",
                "Shilei Wen",
                "Kai Han"
            ],
            "arxiv_id": "2512.13677v1",
            "summary": "In this paper, we present JoVA, a unified framework for joint video-audio generation. Despite recent encouraging advances, existing methods face two critical limitations. First, most existing approaches can only generate ambient sounds and lack the capability to produce human speech synchronized with lip movements. Second, recent attempts at unified human video-audio generation typically rely on explicit fusion or modality-specific alignment modules, which introduce additional architecture design and weaken the model simplicity of the original transformers. To address these issues, JoVA employs joint self-attention across video and audio tokens within each transformer layer, enabling direct and efficient cross-modal interaction without the need for additional alignment modules. Furthermore, to enable high-quality lip-speech synchronization, we introduce a simple yet effective mouth-area loss based on facial keypoint detection, which enhances supervision on the critical mouth region during training without compromising architectural simplicity. Extensive experiments on benchmarks demonstrate that JoVA outperforms or is competitive with both unified and audio-driven state-of-the-art methods in lip-sync accuracy, speech quality, and overall video-audio generation fidelity. Our results establish JoVA as an elegant framework for high-quality multimodal generation.",
            "headline_zh": "提出JoVA框架，通过联合自注意力和嘴部区域损失，实现高质量视频-音频联合生成与唇语同步。",
            "intro_zh": [
                "现有方法难以生成唇语同步的人类语音，且依赖额外对齐模块增加复杂性。",
                "JoVA采用视频和音频令牌的联合自注意力，无需额外模块，实现高效跨模态交互。",
                "引入基于面部关键点检测的嘴部区域损失，提升唇语同步质量，实验显示在准确性和保真度上优于或竞争于先进方法。"
            ],
            "tags_zh": [
                "视频-音频联合生成",
                "唇语同步",
                "跨模态自注意力",
                "面部关键点检测",
                "多模态学习"
            ],
            "_index": 7
        },
        {
            "title": "Towards Interactive Intelligence for Digital Humans",
            "authors": [
                "Yiyi Cai",
                "Xuangeng Chu",
                "Xiwei Gao",
                "Sitong Gong",
                "Yifei Huang",
                "Caixin Kang",
                "Kunhang Li",
                "Haiyang Liu",
                "Ruicong Liu",
                "Yun Liu",
                "Dianwen Ng",
                "Zixiong Su",
                "Erwin Wu",
                "Yuhan Wu",
                "Dingkun Yan",
                "Tianyu Yan",
                "Chang Zeng",
                "Bo Zheng",
                "You Zhou"
            ],
            "arxiv_id": "2512.13674v1",
            "summary": "We introduce Interactive Intelligence, a novel paradigm of digital human that is capable of personality-aligned expression, adaptive interaction, and self-evolution. To realize this, we present Mio (Multimodal Interactive Omni-Avatar), an end-to-end framework composed of five specialized modules: Thinker, Talker, Face Animator, Body Animator, and Renderer. This unified architecture integrates cognitive reasoning with real-time multimodal embodiment to enable fluid, consistent interaction. Furthermore, we establish a new benchmark to rigorously evaluate the capabilities of interactive intelligence. Extensive experiments demonstrate that our framework achieves superior performance compared to state-of-the-art methods across all evaluated dimensions. Together, these contributions move digital humans beyond superficial imitation toward intelligent interaction.",
            "headline_zh": "提出交互智能范式Mio框架，实现数字人的人格化表达与自适应交互。",
            "intro_zh": [
                "核心问题：数字人缺乏智能交互能力，需超越表面模仿。",
                "方法要点：Mio框架集成五个模块，统一认知推理与多模态实时体现。",
                "实验或效果：新基准评估显示，在多个维度上优于现有方法。"
            ],
            "tags_zh": [
                "数字人",
                "交互智能",
                "多模态框架",
                "自适应交互",
                "基准评估"
            ],
            "_index": 8
        },
        {
            "title": "Directional Textual Inversion for Personalized Text-to-Image Generation",
            "authors": [
                "Kunhee Kim",
                "NaHyeon Park",
                "Kibeom Hong",
                "Hyunjung Shim"
            ],
            "arxiv_id": "2512.13672v1",
            "summary": "Textual Inversion (TI) is an efficient approach to text-to-image personalization but often fails on complex prompts. We trace these failures to embedding norm inflation: learned tokens drift to out-of-distribution magnitudes, degrading prompt conditioning in pre-norm Transformers. Empirically, we show semantics are primarily encoded by direction in CLIP token space, while inflated norms harm contextualization; theoretically, we analyze how large magnitudes attenuate positional information and hinder residual updates in pre-norm blocks. We propose Directional Textual Inversion (DTI), which fixes the embedding magnitude to an in-distribution scale and optimizes only direction on the unit hypersphere via Riemannian SGD. We cast direction learning as MAP with a von Mises-Fisher prior, yielding a constant-direction prior gradient that is simple and efficient to incorporate. Across personalization tasks, DTI improves text fidelity over TI and TI-variants while maintaining subject similarity. Crucially, DTI's hyperspherical parameterization enables smooth, semantically coherent interpolation between learned concepts (slerp), a capability that is absent in standard TI. Our findings suggest that direction-only optimization is a robust and scalable path for prompt-faithful personalization.",
            "headline_zh": "提出方向性文本反转以解决文本到图像个性化中嵌入范数膨胀导致的提示条件退化问题",
            "intro_zh": [
                "核心问题：文本反转在复杂提示下失败，源于嵌入范数膨胀导致预归一化Transformer中提示条件退化",
                "方法要点：固定嵌入范数，通过黎曼SGD在单位超球面上优化方向，采用冯·米塞斯-费舍尔先验简化梯度计算",
                "实验或效果：在个性化任务中提升文本保真度，保持主体相似性，并支持概念间平滑语义插值"
            ],
            "tags_zh": [
                "文本到图像个性化",
                "方向性文本反转",
                "嵌入范数膨胀",
                "黎曼优化",
                "超球面参数化",
                "语义插值"
            ],
            "_index": 9
        },
        {
            "title": "AgentIAD: Tool-Augmented Single-Agent for Industrial Anomaly Detection",
            "authors": [
                "Junwen Miao",
                "Penghui Du",
                "Yi Liu",
                "Yu Wang",
                "Yan Wang"
            ],
            "arxiv_id": "2512.13671v1",
            "summary": "Industrial anomaly detection (IAD) is difficult due to the scarcity of normal reference samples and the subtle, localized nature of many defects. Single-pass vision-language models (VLMs) often overlook small abnormalities and lack explicit mechanisms to compare against canonical normal patterns. We propose AgentIAD, a tool-driven agentic framework that enables multi-stage visual inspection. The agent is equipped with a Perceptive Zoomer (PZ) for localized fine-grained analysis and a Comparative Retriever (CR) for querying normal exemplars when evidence is ambiguous. To teach these inspection behaviors, we construct structured perceptive and comparative trajectories from the MMAD dataset and train the model in two stages: supervised fine-tuning followed by reinforcement learning. A two-part reward design drives this process: a perception reward that supervises classification accuracy, spatial alignment, and type correctness, and a behavior reward that encourages efficient tool use. Together, these components enable the model to refine its judgment through step-wise observation, zooming, and verification. AgentIAD achieves a new state-of-the-art 97.62% classification accuracy on MMAD, surpassing prior MLLM-based approaches while producing transparent and interpretable inspection traces.",
            "headline_zh": "提出AgentIAD工具增强单代理框架，以解决工业异常检测中样本稀缺和缺陷细微的问题。",
            "intro_zh": [
                "核心问题：工业异常检测因正常样本稀缺和缺陷局部细微而困难，单次视觉语言模型易忽略小异常且缺乏显式比较机制。",
                "方法要点：采用工具驱动代理框架，配备感知缩放器和比较检索器，通过监督微调和强化学习两阶段训练，结合感知和行为奖励设计。",
                "实验或效果：在MMAD数据集上达到97.62%分类准确率，超越先前基于MLLM的方法，并产生透明可解释的检测轨迹。"
            ],
            "tags_zh": [
                "工业异常检测",
                "工具增强代理",
                "视觉语言模型",
                "强化学习",
                "多阶段视觉检查",
                "可解释性"
            ],
            "_index": 10
        },
        {
            "title": "NL2SpaTiaL: Generating Geometric Spatio-Temporal Logic Specifications from Natural Language for Manipulation Tasks",
            "authors": [
                "Licheng Luo",
                "Yu Xia",
                "Kaier Liang",
                "Mingyu Cai"
            ],
            "arxiv_id": "2512.13670v1",
            "summary": "Spatio-Temporal Logic (SpaTiaL) offers a principled formalism for expressing geometric spatial requirements-an essential component of robotic manipulation, where object locations, neighborhood relations, pose constraints, and interactions directly determine task success. Yet prior works have largely relied on standard temporal logic (TL), which models only robot trajectories and overlooks object-level interactions. Existing datasets built from randomly generated TL formulas paired with natural-language descriptions therefore cover temporal operators but fail to represent the layered spatial relations that manipulation tasks depend on. To address this gap, we introduce a dataset generation framework that synthesizes SpaTiaL specifications and converts them into natural-language descriptions through a deterministic, semantics-preserving back-translation procedure. This pipeline produces the NL2SpaTiaL dataset, aligning natural language with multi-level spatial relations and temporal objectives to reflect the compositional structure of manipulation tasks. Building on this foundation, we propose a translation-verification framework equipped with a language-based semantic checker that ensures the generated SpaTiaL formulas faithfully encode the semantics specified by the input description. Experiments across a suite of manipulation tasks show that SpaTiaL-based representations yield more interpretable, verifiable, and compositional grounding for instruction following. Project website: https://sites.google.com/view/nl2spatial",
            "headline_zh": "提出NL2SpaTiaL框架，从自然语言生成几何时空逻辑规范以解决机器人操作任务中的空间关系建模问题。",
            "intro_zh": [
                "核心问题：现有方法依赖标准时序逻辑，忽略对象级交互和分层空间关系，导致机器人操作任务建模不足。",
                "方法要点：引入数据集生成框架，合成SpaTiaL规范并通过确定性回译转换为自然语言描述，构建NL2SpaTiaL数据集。",
                "实验或效果：基于SpaTiaL的表示在操作任务中实现更可解释、可验证和组合的指令遵循，提升任务性能。"
            ],
            "tags_zh": [
                "时空逻辑生成",
                "自然语言处理",
                "机器人操作",
                "数据集构建",
                "语义验证"
            ],
            "_index": 11
        },
        {
            "title": "A Scientific Reasoning Model for Organic Synthesis Procedure Generation",
            "authors": [
                "Guoqing Liu",
                "Junren Li",
                "Zihan Zhao",
                "Eray Inanc",
                "Krzysztof Maziarz",
                "Jose Garrido Torres",
                "Victor Garcia Satorras",
                "Shoko Ueda",
                "Christopher M. Bishop",
                "Marwin Segler"
            ],
            "arxiv_id": "2512.13668v1",
            "summary": "Solving computer-aided synthesis planning is essential for enabling fully automated, robot-assisted synthesis workflows and improving the efficiency of drug discovery. A key challenge, however, is bridging the gap between computational route design and practical laboratory execution, particularly the accurate prediction of viable experimental procedures for each synthesis step. In this work, we present QFANG, a scientific reasoning language model capable of generating precise, structured experimental procedures directly from reaction equations, with explicit chain-of-thought reasoning. To develop QFANG, we curated a high-quality dataset comprising 905,990 chemical reactions paired with structured action sequences, extracted and processed from patent literature using large language models. We introduce a Chemistry-Guided Reasoning (CGR) framework that produces chain-of-thought data grounded in chemical knowledge at scale. The model subsequently undergoes supervised fine-tuning to elicit complex chemistry reasoning. Finally, we apply Reinforcement Learning from Verifiable Rewards (RLVR) to further enhance procedural accuracy. Experimental results demonstrate that QFANG outperforms advanced general-purpose reasoning models and nearest-neighbor retrieval baselines, measured by traditional NLP similarity metrics and a chemically aware evaluator using an LLM-as-a-judge. Moreover, QFANG generalizes to certain out-of-domain reaction classes and adapts to variations in laboratory conditions and user-specific constraints. We believe that QFANG's ability to generate high-quality synthesis procedures represents an important step toward bridging the gap between computational synthesis planning and fully automated laboratory synthesis.",
            "headline_zh": "提出QFANG科学推理语言模型，以生成有机合成实验程序，解决计算路线设计与实验室执行间的差距。",
            "intro_zh": [
                "核心问题：计算机辅助合成规划中，准确预测可行实验程序是连接计算设计与实际执行的关键挑战。",
                "方法要点：基于化学知识构建Chemistry-Guided Reasoning框架，通过监督微调和强化学习增强模型推理能力。",
                "实验或效果：QFANG在传统NLP指标和化学感知评估中优于先进通用推理模型，并能泛化到特定域外反应类。"
            ],
            "tags_zh": [
                "有机合成规划",
                "科学推理语言模型",
                "化学引导推理",
                "强化学习",
                "实验程序生成"
            ],
            "_index": 12
        },
        {
            "title": "SEDULity: A Proof-of-Learning Framework for Distributed and Secure Blockchains with Efficient Useful Work",
            "authors": [
                "Weihang Cao",
                "Mustafa Doger",
                "Sennur Ulukus"
            ],
            "arxiv_id": "2512.13666v1",
            "summary": "The security and decentralization of Proof-of-Work (PoW) have been well-tested in existing blockchain systems. However, its tremendous energy waste has raised concerns about sustainability. Proof-of-Useful-Work (PoUW) aims to redirect the meaningless computation to meaningful tasks such as solving machine learning (ML) problems, giving rise to the branch of Proof-of-Learning (PoL). While previous studies have proposed various PoLs, they all, to some degree, suffer from security, decentralization, or efficiency issues. In this paper, we propose a PoL framework that trains ML models efficiently while maintaining blockchain security in a fully distributed manner. We name the framework SEDULity, which stands for a Secure, Efficient, Distributed, and Useful Learning-based blockchain system. Specifically, we encode the template block into the training process and design a useful function that is difficult to solve but relatively easy to verify, as a substitute for the PoW puzzle. We show that our framework is distributed, secure, and efficiently trains ML models. We further demonstrate that the proposed PoL framework can be extended to other types of useful work and design an incentive mechanism to incentivize task verification. We show theoretically that a rational miner is incentivized to train fully honestly with well-designed system parameters. Finally, we present simulation results to demonstrate the performance of our framework and validate our analysis.",
            "headline_zh": "提出SEDULity框架，以高效训练机器学习模型替代工作量证明，实现分布式安全区块链。",
            "intro_zh": [
                "核心问题：现有工作量证明能耗高，有用工作量证明在安全、去中心化或效率方面存在不足。",
                "方法要点：设计基于机器学习的训练过程作为有用函数，替代工作量证明谜题，并编码模板块以确保安全。",
                "实验或效果：理论分析显示理性矿工诚实训练，仿真结果验证框架性能与安全性。"
            ],
            "tags_zh": [
                "区块链安全",
                "机器学习训练",
                "分布式系统",
                "有用工作量证明",
                "激励机制"
            ],
            "_index": 13
        },
        {
            "title": "Grab-3D: Detecting AI-Generated Videos from 3D Geometric Temporal Consistency",
            "authors": [
                "Wenhan Chen",
                "Sezer Karaoglu",
                "Theo Gevers"
            ],
            "arxiv_id": "2512.13665v1",
            "summary": "Recent advances in diffusion-based generation techniques enable AI models to produce highly realistic videos, heightening the need for reliable detection mechanisms. However, existing detection methods provide only limited exploration of the 3D geometric patterns present in generated videos. In this paper, we use vanishing points as an explicit representation of 3D geometry patterns, revealing fundamental discrepancies in geometric consistency between real and AI-generated videos. We introduce Grab-3D, a geometry-aware transformer framework for detecting AI-generated videos based on 3D geometric temporal consistency. To enable reliable evaluation, we construct an AI-generated video dataset of static scenes, allowing stable 3D geometric feature extraction. We propose a geometry-aware transformer equipped with geometric positional encoding, temporal-geometric attention, and an EMA-based geometric classifier head to explicitly inject 3D geometric awareness into temporal modeling. Experiments demonstrate that Grab-3D significantly outperforms state-of-the-art detectors, achieving robust cross-domain generalization to unseen generators.",
            "headline_zh": "提出Grab-3D框架，基于3D几何时间一致性检测AI生成视频",
            "intro_zh": [
                "核心问题：现有方法对AI生成视频中3D几何模式探索有限，需可靠检测机制",
                "方法要点：使用消失点表示3D几何，设计几何感知Transformer，注入几何位置编码和注意力",
                "实验或效果：在静态场景数据集上验证，显著优于现有检测器，具有跨域泛化能力"
            ],
            "tags_zh": [
                "AI生成视频检测",
                "3D几何一致性",
                "Transformer框架",
                "消失点分析",
                "跨域泛化"
            ],
            "_index": 14
        },
        {
            "title": "RoboTracer: Mastering Spatial Trace with Reasoning in Vision-Language Models for Robotics",
            "authors": [
                "Enshen Zhou",
                "Cheng Chi",
                "Yibo Li",
                "Jingkun An",
                "Jiayuan Zhang",
                "Shanyu Rong",
                "Yi Han",
                "Yuheng Ji",
                "Mengzhen Liu",
                "Pengwei Wang",
                "Zhongyuan Wang",
                "Lu Sheng",
                "Shanghang Zhang"
            ],
            "arxiv_id": "2512.13660v1",
            "summary": "Spatial tracing, as a fundamental embodied interaction ability for robots, is inherently challenging as it requires multi-step metric-grounded reasoning compounded with complex spatial referring and real-world metric measurement. However, existing methods struggle with this compositional task. To this end, we propose RoboTracer, a 3D-aware VLM that first achieves both 3D spatial referring and measuring via a universal spatial encoder and a regression-supervised decoder to enhance scale awareness during supervised fine-tuning (SFT). Moreover, RoboTracer advances multi-step metric-grounded reasoning via reinforcement fine-tuning (RFT) with metric-sensitive process rewards, supervising key intermediate perceptual cues to accurately generate spatial traces. To support SFT and RFT training, we introduce TraceSpatial, a large-scale dataset of 30M QA pairs, spanning outdoor/indoor/tabletop scenes and supporting complex reasoning processes (up to 9 steps). We further present TraceSpatial-Bench, a challenging benchmark filling the gap to evaluate spatial tracing. Experimental results show that RoboTracer surpasses baselines in spatial understanding, measuring, and referring, with an average success rate of 79.1%, and also achieves SOTA performance on TraceSpatial-Bench by a large margin, exceeding Gemini-2.5-Pro by 36% accuracy. Notably, RoboTracer can be integrated with various control policies to execute long-horizon, dynamic tasks across diverse robots (UR5, G1 humanoid) in cluttered real-world scenes.",
            "headline_zh": "提出RoboTracer以解决机器人空间追踪中的多步度量推理与空间指代难题",
            "intro_zh": [
                "核心问题：空间追踪需多步度量推理与复杂空间指代，现有方法难以处理。",
                "方法要点：通过通用空间编码器和回归监督解码器增强尺度感知，结合度量敏感奖励进行强化微调。",
                "实验或效果：在TraceSpatial-Bench上超越基线，平均成功率79.1%，优于Gemini-2.5-Pro 36%。"
            ],
            "tags_zh": [
                "空间追踪",
                "视觉语言模型",
                "度量推理",
                "强化微调",
                "机器人控制"
            ],
            "_index": 15
        },
        {
            "title": "Embedding-Based Rankings of Educational Resources based on Learning Outcome Alignment: Benchmarking, Expert Validation, and Learner Performance",
            "authors": [
                "Mohammadreza Molavi",
                "Mohammad Moein",
                "Mohammadreza Tavakoli",
                "Abdolali Faraji",
                "Stefan T. Mol",
                "Gábor Kismihók"
            ],
            "arxiv_id": "2512.13658v1",
            "summary": "As the online learning landscape evolves, the need for personalization is increasingly evident. Although educational resources are burgeoning, educators face challenges selecting materials that both align with intended learning outcomes and address diverse learner needs. Large Language Models (LLMs) are attracting growing interest for their potential to create learning resources that better support personalization, but verifying coverage of intended outcomes still requires human alignment review, which is costly and limits scalability. We propose a framework that supports the cost-effective automation of evaluating alignment between educational resources and intended learning outcomes. Using human-generated materials, we benchmarked LLM-based text-embedding models and found that the most accurate model (Voyage) achieved 79% accuracy in detecting alignment. We then applied the optimal model to LLM-generated resources and, via expert evaluation, confirmed that it reliably assessed correspondence to intended outcomes (83% accuracy). Finally, in a three-group experiment with 360 learners, higher alignment scores were positively related to greater learning performance, chi-squared(2, N = 360) = 15.39, p < 0.001. These findings show that embedding-based alignment scores can facilitate scalable personalization by confirming alignment with learning outcomes, which allows teachers to focus on tailoring content to diverse learner needs.",
            "headline_zh": "提出基于嵌入的框架以自动化评估教育资源与学习成果的对齐，支持个性化学习。",
            "intro_zh": [
                "在线教育资源激增，但人工对齐评估成本高且难以扩展。",
                "基准测试显示Voyage模型在检测对齐时准确率达79%，专家验证确认其可靠性。",
                "学习者实验表明，对齐分数与学习表现正相关，支持可扩展个性化。"
            ],
            "tags_zh": [
                "教育资源对齐",
                "文本嵌入模型",
                "学习成果评估",
                "个性化学习",
                "自动化验证"
            ],
            "_index": 16
        },
        {
            "title": "Large-Language Memorization During the Classification of United States Supreme Court Cases",
            "authors": [
                "John E. Ortega",
                "Dhruv D. Joshi",
                "Matt P. Borkowski"
            ],
            "arxiv_id": "2512.13654v1",
            "summary": "Large-language models (LLMs) have been shown to respond in a variety of ways for classification tasks outside of question-answering. LLM responses are sometimes called \"hallucinations\" since the output is not what is ex pected. Memorization strategies in LLMs are being studied in detail, with the goal of understanding how LLMs respond. We perform a deep dive into a classification task based on United States Supreme Court (SCOTUS) decisions. The SCOTUS corpus is an ideal classification task to study for LLM memory accuracy because it presents significant challenges due to extensive sentence length, complex legal terminology, non-standard structure, and domain-specific vocabulary. Experimentation is performed with the latest LLM fine tuning and retrieval-based approaches, such as parameter-efficient fine-tuning, auto-modeling, and others, on two traditional category-based SCOTUS classification tasks: one with 15 labeled topics and another with 279. We show that prompt-based models with memories, such as DeepSeek, can be more robust than previous BERT-based models on both tasks scoring about 2 points better than previous models not based on prompting.",
            "headline_zh": "研究大语言模型在美国最高法院案例分类中的记忆策略，提升分类准确性",
            "intro_zh": [
                "核心问题：大语言模型在分类任务中可能产生幻觉，需探究其记忆机制以优化响应",
                "方法要点：采用参数高效微调、自动建模等最新技术，结合提示记忆模型如DeepSeek",
                "实验或效果：在15和279个主题的SCOTUS分类任务中，提示记忆模型比传统BERT模型准确率提高约2点"
            ],
            "tags_zh": [
                "大语言模型记忆",
                "最高法院案例分类",
                "参数高效微调",
                "提示记忆模型",
                "法律文本处理"
            ],
            "_index": 17
        },
        {
            "title": "World Models Can Leverage Human Videos for Dexterous Manipulation",
            "authors": [
                "Raktim Gautam Goswami",
                "Amir Bar",
                "David Fan",
                "Tsung-Yen Yang",
                "Gaoyue Zhou",
                "Prashanth Krishnamurthy",
                "Michael Rabbat",
                "Farshad Khorrami",
                "Yann LeCun"
            ],
            "arxiv_id": "2512.13644v1",
            "summary": "Dexterous manipulation is challenging because it requires understanding how subtle hand motion influences the environment through contact with objects. We introduce DexWM, a Dexterous Manipulation World Model that predicts the next latent state of the environment conditioned on past states and dexterous actions. To overcome the scarcity of dexterous manipulation datasets, DexWM is trained on over 900 hours of human and non-dexterous robot videos. To enable fine-grained dexterity, we find that predicting visual features alone is insufficient; therefore, we introduce an auxiliary hand consistency loss that enforces accurate hand configurations. DexWM outperforms prior world models conditioned on text, navigation, and full-body actions, achieving more accurate predictions of future states. DexWM also demonstrates strong zero-shot generalization to unseen manipulation skills when deployed on a Franka Panda arm equipped with an Allegro gripper, outperforming Diffusion Policy by over 50% on average in grasping, placing, and reaching tasks.",
            "headline_zh": "提出DexWM世界模型，利用人类视频解决灵巧操作预测问题。",
            "intro_zh": [
                "核心问题：灵巧操作需理解手部细微运动对物体的接触影响，但数据集稀缺。",
                "方法要点：训练世界模型预测潜在状态，引入手部一致性损失提升精细操作能力。",
                "实验或效果：在未见技能上零样本泛化，部署机器人任务中超越Diffusion Policy。"
            ],
            "tags_zh": [
                "灵巧操作",
                "世界模型",
                "视频训练",
                "手部一致性损失",
                "零样本泛化"
            ],
            "_index": 18
        },
        {
            "title": "From Code to Field: Evaluating the Robustness of Convolutional Neural Networks for Disease Diagnosis in Mango Leaves",
            "authors": [
                "Gabriel Vitorino de Andrade",
                "Saulo Roberto dos Santos",
                "Itallo Patrick Castro Alves da Silva",
                "Emanuel Adler Medeiros Pereira",
                "Erick de Andrade Barboza"
            ],
            "arxiv_id": "2512.13641v1",
            "summary": "The validation and verification of artificial intelligence (AI) models through robustness assessment are essential to guarantee the reliable performance of intelligent systems facing real-world challenges, such as image corruptions including noise, blurring, and weather variations. Despite the global importance of mango (Mangifera indica L.), there is a lack of studies on the robustness of models for the diagnosis of disease in its leaves. This paper proposes a methodology to evaluate convolutional neural networks (CNNs) under adverse conditions. We adapted the MangoLeafDB dataset, generating MangoLeafDB-C with 19 types of artificial corruptions at five severity levels. We conducted a benchmark comparing five architectures: ResNet-50, ResNet-101, VGG-16, Xception, and LCNN (the latter being a lightweight architecture designed specifically for mango leaf diagnosis). The metrics include the F1 score, the corruption error (CE) and the relative mean corruption error (relative mCE). The results show that LCNN outperformed complex models in corruptions that can be present in real-world scenarios such as Defocus Blur, Motion Blur, while also achieving the lowest mCE. Modern architectures (e.g., ResNet-101) exhibited significant performance degradation in corrupted scenarios, despite their high accuracy under ideal conditions. These findings suggest that lightweight and specialized models may be more suitable for real-world applications in edge devices, where robustness and efficiency are critical. The study highlights the need to incorporate robustness assessments in the development of intelligent systems for agriculture, particularly in regions with technological limitations.",
            "headline_zh": "提出评估卷积神经网络在芒果叶病害诊断中鲁棒性的方法，以应对图像损坏场景。",
            "intro_zh": [
                "核心问题：缺乏针对芒果叶病害诊断模型在噪声、模糊等图像损坏下的鲁棒性研究。",
                "方法要点：基于MangoLeafDB数据集生成MangoLeafDB-C，包含19种人工损坏类型和五个严重级别。",
                "实验或效果：比较五种架构，发现轻量级LCNN在真实场景损坏下表现更优，且平均损坏误差最低。"
            ],
            "tags_zh": [
                "卷积神经网络",
                "鲁棒性评估",
                "芒果叶病害诊断",
                "图像损坏",
                "轻量级模型",
                "农业智能系统"
            ],
            "_index": 19
        },
        {
            "title": "Charge: A Comprehensive Novel View Synthesis Benchmark and Dataset to Bind Them All",
            "authors": [
                "Michal Nazarczuk",
                "Thomas Tanay",
                "Arthur Moreau",
                "Zhensong Zhang",
                "Eduardo Pérez-Pellitero"
            ],
            "arxiv_id": "2512.13639v1",
            "summary": "This paper presents a new dataset for Novel View Synthesis, generated from a high-quality, animated film with stunning realism and intricate detail. Our dataset captures a variety of dynamic scenes, complete with detailed textures, lighting, and motion, making it ideal for training and evaluating cutting-edge 4D scene reconstruction and novel view generation models. In addition to high-fidelity RGB images, we provide multiple complementary modalities, including depth, surface normals, object segmentation and optical flow, enabling a deeper understanding of scene geometry and motion. The dataset is organised into three distinct benchmarking scenarios: a dense multi-view camera setup, a sparse camera arrangement, and monocular video sequences, enabling a wide range of experimentation and comparison across varying levels of data sparsity. With its combination of visual richness, high-quality annotations, and diverse experimental setups, this dataset offers a unique resource for pushing the boundaries of view synthesis and 3D vision.",
            "headline_zh": "提出Charge数据集以支持新颖视图合成的多模态基准测试",
            "intro_zh": [
                "核心问题：现有新颖视图合成数据集缺乏高质量动态场景和多样化实验设置",
                "方法要点：基于高质量动画电影构建数据集，提供RGB、深度、法线、分割和光流等多模态数据",
                "实验或效果：组织为密集多视图、稀疏相机和单目视频三种基准场景，促进不同数据稀疏度下的模型评估"
            ],
            "tags_zh": [
                "新颖视图合成",
                "4D场景重建",
                "多模态数据集",
                "动态场景",
                "基准测试"
            ],
            "_index": 20
        },
        {
            "title": "MindDrive: A Vision-Language-Action Model for Autonomous Driving via Online Reinforcement Learning",
            "authors": [
                "Haoyu Fu",
                "Diankun Zhang",
                "Zongchuang Zhao",
                "Jianfeng Cui",
                "Hongwei Xie",
                "Bing Wang",
                "Guang Chen",
                "Dingkang Liang",
                "Xiang Bai"
            ],
            "arxiv_id": "2512.13636v1",
            "summary": "Current Vision-Language-Action (VLA) paradigms in autonomous driving primarily rely on Imitation Learning (IL), which introduces inherent challenges such as distribution shift and causal confusion. Online Reinforcement Learning offers a promising pathway to address these issues through trial-and-error learning. However, applying online reinforcement learning to VLA models in autonomous driving is hindered by inefficient exploration in continuous action spaces. To overcome this limitation, we propose MindDrive, a VLA framework comprising a large language model (LLM) with two distinct sets of LoRA parameters. The one LLM serves as a Decision Expert for scenario reasoning and driving decision-making, while the other acts as an Action Expert that dynamically maps linguistic decisions into feasible trajectories. By feeding trajectory-level rewards back into the reasoning space, MindDrive enables trial-and-error learning over a finite set of discrete linguistic driving decisions, instead of operating directly in a continuous action space. This approach effectively balances optimal decision-making in complex scenarios, human-like driving behavior, and efficient exploration in online reinforcement learning. MindDrive achieves strong closed-loop performance on the challenging Bench2Drive benchmark, with a Driving Score (DS) of 78.04 and a Success Rate (SR) of 55.09%. To the best of our knowledge, this is the first work to demonstrate the effectiveness of online reinforcement learning for the VLA model in autonomous driving.",
            "headline_zh": "提出MindDrive框架，通过在线强化学习解决自动驾驶中VLA模型的探索效率问题",
            "intro_zh": [
                "当前自动驾驶VLA模型依赖模仿学习，存在分布偏移和因果混淆问题",
                "MindDrive采用双LoRA参数LLM，将连续动作空间映射为离散语言决策以优化探索",
                "在Bench2Drive基准上实现驾驶分数78.04%和成功率55.09%的闭环性能"
            ],
            "tags_zh": [
                "自动驾驶",
                "视觉-语言-动作模型",
                "在线强化学习",
                "大语言模型",
                "轨迹规划",
                "探索效率"
            ],
            "_index": 21
        },
        {
            "title": "SCR2-ST: Combine Single Cell with Spatial Transcriptomics for Efficient Active Sampling via Reinforcement Learning",
            "authors": [
                "Junchao Zhu",
                "Ruining Deng",
                "Junlin Guo",
                "Tianyuan Yao",
                "Chongyu Qu",
                "Juming Xiong",
                "Siqi Lu",
                "Zhengyi Lu",
                "Yanfan Zhu",
                "Marilyn Lionts",
                "Yuechen Yang",
                "Yalin Zheng",
                "Yu Wang",
                "Shilin Zhao",
                "Haichun Yang",
                "Yuankai Huo"
            ],
            "arxiv_id": "2512.13635v1",
            "summary": "Spatial transcriptomics (ST) is an emerging technology that enables researchers to investigate the molecular relationships underlying tissue morphology. However, acquiring ST data remains prohibitively expensive, and traditional fixed-grid sampling strategies lead to redundant measurements of morphologically similar or biologically uninformative regions, thus resulting in scarce data that constrain current methods. The well-established single-cell sequencing field, however, could provide rich biological data as an effective auxiliary source to mitigate this limitation. To bridge these gaps, we introduce SCR2-ST, a unified framework that leverages single-cell prior knowledge to guide efficient data acquisition and accurate expression prediction. SCR2-ST integrates a single-cell guided reinforcement learning-based (SCRL) active sampling and a hybrid regression-retrieval prediction network SCR2Net. SCRL combines single-cell foundation model embeddings with spatial density information to construct biologically grounded reward signals, enabling selective acquisition of informative tissue regions under constrained sequencing budgets. SCR2Net then leverages the actively sampled data through a hybrid architecture combining regression-based modeling with retrieval-augmented inference, where a majority cell-type filtering mechanism suppresses noisy matches and retrieved expression profiles serve as soft labels for auxiliary supervision. We evaluated SCR2-ST on three public ST datasets, demonstrating SOTA performance in both sampling efficiency and prediction accuracy, particularly under low-budget scenarios. Code is publicly available at: https://github.com/hrlblab/SCR2ST",
            "headline_zh": "提出SCR2-ST框架，结合单细胞与空间转录组学，通过强化学习实现高效主动采样与表达预测。",
            "intro_zh": [
                "核心问题：空间转录组学数据获取昂贵，固定网格采样导致冗余，限制方法发展。",
                "方法要点：利用单细胞先验知识，通过强化学习指导采样，结合回归-检索网络进行预测。",
                "实验或效果：在三个公共数据集上验证，在低预算场景下实现采样效率和预测准确性的SOTA性能。"
            ],
            "tags_zh": [
                "空间转录组学",
                "单细胞测序",
                "强化学习",
                "主动采样",
                "表达预测",
                "混合网络"
            ],
            "_index": 22
        },
        {
            "title": "Universality of high-dimensional scaling limits of stochastic gradient descent",
            "authors": [
                "Reza Gheissari",
                "Aukosh Jagannath"
            ],
            "arxiv_id": "2512.13634v1",
            "summary": "We consider statistical tasks in high dimensions whose loss depends on the data only through its projection into a fixed-dimensional subspace spanned by the parameter vectors and certain ground truth vectors. This includes classifying mixture distributions with cross-entropy loss with one and two-layer networks, and learning single and multi-index models with one and two-layer networks. When the data is drawn from an isotropic Gaussian mixture distribution, it is known that the evolution of a finite family of summary statistics under stochastic gradient descent converges to an autonomous ordinary differential equation (ODE), as the dimension and sample size go to $\\infty$ and the step size goes to $0$ commensurately. Our main result is that these ODE limits are universal in that this convergence occurs even when the data is drawn from mixtures of product measures provided the first two moments match the corresponding Gaussian distribution and the initialization and ground truth vectors are sufficiently coordinate-delocalized. We complement this by proving two corresponding non-universality results. We provide a simple example where the ODE limits are non-universal if the initialization is coordinate aligned. We also show that the stochastic differential equation limits arising as fluctuations of the summary statistics around their ODE's fixed points are not universal.",
            "headline_zh": "证明高维随机梯度下降的ODE极限在数据分布满足矩匹配和初始化去局部化时具有普适性",
            "intro_zh": [
                "研究高维统计任务中，损失仅依赖于数据在参数向量和真实向量张成子空间上的投影",
                "证明当数据来自满足前两矩匹配的高斯混合分布时，SGD的摘要统计量演化收敛到自治ODE，且该极限在数据为乘积测度混合时仍成立",
                "通过反例展示初始化坐标对齐时ODE极限非普适，且摘要统计量围绕ODE固定点的SDE极限也非普适"
            ],
            "tags_zh": [
                "高维统计",
                "随机梯度下降",
                "ODE极限",
                "普适性",
                "乘积测度",
                "摘要统计量"
            ],
            "_index": 23
        },
        {
            "title": "StutterFuse: Mitigating Modality Collapse in Stuttering Detection with Jaccard-Weighted Metric Learning and Gated Fusion",
            "authors": [
                "Guransh Singh",
                "Md Shah Fahad"
            ],
            "arxiv_id": "2512.13632v1",
            "summary": "Stuttering detection breaks down when disfluencies overlap. Existing parametric models struggle to distinguish complex, simultaneous disfluencies (e.g., a 'block' with a 'prolongation') due to the scarcity of these specific combinations in training data. While Retrieval-Augmented Generation (RAG) has revolutionized NLP by grounding models in external knowledge, this paradigm remains unexplored in pathological speech processing. To bridge this gap, we introduce StutterFuse, the first Retrieval-Augmented Classifier (RAC) for multi-label stuttering detection. By conditioning a Conformer encoder on a non-parametric memory bank of clinical examples, we allow the model to classify by reference rather than memorization. We further identify and solve \"Modality Collapse\", an \"Echo Chamber\" effect where naive retrieval boosts recall but degrades precision. We mitigate this using: (1) SetCon, a Jaccard-Weighted Metric Learning objective that optimizes for multi-label set similarity, and (2) a Gated Mixture-of-Experts fusion strategy that dynamically arbitrates between acoustic evidence and retrieved context. On the SEP-28k dataset, StutterFuse achieves a weighted F1-score of 0.65, outperforming strong baselines and demonstrating remarkable zero-shot cross-lingual generalization.",
            "headline_zh": "提出StutterFuse检索增强分类器，通过Jaccard加权度量学习和门控融合缓解口吃检测中的模态崩溃问题",
            "intro_zh": [
                "核心问题：现有参数化模型难以区分训练数据中稀缺的重叠性口吃现象，且检索增强方法在病理语音处理中尚未探索",
                "方法要点：构建基于临床案例的非参数记忆库，采用Jaccard加权度量学习优化多标签集合相似性，设计门控专家混合融合策略",
                "实验效果：在SEP-28k数据集上获得0.65加权F1分数，表现出优异的零样本跨语言泛化能力"
            ],
            "tags_zh": [
                "口吃检测",
                "检索增强分类",
                "多标签学习",
                "度量学习",
                "门控融合",
                "病理语音处理"
            ],
            "_index": 24
        },
        {
            "title": "Temporal Tokenization Strategies for Event Sequence Modeling with Large Language Models",
            "authors": [
                "Zefang Liu",
                "Nam Nguyen",
                "Yinzhu Quan",
                "Austin Zhang"
            ],
            "arxiv_id": "2512.13618v1",
            "summary": "Representing continuous time is a critical and under-explored challenge in modeling temporal event sequences with large language models (LLMs). Various strategies like byte-level representations or calendar tokens have been proposed. However, the optimal approach remains unclear, especially given the diverse statistical distributions of real-world event data, which range from smooth log-normal to discrete, spiky patterns. This paper presents the first empirical study of temporal tokenization for event sequences, comparing distinct encoding strategies: naive numeric strings, high-precision byte-level representations, human-semantic calendar tokens, classic uniform binning, and adaptive residual scalar quantization. We evaluate these strategies by fine-tuning LLMs on real-world datasets that exemplify these diverse distributions. Our analysis reveals that no single strategy is universally superior; instead, prediction performance depends heavily on aligning the tokenizer with the data's statistical properties, with log-based strategies excelling on skewed distributions and human-centric formats proving robust for mixed modalities.",
            "headline_zh": "比较多种时间标记化策略以优化大语言模型在事件序列建模中的性能",
            "intro_zh": [
                "核心问题：连续时间表示是事件序列建模中的关键挑战，现有策略如字节级表示或日历标记的优劣未知。",
                "方法要点：首次实证研究五种时间标记化策略，包括朴素数字字符串、高精度字节级表示、人类语义日历标记、经典均匀分箱和自适应残差标量量化。",
                "实验或效果：在真实数据集上微调大语言模型，发现性能取决于标记化策略与数据统计特性的对齐，无单一最优策略。"
            ],
            "tags_zh": [
                "时间标记化",
                "事件序列建模",
                "大语言模型",
                "连续时间表示",
                "统计分布对齐"
            ],
            "_index": 25
        },
        {
            "title": "LightTopoGAT: Enhancing Graph Attention Networks with Topological Features for Efficient Graph Classification",
            "authors": [
                "Ankit Sharma",
                "Sayan Roy Gupta"
            ],
            "arxiv_id": "2512.13617v1",
            "summary": "Graph Neural Networks have demonstrated significant success in graph classification tasks, yet they often require substantial computational resources and struggle to capture global graph properties effectively. We introduce LightTopoGAT, a lightweight graph attention network that enhances node features through topological augmentation by incorporating node degree and local clustering coefficient to improve graph representation learning. The proposed approach maintains parameter efficiency through streamlined attention mechanisms while integrating structural information that is typically overlooked by local message passing schemes. Through comprehensive experiments on three benchmark datasets, MUTAG, ENZYMES, and PROTEINS, we show that LightTopoGAT achieves superior performance compared to established baselines including GCN, GraphSAGE, and standard GAT, with a 6.6 percent improvement in accuracy on MUTAG and a 2.2 percent improvement on PROTEINS. Ablation studies further confirm that these performance gains arise directly from the inclusion of topological features, demonstrating a simple yet effective strategy for enhancing graph neural network performance without increasing architectural complexity.",
            "headline_zh": "提出LightTopoGAT，通过拓扑特征增强图注意力网络以提升图分类效率。",
            "intro_zh": [
                "核心问题：图神经网络计算资源需求大且难以有效捕获全局图属性。",
                "方法要点：引入节点度和局部聚类系数进行拓扑增强，保持参数效率的注意力机制。",
                "实验或效果：在MUTAG和PROTEINS数据集上准确率分别提升6.6%和2.2%，性能增益源于拓扑特征。"
            ],
            "tags_zh": [
                "图神经网络",
                "图分类",
                "拓扑特征",
                "注意力机制",
                "轻量化模型"
            ],
            "_index": 26
        },
        {
            "title": "Do-Undo: Generating and Reversing Physical Actions in Vision-Language Models",
            "authors": [
                "Shweta Mahajan",
                "Shreya Kadambi",
                "Hoang Le",
                "Munawar Hayat",
                "Fatih Porikli"
            ],
            "arxiv_id": "2512.13609v1",
            "summary": "We introduce the Do-Undo task and benchmark to address a critical gap in vision-language models: understanding and generating physically plausible scene transformations driven by real-world actions. Unlike prior work focused on object-level edits, Do-Undo requires models to simulate the outcome of a physical action and then accurately reverse it, reflecting true cause-and-effect in the visual world. We curate a large-scale dataset of reversible actions from real-world videos and design a training strategy enforcing consistency for robust action grounding. Our experiments reveal that current models struggle with physical reversibility, underscoring the importance of this task for embodied AI, robotics, and physics-aware generative modeling. Do-Undo establishes an intuitive testbed for evaluating and advancing physical reasoning in multimodal systems.",
            "headline_zh": "提出Do-Undo任务与基准，以解决视觉语言模型在物理动作理解与生成中的可逆性挑战。",
            "intro_zh": [
                "核心问题：现有模型缺乏对物理动作可逆性的理解，难以模拟真实世界中的因果变换。",
                "方法要点：构建大规模可逆动作数据集，设计训练策略强化动作一致性与物理合理性。",
                "实验或效果：实验显示当前模型在物理可逆任务上表现不佳，突显该任务对具身AI和物理感知生成的重要性。"
            ],
            "tags_zh": [
                "物理动作理解",
                "可逆性基准",
                "视觉语言模型",
                "具身AI",
                "物理感知生成"
            ],
            "_index": 27
        },
        {
            "title": "DBT-DINO: Towards Foundation model based analysis of Digital Breast Tomosynthesis",
            "authors": [
                "Felix J. Dorfner",
                "Manon A. Dorster",
                "Ryan Connolly",
                "Oscar Gentilhomme",
                "Edward Gibbs",
                "Steven Graham",
                "Seth Wander",
                "Thomas Schultz",
                "Manisha Bahl",
                "Dania Daye",
                "Albert E. Kim",
                "Christopher P. Bridge"
            ],
            "arxiv_id": "2512.13608v1",
            "summary": "Foundation models have shown promise in medical imaging but remain underexplored for three-dimensional imaging modalities. No foundation model currently exists for Digital Breast Tomosynthesis (DBT), despite its use for breast cancer screening.\n  To develop and evaluate a foundation model for DBT (DBT-DINO) across multiple clinical tasks and assess the impact of domain-specific pre-training.\n  Self-supervised pre-training was performed using the DINOv2 methodology on over 25 million 2D slices from 487,975 DBT volumes from 27,990 patients. Three downstream tasks were evaluated: (1) breast density classification using 5,000 screening exams; (2) 5-year risk of developing breast cancer using 106,417 screening exams; and (3) lesion detection using 393 annotated volumes.\n  For breast density classification, DBT-DINO achieved an accuracy of 0.79 (95\\% CI: 0.76--0.81), outperforming both the MetaAI DINOv2 baseline (0.73, 95\\% CI: 0.70--0.76, p<.001) and DenseNet-121 (0.74, 95\\% CI: 0.71--0.76, p<.001). For 5-year breast cancer risk prediction, DBT-DINO achieved an AUROC of 0.78 (95\\% CI: 0.76--0.80) compared to DINOv2's 0.76 (95\\% CI: 0.74--0.78, p=.57). For lesion detection, DINOv2 achieved a higher average sensitivity of 0.67 (95\\% CI: 0.60--0.74) compared to DBT-DINO with 0.62 (95\\% CI: 0.53--0.71, p=.60). DBT-DINO demonstrated better performance on cancerous lesions specifically with a detection rate of 78.8\\% compared to Dinov2's 77.3\\%.\n  Using a dataset of unprecedented size, we developed DBT-DINO, the first foundation model for DBT. DBT-DINO demonstrated strong performance on breast density classification and cancer risk prediction. However, domain-specific pre-training showed variable benefits on the detection task, with ImageNet baseline outperforming DBT-DINO on general lesion detection, indicating that localized detection tasks require further methodological development.",
            "headline_zh": "提出DBT-DINO基础模型，用于数字乳腺断层合成影像分析，提升临床任务性能。",
            "intro_zh": [
                "核心问题：数字乳腺断层合成缺乏基础模型，影响三维医学影像分析。",
                "方法要点：基于DINOv2自监督预训练，使用超过2500万张二维切片构建模型。",
                "实验效果：在乳腺密度分类和癌症风险预测任务中表现优于基线，但病灶检测任务效果有限。"
            ],
            "tags_zh": [
                "数字乳腺断层合成",
                "基础模型",
                "自监督学习",
                "医学影像分析",
                "乳腺癌筛查"
            ],
            "_index": 28
        },
        {
            "title": "Nemotron-Cascade: Scaling Cascaded Reinforcement Learning for General-Purpose Reasoning Models",
            "authors": [
                "Boxin Wang",
                "Chankyu Lee",
                "Nayeon Lee",
                "Sheng-Chieh Lin",
                "Wenliang Dai",
                "Yang Chen",
                "Yangyi Chen",
                "Zhuolin Yang",
                "Zihan Liu",
                "Mohammad Shoeybi",
                "Bryan Catanzaro",
                "Wei Ping"
            ],
            "arxiv_id": "2512.13607v1",
            "summary": "Building general-purpose reasoning models with reinforcement learning (RL) entails substantial cross-domain heterogeneity, including large variation in inference-time response lengths and verification latency. Such variability complicates the RL infrastructure, slows training, and makes training curriculum (e.g., response length extension) and hyperparameter selection challenging. In this work, we propose cascaded domain-wise reinforcement learning (Cascade RL) to develop general-purpose reasoning models, Nemotron-Cascade, capable of operating in both instruct and deep thinking modes. Departing from conventional approaches that blend heterogeneous prompts from different domains, Cascade RL orchestrates sequential, domain-wise RL, reducing engineering complexity and delivering state-of-the-art performance across a wide range of benchmarks. Notably, RLHF for alignment, when used as a pre-step, boosts the model's reasoning ability far beyond mere preference optimization, and subsequent domain-wise RLVR stages rarely degrade the benchmark performance attained in earlier domains and may even improve it (see an illustration in Figure 1). Our 14B model, after RL, outperforms its SFT teacher, DeepSeek-R1-0528, on LiveCodeBench v5/v6/Pro and achieves silver-medal performance in the 2025 International Olympiad in Informatics (IOI). We transparently share our training and data recipes.",
            "headline_zh": "提出级联域强化学习以构建通用推理模型，解决跨域异质性挑战。",
            "intro_zh": [
                "核心问题：跨域异质性如响应长度和验证延迟差异，使RL基础设施复杂化并阻碍训练。",
                "方法要点：采用级联域强化学习，按域顺序训练，降低工程复杂度并提升性能。",
                "实验或效果：14B模型在RL后超越SFT教师，在多个基准和IOI中表现优异。"
            ],
            "tags_zh": [
                "级联强化学习",
                "通用推理模型",
                "跨域异质性",
                "RLHF对齐",
                "基准测试",
                "模型训练"
            ],
            "_index": 29
        },
        {
            "title": "LongVie 2: Multimodal Controllable Ultra-Long Video World Model",
            "authors": [
                "Jianxiong Gao",
                "Zhaoxi Chen",
                "Xian Liu",
                "Junhao Zhuang",
                "Chengming Xu",
                "Jianfeng Feng",
                "Yu Qiao",
                "Yanwei Fu",
                "Chenyang Si",
                "Ziwei Liu"
            ],
            "arxiv_id": "2512.13604v1",
            "summary": "Building video world models upon pretrained video generation systems represents an important yet challenging step toward general spatiotemporal intelligence. A world model should possess three essential properties: controllability, long-term visual quality, and temporal consistency. To this end, we take a progressive approach-first enhancing controllability and then extending toward long-term, high-quality generation. We present LongVie 2, an end-to-end autoregressive framework trained in three stages: (1) Multi-modal guidance, which integrates dense and sparse control signals to provide implicit world-level supervision and improve controllability; (2) Degradation-aware training on the input frame, bridging the gap between training and long-term inference to maintain high visual quality; and (3) History-context guidance, which aligns contextual information across adjacent clips to ensure temporal consistency. We further introduce LongVGenBench, a comprehensive benchmark comprising 100 high-resolution one-minute videos covering diverse real-world and synthetic environments. Extensive experiments demonstrate that LongVie 2 achieves state-of-the-art performance in long-range controllability, temporal coherence, and visual fidelity, and supports continuous video generation lasting up to five minutes, marking a significant step toward unified video world modeling.",
            "headline_zh": "提出LongVie 2以解决长视频世界模型的可控性、视觉质量和时间一致性问题",
            "intro_zh": [
                "核心问题：构建视频世界模型需兼顾可控性、长期视觉质量和时间一致性",
                "方法要点：采用三阶段自回归训练，集成多模态引导、退化感知训练和历史上下文引导",
                "实验或效果：在LongVGenBench上实现先进性能，支持长达五分钟的连续视频生成"
            ],
            "tags_zh": [
                "长视频世界模型",
                "多模态可控生成",
                "自回归框架",
                "时间一致性",
                "视觉质量保持"
            ],
            "_index": 30
        },
        {
            "title": "DA-SSL: self-supervised domain adaptor to leverage foundational models in turbt histopathology slides",
            "authors": [
                "Haoyue Zhang",
                "Meera Chappidi",
                "Erolcan Sayar",
                "Helen Richards",
                "Zhijun Chen",
                "Lucas Liu",
                "Roxanne Wadia",
                "Peter A Humphrey",
                "Fady Ghali",
                "Alberto Contreras-Sanz",
                "Peter Black",
                "Jonathan Wright",
                "Stephanie Harmon",
                "Michael Haffner"
            ],
            "arxiv_id": "2512.13600v1",
            "summary": "Recent deep learning frameworks in histopathology, particularly multiple instance learning (MIL) combined with pathology foundational models (PFMs), have shown strong performance. However, PFMs exhibit limitations on certain cancer or specimen types due to domain shifts - these cancer types were rarely used for pretraining or specimens contain tissue-based artifacts rarely seen within the pretraining population. Such is the case for transurethral resection of bladder tumor (TURBT), which are essential for diagnosing muscle-invasive bladder cancer (MIBC), but contain fragmented tissue chips and electrocautery artifacts and were not widely used in publicly available PFMs. To address this, we propose a simple yet effective domain-adaptive self-supervised adaptor (DA-SSL) that realigns pretrained PFM features to the TURBT domain without fine-tuning the foundational model itself. We pilot this framework for predicting treatment response in TURBT, where histomorphological features are currently underutilized and identifying patients who will benefit from neoadjuvant chemotherapy (NAC) is challenging. In our multi-center study, DA-SSL achieved an AUC of 0.77+/-0.04 in five-fold cross-validation and an external test accuracy of 0.84, sensitivity of 0.71, and specificity of 0.91 using majority voting. Our results demonstrate that lightweight domain adaptation with self-supervision can effectively enhance PFM-based MIL pipelines for clinically challenging histopathology tasks. Code is Available at https://github.com/zhanghaoyue/DA_SSL_TURBT.",
            "headline_zh": "提出DA-SSL自监督域适配器，以提升病理基础模型在TURBT切片中的性能。",
            "intro_zh": [
                "核心问题：病理基础模型在TURBT切片上因域偏移和伪影导致性能受限。",
                "方法要点：通过自监督域适配对齐特征，无需微调基础模型。",
                "实验或效果：在多中心研究中，DA-SSL在预测治疗响应任务中取得高AUC和准确率。"
            ],
            "tags_zh": [
                "自监督学习",
                "域适应",
                "病理学基础模型",
                "多实例学习",
                "膀胱癌诊断"
            ],
            "_index": 31
        },
        {
            "title": "Textual Gradients are a Flawed Metaphor for Automatic Prompt Optimization",
            "authors": [
                "Daniel Melcer",
                "Qi Chen",
                "Wen-Hao Chiang",
                "Shweta Garg",
                "Pranav Garg",
                "Christian Bock"
            ],
            "arxiv_id": "2512.13598v1",
            "summary": "A well-engineered prompt can increase the performance of large language models; automatic prompt optimization techniques aim to increase performance without requiring human effort to tune the prompts. One leading class of prompt optimization techniques introduces the analogy of textual gradients. We investigate the behavior of these textual gradient methods through a series of experiments and case studies. While such methods often result in a performance improvement, our experiments suggest that the gradient analogy does not accurately explain their behavior. Our insights may inform the selection of prompt optimization strategies, and development of new approaches.",
            "headline_zh": "揭示文本梯度类比在自动提示优化中的局限性，为策略选择提供依据",
            "intro_zh": [
                "核心问题：文本梯度类比是否能准确解释自动提示优化方法的行为",
                "方法要点：通过实验和案例研究分析文本梯度方法的实际表现",
                "实验或效果：发现梯度类比不准确，但方法常能提升性能"
            ],
            "tags_zh": [
                "自动提示优化",
                "文本梯度",
                "大语言模型",
                "性能提升",
                "实验分析"
            ],
            "_index": 32
        },
        {
            "title": "Lighting in Motion: Spatiotemporal HDR Lighting Estimation",
            "authors": [
                "Christophe Bolduc",
                "Julien Philip",
                "Li Ma",
                "Mingming He",
                "Paul Debevec",
                "Jean-François Lalonde"
            ],
            "arxiv_id": "2512.13597v1",
            "summary": "We present Lighting in Motion (LiMo), a diffusion-based approach to spatiotemporal lighting estimation. LiMo targets both realistic high-frequency detail prediction and accurate illuminance estimation. To account for both, we propose generating a set of mirrored and diffuse spheres at different exposures, based on their 3D positions in the input. Making use of diffusion priors, we fine-tune powerful existing diffusion models on a large-scale customized dataset of indoor and outdoor scenes, paired with spatiotemporal light probes. For accurate spatial conditioning, we demonstrate that depth alone is insufficient and we introduce a new geometric condition to provide the relative position of the scene to the target 3D position. Finally, we combine diffuse and mirror predictions at different exposures into a single HDRI map leveraging differentiable rendering. We thoroughly evaluate our method and design choices to establish LiMo as state-of-the-art for both spatial control and prediction accuracy.",
            "headline_zh": "提出LiMo扩散模型，用于时空高动态范围光照估计，提升细节与照度准确性。",
            "intro_zh": [
                "核心问题：时空光照估计需兼顾高频细节与照度准确性，现有方法空间控制不足。",
                "方法要点：基于扩散先验，生成多曝光球体，引入新几何条件增强空间控制。",
                "实验或效果：在定制数据集上评估，LiMo在空间控制与预测精度上达到先进水平。"
            ],
            "tags_zh": [
                "时空光照估计",
                "扩散模型",
                "高动态范围成像",
                "几何条件",
                "可微分渲染"
            ],
            "_index": 33
        },
        {
            "title": "Scalable Formal Verification via Autoencoder Latent Space Abstraction",
            "authors": [
                "Robert Reed",
                "Morteza Lahijanian",
                "Luca Laurenti"
            ],
            "arxiv_id": "2512.13593v1",
            "summary": "Finite Abstraction methods provide a powerful formal framework for proving that systems satisfy their specifications. However, these techniques face scalability challenges for high-dimensional systems, as they rely on state-space discretization which grows exponentially with dimension. Learning-based approaches to dimensionality reduction, utilizing neural networks and autoencoders, have shown great potential to alleviate this problem. However, ensuring the correctness of the resulting verification results remains an open question. In this work, we provide a formal approach to reduce the dimensionality of systems via convex autoencoders and learn the dynamics in the latent space through a kernel-based method. We then construct a finite abstraction from the learned model in the latent space and guarantee that the abstraction contains the true behaviors of the original system. We show that the verification results in the latent space can be mapped back to the original system. Finally, we demonstrate the effectiveness of our approach on multiple systems, including a 26D system controlled by a neural network, showing significant scalability improvements without loss of rigor.",
            "headline_zh": "提出基于凸自编码器潜在空间抽象的可扩展形式验证方法，以解决高维系统验证的扩展性问题。",
            "intro_zh": [
                "核心问题：高维系统形式验证面临状态空间离散化导致的指数级扩展性挑战。",
                "方法要点：使用凸自编码器降维，基于核方法学习潜在空间动态，并构建包含原始系统行为的有限抽象。",
                "实验或效果：在包括26D神经网络控制系统在内的多个系统上验证，显著提升扩展性而不失严谨性。"
            ],
            "tags_zh": [
                "形式验证",
                "自编码器",
                "潜在空间抽象",
                "可扩展性",
                "高维系统",
                "核方法"
            ],
            "_index": 34
        },
        {
            "title": "Image Diffusion Preview with Consistency Solver",
            "authors": [
                "Fu-Yun Wang",
                "Hao Zhou",
                "Liangzhe Yuan",
                "Sanghyun Woo",
                "Boqing Gong",
                "Bohyung Han",
                "Ming-Hsuan Yang",
                "Han Zhang",
                "Yukun Zhu",
                "Ting Liu",
                "Long Zhao"
            ],
            "arxiv_id": "2512.13592v1",
            "summary": "The slow inference process of image diffusion models significantly degrades interactive user experiences. To address this, we introduce Diffusion Preview, a novel paradigm employing rapid, low-step sampling to generate preliminary outputs for user evaluation, deferring full-step refinement until the preview is deemed satisfactory. Existing acceleration methods, including training-free solvers and post-training distillation, struggle to deliver high-quality previews or ensure consistency between previews and final outputs. We propose ConsistencySolver derived from general linear multistep methods, a lightweight, trainable high-order solver optimized via Reinforcement Learning, that enhances preview quality and consistency. Experimental results demonstrate that ConsistencySolver significantly improves generation quality and consistency in low-step scenarios, making it ideal for efficient preview-and-refine workflows. Notably, it achieves FID scores on-par with Multistep DPM-Solver using 47% fewer steps, while outperforming distillation baselines. Furthermore, user studies indicate our approach reduces overall user interaction time by nearly 50% while maintaining generation quality. Code is available at https://github.com/G-U-N/consolver.",
            "headline_zh": "提出ConsistencySolver以加速图像扩散模型的预览生成，提升预览质量与一致性。",
            "intro_zh": [
                "核心问题：图像扩散模型推理慢，影响交互体验，现有方法难以保证预览质量与最终输出的一致性。",
                "方法要点：基于通用线性多步方法，设计轻量可训练高阶求解器，通过强化学习优化预览质量和一致性。",
                "实验或效果：在低步数场景下显著提升生成质量和一致性，减少用户交互时间近50%，代码已开源。"
            ],
            "tags_zh": [
                "图像扩散模型",
                "预览生成",
                "一致性求解器",
                "强化学习优化",
                "交互加速"
            ],
            "_index": 35
        },
        {
            "title": "ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding",
            "authors": [
                "Jia-Nan Li",
                "Jian Guan",
                "Wei Wu",
                "Chongxuan Li"
            ],
            "arxiv_id": "2512.13586v1",
            "summary": "Autoregressive models (ARMs) are hindered by slow sequential inference. While masked diffusion models (MDMs) offer a parallel alternative, they suffer from critical drawbacks: high computational overhead from precluding Key-Value (KV) caching, and incoherent generation arising from learning dependencies over an intractable space of token combinations. To address these limitations, we introduce ReFusion, a novel masked diffusion model that achieves superior performance and efficiency by elevating parallel decoding from the token level to a higher slot level, where each slot is a fixed-length, contiguous sub-sequence. This is achieved through an iterative ``plan-and-infill'' decoding process: a diffusion-based planning step first identifies a set of weakly dependent slots, and an autoregressive infilling step then decodes these selected slots in parallel. The slot-based design simultaneously unlocks full KV cache reuse with a unified causal framework and reduces the learning complexity from the token combination space to a manageable slot-level permutation space. Extensive experiments on seven diverse benchmarks show that ReFusion not only overwhelmingly surpasses prior MDMs with 34% performance gains and an over 18$\\times$ speedup on average, but also bridges the performance gap to strong ARMs while maintaining a 2.33$\\times$ average speedup.",
            "headline_zh": "提出ReFusion以解决掩码扩散模型在并行解码中的计算与生成一致性问题",
            "intro_zh": [
                "核心问题：掩码扩散模型存在高计算开销和生成不连贯的缺陷，阻碍并行解码效率。",
                "方法要点：通过槽级并行解码，采用规划-填充策略，在槽级别进行扩散规划和自回归填充。",
                "实验或效果：在七个基准测试中，性能提升34%，速度平均加速18倍以上，接近自回归模型性能。"
            ],
            "tags_zh": [
                "并行解码",
                "掩码扩散模型",
                "自回归模型",
                "槽级规划",
                "KV缓存重用",
                "语言模型加速"
            ],
            "_index": 36
        },
        {
            "title": "DP-CSGP: Differentially Private Stochastic Gradient Push with Compressed Communication",
            "authors": [
                "Zehan Zhu",
                "Heng Zhao",
                "Yan Huang",
                "Joey Tianyi Zhou",
                "Shouling Ji",
                "Jinming Xu"
            ],
            "arxiv_id": "2512.13583v1",
            "summary": "In this paper, we propose a Differentially Private Stochastic Gradient Push with Compressed communication (termed DP-CSGP) for decentralized learning over directed graphs. Different from existing works, the proposed algorithm is designed to maintain high model utility while ensuring both rigorous differential privacy (DP) guarantees and efficient communication. For general non-convex and smooth objective functions, we show that the proposed algorithm achieves a tight utility bound of $\\mathcal{O}\\left( \\sqrt{d\\log \\left( \\frac{1}δ \\right)}/(\\sqrt{n}Jε) \\right)$ ($J$ and $d$ are the number of local samples and the dimension of decision variables, respectively) with $\\left(ε, δ\\right)$-DP guarantee for each node, matching that of decentralized counterparts with exact communication. Extensive experiments on benchmark tasks show that, under the same privacy budget, DP-CSGP achieves comparable model accuracy with significantly lower communication cost than existing decentralized counterparts with exact communication.",
            "headline_zh": "提出DP-CSGP算法，在定向图去中心化学习中实现差分隐私与压缩通信的高效结合。",
            "intro_zh": [
                "针对去中心化学习在定向图中兼顾隐私保护与通信效率的挑战。",
                "通过差分隐私随机梯度推送与压缩通信技术，在非凸平滑目标下保持紧致效用界。",
                "实验表明，在相同隐私预算下，相比精确通信方法，显著降低通信成本并保持模型精度。"
            ],
            "tags_zh": [
                "差分隐私",
                "去中心化学习",
                "压缩通信",
                "随机梯度推送",
                "定向图",
                "非凸优化"
            ],
            "_index": 37
        },
        {
            "title": "MMhops-R1: Multimodal Multi-hop Reasoning",
            "authors": [
                "Tao Zhang",
                "Ziqi Zhang",
                "Zongyang Ma",
                "Yuxin Chen",
                "Bing Li",
                "Chunfeng Yuan",
                "Guangting Wang",
                "Fengyun Rao",
                "Ying Shan",
                "Weiming Hu"
            ],
            "arxiv_id": "2512.13573v1",
            "summary": "The ability to perform multi-modal multi-hop reasoning by iteratively integrating information across various modalities and external knowledge is critical for addressing complex real-world challenges. However, existing Multi-modal Large Language Models (MLLMs) are predominantly limited to single-step reasoning, as existing benchmarks lack the complexity needed to evaluate and drive multi-hop abilities. To bridge this gap, we introduce MMhops, a novel, large-scale benchmark designed to systematically evaluate and foster multi-modal multi-hop reasoning. MMhops dataset comprises two challenging task formats, Bridging and Comparison, which necessitate that models dynamically construct complex reasoning chains by integrating external knowledge. To tackle the challenges posed by MMhops, we propose MMhops-R1, a novel multi-modal Retrieval-Augmented Generation (mRAG) framework for dynamic reasoning. Our framework utilizes reinforcement learning to optimize the model for autonomously planning reasoning paths, formulating targeted queries, and synthesizing multi-level information. Comprehensive experiments demonstrate that MMhops-R1 significantly outperforms strong baselines on MMhops, highlighting that dynamic planning and multi-modal knowledge integration are crucial for complex reasoning. Moreover, MMhops-R1 demonstrates strong generalization to tasks requiring fixed-hop reasoning, underscoring the robustness of our dynamic planning approach. In conclusion, our work contributes a challenging new benchmark and a powerful baseline model, and we will release the associated code, data, and weights to catalyze future research in this critical area.",
            "headline_zh": "提出MMhops-R1多模态检索增强生成框架以解决多模态多跳推理的复杂挑战",
            "intro_zh": [
                "现有MLLMs多限于单步推理，缺乏评估多跳能力的复杂基准",
                "提出MMhops基准和MMhops-R1框架，利用强化学习优化动态推理路径规划",
                "实验显示MMhops-R1显著优于基线，并展示对固定跳数任务的强泛化能力"
            ],
            "tags_zh": [
                "多模态多跳推理",
                "检索增强生成",
                "强化学习",
                "动态规划",
                "基准数据集",
                "知识集成"
            ],
            "_index": 38
        },
        {
            "title": "Superposition as Lossy Compression: Measure with Sparse Autoencoders and Connect to Adversarial Vulnerability",
            "authors": [
                "Leonard Bereska",
                "Zoe Tzifa-Kratira",
                "Reza Samavi",
                "Efstratios Gavves"
            ],
            "arxiv_id": "2512.13568v1",
            "summary": "Neural networks achieve remarkable performance through superposition: encoding multiple features as overlapping directions in activation space rather than dedicating individual neurons to each feature. This challenges interpretability, yet we lack principled methods to measure superposition. We present an information-theoretic framework measuring a neural representation's effective degrees of freedom. We apply Shannon entropy to sparse autoencoder activations to compute the number of effective features as the minimum neurons needed for interference-free encoding. Equivalently, this measures how many \"virtual neurons\" the network simulates through superposition. When networks encode more effective features than actual neurons, they must accept interference as the price of compression. Our metric strongly correlates with ground truth in toy models, detects minimal superposition in algorithmic tasks, and reveals systematic reduction under dropout. Layer-wise patterns mirror intrinsic dimensionality studies on Pythia-70M. The metric also captures developmental dynamics, detecting sharp feature consolidation during grokking. Surprisingly, adversarial training can increase effective features while improving robustness, contradicting the hypothesis that superposition causes vulnerability. Instead, the effect depends on task complexity and network capacity: simple tasks with ample capacity allow feature expansion (abundance regime), while complex tasks or limited capacity force reduction (scarcity regime). By defining superposition as lossy compression, this work enables principled measurement of how neural networks organize information under computational constraints, connecting superposition to adversarial robustness.",
            "headline_zh": "提出基于稀疏自编码器的信息论框架，以测量神经网络中的叠加现象及其与对抗鲁棒性的关系。",
            "intro_zh": [
                "核心问题：缺乏量化神经网络中特征叠加现象的原则性方法，影响可解释性。",
                "方法要点：应用香农熵于稀疏自编码器激活，计算有效特征数作为无干扰编码所需最小神经元数。",
                "实验或效果：在玩具模型中验证相关性，揭示对抗训练可增加有效特征，挑战叠加导致脆弱性的假设。"
            ],
            "tags_zh": [
                "叠加现象",
                "稀疏自编码器",
                "信息论测量",
                "对抗鲁棒性",
                "特征压缩",
                "神经网络可解释性"
            ],
            "_index": 39
        },
        {
            "title": "A Nonparametric Statistics Approach to Feature Selection in Deep Neural Networks with Theoretical Guarantees",
            "authors": [
                "Junye Du",
                "Zhenghao Li",
                "Zhutong Gu",
                "Long Feng"
            ],
            "arxiv_id": "2512.13565v1",
            "summary": "This paper tackles the problem of feature selection in a highly challenging setting: $\\mathbb{E}(y | \\boldsymbol{x}) = G(\\boldsymbol{x}_{\\mathcal{S}_0})$, where $\\mathcal{S}_0$ is the set of relevant features and $G$ is an unknown, potentially nonlinear function subject to mild smoothness conditions. Our approach begins with feature selection in deep neural networks, then generalizes the results to H{ö}lder smooth functions by exploiting the strong approximation capabilities of neural networks. Unlike conventional optimization-based deep learning methods, we reformulate neural networks as index models and estimate $\\mathcal{S}_0$ using the second-order Stein's formula. This gradient-descent-free strategy guarantees feature selection consistency with a sample size requirement of $n = Ω(p^2)$, where $p$ is the feature dimension. To handle high-dimensional scenarios, we further introduce a screening-and-selection mechanism that achieves nonlinear selection consistency when $n = Ω(s \\log p)$, with $s$ representing the sparsity level. Additionally, we refit a neural network on the selected features for prediction and establish performance guarantees under a relaxed sparsity assumption. Extensive simulations and real-data analyses demonstrate the strong performance of our method even in the presence of complex feature interactions.",
            "headline_zh": "提出基于非参数统计的深度神经网络特征选择方法，在非线性高维场景下保证理论一致性。",
            "intro_zh": [
                "核心问题：在未知非线性函数下，从高维特征中识别相关特征集，满足E(y|x)=G(x_S0)。",
                "方法要点：将神经网络重构为索引模型，利用二阶Stein公式进行无梯度特征选择，结合筛选机制处理高维稀疏性。",
                "实验或效果：通过模拟和真实数据分析验证方法性能，即使在复杂特征交互下也表现优异。"
            ],
            "tags_zh": [
                "特征选择",
                "非参数统计",
                "深度神经网络",
                "理论保证",
                "高维数据",
                "非线性模型"
            ],
            "_index": 40
        },
        {
            "title": "Memory in the Age of AI Agents",
            "authors": [
                "Yuyang Hu",
                "Shichun Liu",
                "Yanwei Yue",
                "Guibin Zhang",
                "Boyang Liu",
                "Fangyi Zhu",
                "Jiahang Lin",
                "Honglin Guo",
                "Shihan Dou",
                "Zhiheng Xi",
                "Senjie Jin",
                "Jiejun Tan",
                "Yanbin Yin",
                "Jiongnan Liu",
                "Zeyu Zhang",
                "Zhongxiang Sun",
                "Yutao Zhu",
                "Hao Sun",
                "Boci Peng",
                "Zhenrong Cheng",
                "Xuanbo Fan",
                "Jiaxin Guo",
                "Xinlei Yu",
                "Zhenhong Zhou",
                "Zewen Hu",
                "Jiahao Huo",
                "Junhao Wang",
                "Yuwei Niu",
                "Yu Wang",
                "Zhenfei Yin",
                "Xiaobin Hu",
                "Yue Liao",
                "Qiankun Li",
                "Kun Wang",
                "Wangchunshu Zhou",
                "Yixin Liu",
                "Dawei Cheng",
                "Qi Zhang",
                "Tao Gui",
                "Shirui Pan",
                "Yan Zhang",
                "Philip Torr",
                "Zhicheng Dou",
                "Ji-Rong Wen",
                "Xuanjing Huang",
                "Yu-Gang Jiang",
                "Shuicheng Yan"
            ],
            "arxiv_id": "2512.13564v1",
            "summary": "Memory has emerged, and will continue to remain, a core capability of foundation model-based agents. As research on agent memory rapidly expands and attracts unprecedented attention, the field has also become increasingly fragmented. Existing works that fall under the umbrella of agent memory often differ substantially in their motivations, implementations, and evaluation protocols, while the proliferation of loosely defined memory terminologies has further obscured conceptual clarity. Traditional taxonomies such as long/short-term memory have proven insufficient to capture the diversity of contemporary agent memory systems. This work aims to provide an up-to-date landscape of current agent memory research. We begin by clearly delineating the scope of agent memory and distinguishing it from related concepts such as LLM memory, retrieval augmented generation (RAG), and context engineering. We then examine agent memory through the unified lenses of forms, functions, and dynamics. From the perspective of forms, we identify three dominant realizations of agent memory, namely token-level, parametric, and latent memory. From the perspective of functions, we propose a finer-grained taxonomy that distinguishes factual, experiential, and working memory. From the perspective of dynamics, we analyze how memory is formed, evolved, and retrieved over time. To support practical development, we compile a comprehensive summary of memory benchmarks and open-source frameworks. Beyond consolidation, we articulate a forward-looking perspective on emerging research frontiers, including memory automation, reinforcement learning integration, multimodal memory, multi-agent memory, and trustworthiness issues. We hope this survey serves not only as a reference for existing work, but also as a conceptual foundation for rethinking memory as a first-class primitive in the design of future agentic intelligence.",
            "headline_zh": "综述基于基础模型的智能体记忆研究，提出形式、功能与动态的统一分析框架。",
            "intro_zh": [
                "核心问题：智能体记忆研究碎片化，传统分类如长/短期记忆不足以描述多样性，需清晰界定概念。",
                "方法要点：从形式、功能、动态三个视角分析记忆，形式包括令牌级、参数化、潜在记忆，功能细分为事实、经验、工作记忆。",
                "实验或效果：汇编记忆基准测试与开源框架，展望自动化、强化学习集成、多模态等前沿方向。"
            ],
            "tags_zh": [
                "智能体记忆",
                "基础模型",
                "记忆分类",
                "记忆动态",
                "基准测试",
                "多模态记忆"
            ],
            "_index": 41
        },
        {
            "title": "Near-Field Perception for Safety Enhancement of Autonomous Mobile Robots in Manufacturing Environments",
            "authors": [
                "Li-Wei Shih",
                "Ruo-Syuan Mei",
                "Jesse Heidrich",
                "Hui-Ping Wang",
                "Joel Hooton",
                "Joshua Solomon",
                "Jorge Arinez",
                "Guangze Li",
                "Chenhui Shao"
            ],
            "arxiv_id": "2512.13561v1",
            "summary": "Near-field perception is essential for the safe operation of autonomous mobile robots (AMRs) in manufacturing environments. Conventional ranging sensors such as light detection and ranging (LiDAR) and ultrasonic devices provide broad situational awareness but often fail to detect small objects near the robot base. To address this limitation, this paper presents a three-tier near-field perception framework. The first approach employs light-discontinuity detection, which projects a laser stripe across the near-field zone and identifies interruptions in the stripe to perform fast, binary cutoff sensing for obstacle presence. The second approach utilizes light-displacement measurement to estimate object height by analyzing the geometric displacement of a projected stripe in the camera image, which provides quantitative obstacle height information with minimal computational overhead. The third approach employs a computer vision-based object detection model on embedded AI hardware to classify objects, enabling semantic perception and context-aware safety decisions. All methods are implemented on a Raspberry Pi 5 system, achieving real-time performance at 25 or 50 frames per second. Experimental evaluation and comparative analysis demonstrate that the proposed hierarchy balances precision, computation, and cost, thereby providing a scalable perception solution for enabling safe operations of AMRs in manufacturing environments.",
            "headline_zh": "提出三层近场感知框架以增强制造环境中自主移动机器人的安全性",
            "intro_zh": [
                "核心问题：传统测距传感器难以检测机器人基座附近的小物体，影响AMR安全操作。",
                "方法要点：采用光间断检测、光位移测量和基于计算机视觉的对象检测，实现快速障碍感知、高度估计和语义分类。",
                "实验或效果：在Raspberry Pi 5上实时运行，帧率达25或50fps，平衡精度、计算和成本，提升AMR安全性。"
            ],
            "tags_zh": [
                "近场感知",
                "自主移动机器人",
                "制造环境安全",
                "光间断检测",
                "光位移测量",
                "嵌入式AI"
            ],
            "_index": 42
        },
        {
            "title": "3D Human-Human Interaction Anomaly Detection",
            "authors": [
                "Shun Maeda",
                "Chunzhi Gu",
                "Koichiro Kamide",
                "Katsuya Hotta",
                "Shangce Gao",
                "Chao Zhang"
            ],
            "arxiv_id": "2512.13560v1",
            "summary": "Human-centric anomaly detection (AD) has been primarily studied to specify anomalous behaviors in a single person. However, as humans by nature tend to act in a collaborative manner, behavioral anomalies can also arise from human-human interactions. Detecting such anomalies using existing single-person AD models is prone to low accuracy, as these approaches are typically not designed to capture the complex and asymmetric dynamics of interactions. In this paper, we introduce a novel task, Human-Human Interaction Anomaly Detection (H2IAD), which aims to identify anomalous interactive behaviors within collaborative 3D human actions. To address H2IAD, we then propose Interaction Anomaly Detection Network (IADNet), which is formalized with a Temporal Attention Sharing Module (TASM). Specifically, in designing TASM, we share the encoded motion embeddings across both people such that collaborative motion correlations can be effectively synchronized. Moreover, we notice that in addition to temporal dynamics, human interactions are also characterized by spatial configurations between two people. We thus introduce a Distance-Based Relational Encoding Module (DREM) to better reflect social cues in H2IAD. The normalizing flow is eventually employed for anomaly scoring. Extensive experiments on human-human motion benchmarks demonstrate that IADNet outperforms existing Human-centric AD baselines in H2IAD.",
            "headline_zh": "提出IADNet以解决3D人-人交互异常检测任务，通过共享时间注意力和距离编码提升准确性。",
            "intro_zh": [
                "核心问题：现有单人异常检测模型难以捕捉人-人交互的复杂不对称动态，导致检测准确性低。",
                "方法要点：设计TASM共享运动嵌入以同步协作相关性，并引入DREM编码空间配置以反映社交线索。",
                "实验或效果：在基准测试中，IADNet优于现有以人为中心的异常检测基线，验证了其有效性。"
            ],
            "tags_zh": [
                "3D人-人交互异常检测",
                "时间注意力共享",
                "距离关系编码",
                "正常化流",
                "协作运动分析"
            ],
            "_index": 43
        },
        {
            "title": "Verifying Rumors via Stance-Aware Structural Modeling",
            "authors": [
                "Gibson Nkhata",
                "Uttamasha Anjally Oyshi",
                "Quan Mai",
                "Susan Gauch"
            ],
            "arxiv_id": "2512.13559v1",
            "summary": "Verifying rumors on social media is critical for mitigating the spread of false information. The stances of conversation replies often provide important cues to determine a rumor's veracity. However, existing models struggle to jointly capture semantic content, stance information, and conversation strructure, especially under the sequence length constraints of transformer-based encoders. In this work, we propose a stance-aware structural modeling that encodes each post in a discourse with its stance signal and aggregates reply embedddings by stance category enabling a scalable and semantically enriched representation of the entire thread. To enhance structural awareness, we introduce stance distribution and hierarchical depth as covariates, capturing stance imbalance and the influence of reply depth. Extensive experiments on benchmark datasets demonstrate that our approach significantly outperforms prior methods in the ability to predict truthfulness of a rumor. We also demonstrate that our model is versatile for early detection and cross-platfrom generalization.",
            "headline_zh": "提出基于立场感知的结构建模方法，以增强社交媒体谣言验证的准确性。",
            "intro_zh": [
                "核心问题：现有模型难以同时捕捉语义内容、立场信息和对话结构，尤其在Transformer编码器的序列长度限制下。",
                "方法要点：通过编码帖子及其立场信号，按立场类别聚合回复嵌入，并引入立场分布和层次深度作为协变量，以增强结构感知。",
                "实验或效果：在基准数据集上显著优于先前方法，验证了模型在谣言真实性预测、早期检测和跨平台泛化方面的有效性。"
            ],
            "tags_zh": [
                "社交媒体谣言验证",
                "立场感知建模",
                "对话结构分析",
                "Transformer编码器",
                "跨平台泛化"
            ],
            "_index": 44
        },
        {
            "title": "Pancakes: Consistent Multi-Protocol Image Segmentation Across Biomedical Domains",
            "authors": [
                "Marianne Rakic",
                "Siyu Gai",
                "Etienne Chollet",
                "John V. Guttag",
                "Adrian V. Dalca"
            ],
            "arxiv_id": "2512.13534v1",
            "summary": "A single biomedical image can be meaningfully segmented in multiple ways, depending on the desired application. For instance, a brain MRI can be segmented according to tissue types, vascular territories, broad anatomical regions, fine-grained anatomy, or pathology, etc. Existing automatic segmentation models typically either (1) support only a single protocol, the one they were trained on, or (2) require labor-intensive manual prompting to specify the desired segmentation. We introduce Pancakes, a framework that, given a new image from a previously unseen domain, automatically generates multi-label segmentation maps for multiple plausible protocols, while maintaining semantic consistency across related images. Pancakes introduces a new problem formulation that is not currently attainable by existing foundation models. In a series of experiments on seven held-out datasets, we demonstrate that our model can significantly outperform existing foundation models in producing several plausible whole-image segmentations, that are semantically coherent across images.",
            "headline_zh": "提出Pancakes框架以解决生物医学图像多协议分割的一致性问题",
            "intro_zh": [
                "核心问题：现有模型通常仅支持单一分割协议或需手动指定，缺乏自动多协议分割能力",
                "方法要点：引入新问题表述，自动生成多标签分割图，确保跨图像语义一致性",
                "实验或效果：在七个未见数据集上显著优于现有基础模型，产生语义连贯的分割结果"
            ],
            "tags_zh": [
                "生物医学图像分割",
                "多协议分割",
                "语义一致性",
                "基础模型",
                "自动分割"
            ],
            "_index": 45
        },
        {
            "title": "Adaptive Sampling for Hydrodynamic Stability",
            "authors": [
                "Anshima Singh",
                "David J. Silvester"
            ],
            "arxiv_id": "2512.13532v1",
            "summary": "An adaptive sampling approach for efficient detection of bifurcation boundaries in parametrized fluid flow problems is presented herein. The study extends the machine-learning approach of Silvester (Machine Learning for Hydrodynamic Stability, arXiv:2407.09572), where a classifier network was trained on preselected simulation data to identify bifurcated and nonbifurcated flow regimes. In contrast, the proposed methodology introduces adaptivity through a flow-based deep generative model that automatically refines the sampling of the parameter space. The strategy has two components: a classifier network maps the flow parameters to a bifurcation probability, and a probability density estimation technique (KRnet) for the generation of new samples at each adaptive step. The classifier output provides a probabilistic measure of flow stability, and the Shannon entropy of these predictions is employed as an uncertainty indicator. KRnet is trained to approximate a probability density function that concentrates sampling in regions of high entropy, thereby directing computational effort towards the evolving bifurcation boundary. This coupling between classification and generative modeling establishes a feedback-driven adaptive learning process analogous to error-indicator based refinement in contemporary partial differential equation solution strategies. Starting from a uniform parameter distribution, the new approach achieves accurate bifurcation boundary identification with significantly fewer Navier--Stokes simulations, providing a scalable foundation for high-dimensional stability analysis.",
            "headline_zh": "提出自适应采样方法以高效检测参数化流体流动中的分岔边界",
            "intro_zh": [
                "核心问题：参数化流体流动中分岔边界检测的计算成本高，需高效采样策略。",
                "方法要点：结合分类网络和概率密度估计（KRnet），通过熵引导自适应采样，形成反馈驱动学习过程。",
                "实验或效果：从均匀分布出发，显著减少Navier-Stokes模拟次数，实现准确分岔边界识别，支持高维稳定性分析。"
            ],
            "tags_zh": [
                "自适应采样",
                "流体稳定性分析",
                "分岔边界检测",
                "深度生成模型",
                "概率密度估计",
                "机器学习应用"
            ],
            "_index": 46
        },
        {
            "title": "Actively Learning Joint Contours of Multiple Computer Experiments",
            "authors": [
                "Shih-Ni Prim",
                "Kevin R. Quinlan",
                "Paul Hawkins",
                "Jagadeesh Movva",
                "Annie S. Booth"
            ],
            "arxiv_id": "2512.13530v1",
            "summary": "Contour location$\\unicode{x2014}$the process of sequentially training a surrogate model to identify the design inputs that result in a pre-specified response value from a single computer experiment$\\unicode{x2014}$is a well-studied active learning problem. Here, we tackle a related but distinct problem: identifying the input configuration that returns pre-specified values of multiple independent computer experiments simultaneously. Motivated by computer experiments of the rotational torques acting upon a vehicle in flight, we aim to identify stable flight conditions which result in zero torque forces. We propose a \"joint contour location\" (jCL) scheme that strikes a strategic balance between exploring the multiple response surfaces while exploiting learning of the intersecting contours. We employ both shallow and deep Gaussian process surrogates, but our jCL procedure is applicable to any surrogate that can provide posterior predictive distributions. Our jCL designs significantly outperform existing (single response) CL strategies, enabling us to efficiently locate the joint contour of our motivating computer experiments.",
            "headline_zh": "提出联合轮廓定位方案以解决多计算机实验联合轮廓识别问题",
            "intro_zh": [
                "核心问题：识别多独立计算机实验同时返回预设值的输入配置，如飞行器零扭矩稳定条件",
                "方法要点：基于高斯过程代理模型，平衡探索响应曲面与利用轮廓交叉学习",
                "实验或效果：在飞行器扭矩实验中，显著优于单响应轮廓定位策略，高效定位联合轮廓"
            ],
            "tags_zh": [
                "联合轮廓定位",
                "主动学习",
                "高斯过程",
                "计算机实验",
                "多响应优化"
            ],
            "_index": 47
        },
        {
            "title": "Enhancing lithological interpretation from petrophysical well log of IODP expedition 390/393 using machine learning",
            "authors": [
                "Raj Sahu",
                "Saumen Maiti"
            ],
            "arxiv_id": "2512.13529v1",
            "summary": "Enhanced lithological interpretation from well logs plays a key role in geological resource exploration and mapping, as well as in geo-environmental modeling studies. Core and cutting information is useful for making sound interpretations of well logs; however, these are rarely collected at each depth due to high costs. Moreover, well log interpretation using traditional methods is constrained by poor borehole conditions. Traditional statistical methods are mostly linear, often failing to discriminate between lithology and rock facies, particularly when dealing with overlapping well log signals characterized by the structural and compositional variation of rock types. In this study, we develop multiple supervised and unsupervised machine learning algorithms to jointly analyze multivariate well log data from Integrated Ocean Drilling Program (IODP) expeditions 390 and 393 for enhanced lithological interpretations. Among the algorithms, Logistic Regression, Decision Trees, Gradient Boosting, Support Vector Machines (SVM), k-Nearest Neighbors (KNN), and Multi-Layer Perceptron (MLP) neural network models, the Decision Tree and Gradient Boosting models outperformed the others, achieving an accuracy of 0.9950 and an F1-score of 0.9951. While unsupervised machine learning (ML) provides the foundation for cluster information that inherently supports the classification algorithm, supervised ML is applied to devise a data-driven lithology clustering mechanism for IODP datasets. The joint ML-based method developed here has the potential to be further explored for analyzing other well log datasets from the world's oceans.",
            "headline_zh": "提出联合机器学习方法以增强IODP测井数据的岩性解释",
            "intro_zh": [
                "核心问题：传统线性统计方法难以区分重叠测井信号中的岩性和岩相。",
                "方法要点：开发监督和无监督机器学习算法，联合分析多变量测井数据。",
                "实验或效果：决策树和梯度提升模型表现最佳，准确率达0.9950，F1分数为0.9951。"
            ],
            "tags_zh": [
                "岩性解释",
                "机器学习",
                "测井数据分析",
                "监督学习",
                "无监督学习",
                "IODP"
            ],
            "_index": 48
        },
        {
            "title": "Async Control: Stress-testing Asynchronous Control Measures for LLM Agents",
            "authors": [
                "Asa Cooper Stickland",
                "Jan Michelfeit",
                "Arathi Mani",
                "Charlie Griffin",
                "Ollie Matthews",
                "Tomek Korbak",
                "Rogan Inglis",
                "Oliver Makins",
                "Alan Cooney"
            ],
            "arxiv_id": "2512.13526v1",
            "summary": "LLM-based software engineering agents are increasingly used in real-world development tasks, often with access to sensitive data or security-critical codebases. Such agents could intentionally sabotage these codebases if they were misaligned. We investigate asynchronous monitoring, in which a monitoring system reviews agent actions after the fact. Unlike synchronous monitoring, this approach does not impose runtime latency, while still attempting to disrupt attacks before irreversible harm occurs. We treat monitor development as an adversarial game between a blue team (who design monitors) and a red team (who create sabotaging agents). We attempt to set the game rules such that they upper bound the sabotage potential of an agent based on Claude 4.1 Opus. To ground this game in a realistic, high-stakes deployment scenario, we develop a suite of 5 diverse software engineering environments that simulate tasks that an agent might perform within an AI developer's internal infrastructure. Over the course of the game, we develop an ensemble monitor that achieves a 6% false negative rate at 1% false positive rate on a held out test environment. Then, we estimate risk of sabotage at deployment time by extrapolating from our monitor's false negative rate. We describe one simple model for this extrapolation, present a sensitivity analysis, and describe situations in which the model would be invalid. Code is available at: https://github.com/UKGovernmentBEIS/async-control.",
            "headline_zh": "提出异步监控方法以评估LLM代理在软件工程中的破坏风险",
            "intro_zh": [
                "核心问题：LLM代理在敏感代码库中可能故意破坏，需监控其行为以防止不可逆损害。",
                "方法要点：采用异步监控，通过红蓝队对抗游戏开发监控系统，不增加运行时延迟。",
                "实验或效果：在5个软件工程环境中，集成监控器在1%误报率下达到6%漏报率，并基于此估计部署风险。"
            ],
            "tags_zh": [
                "LLM代理安全",
                "异步监控",
                "对抗游戏",
                "软件工程环境",
                "风险评估"
            ],
            "_index": 49
        },
        {
            "title": "A Deep Learning Model of Mental Rotation Informed by Interactive VR Experiments",
            "authors": [
                "Raymond Khazoum",
                "Daniela Fernandes",
                "Aleksandr Krylov",
                "Qin Li",
                "Stephane Deny"
            ],
            "arxiv_id": "2512.13517v1",
            "summary": "Mental rotation -- the ability to compare objects seen from different viewpoints -- is a fundamental example of mental simulation and spatial world modelling in humans. Here we propose a mechanistic model of human mental rotation, leveraging advances in deep, equivariant, and neuro-symbolic learning. Our model consists of three stacked components: (1) an equivariant neural encoder, taking images as input and producing 3D spatial representations of objects, (2) a neuro-symbolic object encoder, deriving symbolic descriptions of objects from these spatial representations, and (3) a neural decision agent, comparing these symbolic descriptions to prescribe rotation simulations in 3D latent space via a recurrent pathway. Our model design is guided by the abundant experimental literature on mental rotation, which we complemented with experiments in VR where participants could at times manipulate the objects to compare, providing us with additional insights into the cognitive process of mental rotation. Our model captures well the performance, response times and behavior of participants in our and others' experiments. The necessity of each model component is shown through systematic ablations. Our work adds to a recent collection of deep neural models of human spatial reasoning, further demonstrating the potency of integrating deep, equivariant, and symbolic representations to model the human mind.",
            "headline_zh": "提出基于深度、等变和神经符号学习的心理旋转机制模型，结合VR实验验证",
            "intro_zh": [
                "核心问题：建模人类心理旋转能力，即从不同视角比较物体的空间推理过程",
                "方法要点：模型包含等变神经编码器、神经符号对象编码器和神经决策代理，集成深度、等变和符号表示",
                "实验或效果：通过VR实验和系统消融验证模型性能，能捕捉实验参与者的表现、反应时间和行为"
            ],
            "tags_zh": [
                "心理旋转建模",
                "等变神经网络",
                "神经符号学习",
                "空间推理",
                "虚拟现实实验",
                "机制模型"
            ],
            "_index": 50
        },
        {
            "title": "Reinforcement Learning based 6-DoF Maneuvers for Microgravity Intravehicular Docking: A Simulation Study with Int-Ball2 in ISS-JEM",
            "authors": [
                "Aman Arora",
                "Matteo El-Hariry",
                "Miguel Olivares-Mendez"
            ],
            "arxiv_id": "2512.13514v1",
            "summary": "Autonomous free-flyers play a critical role in intravehicular tasks aboard the International Space Station (ISS), where their precise docking under sensing noise, small actuation mismatches, and environmental variability remains a nontrivial challenge. This work presents a reinforcement learning (RL) framework for six-degree-of-freedom (6-DoF) docking of JAXA's Int-Ball2 robot inside a high-fidelity Isaac Sim model of the Japanese Experiment Module (JEM). Using Proximal Policy Optimization (PPO), we train and evaluate controllers under domain-randomized dynamics and bounded observation noise, while explicitly modeling propeller drag-torque effects and polarity structure. This enables a controlled study of how Int-Ball2's propulsion physics influence RL-based docking performance in constrained microgravity interiors. The learned policy achieves stable and reliable docking across varied conditions and lays the groundwork for future extensions pertaining to Int-Ball2 in collision-aware navigation, safe RL, propulsion-accurate sim-to-real transfer, and vision-based end-to-end docking.",
            "headline_zh": "提出基于强化学习的6自由度对接框架，用于国际空间站内微重力环境下Int-Ball2机器人的精确对接。",
            "intro_zh": [
                "核心问题：国际空间站内自主飞行器在传感噪声、执行器失配和环境变化下的精确对接挑战。",
                "方法要点：使用近端策略优化在Isaac Sim高保真模型中训练控制器，建模推进器拖曳扭矩和极性结构。",
                "实验或效果：在域随机化和有界观测噪声下实现稳定可靠对接，为未来扩展奠定基础。"
            ],
            "tags_zh": [
                "强化学习",
                "6自由度对接",
                "微重力环境",
                "国际空间站",
                "近端策略优化",
                "仿真研究"
            ],
            "_index": 51
        },
        {
            "title": "TARA: Simple and Efficient Time Aware Retrieval Adaptation of MLLMs for Video Understanding",
            "authors": [
                "Piyush Bagad",
                "Andrew Zisserman"
            ],
            "arxiv_id": "2512.13511v1",
            "summary": "Our objective is to build a general time-aware video-text embedding model for retrieval. To that end, we propose a simple and efficient recipe, dubbed TARA (Time Aware Retrieval Adaptation), to adapt Multimodal LLMs (MLLMs) to a time-aware video-text embedding model without using any video data at all. For evaluating time-awareness in retrieval, we propose a new benchmark with temporally opposite (chiral) actions as hard negatives and curated splits for chiral and non-chiral actions. We show that TARA outperforms all existing video-text models on this chiral benchmark while also achieving strong results on standard benchmarks. Furthermore, we discover additional benefits of TARA beyond time-awareness: (i) TARA embeddings are negation-aware as shown in NegBench benchmark that evaluates negation in video retrieval, (ii) TARA achieves state of the art performance on verb and adverb understanding in videos. Overall, TARA yields a strong, versatile, time-aware video-text embedding model with state of the art zero-shot performance.",
            "headline_zh": "提出TARA方法，无需视频数据适配MLLMs为时间感知视频-文本检索模型",
            "intro_zh": [
                "核心问题：构建通用时间感知视频-文本嵌入模型，用于视频检索。",
                "方法要点：通过简单高效配方TARA，适配多模态大语言模型，无需视频数据实现时间感知。",
                "实验或效果：在时间对立动作基准上超越现有模型，并在否定感知、动词副词理解方面表现优异。"
            ],
            "tags_zh": [
                "视频检索",
                "时间感知",
                "多模态大语言模型",
                "零样本性能",
                "嵌入模型"
            ],
            "_index": 52
        },
        {
            "title": "MedCEG: Reinforcing Verifiable Medical Reasoning with Critical Evidence Graph",
            "authors": [
                "Linjie Mu",
                "Yannian Gu",
                "Zhongzhen Huang",
                "Yakun Zhu",
                "Shaoting Zhang",
                "Xiaofan Zhang"
            ],
            "arxiv_id": "2512.13510v1",
            "summary": "Large language models with reasoning capabilities have demonstrated impressive performance across a wide range of domains. In clinical applications, a transparent, step-by-step reasoning process provides physicians with strong evidence to support decision-making. While reinforcement learning has effectively enhanced reasoning performance in medical contexts, the clinical reliability of these reasoning processes remains limited because their accuracy and validity are often overlooked during training. To address this gap, we propose MedCEG, a framework that augments medical language models with clinically valid reasoning pathways by explicitly supervising the reasoning process through a Critical Evidence Graph (CEG). We curate a dataset of challenging clinical cases and algorithmically construct a CEG for each sample to represent a high-quality verifiable reasoning pathway. To guide the reasoning process, we introduce a Clinical Reasoning Procedure Reward, which evaluates Node Coverage, Structural Correctness, and Chain Completeness, thereby providing a holistic assessment of reasoning quality. Experimental results show that MedCEG surpasses existing methods in performance while producing clinically valid reasoning chains, representing a solid advancement in reliable medical AI reasoning. The code and models are available at https://github.com/LinjieMu/MedCEG.",
            "headline_zh": "提出MedCEG框架，通过关键证据图增强医学语言模型的可验证推理能力。",
            "intro_zh": [
                "核心问题：医学推理中强化学习常忽视准确性和有效性，导致临床可靠性受限。",
                "方法要点：构建关键证据图监督推理过程，引入临床推理程序奖励评估节点覆盖、结构正确性和链完整性。",
                "实验或效果：在挑战性临床案例数据集上，MedCEG超越现有方法，生成临床有效推理链。"
            ],
            "tags_zh": [
                "医学推理",
                "关键证据图",
                "强化学习",
                "临床可靠性",
                "语言模型增强"
            ],
            "_index": 53
        },
        {
            "title": "Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model",
            "authors": [
                "Siyan Chen",
                "Yanfei Chen",
                "Ying Chen",
                "Zhuo Chen",
                "Feng Cheng",
                "Xuyan Chi",
                "Jian Cong",
                "Qinpeng Cui",
                "Qide Dong",
                "Junliang Fan",
                "Jing Fang",
                "Zetao Fang",
                "Chengjian Feng",
                "Han Feng",
                "Mingyuan Gao",
                "Yu Gao",
                "Qiushan Guo",
                "Boyang Hao",
                "Qingkai Hao",
                "Bibo He",
                "Qian He",
                "Tuyen Hoang",
                "Ruoqing Hu",
                "Xi Hu",
                "Weilin Huang",
                "Zhaoyang Huang",
                "Zhongyi Huang",
                "Siqi Jiang",
                "Wei Jiang",
                "Yunpu Jiang",
                "Zhuo Jiang",
                "Ashley Kim",
                "Jianan Kong",
                "Zhichao Lai",
                "Shanshan Lao",
                "Ai Li",
                "Feiya Li",
                "Gen Li",
                "Huixia Li",
                "JiaShi Li",
                "Liang Li",
                "Ming Li",
                "Tao Li",
                "Xian Li",
                "Xiaojie Li",
                "Xiaoyang Li",
                "Xingxing Li",
                "Yameng Li",
                "Yifu Li",
                "Yiying Li",
                "Chao Liang",
                "Ying Liang",
                "Zhiqiang Liang",
                "Wang Liao",
                "Yalin Liao",
                "Heng Lin",
                "Kengyu Lin",
                "Shanchuan Lin",
                "Xi Lin",
                "Zhijie Lin",
                "Feng Ling",
                "Fangfang Liu",
                "Gaohong Liu",
                "Jiawei Liu",
                "Jie Liu",
                "Shouda Liu",
                "Shu Liu",
                "Sichao Liu",
                "Songwei Liu",
                "Xin Liu",
                "Xue Liu",
                "Yibo Liu",
                "Zikun Liu",
                "Zuxi Liu",
                "Junlin Lyu",
                "Lecheng Lyu",
                "Qian Lyu",
                "Han Mu",
                "Xiaonan Nie",
                "Jingzhe Ning",
                "Xitong Pan",
                "Yanghua Peng",
                "Lianke Qin",
                "Xueqiong Qu",
                "Yuxi Ren",
                "Yuchen Shen",
                "Guang Shi",
                "Lei Shi",
                "Yan Song",
                "Yinglong Song",
                "Fan Sun",
                "Li Sun",
                "Renfei Sun",
                "Zeyu Sun",
                "Wenjing Tang",
                "Zirui Tao",
                "Feng Wang",
                "Furui Wang",
                "Jinran Wang",
                "Junkai Wang",
                "Ke Wang",
                "Kexin Wang",
                "Qingyi Wang",
                "Rui Wang",
                "Sen Wang",
                "Shuai Wang",
                "Tingru Wang",
                "Weichen Wang",
                "Xin Wang",
                "Yanhui Wang",
                "Yue Wang",
                "Yuping Wang",
                "Yuxuan Wang",
                "Ziyu Wang",
                "Guoqiang Wei",
                "Wanru Wei",
                "Di Wu",
                "Guohong Wu",
                "Hanjie Wu",
                "Jian Wu",
                "Jie Wu",
                "Ruolan Wu",
                "Xinglong Wu",
                "Yonghui Wu",
                "Ruiqi Xia",
                "Liang Xiang",
                "Fei Xiao",
                "XueFeng Xiao",
                "Pan Xie",
                "Shuangyi Xie",
                "Shuang Xu",
                "Jinlan Xue",
                "Bangbang Yang",
                "Ceyuan Yang",
                "Jiaqi Yang",
                "Runkai Yang",
                "Tao Yang",
                "Yang Yang",
                "Yihang Yang",
                "ZhiXian Yang",
                "Ziyan Yang",
                "Yifan Yao",
                "Zilyu Ye",
                "Bowen Yu",
                "Chujie Yuan",
                "Linxiao Yuan",
                "Sichun Zeng",
                "Weihong Zeng",
                "Xuejiao Zeng",
                "Yan Zeng",
                "Chuntao Zhang",
                "Heng Zhang",
                "Jingjie Zhang",
                "Kuo Zhang",
                "Liang Zhang",
                "Liying Zhang",
                "Manlin Zhang",
                "Ting Zhang",
                "Weida Zhang",
                "Xiaohe Zhang",
                "Xinyan Zhang",
                "Yan Zhang",
                "Yuan Zhang",
                "Zixiang Zhang",
                "Fengxuan Zhao",
                "Huating Zhao",
                "Yang Zhao",
                "Hao Zheng",
                "Jianbin Zheng",
                "Xiaozheng Zheng",
                "Yangyang Zheng",
                "Yijie Zheng",
                "Jiexin Zhou",
                "Kuan Zhu",
                "Shenhan Zhu",
                "Wenjia Zhu",
                "Benhui Zou",
                "Feilong Zuo"
            ],
            "arxiv_id": "2512.13507v1",
            "summary": "Recent strides in video generation have paved the way for unified audio-visual generation. In this work, we present Seedance 1.5 pro, a foundational model engineered specifically for native, joint audio-video generation. Leveraging a dual-branch Diffusion Transformer architecture, the model integrates a cross-modal joint module with a specialized multi-stage data pipeline, achieving exceptional audio-visual synchronization and superior generation quality. To ensure practical utility, we implement meticulous post-training optimizations, including Supervised Fine-Tuning (SFT) on high-quality datasets and Reinforcement Learning from Human Feedback (RLHF) with multi-dimensional reward models. Furthermore, we introduce an acceleration framework that boosts inference speed by over 10X. Seedance 1.5 pro distinguishes itself through precise multilingual and dialect lip-syncing, dynamic cinematic camera control, and enhanced narrative coherence, positioning it as a robust engine for professional-grade content creation. Seedance 1.5 pro is now accessible on Volcano Engine at https://console.volcengine.com/ark/region:ark+cn-beijing/experience/vision?type=GenVideo.",
            "headline_zh": "提出Seedance 1.5 pro原生音视频联合生成基础模型，用于专业级内容创作。",
            "intro_zh": [
                "核心问题：实现高质量、同步的音视频联合生成，提升专业内容实用性。",
                "方法要点：采用双分支Diffusion Transformer架构，结合跨模态联合模块和多阶段数据管道。",
                "实验或效果：通过SFT和RLHF优化，实现精确唇形同步、动态相机控制和叙事连贯性，推理加速超10倍。"
            ],
            "tags_zh": [
                "音视频联合生成",
                "扩散变换器",
                "跨模态同步",
                "强化学习人类反馈",
                "推理加速",
                "专业内容创作"
            ],
            "_index": 54
        },
        {
            "title": "Learning under Distributional Drift: Reproducibility as an Intrinsic Statistical Resource",
            "authors": [
                "Sofiya Zaichyk"
            ],
            "arxiv_id": "2512.13506v1",
            "summary": "Statistical learning under distributional drift remains insufficiently characterized: when each observation alters the data-generating law, classical generalization bounds can collapse. We introduce a new statistical primitive, the reproducibility budget $C_T$, which quantifies a system's finite capacity for statistical reproducibility - the extent to which its sampling process can remain governed by a consistent underlying distribution in the presence of both exogenous change and endogenous feedback. Formally, $C_T$ is defined as the cumulative Fisher-Rao path length of the coupled learner-environment evolution, measuring the total distributional motion accumulated during learning. From this construct we derive a drift-feedback generalization bound of order $O(T^{-1/2} + C_T/T)$, and we prove a matching minimax lower bound showing that this rate is minimax-optimal. Consequently, the results establish a reproducibility speed limit: no algorithm can achieve smaller worst-case generalization error than that imposed by the average Fisher-Rao drift rate $C_T/T$ of the data-generating process. The framework situates exogenous drift, adaptive data analysis, and performative prediction within a common geometric structure, with $C_T$ emerging as the intrinsic quantity measuring distributional motion across these settings.",
            "headline_zh": "提出可重复性预算以量化分布漂移下的统计学习极限",
            "intro_zh": [
                "核心问题：分布漂移导致经典泛化界失效，需量化系统统计可重复性",
                "方法要点：定义可重复性预算为Fisher-Rao路径长度，推导最优泛化界",
                "实验或效果：证明该界为极小极大最优，建立可重复性速度极限"
            ],
            "tags_zh": [
                "分布漂移",
                "统计可重复性",
                "Fisher-Rao度量",
                "泛化界",
                "极小极大最优",
                "自适应数据分析"
            ],
            "_index": 55
        },
        {
            "title": "Defending the Hierarchical Result Models of Precedential Constraint",
            "authors": [
                "Henry Prakken",
                "Wijnand van Woerkom"
            ],
            "arxiv_id": "2512.13505v1",
            "summary": "In recent years, hierarchical case-based-reasoning models of precedential constraint have been proposed. In various papers, Trevor Bench-Capon criticised these models on the grounds that they would give incorrect outcomes in some cases. In particular, the models would not account for the possibility that intermediate factors are established with different strengths by different base-level factors. In this paper we respond to these criticisms for van Woerkom's result-based hierarchical models. We argue that in some examples Bench-Capon seems to interpret intermediate factors as dimensions, and that applying van Woerkom's dimension-based version of the hierarchical result model to these examples avoids Bench-Capon's criticisms.",
            "headline_zh": "为基于结果的层次模型辩护，以应对先例约束中的批评",
            "intro_zh": [
                "核心问题：回应Bench-Capon对层次先例约束模型的批评，特别是中间因素强度差异问题。",
                "方法要点：指出批评可能源于将中间因素误解为维度，应用基于维度的层次模型可避免问题。",
                "实验或效果：通过理论论证，展示van Woerkom的模型在相关示例中能正确应对批评。"
            ],
            "tags_zh": [
                "先例约束",
                "层次模型",
                "案例推理",
                "法律推理",
                "模型辩护"
            ],
            "_index": 56
        },
        {
            "title": "Behavior-Aware and Generalizable Defense Against Black-Box Adversarial Attacks for ML-Based IDS",
            "authors": [
                "Sabrine Ennaji",
                "Elhadj Benkhelifa",
                "Luigi Vincenzo Mancini"
            ],
            "arxiv_id": "2512.13501v1",
            "summary": "Machine learning based intrusion detection systems are increasingly targeted by black box adversarial attacks, where attackers craft evasive inputs using indirect feedback such as binary outputs or behavioral signals like response time and resource usage. While several defenses have been proposed, including input transformation, adversarial training, and surrogate detection, they often fall short in practice. Most are tailored to specific attack types, require internal model access, or rely on static mechanisms that fail to generalize across evolving attack strategies. Furthermore, defenses such as input transformation can degrade intrusion detection system performance, making them unsuitable for real time deployment.\n  To address these limitations, we propose Adaptive Feature Poisoning, a lightweight and proactive defense mechanism designed specifically for realistic black box scenarios. Adaptive Feature Poisoning assumes that probing can occur silently and continuously, and introduces dynamic and context aware perturbations to selected traffic features, corrupting the attacker feedback loop without impacting detection capabilities. The method leverages traffic profiling, change point detection, and adaptive scaling to selectively perturb features that an attacker is likely exploiting, based on observed deviations.\n  We evaluate Adaptive Feature Poisoning against multiple realistic adversarial attack strategies, including silent probing, transferability based attacks, and decision boundary based attacks. The results demonstrate its ability to confuse attackers, degrade attack effectiveness, and preserve detection performance. By offering a generalizable, attack agnostic, and undetectable defense, Adaptive Feature Poisoning represents a significant step toward practical and robust adversarial resilience in machine learning based intrusion detection systems.",
            "headline_zh": "提出自适应特征污染以防御基于机器学习的入侵检测系统中的黑盒对抗攻击",
            "intro_zh": [
                "核心问题：现有防御方法针对特定攻击、需模型访问或静态机制，难以泛化且影响检测性能。",
                "方法要点：通过流量分析、变化点检测和自适应缩放，动态扰动攻击者可能利用的特征，破坏反馈循环。",
                "实验或效果：评估显示能混淆攻击者、降低攻击效果并保持检测性能，具有通用性和不可检测性。"
            ],
            "tags_zh": [
                "黑盒对抗攻击",
                "入侵检测系统",
                "自适应特征污染",
                "流量分析",
                "变化点检测",
                "攻击防御"
            ],
            "_index": 57
        },
        {
            "title": "On-Device Continual Learning for Unsupervised Visual Anomaly Detection in Dynamic Manufacturing",
            "authors": [
                "Haoyu Ren",
                "Kay Koehle",
                "Kirill Dorofeev",
                "Darko Anicic"
            ],
            "arxiv_id": "2512.13497v1",
            "summary": "In modern manufacturing, Visual Anomaly Detection (VAD) is essential for automated inspection and consistent product quality. Yet, increasingly dynamic and flexible production environments introduce key challenges: First, frequent product changes in small-batch and on-demand manufacturing require rapid model updates. Second, legacy edge hardware lacks the resources to train and run large AI models. Finally, both anomalous and normal training data are often scarce, particularly for newly introduced product variations. We investigate on-device continual learning for unsupervised VAD with localization, extending the PatchCore to incorporate online learning for real-world industrial scenarios. The proposed method leverages a lightweight feature extractor and an incremental coreset update mechanism based on k-center selection, enabling rapid, memory-efficient adaptation from limited data while eliminating costly cloud retraining. Evaluations on an industrial use case are conducted using a testbed designed to emulate flexible production with frequent variant changes in a controlled environment. Our method achieves a 12% AUROC improvement over the baseline, an 80% reduction in memory usage, and faster training compared to batch retraining. These results confirm that our method delivers accurate, resource-efficient, and adaptive VAD suitable for dynamic and smart manufacturing.",
            "headline_zh": "提出基于设备端持续学习的无监督视觉异常检测方法，以应对动态制造中的快速产品变化和资源限制。",
            "intro_zh": [
                "核心问题：动态制造中产品频繁变化、边缘设备资源有限及异常数据稀缺，导致传统视觉异常检测难以适应。",
                "方法要点：扩展PatchCore，采用轻量特征提取器和基于k中心选择的增量核心集更新机制，实现设备端在线学习。",
                "实验或效果：在工业用例中，AUROC提升12%，内存使用减少80%，训练速度优于批量重训练。"
            ],
            "tags_zh": [
                "设备端持续学习",
                "无监督视觉异常检测",
                "动态制造",
                "边缘计算",
                "核心集更新",
                "工业应用"
            ],
            "_index": 58
        },
        {
            "title": "Soul: Breathe Life into Digital Human for High-fidelity Long-term Multimodal Animation",
            "authors": [
                "Jiangning Zhang",
                "Junwei Zhu",
                "Zhenye Gan",
                "Donghao Luo",
                "Chuming Lin",
                "Feifan Xu",
                "Xu Peng",
                "Jianlong Hu",
                "Yuansen Liu",
                "Yijia Hong",
                "Weijian Cao",
                "Han Feng",
                "Xu Chen",
                "Chencan Fu",
                "Keke He",
                "Xiaobin Hu",
                "Chengjie Wang"
            ],
            "arxiv_id": "2512.13495v1",
            "summary": "We propose a multimodal-driven framework for high-fidelity long-term digital human animation termed $\\textbf{Soul}$, which generates semantically coherent videos from a single-frame portrait image, text prompts, and audio, achieving precise lip synchronization, vivid facial expressions, and robust identity preservation. We construct Soul-1M, containing 1 million finely annotated samples with a precise automated annotation pipeline (covering portrait, upper-body, full-body, and multi-person scenes) to mitigate data scarcity, and we carefully curate Soul-Bench for comprehensive and fair evaluation of audio-/text-guided animation methods. The model is built on the Wan2.2-5B backbone, integrating audio-injection layers and multiple training strategies together with threshold-aware codebook replacement to ensure long-term generation consistency. Meanwhile, step/CFG distillation and a lightweight VAE are used to optimize inference efficiency, achieving an 11.4$\\times$ speedup with negligible quality loss. Extensive experiments show that Soul significantly outperforms current leading open-source and commercial models on video quality, video-text alignment, identity preservation, and lip-synchronization accuracy, demonstrating broad applicability in real-world scenarios such as virtual anchors and film production. Project page at https://zhangzjn.github.io/projects/Soul/",
            "headline_zh": "提出Soul框架，通过多模态输入生成高保真长时数字人动画，应用于虚拟主播和影视制作。",
            "intro_zh": [
                "核心问题：数字人动画面临数据稀缺、长期生成一致性差和推理效率低等挑战。",
                "方法要点：基于Wan2.2-5B骨干，集成音频注入层、阈值感知码本替换和蒸馏策略，优化生成质量与速度。",
                "实验或效果：构建Soul-1M数据集和Soul-Bench基准，在视频质量、唇同步和身份保持上显著超越现有模型。"
            ],
            "tags_zh": [
                "数字人动画",
                "多模态生成",
                "长时一致性",
                "唇同步",
                "蒸馏训练",
                "数据集构建"
            ],
            "_index": 59
        },
        {
            "title": "SkipCat: Rank-Maximized Low-Rank Compression of Large Language Models via Shared Projection and Block Skipping",
            "authors": [
                "Yu-Chen Lu",
                "Sheng-Feng Yu",
                "Hui-Hsien Weng",
                "Pei-Shuo Wang",
                "Yu-Fang Hu",
                "Liang Hung-Chun",
                "Hung-Yueh Chiang",
                "Kai-Chiang Wu"
            ],
            "arxiv_id": "2512.13494v1",
            "summary": "Large language models (LLM) have achieved remarkable performance across a wide range of tasks. However, their substantial parameter sizes pose significant challenges for deployment on edge devices with limited computational and memory resources. Low-rank compression is a promising approach to address this issue, as it reduces both computational and memory costs, making LLM more suitable for resource-constrained environments. Nonetheless, naïve low-rank compression methods require a significant reduction in the retained rank to achieve meaningful memory and computation savings. For a low-rank model, the ranks need to be reduced by more than half to yield efficiency gains. Such aggressive truncation, however, typically results in substantial performance degradation. To address this trade-off, we propose SkipCat, a novel low-rank compression framework that enables the use of higher ranks while achieving the same compression rates. First, we introduce an intra-layer shared low-rank projection method, where multiple matrices that share the same input use a common projection. This reduces redundancy and improves compression efficiency. Second, we propose a block skipping technique that omits computations and memory transfers for selected sub-blocks within the low-rank decomposition. These two techniques jointly enable our compressed model to retain more effective ranks under the same compression budget. Experimental results show that, without any additional fine-tuning, our method outperforms previous low-rank compression approaches by 7% accuracy improvement on zero-shot tasks under the same compression rate. These results highlight the effectiveness of our rank-maximized compression strategy in preserving model performance under tight resource constraints.",
            "headline_zh": "提出SkipCat框架，通过共享投影和块跳过实现大语言模型的高效低秩压缩",
            "intro_zh": [
                "核心问题：低秩压缩需大幅降低秩以节省资源，但导致性能显著下降",
                "方法要点：引入层内共享低秩投影和块跳过技术，在相同压缩率下保留更多有效秩",
                "实验或效果：在零样本任务上，相同压缩率下准确率提升7%，无需额外微调"
            ],
            "tags_zh": [
                "大语言模型压缩",
                "低秩分解",
                "共享投影",
                "块跳过",
                "资源受限部署",
                "零样本性能"
            ],
            "_index": 60
        },
        {
            "title": "Transform Trained Transformer: Accelerating Naive 4K Video Generation Over 10$\\times$",
            "authors": [
                "Jiangning Zhang",
                "Junwei Zhu",
                "Teng Hu",
                "Yabiao Wang",
                "Donghao Luo",
                "Weijian Cao",
                "Zhenye Gan",
                "Xiaobin Hu",
                "Zhucun Xue",
                "Chengjie Wang"
            ],
            "arxiv_id": "2512.13492v1",
            "summary": "Native 4K (2160$\\times$3840) video generation remains a critical challenge due to the quadratic computational explosion of full-attention as spatiotemporal resolution increases, making it difficult for models to strike a balance between efficiency and quality. This paper proposes a novel Transformer retrofit strategy termed $\\textbf{T3}$ ($\\textbf{T}$ransform $\\textbf{T}$rained $\\textbf{T}$ransformer) that, without altering the core architecture of full-attention pretrained models, significantly reduces compute requirements by optimizing their forward logic. Specifically, $\\textbf{T3-Video}$ introduces a multi-scale weight-sharing window attention mechanism and, via hierarchical blocking together with an axis-preserving full-attention design, can effect an \"attention pattern\" transformation of a pretrained model using only modest compute and data. Results on 4K-VBench show that $\\textbf{T3-Video}$ substantially outperforms existing approaches: while delivering performance improvements (+4.29$\\uparrow$ VQA and +0.08$\\uparrow$ VTC), it accelerates native 4K video generation by more than 10$\\times$. Project page at https://zhangzjn.github.io/projects/T3-Video",
            "headline_zh": "提出T3-Video方法，通过优化前向逻辑加速原生4K视频生成超10倍。",
            "intro_zh": [
                "核心问题：原生4K视频生成面临全注意力计算爆炸，效率与质量难以平衡。",
                "方法要点：引入多尺度权重共享窗口注意力，结合分层分块和轴保持全注意力设计，无需改变预训练模型架构。",
                "实验或效果：在4K-VBench上性能提升（VQA +4.29，VTC +0.08），加速超10倍。"
            ],
            "tags_zh": [
                "视频生成",
                "Transformer优化",
                "4K分辨率",
                "注意力机制",
                "计算加速"
            ],
            "_index": 61
        },
        {
            "title": "From Zipf's Law to Neural Scaling through Heaps' Law and Hilberg's Hypothesis",
            "authors": [
                "Łukasz Dębowski"
            ],
            "arxiv_id": "2512.13491v1",
            "summary": "We inspect the deductive connection between the neural scaling law and Zipf's law -- two statements discussed in machine learning and quantitative linguistics. The neural scaling law describes how the cross entropy rate of a foundation model -- such as a large language model -- changes with respect to the amount of training tokens, parameters, and compute. By contrast, Zipf's law posits that the distribution of tokens exhibits a power law tail. Whereas similar claims have been made in more specific settings, we show that the neural scaling law is a consequence of Zipf's law under certain broad assumptions that we reveal systematically. The derivation steps are as follows: We derive Heaps' law on the vocabulary growth from Zipf's law, Hilberg's hypothesis on the entropy scaling from Heaps' law, and the neural scaling from Hilberg's hypothesis. We illustrate these inference steps by a toy example of the Santa Fe process that satisfies all the four statistical laws.",
            "headline_zh": "从Zipf定律推导神经缩放定律，揭示语言模型统计规律间的演绎联系",
            "intro_zh": [
                "核心问题：探究神经缩放定律与Zipf定律之间的演绎关系，解释基础模型性能随训练数据、参数和计算量变化的统计基础",
                "方法要点：通过系统假设，从Zipf定律推导Heaps定律，再推导Hilberg假设，最终得出神经缩放定律",
                "实验或效果：以Santa Fe过程为例，验证了四种统计定律的满足情况，支持理论推导"
            ],
            "tags_zh": [
                "神经缩放定律",
                "Zipf定律",
                "Heaps定律",
                "Hilberg假设",
                "基础模型",
                "统计语言学"
            ],
            "_index": 62
        },
        {
            "title": "Real-Time AI-Driven Milling Digital Twin Towards Extreme Low-Latency",
            "authors": [
                "Wenyi Liu",
                "R. Sharma",
                "W. \"Grace\" Guo",
                "J. Yi",
                "Y. B. Guo"
            ],
            "arxiv_id": "2512.13482v1",
            "summary": "Digital twin (DT) enables smart manufacturing by leveraging real-time data, AI models, and intelligent control systems. This paper presents a state-of-the-art analysis on the emerging field of DTs in the context of milling. The critical aspects of DT are explored through the lens of virtual models of physical milling, data flow from physical milling to virtual model, and feedback from virtual model to physical milling. Live data streaming protocols and virtual modeling methods are highlighted. A case study showcases the transformative capability of a real-time machine learning-driven live DT of tool-work contact in a milling process. Future research directions are outlined to achieve the goals of Industry 4.0 and beyond.",
            "headline_zh": "提出实时AI驱动的铣削数字孪生，实现极低延迟的智能制造",
            "intro_zh": [
                "核心问题：数字孪生在铣削应用中需处理实时数据流与虚拟模型同步，以实现智能控制。",
                "方法要点：分析物理铣削虚拟模型、数据流协议和建模方法，强调AI模型在实时反馈中的作用。",
                "实验或效果：通过铣削工具-工件接触的案例研究，展示机器学习驱动的实时数字孪生变革能力。"
            ],
            "tags_zh": [
                "数字孪生",
                "实时AI",
                "铣削加工",
                "智能制造",
                "数据流协议",
                "虚拟建模"
            ],
            "_index": 63
        },
        {
            "title": "neuralFOMO: Can LLMs Handle Being Second Best? Measuring Envy-Like Preferences in Multi-Agent Settings",
            "authors": [
                "Ojas Pungalia",
                "Rashi Upadhyay",
                "Abhishek Mishra",
                "Abhiram H",
                "Tejasvi Alladi",
                "Sujan Yenuganti",
                "Dhruv Kumar"
            ],
            "arxiv_id": "2512.13481v1",
            "summary": "Envy is a common human behavior that shapes competitiveness and can alter outcomes in team settings. As large language models (LLMs) increasingly act on behalf of humans in collaborative and competitive workflows, there is a pressing need to evaluate whether and under what conditions they exhibit envy-like preferences. In this paper, we test whether LLMs show envy-like behavior toward each other. We considered two scenarios: (1) A point allocation game that tests whether a model tries to win over its peer. (2) A workplace setting observing behaviour when recognition is unfair. Our findings reveal consistent evidence of envy-like patterns in certain LLMs, with large variation across models and contexts. For instance, GPT-5-mini and Claude-3.7-Sonnet show a clear tendency to pull down the peer model to equalize outcomes, whereas Mistral-Small-3.2-24B instead focuses on maximizing its own individual gains. These results highlight the need to consider competitive dispositions as a safety and design factor in LLM-based multi-agent systems.",
            "headline_zh": "提出neuralFOMO框架，评估LLM在多人场景中是否表现出嫉妒类偏好",
            "intro_zh": [
                "核心问题：LLM在协作与竞争场景中是否会产生类似人类嫉妒的行为模式",
                "方法要点：设计点分配游戏和工作场所情境，测试LLM对同伴的反应策略",
                "实验效果：发现GPT-5-mini等模型有明显拉平结果的倾向，不同模型行为差异显著"
            ],
            "tags_zh": [
                "大语言模型",
                "多智能体系统",
                "嫉妒行为",
                "社会偏好",
                "模型评估",
                "人机协作"
            ],
            "_index": 64
        },
        {
            "title": "Element-wise Modulation of Random Matrices for Efficient Neural Layers",
            "authors": [
                "Maksymilian Szorc"
            ],
            "arxiv_id": "2512.13480v1",
            "summary": "Fully connected layers are a primary source of memory and computational overhead in deep neural networks due to their dense, often redundant parameterization. While various compression techniques exist, they frequently introduce complex engineering trade-offs or degrade model performance. We propose the Parametrized Random Projection (PRP) layer, a novel approach that decouples feature mixing from adaptation by utilizing a fixed random matrix modulated by lightweight, learnable element-wise parameters. This architecture drastically reduces the trainable parameter count to a linear scale while retaining reliable accuracy across various benchmarks. The design serves as a stable, computationally efficient solution for architectural scaling and deployment in resource-limited settings.",
            "headline_zh": "提出参数化随机投影层以解决全连接层内存与计算开销问题",
            "intro_zh": [
                "全连接层因密集参数化导致内存与计算开销大，现有压缩技术常引入复杂权衡或性能下降",
                "使用固定随机矩阵与轻量可学习逐元素参数解耦特征混合与适应，大幅减少可训练参数至线性规模",
                "在多种基准测试中保持可靠精度，为资源受限场景提供稳定高效架构"
            ],
            "tags_zh": [
                "全连接层压缩",
                "随机投影",
                "参数化调制",
                "轻量神经网络",
                "资源受限部署"
            ],
            "_index": 65
        },
        {
            "title": "Non-Resolution Reasoning: A Framework for Preserving Semantic Ambiguity in Language Models",
            "authors": [
                "Kei Saito"
            ],
            "arxiv_id": "2512.13478v1",
            "summary": "Premature semantic collapse -- the forced early commitment to a single meaning -- remains a core architectural limitation of current language models. Softmax-driven competition and greedy decoding cause models to discard valid interpretations before sufficient context is available, resulting in brittle reasoning and context failures. We introduce Non-Resolution Reasoning (NRR), a general computational framework that preserves semantic ambiguity during inference and performs resolution only when explicitly required. NRR integrates three components: (1) Multi-Vector Embeddings that maintain multiple viable interpretations per token, (2) Non-Collapsing Attention that prevents winner-take-all dynamics across layers, and (3) Contextual Identity Tracking (CIT), which assigns context-specific identities to recurring entities (e.g., distinguishing \"Dr. Smith the cardiologist\" from \"Dr. Smith the researcher\"). These mechanisms are unified by an external Resolution Operator $ρ$ that makes semantic commitment explicit, controllable, and task-dependent. Unlike standard architectures, NRR separates representation from resolution, allowing a single model to shift between creative, factual, and ambiguity-preserving reasoning without retraining. A synthetic evaluation demonstrates NRR's ability to preserve ambiguity and track context: CIT-enhanced models achieve 90.9% accuracy on out-of-distribution identity-shift tasks, compared to 9.1% for transformer baselines. NRR provides a principled alternative to premature collapse, reframing ambiguity as an explicit representational state rather than a failure mode. The question is not whether AI should resolve ambiguity, but when, how, and under whose control.",
            "headline_zh": "提出非解析推理框架以解决语言模型过早语义塌陷问题",
            "intro_zh": [
                "核心问题：当前语言模型因softmax竞争和贪婪解码导致过早语义塌陷，丢弃有效解释。",
                "方法要点：集成多向量嵌入、非塌陷注意力和上下文身份跟踪，通过外部解析算子控制语义承诺。",
                "实验或效果：合成评估显示，在分布外身份转移任务中准确率达90.9%，远超基线9.1%。"
            ],
            "tags_zh": [
                "语义模糊性",
                "推理框架",
                "注意力机制",
                "上下文跟踪",
                "语言模型架构"
            ],
            "_index": 66
        },
        {
            "title": "Evaluating the Navigation Capabilities of a Modified COAST Guidewire Robot in an Anatomical Phantom Model",
            "authors": [
                "Timothy A. Brumfiel",
                "Revanth Konda",
                "Drew Elliott",
                "Jaydev P. Desai"
            ],
            "arxiv_id": "2512.13477v1",
            "summary": "To address the issues that arise due to the manual navigation of guidewires in endovascular interventions, research in medical robotics has taken a strong interest in developing robotically steerable guidewires, which offer the possibility of enhanced maneuverability and navigation, as the tip of the guidewire can be actively steered. The COaxially Aligned STeerable (COAST) guidewire robot has the ability to generate a wide variety of motions including bending motion with different bending lengths, follow-the-leader motion, and feedforward motion. In our past studies, we have explored different designs of the COAST guidewire robot and developed modeling, control, and sensing strategies for the COAST guidewire robot. In this study, the performance of a modified COAST guidewire robot is evaluated by conducting navigation experiments in an anatomical phantom model with pulsatile flow. The modified COAST guidewire robot is a simplified version of the COAST guidewire robot and consists of two tubes as opposed to three tubes. Through this study, we demonstrate the effectiveness of the modified COAST guidewire robot in navigating the tortuous phantom vasculature.",
            "headline_zh": "评估改进型COAST导丝机器人在解剖模型中的导航性能，以解决手动导航问题。",
            "intro_zh": [
                "核心问题：手动导丝导航在血管介入中易引发操作困难，需机器人辅助提升可操作性。",
                "方法要点：改进COAST导丝机器人，简化设计为双管结构，支持弯曲、跟随和前进等多种运动。",
                "实验或效果：在带脉动流的解剖模型中进行导航实验，验证其在复杂血管路径中的有效性。"
            ],
            "tags_zh": [
                "医疗机器人",
                "导丝导航",
                "解剖模型",
                "脉动流",
                "机器人控制"
            ],
            "_index": 67
        },
        {
            "title": "PoseAnything: Universal Pose-guided Video Generation with Part-aware Temporal Coherence",
            "authors": [
                "Ruiyan Wang",
                "Teng Hu",
                "Kaihui Huang",
                "Zihan Su",
                "Ran Yi",
                "Lizhuang Ma"
            ],
            "arxiv_id": "2512.13465v1",
            "summary": "Pose-guided video generation refers to controlling the motion of subjects in generated video through a sequence of poses. It enables precise control over subject motion and has important applications in animation. However, current pose-guided video generation methods are limited to accepting only human poses as input, thus generalizing poorly to pose of other subjects. To address this issue, we propose PoseAnything, the first universal pose-guided video generation framework capable of handling both human and non-human characters, supporting arbitrary skeletal inputs. To enhance consistency preservation during motion, we introduce Part-aware Temporal Coherence Module, which divides the subject into different parts, establishes part correspondences, and computes cross-attention between corresponding parts across frames to achieve fine-grained part-level consistency. Additionally, we propose Subject and Camera Motion Decoupled CFG, a novel guidance strategy that, for the first time, enables independent camera movement control in pose-guided video generation, by separately injecting subject and camera motion control information into the positive and negative anchors of CFG. Furthermore, we present XPose, a high-quality public dataset containing 50,000 non-human pose-video pairs, along with an automated pipeline for annotation and filtering. Extensive experiments demonstrate that Pose-Anything significantly outperforms state-of-the-art methods in both effectiveness and generalization.",
            "headline_zh": "提出PoseAnything框架以解决姿态引导视频生成中仅支持人类姿态的局限性，实现通用姿态控制。",
            "intro_zh": [
                "核心问题：现有方法仅接受人类姿态输入，泛化能力差，无法处理非人类角色。",
                "方法要点：引入Part-aware Temporal Coherence Module实现细粒度部分一致性，并设计Subject and Camera Motion Decoupled CFG独立控制相机运动。",
                "实验或效果：在XPose数据集上验证，显著优于现有方法，支持任意骨骼输入和高质量视频生成。"
            ],
            "tags_zh": [
                "姿态引导视频生成",
                "通用姿态控制",
                "部分感知时序一致性",
                "相机运动解耦",
                "非人类姿态数据集"
            ],
            "_index": 68
        },
        {
            "title": "DP-EMAR: A Differentially Private Framework for Autonomous Model Weight Repair in Federated IoT Systems",
            "authors": [
                "Chethana Prasad Kabgere",
                "Shylaja S S"
            ],
            "arxiv_id": "2512.13460v1",
            "summary": "Federated Learning (FL) enables decentralized model training without sharing raw data, but model weight distortion remains a major challenge in resource constrained IoT networks. In multi tier Federated IoT (Fed-IoT) systems, unstable connectivity and adversarial interference can silently alter transmitted parameters, degrading convergence. We propose DP-EMAR, a differentially private, error model based autonomous repair framework that detects and reconstructs transmission induced distortions during FL aggregation. DP-EMAR estimates corruption patterns and applies adaptive correction before privacy noise is added, enabling reliable in network repair without violating confidentiality. By integrating Differential Privacy (DP) with Secure Aggregation (SA), the framework distinguishes DP noise from genuine transmission errors. Experiments on heterogeneous IoT sensor and graph datasets show that DP-EMAR preserves convergence stability and maintains near baseline performance under communication corruption while ensuring strict (epsilon, delta)-DP guarantees. The framework enhances robustness, communication efficiency, and trust in privacy preserving Federated IoT learning.",
            "headline_zh": "提出DP-EMAR框架，以差分隐私方式修复联邦物联网系统中的模型权重传输失真问题。",
            "intro_zh": [
                "核心问题：联邦物联网中模型权重因传输失真影响收敛，需在隐私保护下修复。",
                "方法要点：结合差分隐私与安全聚合，检测并自适应校正传输错误，区分噪声与真实失真。",
                "实验或效果：在异构数据集上验证，保持收敛稳定性和近基线性能，确保严格差分隐私保证。"
            ],
            "tags_zh": [
                "联邦学习",
                "差分隐私",
                "物联网系统",
                "模型修复",
                "安全聚合"
            ],
            "_index": 69
        },
        {
            "title": "SSAS: Cross-subject EEG-based Emotion Recognition through Source Selection with Adversarial Strategy",
            "authors": [
                "Yici Liu",
                "Qi Wei Oung",
                "Hoi Leong Lee"
            ],
            "arxiv_id": "2512.13458v1",
            "summary": "Electroencephalographic (EEG) signals have long been applied in the field of affective brain-computer interfaces (aBCIs). Cross-subject EEG-based emotion recognition has demonstrated significant potential in practical applications due to its suitability across diverse people. However, most studies on cross-subject EEG-based emotion recognition neglect the presence of inter-individual variability and negative transfer phenomena during model training. To address this issue, a cross-subject EEG-based emotion recognition through source selection with adversarial strategy is introduced in this paper. The proposed method comprises two modules: the source selection network (SS) and the adversarial strategies network (AS). The SS uses domain labels to reverse-engineer the training process of domain adaptation. Its key idea is to disrupt class separability and magnify inter-domain differences, thereby raising the classification difficulty and forcing the model to learn domain-invariant yet emotion-relevant representations. The AS gets the source domain selection results and the pretrained domain discriminators from SS. The pretrained domain discriminators compute a novel loss aimed at enhancing the performance of domain classification during adversarial training, ensuring the balance of adversarial strategies. This paper provides theoretical insights into the proposed method and achieves outstanding performance on two EEG-based emotion datasets, SEED and SEED-IV. The code can be found at https://github.com/liuyici/SSAS.",
            "headline_zh": "提出SSAS方法，通过源选择与对抗策略解决跨被试EEG情绪识别中的个体差异与负迁移问题。",
            "intro_zh": [
                "核心问题：跨被试EEG情绪识别存在个体差异与负迁移，影响模型泛化能力。",
                "方法要点：结合源选择网络与对抗策略网络，学习域不变且情绪相关的表示。",
                "实验或效果：在SEED和SEED-IV数据集上表现优异，代码已开源。"
            ],
            "tags_zh": [
                "跨被试情绪识别",
                "脑电信号处理",
                "源选择网络",
                "对抗训练",
                "域适应"
            ],
            "_index": 70
        },
        {
            "title": "Test-Time Modification: Inverse Domain Transformation for Robust Perception",
            "authors": [
                "Arpit Jadon",
                "Joshua Niemeijer",
                "Yuki M. Asano"
            ],
            "arxiv_id": "2512.13454v1",
            "summary": "Generative foundation models contain broad visual knowledge and can produce diverse image variations, making them particularly promising for advancing domain generalization tasks. While they can be used for training data augmentation, synthesizing comprehensive target-domain variations remains slow, expensive, and incomplete. We propose an alternative: using diffusion models at test time to map target images back to the source distribution where the downstream model was trained. This approach requires only a source domain description, preserves the task model, and eliminates large-scale synthetic data generation. We demonstrate consistent improvements across segmentation, detection, and classification tasks under challenging environmental shifts in real-to-real domain generalization scenarios with unknown target distributions. Our analysis spans multiple generative and downstream models, including an ensemble variant for enhanced robustness. The method achieves substantial relative gains: 137% on BDD100K-Night, 68% on ImageNet-R, and 62% on DarkZurich.",
            "headline_zh": "提出测试时逆域变换方法，利用扩散模型将目标图像映射回源分布以提升未知域泛化性能。",
            "intro_zh": [
                "核心问题：生成模型用于训练数据增强时，合成全面目标域变体成本高且不完整。",
                "方法要点：在测试时使用扩散模型将目标图像逆变换到源分布，无需大规模合成数据。",
                "实验或效果：在分割、检测和分类任务中，对未知目标分布实现显著性能提升，如BDD100K-Night上相对增益137%。"
            ],
            "tags_zh": [
                "域泛化",
                "测试时修改",
                "扩散模型",
                "逆域变换",
                "鲁棒感知"
            ],
            "_index": 71
        },
        {
            "title": "XNNTab -- Interpretable Neural Networks for Tabular Data using Sparse Autoencoders",
            "authors": [
                "Khawla Elhadri",
                "Jörg Schlötterer",
                "Christin Seifert"
            ],
            "arxiv_id": "2512.13442v1",
            "summary": "In data-driven applications relying on tabular data, where interpretability is key, machine learning models such as decision trees and linear regression are applied. Although neural networks can provide higher predictive performance, they are not used because of their blackbox nature. In this work, we present XNNTab, a neural architecture that combines the expressiveness of neural networks and interpretability. XNNTab first learns highly non-linear feature representations, which are decomposed into monosemantic features using a sparse autoencoder (SAE). These features are then assigned human-interpretable concepts, making the overall model prediction intrinsically interpretable. XNNTab outperforms interpretable predictive models, and achieves comparable performance to its non-interpretable counterparts.",
            "headline_zh": "提出XNNTab，结合稀疏自编码器实现表格数据神经网络的解释性，以解决黑盒问题。",
            "intro_zh": [
                "核心问题：表格数据应用中，神经网络因黑盒特性难以用于需解释性的场景。",
                "方法要点：使用稀疏自编码器分解非线性特征为单语义特征，并赋予可解释概念。",
                "实验或效果：XNNTab优于可解释模型，性能与非解释性神经网络相当。"
            ],
            "tags_zh": [
                "表格数据",
                "神经网络解释性",
                "稀疏自编码器",
                "可解释机器学习",
                "特征分解"
            ],
            "_index": 72
        },
        {
            "title": "IMILIA: interpretable multiple instance learning for inflammation prediction in IBD from H&E whole slide images",
            "authors": [
                "Thalyssa Baiocco-Rodrigues",
                "Antoine Olivier",
                "Reda Belbahri",
                "Thomas Duboudin",
                "Pierre-Antoine Bannier",
                "Benjamin Adjadj",
                "Katharina Von Loga",
                "Nathan Noiry",
                "Maxime Touzot",
                "Hector Roux de Bezieux"
            ],
            "arxiv_id": "2512.13440v1",
            "summary": "As the therapeutic target for Inflammatory Bowel Disease (IBD) shifts toward histologic remission, the accurate assessment of microscopic inflammation has become increasingly central for evaluating disease activity and response to treatment. In this work, we introduce IMILIA (Interpretable Multiple Instance Learning for Inflammation Analysis), an end-to-end framework designed for the prediction of inflammation presence in IBD digitized slides stained with hematoxylin and eosin (H&E), followed by the automated computation of markers characterizing tissue regions driving the predictions. IMILIA is composed of an inflammation prediction module, consisting of a Multiple Instance Learning (MIL) model, and an interpretability module, divided in two blocks: HistoPLUS, for cell instance detection, segmentation and classification; and EpiSeg, for epithelium segmentation. IMILIA achieves a cross-validation ROC-AUC of 0.83 on the discovery cohort, and a ROC-AUC of 0.99 and 0.84 on two external validation cohorts. The interpretability module yields biologically consistent insights: tiles with higher predicted scores show increased densities of immune cells (lymphocytes, plasmocytes, neutrophils and eosinophils), whereas lower-scored tiles predominantly contain normal epithelial cells. Notably, these patterns were consistent across all datasets. Code and models to partially replicate the results on the public IBDColEpi dataset can be found at https://github.com/owkin/imilia.",
            "headline_zh": "提出IMILIA框架，用于从H&E全切片图像预测IBD炎症并解释预测结果。",
            "intro_zh": [
                "核心问题：IBD治疗转向组织学缓解，需准确评估微观炎症以指导治疗。",
                "方法要点：结合多示例学习预测炎症，通过细胞检测和上皮分割模块提供可解释性。",
                "实验或效果：在发现队列中ROC-AUC为0.83，外部验证队列中最高达0.99，可解释结果与生物学一致。"
            ],
            "tags_zh": [
                "炎症性肠病",
                "多示例学习",
                "全切片图像分析",
                "可解释性AI",
                "组织病理学"
            ],
            "_index": 73
        },
        {
            "title": "From User Interface to Agent Interface: Efficiency Optimization of UI Representations for LLM Agents",
            "authors": [
                "Dezhi Ran",
                "Zhi Gong",
                "Yuzhe Guo",
                "Mengzhou Wu",
                "Yuan Cao",
                "Haochuan Lu",
                "Hengyu Zhang",
                "Xia Zeng",
                "Gang Cao",
                "Liangchao Yao",
                "Yuetang Deng",
                "Wei Yang",
                "Tao Xie"
            ],
            "arxiv_id": "2512.13438v1",
            "summary": "While Large Language Model (LLM) agents show great potential for automated UI navigation such as automated UI testing and AI assistants, their efficiency has been largely overlooked. Our motivating study reveals that inefficient UI representation creates a critical performance bottleneck. However, UI representation optimization, formulated as the task of automatically generating programs that transform UI representations, faces two unique challenges. First, the lack of Boolean oracles, which traditional program synthesis uses to decisively validate semantic correctness, poses a fundamental challenge to co-optimization of token efficiency and completeness. Second, the need to process large, complex UI trees as input while generating long, compositional transformation programs, making the search space vast and error-prone. Toward addressing the preceding limitations, we present UIFormer, the first automated optimization framework that synthesizes UI transformation programs by conducting constraint-based optimization with structured decomposition of the complex synthesis task. First, UIFormer restricts the program space using a domain-specific language (DSL) that captures UI-specific operations. Second, UIFormer conducts LLM-based iterative refinement with correctness and efficiency rewards, providing guidance for achieving the efficiency-completeness co-optimization. UIFormer operates as a lightweight plugin that applies transformation programs for seamless integration with existing LLM agents, requiring minimal modifications to their core logic. Evaluations across three UI navigation benchmarks spanning Android and Web platforms with five LLMs demonstrate that UIFormer achieves 48.7% to 55.8% token reduction with minimal runtime overhead while maintaining or improving agent performance. Real-world industry deployment at WeChat further validates the practical impact of UIFormer.",
            "headline_zh": "提出UIFormer框架以优化UI表示，提升LLM代理在自动化UI导航中的效率",
            "intro_zh": [
                "核心问题：UI表示效率低下成为LLM代理性能瓶颈，缺乏布尔预言机阻碍语义正确性验证",
                "方法要点：基于DSL限制程序空间，结合LLM迭代优化，实现效率与完整性的协同优化",
                "实验或效果：在Android和Web基准测试中实现48.7%至55.8%的令牌减少，保持或提升代理性能"
            ],
            "tags_zh": [
                "UI表示优化",
                "LLM代理",
                "程序合成",
                "自动化UI导航",
                "效率提升"
            ],
            "_index": 74
        },
        {
            "title": "Self-Supervised Ultrasound Representation Learning for Renal Anomaly Prediction in Prenatal Imaging",
            "authors": [
                "Youssef Megahed",
                "Inok Lee",
                "Robin Ducharme",
                "Kevin Dick",
                "Adrian D. C. Chan",
                "Steven Hawken",
                "Mark C. Walker"
            ],
            "arxiv_id": "2512.13434v1",
            "summary": "Prenatal ultrasound is the cornerstone for detecting congenital anomalies of the kidneys and urinary tract, but diagnosis is limited by operator dependence and suboptimal imaging conditions. We sought to assess the performance of a self-supervised ultrasound foundation model for automated fetal renal anomaly classification using a curated dataset of 969 two-dimensional ultrasound images. A pretrained Ultrasound Self-Supervised Foundation Model with Masked Autoencoding (USF-MAE) was fine-tuned for binary and multi-class classification of normal kidneys, urinary tract dilation, and multicystic dysplastic kidney. Models were compared with a DenseNet-169 convolutional baseline using cross-validation and an independent test set. USF-MAE consistently improved upon the baseline across all evaluation metrics in both binary and multi-class settings. USF-MAE achieved an improvement of about 1.87% (AUC) and 7.8% (F1-score) on the validation set, 2.32% (AUC) and 4.33% (F1-score) on the independent holdout test set. The largest gains were observed in the multi-class setting, where the improvement in AUC was 16.28% and 46.15% in F1-score. To facilitate model interpretability, Score-CAM visualizations were adapted for a transformer architecture and show that model predictions were informed by known, clinically relevant renal structures, including the renal pelvis in urinary tract dilation and cystic regions in multicystic dysplastic kidney. These results show that ultrasound-specific self-supervised learning can generate a useful representation as a foundation for downstream diagnostic tasks. The proposed framework offers a robust, interpretable approach to support the prenatal detection of renal anomalies and demonstrates the promise of foundation models in obstetric imaging.",
            "headline_zh": "提出自监督超声基础模型USF-MAE，用于产前超声图像中胎儿肾脏异常的自动分类。",
            "intro_zh": [
                "核心问题：产前超声诊断肾脏异常受操作者依赖和成像条件限制，需自动化辅助。",
                "方法要点：使用掩码自编码预训练的超声基础模型USF-MAE，微调进行二分类和多分类任务。",
                "实验或效果：USF-MAE在验证集和独立测试集上性能优于DenseNet-169基线，多分类提升显著，并通过Score-CAM增强可解释性。"
            ],
            "tags_zh": [
                "自监督学习",
                "超声图像分析",
                "产前诊断",
                "肾脏异常分类",
                "基础模型",
                "可解释性"
            ],
            "_index": 75
        },
        {
            "title": "A Domain-Adapted Lightweight Ensemble for Resource-Efficient Few-Shot Plant Disease Classification",
            "authors": [
                "Anika Islam",
                "Tasfia Tahsin",
                "Zaarin Anjum",
                "Md. Bakhtiar Hasan",
                "Md. Hasanul Kabir"
            ],
            "arxiv_id": "2512.13428v1",
            "summary": "Accurate and timely identification of plant leaf diseases is essential for resilient and sustainable agriculture, yet most deep learning approaches rely on large annotated datasets and computationally intensive models that are unsuitable for data-scarce and resource-constrained environments. To address these challenges we present a few-shot learning approach within a lightweight yet efficient framework that combines domain-adapted MobileNetV2 and MobileNetV3 models as feature extractors, along with a feature fusion technique to generate robust feature representation. For the classification task, the fused features are passed through a Bi-LSTM classifier enhanced with attention mechanisms to capture sequential dependencies and focus on the most relevant features, thereby achieving optimal classification performance even in complex, real-world environments with noisy or cluttered backgrounds. The proposed framework was evaluated across multiple experimental setups, including both laboratory-controlled and field-captured datasets. On tomato leaf diseases from the PlantVillage dataset, it consistently improved performance across 1 to 15 shot scenarios, reaching 98.23+-0.33% at 15 shot, closely approaching the 99.98% SOTA benchmark achieved by a Transductive LSTM with attention, while remaining lightweight and mobile-friendly. Under real-world conditions using field images from the Dhan Shomadhan dataset, it maintained robust performance, reaching 69.28+-1.49% at 15-shot and demonstrating strong resilience to complex backgrounds. Notably, it also outperformed the previous SOTA accuracy of 96.0% on six diseases from PlantVillage, achieving 99.72% with only 15-shot learning. With a compact model size of approximately 40 MB and inference complexity of approximately 1.12 GFLOPs, this work establishes a scalable, mobile-ready foundation for precise plant disease diagnostics in data-scarce regions.",
            "headline_zh": "提出轻量级域适应集成框架，用于资源受限环境下的少样本植物病害分类",
            "intro_zh": [
                "核心问题：传统深度学习方法依赖大数据和计算资源，不适用于数据稀缺和资源受限的农业环境。",
                "方法要点：结合域适应MobileNetV2/V3特征提取、特征融合和注意力增强Bi-LSTM分类器，实现高效少样本学习。",
                "实验或效果：在PlantVillage数据集上15-shot达98.23%准确率，接近SOTA；在真实场景Dhan Shomadhan数据集上保持稳健性能，模型轻量约40MB。"
            ],
            "tags_zh": [
                "少样本学习",
                "植物病害分类",
                "轻量级模型",
                "域适应",
                "注意力机制",
                "移动计算"
            ],
            "_index": 76
        },
        {
            "title": "MineTheGap: Automatic Mining of Biases in Text-to-Image Models",
            "authors": [
                "Noa Cohen",
                "Nurit Spingarn-Eliezer",
                "Inbar Huberman-Spiegelglas",
                "Tomer Michaeli"
            ],
            "arxiv_id": "2512.13427v1",
            "summary": "Text-to-Image (TTI) models generate images based on text prompts, which often leave certain aspects of the desired image ambiguous. When faced with these ambiguities, TTI models have been shown to exhibit biases in their interpretations. These biases can have societal impacts, e.g., when showing only a certain race for a stated occupation. They can also affect user experience when creating redundancy within a set of generated images instead of spanning diverse possibilities. Here, we introduce MineTheGap - a method for automatically mining prompts that cause a TTI model to generate biased outputs. Our method goes beyond merely detecting bias for a given prompt. Rather, it leverages a genetic algorithm to iteratively refine a pool of prompts, seeking for those that expose biases. This optimization process is driven by a novel bias score, which ranks biases according to their severity, as we validate on a dataset with known biases. For a given prompt, this score is obtained by comparing the distribution of generated images to the distribution of LLM-generated texts that constitute variations on the prompt. Code and examples are available on the project's webpage.",
            "headline_zh": "提出MineTheGap方法，自动挖掘文本到图像模型的偏见提示",
            "intro_zh": [
                "核心问题：文本到图像模型在模糊提示下产生偏见，影响社会多样性和用户体验",
                "方法要点：使用遗传算法迭代优化提示，基于新偏见分数比较图像与文本分布",
                "实验或效果：在已知偏见数据集上验证偏见分数，提供代码和示例"
            ],
            "tags_zh": [
                "文本到图像模型",
                "偏见挖掘",
                "遗传算法",
                "偏见分数",
                "自动优化",
                "社会影响"
            ],
            "_index": 77
        },
        {
            "title": "RecTok: Reconstruction Distillation along Rectified Flow",
            "authors": [
                "Qingyu Shi",
                "Size Wu",
                "Jinbin Bai",
                "Kaidong Yu",
                "Yujing Wang",
                "Yunhai Tong",
                "Xiangtai Li",
                "Xuelong Li"
            ],
            "arxiv_id": "2512.13421v1",
            "summary": "Visual tokenizers play a crucial role in diffusion models. The dimensionality of latent space governs both reconstruction fidelity and the semantic expressiveness of the latent feature. However, a fundamental trade-off is inherent between dimensionality and generation quality, constraining existing methods to low-dimensional latent spaces. Although recent works have leveraged vision foundation models to enrich the semantics of visual tokenizers and accelerate convergence, high-dimensional tokenizers still underperform their low-dimensional counterparts. In this work, we propose RecTok, which overcomes the limitations of high-dimensional visual tokenizers through two key innovations: flow semantic distillation and reconstruction--alignment distillation. Our key insight is to make the forward flow in flow matching semantically rich, which serves as the training space of diffusion transformers, rather than focusing on the latent space as in previous works. Specifically, our method distills the semantic information in VFMs into the forward flow trajectories in flow matching. And we further enhance the semantics by introducing a masked feature reconstruction loss. Our RecTok achieves superior image reconstruction, generation quality, and discriminative performance. It achieves state-of-the-art results on the gFID-50K under both with and without classifier-free guidance settings, while maintaining a semantically rich latent space structure. Furthermore, as the latent dimensionality increases, we observe consistent improvements. Code and model are available at https://shi-qingyu.github.io/rectok.github.io.",
            "headline_zh": "提出RecTok通过流语义蒸馏和重建对齐蒸馏，提升高维视觉分词器在扩散模型中的性能。",
            "intro_zh": [
                "核心问题：高维视觉分词器在重建保真度和语义表达间存在权衡，导致生成质量受限。",
                "方法要点：利用流匹配中的前向流作为训练空间，通过蒸馏视觉基础模型语义和掩码特征重建损失增强语义。",
                "实验或效果：在gFID-50K上达到SOTA，随维度增加性能持续提升，保持语义丰富的潜在空间结构。"
            ],
            "tags_zh": [
                "视觉分词器",
                "扩散模型",
                "流匹配",
                "语义蒸馏",
                "图像重建",
                "生成质量"
            ],
            "_index": 78
        },
        {
            "title": "Learning to Generate Cross-Task Unexploitable Examples",
            "authors": [
                "Haoxuan Qu",
                "Qiuchi Xiang",
                "Yujun Cai",
                "Yirui Wu",
                "Majid Mirmehdi",
                "Hossein Rahmani",
                "Jun Liu"
            ],
            "arxiv_id": "2512.13416v1",
            "summary": "Unexploitable example generation aims to transform personal images into their unexploitable (unlearnable) versions before they are uploaded online, thereby preventing unauthorized exploitation of online personal images. Recently, this task has garnered significant research attention due to its critical relevance to personal data privacy. Yet, despite recent progress, existing methods for this task can still suffer from limited practical applicability, as they can fail to generate examples that are broadly unexploitable across different real-world computer vision tasks. To deal with this problem, in this work, we propose a novel Meta Cross-Task Unexploitable Example Generation (MCT-UEG) framework. At the core of our framework, to optimize the unexploitable example generator for effectively producing broadly unexploitable examples, we design a flat-minima-oriented meta training and testing scheme. Extensive experiments show the efficacy of our framework.",
            "headline_zh": "提出MCT-UEG框架以生成跨任务不可利用示例，保护在线个人图像隐私。",
            "intro_zh": [
                "核心问题：现有方法生成的不可利用示例在跨不同计算机视觉任务时效果有限，影响实际应用。",
                "方法要点：设计基于平坦最小值的元训练与测试方案，优化生成器以产生广泛不可利用的示例。",
                "实验或效果：大量实验验证了框架的有效性，但具体性能指标未知。"
            ],
            "tags_zh": [
                "不可利用示例生成",
                "跨任务学习",
                "元训练",
                "个人数据隐私",
                "计算机视觉安全"
            ],
            "_index": 79
        },
        {
            "title": "USTM: Unified Spatial and Temporal Modeling for Continuous Sign Language Recognition",
            "authors": [
                "Ahmed Abul Hasanaath",
                "Hamzah Luqman"
            ],
            "arxiv_id": "2512.13415v1",
            "summary": "Continuous sign language recognition (CSLR) requires precise spatio-temporal modeling to accurately recognize sequences of gestures in videos. Existing frameworks often rely on CNN-based spatial backbones combined with temporal convolution or recurrent modules. These techniques fail in capturing fine-grained hand and facial cues and modeling long-range temporal dependencies. To address these limitations, we propose the Unified Spatio-Temporal Modeling (USTM) framework, a spatio-temporal encoder that effectively models complex patterns using a combination of a Swin Transformer backbone enhanced with lightweight temporal adapter with positional embeddings (TAPE). Our framework captures fine-grained spatial features alongside short and long-term temporal context, enabling robust sign language recognition from RGB videos without relying on multi-stream inputs or auxiliary modalities. Extensive experiments on benchmarked datasets including PHOENIX14, PHOENIX14T, and CSL-Daily demonstrate that USTM achieves state-of-the-art performance against RGB-based as well as multi-modal CSLR approaches, while maintaining competitive performance against multi-stream approaches. These results highlight the strength and efficacy of the USTM framework for CSLR. The code is available at https://github.com/gufranSabri/USTM",
            "headline_zh": "提出USTM框架，通过统一时空建模解决连续手语识别中细粒度特征和长程依赖问题。",
            "intro_zh": [
                "核心问题：现有方法难以捕捉手语视频中的细粒度手部和面部线索及长程时间依赖。",
                "方法要点：结合Swin Transformer骨干与轻量级时间适配器TAPE，实现高效时空特征提取。",
                "实验或效果：在PHOENIX14等数据集上达到SOTA性能，仅用RGB视频超越多模态方法。"
            ],
            "tags_zh": [
                "连续手语识别",
                "时空建模",
                "Swin Transformer",
                "轻量级适配器",
                "RGB视频处理"
            ],
            "_index": 80
        },
        {
            "title": "Computer vision training dataset generation for robotic environments using Gaussian splatting",
            "authors": [
                "Patryk Niżeniec",
                "Marcin Iwanowski"
            ],
            "arxiv_id": "2512.13411v1",
            "summary": "This paper introduces a novel pipeline for generating large-scale, highly realistic, and automatically labeled datasets for computer vision tasks in robotic environments. Our approach addresses the critical challenges of the domain gap between synthetic and real-world imagery and the time-consuming bottleneck of manual annotation. We leverage 3D Gaussian Splatting (3DGS) to create photorealistic representations of the operational environment and objects. These assets are then used in a game engine where physics simulations create natural arrangements. A novel, two-pass rendering technique combines the realism of splats with a shadow map generated from proxy meshes. This map is then algorithmically composited with the image to add both physically plausible shadows and subtle highlights, significantly enhancing realism. Pixel-perfect segmentation masks are generated automatically and formatted for direct use with object detection models like YOLO. Our experiments show that a hybrid training strategy, combining a small set of real images with a large volume of our synthetic data, yields the best detection and segmentation performance, confirming this as an optimal strategy for efficiently achieving robust and accurate models.",
            "headline_zh": "提出基于高斯泼溅的机器人视觉数据集生成流水线，以解决合成与真实图像域差距和手动标注瓶颈。",
            "intro_zh": [
                "核心问题：合成与真实图像域差距及手动标注耗时，阻碍机器人视觉数据集生成。",
                "方法要点：利用3D高斯泼溅创建逼真环境，结合游戏引擎物理模拟和两遍渲染增强真实感。",
                "实验或效果：混合少量真实图像与大量合成数据训练，提升检测与分割性能，验证为高效策略。"
            ],
            "tags_zh": [
                "高斯泼溅",
                "数据集生成",
                "机器人视觉",
                "合成数据",
                "域适应",
                "自动标注"
            ],
            "_index": 81
        },
        {
            "title": "Multiclass Graph-Based Large Margin Classifiers: Unified Approach for Support Vectors and Neural Networks",
            "authors": [
                "Vítor M. Hanriot",
                "Luiz C. B. Torres",
                "Antônio P. Braga"
            ],
            "arxiv_id": "2512.13410v1",
            "summary": "While large margin classifiers are originally an outcome of an optimization framework, support vectors (SVs) can be obtained from geometric approaches. This article presents advances in the use of Gabriel graphs (GGs) in binary and multiclass classification problems. For Chipclass, a hyperparameter-less and optimization-less GG-based binary classifier, we discuss how activation functions and support edge (SE)-centered neurons affect the classification, proposing smoother functions and structural SV (SSV)-centered neurons to achieve margins with low probabilities and smoother classification contours. We extend the neural network architecture, which can be trained with backpropagation with a softmax function and a cross-entropy loss, or by solving a system of linear equations. A new subgraph-/distance-based membership function for graph regularization is also proposed, along with a new GG recomputation algorithm that is less computationally expensive than the standard approach. Experimental results with the Friedman test show that our method was better than previous GG-based classifiers and statistically equivalent to tree-based models.",
            "headline_zh": "提出基于Gabriel图的多元大间隔分类器，统一支持向量与神经网络方法",
            "intro_zh": [
                "核心问题：Gabriel图在多元分类中的应用扩展与优化，提升分类性能与计算效率",
                "方法要点：引入平滑激活函数、结构支持向量神经元、新子图距离成员函数和高效Gabriel图重计算算法",
                "实验或效果：实验显示方法优于先前Gabriel图分类器，统计等效于树模型"
            ],
            "tags_zh": [
                "Gabriel图分类",
                "大间隔分类器",
                "神经网络架构",
                "图正则化",
                "多元分类",
                "支持向量"
            ],
            "_index": 82
        },
        {
            "title": "End2Reg: Learning Task-Specific Segmentation for Markerless Registration in Spine Surgery",
            "authors": [
                "Lorenzo Pettinari",
                "Sidaty El Hadramy",
                "Michael Wehrli",
                "Philippe C. Cattin",
                "Daniel Studer",
                "Carol C. Hasler",
                "Maria Licci"
            ],
            "arxiv_id": "2512.13402v1",
            "summary": "Purpose: Intraoperative navigation in spine surgery demands millimeter-level accuracy. Current systems based on intraoperative radiographic imaging and bone-anchored markers are invasive, radiation-intensive and workflow disruptive. Recent markerless RGB-D registration methods offer a promising alternative, but existing approaches rely on weak segmentation labels to isolate relevant anatomical structures, which can propagate errors throughout registration. Methods: We present End2Reg an end-to-end deep learning framework that jointly optimizes segmentation and registration, eliminating the need for weak segmentation labels and manual steps. The network learns segmentation masks specifically optimized for registration, guided solely by the registration objective without direct segmentation supervision. Results: The proposed framework achieves state-of-the-art performance on ex- and in-vivo benchmarks, reducing median Target Registration Error by 32% to 1.83mm and mean Root Mean Square Error by 45% to 3.95mm, respectively. An ablation study confirms that end-to-end optimization significantly improves registration accuracy. Conclusion: The presented end-to-end RGB-D registration pipeline removes dependency on weak labels and manual steps, advancing towards fully automatic, markerless intraoperative navigation. Code and interactive visualizations are available at: https://lorenzopettinari.github.io/end-2-reg/.",
            "headline_zh": "提出End2Reg端到端深度学习框架，以解决脊柱手术中无标记注册依赖弱分割标签的问题。",
            "intro_zh": [
                "脊柱手术导航需毫米级精度，现有基于X光和骨锚标记的方法有创且干扰工作流。",
                "End2Reg联合优化分割与注册，无需弱分割标签或手动步骤，仅通过注册目标指导学习。",
                "在离体和在体基准测试中，中位目标注册误差降低32%至1.83mm，均方根误差降低45%至3.95mm。"
            ],
            "tags_zh": [
                "脊柱手术导航",
                "无标记注册",
                "端到端学习",
                "RGB-D注册",
                "分割优化",
                "深度学习框架"
            ],
            "_index": 83
        },
        {
            "title": "Differentiable Evolutionary Reinforcement Learning",
            "authors": [
                "Sitao Cheng",
                "Tianle Li",
                "Xuhan Huang",
                "Xunjian Yin",
                "Difan Zou"
            ],
            "arxiv_id": "2512.13399v1",
            "summary": "The design of effective reward functions presents a central and often arduous challenge in reinforcement learning (RL), particularly when developing autonomous agents for complex reasoning tasks. While automated reward optimization approaches exist, they typically rely on derivative-free evolutionary heuristics that treat the reward function as a black box, failing to capture the causal relationship between reward structure and task performance. To bridge this gap, we propose Differentiable Evolutionary Reinforcement Learning (DERL), a bilevel framework that enables the autonomous discovery of optimal reward signals. In DERL, a Meta-Optimizer evolves a reward function (i.e., Meta-Reward) by composing structured atomic primitives, guiding the training of an inner-loop policy. Crucially, unlike previous evolution, DERL is differentiable in its metaoptimization: it treats the inner-loop validation performance as a signal to update the Meta-Optimizer via reinforcement learning. This allows DERL to approximate the \"meta-gradient\" of task success, progressively learning to generate denser and more actionable feedback. We validate DERL across three distinct domains: robotic agent (ALFWorld), scientific simulation (ScienceWorld), and mathematical reasoning (GSM8k, MATH). Experimental results show that DERL achieves state-of-the-art performance on ALFWorld and ScienceWorld, significantly outperforming methods relying on heuristic rewards, especially in out-of-distribution scenarios. Analysis of the evolutionary trajectory demonstrates that DERL successfully captures the intrinsic structure of tasks, enabling selfimproving agent alignment without human intervention.",
            "headline_zh": "提出可微分进化强化学习框架，以自动发现最优奖励信号，解决复杂推理任务中奖励函数设计难题。",
            "intro_zh": [
                "核心问题：强化学习中奖励函数设计困难，现有进化方法忽略奖励结构与任务性能的因果关系。",
                "方法要点：采用双层框架，通过可微分元优化，利用内循环策略验证性能更新元优化器，近似任务成功的元梯度。",
                "实验或效果：在机器人、科学模拟和数学推理领域验证，在ALFWorld和ScienceWorld上达到最先进性能，尤其在分布外场景表现突出。"
            ],
            "tags_zh": [
                "可微分进化强化学习",
                "奖励函数优化",
                "双层框架",
                "元优化",
                "复杂推理任务",
                "分布外泛化"
            ],
            "_index": 84
        },
        {
            "title": "rNCA: Self-Repairing Segmentation Masks",
            "authors": [
                "Malte Silbernagel",
                "Albert Alonso",
                "Jens Petersen",
                "Bulat Ibragimov",
                "Marleen de Bruijne",
                "Madeleine K. Wyburd"
            ],
            "arxiv_id": "2512.13397v1",
            "summary": "Accurately predicting topologically correct masks remains a difficult task for general segmentation models, which often produce fragmented or disconnected outputs. Fixing these artifacts typically requires hand-crafted refinement rules or architectures specialized to a particular task. Here, we show that Neural Cellular Automata (NCA) can be directly re-purposed as an effective refinement mechanism, using local, iterative updates guided by image context to repair segmentation masks. By training on imperfect masks and ground truths, the automaton learns the structural properties of the target shape while relying solely on local information. When applied to coarse, globally predicted masks, the learned dynamics progressively reconnect broken regions, prune loose fragments and converge towards stable, topologically consistent results. We show how refinement NCA (rNCA) can be easily applied to repair common topological errors produced by different base segmentation models and tasks: for fragmented retinal vessels, it yields 2-3% gains in Dice/clDice and improves Betti errors, reducing $β_0$ errors by 60% and $β_1$ by 20%; for myocardium, it repairs 61.5% of broken cases in a zero-shot setting while lowering ASSD and HD by 19% and 16%, respectively. This showcases NCA as effective and broadly applicable refiners.",
            "headline_zh": "提出rNCA作为通用分割模型的后处理机制，通过局部迭代更新修复拓扑错误。",
            "intro_zh": [
                "核心问题：通用分割模型常产生碎片化或不连通的掩码，需手动或专用方法修复。",
                "方法要点：利用神经细胞自动机（NCA）作为细化机制，基于图像上下文进行局部迭代更新。",
                "实验或效果：在视网膜血管和心肌分割任务中，提升Dice/clDice指标，显著减少拓扑错误。"
            ],
            "tags_zh": [
                "分割掩码修复",
                "神经细胞自动机",
                "拓扑一致性",
                "后处理技术",
                "局部迭代更新"
            ],
            "_index": 85
        },
        {
            "title": "Beyond the Visible: Disocclusion-Aware Editing via Proxy Dynamic Graphs",
            "authors": [
                "Anran Qi",
                "Changjian Li",
                "Adrien Bousseau",
                "Niloy J. Mitra"
            ],
            "arxiv_id": "2512.13392v1",
            "summary": "We address image-to-video generation with explicit user control over the final frame's disoccluded regions. Current image-to-video pipelines produce plausible motion but struggle to generate predictable, articulated motions while enforcing user-specified content in newly revealed areas. Our key idea is to separate motion specification from appearance synthesis: we introduce a lightweight, user-editable Proxy Dynamic Graph (PDG) that deterministically yet approximately drives part motion, while a frozen diffusion prior is used to synthesize plausible appearance that follows that motion. In our training-free pipeline, the user loosely annotates and reposes a PDG, from which we compute a dense motion flow to leverage diffusion as a motion-guided shader. We then let the user edit appearance in the disoccluded areas of the image, and exploit the visibility information encoded by the PDG to perform a latent-space composite that reconciles motion with user intent in these areas. This design yields controllable articulation and user control over disocclusions without fine-tuning. We demonstrate clear advantages against state-of-the-art alternatives towards images turned into short videos of articulated objects, furniture, vehicles, and deformables. Our method mixes generative control, in the form of loose pose and structure, with predictable controls, in the form of appearance specification in the final frame in the disoccluded regions, unlocking a new image-to-video workflow. Code will be released on acceptance. Project page: https://anranqi.github.io/beyondvisible.github.io/",
            "headline_zh": "提出代理动态图方法以解决图像转视频中用户控制新显露区域内容的问题",
            "intro_zh": [
                "核心问题：现有图像转视频方法难以生成可预测的关节运动并控制新显露区域内容",
                "方法要点：使用轻量级代理动态图分离运动指定与外观合成，无需训练",
                "实验或效果：在关节物体、家具等场景中优于现有方法，实现可控运动与外观编辑"
            ],
            "tags_zh": [
                "图像转视频生成",
                "代理动态图",
                "运动控制",
                "外观合成",
                "用户编辑",
                "去遮挡区域"
            ],
            "_index": 86
        },
        {
            "title": "Dual-Phase Federated Deep Unlearning via Weight-Aware Rollback and Reconstruction",
            "authors": [
                "Changjun Zhou",
                "Jintao Zheng",
                "Leyou Yang",
                "Pengfei Wang"
            ],
            "arxiv_id": "2512.13381v1",
            "summary": "Federated Unlearning (FUL) focuses on client data and computing power to offer a privacy-preserving solution. However, high computational demands, complex incentive mechanisms, and disparities in client-side computing power often lead to long times and higher costs. To address these challenges, many existing methods rely on server-side knowledge distillation that solely removes the updates of the target client, overlooking the privacy embedded in the contributions of other clients, which can lead to privacy leakage. In this work, we introduce DPUL, a novel server-side unlearning method that deeply unlearns all influential weights to prevent privacy pitfalls. Our approach comprises three components: (i) identifying high-weight parameters by filtering client update magnitudes, and rolling them back to ensure deep removal. (ii) leveraging the variational autoencoder (VAE) to reconstruct and eliminate low-weight parameters. (iii) utilizing a projection-based technique to recover the model. Experimental results on four datasets demonstrate that DPUL surpasses state-of-the-art baselines, providing a 1%-5% improvement in accuracy and up to 12x reduction in time cost.",
            "headline_zh": "提出双阶段联邦深度遗忘方法，通过权重感知回滚与重建解决隐私泄露问题",
            "intro_zh": [
                "核心问题：现有联邦遗忘方法依赖服务器端知识蒸馏，仅移除目标客户端更新，忽略其他客户端贡献中的隐私，可能导致隐私泄露。",
                "方法要点：基于权重感知，回滚高权重参数，利用变分自编码器重建并消除低权重参数，结合投影技术恢复模型。",
                "实验或效果：在四个数据集上验证，相比基线方法，准确率提升1%-5%，时间成本降低高达12倍。"
            ],
            "tags_zh": [
                "联邦学习",
                "隐私保护",
                "深度遗忘",
                "权重回滚",
                "变分自编码器",
                "模型重建"
            ],
            "_index": 87
        },
        {
            "title": "Universal Dexterous Functional Grasping via Demonstration-Editing Reinforcement Learning",
            "authors": [
                "Chuan Mao",
                "Haoqi Yuan",
                "Ziye Huang",
                "Chaoyi Xu",
                "Kai Ma",
                "Zongqing Lu"
            ],
            "arxiv_id": "2512.13380v1",
            "summary": "Reinforcement learning (RL) has achieved great success in dexterous grasping, significantly improving grasp performance and generalization from simulation to the real world. However, fine-grained functional grasping, which is essential for downstream manipulation tasks, remains underexplored and faces several challenges: the complexity of specifying goals and reward functions for functional grasps across diverse objects, the difficulty of multi-task RL exploration, and the challenge of sim-to-real transfer. In this work, we propose DemoFunGrasp for universal dexterous functional grasping. We factorize functional grasping conditions into two complementary components - grasping style and affordance - and integrate them into an RL framework that can learn to grasp any object with any functional grasping condition. To address the multi-task optimization challenge, we leverage a single grasping demonstration and reformulate the RL problem as one-step demonstration editing, substantially enhancing sample efficiency and performance. Experimental results in both simulation and the real world show that DemoFunGrasp generalizes to unseen combinations of objects, affordances, and grasping styles, outperforming baselines in both success rate and functional grasping accuracy. In addition to strong sim-to-real capability, by incorporating a vision-language model (VLM) for planning, our system achieves autonomous instruction-following grasp execution.",
            "headline_zh": "提出DemoFunGrasp，通过演示编辑强化学习实现通用灵巧功能抓取",
            "intro_zh": [
                "核心问题：灵巧抓取中细粒度功能抓取的目标与奖励函数设计复杂，多任务探索困难，仿真到现实迁移挑战大。",
                "方法要点：将功能抓取条件分解为抓取风格和可供性，集成到强化学习框架，利用单次演示进行一步编辑优化。",
                "实验或效果：在仿真和现实中泛化至未见对象、可供性和抓取风格组合，成功率和功能抓取准确率优于基线，具备自主指令跟随能力。"
            ],
            "tags_zh": [
                "灵巧抓取",
                "强化学习",
                "演示编辑",
                "功能抓取",
                "仿真到现实迁移",
                "视觉语言模型"
            ],
            "_index": 88
        },
        {
            "title": "Unlocking Generalization in Polyp Segmentation with DINO Self-Attention \"keys\"",
            "authors": [
                "Carla Monteiro",
                "Valentina Corbetta",
                "Regina Beets-Tan",
                "Luís F. Teixeira",
                "Wilson Silva"
            ],
            "arxiv_id": "2512.13376v1",
            "summary": "Automatic polyp segmentation is crucial for improving the clinical identification of colorectal cancer (CRC). While Deep Learning (DL) techniques have been extensively researched for this problem, current methods frequently struggle with generalization, particularly in data-constrained or challenging settings. Moreover, many existing polyp segmentation methods rely on complex, task-specific architectures. To address these limitations, we present a framework that leverages the intrinsic robustness of DINO self-attention \"key\" features for robust segmentation. Unlike traditional methods that extract tokens from the deepest layers of the Vision Transformer (ViT), our approach leverages the key features of the self-attention module with a simple convolutional decoder to predict polyp masks, resulting in enhanced performance and better generalizability. We validate our approach using a multi-center dataset under two rigorous protocols: Domain Generalization (DG) and Extreme Single Domain Generalization (ESDG). Our results, supported by a comprehensive statistical analysis, demonstrate that this pipeline achieves state-of-the-art (SOTA) performance, significantly enhancing generalization, particularly in data-scarce and challenging scenarios. While avoiding a polyp-specific architecture, we surpass well-established models like nnU-Net and UM-Net. Additionally, we provide a systematic benchmark of the DINO framework's evolution, quantifying the specific impact of architectural advancements on downstream polyp segmentation performance.",
            "headline_zh": "提出利用DINO自注意力键特征框架以增强息肉分割的泛化能力",
            "intro_zh": [
                "核心问题：现有息肉分割方法泛化性差，尤其在数据受限或挑战性场景中。",
                "方法要点：利用DINO自注意力键特征，结合简单卷积解码器，避免复杂任务特定架构。",
                "实验或效果：在多中心数据集上验证，在域泛化和极端单域泛化协议下达到SOTA性能。"
            ],
            "tags_zh": [
                "息肉分割",
                "自注意力机制",
                "域泛化",
                "DINO框架",
                "卷积解码器"
            ],
            "_index": 89
        },
        {
            "title": "Behavior and Representation in Large Language Models for Combinatorial Optimization: From Feature Extraction to Algorithm Selection",
            "authors": [
                "Francesca Da Ros",
                "Luca Di Gaspero",
                "Kevin Roitero"
            ],
            "arxiv_id": "2512.13374v1",
            "summary": "Recent advances in Large Language Models (LLMs) have opened new perspectives for automation in optimization. While several studies have explored how LLMs can generate or solve optimization models, far less is understood about what these models actually learn regarding problem structure or algorithmic behavior. This study investigates how LLMs internally represent combinatorial optimization problems and whether such representations can support downstream decision tasks. We adopt a twofold methodology combining direct querying, which assesses LLM capacity to explicitly extract instance features, with probing analyses that examine whether such information is implicitly encoded within their hidden layers. The probing framework is further extended to a per-instance algorithm selection task, evaluating whether LLM-derived representations can predict the best-performing solver. Experiments span four benchmark problems and three instance representations. Results show that LLMs exhibit moderate ability to recover feature information from problem instances, either through direct querying or probing. Notably, the predictive power of LLM hidden-layer representations proves comparable to that achieved through traditional feature extraction, suggesting that LLMs capture meaningful structural information relevant to optimization performance.",
            "headline_zh": "探究大语言模型在组合优化中的内部表示与算法选择能力",
            "intro_zh": [
                "核心问题：大语言模型如何内部表示组合优化问题结构及其对算法性能的预测能力",
                "方法要点：结合直接查询与探测分析，评估特征提取和隐藏层表示",
                "实验或效果：在四个基准问题上，LLM表示与传统特征提取预测能力相当"
            ],
            "tags_zh": [
                "大语言模型",
                "组合优化",
                "特征提取",
                "算法选择",
                "隐藏层表示",
                "探测分析"
            ],
            "_index": 90
        },
        {
            "title": "Detecting Emotion Drift in Mental Health Text Using Pre-Trained Transformers",
            "authors": [
                "Shibani Sankpal"
            ],
            "arxiv_id": "2512.13363v1",
            "summary": "This study investigates emotion drift: the change in emotional state across a single text, within mental health-related messages. While sentiment analysis typically classifies an entire message as positive, negative, or neutral, the nuanced shift of emotions over the course of a message is often overlooked. This study detects sentence-level emotions and measures emotion drift scores using pre-trained transformer models such as DistilBERT and RoBERTa. The results provide insights into patterns of emotional escalation or relief in mental health conversations. This methodology can be applied to better understand emotional dynamics in content.",
            "headline_zh": "提出基于预训练Transformer的句子级情感漂移检测方法，以分析心理健康文本中的情绪动态变化。",
            "intro_zh": [
                "核心问题：心理健康文本中情感漂移（情绪状态在单文本内的变化）常被忽略，传统情感分析无法捕捉细微情绪转变。",
                "方法要点：使用DistilBERT和RoBERTa等预训练Transformer模型检测句子级情感，并计算情感漂移分数。",
                "实验或效果：结果揭示了心理健康对话中情绪升级或缓解的模式，有助于理解内容中的情绪动态。"
            ],
            "tags_zh": [
                "情感漂移检测",
                "预训练Transformer",
                "心理健康文本分析",
                "句子级情感分析",
                "情绪动态建模"
            ],
            "_index": 91
        },
        {
            "title": "Automated User Identification from Facial Thermograms with Siamese Networks",
            "authors": [
                "Elizaveta Prozorova",
                "Anton Konev",
                "Vladimir Faerman"
            ],
            "arxiv_id": "2512.13361v1",
            "summary": "The article analyzes the use of thermal imaging technologies for biometric identification based on facial thermograms. It presents a comparative analysis of infrared spectral ranges (NIR, SWIR, MWIR, and LWIR). The paper also defines key requirements for thermal cameras used in biometric systems, including sensor resolution, thermal sensitivity, and a frame rate of at least 30 Hz. Siamese neural networks are proposed as an effective approach for automating the identification process. In experiments conducted on a proprietary dataset, the proposed method achieved an accuracy of approximately 80%. The study also examines the potential of hybrid systems that combine visible and infrared spectra to overcome the limitations of individual modalities. The results indicate that thermal imaging is a promising technology for developing reliable security systems.",
            "headline_zh": "提出基于孪生网络的自动化面部热图用户识别方法，用于生物特征识别系统。",
            "intro_zh": [
                "核心问题：利用热成像技术进行面部生物特征识别，分析不同红外光谱范围（NIR、SWIR、MWIR、LWIR）的适用性。",
                "方法要点：采用孪生神经网络自动化识别过程，定义热相机关键参数如分辨率、热灵敏度和帧率要求。",
                "实验或效果：在专有数据集上实验，方法达到约80%准确率，并探讨可见光与红外光谱混合系统的潜力。"
            ],
            "tags_zh": [
                "面部热图识别",
                "孪生神经网络",
                "红外光谱分析",
                "生物特征识别",
                "热成像技术",
                "混合模态系统"
            ],
            "_index": 92
        },
        {
            "title": "Fast Policy Learning for 6-DOF Position Control of Underwater Vehicles",
            "authors": [
                "Sümer Tunçay",
                "Alain Andres",
                "Ignacio Carlucho"
            ],
            "arxiv_id": "2512.13359v1",
            "summary": "Autonomous Underwater Vehicles (AUVs) require reliable six-degree-of-freedom (6-DOF) position control to operate effectively in complex and dynamic marine environments. Traditional controllers are effective under nominal conditions but exhibit degraded performance when faced with unmodeled dynamics or environmental disturbances. Reinforcement learning (RL) provides a powerful alternative but training is typically slow and sim-to-real transfer remains challenging. This work introduces a GPU-accelerated RL training pipeline built in JAX and MuJoCo-XLA (MJX). By jointly JIT-compiling large-scale parallel physics simulation and learning updates, we achieve training times of under two minutes.Through systematic evaluation of multiple RL algorithms, we show robust 6-DOF trajectory tracking and effective disturbance rejection in real underwater experiments, with policies transferred zero-shot from simulation. Our results provide the first explicit real-world demonstration of RL-based AUV position control across all six degrees of freedom.",
            "headline_zh": "提出GPU加速强化学习训练管道，实现水下机器人六自由度位置控制",
            "intro_zh": [
                "核心问题：传统控制器在未建模动态或环境扰动下性能下降，强化学习训练慢且仿真到现实迁移难。",
                "方法要点：基于JAX和MuJoCo-XLA构建GPU加速训练管道，通过JIT编译大规模并行物理仿真和学习更新。",
                "实验或效果：在真实水下实验中实现稳健六自由度轨迹跟踪和扰动抑制，策略零样本从仿真迁移。"
            ],
            "tags_zh": [
                "水下机器人控制",
                "强化学习",
                "GPU加速训练",
                "仿真到现实迁移",
                "六自由度位置控制"
            ],
            "_index": 93
        },
        {
            "title": "Control of a Twin Rotor using Twin Delayed Deep Deterministic Policy Gradient (TD3)",
            "authors": [
                "Zeyad Gamal",
                "Youssef Mahran",
                "Ayman El-Badawy"
            ],
            "arxiv_id": "2512.13356v1",
            "summary": "This paper proposes a reinforcement learning (RL) framework for controlling and stabilizing the Twin Rotor Aerodynamic System (TRAS) at specific pitch and azimuth angles and tracking a given trajectory. The complex dynamics and non-linear characteristics of the TRAS make it challenging to control using traditional control algorithms. However, recent developments in RL have attracted interest due to their potential applications in the control of multirotors. The Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm was used in this paper to train the RL agent. This algorithm is used for environments with continuous state and action spaces, similar to the TRAS, as it does not require a model of the system. The simulation results illustrated the effectiveness of the RL control method. Next, external disturbances in the form of wind disturbances were used to test the controller's effectiveness compared to conventional PID controllers. Lastly, experiments on a laboratory setup were carried out to confirm the controller's effectiveness in real-world applications.",
            "headline_zh": "提出基于TD3的强化学习框架以控制双旋翼系统，实现稳定与轨迹跟踪。",
            "intro_zh": [
                "核心问题：双旋翼系统动态复杂非线性，传统控制算法难以有效控制。",
                "方法要点：采用TD3算法训练强化学习代理，无需系统模型，适用于连续状态动作空间。",
                "实验或效果：仿真验证有效性，对比PID控制器测试抗风扰，实验室实验确认实际应用。"
            ],
            "tags_zh": [
                "强化学习控制",
                "双旋翼系统",
                "TD3算法",
                "轨迹跟踪",
                "抗干扰控制"
            ],
            "_index": 94
        },
        {
            "title": "On the Effectiveness of Membership Inference in Targeted Data Extraction from Large Language Models",
            "authors": [
                "Ali Al Sahili",
                "Ali Chehab",
                "Razane Tajeddine"
            ],
            "arxiv_id": "2512.13352v1",
            "summary": "Large Language Models (LLMs) are prone to mem- orizing training data, which poses serious privacy risks. Two of the most prominent concerns are training data extraction and Membership Inference Attacks (MIAs). Prior research has shown that these threats are interconnected: adversaries can extract training data from an LLM by querying the model to generate a large volume of text and subsequently applying MIAs to verify whether a particular data point was included in the training set. In this study, we integrate multiple MIA techniques into the data extraction pipeline to systematically benchmark their effectiveness. We then compare their performance in this integrated setting against results from conventional MIA bench- marks, allowing us to evaluate their practical utility in real-world extraction scenarios.",
            "headline_zh": "集成多种成员推理攻击技术，评估其在大型语言模型训练数据提取中的有效性",
            "intro_zh": [
                "核心问题：大型语言模型记忆训练数据，引发隐私风险，包括训练数据提取和成员推理攻击。",
                "方法要点：将多种成员推理攻击技术整合到数据提取流程中，系统评估其性能。",
                "实验或效果：比较集成设置与传统基准下的攻击效果，评估实际提取场景中的实用性。"
            ],
            "tags_zh": [
                "大型语言模型",
                "训练数据提取",
                "成员推理攻击",
                "隐私风险",
                "基准评估"
            ],
            "_index": 95
        },
        {
            "title": "Link-Aware Energy-Frugal Continual Learning for Fault Detection in IoT Networks",
            "authors": [
                "Henrik C. M. Frederiksen",
                "Junya Shiraishi",
                "Cedomir Stefanovic",
                "Hei Victor Cheng",
                "Shashi Raj Pandey"
            ],
            "arxiv_id": "2512.13340v1",
            "summary": "The use of lightweight machine learning (ML) models in internet of things (IoT) networks enables resource constrained IoT devices to perform on-device inference for several critical applications. However, the inference accuracy deteriorates due to the non-stationarity in the IoT environment and limited initial training data. To counteract this, the deployed models can be updated occasionally with new observed data samples. However, this approach consumes additional energy, which is undesirable for energy constrained IoT devices. This letter introduces an event-driven communication framework that strategically integrates continual learning (CL) in IoT networks for energy-efficient fault detection. Our framework enables the IoT device and the edge server (ES) to collaboratively update the lightweight ML model by adapting to the wireless link conditions for communication and the available energy budget. Evaluation on real-world datasets show that the proposed approach can outperform both periodic sampling and non-adaptive CL in terms of inference recall; our proposed approach achieves up to a 42.8% improvement, even under tight energy and bandwidth constraint.",
            "headline_zh": "提出事件驱动通信框架，集成持续学习以在物联网网络中实现节能故障检测。",
            "intro_zh": [
                "核心问题：物联网设备资源受限，环境非平稳导致模型推理精度下降，更新模型能耗高。",
                "方法要点：基于无线链路条件和能量预算，设备与边缘服务器协作更新轻量级机器学习模型。",
                "实验或效果：在真实数据集上，相比周期性采样和非自适应持续学习，推理召回率提升高达42.8%。"
            ],
            "tags_zh": [
                "物联网网络",
                "故障检测",
                "持续学习",
                "节能通信",
                "轻量级机器学习",
                "边缘计算"
            ],
            "_index": 96
        },
        {
            "title": "FROC: A Unified Framework with Risk-Optimized Control for Machine Unlearning in LLMs",
            "authors": [
                "Si Qi Goh",
                "Yongsen Zheng",
                "Ziyao Liu",
                "Sami Hormi",
                "Kwok-Yan Lam"
            ],
            "arxiv_id": "2512.13337v1",
            "summary": "Machine unlearning (MU) seeks to eliminate the influence of specific training examples from deployed models. As large language models (LLMs) become widely used, managing risks arising from insufficient forgetting or utility loss is increasingly crucial. Current MU techniques lack effective mechanisms for evaluating and controlling these risks, hindering the selection of strategies that appropriately balance safety and utility, and raising trust concerns surrounding the \"right to be forgotten.\" To address these issues, we propose FROC, a unified framework with Risk-Optimized Control for machine unlearning in LLMs. FROC is built around a conformal-style risk-control formulation that expresses a user-specified risk budget on unlearning behavior. This probability-based constraint enables FROC to compare MU strategies, identify feasible operating regions, and guide hyperparameter selection according to desired trade-offs between forgetting sufficiency and utility preservation. To operationalize this constraint, FROC introduces a smoothly varying continuous risk model that aggregates forgetting deficiency and utility degradation into a single configuration-level score. Building on conformal risk analysis, FROC computes (1) the Conformal Unlearning Risk (CUR), a data-driven estimated value on the probability that forgotten samples continue to influence model predictions, and (2) risk-controlled configuration sets, which identify unlearning hyperparameters that are valid under the specified risk budget. Experiments across multiple LLM MU methods demonstrate that FROC produces stable, interpretable risk landscapes and reveals consistent relationships between unlearning configurations, semantic shift, and utility impact. FROC reframes MU as a controllable, risk-aware process and offers a practical foundation for managing unlearning behavior in large-scale LLM deployments.",
            "headline_zh": "提出FROC框架，通过风险优化控制解决大语言模型机器遗忘中的风险平衡问题。",
            "intro_zh": [
                "核心问题：现有机器遗忘技术缺乏有效风险评估与控制机制，难以平衡遗忘充分性与效用保留。",
                "方法要点：基于保形风险分析，引入连续风险模型和保形遗忘风险，以概率约束指导超参数选择。",
                "实验或效果：多方法实验显示FROC能生成稳定风险景观，揭示配置与语义偏移、效用影响的关系。"
            ],
            "tags_zh": [
                "机器遗忘",
                "大语言模型",
                "风险控制",
                "保形风险分析",
                "超参数优化"
            ],
            "_index": 97
        },
        {
            "title": "KD-PINN: Knowledge-Distilled PINNs for ultra-low-latency real-time neural PDE solvers",
            "authors": [
                "Karim Bounja",
                "Lahcen Laayouni",
                "Abdeljalil Sakat"
            ],
            "arxiv_id": "2512.13336v1",
            "summary": "This work introduces Knowledge-Distilled Physics-Informed Neural Networks (KD-PINN), a framework that transfers the predictive accuracy of a high-capacity teacher model to a compact student through a continuous adaptation of the Kullback-Leibler divergence. To confirm its generality for various dynamics and dimensionalities, the framework is evaluated on a representative set of partial differential equations (PDEs). In all tested cases, the student model preserved the teacher's physical accuracy, with a mean RMSE increase below 0.64%, and achieved inference speedups ranging from 4.8x (Navier-Stokes) to 6.9x (Burgers). The distillation process also revealed a regularizing effect. With an average inference latency of 5.3 ms on CPU, the distilled models enter the ultra-low-latency real-time regime defined by sub-10 ms performance. Finally, this study examines how knowledge distillation reduces inference latency in PINNs to contribute to the development of accurate ultra-low-latency neural PDE solvers.",
            "headline_zh": "提出知识蒸馏物理信息神经网络框架，用于实现超低延迟实时神经偏微分方程求解器。",
            "intro_zh": [
                "核心问题：物理信息神经网络推理延迟高，难以满足实时求解需求。",
                "方法要点：通过连续适应KL散度，将高容量教师模型预测精度迁移至紧凑学生模型。",
                "实验或效果：在多种偏微分方程上验证，学生模型保持物理精度，推理速度提升4.8-6.9倍，平均延迟5.3毫秒。"
            ],
            "tags_zh": [
                "知识蒸馏",
                "物理信息神经网络",
                "偏微分方程求解",
                "超低延迟",
                "实时计算",
                "模型压缩"
            ],
            "_index": 98
        },
        {
            "title": "FIN-bench-v2: A Unified and Robust Benchmark Suite for Evaluating Finnish Large Language Models",
            "authors": [
                "Joona Kytöniemi",
                "Jousia Piha",
                "Akseli Reunamo",
                "Fedor Vitiugin",
                "Farrokh Mehryary",
                "Sampo Pyysalo"
            ],
            "arxiv_id": "2512.13330v1",
            "summary": "We introduce FIN-bench-v2, a unified benchmark suite for evaluating large language models in Finnish. FIN-bench-v2 consolidates Finnish versions of widely used benchmarks together with an updated and expanded version of the original FIN-bench into a single, consistently formatted collection, covering multiple-choice and generative tasks across reading comprehension, commonsense reasoning, sentiment analysis, world knowledge, and alignment. All datasets are converted to HuggingFace Datasets, which include both cloze and multiple-choice prompt formulations with five variants per task, and we incorporate human annotation or review for machine-translated resources such as GoldenSwag and XED. To select robust tasks, we pretrain a set of 2.15B-parameter decoder-only models and use their learning curves to compute monotonicity, signal-to-noise, non-random performance, and model ordering consistency, retaining only tasks that satisfy all criteria. We further evaluate a set of larger instruction-tuned models to characterize performance across tasks and prompt formulations. All datasets, prompts, and evaluation configurations are publicly available via our fork of the Language Model Evaluation Harness at https://github.com/LumiOpen/lm-evaluation-harness. Supplementary resources are released in a separate repository at https://github.com/TurkuNLP/FIN-bench-v2.",
            "headline_zh": "提出FIN-bench-v2统一基准套件以评估芬兰语大语言模型",
            "intro_zh": [
                "核心问题：缺乏统一且鲁棒的芬兰语大语言模型评估基准",
                "方法要点：整合并扩展现有基准，通过模型学习曲线筛选鲁棒任务",
                "实验或效果：评估指令调优模型，公开数据集和评估配置"
            ],
            "tags_zh": [
                "芬兰语大语言模型",
                "基准评估",
                "任务鲁棒性",
                "多任务覆盖",
                "公开数据集"
            ],
            "_index": 99
        },
        {
            "title": "Security and Detectability Analysis of Unicode Text Watermarking Methods Against Large Language Models",
            "authors": [
                "Malte Hellmeier"
            ],
            "arxiv_id": "2512.13325v1",
            "summary": "Securing digital text is becoming increasingly relevant due to the widespread use of large language models. Individuals' fear of losing control over data when it is being used to train such machine learning models or when distinguishing model-generated output from text written by humans. Digital watermarking provides additional protection by embedding an invisible watermark within the data that requires protection. However, little work has been taken to analyze and verify if existing digital text watermarking methods are secure and undetectable by large language models. In this paper, we investigate the security-related area of watermarking and machine learning models for text data. In a controlled testbed of three experiments, ten existing Unicode text watermarking methods were implemented and analyzed across six large language models: GPT-5, GPT-4o, Teuken 7B, Llama 3.3, Claude Sonnet 4, and Gemini 2.5 Pro. The findings of our experiments indicate that, especially the latest reasoning models, can detect a watermarked text. Nevertheless, all models fail to extract the watermark unless implementation details in the form of source code are provided. We discuss the implications for security researchers and practitioners and outline future research opportunities to address security concerns.",
            "headline_zh": "分析Unicode文本水印方法在大语言模型下的安全性与可检测性",
            "intro_zh": [
                "核心问题：现有Unicode文本水印方法在大语言模型下的安全性和不可检测性缺乏验证",
                "方法要点：在受控测试环境中实现并分析十种Unicode文本水印方法",
                "实验或效果：实验表明最新推理模型能检测水印文本，但所有模型无法提取水印"
            ],
            "tags_zh": [
                "文本水印",
                "大语言模型",
                "安全性分析",
                "Unicode编码",
                "模型检测"
            ],
            "_index": 100
        },
        {
            "title": "Error-Driven Prompt Optimization for Arithmetic Reasoning",
            "authors": [
                "Árpád Pándy",
                "Róbert Lakatos",
                "András Hajdu"
            ],
            "arxiv_id": "2512.13323v1",
            "summary": "Recent advancements in artificial intelligence have sparked interest in industrial agents capable of supporting analysts in regulated sectors, such as finance and healthcare, within tabular data workflows. A key capability for such systems is performing accurate arithmetic operations on structured data while ensuring sensitive information never leaves secure, on-premises environments. Here, we introduce an error-driven optimization framework for arithmetic reasoning that enhances a Code Generation Agent (CGA), specifically applied to on-premises small language models (SLMs). Through a systematic evaluation of a leading SLM (Qwen3 4B), we find that while the base model exhibits fundamental limitations in arithmetic tasks, our proposed error-driven method, which clusters erroneous predictions to refine prompt-rules iteratively, dramatically improves performance, elevating the model's accuracy to 70.8\\%. Our results suggest that developing reliable, interpretable, and industrially deployable AI assistants can be achieved not only through costly fine-tuning but also via systematic, error-driven prompt optimization, enabling small models to surpass larger language models (GPT-3.5 Turbo) in a privacy-compliant manner.",
            "headline_zh": "提出错误驱动提示优化框架，提升小语言模型在算术推理任务中的准确性，适用于隐私合规的工业部署。",
            "intro_zh": [
                "核心问题：小语言模型在算术推理任务中表现有限，需在隐私合规环境下提升准确性。",
                "方法要点：通过聚类错误预测，迭代优化提示规则，以错误驱动方式改进模型性能。",
                "实验或效果：在Qwen3 4B模型上，该方法将准确率提升至70.8%，超越GPT-3.5 Turbo。"
            ],
            "tags_zh": [
                "算术推理",
                "提示优化",
                "小语言模型",
                "错误驱动学习",
                "隐私合规",
                "工业部署"
            ],
            "_index": 101
        },
        {
            "title": "Face Identity Unlearning for Retrieval via Embedding Dispersion",
            "authors": [
                "Mikhail Zakharov"
            ],
            "arxiv_id": "2512.13317v1",
            "summary": "Face recognition systems rely on learning highly discriminative and compact identity clusters to enable accurate retrieval. However, as with other surveillance-oriented technologies, such systems raise serious privacy concerns due to their potential for unauthorized identity tracking. While several works have explored machine unlearning as a means of privacy protection, their applicability to face retrieval - especially for modern embedding-based recognition models - remains largely unexplored. In this work, we study the problem of face identity unlearning for retrieval systems and present its inherent challenges. The goal is to make selected identities unretrievable by dispersing their embeddings on the hypersphere and preventing the formation of compact identity clusters that enable re-identification in the gallery. The primary challenge is to achieve this forgetting effect while preserving the discriminative structure of the embedding space and the retrieval performance of the model for the remaining identities. To address this, we evaluate several existing approximate class unlearning methods (e.g., Random Labeling, Gradient Ascent, Boundary Unlearning, and other recent approaches) in the context of face retrieval and propose a simple yet effective dispersion-based unlearning approach. Extensive experiments on standard benchmarks (VGGFace2, CelebA) demonstrate that our method achieves superior forgetting behavior while preserving retrieval utility.",
            "headline_zh": "提出基于嵌入分散的人脸身份遗忘方法，以保护检索系统中的隐私",
            "intro_zh": [
                "研究人脸检索中的身份遗忘问题，旨在使选定身份不可检索",
                "通过分散嵌入在超球面上，防止紧凑身份簇形成，同时保持其他身份的检索性能",
                "在VGGFace2和CelebA基准上验证方法，实现有效遗忘并保留检索效用"
            ],
            "tags_zh": [
                "人脸检索",
                "身份遗忘",
                "嵌入分散",
                "隐私保护",
                "机器学习遗忘"
            ],
            "_index": 102
        },
        {
            "title": "ALIGN-FL: Architecture-independent Learning through Invariant Generative component sharing in Federated Learning",
            "authors": [
                "Mayank Gulati",
                "Benedikt Groß",
                "Gerhard Wunder"
            ],
            "arxiv_id": "2512.13316v1",
            "summary": "We present ALIGN-FL, a novel approach to distributed learning that addresses the challenge of learning from highly disjoint data distributions through selective sharing of generative components. Instead of exchanging full model parameters, our framework enables privacy-preserving learning by transferring only generative capabilities across clients, while the server performs global training using synthetic samples. Through complementary privacy mechanisms: DP-SGD with adaptive clipping and Lipschitz regularized VAE decoders and a stateful architecture supporting heterogeneous clients, we experimentally validate our approach on MNIST and Fashion-MNIST datasets with cross-domain outliers. Our analysis demonstrates that both privacy mechanisms effectively map sensitive outliers to typical data points while maintaining utility in extreme Non-IID scenarios typical of cross-silo collaborations.\n  Index Terms: Client-invariant Learning, Federated Learning (FL), Privacy-preserving Generative Models, Non-Independent and Identically Distributed (Non-IID), Heterogeneous Architectures",
            "headline_zh": "提出ALIGN-FL，通过选择性共享生成组件解决联邦学习中高度非独立同分布数据的学习挑战。",
            "intro_zh": [
                "核心问题：联邦学习中数据分布高度非独立同分布，导致模型训练困难。",
                "方法要点：仅共享生成能力，结合差分隐私和Lipschitz正则化，支持异构客户端架构。",
                "实验效果：在MNIST和Fashion-MNIST数据集上验证，有效处理异常值并保持模型效用。"
            ],
            "tags_zh": [
                "联邦学习",
                "非独立同分布数据",
                "隐私保护生成模型",
                "异构架构",
                "差分隐私",
                "生成组件共享"
            ],
            "_index": 103
        },
        {
            "title": "KlingAvatar 2.0 Technical Report",
            "authors": [
                "Kling Team",
                "Jialu Chen",
                "Yikang Ding",
                "Zhixue Fang",
                "Kun Gai",
                "Yuan Gao",
                "Kang He",
                "Jingyun Hua",
                "Boyuan Jiang",
                "Mingming Lao",
                "Xiaohan Li",
                "Hui Liu",
                "Jiwen Liu",
                "Xiaoqiang Liu",
                "Yuan Liu",
                "Shun Lu",
                "Yongsen Mao",
                "Yingchao Shao",
                "Huafeng Shi",
                "Xiaoyu Shi",
                "Peiqin Sun",
                "Songlin Tang",
                "Pengfei Wan",
                "Chao Wang",
                "Xuebo Wang",
                "Haoxian Zhang",
                "Yuanxing Zhang",
                "Yan Zhou"
            ],
            "arxiv_id": "2512.13313v1",
            "summary": "Avatar video generation models have achieved remarkable progress in recent years. However, prior work exhibits limited efficiency in generating long-duration high-resolution videos, suffering from temporal drifting, quality degradation, and weak prompt following as video length increases. To address these challenges, we propose KlingAvatar 2.0, a spatio-temporal cascade framework that performs upscaling in both spatial resolution and temporal dimension. The framework first generates low-resolution blueprint video keyframes that capture global semantics and motion, and then refines them into high-resolution, temporally coherent sub-clips using a first-last frame strategy, while retaining smooth temporal transitions in long-form videos. To enhance cross-modal instruction fusion and alignment in extended videos, we introduce a Co-Reasoning Director composed of three modality-specific large language model (LLM) experts. These experts reason about modality priorities and infer underlying user intent, converting inputs into detailed storylines through multi-turn dialogue. A Negative Director further refines negative prompts to improve instruction alignment. Building on these components, we extend the framework to support ID-specific multi-character control. Extensive experiments demonstrate that our model effectively addresses the challenges of efficient, multimodally aligned long-form high-resolution video generation, delivering enhanced visual clarity, realistic lip-teeth rendering with accurate lip synchronization, strong identity preservation, and coherent multimodal instruction following.",
            "headline_zh": "提出KlingAvatar 2.0时空级联框架以解决长时高分辨率虚拟形象视频生成中的效率与对齐问题",
            "intro_zh": [
                "核心问题：现有方法在生成长时高分辨率视频时存在效率低、时间漂移、质量下降和提示跟随弱的问题",
                "方法要点：采用时空级联框架，先生成低分辨率蓝图关键帧，再通过首尾帧策略细化成高分辨率子片段，并引入Co-Reasoning Director增强跨模态指令对齐",
                "实验或效果：模型在长时高分辨率视频生成中表现出增强的视觉清晰度、逼真的唇齿渲染、强身份保持和连贯的多模态指令跟随"
            ],
            "tags_zh": [
                "虚拟形象视频生成",
                "时空级联框架",
                "长时高分辨率视频",
                "跨模态指令对齐",
                "身份控制"
            ],
            "_index": 104
        },
        {
            "title": "Humanoid Robot Running Through Random Stepping Stones and Jumping Over Obstacles: Step Adaptation Using Spring-Mass Trajectories",
            "authors": [
                "Sait Sovukluk",
                "Johannes Englsberger",
                "Christian Ott"
            ],
            "arxiv_id": "2512.13304v1",
            "summary": "This study proposes a step adaptation framework for running through spring-mass trajectories and deadbeat control gain libraries. It includes four main parts: (1) Automatic spring-mass trajectory library generation; (2) Deadbeat control gain library generation through an actively controlled template model that resembles the whole-body dynamics well; (3) Trajectory selection policy development for step adaptation; (4) Mapping spring-mass trajectories to a humanoid model through a whole-body control (WBC) framework also accounting for closed-kinematic chain systems, self collisions, and reactive limb swinging. We show the inclusiveness and the robustness of the proposed framework through various challenging and agile behaviors such as running through randomly generated stepping stones, jumping over random obstacles, performing slalom motions, changing the running direction suddenly with a random leg, and rejecting significant disturbances and uncertainties through the MuJoCo physics simulator. We also perform additional simulations under a comprehensive set of uncertainties and noise to better justify the proposed method's robustness to real-world challenges, including signal noise, imprecision, modeling errors, and delays. All the aforementioned behaviors are performed with a single library and the same set of WBC control parameters without additional tuning. The spring-mass and the deadbeat control gain library are automatically computed in 4.5 seconds in total for 315 different trajectories.",
            "headline_zh": "提出基于弹簧质量轨迹的步态适应框架，实现人形机器人随机踏石与跳跃障碍的敏捷运动。",
            "intro_zh": [
                "核心问题：人形机器人在动态环境中（如随机踏石、跳跃障碍）的步态适应与鲁棒控制。",
                "方法要点：自动生成弹簧质量轨迹库和死拍控制增益库，结合轨迹选择策略和全身控制框架映射。",
                "实验或效果：在MuJoCo模拟器中验证了多种敏捷行为的鲁棒性，包括抗干扰和不确定性，无需额外调参。"
            ],
            "tags_zh": [
                "人形机器人控制",
                "步态适应",
                "弹簧质量模型",
                "全身控制",
                "鲁棒性验证",
                "动态环境模拟"
            ],
            "_index": 105
        },
        {
            "title": "ShowTable: Unlocking Creative Table Visualization with Collaborative Reflection and Refinement",
            "authors": [
                "Zhihang Liu",
                "Xiaoyi Bao",
                "Pandeng Li",
                "Junjie Zhou",
                "Zhaohe Liao",
                "Yefei He",
                "Kaixun Jiang",
                "Chen-Wei Xie",
                "Yun Zheng",
                "Hongtao Xie"
            ],
            "arxiv_id": "2512.13303v1",
            "summary": "While existing generation and unified models excel at general image generation, they struggle with tasks requiring deep reasoning, planning, and precise data-to-visual mapping abilities beyond general scenarios. To push beyond the existing limitations, we introduce a new and challenging task: creative table visualization, requiring the model to generate an infographic that faithfully and aesthetically visualizes the data from a given table. To address this challenge, we propose ShowTable, a pipeline that synergizes MLLMs with diffusion models via a progressive self-correcting process. The MLLM acts as the central orchestrator for reasoning the visual plan and judging visual errors to provide refined instructions, the diffusion execute the commands from MLLM, achieving high-fidelity results. To support this task and our pipeline, we introduce three automated data construction pipelines for training different modules. Furthermore, we introduce TableVisBench, a new benchmark with 800 challenging instances across 5 evaluation dimensions, to assess performance on this task. Experiments demonstrate that our pipeline, instantiated with different models, significantly outperforms baselines, highlighting its effective multi-modal reasoning, generation, and error correction capabilities.",
            "headline_zh": "提出ShowTable管道，通过协作反思与精炼解决创意表格可视化任务",
            "intro_zh": [
                "核心问题：现有模型在需要深度推理和精确数据映射的创意表格可视化任务上表现不足",
                "方法要点：结合MLLM作为中央协调器进行推理和错误校正，扩散模型执行指令，实现高保真生成",
                "实验或效果：在TableVisBench基准上显著优于基线，验证了多模态推理和错误校正能力"
            ],
            "tags_zh": [
                "创意表格可视化",
                "多模态大语言模型",
                "扩散模型",
                "协作反思",
                "数据构造管道",
                "基准评估"
            ],
            "_index": 106
        },
        {
            "title": "No One Left Behind: How to Exploit the Incomplete and Skewed Multi-Label Data for Conversion Rate Prediction",
            "authors": [
                "Qinglin Jia",
                "Zhaocheng Du",
                "Chuhan Wu",
                "Huifeng Guo",
                "Ruiming Tang",
                "Shuting Shi",
                "Muyu Zhang"
            ],
            "arxiv_id": "2512.13300v1",
            "summary": "In most real-world online advertising systems, advertisers typically have diverse customer acquisition goals. A common solution is to use multi-task learning (MTL) to train a unified model on post-click data to estimate the conversion rate (CVR) for these diverse targets. In practice, CVR prediction often encounters missing conversion data as many advertisers submit only a subset of user conversion actions due to privacy or other constraints, making the labels of multi-task data incomplete. If the model is trained on all available samples where advertisers submit user conversion actions, it may struggle when deployed to serve a subset of advertisers targeting specific conversion actions, as the training and deployment data distributions are mismatched. While considerable MTL efforts have been made, a long-standing challenge is how to effectively train a unified model with the incomplete and skewed multi-label data. In this paper, we propose a fine-grained Knowledge transfer framework for Asymmetric Multi-Label data (KAML). We introduce an attribution-driven masking strategy (ADM) to better utilize data with asymmetric multi-label data in training. However, the more relaxed masking in ADM is a double-edged sword: it provides additional training signals but also introduces noise due to skewed data. To address this, we propose a hierarchical knowledge extraction mechanism (HKE) to model the sample discrepancy within the target task tower. Finally, to maximize the utility of unlabeled samples, we incorporate ranking loss strategy to further enhance our model. The effectiveness of KAML has been demonstrated through comprehensive evaluations on offline industry datasets and online A/B tests, which show significant performance improvements over existing MTL baselines.",
            "headline_zh": "提出KAML框架以解决在线广告中多任务学习数据不完整和偏斜问题",
            "intro_zh": [
                "核心问题：广告系统多任务数据标签不完整且分布偏斜，导致训练与部署数据不匹配",
                "方法要点：引入ADM策略和HKE机制，结合排序损失，优化知识迁移",
                "实验或效果：在离线行业数据集和在线A/B测试中显著优于现有基线"
            ],
            "tags_zh": [
                "多任务学习",
                "转化率预测",
                "知识迁移",
                "数据偏斜",
                "在线广告"
            ],
            "_index": 107
        },
        {
            "title": "MiniLingua: A Small Open-Source LLM for European Languages",
            "authors": [
                "Anna Aksenova",
                "Boris Zverkov",
                "Nicola Dainese",
                "Alexander Nikitin",
                "Pekka Marttinen"
            ],
            "arxiv_id": "2512.13298v1",
            "summary": "Large language models are powerful but often limited by high computational cost, privacy concerns, and English-centric training. Recent progress demonstrates that small, efficient models with around one billion parameters can deliver strong results and enable on-device use. This paper introduces MiniLingua, a multilingual open-source LLM of one billion parameters trained from scratch for 13 European languages, designed to balance coverage and instruction-following capabilities. Based on evaluation results, the instruction-tuned version of MiniLingua outperforms EuroLLM, a model with a similar training approach but a larger training budget, on summarization, classification and both open- and closed-book question answering. Moreover, it remains competitive with more advanced state-of-the-art models on open-ended generation tasks. We release model weights, tokenizer and source code used for data processing and model training.",
            "headline_zh": "提出MiniLingua小型开源多语言大模型，以解决欧洲语言覆盖与指令跟随的平衡问题。",
            "intro_zh": [
                "核心问题：大模型计算成本高、隐私担忧及英语中心化训练限制应用。",
                "方法要点：从头训练10亿参数模型，覆盖13种欧洲语言，注重指令跟随能力。",
                "实验或效果：在多项任务中优于EuroLLM，与先进模型在开放生成任务中竞争。"
            ],
            "tags_zh": [
                "多语言大模型",
                "小型模型",
                "指令跟随",
                "欧洲语言",
                "开源模型"
            ],
            "_index": 108
        },
        {
            "title": "MedInsightBench: Evaluating Medical Analytics Agents Through Multi-Step Insight Discovery in Multimodal Medical Data",
            "authors": [
                "Zhenghao Zhu",
                "Chuxue Cao",
                "Sirui Han",
                "Yuanfeng Song",
                "Xing Chen",
                "Caleb Chen Cao",
                "Yike Guo"
            ],
            "arxiv_id": "2512.13297v1",
            "summary": "In medical data analysis, extracting deep insights from complex, multi-modal datasets is essential for improving patient care, increasing diagnostic accuracy, and optimizing healthcare operations. However, there is currently a lack of high-quality datasets specifically designed to evaluate the ability of large multi-modal models (LMMs) to discover medical insights. In this paper, we introduce MedInsightBench, the first benchmark that comprises 332 carefully curated medical cases, each annotated with thoughtfully designed insights. This benchmark is intended to evaluate the ability of LMMs and agent frameworks to analyze multi-modal medical image data, including posing relevant questions, interpreting complex findings, and synthesizing actionable insights and recommendations. Our analysis indicates that existing LMMs exhibit limited performance on MedInsightBench, which is primarily attributed to their challenges in extracting multi-step, deep insights and the absence of medical expertise. Therefore, we propose MedInsightAgent, an automated agent framework for medical data analysis, composed of three modules: Visual Root Finder, Analytical Insight Agent, and Follow-up Question Composer. Experiments on MedInsightBench highlight pervasive challenges and demonstrate that MedInsightAgent can improve the performance of general LMMs in medical data insight discovery.",
            "headline_zh": "提出MedInsightBench基准与MedInsightAgent框架，以评估和改进多模态模型在医疗数据分析中的洞察发现能力。",
            "intro_zh": [
                "核心问题：缺乏高质量数据集评估多模态模型在复杂医疗数据中的深度洞察能力。",
                "方法要点：构建包含332个医疗案例的基准，并设计三模块自动化代理框架进行多步分析。",
                "实验或效果：现有模型表现有限，MedInsightAgent能提升通用模型在医疗洞察发现中的性能。"
            ],
            "tags_zh": [
                "医疗数据分析",
                "多模态基准",
                "自动化代理框架",
                "深度洞察发现",
                "医疗图像理解"
            ],
            "_index": 109
        },
        {
            "title": "Intrinsic-Motivation Multi-Robot Social Formation Navigation with Coordinated Exploration",
            "authors": [
                "Hao Fua",
                "Wei Liu",
                "Shuai Zhoua"
            ],
            "arxiv_id": "2512.13293v1",
            "summary": "This paper investigates the application of reinforcement learning (RL) to multi-robot social formation navigation, a critical capability for enabling seamless human-robot coexistence. While RL offers a promising paradigm, the inherent unpredictability and often uncooperative dynamics of pedestrian behavior pose substantial challenges, particularly concerning the efficiency of coordinated exploration among robots. To address this, we propose a novel coordinated-exploration multi-robot RL algorithm introducing an intrinsic motivation exploration. Its core component is a self-learning intrinsic reward mechanism designed to collectively alleviate policy conservatism. Moreover, this algorithm incorporates a dual-sampling mode within the centralized training and decentralized execution framework to enhance the representation of both the navigation policy and the intrinsic reward, leveraging a two-time-scale update rule to decouple parameter updates. Empirical results on social formation navigation benchmarks demonstrate the proposed algorithm's superior performance over existing state-of-the-art methods across crucial metrics. Our code and video demos are available at: https://github.com/czxhunzi/CEMRRL.",
            "headline_zh": "提出基于内在动机协调探索的多机器人强化学习算法以解决社交编队导航中的探索效率问题",
            "intro_zh": [
                "核心问题：多机器人社交编队导航中，行人行为不可预测且不合作，导致协调探索效率低下",
                "方法要点：引入内在奖励机制缓解策略保守性，采用双采样模式和两时间尺度更新规则优化策略与奖励表示",
                "实验或效果：在社交编队导航基准测试中，算法在关键指标上优于现有先进方法"
            ],
            "tags_zh": [
                "多机器人强化学习",
                "社交编队导航",
                "内在动机探索",
                "协调探索",
                "集中训练分散执行"
            ],
            "_index": 110
        },
        {
            "title": "LINA: Learning INterventions Adaptively for Physical Alignment and Generalization in Diffusion Models",
            "authors": [
                "Shu Yu",
                "Chaochao Lu"
            ],
            "arxiv_id": "2512.13290v1",
            "summary": "Diffusion models (DMs) have achieved remarkable success in image and video generation. However, they still struggle with (1) physical alignment and (2) out-of-distribution (OOD) instruction following. We argue that these issues stem from the models' failure to learn causal directions and to disentangle causal factors for novel recombination. We introduce the Causal Scene Graph (CSG) and the Physical Alignment Probe (PAP) dataset to enable diagnostic interventions. This analysis yields three key insights. First, DMs struggle with multi-hop reasoning for elements not explicitly determined in the prompt. Second, the prompt embedding contains disentangled representations for texture and physics. Third, visual causal structure is disproportionately established during the initial, computationally limited denoising steps. Based on these findings, we introduce LINA (Learning INterventions Adaptively), a novel framework that learns to predict prompt-specific interventions, which employs (1) targeted guidance in the prompt and visual latent spaces, and (2) a reallocated, causality-aware denoising schedule. Our approach enforces both physical alignment and OOD instruction following in image and video DMs, achieving state-of-the-art performance on challenging causal generation tasks and the Winoground dataset. Our project page is at https://opencausalab.github.io/LINA.",
            "headline_zh": "提出LINA框架，通过自适应学习干预解决扩散模型物理对齐和分布外指令遵循问题。",
            "intro_zh": [
                "核心问题：扩散模型在物理对齐和分布外指令遵循上表现不佳，源于因果方向学习和因子解耦的失败。",
                "方法要点：引入因果场景图和物理对齐探测数据集进行诊断，基于洞察设计自适应干预预测框架，结合提示与视觉潜在空间引导及因果感知去噪调度。",
                "实验或效果：在图像和视频扩散模型中实现物理对齐和分布外指令遵循，在因果生成任务和Winoground数据集上达到最先进性能。"
            ],
            "tags_zh": [
                "扩散模型",
                "物理对齐",
                "因果推理",
                "自适应干预",
                "分布外指令遵循",
                "因果场景图"
            ],
            "_index": 111
        },
        {
            "title": "CausalCLIP: Causally-Informed Feature Disentanglement and Filtering for Generalizable Detection of Generated Images",
            "authors": [
                "Bo Liu",
                "Qiao Qin",
                "Qinghui He"
            ],
            "arxiv_id": "2512.13285v1",
            "summary": "The rapid advancement of generative models has increased the demand for generated image detectors capable of generalizing across diverse and evolving generation techniques. However, existing methods, including those leveraging pre-trained vision-language models, often produce highly entangled representations, mixing task-relevant forensic cues (causal features) with spurious or irrelevant patterns (non-causal features), thus limiting generalization. To address this issue, we propose CausalCLIP, a framework that explicitly disentangles causal from non-causal features and employs targeted filtering guided by causal inference principles to retain only the most transferable and discriminative forensic cues. By modeling the generation process with a structural causal model and enforcing statistical independence through Gumbel-Softmax-based feature masking and Hilbert-Schmidt Independence Criterion (HSIC) constraints, CausalCLIP isolates stable causal features robust to distribution shifts. When tested on unseen generative models from different series, CausalCLIP demonstrates strong generalization ability, achieving improvements of 6.83% in accuracy and 4.06% in average precision over state-of-the-art methods.",
            "headline_zh": "提出CausalCLIP框架，通过因果特征解耦与过滤提升生成图像检测的泛化能力",
            "intro_zh": [
                "现有方法特征高度纠缠，混合因果与非因果特征，限制泛化",
                "基于结构因果模型，利用Gumbel-Softmax掩码和HSIC约束解耦特征",
                "在未见生成模型上测试，准确率和平均精度显著优于先进方法"
            ],
            "tags_zh": [
                "生成图像检测",
                "特征解耦",
                "因果推理",
                "泛化能力",
                "CLIP模型"
            ],
            "_index": 112
        },
        {
            "title": "SAMAY: System for Acoustic Measurement and Analysis",
            "authors": [
                "Adheep Arya G R",
                "Vaibhav Pratap Singh",
                "Mayank Kumar",
                "Niyathi Shenoy",
                "Tejas Suryawanshi",
                "Ruchi Juyal",
                "Sangit Saha",
                "Kaushik Nanda",
                "Hari Babu Pasupuleti",
                "S D Sudarsan"
            ],
            "arxiv_id": "2512.13284v1",
            "summary": "This paper describes an automatic bird call recording system called SAMAY, which is developed to study bird species by creating a database of large amounts of bird acoustic data. By analysing the recorded bird call data, the system can also be used for automatic classification of bird species, monitoring bird populations and analysing the impact of environmental changes. The system is driven through a powerful STM32F407 series microcontroller, supports 4 microphones, is equipped with 128 GB of storage capacity, and is powered by a 10400 mAh battery pack interfaced with a solar charger. In addition, the device is user-configurable over USB and Wi-Fi during runtime, ensuring user-friendly operation during field deployment.",
            "headline_zh": "提出SAMAY自动鸟类鸣叫记录系统，用于构建鸟类声学数据库以研究物种、分类和监测种群。",
            "intro_zh": [
                "核心问题：缺乏大规模鸟类声学数据，难以自动研究物种、分类和监测种群。",
                "方法要点：基于STM32F407微控制器，支持4麦克风、128GB存储和太阳能充电，可USB/Wi-Fi配置。",
                "实验或效果：系统能自动记录鸟鸣，支持分类和环境影响分析，适用于野外部署。"
            ],
            "tags_zh": [
                "鸟类声学记录",
                "自动物种分类",
                "环境监测系统",
                "STM32微控制器",
                "太阳能供电"
            ],
            "_index": 113
        },
        {
            "title": "Video Reality Test: Can AI-Generated ASMR Videos fool VLMs and Humans?",
            "authors": [
                "Jiaqi Wang",
                "Weijia Wu",
                "Yi Zhan",
                "Rui Zhao",
                "Ming Hu",
                "James Cheng",
                "Wei Liu",
                "Philip Torr",
                "Kevin Qinghong Lin"
            ],
            "arxiv_id": "2512.13281v1",
            "summary": "Recent advances in video generation have produced vivid content that are often indistinguishable from real videos, making AI-generated video detection an emerging societal challenge. Prior AIGC detection benchmarks mostly evaluate video without audio, target broad narrative domains, and focus on classification solely. Yet it remains unclear whether state-of-the-art video generation models can produce immersive, audio-paired videos that reliably deceive humans and VLMs. To this end, we introduce Video Reality Test, an ASMR-sourced video benchmark suite for testing perceptual realism under tight audio-visual coupling, featuring the following dimensions: \\textbf{(i) Immersive ASMR video-audio sources.} Built on carefully curated real ASMR videos, the benchmark targets fine-grained action-object interactions with diversity across objects, actions, and backgrounds. \\textbf{(ii) Peer-Review evaluation.} An adversarial creator-reviewer protocol where video generation models act as creators aiming to fool reviewers, while VLMs serve as reviewers seeking to identify fakeness. Our experimental findings show: The best creator Veo3.1-Fast even fools most VLMs: the strongest reviewer (Gemini 2.5-Pro) achieves only 56\\% accuracy (random 50\\%), far below that of human experts (81.25\\%). Adding audio improves real-fake discrimination, yet superficial cues such as watermarks can still significantly mislead models. These findings delineate the current boundary of video generation realism and expose limitations of VLMs in perceptual fidelity and audio-visual consistency. Our code is available at https://github.com/video-reality-test/video-reality-test.",
            "headline_zh": "提出Video Reality Test基准，评估AI生成ASMR视频在视听耦合下欺骗人类和视觉语言模型的能力。",
            "intro_zh": [
                "核心问题：AI生成视频的感知真实性，尤其在视听同步的沉浸式场景中，能否可靠欺骗人类和模型。",
                "方法要点：基于真实ASMR视频构建基准，采用对抗性创作者-评审者协议，测试视频生成模型与视觉语言模型的交互。",
                "实验或效果：最佳生成模型Veo3.1-Fast在欺骗视觉语言模型方面表现突出，但人类专家识别准确率更高，音频提升鉴别但水印等表面线索仍误导模型。"
            ],
            "tags_zh": [
                "AI生成视频检测",
                "视听一致性",
                "ASMR视频基准",
                "对抗性评估",
                "视觉语言模型",
                "感知真实性"
            ],
            "_index": 114
        },
        {
            "title": "AutoTool: Dynamic Tool Selection and Integration for Agentic Reasoning",
            "authors": [
                "Jiaru Zou",
                "Ling Yang",
                "Yunzhe Qi",
                "Sirui Chen",
                "Mengting Ai",
                "Ke Shen",
                "Jingrui He",
                "Mengdi Wang"
            ],
            "arxiv_id": "2512.13278v1",
            "summary": "Agentic reinforcement learning has advanced large language models (LLMs) to reason through long chain-of-thought trajectories while interleaving external tool use. Existing approaches assume a fixed inventory of tools, limiting LLM agents' adaptability to new or evolving toolsets. We present AutoTool, a framework that equips LLM agents with dynamic tool-selection capabilities throughout their reasoning trajectories. We first construct a 200k dataset with explicit tool-selection rationales across 1,000+ tools and 100+ tasks spanning mathematics, science, code generation, and multimodal reasoning. Building on this data foundation, AutoTool employs a dual-phase optimization pipeline: (i) supervised and RL-based trajectory stabilization for coherent reasoning, and (ii) KL-regularized Plackett-Luce ranking to refine consistent multi-step tool selection. Across ten diverse benchmarks, we train two base models, Qwen3-8B and Qwen2.5-VL-7B, with AutoTool. With fewer parameters, AutoTool consistently outperforms advanced LLM agents and tool-integration methods, yielding average gains of 6.4% in math & science reasoning, 4.5% in search-based QA, 7.7% in code generation, and 6.9% in multimodal understanding. In addition, AutoTool exhibits stronger generalization by dynamically leveraging unseen tools from evolving toolsets during inference.",
            "headline_zh": "提出AutoTool框架，使LLM代理在推理轨迹中动态选择工具以增强适应性。",
            "intro_zh": [
                "现有方法假设固定工具集，限制LLM代理对新工具集的适应性。",
                "AutoTool通过双阶段优化管道实现动态工具选择，包括轨迹稳定和KL正则化排名。",
                "在多个基准测试中，AutoTool以较少参数实现性能提升，并展示对未见工具的泛化能力。"
            ],
            "tags_zh": [
                "动态工具选择",
                "代理推理",
                "强化学习",
                "多任务基准",
                "工具集成",
                "泛化能力"
            ],
            "_index": 115
        },
        {
            "title": "CogniEdit: Dense Gradient Flow Optimization for Fine-Grained Image Editing",
            "authors": [
                "Yan Li",
                "Lin Liu",
                "Xiaopeng Zhang",
                "Wei Xue",
                "Wenhan Luo",
                "Yike Guo",
                "Qi Tian"
            ],
            "arxiv_id": "2512.13276v1",
            "summary": "Instruction-based image editing with diffusion models has achieved impressive results, yet existing methods strug- gle with fine-grained instructions specifying precise attributes such as colors, positions, and quantities. While recent approaches employ Group Relative Policy Optimization (GRPO) for alignment, they optimize only at individual sampling steps, providing sparse feedback that limits trajectory-level control. We propose a unified framework CogniEdit, combining multi-modal reasoning with dense reward optimization that propagates gradients across con- secutive denoising steps, enabling trajectory-level gradient flow through the sampling process. Our method comprises three components: (1) Multi-modal Large Language Models for decomposing complex instructions into actionable directives, (2) Dynamic Token Focus Relocation that adaptively emphasizes fine-grained attributes, and (3) Dense GRPO-based optimization that propagates gradients across consecutive steps for trajectory-level supervision. Extensive experiments on benchmark datasets demonstrate that our CogniEdit achieves state-of-the-art performance in balancing fine-grained instruction following with visual quality and editability preservation",
            "headline_zh": "提出CogniEdit框架，通过密集梯度流优化解决扩散模型在细粒度图像编辑中的指令遵循问题",
            "intro_zh": [
                "现有方法在遵循细粒度指令（如颜色、位置）时存在稀疏反馈限制轨迹级控制的问题",
                "CogniEdit结合多模态推理与密集奖励优化，在去噪步骤间传播梯度以实现轨迹级监督",
                "实验表明CogniEdit在基准数据集上实现了细粒度指令遵循与视觉质量、可编辑性保持的平衡"
            ],
            "tags_zh": [
                "细粒度图像编辑",
                "扩散模型",
                "梯度流优化",
                "多模态推理",
                "指令遵循"
            ],
            "_index": 116
        },
        {
            "title": "Lightweight Dynamic Modeling of Cable-Driven Continuum Robots Based on Actuation-Space Energy Formulation",
            "authors": [
                "Fangju Yang",
                "Hang Yang",
                "Ibrahim Alsarraj",
                "Yuhao Wang",
                "Ke Wu"
            ],
            "arxiv_id": "2512.13271v1",
            "summary": "Cable-driven continuum robots (CDCRs) require accurate, real-time dynamic models for high-speed dynamics prediction or model-based control, making such capability an urgent need. In this paper, we propose the Lightweight Actuation-Space Energy Modeling (LASEM) framework for CDCRs, which formulates actuation potential energy directly in actuation space to enable lightweight yet accurate dynamic modeling. Through a unified variational derivation, the governing dynamics reduce to a single partial differential equation (PDE), requiring only the Euler moment balance while implicitly incorporating the Newton force balance. By also avoiding explicit computation of cable-backbone contact forces, the formulation simplifies the model structure and improves computational efficiency while preserving geometric accuracy and physical consistency. Importantly, the proposed framework for dynamic modeling natively supports both force-input and displacement-input actuation modes, a capability seldom achieved in existing dynamic formulations. Leveraging this lightweight structure, a Galerkin space-time modal discretization with analytical time-domain derivatives of the reduced state further enables an average 62.3% computational speedup over state-of-the-art real-time dynamic modeling approaches.",
            "headline_zh": "提出基于驱动空间能量公式的轻量动态建模框架，以提升缆驱连续体机器人的实时动态预测与控制效率。",
            "intro_zh": [
                "核心问题：缆驱连续体机器人需高精度实时动态模型，现有方法计算复杂或功能受限。",
                "方法要点：在驱动空间直接公式化势能，简化模型结构，避免显式计算接触力，支持力/位移输入模式。",
                "实验或效果：通过模态离散化，计算速度比先进实时方法平均提升62.3%。"
            ],
            "tags_zh": [
                "缆驱连续体机器人",
                "轻量动态建模",
                "驱动空间能量公式",
                "实时预测",
                "模型简化",
                "计算效率"
            ],
            "_index": 117
        },
        {
            "title": "Post-Training and Test-Time Scaling of Generative Agent Behavior Models for Interactive Autonomous Driving",
            "authors": [
                "Hyunki Seong",
                "Jeong-Kyun Lee",
                "Heesoo Myeong",
                "Yongho Shin",
                "Hyun-Mook Cho",
                "Duck Hoon Kim",
                "Pranav Desai",
                "Monu Surana"
            ],
            "arxiv_id": "2512.13262v1",
            "summary": "Learning interactive motion behaviors among multiple agents is a core challenge in autonomous driving. While imitation learning models generate realistic trajectories, they often inherit biases from datasets dominated by safe demonstrations, limiting robustness in safety-critical cases. Moreover, most studies rely on open-loop evaluation, overlooking compounding errors in closed-loop execution. We address these limitations with two complementary strategies. First, we propose Group Relative Behavior Optimization (GRBO), a reinforcement learning post-training method that fine-tunes pretrained behavior models via group relative advantage maximization with human regularization. Using only 10% of the training dataset, GRBO improves safety performance by over 40% while preserving behavioral realism. Second, we introduce Warm-K, a warm-started Top-K sampling strategy that balances consistency and diversity in motion selection. Our Warm-K method-based test-time scaling enhances behavioral consistency and reactivity at test time without retraining, mitigating covariate shift and reducing performance discrepancies. Demo videos are available in the supplementary material.",
            "headline_zh": "提出GRBO和Warm-K方法，以增强自动驾驶中生成式智能体行为模型的安全性和闭环性能。",
            "intro_zh": [
                "核心问题：模仿学习模型存在数据集偏差，导致安全关键场景下鲁棒性不足，且多数研究依赖开环评估，忽略闭环执行中的累积误差。",
                "方法要点：GRBO通过强化学习后训练，利用群体相对优势最大化和人类正则化微调预训练模型；Warm-K采用热启动Top-K采样策略，在测试时平衡一致性和多样性。",
                "实验或效果：GRBO仅用10%训练数据提升安全性能超40%，保持行为真实性；Warm-K增强测试时行为一致性和反应性，无需重新训练。"
            ],
            "tags_zh": [
                "自动驾驶行为建模",
                "强化学习后训练",
                "测试时采样策略",
                "闭环评估",
                "安全性能优化",
                "群体交互学习"
            ],
            "_index": 118
        },
        {
            "title": "BézierFlow: Bézier Stochastic Interpolant Schedulers for Few-Step Generation",
            "authors": [
                "Yunhong Min",
                "Juil Koo",
                "Seungwoo Yoo",
                "Minhyuk Sung"
            ],
            "arxiv_id": "2512.13255v1",
            "summary": "We introduce BézierFlow, a lightweight training approach for few-step generation with pretrained diffusion and flow models. BézierFlow achieves a 2-3x performance improvement for sampling with $\\leq$ 10 NFEs while requiring only 15 minutes of training. Recent lightweight training approaches have shown promise by learning optimal timesteps, but their scope remains restricted to ODE discretizations. To broaden this scope, we propose learning the optimal transformation of the sampling trajectory by parameterizing stochastic interpolant (SI) schedulers. The main challenge lies in designing a parameterization that satisfies critical desiderata, including boundary conditions, differentiability, and monotonicity of the SNR. To effectively meet these requirements, we represent scheduler functions as Bézier functions, where control points naturally enforce these properties. This reduces the problem to learning an ordered set of points in the time range, while the interpretation of the points changes from ODE timesteps to Bézier control points. Across a range of pretrained diffusion and flow models, BézierFlow consistently outperforms prior timestep-learning methods, demonstrating the effectiveness of expanding the search space from discrete timesteps to Bézier-based trajectory transformations.",
            "headline_zh": "提出BézierFlow，通过参数化随机插值调度器优化采样轨迹，实现少步生成性能提升。",
            "intro_zh": [
                "核心问题：现有轻量训练方法局限于ODE离散化，难以优化采样轨迹变换。",
                "方法要点：使用Bézier函数参数化调度器，满足边界条件和单调性等关键需求。",
                "实验或效果：在预训练扩散和流模型中，BézierFlow在≤10步采样时性能提升2-3倍，训练仅需15分钟。"
            ],
            "tags_zh": [
                "少步生成",
                "随机插值调度器",
                "Bézier函数",
                "扩散模型",
                "流模型",
                "轻量训练"
            ],
            "_index": 119
        },
        {
            "title": "Toward Ambulatory Vision: Learning Visually-Grounded Active View Selection",
            "authors": [
                "Juil Koo",
                "Daehyeon Choi",
                "Sangwoo Youn",
                "Phillip Y. Lee",
                "Minhyuk Sung"
            ],
            "arxiv_id": "2512.13250v1",
            "summary": "Vision Language Models (VLMs) excel at visual question answering (VQA) but remain limited to snapshot vision, reasoning from static images. In contrast, embodied agents require ambulatory vision, actively moving to obtain more informative views. We introduce Visually Grounded Active View Selection (VG-AVS), a task that selects the most informative next viewpoint using only the visual information in the current image, without relying on scene memory or external knowledge. To support this task, we construct a synthetic dataset with automatically generated paired query-target views and question-answer prompts. We also propose a framework that fine-tunes pretrained VLMs through supervised fine-tuning (SFT) followed by RL-based policy optimization. Our approach achieves strong question answering performance based on viewpoint selection and generalizes robustly to unseen synthetic and real scenes. Furthermore, incorporating our learned VG-AVS framework into existing scene-exploration-based EQA systems improves downstream question-answering accuracy.",
            "headline_zh": "提出视觉基础主动视角选择框架，以增强视觉语言模型在移动场景中的视觉信息获取能力。",
            "intro_zh": [
                "核心问题：视觉语言模型局限于静态图像，无法主动选择视角以获取更丰富视觉信息。",
                "方法要点：通过监督微调和强化学习优化预训练模型，实现仅基于当前图像的视角选择。",
                "实验或效果：在合成和真实场景中泛化良好，并提升现有场景探索问答系统的准确性。"
            ],
            "tags_zh": [
                "主动视角选择",
                "视觉语言模型",
                "强化学习",
                "移动视觉",
                "视觉问答",
                "场景探索"
            ],
            "_index": 120
        },
        {
            "title": "STARCaster: Spatio-Temporal AutoRegressive Video Diffusion for Identity- and View-Aware Talking Portraits",
            "authors": [
                "Foivos Paraperas Papantoniou",
                "Stathis Galanakis",
                "Rolandos Alexandros Potamias",
                "Bernhard Kainz",
                "Stefanos Zafeiriou"
            ],
            "arxiv_id": "2512.13247v1",
            "summary": "This paper presents STARCaster, an identity-aware spatio-temporal video diffusion model that addresses both speech-driven portrait animation and free-viewpoint talking portrait synthesis, given an identity embedding or reference image, within a unified framework. Existing 2D speech-to-video diffusion models depend heavily on reference guidance, leading to limited motion diversity. At the same time, 3D-aware animation typically relies on inversion through pre-trained tri-plane generators, which often leads to imperfect reconstructions and identity drift. We rethink reference- and geometry-based paradigms in two ways. First, we deviate from strict reference conditioning at pre-training by introducing softer identity constraints. Second, we address 3D awareness implicitly within the 2D video domain by leveraging the inherent multi-view nature of video data. STARCaster adopts a compositional approach progressing from ID-aware motion modeling, to audio-visual synchronization via lip reading-based supervision, and finally to novel view animation through temporal-to-spatial adaptation. To overcome the scarcity of 4D audio-visual data, we propose a decoupled learning approach in which view consistency and temporal coherence are trained independently. A self-forcing training scheme enables the model to learn from longer temporal contexts than those generated at inference, mitigating the overly static animations common in existing autoregressive approaches. Comprehensive evaluations demonstrate that STARCaster generalizes effectively across tasks and identities, consistently surpassing prior approaches in different benchmarks.",
            "headline_zh": "提出STARCaster以解决身份感知和自由视角的说话肖像视频生成问题",
            "intro_zh": [
                "核心问题：现有2D语音驱动视频模型依赖参考导致运动多样性有限，3D感知动画基于预训练生成器导致重建不完美和身份漂移",
                "方法要点：采用软身份约束和隐式3D感知，通过组合方法从身份感知运动建模到音频-视觉同步再到新视角动画",
                "实验或效果：在多个基准测试中超越先前方法，有效泛化到不同任务和身份"
            ],
            "tags_zh": [
                "说话肖像动画",
                "视频扩散模型",
                "身份感知",
                "自由视角合成",
                "音频-视觉同步",
                "自回归训练"
            ],
            "_index": 121
        },
        {
            "title": "Reflective Preference Optimization (RPO): Enhancing On-Policy Alignment via Hint-Guided Reflection",
            "authors": [
                "Zihui Zhao",
                "Zechang Li"
            ],
            "arxiv_id": "2512.13240v1",
            "summary": "Direct Preference Optimization (DPO) has emerged as a lightweight and effective alternative to Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning with AI Feedback (RLAIF) for aligning large language and vision-language models. However, the standard DPO formulation, in which both the chosen and rejected responses are generated by the same policy, suffers from a weak learning signal because the two responses often share similar errors and exhibit small Kullback-Leibler (KL) divergence. This leads to slow and unstable convergence. To address this limitation, we introduce Reflective Preference Optimization (RPO), a new framework that incorporates hint-guided reflection into the DPO paradigm. RPO uses external models to identify hallucination sources and generate concise reflective hints, enabling the construction of on-policy preference pairs with stronger contrastiveness and clearer preference signals. We theoretically show that conditioning on hints increases the expected preference margin through mutual information and improves sample efficiency while remaining within the policy distribution family. Empirically, RPO achieves superior alignment with fewer training samples and iterations, substantially reducing hallucination rates and delivering state-of-the-art performance across multimodal benchmarks.",
            "headline_zh": "提出反射偏好优化以增强同策略对齐，通过提示引导反射解决直接偏好优化学习信号弱的问题。",
            "intro_zh": [
                "直接偏好优化在同策略响应生成中因KL散度小导致学习信号弱和收敛慢。",
                "RPO引入外部模型生成反射提示，构建对比性更强的偏好对以增强信号。",
                "实验显示RPO在减少幻觉和提升多模态基准性能方面优于现有方法。"
            ],
            "tags_zh": [
                "偏好优化",
                "模型对齐",
                "反射学习",
                "多模态基准",
                "幻觉减少"
            ],
            "_index": 122
        },
        {
            "title": "Ego-EXTRA: video-language Egocentric Dataset for EXpert-TRAinee assistance",
            "authors": [
                "Francesco Ragusa",
                "Michele Mazzamuto",
                "Rosario Forte",
                "Irene D'Ambra",
                "James Fort",
                "Jakob Engel",
                "Antonino Furnari",
                "Giovanni Maria Farinella"
            ],
            "arxiv_id": "2512.13238v1",
            "summary": "We present Ego-EXTRA, a video-language Egocentric Dataset for EXpert-TRAinee assistance. Ego-EXTRA features 50 hours of unscripted egocentric videos of subjects performing procedural activities (the trainees) while guided by real-world experts who provide guidance and answer specific questions using natural language. Following a ``Wizard of OZ'' data collection paradigm, the expert enacts a wearable intelligent assistant, looking at the activities performed by the trainee exclusively from their egocentric point of view, answering questions when asked by the trainee, or proactively interacting with suggestions during the procedures. This unique data collection protocol enables Ego-EXTRA to capture a high-quality dialogue in which expert-level feedback is provided to the trainee. Two-way dialogues between experts and trainees are recorded, transcribed, and used to create a novel benchmark comprising more than 15k high-quality Visual Question Answer sets, which we use to evaluate Multimodal Large Language Models. The results show that Ego-EXTRA is challenging and highlight the limitations of current models when used to provide expert-level assistance to the user. The Ego-EXTRA dataset is publicly available to support the benchmark of egocentric video-language assistants: https://fpv-iplab.github.io/Ego-EXTRA/.",
            "headline_zh": "提出Ego-EXTRA数据集，以支持专家-学员辅助场景下的视频-语言多模态研究。",
            "intro_zh": [
                "核心问题：缺乏高质量专家指导的自我中心视频-语言对话数据集，用于评估多模态助手。",
                "方法要点：采用“Wizard of OZ”范式收集50小时非脚本视频，包含专家从学员视角提供的自然语言反馈。",
                "实验或效果：构建超过15k视觉问答对基准，测试显示当前多模态大语言模型在专家级辅助任务上存在局限。"
            ],
            "tags_zh": [
                "自我中心视频",
                "视频-语言对话",
                "专家-学员辅助",
                "多模态基准",
                "视觉问答"
            ],
            "_index": 123
        },
        {
            "title": "Learning to Retrieve with Weakened Labels: Robust Training under Label Noise",
            "authors": [
                "Arnab Sharma"
            ],
            "arxiv_id": "2512.13237v1",
            "summary": "Neural Encoders are frequently used in the NLP domain to perform dense retrieval tasks, for instance, to generate the candidate documents for a given query in question-answering tasks. However, sparse annotation and label noise in the training data make it challenging to train or fine-tune such retrieval models. Although existing works have attempted to mitigate these problems by incorporating modified loss functions or data cleaning, these approaches either require some hyperparameters to tune during training or add substantial complexity to the training setup. In this work, we consider a label weakening approach to generate robust retrieval models in the presence of label noise. Instead of enforcing a single, potentially erroneous label for each query document pair, we allow for a set of plausible labels derived from both the observed supervision and the model's confidence scores. We perform an extensive evaluation considering two retrieval models, one re-ranking model, considering four diverse ranking datasets. To this end, we also consider a realistic noisy setting by using a semantic-aware noise generation technique to generate different ratios of noise. Our initial results show that label weakening can improve the performance of the retrieval tasks in comparison to 10 different state-of-the-art loss functions.",
            "headline_zh": "提出标签弱化方法以在标签噪声下训练鲁棒检索模型",
            "intro_zh": [
                "核心问题：训练数据稀疏标注和标签噪声阻碍神经编码器在密集检索任务中的训练",
                "方法要点：采用标签弱化，基于监督和模型置信度生成一组可能标签，避免强制单一错误标签",
                "实验或效果：在四个排名数据集上评估，使用语义感知噪声生成，相比10种先进损失函数提升性能"
            ],
            "tags_zh": [
                "密集检索",
                "标签噪声",
                "神经编码器",
                "鲁棒训练",
                "标签弱化"
            ],
            "_index": 124
        },
        {
            "title": "CORE: Contrastive Masked Feature Reconstruction on Graphs",
            "authors": [
                "Jianyuan Bo",
                "Yuan Fang"
            ],
            "arxiv_id": "2512.13235v1",
            "summary": "In the rapidly evolving field of self-supervised learning on graphs, generative and contrastive methodologies have emerged as two dominant approaches. Our study focuses on masked feature reconstruction (MFR), a generative technique where a model learns to restore the raw features of masked nodes in a self-supervised manner. We observe that both MFR and graph contrastive learning (GCL) aim to maximize agreement between similar elements. Building on this observation, we reveal a novel theoretical insight: under specific conditions, the objectives of MFR and node-level GCL converge, despite their distinct operational mechanisms. This theoretical connection suggests these approaches are complementary rather than fundamentally different, prompting us to explore their integration to enhance self-supervised learning on graphs. Our research presents Contrastive Masked Feature Reconstruction (CORE), a novel graph self-supervised learning framework that integrates contrastive learning into MFR. Specifically, we form positive pairs exclusively between the original and reconstructed features of masked nodes, encouraging the encoder to prioritize contextual information over the node's own features. Additionally, we leverage the masked nodes themselves as negative samples, combining MFR's reconstructive power with GCL's discriminative ability to better capture intrinsic graph structures. Empirically, our proposed framework CORE significantly outperforms MFR across node and graph classification tasks, demonstrating state-of-the-art results. In particular, CORE surpasses GraphMAE and GraphMAE2 by up to 2.80% and 3.72% on node classification tasks, and by up to 3.82% and 3.76% on graph classification tasks.",
            "headline_zh": "提出CORE框架，通过结合掩码特征重建与对比学习增强图自监督学习性能",
            "intro_zh": [
                "核心问题：图自监督学习中生成式与对比式方法互补性不足，影响学习效果",
                "方法要点：在掩码特征重建中引入对比学习，利用原始与重建特征作为正对，掩码节点作为负样本",
                "实验或效果：在节点和图分类任务上显著超越MFR及GraphMAE等基线，达到先进水平"
            ],
            "tags_zh": [
                "图自监督学习",
                "掩码特征重建",
                "对比学习",
                "节点分类",
                "图分类",
                "特征重建"
            ],
            "_index": 125
        },
        {
            "title": "ModSSC: A Modular Framework for Semi-Supervised Classification on Heterogeneous Data",
            "authors": [
                "Melvin Barbaux"
            ],
            "arxiv_id": "2512.13228v1",
            "summary": "Semi-supervised classification leverages both labeled and unlabeled data to improve predictive performance, but existing software support is fragmented across methods and modalities. We introduce ModSSC, an open source Python framework that unifies inductive and transductive semi-supervised classification in a modular code base. ModSSC implements a broad range of classical and recent algorithms, provides loaders for tabular, image, text, audio and graph datasets, and exposes a single configuration interface for specifying datasets, models and evaluation protocols. It supports both lightweight classical methods on small datasets running on CPU and recent deep approaches that can exploit multiple GPUs within the same experimental framework. Experiments are described declaratively in YAML, which facilitates reproducing existing work and running large comparative studies. ModSSC 1.0.0 is released under the MIT license with extensive documentation and tests, and is available at https://github.com/ModSSC/ModSSC.",
            "headline_zh": "提出ModSSC框架以统一异构数据上的半监督分类算法实现与实验配置",
            "intro_zh": [
                "核心问题：现有半监督分类软件支持分散，缺乏统一框架处理异构数据和方法。",
                "方法要点：提供模块化开源Python框架，集成归纳与直推式算法，支持多种数据类型和硬件。",
                "实验或效果：通过YAML声明式实验配置，便于复现和比较研究，已发布1.0.0版本。"
            ],
            "tags_zh": [
                "半监督分类",
                "异构数据处理",
                "模块化框架",
                "开源软件",
                "实验复现"
            ],
            "_index": 126
        },
        {
            "title": "Better LMO-based Momentum Methods with Second-Order Information",
            "authors": [
                "Sarit Khirirat",
                "Abdurakhmon Sadiev",
                "Yury Demidovich",
                "Peter Richtárik"
            ],
            "arxiv_id": "2512.13227v1",
            "summary": "The use of momentum in stochastic optimization algorithms has shown empirical success across a range of machine learning tasks. Recently, a new class of stochastic momentum algorithms has emerged within the Linear Minimization Oracle (LMO) framework--leading to state-of-the-art methods, such as Muon, Scion, and Gluon, that effectively solve deep neural network training problems. However, traditional stochastic momentum methods offer convergence guarantees no better than the ${O}(1/K^{1/4})$ rate. While several approaches--such as Hessian-Corrected Momentum (HCM)--have aimed to improve this rate, their theoretical results are generally restricted to the Euclidean norm setting. This limitation hinders their applicability in problems, where arbitrary norms are often required. In this paper, we extend the LMO-based framework by integrating HCM, and provide convergence guarantees under relaxed smoothness and arbitrary norm settings. We establish improved convergence rates of ${O}(1/K^{1/3})$ for HCM, which can adapt to the geometry of the problem and achieve a faster rate than traditional momentum. Experimental results on training Multi-Layer Perceptrons (MLPs) and Long Short-Term Memory (LSTM) networks verify our theoretical observations.",
            "headline_zh": "提出基于线性最小化预言机的二阶信息动量方法，以提升任意范数下的收敛速度",
            "intro_zh": [
                "传统随机动量方法收敛率受限，在任意范数问题中应用受限",
                "扩展LMO框架，集成Hessian校正动量，提供改进的收敛保证",
                "实验验证在MLP和LSTM训练中实现更快收敛"
            ],
            "tags_zh": [
                "随机优化",
                "动量方法",
                "线性最小化预言机",
                "二阶信息",
                "收敛分析",
                "神经网络训练"
            ],
            "_index": 127
        },
        {
            "title": "A Unified Framework for Automated Assembly Sequence and Production Line Planning using Graph-based Optimization",
            "authors": [
                "Christoph Hartmann",
                "Marios Demetriades",
                "Kevin Prüfer",
                "Zichen Zhang",
                "Klaus Spindler",
                "Stefan Weltge"
            ],
            "arxiv_id": "2512.13219v1",
            "summary": "This paper presents PyCAALP (Python-based Computer-Aided Assembly Line Planning), a framework for automated Assembly Sequence Planning (ASP) and Production Line Planning (PLP), employing a graph-based approach to model components and joints within production modules. The framework integrates kinematic boundary conditions, such as potential part collisions, to guarantee the feasibility of automated assembly planning. The developed algorithm computes all feasible production sequences, integrating modules for detecting spatial relationships and formulating geometric constraints. The algorithm incorporates additional attributes, including handling feasibility, tolerance matching, and joint compatibility, to manage the high combinatorial complexity inherent in assembly sequence generation. Heuristics, such as Single-Piece Flow assembly and geometrical constraint enforcement, are utilized to further refine the solution space, facilitating more efficient planning for complex assemblies. The PLP stage is formulated as a Mixed-Integer Program (MIP), balancing the total times of a fixed number of manufacturing stations. While some complexity reduction techniques may sacrifice optimality, they significantly reduce the MIPs computational time. Furthermore, the framework enables customization of engineering constraints and supports a flexible trade-off between ASP and PLP. The open-source nature of the framework, available at https://github.com/TUM-utg/PyCAALP, promotes further collaboration and adoption in both industrial and production research applications.",
            "headline_zh": "提出PyCAALP框架，基于图优化自动化解决装配序列与生产线规划问题",
            "intro_zh": [
                "核心问题：处理装配序列生成的高组合复杂性，确保自动化规划的可行性。",
                "方法要点：采用图模型整合运动学边界条件，结合启发式与混合整数规划优化。",
                "实验或效果：开源框架支持工程约束定制，平衡规划效率与计算时间。"
            ],
            "tags_zh": [
                "装配序列规划",
                "生产线规划",
                "图优化",
                "混合整数规划",
                "自动化装配",
                "开源框架"
            ],
            "_index": 128
        },
        {
            "title": "Rethinking Physics-Informed Regression Beyond Training Loops and Bespoke Architectures",
            "authors": [
                "Lorenzo Sabug",
                "Eric Kerrigan"
            ],
            "arxiv_id": "2512.13217v1",
            "summary": "We revisit the problem of physics-informed regression, and propose a method that directly computes the state at the prediction point, simultaneously with the derivative and curvature information of the existing samples. We frame each prediction as a constrained optimisation problem, leveraging multivariate Taylor series expansions and explicitly enforcing physical laws. Each individual query can be processed with low computational cost without any pre- or re-training, in contrast to global function approximator-based solutions such as neural networks. Our comparative benchmarks on a reaction-diffusion system show competitive predictive accuracy relative to a neural network-based solution, while completely eliminating the need for long training loops, and remaining robust to changes in the sampling layout.",
            "headline_zh": "提出基于约束优化的物理信息回归方法，直接计算预测点状态，无需训练循环。",
            "intro_zh": [
                "核心问题：传统物理信息回归依赖全局函数逼近器如神经网络，需长训练循环且对采样布局敏感。",
                "方法要点：将每个预测建模为约束优化问题，利用多元泰勒展开并显式强制物理定律，实现低计算成本查询。",
                "实验或效果：在反应-扩散系统上，与神经网络方法相比，预测精度竞争，同时消除训练需求并保持对采样布局的鲁棒性。"
            ],
            "tags_zh": [
                "物理信息回归",
                "约束优化",
                "多元泰勒展开",
                "反应-扩散系统",
                "无训练预测"
            ],
            "_index": 129
        },
        {
            "title": "Multi-directional Safe Rectangle Corridor-Based MPC for Nonholonomic Robots Navigation in Cluttered Environment",
            "authors": [
                "Yinsong Qu",
                "Yunxiang Li",
                "Shanlin Zhong"
            ],
            "arxiv_id": "2512.13215v1",
            "summary": "Autonomous Mobile Robots (AMRs) have become indispensable in industrial applications due to their operational flexibility and efficiency. Navigation serves as a crucial technical foundation for accomplishing complex tasks. However, navigating AMRs in dense, cluttered, and semi-structured environments remains challenging, primarily due to nonholonomic vehicle dynamics, interactions with mixed static/dynamic obstacles, and the non-convex constrained nature of such operational spaces. To solve these problems, this paper proposes an Improved Sequential Model Predictive Control (ISMPC) navigation framework that systematically reformulates navigation tasks as sequential switched optimal control problems. The framework addresses the aforementioned challenges through two key innovations: 1) Implementation of a Multi-Directional Safety Rectangular Corridor (MDSRC) algorithm, which encodes the free space through rectangular convex regions to avoid collision with static obstacles, eliminating redundant computational burdens and accelerating solver convergence; 2) A sequential MPC navigation framework that integrates corridor constraints with barrier function constraints is proposed to achieve static and dynamic obstacle avoidance. The ISMPC navigation framework enables direct velocity generation for AMRs, simplifying traditional navigation algorithm architectures. Comparative experiments demonstrate the framework's superiority in free-space utilization ( an increase of 41.05$\\%$ in the average corridor area) while maintaining real-time computational performance (average corridors generation latency of 3 ms).",
            "headline_zh": "提出基于多向安全矩形走廊的改进序列模型预测控制框架，以解决非完整机器人在杂乱环境中的导航问题。",
            "intro_zh": [
                "核心问题：非完整机器人动力学、静态/动态障碍物交互及非凸约束空间导致导航困难。",
                "方法要点：采用多向安全矩形走廊算法编码自由空间，结合序列MPC框架实现障碍物避让。",
                "实验或效果：实验显示平均走廊面积增加41.05%，生成延迟3毫秒，提升实时性能。"
            ],
            "tags_zh": [
                "非完整机器人导航",
                "模型预测控制",
                "安全走廊",
                "障碍物避让",
                "实时计算",
                "杂乱环境"
            ],
            "_index": 130
        },
        {
            "title": "Differentiable Material Point Method for the Control of Deformable Objects",
            "authors": [
                "Diego Bolliger",
                "Gabriele Fadini",
                "Markus Bambach",
                "Alisa Rupenyan"
            ],
            "arxiv_id": "2512.13214v1",
            "summary": "Controlling the deformation of flexible objects is challenging due to their non-linear dynamics and high-dimensional configuration space. This work presents a differentiable Material Point Method (MPM) simulator targeted at control applications. We exploit the differentiability of the simulator to optimize a control trajectory in an active damping problem for a hyperelastic rope. The simulator effectively minimizes the kinetic energy of the rope around 2$\\times$ faster than a baseline MPPI method and to a 20% lower energy level, while using about 3% of the computation time.",
            "headline_zh": "提出可微分材料点法模拟器以优化柔性物体的控制轨迹",
            "intro_zh": [
                "核心问题：柔性物体因非线性动力学和高维配置空间而难以控制变形",
                "方法要点：开发可微分MPM模拟器，利用其可微性优化控制轨迹",
                "实验或效果：在超弹性绳索主动阻尼问题中，比基线MPPI方法快约2倍，能量降低20%，计算时间减少约97%"
            ],
            "tags_zh": [
                "可微分模拟",
                "材料点法",
                "柔性物体控制",
                "主动阻尼",
                "轨迹优化"
            ],
            "_index": 131
        },
        {
            "title": "Evaluating Adversarial Attacks on Federated Learning for Temperature Forecasting",
            "authors": [
                "Karina Chichifoi",
                "Fabio Merizzi",
                "Michele Colajanni"
            ],
            "arxiv_id": "2512.13207v1",
            "summary": "Deep learning and federated learning (FL) are becoming powerful partners for next-generation weather forecasting. Deep learning enables high-resolution spatiotemporal forecasts that can surpass traditional numerical models, while FL allows institutions in different locations to collaboratively train models without sharing raw data, addressing efficiency and security concerns. While FL has shown promise across heterogeneous regions, its distributed nature introduces new vulnerabilities. In particular, data poisoning attacks, in which compromised clients inject manipulated training data, can degrade performance or introduce systematic biases. These threats are amplified by spatial dependencies in meteorological data, allowing localized perturbations to influence broader regions through global model aggregation. In this study, we investigate how adversarial clients distort federated surface temperature forecasts trained on the Copernicus European Regional ReAnalysis (CERRA) dataset. We simulate geographically distributed clients and evaluate patch-based and global biasing attacks on regional temperature forecasts. Our results show that even a small fraction of poisoned clients can mislead predictions across large, spatially connected areas. A global temperature bias attack from a single compromised client shifts predictions by up to -1.7 K, while coordinated patch attacks more than triple the mean squared error and produce persistent regional anomalies exceeding +3.5 K. Finally, we assess trimmed mean aggregation as a defense mechanism, showing that it successfully defends against global bias attacks (2-13\\% degradation) but fails against patch attacks (281-603\\% amplification), exposing limitations of outlier-based defenses for spatially correlated data.",
            "headline_zh": "评估对抗攻击对联邦学习温度预测的影响，揭示空间依赖性放大威胁",
            "intro_zh": [
                "研究联邦学习在温度预测中面临的数据投毒攻击，特别是基于空间依赖性的威胁",
                "模拟分布式客户端，评估全局偏置和基于补丁的攻击对区域预测的扭曲效果",
                "实验显示少量中毒客户端可误导大面积预测，修剪均值聚合防御对全局攻击有效但对补丁攻击失败"
            ],
            "tags_zh": [
                "联邦学习",
                "对抗攻击",
                "温度预测",
                "数据投毒",
                "空间依赖性",
                "防御机制"
            ],
            "_index": 132
        },
        {
            "title": "ALBATROSS: A robotised system for high-throughput electrolyte screening via automated electrolyte formulation, coin-cell fabrication, and electrochemical evaluation",
            "authors": [
                "Hyun-Gi Lee",
                "Jaekyeong Han",
                "Minjun Kwon",
                "Hyeonuk Kwon",
                "Jooha Park",
                "Hoe Jin Ha",
                "Dong-Hwa Seo"
            ],
            "arxiv_id": "2512.13198v1",
            "summary": "As battery technologies advance toward higher stability and energy density, the need for extensive cell-level testing across various component configurations becomes critical. To evaluate performance and understand the operating principles of batteries in laboratory scale, fabrication and evaluation of coin cells are essential processes. However, the conventional coin-cell assembly and testing processes require significant time and labor from researchers, posing challenges to high-throughput screening research. In this study, we introduce an Automated Li-ion BAttery Testing RObot SyStem (ALBATROSS), an automated system capable of electrolyte formulation, coin-cell assembly, and electrochemical evaluation. The system, integrated within a argon-filled glovebox, enables fully automated assembly and testing of up to 48 cells without researcher intervention. By incorporating custom-designed robot gripper and 3D-printed structures optimized for precise cell handling, ALBATROSS achieved high assembly reliability, yielding a relative standard deviation (RSD) of less than 1.2% in discharge capacity and a standard deviation of less than 3 Ω in EIS measurements for NCM811||Li half cells. Owing to its high reliability and automation capability, ALBATROSS allows for the acquisition of high-quality coin-cell datasets, which are expected to accelerate the development of next-generation electrolytes.",
            "headline_zh": "提出ALBATROSS自动化系统以解决锂离子电池电解液高通量筛选中的耗时与人工挑战。",
            "intro_zh": [
                "核心问题：传统纽扣电池组装与测试过程耗时耗力，阻碍高通量筛选研究。",
                "方法要点：开发集成于氩气手套箱的自动化系统，实现电解液配制、电池组装和电化学评估。",
                "实验或效果：系统组装可靠性高，放电容量相对标准偏差小于1.2%，EIS测量标准偏差小于3Ω。"
            ],
            "tags_zh": [
                "锂离子电池",
                "高通量筛选",
                "自动化系统",
                "纽扣电池",
                "电解液开发",
                "电化学评估"
            ],
            "_index": 133
        },
        {
            "title": "MicroPhaseNO: Adapting an Earthquake-Trained Phase Neural Operator for Microseismic Phase Picking",
            "authors": [
                "Ayrat Abdullin",
                "Umair bin Waheed",
                "Leo Eisner",
                "Naveed Iqbal"
            ],
            "arxiv_id": "2512.13197v1",
            "summary": "Seismic phase picking is very often used for microseismic monitoring and subsurface imaging. Traditional manual processing is not feasible for either real-time applications or large arrays. Deep learning-based pickers trained on large earthquake catalogs offer an automated alternative. However, they are typically optimized for high signal-to-noise, long-duration networks and struggle with the challenges presented by microseismic datasets, which are purpose-built for limited time without previously detected seismicity. In this study, we demonstrate how a network-wide earthquake phase picker, the Phase Neural Operator (PhaseNO), can be adapted to microseismic monitoring using transfer learning. Starting from a PhaseNO model pre-trained on more than 57,000 three-component earthquake and noise records, we fine-tune the model using only 200 labeled and noise seismograms from induced events in hydraulic-fracturing settings. The fine-tuned model thus preserves the rich spatio-temporal representation learned from abundant earthquake data, while adapting to the characteristics and labeling conventions of microseismic phases, which are often picked on peaks or troughs rather than onsets. We evaluate performance on three distinct real-world microseismic datasets with different network geometries and acquisition parameters. Compared to the original PhaseNO and a conventional workflow, the adapted model increases F1 score and accuracy by up to 30%, and strongly reduces systematic timing bias and pick uncertainty. Because the adaptation relies on a small, campaign-specific calibration set, the approach is readily transferable to other microseismic tasks where public earthquake data and pre-trained models are accessible. The associated code will be released openly at https://github.com/ayratabd/MicroPhaseNO.",
            "headline_zh": "提出MicroPhaseNO，通过迁移学习将地震相位神经算子适配于微地震相位拾取",
            "intro_zh": [
                "核心问题：传统地震相位拾取器在微地震数据上性能不佳，因微地震信号噪声高、网络时间短且标注方式不同。",
                "方法要点：基于预训练的PhaseNO模型，仅用200个微地震记录进行微调，保留地震数据学习到的时空表示，适应微地震特征。",
                "实验或效果：在三个真实微地震数据集上评估，F1分数和准确率提升高达30%，显著减少时间偏差和拾取不确定性。"
            ],
            "tags_zh": [
                "微地震监测",
                "相位拾取",
                "迁移学习",
                "神经算子",
                "地震数据处理",
                "深度学习"
            ],
            "_index": 134
        },
        {
            "title": "Noise-Resilient Quantum Aggregation on NISQ for Federated ADAS Learning",
            "authors": [
                "Chethana Prasad Kabgere",
                "Sudarshan T S B"
            ],
            "arxiv_id": "2512.13196v1",
            "summary": "Advanced Driver Assistance Systems (ADAS) increasingly employ Federated Learning (FL) to collaboratively train models across distributed vehicular nodes while preserving data privacy. Yet, conventional FL aggregation remains susceptible to noise, latency, and security constraints inherent to real-time vehicular networks. This paper introduces Noise-Resilient Quantum Federated Learning (NR-QFL), a hybrid quantum-classical framework that enables secure, low-latency aggregation through variational quantum circuits (VQCs) operating under Noisy Intermediate-Scale Quantum (NISQ) conditions. The framework encodes model parameters as quantum states with adaptive gate reparameterization, ensuring bounded-error convergence and provable resilience under Completely Positive Trace-Preserving (CPTP) dynamics. NR-QFL employs quantum entropy-based client selection and multi-server coordination for fairness and stability. Empirical validation shows consistent convergence with reduced gradient variance, lower communication overhead, and enhanced noise tolerance under constrained edge conditions. The framework establishes a scalable foundation for quantum-enhanced federated learning, enabling secure, efficient, and dynamically stable ADAS intelligence at the vehicular edge.",
            "headline_zh": "提出噪声弹性量子联邦学习框架以解决车载ADAS中联邦学习的噪声、延迟和安全问题。",
            "intro_zh": [
                "核心问题：传统联邦学习在实时车载网络中易受噪声、延迟和安全约束影响。",
                "方法要点：采用变分量子电路在NISQ条件下编码模型参数，实现有界误差收敛和噪声弹性。",
                "实验或效果：经验验证显示收敛稳定，梯度方差降低，通信开销减少，噪声容忍度增强。"
            ],
            "tags_zh": [
                "量子联邦学习",
                "噪声弹性聚合",
                "变分量子电路",
                "车载边缘计算",
                "模型参数编码",
                "多服务器协调"
            ],
            "_index": 135
        },
        {
            "title": "Efficient Adaptive Rejection Sampling for Accelerating Speculative Decoding in Large Language Models",
            "authors": [
                "Chendong Sun"
            ],
            "arxiv_id": "2512.13194v1",
            "summary": "Speculative Decoding is a prominent technique for accelerating the autoregressive inference of large language models (LLMs) by employing a fast draft model to propose candidate token sequences and a large target model to verify them in parallel. However, its core component -- the rejection sampling mechanism -- relies on a fixed, context-independent random threshold. This leads to a significant \"random rejection\" problem in high-uncertainty generation scenarios, where plausible candidate tokens are frequently rejected due to random chance, undermining inference efficiency. This paper introduces Efficient Adaptive Rejection Sampling (EARS), a novel method that dynamically adjusts the acceptance threshold by incorporating the target model's own predictive uncertainty, measured as \\(1 - \\max(P_{\\mathrm{target}})\\). By introducing a tolerance term proportional to this uncertainty, EARS intelligently relaxes the acceptance criterion when the model is uncertain, effectively reducing random rejections while maintaining strict standards when the model is confident. Experiments on creative writing and open-domain QA tasks demonstrate that EARS significantly enhances the efficiency of speculative decoding, achieving up to an 18.12% increase in throughput with a negligible 0.84% accuracy drop on the GSM8K benchmark. The method requires no modifications to model architectures and can be seamlessly integrated into existing speculative decoding frameworks.",
            "headline_zh": "提出高效自适应拒绝采样以解决推测解码中随机拒绝问题",
            "intro_zh": [
                "推测解码的拒绝采样机制依赖固定阈值，导致高不确定性场景下随机拒绝频发",
                "EARS方法动态调整接受阈值，基于目标模型预测不确定性，减少随机拒绝",
                "实验显示EARS在GSM8K基准上提升吞吐量18.12%，精度仅下降0.84%"
            ],
            "tags_zh": [
                "推测解码",
                "拒绝采样",
                "大语言模型加速",
                "自适应阈值",
                "推理效率"
            ],
            "_index": 136
        },
        {
            "title": "POLAR: A Portrait OLAT Dataset and Generative Framework for Illumination-Aware Face Modeling",
            "authors": [
                "Zhuo Chen",
                "Chengqun Yang",
                "Zhuo Su",
                "Zheng Lv",
                "Jingnan Gao",
                "Xiaoyuan Zhang",
                "Xiaokang Yang",
                "Yichao Yan"
            ],
            "arxiv_id": "2512.13192v1",
            "summary": "Face relighting aims to synthesize realistic portraits under novel illumination while preserving identity and geometry. However, progress remains constrained by the limited availability of large-scale, physically consistent illumination data. To address this, we introduce POLAR, a large-scale and physically calibrated One-Light-at-a-Time (OLAT) dataset containing over 200 subjects captured under 156 lighting directions, multiple views, and diverse expressions. Building upon POLAR, we develop a flow-based generative model POLARNet that predicts per-light OLAT responses from a single portrait, capturing fine-grained and direction-aware illumination effects while preserving facial identity. Unlike diffusion or background-conditioned methods that rely on statistical or contextual cues, our formulation models illumination as a continuous, physically interpretable transformation between lighting states, enabling scalable and controllable relighting. Together, POLAR and POLARNet form a unified illumination learning framework that links real data, generative synthesis, and physically grounded relighting, establishing a self-sustaining \"chicken-and-egg\" cycle for scalable and reproducible portrait illumination.",
            "headline_zh": "提出POLAR数据集与POLARNet模型以解决人脸重光照中大规模物理一致数据不足的问题",
            "intro_zh": [
                "核心问题：人脸重光照受限于大规模、物理一致的光照数据可用性",
                "方法要点：基于POLAR数据集开发流式生成模型POLARNet，从单张肖像预测每光OLAT响应",
                "实验或效果：模型能捕捉细粒度方向感知光照效果，保持身份，实现可扩展可控重光照"
            ],
            "tags_zh": [
                "人脸重光照",
                "OLAT数据集",
                "流式生成模型",
                "物理一致光照",
                "肖像建模",
                "光照学习框架"
            ],
            "_index": 137
        },
        {
            "title": "CoRA: A Collaborative Robust Architecture with Hybrid Fusion for Efficient Perception",
            "authors": [
                "Gong Chen",
                "Chaokun Zhang",
                "Pengcheng Lv",
                "Xiaohui Xie"
            ],
            "arxiv_id": "2512.13191v1",
            "summary": "Collaborative perception has garnered significant attention as a crucial technology to overcome the perceptual limitations of single-agent systems. Many state-of-the-art (SOTA) methods have achieved communication efficiency and high performance via intermediate fusion. However, they share a critical vulnerability: their performance degrades under adverse communication conditions due to the misalignment induced by data transmission, which severely hampers their practical deployment. To bridge this gap, we re-examine different fusion paradigms, and recover that the strengths of intermediate and late fusion are not a trade-off, but a complementary pairing. Based on this key insight, we propose CoRA, a novel collaborative robust architecture with a hybrid approach to decouple performance from robustness with low communication. It is composed of two components: a feature-level fusion branch and an object-level correction branch. Its first branch selects critical features and fuses them efficiently to ensure both performance and scalability. The second branch leverages semantic relevance to correct spatial displacements, guaranteeing resilience against pose errors. Experiments demonstrate the superiority of CoRA. Under extreme scenarios, CoRA improves upon its baseline performance by approximately 19% in AP@0.7 with more than 5x less communication volume, which makes it a promising solution for robust collaborative perception.",
            "headline_zh": "提出CoRA架构，通过混合融合解决协作感知在恶劣通信下的性能下降问题。",
            "intro_zh": [
                "核心问题：现有协作感知方法在通信条件不佳时性能下降，因数据传输导致特征错位。",
                "方法要点：结合中间融合和后期融合优势，设计特征级融合分支和对象级校正分支。",
                "实验或效果：在极端场景下，AP@0.7提升约19%，通信量减少5倍以上。"
            ],
            "tags_zh": [
                "协作感知",
                "混合融合",
                "鲁棒性",
                "通信效率",
                "特征融合",
                "对象校正"
            ],
            "_index": 138
        },
        {
            "title": "WAY: Estimation of Vessel Destination in Worldwide AIS Trajectory",
            "authors": [
                "Jin Sob Kim",
                "Hyun Joon Park",
                "Wooseok Shin",
                "Dongil Park",
                "Sung Won Han"
            ],
            "arxiv_id": "2512.13190v1",
            "summary": "The Automatic Identification System (AIS) enables data-driven maritime surveillance but suffers from reliability issues and irregular intervals. We address vessel destination estimation using global-scope AIS data by proposing a differentiated approach that recasts long port-to-port trajectories as a nested sequence structure. Using spatial grids, this method mitigates spatio-temporal bias while preserving detailed resolution. We introduce a novel deep learning architecture, WAY, designed to process these reformulated trajectories for long-term destination estimation days to weeks in advance. WAY comprises a trajectory representation layer and Channel-Aggregative Sequential Processing (CASP) blocks. The representation layer generates multi-channel vector sequences from kinematic and non-kinematic features. CASP blocks utilize multi-headed channel- and self-attention for aggregation and sequential information delivery. Additionally, we propose a task-specialized Gradient Dropout (GD) technique to enable many-to-many training on single labels, preventing biased feedback surges by stochastically blocking gradient flow based on sample length. Experiments on 5-year AIS data demonstrate WAY's superiority over conventional spatial grid-based approaches regardless of trajectory progression. Results further confirm that adopting GD leads to performance gains. Finally, we explore WAY's potential for real-world application through multitask learning for ETA estimation.",
            "headline_zh": "提出WAY方法，基于全球AIS数据通过嵌套序列结构和深度学习估计船舶目的地",
            "intro_zh": [
                "核心问题：AIS数据存在可靠性问题和时间间隔不规则，影响船舶目的地估计的准确性。",
                "方法要点：将长轨迹重构为嵌套序列，使用多通道表示和CASP块进行深度学习，并引入梯度丢弃技术优化训练。",
                "实验或效果：在5年AIS数据上验证，WAY优于传统方法，梯度丢弃提升性能，并探索多任务学习应用。"
            ],
            "tags_zh": [
                "船舶目的地估计",
                "AIS轨迹分析",
                "深度学习架构",
                "梯度丢弃",
                "多任务学习",
                "时空数据处理"
            ],
            "_index": 139
        },
        {
            "title": "PolySet: Restoring the Statistical Ensemble Nature of Polymers for Machine Learning",
            "authors": [
                "Khalid Ferji"
            ],
            "arxiv_id": "2512.13186v1",
            "summary": "Machine-learning (ML) models in polymer science typically treat a polymer as a single, perfectly defined molecular graph, even though real materials consist of stochastic ensembles of chains with distributed lengths. This mismatch between physical reality and digital representation limits the ability of current models to capture polymer behaviour. Here we introduce PolySet, a framework that represents a polymer as a finite, weighted ensemble of chains sampled from an assumed molar-mass distribution. This ensemble-based encoding is independent of chemical detail, compatible with any molecular representation and illustrated here in the homopolymer case using a minimal language model. We show that PolySet retains higher-order distributional moments (such as Mz, Mz+1), enabling ML models to learn tail-sensitive properties with greatly improved stability and accuracy. By explicitly acknowledging the statistical nature of polymer matter, PolySet establishes a physically grounded foundation for future polymer machine learning, naturally extensible to copolymers, block architectures, and other complex topologies.",
            "headline_zh": "提出PolySet框架，通过加权链集合表示聚合物，以解决机器学习中聚合物统计性质缺失的问题。",
            "intro_zh": [
                "核心问题：现有机器学习模型将聚合物视为单一分子图，忽略真实材料中链长分布的统计集合性质，限制模型捕捉聚合物行为的能力。",
                "方法要点：PolySet将聚合物表示为从摩尔质量分布采样的有限加权链集合，编码独立于化学细节，适用于任何分子表示，以均聚物为例使用最小语言模型展示。",
                "实验或效果：PolySet保留高阶分布矩（如Mz、Mz+1），使机器学习模型能学习尾部敏感性质，显著提高稳定性和准确性。"
            ],
            "tags_zh": [
                "聚合物机器学习",
                "统计集合表示",
                "加权链集合",
                "摩尔质量分布",
                "尾部敏感性质",
                "均聚物建模"
            ],
            "_index": 140
        },
        {
            "title": "Efficient Generation of Smooth Paths with Curvature Guarantees by Mollification",
            "authors": [
                "Alfredo González-Calvin",
                "Juan F. Jiménez",
                "Héctor García de Marina"
            ],
            "arxiv_id": "2512.13183v1",
            "summary": "Most path following and trajectory tracking algorithms in mobile robotics require the desired path or trajectory to be defined by at least twice continuously differentiable functions to guarantee key properties such as global convergence, especially for nonholonomic robots like unicycles with speed constraints. Consequently, these algorithms typically exclude continuous but non-differentiable paths, such as piecewise functions. Despite this exclusion, such paths provide convenient high-level inputs for describing robot missions or behavior. While techniques such as spline interpolation or optimization-based methods are commonly used to smooth non-differentiable paths or create feasible ones from sequences of waypoints, they either can produce unnecessarily complex trajectories or are computationally expensive. In this work, we present a method to regularize non-differentiable functions and generate feasible paths through mollification. Specifically, we approximate an arbitrary path with a differentiable function that can converge to it with arbitrary precision. Additionally, we provide a systematic method for bounding the curvature of generated paths, which we demonstrate by applying it to paths resulting from linking a sequence of waypoints with segments. The proposed approach is computationally efficient, enabling real-time implementation on microcontrollers and compatibility with standard trajectory tracking and path following algorithms.",
            "headline_zh": "提出基于磨光法的平滑路径生成方法，以解决非完整机器人路径跟踪中非可微路径的可行性问题。",
            "intro_zh": [
                "核心问题：非可微路径（如分段函数）在机器人路径跟踪中因缺乏二阶可微性而被排除，但作为高层输入方便。",
                "方法要点：通过磨光法正则化非可微函数，生成可微路径并保证曲率有界，计算高效。",
                "实验或效果：应用于连接航点的路径，实现实时微控制器兼容，支持标准跟踪算法。"
            ],
            "tags_zh": [
                "路径平滑",
                "磨光法",
                "曲率约束",
                "非完整机器人",
                "实时路径生成",
                "轨迹跟踪"
            ],
            "_index": 141
        },
        {
            "title": "MMDrive: Interactive Scene Understanding Beyond Vision with Multi-representational Fusion",
            "authors": [
                "Minghui Hou",
                "Wei-Hsing Huang",
                "Shaofeng Liang",
                "Daizong Liu",
                "Tai-Hao Wen",
                "Gang Wang",
                "Runwei Guan",
                "Weiping Ding"
            ],
            "arxiv_id": "2512.13177v1",
            "summary": "Vision-language models enable the understanding and reasoning of complex traffic scenarios through multi-source information fusion, establishing it as a core technology for autonomous driving. However, existing vision-language models are constrained by the image understanding paradigm in 2D plane, which restricts their capability to perceive 3D spatial information and perform deep semantic fusion, resulting in suboptimal performance in complex autonomous driving environments. This study proposes MMDrive, an multimodal vision-language model framework that extends traditional image understanding to a generalized 3D scene understanding framework. MMDrive incorporates three complementary modalities, including occupancy maps, LiDAR point clouds, and textual scene descriptions. To this end, it introduces two novel components for adaptive cross-modal fusion and key information extraction. Specifically, the Text-oriented Multimodal Modulator dynamically weights the contributions of each modality based on the semantic cues in the question, guiding context-aware feature integration. The Cross-Modal Abstractor employs learnable abstract tokens to generate compact, cross-modal summaries that highlight key regions and essential semantics. Comprehensive evaluations on the DriveLM and NuScenes-QA benchmarks demonstrate that MMDrive achieves significant performance gains over existing vision-language models for autonomous driving, with a BLEU-4 score of 54.56 and METEOR of 41.78 on DriveLM, and an accuracy score of 62.7% on NuScenes-QA. MMDrive effectively breaks the traditional image-only understanding barrier, enabling robust multimodal reasoning in complex driving environments and providing a new foundation for interpretable autonomous driving scene understanding.",
            "headline_zh": "提出MMDrive多模态融合框架以解决自动驾驶中传统视觉语言模型在3D场景理解与语义融合上的局限。",
            "intro_zh": [
                "核心问题：现有视觉语言模型受限于2D图像理解，难以感知3D空间信息并进行深度语义融合，影响复杂自动驾驶环境性能。",
                "方法要点：引入文本导向多模态调制器和跨模态抽象器，动态融合占用图、LiDAR点云和文本描述，实现自适应特征整合与关键信息提取。",
                "实验或效果：在DriveLM和NuScenes-QA基准测试中，MMDrive显著超越现有模型，如BLEU-4达54.56，准确率62.7%，提升自动驾驶场景理解能力。"
            ],
            "tags_zh": [
                "多模态融合",
                "3D场景理解",
                "自动驾驶",
                "视觉语言模型",
                "跨模态抽象"
            ],
            "_index": 142
        },
        {
            "title": "Seeing the Whole Picture: Distribution-Guided Data-Free Distillation for Semantic Segmentation",
            "authors": [
                "Hongxuan Sun",
                "Tao Wu"
            ],
            "arxiv_id": "2512.13175v1",
            "summary": "Semantic segmentation requires a holistic understanding of the physical world, as it assigns semantic labels to spatially continuous and structurally coherent objects rather than to isolated pixels. However, existing data-free knowledge distillation (DFKD) methods-primarily designed for classification-often disregard this continuity, resulting in significant performance degradation when applied directly to segmentation tasks. In this paper, we introduce DFSS, a novel data-free distillation framework tailored for semantic segmentation. Unlike prior approaches that treat pixels independently, DFSS respects the structural and contextual continuity of real-world scenes. Our key insight is to leverage Batch Normalization (BN) statistics from a teacher model to guide Approximate Distribution Sampling (ADS), enabling the selection of data that better reflects the original training distribution-without relying on potentially misleading teacher predictions. Additionally, we propose Weighted Distribution Progressive Distillation (WDPD), which dynamically prioritizes reliable samples that are more closely aligned with the original data distribution early in training and gradually incorporates more challenging cases, mirroring the natural progression of learning in human perception. Extensive experiments on standard benchmarks demonstrate that DFSS consistently outperforms existing data-free distillation methods for semantic segmentation, achieving state-of-the-art results with significantly reduced reliance on auxiliary data.",
            "headline_zh": "提出DFSS框架以解决数据无知识蒸馏在语义分割中忽视结构连续性的问题",
            "intro_zh": [
                "核心问题：现有数据无知识蒸馏方法直接用于语义分割时忽略场景结构连续性，导致性能下降",
                "方法要点：利用教师模型BN统计指导近似分布采样，并引入加权分布渐进蒸馏动态优化样本选择",
                "实验或效果：在标准基准测试中优于现有方法，实现先进结果且减少对辅助数据的依赖"
            ],
            "tags_zh": [
                "语义分割",
                "数据无知识蒸馏",
                "分布引导",
                "渐进蒸馏",
                "BN统计"
            ],
            "_index": 143
        },
        {
            "title": "Carrot, stick, or both? Price incentives for sustainable food choice in competitive environments",
            "authors": [
                "Francesco Salvi",
                "Giuseppe Russo",
                "Adam Barla",
                "Vincent Moreau",
                "Robert West"
            ],
            "arxiv_id": "2512.13174v1",
            "summary": "Meat consumption is a major driver of global greenhouse gas emissions. While pricing interventions have shown potential to reduce meat intake, previous studies have focused on highly constrained environments with limited consumer choice. Here, we present the first large-scale field experiment to evaluate multiple pricing interventions in a real-world, competitive setting. Using a sequential crossover design with matched menus in a Swiss university campus, we systematically compared vegetarian-meal discounts (-2.5 CHF), meat surcharges (+2.5 CHF), and a combined scheme (-1.2 CHF=+1.2 CHF) across four campus cafeterias. Only the surcharge and combined interventions led to significant increases in vegetarian meal uptake--by 26.4% and 16.6%, respectively--and reduced CO2 emissions per meal by 7.4% and 11.3%, respectively. The surcharge, while effective, triggered a 12.3% drop in sales at intervention sites and a corresponding 14.9% increase in non-treated locations, hence causing a spillover effect that completely offset environmental gains. In contrast, the combined approach achieved meaningful emission reductions without significant effects on overall sales or revenue, making it both effective and economically viable. Notably, pricing interventions were equally effective for both vegetarian-leaning customers and habitual meat-eaters, stimulating change even within entrenched dietary habits. Our results show that balanced pricing strategies can reduce the carbon footprint of realistic food environments, but require coordinated implementation to maximize climate benefits and avoid unintended spillover effects.",
            "headline_zh": "提出平衡定价策略以在竞争性环境中促进可持续食品选择并减少碳排放",
            "intro_zh": [
                "核心问题：肉类消费是温室气体排放的主要驱动因素，现有定价干预研究局限于受限环境，缺乏真实竞争场景评估。",
                "方法要点：在瑞士大学校园采用序贯交叉设计，系统比较素食折扣、肉类附加费和组合方案对食品选择的影响。",
                "实验或效果：仅附加费和组合方案显著提升素食餐选择并减少碳排放，组合方案在保持销售稳定下实现有效减排。"
            ],
            "tags_zh": [
                "可持续食品选择",
                "定价干预",
                "碳排放减少",
                "竞争环境",
                "现场实验",
                "平衡策略"
            ],
            "_index": 144
        },
        {
            "title": "Iterative Tuning of Nonlinear Model Predictive Control for Robotic Manufacturing Tasks",
            "authors": [
                "Deepak Ingole",
                "Valentin Bhend",
                "Shiva Ganesh Murali",
                "Oliver Dobrich",
                "Alisa Rupenayan"
            ],
            "arxiv_id": "2512.13170v1",
            "summary": "Manufacturing processes are often perturbed by drifts in the environment and wear in the system, requiring control re-tuning even in the presence of repetitive operations. This paper presents an iterative learning framework for automatic tuning of Nonlinear Model Predictive Control (NMPC) weighting matrices based on task-level performance feedback. Inspired by norm-optimal Iterative Learning Control (ILC), the proposed method adaptively adjusts NMPC weights Q and R across task repetitions to minimize key performance indicators (KPIs) related to tracking accuracy, control effort, and saturation. Unlike gradient-based approaches that require differentiating through the NMPC solver, we construct an empirical sensitivity matrix, enabling structured weight updates without analytic derivatives. The framework is validated through simulation on a UR10e robot performing carbon fiber winding on a tetrahedral core. Results demonstrate that the proposed approach converges to near-optimal tracking performance (RMSE within 0.3% of offline Bayesian Optimization (BO)) in just 4 online repetitions, compared to 100 offline evaluations required by BO algorithm. The method offers a practical solution for adaptive NMPC tuning in repetitive robotic tasks, combining the precision of carefully optimized controllers with the flexibility of online adaptation.",
            "headline_zh": "提出基于任务级反馈的迭代学习框架，用于自动调优非线性模型预测控制权重，以应对机器人制造任务中的环境漂移和系统磨损。",
            "intro_zh": [
                "核心问题：制造过程常受环境漂移和系统磨损扰动，需在重复操作中重新调优控制参数。",
                "方法要点：采用迭代学习控制思想，通过经验灵敏度矩阵自适应调整NMPC权重，避免解析导数计算。",
                "实验或效果：在UR10e机器人碳纤维缠绕仿真中，仅4次在线重复即达到接近离线贝叶斯优化的跟踪性能。"
            ],
            "tags_zh": [
                "非线性模型预测控制",
                "迭代学习控制",
                "机器人制造",
                "自适应调优",
                "经验灵敏度矩阵"
            ],
            "_index": 145
        },
        {
            "title": "Finch: Benchmarking Finance & Accounting across Spreadsheet-Centric Enterprise Workflows",
            "authors": [
                "Haoyu Dong",
                "Pengkun Zhang",
                "Yan Gao",
                "Xuanyu Dong",
                "Yilin Cheng",
                "Mingzhe Lu",
                "Adina Yakefu",
                "Shuxin Zheng"
            ],
            "arxiv_id": "2512.13168v1",
            "summary": "We introduce a finance & accounting benchmark (Finch) for evaluating AI agents on real-world, enterprise-grade professional workflows -- interleaving data entry, structuring, formatting, web search, cross-file retrieval, calculation, modeling, validation, translation, visualization, and reporting. Finch is sourced from authentic enterprise workspaces at Enron (15,000 spreadsheets and 500,000 emails from 150 employees) and other financial institutions, preserving in-the-wild messiness across multimodal artifacts (text, tables, formulas, charts, code, and images) and spanning diverse domains such as budgeting, trading, and asset management.\n  We propose a workflow construction process that combines LLM-assisted discovery with expert annotation: (1) LLM-assisted, expert-verified derivation of workflows from real-world email threads and version histories of spreadsheet files, and (2) meticulous expert annotation for workflows, requiring over 700 hours of domain-expert effort. This yields 172 composite workflows with 384 tasks, involving 1,710 spreadsheets with 27 million cells, along with PDFs and other artifacts, capturing the intrinsically messy, long-horizon, knowledge-intensive, and collaborative nature of real-world enterprise work.\n  We conduct both human and automated evaluations of frontier AI systems including GPT 5.1, Claude Sonnet 4.5, Gemini 3 Pro, Grok 4, and Qwen 3 Max, and GPT 5.1 Pro spends 48 hours in total yet passes only 38.4% of workflows, while Claude Sonnet 4.5 passes just 25.0%. Comprehensive case studies further surface the challenges that real-world enterprise workflows pose for AI agents.",
            "headline_zh": "提出Finch基准以评估AI代理在真实企业财务与会计工作流中的性能",
            "intro_zh": [
                "核心问题：现有AI评估缺乏真实企业级财务工作流基准，涉及多模态、长时程和协作任务",
                "方法要点：基于真实企业数据，结合LLM辅助发现与专家标注，构建172个复合工作流",
                "实验或效果：评估前沿AI系统，GPT 5.1 Pro仅通过38.4%工作流，凸显企业工作流对AI的挑战"
            ],
            "tags_zh": [
                "财务基准",
                "企业工作流",
                "多模态评估",
                "电子表格分析",
                "AI代理测试"
            ],
            "_index": 146
        },
        {
            "title": "SACn: Soft Actor-Critic with n-step Returns",
            "authors": [
                "Jakub Łyskawa",
                "Jakub Lewandowski",
                "Paweł Wawrzyński"
            ],
            "arxiv_id": "2512.13165v1",
            "summary": "Soft Actor-Critic (SAC) is widely used in practical applications and is now one of the most relevant off-policy online model-free reinforcement learning (RL) methods. The technique of n-step returns is known to increase the convergence speed of RL algorithms compared to their 1-step returns-based versions. However, SAC is notoriously difficult to combine with n-step returns, since their usual combination introduces bias in off-policy algorithms due to the changes in action distribution. While this problem is solved by importance sampling, a method for estimating expected values of one distribution using samples from another distribution, importance sampling may result in numerical instability. In this work, we combine SAC with n-step returns in a way that overcomes this issue. We present an approach to applying numerically stable importance sampling with simplified hyperparameter selection. Furthermore, we analyze the entropy estimation approach of Soft Actor-Critic in the context of the n-step maximum entropy framework and formulate the $τ$-sampled entropy estimation to reduce the variance of the learning target. Finally, we formulate the Soft Actor-Critic with n-step returns (SAC$n$) algorithm that we experimentally verify on MuJoCo simulated environments.",
            "headline_zh": "提出SACn算法，结合n步回报与稳定重要性采样，解决SAC在离策略强化学习中的偏差问题。",
            "intro_zh": [
                "核心问题：SAC结合n步回报时，因动作分布变化引入偏差，重要性采样可能导致数值不稳定。",
                "方法要点：采用数值稳定的重要性采样，简化超参数选择，并引入τ-采样熵估计以降低方差。",
                "实验或效果：在MuJoCo模拟环境中验证SACn算法，提升收敛速度与稳定性。"
            ],
            "tags_zh": [
                "强化学习",
                "离策略算法",
                "n步回报",
                "重要性采样",
                "熵估计",
                "MuJoCo环境"
            ],
            "_index": 147
        },
        {
            "title": "A Semantically Enhanced Generative Foundation Model Improves Pathological Image Synthesis",
            "authors": [
                "Xianchao Guan",
                "Zhiyuan Fan",
                "Yifeng Wang",
                "Fuqiang Chen",
                "Yanjiang Zhou",
                "Zengyang Che",
                "Hongxue Meng",
                "Xin Li",
                "Yaowei Wang",
                "Hongpeng Wang",
                "Min Zhang",
                "Heng Tao Shen",
                "Zheng Zhang",
                "Yongbing Zhang"
            ],
            "arxiv_id": "2512.13164v1",
            "summary": "The development of clinical-grade artificial intelligence in pathology is limited by the scarcity of diverse, high-quality annotated datasets. Generative models offer a potential solution but suffer from semantic instability and morphological hallucinations that compromise diagnostic reliability. To address this challenge, we introduce a Correlation-Regulated Alignment Framework for Tissue Synthesis (CRAFTS), the first generative foundation model for pathology-specific text-to-image synthesis. By leveraging a dual-stage training strategy on approximately 2.8 million image-caption pairs, CRAFTS incorporates a novel alignment mechanism that suppresses semantic drift to ensure biological accuracy. This model generates diverse pathological images spanning 30 cancer types, with quality rigorously validated by objective metrics and pathologist evaluations. Furthermore, CRAFTS-augmented datasets enhance the performance across various clinical tasks, including classification, cross-modal retrieval, self-supervised learning, and visual question answering. In addition, coupling CRAFTS with ControlNet enables precise control over tissue architecture from inputs such as nuclear segmentation masks and fluorescence images. By overcoming the critical barriers of data scarcity and privacy concerns, CRAFTS provides a limitless source of diverse, annotated histology data, effectively unlocking the creation of robust diagnostic tools for rare and complex cancer phenotypes.",
            "headline_zh": "提出CRAFTS生成基础模型以解决病理图像合成中的语义不稳定问题",
            "intro_zh": [
                "核心问题：病理AI发展受限于高质量标注数据稀缺，现有生成模型存在语义漂移和形态幻觉。",
                "方法要点：采用双阶段训练和相关性对齐机制，基于280万图像-文本对，确保生物准确性。",
                "实验或效果：生成30种癌症类型图像，通过客观指标和病理学家评估验证，增强多种临床任务性能。"
            ],
            "tags_zh": [
                "病理图像合成",
                "生成基础模型",
                "语义对齐",
                "数据增强",
                "临床AI"
            ],
            "_index": 148
        },
        {
            "title": "SpeakRL: Synergizing Reasoning, Speaking, and Acting in Language Models with Reinforcement Learning",
            "authors": [
                "Emre Can Acikgoz",
                "Jinoh Oh",
                "Jie Hao",
                "Joo Hyuk Jeon",
                "Heng Ji",
                "Dilek Hakkani-Tür",
                "Gokhan Tur",
                "Xiang Li",
                "Chengyuan Ma",
                "Xing Fan"
            ],
            "arxiv_id": "2512.13159v1",
            "summary": "Effective human-agent collaboration is increasingly prevalent in real-world applications. Current trends in such collaborations are predominantly unidirectional, with users providing instructions or posing questions to agents, where agents respond directly without seeking necessary clarifications or confirmations. However, the evolving capabilities of these agents require more proactive engagement, where agents should dynamically participate in conversations to clarify user intents, resolve ambiguities, and adapt to changing circumstances. Existing prior work under-utilize the conversational capabilities of language models (LMs), thereby optimizing agents as better followers rather than effective speakers. In this work, we introduce SpeakRL, a reinforcement learning (RL) method that enhances agents' conversational capabilities by rewarding proactive interactions with users, such as asking right clarification questions when necessary. To support this, we curate SpeakER, a synthetic dataset that includes diverse scenarios from task-oriented dialogues, where tasks are resolved through interactive clarification questions. We present a systematic analysis of reward design for conversational proactivity and propose a principled reward formulation for teaching agents to balance asking with acting. Empirical evaluations demonstrate that our approach achieves a 20.14% absolute improvement in task completion over base models without increasing conversation turns even surpassing even much larger proprietary models, demonstrating the promise of clarification-centric user-agent interactions.",
            "headline_zh": "提出SpeakRL强化学习方法，通过奖励主动交互提升语言模型在任务导向对话中的协作能力。",
            "intro_zh": [
                "核心问题：现有语言模型在协作中多为被动响应，缺乏主动澄清用户意图的能力。",
                "方法要点：使用强化学习奖励模型主动提问，平衡询问与行动，并构建SpeakER合成数据集支持训练。",
                "实验或效果：在任务完成率上比基础模型提升20.14%，对话轮次未增加，优于更大专有模型。"
            ],
            "tags_zh": [
                "强化学习",
                "任务导向对话",
                "主动交互",
                "语言模型优化",
                "合成数据集"
            ],
            "_index": 149
        },
        {
            "title": "Intrinsic Image Fusion for Multi-View 3D Material Reconstruction",
            "authors": [
                "Peter Kocsis",
                "Lukas Höllein",
                "Matthias Nießner"
            ],
            "arxiv_id": "2512.13157v1",
            "summary": "We introduce Intrinsic Image Fusion, a method that reconstructs high-quality physically based materials from multi-view images. Material reconstruction is highly underconstrained and typically relies on analysis-by-synthesis, which requires expensive and noisy path tracing. To better constrain the optimization, we incorporate single-view priors into the reconstruction process. We leverage a diffusion-based material estimator that produces multiple, but often inconsistent, candidate decompositions per view. To reduce the inconsistency, we fit an explicit low-dimensional parametric function to the predictions. We then propose a robust optimization framework using soft per-view prediction selection together with confidence-based soft multi-view inlier set to fuse the most consistent predictions of the most confident views into a consistent parametric material space. Finally, we use inverse path tracing to optimize for the low-dimensional parameters. Our results outperform state-of-the-art methods in material disentanglement on both synthetic and real scenes, producing sharp and clean reconstructions suitable for high-quality relighting.",
            "headline_zh": "提出内在图像融合方法，从多视角图像重建高质量物理材质",
            "intro_zh": [
                "核心问题：多视角材质重建高度欠约束，依赖昂贵且噪声的路径追踪分析合成方法",
                "方法要点：融合单视角先验，通过扩散模型生成候选分解，并优化低维参数函数以减少不一致性",
                "实验或效果：在合成和真实场景中优于现有方法，实现清晰材质解缠，适合高质量重光照"
            ],
            "tags_zh": [
                "多视角材质重建",
                "内在图像分解",
                "扩散模型",
                "参数优化",
                "路径追踪",
                "材质解缠"
            ],
            "_index": 150
        },
        {
            "title": "MAC: A Multi-Agent Framework for Interactive User Clarification in Multi-turn Conversations",
            "authors": [
                "Emre Can Acikgoz",
                "Jinoh Oh",
                "Joo Hyuk Jeon",
                "Jie Hao",
                "Heng Ji",
                "Dilek Hakkani-Tür",
                "Gokhan Tur",
                "Xiang Li",
                "Chengyuan Ma",
                "Xing Fan"
            ],
            "arxiv_id": "2512.13154v1",
            "summary": "Conversational agents often encounter ambiguous user requests, requiring an effective clarification to successfully complete tasks. While recent advancements in real-world applications favor multi-agent architectures to manage complex conversational scenarios efficiently, ambiguity resolution remains a critical and underexplored challenge--particularly due to the difficulty of determining which agent should initiate a clarification and how agents should coordinate their actions when faced with uncertain or incomplete user input. The fundamental questions of when to interrupt a user and how to formulate the optimal clarification query within the most optimal multi-agent settings remain open. In this paper, we propose MAC (Multi-Agent Clarification), an interactive multi-agent framework specifically optimized to resolve user ambiguities by strategically managing clarification dialogues. We first introduce a novel taxonomy categorizing user ambiguities to systematically guide clarification strategies. Then, we present MAC that autonomously coordinates multiple agents to interact synergistically with users. Empirical evaluations on MultiWOZ 2.4 demonstrate that enabling clarification at both levels increases task success rate 7.8\\% (54.5 to 62.3) and reduces the average number of dialogue turns (6.53 to 4.86) by eliciting all required user information up front and minimizing repetition. Our findings highlight the importance of active user interaction and role-aware clarification for more reliable human-agent communication.",
            "headline_zh": "提出多智能体框架MAC，通过协同交互解决多轮对话中用户请求的歧义问题。",
            "intro_zh": [
                "核心问题：多智能体对话中，如何协调智能体以最优方式发起和制定澄清查询，解决用户歧义。",
                "方法要点：引入歧义分类法，设计MAC框架，使多智能体协同管理澄清对话，主动交互用户。",
                "实验或效果：在MultiWOZ 2.4上评估，任务成功率提升7.8%，平均对话轮数减少，提高通信可靠性。"
            ],
            "tags_zh": [
                "多智能体对话",
                "歧义澄清",
                "交互框架",
                "任务导向对话",
                "协同协调"
            ],
            "_index": 151
        },
        {
            "title": "START: Traversing Sparse Footholds with Terrain Reconstruction",
            "authors": [
                "Ruiqi Yu",
                "Qianshi Wang",
                "Hongyi Li",
                "Zheng Jun",
                "Zhicheng Wang",
                "Jun Wu",
                "Qiuguo Zhu"
            ],
            "arxiv_id": "2512.13153v1",
            "summary": "Traversing terrains with sparse footholds like legged animals presents a promising yet challenging task for quadruped robots, as it requires precise environmental perception and agile control to secure safe foot placement while maintaining dynamic stability. Model-based hierarchical controllers excel in laboratory settings, but suffer from limited generalization and overly conservative behaviors. End-to-end learning-based approaches unlock greater flexibility and adaptability, but existing state-of-the-art methods either rely on heightmaps that introduce noise and complex, costly pipelines, or implicitly infer terrain features from egocentric depth images, often missing accurate critical geometric cues and leading to inefficient learning and rigid gaits. To overcome these limitations, we propose START, a single-stage learning framework that enables agile, stable locomotion on highly sparse and randomized footholds. START leverages only low-cost onboard vision and proprioception to accurately reconstruct local terrain heightmap, providing an explicit intermediate representation to convey essential features relevant to sparse foothold regions. This supports comprehensive environmental understanding and precise terrain assessment, reducing exploration cost and accelerating skill acquisition. Experimental results demonstrate that START achieves zero-shot transfer across diverse real-world scenarios, showcasing superior adaptability, precise foothold placement, and robust locomotion.",
            "headline_zh": "提出START单阶段学习框架，实现四足机器人在稀疏立足点地形上的敏捷稳定运动。",
            "intro_zh": [
                "核心问题：现有方法在稀疏立足点地形上存在泛化性差、感知噪声大或几何信息缺失，导致运动效率低和步态僵硬。",
                "方法要点：仅利用低成本机载视觉和本体感知，精确重建局部地形高度图，提供显式中间表示以支持环境理解和立足点评估。",
                "实验或效果：在多样化真实场景中实现零样本迁移，展示出优越的适应性、精确立足点放置和鲁棒运动能力。"
            ],
            "tags_zh": [
                "四足机器人运动控制",
                "稀疏立足点地形",
                "单阶段学习框架",
                "地形重建",
                "零样本迁移",
                "机载视觉感知"
            ],
            "_index": 152
        },
        {
            "title": "Enhancing Node-Level Graph Domain Adaptation by Alleviating Local Dependency",
            "authors": [
                "Xinwei Tai",
                "Dongmian Zou",
                "Hongfei Wang"
            ],
            "arxiv_id": "2512.13149v1",
            "summary": "Recent years have witnessed significant advancements in machine learning methods on graphs. However, transferring knowledge effectively from one graph to another remains a critical challenge. This highlights the need for algorithms capable of applying information extracted from a source graph to an unlabeled target graph, a task known as unsupervised graph domain adaptation (GDA). One key difficulty in unsupervised GDA is conditional shift, which hinders transferability. In this paper, we show that conditional shift can be observed only if there exists local dependencies among node features. To support this claim, we perform a rigorous analysis and also further provide generalization bounds of GDA when dependent node features are modeled using markov chains. Guided by the theoretical findings, we propose to improve GDA by decorrelating node features, which can be specifically implemented through decorrelated GCN layers and graph transformer layers. Our experimental results demonstrate the effectiveness of this approach, showing not only substantial performance enhancements over baseline GDA methods but also clear visualizations of small intra-class distances in the learned representations. Our code is available at https://github.com/TechnologyAiGroup/DFT",
            "headline_zh": "提出通过解相关节点特征以缓解局部依赖，提升无监督图域适应性能",
            "intro_zh": [
                "核心问题：无监督图域适应中，节点特征的局部依赖导致条件偏移，阻碍知识迁移。",
                "方法要点：理论分析表明条件偏移源于局部依赖，提出解相关GCN层和图Transformer层来消除依赖。",
                "实验或效果：实验显示方法优于基线，学习到的表示具有小的类内距离，代码已开源。"
            ],
            "tags_zh": [
                "图域适应",
                "节点特征解相关",
                "条件偏移",
                "图神经网络",
                "无监督学习",
                "图Transformer"
            ],
            "_index": 153
        },
        {
            "title": "StarryGazer: Leveraging Monocular Depth Estimation Models for Domain-Agnostic Single Depth Image Completion",
            "authors": [
                "Sangmin Hong",
                "Suyoung Lee",
                "Kyoung Mu Lee"
            ],
            "arxiv_id": "2512.13147v1",
            "summary": "The problem of depth completion involves predicting a dense depth image from a single sparse depth map and an RGB image. Unsupervised depth completion methods have been proposed for various datasets where ground truth depth data is unavailable and supervised methods cannot be applied. However, these models require auxiliary data to estimate depth values, which is far from real scenarios. Monocular depth estimation (MDE) models can produce a plausible relative depth map from a single image, but there is no work to properly combine the sparse depth map with MDE for depth completion; a simple affine transformation to the depth map will yield a high error since MDE are inaccurate at estimating depth difference between objects. We introduce StarryGazer, a domain-agnostic framework that predicts dense depth images from a single sparse depth image and an RGB image without relying on ground-truth depth by leveraging the power of large MDE models. First, we employ a pre-trained MDE model to produce relative depth images. These images are segmented and randomly rescaled to form synthetic pairs for dense pseudo-ground truth and corresponding sparse depths. A refinement network is trained with the synthetic pairs, incorporating the relative depth maps and RGB images to improve the model's accuracy and robustness. StarryGazer shows superior results over existing unsupervised methods and transformed MDE results on various datasets, demonstrating that our framework exploits the power of MDE models while appropriately fixing errors using sparse depth information.",
            "headline_zh": "提出StarryGazer框架，利用单目深度估计模型实现领域无关的单深度图像补全",
            "intro_zh": [
                "核心问题：现有无监督深度补全方法依赖辅助数据，单目深度估计模型无法直接结合稀疏深度图",
                "方法要点：使用预训练单目深度估计模型生成相对深度图，通过分割和随机缩放合成伪真值对训练细化网络",
                "实验或效果：在多个数据集上优于现有无监督方法，有效利用单目深度估计模型并修正误差"
            ],
            "tags_zh": [
                "深度补全",
                "单目深度估计",
                "无监督学习",
                "领域无关",
                "稀疏深度图",
                "图像合成"
            ],
            "_index": 154
        },
        {
            "title": "Weight Space Correlation Analysis: Quantifying Feature Utilization in Deep Learning Models",
            "authors": [
                "Chun Kit Wong",
                "Paraskevas Pegios",
                "Nina Weng",
                "Emilie Pi Fogtmann Sejer",
                "Martin Grønnebæk Tolsgaard",
                "Anders Nymark Christensen",
                "Aasa Feragen"
            ],
            "arxiv_id": "2512.13144v1",
            "summary": "Deep learning models in medical imaging are susceptible to shortcut learning, relying on confounding metadata (e.g., scanner model) that is often encoded in image embeddings. The crucial question is whether the model actively utilizes this encoded information for its final prediction. We introduce Weight Space Correlation Analysis, an interpretable methodology that quantifies feature utilization by measuring the alignment between the classification heads of a primary clinical task and auxiliary metadata tasks. We first validate our method by successfully detecting artificially induced shortcut learning. We then apply it to probe the feature utilization of an SA-SonoNet model trained for Spontaneous Preterm Birth (sPTB) prediction. Our analysis confirmed that while the embeddings contain substantial metadata, the sPTB classifier's weight vectors were highly correlated with clinically relevant factors (e.g., birth weight) but decoupled from clinically irrelevant acquisition factors (e.g. scanner). Our methodology provides a tool to verify model trustworthiness, demonstrating that, in the absence of induced bias, the clinical model selectively utilizes features related to the genuine clinical signal.",
            "headline_zh": "提出权重空间相关性分析以量化医学影像深度学习模型的特征利用，验证模型可信度。",
            "intro_zh": [
                "核心问题：医学影像模型易受捷径学习影响，依赖混杂元数据，需判断其是否主动用于预测。",
                "方法要点：通过测量主临床任务与辅助元数据任务分类头之间的对齐，量化特征利用。",
                "实验或效果：验证方法检测人工诱导捷径学习，应用于早产预测模型，确认其选择性利用临床相关特征。"
            ],
            "tags_zh": [
                "医学影像分析",
                "捷径学习检测",
                "特征利用量化",
                "模型可信度验证",
                "权重空间相关性"
            ],
            "_index": 155
        },
        {
            "title": "Can AI Understand What We Cannot Say? Measuring Multilevel Alignment Through Abortion Stigma Across Cognitive, Interpersonal, and Structural Levels",
            "authors": [
                "Anika Sharma",
                "Malavika Mampally",
                "Chidaksh Ravuru",
                "Kandyce Brennan",
                "Neil Gaikwad"
            ],
            "arxiv_id": "2512.13142v1",
            "summary": "As large language models increasingly mediate stigmatized health decisions, their capacity to genuinely understand complex psychological and physiological phenomena remains poorly evaluated. Can AI understand what we cannot say? We investigate whether LLMs coherently represent abortion stigma across the cognitive, interpersonal, and structural levels where it operates. We systematically tested 627 demographically diverse personas across five leading LLMs using the validated Individual Level Abortion Stigma Scale (ILAS). Our multilevel analysis examined whether models coherently represent stigma at the cognitive level (self-judgment), interpersonal level (anticipated judgment and isolation), and structural level (community condemnation and disclosure patterns), as well as overall stigma. Models fail tests of genuine understanding across all levels. They overestimate interpersonal stigma while underestimating cognitive stigma, assume uniform community condemnation, introduce demographic biases absent from human validation data, miss the empirically validated stigma-secrecy relationship, and contradict themselves within theoretical constructs. These patterns reveal that current alignment approaches ensure appropriate language but not coherent multilevel understanding. This work provides empirical evidence that current LLMs lack coherent multilevel understanding of psychological and physiological constructs. AI safety in high-stakes contexts demands new approaches to design (multilevel coherence), evaluation (continuous auditing), governance and regulation (mandatory audits, accountability, deployment restrictions), and AI literacy in domains where understanding what people cannot say determines whether support helps or harms.",
            "headline_zh": "评估大语言模型对堕胎耻辱的多层次理解，揭示其缺乏连贯心理生理认知",
            "intro_zh": [
                "核心问题：大语言模型能否真实理解复杂心理生理现象如堕胎耻辱，在认知、人际和结构层次上保持连贯性",
                "方法要点：使用验证过的个体水平堕胎耻辱量表，系统测试五个领先模型在627个多样化人物角色上的表现",
                "实验或效果：模型在所有层次上均未通过真实理解测试，表现出偏差、矛盾和不一致，强调需新设计、评估和治理方法"
            ],
            "tags_zh": [
                "大语言模型评估",
                "堕胎耻辱",
                "多层次对齐",
                "心理生理理解",
                "AI安全",
                "偏见检测"
            ],
            "_index": 156
        },
        {
            "title": "Towards Unified Co-Speech Gesture Generation via Hierarchical Implicit Periodicity Learning",
            "authors": [
                "Xin Guo",
                "Yifan Zhao",
                "Jia Li"
            ],
            "arxiv_id": "2512.13131v1",
            "summary": "Generating 3D-based body movements from speech shows great potential in extensive downstream applications, while it still suffers challenges in imitating realistic human movements. Predominant research efforts focus on end-to-end generation schemes to generate co-speech gestures, spanning GANs, VQ-VAE, and recent diffusion models. As an ill-posed problem, in this paper, we argue that these prevailing learning schemes fail to model crucial inter- and intra-correlations across different motion units, i.e. head, body, and hands, thus leading to unnatural movements and poor coordination. To delve into these intrinsic correlations, we propose a unified Hierarchical Implicit Periodicity (HIP) learning approach for audio-inspired 3D gesture generation. Different from predominant research, our approach models this multi-modal implicit relationship by two explicit technique insights: i) To disentangle the complicated gesture movements, we first explore the gesture motion phase manifolds with periodic autoencoders to imitate human natures from realistic distributions while incorporating non-period ones from current latent states for instance-level diversities. ii) To model the hierarchical relationship of face motions, body gestures, and hand movements, driving the animation with cascaded guidance during learning. We exhibit our proposed approach on 3D avatars and extensive experiments show our method outperforms the state-of-the-art co-speech gesture generation methods by both quantitative and qualitative evaluations. Code and models will be publicly available.",
            "headline_zh": "提出分层隐式周期性学习以统一生成语音驱动的3D手势，解决运动单元间协调性问题。",
            "intro_zh": [
                "核心问题：现有端到端方法难以建模头、身体和手部运动单元间的内在关联，导致不自然运动。",
                "方法要点：通过周期性自编码器探索手势运动相位流形，结合级联引导建模分层关系，增强多样性和协调性。",
                "实验或效果：在3D虚拟人上验证，定量和定性评估均优于当前最优方法，代码和模型将公开。"
            ],
            "tags_zh": [
                "语音驱动手势生成",
                "分层隐式周期性学习",
                "3D运动生成",
                "周期性自编码器",
                "多模态协调"
            ],
            "_index": 157
        },
        {
            "title": "LeafTrackNet: A Deep Learning Framework for Robust Leaf Tracking in Top-Down Plant Phenotyping",
            "authors": [
                "Shanghua Liu",
                "Majharulislam Babor",
                "Christoph Verduyn",
                "Breght Vandenberghe",
                "Bruno Betoni Parodi",
                "Cornelia Weltzien",
                "Marina M. -C. Höhne"
            ],
            "arxiv_id": "2512.13130v1",
            "summary": "High resolution phenotyping at the level of individual leaves offers fine-grained insights into plant development and stress responses. However, the full potential of accurate leaf tracking over time remains largely unexplored due to the absence of robust tracking methods-particularly for structurally complex crops such as canola. Existing plant-specific tracking methods are typically limited to small-scale species or rely on constrained imaging conditions. In contrast, generic multi-object tracking (MOT) methods are not designed for dynamic biological scenes. Progress in the development of accurate leaf tracking models has also been hindered by a lack of large-scale datasets captured under realistic conditions. In this work, we introduce CanolaTrack, a new benchmark dataset comprising 5,704 RGB images with 31,840 annotated leaf instances spanning the early growth stages of 184 canola plants. To enable accurate leaf tracking over time, we introduce LeafTrackNet, an efficient framework that combines a YOLOv10-based leaf detector with a MobileNetV3-based embedding network. During inference, leaf identities are maintained over time through an embedding-based memory association strategy. LeafTrackNet outperforms both plant-specific trackers and state-of-the-art MOT baselines, achieving a 9% HOTA improvement on CanolaTrack. With our work we provide a new standard for leaf-level tracking under realistic conditions and we provide CanolaTrack - the largest dataset for leaf tracking in agriculture crops, which will contribute to future research in plant phenotyping. Our code and dataset are publicly available at https://github.com/shl-shawn/LeafTrackNet.",
            "headline_zh": "提出LeafTrackNet框架以解决复杂作物叶片在真实条件下的鲁棒跟踪问题",
            "intro_zh": [
                "核心问题：缺乏鲁棒方法跟踪复杂作物叶片，现有方法受限或不适于动态生物场景",
                "方法要点：结合YOLOv10检测器和MobileNetV3嵌入网络，采用基于嵌入的记忆关联策略",
                "实验或效果：在CanolaTrack数据集上优于植物专用跟踪器和MOT基线，HOTA提升9%"
            ],
            "tags_zh": [
                "叶片跟踪",
                "深度学习框架",
                "植物表型分析",
                "多目标跟踪",
                "数据集构建"
            ],
            "_index": 158
        },
        {
            "title": "Quanvolutional Neural Networks for Spectrum Peak-Finding",
            "authors": [
                "Lukas Bischof",
                "Rudolf M. Füchslin",
                "Kurt Stockinger",
                "Pavel Sulimov"
            ],
            "arxiv_id": "2512.13125v1",
            "summary": "The analysis of spectra, such as Nuclear Magnetic Resonance (NMR) spectra, for the comprehensive characterization of peaks is a challenging task for both experts and machines, especially with complex molecules. This process, also known as deconvolution, involves identifying and quantifying the peaks in the spectrum. Machine learning techniques have shown promising results in automating this process. With the advent of quantum computing, there is potential to further enhance these techniques. In this work, inspired by the success of classical Convolutional Neural Networks (CNNs), we explore the use of Quanvolutional Neural Networks (QuanvNNs) for the multi-task peak finding problem, involving both peak counting and position estimation. We implement a simple and interpretable QuanvNN architecture that can be directly compared to its classical CNN counterpart, and evaluate its performance on a synthetic NMR-inspired dataset. Our results demonstrate that QuanvNNs outperform classical CNNs on challenging spectra, achieving an 11\\% improvement in F1 score and a 30\\% reduction in mean absolute error for peak position estimation. Additionally, QuanvNNs appear to exhibit better convergence stability for harder problems.",
            "headline_zh": "提出量子卷积神经网络用于光谱峰值查找，在合成NMR数据集上优于经典CNN。",
            "intro_zh": [
                "核心问题：光谱（如NMR）峰值查找与定位是复杂分子分析中的挑战性任务。",
                "方法要点：设计可解释的量子卷积神经网络架构，直接与经典CNN对比。",
                "实验或效果：在困难光谱上，F1分数提升11%，峰值位置估计平均绝对误差降低30%。"
            ],
            "tags_zh": [
                "量子卷积神经网络",
                "光谱分析",
                "峰值查找",
                "NMR光谱",
                "量子机器学习"
            ],
            "_index": 159
        },
        {
            "title": "Stopping Rules for Stochastic Gradient Descent via Anytime-Valid Confidence Sequences",
            "authors": [
                "Liviu Aolaritei",
                "Michael I. Jordan"
            ],
            "arxiv_id": "2512.13123v1",
            "summary": "We study stopping rules for stochastic gradient descent (SGD) for convex optimization from the perspective of anytime-valid confidence sequences. Classical analyses of SGD provide convergence guarantees in expectation or at a fixed horizon, but offer no statistically valid way to assess, at an arbitrary time, how close the current iterate is to the optimum. We develop an anytime-valid, data-dependent upper confidence sequence for the weighted average suboptimality of projected SGD, constructed via nonnegative supermartingales and requiring no smoothness or strong convexity. This confidence sequence yields a simple stopping rule that is provably $\\varepsilon$-optimal with probability at least $1-α$ and is almost surely finite under standard stochastic approximation stepsizes. To the best of our knowledge, these are the first rigorous, time-uniform performance guarantees and finite-time $\\varepsilon$-optimality certificates for projected SGD with general convex objectives, based solely on observable trajectory quantities.",
            "headline_zh": "提出基于任意时间有效置信序列的随机梯度下降停止规则，用于凸优化",
            "intro_zh": [
                "核心问题：传统SGD分析缺乏在任意时间评估当前迭代接近最优解的统计有效方法",
                "方法要点：通过非负超鞅构建投影SGD加权平均次优性的任意时间有效置信序列，无需平滑性或强凸性",
                "实验或效果：停止规则在概率至少1-α下证明为ε-最优，且在标准步长下几乎必然有限"
            ],
            "tags_zh": [
                "随机梯度下降",
                "凸优化",
                "停止规则",
                "置信序列",
                "任意时间有效性",
                "非负超鞅"
            ],
            "_index": 160
        },
        {
            "title": "DePT3R: Joint Dense Point Tracking and 3D Reconstruction of Dynamic Scenes in a Single Forward Pass",
            "authors": [
                "Vivek Alumootil",
                "Tuan-Anh Vu",
                "M. Khalid Jawed"
            ],
            "arxiv_id": "2512.13122v1",
            "summary": "Current methods for dense 3D point tracking in dynamic scenes typically rely on pairwise processing, require known camera poses, or assume a temporal ordering to input frames, constraining their flexibility and applicability. Additionally, recent advances have successfully enabled efficient 3D reconstruction from large-scale, unposed image collections, underscoring opportunities for unified approaches to dynamic scene understanding. Motivated by this, we propose DePT3R, a novel framework that simultaneously performs dense point tracking and 3D reconstruction of dynamic scenes from multiple images in a single forward pass. This multi-task learning is achieved by extracting deep spatio-temporal features with a powerful backbone and regressing pixel-wise maps with dense prediction heads. Crucially, DePT3R operates without requiring camera poses, substantially enhancing its adaptability and efficiency-especially important in dynamic environments with rapid changes. We validate DePT3R on several challenging benchmarks involving dynamic scenes, demonstrating strong performance and significant improvements in memory efficiency over existing state-of-the-art methods. Data and codes are available via the open repository: https://github.com/StructuresComp/DePT3R",
            "headline_zh": "提出DePT3R框架，在单次前向传播中联合实现动态场景的密集点跟踪与3D重建。",
            "intro_zh": [
                "核心问题：现有动态场景密集点跟踪方法依赖成对处理、已知相机位姿或时序假设，限制灵活性。",
                "方法要点：通过强大骨干网络提取时空特征，使用密集预测头回归像素级映射，无需相机位姿。",
                "实验或效果：在动态场景基准测试中表现优异，内存效率显著提升，代码开源。"
            ],
            "tags_zh": [
                "动态场景理解",
                "密集点跟踪",
                "3D重建",
                "多任务学习",
                "无相机位姿",
                "单次前向传播"
            ],
            "_index": 161
        },
        {
            "title": "Towards Practical Large-scale Dynamical Heterogeneous Graph Embedding: Cold-start Resilient Recommendation",
            "authors": [
                "Mabiao Long",
                "Jiaxi Liu",
                "Yufeng Li",
                "Hao Xiong",
                "Junchi Yan",
                "Kefan Wang",
                "Yi Cao",
                "Jiandong Ding"
            ],
            "arxiv_id": "2512.13120v1",
            "summary": "Deploying dynamic heterogeneous graph embeddings in production faces key challenges of scalability, data freshness, and cold-start. This paper introduces a practical, two-stage solution that balances deep graph representation with low-latency incremental updates. Our framework combines HetSGFormer, a scalable graph transformer for static learning, with Incremental Locally Linear Embedding (ILLE), a lightweight, CPU-based algorithm for real-time updates. HetSGFormer captures global structure with linear scalability, while ILLE provides rapid, targeted updates to incorporate new data, thus avoiding costly full retraining. This dual approach is cold-start resilient, leveraging the graph to create meaningful embeddings from sparse data. On billion-scale graphs, A/B tests show HetSGFormer achieved up to a 6.11% lift in Advertiser Value over previous methods, while the ILLE module added another 3.22% lift and improved embedding refresh timeliness by 83.2%. Our work provides a validated framework for deploying dynamic graph learning in production environments.",
            "headline_zh": "提出两阶段动态异构图嵌入框架，以解决大规模生产环境中的可扩展性、数据新鲜度和冷启动问题。",
            "intro_zh": [
                "核心问题：动态异构图嵌入在生产部署中面临可扩展性、数据新鲜度和冷启动挑战。",
                "方法要点：结合HetSGFormer进行静态全局学习，使用ILLE进行轻量级实时增量更新，避免全图重训练。",
                "实验或效果：在十亿级图上，A/B测试显示HetSGFormer提升广告价值6.11%，ILLE额外提升3.22%，刷新时效性提高83.2%。"
            ],
            "tags_zh": [
                "动态异构图嵌入",
                "冷启动推荐",
                "增量学习",
                "图变换器",
                "实时更新",
                "生产部署"
            ],
            "_index": 162
        },
        {
            "title": "From Overfitting to Reliability: Introducing the Hierarchical Approximate Bayesian Neural Network",
            "authors": [
                "Hayk Amirkhanian",
                "Marco F. Huber"
            ],
            "arxiv_id": "2512.13111v1",
            "summary": "In recent years, neural networks have revolutionized various domains, yet challenges such as hyperparameter tuning and overfitting remain significant hurdles. Bayesian neural networks offer a framework to address these challenges by incorporating uncertainty directly into the model, yielding more reliable predictions, particularly for out-of-distribution data. This paper presents Hierarchical Approximate Bayesian Neural Network, a novel approach that uses a Gaussian-inverse-Wishart distribution as a hyperprior of the network's weights to increase both the robustness and performance of the model. We provide analytical representations for the predictive distribution and weight posterior, which amount to the calculation of the parameters of Student's t-distributions in closed form with linear complexity with respect to the number of weights. Our method demonstrates robust performance, effectively addressing issues of overfitting and providing reliable uncertainty estimates, particularly for out-of-distribution tasks. Experimental results indicate that HABNN not only matches but often outperforms state-of-the-art models, suggesting a promising direction for future applications in safety-critical environments.",
            "headline_zh": "提出分层近似贝叶斯神经网络以解决过拟合和不确定性估计问题，适用于安全关键环境。",
            "intro_zh": [
                "核心问题：神经网络存在超参数调优和过拟合挑战，影响模型可靠性。",
                "方法要点：使用高斯-逆Wishart分布作为权重超先验，提供闭式预测分布和权重后验，计算复杂度线性。",
                "实验或效果：模型在分布外任务中表现稳健，性能常优于先进模型，提供可靠不确定性估计。"
            ],
            "tags_zh": [
                "贝叶斯神经网络",
                "过拟合缓解",
                "不确定性估计",
                "分层先验",
                "闭式计算",
                "安全关键应用"
            ],
            "_index": 163
        },
        {
            "title": "Uncovering the Role of Initial Saliency in U-Shaped Attention Bias: Scaling Initial Token Weight for Enhanced Long-Text Processing",
            "authors": [
                "Zewen Qiang",
                "Sendong Zhao",
                "Haochun Wang",
                "Bing Qin",
                "Ting Liu"
            ],
            "arxiv_id": "2512.13109v1",
            "summary": "Large language models (LLMs) have demonstrated strong performance on a variety of natural language processing (NLP) tasks. However, they often struggle with long-text sequences due to the ``lost in the middle'' phenomenon. This issue has been shown to arise from a U-shaped attention bias, where attention is disproportionately focused on the beginning and end of a text, leaving the middle section underrepresented. While previous studies have attributed this bias to position encoding, our research first identifies an additional factor: initial saliency. It means that in the attention computation for each token, tokens with higher attention weights relative to the initial token tend to receive more attention in the prediction of the next token. We further find that utilizing this property by scaling attention weight between the initial token and others improves the model's ability to process long contexts, achieving a maximum improvement of 3.6\\% in MDQA dataset. Moreover, combining this approach with existing methods to reduce position encoding bias further enhances performance, achieving a maximum improvement of 3.4\\% in KV-Retrieval tasks.",
            "headline_zh": "提出缩放初始令牌权重以缓解U形注意力偏差，增强大语言模型的长文本处理能力",
            "intro_zh": [
                "核心问题：大语言模型在长文本处理中存在'中间丢失'现象，源于U形注意力偏差，即注意力过度集中于文本首尾。",
                "方法要点：研究发现初始显著性因素，通过缩放初始令牌与其他令牌的注意力权重来改善偏差。",
                "实验或效果：在MDQA数据集上最高提升3.6%，结合现有方法在KV-Retrieval任务中最高提升3.4%。"
            ],
            "tags_zh": [
                "长文本处理",
                "注意力机制",
                "U形偏差",
                "初始显著性",
                "大语言模型优化"
            ],
            "_index": 164
        },
        {
            "title": "Diffusion-Based Restoration for Multi-Modal 3D Object Detection in Adverse Weather",
            "authors": [
                "Zhijian He",
                "Feifei Liu",
                "Yuwei Li",
                "Zhanpeng Liu",
                "Jintao Cheng",
                "Xieyuanli Chen",
                "Xiaoyu Tang"
            ],
            "arxiv_id": "2512.13107v1",
            "summary": "Multi-modal 3D object detection is important for reliable perception in robotics and autonomous driving. However, its effectiveness remains limited under adverse weather conditions due to weather-induced distortions and misalignment between different data modalities. In this work, we propose DiffFusion, a novel framework designed to enhance robustness in challenging weather through diffusion-based restoration and adaptive cross-modal fusion. Our key insight is that diffusion models possess strong capabilities for denoising and generating data that can adapt to various weather conditions. Building on this, DiffFusion introduces Diffusion-IR restoring images degraded by weather effects and Point Cloud Restoration (PCR) compensating for corrupted LiDAR data using image object cues. To tackle misalignments between two modalities, we develop Bidirectional Adaptive Fusion and Alignment Module (BAFAM). It enables dynamic multi-modal fusion and bidirectional bird's-eye view (BEV) alignment to maintain consistent spatial correspondence. Extensive experiments on three public datasets show that DiffFusion achieves state-of-the-art robustness under adverse weather while preserving strong clean-data performance. Zero-shot results on the real-world DENSE dataset further validate its generalization. The implementation of our DiffFusion will be released as open-source.",
            "headline_zh": "提出DiffFusion框架，通过扩散模型恢复和自适应跨模态融合，增强恶劣天气下多模态3D目标检测的鲁棒性。",
            "intro_zh": [
                "核心问题：恶劣天气导致多模态数据失真和模态间错位，限制3D目标检测的可靠性。",
                "方法要点：使用Diffusion-IR恢复图像，PCR补偿LiDAR数据，BAFAM模块实现动态融合和BEV对齐。",
                "实验或效果：在三个公开数据集上实现最先进的鲁棒性，并在DENSE数据集上验证零样本泛化能力。"
            ],
            "tags_zh": [
                "多模态3D目标检测",
                "扩散模型",
                "恶劣天气鲁棒性",
                "跨模态融合",
                "BEV对齐",
                "零样本泛化"
            ],
            "_index": 165
        },
        {
            "title": "TraPO: A Semi-Supervised Reinforcement Learning Framework for Boosting LLM Reasoning",
            "authors": [
                "Shenzhi Yang",
                "Guangcheng Zhu",
                "Xing Zheng",
                "Yingfan MA",
                "Zhongqi Chen",
                "Bowen Song",
                "Weiqiang Wang",
                "Junbo Zhao",
                "Gang Chen",
                "Haobo Wang"
            ],
            "arxiv_id": "2512.13106v1",
            "summary": "Reinforcement learning with verifiable rewards (RLVR) has proven effective in training large reasoning models (LRMs) by leveraging answer-verifiable signals to guide policy optimization, which, however, suffers from high annotation costs. To alleviate this problem, recent work has explored unsupervised RLVR methods that derive rewards solely from the model's internal consistency, such as through entropy and majority voting. While seemingly promising, these methods often suffer from model collapse in the later stages of training, which may arise from the reinforcement of incorrect reasoning patterns in the absence of external supervision. In this work, we investigate a novel semi-supervised RLVR paradigm that utilizes a small labeled set to guide RLVR training on unlabeled samples. Our key insight is that supervised rewards are essential for stabilizing consistency-based training on unlabeled samples, ensuring that only reasoning patterns verified on labeled instances are incorporated into RL training. Technically, we propose an effective policy optimization algorithm, TraPO, that identifies reliable unlabeled samples by matching their learning trajectory similarity to labeled ones. Building on this, TraPO achieves remarkable data efficiency and strong generalization on six widely used mathematical reasoning benchmarks (AIME24/25, AMC, MATH-500, Minerva, and Olympiad) and three out-of-distribution tasks (ARC-c, GPQA-diamond, and MMLU-pro). With only 1K labeled and 3K unlabeled samples, TraPO reaches 42.6% average accuracy, surpassing the best unsupervised method trained on 45K unlabeled samples (38.3%). Notably, when using 4K labeled and 12K unlabeled samples, TraPO even outperforms the fully supervised model trained on the full 45K labeled samples on all benchmarks, while using only 10% of the labeled data. The code is available via https://github.com/ShenzhiYang2000/TRAPO.",
            "headline_zh": "提出TraPO半监督强化学习框架，以少量标注样本提升大语言模型推理能力",
            "intro_zh": [
                "核心问题：无监督RLVR方法在训练后期易发生模型崩溃，源于缺乏外部监督强化错误推理模式",
                "方法要点：利用小规模标注集引导无标注样本的RLVR训练，通过轨迹相似性匹配识别可靠样本",
                "实验或效果：在六个数学推理基准和三个分布外任务上实现高数据效率和强泛化，仅用10%标注数据超越全监督模型"
            ],
            "tags_zh": [
                "半监督强化学习",
                "大语言模型推理",
                "轨迹相似性匹配",
                "数学推理基准",
                "数据效率优化",
                "泛化能力提升"
            ],
            "_index": 166
        },
        {
            "title": "FID-Net: A Feature-Enhanced Deep Learning Network for Forest Infestation Detection",
            "authors": [
                "Yan Zhang",
                "Baoxin Li",
                "Han Sun",
                "Yuhang Gao",
                "Mingtai Zhang",
                "Pei Wang"
            ],
            "arxiv_id": "2512.13104v1",
            "summary": "Forest pests threaten ecosystem stability, requiring efficient monitoring. To overcome the limitations of traditional methods in large-scale, fine-grained detection, this study focuses on accurately identifying infected trees and analyzing infestation patterns. We propose FID-Net, a deep learning model that detects pest-affected trees from UAV visible-light imagery and enables infestation analysis via three spatial metrics. Based on YOLOv8n, FID-Net introduces a lightweight Feature Enhancement Module (FEM) to extract disease-sensitive cues, an Adaptive Multi-scale Feature Fusion Module (AMFM) to align and fuse dual-branch features (RGB and FEM-enhanced), and an Efficient Channel Attention (ECA) mechanism to enhance discriminative information efficiently. From detection results, we construct a pest situation analysis framework using: (1) Kernel Density Estimation to locate infection hotspots; (2) neighborhood evaluation to assess healthy trees' infection risk; (3) DBSCAN clustering to identify high-density healthy clusters as priority protection zones. Experiments on UAV imagery from 32 forest plots in eastern Tianshan, China, show that FID-Net achieves 86.10% precision, 75.44% recall, 82.29% mAP@0.5, and 64.30% mAP@0.5:0.95, outperforming mainstream YOLO models. Analysis confirms infected trees exhibit clear clustering, supporting targeted forest protection. FID-Net enables accurate tree health discrimination and, combined with spatial metrics, provides reliable data for intelligent pest monitoring, early warning, and precise management.",
            "headline_zh": "提出FID-Net以解决无人机可见光图像中森林病虫害树木的检测与空间分析问题",
            "intro_zh": [
                "核心问题：传统方法在大规模、细粒度森林病虫害检测中存在局限性，需高效监测以维护生态系统稳定。",
                "方法要点：基于YOLOv8n，引入特征增强模块、自适应多尺度特征融合模块和高效通道注意力机制，提升检测精度。",
                "实验或效果：在32个森林样地数据集上，FID-Net在精度、召回率和mAP指标上优于主流YOLO模型，并支持空间分析。"
            ],
            "tags_zh": [
                "森林病虫害检测",
                "无人机图像分析",
                "深度学习模型",
                "特征增强",
                "空间分析",
                "YOLOv8"
            ],
            "_index": 167
        },
        {
            "title": "Socratic Students: Teaching Language Models to Learn by Asking Questions",
            "authors": [
                "Rajeev Bhatt Ambati",
                "Tianyi Niu",
                "Aashu Singh",
                "Shlok Mishra",
                "Shashank Srivastava",
                "Snigdha Chaturvedi"
            ],
            "arxiv_id": "2512.13102v1",
            "summary": "Large Language Models (LLMs) excel at static interactions, where they answer user queries by retrieving knowledge encoded in their parameters. However, in many real-world settings, such as educational tutoring or medical assistance, relevant information is not directly available and must be actively acquired through dynamic interactions. An interactive agent would recognize its own uncertainty, ask targeted questions, and retain new knowledge efficiently. Prior work has primarily explored effective ways for a teacher to instruct the student, where the teacher identifies student gaps and provides guidance. In this work, we shift the focus to the student and investigate effective strategies to actively query the teacher in seeking useful information. Across math and coding benchmarks, where baseline student models begin with near-zero performance, we show that student-led approaches consistently yield absolute Pass@k improvements of at least 0.5 over static baselines. To improve question quality, we train students using Direct Preference Optimization (DPO) with guidance from either self or stronger students. We find that this guided training enables smaller models to learn how to ask better questions, further enhancing learning efficiency.",
            "headline_zh": "提出学生主导提问方法以提升语言模型在动态交互中的学习效率",
            "intro_zh": [
                "核心问题：语言模型在静态交互中表现优异，但在需要主动获取信息的动态场景（如教育辅导）中效率不足",
                "方法要点：通过学生模型主动向教师提问，并利用直接偏好优化训练提升问题质量",
                "实验或效果：在数学和编码基准测试中，学生主导方法相比静态基线带来至少0.5的绝对Pass@k提升"
            ],
            "tags_zh": [
                "语言模型学习",
                "动态交互",
                "提问策略",
                "直接偏好优化",
                "教育辅导"
            ],
            "_index": 168
        },
        {
            "title": "Harmonizing Generalization and Specialization: Uncertainty-Informed Collaborative Learning for Semi-supervised Medical Image Segmentation",
            "authors": [
                "Wenjing Lu",
                "Yi Hong",
                "Yang Yang"
            ],
            "arxiv_id": "2512.13101v1",
            "summary": "Vision foundation models have demonstrated strong generalization in medical image segmentation by leveraging large-scale, heterogeneous pretraining. However, they often struggle to generalize to specialized clinical tasks under limited annotations or rare pathological variations, due to a mismatch between general priors and task-specific requirements. To address this, we propose Uncertainty-informed Collaborative Learning (UnCoL), a dual-teacher framework that harmonizes generalization and specialization in semi-supervised medical image segmentation. Specifically, UnCoL distills both visual and semantic representations from a frozen foundation model to transfer general knowledge, while concurrently maintaining a progressively adapting teacher to capture fine-grained and task-specific representations. To balance guidance from both teachers, pseudo-label learning in UnCoL is adaptively regulated by predictive uncertainty, which selectively suppresses unreliable supervision and stabilizes learning in ambiguous regions. Experiments on diverse 2D and 3D segmentation benchmarks show that UnCoL consistently outperforms state-of-the-art semi-supervised methods and foundation model baselines. Moreover, our model delivers near fully supervised performance with markedly reduced annotation requirements.",
            "headline_zh": "提出不确定性协同学习框架以解决半监督医学图像分割中泛化与特化不平衡问题",
            "intro_zh": [
                "核心问题：视觉基础模型在有限标注或罕见病理变化下泛化不足，因通用先验与任务需求不匹配",
                "方法要点：采用双教师框架，结合冻结基础模型和自适应教师，通过不确定性调节伪标签学习",
                "实验或效果：在多种2D和3D分割基准上优于现有方法，显著减少标注需求接近全监督性能"
            ],
            "tags_zh": [
                "半监督学习",
                "医学图像分割",
                "不确定性估计",
                "知识蒸馏",
                "双教师框架",
                "伪标签学习"
            ],
            "_index": 169
        },
        {
            "title": "OXE-AugE: A Large-Scale Robot Augmentation of OXE for Scaling Cross-Embodiment Policy Learning",
            "authors": [
                "Guanhua Ji",
                "Harsha Polavaram",
                "Lawrence Yunliang Chen",
                "Sandeep Bajamahal",
                "Zehan Ma",
                "Simeon Adebola",
                "Chenfeng Xu",
                "Ken Goldberg"
            ],
            "arxiv_id": "2512.13100v1",
            "summary": "Large and diverse datasets are needed for training generalist robot policies that have potential to control a variety of robot embodiments -- robot arm and gripper combinations -- across diverse tasks and environments. As re-collecting demonstrations and retraining for each new hardware platform are prohibitively costly, we show that existing robot data can be augmented for transfer and generalization. The Open X-Embodiment (OXE) dataset, which aggregates demonstrations from over 60 robot datasets, has been widely used as the foundation for training generalist policies. However, it is highly imbalanced: the top four robot types account for over 85\\% of its real data, which risks overfitting to robot--scene combinations. We present AugE-Toolkit, a scalable robot augmentation pipeline, and OXE-AugE, a high-quality open-source dataset that augments OXE with 9 different robot embodiments. OXE-AugE provides over 4.4 million trajectories, more than triple the size of the original OXE. We conduct a systematic study of how scaling robot augmentation impacts cross-embodiment learning. Results suggest that augmenting datasets with diverse arms and grippers improves policy performance not only on the augmented robots, but also on unseen robots and even the original robots under distribution shifts. In physical experiments, we demonstrate that state-of-the-art generalist policies such as OpenVLA and $π_0$ benefit from fine-tuning on OXE-AugE, improving success rates by 24-45% on previously unseen robot--gripper combinations across four real-world manipulation tasks. Project website: https://OXE-AugE.github.io/.",
            "headline_zh": "提出OXE-AugE数据集与AugE-Toolkit流水线，通过机器人增强解决跨具身策略学习的数据不平衡问题。",
            "intro_zh": [
                "核心问题：Open X-Embodiment数据集高度不平衡，前四种机器人类型占真实数据超85%，可能导致过拟合。",
                "方法要点：开发AugE-Toolkit流水线，生成OXE-AugE数据集，增强9种机器人具身，轨迹数超440万。",
                "实验或效果：增强提升策略性能，包括未见机器人；微调OpenVLA和π_0在真实任务中成功率提高24-45%。"
            ],
            "tags_zh": [
                "跨具身策略学习",
                "机器人数据增强",
                "大规模数据集",
                "通用机器人策略",
                "分布偏移"
            ],
            "_index": 170
        },
        {
            "title": "ADHint: Adaptive Hints with Difficulty Priors for Reinforcement Learning",
            "authors": [
                "Feng Zhang",
                "Zezhong Tan",
                "Xinhong Ma",
                "Ziqiang Dong",
                "Xi Leng",
                "Jianfei Zhao",
                "Xin Sun",
                "Yang Yang"
            ],
            "arxiv_id": "2512.13095v1",
            "summary": "To combine the advantages of Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), recent methods have integrated ''hints'' into post-training, which are prefix segments of complete reasoning trajectories, aiming for powerful knowledge expansion and reasoning generalization. However, existing hint-based RL methods typically ignore difficulty when scheduling hint ratios and estimating relative advantages, leading to unstable learning and excessive imitation of off-policy hints. In this work, we propose ADHint, which treats difficulty as a key factor in both hint-ratio schedule and relative-advantage estimation to achieve a better trade-off between exploration and imitation. Specifically, we propose Adaptive Hint with Sample Difficulty Prior, which evaluates each sample's difficulty under the policy model and accordingly schedules an appropriate hint ratio to guide its rollouts. We also introduce Consistency-based Gradient Modulation and Selective Masking for Hint Preservation to modulate token-level gradients within hints, preventing biased and destructive updates. Additionally, we propose Advantage Estimation with Rollout Difficulty Posterior, which leverages the relative difficulty of rollouts with and without hints to estimate their respective advantages, thereby achieving more balanced updates. Extensive experiments across diverse modalities, model scales, and domains demonstrate that ADHint delivers superior reasoning ability and out-of-distribution generalization, consistently surpassing existing methods in both pass@1 and avg@8. Our code and dataset will be made publicly available upon paper acceptance.",
            "headline_zh": "提出ADHint，通过难度先验和后验优化提示调度与优势估计，以提升强化学习中的推理泛化能力。",
            "intro_zh": [
                "核心问题：现有基于提示的强化学习方法忽略难度因素，导致学习不稳定和过度模仿。",
                "方法要点：引入自适应提示调度、基于一致性的梯度调制和选择性掩码，以及基于难度后验的优势估计。",
                "实验或效果：在多模态、多规模和领域实验中，ADHint在推理能力和分布外泛化上优于现有方法。"
            ],
            "tags_zh": [
                "强化学习",
                "提示调度",
                "难度先验",
                "梯度调制",
                "优势估计",
                "推理泛化"
            ],
            "_index": 171
        },
        {
            "title": "Sequence of Expert: Boosting Imitation Planners for Autonomous Driving through Temporal Alternation",
            "authors": [
                "Xiang Li",
                "Gang Liu",
                "Weitao Zhou",
                "Hongyi Zhu",
                "Zhong Cao"
            ],
            "arxiv_id": "2512.13094v1",
            "summary": "Imitation learning (IL) has emerged as a central paradigm in autonomous driving. While IL excels in matching expert behavior in open-loop settings by minimizing per-step prediction errors, its performance degrades unexpectedly in closed-loop due to the gradual accumulation of small, often imperceptible errors over time.Over successive planning cycles, these errors compound, potentially resulting in severe failures.Current research efforts predominantly rely on increasingly sophisticated network architectures or high-fidelity training datasets to enhance the robustness of IL planners against error accumulation, focusing on the state-level robustness at a single time point. However, autonomous driving is inherently a continuous-time process, and leveraging the temporal scale to enhance robustness may provide a new perspective for addressing this issue.To this end, we propose a method termed Sequence of Experts (SoE), a temporal alternation policy that enhances closed-loop performance without increasing model size or data requirements. Our experiments on large-scale autonomous driving benchmarks nuPlan demonstrate that SoE method consistently and significantly improves the performance of all the evaluated models, and achieves state-of-the-art performance.This module may provide a key and widely applicable support for improving the training efficiency of autonomous driving models.",
            "headline_zh": "提出Sequence of Experts方法，通过时序交替策略提升自动驾驶模仿规划器的闭环性能。",
            "intro_zh": [
                "核心问题：模仿学习在自动驾驶中因误差累积导致闭环性能下降，现有方法多关注单时间点状态级鲁棒性。",
                "方法要点：引入Sequence of Experts，一种时序交替策略，无需增加模型规模或数据需求，利用时间尺度增强鲁棒性。",
                "实验或效果：在nuPlan基准测试中，SoE方法显著提升所有评估模型性能，达到未知水平。"
            ],
            "tags_zh": [
                "自动驾驶",
                "模仿学习",
                "时序交替",
                "闭环规划",
                "鲁棒性增强",
                "nuPlan基准"
            ],
            "_index": 172
        },
        {
            "title": "PvP: Data-Efficient Humanoid Robot Learning with Proprioceptive-Privileged Contrastive Representations",
            "authors": [
                "Mingqi Yuan",
                "Tao Yu",
                "Haolin Song",
                "Bo Li",
                "Xin Jin",
                "Hua Chen",
                "Wenjun Zeng"
            ],
            "arxiv_id": "2512.13093v1",
            "summary": "Achieving efficient and robust whole-body control (WBC) is essential for enabling humanoid robots to perform complex tasks in dynamic environments. Despite the success of reinforcement learning (RL) in this domain, its sample inefficiency remains a significant challenge due to the intricate dynamics and partial observability of humanoid robots. To address this limitation, we propose PvP, a Proprioceptive-Privileged contrastive learning framework that leverages the intrinsic complementarity between proprioceptive and privileged states. PvP learns compact and task-relevant latent representations without requiring hand-crafted data augmentations, enabling faster and more stable policy learning. To support systematic evaluation, we develop SRL4Humanoid, the first unified and modular framework that provides high-quality implementations of representative state representation learning (SRL) methods for humanoid robot learning. Extensive experiments on the LimX Oli robot across velocity tracking and motion imitation tasks demonstrate that PvP significantly improves sample efficiency and final performance compared to baseline SRL methods. Our study further provides practical insights into integrating SRL with RL for humanoid WBC, offering valuable guidance for data-efficient humanoid robot learning.",
            "headline_zh": "提出PvP框架，利用本体感知与特权状态互补性，提升人形机器人强化学习的数据效率。",
            "intro_zh": [
                "核心问题：人形机器人强化学习样本效率低，源于复杂动力学和部分可观测性。",
                "方法要点：PvP通过对比学习学习紧凑任务相关表示，无需手工数据增强。",
                "实验或效果：在LimX Oli机器人上，PvP在速度跟踪和运动模仿任务中显著优于基线方法。"
            ],
            "tags_zh": [
                "人形机器人控制",
                "对比学习",
                "状态表示学习",
                "数据效率",
                "强化学习",
                "本体感知"
            ],
            "_index": 173
        },
        {
            "title": "Multi-Robot Motion Planning from Vision and Language using Heat-Inspired Diffusion",
            "authors": [
                "Jebeom Chae",
                "Junwoo Chang",
                "Seungho Yeom",
                "Yujin Kim",
                "Jongeun Choi"
            ],
            "arxiv_id": "2512.13090v1",
            "summary": "Diffusion models have recently emerged as powerful tools for robot motion planning by capturing the multi-modal distribution of feasible trajectories. However, their extension to multi-robot settings with flexible, language-conditioned task specifications remains limited. Furthermore, current diffusion-based approaches incur high computational cost during inference and struggle with generalization because they require explicit construction of environment representations and lack mechanisms for reasoning about geometric reachability. To address these limitations, we present Language-Conditioned Heat-Inspired Diffusion (LCHD), an end-to-end vision-based framework that generates language-conditioned, collision-free trajectories. LCHD integrates CLIP-based semantic priors with a collision-avoiding diffusion kernel serving as a physical inductive bias that enables the planner to interpret language commands strictly within the reachable workspace. This naturally handles out-of-distribution scenarios -- in terms of reachability -- by guiding robots toward accessible alternatives that match the semantic intent, while eliminating the need for explicit obstacle information at inference time. Extensive evaluations on diverse real-world-inspired maps, along with real-robot experiments, show that LCHD consistently outperforms prior diffusion-based planners in success rate, while reducing planning latency.",
            "headline_zh": "提出语言条件热启发扩散框架，以解决多机器人视觉语言运动规划中的泛化与计算效率问题。",
            "intro_zh": [
                "核心问题：扩散模型在多机器人语言条件规划中泛化差、计算成本高，缺乏几何可达性推理。",
                "方法要点：集成CLIP语义先验与碰撞避免扩散核，无需显式障碍信息，严格约束语言命令于可达工作空间。",
                "实验或效果：在多样化地图和真实机器人实验中，成功率和规划延迟优于现有扩散规划器。"
            ],
            "tags_zh": [
                "多机器人运动规划",
                "扩散模型",
                "语言条件规划",
                "视觉导航",
                "碰撞避免",
                "语义先验"
            ],
            "_index": 174
        },
        {
            "title": "UniVCD: A New Method for Unsupervised Change Detection in the Open-Vocabulary Era",
            "authors": [
                "Ziqiang Zhu",
                "Bowei Yang"
            ],
            "arxiv_id": "2512.13089v1",
            "summary": "Change detection (CD) identifies scene changes from multi-temporal observations and is widely used in urban development and environmental monitoring. Most existing CD methods rely on supervised learning, making performance strongly dataset-dependent and incurring high annotation costs; they typically focus on a few predefined categories and generalize poorly to diverse scenes. With the rise of vision foundation models such as SAM2 and CLIP, new opportunities have emerged to relax these constraints. We propose Unified Open-Vocabulary Change Detection (UniVCD), an unsupervised, open-vocabulary change detection method built on frozen SAM2 and CLIP. UniVCD detects category-agnostic changes across diverse scenes and imaging geometries without any labeled data or paired change images. A lightweight feature alignment module is introduced to bridge the spatially detailed representations from SAM2 and the semantic priors from CLIP, enabling high-resolution, semantically aware change estimation while keeping the number of trainable parameters small. On top of this, a streamlined post-processing pipeline is further introduced to suppress noise and pseudo-changes, improving the detection accuracy for objects with well-defined boundaries. Experiments on several public BCD (Binary Change Detection) and SCD (Semantic Change Detection) benchmarks show that UniVCD achieves consistently strong performance and matches or surpasses existing open-vocabulary CD methods in key metrics such as F1 and IoU. The results demonstrate that unsupervised change detection with frozen vision foundation models and lightweight multi-modal alignment is a practical and effective paradigm for open-vocabulary CD. Code and pretrained models will be released at https://github.com/Die-Xie/UniVCD.",
            "headline_zh": "提出UniVCD方法，基于冻结SAM2和CLIP实现无监督开放词汇变化检测。",
            "intro_zh": [
                "现有变化检测方法依赖监督学习，标注成本高且泛化能力差。",
                "UniVCD结合SAM2的空间细节和CLIP的语义先验，通过轻量特征对齐模块实现高分辨率变化估计。",
                "在多个公开基准测试中，UniVCD在F1和IoU等关键指标上表现优异，匹配或超越现有开放词汇方法。"
            ],
            "tags_zh": [
                "无监督变化检测",
                "开放词汇",
                "多模态对齐",
                "轻量模型",
                "语义感知"
            ],
            "_index": 175
        },
        {
            "title": "DiRe: Diversity-promoting Regularization for Dataset Condensation",
            "authors": [
                "Saumyaranjan Mohanty",
                "Aravind Reddy",
                "Konda Reddy Mopuri"
            ],
            "arxiv_id": "2512.13083v1",
            "summary": "In Dataset Condensation, the goal is to synthesize a small dataset that replicates the training utility of a large original dataset. Existing condensation methods synthesize datasets with significant redundancy, so there is a dire need to reduce redundancy and improve the diversity of the synthesized datasets. To tackle this, we propose an intuitive Diversity Regularizer (DiRe) composed of cosine similarity and Euclidean distance, which can be applied off-the-shelf to various state-of-the-art condensation methods. Through extensive experiments, we demonstrate that the addition of our regularizer improves state-of-the-art condensation methods on various benchmark datasets from CIFAR-10 to ImageNet-1K with respect to generalization and diversity metrics.",
            "headline_zh": "提出多样性正则化器DiRe以提升数据集压缩中的合成数据集多样性",
            "intro_zh": [
                "核心问题：现有数据集压缩方法合成数据集冗余度高，缺乏多样性。",
                "方法要点：设计基于余弦相似度和欧氏距离的多样性正则化器，可即插即用于多种先进压缩方法。",
                "实验或效果：在CIFAR-10至ImageNet-1K等基准数据集上，提升压缩方法的泛化性能和多样性指标。"
            ],
            "tags_zh": [
                "数据集压缩",
                "多样性正则化",
                "余弦相似度",
                "欧氏距离",
                "合成数据集",
                "泛化性能"
            ],
            "_index": 176
        },
        {
            "title": "Spatial-Aware VLA Pretraining through Visual-Physical Alignment from Human Videos",
            "authors": [
                "Yicheng Feng",
                "Wanpeng Zhang",
                "Ye Wang",
                "Hao Luo",
                "Haoqi Yuan",
                "Sipeng Zheng",
                "Zongqing Lu"
            ],
            "arxiv_id": "2512.13080v1",
            "summary": "Vision-Language-Action (VLA) models provide a promising paradigm for robot learning by integrating visual perception with language-guided policy learning. However, most existing approaches rely on 2D visual inputs to perform actions in 3D physical environments, creating a significant gap between perception and action grounding. To bridge this gap, we propose a Spatial-Aware VLA Pretraining paradigm that performs explicit alignment between visual space and physical space during pretraining, enabling models to acquire 3D spatial understanding before robot policy learning. Starting from pretrained vision-language models, we leverage large-scale human demonstration videos to extract 3D visual and 3D action annotations, forming a new source of supervision that aligns 2D visual observations with 3D spatial reasoning. We instantiate this paradigm with VIPA-VLA, a dual-encoder architecture that incorporates a 3D visual encoder to augment semantic visual representations with 3D-aware features. When adapted to downstream robot tasks, VIPA-VLA achieves significantly improved grounding between 2D vision and 3D action, resulting in more robust and generalizable robotic policies.",
            "headline_zh": "提出空间感知VLA预训练范式，通过人类视频的视觉-物理对齐解决2D视觉与3D动作的鸿沟。",
            "intro_zh": [
                "核心问题：现有VLA模型依赖2D视觉输入在3D环境中执行动作，导致感知与动作基础之间存在显著差距。",
                "方法要点：利用大规模人类演示视频提取3D视觉和动作标注，通过双编码器架构VIPA-VLA增强3D空间理解。",
                "实验或效果：在下游机器人任务中，VIPA-VLA显著提升2D视觉与3D动作的基础性，实现更鲁棒和可泛化的策略。"
            ],
            "tags_zh": [
                "视觉-语言-动作模型",
                "空间感知预训练",
                "3D视觉编码",
                "机器人学习",
                "人类视频数据"
            ],
            "_index": 177
        },
        {
            "title": "Heart Disease Prediction using Case Based Reasoning (CBR)",
            "authors": [
                "Mohaiminul Islam Bhuiyan",
                "Chan Hue Wah",
                "Nur Shazwani Kamarudin",
                "Nur Hafieza Ismail",
                "Ahmad Fakhri Ab Nasir"
            ],
            "arxiv_id": "2512.13078v1",
            "summary": "This study provides an overview of heart disease prediction using an intelligent system. Predicting disease accurately is crucial in the medical field, but traditional methods relying solely on a doctor's experience often lack precision. To address this limitation, intelligent systems are applied as an alternative to traditional approaches. While various intelligent system methods exist, this study focuses on three: Fuzzy Logic, Neural Networks, and Case-Based Reasoning (CBR). A comparison of these techniques in terms of accuracy was conducted, and ultimately, Case-Based Reasoning (CBR) was selected for heart disease prediction. In the prediction phase, the heart disease dataset underwent data pre-processing to clean the data and data splitting to separate it into training and testing sets. The chosen intelligent system was then employed to predict heart disease outcomes based on the processed data. The experiment concluded with Case-Based Reasoning (CBR) achieving a notable accuracy rate of 97.95% in predicting heart disease. The findings also revealed that the probability of heart disease was 57.76% for males and 42.24% for females. Further analysis from related studies suggests that factors such as smoking and alcohol consumption are significant contributors to heart disease, particularly among males.",
            "headline_zh": "采用案例推理（CBR）预测心脏病，实现97.95%准确率",
            "intro_zh": [
                "核心问题：传统心脏病预测依赖医生经验，精度不足，需智能系统提升准确性。",
                "方法要点：比较模糊逻辑、神经网络和案例推理（CBR），选择CBR进行预测，并进行数据预处理和分割。",
                "实验或效果：CBR在心脏病预测中达到97.95%准确率，男性患病概率57.76%，女性42.24%。"
            ],
            "tags_zh": [
                "心脏病预测",
                "案例推理",
                "智能系统",
                "数据预处理",
                "准确率分析"
            ],
            "_index": 178
        },
        {
            "title": "LikeBench: Evaluating Subjective Likability in LLMs for Personalization",
            "authors": [
                "Md Awsafur Rahman",
                "Adam Gabrys",
                "Doug Kang",
                "Jingjing Sun",
                "Tian Tan",
                "Ashwin Chandramouli"
            ],
            "arxiv_id": "2512.13077v1",
            "summary": "A personalized LLM should remember user facts, apply them correctly, and adapt over time to provide responses that the user prefers. Existing LLM personalization benchmarks are largely centered on two axes: accurately recalling user information and accurately applying remembered information in downstream tasks. We argue that a third axis, likability, is both subjective and central to user experience, yet under-measured by current benchmarks. To measure likability holistically, we introduce LikeBench, a multi-session, dynamic evaluation framework that measures likability across multiple dimensions by how much an LLM can adapt over time to a user's preferences to provide more likable responses. In LikeBench, the LLMs engage in conversation with a simulated user and learn preferences only from the ongoing dialogue. As the interaction unfolds, models try to adapt to responses, and after each turn, they are evaluated for likability across seven dimensions by the same simulated user. To the best of our knowledge, we are the first to decompose likability into multiple diagnostic metrics: emotional adaptation, formality matching, knowledge adaptation, reference understanding, conversation length fit, humor fit, and callback, which makes it easier to pinpoint where a model falls short. To make the simulated user more realistic and discriminative, LikeBench uses fine-grained, psychologically grounded descriptive personas rather than the coarse high/low trait rating based personas used in prior work. Our benchmark shows that strong memory performance does not guarantee high likability: DeepSeek R1, with lower memory accuracy (86%, 17 facts/profile), outperformed Qwen3 by 28% on likability score despite Qwen3's higher memory accuracy (93%, 43 facts/profile). Even SOTA models like GPT-5 adapt well in short exchanges but show only limited robustness in longer, noisier interactions.",
            "headline_zh": "提出LikeBench评估框架，以多维度主观喜好性衡量LLM个性化能力",
            "intro_zh": [
                "核心问题：现有LLM个性化基准忽视主观喜好性，影响用户体验评估",
                "方法要点：引入多会话动态框架，分解喜好性为七个诊断维度，使用细粒度心理人物模拟",
                "实验或效果：实验显示记忆准确性不保证高喜好性，SOTA模型在长噪声交互中适应性有限"
            ],
            "tags_zh": [
                "LLM个性化评估",
                "主观喜好性",
                "多维度诊断",
                "动态交互框架",
                "心理人物模拟"
            ],
            "_index": 179
        },
        {
            "title": "A Simple and Effective Framework for Symmetric Consistent Indexing in Large-Scale Dense Retrieval",
            "authors": [
                "Huimu Wang",
                "Yiming Qiu",
                "Xingzhi Yao",
                "Zhiguo Chen",
                "Guoyu Tang",
                "Songlin Wang",
                "Sulong Xu",
                "Mingming Li"
            ],
            "arxiv_id": "2512.13074v1",
            "summary": "Dense retrieval has become the industry standard in large-scale information retrieval systems due to its high efficiency and competitive accuracy. Its core relies on a coarse-to-fine hierarchical architecture that enables rapid candidate selection and precise semantic matching, achieving millisecond-level response over billion-scale corpora. This capability makes it essential not only in traditional search and recommendation scenarios but also in the emerging paradigm of generative recommendation driven by large language models, where semantic IDs-themselves a form of coarse-to-fine representation-play a foundational role. However, the widely adopted dual-tower encoding architecture introduces inherent challenges, primarily representational space misalignment and retrieval index inconsistency, which degrade matching accuracy, retrieval stability, and performance on long-tail queries. These issues are further magnified in semantic ID generation, ultimately limiting the performance ceiling of downstream generative models.\n  To address these challenges, this paper proposes a simple and effective framework named SCI comprising two synergistic modules: a symmetric representation alignment module that employs an innovative input-swapping mechanism to unify the dual-tower representation space without adding parameters, and an consistent indexing with dual-tower synergy module that redesigns retrieval paths using a dual-view indexing strategy to maintain consistency from training to inference. The framework is systematic, lightweight, and engineering-friendly, requiring minimal overhead while fully supporting billion-scale deployment. We provide theoretical guarantees for our approach, with its effectiveness validated by results across public datasets and real-world e-commerce datasets.",
            "headline_zh": "提出对称一致索引框架以解决大规模稠密检索中的表示空间错位与索引不一致问题",
            "intro_zh": [
                "核心问题：双塔编码架构导致表示空间错位和检索索引不一致，影响匹配精度和长尾查询性能",
                "方法要点：通过对称表示对齐模块和双塔协同一致索引模块，统一表示空间并保持训练到推理的一致性",
                "实验或效果：在公开和电商数据集上验证有效性，支持十亿级部署且开销小"
            ],
            "tags_zh": [
                "稠密检索",
                "对称表示对齐",
                "一致索引",
                "双塔架构",
                "语义ID生成",
                "大规模信息检索"
            ],
            "_index": 180
        },
        {
            "title": "Forging a Dynamic Memory: Retrieval-Guided Continual Learning for Generalist Medical Foundation Models",
            "authors": [
                "Zizhi Chen",
                "Yizhen Gao",
                "Minghao Han",
                "Yizhou Liu",
                "Zhaoyu Chen",
                "Dingkang Yang",
                "Lihua Zhang"
            ],
            "arxiv_id": "2512.13072v1",
            "summary": "Multimodal biomedical Vision-Language Models (VLMs) exhibit immense potential in the field of Continual Learning (CL). However, they confront a core dilemma: how to preserve fine-grained intra-modality features while bridging the significant domain gap across different modalities. To address this challenge, we propose a comprehensive framework. Leveraging our 18-million multimodal and comprehensive medical retrieval database derived from PubMed scientific papers, we pioneer the integration of Retrieval-Augmented Generation (RAG) into CL. Specifically, we employ a multi-modal, multi-layer RAG system that provides real-time guidance for model fine-tuning through dynamic, on-demand knowledge retrieval. Building upon this, we introduce a dynamic knowledge distillation framework. This framework precisely resolves the aforementioned core dilemma by dynamically modulating the importance of the parameter space, the granularity of the distilled knowledge, and the data distribution of the reference dataset in accordance with the required level of detail. To thoroughly validate the clinical value of our strategy, we have designed a more rigorous \\textbf{M}edical Generalist Task Incremental Learning (MGTIL) benchmark. This benchmark is engineered to simultaneously evaluate the model's capacity for adaptation to significant domain shifts, retention of subtle intra-domain features, and real-time learning of novel and complex medical tasks. Extensive experimental results demonstrate that our proposed method achieves state-of-the-art (SOTA) performance across all metrics. The code is provided in the supplementary materials.",
            "headline_zh": "提出检索增强生成与动态知识蒸馏框架，以解决医学多模态基础模型在持续学习中保留细粒度特征与跨模态域差距的难题。",
            "intro_zh": [
                "核心问题：医学多模态视觉语言模型在持续学习中面临保留细粒度模态内特征与跨越模态域差距的困境。",
                "方法要点：基于1800万医学检索数据库，集成多模态多层检索增强生成，并引入动态知识蒸馏框架，动态调节参数空间、知识粒度与数据分布。",
                "实验或效果：设计医学通用任务增量学习基准，实验显示方法在所有指标上达到最先进性能，代码已提供。"
            ],
            "tags_zh": [
                "医学多模态基础模型",
                "持续学习",
                "检索增强生成",
                "动态知识蒸馏",
                "医学任务增量学习",
                "跨模态域差距"
            ],
            "_index": 181
        },
        {
            "title": "M-GRPO: Stabilizing Self-Supervised Reinforcement Learning for Large Language Models with Momentum-Anchored Policy Optimization",
            "authors": [
                "Bizhe Bai",
                "Hongming Wu",
                "Peng Ye",
                "Tao Chen"
            ],
            "arxiv_id": "2512.13070v1",
            "summary": "Self-supervised reinforcement learning (RL) presents a promising approach for enhancing the reasoning capabilities of Large Language Models (LLMs) without reliance on expensive human-annotated data. However, we find that existing methods suffer from a critical failure mode under long-horizon training: a \"policy collapse\" where performance precipitously degrades. We diagnose this instability and demonstrate that simply scaling the number of rollouts -- a common strategy to improve performance -- only delays, but does not prevent, this collapse. To counteract this instability, we first introduce M-GRPO (Momentum-Anchored Group Relative Policy Optimization), a framework that leverages a slowly evolving momentum model to provide a stable training target. In addition, we identify that this process is often accompanied by a rapid collapse in policy entropy, resulting in a prematurely confident and suboptimal policy. To specifically address this issue, we propose a second contribution: an adaptive filtering method based on the interquartile range (IQR) that dynamically prunes low-entropy trajectories, preserving essential policy diversity. Our extensive experiments on multiple reasoning benchmarks demonstrate that M-GRPO stabilizes the training process while the IQR filter prevents premature convergence. The combination of these two innovations leads to superior training stability and state-of-the-art performance.",
            "headline_zh": "提出M-GRPO与IQR过滤以稳定大语言模型的自监督强化学习训练",
            "intro_zh": [
                "现有自监督强化学习方法在长时训练中易发生策略崩溃，性能急剧下降",
                "M-GRPO利用动量模型提供稳定训练目标，IQR过滤动态剪枝低熵轨迹以保持策略多样性",
                "实验表明该方法提升训练稳定性并在多个推理基准上达到先进性能"
            ],
            "tags_zh": [
                "自监督强化学习",
                "大语言模型",
                "策略优化",
                "训练稳定性",
                "推理能力"
            ],
            "_index": 182
        },
        {
            "title": "Multi-fidelity aerodynamic data fusion by autoencoder transfer learning",
            "authors": [
                "Javier Nieto-Centenero",
                "Esther Andrés",
                "Rodrigo Castellanos"
            ],
            "arxiv_id": "2512.13069v1",
            "summary": "Accurate aerodynamic prediction often relies on high-fidelity simulations; however, their prohibitive computational costs severely limit their applicability in data-driven modeling. This limitation motivates the development of multi-fidelity strategies that leverage inexpensive low-fidelity information without compromising accuracy. Addressing this challenge, this work presents a multi-fidelity deep learning framework that combines autoencoder-based transfer learning with a newly developed Multi-Split Conformal Prediction (MSCP) strategy to achieve uncertainty-aware aerodynamic data fusion under extreme data scarcity. The methodology leverages abundant Low-Fidelity (LF) data to learn a compact latent physics representation, which acts as a frozen knowledge base for a decoder that is subsequently fine-tuned using scarce HF samples. Tested on surface-pressure distributions for NACA airfoils (2D) and a transonic wing (3D) databases, the model successfully corrects LF deviations and achieves high-accuracy pressure predictions using minimal HF training data. Furthermore, the MSCP framework produces robust, actionable uncertainty bands with pointwise coverage exceeding 95%. By combining extreme data efficiency with uncertainty quantification, this work offers a scalable and reliable solution for aerodynamic regression in data-scarce environments.",
            "headline_zh": "提出基于自编码器迁移学习与多分割保形预测的多保真度框架，以解决数据稀缺下气动预测的准确性与不确定性量化问题。",
            "intro_zh": [
                "核心问题：高保真度气动模拟计算成本高，数据稀缺限制数据驱动建模的准确性。",
                "方法要点：利用低保真度数据学习潜在物理表示，通过迁移学习微调解码器，结合多分割保形预测进行不确定性量化。",
                "实验或效果：在NACA翼型和跨音速机翼数据库上，模型以极少高保真度数据实现高精度压力预测，不确定性覆盖超过95%。"
            ],
            "tags_zh": [
                "多保真度数据融合",
                "自编码器迁移学习",
                "不确定性量化",
                "气动预测",
                "数据稀缺建模"
            ],
            "_index": 183
        },
        {
            "title": "LLM Rationalis? Measuring Bargaining Capabilities of AI Negotiators",
            "authors": [
                "Cheril Shah",
                "Akshit Agarwal",
                "Kanak Garg",
                "Mourad Heddaya"
            ],
            "arxiv_id": "2512.13063v1",
            "summary": "Bilateral negotiation is a complex, context-sensitive task in which human negotiators dynamically adjust anchors, pacing, and flexibility to exploit power asymmetries and informal cues. We introduce a unified mathematical framework for modeling concession dynamics based on a hyperbolic tangent curve, and propose two metrics burstiness tau and the Concession-Rigidity Index (CRI) to quantify the timing and rigidity of offer trajectories. We conduct a large-scale empirical comparison between human negotiators and four state-of-the-art large language models (LLMs) across natural-language and numeric-offers settings, with and without rich market context, as well as six controlled power-asymmetry scenarios. Our results reveal that, unlike humans who smoothly adapt to situations and infer the opponents position and strategies, LLMs systematically anchor at extremes of the possible agreement zone for negotiations and optimize for fixed points irrespective of leverage or context. Qualitative analysis further shows limited strategy diversity and occasional deceptive tactics used by LLMs. Moreover the ability of LLMs to negotiate does not improve with better models. These findings highlight fundamental limitations in current LLM negotiation capabilities and point to the need for models that better internalize opponent reasoning and context-dependent strategy.",
            "headline_zh": "提出双曲正切框架与CRI指标，量化LLM在双边谈判中的锚定与僵化行为。",
            "intro_zh": [
                "核心问题：双边谈判中LLM缺乏人类般的动态适应与情境推理能力。",
                "方法要点：基于双曲正切曲线建模让步动态，引入爆发性τ和CRI指标。",
                "实验或效果：大规模比较人类与LLM，揭示LLM锚定极端、策略单一且能力不随模型提升。"
            ],
            "tags_zh": [
                "双边谈判",
                "让步动态建模",
                "LLM评估",
                "锚定行为",
                "策略多样性"
            ],
            "_index": 184
        },
        {
            "title": "Deep Q-Learning-Based Intelligent Scheduling for ETL Optimization in Heterogeneous Data Environments",
            "authors": [
                "Kangning Gao",
                "Yi Hu",
                "Cong Nie",
                "Wei Li"
            ],
            "arxiv_id": "2512.13060v1",
            "summary": "This paper addresses the challenges of low scheduling efficiency, unbalanced resource allocation, and poor adaptability in ETL (Extract-Transform-Load) processes under heterogeneous data environments by proposing an intelligent scheduling optimization framework based on deep Q-learning. The framework formalizes the ETL scheduling process as a Markov Decision Process and enables adaptive decision-making by a reinforcement learning agent in high-dimensional state spaces to dynamically optimize task allocation and resource scheduling. The model consists of a state representation module, a feature embedding network, a Q-value estimator, and a reward evaluation mechanism, which collectively consider task dependencies, node load states, and data flow characteristics to derive the optimal scheduling strategy in complex environments. A multi-objective reward function is designed to balance key performance indicators such as average scheduling delay, task completion rate, throughput, and resource utilization. Sensitivity experiments further verify the model's robustness under changes in hyperparameters, environmental dynamics, and data scale. Experimental results show that the proposed deep Q-learning scheduling framework significantly reduces scheduling delay, improves system throughput, and enhances execution stability under multi-source heterogeneous task conditions, demonstrating the strong potential of reinforcement learning in complex data scheduling and resource management, and providing an efficient and scalable optimization strategy for intelligent data pipeline construction.",
            "headline_zh": "提出基于深度Q学习的智能调度框架以优化异构数据环境中的ETL过程",
            "intro_zh": [
                "核心问题：异构数据环境下ETL调度效率低、资源分配不均、适应性差",
                "方法要点：将ETL调度建模为马尔可夫决策过程，利用深度Q学习进行自适应决策优化",
                "实验或效果：显著降低调度延迟、提高吞吐量，验证了模型在复杂环境中的鲁棒性"
            ],
            "tags_zh": [
                "ETL优化",
                "深度Q学习",
                "智能调度",
                "异构数据环境",
                "资源管理",
                "强化学习"
            ],
            "_index": 185
        },
        {
            "title": "Towards Test-time Efficient Visual Place Recognition via Asymmetric Query Processing",
            "authors": [
                "Jaeyoon Kim",
                "Yoonki Cho",
                "Sung-Eui Yoon"
            ],
            "arxiv_id": "2512.13055v1",
            "summary": "Visual Place Recognition (VPR) has advanced significantly with high-capacity foundation models like DINOv2, achieving remarkable performance. Nonetheless, their substantial computational cost makes deployment on resource-constrained devices impractical. In this paper, we introduce an efficient asymmetric VPR framework that incorporates a high-capacity gallery model for offline feature extraction with a lightweight query network for online processing. A key challenge in this setting is ensuring compatibility between these heterogeneous networks, which conventional approaches address through computationally expensive k-NN-based compatible training. To overcome this, we propose a geographical memory bank that structures gallery features using geolocation metadata inherent in VPR databases, eliminating the need for exhaustive k-NN computations. Additionally, we introduce an implicit embedding augmentation technique that enhances the query network to model feature variations despite its limited capacity. Extensive experiments demonstrate that our method not only significantly reduces computational costs but also outperforms existing asymmetric retrieval techniques, establishing a new aspect for VPR in resource-limited environments. The code is available at https://github.com/jaeyoon1603/AsymVPR",
            "headline_zh": "提出非对称查询处理框架以解决资源受限设备上的视觉地点识别效率问题",
            "intro_zh": [
                "核心问题：高容量基础模型在视觉地点识别中计算成本高，难以部署于资源受限设备",
                "方法要点：采用非对称框架，结合离线高容量图库模型和在线轻量查询网络，引入地理记忆库和隐式嵌入增强技术",
                "实验或效果：显著降低计算成本，超越现有非对称检索方法，适用于资源有限环境"
            ],
            "tags_zh": [
                "视觉地点识别",
                "非对称查询处理",
                "地理记忆库",
                "隐式嵌入增强",
                "资源受限设备",
                "高效检索"
            ],
            "_index": 186
        },
        {
            "title": "GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training",
            "authors": [
                "Tong Wei",
                "Yijun Yang",
                "Changhao Zhang",
                "Junliang Xing",
                "Yuanchun Shi",
                "Zongqing Lu",
                "Deheng Ye"
            ],
            "arxiv_id": "2512.13043v1",
            "summary": "Multi-turn reinforcement learning (RL) for multi-modal agents built upon vision-language models (VLMs) is hampered by sparse rewards and long-horizon credit assignment. Recent methods densify the reward by querying a teacher that provides step-level feedback, e.g., Guided Thought Reinforcement (GTR) and On-Policy Distillation, but rely on costly, often privileged models as the teacher, limiting practicality and reproducibility. We introduce GTR-Turbo, a highly efficient upgrade to GTR, which matches the performance without training or querying an expensive teacher model. Specifically, GTR-Turbo merges the weights of checkpoints produced during the ongoing RL training, and then uses this merged model as a \"free\" teacher to guide the subsequent RL via supervised fine-tuning or soft logit distillation. This design removes dependence on privileged VLMs (e.g., GPT or Gemini), mitigates the \"entropy collapse\" observed in prior work, and keeps training stable. Across diverse visual agentic tasks, GTR-Turbo improves the accuracy of the baseline model by 10-30% while reducing wall-clock training time by 50% and compute cost by 60% relative to GTR.",
            "headline_zh": "提出GTR-Turbo，通过合并检查点作为免费教师，高效提升多模态智能体训练性能。",
            "intro_zh": [
                "核心问题：多模态智能体强化学习面临稀疏奖励和长程信用分配难题，依赖昂贵教师模型。",
                "方法要点：在训练中合并检查点权重作为免费教师，通过监督微调或软标签蒸馏指导后续强化学习。",
                "实验或效果：在视觉任务中提升基线模型准确率10-30%，训练时间减少50%，计算成本降低60%。"
            ],
            "tags_zh": [
                "多模态智能体",
                "强化学习",
                "检查点合并",
                "蒸馏训练",
                "视觉语言模型",
                "高效训练"
            ],
            "_index": 187
        },
        {
            "title": "Understanding Structured Financial Data with LLMs: A Case Study on Fraud Detection",
            "authors": [
                "Xuwei Tan",
                "Yao Ma",
                "Xueru Zhang"
            ],
            "arxiv_id": "2512.13040v1",
            "summary": "Detecting fraud in financial transactions typically relies on tabular models that demand heavy feature engineering to handle high-dimensional data and offer limited interpretability, making it difficult for humans to understand predictions. Large Language Models (LLMs), in contrast, can produce human-readable explanations and facilitate feature analysis, potentially reducing the manual workload of fraud analysts and informing system refinements. However, they perform poorly when applied directly to tabular fraud detection due to the difficulty of reasoning over many features, the extreme class imbalance, and the absence of contextual information. To bridge this gap, we introduce FinFRE-RAG, a two-stage approach that applies importance-guided feature reduction to serialize a compact subset of numeric/categorical attributes into natural language and performs retrieval-augmented in-context learning over label-aware, instance-level exemplars. Across four public fraud datasets and three families of open-weight LLMs, FinFRE-RAG substantially improves F1/MCC over direct prompting and is competitive with strong tabular baselines in several settings. Although these LLMs still lag behind specialized classifiers, they narrow the performance gap and provide interpretable rationales, highlighting their value as assistive tools in fraud analysis.",
            "headline_zh": "提出FinFRE-RAG方法，通过特征缩减和检索增强学习提升LLMs在金融欺诈检测中的性能与可解释性。",
            "intro_zh": [
                "核心问题：LLMs直接应用于表格欺诈检测时性能差，因特征多、类别不平衡和缺乏上下文信息。",
                "方法要点：采用两阶段方法，先重要性引导特征缩减序列化，再检索增强上下文学习。",
                "实验或效果：在四个公共数据集上显著提升F1/MCC，接近表格基线并提供可解释理由。"
            ],
            "tags_zh": [
                "金融欺诈检测",
                "大型语言模型",
                "特征缩减",
                "检索增强生成",
                "表格数据理解",
                "可解释性"
            ],
            "_index": 188
        },
        {
            "title": "Bi-Erasing: A Bidirectional Framework for Concept Removal in Diffusion Models",
            "authors": [
                "Hao Chen",
                "Yiwei Wang",
                "Songze Li"
            ],
            "arxiv_id": "2512.13039v1",
            "summary": "Concept erasure, which fine-tunes diffusion models to remove undesired or harmful visual concepts, has become a mainstream approach to mitigating unsafe or illegal image generation in text-to-image models.However, existing removal methods typically adopt a unidirectional erasure strategy by either suppressing the target concept or reinforcing safe alternatives, making it difficult to achieve a balanced trade-off between concept removal and generation quality. To address this limitation, we propose a novel Bidirectional Image-Guided Concept Erasure (Bi-Erasing) framework that performs concept suppression and safety enhancement simultaneously. Specifically, based on the joint representation of text prompts and corresponding images, Bi-Erasing introduces two decoupled image branches: a negative branch responsible for suppressing harmful semantics and a positive branch providing visual guidance for safe alternatives. By jointly optimizing these complementary directions, our approach achieves a balance between erasure efficacy and generation usability. In addition, we apply mask-based filtering to the image branches to prevent interference from irrelevant content during the erasure process. Across extensive experiment evaluations, the proposed Bi-Erasing outperforms baseline methods in balancing concept removal effectiveness and visual fidelity.",
            "headline_zh": "提出双向图像引导概念擦除框架以解决扩散模型中概念移除与生成质量的平衡问题",
            "intro_zh": [
                "现有概念擦除方法多为单向策略，难以平衡移除效果与生成质量",
                "提出双向框架，通过负分支抑制有害概念、正分支引导安全替代，同时优化互补方向",
                "实验表明该方法在概念移除效果与视觉保真度平衡上优于基线方法"
            ],
            "tags_zh": [
                "扩散模型",
                "概念擦除",
                "文本到图像生成",
                "安全增强",
                "双向优化",
                "图像引导"
            ],
            "_index": 189
        },
        {
            "title": "Progressive Refinement of E-commerce Search Ranking Based on Short-Term Activities of the Buyer",
            "authors": [
                "Taoran Sheng",
                "Sathappan Muthiah",
                "Atiq Islam",
                "Jinming Feng"
            ],
            "arxiv_id": "2512.13037v1",
            "summary": "In e-commerce shopping, aligning search results with a buyer's immediate needs and preferences presents a significant challenge, particularly in adapting search results throughout the buyer's shopping journey as they move from the initial stages of browsing to making a purchase decision or shift from one intent to another. This study presents a systematic approach to adapting e-commerce search results based on the current context. We start with basic methods and incrementally incorporate more contextual information and state-of-the-art techniques to improve the search outcomes. By applying this evolving contextual framework to items displayed on the search engine results page (SERP), we progressively align search outcomes more closely with the buyer's interests and current search intentions. Our findings demonstrate that this incremental enhancement, from simple heuristic autoregressive features to advanced sequence models, significantly improves ranker performance. The integration of contextual techniques enhances the performance of our production ranker, leading to improved search results in both offline and online A/B testing in terms of Mean Reciprocal Rank (MRR). Overall, the paper details iterative methodologies and their substantial contributions to search result contextualization on e-commerce platforms.",
            "headline_zh": "提出基于买家短期活动的渐进式搜索排序优化方法，以提升电商搜索结果的上下文适配性。",
            "intro_zh": [
                "核心问题：电商搜索中，如何根据买家从浏览到购买的动态意图变化，实时调整搜索结果以匹配其即时需求。",
                "方法要点：从基础启发式特征开始，逐步整合上下文信息和先进序列模型，构建渐进式框架来优化搜索引擎结果页的排序。",
                "实验或效果：通过离线与在线A/B测试，该方法显著提升了平均倒数排名（MRR），增强了生产排序器的性能。"
            ],
            "tags_zh": [
                "电商搜索排序",
                "上下文适配",
                "渐进式优化",
                "序列模型",
                "A/B测试",
                "平均倒数排名"
            ],
            "_index": 190
        },
        {
            "title": "Alada: Alternating Adaptation of Momentum Method for Memory-Efficient Matrix Optimization",
            "authors": [
                "Xiaoyu He",
                "Yu Cai",
                "Jin Jia",
                "Canxi Huang",
                "Wenqing Chen",
                "Zibin Zheng"
            ],
            "arxiv_id": "2512.13034v1",
            "summary": "This work proposes Alada, an adaptive momentum method for stochastic optimization over large-scale matrices. Alada employs a rank-one factorization approach to estimate the second moment of gradients, where factors are updated alternatively to minimize the estimation error. Alada achieves sublinear memory overheads and can be readily extended to optimizing tensor-shaped variables.We also equip Alada with a first moment estimation rule, which enhances the algorithm's robustness without incurring additional memory overheads. The theoretical performance of Alada aligns with that of traditional methods such as Adam. Numerical studies conducted on several natural language processing tasks demonstrate the reduction in memory overheads and the robustness in training large models relative to Adam and its variants.",
            "headline_zh": "提出Alada自适应动量方法，用于大规模矩阵优化的内存高效训练。",
            "intro_zh": [
                "核心问题：大规模矩阵优化中梯度二阶矩估计的内存开销高。",
                "方法要点：采用秩一分解交替更新因子，减少估计误差和内存占用。",
                "实验或效果：在自然语言处理任务中，相比Adam及其变体，内存开销降低且训练稳健。"
            ],
            "tags_zh": [
                "自适应动量方法",
                "矩阵优化",
                "内存高效训练",
                "秩一分解",
                "自然语言处理",
                "大规模模型训练"
            ],
            "_index": 191
        },
        {
            "title": "Scaling Bidirectional Spans and Span Violations in Attention Mechanism",
            "authors": [
                "Jongwook Kim",
                "Sangheon Yun",
                "Sukjin Yoon"
            ],
            "arxiv_id": "2512.13033v1",
            "summary": "The canonical $O(N^2)$ Transformer remains the empirical performance frontier in sequence modeling, and its training can be further optimized by addressing geometric inefficiency. We propose an optimization framework that leverages an asymmetric projection to decompose the backward-pass gradients into parallel spans and orthogonal violations, while keeping the canonical forward-pass $QKV$ structure intact. Through consistent experimental validation across various decomposition and projection setups, we provide strong theoretical evidence: the standard attention gradient is suboptimal. We demonstrated that selectively scaling these components, focusing primarily on $0^{th}$ order bidirectional parallel spans, yields the most effective learning signal. On the limited WikiText-2 dataset, and using a crude configuration, this method achieved a $0.56\\%$ reduction in validation loss, confirming the framework's fundamental validity and suggesting significant potential gains on larger datasets and deeper training regimes",
            "headline_zh": "提出基于非对称投影的注意力梯度优化框架，以提升Transformer训练效率。",
            "intro_zh": [
                "核心问题：标准注意力梯度在训练中存在几何低效性，导致次优学习信号。",
                "方法要点：通过非对称投影将反向梯度分解为并行跨度和正交违规，保持前向QKV结构不变。",
                "实验或效果：在WikiText-2数据集上验证损失降低0.56%，表明框架有效且具扩展潜力。"
            ],
            "tags_zh": [
                "注意力机制",
                "梯度优化",
                "Transformer训练",
                "非对称投影",
                "序列建模"
            ],
            "_index": 192
        },
        {
            "title": "Comprehensive Evaluation of Rule-Based, Machine Learning, and Deep Learning in Human Estimation Using Radio Wave Sensing: Accuracy, Spatial Generalization, and Output Granularity Trade-offs",
            "authors": [
                "Tomoya Tanaka",
                "Tomonori Ikeda",
                "Ryo Yonemoto"
            ],
            "arxiv_id": "2512.13031v1",
            "summary": "This study presents the first comprehensive comparison of rule-based methods, traditional machine learning models, and deep learning models in radio wave sensing with frequency modulated continuous wave multiple input multiple output radar. We systematically evaluated five approaches in two indoor environments with distinct layouts: a rule-based connected component method; three traditional machine learning models, namely k-nearest neighbors, random forest, and support vector machine; and a deep learning model combining a convolutional neural network and long short term memory. In the training environment, the convolutional neural network long short term memory model achieved the highest accuracy, while traditional machine learning models provided moderate performance. In a new layout, however, all learning based methods showed significant degradation, whereas the rule-based method remained stable. Notably, for binary detection of presence versus absence of people, all models consistently achieved high accuracy across layouts. These results demonstrate that high capacity models can produce fine grained outputs with high accuracy in the same environment, but they are vulnerable to domain shift. In contrast, rule-based methods cannot provide fine grained outputs but exhibit robustness against domain shift. Moreover, regardless of the model type, a clear trade off was revealed between spatial generalization performance and output granularity.",
            "headline_zh": "比较基于规则、机器学习和深度学习在无线电波人体估计中的性能，揭示空间泛化与输出粒度权衡",
            "intro_zh": [
                "核心问题：评估不同方法在无线电波人体估计中的准确性、空间泛化能力和输出粒度权衡",
                "方法要点：系统比较基于规则、传统机器学习和深度学习模型，使用FMCW MIMO雷达在两种室内布局中测试",
                "实验或效果：深度学习在训练环境精度最高，但空间泛化差；基于规则方法泛化强但输出粒度粗"
            ],
            "tags_zh": [
                "无线电波感知",
                "人体估计",
                "空间泛化",
                "输出粒度",
                "FMCW MIMO雷达",
                "模型比较"
            ],
            "_index": 193
        },
        {
            "title": "Motus: A Unified Latent Action World Model",
            "authors": [
                "Hongzhe Bi",
                "Hengkai Tan",
                "Shenghao Xie",
                "Zeyuan Wang",
                "Shuhe Huang",
                "Haitian Liu",
                "Ruowen Zhao",
                "Yao Feng",
                "Chendong Xiang",
                "Yinze Rong",
                "Hongyan Zhao",
                "Hanyu Liu",
                "Zhizhong Su",
                "Lei Ma",
                "Hang Su",
                "Jun Zhu"
            ],
            "arxiv_id": "2512.13030v1",
            "summary": "While a general embodied agent must function as a unified system, current methods are built on isolated models for understanding, world modeling, and control. This fragmentation prevents unifying multimodal generative capabilities and hinders learning from large-scale, heterogeneous data. In this paper, we propose Motus, a unified latent action world model that leverages existing general pretrained models and rich, sharable motion information. Motus introduces a Mixture-of-Transformer (MoT) architecture to integrate three experts (i.e., understanding, video generation, and action) and adopts a UniDiffuser-style scheduler to enable flexible switching between different modeling modes (i.e., world models, vision-language-action models, inverse dynamics models, video generation models, and video-action joint prediction models). Motus further leverages the optical flow to learn latent actions and adopts a recipe with three-phase training pipeline and six-layer data pyramid, thereby extracting pixel-level \"delta action\" and enabling large-scale action pretraining. Experiments show that Motus achieves superior performance against state-of-the-art methods in both simulation (a +15% improvement over X-VLA and a +45% improvement over Pi0.5) and real-world scenarios(improved by +11~48%), demonstrating unified modeling of all functionalities and priors significantly benefits downstream robotic tasks.",
            "headline_zh": "提出Motus统一潜在动作世界模型，以解决具身智能中模型碎片化问题。",
            "intro_zh": [
                "核心问题：当前具身智能方法依赖孤立模型，阻碍多模态生成能力统一和大规模异构数据学习。",
                "方法要点：采用混合Transformer架构集成专家，结合光流学习潜在动作，支持灵活建模模式切换。",
                "实验或效果：在仿真和真实场景中优于现有方法，提升性能达11%~48%，验证统一建模对下游机器人任务有益。"
            ],
            "tags_zh": [
                "统一世界模型",
                "潜在动作学习",
                "混合Transformer",
                "多模态生成",
                "机器人控制",
                "光流分析"
            ],
            "_index": 194
        },
        {
            "title": "SneakPeek: Future-Guided Instructional Streaming Video Generation",
            "authors": [
                "Cheeun Hong",
                "German Barquero",
                "Fadime Sener",
                "Markos Georgopoulos",
                "Edgar Schönfeld",
                "Stefan Popov",
                "Yuming Du",
                "Oscar Mañas",
                "Albert Pumarola"
            ],
            "arxiv_id": "2512.13019v1",
            "summary": "Instructional video generation is an emerging task that aims to synthesize coherent demonstrations of procedural activities from textual descriptions. Such capability has broad implications for content creation, education, and human-AI interaction, yet existing video diffusion models struggle to maintain temporal consistency and controllability across long sequences of multiple action steps. We introduce a pipeline for future-driven streaming instructional video generation, dubbed SneakPeek, a diffusion-based autoregressive framework designed to generate precise, stepwise instructional videos conditioned on an initial image and structured textual prompts. Our approach introduces three key innovations to enhance consistency and controllability: (1) predictive causal adaptation, where a causal model learns to perform next-frame prediction and anticipate future keyframes; (2) future-guided self-forcing with a dual-region KV caching scheme to address the exposure bias issue at inference time; (3) multi-prompt conditioning, which provides fine-grained and procedural control over multi-step instructions. Together, these components mitigate temporal drift, preserve motion consistency, and enable interactive video generation where future prompt updates dynamically influence ongoing streaming video generation. Experimental results demonstrate that our method produces temporally coherent and semantically faithful instructional videos that accurately follow complex, multi-step task descriptions.",
            "headline_zh": "提出SneakPeek框架，通过未来引导的流式生成解决教学视频中长序列动作的时序一致性和可控性问题。",
            "intro_zh": [
                "核心问题：现有视频扩散模型在生成多步骤教学视频时，难以保持时序一致性和可控性。",
                "方法要点：引入预测因果适应、未来引导自强制和多提示条件化，以增强一致性和交互控制。",
                "实验或效果：实验表明，该方法能生成时序连贯、语义忠实且准确遵循复杂多步骤描述的教学视频。"
            ],
            "tags_zh": [
                "教学视频生成",
                "时序一致性",
                "扩散模型",
                "流式生成",
                "多步骤控制",
                "未来预测"
            ],
            "_index": 195
        },
        {
            "title": "Comprehensive Deployment-Oriented Assessment for Cross-Environment Generalization in Deep Learning-Based mmWave Radar Sensing",
            "authors": [
                "Tomoya Tanaka",
                "Tomonori Ikeda",
                "Ryo Yonemoto"
            ],
            "arxiv_id": "2512.13018v1",
            "summary": "This study presents the first comprehensive evaluation of spatial generalization techniques, which are essential for the practical deployment of deep learning-based radio-frequency (RF) sensing. Focusing on people counting in indoor environments using frequency-modulated continuous-wave (FMCW) multiple-input multiple-output (MIMO) radar, we systematically investigate a broad set of approaches, including amplitude-based statistical preprocessing (sigmoid weighting and threshold zeroing), frequency-domain filtering, autoencoder-based background suppression, data augmentation strategies, and transfer learning. Experimental results collected across two environments with different layouts demonstrate that sigmoid-based amplitude weighting consistently achieves superior cross-environment performance, yielding 50.1% and 55.2% reductions in root-mean-square error (RMSE) and mean absolute error (MAE), respectively, compared with baseline methods. Data augmentation provides additional though modest benefits, with improvements up to 8.8% in MAE. By contrast, transfer learning proves indispensable for large spatial shifts, achieving 82.1% and 91.3% reductions in RMSE and MAE, respectively, with 540 target-domain samples. Taken together, these findings establish a highly practical direction for developing radar sensing systems capable of maintaining robust accuracy under spatial variations by integrating deep learning models with amplitude-based preprocessing and efficient transfer learning.",
            "headline_zh": "评估深度学习毫米波雷达感知的跨环境泛化技术，提出基于幅度预处理的实用部署方案。",
            "intro_zh": [
                "核心问题：深度学习RF感知在空间变化下的部署泛化能力不足，影响实际应用。",
                "方法要点：系统评估幅度预处理、数据增强和迁移学习等多种泛化技术。",
                "实验或效果：幅度加权在跨环境中表现最佳，迁移学习在大空间偏移下效果显著。"
            ],
            "tags_zh": [
                "毫米波雷达感知",
                "跨环境泛化",
                "深度学习部署",
                "幅度预处理",
                "迁移学习",
                "室内人员计数"
            ],
            "_index": 196
        },
        {
            "title": "What Happens Next? Next Scene Prediction with a Unified Video Model",
            "authors": [
                "Xinjie Li",
                "Zhimin Chen",
                "Rui Zhao",
                "Florian Schiffers",
                "Zhenyu Liao",
                "Vimal Bhat"
            ],
            "arxiv_id": "2512.13015v1",
            "summary": "Recent unified models for joint understanding and generation have significantly advanced visual generation capabilities. However, their focus on conventional tasks like text-to-video generation has left the temporal reasoning potential of unified models largely underexplored. To address this gap, we introduce Next Scene Prediction (NSP), a new task that pushes unified video models toward temporal and causal reasoning. Unlike text-to-video generation, NSP requires predicting plausible futures from preceding context, demanding deeper understanding and reasoning. To tackle this task, we propose a unified framework combining Qwen-VL for comprehension and LTX for synthesis, bridged by a latent query embedding and a connector module. This model is trained in three stages on our newly curated, large-scale NSP dataset: text-to-video pre-training, supervised fine-tuning, and reinforcement learning (via GRPO) with our proposed causal consistency reward. Experiments demonstrate our model achieves state-of-the-art performance on our benchmark, advancing the capability of generalist multimodal systems to anticipate what happens next.",
            "headline_zh": "提出Next Scene Prediction任务与统一框架，以增强视频模型的时序与因果推理能力。",
            "intro_zh": [
                "核心问题：统一视频模型在时序推理方面潜力未充分探索，需预测未来场景。",
                "方法要点：结合Qwen-VL和LTX，通过潜在查询嵌入和连接模块，分三阶段训练。",
                "实验或效果：在新数据集上实现最佳性能，提升多模态系统预测能力。"
            ],
            "tags_zh": [
                "Next Scene Prediction",
                "统一视频模型",
                "时序推理",
                "因果一致性奖励",
                "多模态系统",
                "强化学习"
            ],
            "_index": 197
        },
        {
            "title": "JoDiffusion: Jointly Diffusing Image with Pixel-Level Annotations for Semantic Segmentation Promotion",
            "authors": [
                "Haoyu Wang",
                "Lei Zhang",
                "Wenrui Liu",
                "Dengyang Jiang",
                "Wei Wei",
                "Chen Ding"
            ],
            "arxiv_id": "2512.13014v1",
            "summary": "Given the inherently costly and time-intensive nature of pixel-level annotation, the generation of synthetic datasets comprising sufficiently diverse synthetic images paired with ground-truth pixel-level annotations has garnered increasing attention recently for training high-performance semantic segmentation models. However, existing methods necessitate to either predict pseudo annotations after image generation or generate images conditioned on manual annotation masks, which incurs image-annotation semantic inconsistency or scalability problem. To migrate both problems with one stone, we present a novel dataset generative diffusion framework for semantic segmentation, termed JoDiffusion. Firstly, given a standard latent diffusion model, JoDiffusion incorporates an independent annotation variational auto-encoder (VAE) network to map annotation masks into the latent space shared by images. Then, the diffusion model is tailored to capture the joint distribution of each image and its annotation mask conditioned on a text prompt. By doing these, JoDiffusion enables simultaneously generating paired images and semantically consistent annotation masks solely conditioned on text prompts, thereby demonstrating superior scalability. Additionally, a mask optimization strategy is developed to mitigate the annotation noise produced during generation. Experiments on Pascal VOC, COCO, and ADE20K datasets show that the annotated dataset generated by JoDiffusion yields substantial performance improvements in semantic segmentation compared to existing methods.",
            "headline_zh": "提出JoDiffusion框架，通过联合扩散生成图像与像素级标注以提升语义分割性能。",
            "intro_zh": [
                "核心问题：现有方法生成合成数据集时存在图像-标注语义不一致或可扩展性问题。",
                "方法要点：引入独立标注VAE网络，使扩散模型捕获图像与标注的联合分布，仅基于文本提示同时生成配对数据。",
                "实验或效果：在Pascal VOC等数据集上，生成数据集显著提升语义分割模型性能。"
            ],
            "tags_zh": [
                "语义分割",
                "扩散模型",
                "数据集生成",
                "像素级标注",
                "联合分布建模"
            ],
            "_index": 198
        },
        {
            "title": "Deep Learning-Driven Inversion Framework for Shear Modulus Estimation in Magnetic Resonance Elastography (DIME)",
            "authors": [
                "Hassan Iftikhar",
                "Rizwan Ahmad",
                "Arunark Kolipaka"
            ],
            "arxiv_id": "2512.13010v1",
            "summary": "The Multimodal Direct Inversion (MMDI) algorithm is widely used in Magnetic Resonance Elastography (MRE) to estimate tissue shear stiffness. However, MMDI relies on the Helmholtz equation, which assumes wave propagation in a uniform, homogeneous, and infinite medium. Furthermore, the use of the Laplacian operator makes MMDI highly sensitive to noise, which compromises the accuracy and reliability of stiffness estimates. In this study, we propose the Deep-Learning driven Inversion Framework for Shear Modulus Estimation in MRE (DIME), aimed at enhancing the robustness of inversion. DIME is trained on the displacement fields-stiffness maps pair generated through Finite Element Modelling (FEM) simulations. To capture local wave behavior and improve robustness to global image variations, DIME is trained on small image patches. We first validated DIME using homogeneous and heterogeneous datasets simulated with FEM, where DIME produced stiffness maps with low inter-pixel variability, accurate boundary delineation, and higher correlation with ground truth (GT) compared to MMDI. Next, DIME was evaluated in a realistic anatomy-informed simulated liver dataset with known GT and compared directly to MMDI. DIME reproduced ground-truth stiffness patterns with high fidelity (r = 0.99, R^2 = 0.98), while MMDI showed greater underestimation. After validating DIME on synthetic data, we tested the model in in vivo liver MRE data from eight healthy and seven fibrotic subjects. DIME preserved physiologically consistent stiffness patterns and closely matched MMDI, which showed directional bias. Overall, DIME showed higher correlation with ground truth and visually similar stiffness patterns, whereas MMDI displayed a larger bias that can potentially be attributed to directional filtering. These preliminary results highlight the feasibility of DIME for clinical applications in MRE.",
            "headline_zh": "提出深度学习驱动的反演框架DIME，以增强磁共振弹性成像中剪切模量估计的鲁棒性。",
            "intro_zh": [
                "核心问题：传统MMDI算法基于均匀介质假设，对噪声敏感，影响刚度估计的准确性。",
                "方法要点：DIME基于有限元模拟生成的位移场-刚度图对训练，采用小图像块捕获局部波行为，提升鲁棒性。",
                "实验或效果：在合成和真实肝脏数据中，DIME相比MMDI显示更高相关性、更低偏差和更准确边界描绘。"
            ],
            "tags_zh": [
                "磁共振弹性成像",
                "剪切模量估计",
                "深度学习反演",
                "有限元模拟",
                "鲁棒性增强"
            ],
            "_index": 199
        },
        {
            "title": "K-VARK: Kernelized Variance-Aware Residual Kalman Filter for Sensorless Force Estimation in Collaborative Robots",
            "authors": [
                "Oğuzhan Akbıyık",
                "Naseem Alhousani",
                "Fares J. Abu-Dakka"
            ],
            "arxiv_id": "2512.13009v1",
            "summary": "Reliable estimation of contact forces is crucial for ensuring safe and precise interaction of robots with unstructured environments. However, accurate sensorless force estimation remains challenging due to inherent modeling errors and complex residual dynamics and friction. To address this challenge, in this paper, we propose K-VARK (Kernelized Variance-Aware Residual Kalman filter), a novel approach that integrates a kernelized, probabilistic model of joint residual torques into an adaptive Kalman filter framework. Through Kernelized Movement Primitives trained on optimized excitation trajectories, K-VARK captures both the predictive mean and input-dependent heteroscedastic variance of residual torques, reflecting data variability and distance-to-training effects. These statistics inform a variance-aware virtual measurement update by augmenting the measurement noise covariance, while the process noise covariance adapts online via variational Bayesian optimization to handle dynamic disturbances. Experimental validation on a 6-DoF collaborative manipulator demonstrates that K-VARK achieves over 20% reduction in RMSE compared to state-of-the-art sensorless force estimation methods, yielding robust and accurate external force/torque estimation suitable for advanced tasks such as polishing and assembly.",
            "headline_zh": "提出K-VARK核化方差感知残差卡尔曼滤波器，用于协作机器人无传感器力估计。",
            "intro_zh": [
                "核心问题：无传感器力估计因建模误差和复杂残差动力学而困难。",
                "方法要点：集成核化概率模型到自适应卡尔曼滤波器，捕捉残差力矩的均值和异方差方差。",
                "实验或效果：在6自由度协作机械臂上验证，RMSE降低超20%，适用于抛光和装配任务。"
            ],
            "tags_zh": [
                "无传感器力估计",
                "卡尔曼滤波器",
                "核化方法",
                "方差感知",
                "协作机器人",
                "残差建模"
            ],
            "_index": 200
        },
        {
            "title": "TWLR: Text-Guided Weakly-Supervised Lesion Localization and Severity Regression for Explainable Diabetic Retinopathy Grading",
            "authors": [
                "Xi Luo",
                "Shixin Xu",
                "Ying Xie",
                "JianZhong Hu",
                "Yuwei He",
                "Yuhui Deng",
                "Huaxiong Huang"
            ],
            "arxiv_id": "2512.13008v1",
            "summary": "Accurate medical image analysis can greatly assist clinical diagnosis, but its effectiveness relies on high-quality expert annotations Obtaining pixel-level labels for medical images, particularly fundus images, remains costly and time-consuming. Meanwhile, despite the success of deep learning in medical imaging, the lack of interpretability limits its clinical adoption. To address these challenges, we propose TWLR, a two-stage framework for interpretable diabetic retinopathy (DR) assessment. In the first stage, a vision-language model integrates domain-specific ophthalmological knowledge into text embeddings to jointly perform DR grading and lesion classification, effectively linking semantic medical concepts with visual features. The second stage introduces an iterative severity regression framework based on weakly-supervised semantic segmentation. Lesion saliency maps generated through iterative refinement direct a progressive inpainting mechanism that systematically eliminates pathological features, effectively downgrading disease severity toward healthier fundus appearances. Critically, this severity regression approach achieves dual benefits: accurate lesion localization without pixel-level supervision and providing an interpretable visualization of disease-to-healthy transformations. Experimental results on the FGADR, DDR, and a private dataset demonstrate that TWLR achieves competitive performance in both DR classification and lesion segmentation, offering a more explainable and annotation-efficient solution for automated retinal image analysis.",
            "headline_zh": "提出TWLR框架以解决糖尿病视网膜病变分级中标注成本高和模型可解释性差的问题。",
            "intro_zh": [
                "核心问题：医学图像像素级标注昂贵，深度学习模型缺乏可解释性，限制临床应用。",
                "方法要点：两阶段框架，第一阶段结合视觉语言模型进行分级和病变分类，第二阶段基于弱监督分割迭代回归病变严重性。",
                "实验或效果：在FGADR、DDR和私有数据集上验证，实现竞争性DR分类和病变分割，提供可解释的疾病到健康转换可视化。"
            ],
            "tags_zh": [
                "糖尿病视网膜病变分级",
                "弱监督病变定位",
                "视觉语言模型",
                "可解释性医学图像分析",
                "迭代严重性回归"
            ],
            "_index": 201
        },
        {
            "title": "Light Field Based 6DoF Tracking of Previously Unobserved Objects",
            "authors": [
                "Nikolai Goncharov",
                "James L. Gray",
                "Donald G. Dansereau"
            ],
            "arxiv_id": "2512.13007v1",
            "summary": "Object tracking is an important step in robotics and reautonomous driving pipelines, which has to generalize to previously unseen and complex objects. Existing high-performing methods often rely on pre-captured object views to build explicit reference models, which restricts them to a fixed set of known objects. However, such reference models can struggle with visually complex appearance, reducing the quality of tracking. In this work, we introduce an object tracking method based on light field images that does not depend on a pre-trained model, while being robust to complex visual behavior, such as reflections. We extract semantic and geometric features from light field inputs using vision foundation models and convert them into view-dependent Gaussian splats. These splats serve as a unified object representation, supporting differentiable rendering and pose optimization. We further introduce a light field object tracking dataset containing challenging reflective objects with precise ground truth poses. Experiments demonstrate that our method is competitive with state-of-the-art model-based trackers in these difficult cases, paving the way toward universal object tracking in robotic systems. Code/data available at https://github.com/nagonch/LiFT-6DoF.",
            "headline_zh": "提出基于光场图像的6DoF跟踪方法，无需预训练模型，适用于未观测复杂物体。",
            "intro_zh": [
                "核心问题：现有高性能跟踪方法依赖预捕获物体视图，限制于已知物体集，且对复杂外观（如反射）敏感。",
                "方法要点：利用视觉基础模型从光场输入提取语义和几何特征，转换为视图相关高斯溅射作为统一对象表示，支持可微渲染和姿态优化。",
                "实验或效果：在包含挑战性反射物体的光场跟踪数据集上实验，与最先进基于模型的跟踪器竞争，推动机器人系统通用物体跟踪。"
            ],
            "tags_zh": [
                "光场图像",
                "6DoF跟踪",
                "未观测物体",
                "高斯溅射",
                "可微渲染",
                "机器人视觉"
            ],
            "_index": 202
        },
        {
            "title": "Few-Step Distillation for Text-to-Image Generation: A Practical Guide",
            "authors": [
                "Yifan Pu",
                "Yizeng Han",
                "Zhiwei Tang",
                "Jiasheng Tang",
                "Fan Wang",
                "Bohan Zhuang",
                "Gao Huang"
            ],
            "arxiv_id": "2512.13006v1",
            "summary": "Diffusion distillation has dramatically accelerated class-conditional image synthesis, but its applicability to open-ended text-to-image (T2I) generation is still unclear. We present the first systematic study that adapts and compares state-of-the-art distillation techniques on a strong T2I teacher model, FLUX.1-lite. By casting existing methods into a unified framework, we identify the key obstacles that arise when moving from discrete class labels to free-form language prompts. Beyond a thorough methodological analysis, we offer practical guidelines on input scaling, network architecture, and hyperparameters, accompanied by an open-source implementation and pretrained student models. Our findings establish a solid foundation for deploying fast, high-fidelity, and resource-efficient diffusion generators in real-world T2I applications. Code is available on github.com/alibaba-damo-academy/T2I-Distill.",
            "headline_zh": "提出少步蒸馏方法以加速开放域文本到图像生成，提供实用指南与开源实现。",
            "intro_zh": [
                "核心问题：扩散蒸馏在开放域文本到图像生成中的应用障碍与效果未知。",
                "方法要点：系统比较并适配先进蒸馏技术于FLUX.1-lite教师模型，统一框架分析关键挑战。",
                "实验或效果：提供输入缩放、网络架构和超参数指南，开源代码与预训练学生模型。"
            ],
            "tags_zh": [
                "文本到图像生成",
                "扩散蒸馏",
                "少步生成",
                "模型加速",
                "开源实现"
            ],
            "_index": 203
        },
        {
            "title": "General OOD Detection via Model-aware and Subspace-aware Variable Priority",
            "authors": [
                "Min Lu",
                "Hemant Ishwaran"
            ],
            "arxiv_id": "2512.13003v1",
            "summary": "Out-of-distribution (OOD) detection is essential for determining when a supervised model encounters inputs that differ meaningfully from its training distribution. While widely studied in classification, OOD detection for regression and survival analysis remains limited due to the absence of discrete labels and the challenge of quantifying predictive uncertainty. We introduce a framework for OOD detection that is simultaneously model aware and subspace aware, and that embeds variable prioritization directly into the detection step. The method uses the fitted predictor to construct localized neighborhoods around each test case that emphasize the features driving the model's learned relationship and downweight directions that are less relevant to prediction. It produces OOD scores without relying on global distance metrics or estimating the full feature density. The framework is applicable across outcome types, and in our implementation we use random forests, where the rule structure yields transparent neighborhoods and effective scoring. Experiments on synthetic and real data benchmarks designed to isolate functional shifts show consistent improvements over existing methods. We further demonstrate the approach in an esophageal cancer survival study, where distribution shifts related to lymphadenectomy identify patterns relevant to surgical guidelines.",
            "headline_zh": "提出模型感知与子空间感知的变量优先级框架，用于通用OOD检测，适用于回归和生存分析。",
            "intro_zh": [
                "核心问题：回归和生存分析中OOD检测因缺乏离散标签和量化预测不确定性而受限。",
                "方法要点：利用拟合预测器构建局部邻域，强调驱动模型关系的特征，并嵌入变量优先级。",
                "实验或效果：在合成和真实数据基准测试中，针对功能偏移显示优于现有方法的性能提升。"
            ],
            "tags_zh": [
                "OOD检测",
                "回归分析",
                "生存分析",
                "随机森林",
                "变量优先级",
                "模型感知"
            ],
            "_index": 204
        },
        {
            "title": "Calibrating Uncertainty for Zero-Shot Adversarial CLIP",
            "authors": [
                "Wenjing lu",
                "Zerui Tao",
                "Dongping Zhang",
                "Yuning Qiu",
                "Yang Yang",
                "Qibin Zhao"
            ],
            "arxiv_id": "2512.12997v1",
            "summary": "CLIP delivers strong zero-shot classification but remains highly vulnerable to adversarial attacks. Previous work of adversarial fine-tuning largely focuses on matching the predicted logits between clean and adversarial examples, which overlooks uncertainty calibration and may degrade the zero-shot generalization. A common expectation in reliable uncertainty estimation is that predictive uncertainty should increase as inputs become more difficult or shift away from the training distribution. However, we frequently observe the opposite in the adversarial setting: perturbations not only degrade accuracy but also suppress uncertainty, leading to severe miscalibration and unreliable over-confidence. This overlooked phenomenon highlights a critical reliability gap beyond robustness. To bridge this gap, we propose a novel adversarial fine-tuning objective for CLIP considering both prediction accuracy and uncertainty alignments. By reparameterizing the output of CLIP as the concentration parameter of a Dirichlet distribution, we propose a unified representation that captures relative semantic structure and the magnitude of predictive confidence. Our objective aligns these distributions holistically under perturbations, moving beyond single-logit anchoring and restoring calibrated uncertainty. Experiments on multiple zero-shot classification benchmarks demonstrate that our approach effectively restores calibrated uncertainty and achieves competitive adversarial robustness while maintaining clean accuracy.",
            "headline_zh": "提出基于狄利克雷分布重参数化的对抗微调目标，以校准零样本对抗CLIP的不确定性。",
            "intro_zh": [
                "核心问题：对抗扰动导致CLIP不确定性被抑制，产生错误校准和过度自信。",
                "方法要点：通过狄利克雷分布重参数化CLIP输出，统一表示语义结构和预测置信度。",
                "实验或效果：在多个零样本分类基准上恢复校准不确定性，保持清洁精度和对抗鲁棒性。"
            ],
            "tags_zh": [
                "零样本学习",
                "对抗鲁棒性",
                "不确定性校准",
                "CLIP模型",
                "狄利克雷分布"
            ],
            "_index": 205
        },
        {
            "title": "Learning Terrain Aware Bipedal Locomotion via Reduced Dimensional Perceptual Representations",
            "authors": [
                "Guillermo A. Castillo",
                "Himanshu Lodha",
                "Ayonga Hereid"
            ],
            "arxiv_id": "2512.12993v1",
            "summary": "This work introduces a hierarchical strategy for terrain-aware bipedal locomotion that integrates reduced-dimensional perceptual representations to enhance reinforcement learning (RL)-based high-level (HL) policies for real-time gait generation. Unlike end-to-end approaches, our framework leverages latent terrain encodings via a Convolutional Variational Autoencoder (CNN-VAE) alongside reduced-order robot dynamics, optimizing the locomotion decision process with a compact state. We systematically analyze the impact of latent space dimensionality on learning efficiency and policy robustness. Additionally, we extend our method to be history-aware, incorporating sequences of recent terrain observations into the latent representation to improve robustness. To address real-world feasibility, we introduce a distillation method to learn the latent representation directly from depth camera images and provide preliminary hardware validation by comparing simulated and real sensor data. We further validate our framework using the high-fidelity Agility Robotics (AR) simulator, incorporating realistic sensor noise, state estimation, and actuator dynamics. The results confirm the robustness and adaptability of our method, underscoring its potential for hardware deployment.",
            "headline_zh": "提出基于降维感知表示的分层策略，以增强地形感知双足机器人强化学习步态生成。",
            "intro_zh": [
                "核心问题：传统端到端方法在地形感知双足步态生成中效率低、鲁棒性差。",
                "方法要点：使用CNN-VAE提取地形潜在编码，结合降阶动力学和历史感知，优化决策状态。",
                "实验或效果：通过高保真模拟和硬件验证，确认方法在噪声和动态环境下的鲁棒性与适应性。"
            ],
            "tags_zh": [
                "双足机器人步态生成",
                "地形感知强化学习",
                "降维感知表示",
                "CNN-VAE",
                "分层控制策略",
                "硬件部署验证"
            ],
            "_index": 206
        },
        {
            "title": "Tackling Snow-Induced Challenges: Safe Autonomous Lane-Keeping with Robust Reinforcement Learning",
            "authors": [
                "Amin Jalal Aghdasian",
                "Farzaneh Abdollahi",
                "Ali Kamali Iglie"
            ],
            "arxiv_id": "2512.12987v1",
            "summary": "This paper proposes two new algorithms for the lane keeping system (LKS) in autonomous vehicles (AVs) operating under snowy road conditions. These algorithms use deep reinforcement learning (DRL) to handle uncertainties and slippage. They include Action-Robust Recurrent Deep Deterministic Policy Gradient (AR-RDPG) and end-to-end Action-Robust convolutional neural network Attention Deterministic Policy Gradient (AR-CADPG), two action-robust approaches for decision-making. In the AR-RDPG method, within the perception layer, camera images are first denoised using multi-scale neural networks. Then, the centerline coefficients are extracted by a pre-trained deep convolutional neural network (DCNN). These coefficients, concatenated with the driving characteristics, are used as input to the control layer. The AR-CADPG method presents an end-to-end approach in which a convolutional neural network (CNN) and an attention mechanism are integrated within a DRL framework. Both methods are first trained in the CARLA simulator and validated under various snowy scenarios. Real-world experiments on a Jetson Nano-based autonomous vehicle confirm the feasibility and stability of the learned policies. Among the two models, the AR-CADPG approach demonstrates superior path-tracking accuracy and robustness, highlighting the effectiveness of combining temporal memory, adversarial resilience, and attention mechanisms in AVs.",
            "headline_zh": "提出两种动作鲁棒强化学习算法以解决自动驾驶车辆在雪天路况下的车道保持问题。",
            "intro_zh": [
                "核心问题：雪天路况导致车道线识别困难、车辆打滑，影响自动驾驶车道保持系统的安全性和稳定性。",
                "方法要点：开发AR-RDPG和AR-CADPG两种算法，前者结合去噪网络和预训练DCNN提取车道线，后者集成CNN和注意力机制实现端到端决策。",
                "实验或效果：在CARLA模拟器中训练和验证，并在基于Jetson Nano的真实车辆上测试，AR-CADPG在路径跟踪精度和鲁棒性上表现更优。"
            ],
            "tags_zh": [
                "自动驾驶车道保持",
                "雪天路况鲁棒性",
                "深度强化学习",
                "动作鲁棒算法",
                "端到端决策",
                "注意力机制"
            ],
            "_index": 207
        },
        {
            "title": "VoroLight: Learning Quality Volumetric Voronoi Meshes from General Inputs",
            "authors": [
                "Jiayin Lu",
                "Ying Jiang",
                "Yin Yang",
                "Chenfanfu Jiang"
            ],
            "arxiv_id": "2512.12984v1",
            "summary": "We present VoroLight, a differentiable framework for 3D shape reconstruction based on Voronoi meshing. Our approach generates smooth, watertight surfaces and topologically consistent volumetric meshes directly from diverse inputs, including images, implicit shape level-set fields, point clouds and meshes. VoroLight operates in three stages: it first initializes a surface using a differentiable Voronoi formulation, then refines surface quality through a polygon-face sphere training stage, and finally reuses the differentiable Voronoi formulation for volumetric optimization with additional interior generator points. Project page: https://jiayinlu19960224.github.io/vorolight/",
            "headline_zh": "提出VoroLight框架，基于可微Voronoi网格化从多样输入重建高质量三维形状。",
            "intro_zh": [
                "核心问题：从图像、隐式场、点云和网格等多样输入生成平滑、水密且拓扑一致的体网格。",
                "方法要点：采用三阶段流程，包括可微Voronoi初始化、多边形面球训练优化表面质量和体优化。",
                "实验或效果：直接生成高质量表面和体网格，支持多种输入类型，提升重建精度和拓扑一致性。"
            ],
            "tags_zh": [
                "三维形状重建",
                "可微Voronoi网格化",
                "体网格优化",
                "多样输入处理",
                "拓扑一致性"
            ],
            "_index": 208
        },
        {
            "title": "Scaling Up AI-Generated Image Detection via Generator-Aware Prototypes",
            "authors": [
                "Ziheng Qin",
                "Yuheng Ji",
                "Renshuai Tao",
                "Yuxuan Tian",
                "Yuyang Liu",
                "Yipu Wang",
                "Xiaolong Zheng"
            ],
            "arxiv_id": "2512.12982v1",
            "summary": "The pursuit of a universal AI-generated image (AIGI) detector often relies on aggregating data from numerous generators to improve generalization. However, this paper identifies a paradoxical phenomenon we term the Benefit then Conflict dilemma, where detector performance stagnates and eventually degrades as source diversity expands. Our systematic analysis, diagnoses this failure by identifying two core issues: severe data-level heterogeneity, which causes the feature distributions of real and synthetic images to increasingly overlap, and a critical model-level bottleneck from fixed, pretrained encoders that cannot adapt to the rising complexity. To address these challenges, we propose Generator-Aware Prototype Learning (GAPL), a framework that constrain representation with a structured learning paradigm. GAPL learns a compact set of canonical forgery prototypes to create a unified, low-variance feature space, effectively countering data heterogeneity.To resolve the model bottleneck, it employs a two-stage training scheme with Low-Rank Adaptation, enhancing its discriminative power while preserving valuable pretrained knowledge. This approach establishes a more robust and generalizable decision boundary. Through extensive experiments, we demonstrate that GAPL achieves state-of-the-art performance, showing superior detection accuracy across a wide variety of GAN and diffusion-based generators. Code is available at https://github.com/UltraCapture/GAPL",
            "headline_zh": "提出生成器感知原型学习以解决AI生成图像检测中的多样性与模型瓶颈问题",
            "intro_zh": [
                "核心问题：检测器性能随生成器多样性增加而停滞或下降，源于数据异质性和固定编码器瓶颈",
                "方法要点：通过结构化原型学习统一特征空间，结合低秩适应增强模型判别力",
                "实验或效果：在多种生成器上实现最先进检测精度，代码开源"
            ],
            "tags_zh": [
                "AI生成图像检测",
                "原型学习",
                "低秩适应",
                "特征空间统一",
                "生成器多样性"
            ],
            "_index": 209
        },
        {
            "title": "CoDeQ: End-to-End Joint Model Compression with Dead-Zone Quantizer for High-Sparsity and Low-Precision Networks",
            "authors": [
                "Jonathan Wenshøj",
                "Tong Chen",
                "Bob Pepin",
                "Raghavendra Selvan"
            ],
            "arxiv_id": "2512.12981v1",
            "summary": "While joint pruning--quantization is theoretically superior to sequential application, current joint methods rely on auxiliary procedures outside the training loop for finding compression parameters. This reliance adds engineering complexity and hyperparameter tuning, while also lacking a direct data-driven gradient signal, which might result in sub-optimal compression. In this paper, we introduce CoDeQ, a simple, fully differentiable method for joint pruning--quantization. Our approach builds on a key observation: the dead-zone of a scalar quantizer is equivalent to magnitude pruning, and can be used to induce sparsity directly within the quantization operator. Concretely, we parameterize the dead-zone width and learn it via backpropagation, alongside the quantization parameters. This design provides explicit control of sparsity, regularized by a single global hyperparameter, while decoupling sparsity selection from bit-width selection. The result is a method for Compression with Dead-zone Quantizer (CoDeQ) that supports both fixed-precision and mixed-precision quantization (controlled by an optional second hyperparameter). It simultaneously determines the sparsity pattern and quantization parameters in a single end-to-end optimization. Consequently, CoDeQ does not require any auxiliary procedures, making the method architecture-agnostic and straightforward to implement. On ImageNet with ResNet-18, CoDeQ reduces bit operations to ~5% while maintaining close to full precision accuracy in both fixed and mixed-precision regimes.",
            "headline_zh": "提出CoDeQ方法，通过可学习死区量化器实现端到端联合剪枝与量化，用于高稀疏低精度网络。",
            "intro_zh": [
                "现有联合剪枝-量化方法依赖训练循环外的辅助过程，导致工程复杂和次优压缩。",
                "CoDeQ利用量化器死区等效于幅度剪枝，通过参数化死区宽度实现端到端可微分优化。",
                "在ImageNet上，CoDeQ将ResNet-18的比特操作降至约5%，同时保持接近全精度准确率。"
            ],
            "tags_zh": [
                "模型压缩",
                "联合剪枝量化",
                "死区量化器",
                "端到端优化",
                "高稀疏网络",
                "低精度网络"
            ],
            "_index": 210
        },
        {
            "title": "VLCache: Computing 2% Vision Tokens and Reusing 98% for Vision-Language Inference",
            "authors": [
                "Shengling Qin",
                "Hao Yu",
                "Chenxin Wu",
                "Zheng Li",
                "Yizhong Cao",
                "Zhengyang Zhuge",
                "Yuxin Zhou",
                "Wentao Yao",
                "Yi Zhang",
                "Zhengheng Wang",
                "Shuai Bai",
                "Jianwei Zhang",
                "Junyang Lin"
            ],
            "arxiv_id": "2512.12977v1",
            "summary": "This paper presents VLCache, a cache reuse framework that exploits both Key-Value (KV) cache and encoder cache from prior multimodal inputs to eliminate costly recomputation when the same multimodal inputs recur. Unlike previous heuristic approaches, we formally identify the cumulative reuse error effect and demonstrate how to minimize the non-prefix cache reuse error effectively. We further analyze the varying importance of model layers and propose a dynamic, layer-aware recomputation strategy to balance accuracy and efficiency. Experimental results show that VLCache achieves an accuracy on par with full recomputation, while requiring only 2-5% of the tokens to compute, yielding 1.2x-16x TTFT speedups. The proposed VLCache pipeline has been integrated into SGLang, enabling significantly faster inference in practical deployments.",
            "headline_zh": "提出VLCache框架，通过复用多模态输入的KV缓存和编码器缓存，减少视觉语言推理中的重复计算。",
            "intro_zh": [
                "核心问题：多模态输入重复出现时，传统方法需全量重计算，导致推理效率低下。",
                "方法要点：形式化分析累积复用误差，提出动态层感知重计算策略，平衡精度与效率。",
                "实验或效果：仅需计算2-5%的视觉令牌，实现1.2x-16x的首次令牌时间加速，精度与全重计算相当。"
            ],
            "tags_zh": [
                "视觉语言推理",
                "缓存复用",
                "KV缓存",
                "编码器缓存",
                "动态重计算",
                "推理加速"
            ],
            "_index": 211
        },
        {
            "title": "Application of Deep Learning in Biological Data Compression",
            "authors": [
                "Chunyu Zou"
            ],
            "arxiv_id": "2512.12975v1",
            "summary": "Cryogenic electron microscopy (Cryo-EM) has become an essential tool for capturing high-resolution biological structures. Despite its advantage in visualizations, the large storage size of Cryo-EM data file poses significant challenges for researchers and educators. This paper investigates the application of deep learning, specifically implicit neural representation (INR), to compress Cryo-EM biological data. The proposed approach first extracts the binary map of each file according to the density threshold. The density map is highly repetitive, ehich can be effectively compressed by GZIP. The neural network then trains to encode spatial density information, allowing the storage of network parameters and learnable latent vectors. To improve reconstruction accuracy, I further incorporate the positional encoding to enhance spatial representation and a weighted Mean Squared Error (MSE) loss function to balance density distribution variations. Using this approach, my aim is to provide a practical and efficient biological data compression solution that can be used for educational and research purpose, while maintaining a reasonable compression ratio and reconstruction quality from file to file.",
            "headline_zh": "提出基于隐式神经表示的深度学习方法来压缩冷冻电镜生物数据",
            "intro_zh": [
                "核心问题：冷冻电镜数据文件存储量大，对研究和教育构成挑战。",
                "方法要点：提取密度图后，用GZIP压缩，神经网络编码空间密度信息，结合位置编码和加权MSE损失提升重建精度。",
                "实验或效果：旨在提供实用高效的压缩方案，保持合理压缩比和文件间重建质量。"
            ],
            "tags_zh": [
                "冷冻电镜数据压缩",
                "隐式神经表示",
                "深度学习压缩",
                "生物数据存储",
                "位置编码",
                "加权MSE损失"
            ],
            "_index": 212
        },
        {
            "title": "Towards Open Standards for Systemic Complexity in Digital Forensics",
            "authors": [
                "Paola Di Maio"
            ],
            "arxiv_id": "2512.12970v1",
            "summary": "The intersection of artificial intelligence (AI) and digital forensics (DF) is becoming increasingly complex, ubiquitous, and pervasive, with overlapping techniques and technologies being adopted in all types of scientific and technical inquiry. Despite incredible advances, forensic sciences are not exempt from errors and remain vulnerable to fallibility. To mitigate the limitations of errors in DF, the systemic complexity is identified and addressed with the adoption of human-readable artifacts and open standards. A DF AI model schema based on the state of the art is outlined.",
            "headline_zh": "提出基于开放标准和人可读工件的数字取证AI模型架构，以应对系统复杂性挑战。",
            "intro_zh": [
                "核心问题：数字取证与AI交叉领域存在系统复杂性，易导致错误和脆弱性。",
                "方法要点：采用人可读工件和开放标准来识别和缓解系统复杂性。",
                "实验或效果：基于当前技术前沿，概述了数字取证AI模型架构。"
            ],
            "tags_zh": [
                "数字取证",
                "人工智能",
                "系统复杂性",
                "开放标准",
                "人可读工件",
                "模型架构"
            ],
            "_index": 213
        },
        {
            "title": "SCAdapter: Content-Style Disentanglement for Diffusion Style Transfer",
            "authors": [
                "Luan Thanh Trinh",
                "Kenji Doi",
                "Atsuki Osanai"
            ],
            "arxiv_id": "2512.12963v1",
            "summary": "Diffusion models have emerged as the leading approach for style transfer, yet they struggle with photo-realistic transfers, often producing painting-like results or missing detailed stylistic elements. Current methods inadequately address unwanted influence from original content styles and style reference content features. We introduce SCAdapter, a novel technique leveraging CLIP image space to effectively separate and integrate content and style features. Our key innovation systematically extracts pure content from content images and style elements from style references, ensuring authentic transfers. This approach is enhanced through three components: Controllable Style Adaptive Instance Normalization (CSAdaIN) for precise multi-style blending, KVS Injection for targeted style integration, and a style transfer consistency objective maintaining process coherence. Comprehensive experiments demonstrate SCAdapter significantly outperforms state-of-the-art methods in both conventional and diffusion-based baselines. By eliminating DDIM inversion and inference-stage optimization, our method achieves at least $2\\times$ faster inference than other diffusion-based approaches, making it both more effective and efficient for practical applications.",
            "headline_zh": "提出SCAdapter以解决扩散模型风格迁移中内容-风格纠缠和细节缺失问题",
            "intro_zh": [
                "核心问题：扩散模型在风格迁移中易产生绘画化结果或丢失细节，现有方法未能有效分离内容风格和风格参考内容特征",
                "方法要点：利用CLIP图像空间分离内容与风格特征，结合CSAdaIN、KVS注入和一致性目标实现精确迁移",
                "实验或效果：在传统和扩散基线中显著优于现有方法，无需DDIM反转和推理优化，推理速度至少快2倍"
            ],
            "tags_zh": [
                "扩散模型",
                "风格迁移",
                "内容-风格解耦",
                "CLIP特征",
                "快速推理",
                "图像生成"
            ],
            "_index": 214
        },
        {
            "title": "Leveraging Compression to Construct Transferable Bitrate Ladders",
            "authors": [
                "Krishna Srikar Durbha",
                "Hassene Tmar",
                "Ping-Hao Wu",
                "Ioannis Katsavounidis",
                "Alan C. Bovik"
            ],
            "arxiv_id": "2512.12952v1",
            "summary": "Over the past few years, per-title and per-shot video encoding techniques have demonstrated significant gains as compared to conventional techniques such as constant CRF encoding and the fixed bitrate ladder. These techniques have demonstrated that constructing content-gnostic per-shot bitrate ladders can provide significant bitrate gains and improved Quality of Experience (QoE) for viewers under various network conditions. However, constructing a convex hull for every video incurs a significant computational overhead. Recently, machine learning-based bitrate ladder construction techniques have emerged as a substitute for convex hull construction. These methods operate by extracting features from source videos to train machine learning (ML) models to construct content-adaptive bitrate ladders. Here, we present a new ML-based bitrate ladder construction technique that accurately predicts the VMAF scores of compressed videos, by analyzing the compression procedure and by making perceptually relevant measurements on the source videos prior to compression. We evaluate the performance of our proposed framework against leading prior methods on a large corpus of videos. Since training ML models on every encoder setting is time-consuming, we also investigate how per-shot bitrate ladders perform under different encoding settings. We evaluate the performance of all models against the fixed bitrate ladder and the best possible convex hull constructed using exhaustive encoding with Bjontegaard-delta metrics.",
            "headline_zh": "提出基于压缩分析的机器学习方法，以构建可转移的比特率阶梯，提升视频编码效率。",
            "intro_zh": [
                "核心问题：传统比特率阶梯构建计算开销大，需高效替代方案。",
                "方法要点：通过分析压缩过程和源视频感知测量，预测压缩视频的VMAF分数。",
                "实验或效果：在大规模视频集上评估，优于现有方法，并探索不同编码设置下的性能。"
            ],
            "tags_zh": [
                "视频编码",
                "比特率阶梯",
                "机器学习",
                "VMAF预测",
                "压缩分析",
                "质量评估"
            ],
            "_index": 215
        },
        {
            "title": "Building from Scratch: A Multi-Agent Framework with Human-in-the-Loop for Multilingual Legal Terminology Mapping",
            "authors": [
                "Lingyi Meng",
                "Maolin Liu",
                "Hao Wang",
                "Yilan Cheng",
                "Qi Yang",
                "Idlkaid Mohanmmed"
            ],
            "arxiv_id": "2512.12950v1",
            "summary": "Accurately mapping legal terminology across languages remains a significant challenge, especially for language pairs like Chinese and Japanese, which share a large number of homographs with different meanings. Existing resources and standardized tools for these languages are limited. To address this, we propose a human-AI collaborative approach for building a multilingual legal terminology database, based on a multi-agent framework. This approach integrates advanced large language models and legal domain experts throughout the entire process-from raw document preprocessing, article-level alignment, to terminology extraction, mapping, and quality assurance. Unlike a single automated pipeline, our approach places greater emphasis on how human experts participate in this multi-agent system. Humans and AI agents take on different roles: AI agents handle specific, repetitive tasks, such as OCR, text segmentation, semantic alignment, and initial terminology extraction, while human experts provide crucial oversight, review, and supervise the outputs with contextual knowledge and legal judgment. We tested the effectiveness of this framework using a trilingual parallel corpus comprising 35 key Chinese statutes, along with their English and Japanese translations. The experimental results show that this human-in-the-loop, multi-agent workflow not only improves the precision and consistency of multilingual legal terminology mapping but also offers greater scalability compared to traditional manual methods.",
            "headline_zh": "提出人机协同多智能体框架以解决多语言法律术语映射的挑战",
            "intro_zh": [
                "核心问题：中、日等语言存在大量同形异义词，现有资源和工具有限，导致法律术语跨语言映射困难。",
                "方法要点：结合大语言模型与法律专家，通过多智能体系统分工处理文档预处理、对齐、术语提取等任务，强调人类监督。",
                "实验或效果：基于中英日三语平行语料测试，该框架提高了映射精度和一致性，并展现出比传统方法更好的可扩展性。"
            ],
            "tags_zh": [
                "多语言法律术语映射",
                "人机协同",
                "多智能体框架",
                "大语言模型",
                "法律专家监督",
                "平行语料库"
            ],
            "_index": 216
        },
        {
            "title": "Understanding When Graph Convolutional Networks Help: A Diagnostic Study on Label Scarcity and Structural Properties",
            "authors": [
                "Nischal Subedi",
                "Ember Kerstetter",
                "Winnie Li",
                "Silo Murphy"
            ],
            "arxiv_id": "2512.12947v1",
            "summary": "Graph Convolutional Networks (GCNs) have become a standard approach for semi-supervised node classification, yet practitioners lack clear guidance on when GCNs provide meaningful improvements over simpler baselines. We present a diagnostic study using the Amazon Computers co-purchase data to understand when and why GCNs help. Through systematic experiments with simulated label scarcity, feature ablation, and per-class analysis, we find that GCN performance depends critically on the interaction between graph homophily and feature quality. GCNs provide the largest gains under extreme label scarcity, where they leverage neighborhood structure to compensate for limited supervision. Surprisingly, GCNs can match their original performance even when node features are replaced with random noise, suggesting that structure alone carries sufficient signal on highly homophilous graphs. However, GCNs hurt performance when homophily is low and features are already strong, as noisy neighbors corrupt good predictions. Our quadrant analysis reveals that GCNs help in three of four conditions and only hurt when low homophily meets strong features. These findings offer practical guidance for practitioners deciding whether to adopt graph-based methods.",
            "headline_zh": "通过诊断研究揭示图卷积网络在标签稀缺与结构特性下的适用条件",
            "intro_zh": [
                "核心问题：缺乏明确指导判断图卷积网络何时优于简单基线方法",
                "方法要点：基于亚马逊计算机共购数据，模拟标签稀缺、特征消融和按类分析",
                "实验或效果：发现图卷积网络性能取决于图同质性与特征质量的交互作用"
            ],
            "tags_zh": [
                "图卷积网络",
                "半监督节点分类",
                "标签稀缺",
                "图同质性",
                "诊断研究",
                "亚马逊数据集"
            ],
            "_index": 217
        },
        {
            "title": "SLIM-VDB: A Real-Time 3D Probabilistic Semantic Mapping Framework",
            "authors": [
                "Anja Sheppard",
                "Parker Ewen",
                "Joey Wilson",
                "Advaith V. Sethuraman",
                "Benard Adewole",
                "Anran Li",
                "Yuzhen Chen",
                "Ram Vasudevan",
                "Katherine A. Skinner"
            ],
            "arxiv_id": "2512.12945v1",
            "summary": "This paper introduces SLIM-VDB, a new lightweight semantic mapping system with probabilistic semantic fusion for closed-set or open-set dictionaries. Advances in data structures from the computer graphics community, such as OpenVDB, have demonstrated significantly improved computational and memory efficiency in volumetric scene representation. Although OpenVDB has been used for geometric mapping in robotics applications, semantic mapping for scene understanding with OpenVDB remains unexplored. In addition, existing semantic mapping systems lack support for integrating both fixed-category and open-language label predictions within a single framework. In this paper, we propose a novel 3D semantic mapping system that leverages the OpenVDB data structure and integrates a unified Bayesian update framework for both closed- and open-set semantic fusion. Our proposed framework, SLIM-VDB, achieves significant reduction in both memory and integration times compared to current state-of-the-art semantic mapping approaches, while maintaining comparable mapping accuracy. An open-source C++ codebase with a Python interface is available at https://github.com/umfieldrobotics/slim-vdb.",
            "headline_zh": "提出SLIM-VDB框架，利用OpenVDB数据结构实现实时3D概率语义建图，支持闭集和开集语义融合。",
            "intro_zh": [
                "现有语义建图系统缺乏统一框架支持固定类别和开放语言标签预测的集成。",
                "采用OpenVDB数据结构，结合贝叶斯更新框架，实现闭集和开集语义的概率融合。",
                "相比当前先进方法，显著减少内存占用和集成时间，同时保持可比建图精度。"
            ],
            "tags_zh": [
                "3D语义建图",
                "概率语义融合",
                "OpenVDB数据结构",
                "实时建图",
                "闭集与开集字典"
            ],
            "_index": 218
        },
        {
            "title": "UAGLNet: Uncertainty-Aggregated Global-Local Fusion Network with Cooperative CNN-Transformer for Building Extraction",
            "authors": [
                "Siyuan Yao",
                "Dongxiu Liu",
                "Taotao Li",
                "Shengjie Li",
                "Wenqi Ren",
                "Xiaochun Cao"
            ],
            "arxiv_id": "2512.12941v1",
            "summary": "Building extraction from remote sensing images is a challenging task due to the complex structure variations of the buildings. Existing methods employ convolutional or self-attention blocks to capture the multi-scale features in the segmentation models, while the inherent gap of the feature pyramids and insufficient global-local feature integration leads to inaccurate, ambiguous extraction results. To address this issue, in this paper, we present an Uncertainty-Aggregated Global-Local Fusion Network (UAGLNet), which is capable to exploit high-quality global-local visual semantics under the guidance of uncertainty modeling. Specifically, we propose a novel cooperative encoder, which adopts hybrid CNN and transformer layers at different stages to capture the local and global visual semantics, respectively. An intermediate cooperative interaction block (CIB) is designed to narrow the gap between the local and global features when the network becomes deeper. Afterwards, we propose a Global-Local Fusion (GLF) module to complementarily fuse the global and local representations. Moreover, to mitigate the segmentation ambiguity in uncertain regions, we propose an Uncertainty-Aggregated Decoder (UAD) to explicitly estimate the pixel-wise uncertainty to enhance the segmentation accuracy. Extensive experiments demonstrate that our method achieves superior performance to other state-of-the-art methods. Our code is available at https://github.com/Dstate/UAGLNet",
            "headline_zh": "提出UAGLNet，通过不确定性聚合的全局-局部融合网络解决遥感图像建筑提取中的结构复杂性问题。",
            "intro_zh": [
                "核心问题：现有方法因特征金字塔间隙和全局-局部特征融合不足，导致建筑提取结果不准确和模糊。",
                "方法要点：设计合作编码器结合CNN和Transformer，引入全局-局部融合模块和不确定性聚合解码器以提升语义质量和减少不确定性。",
                "实验或效果：在广泛实验中，UAGLNet优于其他先进方法，代码已开源。"
            ],
            "tags_zh": [
                "建筑提取",
                "遥感图像",
                "全局-局部融合",
                "不确定性建模",
                "CNN-Transformer合作",
                "语义分割"
            ],
            "_index": 219
        },
        {
            "title": "Continuous Edit Distance, Geodesics and Barycenters of Time-varying Persistence Diagrams",
            "authors": [
                "Sebastien Tchitchek",
                "Mohamed Kissi",
                "Julien Tierny"
            ],
            "arxiv_id": "2512.12939v1",
            "summary": "We introduce the Continuous Edit Distance (CED), a geodesic and elastic distance for time-varying persistence diagrams (TVPDs). The CED extends edit-distance ideas to TVPDs by combining local substitution costs with penalized deletions/insertions, controlled by two parameters: \\(α\\) (trade-off between temporal misalignment and diagram discrepancy) and \\(β\\) (gap penalty). We also provide an explicit construction of CED-geodesics. Building on these ingredients, we present two practical barycenter solvers, one stochastic and one greedy, that monotonically decrease the CED Frechet energy. Empirically, the CED is robust to additive perturbations (both temporal and spatial), recovers temporal shifts, and supports temporal pattern search. On real-life datasets, the CED achieves clustering performance comparable or better than standard elastic dissimilarities, while our clustering based on CED-barycenters yields superior classification results. Overall, the CED equips TVPD analysis with a principled distance, interpretable geodesics, and practical barycenters, enabling alignment, comparison, averaging, and clustering directly in the space of TVPDs. A C++ implementation is provided for reproducibility at the following address https://github.com/sebastien-tchitchek/ContinuousEditDistance.",
            "headline_zh": "提出连续编辑距离以分析时变持久图，支持对齐、比较和聚类。",
            "intro_zh": [
                "核心问题：时变持久图缺乏原则性距离，难以直接进行对齐和平均。",
                "方法要点：引入连续编辑距离，结合局部替换成本和惩罚删除/插入，提供显式测地线构造。",
                "实验或效果：在真实数据集上，聚类性能优于标准弹性差异，基于重心的方法提升分类结果。"
            ],
            "tags_zh": [
                "时变持久图",
                "连续编辑距离",
                "测地线",
                "重心计算",
                "聚类分析",
                "拓扑数据分析"
            ],
            "_index": 220
        },
        {
            "title": "Content Adaptive based Motion Alignment Framework for Learned Video Compression",
            "authors": [
                "Tiange Zhang",
                "Xiandong Meng",
                "Siwei Ma"
            ],
            "arxiv_id": "2512.12936v1",
            "summary": "Recent advances in end-to-end video compression have shown promising results owing to their unified end-to-end learning optimization. However, such generalized frameworks often lack content-specific adaptation, leading to suboptimal compression performance. To address this, this paper proposes a content adaptive based motion alignment framework that improves performance by adapting encoding strategies to diverse content characteristics. Specifically, we first introduce a two-stage flow-guided deformable warping mechanism that refines motion compensation with coarse-to-fine offset prediction and mask modulation, enabling precise feature alignment. Second, we propose a multi-reference quality aware strategy that adjusts distortion weights based on reference quality, and applies it to hierarchical training to reduce error propagation. Third, we integrate a training-free module that downsamples frames by motion magnitude and resolution to obtain smooth motion estimation. Experimental results on standard test datasets demonstrate that our framework CAMA achieves significant improvements over state-of-the-art Neural Video Compression models, achieving a 24.95% BD-rate (PSNR) savings over our baseline model DCVC-TCM, while also outperforming reproduced DCVC-DC and traditional codec HM-16.25.",
            "headline_zh": "提出内容自适应运动对齐框架以提升端到端视频压缩性能",
            "intro_zh": [
                "核心问题：端到端视频压缩框架缺乏内容特定适应，导致压缩性能次优。",
                "方法要点：引入两阶段流引导可变形扭曲机制、多参考质量感知策略和免训练下采样模块。",
                "实验或效果：在标准测试集上，CAMA框架相比基线模型DCVC-TCM节省24.95% BD-rate（PSNR）。"
            ],
            "tags_zh": [
                "端到端视频压缩",
                "运动对齐",
                "内容自适应",
                "可变形扭曲",
                "多参考策略",
                "免训练模块"
            ],
            "_index": 221
        },
        {
            "title": "Unified Interactive Multimodal Moment Retrieval via Cascaded Embedding-Reranking and Temporal-Aware Score Fusion",
            "authors": [
                "Toan Le Ngo Thanh",
                "Phat Ha Huu",
                "Tan Nguyen Dang Duy",
                "Thong Nguyen Le Minh",
                "Anh Nguyen Nhu Tinh"
            ],
            "arxiv_id": "2512.12935v1",
            "summary": "The exponential growth of video content has created an urgent need for efficient multimodal moment retrieval systems. However, existing approaches face three critical challenges: (1) fixed-weight fusion strategies fail across cross modal noise and ambiguous queries, (2) temporal modeling struggles to capture coherent event sequences while penalizing unrealistic gaps, and (3) systems require manual modality selection, reducing usability. We propose a unified multimodal moment retrieval system with three key innovations. First, a cascaded dual-embedding pipeline combines BEIT-3 and SigLIP for broad retrieval, refined by BLIP-2 based reranking to balance recall and precision. Second, a temporal-aware scoring mechanism applies exponential decay penalties to large temporal gaps via beam search, constructing coherent event sequences rather than isolated frames. Third, Agent-guided query decomposition (GPT-4o) automatically interprets ambiguous queries, decomposes them into modality specific sub-queries (visual/OCR/ASR), and performs adaptive score fusion eliminating manual modality selection. Qualitative analysis demonstrates that our system effectively handles ambiguous queries, retrieves temporally coherent sequences, and dynamically adapts fusion strategies, advancing interactive moment search capabilities.",
            "headline_zh": "提出统一交互式多模态时刻检索系统，通过级联嵌入-重排序和时序感知评分融合解决跨模态噪声和模糊查询问题。",
            "intro_zh": [
                "核心问题：现有方法面临固定权重融合策略失效、时序建模难以捕捉连贯事件序列、需手动模态选择降低可用性。",
                "方法要点：采用级联双嵌入管道结合BEIT-3和SigLIP进行广泛检索，BLIP-2重排序优化；时序感知评分机制通过波束搜索施加指数衰减惩罚；Agent引导查询分解自动解释模糊查询并自适应融合分数。",
                "实验或效果：系统有效处理模糊查询，检索时序连贯序列，动态适应融合策略，提升交互式时刻搜索能力。"
            ],
            "tags_zh": [
                "多模态时刻检索",
                "时序建模",
                "查询分解",
                "嵌入融合",
                "交互式搜索",
                "视频理解"
            ],
            "_index": 222
        },
        {
            "title": "Investigating Data Pruning for Pretraining Biological Foundation Models at Scale",
            "authors": [
                "Yifan Wu",
                "Jiyue Jiang",
                "Xichen Ye",
                "Yiqi Wang",
                "Chang Zhou",
                "Yitao Xu",
                "Jiayang Chen",
                "He Hu",
                "Weizhong Zhang",
                "Cheng Jin",
                "Jiao Yuan",
                "Yu Li"
            ],
            "arxiv_id": "2512.12932v1",
            "summary": "Biological foundation models (BioFMs), pretrained on large-scale biological sequences, have recently shown strong potential in providing meaningful representations for diverse downstream bioinformatics tasks. However, such models often rely on millions to billions of training sequences and billions of parameters, resulting in prohibitive computational costs and significant barriers to reproducibility and accessibility, particularly for academic labs. To address these challenges, we investigate the feasibility of data pruning for BioFM pretraining and propose a post-hoc influence-guided data pruning framework tailored to biological domains. Our approach introduces a subset-based self-influence formulation that enables efficient estimation of sample importance at low computational cost, and builds upon it two simple yet effective selection strategies, namely Top-k Influence (Top I) and Coverage-Centric Influence (CCI). We empirically validate our method on two representative BioFMs, RNA-FM and ESM-C. For RNA, our framework consistently outperforms random selection baselines under an extreme pruning rate of over 99 percent, demonstrating its effectiveness. Furthermore, we show the generalizability of our framework on protein-related tasks using ESM-C. In particular, our coreset even outperforms random subsets that are ten times larger in both RNA and protein settings, revealing substantial redundancy in biological sequence datasets. These findings underscore the potential of influence-guided data pruning to substantially reduce the computational cost of BioFM pretraining, paving the way for more efficient, accessible, and sustainable biological AI research.",
            "headline_zh": "提出影响引导的数据剪枝框架以降低生物基础模型预训练的计算成本",
            "intro_zh": [
                "生物基础模型预训练依赖海量数据，计算成本高且可复现性差",
                "引入基于子集的自影响公式，高效估计样本重要性，设计Top-k和覆盖中心影响选择策略",
                "在RNA-FM和ESM-C上验证，极端剪枝率超99%时优于随机基线，核心集性能优于十倍大随机子集"
            ],
            "tags_zh": [
                "生物基础模型",
                "数据剪枝",
                "影响估计",
                "预训练优化",
                "计算效率"
            ],
            "_index": 223
        },
        {
            "title": "SeVeDo: A Heterogeneous Transformer Accelerator for Low-Bit Inference via Hierarchical Group Quantization and SVD-Guided Mixed Precision",
            "authors": [
                "Yuseon Choi",
                "Sangjin Kim",
                "Jungjun Oh",
                "Byeongcheol Kim",
                "Hoi-Jun Yoo"
            ],
            "arxiv_id": "2512.12930v1",
            "summary": "Low-bit quantization is a promising technique for efficient transformer inference by reducing computational and memory overhead. However, aggressive bitwidth reduction remains challenging due to activation outliers, leading to accuracy degradation. Existing methods, such as outlier-handling and group quantization, achieve high accuracy but incur substantial energy consumption. To address this, we propose SeVeDo, an energy-efficient SVD-based heterogeneous accelerator that structurally separates outlier-sensitive components into a high-precision low-rank path, while the remaining computations are executed in a low-bit residual datapath with group quantization. To further enhance efficiency, Hierarchical Group Quantization (HGQ) combines coarse-grained floating-point scaling with fine-grained shifting, effectively reducing dequantization cost. Also, SVD-guided mixed precision (SVD-MP) statically allocates higher bitwidths to precision-sensitive components identified through low-rank decomposition, thereby minimizing floating-point operation cost. Experimental results show that SeVeDo achieves a peak energy efficiency of 13.8TOPS/W, surpassing conventional designs, with 12.7TOPS/W on ViT-Base and 13.4TOPS/W on Llama2-7B benchmarks.",
            "headline_zh": "提出SeVeDo异构加速器，通过分层组量化和SVD引导混合精度解决低比特推理中的激活异常值问题",
            "intro_zh": [
                "核心问题：低比特量化因激活异常值导致精度下降，现有方法能耗高",
                "方法要点：异构架构分离异常值敏感组件，结合分层组量化和SVD引导混合精度",
                "实验或效果：在ViT-Base和Llama2-7B上实现最高13.8TOPS/W的能效，超越传统设计"
            ],
            "tags_zh": [
                "低比特量化",
                "异构加速器",
                "分层组量化",
                "SVD引导混合精度",
                "能效优化",
                "Transformer推理"
            ],
            "_index": 224
        },
        {
            "title": "MADTempo: An Interactive System for Multi-Event Temporal Video Retrieval with Query Augmentation",
            "authors": [
                "Huu-An Vu",
                "Van-Khanh Mai",
                "Trong-Tam Nguyen",
                "Quang-Duc Dam",
                "Tien-Huy Nguyen",
                "Thanh-Huong Le"
            ],
            "arxiv_id": "2512.12929v1",
            "summary": "The rapid expansion of video content across online platforms has accelerated the need for retrieval systems capable of understanding not only isolated visual moments but also the temporal structure of complex events. Existing approaches often fall short in modeling temporal dependencies across multiple events and in handling queries that reference unseen or rare visual concepts. To address these challenges, we introduce MADTempo, a video retrieval framework developed by our team, AIO_Trinh, that unifies temporal search with web-scale visual grounding. Our temporal search mechanism captures event-level continuity by aggregating similarity scores across sequential video segments, enabling coherent retrieval of multi-event queries. Complementarily, a Google Image Search-based fallback module expands query representations with external web imagery, effectively bridging gaps in pretrained visual embeddings and improving robustness against out-of-distribution (OOD) queries. Together, these components advance the temporal rea- soning and generalization capabilities of modern video retrieval systems, paving the way for more semantically aware and adaptive retrieval across large-scale video corpora.",
            "headline_zh": "提出MADTempo框架，通过时序搜索与查询增强解决多事件视频检索中的时序依赖和OOD查询问题。",
            "intro_zh": [
                "核心问题：现有方法难以建模多事件间的时序依赖，且对未见或罕见视觉概念的查询处理不足。",
                "方法要点：结合时序搜索机制捕获事件连续性，并利用Google图像搜索模块增强查询表示以提升泛化能力。",
                "实验或效果：系统提升了视频检索的时序推理和泛化能力，适用于大规模视频库的语义感知检索。"
            ],
            "tags_zh": [
                "多事件视频检索",
                "时序搜索",
                "查询增强",
                "视觉基础",
                "泛化能力",
                "大规模视频库"
            ],
            "_index": 225
        },
        {
            "title": "Sharpness-aware Dynamic Anchor Selection for Generalized Category Discovery",
            "authors": [
                "Zhimao Peng",
                "Enguang Wang",
                "Fei Yang",
                "Xialei Liu",
                "Ming-Ming Cheng"
            ],
            "arxiv_id": "2512.12925v1",
            "summary": "Generalized category discovery (GCD) is an important and challenging task in open-world learning. Specifically, given some labeled data of known classes, GCD aims to cluster unlabeled data that contain both known and unknown classes. Current GCD methods based on parametric classification adopt the DINO-like pseudo-labeling strategy, where the sharpened probability output of one view is used as supervision information for the other view. However, large pre-trained models have a preference for some specific visual patterns, resulting in encoding spurious correlation for unlabeled data and generating noisy pseudo-labels. To address this issue, we propose a novel method, which contains two modules: Loss Sharpness Penalty (LSP) and Dynamic Anchor Selection (DAS). LSP enhances the robustness of model parameters to small perturbations by minimizing the worst-case loss sharpness of the model, which suppressing the encoding of trivial features, thereby reducing overfitting of noise samples and improving the quality of pseudo-labels. Meanwhile, DAS selects representative samples for the unknown classes based on KNN density and class probability during the model training and assigns hard pseudo-labels to them, which not only alleviates the confidence difference between known and unknown classes but also enables the model to quickly learn more accurate feature distribution for the unknown classes, thus further improving the clustering accuracy. Extensive experiments demonstrate that the proposed method can effectively mitigate the noise of pseudo-labels, and achieve state-of-the-art results on multiple GCD benchmarks.",
            "headline_zh": "提出Sharpness-aware Dynamic Anchor Selection以解决广义类别发现中伪标签噪声问题",
            "intro_zh": [
                "核心问题：大预训练模型偏好特定视觉模式，导致未标记数据编码伪相关和生成噪声伪标签。",
                "方法要点：引入损失锐度惩罚增强模型鲁棒性，动态锚点选择基于KNN密度和类概率选取未知类代表样本。",
                "实验或效果：在多个GCD基准测试中实现最先进结果，有效减轻伪标签噪声。"
            ],
            "tags_zh": [
                "广义类别发现",
                "伪标签噪声",
                "损失锐度惩罚",
                "动态锚点选择",
                "开放世界学习"
            ],
            "_index": 226
        },
        {
            "title": "LLM-based Personalized Portfolio Recommender: Integrating Large Language Models and Reinforcement Learning for Intelligent Investment Strategy Optimization",
            "authors": [
                "Bangyu Li",
                "Boping Gu",
                "Ziyang Ding"
            ],
            "arxiv_id": "2512.12922v1",
            "summary": "In modern financial markets, investors increasingly seek personalized and adaptive portfolio strategies that reflect their individual risk preferences and respond to dynamic market conditions. Traditional rule-based or static optimization approaches often fail to capture the nonlinear interactions among investor behavior, market volatility, and evolving financial objectives. To address these limitations, this paper introduces the LLM-based Personalized Portfolio Recommender , an integrated framework that combines Large Language Models, reinforcement learning, and individualized risk preference modeling to support intelligent investment decision-making.",
            "headline_zh": "提出基于大语言模型和强化学习的个性化投资组合推荐框架，以优化智能投资策略。",
            "intro_zh": [
                "核心问题：传统投资策略难以适应个性化风险偏好和动态市场变化。",
                "方法要点：结合大语言模型、强化学习和个体风险偏好建模。",
                "实验或效果：未知。"
            ],
            "tags_zh": [
                "个性化投资组合推荐",
                "大语言模型",
                "强化学习",
                "风险偏好建模",
                "智能投资决策"
            ],
            "_index": 227
        },
        {
            "title": "Cisco Integrated AI Security and Safety Framework Report",
            "authors": [
                "Amy Chang",
                "Tiffany Saade",
                "Sanket Mendapara",
                "Adam Swanda",
                "Ankit Garg"
            ],
            "arxiv_id": "2512.12921v1",
            "summary": "Artificial intelligence (AI) systems are being readily and rapidly adopted, increasingly permeating critical domains: from consumer platforms and enterprise software to networked systems with embedded agents. While this has unlocked potential for human productivity gains, the attack surface has expanded accordingly: threats now span content safety failures (e.g., harmful or deceptive outputs), model and data integrity compromise (e.g., poisoning, supply-chain tampering), runtime manipulations (e.g., prompt injection, tool and agent misuse), and ecosystem risks (e.g., orchestration abuse, multi-agent collusion). Existing frameworks such as MITRE ATLAS, National Institute of Standards and Technology (NIST) AI 100-2 Adversarial Machine Learning (AML) taxonomy, and OWASP Top 10s for Large Language Models (LLMs) and Agentic AI Applications provide valuable viewpoints, but each covers only slices of this multi-dimensional space.\n  This paper presents Cisco's Integrated AI Security and Safety Framework (\"AI Security Framework\"), a unified, lifecycle-aware taxonomy and operationalization framework that can be used to classify, integrate, and operationalize the full range of AI risks. It integrates AI security and AI safety across modalities, agents, pipelines, and the broader ecosystem. The AI Security Framework is designed to be practical for threat identification, red-teaming, risk prioritization, and it is comprehensive in scope and can be extensible to emerging deployments in multimodal contexts, humanoids, wearables, and sensory infrastructures. We analyze gaps in prevailing frameworks, discuss design principles for our framework, and demonstrate how the taxonomy provides structure for understanding how modern AI systems fail, how adversaries exploit these failures, and how organizations can build defenses across the AI lifecycle that evolve alongside capability advancements.",
            "headline_zh": "提出思科集成AI安全与安全框架，以统一分类和操作化AI风险，覆盖全生命周期和多模态部署。",
            "intro_zh": [
                "核心问题：AI系统快速普及导致攻击面扩大，威胁包括内容安全失败、模型完整性受损、运行时操纵和生态系统风险。",
                "方法要点：设计统一、生命周期感知的分类和操作化框架，整合现有框架如MITRE ATLAS和NIST AI 100-2，覆盖多模态、代理和管道。",
                "实验或效果：分析现有框架的差距，讨论设计原则，展示分类如何帮助理解AI系统失败、对手利用方式，并构建防御措施。"
            ],
            "tags_zh": [
                "AI安全框架",
                "生命周期风险管理",
                "多模态AI部署",
                "威胁分类",
                "操作化防御",
                "生态系统安全"
            ],
            "_index": 228
        },
        {
            "title": "Satisfiability Modulo Theory Meets Inductive Logic Programming",
            "authors": [
                "Nijesh Upreti",
                "Vaishak Belle"
            ],
            "arxiv_id": "2512.12918v1",
            "summary": "Inductive Logic Programming (ILP) provides interpretable rule learning in relational domains, yet remains limited in its ability to induce and reason with numerical constraints. Classical ILP systems operate over discrete predicates and typically rely on discretisation or hand-crafted numerical predicates, making it difficult to infer thresholds or arithmetic relations that must hold jointly across examples. Recent work has begun to address these limitations through tighter integrations of ILP with Satisfiability Modulo Theories (SMT) or specialised numerical inference mechanisms. In this paper we investigate a modular alternative that couples the ILP system PyGol with the SMT solver Z3. Candidate clauses proposed by PyGol are interpreted as quantifier-free formulas over background theories such as linear or nonlinear real arithmetic, allowing numerical parameters to be instantiated and verified by the SMT solver while preserving ILP's declarative relational bias. This supports the induction of hybrid rules that combine symbolic predicates with learned numerical constraints, including thresholds, intervals, and multi-literal arithmetic relations. We formalise this SMT-ILP setting and evaluate it on a suite of synthetic datasets designed to probe linear, relational, nonlinear, and multi-hop reasoning. The results illustrate how a modular SMT-ILP architecture can extend the expressivity of symbolic rule learning, complementing prior numerical ILP approaches while providing a flexible basis for future extensions toward richer theory-aware induction.",
            "headline_zh": "提出SMT-ILP模块化架构以增强归纳逻辑编程在数值约束学习中的表达能力",
            "intro_zh": [
                "归纳逻辑编程在关系域中学习可解释规则，但难以处理数值约束和算术关系。",
                "通过耦合PyGol与Z3，将候选子句解释为无量化公式，支持符号谓词与数值约束的混合规则学习。",
                "在合成数据集上评估，展示该架构在线性、非线性及多跳推理中的扩展表达能力。"
            ],
            "tags_zh": [
                "归纳逻辑编程",
                "可满足性模理论",
                "数值约束学习",
                "混合规则学习",
                "模块化架构",
                "符号推理"
            ],
            "_index": 229
        },
        {
            "title": "CTIGuardian: A Few-Shot Framework for Mitigating Privacy Leakage in Fine-Tuned LLMs",
            "authors": [
                "Shashie Dilhara Batan Arachchige",
                "Benjamin Zi Hao Zhao",
                "Hassan Jameel Asghar",
                "Dinusha Vatsalan",
                "Dali Kaafar"
            ],
            "arxiv_id": "2512.12914v1",
            "summary": "Large Language Models (LLMs) are often fine-tuned to adapt their general-purpose knowledge to specific tasks and domains such as cyber threat intelligence (CTI). Fine-tuning is mostly done through proprietary datasets that may contain sensitive information. Owners expect their fine-tuned model to not inadvertently leak this information to potentially adversarial end users. Using CTI as a use case, we demonstrate that data-extraction attacks can recover sensitive information from fine-tuned models on CTI reports, underscoring the need for mitigation. Retraining the full model to eliminate this leakage is computationally expensive and impractical. We propose an alternative approach, which we call privacy alignment, inspired by safety alignment in LLMs. Just like safety alignment teaches the model to abide by safety constraints through a few examples, we enforce privacy alignment through few-shot supervision, integrating a privacy classifier and a privacy redactor, both handled by the same underlying LLM. We evaluate our system, called CTIGuardian, using GPT-4o mini and Mistral-7B Instruct models, benchmarking against Presidio, a named entity recognition (NER) baseline. Results show that CTIGuardian provides a better privacy-utility trade-off than NER based models. While we demonstrate its effectiveness on a CTI use case, the framework is generic enough to be applicable to other sensitive domains.",
            "headline_zh": "提出CTIGuardian框架，通过少样本隐私对齐缓解微调LLMs中的隐私泄露问题",
            "intro_zh": [
                "微调LLMs时，专有数据集中的敏感信息易受数据提取攻击，需高效缓解方法",
                "采用少样本监督实现隐私对齐，集成隐私分类器和重写器，基于同一LLM处理",
                "在CTI用例中评估，相比NER基线，CTIGuardian提供更好的隐私-效用权衡"
            ],
            "tags_zh": [
                "隐私保护",
                "大语言模型微调",
                "少样本学习",
                "数据提取攻击",
                "隐私对齐",
                "网络安全威胁情报"
            ],
            "_index": 230
        },
        {
            "title": "Evaluating Singular Value Thresholds for DNN Weight Matrices based on Random Matrix Theory",
            "authors": [
                "Kohei Nishikawa",
                "Koki Shimizu",
                "Hashiguchi Hiroki"
            ],
            "arxiv_id": "2512.12911v1",
            "summary": "This study evaluates thresholds for removing singular values from singular value decomposition-based low-rank approximations of deep neural network weight matrices. Each weight matrix is modeled as the sum of signal and noise matrices. The low-rank approximation is obtained by removing noise-related singular values using a threshold based on random matrix theory. To assess the adequacy of this threshold, we propose an evaluation metric based on the cosine similarity between the singular vectors of the signal and original weight matrices. The proposed metric is used in numerical experiments to compare two threshold estimation methods.",
            "headline_zh": "提出基于随机矩阵理论的奇异值阈值评估方法，用于深度神经网络权重矩阵的低秩近似。",
            "intro_zh": [
                "核心问题：评估奇异值分解低秩近似中去除噪声相关奇异值的阈值是否合适。",
                "方法要点：将权重矩阵建模为信号与噪声之和，基于随机矩阵理论设定阈值，并引入余弦相似度指标评估阈值。",
                "实验或效果：通过数值实验比较两种阈值估计方法，验证所提评估指标的有效性。"
            ],
            "tags_zh": [
                "奇异值分解",
                "低秩近似",
                "随机矩阵理论",
                "深度神经网络",
                "权重矩阵",
                "噪声去除"
            ],
            "_index": 231
        },
        {
            "title": "Machine Learning Architectures for the Estimation of Predicted Occupancy Grids in Road Traffic",
            "authors": [
                "Parthasarathy Nadarajan",
                "Michael Botsch",
                "Sebastian Sardina"
            ],
            "arxiv_id": "2512.12907v1",
            "summary": "This paper introduces a novel machine learning architecture for an efficient estimation of the probabilistic space-time representation of complex traffic scenarios. A detailed representation of the future traffic scenario is of significant importance for autonomous driving and for all active safety systems. In order to predict the future space-time representation of the traffic scenario, first the type of traffic scenario is identified and then the machine learning algorithm maps the current state of the scenario to possible future states. The input to the machine learning algorithms is the current state representation of a traffic scenario, termed as the Augmented Occupancy Grid (AOG). The output is the probabilistic space-time representation which includes uncertainties regarding the behaviour of the traffic participants and is termed as the Predicted Occupancy Grid (POG). The novel architecture consists of two Stacked Denoising Autoencoders (SDAs) and a set of Random Forests. It is then compared with the other two existing architectures that comprise of SDAs and DeconvNet. The architectures are validated with the help of simulations and the comparisons are made both in terms of accuracy and computational time. Also, a brief overview on the applications of POGs in the field of active safety is presented.",
            "headline_zh": "提出基于堆叠去噪自编码器和随机森林的机器学习架构，用于高效估计道路交通场景的预测占用网格。",
            "intro_zh": [
                "核心问题：预测复杂交通场景的未来时空表示，对自动驾驶和主动安全系统至关重要。",
                "方法要点：先识别交通场景类型，再通过机器学习将当前状态映射到未来状态，输入为增强占用网格，输出为预测占用网格。",
                "实验或效果：与现有架构比较，在模拟中验证准确性和计算时间，并概述预测占用网格在主动安全中的应用。"
            ],
            "tags_zh": [
                "预测占用网格",
                "堆叠去噪自编码器",
                "随机森林",
                "自动驾驶",
                "交通场景预测",
                "主动安全系统"
            ],
            "_index": 232
        },
        {
            "title": "Predictive Sample Assignment for Semantically Coherent Out-of-Distribution Detection",
            "authors": [
                "Zhimao Peng",
                "Enguang Wang",
                "Xialei Liu",
                "Ming-Ming Cheng"
            ],
            "arxiv_id": "2512.12906v1",
            "summary": "Semantically coherent out-of-distribution detection (SCOOD) is a recently proposed realistic OOD detection setting: given labeled in-distribution (ID) data and mixed in-distribution and out-of-distribution unlabeled data as the training data, SCOOD aims to enable the trained model to accurately identify OOD samples in the testing data. Current SCOOD methods mainly adopt various clustering-based in-distribution sample filtering (IDF) strategies to select clean ID samples from unlabeled data, and take the remaining samples as auxiliary OOD data, which inevitably introduces a large number of noisy samples in training. To address the above issue, we propose a concise SCOOD framework based on predictive sample assignment (PSA). PSA includes a dual-threshold ternary sample assignment strategy based on the predictive energy score that can significantly improve the purity of the selected ID and OOD sample sets by assigning unconfident unlabeled data to an additional discard sample set, and a concept contrastive representation learning loss to further expand the distance between ID and OOD samples in the representation space to assist ID/OOD discrimination. In addition, we also introduce a retraining strategy to help the model fully fit the selected auxiliary ID/OOD samples. Experiments on two standard SCOOD benchmarks demonstrate that our approach outperforms the state-of-the-art methods by a significant margin.",
            "headline_zh": "提出基于预测样本分配的语义一致分布外检测框架，以解决训练中噪声样本问题",
            "intro_zh": [
                "核心问题：现有语义一致分布外检测方法通过聚类过滤引入大量噪声样本，影响模型性能",
                "方法要点：采用双阈值三元样本分配策略提升样本集纯度，并结合概念对比表示学习损失增强分布内外样本区分",
                "实验或效果：在两个标准基准上显著超越现有方法，验证了框架的有效性"
            ],
            "tags_zh": [
                "语义一致分布外检测",
                "预测样本分配",
                "三元样本分配",
                "概念对比学习",
                "分布外检测",
                "表示学习"
            ],
            "_index": 233
        },
        {
            "title": "PAC-Bayes Bounds for Multivariate Linear Regression and Linear Autoencoders",
            "authors": [
                "Ruixin Guo",
                "Ruoming Jin",
                "Xinyu Li",
                "Yang Zhou"
            ],
            "arxiv_id": "2512.12905v1",
            "summary": "Linear Autoencoders (LAEs) have shown strong performance in state-of-the-art recommender systems. However, this success remains largely empirical, with limited theoretical understanding. In this paper, we investigate the generalizability -- a theoretical measure of model performance in statistical learning -- of multivariate linear regression and LAEs. We first propose a PAC-Bayes bound for multivariate linear regression, extending the earlier bound for single-output linear regression by Shalaeva et al., and establish sufficient conditions for its convergence. We then show that LAEs, when evaluated under a relaxed mean squared error, can be interpreted as constrained multivariate linear regression models on bounded data, to which our bound adapts. Furthermore, we develop theoretical methods to improve the computational efficiency of optimizing the LAE bound, enabling its practical evaluation on large models and real-world datasets. Experimental results demonstrate that our bound is tight and correlates well with practical ranking metrics such as Recall@K and NDCG@K.",
            "headline_zh": "提出多元线性回归与线性自编码器的PAC-Bayes泛化界，以增强推荐系统理论理解。",
            "intro_zh": [
                "研究多元线性回归和线性自编码器的泛化性，填补理论空白。",
                "扩展单输出线性回归的PAC-Bayes界，建立收敛条件并应用于线性自编码器。",
                "实验显示界紧致，与召回率和NDCG等实用排名指标相关良好。"
            ],
            "tags_zh": [
                "PAC-Bayes界",
                "多元线性回归",
                "线性自编码器",
                "泛化理论",
                "推荐系统",
                "统计学习"
            ],
            "_index": 234
        },
        {
            "title": "Next-generation reservoir computing validated by classification task",
            "authors": [
                "Ken-ichi Kitayama"
            ],
            "arxiv_id": "2512.12903v1",
            "summary": "An emerging computing paradigm, so-called next-generation reservoir computing (NG-RC) is investigated. True to its namesake, NG-RC requires no actual reservoirs for input data mixing but rather computing the polynomial terms directly from the time series inputs. However, benchmark tests so far reported have been one-sided, limited to prediction tasks of temporal waveforms such as Lorenz 63 attractor and Mackey-Glass chaotic signal. We will demonstrate for the first time that NG-RC can perform classification task as good as conventional RC. This validates the versatile computational capability of NG-RC in tasks of both prediction and classification.",
            "headline_zh": "验证下一代储层计算在分类任务中的性能，扩展其应用场景",
            "intro_zh": [
                "核心问题：现有NG-RC基准测试局限于预测任务，缺乏分类任务验证",
                "方法要点：NG-RC无需实际储层，直接从时间序列输入计算多项式项",
                "实验或效果：首次证明NG-RC在分类任务中性能与传统储层计算相当"
            ],
            "tags_zh": [
                "下一代储层计算",
                "分类任务",
                "时间序列分析",
                "计算范式",
                "基准测试"
            ],
            "_index": 235
        }
    ]
}