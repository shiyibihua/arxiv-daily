{
    "papers": [
        {
            "title": "Coupled Diffusion Sampling for Training-Free Multi-View Image Editing",
            "authors": [
                "Hadi Alzayer",
                "Yunzhi Zhang",
                "Chen Geng",
                "Jia-Bin Huang",
                "Jiajun Wu"
            ],
            "arxiv_id": "2510.14981v1",
            "summary": "We present an inference-time diffusion sampling method to perform multi-view\nconsistent image editing using pre-trained 2D image editing models. These\nmodels can independently produce high-quality edits for each image in a set of\nmulti-view images of a 3D scene or object, but they do not maintain consistency\nacross views. Existing approaches typically address this by optimizing over\nexplicit 3D representations, but they suffer from a lengthy optimization\nprocess and instability under sparse view settings. We propose an implicit 3D\nregularization approach by constraining the generated 2D image sequences to\nadhere to a pre-trained multi-view image distribution. This is achieved through\ncoupled diffusion sampling, a simple diffusion sampling technique that\nconcurrently samples two trajectories from both a multi-view image distribution\nand a 2D edited image distribution, using a coupling term to enforce the\nmulti-view consistency among the generated images. We validate the\neffectiveness and generality of this framework on three distinct multi-view\nimage editing tasks, demonstrating its applicability across various model\narchitectures and highlighting its potential as a general solution for\nmulti-view consistent editing.",
            "headline_zh": "提出耦合扩散采样方法，实现免训练的多视角图像编辑一致性",
            "intro_zh": [
                "核心问题：预训练2D编辑模型在多视角图像编辑中缺乏跨视角一致性",
                "方法要点：通过耦合扩散采样，约束生成图像序列符合多视角分布",
                "实验或效果：在三个多视角编辑任务中验证有效性和通用性"
            ],
            "tags_zh": [
                "多视角图像编辑",
                "扩散模型",
                "一致性约束",
                "免训练方法",
                "图像生成"
            ],
            "_index": 0
        },
        {
            "title": "From Pixels to Words -- Towards Native Vision-Language Primitives at Scale",
            "authors": [
                "Haiwen Diao",
                "Mingxuan Li",
                "Silei Wu",
                "Linjun Dai",
                "Xiaohua Wang",
                "Hanming Deng",
                "Lewei Lu",
                "Dahua Lin",
                "Ziwei Liu"
            ],
            "arxiv_id": "2510.14979v1",
            "summary": "The edifice of native Vision-Language Models (VLMs) has emerged as a rising\ncontender to typical modular VLMs, shaped by evolving model architectures and\ntraining paradigms. Yet, two lingering clouds cast shadows over its widespread\nexploration and promotion: (-) What fundamental constraints set native VLMs\napart from modular ones, and to what extent can these barriers be overcome? (-)\nHow to make research in native VLMs more accessible and democratized, thereby\naccelerating progress in the field. In this paper, we clarify these challenges\nand outline guiding principles for constructing native VLMs. Specifically, one\nnative VLM primitive should: (i) effectively align pixel and word\nrepresentations within a shared semantic space; (ii) seamlessly integrate the\nstrengths of formerly separate vision and language modules; (iii) inherently\nembody various cross-modal properties that support unified vision-language\nencoding, aligning, and reasoning. Hence, we launch NEO, a novel family of\nnative VLMs built from first principles, capable of rivaling top-tier modular\ncounterparts across diverse real-world scenarios. With only 390M image-text\nexamples, NEO efficiently develops visual perception from scratch while\nmitigating vision-language conflicts inside a dense and monolithic model\ncrafted from our elaborate primitives. We position NEO as a cornerstone for\nscalable and powerful native VLMs, paired with a rich set of reusable\ncomponents that foster a cost-effective and extensible ecosystem. Our code and\nmodels are publicly available at: https://github.com/EvolvingLMMs-Lab/NEO.",
            "headline_zh": "提出NEO原生视觉语言模型以解决视觉语言对齐与集成问题",
            "intro_zh": [
                "核心问题：原生视觉语言模型与模块化模型的区别及可访问性挑战",
                "方法要点：定义原生VLM原则，包括像素-词对齐、模块集成和跨模态属性",
                "实验或效果：NEO模型在390M图像-文本数据上实现高效视觉感知，媲美顶级模块化模型"
            ],
            "tags_zh": [
                "原生视觉语言模型",
                "像素-词对齐",
                "跨模态推理",
                "模型架构",
                "视觉语言集成",
                "可扩展生态系统"
            ],
            "_index": 1
        },
        {
            "title": "Agentic Design of Compositional Machines",
            "authors": [
                "Wenqian Zhang",
                "Weiyang Liu",
                "Zhen Liu"
            ],
            "arxiv_id": "2510.14980v1",
            "summary": "The design of complex machines stands as both a marker of human intelligence\nand a foundation of engineering practice. Given recent advances in large\nlanguage models (LLMs), we ask whether they, too, can learn to create. We\napproach this question through the lens of compositional machine design: a task\nin which machines are assembled from standardized components to meet functional\ndemands like locomotion or manipulation in a simulated physical environment. To\nsupport this investigation, we introduce BesiegeField, a testbed built on the\nmachine-building game Besiege, which enables part-based construction, physical\nsimulation and reward-driven evaluation. Using BesiegeField, we benchmark\nstate-of-the-art LLMs with agentic workflows and identify key capabilities\nrequired for success, including spatial reasoning, strategic assembly, and\ninstruction-following. As current open-source models fall short, we explore\nreinforcement learning (RL) as a path to improvement: we curate a cold-start\ndataset, conduct RL finetuning experiments, and highlight open challenges at\nthe intersection of language, machine design, and physical reasoning.",
            "headline_zh": "提出BesiegeField测试床和RL微调方法，以提升LLMs在组合机器设计中的能力。",
            "intro_zh": [
                "核心问题：LLMs能否学习在模拟物理环境中设计组合机器以满足功能需求。",
                "方法要点：引入BesiegeField测试床，支持基于部件的构建、物理模拟和奖励评估。",
                "实验或效果：基准测试LLMs，识别关键能力不足，探索RL微调以改进性能。"
            ],
            "tags_zh": [
                "组合机器设计",
                "大型语言模型",
                "物理模拟",
                "强化学习微调",
                "测试床构建"
            ],
            "_index": 2
        },
        {
            "title": "Learning an Image Editing Model without Image Editing Pairs",
            "authors": [
                "Nupur Kumari",
                "Sheng-Yu Wang",
                "Nanxuan Zhao",
                "Yotam Nitzan",
                "Yuheng Li",
                "Krishna Kumar Singh",
                "Richard Zhang",
                "Eli Shechtman",
                "Jun-Yan Zhu",
                "Xun Huang"
            ],
            "arxiv_id": "2510.14978v1",
            "summary": "Recent image editing models have achieved impressive results while following\nnatural language editing instructions, but they rely on supervised fine-tuning\nwith large datasets of input-target pairs. This is a critical bottleneck, as\nsuch naturally occurring pairs are hard to curate at scale. Current workarounds\nuse synthetic training pairs that leverage the zero-shot capabilities of\nexisting models. However, this can propagate and magnify the artifacts of the\npretrained model into the final trained model. In this work, we present a new\ntraining paradigm that eliminates the need for paired data entirely. Our\napproach directly optimizes a few-step diffusion model by unrolling it during\ntraining and leveraging feedback from vision-language models (VLMs). For each\ninput and editing instruction, the VLM evaluates if an edit follows the\ninstruction and preserves unchanged content, providing direct gradients for\nend-to-end optimization. To ensure visual fidelity, we incorporate distribution\nmatching loss (DMD), which constrains generated images to remain within the\nimage manifold learned by pretrained models. We evaluate our method on standard\nbenchmarks and include an extensive ablation study. Without any paired data,\nour method performs on par with various image editing diffusion models trained\non extensive supervised paired data, under the few-step setting. Given the same\nVLM as the reward model, we also outperform RL-based techniques like Flow-GRPO.",
            "headline_zh": "提出无配对数据图像编辑训练范式，利用视觉语言模型反馈优化扩散模型",
            "intro_zh": [
                "核心问题：图像编辑模型依赖大规模输入-目标配对数据，难以获取且易传播预训练模型伪影",
                "方法要点：通过展开扩散模型训练，结合视觉语言模型评估编辑指令遵循和内容保留，提供端到端梯度优化",
                "实验或效果：在标准基准测试中，无配对数据下性能与监督训练模型相当，优于基于强化学习的方法"
            ],
            "tags_zh": [
                "图像编辑",
                "无配对训练",
                "扩散模型",
                "视觉语言模型",
                "分布匹配损失",
                "端到端优化"
            ],
            "_index": 3
        },
        {
            "title": "Ponimator: Unfolding Interactive Pose for Versatile Human-human Interaction Animation",
            "authors": [
                "Shaowei Liu",
                "Chuan Guo",
                "Bing Zhou",
                "Jian Wang"
            ],
            "arxiv_id": "2510.14976v1",
            "summary": "Close-proximity human-human interactive poses convey rich contextual\ninformation about interaction dynamics. Given such poses, humans can\nintuitively infer the context and anticipate possible past and future dynamics,\ndrawing on strong priors of human behavior. Inspired by this observation, we\npropose Ponimator, a simple framework anchored on proximal interactive poses\nfor versatile interaction animation. Our training data consists of\nclose-contact two-person poses and their surrounding temporal context from\nmotion-capture interaction datasets. Leveraging interactive pose priors,\nPonimator employs two conditional diffusion models: (1) a pose animator that\nuses the temporal prior to generate dynamic motion sequences from interactive\nposes, and (2) a pose generator that applies the spatial prior to synthesize\ninteractive poses from a single pose, text, or both when interactive poses are\nunavailable. Collectively, Ponimator supports diverse tasks, including\nimage-based interaction animation, reaction animation, and text-to-interaction\nsynthesis, facilitating the transfer of interaction knowledge from high-quality\nmocap data to open-world scenarios. Empirical experiments across diverse\ndatasets and applications demonstrate the universality of the pose prior and\nthe effectiveness and robustness of our framework.",
            "headline_zh": "提出Ponimator框架，基于近距交互姿态实现多样化人-人交互动画",
            "intro_zh": [
                "核心问题：如何从近距人-人交互姿态推断和生成动态交互动画",
                "方法要点：使用两个条件扩散模型，分别处理时空先验生成运动和姿态",
                "实验效果：在多个数据集验证框架通用性、有效性和鲁棒性"
            ],
            "tags_zh": [
                "人-人交互动画",
                "条件扩散模型",
                "姿态先验",
                "运动生成",
                "文本到交互合成"
            ],
            "_index": 4
        },
        {
            "title": "Terra: Explorable Native 3D World Model with Point Latents",
            "authors": [
                "Yuanhui Huang",
                "Weiliang Chen",
                "Wenzhao Zheng",
                "Xin Tao",
                "Pengfei Wan",
                "Jie Zhou",
                "Jiwen Lu"
            ],
            "arxiv_id": "2510.14977v1",
            "summary": "World models have garnered increasing attention for comprehensive modeling of\nthe real world. However, most existing methods still rely on pixel-aligned\nrepresentations as the basis for world evolution, neglecting the inherent 3D\nnature of the physical world. This could undermine the 3D consistency and\ndiminish the modeling efficiency of world models. In this paper, we present\nTerra, a native 3D world model that represents and generates explorable\nenvironments in an intrinsic 3D latent space. Specifically, we propose a novel\npoint-to-Gaussian variational autoencoder (P2G-VAE) that encodes 3D inputs into\na latent point representation, which is subsequently decoded as 3D Gaussian\nprimitives to jointly model geometry and appearance. We then introduce a sparse\npoint flow matching network (SPFlow) for generating the latent point\nrepresentation, which simultaneously denoises the positions and features of the\npoint latents. Our Terra enables exact multi-view consistency with native 3D\nrepresentation and architecture, and supports flexible rendering from any\nviewpoint with only a single generation process. Furthermore, Terra achieves\nexplorable world modeling through progressive generation in the point latent\nspace. We conduct extensive experiments on the challenging indoor scenes from\nScanNet v2. Terra achieves state-of-the-art performance in both reconstruction\nand generation with high 3D consistency.",
            "headline_zh": "提出Terra原生3D世界模型，以点潜在表示解决3D一致性与建模效率问题。",
            "intro_zh": [
                "现有世界模型依赖像素对齐表示，忽略物理世界3D本质，影响一致性与效率。",
                "引入P2G-VAE编码3D输入为点潜在，解码为3D高斯基元建模几何与外观。",
                "在ScanNet v2上实验，Terra实现多视图一致，重建与生成性能领先。"
            ],
            "tags_zh": [
                "3D世界模型",
                "点潜在表示",
                "变分自编码器",
                "高斯基元",
                "多视图一致性",
                "室内场景生成"
            ],
            "_index": 5
        },
        {
            "title": "WithAnyone: Towards Controllable and ID Consistent Image Generation",
            "authors": [
                "Hengyuan Xu",
                "Wei Cheng",
                "Peng Xing",
                "Yixiao Fang",
                "Shuhan Wu",
                "Rui Wang",
                "Xianfang Zeng",
                "Daxin Jiang",
                "Gang Yu",
                "Xingjun Ma",
                "Yu-Gang Jiang"
            ],
            "arxiv_id": "2510.14975v1",
            "summary": "Identity-consistent generation has become an important focus in text-to-image\nresearch, with recent models achieving notable success in producing images\naligned with a reference identity. Yet, the scarcity of large-scale paired\ndatasets containing multiple images of the same individual forces most\napproaches to adopt reconstruction-based training. This reliance often leads to\na failure mode we term copy-paste, where the model directly replicates the\nreference face rather than preserving identity across natural variations in\npose, expression, or lighting. Such over-similarity undermines controllability\nand limits the expressive power of generation. To address these limitations, we\n(1) construct a large-scale paired dataset MultiID-2M, tailored for\nmulti-person scenarios, providing diverse references for each identity; (2)\nintroduce a benchmark that quantifies both copy-paste artifacts and the\ntrade-off between identity fidelity and variation; and (3) propose a novel\ntraining paradigm with a contrastive identity loss that leverages paired data\nto balance fidelity with diversity. These contributions culminate in\nWithAnyone, a diffusion-based model that effectively mitigates copy-paste while\npreserving high identity similarity. Extensive qualitative and quantitative\nexperiments demonstrate that WithAnyone significantly reduces copy-paste\nartifacts, improves controllability over pose and expression, and maintains\nstrong perceptual quality. User studies further validate that our method\nachieves high identity fidelity while enabling expressive controllable\ngeneration.",
            "headline_zh": "提出WithAnyone模型以解决身份一致图像生成中的复制粘贴问题",
            "intro_zh": [
                "核心问题：现有模型因依赖重建训练，易产生复制粘贴，限制姿态和表情可控性。",
                "方法要点：构建MultiID-2M数据集，引入对比身份损失，平衡身份保真与多样性。",
                "实验或效果：模型显著减少复制粘贴，提升可控性，保持高身份相似和感知质量。"
            ],
            "tags_zh": [
                "身份一致生成",
                "扩散模型",
                "复制粘贴缓解",
                "可控图像生成",
                "对比学习",
                "多身份数据集"
            ],
            "_index": 6
        },
        {
            "title": "pi-Flow: Policy-Based Few-Step Generation via Imitation Distillation",
            "authors": [
                "Hansheng Chen",
                "Kai Zhang",
                "Hao Tan",
                "Leonidas Guibas",
                "Gordon Wetzstein",
                "Sai Bi"
            ],
            "arxiv_id": "2510.14974v1",
            "summary": "Few-step diffusion or flow-based generative models typically distill a\nvelocity-predicting teacher into a student that predicts a shortcut towards\ndenoised data. This format mismatch has led to complex distillation procedures\nthat often suffer from a quality-diversity trade-off. To address this, we\npropose policy-based flow models ($\\pi$-Flow). $\\pi$-Flow modifies the output\nlayer of a student flow model to predict a network-free policy at one timestep.\nThe policy then produces dynamic flow velocities at future substeps with\nnegligible overhead, enabling fast and accurate ODE integration on these\nsubsteps without extra network evaluations. To match the policy's ODE\ntrajectory to the teacher's, we introduce a novel imitation distillation\napproach, which matches the policy's velocity to the teacher's along the\npolicy's trajectory using a standard $\\ell_2$ flow matching loss. By simply\nmimicking the teacher's behavior, $\\pi$-Flow enables stable and scalable\ntraining and avoids the quality-diversity trade-off. On ImageNet 256$^2$, it\nattains a 1-NFE FID of 2.85, outperforming MeanFlow of the same DiT\narchitecture. On FLUX.1-12B and Qwen-Image-20B at 4 NFEs, $\\pi$-Flow achieves\nsubstantially better diversity than state-of-the-art few-step methods, while\nmaintaining teacher-level quality.",
            "headline_zh": "提出π-Flow策略模型以解决少步生成中的质量-多样性权衡问题",
            "intro_zh": [
                "核心问题：少步扩散或流模型蒸馏中格式不匹配导致复杂蒸馏和质量-多样性权衡",
                "方法要点：修改学生模型输出层为无网络策略，预测动态流速实现快速ODE积分",
                "实验或效果：在ImageNet 256²上1-NFE FID达2.85，优于MeanFlow，4 NFEs下保持教师质量并提升多样性"
            ],
            "tags_zh": [
                "少步生成模型",
                "策略蒸馏",
                "流匹配",
                "ODE积分",
                "质量-多样性权衡",
                "模仿学习"
            ],
            "_index": 7
        },
        {
            "title": "RDD: Retrieval-Based Demonstration Decomposer for Planner Alignment in Long-Horizon Tasks",
            "authors": [
                "Mingxuan Yan",
                "Yuping Wang",
                "Zechun Liu",
                "Jiachen Li"
            ],
            "arxiv_id": "2510.14968v1",
            "summary": "To tackle long-horizon tasks, recent hierarchical vision-language-action\n(VLAs) frameworks employ vision-language model (VLM)-based planners to\ndecompose complex manipulation tasks into simpler sub-tasks that low-level\nvisuomotor policies can easily handle. Typically, the VLM planner is finetuned\nto learn to decompose a target task. This finetuning requires target task\ndemonstrations segmented into sub-tasks by either human annotation or heuristic\nrules. However, the heuristic subtasks can deviate significantly from the\ntraining data of the visuomotor policy, which degrades task performance. To\naddress these issues, we propose a Retrieval-based Demonstration Decomposer\n(RDD) that automatically decomposes demonstrations into sub-tasks by aligning\nthe visual features of the decomposed sub-task intervals with those from the\ntraining data of the low-level visuomotor policies. Our method outperforms the\nstate-of-the-art sub-task decomposer on both simulation and real-world tasks,\ndemonstrating robustness across diverse settings. Code and more results are\navailable at rdd-neurips.github.io.",
            "headline_zh": "提出基于检索的演示分解器以对齐长视野任务中的规划器",
            "intro_zh": [
                "核心问题：VLM规划器微调依赖人工或启发式分解，子任务与低层策略训练数据不匹配，影响性能。",
                "方法要点：RDD通过视觉特征检索自动分解演示，使子任务与低层策略训练数据对齐。",
                "实验或效果：在仿真和真实任务中优于现有分解器，展现跨场景鲁棒性。"
            ],
            "tags_zh": [
                "长视野任务",
                "视觉语言动作框架",
                "演示分解",
                "视觉特征检索",
                "规划器对齐",
                "机器人操作"
            ],
            "_index": 8
        },
        {
            "title": "ChangingGrounding: 3D Visual Grounding in Changing Scenes",
            "authors": [
                "Miao Hu",
                "Zhiwei Huang",
                "Tai Wang",
                "Jiangmiao Pang",
                "Dahua Lin",
                "Nanning Zheng",
                "Runsen Xu"
            ],
            "arxiv_id": "2510.14965v1",
            "summary": "Real-world robots localize objects from natural-language instructions while\nscenes around them keep changing. Yet most of the existing 3D visual grounding\n(3DVG) method still assumes a reconstructed and up-to-date point cloud, an\nassumption that forces costly re-scans and hinders deployment. We argue that\n3DVG should be formulated as an active, memory-driven problem, and we introduce\nChangingGrounding, the first benchmark that explicitly measures how well an\nagent can exploit past observations, explore only where needed, and still\ndeliver precise 3D boxes in changing scenes. To set a strong reference point,\nwe also propose Mem-ChangingGrounder, a zero-shot method for this task that\nmarries cross-modal retrieval with lightweight multi-view fusion: it identifies\nthe object type implied by the query, retrieves relevant memories to guide\nactions, then explores the target efficiently in the scene, falls back when\nprevious operations are invalid, performs multi-view scanning of the target,\nand projects the fused evidence from multi-view scans to get accurate object\nbounding boxes. We evaluate different baselines on ChangingGrounding, and our\nMem-ChangingGrounder achieves the highest localization accuracy while greatly\nreducing exploration cost. We hope this benchmark and method catalyze a shift\ntoward practical, memory-centric 3DVG research for real-world applications.\nProject page: https://hm123450.github.io/CGB/ .",
            "headline_zh": "提出ChangingGrounding基准与Mem-ChangingGrounder方法，以解决动态场景中的3D视觉定位问题。",
            "intro_zh": [
                "核心问题：现有3D视觉定位方法依赖更新点云，在动态场景中成本高且不实用。",
                "方法要点：结合跨模态检索与轻量多视图融合，实现零样本目标定位与高效探索。",
                "实验或效果：在ChangingGrounding基准上，Mem-ChangingGrounder定位精度最高且探索成本显著降低。"
            ],
            "tags_zh": [
                "3D视觉定位",
                "动态场景",
                "跨模态检索",
                "多视图融合",
                "零样本学习",
                "基准数据集"
            ],
            "_index": 9
        },
        {
            "title": "RainDiff: End-to-end Precipitation Nowcasting Via Token-wise Attention Diffusion",
            "authors": [
                "Thao Nguyen",
                "Jiaqi Ma",
                "Fahad Shahbaz Khan",
                "Souhaib Ben Taieb",
                "Salman Khan"
            ],
            "arxiv_id": "2510.14962v1",
            "summary": "Precipitation nowcasting, predicting future radar echo sequences from current\nobservations, is a critical yet challenging task due to the inherently chaotic\nand tightly coupled spatio-temporal dynamics of the atmosphere. While recent\nadvances in diffusion-based models attempt to capture both large-scale motion\nand fine-grained stochastic variability, they often suffer from scalability\nissues: latent-space approaches require a separately trained autoencoder,\nadding complexity and limiting generalization, while pixel-space approaches are\ncomputationally intensive and often omit attention mechanisms, reducing their\nability to model long-range spatio-temporal dependencies. To address these\nlimitations, we propose a Token-wise Attention integrated into not only the\nU-Net diffusion model but also the spatio-temporal encoder that dynamically\ncaptures multi-scale spatial interactions and temporal evolution. Unlike prior\napproaches, our method natively integrates attention into the architecture\nwithout incurring the high resource cost typical of pixel-space diffusion,\nthereby eliminating the need for separate latent modules. Our extensive\nexperiments and visual evaluations across diverse datasets demonstrate that the\nproposed method significantly outperforms state-of-the-art approaches, yielding\nsuperior local fidelity, generalization, and robustness in complex\nprecipitation forecasting scenarios.",
            "headline_zh": "提出RainDiff方法，通过令牌注意力扩散模型解决降水临近预报中的可扩展性问题。",
            "intro_zh": [
                "降水临近预报面临混沌时空动态和可扩展性挑战，如潜在空间方法复杂、像素空间方法计算密集。",
                "方法将令牌注意力集成到U-Net扩散模型和时空编码器中，动态捕获多尺度时空依赖，无需额外模块。",
                "实验显示在多个数据集上优于现有方法，提升局部保真度、泛化性和鲁棒性。"
            ],
            "tags_zh": [
                "降水临近预报",
                "扩散模型",
                "令牌注意力",
                "时空编码",
                "U-Net架构",
                "多尺度建模"
            ],
            "_index": 10
        },
        {
            "title": "C4D: 4D Made from 3D through Dual Correspondences",
            "authors": [
                "Shizun Wang",
                "Zhenxiang Jiang",
                "Xingyi Yang",
                "Xinchao Wang"
            ],
            "arxiv_id": "2510.14960v1",
            "summary": "Recovering 4D from monocular video, which jointly estimates dynamic geometry\nand camera poses, is an inevitably challenging problem. While recent\npointmap-based 3D reconstruction methods (e.g., DUSt3R) have made great\nprogress in reconstructing static scenes, directly applying them to dynamic\nscenes leads to inaccurate results. This discrepancy arises because moving\nobjects violate multi-view geometric constraints, disrupting the\nreconstruction. To address this, we introduce C4D, a framework that leverages\ntemporal Correspondences to extend existing 3D reconstruction formulation to\n4D. Specifically, apart from predicting pointmaps, C4D captures two types of\ncorrespondences: short-term optical flow and long-term point tracking. We train\na dynamic-aware point tracker that provides additional mobility information,\nfacilitating the estimation of motion masks to separate moving elements from\nthe static background, thus offering more reliable guidance for dynamic scenes.\nFurthermore, we introduce a set of dynamic scene optimization objectives to\nrecover per-frame 3D geometry and camera parameters. Simultaneously, the\ncorrespondences lift 2D trajectories into smooth 3D trajectories, enabling\nfully integrated 4D reconstruction. Experiments show that our framework\nachieves complete 4D recovery and demonstrates strong performance across\nmultiple downstream tasks, including depth estimation, camera pose estimation,\nand point tracking. Project Page: https://littlepure2333.github.io/C4D",
            "headline_zh": "提出C4D框架，通过双对应关系从单目视频恢复4D动态场景",
            "intro_zh": [
                "核心问题：单目视频中动态物体破坏多视图几何约束，导致4D重建不准确",
                "方法要点：结合短时光流和长时点跟踪，估计运动掩码并优化动态场景目标",
                "实验效果：在深度估计、相机姿态估计和点跟踪等任务中表现优异"
            ],
            "tags_zh": [
                "4D重建",
                "单目视频",
                "点跟踪",
                "动态场景优化",
                "运动掩码"
            ],
            "_index": 11
        },
        {
            "title": "MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal Mathematical Reasoning",
            "authors": [
                "Weikang Shi",
                "Aldrich Yu",
                "Rongyao Fang",
                "Houxing Ren",
                "Ke Wang",
                "Aojun Zhou",
                "Changyao Tian",
                "Xinyu Fu",
                "Yuxuan Hu",
                "Zimu Lu",
                "Linjiang Huang",
                "Si Liu",
                "Rui Liu",
                "Hongsheng Li"
            ],
            "arxiv_id": "2510.14958v1",
            "summary": "While Large Language Models (LLMs) have excelled in textual reasoning, they\nstruggle with mathematical domains like geometry that intrinsically rely on\nvisual aids. Existing approaches to Visual Chain-of-Thought (VCoT) are often\nlimited by rigid external tools or fail to generate the high-fidelity,\nstrategically-timed diagrams necessary for complex problem-solving. To bridge\nthis gap, we introduce MathCanvas, a comprehensive framework designed to endow\nunified Large Multimodal Models (LMMs) with intrinsic VCoT capabilities for\nmathematics. Our approach consists of two phases. First, a Visual Manipulation\nstage pre-trains the model on a novel 15.2M-pair corpus, comprising 10M\ncaption-to-diagram pairs (MathCanvas-Imagen) and 5.2M step-by-step editing\ntrajectories (MathCanvas-Edit), to master diagram generation and editing.\nSecond, a Strategic Visual-Aided Reasoning stage fine-tunes the model on\nMathCanvas-Instruct, a new 219K-example dataset of interleaved visual-textual\nreasoning paths, teaching it when and how to leverage visual aids. To\nfacilitate rigorous evaluation, we introduce MathCanvas-Bench, a challenging\nbenchmark with 3K problems that require models to produce interleaved\nvisual-textual solutions. Our model, BAGEL-Canvas, trained under this\nframework, achieves an 86% relative improvement over strong LMM baselines on\nMathCanvas-Bench, demonstrating excellent generalization to other public math\nbenchmarks. Our work provides a complete toolkit-framework, datasets, and\nbenchmark-to unlock complex, human-like visual-aided reasoning in LMMs. Project\nPage: https://mathcanvas.github.io/",
            "headline_zh": "提出MathCanvas框架以增强多模态模型在数学推理中的视觉思维链能力",
            "intro_zh": [
                "核心问题：现有视觉思维链方法在数学领域受限于外部工具，难以生成高保真、适时图表。",
                "方法要点：通过视觉操作预训练和策略性视觉辅助推理微调，实现内在视觉思维链。",
                "实验或效果：在MathCanvas-Bench上相对基线提升86%，并泛化至其他数学基准。"
            ],
            "tags_zh": [
                "多模态数学推理",
                "视觉思维链",
                "图表生成",
                "预训练数据集",
                "基准评估"
            ],
            "_index": 12
        },
        {
            "title": "CBF-RL: Safety Filtering Reinforcement Learning in Training with Control Barrier Functions",
            "authors": [
                "Lizhi Yang",
                "Blake Werner",
                "Massimiliano de Sa Aaron D. Ames"
            ],
            "arxiv_id": "2510.14959v1",
            "summary": "Reinforcement learning (RL), while powerful and expressive, can often\nprioritize performance at the expense of safety. Yet safety violations can lead\nto catastrophic outcomes in real-world deployments. Control Barrier Functions\n(CBFs) offer a principled method to enforce dynamic safety -- traditionally\ndeployed \\emph{online} via safety filters. While the result is safe behavior,\nthe fact that the RL policy does not have knowledge of the CBF can lead to\nconservative behaviors. This paper proposes CBF-RL, a framework for generating\nsafe behaviors with RL by enforcing CBFs \\emph{in training}. CBF-RL has two key\nattributes: (1) minimally modifying a nominal RL policy to encode safety\nconstraints via a CBF term, (2) and safety filtering of the policy rollouts in\ntraining. Theoretically, we prove that continuous-time safety filters can be\ndeployed via closed-form expressions on discrete-time roll-outs. Practically,\nwe demonstrate that CBF-RL internalizes the safety constraints in the learned\npolicy -- both enforcing safer actions and biasing towards safer rewards --\nenabling safe deployment without the need for an online safety filter. We\nvalidate our framework through ablation studies on navigation tasks and on the\nUnitree G1 humanoid robot, where CBF-RL enables safer exploration, faster\nconvergence, and robust performance under uncertainty, enabling the humanoid\nrobot to avoid obstacles and climb stairs safely in real-world settings without\na runtime safety filter.",
            "headline_zh": "提出CBF-RL框架，在强化学习训练中集成控制屏障函数以提升安全性",
            "intro_zh": [
                "强化学习常牺牲安全性追求性能，可能导致灾难性后果",
                "在训练中通过CBF项修改策略并过滤策略rollout，内化安全约束",
                "实验验证在导航和人形机器人任务中实现安全探索和鲁棒性能"
            ],
            "tags_zh": [
                "强化学习",
                "控制屏障函数",
                "安全过滤",
                "机器人控制",
                "安全约束",
                "策略学习"
            ],
            "_index": 13
        },
        {
            "title": "RealDPO: Real or Not Real, that is the Preference",
            "authors": [
                "Guo Cheng",
                "Danni Yang",
                "Ziqi Huang",
                "Jianlou Si",
                "Chenyang Si",
                "Ziwei Liu"
            ],
            "arxiv_id": "2510.14955v1",
            "summary": "Video generative models have recently achieved notable advancements in\nsynthesis quality. However, generating complex motions remains a critical\nchallenge, as existing models often struggle to produce natural, smooth, and\ncontextually consistent movements. This gap between generated and real-world\nmotions limits their practical applicability. To address this issue, we\nintroduce RealDPO, a novel alignment paradigm that leverages real-world data as\npositive samples for preference learning, enabling more accurate motion\nsynthesis. Unlike traditional supervised fine-tuning (SFT), which offers\nlimited corrective feedback, RealDPO employs Direct Preference Optimization\n(DPO) with a tailored loss function to enhance motion realism. By contrasting\nreal-world videos with erroneous model outputs, RealDPO enables iterative\nself-correction, progressively refining motion quality. To support\npost-training in complex motion synthesis, we propose RealAction-5K, a curated\ndataset of high-quality videos capturing human daily activities with rich and\nprecise motion details. Extensive experiments demonstrate that RealDPO\nsignificantly improves video quality, text alignment, and motion realism\ncompared to state-of-the-art models and existing preference optimization\ntechniques.",
            "headline_zh": "提出RealDPO对齐范式，利用真实世界数据优化视频生成中的复杂运动合成。",
            "intro_zh": [
                "核心问题：视频生成模型在复杂运动合成中难以产生自然、流畅且上下文一致的运动。",
                "方法要点：采用Direct Preference Optimization，以真实视频为正样本对比模型输出进行迭代自校正。",
                "实验或效果：RealDPO显著提升视频质量、文本对齐和运动真实性，优于现有方法。"
            ],
            "tags_zh": [
                "视频生成",
                "偏好优化",
                "运动合成",
                "真实世界数据",
                "对齐学习"
            ],
            "_index": 14
        },
        {
            "title": "OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression",
            "authors": [
                "Zhe Li",
                "Weihao Yuan",
                "Weichao Shen",
                "Siyu Zhu",
                "Zilong Dong",
                "Chang Xu"
            ],
            "arxiv_id": "2510.14954v1",
            "summary": "Whole-body multi-modal human motion generation poses two primary challenges:\ncreating an effective motion generation mechanism and integrating various\nmodalities, such as text, speech, and music, into a cohesive framework. Unlike\nprevious methods that usually employ discrete masked modeling or autoregressive\nmodeling, we develop a continuous masked autoregressive motion transformer,\nwhere a causal attention is performed considering the sequential nature within\nthe human motion. Within this transformer, we introduce a gated linear\nattention and an RMSNorm module, which drive the transformer to pay attention\nto the key actions and suppress the instability caused by either the abnormal\nmovements or the heterogeneous distributions within multi-modalities. To\nfurther enhance both the motion generation and the multimodal generalization,\nwe employ the DiT structure to diffuse the conditions from the transformer\ntowards the targets. To fuse different modalities, AdaLN and cross-attention\nare leveraged to inject the text, speech, and music signals. Experimental\nresults demonstrate that our framework outperforms previous methods across all\nmodalities, including text-to-motion, speech-to-gesture, and music-to-dance.\nThe code of our method will be made public.",
            "headline_zh": "提出连续掩码自回归运动变换器以解决多模态人体运动生成问题",
            "intro_zh": [
                "核心问题：多模态人体运动生成需有效机制与模态融合，如文本、语音和音乐",
                "方法要点：采用连续掩码自回归变换器，结合门控线性注意力和RMSNorm模块",
                "实验或效果：在文本到运动、语音到手势和音乐到舞蹈任务中优于先前方法"
            ],
            "tags_zh": [
                "人体运动生成",
                "多模态融合",
                "自回归变换器",
                "连续掩码建模",
                "门控注意力"
            ],
            "_index": 15
        },
        {
            "title": "From Language to Locomotion: Retargeting-free Humanoid Control via Motion Latent Guidance",
            "authors": [
                "Zhe Li",
                "Cheng Chi",
                "Yangyang Wei",
                "Boan Zhu",
                "Yibo Peng",
                "Tao Huang",
                "Pengwei Wang",
                "Zhongyuan Wang",
                "Shanghang Zhang",
                "Chang Xu"
            ],
            "arxiv_id": "2510.14952v1",
            "summary": "Natural language offers a natural interface for humanoid robots, but existing\nlanguage-guided humanoid locomotion pipelines remain cumbersome and unreliable.\nThey typically decode human motion, retarget it to robot morphology, and then\ntrack it with a physics-based controller. However, this multi-stage process is\nprone to cumulative errors, introduces high latency, and yields weak coupling\nbetween semantics and control. These limitations call for a more direct pathway\nfrom language to action, one that eliminates fragile intermediate stages.\nTherefore, we present RoboGhost, a retargeting-free framework that directly\nconditions humanoid policies on language-grounded motion latents. By bypassing\nexplicit motion decoding and retargeting, RoboGhost enables a diffusion-based\npolicy to denoise executable actions directly from noise, preserving semantic\nintent and supporting fast, reactive control. A hybrid causal\ntransformer-diffusion motion generator further ensures long-horizon consistency\nwhile maintaining stability and diversity, yielding rich latent representations\nfor precise humanoid behavior. Extensive experiments demonstrate that RoboGhost\nsubstantially reduces deployment latency, improves success rates and tracking\naccuracy, and produces smooth, semantically aligned locomotion on real\nhumanoids. Beyond text, the framework naturally extends to other modalities\nsuch as images, audio, and music, providing a general foundation for\nvision-language-action humanoid systems.",
            "headline_zh": "提出RoboGhost框架，通过运动潜在引导实现免重定向的人形机器人控制",
            "intro_zh": [
                "现有语言引导人形机器人运动流程繁琐，易产生累积误差和延迟",
                "使用扩散策略直接从噪声去噪生成可执行动作，避免中间阶段",
                "实验显示降低延迟、提高成功率，并支持多模态扩展"
            ],
            "tags_zh": [
                "人形机器人控制",
                "语言到动作",
                "扩散模型",
                "运动潜在表示",
                "免重定向框架"
            ],
            "_index": 16
        },
        {
            "title": "DialectGen: Benchmarking and Improving Dialect Robustness in Multimodal Generation",
            "authors": [
                "Yu Zhou",
                "Sohyun An",
                "Haikang Deng",
                "Da Yin",
                "Clark Peng",
                "Cho-Jui Hsieh",
                "Kai-Wei Chang",
                "Nanyun Peng"
            ],
            "arxiv_id": "2510.14949v1",
            "summary": "Contact languages like English exhibit rich regional variations in the form\nof dialects, which are often used by dialect speakers interacting with\ngenerative models. However, can multimodal generative models effectively\nproduce content given dialectal textual input? In this work, we study this\nquestion by constructing a new large-scale benchmark spanning six common\nEnglish dialects. We work with dialect speakers to collect and verify over 4200\nunique prompts and evaluate on 17 image and video generative models. Our\nautomatic and human evaluation results show that current state-of-the-art\nmultimodal generative models exhibit 32.26% to 48.17% performance degradation\nwhen a single dialect word is used in the prompt. Common mitigation methods\nsuch as fine-tuning and prompt rewriting can only improve dialect performance\nby small margins (< 7%), while potentially incurring significant performance\ndegradation in Standard American English (SAE). To this end, we design a\ngeneral encoder-based mitigation strategy for multimodal generative models. Our\nmethod teaches the model to recognize new dialect features while preserving SAE\nperformance. Experiments on models such as Stable Diffusion 1.5 show that our\nmethod is able to simultaneously raise performance on five dialects to be on\npar with SAE (+34.4%), while incurring near zero cost to SAE performance.",
            "headline_zh": "提出编码器方法以提升多模态生成模型对英语方言的鲁棒性",
            "intro_zh": [
                "核心问题：多模态生成模型在方言文本输入下性能显著下降，降幅达32.26%至48.17%",
                "方法要点：设计编码器策略，使模型识别新方言特征并保持标准英语性能",
                "实验或效果：在Stable Diffusion 1.5等模型上，方言性能提升34.4%，标准英语性能几乎无损"
            ],
            "tags_zh": [
                "多模态生成",
                "方言鲁棒性",
                "编码器方法",
                "基准测试",
                "性能优化"
            ],
            "_index": 17
        },
        {
            "title": "Architecture Is All You Need: Diversity-Enabled Sweet Spots for Robust Humanoid Locomotion",
            "authors": [
                "Blake Werner",
                "Lizhi Yang",
                "Aaron D. Ames"
            ],
            "arxiv_id": "2510.14947v1",
            "summary": "Robust humanoid locomotion in unstructured environments requires\narchitectures that balance fast low-level stabilization with slower perceptual\ndecision-making. We show that a simple layered control architecture (LCA), a\nproprioceptive stabilizer running at high rate, coupled with a compact low-rate\nperceptual policy, enables substantially more robust performance than\nmonolithic end-to-end designs, even when using minimal perception encoders.\nThrough a two-stage training curriculum (blind stabilizer pretraining followed\nby perceptual fine-tuning), we demonstrate that layered policies consistently\noutperform one-stage alternatives in both simulation and hardware. On a Unitree\nG1 humanoid, our approach succeeds across stair and ledge tasks where one-stage\nperceptual policies fail. These results highlight that architectural separation\nof timescales, rather than network scale or complexity, is the key enabler for\nrobust perception-conditioned locomotion.",
            "headline_zh": "提出分层控制架构以提升人形机器人在非结构化环境中的鲁棒运动能力",
            "intro_zh": [
                "核心问题：人形机器人在非结构化环境中需要平衡快速低层稳定与慢速感知决策",
                "方法要点：采用分层控制架构，结合高速本体感知稳定器和低速感知策略",
                "实验或效果：在仿真和硬件上优于单阶段策略，成功应对楼梯和边缘任务"
            ],
            "tags_zh": [
                "人形机器人运动",
                "分层控制架构",
                "鲁棒性",
                "感知策略",
                "训练课程"
            ],
            "_index": 18
        },
        {
            "title": "EdgeNavMamba: Mamba Optimized Object Detection for Energy Efficient Edge Devices",
            "authors": [
                "Romina Aalishah",
                "Mozhgan Navardi",
                "Tinoosh Mohsenin"
            ],
            "arxiv_id": "2510.14946v1",
            "summary": "Deployment of efficient and accurate Deep Learning models has long been a\nchallenge in autonomous navigation, particularly for real-time applications on\nresource-constrained edge devices. Edge devices are limited in computing power\nand memory, making model efficiency and compression essential. In this work, we\npropose EdgeNavMamba, a reinforcement learning-based framework for\ngoal-directed navigation using an efficient Mamba object detection model. To\ntrain and evaluate the detector, we introduce a custom shape detection dataset\ncollected in diverse indoor settings, reflecting visual cues common in\nreal-world navigation. The object detector serves as a pre-processing module,\nextracting bounding boxes (BBOX) from visual input, which are then passed to an\nRL policy to control goal-oriented navigation. Experimental results show that\nthe student model achieved a reduction of 67% in size, and up to 73% in energy\nper inference on edge devices of NVIDIA Jetson Orin Nano and Raspberry Pi 5,\nwhile keeping the same performance as the teacher model. EdgeNavMamba also\nmaintains high detection accuracy in MiniWorld and IsaacLab simulators while\nreducing parameters by 31% compared to the baseline. In the MiniWorld\nsimulator, the navigation policy achieves over 90% success across environments\nof varying complexity.",
            "headline_zh": "提出EdgeNavMamba框架，结合Mamba目标检测与强化学习，优化边缘设备导航效率。",
            "intro_zh": [
                "边缘设备计算资源有限，需高效深度学习模型支持实时自主导航。",
                "采用强化学习框架，集成高效Mamba目标检测器提取边界框，指导导航策略。",
                "实验显示模型尺寸减少67%，能耗降低73%，导航成功率超90%。"
            ],
            "tags_zh": [
                "边缘计算",
                "目标检测",
                "强化学习",
                "模型压缩",
                "自主导航",
                "能效优化"
            ],
            "_index": 19
        },
        {
            "title": "3D Scene Prompting for Scene-Consistent Camera-Controllable Video Generation",
            "authors": [
                "JoungBin Lee",
                "Jaewoo Jung",
                "Jisang Han",
                "Takuya Narihira",
                "Kazumi Fukuda",
                "Junyoung Seo",
                "Sunghwan Hong",
                "Yuki Mitsufuji",
                "Seungryong Kim"
            ],
            "arxiv_id": "2510.14945v1",
            "summary": "We present 3DScenePrompt, a framework that generates the next video chunk\nfrom arbitrary-length input while enabling precise camera control and\npreserving scene consistency. Unlike methods conditioned on a single image or a\nshort clip, we employ dual spatio-temporal conditioning that reformulates\ncontext-view referencing across the input video. Our approach conditions on\nboth temporally adjacent frames for motion continuity and spatially adjacent\ncontent for scene consistency. However, when generating beyond temporal\nboundaries, directly using spatially adjacent frames would incorrectly preserve\ndynamic elements from the past. We address this by introducing a 3D scene\nmemory that represents exclusively the static geometry extracted from the\nentire input video. To construct this memory, we leverage dynamic SLAM with our\nnewly introduced dynamic masking strategy that explicitly separates static\nscene geometry from moving elements. The static scene representation can then\nbe projected to any target viewpoint, providing geometrically consistent warped\nviews that serve as strong 3D spatial prompts while allowing dynamic regions to\nevolve naturally from temporal context. This enables our model to maintain\nlong-range spatial coherence and precise camera control without sacrificing\ncomputational efficiency or motion realism. Extensive experiments demonstrate\nthat our framework significantly outperforms existing methods in scene\nconsistency, camera controllability, and generation quality. Project page :\nhttps://cvlab-kaist.github.io/3DScenePrompt/",
            "headline_zh": "提出3DScenePrompt框架，通过3D场景记忆实现场景一致和相机可控的视频生成。",
            "intro_zh": [
                "核心问题：长视频生成中场景一致性和相机控制难以兼顾，动态元素易干扰静态几何。",
                "方法要点：使用动态SLAM和动态掩码分离静态几何，构建3D场景记忆作为空间提示。",
                "实验或效果：在场景一致性、相机可控性和生成质量上显著优于现有方法。"
            ],
            "tags_zh": [
                "视频生成",
                "3D场景建模",
                "相机控制",
                "场景一致性",
                "动态SLAM",
                "空间提示"
            ],
            "_index": 20
        },
        {
            "title": "VT-Refine: Learning Bimanual Assembly with Visuo-Tactile Feedback via Simulation Fine-Tunin",
            "authors": [
                "Binghao Huang",
                "Jie Xu",
                "Iretiayo Akinola",
                "Wei Yang",
                "Balakumar Sundaralingam",
                "Rowland O'Flaherty",
                "Dieter Fox",
                "Xiaolong Wang",
                "Arsalan Mousavian",
                "Yu-Wei Chao",
                "Yunzhu Li"
            ],
            "arxiv_id": "2510.14930v1",
            "summary": "Humans excel at bimanual assembly tasks by adapting to rich tactile feedback\n-- a capability that remains difficult to replicate in robots through\nbehavioral cloning alone, due to the suboptimality and limited diversity of\nhuman demonstrations. In this work, we present VT-Refine, a visuo-tactile\npolicy learning framework that combines real-world demonstrations,\nhigh-fidelity tactile simulation, and reinforcement learning to tackle precise,\ncontact-rich bimanual assembly. We begin by training a diffusion policy on a\nsmall set of demonstrations using synchronized visual and tactile inputs. This\npolicy is then transferred to a simulated digital twin equipped with simulated\ntactile sensors and further refined via large-scale reinforcement learning to\nenhance robustness and generalization. To enable accurate sim-to-real transfer,\nwe leverage high-resolution piezoresistive tactile sensors that provide normal\nforce signals and can be realistically modeled in parallel using\nGPU-accelerated simulation. Experimental results show that VT-Refine improves\nassembly performance in both simulation and the real world by increasing data\ndiversity and enabling more effective policy fine-tuning. Our project page is\navailable at https://binghao-huang.github.io/vt_refine/.",
            "headline_zh": "提出VT-Refine框架，结合视觉触觉反馈与仿真微调，解决机器人双手装配任务",
            "intro_zh": [
                "核心问题：仅靠行为克隆难以复制人类双手装配的触觉适应能力，因演示数据有限且次优。",
                "方法要点：使用扩散策略从真实演示学习，再通过仿真强化学习微调，提升鲁棒性和泛化。",
                "实验效果：在仿真和真实世界中提高装配性能，增加数据多样性和策略优化效果。"
            ],
            "tags_zh": [
                "双手装配",
                "视觉触觉反馈",
                "扩散策略",
                "强化学习",
                "仿真到真实迁移",
                "触觉传感器"
            ],
            "_index": 21
        },
        {
            "title": "Design of Paper Robot Building Kits",
            "authors": [
                "Ruhan Yang",
                "Ellen Yi-Luen Do"
            ],
            "arxiv_id": "2510.14914v1",
            "summary": "Building robots is an engaging activity that provides opportunities for\nhands-on learning. However, traditional robot-building kits are usually costly\nwith limited functionality due to material and technology constraints. To\nimprove the accessibility and flexibility of such kits, we take paper as the\nbuilding material and extensively explore the versatility of paper-based\ninteractions. Based on an analysis of current robot-building kits and\npaper-based interaction research, we propose a design space for devising paper\nrobots. We also analyzed our building kit designs using this design space,\nwhere these kits demonstrate the potential of paper as a cost-effective\nmaterial for robot building. As a starting point, our design space and building\nkit examples provide a guideline that inspires and informs future research and\ndevelopment of novel paper robot-building kits.",
            "headline_zh": "提出纸基机器人设计空间以提升机器人套件的可访问性和灵活性",
            "intro_zh": [
                "传统机器人套件成本高且功能受限，材料和技术限制是核心问题",
                "基于纸材料探索交互多样性，构建设计空间指导纸机器人开发",
                "分析套件设计验证纸作为低成本材料的潜力，为未来研究提供起点"
            ],
            "tags_zh": [
                "纸基机器人",
                "设计空间",
                "低成本材料",
                "交互设计",
                "机器人套件",
                "可访问性"
            ],
            "_index": 22
        },
        {
            "title": "MaskCaptioner : Learning to Jointly Segment and Caption Object Trajectories in Videos",
            "authors": [
                "Gabriel Fiastre",
                "Antoine Yang",
                "Cordelia Schmid"
            ],
            "arxiv_id": "2510.14904v1",
            "summary": "Dense Video Object Captioning (DVOC) is the task of jointly detecting,\ntracking, and captioning object trajectories in a video, requiring the ability\nto understand spatio-temporal details and describe them in natural language.\nDue to the complexity of the task and the high cost associated with manual\nannotation, previous approaches resort to disjoint training strategies,\npotentially leading to suboptimal performance. To circumvent this issue, we\npropose to generate captions about spatio-temporally localized entities\nleveraging a state-of-the-art VLM. By extending the LVIS and LV-VIS datasets\nwith our synthetic captions (LVISCap and LV-VISCap), we train MaskCaptioner, an\nend-to-end model capable of jointly detecting, segmenting, tracking and\ncaptioning object trajectories. Moreover, with pretraining on LVISCap and\nLV-VISCap, MaskCaptioner achieves state-of-the-art DVOC results on three\nexisting benchmarks, VidSTG, VLN and BenSMOT. The datasets and code are\navailable at https://www.gabriel.fiastre.fr/maskcaptioner/.",
            "headline_zh": "提出MaskCaptioner以联合分割和描述视频中物体轨迹，解决密集视频物体描述任务。",
            "intro_zh": [
                "核心问题：密集视频物体描述需联合检测、跟踪和描述物体轨迹，但手动标注成本高，现有方法训练不连贯。",
                "方法要点：利用先进视觉语言模型生成合成描述，扩展LVIS和LV-VIS数据集，训练端到端模型。",
                "实验或效果：在VidSTG、VLN和BenSMOT基准上实现最先进性能，提供数据集和代码。"
            ],
            "tags_zh": [
                "密集视频物体描述",
                "端到端模型",
                "合成数据生成",
                "物体轨迹分割",
                "视觉语言模型",
                "视频理解"
            ],
            "_index": 23
        },
        {
            "title": "VLA^2: Empowering Vision-Language-Action Models with an Agentic Framework for Unseen Concept Manipulation",
            "authors": [
                "Han Zhao",
                "Jiaxuan Zhang",
                "Wenxuan Song",
                "Pengxiang Ding",
                "Donglin Wang"
            ],
            "arxiv_id": "2510.14902v1",
            "summary": "Current vision-language-action (VLA) models, pre-trained on large-scale\nrobotic data, exhibit strong multi-task capabilities and generalize well to\nvariations in visual and language instructions for manipulation. However, their\nsuccess rate drops significantly when faced with object concepts outside the\ntraining data, such as unseen object descriptions and textures in the dataset.\nTo address this, we propose a novel agentic framework, VLA^2, which leverages\nOpenVLA as the execution backbone and effectively leverages external modules\nsuch as web retrieval and object detection to provide visual and textual\nknowledge about target objects to the VLA. This approach mitigates\ngeneralization failure when handling out-of-distribution objects. Based on the\nLIBERO simulation environment, we introduced novel objects and object\ndescriptions to construct a new evaluation benchmark with three difficulty\nlevels to test the effectiveness of our method. Our framework successfully\noutperformed the current state-of-the-art models on our designed hard-level\ngeneralization benchmark. Compared to the standalone OpenVLA baseline, VLA^2\nachieves a 44.2% improvement in the success rate in the hard-level benchmark\nand an average improvement of 20.2% in all customized environments without any\nperformance degradation on in-domain tasks. Project website:\nhttps://vla-2.github.io.",
            "headline_zh": "提出VLA^2代理框架以解决视觉-语言-动作模型对未见概念泛化失败问题",
            "intro_zh": [
                "核心问题：VLA模型在训练数据外对象概念上成功率显著下降",
                "方法要点：利用OpenVLA执行骨干，集成外部模块提供目标对象知识",
                "实验或效果：在硬级泛化基准上成功率提升44.2%，平均提升20.2%"
            ],
            "tags_zh": [
                "视觉-语言-动作模型",
                "代理框架",
                "泛化能力",
                "未见概念",
                "机器人操作"
            ],
            "_index": 24
        },
        {
            "title": "Leveraging Multimodal LLM Descriptions of Activity for Explainable Semi-Supervised Video Anomaly Detection",
            "authors": [
                "Furkan Mumcu",
                "Michael J. Jones",
                "Anoop Cherian",
                "Yasin Yilmaz"
            ],
            "arxiv_id": "2510.14896v1",
            "summary": "Existing semi-supervised video anomaly detection (VAD) methods often struggle\nwith detecting complex anomalies involving object interactions and generally\nlack explainability. To overcome these limitations, we propose a novel VAD\nframework leveraging Multimodal Large Language Models (MLLMs). Unlike previous\nMLLM-based approaches that make direct anomaly judgments at the frame level,\nour method focuses on extracting and interpreting object activity and\ninteractions over time. By querying an MLLM with visual inputs of object pairs\nat different moments, we generate textual descriptions of the activity and\ninteractions from nominal videos. These textual descriptions serve as a\nhigh-level representation of the activity and interactions of objects in a\nvideo. They are used to detect anomalies during test time by comparing them to\ntextual descriptions found in nominal training videos. Our approach inherently\nprovides explainability and can be combined with many traditional VAD methods\nto further enhance their interpretability. Extensive experiments on benchmark\ndatasets demonstrate that our method not only detects complex interaction-based\nanomalies effectively but also achieves state-of-the-art performance on\ndatasets without interaction anomalies.",
            "headline_zh": "提出基于多模态大语言模型描述活动的半监督视频异常检测框架，以提升复杂异常检测与可解释性。",
            "intro_zh": [
                "现有半监督视频异常检测方法难以检测对象交互的复杂异常且缺乏可解释性。",
                "方法利用MLLM生成对象活动与交互的文本描述，通过比较训练与测试视频描述检测异常。",
                "实验在基准数据集上显示，该方法有效检测交互异常并在无交互数据集上达到先进性能。"
            ],
            "tags_zh": [
                "视频异常检测",
                "多模态大语言模型",
                "半监督学习",
                "可解释性",
                "对象交互"
            ],
            "_index": 25
        },
        {
            "title": "STITCHER: Constrained Trajectory Planning in Known Environments with Real-Time Motion Primitive Search",
            "authors": [
                "Helene J. Levy",
                "Brett T. Lopez"
            ],
            "arxiv_id": "2510.14893v1",
            "summary": "Autonomous high-speed navigation through large, complex environments requires\nreal-time generation of agile trajectories that are dynamically feasible,\ncollision-free, and satisfy state or actuator constraints. Modern trajectory\nplanning techniques primarily use numerical optimization, as they enable the\nsystematic computation of high-quality, expressive trajectories that satisfy\nvarious constraints. However, stringent requirements on computation time and\nthe risk of numerical instability can limit the use of optimization-based\nplanners in safety-critical scenarios. This work presents an optimization-free\nplanning framework called STITCHER that stitches short trajectory segments\ntogether with graph search to compute long-range, expressive, and near-optimal\ntrajectories in real-time. STITCHER outperforms modern optimization-based\nplanners through our innovative planning architecture and several algorithmic\ndevelopments that make real-time planning possible. Extensive simulation\ntesting is performed to analyze the algorithmic components that make up\nSTITCHER, along with a thorough comparison with two state-of-the-art\noptimization planners. Simulation tests show that safe trajectories can be\ncreated within a few milliseconds for paths that span the entirety of two 50 m\nx 50 m environments. Hardware tests with a custom quadrotor verify that\nSTITCHER can produce trackable paths in real-time while respecting nonconvex\nconstraints, such as limits on tilt angle and motor forces, which are otherwise\nhard to include in optimization-based planners.",
            "headline_zh": "提出STITCHER框架，通过实时运动基元搜索在已知环境中规划约束轨迹",
            "intro_zh": [
                "核心问题：高速自主导航需实时生成动态可行、无碰撞且满足约束的轨迹，优化方法存在计算时间和数值稳定性限制",
                "方法要点：采用无优化方法，缝合短轨迹段，结合图搜索实现长距离、表达性强且近最优的实时规划",
                "实验或效果：仿真测试显示在50m x 50m环境中数毫秒内生成安全轨迹，硬件测试验证实时跟踪路径并处理非凸约束"
            ],
            "tags_zh": [
                "轨迹规划",
                "实时运动基元搜索",
                "无优化规划",
                "约束满足",
                "自主导航",
                "图搜索"
            ],
            "_index": 26
        },
        {
            "title": "You May Speak Freely: Improving the Fine-Grained Visual Recognition Capabilities of Multimodal Large Language Models with Answer Extraction",
            "authors": [
                "Logan Lawrence",
                "Oindrila Saha",
                "Megan Wei",
                "Chen Sun",
                "Subhransu Maji",
                "Grant Van Horn"
            ],
            "arxiv_id": "2510.14885v1",
            "summary": "Despite the renewed interest in zero-shot visual classification due to the\nrise of Multimodal Large Language Models (MLLMs), the problem of evaluating\nfree-form responses of auto-regressive models remains a persistent challenge.\nMost existing works focus on language-only tasks or don't consider Multiple\nChoice Questions (MCQs) beyond 5-way options, both of which are critical\ncapabilities to solve tasks in Fine-Grained Visual Classification (FGVC) where\nchoice counts are in the hundreds to thousands and the choices are highly\nrelated. Furthermore, in this highly multi-way MCQ setting it is not clear how\nto extend LLM choice extraction to retrieval-based problems, where computing\nprobabilities over the choice set is computationally costly. In this work we\ninvestigate nlg2choice, a simple two-stage method which first asks the MLLM an\nopen-ended question for the task with minimal constraints, then uses text-only\nconstrained decoding to predict the most likely choice. In retrieval settings,\nwe compute the probability of the constrained response taking that choice with\nan early stopping method to significantly improve throughput. Our results show\nimprovement over a suite of seven fine-grained visual datasets when evaluating\nin terms of classification and retrieval, and show that this performance holds\nover the various ways that users of LLMs can implement tasks in natural\nlanguage.",
            "headline_zh": "提出nlg2choice方法以提升多模态大语言模型在细粒度视觉分类中的性能",
            "intro_zh": [
                "核心问题：多模态大语言模型在细粒度视觉分类中，处理自由形式响应和高选项多选问题时评估困难。",
                "方法要点：采用两阶段方法，先开放提问，再文本约束解码预测最可能选项，提升检索效率。",
                "实验或效果：在七个细粒度视觉数据集上，分类和检索性能均优于基线方法。"
            ],
            "tags_zh": [
                "多模态大语言模型",
                "细粒度视觉分类",
                "答案提取",
                "约束解码",
                "零样本分类",
                "检索效率"
            ],
            "_index": 27
        },
        {
            "title": "ScaleWeaver: Weaving Efficient Controllable T2I Generation with Multi-Scale Reference Attention",
            "authors": [
                "Keli Liu",
                "Zhendong Wang",
                "Wengang Zhou",
                "Shaodong Xu",
                "Ruixiao Dong",
                "Houqiang Li"
            ],
            "arxiv_id": "2510.14882v1",
            "summary": "Text-to-image generation with visual autoregressive~(VAR) models has recently\nachieved impressive advances in generation fidelity and inference efficiency.\nWhile control mechanisms have been explored for diffusion models, enabling\nprecise and flexible control within VAR paradigm remains underexplored. To\nbridge this critical gap, in this paper, we introduce ScaleWeaver, a novel\nframework designed to achieve high-fidelity, controllable generation upon\nadvanced VAR models through parameter-efficient fine-tuning. The core module in\nScaleWeaver is the improved MMDiT block with the proposed Reference Attention\nmodule, which efficiently and effectively incorporates conditional information.\nDifferent from MM Attention, the proposed Reference Attention module discards\nthe unnecessary attention from image$\\rightarrow$condition, reducing\ncomputational cost while stabilizing control injection. Besides, it\nstrategically emphasizes parameter reuse, leveraging the capability of the VAR\nbackbone itself with a few introduced parameters to process control\ninformation, and equipping a zero-initialized linear projection to ensure that\ncontrol signals are incorporated effectively without disrupting the generative\ncapability of the base model. Extensive experiments show that ScaleWeaver\ndelivers high-quality generation and precise control while attaining superior\nefficiency over diffusion-based methods, making ScaleWeaver a practical and\neffective solution for controllable text-to-image generation within the visual\nautoregressive paradigm. Code and models will be released.",
            "headline_zh": "提出ScaleWeaver框架，通过多尺度参考注意力实现高效可控文本到图像生成。",
            "intro_zh": [
                "核心问题：视觉自回归模型中可控生成机制不足，难以实现精确灵活控制。",
                "方法要点：引入参考注意力模块，优化MMDiT块，减少计算成本并稳定控制注入。",
                "实验或效果：实验显示生成质量高、控制精确，效率优于基于扩散的方法。"
            ],
            "tags_zh": [
                "文本到图像生成",
                "视觉自回归模型",
                "可控生成",
                "参考注意力",
                "参数高效微调",
                "多尺度处理"
            ],
            "_index": 28
        },
        {
            "title": "BADAS: Context Aware Collision Prediction Using Real-World Dashcam Data",
            "authors": [
                "Roni Goldshmidt",
                "Hamish Scott",
                "Lorenzo Niccolini",
                "Shizhan Zhu",
                "Daniel Moura",
                "Orly Zvitia"
            ],
            "arxiv_id": "2510.14876v1",
            "summary": "Existing collision prediction methods often fail to distinguish between\nego-vehicle threats and random accidents not involving the ego vehicle, leading\nto excessive false alerts in real-world deployment. We present BADAS, a family\nof collision prediction models trained on Nexar's real-world dashcam collision\ndataset -- the first benchmark designed explicitly for ego-centric evaluation.\nWe re-annotate major benchmarks to identify ego involvement, add consensus\nalert-time labels, and synthesize negatives where needed, enabling fair AP/AUC\nand temporal evaluation. BADAS uses a V-JEPA2 backbone trained end-to-end and\ncomes in two variants: BADAS-Open (trained on our 1.5k public videos) and\nBADAS1.0 (trained on 40k proprietary videos). Across DAD, DADA-2000, DoTA, and\nNexar, BADAS achieves state-of-the-art AP/AUC and outperforms a\nforward-collision ADAS baseline while producing more realistic time-to-accident\nestimates. We release our BADAS-Open model weights and code, along with\nre-annotations of all evaluation datasets to promote ego-centric collision\nprediction research.",
            "headline_zh": "提出BADAS模型，利用真实行车记录仪数据预测自车碰撞，减少误报。",
            "intro_zh": [
                "现有碰撞预测方法难以区分自车威胁与无关事故，导致高误报率。",
                "使用V-JEPA2骨干网络端到端训练，提供公开和专有两种变体。",
                "在多个数据集上实现最优AP/AUC，并改进碰撞时间估计。"
            ],
            "tags_zh": [
                "碰撞预测",
                "自车中心评估",
                "行车记录仪数据",
                "V-JEPA2骨干网络",
                "端到端训练",
                "时间到事故估计"
            ],
            "_index": 29
        },
        {
            "title": "TOUCH: Text-guided Controllable Generation of Free-Form Hand-Object Interactions",
            "authors": [
                "Guangyi Han",
                "Wei Zhai",
                "Yuhang Yang",
                "Yang Cao",
                "Zheng-Jun Zha"
            ],
            "arxiv_id": "2510.14874v1",
            "summary": "Hand-object interaction (HOI) is fundamental for humans to express intent.\nExisting HOI generation research is predominantly confined to fixed grasping\npatterns, where control is tied to physical priors such as force closure or\ngeneric intent instructions, even when expressed through elaborate language.\nSuch an overly general conditioning imposes a strong inductive bias for stable\ngrasps, thus failing to capture the diversity of daily HOI. To address these\nlimitations, we introduce Free-Form HOI Generation, which aims to generate\ncontrollable, diverse, and physically plausible HOI conditioned on fine-grained\nintent, extending HOI from grasping to free-form interactions, like pushing,\npoking, and rotating. To support this task, we construct WildO2, an in-the-wild\ndiverse 3D HOI dataset, which includes diverse HOI derived from internet\nvideos. Specifically, it contains 4.4k unique interactions across 92 intents\nand 610 object categories, each with detailed semantic annotations. Building on\nthis dataset, we propose TOUCH, a three-stage framework centered on a\nmulti-level diffusion model that facilitates fine-grained semantic control to\ngenerate versatile hand poses beyond grasping priors. This process leverages\nexplicit contact modeling for conditioning and is subsequently refined with\ncontact consistency and physical constraints to ensure realism. Comprehensive\nexperiments demonstrate our method's ability to generate controllable, diverse,\nand physically plausible hand interactions representative of daily activities.\nThe project page is $\\href{https://guangyid.github.io/hoi123touch}{here}$.",
            "headline_zh": "提出TOUCH框架以生成可控、多样且物理合理的手-物体自由交互",
            "intro_zh": [
                "现有手-物体交互生成局限于固定抓取模式，缺乏日常交互多样性。",
                "TOUCH使用多级扩散模型，结合细粒度语义控制，生成超越抓取的手部姿势。",
                "实验验证方法能生成可控、多样且物理合理的手交互，代表日常活动。"
            ],
            "tags_zh": [
                "手-物体交互生成",
                "自由形式交互",
                "多级扩散模型",
                "细粒度语义控制",
                "物理约束优化",
                "WildO2数据集"
            ],
            "_index": 30
        },
        {
            "title": "Benchmarking Multimodal Large Language Models for Face Recognition",
            "authors": [
                "Hatef Otroshi Shahreza",
                "Sébastien Marcel"
            ],
            "arxiv_id": "2510.14866v1",
            "summary": "Multimodal large language models (MLLMs) have achieved remarkable performance\nacross diverse vision-and-language tasks. However, their potential in face\nrecognition remains underexplored. In particular, the performance of\nopen-source MLLMs needs to be evaluated and compared with existing face\nrecognition models on standard benchmarks with similar protocol. In this work,\nwe present a systematic benchmark of state-of-the-art MLLMs for face\nrecognition on several face recognition datasets, including LFW, CALFW, CPLFW,\nCFP, AgeDB and RFW. Experimental results reveal that while MLLMs capture rich\nsemantic cues useful for face-related tasks, they lag behind specialized models\nin high-precision recognition scenarios in zero-shot applications. This\nbenchmark provides a foundation for advancing MLLM-based face recognition,\noffering insights for the design of next-generation models with higher accuracy\nand generalization. The source code of our benchmark is publicly available in\nthe project page.",
            "headline_zh": "基准测试多模态大语言模型在标准人脸识别数据集上的性能",
            "intro_zh": [
                "核心问题：多模态大语言模型在人脸识别中的潜力未被充分探索，需与专业模型比较。",
                "方法要点：系统评估开源MLLMs在LFW等数据集上的零样本识别能力。",
                "实验或效果：MLLMs捕获语义线索，但在高精度场景下落后于专业模型。"
            ],
            "tags_zh": [
                "多模态大语言模型",
                "人脸识别",
                "基准测试",
                "零样本学习",
                "开源模型"
            ],
            "_index": 31
        },
        {
            "title": "Multi-modal video data-pipelines for machine learning with minimal human supervision",
            "authors": [
                "Mihai-Cristian Pîrvu",
                "Marius Leordeanu"
            ],
            "arxiv_id": "2510.14862v1",
            "summary": "The real-world is inherently multi-modal at its core. Our tools observe and\ntake snapshots of it, in digital form, such as videos or sounds, however much\nof it is lost. Similarly for actions and information passing between humans,\nlanguages are used as a written form of communication. Traditionally, Machine\nLearning models have been unimodal (i.e. rgb -> semantic or text ->\nsentiment_class). Recent trends go towards bi-modality, where images and text\nare learned together, however, in order to truly understand the world, we need\nto integrate all these independent modalities. In this work we try to combine\nas many visual modalities as we can using little to no human supervision. In\norder to do this, we use pre-trained experts and procedural combinations\nbetween them on top of raw videos using a fully autonomous data-pipeline, which\nwe also open-source. We then make use of PHG-MAE, a model specifically designed\nto leverage multi-modal data. We show that this model which was efficiently\ndistilled into a low-parameter (<1M) can have competitive results compared to\nmodels of ~300M parameters. We deploy this model and analyze the use-case of\nreal-time semantic segmentation from handheld devices or webcams on commodity\nhardware. Finally, we deploy other off-the-shelf models using the same\nframework, such as DPT for near real-time depth estimation.",
            "headline_zh": "提出多模态视频数据流水线，实现无监督学习并部署于实时语义分割",
            "intro_zh": [
                "核心问题：现实世界多模态数据在数字化中丢失，需整合多模态以提升理解。",
                "方法要点：使用预训练专家和自主数据流水线，结合PHG-MAE模型进行多模态学习。",
                "实验或效果：模型参数量小于1M，性能媲美300M参数模型，并部署于实时应用。"
            ],
            "tags_zh": [
                "多模态学习",
                "无监督数据流水线",
                "模型蒸馏",
                "实时语义分割",
                "深度估计"
            ],
            "_index": 32
        },
        {
            "title": "A Multi-Task Deep Learning Framework for Skin Lesion Classification, ABCDE Feature Quantification, and Evolution Simulation",
            "authors": [
                "Harsha Kotla",
                "Arun Kumar Rajasekaran",
                "Hannah Rana"
            ],
            "arxiv_id": "2510.14855v1",
            "summary": "Early detection of melanoma has grown to be essential because it\nsignificantly improves survival rates, but automated analysis of skin lesions\nstill remains challenging. ABCDE, which stands for Asymmetry, Border\nirregularity, Color variation, Diameter, and Evolving, is a well-known\nclassification method for skin lesions, but most deep learning mechanisms treat\nit as a black box, as most of the human interpretable features are not\nexplained. In this work, we propose a deep learning framework that both\nclassifies skin lesions into categories and also quantifies scores for each\nABCD feature. It simulates the evolution of these features over time in order\nto represent the E aspect, opening more windows for future exploration. The A,\nB, C, and D values are quantified particularly within this work. Moreover, this\nframework also visualizes ABCD feature trajectories in latent space as skin\nlesions evolve from benign nevuses to malignant melanoma. The experiments are\nconducted using the HAM10000 dataset that contains around ten thousand images\nof skin lesions of varying stages. In summary, the classification worked with\nan accuracy of around 89 percent, with melanoma AUC being 0.96, while the\nfeature evaluation performed well in predicting asymmetry, color variation, and\ndiameter, though border irregularity remains more difficult to model. Overall,\nthis work provides a deep learning framework that will allow doctors to link ML\ndiagnoses to clinically relevant criteria, thus improving our understanding of\nskin cancer progression.",
            "headline_zh": "提出多任务深度学习框架，用于皮肤病变分类、ABCDE特征量化与演化模拟。",
            "intro_zh": [
                "核心问题：自动皮肤病变分析挑战大，ABCDE特征缺乏可解释性。",
                "方法要点：框架同时分类病变并量化ABCD特征，模拟E演化。",
                "实验或效果：在HAM10000数据集上，分类准确率约89%，AUC达0.96。"
            ],
            "tags_zh": [
                "皮肤病变分类",
                "多任务学习",
                "ABCDE特征量化",
                "演化模拟",
                "医学图像分析"
            ],
            "_index": 33
        },
        {
            "title": "SADCHER: Scheduling using Attention-based Dynamic Coalitions of Heterogeneous Robots in Real-Time",
            "authors": [
                "Jakob Bichler",
                "Andreu Matoses Gimenez",
                "Javier Alonso-Mora"
            ],
            "arxiv_id": "2510.14851v1",
            "summary": "We present Sadcher, a real-time task assignment framework for heterogeneous\nmulti-robot teams that incorporates dynamic coalition formation and task\nprecedence constraints. Sadcher is trained through Imitation Learning and\ncombines graph attention and transformers to predict assignment rewards between\nrobots and tasks. Based on the predicted rewards, a relaxed bipartite matching\nstep generates high-quality schedules with feasibility guarantees. We\nexplicitly model robot and task positions, task durations, and robots'\nremaining processing times, enabling advanced temporal and spatial reasoning\nand generalization to environments with different spatiotemporal distributions\ncompared to training. Trained on optimally solved small-scale instances, our\nmethod can scale to larger task sets and team sizes. Sadcher outperforms other\nlearning-based and heuristic baselines on randomized, unseen problems for small\nand medium-sized teams with computation times suitable for real-time operation.\nWe also explore sampling-based variants and evaluate scalability across robot\nand task counts. In addition, we release our dataset of 250,000 optimal\nschedules: https://autonomousrobots.nl/paper_websites/sadcher_MRTA/",
            "headline_zh": "提出SADCHER框架，用于异构多机器人实时任务分配，结合动态联盟与任务优先级约束。",
            "intro_zh": [
                "核心问题：异构多机器人团队在实时环境中的任务分配，需处理动态联盟和任务优先级约束。",
                "方法要点：使用模仿学习训练，结合图注意力和变换器预测奖励，并通过松弛二分匹配生成可行调度。",
                "实验或效果：在随机未见问题上优于其他方法，计算时间适合实时操作，并评估了可扩展性。"
            ],
            "tags_zh": [
                "异构多机器人系统",
                "实时任务分配",
                "动态联盟形成",
                "图注意力网络",
                "模仿学习",
                "可扩展调度"
            ],
            "_index": 34
        },
        {
            "title": "Multi Agent Switching Mode Controller for Sound Source localization",
            "authors": [
                "Marcello Sorge",
                "Nicola Cigarini",
                "Riccardo Lorigiola",
                "Giulia Michieletto",
                "Andrea Masiero",
                "Angelo Cenedese",
                "Alberto Guarnieri"
            ],
            "arxiv_id": "2510.14849v1",
            "summary": "Source seeking is an important topic in robotic research, especially\nconsidering sound-based sensors since they allow the agents to locate a target\neven in critical conditions where it is not possible to establish a direct line\nof sight. In this work, we design a multi- agent switching mode control\nstrategy for acoustic-based target localization. Two scenarios are considered:\nsingle source localization, in which the agents are driven maintaining a rigid\nformation towards the target, and multi-source scenario, in which each agent\nsearches for the targets independently from the others.",
            "headline_zh": "提出多智能体切换模式控制策略，用于单源和多源声源定位。",
            "intro_zh": [
                "核心问题：在无直接视线条件下，机器人如何利用声学传感器定位目标。",
                "方法要点：设计切换模式控制，单源时保持刚性编队，多源时独立搜索。",
                "实验或效果：未知具体性能，但针对单源和多源场景进行了策略设计。"
            ],
            "tags_zh": [
                "声源定位",
                "多智能体控制",
                "切换模式控制",
                "机器人感知",
                "声学传感器"
            ],
            "_index": 35
        },
        {
            "title": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints",
            "authors": [
                "Meiqi Wu",
                "Jiashu Zhu",
                "Xiaokun Feng",
                "Chubin Chen",
                "Chen Zhu",
                "Bingze Song",
                "Fangyuan Mao",
                "Jiahong Wu",
                "Xiangxiang Chu",
                "Kaiqi Huang"
            ],
            "arxiv_id": "2510.14847v1",
            "summary": "Video generation models have achieved remarkable progress, particularly\nexcelling in realistic scenarios; however, their performance degrades notably\nin imaginative scenarios. These prompts often involve rarely co-occurring\nconcepts with long-distance semantic relationships, falling outside training\ndistributions. Existing methods typically apply test-time scaling for improving\nvideo quality, but their fixed search spaces and static reward designs limit\nadaptability to imaginative scenarios. To fill this gap, we propose\nImagerySearch, a prompt-guided adaptive test-time search strategy that\ndynamically adjusts both the inference search space and reward function\naccording to semantic relationships in the prompt. This enables more coherent\nand visually plausible videos in challenging imaginative settings. To evaluate\nprogress in this direction, we introduce LDT-Bench, the first dedicated\nbenchmark for long-distance semantic prompts, consisting of 2,839 diverse\nconcept pairs and an automated protocol for assessing creative generation\ncapabilities. Extensive experiments show that ImagerySearch consistently\noutperforms strong video generation baselines and existing test-time scaling\napproaches on LDT-Bench, and achieves competitive improvements on VBench,\ndemonstrating its effectiveness across diverse prompt types. We will release\nLDT-Bench and code to facilitate future research on imaginative video\ngeneration.",
            "headline_zh": "提出ImagerySearch自适应测试时搜索策略以提升想象力场景视频生成质量",
            "intro_zh": [
                "核心问题：视频生成模型在想象力场景中性能下降，因提示涉及罕见共现概念和长距离语义关系。",
                "方法要点：动态调整推理搜索空间和奖励函数，基于提示语义关系实现自适应测试时搜索。",
                "实验或效果：在LDT-Bench和VBench上优于基线，验证了方法的有效性和泛化能力。"
            ],
            "tags_zh": [
                "视频生成",
                "测试时搜索",
                "长距离语义",
                "自适应优化",
                "想象力场景",
                "基准评估"
            ],
            "_index": 36
        },
        {
            "title": "Backdoor Unlearning by Linear Task Decomposition",
            "authors": [
                "Amel Abdelraheem",
                "Alessandro Favero",
                "Gerome Bovet",
                "Pascal Frossard"
            ],
            "arxiv_id": "2510.14845v1",
            "summary": "Foundation models have revolutionized computer vision by enabling broad\ngeneralization across diverse tasks. Yet, they remain highly susceptible to\nadversarial perturbations and targeted backdoor attacks. Mitigating such\nvulnerabilities remains an open challenge, especially given that the\nlarge-scale nature of the models prohibits retraining to ensure safety.\nExisting backdoor removal approaches rely on costly fine-tuning to override the\nharmful behavior, and can often degrade performance on other unrelated tasks.\nThis raises the question of whether backdoors can be removed without\ncompromising the general capabilities of the models. In this work, we address\nthis question and study how backdoors are encoded in the model weight space,\nfinding that they are disentangled from other benign tasks. Specifically, this\nseparation enables the isolation and erasure of the backdoor's influence on the\nmodel with minimal impact on clean performance. Building on this insight, we\nintroduce a simple unlearning method that leverages such disentanglement.\nThrough extensive experiments with CLIP-based models and common adversarial\ntriggers, we show that, given the knowledge of the attack, our method achieves\napproximately perfect unlearning, while retaining, on average, 96% of clean\naccuracy. Additionally, we demonstrate that even when the attack and its\npresence are unknown, our method successfully unlearns backdoors by proper\nestimation using reverse-engineered triggers. Overall, our method consistently\nyields better unlearning and clean accuracy tradeoffs when compared to present\nstate-of-the-art defenses.",
            "headline_zh": "提出线性任务分解方法以移除基础模型中的后门攻击",
            "intro_zh": [
                "基础模型易受后门攻击，现有方法需高成本微调且影响其他任务性能",
                "发现后门在权重空间中与良性任务解耦，可隔离并擦除其影响",
                "实验显示方法近乎完美移除后门，平均保持96%清洁准确率"
            ],
            "tags_zh": [
                "后门移除",
                "基础模型安全",
                "线性任务分解",
                "权重空间分析",
                "CLIP模型",
                "对抗攻击防御"
            ],
            "_index": 37
        },
        {
            "title": "QDepth-VLA: Quantized Depth Prediction as Auxiliary Supervision for Vision-Language-Action Models",
            "authors": [
                "Yixuan Li",
                "Yuhui Chen",
                "Mingcai Zhou",
                "Haoran Li"
            ],
            "arxiv_id": "2510.14836v1",
            "summary": "Spatial perception and reasoning are crucial for Vision-Language-Action (VLA)\nmodels to accomplish fine-grained manipulation tasks. However, existing\napproaches often lack the ability to understand and reason over the essential\n3D structures necessary for precise control. To address this limitation, we\npropose QDepth-VLA, a general framework that augments VLA models with an\nauxiliary depth prediction task. A dedicated depth expert is designed to\npredict quantized latent tokens of depth maps obtained from a VQ-VAE encoder,\nenabling the model to learn depth-aware representations that capture critical\ngeometric cues. Experimental results on the simulation benchmarks and\nreal-world tasks demonstrate that QDepth-VLA yields strong spatial reasoning\nand competitive performance on manipulation tasks.",
            "headline_zh": "提出QDepth-VLA框架，通过量化深度预测增强VLA模型的空间推理能力",
            "intro_zh": [
                "现有VLA模型缺乏对3D结构的理解，影响精细操作任务性能",
                "引入辅助深度预测任务，使用VQ-VAE编码量化深度图以学习深度感知表示",
                "在仿真和真实任务中验证，提升空间推理和操作任务表现"
            ],
            "tags_zh": [
                "视觉-语言-动作模型",
                "深度预测",
                "空间推理",
                "量化表示",
                "辅助监督"
            ],
            "_index": 38
        },
        {
            "title": "Scaling Tumor Segmentation: Best Lessons from Real and Synthetic Data",
            "authors": [
                "Qi Chen",
                "Xinze Zhou",
                "Chen Liu",
                "Hao Chen",
                "Wenxuan Li",
                "Zekun Jiang",
                "Ziyan Huang",
                "Yuxuan Zhao",
                "Dexin Yu",
                "Junjun He",
                "Yefeng Zheng",
                "Ling Shao",
                "Alan Yuille",
                "Zongwei Zhou"
            ],
            "arxiv_id": "2510.14831v1",
            "summary": "AI for tumor segmentation is limited by the lack of large, voxel-wise\nannotated datasets, which are hard to create and require medical experts. In\nour proprietary JHH dataset of 3,000 annotated pancreatic tumor scans, we found\nthat AI performance stopped improving after 1,500 scans. With synthetic data,\nwe reached the same performance using only 500 real scans. This finding\nsuggests that synthetic data can steepen data scaling laws, enabling more\nefficient model training than real data alone. Motivated by these lessons, we\ncreated AbdomenAtlas 2.0--a dataset of 10,135 CT scans with a total of 15,130\ntumor instances per-voxel manually annotated in six organs (pancreas, liver,\nkidney, colon, esophagus, and uterus) and 5,893 control scans. Annotated by 23\nexpert radiologists, it is several orders of magnitude larger than existing\npublic tumor datasets. While we continue expanding the dataset, the current\nversion of AbdomenAtlas 2.0 already provides a strong foundation--based on\nlessons from the JHH dataset--for training AI to segment tumors in six organs.\nIt achieves notable improvements over public datasets, with a +7% DSC gain on\nin-distribution tests and +16% on out-of-distribution tests.",
            "headline_zh": "提出AbdomenAtlas 2.0数据集，结合真实与合成数据提升肿瘤分割效率",
            "intro_zh": [
                "肿瘤分割AI受限于缺乏大规模体素标注数据，标注困难且需专家参与",
                "利用合成数据可减少真实数据需求，在JHH数据集中以500真实扫描达到1500扫描性能",
                "AbdomenAtlas 2.0包含10,135 CT扫描，在六器官分割中DSC提升7%（分布内）和16%（分布外）"
            ],
            "tags_zh": [
                "肿瘤分割",
                "合成数据",
                "医学影像",
                "数据扩展",
                "体素标注"
            ],
            "_index": 39
        },
        {
            "title": "RL-100: Performant Robotic Manipulation with Real-World Reinforcement Learning",
            "authors": [
                "Kun Lei",
                "Huanyu Li",
                "Dongjie Yu",
                "Zhenyu Wei",
                "Lingxiao Guo",
                "Zhennan Jiang",
                "Ziyu Wang",
                "Shiyu Liang",
                "Huazhe Xu"
            ],
            "arxiv_id": "2510.14830v1",
            "summary": "Real-world robotic manipulation in homes and factories demands reliability,\nefficiency, and robustness that approach or surpass skilled human operators. We\npresent RL-100, a real-world reinforcement learning training framework built on\ndiffusion visuomotor policies trained bu supervised learning. RL-100 introduces\na three-stage pipeline. First, imitation learning leverages human priors.\nSecond, iterative offline reinforcement learning uses an Offline Policy\nEvaluation procedure, abbreviated OPE, to gate PPO-style updates that are\napplied in the denoising process for conservative and reliable improvement.\nThird, online reinforcement learning eliminates residual failure modes. An\nadditional lightweight consistency distillation head compresses the multi-step\nsampling process in diffusion into a single-step policy, enabling\nhigh-frequency control with an order-of-magnitude reduction in latency while\npreserving task performance. The framework is task-, embodiment-, and\nrepresentation-agnostic and supports both 3D point clouds and 2D RGB inputs, a\nvariety of robot platforms, and both single-step and action-chunk policies. We\nevaluate RL-100 on seven real-robot tasks spanning dynamic rigid-body control,\nsuch as Push-T and Agile Bowling, fluids and granular pouring, deformable cloth\nfolding, precise dexterous unscrewing, and multi-stage orange juicing. RL-100\nattains 100\\% success across evaluated trials for a total of 900 out of 900\nepisodes, including up to 250 out of 250 consecutive trials on one task. The\nmethod achieves near-human teleoperation or better time efficiency and\ndemonstrates multi-hour robustness with uninterrupted operation lasting up to\ntwo hours.",
            "headline_zh": "提出RL-100框架，通过三阶段强化学习实现真实世界机器人操作的高性能。",
            "intro_zh": [
                "核心问题：真实世界机器人操作需高可靠性、效率和鲁棒性，以接近或超越人类操作员。",
                "方法要点：采用模仿学习、离线强化学习和在线强化学习三阶段管道，结合扩散策略和一致性蒸馏。",
                "实验或效果：在七项任务中实现100%成功率，共900次试验，并展示多小时不间断操作。"
            ],
            "tags_zh": [
                "机器人操作",
                "强化学习",
                "扩散策略",
                "离线策略评估",
                "一致性蒸馏",
                "真实世界应用"
            ],
            "_index": 40
        },
        {
            "title": "RoboGPT-R1: Enhancing Robot Planning with Reinforcement Learning",
            "authors": [
                "Jinrui Liu",
                "Bingyan Nie",
                "Boyu Li",
                "Yaran Chen",
                "Yuze Wang",
                "Shunsen He",
                "Haoran Li"
            ],
            "arxiv_id": "2510.14828v1",
            "summary": "Improving the reasoning capabilities of embodied agents is crucial for robots\nto complete complex human instructions in long-view manipulation tasks\nsuccessfully. Despite the success of large language models and vision language\nmodels based on Supervised Fine-Tuning (SFT) in planning tasks, they continue\nfacing challenges in performing long-horizon manipulation tasks in complex\nreal-world environments, owing to their restricted common sense and reasoning\ncapabilities. Considering that aligning general-purpose vision language models\nto robotic planning tasks via supervised fine-tuning suffers from poor\ngeneralization and insufficient physical understanding, we propose RoboGPT-R1,\na two-stage fine-tuning framework for embodied planning. In this framework,\nsupervised training acquires foundational knowledge through expert sequences,\nfollowed by RL to address the model's shortcomings in visual-spatial\nunderstanding and reasoning. To achieve physical understanding and action\nsequence consistency in multi-step reasoning tasks, we design a rule-based\nreward function that simultaneously considers long-horizon performance and\naction constraint in the environment. The reasoning model, trained on\nQwen2.5-VL-3B, significantly outperforms the larger-scale model, GPT-4o-mini,\nby 21.33% and surpasses other work trained on Qwen2.5-VL-7B by 20.33% on the\nEmbodiedBench benchmark.",
            "headline_zh": "提出RoboGPT-R1两阶段微调框架以增强机器人在长视距操作任务中的规划能力",
            "intro_zh": [
                "核心问题：通用视觉语言模型在机器人规划中泛化差且物理理解不足，难以处理复杂长视距任务。",
                "方法要点：采用监督微调获取基础知识，再通过强化学习提升视觉空间理解和推理能力。",
                "实验或效果：在EmbodiedBench基准上，显著超越GPT-4o-mini和其他模型，提升超过20%。"
            ],
            "tags_zh": [
                "机器人规划",
                "强化学习",
                "视觉语言模型",
                "长视距操作",
                "两阶段微调",
                "奖励函数设计"
            ],
            "_index": 41
        },
        {
            "title": "Neural Implicit Flow Fields for Spatio-Temporal Motion Mapping",
            "authors": [
                "Yufei Zhu",
                "Shih-Min Yang",
                "Andrey Rudenko",
                "Tomasz P. Kucner",
                "Achim J. Lilienthal",
                "Martin Magnusson"
            ],
            "arxiv_id": "2510.14827v1",
            "summary": "Safe and efficient robot operation in complex human environments can benefit\nfrom good models of site-specific motion patterns. Maps of Dynamics (MoDs)\nprovide such models by encoding statistical motion patterns in a map, but\nexisting representations use discrete spatial sampling and typically require\ncostly offline construction. We propose a continuous spatio-temporal MoD\nrepresentation based on implicit neural functions that directly map coordinates\nto the parameters of a Semi-Wrapped Gaussian Mixture Model. This removes the\nneed for discretization and imputation for unevenly sampled regions, enabling\nsmooth generalization across both space and time. Evaluated on a large public\ndataset with long-term real-world people tracking data, our method achieves\nbetter accuracy of motion representation and smoother velocity distributions in\nsparse regions while still being computationally efficient, compared to\navailable baselines. The proposed approach demonstrates a powerful and\nefficient way of modeling complex human motion patterns.",
            "headline_zh": "提出基于隐式神经函数的连续时空运动映射方法，以提升机器人在复杂人类环境中的安全与效率。",
            "intro_zh": [
                "核心问题：现有动态地图使用离散空间采样，构建成本高且难以处理不均匀采样区域。",
                "方法要点：利用隐式神经函数直接映射坐标到半包裹高斯混合模型参数，实现连续时空建模。",
                "实验或效果：在真实世界人类跟踪数据集上，相比基线方法，精度更高、稀疏区域速度分布更平滑。"
            ],
            "tags_zh": [
                "隐式神经表示",
                "动态地图建模",
                "时空运动模式",
                "机器人导航",
                "高斯混合模型"
            ],
            "_index": 42
        },
        {
            "title": "Supervised Fine-Tuning or Contrastive Learning? Towards Better Multimodal LLM Reranking",
            "authors": [
                "Ziqi Dai",
                "Xin Zhang",
                "Mingxin Li",
                "Yanzhao Zhang",
                "Dingkun Long",
                "Pengjun Xie",
                "Meishan Zhang",
                "Wenjie Li",
                "Min Zhang"
            ],
            "arxiv_id": "2510.14824v1",
            "summary": "In information retrieval, training reranking models mainly focuses on two\ntypes of objectives: metric learning (e.g. contrastive loss to increase the\npredicted scores on relevant query-document pairs) and classification (binary\nlabel prediction of relevance vs. irrelevance). For BERT-style encoders,\nvarious studies have shown that contrastive learning (CL) can be more effective\nthan discriminative (classification) learning. However, for large language\nmodels (LLMs), classification via supervised fine-tuning (SFT), which predicts\n''yes'' (resp. ''no'') token for relevant (resp. irrelevant) pairs, appears\nmore promising as it aligns well with the generative nature of LLMs. This\ndivergence raises a central question: which objective is intrinsically better\nsuited to LLM-based reranking, and what mechanism underlies the difference? In\nthis work, we conduct a comprehensive comparison and analysis between CL and\nSFT for reranking, taking the universal multimodal retrieval (UMR) as the\nexperimental playground. We first decompose the objectives into two components:\nweight, which controls the magnitude of those updates, and direction, which\nguides the model updates, then present a unified framework for understanding\ntheir interactions. Through probing experiments, we find that SFT provides a\nsubstantially stronger weighting scheme than CL, whereas the preferred scoring\ndirection shows no clear winner. Taken together, these results point to a\nconsistent advantage of SFT over CL for LLM reranking. To further validate our\nfindings, we conduct large-scale training with SFT and present new\nstate-of-the-art rerankers on the MRB benchmark. We also provide ablations on\nSFT settings and expect our findings to benefit future research and\napplications in this area.",
            "headline_zh": "比较监督微调与对比学习在LLM重排序中的优劣，发现SFT更优",
            "intro_zh": [
                "核心问题：LLM重排序中，监督微调与对比学习哪种目标更有效？",
                "方法要点：将目标分解为权重和方向，提出统一分析框架",
                "实验或效果：SFT权重更强，在MRB基准上实现SOTA性能"
            ],
            "tags_zh": [
                "LLM重排序",
                "监督微调",
                "对比学习",
                "多模态检索",
                "权重分析"
            ],
            "_index": 43
        },
        {
            "title": "FraQAT: Quantization Aware Training with Fractional bits",
            "authors": [
                "Luca Morreale",
                "Alberto Gil C. P. Ramos",
                "Malcolm Chadwick",
                "Mehid Noroozi",
                "Ruchika Chavhan",
                "Abhinav Mehrotra",
                "Sourav Bhattacharya"
            ],
            "arxiv_id": "2510.14823v1",
            "summary": "State-of-the-art (SOTA) generative models have demonstrated impressive\ncapabilities in image synthesis or text generation, often with a large capacity\nmodel. However, these large models cannot be deployed on smartphones due to the\nlimited availability of on-board memory and computations. Quantization methods\nlower the precision of the model parameters, allowing for efficient\ncomputations, \\eg, in \\INT{8}. Although aggressive quantization addresses\nefficiency and memory constraints, preserving the quality of the model remains\na challenge. To retain quality in previous aggressive quantization, we propose\na new fractional bits quantization (\\short) approach. The novelty is a simple\nyet effective idea: we progressively reduce the model's precision from 32 to 4\nbits per parameter, and exploit the fractional bits during optimization to\nmaintain high generation quality. We show that the \\short{} yields improved\nquality on a variety of diffusion models, including SD3.5-Medium, Sana,\n\\pixart, and FLUX.1-schnell, while achieving $4-7\\%$ lower FiD than standard\nQAT. Finally, we deploy and run Sana on a Samsung S25U, which runs on the\nQualcomm SM8750-AB Snapdragon 8 Elite Hexagon Tensor Processor (HTP).",
            "headline_zh": "提出分数位量化方法以在移动设备上保持生成模型质量",
            "intro_zh": [
                "大模型在移动设备部署受限于内存和计算资源，量化后质量下降。",
                "方法逐步从32位降至4位量化，利用分数位优化维持生成质量。",
                "在多种扩散模型上验证，FiD降低4-7%，并在三星S25U上成功部署。"
            ],
            "tags_zh": [
                "量化感知训练",
                "分数位量化",
                "生成模型",
                "移动部署",
                "扩散模型",
                "模型压缩"
            ],
            "_index": 44
        },
        {
            "title": "Unifying Environment Perception and Route Choice Modeling for Trajectory Representation Learning",
            "authors": [
                "Ji Cao",
                "Yu Wang",
                "Tongya Zheng",
                "Zujie Ren",
                "Canghong Jin",
                "Gang Chen",
                "Mingli Song"
            ],
            "arxiv_id": "2510.14819v1",
            "summary": "Trajectory Representation Learning (TRL) aims to encode raw trajectories into\nlow-dimensional vectors, which can then be leveraged in various downstream\ntasks, including travel time estimation, location prediction, and trajectory\nsimilarity analysis. However, existing TRL methods suffer from a key oversight:\ntreating trajectories as isolated spatio-temporal sequences, without\nconsidering the external environment and internal route choice behavior that\ngovern their formation. To bridge this gap, we propose a novel framework that\nunifies comprehensive environment \\textbf{P}erception and explicit\n\\textbf{R}oute choice modeling for effective \\textbf{Traj}ectory representation\nlearning, dubbed \\textbf{PRTraj}. Specifically, PRTraj first introduces an\nEnvironment Perception Module to enhance the road network by capturing\nmulti-granularity environmental semantics from surrounding POI distributions.\nBuilding on this environment-aware backbone, a Route Choice Encoder then\ncaptures the route choice behavior inherent in each trajectory by modeling its\nconstituent road segment transitions as a sequence of decisions. These\nroute-choice-aware representations are finally aggregated to form the global\ntrajectory embedding. Extensive experiments on 3 real-world datasets across 5\ndownstream tasks validate the effectiveness and generalizability of PRTraj.\nMoreover, PRTraj demonstrates strong data efficiency, maintaining robust\nperformance under few-shot scenarios. Our code is available at:\nhttps://anonymous.4open.science/r/PRTraj.",
            "headline_zh": "提出PRTraj框架，统一环境感知与路径选择建模以改进轨迹表示学习",
            "intro_zh": [
                "现有轨迹表示学习方法忽略外部环境和内部路径选择行为，导致表示不完整",
                "PRTraj通过环境感知模块和路径选择编码器，捕获多粒度语义和决策序列",
                "在3个真实数据集和5个下游任务中验证有效性，并展示强数据效率"
            ],
            "tags_zh": [
                "轨迹表示学习",
                "环境感知",
                "路径选择建模",
                "多粒度语义",
                "下游任务评估",
                "数据效率"
            ],
            "_index": 45
        },
        {
            "title": "Scaling Artificial Intelligence for Multi-Tumor Early Detection with More Reports, Fewer Masks",
            "authors": [
                "Pedro R. A. S. Bassi",
                "Xinze Zhou",
                "Wenxuan Li",
                "Szymon Płotka",
                "Jieneng Chen",
                "Qi Chen",
                "Zheren Zhu",
                "Jakub Prządo",
                "Ibrahim E. Hamacı",
                "Sezgin Er",
                "Yuhan Wang",
                "Ashwin Kumar",
                "Bjoern Menze",
                "Jarosław B. Ćwikła",
                "Yuyin Zhou",
                "Akshay S. Chaudhari",
                "Curtis P. Langlotz",
                "Sergio Decherchi",
                "Andrea Cavalli",
                "Kang Wang",
                "Yang Yang",
                "Alan L. Yuille",
                "Zongwei Zhou"
            ],
            "arxiv_id": "2510.14803v1",
            "summary": "Early tumor detection save lives. Each year, more than 300 million computed\ntomography (CT) scans are performed worldwide, offering a vast opportunity for\neffective cancer screening. However, detecting small or early-stage tumors on\nthese CT scans remains challenging, even for experts. Artificial intelligence\n(AI) models can assist by highlighting suspicious regions, but training such\nmodels typically requires extensive tumor masks--detailed, voxel-wise outlines\nof tumors manually drawn by radiologists. Drawing these masks is costly,\nrequiring years of effort and millions of dollars. In contrast, nearly every CT\nscan in clinical practice is already accompanied by medical reports describing\nthe tumor's size, number, appearance, and sometimes, pathology\nresults--information that is rich, abundant, and often underutilized for AI\ntraining. We introduce R-Super, which trains AI to segment tumors that match\ntheir descriptions in medical reports. This approach scales AI training with\nlarge collections of readily available medical reports, substantially reducing\nthe need for manually drawn tumor masks. When trained on 101,654 reports, AI\nmodels achieved performance comparable to those trained on 723 masks. Combining\nreports and masks further improved sensitivity by +13% and specificity by +8%,\nsurpassing radiologists in detecting five of the seven tumor types. Notably,\nR-Super enabled segmentation of tumors in the spleen, gallbladder, prostate,\nbladder, uterus, and esophagus, for which no public masks or AI models\npreviously existed. This study challenges the long-held belief that\nlarge-scale, labor-intensive tumor mask creation is indispensable, establishing\na scalable and accessible path toward early detection across diverse tumor\ntypes.\n  We plan to release our trained models, code, and dataset at\nhttps://github.com/MrGiovanni/R-Super",
            "headline_zh": "提出R-Super方法，利用医学报告训练AI进行多肿瘤分割，减少对人工标注的依赖。",
            "intro_zh": [
                "核心问题：AI肿瘤分割需大量人工标注，成本高昂且难以扩展。",
                "方法要点：使用医学报告描述训练AI模型，匹配肿瘤特征进行分割。",
                "实验或效果：在101,654份报告上训练，性能媲美723个掩码，组合使用提升敏感性和特异性。"
            ],
            "tags_zh": [
                "肿瘤分割",
                "医学报告利用",
                "AI训练扩展",
                "多肿瘤检测",
                "CT扫描分析"
            ],
            "_index": 46
        },
        {
            "title": "Morphology-Aware Prognostic model for Five-Year Survival Prediction in Colorectal Cancer from H&E Whole Slide Images",
            "authors": [
                "Usama Sajjad",
                "Abdul Rehman Akbar",
                "Ziyu Su",
                "Deborah Knight",
                "Wendy L. Frankel",
                "Metin N. Gurcan",
                "Wei Chen",
                "Muhammad Khalid Khan Niazi"
            ],
            "arxiv_id": "2510.14800v1",
            "summary": "Colorectal cancer (CRC) remains the third most prevalent malignancy globally,\nwith approximately 154,000 new cases and 54,000 projected deaths anticipated\nfor 2025. The recent advancement of foundation models in computational\npathology has been largely propelled by task agnostic methodologies that can\noverlook organ-specific crucial morphological patterns that represent distinct\nbiological processes that can fundamentally influence tumor behavior,\ntherapeutic response, and patient outcomes. The aim of this study is to develop\na novel, interpretable AI model, PRISM (Prognostic Representation of Integrated\nSpatial Morphology), that incorporates a continuous variability spectrum within\neach distinct morphology to characterize phenotypic diversity and reflecting\nthe principle that malignant transformation occurs through incremental\nevolutionary processes rather than abrupt phenotypic shifts. PRISM is trained\non 8.74 million histological images extracted from surgical resection specimens\nof 424 patients with stage III CRC. PRISM achieved superior prognostic\nperformance for five-year OS (AUC = 0.70 +- 0.04; accuracy = 68.37% +- 4.75%;\nHR = 3.34, 95% CI = 2.28-4.90; p < 0.0001), outperforming existing CRC-specific\nmethods by 15% and AI foundation models by ~23% accuracy. It showed\nsex-agnostic robustness (AUC delta = 0.02; accuracy delta = 0.15%) and stable\nperformance across clinicopathological subgroups, with minimal accuracy\nfluctuation (delta = 1.44%) between 5FU/LV and CPT-11/5FU/LV regimens,\nreplicating the Alliance cohort finding of no survival difference between\ntreatments.",
            "headline_zh": "提出PRISM模型以预测结直肠癌五年生存率，整合形态学连续变异谱",
            "intro_zh": [
                "核心问题：现有AI模型忽视器官特异性形态模式，影响结直肠癌预后预测准确性。",
                "方法要点：开发PRISM模型，基于形态学连续变异谱表征表型多样性，增强可解释性。",
                "实验或效果：在424患者数据上训练，AUC达0.70，准确率68.37%，优于现有方法。"
            ],
            "tags_zh": [
                "结直肠癌预后预测",
                "形态学AI模型",
                "全切片图像分析",
                "生存分析",
                "可解释人工智能"
            ],
            "_index": 47
        },
        {
            "title": "CoT-PL: Visual Chain-of-Thought Reasoning Meets Pseudo-Labeling for Open-Vocabulary Object Detection",
            "authors": [
                "Hojun Choi",
                "Youngsun Lim",
                "Jaeyo Shin",
                "Hyunjung Shim"
            ],
            "arxiv_id": "2510.14792v1",
            "summary": "Open-vocabulary object detection (OVD) seeks to recognize and localize object\ncategories beyond those seen during training. Recent approaches typically\nleverage vision-language models (VLMs) to generate pseudo-labels using\nimage-text alignment, allowing detectors to generalize to unseen classes\nwithout explicit supervision. However, these methods depend heavily on direct\nimage-text matching, neglecting the intermediate reasoning steps essential for\ninterpreting semantically complex scenes. This results in limited robustness\nwhen confronted with crowded or occluded visual contexts. In this paper, we\nintroduce CoT-PL, a new framework that employs structured visual\nchain-of-thought (CoT) reasoning into the pseudo-labeling process. CoT-PL\ndecomposes object understanding into three interpretable steps: (1) region\nperception even for unseen objects, (2) category recognition via zero-shot\nreasoning, and (3) background grounding to separate semantically complex\nobjects. Crucially, the third step naturally motivates our contrastive\nbackground learning (CBL) that uses the pre-computed background cues as\nnegatives to promote feature disentanglement between objects and background. In\nthis way, CoT reasoning and CBL form an integrated pipeline tailored to robust\npseudo-labeling in crowded or occluded scenes. Notably, in these two settings,\nour novel-class pseudo-label quality achieves relative improvements of 103.4%\nand 168.4% over the best prior, respectively. Our extensive experiments\ndemonstrate that CoT-PL achieves +7.7 AP50 on open-vocabulary COCO and +2.9\nmask AP on LVIS for novel classes, setting a new state of the art.",
            "headline_zh": "提出CoT-PL框架，结合视觉链式推理与伪标注，提升拥挤或遮挡场景下的开放词汇目标检测鲁棒性。",
            "intro_zh": [
                "核心问题：现有方法依赖直接图像-文本匹配，忽略中间推理步骤，导致在拥挤或遮挡场景中鲁棒性不足。",
                "方法要点：引入视觉链式推理，分解对象理解为区域感知、类别识别和背景接地三步，并集成对比背景学习。",
                "实验或效果：在开放词汇COCO和LVIS数据集上，新类检测性能显著提升，伪标签质量相对改进超100%。"
            ],
            "tags_zh": [
                "开放词汇目标检测",
                "视觉链式推理",
                "伪标注",
                "对比背景学习",
                "零样本识别",
                "场景鲁棒性"
            ],
            "_index": 48
        },
        {
            "title": "SkyDreamer: Interpretable End-to-End Vision-Based Drone Racing with Model-Based Reinforcement Learning",
            "authors": [
                "Aderik Verraest",
                "Stavrow Bahnam",
                "Robin Ferede",
                "Guido de Croon",
                "Christophe De Wagter"
            ],
            "arxiv_id": "2510.14783v1",
            "summary": "Autonomous drone racing (ADR) systems have recently achieved champion-level\nperformance, yet remain highly specific to drone racing. While end-to-end\nvision-based methods promise broader applicability, no system to date\nsimultaneously achieves full sim-to-real transfer, onboard execution, and\nchampion-level performance. In this work, we present SkyDreamer, to the best of\nour knowledge, the first end-to-end vision-based ADR policy that maps directly\nfrom pixel-level representations to motor commands. SkyDreamer builds on\ninformed Dreamer, a model-based reinforcement learning approach where the world\nmodel decodes to privileged information only available during training. By\nextending this concept to end-to-end vision-based ADR, the world model\neffectively functions as an implicit state and parameter estimator, greatly\nimproving interpretability. SkyDreamer runs fully onboard without external aid,\nresolves visual ambiguities by tracking progress using the state decoded from\nthe world model's hidden state, and requires no extrinsic camera calibration,\nenabling rapid deployment across different drones without retraining.\nReal-world experiments show that SkyDreamer achieves robust, high-speed flight,\nexecuting tight maneuvers such as an inverted loop, a split-S and a ladder,\nreaching speeds of up to 21 m/s and accelerations of up to 6 g. It further\ndemonstrates a non-trivial visual sim-to-real transfer by operating on\npoor-quality segmentation masks, and exhibits robustness to battery depletion\nby accurately estimating the maximum attainable motor RPM and adjusting its\nflight path in real-time. These results highlight SkyDreamer's adaptability to\nimportant aspects of the reality gap, bringing robustness while still achieving\nextremely high-speed, agile flight.",
            "headline_zh": "提出SkyDreamer以解决端到端视觉无人机竞速的鲁棒性与高性能问题",
            "intro_zh": [
                "核心问题：现有自主无人机竞速系统缺乏端到端视觉方法，无法同时实现全仿真到真实转移、机载执行和冠军级性能。",
                "方法要点：基于模型强化学习，世界模型解码特权信息，实现隐式状态估计，提升可解释性。",
                "实验或效果：在真实世界实现高速飞行，最高速度21 m/s，加速度6 g，并展示对视觉模糊和电池耗尽的鲁棒性。"
            ],
            "tags_zh": [
                "无人机竞速",
                "端到端视觉",
                "模型强化学习",
                "仿真到真实转移",
                "机载执行",
                "状态估计"
            ],
            "_index": 49
        },
        {
            "title": "Open TeleDex: A Hardware-Agnostic Teleoperation System for Imitation Learning based Dexterous Manipulation",
            "authors": [
                "Xu Chi",
                "Chao Zhang",
                "Yang Su",
                "Lingfeng Dou",
                "Fujia Yang",
                "Jiakuo Zhao",
                "Haoyu Zhou",
                "Xiaoyou Jia",
                "Yong Zhou",
                "Shan An"
            ],
            "arxiv_id": "2510.14771v1",
            "summary": "Accurate and high-fidelity demonstration data acquisition is a critical\nbottleneck for deploying robot Imitation Learning (IL) systems, particularly\nwhen dealing with heterogeneous robotic platforms. Existing teleoperation\nsystems often fail to guarantee high-precision data collection across diverse\ntypes of teleoperation devices. To address this, we developed Open TeleDex, a\nunified teleoperation framework engineered for demonstration data collection.\nOpen TeleDex specifically tackles the TripleAny challenge, seamlessly\nsupporting any robotic arm, any dexterous hand, and any external input device.\nFurthermore, we propose a novel hand pose retargeting algorithm that\nsignificantly boosts the interoperability of Open TeleDex, enabling robust and\naccurate compatibility with an even wider spectrum of heterogeneous master and\nslave equipment. Open TeleDex establishes a foundational, high-quality, and\npublicly available platform for accelerating both academic research and\nindustry development in complex robotic manipulation and IL.",
            "headline_zh": "提出Open TeleDex统一遥操作系统以解决异构机器人平台模仿学习数据采集难题",
            "intro_zh": [
                "核心问题：异构机器人平台难以获取高精度演示数据，阻碍模仿学习部署。",
                "方法要点：开发硬件无关框架，支持任意机械臂、灵巧手和输入设备。",
                "实验或效果：提出新手部姿态重定向算法，提升系统兼容性和数据质量。"
            ],
            "tags_zh": [
                "遥操作系统",
                "模仿学习",
                "灵巧操作",
                "数据采集",
                "硬件无关性",
                "姿态重定向"
            ],
            "_index": 50
        },
        {
            "title": "MoCom: Motion-based Inter-MAV Visual Communication Using Event Vision and Spiking Neural Networks",
            "authors": [
                "Zhang Nengbo",
                "Hann Woei Ho",
                "Ye Zhou"
            ],
            "arxiv_id": "2510.14770v1",
            "summary": "Reliable communication in Micro Air Vehicle (MAV) swarms is challenging in\nenvironments, where conventional radio-based methods suffer from spectrum\ncongestion, jamming, and high power consumption. Inspired by the waggle dance\nof honeybees, which efficiently communicate the location of food sources\nwithout sound or contact, we propose a novel visual communication framework for\nMAV swarms using motion-based signaling. In this framework, MAVs convey\ninformation, such as heading and distance, through deliberate flight patterns,\nwhich are passively captured by event cameras and interpreted using a\npredefined visual codebook of four motion primitives: vertical (up/down),\nhorizontal (left/right), left-to-up-to-right, and left-to-down-to-right,\nrepresenting control symbols (``start'', ``end'', ``1'', ``0''). To decode\nthese signals, we design an event frame-based segmentation model and a\nlightweight Spiking Neural Network (SNN) for action recognition. An integrated\ndecoding algorithm then combines segmentation and classification to robustly\ninterpret MAV motion sequences. Experimental results validate the framework's\neffectiveness, which demonstrates accurate decoding and low power consumption,\nand highlights its potential as an energy-efficient alternative for MAV\ncommunication in constrained environments.",
            "headline_zh": "提出基于运动和事件视觉的MAV间通信框架以解决受限环境中的通信挑战",
            "intro_zh": [
                "核心问题：MAV群在频谱拥堵和干扰环境中可靠通信困难，传统无线电方法功耗高",
                "方法要点：使用事件相机捕捉MAV飞行模式，通过SNN和预定义运动基元解码信息",
                "实验或效果：实验验证解码准确且功耗低，适合受限环境中的高效通信"
            ],
            "tags_zh": [
                "微飞行器通信",
                "事件视觉",
                "脉冲神经网络",
                "运动基元",
                "视觉解码"
            ],
            "_index": 51
        },
        {
            "title": "Leveraging Neural Descriptor Fields for Learning Contact-Aware Dynamic Recovery",
            "authors": [
                "Fan Yang",
                "Zixuan Huang",
                "Abhinav Kumar",
                "Sergio Aguilera Marinovic",
                "Soshi Iba",
                "Rana Soltani Zarrin",
                "Dmitry Berenson"
            ],
            "arxiv_id": "2510.14768v1",
            "summary": "Real-world dexterous manipulation often encounters unexpected errors and\ndisturbances, which can lead to catastrophic failures, such as dropping the\nmanipulated object. To address this challenge, we focus on the problem of\ncatching a falling object while it remains within grasping range and,\nimportantly, resetting the system to a configuration favorable for resuming the\nprimary manipulation task. We propose Contact-Aware Dynamic Recovery (CADRE), a\nreinforcement learning framework that incorporates a Neural Descriptor Field\n(NDF)-inspired module to extract implicit contact features. Compared to methods\nthat rely solely on object pose or point cloud input, NDFs can directly reason\nabout finger-object correspondence and adapt to different object geometries.\nOur experiments show that incorporating contact features improves training\nefficiency, enhances convergence performance for RL training, and ultimately\nleads to more successful recoveries. Additionally, we demonstrate that CADRE\ncan generalize zero-shot to unseen objects with different geometries.",
            "headline_zh": "提出CADRE框架以解决灵巧操作中动态恢复问题",
            "intro_zh": [
                "核心问题：现实灵巧操作易受干扰，导致物体掉落等失败。",
                "方法要点：结合NDF提取隐式接触特征，提升手指-物体对应推理。",
                "实验效果：提高训练效率与收敛性能，零样本泛化至新物体。"
            ],
            "tags_zh": [
                "强化学习",
                "神经描述场",
                "动态恢复",
                "灵巧操作",
                "零样本泛化"
            ],
            "_index": 52
        },
        {
            "title": "Inpainting the Red Planet: Diffusion Models for the Reconstruction of Martian Environments in Virtual Reality",
            "authors": [
                "Giuseppe Lorenzo Catalano",
                "Agata Marta Soccini"
            ],
            "arxiv_id": "2510.14765v1",
            "summary": "Space exploration increasingly relies on Virtual Reality for several tasks,\nsuch as mission planning, multidisciplinary scientific analysis, and astronaut\ntraining. A key factor for the reliability of the simulations is having\naccurate 3D representations of planetary terrains. Extraterrestrial heightmaps\nderived from satellite imagery often contain missing values due to acquisition\nand transmission constraints. Mars is among the most studied planets beyond\nEarth, and its extensive terrain datasets make the Martian surface\nreconstruction a valuable task, although many areas remain unmapped. Deep\nlearning algorithms can support void-filling tasks; however, whereas Earth's\ncomprehensive datasets enables the use of conditional methods, such approaches\ncannot be applied to Mars. Current approaches rely on simpler interpolation\ntechniques which, however, often fail to preserve geometric coherence. In this\nwork, we propose a method for reconstructing the surface of Mars based on an\nunconditional diffusion model. Training was conducted on an augmented dataset\nof 12000 Martian heightmaps derived from NASA's HiRISE survey. A\nnon-homogeneous rescaling strategy captures terrain features across multiple\nscales before resizing to a fixed 128x128 model resolution. We compared our\nmethod against established void-filling and inpainting techniques, including\nInverse Distance Weighting, kriging, and Navier-Stokes algorithm, on an\nevaluation set of 1000 samples. Results show that our approach consistently\noutperforms these methods in terms of reconstruction accuracy (4-15% on RMSE)\nand perceptual similarity (29-81% on LPIPS) with the original data.",
            "headline_zh": "提出无条件扩散模型以重建火星地形缺失数据",
            "intro_zh": [
                "火星高度图常因采集限制存在缺失值，影响虚拟现实模拟准确性",
                "使用无条件扩散模型，在12000张增强高度图上训练，多尺度特征提取",
                "相比传统方法，重建精度和感知相似性显著提升，RMSE降低4-15%"
            ],
            "tags_zh": [
                "火星地形重建",
                "扩散模型",
                "高度图修复",
                "虚拟现实",
                "深度学习"
            ],
            "_index": 53
        },
        {
            "title": "LightQANet: Quantized and Adaptive Feature Learning for Low-Light Image Enhancement",
            "authors": [
                "Xu Wu",
                "Zhihui Lai",
                "Xianxu Hou",
                "Jie Zhou",
                "Ya-nan Zhang",
                "Linlin Shen"
            ],
            "arxiv_id": "2510.14753v1",
            "summary": "Low-light image enhancement (LLIE) aims to improve illumination while\npreserving high-quality color and texture. However, existing methods often fail\nto extract reliable feature representations due to severely degraded\npixel-level information under low-light conditions, resulting in poor texture\nrestoration, color inconsistency, and artifact. To address these challenges, we\npropose LightQANet, a novel framework that introduces quantized and adaptive\nfeature learning for low-light enhancement, aiming to achieve consistent and\nrobust image quality across diverse lighting conditions. From the static\nmodeling perspective, we design a Light Quantization Module (LQM) to explicitly\nextract and quantify illumination-related factors from image features. By\nenforcing structured light factor learning, LQM enhances the extraction of\nlight-invariant representations and mitigates feature inconsistency across\nvarying illumination levels. From the dynamic adaptation perspective, we\nintroduce a Light-Aware Prompt Module (LAPM), which encodes illumination priors\ninto learnable prompts to dynamically guide the feature learning process. LAPM\nenables the model to flexibly adapt to complex and continuously changing\nlighting conditions, further improving image enhancement. Extensive experiments\non multiple low-light datasets demonstrate that our method achieves\nstate-of-the-art performance, delivering superior qualitative and quantitative\nresults across various challenging lighting scenarios.",
            "headline_zh": "提出LightQANet，通过量化与自适应特征学习增强低光图像",
            "intro_zh": [
                "核心问题：低光图像像素信息退化，导致纹理恢复差、颜色不一致和伪影",
                "方法要点：设计LQM量化光照因素，LAPM用可学习提示动态引导特征学习",
                "实验或效果：在多个数据集上实现SOTA，提升各种光照场景的图像质量"
            ],
            "tags_zh": [
                "低光图像增强",
                "量化特征学习",
                "自适应学习",
                "光照建模",
                "图像质量提升"
            ],
            "_index": 54
        },
        {
            "title": "DEXTER: Diffusion-Guided EXplanations with TExtual Reasoning for Vision Models",
            "authors": [
                "Simone Carnemolla",
                "Matteo Pennisi",
                "Sarinda Samarasinghe",
                "Giovanni Bellitto",
                "Simone Palazzo",
                "Daniela Giordano",
                "Mubarak Shah",
                "Concetto Spampinato"
            ],
            "arxiv_id": "2510.14741v1",
            "summary": "Understanding and explaining the behavior of machine learning models is\nessential for building transparent and trustworthy AI systems. We introduce\nDEXTER, a data-free framework that employs diffusion models and large language\nmodels to generate global, textual explanations of visual classifiers. DEXTER\noperates by optimizing text prompts to synthesize class-conditional images that\nstrongly activate a target classifier. These synthetic samples are then used to\nelicit detailed natural language reports that describe class-specific decision\npatterns and biases. Unlike prior work, DEXTER enables natural language\nexplanation about a classifier's decision process without access to training\ndata or ground-truth labels. We demonstrate DEXTER's flexibility across three\ntasks-activation maximization, slice discovery and debiasing, and bias\nexplanation-each illustrating its ability to uncover the internal mechanisms of\nvisual classifiers. Quantitative and qualitative evaluations, including a user\nstudy, show that DEXTER produces accurate, interpretable outputs. Experiments\non ImageNet, Waterbirds, CelebA, and FairFaces confirm that DEXTER outperforms\nexisting approaches in global model explanation and class-level bias reporting.\nCode is available at https://github.com/perceivelab/dexter.",
            "headline_zh": "提出DEXTER框架，利用扩散模型和大型语言模型生成视觉分类器的全局文本解释。",
            "intro_zh": [
                "核心问题：理解视觉分类器决策过程，无需训练数据或真实标签。",
                "方法要点：优化文本提示合成类条件图像，激活分类器并生成自然语言报告。",
                "实验或效果：在ImageNet等数据集上验证，优于现有方法，输出准确可解释。"
            ],
            "tags_zh": [
                "视觉模型解释",
                "扩散模型",
                "大型语言模型",
                "数据自由框架",
                "分类器偏见分析"
            ],
            "_index": 55
        },
        {
            "title": "Free-Grained Hierarchical Recognition",
            "authors": [
                "Seulki Park",
                "Zilin Wang",
                "Stella X. Yu"
            ],
            "arxiv_id": "2510.14737v1",
            "summary": "Hierarchical image classification predicts labels across a semantic taxonomy,\nbut existing methods typically assume complete, fine-grained annotations, an\nassumption rarely met in practice. Real-world supervision varies in\ngranularity, influenced by image quality, annotator expertise, and task\ndemands; a distant bird may be labeled Bird, while a close-up reveals Bald\neagle. We introduce ImageNet-F, a large-scale benchmark curated from ImageNet\nand structured into cognitively inspired basic, subordinate, and fine-grained\nlevels. Using CLIP as a proxy for semantic ambiguity, we simulate realistic,\nmixed-granularity labels reflecting human annotation behavior. We propose\nfree-grain learning, with heterogeneous supervision across instances. We\ndevelop methods that enhance semantic guidance via pseudo-attributes from\nvision-language models and visual guidance via semi-supervised learning. These,\nalong with strong baselines, substantially improve performance under mixed\nsupervision. Together, our benchmark and methods advance hierarchical\nclassification under real-world constraints.",
            "headline_zh": "提出自由粒度学习以解决混合粒度监督下的层次图像分类问题",
            "intro_zh": [
                "核心问题：现实世界图像标注粒度不一，现有方法依赖完整细粒度标注，难以实用。",
                "方法要点：引入ImageNet-F基准，利用CLIP模拟混合粒度标签，开发伪属性和半监督方法。",
                "实验或效果：在混合监督下，所提方法显著提升层次分类性能，优于强基线。"
            ],
            "tags_zh": [
                "层次图像分类",
                "混合粒度监督",
                "自由粒度学习",
                "ImageNet-F基准",
                "CLIP模型",
                "半监督学习"
            ],
            "_index": 56
        },
        {
            "title": "Cross-Layer Feature Self-Attention Module for Multi-Scale Object Detection",
            "authors": [
                "Dingzhou Xie",
                "Rushi Lan",
                "Cheng Pang",
                "Enhao Ning",
                "Jiahao Zeng",
                "Wei Zheng"
            ],
            "arxiv_id": "2510.14726v1",
            "summary": "Recent object detection methods have made remarkable progress by leveraging\nattention mechanisms to improve feature discriminability. However, most\nexisting approaches are confined to refining single-layer or fusing dual-layer\nfeatures, overlooking the rich inter-layer dependencies across multi-scale\nrepresentations. This limits their ability to capture comprehensive contextual\ninformation essential for detecting objects with large scale variations. In\nthis paper, we propose a novel Cross-Layer Feature Self-Attention Module\n(CFSAM), which holistically models both local and global dependencies within\nmulti-scale feature maps. CFSAM consists of three key components: a\nconvolutional local feature extractor, a Transformer-based global modeling unit\nthat efficiently captures cross-layer interactions, and a feature fusion\nmechanism to restore and enhance the original representations. When integrated\ninto the SSD300 framework, CFSAM significantly boosts detection performance,\nachieving 78.6% mAP on PASCAL VOC (vs. 75.5% baseline) and 52.1% mAP on COCO\n(vs. 43.1% baseline), outperforming existing attention modules. Moreover, the\nmodule accelerates convergence during training without introducing substantial\ncomputational overhead. Our work highlights the importance of explicit\ncross-layer attention modeling in advancing multi-scale object detection.",
            "headline_zh": "提出跨层特征自注意力模块以提升多尺度目标检测性能",
            "intro_zh": [
                "现有方法忽视多尺度特征间的层间依赖，限制检测大尺度变化物体的能力",
                "模块结合卷积局部特征提取、Transformer全局建模和特征融合，增强多尺度表示",
                "集成SSD300显著提升PASCAL VOC和COCO数据集mAP，加速收敛且计算开销小"
            ],
            "tags_zh": [
                "多尺度目标检测",
                "跨层注意力",
                "特征融合",
                "自注意力机制",
                "SSD框架"
            ],
            "_index": 57
        },
        {
            "title": "Camera Movement Classification in Historical Footage: A Comparative Study of Deep Video Models",
            "authors": [
                "Tingyu Lin",
                "Armin Dadras",
                "Florian Kleber",
                "Robert Sablatnig"
            ],
            "arxiv_id": "2510.14713v1",
            "summary": "Camera movement conveys spatial and narrative information essential for\nunderstanding video content. While recent camera movement classification (CMC)\nmethods perform well on modern datasets, their generalization to historical\nfootage remains unexplored. This paper presents the first systematic evaluation\nof deep video CMC models on archival film material. We summarize representative\nmethods and datasets, highlighting differences in model design and label\ndefinitions. Five standard video classification models are assessed on the\nHISTORIAN dataset, which includes expert-annotated World War II footage. The\nbest-performing model, Video Swin Transformer, achieves 80.25% accuracy,\nshowing strong convergence despite limited training data. Our findings\nhighlight the challenges and potential of adapting existing models to\nlow-quality video and motivate future work combining diverse input modalities\nand temporal architectures.",
            "headline_zh": "评估深度视频模型在历史影像中的相机运动分类性能",
            "intro_zh": [
                "核心问题：现有相机运动分类方法在现代数据集表现良好，但泛化到历史影像未知。",
                "方法要点：系统评估五种标准视频分类模型，使用Video Swin Transformer等。",
                "实验或效果：在HISTORIAN数据集上，最佳模型准确率达80.25%，显示强收敛性。"
            ],
            "tags_zh": [
                "相机运动分类",
                "历史影像分析",
                "视频分类模型",
                "Video Swin Transformer",
                "HISTORIAN数据集"
            ],
            "_index": 58
        },
        {
            "title": "Where are the Whales: A Human-in-the-loop Detection Method for Identifying Whales in High-resolution Satellite Imagery",
            "authors": [
                "Caleb Robinson",
                "Kimberly T. Goetz",
                "Christin B. Khan",
                "Meredith Sackett",
                "Kathleen Leonard",
                "Rahul Dodhia",
                "Juan M. Lavista Ferres"
            ],
            "arxiv_id": "2510.14709v1",
            "summary": "Effective monitoring of whale populations is critical for conservation, but\ntraditional survey methods are expensive and difficult to scale. While prior\nwork has shown that whales can be identified in very high-resolution (VHR)\nsatellite imagery, large-scale automated detection remains challenging due to a\nlack of annotated imagery, variability in image quality and environmental\nconditions, and the cost of building robust machine learning pipelines over\nmassive remote sensing archives. We present a semi-automated approach for\nsurfacing possible whale detections in VHR imagery using a statistical anomaly\ndetection method that flags spatial outliers, i.e. \"interesting points\". We\npair this detector with a web-based labeling interface designed to enable\nexperts to quickly annotate the interesting points. We evaluate our system on\nthree benchmark scenes with known whale annotations and achieve recalls of\n90.3% to 96.4%, while reducing the area requiring expert inspection by up to\n99.8% -- from over 1,000 sq km to less than 2 sq km in some cases. Our method\ndoes not rely on labeled training data and offers a scalable first step toward\nfuture machine-assisted marine mammal monitoring from space. We have open\nsourced this pipeline at https://github.com/microsoft/whales.",
            "headline_zh": "提出半自动化异常检测方法以识别高分辨率卫星图像中的鲸鱼",
            "intro_zh": [
                "核心问题：传统鲸鱼监测方法成本高、难扩展，自动化检测面临标注数据不足和环境变化挑战。",
                "方法要点：使用统计异常检测标记空间异常点，结合Web标注界面供专家快速验证。",
                "实验或效果：在三个基准场景中召回率达90.3%-96.4%，专家检查面积减少高达99.8%。"
            ],
            "tags_zh": [
                "鲸鱼检测",
                "高分辨率卫星图像",
                "异常检测",
                "半自动化方法",
                "海洋哺乳动物监测",
                "开源工具"
            ],
            "_index": 59
        },
        {
            "title": "Leveraging Learned Image Prior for 3D Gaussian Compression",
            "authors": [
                "Seungjoo Shin",
                "Jaesik Park",
                "Sunghyun Cho"
            ],
            "arxiv_id": "2510.14705v1",
            "summary": "Compression techniques for 3D Gaussian Splatting (3DGS) have recently\nachieved considerable success in minimizing storage overhead for 3D Gaussians\nwhile preserving high rendering quality. Despite the impressive storage\nreduction, the lack of learned priors restricts further advances in the\nrate-distortion trade-off for 3DGS compression tasks. To address this, we\nintroduce a novel 3DGS compression framework that leverages the powerful\nrepresentational capacity of learned image priors to recover\ncompression-induced quality degradation. Built upon initially compressed\nGaussians, our restoration network effectively models the compression artifacts\nin the image space between degraded and original Gaussians. To enhance the\nrate-distortion performance, we provide coarse rendering residuals into the\nrestoration network as side information. By leveraging the supervision of\nrestored images, the compressed Gaussians are refined, resulting in a highly\ncompact representation with enhanced rendering performance. Our framework is\ndesigned to be compatible with existing Gaussian compression methods, making it\nbroadly applicable across different baselines. Extensive experiments validate\nthe effectiveness of our framework, demonstrating superior rate-distortion\nperformance and outperforming the rendering quality of state-of-the-art 3DGS\ncompression methods while requiring substantially less storage.",
            "headline_zh": "提出基于学习图像先验的3D高斯压缩框架，以提升率失真性能。",
            "intro_zh": [
                "核心问题：现有3D高斯压缩方法缺乏学习先验，限制率失真权衡的进一步优化。",
                "方法要点：利用学习图像先验和粗渲染残差，构建恢复网络修复压缩伪影。",
                "实验或效果：在多个基线方法上验证，实现更优率失真性能和渲染质量，存储需求显著降低。"
            ],
            "tags_zh": [
                "3D高斯压缩",
                "学习图像先验",
                "率失真优化",
                "压缩伪影恢复",
                "渲染质量提升"
            ],
            "_index": 60
        },
        {
            "title": "When Planners Meet Reality: How Learned, Reactive Traffic Agents Shift nuPlan Benchmarks",
            "authors": [
                "Steffen Hagedorn",
                "Luka Donkov",
                "Aron Distelzweig",
                "Alexandru P. Condurache"
            ],
            "arxiv_id": "2510.14677v1",
            "summary": "Planner evaluation in closed-loop simulation often uses rule-based traffic\nagents, whose simplistic and passive behavior can hide planner deficiencies and\nbias rankings. Widely used IDM agents simply follow a lead vehicle and cannot\nreact to vehicles in adjacent lanes, hindering tests of complex interaction\ncapabilities. We address this issue by integrating the state-of-the-art learned\ntraffic agent model SMART into nuPlan. Thus, we are the first to evaluate\nplanners under more realistic conditions and quantify how conclusions shift\nwhen narrowing the sim-to-real gap. Our analysis covers 14 recent planners and\nestablished baselines and shows that IDM-based simulation overestimates\nplanning performance: nearly all scores deteriorate. In contrast, many planners\ninteract better than previously assumed and even improve in multi-lane,\ninteraction-heavy scenarios like lane changes or turns. Methods trained in\nclosed-loop demonstrate the best and most stable driving performance. However,\nwhen reaching their limits in augmented edge-case scenarios, all learned\nplanners degrade abruptly, whereas rule-based planners maintain reasonable\nbasic behavior. Based on our results, we suggest SMART-reactive simulation as a\nnew standard closed-loop benchmark in nuPlan and release the SMART agents as a\ndrop-in alternative to IDM at https://github.com/shgd95/InteractiveClosedLoop.",
            "headline_zh": "集成SMART智能体以解决nuPlan基准中规则交通代理的局限性",
            "intro_zh": [
                "规则交通代理如IDM行为简单，隐藏规划器缺陷并偏置评估。",
                "集成学习型交通代理SMART，模拟更真实交互，评估14个规划器。",
                "SMART模拟显示规划器性能普遍下降，但在复杂场景中交互能力提升。"
            ],
            "tags_zh": [
                "交通代理模拟",
                "规划器评估",
                "nuPlan基准",
                "学习型代理",
                "闭环仿真"
            ],
            "_index": 61
        },
        {
            "title": "VTimeCoT: Thinking by Drawing for Video Temporal Grounding and Reasoning",
            "authors": [
                "Jinglei Zhang",
                "Yuanfan Guo",
                "Rolandos Alexandros Potamias",
                "Jiankang Deng",
                "Hang Xu",
                "Chao Ma"
            ],
            "arxiv_id": "2510.14672v1",
            "summary": "In recent years, video question answering based on multimodal large language\nmodels (MLLM) has garnered considerable attention, due to the benefits from the\nsubstantial advancements in LLMs. However, these models have a notable\ndeficiency in the domains of video temporal grounding and reasoning, posing\nchallenges to the development of effective real-world video understanding\nsystems. Inspired by how humans use video players to interact with the progress\nbar for video comprehension, we introduce VTimeCoT, a simple yet effective\ntraining-free framework, designed for high-performance video grounding and\nreasoning. The proposed framework incorporates two novel visual tools of the\nprogress bar: a plug-and-play progress bar integration tool and a\nhigh-efficiency highlighting tool. In addition, to address the limitations of\nconventional text-based chain-of-thought (CoT) approaches, we introduce a\nvisuotemporal CoT process that integrates cross-modality reasoning across both\nvideo and text. Our approach demonstrates significant performance improvements\non both Qwen2VL-7B and GPT4o baselines in tasks of video temporal grounding and\nreasoning-based question answering. Finally, we showcase that the proposed\nframework achieves a compositional and interpretable reasoning process. Project\npage: https://vtimecot.github.io",
            "headline_zh": "提出VTimeCoT框架以解决视频时序定位与推理问题",
            "intro_zh": [
                "多模态大语言模型在视频时序定位与推理方面存在不足",
                "引入进度条工具和视觉时序链式思维过程，无需训练",
                "在Qwen2VL-7B和GPT4o基准上性能显著提升"
            ],
            "tags_zh": [
                "视频时序定位",
                "多模态推理",
                "链式思维",
                "进度条工具",
                "视频问答"
            ],
            "_index": 62
        },
        {
            "title": "WeCKD: Weakly-supervised Chained Distillation Network for Efficient Multimodal Medical Imaging",
            "authors": [
                "Md. Abdur Rahman",
                "Mohaimenul Azam Khan Raiaan",
                "Sami Azam",
                "Asif Karim",
                "Jemima Beissbarth",
                "Amanda Leach"
            ],
            "arxiv_id": "2510.14668v1",
            "summary": "Knowledge distillation (KD) has traditionally relied on a static\nteacher-student framework, where a large, well-trained teacher transfers\nknowledge to a single student model. However, these approaches often suffer\nfrom knowledge degradation, inefficient supervision, and reliance on either a\nvery strong teacher model or large labeled datasets, which limits their\neffectiveness in real-world, limited-data scenarios. To address these, we\npresent the first-ever Weakly-supervised Chain-based KD network (WeCKD) that\nredefines knowledge transfer through a structured sequence of interconnected\nmodels. Unlike conventional KD, it forms a progressive distillation chain,\nwhere each model not only learns from its predecessor but also refines the\nknowledge before passing it forward. This structured knowledge transfer further\nenhances feature learning, reduces data dependency, and mitigates the\nlimitations of one-step KD. Each model in the distillation chain is trained on\nonly a fraction of the dataset and demonstrates that effective learning can be\nachieved with minimal supervision. Extensive evaluations across four otoscopic\nimaging datasets demonstrate that it not only matches but in many cases\nsurpasses the performance of existing supervised methods. Experimental results\non two other datasets further underscore its generalization across diverse\nmedical imaging modalities, including microscopic and magnetic resonance\nimaging. Furthermore, our evaluations resulted in cumulative accuracy gains of\nup to +23% over a single backbone trained on the same limited data, which\nhighlights its potential for real-world adoption.",
            "headline_zh": "提出弱监督链式蒸馏网络以解决有限数据下医学图像分析问题",
            "intro_zh": [
                "传统知识蒸馏依赖强教师或大标注数据，易导致知识退化和监督低效",
                "采用链式结构，模型逐级学习和精炼知识，减少数据依赖并增强特征学习",
                "在多个医学图像数据集上验证，性能超越现有方法，准确率提升高达23%"
            ],
            "tags_zh": [
                "知识蒸馏",
                "弱监督学习",
                "医学图像分析",
                "链式结构",
                "多模态成像"
            ],
            "_index": 63
        },
        {
            "title": "EuroMineNet: A Multitemporal Sentinel-2 Benchmark for Spatiotemporal Mining Footprint Analysis in the European Union (2015-2024)",
            "authors": [
                "Weikang Yu",
                "Vincent Nwazelibe",
                "Xianping Ma",
                "Xiaokang Zhang",
                "Richard Gloaguen",
                "Xiao Xiang Zhu",
                "Pedram Ghamisi"
            ],
            "arxiv_id": "2510.14661v1",
            "summary": "Mining activities are essential for industrial and economic development, but\nremain a leading source of environmental degradation, contributing to\ndeforestation, soil erosion, and water contamination. Sustainable resource\nmanagement and environmental governance require consistent, long-term\nmonitoring of mining-induced land surface changes, yet existing datasets are\noften limited in temporal depth or geographic scope. To address this gap, we\npresent EuroMineNet, the first comprehensive multitemporal benchmark for mining\nfootprint mapping and monitoring based on Sentinel-2 multispectral imagery.\nSpanning 133 mining sites across the European Union, EuroMineNet provides\nannual observations and expert-verified annotations from 2015 to 2024, enabling\nGeoAI-based models to analyze environmental dynamics at a continental scale. It\nsupports two sustainability-driven tasks: (1) multitemporal mining footprint\nmapping for consistent annual land-use delineation, evaluated with a novel\nChange-Aware Temporal IoU (CA-TIoU) metric, and (2) cross-temporal change\ndetection to capture both gradual and abrupt surface transformations.\nBenchmarking 20 state-of-the-art deep learning models reveals that while GeoAI\nmethods effectively identify long-term environmental changes, challenges remain\nin detecting short-term dynamics critical for timely mitigation. By advancing\ntemporally consistent and explainable mining monitoring, EuroMineNet\ncontributes to sustainable land-use management, environmental resilience, and\nthe broader goal of applying GeoAI for social and environmental good. We\nrelease the codes and datasets by aligning with FAIR and the open science\nparadigm at https://github.com/EricYu97/EuroMineNet.",
            "headline_zh": "提出EuroMineNet基准以解决欧盟采矿足迹长期监测问题",
            "intro_zh": [
                "采矿活动导致环境退化，现有数据集时空覆盖不足",
                "基于Sentinel-2影像，提供133个矿点2015-2024年多时相标注",
                "评估20个GeoAI模型，揭示长期变化检测有效但短期动态挑战大"
            ],
            "tags_zh": [
                "多时相遥感",
                "采矿足迹监测",
                "GeoAI基准",
                "环境变化检测",
                "Sentinel-2影像",
                "可持续发展"
            ],
            "_index": 64
        },
        {
            "title": "Decorrelation Speeds Up Vision Transformers",
            "authors": [
                "Kieran Carrigg",
                "Rob van Gastel",
                "Melda Yeghaian",
                "Sander Dalm",
                "Faysal Boughorbel",
                "Marcel van Gerven"
            ],
            "arxiv_id": "2510.14657v1",
            "summary": "Masked Autoencoder (MAE) pre-training of vision transformers (ViTs) yields\nstrong performance in low-label regimes but comes with substantial\ncomputational costs, making it impractical in time- and resource-constrained\nindustrial settings. We address this by integrating Decorrelated\nBackpropagation (DBP) into MAE pre-training, an optimization method that\niteratively reduces input correlations at each layer to accelerate convergence.\nApplied selectively to the encoder, DBP achieves faster pre-training without\nloss of stability. On ImageNet-1K pre-training with ADE20K fine-tuning, DBP-MAE\nreduces wall-clock time to baseline performance by 21.1%, lowers carbon\nemissions by 21.4% and improves segmentation mIoU by 1.1 points. We observe\nsimilar gains when pre-training and fine-tuning on proprietary industrial data,\nconfirming the method's applicability in real-world scenarios. These results\ndemonstrate that DBP can reduce training time and energy use while improving\ndownstream performance for large-scale ViT pre-training.",
            "headline_zh": "提出去相关反向传播以加速视觉变换器预训练，降低计算成本",
            "intro_zh": [
                "MAE预训练ViT在低标签场景性能强，但计算成本高，工业应用受限",
                "集成DBP到MAE预训练，通过减少层间输入相关性加速收敛，仅用于编码器",
                "实验显示预训练时间减少21.1%，碳排放降低21.4%，分割mIoU提升1.1点"
            ],
            "tags_zh": [
                "视觉变换器",
                "去相关反向传播",
                "掩码自编码器",
                "预训练加速",
                "图像分割"
            ],
            "_index": 65
        },
        {
            "title": "Requirement Identification for Traffic Simulations in Driving Simulators",
            "authors": [
                "Sven Tarlowski",
                "Lutz Eckstein"
            ],
            "arxiv_id": "2510.14653v1",
            "summary": "This paper addresses the challenge of ensuring realistic traffic conditions\nby proposing a methodology that systematically identifies traffic simulation\nrequirements. Using a structured approach based on sub-goals in each study\nphase, specific technical needs are derived for microscopic levels, agent\nmodels, and visual representation. The methodology aims to maintain a high\ndegree of fidelity, enhancing both the validity of experimental outcomes and\nparticipant engagement. By providing a clear link between study objectives and\ntraffic simulation design, this approach supports robust automotive development\nand testing.",
            "headline_zh": "提出基于子目标的方法以识别驾驶模拟器中交通仿真的需求",
            "intro_zh": [
                "核心问题：确保驾驶模拟器中交通条件的真实性，以提升实验有效性。",
                "方法要点：采用结构化方法，从研究阶段子目标推导微观层面、代理模型和视觉表示的需求。",
                "实验或效果：增强仿真保真度，支持稳健的汽车开发和测试。"
            ],
            "tags_zh": [
                "交通仿真",
                "驾驶模拟器",
                "需求识别",
                "微观仿真",
                "代理模型",
                "视觉表示"
            ],
            "_index": 66
        },
        {
            "title": "In-Context Learning with Unpaired Clips for Instruction-based Video Editing",
            "authors": [
                "Xinyao Liao",
                "Xianfang Zeng",
                "Ziye Song",
                "Zhoujie Fu",
                "Gang Yu",
                "Guosheng Lin"
            ],
            "arxiv_id": "2510.14648v1",
            "summary": "Despite the rapid progress of instruction-based image editing, its extension\nto video remains underexplored, primarily due to the prohibitive cost and\ncomplexity of constructing large-scale paired video editing datasets. To\naddress this challenge, we introduce a low-cost pretraining strategy for\ninstruction-based video editing that leverages in-context learning from\nunpaired video clips. We show that pretraining a foundation video generation\nmodel with this strategy endows it with general editing capabilities, such as\nadding, replacing, or deleting operations, according to input editing\ninstructions. The pretrained model can then be efficiently refined with a small\namount of high-quality paired editing data. Built upon HunyuanVideoT2V, our\nframework first pretrains on approximately 1M real video clips to learn basic\nediting concepts, and subsequently fine-tunes on fewer than 150k curated\nediting pairs to extend more editing tasks and improve the editing quality.\nComparative experiments show that our method surpasses existing\ninstruction-based video editing approaches in both instruction alignment and\nvisual fidelity, achieving a 12\\% improvement in editing instruction following\nand a 15\\% improvement in editing quality.",
            "headline_zh": "提出基于上下文学习的无配对视频剪辑预训练策略，以解决指令视频编辑的数据稀缺问题。",
            "intro_zh": [
                "核心问题：指令视频编辑因大规模配对数据成本高而发展受限。",
                "方法要点：利用无配对视频剪辑预训练，赋予模型通用编辑能力，再少量精调。",
                "实验效果：在指令对齐和视觉保真度上优于现有方法，提升12%指令遵循和15%编辑质量。"
            ],
            "tags_zh": [
                "指令视频编辑",
                "上下文学习",
                "无配对数据预训练",
                "视频生成模型",
                "编辑指令对齐"
            ],
            "_index": 67
        },
        {
            "title": "Spatially anchored Tactile Awareness for Robust Dexterous Manipulation",
            "authors": [
                "Jialei Huang",
                "Yang Ye",
                "Yuanqing Gong",
                "Xuezhou Zhu",
                "Yang Gao",
                "Kaifeng Zhang"
            ],
            "arxiv_id": "2510.14647v1",
            "summary": "Dexterous manipulation requires precise geometric reasoning, yet existing\nvisuo-tactile learning methods struggle with sub-millimeter precision tasks\nthat are routine for traditional model-based approaches. We identify a key\nlimitation: while tactile sensors provide rich contact information, current\nlearning frameworks fail to effectively leverage both the perceptual richness\nof tactile signals and their spatial relationship with hand kinematics. We\nbelieve an ideal tactile representation should explicitly ground contact\nmeasurements in a stable reference frame while preserving detailed sensory\ninformation, enabling policies to not only detect contact occurrence but also\nprecisely infer object geometry in the hand's coordinate system. We introduce\nSaTA (Spatially-anchored Tactile Awareness for dexterous manipulation), an\nend-to-end policy framework that explicitly anchors tactile features to the\nhand's kinematic frame through forward kinematics, enabling accurate geometric\nreasoning without requiring object models or explicit pose estimation. Our key\ninsight is that spatially grounded tactile representations allow policies to\nnot only detect contact occurrence but also precisely infer object geometry in\nthe hand's coordinate system. We validate SaTA on challenging dexterous\nmanipulation tasks, including bimanual USB-C mating in free space, a task\ndemanding sub-millimeter alignment precision, as well as light bulb\ninstallation requiring precise thread engagement and rotational control, and\ncard sliding that demands delicate force modulation and angular precision.\nThese tasks represent significant challenges for learning-based methods due to\ntheir stringent precision requirements. Across multiple benchmarks, SaTA\nsignificantly outperforms strong visuo-tactile baselines, improving success\nrates by up to 30 percentage while reducing task completion times by 27\npercentage.",
            "headline_zh": "提出SaTA框架，通过空间锚定触觉特征解决灵巧操作中的亚毫米精度任务",
            "intro_zh": [
                "核心问题：现有视觉触觉学习方法难以处理亚毫米精度任务，缺乏触觉信号与手部运动的空间关联",
                "方法要点：SaTA通过前向运动学将触觉特征锚定到手部坐标系，实现无模型几何推理",
                "实验效果：在USB-C插接等任务中，成功率提升30%，完成时间减少27%"
            ],
            "tags_zh": [
                "灵巧操作",
                "触觉感知",
                "空间锚定",
                "几何推理",
                "强化学习",
                "亚毫米精度"
            ],
            "_index": 68
        },
        {
            "title": "Generative Models From and For Sampling-Based MPC: A Bootstrapped Approach For Adaptive Contact-Rich Manipulation",
            "authors": [
                "Lara Brudermüller",
                "Brandon Hung",
                "Xinghao Zhu",
                "Jiuguang Wang",
                "Nick Hawes",
                "Preston Culbertson",
                "Simon Le Cleac'h"
            ],
            "arxiv_id": "2510.14643v1",
            "summary": "We present a generative predictive control (GPC) framework that amortizes\nsampling-based Model Predictive Control (SPC) by bootstrapping it with\nconditional flow-matching models trained on SPC control sequences collected in\nsimulation. Unlike prior work relying on iterative refinement or gradient-based\nsolvers, we show that meaningful proposal distributions can be learned directly\nfrom noisy SPC data, enabling more efficient and informed sampling during\nonline planning. We further demonstrate, for the first time, the application of\nthis approach to real-world contact-rich loco-manipulation with a quadruped\nrobot. Extensive experiments in simulation and on hardware show that our method\nimproves sample efficiency, reduces planning horizon requirements, and\ngeneralizes robustly across task variations.",
            "headline_zh": "提出生成预测控制框架，通过引导采样优化接触丰富的机器人操作",
            "intro_zh": [
                "核心问题：采样模型预测控制在接触丰富操作中效率低、规划需求高",
                "方法要点：使用条件流匹配模型从模拟数据学习提议分布，提升在线采样效率",
                "实验或效果：在仿真和硬件中验证，提高样本效率、减少规划需求，并泛化任务变化"
            ],
            "tags_zh": [
                "生成预测控制",
                "采样模型预测控制",
                "条件流匹配",
                "机器人操作",
                "接触丰富任务",
                "样本效率优化"
            ],
            "_index": 69
        },
        {
            "title": "SteeringTTA: Guiding Diffusion Trajectories for Robust Test-Time-Adaptation",
            "authors": [
                "Jihyun Yu",
                "Yoojin Oh",
                "Wonho Bae",
                "Mingyu Kim",
                "Junhyug Noh"
            ],
            "arxiv_id": "2510.14634v1",
            "summary": "Test-time adaptation (TTA) aims to correct performance degradation of deep\nmodels under distribution shifts by updating models or inputs using unlabeled\ntest data. Input-only diffusion-based TTA methods improve robustness for\nclassification to corruptions but rely on gradient guidance, limiting\nexploration and generalization across distortion types. We propose SteeringTTA,\nan inference-only framework that adapts Feynman-Kac steering to guide\ndiffusion-based input adaptation for classification with rewards driven by\npseudo-label. SteeringTTA maintains multiple particle trajectories, steered by\na combination of cumulative top-K probabilities and an entropy schedule, to\nbalance exploration and confidence. On ImageNet-C, SteeringTTA consistently\noutperforms the baseline without any model updates or source data.",
            "headline_zh": "提出SteeringTTA以通过引导扩散轨迹提升测试时适应在分布偏移下的鲁棒性",
            "intro_zh": [
                "核心问题：基于梯度的扩散TTA方法探索受限，影响跨失真类型的泛化能力。",
                "方法要点：采用Feynman-Kac引导，结合伪标签奖励和多粒子轨迹平衡探索与置信度。",
                "实验或效果：在ImageNet-C上无需模型更新或源数据，性能优于基线。"
            ],
            "tags_zh": [
                "测试时适应",
                "扩散模型",
                "分布偏移",
                "伪标签",
                "Feynman-Kac引导",
                "图像分类"
            ],
            "_index": 70
        },
        {
            "title": "Adapting Self-Supervised Representations as a Latent Space for Efficient Generation",
            "authors": [
                "Ming Gui",
                "Johannes Schusterbauer",
                "Timy Phan",
                "Felix Krause",
                "Josh Susskind",
                "Miguel Angel Bautista",
                "Björn Ommer"
            ],
            "arxiv_id": "2510.14630v1",
            "summary": "We introduce Representation Tokenizer (RepTok), a generative modeling\nframework that represents an image using a single continuous latent token\nobtained from self-supervised vision transformers. Building on a pre-trained\nSSL encoder, we fine-tune only the semantic token embedding and pair it with a\ngenerative decoder trained jointly using a standard flow matching objective.\nThis adaptation enriches the token with low-level, reconstruction-relevant\ndetails, enabling faithful image reconstruction. To preserve the favorable\ngeometry of the original SSL space, we add a cosine-similarity loss that\nregularizes the adapted token, ensuring the latent space remains smooth and\nsuitable for generation. Our single-token formulation resolves spatial\nredundancies of 2D latent spaces and significantly reduces training costs.\nDespite its simplicity and efficiency, RepTok achieves competitive results on\nclass-conditional ImageNet generation and naturally extends to text-to-image\nsynthesis, reaching competitive zero-shot performance on MS-COCO under\nextremely limited training budgets. Our findings highlight the potential of\nfine-tuned SSL representations as compact and effective latent spaces for\nefficient generative modeling.",
            "headline_zh": "提出RepTok框架，利用自监督表示作为紧凑潜空间，实现高效图像生成。",
            "intro_zh": [
                "核心问题：传统2D潜空间存在空间冗余，训练成本高，影响生成效率。",
                "方法要点：基于预训练SSL编码器，微调语义令牌嵌入，结合流匹配目标训练解码器。",
                "实验效果：在ImageNet类条件生成和MS-COCO文本到图像合成中达到竞争性结果。"
            ],
            "tags_zh": [
                "自监督学习",
                "图像生成",
                "潜空间优化",
                "流匹配",
                "令牌化表示"
            ],
            "_index": 71
        },
        {
            "title": "GOPLA: Generalizable Object Placement Learning via Synthetic Augmentation of Human Arrangement",
            "authors": [
                "Yao Zhong",
                "Hanzhi Chen",
                "Simon Schaefer",
                "Anran Zhang",
                "Stefan Leutenegger"
            ],
            "arxiv_id": "2510.14627v1",
            "summary": "Robots are expected to serve as intelligent assistants, helping humans with\neveryday household organization. A central challenge in this setting is the\ntask of object placement, which requires reasoning about both semantic\npreferences (e.g., common-sense object relations) and geometric feasibility\n(e.g., collision avoidance). We present GOPLA, a hierarchical framework that\nlearns generalizable object placement from augmented human demonstrations. A\nmulti-modal large language model translates human instructions and visual\ninputs into structured plans that specify pairwise object relationships. These\nplans are then converted into 3D affordance maps with geometric common sense by\na spatial mapper, while a diffusion-based planner generates placement poses\nguided by test-time costs, considering multi-plan distributions and collision\navoidance. To overcome data scarcity, we introduce a scalable pipeline that\nexpands human placement demonstrations into diverse synthetic training data.\nExtensive experiments show that our approach improves placement success rates\nby 30.04 percentage points over the runner-up, evaluated on positioning\naccuracy and physical plausibility, demonstrating strong generalization across\na wide range of real-world robotic placement scenarios.",
            "headline_zh": "提出GOPLA框架，通过合成增强人类演示学习通用物体放置，以解决机器人家庭组织任务。",
            "intro_zh": [
                "核心问题：机器人物体放置需兼顾语义偏好和几何可行性，面临数据稀缺挑战。",
                "方法要点：使用多模态大语言模型生成结构化计划，结合扩散规划器生成放置位姿。",
                "实验效果：在真实场景中，放置成功率比次优方法提高30.04个百分点，泛化性强。"
            ],
            "tags_zh": [
                "物体放置学习",
                "合成数据增强",
                "多模态大语言模型",
                "扩散规划器",
                "机器人辅助",
                "几何可行性"
            ],
            "_index": 72
        },
        {
            "title": "Efficient Video Sampling: Pruning Temporally Redundant Tokens for Faster VLM Inference",
            "authors": [
                "Natan Bagrov",
                "Eugene Khvedchenia",
                "Borys Tymchenko",
                "Shay Aharon",
                "Lior Kadoch",
                "Tomer Keren",
                "Ofri Masad",
                "Yonatan Geifman",
                "Ran Zilberstein",
                "Tuomas Rintamaki",
                "Matthieu Le",
                "Andrew Tao"
            ],
            "arxiv_id": "2510.14624v1",
            "summary": "Vision-language models (VLMs) have recently expanded from static image\nunderstanding to video reasoning, but their scalability is fundamentally\nlimited by the quadratic cost of processing dense frame sequences. Long videos\noften exceed the token budget of modern language models, leading to severe\ncontext limitations and latency issues. We introduce Efficient Video Sampling\n(EVS), a simple, plug-and-play method for reducing token redundancy in videos\nby identifying and pruning temporally static patches -- spatial regions that\nremain unchanged across consecutive frames. EVS preserves positional identity,\nrequires no architectural changes or retraining. We show that EVS substantially\nreduces token count while maintaining semantic fidelity, enabling faster\ninference and longer input sequences. Applied at inference time, EVS reduces\nlarge language model (LLM) time-to-first-token (TTFT) by up to 4x with minimal\naccuracy loss. When combined with an uptraining phase using stochastic pruning\nrates, EVS yields models that are robust to varying compression levels and\nretain full performance under aggressive pruning. Extensive experiments\ndemonstrate that EVS consistently improves efficiency-accuracy trade-offs,\nunlocking scalable video-language understanding without sacrificing quality.",
            "headline_zh": "提出高效视频采样方法以解决视频语言模型推理中的冗余令牌问题",
            "intro_zh": [
                "核心问题：视频语言模型处理长视频时令牌数量超出预算，导致上下文限制和延迟问题。",
                "方法要点：通过识别并剪枝时间上静态的补丁来减少令牌冗余，无需架构修改或重训练。",
                "实验或效果：应用时可将大型语言模型首令牌时间减少高达4倍，同时保持语义保真度。"
            ],
            "tags_zh": [
                "视频语言模型",
                "令牌剪枝",
                "推理加速",
                "长视频处理",
                "高效采样"
            ],
            "_index": 73
        },
        {
            "title": "Shot2Tactic-Caption: Multi-Scale Captioning of Badminton Videos for Tactical Understanding",
            "authors": [
                "Ning Ding",
                "Keisuke Fujii",
                "Toru Tamaki"
            ],
            "arxiv_id": "2510.14617v1",
            "summary": "Tactical understanding in badminton involves interpreting not only individual\nactions but also how tactics are dynamically executed over time. In this paper,\nwe propose \\textbf{Shot2Tactic-Caption}, a novel framework for semantic and\ntemporal multi-scale video captioning in badminton, capable of generating\nshot-level captions that describe individual actions and tactic-level captions\nthat capture how these actions unfold over time within a tactical execution. We\nalso introduce the Shot2Tactic-Caption Dataset, the first badminton captioning\ndataset containing 5,494 shot captions and 544 tactic captions.\nShot2Tactic-Caption adopts a dual-branch design, with both branches including a\nvisual encoder, a spatio-temporal Transformer encoder, and a Transformer-based\ndecoder to generate shot and tactic captions. To support tactic captioning, we\nadditionally introduce a Tactic Unit Detector that identifies valid tactic\nunits, tactic types, and tactic states (e.g., Interrupt, Resume). For tactic\ncaptioning, we further incorporate a shot-wise prompt-guided mechanism, where\nthe predicted tactic type and state are embedded as prompts and injected into\nthe decoder via cross-attention. The shot-wise prompt-guided mechanism enables\nour system not only to describe successfully executed tactics but also to\ncapture tactical executions that are temporarily interrupted and later resumed.\nExperimental results demonstrate the effectiveness of our framework in\ngenerating both shot and tactic captions. Ablation studies show that the\nResNet50-based spatio-temporal encoder outperforms other variants, and that\nshot-wise prompt structuring leads to more coherent and accurate tactic\ncaptioning.",
            "headline_zh": "提出Shot2Tactic-Caption框架，用于羽毛球视频的多尺度战术理解与描述",
            "intro_zh": [
                "核心问题：羽毛球战术理解需描述个体动作和动态战术执行过程",
                "方法要点：采用双分支设计，结合视觉编码器和Transformer，引入战术单元检测器与提示引导机制",
                "实验或效果：在自建数据集上验证有效性，ResNet50编码器和提示机制提升准确性"
            ],
            "tags_zh": [
                "视频描述",
                "多尺度理解",
                "羽毛球战术",
                "Transformer",
                "提示引导机制",
                "战术单元检测"
            ],
            "_index": 74
        },
        {
            "title": "Accelerated Multi-Modal Motion Planning Using Context-Conditioned Diffusion Models",
            "authors": [
                "Edward Sandra",
                "Lander Vanroye",
                "Dries Dirckx",
                "Ruben Cartuyvels",
                "Jan Swevers",
                "Wilm Decré"
            ],
            "arxiv_id": "2510.14615v1",
            "summary": "Classical methods in robot motion planning, such as sampling-based and\noptimization-based methods, often struggle with scalability towards\nhigher-dimensional state spaces and complex environments. Diffusion models,\nknown for their capability to learn complex, high-dimensional and multi-modal\ndata distributions, provide a promising alternative when applied to motion\nplanning problems and have already shown interesting results. However, most of\nthe current approaches train their model for a single environment, limiting\ntheir generalization to environments not seen during training. The techniques\nthat do train a model for multiple environments rely on a specific camera to\nprovide the model with the necessary environmental information and therefore\nalways require that sensor. To effectively adapt to diverse scenarios without\nthe need for retraining, this research proposes Context-Aware Motion Planning\nDiffusion (CAMPD). CAMPD leverages a classifier-free denoising probabilistic\ndiffusion model, conditioned on sensor-agnostic contextual information. An\nattention mechanism, integrated in the well-known U-Net architecture,\nconditions the model on an arbitrary number of contextual parameters. CAMPD is\nevaluated on a 7-DoF robot manipulator and benchmarked against state-of-the-art\napproaches on real-world tasks, showing its ability to generalize to unseen\nenvironments and generate high-quality, multi-modal trajectories, at a fraction\nof the time required by existing methods.",
            "headline_zh": "提出CAMPD方法，利用上下文条件扩散模型解决机器人运动规划在未知环境中的泛化问题。",
            "intro_zh": [
                "核心问题：传统运动规划方法在高维状态空间和复杂环境中扩展性差，且现有扩散模型方法难以泛化到未见环境。",
                "方法要点：使用分类器自由去噪扩散模型，通过注意力机制整合传感器无关上下文信息，实现多模态轨迹生成。",
                "实验或效果：在7自由度机械臂上评估，相比现有方法，泛化能力更强、轨迹质量高且计算时间大幅减少。"
            ],
            "tags_zh": [
                "机器人运动规划",
                "扩散模型",
                "上下文条件",
                "多模态轨迹生成",
                "泛化能力",
                "U-Net架构"
            ],
            "_index": 75
        },
        {
            "title": "Proprioceptive Image: An Image Representation of Proprioceptive Data from Quadruped Robots for Contact Estimation Learning",
            "authors": [
                "Gabriel Fischer Abati",
                "João Carlos Virgolino Soares",
                "Giulio Turrisi",
                "Victor Barasuol",
                "Claudio Semini"
            ],
            "arxiv_id": "2510.14612v1",
            "summary": "This paper presents a novel approach for representing proprioceptive\ntime-series data from quadruped robots as structured two-dimensional images,\nenabling the use of convolutional neural networks for learning\nlocomotion-related tasks. The proposed method encodes temporal dynamics from\nmultiple proprioceptive signals, such as joint positions, IMU readings, and\nfoot velocities, while preserving the robot's morphological structure in the\nspatial arrangement of the image. This transformation captures inter-signal\ncorrelations and gait-dependent patterns, providing a richer feature space than\ndirect time-series processing. We apply this concept in the problem of contact\nestimation, a key capability for stable and adaptive locomotion on diverse\nterrains. Experimental evaluations on both real-world datasets and simulated\nenvironments show that our image-based representation consistently enhances\nprediction accuracy and generalization over conventional sequence-based models,\nunderscoring the potential of cross-modal encoding strategies for robotic state\nlearning. Our method achieves superior performance on the contact dataset,\nimproving contact state accuracy from 87.7% to 94.5% over the recently proposed\nMI-HGNN method, using a 15 times shorter window size.",
            "headline_zh": "提出将四足机器人本体感觉数据编码为图像以提升接触估计性能",
            "intro_zh": [
                "核心问题：四足机器人在多变地形上的稳定运动需准确估计足部接触状态",
                "方法要点：将多源本体感觉时间序列数据转换为结构化二维图像，保留机器人形态和动态模式",
                "实验效果：在真实和模拟数据上，接触估计准确率从87.7%提升至94.5%，窗口尺寸缩短15倍"
            ],
            "tags_zh": [
                "四足机器人",
                "本体感觉数据",
                "图像表示",
                "卷积神经网络",
                "接触估计",
                "跨模态编码"
            ],
            "_index": 76
        },
        {
            "title": "Knowledge-based Visual Question Answer with Multimodal Processing, Retrieval and Filtering",
            "authors": [
                "Yuyang Hong",
                "Jiaqi Gu",
                "Qi Yang",
                "Lubin Fan",
                "Yue Wu",
                "Ying Wang",
                "Kun Ding",
                "Shiming Xiang",
                "Jieping Ye"
            ],
            "arxiv_id": "2510.14605v1",
            "summary": "Knowledge-based visual question answering (KB-VQA) requires visual language\nmodels (VLMs) to integrate visual understanding with external knowledge\nretrieval. Although retrieval-augmented generation (RAG) achieves significant\nadvances in this task by combining knowledge-base querying, it still struggles\nwith the quality of multimodal queries and the relevance of retrieved results.\nTo overcome these challenges, we propose a novel three-stage method, termed\nWiki-PRF, including Processing, Retrieval and Filtering stages. The processing\nstage dynamically invokes visual tools to extract precise multimodal\ninformation for retrieval. The retrieval stage integrates visual and text\nfeatures to achieve multimodal knowledge retrieval. The filtering stage\nperforms relevance filtering and concentration on retrieval results. To this\nend, we introduce a visual language model trained with answer accuracy and\nformat consistency as reward signals via a reinforcement learning manner. This\nenhances the model's reasoning, tool invocation for accurate queries, and\nfiltering of irrelevant content. Experiments on benchmark datasets (E-VQA and\nInfoSeek) show significant improvements~(36.0 and 42.8) in answer quality,\nachieving state-of-the-art performance. Code is available at\nhttps://github.com/cqu-student/Wiki-PRF",
            "headline_zh": "提出Wiki-PRF三阶段方法以解决知识型视觉问答中查询质量和检索结果相关性问题",
            "intro_zh": [
                "核心问题：知识型视觉问答中多模态查询质量差和检索结果相关性低",
                "方法要点：采用处理、检索和过滤三阶段，结合视觉工具和强化学习训练",
                "实验或效果：在E-VQA和InfoSeek数据集上答案质量显著提升，达到SOTA性能"
            ],
            "tags_zh": [
                "知识型视觉问答",
                "多模态检索",
                "强化学习训练",
                "检索增强生成",
                "视觉语言模型"
            ],
            "_index": 77
        },
        {
            "title": "Zero-Shot Wildlife Sorting Using Vision Transformers: Evaluating Clustering and Continuous Similarity Ordering",
            "authors": [
                "Hugo Markoff",
                "Jevgenijs Galaktionovs"
            ],
            "arxiv_id": "2510.14596v1",
            "summary": "Camera traps generate millions of wildlife images, yet many datasets contain\nspecies that are absent from existing classifiers. This work evaluates\nzero-shot approaches for organizing unlabeled wildlife imagery using\nself-supervised vision transformers, developed and tested within the Animal\nDetect platform for camera trap analysis. We compare unsupervised clustering\nmethods (DBSCAN, GMM) across three architectures (CLIP, DINOv2, MegaDescriptor)\ncombined with dimensionality reduction techniques (PCA, UMAP), and we\ndemonstrate continuous 1D similarity ordering via t-SNE projection. On a\n5-species test set with ground truth labels used only for evaluation, DINOv2\nwith UMAP and GMM achieves 88.6 percent accuracy (macro-F1 = 0.874), while 1D\nsorting reaches 88.2 percent coherence for mammals and birds and 95.2 percent\nfor fish across 1,500 images. Based on these findings, we deployed continuous\nsimilarity ordering in production, enabling rapid exploratory analysis and\naccelerating manual annotation workflows for biodiversity monitoring.",
            "headline_zh": "评估零样本方法使用视觉变换器组织未标记野生动物图像，提升生物多样性监测效率。",
            "intro_zh": [
                "核心问题：相机陷阱图像中物种缺失分类器，需零样本方法组织未标记数据。",
                "方法要点：比较自监督视觉变换器架构与聚类方法，结合降维技术进行相似性排序。",
                "实验或效果：DINOv2结合UMAP和GMM达88.6%准确率，1D排序在鱼类图像达95.2%一致性。"
            ],
            "tags_zh": [
                "零样本学习",
                "视觉变换器",
                "无监督聚类",
                "降维技术",
                "生物多样性监测",
                "图像排序"
            ],
            "_index": 78
        },
        {
            "title": "Hierarchical Re-Classification: Combining Animal Classification Models with Vision Transformers",
            "authors": [
                "Hugo Markoff",
                "Jevgenijs Galaktionovs"
            ],
            "arxiv_id": "2510.14594v1",
            "summary": "State-of-the-art animal classification models like SpeciesNet provide\npredictions across thousands of species but use conservative rollup strategies,\nresulting in many animals labeled at high taxonomic levels rather than species.\nWe present a hierarchical re-classification system for the Animal Detect\nplatform that combines SpeciesNet EfficientNetV2-M predictions with CLIP\nembeddings and metric learning to refine high-level taxonomic labels toward\nspecies-level identification. Our five-stage pipeline (high-confidence\nacceptance, bird override, centroid building, triplet-loss metric learning, and\nadaptive cosine-distance scoring) is evaluated on a segment of the LILA BC\nDesert Lion Conservation dataset (4,018 images, 15,031 detections). After\nrecovering 761 bird detections from \"blank\" and \"animal\" labels, we re-classify\n456 detections labeled animal, mammal, or blank with 96.5% accuracy, achieving\nspecies-level identification for 64.9 percent",
            "headline_zh": "提出分层重分类系统以提升动物检测平台中物种级识别精度",
            "intro_zh": [
                "现有动物分类模型预测保守，常标记为高级分类而非物种级别",
                "结合SpeciesNet预测、CLIP嵌入和度量学习，构建五阶段重分类流程",
                "在LILA数据集上验证，重分类准确率96.5%，64.9%检测达物种级识别"
            ],
            "tags_zh": [
                "动物分类",
                "分层重分类",
                "Vision Transformer",
                "度量学习",
                "物种识别",
                "CLIP嵌入"
            ],
            "_index": 79
        },
        {
            "title": "STANCE: Motion Coherent Video Generation Via Sparse-to-Dense Anchored Encoding",
            "authors": [
                "Zhifei Chen",
                "Tianshuo Xu",
                "Leyi Wu",
                "Luozhou Wang",
                "Dongyu Yan",
                "Zihan You",
                "Wenting Luo",
                "Guo Zhang",
                "Yingcong Chen"
            ],
            "arxiv_id": "2510.14588v1",
            "summary": "Video generation has recently made striking visual progress, but maintaining\ncoherent object motion and interactions remains difficult. We trace two\npractical bottlenecks: (i) human-provided motion hints (e.g., small 2D maps)\noften collapse to too few effective tokens after encoding, weakening guidance;\nand (ii) optimizing for appearance and motion in a single head can favor\ntexture over temporal consistency. We present STANCE, an image-to-video\nframework that addresses both issues with two simple components. First, we\nintroduce Instance Cues -- a pixel-aligned control signal that turns sparse,\nuser-editable hints into a dense 2.5D (camera-relative) motion field by\naveraging per-instance flow and augmenting with monocular depth over the\ninstance mask. This reduces depth ambiguity compared to 2D arrow inputs while\nremaining easy to use. Second, we preserve the salience of these cues in token\nspace with Dense RoPE, which tags a small set of motion tokens (anchored on the\nfirst frame) with spatial-addressable rotary embeddings. Paired with joint RGB\n\\(+\\) auxiliary-map prediction (segmentation or depth), our model anchors\nstructure while RGB handles appearance, stabilizing optimization and improving\ntemporal coherence without requiring per-frame trajectory scripts.",
            "headline_zh": "提出STANCE框架，通过稀疏到稠密锚定编码解决视频生成中运动一致性问题",
            "intro_zh": [
                "核心问题：视频生成中对象运动不连贯，源于运动提示编码后有效令牌过少和外观-运动优化冲突",
                "方法要点：使用实例线索生成稠密2.5D运动场，并引入Dense RoPE保持运动令牌显著性",
                "实验或效果：结合RGB与辅助图预测，提升时间一致性，无需逐帧轨迹脚本"
            ],
            "tags_zh": [
                "视频生成",
                "运动一致性",
                "稀疏到稠密编码",
                "实例线索",
                "Dense RoPE",
                "时间相干性"
            ],
            "_index": 80
        },
        {
            "title": "A Generalized Placeability Metric for Model-Free Unified Pick-and-Place Reasoning",
            "authors": [
                "Benno Wingender",
                "Nils Dengler",
                "Rohit Menon",
                "Sicong Pan",
                "Maren Bennewitz"
            ],
            "arxiv_id": "2510.14584v1",
            "summary": "To reliably pick and place unknown objects under real-world sensing noise\nremains a challenging task, as existing methods rely on strong object priors\n(e.g., CAD models), or planar-support assumptions, limiting generalization and\nunified reasoning between grasping and placing. In this work, we introduce a\ngeneralized placeability metric that evaluates placement poses directly from\nnoisy point clouds, without any shape priors. The metric jointly scores\nstability, graspability, and clearance. From raw geometry, we extract the\nsupport surfaces of the object to generate diverse candidates for\nmulti-orientation placement and sample contacts that satisfy collision and\nstability constraints. By conditioning grasp scores on each candidate\nplacement, our proposed method enables model-free unified pick-and-place\nreasoning and selects grasp-place pairs that lead to stable, collision-free\nplacements. On unseen real objects and non-planar object supports, our metric\ndelivers CAD-comparable accuracy in predicting stability loss and generally\nproduces more physically plausible placements than learning-based predictors.",
            "headline_zh": "提出广义可放置性度量，用于无模型统一拾放推理，处理未知物体和噪声点云。",
            "intro_zh": [
                "核心问题：现有方法依赖物体先验或平面支撑假设，限制泛化和统一推理。",
                "方法要点：从噪声点云直接评估放置位姿，联合评分稳定性、可抓取性和间隙。",
                "实验或效果：在未知真实物体和非平面支撑上，预测稳定性损失精度与CAD相当。"
            ],
            "tags_zh": [
                "拾放推理",
                "可放置性度量",
                "点云处理",
                "稳定性预测",
                "无模型方法"
            ],
            "_index": 81
        },
        {
            "title": "Talking Points: Describing and Localizing Pixels",
            "authors": [
                "Matan Rusanovsky",
                "Shimon Malnick",
                "Shai Avidan"
            ],
            "arxiv_id": "2510.14583v1",
            "summary": "Vision-language models have achieved remarkable success in cross-modal\nunderstanding. Yet, these models remain limited to object-level or region-level\ngrounding, lacking the capability for pixel-precise keypoint comprehension\nthrough natural language. We introduce a novel framework for pixel level\ngrounding. The framework consists of two complementary components: a Point\nDescriptor that generates rich, contextual descriptions of individual\nkeypoints, and a Point Localizer that regresses precise pixel coordinates from\nthese descriptions. Unlike prior work that relies on templated prompts or\nkeypoint names, our approach produces free-form, coarse-to-fine descriptions\nthat situate keypoints within their visual context. Since there is no available\ndataset to train such a system, we introduce LlamaPointInPart, a carefully\ncurated dataset of 20K+ image-keypoint-description triplets synthesized from\nmultiple vision-language models, capturing multi-scale information from\nscene-level context to visual features around the keypoint. For cross-category\ngeneralization, we optimize the Point Descriptor on AP-10K via GRPO, using the\nfrozen Point Localizer as a reward model to produce descriptions that maximize\nlocalization accuracy. To evaluate our results we establish a new evaluation\nprotocol. Instead of comparing the text description produced by our method to\nthe ground truth, we use the localizer to determine how close is the predicted\npoint generated to the ground truth point. Experiments demonstrate superior\nperformance compared to baseline models on LlamaPointInPart.The bidirectional\nnature of our framework should enable future applications in both\nkeypoint-guided image understanding and language-guided precise localization.\nOur code and dataset are publicly available at\nhttps://github.com/matanr/Talking_Points.",
            "headline_zh": "提出双向框架以解决像素级关键点描述与定位问题",
            "intro_zh": [
                "核心问题：现有视觉语言模型缺乏像素级关键点理解能力",
                "方法要点：结合点描述器和点定位器，生成上下文描述并回归坐标",
                "实验或效果：在LlamaPointInPart数据集上优于基线模型"
            ],
            "tags_zh": [
                "像素级定位",
                "关键点描述",
                "视觉语言模型",
                "数据集构建",
                "双向框架"
            ],
            "_index": 82
        },
        {
            "title": "CALM-Net: Curvature-Aware LiDAR Point Cloud-based Multi-Branch Neural Network for Vehicle Re-Identification",
            "authors": [
                "Dongwook Lee",
                "Sol Han",
                "Jinwhan Kim"
            ],
            "arxiv_id": "2510.14576v1",
            "summary": "This paper presents CALM-Net, a curvature-aware LiDAR point cloud-based\nmulti-branch neural network for vehicle re-identification. The proposed model\naddresses the challenge of learning discriminative and complementary features\nfrom three-dimensional point clouds to distinguish between vehicles. CALM-Net\nemploys a multi-branch architecture that integrates edge convolution, point\nattention, and a curvature embedding that characterizes local surface variation\nin point clouds. By combining these mechanisms, the model learns richer\ngeometric and contextual features that are well suited for the\nre-identification task. Experimental evaluation on the large-scale nuScenes\ndataset demonstrates that CALM-Net achieves a mean re-identification accuracy\nimprovement of approximately 1.97\\% points compared with the strongest baseline\nin our study. The results confirms the effectiveness of incorporating curvature\ninformation into deep learning architectures and highlight the benefit of\nmulti-branch feature learning for LiDAR point cloud-based vehicle\nre-identification.",
            "headline_zh": "提出CALM-Net，通过曲率感知多分支网络解决LiDAR点云车辆重识别问题",
            "intro_zh": [
                "核心问题：从三维点云中学习区分性特征以识别不同车辆",
                "方法要点：集成边缘卷积、点注意力和曲率嵌入的多分支架构",
                "实验效果：在nuScenes数据集上平均重识别准确率提升约1.97%"
            ],
            "tags_zh": [
                "LiDAR点云",
                "车辆重识别",
                "曲率感知",
                "多分支网络",
                "边缘卷积",
                "点注意力"
            ],
            "_index": 83
        },
        {
            "title": "BalanceGS: Algorithm-System Co-design for Efficient 3D Gaussian Splatting Training on GPU",
            "authors": [
                "Junyi Wu",
                "Jiaming Xu",
                "Jinhao Li",
                "Yongkang Zhou",
                "Jiayi Pan",
                "Xingyang Li",
                "Guohao Dai"
            ],
            "arxiv_id": "2510.14564v1",
            "summary": "3D Gaussian Splatting (3DGS) has emerged as a promising 3D reconstruction\ntechnique. The traditional 3DGS training pipeline follows three sequential\nsteps: Gaussian densification, Gaussian projection, and color splatting.\nDespite its promising reconstruction quality, this conventional approach\nsuffers from three critical inefficiencies: (1) Skewed density allocation\nduring Gaussian densification, (2) Imbalanced computation workload during\nGaussian projection and (3) Fragmented memory access during color splatting.\n  To tackle the above challenges, we introduce BalanceGS, the algorithm-system\nco-design for efficient training in 3DGS. (1) At the algorithm level, we\npropose heuristic workload-sensitive Gaussian density control to automatically\nbalance point distributions - removing 80% redundant Gaussians in dense regions\nwhile filling gaps in sparse areas. (2) At the system level, we propose\nSimilarity-based Gaussian sampling and merging, which replaces the static\none-to-one thread-pixel mapping with adaptive workload distribution - threads\nnow dynamically process variable numbers of Gaussians based on local cluster\ndensity. (3) At the mapping level, we propose reordering-based memory access\nmapping strategy that restructures RGB storage and enables batch loading in\nshared memory.\n  Extensive experiments demonstrate that compared with 3DGS, our approach\nachieves a 1.44$\\times$ training speedup on a NVIDIA A100 GPU with negligible\nquality degradation.",
            "headline_zh": "提出BalanceGS以解决3D高斯溅射训练中的效率问题",
            "intro_zh": [
                "核心问题：传统3DGS训练存在密度分配不均、计算负载不平衡和内存访问碎片化",
                "方法要点：算法层优化密度控制，系统层动态分配线程，映射层重构内存访问",
                "实验效果：在A100 GPU上实现1.44倍训练加速，质量损失可忽略"
            ],
            "tags_zh": [
                "3D高斯溅射",
                "算法系统协同设计",
                "GPU训练优化",
                "内存访问优化",
                "计算负载平衡"
            ],
            "_index": 84
        },
        {
            "title": "Eyes Wide Open: Ego Proactive Video-LLM for Streaming Video",
            "authors": [
                "Yulin Zhang",
                "Cheng Shi",
                "Yang Wang",
                "Sibei Yang"
            ],
            "arxiv_id": "2510.14560v1",
            "summary": "Envision an AI capable of functioning in human-like settings, moving beyond\nmere observation to actively understand, anticipate, and proactively respond to\nunfolding events. Towards this vision, we focus on the innovative task where,\ngiven ego-streaming video input, an assistant proactively answers diverse,\nevolving questions at the opportune moment, while maintaining synchronized\nperception and reasoning. This task embodies three key properties: (1)\nProactive Coherence, (2) Just-in-Time Responsiveness, and (3) Synchronized\nEfficiency. To evaluate and address these properties, we first introduce\nESTP-Bench (Ego Streaming Proactive Benchmark) alongside the ESTP-F1 metric-a\nnovel framework designed for their rigorous assessment. Secondly, we propose a\ncomprehensive technical pipeline to enable models to tackle this challenging\ntask. This pipeline comprises: (1) a data engine, (2) a multi-stage training\nstrategy, and (3) a proactive dynamic compression technique. Our proposed model\neffectively addresses these critical properties while outperforming multiple\nbaselines across diverse online and offline benchmarks. Project\nPage:https://zhangyl4.github.io/publications/eyes-wide-open/",
            "headline_zh": "提出Ego Proactive Video-LLM以解决流视频中主动理解与响应问题",
            "intro_zh": [
                "核心问题：在流视频中主动理解、预测和响应动态事件，需满足主动一致性、及时响应和同步效率。",
                "方法要点：引入ESTP-Bench基准与ESTP-F1指标，并开发数据引擎、多阶段训练和主动动态压缩技术。",
                "实验或效果：模型在多个在线和离线基准上优于基线，有效处理关键属性。"
            ],
            "tags_zh": [
                "流视频理解",
                "主动AI",
                "多模态学习",
                "基准评估",
                "动态压缩",
                "同步推理"
            ],
            "_index": 85
        },
        {
            "title": "Consistent text-to-image generation via scene de-contextualization",
            "authors": [
                "Song Tang",
                "Peihao Gong",
                "Kunyu Li",
                "Kai Guo",
                "Boyu Wang",
                "Mao Ye",
                "Jianwei Zhang",
                "Xiatian Zhu"
            ],
            "arxiv_id": "2510.14553v1",
            "summary": "Consistent text-to-image (T2I) generation seeks to produce\nidentity-preserving images of the same subject across diverse scenes, yet it\noften fails due to a phenomenon called identity (ID) shift. Previous methods\nhave tackled this issue, but typically rely on the unrealistic assumption of\nknowing all target scenes in advance. This paper reveals that a key source of\nID shift is the native correlation between subject and scene context, called\nscene contextualization, which arises naturally as T2I models fit the training\ndistribution of vast natural images. We formally prove the near-universality of\nthis scene-ID correlation and derive theoretical bounds on its strength. On\nthis basis, we propose a novel, efficient, training-free prompt embedding\nediting approach, called Scene De-Contextualization (SDeC), that imposes an\ninversion process of T2I's built-in scene contextualization. Specifically, it\nidentifies and suppresses the latent scene-ID correlation within the ID\nprompt's embedding by quantifying the SVD directional stability to adaptively\nre-weight the corresponding eigenvalues. Critically, SDeC allows for per-scene\nuse (one scene per prompt) without requiring prior access to all target scenes.\nThis makes it a highly flexible and general solution well-suited to real-world\napplications where such prior knowledge is often unavailable or varies over\ntime. Experiments demonstrate that SDeC significantly enhances identity\npreservation while maintaining scene diversity.",
            "headline_zh": "提出场景去上下文化方法以解决文本到图像生成中的身份偏移问题",
            "intro_zh": [
                "核心问题：文本到图像生成中身份偏移源于主体与场景上下文的内在相关性",
                "方法要点：通过训练无关的提示嵌入编辑，抑制潜在场景-身份相关性",
                "实验或效果：显著提升身份保持，同时维持场景多样性"
            ],
            "tags_zh": [
                "文本到图像生成",
                "身份保持",
                "场景去上下文化",
                "提示嵌入编辑",
                "训练无关方法"
            ],
            "_index": 86
        },
        {
            "title": "QuASH: Using Natural-Language Heuristics to Query Visual-Language Robotic Maps",
            "authors": [
                "Matti Pekkanen",
                "Francesco Verdoja",
                "Ville Kyrki"
            ],
            "arxiv_id": "2510.14546v1",
            "summary": "Embeddings from Visual-Language Models are increasingly utilized to represent\nsemantics in robotic maps, offering an open-vocabulary scene understanding that\nsurpasses traditional, limited labels. Embeddings enable on-demand querying by\ncomparing embedded user text prompts to map embeddings via a similarity metric.\nThe key challenge in performing the task indicated in a query is that the robot\nmust determine the parts of the environment relevant to the query.\n  This paper proposes a solution to this challenge. We leverage\nnatural-language synonyms and antonyms associated with the query within the\nembedding space, applying heuristics to estimate the language space relevant to\nthe query, and use that to train a classifier to partition the environment into\nmatches and non-matches. We evaluate our method through extensive experiments,\nquerying both maps and standard image benchmarks. The results demonstrate\nincreased queryability of maps and images. Our querying technique is agnostic\nto the representation and encoder used, and requires limited training.",
            "headline_zh": "提出QuASH方法，利用自然语言启发式查询视觉语言机器人地图",
            "intro_zh": [
                "核心问题：机器人需确定查询相关环境部分，以执行任务。",
                "方法要点：利用查询同义词和反义词，训练分类器分区环境。",
                "实验效果：提升地图和图像查询能力，方法通用且训练需求低。"
            ],
            "tags_zh": [
                "视觉语言模型",
                "机器人地图",
                "自然语言查询",
                "嵌入空间",
                "分类器训练",
                "开放词汇理解"
            ],
            "_index": 87
        },
        {
            "title": "Exploring Cross-Modal Flows for Few-Shot Learning",
            "authors": [
                "Ziqi Jiang",
                "Yanghao Wang",
                "Long Chen"
            ],
            "arxiv_id": "2510.14543v1",
            "summary": "Aligning features from different modalities, is one of the most fundamental\nchallenges for cross-modal tasks. Although pre-trained vision-language models\ncan achieve a general alignment between image and text, they often require\nparameter-efficient fine-tuning (PEFT) for further adjustment. Today's PEFT\nmethods (e.g., prompt tuning, LoRA-based, or adapter-based) always selectively\nfine-tune a subset of parameters, which can slightly adjust either visual or\ntextual features, and avoid overfitting. In this paper, we are the first to\nhighlight that all existing PEFT methods perform one-step adjustment. It is\ninsufficient for complex (or difficult) datasets, where features of different\nmodalities are highly entangled. To this end, we propose the first\nmodel-agnostic multi-step adjustment approach by learning a cross-modal\nvelocity field: Flow Matching Alignment (FMA). Specifically, to ensure the\ncorrespondence between categories during training, we first utilize a fixed\ncoupling strategy. Then, we propose a noise augmentation strategy to alleviate\nthe data scarcity issue. Finally, we design an early-stopping solver, which\nterminates the transformation process earlier, improving both efficiency and\naccuracy. Compared with one-step PEFT methods, FMA has the multi-step\nrectification ability to achieve more precise and robust alignment. Extensive\nresults have demonstrated that FMA can consistently yield significant\nperformance gains across various benchmarks and backbones, particularly on\nchallenging datasets.",
            "headline_zh": "提出Flow Matching Alignment以解决复杂数据集中的跨模态对齐问题",
            "intro_zh": [
                "现有参数高效微调方法仅单步调整，难以处理高度纠缠的跨模态特征",
                "FMA通过多步调整学习跨模态速度场，并引入噪声增强和早停求解器",
                "实验显示FMA在多个基准和骨干网络上显著提升性能，尤其在挑战性数据集"
            ],
            "tags_zh": [
                "跨模态学习",
                "少样本学习",
                "参数高效微调",
                "Flow Matching",
                "特征对齐",
                "噪声增强"
            ],
            "_index": 88
        },
        {
            "title": "Exploring Image Representation with Decoupled Classical Visual Descriptors",
            "authors": [
                "Chenyuan Qu",
                "Hao Chen",
                "Jianbo Jiao"
            ],
            "arxiv_id": "2510.14536v1",
            "summary": "Exploring and understanding efficient image representations is a\nlong-standing challenge in computer vision. While deep learning has achieved\nremarkable progress across image understanding tasks, its internal\nrepresentations are often opaque, making it difficult to interpret how visual\ninformation is processed. In contrast, classical visual descriptors (e.g. edge,\ncolour, and intensity distribution) have long been fundamental to image\nanalysis and remain intuitively understandable to humans. Motivated by this\ngap, we ask a central question: Can modern learning benefit from these\nclassical cues? In this paper, we answer it with VisualSplit, a framework that\nexplicitly decomposes images into decoupled classical descriptors, treating\neach as an independent but complementary component of visual knowledge. Through\na reconstruction-driven pre-training scheme, VisualSplit learns to capture the\nessence of each visual descriptor while preserving their interpretability. By\nexplicitly decomposing visual attributes, our method inherently facilitates\neffective attribute control in various advanced visual tasks, including image\ngeneration and editing, extending beyond conventional classification and\nsegmentation, suggesting the effectiveness of this new learning approach for\nvisual understanding. Project page: https://chenyuanqu.com/VisualSplit/.",
            "headline_zh": "提出VisualSplit框架，通过解耦经典视觉描述符提升图像表示的可解释性和控制能力。",
            "intro_zh": [
                "核心问题：深度学习图像表示不透明，难以解释视觉信息处理过程。",
                "方法要点：将图像分解为独立经典描述符，采用重建预训练学习可解释表示。",
                "实验或效果：在图像生成和编辑等任务中实现有效属性控制，超越传统分类和分割。"
            ],
            "tags_zh": [
                "图像表示学习",
                "经典视觉描述符",
                "可解释AI",
                "图像生成",
                "属性控制"
            ],
            "_index": 89
        },
        {
            "title": "Acquisition of interpretable domain information during brain MR image harmonization for content-based image retrieval",
            "authors": [
                "Keima Abe",
                "Hayato Muraki",
                "Shuhei Tomoshige",
                "Kenichi Oishi",
                "Hitoshi Iyatomi"
            ],
            "arxiv_id": "2510.14535v1",
            "summary": "Medical images like MR scans often show domain shifts across imaging sites\ndue to scanner and protocol differences, which degrade machine learning\nperformance in tasks such as disease classification. Domain harmonization is\nthus a critical research focus. Recent approaches encode brain images\n$\\boldsymbol{x}$ into a low-dimensional latent space $\\boldsymbol{z}$, then\ndisentangle it into $\\boldsymbol{z_u}$ (domain-invariant) and\n$\\boldsymbol{z_d}$ (domain-specific), achieving strong results. However, these\nmethods often lack interpretability$-$an essential requirement in medical\napplications$-$leaving practical issues unresolved. We propose\nPseudo-Linear-Style Encoder Adversarial Domain Adaptation (PL-SE-ADA), a\ngeneral framework for domain harmonization and interpretable representation\nlearning that preserves disease-relevant information in brain MR images.\nPL-SE-ADA includes two encoders $f_E$ and $f_{SE}$ to extract\n$\\boldsymbol{z_u}$ and $\\boldsymbol{z_d}$, a decoder to reconstruct the image\n$f_D$, and a domain predictor $g_D$. Beyond adversarial training between the\nencoder and domain predictor, the model learns to reconstruct the input image\n$\\boldsymbol{x}$ by summing reconstructions from $\\boldsymbol{z_u}$ and\n$\\boldsymbol{z_d}$, ensuring both harmonization and informativeness. Compared\nto prior methods, PL-SE-ADA achieves equal or better performance in image\nreconstruction, disease classification, and domain recognition. It also enables\nvisualization of both domain-independent brain features and domain-specific\ncomponents, offering high interpretability across the entire framework.",
            "headline_zh": "提出PL-SE-ADA框架以解决脑MR图像领域偏移和可解释性问题",
            "intro_zh": [
                "脑MR图像因扫描器和协议差异存在领域偏移，影响机器学习性能",
                "使用双编码器分离领域不变和特定特征，通过对抗训练和图像重建实现领域协调",
                "在图像重建、疾病分类和领域识别中表现优异，并提供可视化可解释性"
            ],
            "tags_zh": [
                "脑MR图像",
                "领域协调",
                "可解释表示学习",
                "对抗训练",
                "图像重建",
                "疾病分类"
            ],
            "_index": 90
        },
        {
            "title": "Towards Generalist Intelligence in Dentistry: Vision Foundation Models for Oral and Maxillofacial Radiology",
            "authors": [
                "Xinrui Huang",
                "Fan Xiao",
                "Dongming He",
                "Anqi Gao",
                "Dandan Li",
                "Xiaofan Zhang",
                "Shaoting Zhang",
                "Xudong Wang"
            ],
            "arxiv_id": "2510.14532v1",
            "summary": "Oral and maxillofacial radiology plays a vital role in dental healthcare, but\nradiographic image interpretation is limited by a shortage of trained\nprofessionals. While AI approaches have shown promise, existing dental AI\nsystems are restricted by their single-modality focus, task-specific design,\nand reliance on costly labeled data, hindering their generalization across\ndiverse clinical scenarios. To address these challenges, we introduce DentVFM,\nthe first family of vision foundation models (VFMs) designed for dentistry.\nDentVFM generates task-agnostic visual representations for a wide range of\ndental applications and uses self-supervised learning on DentVista, a large\ncurated dental imaging dataset with approximately 1.6 million multi-modal\nradiographic images from various medical centers. DentVFM includes 2D and 3D\nvariants based on the Vision Transformer (ViT) architecture. To address gaps in\ndental intelligence assessment and benchmarks, we introduce DentBench, a\ncomprehensive benchmark covering eight dental subspecialties, more diseases,\nimaging modalities, and a wide geographical distribution. DentVFM shows\nimpressive generalist intelligence, demonstrating robust generalization to\ndiverse dental tasks, such as disease diagnosis, treatment analysis, biomarker\nidentification, and anatomical landmark detection and segmentation.\nExperimental results indicate DentVFM significantly outperforms supervised,\nself-supervised, and weakly supervised baselines, offering superior\ngeneralization, label efficiency, and scalability. Additionally, DentVFM\nenables cross-modality diagnostics, providing more reliable results than\nexperienced dentists in situations where conventional imaging is unavailable.\nDentVFM sets a new paradigm for dental AI, offering a scalable, adaptable, and\nlabel-efficient model to improve intelligent dental healthcare and address\ncritical gaps in global oral healthcare.",
            "headline_zh": "提出DentVFM视觉基础模型以解决牙科AI泛化不足问题",
            "intro_zh": [
                "牙科放射图像解读受限于专业医生短缺和现有AI系统泛化能力差",
                "基于自监督学习构建多模态牙科图像数据集，开发任务无关视觉表示模型",
                "在综合基准测试中显著优于基线，实现跨任务和跨模态可靠诊断"
            ],
            "tags_zh": [
                "视觉基础模型",
                "自监督学习",
                "牙科放射学",
                "多模态图像",
                "泛化能力",
                "基准测试"
            ],
            "_index": 91
        },
        {
            "title": "PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact Vision-Language Model",
            "authors": [
                "Cheng Cui",
                "Ting Sun",
                "Suyin Liang",
                "Tingquan Gao",
                "Zelun Zhang",
                "Jiaxuan Liu",
                "Xueqing Wang",
                "Changda Zhou",
                "Hongen Liu",
                "Manhui Lin",
                "Yue Zhang",
                "Yubo Zhang",
                "Handong Zheng",
                "Jing Zhang",
                "Jun Zhang",
                "Yi Liu",
                "Dianhai Yu",
                "Yanjun Ma"
            ],
            "arxiv_id": "2510.14528v1",
            "summary": "In this report, we propose PaddleOCR-VL, a SOTA and resource-efficient model\ntailored for document parsing. Its core component is PaddleOCR-VL-0.9B, a\ncompact yet powerful vision-language model (VLM) that integrates a NaViT-style\ndynamic resolution visual encoder with the ERNIE-4.5-0.3B language model to\nenable accurate element recognition. This innovative model efficiently supports\n109 languages and excels in recognizing complex elements (e.g., text, tables,\nformulas, and charts), while maintaining minimal resource consumption. Through\ncomprehensive evaluations on widely used public benchmarks and in-house\nbenchmarks, PaddleOCR-VL achieves SOTA performance in both page-level document\nparsing and element-level recognition. It significantly outperforms existing\nsolutions, exhibits strong competitiveness against top-tier VLMs, and delivers\nfast inference speeds. These strengths make it highly suitable for practical\ndeployment in real-world scenarios.",
            "headline_zh": "提出PaddleOCR-VL以提升多语言文档解析效率，采用超紧凑视觉语言模型。",
            "intro_zh": [
                "核心问题：多语言文档解析需高效识别文本、表格等元素，同时控制资源消耗。",
                "方法要点：集成NaViT视觉编码器与ERNIE语言模型，构建0.9B参数紧凑VLM。",
                "实验效果：在公共与内部基准测试中，性能超越现有方案，推理速度快。"
            ],
            "tags_zh": [
                "文档解析",
                "视觉语言模型",
                "多语言支持",
                "资源效率",
                "元素识别"
            ],
            "_index": 92
        },
        {
            "title": "Noise Projection: Closing the Prompt-Agnostic Gap Behind Text-to-Image Misalignment in Diffusion Models",
            "authors": [
                "Yunze Tong",
                "Didi Zhu",
                "Zijing Hu",
                "Jinluan Yang",
                "Ziyu Zhao"
            ],
            "arxiv_id": "2510.14526v1",
            "summary": "In text-to-image generation, different initial noises induce distinct\ndenoising paths with a pretrained Stable Diffusion (SD) model. While this\npattern could output diverse images, some of them may fail to align well with\nthe prompt. Existing methods alleviate this issue either by altering the\ndenoising dynamics or by drawing multiple noises and conducting post-selection.\nIn this paper, we attribute the misalignment to a training-inference mismatch:\nduring training, prompt-conditioned noises lie in a prompt-specific subset of\nthe latent space, whereas at inference the noise is drawn from a\nprompt-agnostic Gaussian prior. To close this gap, we propose a noise projector\nthat applies text-conditioned refinement to the initial noise before denoising.\nConditioned on the prompt embedding, it maps the noise to a prompt-aware\ncounterpart that better matches the distribution observed during SD training,\nwithout modifying the SD model. Our framework consists of these steps: we first\nsample some noises and obtain token-level feedback for their corresponding\nimages from a vision-language model (VLM), then distill these signals into a\nreward model, and finally optimize the noise projector via a quasi-direct\npreference optimization. Our design has two benefits: (i) it requires no\nreference images or handcrafted priors, and (ii) it incurs small inference\ncost, replacing multi-sample selection with a single forward pass. Extensive\nexperiments further show that our prompt-aware noise projection improves\ntext-image alignment across diverse prompts.",
            "headline_zh": "提出噪声投影器以解决扩散模型中文本-图像不对齐问题",
            "intro_zh": [
                "核心问题：训练与推理噪声分布不匹配导致文本-图像对齐差",
                "方法要点：使用噪声投影器将初始噪声映射为提示感知版本",
                "实验或效果：通过VLM反馈优化，提升多样提示下的对齐效果"
            ],
            "tags_zh": [
                "文本到图像生成",
                "扩散模型",
                "噪声投影",
                "文本-图像对齐",
                "偏好优化"
            ],
            "_index": 93
        },
        {
            "title": "Real-Time Surgical Instrument Defect Detection via Non-Destructive Testing",
            "authors": [
                "Qurrat Ul Ain",
                "Atif Aftab Ahmed Jilani",
                "Zunaira Shafqat",
                "Nigar Azhar Butt"
            ],
            "arxiv_id": "2510.14525v1",
            "summary": "Defective surgical instruments pose serious risks to sterility, mechanical\nintegrity, and patient safety, increasing the likelihood of surgical\ncomplications. However, quality control in surgical instrument manufacturing\noften relies on manual inspection, which is prone to human error and\ninconsistency. This study introduces SurgScan, an AI-powered defect detection\nframework for surgical instruments. Using YOLOv8, SurgScan classifies defects\nin real-time, ensuring high accuracy and industrial scalability. The model is\ntrained on a high-resolution dataset of 102,876 images, covering 11 instrument\ntypes and five major defect categories. Extensive evaluation against\nstate-of-the-art CNN architectures confirms that SurgScan achieves the highest\naccuracy (99.3%) with real-time inference speeds of 4.2-5.8 ms per image,\nmaking it suitable for industrial deployment. Statistical analysis demonstrates\nthat contrast-enhanced preprocessing significantly improves defect detection,\naddressing key limitations in visual inspection. SurgScan provides a scalable,\ncost-effective AI solution for automated quality control, reducing reliance on\nmanual inspection while ensuring compliance with ISO 13485 and FDA standards,\npaving the way for enhanced defect detection in medical manufacturing.",
            "headline_zh": "提出SurgScan框架，通过AI实时检测手术器械缺陷以提升质量控制。",
            "intro_zh": [
                "手术器械缺陷导致灭菌和机械完整性问题，手动检测易出错。",
                "基于YOLOv8构建AI框架，实现实时缺陷分类与高精度检测。",
                "在10万+图像数据集上验证，准确率达99.3%，推理速度快。"
            ],
            "tags_zh": [
                "手术器械缺陷检测",
                "实时目标检测",
                "YOLOv8模型",
                "质量控制自动化",
                "医疗制造标准"
            ],
            "_index": 94
        },
        {
            "title": "Vision Mamba for Permeability Prediction of Porous Media",
            "authors": [
                "Ali Kashefi",
                "Tapan Mukerji"
            ],
            "arxiv_id": "2510.14516v1",
            "summary": "Vision Mamba has recently received attention as an alternative to Vision\nTransformers (ViTs) for image classification. The network size of Vision Mamba\nscales linearly with input image resolution, whereas ViTs scale quadratically,\na feature that improves computational and memory efficiency. Moreover, Vision\nMamba requires a significantly smaller number of trainable parameters than\ntraditional convolutional neural networks (CNNs), and thus, they can be more\nmemory efficient. Because of these features, we introduce, for the first time,\na neural network that uses Vision Mamba as its backbone for predicting the\npermeability of three-dimensional porous media. We compare the performance of\nVision Mamba with ViT and CNN models across multiple aspects of permeability\nprediction and perform an ablation study to assess the effects of its\ncomponents on accuracy. We demonstrate in practice the aforementioned\nadvantages of Vision Mamba over ViTs and CNNs in the permeability prediction of\nthree-dimensional porous media. We make the source code publicly available to\nfacilitate reproducibility and to enable other researchers to build on and\nextend this work. We believe the proposed framework has the potential to be\nintegrated into large vision models in which Vision Mamba is used instead of\nViTs.",
            "headline_zh": "提出基于Vision Mamba的神经网络以预测三维多孔介质渗透率",
            "intro_zh": [
                "核心问题：预测三维多孔介质的渗透率，需高效处理高分辨率图像",
                "方法要点：使用Vision Mamba作为骨干网络，替代ViT和CNN，提升计算和内存效率",
                "实验或效果：比较Vision Mamba与ViT和CNN，验证其在渗透率预测中的优势"
            ],
            "tags_zh": [
                "渗透率预测",
                "Vision Mamba",
                "多孔介质",
                "图像分类",
                "计算效率",
                "神经网络"
            ],
            "_index": 95
        },
        {
            "title": "Stability Criteria and Motor Performance in Delayed Haptic Dyadic Interactions Mediated by Robots",
            "authors": [
                "Mingtian Du",
                "Suhas Raghavendra Kulkarni",
                "Simone Kager",
                "Domenico Campolo"
            ],
            "arxiv_id": "2510.14511v1",
            "summary": "This paper establishes analytical stability criteria for robot-mediated\nhuman-human (dyadic) interaction systems, focusing on haptic communication\nunder network-induced time delays. Through frequency-domain analysis supported\nby numerical simulations, we identify both delay-independent and\ndelay-dependent stability criteria. The delay-independent criterion guarantees\nstability irrespective of the delay, whereas the delay-dependent criterion is\ncharacterised by a maximum tolerable delay before instability occurs. The\ncriteria demonstrate dependence on controller and robot dynamic parameters,\nwhere increasing stiffness reduces the maximum tolerable delay in a non-linear\nmanner, thereby heightening system vulnerability. The proposed criteria can be\ngeneralised to a wide range of robot-mediated interactions and serve as design\nguidelines for stable remote dyadic systems. Experiments with robots performing\nhuman-like movements further illustrate the correlation between stability and\nmotor performance. The findings of this paper suggest the prerequisites for\neffective delay-compensation strategies.",
            "headline_zh": "提出机器人介导触觉交互的稳定性准则，分析网络延迟对系统稳定性的影响。",
            "intro_zh": [
                "核心问题：网络延迟下机器人介导的人-人触觉交互系统稳定性分析。",
                "方法要点：通过频域分析和数值模拟，建立延迟无关和延迟相关稳定性准则。",
                "实验或效果：机器人实验验证稳定性与运动性能的相关性，指导远程系统设计。"
            ],
            "tags_zh": [
                "触觉交互",
                "稳定性分析",
                "网络延迟",
                "机器人控制",
                "频域方法"
            ],
            "_index": 96
        },
        {
            "title": "Grazing Detection using Deep Learning and Sentinel-2 Time Series Data",
            "authors": [
                "Aleksis Pirinen",
                "Delia Fano Yela",
                "Smita Chakraborty",
                "Erik Källman"
            ],
            "arxiv_id": "2510.14493v1",
            "summary": "Grazing shapes both agricultural production and biodiversity, yet scalable\nmonitoring of where grazing occurs remains limited. We study seasonal grazing\ndetection from Sentinel-2 L2A time series: for each polygon-defined field\nboundary, April-October imagery is used for binary prediction (grazed / not\ngrazed). We train an ensemble of CNN-LSTM models on multi-temporal reflectance\nfeatures, and achieve an average F1 score of 77 percent across five validation\nsplits, with 90 percent recall on grazed pastures. Operationally, if inspectors\ncan visit at most 4 percent of sites annually, prioritising fields predicted by\nour model as non-grazed yields 17.2 times more confirmed non-grazing sites than\nrandom inspection. These results indicate that coarse-resolution, freely\navailable satellite data can reliably steer inspection resources for\nconservation-aligned land-use compliance. Code and models have been made\npublicly available.",
            "headline_zh": "提出基于CNN-LSTM集成模型的放牧检测方法，利用Sentinel-2时序数据优化资源分配。",
            "intro_zh": [
                "核心问题：放牧监测缺乏可扩展方法，影响农业和生物多样性管理。",
                "方法要点：使用Sentinel-2 L2A时序影像，训练CNN-LSTM集成模型进行二元分类。",
                "实验或效果：平均F1分数77%，召回率90%，资源分配效率提升17.2倍。"
            ],
            "tags_zh": [
                "放牧检测",
                "时序数据分析",
                "CNN-LSTM模型",
                "卫星遥感",
                "资源优化"
            ],
            "_index": 97
        },
        {
            "title": "Restoring Noisy Demonstration for Imitation Learning With Diffusion Models",
            "authors": [
                "Shang-Fu Chen",
                "Co Yong",
                "Shao-Hua Sun"
            ],
            "arxiv_id": "2510.14467v1",
            "summary": "Imitation learning (IL) aims to learn a policy from expert demonstrations and\nhas been applied to various applications. By learning from the expert policy,\nIL methods do not require environmental interactions or reward signals.\nHowever, most existing imitation learning algorithms assume perfect expert\ndemonstrations, but expert demonstrations often contain imperfections caused by\nerrors from human experts or sensor/control system inaccuracies. To address the\nabove problems, this work proposes a filter-and-restore framework to best\nleverage expert demonstrations with inherent noise. Our proposed method first\nfilters clean samples from the demonstrations and then learns conditional\ndiffusion models to recover the noisy ones. We evaluate our proposed framework\nand existing methods in various domains, including robot arm manipulation,\ndexterous manipulation, and locomotion. The experiment results show that our\nproposed framework consistently outperforms existing methods across all the\ntasks. Ablation studies further validate the effectiveness of each component\nand demonstrate the framework's robustness to different noise types and levels.\nThese results confirm the practical applicability of our framework to noisy\noffline demonstration data.",
            "headline_zh": "提出过滤-恢复框架以解决模仿学习中噪声专家演示问题",
            "intro_zh": [
                "核心问题：专家演示常含噪声，影响模仿学习性能",
                "方法要点：先过滤干净样本，再用条件扩散模型恢复噪声演示",
                "实验效果：在机器人操作和运动任务中优于现有方法，验证鲁棒性"
            ],
            "tags_zh": [
                "模仿学习",
                "噪声演示",
                "扩散模型",
                "机器人操作",
                "离线学习"
            ],
            "_index": 98
        },
        {
            "title": "Pruning Overparameterized Multi-Task Networks for Degraded Web Image Restoration",
            "authors": [
                "Thomas Katraouras",
                "Dimitrios Rafailidis"
            ],
            "arxiv_id": "2510.14463v1",
            "summary": "Image quality is a critical factor in delivering visually appealing content\non web platforms. However, images often suffer from degradation due to lossy\noperations applied by online social networks (OSNs), negatively affecting user\nexperience. Image restoration is the process of recovering a clean high-quality\nimage from a given degraded input. Recently, multi-task (all-in-one) image\nrestoration models have gained significant attention, due to their ability to\nsimultaneously handle different types of image degradations. However, these\nmodels often come with an excessively high number of trainable parameters,\nmaking them computationally inefficient. In this paper, we propose a strategy\nfor compressing multi-task image restoration models. We aim to discover highly\nsparse subnetworks within overparameterized deep models that can match or even\nsurpass the performance of their dense counterparts. The proposed model, namely\nMIR-L, utilizes an iterative pruning strategy that removes low-magnitude\nweights across multiple rounds, while resetting the remaining weights to their\noriginal initialization. This iterative process is important for the multi-task\nimage restoration model's optimization, effectively uncovering \"winning\ntickets\" that maintain or exceed state-of-the-art performance at high sparsity\nlevels. Experimental evaluation on benchmark datasets for the deraining,\ndehazing, and denoising tasks shows that MIR-L retains only 10% of the\ntrainable parameters while maintaining high image restoration performance. Our\ncode, datasets and pre-trained models are made publicly available at\nhttps://github.com/Thomkat/MIR-L.",
            "headline_zh": "提出MIR-L模型以压缩多任务图像恢复网络，提升计算效率。",
            "intro_zh": [
                "多任务图像恢复模型参数过多，导致计算效率低下。",
                "采用迭代剪枝策略，移除低幅值权重并重置剩余权重。",
                "实验显示，保留10%参数仍保持高性能，适用于去雨、去雾和去噪任务。"
            ],
            "tags_zh": [
                "图像恢复",
                "多任务学习",
                "模型剪枝",
                "计算效率",
                "迭代优化"
            ],
            "_index": 99
        },
        {
            "title": "Unsupervised Deep Generative Models for Anomaly Detection in Neuroimaging: A Systematic Scoping Review",
            "authors": [
                "Youwan Mahé",
                "Elise Bannier",
                "Stéphanie Leplaideur",
                "Elisa Fromont",
                "Francesca Galassi"
            ],
            "arxiv_id": "2510.14462v1",
            "summary": "Unsupervised deep generative models are emerging as a promising alternative\nto supervised methods for detecting and segmenting anomalies in brain imaging.\nUnlike fully supervised approaches, which require large voxel-level annotated\ndatasets and are limited to well-characterised pathologies, these models can be\ntrained exclusively on healthy data and identify anomalies as deviations from\nlearned normative brain structures. This PRISMA-guided scoping review\nsynthesises recent work on unsupervised deep generative models for anomaly\ndetection in neuroimaging, including autoencoders, variational autoencoders,\ngenerative adversarial networks, and denoising diffusion models. A total of 49\nstudies published between 2018 - 2025 were identified, covering applications to\nbrain MRI and, less frequently, CT across diverse pathologies such as tumours,\nstroke, multiple sclerosis, and small vessel disease. Reported performance\nmetrics are compared alongside architectural design choices. Across the\nincluded studies, generative models achieved encouraging performance for large\nfocal lesions and demonstrated progress in addressing more subtle\nabnormalities. A key strength of generative models is their ability to produce\ninterpretable pseudo-healthy (also referred to as counterfactual)\nreconstructions, which is particularly valuable when annotated data are scarce,\nas in rare or heterogeneous diseases. Looking ahead, these models offer a\ncompelling direction for anomaly detection, enabling semi-supervised learning,\nsupporting the discovery of novel imaging biomarkers, and facilitating within-\nand cross-disease deviation mapping in unified end-to-end frameworks. To\nrealise clinical impact, future work should prioritise anatomy-aware modelling,\ndevelopment of foundation models, task-appropriate evaluation metrics, and\nrigorous clinical validation.",
            "headline_zh": "综述无监督深度生成模型在神经影像异常检测中的应用与进展",
            "intro_zh": [
                "核心问题：监督方法依赖大量标注数据，难以处理罕见或异质性疾病。",
                "方法要点：使用健康数据训练生成模型，检测异常作为与规范结构的偏差。",
                "实验或效果：模型在大型局灶性病变中表现良好，并生成可解释的伪健康重建。"
            ],
            "tags_zh": [
                "无监督学习",
                "深度生成模型",
                "神经影像异常检测",
                "伪健康重建",
                "脑部MRI",
                "综述研究"
            ],
            "_index": 100
        },
        {
            "title": "Structured Universal Adversarial Attacks on Object Detection for Video Sequences",
            "authors": [
                "Sven Jacob",
                "Weijia Shao",
                "Gjergji Kasneci"
            ],
            "arxiv_id": "2510.14460v1",
            "summary": "Video-based object detection plays a vital role in safety-critical\napplications. While deep learning-based object detectors have achieved\nimpressive performance, they remain vulnerable to adversarial attacks,\nparticularly those involving universal perturbations. In this work, we propose\na minimally distorted universal adversarial attack tailored for video object\ndetection, which leverages nuclear norm regularization to promote structured\nperturbations concentrated in the background. To optimize this formulation\nefficiently, we employ an adaptive, optimistic exponentiated gradient method\nthat enhances both scalability and convergence. Our results demonstrate that\nthe proposed attack outperforms both low-rank projected gradient descent and\nFrank-Wolfe based attacks in effectiveness while maintaining high stealthiness.\nAll code and data are publicly available at\nhttps://github.com/jsve96/AO-Exp-Attack.",
            "headline_zh": "提出结构化通用对抗攻击，针对视频目标检测，优化扰动以提升隐蔽性。",
            "intro_zh": [
                "视频目标检测在安全关键应用中易受通用对抗攻击影响。",
                "采用核范数正则化生成背景集中的结构化扰动，并使用自适应指数梯度方法优化。",
                "实验显示攻击效果优于低秩投影梯度下降和Frank-Wolfe方法，保持高隐蔽性。"
            ],
            "tags_zh": [
                "视频目标检测",
                "通用对抗攻击",
                "核范数正则化",
                "指数梯度优化",
                "隐蔽性攻击"
            ],
            "_index": 101
        },
        {
            "title": "Towards Adaptable Humanoid Control via Adaptive Motion Tracking",
            "authors": [
                "Tao Huang",
                "Huayi Wang",
                "Junli Ren",
                "Kangning Yin",
                "Zirui Wang",
                "Xiao Chen",
                "Feiyu Jia",
                "Wentao Zhang",
                "Junfeng Long",
                "Jingbo Wang",
                "Jiangmiao Pang"
            ],
            "arxiv_id": "2510.14454v1",
            "summary": "Humanoid robots are envisioned to adapt demonstrated motions to diverse\nreal-world conditions while accurately preserving motion patterns. Existing\nmotion prior approaches enable well adaptability with a few motions but often\nsacrifice imitation accuracy, whereas motion-tracking methods achieve accurate\nimitation yet require many training motions and a test-time target motion to\nadapt. To combine their strengths, we introduce AdaMimic, a novel motion\ntracking algorithm that enables adaptable humanoid control from a single\nreference motion. To reduce data dependence while ensuring adaptability, our\nmethod first creates an augmented dataset by sparsifying the single reference\nmotion into keyframes and applying light editing with minimal physical\nassumptions. A policy is then initialized by tracking these sparse keyframes to\ngenerate dense intermediate motions, and adapters are subsequently trained to\nadjust tracking speed and refine low-level actions based on the adjustment,\nenabling flexible time warping that further improves imitation accuracy and\nadaptability. We validate these significant improvements in our approach in\nboth simulation and the real-world Unitree G1 humanoid robot in multiple tasks\nacross a wide range of adaptation conditions. Videos and code are available at\nhttps://taohuang13.github.io/adamimic.github.io/.",
            "headline_zh": "提出AdaMimic算法，实现从单一参考运动进行可适应人形机器人控制。",
            "intro_zh": [
                "核心问题：现有方法在运动适应性和模仿精度间存在权衡，难以从单一运动实现高精度适应。",
                "方法要点：通过稀疏化参考运动为关键帧，训练策略生成密集运动，并使用适配器调整速度和动作。",
                "实验或效果：在仿真和真实人形机器人上验证，在多种适应条件下提升模仿精度和适应性。"
            ],
            "tags_zh": [
                "人形机器人控制",
                "运动跟踪",
                "自适应算法",
                "关键帧稀疏化",
                "时间扭曲"
            ],
            "_index": 102
        },
        {
            "title": "Real-Time Neural Video Compression with Unified Intra and Inter Coding",
            "authors": [
                "Hui Xiang",
                "Yifan Bian",
                "Li Li",
                "Jingran Wu",
                "Xianguo Zhang",
                "Dong Liu"
            ],
            "arxiv_id": "2510.14431v1",
            "summary": "Neural video compression (NVC) technologies have advanced rapidly in recent\nyears, yielding state-of-the-art schemes such as DCVC-RT that offer superior\ncompression efficiency to H.266/VVC and real-time encoding/decoding\ncapabilities. Nonetheless, existing NVC schemes have several limitations,\nincluding inefficiency in dealing with disocclusion and new content, interframe\nerror propagation and accumulation, among others. To eliminate these\nlimitations, we borrow the idea from classic video coding schemes, which allow\nintra coding within inter-coded frames. With the intra coding tool enabled,\ndisocclusion and new content are properly handled, and interframe error\npropagation is naturally intercepted without the need for manual refresh\nmechanisms. We present an NVC framework with unified intra and inter coding,\nwhere every frame is processed by a single model that is trained to perform\nintra/inter coding adaptively. Moreover, we propose a simultaneous two-frame\ncompression design to exploit interframe redundancy not only forwardly but also\nbackwardly. Experimental results show that our scheme outperforms DCVC-RT by an\naverage of 10.7\\% BD-rate reduction, delivers more stable bitrate and quality\nper frame, and retains real-time encoding/decoding performances. Code and\nmodels will be released.",
            "headline_zh": "提出统一帧内帧间编码的神经视频压缩框架，以解决遮挡和误差传播问题。",
            "intro_zh": [
                "现有神经视频压缩存在遮挡处理低效和帧间误差传播问题。",
                "引入帧内编码工具，自适应处理帧内/帧间编码，无需手动刷新机制。",
                "实验显示BD-rate平均降低10.7%，保持实时性能，帧质量更稳定。"
            ],
            "tags_zh": [
                "神经视频压缩",
                "帧内帧间编码",
                "实时编码",
                "误差传播抑制",
                "遮挡处理"
            ],
            "_index": 103
        },
        {
            "title": "Deep Compositional Phase Diffusion for Long Motion Sequence Generation",
            "authors": [
                "Ho Yin Au",
                "Jie Chen",
                "Junkun Jiang",
                "Jingyu Xiang"
            ],
            "arxiv_id": "2510.14427v1",
            "summary": "Recent research on motion generation has shown significant progress in\ngenerating semantically aligned motion with singular semantics. However, when\nemploying these models to create composite sequences containing multiple\nsemantically generated motion clips, they often struggle to preserve the\ncontinuity of motion dynamics at the transition boundaries between clips,\nresulting in awkward transitions and abrupt artifacts. To address these\nchallenges, we present Compositional Phase Diffusion, which leverages the\nSemantic Phase Diffusion Module (SPDM) and Transitional Phase Diffusion Module\n(TPDM) to progressively incorporate semantic guidance and phase details from\nadjacent motion clips into the diffusion process. Specifically, SPDM and TPDM\noperate within the latent motion frequency domain established by the\npre-trained Action-Centric Motion Phase Autoencoder (ACT-PAE). This allows them\nto learn semantically important and transition-aware phase information from\nvariable-length motion clips during training. Experimental results demonstrate\nthe competitive performance of our proposed framework in generating\ncompositional motion sequences that align semantically with the input\nconditions, while preserving phase transitional continuity between preceding\nand succeeding motion clips. Additionally, motion inbetweening task is made\npossible by keeping the phase parameter of the input motion sequences fixed\nthroughout the diffusion process, showcasing the potential for extending the\nproposed framework to accommodate various application scenarios. Codes are\navailable at https://github.com/asdryau/TransPhase.",
            "headline_zh": "提出组合相位扩散方法以解决长运动序列生成中的过渡不连续问题",
            "intro_zh": [
                "核心问题：现有模型生成多语义运动序列时，过渡边界处运动动态不连续，产生突兀伪影。",
                "方法要点：利用SPDM和TPDM在潜在运动频域中逐步融入语义指导和相邻运动相位细节。",
                "实验或效果：框架在生成语义对齐的组合运动序列中表现竞争性，保持相位过渡连续性。"
            ],
            "tags_zh": [
                "运动序列生成",
                "扩散模型",
                "相位扩散",
                "组合运动",
                "过渡连续性",
                "运动插值"
            ],
            "_index": 104
        },
        {
            "title": "RoboANKLE: Design, Development, and Functional Evaluation of a Robotic Ankle with a Motorized Compliant Unit",
            "authors": [
                "Baris Baysal",
                "Omid Arfaie",
                "Ramazan Unal"
            ],
            "arxiv_id": "2510.14414v1",
            "summary": "This study presents a powered transtibial prosthesis with complete push-off\nassistance, RoboANKLE. The design aims to fulfill specific requirements, such\nas a sufficient range of motion (RoM) while providing the necessary torque for\nachieving natural ankle motion in daily activities. Addressing the challenges\nfaced in designing active transtibial prostheses, such as maintaining energetic\nautonomy and minimizing weight, is vital for the study. With this aim, we try\nto imitate the human ankle by providing extensive push-off assistance to\nachieve a natural-like torque profile. Thus, Energy Store and Extended Release\nmechanism (ESER) is employed with a novel Extra Energy Storage (EES) mechanism.\nKinematic and kinetic analyses are carried out to determine the design\nparameters and assess the design performance. Subsequently, a Computer-Aided\nDesign (CAD) model is built and used in comprehensive dynamic and structural\nanalyses. These analyses are used for the design performance evaluation and\ndetermine the forces and torques applied to the prosthesis, which aids in\noptimizing the design for minimal weight via structural analysis and topology\noptimization. The design of the prototype is then finalized and manufactured\nfor experimental evaluation to validate the design and functionality. The\nprototype is realized with a mass of 1.92 kg and dimensions of 261x107x420 mm.\nThe Functional evaluations of the RoboANKLE revealed that it is capable of\nachieving the natural maximum dorsi-flexion angle with 95% accuracy. Also,\nThanks to the implemented mechanisms, the results show that RoboANKLE can\ngenerate 57% higher than the required torque for natural walking. The result of\nthe power generation capacity of the RoboANKLE is 10% more than the natural\npower during the gait cycle.",
            "headline_zh": "提出RoboANKLE机器人踝关节，通过电机化顺应单元实现完整推离辅助，以模拟自然踝部运动。",
            "intro_zh": [
                "核心问题：设计主动式胫骨假体需平衡运动范围、扭矩、能量自主性和轻量化。",
                "方法要点：采用能量存储与扩展释放机制及额外能量存储机制，进行运动学和动力学分析。",
                "实验或效果：原型重1.92公斤，实现95%自然背屈角度准确性和57%额外扭矩。"
            ],
            "tags_zh": [
                "机器人假肢",
                "踝关节设计",
                "能量存储机制",
                "动态分析",
                "结构优化",
                "功能评估"
            ],
            "_index": 105
        },
        {
            "title": "DCMIL: A Progressive Representation Learning Model of Whole Slide Images for Cancer Prognosis Analysis",
            "authors": [
                "Chao Tu",
                "Kun Huang",
                "Jie Zhang",
                "Qianjin Feng",
                "Yu Zhang",
                "Zhenyuan Ning"
            ],
            "arxiv_id": "2510.14403v1",
            "summary": "The burgeoning discipline of computational pathology shows promise in\nharnessing whole slide images (WSIs) to quantify morphological heterogeneity\nand develop objective prognostic modes for human cancers. However, progress is\nimpeded by the computational bottleneck of gigapixel-size inputs and the\nscarcity of dense manual annotations. Current methods often overlook\nfine-grained information across multi-magnification WSIs and variations in\ntumor microenvironments. Here, we propose an easy-to-hard progressive\nrepresentation learning model, termed dual-curriculum contrastive\nmulti-instance learning (DCMIL), to efficiently process WSIs for cancer\nprognosis. The model does not rely on dense annotations and enables the direct\ntransformation of gigapixel-size WSIs into outcome predictions. Extensive\nexperiments on twelve cancer types (5,954 patients, 12.54 million tiles)\ndemonstrate that DCMIL outperforms standard WSI-based prognostic models.\nAdditionally, DCMIL identifies fine-grained prognosis-salient regions, provides\nrobust instance uncertainty estimation, and captures morphological differences\nbetween normal and tumor tissues, with the potential to generate new biological\ninsights. All codes have been made publicly accessible at\nhttps://github.com/tuuuc/DCMIL.",
            "headline_zh": "提出DCMIL模型以解决全切片图像癌症预后分析中的计算瓶颈和标注稀缺问题",
            "intro_zh": [
                "核心问题：全切片图像尺寸巨大且缺乏密集标注，阻碍癌症预后模型发展",
                "方法要点：采用双课程对比多实例学习，无需密集标注，渐进式学习多放大倍数特征",
                "实验或效果：在12种癌症类型上验证，性能优于标准模型，识别预后关键区域"
            ],
            "tags_zh": [
                "计算病理学",
                "全切片图像分析",
                "多实例学习",
                "癌症预后",
                "对比学习",
                "不确定性估计"
            ],
            "_index": 106
        },
        {
            "title": "BoardVision: Deployment-ready and Robust Motherboard Defect Detection with YOLO+Faster-RCNN Ensemble",
            "authors": [
                "Brandon Hill",
                "Kma Solaiman"
            ],
            "arxiv_id": "2510.14389v1",
            "summary": "Motherboard defect detection is critical for ensuring reliability in\nhigh-volume electronics manufacturing. While prior research in PCB inspection\nhas largely targeted bare-board or trace-level defects, assembly-level\ninspection of full motherboards inspection remains underexplored. In this work,\nwe present BoardVision, a reproducible framework for detecting assembly-level\ndefects such as missing screws, loose fan wiring, and surface scratches. We\nbenchmark two representative detectors - YOLOv7 and Faster R-CNN, under\ncontrolled conditions on the MiracleFactory motherboard dataset, providing the\nfirst systematic comparison in this domain. To mitigate the limitations of\nsingle models, where YOLO excels in precision but underperforms in recall and\nFaster R-CNN shows the reverse, we propose a lightweight ensemble,\nConfidence-Temporal Voting (CTV Voter), that balances precision and recall\nthrough interpretable rules. We further evaluate robustness under realistic\nperturbations including sharpness, brightness, and orientation changes,\nhighlighting stability challenges often overlooked in motherboard defect\ndetection. Finally, we release a deployable GUI-driven inspection tool that\nbridges research evaluation with operator usability. Together, these\ncontributions demonstrate how computer vision techniques can transition from\nbenchmark results to practical quality assurance for assembly-level motherboard\nmanufacturing.",
            "headline_zh": "提出BoardVision框架以解决主板组装缺陷检测问题",
            "intro_zh": [
                "核心问题：主板组装级缺陷检测在电子制造中未充分探索，如螺丝缺失和表面划痕",
                "方法要点：结合YOLOv7和Faster R-CNN，采用置信度-时间投票集成平衡精度与召回率",
                "实验或效果：在MiracleFactory数据集上评估，并测试了亮度、方向等扰动下的鲁棒性"
            ],
            "tags_zh": [
                "主板缺陷检测",
                "目标检测集成",
                "鲁棒性评估",
                "部署工具",
                "YOLO",
                "Faster R-CNN"
            ],
            "_index": 107
        },
        {
            "title": "DRBD-Mamba for Robust and Efficient Brain Tumor Segmentation with Analytical Insights",
            "authors": [
                "Danish Ali",
                "Ajmal Mian",
                "Naveed Akhtar",
                "Ghulam Mubashar Hassan"
            ],
            "arxiv_id": "2510.14383v1",
            "summary": "Accurate brain tumor segmentation is significant for clinical diagnosis and\ntreatment. It is challenging due to the heterogeneity of tumor subregions.\nMamba-based State Space Models have demonstrated promising performance.\nHowever, they incur significant computational overhead due to sequential\nfeature computation across multiple spatial axes. Moreover, their robustness\nacross diverse BraTS data partitions remains largely unexplored, leaving a\ncritical gap in reliable evaluation. To address these limitations, we propose\ndual-resolution bi-directional Mamba (DRBD-Mamba), an efficient 3D segmentation\nmodel that captures multi-scale long-range dependencies with minimal\ncomputational overhead. We leverage a space-filling curve to preserve spatial\nlocality during 3D-to-1D feature mapping, thereby reducing reliance on\ncomputationally expensive multi-axial feature scans. To enrich feature\nrepresentation, we propose a gated fusion module that adaptively integrates\nforward and reverse contexts, along with a quantization block that discretizes\nfeatures to improve robustness. In addition, we propose five systematic folds\non BraTS2023 for rigorous evaluation of segmentation techniques under diverse\nconditions and present detailed analysis of common failure scenarios. On the\n20\\% test set used by recent methods, our model achieves Dice improvements of\n0.10\\% for whole tumor, 1.75\\% for tumor core, and 0.93\\% for enhancing tumor.\nEvaluations on the proposed systematic five folds demonstrate that our model\nmaintains competitive whole tumor accuracy while achieving clear average Dice\ngains of 0.86\\% for tumor core and 1.45\\% for enhancing tumor over existing\nstate-of-the-art. Furthermore, our model attains 15 times improvement in\nefficiency while maintaining high segmentation accuracy, highlighting its\nrobustness and computational advantage over existing approaches.",
            "headline_zh": "提出DRBD-Mamba模型以高效鲁棒地分割脑肿瘤",
            "intro_zh": [
                "脑肿瘤分割因肿瘤亚区异质性而具挑战性，且现有Mamba模型计算开销大、鲁棒性未知",
                "采用双分辨率双向Mamba捕获多尺度长程依赖，结合空间填充曲线和门控融合模块提升效率与特征表示",
                "在BraTS2023上验证，模型在肿瘤核心和增强肿瘤上Dice提升，效率提高15倍，保持高精度"
            ],
            "tags_zh": [
                "脑肿瘤分割",
                "状态空间模型",
                "多尺度依赖",
                "计算效率",
                "鲁棒性评估"
            ],
            "_index": 108
        },
        {
            "title": "DOS: Directional Object Separation in Text Embeddings for Multi-Object Image Generation",
            "authors": [
                "Dongnam Byun",
                "Jungwon Park",
                "Jumgmin Ko",
                "Changin Choi",
                "Wonjong Rhee"
            ],
            "arxiv_id": "2510.14376v1",
            "summary": "Recent progress in text-to-image (T2I) generative models has led to\nsignificant improvements in generating high-quality images aligned with text\nprompts. However, these models still struggle with prompts involving multiple\nobjects, often resulting in object neglect or object mixing. Through extensive\nstudies, we identify four problematic scenarios, Similar Shapes, Similar\nTextures, Dissimilar Background Biases, and Many Objects, where inter-object\nrelationships frequently lead to such failures. Motivated by two key\nobservations about CLIP embeddings, we propose DOS (Directional Object\nSeparation), a method that modifies three types of CLIP text embeddings before\npassing them into text-to-image models. Experimental results show that DOS\nconsistently improves the success rate of multi-object image generation and\nreduces object mixing. In human evaluations, DOS significantly outperforms four\ncompeting methods, receiving 26.24%-43.04% more votes across four benchmarks.\nThese results highlight DOS as a practical and effective solution for improving\nmulti-object image generation.",
            "headline_zh": "提出DOS方法以解决多对象图像生成中的对象忽略与混合问题",
            "intro_zh": [
                "核心问题：多对象文本提示下，图像生成模型易出现对象忽略或混合，尤其在相似形状、纹理、背景偏差和对象过多场景。",
                "方法要点：基于CLIP嵌入观察，修改三种文本嵌入类型，实现对象方向性分离，提升多对象生成准确性。",
                "实验或效果：在人类评估中，DOS显著优于四种竞争方法，成功率提升26.24%-43.04%，减少对象混合。"
            ],
            "tags_zh": [
                "文本到图像生成",
                "多对象生成",
                "CLIP嵌入",
                "对象分离",
                "图像质量提升"
            ],
            "_index": 109
        },
        {
            "title": "Spatial Preference Rewarding for MLLMs Spatial Understanding",
            "authors": [
                "Han Qiu",
                "Peng Gao",
                "Lewei Lu",
                "Xiaoqin Zhang",
                "Ling Shao",
                "Shijian Lu"
            ],
            "arxiv_id": "2510.14374v1",
            "summary": "Multimodal large language models~(MLLMs) have demonstrated promising spatial\nunderstanding capabilities, such as referencing and grounding object\ndescriptions. Despite their successes, MLLMs still fall short in fine-grained\nspatial perception abilities, such as generating detailed region descriptions\nor accurately localizing objects. Additionally, they often fail to respond to\nthe user's requirements for desired fine-grained spatial understanding. This\nissue might arise because existing approaches primarily focus on tuning MLLMs\nto model pre-annotated instruction data to inject spatial knowledge, without\ndirect supervision of MLLMs' actual responses. We address this issue by SPR, a\nSpatial Preference Rewarding~(SPR) approach that enhances MLLMs' spatial\ncapabilities by rewarding MLLMs' detailed responses with precise object\nlocalization over vague or inaccurate responses. With randomly selected image\nregions and region descriptions from MLLMs, SPR introduces semantic and\nlocalization scores to comprehensively evaluate the text quality and\nlocalization quality in MLLM-generated descriptions. We also refine the MLLM\ndescriptions with better localization accuracy and pair the best-scored\nrefinement with the initial descriptions of the lowest score for direct\npreference optimization, thereby enhancing fine-grained alignment with visual\ninput. Extensive experiments over standard referring and grounding benchmarks\nshow that SPR improves MLLM spatial understanding capabilities effectively with\nminimal overhead in training. Data and code will be released at\nhttps://github.com/hanqiu-hq/SPR",
            "headline_zh": "提出空间偏好奖励方法以增强多模态大语言模型的细粒度空间理解能力",
            "intro_zh": [
                "核心问题：MLLMs在细粒度空间感知如区域描述和对象定位中表现不足，且难以响应用户需求。",
                "方法要点：SPR通过奖励详细响应和精确定位，使用语义和定位分数评估并优化描述质量。",
                "实验或效果：在标准基准测试中，SPR有效提升空间理解能力，训练开销最小。"
            ],
            "tags_zh": [
                "多模态大语言模型",
                "空间理解",
                "偏好奖励",
                "对象定位",
                "细粒度感知",
                "直接偏好优化"
            ],
            "_index": 110
        },
        {
            "title": "AI for Service: Proactive Assistance with AI Glasses",
            "authors": [
                "Zichen Wen",
                "Yiyu Wang",
                "Chenfei Liao",
                "Boxue Yang",
                "Junxian Li",
                "Weifeng Liu",
                "Haocong He",
                "Bolong Feng",
                "Xuyang Liu",
                "Yuanhuiyi Lyu",
                "Xu Zheng",
                "Xuming Hu",
                "Linfeng Zhang"
            ],
            "arxiv_id": "2510.14359v1",
            "summary": "In an era where AI is evolving from a passive tool into an active and\nadaptive companion, we introduce AI for Service (AI4Service), a new paradigm\nthat enables proactive and real-time assistance in daily life. Existing AI\nservices remain largely reactive, responding only to explicit user commands. We\nargue that a truly intelligent and helpful assistant should be capable of\nanticipating user needs and taking actions proactively when appropriate. To\nrealize this vision, we propose Alpha-Service, a unified framework that\naddresses two fundamental challenges: Know When to intervene by detecting\nservice opportunities from egocentric video streams, and Know How to provide\nboth generalized and personalized services. Inspired by the von Neumann\ncomputer architecture and based on AI glasses, Alpha-Service consists of five\nkey components: an Input Unit for perception, a Central Processing Unit for\ntask scheduling, an Arithmetic Logic Unit for tool utilization, a Memory Unit\nfor long-term personalization, and an Output Unit for natural human\ninteraction. As an initial exploration, we implement Alpha-Service through a\nmulti-agent system deployed on AI glasses. Case studies, including a real-time\nBlackjack advisor, a museum tour guide, and a shopping fit assistant,\ndemonstrate its ability to seamlessly perceive the environment, infer user\nintent, and provide timely and useful assistance without explicit prompts.",
            "headline_zh": "提出Alpha-Service框架，通过AI眼镜实现主动实时服务辅助",
            "intro_zh": [
                "核心问题：现有AI服务多为被动响应，需转向主动预测用户需求",
                "方法要点：基于冯·诺依曼架构，构建多组件系统感知环境与个性化服务",
                "实验或效果：案例展示如Blackjack顾问，能无提示感知意图并提供帮助"
            ],
            "tags_zh": [
                "主动服务",
                "AI眼镜",
                "多智能体系统",
                "第一人称视频",
                "个性化辅助",
                "实时干预"
            ],
            "_index": 111
        },
        {
            "title": "SUM-AgriVLN: Spatial Understanding Memory for Agricultural Vision-and-Language Navigation",
            "authors": [
                "Xiaobei Zhao",
                "Xingqi Lyu",
                "Xiang Li"
            ],
            "arxiv_id": "2510.14357v1",
            "summary": "Agricultural robots are emerging as powerful assistants across a wide range\nof agricultural tasks, nevertheless, still heavily rely on manual operation or\nfixed rail systems for movement. The AgriVLN method and the A2A benchmark\npioneeringly extend Vision-and-Language Navigation (VLN) to the agricultural\ndomain, enabling robots to navigate to the target positions following the\nnatural language instructions. In practical agricultural scenarios, navigation\ninstructions often repeatedly occur, yet AgriVLN treat each instruction as an\nindependent episode, overlooking the potential of past experiences to provide\nspatial context for subsequent ones. To bridge this gap, we propose the method\nof Spatial Understanding Memory for Agricultural Vision-and-Language Navigation\n(SUM-AgriVLN), in which the SUM module employs spatial understanding and save\nspatial memory through 3D reconstruction and representation. When evaluated on\nthe A2A benchmark, our SUM-AgriVLN effectively improves Success Rate from 0.47\nto 0.54 with slight sacrifice on Navigation Error from 2.91m to 2.93m,\ndemonstrating the state-of-the-art performance in the agricultural domain.\nCode: https://github.com/AlexTraveling/SUM-AgriVLN.",
            "headline_zh": "提出空间理解记忆方法以提升农业视觉语言导航性能",
            "intro_zh": [
                "核心问题：农业导航中重复指令被独立处理，缺乏空间上下文利用。",
                "方法要点：通过3D重建和表示构建空间记忆模块，增强导航理解。",
                "实验效果：在A2A基准上，成功率从0.47提升至0.54，导航误差略增。"
            ],
            "tags_zh": [
                "农业机器人",
                "视觉语言导航",
                "空间记忆",
                "3D重建",
                "A2A基准"
            ],
            "_index": 112
        },
        {
            "title": "Leveraging Cycle-Consistent Anchor Points for Self-Supervised RGB-D Registration",
            "authors": [
                "Siddharth Tourani",
                "Jayaram Reddy",
                "Sarvesh Thakur",
                "K Madhava Krishna",
                "Muhammad Haris Khan",
                "N Dinesh Reddy"
            ],
            "arxiv_id": "2510.14354v1",
            "summary": "With the rise in consumer depth cameras, a wealth of unlabeled RGB-D data has\nbecome available. This prompts the question of how to utilize this data for\ngeometric reasoning of scenes. While many RGB-D registration meth- ods rely on\ngeometric and feature-based similarity, we take a different approach. We use\ncycle-consistent keypoints as salient points to enforce spatial coherence\nconstraints during matching, improving correspondence accuracy. Additionally,\nwe introduce a novel pose block that combines a GRU recurrent unit with\ntransformation synchronization, blending historical and multi-view data. Our\napproach surpasses previous self- supervised registration methods on ScanNet\nand 3DMatch, even outperforming some older supervised methods. We also\nintegrate our components into existing methods, showing their effectiveness.",
            "headline_zh": "提出基于循环一致锚点的自监督RGB-D配准方法，提升场景几何推理精度。",
            "intro_zh": [
                "核心问题：如何利用未标记RGB-D数据进行场景几何推理，避免依赖几何或特征相似性。",
                "方法要点：使用循环一致关键点增强空间一致性，结合GRU与变换同步融合历史与多视图数据。",
                "实验或效果：在ScanNet和3DMatch上超越先前自监督方法，部分优于旧有监督方法。"
            ],
            "tags_zh": [
                "RGB-D配准",
                "自监督学习",
                "循环一致性",
                "关键点检测",
                "变换同步",
                "多视图融合"
            ],
            "_index": 113
        },
        {
            "title": "Vision-Centric Activation and Coordination for Multimodal Large Language Models",
            "authors": [
                "Yunnan Wang",
                "Fan Lu",
                "Kecheng Zheng",
                "Ziyuan Huang",
                "Ziqiang Li",
                "Wenjun Zeng",
                "Xin Jin"
            ],
            "arxiv_id": "2510.14349v1",
            "summary": "Multimodal large language models (MLLMs) integrate image features from visual\nencoders with LLMs, demonstrating advanced comprehension capabilities. However,\nmainstream MLLMs are solely supervised by the next-token prediction of textual\ntokens, neglecting critical vision-centric information essential for analytical\nabilities. To track this dilemma, we introduce VaCo, which optimizes MLLM\nrepresentations through Vision-Centric activation and Coordination from\nmultiple vision foundation models (VFMs). VaCo introduces visual discriminative\nalignment to integrate task-aware perceptual features extracted from VFMs,\nthereby unifying the optimization of both textual and visual outputs in MLLMs.\nSpecifically, we incorporate the learnable Modular Task Queries (MTQs) and\nVisual Alignment Layers (VALs) into MLLMs, activating specific visual signals\nunder the supervision of diverse VFMs. To coordinate representation conflicts\nacross VFMs, the crafted Token Gateway Mask (TGM) restricts the information\nflow among multiple groups of MTQs. Extensive experiments demonstrate that VaCo\nsignificantly improves the performance of different MLLMs on various\nbenchmarks, showcasing its superior capabilities in visual comprehension.",
            "headline_zh": "提出VaCo方法以优化多模态大语言模型的视觉中心表示",
            "intro_zh": [
                "主流MLLMs仅监督文本标记预测，忽视视觉中心信息，影响分析能力",
                "引入视觉判别对齐、模块化任务查询和视觉对齐层，协调多视觉基础模型特征",
                "实验显示VaCo显著提升多种MLLMs在视觉理解基准上的性能"
            ],
            "tags_zh": [
                "多模态大语言模型",
                "视觉中心激活",
                "视觉基础模型",
                "表示优化",
                "视觉理解",
                "模块化任务查询"
            ],
            "_index": 114
        },
        {
            "title": "A Density-Informed Multimodal Artificial Intelligence Framework for Improving Breast Cancer Detection Across All Breast Densities",
            "authors": [
                "Siva Teja Kakileti",
                "Bharath Govindaraju",
                "Sudhakar Sampangi",
                "Geetha Manjunath"
            ],
            "arxiv_id": "2510.14340v1",
            "summary": "Mammography, the current standard for breast cancer screening, has reduced\nsensitivity in women with dense breast tissue, contributing to missed or\ndelayed diagnoses. Thermalytix, an AI-based thermal imaging modality, captures\nfunctional vascular and metabolic cues that may complement mammographic\nstructural data. This study investigates whether a breast density-informed\nmulti-modal AI framework can improve cancer detection by dynamically selecting\nthe appropriate imaging modality based on breast tissue composition. A total of\n324 women underwent both mammography and thermal imaging. Mammography images\nwere analyzed using a multi-view deep learning model, while Thermalytix\nassessed thermal images through vascular and thermal radiomics. The proposed\nframework utilized Mammography AI for fatty breasts and Thermalytix AI for\ndense breasts, optimizing predictions based on tissue type. This multi-modal AI\nframework achieved a sensitivity of 94.55% (95% CI: 88.54-100) and specificity\nof 79.93% (95% CI: 75.14-84.71), outperforming standalone mammography AI\n(sensitivity 81.82%, specificity 86.25%) and Thermalytix AI (sensitivity\n92.73%, specificity 75.46%). Importantly, the sensitivity of Mammography\ndropped significantly in dense breasts (67.86%) versus fatty breasts (96.30%),\nwhereas Thermalytix AI maintained high and consistent sensitivity in both\n(92.59% and 92.86%, respectively). This demonstrates that a density-informed\nmulti-modal AI framework can overcome key limitations of unimodal screening and\ndeliver high performance across diverse breast compositions. The proposed\nframework is interpretable, low-cost, and easily deployable, offering a\npractical path to improving breast cancer screening outcomes in both\nhigh-resource and resource-limited settings.",
            "headline_zh": "提出基于乳腺密度的多模态AI框架，以提升不同密度乳腺的癌症检测性能",
            "intro_zh": [
                "乳腺X线摄影在致密乳腺中敏感性下降，导致漏诊或延迟诊断",
                "结合乳腺X线和热成像AI，根据乳腺密度动态选择最优模态",
                "多模态框架敏感性达94.55%，优于单模态方法，尤其在致密乳腺中表现稳定"
            ],
            "tags_zh": [
                "乳腺癌检测",
                "多模态AI",
                "乳腺密度",
                "热成像",
                "深度学习",
                "放射组学"
            ],
            "_index": 115
        },
        {
            "title": "Risk-Aware Reinforcement Learning with Bandit-Based Adaptation for Quadrupedal Locomotion",
            "authors": [
                "Yuanhong Zeng",
                "Anushri Dixit"
            ],
            "arxiv_id": "2510.14338v1",
            "summary": "In this work, we study risk-aware reinforcement learning for quadrupedal\nlocomotion. Our approach trains a family of risk-conditioned policies using a\nConditional Value-at-Risk (CVaR) constrained policy optimization technique that\nprovides improved stability and sample efficiency. At deployment, we adaptively\nselect the best performing policy from the family of policies using a\nmulti-armed bandit framework that uses only observed episodic returns, without\nany privileged environment information, and adapts to unknown conditions on the\nfly. Hence, we train quadrupedal locomotion policies at various levels of\nrobustness using CVaR and adaptively select the desired level of robustness\nonline to ensure performance in unknown environments. We evaluate our method in\nsimulation across eight unseen settings (by changing dynamics, contacts,\nsensing noise, and terrain) and on a Unitree Go2 robot in previously unseen\nterrains. Our risk-aware policy attains nearly twice the mean and tail\nperformance in unseen environments compared to other baselines and our\nbandit-based adaptation selects the best-performing risk-aware policy in\nunknown terrain within two minutes of operation.",
            "headline_zh": "提出风险感知强化学习与多臂老虎机自适应方法，以提升四足机器人在未知环境中的运动性能。",
            "intro_zh": [
                "核心问题：四足机器人在未知环境中运动时，面临稳定性与性能下降的风险。",
                "方法要点：使用CVaR约束策略优化训练风险条件策略家族，并通过多臂老虎机在线自适应选择最优策略。",
                "实验效果：在模拟和真实机器人测试中，风险感知策略在未知环境中性能提升近一倍，自适应选择在2分钟内完成。"
            ],
            "tags_zh": [
                "风险感知强化学习",
                "四足机器人运动",
                "CVaR约束优化",
                "多臂老虎机",
                "在线自适应",
                "未知环境鲁棒性"
            ],
            "_index": 116
        },
        {
            "title": "A Multi-domain Image Translative Diffusion StyleGAN for Iris Presentation Attack Detection",
            "authors": [
                "Shivangi Yadav",
                "Arun Ross"
            ],
            "arxiv_id": "2510.14314v1",
            "summary": "An iris biometric system can be compromised by presentation attacks (PAs)\nwhere artifacts such as artificial eyes, printed eye images, or cosmetic\ncontact lenses are presented to the system. To counteract this, several\npresentation attack detection (PAD) methods have been developed. However, there\nis a scarcity of datasets for training and evaluating iris PAD techniques due\nto the implicit difficulties in constructing and imaging PAs. To address this,\nwe introduce the Multi-domain Image Translative Diffusion StyleGAN\n(MID-StyleGAN), a new framework for generating synthetic ocular images that\ncaptures the PA and bonafide characteristics in multiple domains such as\nbonafide, printed eyes and cosmetic contact lens. MID-StyleGAN combines the\nstrengths of diffusion models and generative adversarial networks (GANs) to\nproduce realistic and diverse synthetic data. Our approach utilizes a\nmulti-domain architecture that enables the translation between bonafide ocular\nimages and different PA domains. The model employs an adaptive loss function\ntailored for ocular data to maintain domain consistency. Extensive experiments\ndemonstrate that MID-StyleGAN outperforms existing methods in generating\nhigh-quality synthetic ocular images. The generated data was used to\nsignificantly enhance the performance of PAD systems, providing a scalable\nsolution to the data scarcity problem in iris and ocular biometrics. For\nexample, on the LivDet2020 dataset, the true detect rate at 1% false detect\nrate improved from 93.41% to 98.72%, showcasing the impact of the proposed\nmethod.",
            "headline_zh": "提出MID-StyleGAN以解决虹膜呈现攻击检测中数据稀缺问题",
            "intro_zh": [
                "核心问题：虹膜生物识别系统易受呈现攻击，但缺乏训练数据。",
                "方法要点：结合扩散模型与GAN，生成多领域合成眼部图像。",
                "实验或效果：在LivDet2020数据集上，真检测率从93.41%提升至98.72%。"
            ],
            "tags_zh": [
                "虹膜呈现攻击检测",
                "合成数据生成",
                "多领域图像翻译",
                "扩散模型",
                "生成对抗网络"
            ],
            "_index": 117
        },
        {
            "title": "Watermarking for Factuality: Guiding Vision-Language Models Toward Truth via Tri-layer Contrastive Decoding",
            "authors": [
                "Kyungryul Back",
                "Seongbeom Park",
                "Milim Kim",
                "Mincheol Kwon",
                "SangHyeok Lee",
                "Hyunyoung Lee",
                "Junhee Cho",
                "Seunghyun Park",
                "Jinkyu Kim"
            ],
            "arxiv_id": "2510.14304v1",
            "summary": "Large Vision-Language Models (LVLMs) have recently shown promising results on\nvarious multimodal tasks, even achieving human-comparable performance in\ncertain cases. Nevertheless, LVLMs remain prone to hallucinations -- they often\nrely heavily on a single modality or memorize training data without properly\ngrounding their outputs. To address this, we propose a training-free, tri-layer\ncontrastive decoding with watermarking, which proceeds in three steps: (1)\nselect a mature layer and an amateur layer among the decoding layers, (2)\nidentify a pivot layer using a watermark-related question to assess whether the\nlayer is visually well-grounded, and (3) apply tri-layer contrastive decoding\nto generate the final output. Experiments on public benchmarks such as POPE,\nMME and AMBER demonstrate that our method achieves state-of-the-art performance\nin reducing hallucinations in LVLMs and generates more visually grounded\nresponses.",
            "headline_zh": "提出基于水印的三层对比解码方法，以减少视觉语言模型的幻觉问题。",
            "intro_zh": [
                "核心问题：视觉语言模型易产生幻觉，依赖单一模态或记忆训练数据。",
                "方法要点：无训练三层对比解码，选择成熟层、业余层和视觉接地支点层。",
                "实验或效果：在POPE等基准上实现SOTA，减少幻觉并增强视觉接地响应。"
            ],
            "tags_zh": [
                "视觉语言模型",
                "幻觉减少",
                "对比解码",
                "视觉接地",
                "无训练方法"
            ],
            "_index": 118
        },
        {
            "title": "Expertise need not monopolize: Action-Specialized Mixture of Experts for Vision-Language-Action Learning",
            "authors": [
                "Weijie Shen",
                "Yitian Liu",
                "Yuhao Wu",
                "Zhixuan Liang",
                "Sijia Gu",
                "Dehui Wang",
                "Tian Nian",
                "Lei Xu",
                "Yusen Qin",
                "Jiangmiao Pang",
                "Xinping Guan",
                "Xiaokang Yang",
                "Yao Mu"
            ],
            "arxiv_id": "2510.14300v1",
            "summary": "Vision-Language-Action (VLA) models are experiencing rapid development and\ndemonstrating promising capabilities in robotic manipulation tasks. However,\nscaling up VLA models presents several critical challenges: (1) Training new\nVLA models from scratch demands substantial computational resources and\nextensive datasets. Given the current scarcity of robot data, it becomes\nparticularly valuable to fully leverage well-pretrained VLA model weights\nduring the scaling process. (2) Real-time control requires carefully balancing\nmodel capacity with computational efficiency. To address these challenges, We\npropose AdaMoE, a Mixture-of-Experts (MoE) architecture that inherits\npretrained weights from dense VLA models, and scales up the action expert by\nsubstituting the feedforward layers into sparsely activated MoE layers. AdaMoE\nemploys a decoupling technique that decouples expert selection from expert\nweighting through an independent scale adapter working alongside the\ntraditional router. This enables experts to be selected based on task relevance\nwhile contributing with independently controlled weights, allowing\ncollaborative expert utilization rather than winner-takes-all dynamics. Our\napproach demonstrates that expertise need not monopolize. Instead, through\ncollaborative expert utilization, we can achieve superior performance while\nmaintaining computational efficiency. AdaMoE consistently outperforms the\nbaseline model across key benchmarks, delivering performance gains of 1.8% on\nLIBERO and 9.3% on RoboTwin. Most importantly, a substantial 21.5% improvement\nin real-world experiments validates its practical effectiveness for robotic\nmanipulation tasks.",
            "headline_zh": "提出AdaMoE架构以解决视觉-语言-动作模型扩展中的计算效率与性能平衡问题",
            "intro_zh": [
                "核心问题：VLA模型扩展需高计算资源与稀缺机器人数据，且需平衡模型容量与实时控制效率",
                "方法要点：采用混合专家架构，继承预训练权重，通过解耦专家选择与权重实现协作利用",
                "实验或效果：在LIBERO和RoboTwin基准上性能提升1.8%和9.3%，真实世界实验提升21.5%"
            ],
            "tags_zh": [
                "视觉-语言-动作模型",
                "混合专家架构",
                "机器人操作",
                "模型扩展",
                "计算效率"
            ],
            "_index": 119
        },
        {
            "title": "Learning Human-Humanoid Coordination for Collaborative Object Carrying",
            "authors": [
                "Yushi Du",
                "Yixuan Li",
                "Baoxiong Jia",
                "Yutang Lin",
                "Pei Zhou",
                "Wei Liang",
                "Yanchao Yang",
                "Siyuan Huang"
            ],
            "arxiv_id": "2510.14293v1",
            "summary": "Human-humanoid collaboration shows significant promise for applications in\nhealthcare, domestic assistance, and manufacturing. While compliant robot-human\ncollaboration has been extensively developed for robotic arms, enabling\ncompliant human-humanoid collaboration remains largely unexplored due to\nhumanoids' complex whole-body dynamics. In this paper, we propose a\nproprioception-only reinforcement learning approach, COLA, that combines leader\nand follower behaviors within a single policy. The model is trained in a\nclosed-loop environment with dynamic object interactions to predict object\nmotion patterns and human intentions implicitly, enabling compliant\ncollaboration to maintain load balance through coordinated trajectory planning.\nWe evaluate our approach through comprehensive simulator and real-world\nexperiments on collaborative carrying tasks, demonstrating the effectiveness,\ngeneralization, and robustness of our model across various terrains and\nobjects. Simulation experiments demonstrate that our model reduces human effort\nby 24.7%. compared to baseline approaches while maintaining object stability.\nReal-world experiments validate robust collaborative carrying across different\nobject types (boxes, desks, stretchers, etc.) and movement patterns\n(straight-line, turning, slope climbing). Human user studies with 23\nparticipants confirm an average improvement of 27.4% compared to baseline\nmodels. Our method enables compliant human-humanoid collaborative carrying\nwithout requiring external sensors or complex interaction models, offering a\npractical solution for real-world deployment.",
            "headline_zh": "提出COLA方法以解决人形机器人与人协作搬运中的顺应性问题",
            "intro_zh": [
                "核心问题：人形机器人全身动力学复杂，难以实现顺应性人机协作搬运。",
                "方法要点：使用仅本体感知的强化学习，结合领导者和跟随者行为于单一策略。",
                "实验效果：模拟实验减少人力24.7%，真实实验验证多对象和地形下的鲁棒性。"
            ],
            "tags_zh": [
                "人形机器人协作",
                "强化学习",
                "本体感知",
                "轨迹规划",
                "负载平衡"
            ],
            "_index": 120
        },
        {
            "title": "CLEAR: Causal Learning Framework For Robust Histopathology Tumor Detection Under Out-Of-Distribution Shifts",
            "authors": [
                "Kieu-Anh Truong Thi",
                "Huy-Hieu Pham",
                "Duc-Trong Le"
            ],
            "arxiv_id": "2510.14273v1",
            "summary": "Domain shift in histopathology, often caused by differences in acquisition\nprocesses or data sources, poses a major challenge to the generalization\nability of deep learning models. Existing methods primarily rely on modeling\nstatistical correlations by aligning feature distributions or introducing\nstatistical variation, yet they often overlook causal relationships. In this\nwork, we propose a novel causal-inference-based framework that leverages\nsemantic features while mitigating the impact of confounders. Our method\nimplements the front-door principle by designing transformation strategies that\nexplicitly incorporate mediators and observed tissue slides. We validate our\nmethod on the CAMELYON17 dataset and a private histopathology dataset,\ndemonstrating consistent performance gains across unseen domains. As a result,\nour approach achieved up to a 7% improvement in both the CAMELYON17 dataset and\nthe private histopathology dataset, outperforming existing baselines. These\nresults highlight the potential of causal inference as a powerful tool for\naddressing domain shift in histopathology image analysis.",
            "headline_zh": "提出因果学习框架CLEAR以解决组织病理学肿瘤检测中的域偏移问题",
            "intro_zh": [
                "核心问题：组织病理学图像因采集过程或数据源差异导致域偏移，影响深度学习模型泛化能力",
                "方法要点：基于因果推断，利用前门原理设计转换策略，结合语义特征和中介变量减少混杂因素影响",
                "实验或效果：在CAMELYON17和私有数据集上验证，域外性能提升达7%，优于现有基线方法"
            ],
            "tags_zh": [
                "组织病理学图像分析",
                "因果推断",
                "域偏移",
                "肿瘤检测",
                "前门原理",
                "深度学习"
            ],
            "_index": 121
        },
        {
            "title": "GauSSmart: Enhanced 3D Reconstruction through 2D Foundation Models and Geometric Filtering",
            "authors": [
                "Alexander Valverde",
                "Brian Xu",
                "Yuyin Zhou",
                "Meng Xu",
                "Hongyun Wang"
            ],
            "arxiv_id": "2510.14270v1",
            "summary": "Scene reconstruction has emerged as a central challenge in computer vision,\nwith approaches such as Neural Radiance Fields (NeRF) and Gaussian Splatting\nachieving remarkable progress. While Gaussian Splatting demonstrates strong\nperformance on large-scale datasets, it often struggles to capture fine details\nor maintain realism in regions with sparse coverage, largely due to the\ninherent limitations of sparse 3D training data.\n  In this work, we propose GauSSmart, a hybrid method that effectively bridges\n2D foundational models and 3D Gaussian Splatting reconstruction. Our approach\nintegrates established 2D computer vision techniques, including convex\nfiltering and semantic feature supervision from foundational models such as\nDINO, to enhance Gaussian-based scene reconstruction. By leveraging 2D\nsegmentation priors and high-dimensional feature embeddings, our method guides\nthe densification and refinement of Gaussian splats, improving coverage in\nunderrepresented areas and preserving intricate structural details.\n  We validate our approach across three datasets, where GauSSmart consistently\noutperforms existing Gaussian Splatting in the majority of evaluated scenes.\nOur results demonstrate the significant potential of hybrid 2D-3D approaches,\nhighlighting how the thoughtful combination of 2D foundational models with 3D\nreconstruction pipelines can overcome the limitations inherent in either\napproach alone.",
            "headline_zh": "提出GauSSmart，通过2D基础模型和几何过滤增强3D高斯溅射重建",
            "intro_zh": [
                "核心问题：高斯溅射在稀疏3D数据区域难以捕捉细节和保持真实感",
                "方法要点：集成2D分割先验和特征嵌入，指导高斯溅射的致密化和细化",
                "实验或效果：在多个数据集上优于现有高斯溅射方法，提升覆盖率和细节保留"
            ],
            "tags_zh": [
                "3D重建",
                "高斯溅射",
                "2D基础模型",
                "几何过滤",
                "语义特征监督"
            ],
            "_index": 122
        },
        {
            "title": "Experimental Demonstration of Event-based Optical Camera Communication in Long-Range Outdoor Environment",
            "authors": [
                "Miu Sumino",
                "Mayu Ishii",
                "Shun Kaizu",
                "Daisuke Hisano",
                "Yu Nakayama"
            ],
            "arxiv_id": "2510.14266v1",
            "summary": "We propose a robust demodulation scheme for optical camera communication\nsystems using an event-based vision sensor, combining OOK with toggle\ndemodulation and a digital phase-locked loop. This is the first report to\nachieve a $\\mathrm{BER} < 10^{-3}$ at 200m-60kbps and 400m-30kbps in outdoor\nexperiments.",
            "headline_zh": "提出基于事件视觉传感器的稳健解调方案，实现长距离室外光相机通信。",
            "intro_zh": [
                "核心问题：在长距离室外环境中，光相机通信系统易受干扰，导致误码率高。",
                "方法要点：结合OOK与切换解调及数字锁相环，提升解调鲁棒性。",
                "实验或效果：首次在200米-60kbps和400米-30kbps下实现误码率低于10^{-3}。"
            ],
            "tags_zh": [
                "光相机通信",
                "事件视觉传感器",
                "稳健解调",
                "长距离通信",
                "室外实验"
            ],
            "_index": 123
        },
        {
            "title": "MatchAttention: Matching the Relative Positions for High-Resolution Cross-View Matching",
            "authors": [
                "Tingman Yan",
                "Tao Liu",
                "Xilian Yang",
                "Qunfei Zhao",
                "Zeyang Xia"
            ],
            "arxiv_id": "2510.14260v1",
            "summary": "Cross-view matching is fundamentally achieved through cross-attention\nmechanisms. However, matching of high-resolution images remains challenging due\nto the quadratic complexity and lack of explicit matching constraints in the\nexisting cross-attention. This paper proposes an attention mechanism,\nMatchAttention, that dynamically matches relative positions. The relative\nposition determines the attention sampling center of the key-value pairs given\na query. Continuous and differentiable sliding-window attention sampling is\nachieved by the proposed BilinearSoftmax. The relative positions are\niteratively updated through residual connections across layers by embedding\nthem into the feature channels. Since the relative position is exactly the\nlearning target for cross-view matching, an efficient hierarchical cross-view\ndecoder, MatchDecoder, is designed with MatchAttention as its core component.\nTo handle cross-view occlusions, gated cross-MatchAttention and a\nconsistency-constrained loss are proposed. These two components collectively\nmitigate the impact of occlusions in both forward and backward passes, allowing\nthe model to focus more on learning matching relationships. When applied to\nstereo matching, MatchStereo-B ranked 1st in average error on the public\nMiddlebury benchmark and requires only 29ms for KITTI-resolution inference.\nMatchStereo-T can process 4K UHD images in 0.1 seconds using only 3GB of GPU\nmemory. The proposed models also achieve state-of-the-art performance on KITTI\n2012, KITTI 2015, ETH3D, and Spring flow datasets. The combination of high\naccuracy and low computational complexity makes real-time, high-resolution, and\nhigh-accuracy cross-view matching possible. Code is available at\nhttps://github.com/TingmanYan/MatchAttention.",
            "headline_zh": "提出MatchAttention机制以解决高分辨率跨视图匹配的效率和精度问题",
            "intro_zh": [
                "核心问题：高分辨率图像跨视图匹配存在二次复杂度和缺乏显式匹配约束的挑战",
                "方法要点：通过BilinearSoftmax实现连续可微滑动窗口注意力采样，迭代更新相对位置",
                "实验或效果：在多个基准数据集上达到SOTA，支持实时高分辨率处理，如4K图像0.1秒推理"
            ],
            "tags_zh": [
                "跨视图匹配",
                "注意力机制",
                "立体匹配",
                "高分辨率图像处理",
                "实时推理"
            ],
            "_index": 124
        },
        {
            "title": "Identity-GRPO: Optimizing Multi-Human Identity-preserving Video Generation via Reinforcement Learning",
            "authors": [
                "Xiangyu Meng",
                "Zixian Zhang",
                "Zhenghao Zhang",
                "Junchao Liao",
                "Long Qin",
                "Weizhi Wang"
            ],
            "arxiv_id": "2510.14256v1",
            "summary": "While advanced methods like VACE and Phantom have advanced video generation\nfor specific subjects in diverse scenarios, they struggle with multi-human\nidentity preservation in dynamic interactions, where consistent identities\nacross multiple characters are critical. To address this, we propose\nIdentity-GRPO, a human feedback-driven optimization pipeline for refining\nmulti-human identity-preserving video generation. First, we construct a video\nreward model trained on a large-scale preference dataset containing\nhuman-annotated and synthetic distortion data, with pairwise annotations\nfocused on maintaining human consistency throughout the video. We then employ a\nGRPO variant tailored for multi-human consistency, which greatly enhances both\nVACE and Phantom. Through extensive ablation studies, we evaluate the impact of\nannotation quality and design choices on policy optimization. Experiments show\nthat Identity-GRPO achieves up to 18.9% improvement in human consistency\nmetrics over baseline methods, offering actionable insights for aligning\nreinforcement learning with personalized video generation.",
            "headline_zh": "提出Identity-GRPO以优化多人类身份保持的视频生成",
            "intro_zh": [
                "核心问题：现有方法在动态交互中难以保持多人类身份一致性",
                "方法要点：构建视频奖励模型，并采用GRPO变体优化生成策略",
                "实验或效果：在人类一致性指标上比基线方法提升高达18.9%"
            ],
            "tags_zh": [
                "视频生成",
                "身份保持",
                "强化学习",
                "多人类一致性",
                "奖励模型"
            ],
            "_index": 125
        },
        {
            "title": "Identity-Preserving Image-to-Video Generation via Reward-Guided Optimization",
            "authors": [
                "Liao Shen",
                "Wentao Jiang",
                "Yiran Zhu",
                "Tiezheng Ge",
                "Zhiguo Cao",
                "Bo Zheng"
            ],
            "arxiv_id": "2510.14255v1",
            "summary": "Recent advances in image-to-video (I2V) generation have achieved remarkable\nprogress in synthesizing high-quality, temporally coherent videos from static\nimages. Among all the applications of I2V, human-centric video generation\nincludes a large portion. However, existing I2V models encounter difficulties\nin maintaining identity consistency between the input human image and the\ngenerated video, especially when the person in the video exhibits significant\nexpression changes and movements. This issue becomes critical when the human\nface occupies merely a small fraction of the image. Since humans are highly\nsensitive to identity variations, this poses a critical yet under-explored\nchallenge in I2V generation. In this paper, we propose Identity-Preserving\nReward-guided Optimization (IPRO), a novel video diffusion framework based on\nreinforcement learning to enhance identity preservation. Instead of introducing\nauxiliary modules or altering model architectures, our approach introduces a\ndirect and effective tuning algorithm that optimizes diffusion models using a\nface identity scorer. To improve performance and accelerate convergence, our\nmethod backpropagates the reward signal through the last steps of the sampling\nchain, enabling richer gradient feedback. We also propose a novel facial\nscoring mechanism that treats faces in ground-truth videos as facial feature\npools, providing multi-angle facial information to enhance generalization. A\nKL-divergence regularization is further incorporated to stabilize training and\nprevent overfitting to the reward signal. Extensive experiments on Wan 2.2 I2V\nmodel and our in-house I2V model demonstrate the effectiveness of our method.\nOur project and code are available at\n\\href{https://ipro-alimama.github.io/}{https://ipro-alimama.github.io/}.",
            "headline_zh": "提出身份保持奖励引导优化方法以解决图像到视频生成中身份一致性问题",
            "intro_zh": [
                "核心问题：现有图像到视频模型在人物表情和运动变化时难以保持输入图像与生成视频的身份一致性",
                "方法要点：基于强化学习优化扩散模型，使用人脸身份评分器和KL散度正则化提升身份保持",
                "实验或效果：在Wan 2.2和内部模型上验证方法有效性，增强身份保持和泛化能力"
            ],
            "tags_zh": [
                "图像到视频生成",
                "身份保持",
                "奖励引导优化",
                "扩散模型",
                "人脸身份评分"
            ],
            "_index": 126
        },
        {
            "title": "MACE: Mixture-of-Experts Accelerated Coordinate Encoding for Large-Scale Scene Localization and Rendering",
            "authors": [
                "Mingkai Liu",
                "Dikai Fan",
                "Haohua Que",
                "Haojia Gao",
                "Xiao Liu",
                "Shuxue Peng",
                "Meixia Lin",
                "Shengyu Gu",
                "Ruicong Ye",
                "Wanli Qiu",
                "Handong Yao",
                "Ruopeng Zhang",
                "Xianliang Huang"
            ],
            "arxiv_id": "2510.14251v1",
            "summary": "Efficient localization and high-quality rendering in large-scale scenes\nremain a significant challenge due to the computational cost involved. While\nScene Coordinate Regression (SCR) methods perform well in small-scale\nlocalization, they are limited by the capacity of a single network when\nextended to large-scale scenes. To address these challenges, we propose the\nMixed Expert-based Accelerated Coordinate Encoding method (MACE), which enables\nefficient localization and high-quality rendering in large-scale scenes.\nInspired by the remarkable capabilities of MOE in large model domains, we\nintroduce a gating network to implicitly classify and select sub-networks,\nensuring that only a single sub-network is activated during each inference.\nFurtheremore, we present Auxiliary-Loss-Free Load Balancing(ALF-LB) strategy to\nenhance the localization accuracy on large-scale scene. Our framework provides\na significant reduction in costs while maintaining higher precision, offering\nan efficient solution for large-scale scene applications. Additional\nexperiments on the Cambridge test set demonstrate that our method achieves\nhigh-quality rendering results with merely 10 minutes of training.",
            "headline_zh": "提出MACE方法以解决大规模场景定位与渲染中的计算效率问题",
            "intro_zh": [
                "核心问题：大规模场景定位与渲染计算成本高，单网络容量不足。",
                "方法要点：引入门控网络选择子网络，仅激活单个子网络进行推理。",
                "实验效果：在剑桥测试集上，仅10分钟训练即可实现高质量渲染。"
            ],
            "tags_zh": [
                "场景坐标回归",
                "混合专家模型",
                "门控网络",
                "负载均衡",
                "大规模场景定位",
                "高效渲染"
            ],
            "_index": 127
        },
        {
            "title": "Event Interval Modulation: A Novel Scheme for Event-based Optical Camera Communication",
            "authors": [
                "Miu Sumino",
                "Mayu Ishii",
                "Shun Kaizu",
                "Daisuke Hisano",
                "Yu Nakayama"
            ],
            "arxiv_id": "2510.14245v1",
            "summary": "Optical camera communication (OCC) represents a promising visible light\ncommunication technology. Nonetheless, typical OCC systems utilizing\nframe-based cameras are encumbered by limitations, including low bit rate and\nhigh processing load. To address these issues, OCC system utilizing an\nevent-based vision sensor (EVS) as receivers have been proposed. The EVS\nenables high-speed, low-latency, and robust communication due to its\nasynchronous operation and high dynamic range. In existing event-based OCC\nsystems, conventional modulation schemes such as on-off keying (OOK) and pulse\nposition modulation have been applied, however, to the best of our knowledge,\nno modulation method has been proposed that fully exploits the unique\ncharacteristics of the EVS. This paper proposes a novel modulation scheme,\ncalled the event interval modulation (EIM) scheme, specifically designed for\nevent-based OCC. EIM enables improvement in transmission speed by modulating\ninformation using the intervals between events. This paper proposes a\ntheoretical model of EIM and conducts a proof-of-concept experiment. First, the\nparameters of the EVS are tuned and customized to optimize the frequency\nresponse specifically for EIM. Then, the maximum modulation order usable in EIM\nis determined experimentally. We conduct transmission experiments based on the\nobtained parameters. Finally, we report successful transmission at 28 kbps over\n10 meters and 8.4 kbps over 50 meters in an indoor environment. This sets a new\nbenchmark for bit rate in event-based OCC systems.",
            "headline_zh": "提出事件间隔调制方案以提升事件型光学相机通信的传输速度",
            "intro_zh": [
                "传统光学相机通信系统存在低比特率和高处理负载问题",
                "新方案通过调制事件间隔来编码信息，优化事件视觉传感器参数",
                "实验实现室内28 kbps/10米和8.4 kbps/50米传输，创比特率新纪录"
            ],
            "tags_zh": [
                "事件间隔调制",
                "光学相机通信",
                "事件视觉传感器",
                "比特率提升",
                "室内传输实验"
            ],
            "_index": 128
        },
        {
            "title": "Reinforcement Learning for Unsupervised Domain Adaptation in Spatio-Temporal Echocardiography Segmentation",
            "authors": [
                "Arnaud Judge",
                "Nicolas Duchateau",
                "Thierry Judge",
                "Roman A. Sandler",
                "Joseph Z. Sokol",
                "Christian Desrosiers",
                "Olivier Bernard",
                "Pierre-Marc Jodoin"
            ],
            "arxiv_id": "2510.14244v1",
            "summary": "Domain adaptation methods aim to bridge the gap between datasets by enabling\nknowledge transfer across domains, reducing the need for additional expert\nannotations. However, many approaches struggle with reliability in the target\ndomain, an issue particularly critical in medical image segmentation, where\naccuracy and anatomical validity are essential. This challenge is further\nexacerbated in spatio-temporal data, where the lack of temporal consistency can\nsignificantly degrade segmentation quality, and particularly in\nechocardiography, where the presence of artifacts and noise can further hinder\nsegmentation performance. To address these issues, we present RL4Seg3D, an\nunsupervised domain adaptation framework for 2D + time echocardiography\nsegmentation. RL4Seg3D integrates novel reward functions and a fusion scheme to\nenhance key landmark precision in its segmentations while processing full-sized\ninput videos. By leveraging reinforcement learning for image segmentation, our\napproach improves accuracy, anatomical validity, and temporal consistency while\nalso providing, as a beneficial side effect, a robust uncertainty estimator,\nwhich can be used at test time to further enhance segmentation performance. We\ndemonstrate the effectiveness of our framework on over 30,000 echocardiographic\nvideos, showing that it outperforms standard domain adaptation techniques\nwithout the need for any labels on the target domain. Code is available at\nhttps://github.com/arnaudjudge/RL4Seg3D.",
            "headline_zh": "提出RL4Seg3D框架，用于无监督领域自适应的心动图时空分割，提升精度与一致性。",
            "intro_zh": [
                "核心问题：心动图分割中领域差异导致精度低、时空不一致，且噪声干扰严重。",
                "方法要点：结合强化学习、新奖励函数和融合方案，增强关键地标精度与不确定性估计。",
                "实验或效果：在3万+心动图视频上验证，优于标准方法，无需目标域标签。"
            ],
            "tags_zh": [
                "无监督领域自适应",
                "心动图分割",
                "强化学习",
                "时空一致性",
                "不确定性估计"
            ],
            "_index": 129
        },
        {
            "title": "PIA: Deepfake Detection Using Phoneme-Temporal and Identity-Dynamic Analysis",
            "authors": [
                "Soumyya Kanti Datta",
                "Tanvi Ranga",
                "Chengzhe Sun",
                "Siwei Lyu"
            ],
            "arxiv_id": "2510.14241v1",
            "summary": "The rise of manipulated media has made deepfakes a particularly insidious\nthreat, involving various generative manipulations such as lip-sync\nmodifications, face-swaps, and avatar-driven facial synthesis. Conventional\ndetection methods, which predominantly depend on manually designed\nphoneme-viseme alignment thresholds, fundamental frame-level consistency\nchecks, or a unimodal detection strategy, inadequately identify modern-day\ndeepfakes generated by advanced generative models such as GANs, diffusion\nmodels, and neural rendering techniques. These advanced techniques generate\nnearly perfect individual frames yet inadvertently create minor temporal\ndiscrepancies frequently overlooked by traditional detectors. We present a\nnovel multimodal audio-visual framework, Phoneme-Temporal and Identity-Dynamic\nAnalysis(PIA), incorporating language, dynamic face motion, and facial\nidentification cues to address these limitations. We utilize phoneme sequences,\nlip geometry data, and advanced facial identity embeddings. This integrated\nmethod significantly improves the detection of subtle deepfake alterations by\nidentifying inconsistencies across multiple complementary modalities. Code is\navailable at https://github.com/skrantidatta/PIA",
            "headline_zh": "提出PIA框架，通过音素-时间与身份-动态分析检测高级深度伪造",
            "intro_zh": [
                "核心问题：传统方法难以检测基于GAN和扩散模型生成的深度伪造，因忽略细微时间不一致性。",
                "方法要点：结合音素序列、唇部几何和面部身份嵌入，多模态分析音频-视觉不一致。",
                "实验或效果：集成方法显著提升检测精度，代码已开源供验证。"
            ],
            "tags_zh": [
                "深度伪造检测",
                "多模态分析",
                "音素-时间分析",
                "身份-动态分析",
                "音频-视觉不一致"
            ],
            "_index": 130
        },
        {
            "title": "Prescribed Performance Control of Deformable Object Manipulation in Spatial Latent Space",
            "authors": [
                "Ning Han",
                "Gu Gong",
                "Bin Zhang",
                "Yuexuan Xu",
                "Bohan Yang",
                "Yunhui Liu",
                "David Navarro-Alarcon"
            ],
            "arxiv_id": "2510.14234v1",
            "summary": "Manipulating three-dimensional (3D) deformable objects presents significant\nchallenges for robotic systems due to their infinite-dimensional state space\nand complex deformable dynamics. This paper proposes a novel model-free\napproach for shape control with constraints imposed on key points. Unlike\nexisting methods that rely on feature dimensionality reduction, the proposed\ncontroller leverages the coordinates of key points as the feature vector, which\nare extracted from the deformable object's point cloud using deep learning\nmethods. This approach not only reduces the dimensionality of the feature space\nbut also retains the spatial information of the object. By extracting key\npoints, the manipulation of deformable objects is simplified into a visual\nservoing problem, where the shape dynamics are described using a deformation\nJacobian matrix. To enhance control accuracy, a prescribed performance control\nmethod is developed by integrating barrier Lyapunov functions (BLF) to enforce\nconstraints on the key points. The stability of the closed-loop system is\nrigorously analyzed and verified using the Lyapunov method. Experimental\nresults further demonstrate the effectiveness and robustness of the proposed\nmethod.",
            "headline_zh": "提出基于关键点与规定性能控制的模型无关方法，用于三维可变形物体形状控制。",
            "intro_zh": [
                "核心问题：三维可变形物体操纵因无限维状态空间和复杂变形动力学而具挑战性。",
                "方法要点：使用关键点坐标作为特征向量，结合变形雅可比矩阵和障碍李雅普诺夫函数实现规定性能控制。",
                "实验或效果：实验验证了方法的有效性和鲁棒性，闭环系统稳定性通过李雅普诺夫方法分析。"
            ],
            "tags_zh": [
                "可变形物体操纵",
                "规定性能控制",
                "关键点提取",
                "视觉伺服",
                "障碍李雅普诺夫函数",
                "变形雅可比矩阵"
            ],
            "_index": 131
        },
        {
            "title": "LOTA: Bit-Planes Guided AI-Generated Image Detection",
            "authors": [
                "Hongsong Wang",
                "Renxi Cheng",
                "Yang Zhang",
                "Chaolei Han",
                "Jie Gui"
            ],
            "arxiv_id": "2510.14230v1",
            "summary": "The rapid advancement of GAN and Diffusion models makes it more difficult to\ndistinguish AI-generated images from real ones. Recent studies often use\nimage-based reconstruction errors as an important feature for determining\nwhether an image is AI-generated. However, these approaches typically incur\nhigh computational costs and also fail to capture intrinsic noisy features\npresent in the raw images. To solve these problems, we innovatively refine\nerror extraction by using bit-plane-based image processing, as lower bit planes\nindeed represent noise patterns in images. We introduce an effective bit-planes\nguided noisy image generation and exploit various image normalization\nstrategies, including scaling and thresholding. Then, to amplify the noise\nsignal for easier AI-generated image detection, we design a maximum gradient\npatch selection that applies multi-directional gradients to compute the noise\nscore and selects the region with the highest score. Finally, we propose a\nlightweight and effective classification head and explore two different\nstructures: noise-based classifier and noise-guided classifier. Extensive\nexperiments on the GenImage benchmark demonstrate the outstanding performance\nof our method, which achieves an average accuracy of \\textbf{98.9\\%}\n(\\textbf{11.9}\\%~$\\uparrow$) and shows excellent cross-generator generalization\ncapability. Particularly, our method achieves an accuracy of over 98.2\\% from\nGAN to Diffusion and over 99.2\\% from Diffusion to GAN. Moreover, it performs\nerror extraction at the millisecond level, nearly a hundred times faster than\nexisting methods. The code is at https://github.com/hongsong-wang/LOTA.",
            "headline_zh": "提出基于位平面引导的噪声提取方法以高效检测AI生成图像",
            "intro_zh": [
                "核心问题：现有方法计算成本高且难以捕捉原始图像中的固有噪声特征",
                "方法要点：利用位平面处理提取噪声，设计最大梯度补丁选择以放大噪声信号",
                "实验或效果：在GenImage基准上平均准确率达98.9%，检测速度提升近百倍"
            ],
            "tags_zh": [
                "AI生成图像检测",
                "位平面处理",
                "噪声提取",
                "梯度补丁选择",
                "轻量分类头"
            ],
            "_index": 132
        },
        {
            "title": "Joint Modeling of Big Five and HEXACO for Multimodal Apparent Personality-trait Recognition",
            "authors": [
                "Ryo Masumura",
                "Shota Orihashi",
                "Mana Ihori",
                "Tomohiro Tanaka",
                "Naoki Makishima",
                "Taiga Yamane",
                "Naotaka Kawata",
                "Satoshi Suzuki",
                "Taichi Katayama"
            ],
            "arxiv_id": "2510.14203v1",
            "summary": "This paper proposes a joint modeling method of the Big Five, which has long\nbeen studied, and HEXACO, which has recently attracted attention in psychology,\nfor automatically recognizing apparent personality traits from multimodal human\nbehavior. Most previous studies have used the Big Five for multimodal apparent\npersonality-trait recognition. However, no study has focused on apparent HEXACO\nwhich can evaluate an Honesty-Humility trait related to displaced aggression\nand vengefulness, social-dominance orientation, etc. In addition, the\nrelationships between the Big Five and HEXACO when modeled by machine learning\nhave not been clarified. We expect awareness of multimodal human behavior to\nimprove by considering these relationships. The key advance of our proposed\nmethod is to optimize jointly recognizing the Big Five and HEXACO. Experiments\nusing a self-introduction video dataset demonstrate that the proposed method\ncan effectively recognize the Big Five and HEXACO.",
            "headline_zh": "提出联合建模Big Five和HEXACO的方法，用于多模态人类行为中的外显人格特质识别。",
            "intro_zh": [
                "核心问题：现有研究未关注HEXACO外显特质及其与Big Five的关系。",
                "方法要点：通过联合优化同时识别Big Five和HEXACO人格特质。",
                "实验或效果：在自我介绍视频数据集上验证方法有效识别两种人格特质。"
            ],
            "tags_zh": [
                "多模态人格识别",
                "Big Five模型",
                "HEXACO模型",
                "联合建模",
                "视频行为分析"
            ],
            "_index": 133
        },
        {
            "title": "Virtually Being: Customizing Camera-Controllable Video Diffusion Models with Multi-View Performance Captures",
            "authors": [
                "Yuancheng Xu",
                "Wenqi Xian",
                "Li Ma",
                "Julien Philip",
                "Ahmet Levent Taşel",
                "Yiwei Zhao",
                "Ryan Burgert",
                "Mingming He",
                "Oliver Hermann",
                "Oliver Pilarski",
                "Rahul Garg",
                "Paul Debevec",
                "Ning Yu"
            ],
            "arxiv_id": "2510.14179v1",
            "summary": "We introduce a framework that enables both multi-view character consistency\nand 3D camera control in video diffusion models through a novel customization\ndata pipeline. We train the character consistency component with recorded\nvolumetric capture performances re-rendered with diverse camera trajectories\nvia 4D Gaussian Splatting (4DGS), lighting variability obtained with a video\nrelighting model. We fine-tune state-of-the-art open-source video diffusion\nmodels on this data to provide strong multi-view identity preservation, precise\ncamera control, and lighting adaptability. Our framework also supports core\ncapabilities for virtual production, including multi-subject generation using\ntwo approaches: joint training and noise blending, the latter enabling\nefficient composition of independently customized models at inference time; it\nalso achieves scene and real-life video customization as well as control over\nmotion and spatial layout during customization. Extensive experiments show\nimproved video quality, higher personalization accuracy, and enhanced camera\ncontrol and lighting adaptability, advancing the integration of video\ngeneration into virtual production. Our project page is available at:\nhttps://eyeline-labs.github.io/Virtually-Being.",
            "headline_zh": "提出定制化视频扩散模型框架，实现多视角角色一致性和3D相机控制。",
            "intro_zh": [
                "核心问题：视频扩散模型在多视角角色一致性和3D相机控制方面存在不足。",
                "方法要点：使用4D高斯溅射和视频重光照模型构建定制数据管道进行微调。",
                "实验或效果：实验显示视频质量、个性化精度和相机控制能力得到提升。"
            ],
            "tags_zh": [
                "视频扩散模型",
                "多视角一致性",
                "3D相机控制",
                "4D高斯溅射",
                "虚拟生产"
            ],
            "_index": 134
        }
    ]
}