{
  "count": 118,
  "papers": [
    {
      "title": "ConsistEdit: Highly Consistent and Precise Training-free Visual Editing",
      "authors": [
        "Zixin Yin",
        "Ling-Hao Chen",
        "Lionel Ni",
        "Xili Dai"
      ],
      "arxiv_id": "2510.17803v1",
      "summary": "Recent advances in training-free attention control methods have enabled\nflexible and efficient text-guided editing capabilities for existing generation\nmodels. However, current approaches struggle to simultaneously deliver strong\nediting strength while preserving consistency with the source. This limitation\nbecomes particularly critical in multi-round and video editing, where visual\nerrors can accumulate over time. Moreover, most existing methods enforce global\nconsistency, which limits their ability to modify individual attributes such as\ntexture while preserving others, thereby hindering fine-grained editing.\nRecently, the architectural shift from U-Net to MM-DiT has brought significant\nimprovements in generative performance and introduced a novel mechanism for\nintegrating text and vision modalities. These advancements pave the way for\novercoming challenges that previous methods failed to resolve. Through an\nin-depth analysis of MM-DiT, we identify three key insights into its attention\nmechanisms. Building on these, we propose ConsistEdit, a novel attention\ncontrol method specifically tailored for MM-DiT. ConsistEdit incorporates\nvision-only attention control, mask-guided pre-attention fusion, and\ndifferentiated manipulation of the query, key, and value tokens to produce\nconsistent, prompt-aligned edits. Extensive experiments demonstrate that\nConsistEdit achieves state-of-the-art performance across a wide range of image\nand video editing tasks, including both structure-consistent and\nstructure-inconsistent scenarios. Unlike prior methods, it is the first\napproach to perform editing across all inference steps and attention layers\nwithout handcraft, significantly enhancing reliability and consistency, which\nenables robust multi-round and multi-region editing. Furthermore, it supports\nprogressive adjustment of structural consistency, enabling finer control."
    },
    {
      "title": "Robobench: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models as Embodied Brain",
      "authors": [
        "Yulin Luo",
        "Chun-Kai Fan",
        "Menghang Dong",
        "Jiayu Shi",
        "Mengdi Zhao",
        "Bo-Wen Zhang",
        "Cheng Chi",
        "Jiaming Liu",
        "Gaole Dai",
        "Rongyu Zhang",
        "Ruichuan An",
        "Kun Wu",
        "Zhengping Che",
        "Shaoxuan Xie",
        "Guocai Yao",
        "Zhongxia Zhao",
        "Pengwei Wang",
        "Guang Liu",
        "Zhongyuan Wang",
        "Tiejun Huang",
        "Shanghang Zhang"
      ],
      "arxiv_id": "2510.17801v1",
      "summary": "Building robots that can perceive, reason, and act in dynamic, unstructured\nenvironments remains a core challenge. Recent embodied systems often adopt a\ndual-system paradigm, where System 2 handles high-level reasoning while System\n1 executes low-level control. In this work, we refer to System 2 as the\nembodied brain, emphasizing its role as the cognitive core for reasoning and\ndecision-making in manipulation tasks. Given this role, systematic evaluation\nof the embodied brain is essential. Yet existing benchmarks emphasize execution\nsuccess, or when targeting high-level reasoning, suffer from incomplete\ndimensions and limited task realism, offering only a partial picture of\ncognitive capability. To bridge this gap, we introduce RoboBench, a benchmark\nthat systematically evaluates multimodal large language models (MLLMs) as\nembodied brains. Motivated by the critical roles across the full manipulation\npipeline, RoboBench defines five dimensions-instruction comprehension,\nperception reasoning, generalized planning, affordance prediction, and failure\nanalysis-spanning 14 capabilities, 25 tasks, and 6092 QA pairs. To ensure\nrealism, we curate datasets across diverse embodiments, attribute-rich objects,\nand multi-view scenes, drawing from large-scale real robotic data. For\nplanning, RoboBench introduces an evaluation framework,\nMLLM-as-world-simulator. It evaluate embodied feasibility by simulating whether\npredicted plans can achieve critical object-state changes. Experiments on 14\nMLLMs reveal fundamental limitations: difficulties with implicit instruction\ncomprehension, spatiotemporal reasoning, cross-scenario planning, fine-grained\naffordance understanding, and execution failure diagnosis. RoboBench provides a\ncomprehensive scaffold to quantify high-level cognition, and guide the\ndevelopment of next-generation embodied MLLMs. The project page is in\nhttps://robo-bench.github.io."
    },
    {
      "title": "Glyph: Scaling Context Windows via Visual-Text Compression",
      "authors": [
        "Jiale Cheng",
        "Yusen Liu",
        "Xinyu Zhang",
        "Yulin Fei",
        "Wenyi Hong",
        "Ruiliang Lyu",
        "Weihan Wang",
        "Zhe Su",
        "Xiaotao Gu",
        "Xiao Liu",
        "Yushi Bai",
        "Jie Tang",
        "Hongning Wang",
        "Minlie Huang"
      ],
      "arxiv_id": "2510.17800v1",
      "summary": "Large language models (LLMs) increasingly rely on long-context modeling for\ntasks such as document understanding, code analysis, and multi-step reasoning.\nHowever, scaling context windows to the million-token level brings prohibitive\ncomputational and memory costs, limiting the practicality of long-context LLMs.\nIn this work, we take a different perspective-visual context scaling-to tackle\nthis challenge. Instead of extending token-based sequences, we propose Glyph, a\nframework that renders long texts into images and processes them with\nvision-language models (VLMs). This approach substantially compresses textual\ninput while preserving semantic information, and we further design an\nLLM-driven genetic search to identify optimal visual rendering configurations\nfor balancing accuracy and compression. Through extensive experiments, we\ndemonstrate that our method achieves 3-4x token compression while maintaining\naccuracy comparable to leading LLMs such as Qwen3-8B on various long-context\nbenchmarks. This compression also leads to around 4x faster prefilling and\ndecoding, and approximately 2x faster SFT training. Furthermore, under extreme\ncompression, a 128K-context VLM could scale to handle 1M-token-level text\ntasks. In addition, the rendered text data benefits real-world multimodal\ntasks, such as document understanding. Our code and model are released at\nhttps://github.com/thu-coai/Glyph."
    },
    {
      "title": "SoftMimic: Learning Compliant Whole-body Control from Examples",
      "authors": [
        "Gabriel B. Margolis",
        "Michelle Wang",
        "Nolan Fey",
        "Pulkit Agrawal"
      ],
      "arxiv_id": "2510.17792v1",
      "summary": "We introduce SoftMimic, a framework for learning compliant whole-body control\npolicies for humanoid robots from example motions. Imitating human motions with\nreinforcement learning allows humanoids to quickly learn new skills, but\nexisting methods incentivize stiff control that aggressively corrects\ndeviations from a reference motion, leading to brittle and unsafe behavior when\nthe robot encounters unexpected contacts. In contrast, SoftMimic enables robots\nto respond compliantly to external forces while maintaining balance and\nposture. Our approach leverages an inverse kinematics solver to generate an\naugmented dataset of feasible compliant motions, which we use to train a\nreinforcement learning policy. By rewarding the policy for matching compliant\nresponses rather than rigidly tracking the reference motion, SoftMimic learns\nto absorb disturbances and generalize to varied tasks from a single motion\nclip. We validate our method through simulations and real-world experiments,\ndemonstrating safe and effective interaction with the environment."
    },
    {
      "title": "UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action",
      "authors": [
        "Yuhao Yang",
        "Zhen Yang",
        "Zi-Yi Dou",
        "Anh Nguyen",
        "Keen You",
        "Omar Attia",
        "Andrew Szot",
        "Michael Feng",
        "Ram Ramrakhya",
        "Alexander Toshev",
        "Chao Huang",
        "Yinfei Yang",
        "Zhe Gan"
      ],
      "arxiv_id": "2510.17790v1",
      "summary": "Multimodal agents for computer use rely exclusively on primitive actions\n(click, type, scroll) that require accurate visual grounding and lengthy\nexecution chains, leading to cascading failures and performance bottlenecks.\nWhile other agents leverage rich programmatic interfaces (APIs, MCP servers,\ntools), computer-use agents (CUAs) remain isolated from these capabilities. We\npresent UltraCUA, a foundation model that bridges this gap through hybrid\naction -- seamlessly integrating GUI primitives with high-level programmatic\ntool calls. To achieve this, our approach comprises four key components: (1) an\nautomated pipeline that scales programmatic tools from software documentation,\nopen-source repositories, and code generation; (2) a synthetic data engine\nproducing over 17,000 verifiable tasks spanning real-world computer-use\nscenarios; (3) a large-scale high-quality hybrid action trajectory collection\nwith both low-level GUI actions and high-level programmatic tool calls; and (4)\na two-stage training pipeline combining supervised fine-tuning with online\nreinforcement learning, enabling strategic alternation between low-level and\nhigh-level actions. Experiments with our 7B and 32B models demonstrate\nsubstantial improvements over state-of-the-art agents. On OSWorld, UltraCUA\nmodels achieve an average 22% relative improvement over base models, while\nbeing 11% faster in terms of steps. Out-of-domain evaluation on\nWindowsAgentArena shows our model reaches 21.7% success rate, outperforming\nbaselines trained on Windows data. The hybrid action mechanism proves critical,\nreducing error propagation while maintaining execution efficiency."
    },
    {
      "title": "Botany-Bot: Digital Twin Monitoring of Occluded and Underleaf Plant Structures with Gaussian Splats",
      "authors": [
        "Simeon Adebola",
        "Chung Min Kim",
        "Justin Kerr",
        "Shuangyu Xie",
        "Prithvi Akella",
        "Jose Luis Susa Rincon",
        "Eugen Solowjow",
        "Ken Goldberg"
      ],
      "arxiv_id": "2510.17783v1",
      "summary": "Commercial plant phenotyping systems using fixed cameras cannot perceive many\nplant details due to leaf occlusion. In this paper, we present Botany-Bot, a\nsystem for building detailed \"annotated digital twins\" of living plants using\ntwo stereo cameras, a digital turntable inside a lightbox, an industrial robot\narm, and 3D segmentated Gaussian Splat models. We also present robot algorithms\nfor manipulating leaves to take high-resolution indexable images of occluded\ndetails such as stem buds and the underside/topside of leaves. Results from\nexperiments suggest that Botany-Bot can segment leaves with 90.8% accuracy,\ndetect leaves with 86.2% accuracy, lift/push leaves with 77.9% accuracy, and\ntake detailed overside/underside images with 77.3% accuracy. Code, videos, and\ndatasets are available at https://berkeleyautomation.github.io/Botany-Bot/."
    },
    {
      "title": "SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference",
      "authors": [
        "Samir Khaki",
        "Junxian Guo",
        "Jiaming Tang",
        "Shang Yang",
        "Yukang Chen",
        "Konstantinos N. Plataniotis",
        "Yao Lu",
        "Song Han",
        "Zhijian Liu"
      ],
      "arxiv_id": "2510.17777v1",
      "summary": "Vision Language Models (VLMs) have rapidly advanced in integrating visual and\ntextual reasoning, powering applications across high-resolution image\nunderstanding, long-video analysis, and multi-turn conversation. However, their\nscalability remains limited by the growing number of visual tokens that\ndominate inference latency. We present SparseVILA, a new paradigm for efficient\nVLM inference that decouples visual sparsity across the prefilling and decoding\nstages. SparseVILA distributes sparsity across stages by pruning redundant\nvisual tokens during prefill and retrieving only query-relevant tokens during\ndecoding. This decoupled design matches leading prefill pruning methods while\npreserving multi-turn fidelity by retaining most of the visual cache so that\nquery-aware tokens can be retrieved at each conversation round. Built on an\nAWQ-optimized inference pipeline, SparseVILA achieves up to 4.0 times faster\nprefilling, 2.5 times faster decoding, and an overall 2.6 times end-to-end\nspeedup on long-context video tasks -- while improving accuracy on\ndocument-understanding and reasoning tasks. By decoupling query-agnostic\npruning and query-aware retrieval, SparseVILA establishes a new direction for\nefficient multimodal inference, offering a training-free, architecture-agnostic\nframework for accelerating large VLMs without sacrificing capability."
    },
    {
      "title": "Towards Explainable Skin Cancer Classification: A Dual-Network Attention Model with Lesion Segmentation and Clinical Metadata Fusion",
      "authors": [
        "Md. Enamul Atiq",
        "Shaikh Anowarul Fattah"
      ],
      "arxiv_id": "2510.17773v1",
      "summary": "Skin cancer is a life-threatening disease where early detection significantly\nimproves patient outcomes. Automated diagnosis from dermoscopic images is\nchallenging due to high intra-class variability and subtle inter-class\ndifferences. Many deep learning models operate as \"black boxes,\" limiting\nclinical trust. In this work, we propose a dual-encoder attention-based\nframework that leverages both segmented lesions and clinical metadata to\nenhance skin lesion classification in terms of both accuracy and\ninterpretability. A novel Deep-UNet architecture with Dual Attention Gates\n(DAG) and Atrous Spatial Pyramid Pooling (ASPP) is first employed to segment\nlesions. The classification stage uses two DenseNet201 encoders-one on the\noriginal image and another on the segmented lesion whose features are fused via\nmulti-head cross-attention. This dual-input design guides the model to focus on\nsalient pathological regions. In addition, a transformer-based module\nincorporates patient metadata (age, sex, lesion site) into the prediction. We\nevaluate our approach on the HAM10000 dataset and the ISIC 2018 and 2019\nchallenges. The proposed method achieves state-of-the-art segmentation\nperformance and significantly improves classification accuracy and average AUC\ncompared to baseline models. To validate our model's reliability, we use\nGradient-weighted Class Activation Mapping (Grad-CAM) to generate heatmaps.\nThese visualizations confirm that our model's predictions are based on the\nlesion area, unlike models that rely on spurious background features. These\nresults demonstrate that integrating precise lesion segmentation and clinical\ndata with attention-based fusion leads to a more accurate and interpretable\nskin cancer classification model."
    },
    {
      "title": "Seeing but Not Believing: Probing the Disconnect Between Visual Attention and Answer Correctness in VLMs",
      "authors": [
        "Zhining Liu",
        "Ziyi Chen",
        "Hui Liu",
        "Chen Luo",
        "Xianfeng Tang",
        "Suhang Wang",
        "Joy Zeng",
        "Zhenwei Dai",
        "Zhan Shi",
        "Tianxin Wei",
        "Benoit Dumoulin",
        "Hanghang Tong"
      ],
      "arxiv_id": "2510.17771v1",
      "summary": "Vision-Language Models (VLMs) achieve strong results on multimodal tasks such\nas visual question answering, yet they can still fail even when the correct\nvisual evidence is present. In this work, we systematically investigate whether\nthese failures arise from not perceiving the evidence or from not leveraging it\neffectively. By examining layer-wise attention dynamics, we find that shallow\nlayers focus primarily on text, while deeper layers sparsely but reliably\nattend to localized evidence regions. Surprisingly, VLMs often perceive the\nvisual evidence when outputting incorrect answers, a phenomenon we term\n``seeing but not believing'' that widely exists in major VLM families. Building\non this, we introduce an inference-time intervention that highlights deep-layer\nevidence regions through selective attention-based masking. It requires no\ntraining and consistently improves accuracy across multiple families, including\nLLaVA, Qwen, Gemma, and InternVL. These results show that VLMs encode reliable\nevidence internally but under-utilize it, making such signals explicit can\nbridge the gap between perception and reasoning, advancing the diagnostic\nunderstanding and reliability of VLMs."
    },
    {
      "title": "VERA-V: Variational Inference Framework for Jailbreaking Vision-Language Models",
      "authors": [
        "Qilin Liao",
        "Anamika Lochab",
        "Ruqi Zhang"
      ],
      "arxiv_id": "2510.17759v1",
      "summary": "Vision-Language Models (VLMs) extend large language models with visual\nreasoning, but their multimodal design also introduces new, underexplored\nvulnerabilities. Existing multimodal red-teaming methods largely rely on\nbrittle templates, focus on single-attack settings, and expose only a narrow\nsubset of vulnerabilities. To address these limitations, we introduce VERA-V, a\nvariational inference framework that recasts multimodal jailbreak discovery as\nlearning a joint posterior distribution over paired text-image prompts. This\nprobabilistic view enables the generation of stealthy, coupled adversarial\ninputs that bypass model guardrails. We train a lightweight attacker to\napproximate the posterior, allowing efficient sampling of diverse jailbreaks\nand providing distributional insights into vulnerabilities. VERA-V further\nintegrates three complementary strategies: (i) typography-based text prompts\nthat embed harmful cues, (ii) diffusion-based image synthesis that introduces\nadversarial signals, and (iii) structured distractors to fragment VLM\nattention. Experiments on HarmBench and HADES benchmarks show that VERA-V\nconsistently outperforms state-of-the-art baselines on both open-source and\nfrontier VLMs, achieving up to 53.75% higher attack success rate (ASR) over the\nbest baseline on GPT-4o."
    },
    {
      "title": "Joint Multi-Condition Representation Modelling via Matrix Factorisation for Visual Place Recognition",
      "authors": [
        "Timur Ismagilov",
        "Shakaiba Majeed",
        "Michael Milford",
        "Tan Viet Tuyen Nguyen",
        "Sarvapali D. Ramchurn",
        "Shoaib Ehsan"
      ],
      "arxiv_id": "2510.17739v1",
      "summary": "We address multi-reference visual place recognition (VPR), where reference\nsets captured under varying conditions are used to improve localisation\nperformance. While deep learning with large-scale training improves robustness,\nincreasing data diversity and model complexity incur extensive computational\ncost during training and deployment. Descriptor-level fusion via voting or\naggregation avoids training, but often targets multi-sensor setups or relies on\nheuristics with limited gains under appearance and viewpoint change. We propose\na training-free, descriptor-agnostic approach that jointly models places using\nmultiple reference descriptors via matrix decomposition into basis\nrepresentations, enabling projection-based residual matching. We also introduce\nSotonMV, a structured benchmark for multi-viewpoint VPR. On multi-appearance\ndata, our method improves Recall@1 by up to ~18% over single-reference and\noutperforms multi-reference baselines across appearance and viewpoint changes,\nwith gains of ~5% on unstructured data, demonstrating strong generalisation\nwhile remaining lightweight."
    },
    {
      "title": "Can Image-To-Video Models Simulate Pedestrian Dynamics?",
      "authors": [
        "Aaron Appelle",
        "Jerome P. Lynch"
      ],
      "arxiv_id": "2510.17731v1",
      "summary": "Recent high-performing image-to-video (I2V) models based on variants of the\ndiffusion transformer (DiT) have displayed remarkable inherent world-modeling\ncapabilities by virtue of training on large scale video datasets. We\ninvestigate whether these models can generate realistic pedestrian movement\npatterns in crowded public scenes. Our framework conditions I2V models on\nkeyframes extracted from pedestrian trajectory benchmarks, then evaluates their\ntrajectory prediction performance using quantitative measures of pedestrian\ndynamics."
    },
    {
      "title": "Signature Forgery Detection: Improving Cross-Dataset Generalization",
      "authors": [
        "Matheus Ramos Parracho"
      ],
      "arxiv_id": "2510.17724v1",
      "summary": "Automated signature verification is a critical biometric technique used in\nbanking, identity authentication, and legal documentation. Despite the notable\nprogress achieved by deep learning methods, most approaches in offline\nsignature verification still struggle to generalize across datasets, as\nvariations in handwriting styles and acquisition protocols often degrade\nperformance. This study investigates feature learning strategies for signature\nforgery detection, focusing on improving cross-dataset generalization -- that\nis, model robustness when trained on one dataset and tested on another. Using\nthree public benchmarks -- CEDAR, ICDAR, and GPDS Synthetic -- two experimental\npipelines were developed: one based on raw signature images and another\nemploying a preprocessing method referred to as shell preprocessing. Several\nbehavioral patterns were identified and analyzed; however, no definitive\nsuperiority between the two approaches was established. The results show that\nthe raw-image model achieved higher performance across benchmarks, while the\nshell-based model demonstrated promising potential for future refinement toward\nrobust, cross-domain signature verification."
    },
    {
      "title": "MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating Multimodal LLMs in Multi-Turn Dialogues",
      "authors": [
        "Yaning Pan",
        "Zekun Wang",
        "Qianqian Xie",
        "Yongqian Wen",
        "Yuanxing Zhang",
        "Guohui Zhang",
        "Haoxuan Hu",
        "Zhiyu Pan",
        "Yibing Huang",
        "Zhidong Gan",
        "Yonghong Lin",
        "An Ping",
        "Tianhao Peng",
        "Jiaheng Liu"
      ],
      "arxiv_id": "2510.17722v1",
      "summary": "The recent development of Multimodal Large Language Models (MLLMs) has\nsignificantly advanced AI's ability to understand visual modalities. However,\nexisting evaluation benchmarks remain limited to single-turn question\nanswering, overlooking the complexity of multi-turn dialogues in real-world\nscenarios. To bridge this gap, we introduce MT-Video-Bench, a holistic video\nunderstanding benchmark for evaluating MLLMs in multi-turn dialogues.\nSpecifically, our MT-Video-Bench mainly assesses six core competencies that\nfocus on perceptivity and interactivity, encompassing 987 meticulously curated\nmulti-turn dialogues from diverse domains. These capabilities are rigorously\naligned with real-world applications, such as interactive sports analysis and\nmulti-turn video-based intelligent tutoring. With MT-Video-Bench, we\nextensively evaluate various state-of-the-art open-source and closed-source\nMLLMs, revealing their significant performance discrepancies and limitations in\nhandling multi-turn video dialogues. The benchmark will be publicly available\nto foster future research."
    },
    {
      "title": "Raindrop GS: A Benchmark for 3D Gaussian Splatting under Raindrop Conditions",
      "authors": [
        "Zhiqiang Teng",
        "Beibei Lin",
        "Tingting Chen",
        "Zifeng Yuan",
        "Xuanyi Li",
        "Xuanyu Zhang",
        "Shunli Zhang"
      ],
      "arxiv_id": "2510.17719v1",
      "summary": "3D Gaussian Splatting (3DGS) under raindrop conditions suffers from severe\nocclusions and optical distortions caused by raindrop contamination on the\ncamera lens, substantially degrading reconstruction quality. Existing\nbenchmarks typically evaluate 3DGS using synthetic raindrop images with known\ncamera poses (constrained images), assuming ideal conditions. However, in\nreal-world scenarios, raindrops often interfere with accurate camera pose\nestimation and point cloud initialization. Moreover, a significant domain gap\nbetween synthetic and real raindrops further impairs generalization. To tackle\nthese issues, we introduce RaindropGS, a comprehensive benchmark designed to\nevaluate the full 3DGS pipeline-from unconstrained, raindrop-corrupted images\nto clear 3DGS reconstructions. Specifically, the whole benchmark pipeline\nconsists of three parts: data preparation, data processing, and raindrop-aware\n3DGS evaluation, including types of raindrop interference, camera pose\nestimation and point cloud initialization, single image rain removal\ncomparison, and 3D Gaussian training comparison. First, we collect a real-world\nraindrop reconstruction dataset, in which each scene contains three aligned\nimage sets: raindrop-focused, background-focused, and rain-free ground truth,\nenabling a comprehensive evaluation of reconstruction quality under different\nfocus conditions. Through comprehensive experiments and analyses, we reveal\ncritical insights into the performance limitations of existing 3DGS methods on\nunconstrained raindrop images and the varying impact of different pipeline\ncomponents: the impact of camera focus position on 3DGS reconstruction\nperformance, and the interference caused by inaccurate pose and point cloud\ninitialization on reconstruction. These insights establish clear directions for\ndeveloping more robust 3DGS methods under raindrop conditions."
    },
    {
      "title": "Automatic Classification of Circulating Blood Cell Clusters based on Multi-channel Flow Cytometry Imaging",
      "authors": [
        "Suqiang Ma",
        "Subhadeep Sengupta",
        "Yao Lee",
        "Beikang Gu",
        "Xianyan Chen",
        "Xianqiao Wang",
        "Yang Liu",
        "Mengjia Xu",
        "Galit H. Frydman",
        "He Li"
      ],
      "arxiv_id": "2510.17716v1",
      "summary": "Circulating blood cell clusters (CCCs) containing red blood cells (RBCs),\nwhite blood cells(WBCs), and platelets are significant biomarkers linked to\nconditions like thrombosis, infection, and inflammation. Flow cytometry, paired\nwith fluorescence staining, is commonly used to analyze these cell clusters,\nrevealing cell morphology and protein profiles. While computational approaches\nbased on machine learning have advanced the automatic analysis of single-cell\nflow cytometry images, there is a lack of effort to build tools to\nautomatically analyze images containing CCCs. Unlike single cells, cell\nclusters often exhibit irregular shapes and sizes. In addition, these cell\nclusters often consist of heterogeneous cell types, which require multi-channel\nstaining to identify the specific cell types within the clusters. This study\nintroduces a new computational framework for analyzing CCC images and\nidentifying cell types within clusters. Our framework uses a two-step analysis\nstrategy. First, it categorizes images into cell cluster and non-cluster groups\nby fine-tuning the You Only Look Once(YOLOv11) model, which outperforms\ntraditional convolutional neural networks (CNNs), Vision Transformers (ViT).\nThen, it identifies cell types by overlaying cluster contours with regions from\nmulti-channel fluorescence stains, enhancing accuracy despite cell debris and\nstaining artifacts. This approach achieved over 95% accuracy in both cluster\nclassification and phenotype identification. In summary, our automated\nframework effectively analyzes CCC images from flow cytometry, leveraging both\nbright-field and fluorescence data. Initially tested on blood cells, it holds\npotential for broader applications, such as analyzing immune and tumor cell\nclusters, supporting cellular research across various diseases."
    },
    {
      "title": "Improving Cross-Patient Generalization in Parkinson's Disease Detection through Chunk-Based Analysis of Hand-Drawn Patterns",
      "authors": [
        "Mhd Adnan Albani",
        "Riad Sonbol"
      ],
      "arxiv_id": "2510.17703v1",
      "summary": "Parkinson's disease (PD) is a neurodegenerative disease affecting about 1% of\npeople over the age of 60, causing motor impairments that impede hand\ncoordination activities such as writing and drawing. Many approaches have tried\nto support early detection of Parkinson's disease based on hand-drawn images;\nhowever, we identified two major limitations in the related works: (1) the lack\nof sufficient datasets, (2) the robustness when dealing with unseen patient\ndata. In this paper, we propose a new approach to detect Parkinson's disease\nthat consists of two stages: The first stage classifies based on their drawing\ntype(circle, meander, spiral), and the second stage extracts the required\nfeatures from the images and detects Parkinson's disease. We overcame the\nprevious two limitations by applying a chunking strategy where we divide each\nimage into 2x2 chunks. Each chunk is processed separately when extracting\nfeatures and recognizing Parkinson's disease indicators. To make the final\nclassification, an ensemble method is used to merge the decisions made from\neach chunk. Our evaluation shows that our proposed approach outperforms the top\nperforming state-of-the-art approaches, in particular on unseen patients. On\nthe NewHandPD dataset our approach, it achieved 97.08% accuracy for seen\npatients and 94.91% for unseen patients, our proposed approach maintained a gap\nof only 2.17 percentage points, compared to the 4.76-point drop observed in\nprior work."
    },
    {
      "title": "Elastic ViTs from Pretrained Models without Retraining",
      "authors": [
        "Walter Simoncini",
        "Michael Dorkenwald",
        "Tijmen Blankevoort",
        "Cees G. M. Snoek",
        "Yuki M. Asano"
      ],
      "arxiv_id": "2510.17700v1",
      "summary": "Vision foundation models achieve remarkable performance but are only\navailable in a limited set of pre-determined sizes, forcing sub-optimal\ndeployment choices under real-world constraints. We introduce SnapViT:\nSingle-shot network approximation for pruned Vision Transformers, a new\npost-pretraining structured pruning method that enables elastic inference\nacross a continuum of compute budgets. Our approach efficiently combines\ngradient information with cross-network structure correlations, approximated\nvia an evolutionary algorithm, does not require labeled data, generalizes to\nmodels without a classification head, and is retraining-free. Experiments on\nDINO, SigLIPv2, DeIT, and AugReg models demonstrate superior performance over\nstate-of-the-art methods across various sparsities, requiring less than five\nminutes on a single A100 GPU to generate elastic models that can be adjusted to\nany computational budget. Our key contributions include an efficient pruning\nstrategy for pretrained Vision Transformers, a novel evolutionary approximation\nof Hessian off-diagonal structures, and a self-supervised importance scoring\nmechanism that maintains strong performance without requiring retraining or\nlabels. Code and pruned models are available at: https://elastic.ashita.nl/"
    },
    {
      "title": "GAS: Improving Discretization of Diffusion ODEs via Generalized Adversarial Solver",
      "authors": [
        "Aleksandr Oganov",
        "Ilya Bykov",
        "Eva Neudachina",
        "Mishan Aliev",
        "Alexander Tolmachev",
        "Alexander Sidorov",
        "Aleksandr Zuev",
        "Andrey Okhotin",
        "Denis Rakitin",
        "Aibek Alanov"
      ],
      "arxiv_id": "2510.17699v1",
      "summary": "While diffusion models achieve state-of-the-art generation quality, they\nstill suffer from computationally expensive sampling. Recent works address this\nissue with gradient-based optimization methods that distill a few-step ODE\ndiffusion solver from the full sampling process, reducing the number of\nfunction evaluations from dozens to just a few. However, these approaches often\nrely on intricate training techniques and do not explicitly focus on preserving\nfine-grained details. In this paper, we introduce the Generalized Solver: a\nsimple parameterization of the ODE sampler that does not require additional\ntraining tricks and improves quality over existing approaches. We further\ncombine the original distillation loss with adversarial training, which\nmitigates artifacts and enhances detail fidelity. We call the resulting method\nthe Generalized Adversarial Solver and demonstrate its superior performance\ncompared to existing solver training methods under similar resource\nconstraints. Code is available at https://github.com/3145tttt/GAS."
    },
    {
      "title": "Towards 3D Objectness Learning in an Open World",
      "authors": [
        "Taichi Liu",
        "Zhenyu Wang",
        "Ruofeng Liu",
        "Guang Wang",
        "Desheng Zhang"
      ],
      "arxiv_id": "2510.17686v1",
      "summary": "Recent advancements in 3D object detection and novel category detection have\nmade significant progress, yet research on learning generalized 3D objectness\nremains insufficient. In this paper, we delve into learning open-world 3D\nobjectness, which focuses on detecting all objects in a 3D scene, including\nnovel objects unseen during training. Traditional closed-set 3D detectors\nstruggle to generalize to open-world scenarios, while directly incorporating 3D\nopen-vocabulary models for open-world ability struggles with vocabulary\nexpansion and semantic overlap. To achieve generalized 3D object discovery, We\npropose OP3Det, a class-agnostic Open-World Prompt-free 3D Detector to detect\nany objects within 3D scenes without relying on hand-crafted text prompts. We\nintroduce the strong generalization and zero-shot capabilities of 2D foundation\nmodels, utilizing both 2D semantic priors and 3D geometric priors for\nclass-agnostic proposals to broaden 3D object discovery. Then, by integrating\ncomplementary information from point cloud and RGB image in the cross-modal\nmixture of experts, OP3Det dynamically routes uni-modal and multi-modal\nfeatures to learn generalized 3D objectness. Extensive experiments demonstrate\nthe extraordinary performance of OP3Det, which significantly surpasses existing\nopen-world 3D detectors by up to 16.0% in AR and achieves a 13.5% improvement\ncompared to closed-world 3D detectors."
    },
    {
      "title": "Multilingual Text-to-Image Person Retrieval via Bidirectional Relation Reasoning and Aligning",
      "authors": [
        "Min Cao",
        "Xinyu Zhou",
        "Ding Jiang",
        "Bo Du",
        "Mang Ye",
        "Min Zhang"
      ],
      "arxiv_id": "2510.17685v1",
      "summary": "Text-to-image person retrieval (TIPR) aims to identify the target person\nusing textual descriptions, facing challenge in modality heterogeneity. Prior\nworks have attempted to address it by developing cross-modal global or local\nalignment strategies. However, global methods typically overlook fine-grained\ncross-modal differences, whereas local methods require prior information to\nexplore explicit part alignments. Additionally, current methods are\nEnglish-centric, restricting their application in multilingual contexts. To\nalleviate these issues, we pioneer a multilingual TIPR task by developing a\nmultilingual TIPR benchmark, for which we leverage large language models for\ninitial translations and refine them by integrating domain-specific knowledge.\nCorrespondingly, we propose Bi-IRRA: a Bidirectional Implicit Relation\nReasoning and Aligning framework to learn alignment across languages and\nmodalities. Within Bi-IRRA, a bidirectional implicit relation reasoning module\nenables bidirectional prediction of masked image and text, implicitly enhancing\nthe modeling of local relations across languages and modalities, a\nmulti-dimensional global alignment module is integrated to bridge the modality\nheterogeneity. The proposed method achieves new state-of-the-art results on all\nmultilingual TIPR datasets. Data and code are presented in\nhttps://github.com/Flame-Chasers/Bi-IRRA."
    },
    {
      "title": "Intelligent Communication Mixture-of-Experts Boosted-Medical Image Segmentation Foundation Model",
      "authors": [
        "Xinwei Zhang",
        "Hu Chen",
        "Zhe Yuan",
        "Sukun Tian",
        "Peng Feng"
      ],
      "arxiv_id": "2510.17684v1",
      "summary": "Foundation models for medical image segmentation have achieved remarkable\nperformance. Adaptive fine-tuning of natural image segmentation foundation\nmodels is crucial for medical image segmentation tasks. However, some\nlimitations exist in existing fine-tuning methods: 1) insufficient\nrepresentation of high-level features and 2) the fine-tuning process disrupts\nthe structural integrity of pretrained weights. Inspired by these critical\nproblems, we propose an intelligent communication mixture-of-experts\nboosted-medical image segmentation foundation model, named IC-MoE, with twofold\nideas: 1) We construct basic experts, semantic experts, and adaptive experts.\nMoreover, we implement a pixel probability adaptive voting strategy, which\nenables expert selection and fusion through label consistency and load\nbalancing. This approach preliminarily enhances the representation capability\nof high-level features while preserving the structural integrity of pretrained\nweights. 2) We propose a semantic-guided contrastive learning method to address\nthe issue of weak supervision in contrastive learning. This method further\nenhances the representation capability of high-level features while preserving\nthe structural integrity of pretrained weights. Extensive experiments across\nthree public medical image segmentation datasets demonstrate that the IC-MoE\noutperforms other SOTA models. Consequently, the proposed IC-MoE effectively\nsupplements foundational medical image segmentation models with high-level\nfeatures and pretrained structural integrity. We also validate the superior\ngeneralizability of the IC-MoE across diverse medical image segmentation\nscenarios."
    },
    {
      "title": "PICABench: How Far Are We from Physically Realistic Image Editing?",
      "authors": [
        "Yuandong Pu",
        "Le Zhuo",
        "Songhao Han",
        "Jinbo Xing",
        "Kaiwen Zhu",
        "Shuo Cao",
        "Bin Fu",
        "Si Liu",
        "Hongsheng Li",
        "Yu Qiao",
        "Wenlong Zhang",
        "Xi Chen",
        "Yihao Liu"
      ],
      "arxiv_id": "2510.17681v1",
      "summary": "Image editing has achieved remarkable progress recently. Modern editing\nmodels could already follow complex instructions to manipulate the original\ncontent. However, beyond completing the editing instructions, the accompanying\nphysical effects are the key to the generation realism. For example, removing\nan object should also remove its shadow, reflections, and interactions with\nnearby objects. Unfortunately, existing models and benchmarks mainly focus on\ninstruction completion but overlook these physical effects. So, at this moment,\nhow far are we from physically realistic image editing? To answer this, we\nintroduce PICABench, which systematically evaluates physical realism across\neight sub-dimension (spanning optics, mechanics, and state transitions) for\nmost of the common editing operations (add, remove, attribute change, etc). We\nfurther propose the PICAEval, a reliable evaluation protocol that uses\nVLM-as-a-judge with per-case, region-level human annotations and questions.\nBeyond benchmarking, we also explore effective solutions by learning physics\nfrom videos and construct a training dataset PICA-100K. After evaluating most\nof the mainstream models, we observe that physical realism remains a\nchallenging problem with large rooms to explore. We hope that our benchmark and\nproposed solutions can serve as a foundation for future work moving from naive\ncontent editing toward physically consistent realism."
    },
    {
      "title": "4DSegStreamer: Streaming 4D Panoptic Segmentation via Dual Threads",
      "authors": [
        "Ling Liu",
        "Jun Tian",
        "Li Yi"
      ],
      "arxiv_id": "2510.17664v1",
      "summary": "4D panoptic segmentation in a streaming setting is critical for highly\ndynamic environments, such as evacuating dense crowds and autonomous driving in\ncomplex scenarios, where real-time, fine-grained perception within a\nconstrained time budget is essential. In this paper, we introduce\n4DSegStreamer, a novel framework that employs a Dual-Thread System to\nefficiently process streaming frames. The framework is general and can be\nseamlessly integrated into existing 3D and 4D segmentation methods to enable\nreal-time capability. It also demonstrates superior robustness compared to\nexisting streaming perception approaches, particularly under high FPS\nconditions. The system consists of a predictive thread and an inference thread.\nThe predictive thread leverages historical motion and geometric information to\nextract features and forecast future dynamics. The inference thread ensures\ntimely prediction for incoming frames by aligning with the latest memory and\ncompensating for ego-motion and dynamic object movements. We evaluate\n4DSegStreamer on the indoor HOI4D dataset and the outdoor SemanticKITTI and\nnuScenes datasets. Comprehensive experiments demonstrate the effectiveness of\nour approach, particularly in accurately predicting dynamic objects in complex\nscenes."
    },
    {
      "title": "Frugal Federated Learning for Violence Detection: A Comparison of LoRA-Tuned VLMs and Personalized CNNs",
      "authors": [
        "Sébastien Thuau",
        "Siba Haidar",
        "Ayush Bajracharya",
        "Rachid Chelouah"
      ],
      "arxiv_id": "2510.17651v1",
      "summary": "We examine frugal federated learning approaches to violence detection by\ncomparing two complementary strategies: (i) zero-shot and federated fine-tuning\nof vision-language models (VLMs), and (ii) personalized training of a compact\n3D convolutional neural network (CNN3D). Using LLaVA-7B and a 65.8M parameter\nCNN3D as representative cases, we evaluate accuracy, calibration, and energy\nusage under realistic non-IID settings.\n  Both approaches exceed 90% accuracy. CNN3D slightly outperforms Low-Rank\nAdaptation(LoRA)-tuned VLMs in ROC AUC and log loss, while using less energy.\nVLMs remain favorable for contextual reasoning and multimodal inference. We\nquantify energy and CO$_2$ emissions across training and inference, and analyze\nsustainability trade-offs for deployment.\n  To our knowledge, this is the first comparative study of LoRA-tuned\nvision-language models and personalized CNNs for federated violence detection,\nwith an emphasis on energy efficiency and environmental metrics.\n  These findings support a hybrid model: lightweight CNNs for routine\nclassification, with selective VLM activation for complex or descriptive\nscenarios. The resulting framework offers a reproducible baseline for\nresponsible, resource-aware AI in video surveillance, with extensions toward\nreal-time, multimodal, and lifecycle-aware systems."
    },
    {
      "title": "ZACH-ViT: A Zero-Token Vision Transformer with ShuffleStrides Data Augmentation for Robust Lung Ultrasound Classification",
      "authors": [
        "Athanasios Angelakis",
        "Amne Mousa",
        "Micah L. A. Heldeweg",
        "Laurens A. Biesheuvel",
        "Mark A. Haaksma",
        "Jasper M. Smit",
        "Pieter R. Tuinman",
        "Paul W. G. Elbers"
      ],
      "arxiv_id": "2510.17650v1",
      "summary": "Differentiating cardiogenic pulmonary oedema (CPE) from non-cardiogenic and\nstructurally normal lungs in lung ultrasound (LUS) videos remains challenging\ndue to the high visual variability of non-cardiogenic inflammatory patterns\n(NCIP/ARDS-like), interstitial lung disease, and healthy lungs. This\nheterogeneity complicates automated classification as overlapping B-lines and\npleural artefacts are common. We introduce ZACH-ViT (Zero-token Adaptive\nCompact Hierarchical Vision Transformer), a 0.25 M-parameter Vision Transformer\nvariant that removes both positional embeddings and the [CLS] token, making it\nfully permutation-invariant and suitable for unordered medical image data. To\nenhance generalization, we propose ShuffleStrides Data Augmentation (SSDA),\nwhich permutes probe-view sequences and frame orders while preserving\nanatomical validity. ZACH-ViT was evaluated on 380 LUS videos from 95\ncritically ill patients against nine state-of-the-art baselines. Despite the\nheterogeneity of the non-cardiogenic group, ZACH-ViT achieved the highest\nvalidation and test ROC-AUC (0.80 and 0.79) with balanced sensitivity (0.60)\nand specificity (0.91), while all competing models collapsed to trivial\nclassification. It trains 1.35x faster than Minimal ViT (0.62M parameters) with\n2.5x fewer parameters, supporting real-time clinical deployment. These results\nshow that aligning architectural design with data structure can outperform\nscale in small-data medical imaging."
    },
    {
      "title": "Self-supervised Pre-training for Mapping of Archaeological Stone Wall in Historic Landscapes Using High-Resolution DEM Derivatives",
      "authors": [
        "Zexian Huang",
        "Mashnoon Islam",
        "Brian Armstrong",
        "Kourosh Khoshelham",
        "Martin Tomko"
      ],
      "arxiv_id": "2510.17644v1",
      "summary": "Dry-stone walls hold significant heritage and environmental value. Mapping\nthese structures is essential for ecosystem preservation and wildfire\nmanagement in Australia. Yet, many walls remain unidentified due to their\ninaccessibility and the high cost of manual mapping. Deep learning-based\nsegmentation offers a scalable solution, but two major challenges persist: (1)\nvisual occlusion of low-lying walls by dense vegetation, and (2) limited\nlabeled data for supervised training. We propose DINO-CV, a segmentation\nframework for automatic mapping of low-lying dry-stone walls using\nhigh-resolution Airborne LiDAR-derived digital elevation models (DEMs). DEMs\novercome visual occlusion by capturing terrain structures hidden beneath\nvegetation, enabling analysis of structural rather than spectral cues. DINO-CV\nintroduces a self-supervised cross-view pre-training strategy based on\nknowledge distillation to mitigate data scarcity. It learns invariant visual\nand geometric representations across multiple DEM derivatives, supporting\nvarious vision backbones including ResNet, Wide ResNet, and Vision\nTransformers. Applied to the UNESCO World Heritage cultural landscape of Budj\nBim, Victoria, the method identifies one of Australia's densest collections of\ncolonial dry-stone walls beyond Indigenous heritage contexts. DINO-CV achieves\na mean Intersection over Union (mIoU) of 68.6% on test areas and maintains\n63.8% mIoU when fine-tuned with only 10% labeled data. These results\ndemonstrate the potential of self-supervised learning on high-resolution DEM\nderivatives for automated dry-stone wall mapping in vegetated and heritage-rich\nenvironments with scarce annotations."
    },
    {
      "title": "RESample: A Robust Data Augmentation Framework via Exploratory Sampling for Robotic Manipulation",
      "authors": [
        "Yuquan Xue",
        "Guanxing Lu",
        "Zhenyu Wu",
        "Chuanrui Zhang",
        "Bofang Jia",
        "Zhengyi Gu",
        "Yansong Tang",
        "Ziwei Wang"
      ],
      "arxiv_id": "2510.17640v1",
      "summary": "Vision-Language-Action models (VLAs) have demonstrated remarkable performance\non complex robotic manipulation tasks through imitation learning. However,\nexisting imitation learning datasets contain only successful trajectories and\nlack failure or recovery data, especially for out-of-distribution (OOD) states\nwhere the robot deviates from the main policy due to minor perturbations or\nerrors, leading VLA models to struggle with states deviating from the training\ndistribution. To this end, we propose an automated OOD data augmentation\nframework named RESample through exploratory sampling. Specifically, we first\nleverage offline reinforcement learning to obtain an action-value network that\naccurately identifies sub-optimal actions under the current manipulation\npolicy. We further sample potential OOD states from trajectories via rollout,\nand design an exploratory sampling mechanism that adaptively incorporates these\naction proxies into the training dataset to ensure efficiency. Subsequently,\nour framework explicitly encourages the VLAs to recover from OOD states and\nenhances their robustness against distributional shifts. We conduct extensive\nexperiments on the LIBERO benchmark as well as real-world robotic manipulation\ntasks, demonstrating that RESample consistently improves the stability and\ngeneralization ability of VLA models."
    },
    {
      "title": "CaMiT: A Time-Aware Car Model Dataset for Classification and Generation",
      "authors": [
        "Frédéric LIN",
        "Biruk Abere Ambaw",
        "Adrian Popescu",
        "Hejer Ammar",
        "Romaric Audigier",
        "Hervé Le Borgne"
      ],
      "arxiv_id": "2510.17626v1",
      "summary": "AI systems must adapt to evolving visual environments, especially in domains\nwhere object appearances change over time. We introduce Car Models in Time\n(CaMiT), a fine-grained dataset capturing the temporal evolution of car models,\na representative class of technological artifacts. CaMiT includes 787K labeled\nsamples of 190 car models (2007-2023) and 5.1M unlabeled samples (2005-2023),\nsupporting both supervised and self-supervised learning. Static pretraining on\nin-domain data achieves competitive performance with large-scale generalist\nmodels while being more resource-efficient, yet accuracy declines when models\nare tested across years. To address this, we propose a time-incremental\nclassification setting, a realistic continual learning scenario with emerging,\nevolving, and disappearing classes. We evaluate two strategies:\ntime-incremental pretraining, which updates the backbone, and time-incremental\nclassifier learning, which updates only the final layer, both improving\ntemporal robustness. Finally, we explore time-aware image generation that\nleverages temporal metadata during training, yielding more realistic outputs.\nCaMiT offers a rich benchmark for studying temporal adaptation in fine-grained\nvisual recognition and generation."
    },
    {
      "title": "ImaGGen: Zero-Shot Generation of Co-Speech Semantic Gestures Grounded in Language and Image Input",
      "authors": [
        "Hendric Voss",
        "Stefan Kopp"
      ],
      "arxiv_id": "2510.17617v1",
      "summary": "Human communication combines speech with expressive nonverbal cues such as\nhand gestures that serve manifold communicative functions. Yet, current\ngenerative gesture generation approaches are restricted to simple, repetitive\nbeat gestures that accompany the rhythm of speaking but do not contribute to\ncommunicating semantic meaning. This paper tackles a core challenge in\nco-speech gesture synthesis: generating iconic or deictic gestures that are\nsemantically coherent with a verbal utterance. Such gestures cannot be derived\nfrom language input alone, which inherently lacks the visual meaning that is\noften carried autonomously by gestures. We therefore introduce a zero-shot\nsystem that generates gestures from a given language input and additionally is\ninformed by imagistic input, without manual annotation or human intervention.\nOur method integrates an image analysis pipeline that extracts key object\nproperties such as shape, symmetry, and alignment, together with a semantic\nmatching module that links these visual details to spoken text. An inverse\nkinematics engine then synthesizes iconic and deictic gestures and combines\nthem with co-generated natural beat gestures for coherent multimodal\ncommunication. A comprehensive user study demonstrates the effectiveness of our\napproach. In scenarios where speech alone was ambiguous, gestures generated by\nour system significantly improved participants' ability to identify object\nproperties, confirming their interpretability and communicative value. While\nchallenges remain in representing complex shapes, our results highlight the\nimportance of context-aware semantic gestures for creating expressive and\ncollaborative virtual agents or avatars, marking a substantial step forward\ntowards efficient and robust, embodied human-agent interaction. More\ninformation and example videos are available here:\nhttps://review-anon-io.github.io/ImaGGen.github.io/"
    },
    {
      "title": "One Dinomaly2 Detect Them All: A Unified Framework for Full-Spectrum Unsupervised Anomaly Detection",
      "authors": [
        "Jia Guo",
        "Shuai Lu",
        "Lei Fan",
        "Zelin Li",
        "Donglin Di",
        "Yang Song",
        "Weihang Zhang",
        "Wenbing Zhu",
        "Hong Yan",
        "Fang Chen",
        "Huiqi Li",
        "Hongen Liao"
      ],
      "arxiv_id": "2510.17611v1",
      "summary": "Unsupervised anomaly detection (UAD) has evolved from building specialized\nsingle-class models to unified multi-class models, yet existing multi-class\nmodels significantly underperform the most advanced one-for-one counterparts.\nMoreover, the field has fragmented into specialized methods tailored to\nspecific scenarios (multi-class, 3D, few-shot, etc.), creating deployment\nbarriers and highlighting the need for a unified solution. In this paper, we\npresent Dinomaly2, the first unified framework for full-spectrum image UAD,\nwhich bridges the performance gap in multi-class models while seamlessly\nextending across diverse data modalities and task settings. Guided by the \"less\nis more\" philosophy, we demonstrate that the orchestration of five simple\nelement achieves superior performance in a standard reconstruction-based\nframework. This methodological minimalism enables natural extension across\ndiverse tasks without modification, establishing that simplicity is the\nfoundation of true universality. Extensive experiments on 12 UAD benchmarks\ndemonstrate Dinomaly2's full-spectrum superiority across multiple modalities\n(2D, multi-view, RGB-3D, RGB-IR), task settings (single-class, multi-class,\ninference-unified multi-class, few-shot) and application domains (industrial,\nbiological, outdoor). For example, our multi-class model achieves unprecedented\n99.9% and 99.3% image-level (I-) AUROC on MVTec-AD and VisA respectively. For\nmulti-view and multi-modal inspection, Dinomaly2 demonstrates state-of-the-art\nperformance with minimum adaptations. Moreover, using only 8 normal examples\nper class, our method surpasses previous full-shot models, achieving 98.7% and\n97.4% I-AUROC on MVTec-AD and VisA. The combination of minimalistic design,\ncomputational scalability, and universal applicability positions Dinomaly2 as a\nunified solution for the full spectrum of real-world anomaly detection\napplications."
    },
    {
      "title": "Integrating BIM and UAV-based photogrammetry for Automated 3D Structure Model Segmentation",
      "authors": [
        "Siqi Chen",
        "Shanyue Guan"
      ],
      "arxiv_id": "2510.17609v1",
      "summary": "The advancement of UAV technology has enabled efficient, non-contact\nstructural health monitoring. Combined with photogrammetry, UAVs can capture\nhigh-resolution scans and reconstruct detailed 3D models of infrastructure.\nHowever, a key challenge remains in segmenting specific structural components\nfrom these models-a process traditionally reliant on time-consuming and\nerror-prone manual labeling. To address this issue, we propose a machine\nlearning-based framework for automated segmentation of 3D point clouds. Our\napproach uses the complementary strengths of real-world UAV-scanned point\nclouds and synthetic data generated from Building Information Modeling (BIM) to\novercome the limitations associated with manual labeling. Validation on a\nrailroad track dataset demonstrated high accuracy in identifying and segmenting\nmajor components such as rails and crossties. Moreover, by using smaller-scale\ndatasets supplemented with BIM data, the framework significantly reduced\ntraining time while maintaining reasonable segmentation accuracy. This\nautomated approach improves the precision and efficiency of 3D infrastructure\nmodel segmentation and advances the integration of UAV and BIM technologies in\nstructural health monitoring and infrastructure management."
    },
    {
      "title": "Learned Inertial Odometry for Cycling Based on Mixture of Experts Algorithm",
      "authors": [
        "Hao Qiao",
        "Yan Wang",
        "Shuo Yang",
        "Xiaoyao Yu",
        "Jian kuang",
        "Xiaoji Niu"
      ],
      "arxiv_id": "2510.17604v1",
      "summary": "With the rapid growth of bike sharing and the increasing diversity of cycling\napplications, accurate bicycle localization has become essential. traditional\nGNSS-based methods suffer from multipath effects, while existing inertial\nnavigation approaches rely on precise modeling and show limited robustness.\nTight Learned Inertial Odometry (TLIO) achieves low position drift by combining\nraw IMU data with predicted displacements by neural networks, but its high\ncomputational cost restricts deployment on mobile devices. To overcome this, we\nextend TLIO to bicycle localization and introduce an improved Mixture-of\nExperts (MoE) model that reduces both training and inference costs. Experiments\nshow that, compared to the state-of-the-art LLIO framework, our method achieves\ncomparable accuracy while reducing parameters by 64.7% and computational cost\nby 81.8%."
    },
    {
      "title": "ShapeCraft: LLM Agents for Structured, Textured and Interactive 3D Modeling",
      "authors": [
        "Shuyuan Zhang",
        "Chenhan Jiang",
        "Zuoou Li",
        "Jiankang Deng"
      ],
      "arxiv_id": "2510.17603v1",
      "summary": "3D generation from natural language offers significant potential to reduce\nexpert manual modeling efforts and enhance accessibility to 3D assets. However,\nexisting methods often yield unstructured meshes and exhibit poor\ninteractivity, making them impractical for artistic workflows. To address these\nlimitations, we represent 3D assets as shape programs and introduce ShapeCraft,\na novel multi-agent framework for text-to-3D generation. At its core, we\npropose a Graph-based Procedural Shape (GPS) representation that decomposes\ncomplex natural language into a structured graph of sub-tasks, thereby\nfacilitating accurate LLM comprehension and interpretation of spatial\nrelationships and semantic shape details. Specifically, LLM agents\nhierarchically parse user input to initialize GPS, then iteratively refine\nprocedural modeling and painting to produce structured, textured, and\ninteractive 3D assets. Qualitative and quantitative experiments demonstrate\nShapeCraft's superior performance in generating geometrically accurate and\nsemantically rich 3D assets compared to existing LLM-based agents. We further\nshow the versatility of ShapeCraft through examples of animated and\nuser-customized editing, highlighting its potential for broader interactive\napplications."
    },
    {
      "title": "Conveying Meaning through Gestures: An Investigation into Semantic Co-Speech Gesture Generation",
      "authors": [
        "Hendric Voss",
        "Lisa Michelle Bohnenkamp",
        "Stefan Kopp"
      ],
      "arxiv_id": "2510.17599v1",
      "summary": "This study explores two frameworks for co-speech gesture generation, AQ-GT\nand its semantically-augmented variant AQ-GT-a, to evaluate their ability to\nconvey meaning through gestures and how humans perceive the resulting\nmovements. Using sentences from the SAGA spatial communication corpus,\ncontextually similar sentences, and novel movement-focused sentences, we\nconducted a user-centered evaluation of concept recognition and human-likeness.\nResults revealed a nuanced relationship between semantic annotations and\nperformance. The original AQ-GT framework, lacking explicit semantic input, was\nsurprisingly more effective at conveying concepts within its training domain.\nConversely, the AQ-GT-a framework demonstrated better generalization,\nparticularly for representing shape and size in novel contexts. While\nparticipants rated gestures from AQ-GT-a as more expressive and helpful, they\ndid not perceive them as more human-like. These findings suggest that explicit\nsemantic enrichment does not guarantee improved gesture generation and that its\neffectiveness is highly dependent on the context, indicating a potential\ntrade-off between specialization and generalization."
    },
    {
      "title": "MIRAGE: Agentic Framework for Multimodal Misinformation Detection with Web-Grounded Reasoning",
      "authors": [
        "Mir Nafis Sharear Shopnil",
        "Sharad Duwal",
        "Abhishek Tyagi",
        "Adiba Mahbub Proma"
      ],
      "arxiv_id": "2510.17590v1",
      "summary": "Misinformation spreads across web platforms through billions of daily\nmultimodal posts that combine text and images, overwhelming manual\nfact-checking capacity. Supervised detection models require domain-specific\ntraining data and fail to generalize across diverse manipulation tactics. We\npresent MIRAGE, an inference-time, model-pluggable agentic framework that\ndecomposes multimodal verification into four sequential modules: visual\nveracity assessment detects AI-generated images, cross-modal consistency\nanalysis identifies out-of-context repurposing, retrieval-augmented factual\nchecking grounds claims in web evidence through iterative question generation,\nand a calibrated judgment module integrates all signals. MIRAGE orchestrates\nvision-language model reasoning with targeted web retrieval, outputs structured\nand citation-linked rationales. On MMFakeBench validation set (1,000 samples),\nMIRAGE with GPT-4o-mini achieves 81.65% F1 and 75.1% accuracy, outperforming\nthe strongest zero-shot baseline (GPT-4V with MMD-Agent at 74.0% F1) by 7.65\npoints while maintaining 34.3% false positive rate versus 97.3% for a\njudge-only baseline. Test set results (5,000 samples) confirm generalization\nwith 81.44% F1 and 75.08% accuracy. Ablation studies show visual verification\ncontributes 5.18 F1 points and retrieval-augmented reasoning contributes 2.97\npoints. Our results demonstrate that decomposed agentic reasoning with web\nretrieval can match supervised detector performance without domain-specific\ntraining, enabling misinformation detection across modalities where labeled\ndata remains scarce."
    },
    {
      "title": "Expose Camouflage in the Water: Underwater Camouflaged Instance Segmentation and Dataset",
      "authors": [
        "Chuhong Wang",
        "Hua Li",
        "Chongyi Li",
        "Huazhong Liu",
        "Xiongxin Tang",
        "Sam Kwong"
      ],
      "arxiv_id": "2510.17585v1",
      "summary": "With the development of underwater exploration and marine protection,\nunderwater vision tasks are widespread. Due to the degraded underwater\nenvironment, characterized by color distortion, low contrast, and blurring,\ncamouflaged instance segmentation (CIS) faces greater challenges in accurately\nsegmenting objects that blend closely with their surroundings. Traditional\ncamouflaged instance segmentation methods, trained on terrestrial-dominated\ndatasets with limited underwater samples, may exhibit inadequate performance in\nunderwater scenes. To address these issues, we introduce the first underwater\ncamouflaged instance segmentation (UCIS) dataset, abbreviated as UCIS4K, which\ncomprises 3,953 images of camouflaged marine organisms with instance-level\nannotations. In addition, we propose an Underwater Camouflaged Instance\nSegmentation network based on Segment Anything Model (UCIS-SAM). Our UCIS-SAM\nincludes three key modules. First, the Channel Balance Optimization Module\n(CBOM) enhances channel characteristics to improve underwater feature learning,\neffectively addressing the model's limited understanding of underwater\nenvironments. Second, the Frequency Domain True Integration Module (FDTIM) is\nproposed to emphasize intrinsic object features and reduce interference from\ncamouflage patterns, enhancing the segmentation performance of camouflaged\nobjects blending with their surroundings. Finally, the Multi-scale Feature\nFrequency Aggregation Module (MFFAM) is designed to strengthen the boundaries\nof low-contrast camouflaged instances across multiple frequency bands,\nimproving the model's ability to achieve more precise segmentation of\ncamouflaged objects. Extensive experiments on the proposed UCIS4K and public\nbenchmarks show that our UCIS-SAM outperforms state-of-the-art approaches."
    },
    {
      "title": "Intent-Driven LLM Ensemble Planning for Flexible Multi-Robot Disassembly: Demonstration on EV Batteries",
      "authors": [
        "Cansu Erdogan",
        "Cesar Alan Contreras",
        "Alireza Rastegarpanah",
        "Manolis Chiou",
        "Rustam Stolkin"
      ],
      "arxiv_id": "2510.17576v1",
      "summary": "This paper addresses the problem of planning complex manipulation tasks, in\nwhich multiple robots with different end-effectors and capabilities, informed\nby computer vision, must plan and execute concatenated sequences of actions on\na variety of objects that can appear in arbitrary positions and configurations\nin unstructured scenes. We propose an intent-driven planning pipeline which can\nrobustly construct such action sequences with varying degrees of supervisory\ninput from a human using simple language instructions. The pipeline integrates:\n(i) perception-to-text scene encoding, (ii) an ensemble of large language\nmodels (LLMs) that generate candidate removal sequences based on the operator's\nintent, (iii) an LLM-based verifier that enforces formatting and precedence\nconstraints, and (iv) a deterministic consistency filter that rejects\nhallucinated objects. The pipeline is evaluated on an example task in which two\nrobot arms work collaboratively to dismantle an Electric Vehicle battery for\nrecycling applications. A variety of components must be grasped and removed in\nspecific sequences, determined by human instructions and/or by task-order\nfeasibility decisions made by the autonomous system. On 200 real scenes with\n600 operator prompts across five component classes, we used metrics of\nfull-sequence correctness and next-task correctness to evaluate and compare\nfive LLM-based planners (including ablation analyses of pipeline components).\nWe also evaluated the LLM-based human interface in terms of time to execution\nand NASA TLX with human participant experiments. Results indicate that our\nensemble-with-verification approach reliably maps operator intent to safe,\nexecutable multi-robot plans while maintaining low user effort."
    },
    {
      "title": "PAGE-4D: Disentangled Pose and Geometry Estimation for 4D Perception",
      "authors": [
        "Kaichen Zhou",
        "Yuhan Wang",
        "Grace Chen",
        "Xinhai Chang",
        "Gaspard Beaudouin",
        "Fangneng Zhan",
        "Paul Pu Liang",
        "Mengyu Wang"
      ],
      "arxiv_id": "2510.17568v1",
      "summary": "Recent 3D feed-forward models, such as the Visual Geometry Grounded\nTransformer (VGGT), have shown strong capability in inferring 3D attributes of\nstatic scenes. However, since they are typically trained on static datasets,\nthese models often struggle in real-world scenarios involving complex dynamic\nelements, such as moving humans or deformable objects like umbrellas. To\naddress this limitation, we introduce PAGE-4D, a feedforward model that extends\nVGGT to dynamic scenes, enabling camera pose estimation, depth prediction, and\npoint cloud reconstruction -- all without post-processing. A central challenge\nin multi-task 4D reconstruction is the inherent conflict between tasks:\naccurate camera pose estimation requires suppressing dynamic regions, while\ngeometry reconstruction requires modeling them. To resolve this tension, we\npropose a dynamics-aware aggregator that disentangles static and dynamic\ninformation by predicting a dynamics-aware mask -- suppressing motion cues for\npose estimation while amplifying them for geometry reconstruction. Extensive\nexperiments show that PAGE-4D consistently outperforms the original VGGT in\ndynamic scenarios, achieving superior results in camera pose estimation,\nmonocular and video depth estimation, and dense point map reconstruction."
    },
    {
      "title": "WP-CrackNet: A Collaborative Adversarial Learning Framework for End-to-End Weakly-Supervised Road Crack Detection",
      "authors": [
        "Nachuan Ma",
        "Zhengfei Song",
        "Qiang Hu",
        "Xiaoyu Tang",
        "Chengxi Zhang",
        "Rui Fan",
        "Lihua Xie"
      ],
      "arxiv_id": "2510.17566v1",
      "summary": "Road crack detection is essential for intelligent infrastructure maintenance\nin smart cities. To reduce reliance on costly pixel-level annotations, we\npropose WP-CrackNet, an end-to-end weakly-supervised method that trains with\nonly image-level labels for pixel-wise crack detection. WP-CrackNet integrates\nthree components: a classifier generating class activation maps (CAMs), a\nreconstructor measuring feature inferability, and a detector producing\npixel-wise road crack detection results. During training, the classifier and\nreconstructor alternate in adversarial learning to encourage crack CAMs to\ncover complete crack regions, while the detector learns from pseudo labels\nderived from post-processed crack CAMs. This mutual feedback among the three\ncomponents improves learning stability and detection accuracy. To further boost\ndetection performance, we design a path-aware attention module (PAAM) that\nfuses high-level semantics from the classifier with low-level structural cues\nfrom the reconstructor by modeling spatial and channel-wise dependencies.\nAdditionally, a center-enhanced CAM consistency module (CECCM) is proposed to\nrefine crack CAMs using center Gaussian weighting and consistency constraints,\nenabling better pseudo-label generation. We create three image-level datasets\nand extensive experiments show that WP-CrackNet achieves comparable results to\nsupervised methods and outperforms existing weakly-supervised methods,\nsignificantly advancing scalable road inspection. The source code package and\ndatasets are available at https://mias.group/WP-CrackNet/."
    },
    {
      "title": "An Empirical Study of Lagrangian Methods in Safe Reinforcement Learning",
      "authors": [
        "Lindsay Spoor",
        "Álvaro Serra-Gómez",
        "Aske Plaat",
        "Thomas Moerland"
      ],
      "arxiv_id": "2510.17564v1",
      "summary": "In safety-critical domains such as robotics, navigation and power systems,\nconstrained optimization problems arise where maximizing performance must be\ncarefully balanced with associated constraints. Safe reinforcement learning\nprovides a framework to address these challenges, with Lagrangian methods being\na popular choice. However, the effectiveness of Lagrangian methods crucially\ndepends on the choice of the Lagrange multiplier $\\lambda$, which governs the\ntrade-off between return and constraint cost. A common approach is to update\nthe multiplier automatically during training. Although this is standard in\npractice, there remains limited empirical evidence on the robustness of an\nautomated update and its influence on overall performance. Therefore, we\nanalyze (i) optimality and (ii) stability of Lagrange multipliers in safe\nreinforcement learning across a range of tasks. We provide $\\lambda$-profiles\nthat give a complete visualization of the trade-off between return and\nconstraint cost of the optimization problem. These profiles show the highly\nsensitive nature of $\\lambda$ and moreover confirm the lack of general\nintuition for choosing the optimal value $\\lambda^*$. Our findings additionally\nshow that automated multiplier updates are able to recover and sometimes even\nexceed the optimal performance found at $\\lambda^*$ due to the vast difference\nin their learning trajectories. Furthermore, we show that automated multiplier\nupdates exhibit oscillatory behavior during training, which can be mitigated\nthrough PID-controlled updates. However, this method requires careful tuning to\nachieve consistently better performance across tasks. This highlights the need\nfor further research on stabilizing Lagrangian methods in safe reinforcement\nlearning. The code used to reproduce our results can be found at\nhttps://github.com/lindsayspoor/Lagrangian_SafeRL."
    },
    {
      "title": "Distributed Spatial-Temporal Trajectory Optimization for Unmanned-Aerial-Vehicle Swarm",
      "authors": [
        "Xiaobo Zheng",
        "Pan Tang",
        "Defu Lin",
        "Shaoming He"
      ],
      "arxiv_id": "2510.17541v1",
      "summary": "Swarm trajectory optimization problems are a well-recognized class of\nmulti-agent optimal control problems with strong nonlinearity. However, the\nheuristic nature of needing to set the final time for agents beforehand and the\ntime-consuming limitation of the significant number of iterations prohibit the\napplication of existing methods to large-scale swarm of Unmanned Aerial\nVehicles (UAVs) in practice. In this paper, we propose a spatial-temporal\ntrajectory optimization framework that accomplishes multi-UAV consensus based\non the Alternating Direction Multiplier Method (ADMM) and uses Differential\nDynamic Programming (DDP) for fast local planning of individual UAVs. The\nintroduced framework is a two-level architecture that employs Parameterized DDP\n(PDDP) as the trajectory optimizer for each UAV, and ADMM to satisfy the local\nconstraints and accomplish the spatial-temporal parameter consensus among all\nUAVs. This results in a fully distributed algorithm called Distributed\nParameterized DDP (D-PDDP). In addition, an adaptive tuning criterion based on\nthe spectral gradient method for the penalty parameter is proposed to reduce\nthe number of algorithmic iterations. Several simulation examples are presented\nto verify the effectiveness of the proposed algorithm."
    },
    {
      "title": "Detecting streaks in smart telescopes images with Deep Learning",
      "authors": [
        "Olivier Parisot",
        "Mahmoud Jaziri"
      ],
      "arxiv_id": "2510.17540v1",
      "summary": "The growing negative impact of the visibility of satellites in the night sky\nis influencing the practice of astronomy and astrophotograph, both at the\namateur and professional levels. The presence of these satellites has the\neffect of introducing streaks into the images captured during astronomical\nobservation, requiring the application of additional post processing to\nmitigate the undesirable impact, whether for data loss or cosmetic reasons. In\nthis paper, we show how we test and adapt various Deep Learning approaches to\ndetect streaks in raw astronomical data captured between March 2022 and\nFebruary 2023 with smart telescopes."
    },
    {
      "title": "MambaX-Net: Dual-Input Mamba-Enhanced Cross-Attention Network for Longitudinal MRI Segmentation",
      "authors": [
        "Yovin Yahathugoda",
        "Davide Prezzi",
        "Piyalitt Ittichaiwong",
        "Vicky Goh",
        "Sebastien Ourselin",
        "Michela Antonelli"
      ],
      "arxiv_id": "2510.17529v1",
      "summary": "Active Surveillance (AS) is a treatment option for managing low and\nintermediate-risk prostate cancer (PCa), aiming to avoid overtreatment while\nmonitoring disease progression through serial MRI and clinical follow-up.\nAccurate prostate segmentation is an important preliminary step for automating\nthis process, enabling automated detection and diagnosis of PCa. However,\nexisting deep-learning segmentation models are often trained on\nsingle-time-point and expertly annotated datasets, making them unsuitable for\nlongitudinal AS analysis, where multiple time points and a scarcity of expert\nlabels hinder their effective fine-tuning. To address these challenges, we\npropose MambaX-Net, a novel semi-supervised, dual-scan 3D segmentation\narchitecture that computes the segmentation for time point t by leveraging the\nMRI and the corresponding segmentation mask from the previous time point. We\nintroduce two new components: (i) a Mamba-enhanced Cross-Attention Module,\nwhich integrates the Mamba block into cross attention to efficiently capture\ntemporal evolution and long-range spatial dependencies, and (ii) a Shape\nExtractor Module that encodes the previous segmentation mask into a latent\nanatomical representation for refined zone delination. Moreover, we introduce a\nsemi-supervised self-training strategy that leverages pseudo-labels generated\nfrom a pre-trained nnU-Net, enabling effective learning without expert\nannotations. MambaX-Net was evaluated on a longitudinal AS dataset, and results\nshowed that it significantly outperforms state-of-the-art U-Net and\nTransformer-based models, achieving superior prostate zone segmentation even\nwhen trained on limited and noisy data."
    },
    {
      "title": "HumanMPC - Safe and Efficient MAV Navigation among Humans",
      "authors": [
        "Simon Schaefer",
        "Helen Oleynikova",
        "Sandra Hirche",
        "Stefan Leutenegger"
      ],
      "arxiv_id": "2510.17525v1",
      "summary": "Safe and efficient robotic navigation among humans is essential for\nintegrating robots into everyday environments. Most existing approaches focus\non simplified 2D crowd navigation and fail to account for the full complexity\nof human body dynamics beyond root motion. We present HumanMPC, a Model\nPredictive Control (MPC) framework for 3D Micro Air Vehicle (MAV) navigation\namong humans that combines theoretical safety guarantees with data-driven\nmodels for realistic human motion forecasting. Our approach introduces a novel\ntwist to reachability-based safety formulation that constrains only the initial\ncontrol input for safety while modeling its effects over the entire planning\nhorizon, enabling safe yet efficient navigation. We validate HumanMPC in both\nsimulated experiments using real human trajectories and in the real-world,\ndemonstrating its effectiveness across tasks ranging from goal-directed\nnavigation to visual servoing for human tracking. While we apply our method to\nMAVs in this work, it is generic and can be adapted by other platforms. Our\nresults show that the method ensures safety without excessive conservatism and\noutperforms baseline approaches in both efficiency and reliability."
    },
    {
      "title": "MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation Models",
      "authors": [
        "Yongshun Zhang",
        "Zhongyi Fan",
        "Yonghang Zhang",
        "Zhangzikang Li",
        "Weifeng Chen",
        "Zhongwei Feng",
        "Chaoyue Wang",
        "Peng Hou",
        "Anxiang Zeng"
      ],
      "arxiv_id": "2510.17519v1",
      "summary": "In recent years, large-scale generative models for visual content\n(\\textit{e.g.,} images, videos, and 3D objects/scenes) have made remarkable\nprogress. However, training large-scale video generation models remains\nparticularly challenging and resource-intensive due to cross-modal text-video\nalignment, the long sequences involved, and the complex spatiotemporal\ndependencies. To address these challenges, we present a training framework that\noptimizes four pillars: (i) data processing, (ii) model architecture, (iii)\ntraining strategy, and (iv) infrastructure for large-scale video generation\nmodels. These optimizations delivered significant efficiency gains and\nperformance improvements across all stages of data preprocessing, video\ncompression, parameter scaling, curriculum-based pretraining, and\nalignment-focused post-training. Our resulting model, MUG-V 10B, matches recent\nstate-of-the-art video generators overall and, on e-commerce-oriented video\ngeneration tasks, surpasses leading open-source baselines in human evaluations.\nMore importantly, we open-source the complete stack, including model weights,\nMegatron-Core-based large-scale training code, and inference pipelines for\nvideo generation and enhancement. To our knowledge, this is the first public\nrelease of large-scale video generation training code that exploits\nMegatron-Core to achieve high training efficiency and near-linear multi-node\nscaling, details are available in\n\\href{https://github.com/Shopee-MUG/MUG-V}{our webpage}."
    },
    {
      "title": "Context-Aware Pseudo-Label Scoring for Zero-Shot Video Summarization",
      "authors": [
        "Yuanli Wu",
        "Long Zhang",
        "Yue Du",
        "Bin Li"
      ],
      "arxiv_id": "2510.17501v1",
      "summary": "With the rapid proliferation of video content across social media,\nsurveillance, and education platforms, efficiently summarizing long videos into\nconcise yet semantically faithful surrogates has become increasingly vital.\nExisting supervised methods achieve strong in-domain accuracy by learning from\ndense annotations but suffer from high labeling costs and limited cross-dataset\ngeneralization, while unsupervised approaches, though label-free, often fail to\ncapture high-level human semantics and fine-grained narrative cues. More\nrecently, zero-shot prompting pipelines have leveraged large language models\n(LLMs) for training-free video summarization, yet remain highly sensitive to\nhandcrafted prompt templates and dataset-specific score normalization. To\novercome these limitations, we introduce a rubric-guided, pseudo-labeled\nprompting framework that transforms a small subset of ground-truth annotations\ninto high-confidence pseudo labels, which are aggregated into structured,\ndataset-adaptive scoring rubrics guiding interpretable scene evaluation. During\ninference, first and last segments are scored based solely on their\ndescriptions, whereas intermediate ones incorporate brief contextual summaries\nof adjacent scenes to assess narrative progression and redundancy. This\ncontextual prompting enables the LLM to balance local salience and global\ncoherence without parameter tuning. On SumMe and TVSum, our method achieves F1\nscores of \\textbf{57.58} and \\textbf{63.05}, surpassing unsupervised and prior\nzero-shot baselines while approaching supervised performance. The results\ndemonstrate that rubric-guided pseudo labeling effectively stabilizes LLM-based\nscoring and establishes a general, interpretable zero-shot paradigm for video\nsummarization."
    },
    {
      "title": "Split-Fuse-Transport: Annotation-Free Saliency via Dual Clustering and Optimal Transport Alignment",
      "authors": [
        "Muhammad Umer Ramzan",
        "Ali Zia",
        "Abdelwahed Khamis",
        "Noman Ali",
        "Usman Ali",
        "Wei Xiang"
      ],
      "arxiv_id": "2510.17484v1",
      "summary": "Salient object detection (SOD) aims to segment visually prominent regions in\nimages and serves as a foundational task for various computer vision\napplications. We posit that SOD can now reach near-supervised accuracy without\na single pixel-level label, but only when reliable pseudo-masks are available.\nWe revisit the prototype-based line of work and make two key observations.\nFirst, boundary pixels and interior pixels obey markedly different geometry;\nsecond, the global consistency enforced by optimal transport (OT) is\nunderutilized if prototype quality is weak. To address this, we introduce\nPOTNet, an adaptation of Prototypical Optimal Transport that replaces POT's\nsingle k-means step with an entropy-guided dual-clustering head: high-entropy\npixels are organized by spectral clustering, low-entropy pixels by k-means, and\nthe two prototype sets are subsequently aligned by OT. This\nsplit-fuse-transport design yields sharper, part-aware pseudo-masks in a single\nforward pass, without handcrafted priors. Those masks supervise a standard\nMaskFormer-style encoder-decoder, giving rise to AutoSOD, an end-to-end\nunsupervised SOD pipeline that eliminates SelfMask's offline voting yet\nimproves both accuracy and training efficiency. Extensive experiments on five\nbenchmarks show that AutoSOD outperforms unsupervised methods by up to 26% and\nweakly supervised methods by up to 36% in F-measure, further narrowing the gap\nto fully supervised models."
    },
    {
      "title": "SparseWorld: A Flexible, Adaptive, and Efficient 4D Occupancy World Model Powered by Sparse and Dynamic Queries",
      "authors": [
        "Chenxu Dang",
        "Haiyan Liu",
        "Guangjun Bao",
        "Pei An",
        "Xinyue Tang",
        "Jie Ma",
        "Bingchuan Sun",
        "Yan Wang"
      ],
      "arxiv_id": "2510.17482v1",
      "summary": "Semantic occupancy has emerged as a powerful representation in world models\nfor its ability to capture rich spatial semantics. However, most existing\noccupancy world models rely on static and fixed embeddings or grids, which\ninherently limit the flexibility of perception. Moreover, their ``in-place\nclassification\" over grids exhibits a potential misalignment with the dynamic\nand continuous nature of real scenarios.In this paper, we propose SparseWorld,\na novel 4D occupancy world model that is flexible, adaptive, and efficient,\npowered by sparse and dynamic queries. We propose a Range-Adaptive Perception\nmodule, in which learnable queries are modulated by the ego vehicle states and\nenriched with temporal-spatial associations to enable extended-range\nperception. To effectively capture the dynamics of the scene, we design a\nState-Conditioned Forecasting module, which replaces classification-based\nforecasting with regression-guided formulation, precisely aligning the dynamic\nqueries with the continuity of the 4D environment. In addition, We specifically\ndevise a Temporal-Aware Self-Scheduling training strategy to enable smooth and\nefficient training. Extensive experiments demonstrate that SparseWorld achieves\nstate-of-the-art performance across perception, forecasting, and planning\ntasks. Comprehensive visualizations and ablation studies further validate the\nadvantages of SparseWorld in terms of flexibility, adaptability, and\nefficiency. The code is available at https://github.com/MSunDYY/SparseWorld."
    },
    {
      "title": "Initialize to Generalize: A Stronger Initialization Pipeline for Sparse-View 3DGS",
      "authors": [
        "Feng Zhou",
        "Wenkai Guo",
        "Pu Cao",
        "Zhicheng Zhang",
        "Jianqin Yin"
      ],
      "arxiv_id": "2510.17479v1",
      "summary": "Sparse-view 3D Gaussian Splatting (3DGS) often overfits to the training\nviews, leading to artifacts like blurring in novel view rendering. Prior work\naddresses it either by enhancing the initialization (\\emph{i.e.}, the point\ncloud from Structure-from-Motion (SfM)) or by adding training-time constraints\n(regularization) to the 3DGS optimization. Yet our controlled ablations reveal\nthat initialization is the decisive factor: it determines the attainable\nperformance band in sparse-view 3DGS, while training-time constraints yield\nonly modest within-band improvements at extra cost. Given initialization's\nprimacy, we focus our design there. Although SfM performs poorly under sparse\nviews due to its reliance on feature matching, it still provides reliable seed\npoints. Thus, building on SfM, our effort aims to supplement the regions it\nfails to cover as comprehensively as possible. Specifically, we design: (i)\nfrequency-aware SfM that improves low-texture coverage via low-frequency view\naugmentation and relaxed multi-view correspondences; (ii) 3DGS\nself-initialization that lifts photometric supervision into additional points,\ncompensating SfM-sparse regions with learned Gaussian centers; and (iii)\npoint-cloud regularization that enforces multi-view consistency and uniform\nspatial coverage through simple geometric/visibility priors, yielding a clean\nand reliable point cloud. Our experiments on LLFF and Mip-NeRF360 demonstrate\nconsistent gains in sparse-view settings, establishing our approach as a\nstronger initialization strategy. Code is available at\nhttps://github.com/zss171999645/ItG-GS."
    },
    {
      "title": "Inverse Optimal Control of Muscle Force Sharing During Pathological Gait",
      "authors": [
        "Filip Bečanović",
        "Vincent Bonnet",
        "Kosta Jovanović",
        "Samer Mohammed",
        "Raphaël Dumas"
      ],
      "arxiv_id": "2510.17456v1",
      "summary": "Muscle force sharing is typically resolved by minimizing a specific objective\nfunction to approximate neural control strategies. An inverse optimal control\napproach was applied to identify the \"best\" objective function, among a\npositive linear combination of basis objective functions, associated with the\ngait of two post-stroke males, one high-functioning (subject S1) and one\nlow-functioning (subject S2). It was found that the \"best\" objective function\nis subject- and leg-specific. No single function works universally well, yet\nthe best options are usually differently weighted combinations of muscle\nactivation- and power-minimization. Subject-specific inverse optimal control\nmodels performed best on their respective limbs (\\textbf{RMSE 178/213 N, CC\n0.71/0.61} for non-paretic and paretic legs of S1; \\textbf{RMSE 205/165 N, CC\n0.88/0.85} for respective legs of S2), but cross-subject generalization was\npoor, particularly for paretic legs. Moreover, minimizing the root mean square\nof muscle power emerged as important for paretic limbs, while minimizing\nactivation-based functions dominated for non-paretic limbs. This may suggest\ndifferent neural control strategies between affected and unaffected sides,\npossibly altered by the presence of spasticity. Among the 15 considered\nobjective functions commonly used in inverse dynamics-based computations, the\nroot mean square of muscle power was the only one explicitly incorporating\nmuscle velocity, leading to a possible model for spasticity in the paretic\nlimbs. Although this objective function has been rarely used, it may be\nrelevant for modeling pathological gait, such as post-stroke gait."
    },
    {
      "title": "A Generalization of Input-Output Linearization via Dynamic Switching Between Melds of Output Functions",
      "authors": [
        "Mirko Mizzoni",
        "Pieter van Goor",
        "Barbara Bazzana",
        "Antonio Franchi"
      ],
      "arxiv_id": "2510.17448v1",
      "summary": "This letter presents a systematic framework for switching between different\nsets of outputs for the control of nonlinear systems via feedback\nlinearization. We introduce the concept of a meld to formally define a valid,\nfeedback-linearizable subset of outputs that can be selected from a larger deck\nof possible outputs. The main contribution is a formal proof establishing that\nunder suitable dwell-time and compatibility conditions, it is possible to\nswitch between different melds while guaranteeing the uniform boundedness of\nthe system state. We further show that the error dynamics of the active outputs\nremain exponentially stable within each switching interval and that outputs\ncommon to consecutive melds are tracked seamlessly through transitions. The\nproposed theory is valid for any feedback linearizable nonlinear system, such\nas, e.g., robots, aerial and terrestrial vehicles, etc.. We demonstrate it on a\nsimple numerical simulation of a robotic manipulator."
    },
    {
      "title": "Rethinking Nighttime Image Deraining via Learnable Color Space Transformation",
      "authors": [
        "Qiyuan Guan",
        "Xiang Chen",
        "Guiyue Jin",
        "Jiyu Jin",
        "Shumin Fan",
        "Tianyu Song",
        "Jinshan Pan"
      ],
      "arxiv_id": "2510.17440v1",
      "summary": "Compared to daytime image deraining, nighttime image deraining poses\nsignificant challenges due to inherent complexities of nighttime scenarios and\nthe lack of high-quality datasets that accurately represent the coupling effect\nbetween rain and illumination. In this paper, we rethink the task of nighttime\nimage deraining and contribute a new high-quality benchmark, HQ-NightRain,\nwhich offers higher harmony and realism compared to existing datasets. In\naddition, we develop an effective Color Space Transformation Network (CST-Net)\nfor better removing complex rain from nighttime scenes. Specifically, we\npropose a learnable color space converter (CSC) to better facilitate rain\nremoval in the Y channel, as nighttime rain is more pronounced in the Y channel\ncompared to the RGB color space. To capture illumination information for\nguiding nighttime deraining, implicit illumination guidance is introduced\nenabling the learned features to improve the model's robustness in complex\nscenarios. Extensive experiments show the value of our dataset and the\neffectiveness of our method. The source code and datasets are available at\nhttps://github.com/guanqiyuan/CST-Net."
    },
    {
      "title": "From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors",
      "authors": [
        "Zhengshen Zhang",
        "Hao Li",
        "Yalun Dai",
        "Zhengbang Zhu",
        "Lei Zhou",
        "Chenchen Liu",
        "Dong Wang",
        "Francis E. H. Tay",
        "Sijin Chen",
        "Ziwei Liu",
        "Yuxiao Liu",
        "Xinghang Li",
        "Pan Zhou"
      ],
      "arxiv_id": "2510.17439v1",
      "summary": "Existing vision-language-action (VLA) models act in 3D real-world but are\ntypically built on 2D encoders, leaving a spatial reasoning gap that limits\ngeneralization and adaptability. Recent 3D integration techniques for VLAs\neither require specialized sensors and transfer poorly across modalities, or\ninject weak cues that lack geometry and degrade vision-language alignment. In\nthis work, we introduce FALCON (From Spatial to Action), a novel paradigm that\ninjects rich 3D spatial tokens into the action head. FALCON leverages spatial\nfoundation models to deliver strong geometric priors from RGB alone, and\nincludes an Embodied Spatial Model that can optionally fuse depth, or pose for\nhigher fidelity when available, without retraining or architectural changes. To\npreserve language reasoning, spatial tokens are consumed by a Spatial-Enhanced\nAction Head rather than being concatenated into the vision-language backbone.\nThese designs enable FALCON to address limitations in spatial representation,\nmodality transferability, and alignment. In comprehensive evaluations across\nthree simulation benchmarks and eleven real-world tasks, our proposed FALCON\nachieves state-of-the-art performance, consistently surpasses competitive\nbaselines, and remains robust under clutter, spatial-prompt conditioning, and\nvariations in object scale and height."
    },
    {
      "title": "Leveraging AV1 motion vectors for Fast and Dense Feature Matching",
      "authors": [
        "Julien Zouein",
        "Hossein Javidnia",
        "François Pitié",
        "Anil Kokaram"
      ],
      "arxiv_id": "2510.17434v1",
      "summary": "We repurpose AV1 motion vectors to produce dense sub-pixel correspondences\nand short tracks filtered by cosine consistency. On short videos, this\ncompressed-domain front end runs comparably to sequential SIFT while using far\nless CPU, and yields denser matches with competitive pairwise geometry. As a\nsmall SfM demo on a 117-frame clip, MV matches register all images and\nreconstruct 0.46-0.62M points at 0.51-0.53,px reprojection error; BA time grows\nwith match density. These results show compressed-domain correspondences are a\npractical, resource-efficient front end with clear paths to scaling in full\npipelines."
    },
    {
      "title": "DeepDetect: Learning All-in-One Dense Keypoints",
      "authors": [
        "Shaharyar Ahmed Khan Tareen",
        "Filza Khan Tareen"
      ],
      "arxiv_id": "2510.17422v1",
      "summary": "Keypoint detection is the foundation of many computer vision tasks, including\nimage registration, structure-from motion, 3D reconstruction, visual odometry,\nand SLAM. Traditional detectors (SIFT, SURF, ORB, BRISK, etc.) and learning\nbased methods (SuperPoint, R2D2, LF-Net, D2-Net, etc.) have shown strong\nperformance yet suffer from key limitations: sensitivity to photometric\nchanges, low keypoint density and repeatability, limited adaptability to\nchallenging scenes, and lack of semantic understanding, often failing to\nprioritize visually important regions. We present DeepDetect, an intelligent,\nall-in-one, dense keypoint detector that unifies the strengths of classical\ndetectors using deep learning. Firstly, we create ground-truth masks by fusing\noutputs of 7 keypoint and 2 edge detectors, extracting diverse visual cues from\ncorners and blobs to prominent edges and textures in the images. Afterwards, a\nlightweight and efficient model: ESPNet, is trained using these masks as\nlabels, enabling DeepDetect to focus semantically on images while producing\nhighly dense keypoints, that are adaptable to diverse and visually degraded\nconditions. Evaluations on the Oxford Affine Covariant Regions dataset\ndemonstrate that DeepDetect surpasses other detectors in keypoint density,\nrepeatability, and the number of correct matches, achieving maximum values of\n0.5143 (average keypoint density), 0.9582 (average repeatability), and 59,003\n(correct matches)."
    },
    {
      "title": "Monitoring Horses in Stalls: From Object to Event Detection",
      "authors": [
        "Dmitrii Galimzianov",
        "Viacheslav Vyshegorodtsev",
        "Ivan Nezhivykh"
      ],
      "arxiv_id": "2510.17409v1",
      "summary": "Monitoring the behavior of stalled horses is essential for early detection of\nhealth and welfare issues but remains labor-intensive and time-consuming. In\nthis study, we present a prototype vision-based monitoring system that\nautomates the detection and tracking of horses and people inside stables using\nobject detection and multi-object tracking techniques. The system leverages\nYOLOv11 and BoT-SORT for detection and tracking, while event states are\ninferred based on object trajectories and spatial relations within the stall.\nTo support development, we constructed a custom dataset annotated with\nassistance from foundation models CLIP and GroundingDINO. The system\ndistinguishes between five event types and accounts for the camera's blind\nspots. Qualitative evaluation demonstrated reliable performance for\nhorse-related events, while highlighting limitations in detecting people due to\ndata scarcity. This work provides a foundation for real-time behavioral\nmonitoring in equine facilities, with implications for animal welfare and\nstable management."
    },
    {
      "title": "Integrating Trustworthy Artificial Intelligence with Energy-Efficient Robotic Arms for Waste Sorting",
      "authors": [
        "Halima I. Kure",
        "Jishna Retnakumari",
        "Augustine O. Nwajana",
        "Umar M. Ismail",
        "Bilyaminu A. Romo",
        "Ehigiator Egho-Promise"
      ],
      "arxiv_id": "2510.17408v1",
      "summary": "This paper presents a novel methodology that integrates trustworthy\nartificial intelligence (AI) with an energy-efficient robotic arm for\nintelligent waste classification and sorting. By utilizing a convolutional\nneural network (CNN) enhanced through transfer learning with MobileNetV2, the\nsystem accurately classifies waste into six categories: plastic, glass, metal,\npaper, cardboard, and trash. The model achieved a high training accuracy of\n99.8% and a validation accuracy of 80.5%, demonstrating strong learning and\ngeneralization. A robotic arm simulator is implemented to perform virtual\nsorting, calculating the energy cost for each action using Euclidean distance\nto ensure optimal and efficient movement. The framework incorporates key\nelements of trustworthy AI, such as transparency, robustness, fairness, and\nsafety, making it a reliable and scalable solution for smart waste management\nsystems in urban settings."
    },
    {
      "title": "MILES: Modality-Informed Learning Rate Scheduler for Balancing Multimodal Learning",
      "authors": [
        "Alejandro Guerra-Manzanares",
        "Farah E. Shamout"
      ],
      "arxiv_id": "2510.17394v1",
      "summary": "The aim of multimodal neural networks is to combine diverse data sources,\nreferred to as modalities, to achieve enhanced performance compared to relying\non a single modality. However, training of multimodal networks is typically\nhindered by modality overfitting, where the network relies excessively on one\nof the available modalities. This often yields sub-optimal performance,\nhindering the potential of multimodal learning and resulting in marginal\nimprovements relative to unimodal models. In this work, we present the\nModality-Informed Learning ratE Scheduler (MILES) for training multimodal joint\nfusion models in a balanced manner. MILES leverages the differences in\nmodality-wise conditional utilization rates during training to effectively\nbalance multimodal learning. The learning rate is dynamically adjusted during\ntraining to balance the speed of learning from each modality by the multimodal\nmodel, aiming for enhanced performance in both multimodal and unimodal\npredictions. We extensively evaluate MILES on four multimodal joint fusion\ntasks and compare its performance to seven state-of-the-art baselines. Our\nresults show that MILES outperforms all baselines across all tasks and fusion\nmethods considered in our study, effectively balancing modality usage during\ntraining. This results in improved multimodal performance and stronger modality\nencoders, which can be leveraged when dealing with unimodal samples or absent\nmodalities. Overall, our work highlights the impact of balancing multimodal\nlearning on improving model performance."
    },
    {
      "title": "Closed-Loop Transfer for Weakly-supervised Affordance Grounding",
      "authors": [
        "Jiajin Tang",
        "Zhengxuan Wei",
        "Ge Zheng",
        "Sibei Yang"
      ],
      "arxiv_id": "2510.17384v1",
      "summary": "Humans can perform previously unexperienced interactions with novel objects\nsimply by observing others engage with them. Weakly-supervised affordance\ngrounding mimics this process by learning to locate object regions that enable\nactions on egocentric images, using exocentric interaction images with\nimage-level annotations. However, extracting affordance knowledge solely from\nexocentric images and transferring it one-way to egocentric images limits the\napplicability of previous works in complex interaction scenarios. Instead, this\nstudy introduces LoopTrans, a novel closed-loop framework that not only\ntransfers knowledge from exocentric to egocentric but also transfers back to\nenhance exocentric knowledge extraction. Within LoopTrans, several innovative\nmechanisms are introduced, including unified cross-modal localization and\ndenoising knowledge distillation, to bridge domain gaps between object-centered\negocentric and interaction-centered exocentric images while enhancing knowledge\ntransfer. Experiments show that LoopTrans achieves consistent improvements\nacross all metrics on image and video benchmarks, even handling challenging\nscenarios where object interaction regions are fully occluded by the human\nbody."
    },
    {
      "title": "Latent Spaces Beyond Synthesis: From GANs to Diffusion Models",
      "authors": [
        "Ludovica Schaerf"
      ],
      "arxiv_id": "2510.17383v1",
      "summary": "This paper examines the evolving nature of internal representations in\ngenerative visual models, focusing on the conceptual and technical shift from\nGANs and VAEs to diffusion-based architectures. Drawing on Beatrice Fazi's\naccount of synthesis as the amalgamation of distributed representations, we\npropose a distinction between \"synthesis in a strict sense\", where a compact\nlatent space wholly determines the generative process, and \"synthesis in a\nbroad sense,\" which characterizes models whose representational labor is\ndistributed across layers. Through close readings of model architectures and a\ntargeted experimental setup that intervenes in layerwise representations, we\nshow how diffusion models fragment the burden of representation and thereby\nchallenge assumptions of unified internal space. By situating these findings\nwithin media theoretical frameworks and critically engaging with metaphors such\nas the latent space and the Platonic Representation Hypothesis, we argue for a\nreorientation of how generative AI is understood: not as a direct synthesis of\ncontent, but as an emergent configuration of specialized processes."
    },
    {
      "title": "Graph Attention-Guided Search for Dense Multi-Agent Pathfinding",
      "authors": [
        "Rishabh Jain",
        "Keisuke Okumura",
        "Michael Amir",
        "Amanda Prorok"
      ],
      "arxiv_id": "2510.17382v1",
      "summary": "Finding near-optimal solutions for dense multi-agent pathfinding (MAPF)\nproblems in real-time remains challenging even for state-of-the-art planners.\nTo this end, we develop a hybrid framework that integrates a learned heuristic\nderived from MAGAT, a neural MAPF policy with a graph attention scheme, into a\nleading search-based algorithm, LaCAM. While prior work has explored\nlearning-guided search in MAPF, such methods have historically underperformed.\nIn contrast, our approach, termed LaGAT, outperforms both purely search-based\nand purely learning-based methods in dense scenarios. This is achieved through\nan enhanced MAGAT architecture, a pre-train-then-fine-tune strategy on maps of\ninterest, and a deadlock detection scheme to account for imperfect neural\nguidance. Our results demonstrate that, when carefully designed, hybrid search\noffers a powerful solution for tightly coupled, challenging multi-agent\ncoordination problems."
    },
    {
      "title": "Facial Expression-based Parkinson's Disease Severity Diagnosis via Feature Fusion and Adaptive Class Balancing",
      "authors": [
        "Yintao Zhou",
        "Wei Huang",
        "Zhengyu Li",
        "Jing Huang",
        "Meng Pang"
      ],
      "arxiv_id": "2510.17373v1",
      "summary": "Parkinson's disease (PD) severity diagnosis is crucial for early detecting\npotential patients and adopting tailored interventions. Diagnosing PD based on\nfacial expression is grounded in PD patients' \"masked face\" symptom and gains\ngrowing interest recently for its convenience and affordability. However,\ncurrent facial expression-based approaches often rely on single type of\nexpression which can lead to misdiagnosis, and ignore the class imbalance\nacross different PD stages which degrades the prediction performance. Moreover,\nmost existing methods focus on binary classification (i.e., PD / non-PD) rather\nthan diagnosing the severity of PD. To address these issues, we propose a new\nfacial expression-based method for PD severity diagnosis which integrates\nmultiple facial expression features through attention-based feature fusion.\nMoreover, we mitigate the class imbalance problem via an adaptive class\nbalancing strategy which dynamically adjusts the contribution of training\nsamples based on their class distribution and classification difficulty.\nExperimental results demonstrate the promising performance of the proposed\nmethod for PD severity diagnosis, as well as the efficacy of attention-based\nfeature fusion and adaptive class balancing."
    },
    {
      "title": "Beyond Real Faces: Synthetic Datasets Can Achieve Reliable Recognition Performance without Privacy Compromise",
      "authors": [
        "Paweł Borsukiewicz",
        "Fadi Boutros",
        "Iyiola E. Olatunji",
        "Charles Beumier",
        "Wendkûuni C. Ouedraogo",
        "Jacques Klein",
        "Tegawendé F. Bissyandé"
      ],
      "arxiv_id": "2510.17372v1",
      "summary": "The deployment of facial recognition systems has created an ethical dilemma:\nachieving high accuracy requires massive datasets of real faces collected\nwithout consent, leading to dataset retractions and potential legal liabilities\nunder regulations like GDPR. While synthetic facial data presents a promising\nprivacy-preserving alternative, the field lacks comprehensive empirical\nevidence of its viability. This study addresses this critical gap through\nextensive evaluation of synthetic facial recognition datasets. We present a\nsystematic literature review identifying 25 synthetic facial recognition\ndatasets (2018-2025), combined with rigorous experimental validation. Our\nmethodology examines seven key requirements for privacy-preserving synthetic\ndata: identity leakage prevention, intra-class variability, identity\nseparability, dataset scale, ethical data sourcing, bias mitigation, and\nbenchmark reliability. Through experiments involving over 10 million synthetic\nsamples, extended by a comparison of results reported on five standard\nbenchmarks, we provide the first comprehensive empirical assessment of\nsynthetic data's capability to replace real datasets. Best-performing synthetic\ndatasets (VariFace, VIGFace) achieve recognition accuracies of 95.67% and\n94.91% respectively, surpassing established real datasets including\nCASIA-WebFace (94.70%). While those images remain private, publicly available\nalternatives Vec2Face (93.52%) and CemiFace (93.22%) come close behind. Our\nfindings reveal that they ensure proper intra-class variability while\nmaintaining identity separability. Demographic bias analysis shows that, even\nthough synthetic data inherits limited biases, it offers unprecedented control\nfor bias mitigation through generation parameters. These results establish\nsynthetic facial data as a scientifically viable and ethically imperative\nalternative for facial recognition research."
    },
    {
      "title": "Bridging Embodiment Gaps: Deploying Vision-Language-Action Models on Soft Robots",
      "authors": [
        "Haochen Su",
        "Cristian Meo",
        "Francesco Stella",
        "Andrea Peirone",
        "Kai Junge",
        "Josie Hughes"
      ],
      "arxiv_id": "2510.17369v1",
      "summary": "Robotic systems are increasingly expected to operate in human-centered,\nunstructured environments where safety, adaptability, and generalization are\nessential. Vision-Language-Action (VLA) models have been proposed as a language\nguided generalized control framework for real robots. However, their deployment\nhas been limited to conventional serial link manipulators. Coupled by their\nrigidity and unpredictability of learning based control, the ability to safely\ninteract with the environment is missing yet critical. In this work, we present\nthe deployment of a VLA model on a soft continuum manipulator to demonstrate\nautonomous safe human-robot interaction. We present a structured finetuning and\ndeployment pipeline evaluating two state-of-the-art VLA models (OpenVLA-OFT and\n$\\pi_0$) across representative manipulation tasks, and show while\nout-of-the-box policies fail due to embodiment mismatch, through targeted\nfinetuning the soft robot performs equally to the rigid counterpart. Our\nfindings highlight the necessity of finetuning for bridging embodiment gaps,\nand demonstrate that coupling VLA models with soft robots enables safe and\nflexible embodied AI in human-shared environments."
    },
    {
      "title": "Recurrent Attention-based Token Selection for Efficient Streaming Video-LLMs",
      "authors": [
        "Vaggelis Dorovatas",
        "Soroush Seifi",
        "Gunshi Gupta",
        "Rahaf Aljundi"
      ],
      "arxiv_id": "2510.17364v1",
      "summary": "Video Large Language Models (Video-LLMs) excel at understanding videos\nin-context, provided they have full access to the video when answering queries.\nHowever, these models face challenges in streaming scenarios where hour-long\nvideos must be processed online, and questions need timely responses. In this\nwork, we propose a training-free approach compatible with standard Video-LLMs,\nleveraging three key concepts: 1) LLM-informed selection of visual tokens to\nidentify those that the LLM has attended to and contributed to its\nunderstanding of each short clip. Our attention-based selection allows us to\ndiscard up to ~95% of unimportant visual tokens with minimal performance loss;\n2) Recurrent processing of past selected tokens to generate temporally coherent\nunderstanding of each processed clip; 3) Caption-based question answering for\nlightweight and accurate responses. Our method achieves state-of-the-art\nperformance on streaming video benchmarks, striking a balance between\nefficiency and effectiveness."
    },
    {
      "title": "M2H: Multi-Task Learning with Efficient Window-Based Cross-Task Attention for Monocular Spatial Perception",
      "authors": [
        "U. V. B. L Udugama",
        "George Vosselman",
        "Francesco Nex"
      ],
      "arxiv_id": "2510.17363v1",
      "summary": "Deploying real-time spatial perception on edge devices requires efficient\nmulti-task models that leverage complementary task information while minimizing\ncomputational overhead. This paper introduces Multi-Mono-Hydra (M2H), a novel\nmulti-task learning framework designed for semantic segmentation and depth,\nedge, and surface normal estimation from a single monocular image. Unlike\nconventional approaches that rely on independent single-task models or shared\nencoder-decoder architectures, M2H introduces a Window-Based Cross-Task\nAttention Module that enables structured feature exchange while preserving\ntask-specific details, improving prediction consistency across tasks. Built on\na lightweight ViT-based DINOv2 backbone, M2H is optimized for real-time\ndeployment and serves as the foundation for monocular spatial perception\nsystems supporting 3D scene graph construction in dynamic environments.\nComprehensive evaluations show that M2H outperforms state-of-the-art multi-task\nmodels on NYUDv2, surpasses single-task depth and semantic baselines on\nHypersim, and achieves superior performance on the Cityscapes dataset, all\nwhile maintaining computational efficiency on laptop hardware. Beyond\nbenchmarks, M2H is validated on real-world data, demonstrating its practicality\nin spatial perception tasks."
    },
    {
      "title": "Exploring The Missing Semantics In Event Modality",
      "authors": [
        "Jingqian Wu",
        "Shengpeng Xu",
        "Yunbo Jia",
        "Edmund Y. Lam"
      ],
      "arxiv_id": "2510.17347v1",
      "summary": "Event cameras offer distinct advantages such as low latency, high dynamic\nrange, and efficient motion capture. However, event-to-video reconstruction\n(E2V), a fundamental event-based vision task, remains challenging, particularly\nfor reconstructing and recovering semantic information. This is primarily due\nto the nature of the event camera, as it only captures intensity changes,\nignoring static objects and backgrounds, resulting in a lack of semantic\ninformation in captured event modality. Further, semantic information plays a\ncrucial role in video and frame reconstruction, yet is often overlooked by\nexisting E2V approaches. To bridge this gap, we propose Semantic-E2VID, an E2V\nframework that explores the missing visual semantic knowledge in event modality\nand leverages it to enhance event-to-video reconstruction. Specifically,\nSemantic-E2VID introduces a cross-modal feature alignment (CFA) module to\ntransfer the robust visual semantics from a frame-based vision foundation\nmodel, the Segment Anything Model (SAM), to the event encoder, while aligning\nthe high-level features from distinct modalities. To better utilize the learned\nsemantic feature, we further propose a semantic-aware feature fusion (SFF)\nblock to integrate learned semantics in frame modality to form event\nrepresentations with rich semantics that can be decoded by the event decoder.\nFurther, to facilitate the reconstruction of semantic information, we propose a\nnovel Semantic Perceptual E2V Supervision that helps the model to reconstruct\nsemantic details by leveraging SAM-generated categorical labels. Extensive\nexperiments demonstrate that Semantic-E2VID significantly enhances frame\nquality, outperforming state-of-the-art E2V methods across multiple benchmarks.\nThe sample code is included in the supplementary material."
    },
    {
      "title": "Interactive Force-Impedance Control",
      "authors": [
        "Fan Shao",
        "Satoshi Endo",
        "Sandra Hirche",
        "Fanny Ficuciello"
      ],
      "arxiv_id": "2510.17341v1",
      "summary": "Human collaboration with robots requires flexible role adaptation, enabling\nrobot to switch between active leader and passive follower. Effective role\nswitching depends on accurately estimating human intention, which is typically\nachieved through external force analysis, nominal robot dynamics, or\ndata-driven approaches. However, these methods are primarily effective in\ncontact-sparse environments. When robots under hybrid or unified\nforce-impedance control physically interact with active humans or non-passive\nenvironments, the robotic system may lose passivity and thus compromise safety.\nTo address this challenge, this paper proposes the unified Interactive\nForce-Impedance Control (IFIC) framework that adapts to the interaction power\nflow, ensuring effortless and safe interaction in contact-rich environments.\nThe proposed control architecture is formulated within a port-Hamiltonian\nframework, incorporating both interaction and task control ports, through which\nsystem passivity is guaranteed."
    },
    {
      "title": "Nearest-Class Mean and Logits Agreement for Wildlife Open-Set Recognition",
      "authors": [
        "Jiahao Huo",
        "Mufhumudzi Muthivhi",
        "Terence L. van Zyl",
        "Fredrik Gustafsson"
      ],
      "arxiv_id": "2510.17338v1",
      "summary": "Current state-of-the-art Wildlife classification models are trained under the\nclosed world setting. When exposed to unknown classes, they remain\noverconfident in their predictions. Open-set Recognition (OSR) aims to classify\nknown classes while rejecting unknown samples. Several OSR methods have been\nproposed to model the closed-set distribution by observing the feature, logit,\nor softmax probability space. A significant drawback of many existing\napproaches is the requirement to retrain the pre-trained classification model\nwith the OSR-specific strategy. This study contributes a post-processing OSR\nmethod that measures the agreement between the models' features and predicted\nlogits. We propose a probability distribution based on an input's distance to\nits Nearest Class Mean (NCM). The NCM-based distribution is then compared with\nthe softmax probabilities from the logit space to measure agreement between the\nNCM and the classification head. Our proposed strategy ranks within the top\nthree on two evaluated datasets, showing consistent performance across the two\ndatasets. In contrast, current state-of-the-art methods excel on a single\ndataset. We achieve an AUROC of 93.41 and 95.35 for African and Swedish\nanimals. The code can be found\nhttps://github.com/Applied-Representation-Learning-Lab/OSR."
    },
    {
      "title": "DDBot: Differentiable Physics-based Digging Robot for Unknown Granular Materials",
      "authors": [
        "Xintong Yang",
        "Minglun Wei",
        "Ze Ji",
        "Yu-Kun Lai"
      ],
      "arxiv_id": "2510.17335v1",
      "summary": "Automating the manipulation of granular materials poses significant\nchallenges due to complex contact dynamics, unpredictable material properties,\nand intricate system states. Existing approaches often fail to achieve\nefficiency and accuracy in such tasks. To fill the research gap, this paper\nstudies the small-scale and high-precision granular material digging task with\nunknown physical properties. A new framework, named differentiable digging\nrobot (DDBot), is proposed to manipulate granular materials, including sand and\nsoil.\n  Specifically, we equip DDBot with a differentiable physics-based simulator,\ntailored for granular material manipulation, powered by GPU-accelerated\nparallel computing and automatic differentiation. DDBot can perform efficient\ndifferentiable system identification and high-precision digging skill\noptimisation for unknown granular materials, which is enabled by a\ndifferentiable skill-to-action mapping, a task-oriented demonstration method,\ngradient clipping and line search-based gradient descent.\n  Experimental results show that DDBot can efficiently (converge within 5 to 20\nminutes) identify unknown granular material dynamics and optimise digging\nskills, with high-precision results in zero-shot real-world deployments,\nhighlighting its practicality. Benchmark results against state-of-the-art\nbaselines also confirm the robustness and efficiency of DDBot in such digging\ntasks."
    },
    {
      "title": "iDETEX: Empowering MLLMs for Intelligent DETailed EXplainable IQA",
      "authors": [
        "Zhaoran Zhao",
        "Xinli Yue",
        "Jianhui Sun",
        "Yuhao Xie",
        "Tao Shao",
        "Liangchao Yao",
        "Fan Xia",
        "Yuetang Deng"
      ],
      "arxiv_id": "2510.17332v1",
      "summary": "Image Quality Assessment (IQA) has progressed from scalar quality prediction\nto more interpretable, human-aligned evaluation paradigms. In this work, we\naddress the emerging challenge of detailed and explainable IQA by proposing\niDETEX-a unified multimodal large language model (MLLM) capable of\nsimultaneously performing three key tasks: quality grounding, perception, and\ndescription. To facilitate efficient and generalizable training across these\nheterogeneous subtasks, we design a suite of task-specific offline augmentation\nmodules and a data mixing strategy. These are further complemented by online\nenhancement strategies to fully exploit multi-sourced supervision. We validate\nour approach on the large-scale ViDA-UGC benchmark, where iDETEX achieves\nstate-of-the-art performance across all subtasks. Our model ranks first in the\nICCV MIPI 2025 Detailed Image Quality Assessment Challenge, demonstrating its\neffectiveness and robustness in delivering accurate and interpretable quality\nassessments."
    },
    {
      "title": "CharDiff: A Diffusion Model with Character-Level Guidance for License Plate Image Restoration",
      "authors": [
        "Gyuhwan Park",
        "Kihyun Na",
        "Injung Kim"
      ],
      "arxiv_id": "2510.17330v1",
      "summary": "The significance of license plate image restoration goes beyond the\npreprocessing stage of License Plate Recognition (LPR) systems, as it also\nserves various purposes, including increasing evidential value, enhancing the\nclarity of visual interface, and facilitating further utilization of license\nplate images. We propose a novel diffusion-based framework with character-level\nguidance, CharDiff, which effectively restores and recognizes severely degraded\nlicense plate images captured under realistic conditions. CharDiff leverages\nfine-grained character-level priors extracted through external segmentation and\nOptical Character Recognition (OCR) modules tailored for low-quality license\nplate images. For precise and focused guidance, CharDiff incorporates a novel\nCharacter-guided Attention through Region-wise Masking (CHARM) module, which\nensures that each character's guidance is restricted to its own region, thereby\navoiding interference with other regions. In experiments, CharDiff\nsignificantly outperformed the baseline restoration models in both restoration\nquality and recognition accuracy, achieving a 28% relative reduction in CER on\nthe Roboflow-LP dataset, compared to the best-performing baseline model. These\nresults indicate that the structured character-guided conditioning effectively\nenhances the robustness of diffusion-based license plate restoration and\nrecognition in practical deployment scenarios."
    },
    {
      "title": "A Single Set of Adversarial Clothes Breaks Multiple Defense Methods in the Physical World",
      "authors": [
        "Wei Zhang",
        "Zhanhao Hu",
        "Xiao Li",
        "Xiaopei Zhu",
        "Xiaolin Hu"
      ],
      "arxiv_id": "2510.17322v1",
      "summary": "In recent years, adversarial attacks against deep learning-based object\ndetectors in the physical world have attracted much attention. To defend\nagainst these attacks, researchers have proposed various defense methods\nagainst adversarial patches, a typical form of physically-realizable attack.\nHowever, our experiments showed that simply enlarging the patch size could make\nthese defense methods fail. Motivated by this, we evaluated various defense\nmethods against adversarial clothes which have large coverage over the human\nbody. Adversarial clothes provide a good test case for adversarial defense\nagainst patch-based attacks because they not only have large sizes but also\nlook more natural than a large patch on humans. Experiments show that all the\ndefense methods had poor performance against adversarial clothes in both the\ndigital world and the physical world. In addition, we crafted a single set of\nclothes that broke multiple defense methods on Faster R-CNN. The set achieved\nan Attack Success Rate (ASR) of 96.06% against the undefended detector and over\n64.84% ASRs against nine defended models in the physical world, unveiling the\ncommon vulnerability of existing adversarial defense methods against\nadversarial clothes. Code is available at:\nhttps://github.com/weiz0823/adv-clothes-break-multiple-defenses."
    },
    {
      "title": "CausalMamba: Scalable Conditional State Space Models for Neural Causal Inference",
      "authors": [
        "Sangyoon Bae",
        "Jiook Cha"
      ],
      "arxiv_id": "2510.17318v1",
      "summary": "We introduce CausalMamba, a scalable framework that addresses fundamental\nlimitations in fMRI-based causal inference: the ill-posed nature of inferring\nneural causality from hemodynamically distorted BOLD signals and the\ncomputational intractability of existing methods like Dynamic Causal Modeling\n(DCM). Our approach decomposes this complex inverse problem into two tractable\nstages: BOLD deconvolution to recover latent neural activity, followed by\ncausal graph inference using a novel Conditional Mamba architecture. On\nsimulated data, CausalMamba achieves 37% higher accuracy than DCM. Critically,\nwhen applied to real task fMRI data, our method recovers well-established\nneural pathways with 88% fidelity, whereas conventional approaches fail to\nidentify these canonical circuits in over 99% of subjects. Furthermore, our\nnetwork analysis of working memory data reveals that the brain strategically\nshifts its primary causal hub-recruiting executive or salience networks\ndepending on the stimulus-a sophisticated reconfiguration that remains\nundetected by traditional methods. This work provides neuroscientists with a\npractical tool for large-scale causal inference that captures both fundamental\ncircuit motifs and flexible network dynamics underlying cognitive function."
    },
    {
      "title": "Implicit State Estimation via Video Replanning",
      "authors": [
        "Po-Chen Ko",
        "Jiayuan Mao",
        "Yu-Hsiang Fu",
        "Hsien-Jeng Yeh",
        "Chu-Rong Chen",
        "Wei-Chiu Ma",
        "Yilun Du",
        "Shao-Hua Sun"
      ],
      "arxiv_id": "2510.17315v1",
      "summary": "Video-based representations have gained prominence in planning and\ndecision-making due to their ability to encode rich spatiotemporal dynamics and\ngeometric relationships. These representations enable flexible and\ngeneralizable solutions for complex tasks such as object manipulation and\nnavigation. However, existing video planning frameworks often struggle to adapt\nto failures at interaction time due to their inability to reason about\nuncertainties in partially observed environments. To overcome these\nlimitations, we introduce a novel framework that integrates interaction-time\ndata into the planning process. Our approach updates model parameters online\nand filters out previously failed plans during generation. This enables\nimplicit state estimation, allowing the system to adapt dynamically without\nexplicitly modeling unknown state variables. We evaluate our framework through\nextensive experiments on a new simulated manipulation benchmark, demonstrating\nits ability to improve replanning performance and advance the field of\nvideo-based decision-making."
    },
    {
      "title": "LongInsightBench: A Comprehensive Benchmark for Evaluating Omni-Modal Models on Human-Centric Long-Video Understanding",
      "authors": [
        "ZhaoYang Han",
        "Qihan Lin",
        "Hao Liang",
        "Bowen Chen",
        "Zhou Liu",
        "Wentao Zhang"
      ],
      "arxiv_id": "2510.17305v1",
      "summary": "We introduce \\textbf{LongInsightBench}, the first benchmark designed to\nassess models' ability to understand long videos, with a focus on human\nlanguage, viewpoints, actions, and other contextual elements, while integrating\n\\textbf{visual, audio, and text} modalities. Our benchmark excels in three key\nareas: \\textbf{a) Long-Duration, Information-Dense Videos:} We carefully select\napproximately 1,000 videos from open-source datasets FineVideo based on\nduration limit and the information density of both visual and audio modalities,\nfocusing on content like lectures, interviews, and vlogs, which contain rich\nlanguage elements. \\textbf{b) Diverse and Challenging Task Scenarios:} We have\ndesigned six challenging task scenarios, including both Intra-Event and\nInter-Event Tasks. \\textbf{c) Rigorous and Comprehensive Quality Assurance\nPipelines:} We have developed a three-step, semi-automated data quality\nassurance pipeline to ensure the difficulty and validity of the synthesized\nquestions and answer options. Based on LongInsightBench, we designed a series\nof experiments. Experimental results shows that Omni-modal models(OLMs) still\nface challenge in tasks requiring precise temporal localization (T-Loc) and\nlong-range causal inference (CE-Caus). Extended experiments reveal the\ninformation loss and processing bias in multi-modal fusion of OLMs. Our dataset\nand code is available at\nhttps://anonymous.4open.science/r/LongInsightBench-910F/."
    },
    {
      "title": "Exploring Structural Degradation in Dense Representations for Self-supervised Learning",
      "authors": [
        "Siran Dai",
        "Qianqian Xu",
        "Peisong Wen",
        "Yang Liu",
        "Qingming Huang"
      ],
      "arxiv_id": "2510.17299v1",
      "summary": "In this work, we observe a counterintuitive phenomenon in self-supervised\nlearning (SSL): longer training may impair the performance of dense prediction\ntasks (e.g., semantic segmentation). We refer to this phenomenon as\nSelf-supervised Dense Degradation (SDD) and demonstrate its consistent presence\nacross sixteen state-of-the-art SSL methods with various losses, architectures,\nand datasets. When the model performs suboptimally on dense tasks at the end of\ntraining, measuring the performance during training becomes essential. However,\nevaluating dense performance effectively without annotations remains an open\nchallenge. To tackle this issue, we introduce a Dense representation Structure\nEstimator (DSE), composed of a class-relevance measure and an effective\ndimensionality measure. The proposed DSE is both theoretically grounded and\nempirically validated to be closely correlated with the downstream performance.\nBased on this metric, we introduce a straightforward yet effective model\nselection strategy and a DSE-based regularization method. Experiments on\nsixteen SSL methods across four benchmarks confirm that model selection\nimproves mIoU by $3.0\\%$ on average with negligible computational cost.\nAdditionally, DSE regularization consistently mitigates the effects of dense\ndegradation. Code is available at\nhttps://github.com/EldercatSAM/SSL-Degradation."
    },
    {
      "title": "Machine Vision-Based Surgical Lighting System:Design and Implementation",
      "authors": [
        "Amir Gharghabi",
        "Mahdi Hakiminezhad",
        "Maryam Shafaei",
        "Shaghayegh Gharghabi"
      ],
      "arxiv_id": "2510.17287v1",
      "summary": "Effortless and ergonomically designed surgical lighting is critical for\nprecision and safety during procedures. However, traditional systems often rely\non manual adjustments, leading to surgeon fatigue, neck strain, and\ninconsistent illumination due to drift and shadowing. To address these\nchallenges, we propose a novel surgical lighting system that leverages the\nYOLOv11 object detection algorithm to identify a blue marker placed above the\ntarget surgical site. A high-power LED light source is then directed to the\nidentified location using two servomotors equipped with tilt-pan brackets. The\nYOLO model achieves 96.7% mAP@50 on the validation set consisting of annotated\nimages simulating surgical scenes with the blue spherical marker. By automating\nthe lighting process, this machine vision-based solution reduces physical\nstrain on surgeons, improves consistency in illumination, and supports improved\nsurgical outcomes."
    },
    {
      "title": "SG-CLDFF: A Novel Framework for Automated White Blood Cell Classification and Segmentation",
      "authors": [
        "Mehdi Zekriyapanah Gashti",
        "Mostafa Mohammadpour",
        "Ghasem Farjamnia"
      ],
      "arxiv_id": "2510.17278v1",
      "summary": "Accurate segmentation and classification of white blood cells (WBCs) in\nmicroscopic images are essential for diagnosis and monitoring of many\nhematological disorders, yet remain challenging due to staining variability,\ncomplex backgrounds, and class imbalance. In this paper, we introduce a novel\nSaliency-Guided Cross-Layer Deep Feature Fusion framework (SG-CLDFF) that\ntightly integrates saliency-driven preprocessing with multi-scale deep feature\naggregation to improve both robustness and interpretability for WBC analysis.\nSG-CLDFF first computes saliency priors to highlight candidate WBC regions and\nguide subsequent feature extraction. A lightweight hybrid backbone\n(EfficientSwin-style) produces multi-resolution representations, which are\nfused by a ResNeXt-CC-inspired cross-layer fusion module to preserve\ncomplementary information from shallow and deep layers. The network is trained\nin a multi-task setup with concurrent segmentation and cell-type classification\nheads, using class-aware weighted losses and saliency-alignment regularization\nto mitigate imbalance and suppress background activation. Interpretability is\nenforced through Grad-CAM visualizations and saliency consistency checks,\nallowing model decisions to be inspected at the regional level. We validate the\nframework on standard public benchmarks (BCCD, LISC, ALL-IDB), reporting\nconsistent gains in IoU, F1, and classification accuracy compared to strong CNN\nand transformer baselines. An ablation study also demonstrates the individual\ncontributions of saliency preprocessing and cross-layer fusion. SG-CLDFF offers\na practical and explainable path toward more reliable automated WBC analysis in\nclinical workflows."
    },
    {
      "title": "Enhanced Motion Forecasting with Plug-and-Play Multimodal Large Language Models",
      "authors": [
        "Katie Luo",
        "Jingwei Ji",
        "Tong He",
        "Runsheng Xu",
        "Yichen Xie",
        "Dragomir Anguelov",
        "Mingxing Tan"
      ],
      "arxiv_id": "2510.17274v1",
      "summary": "Current autonomous driving systems rely on specialized models for perceiving\nand predicting motion, which demonstrate reliable performance in standard\nconditions. However, generalizing cost-effectively to diverse real-world\nscenarios remains a significant challenge. To address this, we propose\nPlug-and-Forecast (PnF), a plug-and-play approach that augments existing motion\nforecasting models with multimodal large language models (MLLMs). PnF builds on\nthe insight that natural language provides a more effective way to describe and\nhandle complex scenarios, enabling quick adaptation to targeted behaviors. We\ndesign prompts to extract structured scene understanding from MLLMs and distill\nthis information into learnable embeddings to augment existing behavior\nprediction models. Our method leverages the zero-shot reasoning capabilities of\nMLLMs to achieve significant improvements in motion prediction performance,\nwhile requiring no fine-tuning -- making it practical to adopt. We validate our\napproach on two state-of-the-art motion forecasting models using the Waymo Open\nMotion Dataset and the nuScenes Dataset, demonstrating consistent performance\nimprovements across both benchmarks."
    },
    {
      "title": "Floating-Base Deep Lagrangian Networks",
      "authors": [
        "Lucas Schulze",
        "Juliano Decico Negri",
        "Victor Barasuol",
        "Vivian Suzano Medeiros",
        "Marcelo Becker",
        "Jan Peters",
        "Oleg Arenz"
      ],
      "arxiv_id": "2510.17270v1",
      "summary": "Grey-box methods for system identification combine deep learning with\nphysics-informed constraints, capturing complex dependencies while improving\nout-of-distribution generalization. Yet, despite the growing importance of\nfloating-base systems such as humanoids and quadrupeds, current grey-box models\nignore their specific physical constraints. For instance, the inertia matrix is\nnot only positive definite but also exhibits branch-induced sparsity and input\nindependence. Moreover, the 6x6 composite spatial inertia of the floating base\ninherits properties of single-rigid-body inertia matrices. As we show, this\nincludes the triangle inequality on the eigenvalues of the composite rotational\ninertia. To address the lack of physical consistency in deep learning models of\nfloating-base systems, we introduce a parameterization of inertia matrices that\nsatisfies all these constraints. Inspired by Deep Lagrangian Networks (DeLaN),\nwe train neural networks to predict physically plausible inertia matrices that\nminimize inverse dynamics error under Lagrangian mechanics. For evaluation, we\ncollected and released a dataset on multiple quadrupeds and humanoids. In these\nexperiments, our Floating-Base Deep Lagrangian Networks (FeLaN) achieve highly\ncompetitive performance on both simulated and real robots, while providing\ngreater physical interpretability."
    },
    {
      "title": "FineVision: Open Data Is All You Need",
      "authors": [
        "Luis Wiedmann",
        "Orr Zohar",
        "Amir Mahla",
        "Xiaohan Wang",
        "Rui Li",
        "Thibaud Frere",
        "Leandro von Werra",
        "Aritra Roy Gosthipaty",
        "Andrés Marafioti"
      ],
      "arxiv_id": "2510.17269v1",
      "summary": "The advancement of vision-language models (VLMs) is hampered by a fragmented\nlandscape of inconsistent and contaminated public datasets. We introduce\nFineVision, a meticulously collected, curated, and unified corpus of 24 million\nsamples - the largest open resource of its kind. We unify more than 200 sources\ninto 185 subsets via a semi-automated, human-in-the-loop pipeline: automation\nperforms bulk ingestion and schema mapping, while reviewers audit mappings and\nspot-check outputs to verify faithful consumption of annotations, appropriate\nformatting and diversity, and safety; issues trigger targeted fixes and\nre-runs. The workflow further applies rigorous de-duplication within and across\nsources and decontamination against 66 public benchmarks. FineVision also\nencompasses agentic/GUI tasks with a unified action space; reviewers validate\nschemas and inspect a sample of trajectories to confirm executable fidelity.\nModels trained on FineVision consistently outperform those trained on existing\nopen mixtures across a broad evaluation suite, underscoring the benefits of\nscale, data hygiene, and balanced automation with human oversight. We release\nthe corpus and curation tools to accelerate data-centric VLM research."
    },
    {
      "title": "Fair and Interpretable Deepfake Detection in Videos",
      "authors": [
        "Akihito Yoshii",
        "Ryosuke Sonoda",
        "Ramya Srinivasan"
      ],
      "arxiv_id": "2510.17264v1",
      "summary": "Existing deepfake detection methods often exhibit bias, lack transparency,\nand fail to capture temporal information, leading to biased decisions and\nunreliable results across different demographic groups. In this paper, we\npropose a fairness-aware deepfake detection framework that integrates temporal\nfeature learning and demographic-aware data augmentation to enhance fairness\nand interpretability. Our method leverages sequence-based clustering for\ntemporal modeling of deepfake videos and concept extraction to improve\ndetection reliability while also facilitating interpretable decisions for\nnon-expert users. Additionally, we introduce a demography-aware data\naugmentation method that balances underrepresented groups and applies\nfrequency-domain transformations to preserve deepfake artifacts, thereby\nmitigating bias and improving generalization. Extensive experiments on\nFaceForensics++, DFD, Celeb-DF, and DFDC datasets using state-of-the-art (SoTA)\narchitectures (Xception, ResNet) demonstrate the efficacy of the proposed\nmethod in obtaining the best tradeoff between fairness and accuracy when\ncompared to SoTA."
    },
    {
      "title": "High-Level Multi-Robot Trajectory Planning And Spurious Behavior Detection",
      "authors": [
        "Fernando Salanova",
        "Jesús Roche",
        "Cristian Mahuela",
        "Eduardo Montijano"
      ],
      "arxiv_id": "2510.17261v1",
      "summary": "The reliable execution of high-level missions in multi-robot systems with\nheterogeneous agents, requires robust methods for detecting spurious behaviors.\nIn this paper, we address the challenge of identifying spurious executions of\nplans specified as a Linear Temporal Logic (LTL) formula, as incorrect task\nsequences, violations of spatial constraints, timing inconsis- tencies, or\ndeviations from intended mission semantics. To tackle this, we introduce a\nstructured data generation framework based on the Nets-within-Nets (NWN)\nparadigm, which coordinates robot actions with LTL-derived global mission\nspecifications. We further propose a Transformer-based anomaly detection\npipeline that classifies robot trajectories as normal or anomalous. Experi-\nmental evaluations show that our method achieves high accuracy (91.3%) in\nidentifying execution inefficiencies, and demonstrates robust detection\ncapabilities for core mission violations (88.3%) and constraint-based adaptive\nanomalies (66.8%). An ablation experiment of the embedding and architecture was\ncarried out, obtaining successful results where our novel proposition performs\nbetter than simpler representations."
    },
    {
      "title": "An adaptive hierarchical control framework for quadrupedal robots in planetary exploration",
      "authors": [
        "Franek Stark",
        "Rohit Kumar",
        "Shubham Vyas",
        "Hannah Isermann",
        "Jonas Haack",
        "Mihaela Popescu",
        "Jakob Middelberg",
        "Dennis Mronga",
        "Frank Kirchner"
      ],
      "arxiv_id": "2510.17249v1",
      "summary": "Planetary exploration missions require robots capable of navigating extreme\nand unknown environments. While wheeled rovers have dominated past missions,\ntheir mobility is limited to traversable surfaces. Legged robots, especially\nquadrupeds, can overcome these limitations by handling uneven, obstacle-rich,\nand deformable terrains. However, deploying such robots in unknown conditions\nis challenging due to the need for environment-specific control, which is\ninfeasible when terrain and robot parameters are uncertain. This work presents\na modular control framework that combines model-based dynamic control with\nonline model adaptation and adaptive footstep planning to address uncertainties\nin both robot and terrain properties. The framework includes state estimation\nfor quadrupeds with and without contact sensing, supports runtime\nreconfiguration, and is integrated into ROS 2 with open-source availability.\nIts performance was validated on two quadruped platforms, multiple hardware\narchitectures, and in a volcano field test, where the robot walked over 700 m."
    },
    {
      "title": "From Preferences to Prejudice: The Role of Alignment Tuning in Shaping Social Bias in Video Diffusion Models",
      "authors": [
        "Zefan Cai",
        "Haoyi Qiu",
        "Haozhe Zhao",
        "Ke Wan",
        "Jiachen Li",
        "Jiuxiang Gu",
        "Wen Xiao",
        "Nanyun Peng",
        "Junjie Hu"
      ],
      "arxiv_id": "2510.17247v1",
      "summary": "Recent advances in video diffusion models have significantly enhanced\ntext-to-video generation, particularly through alignment tuning using reward\nmodels trained on human preferences. While these methods improve visual\nquality, they can unintentionally encode and amplify social biases. To\nsystematically trace how such biases evolve throughout the alignment pipeline,\nwe introduce VideoBiasEval, a comprehensive diagnostic framework for evaluating\nsocial representation in video generation. Grounded in established social bias\ntaxonomies, VideoBiasEval employs an event-based prompting strategy to\ndisentangle semantic content (actions and contexts) from actor attributes\n(gender and ethnicity). It further introduces multi-granular metrics to\nevaluate (1) overall ethnicity bias, (2) gender bias conditioned on ethnicity,\n(3) distributional shifts in social attributes across model variants, and (4)\nthe temporal persistence of bias within videos. Using this framework, we\nconduct the first end-to-end analysis connecting biases in human preference\ndatasets, their amplification in reward models, and their propagation through\nalignment-tuned video diffusion models. Our results reveal that alignment\ntuning not only strengthens representational biases but also makes them\ntemporally stable, producing smoother yet more stereotyped portrayals. These\nfindings highlight the need for bias-aware evaluation and mitigation throughout\nthe alignment process to ensure fair and socially responsible video generation."
    },
    {
      "title": "Pole-Image: A Self-Supervised Pole-Anchored Descriptor for Long-Term LiDAR Localization and Map Maintenance",
      "authors": [
        "Wuhao Xie",
        "Kanji Tanaka"
      ],
      "arxiv_id": "2510.17237v1",
      "summary": "Long-term autonomy for mobile robots requires both robust self-localization\nand reliable map maintenance. Conventional landmark-based methods face a\nfundamental trade-off between landmarks with high detectability but low\ndistinctiveness (e.g., poles) and those with high distinctiveness but difficult\nstable detection (e.g., local point cloud structures). This work addresses the\nchallenge of descriptively identifying a unique \"signature\" (local point cloud)\nby leveraging a detectable, high-precision \"anchor\" (like a pole). To solve\nthis, we propose a novel canonical representation, \"Pole-Image,\" as a hybrid\nmethod that uses poles as anchors to generate signatures from the surrounding\n3D structure. Pole-Image represents a pole-like landmark and its surrounding\nenvironment, detected from a LiDAR point cloud, as a 2D polar coordinate image\nwith the pole itself as the origin. This representation leverages the pole's\nnature as a high-precision reference point, explicitly encoding the \"relative\ngeometry\" between the stable pole and the variable surrounding point cloud. The\nkey advantage of pole landmarks is that \"detection\" is extremely easy. This\nease of detection allows the robot to easily track the same pole, enabling the\nautomatic and large-scale collection of diverse observational data (positive\npairs). This data acquisition feasibility makes \"Contrastive Learning (CL)\"\napplicable. By applying CL, the model learns a viewpoint-invariant and highly\ndiscriminative descriptor. The contributions are twofold: 1) The descriptor\novercomes perceptual aliasing, enabling robust self-localization. 2) The\nhigh-precision encoding enables high-sensitivity change detection, contributing\nto map maintenance."
    },
    {
      "title": "Taming Modality Entanglement in Continual Audio-Visual Segmentation",
      "authors": [
        "Yuyang Hong",
        "Qi Yang",
        "Tao Zhang",
        "Zili Wang",
        "Zhaojin Fu",
        "Kun Ding",
        "Bin Fan",
        "Shiming Xiang"
      ],
      "arxiv_id": "2510.17234v1",
      "summary": "Recently, significant progress has been made in multi-modal continual\nlearning, aiming to learn new tasks sequentially in multi-modal settings while\npreserving performance on previously learned ones. However, existing methods\nmainly focus on coarse-grained tasks, with limitations in addressing modality\nentanglement in fine-grained continual learning settings. To bridge this gap,\nwe introduce a novel Continual Audio-Visual Segmentation (CAVS) task, aiming to\ncontinuously segment new classes guided by audio. Through comprehensive\nanalysis, two critical challenges are identified: 1) multi-modal semantic\ndrift, where a sounding objects is labeled as background in sequential tasks;\n2) co-occurrence confusion, where frequent co-occurring classes tend to be\nconfused. In this work, a Collision-based Multi-modal Rehearsal (CMR) framework\nis designed to address these challenges. Specifically, for multi-modal semantic\ndrift, a Multi-modal Sample Selection (MSS) strategy is proposed to select\nsamples with high modal consistency for rehearsal. Meanwhile, for co-occurence\nconfusion, a Collision-based Sample Rehearsal (CSR) mechanism is designed,\nallowing for the increase of rehearsal sample frequency of those confusable\nclasses during training process. Moreover, we construct three audio-visual\nincremental scenarios to verify effectiveness of our method. Comprehensive\nexperiments demonstrate that our method significantly outperforms single-modal\ncontinual learning methods."
    },
    {
      "title": "When One Moment Isn't Enough: Multi-Moment Retrieval with Cross-Moment Interactions",
      "authors": [
        "Zhuo Cao",
        "Heming Du",
        "Bingqing Zhang",
        "Xin Yu",
        "Xue Li",
        "Sen Wang"
      ],
      "arxiv_id": "2510.17218v1",
      "summary": "Existing Moment retrieval (MR) methods focus on Single-Moment Retrieval\n(SMR). However, one query can correspond to multiple relevant moments in\nreal-world applications. This makes the existing datasets and methods\ninsufficient for video temporal grounding. By revisiting the gap between\ncurrent MR tasks and real-world applications, we introduce a high-quality\ndatasets called QVHighlights Multi-Moment Dataset (QV-M$^2$), along with new\nevaluation metrics tailored for multi-moment retrieval (MMR). QV-M$^2$ consists\nof 2,212 annotations covering 6,384 video segments. Building on existing\nefforts in MMR, we propose a framework called FlashMMR. Specifically, we\npropose a Multi-moment Post-verification module to refine the moment\nboundaries. We introduce constrained temporal adjustment and subsequently\nleverage a verification module to re-evaluate the candidate segments. Through\nthis sophisticated filtering pipeline, low-confidence proposals are pruned, and\nrobust multi-moment alignment is achieved. We retrain and evaluate 6 existing\nMR methods on QV-M$^2$ and QVHighlights under both SMR and MMR settings.\nResults show that QV-M$^2$ serves as an effective benchmark for training and\nevaluating MMR models, while FlashMMR provides a strong baseline. Specifically,\non QV-M$^2$, it achieves improvements over prior SOTA method by 3.00% on G-mAP,\n2.70% on mAP@3+tgt, and 2.56% on mR@3. The proposed benchmark and method\nestablish a foundation for advancing research in more realistic and challenging\nvideo temporal grounding scenarios. Code is released at\nhttps://github.com/Zhuo-Cao/QV-M2."
    },
    {
      "title": "$\\mathcal{V}isi\\mathcal{P}runer$: Decoding Discontinuous Cross-Modal Dynamics for Efficient Multimodal LLMs",
      "authors": [
        "Yingqi Fan",
        "Anhao Zhao",
        "Jinlan Fu",
        "Junlong Tong",
        "Hui Su",
        "Yijie Pan",
        "Wei Zhang",
        "Xiaoyu Shen"
      ],
      "arxiv_id": "2510.17205v1",
      "summary": "Multimodal Large Language Models (MLLMs) have achieved strong performance\nacross vision-language tasks, but suffer from significant computational\noverhead due to the quadratic growth of attention computations with the number\nof multimodal tokens. Though efforts have been made to prune tokens in MLLMs,\n\\textit{they lack a fundamental understanding of how MLLMs process and fuse\nmultimodal information.} Through systematic analysis, we uncover a\n\\textbf{three-stage} cross-modal interaction process: (1) Shallow layers\nrecognize task intent, with visual tokens acting as passive attention sinks;\n(2) Cross-modal fusion occurs abruptly in middle layers, driven by a few\ncritical visual tokens; (3) Deep layers discard vision tokens, focusing solely\non linguistic refinement. Based on these findings, we propose\n\\emph{VisiPruner}, a training-free pruning framework that reduces up to 99\\% of\nvision-related attention computations and 53.9\\% of FLOPs on LLaVA-v1.5 7B. It\nsignificantly outperforms existing token pruning methods and generalizes across\ndiverse MLLMs. Beyond pruning, our insights further provide actionable\nguidelines for training efficient MLLMs by aligning model architecture with its\nintrinsic layer-wise processing dynamics. Our code is available at:\nhttps://github.com/EIT-NLP/VisiPruner."
    },
    {
      "title": "Performance Evaluation of an Integrated System for Visible Light Communication and Positioning Using an Event Camera",
      "authors": [
        "Ryota Soga",
        "Masataka Kobayashi",
        "Tsukasa Shimizu",
        "Shintaro Shiba",
        "Quan Kong",
        "Shan Lu",
        "Takaya Yamazato"
      ],
      "arxiv_id": "2510.17203v1",
      "summary": "Event cameras, featuring high temporal resolution and high dynamic range,\noffer visual sensing capabilities comparable to conventional image sensors\nwhile capturing fast-moving objects and handling scenes with extreme lighting\ncontrasts such as tunnel exits. Leveraging these properties, this study\nproposes a novel self-localization system that integrates visible light\ncommunication (VLC) and visible light positioning (VLP) within a single event\ncamera. The system enables a vehicle to estimate its position even in\nGPS-denied environments, such as tunnels, by using VLC to obtain coordinate\ninformation from LED transmitters and VLP to estimate the distance to each\ntransmitter.\n  Multiple LEDs are installed on the transmitter side, each assigned a unique\npilot sequence based on Walsh-Hadamard codes. The event camera identifies\nindividual LEDs within its field of view by correlating the received signal\nwith these codes, allowing clear separation and recognition of each light\nsource. This mechanism enables simultaneous high-capacity MISO (multi-input\nsingle-output) communication through VLC and precise distance estimation via\nphase-only correlation (POC) between multiple LED pairs.\n  To the best of our knowledge, this is the first vehicle-mounted system to\nachieve simultaneous VLC and VLP functionalities using a single event camera.\nField experiments were conducted by mounting the system on a vehicle traveling\nat 30 km/h (8.3 m/s). The results demonstrated robust real-world performance,\nwith a root mean square error (RMSE) of distance estimation within 0.75 m for\nranges up to 100 m and a bit error rate (BER) below 0.01 across the same range."
    },
    {
      "title": "Optimizing DINOv2 with Registers for Face Anti-Spoofing",
      "authors": [
        "Mika Feng",
        "Pierre Gallin-Martel",
        "Koichi Ito",
        "Takafumi Aoki"
      ],
      "arxiv_id": "2510.17201v1",
      "summary": "Face recognition systems are designed to be robust against variations in head\npose, illumination, and image blur during capture. However, malicious actors\ncan exploit these systems by presenting a face photo of a registered user,\npotentially bypassing the authentication process. Such spoofing attacks must be\ndetected prior to face recognition. In this paper, we propose a DINOv2-based\nspoofing attack detection method to discern minute differences between live and\nspoofed face images. Specifically, we employ DINOv2 with registers to extract\ngeneralizable features and to suppress perturbations in the attention\nmechanism, which enables focused attention on essential and minute features. We\ndemonstrate the effectiveness of the proposed method through experiments\nconducted on the dataset provided by ``The 6th Face Anti-Spoofing Workshop:\nUnified Physical-Digital Attacks Detection@ICCV2025'' and SiW dataset."
    },
    {
      "title": "EndoCIL: A Class-Incremental Learning Framework for Endoscopic Image Classification",
      "authors": [
        "Bingrong Liu",
        "Jun Shi",
        "Yushan Zheng"
      ],
      "arxiv_id": "2510.17200v1",
      "summary": "Class-incremental learning (CIL) for endoscopic image analysis is crucial for\nreal-world clinical applications, where diagnostic models should continuously\nadapt to evolving clinical data while retaining performance on previously\nlearned ones. However, existing replay-based CIL methods fail to effectively\nmitigate catastrophic forgetting due to severe domain discrepancies and class\nimbalance inherent in endoscopic imaging. To tackle these challenges, we\npropose EndoCIL, a novel and unified CIL framework specifically tailored for\nendoscopic image diagnosis. EndoCIL incorporates three key components: Maximum\nMean Discrepancy Based Replay (MDBR), employing a distribution-aligned greedy\nstrategy to select diverse and representative exemplars, Prior Regularized\nClass Balanced Loss (PRCBL), designed to alleviate both inter-phase and\nintra-phase class imbalance by integrating prior class distributions and\nbalance weights into the loss function, and Calibration of Fully-Connected\nGradients (CFG), which adjusts the classifier gradients to mitigate bias toward\nnew classes. Extensive experiments conducted on four public endoscopic datasets\ndemonstrate that EndoCIL generally outperforms state-of-the-art CIL methods\nacross varying buffer sizes and evaluation metrics. The proposed framework\neffectively balances stability and plasticity in lifelong endoscopic diagnosis,\nshowing promising potential for clinical scalability and deployment."
    },
    {
      "title": "Round Outcome Prediction in VALORANT Using Tactical Features from Video Analysis",
      "authors": [
        "Nirai Hayakawa",
        "Kazumasa Shimari",
        "Kazuma Yamasaki",
        "Hirotatsu Hoshikawa",
        "Rikuto Tsuchida",
        "Kenichi Matsumoto"
      ],
      "arxiv_id": "2510.17199v1",
      "summary": "Recently, research on predicting match outcomes in esports has been actively\nconducted, but much of it is based on match log data and statistical\ninformation. This research targets the FPS game VALORANT, which requires\ncomplex strategies, and aims to build a round outcome prediction model by\nanalyzing minimap information in match footage. Specifically, based on the\nvideo recognition model TimeSformer, we attempt to improve prediction accuracy\nby incorporating detailed tactical features extracted from minimap information,\nsuch as character position information and other in-game events. This paper\nreports preliminary results showing that a model trained on a dataset augmented\nwith such tactical event labels achieved approximately 81% prediction accuracy,\nespecially from the middle phases of a round onward, significantly\noutperforming a model trained on a dataset with the minimap information itself.\nThis suggests that leveraging tactical features from match footage is highly\neffective for predicting round outcomes in VALORANT."
    },
    {
      "title": "From Pixels to People: Satellite-Based Mapping and Quantification of Riverbank Erosion and Lost Villages in Bangladesh",
      "authors": [
        "M Saifuzzaman Rafat",
        "Mohd Ruhul Ameen",
        "Akif Islam",
        "Abu Saleh Musa Miah",
        "Jungpil Shin"
      ],
      "arxiv_id": "2510.17198v1",
      "summary": "The great rivers of Bangladesh, arteries of commerce and sustenance, are also\nagents of relentless destruction. Each year, they swallow whole villages and\nvast tracts of farmland, erasing communities from the map and displacing\nthousands of families. To track this slow-motion catastrophe has, until now,\nbeen a Herculean task for human analysts. Here we show how a powerful\ngeneral-purpose vision model, the Segment Anything Model (SAM), can be adapted\nto this task with remarkable precision. To do this, we assembled a new dataset\n- a digital chronicle of loss compiled from historical Google Earth imagery of\nBangladesh's most vulnerable regions, including Mokterer Char Union, Kedarpur\nUnion, Balchipara village, and Chowhali Upazila, from 2003 to 2025. Crucially,\nthis dataset is the first to include manually annotated data on the settlements\nthat have vanished beneath the water. Our method first uses a simple\ncolor-channel analysis to provide a rough segmentation of land and water, and\nthen fine-tunes SAM's mask decoder to recognize the subtle signatures of\nriverbank erosion. The resulting model demonstrates a keen eye for this\ndestructive process, achieving a mean Intersection over Union of 86.30% and a\nDice score of 92.60% - a performance that significantly surpasses traditional\nmethods and off-the-shelf deep learning models. This work delivers three key\ncontributions: the first annotated dataset of disappeared settlements in\nBangladesh due to river erosion; a specialized AI model fine-tuned for this\ncritical task; and a method for quantifying land loss with compelling visual\nevidence. Together, these tools provide a powerful new lens through which\npolicymakers and disaster management agencies can monitor erosion, anticipate\nits trajectory, and ultimately protect the vulnerable communities in its path."
    },
    {
      "title": "ZSPAPrune: Zero-Shot Prompt-Aware Token Pruning for Vision-Language Models",
      "authors": [
        "Pu Zhang",
        "Yuwei Li",
        "Xingyuan Xian",
        "Guoming Tang"
      ],
      "arxiv_id": "2510.17197v1",
      "summary": "As the capabilities of Vision-Language Models (VLMs) advance, they can\nprocess increasingly large inputs, which, unlike in LLMs, generates significant\nvisual token redundancy and leads to prohibitive inference costs. While many\nmethods aim to reduce these costs by pruning visual tokens, existing\napproaches, whether based on attention or diversity, typically neglect the\nguidance of the text prompt and thus fail to prioritize task relevance. In this\nwork, we propose a novel, zero-shot method that reframes the problem by\nintroducing a prompt-aware perspective, explicitly modeling visual token\npruning as a balance between task relevance and information diversity. Our\nhierarchical approach first selects a core set of task-relevant visual tokens\nand then supplements them with diversity tokens to preserve broader context.\nExperiments across multiple models and benchmarks show that our method achieves\nperformance that matches or surpasses the state-of-the-art with only minimal\naccuracy loss, even when pruning up to 90\\% of the tokens. Furthermore, these\ngains are accompanied by significant reductions in GPU memory footprint and\ninference latency."
    },
    {
      "title": "SimpleVSF: VLM-Scoring Fusion for Trajectory Prediction of End-to-End Autonomous Driving",
      "authors": [
        "Peiru Zheng",
        "Yun Zhao",
        "Zhan Gong",
        "Hong Zhu",
        "Shaohua Wu"
      ],
      "arxiv_id": "2510.17191v1",
      "summary": "End-to-end autonomous driving has emerged as a promising paradigm for\nachieving robust and intelligent driving policies. However, existing end-to-end\nmethods still face significant challenges, such as suboptimal decision-making\nin complex scenarios. In this paper,we propose SimpleVSF (Simple VLM-Scoring\nFusion), a novel framework that enhances end-to-end planning by leveraging the\ncognitive capabilities of Vision-Language Models (VLMs) and advanced trajectory\nfusion techniques. We utilize the conventional scorers and the novel\nVLM-enhanced scorers. And we leverage a robust weight fusioner for quantitative\naggregation and a powerful VLM-based fusioner for qualitative, context-aware\ndecision-making. As the leading approach in the ICCV 2025 NAVSIM v2 End-to-End\nDriving Challenge, our SimpleVSF framework demonstrates state-of-the-art\nperformance, achieving a superior balance between safety, comfort, and\nefficiency."
    },
    {
      "title": "HIDISC: A Hyperbolic Framework for Domain Generalization with Generalized Category Discovery",
      "authors": [
        "Vaibhav Rathore",
        "Divyam Gupta",
        "Biplab Banerjee"
      ],
      "arxiv_id": "2510.17188v1",
      "summary": "Generalized Category Discovery (GCD) aims to classify test-time samples into\neither seen categories** -- available during training -- or novel ones, without\nrelying on label supervision. Most existing GCD methods assume simultaneous\naccess to labeled and unlabeled data during training and arising from the same\ndomain, limiting applicability in open-world scenarios involving distribution\nshifts. Domain Generalization with GCD (DG-GCD) lifts this constraint by\nrequiring models to generalize to unseen domains containing novel categories,\nwithout accessing targetdomain data during training. The only prior DG-GCD\nmethod, DG2CD-Net, relies on episodic training with multiple synthetic domains\nand task vector aggregation, incurring high computational cost and error\naccumulation. We propose HIDISC, a hyperbolic representation learning framework\nthat achieves domain and category-level generalization without episodic\nsimulation. To expose the model to minimal but diverse domain variations, we\naugment the source domain using GPT-guided diffusion, avoiding overfitting\nwhile maintaining efficiency. To structure the representation space, we\nintroduce Tangent CutMix, a curvature-aware interpolation that synthesizes\npseudo-novel samples in tangent space, preserving manifold consistency. A\nunified loss -- combining penalized Busemann alignment, hybrid hyperbolic\ncontrastive regularization, and adaptive outlier repulsion -- **facilitates\ncompact, semantically structured embeddings. A learnable curvature parameter\nfurther adapts the geometry to dataset complexity. HIDISC achieves\nstate-of-the-art results on PACS , Office-Home , and DomainNet, consistently\noutperforming the existing Euclidean and hyperbolic (DG)-GCD baselines."
    },
    {
      "title": "Capturing Head Avatar with Hand Contacts from a Monocular Video",
      "authors": [
        "Haonan He",
        "Yufeng Zheng",
        "Jie Song"
      ],
      "arxiv_id": "2510.17181v1",
      "summary": "Photorealistic 3D head avatars are vital for telepresence, gaming, and VR.\nHowever, most methods focus solely on facial regions, ignoring natural\nhand-face interactions, such as a hand resting on the chin or fingers gently\ntouching the cheek, which convey cognitive states like pondering. In this work,\nwe present a novel framework that jointly learns detailed head avatars and the\nnon-rigid deformations induced by hand-face interactions.\n  There are two principal challenges in this task. First, naively tracking hand\nand face separately fails to capture their relative poses. To overcome this, we\npropose to combine depth order loss with contact regularization during pose\ntracking, ensuring correct spatial relationships between the face and hand.\nSecond, no publicly available priors exist for hand-induced deformations,\nmaking them non-trivial to learn from monocular videos. To address this, we\nlearn a PCA basis specific to hand-induced facial deformations from a face-hand\ninteraction dataset. This reduces the problem to estimating a compact set of\nPCA parameters rather than a full spatial deformation field. Furthermore,\ninspired by physics-based simulation, we incorporate a contact loss that\nprovides additional supervision, significantly reducing interpenetration\nartifacts and enhancing the physical plausibility of the results.\n  We evaluate our approach on RGB(D) videos captured by an iPhone.\nAdditionally, to better evaluate the reconstructed geometry, we construct a\nsynthetic dataset of avatars with various types of hand interactions. We show\nthat our method can capture better appearance and more accurate deforming\ngeometry of the face than SOTA surface reconstruction methods."
    },
    {
      "title": "Benchmarking Out-of-Distribution Detection for Plankton Recognition: A Systematic Evaluation of Advanced Methods in Marine Ecological Monitoring",
      "authors": [
        "Yingzi Han",
        "Jiakai He",
        "Chuanlong Xie",
        "Jianping Li"
      ],
      "arxiv_id": "2510.17179v1",
      "summary": "Automated plankton recognition models face significant challenges during\nreal-world deployment due to distribution shifts (Out-of-Distribution, OoD)\nbetween training and test data. This stems from plankton's complex\nmorphologies, vast species diversity, and the continuous discovery of novel\nspecies, which leads to unpredictable errors during inference. Despite rapid\nadvancements in OoD detection methods in recent years, the field of plankton\nrecognition still lacks a systematic integration of the latest computer vision\ndevelopments and a unified benchmark for large-scale evaluation. To address\nthis, this paper meticulously designed a series of OoD benchmarks simulating\nvarious distribution shift scenarios based on the DYB-PlanktonNet dataset\n\\cite{875n-f104-21}, and systematically evaluated twenty-two OoD detection\nmethods. Extensive experimental results demonstrate that the ViM\n\\cite{wang2022vim} method significantly outperforms other approaches in our\nconstructed benchmarks, particularly excelling in Far-OoD scenarios with\nsubstantial improvements in key metrics. This comprehensive evaluation not only\nprovides a reliable reference for algorithm selection in automated plankton\nrecognition but also lays a solid foundation for future research in plankton\nOoD detection. To our knowledge, this study marks the first large-scale,\nsystematic evaluation and analysis of Out-of-Distribution data detection\nmethods in plankton recognition. Code is available at\nhttps://github.com/BlackJack0083/PlanktonOoD."
    },
    {
      "title": "Generation then Reconstruction: Accelerating Masked Autoregressive Models via Two-Stage Sampling",
      "authors": [
        "Feihong Yan",
        "Peiru Wang",
        "Yao Zhu",
        "Kaiyu Pang",
        "Qingyan Wei",
        "Huiqi Li",
        "Linfeng Zhang"
      ],
      "arxiv_id": "2510.17171v1",
      "summary": "Masked Autoregressive (MAR) models promise better efficiency in visual\ngeneration than autoregressive (AR) models for the ability of parallel\ngeneration, yet their acceleration potential remains constrained by the\nmodeling complexity of spatially correlated visual tokens in a single step. To\naddress this limitation, we introduce Generation then Reconstruction (GtR), a\ntraining-free hierarchical sampling strategy that decomposes generation into\ntwo stages: structure generation establishing global semantic scaffolding,\nfollowed by detail reconstruction efficiently completing remaining tokens.\nAssuming that it is more difficult to create an image from scratch than to\ncomplement images based on a basic image framework, GtR is designed to achieve\nacceleration by computing the reconstruction stage quickly while maintaining\nthe generation quality by computing the generation stage slowly. Moreover,\nobserving that tokens on the details of an image often carry more semantic\ninformation than tokens in the salient regions, we further propose\nFrequency-Weighted Token Selection (FTS) to offer more computation budget to\ntokens on image details, which are localized based on the energy of high\nfrequency information. Extensive experiments on ImageNet class-conditional and\ntext-to-image generation demonstrate 3.72x speedup on MAR-H while maintaining\ncomparable quality (e.g., FID: 1.59, IS: 304.4 vs. original 1.59, 299.1),\nsubstantially outperforming existing acceleration methods across various model\nscales and generation tasks. Our codes will be released in\nhttps://github.com/feihongyan1/GtR."
    },
    {
      "title": "Investigating Adversarial Robustness against Preprocessing used in Blackbox Face Recognition",
      "authors": [
        "Roland Croft",
        "Brian Du",
        "Darcy Joseph",
        "Sharath Kumar"
      ],
      "arxiv_id": "2510.17169v1",
      "summary": "Face Recognition (FR) models have been shown to be vulnerable to adversarial\nexamples that subtly alter benign facial images, exposing blind spots in these\nsystems, as well as protecting user privacy. End-to-end FR systems first obtain\npreprocessed faces from diverse facial imagery prior to computing the\nsimilarity of the deep feature embeddings. Whilst face preprocessing is a\ncritical component of FR systems, and hence adversarial attacks against them,\nwe observe that this preprocessing is often overlooked in blackbox settings.\nOur study seeks to investigate the transferability of several out-of-the-box\nstate-of-the-art adversarial attacks against FR when applied against different\npreprocessing techniques used in a blackbox setting. We observe that the choice\nof face detection model can degrade the attack success rate by up to 78%,\nwhereas choice of interpolation method during downsampling has relatively\nminimal impacts. Furthermore, we find that the requirement for facial\npreprocessing even degrades attack strength in a whitebox setting, due to the\nunintended interaction of produced noise vectors against face detection models.\nBased on these findings, we propose a preprocessing-invariant method using\ninput transformations that improves the transferability of the studied attacks\nby up to 27%. Our findings highlight the importance of preprocessing in FR\nsystems, and the need for its consideration towards improving the adversarial\ngeneralisation of facial adversarial examples."
    },
    {
      "title": "GACO-CAD: Geometry-Augmented and Conciseness-Optimized CAD Model Generation from Single Image",
      "authors": [
        "Yinghui Wang",
        "Xinyu Zhang",
        "Peng Du"
      ],
      "arxiv_id": "2510.17157v1",
      "summary": "Generating editable, parametric CAD models from a single image holds great\npotential to lower the barriers of industrial concept design. However, current\nmulti-modal large language models (MLLMs) still struggle with accurately\ninferring 3D geometry from 2D images due to limited spatial reasoning\ncapabilities. We address this limitation by introducing GACO-CAD, a novel\ntwo-stage post-training framework. It is designed to achieve a joint objective:\nsimultaneously improving the geometric accuracy of the generated CAD models and\nencouraging the use of more concise modeling procedures. First, during\nsupervised fine-tuning, we leverage depth and surface normal maps as dense\ngeometric priors, combining them with the RGB image to form a multi-channel\ninput. In the context of single-view reconstruction, these priors provide\ncomplementary spatial cues that help the MLLM more reliably recover 3D geometry\nfrom 2D observations. Second, during reinforcement learning, we introduce a\ngroup length reward that, while preserving high geometric fidelity, promotes\nthe generation of more compact and less redundant parametric modeling\nsequences. A simple dynamic weighting strategy is adopted to stabilize\ntraining. Experiments on the DeepCAD and Fusion360 datasets show that GACO-CAD\nachieves state-of-the-art performance under the same MLLM backbone,\nconsistently outperforming existing methods in terms of code validity,\ngeometric accuracy, and modeling conciseness."
    },
    {
      "title": "OmniVIC: A Self-Improving Variable Impedance Controller with Vision-Language In-Context Learning for Safe Robotic Manipulation",
      "authors": [
        "Heng Zhang",
        "Wei-Hsing Huang",
        "Gokhan Solak",
        "Arash Ajoudani"
      ],
      "arxiv_id": "2510.17150v1",
      "summary": "We present OmniVIC, a universal variable impedance controller (VIC) enhanced\nby a vision language model (VLM), which improves safety and adaptation in any\ncontact-rich robotic manipulation task to enhance safe physical interaction.\nTraditional VIC have shown advantages when the robot physically interacts with\nthe environment, but lack generalization in unseen, complex, and unstructured\nsafe interactions in universal task scenarios involving contact or uncertainty.\nTo this end, the proposed OmniVIC interprets task context derived reasoning\nfrom images and natural language and generates adaptive impedance parameters\nfor a VIC controller. Specifically, the core of OmniVIC is a self-improving\nRetrieval-Augmented Generation(RAG) and in-context learning (ICL), where RAG\nretrieves relevant prior experiences from a structured memory bank to inform\nthe controller about similar past tasks, and ICL leverages these retrieved\nexamples and the prompt of current task to query the VLM for generating\ncontext-aware and adaptive impedance parameters for the current manipulation\nscenario. Therefore, a self-improved RAG and ICL guarantee OmniVIC works in\nuniversal task scenarios. The impedance parameter regulation is further\ninformed by real-time force/torque feedback to ensure interaction forces remain\nwithin safe thresholds. We demonstrate that our method outperforms baselines on\na suite of complex contact-rich tasks, both in simulation and on real-world\nrobotic tasks, with improved success rates and reduced force violations.\nOmniVIC takes a step towards bridging high-level semantic reasoning and\nlow-level compliant control, enabling safer and more generalizable\nmanipulation. Overall, the average success rate increases from 27% (baseline)\nto 61.4% (OmniVIC)."
    },
    {
      "title": "DiffVLA++: Bridging Cognitive Reasoning and End-to-End Driving through Metric-Guided Alignment",
      "authors": [
        "Yu Gao",
        "Yiru Wang",
        "Anqing Jiang",
        "Heng Yuwen",
        "Wang Shuo",
        "Sun Hao",
        "Wang Jijun"
      ],
      "arxiv_id": "2510.17148v1",
      "summary": "Conventional end-to-end (E2E) driving models are effective at generating\nphysically plausible trajectories, but often fail to generalize to long-tail\nscenarios due to the lack of essential world knowledge to understand and reason\nabout surrounding environments. In contrast, Vision-Language-Action (VLA)\nmodels leverage world knowledge to handle challenging cases, but their limited\n3D reasoning capability can lead to physically infeasible actions. In this work\nwe introduce DiffVLA++, an enhanced autonomous driving framework that\nexplicitly bridges cognitive reasoning and E2E planning through metric-guided\nalignment. First, we build a VLA module directly generating semantically\ngrounded driving trajectories. Second, we design an E2E module with a dense\ntrajectory vocabulary that ensures physical feasibility. Third, and most\ncritically, we introduce a metric-guided trajectory scorer that guides and\naligns the outputs of the VLA and E2E modules, thereby integrating their\ncomplementary strengths. The experiment on the ICCV 2025 Autonomous Grand\nChallenge leaderboard shows that DiffVLA++ achieves EPDMS of 49.12."
    },
    {
      "title": "Decentralized Real-Time Planning for Multi-UAV Cooperative Manipulation via Imitation Learning",
      "authors": [
        "Shantnav Agarwal",
        "Javier Alonso-Mora",
        "Sihao Sun"
      ],
      "arxiv_id": "2510.17143v1",
      "summary": "Existing approaches for transporting and manipulating cable-suspended loads\nusing multiple UAVs along reference trajectories typically rely on either\ncentralized control architectures or reliable inter-agent communication. In\nthis work, we propose a novel machine learning based method for decentralized\nkinodynamic planning that operates effectively under partial observability and\nwithout inter-agent communication. Our method leverages imitation learning to\ntrain a decentralized student policy for each UAV by imitating a centralized\nkinodynamic motion planner with access to privileged global observations. The\nstudent policy generates smooth trajectories using physics-informed neural\nnetworks that respect the derivative relationships in motion. During training,\nthe student policies utilize the full trajectory generated by the teacher\npolicy, leading to improved sample efficiency. Moreover, each student policy\ncan be trained in under two hours on a standard laptop. We validate our method\nin both simulation and real-world environments to follow an agile reference\ntrajectory, demonstrating performance comparable to that of centralized\napproaches."
    },
    {
      "title": "KineDiff3D: Kinematic-Aware Diffusion for Category-Level Articulated Object Shape Reconstruction and Generation",
      "authors": [
        "WenBo Xu",
        "Liu Liu",
        "Li Zhang",
        "Ran Zhang",
        "Hao Wu",
        "Dan Guo",
        "Meng Wang"
      ],
      "arxiv_id": "2510.17137v1",
      "summary": "Articulated objects, such as laptops and drawers, exhibit significant\nchallenges for 3D reconstruction and pose estimation due to their multi-part\ngeometries and variable joint configurations, which introduce structural\ndiversity across different states. To address these challenges, we propose\nKineDiff3D: Kinematic-Aware Diffusion for Category-Level Articulated Object\nShape Reconstruction and Generation, a unified framework for reconstructing\ndiverse articulated instances and pose estimation from single view input.\nSpecifically, we first encode complete geometry (SDFs), joint angles, and part\nsegmentation into a structured latent space via a novel Kinematic-Aware VAE\n(KA-VAE). In addition, we employ two conditional diffusion models: one for\nregressing global pose (SE(3)) and joint parameters, and another for generating\nthe kinematic-aware latent code from partial observations. Finally, we produce\nan iterative optimization module that bidirectionally refines reconstruction\naccuracy and kinematic parameters via Chamfer-distance minimization while\npreserving articulation constraints. Experimental results on synthetic,\nsemi-synthetic, and real-world datasets demonstrate the effectiveness of our\napproach in accurately reconstructing articulated objects and estimating their\nkinematic properties."
    },
    {
      "title": "GOOD: Training-Free Guided Diffusion Sampling for Out-of-Distribution Detection",
      "authors": [
        "Xin Gao",
        "Jiyao Liu",
        "Guanghao Li",
        "Yueming Lyu",
        "Jianxiong Gao",
        "Weichen Yu",
        "Ningsheng Xu",
        "Liang Wang",
        "Caifeng Shan",
        "Ziwei Liu",
        "Chenyang Si"
      ],
      "arxiv_id": "2510.17131v1",
      "summary": "Recent advancements have explored text-to-image diffusion models for\nsynthesizing out-of-distribution (OOD) samples, substantially enhancing the\nperformance of OOD detection. However, existing approaches typically rely on\nperturbing text-conditioned embeddings, resulting in semantic instability and\ninsufficient shift diversity, which limit generalization to realistic OOD. To\naddress these challenges, we propose GOOD, a novel and flexible framework that\ndirectly guides diffusion sampling trajectories towards OOD regions using\noff-the-shelf in-distribution (ID) classifiers. GOOD incorporates dual-level\nguidance: (1) Image-level guidance based on the gradient of log partition to\nreduce input likelihood, drives samples toward low-density regions in pixel\nspace. (2) Feature-level guidance, derived from k-NN distance in the\nclassifier's latent space, promotes sampling in feature-sparse regions. Hence,\nthis dual-guidance design enables more controllable and diverse OOD sample\ngeneration. Additionally, we introduce a unified OOD score that adaptively\ncombines image and feature discrepancies, enhancing detection robustness. We\nperform thorough quantitative and qualitative analyses to evaluate the\neffectiveness of GOOD, demonstrating that training with samples generated by\nGOOD can notably enhance OOD detection performance."
    },
    {
      "title": "Matricial Free Energy as a Gaussianizing Regularizer: Enhancing Autoencoders for Gaussian Code Generation",
      "authors": [
        "Rishi Sonthalia",
        "Raj Rao Nadakuditi"
      ],
      "arxiv_id": "2510.17120v1",
      "summary": "We introduce a novel regularization scheme for autoencoders based on\nmatricial free energy. Our approach defines a differentiable loss function in\nterms of the singular values of the code matrix (code dimension x batch size).\nFrom the standpoint of free probability an d random matrix theory, this loss\nachieves its minimum when the singular value distribution of the code matrix\ncoincides with that of an appropriately sculpted random metric with i.i.d.\nGaussian entries. Empirical simulations demonstrate that minimizing the\nnegative matricial free energy through standard stochastic gradient-based\ntraining yields Gaussian-like codes that generalize across training and test\nsets. Building on this foundation, we propose a matricidal free energy\nmaximizing autoencoder that reliably produces Gaussian codes and show its\napplication to underdetermined inverse problems."
    },
    {
      "title": "Towards Imperceptible Watermarking Via Environment Illumination for Consumer Cameras",
      "authors": [
        "Hodaka Kawachi",
        "Tomoya Nakamura",
        "Hiroaki Santo",
        "SaiKiran Kumar Tedla",
        "Trevor Dalton Canham",
        "Yasushi Yagi",
        "Michael S. Brown"
      ],
      "arxiv_id": "2510.17114v1",
      "summary": "This paper introduces a method for using LED-based environmental lighting to\nproduce visually imperceptible watermarks for consumer cameras. Our approach\noptimizes an LED light source's spectral profile to be minimally visible to the\nhuman eye while remaining highly detectable by typical consumer cameras. The\nmethod jointly considers the human visual system's sensitivity to visible\nspectra, modern consumer camera sensors' spectral sensitivity, and narrowband\nLEDs' ability to generate broadband spectra perceived as \"white light\"\n(specifically, D65 illumination). To ensure imperceptibility, we employ\nspectral modulation rather than intensity modulation. Unlike conventional\nvisible light communication, our approach enables watermark extraction at\nstandard low frame rates (30-60 fps). While the information transfer rate is\nmodest-embedding 128 bits within a 10-second video clip-this capacity is\nsufficient for essential metadata supporting privacy protection and content\nverification."
    },
    {
      "title": "Efficient Vision-Language-Action Models for Embodied Manipulation: A Systematic Survey",
      "authors": [
        "Weifan Guan",
        "Qinghao Hu",
        "Aosheng Li",
        "Jian Cheng"
      ],
      "arxiv_id": "2510.17111v1",
      "summary": "Vision-Language-Action (VLA) models extend vision-language models to embodied\ncontrol by mapping natural-language instructions and visual observations to\nrobot actions. Despite their capabilities, VLA systems face significant\nchallenges due to their massive computational and memory demands, which\nconflict with the constraints of edge platforms such as on-board mobile\nmanipulators that require real-time performance. Addressing this tension has\nbecome a central focus of recent research. In light of the growing efforts\ntoward more efficient and scalable VLA systems, this survey provides a\nsystematic review of approaches for improving VLA efficiency, with an emphasis\non reducing latency, memory footprint, and training and inference costs. We\ncategorize existing solutions into four dimensions: model architecture,\nperception feature, action generation, and training/inference strategies,\nsummarizing representative techniques within each category. Finally, we discuss\nfuture trends and open challenges, highlighting directions for advancing\nefficient embodied intelligence."
    },
    {
      "title": "Boosting Fidelity for Pre-Trained-Diffusion-Based Low-Light Image Enhancement via Condition Refinement",
      "authors": [
        "Xiaogang Xu",
        "Jian Wang",
        "Yunfan Lu",
        "Ruihang Chu",
        "Ruixing Wang",
        "Jiafei Wu",
        "Bei Yu",
        "Liang Lin"
      ],
      "arxiv_id": "2510.17105v1",
      "summary": "Diffusion-based methods, leveraging pre-trained large models like Stable\nDiffusion via ControlNet, have achieved remarkable performance in several\nlow-level vision tasks. However, Pre-Trained Diffusion-Based (PTDB) methods\noften sacrifice content fidelity to attain higher perceptual realism. This\nissue is exacerbated in low-light scenarios, where severely degraded\ninformation caused by the darkness limits effective control. We identify two\nprimary causes of fidelity loss: the absence of suitable conditional latent\nmodeling and the lack of bidirectional interaction between the conditional\nlatent and noisy latent in the diffusion process. To address this, we propose a\nnovel optimization strategy for conditioning in pre-trained diffusion models,\nenhancing fidelity while preserving realism and aesthetics. Our method\nintroduces a mechanism to recover spatial details lost during VAE encoding,\ni.e., a latent refinement pipeline incorporating generative priors.\nAdditionally, the refined latent condition interacts dynamically with the noisy\nlatent, leading to improved restoration performance. Our approach is\nplug-and-play, seamlessly integrating into existing diffusion networks to\nprovide more effective control. Extensive experiments demonstrate significant\nfidelity improvements in PTDB methods."
    },
    {
      "title": "Shape-aware Inertial Poser: Motion Tracking for Humans with Diverse Shapes Using Sparse Inertial Sensors",
      "authors": [
        "Lu Yin",
        "Ziying Shi",
        "Yinghao Wu",
        "Xinyu Yi",
        "Feng Xu",
        "Shihui Guo"
      ],
      "arxiv_id": "2510.17101v1",
      "summary": "Human motion capture with sparse inertial sensors has gained significant\nattention recently. However, existing methods almost exclusively rely on a\ntemplate adult body shape to model the training data, which poses challenges\nwhen generalizing to individuals with largely different body shapes (such as a\nchild). This is primarily due to the variation in IMU-measured acceleration\ncaused by changes in body shape. To fill this gap, we propose Shape-aware\nInertial Poser (SAIP), the first solution considering body shape differences in\nsparse inertial-based motion capture. Specifically, we decompose the sensor\nmeasurements related to shape and pose in order to effectively model their\njoint correlations. Firstly, we train a regression model to transfer the\nIMU-measured accelerations of a real body to match the template adult body\nmodel, compensating for the shape-related sensor measurements. Then, we can\neasily follow the state-of-the-art methods to estimate the full body motions of\nthe template-shaped body. Finally, we utilize a second regression model to map\nthe joint velocities back to the real body, combined with a shape-aware\nphysical optimization strategy to calculate global motions on the subject.\nFurthermore, our method relies on body shape awareness, introducing the first\ninertial shape estimation scheme. This is accomplished by modeling the\nshape-conditioned IMU-pose correlation using an MLP-based network. To validate\nthe effectiveness of SAIP, we also present the first IMU motion capture dataset\ncontaining individuals of different body sizes. This dataset features 10\nchildren and 10 adults, with heights ranging from 110 cm to 190 cm, and a total\nof 400 minutes of paired IMU-Motion samples. Extensive experimental results\ndemonstrate that SAIP can effectively handle motion capture tasks for diverse\nbody shapes. The code and dataset are available at\nhttps://github.com/yinlu5942/SAIP."
    },
    {
      "title": "GSPlane: Concise and Accurate Planar Reconstruction via Structured Representation",
      "authors": [
        "Ruitong Gan",
        "Junran Peng",
        "Yang Liu",
        "Chuanchen Luo",
        "Qing Li",
        "Zhaoxiang Zhang"
      ],
      "arxiv_id": "2510.17095v1",
      "summary": "Planes are fundamental primitives of 3D sences, especially in man-made\nenvironments such as indoor spaces and urban streets. Representing these planes\nin a structured and parameterized format facilitates scene editing and physical\nsimulations in downstream applications. Recently, Gaussian Splatting (GS) has\ndemonstrated remarkable effectiveness in the Novel View Synthesis task, with\nextensions showing great potential in accurate surface reconstruction. However,\neven state-of-the-art GS representations often struggle to reconstruct planar\nregions with sufficient smoothness and precision. To address this issue, we\npropose GSPlane, which recovers accurate geometry and produces clean and\nwell-structured mesh connectivity for plane regions in the reconstructed scene.\nBy leveraging off-the-shelf segmentation and normal prediction models, GSPlane\nextracts robust planar priors to establish structured representations for\nplanar Gaussian coordinates, which help guide the training process by enforcing\ngeometric consistency. To further enhance training robustness, a Dynamic\nGaussian Re-classifier is introduced to adaptively reclassify planar Gaussians\nwith persistently high gradients as non-planar, ensuring more reliable\noptimization. Furthermore, we utilize the optimized planar priors to refine the\nmesh layouts, significantly improving topological structure while reducing the\nnumber of vertices and faces. We also explore applications of the structured\nplanar representation, which enable decoupling and flexible manipulation of\nobjects on supportive planes. Extensive experiments demonstrate that, with no\nsacrifice in rendering quality, the introduction of planar priors significantly\nimproves the geometric accuracy of the extracted meshes across various\nbaselines."
    },
    {
      "title": "Learning to Design Soft Hands using Reward Models",
      "authors": [
        "Xueqian Bai",
        "Nicklas Hansen",
        "Adabhav Singh",
        "Michael T. Tolley",
        "Yan Duan",
        "Pieter Abbeel",
        "Xiaolong Wang",
        "Sha Yi"
      ],
      "arxiv_id": "2510.17086v1",
      "summary": "Soft robotic hands promise to provide compliant and safe interaction with\nobjects and environments. However, designing soft hands to be both compliant\nand functional across diverse use cases remains challenging. Although co-design\nof hardware and control better couples morphology to behavior, the resulting\nsearch space is high-dimensional, and even simulation-based evaluation is\ncomputationally expensive. In this paper, we propose a Cross-Entropy Method\nwith Reward Model (CEM-RM) framework that efficiently optimizes tendon-driven\nsoft robotic hands based on teleoperation control policy, reducing design\nevaluations by more than half compared to pure optimization while learning a\ndistribution of optimized hand designs from pre-collected teleoperation data.\nWe derive a design space for a soft robotic hand composed of flexural soft\nfingers and implement parallelized training in simulation. The optimized hands\nare then 3D-printed and deployed in the real world using both teleoperation\ndata and real-time teleoperation. Experiments in both simulation and hardware\ndemonstrate that our optimized design significantly outperforms baseline hands\nin grasping success rates across a diverse set of challenging objects."
    },
    {
      "title": "Towards a Generalizable Fusion Architecture for Multimodal Object Detection",
      "authors": [
        "Jad Berjawi",
        "Yoann Dupas",
        "Christophe C'erin"
      ],
      "arxiv_id": "2510.17078v1",
      "summary": "Multimodal object detection improves robustness in chal- lenging conditions\nby leveraging complementary cues from multiple sensor modalities. We introduce\nFiltered Multi- Modal Cross Attention Fusion (FMCAF), a preprocess- ing\narchitecture designed to enhance the fusion of RGB and infrared (IR) inputs.\nFMCAF combines a frequency- domain filtering block (Freq-Filter) to suppress\nredun- dant spectral features with a cross-attention-based fusion module (MCAF)\nto improve intermodal feature sharing. Unlike approaches tailored to specific\ndatasets, FMCAF aims for generalizability, improving performance across\ndifferent multimodal challenges without requiring dataset- specific tuning. On\nLLVIP (low-light pedestrian detec- tion) and VEDAI (aerial vehicle detection),\nFMCAF outper- forms traditional fusion (concatenation), achieving +13.9% mAP@50\non VEDAI and +1.1% on LLVIP. These results support the potential of FMCAF as a\nflexible foundation for robust multimodal fusion in future detection pipelines."
    },
    {
      "title": "ProDAT: Progressive Density-Aware Tail-Drop for Point Cloud Coding",
      "authors": [
        "Zhe Luo",
        "Wenjing Jia",
        "Stuart Perry"
      ],
      "arxiv_id": "2510.17068v1",
      "summary": "Three-dimensional (3D) point clouds are becoming increasingly vital in\napplications such as autonomous driving, augmented reality, and immersive\ncommunication, demanding real-time processing and low latency. However, their\nlarge data volumes and bandwidth constraints hinder the deployment of\nhigh-quality services in resource-limited environments. Progres- sive coding,\nwhich allows for decoding at varying levels of detail, provides an alternative\nby allowing initial partial decoding with subsequent refinement. Although\nrecent learning-based point cloud geometry coding methods have achieved notable\nsuccess, their fixed latent representation does not support progressive\ndecoding. To bridge this gap, we propose ProDAT, a novel density-aware\ntail-drop mechanism for progressive point cloud coding. By leveraging density\ninformation as a guidance signal, latent features and coordinates are decoded\nadaptively based on their significance, therefore achieving progressive\ndecoding at multiple bitrates using one single model. Experimental results on\nbenchmark datasets show that the proposed ProDAT not only enables progressive\ncoding but also achieves superior coding efficiency compared to\nstate-of-the-art learning-based coding techniques, with over 28.6% BD-rate\nimprovement for PSNR- D2 on SemanticKITTI and over 18.15% for ShapeNet"
    }
  ]
}