{
    "papers": [
        {
            "title": "ConsistEdit: Highly Consistent and Precise Training-free Visual Editing",
            "authors": [
                "Zixin Yin",
                "Ling-Hao Chen",
                "Lionel Ni",
                "Xili Dai"
            ],
            "arxiv_id": "2510.17803v1",
            "summary": "Recent advances in training-free attention control methods have enabled\nflexible and efficient text-guided editing capabilities for existing generation\nmodels. However, current approaches struggle to simultaneously deliver strong\nediting strength while preserving consistency with the source. This limitation\nbecomes particularly critical in multi-round and video editing, where visual\nerrors can accumulate over time. Moreover, most existing methods enforce global\nconsistency, which limits their ability to modify individual attributes such as\ntexture while preserving others, thereby hindering fine-grained editing.\nRecently, the architectural shift from U-Net to MM-DiT has brought significant\nimprovements in generative performance and introduced a novel mechanism for\nintegrating text and vision modalities. These advancements pave the way for\novercoming challenges that previous methods failed to resolve. Through an\nin-depth analysis of MM-DiT, we identify three key insights into its attention\nmechanisms. Building on these, we propose ConsistEdit, a novel attention\ncontrol method specifically tailored for MM-DiT. ConsistEdit incorporates\nvision-only attention control, mask-guided pre-attention fusion, and\ndifferentiated manipulation of the query, key, and value tokens to produce\nconsistent, prompt-aligned edits. Extensive experiments demonstrate that\nConsistEdit achieves state-of-the-art performance across a wide range of image\nand video editing tasks, including both structure-consistent and\nstructure-inconsistent scenarios. Unlike prior methods, it is the first\napproach to perform editing across all inference steps and attention layers\nwithout handcraft, significantly enhancing reliability and consistency, which\nenables robust multi-round and multi-region editing. Furthermore, it supports\nprogressive adjustment of structural consistency, enabling finer control.",
            "headline_zh": "提出ConsistEdit方法，基于MM-DiT实现高一致性和精确的无训练视觉编辑",
            "intro_zh": [
                "当前无训练注意力控制方法在编辑强度与源一致性间难以平衡，影响多轮和视频编辑",
                "通过视觉专用注意力控制、掩码引导预融合和QKV差异化操作，提升编辑一致性和对齐性",
                "实验显示在图像和视频编辑任务中达到SOTA，支持多轮、多区域和渐进结构控制"
            ],
            "tags_zh": [
                "无训练视觉编辑",
                "注意力控制",
                "MM-DiT模型",
                "多轮编辑",
                "视频编辑"
            ],
            "_index": 0
        },
        {
            "title": "Robobench: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models as Embodied Brain",
            "authors": [
                "Yulin Luo",
                "Chun-Kai Fan",
                "Menghang Dong",
                "Jiayu Shi",
                "Mengdi Zhao",
                "Bo-Wen Zhang",
                "Cheng Chi",
                "Jiaming Liu",
                "Gaole Dai",
                "Rongyu Zhang",
                "Ruichuan An",
                "Kun Wu",
                "Zhengping Che",
                "Shaoxuan Xie",
                "Guocai Yao",
                "Zhongxia Zhao",
                "Pengwei Wang",
                "Guang Liu",
                "Zhongyuan Wang",
                "Tiejun Huang",
                "Shanghang Zhang"
            ],
            "arxiv_id": "2510.17801v1",
            "summary": "Building robots that can perceive, reason, and act in dynamic, unstructured\nenvironments remains a core challenge. Recent embodied systems often adopt a\ndual-system paradigm, where System 2 handles high-level reasoning while System\n1 executes low-level control. In this work, we refer to System 2 as the\nembodied brain, emphasizing its role as the cognitive core for reasoning and\ndecision-making in manipulation tasks. Given this role, systematic evaluation\nof the embodied brain is essential. Yet existing benchmarks emphasize execution\nsuccess, or when targeting high-level reasoning, suffer from incomplete\ndimensions and limited task realism, offering only a partial picture of\ncognitive capability. To bridge this gap, we introduce RoboBench, a benchmark\nthat systematically evaluates multimodal large language models (MLLMs) as\nembodied brains. Motivated by the critical roles across the full manipulation\npipeline, RoboBench defines five dimensions-instruction comprehension,\nperception reasoning, generalized planning, affordance prediction, and failure\nanalysis-spanning 14 capabilities, 25 tasks, and 6092 QA pairs. To ensure\nrealism, we curate datasets across diverse embodiments, attribute-rich objects,\nand multi-view scenes, drawing from large-scale real robotic data. For\nplanning, RoboBench introduces an evaluation framework,\nMLLM-as-world-simulator. It evaluate embodied feasibility by simulating whether\npredicted plans can achieve critical object-state changes. Experiments on 14\nMLLMs reveal fundamental limitations: difficulties with implicit instruction\ncomprehension, spatiotemporal reasoning, cross-scenario planning, fine-grained\naffordance understanding, and execution failure diagnosis. RoboBench provides a\ncomprehensive scaffold to quantify high-level cognition, and guide the\ndevelopment of next-generation embodied MLLMs. The project page is in\nhttps://robo-bench.github.io.",
            "headline_zh": "提出RoboBench基准以系统评估多模态大语言模型在具身机器人中的认知能力",
            "intro_zh": [
                "现有基准在评估具身大脑时存在执行成功偏重和任务真实性不足的问题",
                "定义五个维度评估多模态大语言模型，涵盖指令理解、感知推理等能力",
                "实验揭示模型在隐式指令理解和时空推理等方面存在根本性局限"
            ],
            "tags_zh": [
                "多模态大语言模型",
                "具身智能",
                "机器人基准",
                "认知评估",
                "规划模拟",
                "失败分析"
            ],
            "_index": 1
        },
        {
            "title": "Glyph: Scaling Context Windows via Visual-Text Compression",
            "authors": [
                "Jiale Cheng",
                "Yusen Liu",
                "Xinyu Zhang",
                "Yulin Fei",
                "Wenyi Hong",
                "Ruiliang Lyu",
                "Weihan Wang",
                "Zhe Su",
                "Xiaotao Gu",
                "Xiao Liu",
                "Yushi Bai",
                "Jie Tang",
                "Hongning Wang",
                "Minlie Huang"
            ],
            "arxiv_id": "2510.17800v1",
            "summary": "Large language models (LLMs) increasingly rely on long-context modeling for\ntasks such as document understanding, code analysis, and multi-step reasoning.\nHowever, scaling context windows to the million-token level brings prohibitive\ncomputational and memory costs, limiting the practicality of long-context LLMs.\nIn this work, we take a different perspective-visual context scaling-to tackle\nthis challenge. Instead of extending token-based sequences, we propose Glyph, a\nframework that renders long texts into images and processes them with\nvision-language models (VLMs). This approach substantially compresses textual\ninput while preserving semantic information, and we further design an\nLLM-driven genetic search to identify optimal visual rendering configurations\nfor balancing accuracy and compression. Through extensive experiments, we\ndemonstrate that our method achieves 3-4x token compression while maintaining\naccuracy comparable to leading LLMs such as Qwen3-8B on various long-context\nbenchmarks. This compression also leads to around 4x faster prefilling and\ndecoding, and approximately 2x faster SFT training. Furthermore, under extreme\ncompression, a 128K-context VLM could scale to handle 1M-token-level text\ntasks. In addition, the rendered text data benefits real-world multimodal\ntasks, such as document understanding. Our code and model are released at\nhttps://github.com/thu-coai/Glyph.",
            "headline_zh": "提出Glyph框架，通过视觉-文本压缩扩展上下文窗口以降低计算成本。",
            "intro_zh": [
                "核心问题：长上下文LLMs计算和内存成本高，限制百万级token任务实用性。",
                "方法要点：将长文本渲染为图像，使用视觉语言模型处理，实现语义保留压缩。",
                "实验效果：实现3-4倍token压缩，准确率可比Qwen3-8B，提升推理和训练速度。"
            ],
            "tags_zh": [
                "长上下文建模",
                "视觉-文本压缩",
                "视觉语言模型",
                "遗传搜索优化",
                "文档理解"
            ],
            "_index": 2
        },
        {
            "title": "SoftMimic: Learning Compliant Whole-body Control from Examples",
            "authors": [
                "Gabriel B. Margolis",
                "Michelle Wang",
                "Nolan Fey",
                "Pulkit Agrawal"
            ],
            "arxiv_id": "2510.17792v1",
            "summary": "We introduce SoftMimic, a framework for learning compliant whole-body control\npolicies for humanoid robots from example motions. Imitating human motions with\nreinforcement learning allows humanoids to quickly learn new skills, but\nexisting methods incentivize stiff control that aggressively corrects\ndeviations from a reference motion, leading to brittle and unsafe behavior when\nthe robot encounters unexpected contacts. In contrast, SoftMimic enables robots\nto respond compliantly to external forces while maintaining balance and\nposture. Our approach leverages an inverse kinematics solver to generate an\naugmented dataset of feasible compliant motions, which we use to train a\nreinforcement learning policy. By rewarding the policy for matching compliant\nresponses rather than rigidly tracking the reference motion, SoftMimic learns\nto absorb disturbances and generalize to varied tasks from a single motion\nclip. We validate our method through simulations and real-world experiments,\ndemonstrating safe and effective interaction with the environment.",
            "headline_zh": "提出SoftMimic框架，从示例运动学习人形机器人柔顺全身控制以应对意外接触。",
            "intro_zh": [
                "核心问题：现有模仿方法导致僵硬控制，在意外接触时行为脆弱不安全。",
                "方法要点：利用逆运动学生成柔顺运动数据集，训练强化学习策略匹配柔顺响应。",
                "实验或效果：通过仿真和真实实验验证，机器人能吸收扰动并泛化到多种任务。"
            ],
            "tags_zh": [
                "人形机器人控制",
                "柔顺运动模仿",
                "强化学习",
                "逆运动学",
                "全身控制",
                "扰动吸收"
            ],
            "_index": 3
        },
        {
            "title": "UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action",
            "authors": [
                "Yuhao Yang",
                "Zhen Yang",
                "Zi-Yi Dou",
                "Anh Nguyen",
                "Keen You",
                "Omar Attia",
                "Andrew Szot",
                "Michael Feng",
                "Ram Ramrakhya",
                "Alexander Toshev",
                "Chao Huang",
                "Yinfei Yang",
                "Zhe Gan"
            ],
            "arxiv_id": "2510.17790v1",
            "summary": "Multimodal agents for computer use rely exclusively on primitive actions\n(click, type, scroll) that require accurate visual grounding and lengthy\nexecution chains, leading to cascading failures and performance bottlenecks.\nWhile other agents leverage rich programmatic interfaces (APIs, MCP servers,\ntools), computer-use agents (CUAs) remain isolated from these capabilities. We\npresent UltraCUA, a foundation model that bridges this gap through hybrid\naction -- seamlessly integrating GUI primitives with high-level programmatic\ntool calls. To achieve this, our approach comprises four key components: (1) an\nautomated pipeline that scales programmatic tools from software documentation,\nopen-source repositories, and code generation; (2) a synthetic data engine\nproducing over 17,000 verifiable tasks spanning real-world computer-use\nscenarios; (3) a large-scale high-quality hybrid action trajectory collection\nwith both low-level GUI actions and high-level programmatic tool calls; and (4)\na two-stage training pipeline combining supervised fine-tuning with online\nreinforcement learning, enabling strategic alternation between low-level and\nhigh-level actions. Experiments with our 7B and 32B models demonstrate\nsubstantial improvements over state-of-the-art agents. On OSWorld, UltraCUA\nmodels achieve an average 22% relative improvement over base models, while\nbeing 11% faster in terms of steps. Out-of-domain evaluation on\nWindowsAgentArena shows our model reaches 21.7% success rate, outperforming\nbaselines trained on Windows data. The hybrid action mechanism proves critical,\nreducing error propagation while maintaining execution efficiency.",
            "headline_zh": "提出UltraCUA基础模型，通过混合动作解决计算机使用代理的失败传播与效率瓶颈问题。",
            "intro_zh": [
                "核心问题：计算机使用代理依赖原始GUI动作，易导致错误传播和性能瓶颈。",
                "方法要点：结合GUI原始动作与高级程序化工具调用，实现混合动作机制。",
                "实验效果：在OSWorld和WindowsAgentArena上显著提升成功率和执行效率。"
            ],
            "tags_zh": [
                "计算机使用代理",
                "混合动作",
                "基础模型",
                "强化学习",
                "程序化工具调用",
                "GUI自动化"
            ],
            "_index": 4
        },
        {
            "title": "Botany-Bot: Digital Twin Monitoring of Occluded and Underleaf Plant Structures with Gaussian Splats",
            "authors": [
                "Simeon Adebola",
                "Chung Min Kim",
                "Justin Kerr",
                "Shuangyu Xie",
                "Prithvi Akella",
                "Jose Luis Susa Rincon",
                "Eugen Solowjow",
                "Ken Goldberg"
            ],
            "arxiv_id": "2510.17783v1",
            "summary": "Commercial plant phenotyping systems using fixed cameras cannot perceive many\nplant details due to leaf occlusion. In this paper, we present Botany-Bot, a\nsystem for building detailed \"annotated digital twins\" of living plants using\ntwo stereo cameras, a digital turntable inside a lightbox, an industrial robot\narm, and 3D segmentated Gaussian Splat models. We also present robot algorithms\nfor manipulating leaves to take high-resolution indexable images of occluded\ndetails such as stem buds and the underside/topside of leaves. Results from\nexperiments suggest that Botany-Bot can segment leaves with 90.8% accuracy,\ndetect leaves with 86.2% accuracy, lift/push leaves with 77.9% accuracy, and\ntake detailed overside/underside images with 77.3% accuracy. Code, videos, and\ndatasets are available at https://berkeleyautomation.github.io/Botany-Bot/.",
            "headline_zh": "提出Botany-Bot系统以解决植物叶片遮挡导致的细节感知问题",
            "intro_zh": [
                "核心问题：固定相机系统因叶片遮挡无法感知植物细节，如茎芽和叶片正反面",
                "方法要点：使用立体相机、机器人臂和3D高斯溅射模型构建带注释的数字孪生",
                "实验或效果：系统在叶片分割、检测、操作和图像采集任务中准确率达77.3%-90.8%"
            ],
            "tags_zh": [
                "植物表型监测",
                "数字孪生",
                "高斯溅射",
                "机器人操作",
                "3D分割",
                "遮挡处理"
            ],
            "_index": 5
        },
        {
            "title": "SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference",
            "authors": [
                "Samir Khaki",
                "Junxian Guo",
                "Jiaming Tang",
                "Shang Yang",
                "Yukang Chen",
                "Konstantinos N. Plataniotis",
                "Yao Lu",
                "Song Han",
                "Zhijian Liu"
            ],
            "arxiv_id": "2510.17777v1",
            "summary": "Vision Language Models (VLMs) have rapidly advanced in integrating visual and\ntextual reasoning, powering applications across high-resolution image\nunderstanding, long-video analysis, and multi-turn conversation. However, their\nscalability remains limited by the growing number of visual tokens that\ndominate inference latency. We present SparseVILA, a new paradigm for efficient\nVLM inference that decouples visual sparsity across the prefilling and decoding\nstages. SparseVILA distributes sparsity across stages by pruning redundant\nvisual tokens during prefill and retrieving only query-relevant tokens during\ndecoding. This decoupled design matches leading prefill pruning methods while\npreserving multi-turn fidelity by retaining most of the visual cache so that\nquery-aware tokens can be retrieved at each conversation round. Built on an\nAWQ-optimized inference pipeline, SparseVILA achieves up to 4.0 times faster\nprefilling, 2.5 times faster decoding, and an overall 2.6 times end-to-end\nspeedup on long-context video tasks -- while improving accuracy on\ndocument-understanding and reasoning tasks. By decoupling query-agnostic\npruning and query-aware retrieval, SparseVILA establishes a new direction for\nefficient multimodal inference, offering a training-free, architecture-agnostic\nframework for accelerating large VLMs without sacrificing capability.",
            "headline_zh": "提出SparseVILA以解决视觉语言模型推理效率问题",
            "intro_zh": [
                "视觉令牌数量增长导致VLM推理延迟增加",
                "在预填充阶段剪枝冗余令牌，解码阶段检索查询相关令牌",
                "实现端到端加速2.6倍，同时提升文档理解和推理任务准确率"
            ],
            "tags_zh": [
                "视觉语言模型",
                "推理加速",
                "令牌剪枝",
                "多模态推理",
                "长视频分析"
            ],
            "_index": 6
        },
        {
            "title": "Towards Explainable Skin Cancer Classification: A Dual-Network Attention Model with Lesion Segmentation and Clinical Metadata Fusion",
            "authors": [
                "Md. Enamul Atiq",
                "Shaikh Anowarul Fattah"
            ],
            "arxiv_id": "2510.17773v1",
            "summary": "Skin cancer is a life-threatening disease where early detection significantly\nimproves patient outcomes. Automated diagnosis from dermoscopic images is\nchallenging due to high intra-class variability and subtle inter-class\ndifferences. Many deep learning models operate as \"black boxes,\" limiting\nclinical trust. In this work, we propose a dual-encoder attention-based\nframework that leverages both segmented lesions and clinical metadata to\nenhance skin lesion classification in terms of both accuracy and\ninterpretability. A novel Deep-UNet architecture with Dual Attention Gates\n(DAG) and Atrous Spatial Pyramid Pooling (ASPP) is first employed to segment\nlesions. The classification stage uses two DenseNet201 encoders-one on the\noriginal image and another on the segmented lesion whose features are fused via\nmulti-head cross-attention. This dual-input design guides the model to focus on\nsalient pathological regions. In addition, a transformer-based module\nincorporates patient metadata (age, sex, lesion site) into the prediction. We\nevaluate our approach on the HAM10000 dataset and the ISIC 2018 and 2019\nchallenges. The proposed method achieves state-of-the-art segmentation\nperformance and significantly improves classification accuracy and average AUC\ncompared to baseline models. To validate our model's reliability, we use\nGradient-weighted Class Activation Mapping (Grad-CAM) to generate heatmaps.\nThese visualizations confirm that our model's predictions are based on the\nlesion area, unlike models that rely on spurious background features. These\nresults demonstrate that integrating precise lesion segmentation and clinical\ndata with attention-based fusion leads to a more accurate and interpretable\nskin cancer classification model.",
            "headline_zh": "提出双网络注意力模型，融合病灶分割与临床元数据以提升皮肤癌分类准确性与可解释性。",
            "intro_zh": [
                "皮肤癌诊断面临类内高变异性与类间细微差异，且深度学习模型常为黑箱，影响临床信任。",
                "方法采用Deep-UNet分割病灶，双DenseNet201编码器融合特征，并集成临床元数据以增强分类。",
                "在HAM10000等数据集上验证，模型在分割与分类性能上优于基线，并通过Grad-CAM热图提升可解释性。"
            ],
            "tags_zh": [
                "皮肤癌分类",
                "病灶分割",
                "注意力机制",
                "临床元数据融合",
                "可解释性AI",
                "深度学习"
            ],
            "_index": 7
        },
        {
            "title": "Seeing but Not Believing: Probing the Disconnect Between Visual Attention and Answer Correctness in VLMs",
            "authors": [
                "Zhining Liu",
                "Ziyi Chen",
                "Hui Liu",
                "Chen Luo",
                "Xianfeng Tang",
                "Suhang Wang",
                "Joy Zeng",
                "Zhenwei Dai",
                "Zhan Shi",
                "Tianxin Wei",
                "Benoit Dumoulin",
                "Hanghang Tong"
            ],
            "arxiv_id": "2510.17771v1",
            "summary": "Vision-Language Models (VLMs) achieve strong results on multimodal tasks such\nas visual question answering, yet they can still fail even when the correct\nvisual evidence is present. In this work, we systematically investigate whether\nthese failures arise from not perceiving the evidence or from not leveraging it\neffectively. By examining layer-wise attention dynamics, we find that shallow\nlayers focus primarily on text, while deeper layers sparsely but reliably\nattend to localized evidence regions. Surprisingly, VLMs often perceive the\nvisual evidence when outputting incorrect answers, a phenomenon we term\n``seeing but not believing'' that widely exists in major VLM families. Building\non this, we introduce an inference-time intervention that highlights deep-layer\nevidence regions through selective attention-based masking. It requires no\ntraining and consistently improves accuracy across multiple families, including\nLLaVA, Qwen, Gemma, and InternVL. These results show that VLMs encode reliable\nevidence internally but under-utilize it, making such signals explicit can\nbridge the gap between perception and reasoning, advancing the diagnostic\nunderstanding and reliability of VLMs.",
            "headline_zh": "提出基于深度层注意力干预的方法，以解决视觉语言模型中视觉证据感知与答案正确性脱节的问题。",
            "intro_zh": [
                "核心问题：视觉语言模型在视觉证据存在时仍输出错误答案，出现'看见但不相信'现象。",
                "方法要点：通过分析层间注意力动态，引入无需训练的推理时干预，突出深度层证据区域。",
                "实验或效果：干预方法在LLaVA、Qwen等模型上一致提升准确性，增强模型可靠性。"
            ],
            "tags_zh": [
                "视觉语言模型",
                "注意力机制",
                "推理干预",
                "视觉问答",
                "模型诊断"
            ],
            "_index": 8
        },
        {
            "title": "VERA-V: Variational Inference Framework for Jailbreaking Vision-Language Models",
            "authors": [
                "Qilin Liao",
                "Anamika Lochab",
                "Ruqi Zhang"
            ],
            "arxiv_id": "2510.17759v1",
            "summary": "Vision-Language Models (VLMs) extend large language models with visual\nreasoning, but their multimodal design also introduces new, underexplored\nvulnerabilities. Existing multimodal red-teaming methods largely rely on\nbrittle templates, focus on single-attack settings, and expose only a narrow\nsubset of vulnerabilities. To address these limitations, we introduce VERA-V, a\nvariational inference framework that recasts multimodal jailbreak discovery as\nlearning a joint posterior distribution over paired text-image prompts. This\nprobabilistic view enables the generation of stealthy, coupled adversarial\ninputs that bypass model guardrails. We train a lightweight attacker to\napproximate the posterior, allowing efficient sampling of diverse jailbreaks\nand providing distributional insights into vulnerabilities. VERA-V further\nintegrates three complementary strategies: (i) typography-based text prompts\nthat embed harmful cues, (ii) diffusion-based image synthesis that introduces\nadversarial signals, and (iii) structured distractors to fragment VLM\nattention. Experiments on HarmBench and HADES benchmarks show that VERA-V\nconsistently outperforms state-of-the-art baselines on both open-source and\nfrontier VLMs, achieving up to 53.75% higher attack success rate (ASR) over the\nbest baseline on GPT-4o.",
            "headline_zh": "提出VERA-V变分推理框架以解决多模态模型越狱攻击问题",
            "intro_zh": [
                "多模态视觉语言模型存在未充分探索的脆弱性，现有攻击方法依赖脆弱模板且覆盖范围窄",
                "将越狱发现建模为学习文本-图像提示的联合后验分布，生成隐蔽对抗输入并集成多种策略",
                "在HarmBench和HADES基准测试中，攻击成功率最高提升53.75%，优于现有方法"
            ],
            "tags_zh": [
                "多模态模型安全",
                "变分推理",
                "对抗攻击",
                "视觉语言模型",
                "越狱发现",
                "后验分布学习"
            ],
            "_index": 9
        },
        {
            "title": "Joint Multi-Condition Representation Modelling via Matrix Factorisation for Visual Place Recognition",
            "authors": [
                "Timur Ismagilov",
                "Shakaiba Majeed",
                "Michael Milford",
                "Tan Viet Tuyen Nguyen",
                "Sarvapali D. Ramchurn",
                "Shoaib Ehsan"
            ],
            "arxiv_id": "2510.17739v1",
            "summary": "We address multi-reference visual place recognition (VPR), where reference\nsets captured under varying conditions are used to improve localisation\nperformance. While deep learning with large-scale training improves robustness,\nincreasing data diversity and model complexity incur extensive computational\ncost during training and deployment. Descriptor-level fusion via voting or\naggregation avoids training, but often targets multi-sensor setups or relies on\nheuristics with limited gains under appearance and viewpoint change. We propose\na training-free, descriptor-agnostic approach that jointly models places using\nmultiple reference descriptors via matrix decomposition into basis\nrepresentations, enabling projection-based residual matching. We also introduce\nSotonMV, a structured benchmark for multi-viewpoint VPR. On multi-appearance\ndata, our method improves Recall@1 by up to ~18% over single-reference and\noutperforms multi-reference baselines across appearance and viewpoint changes,\nwith gains of ~5% on unstructured data, demonstrating strong generalisation\nwhile remaining lightweight.",
            "headline_zh": "提出基于矩阵分解的多条件联合建模方法以提升视觉地点识别性能",
            "intro_zh": [
                "多参考视觉地点识别中，数据多样性和模型复杂性导致高计算成本",
                "采用无训练、描述符无关的矩阵分解方法，分解为基表示并实现残差匹配",
                "在SotonMV基准上，Recall@1提升达18%，泛化性强且保持轻量级"
            ],
            "tags_zh": [
                "视觉地点识别",
                "矩阵分解",
                "多条件建模",
                "无训练方法",
                "残差匹配",
                "基准数据集"
            ],
            "_index": 10
        },
        {
            "title": "Can Image-To-Video Models Simulate Pedestrian Dynamics?",
            "authors": [
                "Aaron Appelle",
                "Jerome P. Lynch"
            ],
            "arxiv_id": "2510.17731v1",
            "summary": "Recent high-performing image-to-video (I2V) models based on variants of the\ndiffusion transformer (DiT) have displayed remarkable inherent world-modeling\ncapabilities by virtue of training on large scale video datasets. We\ninvestigate whether these models can generate realistic pedestrian movement\npatterns in crowded public scenes. Our framework conditions I2V models on\nkeyframes extracted from pedestrian trajectory benchmarks, then evaluates their\ntrajectory prediction performance using quantitative measures of pedestrian\ndynamics.",
            "headline_zh": "评估图像到视频模型模拟拥挤场景中行人动态的能力",
            "intro_zh": [
                "核心问题：图像到视频模型能否生成真实的行人运动模式",
                "方法要点：基于关键帧条件化扩散变换器模型进行视频生成",
                "实验或效果：使用行人轨迹基准进行定量评估动态性能"
            ],
            "tags_zh": [
                "图像到视频模型",
                "行人动态模拟",
                "扩散变换器",
                "轨迹预测",
                "关键帧条件化"
            ],
            "_index": 11
        },
        {
            "title": "Signature Forgery Detection: Improving Cross-Dataset Generalization",
            "authors": [
                "Matheus Ramos Parracho"
            ],
            "arxiv_id": "2510.17724v1",
            "summary": "Automated signature verification is a critical biometric technique used in\nbanking, identity authentication, and legal documentation. Despite the notable\nprogress achieved by deep learning methods, most approaches in offline\nsignature verification still struggle to generalize across datasets, as\nvariations in handwriting styles and acquisition protocols often degrade\nperformance. This study investigates feature learning strategies for signature\nforgery detection, focusing on improving cross-dataset generalization -- that\nis, model robustness when trained on one dataset and tested on another. Using\nthree public benchmarks -- CEDAR, ICDAR, and GPDS Synthetic -- two experimental\npipelines were developed: one based on raw signature images and another\nemploying a preprocessing method referred to as shell preprocessing. Several\nbehavioral patterns were identified and analyzed; however, no definitive\nsuperiority between the two approaches was established. The results show that\nthe raw-image model achieved higher performance across benchmarks, while the\nshell-based model demonstrated promising potential for future refinement toward\nrobust, cross-domain signature verification.",
            "headline_zh": "研究特征学习策略以提升签名伪造检测的跨数据集泛化能力",
            "intro_zh": [
                "核心问题：离线签名验证模型在跨数据集时泛化性能差，受手写风格和采集协议变化影响。",
                "方法要点：开发两种实验流程，基于原始图像和壳预处理，探索特征学习策略。",
                "实验或效果：原始图像模型性能更高，壳预处理模型显示未来改进潜力。"
            ],
            "tags_zh": [
                "签名伪造检测",
                "跨数据集泛化",
                "特征学习",
                "离线签名验证",
                "壳预处理"
            ],
            "_index": 12
        },
        {
            "title": "MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating Multimodal LLMs in Multi-Turn Dialogues",
            "authors": [
                "Yaning Pan",
                "Zekun Wang",
                "Qianqian Xie",
                "Yongqian Wen",
                "Yuanxing Zhang",
                "Guohui Zhang",
                "Haoxuan Hu",
                "Zhiyu Pan",
                "Yibing Huang",
                "Zhidong Gan",
                "Yonghong Lin",
                "An Ping",
                "Tianhao Peng",
                "Jiaheng Liu"
            ],
            "arxiv_id": "2510.17722v1",
            "summary": "The recent development of Multimodal Large Language Models (MLLMs) has\nsignificantly advanced AI's ability to understand visual modalities. However,\nexisting evaluation benchmarks remain limited to single-turn question\nanswering, overlooking the complexity of multi-turn dialogues in real-world\nscenarios. To bridge this gap, we introduce MT-Video-Bench, a holistic video\nunderstanding benchmark for evaluating MLLMs in multi-turn dialogues.\nSpecifically, our MT-Video-Bench mainly assesses six core competencies that\nfocus on perceptivity and interactivity, encompassing 987 meticulously curated\nmulti-turn dialogues from diverse domains. These capabilities are rigorously\naligned with real-world applications, such as interactive sports analysis and\nmulti-turn video-based intelligent tutoring. With MT-Video-Bench, we\nextensively evaluate various state-of-the-art open-source and closed-source\nMLLMs, revealing their significant performance discrepancies and limitations in\nhandling multi-turn video dialogues. The benchmark will be publicly available\nto foster future research.",
            "headline_zh": "提出MT-Video-Bench基准以评估多模态大语言模型在多轮视频对话中的表现",
            "intro_zh": [
                "现有基准局限于单轮问答，无法评估多轮对话的复杂性",
                "构建包含987个多轮对话的基准，评估感知与交互等六项核心能力",
                "评估显示开源与闭源模型在多轮视频对话中存在显著性能差距"
            ],
            "tags_zh": [
                "多模态大语言模型",
                "视频理解",
                "多轮对话",
                "基准评估",
                "交互能力",
                "感知能力"
            ],
            "_index": 13
        },
        {
            "title": "Raindrop GS: A Benchmark for 3D Gaussian Splatting under Raindrop Conditions",
            "authors": [
                "Zhiqiang Teng",
                "Beibei Lin",
                "Tingting Chen",
                "Zifeng Yuan",
                "Xuanyi Li",
                "Xuanyu Zhang",
                "Shunli Zhang"
            ],
            "arxiv_id": "2510.17719v1",
            "summary": "3D Gaussian Splatting (3DGS) under raindrop conditions suffers from severe\nocclusions and optical distortions caused by raindrop contamination on the\ncamera lens, substantially degrading reconstruction quality. Existing\nbenchmarks typically evaluate 3DGS using synthetic raindrop images with known\ncamera poses (constrained images), assuming ideal conditions. However, in\nreal-world scenarios, raindrops often interfere with accurate camera pose\nestimation and point cloud initialization. Moreover, a significant domain gap\nbetween synthetic and real raindrops further impairs generalization. To tackle\nthese issues, we introduce RaindropGS, a comprehensive benchmark designed to\nevaluate the full 3DGS pipeline-from unconstrained, raindrop-corrupted images\nto clear 3DGS reconstructions. Specifically, the whole benchmark pipeline\nconsists of three parts: data preparation, data processing, and raindrop-aware\n3DGS evaluation, including types of raindrop interference, camera pose\nestimation and point cloud initialization, single image rain removal\ncomparison, and 3D Gaussian training comparison. First, we collect a real-world\nraindrop reconstruction dataset, in which each scene contains three aligned\nimage sets: raindrop-focused, background-focused, and rain-free ground truth,\nenabling a comprehensive evaluation of reconstruction quality under different\nfocus conditions. Through comprehensive experiments and analyses, we reveal\ncritical insights into the performance limitations of existing 3DGS methods on\nunconstrained raindrop images and the varying impact of different pipeline\ncomponents: the impact of camera focus position on 3DGS reconstruction\nperformance, and the interference caused by inaccurate pose and point cloud\ninitialization on reconstruction. These insights establish clear directions for\ndeveloping more robust 3DGS methods under raindrop conditions.",
            "headline_zh": "提出RaindropGS基准以评估雨滴条件下3D高斯泼溅重建性能",
            "intro_zh": [
                "核心问题：雨滴导致遮挡和光学畸变，影响3D高斯泼溅重建质量与相机姿态估计",
                "方法要点：构建真实雨滴数据集，包含多焦点图像集，支持全流程评估",
                "实验或效果：揭示相机焦点位置和姿态初始化对重建性能的关键影响"
            ],
            "tags_zh": [
                "3D高斯泼溅",
                "雨滴干扰",
                "基准评估",
                "相机姿态估计",
                "点云初始化",
                "图像去雨"
            ],
            "_index": 14
        },
        {
            "title": "Automatic Classification of Circulating Blood Cell Clusters based on Multi-channel Flow Cytometry Imaging",
            "authors": [
                "Suqiang Ma",
                "Subhadeep Sengupta",
                "Yao Lee",
                "Beikang Gu",
                "Xianyan Chen",
                "Xianqiao Wang",
                "Yang Liu",
                "Mengjia Xu",
                "Galit H. Frydman",
                "He Li"
            ],
            "arxiv_id": "2510.17716v1",
            "summary": "Circulating blood cell clusters (CCCs) containing red blood cells (RBCs),\nwhite blood cells(WBCs), and platelets are significant biomarkers linked to\nconditions like thrombosis, infection, and inflammation. Flow cytometry, paired\nwith fluorescence staining, is commonly used to analyze these cell clusters,\nrevealing cell morphology and protein profiles. While computational approaches\nbased on machine learning have advanced the automatic analysis of single-cell\nflow cytometry images, there is a lack of effort to build tools to\nautomatically analyze images containing CCCs. Unlike single cells, cell\nclusters often exhibit irregular shapes and sizes. In addition, these cell\nclusters often consist of heterogeneous cell types, which require multi-channel\nstaining to identify the specific cell types within the clusters. This study\nintroduces a new computational framework for analyzing CCC images and\nidentifying cell types within clusters. Our framework uses a two-step analysis\nstrategy. First, it categorizes images into cell cluster and non-cluster groups\nby fine-tuning the You Only Look Once(YOLOv11) model, which outperforms\ntraditional convolutional neural networks (CNNs), Vision Transformers (ViT).\nThen, it identifies cell types by overlaying cluster contours with regions from\nmulti-channel fluorescence stains, enhancing accuracy despite cell debris and\nstaining artifacts. This approach achieved over 95% accuracy in both cluster\nclassification and phenotype identification. In summary, our automated\nframework effectively analyzes CCC images from flow cytometry, leveraging both\nbright-field and fluorescence data. Initially tested on blood cells, it holds\npotential for broader applications, such as analyzing immune and tumor cell\nclusters, supporting cellular research across various diseases.",
            "headline_zh": "提出基于多通道流式细胞术成像的循环血细胞簇自动分类框架",
            "intro_zh": [
                "核心问题：循环血细胞簇自动分析工具缺乏，其不规则形状和异质细胞类型增加识别难度",
                "方法要点：采用两步策略，先微调YOLOv11模型分类图像，再叠加轮廓与荧光区域识别细胞类型",
                "实验或效果：在簇分类和表型识别中准确率超过95%，有效处理细胞碎片和染色伪影"
            ],
            "tags_zh": [
                "循环血细胞簇",
                "多通道流式细胞术",
                "YOLOv11模型",
                "细胞类型识别",
                "图像分类",
                "荧光染色"
            ],
            "_index": 15
        },
        {
            "title": "Improving Cross-Patient Generalization in Parkinson's Disease Detection through Chunk-Based Analysis of Hand-Drawn Patterns",
            "authors": [
                "Mhd Adnan Albani",
                "Riad Sonbol"
            ],
            "arxiv_id": "2510.17703v1",
            "summary": "Parkinson's disease (PD) is a neurodegenerative disease affecting about 1% of\npeople over the age of 60, causing motor impairments that impede hand\ncoordination activities such as writing and drawing. Many approaches have tried\nto support early detection of Parkinson's disease based on hand-drawn images;\nhowever, we identified two major limitations in the related works: (1) the lack\nof sufficient datasets, (2) the robustness when dealing with unseen patient\ndata. In this paper, we propose a new approach to detect Parkinson's disease\nthat consists of two stages: The first stage classifies based on their drawing\ntype(circle, meander, spiral), and the second stage extracts the required\nfeatures from the images and detects Parkinson's disease. We overcame the\nprevious two limitations by applying a chunking strategy where we divide each\nimage into 2x2 chunks. Each chunk is processed separately when extracting\nfeatures and recognizing Parkinson's disease indicators. To make the final\nclassification, an ensemble method is used to merge the decisions made from\neach chunk. Our evaluation shows that our proposed approach outperforms the top\nperforming state-of-the-art approaches, in particular on unseen patients. On\nthe NewHandPD dataset our approach, it achieved 97.08% accuracy for seen\npatients and 94.91% for unseen patients, our proposed approach maintained a gap\nof only 2.17 percentage points, compared to the 4.76-point drop observed in\nprior work.",
            "headline_zh": "提出基于图像分块的帕金森病检测方法以提升跨患者泛化能力",
            "intro_zh": [
                "核心问题：现有方法在未见患者数据上泛化能力不足且数据集有限。",
                "方法要点：采用2x2分块策略，分阶段分类绘图类型并提取特征，使用集成方法融合决策。",
                "实验或效果：在NewHandPD数据集上，未见患者准确率达94.91%，泛化差距仅2.17个百分点。"
            ],
            "tags_zh": [
                "帕金森病检测",
                "图像分块分析",
                "跨患者泛化",
                "手绘图识别",
                "集成学习"
            ],
            "_index": 16
        },
        {
            "title": "Elastic ViTs from Pretrained Models without Retraining",
            "authors": [
                "Walter Simoncini",
                "Michael Dorkenwald",
                "Tijmen Blankevoort",
                "Cees G. M. Snoek",
                "Yuki M. Asano"
            ],
            "arxiv_id": "2510.17700v1",
            "summary": "Vision foundation models achieve remarkable performance but are only\navailable in a limited set of pre-determined sizes, forcing sub-optimal\ndeployment choices under real-world constraints. We introduce SnapViT:\nSingle-shot network approximation for pruned Vision Transformers, a new\npost-pretraining structured pruning method that enables elastic inference\nacross a continuum of compute budgets. Our approach efficiently combines\ngradient information with cross-network structure correlations, approximated\nvia an evolutionary algorithm, does not require labeled data, generalizes to\nmodels without a classification head, and is retraining-free. Experiments on\nDINO, SigLIPv2, DeIT, and AugReg models demonstrate superior performance over\nstate-of-the-art methods across various sparsities, requiring less than five\nminutes on a single A100 GPU to generate elastic models that can be adjusted to\nany computational budget. Our key contributions include an efficient pruning\nstrategy for pretrained Vision Transformers, a novel evolutionary approximation\nof Hessian off-diagonal structures, and a self-supervised importance scoring\nmechanism that maintains strong performance without requiring retraining or\nlabels. Code and pruned models are available at: https://elastic.ashita.nl/",
            "headline_zh": "提出SnapViT方法，实现预训练视觉Transformer的弹性推理，无需重训练。",
            "intro_zh": [
                "视觉基础模型尺寸固定，难以适应实际部署的计算约束。",
                "结合梯度与跨网络结构相关性，使用进化算法近似Hessian结构，无需标签或重训练。",
                "在多种模型上优于现有方法，生成弹性模型仅需数分钟，支持任意计算预算。"
            ],
            "tags_zh": [
                "视觉Transformer",
                "结构化剪枝",
                "弹性推理",
                "进化算法",
                "自监督评分",
                "预训练模型"
            ],
            "_index": 17
        },
        {
            "title": "GAS: Improving Discretization of Diffusion ODEs via Generalized Adversarial Solver",
            "authors": [
                "Aleksandr Oganov",
                "Ilya Bykov",
                "Eva Neudachina",
                "Mishan Aliev",
                "Alexander Tolmachev",
                "Alexander Sidorov",
                "Aleksandr Zuev",
                "Andrey Okhotin",
                "Denis Rakitin",
                "Aibek Alanov"
            ],
            "arxiv_id": "2510.17699v1",
            "summary": "While diffusion models achieve state-of-the-art generation quality, they\nstill suffer from computationally expensive sampling. Recent works address this\nissue with gradient-based optimization methods that distill a few-step ODE\ndiffusion solver from the full sampling process, reducing the number of\nfunction evaluations from dozens to just a few. However, these approaches often\nrely on intricate training techniques and do not explicitly focus on preserving\nfine-grained details. In this paper, we introduce the Generalized Solver: a\nsimple parameterization of the ODE sampler that does not require additional\ntraining tricks and improves quality over existing approaches. We further\ncombine the original distillation loss with adversarial training, which\nmitigates artifacts and enhances detail fidelity. We call the resulting method\nthe Generalized Adversarial Solver and demonstrate its superior performance\ncompared to existing solver training methods under similar resource\nconstraints. Code is available at https://github.com/3145tttt/GAS.",
            "headline_zh": "提出广义对抗求解器以改进扩散ODE离散化，提升生成质量与效率",
            "intro_zh": [
                "扩散模型采样计算成本高，现有蒸馏方法依赖复杂训练且细节保留不足",
                "引入广义求解器参数化，无需额外训练技巧，结合对抗训练减少伪影",
                "在相似资源约束下，性能优于现有求解器训练方法，代码已开源"
            ],
            "tags_zh": [
                "扩散模型",
                "ODE求解器",
                "对抗训练",
                "模型蒸馏",
                "采样效率"
            ],
            "_index": 18
        },
        {
            "title": "Towards 3D Objectness Learning in an Open World",
            "authors": [
                "Taichi Liu",
                "Zhenyu Wang",
                "Ruofeng Liu",
                "Guang Wang",
                "Desheng Zhang"
            ],
            "arxiv_id": "2510.17686v1",
            "summary": "Recent advancements in 3D object detection and novel category detection have\nmade significant progress, yet research on learning generalized 3D objectness\nremains insufficient. In this paper, we delve into learning open-world 3D\nobjectness, which focuses on detecting all objects in a 3D scene, including\nnovel objects unseen during training. Traditional closed-set 3D detectors\nstruggle to generalize to open-world scenarios, while directly incorporating 3D\nopen-vocabulary models for open-world ability struggles with vocabulary\nexpansion and semantic overlap. To achieve generalized 3D object discovery, We\npropose OP3Det, a class-agnostic Open-World Prompt-free 3D Detector to detect\nany objects within 3D scenes without relying on hand-crafted text prompts. We\nintroduce the strong generalization and zero-shot capabilities of 2D foundation\nmodels, utilizing both 2D semantic priors and 3D geometric priors for\nclass-agnostic proposals to broaden 3D object discovery. Then, by integrating\ncomplementary information from point cloud and RGB image in the cross-modal\nmixture of experts, OP3Det dynamically routes uni-modal and multi-modal\nfeatures to learn generalized 3D objectness. Extensive experiments demonstrate\nthe extraordinary performance of OP3Det, which significantly surpasses existing\nopen-world 3D detectors by up to 16.0% in AR and achieves a 13.5% improvement\ncompared to closed-world 3D detectors.",
            "headline_zh": "提出OP3Det以解决开放世界3D物体检测问题",
            "intro_zh": [
                "核心问题：传统3D检测器在开放世界中泛化不足，难以检测未见物体。",
                "方法要点：融合2D语义先验和3D几何先验，实现类别无关的物体提议。",
                "实验或效果：在AR指标上超越现有方法达16.0%，泛化性能显著提升。"
            ],
            "tags_zh": [
                "3D物体检测",
                "开放世界学习",
                "跨模态融合",
                "零样本检测",
                "类别无关检测"
            ],
            "_index": 19
        },
        {
            "title": "Multilingual Text-to-Image Person Retrieval via Bidirectional Relation Reasoning and Aligning",
            "authors": [
                "Min Cao",
                "Xinyu Zhou",
                "Ding Jiang",
                "Bo Du",
                "Mang Ye",
                "Min Zhang"
            ],
            "arxiv_id": "2510.17685v1",
            "summary": "Text-to-image person retrieval (TIPR) aims to identify the target person\nusing textual descriptions, facing challenge in modality heterogeneity. Prior\nworks have attempted to address it by developing cross-modal global or local\nalignment strategies. However, global methods typically overlook fine-grained\ncross-modal differences, whereas local methods require prior information to\nexplore explicit part alignments. Additionally, current methods are\nEnglish-centric, restricting their application in multilingual contexts. To\nalleviate these issues, we pioneer a multilingual TIPR task by developing a\nmultilingual TIPR benchmark, for which we leverage large language models for\ninitial translations and refine them by integrating domain-specific knowledge.\nCorrespondingly, we propose Bi-IRRA: a Bidirectional Implicit Relation\nReasoning and Aligning framework to learn alignment across languages and\nmodalities. Within Bi-IRRA, a bidirectional implicit relation reasoning module\nenables bidirectional prediction of masked image and text, implicitly enhancing\nthe modeling of local relations across languages and modalities, a\nmulti-dimensional global alignment module is integrated to bridge the modality\nheterogeneity. The proposed method achieves new state-of-the-art results on all\nmultilingual TIPR datasets. Data and code are presented in\nhttps://github.com/Flame-Chasers/Bi-IRRA.",
            "headline_zh": "提出Bi-IRRA框架以解决多语言文本到图像行人检索中的模态异质性问题",
            "intro_zh": [
                "核心问题：文本与图像模态异质性及英语中心化限制多语言应用",
                "方法要点：双向隐式关系推理模块和多维全局对齐模块实现跨语言模态对齐",
                "实验或效果：在多语言TIPR数据集上达到新最优结果，提供数据和代码"
            ],
            "tags_zh": [
                "多语言文本到图像检索",
                "模态对齐",
                "行人检索",
                "双向推理",
                "隐式关系建模"
            ],
            "_index": 20
        },
        {
            "title": "Intelligent Communication Mixture-of-Experts Boosted-Medical Image Segmentation Foundation Model",
            "authors": [
                "Xinwei Zhang",
                "Hu Chen",
                "Zhe Yuan",
                "Sukun Tian",
                "Peng Feng"
            ],
            "arxiv_id": "2510.17684v1",
            "summary": "Foundation models for medical image segmentation have achieved remarkable\nperformance. Adaptive fine-tuning of natural image segmentation foundation\nmodels is crucial for medical image segmentation tasks. However, some\nlimitations exist in existing fine-tuning methods: 1) insufficient\nrepresentation of high-level features and 2) the fine-tuning process disrupts\nthe structural integrity of pretrained weights. Inspired by these critical\nproblems, we propose an intelligent communication mixture-of-experts\nboosted-medical image segmentation foundation model, named IC-MoE, with twofold\nideas: 1) We construct basic experts, semantic experts, and adaptive experts.\nMoreover, we implement a pixel probability adaptive voting strategy, which\nenables expert selection and fusion through label consistency and load\nbalancing. This approach preliminarily enhances the representation capability\nof high-level features while preserving the structural integrity of pretrained\nweights. 2) We propose a semantic-guided contrastive learning method to address\nthe issue of weak supervision in contrastive learning. This method further\nenhances the representation capability of high-level features while preserving\nthe structural integrity of pretrained weights. Extensive experiments across\nthree public medical image segmentation datasets demonstrate that the IC-MoE\noutperforms other SOTA models. Consequently, the proposed IC-MoE effectively\nsupplements foundational medical image segmentation models with high-level\nfeatures and pretrained structural integrity. We also validate the superior\ngeneralizability of the IC-MoE across diverse medical image segmentation\nscenarios.",
            "headline_zh": "提出IC-MoE模型以增强医学图像分割基础模型的高层特征表示和预训练权重完整性",
            "intro_zh": [
                "现有方法高层特征表示不足且微调破坏预训练权重结构完整性",
                "构建多专家模块并采用像素概率自适应投票策略进行专家选择与融合",
                "在三个公共数据集上实验显示IC-MoE优于其他SOTA模型，验证其泛化能力"
            ],
            "tags_zh": [
                "医学图像分割",
                "混合专家模型",
                "对比学习",
                "自适应微调",
                "高层特征表示"
            ],
            "_index": 21
        },
        {
            "title": "PICABench: How Far Are We from Physically Realistic Image Editing?",
            "authors": [
                "Yuandong Pu",
                "Le Zhuo",
                "Songhao Han",
                "Jinbo Xing",
                "Kaiwen Zhu",
                "Shuo Cao",
                "Bin Fu",
                "Si Liu",
                "Hongsheng Li",
                "Yu Qiao",
                "Wenlong Zhang",
                "Xi Chen",
                "Yihao Liu"
            ],
            "arxiv_id": "2510.17681v1",
            "summary": "Image editing has achieved remarkable progress recently. Modern editing\nmodels could already follow complex instructions to manipulate the original\ncontent. However, beyond completing the editing instructions, the accompanying\nphysical effects are the key to the generation realism. For example, removing\nan object should also remove its shadow, reflections, and interactions with\nnearby objects. Unfortunately, existing models and benchmarks mainly focus on\ninstruction completion but overlook these physical effects. So, at this moment,\nhow far are we from physically realistic image editing? To answer this, we\nintroduce PICABench, which systematically evaluates physical realism across\neight sub-dimension (spanning optics, mechanics, and state transitions) for\nmost of the common editing operations (add, remove, attribute change, etc). We\nfurther propose the PICAEval, a reliable evaluation protocol that uses\nVLM-as-a-judge with per-case, region-level human annotations and questions.\nBeyond benchmarking, we also explore effective solutions by learning physics\nfrom videos and construct a training dataset PICA-100K. After evaluating most\nof the mainstream models, we observe that physical realism remains a\nchallenging problem with large rooms to explore. We hope that our benchmark and\nproposed solutions can serve as a foundation for future work moving from naive\ncontent editing toward physically consistent realism.",
            "headline_zh": "提出PICABench以评估图像编辑的物理真实性问题",
            "intro_zh": [
                "核心问题：现有图像编辑模型忽视物理效果，如阴影和反射的移除",
                "方法要点：构建PICABench基准，涵盖八维物理属性和多种编辑操作",
                "实验或效果：评估主流模型，发现物理真实性问题仍具挑战性"
            ],
            "tags_zh": [
                "图像编辑",
                "物理真实",
                "基准评估",
                "视觉语言模型",
                "数据集构建"
            ],
            "_index": 22
        },
        {
            "title": "4DSegStreamer: Streaming 4D Panoptic Segmentation via Dual Threads",
            "authors": [
                "Ling Liu",
                "Jun Tian",
                "Li Yi"
            ],
            "arxiv_id": "2510.17664v1",
            "summary": "4D panoptic segmentation in a streaming setting is critical for highly\ndynamic environments, such as evacuating dense crowds and autonomous driving in\ncomplex scenarios, where real-time, fine-grained perception within a\nconstrained time budget is essential. In this paper, we introduce\n4DSegStreamer, a novel framework that employs a Dual-Thread System to\nefficiently process streaming frames. The framework is general and can be\nseamlessly integrated into existing 3D and 4D segmentation methods to enable\nreal-time capability. It also demonstrates superior robustness compared to\nexisting streaming perception approaches, particularly under high FPS\nconditions. The system consists of a predictive thread and an inference thread.\nThe predictive thread leverages historical motion and geometric information to\nextract features and forecast future dynamics. The inference thread ensures\ntimely prediction for incoming frames by aligning with the latest memory and\ncompensating for ego-motion and dynamic object movements. We evaluate\n4DSegStreamer on the indoor HOI4D dataset and the outdoor SemanticKITTI and\nnuScenes datasets. Comprehensive experiments demonstrate the effectiveness of\nour approach, particularly in accurately predicting dynamic objects in complex\nscenes.",
            "headline_zh": "提出4DSegStreamer框架，通过双线程系统实现流式4D全景分割，适用于动态环境实时感知。",
            "intro_zh": [
                "核心问题：流式4D全景分割在动态环境中需实时细粒度感知，时间预算受限。",
                "方法要点：采用预测线程和推理线程，预测未来动态并补偿运动，确保及时预测。",
                "实验或效果：在HOI4D、SemanticKITTI和nuScenes数据集上验证有效性，尤其在高FPS下鲁棒。"
            ],
            "tags_zh": [
                "4D全景分割",
                "流式感知",
                "双线程系统",
                "动态环境",
                "实时预测"
            ],
            "_index": 23
        },
        {
            "title": "Frugal Federated Learning for Violence Detection: A Comparison of LoRA-Tuned VLMs and Personalized CNNs",
            "authors": [
                "Sébastien Thuau",
                "Siba Haidar",
                "Ayush Bajracharya",
                "Rachid Chelouah"
            ],
            "arxiv_id": "2510.17651v1",
            "summary": "We examine frugal federated learning approaches to violence detection by\ncomparing two complementary strategies: (i) zero-shot and federated fine-tuning\nof vision-language models (VLMs), and (ii) personalized training of a compact\n3D convolutional neural network (CNN3D). Using LLaVA-7B and a 65.8M parameter\nCNN3D as representative cases, we evaluate accuracy, calibration, and energy\nusage under realistic non-IID settings.\n  Both approaches exceed 90% accuracy. CNN3D slightly outperforms Low-Rank\nAdaptation(LoRA)-tuned VLMs in ROC AUC and log loss, while using less energy.\nVLMs remain favorable for contextual reasoning and multimodal inference. We\nquantify energy and CO$_2$ emissions across training and inference, and analyze\nsustainability trade-offs for deployment.\n  To our knowledge, this is the first comparative study of LoRA-tuned\nvision-language models and personalized CNNs for federated violence detection,\nwith an emphasis on energy efficiency and environmental metrics.\n  These findings support a hybrid model: lightweight CNNs for routine\nclassification, with selective VLM activation for complex or descriptive\nscenarios. The resulting framework offers a reproducible baseline for\nresponsible, resource-aware AI in video surveillance, with extensions toward\nreal-time, multimodal, and lifecycle-aware systems.",
            "headline_zh": "比较LoRA调优视觉语言模型与个性化CNN在联邦暴力检测中的性能与能效",
            "intro_zh": [
                "核心问题：在非独立同分布联邦学习设置下，实现高精度、低能耗的暴力检测。",
                "方法要点：对比零样本/联邦微调视觉语言模型与个性化训练紧凑3D卷积神经网络。",
                "实验或效果：两种方法准确率超90%，CNN3D在ROC AUC和能耗上略优，VLMs适合上下文推理。"
            ],
            "tags_zh": [
                "联邦学习",
                "暴力检测",
                "视觉语言模型",
                "低秩适应",
                "3D卷积神经网络",
                "能效分析"
            ],
            "_index": 24
        },
        {
            "title": "ZACH-ViT: A Zero-Token Vision Transformer with ShuffleStrides Data Augmentation for Robust Lung Ultrasound Classification",
            "authors": [
                "Athanasios Angelakis",
                "Amne Mousa",
                "Micah L. A. Heldeweg",
                "Laurens A. Biesheuvel",
                "Mark A. Haaksma",
                "Jasper M. Smit",
                "Pieter R. Tuinman",
                "Paul W. G. Elbers"
            ],
            "arxiv_id": "2510.17650v1",
            "summary": "Differentiating cardiogenic pulmonary oedema (CPE) from non-cardiogenic and\nstructurally normal lungs in lung ultrasound (LUS) videos remains challenging\ndue to the high visual variability of non-cardiogenic inflammatory patterns\n(NCIP/ARDS-like), interstitial lung disease, and healthy lungs. This\nheterogeneity complicates automated classification as overlapping B-lines and\npleural artefacts are common. We introduce ZACH-ViT (Zero-token Adaptive\nCompact Hierarchical Vision Transformer), a 0.25 M-parameter Vision Transformer\nvariant that removes both positional embeddings and the [CLS] token, making it\nfully permutation-invariant and suitable for unordered medical image data. To\nenhance generalization, we propose ShuffleStrides Data Augmentation (SSDA),\nwhich permutes probe-view sequences and frame orders while preserving\nanatomical validity. ZACH-ViT was evaluated on 380 LUS videos from 95\ncritically ill patients against nine state-of-the-art baselines. Despite the\nheterogeneity of the non-cardiogenic group, ZACH-ViT achieved the highest\nvalidation and test ROC-AUC (0.80 and 0.79) with balanced sensitivity (0.60)\nand specificity (0.91), while all competing models collapsed to trivial\nclassification. It trains 1.35x faster than Minimal ViT (0.62M parameters) with\n2.5x fewer parameters, supporting real-time clinical deployment. These results\nshow that aligning architectural design with data structure can outperform\nscale in small-data medical imaging.",
            "headline_zh": "提出ZACH-ViT与ShuffleStrides数据增强，用于鲁棒的肺部超声分类。",
            "intro_zh": [
                "核心问题：肺部超声视频中区分心源性肺水肿与非心源性/正常肺的视觉变异性高，导致分类困难。",
                "方法要点：ZACH-ViT移除位置嵌入和[CLS]标记，实现全排列不变性；SSDA增强泛化能力。",
                "实验或效果：在380个视频上验证，ZACH-ViT取得最高ROC-AUC，训练更快，参数更少。"
            ],
            "tags_zh": [
                "零标记视觉Transformer",
                "数据增强",
                "肺部超声分类",
                "排列不变性",
                "小数据医学成像"
            ],
            "_index": 25
        },
        {
            "title": "Self-supervised Pre-training for Mapping of Archaeological Stone Wall in Historic Landscapes Using High-Resolution DEM Derivatives",
            "authors": [
                "Zexian Huang",
                "Mashnoon Islam",
                "Brian Armstrong",
                "Kourosh Khoshelham",
                "Martin Tomko"
            ],
            "arxiv_id": "2510.17644v1",
            "summary": "Dry-stone walls hold significant heritage and environmental value. Mapping\nthese structures is essential for ecosystem preservation and wildfire\nmanagement in Australia. Yet, many walls remain unidentified due to their\ninaccessibility and the high cost of manual mapping. Deep learning-based\nsegmentation offers a scalable solution, but two major challenges persist: (1)\nvisual occlusion of low-lying walls by dense vegetation, and (2) limited\nlabeled data for supervised training. We propose DINO-CV, a segmentation\nframework for automatic mapping of low-lying dry-stone walls using\nhigh-resolution Airborne LiDAR-derived digital elevation models (DEMs). DEMs\novercome visual occlusion by capturing terrain structures hidden beneath\nvegetation, enabling analysis of structural rather than spectral cues. DINO-CV\nintroduces a self-supervised cross-view pre-training strategy based on\nknowledge distillation to mitigate data scarcity. It learns invariant visual\nand geometric representations across multiple DEM derivatives, supporting\nvarious vision backbones including ResNet, Wide ResNet, and Vision\nTransformers. Applied to the UNESCO World Heritage cultural landscape of Budj\nBim, Victoria, the method identifies one of Australia's densest collections of\ncolonial dry-stone walls beyond Indigenous heritage contexts. DINO-CV achieves\na mean Intersection over Union (mIoU) of 68.6% on test areas and maintains\n63.8% mIoU when fine-tuned with only 10% labeled data. These results\ndemonstrate the potential of self-supervised learning on high-resolution DEM\nderivatives for automated dry-stone wall mapping in vegetated and heritage-rich\nenvironments with scarce annotations.",
            "headline_zh": "提出DINO-CV自监督预训练框架，用于高分辨率DEM衍生物中考古石墙的自动映射。",
            "intro_zh": [
                "核心问题：植被遮挡和标注数据稀缺阻碍干石墙的自动映射。",
                "方法要点：基于知识蒸馏的自监督跨视图预训练，学习DEM衍生物的视觉和几何不变表示。",
                "实验或效果：在Budj Bim测试中，mIoU达68.6%，仅用10%标注数据时保持63.8%。"
            ],
            "tags_zh": [
                "自监督学习",
                "DEM衍生物分割",
                "知识蒸馏",
                "考古石墙映射",
                "高分辨率LiDAR"
            ],
            "_index": 26
        },
        {
            "title": "RESample: A Robust Data Augmentation Framework via Exploratory Sampling for Robotic Manipulation",
            "authors": [
                "Yuquan Xue",
                "Guanxing Lu",
                "Zhenyu Wu",
                "Chuanrui Zhang",
                "Bofang Jia",
                "Zhengyi Gu",
                "Yansong Tang",
                "Ziwei Wang"
            ],
            "arxiv_id": "2510.17640v1",
            "summary": "Vision-Language-Action models (VLAs) have demonstrated remarkable performance\non complex robotic manipulation tasks through imitation learning. However,\nexisting imitation learning datasets contain only successful trajectories and\nlack failure or recovery data, especially for out-of-distribution (OOD) states\nwhere the robot deviates from the main policy due to minor perturbations or\nerrors, leading VLA models to struggle with states deviating from the training\ndistribution. To this end, we propose an automated OOD data augmentation\nframework named RESample through exploratory sampling. Specifically, we first\nleverage offline reinforcement learning to obtain an action-value network that\naccurately identifies sub-optimal actions under the current manipulation\npolicy. We further sample potential OOD states from trajectories via rollout,\nand design an exploratory sampling mechanism that adaptively incorporates these\naction proxies into the training dataset to ensure efficiency. Subsequently,\nour framework explicitly encourages the VLAs to recover from OOD states and\nenhances their robustness against distributional shifts. We conduct extensive\nexperiments on the LIBERO benchmark as well as real-world robotic manipulation\ntasks, demonstrating that RESample consistently improves the stability and\ngeneralization ability of VLA models.",
            "headline_zh": "提出RESample框架，通过探索性采样增强机器人操作中视觉-语言-动作模型的鲁棒性。",
            "intro_zh": [
                "核心问题：模仿学习数据集缺乏失败和恢复数据，导致模型在分布外状态表现不佳。",
                "方法要点：利用离线强化学习识别次优动作，并通过探索性采样自动扩充OOD数据。",
                "实验或效果：在LIBERO基准和真实任务中验证，提升模型稳定性和泛化能力。"
            ],
            "tags_zh": [
                "机器人操作",
                "数据增强",
                "视觉-语言-动作模型",
                "离线强化学习",
                "分布外状态",
                "探索性采样"
            ],
            "_index": 27
        },
        {
            "title": "CaMiT: A Time-Aware Car Model Dataset for Classification and Generation",
            "authors": [
                "Frédéric LIN",
                "Biruk Abere Ambaw",
                "Adrian Popescu",
                "Hejer Ammar",
                "Romaric Audigier",
                "Hervé Le Borgne"
            ],
            "arxiv_id": "2510.17626v1",
            "summary": "AI systems must adapt to evolving visual environments, especially in domains\nwhere object appearances change over time. We introduce Car Models in Time\n(CaMiT), a fine-grained dataset capturing the temporal evolution of car models,\na representative class of technological artifacts. CaMiT includes 787K labeled\nsamples of 190 car models (2007-2023) and 5.1M unlabeled samples (2005-2023),\nsupporting both supervised and self-supervised learning. Static pretraining on\nin-domain data achieves competitive performance with large-scale generalist\nmodels while being more resource-efficient, yet accuracy declines when models\nare tested across years. To address this, we propose a time-incremental\nclassification setting, a realistic continual learning scenario with emerging,\nevolving, and disappearing classes. We evaluate two strategies:\ntime-incremental pretraining, which updates the backbone, and time-incremental\nclassifier learning, which updates only the final layer, both improving\ntemporal robustness. Finally, we explore time-aware image generation that\nleverages temporal metadata during training, yielding more realistic outputs.\nCaMiT offers a rich benchmark for studying temporal adaptation in fine-grained\nvisual recognition and generation.",
            "headline_zh": "提出CaMiT数据集以解决汽车模型随时间演变的视觉识别与生成问题",
            "intro_zh": [
                "核心问题：AI系统需适应视觉环境随时间变化，尤其在对象外观演变的领域",
                "方法要点：引入时间增量分类和生成策略，提升模型对新兴、演变和消失类的鲁棒性",
                "实验或效果：静态预训练资源高效，时间增量方法改善跨年测试准确性"
            ],
            "tags_zh": [
                "时间感知数据集",
                "细粒度分类",
                "持续学习",
                "图像生成",
                "汽车模型识别"
            ],
            "_index": 28
        },
        {
            "title": "ImaGGen: Zero-Shot Generation of Co-Speech Semantic Gestures Grounded in Language and Image Input",
            "authors": [
                "Hendric Voss",
                "Stefan Kopp"
            ],
            "arxiv_id": "2510.17617v1",
            "summary": "Human communication combines speech with expressive nonverbal cues such as\nhand gestures that serve manifold communicative functions. Yet, current\ngenerative gesture generation approaches are restricted to simple, repetitive\nbeat gestures that accompany the rhythm of speaking but do not contribute to\ncommunicating semantic meaning. This paper tackles a core challenge in\nco-speech gesture synthesis: generating iconic or deictic gestures that are\nsemantically coherent with a verbal utterance. Such gestures cannot be derived\nfrom language input alone, which inherently lacks the visual meaning that is\noften carried autonomously by gestures. We therefore introduce a zero-shot\nsystem that generates gestures from a given language input and additionally is\ninformed by imagistic input, without manual annotation or human intervention.\nOur method integrates an image analysis pipeline that extracts key object\nproperties such as shape, symmetry, and alignment, together with a semantic\nmatching module that links these visual details to spoken text. An inverse\nkinematics engine then synthesizes iconic and deictic gestures and combines\nthem with co-generated natural beat gestures for coherent multimodal\ncommunication. A comprehensive user study demonstrates the effectiveness of our\napproach. In scenarios where speech alone was ambiguous, gestures generated by\nour system significantly improved participants' ability to identify object\nproperties, confirming their interpretability and communicative value. While\nchallenges remain in representing complex shapes, our results highlight the\nimportance of context-aware semantic gestures for creating expressive and\ncollaborative virtual agents or avatars, marking a substantial step forward\ntowards efficient and robust, embodied human-agent interaction. More\ninformation and example videos are available here:\nhttps://review-anon-io.github.io/ImaGGen.github.io/",
            "headline_zh": "提出ImaGGen系统，通过语言和图像输入零样本生成语义手势以增强人机交互。",
            "intro_zh": [
                "核心问题：现有手势生成方法仅产生节拍手势，缺乏语义连贯性。",
                "方法要点：结合图像分析提取物体属性，语义匹配链接视觉与语言，逆运动学合成手势。",
                "实验或效果：用户研究表明，生成手势在模糊语音场景中显著提升物体属性识别能力。"
            ],
            "tags_zh": [
                "手势生成",
                "零样本学习",
                "语义匹配",
                "逆运动学",
                "多模态交互",
                "虚拟代理"
            ],
            "_index": 29
        },
        {
            "title": "One Dinomaly2 Detect Them All: A Unified Framework for Full-Spectrum Unsupervised Anomaly Detection",
            "authors": [
                "Jia Guo",
                "Shuai Lu",
                "Lei Fan",
                "Zelin Li",
                "Donglin Di",
                "Yang Song",
                "Weihang Zhang",
                "Wenbing Zhu",
                "Hong Yan",
                "Fang Chen",
                "Huiqi Li",
                "Hongen Liao"
            ],
            "arxiv_id": "2510.17611v1",
            "summary": "Unsupervised anomaly detection (UAD) has evolved from building specialized\nsingle-class models to unified multi-class models, yet existing multi-class\nmodels significantly underperform the most advanced one-for-one counterparts.\nMoreover, the field has fragmented into specialized methods tailored to\nspecific scenarios (multi-class, 3D, few-shot, etc.), creating deployment\nbarriers and highlighting the need for a unified solution. In this paper, we\npresent Dinomaly2, the first unified framework for full-spectrum image UAD,\nwhich bridges the performance gap in multi-class models while seamlessly\nextending across diverse data modalities and task settings. Guided by the \"less\nis more\" philosophy, we demonstrate that the orchestration of five simple\nelement achieves superior performance in a standard reconstruction-based\nframework. This methodological minimalism enables natural extension across\ndiverse tasks without modification, establishing that simplicity is the\nfoundation of true universality. Extensive experiments on 12 UAD benchmarks\ndemonstrate Dinomaly2's full-spectrum superiority across multiple modalities\n(2D, multi-view, RGB-3D, RGB-IR), task settings (single-class, multi-class,\ninference-unified multi-class, few-shot) and application domains (industrial,\nbiological, outdoor). For example, our multi-class model achieves unprecedented\n99.9% and 99.3% image-level (I-) AUROC on MVTec-AD and VisA respectively. For\nmulti-view and multi-modal inspection, Dinomaly2 demonstrates state-of-the-art\nperformance with minimum adaptations. Moreover, using only 8 normal examples\nper class, our method surpasses previous full-shot models, achieving 98.7% and\n97.4% I-AUROC on MVTec-AD and VisA. The combination of minimalistic design,\ncomputational scalability, and universal applicability positions Dinomaly2 as a\nunified solution for the full spectrum of real-world anomaly detection\napplications.",
            "headline_zh": "提出Dinomaly2统一框架，解决全谱无监督异常检测性能与通用性问题",
            "intro_zh": [
                "现有多类无监督异常检测模型性能落后于单类模型，且方法碎片化阻碍部署",
                "基于重构框架，协调五个简单元素实现高性能，无需修改即可扩展多任务",
                "在12个基准测试中，多类、少样本等场景下表现优异，如MVTec-AD达99.9% AUROC"
            ],
            "tags_zh": [
                "无监督异常检测",
                "多类检测",
                "重构框架",
                "全谱应用",
                "少样本学习"
            ],
            "_index": 30
        },
        {
            "title": "Integrating BIM and UAV-based photogrammetry for Automated 3D Structure Model Segmentation",
            "authors": [
                "Siqi Chen",
                "Shanyue Guan"
            ],
            "arxiv_id": "2510.17609v1",
            "summary": "The advancement of UAV technology has enabled efficient, non-contact\nstructural health monitoring. Combined with photogrammetry, UAVs can capture\nhigh-resolution scans and reconstruct detailed 3D models of infrastructure.\nHowever, a key challenge remains in segmenting specific structural components\nfrom these models-a process traditionally reliant on time-consuming and\nerror-prone manual labeling. To address this issue, we propose a machine\nlearning-based framework for automated segmentation of 3D point clouds. Our\napproach uses the complementary strengths of real-world UAV-scanned point\nclouds and synthetic data generated from Building Information Modeling (BIM) to\novercome the limitations associated with manual labeling. Validation on a\nrailroad track dataset demonstrated high accuracy in identifying and segmenting\nmajor components such as rails and crossties. Moreover, by using smaller-scale\ndatasets supplemented with BIM data, the framework significantly reduced\ntraining time while maintaining reasonable segmentation accuracy. This\nautomated approach improves the precision and efficiency of 3D infrastructure\nmodel segmentation and advances the integration of UAV and BIM technologies in\nstructural health monitoring and infrastructure management.",
            "headline_zh": "提出基于机器学习的框架，结合BIM与无人机摄影测量，实现3D结构模型自动分割。",
            "intro_zh": [
                "核心问题：从无人机扫描的3D点云中分割结构组件依赖耗时且易错的手动标注。",
                "方法要点：利用真实无人机点云与BIM合成数据的互补优势，训练机器学习模型。",
                "实验或效果：在铁路轨道数据集上验证，高精度分割轨道和枕木，减少训练时间。"
            ],
            "tags_zh": [
                "3D点云分割",
                "无人机摄影测量",
                "BIM集成",
                "结构健康监测",
                "机器学习框架"
            ],
            "_index": 31
        },
        {
            "title": "Learned Inertial Odometry for Cycling Based on Mixture of Experts Algorithm",
            "authors": [
                "Hao Qiao",
                "Yan Wang",
                "Shuo Yang",
                "Xiaoyao Yu",
                "Jian kuang",
                "Xiaoji Niu"
            ],
            "arxiv_id": "2510.17604v1",
            "summary": "With the rapid growth of bike sharing and the increasing diversity of cycling\napplications, accurate bicycle localization has become essential. traditional\nGNSS-based methods suffer from multipath effects, while existing inertial\nnavigation approaches rely on precise modeling and show limited robustness.\nTight Learned Inertial Odometry (TLIO) achieves low position drift by combining\nraw IMU data with predicted displacements by neural networks, but its high\ncomputational cost restricts deployment on mobile devices. To overcome this, we\nextend TLIO to bicycle localization and introduce an improved Mixture-of\nExperts (MoE) model that reduces both training and inference costs. Experiments\nshow that, compared to the state-of-the-art LLIO framework, our method achieves\ncomparable accuracy while reducing parameters by 64.7% and computational cost\nby 81.8%.",
            "headline_zh": "提出基于专家混合算法的学习惯性里程计，用于自行车定位并降低计算成本",
            "intro_zh": [
                "自行车定位受GNSS多路径和惯性导航建模限制影响，准确性不足",
                "扩展TLIO方法，引入改进MoE模型，减少训练和推理计算开销",
                "实验显示，相比LLIO，精度相当，参数和计算成本分别降低64.7%和81.8%"
            ],
            "tags_zh": [
                "学习惯性里程计",
                "自行车定位",
                "专家混合算法",
                "计算优化",
                "惯性导航"
            ],
            "_index": 32
        },
        {
            "title": "ShapeCraft: LLM Agents for Structured, Textured and Interactive 3D Modeling",
            "authors": [
                "Shuyuan Zhang",
                "Chenhan Jiang",
                "Zuoou Li",
                "Jiankang Deng"
            ],
            "arxiv_id": "2510.17603v1",
            "summary": "3D generation from natural language offers significant potential to reduce\nexpert manual modeling efforts and enhance accessibility to 3D assets. However,\nexisting methods often yield unstructured meshes and exhibit poor\ninteractivity, making them impractical for artistic workflows. To address these\nlimitations, we represent 3D assets as shape programs and introduce ShapeCraft,\na novel multi-agent framework for text-to-3D generation. At its core, we\npropose a Graph-based Procedural Shape (GPS) representation that decomposes\ncomplex natural language into a structured graph of sub-tasks, thereby\nfacilitating accurate LLM comprehension and interpretation of spatial\nrelationships and semantic shape details. Specifically, LLM agents\nhierarchically parse user input to initialize GPS, then iteratively refine\nprocedural modeling and painting to produce structured, textured, and\ninteractive 3D assets. Qualitative and quantitative experiments demonstrate\nShapeCraft's superior performance in generating geometrically accurate and\nsemantically rich 3D assets compared to existing LLM-based agents. We further\nshow the versatility of ShapeCraft through examples of animated and\nuser-customized editing, highlighting its potential for broader interactive\napplications.",
            "headline_zh": "提出ShapeCraft多智能体框架，通过图程序形状表示解决文本到3D生成的结构化和交互性问题",
            "intro_zh": [
                "现有文本到3D方法生成非结构化网格且交互性差，难以用于艺术工作流",
                "引入图程序形状表示，分解自然语言为子任务图，提升LLM对空间关系和语义细节的理解",
                "实验显示ShapeCraft在几何精度和语义丰富度上优于现有方法，支持动画和用户定制编辑"
            ],
            "tags_zh": [
                "文本到3D生成",
                "多智能体框架",
                "图程序形状表示",
                "结构化3D建模",
                "交互式3D资产"
            ],
            "_index": 33
        },
        {
            "title": "Conveying Meaning through Gestures: An Investigation into Semantic Co-Speech Gesture Generation",
            "authors": [
                "Hendric Voss",
                "Lisa Michelle Bohnenkamp",
                "Stefan Kopp"
            ],
            "arxiv_id": "2510.17599v1",
            "summary": "This study explores two frameworks for co-speech gesture generation, AQ-GT\nand its semantically-augmented variant AQ-GT-a, to evaluate their ability to\nconvey meaning through gestures and how humans perceive the resulting\nmovements. Using sentences from the SAGA spatial communication corpus,\ncontextually similar sentences, and novel movement-focused sentences, we\nconducted a user-centered evaluation of concept recognition and human-likeness.\nResults revealed a nuanced relationship between semantic annotations and\nperformance. The original AQ-GT framework, lacking explicit semantic input, was\nsurprisingly more effective at conveying concepts within its training domain.\nConversely, the AQ-GT-a framework demonstrated better generalization,\nparticularly for representing shape and size in novel contexts. While\nparticipants rated gestures from AQ-GT-a as more expressive and helpful, they\ndid not perceive them as more human-like. These findings suggest that explicit\nsemantic enrichment does not guarantee improved gesture generation and that its\neffectiveness is highly dependent on the context, indicating a potential\ntrade-off between specialization and generalization.",
            "headline_zh": "比较AQ-GT与AQ-GT-a框架，评估语义增强在共语手势生成中的效果与感知。",
            "intro_zh": [
                "核心问题：语义增强能否提升共语手势生成的概念传达与人类感知。",
                "方法要点：使用AQ-GT及其语义增强变体AQ-GT-a生成手势，并基于SAGA语料进行实验。",
                "实验或效果：AQ-GT在训练域内概念传达更优，AQ-GT-a在形状大小表示上泛化更好。"
            ],
            "tags_zh": [
                "共语手势生成",
                "语义增强",
                "概念传达",
                "人类感知评估",
                "泛化能力"
            ],
            "_index": 34
        },
        {
            "title": "MIRAGE: Agentic Framework for Multimodal Misinformation Detection with Web-Grounded Reasoning",
            "authors": [
                "Mir Nafis Sharear Shopnil",
                "Sharad Duwal",
                "Abhishek Tyagi",
                "Adiba Mahbub Proma"
            ],
            "arxiv_id": "2510.17590v1",
            "summary": "Misinformation spreads across web platforms through billions of daily\nmultimodal posts that combine text and images, overwhelming manual\nfact-checking capacity. Supervised detection models require domain-specific\ntraining data and fail to generalize across diverse manipulation tactics. We\npresent MIRAGE, an inference-time, model-pluggable agentic framework that\ndecomposes multimodal verification into four sequential modules: visual\nveracity assessment detects AI-generated images, cross-modal consistency\nanalysis identifies out-of-context repurposing, retrieval-augmented factual\nchecking grounds claims in web evidence through iterative question generation,\nand a calibrated judgment module integrates all signals. MIRAGE orchestrates\nvision-language model reasoning with targeted web retrieval, outputs structured\nand citation-linked rationales. On MMFakeBench validation set (1,000 samples),\nMIRAGE with GPT-4o-mini achieves 81.65% F1 and 75.1% accuracy, outperforming\nthe strongest zero-shot baseline (GPT-4V with MMD-Agent at 74.0% F1) by 7.65\npoints while maintaining 34.3% false positive rate versus 97.3% for a\njudge-only baseline. Test set results (5,000 samples) confirm generalization\nwith 81.44% F1 and 75.08% accuracy. Ablation studies show visual verification\ncontributes 5.18 F1 points and retrieval-augmented reasoning contributes 2.97\npoints. Our results demonstrate that decomposed agentic reasoning with web\nretrieval can match supervised detector performance without domain-specific\ntraining, enabling misinformation detection across modalities where labeled\ndata remains scarce.",
            "headline_zh": "提出MIRAGE框架以解决多模态虚假信息检测问题，通过分解推理与网络检索实现零样本高性能。",
            "intro_zh": [
                "核心问题：多模态虚假信息泛滥，监督模型泛化差且依赖标注数据。",
                "方法要点：框架分解为视觉真实性、跨模态一致性、检索增强事实检查和校准判断模块。",
                "实验效果：在MMFakeBench上F1达81.65%，优于零-shot基线，无需领域特定训练。"
            ],
            "tags_zh": [
                "多模态虚假信息检测",
                "代理推理框架",
                "检索增强生成",
                "视觉语言模型",
                "零样本学习",
                "网络检索"
            ],
            "_index": 35
        },
        {
            "title": "Expose Camouflage in the Water: Underwater Camouflaged Instance Segmentation and Dataset",
            "authors": [
                "Chuhong Wang",
                "Hua Li",
                "Chongyi Li",
                "Huazhong Liu",
                "Xiongxin Tang",
                "Sam Kwong"
            ],
            "arxiv_id": "2510.17585v1",
            "summary": "With the development of underwater exploration and marine protection,\nunderwater vision tasks are widespread. Due to the degraded underwater\nenvironment, characterized by color distortion, low contrast, and blurring,\ncamouflaged instance segmentation (CIS) faces greater challenges in accurately\nsegmenting objects that blend closely with their surroundings. Traditional\ncamouflaged instance segmentation methods, trained on terrestrial-dominated\ndatasets with limited underwater samples, may exhibit inadequate performance in\nunderwater scenes. To address these issues, we introduce the first underwater\ncamouflaged instance segmentation (UCIS) dataset, abbreviated as UCIS4K, which\ncomprises 3,953 images of camouflaged marine organisms with instance-level\nannotations. In addition, we propose an Underwater Camouflaged Instance\nSegmentation network based on Segment Anything Model (UCIS-SAM). Our UCIS-SAM\nincludes three key modules. First, the Channel Balance Optimization Module\n(CBOM) enhances channel characteristics to improve underwater feature learning,\neffectively addressing the model's limited understanding of underwater\nenvironments. Second, the Frequency Domain True Integration Module (FDTIM) is\nproposed to emphasize intrinsic object features and reduce interference from\ncamouflage patterns, enhancing the segmentation performance of camouflaged\nobjects blending with their surroundings. Finally, the Multi-scale Feature\nFrequency Aggregation Module (MFFAM) is designed to strengthen the boundaries\nof low-contrast camouflaged instances across multiple frequency bands,\nimproving the model's ability to achieve more precise segmentation of\ncamouflaged objects. Extensive experiments on the proposed UCIS4K and public\nbenchmarks show that our UCIS-SAM outperforms state-of-the-art approaches.",
            "headline_zh": "提出UCIS-SAM网络与UCIS4K数据集以解决水下伪装实例分割挑战",
            "intro_zh": [
                "核心问题：水下环境退化与伪装对象融合导致实例分割困难",
                "方法要点：基于SAM设计CBOM、FDTIM和MFFAM模块提升特征学习与分割精度",
                "实验或效果：在UCIS4K和公共基准上优于现有方法"
            ],
            "tags_zh": [
                "水下伪装实例分割",
                "UCIS4K数据集",
                "UCIS-SAM网络",
                "通道平衡优化",
                "频域特征集成",
                "多尺度特征聚合"
            ],
            "_index": 36
        },
        {
            "title": "Intent-Driven LLM Ensemble Planning for Flexible Multi-Robot Disassembly: Demonstration on EV Batteries",
            "authors": [
                "Cansu Erdogan",
                "Cesar Alan Contreras",
                "Alireza Rastegarpanah",
                "Manolis Chiou",
                "Rustam Stolkin"
            ],
            "arxiv_id": "2510.17576v1",
            "summary": "This paper addresses the problem of planning complex manipulation tasks, in\nwhich multiple robots with different end-effectors and capabilities, informed\nby computer vision, must plan and execute concatenated sequences of actions on\na variety of objects that can appear in arbitrary positions and configurations\nin unstructured scenes. We propose an intent-driven planning pipeline which can\nrobustly construct such action sequences with varying degrees of supervisory\ninput from a human using simple language instructions. The pipeline integrates:\n(i) perception-to-text scene encoding, (ii) an ensemble of large language\nmodels (LLMs) that generate candidate removal sequences based on the operator's\nintent, (iii) an LLM-based verifier that enforces formatting and precedence\nconstraints, and (iv) a deterministic consistency filter that rejects\nhallucinated objects. The pipeline is evaluated on an example task in which two\nrobot arms work collaboratively to dismantle an Electric Vehicle battery for\nrecycling applications. A variety of components must be grasped and removed in\nspecific sequences, determined by human instructions and/or by task-order\nfeasibility decisions made by the autonomous system. On 200 real scenes with\n600 operator prompts across five component classes, we used metrics of\nfull-sequence correctness and next-task correctness to evaluate and compare\nfive LLM-based planners (including ablation analyses of pipeline components).\nWe also evaluated the LLM-based human interface in terms of time to execution\nand NASA TLX with human participant experiments. Results indicate that our\nensemble-with-verification approach reliably maps operator intent to safe,\nexecutable multi-robot plans while maintaining low user effort.",
            "headline_zh": "提出意图驱动的LLM集成规划方法，用于多机器人协作拆卸电动汽车电池。",
            "intro_zh": [
                "核心问题：多机器人在非结构化场景中规划复杂操作序列，需处理物体位置和配置的任意性。",
                "方法要点：集成感知到文本编码、LLM生成候选序列、验证器约束和一致性过滤。",
                "实验效果：在真实场景中评估，可靠映射意图到安全可执行计划，降低用户负担。"
            ],
            "tags_zh": [
                "多机器人规划",
                "意图驱动",
                "LLM集成",
                "拆卸任务",
                "计算机视觉",
                "电动汽车电池"
            ],
            "_index": 37
        },
        {
            "title": "PAGE-4D: Disentangled Pose and Geometry Estimation for 4D Perception",
            "authors": [
                "Kaichen Zhou",
                "Yuhan Wang",
                "Grace Chen",
                "Xinhai Chang",
                "Gaspard Beaudouin",
                "Fangneng Zhan",
                "Paul Pu Liang",
                "Mengyu Wang"
            ],
            "arxiv_id": "2510.17568v1",
            "summary": "Recent 3D feed-forward models, such as the Visual Geometry Grounded\nTransformer (VGGT), have shown strong capability in inferring 3D attributes of\nstatic scenes. However, since they are typically trained on static datasets,\nthese models often struggle in real-world scenarios involving complex dynamic\nelements, such as moving humans or deformable objects like umbrellas. To\naddress this limitation, we introduce PAGE-4D, a feedforward model that extends\nVGGT to dynamic scenes, enabling camera pose estimation, depth prediction, and\npoint cloud reconstruction -- all without post-processing. A central challenge\nin multi-task 4D reconstruction is the inherent conflict between tasks:\naccurate camera pose estimation requires suppressing dynamic regions, while\ngeometry reconstruction requires modeling them. To resolve this tension, we\npropose a dynamics-aware aggregator that disentangles static and dynamic\ninformation by predicting a dynamics-aware mask -- suppressing motion cues for\npose estimation while amplifying them for geometry reconstruction. Extensive\nexperiments show that PAGE-4D consistently outperforms the original VGGT in\ndynamic scenarios, achieving superior results in camera pose estimation,\nmonocular and video depth estimation, and dense point map reconstruction.",
            "headline_zh": "提出PAGE-4D以解决动态场景中相机姿态与几何重建的冲突",
            "intro_zh": [
                "核心问题：静态模型在动态场景中表现不佳，任务间存在冲突",
                "方法要点：使用动态感知聚合器解耦静态与动态信息",
                "实验或效果：在动态场景中优于VGGT，提升姿态估计与重建精度"
            ],
            "tags_zh": [
                "4D感知",
                "相机姿态估计",
                "深度预测",
                "点云重建",
                "动态场景处理",
                "多任务学习"
            ],
            "_index": 38
        },
        {
            "title": "WP-CrackNet: A Collaborative Adversarial Learning Framework for End-to-End Weakly-Supervised Road Crack Detection",
            "authors": [
                "Nachuan Ma",
                "Zhengfei Song",
                "Qiang Hu",
                "Xiaoyu Tang",
                "Chengxi Zhang",
                "Rui Fan",
                "Lihua Xie"
            ],
            "arxiv_id": "2510.17566v1",
            "summary": "Road crack detection is essential for intelligent infrastructure maintenance\nin smart cities. To reduce reliance on costly pixel-level annotations, we\npropose WP-CrackNet, an end-to-end weakly-supervised method that trains with\nonly image-level labels for pixel-wise crack detection. WP-CrackNet integrates\nthree components: a classifier generating class activation maps (CAMs), a\nreconstructor measuring feature inferability, and a detector producing\npixel-wise road crack detection results. During training, the classifier and\nreconstructor alternate in adversarial learning to encourage crack CAMs to\ncover complete crack regions, while the detector learns from pseudo labels\nderived from post-processed crack CAMs. This mutual feedback among the three\ncomponents improves learning stability and detection accuracy. To further boost\ndetection performance, we design a path-aware attention module (PAAM) that\nfuses high-level semantics from the classifier with low-level structural cues\nfrom the reconstructor by modeling spatial and channel-wise dependencies.\nAdditionally, a center-enhanced CAM consistency module (CECCM) is proposed to\nrefine crack CAMs using center Gaussian weighting and consistency constraints,\nenabling better pseudo-label generation. We create three image-level datasets\nand extensive experiments show that WP-CrackNet achieves comparable results to\nsupervised methods and outperforms existing weakly-supervised methods,\nsignificantly advancing scalable road inspection. The source code package and\ndatasets are available at https://mias.group/WP-CrackNet/.",
            "headline_zh": "提出WP-CrackNet以弱监督方式实现端到端道路裂缝检测",
            "intro_zh": [
                "核心问题：减少对像素级标注的依赖，仅使用图像级标签进行道路裂缝检测。",
                "方法要点：集成分类器、重建器和检测器，通过对抗学习和伪标签优化检测结果。",
                "实验或效果：在自建数据集上，性能接近监督方法，优于现有弱监督方法。"
            ],
            "tags_zh": [
                "弱监督学习",
                "道路裂缝检测",
                "对抗学习",
                "类激活图",
                "端到端框架"
            ],
            "_index": 39
        },
        {
            "title": "An Empirical Study of Lagrangian Methods in Safe Reinforcement Learning",
            "authors": [
                "Lindsay Spoor",
                "Álvaro Serra-Gómez",
                "Aske Plaat",
                "Thomas Moerland"
            ],
            "arxiv_id": "2510.17564v1",
            "summary": "In safety-critical domains such as robotics, navigation and power systems,\nconstrained optimization problems arise where maximizing performance must be\ncarefully balanced with associated constraints. Safe reinforcement learning\nprovides a framework to address these challenges, with Lagrangian methods being\na popular choice. However, the effectiveness of Lagrangian methods crucially\ndepends on the choice of the Lagrange multiplier $\\lambda$, which governs the\ntrade-off between return and constraint cost. A common approach is to update\nthe multiplier automatically during training. Although this is standard in\npractice, there remains limited empirical evidence on the robustness of an\nautomated update and its influence on overall performance. Therefore, we\nanalyze (i) optimality and (ii) stability of Lagrange multipliers in safe\nreinforcement learning across a range of tasks. We provide $\\lambda$-profiles\nthat give a complete visualization of the trade-off between return and\nconstraint cost of the optimization problem. These profiles show the highly\nsensitive nature of $\\lambda$ and moreover confirm the lack of general\nintuition for choosing the optimal value $\\lambda^*$. Our findings additionally\nshow that automated multiplier updates are able to recover and sometimes even\nexceed the optimal performance found at $\\lambda^*$ due to the vast difference\nin their learning trajectories. Furthermore, we show that automated multiplier\nupdates exhibit oscillatory behavior during training, which can be mitigated\nthrough PID-controlled updates. However, this method requires careful tuning to\nachieve consistently better performance across tasks. This highlights the need\nfor further research on stabilizing Lagrangian methods in safe reinforcement\nlearning. The code used to reproduce our results can be found at\nhttps://github.com/lindsayspoor/Lagrangian_SafeRL.",
            "headline_zh": "分析拉格朗日乘子在安全强化学习中的最优性与稳定性，提出λ-剖面可视化方法",
            "intro_zh": [
                "核心问题：拉格朗日乘子λ的选择对安全强化学习性能与约束平衡高度敏感，缺乏通用直觉",
                "方法要点：使用λ-剖面可视化性能与约束权衡，并评估自动乘子更新与PID控制方法",
                "实验或效果：自动更新可恢复或超越最优性能，但存在振荡，需调优以稳定"
            ],
            "tags_zh": [
                "安全强化学习",
                "拉格朗日方法",
                "约束优化",
                "乘子更新",
                "稳定性分析",
                "λ-剖面"
            ],
            "_index": 40
        },
        {
            "title": "Distributed Spatial-Temporal Trajectory Optimization for Unmanned-Aerial-Vehicle Swarm",
            "authors": [
                "Xiaobo Zheng",
                "Pan Tang",
                "Defu Lin",
                "Shaoming He"
            ],
            "arxiv_id": "2510.17541v1",
            "summary": "Swarm trajectory optimization problems are a well-recognized class of\nmulti-agent optimal control problems with strong nonlinearity. However, the\nheuristic nature of needing to set the final time for agents beforehand and the\ntime-consuming limitation of the significant number of iterations prohibit the\napplication of existing methods to large-scale swarm of Unmanned Aerial\nVehicles (UAVs) in practice. In this paper, we propose a spatial-temporal\ntrajectory optimization framework that accomplishes multi-UAV consensus based\non the Alternating Direction Multiplier Method (ADMM) and uses Differential\nDynamic Programming (DDP) for fast local planning of individual UAVs. The\nintroduced framework is a two-level architecture that employs Parameterized DDP\n(PDDP) as the trajectory optimizer for each UAV, and ADMM to satisfy the local\nconstraints and accomplish the spatial-temporal parameter consensus among all\nUAVs. This results in a fully distributed algorithm called Distributed\nParameterized DDP (D-PDDP). In addition, an adaptive tuning criterion based on\nthe spectral gradient method for the penalty parameter is proposed to reduce\nthe number of algorithmic iterations. Several simulation examples are presented\nto verify the effectiveness of the proposed algorithm.",
            "headline_zh": "提出分布式时空轨迹优化框架以解决无人机集群大规模轨迹优化问题",
            "intro_zh": [
                "核心问题：无人机集群轨迹优化存在非线性强、需预设最终时间及迭代耗时等限制",
                "方法要点：结合ADMM实现多无人机共识，使用PDDP进行快速局部轨迹规划",
                "实验或效果：通过模拟验证算法有效性，并引入自适应惩罚参数减少迭代次数"
            ],
            "tags_zh": [
                "无人机集群",
                "轨迹优化",
                "分布式算法",
                "ADMM",
                "微分动态规划"
            ],
            "_index": 41
        },
        {
            "title": "Detecting streaks in smart telescopes images with Deep Learning",
            "authors": [
                "Olivier Parisot",
                "Mahmoud Jaziri"
            ],
            "arxiv_id": "2510.17540v1",
            "summary": "The growing negative impact of the visibility of satellites in the night sky\nis influencing the practice of astronomy and astrophotograph, both at the\namateur and professional levels. The presence of these satellites has the\neffect of introducing streaks into the images captured during astronomical\nobservation, requiring the application of additional post processing to\nmitigate the undesirable impact, whether for data loss or cosmetic reasons. In\nthis paper, we show how we test and adapt various Deep Learning approaches to\ndetect streaks in raw astronomical data captured between March 2022 and\nFebruary 2023 with smart telescopes.",
            "headline_zh": "测试并适配深度学习方法以检测智能望远镜图像中的卫星条纹",
            "intro_zh": [
                "卫星在夜空中可见性增加，导致天文图像出现条纹，影响数据质量和美观。",
                "采用多种深度学习技术，在2022年3月至2023年2月采集的原始天文数据上进行测试和适配。",
                "未知具体检测效果，但旨在减少数据损失和改善图像后处理。"
            ],
            "tags_zh": [
                "卫星条纹检测",
                "深度学习",
                "智能望远镜",
                "天文图像处理",
                "数据损失缓解"
            ],
            "_index": 42
        },
        {
            "title": "MambaX-Net: Dual-Input Mamba-Enhanced Cross-Attention Network for Longitudinal MRI Segmentation",
            "authors": [
                "Yovin Yahathugoda",
                "Davide Prezzi",
                "Piyalitt Ittichaiwong",
                "Vicky Goh",
                "Sebastien Ourselin",
                "Michela Antonelli"
            ],
            "arxiv_id": "2510.17529v1",
            "summary": "Active Surveillance (AS) is a treatment option for managing low and\nintermediate-risk prostate cancer (PCa), aiming to avoid overtreatment while\nmonitoring disease progression through serial MRI and clinical follow-up.\nAccurate prostate segmentation is an important preliminary step for automating\nthis process, enabling automated detection and diagnosis of PCa. However,\nexisting deep-learning segmentation models are often trained on\nsingle-time-point and expertly annotated datasets, making them unsuitable for\nlongitudinal AS analysis, where multiple time points and a scarcity of expert\nlabels hinder their effective fine-tuning. To address these challenges, we\npropose MambaX-Net, a novel semi-supervised, dual-scan 3D segmentation\narchitecture that computes the segmentation for time point t by leveraging the\nMRI and the corresponding segmentation mask from the previous time point. We\nintroduce two new components: (i) a Mamba-enhanced Cross-Attention Module,\nwhich integrates the Mamba block into cross attention to efficiently capture\ntemporal evolution and long-range spatial dependencies, and (ii) a Shape\nExtractor Module that encodes the previous segmentation mask into a latent\nanatomical representation for refined zone delination. Moreover, we introduce a\nsemi-supervised self-training strategy that leverages pseudo-labels generated\nfrom a pre-trained nnU-Net, enabling effective learning without expert\nannotations. MambaX-Net was evaluated on a longitudinal AS dataset, and results\nshowed that it significantly outperforms state-of-the-art U-Net and\nTransformer-based models, achieving superior prostate zone segmentation even\nwhen trained on limited and noisy data.",
            "headline_zh": "提出MambaX-Net以解决纵向MRI前列腺分割中多时间点与标注稀缺问题",
            "intro_zh": [
                "核心问题：纵向主动监测中多时间点MRI分割因标注稀缺难以微调现有模型",
                "方法要点：集成Mamba块与交叉注意力，利用前一时间点MRI和分割掩码",
                "实验或效果：在纵向数据集上优于U-Net和Transformer模型，提升分割精度"
            ],
            "tags_zh": [
                "纵向MRI分割",
                "Mamba增强交叉注意力",
                "半监督学习",
                "前列腺癌监测",
                "3D分割架构"
            ],
            "_index": 43
        },
        {
            "title": "HumanMPC - Safe and Efficient MAV Navigation among Humans",
            "authors": [
                "Simon Schaefer",
                "Helen Oleynikova",
                "Sandra Hirche",
                "Stefan Leutenegger"
            ],
            "arxiv_id": "2510.17525v1",
            "summary": "Safe and efficient robotic navigation among humans is essential for\nintegrating robots into everyday environments. Most existing approaches focus\non simplified 2D crowd navigation and fail to account for the full complexity\nof human body dynamics beyond root motion. We present HumanMPC, a Model\nPredictive Control (MPC) framework for 3D Micro Air Vehicle (MAV) navigation\namong humans that combines theoretical safety guarantees with data-driven\nmodels for realistic human motion forecasting. Our approach introduces a novel\ntwist to reachability-based safety formulation that constrains only the initial\ncontrol input for safety while modeling its effects over the entire planning\nhorizon, enabling safe yet efficient navigation. We validate HumanMPC in both\nsimulated experiments using real human trajectories and in the real-world,\ndemonstrating its effectiveness across tasks ranging from goal-directed\nnavigation to visual servoing for human tracking. While we apply our method to\nMAVs in this work, it is generic and can be adapted by other platforms. Our\nresults show that the method ensures safety without excessive conservatism and\noutperforms baseline approaches in both efficiency and reliability.",
            "headline_zh": "提出HumanMPC框架以安全高效导航MAV于人群中",
            "intro_zh": [
                "现有方法多限于2D人群导航，忽略人体动态复杂性。",
                "结合可达性安全保证与数据驱动模型，优化控制输入。",
                "仿真与真实实验验证安全性和效率优于基线方法。"
            ],
            "tags_zh": [
                "模型预测控制",
                "无人机导航",
                "人体运动预测",
                "可达性分析",
                "安全保证"
            ],
            "_index": 44
        },
        {
            "title": "MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation Models",
            "authors": [
                "Yongshun Zhang",
                "Zhongyi Fan",
                "Yonghang Zhang",
                "Zhangzikang Li",
                "Weifeng Chen",
                "Zhongwei Feng",
                "Chaoyue Wang",
                "Peng Hou",
                "Anxiang Zeng"
            ],
            "arxiv_id": "2510.17519v1",
            "summary": "In recent years, large-scale generative models for visual content\n(\\textit{e.g.,} images, videos, and 3D objects/scenes) have made remarkable\nprogress. However, training large-scale video generation models remains\nparticularly challenging and resource-intensive due to cross-modal text-video\nalignment, the long sequences involved, and the complex spatiotemporal\ndependencies. To address these challenges, we present a training framework that\noptimizes four pillars: (i) data processing, (ii) model architecture, (iii)\ntraining strategy, and (iv) infrastructure for large-scale video generation\nmodels. These optimizations delivered significant efficiency gains and\nperformance improvements across all stages of data preprocessing, video\ncompression, parameter scaling, curriculum-based pretraining, and\nalignment-focused post-training. Our resulting model, MUG-V 10B, matches recent\nstate-of-the-art video generators overall and, on e-commerce-oriented video\ngeneration tasks, surpasses leading open-source baselines in human evaluations.\nMore importantly, we open-source the complete stack, including model weights,\nMegatron-Core-based large-scale training code, and inference pipelines for\nvideo generation and enhancement. To our knowledge, this is the first public\nrelease of large-scale video generation training code that exploits\nMegatron-Core to achieve high training efficiency and near-linear multi-node\nscaling, details are available in\n\\href{https://github.com/Shopee-MUG/MUG-V}{our webpage}.",
            "headline_zh": "提出高效训练框架以解决大规模视频生成模型的资源挑战",
            "intro_zh": [
                "核心问题：大规模视频生成训练面临跨模态对齐、长序列和时空依赖等资源密集型挑战",
                "方法要点：优化数据处理、模型架构、训练策略和基础设施四大支柱，提升效率",
                "实验或效果：MUG-V 10B模型在电商视频生成任务中超越开源基线，并开源完整堆栈"
            ],
            "tags_zh": [
                "大规模视频生成",
                "高效训练框架",
                "跨模态对齐",
                "开源代码",
                "多节点扩展"
            ],
            "_index": 45
        },
        {
            "title": "Context-Aware Pseudo-Label Scoring for Zero-Shot Video Summarization",
            "authors": [
                "Yuanli Wu",
                "Long Zhang",
                "Yue Du",
                "Bin Li"
            ],
            "arxiv_id": "2510.17501v1",
            "summary": "With the rapid proliferation of video content across social media,\nsurveillance, and education platforms, efficiently summarizing long videos into\nconcise yet semantically faithful surrogates has become increasingly vital.\nExisting supervised methods achieve strong in-domain accuracy by learning from\ndense annotations but suffer from high labeling costs and limited cross-dataset\ngeneralization, while unsupervised approaches, though label-free, often fail to\ncapture high-level human semantics and fine-grained narrative cues. More\nrecently, zero-shot prompting pipelines have leveraged large language models\n(LLMs) for training-free video summarization, yet remain highly sensitive to\nhandcrafted prompt templates and dataset-specific score normalization. To\novercome these limitations, we introduce a rubric-guided, pseudo-labeled\nprompting framework that transforms a small subset of ground-truth annotations\ninto high-confidence pseudo labels, which are aggregated into structured,\ndataset-adaptive scoring rubrics guiding interpretable scene evaluation. During\ninference, first and last segments are scored based solely on their\ndescriptions, whereas intermediate ones incorporate brief contextual summaries\nof adjacent scenes to assess narrative progression and redundancy. This\ncontextual prompting enables the LLM to balance local salience and global\ncoherence without parameter tuning. On SumMe and TVSum, our method achieves F1\nscores of \\textbf{57.58} and \\textbf{63.05}, surpassing unsupervised and prior\nzero-shot baselines while approaching supervised performance. The results\ndemonstrate that rubric-guided pseudo labeling effectively stabilizes LLM-based\nscoring and establishes a general, interpretable zero-shot paradigm for video\nsummarization.",
            "headline_zh": "提出基于规则引导伪标签评分的零样本视频摘要方法，以提升泛化性和语义捕捉能力",
            "intro_zh": [
                "现有方法依赖密集标注或手工提示，泛化性差且成本高",
                "利用少量真实标注生成伪标签，构建数据集自适应评分规则指导场景评估",
                "在SumMe和TVSum上F1分数达57.58和63.05，超越无监督和零样本基线"
            ],
            "tags_zh": [
                "零样本视频摘要",
                "伪标签评分",
                "上下文感知提示",
                "大语言模型应用",
                "规则引导评估"
            ],
            "_index": 46
        },
        {
            "title": "Split-Fuse-Transport: Annotation-Free Saliency via Dual Clustering and Optimal Transport Alignment",
            "authors": [
                "Muhammad Umer Ramzan",
                "Ali Zia",
                "Abdelwahed Khamis",
                "Noman Ali",
                "Usman Ali",
                "Wei Xiang"
            ],
            "arxiv_id": "2510.17484v1",
            "summary": "Salient object detection (SOD) aims to segment visually prominent regions in\nimages and serves as a foundational task for various computer vision\napplications. We posit that SOD can now reach near-supervised accuracy without\na single pixel-level label, but only when reliable pseudo-masks are available.\nWe revisit the prototype-based line of work and make two key observations.\nFirst, boundary pixels and interior pixels obey markedly different geometry;\nsecond, the global consistency enforced by optimal transport (OT) is\nunderutilized if prototype quality is weak. To address this, we introduce\nPOTNet, an adaptation of Prototypical Optimal Transport that replaces POT's\nsingle k-means step with an entropy-guided dual-clustering head: high-entropy\npixels are organized by spectral clustering, low-entropy pixels by k-means, and\nthe two prototype sets are subsequently aligned by OT. This\nsplit-fuse-transport design yields sharper, part-aware pseudo-masks in a single\nforward pass, without handcrafted priors. Those masks supervise a standard\nMaskFormer-style encoder-decoder, giving rise to AutoSOD, an end-to-end\nunsupervised SOD pipeline that eliminates SelfMask's offline voting yet\nimproves both accuracy and training efficiency. Extensive experiments on five\nbenchmarks show that AutoSOD outperforms unsupervised methods by up to 26% and\nweakly supervised methods by up to 36% in F-measure, further narrowing the gap\nto fully supervised models.",
            "headline_zh": "提出POTNet方法，通过双聚类和最优传输实现无标注显著目标检测",
            "intro_zh": [
                "核心问题：显著目标检测需可靠伪掩码，但现有方法在原型质量和全局一致性上不足",
                "方法要点：采用熵引导双聚类，高熵像素谱聚类、低熵像素k均值，并用最优传输对齐",
                "实验或效果：在五个基准上，F-measure优于无监督方法26%、弱监督方法36%"
            ],
            "tags_zh": [
                "显著目标检测",
                "无监督学习",
                "最优传输",
                "双聚类",
                "伪掩码生成",
                "计算机视觉"
            ],
            "_index": 47
        },
        {
            "title": "SparseWorld: A Flexible, Adaptive, and Efficient 4D Occupancy World Model Powered by Sparse and Dynamic Queries",
            "authors": [
                "Chenxu Dang",
                "Haiyan Liu",
                "Guangjun Bao",
                "Pei An",
                "Xinyue Tang",
                "Jie Ma",
                "Bingchuan Sun",
                "Yan Wang"
            ],
            "arxiv_id": "2510.17482v1",
            "summary": "Semantic occupancy has emerged as a powerful representation in world models\nfor its ability to capture rich spatial semantics. However, most existing\noccupancy world models rely on static and fixed embeddings or grids, which\ninherently limit the flexibility of perception. Moreover, their ``in-place\nclassification\" over grids exhibits a potential misalignment with the dynamic\nand continuous nature of real scenarios.In this paper, we propose SparseWorld,\na novel 4D occupancy world model that is flexible, adaptive, and efficient,\npowered by sparse and dynamic queries. We propose a Range-Adaptive Perception\nmodule, in which learnable queries are modulated by the ego vehicle states and\nenriched with temporal-spatial associations to enable extended-range\nperception. To effectively capture the dynamics of the scene, we design a\nState-Conditioned Forecasting module, which replaces classification-based\nforecasting with regression-guided formulation, precisely aligning the dynamic\nqueries with the continuity of the 4D environment. In addition, We specifically\ndevise a Temporal-Aware Self-Scheduling training strategy to enable smooth and\nefficient training. Extensive experiments demonstrate that SparseWorld achieves\nstate-of-the-art performance across perception, forecasting, and planning\ntasks. Comprehensive visualizations and ablation studies further validate the\nadvantages of SparseWorld in terms of flexibility, adaptability, and\nefficiency. The code is available at https://github.com/MSunDYY/SparseWorld.",
            "headline_zh": "提出SparseWorld 4D占用世界模型，通过稀疏动态查询解决感知灵活性与动态场景对齐问题。",
            "intro_zh": [
                "现有占用世界模型依赖静态嵌入或网格，限制感知灵活性且与动态场景不匹配。",
                "引入范围自适应感知和状态条件预测模块，使用动态查询实现扩展感知和连续环境对齐。",
                "实验显示在感知、预测和规划任务中达到先进水平，验证了灵活性、适应性和效率。"
            ],
            "tags_zh": [
                "4D占用世界模型",
                "稀疏动态查询",
                "范围自适应感知",
                "状态条件预测",
                "自动驾驶场景",
                "自监督训练"
            ],
            "_index": 48
        },
        {
            "title": "Initialize to Generalize: A Stronger Initialization Pipeline for Sparse-View 3DGS",
            "authors": [
                "Feng Zhou",
                "Wenkai Guo",
                "Pu Cao",
                "Zhicheng Zhang",
                "Jianqin Yin"
            ],
            "arxiv_id": "2510.17479v1",
            "summary": "Sparse-view 3D Gaussian Splatting (3DGS) often overfits to the training\nviews, leading to artifacts like blurring in novel view rendering. Prior work\naddresses it either by enhancing the initialization (\\emph{i.e.}, the point\ncloud from Structure-from-Motion (SfM)) or by adding training-time constraints\n(regularization) to the 3DGS optimization. Yet our controlled ablations reveal\nthat initialization is the decisive factor: it determines the attainable\nperformance band in sparse-view 3DGS, while training-time constraints yield\nonly modest within-band improvements at extra cost. Given initialization's\nprimacy, we focus our design there. Although SfM performs poorly under sparse\nviews due to its reliance on feature matching, it still provides reliable seed\npoints. Thus, building on SfM, our effort aims to supplement the regions it\nfails to cover as comprehensively as possible. Specifically, we design: (i)\nfrequency-aware SfM that improves low-texture coverage via low-frequency view\naugmentation and relaxed multi-view correspondences; (ii) 3DGS\nself-initialization that lifts photometric supervision into additional points,\ncompensating SfM-sparse regions with learned Gaussian centers; and (iii)\npoint-cloud regularization that enforces multi-view consistency and uniform\nspatial coverage through simple geometric/visibility priors, yielding a clean\nand reliable point cloud. Our experiments on LLFF and Mip-NeRF360 demonstrate\nconsistent gains in sparse-view settings, establishing our approach as a\nstronger initialization strategy. Code is available at\nhttps://github.com/zss171999645/ItG-GS.",
            "headline_zh": "提出更强初始化管道以解决稀疏视图3D高斯溅射的过拟合问题",
            "intro_zh": [
                "核心问题：稀疏视图3DGS易过拟合训练视图，导致新视图渲染模糊。",
                "方法要点：基于SfM，设计频率感知SfM、3DGS自初始化和点云正则化。",
                "实验效果：在LLFF和Mip-NeRF360数据集上，稀疏视图设置下性能一致提升。"
            ],
            "tags_zh": [
                "稀疏视图3D重建",
                "高斯溅射初始化",
                "结构从运动增强",
                "点云正则化",
                "新视图合成"
            ],
            "_index": 49
        },
        {
            "title": "Inverse Optimal Control of Muscle Force Sharing During Pathological Gait",
            "authors": [
                "Filip Bečanović",
                "Vincent Bonnet",
                "Kosta Jovanović",
                "Samer Mohammed",
                "Raphaël Dumas"
            ],
            "arxiv_id": "2510.17456v1",
            "summary": "Muscle force sharing is typically resolved by minimizing a specific objective\nfunction to approximate neural control strategies. An inverse optimal control\napproach was applied to identify the \"best\" objective function, among a\npositive linear combination of basis objective functions, associated with the\ngait of two post-stroke males, one high-functioning (subject S1) and one\nlow-functioning (subject S2). It was found that the \"best\" objective function\nis subject- and leg-specific. No single function works universally well, yet\nthe best options are usually differently weighted combinations of muscle\nactivation- and power-minimization. Subject-specific inverse optimal control\nmodels performed best on their respective limbs (\\textbf{RMSE 178/213 N, CC\n0.71/0.61} for non-paretic and paretic legs of S1; \\textbf{RMSE 205/165 N, CC\n0.88/0.85} for respective legs of S2), but cross-subject generalization was\npoor, particularly for paretic legs. Moreover, minimizing the root mean square\nof muscle power emerged as important for paretic limbs, while minimizing\nactivation-based functions dominated for non-paretic limbs. This may suggest\ndifferent neural control strategies between affected and unaffected sides,\npossibly altered by the presence of spasticity. Among the 15 considered\nobjective functions commonly used in inverse dynamics-based computations, the\nroot mean square of muscle power was the only one explicitly incorporating\nmuscle velocity, leading to a possible model for spasticity in the paretic\nlimbs. Although this objective function has been rarely used, it may be\nrelevant for modeling pathological gait, such as post-stroke gait.",
            "headline_zh": "应用逆最优控制识别中风后步态中肌肉力分配的最佳目标函数",
            "intro_zh": [
                "核心问题：如何确定中风后步态中肌肉力分配的最佳目标函数，以近似神经控制策略。",
                "方法要点：使用逆最优控制，从基础目标函数的正线性组合中识别个体和腿部特定的最佳函数。",
                "实验效果：模型在各自肢体上表现良好，但跨对象泛化差，尤其对偏瘫腿。"
            ],
            "tags_zh": [
                "逆最优控制",
                "肌肉力分配",
                "病理步态",
                "中风康复",
                "目标函数优化"
            ],
            "_index": 50
        },
        {
            "title": "A Generalization of Input-Output Linearization via Dynamic Switching Between Melds of Output Functions",
            "authors": [
                "Mirko Mizzoni",
                "Pieter van Goor",
                "Barbara Bazzana",
                "Antonio Franchi"
            ],
            "arxiv_id": "2510.17448v1",
            "summary": "This letter presents a systematic framework for switching between different\nsets of outputs for the control of nonlinear systems via feedback\nlinearization. We introduce the concept of a meld to formally define a valid,\nfeedback-linearizable subset of outputs that can be selected from a larger deck\nof possible outputs. The main contribution is a formal proof establishing that\nunder suitable dwell-time and compatibility conditions, it is possible to\nswitch between different melds while guaranteeing the uniform boundedness of\nthe system state. We further show that the error dynamics of the active outputs\nremain exponentially stable within each switching interval and that outputs\ncommon to consecutive melds are tracked seamlessly through transitions. The\nproposed theory is valid for any feedback linearizable nonlinear system, such\nas, e.g., robots, aerial and terrestrial vehicles, etc.. We demonstrate it on a\nsimple numerical simulation of a robotic manipulator.",
            "headline_zh": "提出通过动态切换输出函数集合实现非线性系统反馈线性化的通用框架。",
            "intro_zh": [
                "核心问题：非线性系统控制中如何安全切换不同输出集合以保持系统稳定性。",
                "方法要点：引入meld概念，定义可线性化输出子集，并证明切换条件下的状态有界性。",
                "实验或效果：在机器人操纵器数值模拟中验证理论，确保输出跟踪和指数稳定。"
            ],
            "tags_zh": [
                "非线性系统控制",
                "反馈线性化",
                "输出切换",
                "稳定性分析",
                "机器人控制"
            ],
            "_index": 51
        },
        {
            "title": "Rethinking Nighttime Image Deraining via Learnable Color Space Transformation",
            "authors": [
                "Qiyuan Guan",
                "Xiang Chen",
                "Guiyue Jin",
                "Jiyu Jin",
                "Shumin Fan",
                "Tianyu Song",
                "Jinshan Pan"
            ],
            "arxiv_id": "2510.17440v1",
            "summary": "Compared to daytime image deraining, nighttime image deraining poses\nsignificant challenges due to inherent complexities of nighttime scenarios and\nthe lack of high-quality datasets that accurately represent the coupling effect\nbetween rain and illumination. In this paper, we rethink the task of nighttime\nimage deraining and contribute a new high-quality benchmark, HQ-NightRain,\nwhich offers higher harmony and realism compared to existing datasets. In\naddition, we develop an effective Color Space Transformation Network (CST-Net)\nfor better removing complex rain from nighttime scenes. Specifically, we\npropose a learnable color space converter (CSC) to better facilitate rain\nremoval in the Y channel, as nighttime rain is more pronounced in the Y channel\ncompared to the RGB color space. To capture illumination information for\nguiding nighttime deraining, implicit illumination guidance is introduced\nenabling the learned features to improve the model's robustness in complex\nscenarios. Extensive experiments show the value of our dataset and the\neffectiveness of our method. The source code and datasets are available at\nhttps://github.com/guanqiyuan/CST-Net.",
            "headline_zh": "提出CST-Net和HQ-NightRain数据集以解决夜间图像去雨问题",
            "intro_zh": [
                "夜间图像去雨面临雨与光照耦合的复杂性和高质量数据集缺乏的核心问题",
                "方法要点包括可学习颜色空间转换器在Y通道去雨和隐式光照引导增强鲁棒性",
                "实验验证新数据集的高质量与模型在复杂场景中的有效性"
            ],
            "tags_zh": [
                "夜间图像去雨",
                "颜色空间转换",
                "隐式光照引导",
                "高质量数据集",
                "深度学习"
            ],
            "_index": 52
        },
        {
            "title": "From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors",
            "authors": [
                "Zhengshen Zhang",
                "Hao Li",
                "Yalun Dai",
                "Zhengbang Zhu",
                "Lei Zhou",
                "Chenchen Liu",
                "Dong Wang",
                "Francis E. H. Tay",
                "Sijin Chen",
                "Ziwei Liu",
                "Yuxiao Liu",
                "Xinghang Li",
                "Pan Zhou"
            ],
            "arxiv_id": "2510.17439v1",
            "summary": "Existing vision-language-action (VLA) models act in 3D real-world but are\ntypically built on 2D encoders, leaving a spatial reasoning gap that limits\ngeneralization and adaptability. Recent 3D integration techniques for VLAs\neither require specialized sensors and transfer poorly across modalities, or\ninject weak cues that lack geometry and degrade vision-language alignment. In\nthis work, we introduce FALCON (From Spatial to Action), a novel paradigm that\ninjects rich 3D spatial tokens into the action head. FALCON leverages spatial\nfoundation models to deliver strong geometric priors from RGB alone, and\nincludes an Embodied Spatial Model that can optionally fuse depth, or pose for\nhigher fidelity when available, without retraining or architectural changes. To\npreserve language reasoning, spatial tokens are consumed by a Spatial-Enhanced\nAction Head rather than being concatenated into the vision-language backbone.\nThese designs enable FALCON to address limitations in spatial representation,\nmodality transferability, and alignment. In comprehensive evaluations across\nthree simulation benchmarks and eleven real-world tasks, our proposed FALCON\nachieves state-of-the-art performance, consistently surpasses competitive\nbaselines, and remains robust under clutter, spatial-prompt conditioning, and\nvariations in object scale and height.",
            "headline_zh": "提出FALCON范式，通过注入3D空间令牌解决VLA模型空间推理不足问题",
            "intro_zh": [
                "现有VLA模型基于2D编码器，存在空间推理差距，限制泛化与适应性",
                "FALCON利用空间基础模型从RGB提取几何先验，并可选融合深度或姿态",
                "在模拟和真实世界任务中实现SOTA性能，保持鲁棒性"
            ],
            "tags_zh": [
                "视觉-语言-动作模型",
                "3D空间推理",
                "空间基础模型",
                "模态融合",
                "动作头增强"
            ],
            "_index": 53
        },
        {
            "title": "Leveraging AV1 motion vectors for Fast and Dense Feature Matching",
            "authors": [
                "Julien Zouein",
                "Hossein Javidnia",
                "François Pitié",
                "Anil Kokaram"
            ],
            "arxiv_id": "2510.17434v1",
            "summary": "We repurpose AV1 motion vectors to produce dense sub-pixel correspondences\nand short tracks filtered by cosine consistency. On short videos, this\ncompressed-domain front end runs comparably to sequential SIFT while using far\nless CPU, and yields denser matches with competitive pairwise geometry. As a\nsmall SfM demo on a 117-frame clip, MV matches register all images and\nreconstruct 0.46-0.62M points at 0.51-0.53,px reprojection error; BA time grows\nwith match density. These results show compressed-domain correspondences are a\npractical, resource-efficient front end with clear paths to scaling in full\npipelines.",
            "headline_zh": "利用AV1运动矢量实现快速密集特征匹配，作为资源高效的前端方法",
            "intro_zh": [
                "核心问题：传统特征匹配方法如SIFT计算密集，资源消耗大，难以扩展到长视频。",
                "方法要点：重新利用AV1压缩域的运动矢量，生成密集亚像素对应和余弦一致性过滤的短轨迹。",
                "实验效果：在短视频上，匹配密度高，重建点数达46万-62万，重投影误差0.51-0.53像素。"
            ],
            "tags_zh": [
                "运动矢量利用",
                "密集特征匹配",
                "压缩域处理",
                "结构从运动",
                "资源优化",
                "视频分析"
            ],
            "_index": 54
        },
        {
            "title": "DeepDetect: Learning All-in-One Dense Keypoints",
            "authors": [
                "Shaharyar Ahmed Khan Tareen",
                "Filza Khan Tareen"
            ],
            "arxiv_id": "2510.17422v1",
            "summary": "Keypoint detection is the foundation of many computer vision tasks, including\nimage registration, structure-from motion, 3D reconstruction, visual odometry,\nand SLAM. Traditional detectors (SIFT, SURF, ORB, BRISK, etc.) and learning\nbased methods (SuperPoint, R2D2, LF-Net, D2-Net, etc.) have shown strong\nperformance yet suffer from key limitations: sensitivity to photometric\nchanges, low keypoint density and repeatability, limited adaptability to\nchallenging scenes, and lack of semantic understanding, often failing to\nprioritize visually important regions. We present DeepDetect, an intelligent,\nall-in-one, dense keypoint detector that unifies the strengths of classical\ndetectors using deep learning. Firstly, we create ground-truth masks by fusing\noutputs of 7 keypoint and 2 edge detectors, extracting diverse visual cues from\ncorners and blobs to prominent edges and textures in the images. Afterwards, a\nlightweight and efficient model: ESPNet, is trained using these masks as\nlabels, enabling DeepDetect to focus semantically on images while producing\nhighly dense keypoints, that are adaptable to diverse and visually degraded\nconditions. Evaluations on the Oxford Affine Covariant Regions dataset\ndemonstrate that DeepDetect surpasses other detectors in keypoint density,\nrepeatability, and the number of correct matches, achieving maximum values of\n0.5143 (average keypoint density), 0.9582 (average repeatability), and 59,003\n(correct matches).",
            "headline_zh": "提出DeepDetect以解决关键点检测在密度、重复性和适应性方面的不足",
            "intro_zh": [
                "关键点检测存在对光照变化敏感、密度低、重复性差及缺乏语义理解等问题",
                "融合多种检测器输出生成真值掩码，训练轻量ESPNet模型实现语义感知的密集关键点检测",
                "在牛津数据集上评估，关键点密度、重复性和正确匹配数均优于现有方法"
            ],
            "tags_zh": [
                "关键点检测",
                "深度学习",
                "密集检测",
                "语义理解",
                "图像配准",
                "3D重建"
            ],
            "_index": 55
        },
        {
            "title": "Monitoring Horses in Stalls: From Object to Event Detection",
            "authors": [
                "Dmitrii Galimzianov",
                "Viacheslav Vyshegorodtsev",
                "Ivan Nezhivykh"
            ],
            "arxiv_id": "2510.17409v1",
            "summary": "Monitoring the behavior of stalled horses is essential for early detection of\nhealth and welfare issues but remains labor-intensive and time-consuming. In\nthis study, we present a prototype vision-based monitoring system that\nautomates the detection and tracking of horses and people inside stables using\nobject detection and multi-object tracking techniques. The system leverages\nYOLOv11 and BoT-SORT for detection and tracking, while event states are\ninferred based on object trajectories and spatial relations within the stall.\nTo support development, we constructed a custom dataset annotated with\nassistance from foundation models CLIP and GroundingDINO. The system\ndistinguishes between five event types and accounts for the camera's blind\nspots. Qualitative evaluation demonstrated reliable performance for\nhorse-related events, while highlighting limitations in detecting people due to\ndata scarcity. This work provides a foundation for real-time behavioral\nmonitoring in equine facilities, with implications for animal welfare and\nstable management.",
            "headline_zh": "提出基于视觉的监控系统以自动检测马厩中马匹和人的行为事件",
            "intro_zh": [
                "核心问题：马厩马匹行为监控依赖人工，耗时且效率低，需自动化早期健康问题检测。",
                "方法要点：使用YOLOv11和BoT-SORT进行目标检测与跟踪，结合轨迹和空间关系推断事件状态。",
                "实验或效果：定性评估显示马相关事件检测可靠，但人员检测因数据不足存在局限。"
            ],
            "tags_zh": [
                "目标检测",
                "多目标跟踪",
                "事件检测",
                "马匹监控",
                "自定义数据集"
            ],
            "_index": 56
        },
        {
            "title": "Integrating Trustworthy Artificial Intelligence with Energy-Efficient Robotic Arms for Waste Sorting",
            "authors": [
                "Halima I. Kure",
                "Jishna Retnakumari",
                "Augustine O. Nwajana",
                "Umar M. Ismail",
                "Bilyaminu A. Romo",
                "Ehigiator Egho-Promise"
            ],
            "arxiv_id": "2510.17408v1",
            "summary": "This paper presents a novel methodology that integrates trustworthy\nartificial intelligence (AI) with an energy-efficient robotic arm for\nintelligent waste classification and sorting. By utilizing a convolutional\nneural network (CNN) enhanced through transfer learning with MobileNetV2, the\nsystem accurately classifies waste into six categories: plastic, glass, metal,\npaper, cardboard, and trash. The model achieved a high training accuracy of\n99.8% and a validation accuracy of 80.5%, demonstrating strong learning and\ngeneralization. A robotic arm simulator is implemented to perform virtual\nsorting, calculating the energy cost for each action using Euclidean distance\nto ensure optimal and efficient movement. The framework incorporates key\nelements of trustworthy AI, such as transparency, robustness, fairness, and\nsafety, making it a reliable and scalable solution for smart waste management\nsystems in urban settings.",
            "headline_zh": "提出可信AI与节能机械臂集成方法以解决城市垃圾智能分类问题",
            "intro_zh": [
                "核心问题：城市垃圾智能分类需高精度、节能且可信赖的自动化系统。",
                "方法要点：使用MobileNetV2增强CNN进行迁移学习，实现六类垃圾准确分类。",
                "实验或效果：模型训练准确率99.8%，验证准确率80.5%；虚拟仿真优化能耗。"
            ],
            "tags_zh": [
                "可信人工智能",
                "节能机械臂",
                "垃圾分类",
                "卷积神经网络",
                "迁移学习",
                "能量优化"
            ],
            "_index": 57
        },
        {
            "title": "MILES: Modality-Informed Learning Rate Scheduler for Balancing Multimodal Learning",
            "authors": [
                "Alejandro Guerra-Manzanares",
                "Farah E. Shamout"
            ],
            "arxiv_id": "2510.17394v1",
            "summary": "The aim of multimodal neural networks is to combine diverse data sources,\nreferred to as modalities, to achieve enhanced performance compared to relying\non a single modality. However, training of multimodal networks is typically\nhindered by modality overfitting, where the network relies excessively on one\nof the available modalities. This often yields sub-optimal performance,\nhindering the potential of multimodal learning and resulting in marginal\nimprovements relative to unimodal models. In this work, we present the\nModality-Informed Learning ratE Scheduler (MILES) for training multimodal joint\nfusion models in a balanced manner. MILES leverages the differences in\nmodality-wise conditional utilization rates during training to effectively\nbalance multimodal learning. The learning rate is dynamically adjusted during\ntraining to balance the speed of learning from each modality by the multimodal\nmodel, aiming for enhanced performance in both multimodal and unimodal\npredictions. We extensively evaluate MILES on four multimodal joint fusion\ntasks and compare its performance to seven state-of-the-art baselines. Our\nresults show that MILES outperforms all baselines across all tasks and fusion\nmethods considered in our study, effectively balancing modality usage during\ntraining. This results in improved multimodal performance and stronger modality\nencoders, which can be leveraged when dealing with unimodal samples or absent\nmodalities. Overall, our work highlights the impact of balancing multimodal\nlearning on improving model performance.",
            "headline_zh": "提出MILES学习率调度器以平衡多模态学习中的模态过拟合问题",
            "intro_zh": [
                "多模态网络训练常因模态过拟合导致性能不佳，依赖单一模态",
                "MILES利用模态条件利用率差异动态调整学习率，平衡各模态学习速度",
                "在四个多模态任务中优于七种基线，提升多模态和单模态预测性能"
            ],
            "tags_zh": [
                "多模态学习",
                "学习率调度",
                "模态平衡",
                "联合融合模型",
                "条件利用率"
            ],
            "_index": 58
        },
        {
            "title": "Closed-Loop Transfer for Weakly-supervised Affordance Grounding",
            "authors": [
                "Jiajin Tang",
                "Zhengxuan Wei",
                "Ge Zheng",
                "Sibei Yang"
            ],
            "arxiv_id": "2510.17384v1",
            "summary": "Humans can perform previously unexperienced interactions with novel objects\nsimply by observing others engage with them. Weakly-supervised affordance\ngrounding mimics this process by learning to locate object regions that enable\nactions on egocentric images, using exocentric interaction images with\nimage-level annotations. However, extracting affordance knowledge solely from\nexocentric images and transferring it one-way to egocentric images limits the\napplicability of previous works in complex interaction scenarios. Instead, this\nstudy introduces LoopTrans, a novel closed-loop framework that not only\ntransfers knowledge from exocentric to egocentric but also transfers back to\nenhance exocentric knowledge extraction. Within LoopTrans, several innovative\nmechanisms are introduced, including unified cross-modal localization and\ndenoising knowledge distillation, to bridge domain gaps between object-centered\negocentric and interaction-centered exocentric images while enhancing knowledge\ntransfer. Experiments show that LoopTrans achieves consistent improvements\nacross all metrics on image and video benchmarks, even handling challenging\nscenarios where object interaction regions are fully occluded by the human\nbody.",
            "headline_zh": "提出LoopTrans闭环框架以解决弱监督可承受性接地在复杂交互场景中的局限性",
            "intro_zh": [
                "核心问题：现有方法仅从外中心图像单向转移知识，限制在复杂交互场景的应用。",
                "方法要点：引入闭环框架，双向转移知识，并采用统一跨模态定位和去噪知识蒸馏机制。",
                "实验效果：在图像和视频基准上实现指标一致提升，处理完全遮挡场景。"
            ],
            "tags_zh": [
                "弱监督学习",
                "可承受性接地",
                "闭环知识转移",
                "跨模态定位",
                "知识蒸馏",
                "遮挡处理"
            ],
            "_index": 59
        },
        {
            "title": "Latent Spaces Beyond Synthesis: From GANs to Diffusion Models",
            "authors": [
                "Ludovica Schaerf"
            ],
            "arxiv_id": "2510.17383v1",
            "summary": "This paper examines the evolving nature of internal representations in\ngenerative visual models, focusing on the conceptual and technical shift from\nGANs and VAEs to diffusion-based architectures. Drawing on Beatrice Fazi's\naccount of synthesis as the amalgamation of distributed representations, we\npropose a distinction between \"synthesis in a strict sense\", where a compact\nlatent space wholly determines the generative process, and \"synthesis in a\nbroad sense,\" which characterizes models whose representational labor is\ndistributed across layers. Through close readings of model architectures and a\ntargeted experimental setup that intervenes in layerwise representations, we\nshow how diffusion models fragment the burden of representation and thereby\nchallenge assumptions of unified internal space. By situating these findings\nwithin media theoretical frameworks and critically engaging with metaphors such\nas the latent space and the Platonic Representation Hypothesis, we argue for a\nreorientation of how generative AI is understood: not as a direct synthesis of\ncontent, but as an emergent configuration of specialized processes.",
            "headline_zh": "区分严格与广义合成，分析扩散模型如何挑战统一潜在空间假设",
            "intro_zh": [
                "核心问题：生成模型中内部表示的统一性，从GANs/VAEs到扩散模型的演变",
                "方法要点：提出严格与广义合成概念，通过层间表示干预实验验证",
                "实验或效果：扩散模型分散表示负担，挑战潜在空间统一假设"
            ],
            "tags_zh": [
                "生成模型",
                "潜在空间",
                "扩散模型",
                "表示学习",
                "媒体理论"
            ],
            "_index": 60
        },
        {
            "title": "Graph Attention-Guided Search for Dense Multi-Agent Pathfinding",
            "authors": [
                "Rishabh Jain",
                "Keisuke Okumura",
                "Michael Amir",
                "Amanda Prorok"
            ],
            "arxiv_id": "2510.17382v1",
            "summary": "Finding near-optimal solutions for dense multi-agent pathfinding (MAPF)\nproblems in real-time remains challenging even for state-of-the-art planners.\nTo this end, we develop a hybrid framework that integrates a learned heuristic\nderived from MAGAT, a neural MAPF policy with a graph attention scheme, into a\nleading search-based algorithm, LaCAM. While prior work has explored\nlearning-guided search in MAPF, such methods have historically underperformed.\nIn contrast, our approach, termed LaGAT, outperforms both purely search-based\nand purely learning-based methods in dense scenarios. This is achieved through\nan enhanced MAGAT architecture, a pre-train-then-fine-tune strategy on maps of\ninterest, and a deadlock detection scheme to account for imperfect neural\nguidance. Our results demonstrate that, when carefully designed, hybrid search\noffers a powerful solution for tightly coupled, challenging multi-agent\ncoordination problems.",
            "headline_zh": "提出LaGAT框架以解决密集多智能体路径规划问题",
            "intro_zh": [
                "密集多智能体路径规划在实时求解中面临挑战，现有方法性能不足",
                "集成学习启发式与搜索算法，采用图注意力机制和死锁检测",
                "在密集场景中优于纯搜索和纯学习方法，验证混合方法的有效性"
            ],
            "tags_zh": [
                "多智能体路径规划",
                "图注意力机制",
                "混合搜索算法",
                "死锁检测",
                "学习引导搜索"
            ],
            "_index": 61
        },
        {
            "title": "Facial Expression-based Parkinson's Disease Severity Diagnosis via Feature Fusion and Adaptive Class Balancing",
            "authors": [
                "Yintao Zhou",
                "Wei Huang",
                "Zhengyu Li",
                "Jing Huang",
                "Meng Pang"
            ],
            "arxiv_id": "2510.17373v1",
            "summary": "Parkinson's disease (PD) severity diagnosis is crucial for early detecting\npotential patients and adopting tailored interventions. Diagnosing PD based on\nfacial expression is grounded in PD patients' \"masked face\" symptom and gains\ngrowing interest recently for its convenience and affordability. However,\ncurrent facial expression-based approaches often rely on single type of\nexpression which can lead to misdiagnosis, and ignore the class imbalance\nacross different PD stages which degrades the prediction performance. Moreover,\nmost existing methods focus on binary classification (i.e., PD / non-PD) rather\nthan diagnosing the severity of PD. To address these issues, we propose a new\nfacial expression-based method for PD severity diagnosis which integrates\nmultiple facial expression features through attention-based feature fusion.\nMoreover, we mitigate the class imbalance problem via an adaptive class\nbalancing strategy which dynamically adjusts the contribution of training\nsamples based on their class distribution and classification difficulty.\nExperimental results demonstrate the promising performance of the proposed\nmethod for PD severity diagnosis, as well as the efficacy of attention-based\nfeature fusion and adaptive class balancing.",
            "headline_zh": "提出基于多表情特征融合与自适应类平衡的方法，用于帕金森病严重程度诊断。",
            "intro_zh": [
                "核心问题：单表情诊断易误诊，类不平衡影响多阶段PD严重程度预测性能。",
                "方法要点：采用注意力机制融合多表情特征，动态调整样本贡献以平衡类别。",
                "实验或效果：实验显示方法在PD严重程度诊断中表现优异，验证了融合与平衡策略有效性。"
            ],
            "tags_zh": [
                "帕金森病诊断",
                "面部表情分析",
                "特征融合",
                "类不平衡处理",
                "注意力机制",
                "多分类任务"
            ],
            "_index": 62
        },
        {
            "title": "Beyond Real Faces: Synthetic Datasets Can Achieve Reliable Recognition Performance without Privacy Compromise",
            "authors": [
                "Paweł Borsukiewicz",
                "Fadi Boutros",
                "Iyiola E. Olatunji",
                "Charles Beumier",
                "Wendkûuni C. Ouedraogo",
                "Jacques Klein",
                "Tegawendé F. Bissyandé"
            ],
            "arxiv_id": "2510.17372v1",
            "summary": "The deployment of facial recognition systems has created an ethical dilemma:\nachieving high accuracy requires massive datasets of real faces collected\nwithout consent, leading to dataset retractions and potential legal liabilities\nunder regulations like GDPR. While synthetic facial data presents a promising\nprivacy-preserving alternative, the field lacks comprehensive empirical\nevidence of its viability. This study addresses this critical gap through\nextensive evaluation of synthetic facial recognition datasets. We present a\nsystematic literature review identifying 25 synthetic facial recognition\ndatasets (2018-2025), combined with rigorous experimental validation. Our\nmethodology examines seven key requirements for privacy-preserving synthetic\ndata: identity leakage prevention, intra-class variability, identity\nseparability, dataset scale, ethical data sourcing, bias mitigation, and\nbenchmark reliability. Through experiments involving over 10 million synthetic\nsamples, extended by a comparison of results reported on five standard\nbenchmarks, we provide the first comprehensive empirical assessment of\nsynthetic data's capability to replace real datasets. Best-performing synthetic\ndatasets (VariFace, VIGFace) achieve recognition accuracies of 95.67% and\n94.91% respectively, surpassing established real datasets including\nCASIA-WebFace (94.70%). While those images remain private, publicly available\nalternatives Vec2Face (93.52%) and CemiFace (93.22%) come close behind. Our\nfindings reveal that they ensure proper intra-class variability while\nmaintaining identity separability. Demographic bias analysis shows that, even\nthough synthetic data inherits limited biases, it offers unprecedented control\nfor bias mitigation through generation parameters. These results establish\nsynthetic facial data as a scientifically viable and ethically imperative\nalternative for facial recognition research.",
            "headline_zh": "提出合成人脸数据集以解决隐私问题并实现可靠识别性能",
            "intro_zh": [
                "核心问题：真实人脸数据集收集引发隐私和伦理问题，需隐私保护替代方案。",
                "方法要点：系统评估合成数据集，关注身份泄漏预防、变异性等七项隐私要求。",
                "实验或效果：合成数据集VariFace和VIGFace准确率超95%，优于部分真实数据集。"
            ],
            "tags_zh": [
                "合成人脸数据",
                "隐私保护",
                "人脸识别",
                "数据集评估",
                "偏见缓解",
                "身份分离"
            ],
            "_index": 63
        },
        {
            "title": "Bridging Embodiment Gaps: Deploying Vision-Language-Action Models on Soft Robots",
            "authors": [
                "Haochen Su",
                "Cristian Meo",
                "Francesco Stella",
                "Andrea Peirone",
                "Kai Junge",
                "Josie Hughes"
            ],
            "arxiv_id": "2510.17369v1",
            "summary": "Robotic systems are increasingly expected to operate in human-centered,\nunstructured environments where safety, adaptability, and generalization are\nessential. Vision-Language-Action (VLA) models have been proposed as a language\nguided generalized control framework for real robots. However, their deployment\nhas been limited to conventional serial link manipulators. Coupled by their\nrigidity and unpredictability of learning based control, the ability to safely\ninteract with the environment is missing yet critical. In this work, we present\nthe deployment of a VLA model on a soft continuum manipulator to demonstrate\nautonomous safe human-robot interaction. We present a structured finetuning and\ndeployment pipeline evaluating two state-of-the-art VLA models (OpenVLA-OFT and\n$\\pi_0$) across representative manipulation tasks, and show while\nout-of-the-box policies fail due to embodiment mismatch, through targeted\nfinetuning the soft robot performs equally to the rigid counterpart. Our\nfindings highlight the necessity of finetuning for bridging embodiment gaps,\nand demonstrate that coupling VLA models with soft robots enables safe and\nflexible embodied AI in human-shared environments.",
            "headline_zh": "部署视觉-语言-动作模型于软机器人，实现安全人机交互",
            "intro_zh": [
                "核心问题：视觉-语言-动作模型部署于刚性机器人，缺乏安全交互能力。",
                "方法要点：提出结构化微调流程，评估OpenVLA-OFT和π_0模型。",
                "实验或效果：微调后软机器人性能与刚性机器人相当，支持安全交互。"
            ],
            "tags_zh": [
                "视觉-语言-动作模型",
                "软机器人",
                "微调部署",
                "人机交互",
                "安全控制"
            ],
            "_index": 64
        },
        {
            "title": "Recurrent Attention-based Token Selection for Efficient Streaming Video-LLMs",
            "authors": [
                "Vaggelis Dorovatas",
                "Soroush Seifi",
                "Gunshi Gupta",
                "Rahaf Aljundi"
            ],
            "arxiv_id": "2510.17364v1",
            "summary": "Video Large Language Models (Video-LLMs) excel at understanding videos\nin-context, provided they have full access to the video when answering queries.\nHowever, these models face challenges in streaming scenarios where hour-long\nvideos must be processed online, and questions need timely responses. In this\nwork, we propose a training-free approach compatible with standard Video-LLMs,\nleveraging three key concepts: 1) LLM-informed selection of visual tokens to\nidentify those that the LLM has attended to and contributed to its\nunderstanding of each short clip. Our attention-based selection allows us to\ndiscard up to ~95% of unimportant visual tokens with minimal performance loss;\n2) Recurrent processing of past selected tokens to generate temporally coherent\nunderstanding of each processed clip; 3) Caption-based question answering for\nlightweight and accurate responses. Our method achieves state-of-the-art\nperformance on streaming video benchmarks, striking a balance between\nefficiency and effectiveness.",
            "headline_zh": "提出基于循环注意力的令牌选择方法，以提升流式视频-大语言模型的效率。",
            "intro_zh": [
                "核心问题：流式场景下，长视频需在线处理，标准Video-LLMs难以实时响应查询。",
                "方法要点：利用LLM注意力选择视觉令牌，结合循环处理和历史令牌，实现轻量级问答。",
                "实验或效果：在流式视频基准上达到SOTA，丢弃95%令牌时性能损失最小。"
            ],
            "tags_zh": [
                "流式视频处理",
                "视觉令牌选择",
                "注意力机制",
                "视频-大语言模型",
                "效率优化"
            ],
            "_index": 65
        },
        {
            "title": "M2H: Multi-Task Learning with Efficient Window-Based Cross-Task Attention for Monocular Spatial Perception",
            "authors": [
                "U. V. B. L Udugama",
                "George Vosselman",
                "Francesco Nex"
            ],
            "arxiv_id": "2510.17363v1",
            "summary": "Deploying real-time spatial perception on edge devices requires efficient\nmulti-task models that leverage complementary task information while minimizing\ncomputational overhead. This paper introduces Multi-Mono-Hydra (M2H), a novel\nmulti-task learning framework designed for semantic segmentation and depth,\nedge, and surface normal estimation from a single monocular image. Unlike\nconventional approaches that rely on independent single-task models or shared\nencoder-decoder architectures, M2H introduces a Window-Based Cross-Task\nAttention Module that enables structured feature exchange while preserving\ntask-specific details, improving prediction consistency across tasks. Built on\na lightweight ViT-based DINOv2 backbone, M2H is optimized for real-time\ndeployment and serves as the foundation for monocular spatial perception\nsystems supporting 3D scene graph construction in dynamic environments.\nComprehensive evaluations show that M2H outperforms state-of-the-art multi-task\nmodels on NYUDv2, surpasses single-task depth and semantic baselines on\nHypersim, and achieves superior performance on the Cityscapes dataset, all\nwhile maintaining computational efficiency on laptop hardware. Beyond\nbenchmarks, M2H is validated on real-world data, demonstrating its practicality\nin spatial perception tasks.",
            "headline_zh": "提出M2H多任务学习框架，通过窗口跨任务注意力提升单目图像空间感知效率",
            "intro_zh": [
                "核心问题：边缘设备实时空间感知需高效多任务模型，减少计算开销",
                "方法要点：引入窗口跨任务注意力模块，结构化交换特征，保持任务细节",
                "实验或效果：在NYUDv2等数据集超越SOTA，保持计算高效，验证实际应用"
            ],
            "tags_zh": [
                "多任务学习",
                "单目空间感知",
                "窗口注意力",
                "轻量级ViT",
                "实时部署",
                "3D场景图"
            ],
            "_index": 66
        },
        {
            "title": "Exploring The Missing Semantics In Event Modality",
            "authors": [
                "Jingqian Wu",
                "Shengpeng Xu",
                "Yunbo Jia",
                "Edmund Y. Lam"
            ],
            "arxiv_id": "2510.17347v1",
            "summary": "Event cameras offer distinct advantages such as low latency, high dynamic\nrange, and efficient motion capture. However, event-to-video reconstruction\n(E2V), a fundamental event-based vision task, remains challenging, particularly\nfor reconstructing and recovering semantic information. This is primarily due\nto the nature of the event camera, as it only captures intensity changes,\nignoring static objects and backgrounds, resulting in a lack of semantic\ninformation in captured event modality. Further, semantic information plays a\ncrucial role in video and frame reconstruction, yet is often overlooked by\nexisting E2V approaches. To bridge this gap, we propose Semantic-E2VID, an E2V\nframework that explores the missing visual semantic knowledge in event modality\nand leverages it to enhance event-to-video reconstruction. Specifically,\nSemantic-E2VID introduces a cross-modal feature alignment (CFA) module to\ntransfer the robust visual semantics from a frame-based vision foundation\nmodel, the Segment Anything Model (SAM), to the event encoder, while aligning\nthe high-level features from distinct modalities. To better utilize the learned\nsemantic feature, we further propose a semantic-aware feature fusion (SFF)\nblock to integrate learned semantics in frame modality to form event\nrepresentations with rich semantics that can be decoded by the event decoder.\nFurther, to facilitate the reconstruction of semantic information, we propose a\nnovel Semantic Perceptual E2V Supervision that helps the model to reconstruct\nsemantic details by leveraging SAM-generated categorical labels. Extensive\nexperiments demonstrate that Semantic-E2VID significantly enhances frame\nquality, outperforming state-of-the-art E2V methods across multiple benchmarks.\nThe sample code is included in the supplementary material.",
            "headline_zh": "提出Semantic-E2VID框架，通过引入语义信息增强事件到视频重建质量",
            "intro_zh": [
                "核心问题：事件相机仅捕捉强度变化，缺乏语义信息，导致事件到视频重建困难。",
                "方法要点：使用跨模态特征对齐和语义感知特征融合，从SAM模型迁移语义知识。",
                "实验或效果：在多个基准测试中显著提升帧质量，优于现有先进方法。"
            ],
            "tags_zh": [
                "事件到视频重建",
                "语义信息增强",
                "跨模态特征对齐",
                "Segment Anything Model",
                "事件相机视觉"
            ],
            "_index": 67
        },
        {
            "title": "Interactive Force-Impedance Control",
            "authors": [
                "Fan Shao",
                "Satoshi Endo",
                "Sandra Hirche",
                "Fanny Ficuciello"
            ],
            "arxiv_id": "2510.17341v1",
            "summary": "Human collaboration with robots requires flexible role adaptation, enabling\nrobot to switch between active leader and passive follower. Effective role\nswitching depends on accurately estimating human intention, which is typically\nachieved through external force analysis, nominal robot dynamics, or\ndata-driven approaches. However, these methods are primarily effective in\ncontact-sparse environments. When robots under hybrid or unified\nforce-impedance control physically interact with active humans or non-passive\nenvironments, the robotic system may lose passivity and thus compromise safety.\nTo address this challenge, this paper proposes the unified Interactive\nForce-Impedance Control (IFIC) framework that adapts to the interaction power\nflow, ensuring effortless and safe interaction in contact-rich environments.\nThe proposed control architecture is formulated within a port-Hamiltonian\nframework, incorporating both interaction and task control ports, through which\nsystem passivity is guaranteed.",
            "headline_zh": "提出统一交互力-阻抗控制框架，确保接触丰富环境中的安全交互。",
            "intro_zh": [
                "核心问题：机器人在接触丰富环境中交互时可能失去被动性，影响安全性。",
                "方法要点：基于端口哈密顿框架设计控制架构，保证系统被动性。",
                "实验或效果：未知，但框架旨在实现轻松安全的物理交互。"
            ],
            "tags_zh": [
                "力-阻抗控制",
                "人机交互",
                "端口哈密顿框架",
                "系统被动性",
                "安全控制"
            ],
            "_index": 68
        },
        {
            "title": "Nearest-Class Mean and Logits Agreement for Wildlife Open-Set Recognition",
            "authors": [
                "Jiahao Huo",
                "Mufhumudzi Muthivhi",
                "Terence L. van Zyl",
                "Fredrik Gustafsson"
            ],
            "arxiv_id": "2510.17338v1",
            "summary": "Current state-of-the-art Wildlife classification models are trained under the\nclosed world setting. When exposed to unknown classes, they remain\noverconfident in their predictions. Open-set Recognition (OSR) aims to classify\nknown classes while rejecting unknown samples. Several OSR methods have been\nproposed to model the closed-set distribution by observing the feature, logit,\nor softmax probability space. A significant drawback of many existing\napproaches is the requirement to retrain the pre-trained classification model\nwith the OSR-specific strategy. This study contributes a post-processing OSR\nmethod that measures the agreement between the models' features and predicted\nlogits. We propose a probability distribution based on an input's distance to\nits Nearest Class Mean (NCM). The NCM-based distribution is then compared with\nthe softmax probabilities from the logit space to measure agreement between the\nNCM and the classification head. Our proposed strategy ranks within the top\nthree on two evaluated datasets, showing consistent performance across the two\ndatasets. In contrast, current state-of-the-art methods excel on a single\ndataset. We achieve an AUROC of 93.41 and 95.35 for African and Swedish\nanimals. The code can be found\nhttps://github.com/Applied-Representation-Learning-Lab/OSR.",
            "headline_zh": "提出基于最近类均值和logits一致性的后处理开放集识别方法，用于野生动物分类。",
            "intro_zh": [
                "核心问题：现有野生动物分类模型在开放集场景中过度自信，无法拒绝未知类样本。",
                "方法要点：通过比较输入特征与最近类均值的距离分布和softmax概率，衡量一致性。",
                "实验或效果：在两个数据集上排名前三，AUROC达93.41和95.35，性能稳定。"
            ],
            "tags_zh": [
                "开放集识别",
                "野生动物分类",
                "最近类均值",
                "logits一致性",
                "后处理方法",
                "特征距离"
            ],
            "_index": 69
        },
        {
            "title": "DDBot: Differentiable Physics-based Digging Robot for Unknown Granular Materials",
            "authors": [
                "Xintong Yang",
                "Minglun Wei",
                "Ze Ji",
                "Yu-Kun Lai"
            ],
            "arxiv_id": "2510.17335v1",
            "summary": "Automating the manipulation of granular materials poses significant\nchallenges due to complex contact dynamics, unpredictable material properties,\nand intricate system states. Existing approaches often fail to achieve\nefficiency and accuracy in such tasks. To fill the research gap, this paper\nstudies the small-scale and high-precision granular material digging task with\nunknown physical properties. A new framework, named differentiable digging\nrobot (DDBot), is proposed to manipulate granular materials, including sand and\nsoil.\n  Specifically, we equip DDBot with a differentiable physics-based simulator,\ntailored for granular material manipulation, powered by GPU-accelerated\nparallel computing and automatic differentiation. DDBot can perform efficient\ndifferentiable system identification and high-precision digging skill\noptimisation for unknown granular materials, which is enabled by a\ndifferentiable skill-to-action mapping, a task-oriented demonstration method,\ngradient clipping and line search-based gradient descent.\n  Experimental results show that DDBot can efficiently (converge within 5 to 20\nminutes) identify unknown granular material dynamics and optimise digging\nskills, with high-precision results in zero-shot real-world deployments,\nhighlighting its practicality. Benchmark results against state-of-the-art\nbaselines also confirm the robustness and efficiency of DDBot in such digging\ntasks.",
            "headline_zh": "提出可微分物理挖掘机器人DDBot，用于未知颗粒材料的高精度挖掘任务。",
            "intro_zh": [
                "核心问题：颗粒材料操作因复杂接触动态和未知物理属性而难以实现高效精准。",
                "方法要点：采用可微分物理模拟器，结合GPU加速和自动微分，优化系统识别与挖掘技能。",
                "实验效果：在5-20分钟内收敛，零样本真实部署中实现高精度，优于现有基线。"
            ],
            "tags_zh": [
                "可微分物理模拟",
                "颗粒材料操作",
                "系统识别",
                "挖掘技能优化",
                "GPU加速计算",
                "零样本部署"
            ],
            "_index": 70
        },
        {
            "title": "iDETEX: Empowering MLLMs for Intelligent DETailed EXplainable IQA",
            "authors": [
                "Zhaoran Zhao",
                "Xinli Yue",
                "Jianhui Sun",
                "Yuhao Xie",
                "Tao Shao",
                "Liangchao Yao",
                "Fan Xia",
                "Yuetang Deng"
            ],
            "arxiv_id": "2510.17332v1",
            "summary": "Image Quality Assessment (IQA) has progressed from scalar quality prediction\nto more interpretable, human-aligned evaluation paradigms. In this work, we\naddress the emerging challenge of detailed and explainable IQA by proposing\niDETEX-a unified multimodal large language model (MLLM) capable of\nsimultaneously performing three key tasks: quality grounding, perception, and\ndescription. To facilitate efficient and generalizable training across these\nheterogeneous subtasks, we design a suite of task-specific offline augmentation\nmodules and a data mixing strategy. These are further complemented by online\nenhancement strategies to fully exploit multi-sourced supervision. We validate\nour approach on the large-scale ViDA-UGC benchmark, where iDETEX achieves\nstate-of-the-art performance across all subtasks. Our model ranks first in the\nICCV MIPI 2025 Detailed Image Quality Assessment Challenge, demonstrating its\neffectiveness and robustness in delivering accurate and interpretable quality\nassessments.",
            "headline_zh": "提出iDETEX模型以解决详细可解释图像质量评估问题",
            "intro_zh": [
                "核心问题：图像质量评估需从标量预测转向详细可解释评估。",
                "方法要点：设计多模态大语言模型，统一执行质量定位、感知和描述任务。",
                "实验或效果：在ViDA-UGC基准上实现SOTA，并在ICCV挑战赛中排名第一。"
            ],
            "tags_zh": [
                "图像质量评估",
                "多模态大语言模型",
                "可解释AI",
                "质量定位",
                "数据增强策略"
            ],
            "_index": 71
        },
        {
            "title": "CharDiff: A Diffusion Model with Character-Level Guidance for License Plate Image Restoration",
            "authors": [
                "Gyuhwan Park",
                "Kihyun Na",
                "Injung Kim"
            ],
            "arxiv_id": "2510.17330v1",
            "summary": "The significance of license plate image restoration goes beyond the\npreprocessing stage of License Plate Recognition (LPR) systems, as it also\nserves various purposes, including increasing evidential value, enhancing the\nclarity of visual interface, and facilitating further utilization of license\nplate images. We propose a novel diffusion-based framework with character-level\nguidance, CharDiff, which effectively restores and recognizes severely degraded\nlicense plate images captured under realistic conditions. CharDiff leverages\nfine-grained character-level priors extracted through external segmentation and\nOptical Character Recognition (OCR) modules tailored for low-quality license\nplate images. For precise and focused guidance, CharDiff incorporates a novel\nCharacter-guided Attention through Region-wise Masking (CHARM) module, which\nensures that each character's guidance is restricted to its own region, thereby\navoiding interference with other regions. In experiments, CharDiff\nsignificantly outperformed the baseline restoration models in both restoration\nquality and recognition accuracy, achieving a 28% relative reduction in CER on\nthe Roboflow-LP dataset, compared to the best-performing baseline model. These\nresults indicate that the structured character-guided conditioning effectively\nenhances the robustness of diffusion-based license plate restoration and\nrecognition in practical deployment scenarios.",
            "headline_zh": "提出CharDiff扩散模型，通过字符级指导解决真实场景下车牌图像恢复问题。",
            "intro_zh": [
                "核心问题：真实场景下车牌图像严重退化，影响识别和证据价值。",
                "方法要点：结合字符分割和OCR提取先验，使用CHARM模块实现区域掩码注意力指导。",
                "实验或效果：在Roboflow-LP数据集上，CER相对降低28%，恢复质量和识别精度显著提升。"
            ],
            "tags_zh": [
                "车牌图像恢复",
                "扩散模型",
                "字符级指导",
                "光学字符识别",
                "注意力机制",
                "图像增强"
            ],
            "_index": 72
        },
        {
            "title": "A Single Set of Adversarial Clothes Breaks Multiple Defense Methods in the Physical World",
            "authors": [
                "Wei Zhang",
                "Zhanhao Hu",
                "Xiao Li",
                "Xiaopei Zhu",
                "Xiaolin Hu"
            ],
            "arxiv_id": "2510.17322v1",
            "summary": "In recent years, adversarial attacks against deep learning-based object\ndetectors in the physical world have attracted much attention. To defend\nagainst these attacks, researchers have proposed various defense methods\nagainst adversarial patches, a typical form of physically-realizable attack.\nHowever, our experiments showed that simply enlarging the patch size could make\nthese defense methods fail. Motivated by this, we evaluated various defense\nmethods against adversarial clothes which have large coverage over the human\nbody. Adversarial clothes provide a good test case for adversarial defense\nagainst patch-based attacks because they not only have large sizes but also\nlook more natural than a large patch on humans. Experiments show that all the\ndefense methods had poor performance against adversarial clothes in both the\ndigital world and the physical world. In addition, we crafted a single set of\nclothes that broke multiple defense methods on Faster R-CNN. The set achieved\nan Attack Success Rate (ASR) of 96.06% against the undefended detector and over\n64.84% ASRs against nine defended models in the physical world, unveiling the\ncommon vulnerability of existing adversarial defense methods against\nadversarial clothes. Code is available at:\nhttps://github.com/weiz0823/adv-clothes-break-multiple-defenses.",
            "headline_zh": "提出对抗性衣物攻击，揭示多种防御方法在物理世界中的共同脆弱性。",
            "intro_zh": [
                "核心问题：现有防御方法对基于补丁的物理世界对抗攻击存在漏洞，尤其在大尺寸攻击下。",
                "方法要点：通过扩大补丁尺寸并设计自然外观的对抗性衣物，评估多种防御方法的鲁棒性。",
                "实验或效果：单一对抗衣物在物理世界中攻击成功率高达96.06%，对九种防御模型均超过64.84%。"
            ],
            "tags_zh": [
                "对抗攻击",
                "物理世界防御",
                "对象检测",
                "对抗衣物",
                "攻击成功率",
                "深度学习安全"
            ],
            "_index": 73
        },
        {
            "title": "CausalMamba: Scalable Conditional State Space Models for Neural Causal Inference",
            "authors": [
                "Sangyoon Bae",
                "Jiook Cha"
            ],
            "arxiv_id": "2510.17318v1",
            "summary": "We introduce CausalMamba, a scalable framework that addresses fundamental\nlimitations in fMRI-based causal inference: the ill-posed nature of inferring\nneural causality from hemodynamically distorted BOLD signals and the\ncomputational intractability of existing methods like Dynamic Causal Modeling\n(DCM). Our approach decomposes this complex inverse problem into two tractable\nstages: BOLD deconvolution to recover latent neural activity, followed by\ncausal graph inference using a novel Conditional Mamba architecture. On\nsimulated data, CausalMamba achieves 37% higher accuracy than DCM. Critically,\nwhen applied to real task fMRI data, our method recovers well-established\nneural pathways with 88% fidelity, whereas conventional approaches fail to\nidentify these canonical circuits in over 99% of subjects. Furthermore, our\nnetwork analysis of working memory data reveals that the brain strategically\nshifts its primary causal hub-recruiting executive or salience networks\ndepending on the stimulus-a sophisticated reconfiguration that remains\nundetected by traditional methods. This work provides neuroscientists with a\npractical tool for large-scale causal inference that captures both fundamental\ncircuit motifs and flexible network dynamics underlying cognitive function.",
            "headline_zh": "提出CausalMamba框架以解决fMRI因果推断中的病态逆问题和计算难题",
            "intro_zh": [
                "核心问题：从血氧水平依赖信号推断神经因果性存在病态逆问题和计算不可行性",
                "方法要点：将问题分解为BOLD去卷积和因果图推断，采用条件Mamba架构",
                "实验或效果：在模拟数据上准确率比DCM高37%，真实数据中恢复已知神经通路达88%"
            ],
            "tags_zh": [
                "因果推断",
                "状态空间模型",
                "fMRI分析",
                "BOLD去卷积",
                "神经网络动态"
            ],
            "_index": 74
        },
        {
            "title": "Implicit State Estimation via Video Replanning",
            "authors": [
                "Po-Chen Ko",
                "Jiayuan Mao",
                "Yu-Hsiang Fu",
                "Hsien-Jeng Yeh",
                "Chu-Rong Chen",
                "Wei-Chiu Ma",
                "Yilun Du",
                "Shao-Hua Sun"
            ],
            "arxiv_id": "2510.17315v1",
            "summary": "Video-based representations have gained prominence in planning and\ndecision-making due to their ability to encode rich spatiotemporal dynamics and\ngeometric relationships. These representations enable flexible and\ngeneralizable solutions for complex tasks such as object manipulation and\nnavigation. However, existing video planning frameworks often struggle to adapt\nto failures at interaction time due to their inability to reason about\nuncertainties in partially observed environments. To overcome these\nlimitations, we introduce a novel framework that integrates interaction-time\ndata into the planning process. Our approach updates model parameters online\nand filters out previously failed plans during generation. This enables\nimplicit state estimation, allowing the system to adapt dynamically without\nexplicitly modeling unknown state variables. We evaluate our framework through\nextensive experiments on a new simulated manipulation benchmark, demonstrating\nits ability to improve replanning performance and advance the field of\nvideo-based decision-making.",
            "headline_zh": "提出视频重规划框架以解决部分观测环境中的不确定性适应问题",
            "intro_zh": [
                "核心问题：视频规划框架难以适应交互失败，因无法推理部分观测环境的不确定性",
                "方法要点：在线更新模型参数并过滤失败计划，实现隐式状态估计",
                "实验或效果：在模拟操作基准上验证，提升重规划性能和视频决策能力"
            ],
            "tags_zh": [
                "视频规划",
                "隐式状态估计",
                "在线学习",
                "重规划",
                "模拟操作"
            ],
            "_index": 75
        },
        {
            "title": "LongInsightBench: A Comprehensive Benchmark for Evaluating Omni-Modal Models on Human-Centric Long-Video Understanding",
            "authors": [
                "ZhaoYang Han",
                "Qihan Lin",
                "Hao Liang",
                "Bowen Chen",
                "Zhou Liu",
                "Wentao Zhang"
            ],
            "arxiv_id": "2510.17305v1",
            "summary": "We introduce \\textbf{LongInsightBench}, the first benchmark designed to\nassess models' ability to understand long videos, with a focus on human\nlanguage, viewpoints, actions, and other contextual elements, while integrating\n\\textbf{visual, audio, and text} modalities. Our benchmark excels in three key\nareas: \\textbf{a) Long-Duration, Information-Dense Videos:} We carefully select\napproximately 1,000 videos from open-source datasets FineVideo based on\nduration limit and the information density of both visual and audio modalities,\nfocusing on content like lectures, interviews, and vlogs, which contain rich\nlanguage elements. \\textbf{b) Diverse and Challenging Task Scenarios:} We have\ndesigned six challenging task scenarios, including both Intra-Event and\nInter-Event Tasks. \\textbf{c) Rigorous and Comprehensive Quality Assurance\nPipelines:} We have developed a three-step, semi-automated data quality\nassurance pipeline to ensure the difficulty and validity of the synthesized\nquestions and answer options. Based on LongInsightBench, we designed a series\nof experiments. Experimental results shows that Omni-modal models(OLMs) still\nface challenge in tasks requiring precise temporal localization (T-Loc) and\nlong-range causal inference (CE-Caus). Extended experiments reveal the\ninformation loss and processing bias in multi-modal fusion of OLMs. Our dataset\nand code is available at\nhttps://anonymous.4open.science/r/LongInsightBench-910F/.",
            "headline_zh": "提出LongInsightBench基准以评估全模态模型在人类中心长视频理解中的表现",
            "intro_zh": [
                "核心问题：全模态模型在长视频理解中面临时间定位和长程因果推理挑战",
                "方法要点：构建包含视觉、音频和文本的多模态长视频基准，涵盖六种任务场景",
                "实验或效果：实验显示模型在T-Loc和CE-Caus任务中表现不佳，揭示多模态融合偏差"
            ],
            "tags_zh": [
                "长视频理解",
                "全模态模型",
                "基准评估",
                "多模态融合",
                "时间定位",
                "因果推理"
            ],
            "_index": 76
        },
        {
            "title": "Exploring Structural Degradation in Dense Representations for Self-supervised Learning",
            "authors": [
                "Siran Dai",
                "Qianqian Xu",
                "Peisong Wen",
                "Yang Liu",
                "Qingming Huang"
            ],
            "arxiv_id": "2510.17299v1",
            "summary": "In this work, we observe a counterintuitive phenomenon in self-supervised\nlearning (SSL): longer training may impair the performance of dense prediction\ntasks (e.g., semantic segmentation). We refer to this phenomenon as\nSelf-supervised Dense Degradation (SDD) and demonstrate its consistent presence\nacross sixteen state-of-the-art SSL methods with various losses, architectures,\nand datasets. When the model performs suboptimally on dense tasks at the end of\ntraining, measuring the performance during training becomes essential. However,\nevaluating dense performance effectively without annotations remains an open\nchallenge. To tackle this issue, we introduce a Dense representation Structure\nEstimator (DSE), composed of a class-relevance measure and an effective\ndimensionality measure. The proposed DSE is both theoretically grounded and\nempirically validated to be closely correlated with the downstream performance.\nBased on this metric, we introduce a straightforward yet effective model\nselection strategy and a DSE-based regularization method. Experiments on\nsixteen SSL methods across four benchmarks confirm that model selection\nimproves mIoU by $3.0\\%$ on average with negligible computational cost.\nAdditionally, DSE regularization consistently mitigates the effects of dense\ndegradation. Code is available at\nhttps://github.com/EldercatSAM/SSL-Degradation.",
            "headline_zh": "提出DSE指标与策略以缓解自监督学习中的密集预测退化问题",
            "intro_zh": [
                "自监督学习中，训练时间延长可能损害密集预测任务性能，称为SDD现象",
                "引入DSE指标，结合类相关性和有效维度，无需标注评估密集表示结构",
                "实验验证模型选择和正则化策略平均提升mIoU 3.0%，有效缓解退化"
            ],
            "tags_zh": [
                "自监督学习",
                "密集预测",
                "表示结构评估",
                "模型选择",
                "正则化方法"
            ],
            "_index": 77
        },
        {
            "title": "Machine Vision-Based Surgical Lighting System:Design and Implementation",
            "authors": [
                "Amir Gharghabi",
                "Mahdi Hakiminezhad",
                "Maryam Shafaei",
                "Shaghayegh Gharghabi"
            ],
            "arxiv_id": "2510.17287v1",
            "summary": "Effortless and ergonomically designed surgical lighting is critical for\nprecision and safety during procedures. However, traditional systems often rely\non manual adjustments, leading to surgeon fatigue, neck strain, and\ninconsistent illumination due to drift and shadowing. To address these\nchallenges, we propose a novel surgical lighting system that leverages the\nYOLOv11 object detection algorithm to identify a blue marker placed above the\ntarget surgical site. A high-power LED light source is then directed to the\nidentified location using two servomotors equipped with tilt-pan brackets. The\nYOLO model achieves 96.7% mAP@50 on the validation set consisting of annotated\nimages simulating surgical scenes with the blue spherical marker. By automating\nthe lighting process, this machine vision-based solution reduces physical\nstrain on surgeons, improves consistency in illumination, and supports improved\nsurgical outcomes.",
            "headline_zh": "提出基于YOLOv11的自动手术照明系统以解决手动调整导致的疲劳和照明不一致问题",
            "intro_zh": [
                "传统手术照明系统依赖手动调整，易导致外科医生疲劳、颈部劳损和照明漂移阴影问题",
                "使用YOLOv11算法检测蓝色标记，通过伺服电机控制LED光源自动对准目标位置",
                "在模拟手术场景验证集上，YOLO模型达到96.7% mAP@50，提升照明一致性和手术效果"
            ],
            "tags_zh": [
                "手术照明系统",
                "YOLOv11目标检测",
                "伺服电机控制",
                "机器视觉应用",
                "医疗自动化"
            ],
            "_index": 78
        },
        {
            "title": "SG-CLDFF: A Novel Framework for Automated White Blood Cell Classification and Segmentation",
            "authors": [
                "Mehdi Zekriyapanah Gashti",
                "Mostafa Mohammadpour",
                "Ghasem Farjamnia"
            ],
            "arxiv_id": "2510.17278v1",
            "summary": "Accurate segmentation and classification of white blood cells (WBCs) in\nmicroscopic images are essential for diagnosis and monitoring of many\nhematological disorders, yet remain challenging due to staining variability,\ncomplex backgrounds, and class imbalance. In this paper, we introduce a novel\nSaliency-Guided Cross-Layer Deep Feature Fusion framework (SG-CLDFF) that\ntightly integrates saliency-driven preprocessing with multi-scale deep feature\naggregation to improve both robustness and interpretability for WBC analysis.\nSG-CLDFF first computes saliency priors to highlight candidate WBC regions and\nguide subsequent feature extraction. A lightweight hybrid backbone\n(EfficientSwin-style) produces multi-resolution representations, which are\nfused by a ResNeXt-CC-inspired cross-layer fusion module to preserve\ncomplementary information from shallow and deep layers. The network is trained\nin a multi-task setup with concurrent segmentation and cell-type classification\nheads, using class-aware weighted losses and saliency-alignment regularization\nto mitigate imbalance and suppress background activation. Interpretability is\nenforced through Grad-CAM visualizations and saliency consistency checks,\nallowing model decisions to be inspected at the regional level. We validate the\nframework on standard public benchmarks (BCCD, LISC, ALL-IDB), reporting\nconsistent gains in IoU, F1, and classification accuracy compared to strong CNN\nand transformer baselines. An ablation study also demonstrates the individual\ncontributions of saliency preprocessing and cross-layer fusion. SG-CLDFF offers\na practical and explainable path toward more reliable automated WBC analysis in\nclinical workflows.",
            "headline_zh": "提出SG-CLDFF框架以改进白细胞分割与分类的鲁棒性和可解释性",
            "intro_zh": [
                "核心问题：白细胞图像分割与分类受染色变异、复杂背景和类别不平衡影响",
                "方法要点：结合显著性引导预处理与跨层深度特征融合，使用多任务训练",
                "实验或效果：在公开基准测试中IoU、F1和分类准确率优于基线模型"
            ],
            "tags_zh": [
                "白细胞分类",
                "图像分割",
                "显著性检测",
                "特征融合",
                "多任务学习",
                "可解释AI"
            ],
            "_index": 79
        },
        {
            "title": "Enhanced Motion Forecasting with Plug-and-Play Multimodal Large Language Models",
            "authors": [
                "Katie Luo",
                "Jingwei Ji",
                "Tong He",
                "Runsheng Xu",
                "Yichen Xie",
                "Dragomir Anguelov",
                "Mingxing Tan"
            ],
            "arxiv_id": "2510.17274v1",
            "summary": "Current autonomous driving systems rely on specialized models for perceiving\nand predicting motion, which demonstrate reliable performance in standard\nconditions. However, generalizing cost-effectively to diverse real-world\nscenarios remains a significant challenge. To address this, we propose\nPlug-and-Forecast (PnF), a plug-and-play approach that augments existing motion\nforecasting models with multimodal large language models (MLLMs). PnF builds on\nthe insight that natural language provides a more effective way to describe and\nhandle complex scenarios, enabling quick adaptation to targeted behaviors. We\ndesign prompts to extract structured scene understanding from MLLMs and distill\nthis information into learnable embeddings to augment existing behavior\nprediction models. Our method leverages the zero-shot reasoning capabilities of\nMLLMs to achieve significant improvements in motion prediction performance,\nwhile requiring no fine-tuning -- making it practical to adopt. We validate our\napproach on two state-of-the-art motion forecasting models using the Waymo Open\nMotion Dataset and the nuScenes Dataset, demonstrating consistent performance\nimprovements across both benchmarks.",
            "headline_zh": "提出Plug-and-Forecast方法，通过多模态大语言模型增强运动预测模型以处理复杂场景",
            "intro_zh": [
                "核心问题：现有自动驾驶运动预测模型在多样化真实场景中泛化成本高且效果有限",
                "方法要点：设计提示从MLLMs提取结构化场景理解，蒸馏为可学习嵌入以增强预测模型",
                "实验或效果：在Waymo和nuScenes数据集上验证，无需微调即显著提升预测性能"
            ],
            "tags_zh": [
                "运动预测",
                "多模态大语言模型",
                "蒸馏嵌入",
                "自动驾驶",
                "零样本推理"
            ],
            "_index": 80
        },
        {
            "title": "Floating-Base Deep Lagrangian Networks",
            "authors": [
                "Lucas Schulze",
                "Juliano Decico Negri",
                "Victor Barasuol",
                "Vivian Suzano Medeiros",
                "Marcelo Becker",
                "Jan Peters",
                "Oleg Arenz"
            ],
            "arxiv_id": "2510.17270v1",
            "summary": "Grey-box methods for system identification combine deep learning with\nphysics-informed constraints, capturing complex dependencies while improving\nout-of-distribution generalization. Yet, despite the growing importance of\nfloating-base systems such as humanoids and quadrupeds, current grey-box models\nignore their specific physical constraints. For instance, the inertia matrix is\nnot only positive definite but also exhibits branch-induced sparsity and input\nindependence. Moreover, the 6x6 composite spatial inertia of the floating base\ninherits properties of single-rigid-body inertia matrices. As we show, this\nincludes the triangle inequality on the eigenvalues of the composite rotational\ninertia. To address the lack of physical consistency in deep learning models of\nfloating-base systems, we introduce a parameterization of inertia matrices that\nsatisfies all these constraints. Inspired by Deep Lagrangian Networks (DeLaN),\nwe train neural networks to predict physically plausible inertia matrices that\nminimize inverse dynamics error under Lagrangian mechanics. For evaluation, we\ncollected and released a dataset on multiple quadrupeds and humanoids. In these\nexperiments, our Floating-Base Deep Lagrangian Networks (FeLaN) achieve highly\ncompetitive performance on both simulated and real robots, while providing\ngreater physical interpretability.",
            "headline_zh": "提出FeLaN以解决浮动基系统物理约束缺失问题",
            "intro_zh": [
                "核心问题：浮动基系统如人形和四足机器人缺乏物理一致的深度学习模型",
                "方法要点：基于Deep Lagrangian Networks，参数化满足约束的惯性矩阵",
                "实验或效果：在模拟和真实机器人上实现高性能和物理可解释性"
            ],
            "tags_zh": [
                "浮动基系统",
                "灰盒方法",
                "惯性矩阵参数化",
                "拉格朗日力学",
                "系统辨识",
                "物理约束"
            ],
            "_index": 81
        },
        {
            "title": "FineVision: Open Data Is All You Need",
            "authors": [
                "Luis Wiedmann",
                "Orr Zohar",
                "Amir Mahla",
                "Xiaohan Wang",
                "Rui Li",
                "Thibaud Frere",
                "Leandro von Werra",
                "Aritra Roy Gosthipaty",
                "Andrés Marafioti"
            ],
            "arxiv_id": "2510.17269v1",
            "summary": "The advancement of vision-language models (VLMs) is hampered by a fragmented\nlandscape of inconsistent and contaminated public datasets. We introduce\nFineVision, a meticulously collected, curated, and unified corpus of 24 million\nsamples - the largest open resource of its kind. We unify more than 200 sources\ninto 185 subsets via a semi-automated, human-in-the-loop pipeline: automation\nperforms bulk ingestion and schema mapping, while reviewers audit mappings and\nspot-check outputs to verify faithful consumption of annotations, appropriate\nformatting and diversity, and safety; issues trigger targeted fixes and\nre-runs. The workflow further applies rigorous de-duplication within and across\nsources and decontamination against 66 public benchmarks. FineVision also\nencompasses agentic/GUI tasks with a unified action space; reviewers validate\nschemas and inspect a sample of trajectories to confirm executable fidelity.\nModels trained on FineVision consistently outperform those trained on existing\nopen mixtures across a broad evaluation suite, underscoring the benefits of\nscale, data hygiene, and balanced automation with human oversight. We release\nthe corpus and curation tools to accelerate data-centric VLM research.",
            "headline_zh": "提出FineVision大规模开放数据集以解决视觉语言模型数据碎片化问题",
            "intro_zh": [
                "视觉语言模型发展受限于公共数据集不一致和污染问题",
                "通过半自动化流程统一200多源数据，并进行去重和去污染处理",
                "基于FineVision训练的模型在广泛评估中优于现有开放数据集模型"
            ],
            "tags_zh": [
                "视觉语言模型",
                "数据集构建",
                "数据去重",
                "数据去污染",
                "半自动化流程",
                "开放数据"
            ],
            "_index": 82
        },
        {
            "title": "Fair and Interpretable Deepfake Detection in Videos",
            "authors": [
                "Akihito Yoshii",
                "Ryosuke Sonoda",
                "Ramya Srinivasan"
            ],
            "arxiv_id": "2510.17264v1",
            "summary": "Existing deepfake detection methods often exhibit bias, lack transparency,\nand fail to capture temporal information, leading to biased decisions and\nunreliable results across different demographic groups. In this paper, we\npropose a fairness-aware deepfake detection framework that integrates temporal\nfeature learning and demographic-aware data augmentation to enhance fairness\nand interpretability. Our method leverages sequence-based clustering for\ntemporal modeling of deepfake videos and concept extraction to improve\ndetection reliability while also facilitating interpretable decisions for\nnon-expert users. Additionally, we introduce a demography-aware data\naugmentation method that balances underrepresented groups and applies\nfrequency-domain transformations to preserve deepfake artifacts, thereby\nmitigating bias and improving generalization. Extensive experiments on\nFaceForensics++, DFD, Celeb-DF, and DFDC datasets using state-of-the-art (SoTA)\narchitectures (Xception, ResNet) demonstrate the efficacy of the proposed\nmethod in obtaining the best tradeoff between fairness and accuracy when\ncompared to SoTA.",
            "headline_zh": "提出公平感知的深度伪造检测框架，集成时序特征学习和人口统计感知数据增强以提升公平性和可解释性。",
            "intro_zh": [
                "现有深度伪造检测方法存在偏见、缺乏透明度，且忽略时序信息，导致跨人口群体决策不可靠。",
                "方法结合序列聚类进行时序建模和概念提取，并引入人口统计感知数据增强以平衡群体和保留伪造伪影。",
                "在多个数据集上实验显示，该方法在公平性和准确性间取得最佳权衡，优于现有技术。"
            ],
            "tags_zh": [
                "深度伪造检测",
                "公平性增强",
                "时序建模",
                "数据增强",
                "可解释性",
                "人口统计感知"
            ],
            "_index": 83
        },
        {
            "title": "High-Level Multi-Robot Trajectory Planning And Spurious Behavior Detection",
            "authors": [
                "Fernando Salanova",
                "Jesús Roche",
                "Cristian Mahuela",
                "Eduardo Montijano"
            ],
            "arxiv_id": "2510.17261v1",
            "summary": "The reliable execution of high-level missions in multi-robot systems with\nheterogeneous agents, requires robust methods for detecting spurious behaviors.\nIn this paper, we address the challenge of identifying spurious executions of\nplans specified as a Linear Temporal Logic (LTL) formula, as incorrect task\nsequences, violations of spatial constraints, timing inconsis- tencies, or\ndeviations from intended mission semantics. To tackle this, we introduce a\nstructured data generation framework based on the Nets-within-Nets (NWN)\nparadigm, which coordinates robot actions with LTL-derived global mission\nspecifications. We further propose a Transformer-based anomaly detection\npipeline that classifies robot trajectories as normal or anomalous. Experi-\nmental evaluations show that our method achieves high accuracy (91.3%) in\nidentifying execution inefficiencies, and demonstrates robust detection\ncapabilities for core mission violations (88.3%) and constraint-based adaptive\nanomalies (66.8%). An ablation experiment of the embedding and architecture was\ncarried out, obtaining successful results where our novel proposition performs\nbetter than simpler representations.",
            "headline_zh": "提出基于NWN和Transformer的框架以检测多机器人系统中的虚假行为",
            "intro_zh": [
                "核心问题：多机器人系统中LTL计划执行时的虚假行为检测，如任务序列错误和约束违反。",
                "方法要点：使用Nets-within-Nets框架协调机器人动作，并采用Transformer进行异常分类。",
                "实验或效果：实验显示高准确率，虚假行为检测达91.3%，核心任务违反检测为88.3%。"
            ],
            "tags_zh": [
                "多机器人系统",
                "轨迹规划",
                "虚假行为检测",
                "线性时序逻辑",
                "Transformer模型",
                "异常检测"
            ],
            "_index": 84
        },
        {
            "title": "An adaptive hierarchical control framework for quadrupedal robots in planetary exploration",
            "authors": [
                "Franek Stark",
                "Rohit Kumar",
                "Shubham Vyas",
                "Hannah Isermann",
                "Jonas Haack",
                "Mihaela Popescu",
                "Jakob Middelberg",
                "Dennis Mronga",
                "Frank Kirchner"
            ],
            "arxiv_id": "2510.17249v1",
            "summary": "Planetary exploration missions require robots capable of navigating extreme\nand unknown environments. While wheeled rovers have dominated past missions,\ntheir mobility is limited to traversable surfaces. Legged robots, especially\nquadrupeds, can overcome these limitations by handling uneven, obstacle-rich,\nand deformable terrains. However, deploying such robots in unknown conditions\nis challenging due to the need for environment-specific control, which is\ninfeasible when terrain and robot parameters are uncertain. This work presents\na modular control framework that combines model-based dynamic control with\nonline model adaptation and adaptive footstep planning to address uncertainties\nin both robot and terrain properties. The framework includes state estimation\nfor quadrupeds with and without contact sensing, supports runtime\nreconfiguration, and is integrated into ROS 2 with open-source availability.\nIts performance was validated on two quadruped platforms, multiple hardware\narchitectures, and in a volcano field test, where the robot walked over 700 m.",
            "headline_zh": "提出自适应分层控制框架以解决四足机器人在未知行星探索中的不确定性问题",
            "intro_zh": [
                "核心问题：未知环境和机器人参数不确定性限制四足机器人在行星探索中的部署。",
                "方法要点：结合模型动态控制、在线模型适应和自适应脚步规划，支持状态估计和运行时重配置。",
                "实验或效果：在多个平台和火山实地测试中验证，机器人行走超过700米。"
            ],
            "tags_zh": [
                "四足机器人控制",
                "行星探索",
                "自适应控制",
                "模型适应",
                "脚步规划",
                "ROS 2集成"
            ],
            "_index": 85
        },
        {
            "title": "From Preferences to Prejudice: The Role of Alignment Tuning in Shaping Social Bias in Video Diffusion Models",
            "authors": [
                "Zefan Cai",
                "Haoyi Qiu",
                "Haozhe Zhao",
                "Ke Wan",
                "Jiachen Li",
                "Jiuxiang Gu",
                "Wen Xiao",
                "Nanyun Peng",
                "Junjie Hu"
            ],
            "arxiv_id": "2510.17247v1",
            "summary": "Recent advances in video diffusion models have significantly enhanced\ntext-to-video generation, particularly through alignment tuning using reward\nmodels trained on human preferences. While these methods improve visual\nquality, they can unintentionally encode and amplify social biases. To\nsystematically trace how such biases evolve throughout the alignment pipeline,\nwe introduce VideoBiasEval, a comprehensive diagnostic framework for evaluating\nsocial representation in video generation. Grounded in established social bias\ntaxonomies, VideoBiasEval employs an event-based prompting strategy to\ndisentangle semantic content (actions and contexts) from actor attributes\n(gender and ethnicity). It further introduces multi-granular metrics to\nevaluate (1) overall ethnicity bias, (2) gender bias conditioned on ethnicity,\n(3) distributional shifts in social attributes across model variants, and (4)\nthe temporal persistence of bias within videos. Using this framework, we\nconduct the first end-to-end analysis connecting biases in human preference\ndatasets, their amplification in reward models, and their propagation through\nalignment-tuned video diffusion models. Our results reveal that alignment\ntuning not only strengthens representational biases but also makes them\ntemporally stable, producing smoother yet more stereotyped portrayals. These\nfindings highlight the need for bias-aware evaluation and mitigation throughout\nthe alignment process to ensure fair and socially responsible video generation.",
            "headline_zh": "提出VideoBiasEval框架以评估视频扩散模型对齐调优中的社会偏见",
            "intro_zh": [
                "对齐调优在提升视频生成质量时可能无意中编码和放大社会偏见",
                "引入VideoBiasEval框架，基于事件提示和多粒度指标系统评估偏见",
                "实验显示对齐调优强化偏见并使其在视频中时间稳定"
            ],
            "tags_zh": [
                "视频扩散模型",
                "对齐调优",
                "社会偏见评估",
                "偏见放大",
                "多粒度指标",
                "时间稳定性"
            ],
            "_index": 86
        },
        {
            "title": "Pole-Image: A Self-Supervised Pole-Anchored Descriptor for Long-Term LiDAR Localization and Map Maintenance",
            "authors": [
                "Wuhao Xie",
                "Kanji Tanaka"
            ],
            "arxiv_id": "2510.17237v1",
            "summary": "Long-term autonomy for mobile robots requires both robust self-localization\nand reliable map maintenance. Conventional landmark-based methods face a\nfundamental trade-off between landmarks with high detectability but low\ndistinctiveness (e.g., poles) and those with high distinctiveness but difficult\nstable detection (e.g., local point cloud structures). This work addresses the\nchallenge of descriptively identifying a unique \"signature\" (local point cloud)\nby leveraging a detectable, high-precision \"anchor\" (like a pole). To solve\nthis, we propose a novel canonical representation, \"Pole-Image,\" as a hybrid\nmethod that uses poles as anchors to generate signatures from the surrounding\n3D structure. Pole-Image represents a pole-like landmark and its surrounding\nenvironment, detected from a LiDAR point cloud, as a 2D polar coordinate image\nwith the pole itself as the origin. This representation leverages the pole's\nnature as a high-precision reference point, explicitly encoding the \"relative\ngeometry\" between the stable pole and the variable surrounding point cloud. The\nkey advantage of pole landmarks is that \"detection\" is extremely easy. This\nease of detection allows the robot to easily track the same pole, enabling the\nautomatic and large-scale collection of diverse observational data (positive\npairs). This data acquisition feasibility makes \"Contrastive Learning (CL)\"\napplicable. By applying CL, the model learns a viewpoint-invariant and highly\ndiscriminative descriptor. The contributions are twofold: 1) The descriptor\novercomes perceptual aliasing, enabling robust self-localization. 2) The\nhigh-precision encoding enables high-sensitivity change detection, contributing\nto map maintenance.",
            "headline_zh": "提出Pole-Image方法以解决长期LiDAR定位与地图维护中的地标描述问题",
            "intro_zh": [
                "核心问题：传统地标方法在可检测性与独特性间存在权衡，难以稳定识别独特签名",
                "方法要点：使用杆状地标作为锚点，生成2D极坐标图像编码相对几何，应用对比学习训练描述符",
                "实验或效果：描述符克服感知混淆，实现鲁棒定位；高精度编码支持高灵敏度变化检测，促进地图维护"
            ],
            "tags_zh": [
                "LiDAR定位",
                "自监督学习",
                "地标描述",
                "对比学习",
                "地图维护",
                "杆状锚点"
            ],
            "_index": 87
        },
        {
            "title": "Taming Modality Entanglement in Continual Audio-Visual Segmentation",
            "authors": [
                "Yuyang Hong",
                "Qi Yang",
                "Tao Zhang",
                "Zili Wang",
                "Zhaojin Fu",
                "Kun Ding",
                "Bin Fan",
                "Shiming Xiang"
            ],
            "arxiv_id": "2510.17234v1",
            "summary": "Recently, significant progress has been made in multi-modal continual\nlearning, aiming to learn new tasks sequentially in multi-modal settings while\npreserving performance on previously learned ones. However, existing methods\nmainly focus on coarse-grained tasks, with limitations in addressing modality\nentanglement in fine-grained continual learning settings. To bridge this gap,\nwe introduce a novel Continual Audio-Visual Segmentation (CAVS) task, aiming to\ncontinuously segment new classes guided by audio. Through comprehensive\nanalysis, two critical challenges are identified: 1) multi-modal semantic\ndrift, where a sounding objects is labeled as background in sequential tasks;\n2) co-occurrence confusion, where frequent co-occurring classes tend to be\nconfused. In this work, a Collision-based Multi-modal Rehearsal (CMR) framework\nis designed to address these challenges. Specifically, for multi-modal semantic\ndrift, a Multi-modal Sample Selection (MSS) strategy is proposed to select\nsamples with high modal consistency for rehearsal. Meanwhile, for co-occurence\nconfusion, a Collision-based Sample Rehearsal (CSR) mechanism is designed,\nallowing for the increase of rehearsal sample frequency of those confusable\nclasses during training process. Moreover, we construct three audio-visual\nincremental scenarios to verify effectiveness of our method. Comprehensive\nexperiments demonstrate that our method significantly outperforms single-modal\ncontinual learning methods.",
            "headline_zh": "提出碰撞式多模态排练框架以解决持续音频-视觉分割中的模态纠缠问题",
            "intro_zh": [
                "核心问题：模态纠缠导致多模态语义漂移和共现混淆，影响细粒度持续学习。",
                "方法要点：设计多模态样本选择和碰撞式样本排练机制，增强模态一致性和减少混淆。",
                "实验或效果：在三个音频-视觉增量场景中验证，显著优于单模态持续学习方法。"
            ],
            "tags_zh": [
                "持续学习",
                "多模态分割",
                "音频-视觉",
                "模态纠缠",
                "样本排练",
                "语义漂移"
            ],
            "_index": 88
        },
        {
            "title": "When One Moment Isn't Enough: Multi-Moment Retrieval with Cross-Moment Interactions",
            "authors": [
                "Zhuo Cao",
                "Heming Du",
                "Bingqing Zhang",
                "Xin Yu",
                "Xue Li",
                "Sen Wang"
            ],
            "arxiv_id": "2510.17218v1",
            "summary": "Existing Moment retrieval (MR) methods focus on Single-Moment Retrieval\n(SMR). However, one query can correspond to multiple relevant moments in\nreal-world applications. This makes the existing datasets and methods\ninsufficient for video temporal grounding. By revisiting the gap between\ncurrent MR tasks and real-world applications, we introduce a high-quality\ndatasets called QVHighlights Multi-Moment Dataset (QV-M$^2$), along with new\nevaluation metrics tailored for multi-moment retrieval (MMR). QV-M$^2$ consists\nof 2,212 annotations covering 6,384 video segments. Building on existing\nefforts in MMR, we propose a framework called FlashMMR. Specifically, we\npropose a Multi-moment Post-verification module to refine the moment\nboundaries. We introduce constrained temporal adjustment and subsequently\nleverage a verification module to re-evaluate the candidate segments. Through\nthis sophisticated filtering pipeline, low-confidence proposals are pruned, and\nrobust multi-moment alignment is achieved. We retrain and evaluate 6 existing\nMR methods on QV-M$^2$ and QVHighlights under both SMR and MMR settings.\nResults show that QV-M$^2$ serves as an effective benchmark for training and\nevaluating MMR models, while FlashMMR provides a strong baseline. Specifically,\non QV-M$^2$, it achieves improvements over prior SOTA method by 3.00% on G-mAP,\n2.70% on mAP@3+tgt, and 2.56% on mR@3. The proposed benchmark and method\nestablish a foundation for advancing research in more realistic and challenging\nvideo temporal grounding scenarios. Code is released at\nhttps://github.com/Zhuo-Cao/QV-M2.",
            "headline_zh": "提出FlashMMR框架与QV-M²数据集以解决视频多时刻检索问题",
            "intro_zh": [
                "核心问题：现有单时刻检索方法无法处理查询对应多个相关时刻的现实场景",
                "方法要点：引入多时刻后验证模块，通过约束时间调整和验证优化时刻边界",
                "实验或效果：在QV-M²数据集上，FlashMMR在多个指标上优于现有方法，提升达3.00%"
            ],
            "tags_zh": [
                "多时刻检索",
                "视频时序定位",
                "数据集构建",
                "后验证模块",
                "基准评估"
            ],
            "_index": 89
        },
        {
            "title": "$\\mathcal{V}isi\\mathcal{P}runer$: Decoding Discontinuous Cross-Modal Dynamics for Efficient Multimodal LLMs",
            "authors": [
                "Yingqi Fan",
                "Anhao Zhao",
                "Jinlan Fu",
                "Junlong Tong",
                "Hui Su",
                "Yijie Pan",
                "Wei Zhang",
                "Xiaoyu Shen"
            ],
            "arxiv_id": "2510.17205v1",
            "summary": "Multimodal Large Language Models (MLLMs) have achieved strong performance\nacross vision-language tasks, but suffer from significant computational\noverhead due to the quadratic growth of attention computations with the number\nof multimodal tokens. Though efforts have been made to prune tokens in MLLMs,\n\\textit{they lack a fundamental understanding of how MLLMs process and fuse\nmultimodal information.} Through systematic analysis, we uncover a\n\\textbf{three-stage} cross-modal interaction process: (1) Shallow layers\nrecognize task intent, with visual tokens acting as passive attention sinks;\n(2) Cross-modal fusion occurs abruptly in middle layers, driven by a few\ncritical visual tokens; (3) Deep layers discard vision tokens, focusing solely\non linguistic refinement. Based on these findings, we propose\n\\emph{VisiPruner}, a training-free pruning framework that reduces up to 99\\% of\nvision-related attention computations and 53.9\\% of FLOPs on LLaVA-v1.5 7B. It\nsignificantly outperforms existing token pruning methods and generalizes across\ndiverse MLLMs. Beyond pruning, our insights further provide actionable\nguidelines for training efficient MLLMs by aligning model architecture with its\nintrinsic layer-wise processing dynamics. Our code is available at:\nhttps://github.com/EIT-NLP/VisiPruner.",
            "headline_zh": "提出VisiPruner以高效减少多模态大语言模型的计算开销",
            "intro_zh": [
                "多模态大语言模型计算开销大，源于注意力计算随多模态令牌数二次增长",
                "基于三阶段跨模态交互分析，设计无需训练的剪枝框架，移除冗余视觉令牌",
                "在LLaVA-v1.5 7B上减少99%视觉相关注意力计算和53.9% FLOPs，优于现有方法"
            ],
            "tags_zh": [
                "多模态大语言模型",
                "令牌剪枝",
                "跨模态交互",
                "计算效率",
                "注意力机制",
                "模型优化"
            ],
            "_index": 90
        },
        {
            "title": "Performance Evaluation of an Integrated System for Visible Light Communication and Positioning Using an Event Camera",
            "authors": [
                "Ryota Soga",
                "Masataka Kobayashi",
                "Tsukasa Shimizu",
                "Shintaro Shiba",
                "Quan Kong",
                "Shan Lu",
                "Takaya Yamazato"
            ],
            "arxiv_id": "2510.17203v1",
            "summary": "Event cameras, featuring high temporal resolution and high dynamic range,\noffer visual sensing capabilities comparable to conventional image sensors\nwhile capturing fast-moving objects and handling scenes with extreme lighting\ncontrasts such as tunnel exits. Leveraging these properties, this study\nproposes a novel self-localization system that integrates visible light\ncommunication (VLC) and visible light positioning (VLP) within a single event\ncamera. The system enables a vehicle to estimate its position even in\nGPS-denied environments, such as tunnels, by using VLC to obtain coordinate\ninformation from LED transmitters and VLP to estimate the distance to each\ntransmitter.\n  Multiple LEDs are installed on the transmitter side, each assigned a unique\npilot sequence based on Walsh-Hadamard codes. The event camera identifies\nindividual LEDs within its field of view by correlating the received signal\nwith these codes, allowing clear separation and recognition of each light\nsource. This mechanism enables simultaneous high-capacity MISO (multi-input\nsingle-output) communication through VLC and precise distance estimation via\nphase-only correlation (POC) between multiple LED pairs.\n  To the best of our knowledge, this is the first vehicle-mounted system to\nachieve simultaneous VLC and VLP functionalities using a single event camera.\nField experiments were conducted by mounting the system on a vehicle traveling\nat 30 km/h (8.3 m/s). The results demonstrated robust real-world performance,\nwith a root mean square error (RMSE) of distance estimation within 0.75 m for\nranges up to 100 m and a bit error rate (BER) below 0.01 across the same range.",
            "headline_zh": "提出集成事件相机的可见光通信与定位系统，用于GPS缺失环境下的车辆自定位。",
            "intro_zh": [
                "核心问题：车辆在GPS缺失环境（如隧道）中难以实现高精度自定位。",
                "方法要点：使用事件相机结合Walsh-Hadamard码识别LED，实现VLC和VLP集成。",
                "实验或效果：车辆以30 km/h行驶，距离估计RMSE≤0.75 m，BER<0.01。"
            ],
            "tags_zh": [
                "事件相机",
                "可见光通信",
                "可见光定位",
                "车辆自定位",
                "Walsh-Hadamard码",
                "相位相关"
            ],
            "_index": 91
        },
        {
            "title": "Optimizing DINOv2 with Registers for Face Anti-Spoofing",
            "authors": [
                "Mika Feng",
                "Pierre Gallin-Martel",
                "Koichi Ito",
                "Takafumi Aoki"
            ],
            "arxiv_id": "2510.17201v1",
            "summary": "Face recognition systems are designed to be robust against variations in head\npose, illumination, and image blur during capture. However, malicious actors\ncan exploit these systems by presenting a face photo of a registered user,\npotentially bypassing the authentication process. Such spoofing attacks must be\ndetected prior to face recognition. In this paper, we propose a DINOv2-based\nspoofing attack detection method to discern minute differences between live and\nspoofed face images. Specifically, we employ DINOv2 with registers to extract\ngeneralizable features and to suppress perturbations in the attention\nmechanism, which enables focused attention on essential and minute features. We\ndemonstrate the effectiveness of the proposed method through experiments\nconducted on the dataset provided by ``The 6th Face Anti-Spoofing Workshop:\nUnified Physical-Digital Attacks Detection@ICCV2025'' and SiW dataset.",
            "headline_zh": "提出基于DINOv2与寄存器的面部反欺骗方法，以检测活体与欺骗图像的细微差异。",
            "intro_zh": [
                "核心问题：面部识别系统易受照片欺骗攻击，需在识别前检测活体与欺骗图像。",
                "方法要点：使用DINOv2与寄存器提取泛化特征，抑制注意力机制中的扰动。",
                "实验或效果：在ICCV2025工作坊数据集和SiW数据集上验证方法有效性。"
            ],
            "tags_zh": [
                "面部反欺骗",
                "DINOv2",
                "注意力机制",
                "寄存器优化",
                "活体检测"
            ],
            "_index": 92
        },
        {
            "title": "EndoCIL: A Class-Incremental Learning Framework for Endoscopic Image Classification",
            "authors": [
                "Bingrong Liu",
                "Jun Shi",
                "Yushan Zheng"
            ],
            "arxiv_id": "2510.17200v1",
            "summary": "Class-incremental learning (CIL) for endoscopic image analysis is crucial for\nreal-world clinical applications, where diagnostic models should continuously\nadapt to evolving clinical data while retaining performance on previously\nlearned ones. However, existing replay-based CIL methods fail to effectively\nmitigate catastrophic forgetting due to severe domain discrepancies and class\nimbalance inherent in endoscopic imaging. To tackle these challenges, we\npropose EndoCIL, a novel and unified CIL framework specifically tailored for\nendoscopic image diagnosis. EndoCIL incorporates three key components: Maximum\nMean Discrepancy Based Replay (MDBR), employing a distribution-aligned greedy\nstrategy to select diverse and representative exemplars, Prior Regularized\nClass Balanced Loss (PRCBL), designed to alleviate both inter-phase and\nintra-phase class imbalance by integrating prior class distributions and\nbalance weights into the loss function, and Calibration of Fully-Connected\nGradients (CFG), which adjusts the classifier gradients to mitigate bias toward\nnew classes. Extensive experiments conducted on four public endoscopic datasets\ndemonstrate that EndoCIL generally outperforms state-of-the-art CIL methods\nacross varying buffer sizes and evaluation metrics. The proposed framework\neffectively balances stability and plasticity in lifelong endoscopic diagnosis,\nshowing promising potential for clinical scalability and deployment.",
            "headline_zh": "提出EndoCIL框架以解决内窥镜图像分类中的类增量学习问题",
            "intro_zh": [
                "核心问题：现有重放方法因领域差异和类不平衡导致灾难性遗忘",
                "方法要点：集成分布对齐重放、先验正则化损失和梯度校准组件",
                "实验或效果：在四个数据集上优于先进方法，平衡稳定性和可塑性"
            ],
            "tags_zh": [
                "类增量学习",
                "内窥镜图像分类",
                "灾难性遗忘",
                "分布对齐",
                "类不平衡",
                "梯度校准"
            ],
            "_index": 93
        },
        {
            "title": "Round Outcome Prediction in VALORANT Using Tactical Features from Video Analysis",
            "authors": [
                "Nirai Hayakawa",
                "Kazumasa Shimari",
                "Kazuma Yamasaki",
                "Hirotatsu Hoshikawa",
                "Rikuto Tsuchida",
                "Kenichi Matsumoto"
            ],
            "arxiv_id": "2510.17199v1",
            "summary": "Recently, research on predicting match outcomes in esports has been actively\nconducted, but much of it is based on match log data and statistical\ninformation. This research targets the FPS game VALORANT, which requires\ncomplex strategies, and aims to build a round outcome prediction model by\nanalyzing minimap information in match footage. Specifically, based on the\nvideo recognition model TimeSformer, we attempt to improve prediction accuracy\nby incorporating detailed tactical features extracted from minimap information,\nsuch as character position information and other in-game events. This paper\nreports preliminary results showing that a model trained on a dataset augmented\nwith such tactical event labels achieved approximately 81% prediction accuracy,\nespecially from the middle phases of a round onward, significantly\noutperforming a model trained on a dataset with the minimap information itself.\nThis suggests that leveraging tactical features from match footage is highly\neffective for predicting round outcomes in VALORANT.",
            "headline_zh": "提出基于视频战术特征的模型以预测VALORANT回合结果",
            "intro_zh": [
                "核心问题：现有电竞预测多依赖日志数据，缺乏复杂策略分析。",
                "方法要点：使用TimeSformer从小地图提取位置和事件特征，增强预测模型。",
                "实验效果：模型在回合中后期预测准确率达81%，优于仅用小地图信息。"
            ],
            "tags_zh": [
                "回合结果预测",
                "视频分析",
                "战术特征提取",
                "TimeSformer模型",
                "电竞数据分析"
            ],
            "_index": 94
        },
        {
            "title": "From Pixels to People: Satellite-Based Mapping and Quantification of Riverbank Erosion and Lost Villages in Bangladesh",
            "authors": [
                "M Saifuzzaman Rafat",
                "Mohd Ruhul Ameen",
                "Akif Islam",
                "Abu Saleh Musa Miah",
                "Jungpil Shin"
            ],
            "arxiv_id": "2510.17198v1",
            "summary": "The great rivers of Bangladesh, arteries of commerce and sustenance, are also\nagents of relentless destruction. Each year, they swallow whole villages and\nvast tracts of farmland, erasing communities from the map and displacing\nthousands of families. To track this slow-motion catastrophe has, until now,\nbeen a Herculean task for human analysts. Here we show how a powerful\ngeneral-purpose vision model, the Segment Anything Model (SAM), can be adapted\nto this task with remarkable precision. To do this, we assembled a new dataset\n- a digital chronicle of loss compiled from historical Google Earth imagery of\nBangladesh's most vulnerable regions, including Mokterer Char Union, Kedarpur\nUnion, Balchipara village, and Chowhali Upazila, from 2003 to 2025. Crucially,\nthis dataset is the first to include manually annotated data on the settlements\nthat have vanished beneath the water. Our method first uses a simple\ncolor-channel analysis to provide a rough segmentation of land and water, and\nthen fine-tunes SAM's mask decoder to recognize the subtle signatures of\nriverbank erosion. The resulting model demonstrates a keen eye for this\ndestructive process, achieving a mean Intersection over Union of 86.30% and a\nDice score of 92.60% - a performance that significantly surpasses traditional\nmethods and off-the-shelf deep learning models. This work delivers three key\ncontributions: the first annotated dataset of disappeared settlements in\nBangladesh due to river erosion; a specialized AI model fine-tuned for this\ncritical task; and a method for quantifying land loss with compelling visual\nevidence. Together, these tools provide a powerful new lens through which\npolicymakers and disaster management agencies can monitor erosion, anticipate\nits trajectory, and ultimately protect the vulnerable communities in its path.",
            "headline_zh": "提出基于SAM的河流侵蚀监测方法，用于孟加拉国村庄损失量化",
            "intro_zh": [
                "核心问题：孟加拉国河流侵蚀导致村庄消失和土地损失，传统监测困难。",
                "方法要点：结合颜色通道分析和微调SAM掩码解码器，识别侵蚀特征。",
                "实验或效果：在自定义数据集上，mIoU达86.30%，Dice分数92.60%，优于传统方法。"
            ],
            "tags_zh": [
                "河流侵蚀监测",
                "卫星图像分割",
                "Segment Anything模型",
                "土地损失量化",
                "孟加拉国数据集"
            ],
            "_index": 95
        },
        {
            "title": "ZSPAPrune: Zero-Shot Prompt-Aware Token Pruning for Vision-Language Models",
            "authors": [
                "Pu Zhang",
                "Yuwei Li",
                "Xingyuan Xian",
                "Guoming Tang"
            ],
            "arxiv_id": "2510.17197v1",
            "summary": "As the capabilities of Vision-Language Models (VLMs) advance, they can\nprocess increasingly large inputs, which, unlike in LLMs, generates significant\nvisual token redundancy and leads to prohibitive inference costs. While many\nmethods aim to reduce these costs by pruning visual tokens, existing\napproaches, whether based on attention or diversity, typically neglect the\nguidance of the text prompt and thus fail to prioritize task relevance. In this\nwork, we propose a novel, zero-shot method that reframes the problem by\nintroducing a prompt-aware perspective, explicitly modeling visual token\npruning as a balance between task relevance and information diversity. Our\nhierarchical approach first selects a core set of task-relevant visual tokens\nand then supplements them with diversity tokens to preserve broader context.\nExperiments across multiple models and benchmarks show that our method achieves\nperformance that matches or surpasses the state-of-the-art with only minimal\naccuracy loss, even when pruning up to 90\\% of the tokens. Furthermore, these\ngains are accompanied by significant reductions in GPU memory footprint and\ninference latency.",
            "headline_zh": "提出零样本提示感知令牌剪枝方法以降低视觉语言模型推理成本",
            "intro_zh": [
                "视觉语言模型输入大导致视觉令牌冗余，增加推理开销",
                "基于提示感知建模剪枝，平衡任务相关性与信息多样性",
                "实验显示剪枝90%令牌时性能接近最优，显著减少内存和延迟"
            ],
            "tags_zh": [
                "视觉语言模型",
                "令牌剪枝",
                "零样本学习",
                "推理优化",
                "提示感知"
            ],
            "_index": 96
        },
        {
            "title": "SimpleVSF: VLM-Scoring Fusion for Trajectory Prediction of End-to-End Autonomous Driving",
            "authors": [
                "Peiru Zheng",
                "Yun Zhao",
                "Zhan Gong",
                "Hong Zhu",
                "Shaohua Wu"
            ],
            "arxiv_id": "2510.17191v1",
            "summary": "End-to-end autonomous driving has emerged as a promising paradigm for\nachieving robust and intelligent driving policies. However, existing end-to-end\nmethods still face significant challenges, such as suboptimal decision-making\nin complex scenarios. In this paper,we propose SimpleVSF (Simple VLM-Scoring\nFusion), a novel framework that enhances end-to-end planning by leveraging the\ncognitive capabilities of Vision-Language Models (VLMs) and advanced trajectory\nfusion techniques. We utilize the conventional scorers and the novel\nVLM-enhanced scorers. And we leverage a robust weight fusioner for quantitative\naggregation and a powerful VLM-based fusioner for qualitative, context-aware\ndecision-making. As the leading approach in the ICCV 2025 NAVSIM v2 End-to-End\nDriving Challenge, our SimpleVSF framework demonstrates state-of-the-art\nperformance, achieving a superior balance between safety, comfort, and\nefficiency.",
            "headline_zh": "提出SimpleVSF框架，融合VLM评分以提升端到端自动驾驶轨迹预测性能",
            "intro_zh": [
                "端到端自动驾驶在复杂场景中决策不佳，面临安全与效率挑战",
                "结合传统评分器与VLM增强评分器，通过权重和VLM融合器进行定量与定性决策",
                "在ICCV 2025挑战中表现领先，实现安全、舒适和效率的平衡"
            ],
            "tags_zh": [
                "端到端自动驾驶",
                "轨迹预测",
                "视觉语言模型",
                "评分融合",
                "决策优化"
            ],
            "_index": 97
        },
        {
            "title": "HIDISC: A Hyperbolic Framework for Domain Generalization with Generalized Category Discovery",
            "authors": [
                "Vaibhav Rathore",
                "Divyam Gupta",
                "Biplab Banerjee"
            ],
            "arxiv_id": "2510.17188v1",
            "summary": "Generalized Category Discovery (GCD) aims to classify test-time samples into\neither seen categories** -- available during training -- or novel ones, without\nrelying on label supervision. Most existing GCD methods assume simultaneous\naccess to labeled and unlabeled data during training and arising from the same\ndomain, limiting applicability in open-world scenarios involving distribution\nshifts. Domain Generalization with GCD (DG-GCD) lifts this constraint by\nrequiring models to generalize to unseen domains containing novel categories,\nwithout accessing targetdomain data during training. The only prior DG-GCD\nmethod, DG2CD-Net, relies on episodic training with multiple synthetic domains\nand task vector aggregation, incurring high computational cost and error\naccumulation. We propose HIDISC, a hyperbolic representation learning framework\nthat achieves domain and category-level generalization without episodic\nsimulation. To expose the model to minimal but diverse domain variations, we\naugment the source domain using GPT-guided diffusion, avoiding overfitting\nwhile maintaining efficiency. To structure the representation space, we\nintroduce Tangent CutMix, a curvature-aware interpolation that synthesizes\npseudo-novel samples in tangent space, preserving manifold consistency. A\nunified loss -- combining penalized Busemann alignment, hybrid hyperbolic\ncontrastive regularization, and adaptive outlier repulsion -- **facilitates\ncompact, semantically structured embeddings. A learnable curvature parameter\nfurther adapts the geometry to dataset complexity. HIDISC achieves\nstate-of-the-art results on PACS , Office-Home , and DomainNet, consistently\noutperforming the existing Euclidean and hyperbolic (DG)-GCD baselines.",
            "headline_zh": "提出HIDISC双曲框架以解决域泛化与广义类别发现问题",
            "intro_zh": [
                "核心问题：域泛化与广义类别发现结合，需在未见域中分类已知和未知类别",
                "方法要点：使用双曲表示学习、GPT引导扩散增强和切线空间插值",
                "实验或效果：在PACS等数据集上优于现有方法，实现高效泛化"
            ],
            "tags_zh": [
                "域泛化",
                "广义类别发现",
                "双曲表示学习",
                "扩散增强",
                "切线插值",
                "对比学习"
            ],
            "_index": 98
        },
        {
            "title": "Capturing Head Avatar with Hand Contacts from a Monocular Video",
            "authors": [
                "Haonan He",
                "Yufeng Zheng",
                "Jie Song"
            ],
            "arxiv_id": "2510.17181v1",
            "summary": "Photorealistic 3D head avatars are vital for telepresence, gaming, and VR.\nHowever, most methods focus solely on facial regions, ignoring natural\nhand-face interactions, such as a hand resting on the chin or fingers gently\ntouching the cheek, which convey cognitive states like pondering. In this work,\nwe present a novel framework that jointly learns detailed head avatars and the\nnon-rigid deformations induced by hand-face interactions.\n  There are two principal challenges in this task. First, naively tracking hand\nand face separately fails to capture their relative poses. To overcome this, we\npropose to combine depth order loss with contact regularization during pose\ntracking, ensuring correct spatial relationships between the face and hand.\nSecond, no publicly available priors exist for hand-induced deformations,\nmaking them non-trivial to learn from monocular videos. To address this, we\nlearn a PCA basis specific to hand-induced facial deformations from a face-hand\ninteraction dataset. This reduces the problem to estimating a compact set of\nPCA parameters rather than a full spatial deformation field. Furthermore,\ninspired by physics-based simulation, we incorporate a contact loss that\nprovides additional supervision, significantly reducing interpenetration\nartifacts and enhancing the physical plausibility of the results.\n  We evaluate our approach on RGB(D) videos captured by an iPhone.\nAdditionally, to better evaluate the reconstructed geometry, we construct a\nsynthetic dataset of avatars with various types of hand interactions. We show\nthat our method can capture better appearance and more accurate deforming\ngeometry of the face than SOTA surface reconstruction methods.",
            "headline_zh": "提出联合学习头部化身与手-脸交互变形框架，以解决单目视频中自然交互建模问题",
            "intro_zh": [
                "核心问题：现有方法忽略手-脸交互，导致无法捕捉认知状态如沉思时的自然变形",
                "方法要点：结合深度顺序损失与接触正则化进行姿态跟踪，并学习手诱导面部变形的PCA基",
                "实验或效果：在iPhone视频和合成数据集上评估，优于SOTA方法，减少穿插伪影"
            ],
            "tags_zh": [
                "头部化身建模",
                "手-脸交互",
                "单目视频重建",
                "非刚性变形",
                "PCA基学习",
                "接触损失"
            ],
            "_index": 99
        },
        {
            "title": "Benchmarking Out-of-Distribution Detection for Plankton Recognition: A Systematic Evaluation of Advanced Methods in Marine Ecological Monitoring",
            "authors": [
                "Yingzi Han",
                "Jiakai He",
                "Chuanlong Xie",
                "Jianping Li"
            ],
            "arxiv_id": "2510.17179v1",
            "summary": "Automated plankton recognition models face significant challenges during\nreal-world deployment due to distribution shifts (Out-of-Distribution, OoD)\nbetween training and test data. This stems from plankton's complex\nmorphologies, vast species diversity, and the continuous discovery of novel\nspecies, which leads to unpredictable errors during inference. Despite rapid\nadvancements in OoD detection methods in recent years, the field of plankton\nrecognition still lacks a systematic integration of the latest computer vision\ndevelopments and a unified benchmark for large-scale evaluation. To address\nthis, this paper meticulously designed a series of OoD benchmarks simulating\nvarious distribution shift scenarios based on the DYB-PlanktonNet dataset\n\\cite{875n-f104-21}, and systematically evaluated twenty-two OoD detection\nmethods. Extensive experimental results demonstrate that the ViM\n\\cite{wang2022vim} method significantly outperforms other approaches in our\nconstructed benchmarks, particularly excelling in Far-OoD scenarios with\nsubstantial improvements in key metrics. This comprehensive evaluation not only\nprovides a reliable reference for algorithm selection in automated plankton\nrecognition but also lays a solid foundation for future research in plankton\nOoD detection. To our knowledge, this study marks the first large-scale,\nsystematic evaluation and analysis of Out-of-Distribution data detection\nmethods in plankton recognition. Code is available at\nhttps://github.com/BlackJack0083/PlanktonOoD.",
            "headline_zh": "系统评估浮游生物识别中的OoD检测方法，ViM在远分布偏移场景表现最佳",
            "intro_zh": [
                "浮游生物识别模型面临训练与测试数据分布偏移问题，导致部署时错误",
                "基于DYB-PlanktonNet数据集构建OoD基准，评估22种检测方法",
                "ViM方法在远分布偏移场景显著优于其他，提升关键指标"
            ],
            "tags_zh": [
                "浮游生物识别",
                "分布外检测",
                "基准评估",
                "计算机视觉",
                "海洋生态监测"
            ],
            "_index": 100
        },
        {
            "title": "Generation then Reconstruction: Accelerating Masked Autoregressive Models via Two-Stage Sampling",
            "authors": [
                "Feihong Yan",
                "Peiru Wang",
                "Yao Zhu",
                "Kaiyu Pang",
                "Qingyan Wei",
                "Huiqi Li",
                "Linfeng Zhang"
            ],
            "arxiv_id": "2510.17171v1",
            "summary": "Masked Autoregressive (MAR) models promise better efficiency in visual\ngeneration than autoregressive (AR) models for the ability of parallel\ngeneration, yet their acceleration potential remains constrained by the\nmodeling complexity of spatially correlated visual tokens in a single step. To\naddress this limitation, we introduce Generation then Reconstruction (GtR), a\ntraining-free hierarchical sampling strategy that decomposes generation into\ntwo stages: structure generation establishing global semantic scaffolding,\nfollowed by detail reconstruction efficiently completing remaining tokens.\nAssuming that it is more difficult to create an image from scratch than to\ncomplement images based on a basic image framework, GtR is designed to achieve\nacceleration by computing the reconstruction stage quickly while maintaining\nthe generation quality by computing the generation stage slowly. Moreover,\nobserving that tokens on the details of an image often carry more semantic\ninformation than tokens in the salient regions, we further propose\nFrequency-Weighted Token Selection (FTS) to offer more computation budget to\ntokens on image details, which are localized based on the energy of high\nfrequency information. Extensive experiments on ImageNet class-conditional and\ntext-to-image generation demonstrate 3.72x speedup on MAR-H while maintaining\ncomparable quality (e.g., FID: 1.59, IS: 304.4 vs. original 1.59, 299.1),\nsubstantially outperforming existing acceleration methods across various model\nscales and generation tasks. Our codes will be released in\nhttps://github.com/feihongyan1/GtR.",
            "headline_zh": "提出GtR两阶段采样方法以加速掩码自回归模型，保持生成质量",
            "intro_zh": [
                "掩码自回归模型并行生成潜力受限于空间相关视觉令牌建模复杂度",
                "GtR分结构生成与细节重建两阶段，结合频率加权令牌选择优化计算分配",
                "实验显示在ImageNet等任务上实现3.72倍加速，质量与原始模型相当"
            ],
            "tags_zh": [
                "掩码自回归模型",
                "两阶段采样",
                "视觉生成加速",
                "频率分析",
                "训练无关方法"
            ],
            "_index": 101
        },
        {
            "title": "Investigating Adversarial Robustness against Preprocessing used in Blackbox Face Recognition",
            "authors": [
                "Roland Croft",
                "Brian Du",
                "Darcy Joseph",
                "Sharath Kumar"
            ],
            "arxiv_id": "2510.17169v1",
            "summary": "Face Recognition (FR) models have been shown to be vulnerable to adversarial\nexamples that subtly alter benign facial images, exposing blind spots in these\nsystems, as well as protecting user privacy. End-to-end FR systems first obtain\npreprocessed faces from diverse facial imagery prior to computing the\nsimilarity of the deep feature embeddings. Whilst face preprocessing is a\ncritical component of FR systems, and hence adversarial attacks against them,\nwe observe that this preprocessing is often overlooked in blackbox settings.\nOur study seeks to investigate the transferability of several out-of-the-box\nstate-of-the-art adversarial attacks against FR when applied against different\npreprocessing techniques used in a blackbox setting. We observe that the choice\nof face detection model can degrade the attack success rate by up to 78%,\nwhereas choice of interpolation method during downsampling has relatively\nminimal impacts. Furthermore, we find that the requirement for facial\npreprocessing even degrades attack strength in a whitebox setting, due to the\nunintended interaction of produced noise vectors against face detection models.\nBased on these findings, we propose a preprocessing-invariant method using\ninput transformations that improves the transferability of the studied attacks\nby up to 27%. Our findings highlight the importance of preprocessing in FR\nsystems, and the need for its consideration towards improving the adversarial\ngeneralisation of facial adversarial examples.",
            "headline_zh": "提出预处理不变方法以提升黑盒人脸识别中对抗攻击的迁移性",
            "intro_zh": [
                "核心问题：黑盒人脸识别系统中预处理常被忽视，影响对抗攻击迁移性。",
                "方法要点：使用输入变换构建预处理不变方法，增强攻击鲁棒性。",
                "实验或效果：攻击成功率提升达27%，预处理选择可降低成功率78%。"
            ],
            "tags_zh": [
                "人脸识别",
                "对抗攻击",
                "黑盒设置",
                "预处理技术",
                "迁移性提升"
            ],
            "_index": 102
        },
        {
            "title": "GACO-CAD: Geometry-Augmented and Conciseness-Optimized CAD Model Generation from Single Image",
            "authors": [
                "Yinghui Wang",
                "Xinyu Zhang",
                "Peng Du"
            ],
            "arxiv_id": "2510.17157v1",
            "summary": "Generating editable, parametric CAD models from a single image holds great\npotential to lower the barriers of industrial concept design. However, current\nmulti-modal large language models (MLLMs) still struggle with accurately\ninferring 3D geometry from 2D images due to limited spatial reasoning\ncapabilities. We address this limitation by introducing GACO-CAD, a novel\ntwo-stage post-training framework. It is designed to achieve a joint objective:\nsimultaneously improving the geometric accuracy of the generated CAD models and\nencouraging the use of more concise modeling procedures. First, during\nsupervised fine-tuning, we leverage depth and surface normal maps as dense\ngeometric priors, combining them with the RGB image to form a multi-channel\ninput. In the context of single-view reconstruction, these priors provide\ncomplementary spatial cues that help the MLLM more reliably recover 3D geometry\nfrom 2D observations. Second, during reinforcement learning, we introduce a\ngroup length reward that, while preserving high geometric fidelity, promotes\nthe generation of more compact and less redundant parametric modeling\nsequences. A simple dynamic weighting strategy is adopted to stabilize\ntraining. Experiments on the DeepCAD and Fusion360 datasets show that GACO-CAD\nachieves state-of-the-art performance under the same MLLM backbone,\nconsistently outperforming existing methods in terms of code validity,\ngeometric accuracy, and modeling conciseness.",
            "headline_zh": "提出GACO-CAD框架，通过几何增强与简洁优化从单图像生成可编辑CAD模型",
            "intro_zh": [
                "核心问题：多模态大语言模型从2D图像推断3D几何时空间推理能力不足",
                "方法要点：两阶段后训练，结合深度与法线图作为几何先验，并引入组长度奖励优化建模序列",
                "实验或效果：在DeepCAD和Fusion360数据集上实现SOTA，提升代码有效性、几何精度和建模简洁性"
            ],
            "tags_zh": [
                "CAD模型生成",
                "单视图重建",
                "几何增强",
                "强化学习",
                "多模态大语言模型",
                "建模简洁性"
            ],
            "_index": 103
        },
        {
            "title": "OmniVIC: A Self-Improving Variable Impedance Controller with Vision-Language In-Context Learning for Safe Robotic Manipulation",
            "authors": [
                "Heng Zhang",
                "Wei-Hsing Huang",
                "Gokhan Solak",
                "Arash Ajoudani"
            ],
            "arxiv_id": "2510.17150v1",
            "summary": "We present OmniVIC, a universal variable impedance controller (VIC) enhanced\nby a vision language model (VLM), which improves safety and adaptation in any\ncontact-rich robotic manipulation task to enhance safe physical interaction.\nTraditional VIC have shown advantages when the robot physically interacts with\nthe environment, but lack generalization in unseen, complex, and unstructured\nsafe interactions in universal task scenarios involving contact or uncertainty.\nTo this end, the proposed OmniVIC interprets task context derived reasoning\nfrom images and natural language and generates adaptive impedance parameters\nfor a VIC controller. Specifically, the core of OmniVIC is a self-improving\nRetrieval-Augmented Generation(RAG) and in-context learning (ICL), where RAG\nretrieves relevant prior experiences from a structured memory bank to inform\nthe controller about similar past tasks, and ICL leverages these retrieved\nexamples and the prompt of current task to query the VLM for generating\ncontext-aware and adaptive impedance parameters for the current manipulation\nscenario. Therefore, a self-improved RAG and ICL guarantee OmniVIC works in\nuniversal task scenarios. The impedance parameter regulation is further\ninformed by real-time force/torque feedback to ensure interaction forces remain\nwithin safe thresholds. We demonstrate that our method outperforms baselines on\na suite of complex contact-rich tasks, both in simulation and on real-world\nrobotic tasks, with improved success rates and reduced force violations.\nOmniVIC takes a step towards bridging high-level semantic reasoning and\nlow-level compliant control, enabling safer and more generalizable\nmanipulation. Overall, the average success rate increases from 27% (baseline)\nto 61.4% (OmniVIC).",
            "headline_zh": "提出OmniVIC可变阻抗控制器，结合视觉语言模型提升安全机器人操作通用性",
            "intro_zh": [
                "传统可变阻抗控制器在未知复杂任务中泛化能力不足，影响安全交互",
                "采用检索增强生成和上下文学习，从图像和语言推理生成自适应阻抗参数",
                "实验显示平均成功率从27%提升至61.4%，减少力违规，验证通用性"
            ],
            "tags_zh": [
                "可变阻抗控制",
                "视觉语言模型",
                "检索增强生成",
                "上下文学习",
                "安全机器人操作",
                "自适应控制"
            ],
            "_index": 104
        },
        {
            "title": "DiffVLA++: Bridging Cognitive Reasoning and End-to-End Driving through Metric-Guided Alignment",
            "authors": [
                "Yu Gao",
                "Yiru Wang",
                "Anqing Jiang",
                "Heng Yuwen",
                "Wang Shuo",
                "Sun Hao",
                "Wang Jijun"
            ],
            "arxiv_id": "2510.17148v1",
            "summary": "Conventional end-to-end (E2E) driving models are effective at generating\nphysically plausible trajectories, but often fail to generalize to long-tail\nscenarios due to the lack of essential world knowledge to understand and reason\nabout surrounding environments. In contrast, Vision-Language-Action (VLA)\nmodels leverage world knowledge to handle challenging cases, but their limited\n3D reasoning capability can lead to physically infeasible actions. In this work\nwe introduce DiffVLA++, an enhanced autonomous driving framework that\nexplicitly bridges cognitive reasoning and E2E planning through metric-guided\nalignment. First, we build a VLA module directly generating semantically\ngrounded driving trajectories. Second, we design an E2E module with a dense\ntrajectory vocabulary that ensures physical feasibility. Third, and most\ncritically, we introduce a metric-guided trajectory scorer that guides and\naligns the outputs of the VLA and E2E modules, thereby integrating their\ncomplementary strengths. The experiment on the ICCV 2025 Autonomous Grand\nChallenge leaderboard shows that DiffVLA++ achieves EPDMS of 49.12.",
            "headline_zh": "提出DiffVLA++框架，通过度量引导对齐桥接认知推理与端到端驾驶",
            "intro_zh": [
                "核心问题：端到端驾驶模型泛化能力差，VLA模型物理可行性不足",
                "方法要点：结合VLA模块生成语义轨迹与E2E模块确保物理可行性",
                "实验效果：在ICCV 2025挑战中EPDMS达49.12，提升驾驶性能"
            ],
            "tags_zh": [
                "自动驾驶",
                "视觉语言动作模型",
                "端到端规划",
                "度量引导对齐",
                "轨迹生成"
            ],
            "_index": 105
        },
        {
            "title": "Decentralized Real-Time Planning for Multi-UAV Cooperative Manipulation via Imitation Learning",
            "authors": [
                "Shantnav Agarwal",
                "Javier Alonso-Mora",
                "Sihao Sun"
            ],
            "arxiv_id": "2510.17143v1",
            "summary": "Existing approaches for transporting and manipulating cable-suspended loads\nusing multiple UAVs along reference trajectories typically rely on either\ncentralized control architectures or reliable inter-agent communication. In\nthis work, we propose a novel machine learning based method for decentralized\nkinodynamic planning that operates effectively under partial observability and\nwithout inter-agent communication. Our method leverages imitation learning to\ntrain a decentralized student policy for each UAV by imitating a centralized\nkinodynamic motion planner with access to privileged global observations. The\nstudent policy generates smooth trajectories using physics-informed neural\nnetworks that respect the derivative relationships in motion. During training,\nthe student policies utilize the full trajectory generated by the teacher\npolicy, leading to improved sample efficiency. Moreover, each student policy\ncan be trained in under two hours on a standard laptop. We validate our method\nin both simulation and real-world environments to follow an agile reference\ntrajectory, demonstrating performance comparable to that of centralized\napproaches.",
            "headline_zh": "提出基于模仿学习的去中心化规划方法，用于多无人机协同操纵电缆悬挂负载。",
            "intro_zh": [
                "核心问题：现有方法依赖集中控制或可靠通信，难以在部分可观测和无通信下运行。",
                "方法要点：通过模仿学习训练去中心化策略，使用物理信息神经网络生成平滑轨迹。",
                "实验或效果：在仿真和真实环境中验证，性能接近集中式方法，训练高效。"
            ],
            "tags_zh": [
                "去中心化规划",
                "模仿学习",
                "多无人机协同",
                "物理信息神经网络",
                "电缆悬挂负载操纵"
            ],
            "_index": 106
        },
        {
            "title": "KineDiff3D: Kinematic-Aware Diffusion for Category-Level Articulated Object Shape Reconstruction and Generation",
            "authors": [
                "WenBo Xu",
                "Liu Liu",
                "Li Zhang",
                "Ran Zhang",
                "Hao Wu",
                "Dan Guo",
                "Meng Wang"
            ],
            "arxiv_id": "2510.17137v1",
            "summary": "Articulated objects, such as laptops and drawers, exhibit significant\nchallenges for 3D reconstruction and pose estimation due to their multi-part\ngeometries and variable joint configurations, which introduce structural\ndiversity across different states. To address these challenges, we propose\nKineDiff3D: Kinematic-Aware Diffusion for Category-Level Articulated Object\nShape Reconstruction and Generation, a unified framework for reconstructing\ndiverse articulated instances and pose estimation from single view input.\nSpecifically, we first encode complete geometry (SDFs), joint angles, and part\nsegmentation into a structured latent space via a novel Kinematic-Aware VAE\n(KA-VAE). In addition, we employ two conditional diffusion models: one for\nregressing global pose (SE(3)) and joint parameters, and another for generating\nthe kinematic-aware latent code from partial observations. Finally, we produce\nan iterative optimization module that bidirectionally refines reconstruction\naccuracy and kinematic parameters via Chamfer-distance minimization while\npreserving articulation constraints. Experimental results on synthetic,\nsemi-synthetic, and real-world datasets demonstrate the effectiveness of our\napproach in accurately reconstructing articulated objects and estimating their\nkinematic properties.",
            "headline_zh": "提出KineDiff3D以解决单视角下铰接物体形状重建与姿态估计问题",
            "intro_zh": [
                "铰接物体因多部件几何和关节配置导致结构多样性，重建与姿态估计困难",
                "使用KA-VAE编码几何、关节角和分割，结合扩散模型回归姿态和生成潜在码",
                "实验在合成和真实数据集验证了准确重建和运动学参数估计的有效性"
            ],
            "tags_zh": [
                "铰接物体重建",
                "扩散模型",
                "运动学感知",
                "3D形状生成",
                "姿态估计"
            ],
            "_index": 107
        },
        {
            "title": "GOOD: Training-Free Guided Diffusion Sampling for Out-of-Distribution Detection",
            "authors": [
                "Xin Gao",
                "Jiyao Liu",
                "Guanghao Li",
                "Yueming Lyu",
                "Jianxiong Gao",
                "Weichen Yu",
                "Ningsheng Xu",
                "Liang Wang",
                "Caifeng Shan",
                "Ziwei Liu",
                "Chenyang Si"
            ],
            "arxiv_id": "2510.17131v1",
            "summary": "Recent advancements have explored text-to-image diffusion models for\nsynthesizing out-of-distribution (OOD) samples, substantially enhancing the\nperformance of OOD detection. However, existing approaches typically rely on\nperturbing text-conditioned embeddings, resulting in semantic instability and\ninsufficient shift diversity, which limit generalization to realistic OOD. To\naddress these challenges, we propose GOOD, a novel and flexible framework that\ndirectly guides diffusion sampling trajectories towards OOD regions using\noff-the-shelf in-distribution (ID) classifiers. GOOD incorporates dual-level\nguidance: (1) Image-level guidance based on the gradient of log partition to\nreduce input likelihood, drives samples toward low-density regions in pixel\nspace. (2) Feature-level guidance, derived from k-NN distance in the\nclassifier's latent space, promotes sampling in feature-sparse regions. Hence,\nthis dual-guidance design enables more controllable and diverse OOD sample\ngeneration. Additionally, we introduce a unified OOD score that adaptively\ncombines image and feature discrepancies, enhancing detection robustness. We\nperform thorough quantitative and qualitative analyses to evaluate the\neffectiveness of GOOD, demonstrating that training with samples generated by\nGOOD can notably enhance OOD detection performance.",
            "headline_zh": "提出GOOD框架，利用ID分类器引导扩散采样以增强OOD检测",
            "intro_zh": [
                "现有方法扰动文本嵌入导致语义不稳定和多样性不足，限制OOD检测泛化",
                "GOOD采用图像级和特征级双重引导，驱动采样至低密度和稀疏区域",
                "实验表明GOOD生成样本可显著提升OOD检测性能，增强鲁棒性"
            ],
            "tags_zh": [
                "扩散模型",
                "OOD检测",
                "引导采样",
                "分类器指导",
                "样本生成"
            ],
            "_index": 108
        },
        {
            "title": "Matricial Free Energy as a Gaussianizing Regularizer: Enhancing Autoencoders for Gaussian Code Generation",
            "authors": [
                "Rishi Sonthalia",
                "Raj Rao Nadakuditi"
            ],
            "arxiv_id": "2510.17120v1",
            "summary": "We introduce a novel regularization scheme for autoencoders based on\nmatricial free energy. Our approach defines a differentiable loss function in\nterms of the singular values of the code matrix (code dimension x batch size).\nFrom the standpoint of free probability an d random matrix theory, this loss\nachieves its minimum when the singular value distribution of the code matrix\ncoincides with that of an appropriately sculpted random metric with i.i.d.\nGaussian entries. Empirical simulations demonstrate that minimizing the\nnegative matricial free energy through standard stochastic gradient-based\ntraining yields Gaussian-like codes that generalize across training and test\nsets. Building on this foundation, we propose a matricidal free energy\nmaximizing autoencoder that reliably produces Gaussian codes and show its\napplication to underdetermined inverse problems.",
            "headline_zh": "提出基于矩阵自由能的正则化方法，增强自编码器生成高斯代码",
            "intro_zh": [
                "核心问题：自编码器代码分布需高斯化以提升泛化能力",
                "方法要点：定义矩阵自由能损失，优化代码矩阵奇异值分布",
                "实验或效果：经验模拟显示高斯代码在训练和测试集上泛化良好"
            ],
            "tags_zh": [
                "自编码器",
                "矩阵自由能",
                "高斯代码生成",
                "正则化方法",
                "随机矩阵理论"
            ],
            "_index": 109
        },
        {
            "title": "Towards Imperceptible Watermarking Via Environment Illumination for Consumer Cameras",
            "authors": [
                "Hodaka Kawachi",
                "Tomoya Nakamura",
                "Hiroaki Santo",
                "SaiKiran Kumar Tedla",
                "Trevor Dalton Canham",
                "Yasushi Yagi",
                "Michael S. Brown"
            ],
            "arxiv_id": "2510.17114v1",
            "summary": "This paper introduces a method for using LED-based environmental lighting to\nproduce visually imperceptible watermarks for consumer cameras. Our approach\noptimizes an LED light source's spectral profile to be minimally visible to the\nhuman eye while remaining highly detectable by typical consumer cameras. The\nmethod jointly considers the human visual system's sensitivity to visible\nspectra, modern consumer camera sensors' spectral sensitivity, and narrowband\nLEDs' ability to generate broadband spectra perceived as \"white light\"\n(specifically, D65 illumination). To ensure imperceptibility, we employ\nspectral modulation rather than intensity modulation. Unlike conventional\nvisible light communication, our approach enables watermark extraction at\nstandard low frame rates (30-60 fps). While the information transfer rate is\nmodest-embedding 128 bits within a 10-second video clip-this capacity is\nsufficient for essential metadata supporting privacy protection and content\nverification.",
            "headline_zh": "提出基于环境光照的不可见水印方法，用于消费相机隐私保护和内容验证",
            "intro_zh": [
                "核心问题：如何在消费相机中嵌入不可见水印，避免人眼察觉",
                "方法要点：优化LED光谱，结合人眼和相机传感器敏感性，使用光谱调制",
                "实验或效果：在30-60 fps视频中嵌入128位数据，支持隐私和验证应用"
            ],
            "tags_zh": [
                "不可见水印",
                "环境光照",
                "光谱调制",
                "消费相机",
                "隐私保护",
                "内容验证"
            ],
            "_index": 110
        },
        {
            "title": "Efficient Vision-Language-Action Models for Embodied Manipulation: A Systematic Survey",
            "authors": [
                "Weifan Guan",
                "Qinghao Hu",
                "Aosheng Li",
                "Jian Cheng"
            ],
            "arxiv_id": "2510.17111v1",
            "summary": "Vision-Language-Action (VLA) models extend vision-language models to embodied\ncontrol by mapping natural-language instructions and visual observations to\nrobot actions. Despite their capabilities, VLA systems face significant\nchallenges due to their massive computational and memory demands, which\nconflict with the constraints of edge platforms such as on-board mobile\nmanipulators that require real-time performance. Addressing this tension has\nbecome a central focus of recent research. In light of the growing efforts\ntoward more efficient and scalable VLA systems, this survey provides a\nsystematic review of approaches for improving VLA efficiency, with an emphasis\non reducing latency, memory footprint, and training and inference costs. We\ncategorize existing solutions into four dimensions: model architecture,\nperception feature, action generation, and training/inference strategies,\nsummarizing representative techniques within each category. Finally, we discuss\nfuture trends and open challenges, highlighting directions for advancing\nefficient embodied intelligence.",
            "headline_zh": "系统综述高效视觉-语言-动作模型方法，以解决边缘平台实时控制挑战",
            "intro_zh": [
                "核心问题：VLA模型计算和内存需求高，与边缘平台实时性能要求冲突。",
                "方法要点：从模型架构、感知特征、动作生成和训练/推理策略四维度改进效率。",
                "实验或效果：未知具体实验，但总结了代表性技术并讨论未来趋势。"
            ],
            "tags_zh": [
                "视觉-语言-动作模型",
                "边缘计算",
                "模型效率优化",
                "机器人控制",
                "系统综述"
            ],
            "_index": 111
        },
        {
            "title": "Boosting Fidelity for Pre-Trained-Diffusion-Based Low-Light Image Enhancement via Condition Refinement",
            "authors": [
                "Xiaogang Xu",
                "Jian Wang",
                "Yunfan Lu",
                "Ruihang Chu",
                "Ruixing Wang",
                "Jiafei Wu",
                "Bei Yu",
                "Liang Lin"
            ],
            "arxiv_id": "2510.17105v1",
            "summary": "Diffusion-based methods, leveraging pre-trained large models like Stable\nDiffusion via ControlNet, have achieved remarkable performance in several\nlow-level vision tasks. However, Pre-Trained Diffusion-Based (PTDB) methods\noften sacrifice content fidelity to attain higher perceptual realism. This\nissue is exacerbated in low-light scenarios, where severely degraded\ninformation caused by the darkness limits effective control. We identify two\nprimary causes of fidelity loss: the absence of suitable conditional latent\nmodeling and the lack of bidirectional interaction between the conditional\nlatent and noisy latent in the diffusion process. To address this, we propose a\nnovel optimization strategy for conditioning in pre-trained diffusion models,\nenhancing fidelity while preserving realism and aesthetics. Our method\nintroduces a mechanism to recover spatial details lost during VAE encoding,\ni.e., a latent refinement pipeline incorporating generative priors.\nAdditionally, the refined latent condition interacts dynamically with the noisy\nlatent, leading to improved restoration performance. Our approach is\nplug-and-play, seamlessly integrating into existing diffusion networks to\nprovide more effective control. Extensive experiments demonstrate significant\nfidelity improvements in PTDB methods.",
            "headline_zh": "提出条件优化策略以提升预训练扩散模型在低光图像增强中的保真度",
            "intro_zh": [
                "预训练扩散方法在低光场景下因条件建模不足和交互缺失导致保真度下降",
                "引入潜在细化管道和动态交互机制，恢复空间细节并增强控制",
                "实验显示该方法可无缝集成现有网络，显著提升保真度"
            ],
            "tags_zh": [
                "低光图像增强",
                "预训练扩散模型",
                "条件优化",
                "潜在细化",
                "动态交互",
                "保真度提升"
            ],
            "_index": 112
        },
        {
            "title": "Shape-aware Inertial Poser: Motion Tracking for Humans with Diverse Shapes Using Sparse Inertial Sensors",
            "authors": [
                "Lu Yin",
                "Ziying Shi",
                "Yinghao Wu",
                "Xinyu Yi",
                "Feng Xu",
                "Shihui Guo"
            ],
            "arxiv_id": "2510.17101v1",
            "summary": "Human motion capture with sparse inertial sensors has gained significant\nattention recently. However, existing methods almost exclusively rely on a\ntemplate adult body shape to model the training data, which poses challenges\nwhen generalizing to individuals with largely different body shapes (such as a\nchild). This is primarily due to the variation in IMU-measured acceleration\ncaused by changes in body shape. To fill this gap, we propose Shape-aware\nInertial Poser (SAIP), the first solution considering body shape differences in\nsparse inertial-based motion capture. Specifically, we decompose the sensor\nmeasurements related to shape and pose in order to effectively model their\njoint correlations. Firstly, we train a regression model to transfer the\nIMU-measured accelerations of a real body to match the template adult body\nmodel, compensating for the shape-related sensor measurements. Then, we can\neasily follow the state-of-the-art methods to estimate the full body motions of\nthe template-shaped body. Finally, we utilize a second regression model to map\nthe joint velocities back to the real body, combined with a shape-aware\nphysical optimization strategy to calculate global motions on the subject.\nFurthermore, our method relies on body shape awareness, introducing the first\ninertial shape estimation scheme. This is accomplished by modeling the\nshape-conditioned IMU-pose correlation using an MLP-based network. To validate\nthe effectiveness of SAIP, we also present the first IMU motion capture dataset\ncontaining individuals of different body sizes. This dataset features 10\nchildren and 10 adults, with heights ranging from 110 cm to 190 cm, and a total\nof 400 minutes of paired IMU-Motion samples. Extensive experimental results\ndemonstrate that SAIP can effectively handle motion capture tasks for diverse\nbody shapes. The code and dataset are available at\nhttps://github.com/yinlu5942/SAIP.",
            "headline_zh": "提出Shape-aware Inertial Poser以解决稀疏惯性传感器在多样化人体形状下的运动捕捉问题",
            "intro_zh": [
                "现有方法依赖模板成人身体形状，难以泛化到不同体型如儿童，因身体形状变化影响IMU加速度测量",
                "方法分解传感器测量为形状和姿态相关部分，通过回归模型补偿形状差异并估计全局运动",
                "实验基于首个包含不同体型个体的IMU数据集，验证SAIP能有效处理多样化身体形状的运动捕捉"
            ],
            "tags_zh": [
                "惯性运动捕捉",
                "身体形状估计",
                "传感器测量分解",
                "回归模型",
                "物理优化",
                "多样化体型数据集"
            ],
            "_index": 113
        },
        {
            "title": "GSPlane: Concise and Accurate Planar Reconstruction via Structured Representation",
            "authors": [
                "Ruitong Gan",
                "Junran Peng",
                "Yang Liu",
                "Chuanchen Luo",
                "Qing Li",
                "Zhaoxiang Zhang"
            ],
            "arxiv_id": "2510.17095v1",
            "summary": "Planes are fundamental primitives of 3D sences, especially in man-made\nenvironments such as indoor spaces and urban streets. Representing these planes\nin a structured and parameterized format facilitates scene editing and physical\nsimulations in downstream applications. Recently, Gaussian Splatting (GS) has\ndemonstrated remarkable effectiveness in the Novel View Synthesis task, with\nextensions showing great potential in accurate surface reconstruction. However,\neven state-of-the-art GS representations often struggle to reconstruct planar\nregions with sufficient smoothness and precision. To address this issue, we\npropose GSPlane, which recovers accurate geometry and produces clean and\nwell-structured mesh connectivity for plane regions in the reconstructed scene.\nBy leveraging off-the-shelf segmentation and normal prediction models, GSPlane\nextracts robust planar priors to establish structured representations for\nplanar Gaussian coordinates, which help guide the training process by enforcing\ngeometric consistency. To further enhance training robustness, a Dynamic\nGaussian Re-classifier is introduced to adaptively reclassify planar Gaussians\nwith persistently high gradients as non-planar, ensuring more reliable\noptimization. Furthermore, we utilize the optimized planar priors to refine the\nmesh layouts, significantly improving topological structure while reducing the\nnumber of vertices and faces. We also explore applications of the structured\nplanar representation, which enable decoupling and flexible manipulation of\nobjects on supportive planes. Extensive experiments demonstrate that, with no\nsacrifice in rendering quality, the introduction of planar priors significantly\nimproves the geometric accuracy of the extracted meshes across various\nbaselines.",
            "headline_zh": "提出GSPlane以解决高斯溅射在平面重建中平滑性和精度不足的问题",
            "intro_zh": [
                "高斯溅射在平面区域重建中常缺乏平滑性和精度",
                "利用分割和法线预测模型提取平面先验，增强几何一致性",
                "实验显示在保持渲染质量下，显著提升网格几何准确性"
            ],
            "tags_zh": [
                "平面重建",
                "高斯溅射",
                "几何优化",
                "结构化表示",
                "网格简化"
            ],
            "_index": 114
        },
        {
            "title": "Learning to Design Soft Hands using Reward Models",
            "authors": [
                "Xueqian Bai",
                "Nicklas Hansen",
                "Adabhav Singh",
                "Michael T. Tolley",
                "Yan Duan",
                "Pieter Abbeel",
                "Xiaolong Wang",
                "Sha Yi"
            ],
            "arxiv_id": "2510.17086v1",
            "summary": "Soft robotic hands promise to provide compliant and safe interaction with\nobjects and environments. However, designing soft hands to be both compliant\nand functional across diverse use cases remains challenging. Although co-design\nof hardware and control better couples morphology to behavior, the resulting\nsearch space is high-dimensional, and even simulation-based evaluation is\ncomputationally expensive. In this paper, we propose a Cross-Entropy Method\nwith Reward Model (CEM-RM) framework that efficiently optimizes tendon-driven\nsoft robotic hands based on teleoperation control policy, reducing design\nevaluations by more than half compared to pure optimization while learning a\ndistribution of optimized hand designs from pre-collected teleoperation data.\nWe derive a design space for a soft robotic hand composed of flexural soft\nfingers and implement parallelized training in simulation. The optimized hands\nare then 3D-printed and deployed in the real world using both teleoperation\ndata and real-time teleoperation. Experiments in both simulation and hardware\ndemonstrate that our optimized design significantly outperforms baseline hands\nin grasping success rates across a diverse set of challenging objects.",
            "headline_zh": "提出CEM-RM框架以优化肌腱驱动软体手设计，提升抓取成功率",
            "intro_zh": [
                "软体手设计需兼顾柔顺性与功能性，但硬件与控制协同设计空间高维且评估昂贵",
                "基于遥操作控制策略，使用CEM-RM框架高效优化设计，减少超半评估次数",
                "仿真与硬件实验显示，优化设计在多样挑战物体上抓取成功率显著优于基线"
            ],
            "tags_zh": [
                "软体机器人设计",
                "硬件控制协同优化",
                "奖励模型",
                "肌腱驱动",
                "仿真训练",
                "抓取成功率"
            ],
            "_index": 115
        },
        {
            "title": "Towards a Generalizable Fusion Architecture for Multimodal Object Detection",
            "authors": [
                "Jad Berjawi",
                "Yoann Dupas",
                "Christophe C'erin"
            ],
            "arxiv_id": "2510.17078v1",
            "summary": "Multimodal object detection improves robustness in chal- lenging conditions\nby leveraging complementary cues from multiple sensor modalities. We introduce\nFiltered Multi- Modal Cross Attention Fusion (FMCAF), a preprocess- ing\narchitecture designed to enhance the fusion of RGB and infrared (IR) inputs.\nFMCAF combines a frequency- domain filtering block (Freq-Filter) to suppress\nredun- dant spectral features with a cross-attention-based fusion module (MCAF)\nto improve intermodal feature sharing. Unlike approaches tailored to specific\ndatasets, FMCAF aims for generalizability, improving performance across\ndifferent multimodal challenges without requiring dataset- specific tuning. On\nLLVIP (low-light pedestrian detec- tion) and VEDAI (aerial vehicle detection),\nFMCAF outper- forms traditional fusion (concatenation), achieving +13.9% mAP@50\non VEDAI and +1.1% on LLVIP. These results support the potential of FMCAF as a\nflexible foundation for robust multimodal fusion in future detection pipelines.",
            "headline_zh": "提出FMCAF架构以增强RGB与红外图像融合，提升多模态目标检测的泛化性。",
            "intro_zh": [
                "多模态目标检测在挑战条件下依赖多传感器互补，但传统方法泛化性不足。",
                "FMCAF结合频域滤波与跨注意力融合，抑制冗余特征并促进模态间特征共享。",
                "在LLVIP和VEDAI数据集上，FMCAF优于传统融合方法，提升mAP@50指标。"
            ],
            "tags_zh": [
                "多模态目标检测",
                "图像融合",
                "跨注意力机制",
                "频域滤波",
                "泛化性提升"
            ],
            "_index": 116
        },
        {
            "title": "ProDAT: Progressive Density-Aware Tail-Drop for Point Cloud Coding",
            "authors": [
                "Zhe Luo",
                "Wenjing Jia",
                "Stuart Perry"
            ],
            "arxiv_id": "2510.17068v1",
            "summary": "Three-dimensional (3D) point clouds are becoming increasingly vital in\napplications such as autonomous driving, augmented reality, and immersive\ncommunication, demanding real-time processing and low latency. However, their\nlarge data volumes and bandwidth constraints hinder the deployment of\nhigh-quality services in resource-limited environments. Progres- sive coding,\nwhich allows for decoding at varying levels of detail, provides an alternative\nby allowing initial partial decoding with subsequent refinement. Although\nrecent learning-based point cloud geometry coding methods have achieved notable\nsuccess, their fixed latent representation does not support progressive\ndecoding. To bridge this gap, we propose ProDAT, a novel density-aware\ntail-drop mechanism for progressive point cloud coding. By leveraging density\ninformation as a guidance signal, latent features and coordinates are decoded\nadaptively based on their significance, therefore achieving progressive\ndecoding at multiple bitrates using one single model. Experimental results on\nbenchmark datasets show that the proposed ProDAT not only enables progressive\ncoding but also achieves superior coding efficiency compared to\nstate-of-the-art learning-based coding techniques, with over 28.6% BD-rate\nimprovement for PSNR- D2 on SemanticKITTI and over 18.15% for ShapeNet",
            "headline_zh": "提出ProDAT密度感知尾丢机制，实现点云渐进式编码以应对带宽限制",
            "intro_zh": [
                "核心问题：点云数据量大，固定潜在表示不支持渐进解码，阻碍资源受限环境应用",
                "方法要点：利用密度信息指导自适应解码潜在特征和坐标，实现多比特率渐进编码",
                "实验效果：在基准数据集上，ProDAT实现渐进编码，BD-rate提升超28.6%"
            ],
            "tags_zh": [
                "点云编码",
                "渐进解码",
                "密度感知",
                "学习型压缩",
                "三维数据处理"
            ],
            "_index": 117
        }
    ]
}