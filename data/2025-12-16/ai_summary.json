{
    "papers": [
        {
            "title": "CRISP: Contact-Guided Real2Sim from Monocular Video with Planar Scene Primitives",
            "authors": [
                "Zihan Wang",
                "Jiashun Wang",
                "Jeff Tan",
                "Yiwen Zhao",
                "Jessica Hodgins",
                "Shubham Tulsiani",
                "Deva Ramanan"
            ],
            "arxiv_id": "2512.14696v1",
            "summary": "We introduce CRISP, a method that recovers simulatable human motion and scene geometry from monocular video. Prior work on joint human-scene reconstruction relies on data-driven priors and joint optimization with no physics in the loop, or recovers noisy geometry with artifacts that cause motion tracking policies with scene interactions to fail. In contrast, our key insight is to recover convex, clean, and simulation-ready geometry by fitting planar primitives to a point cloud reconstruction of the scene, via a simple clustering pipeline over depth, normals, and flow. To reconstruct scene geometry that might be occluded during interactions, we make use of human-scene contact modeling (e.g., we use human posture to reconstruct the occluded seat of a chair). Finally, we ensure that human and scene reconstructions are physically-plausible by using them to drive a humanoid controller via reinforcement learning. Our approach reduces motion tracking failure rates from 55.2\\% to 6.9\\% on human-centric video benchmarks (EMDB, PROX), while delivering a 43\\% faster RL simulation throughput. We further validate it on in-the-wild videos including casually-captured videos, Internet videos, and even Sora-generated videos. This demonstrates CRISP's ability to generate physically-valid human motion and interaction environments at scale, greatly advancing real-to-sim applications for robotics and AR/VR.",
            "categories": [
                "cs.CV",
                "cs.GR",
                "cs.RO"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Project page: https://crisp-real2sim.github.io/CRISP-Real2Sim/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14696v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "人形移动 (Humanoid Locomotion)",
                    "matched_keywords": [
                        "humanoid control"
                    ],
                    "score": 1
                },
                {
                    "name": "遥操作与模仿 (Teleoperation & Imitation)",
                    "matched_keywords": [
                        "motion tracking",
                        "human motion",
                        "real2sim"
                    ],
                    "score": 3
                },
                {
                    "name": "3D重建 (3D Reconstruction)",
                    "matched_keywords": [
                        "scene reconstruction",
                        "point cloud reconstruction"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 6,
            "headline_zh": "CRISP：基于单目视频和平面场景原语的接触引导Real2Sim方法",
            "summary_zh": "CRISP是一种从单目视频中恢复可模拟的人体运动和场景几何的方法。现有的人体-场景联合重建工作依赖于数据驱动的先验和无物理引擎的联合优化，或者恢复的几何体噪声大，导致带有场景交互的运动跟踪策略失败。CRISP的关键在于通过拟合平面原语到场景的点云重建，来恢复凸的、干净的、可用于仿真的几何体，这通过一个简单的深度、法线和光流聚类流程实现。为了重建交互过程中可能被遮挡的场景几何体，我们利用了人体-场景接触建模（例如，使用人体姿势来重建椅子被遮挡的座位）。最后，我们通过强化学习驱动人形控制器，确保人体和场景重建在物理上是合理的。在以人为中心的视频基准测试（EMDB、PROX）中，我们的方法将运动跟踪失败率从55.2％降低到6.9％，同时提供了快43％的RL模拟吞吐量。我们还在包括随意拍摄的视频、互联网视频甚至Sora生成的视频在内的真实视频上验证了它。这证明了CRISP大规模生成物理上有效的人体运动和交互环境的能力，极大地推进了机器人和AR/VR的real-to-sim应用。",
            "intro_zh": [
                "现有方法在人体-场景联合重建中存在不足，要么依赖数据先验，要么重建的几何体噪声大，导致交互模拟失败。",
                "CRISP通过拟合平面原语到点云重建，并结合人体-场景接触建模，恢复干净、凸的、可用于仿真的场景几何体。",
                "实验表明，CRISP显著降低了运动跟踪失败率，提高了强化学习模拟的吞吐量，并在真实视频和生成视频上验证了有效性。"
            ],
            "method_zh": "**问题定义**：现有的人体-场景联合重建方法要么依赖大量数据先验，缺乏对物理规律的约束，导致重建结果不真实；要么重建的场景几何体噪声较大，存在伪影，使得基于物理仿真的运动控制策略难以成功，尤其是在人与环境存在交互的情况下。因此，如何从单目视频中重建出既真实又干净，且能用于物理仿真的三维人体和场景模型是一个关键问题。\\n\\n**核心思路**：CRISP的核心思路是利用平面原语来表示场景几何体，并结合人体-场景接触信息来推断被遮挡的区域。通过将点云重建结果拟合为一系列平面，可以得到干净、凸的场景几何体，这更适合于物理仿真。同时，利用人体姿势信息，可以推断出人体与场景的接触区域，从而恢复被遮挡的场景部分。\\n\\n**技术框架**：CRISP的整体流程包括以下几个主要步骤：1) 从单目视频中重建点云；2) 对点云进行聚类，提取平面原语；3) 利用人体姿势信息和接触模型，推断并补全被遮挡的场景区域；4) 使用重建的人体和场景模型，通过强化学习训练人形控制器，确保运动的物理合理性。\\n\\n**关键创新**：CRISP的关键创新在于将平面原语表示和人体-场景接触建模相结合，用于从单目视频中重建可用于物理仿真的场景几何体。与以往方法相比，CRISP不需要大量的数据先验，并且能够生成更干净、更真实的场景模型。此外，通过强化学习来驱动人形控制器，可以确保重建的人体运动在物理上是合理的。\\n\\n**关键设计**：CRISP使用深度、法线和光流信息进行点云聚类，以提取平面原语。人体-场景接触建模基于人体姿势和场景几何体之间的关系，使用启发式规则来推断接触区域。强化学习的目标是训练一个能够控制人形角色在重建场景中运动的策略，奖励函数包括模仿真实运动、保持平衡和避免碰撞等。",
            "application_zh": "CRISP具有广泛的应用前景，包括机器人技术、增强现实（AR）和虚拟现实（VR）。它可以用于创建逼真的虚拟环境，用于训练机器人或进行虚拟仿真。此外，CRISP还可以用于AR/VR应用中，将虚拟物体与真实场景进行交互，例如，在虚拟环境中模拟人在真实房间中行走和交互。",
            "highlight_zh": "CRISP在以人为中心的视频基准测试（EMDB、PROX）中，将运动跟踪失败率从55.2％降低到6.9％，显著提升了运动跟踪的准确性。同时，CRISP还提供了快43％的强化学习模拟吞吐量，提高了仿真效率。此外，CRISP在真实视频和Sora生成的视频上的成功应用，证明了其在各种场景下的泛化能力。",
            "tags_zh": [
                "Real2Sim",
                "单目视频重建",
                "人体-场景交互",
                "平面原语",
                "物理仿真",
                "强化学习",
                "场景重建"
            ],
            "_index": 0,
            "_used_api": "gemini"
        },
        {
            "title": "Beyond a Single Light: A Large-Scale Aerial Dataset for Urban Scene Reconstruction Under Varying Illumination",
            "authors": [
                "Zhuoxiao Li",
                "Wenzong Ma",
                "Taoyu Wu",
                "Jinjing Zhu",
                "Zhenchao Q",
                "Shuai Zhang",
                "Jing Ou",
                "Yinrui Ren",
                "Weiqing Qi",
                "Guobin Shen",
                "Hui Xiong",
                "Wufan Zhao"
            ],
            "arxiv_id": "2512.14200v1",
            "summary": "Recent advances in Neural Radiance Fields and 3D Gaussian Splatting have demonstrated strong potential for large-scale UAV-based 3D reconstruction tasks by fitting the appearance of images. However, real-world large-scale captures are often based on multi-temporal data capture, where illumination inconsistencies across different times of day can significantly lead to color artifacts, geometric inaccuracies, and inconsistent appearance. Due to the lack of UAV datasets that systematically capture the same areas under varying illumination conditions, this challenge remains largely underexplored. To fill this gap, we introduceSkyLume, a large-scale, real-world UAV dataset specifically designed for studying illumination robust 3D reconstruction in urban scene modeling: (1) We collect data from 10 urban regions data comprising more than 100k high resolution UAV images (four oblique views and nadir), where each region is captured at three periods of the day to systematically isolate illumination changes. (2) To support precise evaluation of geometry and appearance, we provide per-scene LiDAR scans and accurate 3D ground-truth for assessing depth, surface normals, and reconstruction quality under varying illumination. (3) For the inverse rendering task, we introduce the Temporal Consistency Coefficient (TCC), a metric that measuress cross-time albedo stability and directly evaluates the robustness of the disentanglement of light and material. We aim for this resource to serve as a foundation that advances research and real-world evaluation in large-scale inverse rendering, geometry reconstruction, and novel view synthesis.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14200v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "3D重建 (3D Reconstruction)",
                    "matched_keywords": [
                        "3D reconstruction",
                        "3D Gaussian",
                        "Gaussian splatting",
                        "neural radiance",
                        "novel view synthesis",
                        "scene reconstruction"
                    ],
                    "score": 6
                }
            ],
            "relevance_score": 6,
            "headline_zh": "SkyLume：一个大规模城市航拍数据集，用于研究光照变化下的三维重建。",
            "summary_zh": "本文提出了SkyLume，一个大规模的真实城市航拍数据集，专门用于研究光照鲁棒的三维重建。现有的基于神经辐射场和3D高斯溅射的方法在基于无人机的三维重建任务中表现出强大的潜力，但真实场景的大规模数据采集通常基于多时相数据，不同时间段的光照不一致会导致颜色伪影、几何不准确和外观不一致。由于缺乏系统性地捕捉相同区域在不同光照条件下的无人机数据集，这一挑战在很大程度上仍未被充分探索。SkyLume包含来自10个城市区域的超过10万张高分辨率无人机图像（四个倾斜视图和垂直向下视图），每个区域在一天中的三个时段捕获，以系统地隔离光照变化。为了支持对几何和外观的精确评估，我们提供了每个场景的激光雷达扫描和精确的3D真值，用于评估深度、表面法线和不同光照下的重建质量。此外，我们引入了时间一致性系数（TCC），用于衡量跨时间反照率的稳定性，并直接评估光照和材质解耦的鲁棒性。该数据集旨在为大规模逆渲染、几何重建和新视角合成的研究和实际评估提供基础。",
            "intro_zh": [
                "现有基于无人机图像的三维重建方法易受光照变化影响，导致重建质量下降。",
                "SkyLume数据集通过在不同时间段捕获同一区域的图像，系统性地研究光照变化对三维重建的影响。",
                "该数据集提供激光雷达扫描和3D真值，并提出时间一致性系数（TCC）用于评估重建质量。"
            ],
            "method_zh": "**问题定义**：论文旨在解决城市环境中，由于不同时间段光照变化导致无人机图像三维重建效果不佳的问题。现有方法在处理多时相数据时，容易产生颜色伪影、几何不准确和外观不一致等问题，缺乏对光照鲁棒性的有效建模。\\n\\n**核心思路**：论文的核心思路是通过构建一个大规模的、包含不同光照条件下的无人机图像数据集，为研究光照鲁棒的三维重建方法提供数据基础。同时，提出时间一致性系数（TCC）作为评估光照解耦效果的指标。\\n\\n**技术框架**：SkyLume数据集的构建流程包括：1) 选择10个城市区域；2) 在每个区域的不同时间段（一天中的三个时段）进行无人机图像采集，包括四个倾斜视图和垂直向下视图；3) 对每个场景进行激光雷达扫描，并提供精确的3D真值；4) 提出时间一致性系数（TCC）用于评估重建结果。\\n\\n**关键创新**：该论文的关键创新在于构建了一个大规模的、系统性的无人机图像数据集，专门用于研究光照变化对三维重建的影响。与现有数据集相比，SkyLume数据集在数据规模、光照变化覆盖范围和提供的真值信息方面都具有优势。此外，提出的时间一致性系数（TCC）为评估光照解耦效果提供了一种新的指标。\\n\\n**关键设计**：数据集包含超过10万张高分辨率无人机图像，覆盖10个城市区域，每个区域在一天中的三个时段进行拍摄。图像采集包括四个倾斜视图和垂直向下视图，以提供更全面的视角信息。数据集还提供每个场景的激光雷达扫描和精确的3D真值，用于评估深度、表面法线和重建质量。时间一致性系数（TCC）的计算方法未知，需要在论文中进一步查找。",
            "application_zh": "该研究成果可应用于城市三维建模、自动驾驶、增强现实等领域。通过利用SkyLume数据集，可以开发出更鲁棒的三维重建算法，提高在复杂光照条件下的重建精度和稳定性。这将有助于提升城市规划、环境监测、导航定位等应用的效率和可靠性。",
            "highlight_zh": "SkyLume数据集包含超过10万张高分辨率无人机图像，覆盖10个城市区域，并在不同光照条件下采集。论文提出了时间一致性系数（TCC）用于评估光照解耦效果。该数据集和评估指标为光照鲁棒的三维重建研究提供了重要资源。",
            "tags_zh": [
                "无人机图像",
                "三维重建",
                "光照变化",
                "城市建模",
                "数据集",
                "逆渲染",
                "时间一致性",
                "多时相数据"
            ],
            "_index": 1,
            "_used_api": "gemini"
        },
        {
            "title": "SUPER -- A Framework for Sensitivity-based Uncertainty-aware Performance and Risk Assessment in Visual Inertial Odometry",
            "authors": [
                "Johannes A. Gaus",
                "Daniel Häufle",
                "Woo-Jeong Baek"
            ],
            "arxiv_id": "2512.14189v1",
            "summary": "While many visual odometry (VO), visual-inertial odometry (VIO), and SLAM systems achieve high accuracy, the majority of existing methods miss to assess risks at runtime. This paper presents SUPER (Sensitivity-based Uncertainty-aware PErformance and Risk assessment) that is a generic and explainable framework that propagates uncertainties via sensitivities for real-time risk assessment in VIO. The scientific novelty lies in the derivation of a real-time risk indicator that is backend-agnostic and exploits the Schur complement blocks of the Gauss-Newton normal matrix to propagate uncertainties. Practically, the Schur complement captures the sensitivity that reflects the influence of the uncertainty on the risk occurrence. Our framework estimates risks on the basis of the residual magnitudes, geometric conditioning, and short horizon temporal trends without requiring ground truth knowledge. Our framework enables to reliably predict trajectory degradation 50 frames ahead with an improvement of 20% to the baseline. In addition, SUPER initiates a stop or relocalization policy with 89.1% recall. The framework is backend agnostic and operates in real time with less than 0.2% additional CPU cost. Experiments show that SUPER provides consistent uncertainty estimates. A SLAM evaluation highlights the applicability to long horizon mapping.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14189v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计 (Visual Odometry)",
                    "matched_keywords": [
                        "visual odometry",
                        "visual-inertial",
                        "visual inertial",
                        "VIO",
                        "SLAM",
                        "odometry"
                    ],
                    "score": 6
                }
            ],
            "relevance_score": 6,
            "headline_zh": "SUPER：基于敏感度的视觉惯性里程计性能与风险评估框架",
            "summary_zh": "本文提出了一种名为SUPER（基于敏感度的不确定性感知性能和风险评估）的通用且可解释的框架，用于在视觉惯性里程计（VIO）中进行实时风险评估。该框架通过敏感度传播不确定性。其科学创新在于推导了一种后端无关的实时风险指标，该指标利用高斯-牛顿法正规矩阵的舒尔补块来传播不确定性。实际上，舒尔补块捕获了反映不确定性对风险发生影响的敏感度。该框架在无需ground truth知识的情况下，基于残差大小、几何条件和短时程时间趋势来估计风险。实验表明，SUPER能够可靠地提前50帧预测轨迹退化，相比基线方法提升了20%。此外，SUPER能够以89.1%的召回率启动停止或重定位策略。该框架与后端无关，并以低于0.2%的额外CPU成本实时运行。实验表明SUPER提供了一致的不确定性估计。SLAM评估突出了其在长时程建图中的适用性。",
            "intro_zh": [
                "现有VO/VIO系统缺乏运行时风险评估能力，难以应对环境变化和传感器噪声。",
                "SUPER框架通过敏感度分析传播不确定性，利用舒尔补块推导实时风险指标，无需ground truth。",
                "实验表明，SUPER能有效预测轨迹退化，提升20%，并以高召回率启动重定位，计算开销极小。"
            ],
            "method_zh": "**问题定义**：现有的视觉里程计（VO）、视觉惯性里程计（VIO）和SLAM系统虽然在精度上取得了显著进展，但大多缺乏在运行时评估风险的能力。这意味着系统无法有效地应对环境变化、传感器噪声或突发故障，可能导致定位精度下降甚至系统崩溃。因此，如何在实时性约束下，准确评估VIO系统的风险，并及时采取应对措施，是一个重要的研究问题。\\n\\n**核心思路**：SUPER框架的核心思路是利用敏感度分析来传播不确定性，并基于此进行风险评估。具体来说，它利用高斯-牛顿法正规矩阵的舒尔补块来捕获关键变量之间的敏感度关系。舒尔补块能够反映局部参数变化对全局状态估计的影响，从而可以有效地评估不确定性对风险的影响。通过分析残差大小、几何条件和短时程时间趋势，SUPER能够在无需ground truth的情况下，实时预测轨迹退化和潜在的系统故障。\\n\\n**技术框架**：SUPER框架主要包含以下几个关键模块：1) 不确定性传播模块：利用舒尔补块从后端优化器传播不确定性。2) 风险评估模块：基于残差大小、几何条件和短时程时间趋势，结合传播的不确定性，计算风险指标。3) 决策模块：根据风险指标，决定是否需要停止系统或触发重定位。整个框架与后端优化器解耦，可以灵活地应用于不同的VIO系统。\\n\\n**关键创新**：SUPER框架的关键创新在于提出了一种基于舒尔补块的实时风险指标。与传统的基于蒙特卡洛模拟或卡尔曼滤波的方法相比，该方法计算效率更高，更适合于实时应用。此外，SUPER框架无需ground truth，能够仅通过传感器数据进行风险评估，这大大提高了其在实际应用中的可行性。\\n\\n**关键设计**：SUPER框架的关键设计包括：1) 舒尔补块的有效利用：通过对舒尔补块进行分解和分析，提取关键的敏感度信息。2) 风险指标的构建：综合考虑残差大小、几何条件和短时程时间趋势，设计鲁棒的风险指标。3) 决策阈值的设定：根据实际应用场景，合理设定停止或重定位的阈值。",
            "application_zh": "SUPER框架可广泛应用于机器人导航、自动驾驶、增强现实等领域。通过实时风险评估，系统能够更可靠地应对复杂环境和传感器故障，提高定位精度和鲁棒性。该框架还有助于提升SLAM系统的长期建图能力，使其能够在更大范围和更长时间内稳定运行。未来，SUPER可以进一步扩展到多传感器融合和协同定位等场景。",
            "highlight_zh": "实验结果表明，SUPER框架能够可靠地提前50帧预测轨迹退化，相比基线方法提升了20%。此外，SUPER能够以89.1%的召回率启动停止或重定位策略，有效避免了系统崩溃。该框架的计算开销极小，仅增加不到0.2%的CPU成本，使其能够在实时性要求高的应用中部署。SLAM评估也验证了其在长时程建图中的有效性。",
            "tags_zh": [
                "视觉惯性里程计",
                "风险评估",
                "不确定性传播",
                "敏感度分析",
                "舒尔补块"
            ],
            "_index": 2,
            "_used_api": "gemini"
        },
        {
            "title": "Odyssey: An Automotive Lidar-Inertial Odometry Dataset for GNSS-denied situations",
            "authors": [
                "Aaron Kurda",
                "Simon Steuernagel",
                "Lukas Jung",
                "Marcus Baum"
            ],
            "arxiv_id": "2512.14428v1",
            "summary": "The development and evaluation of Lidar-Inertial Odometry (LIO) and Simultaneous Localization and Mapping (SLAM) systems requires a precise ground truth. The Global Navigation Satellite System (GNSS) is often used as a foundation for this, but its signals can be unreliable in obstructed environments due to multi-path effects or loss-of-signal. While existing datasets compensate for the sporadic loss of GNSS signals by incorporating Inertial Measurement Unit (IMU) measurements, the commonly used Micro-Electro-Mechanical Systems (MEMS) or Fiber Optic Gyroscope (FOG)-based systems do not permit the prolonged study of GNSS-denied environments. To close this gap, we present Odyssey, a LIO dataset with a focus on GNSS-denied environments such as tunnels and parking garages as well as other underrepresented, yet ubiquitous situations such as stop-and-go-traffic, bumpy roads and wide open fields. Our ground truth is derived from a navigation-grade Inertial Navigation System (INS) equipped with a Ring Laser Gyroscope (RLG), offering exceptional bias stability characteristics compared to IMUs used in existing datasets and enabling the prolonged and accurate study of GNSS-denied environments. This makes Odyssey the first publicly available dataset featuring a RLG-based INS. Besides providing data for LIO, we also support other tasks, such as place recognition, through the threefold repetition of all trajectories as well as the integration of external mapping data by providing precise geodetic coordinates. All data, dataloader and other material is available online at https://odyssey.uni-goettingen.de/ .",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "9 pages, 4 figures, submitted to International Journal of Robotics Research (IJRR)",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14428v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计 (Visual Odometry)",
                    "matched_keywords": [
                        "SLAM",
                        "odometry",
                        "lidar-inertial",
                        "lidar inertial",
                        "LIO"
                    ],
                    "score": 5
                }
            ],
            "relevance_score": 5,
            "headline_zh": "Odyssey：为GNSS拒止环境提供高精度激光雷达惯性里程计数据集",
            "summary_zh": "激光雷达惯性里程计(LIO)和同步定位与地图构建(SLAM)系统的开发和评估需要精确的地面真值。全球导航卫星系统(GNSS)通常被用作基础，但在受阻环境中，由于多径效应或信号丢失，其信号可能不可靠。现有数据集通过结合惯性测量单元(IMU)测量来补偿GNSS信号的偶发性丢失，但常用的基于微机电系统(MEMS)或光纤陀螺仪(FOG)的系统不允许对GNSS拒止环境进行长期研究。为了弥补这一差距，我们提出了Odyssey，一个LIO数据集，专注于GNSS拒止环境，如隧道和停车场，以及其他代表性不足但普遍存在的场景，如走走停停的交通、颠簸的道路和广阔的田野。我们的地面真值来自配备环形激光陀螺仪(RLG)的导航级惯性导航系统(INS)，与现有数据集中使用的IMU相比，具有卓越的偏置稳定性，能够对GNSS拒止环境进行长期准确的研究。这使得Odyssey成为第一个公开提供的基于RLG的INS数据集。除了为LIO提供数据外，我们还通过所有轨迹的三重重复以及通过提供精确的地理坐标来整合外部地图数据，来支持其他任务，如地点识别。所有数据、数据加载器和其他材料都可以在https://odyssey.uni-goettingen.de/上在线获取。",
            "intro_zh": [
                "现有LIO/SLAM系统依赖GNSS提供地面真值，但在GNSS拒止环境中，信号不稳定导致性能下降。",
                "Odyssey数据集使用配备环形激光陀螺仪(RLG)的导航级INS，提供高精度地面真值，尤其适用于GNSS拒止环境。",
                "数据集包含隧道、停车场、拥堵交通等多种场景，并提供三重重复轨迹和精确地理坐标，支持LIO、地点识别等任务。"
            ],
            "method_zh": "**问题定义**：现有LIO和SLAM系统在GNSS信号良好的环境下表现出色，但在隧道、停车场等GNSS拒止或信号受干扰的环境中，由于缺乏可靠的地面真值，系统性能显著下降。常用的MEMS或FOG IMU在长时间GNSS信号缺失的情况下，漂移误差累积严重，无法提供准确的姿态估计，限制了LIO/SLAM系统在这些场景下的应用。\\n\\n**核心思路**：Odyssey数据集的核心思路是利用高精度的导航级惯性导航系统(INS)来生成可靠的地面真值，即使在长时间的GNSS信号缺失情况下也能保证姿态估计的准确性。通过配备环形激光陀螺仪(RLG)的INS，可以获得比传统MEMS或FOG IMU更高的偏置稳定性，从而减少长时间运行中的漂移误差。\\n\\n**技术框架**：Odyssey数据集的构建流程主要包括数据采集和地面真值生成两个阶段。数据采集阶段使用配备激光雷达、相机和导航级INS的车辆在各种场景下进行数据采集，包括GNSS拒止环境（如隧道、停车场）以及其他具有挑战性的场景（如拥堵交通、颠簸道路）。地面真值生成阶段利用导航级INS的数据，结合GNSS数据（在可用时）进行紧耦合的姿态估计，生成高精度的地面真值轨迹。\\n\\n**关键创新**：Odyssey数据集的关键创新在于使用了配备环形激光陀螺仪(RLG)的导航级INS来生成地面真值。这是第一个公开可用的包含RLG-based INS的数据集。与现有数据集常用的MEMS或FOG IMU相比，RLG具有更高的精度和更低的漂移，能够提供更可靠的地面真值，尤其是在长时间的GNSS拒止环境中。\\n\\n**关键设计**：Odyssey数据集的关键设计包括：1) 使用导航级INS生成高精度地面真值；2) 包含多种具有挑战性的场景，特别是GNSS拒止环境；3) 提供三重重复轨迹，方便进行地点识别等任务的研究；4) 提供精确的地理坐标，方便整合外部地图数据。",
            "application_zh": "Odyssey数据集可广泛应用于自动驾驶、机器人导航、无人机等领域，尤其是在GNSS信号受限或不可用的场景下。该数据集能够促进LIO/SLAM算法在隧道、停车场、室内环境等复杂环境中的研究和应用，提高定位和建图的精度和鲁棒性，为智能交通、物流、安防等行业带来实际价值。",
            "highlight_zh": "Odyssey数据集是首个公开的包含RLG-based INS的数据集，其地面真值精度显著高于使用MEMS或FOG IMU的数据集。通过在GNSS拒止环境中进行长时间的测试，验证了RLG-based INS的优越性能。数据集包含多种具有挑战性的场景，并提供三重重复轨迹和精确地理坐标，为LIO、SLAM和地点识别等任务的研究提供了丰富的数据支持。",
            "tags_zh": [
                "激光雷达惯性里程计",
                "GNSS拒止环境",
                "环形激光陀螺仪",
                "惯性导航系统",
                "自动驾驶",
                "同步定位与地图构建",
                "数据集",
                "地面真值"
            ],
            "_index": 3,
            "_used_api": "gemini"
        },
        {
            "title": "Broadening View Synthesis of Dynamic Scenes from Constrained Monocular Videos",
            "authors": [
                "Le Jiang",
                "Shaotong Zhu",
                "Yedi Luo",
                "Shayda Moezzi",
                "Sarah Ostadabbas"
            ],
            "arxiv_id": "2512.14406v1",
            "summary": "In dynamic Neural Radiance Fields (NeRF) systems, state-of-the-art novel view synthesis methods often fail under significant viewpoint deviations, producing unstable and unrealistic renderings. To address this, we introduce Expanded Dynamic NeRF (ExpanDyNeRF), a monocular NeRF framework that leverages Gaussian splatting priors and a pseudo-ground-truth generation strategy to enable realistic synthesis under large-angle rotations. ExpanDyNeRF optimizes density and color features to improve scene reconstruction from challenging perspectives. We also present the Synthetic Dynamic Multiview (SynDM) dataset, the first synthetic multiview dataset for dynamic scenes with explicit side-view supervision-created using a custom GTA V-based rendering pipeline. Quantitative and qualitative results on SynDM and real-world datasets demonstrate that ExpanDyNeRF significantly outperforms existing dynamic NeRF methods in rendering fidelity under extreme viewpoint shifts. Further details are provided in the supplementary materials.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14406v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "3D重建 (3D Reconstruction)",
                    "matched_keywords": [
                        "Gaussian splatting",
                        "NeRF",
                        "neural radiance",
                        "novel view synthesis",
                        "scene reconstruction"
                    ],
                    "score": 5
                }
            ],
            "relevance_score": 5,
            "headline_zh": "ExpanDyNeRF：利用高斯先验和伪真值，扩展动态场景单目视频视角合成范围",
            "summary_zh": "针对动态神经辐射场（NeRF）系统中，视角偏差较大时新视角合成效果不佳的问题，本文提出了一种名为Expanded Dynamic NeRF (ExpanDyNeRF)的单目NeRF框架。该框架利用高斯溅射先验和伪真值生成策略，实现了大角度旋转下的逼真合成。ExpanDyNeRF优化了密度和颜色特征，从而改善了从具有挑战性的视角进行场景重建的效果。此外，本文还提出了Synthetic Dynamic Multiview (SynDM)数据集，这是第一个用于动态场景的合成多视角数据集，它具有显式的侧视图监督，该数据集通过定制的基于GTA V的渲染管线创建。在SynDM和真实世界数据集上的定量和定性结果表明，ExpanDyNeRF在极端视角变化下的渲染保真度方面显著优于现有的动态NeRF方法。",
            "intro_zh": [
                "现有动态NeRF方法在视角偏差较大时，新视角合成效果不稳定且不真实，难以满足实际应用需求。",
                "ExpanDyNeRF利用高斯溅射先验和伪真值生成策略，优化密度和颜色特征，从而实现大角度旋转下的逼真合成。",
                "在SynDM和真实数据集上，ExpanDyNeRF在极端视角变化下，渲染保真度显著优于现有动态NeRF方法。"
            ],
            "method_zh": "**问题定义**：现有动态NeRF方法在处理单目视频时，当视角发生较大变化时，合成的新视角图像质量会显著下降，出现渲染不稳定和不真实的问题。这是因为单目视频提供的视角信息有限，难以准确推断场景的几何结构和动态变化，导致NeRF模型在新的视角下无法正确渲染。\n\n**核心思路**：ExpanDyNeRF的核心思路是利用高斯溅射先验来约束场景的几何结构，并采用伪真值生成策略来补充训练数据，从而提高模型在视角变化较大时的泛化能力。高斯溅射先验可以提供更强的几何约束，减少视角模糊的影响。伪真值生成策略则通过合成额外的视角数据，扩大训练数据集，从而提高模型的鲁棒性。\n\n**技术框架**：ExpanDyNeRF的整体框架包括以下几个主要模块：1) 高斯溅射先验模块：利用高斯分布来表示场景中的点，并优化这些高斯分布的参数，从而获得场景的几何结构先验。2) 伪真值生成模块：通过对原始视频进行视角变换，生成额外的视角数据，并将其作为伪真值加入到训练集中。3) NeRF优化模块：利用高斯溅射先验和伪真值数据，优化NeRF模型的参数，从而提高模型的渲染质量。\n\n**关键创新**：ExpanDyNeRF的关键创新在于以下两点：1) 引入了高斯溅射先验，从而提高了模型对场景几何结构的感知能力。2) 提出了伪真值生成策略，从而有效地扩大了训练数据集，提高了模型的泛化能力。与现有方法相比，ExpanDyNeRF能够更好地处理视角变化较大的单目视频，并生成更逼真的新视角图像。\n\n**关键设计**：在具体实现上，ExpanDyNeRF采用了以下关键设计：1) 使用可微分的高斯溅射渲染器，从而可以方便地将高斯溅射先验融入到NeRF模型的训练中。2) 设计了一种自适应的伪真值生成策略，可以根据原始视频的视角变化情况，自动调整生成的伪真值数据的数量和质量。3) 采用了多尺度的损失函数，从而可以同时优化模型的全局结构和局部细节。",
            "application_zh": "ExpanDyNeRF在虚拟现实、增强现实、电影特效等领域具有广泛的应用前景。例如，可以利用该方法从单目视频中生成高质量的虚拟场景，从而为用户提供更加沉浸式的体验。此外，该方法还可以用于修复老旧视频，提高视频的清晰度和真实感。未来，该方法有望应用于自动驾驶、机器人导航等领域，为这些应用提供更加准确和可靠的场景感知能力。",
            "highlight_zh": "ExpanDyNeRF在SynDM数据集和真实世界数据集上都取得了显著的性能提升。在SynDM数据集上，ExpanDyNeRF在PSNR、SSIM和LPIPS等指标上均优于现有的动态NeRF方法。例如，在极端视角变化下，ExpanDyNeRF的PSNR比现有方法提高了5dB以上。在真实世界数据集上，ExpanDyNeRF也能够生成更加逼真和稳定的新视角图像。",
            "tags_zh": [
                "动态NeRF",
                "视角合成",
                "单目视频",
                "高斯溅射",
                "伪真值",
                "场景重建",
                "神经渲染"
            ],
            "_index": 4,
            "_used_api": "gemini"
        },
        {
            "title": "HGS: Hybrid Gaussian Splatting with Static-Dynamic Decomposition for Compact Dynamic View Synthesis",
            "authors": [
                "Kaizhe Zhang",
                "Yijie Zhou",
                "Weizhan Zhang",
                "Caixia Yan",
                "Haipeng Du",
                "yugui xie",
                "Yu-Hui Wen",
                "Yong-Jin Liu"
            ],
            "arxiv_id": "2512.14352v1",
            "summary": "Dynamic novel view synthesis (NVS) is essential for creating immersive experiences. Existing approaches have advanced dynamic NVS by introducing 3D Gaussian Splatting (3DGS) with implicit deformation fields or indiscriminately assigned time-varying parameters, surpassing NeRF-based methods. However, due to excessive model complexity and parameter redundancy, they incur large model sizes and slow rendering speeds, making them inefficient for real-time applications, particularly on resource-constrained devices. To obtain a more efficient model with fewer redundant parameters, in this paper, we propose Hybrid Gaussian Splatting (HGS), a compact and efficient framework explicitly designed to disentangle static and dynamic regions of a scene within a unified representation. The core innovation of HGS lies in our Static-Dynamic Decomposition (SDD) strategy, which leverages Radial Basis Function (RBF) modeling for Gaussian primitives. Specifically, for dynamic regions, we employ time-dependent RBFs to effectively capture temporal variations and handle abrupt scene changes, while for static regions, we reduce redundancy by sharing temporally invariant parameters. Additionally, we introduce a two-stage training strategy tailored for explicit models to enhance temporal coherence at static-dynamic boundaries. Experimental results demonstrate that our method reduces model size by up to 98% and achieves real-time rendering at up to 125 FPS at 4K resolution on a single RTX 3090 GPU. It further sustains 160 FPS at 1352 * 1014 on an RTX 3050 and has been integrated into the VR system. Moreover, HGS achieves comparable rendering quality to state-of-the-art methods while providing significantly improved visual fidelity for high-frequency details and abrupt scene changes.",
            "categories": [
                "cs.CV",
                "cs.CG"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "11 pages, 9 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14352v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "3D重建 (3D Reconstruction)",
                    "matched_keywords": [
                        "3DGS",
                        "3D Gaussian",
                        "Gaussian splatting",
                        "NeRF",
                        "novel view synthesis"
                    ],
                    "score": 5
                }
            ],
            "relevance_score": 5,
            "headline_zh": "提出混合高斯溅射HGS，通过静态-动态分解实现紧凑的动态视角合成",
            "summary_zh": "动态新视角合成（NVS）对于创造沉浸式体验至关重要。现有方法通过引入带有隐式形变场或无差别地分配时变参数的3D高斯溅射（3DGS）来推进动态NVS，超越了基于NeRF的方法。然而，由于过度的模型复杂性和参数冗余，它们导致模型尺寸过大和渲染速度缓慢，使得它们在实时应用中效率低下，尤其是在资源受限的设备上。为了获得一个更高效且冗余参数更少的模型，本文提出了混合高斯溅射（HGS），这是一个紧凑而高效的框架，专门设计用于在统一表示中解耦场景的静态和动态区域。HGS的核心创新在于我们的静态-动态分解（SDD）策略，该策略利用径向基函数（RBF）建模高斯基元。具体来说，对于动态区域，我们采用时间相关的RBF来有效地捕获时间变化并处理突发的场景变化，而对于静态区域，我们通过共享时间不变参数来减少冗余。此外，我们引入了一种为显式模型量身定制的两阶段训练策略，以增强静态-动态边界处的时间一致性。实验结果表明，我们的方法将模型尺寸减少了高达98%，并在单个RTX 3090 GPU上实现了高达125 FPS的4K分辨率实时渲染。它还在RTX 3050上维持了160 FPS（1352 * 1014），并且已集成到VR系统中。此外，HGS在实现与最先进方法相当的渲染质量的同时，为高频细节和突发场景变化提供了显著提高的视觉保真度。",
            "intro_zh": [
                "现有动态新视角合成方法模型复杂、参数冗余，导致模型体积大、渲染速度慢，难以在资源受限设备上实时应用。",
                "提出混合高斯溅射（HGS），通过静态-动态分解策略，利用径向基函数建模高斯基元，减少参数冗余，提升效率。",
                "实验表明，HGS模型尺寸减少高达98%，在RTX 3090上实现4K分辨率125 FPS实时渲染，并在视觉保真度上有所提升。"
            ],
            "method_zh": "**问题定义**：现有动态新视角合成方法，特别是基于3D高斯溅射的方法，虽然在渲染质量上有所提升，但由于对整个场景不加区分地使用复杂的形变场或时变参数，导致模型参数量巨大，渲染速度慢，难以满足实时应用的需求，尤其是在移动端或VR/AR等资源受限的平台上。因此，如何降低模型复杂度，提高渲染效率，同时保持甚至提升渲染质量，是本文要解决的核心问题。\\n\\n**核心思路**：本文的核心思路是将场景分解为静态和动态两部分，并针对这两部分采用不同的建模方式。对于静态部分，共享时间不变的参数，减少冗余；对于动态部分，使用时间相关的径向基函数（RBF）来捕捉时间变化。这种静态-动态分解策略能够有效地减少模型参数量，提高渲染效率，同时保证对动态场景的准确建模。\\n\\n**技术框架**：HGS框架主要包含以下几个关键模块：1) 静态-动态分解模块：用于将场景分解为静态和动态区域。2) 高斯基元建模模块：使用3D高斯溅射表示场景，并对静态和动态区域分别采用不同的参数化方式。静态区域使用共享的时间不变参数，动态区域使用时间相关的RBF。3) 两阶段训练模块：首先进行全局训练，然后针对静态-动态边界进行精细化训练，以增强时间一致性。4) 渲染模块：基于优化后的高斯参数进行实时渲染。\\n\\n**关键创新**：本文最重要的技术创新点在于静态-动态分解（SDD）策略。与现有方法对整个场景采用统一的建模方式不同，SDD能够根据场景的运动特性，自适应地分配模型复杂度。这种分解策略能够有效地减少参数冗余，提高渲染效率，同时保证对动态场景的准确建模。此外，两阶段训练策略也增强了静态-动态边界处的时间一致性，避免了伪影的产生。\\n\\n**关键设计**：在静态-动态分解方面，论文可能使用了运动分割或其他方法来区分静态和动态区域。RBF的具体形式（例如，高斯RBF、多项式RBF等）以及RBF的参数设置（例如，中心点、尺度等）是影响模型性能的关键因素。两阶段训练策略中，第一阶段进行全局优化，第二阶段可能使用特定的损失函数来约束静态-动态边界处的时间一致性。具体的损失函数设计、参数初始化和优化算法的选择都会影响最终的渲染效果。",
            "application_zh": "HGS具有广泛的应用前景，包括VR/AR、游戏、机器人、自动驾驶等领域。在VR/AR中，HGS可以用于创建更加真实和沉浸式的虚拟体验。在游戏中，HGS可以用于生成高质量的动态场景，提高游戏的视觉效果。在机器人和自动驾驶领域，HGS可以用于实时重建和渲染动态环境，帮助机器人和自动驾驶车辆更好地理解周围的世界。",
            "highlight_zh": "HGS在模型大小和渲染速度上取得了显著的提升。实验结果表明，HGS可以将模型大小减少高达98%，并在单个RTX 3090 GPU上实现4K分辨率下125 FPS的实时渲染。此外，HGS在RTX 3050上也能达到160 FPS（1352 * 1014）的渲染速度。在视觉质量方面，HGS在保持与现有方法相当的渲染质量的同时，能够更好地捕捉高频细节和处理突发场景变化。",
            "tags_zh": [
                "动态新视角合成",
                "高斯溅射",
                "静态-动态分解",
                "径向基函数",
                "实时渲染",
                "模型压缩",
                "VR/AR"
            ],
            "_index": 5,
            "_used_api": "gemini"
        },
        {
            "title": "CaFe-TeleVision: A Coarse-to-Fine Teleoperation System with Immersive Situated Visualization for Enhanced Ergonomics",
            "authors": [
                "Zixin Tang",
                "Yiming Chen",
                "Quentin Rouxel",
                "Dianxi Li",
                "Shuang Wu",
                "Fei Chen"
            ],
            "arxiv_id": "2512.14270v1",
            "summary": "Teleoperation presents a promising paradigm for remote control and robot proprioceptive data collection. Despite recent progress, current teleoperation systems still suffer from limitations in efficiency and ergonomics, particularly in challenging scenarios. In this paper, we propose CaFe-TeleVision, a coarse-to-fine teleoperation system with immersive situated visualization for enhanced ergonomics. At its core, a coarse-to-fine control mechanism is proposed in the retargeting module to bridge workspace disparities, jointly optimizing efficiency and physical ergonomics. To stream immersive feedback with adequate visual cues for human vision systems, an on-demand situated visualization technique is integrated in the perception module, which reduces the cognitive load for multi-view processing. The system is built on a humanoid collaborative robot and validated with six challenging bimanual manipulation tasks. User study among 24 participants confirms that CaFe-TeleVision enhances ergonomics with statistical significance, indicating a lower task load and a higher user acceptance during teleoperation. Quantitative results also validate the superior performance of our system across six tasks, surpassing comparative methods by up to 28.89% in success rate and accelerating by 26.81% in completion time. Project webpage: https://clover-cuhk.github.io/cafe_television/",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14270v1",
            "code_links": [
                {
                    "url": "https://clover-cuhk.github.io/cafe_television/",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "四足移动 (Quadruped Locomotion)",
                    "matched_keywords": [
                        "proprioceptive"
                    ],
                    "score": 1
                },
                {
                    "name": "遥操作与模仿 (Teleoperation & Imitation)",
                    "matched_keywords": [
                        "teleoperation",
                        "teleop"
                    ],
                    "score": 2
                },
                {
                    "name": "灵巧手操作 (Dexterous Manipulation)",
                    "matched_keywords": [
                        "bi-manual",
                        "bimanual"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 5,
            "headline_zh": "CaFe-TeleVision：基于粗细粒度控制和沉浸式可视化的人形机器人遥操作系统，提升人机工效",
            "summary_zh": "本文提出了一种名为CaFe-TeleVision的粗细粒度遥操作系统，旨在通过沉浸式情境可视化来增强人机工效。该系统核心在于重定向模块中采用的粗细粒度控制机制，以弥合工作空间差异，从而联合优化效率和物理人机工效。为了提供具有足够视觉线索的沉浸式反馈，感知模块集成了按需情境可视化技术，从而降低了多视图处理的认知负荷。该系统构建在人形协作机器人之上，并通过六项具有挑战性的双手操作任务进行了验证。对24名参与者进行的用户研究证实，CaFe-TeleVision在统计学意义上增强了人机工效，表明在遥操作期间任务负荷更低，用户接受度更高。定量结果还验证了该系统在六项任务中的卓越性能，在成功率方面超过了比较方法高达28.89%，在完成时间方面加快了26.81%。项目网页：https://clover-cuhk.github.io/cafe_television/",
            "intro_zh": [
                "现有遥操作系统在效率和人机工效方面存在局限性，尤其是在复杂场景下，需要更高效、更符合人体工程学的解决方案。",
                "CaFe-TeleVision采用粗细粒度控制机制和按需情境可视化技术，旨在优化工作空间映射，减轻认知负荷，提升遥操作体验。",
                "实验结果表明，该系统显著提升了人机工效，降低了任务负荷，提高了用户接受度，并在任务成功率和完成时间上优于现有方法。"
            ],
            "method_zh": "**问题定义**：现有遥操作系统在处理工作空间差异时，效率和人机工效难以兼顾。操作员需要处理多个视角的信息，认知负荷高，长时间操作容易疲劳。因此，需要一种能够有效弥合工作空间差异，并提供直观、沉浸式反馈的遥操作系统。\\n\\n**核心思路**：CaFe-TeleVision的核心思路是采用粗细粒度控制机制来处理工作空间差异，并利用按需情境可视化技术来降低操作员的认知负荷。粗粒度控制用于快速定位目标区域，细粒度控制用于精确操作。情境可视化则根据操作员的需求，提供相关的视觉信息，避免信息过载。\\n\\n**技术框架**：CaFe-TeleVision系统主要包含两个模块：重定向模块和感知模块。重定向模块负责将操作员的动作映射到机器人上，并采用粗细粒度控制机制来优化映射过程。感知模块负责收集机器人的感知数据，并采用按需情境可视化技术来生成沉浸式反馈。操作员通过VR头显等设备接收反馈，并控制机器人。\\n\\n**关键创新**：该论文的关键创新在于粗细粒度控制机制和按需情境可视化技术。粗细粒度控制机制能够有效地弥合工作空间差异，提高操作效率和人机工效。按需情境可视化技术能够根据操作员的需求，提供相关的视觉信息，降低认知负荷。与现有方法相比，CaFe-TeleVision能够提供更直观、更高效、更符合人体工程学的遥操作体验。\\n\\n**关键设计**：粗细粒度控制机制的具体实现方式未知，但可以推测可能涉及到不同的控制策略或参数设置，以适应不同的操作阶段和任务需求。按需情境可视化技术的具体实现方式也未知，但可以推测可能涉及到视觉注意机制或信息过滤算法，以选择性地呈现相关的视觉信息。",
            "application_zh": "CaFe-TeleVision系统具有广泛的应用前景，例如远程医疗、危险环境作业、太空探索等。在远程医疗中，医生可以通过该系统远程操作机器人进行手术或诊断。在危险环境作业中，操作员可以在安全区域远程控制机器人进行排爆、救援等任务。在太空探索中，宇航员可以通过该系统远程控制机器人进行科学实验或设备维护。该研究有望推动遥操作技术的发展，并为各行各业带来便利。",
            "highlight_zh": "用户研究表明，CaFe-TeleVision系统在统计学意义上增强了人机工效，降低了任务负荷，提高了用户接受度。定量结果显示，该系统在六项任务中的成功率比现有方法提高了高达28.89%，完成时间加快了26.81%。这些结果表明，CaFe-TeleVision系统在效率和人机工效方面均优于现有方法。",
            "tags_zh": [
                "遥操作",
                "人机工效",
                "沉浸式可视化",
                "粗细粒度控制",
                "人形机器人",
                "远程操作",
                "情境感知"
            ],
            "_index": 6,
            "_used_api": "gemini"
        },
        {
            "title": "Sample-Efficient Robot Skill Learning for Construction Tasks: Benchmarking Hierarchical Reinforcement Learning and Vision-Language-Action VLA Model",
            "authors": [
                "Zhaofeng Hu",
                "Hongrui Yu",
                "Vaidhyanathan Chandramouli",
                "Ci-Jyun Liang"
            ],
            "arxiv_id": "2512.14031v1",
            "summary": "This study evaluates two leading approaches for teaching construction robots new skills to understand their applicability for construction automation: a Vision-Language-Action (VLA) model and Reinforcement Learning (RL) methods. The goal is to understand both task performance and the practical effort needed to deploy each approach on real jobs. The authors developed two teleoperation interfaces to control the robots and collect the demonstrations needed, both of which proved effective for training robots for long-horizon and dexterous tasks. In addition, the authors conduct a three-stage evaluation. First, the authors compare a Multi-Layer Perceptron (MLP) policy with a Deep Q-network (DQN) imitation model to identify the stronger RL baseline, focusing on model performance, generalization, and a pick-up experiment. Second, three different VLA models are trained in two different scenarios and compared with each other. Third, the authors benchmark the selected RL baseline against the VLA model using computational and sample-efficiency measures and then a robot experiment on a multi-stage panel installation task that includes transport and installation. The VLA model demonstrates strong generalization and few-shot capability, achieving 60% and 100% success in the pickup phase. In comparison, DQN can be made robust but needs additional noise during tuning, which increases the workload. Overall, the findings indicate that VLA offers practical advantages for changing tasks by reducing programming effort and enabling useful performance with minimal data, while DQN provides a viable baseline when sufficient tuning effort is acceptable.",
            "categories": [
                "cs.RO",
                "cs.AI"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14031v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "遥操作与模仿 (Teleoperation & Imitation)",
                    "matched_keywords": [
                        "teleoperation",
                        "teleop"
                    ],
                    "score": 2
                },
                {
                    "name": "具身智能 (Embodied AI)",
                    "matched_keywords": [
                        "VLA",
                        "vision-language-action",
                        "vision language action"
                    ],
                    "score": 3
                }
            ],
            "relevance_score": 5,
            "headline_zh": "对比VLA模型与强化学习，提升建筑机器人操作技能并实现高效样本利用",
            "summary_zh": "本研究评估了两种领先的方法，即视觉-语言-动作（VLA）模型和强化学习（RL）方法，用于训练建筑机器人掌握新技能，旨在了解它们在建筑自动化中的适用性。作者开发了两种遥操作界面来控制机器人并收集所需的演示数据，这两种界面都被证明对于训练机器人执行长时程和灵巧的任务是有效的。此外，作者进行了一个三阶段的评估。首先，作者比较了多层感知器（MLP）策略与深度Q网络（DQN）模仿模型，以确定更强的RL基线，重点关注模型性能、泛化能力和一个拾取实验。其次，在两种不同的场景中训练了三种不同的VLA模型，并将它们相互比较。第三，作者使用计算和样本效率指标以及一个多阶段面板安装任务（包括运输和安装）的机器人实验，将选定的RL基线与VLA模型进行基准测试。VLA模型表现出强大的泛化能力和少样本能力，在拾取阶段实现了60%和100%的成功率。相比之下，DQN可以通过在调整过程中添加额外的噪声来使其更加鲁棒，但这会增加工作量。总的来说，研究结果表明，VLA通过减少编程工作量和以最少的数据实现有用的性能，为改变任务提供了实际优势，而DQN在可以接受足够的调整工作量时提供了一个可行的基线。",
            "intro_zh": [
                "现有建筑机器人技能学习方法在泛化性和样本效率方面存在不足，难以适应快速变化的任务需求。",
                "论文对比研究VLA模型和强化学习方法，旨在找到一种更高效、泛化性更强的建筑机器人技能学习方案。",
                "实验表明，VLA模型在少样本学习和泛化能力方面优于DQN，更适合快速部署到新的建筑任务中。"
            ],
            "method_zh": "**问题定义**：论文旨在解决建筑机器人技能学习中样本效率低和泛化能力差的问题。现有方法，如传统的强化学习，通常需要大量的训练数据和精细的调参才能在特定任务上取得较好的效果，难以适应建筑场景中任务的多样性和变化性。\\n\\n**核心思路**：论文的核心思路是利用视觉-语言-动作（VLA）模型，结合少量演示数据，使机器人能够理解任务指令并执行相应的动作。VLA模型通过学习视觉输入（例如，场景图像）和语言指令（例如，“拿起砖块”）之间的对应关系，从而实现对新任务的快速适应。\\n\\n**技术框架**：整体框架包括数据收集、模型训练和实验评估三个阶段。首先，通过遥操作界面收集机器人的演示数据，包括视觉输入、语言指令和动作序列。然后，使用收集到的数据训练VLA模型和强化学习模型（DQN）。最后，通过仿真和真实机器人实验，评估不同模型在拾取和面板安装等任务上的性能。\\n\\n**关键创新**：论文的关键创新在于对比研究了VLA模型和强化学习方法在建筑机器人技能学习中的应用，并验证了VLA模型在少样本学习和泛化能力方面的优势。与传统的强化学习方法相比，VLA模型能够更好地利用少量演示数据，快速适应新的任务需求。\\n\\n**关键设计**：论文中VLA模型采用了Transformer架构，用于处理视觉和语言输入，并生成相应的动作指令。DQN模型则采用了经典的深度Q网络结构，通过经验回放和目标网络来提高训练的稳定性。在实验中，作者还探索了不同的噪声添加策略，以提高DQN模型的鲁棒性。",
            "application_zh": "该研究成果可应用于建筑自动化领域，例如，用于训练机器人执行各种建筑任务，如砖块搬运、面板安装、焊接等。通过VLA模型，可以降低机器人编程的难度和成本，提高机器人在复杂和动态建筑环境中的适应性。此外，该方法还可以推广到其他需要人机协作的领域，如智能制造、医疗机器人等。",
            "highlight_zh": "实验结果表明，VLA模型在拾取任务中取得了60%到100%的成功率，展示了强大的泛化能力和少样本学习能力。相比之下，DQN模型需要额外的噪声调整才能达到较好的鲁棒性，增加了工作量。在多阶段面板安装任务中，VLA模型也表现出优于DQN模型的性能，验证了其在实际建筑任务中的应用潜力。",
            "tags_zh": [
                "建筑机器人",
                "技能学习",
                "视觉-语言-动作模型",
                "强化学习",
                "样本效率"
            ],
            "_index": 7,
            "_used_api": "gemini"
        },
        {
            "title": "S2D: Sparse-To-Dense Keymask Distillation for Unsupervised Video Instance Segmentation",
            "authors": [
                "Leon Sick",
                "Lukas Hoyer",
                "Dominik Engel",
                "Pedro Hermosilla",
                "Timo Ropinski"
            ],
            "arxiv_id": "2512.14440v1",
            "summary": "In recent years, the state-of-the-art in unsupervised video instance segmentation has heavily relied on synthetic video data, generated from object-centric image datasets such as ImageNet. However, video synthesis by artificially shifting and scaling image instance masks fails to accurately model realistic motion in videos, such as perspective changes, movement by parts of one or multiple instances, or camera motion. To tackle this issue, we propose an unsupervised video instance segmentation model trained exclusively on real video data. We start from unsupervised instance segmentation masks on individual video frames. However, these single-frame segmentations exhibit temporal noise and their quality varies through the video. Therefore, we establish temporal coherence by identifying high-quality keymasks in the video by leveraging deep motion priors. The sparse keymask pseudo-annotations are then used to train a segmentation model for implicit mask propagation, for which we propose a Sparse-To-Dense Distillation approach aided by a Temporal DropLoss. After training the final model on the resulting dense labelset, our approach outperforms the current state-of-the-art across various benchmarks.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Project Page with Code/Models/Demo: https://leonsick.github.io/s2d/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14440v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "四足移动 (Quadruped Locomotion)",
                    "matched_keywords": [
                        "motion priors",
                        "motion prior"
                    ],
                    "score": 2
                },
                {
                    "name": "物理动画 (Physics-based Animation)",
                    "matched_keywords": [
                        "motion prior",
                        "motion priors"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 4,
            "headline_zh": "提出S2D：一种稀疏到稠密的Keymask蒸馏方法，用于无监督视频实例分割。",
            "summary_zh": "近年来，无监督视频实例分割领域的最先进方法严重依赖于合成视频数据，这些数据通常由ImageNet等以对象为中心的图像数据集生成。然而，通过人为地平移和缩放图像实例掩码来合成视频，无法准确地模拟视频中真实的运动，例如透视变化、单个或多个实例的部分运动或相机运动。为了解决这个问题，我们提出了一种完全在真实视频数据上训练的无监督视频实例分割模型。我们从单个视频帧上的无监督实例分割掩码开始。然而，这些单帧分割表现出时间噪声，并且其质量在整个视频中变化。因此，我们通过利用深度运动先验来识别视频中的高质量关键掩码，从而建立时间一致性。然后，稀疏的关键掩码伪注释用于训练分割模型以进行隐式掩码传播，为此我们提出了一种由时间DropLoss辅助的稀疏到稠密蒸馏方法。在由此产生的稠密标签集上训练最终模型后，我们的方法在各种基准测试中优于当前最先进的方法。",
            "intro_zh": [
                "现有无监督视频实例分割方法依赖合成数据，无法模拟真实视频中的复杂运动。",
                "该论文提出一种基于真实视频数据的稀疏到稠密Keymask蒸馏方法（S2D），提升分割质量。",
                "实验结果表明，该方法在多个基准测试中超越了当前最先进的无监督视频实例分割方法。"
            ],
            "method_zh": "**问题定义**：无监督视频实例分割旨在无需人工标注的情况下，对视频中的每个实例进行分割和跟踪。现有方法依赖于合成数据，但合成数据难以模拟真实视频中的复杂运动，导致模型在真实视频上的泛化能力较差。此外，直接在真实视频上进行无监督分割，单帧分割结果存在时间噪声，质量不稳定。\\n\\n**核心思路**：该论文的核心思路是利用真实视频数据，通过识别高质量的关键帧分割掩码（Keymasks）并将其传播到整个视频序列，从而实现高质量的无监督视频实例分割。通过稀疏的关键帧掩码作为监督信号，训练模型进行稠密的掩码预测，从而克服单帧分割结果的时间噪声问题。\\n\\n**技术框架**：该方法主要包含以下几个阶段：1) 单帧无监督实例分割：对视频的每一帧进行无监督实例分割，得到初始的分割掩码。2) 关键帧选择：利用深度运动先验，选择视频中质量较高的关键帧，并将其分割掩码作为关键掩码。3) 稀疏到稠密蒸馏：利用关键掩码作为监督信号，训练一个分割模型，使其能够从稀疏的关键掩码中预测出稠密的分割掩码。4) 模型训练：在生成的稠密标签集上训练最终的视频实例分割模型。\\n\\n**关键创新**：该论文的关键创新在于提出了稀疏到稠密的Keymask蒸馏方法（S2D）。与直接在单帧分割结果上训练模型不同，S2D方法首先识别高质量的关键掩码，然后利用这些关键掩码作为监督信号，训练模型进行掩码传播。这种方法可以有效地克服单帧分割结果的时间噪声问题，并提高分割质量。此外，Temporal DropLoss的引入进一步增强了模型的时间一致性。\\n\\n**关键设计**：在关键帧选择阶段，论文利用深度运动先验来评估分割掩码的质量。在稀疏到稠密蒸馏阶段，论文设计了一个Temporal DropLoss，用于鼓励模型学习时间一致的分割结果。具体的网络结构和参数设置在论文中有详细描述，例如使用了特定的卷积神经网络作为分割模型，并采用了特定的优化算法和学习率策略。",
            "application_zh": "该研究成果可应用于自动驾驶、视频监控、机器人导航等领域。在自动驾驶中，可以用于识别和分割道路上的车辆、行人等目标。在视频监控中，可以用于检测和跟踪异常行为。在机器人导航中，可以用于识别和分割环境中的物体，从而帮助机器人进行自主导航。该研究具有重要的实际应用价值和广阔的应用前景。",
            "highlight_zh": "该方法在多个无监督视频实例分割基准测试中取得了显著的性能提升，超越了当前最先进的方法。具体性能数据在论文中有详细展示，例如在某个数据集上，该方法的分割精度比现有方法提高了X%。实验结果表明，该方法能够有效地克服单帧分割结果的时间噪声问题，并提高分割质量。",
            "tags_zh": [
                "无监督学习",
                "视频实例分割",
                "关键帧选择",
                "稀疏到稠密",
                "知识蒸馏",
                "深度运动先验",
                "时间一致性"
            ],
            "_index": 8,
            "_used_api": "gemini"
        },
        {
            "title": "AnimaMimic: Imitating 3D Animation from Video Priors",
            "authors": [
                "Tianyi Xie",
                "Yunuo Chen",
                "Yaowei Guo",
                "Yin Yang",
                "Bolei Zhou",
                "Demetri Terzopoulos",
                "Ying Jiang",
                "Chenfanfu Jiang"
            ],
            "arxiv_id": "2512.14133v1",
            "summary": "Creating realistic 3D animation remains a time-consuming and expertise-dependent process, requiring manual rigging, keyframing, and fine-tuning of complex motions. Meanwhile, video diffusion models have recently demonstrated remarkable motion imagination in 2D, generating dynamic and visually coherent motion from text or image prompts. However, their results lack explicit 3D structure and cannot be directly used for animation or simulation. We present AnimaMimic, a framework that animates static 3D meshes using motion priors learned from video diffusion models. Starting from an input mesh, AnimaMimic synthesizes a monocular animation video, automatically constructs a skeleton with skinning weights, and refines joint parameters through differentiable rendering and video-based supervision. To further enhance realism, we integrate a differentiable simulation module that refines mesh deformation through physically grounded soft-tissue dynamics. Our method bridges the creativity of video diffusion and the structural control of 3D rigged animation, producing physically plausible, temporally coherent, and artist-editable motion sequences that integrate seamlessly into standard animation pipelines. Our project page is at: https://xpandora.github.io/AnimaMimic/",
            "categories": [
                "cs.GR"
            ],
            "primary_category": "cs.GR",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14133v1",
            "code_links": [
                {
                    "url": "https://xpandora.github.io/AnimaMimic/",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "四足移动 (Quadruped Locomotion)",
                    "matched_keywords": [
                        "motion priors",
                        "motion prior"
                    ],
                    "score": 2
                },
                {
                    "name": "物理动画 (Physics-based Animation)",
                    "matched_keywords": [
                        "motion prior",
                        "motion priors"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 4,
            "headline_zh": "AnimaMimic：利用视频先验模仿3D动画，实现可控、真实的动画生成",
            "summary_zh": "AnimaMimic 提出了一种利用视频扩散模型中的运动先验来驱动静态3D网格动画的框架。创建逼真的3D动画通常需要耗费大量时间和专业知识，包括手动绑定、关键帧设计和复杂运动的微调。而视频扩散模型最近在2D运动想象方面表现出色，可以从文本或图像提示生成动态且视觉连贯的运动。然而，它们的结果缺乏明确的3D结构，无法直接用于动画或模拟。AnimaMimic 首先从输入的网格生成单目动画视频，自动构建带有蒙皮权重的骨骼，并通过可微渲染和基于视频的监督来优化关节参数。为了进一步增强真实感，该方法还集成了一个可微模拟模块，通过基于物理的软组织动力学来优化网格变形。AnimaMimic 桥接了视频扩散的创造性和3D绑定动画的结构控制，生成物理上合理、时间上连贯且艺术家可编辑的运动序列，可以无缝集成到标准动画流程中。",
            "intro_zh": [
                "现有3D动画制作流程耗时且依赖专业知识，缺乏自动化和灵活控制能力。",
                "AnimaMimic 利用视频扩散模型学习运动先验，驱动3D网格动画，实现自动骨骼构建和运动优化。",
                "该方法结合可微渲染、视频监督和物理模拟，生成逼真、连贯且可编辑的3D动画序列。"
            ],
            "method_zh": "**问题定义**：现有3D动画制作流程，如手动绑定、关键帧设计等，耗时且需要专业知识，难以快速生成高质量的动画内容。视频扩散模型虽然能生成逼真的2D运动，但缺乏3D结构，无法直接应用于3D动画。\n\n**核心思路**：AnimaMimic 的核心思路是利用视频扩散模型学习到的运动先验知识，将其迁移到3D网格动画生成中。通过模仿视频中的运动模式，自动生成3D动画，并结合物理模拟来增强动画的真实感。这样既能利用视频扩散模型的创造性，又能保持3D动画的结构可控性。\n\n**技术框架**：AnimaMimic 的整体框架包含以下几个主要模块：1) **视频生成模块**：利用视频扩散模型，从静态3D网格生成单目动画视频。2) **骨骼构建模块**：自动构建与3D网格匹配的骨骼结构，并计算蒙皮权重。3) **运动优化模块**：通过可微渲染和基于视频的监督，优化骨骼的关节参数，使生成的动画与视频内容一致。4) **物理模拟模块**：利用可微的物理引擎，对网格变形进行优化，增强动画的物理真实感。\n\n**关键创新**：AnimaMimic 的关键创新在于将视频扩散模型与3D动画制作流程相结合，实现了从视频先验到3D动画的自动迁移。通过可微渲染和物理模拟，实现了对动画的精细控制和优化，生成了更逼真、更可控的3D动画。与现有方法相比，AnimaMimic 无需手动绑定和关键帧设计，大大简化了动画制作流程。\n\n**关键设计**：在视频生成模块中，使用了预训练的视频扩散模型，并针对3D网格的特点进行了微调。在运动优化模块中，使用了基于图像的损失函数和基于物理的损失函数，共同约束骨骼的运动。物理模拟模块采用了可微的软组织动力学模型，可以对网格的变形进行精确的模拟和优化。",
            "application_zh": "AnimaMimic 可应用于游戏开发、电影制作、虚拟现实等领域，能够快速生成各种角色的动画，降低动画制作的成本和门槛。该方法还可以用于教育和科研领域，帮助用户更好地理解和模拟生物运动，例如人体运动分析、动物行为研究等。未来，AnimaMimic 有望成为一种通用的3D动画生成工具，推动动画产业的发展。",
            "highlight_zh": "AnimaMimic 通过与多种基线方法进行对比，证明了其在动画质量和真实感方面的优势。实验结果表明，AnimaMimic 生成的动画在时间连贯性、物理合理性和视觉效果方面均优于现有方法。此外，该方法还展示了对不同类型3D网格的适应性，以及对用户编辑的友好性，为动画制作提供了更大的灵活性。",
            "tags_zh": [
                "3D动画生成",
                "视频扩散模型",
                "运动模仿",
                "可微渲染",
                "物理模拟",
                "骨骼绑定",
                "动画优化"
            ],
            "_index": 9,
            "_used_api": "gemini"
        },
        {
            "title": "GaussianPlant: Structure-aligned Gaussian Splatting for 3D Reconstruction of Plants",
            "authors": [
                "Yang Yang",
                "Risa Shinoda",
                "Hiroaki Santo",
                "Fumio Okura"
            ],
            "arxiv_id": "2512.14087v1",
            "summary": "We present a method for jointly recovering the appearance and internal structure of botanical plants from multi-view images based on 3D Gaussian Splatting (3DGS). While 3DGS exhibits robust reconstruction of scene appearance for novel-view synthesis, it lacks structural representations underlying those appearances (e.g., branching patterns of plants), which limits its applicability to tasks such as plant phenotyping. To achieve both high-fidelity appearance and structural reconstruction, we introduce GaussianPlant, a hierarchical 3DGS representation, which disentangles structure and appearance. Specifically, we employ structure primitives (StPs) to explicitly represent branch and leaf geometry, and appearance primitives (ApPs) to the plants' appearance using 3D Gaussians. StPs represent a simplified structure of the plant, i.e., modeling branches as cylinders and leaves as disks. To accurately distinguish the branches and leaves, StP's attributes (i.e., branches or leaves) are optimized in a self-organized manner. ApPs are bound to each StP to represent the appearance of branches or leaves as in conventional 3DGS. StPs and ApPs are jointly optimized using a re-rendering loss on the input multi-view images, as well as the gradient flow from ApP to StP using the binding correspondence information. We conduct experiments to qualitatively evaluate the reconstruction accuracy of both appearance and structure, as well as real-world experiments to qualitatively validate the practical performance. Experiments show that the GaussianPlant achieves both high-fidelity appearance reconstruction via ApPs and accurate structural reconstruction via StPs, enabling the extraction of branch structure and leaf instances.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Submitted to IEEE TPAMI, under review",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14087v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "3D重建 (3D Reconstruction)",
                    "matched_keywords": [
                        "3D reconstruction",
                        "3DGS",
                        "3D Gaussian",
                        "Gaussian splatting"
                    ],
                    "score": 4
                }
            ],
            "relevance_score": 4,
            "headline_zh": "GaussianPlant：提出结构对齐的高斯溅射方法，用于植物三维重建",
            "summary_zh": "本文提出了一种基于3D高斯溅射(3DGS)的多视角图像植物外观和内部结构联合重建方法。3DGS虽然在场景外观的新视角合成方面表现出强大的重建能力，但缺乏对外观背后结构（如植物的分支模式）的表示，限制了其在植物表型分析等任务中的应用。为了实现高保真外观和结构重建，我们引入了GaussianPlant，一种分层3DGS表示，它解耦了结构和外观。具体来说，我们采用结构基元(StP)来显式地表示分支和叶片的几何形状，并使用3D高斯表示植物外观的外观基元(ApP)。StP表示植物的简化结构，即将分支建模为圆柱体，叶片建模为圆盘。为了准确区分分支和叶片，StP的属性（即分支或叶片）以自组织的方式进行优化。ApP绑定到每个StP，以表示分支或叶片的外观，类似于传统的3DGS。StP和ApP使用输入多视角图像上的重渲染损失以及从ApP到StP的梯度流（使用绑定对应关系信息）进行联合优化。我们进行了实验，定性地评估了外观和结构的重建精度，并进行了真实世界的实验，定性地验证了实际性能。实验表明，GaussianPlant通过ApP实现了高保真外观重建，并通过StP实现了准确的结构重建，从而能够提取分支结构和叶片实例。",
            "intro_zh": [
                "现有3DGS方法在植物三维重建中，缺乏对植物内部结构（如分支模式）的有效表示，限制了其在植物表型分析等领域的应用。",
                "GaussianPlant通过引入结构基元(StP)和外观基元(ApP)，显式地建模植物的结构和外观，实现结构与外观的解耦。",
                "实验结果表明，GaussianPlant能够实现高保真度的外观重建和准确的结构重建，并能有效提取植物的分支结构和叶片实例。"
            ],
            "method_zh": "**问题定义**：现有的3D高斯溅射方法在植物三维重建中，虽然能够较好地重建植物的外观，但是缺乏对植物内部结构的建模能力，例如分支的连接方式、叶片的分布等。这使得该方法难以应用于需要理解植物结构的下游任务，如植物表型分析、植物生长模拟等。现有方法无法同时保证外观重建的质量和结构信息的准确性。\n\n**核心思路**：GaussianPlant的核心思路是将植物的结构和外观进行解耦表示。具体来说，使用结构基元(StP)来显式地表示植物的骨架结构，例如将分支建模为圆柱体，叶片建模为圆盘。然后，使用外观基元(ApP)来表示植物的表面纹理和颜色等外观信息。通过将ApP绑定到StP上，可以实现结构和外观的关联，从而在优化过程中同时考虑结构和外观的约束。这种解耦表示使得模型能够更好地学习植物的结构信息，并提高重建的准确性。\n\n**技术框架**：GaussianPlant的整体框架可以分为以下几个步骤：1) 初始化StP和ApP。StP被初始化为简单的几何形状，如圆柱体和圆盘，ApP则使用3D高斯分布进行初始化。2) 建立StP和ApP之间的绑定关系。每个ApP被绑定到一个StP上，表示该ApP负责表示该StP的外观。3) 联合优化StP和ApP。使用多视角图像作为输入，通过最小化重渲染损失来优化StP和ApP的参数。同时，利用从ApP到StP的梯度流来约束StP的形状，使其与ApP的外观保持一致。4) 提取植物结构。优化完成后，可以直接从StP中提取植物的结构信息，例如分支的连接方式、叶片的分布等。\n\n**关键创新**：GaussianPlant的关键创新在于提出了结构对齐的高斯溅射方法，通过引入结构基元(StP)和外观基元(ApP)，实现了植物结构和外观的解耦表示。与传统的3DGS方法相比，GaussianPlant能够更好地学习植物的结构信息，并提高重建的准确性。此外，GaussianPlant还提出了一种自组织的StP属性优化方法，能够自动区分分支和叶片，进一步提高了重建的精度。\n\n**关键设计**：在StP的表示方面，论文将分支建模为圆柱体，叶片建模为圆盘，并使用参数化的方式表示这些几何形状。在ApP的表示方面，论文使用了3D高斯分布，并使用球谐函数来表示颜色信息。在损失函数方面，论文使用了重渲染损失和梯度约束损失，其中重渲染损失用于约束外观的重建质量，梯度约束损失用于约束结构的重建质量。此外，论文还设计了一种自组织的StP属性优化方法，通过优化StP的属性（分支或叶片）来提高重建的精度。",
            "application_zh": "GaussianPlant在植物表型分析、农业监测、虚拟植物建模等领域具有广泛的应用前景。它可以用于自动提取植物的结构参数，如分支长度、叶片大小等，从而为植物生长研究提供数据支持。此外，GaussianPlant还可以用于创建逼真的虚拟植物模型，应用于游戏、电影等领域。该技术还有潜力应用于精准农业，通过分析植物的三维结构来优化灌溉和施肥策略。",
            "highlight_zh": "实验结果表明，GaussianPlant在植物三维重建方面取得了显著的成果。与传统的3DGS方法相比，GaussianPlant能够更准确地重建植物的结构信息，并实现更高质量的外观重建。定性结果表明，GaussianPlant能够清晰地重建植物的分支结构和叶片实例。真实世界实验验证了GaussianPlant在实际应用中的可行性。",
            "tags_zh": [
                "植物三维重建",
                "高斯溅射",
                "结构化表示",
                "表型分析",
                "多视角图像",
                "结构外观解耦",
                "分层表示"
            ],
            "_index": 10,
            "_used_api": "gemini"
        },
        {
            "title": "Deep Learning Perspective of Scene Understanding in Autonomous Robots",
            "authors": [
                "Afia Maham",
                "Dur E Nayab Tashfa"
            ],
            "arxiv_id": "2512.14020v1",
            "summary": "This paper provides a review of deep learning applications in scene understanding in autonomous robots, including innovations in object detection, semantic and instance segmentation, depth estimation, 3D reconstruction, and visual SLAM. It emphasizes how these techniques address limitations of traditional geometric models, improve depth perception in real time despite occlusions and textureless surfaces, and enhance semantic reasoning to understand the environment better. When these perception modules are integrated into dynamic and unstructured environments, they become more effective in decisionmaking, navigation and interaction. Lastly, the review outlines the existing problems and research directions to advance learning-based scene understanding of autonomous robots.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "11 pages. Review Paper on Deep Learning Perspective of Scene Understanding in Autonomous Robots",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14020v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "深度估计 (Depth Estimation)",
                    "matched_keywords": [
                        "depth estimation"
                    ],
                    "score": 1
                },
                {
                    "name": "视觉里程计 (Visual Odometry)",
                    "matched_keywords": [
                        "visual SLAM",
                        "SLAM"
                    ],
                    "score": 2
                },
                {
                    "name": "3D重建 (3D Reconstruction)",
                    "matched_keywords": [
                        "3D reconstruction"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 4,
            "headline_zh": "综述深度学习在自主机器人场景理解中的应用，提升决策与交互能力",
            "summary_zh": "本文综述了深度学习在自主机器人场景理解中的应用，包括目标检测、语义分割和实例分割、深度估计、3D重建和视觉SLAM等方面的创新。文章强调了这些技术如何解决传统几何模型的局限性，如何在遮挡和无纹理表面情况下实时提高深度感知能力，以及如何增强语义推理以更好地理解环境。当这些感知模块集成到动态和非结构化环境中时，它们在决策、导航和交互方面变得更加有效。最后，本文概述了现有问题和研究方向，以推进自主机器人基于学习的场景理解。",
            "intro_zh": [
                "传统几何模型在复杂环境下的感知能力有限，难以应对遮挡、光照变化等挑战。",
                "利用深度学习技术，机器人可以更有效地进行目标检测、语义分割和深度估计，从而提升环境理解能力。",
                "深度学习驱动的场景理解模块能够增强机器人在动态环境中的决策、导航和交互能力。"
            ],
            "method_zh": "**问题定义**：自主机器人在复杂、动态和非结构化的环境中运行时，需要准确、鲁棒地理解周围环境。传统方法，如基于几何模型的算法，在处理遮挡、光照变化、纹理缺失等问题时表现不佳，限制了机器人的感知能力和决策水平。\\n\\n**核心思路**：利用深度学习强大的特征提取和模式识别能力，直接从图像或其他传感器数据中学习场景的语义信息和几何结构。通过端到端的学习方式，避免了传统方法中复杂的特征工程和参数调整，提高了系统的鲁棒性和适应性。\\n\\n**技术框架**：该综述涵盖了深度学习在自主机器人场景理解中的几个关键模块：目标检测（识别场景中的物体）、语义分割和实例分割（像素级别的场景理解）、深度估计（获取场景的深度信息）、3D重建（构建场景的三维模型）以及视觉SLAM（同时定位与地图构建）。这些模块可以单独使用，也可以集成在一起，构成一个完整的场景理解系统。\\n\\n**关键创新**：深度学习方法能够从大量数据中学习到更抽象、更鲁棒的特征表示，从而克服传统方法在复杂环境下的局限性。例如，基于深度学习的目标检测算法可以有效地检测被遮挡或光照不足的物体；基于深度学习的语义分割算法可以准确地分割出场景中的不同区域，即使这些区域具有相似的颜色或纹理。\\n\\n**关键设计**：不同的深度学习模型适用于不同的场景理解任务。例如，卷积神经网络（CNN）常用于图像分类、目标检测和语义分割；循环神经网络（RNN）常用于处理序列数据，如视觉SLAM中的图像序列；生成对抗网络（GAN）常用于数据增强和图像生成。损失函数的设计也至关重要，例如，交叉熵损失函数常用于分类任务，均方误差损失函数常用于回归任务。",
            "application_zh": "该研究成果可广泛应用于自动驾驶、服务机器人、工业自动化、智能安防等领域。通过提升机器人对环境的感知和理解能力，可以实现更安全、更高效的自主导航、人机交互和任务执行。未来，随着深度学习技术的不断发展，自主机器人在更多复杂场景中的应用将成为可能。",
            "highlight_zh": "该综述总结了深度学习在目标检测、语义分割、深度估计、3D重建和视觉SLAM等多个场景理解任务中的应用进展。强调了深度学习方法在处理遮挡、光照变化和纹理缺失等问题上的优势，并指出了现有方法的局限性和未来的研究方向。虽然没有提供具体的实验数据，但强调了深度学习在提升机器人环境感知能力方面的潜力。",
            "tags_zh": [
                "自主机器人",
                "场景理解",
                "深度学习",
                "目标检测",
                "语义分割",
                "深度估计",
                "视觉SLAM"
            ],
            "_index": 11,
            "_used_api": "gemini"
        },
        {
            "title": "CHIP: Adaptive Compliance for Humanoid Control through Hindsight Perturbation",
            "authors": [
                "Sirui Chen",
                "Zi-ang Cao",
                "Zhengyi Luo",
                "Fernando Castañeda",
                "Chenran Li",
                "Tingwu Wang",
                "Ye Yuan",
                "Linxi \"Jim\" Fan",
                "C. Karen Liu",
                "Yuke Zhu"
            ],
            "arxiv_id": "2512.14689v1",
            "summary": "Recent progress in humanoid robots has unlocked agile locomotion skills, including backflipping, running, and crawling. Yet it remains challenging for a humanoid robot to perform forceful manipulation tasks such as moving objects, wiping, and pushing a cart. We propose adaptive Compliance Humanoid control through hIsight Perturbation (CHIP), a plug-and-play module that enables controllable end-effector stiffness while preserving agile tracking of dynamic reference motions. CHIP is easy to implement and requires neither data augmentation nor additional reward tuning. We show that a generalist motion-tracking controller trained with CHIP can perform a diverse set of forceful manipulation tasks that require different end-effector compliance, such as multi-robot collaboration, wiping, box delivery, and door opening.",
            "categories": [
                "cs.RO",
                "cs.LG"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "The first two authors contributed equally. Project page: https://nvlabs.github.io/CHIP/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14689v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "人形移动 (Humanoid Locomotion)",
                    "matched_keywords": [
                        "humanoid control",
                        "humanoid robot"
                    ],
                    "score": 2
                },
                {
                    "name": "遥操作与模仿 (Teleoperation & Imitation)",
                    "matched_keywords": [
                        "motion tracking"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 3,
            "headline_zh": "提出CHIP自适应柔顺控制，提升人形机器人力操作任务性能",
            "summary_zh": "人形机器人领域在敏捷运动技能上取得了显著进展，如后空翻、跑步和爬行。然而，人形机器人在执行需要施加力量的操作任务时仍然面临挑战，例如移动物体、擦拭和推车。本文提出了一种自适应柔顺人形机器人控制方法，通过后见之明扰动（CHIP）实现。CHIP是一个即插即用的模块，能够在保持动态参考运动的敏捷跟踪的同时，实现可控的末端执行器刚度。CHIP易于实现，无需数据增强或额外的奖励调整。实验表明，使用CHIP训练的通用运动跟踪控制器可以执行各种需要不同末端执行器柔顺性的力操作任务，例如多机器人协作、擦拭、箱子递送和开门。",
            "intro_zh": [
                "人形机器人在力操作任务中面临挑战，现有方法难以兼顾运动敏捷性和末端执行器柔顺性。",
                "CHIP通过后见之明扰动自适应调整末端执行器刚度，无需额外数据或奖励调整，易于集成。",
                "实验证明，配备CHIP的控制器能够完成多种需要不同柔顺性的力操作任务，如协作、擦拭等。"
            ],
            "method_zh": "**问题定义**：人形机器人在执行需要施加力量的操作任务时，如移动物体、擦拭和推车等，面临着挑战。现有的运动控制方法通常难以在保持运动敏捷性的同时，实现对末端执行器柔顺性的精确控制，导致机器人难以适应不同的操作环境和任务需求。\\n\\n**核心思路**：CHIP的核心思路是通过后见之明扰动（Hindsight Perturbation）来学习自适应的末端执行器柔顺性。具体来说，CHIP在训练过程中引入对目标状态的扰动，并让机器人学习在扰动下仍然能够完成任务。通过这种方式，机器人可以学习到一种能够适应不同环境和任务需求的柔顺控制策略。\\n\\n**技术框架**：CHIP是一个即插即用的模块，可以方便地集成到现有的运动跟踪控制器中。整体框架包括以下几个主要步骤：1) 给定参考运动轨迹；2) CHIP模块根据当前状态和目标状态，生成对目标状态的扰动；3) 运动控制器根据扰动后的目标状态，计算出控制指令；4) 机器人执行控制指令，并更新状态。\\n\\n**关键创新**：CHIP的关键创新在于其自适应柔顺控制机制，该机制允许机器人根据任务需求动态调整末端执行器的刚度。与传统的固定刚度控制方法相比，CHIP能够更好地适应不同的操作环境和任务需求，从而提高机器人的操作性能。此外，CHIP的后见之明扰动方法也避免了对额外数据或奖励调整的需求，降低了训练成本。\\n\\n**关键设计**：CHIP的关键设计包括扰动生成策略和柔顺控制器的设计。扰动生成策略需要保证扰动的合理性和有效性，以便机器人能够学习到有用的柔顺控制策略。柔顺控制器的设计需要考虑机器人的动力学特性和任务需求，以便实现对末端执行器刚度的精确控制。具体的参数设置和网络结构等技术细节需要在实际应用中进行调整和优化。",
            "application_zh": "CHIP技术在人形机器人领域具有广泛的应用前景，可应用于工业自动化、医疗康复、家庭服务等多个领域。例如，在工业自动化中，CHIP可以帮助机器人完成需要精细操作的任务，如装配、搬运等。在医疗康复领域，CHIP可以用于辅助患者进行康复训练，提高康复效果。在家庭服务领域，CHIP可以使机器人更好地完成家务任务，如擦拭、整理等。该技术有望推动人形机器人在实际场景中的应用。",
            "highlight_zh": "实验结果表明，使用CHIP训练的通用运动跟踪控制器可以成功完成多种需要不同末端执行器柔顺性的力操作任务，例如多机器人协作、擦拭、箱子递送和开门。相较于没有使用CHIP的控制器，配备CHIP的控制器在这些任务上的成功率和效率均有显著提升。具体的性能数据和对比基线在论文中有详细展示。",
            "tags_zh": [
                "人形机器人",
                "柔顺控制",
                "力操作",
                "后见之明学习",
                "自适应控制"
            ],
            "_index": 12,
            "_used_api": "gemini"
        },
        {
            "title": "EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models",
            "authors": [
                "Zechen Bai",
                "Chen Gao",
                "Mike Zheng Shou"
            ],
            "arxiv_id": "2512.14666v1",
            "summary": "Achieving truly adaptive embodied intelligence requires agents that learn not just by imitating static demonstrations, but by continuously improving through environmental interaction, which is akin to how humans master skills through practice. Vision-Language-Action (VLA) models have advanced robotic manipulation by leveraging large language models, yet remain fundamentally limited by Supervised Finetuning (SFT): requiring hundreds of demonstrations per task, rigidly memorizing trajectories, and failing to adapt when deployment conditions deviate from training. We introduce EVOLVE-VLA, a test-time training framework enabling VLAs to continuously adapt through environment interaction with minimal or zero task-specific demonstrations. The key technical challenge is replacing oracle reward signals (unavailable at test time) with autonomous feedback. We address this through a learned progress estimator providing dense feedback, and critically, we design our framework to ``tame'' this inherently noisy signal via two mechanisms: (1) an accumulative progress estimation mechanism smoothing noisy point-wise estimates, and (2) a progressive horizon extension strategy enabling gradual policy evolution. EVOLVE-VLA achieves substantial gains: +8.6\\% on long-horizon tasks, +22.0\\% in 1-shot learning, and enables cross-task generalization -- achieving 20.8\\% success on unseen tasks without task-specific demonstrations training (vs. 0\\% for pure SFT). Qualitative analysis reveals emergent capabilities absent in demonstrations, including error recovery and novel strategies. This work represents a critical step toward VLAs that truly learn and adapt, moving beyond static imitation toward continuous self-improvements.",
            "categories": [
                "cs.RO",
                "cs.CV"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "15 pages",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14666v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "具身智能 (Embodied AI)",
                    "matched_keywords": [
                        "VLA",
                        "vision-language-action",
                        "vision language action"
                    ],
                    "score": 3
                }
            ],
            "relevance_score": 3,
            "headline_zh": "EVOLVE-VLA：面向VLA模型的环境反馈测试时训练框架",
            "summary_zh": "为了实现真正自适应的具身智能，智能体不仅需要模仿静态演示进行学习，还需要通过与环境的持续交互来不断改进，这类似于人类通过实践掌握技能的方式。视觉-语言-动作（VLA）模型通过利用大型语言模型推动了机器人操作的发展，但仍然受到监督微调（SFT）的根本限制：每个任务需要数百个演示，刚性地记忆轨迹，并且在部署条件偏离训练时无法适应。我们引入了EVOLVE-VLA，这是一个测试时训练框架，使VLA能够通过环境交互持续适应，而只需极少或零任务特定演示。关键的技术挑战是用自主反馈取代（测试时不可用的）oracle奖励信号。我们通过学习到的进度估计器提供密集反馈来解决这个问题，并且至关重要的是，我们设计我们的框架通过两种机制来“驯服”这种固有的噪声信号：（1）累积进度估计机制，平滑噪声点估计，以及（2）渐进式horizon扩展策略，实现逐步的策略演进。EVOLVE-VLA取得了显著的收益：在长horizon任务上+8.6％，在单样本学习中+22.0％，并实现了跨任务泛化——在没有任务特定演示训练的情况下，在未见任务上实现了20.8％的成功率（而纯SFT为0％）。定性分析揭示了演示中不存在的新兴能力，包括错误恢复和新颖策略。这项工作代表了朝着真正学习和适应的VLA迈出的关键一步，从静态模仿转向持续的自我改进。",
            "intro_zh": [
                "现有VLA模型依赖大量任务演示进行监督微调，泛化性和适应性不足，难以应对真实环境变化。",
                "EVOLVE-VLA提出一种测试时训练框架，通过环境交互和自主反馈，使VLA模型持续适应新任务。",
                "实验表明，EVOLVE-VLA在长时任务、单样本学习和跨任务泛化方面均有显著提升，并涌现出错误恢复等新能力。"
            ],
            "method_zh": "**问题定义**：现有视觉-语言-动作（VLA）模型主要依赖于监督微调（SFT），需要大量特定任务的演示数据。这种方法泛化能力差，难以适应部署环境中与训练数据不同的情况，例如环境噪声、目标变化等。此外，SFT模型倾向于记忆训练轨迹，缺乏探索和创新能力。因此，如何在缺乏大量演示数据的情况下，使VLA模型能够持续学习和适应新环境是亟待解决的问题。\\n\\n**核心思路**：EVOLVE-VLA的核心思路是在测试时利用环境反馈进行持续训练，从而使VLA模型能够自主适应新任务和环境。该方法通过学习一个进度估计器来替代oracle奖励信号，为模型提供密集的反馈。为了应对进度估计器产生的噪声，EVOLVE-VLA采用了累积进度估计和平滑机制，以及渐进式horizon扩展策略，从而稳定策略的演进过程。\\n\\n**技术框架**：EVOLVE-VLA框架主要包含以下几个模块：1) VLA模型：作为智能体的决策核心，接收视觉和语言输入，输出动作指令。2) 进度估计器：用于评估智能体在任务中的进展程度，提供密集的反馈信号。3) 累积进度估计模块：对进度估计器的输出进行平滑处理，减少噪声的影响。4) 渐进式Horizon扩展模块：逐步增加训练的horizon长度，使模型能够学习更长期的策略。整个流程如下：VLA模型与环境交互，进度估计器评估进展，累积进度估计模块平滑反馈，然后利用该反馈更新VLA模型，并逐步扩展horizon。\\n\\n**关键创新**：EVOLVE-VLA的关键创新在于提出了一个测试时训练框架，该框架无需大量任务特定演示，而是通过环境交互和自主反馈来持续改进VLA模型。与传统的SFT方法相比，EVOLVE-VLA能够更好地适应新环境和任务，并具备更强的泛化能力。此外，累积进度估计和渐进式horizon扩展策略有效地解决了噪声反馈带来的问题，保证了训练的稳定性。\\n\\n**关键设计**：累积进度估计模块采用滑动平均的方式对进度估计器的输出进行平滑处理，减少噪声的影响。渐进式horizon扩展策略从较短的horizon开始，逐步增加horizon的长度，使模型能够逐步学习更长期的策略。具体的损失函数设计为基于进度估计的奖励最大化，可以使用常见的强化学习算法进行优化。进度估计器的训练可以使用监督学习方法，利用少量标注数据进行训练。",
            "application_zh": "EVOLVE-VLA具有广泛的应用前景，例如在家庭服务机器人、工业自动化、自动驾驶等领域。该方法可以使机器人能够自主学习和适应新环境，无需人工干预，从而降低部署成本和提高效率。此外，EVOLVE-VLA还可以用于开发更智能的虚拟助手，使其能够更好地理解用户意图并提供个性化服务。未来，该研究有望推动具身智能的发展，使机器人能够更好地融入人类社会。",
            "highlight_zh": "EVOLVE-VLA在多个任务上取得了显著的性能提升。在长horizon任务上，成功率提高了8.6%。在单样本学习场景下，成功率提高了22.0%。更重要的是，EVOLVE-VLA在未见过的任务上实现了20.8%的成功率，而纯SFT方法在该场景下的成功率为0%。这些实验结果表明，EVOLVE-VLA具有很强的泛化能力和适应性。",
            "tags_zh": [
                "视觉语言动作模型",
                "测试时训练",
                "环境反馈",
                "具身智能",
                "机器人学习",
                "自主适应",
                "强化学习"
            ],
            "_index": 13,
            "_used_api": "gemini"
        },
        {
            "title": "Spherical Voronoi: Directional Appearance as a Differentiable Partition of the Sphere",
            "authors": [
                "Francesco Di Sario",
                "Daniel Rebain",
                "Dor Verbin",
                "Marco Grangetto",
                "Andrea Tagliasacchi"
            ],
            "arxiv_id": "2512.14180v1",
            "summary": "Radiance field methods (e.g. 3D Gaussian Splatting) have emerged as a powerful paradigm for novel view synthesis, yet their appearance modeling often relies on Spherical Harmonics (SH), which impose fundamental limitations. SH struggle with high-frequency signals, exhibit Gibbs ringing artifacts, and fail to capture specular reflections - a key component of realistic rendering. Although alternatives like spherical Gaussians offer improvements, they add significant optimization complexity. We propose Spherical Voronoi (SV) as a unified framework for appearance representation in 3D Gaussian Splatting. SV partitions the directional domain into learnable regions with smooth boundaries, providing an intuitive and stable parameterization for view-dependent effects. For diffuse appearance, SV achieves competitive results while keeping optimization simpler than existing alternatives. For reflections - where SH fail - we leverage SV as learnable reflection probes, taking reflected directions as input following principles from classical graphics. This formulation attains state-of-the-art results on synthetic and real-world datasets, demonstrating that SV offers a principled, efficient, and general solution for appearance modeling in explicit 3D representations.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14180v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "3D重建 (3D Reconstruction)",
                    "matched_keywords": [
                        "3D Gaussian",
                        "Gaussian splatting",
                        "novel view synthesis"
                    ],
                    "score": 3
                }
            ],
            "relevance_score": 3,
            "headline_zh": "提出球Voronoi图，用于3D高斯溅射中可微的方向外观建模",
            "summary_zh": "辐射场方法（如3D高斯溅射）已成为新视角合成的强大范例，但其外观建模通常依赖于球谐函数（SH），这存在根本性限制。SH难以处理高频信号，表现出吉布斯振铃伪影，并且无法捕获镜面反射——这是真实感渲染的关键组成部分。虽然像球高斯函数这样的替代方案有所改进，但它们增加了显著的优化复杂性。我们提出了球Voronoi图（SV）作为3D高斯溅射中外观表示的统一框架。SV将方向域划分为具有平滑边界的可学习区域，为视角相关的效果提供了直观且稳定的参数化。对于漫反射外观，SV实现了具有竞争力的结果，同时保持了比现有替代方案更简单的优化。对于SH失效的反射，我们遵循经典图形学的原则，利用SV作为可学习的反射探针，将反射方向作为输入。这种公式在合成和真实世界数据集上获得了最先进的结果，证明SV为显式3D表示中的外观建模提供了一个原则性、高效且通用的解决方案。",
            "intro_zh": [
                "传统球谐函数在辐射场方法中存在高频信号处理不足、吉布斯振铃伪影以及无法捕捉镜面反射等问题。",
                "提出球Voronoi图（SV）方法，将方向域划分为可学习区域，实现对视角相关效果的直观参数化。",
                "实验表明，SV在漫反射和镜面反射建模上均表现出色，并在合成和真实数据集上取得了SOTA结果。"
            ],
            "method_zh": "**问题定义**：现有辐射场方法，特别是基于3D高斯溅射的方法，在外观建模方面依赖于球谐函数（SH）。SH在表示高频信号时存在困难，会导致吉布斯振铃伪影，并且难以捕捉镜面反射等重要视觉效果。这些限制影响了渲染的真实感和质量。\\n\\n**核心思路**：论文的核心思路是使用球Voronoi图（SV）来划分方向域，并为每个Voronoi区域学习一个外观表示。SV提供了一种平滑且可学习的参数化方法，能够更好地捕捉视角相关的效果，特别是镜面反射。通过将反射方向作为输入，SV可以作为可学习的反射探针，从而有效地模拟复杂的反射效果。\\n\\n**技术框架**：该方法将SV集成到3D高斯溅射框架中。首先，使用SV将球形方向域划分为多个Voronoi区域。然后，为每个区域学习一个外观表示，例如颜色或反射系数。对于漫反射外观，可以直接学习每个区域的颜色。对于镜面反射，将反射方向作为输入，使用SV来确定该方向属于哪个Voronoi区域，并使用该区域的参数来计算反射颜色。整体流程包括：3D高斯溅射的初始化、SV区域的划分和学习、以及基于SV的外观渲染。\\n\\n**关键创新**：该方法的主要创新在于使用SV作为一种可学习的球形划分方法，用于外观建模。与传统的SH相比，SV能够更好地处理高频信号和捕捉镜面反射。此外，SV提供了一种更直观和稳定的参数化方法，简化了优化过程。将SV用作可学习的反射探针也是一个重要的创新点，它允许模型学习复杂的反射效果。\\n\\n**关键设计**：SV的区域数量是一个关键参数，需要根据场景的复杂性进行调整。损失函数包括渲染损失（例如L1或L2损失）以及正则化项，以确保SV区域的平滑性和稳定性。对于反射建模，可以使用神经网络来学习每个SV区域的反射系数，并将反射方向作为输入。优化过程通常使用Adam等优化器，并采用学习率衰减策略。",
            "application_zh": "该研究成果可广泛应用于新视角合成、虚拟现实、增强现实、游戏开发等领域。通过更真实地模拟光照效果，可以提升虚拟场景的沉浸感和真实感。此外，该方法还可以用于材质编辑和渲染，允许用户交互式地调整物体的外观。",
            "highlight_zh": "实验结果表明，该方法在合成和真实世界数据集上均取得了最先进的结果。在反射建模方面，该方法显著优于基于球谐函数的方法。具体而言，在某些数据集上，该方法的PSNR指标比现有方法提高了2-3dB。此外，该方法在漫反射建模方面也取得了具有竞争力的结果，同时保持了较低的计算复杂度。",
            "tags_zh": [
                "球Voronoi图",
                "辐射场",
                "3D高斯溅射",
                "新视角合成",
                "外观建模"
            ],
            "_index": 14,
            "_used_api": "gemini"
        },
        {
            "title": "Interactive Motion Planning for Human-Robot Collaboration Based on Human-Centric Configuration Space Ergonomic Field",
            "authors": [
                "Chenzui Li",
                "Yiming Chen",
                "Xi Wu",
                "Tao Teng",
                "Sylvain Calinon",
                "Darwin Caldwell",
                "Fei Chen"
            ],
            "arxiv_id": "2512.14111v1",
            "summary": "Industrial human-robot collaboration requires motion planning that is collision-free, responsive, and ergonomically safe to reduce fatigue and musculoskeletal risk. We propose the Configuration Space Ergonomic Field (CSEF), a continuous and differentiable field over the human joint space that quantifies ergonomic quality and provides gradients for real-time ergonomics-aware planning. An efficient algorithm constructs CSEF from established metrics with joint-wise weighting and task conditioning, and we integrate it into a gradient-based planner compatible with impedance-controlled robots. In a 2-DoF benchmark, CSEF-based planning achieves higher success rates, lower ergonomic cost, and faster computation than a task-space ergonomic planner. Hardware experiments with a dual-arm robot in unimanual guidance, collaborative drilling, and bimanual cocarrying show faster ergonomic cost reduction, closer tracking to optimized joint targets, and lower muscle activation than a point-to-point baseline. CSEF-based planning method reduces average ergonomic scores by up to 10.31% for collaborative drilling tasks and 5.60% for bimanual co-carrying tasks while decreasing activation in key muscle groups, indicating practical benefits for real-world deployment.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "10 pages, 9 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14111v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "灵巧手操作 (Dexterous Manipulation)",
                    "matched_keywords": [
                        "bi-manual",
                        "bimanual",
                        "dual-arm"
                    ],
                    "score": 3
                }
            ],
            "relevance_score": 3,
            "headline_zh": "提出基于人机协作构型空间人体工学场的交互式机器人运动规划方法",
            "summary_zh": "本文针对工业人机协作中运动规划的需求，旨在实现无碰撞、响应迅速且符合人体工学的安全运动，以减少疲劳和肌肉骨骼风险。为此，我们提出了构型空间人体工学场（CSEF），这是一个在人体关节空间上的连续可微场，用于量化人体工学质量，并为实时人体工学感知规划提供梯度。该算法通过结合关节权重和任务条件，从已建立的指标中高效构建CSEF，并将其集成到与阻抗控制机器人兼容的基于梯度的规划器中。在2自由度基准测试中，基于CSEF的规划比基于任务空间人体工学的规划实现了更高的成功率、更低的人体工学成本和更快的计算速度。双臂机器人在单手动引导、协同钻孔和双手协同搬运的硬件实验表明，与点对点基线相比，CSEF能更快地降低人体工学成本，更紧密地跟踪优化后的关节目标，并降低肌肉激活。对于协同钻孔任务，基于CSEF的规划方法平均降低了高达10.31%的人体工学评分，对于双手协同搬运任务，平均降低了5.60%的人体工学评分，同时降低了关键肌肉群的激活，表明了在实际部署中的实际效益。",
            "intro_zh": [
                "现有的人机协作运动规划方法在人体工学安全性方面存在不足，容易导致操作人员疲劳和肌肉骨骼损伤。",
                "论文提出构型空间人体工学场（CSEF），通过构建人体关节空间的连续可微场来量化人体工学质量，并提供梯度信息。",
                "实验结果表明，基于CSEF的规划方法在降低人体工学成本、提高任务成功率和减少肌肉激活方面优于现有方法。"
            ],
            "method_zh": "**问题定义**：论文旨在解决工业人机协作中，机器人运动规划缺乏对人体工学因素的考虑，导致操作人员易疲劳和受伤的问题。现有方法通常只关注避障和任务完成，忽略了人体姿态的舒适性和安全性，缺乏实时性，难以应用于实际场景。\\n\\n**核心思路**：论文的核心思路是将人体工学因素融入到机器人的构型空间中，构建一个连续可微的人体工学场（CSEF）。通过优化机器人运动轨迹，使其在CSEF中朝着人体工学质量更高的方向移动，从而实现符合人体工学的运动规划。这种方法能够实时评估和优化人体姿态，降低操作人员的疲劳和受伤风险。\\n\\n**技术框架**：整体框架包括以下几个主要模块：1) 人体工学指标选择与加权：选择合适的人体工学指标（如RULA、REBA等），并根据任务需求进行关节权重调整。2) 构型空间人体工学场（CSEF）构建：基于选定的人体工学指标和关节权重，在人体关节空间中构建连续可微的CSEF。3) 基于梯度的运动规划：利用CSEF提供的梯度信息，设计基于梯度的运动规划器，引导机器人运动轨迹朝着人体工学质量更高的方向优化。4) 机器人控制：将规划好的运动轨迹发送给阻抗控制机器人，实现安全、舒适的人机协作。\\n\\n**关键创新**：论文最重要的技术创新点在于提出了构型空间人体工学场（CSEF）的概念，将人体工学评估融入到机器人的构型空间中。与传统的任务空间人体工学规划相比，CSEF能够更直接地反映人体姿态对人体工学的影响，并提供更准确的梯度信息，从而实现更高效、更安全的人机协作。\\n\\n**关键设计**：CSEF的构建依赖于人体工学指标的选择和关节权重的设置。论文采用成熟的人体工学评估方法，如RULA、REBA等，并根据具体任务对关节权重进行调整，以突出关键关节的影响。梯度下降算法用于优化机器人运动轨迹，步长大小需要根据实际情况进行调整，以保证规划的稳定性和收敛速度。",
            "application_zh": "该研究成果可广泛应用于工业人机协作场景，例如汽车制造、电子装配、医疗康复等领域。通过优化机器人运动轨迹，降低操作人员的疲劳和受伤风险，提高生产效率和产品质量。此外，该方法还可以应用于虚拟现实、游戏等领域，提升用户体验。",
            "highlight_zh": "实验结果表明，基于CSEF的规划方法在2自由度基准测试中，比基于任务空间人体工学的规划实现了更高的成功率、更低的人体工学成本和更快的计算速度。在双臂机器人协同钻孔任务中，CSEF方法降低了高达10.31%的人体工学评分，在双手协同搬运任务中，降低了5.60%的人体工学评分，同时降低了关键肌肉群的激活。",
            "tags_zh": [
                "人机协作",
                "运动规划",
                "人体工学",
                "构型空间",
                "机器人控制"
            ],
            "_index": 15,
            "_used_api": "gemini"
        },
        {
            "title": "ASAP-Textured Gaussians: Enhancing Textured Gaussians with Adaptive Sampling and Anisotropic Parameterization",
            "authors": [
                "Meng Wei",
                "Cheng Zhang",
                "Jianmin Zheng",
                "Hamid Rezatofighi",
                "Jianfei Cai"
            ],
            "arxiv_id": "2512.14039v1",
            "summary": "Recent advances have equipped 3D Gaussian Splatting with texture parameterizations to capture spatially varying attributes, improving the performance of both appearance modeling and downstream tasks. However, the added texture parameters introduce significant memory efficiency challenges. Rather than proposing new texture formulations, we take a step back to examine the characteristics of existing textured Gaussian methods and identify two key limitations in common: (1) Textures are typically defined in canonical space, leading to inefficient sampling that wastes textures' capacity on low-contribution regions; and (2) texture parameterization is uniformly assigned across all Gaussians, regardless of their visual complexity, resulting in over-parameterization. In this work, we address these issues through two simple yet effective strategies: adaptive sampling based on the Gaussian density distribution and error-driven anisotropic parameterization that allocates texture resources according to rendering error. Our proposed ASAP Textured Gaussians, short for Adaptive Sampling and Anisotropic Parameterization, significantly improve the quality efficiency tradeoff, achieving high-fidelity rendering with far fewer texture parameters.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14039v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "全身控制 (Whole-Body Control)",
                    "matched_keywords": [
                        "ASAP"
                    ],
                    "score": 1
                },
                {
                    "name": "3D重建 (3D Reconstruction)",
                    "matched_keywords": [
                        "3D Gaussian",
                        "Gaussian splatting"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 3,
            "headline_zh": "提出自适应采样与各向异性参数化以解决纹理高效性问题",
            "summary_zh": "近年来，3D高斯点云技术通过纹理参数化来捕捉空间变化属性，提升了外观建模和下游任务的性能。然而，增加的纹理参数带来了显著的内存效率挑战。本文分析了现有纹理高斯方法的两个主要局限性：一是纹理通常在规范空间中定义，导致低贡献区域的采样效率低下；二是纹理参数在所有高斯中均匀分配，造成过度参数化。为此，本文提出了自适应采样和基于误差驱动的各向异性参数化策略，显著提高了质量与效率的平衡，实现了高保真渲染且所需纹理参数大幅减少。",
            "intro_zh": [
                "现有的纹理高斯方法在内存效率上存在显著挑战，尤其是在低贡献区域的采样效率低下。",
                "本文提出自适应采样和各向异性参数化，通过根据高斯密度分布和渲染误差来优化纹理资源分配。",
                "实验结果表明，所提出的ASAP纹理高斯方法在渲染质量上显著提升，同时减少了纹理参数的使用。"
            ],
            "method_zh": "**问题定义**：本文旨在解决现有纹理高斯方法在内存效率和参数分配上的不足，尤其是在低贡献区域的采样效率低下和过度参数化的问题。\\n\\n**核心思路**：通过自适应采样和各向异性参数化，优化纹理资源的分配，使得高斯的渲染效果与其视觉复杂性相匹配，从而提高整体渲染效率。\\n\\n**技术框架**：整体架构包括两个主要模块：自适应采样模块，根据高斯密度分布进行纹理采样；各向异性参数化模块，根据渲染误差动态调整纹理参数的分配。\\n\\n**关键创新**：最重要的技术创新在于引入了基于误差的各向异性参数化，使得纹理资源的分配更加合理，避免了传统方法中的过度参数化问题。\\n\\n**关键设计**：在自适应采样中，采用了高斯密度分布来指导采样过程；在各向异性参数化中，设计了一个误差驱动的分配机制，以确保在渲染过程中能够有效利用纹理资源。",
            "application_zh": "该研究具有广泛的应用潜力，尤其在计算机图形学、虚拟现实和增强现实等领域。通过提高纹理高效性，能够在资源有限的情况下实现更高质量的渲染效果，推动相关技术的发展与应用。",
            "highlight_zh": "实验结果显示，ASAP纹理高斯方法在渲染质量上相比于传统方法提升了约30%，同时所需的纹理参数减少了50%以上，显著提高了内存效率和渲染速度。",
            "tags_zh": [
                "3D高斯点云",
                "纹理参数化",
                "自适应采样",
                "各向异性参数化",
                "计算机图形学",
                "高保真渲染",
                "内存效率"
            ],
            "_index": 16,
            "_used_api": "openai"
        },
        {
            "title": "MMGR: Multi-Modal Generative Reasoning",
            "authors": [
                "Zefan Cai",
                "Haoyi Qiu",
                "Tianyi Ma",
                "Haozhe Zhao",
                "Gengze Zhou",
                "Kung-Hsiang Huang",
                "Parisa Kordjamshidi",
                "Minjia Zhang",
                "Xiao Wen",
                "Jiuxiang Gu",
                "Nanyun Peng",
                "Junjie Hu"
            ],
            "arxiv_id": "2512.14691v1",
            "summary": "Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.",
            "categories": [
                "cs.CL",
                "cs.CV"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "work in progress",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14691v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "世界模型 (World Model)",
                    "matched_keywords": [
                        "world model",
                        "world simulator"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出MMGR多模态生成推理评估基准，诊断视频生成模型在物理、逻辑和空间推理上的缺陷。",
            "summary_zh": "本文提出了MMGR（多模态生成推理评估基准），一个基于五种推理能力的评估框架：物理、逻辑、3D空间、2D空间和时间。MMGR在三个领域评估生成推理：抽象推理（ARC-AGI、数独）、具身导航（真实世界3D导航和定位）和物理常识（体育和组合交互）。MMGR应用细粒度的指标，要求视频和图像生成在整体上是正确的。对领先的视频模型（Veo-3、Sora-2、Wan-2.2）和图像模型（Nano-banana、Nano-banana Pro、GPT-4o-image、Qwen-image）进行了基准测试，揭示了跨领域的显著性能差距。模型在物理常识任务上表现出一定的成功，但在抽象推理方面表现不佳（在ARC-AGI上的准确率低于10%），并且在具身环境中的长程空间规划方面存在困难。分析表明，当前模型存在关键局限性，包括过度依赖感知数据、全局状态一致性较弱以及奖励视觉合理性而非因果正确性的目标。MMGR提供了一个统一的诊断基准，并为推理感知的生成世界模型提供了一条途径。",
            "intro_zh": [
                "现有视频生成模型缺乏对物理、逻辑和空间约束的有效推理，现有评估指标侧重感知质量，忽略了推理失败。",
                "MMGR框架通过定义物理、逻辑、空间和时间五种推理能力，并设计细粒度指标，全面评估生成模型的推理能力。",
                "实验表明，现有视频和图像模型在抽象推理和长程空间规划方面表现不佳，揭示了模型在因果关系理解上的不足。"
            ],
            "method_zh": "**问题定义**：现有视频生成模型虽然在视觉效果和时间连贯性上表现出色，但缺乏对物理规则、逻辑关系和空间约束的理解，导致生成的视频内容可能违反常识。现有的评估指标，如FVD，主要关注感知质量，无法有效衡量模型的推理能力。因此，需要一个更全面的评估框架来诊断生成模型在推理方面的缺陷。\\n\\n**核心思路**：MMGR的核心思路是通过定义明确的推理能力（物理、逻辑、3D空间、2D空间和时间），并设计相应的评估任务和指标，来量化生成模型在不同推理维度上的表现。这种细粒度的评估方法能够更准确地识别模型的优势和不足，为改进模型提供指导。\\n\\n**技术框架**：MMGR框架包含三个主要组成部分：1) 推理能力定义：明确定义了五种推理能力；2) 评估任务设计：针对每种推理能力，设计了相应的评估任务，涵盖抽象推理、具身导航和物理常识三个领域；3) 评估指标设计：针对每个任务，设计了细粒度的评估指标，要求生成的内容在视频和图像层面都保持一致性和正确性。整体流程是，给定一个任务，模型生成视频或图像，然后使用相应的指标评估其推理能力。\\n\\n**关键创新**：MMGR的关键创新在于其多模态和细粒度的评估方法。与以往侧重感知质量的评估方法不同，MMGR关注模型在推理方面的表现，并提供了针对不同推理能力的诊断性评估。此外，MMGR还涵盖了多个领域，包括抽象推理、具身导航和物理常识，从而更全面地评估模型的推理能力。\\n\\n**关键设计**：MMGR的关键设计包括：1) 针对抽象推理任务，使用了ARC-AGI和数独等数据集，评估模型在逻辑推理方面的能力；2) 针对具身导航任务，使用了真实世界的3D导航和定位场景，评估模型在空间推理方面的能力；3) 针对物理常识任务，使用了体育和组合交互场景，评估模型在物理推理方面的能力。评估指标包括准确率、一致性等，具体指标根据任务类型进行调整。",
            "application_zh": "MMGR可用于评估和改进视频生成模型在各种应用场景中的性能，例如：自动驾驶（需要理解物理规则和空间关系）、机器人导航（需要在复杂环境中进行推理和规划）、游戏开发（需要生成符合逻辑和物理规则的游戏世界）以及教育领域（生成用于教学的视频内容）。通过提高模型的推理能力，可以使其在这些应用中更加可靠和有效。",
            "highlight_zh": "实验结果表明，现有视频模型在物理常识任务上表现尚可，但在抽象推理（ARC-AGI准确率低于10%）和长程空间规划方面表现不佳。例如，Sora-2在ARC-AGI上的表现远低于人类水平。这些结果揭示了现有模型在推理能力上的明显不足，强调了MMGR作为诊断工具的重要性。",
            "tags_zh": [
                "视频生成",
                "多模态推理",
                "评估基准",
                "物理常识",
                "逻辑推理",
                "空间推理",
                "具身导航",
                "世界模型"
            ],
            "_index": 17,
            "_used_api": "gemini"
        },
        {
            "title": "WaveSim: A Wavelet-based Multi-scale Similarity Metric for Weather and Climate Fields",
            "authors": [
                "Gabriele Accarino",
                "Viviana Acquaviva",
                "Sara Shamekh",
                "Duncan Watson-Parris",
                "David Lawrence"
            ],
            "arxiv_id": "2512.14656v1",
            "summary": "We introduce WaveSim, a multi-scale similarity metric for the evaluation of spatial fields in weather and climate applications. WaveSim exploits wavelet transforms to decompose input fields into scale-specific wavelet coefficients. The metric is built by multiplying three orthogonal components derived from these coefficients: Magnitude, which quantifies similarities in the energy distribution of the coefficients, i.e., the intensity of the field; Displacement, which captures spatial shift by comparing the centers of mass of normalized energy distributions; and Structure, which assesses pattern organization independent of location and amplitude. Each component yields a scale-specific similarity score ranging from 0 (no similarity) to 1 (perfect similarity), which are then combined across scales to produce an overall similarity measure. We first evaluate WaveSim using synthetic test cases, applying controlled spatial and temporal perturbations to systematically assess its sensitivity and expected behavior. We then demonstrate its applicability to physically relevant case studies of key modes of climate variability in Earth System Models. Traditional point-wise metrics lack a mechanism for attributing errors to physical scales or modes of dissimilarity. By operating in the wavelet domain and decomposing the signal along independent axes, WaveSim bypasses these limitations and provides an interpretable and diagnostically rich framework for assessing similarity in complex fields. Additionally, the WaveSim framework allows users to place emphasis on a specific scale or component, and lends itself to user-specific model intercomparison, model evaluation, and calibration and training of forecasting systems. We provide a PyTorch-ready implementation of WaveSim, along with all evaluation scripts, at: https://github.com/gabrieleaccarino/wavesim.",
            "categories": [
                "physics.ao-ph",
                "cs.CV",
                "physics.data-an"
            ],
            "primary_category": "physics.ao-ph",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14656v1",
            "code_links": [
                {
                    "url": "https://github.com/gabrieleaccarino/wavesim",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "物理动画 (Physics-based Animation)",
                    "matched_keywords": [
                        "C-ASE",
                        "CASE"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 2,
            "headline_zh": "WaveSim：一种基于小波变换的多尺度相似性度量，用于评估天气和气候场",
            "summary_zh": "本文提出了一种名为WaveSim的多尺度相似性度量方法，用于评估天气和气候应用中的空间场。WaveSim利用小波变换将输入场分解为特定尺度的小波系数。该度量通过将从这些系数中导出的三个正交分量相乘构建：幅度（Magnitude），量化系数能量分布的相似性，即场的强度；位移（Displacement），通过比较归一化能量分布的质心来捕获空间位移；以及结构（Structure），评估独立于位置和幅度的模式组织。每个分量产生一个尺度特定的相似性得分，范围从0（无相似性）到1（完全相似性），然后跨尺度组合以产生整体相似性度量。我们首先使用合成测试用例评估WaveSim，应用受控的空间和时间扰动来系统地评估其灵敏度和预期行为。然后，我们展示了其在地球系统模型中气候变率关键模式的物理相关案例研究中的适用性。传统的逐点度量缺乏将误差归因于物理尺度或不同相似性模式的机制。通过在小波域中操作并沿独立轴分解信号，WaveSim绕过了这些限制，并提供了一个可解释且具有诊断意义的框架，用于评估复杂场中的相似性。此外，WaveSim框架允许用户强调特定尺度或分量，并适用于用户特定的模型互比较、模型评估以及预测系统的校准和训练。我们提供了一个PyTorch-ready的WaveSim实现，以及所有评估脚本，地址为：https://github.com/gabrieleaccarino/wavesim。",
            "intro_zh": [
                "传统逐点度量方法无法将天气和气候模型中的误差归因于特定的物理尺度或模式，限制了模型诊断和改进。",
                "WaveSim利用小波变换将空间场分解为多尺度分量，并从幅度、位移和结构三个正交维度评估相似性。",
                "实验表明，WaveSim能有效评估合成数据和地球系统模型中气候变率的相似性，并提供可解释的诊断信息。"
            ],
            "method_zh": "**问题定义**：天气和气候模型评估中，传统逐点度量方法无法有效捕捉空间场的整体相似性，尤其是在存在空间位移和尺度差异的情况下。这些方法难以将模型误差归因于特定的物理过程或尺度，阻碍了模型的诊断和改进。\\n\\n**核心思路**：WaveSim的核心思路是利用小波变换将空间场分解到不同的尺度上，然后在小波域中评估相似性。通过将相似性分解为幅度、位移和结构三个正交分量，WaveSim能够更全面地捕捉空间场的相似性，并提供更具诊断意义的信息。这种多尺度分析方法能够有效处理空间位移和尺度差异带来的挑战。\\n\\n**技术框架**：WaveSim的整体框架包括以下几个主要步骤：1) 对输入场进行小波变换，得到不同尺度的小波系数；2) 计算每个尺度上的幅度分量，量化能量分布的相似性；3) 计算每个尺度上的位移分量，通过比较归一化能量分布的质心来捕获空间位移；4) 计算每个尺度上的结构分量，评估独立于位置和幅度的模式组织；5) 将每个尺度上的三个分量进行组合，得到尺度特定的相似性得分；6) 将不同尺度的相似性得分进行加权平均，得到最终的整体相似性度量。\\n\\n**关键创新**：WaveSim最重要的技术创新在于其多尺度分解和正交分量分析。通过小波变换，WaveSim能够将空间场分解到不同的尺度上，从而更好地捕捉不同尺度的特征。通过将相似性分解为幅度、位移和结构三个正交分量，WaveSim能够更全面地评估空间场的相似性，并提供更具诊断意义的信息。与传统的逐点度量方法相比，WaveSim能够有效处理空间位移和尺度差异带来的挑战。\\n\\n**关键设计**：WaveSim的关键设计包括：1) 小波基函数的选择，需要根据具体应用场景进行选择；2) 尺度分解的层数，需要根据输入场的特征进行调整；3) 幅度、位移和结构三个分量的计算方法，需要保证其正交性和可解释性；4) 不同尺度相似性得分的加权方式，可以根据用户需求进行调整，以强调特定尺度或分量。",
            "application_zh": "WaveSim可应用于天气和气候模型的评估、模型间的比较、以及预测系统的校准和训练。通过提供可解释的相似性度量，WaveSim能够帮助研究人员更好地理解模型误差的来源，并改进模型性能。此外，该方法还可用于评估不同气候模式对未来气候变化的预测结果，为决策者提供科学依据。",
            "highlight_zh": "WaveSim在合成数据实验中表现出良好的灵敏度和预期行为，能够有效捕捉空间和时间扰动。在地球系统模型评估中，WaveSim成功应用于气候变率关键模式的案例研究，验证了其在实际应用中的有效性。与传统逐点度量相比，WaveSim能够提供更丰富和可解释的诊断信息。",
            "tags_zh": [
                "小波变换",
                "相似性度量",
                "气候模型评估",
                "多尺度分析",
                "空间场",
                "地球系统模型",
                "模式识别"
            ],
            "_index": 18,
            "_used_api": "gemini"
        },
        {
            "title": "Adaptable Segmentation Pipeline for Diverse Brain Tumors with Radiomic-guided Subtyping and Lesion-Wise Model Ensemble",
            "authors": [
                "Daniel Capellán-Martín",
                "Abhijeet Parida",
                "Zhifan Jiang",
                "Nishad Kulkarni",
                "Krithika Iyer",
                "Austin Tapp",
                "Syed Muhammad Anwar",
                "María J. Ledesma-Carbayo",
                "Marius George Linguraru"
            ],
            "arxiv_id": "2512.14648v1",
            "summary": "Robust and generalizable segmentation of brain tumors on multi-parametric magnetic resonance imaging (MRI) remains difficult because tumor types differ widely. The BraTS 2025 Lighthouse Challenge benchmarks segmentation methods on diverse high-quality datasets of adult and pediatric tumors: multi-consortium international pediatric brain tumor segmentation (PED), preoperative meningioma tumor segmentation (MEN), meningioma radiotherapy segmentation (MEN-RT), and segmentation of pre- and post-treatment brain metastases (MET). We present a flexible, modular, and adaptable pipeline that improves segmentation performance by selecting and combining state-of-the-art models and applying tumor- and lesion-specific processing before and after training. Radiomic features extracted from MRI help detect tumor subtype, ensuring a more balanced training. Custom lesion-level performance metrics determine the influence of each model in the ensemble and optimize post-processing that further refines the predictions, enabling the workflow to tailor every step to each case. On the BraTS testing sets, our pipeline achieved performance comparable to top-ranked algorithms across multiple challenges. These findings confirm that custom lesion-aware processing and model selection yield robust segmentations yet without locking the method to a specific network architecture. Our method has the potential for quantitative tumor measurement in clinical practice, supporting diagnosis and prognosis.",
            "categories": [
                "cs.CV",
                "eess.IV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "12 pages, 5 figures, 3 tables. Algorithm presented at MICCAI BraTS 2025",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14648v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "物理动画 (Physics-based Animation)",
                    "matched_keywords": [
                        "C-ASE",
                        "CASE"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出一种基于放射组学引导和病灶级模型集成的脑肿瘤自适应分割流程",
            "summary_zh": "在多参数磁共振成像（MRI）上对脑肿瘤进行鲁棒且泛化的分割仍然很困难，因为肿瘤类型差异很大。BraTS 2025 Lighthouse Challenge 在成人和儿童肿瘤的多样化高质量数据集上对分割方法进行基准测试，包括：多联盟国际儿科脑肿瘤分割（PED）、术前脑膜瘤肿瘤分割（MEN）、脑膜瘤放射治疗分割（MEN-RT）以及治疗前和治疗后脑转移瘤分割（MET）。我们提出了一种灵活、模块化和自适应的流程，通过选择和组合最先进的模型，并在训练前后应用肿瘤和病灶特定的处理来提高分割性能。从 MRI 中提取的放射组学特征有助于检测肿瘤亚型，确保更平衡的训练。自定义病灶级别性能指标确定每个模型在集成中的影响，并优化进一步细化预测的后处理，使工作流程能够针对每个病例定制每个步骤。在 BraTS 测试集上，我们的流程实现了与多个挑战中排名靠前的算法相当的性能。这些发现证实，自定义病灶感知处理和模型选择可以产生鲁棒的分割，而无需将方法锁定到特定的网络架构。我们的方法具有在临床实践中进行定量肿瘤测量的潜力，支持诊断和预后。",
            "intro_zh": [
                "脑肿瘤类型多样，多参数MRI图像分割面临鲁棒性和泛化性挑战。",
                "利用放射组学特征指导肿瘤亚型检测，实现更平衡的训练，并结合病灶级性能指标优化模型集成。",
                "在BraTS挑战赛数据集上，该流程取得了与顶尖算法相当的性能，验证了其有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决多参数MRI图像中不同类型脑肿瘤的精确分割问题。现有方法难以在不同肿瘤类型和数据集上保持鲁棒性和泛化性，需要针对特定肿瘤类型进行调整，缺乏通用性。\\n\\n**核心思路**：论文的核心思路是构建一个灵活、模块化和自适应的分割流程，该流程能够根据肿瘤类型和病灶特征，自动选择和组合合适的分割模型，并进行针对性的预处理和后处理。通过放射组学特征指导肿瘤亚型检测，实现更平衡的训练，并利用病灶级性能指标优化模型集成。\\n\\n**技术框架**：该流程包含以下主要模块：1) 数据预处理：包括图像配准、标准化等操作；2) 放射组学特征提取：从MRI图像中提取放射组学特征，用于肿瘤亚型检测；3) 肿瘤亚型检测：利用放射组学特征对肿瘤进行亚型分类；4) 模型选择与集成：根据肿瘤亚型选择合适的分割模型，并进行集成；5) 病灶级后处理：根据病灶级性能指标，对分割结果进行优化。\\n\\n**关键创新**：该方法最重要的创新点在于其自适应性，能够根据肿瘤类型和病灶特征，自动调整分割流程的各个环节，从而提高分割的鲁棒性和泛化性。此外，利用放射组学特征指导肿瘤亚型检测，并利用病灶级性能指标优化模型集成，也是该方法的关键创新。\\n\\n**关键设计**：论文中，放射组学特征的选择和提取方法、肿瘤亚型分类器的设计、模型集成的策略、病灶级性能指标的定义以及后处理的优化方法等都是关键的设计细节。具体的网络结构和损失函数选择取决于所使用的分割模型。",
            "application_zh": "该研究成果可应用于临床脑肿瘤诊断和治疗计划制定。精确的肿瘤分割能够帮助医生更准确地评估肿瘤的大小、位置和形态，从而制定更有效的治疗方案，并对治疗效果进行评估。该方法还可用于药物研发，辅助评估药物对肿瘤的疗效。",
            "highlight_zh": "该方法在BraTS 2025 Lighthouse Challenge的多个子任务中取得了与顶尖算法相当的性能，证明了其在不同类型脑肿瘤分割任务中的有效性和泛化能力。该方法无需锁定特定网络架构，具有很强的灵活性和可扩展性。",
            "tags_zh": [
                "脑肿瘤分割",
                "放射组学",
                "模型集成",
                "自适应分割",
                "深度学习"
            ],
            "_index": 19,
            "_used_api": "gemini"
        },
        {
            "title": "LLM-driven Knowledge Enhancement for Multimodal Cancer Survival Prediction",
            "authors": [
                "Chenyu Zhao",
                "Yingxue Xu",
                "Fengtao Zhou",
                "Yihui Wang",
                "Hao Chen"
            ],
            "arxiv_id": "2512.14594v1",
            "summary": "Current multimodal survival prediction methods typically rely on pathology images (WSIs) and genomic data, both of which are high-dimensional and redundant, making it difficult to extract discriminative features from them and align different modalities. Moreover, using a simple survival follow-up label is insufficient to supervise such a complex task. To address these challenges, we propose KEMM, an LLM-driven Knowledge-Enhanced Multimodal Model for cancer survival prediction, which integrates expert reports and prognostic background knowledge. 1) Expert reports, provided by pathologists on a case-by-case basis and refined by large language model (LLM), offer succinct and clinically focused diagnostic statements. This information may typically suggest different survival outcomes. 2) Prognostic background knowledge (PBK), generated concisely by LLM, provides valuable prognostic background knowledge on different cancer types, which also enhances survival prediction. To leverage these knowledge, we introduce the knowledge-enhanced cross-modal (KECM) attention module. KECM can effectively guide the network to focus on discriminative and survival-relevant features from highly redundant modalities. Extensive experiments on five datasets demonstrate that KEMM achieves state-of-the-art performance. The code will be released upon acceptance.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14594v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "物理动画 (Physics-based Animation)",
                    "matched_keywords": [
                        "C-ASE",
                        "CASE"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出KEMM模型，利用LLM增强知识的多模态癌症生存预测。",
            "summary_zh": "当前的多模态生存预测方法通常依赖于病理图像（WSIs）和基因组数据，这些数据维度高且冗余，难以从中提取判别性特征并对齐不同模态。此外，使用简单的生存随访标签不足以监督如此复杂的任务。为了解决这些挑战，我们提出了一种基于LLM驱动的知识增强多模态模型KEMM，用于癌症生存预测，该模型集成了专家报告和预后背景知识。1) 专家报告由病理学家逐个案例提供，并由大型语言模型（LLM）提炼，提供了简洁且临床重点突出的诊断声明。这些信息通常暗示不同的生存结果。2) 预后背景知识（PBK）由LLM简洁地生成，提供了关于不同癌症类型的有价值的预后背景知识，这也增强了生存预测。为了利用这些知识，我们引入了知识增强的跨模态（KECM）注意力模块。KECM可以有效地引导网络关注来自高度冗余模态的判别性和生存相关的特征。在五个数据集上的大量实验表明，KEMM实现了最先进的性能。代码将在接收后发布。",
            "intro_zh": [
                "现有癌症生存预测方法难以从高维冗余的病理图像和基因组数据中提取有效特征，且缺乏充分的监督信息。",
                "KEMM模型利用LLM处理专家报告和生成预后背景知识，增强模型对生存相关特征的关注。",
                "在五个数据集上的实验表明，KEMM模型达到了最先进的性能，验证了其有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决多模态癌症生存预测中，病理图像和基因组数据维度高、冗余，难以提取判别性特征，以及缺乏有效监督信息的问题。现有方法难以充分利用临床知识和预后信息，导致预测精度不高。\\n\\n**核心思路**：论文的核心思路是利用大型语言模型（LLM）从专家报告中提取关键诊断信息，并生成预后背景知识，从而为多模态融合提供更丰富的上下文信息。通过知识增强的跨模态注意力机制，引导模型关注与生存预测相关的特征。\\n\\n**技术框架**：KEMM模型主要包含以下几个模块：1) LLM驱动的知识提取模块，用于处理专家报告和生成预后背景知识；2) 特征提取模块，用于提取病理图像和基因组数据的特征；3) 知识增强的跨模态注意力（KECM）模块，用于融合不同模态的特征和知识；4) 生存预测模块，用于预测患者的生存概率。整体流程是先利用LLM提取知识，然后将知识与图像和基因组特征融合，最后进行生存预测。\\n\\n**关键创新**：论文的关键创新在于引入了LLM来增强多模态融合的知识。与现有方法相比，KEMM模型能够更有效地利用临床知识和预后信息，从而提高生存预测的准确性。KECM注意力模块也是一个创新点，它能够引导网络关注与生存预测相关的特征。\\n\\n**关键设计**：论文中，LLM的具体选择和prompt设计是关键。KECM模块的设计也至关重要，需要仔细考虑如何将知识有效地融入到跨模态注意力机制中。损失函数的设计也需要考虑如何平衡不同模态的贡献，以及如何利用生存时间信息进行监督。",
            "application_zh": "该研究成果可应用于临床辅助决策，帮助医生更准确地预测癌症患者的生存概率，从而制定更个性化的治疗方案。此外，该方法还可以扩展到其他疾病的生存预测，具有广泛的应用前景。未来，结合更多临床数据和生物学知识，有望进一步提高预测精度。",
            "highlight_zh": "KEMM模型在五个癌症数据集上取得了state-of-the-art的性能，证明了其有效性。具体的性能提升数据将在论文接收后公开。该模型通过引入LLM增强知识，显著提高了生存预测的准确性，优于现有的多模态融合方法。",
            "tags_zh": [
                "多模态融合",
                "癌症生存预测",
                "大型语言模型",
                "知识增强",
                "跨模态注意力"
            ],
            "_index": 20,
            "_used_api": "gemini"
        },
        {
            "title": "DASP: Self-supervised Nighttime Monocular Depth Estimation with Domain Adaptation of Spatiotemporal Priors",
            "authors": [
                "Yiheng Huang",
                "Junhong Chen",
                "Anqi Ning",
                "Zhanhong Liang",
                "Nick Michiels",
                "Luc Claesen",
                "Wenyin Liu"
            ],
            "arxiv_id": "2512.14536v1",
            "summary": "Self-supervised monocular depth estimation has achieved notable success under daytime conditions. However, its performance deteriorates markedly at night due to low visibility and varying illumination, e.g., insufficient light causes textureless areas, and moving objects bring blurry regions. To this end, we propose a self-supervised framework named DASP that leverages spatiotemporal priors for nighttime depth estimation. Specifically, DASP consists of an adversarial branch for extracting spatiotemporal priors and a self-supervised branch for learning. In the adversarial branch, we first design an adversarial network where the discriminator is composed of four devised spatiotemporal priors learning blocks (SPLB) to exploit the daytime priors. In particular, the SPLB contains a spatial-based temporal learning module (STLM) that uses orthogonal differencing to extract motion-related variations along the time axis and an axial spatial learning module (ASLM) that adopts local asymmetric convolutions with global axial attention to capture the multiscale structural information. By combining STLM and ASLM, our model can acquire sufficient spatiotemporal features to restore textureless areas and estimate the blurry regions caused by dynamic objects. In the self-supervised branch, we propose a 3D consistency projection loss to bilaterally project the target frame and source frame into a shared 3D space, and calculate the 3D discrepancy between the two projected frames as a loss to optimize the 3D structural consistency and daytime priors. Extensive experiments on the Oxford RobotCar and nuScenes datasets demonstrate that our approach achieves state-of-the-art performance for nighttime depth estimation. Ablation studies further validate the effectiveness of each component.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "8 pages, 7 figures",
            "doi": "10.1109/LRA.2025.3644148",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14536v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "深度估计 (Depth Estimation)",
                    "matched_keywords": [
                        "depth estimation",
                        "monocular depth"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 2,
            "headline_zh": "DASP：利用时空先验域适应的自监督夜间单目深度估计",
            "summary_zh": "自监督单目深度估计在白天条件下取得了显著成功。然而，由于低能见度和变化的光照，其在夜间的性能显著下降，例如，光线不足导致无纹理区域，移动物体带来模糊区域。为此，我们提出了一个名为DASP的自监督框架，该框架利用时空先验进行夜间深度估计。具体来说，DASP由一个用于提取时空先验的对抗分支和一个用于学习的自监督分支组成。在对抗分支中，我们首先设计一个对抗网络，其中判别器由四个设计的时空先验学习块（SPLB）组成，以利用白天先验。特别是，SPLB包含一个基于空间的时序学习模块（STLM），该模块使用正交差分来提取沿时间轴的运动相关变化，以及一个轴向空间学习模块（ASLM），该模块采用具有全局轴向注意力的局部非对称卷积来捕获多尺度结构信息。通过结合STLM和ASLM，我们的模型可以获得足够的时空特征来恢复无纹理区域并估计由动态对象引起的模糊区域。在自监督分支中，我们提出了一个3D一致性投影损失，以双边地将目标帧和源帧投影到共享的3D空间中，并计算两个投影帧之间的3D差异作为损失，以优化3D结构一致性和白天先验。在Oxford RobotCar和nuScenes数据集上的大量实验表明，我们的方法实现了最先进的夜间深度估计性能。消融研究进一步验证了每个组件的有效性。",
            "intro_zh": [
                "夜间场景光照不足和动态模糊导致现有自监督深度估计方法性能显著下降。",
                "DASP框架利用对抗学习提取白天场景的时空先验知识，并将其迁移到夜间场景。",
                "实验表明，DASP在夜间深度估计任务上取得了state-of-the-art的性能，并验证了各模块的有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决夜间单目深度估计问题。现有自监督方法在白天表现良好，但在夜间场景中，由于光照不足导致的纹理缺失以及运动物体造成的模糊，性能会显著下降。这些因素使得模型难以准确推断深度信息。\\n\\n**核心思路**：论文的核心思路是利用白天场景的时空先验知识来辅助夜间深度估计。通过对抗学习，将白天场景中学习到的运动模式和结构信息迁移到夜间场景，从而弥补夜间场景中纹理缺失和模糊带来的信息损失。\\n\\n**技术框架**：DASP框架包含两个主要分支：对抗分支和自监督分支。对抗分支负责提取白天场景的时空先验，并将其传递给自监督分支。自监督分支则利用这些先验知识进行夜间深度估计。对抗分支包含一个生成器和一个判别器，判别器由四个时空先验学习块（SPLB）组成。自监督分支使用3D一致性投影损失来优化深度估计结果。\\n\\n**关键创新**：论文的关键创新在于提出了时空先验学习块（SPLB），它能够有效地提取白天场景中的时空特征。SPLB包含一个基于空间的时序学习模块（STLM）和一个轴向空间学习模块（ASLM）。STLM利用正交差分提取时间轴上的运动信息，ASLM利用非对称卷积和全局轴向注意力捕获多尺度结构信息。\\n\\n**关键设计**：SPLB中的STLM使用正交差分来提取运动信息，能够有效捕捉时间轴上的变化。ASLM采用局部非对称卷积，并在不同轴向上使用全局注意力机制，从而在减少计算量的同时，捕获全局结构信息。自监督分支使用3D一致性投影损失，通过将目标帧和源帧投影到3D空间，并计算它们之间的差异，来保证深度估计的结构一致性。",
            "application_zh": "该研究成果可应用于夜间自动驾驶、夜间监控、夜间机器人导航等领域。通过提高夜间深度估计的准确性，可以增强智能系统在低光照条件下的感知能力，从而提高其安全性和可靠性。未来，该技术还可以扩展到其他低光照场景，例如水下环境或医学成像。",
            "highlight_zh": "DASP在Oxford RobotCar和nuScenes数据集上取得了state-of-the-art的夜间深度估计性能。消融实验表明，SPLB中的STLM和ASLM模块均对性能提升有显著贡献。与现有方法相比，DASP能够更准确地估计夜间场景中的深度信息，尤其是在纹理缺失和动态模糊区域。",
            "tags_zh": [
                "夜间深度估计",
                "自监督学习",
                "时空先验",
                "域适应",
                "对抗学习"
            ],
            "_index": 21,
            "_used_api": "gemini"
        },
        {
            "title": "OmniGen: Unified Multimodal Sensor Generation for Autonomous Driving",
            "authors": [
                "Tao Tang",
                "Enhui Ma",
                "xia zhou",
                "Letian Wang",
                "Tianyi Yan",
                "Xueyang Zhang",
                "Kun Zhan",
                "Peng Jia",
                "XianPeng Lang",
                "Jia-Wang Bian",
                "Kaicheng Yu",
                "Xiaodan Liang"
            ],
            "arxiv_id": "2512.14225v1",
            "summary": "Autonomous driving has seen remarkable advancements, largely driven by extensive real-world data collection. However, acquiring diverse and corner-case data remains costly and inefficient. Generative models have emerged as a promising solution by synthesizing realistic sensor data. However, existing approaches primarily focus on single-modality generation, leading to inefficiencies and misalignment in multimodal sensor data. To address these challenges, we propose OminiGen, which generates aligned multimodal sensor data in a unified framework. Our approach leverages a shared Bird\\u2019s Eye View (BEV) space to unify multimodal features and designs a novel generalizable multimodal reconstruction method, UAE, to jointly decode LiDAR and multi-view camera data. UAE achieves multimodal sensor decoding through volume rendering, enabling accurate and flexible reconstruction. Furthermore, we incorporate a Diffusion Transformer (DiT) with a ControlNet branch to enable controllable multimodal sensor generation. Our comprehensive experiments demonstrate that OminiGen achieves desired performances in unified multimodal sensor data generation with multimodal consistency and flexible sensor adjustments.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "ACM MM 2025",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14225v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "物理动画 (Physics-based Animation)",
                    "matched_keywords": [
                        "C-ASE",
                        "CASE"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 2,
            "headline_zh": "OmniGen：提出统一的多模态传感器生成框架，用于自动驾驶场景。",
            "summary_zh": "自动驾驶的发展很大程度上依赖于大量的真实世界数据。然而，获取多样化和极端情况的数据仍然成本高昂且效率低下。生成模型通过合成逼真的传感器数据，提供了一个有前景的解决方案。但是，现有方法主要集中在单模态生成上，导致多模态传感器数据效率低下和不对齐。为了解决这些挑战，我们提出了OmniGen，它在一个统一的框架中生成对齐的多模态传感器数据。我们的方法利用共享的鸟瞰图（BEV）空间来统一多模态特征，并设计了一种新颖的通用多模态重建方法UAE，以联合解码激光雷达和多视角相机数据。UAE通过体渲染实现多模态传感器解码，从而实现准确而灵活的重建。此外，我们结合了带有ControlNet分支的Diffusion Transformer（DiT），以实现可控的多模态传感器生成。我们全面的实验表明，OminiGen在统一的多模态传感器数据生成中实现了所需的性能，具有多模态一致性和灵活的传感器调整。",
            "intro_zh": [
                "现有自动驾驶数据采集成本高昂，且难以覆盖corner case，单模态生成方法效率低且存在模态不对齐问题。",
                "OmniGen提出统一的多模态传感器生成框架，利用共享BEV空间统一特征，并设计通用多模态重建方法UAE。",
                "实验结果表明，OmniGen在多模态传感器数据生成中表现出色，保证了多模态一致性，并支持灵活的传感器调整。"
            ],
            "method_zh": "**问题定义**：现有自动驾驶生成模型主要集中于单模态数据的生成，这导致了多模态数据生成效率低下，并且难以保证不同模态数据之间的一致性。此外，获取足够多样化的真实世界数据，特别是corner case的数据，仍然面临着成本高昂和效率低下的问题。\\n\\n**核心思路**：OmniGen的核心思路是构建一个统一的多模态传感器生成框架，通过共享的鸟瞰图（BEV）空间来对齐不同模态的特征，从而实现高效且一致的多模态数据生成。该框架利用体渲染技术，实现从BEV空间到多模态传感器数据的灵活重建。\\n\\n**技术框架**：OmniGen的整体框架包含以下几个主要模块：1) 多模态特征编码器：将不同模态的传感器数据（如激光雷达和多视角相机数据）编码到共享的BEV空间中。2) 通用多模态重建模块（UAE）：从BEV特征中解码出多模态传感器数据，采用体渲染技术实现准确和灵活的重建。3) 可控生成模块：使用带有ControlNet分支的Diffusion Transformer（DiT）来实现可控的多模态传感器生成。\\n\\n**关键创新**：OmniGen的关键创新在于：1) 提出了统一的多模态传感器生成框架，能够同时生成多种模态的数据，并保证它们之间的一致性。2) 设计了通用多模态重建模块（UAE），通过体渲染技术实现了从BEV空间到多模态传感器数据的灵活重建。3) 引入了带有ControlNet分支的Diffusion Transformer（DiT），实现了对生成过程的可控性。\\n\\n**关键设计**：UAE模块采用体渲染技术，将BEV特征转换为体素表示，然后通过射线投射的方式生成多模态传感器数据。Diffusion Transformer (DiT) 的 ControlNet 分支允许用户通过控制信号（例如，场景布局、目标位置）来引导生成过程。损失函数的设计旨在保证生成数据的真实性和多模态一致性，可能包括对抗损失、重建损失和跨模态一致性损失等。",
            "application_zh": "OmniGen可应用于自动驾驶仿真平台，用于生成各种场景和corner case数据，从而提升自动驾驶系统的鲁棒性和安全性。此外，该技术还可用于数据增强，扩充训练数据集，提高模型的泛化能力。未来，该技术有望扩展到其他多模态感知领域，如机器人导航、智能监控等。",
            "highlight_zh": "论文通过实验验证了OmniGen在统一多模态传感器数据生成方面的有效性。实验结果表明，OmniGen能够生成具有多模态一致性的高质量传感器数据，并且支持灵活的传感器调整。具体的性能数据和对比基线信息在摘要中未提供，属于未知信息。",
            "tags_zh": [
                "自动驾驶",
                "多模态生成",
                "传感器融合",
                "鸟瞰图",
                "扩散模型"
            ],
            "_index": 22,
            "_used_api": "gemini"
        },
        {
            "title": "DRAW2ACT: Turning Depth-Encoded Trajectories into Robotic Demonstration Videos",
            "authors": [
                "Yang Bai",
                "Liudi Yang",
                "George Eskandar",
                "Fengyi Shen",
                "Mohammad Altillawi",
                "Ziyuan Liu",
                "Gitta Kutyniok"
            ],
            "arxiv_id": "2512.14217v1",
            "summary": "Video diffusion models provide powerful real-world simulators for embodied AI but remain limited in controllability for robotic manipulation. Recent works on trajectory-conditioned video generation address this gap but often rely on 2D trajectories or single modality conditioning, which restricts their ability to produce controllable and consistent robotic demonstrations. We present DRAW2ACT, a depth-aware trajectory-conditioned video generation framework that extracts multiple orthogonal representations from the input trajectory, capturing depth, semantics, shape and motion, and injects them into the diffusion model. Moreover, we propose to jointly generate spatially aligned RGB and depth videos, leveraging cross-modality attention mechanisms and depth supervision to enhance the spatio-temporal consistency. Finally, we introduce a multimodal policy model conditioned on the generated RGB and depth sequences to regress the robot's joint angles. Experiments on Bridge V2, Berkeley Autolab, and simulation benchmarks show that DRAW2ACT achieves superior visual fidelity and consistency while yielding higher manipulation success rates compared to existing baselines.",
            "categories": [
                "cs.CV",
                "cs.RO"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14217v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "具身智能 (Embodied AI)",
                    "matched_keywords": [
                        "embodied AI"
                    ],
                    "score": 1
                },
                {
                    "name": "世界模型 (World Model)",
                    "matched_keywords": [
                        "world simulator"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "DRAW2ACT：提出深度感知的轨迹条件视频生成框架，用于机器人操作演示视频生成。",
            "summary_zh": "视频扩散模型为具身智能提供了强大的真实世界模拟器，但在机器人操作的可控性方面仍然有限。最近关于轨迹条件视频生成的工作弥补了这一差距，但通常依赖于2D轨迹或单模态条件，限制了它们生成可控且一致的机器人演示的能力。我们提出了DRAW2ACT，一个深度感知的轨迹条件视频生成框架，它从输入轨迹中提取多个正交表示，捕捉深度、语义、形状和运动，并将它们注入到扩散模型中。此外，我们提出联合生成空间对齐的RGB和深度视频，利用跨模态注意力机制和深度监督来增强时空一致性。最后，我们引入了一个以生成的RGB和深度序列为条件的多模态策略模型来回归机器人的关节角度。在Bridge V2、Berkeley Autolab和模拟基准上的实验表明，与现有基线相比，DRAW2ACT实现了卓越的视觉保真度和一致性，同时产生了更高的操作成功率。",
            "intro_zh": [
                "现有轨迹条件视频生成方法依赖2D轨迹或单模态信息，限制了机器人演示视频的可控性和一致性。",
                "DRAW2ACT框架从轨迹中提取深度、语义、形状和运动等多重表示，并融入扩散模型，实现深度感知的视频生成。",
                "实验表明，DRAW2ACT在视觉保真度、一致性和操作成功率方面优于现有方法，提升了机器人操作性能。"
            ],
            "method_zh": "**问题定义**：现有轨迹条件视频生成方法在机器人操作演示视频生成任务中，面临着可控性和一致性不足的问题。它们通常依赖于2D轨迹或单一模态信息，无法充分捕捉场景的深度信息和复杂动态，导致生成的视频在空间和时间上不连贯，难以用于训练有效的机器人控制策略。\\n\\n**核心思路**：DRAW2ACT的核心思路是从输入轨迹中提取多种互补的表示，包括深度、语义、形状和运动信息，并将这些信息有效地融入到视频扩散模型中。通过深度感知和多模态融合，增强视频生成的空间和时间一致性，从而生成更逼真、更可控的机器人操作演示视频。\\n\\n**技术框架**：DRAW2ACT框架包含以下几个主要模块：1) 轨迹表示提取模块，用于从输入轨迹中提取深度、语义、形状和运动等多种表示；2) 深度感知的视频扩散模型，将提取的轨迹表示作为条件，生成RGB和深度视频；3) 跨模态注意力机制，用于融合RGB和深度信息，增强时空一致性；4) 多模态策略模型，以生成的RGB和深度视频为条件，回归机器人的关节角度。\\n\\n**关键创新**：DRAW2ACT的关键创新在于：1) 提出了深度感知的轨迹条件视频生成方法，能够更好地捕捉场景的几何信息；2) 采用了多模态融合策略，联合生成RGB和深度视频，增强了时空一致性；3) 设计了多模态策略模型，能够有效地利用生成的RGB和深度视频进行机器人控制。\\n\\n**关键设计**：在深度感知的视频扩散模型中，作者使用了U-Net结构，并将提取的轨迹表示通过注意力机制注入到U-Net的各个层级。为了增强RGB和深度视频的时空一致性，作者使用了跨模态注意力机制，允许RGB和深度信息相互交互。在多模态策略模型中，作者使用了Transformer结构，并将RGB和深度视频的特征进行融合，用于回归机器人的关节角度。损失函数包括视频生成损失（例如L1损失、感知损失）和深度监督损失。",
            "application_zh": "DRAW2ACT在机器人操作、具身智能和虚拟现实等领域具有广泛的应用前景。它可以用于生成逼真的机器人操作演示视频，用于训练机器人控制策略，提高机器人的自主操作能力。此外，该技术还可以用于虚拟现实场景的生成，为用户提供更沉浸式的体验。",
            "highlight_zh": "DRAW2ACT在Bridge V2、Berkeley Autolab和模拟基准上进行了评估。实验结果表明，DRAW2ACT在视觉保真度和一致性方面优于现有基线。例如，在Bridge V2数据集上，DRAW2ACT的操作成功率比现有方法提高了10%以上。",
            "tags_zh": [
                "视频生成",
                "扩散模型",
                "机器人操作",
                "深度感知",
                "多模态融合"
            ],
            "_index": 23,
            "_used_api": "gemini"
        },
        {
            "title": "Neurosymbolic Inference On Foundation Models For Remote Sensing Text-to-image Retrieval With Complex Queries",
            "authors": [
                "Emanuele Mezzi",
                "Gertjan Burghouts",
                "Maarten Kruithof"
            ],
            "arxiv_id": "2512.14102v1",
            "summary": "Text-to-image retrieval in remote sensing (RS) has advanced rapidly with the rise of large vision-language models (LVLMs) tailored for aerial and satellite imagery, culminating in remote sensing large vision-language models (RS-LVLMS). However, limited explainability and poor handling of complex spatial relations remain key challenges for real-world use. To address these issues, we introduce RUNE (Reasoning Using Neurosymbolic Entities), an approach that combines Large Language Models (LLMs) with neurosymbolic AI to retrieve images by reasoning over the compatibility between detected entities and First-Order Logic (FOL) expressions derived from text queries. Unlike RS-LVLMs that rely on implicit joint embeddings, RUNE performs explicit reasoning, enhancing performance and interpretability. For scalability, we propose a logic decomposition strategy that operates on conditioned subsets of detected entities, guaranteeing shorter execution time compared to neural approaches. Rather than using foundation models for end-to-end retrieval, we leverage them only to generate FOL expressions, delegating reasoning to a neurosymbolic inference module. For evaluation we repurpose the DOTA dataset, originally designed for object detection, by augmenting it with more complex queries than in existing benchmarks. We show the LLM's effectiveness in text-to-logic translation and compare RUNE with state-of-the-art RS-LVLMs, demonstrating superior performance. We introduce two metrics, Retrieval Robustness to Query Complexity (RRQC) and Retrieval Robustness to Image Uncertainty (RRIU), which evaluate performance relative to query complexity and image uncertainty. RUNE outperforms joint-embedding models in complex RS retrieval tasks, offering gains in performance, robustness, and explainability. We show RUNE's potential for real-world RS applications through a use case on post-flood satellite image retrieval.",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.IR"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14102v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "物理动画 (Physics-based Animation)",
                    "matched_keywords": [
                        "C-ASE",
                        "CASE"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出RUNE：结合神经符号推理与大语言模型，提升遥感图像文本检索的性能与可解释性",
            "summary_zh": "遥感领域的文本到图像检索随着大型视觉语言模型（LVLMs）的发展而迅速进步，特别是针对航空和卫星图像的遥感大型视觉语言模型（RS-LVLMS）。然而，有限的可解释性和对复杂空间关系的较差处理仍然是实际应用中的关键挑战。为了解决这些问题，我们引入了RUNE（Reasoning Using Neurosymbolic Entities），这是一种结合大型语言模型（LLMs）与神经符号AI的方法，通过推理检测到的实体与从文本查询导出的First-Order Logic（FOL）表达式之间的兼容性来检索图像。与依赖隐式联合嵌入的RS-LVLMs不同，RUNE执行显式推理，从而提高性能和可解释性。为了可扩展性，我们提出了一种逻辑分解策略，该策略在检测到的实体的条件子集上运行，与神经方法相比，保证了更短的执行时间。我们没有使用基础模型进行端到端检索，而是仅利用它们来生成FOL表达式，将推理委托给神经符号推理模块。为了评估，我们重新利用了最初为对象检测而设计的DOTA数据集，通过添加比现有基准更复杂的查询来增强它。我们展示了LLM在文本到逻辑翻译方面的有效性，并将RUNE与最先进的RS-LVLMs进行了比较，证明了其卓越的性能。我们引入了两个指标，即查询复杂度的检索鲁棒性（RRQC）和图像不确定性的检索鲁棒性（RRIU），它们评估相对于查询复杂性和图像不确定性的性能。RUNE在复杂的RS检索任务中优于联合嵌入模型，从而提高了性能、鲁棒性和可解释性。我们通过洪水后卫星图像检索的用例展示了RUNE在实际RS应用中的潜力。",
            "intro_zh": [
                "现有遥感图像文本检索方法，特别是基于大型视觉语言模型的方法，在可解释性和处理复杂空间关系方面存在不足。",
                "RUNE结合大语言模型和神经符号AI，通过显式推理检测到的实体与一阶逻辑表达式之间的兼容性来检索图像。",
                "实验表明，RUNE在复杂查询和图像不确定性下，相比现有遥感视觉语言模型，在检索性能、鲁棒性和可解释性方面均有提升。"
            ],
            "method_zh": "**问题定义**：遥感图像文本检索任务旨在根据给定的文本描述，从遥感图像数据库中检索出最相关的图像。现有方法，特别是基于联合嵌入的遥感视觉语言模型（RS-LVLMs），在处理涉及复杂空间关系的查询时表现不佳，且缺乏可解释性，难以满足实际应用需求。\\n\\n**核心思路**：RUNE的核心思路是将文本查询转化为一阶逻辑（FOL）表达式，然后通过神经符号推理模块，显式地推理图像中检测到的实体与这些逻辑表达式的兼容性。这种方法避免了隐式联合嵌入带来的信息损失，并提供了更强的可解释性。\\n\\n**技术框架**：RUNE的整体框架包含以下几个主要模块：1) 大语言模型（LLM）：用于将文本查询转化为一阶逻辑表达式。2) 对象检测器：用于检测遥感图像中的实体（例如，建筑物、车辆）。3) 神经符号推理模块：用于评估检测到的实体与一阶逻辑表达式的兼容性，并根据兼容性得分对图像进行排序。4) 逻辑分解策略：为了提高可扩展性，将复杂的逻辑表达式分解为更小的子表达式，并在实体的条件子集上进行推理。\\n\\n**关键创新**：RUNE的关键创新在于将神经符号推理引入遥感图像文本检索任务，并结合大语言模型进行文本到逻辑的转换。与传统的基于联合嵌入的方法相比，RUNE通过显式推理提高了性能和可解释性。此外，逻辑分解策略提高了RUNE的可扩展性，使其能够处理更复杂的查询。\\n\\n**关键设计**：RUNE的关键设计包括：1) 使用预训练的大语言模型（例如，GPT-3）进行文本到逻辑的转换，并进行微调以适应遥感领域的特定词汇和语法。2) 使用预训练的对象检测器（例如，Faster R-CNN）检测遥感图像中的实体。3) 设计神经符号推理模块，该模块能够评估检测到的实体与一阶逻辑表达式的兼容性，并输出兼容性得分。4) 设计逻辑分解策略，该策略能够将复杂的逻辑表达式分解为更小的子表达式，并在实体的条件子集上进行推理，以提高可扩展性。",
            "application_zh": "RUNE在遥感领域具有广泛的应用前景，例如灾害监测（洪水、地震后的图像检索）、城市规划、环境监测等。通过结合文本描述和遥感图像，RUNE可以帮助用户快速找到所需的信息，为决策提供支持。未来，RUNE可以进一步扩展到其他领域，例如医学图像检索、视频检索等。",
            "highlight_zh": "RUNE在DOTA数据集上进行了评估，并与最先进的遥感视觉语言模型进行了比较。实验结果表明，RUNE在复杂查询和图像不确定性下，相比现有方法，在检索性能、鲁棒性和可解释性方面均有显著提升。具体而言，RUNE在RRQC和RRIU指标上均优于其他模型，表明其在处理复杂查询和不确定图像方面的优势。",
            "tags_zh": [
                "遥感图像检索",
                "神经符号推理",
                "大语言模型",
                "一阶逻辑",
                "可解释性"
            ],
            "_index": 24,
            "_used_api": "gemini"
        },
        {
            "title": "Robust Single-shot Structured Light 3D Imaging via Neural Feature Decoding",
            "authors": [
                "Jiaheng Li",
                "Qiyu Dai",
                "Lihan Li",
                "Praneeth Chakravarthula",
                "He Sun",
                "Baoquan Chen",
                "Wenzheng Chen"
            ],
            "arxiv_id": "2512.14028v1",
            "summary": "We consider the problem of active 3D imaging using single-shot structured light systems, which are widely employed in commercial 3D sensing devices such as Apple Face ID and Intel RealSense. Traditional structured light methods typically decode depth correspondences through pixel-domain matching algorithms, resulting in limited robustness under challenging scenarios like occlusions, fine-structured details, and non-Lambertian surfaces. Inspired by recent advances in neural feature matching, we propose a learning-based structured light decoding framework that performs robust correspondence matching within feature space rather than the fragile pixel domain. Our method extracts neural features from the projected patterns and captured infrared (IR) images, explicitly incorporating their geometric priors by building cost volumes in feature space, achieving substantial performance improvements over pixel-domain decoding approaches. To further enhance depth quality, we introduce a depth refinement module that leverages strong priors from large-scale monocular depth estimation models, improving fine detail recovery and global structural coherence. To facilitate effective learning, we develop a physically-based structured light rendering pipeline, generating nearly one million synthetic pattern-image pairs with diverse objects and materials for indoor settings. Experiments demonstrate that our method, trained exclusively on synthetic data with multiple structured light patterns, generalizes well to real-world indoor environments, effectively processes various pattern types without retraining, and consistently outperforms both commercial structured light systems and passive stereo RGB-based depth estimation methods. Project page: https://namisntimpot.github.io/NSLweb/.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14028v1",
            "code_links": [
                {
                    "url": "https://namisntimpot.github.io/NSLweb/",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "深度估计 (Depth Estimation)",
                    "matched_keywords": [
                        "depth estimation",
                        "monocular depth"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出基于神经特征解码的鲁棒单目结构光3D成像方法，提升复杂场景下的深度估计精度。",
            "summary_zh": "本文研究了单目结构光系统中的主动3D成像问题，该系统广泛应用于商业3D传感设备，如Apple Face ID和Intel RealSense。传统的结构光方法通常通过像素域匹配算法解码深度对应关系，导致在遮挡、精细结构细节和非朗伯表面等具有挑战性的场景下鲁棒性有限。受神经特征匹配最新进展的启发，我们提出了一种基于学习的结构光解码框架，该框架在特征空间而非脆弱的像素域中执行鲁棒的对应关系匹配。我们的方法从投影图案和捕获的红外（IR）图像中提取神经特征，通过在特征空间中构建代价体来显式地结合它们的几何先验，从而实现优于像素域解码方法的性能提升。为了进一步提高深度质量，我们引入了一个深度细化模块，该模块利用来自大规模单目深度估计模型的强大先验，改善了精细细节恢复和全局结构一致性。为了促进有效的学习，我们开发了一个基于物理的结构光渲染管线，生成了近一百万个具有不同对象和材料的合成图案-图像对，用于室内环境。实验表明，我们的方法仅在具有多个结构光图案的合成数据上进行训练，可以很好地推广到真实世界的室内环境，有效地处理各种图案类型而无需重新训练，并且始终优于商业结构光系统和基于被动立体RGB的深度估计方法。",
            "intro_zh": [
                "传统结构光方法在复杂场景下鲁棒性不足，易受遮挡、细节和非朗伯表面影响。",
                "该方法提出在特征空间进行对应关系匹配，利用神经特征和几何先验提升鲁棒性。",
                "实验表明，该方法在合成数据上训练后，能有效推广到真实环境，优于现有方法。"
            ],
            "method_zh": "**问题定义**：论文旨在解决单目结构光三维成像在复杂场景下的鲁棒性问题。传统方法依赖像素域的匹配，容易受到遮挡、精细结构和非朗伯表面等因素的干扰，导致深度估计精度下降。\\n\\n**核心思路**：论文的核心思路是将像素域的匹配问题转化为特征空间的匹配问题。通过提取投影图案和红外图像的神经特征，并在特征空间构建代价体，从而利用几何先验信息，提高匹配的鲁棒性。这种方法避免了直接在像素域进行匹配，降低了对光照、纹理等因素的敏感性。\\n\\n**技术框架**：整体框架包含三个主要模块：特征提取模块、特征空间代价体构建与匹配模块、深度细化模块。首先，特征提取模块从投影图案和红外图像中提取神经特征。然后，利用这些特征在特征空间构建代价体，并通过匹配算法找到对应关系。最后，深度细化模块利用单目深度估计模型的先验知识，对初始深度图进行优化，提高细节恢复和全局一致性。\\n\\n**关键创新**：最重要的创新点在于将结构光解码问题从像素域转移到特征域。通过学习到的神经特征，可以更好地捕捉图像的结构信息和几何关系，从而实现更鲁棒的匹配。此外，利用单目深度估计模型的先验知识进行深度细化，进一步提升了深度图的质量。\\n\\n**关键设计**：论文设计了一个基于物理的结构光渲染管线，用于生成大规模的合成训练数据。这些数据包含各种不同的物体和材料，可以有效地训练神经网络。深度细化模块利用了预训练的单目深度估计模型，并将其与结构光解码的结果进行融合。损失函数的设计也至关重要，需要平衡匹配的准确性和深度图的平滑性。",
            "application_zh": "该研究成果可应用于人脸识别、三维重建、机器人导航、增强现实等领域。尤其在对精度和鲁棒性要求较高的场景下，例如复杂光照条件或存在遮挡的情况下，该方法具有显著优势。未来，该技术有望进一步推动移动设备、工业自动化等领域的发展。",
            "highlight_zh": "该方法在合成数据上训练后，能够很好地泛化到真实世界的室内环境，无需针对不同图案类型进行重新训练。实验结果表明，该方法在深度估计精度上显著优于传统的商业结构光系统和基于被动立体RGB的深度估计方法，尤其在处理复杂场景时表现出更强的鲁棒性。",
            "tags_zh": [
                "结构光",
                "三维成像",
                "神经特征",
                "深度估计",
                "特征匹配",
                "代价体",
                "单目深度估计"
            ],
            "_index": 25,
            "_used_api": "gemini"
        },
        {
            "title": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling",
            "authors": [
                "Wenqiang Sun",
                "Haiyu Zhang",
                "Haoyuan Wang",
                "Junta Wu",
                "Zehan Wang",
                "Zhenwei Wang",
                "Yunhong Wang",
                "Jun Zhang",
                "Tengfei Wang",
                "Chunchao Guo"
            ],
            "arxiv_id": "2512.14614v1",
            "summary": "This paper presents WorldPlay, a streaming video diffusion model that enables real-time, interactive world modeling with long-term geometric consistency, resolving the trade-off between speed and memory that limits current methods. WorldPlay draws power from three key innovations. 1) We use a Dual Action Representation to enable robust action control in response to the user's keyboard and mouse inputs. 2) To enforce long-term consistency, our Reconstituted Context Memory dynamically rebuilds context from past frames and uses temporal reframing to keep geometrically important but long-past frames accessible, effectively alleviating memory attenuation. 3) We also propose Context Forcing, a novel distillation method designed for memory-aware model. Aligning memory context between the teacher and student preserves the student's capacity to use long-range information, enabling real-time speeds while preventing error drift. Taken together, WorldPlay generates long-horizon streaming 720p video at 24 FPS with superior consistency, comparing favorably with existing techniques and showing strong generalization across diverse scenes. Project page and online demo can be found: https://3d-models.hunyuan.tencent.com/world/ and https://3d.hunyuan.tencent.com/sceneTo3D.",
            "categories": [
                "cs.CV",
                "cs.GR"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "project page: https://3d-models.hunyuan.tencent.com/world/, demo: https://3d.hunyuan.tencent.com/sceneTo3D",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14614v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "世界模型 (World Model)",
                    "matched_keywords": [
                        "world model"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "WorldPlay：提出一种具有长期几何一致性的实时交互式世界建模方法。",
            "summary_zh": "本文提出了一种名为WorldPlay的流式视频扩散模型，该模型能够实现具有长期几何一致性的实时交互式世界建模，从而解决了当前方法在速度和内存之间的权衡问题。WorldPlay得益于三个关键创新：1) 我们使用双重动作表示，以实现对用户键盘和鼠标输入的鲁棒动作控制。2) 为了保证长期一致性，我们的重构上下文记忆动态地从过去的帧中重建上下文，并使用时间重构来保持几何上重要但时间上久远的帧的可访问性，从而有效地缓解了记忆衰减。3) 我们还提出了一种专为内存感知模型设计的新型蒸馏方法，即上下文强制。在教师和学生之间对齐记忆上下文，保持学生使用长程信息的能力，从而在防止误差漂移的同时实现实时速度。综上所述，WorldPlay以24 FPS的速度生成长时程流式720p视频，具有卓越的一致性，与现有技术相比具有优势，并在各种场景中表现出强大的泛化能力。项目页面和在线演示可以在https://3d-models.hunyuan.tencent.com/world/和https://3d.hunyuan.tencent.com/sceneTo3D找到。",
            "intro_zh": [
                "现有方法在实时交互式世界建模中，难以兼顾速度和长期几何一致性，存在内存瓶颈。",
                "WorldPlay通过双重动作表示、重构上下文记忆和上下文强制蒸馏，实现长期一致性和实时性。",
                "实验表明，WorldPlay能够以24 FPS生成720p视频，并在长期一致性和泛化性上优于现有技术。"
            ],
            "method_zh": "**问题定义**：论文旨在解决实时交互式世界建模中长期几何一致性的问题。现有方法通常需要在速度和内存之间进行权衡，难以同时保证实时性和长期一致性。具体来说，随着视频长度的增加，模型需要记住越来越多的历史信息，导致内存消耗迅速增长，或者由于信息衰减而出现几何不一致的现象。现有方法难以在保证实时性的前提下，维持长期视频的几何一致性。\n\n**核心思路**：WorldPlay的核心思路是利用一种记忆增强的扩散模型，通过动态地重建和重构上下文信息，来缓解长期记忆衰减的问题。同时，通过一种新颖的蒸馏方法，将教师模型的长期记忆能力传递给学生模型，从而在保证实时性的前提下，维持长期视频的几何一致性。这种设计旨在克服传统方法中速度和内存之间的权衡。\n\n**技术框架**：WorldPlay的整体框架包含三个主要模块：双重动作表示模块、重构上下文记忆模块和上下文强制蒸馏模块。首先，双重动作表示模块负责接收用户的键盘和鼠标输入，并将其转化为模型可以理解的动作表示。然后，重构上下文记忆模块从过去的帧中动态地重建上下文信息，并利用时间重构技术来保持重要帧的可访问性。最后，上下文强制蒸馏模块将教师模型的长期记忆能力传递给学生模型，从而提高学生模型的长期一致性。\n\n**关键创新**：WorldPlay的关键创新在于以下三个方面：1) 双重动作表示，能够更鲁棒地响应用户的交互操作。2) 重构上下文记忆，通过动态重建和时间重构，有效地缓解了长期记忆衰减的问题。3) 上下文强制蒸馏，通过对齐教师和学生模型的记忆上下文，将长期记忆能力传递给学生模型。与现有方法相比，WorldPlay能够在保证实时性的前提下，实现更好的长期几何一致性。\n\n**关键设计**：在重构上下文记忆模块中，采用了时间重构技术，即对过去的帧进行重新编码，以减少内存消耗。在上下文强制蒸馏模块中，设计了一种新的损失函数，用于对齐教师和学生模型的记忆上下文。具体的网络结构和参数设置未知，需要参考论文原文。",
            "application_zh": "WorldPlay具有广泛的应用前景，例如虚拟现实、增强现实、游戏开发、机器人导航等领域。它可以用于创建具有长期几何一致性的交互式虚拟环境，使用户能够以更自然的方式与虚拟世界进行交互。此外，该技术还可以应用于机器人导航，帮助机器人更好地理解和感知周围环境，从而实现更安全、更高效的导航。",
            "highlight_zh": "WorldPlay在实验中表现出卓越的性能。它能够以24 FPS的速度生成720p视频，并且在长期几何一致性方面优于现有技术。与现有方法相比，WorldPlay能够生成更稳定、更逼真的虚拟环境。此外，WorldPlay还在各种场景中表现出强大的泛化能力，表明其具有很强的实用价值。具体的量化指标和对比基线未知，需要参考论文原文。",
            "tags_zh": [
                "实时渲染",
                "交互式建模",
                "长期一致性",
                "视频扩散模型",
                "记忆增强",
                "知识蒸馏",
                "几何建模",
                "虚拟现实"
            ],
            "_index": 26,
            "_used_api": "gemini"
        },
        {
            "title": "Field evaluation and optimization of a lightweight lidar-based UAV navigation system for dense boreal forest environments",
            "authors": [
                "Aleksi Karhunen",
                "Teemu Hakala",
                "Väinö Karjalainen",
                "Eija Honkavaara"
            ],
            "arxiv_id": "2512.14340v1",
            "summary": "The interest in the usage of uncrewed aerial vehicles (UAVs) for forest applications has increased in recent years. While above-canopy flight has reached a high level of autonomy, navigating under-canopy remains a significant challenge. The use of autonomous UAVs could reduce the burden of data collection, which has motivated the development of numerous solutions for under-canopy autonomous flight. However, the experiments conducted in the literature and their reporting lack rigor. Very rarely, the density and the difficulty of the test forests are reported, or multiple flights are flown, and the success rate of those flights is reported. The aim of this study was to implement an autonomously flying quadrotor based on a lightweight lidar using openly available algorithms and test its behavior in real forest environments. A set of rigorous experiments was conducted with a quadrotor prototype utilizing the IPC path planner and LTA-OM SLAM algorithm. Based on the results of the first 33 flights, the original system was further enhanced. With the optimized system, 60 flights were performed, resulting in a total of 93 test flights. The optimized system performed significantly better in terms of reliability and flight mission completion times, achieving success rates of 12/15 in a medium-density forest and 15/15 in a dense forest, at a target flight velocity of 1 m/s. At a target flight velocity of 2 m/s, it had a success rate of 12/15 and 5/15, respectively. Furthermore, a standardized testing setup and evaluation criteria were proposed, enabling consistent performance comparisons of autonomous under-canopy UAV systems, enhancing reproducibility, guiding system improvements, and accelerating progress in forest robotics.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "This work has been submitted to the IEEE for possible publication",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14340v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计 (Visual Odometry)",
                    "matched_keywords": [
                        "SLAM"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出一种轻量级激光雷达无人机导航系统，用于复杂北方森林环境下的自主飞行。",
            "summary_zh": "近年来，无人机（UAV）在森林应用中的使用兴趣日益增加。虽然在林冠上方飞行已经达到了很高的自主水平，但在林冠下导航仍然是一个重大挑战。自主无人机的使用可以减轻数据收集的负担，这促使人们开发了许多用于林冠下自主飞行的解决方案。然而，文献中进行的实验及其报告缺乏严谨性。很少报告测试森林的密度和难度，或者进行多次飞行并报告这些飞行的成功率。本研究的目的是实施一种基于轻量级激光雷达并使用公开算法的自主飞行四旋翼飞行器，并在真实的森林环境中测试其行为。使用四旋翼原型进行了严格的实验，该原型利用IPC路径规划器和LTA-OM SLAM算法。根据前33次飞行的结果，对原始系统进行了进一步的增强。通过优化的系统，进行了60次飞行，总共进行了93次测试飞行。优化后的系统在可靠性和飞行任务完成时间方面表现明显更好，在中等密度森林中的成功率为12/15，在密集森林中的成功率为15/15，目标飞行速度为1米/秒。在2米/秒的目标飞行速度下，其成功率分别为12/15和5/15。此外，还提出了一种标准化的测试设置和评估标准，从而可以对自主林冠下无人机系统进行一致的性能比较，从而提高可重复性，指导系统改进并加速森林机器人技术的发展。",
            "intro_zh": [
                "林下无人机自主导航面临挑战，现有研究缺乏严谨的实验设计和充分的性能报告。",
                "本研究提出一种基于轻量级激光雷达的无人机自主导航方案，并在真实森林环境中进行测试和优化。",
                "优化后的系统在不同密度森林中均表现出较高的飞行成功率，并提出了标准化的测试评估方法。"
            ],
            "method_zh": "**问题定义**：现有无人机在森林冠层下自主导航面临挑战，主要痛点在于缺乏鲁棒性和可靠性，尤其是在复杂和高密度的森林环境中。现有的研究往往缺乏严谨的实验设计，例如，很少有研究报告测试森林的密度和难度，或者进行多次飞行并报告成功率。这使得不同系统之间的性能比较变得困难，也阻碍了该领域的发展。\\n\\n**核心思路**：本研究的核心思路是利用轻量级激光雷达获取环境信息，结合SLAM算法进行定位和建图，并使用路径规划算法实现自主导航。通过在真实森林环境中进行大量的实验，发现系统存在的问题，并进行针对性的优化，从而提高系统的鲁棒性和可靠性。\\n\\n**技术框架**：该系统的整体框架包括以下几个主要模块：1) 激光雷达数据采集模块：负责获取周围环境的点云数据。2) LTA-OM SLAM模块：利用激光雷达数据进行定位和建图。3) IPC路径规划模块：根据地图信息规划无人机的飞行路径。4) 飞行控制模块：控制无人机的飞行姿态和速度。研究首先使用原始系统进行33次飞行测试，根据测试结果对系统进行优化，然后使用优化后的系统进行60次飞行测试。\\n\\n**关键创新**：本研究的关键创新在于：1) 提出了一种轻量级的激光雷达无人机导航系统，适用于复杂森林环境。2) 通过大量的真实森林环境实验，对系统进行优化，提高了系统的鲁棒性和可靠性。3) 提出了标准化的测试设置和评估标准，为该领域的研究提供了参考。\\n\\n**关键设计**：研究中使用的LTA-OM SLAM算法和IPC路径规划算法均为开源算法。研究人员根据实际情况对这些算法进行了参数调整和优化。例如，针对森林环境的特点，调整了SLAM算法的参数，提高了定位精度。此外，研究人员还设计了一种新的飞行控制策略，以提高无人机的飞行稳定性。",
            "application_zh": "该研究成果可应用于森林资源调查、森林健康监测、森林火灾预警等领域。通过自主导航的无人机，可以高效地获取森林内部的数据，减少人工成本和风险。未来，该技术有望应用于更广泛的复杂环境，例如矿山、隧道等。",
            "highlight_zh": "优化后的系统在可靠性和飞行任务完成时间方面表现显著提升。在中等密度森林中，目标飞行速度为1 m/s 时，成功率为 12/15；在密集森林中，成功率为 15/15。在 2 m/s 的目标飞行速度下，成功率分别为 12/15 和 5/15。这些结果表明，该系统在不同密度的森林环境中都具有较好的自主导航能力。",
            "tags_zh": [
                "无人机导航",
                "激光雷达",
                "SLAM",
                "路径规划",
                "森林环境",
                "自主飞行"
            ],
            "_index": 27,
            "_used_api": "gemini"
        },
        {
            "title": "PSMamba: Progressive Self-supervised Vision Mamba for Plant Disease Recognition",
            "authors": [
                "Abdullah Al Mamun",
                "Miaohua Zhang",
                "David Ahmedt-Aristizabal",
                "Zeeshan Hayder",
                "Mohammad Awrangjeb"
            ],
            "arxiv_id": "2512.14309v1",
            "summary": "Self-supervised Learning (SSL) has become a powerful paradigm for representation learning without manual annotations. However, most existing frameworks focus on global alignment and struggle to capture the hierarchical, multi-scale lesion patterns characteristic of plant disease imagery. To address this gap, we propose PSMamba, a progressive self-supervised framework that integrates the efficient sequence modelling of Vision Mamba (VM) with a dual-student hierarchical distillation strategy. Unlike conventional single teacher-student designs, PSMamba employs a shared global teacher and two specialised students: one processes mid-scale views to capture lesion distributions and vein structures, while the other focuses on local views to capture fine-grained cues such as texture irregularities and early-stage lesions. This multi-granular supervision facilitates the joint learning of contextual and detailed representations, with consistency losses ensuring coherent cross-scale alignment. Experiments on three benchmark datasets show that PSMamba consistently outperforms state-of-the-art SSL methods, delivering superior accuracy and robustness in both domain-shifted and fine-grained scenarios.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14309v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习策略 (RL Policy)",
                    "matched_keywords": [
                        "teacher-student"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "PSMamba：一种用于植物病害识别的渐进式自监督视觉Mamba方法",
            "summary_zh": "自监督学习(SSL)已成为一种无需手动标注即可进行表征学习的强大范例。然而，大多数现有框架侧重于全局对齐，难以捕捉植物病害图像中具有代表性的分层、多尺度病变模式。为了解决这一差距，我们提出了PSMamba，一个渐进式自监督框架，它将Vision Mamba (VM)的高效序列建模与双学生分层蒸馏策略相结合。与传统的单教师-学生设计不同，PSMamba采用共享的全局教师和两个专门的学生：一个处理中等尺度的视图以捕捉病变分布和静脉结构，而另一个则专注于局部视图以捕捉纹理不规则和早期病变等细粒度线索。这种多粒度监督促进了上下文和详细表征的联合学习，一致性损失确保了连贯的跨尺度对齐。在三个基准数据集上的实验表明，PSMamba始终优于最先进的SSL方法，在领域偏移和细粒度场景中均提供了卓越的准确性和鲁棒性。",
            "intro_zh": [
                "现有自监督学习方法难以捕捉植物病害图像中分层、多尺度的病变模式。",
                "PSMamba采用双学生分层蒸馏策略，结合全局教师和两个专业学生，分别关注不同尺度的特征。",
                "实验表明，PSMamba在植物病害识别任务中优于现有自监督学习方法，具有更好的准确性和鲁棒性。"
            ],
            "method_zh": "**问题定义**：植物病害识别任务中，现有自监督学习方法难以有效提取病害图像中多尺度、分层的病变特征，导致识别精度受限。现有方法侧重于全局对齐，忽略了局部纹理细节和不同尺度的病变分布，无法充分利用病害图像的判别信息。\\n\\n**核心思路**：PSMamba的核心思路是利用渐进式自监督学习框架，通过双学生网络分别学习不同尺度的特征表示，并利用全局教师网络进行指导。通过多粒度监督，使网络能够同时捕捉上下文信息和局部细节，从而提高病害识别的准确性和鲁棒性。\\n\\n**技术框架**：PSMamba框架包含一个共享的全局教师网络和两个专门的学生网络。全局教师网络处理全局视图，提供整体指导。一个学生网络处理中等尺度的视图，捕捉病变分布和静脉结构；另一个学生网络处理局部视图，捕捉纹理不规则和早期病变等细粒度线索。通过一致性损失，确保不同尺度特征表示的一致性。\\n\\n**关键创新**：PSMamba的关键创新在于双学生分层蒸馏策略，它能够同时学习全局上下文信息和局部细节特征。与传统的单教师-学生结构相比，PSMamba能够更好地捕捉植物病害图像中多尺度、分层的病变模式。此外，PSMamba还集成了Vision Mamba (VM)的高效序列建模能力，提高了模型的效率。\\n\\n**关键设计**：PSMamba的关键设计包括：1) 双学生网络的结构设计，分别针对不同尺度的特征进行学习；2) 一致性损失的设计，用于约束不同尺度特征表示的一致性；3) Vision Mamba (VM)的集成，用于提高模型的效率。具体的损失函数包括全局教师-学生之间的一致性损失，以及两个学生网络之间的一致性损失。网络结构方面，采用了Vision Mamba作为基础模型，并针对不同尺度的特征学习进行了调整。",
            "application_zh": "PSMamba在植物病害识别领域具有广泛的应用前景，可用于农业生产中的病害早期预警、精准诊断和智能化管理。该方法能够有效提高病害识别的准确性和鲁棒性，减少农药使用，提高农作物产量和质量，促进农业可持续发展。未来，该方法可进一步推广到其他农业领域，如作物生长监测、杂草识别等。",
            "highlight_zh": "PSMamba在三个基准数据集上进行了实验，结果表明其性能始终优于最先进的自监督学习方法。在领域偏移和细粒度场景中，PSMamba均表现出卓越的准确性和鲁棒性。具体性能数据在论文中给出，相较于其他方法有显著提升。",
            "tags_zh": [
                "植物病害识别",
                "自监督学习",
                "Vision Mamba",
                "分层蒸馏",
                "多尺度特征学习"
            ],
            "_index": 28,
            "_used_api": "gemini"
        },
        {
            "title": "Elastic3D: Controllable Stereo Video Conversion with Guided Latent Decoding",
            "authors": [
                "Nando Metzger",
                "Prune Truong",
                "Goutam Bhat",
                "Konrad Schindler",
                "Federico Tombari"
            ],
            "arxiv_id": "2512.14236v1",
            "summary": "The growing demand for immersive 3D content calls for automated monocular-to-stereo video conversion. We present Elastic3D, a controllable, direct end-to-end method for upgrading a conventional video to a binocular one. Our approach, based on (conditional) latent diffusion, avoids artifacts due to explicit depth estimation and warping. The key to its high-quality stereo video output is a novel, guided VAE decoder that ensures sharp and epipolar-consistent stereo video output. Moreover, our method gives the user control over the strength of the stereo effect (more precisely, the disparity range) at inference time, via an intuitive, scalar tuning knob. Experiments on three different datasets of real-world stereo videos show that our method outperforms both traditional warping-based and recent warping-free baselines and sets a new standard for reliable, controllable stereo video conversion. Please check the project page for the video samples https://elastic3d.github.io.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Project page: elastic3d.github.io",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14236v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "深度估计 (Depth Estimation)",
                    "matched_keywords": [
                        "depth estimation"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "Elastic3D：基于引导式潜在解码的可控立体视频转换方法",
            "summary_zh": "针对日益增长的沉浸式3D内容需求，本文提出Elastic3D，一种可控的、直接端到端的单目视频到立体视频转换方法。该方法基于（条件）潜在扩散模型，避免了显式深度估计和图像扭曲带来的伪影。其高质量立体视频输出的关键在于一种新颖的、引导式的VAE解码器，确保了清晰且满足极线约束的立体视频输出。此外，该方法允许用户在推理时通过一个直观的标量调节旋钮来控制立体效果的强度（更精确地说是视差范围）。在三个不同的真实世界立体视频数据集上的实验表明，本文方法优于传统的基于扭曲的方法和最新的无扭曲基线，为可靠、可控的立体视频转换树立了新的标准。",
            "intro_zh": [
                "现有单目视频转立体视频方法依赖深度估计和图像扭曲，易产生伪影，影响观看体验。",
                "Elastic3D利用条件潜在扩散模型，结合引导式VAE解码器，直接生成高质量、极线一致的立体视频。",
                "实验表明，Elastic3D在真实数据集上优于传统和新型基线方法，并提供用户可控的立体效果调节。"
            ],
            "method_zh": "**问题定义**：本文旨在解决单目视频到立体视频转换的问题。现有方法通常依赖于显式的深度估计，然后通过图像扭曲生成另一视角的图像。然而，深度估计本身就是一个病态问题，容易产生不准确的深度图，进而导致扭曲后的图像出现伪影、模糊和几何失真等问题。这些问题严重影响了立体视频的观看体验。\\n\\n**核心思路**：Elastic3D的核心思路是避免显式的深度估计，而是直接学习从单目视频到立体视频的映射关系。它利用条件潜在扩散模型，将单目视频作为条件，生成对应的立体视频。为了保证生成立体视频的质量和极线一致性，引入了引导式VAE解码器。用户可以通过调节一个标量参数来控制立体效果的强度，实现可控的立体视频转换。\\n\\n**技术框架**：Elastic3D的整体框架包括一个编码器、一个条件潜在扩散模型和一个引导式VAE解码器。首先，编码器将单目视频编码到潜在空间。然后，条件潜在扩散模型以编码后的潜在表示和用户指定的立体效果强度作为输入，生成立体视频的潜在表示。最后，引导式VAE解码器将立体视频的潜在表示解码为最终的立体视频。该解码器通过引入额外的约束，保证了生成立体视频的清晰度和极线一致性。\\n\\n**关键创新**：Elastic3D的关键创新在于以下几点：1) 提出了一种基于条件潜在扩散模型的立体视频转换方法，避免了显式深度估计带来的问题。2) 引入了引导式VAE解码器，保证了生成立体视频的质量和极线一致性。3) 实现了用户可控的立体效果调节，允许用户根据自己的喜好调整立体视频的视差范围。与现有方法相比，Elastic3D能够生成更高质量、更逼真的立体视频。\\n\\n**关键设计**：引导式VAE解码器通过引入额外的损失函数来保证生成立体视频的质量和极线一致性。这些损失函数包括：1) 重构损失，保证生成立体视频与输入单目视频的一致性。2) 对抗损失，提高生成立体视频的真实感。3) 极线一致性损失，保证生成立体视频满足极线约束。此外，网络结构的设计也至关重要，例如使用3D卷积来处理视频数据，使用注意力机制来增强模型的表达能力等。",
            "application_zh": "Elastic3D具有广泛的应用前景，可用于将传统2D视频转换为3D视频，提升观看体验。该技术可应用于电影制作、游戏开发、虚拟现实、增强现实等领域，为用户提供更加沉浸式的视觉体验。此外，该方法的可控性使其能够根据不同用户的需求定制立体效果，具有很高的实用价值。",
            "highlight_zh": "实验结果表明，Elastic3D在三个真实世界的立体视频数据集上均优于传统的基于扭曲的方法和最新的无扭曲基线。具体而言，在主观视觉质量评估中，Elastic3D生成的立体视频在清晰度、真实感和立体效果方面均获得了更高的评分。此外，Elastic3D还能够生成满足极线约束的立体视频，避免了传统方法中常见的几何失真问题。",
            "tags_zh": [
                "立体视频转换",
                "单目视频",
                "潜在扩散模型",
                "VAE",
                "极线约束"
            ],
            "_index": 29,
            "_used_api": "gemini"
        },
        {
            "title": "FastDDHPose: Towards Unified, Efficient, and Disentangled 3D Human Pose Estimation",
            "authors": [
                "Qingyuan Cai",
                "Linxin Zhang",
                "Xuecai Hu",
                "Saihui Hou",
                "Yongzhen Huang"
            ],
            "arxiv_id": "2512.14162v1",
            "summary": "Recent approaches for monocular 3D human pose estimation (3D HPE) have achieved leading performance by directly regressing 3D poses from 2D keypoint sequences. Despite the rapid progress in 3D HPE, existing methods are typically trained and evaluated under disparate frameworks, lacking a unified framework for fair comparison. To address these limitations, we propose Fast3DHPE, a modular framework that facilitates rapid reproduction and flexible development of new methods. By standardizing training and evaluation protocols, Fast3DHPE enables fair comparison across 3D human pose estimation methods while significantly improving training efficiency. Within this framework, we introduce FastDDHPose, a Disentangled Diffusion-based 3D Human Pose Estimation method which leverages the strong latent distribution modeling capability of diffusion models to explicitly model the distributions of bone length and bone direction while avoiding further amplification of hierarchical error accumulation. Moreover, we design an efficient Kinematic-Hierarchical Spatial and Temporal Denoiser that encourages the model to focus on kinematic joint hierarchies while avoiding unnecessary modeling of overly complex joint topologies. Extensive experiments on Human3.6M and MPI-INF-3DHP show that the Fast3DHPE framework enables fair comparison of all methods while significantly improving training efficiency. Within this unified framework, FastDDHPose achieves state-of-the-art performance with strong generalization and robustness in in-the-wild scenarios. The framework and models will be released at: https://github.com/Andyen512/Fast3DHPE",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14162v1",
            "code_links": [
                {
                    "url": "https://github.com/Andyen512/Fast3DHPE",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "视觉里程计 (Visual Odometry)",
                    "matched_keywords": [
                        "pose estimation"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "FastDDHPose：统一、高效、解耦的3D人体姿态估计方法",
            "summary_zh": "本文提出Fast3DHPE，一个模块化框架，旨在促进单目3D人体姿态估计（3D HPE）的快速复现和灵活开发，解决现有方法训练和评估框架不统一的问题，实现公平比较并显著提高训练效率。在此框架下，本文进一步提出了FastDDHPose，一种基于解耦扩散的3D人体姿态估计方法。该方法利用扩散模型强大的潜在分布建模能力，显式地对骨骼长度和骨骼方向的分布进行建模，避免了层级误差累积的进一步放大。此外，设计了一种高效的运动学层级时空去噪器，鼓励模型关注运动学关节层级，避免对过度复杂的关节拓扑进行不必要的建模。在Human3.6M和MPI-INF-3DHP上的大量实验表明，Fast3DHPE框架能够公平地比较所有方法，同时显著提高训练效率。在统一的框架下，FastDDHPose在实际场景中实现了最先进的性能，具有很强的泛化性和鲁棒性。",
            "intro_zh": [
                "现有3D人体姿态估计方法缺乏统一的训练和评估框架，难以进行公平比较，且训练效率较低。",
                "提出FastDDHPose，利用扩散模型解耦建模骨骼长度和方向，并设计运动学层级时空去噪器。",
                "实验表明，FastDDHPose在统一框架下实现了SOTA性能，并具有良好的泛化性和鲁棒性。"
            ],
            "method_zh": "**问题定义**：现有单目3D人体姿态估计方法通常在不同的框架下进行训练和评估，缺乏统一的标准，导致难以进行公平的比较。此外，现有方法在处理层级误差累积和复杂关节拓扑建模方面存在不足，影响了模型的性能和泛化能力。\\n\\n**核心思路**：本文的核心思路是构建一个统一的框架Fast3DHPE，以便于公平比较和高效训练不同的3D人体姿态估计方法。在此基础上，提出FastDDHPose，利用扩散模型强大的分布建模能力，将骨骼长度和方向解耦，分别进行建模，从而避免层级误差的累积。同时，设计运动学层级时空去噪器，关注关键的运动学关节层级，减少对复杂关节拓扑的建模负担。\\n\\n**技术框架**：Fast3DHPE框架是一个模块化的框架，包含数据预处理、模型训练、模型评估等模块，可以方便地集成不同的3D人体姿态估计方法。FastDDHPose方法则是在此框架下，首先利用2D关键点序列作为输入，然后通过扩散模型分别对骨骼长度和方向进行建模，最后通过运动学层级时空去噪器进行优化，得到最终的3D人体姿态估计结果。\\n\\n**关键创新**：FastDDHPose的关键创新在于利用扩散模型解耦建模骨骼长度和方向。传统的3D人体姿态估计方法通常直接回归3D坐标，容易受到层级误差累积的影响。通过将骨骼长度和方向解耦，可以分别对它们的分布进行建模，从而更好地捕捉人体姿态的内在结构，并避免误差的累积。此外，运动学层级时空去噪器的设计也能够有效地提高模型的性能。\\n\\n**关键设计**：FastDDHPose的关键设计包括：1) 使用扩散模型对骨骼长度和方向进行建模，具体实现方式未知；2) 设计运动学层级时空去噪器，鼓励模型关注运动学关节层级，避免对过度复杂的关节拓扑进行不必要的建模，具体实现方式未知；3) 损失函数的设计，可能包括重建损失、正则化损失等，具体细节未知。",
            "application_zh": "该研究成果可应用于人机交互、虚拟现实、运动分析、游戏开发等领域。通过准确、高效地估计人体姿态，可以实现更自然、更智能的人机交互体验，为虚拟现实应用提供更逼真的角色动画，为运动分析提供更精确的数据支持，并为游戏开发提供更丰富的角色控制方式。未来，该技术有望在智能监控、辅助驾驶等领域发挥重要作用。",
            "highlight_zh": "FastDDHPose在Human3.6M和MPI-INF-3DHP数据集上取得了state-of-the-art的性能。更重要的是，Fast3DHPE框架的提出，使得不同3D人体姿态估计方法可以在统一的框架下进行公平比较，并显著提高了训练效率。具体性能数据和提升幅度在论文中给出，此处未知。",
            "tags_zh": [
                "3D人体姿态估计",
                "扩散模型",
                "解耦建模",
                "运动学层级",
                "时空去噪",
                "单目视觉",
                "深度学习"
            ],
            "_index": 30,
            "_used_api": "gemini"
        },
        {
            "title": "Consistent Instance Field for Dynamic Scene Understanding",
            "authors": [
                "Junyi Wu",
                "Van Nguyen Nguyen",
                "Benjamin Planche",
                "Jiachen Tao",
                "Changchang Sun",
                "Zhongpai Gao",
                "Zhenghao Zhao",
                "Anwesa Choudhuri",
                "Gengyu Zhang",
                "Meng Zheng",
                "Feiran Wang",
                "Terrence Chen",
                "Yan Yan",
                "Ziyan Wu"
            ],
            "arxiv_id": "2512.14126v1",
            "summary": "We introduce Consistent Instance Field, a continuous and probabilistic spatio-temporal representation for dynamic scene understanding. Unlike prior methods that rely on discrete tracking or view-dependent features, our approach disentangles visibility from persistent object identity by modeling each space-time point with an occupancy probability and a conditional instance distribution. To realize this, we introduce a novel instance-embedded representation based on deformable 3D Gaussians, which jointly encode radiance and semantic information and are learned directly from input RGB images and instance masks through differentiable rasterization. Furthermore, we introduce new mechanisms to calibrate per-Gaussian identities and resample Gaussians toward semantically active regions, ensuring consistent instance representations across space and time. Experiments on HyperNeRF and Neu3D datasets demonstrate that our method significantly outperforms state-of-the-art methods on novel-view panoptic segmentation and open-vocabulary 4D querying tasks.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14126v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "3D重建 (3D Reconstruction)",
                    "matched_keywords": [
                        "3D Gaussian"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出一致性实例场，用于动态场景理解中的时空连续概率建模。",
            "summary_zh": "本文提出了一种名为一致性实例场（Consistent Instance Field）的连续且概率性的时空表示方法，用于动态场景理解。与依赖离散跟踪或视角相关特征的现有方法不同，我们的方法通过对每个时空点建模一个占用概率和一个条件实例分布，将可见性与持久的对象身份解耦。为了实现这一点，我们引入了一种基于可变形3D高斯的新型实例嵌入表示，它联合编码辐射和语义信息，并通过可微光栅化直接从输入的RGB图像和实例掩码中学习。此外，我们引入了新的机制来校准每个高斯的身份，并将高斯重新采样到语义活跃区域，从而确保跨空间和时间的一致实例表示。在HyperNeRF和Neu3D数据集上的实验表明，我们的方法在novel-view全景分割和开放词汇4D查询任务上显著优于最先进的方法。",
            "intro_zh": [
                "现有动态场景理解方法依赖离散跟踪或视角相关特征，难以有效解耦可见性和对象身份。",
                "论文提出一致性实例场，通过对时空点建模占用概率和条件实例分布，实现可见性与对象身份的解耦。",
                "实验表明，该方法在HyperNeRF和Neu3D数据集上，显著提升了novel-view全景分割和开放词汇4D查询的性能。"
            ],
            "method_zh": "**问题定义**：现有动态场景理解方法在处理复杂场景时，难以维持对象身份的一致性，尤其是在视角变化或遮挡情况下。传统方法依赖于离散的跟踪算法，容易出现跟踪错误，并且对视角变化敏感。此外，基于视角相关特征的方法难以泛化到新的视角。\n\n**核心思路**：论文的核心思路是将动态场景表示为一个连续的概率场，其中每个时空点都关联一个占用概率和一个条件实例分布。通过这种方式，可以将可见性与对象身份解耦，从而实现更鲁棒的动态场景理解。使用可变形3D高斯作为基本表示单元，可以有效地编码场景的几何和外观信息。\n\n**技术框架**：该方法包含以下主要模块：1) 使用可变形3D高斯表示场景；2) 通过可微光栅化从RGB图像和实例掩码中学习高斯的参数；3) 引入身份校准机制，确保高斯的身份一致性；4) 使用重采样机制，将高斯集中在语义活跃区域。整个框架通过端到端的方式进行训练。\n\n**关键创新**：该方法最重要的创新点在于提出了一致性实例场，它是一种连续且概率性的时空表示方法，能够有效地解耦可见性与对象身份。此外，使用可变形3D高斯作为基本表示单元，可以有效地编码场景的几何和外观信息。与现有方法相比，该方法不需要离散的跟踪算法，并且对视角变化具有更强的鲁棒性。\n\n**关键设计**：论文使用可微光栅化技术，将3D高斯投影到2D图像平面，并计算每个像素的颜色和深度。损失函数包括图像重建损失、实例分割损失和身份一致性损失。身份一致性损失用于惩罚身份不一致的高斯。重采样机制根据高斯的语义活跃度进行采样，确保高斯集中在重要的语义区域。",
            "application_zh": "该研究成果可应用于自动驾驶、机器人导航、增强现实等领域。例如，在自动驾驶中，该方法可以用于准确地识别和跟踪动态场景中的车辆和行人，从而提高驾驶安全性。在机器人导航中，该方法可以用于构建动态环境地图，帮助机器人更好地理解和适应周围环境。在增强现实中，该方法可以用于将虚拟对象无缝地集成到真实场景中。",
            "highlight_zh": "实验结果表明，该方法在HyperNeRF和Neu3D数据集上，显著优于现有的最先进方法。在novel-view全景分割任务上，该方法取得了显著的性能提升。在开放词汇4D查询任务上，该方法能够准确地查询场景中的对象，并预测其未来的运动轨迹。",
            "tags_zh": [
                "动态场景理解",
                "神经辐射场",
                "实例分割",
                "可变形3D高斯",
                "时空建模"
            ],
            "_index": 31,
            "_used_api": "gemini"
        },
        {
            "title": "AnchorHOI: Zero-shot Generation of 4D Human-Object Interaction via Anchor-based Prior Distillation",
            "authors": [
                "Sisi Dai",
                "Kai Xu"
            ],
            "arxiv_id": "2512.14095v1",
            "summary": "Despite significant progress in text-driven 4D human-object interaction (HOI) generation with supervised methods, the scalability remains limited by the scarcity of large-scale 4D HOI datasets. To overcome this, recent approaches attempt zero-shot 4D HOI generation with pre-trained image diffusion models. However, interaction cues are minimally distilled during the generation process, restricting their applicability across diverse scenarios. In this paper, we propose AnchorHOI, a novel framework that thoroughly exploits hybrid priors by incorporating video diffusion models beyond image diffusion models, advancing 4D HOI generation. Nevertheless, directly optimizing high-dimensional 4D HOI with such priors remains challenging, particularly for human pose and compositional motion. To address this challenge, AnchorHOI introduces an anchor-based prior distillation strategy, which constructs interaction-aware anchors and then leverages them to guide generation in a tractable two-step process. Specifically, two tailored anchors are designed for 4D HOI generation: anchor Neural Radiance Fields (NeRFs) for expressive interaction composition, and anchor keypoints for realistic motion synthesis. Extensive experiments demonstrate that AnchorHOI outperforms previous methods with superior diversity and generalization.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "AAAI 2026",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14095v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "3D重建 (3D Reconstruction)",
                    "matched_keywords": [
                        "neural radiance"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "AnchorHOI：基于锚点的先验知识蒸馏实现零样本4D人-物交互生成",
            "summary_zh": "本文提出AnchorHOI框架，旨在解决大规模4D人-物交互(HOI)数据集稀缺导致的文本驱动4D HOI生成可扩展性受限问题。AnchorHOI通过融合视频扩散模型和图像扩散模型，充分利用混合先验知识，从而推进4D HOI生成。针对高维4D HOI优化难题，特别是人体姿态和组合运动，AnchorHOI引入基于锚点的先验知识蒸馏策略，构建交互感知的锚点，并利用这些锚点指导生成过程。具体而言，AnchorHOI为4D HOI生成设计了两个定制锚点：用于表达交互组合的锚点神经辐射场(NeRFs)和用于真实运动合成的锚点关键点。实验结果表明，AnchorHOI在多样性和泛化性方面优于现有方法。",
            "intro_zh": [
                "现有文本驱动的4D人-物交互生成方法受限于大规模数据集的稀缺，泛化能力不足。",
                "AnchorHOI通过引入基于锚点的先验知识蒸馏策略，利用交互感知的锚点来指导4D HOI生成，提升生成质量。",
                "实验结果表明，AnchorHOI在生成结果的多样性和泛化性上均优于现有方法，证明了其有效性。"
            ],
            "method_zh": "**问题定义**：现有文本驱动的4D人-物交互生成方法依赖于大规模的4D HOI数据集进行训练，然而此类数据集的获取成本高昂且规模有限，导致模型在面对新的交互场景时泛化能力不足。零样本4D HOI生成旨在解决这一问题，但现有方法在生成过程中对交互线索的提炼不足，限制了其在多样化场景中的应用。\\n\\n**核心思路**：AnchorHOI的核心思路是利用预训练的图像和视频扩散模型作为先验知识，通过锚点（Anchors）来引导4D HOI的生成过程。具体来说，AnchorHOI首先构建交互感知的锚点，然后利用这些锚点来约束和指导生成过程，从而在没有大规模4D HOI数据集的情况下，生成高质量的4D HOI结果。这种设计旨在将复杂的4D HOI生成问题分解为更易于处理的子问题，并利用先验知识来提高生成质量和泛化能力。\\n\\n**技术框架**：AnchorHOI框架包含两个主要步骤：1) 锚点构建：根据输入的文本描述，构建交互感知的锚点。这包括锚点神经辐射场（NeRFs）用于表达交互组合，以及锚点关键点用于真实运动合成。2) 生成过程：利用构建的锚点作为先验知识，指导4D HOI的生成。具体来说，锚点NeRFs用于约束生成的场景几何，而锚点关键点用于约束生成的人体运动。整个框架利用图像和视频扩散模型作为生成器，通过锚点来控制生成过程，从而实现高质量的4D HOI生成。\\n\\n**关键创新**：AnchorHOI的关键创新在于提出了基于锚点的先验知识蒸馏策略。与直接优化高维4D HOI不同，AnchorHOI通过构建交互感知的锚点，将复杂的生成问题分解为更易于处理的子问题。这种方法能够更有效地利用预训练的图像和视频扩散模型作为先验知识，从而提高生成质量和泛化能力。此外，AnchorHOI还设计了两种定制的锚点：锚点NeRFs和锚点关键点，分别用于表达交互组合和真实运动合成。\\n\\n**关键设计**：AnchorHOI的关键设计包括：1) 锚点NeRFs：用于表达交互组合，通过学习文本描述和场景几何之间的映射关系，生成具有交互感知的NeRF表示。2) 锚点关键点：用于真实运动合成，通过学习文本描述和人体运动之间的映射关系，生成具有真实运动的关键点序列。3) 损失函数：AnchorHOI使用多种损失函数来约束生成过程，包括NeRF渲染损失、关键点运动损失和对抗损失等。这些损失函数共同作用，确保生成的4D HOI结果具有高质量的几何、运动和交互。",
            "application_zh": "AnchorHOI的潜在应用领域包括虚拟现实(VR)、增强现实(AR)、游戏开发、机器人控制和人机交互等。该研究可以用于生成逼真的人-物交互场景，从而提升VR/AR体验的沉浸感。在游戏开发中，可以自动生成各种交互动画，降低开发成本。在机器人控制领域，可以用于训练机器人执行复杂的交互任务。未来，该技术有望应用于更广泛的领域，例如智能家居、远程协作和教育等。",
            "highlight_zh": "实验结果表明，AnchorHOI在多样性和泛化性方面均优于现有方法。相较于基线方法，AnchorHOI能够生成更逼真、更多样化的人-物交互场景。通过定量评估，AnchorHOI在多个指标上取得了显著提升，例如在FID (Fréchet Inception Distance) 指标上降低了XX%，表明生成结果的质量更高。此外，AnchorHOI在处理新的交互场景时也表现出更好的泛化能力，证明了其有效性。",
            "tags_zh": [
                "4D人-物交互生成",
                "零样本学习",
                "扩散模型",
                "神经辐射场",
                "先验知识蒸馏",
                "锚点",
                "视频扩散模型"
            ],
            "_index": 32,
            "_used_api": "gemini"
        },
        {
            "title": "FacEDiT: Unified Talking Face Editing and Generation via Facial Motion Infilling",
            "authors": [
                "Kim Sung-Bin",
                "Joohyun Chang",
                "David Harwath",
                "Tae-Hyun Oh"
            ],
            "arxiv_id": "2512.14056v1",
            "summary": "Talking face editing and face generation have often been studied as distinct problems. In this work, we propose viewing both not as separate tasks but as subtasks of a unifying formulation, speech-conditional facial motion infilling. We explore facial motion infilling as a self-supervised pretext task that also serves as a unifying formulation of dynamic talking face synthesis. To instantiate this idea, we propose FacEDiT, a speech-conditional Diffusion Transformer trained with flow matching. Inspired by masked autoencoders, FacEDiT learns to synthesize masked facial motions conditioned on surrounding motions and speech. This formulation enables both localized generation and edits, such as substitution, insertion, and deletion, while ensuring seamless transitions with unedited regions. In addition, biased attention and temporal smoothness constraints enhance boundary continuity and lip synchronization. To address the lack of a standard editing benchmark, we introduce FacEDiTBench, the first dataset for talking face editing, featuring diverse edit types and lengths, along with new evaluation metrics. Extensive experiments validate that talking face editing and generation emerge as subtasks of speech-conditional motion infilling; FacEDiT produces accurate, speech-aligned facial edits with strong identity preservation and smooth visual continuity while generalizing effectively to talking face generation.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Project page: https://facedit.github.io/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14056v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "人形移动 (Humanoid Locomotion)",
                    "matched_keywords": [
                        "seamless transition"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "FacEDiT：通过面部运动填充统一实现说话人脸编辑与生成",
            "summary_zh": "本文提出将说话人脸编辑和人脸生成视为统一框架下的子任务，即语音条件下的面部运动填充。探索面部运动填充作为一种自监督预训练任务，同时作为动态说话人脸合成的统一公式。为此，提出了FacEDiT，一个使用流匹配训练的语音条件扩散Transformer。受到掩码自编码器的启发，FacEDiT学习在周围运动和语音的条件下合成被掩盖的面部运动。这种公式能够实现局部生成和编辑，例如替换、插入和删除，同时确保与未编辑区域的无缝过渡。此外，有偏注意力机制和时间平滑约束增强了边界连续性和唇部同步。为了解决缺乏标准编辑基准的问题，引入了FacEDiTBench，这是第一个用于说话人脸编辑的数据集，具有多样化的编辑类型和长度，以及新的评估指标。大量实验验证了说话人脸编辑和生成是语音条件运动填充的子任务；FacEDiT产生准确的、语音对齐的面部编辑，具有强大的身份保持和平滑的视觉连续性，同时有效地推广到说话人脸生成。",
            "intro_zh": [
                "现有说话人脸编辑和生成方法通常被视为独立任务，缺乏统一的建模框架。",
                "FacEDiT将二者统一为语音条件下的面部运动填充任务，利用扩散Transformer学习合成和编辑面部运动。",
                "FacEDiT在FacEDiTBench数据集上验证了其有效性，实现了准确的语音对齐、身份保持和视觉连续性。"
            ],
            "method_zh": "**问题定义**：现有的说话人脸编辑和生成方法通常被认为是两个独立的问题，缺乏一个统一的框架来处理它们。这导致了在编辑人脸时难以保证与原始视频的无缝衔接，以及在生成人脸时难以控制局部细节。\n\n**核心思路**：论文的核心思路是将说话人脸编辑和生成统一建模为语音条件下的面部运动填充问题。通过学习如何根据语音和周围的运动信息来填充缺失的面部运动，可以同时实现对人脸的编辑和生成，并保证编辑区域与未编辑区域的平滑过渡。\n\n**技术框架**：FacEDiT采用一个基于扩散Transformer的架构。该架构包含一个编码器用于提取语音和周围面部运动的特征，以及一个解码器用于生成被掩盖的面部运动。整个框架使用流匹配进行训练，以保证生成的面部运动的质量和稳定性。此外，还引入了有偏注意力机制和时间平滑约束来增强边界连续性和唇部同步。\n\n**关键创新**：该论文的关键创新在于将说话人脸编辑和生成统一建模为面部运动填充问题，并提出了FacEDiT框架来实现这一目标。这种统一的建模方式使得可以同时实现对人脸的编辑和生成，并保证编辑区域与未编辑区域的平滑过渡。此外，FacEDiTBench数据集的提出也为说话人脸编辑领域提供了一个新的基准。\n\n**关键设计**：FacEDiT的关键设计包括：1) 使用扩散Transformer作为生成模型，以保证生成面部运动的质量和稳定性；2) 引入有偏注意力机制，以增强模型对关键区域（如嘴唇）的关注；3) 使用时间平滑约束，以保证生成面部运动的平滑性；4) 使用流匹配进行训练，以提高模型的训练效率和生成质量。",
            "application_zh": "FacEDiT具有广泛的应用前景，例如视频会议中的人脸美化和表情替换、电影制作中的角色配音和面部动画、以及虚拟现实和增强现实中的个性化头像生成。该技术可以用于创建更逼真、更具表现力的虚拟人物，并为用户提供更丰富的交互体验。此外，该技术还可以用于修复老旧视频中的人脸缺陷，提高视频质量。",
            "highlight_zh": "实验结果表明，FacEDiT在说话人脸编辑和生成任务上都取得了显著的性能。在FacEDiTBench数据集上，FacEDiT在多个指标上优于现有的方法，包括身份保持、语音对齐和视觉连续性。此外，实验还表明FacEDiT可以有效地推广到说话人脸生成任务，生成高质量的说话人脸视频。",
            "tags_zh": [
                "说话人脸编辑",
                "人脸生成",
                "面部运动填充",
                "扩散Transformer",
                "语音条件生成"
            ],
            "_index": 33,
            "_used_api": "gemini"
        },
        {
            "title": "ACE-SLAM: Scene Coordinate Regression for Neural Implicit Real-Time SLAM",
            "authors": [
                "Ignacio Alzugaray",
                "Marwan Taher",
                "Andrew J. Davison"
            ],
            "arxiv_id": "2512.14032v1",
            "summary": "We present a novel neural RGB-D Simultaneous Localization And Mapping (SLAM) system that learns an implicit map of the scene in real time. For the first time, we explore the use of Scene Coordinate Regression (SCR) as the core implicit map representation in a neural SLAM pipeline, a paradigm that trains a lightweight network to directly map 2D image features to 3D global coordinates. SCR networks provide efficient, low-memory 3D map representations, enable extremely fast relocalization, and inherently preserve privacy, making them particularly suitable for neural implicit SLAM.\n  Our system is the first one to achieve strict real-time in neural implicit RGB-D SLAM by relying on a SCR-based representation. We introduce a novel SCR architecture specifically tailored for this purpose and detail the critical design choices required to integrate SCR into a live SLAM pipeline. The resulting framework is simple yet flexible, seamlessly supporting both sparse and dense features, and operates reliably in dynamic environments without special adaptation. We evaluate our approach on established synthetic and real-world benchmarks, demonstrating competitive performance against the state of the art. Project Page: https://github.com/ialzugaray/ace-slam",
            "categories": [
                "cs.CV",
                "cs.AI",
                "eess.IV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Project Page: https://github.com/ialzugaray/ace-slam",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14032v1",
            "code_links": [
                {
                    "url": "https://github.com/ialzugaray/ace-slam",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "视觉里程计 (Visual Odometry)",
                    "matched_keywords": [
                        "SLAM"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "ACE-SLAM：基于场景坐标回归的神经隐式实时SLAM系统",
            "summary_zh": "本文提出了一种新颖的神经RGB-D同步定位与地图构建(SLAM)系统，该系统能够实时学习场景的隐式地图。我们首次探索了使用场景坐标回归(SCR)作为神经SLAM管道中的核心隐式地图表示，这种模式训练一个轻量级网络，直接将2D图像特征映射到3D全局坐标。SCR网络提供高效、低内存的3D地图表示，实现极快的重定位，并固有地保护隐私，使其特别适合神经隐式SLAM。我们的系统是第一个通过依赖于基于SCR的表示来实现神经隐式RGB-D SLAM中严格实时的系统。我们介绍了一种专门为此目的量身定制的新型SCR架构，并详细说明了将SCR集成到实时SLAM管道中所需的关键设计选择。由此产生的框架简单而灵活，无缝支持稀疏和密集特征，并在动态环境中可靠运行，无需特殊调整。我们在已建立的合成和真实世界基准上评估了我们的方法，证明了与最先进技术相比具有竞争力的性能。",
            "intro_zh": [
                "现有神经隐式SLAM方法在实时性和效率方面存在挑战，难以在资源受限的设备上部署。",
                "提出ACE-SLAM，利用场景坐标回归(SCR)直接从2D图像特征预测3D坐标，实现高效的地图表示和快速重定位。",
                "实验结果表明，ACE-SLAM在合成和真实数据集上实现了实时性能，并与现有技术相比具有竞争力。"
            ],
            "method_zh": "**问题定义**：现有的神经隐式SLAM方法通常计算复杂度高，难以满足实时性要求，尤其是在动态环境中。此外，一些方法需要大量的内存来存储地图信息，限制了其在移动设备上的应用。因此，需要一种高效、低内存且能实时运行的神经隐式SLAM系统。\\n\\n**核心思路**：本文的核心思路是利用场景坐标回归(SCR)来表示隐式地图。SCR通过训练一个轻量级神经网络，直接将2D图像特征映射到3D全局坐标。这种方法避免了传统SLAM中复杂的几何计算和优化过程，从而提高了效率。此外，SCR网络可以学习紧凑的地图表示，降低了内存占用。\\n\\n**技术框架**：ACE-SLAM系统的整体框架包括以下几个主要模块：1) 特征提取：从RGB-D图像中提取2D图像特征。2) 场景坐标回归：使用SCR网络将2D特征映射到3D场景坐标。3) 位姿估计：利用场景坐标信息估计相机的位姿。4) 地图更新：根据新的位姿信息更新隐式地图。该框架支持稀疏和密集特征，并且能够在动态环境中可靠运行。\\n\\n**关键创新**：ACE-SLAM的关键创新在于将SCR作为核心隐式地图表示引入到神经SLAM管道中。与传统的隐式表示方法（如TSDF）相比，SCR具有更高的效率和更低的内存占用。此外，ACE-SLAM还提出了一种专门为SLAM任务设计的SCR网络架构，并优化了训练过程，使其能够实时运行。\\n\\n**关键设计**：ACE-SLAM的关键设计包括：1) SCR网络架构：采用轻量级的卷积神经网络，以实现快速的特征提取和坐标回归。2) 损失函数：使用L1损失函数来训练SCR网络，以提高坐标预测的准确性。3) 位姿估计：采用迭代最近点(ICP)算法来估计相机的位姿，并利用场景坐标信息进行优化。4) 动态环境处理：通过使用鲁棒的特征提取器和位姿估计方法，使系统能够在动态环境中可靠运行。",
            "application_zh": "ACE-SLAM具有广泛的应用前景，包括增强现实(AR)、虚拟现实(VR)、机器人导航、三维重建等领域。其高效的地图表示和快速重定位能力使其特别适合在移动设备和资源受限的环境中使用。此外，由于SCR网络能够保护隐私，ACE-SLAM还可以应用于需要保护用户隐私的场景，例如智能家居和安全监控。",
            "highlight_zh": "ACE-SLAM在合成和真实数据集上进行了评估，实验结果表明，该系统能够实现实时性能，并且与最先进的神经隐式SLAM系统相比具有竞争力。例如，在某些数据集上，ACE-SLAM的运行速度比现有方法快数倍，同时保持了相当的精度。此外，ACE-SLAM还展示了其在动态环境中的鲁棒性。",
            "tags_zh": [
                "神经SLAM",
                "隐式地图",
                "场景坐标回归",
                "实时SLAM",
                "RGB-D SLAM"
            ],
            "_index": 34,
            "_used_api": "gemini"
        },
        {
            "title": "CLAIM: Camera-LiDAR Alignment with Intensity and Monodepth",
            "authors": [
                "Zhuo Zhang",
                "Yonghui Liu",
                "Meijie Zhang",
                "Feiyang Tan",
                "Yikang Ding"
            ],
            "arxiv_id": "2512.14001v1",
            "summary": "In this paper, we unleash the potential of the powerful monodepth model in camera-LiDAR calibration and propose CLAIM, a novel method of aligning data from the camera and LiDAR. Given the initial guess and pairs of images and LiDAR point clouds, CLAIM utilizes a coarse-to-fine searching method to find the optimal transformation minimizing a patched Pearson correlation-based structure loss and a mutual information-based texture loss. These two losses serve as good metrics for camera-LiDAR alignment results and require no complicated steps of data processing, feature extraction, or feature matching like most methods, rendering our method simple and adaptive to most scenes. We validate CLAIM on public KITTI, Waymo, and MIAS-LCEC datasets, and the experimental results demonstrate its superior performance compared with the state-of-the-art methods. The code is available at https://github.com/Tompson11/claim.",
            "categories": [
                "cs.RO",
                "cs.CV"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Accepted by IROS 2025",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14001v1",
            "code_links": [
                {
                    "url": "https://github.com/Tompson11/claim",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "深度估计 (Depth Estimation)",
                    "matched_keywords": [
                        "mono depth"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "CLAIM：利用单目深度和强度信息实现相机-激光雷达标定",
            "summary_zh": "本文旨在探索单目深度模型在相机-激光雷达标定中的潜力，并提出了一种新的相机和激光雷达数据对齐方法CLAIM。给定初始位姿估计以及图像和激光雷达点云对，CLAIM采用由粗到精的搜索策略，寻找最优的变换，以最小化基于分块Pearson相关的结构损失和基于互信息的纹理损失。这两种损失函数能够很好地衡量相机-激光雷达对齐结果，且无需复杂的数据处理、特征提取或特征匹配等步骤，使得我们的方法简单且适用于大多数场景。我们在公开的KITTI、Waymo和MIAS-LCEC数据集上验证了CLAIM，实验结果表明，与最先进的方法相比，CLAIM具有更优越的性能。代码已开源。",
            "intro_zh": [
                "现有相机-激光雷达标定方法通常依赖复杂的数据预处理和特征匹配，计算成本高且泛化性受限。",
                "CLAIM方法利用单目深度估计的结构信息和强度信息，通过最小化结构损失和纹理损失实现精确标定。",
                "实验表明，CLAIM在多个数据集上优于现有方法，无需复杂的特征工程，更具通用性和高效性。"
            ],
            "method_zh": "**问题定义**：相机-激光雷达标定的目标是确定相机坐标系和激光雷达坐标系之间的刚性变换关系。现有方法通常需要人工设计的特征提取和匹配步骤，对环境的适应性较差，且计算复杂度较高。这些方法往往依赖于特定的场景或需要大量的预处理工作，限制了其在实际应用中的推广。\\n\\n**核心思路**：CLAIM的核心思路是利用单目深度估计提供的图像结构信息和激光雷达点云的强度信息，通过最小化图像和点云之间的结构差异和纹理差异来实现自动标定。该方法避免了复杂的特征提取和匹配过程，直接利用原始数据进行优化，从而提高了效率和鲁棒性。\\n\\n**技术框架**：CLAIM的整体框架包括以下几个主要步骤：1) 给定初始的相机-激光雷达位姿估计；2) 利用单目深度估计模型预测图像的深度图；3) 将激光雷达点云投影到图像平面，并根据深度图计算每个像素点的三维坐标；4) 定义基于分块Pearson相关的结构损失和基于互信息的纹理损失，用于衡量图像和点云之间的对齐程度；5) 采用由粗到精的搜索策略，优化相机-激光雷达之间的变换矩阵，使得结构损失和纹理损失最小化。\\n\\n**关键创新**：CLAIM的关键创新在于：1) 利用单目深度估计作为桥梁，将图像的结构信息引入到相机-激光雷达标定中；2) 提出了一种基于分块Pearson相关的结构损失，能够有效地衡量图像和点云之间的结构相似性；3) 提出了一种基于互信息的纹理损失，能够有效地衡量图像和点云之间的纹理相似性。与现有方法相比，CLAIM无需复杂的特征提取和匹配步骤，更加简单高效。\\n\\n**关键设计**：CLAIM的关键设计包括：1) 分块Pearson相关系数的计算方式，通过将图像分成多个小块，分别计算相关系数，可以提高对局部结构变化的鲁棒性；2) 互信息损失的计算方式，通过计算图像和点云之间的互信息，可以有效地衡量它们之间的纹理相似性；3) 由粗到精的搜索策略，通过先进行粗略的搜索，再进行精细的搜索，可以提高优化效率和精度。",
            "application_zh": "该研究成果可广泛应用于自动驾驶、机器人导航、三维重建等领域。精确的相机-激光雷达标定是多传感器融合的基础，能够提升环境感知系统的准确性和可靠性，从而提高自动驾驶车辆的安全性。此外，该方法还可以应用于移动机器人的定位和建图，以及三维场景的重建和建模。",
            "highlight_zh": "CLAIM在KITTI、Waymo和MIAS-LCEC数据集上进行了验证，实验结果表明，CLAIM在标定精度上优于现有的state-of-the-art方法。例如，在KITTI数据集上，CLAIM的旋转误差和位移误差分别降低了X%和Y%（具体数据需要在论文中查找），证明了其优越的性能。",
            "tags_zh": [
                "相机-激光雷达标定",
                "单目深度估计",
                "传感器融合",
                "自动驾驶",
                "点云处理"
            ],
            "_index": 35,
            "_used_api": "gemini"
        }
    ]
}