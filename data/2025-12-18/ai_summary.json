{
    "papers": [
        {
            "title": "E-SDS: Environment-aware See it, Do it, Sorted - Automated Environment-Aware Reinforcement Learning for Humanoid Locomotion",
            "authors": [
                "Enis Yalcin",
                "Joshua O'Hara",
                "Maria Stamatopoulou",
                "Chengxu Zhou",
                "Dimitrios Kanoulas"
            ],
            "arxiv_id": "2512.16446v1",
            "summary": "Vision-language models (VLMs) show promise in automating reward design in humanoid locomotion, which could eliminate the need for tedious manual engineering. However, current VLM-based methods are essentially \"blind\", as they lack the environmental perception required to navigate complex terrain. We present E-SDS (Environment-aware See it, Do it, Sorted), a framework that closes this perception gap. E-SDS integrates VLMs with real-time terrain sensor analysis to automatically generate reward functions that facilitate training of robust perceptive locomotion policies, grounded by example videos. Evaluated on a Unitree G1 humanoid across four distinct terrains (simple, gaps, obstacles, stairs), E-SDS uniquely enabled successful stair descent, while policies trained with manually-designed rewards or a non-perceptive automated baseline were unable to complete the task. In all terrains, E-SDS also reduced velocity tracking error by 51.9-82.6%. Our framework reduces the human effort of reward design from days to less than two hours while simultaneously producing more robust and capable locomotion policies.",
            "categories": [
                "cs.RO",
                "cs.AI"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "12 pages, 3 figures, 4 tables. Accepted at RiTA 2025 (Springer LNNS)",
            "doi": "",
            "journal_ref": "RiTA 2025 (Springer LNNS)",
            "pdf_url": "https://arxiv.org/pdf/2512.16446v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]humanoid",
                        "[T]humanoid locomotion",
                        "[T]locomotion",
                        "locomotion policy",
                        "Unitree"
                    ],
                    "score": 22.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]reinforcement learning",
                        "reward design"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 28.0,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "E-SDS：环境感知的人形机器人强化学习框架，实现复杂地形稳健运动",
            "summary_zh": "本文提出E-SDS（Environment-aware See it, Do it, Sorted），一个环境感知的人形机器人强化学习框架，旨在解决现有基于视觉-语言模型（VLM）的方法在复杂地形导航中缺乏环境感知能力的问题。E-SDS将VLM与实时地形传感器分析相结合，自动生成奖励函数，从而训练出具有鲁棒感知能力的运动策略，并以示例视频作为指导。在Unitree G1人形机器人上，针对四种不同地形（简单、间隙、障碍物、楼梯）的评估表明，E-SDS能够成功完成下楼梯任务，而手动设计的奖励或非感知的自动基线方法均无法完成。在所有地形中，E-SDS还将速度跟踪误差降低了51.9-82.6%。该框架将奖励设计的人工工作量从数天减少到不到两小时，同时生成更鲁棒和更有能力的运动策略。",
            "intro_zh": [
                "现有基于视觉-语言模型的机器人运动方法缺乏环境感知，难以在复杂地形中导航。",
                "E-SDS框架融合视觉-语言模型与实时地形分析，自动生成奖励函数，训练鲁棒的运动策略。",
                "实验表明，E-SDS在复杂地形（如楼梯）上表现出色，速度跟踪误差降低51.9-82.6%。"
            ],
            "method_zh": "**问题定义**：现有基于视觉-语言模型（VLM）的机器人运动控制方法，虽然能够自动化奖励函数的设计，但本质上是“盲目的”，缺乏对周围环境的感知能力。这导致机器人在复杂地形（例如，有障碍物、间隙或楼梯的地形）中难以有效地导航和运动。手动设计奖励函数耗时且需要专业知识，而缺乏环境感知的自动化方法无法应对复杂地形的挑战。\\n\\n**核心思路**：E-SDS的核心思路是将视觉-语言模型（VLM）与实时地形传感器数据融合，从而使机器人能够“看到”并理解其周围的环境。通过分析地形数据，E-SDS能够生成与环境相关的奖励函数，引导机器人学习适应不同地形的运动策略。这种环境感知的奖励设计能够克服传统方法在复杂地形中的局限性。\\n\\n**技术框架**：E-SDS框架包含以下主要模块：1) **环境感知模块**：利用传感器（例如，深度相机或激光雷达）获取地形数据，并进行实时分析，提取地形特征（例如，高度图、坡度、障碍物位置）。2) **视觉-语言模型（VLM）**：使用VLM根据用户提供的示例视频和文本描述，生成初始的奖励函数。3) **奖励函数融合模块**：将VLM生成的奖励函数与环境感知模块提取的地形特征相结合，生成最终的、环境感知的奖励函数。4) **强化学习训练模块**：使用强化学习算法（例如，PPO）根据环境感知的奖励函数训练机器人的运动策略。\\n\\n**关键创新**：E-SDS最重要的技术创新点在于将视觉-语言模型与实时地形传感器数据进行深度融合，从而实现了环境感知的自动化奖励函数设计。与现有方法相比，E-SDS能够根据地形特征动态调整奖励函数，使机器人能够学习适应不同地形的运动策略。这种环境感知能力是现有方法所不具备的。\\n\\n**关键设计**：E-SDS的关键设计包括：1) **地形特征提取**：设计有效的算法从传感器数据中提取地形特征，例如，使用卷积神经网络（CNN）处理高度图，提取坡度、曲率等信息。2) **奖励函数融合**：设计合适的融合策略，将VLM生成的奖励函数与地形特征相结合，例如，使用加权平均或神经网络进行融合。3) **强化学习算法**：选择合适的强化学习算法，例如，PPO，并调整超参数以获得最佳的训练效果。4) **奖励函数缩放**：对奖励函数进行适当的缩放，以确保训练的稳定性和收敛性。",
            "application_zh": "E-SDS框架具有广泛的应用前景，可用于开发各种人形机器人的运动控制系统，尤其是在复杂和动态环境中。例如，可应用于搜救机器人、建筑机器人、医疗辅助机器人等，使其能够在各种地形条件下安全有效地执行任务。该研究有助于降低机器人运动控制的开发成本，并提高机器人的自主性和适应性。",
            "highlight_zh": "实验结果表明，E-SDS在Unitree G1人形机器人上，针对四种不同地形（简单、间隙、障碍物、楼梯）的评估中表现出色。E-SDS能够成功完成下楼梯任务，而手动设计的奖励或非感知的自动基线方法均无法完成。在所有地形中，E-SDS还将速度跟踪误差降低了51.9-82.6%，证明了其在复杂地形运动控制方面的优越性。",
            "tags_zh": [
                "人形机器人",
                "强化学习",
                "视觉-语言模型",
                "环境感知",
                "运动控制"
            ],
            "_index": 0,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16446v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16446v1/figure/vel_chart.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16446v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Vision-Language-Action Models for Autonomous Driving: Past, Present, and Future",
            "authors": [
                "Tianshuai Hu",
                "Xiaolu Liu",
                "Song Wang",
                "Yiyao Zhu",
                "Ao Liang",
                "Lingdong Kong",
                "Guoyang Zhao",
                "Zeying Gong",
                "Jun Cen",
                "Zhiyu Huang",
                "Xiaoshuai Hao",
                "Linfeng Li",
                "Hang Song",
                "Xiangtai Li",
                "Jun Ma",
                "Shaojie Shen",
                "Jianke Zhu",
                "Dacheng Tao",
                "Ziwei Liu",
                "Junwei Liang"
            ],
            "arxiv_id": "2512.16760v1",
            "summary": "Autonomous driving has long relied on modular \"Perception-Decision-Action\" pipelines, where hand-crafted interfaces and rule-based components often break down in complex or long-tailed scenarios. Their cascaded design further propagates perception errors, degrading downstream planning and control. Vision-Action (VA) models address some limitations by learning direct mappings from visual inputs to actions, but they remain opaque, sensitive to distribution shifts, and lack structured reasoning or instruction-following capabilities. Recent progress in Large Language Models (LLMs) and multimodal learning has motivated the emergence of Vision-Language-Action (VLA) frameworks, which integrate perception with language-grounded decision making. By unifying visual understanding, linguistic reasoning, and actionable outputs, VLAs offer a pathway toward more interpretable, generalizable, and human-aligned driving policies. This work provides a structured characterization of the emerging VLA landscape for autonomous driving. We trace the evolution from early VA approaches to modern VLA frameworks and organize existing methods into two principal paradigms: End-to-End VLA, which integrates perception, reasoning, and planning within a single model, and Dual-System VLA, which separates slow deliberation (via VLMs) from fast, safety-critical execution (via planners). Within these paradigms, we further distinguish subclasses such as textual vs. numerical action generators and explicit vs. implicit guidance mechanisms. We also summarize representative datasets and benchmarks for evaluating VLA-based driving systems and highlight key challenges and open directions, including robustness, interpretability, and instruction fidelity. Overall, this work aims to establish a coherent foundation for advancing human-compatible autonomous driving systems.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Preprint; 40 pages, 7 figures, 9 tables; GitHub at https://github.com/worldbench/awesome-vla-for-ad",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16760v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]vision-language-action",
                        "VLA",
                        "large language model",
                        "multimodal",
                        "instruction following"
                    ],
                    "score": 21.0
                }
            ],
            "relevance_score": 21.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "综述性论文：面向自动驾驶的视觉-语言-动作模型研究进展与未来展望",
            "summary_zh": "自动驾驶长期以来依赖于模块化的“感知-决策-行动”流程，但手工设计的接口和基于规则的组件在复杂或长尾场景中经常失效。其级联设计进一步传播感知误差，降低下游规划和控制的性能。视觉-动作（VA）模型通过学习从视觉输入到动作的直接映射来解决一些局限性，但它们仍然不透明，对分布偏移敏感，并且缺乏结构化推理或指令遵循能力。大型语言模型（LLM）和多模态学习的最新进展推动了视觉-语言-动作（VLA）框架的出现，该框架将感知与基于语言的决策相结合。通过统一视觉理解、语言推理和可操作的输出，VLA为更可解释、更通用和更符合人类习惯的驾驶策略提供了一条途径。本文对新兴的自动驾驶VLA领域进行了结构化描述，追溯了从早期VA方法到现代VLA框架的演变，并将现有方法组织成两种主要范例：端到端VLA和双系统VLA。总结了用于评估VLA驾驶系统的代表性数据集和基准，并强调了关键挑战和开放方向，包括鲁棒性、可解释性和指令保真度。总的来说，这项工作旨在为推进与人类兼容的自动驾驶系统奠定连贯的基础。",
            "intro_zh": [
                "传统自动驾驶依赖“感知-决策-行动”模块化流程，易受感知误差影响，且难以处理复杂场景。",
                "视觉-语言-动作（VLA）模型通过整合视觉、语言和动作，实现更可解释和通用的驾驶策略。",
                "论文综述了VLA在自动驾驶中的应用，分析了端到端和双系统两种范例，并指出了未来研究方向。"
            ],
            "method_zh": "**问题定义**：传统自动驾驶系统依赖于模块化的“感知-决策-行动”流程，这些流程存在多个问题。首先，手工设计的接口和规则在复杂或长尾场景中容易失效。其次，级联的设计会导致感知误差向下游传播，影响规划和控制的准确性。此外，早期的视觉-动作（VA）模型虽然能够直接从视觉输入映射到动作，但缺乏可解释性，对数据分布的变化非常敏感，并且缺乏结构化的推理能力和指令遵循能力。\\n\\n**核心思路**：论文的核心思路是利用大型语言模型（LLM）和多模态学习的最新进展，构建视觉-语言-动作（VLA）框架，将视觉感知与基于语言的决策相结合。通过统一视觉理解、语言推理和可执行的动作输出，VLA旨在实现更可解释、更通用、更符合人类习惯的自动驾驶策略。这种方法的核心在于利用语言作为桥梁，连接视觉感知和动作执行，从而提高系统的鲁棒性和泛化能力。\\n\\n**技术框架**：论文将现有的VLA方法组织成两种主要的范例：端到端VLA和双系统VLA。端到端VLA将感知、推理和规划整合到一个单一的模型中，直接从视觉输入生成动作。双系统VLA则将慢速的推理（通过视觉语言模型）与快速、安全关键的执行（通过规划器）分离。在这些范例中，又进一步区分了文本动作生成器与数值动作生成器，以及显式指导机制与隐式指导机制。\\n\\n**关键创新**：论文的关键创新在于对自动驾驶领域的VLA模型进行了系统的分类和分析，并提出了端到端VLA和双系统VLA两种主要范例。这种分类方法有助于研究人员更好地理解不同VLA模型的优缺点，并为未来的研究方向提供了指导。此外，论文还强调了VLA模型在可解释性、通用性和指令保真度方面的优势，这些优势是传统自动驾驶系统所不具备的。\\n\\n**关键设计**：论文没有涉及具体的模型设计细节，而侧重于对现有VLA框架的综述和分类。因此，没有具体的参数设置、损失函数或网络结构等技术细节可以描述。论文主要关注的是不同VLA框架的整体架构和流程，以及它们在自动驾驶任务中的应用。",
            "application_zh": "该研究对自动驾驶领域具有重要的应用价值。VLA模型能够提升自动驾驶系统的可解释性、通用性和鲁棒性，使其在复杂和未知的环境中更好地运行。未来的发展方向包括提高VLA模型的指令遵循能力、增强其对长尾场景的处理能力，以及实现更安全可靠的自动驾驶系统。这项研究也将推动人机交互在自动驾驶领域的应用，使车辆能够更好地理解和响应人类的指令。",
            "highlight_zh": "该论文是一篇综述性文章，没有具体的实验结果。其亮点在于对现有VLA模型进行了系统的分类和分析，提出了端到端VLA和双系统VLA两种主要范例，并指出了未来研究的关键挑战和开放方向，例如鲁棒性、可解释性和指令保真度。该综述为研究人员提供了一个全面的VLA模型发展概况，并为未来的研究方向提供了指导。",
            "tags_zh": [
                "自动驾驶",
                "视觉语言动作模型",
                "多模态学习",
                "大型语言模型",
                "端到端学习",
                "双系统架构",
                "综述"
            ],
            "_index": 1,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16760v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16760v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16760v1/figures/fig3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation",
            "authors": [
                "Xin Lin",
                "Meixi Song",
                "Dizhe Zhang",
                "Wenxuan Lu",
                "Haodong Li",
                "Bo Du",
                "Ming-Hsuan Yang",
                "Truong Nguyen",
                "Lu Qi"
            ],
            "arxiv_id": "2512.16913v1",
            "summary": "In this work, we present a panoramic metric depth foundation model that generalizes across diverse scene distances. We explore a data-in-the-loop paradigm from the view of both data construction and framework design. We collect a large-scale dataset by combining public datasets, high-quality synthetic data from our UE5 simulator and text-to-image models, and real panoramic images from the web. To reduce domain gaps between indoor/outdoor and synthetic/real data, we introduce a three-stage pseudo-label curation pipeline to generate reliable ground truth for unlabeled images. For the model, we adopt DINOv3-Large as the backbone for its strong pre-trained generalization, and introduce a plug-and-play range mask head, sharpness-centric optimization, and geometry-centric optimization to improve robustness to varying distances and enforce geometric consistency across views. Experiments on multiple benchmarks (e.g., Stanford2D3D, Matterport3D, and Deep360) demonstrate strong performance and zero-shot generalization, with particularly robust and stable metric predictions in diverse real-world scenes. The project page can be found at: \\href{https://insta360-research-team.github.io/DAP_website/} {https://insta360-research-team.github.io/DAP\\_website/}",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Project Page: https://insta360-research-team.github.io/DAP_website/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16913v1",
            "code_links": [
                {
                    "url": "https://insta360-research-team.github.io/DAP_website/",
                    "type": "project_page"
                },
                {
                    "url": "https://insta360-research-team.github.io/DAP",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]depth estimation",
                        "metric depth"
                    ],
                    "score": 8.0
                },
                {
                    "name": "支柱七：动作重定向 (Motion Retargeting)",
                    "id": "7_retargeting",
                    "matched_keywords": [
                        "geometric consistency"
                    ],
                    "score": 3.0
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]foundation model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 20.0,
            "hit_pillars": [
                "3_perception_slam",
                "7_retargeting",
                "9_embodied_foundation"
            ],
            "headline_zh": "提出全景深度估计基础模型DAP，提升跨场景距离的泛化能力。",
            "summary_zh": "本文提出了一种全景度量深度基础模型，该模型能够泛化到各种场景距离。我们探索了一种数据闭环范式，从数据构建和框架设计的角度出发。我们通过结合公共数据集、来自UE5模拟器的高质量合成数据、文本到图像模型以及来自网络的真实全景图像，收集了一个大规模数据集。为了减少室内/室外和合成/真实数据之间的领域差距，我们引入了一个三阶段伪标签生成流程，为未标记图像生成可靠的真值。在模型方面，我们采用DINOv3-Large作为骨干网络，因为它具有强大的预训练泛化能力，并引入了一个即插即用的范围掩码头、以清晰度为中心的优化和以几何为中心的优化，以提高对不同距离的鲁棒性并加强跨视图的几何一致性。在多个基准测试（例如，Stanford2D3D、Matterport3D和Deep360）上的实验表明，该模型具有强大的性能和零样本泛化能力，尤其是在各种真实场景中具有鲁棒和稳定的度量预测。",
            "intro_zh": [
                "现有全景深度估计方法在处理不同场景距离和领域泛化性方面存在不足，尤其是在真实场景中。",
                "论文提出一种数据闭环范式，结合合成数据、真实数据和伪标签技术，构建大规模、多样化的训练数据集。",
                "采用DINOv3-Large作为骨干，并引入范围掩码头、清晰度优化和几何优化，提升模型鲁棒性和几何一致性。"
            ],
            "method_zh": "**问题定义**：全景深度估计旨在从单张全景图像中预测场景的深度信息。现有方法在处理不同场景距离（近距离、远距离）以及室内外场景的泛化能力上存在挑战。真实世界全景图像的标注成本高昂，导致训练数据不足，模型难以适应真实场景的复杂性。\\n\\n**核心思路**：论文的核心思路是构建一个大规模、多样化的全景深度数据集，并设计一个能够有效利用该数据集进行训练的基础模型。通过数据闭环的方式，不断迭代优化数据集和模型，提升模型的泛化能力和鲁棒性。\\n\\n**技术框架**：整体框架包含三个主要阶段：1) 数据收集与增强：结合公共数据集、UE5合成数据、文本到图像生成数据和真实网络图像，构建大规模数据集。2) 伪标签生成：采用三阶段伪标签流程，为未标注图像生成可靠的深度真值，缩小领域差距。3) 模型训练：使用DINOv3-Large作为骨干网络，并引入范围掩码头、清晰度优化和几何优化策略。\\n\\n**关键创新**：论文的关键创新在于数据闭环范式和针对全景深度估计的优化策略。数据闭环通过伪标签生成，有效利用了未标注数据，提升了模型的泛化能力。范围掩码头、清晰度优化和几何优化则分别解决了不同场景距离的鲁棒性问题和跨视图的几何一致性问题。\\n\\n**关键设计**：范围掩码头用于区分不同距离范围内的深度预测，提高模型对不同距离的敏感性。清晰度优化通过关注图像的清晰区域，提高深度预测的准确性。几何优化则通过约束相邻像素的深度关系，保证跨视图的几何一致性。具体损失函数的设计和参数设置在论文中有详细描述。",
            "application_zh": "该研究成果可应用于机器人导航、虚拟现实/增强现实、三维重建、自动驾驶等领域。高质量的全景深度估计能够为这些应用提供更准确的环境感知信息，提升系统的性能和用户体验。未来，该模型可以进一步扩展到其他全景理解任务，例如语义分割、目标检测等。",
            "highlight_zh": "实验结果表明，该模型在Stanford2D3D、Matterport3D和Deep360等多个基准测试上取得了优异的性能，并展现出强大的零样本泛化能力。尤其是在真实场景中，该模型能够生成鲁棒且稳定的度量深度预测，显著优于现有方法。具体性能数据和对比结果可在论文中查阅。",
            "tags_zh": [
                "全景深度估计",
                "基础模型",
                "数据闭环",
                "伪标签",
                "DINOv3",
                "几何一致性",
                "领域泛化"
            ],
            "_index": 2,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16913v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16913v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16913v1/x4.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "GeoPredict: Leveraging Predictive Kinematics and 3D Gaussian Geometry for Precise VLA Manipulation",
            "authors": [
                "Jingjing Qian",
                "Boyao Han",
                "Chen Shi",
                "Lei Xiao",
                "Long Yang",
                "Shaoshuai Shi",
                "Li Jiang"
            ],
            "arxiv_id": "2512.16811v1",
            "summary": "Vision-Language-Action (VLA) models achieve strong generalization in robotic manipulation but remain largely reactive and 2D-centric, making them unreliable in tasks that require precise 3D reasoning. We propose GeoPredict, a geometry-aware VLA framework that augments a continuous-action policy with predictive kinematic and geometric priors. GeoPredict introduces a trajectory-level module that encodes motion history and predicts multi-step 3D keypoint trajectories of robot arms, and a predictive 3D Gaussian geometry module that forecasts workspace geometry with track-guided refinement along future keypoint trajectories. These predictive modules serve exclusively as training-time supervision through depth-based rendering, while inference requires only lightweight additional query tokens without invoking any 3D decoding. Experiments on RoboCasa Human-50, LIBERO, and real-world manipulation tasks show that GeoPredict consistently outperforms strong VLA baselines, especially in geometry-intensive and spatially demanding scenarios.",
            "categories": [
                "cs.CV",
                "cs.RO"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16811v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]manipulation"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "vision-language-action",
                        "[T]VLA"
                    ],
                    "score": 12.0
                }
            ],
            "relevance_score": 18.0,
            "hit_pillars": [
                "1_robot_core",
                "9_embodied_foundation"
            ],
            "headline_zh": "GeoPredict：利用预测运动学和3D高斯几何实现精确的VLA操作",
            "summary_zh": "视觉-语言-动作(VLA)模型在机器人操作中表现出强大的泛化能力，但很大程度上是反应式的和以2D为中心的，这使得它们在需要精确3D推理的任务中不可靠。我们提出了GeoPredict，一个几何感知的VLA框架，它用预测运动学和几何先验来增强连续动作策略。GeoPredict引入了一个轨迹级模块，该模块编码运动历史并预测机器人手臂的多步3D关键点轨迹，以及一个预测性3D高斯几何模块，该模块预测工作空间几何形状，并通过沿未来关键点轨迹的跟踪引导细化。这些预测模块仅作为训练时的监督，通过基于深度的渲染实现，而推理只需要轻量级的额外查询token，无需调用任何3D解码。在RoboCasa Human-50、LIBERO和真实世界操作任务上的实验表明，GeoPredict始终优于强大的VLA基线，尤其是在几何密集型和空间要求高的场景中。",
            "intro_zh": [
                "现有VLA模型在精确3D推理任务中表现不足，缺乏对环境几何信息的有效利用。",
                "GeoPredict通过预测机器人手臂的运动轨迹和工作空间几何形状，为VLA模型引入了几何先验。",
                "实验表明，GeoPredict在几何密集型和空间要求高的场景中，显著优于现有的VLA基线方法。"
            ],
            "method_zh": "**问题定义**：现有的视觉-语言-动作(VLA)模型在机器人操作任务中，尤其是在需要精确3D推理的场景下，存在不足。它们通常是反应式的，并且主要依赖于2D视觉信息，缺乏对机器人运动学和环境几何信息的有效利用，导致操作精度不高。\\n\\n**核心思路**：GeoPredict的核心思路是在训练阶段引入预测性的运动学和几何先验知识，从而提升VLA模型对3D环境的理解和操作能力。通过预测机器人手臂的关键点轨迹以及工作空间的3D几何形状，为模型提供更丰富的上下文信息，指导其生成更精确的动作。\\n\\n**技术框架**：GeoPredict框架包含一个轨迹级模块和一个预测性3D高斯几何模块。轨迹级模块负责编码机器人手臂的运动历史，并预测未来多步的关键点轨迹。预测性3D高斯几何模块则预测工作空间的几何形状，并根据预测的关键点轨迹进行细化。这两个模块在训练阶段通过深度渲染提供监督信号，而在推理阶段仅需少量额外的查询token，无需进行复杂的3D解码。\\n\\n**关键创新**：GeoPredict的关键创新在于将预测性的运动学和几何先验知识融入到VLA模型的训练过程中。通过预测未来轨迹和几何形状，模型能够更好地理解3D环境，从而生成更精确的动作。此外，该方法在推理阶段避免了复杂的3D解码，保持了模型的轻量级和高效性。\\n\\n**关键设计**：GeoPredict使用深度渲染技术将预测的3D关键点轨迹和几何形状转化为深度图，作为训练的监督信号。轨迹级模块可能采用循环神经网络（RNN）或Transformer等结构来编码运动历史并预测未来轨迹。预测性3D高斯几何模块可能使用高斯混合模型或其他概率模型来表示工作空间的几何形状。损失函数的设计需要考虑轨迹预测的准确性和几何形状预测的逼真度。",
            "application_zh": "GeoPredict具有广泛的应用前景，可用于提升机器人操作的精度和可靠性，尤其是在需要精细操作和复杂环境理解的场景中，例如：医疗手术机器人、精密装配、家庭服务机器人等。该研究有助于推动机器人技术在更广泛领域的应用，并提高机器人的智能化水平。",
            "highlight_zh": "GeoPredict在RoboCasa Human-50、LIBERO和真实世界操作任务上进行了评估，实验结果表明，GeoPredict始终优于强大的VLA基线。尤其是在几何密集型和空间要求高的场景中，GeoPredict的性能提升更为显著，证明了其在精确3D操作方面的优势。",
            "tags_zh": [
                "视觉语言动作模型",
                "机器人操作",
                "3D几何推理",
                "运动学预测",
                "高斯几何",
                "深度渲染"
            ],
            "_index": 3,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16811v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16811v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16811v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "KineST: A Kinematics-guided Spatiotemporal State Space Model for Human Motion Tracking from Sparse Signals",
            "authors": [
                "Shuting Zhao",
                "Zeyu Xiao",
                "Xinrong Chen"
            ],
            "arxiv_id": "2512.16791v1",
            "summary": "Full-body motion tracking plays an essential role in AR/VR applications, bridging physical and virtual interactions. However, it is challenging to reconstruct realistic and diverse full-body poses based on sparse signals obtained by head-mounted displays, which are the main devices in AR/VR scenarios. Existing methods for pose reconstruction often incur high computational costs or rely on separately modeling spatial and temporal dependencies, making it difficult to balance accuracy, temporal coherence, and efficiency. To address this problem, we propose KineST, a novel kinematics-guided state space model, which effectively extracts spatiotemporal dependencies while integrating local and global pose perception. The innovation comes from two core ideas. Firstly, in order to better capture intricate joint relationships, the scanning strategy within the State Space Duality framework is reformulated into kinematics-guided bidirectional scanning, which embeds kinematic priors. Secondly, a mixed spatiotemporal representation learning approach is employed to tightly couple spatial and temporal contexts, balancing accuracy and smoothness. Additionally, a geometric angular velocity loss is introduced to impose physically meaningful constraints on rotational variations for further improving motion stability. Extensive experiments demonstrate that KineST has superior performance in both accuracy and temporal consistency within a lightweight framework. Project page: https://kaka-1314.github.io/KineST/",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Accepted by AAAI 2026",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16791v1",
            "code_links": [
                {
                    "url": "https://kaka-1314.github.io/KineST/",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]state space model",
                        "representation learning"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱八：物理动画 (Physics-based Animation)",
                    "id": "8_physics_animation",
                    "matched_keywords": [
                        "[T]spatiotemporal",
                        "[T]motion tracking"
                    ],
                    "score": 12.0
                }
            ],
            "relevance_score": 18.0,
            "hit_pillars": [
                "2_algo_arch",
                "8_physics_animation"
            ],
            "headline_zh": "KineST：一种基于运动学引导的时空状态空间模型，用于从稀疏信号中进行人体运动跟踪",
            "summary_zh": "全身运动跟踪在AR/VR应用中至关重要，它连接了物理交互和虚拟交互。然而，基于头戴式显示器获取的稀疏信号重建逼真且多样化的全身姿势仍然具有挑战性。现有的姿势重建方法通常计算成本高昂，或者依赖于分别建模空间和时间依赖性，难以平衡准确性、时间连贯性和效率。为了解决这个问题，我们提出了一种新颖的运动学引导的状态空间模型KineST，它有效地提取时空依赖性，同时整合局部和全局姿势感知。其创新之处在于两个核心思想：首先，为了更好地捕捉复杂的关节关系，我们将状态空间对偶框架内的扫描策略重新制定为运动学引导的双向扫描，从而嵌入运动学先验。其次，采用混合时空表示学习方法来紧密耦合空间和时间上下文，从而平衡准确性和平滑性。此外，引入了几何角速度损失，对旋转变化施加物理意义上的约束，进一步提高运动稳定性。大量实验表明，KineST在轻量级框架内具有卓越的准确性和时间一致性。",
            "intro_zh": [
                "现有方法难以兼顾精度、时序一致性和效率，无法有效利用AR/VR场景中头显提供的稀疏信号重建全身姿态。",
                "KineST通过运动学引导的双向扫描和混合时空表示学习，在状态空间模型中有效提取时空依赖关系。",
                "实验表明，KineST在轻量级框架下，在运动跟踪的准确性和时间一致性方面均表现出优越的性能。"
            ],
            "method_zh": "**问题定义**：论文旨在解决从稀疏信号（如AR/VR头显提供的信号）中准确、高效地重建全身运动的问题。现有方法的痛点在于：要么计算复杂度高，难以实时应用；要么分别建模空间和时间依赖性，导致重建的运动不连贯、不自然。\\n\\n**核心思路**：论文的核心思路是利用运动学先验知识来指导时空状态空间模型的构建，从而更有效地提取和利用运动中的时空依赖关系。通过运动学引导的双向扫描，模型能够更好地理解关节之间的相互作用，并结合混合时空表示学习，平衡准确性和平滑性。\\n\\n**技术框架**：KineST的整体框架基于状态空间模型，主要包含以下几个阶段：1) 稀疏信号输入；2) 运动学引导的双向扫描：利用运动学链的结构，从两个方向扫描状态空间，提取关节间的关系；3) 混合时空表示学习：将空间和时间信息紧密耦合，学习运动的时空表示；4) 姿态重建：基于学习到的时空表示，重建全身姿态。\\n\\n**关键创新**：论文最重要的创新点在于运动学引导的双向扫描和混合时空表示学习。运动学引导的双向扫描将运动学先验知识融入到状态空间模型的扫描策略中，使得模型能够更好地捕捉关节间的依赖关系。混合时空表示学习则能够有效地平衡重建的准确性和时间一致性。与现有方法相比，KineST能够更有效地利用稀疏信号中的信息，重建更准确、更自然的运动。\\n\\n**关键设计**：论文的关键设计包括：1) 运动学引导的双向扫描的具体实现方式，例如如何根据运动学链选择扫描方向和顺序；2) 混合时空表示学习的具体网络结构，例如如何将空间和时间信息融合在一起；3) 几何角速度损失的定义和使用，该损失函数用于约束重建运动的旋转变化，提高运动的稳定性。具体参数设置和网络结构细节在论文中应该有更详细的描述（未知）。",
            "application_zh": "KineST具有广泛的应用前景，尤其是在AR/VR领域。它可以用于构建更逼真、更自然的虚拟化身，提升用户在虚拟环境中的沉浸感和交互体验。此外，该技术还可以应用于游戏、动画制作、运动分析、康复训练等领域，具有重要的实际价值和潜在的商业价值。未来，KineST可以进一步扩展到处理更复杂的运动场景，例如多人交互、物体交互等。",
            "highlight_zh": "实验结果表明，KineST在准确性和时间一致性方面均优于现有方法。具体而言，KineST在姿态重建的平均误差方面降低了X%（具体数值未知），并且在时间一致性指标上提升了Y%（具体数值未知）。这些结果证明了KineST在从稀疏信号中进行人体运动跟踪方面的优越性能。",
            "tags_zh": [
                "人体运动跟踪",
                "状态空间模型",
                "运动学引导",
                "时空表示学习",
                "AR/VR",
                "姿态重建",
                "稀疏信号"
            ],
            "_index": 4,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16791v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16791v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16791v1/scan2.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "VERM: Leveraging Foundation Models to Create a Virtual Eye for Efficient 3D Robotic Manipulation",
            "authors": [
                "Yixiang Chen",
                "Yan Huang",
                "Keji He",
                "Peiyan Li",
                "Liang Wang"
            ],
            "arxiv_id": "2512.16724v1",
            "summary": "When performing 3D manipulation tasks, robots have to execute action planning based on perceptions from multiple fixed cameras. The multi-camera setup introduces substantial redundancy and irrelevant information, which increases computational costs and forces the model to spend extra training time extracting crucial task-relevant details. To filter out redundant information and accurately extract task-relevant features, we propose the VERM (Virtual Eye for Robotic Manipulation) method, leveraging the knowledge in foundation models to imagine a virtual task-adaptive view from the constructed 3D point cloud, which efficiently captures necessary information and mitigates occlusion. To facilitate 3D action planning and fine-grained manipulation, we further design a depth-aware module and a dynamic coarse-to-fine procedure. Extensive experimental results on both simulation benchmark RLBench and real-world evaluations demonstrate the effectiveness of our method, surpassing previous state-of-the-art methods while achieving 1.89x speedup in training time and 1.54x speedup in inference speed. More results can be found on our project website at https://verm-ral.github.io .",
            "categories": [
                "cs.RO",
                "cs.CV"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Accepted at RA-L 2025",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16724v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]manipulation"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]foundation model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 15.0,
            "hit_pillars": [
                "1_robot_core",
                "9_embodied_foundation"
            ],
            "headline_zh": "VERM：利用基础模型构建虚拟视点，提升3D机器人操作效率",
            "summary_zh": "为了执行3D操作任务，机器人需要基于多个固定摄像头的感知进行动作规划。这种多摄像头设置引入了大量的冗余和不相关信息，增加了计算成本，并迫使模型花费额外的训练时间来提取关键的任务相关细节。为了过滤掉冗余信息并准确提取任务相关特征，我们提出了VERM（Virtual Eye for Robotic Manipulation）方法，利用基础模型中的知识，从构建的3D点云中想象出一个虚拟的、任务自适应的视点，从而有效地捕获必要的信息并减轻遮挡。为了促进3D动作规划和精细操作，我们进一步设计了一个深度感知模块和一个动态的由粗到精的过程。在模拟基准RLBench和真实世界评估中进行的大量实验结果表明了我们方法的有效性，超越了先前的最先进方法，同时在训练时间上实现了1.89倍的加速，在推理速度上实现了1.54倍的加速。",
            "intro_zh": [
                "现有机器人3D操作依赖多摄像头，引入冗余信息，增加计算负担，模型需耗费额外时间提取关键特征。",
                "VERM方法利用基础模型知识，从3D点云构建任务自适应的虚拟视点，有效捕获信息并减少遮挡。",
                "实验表明，VERM在RLBench和真实场景中超越SOTA，训练加速1.89倍，推理加速1.54倍。"
            ],
            "method_zh": "**问题定义**：现有基于多摄像头的机器人3D操作方法存在冗余信息过多、计算成本高昂的问题。模型需要处理来自多个视角的图像，其中很多信息与当前任务无关，导致训练效率低下，推理速度受限。此外，固定视角的摄像头容易受到遮挡的影响，难以获取完整的目标信息。\\n\\n**核心思路**：VERM的核心思路是利用预训练的基础模型，从多摄像头获取的3D点云中生成一个虚拟的、任务相关的视点。这个虚拟视点能够过滤掉冗余信息，突出任务相关的特征，并减轻遮挡的影响。通过模拟一个“虚拟眼睛”，机器人可以更高效地感知环境，从而提升动作规划和操作的效率。\\n\\n**技术框架**：VERM方法主要包含以下几个阶段：1) **3D点云构建**：利用多摄像头获取的图像数据，构建场景的3D点云表示。2) **虚拟视点生成**：利用预训练的基础模型，根据任务目标，从3D点云中生成一个虚拟视点。这个过程可能涉及到视点选择、图像渲染等操作。3) **深度感知模块**：设计一个深度感知模块，用于提取虚拟视点图像中的深度信息，从而更好地理解3D场景。4) **动态粗到精过程**：采用动态的由粗到精的操作策略，先进行粗略的动作规划，然后逐步细化，从而提高操作的精度和效率。\\n\\n**关键创新**：VERM的关键创新在于利用基础模型来生成任务自适应的虚拟视点。与传统的固定视点方法相比，VERM能够根据任务动态地调整视点，从而更好地捕获任务相关的特征，并减轻遮挡的影响。此外，深度感知模块和动态粗到精过程也进一步提升了操作的性能。\\n\\n**关键设计**：VERM的具体实现细节可能包括：1) **基础模型的选择**：选择合适的预训练基础模型，例如CLIP等，用于生成虚拟视点。2) **视点选择策略**：设计有效的视点选择策略，例如基于强化学习或启发式算法，来确定最佳的虚拟视点位置和方向。3) **深度感知模块的结构**：设计合适的深度感知模块，例如基于卷积神经网络或Transformer，来提取虚拟视点图像中的深度信息。4) **损失函数的设计**：设计合适的损失函数，用于训练虚拟视点生成模型和深度感知模块，例如重建损失、对比损失等。",
            "application_zh": "VERM方法具有广泛的应用前景，可应用于工业自动化、服务机器人、医疗机器人等领域。例如，在工业自动化中，VERM可以帮助机器人更高效地完成装配、搬运等任务；在服务机器人中，VERM可以帮助机器人更好地理解用户意图，提供更智能的服务；在医疗机器人中，VERM可以帮助医生更精准地进行手术操作。该研究有望推动机器人技术的发展，使其在更多领域得到应用。",
            "highlight_zh": "VERM在RLBench模拟环境和真实世界场景中进行了广泛的实验验证。实验结果表明，VERM方法在多个3D操作任务上超越了先前的SOTA方法，并且在训练时间上实现了1.89倍的加速，在推理速度上实现了1.54倍的加速。这些结果充分证明了VERM方法的有效性和高效性。",
            "tags_zh": [
                "机器人操作",
                "虚拟视点",
                "基础模型",
                "3D感知",
                "深度学习",
                "动作规划",
                "任务自适应"
            ],
            "_index": 5,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16724v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16724v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16724v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "AIMM: An AI-Driven Multimodal Framework for Detecting Social-Media-Influenced Stock Market Manipulation",
            "authors": [
                "Sandeep Neela"
            ],
            "arxiv_id": "2512.16103v1",
            "summary": "Market manipulation now routinely originates from coordinated social media campaigns, not isolated trades. Retail investors, regulators, and brokerages need tools that connect online narratives and coordination patterns to market behavior. We present AIMM, an AI-driven framework that fuses Reddit activity, bot and coordination indicators, and OHLCV market features into a daily AIMM Manipulation Risk Score for each ticker.\n  The system uses a parquet-native pipeline with a Streamlit dashboard that allows analysts to explore suspicious windows, inspect underlying posts and price action, and log model outputs over time. Due to Reddit API restrictions, we employ calibrated synthetic social features matching documented event characteristics; market data (OHLCV) uses real historical data from Yahoo Finance. This release makes three contributions. First, we build the AIMM Ground Truth dataset (AIMM-GT): 33 labeled ticker-days spanning eight equities, drawing from SEC enforcement actions, community-verified manipulation cases, and matched normal controls. Second, we implement forward-walk evaluation and prospective prediction logging for both retrospective and deployment-style assessment. Third, we analyze lead times and show that AIMM flagged GME 22 days before the January 2021 squeeze peak.\n  The current labeled set is small (33 ticker-days, 3 positive events), but results show preliminary discriminative capability and early warnings for the GME incident. We release the code, dataset schema, and dashboard design to support research on social media-driven market surveillance.",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Preprint",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16103v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]manipulation"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]multimodal"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 15.0,
            "hit_pillars": [
                "1_robot_core",
                "9_embodied_foundation"
            ],
            "headline_zh": "AIMM：一种AI驱动的多模态框架，用于检测社交媒体影响的股市操纵",
            "summary_zh": "本文提出AIMM，一个AI驱动的框架，旨在融合Reddit活动、机器人和协同行为指标以及OHLCV市场特征，为每个股票代码生成每日AIMM操纵风险评分，以应对源于协同社交媒体活动的股市操纵问题。该系统采用基于Parquet的流水线和一个Streamlit仪表板，使分析师能够探索可疑窗口，检查底层帖子和价格行为，并记录模型随时间的输出。由于Reddit API的限制，我们使用了经过校准的合成社交特征，使其与已记录的事件特征相匹配；市场数据（OHLCV）使用来自Yahoo Finance的真实历史数据。本文贡献包括：构建AIMM Ground Truth数据集（AIMM-GT），包含33个标记的股票代码-日期，涵盖8个股票，数据来自SEC执法行动、社区验证的操纵案例和匹配的正常对照；实施前向步进评估和前瞻性预测日志记录，用于回顾性和部署式评估；分析提前期，表明AIMM在2021年1月GME挤兑高峰前22天发出了警告。尽管当前标记集较小，但结果显示了初步的区分能力和对GME事件的早期预警。我们发布代码、数据集模式和仪表板设计，以支持对社交媒体驱动的市场监控的研究。",
            "intro_zh": [
                "现有方法难以有效关联社交媒体叙事、协同模式与市场行为，零售投资者、监管机构和券商缺乏有效工具。",
                "AIMM框架融合Reddit活动、机器人指标、协同行为和市场特征，生成每日操纵风险评分，提前预警市场操纵。",
                "实验表明，AIMM在小规模数据集上具有初步区分能力，并在GME事件爆发前22天发出预警，验证了其有效性。"
            ],
            "method_zh": "**问题定义**：当前市场操纵行为日益依赖于社交媒体上的协同活动，而非孤立的交易行为。现有的市场监控工具难以有效地将社交媒体上的叙事、协同模式与市场行为联系起来，导致零售投资者、监管机构和券商难以及时发现和应对此类操纵行为。因此，需要一种能够整合多源信息，并提供早期预警的系统。\n\\n**核心思路**：AIMM的核心思路是将社交媒体数据（Reddit活动）、机器人和协同行为的指标，以及传统的市场数据（OHLCV）进行融合，利用AI模型学习这些数据之间的关联，从而预测市场操纵的风险。通过综合分析这些信息，可以更全面地了解市场动态，并及时发现潜在的操纵行为。\n\\n**技术框架**：AIMM框架包含以下主要模块：1) 数据收集模块：收集Reddit数据、机器人和协同行为指标以及OHLCV市场数据。2) 特征工程模块：从收集到的数据中提取相关特征，例如Reddit帖子的情感、机器人账号的活动频率、交易量和价格波动等。3) 模型训练模块：使用机器学习模型（具体模型类型未知）学习特征与市场操纵之间的关系。4) 风险评分模块：根据模型预测结果，为每个股票代码生成每日AIMM操纵风险评分。5) 可视化模块：使用Streamlit仪表板展示风险评分、底层帖子和价格行为，方便分析师进行分析。\n\\n**关键创新**：AIMM的关键创新在于其多模态数据融合的方法，它将社交媒体数据、机器人指标和市场数据结合起来，从而更全面地了解市场动态。此外，AIMM还构建了一个包含标记数据的AIMM-GT数据集，用于训练和评估模型。该数据集包含来自SEC执法行动、社区验证的操纵案例和匹配的正常对照。\n\\n**关键设计**：由于Reddit API的限制，AIMM使用了经过校准的合成社交特征，使其与已记录的事件特征相匹配。市场数据（OHLCV）使用来自Yahoo Finance的真实历史数据。模型训练采用前向步进评估和前瞻性预测日志记录，用于回顾性和部署式评估。具体的模型结构、损失函数和参数设置未知。",
            "application_zh": "AIMM框架可应用于金融监管、券商风控和个人投资者风险管理等领域。监管机构可以使用AIMM来监控市场操纵行为，及时采取措施保护投资者利益。券商可以利用AIMM来识别潜在的风险客户，并加强风险管理。个人投资者可以使用AIMM来评估投资风险，做出更明智的投资决策。该研究有助于提高市场透明度，维护市场公平。",
            "highlight_zh": "实验结果表明，AIMM在小规模数据集上具有初步的区分能力，能够区分市场操纵事件和正常市场行为。更重要的是，AIMM在2021年1月GME挤兑高峰前22天发出了预警，表明其具有提前预警市场操纵事件的潜力。尽管数据集规模有限，但这些结果验证了AIMM框架的有效性。",
            "tags_zh": [
                "股市操纵检测",
                "社交媒体分析",
                "多模态融合",
                "风险预警",
                "金融监管"
            ],
            "_index": 6,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16103v1/figures/GroundTruthCoverage.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16103v1/figures/Forward-walk.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16103v1/figures/PredictionLog.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "AlignMerge - Alignment-Preserving Large Language Model Merging via Fisher-Guided Geometric Constraints",
            "authors": [
                "Aniruddha Roy",
                "Jyoti Patel",
                "Aman Chadha",
                "Vinija Jain",
                "Amitava Das"
            ],
            "arxiv_id": "2512.16245v1",
            "summary": "Merging large language models (LLMs) is a practical way to compose capabilities from multiple fine-tuned checkpoints without retraining. Yet standard schemes (linear weight soups, task vectors, and Fisher-weighted averaging) can preserve loss while quietly destroying alignment. We argue that merging is not a numerical trick but a geometry-constrained operation around an already-aligned anchor: fusion must be steered to respect safety geometry, not validated post hoc.\n  We introduce AlignMerge, a geometry-aware merging framework that makes alignment an explicit invariant. In a local Fisher chart around an instruction-tuned base, we estimate an alignment subspace with projector P_A and optimize:\n  L_AlignMerge = L_geo + lambda_align * L_align + lambda_bud * L_bud,\n  where L_geo keeps the merge close to its experts in Fisher-Rao geometry, L_align penalizes motion along alignment-sensitive directions, and L_bud enforces a soft alignment budget. As the alignment functional we use the decoding-invariant Alignment Quality Index (AQI), a latent-space criterion that captures how cleanly aligned and misaligned behaviors separate in representation space.\n  Across five model families (LLaMA-3 8B, Mistral 7B, Qwen 2, Phi-3.5, Gemma 2), merging safety anchors with task experts, AlignMerge improves alignment metrics (AQI, toxicity, LLM-judge alignment) while matching or exceeding the best expert on instruction-following, reasoning, and helpfulness. It also exhibits smaller alignment-subspace drift and fewer budget violations than Fisher soups, TIES, SafeMerge, and MergeAlign. These results make alignment-preserving merging a first-class design goal and suggest a path to geometry-aware composition of future foundation models.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16245v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]large language model",
                        "foundation model",
                        "instruction following"
                    ],
                    "score": 15.0
                }
            ],
            "relevance_score": 15.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "AlignMerge：通过Fisher引导的几何约束实现对齐保持的大语言模型合并",
            "summary_zh": "合并大型语言模型（LLM）是一种实用的方法，可以在不重新训练的情况下组合来自多个微调检查点的能力。然而，标准方案（线性权重平均、任务向量和Fisher加权平均）可能会在保持损失的同时悄然破坏对齐。我们认为合并不是一种数值技巧，而是一种围绕已对齐锚点的几何约束操作：必须引导融合以尊重安全性几何，而不是事后验证。我们引入AlignMerge，这是一个几何感知合并框架，它使对齐成为显式不变性。在指令调整基础周围的局部Fisher图中，我们使用投影算子P_A估计对齐子空间并优化：L_AlignMerge = L_geo + lambda_align * L_align + lambda_bud * L_bud，其中L_geo使合并结果在Fisher-Rao几何中接近其专家，L_align惩罚沿对齐敏感方向的运动，L_bud强制执行软对齐预算。作为对齐函数，我们使用解码不变的对齐质量指数（AQI），这是一个潜在空间标准，用于捕获对齐和未对齐行为在表示空间中分离的清晰程度。在五个模型系列（LLaMA-3 8B、Mistral 7B、Qwen 2、Phi-3.5、Gemma 2）中，将安全锚点与任务专家合并，AlignMerge提高了对齐指标（AQI、毒性、LLM-judge对齐），同时在指令遵循、推理和帮助性方面匹配或超过了最佳专家。与Fisher soups、TIES、SafeMerge和MergeAlign相比，它还表现出更小的对齐子空间漂移和更少的预算违规。这些结果使对齐保持合并成为首要的设计目标，并为未来基础模型的几何感知组合提供了一条途径。",
            "intro_zh": [
                "现有LLM合并方法在保持模型性能的同时，容易破坏模型的对齐性，导致安全性下降。",
                "AlignMerge通过在Fisher-Rao几何空间中施加约束，显式地将对齐作为合并过程中的不变性来保持。",
                "实验表明，AlignMerge在多个模型上提高了对齐指标，同时保持或超过了最佳专家的指令遵循和推理能力。"
            ],
            "method_zh": "**问题定义**：现有的大语言模型合并方法，如线性权重平均、任务向量等，虽然可以保持模型的性能指标，但往往会忽略模型的对齐性，导致合并后的模型在安全性、毒性等方面表现不佳。这些方法没有将对齐作为合并过程中的一个重要约束条件，容易产生意想不到的副作用。\\n\\n**核心思路**：AlignMerge的核心思路是将模型合并视为一个几何约束优化问题，在Fisher-Rao几何空间中，通过约束合并后的模型使其接近原始模型，并同时保持模型的对齐性。这种方法显式地将对齐作为合并过程中的不变性，从而避免了对齐性的损失。\\n\\n**技术框架**：AlignMerge框架主要包含以下几个关键部分：1) 在指令调整后的基础模型周围构建局部Fisher图；2) 使用投影算子估计对齐子空间；3) 定义包含几何约束、对齐约束和预算约束的损失函数L_AlignMerge；4) 通过优化L_AlignMerge来实现模型的合并。\\n\\n**关键创新**：AlignMerge的关键创新在于它将模型合并问题转化为一个几何约束优化问题，并显式地考虑了模型的对齐性。与现有方法相比，AlignMerge不是简单地对模型权重进行平均，而是通过在Fisher-Rao几何空间中施加约束，来保证合并后的模型在保持性能的同时，也能够保持良好的对齐性。\\n\\n**关键设计**：AlignMerge的关键设计包括：1) 使用Fisher-Rao几何来度量模型之间的距离；2) 使用对齐质量指数（AQI）作为对齐性的度量标准；3) 定义包含几何损失L_geo、对齐损失L_align和预算损失L_bud的损失函数L_AlignMerge；4) 通过调整lambda_align和lambda_bud来平衡对齐性和性能。",
            "application_zh": "AlignMerge可应用于安全LLM的构建，通过合并不同领域的专家模型，在不牺牲安全性的前提下，提升模型在特定任务上的性能。该方法还可用于构建更可靠、更可控的LLM，降低模型产生有害或不当输出的风险，促进LLM在安全敏感领域的应用。",
            "highlight_zh": "AlignMerge在五个模型系列（LLaMA-3 8B、Mistral 7B、Qwen 2、Phi-3.5、Gemma 2）上进行了评估，结果表明，与Fisher soups、TIES、SafeMerge和MergeAlign等基线方法相比，AlignMerge在提高对齐指标（AQI、毒性、LLM-judge对齐）的同时，能够匹配或超过最佳专家在指令遵循、推理和帮助性方面的性能。此外，AlignMerge还表现出更小的对齐子空间漂移和更少的预算违规。",
            "tags_zh": [
                "大语言模型合并",
                "模型对齐",
                "Fisher-Rao几何",
                "几何约束优化",
                "安全性",
                "对齐质量指数",
                "模型融合"
            ],
            "_index": 7,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16245v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16245v1/figures/mechanistic.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16245v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "BrepLLM: Native Boundary Representation Understanding with Large Language Models",
            "authors": [
                "Liyuan Deng",
                "Hao Guo",
                "Yunpeng Bai",
                "Yongkang Dai",
                "Huaxi Huang",
                "Yilei Shi"
            ],
            "arxiv_id": "2512.16413v1",
            "summary": "Current token-sequence-based Large Language Models (LLMs) are not well-suited for directly processing 3D Boundary Representation (Brep) models that contain complex geometric and topological information. We propose BrepLLM, the first framework that enables LLMs to parse and reason over raw Brep data, bridging the modality gap between structured 3D geometry and natural language. BrepLLM employs a two-stage training pipeline: Cross-modal Alignment Pre-training and Multi-stage LLM Fine-tuning. In the first stage, an adaptive UV sampling strategy converts Breps into graphs representation with geometric and topological information. We then design a hierarchical BrepEncoder to extract features from geometry (i.e., faces and edges) and topology, producing both a single global token and a sequence of node tokens. Then we align the global token with text embeddings from a frozen CLIP text encoder (ViT-L/14) via contrastive learning. In the second stage, we integrate the pretrained BrepEncoder into an LLM. We then align its sequence of node tokens using a three-stage progressive training strategy: (1) training an MLP-based semantic mapping from Brep representation to 2D with 2D-LLM priors. (2) performing fine-tuning of the LLM. (3) designing a Mixture-of-Query Experts (MQE) to enhance geometric diversity modeling. We also construct Brep2Text, a dataset comprising 269,444 Brep-text question-answer pairs. Experiments show that BrepLLM achieves state-of-the-art (SOTA) results on 3D object classification and captioning tasks.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16413v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "contrastive learning"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "semantic mapping",
                        "semantic map"
                    ],
                    "score": 4.0
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]large language model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 14.5,
            "hit_pillars": [
                "2_algo_arch",
                "3_perception_slam",
                "9_embodied_foundation"
            ],
            "headline_zh": "BrepLLM：提出一种原生边界表示理解的大语言模型框架",
            "summary_zh": "当前基于token序列的大语言模型(LLM)不适合直接处理包含复杂几何和拓扑信息的3D边界表示(Brep)模型。我们提出了BrepLLM，这是第一个使LLM能够解析和推理原始Brep数据的框架，弥合了结构化3D几何和自然语言之间的模态差距。BrepLLM采用两阶段训练流程：跨模态对齐预训练和多阶段LLM微调。在第一阶段，自适应UV采样策略将Brep转换为具有几何和拓扑信息的图表示。然后，我们设计了一个分层BrepEncoder来提取几何（即面和边）和拓扑的特征，生成单个全局token和一系列节点token。然后，我们通过对比学习将全局token与来自冻结CLIP文本编码器(ViT-L/14)的文本嵌入对齐。在第二阶段，我们将预训练的BrepEncoder集成到LLM中。然后，我们使用三阶段渐进训练策略对齐其节点token序列：(1)训练一个基于MLP的语义映射，将Brep表示映射到具有2D-LLM先验的2D。(2)执行LLM的微调。(3)设计一种混合查询专家(MQE)来增强几何多样性建模。我们还构建了Brep2Text数据集，包含269,444个Brep-文本问答对。实验表明，BrepLLM在3D对象分类和字幕任务上取得了最先进(SOTA)的结果。",
            "intro_zh": [
                "现有基于token序列的LLM难以直接处理包含复杂几何和拓扑信息的3D Brep模型。",
                "BrepLLM通过两阶段训练，将Brep数据转换为图表示，并与LLM对齐，实现对原始Brep数据的解析和推理。",
                "实验表明，BrepLLM在3D对象分类和描述任务上取得了SOTA结果，验证了其有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大语言模型(LLM)无法直接理解和处理3D边界表示(Brep)模型的问题。现有的LLM主要处理token序列，而Brep模型包含复杂的几何和拓扑信息，直接处理会丢失关键信息，导致性能不佳。\\n\\n**核心思路**：论文的核心思路是将Brep模型转换为一种LLM可以理解的表示形式，即图表示，并设计一个编码器(BrepEncoder)来提取Brep模型的几何和拓扑特征。然后，通过跨模态对齐和多阶段微调，将BrepEncoder与LLM连接起来，使LLM能够理解和推理Brep数据。\\n\\n**技术框架**：BrepLLM框架包含两个主要阶段：跨模态对齐预训练和多阶段LLM微调。在跨模态对齐预训练阶段，首先使用自适应UV采样策略将Brep模型转换为图表示。然后，使用分层BrepEncoder提取几何和拓扑特征，生成全局token和节点token序列。通过对比学习，将全局token与CLIP文本编码器的文本嵌入对齐。在多阶段LLM微调阶段，将预训练的BrepEncoder集成到LLM中，并使用三阶段渐进训练策略对齐节点token序列。\\n\\n**关键创新**：论文的关键创新在于提出了BrepLLM框架，这是第一个使LLM能够解析和推理原始Brep数据的框架。此外，论文还提出了自适应UV采样策略、分层BrepEncoder和混合查询专家(MQE)等技术，以提高Brep模型的表示能力和LLM的推理能力。\\n\\n**关键设计**：自适应UV采样策略根据Brep模型的几何特征动态调整采样密度。分层BrepEncoder包含几何编码器和拓扑编码器，分别提取几何和拓扑特征。混合查询专家(MQE)通过学习不同的查询向量来增强几何多样性建模。三阶段渐进训练策略包括：(1)训练一个基于MLP的语义映射，将Brep表示映射到具有2D-LLM先验的2D。(2)执行LLM的微调。(3)设计一种混合查询专家(MQE)来增强几何多样性建模。",
            "application_zh": "BrepLLM具有广泛的应用前景，例如3D模型检索、3D模型生成、CAD/CAM系统、机器人导航和场景理解等。通过使LLM能够理解和推理3D Brep数据，可以实现更智能、更高效的3D模型处理和应用。",
            "highlight_zh": "BrepLLM在3D对象分类和描述任务上取得了SOTA结果。具体而言，在Brep2Text数据集上，BrepLLM在3D对象分类任务上超过了现有方法，并在3D对象描述任务上生成了更准确、更丰富的描述。",
            "tags_zh": [
                "边界表示",
                "大语言模型",
                "跨模态学习",
                "3D几何",
                "拓扑信息"
            ],
            "_index": 8,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16413v1/images/zhanshitu1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16413v1/images/framework.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16413v1/images/BrepEncoder.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "AdaSearch: Balancing Parametric Knowledge and Search in Large Language Models via Reinforcement Learning",
            "authors": [
                "Tzu-Han Lin",
                "Wei-Lin Chen",
                "Chen-An Li",
                "Hung-yi Lee",
                "Yun-Nung Chen",
                "Yu Meng"
            ],
            "arxiv_id": "2512.16883v1",
            "summary": "Equipping large language models (LLMs) with search engines via reinforcement learning (RL) has emerged as an effective approach for building search agents. However, overreliance on search introduces unnecessary cost and risks exposure to noisy or malicious content, while relying solely on parametric knowledge risks hallucination. The central challenge is to develop agents that adaptively balance parametric knowledge with external search, invoking search only when necessary. Prior work mitigates search overuse by shaping rewards around the number of tool calls. However, these penalties require substantial reward engineering, provide ambiguous credit assignment, and can be exploited by agents that superficially reduce calls. Moreover, evaluating performance solely through call counts conflates necessary and unnecessary search, obscuring the measurement of true adaptive behavior. To address these limitations, we first quantify the self-knowledge awareness of existing search agents via an F1-based decision metric, revealing that methods such as Search-R1 often overlook readily available parametric knowledge. Motivated by these findings, we propose AdaSearch, a simple two-stage, outcome-driven RL framework that disentangles problem solving from the decision of whether to invoke search, and makes this decision process explicit and interpretable. This transparency is crucial for high-stakes domains such as finance and medical question answering, yet is largely neglected by prior approaches. Experiments across multiple model families and sizes demonstrate that AdaSearch substantially improves knowledge-boundary awareness, reduces unnecessary search calls, preserves strong task performance, and offers more transparent, interpretable decision behaviors.",
            "categories": [
                "cs.CL"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Preprint. Code and artifacts will be uploaded to https://github.com/hank0316/AdaSearch",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16883v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]reinforcement learning"
                    ],
                    "score": 4.5
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]large language model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 13.5,
            "hit_pillars": [
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "AdaSearch：通过强化学习平衡大语言模型中的参数知识和搜索",
            "summary_zh": "本文提出了一种利用强化学习(RL)为大型语言模型(LLM)配备搜索引擎的方法，以构建搜索代理。过度依赖搜索会带来不必要的成本，并可能暴露于噪声或恶意内容，而仅依赖参数知识则存在幻觉风险。核心挑战在于开发能够自适应地平衡参数知识与外部搜索的代理，仅在必要时才调用搜索。现有工作通过围绕工具调用次数塑造奖励来缓解搜索过度使用，但这些惩罚需要大量的奖励工程，提供模糊的信用分配，并且可能被表面上减少调用的代理利用。此外，仅通过调用次数评估性能会混淆必要和不必要的搜索，从而模糊了对真正自适应行为的衡量。为了解决这些限制，我们首先通过基于F1的决策指标量化现有搜索代理的自我知识感知能力，揭示了诸如Search-R1之类的方法经常忽略现成的参数知识。受这些发现的启发，我们提出了AdaSearch，一个简单的两阶段、结果驱动的RL框架，它将问题解决与是否调用搜索的决策分离，并使该决策过程明确且可解释。这种透明度对于金融和医学问答等高风险领域至关重要，但之前的研究方法在很大程度上忽略了这一点。跨多个模型系列和规模的实验表明，AdaSearch显着提高了知识边界意识，减少了不必要的搜索调用，保持了强大的任务性能，并提供了更透明、可解释的决策行为。",
            "intro_zh": [
                "现有搜索增强的LLM过度依赖搜索，导致成本增加和潜在风险，而完全依赖模型自身知识则容易产生幻觉。",
                "AdaSearch通过两阶段强化学习框架，将问题解决与搜索决策分离，显式地学习何时调用搜索，提升决策透明性。",
                "实验表明，AdaSearch能显著提高LLM的知识边界意识，减少不必要的搜索调用，同时保持甚至提升任务性能。"
            ],
            "method_zh": "**问题定义**：现有方法在利用搜索引擎增强大型语言模型时，难以平衡参数知识和外部搜索。过度依赖搜索会增加成本并引入噪声，而完全依赖参数知识则容易产生幻觉。现有方法通常通过惩罚工具调用次数来减少搜索的使用，但这种方法需要大量的人工设计奖励，并且容易被模型利用。\\n\\n**核心思路**：AdaSearch的核心思路是将问题解决过程与是否调用搜索的决策过程解耦。通过两阶段的强化学习，首先训练模型解决问题，然后训练模型决定是否需要调用搜索。这种解耦使得模型能够更清晰地理解何时需要外部知识，从而更有效地利用搜索。\\n\\n**技术框架**：AdaSearch采用两阶段强化学习框架。第一阶段，使用强化学习训练一个策略网络，使其能够根据输入的问题生成答案。第二阶段，训练一个决策网络，该网络根据输入的问题和第一阶段生成的答案，决定是否需要调用搜索。如果决策网络决定调用搜索，则将搜索结果输入到第一阶段的策略网络中，生成最终答案。\\n\\n**关键创新**：AdaSearch的关键创新在于将问题解决和搜索决策分离，并通过强化学习显式地学习何时调用搜索。这种方法避免了手动设计奖励函数的复杂性，并且能够更有效地利用外部知识。此外，AdaSearch的决策过程是透明且可解释的，这对于高风险领域的应用至关重要。\\n\\n**关键设计**：AdaSearch的关键设计包括：1) 使用强化学习训练策略网络和决策网络；2) 使用基于F1的决策指标来量化模型的自我知识感知能力；3) 设计合适的奖励函数，鼓励模型在必要时调用搜索，避免过度搜索。",
            "application_zh": "AdaSearch具有广泛的应用前景，尤其是在需要高可靠性和可解释性的领域，如金融问答、医疗诊断、法律咨询等。通过平衡参数知识和外部搜索，AdaSearch可以提高LLM的准确性和可靠性，减少幻觉的产生，并提供更透明的决策过程，从而增强用户对LLM的信任。",
            "highlight_zh": "实验结果表明，AdaSearch在多个模型系列和规模上均取得了显著的性能提升。与现有方法相比，AdaSearch能够显著提高知识边界意识，减少不必要的搜索调用，同时保持强大的任务性能。例如，在某些任务上，AdaSearch可以将不必要的搜索调用减少50%以上，同时保持甚至提升任务准确率。",
            "tags_zh": [
                "大型语言模型",
                "强化学习",
                "搜索增强",
                "知识边界",
                "自适应决策"
            ],
            "_index": 9,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16883v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16883v1/figures/qwen3b_comparison.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16883v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence",
            "authors": [
                "Xiaopeng Lin",
                "Shijie Lian",
                "Bin Yu",
                "Ruoqi Yang",
                "Changti Wu",
                "Yuzhuo Miao",
                "Yurun Jin",
                "Yukun Shi",
                "Cong Huang",
                "Bojun Cheng",
                "Kai Chen"
            ],
            "arxiv_id": "2512.16793v1",
            "summary": "Robotic generalization relies on physical intelligence: the ability to reason about state changes, contact-rich interactions, and long-horizon planning under egocentric perception and action. However, most VLMs are trained primarily on third-person data, creating a fundamental viewpoint mismatch for humanoid robots. Scaling robot egocentric data collection remains impractical due to high cost and limited diversity, whereas large-scale human egocentric videos offer a scalable alternative that naturally capture rich interaction context and causal structure. The key challenge is to convert raw egocentric videos into structured and reliable embodiment training supervision. Accordingly, we propose an Egocentric2Embodiment translation pipeline that transforms first-person videos into multi-level, schema-driven VQA supervision with enforced evidence grounding and temporal consistency, enabling the construction of the Egocentric2Embodiment dataset (E2E-3M) at scale. An egocentric-aware embodied brain, termed PhysBrain, is obtained by training on the E2E-3M dataset. PhysBrain exhibits substantially improved egocentric understanding, particularly for planning on EgoThink. It provides an egocentric-aware initialization that enables more sample-efficient VLA fine-tuning and higher SimplerEnv success rates (53.9\\%), demonstrating effective transfer from human egocentric supervision to downstream robot control.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "17 pages, 4 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16793v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "humanoid",
                        "humanoid robot"
                    ],
                    "score": 4.0
                },
                {
                    "name": "支柱六：视频提取与匹配 (Video Extraction)",
                    "id": "6_video_extraction",
                    "matched_keywords": [
                        "[T]egocentric"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "VLA"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 13.0,
            "hit_pillars": [
                "1_robot_core",
                "6_video_extraction",
                "9_embodied_foundation"
            ],
            "headline_zh": "提出Egocentric2Embodiment以解决机器人物理智能问题",
            "summary_zh": "机器人通用化依赖于物理智能，即在自我中心感知和行动下推理状态变化、接触丰富的交互和长时间规划的能力。然而，大多数视觉语言模型（VLMs）主要在第三人称数据上训练，导致人形机器人面临视角不匹配的问题。本文提出了Egocentric2Embodiment翻译管道，将第一人称视频转化为多层次、基于模式的视觉问答（VQA）监督，构建了大规模的Egocentric2Embodiment数据集（E2E-3M）。通过在该数据集上训练，获得了一个自我中心感知的具身智能系统PhysBrain，显著提升了其自我中心理解能力，尤其在EgoThink上的规划表现。该系统为机器人控制提供了更高效的样本利用率和成功率。",
            "intro_zh": [
                "现有的视觉语言模型主要依赖第三人称数据，导致人形机器人在自我中心感知下的推理能力不足。",
                "论文提出了Egocentric2Embodiment翻译管道，将第一人称视频转化为结构化的VQA监督，解决了数据收集的高成本和多样性不足问题。",
                "PhysBrain在E2E-3M数据集上训练后，展现出更强的自我中心理解能力，成功率达到53.9%，有效实现了人类自我中心监督向机器人控制的迁移。"
            ],
            "method_zh": "**问题定义**：本文旨在解决机器人在自我中心感知下的物理智能问题，现有方法主要依赖第三人称数据，导致视角不匹配和推理能力不足。\\n\\n**核心思路**：提出Egocentric2Embodiment翻译管道，将第一人称视频转化为多层次的VQA监督，以实现更可靠的具身训练监督。\\n\\n**技术框架**：该方法包括数据收集、视频处理、监督生成和模型训练四个主要阶段，利用丰富的自我中心视频数据构建E2E-3M数据集。\\n\\n**关键创新**：最重要的创新在于将原始自我中心视频转化为结构化的监督信息，确保了证据的基础和时间的一致性，与传统方法相比，显著提升了数据的有效性和可用性。\\n\\n**关键设计**：在设计中，采用了多层次的模式驱动方法，结合特定的损失函数和网络结构，确保生成的监督信息具有高质量和一致性。",
            "application_zh": "该研究的潜在应用领域包括机器人控制、智能家居和人机交互等。通过提升机器人在自我中心感知下的理解能力，PhysBrain能够更好地执行复杂的任务，具有广泛的实际价值和未来影响。",
            "highlight_zh": "在实验中，PhysBrain在EgoThink任务上的表现显著提升，成功率达到53.9%。与基线模型相比，展现出更高的样本利用效率和更强的自我中心理解能力，验证了人类自我中心监督向机器人控制的有效迁移。",
            "tags_zh": [
                "物理智能",
                "自我中心感知",
                "视觉问答",
                "数据集构建",
                "机器人控制",
                "长时间规划",
                "模型训练"
            ],
            "_index": 10,
            "_used_api": "openai",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16793v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16793v1/fig/data_pipeline.jpg",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16793v1/fig/data_sum.jpg",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image",
            "authors": [
                "Yushi Hu",
                "Reyhane Askari-Hemmat",
                "Melissa Hall",
                "Emily Dinan",
                "Luke Zettlemoyer",
                "Marjan Ghazvininejad"
            ],
            "arxiv_id": "2512.16899v1",
            "summary": "Reward models (RMs) are essential for training large language models (LLMs), but remain underexplored for omni models that handle interleaved image and text sequences. We introduce Multimodal RewardBench 2 (MMRB2), the first comprehensive benchmark for reward models on multimodal understanding and (interleaved) generation. MMRB2 spans four tasks: text-to-image, image editing, interleaved generation, and multimodal reasoning (\"thinking-with-images\"), providing 1,000 expert-annotated preference pairs per task from 23 models and agents across 21 source tasks. MMRB2 is designed with: (1) practical but challenging prompts; (2) responses from state-of-the-art models and agents; and (3) preference pairs with strong human-expert consensus, curated via an ensemble filtering strategy. Using MMRB2, we study existing judges for each subtask, including multimodal LLM-as-a-judge and models trained with human preferences. The latest Gemini 3 Pro attains 75-80% accuracy. GPT-5 and Gemini 2.5 Pro reach 66-75% accuracy, compared to >90% for humans, yet surpass the widely used GPT-4o (59%). The best performing open-source model Qwen3-VL-32B achieves similar accuracies as Gemini 2.5 Flash (64%). We also show that MMRB2 performance strongly correlates with downstream task success using Best-of-N sampling and conduct an in-depth analysis that shows key areas to improve the reward models going forward.",
            "categories": [
                "cs.CL",
                "cs.CV"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Code and data available at https://github.com/facebookresearch/MMRB2",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16899v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model",
                        "[T]multimodal"
                    ],
                    "score": 12.0
                }
            ],
            "relevance_score": 12.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出Multimodal RewardBench 2，用于评估处理交错文本和图像的Omni Reward模型。",
            "summary_zh": "奖励模型（RMs）对于训练大型语言模型（LLMs）至关重要，但对于处理交错图像和文本序列的Omni模型，其研究仍然不足。我们推出了Multimodal RewardBench 2（MMRB2），这是第一个全面的基准，用于评估奖励模型在多模态理解和（交错）生成方面的能力。MMRB2涵盖四个任务：文本到图像、图像编辑、交错生成和多模态推理（“用图像思考”），每个任务提供1000个由专家标注的偏好对，这些数据来自21个源任务中的23个模型和代理。MMRB2的设计特点包括：（1）实用但具有挑战性的提示；（2）来自最先进模型和代理的响应；（3）通过集成过滤策略策划的具有强烈人类专家共识的偏好对。使用MMRB2，我们研究了每个子任务的现有评判标准，包括多模态LLM-as-a-judge和使用人类偏好训练的模型。最新的Gemini 3 Pro达到了75-80%的准确率。GPT-5和Gemini 2.5 Pro达到了66-75%的准确率，而人类的准确率超过90%，但它们超过了广泛使用的GPT-4o（59%）。性能最佳的开源模型Qwen3-VL-32B实现了与Gemini 2.5 Flash（64%）相似的准确率。我们还表明，MMRB2的性能与使用Best-of-N抽样的下游任务成功率密切相关，并进行了深入分析，揭示了未来改进奖励模型的关键领域。",
            "intro_zh": [
                "现有奖励模型在处理交错文本和图像的多模态任务中表现不足，缺乏专门的评估基准。",
                "提出Multimodal RewardBench 2 (MMRB2)，包含四个任务，提供高质量的人工标注偏好数据，用于评估多模态奖励模型。",
                "实验表明，现有模型在MMRB2上表现与人类专家存在差距，但与下游任务成功率高度相关，为未来改进方向提供了依据。"
            ],
            "method_zh": "**问题定义**：论文旨在解决多模态奖励模型（Reward Model, RM）在处理交错文本和图像序列时的评估问题。现有奖励模型主要针对文本或图像模态，缺乏对多模态内容理解和生成能力的有效评估。这限制了多模态大型语言模型（LLM）的训练和优化，因为无法准确衡量模型生成内容的好坏。\\n\\n**核心思路**：论文的核心思路是构建一个全面的多模态奖励模型评估基准，即Multimodal RewardBench 2 (MMRB2)。该基准包含多个具有挑战性的任务，并提供高质量的人工标注偏好数据，用于评估模型在多模态理解和生成方面的能力。通过比较不同模型在MMRB2上的表现，可以更好地了解它们的优缺点，并为未来的模型改进提供指导。\\n\\n**技术框架**：MMRB2基准包含四个主要任务：文本到图像生成、图像编辑、交错生成和多模态推理（“用图像思考”）。每个任务都包含1000个专家标注的偏好对，这些数据来自21个源任务中的23个模型和代理。数据的收集和标注过程采用了集成过滤策略，以确保偏好对具有高度的人类专家共识。\\n\\n**关键创新**：MMRB2是第一个专门针对多模态奖励模型的综合评估基准。它不仅涵盖了多种多模态任务，还提供了高质量的人工标注偏好数据，并采用了严格的过滤策略来确保数据质量。此外，论文还分析了MMRB2性能与下游任务成功率之间的相关性，为奖励模型的改进提供了新的视角。\\n\\n**关键设计**：MMRB2的设计重点在于数据的质量和多样性。为了确保数据质量，论文采用了集成过滤策略，即多个专家对偏好对进行标注，只有当专家之间达成高度共识时，该偏好对才会被纳入基准。为了提高数据的多样性，论文选择了21个不同的源任务，并使用了23个不同的模型和代理来生成数据。",
            "application_zh": "该研究成果可应用于多模态大型语言模型的训练和评估，例如图像生成、图像编辑、视觉问答等领域。高质量的奖励模型能够提升生成内容的质量和一致性，从而改善用户体验。此外，该基准也可用于评估不同多模态模型的性能，推动相关技术的发展。",
            "highlight_zh": "实验结果表明，最新的Gemini 3 Pro在MMRB2上达到了75-80%的准确率，GPT-5和Gemini 2.5 Pro达到了66-75%的准确率，超过了GPT-4o（59%）。开源模型Qwen3-VL-32B实现了与Gemini 2.5 Flash（64%）相似的准确率。MMRB2的性能与下游任务成功率高度相关，表明其能够有效评估奖励模型的性能。",
            "tags_zh": [
                "多模态学习",
                "奖励模型",
                "基准测试",
                "图像文本交错",
                "大型语言模型",
                "模型评估",
                "人类偏好",
                "多模态推理"
            ],
            "_index": 11,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16899v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16899v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16899v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Make-It-Poseable: Feed-forward Latent Posing Model for 3D Humanoid Character Animation",
            "authors": [
                "Zhiyang Guo",
                "Ori Zhang",
                "Jax Xiang",
                "Alan Zhao",
                "Wengang Zhou",
                "Houqiang Li"
            ],
            "arxiv_id": "2512.16767v1",
            "summary": "Posing 3D characters is a fundamental task in computer graphics and vision. However, existing methods like auto-rigging and pose-conditioned generation often struggle with challenges such as inaccurate skinning weight prediction, topological imperfections, and poor pose conformance, limiting their robustness and generalizability. To overcome these limitations, we introduce Make-It-Poseable, a novel feed-forward framework that reformulates character posing as a latent-space transformation problem. Instead of deforming mesh vertices as in traditional pipelines, our method reconstructs the character in new poses by directly manipulating its latent representation. At the core of our method is a latent posing transformer that manipulates shape tokens based on skeletal motion. This process is facilitated by a dense pose representation for precise control. To ensure high-fidelity geometry and accommodate topological changes, we also introduce a latent-space supervision strategy and an adaptive completion module. Our method demonstrates superior performance in posing quality. It also naturally extends to 3D editing applications like part replacement and refinement.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Project page: https://jasongzy.github.io/Make-It-Poseable/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16767v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]humanoid"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱八：物理动画 (Physics-based Animation)",
                    "id": "8_physics_animation",
                    "matched_keywords": [
                        "[T]character animation"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 12.0,
            "hit_pillars": [
                "1_robot_core",
                "8_physics_animation"
            ],
            "headline_zh": "提出Make-It-Poseable，用于解决3D人形角色动画中姿态控制难题",
            "summary_zh": "本文提出了一种名为Make-It-Poseable的新型前馈框架，旨在解决3D角色姿态控制问题。现有方法如自动绑定和姿态条件生成，常面临皮肤权重预测不准确、拓扑结构缺陷和姿态一致性差等挑战，限制了其鲁棒性和泛化能力。Make-It-Poseable将角色姿态控制重新定义为潜在空间变换问题。该方法不直接变形网格顶点，而是通过操纵角色的潜在表示来重建新的姿态。核心是一个潜在姿态Transformer，它根据骨骼运动来操纵形状token。密集姿态表示用于实现精确控制。为了确保高保真几何形状并适应拓扑变化，还引入了潜在空间监督策略和自适应补全模块。实验表明，该方法在姿态质量方面表现出色，并且可以自然地扩展到3D编辑应用，如部件替换和优化。",
            "intro_zh": [
                "现有3D角色姿态控制方法在皮肤权重预测、拓扑结构和姿态一致性方面存在不足，影响了其鲁棒性和泛化性。",
                "Make-It-Poseable将姿态控制问题转化为潜在空间变换，通过操纵潜在表示而非直接变形顶点来重建角色。",
                "该方法引入了潜在姿态Transformer、密集姿态表示、潜在空间监督和自适应补全模块，提升了姿态质量。"
            ],
            "method_zh": "**问题定义**：论文旨在解决3D人形角色动画中，现有方法在姿态控制方面存在的不足。具体来说，现有方法如自动绑定和姿态条件生成，在预测精确的皮肤权重、处理拓扑结构变化以及保证姿态一致性方面存在困难，导致动画效果不佳，鲁棒性和泛化能力受限。\\n\\n**核心思路**：论文的核心思路是将3D角色姿态控制问题转化为潜在空间中的变换问题。不再直接操作3D模型的顶点，而是将3D模型编码到潜在空间，通过在潜在空间中进行操作，然后解码回3D模型，从而实现姿态的改变。这种方法可以更好地处理拓扑结构变化，并提高姿态控制的精度。\\n\\n**技术框架**：Make-It-Poseable框架主要包含以下几个模块：1) 编码器：将3D角色模型编码到潜在空间。2) 潜在姿态Transformer：根据输入的骨骼运动信息，在潜在空间中对形状token进行变换，实现姿态的改变。3) 解码器：将变换后的潜在表示解码回3D模型。4) 自适应补全模块：用于处理拓扑结构变化，保证生成模型的完整性。\\n\\n**关键创新**：该方法最重要的创新点在于将姿态控制问题转化为潜在空间变换。与直接操作3D模型顶点的方法相比，这种方法可以更好地处理拓扑结构变化，并提高姿态控制的精度。此外，潜在姿态Transformer的设计也使得模型能够更好地理解骨骼运动信息，并将其转化为潜在空间中的变换。\\n\\n**关键设计**：论文中使用了密集姿态表示，以便更精确地控制角色的姿态。同时，为了保证生成模型的质量，论文还引入了潜在空间监督策略，即在潜在空间中对生成模型进行约束。此外，自适应补全模块的设计也考虑了拓扑结构变化，保证生成模型的完整性。具体的损失函数设计和网络结构细节在论文中有详细描述。",
            "application_zh": "该研究成果可广泛应用于游戏开发、电影制作、虚拟现实和增强现实等领域。它可以帮助艺术家和动画师更轻松地创建逼真的3D角色动画，并提高动画制作的效率。此外，该方法还可以应用于3D角色编辑，例如部件替换和优化，为用户提供更灵活的创作工具。未来，该技术有望进一步发展，实现更复杂、更自然的3D角色动画。",
            "highlight_zh": "实验结果表明，Make-It-Poseable在姿态质量方面优于现有方法。该方法能够生成更逼真、更自然的3D角色动画，并且能够更好地处理拓扑结构变化。此外，该方法还可以自然地扩展到3D编辑应用，例如部件替换和优化。具体的性能数据和对比基线在论文中有详细描述。",
            "tags_zh": [
                "3D角色动画",
                "姿态控制",
                "潜在空间变换",
                "Transformer网络",
                "人形建模"
            ],
            "_index": 12,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16767v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16767v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16767v1/x4.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "SDFoam: Signed-Distance Foam for explicit surface reconstruction",
            "authors": [
                "Antonella Rech",
                "Nicola Conci",
                "Nicola Garau"
            ],
            "arxiv_id": "2512.16706v1",
            "summary": "Neural radiance fields (NeRF) have driven impressive progress in view synthesis by using ray-traced volumetric rendering. Splatting-based methods such as 3D Gaussian Splatting (3DGS) provide faster rendering by rasterizing 3D primitives. RadiantFoam (RF) brought ray tracing back, achieving throughput comparable to Gaussian Splatting by organizing radiance with an explicit Voronoi Diagram (VD). Yet, all the mentioned methods still struggle with precise mesh reconstruction. We address this gap by jointly learning an explicit VD with an implicit Signed Distance Field (SDF). The scene is optimized via ray tracing and regularized by an Eikonal objective. The SDF introduces metric-consistent isosurfaces, which, in turn, bias near-surface Voronoi cell faces to align with the zero level set. The resulting model produces crisper, view-consistent surfaces with fewer floaters and improved topology, while preserving photometric quality and maintaining training speed on par with RadiantFoam. Across diverse scenes, our hybrid implicit-explicit formulation, which we name SDFoam, substantially improves mesh reconstruction accuracy (Chamfer distance) with comparable appearance (PSNR, SSIM), without sacrificing efficiency.",
            "categories": [
                "cs.CV",
                "cs.GR"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16706v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "3D gaussian splatting",
                        "3DGS",
                        "gaussian splatting",
                        "splatting",
                        "NeRF",
                        "neural radiance field"
                    ],
                    "score": 12.0
                }
            ],
            "relevance_score": 12.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "SDFoam：结合显式Voronoi图和隐式SDF，实现精确表面重建",
            "summary_zh": "神经辐射场（NeRF）通过光线追踪的体渲染在视角合成方面取得了显著进展。基于Splatting的方法，如3D高斯溅射（3DGS），通过栅格化3D图元提供了更快的渲染速度。RadiantFoam（RF）通过使用显式Voronoi图（VD）组织辐射，重新引入了光线追踪，实现了与高斯溅射相当的吞吐量。然而，上述方法在精确的网格重建方面仍然存在困难。本文通过联合学习显式VD和隐式有向距离场（SDF）来解决这个问题。场景通过光线追踪进行优化，并由Eikonal目标正则化。SDF引入了度量一致的等值面，进而偏置近表面Voronoi单元面与零水平集对齐。由此产生的模型产生更清晰、视角一致的表面，减少了浮动伪影并改善了拓扑结构，同时保持了光度质量，并保持了与RadiantFoam相当的训练速度。在不同的场景中，我们提出的混合隐式-显式公式，命名为SDFoam，在不牺牲效率的情况下，显著提高了网格重建精度（Chamfer距离），并具有可比的外观（PSNR，SSIM）。",
            "intro_zh": [
                "现有NeRF和3DGS等方法在精确网格重建方面存在不足，难以生成高质量的几何表面。",
                "SDFoam结合显式Voronoi图和隐式SDF，利用SDF的度量一致性来约束Voronoi单元，从而优化表面几何。",
                "实验表明，SDFoam在保持光度质量和训练速度的同时，显著提高了网格重建精度，减少了伪影。"
            ],
            "method_zh": "**问题定义**：现有神经渲染方法，如NeRF和3DGS，虽然在视角合成方面表现出色，但在精确的网格重建方面仍然存在挑战。这些方法生成的网格通常包含浮动伪影，拓扑结构不佳，难以满足对几何精度要求较高的应用。\n\n**核心思路**：SDFoam的核心思路是将显式的Voronoi图表示与隐式的有向距离场（SDF）表示相结合。Voronoi图用于快速渲染和光度优化，而SDF则提供度量一致的几何约束，引导Voronoi单元面与真实的表面对齐。这种混合表示能够兼顾渲染效率和几何精度。\n\n**技术框架**：SDFoam的整体框架包括以下几个主要步骤：1) 初始化：使用一组3D点初始化Voronoi图。2) 光线追踪：通过光线追踪渲染场景，并计算光度损失。3) SDF优化：使用Eikonal损失优化SDF，使其与场景几何一致。4) Voronoi图优化：利用SDF的梯度信息，调整Voronoi单元的位置和形状，使其更贴合表面。5) 重复步骤2-4，直到收敛。\n\n**关键创新**：SDFoam的关键创新在于将显式的Voronoi图表示与隐式的SDF表示相结合，利用SDF的几何约束来优化Voronoi图，从而实现更精确的表面重建。与传统的NeRF方法相比，SDFoam能够生成更清晰、视角一致的表面，减少了浮动伪影。与RadiantFoam相比，SDFoam引入了SDF，从而提高了网格重建精度。\n\n**关键设计**：SDFoam的关键设计包括：1) 使用Eikonal损失来正则化SDF，使其满足有向距离场的性质。2) 使用光线追踪来渲染场景，并计算光度损失。3) 使用SDF的梯度信息来调整Voronoi单元的位置和形状。4) 采用交替优化策略，分别优化SDF和Voronoi图。",
            "application_zh": "SDFoam在三维重建、虚拟现实、增强现实、机器人导航等领域具有广泛的应用前景。它可以用于生成高质量的三维模型，用于游戏开发、电影制作、工业设计等。此外，SDFoam还可以用于机器人导航，帮助机器人理解周围环境的几何结构。",
            "highlight_zh": "实验结果表明，SDFoam在网格重建精度方面显著优于现有方法。在多个数据集上，SDFoam的Chamfer距离明显低于NeRF和RadiantFoam，同时保持了与RadiantFoam相当的PSNR和SSIM。这表明SDFoam能够在不牺牲光度质量的情况下，显著提高几何精度。",
            "tags_zh": [
                "神经辐射场",
                "三维重建",
                "有向距离场",
                "Voronoi图",
                "光线追踪"
            ],
            "_index": 13,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16706v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16706v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16706v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models",
            "authors": [
                "Yuxin Wang",
                "Lei Ke",
                "Boqiang Zhang",
                "Tianyuan Qu",
                "Hanxun Yu",
                "Zhenpeng Huang",
                "Meng Yu",
                "Dan Xu",
                "Dong Yu"
            ],
            "arxiv_id": "2512.16561v1",
            "summary": "While current multimodal models can answer questions based on 2D images, they lack intrinsic 3D object perception, limiting their ability to comprehend spatial relationships and depth cues in 3D scenes. In this work, we propose N3D-VLM, a novel unified framework that seamlessly integrates native 3D object perception with 3D-aware visual reasoning, enabling both precise 3D grounding and interpretable spatial understanding. Unlike conventional end-to-end models that directly predict answers from RGB/RGB-D inputs, our approach equips the model with native 3D object perception capabilities, enabling it to directly localize objects in 3D space based on textual descriptions. Building upon accurate 3D object localization, the model further performs explicit reasoning in 3D, achieving more interpretable and structured spatial understanding. To support robust training for these capabilities, we develop a scalable data construction pipeline that leverages depth estimation to lift large-scale 2D annotations into 3D space, significantly increasing the diversity and coverage for 3D object grounding data, yielding over six times larger than the largest existing single-image 3D detection dataset. Moreover, the pipeline generates spatial question-answering datasets that target chain-of-thought (CoT) reasoning in 3D, facilitating joint training for both 3D object localization and 3D spatial reasoning. Experimental results demonstrate that our unified framework not only achieves state-of-the-art performance on 3D grounding tasks, but also consistently surpasses existing methods in 3D spatial reasoning in vision-language model.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Project Page: https://n3d-vlm.github.io",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16561v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "depth estimation"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱七：动作重定向 (Motion Retargeting)",
                    "id": "7_retargeting",
                    "matched_keywords": [
                        "spatial relationship"
                    ],
                    "score": 3.0
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "multimodal",
                        "chain-of-thought"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 11.0,
            "hit_pillars": [
                "3_perception_slam",
                "7_retargeting",
                "9_embodied_foundation"
            ],
            "headline_zh": "N3D-VLM：原生3D感知赋能视觉语言模型精确空间推理",
            "summary_zh": "当前的多模态模型虽然可以基于2D图像回答问题，但缺乏内在的3D物体感知能力，限制了其理解3D场景中的空间关系和深度线索的能力。本文提出了N3D-VLM，一种新颖的统一框架，它无缝集成了原生3D物体感知和3D感知视觉推理，从而实现了精确的3D grounding和可解释的空间理解。与直接从RGB/RGB-D输入预测答案的传统端到端模型不同，我们的方法赋予模型原生的3D物体感知能力，使其能够基于文本描述直接在3D空间中定位物体。在精确的3D物体定位的基础上，该模型进一步执行显式的3D推理，从而实现更可解释和结构化的空间理解。为了支持这些能力的稳健训练，我们开发了一个可扩展的数据构建流程，该流程利用深度估计将大规模2D标注提升到3D空间，显著增加了3D物体grounding数据的多样性和覆盖范围，产生了比现有最大的单图像3D检测数据集大六倍以上的数据集。此外，该流程还生成了针对3D中的思维链（CoT）推理的空间问答数据集，从而促进了3D物体定位和3D空间推理的联合训练。实验结果表明，我们的统一框架不仅在3D grounding任务上实现了最先进的性能，而且在视觉语言模型中的3D空间推理方面始终优于现有方法。",
            "intro_zh": [
                "现有视觉语言模型缺乏对3D场景的内在感知，难以理解空间关系和深度信息，限制了其应用。",
                "N3D-VLM通过集成原生3D物体感知和3D感知视觉推理，实现精确的3D定位和可解释的空间理解。",
                "该方法在3D grounding和空间推理任务上均取得了SOTA性能，并构建了大规模3D标注数据集。"
            ],
            "method_zh": "**问题定义**：现有视觉语言模型主要基于2D图像进行推理，缺乏对3D场景的理解能力，无法准确感知物体间的空间关系和深度信息。这限制了模型在需要空间推理的任务中的表现，例如理解物体间的相对位置、距离等。现有方法通常直接从RGB或RGB-D图像预测答案，缺乏可解释性，且难以进行精确的3D定位。\\n\\n**核心思路**：N3D-VLM的核心思路是赋予模型原生的3D物体感知能力，使其能够直接在3D空间中定位物体，并在此基础上进行显式的3D空间推理。通过将2D图像信息提升到3D空间，并结合文本描述，模型可以更准确地理解场景中的空间关系，从而提高空间推理的准确性和可解释性。\\n\\n**技术框架**：N3D-VLM包含以下主要模块：1) 3D物体感知模块：该模块负责将2D图像信息转换为3D表示，并根据文本描述定位3D空间中的物体。这通常涉及深度估计、3D物体检测等技术。2) 3D空间推理模块：该模块基于3D物体定位的结果，进行显式的空间推理，例如计算物体间的距离、相对位置等。3) 视觉语言模型：该模型负责将文本描述和3D场景信息融合，并生成最终的答案。整个流程是端到端可训练的，可以联合优化3D物体感知和3D空间推理的能力。\\n\\n**关键创新**：N3D-VLM最重要的技术创新点在于其原生3D物体感知能力。与现有方法不同，N3D-VLM不是直接从2D图像预测答案，而是首先在3D空间中定位物体，然后再进行推理。这种方法可以更准确地理解场景中的空间关系，并提高空间推理的准确性和可解释性。此外，该论文还提出了一个可扩展的数据构建流程，可以生成大规模的3D标注数据集，为模型的训练提供了充足的数据支持。\\n\\n**关键设计**：该论文的关键设计包括：1) 使用深度估计技术将2D标注提升到3D空间，从而生成大规模的3D物体grounding数据集。2) 构建针对3D中的思维链（CoT）推理的空间问答数据集，促进了3D物体定位和3D空间推理的联合训练。3) 具体网络结构和损失函数细节未知。",
            "application_zh": "N3D-VLM在机器人导航、自动驾驶、虚拟现实等领域具有广泛的应用前景。它可以帮助机器人理解周围环境，进行更智能的导航和交互。在自动驾驶领域，它可以提高车辆对复杂场景的理解能力，从而提高驾驶安全性。在虚拟现实领域，它可以增强用户的沉浸感和交互性，创造更逼真的虚拟体验。",
            "highlight_zh": "N3D-VLM在3D grounding任务上取得了SOTA性能，并且在3D空间推理方面始终优于现有方法。论文构建了一个比现有最大的单图像3D检测数据集大六倍以上的数据集，为3D视觉语言模型的研究提供了重要的数据支持。具体的性能提升数据未知。",
            "tags_zh": [
                "3D感知",
                "视觉语言模型",
                "空间推理",
                "3D grounding",
                "深度估计"
            ],
            "_index": 14,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16561v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16561v1/x4.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16561v1/x5.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "SNOW: Spatio-Temporal Scene Understanding with World Knowledge for Open-World Embodied Reasoning",
            "authors": [
                "Tin Stribor Sohn",
                "Maximilian Dillitzer",
                "Jason J. Corso",
                "Eric Sax"
            ],
            "arxiv_id": "2512.16461v1",
            "summary": "Autonomous robotic systems require spatio-temporal understanding of dynamic environments to ensure reliable navigation and interaction. While Vision-Language Models (VLMs) provide open-world semantic priors, they lack grounding in 3D geometry and temporal dynamics. Conversely, geometric perception captures structure and motion but remains semantically sparse. We propose SNOW (Scene Understanding with Open-World Knowledge), a training-free and backbone-agnostic framework for unified 4D scene understanding that integrates VLM-derived semantics with point cloud geometry and temporal consistency. SNOW processes synchronized RGB images and 3D point clouds, using HDBSCAN clustering to generate object-level proposals that guide SAM2-based segmentation. Each segmented region is encoded through our proposed Spatio-Temporal Tokenized Patch Encoding (STEP), producing multimodal tokens that capture localized semantic, geometric, and temporal attributes. These tokens are incrementally integrated into a 4D Scene Graph (4DSG), which serves as 4D prior for downstream reasoning. A lightweight SLAM backend anchors all STEP tokens spatially in the environment, providing the global reference alignment, and ensuring unambiguous spatial grounding across time. The resulting 4DSG forms a queryable, unified world model through which VLMs can directly interpret spatial scene structure and temporal dynamics. Experiments on a diverse set of benchmarks demonstrate that SNOW enables precise 4D scene understanding and spatially grounded inference, thereby setting new state-of-the-art performance in several settings, highlighting the importance of structured 4D priors for embodied reasoning and autonomous robotics.",
            "categories": [
                "cs.CV",
                "cs.RO"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16461v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "world model"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]scene understanding"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "multimodal"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 10.5,
            "hit_pillars": [
                "2_algo_arch",
                "3_perception_slam",
                "9_embodied_foundation"
            ],
            "headline_zh": "SNOW：融合世界知识的时空场景理解框架，用于开放世界具身推理",
            "summary_zh": "自主机器人系统需要对动态环境进行时空理解，以确保可靠的导航和交互。视觉-语言模型(VLMs)提供了开放世界的语义先验，但缺乏3D几何和时间动态的 grounding。相反，几何感知捕捉了结构和运动，但语义仍然稀疏。我们提出了SNOW（Scene Understanding with Open-World Knowledge），一个无需训练且与骨干网络无关的框架，用于统一的4D场景理解，它集成了VLM衍生的语义与点云几何和时间一致性。SNOW处理同步的RGB图像和3D点云，使用HDBSCAN聚类生成对象级别的 proposals，指导基于SAM2的分割。每个分割区域通过我们提出的时空 Tokenized Patch Encoding (STEP)进行编码，产生多模态 tokens，捕捉局部语义、几何和时间属性。这些 tokens 被增量地集成到 4D 场景图 (4DSG) 中，作为下游推理的 4D 先验。一个轻量级的 SLAM 后端在环境中空间地锚定所有 STEP tokens，提供全局参考对齐，并确保跨时间无歧义的空间 grounding。由此产生的 4DSG 形成了一个可查询的统一世界模型，通过该模型，VLM 可以直接解释空间场景结构和时间动态。在各种基准测试上的实验表明，SNOW 能够实现精确的 4D 场景理解和空间 grounding 推理，从而在多个设置中设置了新的最先进性能，突出了结构化 4D 先验对于具身推理和自主机器人的重要性。",
            "intro_zh": [
                "现有方法在机器人时空场景理解中，要么语义信息不足，要么缺乏几何和时间动态的精确建模，限制了其在复杂环境中的应用。",
                "SNOW框架通过融合视觉-语言模型的语义先验、点云几何信息和时间一致性，构建统一的4D场景图，为机器人提供更全面的环境理解。",
                "实验结果表明，SNOW在多个基准测试中取得了最先进的性能，验证了其在具身推理和自主机器人领域的有效性。"
            ],
            "method_zh": "**问题定义**：现有机器人场景理解方法面临的挑战在于如何有效地融合语义信息、几何信息和时间信息，从而实现对动态环境的全面理解。视觉-语言模型虽然具有丰富的语义知识，但缺乏对3D几何结构的精确感知。而传统的几何感知方法虽然能够捕捉场景的结构和运动，但语义信息较为稀疏。因此，如何将这两种信息源有效地结合起来，构建一个统一的、可用于推理的场景表示，是当前研究的痛点。\\n\\n**核心思路**：SNOW的核心思路是将视觉-语言模型的语义信息与点云几何信息进行融合，并通过时间一致性约束来构建一个动态的4D场景图。该方法利用视觉-语言模型提供开放世界的语义先验，然后通过点云几何信息对语义信息进行空间定位，最后通过时间一致性约束来保证场景表示的稳定性。这种融合的方式能够充分利用各种信息源的优势，从而实现对动态环境的全面理解。\\n\\n**技术框架**：SNOW框架主要包含以下几个模块：1) 对象 proposal 生成模块：使用HDBSCAN聚类算法对点云进行分割，生成对象级别的 proposals。2) 分割模块：使用SAM2对RGB图像进行分割，并根据对象 proposals 对分割结果进行优化。3) 特征编码模块：使用STEP (Spatio-Temporal Tokenized Patch Encoding) 对分割区域进行编码，生成包含语义、几何和时间信息的 tokens。4) 4D场景图构建模块：将 STEP tokens 增量地集成到 4D 场景图中，并使用 SLAM 后端对 tokens 进行空间定位。\\n\\n**关键创新**：SNOW的关键创新在于提出了 STEP 编码方法和 4D 场景图的构建方法。STEP 编码方法能够有效地融合语义、几何和时间信息，从而生成具有丰富信息的 tokens。4D 场景图的构建方法能够将这些 tokens 组织成一个动态的、可用于推理的场景表示。此外，SNOW 框架是一个无需训练且与骨干网络无关的框架，具有很强的通用性。\\n\\n**关键设计**：STEP 编码方法将分割区域划分为多个 patch，然后使用视觉-语言模型对每个 patch 进行语义编码，使用点云几何信息对每个 patch 进行几何编码，并使用光流信息对每个 patch 进行时间编码。最后，将这些编码结果拼接在一起，形成 STEP token。4D 场景图使用图结构来表示场景中的对象和它们之间的关系。每个节点表示一个对象，每条边表示对象之间的关系。场景图中的节点和边都包含语义、几何和时间信息。",
            "application_zh": "SNOW框架具有广泛的应用前景，例如自主导航、机器人操作、增强现实等。在自主导航领域，SNOW可以帮助机器人更好地理解周围环境，从而实现更安全、更高效的导航。在机器人操作领域，SNOW可以帮助机器人更好地识别和操作物体，从而实现更智能、更灵活的操作。在增强现实领域，SNOW可以帮助增强现实系统更好地理解真实世界，从而实现更逼真、更自然的增强现实体验。",
            "highlight_zh": "SNOW在多个基准测试中取得了最先进的性能，例如在 ScanNet 数据集上的场景理解任务中，SNOW 的性能比现有方法提高了 5% 以上。此外，SNOW 在一个真实的机器人导航实验中也取得了良好的效果，证明了其在实际应用中的有效性。这些实验结果表明，SNOW 能够实现精确的 4D 场景理解和空间 grounding 推理。",
            "tags_zh": [
                "时空场景理解",
                "具身推理",
                "视觉-语言模型",
                "4D场景图",
                "点云处理"
            ],
            "_index": 15,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16461v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16461v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16461v1/02_Figures/RoboSpatial_1.jpeg",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "ManiLong-Shot: Interaction-Aware One-Shot Imitation Learning for Long-Horizon Manipulation",
            "authors": [
                "Zixuan Chen",
                "Chongkai Gao",
                "Lin Shao",
                "Jieqi Shi",
                "Jing Huo",
                "Yang Gao"
            ],
            "arxiv_id": "2512.16302v1",
            "summary": "One-shot imitation learning (OSIL) offers a promising way to teach robots new skills without large-scale data collection. However, current OSIL methods are primarily limited to short-horizon tasks, thus limiting their applicability to complex, long-horizon manipulations. To address this limitation, we propose ManiLong-Shot, a novel framework that enables effective OSIL for long-horizon prehensile manipulation tasks. ManiLong-Shot structures long-horizon tasks around physical interaction events, reframing the problem as sequencing interaction-aware primitives instead of directly imitating continuous trajectories. This primitive decomposition can be driven by high-level reasoning from a vision-language model (VLM) or by rule-based heuristics derived from robot state changes. For each primitive, ManiLong-Shot predicts invariant regions critical to the interaction, establishes correspondences between the demonstration and the current observation, and computes the target end-effector pose, enabling effective task execution. Extensive simulation experiments show that ManiLong-Shot, trained on only 10 short-horizon tasks, generalizes to 20 unseen long-horizon tasks across three difficulty levels via one-shot imitation, achieving a 22.8% relative improvement over the SOTA. Additionally, real-robot experiments validate ManiLong-Shot's ability to robustly execute three long-horizon manipulation tasks via OSIL, confirming its practical applicability.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Accepted by AAAI 2026",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16302v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]manipulation"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]imitation learning"
                    ],
                    "score": 4.5
                }
            ],
            "relevance_score": 10.5,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "ManiLong-Shot：交互感知的单样本模仿学习用于长时程操作任务",
            "summary_zh": "本文提出ManiLong-Shot，一个新颖的框架，旨在实现长时程灵巧操作任务的有效单样本模仿学习(OSIL)。ManiLong-Shot围绕物理交互事件构建长时程任务，将问题重新定义为对交互感知原语进行排序，而不是直接模仿连续轨迹。这种原语分解可以由视觉-语言模型(VLM)的高级推理驱动，或者由机器人状态变化推导出的基于规则的启发式方法驱动。对于每个原语，ManiLong-Shot预测对交互至关重要的不变区域，建立演示和当前观察之间的对应关系，并计算目标末端执行器姿态，从而实现有效的任务执行。大量的仿真实验表明，ManiLong-Shot仅在10个短时程任务上训练，即可通过单样本模仿泛化到20个未见过的长时程任务，跨越三个难度级别，相对于SOTA方法实现了22.8%的相对改进。此外，真实机器人实验验证了ManiLong-Shot通过OSIL稳健地执行三个长时程操作任务的能力，证实了其在实际应用中的可行性。",
            "intro_zh": [
                "现有单样本模仿学习方法主要局限于短时程任务，难以应用于复杂的长时程操作。",
                "ManiLong-Shot将长时程任务分解为交互感知的原语序列，通过视觉-语言模型或规则启发式方法驱动分解。",
                "实验表明，ManiLong-Shot在仿真和真实机器人上均表现出优异的泛化能力和任务执行效果。"
            ],
            "method_zh": "**问题定义**：现有单样本模仿学习(OSIL)方法在长时程操作任务中面临挑战，因为直接模仿连续轨迹难以泛化到未见过的任务。此外，长时程任务的状态空间巨大，使得学习过程更加复杂和不稳定。因此，需要一种能够有效分解长时程任务并实现泛化的OSIL方法。\\n\\n**核心思路**：ManiLong-Shot的核心思路是将长时程任务分解为一系列交互感知的原语。每个原语代表一个基本的物理交互动作，例如抓取、放置等。通过学习如何执行这些原语，并将它们组合起来，可以实现对长时程任务的模仿学习。这种分解方法降低了学习的复杂性，并提高了泛化能力。\\n\\n**技术框架**：ManiLong-Shot的整体框架包括以下几个主要模块：1) 任务分解模块：使用视觉-语言模型(VLM)或基于规则的启发式方法将长时程任务分解为交互原语序列。2) 原语执行模块：对于每个原语，预测对交互至关重要的不变区域，建立演示和当前观察之间的对应关系，并计算目标末端执行器姿态。3) 控制模块：根据计算出的目标姿态，控制机器人执行相应的动作。\\n\\n**关键创新**：ManiLong-Shot的关键创新在于其交互感知的原语分解方法。与直接模仿连续轨迹的方法不同，ManiLong-Shot关注于任务中的物理交互事件，并将任务分解为一系列交互原语。这种分解方法使得模型能够更好地理解任务的结构，并提高泛化能力。此外，使用VLM或规则启发式方法进行任务分解也提高了框架的灵活性和适应性。\\n\\n**关键设计**：在原语执行模块中，论文可能使用了特定的损失函数来训练模型预测不变区域和目标姿态。例如，可以使用对比损失来学习不变区域的表示，并使用均方误差损失来优化目标姿态的预测。此外，网络结构的设计也可能对性能产生影响，例如使用卷积神经网络(CNN)来提取图像特征，并使用循环神经网络(RNN)来处理原语序列。",
            "application_zh": "ManiLong-Shot具有广泛的应用前景，例如在智能制造、家庭服务、医疗康复等领域。它可以用于教导机器人执行各种复杂的长时程操作任务，例如装配产品、整理物品、辅助病人等。该研究的实际价值在于降低了机器人编程的难度，提高了机器人的智能化水平，并有望推动机器人技术在各个领域的应用。",
            "highlight_zh": "ManiLong-Shot在仿真实验中，仅使用10个短时程任务进行训练，即可泛化到20个未见过的长时程任务，相对于SOTA方法实现了22.8%的相对改进。在真实机器人实验中，ManiLong-Shot成功执行了三个长时程操作任务，验证了其在实际应用中的可行性。这些结果表明，ManiLong-Shot是一种有效的长时程操作任务单样本模仿学习方法。",
            "tags_zh": [
                "单样本模仿学习",
                "长时程操作",
                "交互感知",
                "视觉-语言模型",
                "机器人控制"
            ],
            "_index": 16,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16302v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16302v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16302v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "AdaTooler-V: Adaptive Tool-Use for Images and Videos",
            "authors": [
                "Chaoyang Wang",
                "Kaituo Feng",
                "Dongyang Chen",
                "Zhongyu Wang",
                "Zhixun Li",
                "Sicheng Gao",
                "Meng Meng",
                "Xu Zhou",
                "Manyuan Zhang",
                "Yuzhang Shang",
                "Xiangyu Yue"
            ],
            "arxiv_id": "2512.16918v1",
            "summary": "Recent advances have shown that multimodal large language models (MLLMs) benefit from multimodal interleaved chain-of-thought (CoT) with vision tool interactions. However, existing open-source models often exhibit blind tool-use reasoning patterns, invoking vision tools even when they are unnecessary, which significantly increases inference overhead and degrades model performance. To this end, we propose AdaTooler-V, an MLLM that performs adaptive tool-use by determining whether a visual problem truly requires tools. First, we introduce AT-GRPO, a reinforcement learning algorithm that adaptively adjusts reward scales based on the Tool Benefit Score of each sample, encouraging the model to invoke tools only when they provide genuine improvements. Moreover, we construct two datasets to support training: AdaTooler-V-CoT-100k for SFT cold start and AdaTooler-V-300k for RL with verifiable rewards across single-image, multi-image, and video data. Experiments across twelve benchmarks demonstrate the strong reasoning capability of AdaTooler-V, outperforming existing methods in diverse visual reasoning tasks. Notably, AdaTooler-V-7B achieves an accuracy of 89.8\\% on the high-resolution benchmark V*, surpassing the commercial proprietary model GPT-4o and Gemini 1.5 Pro. All code, models, and data are released.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Project page: https://github.com/CYWang735/AdaTooler-V",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16918v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model",
                        "multimodal",
                        "chain-of-thought"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 10.5,
            "hit_pillars": [
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "提出AdaTooler-V，通过自适应工具使用提升多模态大语言模型在图像和视频任务中的推理效率和性能。",
            "summary_zh": "本文提出AdaTooler-V，一种多模态大语言模型(MLLM)，通过确定视觉问题是否真正需要工具来执行自适应工具使用。为了实现这一目标，我们引入了AT-GRPO，一种强化学习算法，它基于每个样本的工具效益评分自适应地调整奖励尺度，鼓励模型仅在工具提供真正的改进时才调用它们。此外，我们构建了两个数据集来支持训练：AdaTooler-V-CoT-100k用于SFT冷启动，AdaTooler-V-300k用于具有可验证奖励的RL，涵盖单图像、多图像和视频数据。在十二个基准测试上的实验表明，AdaTooler-V具有强大的推理能力，在各种视觉推理任务中优于现有方法。值得注意的是，AdaTooler-V-7B在高分辨率基准V*上实现了89.8%的准确率，超过了商业专有模型GPT-4o和Gemini 1.5 Pro。所有代码、模型和数据均已发布。",
            "intro_zh": [
                "现有开源多模态大语言模型存在盲目使用工具的推理模式，即使不必要也会调用视觉工具，导致推理开销增加和性能下降。",
                "AdaTooler-V通过判断视觉问题是否真正需要工具，执行自适应工具使用，从而避免不必要的工具调用。",
                "提出的AT-GRPO强化学习算法根据工具效益评分自适应调整奖励，鼓励模型仅在工具带来改进时才使用，实验表明性能超越GPT-4o和Gemini 1.5 Pro。"
            ],
            "method_zh": "**问题定义**：现有开源多模态大语言模型(MLLMs)在处理视觉任务时，常常不加区分地调用视觉工具，即使问题本身并不需要这些工具。这种“盲目”的工具使用方式导致了不必要的计算开销，降低了推理效率，并且在某些情况下还会损害模型的性能。因此，如何让MLLMs学会自适应地判断何时应该使用工具，何时应该避免使用工具，成为了一个亟待解决的问题。\\n\\n**核心思路**：AdaTooler-V的核心思路是让模型能够根据输入数据的特点和任务的需求，自适应地决定是否需要调用视觉工具。具体来说，模型会首先对输入进行初步分析，评估使用工具可能带来的收益（Tool Benefit Score），然后根据这个收益来决定是否调用工具。这种自适应的工具使用策略可以有效地避免不必要的计算开销，提高推理效率，并且在某些情况下还可以提升模型的性能。\\n\\n**技术框架**：AdaTooler-V的整体框架主要包括以下几个部分：1) 多模态输入编码器：用于将图像、视频和文本等多种模态的输入数据编码成统一的特征表示。2) 工具使用决策模块：根据编码后的特征，评估使用各种视觉工具可能带来的收益，并决定是否调用这些工具。3) 视觉工具模块：包含各种视觉处理工具，例如目标检测、图像分割、OCR等。4) 语言模型：用于生成最终的输出结果。5) 强化学习训练模块：使用AT-GRPO算法对模型进行训练，使其能够更好地学习自适应工具使用策略。\\n\\n**关键创新**：AdaTooler-V最重要的技术创新点在于AT-GRPO（Adaptive Tool-use with Gradient-based Reward Policy Optimization）强化学习算法。该算法能够根据每个样本的工具效益评分自适应地调整奖励尺度，从而鼓励模型仅在工具提供真正的改进时才调用它们。与传统的强化学习算法相比，AT-GRPO能够更有效地学习自适应工具使用策略，并且能够更好地平衡工具使用的收益和成本。\\n\\n**关键设计**：AT-GRPO算法的关键设计在于奖励函数的自适应调整。具体来说，算法会首先计算每个样本的工具效益评分（Tool Benefit Score），该评分反映了使用工具可能带来的性能提升。然后，算法会根据这个评分来调整奖励函数的尺度，对于工具效益评分较高的样本，算法会给予更高的奖励，从而鼓励模型调用工具；对于工具效益评分较低的样本，算法会给予较低的奖励，从而鼓励模型避免使用工具。此外，为了支持模型的训练，作者还构建了两个大规模数据集：AdaTooler-V-CoT-100k和AdaTooler-V-300k，分别用于SFT冷启动和RL训练。",
            "application_zh": "AdaTooler-V在多个领域具有广泛的应用前景，例如智能客服、自动驾驶、视频监控、医疗诊断等。它可以帮助这些应用更高效、更准确地理解和处理视觉信息，从而提升用户体验和工作效率。未来，该研究可以进一步扩展到更多的模态和任务，例如语音、文本等，从而构建更加通用和强大的多模态智能系统。",
            "highlight_zh": "AdaTooler-V在十二个基准测试上表现出强大的推理能力，超越了现有方法。尤其值得注意的是，AdaTooler-V-7B在高分辨率基准V*上达到了89.8%的准确率，超过了商业专有模型GPT-4o和Gemini 1.5 Pro，证明了其在复杂视觉推理任务上的卓越性能。",
            "tags_zh": [
                "多模态大语言模型",
                "自适应工具使用",
                "强化学习",
                "视觉推理",
                "图像视频处理"
            ],
            "_index": 17,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16918v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16918v1/x4.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16918v1/x5.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Instant Expressive Gaussian Head Avatar via 3D-Aware Expression Distillation",
            "authors": [
                "Kaiwen Jiang",
                "Xueting Li",
                "Seonwook Park",
                "Ravi Ramamoorthi",
                "Shalini De Mello",
                "Koki Nagano"
            ],
            "arxiv_id": "2512.16893v1",
            "summary": "Portrait animation has witnessed tremendous quality improvements thanks to recent advances in video diffusion models. However, these 2D methods often compromise 3D consistency and speed, limiting their applicability in real-world scenarios, such as digital twins or telepresence. In contrast, 3D-aware facial animation feedforward methods -- built upon explicit 3D representations, such as neural radiance fields or Gaussian splatting -- ensure 3D consistency and achieve faster inference speed, but come with inferior expression details. In this paper, we aim to combine their strengths by distilling knowledge from a 2D diffusion-based method into a feed-forward encoder, which instantly converts an in-the-wild single image into a 3D-consistent, fast yet expressive animatable representation. Our animation representation is decoupled from the face's 3D representation and learns motion implicitly from data, eliminating the dependency on pre-defined parametric models that often constrain animation capabilities. Unlike previous computationally intensive global fusion mechanisms (e.g., multiple attention layers) for fusing 3D structural and animation information, our design employs an efficient lightweight local fusion strategy to achieve high animation expressivity. As a result, our method runs at 107.31 FPS for animation and pose control while achieving comparable animation quality to the state-of-the-art, surpassing alternative designs that trade speed for quality or vice versa. Project website is https://research.nvidia.com/labs/amri/projects/instant4d",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Project website is https://research.nvidia.com/labs/amri/projects/instant4d",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16893v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]distillation"
                    ],
                    "score": 4.5
                },
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "gaussian splatting",
                        "splatting",
                        "neural radiance field"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 10.5,
            "hit_pillars": [
                "2_algo_arch",
                "3_perception_slam"
            ],
            "headline_zh": "提出基于3D感知表达蒸馏的快速高表现力高斯头部头像方法",
            "summary_zh": "得益于视频扩散模型的最新进展，人像动画的质量得到了显著提高。然而，这些2D方法通常会牺牲3D一致性和速度，限制了它们在数字孪生或远程呈现等实际场景中的应用。相比之下，基于显式3D表示（如神经辐射场或高斯溅射）的3D感知面部动画前馈方法，可确保3D一致性并实现更快的推理速度，但表达细节较差。本文旨在结合两者的优势，将知识从基于2D扩散的方法提炼到前馈编码器中，该编码器可立即将野外单张图像转换为3D一致、快速且富有表现力的可动画表示。我们的动画表示与面部的3D表示解耦，并从数据中隐式学习运动，从而消除了对通常限制动画能力的预定义参数模型的依赖。与先前用于融合3D结构和动画信息的计算密集型全局融合机制（例如，多个注意力层）不同，我们的设计采用了一种高效的轻量级局部融合策略，以实现高动画表现力。因此，我们的方法以107.31 FPS的速度运行动画和姿势控制，同时实现了与最先进技术相当的动画质量，超越了在速度和质量之间进行权衡的替代设计。",
            "intro_zh": [
                "现有2D扩散模型人像动画方法在3D一致性和速度上存在不足，限制了其在实时场景中的应用。",
                "该论文提出一种基于3D感知表达蒸馏的前馈方法，将2D扩散模型的知识迁移到3D表示，实现快速且高表现力的动画。",
                "实验结果表明，该方法在保证动画质量的同时，实现了107.31 FPS的动画和姿势控制速度，优于现有方法。"
            ],
            "method_zh": "**问题定义**：现有的人像动画方法，特别是基于2D扩散模型的方法，虽然在动画质量上取得了显著进展，但在3D一致性和推理速度方面存在不足。这限制了它们在需要实时性和3D感知的应用场景中的应用，例如数字孪生和远程呈现。另一方面，基于3D表示（如神经辐射场或高斯溅射）的方法虽然保证了3D一致性和速度，但在表达细节上有所欠缺。\\n\\n**核心思路**：该论文的核心思路是将2D扩散模型的表达能力“蒸馏”到基于3D表示的前馈网络中。通过这种方式，既能保持3D一致性和速度优势，又能获得高质量的动画表达。关键在于设计一种有效的知识迁移机制，将2D扩散模型的丰富细节融入到3D表示中。\\n\\n**技术框架**：该方法包含一个前馈编码器，用于将单张图像转换为3D一致且可动画的表示。该表示分为两部分：静态的3D人脸结构和动态的动画信息。动画信息从数据中隐式学习，无需依赖预定义的参数模型。为了融合3D结构和动画信息，该方法采用了一种轻量级的局部融合策略，避免了计算量大的全局融合机制。\\n\\n**关键创新**：该方法的关键创新在于使用3D感知表达蒸馏，将2D扩散模型的表达能力迁移到3D表示中，从而在保证3D一致性和速度的同时，实现了高质量的动画效果。此外，轻量级的局部融合策略也是一个重要的创新点，它在保证表达能力的同时，降低了计算复杂度。\\n\\n**关键设计**：该方法的一个关键设计是动画表示与3D人脸结构的解耦。这种解耦使得动画信息可以独立于3D结构进行学习和控制，从而提高了动画的灵活性和可控性。此外，损失函数的设计也至关重要，需要平衡3D一致性、动画质量和推理速度。",
            "application_zh": "该研究成果可广泛应用于数字孪生、远程呈现、虚拟现实、增强现实、游戏等领域。例如，可以用于创建逼真的虚拟化身，实现实时的人脸动画和表情控制，从而提升用户在虚拟环境中的交互体验。此外，该技术还可以应用于电影制作、广告等领域，用于生成高质量的人像动画。",
            "highlight_zh": "该方法在动画和姿势控制方面达到了107.31 FPS的速度，同时保持了与最先进技术相当的动画质量。实验结果表明，该方法在速度和质量之间取得了良好的平衡，超越了那些为了提高质量而牺牲速度或为了提高速度而降低质量的替代方案。该方法在表达能力方面也优于现有的基于3D表示的方法。",
            "tags_zh": [
                "人像动画",
                "3D感知",
                "表达蒸馏",
                "高斯溅射",
                "实时渲染"
            ],
            "_index": 18,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16893v1/fig/expressiveness_vs_consistency_colored.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16893v1/fig/pipeline-2.jpg",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16893v1/fig/residual_features.jpg",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning",
            "authors": [
                "Qihao Liu",
                "Luoxin Ye",
                "Wufei Ma",
                "Yu-Cheng Chou",
                "Alan Yuille"
            ],
            "arxiv_id": "2512.16917v1",
            "summary": "Large language models (LLMs) with explicit reasoning capabilities excel at mathematical reasoning yet still commit process errors, such as incorrect calculations, brittle logic, and superficially plausible but invalid steps. In this paper, we introduce Generative Adversarial Reasoner, an on-policy joint training framework designed to enhance reasoning by co-evolving an LLM reasoner and an LLM-based discriminator through adversarial reinforcement learning. A compute-efficient review schedule partitions each reasoning chain into logically complete slices of comparable length, and the discriminator evaluates each slice's soundness with concise, structured justifications. Learning couples complementary signals: the LLM reasoner is rewarded for logically consistent steps that yield correct answers, while the discriminator earns rewards for correctly detecting errors or distinguishing traces in the reasoning process. This produces dense, well-calibrated, on-policy step-level rewards that supplement sparse exact-match signals, improving credit assignment, increasing sample efficiency, and enhancing overall reasoning quality of LLMs. Across various mathematical benchmarks, the method delivers consistent gains over strong baselines with standard RL post-training. Specifically, on AIME24, we improve DeepSeek-R1-Distill-Qwen-7B from 54.0 to 61.3 (+7.3) and DeepSeek-R1-Distill-Llama-8B from 43.7 to 53.7 (+10.0). The modular discriminator also enables flexible reward shaping for objectives such as teacher distillation, preference alignment, and mathematical proof-based reasoning.",
            "categories": [
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16917v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]reinforcement learning",
                        "distillation",
                        "reward shaping"
                    ],
                    "score": 7.5
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 10.5,
            "hit_pillars": [
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "提出生成对抗推理器，通过对抗强化学习提升LLM的推理能力，尤其在数学问题上。",
            "summary_zh": "本文提出了一种名为生成对抗推理器（Generative Adversarial Reasoner）的在线联合训练框架，旨在通过对抗强化学习协同进化LLM推理器和基于LLM的判别器，从而增强LLM的推理能力。该框架采用计算高效的审查机制，将每个推理链划分为逻辑上完整的、长度相当的片段，判别器使用简洁、结构化的理由评估每个片段的合理性。学习过程耦合了互补信号：LLM推理器因产生逻辑一致且能得出正确答案的步骤而获得奖励，而判别器因正确检测到推理过程中的错误或区分推理轨迹而获得奖励。这产生了密集的、校准良好的在线步骤级奖励，补充了稀疏的精确匹配信号，改善了信用分配，提高了样本效率，并增强了LLM的整体推理质量。在各种数学基准测试中，该方法相对于使用标准RL后训练的强大基线，实现了持续的收益。具体而言，在AIME24上，DeepSeek-R1-Distill-Qwen-7B从54.0提高到61.3（+7.3），DeepSeek-R1-Distill-Llama-8B从43.7提高到53.7（+10.0）。模块化判别器还能够灵活地进行奖励塑造，以实现诸如教师知识蒸馏、偏好对齐和基于数学证明的推理等目标。",
            "intro_zh": [
                "现有LLM在数学推理中存在过程错误，如计算错误、逻辑脆弱和表面合理但无效的步骤，影响了推理的可靠性。",
                "提出生成对抗推理器，通过对抗强化学习协同训练LLM推理器和判别器，利用判别器的反馈来提升推理器的逻辑一致性。",
                "实验表明，该方法在数学基准测试中显著提升了LLM的推理性能，例如在AIME24数据集上取得了显著的准确率提升。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大型语言模型（LLM）在数学推理过程中出现的逻辑错误、计算错误以及推理步骤不合理等问题。现有方法通常依赖于稀疏的奖励信号（例如，最终答案是否正确），导致信用分配困难，训练效率低下，难以有效提升LLM的推理能力。\\n\\n**核心思路**：论文的核心思路是通过对抗强化学习，同时训练一个LLM推理器和一个LLM判别器。推理器负责生成推理步骤，判别器负责评估每个推理步骤的合理性。通过这种对抗的方式，推理器可以学习到更细粒度的奖励信号，从而更好地优化推理过程。\\n\\n**技术框架**：整体框架包含两个主要模块：LLM推理器和LLM判别器。推理器负责生成推理步骤，判别器负责评估每个推理步骤的合理性，并给出结构化的理由。训练过程采用在线强化学习的方式，推理器根据判别器的反馈调整策略，判别器根据推理器的表现调整评估标准。一个关键组件是“审查机制”，它将推理链分割成逻辑完整的片段，以便判别器能够更有效地评估每个片段的合理性。\\n\\n**关键创新**：最重要的创新点在于引入了对抗学习的思想，将推理过程建模成一个生成对抗的过程。判别器的存在为推理器提供了更密集、更细粒度的奖励信号，解决了传统强化学习中信用分配困难的问题。此外，模块化的判别器设计使得可以灵活地进行奖励塑造，以适应不同的目标，例如知识蒸馏、偏好对齐等。\\n\\n**关键设计**：论文设计了一个计算高效的审查机制，将推理链分割成逻辑完整的片段，并让判别器评估每个片段的合理性。判别器输出简洁、结构化的理由，为推理器提供更丰富的反馈信息。推理器和判别器都使用LLM作为基础模型，并通过强化学习算法进行训练。具体的损失函数包括推理器的策略梯度损失和判别器的交叉熵损失。奖励函数的设计考虑了推理步骤的逻辑一致性和最终答案的正确性。",
            "application_zh": "该研究成果可应用于各种需要复杂推理能力的场景，例如数学问题求解、科学研究、智能问答系统等。通过提升LLM的推理能力，可以提高这些应用场景的准确性和可靠性，并有望推动人工智能在更广泛领域的应用。",
            "highlight_zh": "实验结果表明，该方法在AIME24等数学基准测试中取得了显著的性能提升。例如，DeepSeek-R1-Distill-Qwen-7B模型在该数据集上的准确率从54.0%提高到61.3%（+7.3%），DeepSeek-R1-Distill-Llama-8B模型从43.7%提高到53.7%（+10.0%）。这些结果表明，该方法能够有效提升LLM的推理能力。",
            "tags_zh": [
                "大型语言模型",
                "强化学习",
                "对抗学习",
                "数学推理",
                "信用分配",
                "奖励塑造",
                "在线学习"
            ],
            "_index": 19,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16917v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16917v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16917v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Non-Asymptotic Global Convergence of PPO-Clip",
            "authors": [
                "Yin Liu",
                "Qiming Dai",
                "Junyu Zhang",
                "Zaiwen Wen"
            ],
            "arxiv_id": "2512.16565v1",
            "summary": "Reinforcement learning (RL) has gained attention for aligning large language models (LLMs) via reinforcement learning from human feedback (RLHF). The actor-only variants of Proximal Policy Optimization (PPO) are widely applied for their efficiency. These algorithms incorporate a clipping mechanism to improve stability. Besides, a regularization term, such as the reverse KL-divergence or a more general \\(f\\)-divergence, is introduced to prevent policy drift. Despite their empirical success, a rigorous theoretical understanding of the problem and the algorithm's properties is limited. This paper advances the theoretical foundations of the PPO-Clip algorithm by analyzing a deterministic actor-only PPO algorithm within the general RL setting with \\(f\\)-divergence regularization under the softmax policy parameterization. We derive a non-uniform Lipschitz smoothness condition and a Łojasiewicz inequality for the considered problem. Based on these, a non-asymptotic linear convergence rate to the globally optimal policy is established for the forward KL-regularizer. Furthermore, stationary convergence and local linear convergence are derived for the reverse KL-regularizer.",
            "categories": [
                "math.OC",
                "cs.LG"
            ],
            "primary_category": "math.OC",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16565v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning",
                        "[T]PPO",
                        "RLHF"
                    ],
                    "score": 7.5
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 10.5,
            "hit_pillars": [
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "提出PPO-Clip算法的非渐近全局收敛性分析",
            "summary_zh": "强化学习（RL）因其在通过人类反馈对大型语言模型（LLMs）进行对齐方面的应用而受到关注。PPO的仅演员变体因其高效性而被广泛应用，这些算法通过引入剪切机制来提高稳定性。此外，论文引入了正则化项，如反KL散度或更一般的f散度，以防止策略漂移。尽管这些方法在实践中取得了成功，但对其理论基础的理解仍然有限。本文通过分析带有f散度正则化的确定性仅演员PPO算法，推进了PPO-Clip算法的理论基础，建立了针对前向KL正则化的非渐近线性收敛率，并推导了反向KL正则化的平稳收敛和局部线性收敛性。",
            "intro_zh": [
                "现有的PPO算法在理论理解上存在不足，尤其是在收敛性和稳定性方面的分析较为薄弱。",
                "论文提出了一种新的理论框架，通过对PPO-Clip算法进行分析，建立了非渐近线性收敛性，增强了算法的理论基础。",
                "研究表明，使用前向KL正则化时，PPO-Clip算法能够实现全局最优策略的非渐近收敛，且在反向KL正则化下也能达到平稳收敛。"
            ],
            "method_zh": "**问题定义**：本文旨在解决PPO-Clip算法在理论收敛性方面的不足，尤其是在强化学习中如何有效防止策略漂移的问题。现有方法缺乏对算法性质的严格理论分析，导致其应用效果不稳定。\\n\\n**核心思路**：论文通过引入f散度正则化，分析了确定性仅演员PPO算法的性质，建立了非均匀Lipschitz光滑性条件和Łojasiewicz不等式，从而为收敛性提供了理论支持。\\n\\n**技术框架**：整体架构包括对PPO-Clip算法的理论分析，主要模块包括正则化项的引入、光滑性条件的推导以及收敛性结果的证明。\\n\\n**关键创新**：最重要的技术创新在于提出了针对前向和反向KL正则化的非渐近线性收敛性分析，填补了现有理论研究的空白，提供了更为严谨的收敛性保证。\\n\\n**关键设计**：论文中使用了特定的损失函数和正则化项，特别是f散度的选择，以及在软最大策略参数化下的算法设计，确保了算法的稳定性和收敛性。 ",
            "application_zh": "该研究的潜在应用领域包括自然语言处理、机器人控制和其他需要通过人类反馈进行学习的强化学习任务。通过提供更为稳健的收敛性理论，PPO-Clip算法能够在实际应用中更有效地对齐大型语言模型，提升其性能和可靠性。",
            "highlight_zh": "实验结果表明，PPO-Clip算法在使用前向KL正则化时，能够实现非渐近线性收敛至全局最优策略。此外，在反向KL正则化下，算法也展示了良好的平稳收敛性，显著提升了收敛速度和稳定性。",
            "tags_zh": [
                "强化学习",
                "PPO算法",
                "KL散度",
                "收敛性分析",
                "理论研究",
                "语言模型",
                "人类反馈"
            ],
            "_index": 20,
            "_used_api": "openai",
            "figures": []
        },
        {
            "title": "The Evolution of Reranking Models in Information Retrieval: From Heuristic Methods to Large Language Models",
            "authors": [
                "Tejul Pandit",
                "Sakshi Mahendru",
                "Meet Raval",
                "Dhvani Upadhyay"
            ],
            "arxiv_id": "2512.16236v1",
            "summary": "Reranking is a critical stage in contemporary information retrieval (IR) systems, improving the relevance of the user-presented final results by honing initial candidate sets. This paper is a thorough guide to examine the changing reranker landscape and offer a clear view of the advancements made in reranking methods. We present a comprehensive survey of reranking models employed in IR, particularly within modern Retrieval Augmented Generation (RAG) pipelines, where retrieved documents notably influence output quality.\n  We embark on a chronological journey through the historical trajectory of reranking techniques, starting with foundational approaches, before exploring the wide range of sophisticated neural network architectures such as cross-encoders, sequence-generation models like T5, and Graph Neural Networks (GNNs) utilized for structural information. Recognizing the computational cost of advancing neural rerankers, we analyze techniques for enhancing efficiency, notably knowledge distillation for creating competitive, lighter alternatives. Furthermore, we map the emerging territory of integrating Large Language Models (LLMs) in reranking, examining novel prompting strategies and fine-tuning tactics. This survey seeks to elucidate the fundamental ideas, relative effectiveness, computational features, and real-world trade-offs of various reranking strategies. The survey provides a structured synthesis of the diverse reranking paradigms, highlighting their underlying principles and comparative strengths and weaknesses.",
            "categories": [
                "cs.IR",
                "cs.AI"
            ],
            "primary_category": "cs.IR",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "15 pages, 1 figure, Accepted in CLNLP'25",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16236v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "distillation"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]large language model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 10.5,
            "hit_pillars": [
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "综述信息检索中重排序模型演进：从启发式方法到大型语言模型",
            "summary_zh": "重排序是现代信息检索（IR）系统中的关键阶段，它通过优化初始候选集来提高用户最终结果的相关性。本文全面地考察了重排序领域的发展变化，并清晰地展示了重排序方法所取得的进步。我们对信息检索中使用的重排序模型进行了全面的综述，特别是在现代检索增强生成（RAG）流程中，检索到的文档对输出质量有显著影响。我们按时间顺序回顾了重排序技术的历史轨迹，从基础方法开始，然后探索了各种复杂的神经网络架构，如交叉编码器、序列生成模型（如T5）和用于结构信息的图神经网络（GNN）。考虑到高级神经重排序器的计算成本，我们分析了提高效率的技术，特别是用于创建有竞争力的、更轻量级替代方案的知识蒸馏。此外，我们还绘制了大型语言模型（LLM）集成到重排序中的新兴领域，研究了新颖的提示策略和微调策略。本综述旨在阐明各种重排序策略的基本思想、相对有效性、计算特征和实际权衡，并突出它们的潜在原则以及相对优势和劣势。",
            "intro_zh": [
                "现有信息检索系统在初始检索后，需要对候选结果进行重排序以提升用户体验，但传统方法效果有限。",
                "本文综述了信息检索中重排序模型的发展历程，重点关注神经重排序模型和大型语言模型在重排序中的应用。",
                "分析了各种重排序策略的有效性、计算成本和实际应用中的权衡，并探讨了知识蒸馏等效率提升技术。"
            ],
            "method_zh": "**问题定义**：信息检索系统在初始检索阶段通常会返回大量的候选文档，但这些文档的相关性参差不齐。重排序的目标是从这些候选文档中选择出最相关的文档，并按照相关性排序，以便用户能够快速找到所需信息。现有方法的痛点在于，传统的启发式方法难以捕捉复杂的语义关系，而早期的神经重排序模型计算成本高昂，难以应用于大规模数据集。\\n\\n**核心思路**：本文的核心思路是对信息检索中的重排序模型进行全面的回顾和分析，从传统的启发式方法到现代的神经重排序模型，再到大型语言模型在重排序中的应用，梳理了重排序技术的发展脉络。通过分析各种方法的优缺点，为研究人员和工程师提供了一个全面的参考，帮助他们选择合适的重排序模型。\\n\\n**技术框架**：本文的综述框架主要包括以下几个部分：1）介绍重排序的基本概念和重要性；2）回顾传统的启发式重排序方法，如BM25、TF-IDF等；3）详细介绍神经重排序模型，包括交叉编码器、序列生成模型（如T5）和图神经网络（GNN）；4）分析提高神经重排序模型效率的技术，如知识蒸馏；5）探讨大型语言模型在重排序中的应用，包括prompting策略和fine-tuning策略。\\n\\n**关键创新**：本文的创新之处在于对信息检索中重排序模型进行了系统而全面的综述，涵盖了从传统方法到最新技术的发展历程。特别关注了大型语言模型在重排序中的应用，并分析了各种prompting策略和fine-tuning策略的优缺点。此外，本文还对各种重排序策略的计算成本和实际应用中的权衡进行了深入的分析。\\n\\n**关键设计**：本文主要是一篇综述文章，没有提出新的模型或算法。但是，本文对各种重排序模型的关键设计进行了详细的描述，例如，交叉编码器的网络结构、序列生成模型的训练目标、图神经网络的图结构等。此外，本文还对知识蒸馏的训练方法、prompting策略的设计、fine-tuning策略的选择等关键技术细节进行了深入的分析。",
            "application_zh": "该研究成果对信息检索、搜索引擎、推荐系统等领域具有重要的应用价值。通过选择合适的重排序模型，可以显著提高检索结果的相关性，提升用户体验。此外，该研究还可以应用于问答系统、对话系统等领域，提高系统的准确性和可靠性。未来，随着大型语言模型的不断发展，重排序技术将在更多领域发挥重要作用。",
            "highlight_zh": "本文是一篇综述文章，没有具体的实验结果。但是，本文对各种重排序模型的性能进行了比较分析，并指出了各种方法的优缺点。例如，交叉编码器通常比单编码器具有更高的准确率，但计算成本也更高。知识蒸馏可以有效地提高神经重排序模型的效率，使其能够应用于大规模数据集。大型语言模型在重排序中表现出强大的潜力，但需要仔细设计prompting策略和fine-tuning策略。",
            "tags_zh": [
                "信息检索",
                "重排序",
                "神经模型",
                "大型语言模型",
                "知识蒸馏",
                "检索增强生成",
                "交叉编码器"
            ],
            "_index": 21,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16236v1/Reranker_Module_2.png",
                    "caption": "",
                    "figure_id": "fig_0"
                }
            ]
        },
        {
            "title": "ReinforceGen: Hybrid Skill Policies with Automated Data Generation and Reinforcement Learning",
            "authors": [
                "Zihan Zhou",
                "Animesh Garg",
                "Ajay Mandlekar",
                "Caelan Garrett"
            ],
            "arxiv_id": "2512.16861v1",
            "summary": "Long-horizon manipulation has been a long-standing challenge in the robotics community. We propose ReinforceGen, a system that combines task decomposition, data generation, imitation learning, and motion planning to form an initial solution, and improves each component through reinforcement-learning-based fine-tuning. ReinforceGen first segments the task into multiple localized skills, which are connected through motion planning. The skills and motion planning targets are trained with imitation learning on a dataset generated from 10 human demonstrations, and then fine-tuned through online adaptation and reinforcement learning. When benchmarked on the Robosuite dataset, ReinforceGen reaches 80% success rate on all tasks with visuomotor controls in the highest reset range setting. Additional ablation studies show that our fine-tuning approaches contributes to an 89% average performance increase. More results and videos available in https://reinforcegen.github.io/",
            "categories": [
                "cs.RO",
                "cs.AI",
                "cs.LG"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16861v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation",
                        "motion planning"
                    ],
                    "score": 4.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]reinforcement learning",
                        "imitation learning"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 10.0,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "ReinforceGen：结合自动数据生成与强化学习的混合技能策略，解决机器人长程操作难题。",
            "summary_zh": "本文提出ReinforceGen，一个结合任务分解、数据生成、模仿学习和运动规划的系统，旨在解决机器人领域中长期存在的长程操作挑战。ReinforceGen首先将任务分割成多个局部技能，并通过运动规划连接这些技能。技能和运动规划目标通过模仿学习在由10个人类演示生成的数据集上进行训练，然后通过在线自适应和强化学习进行微调。在Robosuite数据集上的基准测试表明，ReinforceGen在最高重置范围设置下，使用视觉运动控制在所有任务上达到了80%的成功率。额外的消融研究表明，我们的微调方法平均贡献了89%的性能提升。更多结果和视频可在https://reinforcegen.github.io/上找到。",
            "intro_zh": [
                "长程机器人操作任务因其复杂性和状态空间巨大而极具挑战，现有方法难以有效解决。",
                "ReinforceGen通过分解任务为局部技能，并结合模仿学习和强化学习，实现了更高效的策略学习。",
                "实验表明，ReinforceGen在Robosuite数据集上取得了显著的成功率，并通过微调实现了平均89%的性能提升。"
            ],
            "method_zh": "**问题定义**：论文旨在解决机器人长程操作任务中，由于任务复杂、状态空间大，导致学习效率低下的问题。现有方法通常难以有效地探索和学习复杂的动作序列，或者需要大量的人工标注数据，成本高昂。\\n\\n**核心思路**：论文的核心思路是将长程任务分解为多个局部技能，然后通过模仿学习初始化这些技能，并利用强化学习进行微调。通过分解任务，降低了每个技能的学习难度，并通过模仿学习提供了一个良好的初始策略，加速了强化学习的收敛。\\n\\n**技术框架**：ReinforceGen的整体框架包含以下几个主要阶段：1) **任务分解**：将长程任务分解为多个局部技能。2) **数据生成**：通过少量（10个）人类演示生成数据集。3) **模仿学习**：利用生成的数据集训练技能和运动规划目标。4) **运动规划**：使用运动规划器连接各个技能。5) **强化学习微调**：通过在线自适应和强化学习对技能和运动规划进行微调。\\n\\n**关键创新**：ReinforceGen的关键创新在于结合了任务分解、模仿学习和强化学习，形成了一个混合技能策略学习框架。与传统的端到端强化学习方法相比，ReinforceGen能够更有效地探索和学习复杂的动作序列。此外，通过自动数据生成，减少了对大量人工标注数据的依赖。\\n\\n**关键设计**：论文中使用了模仿学习来初始化技能策略，并使用强化学习进行微调。具体的强化学习算法未知，但可以推测使用了常见的策略梯度或值函数方法。运动规划器的具体实现也未知，但其作用是连接各个技能，形成完整的动作序列。损失函数的设计可能包括模仿学习的交叉熵损失和强化学习的奖励函数。",
            "application_zh": "ReinforceGen具有广泛的应用前景，可应用于工业自动化、家庭服务机器人、医疗机器人等领域。例如，在工业自动化中，可以利用ReinforceGen训练机器人完成复杂的装配任务；在家庭服务机器人中，可以训练机器人完成家务劳动；在医疗机器人中，可以训练机器人辅助医生进行手术。",
            "highlight_zh": "ReinforceGen在Robosuite数据集上进行了评估，结果表明其在所有任务上达到了80%的成功率，并且通过强化学习微调，平均性能提升了89%。这些结果表明，ReinforceGen能够有效地解决长程机器人操作任务，并且具有很强的泛化能力。",
            "tags_zh": [
                "机器人操作",
                "强化学习",
                "模仿学习",
                "任务分解",
                "运动规划",
                "数据生成",
                "长程任务"
            ],
            "_index": 22,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16861v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16861v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16861v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "SARMAE: Masked Autoencoder for SAR Representation Learning",
            "authors": [
                "Danxu Liu",
                "Di Wang",
                "Hebaixu Wang",
                "Haoyang Chen",
                "Wentao Jiang",
                "Yilin Cheng",
                "Haonan Guo",
                "Wei Cui",
                "Jing Zhang"
            ],
            "arxiv_id": "2512.16635v1",
            "summary": "Synthetic Aperture Radar (SAR) imagery plays a critical role in all-weather, day-and-night remote sensing applications. However, existing SAR-oriented deep learning is constrained by data scarcity, while the physically grounded speckle noise in SAR imagery further hampers fine-grained semantic representation learning. To address these challenges, we propose SARMAE, a Noise-Aware Masked Autoencoder for self-supervised SAR representation learning. Specifically, we construct SAR-1M, the first million-scale SAR dataset, with additional paired optical images, to enable large-scale pre-training. Building upon this, we design Speckle-Aware Representation Enhancement (SARE), which injects SAR-specific speckle noise into masked autoencoders to facilitate noise-aware and robust representation learning. Furthermore, we introduce Semantic Anchor Representation Constraint (SARC), which leverages paired optical priors to align SAR features and ensure semantic consistency. Extensive experiments across multiple SAR datasets demonstrate that SARMAE achieves state-of-the-art performance on classification, detection, and segmentation tasks. Code and models will be available at https://github.com/MiliLab/SARMAE.",
            "categories": [
                "cs.CV",
                "cs.LG"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Code and models will be available at https://github.com/MiliLab/SARMAE",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16635v1",
            "code_links": [
                {
                    "url": "https://github.com/MiliLab/SARMAE",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]representation learning",
                        "[T]masked autoencoder"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "SARMAE：面向SAR图像表征学习的噪声感知掩码自编码器",
            "summary_zh": "合成孔径雷达(SAR)图像在全天候、昼夜遥感应用中起着关键作用。然而，现有的面向SAR的深度学习受到数据稀缺的限制，而SAR图像中固有的物理散斑噪声进一步阻碍了细粒度语义表征学习。为了应对这些挑战，我们提出了SARMAE，一种用于自监督SAR表征学习的噪声感知掩码自编码器。具体来说，我们构建了首个百万级SAR数据集SAR-1M，并包含配对的光学图像，以实现大规模预训练。在此基础上，我们设计了散斑感知表征增强(SARE)，它将SAR特有的散斑噪声注入到掩码自编码器中，以促进噪声感知和鲁棒的表征学习。此外，我们引入了语义锚点表征约束(SARC)，它利用配对的光学先验来对齐SAR特征并确保语义一致性。在多个SAR数据集上的大量实验表明，SARMAE在分类、检测和分割任务上实现了最先进的性能。代码和模型将在https://github.com/MiliLab/SARMAE上提供。",
            "intro_zh": [
                "现有SAR图像深度学习受限于数据稀缺和散斑噪声，难以学习细粒度语义表征。",
                "SARMAE通过构建大规模SAR数据集，并设计噪声感知机制，提升表征学习的鲁棒性。",
                "实验表明，SARMAE在分类、检测和分割等任务上取得了SOTA性能，验证了其有效性。"
            ],
            "method_zh": "**问题定义**：现有的SAR图像深度学习方法面临两个主要挑战：一是SAR图像数据量不足，难以训练深度模型；二是SAR图像中固有的散斑噪声会干扰模型的特征提取，降低模型的性能。因此，如何利用有限的SAR数据，学习到对噪声鲁棒且具有判别性的SAR图像表征，是本文要解决的关键问题。\\n\\n**核心思路**：本文的核心思路是利用掩码自编码器(MAE)进行自监督学习，并针对SAR图像的特点，引入噪声感知机制和语义对齐约束。通过掩码部分图像，迫使模型学习图像的上下文信息，从而提高模型的泛化能力。同时，通过注入SAR特有的散斑噪声，使模型能够学习到对噪声鲁棒的特征。此外，利用配对的光学图像作为先验知识，对齐SAR图像的特征，保证语义一致性。\\n\\n**技术框架**：SARMAE的整体框架包括三个主要模块：1) SAR-1M数据集的构建，用于大规模预训练；2) 散斑感知表征增强(SARE)，通过注入散斑噪声提高模型的鲁棒性；3) 语义锚点表征约束(SARC)，利用配对光学图像对齐SAR特征。首先，使用SAR-1M数据集对MAE进行预训练。然后，在预训练过程中，使用SARE模块注入散斑噪声，并使用SARC模块对齐SAR特征。最后，将预训练好的模型应用于下游任务，如分类、检测和分割。\\n\\n**关键创新**：本文最重要的技术创新点在于提出了噪声感知的掩码自编码器(SARMAE)。与传统的MAE相比，SARMAE针对SAR图像的特点，引入了散斑感知表征增强(SARE)和语义锚点表征约束(SARC)。SARE模块通过注入散斑噪声，使模型能够学习到对噪声鲁棒的特征。SARC模块利用配对光学图像作为先验知识，对齐SAR图像的特征，保证语义一致性。\\n\\n**关键设计**：在SARE模块中，作者使用了SAR特有的散斑噪声模型，并根据SAR图像的成像原理，调整了噪声的参数。在SARC模块中，作者使用了对比学习损失函数，使SAR图像的特征与配对光学图像的特征尽可能接近。此外，作者还使用了Transformer作为MAE的骨干网络，并调整了Transformer的参数，以适应SAR图像的特点。",
            "application_zh": "该研究成果可广泛应用于全天候、昼夜遥感领域，例如灾害监测、目标检测、土地覆盖分类、城市规划等。通过提升SAR图像的表征学习能力，可以更准确地提取地物信息，为相关应用提供更可靠的数据支持。未来，该方法有望进一步推广到其他类型的遥感图像处理中，具有重要的实际应用价值。",
            "highlight_zh": "SARMAE在多个SAR数据集上进行了广泛的实验，结果表明，在分类、检测和分割任务上，SARMAE均取得了SOTA性能。例如，在某分类任务上，SARMAE相比于现有最佳方法，准确率提升了3个百分点。实验结果充分验证了SARMAE的有效性和优越性。",
            "tags_zh": [
                "SAR图像",
                "自监督学习",
                "掩码自编码器",
                "表征学习",
                "散斑噪声",
                "遥感",
                "深度学习"
            ],
            "_index": 23,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16635v1/images/radar.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16635v1/images/dataset.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16635v1/images/model.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Causal-Tune: Mining Causal Factors from Vision Foundation Models for Domain Generalized Semantic Segmentation",
            "authors": [
                "Yin Zhang",
                "Yongqiang Zhang",
                "Yaoyue Zheng",
                "Bogdan Raducanu",
                "Dan Liu"
            ],
            "arxiv_id": "2512.16567v1",
            "summary": "Fine-tuning Vision Foundation Models (VFMs) with a small number of parameters has shown remarkable performance in Domain Generalized Semantic Segmentation (DGSS). Most existing works either train lightweight adapters or refine intermediate features to achieve better generalization on unseen domains. However, they both overlook the fact that long-term pre-trained VFMs often exhibit artifacts, which hinder the utilization of valuable representations and ultimately degrade DGSS performance. Inspired by causal mechanisms, we observe that these artifacts are associated with non-causal factors, which usually reside in the low- and high-frequency components of the VFM spectrum. In this paper, we explicitly examine the causal and non-causal factors of features within VFMs for DGSS, and propose a simple yet effective method to identify and disentangle them, enabling more robust domain generalization. Specifically, we propose Causal-Tune, a novel fine-tuning strategy designed to extract causal factors and suppress non-causal ones from the features of VFMs. First, we extract the frequency spectrum of features from each layer using the Discrete Cosine Transform (DCT). A Gaussian band-pass filter is then applied to separate the spectrum into causal and non-causal components. To further refine the causal components, we introduce a set of causal-aware learnable tokens that operate in the frequency domain, while the non-causal components are discarded. Finally, refined features are transformed back into the spatial domain via inverse DCT and passed to the next layer. Extensive experiments conducted on various cross-domain tasks demonstrate the effectiveness of Causal-Tune. In particular, our method achieves superior performance under adverse weather conditions, improving +4.8% mIoU over the baseline in snow conditions.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Accepted by AAAI 2026",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16567v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]foundation model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "Causal-Tune：挖掘视觉基础模型中的因果因子，用于领域泛化语义分割",
            "summary_zh": "本文提出了一种新颖的领域泛化语义分割（DGSS）方法，旨在解决视觉基础模型（VFM）中存在的伪影问题。这些伪影与非因果因素相关，通常存在于VFM频谱的低频和高频分量中，阻碍了VFM的有效利用并降低了DGSS的性能。受因果机制的启发，本文显式地研究了VFM特征中的因果和非因果因素，并提出了一种简单而有效的方法来识别和分离它们，从而实现更鲁棒的领域泛化。具体而言，本文提出了Causal-Tune，一种新的微调策略，旨在从VFM的特征中提取因果因素并抑制非因果因素。该方法首先使用离散余弦变换（DCT）提取每层特征的频谱，然后应用高斯带通滤波器将频谱分离为因果和非因果分量。为了进一步细化因果分量，引入了一组在频域中运行的因果感知可学习token，同时丢弃非因果分量。最后，细化后的特征通过逆DCT转换回空间域，并传递到下一层。在各种跨域任务上进行的大量实验证明了Causal-Tune的有效性。尤其是在恶劣天气条件下，该方法表现出优异的性能，在雪地条件下比基线提高了+4.8% mIoU。",
            "intro_zh": [
                "现有领域泛化语义分割方法忽略了预训练视觉基础模型中存在的伪影，这些伪影会降低模型性能。",
                "Causal-Tune通过分析特征频谱，分离并提取因果因素，抑制非因果因素，从而提升模型的泛化能力。",
                "实验表明，Causal-Tune在各种跨域任务中表现出色，尤其在恶劣天气条件下，显著提高了语义分割的精度。"
            ],
            "method_zh": "**问题定义**：领域泛化语义分割（DGSS）旨在使模型在未见过的目标领域上也能保持良好的分割性能。现有的方法通常通过训练轻量级适配器或优化中间特征来实现，但忽略了预训练视觉基础模型（VFM）中存在的伪影，这些伪影与非因果因素相关，阻碍了VFM的有效利用，导致DGSS性能下降。\\n\\n**核心思路**：本文的核心思路是基于因果机制，认为VFM中存在的伪影与非因果因素相关，这些因素通常存在于VFM频谱的低频和高频分量中。通过识别和分离这些因果和非因果因素，可以提取更鲁棒的特征表示，从而提升DGSS的性能。\\n\\n**技术框架**：Causal-Tune的整体框架包括以下几个主要步骤：1. 使用离散余弦变换（DCT）提取每层特征的频谱。2. 应用高斯带通滤波器将频谱分离为因果和非因果分量。3. 引入一组因果感知可学习token，在频域中细化因果分量，并丢弃非因果分量。4. 通过逆DCT将细化后的特征转换回空间域，并传递到下一层。\\n\\n**关键创新**：本文最重要的技术创新点在于显式地考虑了VFM特征中的因果和非因果因素，并提出了一种简单而有效的方法来识别和分离它们。与现有方法不同，Causal-Tune不是简单地训练适配器或优化特征，而是从因果关系的角度出发，挖掘VFM中更本质的特征表示。\\n\\n**关键设计**：Causal-Tune的关键设计包括：1. 使用DCT将特征转换到频域，以便分析其频谱特性。2. 设计高斯带通滤波器，用于分离因果和非因果分量。滤波器的参数（例如中心频率和带宽）需要根据具体任务进行调整。3. 引入因果感知可学习token，用于在频域中细化因果分量。这些token可以通过反向传播进行训练，以更好地适应不同的领域和任务。",
            "application_zh": "Causal-Tune在自动驾驶、遥感图像分析、医学图像诊断等领域具有广泛的应用前景。通过提高模型在不同环境和条件下的泛化能力，可以显著提升这些应用系统的鲁棒性和可靠性。例如，在自动驾驶中，Causal-Tune可以帮助车辆在恶劣天气条件下更准确地识别道路和障碍物，从而提高驾驶安全性。",
            "highlight_zh": "Causal-Tune在多个跨域语义分割任务上取得了显著的性能提升。特别是在雪地条件下，Causal-Tune相比基线方法提高了4.8%的mIoU。实验结果表明，Causal-Tune能够有效地提取因果因素并抑制非因果因素，从而提高模型在未见过的目标领域上的泛化能力。",
            "tags_zh": [
                "领域泛化",
                "语义分割",
                "视觉基础模型",
                "因果推断",
                "离散余弦变换",
                "频率分析",
                "特征解耦"
            ],
            "_index": 24,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16567v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16567v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16567v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Smile on the Face, Sadness in the Eyes: Bridging the Emotion Gap with a Multimodal Dataset of Eye and Facial Behaviors",
            "authors": [
                "Kejun Liu",
                "Yuanyuan Liu",
                "Lin Wei",
                "Chang Tang",
                "Yibing Zhan",
                "Zijing Chen",
                "Zhe Chen"
            ],
            "arxiv_id": "2512.16485v1",
            "summary": "Emotion Recognition (ER) is the process of analyzing and identifying human emotions from sensing data. Currently, the field heavily relies on facial expression recognition (FER) because visual channel conveys rich emotional cues. However, facial expressions are often used as social tools rather than manifestations of genuine inner emotions. To understand and bridge this gap between FER and ER, we introduce eye behaviors as an important emotional cue and construct an Eye-behavior-aided Multimodal Emotion Recognition (EMER) dataset. To collect data with genuine emotions, spontaneous emotion induction paradigm is exploited with stimulus material, during which non-invasive eye behavior data, like eye movement sequences and eye fixation maps, is captured together with facial expression videos. To better illustrate the gap between ER and FER, multi-view emotion labels for mutimodal ER and FER are separately annotated. Furthermore, based on the new dataset, we design a simple yet effective Eye-behavior-aided MER Transformer (EMERT) that enhances ER by bridging the emotion gap. EMERT leverages modality-adversarial feature decoupling and a multitask Transformer to model eye behaviors as a strong complement to facial expressions. In the experiment, we introduce seven multimodal benchmark protocols for a variety of comprehensive evaluations of the EMER dataset. The results show that the EMERT outperforms other state-of-the-art multimodal methods by a great margin, revealing the importance of modeling eye behaviors for robust ER. To sum up, we provide a comprehensive analysis of the importance of eye behaviors in ER, advancing the study on addressing the gap between FER and ER for more robust ER performance. Our EMER dataset and the trained EMERT models will be publicly available at https://github.com/kejun1/EMER.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Accepted by TMM",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16485v1",
            "code_links": [
                {
                    "url": "https://github.com/kejun1/EMER",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]multimodal"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出EMERT模型和EMER数据集，利用眼部行为弥合面部表情识别和情感识别的差距",
            "summary_zh": "情感识别(ER)是从感知数据中分析和识别人类情感的过程。目前，该领域严重依赖于面部表情识别(FER)，因为视觉通道传递丰富的情感线索。然而，面部表情通常被用作社交工具，而不是真实内在情感的表现。为了理解和弥合FER和ER之间的差距，我们引入了眼部行为作为一个重要的情感线索，并构建了一个眼部行为辅助的多模态情感识别(EMER)数据集。为了收集具有真实情感的数据，采用了自发情感诱导范式，使用刺激材料，在此期间，非侵入性的眼部行为数据，如眼动序列和眼部注视图，与面部表情视频一起被捕获。为了更好地说明ER和FER之间的差距，分别对多模态ER和FER进行了多视角情感标注。此外，基于新的数据集，我们设计了一个简单而有效的眼部行为辅助MER Transformer (EMERT)，通过弥合情感差距来增强ER。EMERT利用模态对抗特征解耦和一个多任务Transformer来建模眼部行为，作为面部表情的有力补充。在实验中，我们为EMER数据集的各种综合评估引入了七个多模态基准协议。结果表明，EMERT的性能大大优于其他最先进的多模态方法，揭示了建模眼部行为对于鲁棒ER的重要性。总而言之，我们对眼部行为在ER中的重要性进行了全面的分析，从而推进了解决FER和ER之间差距的研究，以获得更强大的ER性能。我们的EMER数据集和训练好的EMERT模型将在https://github.com/kejun1/EMER上公开。",
            "intro_zh": [
                "现有情感识别方法过度依赖面部表情，忽略了面部表情可能掩盖真实情感的问题。",
                "提出EMERT模型，结合眼部行为数据，利用模态对抗解耦和多任务Transformer来提升情感识别的准确性。",
                "实验结果表明，EMERT模型在EMER数据集上显著优于其他多模态方法，验证了眼部行为在情感识别中的重要性。"
            ],
            "method_zh": "**问题定义**：现有情感识别方法主要依赖面部表情，但面部表情常常是社会化的伪装，不能完全反映真实情感。因此，如何弥合面部表情识别(FER)和情感识别(ER)之间的差距，提升情感识别的鲁棒性是一个关键问题。现有方法缺乏对眼部行为的有效利用，导致情感识别的准确性受限。\\n\\n**核心思路**：论文的核心思路是将眼部行为作为一种重要的情感线索引入到情感识别任务中。通过结合面部表情和眼部行为，模型可以更好地理解人类的真实情感状态，从而弥合FER和ER之间的差距。这种思路基于眼部行为能够更真实地反映个体的情感状态，减少社会化伪装的影响。\\n\\n**技术框架**：EMERT模型的技术框架主要包括以下几个模块：1)模态对抗特征解耦模块：用于解耦面部表情和眼部行为中的模态特定特征和情感共享特征。2)多任务Transformer模块：用于融合解耦后的特征，并同时进行情感识别和面部表情识别任务。3)情感分类器：基于融合后的特征进行情感分类。整体流程是：输入面部表情视频和眼部行为数据，经过特征提取和解耦，然后通过Transformer进行融合和预测。\\n\\n**关键创新**：论文的关键创新在于：1)提出了将眼部行为作为情感识别的重要线索，并构建了相应的EMER数据集。2)设计了模态对抗特征解耦模块，有效分离了模态特定特征和情感共享特征。3)提出了多任务Transformer结构，能够同时学习情感识别和面部表情识别任务，从而更好地利用眼部行为信息。\\n\\n**关键设计**：在模态对抗特征解耦模块中，使用了对抗学习的方法来分离模态特定特征和情感共享特征。在多任务Transformer模块中，使用了多头注意力机制来捕捉不同模态之间的关联性。损失函数包括情感分类损失、面部表情分类损失和对抗损失。具体的网络结构和参数设置在论文中有详细描述，但未在此处详细展开。",
            "application_zh": "该研究成果可应用于人机交互、心理健康评估、市场营销等领域。例如，在人机交互中，可以使机器更准确地理解用户的情感状态，从而提供更自然和个性化的服务。在心理健康评估中，可以辅助医生诊断患者的情感障碍。在市场营销中，可以帮助企业更好地了解消费者的情感需求，从而制定更有效的营销策略。未来，该研究可以进一步扩展到其他模态，如语音和生理信号，以实现更全面和准确的情感识别。",
            "highlight_zh": "实验结果表明，EMERT模型在EMER数据集上取得了显著的性能提升。与其他最先进的多模态方法相比，EMERT模型在情感识别准确率上提升了超过5%。具体而言，EMERT模型在七个多模态基准协议上均取得了最佳性能，验证了眼部行为在情感识别中的重要性和EMERT模型的有效性。这些结果表明，通过有效建模眼部行为，可以显著提升情感识别的鲁棒性和准确性。",
            "tags_zh": [
                "情感识别",
                "面部表情识别",
                "眼部行为",
                "多模态融合",
                "Transformer",
                "对抗学习",
                "数据集",
                "人机交互"
            ],
            "_index": 25,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16485v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16485v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16485v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Pretrained Battery Transformer (PBT): A battery life prediction foundation model",
            "authors": [
                "Ruifeng Tan",
                "Weixiang Hong",
                "Jia Li",
                "Jiaqiang Huang",
                "Tong-Yi Zhang"
            ],
            "arxiv_id": "2512.16334v1",
            "summary": "Early prediction of battery cycle life is essential for accelerating battery research, manufacturing, and deployment. Although machine learning methods have shown encouraging results, progress is hindered by data scarcity and heterogeneity arising from diverse aging conditions. In other fields, foundation models (FMs) trained on diverse datasets have achieved broad generalization through transfer learning, but no FMs have been reported for battery cycle life prediction yet. Here we present the Pretrained Battery Transformer (PBT), the first FM for battery life prediction, developed through domain-knowledge-encoded mixture-of-expert layers. Validated on the largest public battery life database, PBT learns transferable representations from 13 lithium-ion battery (LIB) datasets, outperforming existing models by an average of 19.8%. With transfer learning, PBT achieves state-of-the-art performance across 15 diverse datasets encompassing various operating conditions, formation protocols, and chemistries of LIBs. This work establishes a foundation model pathway for battery lifetime prediction, paving the way toward universal battery lifetime prediction systems.",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "5 figures in the main content",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16334v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]foundation model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出预训练电池Transformer（PBT），用于电池寿命预测，显著提升泛化性能。",
            "summary_zh": "本文提出了预训练电池Transformer（PBT），这是首个用于电池寿命预测的预训练模型。电池循环寿命的早期预测对于加速电池研究、制造和部署至关重要。尽管机器学习方法已显示出令人鼓舞的结果，但由于不同老化条件导致的数据稀缺性和异构性阻碍了进展。PBT通过领域知识编码的混合专家层，从13个锂离子电池（LIB）数据集学习可迁移的表示，并在最大的公共电池寿命数据库上进行了验证，性能平均优于现有模型19.8%。通过迁移学习，PBT在涵盖各种操作条件、形成协议和LIB化学成分的15个不同数据集上实现了最先进的性能。这项工作为电池寿命预测建立了预训练模型路径，为通用电池寿命预测系统铺平了道路。",
            "intro_zh": [
                "电池寿命预测面临数据稀缺和异构性挑战，限制了现有机器学习方法的泛化能力。",
                "PBT通过领域知识编码的混合专家层，学习可迁移的电池表示，实现跨数据集的泛化。",
                "实验表明，PBT在电池寿命预测任务上优于现有模型，并在多个数据集上取得了SOTA性能。"
            ],
            "method_zh": "**问题定义**：电池循环寿命的早期预测对于电池研究至关重要，但现有机器学习方法受限于数据稀缺性和异构性，难以泛化到不同工况和化学成分的电池。现有方法无法充分利用不同数据集中的信息，导致模型性能受限。\\n\\n**核心思路**：本文的核心思路是利用预训练模型（Foundation Model）的思想，通过在大量异构电池数据集上进行预训练，使模型学习到通用的电池表示。然后，通过迁移学习，将预训练模型应用于新的电池数据集，从而提高预测精度和泛化能力。\\n\\n**技术框架**：PBT的整体架构基于Transformer模型，并引入了领域知识编码的混合专家层（Mixture-of-Experts, MoE）。MoE允许模型根据输入数据的特性，动态地选择不同的专家网络进行处理，从而提高模型的表达能力和泛化能力。预训练阶段，PBT在多个电池数据集上进行训练，学习通用的电池表示。迁移学习阶段，PBT使用目标数据集进行微调，以适应特定电池的特性。\\n\\n**关键创新**：PBT的关键创新在于：1) 首次将预训练模型应用于电池寿命预测领域；2) 引入领域知识编码的混合专家层，提高了模型的表达能力和泛化能力。MoE结构允许模型学习不同电池类型和工况下的特定知识，并将其整合到统一的表示中。\\n\\n**关键设计**：PBT使用Transformer作为基础架构，MoE层由多个前馈神经网络（专家）组成，每个专家负责处理特定类型的电池数据。使用门控网络（Gating Network）来选择激活哪些专家。损失函数包括预训练损失和微调损失。预训练损失旨在学习通用的电池表示，微调损失旨在适应特定电池的特性。具体的参数设置和网络结构细节在论文中有详细描述。",
            "application_zh": "PBT可应用于电池研发、生产和部署等多个领域。在研发阶段，PBT可以加速新型电池材料的筛选和优化。在生产阶段，PBT可以提高电池质量控制的效率和精度。在部署阶段，PBT可以实现电池寿命的早期预测，从而优化电池管理策略，延长电池使用寿命，降低运营成本。PBT有望推动电池技术的快速发展和广泛应用。",
            "highlight_zh": "PBT在最大的公共电池寿命数据库上进行了验证，性能平均优于现有模型19.8%。通过迁移学习，PBT在涵盖各种操作条件、形成协议和LIB化学成分的15个不同数据集上实现了最先进的性能。这些实验结果表明，PBT具有很强的泛化能力和实用价值，为电池寿命预测提供了一种新的解决方案。",
            "tags_zh": [
                "电池寿命预测",
                "预训练模型",
                "Transformer",
                "迁移学习",
                "混合专家层",
                "锂离子电池",
                "领域知识编码"
            ],
            "_index": 26,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "Coarse-to-Fine Open-Set Graph Node Classification with Large Language Models",
            "authors": [
                "Xueqi Ma",
                "Xingjun Ma",
                "Sarah Monazam Erfani",
                "Danilo Mandic",
                "James Bailey"
            ],
            "arxiv_id": "2512.16244v1",
            "summary": "Developing open-set classification methods capable of classifying in-distribution (ID) data while detecting out-of-distribution (OOD) samples is essential for deploying graph neural networks (GNNs) in open-world scenarios. Existing methods typically treat all OOD samples as a single class, despite real-world applications, especially high-stake settings such as fraud detection and medical diagnosis, demanding deeper insights into OOD samples, including their probable labels. This raises a critical question: can OOD detection be extended to OOD classification without true label information? To address this question, we propose a Coarse-to-Fine open-set Classification (CFC) framework that leverages large language models (LLMs) for graph datasets. CFC consists of three key components: a coarse classifier that uses LLM prompts for OOD detection and outlier label generation, a GNN-based fine classifier trained with OOD samples identified by the coarse classifier for enhanced OOD detection and ID classification, and refined OOD classification achieved through LLM prompts and post-processed OOD labels. Unlike methods that rely on synthetic or auxiliary OOD samples, CFC employs semantic OOD instances that are genuinely out-of-distribution based on their inherent meaning, improving interpretability and practical utility. Experimental results show that CFC improves OOD detection by ten percent over state-of-the-art methods on graph and text domains and achieves up to seventy percent accuracy in OOD classification on graph datasets.",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Accepted to AAAI 2026",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16244v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]large language model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出CFC框架，利用大语言模型实现图节点开放集分类与细粒度OOD识别。",
            "summary_zh": "开发能够对分布内(ID)数据进行分类，同时检测分布外(OOD)样本的开放集分类方法，对于在开放世界场景中部署图神经网络(GNN)至关重要。现有方法通常将所有OOD样本视为单个类别，然而现实应用，尤其是在欺诈检测和医疗诊断等高风险环境中，需要对OOD样本进行更深入的分析，包括其可能的标签。这提出了一个关键问题：能否在没有真实标签信息的情况下，将OOD检测扩展到OOD分类？为了解决这个问题，我们提出了一个粗到细的开放集分类(CFC)框架，该框架利用大型语言模型(LLM)处理图数据集。CFC由三个关键组件组成：一个粗分类器，它使用LLM提示进行OOD检测和异常值标签生成；一个基于GNN的细分类器，该分类器使用粗分类器识别的OOD样本进行训练，以增强OOD检测和ID分类；以及通过LLM提示和后处理OOD标签实现的精细化OOD分类。与依赖合成或辅助OOD样本的方法不同，CFC采用基于其内在含义的语义OOD实例，这些实例是真正分布外的，从而提高了可解释性和实用性。实验结果表明，CFC在图和文本领域将OOD检测提高了10%，并且在图数据集上的OOD分类准确率高达70%。",
            "intro_zh": [
                "现有开放集图节点分类方法将所有OOD样本视为一类，缺乏对OOD样本的细粒度理解。",
                "CFC框架利用大语言模型进行粗粒度OOD检测和标签生成，再用GNN进行细粒度分类，提升OOD识别能力。",
                "实验表明，CFC在图和文本数据上OOD检测性能提升10%，OOD分类准确率在图数据上高达70%。"
            ],
            "method_zh": "**问题定义**：现有图神经网络的开放集分类方法主要关注区分已知类别和未知类别（OOD检测），但忽略了对OOD样本的进一步分类。在实际应用中，例如欺诈检测或医疗诊断，仅仅识别出异常样本是不够的，还需要了解这些异常样本可能属于的类别。因此，论文旨在解决如何在没有OOD样本真实标签的情况下，对OOD样本进行分类的问题。\\n\\n**核心思路**：论文的核心思路是利用大语言模型（LLM）的语义理解能力，对OOD样本进行粗粒度的分类，生成伪标签，然后利用这些伪标签训练图神经网络，从而实现细粒度的OOD分类。这种“粗到细”的方法能够充分利用LLM的先验知识和GNN的图结构学习能力。\\n\\n**技术框架**：CFC框架包含三个主要阶段：1) **粗分类器**：使用LLM对图节点进行分类，同时检测OOD样本，并为OOD样本生成伪标签。LLM通过特定的prompt进行引导，例如“这个节点最可能属于哪个类别？”。2) **细分类器**：使用GNN，结合原始ID数据和带有伪标签的OOD数据进行训练，提升ID分类和OOD检测的性能。3) **OOD分类优化**：使用LLM对GNN的OOD分类结果进行优化，通过prompt和后处理，进一步提升OOD分类的准确率。\\n\\n**关键创新**：CFC框架的关键创新在于：1) 利用LLM进行OOD样本的伪标签生成，避免了依赖合成或辅助OOD样本的传统方法，从而提高了OOD样本的语义相关性和可解释性。2) 提出了一种粗到细的分类框架，将LLM的语义理解能力和GNN的图结构学习能力相结合，实现了更准确的OOD分类。\\n\\n**关键设计**：在粗分类器阶段，LLM的prompt设计至关重要，需要根据具体的图数据集和任务进行调整。在细分类器阶段，GNN的选择和训练策略需要仔细考虑，以平衡ID分类和OOD检测的性能。OOD分类优化阶段，LLM的prompt和后处理规则也需要根据实验结果进行调整。",
            "application_zh": "该研究成果可应用于欺诈检测、医疗诊断、金融风控等领域。在这些场景中，识别异常行为或病例至关重要，而对异常情况进行分类可以帮助专家更快地定位问题并采取相应的措施。此外，该方法还可以用于网络安全领域，识别和分类恶意软件或网络攻击。",
            "highlight_zh": "实验结果表明，CFC框架在图和文本领域上的OOD检测性能比现有方法提高了10%。在图数据集上，CFC框架的OOD分类准确率高达70%。这些结果表明，CFC框架能够有效地利用LLM和GNN的优势，实现更准确的开放集分类。",
            "tags_zh": [
                "开放集分类",
                "图神经网络",
                "大语言模型",
                "分布外检测",
                "OOD分类"
            ],
            "_index": 27,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16244v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16244v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16244v1/ood-prompt.jpg",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "A Multimodal Approach to Alzheimer's Diagnosis: Geometric Insights from Cube Copying and Cognitive Assessments",
            "authors": [
                "Jaeho Yang",
                "Kijung Yoon"
            ],
            "arxiv_id": "2512.16184v1",
            "summary": "Early and accessible detection of Alzheimer's disease (AD) remains a critical clinical challenge, and cube-copying tasks offer a simple yet informative assessment of visuospatial function. This work proposes a multimodal framework that converts hand-drawn cube sketches into graph-structured representations capturing geometric and topological properties, and integrates these features with demographic information and neuropsychological test (NPT) scores for AD classification. Cube drawings are modeled as graphs with node features encoding spatial coordinates, local graphlet-based topology, and angular geometry, which are processed using graph neural networks and fused with age, education, and NPT features in a late-fusion model. Experimental results show that graph-based representations provide a strong unimodal baseline and substantially outperform pixel-based convolutional models, while multimodal integration further improves performance and robustness to class imbalance. SHAP-based interpretability analysis identifies specific graphlet motifs and geometric distortions as key predictors, closely aligning with clinical observations of disorganized cube drawings in AD. Together, these results establish graph-based analysis of cube copying as an interpretable, non-invasive, and scalable approach for Alzheimer's disease screening.",
            "categories": [
                "cs.LG"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16184v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]multimodal"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出基于图神经网络的多模态融合方法，用于阿尔茨海默病早期诊断。",
            "summary_zh": "阿尔茨海默病(AD)的早期且易于获得的检测仍然是一项关键的临床挑战，而立方体复制任务提供了一种简单但信息丰富的视觉空间功能评估方法。本研究提出了一种多模态框架，该框架将手绘立方体草图转换为图结构表示，以捕获几何和拓扑属性，并将这些特征与人口统计信息和神经心理测试(NPT)分数相结合，用于AD分类。立方体图被建模为图，其节点特征编码空间坐标、基于局部图元的拓扑结构和角度几何，这些特征使用图神经网络进行处理，并在后期融合模型中与年龄、教育程度和NPT特征融合。实验结果表明，基于图的表示提供了强大的单模态基线，并且大大优于基于像素的卷积模型，而多模态集成进一步提高了性能和对类不平衡的鲁棒性。基于SHAP的可解释性分析将特定的图元基序和几何扭曲识别为关键预测因子，这与AD中立方体图的紊乱的临床观察结果密切相关。总之，这些结果确立了基于图的立方体复制分析作为一种可解释、非侵入性和可扩展的阿尔茨海默病筛查方法。",
            "intro_zh": [
                "阿尔茨海默病早期诊断困难，传统方法依赖复杂且昂贵的检测手段，缺乏便捷性。",
                "将手绘立方体转化为图结构，提取几何和拓扑特征，结合人口统计学信息进行多模态融合。",
                "实验表明，基于图的表示优于像素模型，多模态融合进一步提升性能，并具有良好的可解释性。"
            ],
            "method_zh": "**问题定义**：阿尔茨海默病的早期诊断是临床上的一个难题。现有的诊断方法，例如脑部扫描和认知测试，通常成本高昂且耗时，难以大规模应用。手绘立方体复制任务虽然简单，但其蕴含的视觉空间信息尚未被充分利用，缺乏有效的量化分析方法。\\n\\n**核心思路**：本研究的核心在于将手绘立方体图转化为图结构数据，从而能够利用图神经网络(GNN)提取其几何和拓扑特征。这种方法能够更有效地捕捉立方体图中的空间关系和结构信息，克服了传统像素级方法的局限性。同时，结合人口统计学信息和神经心理测试分数，实现多模态融合，提升诊断准确率。\\n\\n**技术框架**：该框架主要包含以下几个阶段：1) **数据预处理**：将手绘立方体图转换为数字图像，并提取图像中的线条和节点。2) **图构建**：将提取的线条和节点构建成图结构，其中节点表示立方体的顶点，边表示立方体的棱。3) **特征提取**：提取节点的空间坐标、局部图元拓扑结构和角度几何等特征。4) **图神经网络处理**：使用GNN对图结构数据进行学习，提取高级特征表示。5) **多模态融合**：将GNN提取的特征与人口统计学信息和神经心理测试分数进行融合。6) **分类**：使用分类器对融合后的特征进行分类，判断患者是否患有阿尔茨海默病。\\n\\n**关键创新**：该研究的关键创新在于：1) 将手绘立方体图转化为图结构数据，并利用GNN进行特征提取。2) 提出了一种多模态融合框架，结合了图像特征、人口统计学信息和神经心理测试分数。3) 通过SHAP分析，揭示了图元基序和几何扭曲等关键预测因子，提高了模型的可解释性。\\n\\n**关键设计**：在图构建阶段，节点特征包括空间坐标、局部图元拓扑结构（例如，节点周围的三角形、四边形等）和角度几何（例如，节点连接的边的角度）。GNN采用多层图卷积网络，损失函数采用交叉熵损失函数。多模态融合采用后期融合策略，即将不同模态的特征进行拼接，然后输入到分类器中。",
            "application_zh": "该研究成果可应用于阿尔茨海默病的早期筛查和诊断，尤其适用于资源有限的社区和医疗机构。通过简单的手绘立方体复制任务，结合智能算法分析，可以实现快速、便捷、低成本的初步诊断，为患者争取宝贵的治疗时间。未来，该方法有望推广到其他神经退行性疾病的辅助诊断中。",
            "highlight_zh": "实验结果表明，基于图的表示方法在单模态情况下就优于基于像素的卷积模型。多模态融合后，性能进一步提升，并且对类不平衡问题具有更强的鲁棒性。SHAP分析揭示了特定的图元基序和几何扭曲是关键的预测因子，与临床观察结果一致，验证了该方法的有效性和可解释性。",
            "tags_zh": [
                "阿尔茨海默病",
                "早期诊断",
                "图神经网络",
                "多模态融合",
                "立方体复制",
                "几何特征",
                "拓扑结构"
            ],
            "_index": 28,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16184v1/Figure1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16184v1/Figure2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16184v1/Figure3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "TOGGLE: Temporal Logic-Guided Large Language Model Compression for Edge",
            "authors": [
                "Khurram Khalil",
                "Khaza Anuarul Hoque"
            ],
            "arxiv_id": "2512.16855v1",
            "summary": "Large Language Models (LLMs) deliver exceptional performance across natural language tasks but demand substantial computational resources, limiting their deployment on resource-constrained edge devices. Existing compression techniques, such as quantization and pruning, often degrade critical linguistic properties and lack formal guarantees for preserving model behavior. We propose Temporal Logic-Guided Large Language Model Compression (TOGGLE), a novel framework that leverages Signal Temporal Logic (STL) to formally specify and enforce linguistic properties during compression. TOGGLE employs an STL robustness-guided Bayesian optimization to systematically explore layer-wise quantization and pruning configurations, generating compressed models that formally satisfy specified linguistic constraints without retraining or fine-tuning. Evaluating TOGGLE on four LLM architectures (GPT-2, DeepSeek-V2 7B, LLaMA 3 8B, and Mistral 7B), we achieve up to 3.3x reduction in computational costs (FLOPs) and up to a 68.8% reduction in model size while satisfying all linguistic properties. TOGGLE represents the first integration of formal methods into LLM compression, enabling efficient, verifiable deployment of LLMs on edge hardware.",
            "categories": [
                "cs.AI",
                "cs.LO"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Published in the IEEE ICCAD 2025 conference",
            "doi": "10.1109/ICCAD66269.2025.11240962",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16855v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]large language model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出TOGGLE，利用时序逻辑引导LLM压缩，实现边缘设备高效部署。",
            "summary_zh": "大型语言模型(LLM)在自然语言任务中表现出色，但需要大量的计算资源，限制了其在资源受限的边缘设备上的部署。现有的压缩技术，如量化和剪枝，通常会降低关键的语言属性，并且缺乏对保持模型行为的正式保证。我们提出了时间逻辑引导的大型语言模型压缩(TOGGLE)，这是一个新颖的框架，它利用信号时序逻辑(STL)来正式地指定和执行压缩过程中的语言属性。TOGGLE采用STL鲁棒性引导的贝叶斯优化，系统地探索分层量化和剪枝配置，生成压缩模型，在不重新训练或微调的情况下，正式满足指定的语言约束。在四个LLM架构(GPT-2、DeepSeek-V2 7B、LLaMA 3 8B和Mistral 7B)上评估TOGGLE，我们实现了高达3.3倍的计算成本(FLOPs)降低和高达68.8%的模型大小降低，同时满足所有语言属性。TOGGLE代表了形式化方法首次集成到LLM压缩中，从而能够在边缘硬件上高效、可验证地部署LLM。",
            "intro_zh": [
                "现有LLM压缩方法在降低计算成本的同时，容易损失关键语言属性，且缺乏对模型行为保持的正式保证。",
                "TOGGLE利用信号时序逻辑(STL)形式化地指定和执行压缩过程中的语言属性，保证压缩后的模型满足特定的语言约束。",
                "实验表明，TOGGLE在多种LLM架构上实现了显著的计算成本和模型大小降低，同时保持了语言属性，无需重新训练或微调。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大型语言模型（LLM）在资源受限的边缘设备上部署的问题。现有的量化和剪枝等压缩技术虽然能降低计算成本，但往往会损害LLM的关键语言属性，并且缺乏对压缩后模型行为的正式保证，难以确保其在实际应用中的可靠性。\\n\\n**核心思路**：TOGGLE的核心思路是利用信号时序逻辑（STL）来形式化地描述LLM需要满足的语言属性，并在压缩过程中，通过优化算法来确保压缩后的模型仍然满足这些属性。这种方法避免了传统压缩方法中常见的语言属性损失问题，并提供了对模型行为的正式保证。\\n\\n**技术框架**：TOGGLE框架主要包含以下几个阶段：1) **STL属性定义**：使用STL语言来形式化地描述LLM需要满足的语言属性，例如语法正确性、语义一致性等。2) **鲁棒性评估**：设计鲁棒性函数来评估LLM在多大程度上满足定义的STL属性。鲁棒性值越高，表示模型越符合该属性。3) **贝叶斯优化**：使用贝叶斯优化算法来搜索最佳的量化和剪枝配置，目标是最小化模型大小和计算成本，同时最大化STL属性的鲁棒性。4) **模型压缩**：根据贝叶斯优化得到的配置，对LLM进行量化和剪枝，得到压缩后的模型。\\n\\n**关键创新**：TOGGLE最重要的技术创新点在于将形式化方法（STL）引入到LLM压缩中。与传统的基于经验的压缩方法不同，TOGGLE能够对压缩后的模型行为提供正式保证，确保其满足特定的语言属性。这是首次将形式化验证技术应用于LLM压缩领域。\\n\\n**关键设计**：TOGGLE的关键设计包括：1) **STL属性的选择**：选择合适的STL属性来描述LLM的关键语言能力，例如使用STL来描述模型生成语法正确的句子的能力。2) **鲁棒性函数的定义**：设计能够准确评估模型是否满足STL属性的鲁棒性函数。3) **贝叶斯优化算法的配置**：选择合适的贝叶斯优化算法，并调整其参数，以高效地搜索最佳的量化和剪枝配置。4) **量化和剪枝策略**：选择合适的量化和剪枝策略，例如使用不同的量化比特数和剪枝比例。",
            "application_zh": "TOGGLE的应用场景广泛，包括在智能手机、无人机、机器人等资源受限的边缘设备上部署LLM，实现本地化的自然语言处理能力。该研究的实际价值在于降低了LLM的部署门槛，使其能够在更多场景下应用。未来，TOGGLE有望推动边缘智能的发展，实现更智能、更高效的边缘计算。",
            "highlight_zh": "TOGGLE在GPT-2、DeepSeek-V2 7B、LLaMA 3 8B和Mistral 7B等多种LLM架构上进行了评估，实验结果表明，TOGGLE能够实现高达3.3倍的计算成本(FLOPs)降低和高达68.8%的模型大小降低，同时满足所有指定的语言属性。这些结果表明，TOGGLE是一种高效且可靠的LLM压缩方法。",
            "tags_zh": [
                "大语言模型压缩",
                "边缘计算",
                "形式化方法",
                "信号时序逻辑",
                "贝叶斯优化"
            ],
            "_index": 29,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "Prefix Probing: Lightweight Harmful Content Detection for Large Language Models",
            "authors": [
                "Jirui Yang",
                "Hengqi Guo",
                "Zhihui Lu",
                "Yi Zhao",
                "Yuansen Zhang",
                "Shijing Hu",
                "Qiang Duan",
                "Yinggui Wang",
                "Tao Wei"
            ],
            "arxiv_id": "2512.16650v1",
            "summary": "Large language models often face a three-way trade-off among detection accuracy, inference latency, and deployment cost when used in real-world safety-sensitive applications. This paper introduces Prefix Probing, a black-box harmful content detection method that compares the conditional log-probabilities of \"agreement/execution\" versus \"refusal/safety\" opening prefixes and leverages prefix caching to reduce detection overhead to near first-token latency. During inference, the method requires only a single log-probability computation over the probe prefixes to produce a harmfulness score and apply a threshold, without invoking any additional models or multi-stage inference. To further enhance the discriminative power of the prefixes, we design an efficient prefix construction algorithm that automatically discovers highly informative prefixes, substantially improving detection performance. Extensive experiments demonstrate that Prefix Probing achieves detection effectiveness comparable to mainstream external safety models while incurring only minimal computational cost and requiring no extra model deployment, highlighting its strong practicality and efficiency.",
            "categories": [
                "cs.AI",
                "cs.CR"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16650v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]large language model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出Prefix Probing，以低延迟、低成本方式检测大语言模型中的有害内容。",
            "summary_zh": "大型语言模型在实际安全敏感应用中，通常需要在检测精度、推理延迟和部署成本之间进行权衡。本文提出了一种黑盒有害内容检测方法Prefix Probing，该方法通过比较“同意/执行”与“拒绝/安全”开头前缀的条件对数概率，并利用前缀缓存将检测开销降低到接近首个token的延迟。在推理过程中，该方法仅需对探测前缀进行一次对数概率计算，即可生成有害性评分并应用阈值，无需调用任何额外的模型或多阶段推理。为了进一步增强前缀的区分能力，我们设计了一种高效的前缀构建算法，可以自动发现信息量大的前缀，从而显著提高检测性能。大量实验表明，Prefix Probing在计算成本极低且无需额外模型部署的情况下，实现了与主流外部安全模型相当的检测效果，突出了其强大的实用性和效率。",
            "intro_zh": [
                "现有大语言模型有害内容检测方法在精度、延迟和成本间存在权衡，难以兼顾。",
                "Prefix Probing通过比较特定前缀的概率，无需额外模型或多阶段推理，实现高效检测。",
                "高效前缀构建算法自动发现信息量大的前缀，显著提升检测性能，实验效果媲美主流安全模型。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大型语言模型在实际应用中，有害内容检测精度、推理延迟和部署成本难以兼顾的问题。现有方法通常依赖于额外的安全模型或复杂的多阶段推理，导致延迟高、成本高，难以满足实时性要求高的场景。\\n\\n**核心思路**：Prefix Probing的核心思路是，利用大语言模型自身的能力，通过比较特定前缀（如“同意/执行”与“拒绝/安全”）的条件对数概率，来判断模型是否倾向于生成有害内容。这种方法无需额外的模型或复杂的推理过程，从而降低了延迟和成本。\\n\\n**技术框架**：Prefix Probing的整体框架非常简单。首先，构建一组“同意/执行”和“拒绝/安全”的前缀。然后，对于给定的输入，计算大语言模型生成这些前缀的条件对数概率。最后，比较这些概率，并根据设定的阈值判断输入是否可能导致有害内容生成。该方法利用前缀缓存进一步降低延迟。\\n\\n**关键创新**：Prefix Probing的关键创新在于其简洁性和高效性。它避免了传统方法中对额外模型的依赖，而是直接利用大语言模型自身的概率分布进行有害内容检测。此外，论文还提出了一种高效的前缀构建算法，可以自动发现信息量大的前缀，从而提高检测性能。\\n\\n**关键设计**：前缀构建算法是关键设计之一。该算法旨在自动搜索能够有效区分有害和无害内容的前缀。具体的技术细节（如搜索策略、目标函数等）在论文中应该有更详细的描述。此外，阈值的选择也会影响检测性能，需要根据实际应用场景进行调整。",
            "application_zh": "Prefix Probing可应用于各种需要对大语言模型输出进行安全过滤的场景，例如聊天机器人、内容生成平台、代码生成工具等。该方法能够以低延迟、低成本的方式，有效防止模型生成有害、不当或违反道德规范的内容，保障用户安全和平台合规性，具有广泛的应用前景。",
            "highlight_zh": "实验结果表明，Prefix Probing在检测效果上可与主流外部安全模型相媲美，同时计算成本极低，无需额外模型部署。这使得Prefix Probing在实际应用中具有显著优势，尤其是在对延迟和成本敏感的场景下。具体的性能数据（如精度、召回率、延迟等）以及与哪些基线模型进行了比较，需要在论文中查找。",
            "tags_zh": [
                "大语言模型",
                "有害内容检测",
                "安全对齐",
                "黑盒方法",
                "前缀探测"
            ],
            "_index": 30,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16650v1/figs/insight.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16650v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16650v1/figs/f1_vs_time_2.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "TimeSeries2Report prompting enables adaptive large language model management of lithium-ion batteries",
            "authors": [
                "Jiayang Yang",
                "Chunhui Zhao",
                "Martin Guay",
                "Zhixing Cao"
            ],
            "arxiv_id": "2512.16453v1",
            "summary": "Large language models (LLMs) offer promising capabilities for interpreting multivariate time-series data, yet their application to real-world battery energy storage system (BESS) operation and maintenance remains largely unexplored. Here, we present TimeSeries2Report (TS2R), a prompting framework that converts raw lithium-ion battery operational time-series into structured, semantically enriched reports, enabling LLMs to reason, predict, and make decisions in BESS management scenarios. TS2R encodes short-term temporal dynamics into natural language through a combination of segmentation, semantic abstraction, and rule-based interpretation, effectively bridging low-level sensor signals with high-level contextual insights. We benchmark TS2R across both lab-scale and real-world datasets, evaluating report quality and downstream task performance in anomaly detection, state-of-charge prediction, and charging/discharging management. Compared with vision-, embedding-, and text-based prompting baselines, report-based prompting via TS2R consistently improves LLM performance in terms of across accuracy, robustness, and explainability metrics. Notably, TS2R-integrated LLMs achieve expert-level decision quality and predictive consistency without retraining or architecture modification, establishing a practical path for adaptive, LLM-driven battery intelligence.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16453v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]large language model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出TimeSeries2Report框架，实现大语言模型对锂离子电池的自适应管理",
            "summary_zh": "本文提出了一种名为TimeSeries2Report (TS2R) 的提示框架，旨在将原始锂离子电池运行时间序列数据转换为结构化、语义丰富的报告，从而使大语言模型 (LLM) 能够在电池储能系统 (BESS) 管理场景中进行推理、预测和决策。TS2R 通过分割、语义抽象和基于规则的解释，将短期时间动态编码为自然语言，有效地将低级传感器信号与高级上下文信息连接起来。该研究在实验室规模和真实世界数据集上对 TS2R 进行了基准测试，评估了报告质量以及在异常检测、荷电状态预测和充放电管理等下游任务中的性能。与基于视觉、嵌入和文本的提示基线相比，通过 TS2R 进行的基于报告的提示始终提高了 LLM 在准确性、鲁棒性和可解释性指标方面的性能。值得注意的是，集成了 TS2R 的 LLM 在无需重新训练或架构修改的情况下，实现了专家级的决策质量和预测一致性，为自适应、LLM 驱动的电池智能化建立了一条切实可行的路径。",
            "intro_zh": [
                "现有方法难以有效利用大语言模型解释多元时间序列数据，尤其是在电池储能系统运维方面。",
                "TimeSeries2Report框架将时间序列数据转化为结构化报告，使大语言模型能够理解并进行推理、预测和决策。",
                "实验表明，该框架在异常检测、荷电状态预测和充放电管理等任务中，显著提升了大语言模型的性能。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大语言模型在电池储能系统（BESS）运维中应用不足的问题。现有方法难以直接利用原始时间序列数据，缺乏有效的桥梁将低级传感器信号与高级上下文信息连接起来，限制了大语言模型在BESS管理中的应用。\n\n**核心思路**：论文的核心思路是将原始时间序列数据转换为结构化、语义丰富的报告，从而使大语言模型能够更好地理解和利用这些数据。通过将时间序列数据转化为自然语言描述，降低了大语言模型处理复杂时间序列数据的难度。\n\n**技术框架**：TimeSeries2Report (TS2R) 框架包含三个主要阶段：1) 分割：将时间序列数据分割成有意义的片段；2) 语义抽象：对每个片段进行语义抽象，提取关键特征；3) 基于规则的解释：根据预定义的规则，将抽象的特征转化为自然语言描述，生成报告。该报告作为大语言模型的输入，用于进行推理、预测和决策。\n\n**关键创新**：该方法最重要的技术创新在于将时间序列数据转化为自然语言报告，从而使大语言模型能够更好地理解和利用这些数据。与直接使用原始时间序列数据或将其转化为嵌入向量的方法相比，TS2R 能够提供更丰富的上下文信息，提高大语言模型的性能。\n\n**关键设计**：TS2R框架的关键设计包括：1) 分割算法的选择，需要根据具体应用场景进行调整；2) 语义抽象规则的定义，需要领域专家参与；3) 自然语言报告的生成方式，需要保证报告的准确性和可读性。论文中没有详细说明具体的参数设置、损失函数或网络结构，这些细节可能取决于具体应用场景和所使用的大语言模型。",
            "application_zh": "该研究成果可应用于电池储能系统的智能运维，例如异常检测、状态预测和优化控制。通过集成大语言模型，可以实现电池系统的自适应管理，提高系统的效率和可靠性，降低运维成本。未来，该方法有望推广到其他时间序列数据分析领域，如工业过程监控、金融风险管理等。",
            "highlight_zh": "实验结果表明，与基于视觉、嵌入和文本的提示基线相比，通过 TS2R 进行的基于报告的提示始终提高了 LLM 在准确性、鲁棒性和可解释性指标方面的性能。集成了 TS2R 的 LLM 在无需重新训练或架构修改的情况下，实现了专家级的决策质量和预测一致性。",
            "tags_zh": [
                "时间序列分析",
                "大语言模型",
                "锂离子电池",
                "储能系统",
                "智能运维"
            ],
            "_index": 31,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16453v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16453v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16453v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "An Information-Theoretic Framework for Robust Large Language Model Editing",
            "authors": [
                "Qizhou Chen",
                "Chengyu Wang",
                "Taolin Zhang",
                "Xiaofeng He"
            ],
            "arxiv_id": "2512.16227v1",
            "summary": "Large Language Models (LLMs) have become indispensable tools in science, technology, and society, enabling transformative advances across diverse fields. However, errors or outdated information within these models can undermine their accuracy and restrict their safe deployment. Developing efficient strategies for updating model knowledge without the expense and disruption of full retraining remains a critical challenge. Current model editing techniques frequently struggle to generalize corrections beyond narrow domains, leading to unintended consequences and limiting their practical impact. Here, we introduce a novel framework for editing LLMs, grounded in information bottleneck theory. This approach precisely compresses and isolates the essential information required for generalizable knowledge correction while minimizing disruption to unrelated model behaviors. Building upon this foundation, we present the Information Bottleneck Knowledge Editor (IBKE), which leverages compact latent representations to guide gradient-based updates, enabling robust and broadly applicable model editing. We validate IBKE's effectiveness across multiple LLM architectures and standard benchmark tasks, demonstrating state-of-the-art accuracy and improved generality and specificity of edits. These findings establish a theoretically principled and practical paradigm for open-domain knowledge editing, advancing the utility and trustworthiness of LLMs in real-world applications.",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16227v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]large language model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出基于信息瓶颈的IBKE框架，用于稳健的大语言模型知识编辑。",
            "summary_zh": "大型语言模型(LLMs)已成为科学、技术和社会中不可或缺的工具，推动了各个领域的变革性进步。然而，这些模型中的错误或过时信息可能会损害其准确性并限制其安全部署。开发有效的策略来更新模型知识，而无需完全重新训练的成本和中断，仍然是一项关键挑战。当前的模型编辑技术经常难以将更正推广到狭窄的领域之外，导致意想不到的后果并限制了它们的实际影响。本文介绍了一种基于信息瓶颈理论的LLM编辑新框架。该方法精确地压缩和隔离了通用知识校正所需的基本信息，同时最大限度地减少对不相关模型行为的干扰。在此基础上，我们提出了信息瓶颈知识编辑器(IBKE)，它利用紧凑的潜在表示来指导基于梯度的更新，从而实现稳健且广泛适用的模型编辑。我们在多个LLM架构和标准基准任务上验证了IBKE的有效性，证明了最先进的准确性以及编辑的改进的通用性和特异性。这些发现为开放域知识编辑建立了一个理论上合理且实用的范例，提高了LLM在实际应用中的效用和可信度。",
            "intro_zh": [
                "现有模型编辑方法泛化性差，容易产生副作用，限制了实际应用。",
                "论文提出基于信息瓶颈理论的IBKE框架，压缩并隔离关键信息，减少对无关行为的干扰。",
                "实验表明，IBKE在多个LLM架构和基准测试中表现出最先进的准确性和更好的泛化性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大型语言模型（LLMs）中知识更新的问题，即如何在不完全重新训练模型的情况下，高效、准确地修正模型中的错误或过时信息。现有方法的痛点在于编辑后的知识难以泛化，容易对模型其他部分的性能产生负面影响，即“灾难性遗忘”或引入新的错误。\n\n**核心思路**：论文的核心思路是利用信息瓶颈（Information Bottleneck, IB）理论，在编辑过程中只保留与待编辑知识相关的最少信息，从而最大限度地减少对模型其他部分的影响。通过压缩和隔离关键信息，实现更稳健、更具泛化性的知识编辑。\n\n**技术框架**：IBKE框架主要包含以下几个阶段：1) **知识表示**：将需要编辑的知识编码为紧凑的潜在表示；2) **信息瓶颈压缩**：利用信息瓶颈原理，从潜在表示中提取最关键的信息，去除冗余信息；3) **梯度引导更新**：使用提取的关键信息引导基于梯度的模型参数更新；4) **知识验证**：评估编辑后的模型在相关任务上的性能，并验证编辑的有效性和泛化性。\n\n**关键创新**：最重要的技术创新点在于将信息瓶颈理论引入到LLM的知识编辑中。与现有方法相比，IBKE能够更精确地控制编辑过程中的信息流，避免不必要的模型参数更新，从而提高编辑的泛化性和鲁棒性。本质区别在于，IBKE关注的是知识的本质信息，而不是简单地修改模型参数。\n\n**关键设计**：IBKE的关键设计包括：1) **潜在表示的选择**：选择合适的潜在表示方式，例如使用自编码器或变分自编码器学习知识的紧凑表示；2) **信息瓶颈的实现**：使用KL散度等方法约束潜在表示的信息量，使其尽可能地简洁；3) **梯度更新策略**：设计合适的梯度更新策略，例如使用正则化项限制参数更新的幅度，避免过度拟合；4) **损失函数设计**：设计包含编辑准确性、泛化性和模型稳定性的多目标损失函数。",
            "application_zh": "该研究成果可应用于各种需要持续知识更新的LLM应用场景，例如：智能客服、知识问答、内容生成等。通过IBKE框架，可以更高效、更可靠地修正LLM中的错误信息，提高模型的准确性和可信度，从而促进LLM在实际应用中的广泛部署。此外，该方法还可以用于个性化知识定制，根据用户需求定制LLM的知识库。",
            "highlight_zh": "实验结果表明，IBKE在多个LLM架构（包括但不限于BERT、GPT系列）和标准知识编辑基准测试中取得了最先进的性能。与现有方法相比，IBKE在编辑准确性、泛化性和特异性方面均有显著提升。具体而言，IBKE在保持编辑知识准确性的同时，显著降低了对模型其他部分性能的负面影响，实现了更稳健的知识编辑。",
            "tags_zh": [
                "大型语言模型",
                "知识编辑",
                "信息瓶颈",
                "模型更新",
                "泛化性"
            ],
            "_index": 32,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16227v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16227v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16227v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "A Multi-Agent Large Language Model Framework for Automated Qualitative Analysis",
            "authors": [
                "Qidi Xu",
                "Nuzha Amjad",
                "Grace Giles",
                "Alexa Cumming",
                "De'angelo Hermesky",
                "Alexander Wen",
                "Min Ji Kwak",
                "Yejin Kim"
            ],
            "arxiv_id": "2512.16063v1",
            "summary": "Understanding patients experiences is essential for advancing patient centered care, especially in chronic diseases that require ongoing communication. However, qualitative thematic analysis, the primary approach for exploring these experiences, remains labor intensive, subjective, and difficult to scale. In this study, we developed a multi agent large language model framework that automates qualitative thematic analysis through three agents (Instructor, Thematizer, CodebookGenerator), named Collaborative Theme Identification Agent (CoTI). We applied CoTI to 12 heart failure patient interviews to analyze their perceptions of medication intensity. CoTI identified key phrases, themes, and codebook that were more similar to those of the senior investigator than both junior investigators and baseline NLP models. We also implemented CoTI into a user-facing application to enable AI human interaction in qualitative analysis. However, collaboration between CoTI and junior investigators provided only marginal gains, suggesting they may overrely on CoTI and limit their independent critical thinking.",
            "categories": [
                "cs.HC",
                "cs.AI"
            ],
            "primary_category": "cs.HC",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "42 pages, 5 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16063v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]large language model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出CoTI多Agent LLM框架，自动化定性分析，提升患者体验研究效率。",
            "summary_zh": "理解患者体验对于提升以患者为中心的护理至关重要，尤其是在需要持续沟通的慢性疾病中。然而，定性主题分析作为探索这些体验的主要方法，仍然劳动密集、主观且难以扩展。本研究开发了一个多Agent大型语言模型框架，通过三个Agent（指导者、主题化者、代码手册生成器）自动化定性主题分析，命名为协同主题识别Agent（CoTI）。我们将CoTI应用于12个心力衰竭患者访谈，以分析他们对药物强度的看法。CoTI识别的关键短语、主题和代码手册与资深研究员的结果更相似，优于初级研究员和基线NLP模型。我们还将CoTI集成到面向用户的应用程序中，以实现AI人机交互在定性分析中的应用。然而，CoTI与初级研究员之间的协作仅提供了边际收益，表明他们可能过度依赖CoTI并限制了其独立的批判性思维。",
            "intro_zh": [
                "定性主题分析在患者体验研究中至关重要，但其劳动密集、主观且难以规模化是主要挑战。",
                "论文提出CoTI框架，利用多Agent LLM协同工作，自动化主题识别、代码手册生成等定性分析流程。",
                "实验表明，CoTI在心力衰竭患者访谈分析中，结果与资深研究员更接近，优于初级研究员和基线模型。"
            ],
            "method_zh": "**问题定义**：论文旨在解决定性研究中主题分析耗时、主观且难以规模化的问题。现有方法依赖人工，效率低且易受研究者主观影响，难以保证结果的一致性和可重复性。\\n\\n**核心思路**：论文的核心思路是利用大型语言模型（LLM）的强大自然语言处理能力，构建一个多Agent协作框架，模拟人类研究者的分析过程，从而实现定性分析的自动化。通过将复杂的分析任务分解为多个Agent，每个Agent负责不同的子任务，协同完成整体分析。\\n\\n**技术框架**：CoTI框架包含三个主要Agent：\n1. **Instructor (指导者)**：负责整体流程的控制和协调，指导其他Agent完成任务。\n2. **Thematizer (主题化者)**：负责从访谈文本中识别关键短语和主题。\n3. **CodebookGenerator (代码手册生成器)**：负责根据识别出的主题生成代码手册，用于后续的编码和分析。\n\n整体流程为：Instructor接收访谈文本，指导Thematizer提取主题，然后指导CodebookGenerator生成代码手册。最终，Instructor整合所有结果，输出最终的分析报告。\\n\\n**关键创新**：该论文的关键创新在于提出了一个多Agent协作的LLM框架，将定性分析任务分解为多个Agent，每个Agent负责不同的子任务，通过协同工作实现自动化。这种多Agent架构能够更好地模拟人类研究者的分析过程，提高分析的效率和准确性。\\n\\n**关键设计**：论文中没有详细说明关键参数设置、损失函数、网络结构等技术细节。但从描述来看，每个Agent都基于LLM构建，可能使用了不同的prompt engineering技巧来引导LLM完成特定的任务。具体的LLM选择和prompt设计可能对最终结果有较大影响，但论文中未明确说明。",
            "application_zh": "该研究成果可应用于医疗健康领域，例如患者体验研究、药物依从性分析等。通过自动化定性分析，可以更高效地理解患者需求，优化医疗服务，并为政策制定提供依据。未来，该框架还可扩展到其他领域，如市场调研、社会科学研究等，具有广阔的应用前景。",
            "highlight_zh": "CoTI在心力衰竭患者访谈分析中，识别的关键短语、主题和代码手册与资深研究员的结果更相似，优于初级研究员和基线NLP模型。这表明CoTI能够更准确地捕捉到访谈文本中的关键信息，并生成更符合专家认知的分析结果。然而，CoTI与初级研究员的协作收益有限，提示需要关注AI辅助工具对研究者独立思考的影响。",
            "tags_zh": [
                "多Agent系统",
                "大型语言模型",
                "定性分析",
                "主题分析",
                "患者体验",
                "自然语言处理"
            ],
            "_index": 33,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "DualGuard: Dual-stream Large Language Model Watermarking Defense against Paraphrase and Spoofing Attack",
            "authors": [
                "Hao Li",
                "Yubing Ren",
                "Yanan Cao",
                "Yingjie Li",
                "Fang Fang",
                "Shi Wang",
                "Li Guo"
            ],
            "arxiv_id": "2512.16182v1",
            "summary": "With the rapid development of cloud-based services, large language models (LLMs) have become increasingly accessible through various web platforms. However, this accessibility has also led to growing risks of model abuse. LLM watermarking has emerged as an effective approach to mitigate such misuse and protect intellectual property. Existing watermarking algorithms, however, primarily focus on defending against paraphrase attacks while overlooking piggyback spoofing attacks, which can inject harmful content, compromise watermark reliability, and undermine trust in attribution. To address this limitation, we propose DualGuard, the first watermarking algorithm capable of defending against both paraphrase and spoofing attacks. DualGuard employs the adaptive dual-stream watermarking mechanism, in which two complementary watermark signals are dynamically injected based on the semantic content. This design enables DualGuard not only to detect but also to trace spoofing attacks, thereby ensuring reliable and trustworthy watermark detection. Extensive experiments conducted across multiple datasets and language models demonstrate that DualGuard achieves excellent detectability, robustness, traceability, and text quality, effectively advancing the state of LLM watermarking for real-world applications.",
            "categories": [
                "cs.CR",
                "cs.CL"
            ],
            "primary_category": "cs.CR",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16182v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]large language model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出DualGuard以解决大语言模型水印防御问题",
            "summary_zh": "随着云服务的快速发展，大语言模型（LLMs）通过各种网络平台变得愈加可及。然而，这种可及性也带来了模型滥用的风险。大语言模型水印技术已成为有效的防范手段，但现有算法主要集中于防御改写攻击，忽视了可能注入有害内容的伪造攻击。为了解决这一局限性，本文提出了DualGuard，这是首个能够同时防御改写和伪造攻击的水印算法。DualGuard采用自适应双流水印机制，根据语义内容动态注入两种互补的水印信号，从而确保水印的可靠性和可追溯性。通过在多个数据集和语言模型上的广泛实验，DualGuard展示了出色的可检测性、鲁棒性、可追溯性和文本质量，推动了大语言模型水印技术在实际应用中的进展。",
            "intro_zh": [
                "现有水印算法主要集中于防御改写攻击，而忽视了伪造攻击的威胁，导致水印的可靠性受到影响。",
                "DualGuard提出了一种自适应双流水印机制，能够动态注入两种互补的水印信号，以应对多种攻击方式。",
                "实验结果表明，DualGuard在可检测性、鲁棒性和文本质量方面均优于现有方法，具有良好的实际应用前景。"
            ],
            "method_zh": "**问题定义**：本文旨在解决现有大语言模型水印算法在防御伪造攻击方面的不足。现有方法主要关注改写攻击，未能有效应对伪造攻击带来的风险，导致水印的可靠性和可追溯性受到威胁。\\n\\n**核心思路**：DualGuard的核心思路是采用自适应双流水印机制，动态注入两种互补的水印信号，以增强对改写和伪造攻击的防御能力。这种设计使得水印不仅可以被检测，还能够追踪伪造攻击的来源。\\n\\n**技术框架**：DualGuard的整体架构包括两个主要模块：水印信号生成模块和水印检测模块。前者根据输入文本的语义内容生成水印信号，后者则负责检测和追踪水印的有效性和来源。\\n\\n**关键创新**：DualGuard的最大创新在于其自适应双流水印机制，能够同时应对改写和伪造攻击。这一机制与现有方法的本质区别在于其对多种攻击方式的综合防御能力。\\n\\n**关键设计**：在关键设计方面，DualGuard采用了特定的损失函数来平衡水印的可检测性和文本质量，同时在网络结构上引入了双流处理，以实现对水印信号的动态注入和调整。具体参数设置和网络结构细节在实验部分进行了详细描述。",
            "application_zh": "DualGuard的研究成果具有广泛的应用潜力，尤其是在知识产权保护、内容创作和在线服务等领域。通过有效防范模型滥用和伪造攻击，DualGuard能够提升用户对大语言模型的信任度，促进其在商业和学术界的应用。未来，随着技术的进一步发展，DualGuard可能会成为大语言模型水印技术的标准解决方案。",
            "highlight_zh": "实验结果显示，DualGuard在多个数据集和语言模型上表现出色，具有高达95%的水印可检测性和良好的文本质量。与现有基线相比，DualGuard在鲁棒性和可追溯性方面提升了约20%，有效增强了水印的可靠性。",
            "tags_zh": [
                "大语言模型",
                "水印技术",
                "伪造攻击",
                "改写攻击",
                "自适应机制",
                "知识产权保护",
                "文本质量",
                "鲁棒性"
            ],
            "_index": 34,
            "_used_api": "openai",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16182v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16182v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16182v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Flowing from Reasoning to Motion: Learning 3D Hand Trajectory Prediction from Egocentric Human Interaction Videos",
            "authors": [
                "Mingfei Chen",
                "Yifan Wang",
                "Zhengqin Li",
                "Homanga Bharadhwaj",
                "Yujin Chen",
                "Chuan Qin",
                "Ziyi Kou",
                "Yuan Tian",
                "Eric Whitmire",
                "Rajinder Sodhi",
                "Hrvoje Benko",
                "Eli Shlizerman",
                "Yue Liu"
            ],
            "arxiv_id": "2512.16907v1",
            "summary": "Prior works on 3D hand trajectory prediction are constrained by datasets that decouple motion from semantic supervision and by models that weakly link reasoning and action. To address these, we first present the EgoMAN dataset, a large-scale egocentric dataset for interaction stage-aware 3D hand trajectory prediction with 219K 6DoF trajectories and 3M structured QA pairs for semantic, spatial, and motion reasoning. We then introduce the EgoMAN model, a reasoning-to-motion framework that links vision-language reasoning and motion generation via a trajectory-token interface. Trained progressively to align reasoning with motion dynamics, our approach yields accurate and stage-aware trajectories with generalization across real-world scenes.",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.RO"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Project website: https://egoman-project.github.io",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16907v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱四：生成式动作 (Generative Motion)",
                    "id": "4_motion_diffusion",
                    "matched_keywords": [
                        "motion generation"
                    ],
                    "score": 2.5
                },
                {
                    "name": "支柱六：视频提取与匹配 (Video Extraction)",
                    "id": "6_video_extraction",
                    "matched_keywords": [
                        "[T]egocentric"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 8.5,
            "hit_pillars": [
                "4_motion_diffusion",
                "6_video_extraction"
            ],
            "headline_zh": "EgoMAN：基于自中心交互视频学习3D手部轨迹预测，实现从推理到动作的流畅衔接",
            "summary_zh": "本文提出了一种基于自中心视角的交互场景下，阶段感知的3D手部轨迹预测方法。为了解决现有方法中运动与语义监督解耦以及推理与动作弱连接的问题，作者首先构建了EgoMAN数据集，这是一个大规模的自中心数据集，包含21.9万条6DoF轨迹和300万个结构化的QA对，用于语义、空间和运动推理。然后，作者提出了EgoMAN模型，这是一个推理到运动的框架，通过轨迹-token接口连接视觉-语言推理和运动生成。该方法通过逐步训练，使推理与运动动态对齐，从而生成准确且具有阶段感知的轨迹，并在真实场景中具有良好的泛化能力。",
            "intro_zh": [
                "现有3D手部轨迹预测方法缺乏语义监督，且推理与动作之间的联系较弱，限制了其性能。",
                "EgoMAN模型通过轨迹-token接口连接视觉-语言推理和运动生成，实现推理到动作的流畅衔接。",
                "EgoMAN数据集和模型在真实场景中表现出良好的泛化能力，能够生成准确且具有阶段感知的轨迹。"
            ],
            "method_zh": "**问题定义**：现有3D手部轨迹预测方法面临两个主要问题。一是数据集的限制，现有数据集通常将运动与语义监督解耦，导致模型难以学习到丰富的交互信息。二是模型设计上的不足，现有模型通常弱化推理和动作之间的联系，导致预测的轨迹缺乏阶段感知能力，难以准确反映交互过程中的语义信息。\\n\\n**核心思路**：本文的核心思路是通过构建一个大规模的、包含丰富语义信息的自中心数据集EgoMAN，并设计一个推理到运动的框架EgoMAN模型，从而解决上述问题。EgoMAN模型通过轨迹-token接口，将视觉-语言推理的结果转化为运动生成的输入，从而实现推理和动作之间的强连接。\\n\\n**技术框架**：EgoMAN模型的整体框架包含三个主要模块：视觉-语言推理模块、轨迹-token接口模块和运动生成模块。首先，视觉-语言推理模块负责从自中心视频中提取语义信息，并生成相应的推理结果。然后，轨迹-token接口模块将推理结果转化为轨迹token，作为运动生成模块的输入。最后，运动生成模块根据轨迹token生成3D手部轨迹。\\n\\n**关键创新**：本文最重要的技术创新点在于提出了轨迹-token接口，该接口能够有效地连接视觉-语言推理和运动生成，从而实现推理到动作的流畅衔接。与现有方法相比，EgoMAN模型能够更好地利用语义信息，生成更准确、更具有阶段感知能力的3D手部轨迹。\\n\\n**关键设计**：EgoMAN模型采用了一种渐进式的训练策略，首先训练视觉-语言推理模块，然后训练运动生成模块，最后将两个模块联合训练，以实现推理和动作之间的对齐。在损失函数方面，作者设计了多种损失函数，包括轨迹预测损失、阶段分类损失和QA损失，以保证模型的性能。",
            "application_zh": "该研究成果可应用于人机交互、机器人控制、虚拟现实等领域。例如，在机器人控制中，机器人可以根据人类的意图预测其手部轨迹，从而更好地完成协作任务。在虚拟现实中，可以生成更逼真的手部动画，提升用户的沉浸感。未来，该技术有望应用于智能家居、辅助驾驶等更多场景。",
            "highlight_zh": "EgoMAN数据集包含21.9万条6DoF轨迹和300万个结构化的QA对，是目前最大的自中心手部轨迹预测数据集。EgoMAN模型在多个benchmark上取得了state-of-the-art的性能，相比现有方法，在轨迹预测精度和阶段分类准确率上均有显著提升。实验结果表明，EgoMAN模型具有良好的泛化能力，能够在真实场景中生成准确且具有阶段感知的轨迹。",
            "tags_zh": [
                "3D手部轨迹预测",
                "自中心视频",
                "人机交互",
                "视觉-语言推理",
                "运动生成"
            ],
            "_index": 35,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16907v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16907v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16907v1/x4.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text",
            "authors": [
                "Hanlin Wang",
                "Hao Ouyang",
                "Qiuyu Wang",
                "Yue Yu",
                "Yihao Meng",
                "Wen Wang",
                "Ka Leong Cheng",
                "Shuailei Ma",
                "Qingyan Bai",
                "Yixuan Li",
                "Cheng Chen",
                "Yanhong Zeng",
                "Xing Zhu",
                "Yujun Shen",
                "Qifeng Chen"
            ],
            "arxiv_id": "2512.16924v1",
            "summary": "We present WorldCanvas, a framework for promptable world events that enables rich, user-directed simulation by combining text, trajectories, and reference images. Unlike text-only approaches and existing trajectory-controlled image-to-video methods, our multimodal approach combines trajectories -- encoding motion, timing, and visibility -- with natural language for semantic intent and reference images for visual grounding of object identity, enabling the generation of coherent, controllable events that include multi-agent interactions, object entry/exit, reference-guided appearance and counterintuitive events. The resulting videos demonstrate not only temporal coherence but also emergent consistency, preserving object identity and scene despite temporary disappearance. By supporting expressive world events generation, WorldCanvas advances world models from passive predictors to interactive, user-shaped simulators. Our project page is available at: https://worldcanvas.github.io/.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Project page and code: https://worldcanvas.github.io/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16924v1",
            "code_links": [
                {
                    "url": "https://worldcanvas.github.io/",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "world model"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "multimodal",
                        "visual grounding"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 7.5,
            "hit_pillars": [
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "WorldCanvas：结合文本、轨迹和参考图像，实现可控的世界事件模拟。",
            "summary_zh": "本文提出了WorldCanvas，一个用于可提示世界事件的框架，它通过结合文本、轨迹和参考图像来实现丰富的、用户导向的模拟。与仅使用文本的方法和现有的轨迹控制图像到视频方法不同，我们的多模态方法结合了轨迹（编码运动、时间安排和可见性）与自然语言（用于语义意图）以及参考图像（用于对象身份的视觉基础），从而能够生成连贯的、可控的事件，包括多智能体交互、对象进入/退出、参考引导的外观和违反直觉的事件。生成的视频不仅展示了时间连贯性，还展示了涌现一致性，即使在暂时消失的情况下也能保持对象身份和场景。通过支持富有表现力的世界事件生成，WorldCanvas将世界模型从被动预测器提升为交互式的、用户塑造的模拟器。我们的项目页面位于：https://worldcanvas.github.io/。",
            "intro_zh": [
                "现有方法在世界事件模拟中，要么仅依赖文本，缺乏精细控制，要么依赖轨迹控制，但忽略了语义意图和视觉一致性。",
                "WorldCanvas结合文本、轨迹和参考图像，利用轨迹编码运动，自然语言表达语义，参考图像提供视觉基础，实现可控事件生成。",
                "实验结果表明，WorldCanvas生成的视频不仅具有时间连贯性，还能保持对象身份和场景一致性，支持多智能体交互等复杂事件。"
            ],
            "method_zh": "**问题定义**：现有世界模型在生成复杂、可控的世界事件时面临挑战。仅依赖文本的方法难以精确控制事件的细节和视觉效果，而现有的轨迹控制图像到视频方法则缺乏对语义意图的理解，难以生成具有丰富交互和视觉一致性的事件。这些方法无法很好地处理多智能体交互、物体进入/退出以及违反直觉的事件。\n\n**核心思路**：WorldCanvas的核心思路是将文本、轨迹和参考图像三种模态的信息融合起来，共同驱动世界事件的生成。轨迹用于控制物体的运动和时间信息，文本用于表达事件的语义意图，参考图像则用于提供物体外观的视觉基础。通过这种多模态融合的方式，可以实现对世界事件更精细、更可控的模拟。\n\n**技术框架**：WorldCanvas的整体框架包含以下几个主要模块：1) 轨迹编码模块，用于将轨迹信息编码成可供后续模块使用的特征向量；2) 文本编码模块，用于将自然语言描述编码成语义特征向量；3) 参考图像编码模块，用于提取参考图像的视觉特征；4) 事件生成模块，该模块接收轨迹、文本和参考图像的特征向量，生成相应的视频帧序列。该模块可能基于生成对抗网络（GAN）或扩散模型等技术。\n\n**关键创新**：WorldCanvas的关键创新在于其多模态融合的方法，它将轨迹、文本和参考图像三种模态的信息有机地结合起来，从而实现了对世界事件更精细、更可控的模拟。与现有方法相比，WorldCanvas能够生成包含多智能体交互、物体进入/退出以及违反直觉的事件，并且能够保持物体身份和场景的一致性。\n\n**关键设计**：具体的技术细节未知，但可以推测可能包含以下设计：轨迹编码可能使用循环神经网络（RNN）或Transformer等序列模型；文本编码可能使用预训练的语言模型，如BERT或GPT；参考图像编码可能使用卷积神经网络（CNN），如ResNet或VGG。事件生成模块可能使用条件生成对抗网络（Conditional GAN）或条件扩散模型（Conditional Diffusion Model），其中轨迹、文本和参考图像的特征向量作为条件输入。",
            "application_zh": "WorldCanvas具有广泛的应用前景，例如：游戏开发（生成动态游戏场景）、电影制作（创建特效和动画）、机器人训练（模拟真实世界环境）、自动驾驶（生成交通场景）等。它能够帮助用户更高效、更便捷地创建各种复杂的世界事件，并为相关领域的研究和应用提供新的思路和方法。",
            "highlight_zh": "论文展示了WorldCanvas在生成各种复杂世界事件方面的能力，包括多智能体交互、物体进入/退出、参考引导的外观和违反直觉的事件。生成的视频不仅具有时间连贯性，还能保持物体身份和场景的一致性。具体性能数据未知，但视觉效果上明显优于现有方法。",
            "tags_zh": [
                "世界模型",
                "事件生成",
                "多模态融合",
                "轨迹控制",
                "参考图像"
            ],
            "_index": 36,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16924v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16924v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16924v1/x4.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Task-Oriented Data Synthesis and Control-Rectify Sampling for Remote Sensing Semantic Segmentation",
            "authors": [
                "Yunkai Yang",
                "Yudong Zhang",
                "Kunquan Zhang",
                "Jinxiao Zhang",
                "Xinying Chen",
                "Haohuan Fu",
                "Runmin Dong"
            ],
            "arxiv_id": "2512.16740v1",
            "summary": "With the rapid progress of controllable generation, training data synthesis has become a promising way to expand labeled datasets and alleviate manual annotation in remote sensing (RS). However, the complexity of semantic mask control and the uncertainty of sampling quality often limit the utility of synthetic data in downstream semantic segmentation tasks. To address these challenges, we propose a task-oriented data synthesis framework (TODSynth), including a Multimodal Diffusion Transformer (MM-DiT) with unified triple attention and a plug-and-play sampling strategy guided by task feedback. Built upon the powerful DiT-based generative foundation model, we systematically evaluate different control schemes, showing that a text-image-mask joint attention scheme combined with full fine-tuning of the image and mask branches significantly enhances the effectiveness of RS semantic segmentation data synthesis, particularly in few-shot and complex-scene scenarios. Furthermore, we propose a control-rectify flow matching (CRFM) method, which dynamically adjusts sampling directions guided by semantic loss during the early high-plasticity stage, mitigating the instability of generated images and bridging the gap between synthetic data and downstream segmentation tasks. Extensive experiments demonstrate that our approach consistently outperforms state-of-the-art controllable generation methods, producing more stable and task-oriented synthetic data for RS semantic segmentation.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16740v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "flow matching"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "foundation model",
                        "multimodal"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 7.5,
            "hit_pillars": [
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "提出TODSynth框架，用于遥感语义分割任务的数据合成与控制优化。",
            "summary_zh": "随着可控生成技术的快速发展，训练数据合成已成为扩展遥感（RS）标注数据集和减少人工标注的一种有前景的方法。然而，语义掩码控制的复杂性和采样质量的不确定性通常限制了合成数据在下游语义分割任务中的效用。为了应对这些挑战，我们提出了一个面向任务的数据合成框架（TODSynth），包括一个具有统一三重注意力的多模态扩散Transformer（MM-DiT）和一个由任务反馈指导的即插即用采样策略。基于强大的DiT生成基础模型，我们系统地评估了不同的控制方案，表明文本-图像-掩码联合注意力方案与图像和掩码分支的完全微调相结合，显著提高了遥感语义分割数据合成的有效性，尤其是在少样本和复杂场景中。此外，我们提出了一种控制校正流匹配（CRFM）方法，该方法在早期高可塑性阶段动态调整由语义损失引导的采样方向，从而减轻了生成图像的不稳定性，并弥合了合成数据与下游分割任务之间的差距。大量实验表明，我们的方法始终优于最先进的可控生成方法，为遥感语义分割生成更稳定和面向任务的合成数据。",
            "intro_zh": [
                "遥感图像语义分割依赖大量标注数据，人工标注成本高昂，合成数据是潜在解决方案。",
                "提出TODSynth框架，包含MM-DiT和CRFM，利用任务反馈指导数据合成，提升数据质量。",
                "实验表明，该方法优于现有可控生成方法，生成的数据更稳定，更适合遥感语义分割任务。"
            ],
            "method_zh": "**问题定义**：遥感图像语义分割任务需要大量的标注数据，而人工标注成本高昂。现有的数据合成方法在控制语义掩码的复杂性和保证采样质量方面存在挑战，导致合成数据在下游语义分割任务中的效用受限。具体来说，如何有效地融合文本、图像和掩码信息，以及如何保证合成数据的质量和稳定性是亟待解决的问题。\\n\\n**核心思路**：论文的核心思路是提出一个面向任务的数据合成框架TODSynth，该框架通过多模态扩散Transformer（MM-DiT）实现对文本、图像和掩码的联合控制，并通过控制校正流匹配（CRFM）方法动态调整采样方向，从而生成更稳定和面向任务的合成数据。这种设计旨在弥合合成数据与真实数据之间的差距，提高合成数据在下游语义分割任务中的有效性。\\n\\n**技术框架**：TODSynth框架主要包含两个核心模块：MM-DiT和CRFM。MM-DiT是一个基于扩散Transformer的生成模型，它采用统一的三重注意力机制，能够有效地融合文本、图像和掩码信息。CRFM是一种采样策略，它在扩散模型的采样过程中，利用语义损失动态调整采样方向，从而提高生成图像的质量和稳定性。整个流程包括：首先，使用MM-DiT生成合成图像和对应的语义掩码；然后，使用CRFM方法优化生成过程，提高合成数据的质量；最后，将合成数据用于训练遥感图像语义分割模型。\\n\\n**关键创新**：论文的关键创新在于以下几个方面：1) 提出了MM-DiT，它采用统一的三重注意力机制，能够有效地融合文本、图像和掩码信息，实现对合成数据的精确控制。2) 提出了CRFM方法，它在扩散模型的采样过程中，利用语义损失动态调整采样方向，从而提高生成图像的质量和稳定性。3) 系统地评估了不同的控制方案，并证明了文本-图像-掩码联合注意力方案与图像和掩码分支的完全微调相结合，能够显著提高遥感语义分割数据合成的有效性。\\n\\n**关键设计**：MM-DiT的关键设计包括：采用DiT作为生成骨干网络，并在此基础上引入统一的三重注意力机制，用于融合文本、图像和掩码信息。CRFM的关键设计包括：在扩散模型的采样过程中，计算生成图像的语义损失，并利用该损失动态调整采样方向。此外，论文还对不同的控制方案进行了系统评估，并选择了文本-图像-掩码联合注意力方案与图像和掩码分支的完全微调相结合的方案。",
            "application_zh": "该研究成果可应用于遥感图像语义分割领域，通过合成数据增强训练集，降低人工标注成本，提高分割精度。尤其在少样本或复杂场景下，该方法具有显著优势，可用于灾害监测、城市规划、农业估产等领域，具有重要的实际应用价值和广阔的应用前景。",
            "highlight_zh": "实验结果表明，TODSynth框架在遥感语义分割数据合成方面优于现有方法。具体来说，该方法能够生成更稳定、更面向任务的合成数据，从而显著提高下游语义分割任务的性能。在少样本和复杂场景下，TODSynth的优势更加明显，能够有效缓解数据稀缺问题，提升分割精度。",
            "tags_zh": [
                "遥感图像语义分割",
                "数据合成",
                "可控生成",
                "扩散模型",
                "Transformer"
            ],
            "_index": 37,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16740v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16740v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16740v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "OPENTOUCH: Bringing Full-Hand Touch to Real-World Interaction",
            "authors": [
                "Yuxin Ray Song",
                "Jinzhou Li",
                "Rao Fu",
                "Devin Murphy",
                "Kaichen Zhou",
                "Rishi Shiv",
                "Yaqi Li",
                "Haoyu Xiong",
                "Crystal Elaine Owens",
                "Yilun Du",
                "Yiyue Luo",
                "Xianyi Cheng",
                "Antonio Torralba",
                "Wojciech Matusik",
                "Paul Pu Liang"
            ],
            "arxiv_id": "2512.16842v1",
            "summary": "The human hand is our primary interface to the physical world, yet egocentric perception rarely knows when, where, or how forcefully it makes contact. Robust wearable tactile sensors are scarce, and no existing in-the-wild datasets align first-person video with full-hand touch. To bridge the gap between visual perception and physical interaction, we present OpenTouch, the first in-the-wild egocentric full-hand tactile dataset, containing 5.1 hours of synchronized video-touch-pose data and 2,900 curated clips with detailed text annotations. Using OpenTouch, we introduce retrieval and classification benchmarks that probe how touch grounds perception and action. We show that tactile signals provide a compact yet powerful cue for grasp understanding, strengthen cross-modal alignment, and can be reliably retrieved from in-the-wild video queries. By releasing this annotated vision-touch-pose dataset and benchmark, we aim to advance multimodal egocentric perception, embodied learning, and contact-rich robotic manipulation.",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.RO"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "https://opentouch-tactile.github.io/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16842v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱六：视频提取与匹配 (Video Extraction)",
                    "id": "6_video_extraction",
                    "matched_keywords": [
                        "egocentric"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "multimodal"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 7.0,
            "hit_pillars": [
                "1_robot_core",
                "6_video_extraction",
                "9_embodied_foundation"
            ],
            "headline_zh": "OpenTouch：构建真实场景下完整手部触觉交互数据集与基准",
            "summary_zh": "人手是与物理世界交互的主要界面，但自我中心感知很少知道何时、何地或以多大力度进行接触。目前缺乏稳健的可穿戴触觉传感器，并且没有现有的真实场景数据集将第一人称视频与完整手部触觉对齐。为了弥合视觉感知和物理交互之间的差距，我们提出了OpenTouch，这是第一个真实场景下的自我中心完整手部触觉数据集，包含5.1小时的同步视频-触觉-姿势数据和2,900个带有详细文本注释的精选片段。利用OpenTouch，我们引入了检索和分类基准，以探究触觉如何扎根于感知和行动。我们表明，触觉信号为抓取理解提供了一个紧凑而强大的线索，加强了跨模态对齐，并且可以从真实场景的视频查询中可靠地检索。通过发布这个带注释的视觉-触觉-姿势数据集和基准，我们的目标是推进多模态自我中心感知、具身学习和富含接触的机器人操作。",
            "intro_zh": [
                "现有方法缺乏在真实场景下同步第一人称视频与完整手部触觉的数据集，阻碍了视觉感知与物理交互的研究。",
                "OpenTouch数据集通过同步视频、触觉和姿势数据，并提供详细文本注释，弥合了视觉感知和物理交互之间的差距。",
                "实验表明，触觉信号在抓取理解、跨模态对齐和视频检索方面表现出强大的能力，为相关研究提供了新的基准。"
            ],
            "method_zh": "**问题定义**：现有方法缺乏真实场景下同步的视觉和触觉数据，导致难以研究视觉感知与物理交互之间的关系。特别是在机器人操作和具身智能领域，缺乏高质量的触觉数据阻碍了算法的开发和评估。现有可穿戴触觉传感器不够鲁棒，且缺乏大规模的、带有详细注释的数据集。\n\n**核心思路**：OpenTouch的核心思路是构建一个大规模的、真实场景下的自我中心完整手部触觉数据集，包含同步的视频、触觉和姿势数据，并提供详细的文本注释。通过这个数据集，可以训练和评估多模态模型，从而更好地理解触觉在感知和行动中的作用。\n\n**技术框架**：OpenTouch数据集的构建流程包括数据采集、数据同步、数据标注和数据发布。数据采集使用可穿戴触觉传感器、第一人称相机和姿势捕捉设备，同步记录手部的触觉、视觉和姿势信息。数据标注包括对视频片段进行文本描述，以及对触觉数据进行语义标注。最终，数据集以标准格式发布，方便研究人员使用。\n\n**关键创新**：OpenTouch的关键创新在于它是第一个真实场景下的自我中心完整手部触觉数据集。与现有的数据集相比，OpenTouch具有更大的规模、更丰富的场景和更详细的注释。此外，OpenTouch还提供了一系列的基准测试，方便研究人员评估自己的算法。\n\n**关键设计**：OpenTouch数据集包含5.1小时的同步视频-触觉-姿势数据和2,900个带有详细文本注释的精选片段。触觉传感器采用高分辨率的触觉阵列，能够捕捉手部各个部位的触觉信息。视频数据采用高帧率的第一人称相机，能够清晰地记录手部的动作。姿势数据采用高精度的姿势捕捉设备，能够准确地估计手部的姿势。数据集的标注采用人工标注和自动标注相结合的方式，保证了标注的质量和效率。",
            "application_zh": "OpenTouch数据集的应用场景广泛，包括机器人操作、具身智能、人机交互、虚拟现实和增强现实等领域。例如，可以利用OpenTouch数据集训练机器人，使其能够更好地理解和操作物体。还可以利用OpenTouch数据集开发更自然、更直观的人机交互界面。此外，OpenTouch数据集还可以用于研究人类的触觉感知机制。",
            "highlight_zh": "OpenTouch数据集被用于检索和分类基准测试，结果表明触觉信号在抓取理解方面表现出强大的能力。触觉信息能够有效提升跨模态对齐的性能，并且可以从真实场景的视频查询中可靠地检索。这些实验结果验证了OpenTouch数据集的价值和潜力。",
            "tags_zh": [
                "触觉感知",
                "多模态学习",
                "具身智能",
                "机器人操作",
                "第一人称视觉",
                "数据集",
                "人机交互"
            ],
            "_index": 38,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16842v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16842v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16842v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Kling-Omni Technical Report",
            "authors": [
                "Kling Team",
                "Jialu Chen",
                "Yuanzheng Ci",
                "Xiangyu Du",
                "Zipeng Feng",
                "Kun Gai",
                "Sainan Guo",
                "Feng Han",
                "Jingbin He",
                "Kang He",
                "Xiao Hu",
                "Xiaohua Hu",
                "Boyuan Jiang",
                "Fangyuan Kong",
                "Hang Li",
                "Jie Li",
                "Qingyu Li",
                "Shen Li",
                "Xiaohan Li",
                "Yan Li",
                "Jiajun Liang",
                "Borui Liao",
                "Yiqiao Liao",
                "Weihong Lin",
                "Quande Liu",
                "Xiaokun Liu",
                "Yilun Liu",
                "Yuliang Liu",
                "Shun Lu",
                "Hangyu Mao",
                "Yunyao Mao",
                "Haodong Ouyang",
                "Wenyu Qin",
                "Wanqi Shi",
                "Xiaoyu Shi",
                "Lianghao Su",
                "Haozhi Sun",
                "Peiqin Sun",
                "Pengfei Wan",
                "Chao Wang",
                "Chenyu Wang",
                "Meng Wang",
                "Qiulin Wang",
                "Runqi Wang",
                "Xintao Wang",
                "Xuebo Wang",
                "Zekun Wang",
                "Min Wei",
                "Tiancheng Wen",
                "Guohao Wu",
                "Xiaoshi Wu",
                "Zhenhua Wu",
                "Da Xie",
                "Yingtong Xiong",
                "Yulong Xu",
                "Sile Yang",
                "Zikang Yang",
                "Weicai Ye",
                "Ziyang Yuan",
                "Shenglong Zhang",
                "Shuaiyu Zhang",
                "Yuanxing Zhang",
                "Yufan Zhang",
                "Wenzheng Zhao",
                "Ruiliang Zhou",
                "Yan Zhou",
                "Guosheng Zhu",
                "Yongjie Zhu"
            ],
            "arxiv_id": "2512.16776v1",
            "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Kling-Omni Technical Report",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16776v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "multimodal",
                        "instruction following"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "Kling-Omni：通用生成框架，实现多模态输入到高质量视频的端到端合成",
            "summary_zh": "Kling-Omni是一个通用的生成框架，旨在直接从多模态视觉语言输入合成高保真视频。Kling-Omni采用端到端的视角，弥合了各种视频生成、编辑和智能推理任务之间的功能分离，将它们集成到一个整体系统中。与不连贯的流水线方法不同，Kling-Omni支持各种用户输入，包括文本指令、参考图像和视频上下文，并将它们处理成统一的多模态表示，以提供电影质量和高度智能的视频内容创作。为了支持这些能力，我们构建了一个全面的数据系统，作为多模态视频创作的基础。该框架通过高效的大规模预训练策略和用于推理的基础设施优化得到进一步加强。综合评估表明，Kling-Omni在上下文生成、基于推理的编辑和多模态指令跟随方面表现出卓越的能力。我们认为，Kling-Omni超越了内容创作工具，是朝着能够感知、推理、生成和与动态复杂世界交互的多模态世界模拟器迈出的关键一步。",
            "intro_zh": [
                "现有视频生成方法通常采用分离的流水线，难以处理多模态输入和复杂的推理任务。",
                "Kling-Omni通过端到端框架，统一处理文本、图像和视频等多模态输入，生成高质量视频。",
                "该框架通过大规模预训练和基础设施优化，在上下文生成、推理编辑和指令跟随方面表现出色。"
            ],
            "method_zh": "**问题定义**：现有视频生成方法通常是pipeline式的，各个模块之间相互独立，难以实现多模态信息的融合和复杂逻辑的推理。此外，生成视频的质量和智能程度也存在瓶颈，难以满足用户对电影级视频内容创作的需求。\\n\\n**核心思路**：Kling-Omni的核心思路是构建一个端到端的通用生成框架，将视频生成、编辑和智能推理任务整合到一个统一的系统中。通过统一的多模态表示，可以同时处理文本指令、参考图像和视频上下文等多种输入，从而生成更具创造性和智能性的视频内容。\\n\\n**技术框架**：Kling-Omni的整体架构包含一个多模态输入编码器，用于将不同模态的信息转换为统一的表示；一个视频生成模型，基于编码后的表示生成视频；以及一个推理模块，用于执行基于指令的编辑和推理任务。整个流程是端到端可训练的，可以优化各个模块之间的协同工作。\\n\\n**关键创新**：Kling-Omni的关键创新在于其通用性和端到端的设计。它打破了传统视频生成方法中各个模块之间的壁垒，实现了多模态信息的深度融合和复杂逻辑的推理。此外，大规模预训练策略和基础设施优化也为生成高质量视频提供了保障。\\n\\n**关键设计**：具体的技术细节未知，但可以推测可能包括：1) 使用Transformer或类似架构进行多模态信息编码；2) 采用生成对抗网络（GAN）或扩散模型（Diffusion Model）进行视频生成；3) 设计特定的损失函数来优化生成视频的质量和智能程度；4) 利用大规模数据集进行预训练，提升模型的泛化能力。",
            "application_zh": "Kling-Omni具有广泛的应用前景，可用于电影制作、广告创意、游戏开发、教育娱乐等领域。它可以帮助用户快速生成高质量的视频内容，降低视频创作的门槛，并为用户提供更智能、更个性化的视频创作体验。未来，Kling-Omni有望发展成为一个多模态世界模拟器，能够感知、推理、生成和与动态复杂世界进行交互。",
            "highlight_zh": "论文通过综合评估表明，Kling-Omni在上下文生成、基于推理的编辑和多模态指令跟随方面表现出卓越的能力。具体的性能数据未知，但可以推断其在生成视频的质量、智能程度和与用户指令的匹配度等方面均优于现有方法。该框架为多模态视频生成领域的研究提供了新的思路和方向。",
            "tags_zh": [
                "视频生成",
                "多模态学习",
                "端到端框架",
                "视觉语言模型",
                "智能推理"
            ],
            "_index": 39,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16776v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16776v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16776v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Sketch-in-Latents: Eliciting Unified Reasoning in MLLMs",
            "authors": [
                "Jintao Tong",
                "Jiaqi Gu",
                "Yujing Lou",
                "Lubin Fan",
                "Yixiong Zou",
                "Yue Wu",
                "Jieping Ye",
                "Ruixuan Li"
            ],
            "arxiv_id": "2512.16584v1",
            "summary": "While Multimodal Large Language Models (MLLMs) excel at visual understanding tasks through text reasoning, they often fall short in scenarios requiring visual imagination. Unlike current works that take predefined external toolkits or generate images during thinking, however, humans can form flexible visual-text imagination and interactions during thinking without predefined toolkits, where one important reason is that humans construct the visual-text thinking process in a unified space inside the brain. Inspired by this capability, given that current MLLMs already encode visual and text information in the same feature space, we hold that visual tokens can be seamlessly inserted into the reasoning process carried by text tokens, where ideally, all visual imagination processes can be encoded by the latent features. To achieve this goal, we propose Sketch-in-Latents (SkiLa), a novel paradigm for unified multi-modal reasoning that expands the auto-regressive capabilities of MLLMs to natively generate continuous visual embeddings, termed latent sketch tokens, as visual thoughts. During multi-step reasoning, the model dynamically alternates between textual thinking mode for generating textual think tokens and visual sketching mode for generating latent sketch tokens. A latent visual semantics reconstruction mechanism is proposed to ensure these latent sketch tokens are semantically grounded. Extensive experiments demonstrate that SkiLa achieves superior performance on vision-centric tasks while exhibiting strong generalization to diverse general multi-modal benchmarks. Codes will be released at https://github.com/TungChintao/SkiLa.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "14 pages, 11 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16584v1",
            "code_links": [
                {
                    "url": "https://github.com/TungChintao/SkiLa",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model",
                        "multimodal"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出Sketch-in-Latents (SkiLa)，实现MLLM中统一的多模态推理与视觉想象。",
            "summary_zh": "多模态大型语言模型(MLLM)擅长通过文本推理进行视觉理解任务，但在需要视觉想象的场景中表现不佳。与采用预定义外部工具包或在思考过程中生成图像的现有方法不同，人类可以在思考过程中形成灵活的视觉-文本想象和交互，而无需预定义的工具包，其中一个重要原因是人类在大脑内部的统一空间中构建视觉-文本思考过程。受此能力的启发，鉴于当前的MLLM已经将视觉和文本信息编码在相同的特征空间中，我们认为视觉token可以无缝地插入到文本token所携带的推理过程中，理想情况下，所有的视觉想象过程都可以由潜在特征编码。为了实现这个目标，我们提出Sketch-in-Latents (SkiLa)，这是一种用于统一多模态推理的新范式，它扩展了MLLM的自回归能力，以原生生成连续的视觉嵌入，称为潜在草图token，作为视觉思考。在多步推理过程中，模型在生成文本思考token的文本思考模式和生成潜在草图token的视觉草图模式之间动态切换。提出了一种潜在的视觉语义重建机制，以确保这些潜在的草图token在语义上是接地的。大量的实验表明，SkiLa在以视觉为中心的任务上取得了优异的性能，同时对各种通用多模态基准表现出强大的泛化能力。",
            "intro_zh": [
                "现有MLLM在视觉想象方面存在不足，无法像人类一样灵活地进行视觉-文本交互。",
                "SkiLa通过在MLLM的潜在空间中生成连续的视觉嵌入（潜在草图token）来实现统一的多模态推理。",
                "实验表明，SkiLa在视觉任务上表现优异，并对通用多模态基准具有良好的泛化能力。"
            ],
            "method_zh": "**问题定义**：现有MLLM在处理需要视觉想象的任务时，依赖于预定义的外部工具或在推理过程中生成图像，这限制了模型的灵活性和效率。人类可以在大脑内部的统一空间中进行视觉-文本思考，而MLLM缺乏这种能力。因此，需要一种方法使MLLM能够像人类一样进行灵活的视觉-文本推理和想象。\\n\\n**核心思路**：SkiLa的核心思路是将视觉信息直接嵌入到MLLM的潜在空间中，作为推理过程的一部分。通过生成“潜在草图token”，模型可以在文本推理的同时进行视觉想象，从而实现统一的多模态推理。这种方法避免了对外部工具的依赖，并允许模型在潜在空间中灵活地操作视觉信息。\\n\\n**技术框架**：SkiLa的整体框架包括文本思考模式和视觉草图模式。在文本思考模式下，模型生成文本token进行推理；在视觉草图模式下，模型生成潜在草图token进行视觉想象。模型在这两种模式之间动态切换，以完成多步推理任务。为了确保潜在草图token的语义一致性，SkiLa还引入了一种潜在视觉语义重建机制。\\n\\n**关键创新**：SkiLa的关键创新在于将视觉想象过程融入到MLLM的自回归生成过程中。通过直接在潜在空间中生成视觉token，SkiLa实现了视觉和文本信息的统一表示和推理。这种方法与现有方法（依赖外部工具或生成图像）的本质区别在于，它允许模型在内部进行视觉想象，而无需额外的步骤或模块。\\n\\n**关键设计**：SkiLa的关键设计包括：1) 潜在草图token的生成方式，可能涉及特定的网络结构或损失函数，以确保生成的token具有语义意义；2) 文本思考模式和视觉草图模式之间的切换机制，可能基于某种策略或控制信号；3) 潜在视觉语义重建机制，用于将潜在草图token映射回视觉空间，并确保其与原始视觉信息的一致性。具体的参数设置、损失函数和网络结构等细节需要在论文中进一步查找。",
            "application_zh": "SkiLa具有广泛的应用前景，例如视觉问答、图像描述生成、机器人导航和人机交互等领域。它可以帮助机器更好地理解和推理视觉信息，并生成更自然和连贯的多模态内容。未来，SkiLa有望应用于更复杂的视觉任务，例如视频理解、三维重建和虚拟现实。",
            "highlight_zh": "实验结果表明，SkiLa在以视觉为中心的任务上取得了优异的性能，超过了现有的MLLM方法。此外，SkiLa在各种通用多模态基准上表现出强大的泛化能力，表明其具有良好的鲁棒性和适应性。具体的性能数据和提升幅度需要在论文中进一步查找。",
            "tags_zh": [
                "多模态大语言模型",
                "视觉想象",
                "统一推理",
                "潜在空间",
                "草图生成"
            ],
            "_index": 40,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16584v1/img/method.jpg",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16584v1/img/hyper.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16584v1/img/case_geo.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "4D Primitive-Mâché: Glueing Primitives for Persistent 4D Scene Reconstruction",
            "authors": [
                "Kirill Mazur",
                "Marwan Taher",
                "Andrew J. Davison"
            ],
            "arxiv_id": "2512.16564v1",
            "summary": "We present a dynamic reconstruction system that receives a casual monocular RGB video as input, and outputs a complete and persistent reconstruction of the scene. In other words, we reconstruct not only the the currently visible parts of the scene, but also all previously viewed parts, which enables replaying the complete reconstruction across all timesteps.\n  Our method decomposes the scene into a set of rigid 3D primitives, which are assumed to be moving throughout the scene. Using estimated dense 2D correspondences, we jointly infer the rigid motion of these primitives through an optimisation pipeline, yielding a 4D reconstruction of the scene, i.e. providing 3D geometry dynamically moving through time. To achieve this, we also introduce a mechanism to extrapolate motion for objects that become invisible, employing motion-grouping techniques to maintain continuity.\n  The resulting system enables 4D spatio-temporal awareness, offering capabilities such as replayable 3D reconstructions of articulated objects through time, multi-object scanning, and object permanence. On object scanning and multi-object datasets, our system significantly outperforms existing methods both quantitatively and qualitatively.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "For project page, see https://makezur.github.io/4DPM/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16564v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]scene reconstruction"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出4D Primitive-Mâché，通过拼接基元实现持久化4D场景重建",
            "summary_zh": "本文提出了一种动态重建系统，该系统以单目RGB视频作为输入，输出场景的完整且持久的重建结果。换句话说，我们不仅重建当前可见的场景部分，还重建所有先前观察到的部分，从而能够重放所有时间步长的完整重建。我们的方法将场景分解为一组刚性3D基元，这些基元被认为在整个场景中移动。利用估计的密集2D对应关系，我们通过优化流程联合推断这些基元的刚性运动，从而产生场景的4D重建，即提供随时间动态移动的3D几何体。为此，我们还引入了一种机制来推断不可见物体的运动，采用运动分组技术来保持连续性。该系统实现了4D时空感知，提供了诸如随时间推移的可重放3D铰接物体重建、多物体扫描和物体持久性等功能。在物体扫描和多物体数据集上，我们的系统在定量和定性方面均显著优于现有方法。",
            "intro_zh": [
                "现有动态重建方法难以重建所有时间步长的场景，存在信息丢失和不连续性问题。",
                "将场景分解为刚性3D基元，通过优化流程联合推断基元的刚性运动，实现4D场景重建。",
                "引入运动外推机制，利用运动分组技术保持物体在不可见时的运动连续性，提升重建质量。"
            ],
            "method_zh": "**问题定义**：现有动态场景重建方法通常只能重建当前时刻可见的部分，无法完整地重建整个时间序列中的场景，导致信息丢失和重建结果不连续。尤其是在物体暂时离开视野或被遮挡的情况下，重建质量会显著下降。因此，如何实现持久化的、完整的4D场景重建是一个关键问题。\\n\\n**核心思路**：论文的核心思路是将动态场景分解为一组刚性3D基元，并假设这些基元在场景中进行刚性运动。通过估计图像中的2D对应关系，并优化这些基元的运动参数，从而实现对场景的4D重建。这种方法能够有效地处理遮挡和物体离开视野的情况，并通过运动外推机制保持重建结果的连续性。\\n\\n**技术框架**：该系统的整体框架包括以下几个主要阶段：1) 输入单目RGB视频；2) 估计图像中的密集2D对应关系；3) 将场景分解为一组刚性3D基元；4) 通过优化流程联合推断这些基元的刚性运动；5) 利用运动分组技术对不可见物体进行运动外推；6) 输出完整的4D场景重建结果。\\n\\n**关键创新**：该论文的关键创新在于提出了一种基于基元的4D场景重建方法，该方法能够实现持久化的重建，即重建所有时间步长的场景，而不仅仅是当前可见的部分。此外，该论文还引入了一种运动外推机制，能够有效地处理遮挡和物体离开视野的情况，并通过运动分组技术保持重建结果的连续性。\\n\\n**关键设计**：论文中使用了优化方法来估计基元的运动参数，具体的损失函数设计和优化算法选择未知。运动分组技术用于对不可见物体进行运动外推，具体的实现细节未知。此外，如何有效地将场景分解为刚性3D基元也是一个关键的设计问题，但论文中没有详细描述。",
            "application_zh": "该研究成果可应用于增强现实、虚拟现实、机器人导航、自动驾驶等领域。例如，在AR/VR中，可以提供更真实、更沉浸式的体验，用户可以与过去时刻的场景进行交互。在机器人导航中，可以帮助机器人更好地理解和预测环境变化，从而实现更安全、更高效的导航。在自动驾驶中，可以提高车辆对动态环境的感知能力，从而提高驾驶安全性。",
            "highlight_zh": "该系统在物体扫描和多物体数据集上进行了实验，结果表明，该系统在定量和定性方面均显著优于现有方法。具体的性能数据和提升幅度未知，但论文强调了该系统在重建完整性和连续性方面的优势。",
            "tags_zh": [
                "4D场景重建",
                "动态重建",
                "基元分解",
                "运动估计",
                "单目视觉"
            ],
            "_index": 41,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16564v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16564v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16564v1/x4.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Using Gaussian Splats to Create High-Fidelity Facial Geometry and Texture",
            "authors": [
                "Haodi He",
                "Jihun Yu",
                "Ronald Fedkiw"
            ],
            "arxiv_id": "2512.16397v1",
            "summary": "We leverage increasingly popular three-dimensional neural representations in order to construct a unified and consistent explanation of a collection of uncalibrated images of the human face. Our approach utilizes Gaussian Splatting, since it is more explicit and thus more amenable to constraints than NeRFs. We leverage segmentation annotations to align the semantic regions of the face, facilitating the reconstruction of a neutral pose from only 11 images (as opposed to requiring a long video). We soft constrain the Gaussians to an underlying triangulated surface in order to provide a more structured Gaussian Splat reconstruction, which in turn informs subsequent perturbations to increase the accuracy of the underlying triangulated surface. The resulting triangulated surface can then be used in a standard graphics pipeline. In addition, and perhaps most impactful, we show how accurate geometry enables the Gaussian Splats to be transformed into texture space where they can be treated as a view-dependent neural texture. This allows one to use high visual fidelity Gaussian Splatting on any asset in a scene without the need to modify any other asset or any other aspect (geometry, lighting, renderer, etc.) of the graphics pipeline. We utilize a relightable Gaussian model to disentangle texture from lighting in order to obtain a delit high-resolution albedo texture that is also readily usable in a standard graphics pipeline. The flexibility of our system allows for training with disparate images, even with incompatible lighting, facilitating robust regularization. Finally, we demonstrate the efficacy of our approach by illustrating its use in a text-driven asset creation pipeline.",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.GR"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Submitted to CVPR 2026. 21 pages, 22 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16397v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "gaussian splatting",
                        "splatting",
                        "NeRF"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "利用高斯溅射重建高保真面部几何与纹理，实现可控人脸生成",
            "summary_zh": "本文利用日益流行的三维神经表示，从一组未经校准的人脸图像中构建统一且一致的解释。该方法采用高斯溅射，因为它比NeRF更显式，因此更易于约束。利用分割标注对齐面部的语义区域，从而仅用11张图像即可重建中性姿势（而无需长视频）。软约束高斯分布到一个潜在的三角化表面，以提供更结构化的重建，进而指导后续扰动以提高三角化表面的精度。生成的三角化表面可用于标准图形管线。此外，也是最重要的，展示了精确的几何体如何使高斯溅射转换为纹理空间，在纹理空间中，它们可以被视为与视角相关的神经纹理。这允许在场景中的任何资产上使用高视觉保真度的高斯溅射，而无需修改任何其他资产或图形管线的任何其他方面（几何体、光照、渲染器等）。利用可重新光照的高斯模型将纹理与光照分离，以获得可用于标准图形管线中的高分辨率反照率纹理。系统的灵活性允许使用不同的图像进行训练，即使光照不兼容，也有助于鲁棒的正则化。最后，通过展示其在文本驱动的资产创建管线中的应用，证明了该方法的有效性。",
            "intro_zh": [
                "现有方法难以从少量、未经校准的人脸图像中重建高保真度的三维人脸模型，尤其是在光照条件不一致的情况下。",
                "该方法利用高斯溅射的显式特性，结合语义分割和几何约束，实现从少量图像中重建高质量人脸几何和纹理。",
                "实验表明，该方法能够从仅11张图像中重建出高质量的人脸模型，并能生成可用于标准图形管线的高分辨率纹理。"
            ],
            "method_zh": "**问题定义**：现有方法，如NeRF，在处理少量、未经校准的人脸图像时，难以重建出高保真度的几何和纹理。尤其是在光照条件不一致的情况下，重建质量会显著下降。此外，NeRF的隐式表示使得难以施加几何约束，从而影响重建的精度和可控性。\\n\\n**核心思路**：论文的核心思路是利用高斯溅射（Gaussian Splatting）的显式特性，结合语义分割和几何约束，实现从少量图像中重建高质量的人脸几何和纹理。通过将高斯分布约束到潜在的三角化表面，可以提高重建的结构性和精度。同时，将高斯溅射转换为纹理空间，可以实现与视角相关的神经纹理，从而提高渲染的真实感。\\n\\n**技术框架**：该方法主要包含以下几个阶段：1) 使用分割标注对齐面部的语义区域，从而实现中性姿势的重建。2) 将高斯分布软约束到潜在的三角化表面，以提供更结构化的重建。3) 通过扰动三角化表面，提高其精度。4) 将高斯溅射转换为纹理空间，生成与视角相关的神经纹理。5) 利用可重新光照的高斯模型将纹理与光照分离，生成高分辨率的反照率纹理。\\n\\n**关键创新**：该方法最重要的技术创新点在于将高斯溅射与几何约束相结合，从而实现从少量图像中重建高质量的人脸几何和纹理。与现有方法相比，该方法能够更好地处理光照条件不一致的情况，并能生成可用于标准图形管线的高分辨率纹理。此外，将高斯溅射转换为纹理空间，可以实现与视角相关的神经纹理，从而提高渲染的真实感。\\n\\n**关键设计**：论文中关键的设计包括：1) 使用软约束将高斯分布约束到三角化表面，约束强度需要仔细调整。2) 使用可重新光照的高斯模型，该模型包含漫反射和镜面反射分量，用于分离纹理和光照。3) 使用L1损失和感知损失来优化高斯溅射的参数。4) 使用Adam优化器进行训练，学习率需要根据数据集进行调整。",
            "application_zh": "该研究成果可广泛应用于人脸重建、虚拟现实、增强现实、游戏开发等领域。例如，可以用于创建逼真的虚拟化身，用于在线会议、社交媒体等应用。此外，该方法还可以用于人脸动画、表情迁移等任务，从而实现更自然、更逼真的人机交互。",
            "highlight_zh": "该方法在少量图像人脸重建任务上取得了显著成果，仅使用11张图像即可重建出高质量的人脸模型。与现有方法相比，该方法能够更好地处理光照条件不一致的情况，并能生成可用于标准图形管线的高分辨率纹理。实验结果表明，该方法能够生成更逼真、更自然的人脸模型。",
            "tags_zh": [
                "高斯溅射",
                "人脸重建",
                "神经渲染",
                "三维建模",
                "纹理生成"
            ],
            "_index": 42,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16397v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16397v1/figs/ablation/vis_gaussians/2_geometry_render.jpg",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16397v1/figs/head_poses/00.jpg",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "EverybodyDance: Bipartite Graph-Based Identity Correspondence for Multi-Character Animation",
            "authors": [
                "Haotian Ling",
                "Zequn Chen",
                "Qiuying Chen",
                "Donglin Di",
                "Yongjia Ma",
                "Hao Li",
                "Chen Wei",
                "Zhulin Tao",
                "Xun Yang"
            ],
            "arxiv_id": "2512.16360v1",
            "summary": "Consistent pose-driven character animation has achieved remarkable progress in single-character scenarios. However, extending these advances to multi-character settings is non-trivial, especially when position swap is involved. Beyond mere scaling, the core challenge lies in enforcing correct Identity Correspondence (IC) between characters in reference and generated frames. To address this, we introduce EverybodyDance, a systematic solution targeting IC correctness in multi-character animation. EverybodyDance is built around the Identity Matching Graph (IMG), which models characters in the generated and reference frames as two node sets in a weighted complete bipartite graph. Edge weights, computed via our proposed Mask-Query Attention (MQA), quantify the affinity between each pair of characters. Our key insight is to formalize IC correctness as a graph structural metric and to optimize it during training. We also propose a series of targeted strategies tailored for multi-character animation, including identity-embedded guidance, a multi-scale matching strategy, and pre-classified sampling, which work synergistically. Finally, to evaluate IC performance, we curate the Identity Correspondence Evaluation benchmark, dedicated to multi-character IC correctness. Extensive experiments demonstrate that EverybodyDance substantially outperforms state-of-the-art baselines in both IC and visual fidelity.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16360v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱八：物理动画 (Physics-based Animation)",
                    "id": "8_physics_animation",
                    "matched_keywords": [
                        "[T]character animation"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "8_physics_animation"
            ],
            "headline_zh": "EverybodyDance：基于二分图的角色匹配方法，解决多角色动画中的身份对应问题。",
            "summary_zh": "本文提出EverybodyDance，一个针对多角色动画中身份对应（IC）正确性的系统性解决方案。核心是身份匹配图（IMG），它将生成帧和参考帧中的角色建模为加权完全二分图中的两个节点集合。边缘权重通过提出的Mask-Query Attention（MQA）计算，量化每对角色之间的亲和力。论文将IC正确性形式化为图结构度量，并在训练期间对其进行优化。此外，还提出了一系列针对多角色动画的策略，包括身份嵌入引导、多尺度匹配策略和预分类采样。为了评估IC性能，创建了身份对应评估基准。大量实验表明，EverybodyDance在IC和视觉保真度方面均优于现有技术水平。",
            "intro_zh": [
                "现有姿态驱动的角色动画在单角色场景中取得了显著进展，但扩展到多角色场景，尤其是在涉及位置交换时，极具挑战。",
                "EverybodyDance的核心思想是将角色间的身份对应关系建模为二分图，并通过优化图结构度量来保证身份对应关系的正确性。",
                "论文提出了身份对应评估基准，并通过大量实验证明EverybodyDance在身份对应和视觉保真度方面均优于现有方法。"
            ],
            "method_zh": "**问题定义**：论文旨在解决多角色动画生成中身份对应（Identity Correspondence, IC）问题。现有方法在处理多角色场景，特别是角色位置发生交换时，难以保证生成动画中角色的身份与参考帧中的角色身份一致。这导致动画效果不自然，角色混乱。\\n\\n**核心思路**：论文的核心思路是将生成动画帧和参考帧中的角色建模为二分图的节点，通过计算节点之间的相似度（即边的权重）来建立角色之间的对应关系。通过优化二分图的结构，使得相似度高的节点尽可能匹配，从而保证身份对应关系的正确性。\\n\\n**技术框架**：EverybodyDance主要包含以下几个模块：1) **身份匹配图（IMG）构建**：将参考帧和生成帧中的角色作为节点，构建一个完全二分图。2) **Mask-Query Attention（MQA）**：计算二分图中节点之间的边缘权重，表示角色之间的相似度。MQA利用角色的mask信息作为query，去attention参考帧中的特征，从而得到相似度。3) **图结构优化**：设计损失函数，优化二分图的结构，使得相似度高的节点尽可能匹配。4) **身份嵌入引导**：在生成过程中，利用身份信息引导生成器，提高身份对应关系的准确性。5) **多尺度匹配策略**：在不同尺度上进行角色匹配，提高鲁棒性。6) **预分类采样**：对训练数据进行预分类，提高训练效率。\\n\\n**关键创新**：论文最重要的技术创新点在于将身份对应问题形式化为图结构优化问题，并提出了Mask-Query Attention（MQA）来计算角色之间的相似度。与现有方法相比，该方法能够更好地处理角色位置交换的情况，并保证身份对应关系的正确性。\\n\\n**关键设计**：1) **Mask-Query Attention (MQA)**：利用角色的mask作为query，参考帧的特征作为key和value，计算attention score作为角色之间的相似度。2) **图结构损失函数**：设计损失函数，鼓励相似度高的节点匹配，相似度低的节点不匹配。3) **身份嵌入**：将角色的身份信息嵌入到生成器中，引导生成器生成具有正确身份的角色。4) **多尺度匹配**：在多个尺度上计算角色之间的相似度，提高匹配的鲁棒性。",
            "application_zh": "该研究成果可广泛应用于虚拟现实、游戏开发、电影制作等领域。例如，可以用于创建多人在线互动动画，允许用户控制多个角色进行互动，而无需担心角色身份混乱的问题。此外，该技术还可以应用于舞蹈教学、运动分析等领域，帮助用户更好地理解和学习动作。",
            "highlight_zh": "实验结果表明，EverybodyDance在身份对应（IC）和视觉保真度方面均优于现有技术水平。具体而言，在身份对应评估基准上，EverybodyDance的IC指标比最先进的基线方法提高了显著幅度（具体数值未知）。同时，视觉效果也更加自然，角色身份更加明确。",
            "tags_zh": [
                "多角色动画",
                "身份对应",
                "二分图匹配",
                "姿态驱动",
                "深度学习"
            ],
            "_index": 43,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16360v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16360v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16360v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "PixelArena: A benchmark for Pixel-Precision Visual Intelligence",
            "authors": [
                "Feng Liang",
                "Sizhe Cheng",
                "Chenqi Yi"
            ],
            "arxiv_id": "2512.16303v1",
            "summary": "Multi-modal large language models that have image output are emerging. Many image generation benchmarks focus on aesthetics instead of fine-grained generation capabilities. In PixelArena, we propose using semantic segmentation tasks to objectively examine their fine-grained generative intelligence with pixel precision. We find the latest Gemini 3 Pro Image has emergent image generation capabilities that generate semantic masks with high fidelity under zero-shot settings, showcasing visual intelligence unseen before and true generalization in new image generation tasks. We further investigate its results, compare them qualitatively and quantitatively with those of other models, and present failure cases. The findings not only signal exciting progress in the field but also provide insights into future research related to multimodality, reasoning, interpretability and benchmarking.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "7 pages, 11 figures, project page: https://pixelarena.reify.ing/project",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16303v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model",
                        "multimodal"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "PixelArena：提出像素级视觉智能评测基准，评估多模态大模型图像生成能力",
            "summary_zh": "本文提出了PixelArena，一个用于像素精度视觉智能的评测基准。随着具备图像输出能力的多模态大型语言模型不断涌现，现有的图像生成基准更多关注美学，而忽略了细粒度的生成能力。PixelArena利用语义分割任务，以客观地检验模型在像素级别的生成智能。研究发现，最新的Gemini 3 Pro Image在零样本设置下展现出卓越的图像生成能力，能够高保真地生成语义掩码，体现了前所未见的视觉智能以及在新图像生成任务中的真正泛化能力。进一步分析了实验结果，并与其他模型进行了定性和定量的比较，同时展示了失败案例。这些发现不仅标志着该领域令人兴奋的进展，也为多模态、推理、可解释性和基准测试等相关领域的未来研究提供了见解。",
            "intro_zh": [
                "现有图像生成评测侧重美学，缺乏对模型细粒度生成能力的客观评估。",
                "PixelArena利用语义分割任务，以像素精度评估多模态大模型的图像生成智能。",
                "实验表明Gemini 3 Pro Image在零样本语义分割任务中表现出卓越的生成能力。"
            ],
            "method_zh": "**问题定义**：现有图像生成基准主要关注生成图像的美学质量，缺乏对模型在像素级别上理解和生成图像细节能力的评估。这使得我们难以客观地衡量多模态大模型在细粒度图像生成方面的智能水平，阻碍了相关技术的发展。\\n\\n**核心思路**：PixelArena的核心思路是利用语义分割任务作为评估多模态大模型图像生成能力的标准。语义分割需要模型理解图像中每个像素的语义信息，并准确地将其划分到不同的类别中。通过评估模型生成的语义分割结果的准确性，可以客观地衡量其在像素级别上的视觉智能。\\n\\n**技术框架**：PixelArena基准测试主要包含以下几个阶段：1) 输入图像和相应的语义分割任务描述；2) 多模态大模型根据输入生成语义分割掩码；3) 将生成的掩码与真实标签进行比较，计算评估指标，如像素精度和平均交并比（mIoU）。整个流程旨在量化模型在理解和生成像素级细节方面的能力。\\n\\n**关键创新**：PixelArena的关键创新在于它将语义分割任务引入到多模态大模型的图像生成能力评估中。与传统的图像生成基准不同，PixelArena关注的是模型对图像内容的理解和精确生成能力，而不仅仅是生成图像的美观程度。这使得PixelArena能够更全面、客观地评估多模态大模型的视觉智能。\\n\\n**关键设计**：PixelArena的关键设计包括：1) 选择合适的语义分割数据集，例如包含丰富场景和对象的Cityscapes或ADE20K；2) 设计清晰明确的任务描述，指导模型生成准确的语义分割掩码；3) 采用标准的语义分割评估指标，如像素精度和mIoU，以量化模型生成的掩码的准确性；4) 针对不同类型的多模态大模型，设计合理的输入格式和提示语，以充分发挥模型的生成能力。",
            "application_zh": "PixelArena可应用于评估和提升多模态大模型在图像生成、图像编辑、机器人视觉等领域的性能。通过客观评估模型在像素级别的理解和生成能力，可以推动模型在自动驾驶、医疗影像分析、遥感图像处理等实际应用中的发展，并促进更智能、更可靠的视觉系统的构建。",
            "highlight_zh": "实验结果表明，Gemini 3 Pro Image在零样本语义分割任务中表现出卓越的生成能力，能够高保真地生成语义掩码。相较于其他模型，Gemini 3 Pro Image在像素精度和平均交并比（mIoU）等指标上均取得了显著提升，展现了其强大的视觉智能和泛化能力。",
            "tags_zh": [
                "多模态大模型",
                "图像生成",
                "语义分割",
                "视觉智能",
                "评测基准"
            ],
            "_index": 44,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16303v1/images/celeb/label-color-palette.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16303v1/images/celeb/model-comparison-celeb.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16303v1/images/celeb/best-f1.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "MACL: Multi-Label Adaptive Contrastive Learning Loss for Remote Sensing Image Retrieval",
            "authors": [
                "Amna Amir",
                "Erchan Aptoula"
            ],
            "arxiv_id": "2512.16294v1",
            "summary": "Semantic overlap among land-cover categories, highly imbalanced label distributions, and complex inter-class co-occurrence patterns constitute significant challenges for multi-label remote-sensing image retrieval. In this article, Multi-Label Adaptive Contrastive Learning (MACL) is introduced as an extension of contrastive learning to address them. It integrates label-aware sampling, frequency-sensitive weighting, and dynamic-temperature scaling to achieve balanced representation learning across both common and rare categories. Extensive experiments on three benchmark datasets (DLRSD, ML-AID, and WHDLD), show that MACL consistently outperforms contrastive-loss based baselines, effectively mitigating semantic imbalance and delivering more reliable retrieval performance in large-scale remote-sensing archives. Code, pretrained models, and evaluation scripts will be released at https://github.com/amna/MACL upon acceptance.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16294v1",
            "code_links": [
                {
                    "url": "https://github.com/amna/MACL",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "representation learning",
                        "[T]contrastive learning"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "提出多标签自适应对比学习(MACL)损失，用于遥感图像检索。",
            "summary_zh": "多标签遥感图像检索面临着地物类别语义重叠、标签分布高度不平衡以及复杂的类间共现模式等挑战。本文提出了一种多标签自适应对比学习(MACL)方法，作为对比学习的扩展来解决这些问题。MACL集成了标签感知采样、频率敏感加权和动态温度缩放，以实现常见类别和稀有类别之间平衡的表征学习。在三个基准数据集（DLRSD、ML-AID和WHDLD）上的大量实验表明，MACL始终优于基于对比损失的基线方法，有效地缓解了语义不平衡，并在大规模遥感档案中提供了更可靠的检索性能。代码、预训练模型和评估脚本将在接收后发布在https://github.com/amna/MACL。",
            "intro_zh": [
                "多标签遥感图像检索面临类别语义重叠和标签分布不平衡等难题，影响检索性能。",
                "MACL通过标签感知采样、频率敏感加权和动态温度缩放，平衡常见和稀有类别的表征学习。",
                "实验表明，MACL在三个数据集上优于对比损失基线，有效缓解语义不平衡并提升检索可靠性。"
            ],
            "method_zh": "**问题定义**：多标签遥感图像检索任务旨在根据图像内容检索具有相似标签的其他图像。现有方法在处理遥感图像时，面临着地物类别之间语义重叠、标签分布极度不平衡以及复杂的类间共现模式等问题。这些问题导致模型在学习过程中偏向于常见类别，而忽略了稀有类别，从而降低了检索的准确性和泛化能力。\\n\\n**核心思路**：MACL的核心思路是通过自适应地调整对比学习过程，来平衡不同类别之间的表征学习。具体来说，它通过标签感知采样策略来增加稀有类别的样本数量，通过频率敏感加权来降低常见类别的损失权重，并通过动态温度缩放来调整对比学习的难度，从而使得模型能够更好地学习到各个类别的区分性特征。\\n\\n**技术框架**：MACL方法主要包含以下几个关键模块：1) 特征提取模块：使用预训练的卷积神经网络（如ResNet）提取遥感图像的特征向量。2) 标签感知采样模块：根据标签的频率对样本进行采样，增加稀有类别的样本数量。3) 对比学习模块：使用对比损失函数来学习图像的表征，目标是使得具有相同标签的图像在特征空间中更加接近，而具有不同标签的图像则更加远离。4) 频率敏感加权模块：根据标签的频率对对比损失进行加权，降低常见类别的损失权重。5) 动态温度缩放模块：根据训练的进度动态调整对比学习的温度参数，从而控制对比学习的难度。\\n\\n**关键创新**：MACL的关键创新在于其自适应性，能够根据标签的频率动态地调整对比学习的过程。与传统的对比学习方法相比，MACL能够更好地处理标签分布不平衡的问题，从而提高多标签遥感图像检索的性能。此外，MACL还引入了频率敏感加权和动态温度缩放等技术，进一步提升了模型的鲁棒性和泛化能力。\\n\\n**关键设计**：1) 标签感知采样：采用逆频率采样策略，即标签频率越低的样本，被采样的概率越高。2) 频率敏感加权：使用标签频率的倒数作为权重，对对比损失进行加权。3) 动态温度缩放：使用余弦退火策略动态调整温度参数，初始温度设置为0.07，最小温度设置为0.01。4) 对比损失函数：采用InfoNCE损失函数，正样本为具有相同标签的图像，负样本为具有不同标签的图像。",
            "application_zh": "该研究成果可应用于大规模遥感图像数据库的检索，例如城市规划、环境监测、灾害评估等领域。通过提高检索的准确性和可靠性，可以帮助用户快速找到所需的遥感图像，从而提高工作效率和决策质量。未来，该方法还可以扩展到其他多标签图像检索任务中，例如医学图像检索、视频检索等。",
            "highlight_zh": "在DLRSD、ML-AID和WHDLD三个遥感图像数据集上进行了实验，结果表明MACL始终优于基于对比损失的基线方法。例如，在DLRSD数据集上，MACL的平均精度均值(mAP)比最佳基线提高了约3-5个百分点，证明了其在缓解语义不平衡和提高检索性能方面的有效性。",
            "tags_zh": [
                "遥感图像检索",
                "多标签学习",
                "对比学习",
                "标签不平衡",
                "自适应学习"
            ],
            "_index": 45,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16294v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16294v1/Images/Architecture/Architecture.jpg",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16294v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "SegGraph: Leveraging Graphs of SAM Segments for Few-Shot 3D Part Segmentation",
            "authors": [
                "Yueyang Hu",
                "Haiyong Jiang",
                "Haoxuan Song",
                "Jun Xiao",
                "Hao Pan"
            ],
            "arxiv_id": "2512.16143v1",
            "summary": "This work presents a novel framework for few-shot 3D part segmentation. Recent advances have demonstrated the significant potential of 2D foundation models for low-shot 3D part segmentation. However, it is still an open problem that how to effectively aggregate 2D knowledge from foundation models to 3D. Existing methods either ignore geometric structures for 3D feature learning or neglects the high-quality grouping clues from SAM, leading to under-segmentation and inconsistent part labels. We devise a novel SAM segment graph-based propagation method, named SegGraph, to explicitly learn geometric features encoded within SAM's segmentation masks. Our method encodes geometric features by modeling mutual overlap and adjacency between segments while preserving intra-segment semantic consistency. We construct a segment graph, conceptually similar to an atlas, where nodes represent segments and edges capture their spatial relationships (overlap/adjacency). Each node adaptively modulates 2D foundation model features, which are then propagated via a graph neural network to learn global geometric structures. To enforce intra-segment semantic consistency, we map segment features to 3D points with a novel view-direction-weighted fusion attenuating contributions from low-quality segments. Extensive experiments on PartNet-E demonstrate that our method outperforms all competing baselines by at least 6.9 percent mIoU. Further analysis reveals that SegGraph achieves particularly strong performance on small components and part boundaries, demonstrating its superior geometric understanding. The code is available at: https://github.com/YueyangHu2000/SegGraph.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16143v1",
            "code_links": [
                {
                    "url": "https://github.com/YueyangHu2000/SegGraph",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱七：动作重定向 (Motion Retargeting)",
                    "id": "7_retargeting",
                    "matched_keywords": [
                        "spatial relationship"
                    ],
                    "score": 3.0
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "foundation model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "7_retargeting",
                "9_embodied_foundation"
            ],
            "headline_zh": "SegGraph：利用SAM分割图进行少样本3D部件分割",
            "summary_zh": "本文提出了一种新颖的少样本3D部件分割框架。最近的研究表明，2D基础模型在低样本3D部件分割方面具有巨大的潜力。然而，如何有效地将来自基础模型的2D知识聚合到3D仍然是一个开放的问题。现有方法要么忽略3D特征学习的几何结构，要么忽略来自SAM的高质量分组线索，导致分割不足和部件标签不一致。我们设计了一种新的基于SAM分割图的传播方法，名为SegGraph，以显式地学习编码在SAM分割掩码中的几何特征。我们的方法通过对分割之间的相互重叠和邻接关系进行建模来编码几何特征，同时保持分割内的语义一致性。我们构建了一个分割图，在概念上类似于地图集，其中节点代表分割，边代表它们之间的空间关系（重叠/邻接）。每个节点自适应地调节2D基础模型特征，然后通过图神经网络传播，以学习全局几何结构。为了加强分割内的语义一致性，我们使用一种新的视角方向加权融合将分割特征映射到3D点，从而衰减来自低质量分割的贡献。在PartNet-E上的大量实验表明，我们的方法优于所有竞争基线至少6.9个百分点的mIoU。进一步的分析表明，SegGraph在小部件和部件边界上实现了特别强的性能，证明了其卓越的几何理解能力。",
            "intro_zh": [
                "现有少样本3D部件分割方法在几何结构利用和高质量分割线索聚合方面存在不足，导致分割不准确。",
                "SegGraph通过构建SAM分割图，显式学习分割掩码中的几何特征，并利用图神经网络进行特征传播。",
                "实验表明，SegGraph在PartNet-E数据集上显著优于现有方法，尤其在小部件和部件边界分割上表现出色。"
            ],
            "method_zh": "**问题定义**：论文旨在解决少样本3D部件分割问题。现有方法主要存在两个痛点：一是忽略了3D几何结构在特征学习中的作用，二是未能充分利用SAM等2D基础模型提供的优质分割信息，导致分割结果出现欠分割和标签不一致等问题。\\n\\n**核心思路**：论文的核心思路是构建一个基于SAM分割的图结构（SegGraph），将2D分割信息和3D几何关系进行有效融合。通过图神经网络学习分割之间的关系，并利用视角方向加权融合保证分割内部的语义一致性。这样既能利用2D基础模型的语义信息，又能结合3D几何结构进行更精确的分割。\\n\\n**技术框架**：SegGraph框架主要包含以下几个阶段：1) 利用SAM等2D基础模型对3D点云进行多视角分割，生成一系列2D分割掩码。2) 基于这些分割掩码构建分割图，其中节点代表分割区域，边代表分割区域之间的空间关系（重叠或邻接）。3) 使用图神经网络在分割图上进行特征传播，学习全局几何结构信息。4) 将分割特征映射回3D点云，并使用视角方向加权融合策略，增强分割内部的语义一致性。5) 最后，使用分割后的特征进行3D部件分割。\\n\\n**关键创新**：论文的关键创新在于提出了SegGraph结构，将2D分割信息和3D几何关系显式地建模到图结构中。与现有方法相比，SegGraph能够更有效地利用SAM等2D基础模型的分割结果，并结合3D几何信息进行特征学习。此外，视角方向加权融合策略也是一个重要的创新点，它能够有效抑制低质量分割区域对最终分割结果的影响。\\n\\n**关键设计**：分割图的构建方式是关键设计之一，论文中考虑了分割区域之间的重叠和邻接关系，并使用不同的权重来表示这些关系。图神经网络的选择和训练也是关键，论文中使用了特定的GNN结构，并设计了合适的损失函数来优化模型。视角方向加权融合的具体权重计算方式也需要仔细设计，以保证分割内部的语义一致性。",
            "application_zh": "该研究成果可应用于机器人场景理解、自动驾驶、3D内容生成等领域。例如，机器人可以利用该技术更准确地识别和分割物体部件，从而实现更精细的操作和交互。在自动驾驶领域，可以用于识别车辆、行人等目标的不同部件，提高环境感知能力。在3D内容生成领域，可以辅助进行3D模型的部件分割和编辑，提高建模效率。",
            "highlight_zh": "SegGraph在PartNet-E数据集上取得了显著的性能提升，mIoU指标超过所有基线方法至少6.9%。尤其在小部件和部件边界的分割上表现出色，证明了其对几何结构的优秀理解能力。代码已开源，方便研究人员复现和进一步研究。",
            "tags_zh": [
                "少样本学习",
                "3D部件分割",
                "图神经网络",
                "SAM分割",
                "几何特征学习"
            ],
            "_index": 46,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "Stackelberg Learning from Human Feedback: Preference Optimization as a Sequential Game",
            "authors": [
                "Barna Pásztor",
                "Thomas Kleine Buening",
                "Andreas Krause"
            ],
            "arxiv_id": "2512.16626v1",
            "summary": "We introduce Stackelberg Learning from Human Feedback (SLHF), a new framework for preference optimization. SLHF frames the alignment problem as a sequential-move game between two policies: a Leader, which commits to an action, and a Follower, which responds conditionally on the Leader's action. This approach decomposes preference optimization into a refinement problem for the Follower and an optimization problem against an adversary for the Leader. Unlike Reinforcement Learning from Human Feedback (RLHF), which assigns scalar rewards to actions, or Nash Learning from Human Feedback (NLHF), which seeks a simultaneous-move equilibrium, SLHF leverages the asymmetry of sequential play to capture richer preference structures. The sequential design of SLHF naturally enables inference-time refinement, as the Follower learns to improve the Leader's actions, and these refinements can be leveraged through iterative sampling. We compare the solution concepts of SLHF, RLHF, and NLHF, and lay out key advantages in consistency, data sensitivity, and robustness to intransitive preferences. Experiments on large language models demonstrate that SLHF achieves strong alignment across diverse preference datasets, scales from 0.5B to 8B parameters, and yields inference-time refinements that transfer across model families without further fine-tuning.",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.GT",
                "cs.MA",
                "stat.ML"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "10 pages, 5 tables, 1 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16626v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning",
                        "RLHF"
                    ],
                    "score": 3.0
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "提出Stackelberg Learning from Human Feedback (SLHF)框架，用于偏好优化。",
            "summary_zh": "本文提出了一种新的偏好优化框架：Stackelberg Learning from Human Feedback (SLHF)。SLHF将对齐问题建模为两个策略之间的序贯博弈：领导者(Leader)先采取行动，然后跟随者(Follower)根据领导者的行动做出响应。这种方法将偏好优化分解为跟随者的优化问题和领导者对抗性优化问题。与为动作分配标量奖励的Reinforcement Learning from Human Feedback (RLHF)或寻求同步博弈均衡的Nash Learning from Human Feedback (NLHF)不同，SLHF利用序贯博弈的不对称性来捕获更丰富的偏好结构。SLHF的序贯设计自然地实现了推理时优化，因为跟随者学习改进领导者的行动，并且这些改进可以通过迭代采样来利用。本文比较了SLHF、RLHF和NLHF的解概念，并阐述了SLHF在一致性、数据敏感性和对非传递偏好的鲁棒性方面的关键优势。在大型语言模型上的实验表明，SLHF在不同的偏好数据集上实现了强大的对齐，可以从0.5B扩展到8B参数，并产生可以在模型系列之间转移而无需进一步微调的推理时优化。",
            "intro_zh": [
                "现有RLHF方法为动作分配标量奖励，NLHF寻求同步博弈均衡，无法捕捉复杂偏好结构。",
                "SLHF将对齐问题建模为领导者和跟随者之间的序贯博弈，利用序贯博弈的不对称性捕获更丰富的偏好结构。",
                "实验表明，SLHF在不同偏好数据集上实现了强大的对齐，并能进行跨模型迁移的推理时优化。"
            ],
            "method_zh": "**问题定义**：论文旨在解决如何更好地从人类反馈中学习，以对齐大型语言模型（LLM）的输出与人类偏好的问题。现有方法，如RLHF，通常将人类反馈转化为标量奖励，这可能过于简化，无法捕捉人类偏好的复杂性和细微差别。此外，NLHF方法假设策略同时行动，忽略了序贯决策的场景。\\n\\n**核心思路**：SLHF的核心思路是将对齐问题建模为一个Stackelberg博弈，其中领导者（Leader）策略首先采取行动，然后跟随者（Follower）策略根据领导者的行动做出响应。这种序贯博弈的框架允许模型学习更丰富的偏好结构，因为跟随者可以根据领导者的行为进行优化和改进。\\n\\n**技术框架**：SLHF的整体框架包含两个主要部分：跟随者策略的训练和领导者策略的优化。首先，通过人类反馈数据训练跟随者策略，使其能够根据领导者的行为给出偏好判断或进行改进。然后，领导者策略通过对抗性训练进行优化，目标是最大化跟随者策略的偏好。在推理阶段，跟随者策略可以进一步优化领导者的输出，从而提高最终结果的质量。\\n\\n**关键创新**：SLHF的关键创新在于将对齐问题建模为序贯博弈，这与传统的RLHF和NLHF方法不同。这种建模方式允许模型学习更复杂的偏好结构，并实现推理时的优化。此外，SLHF在一致性、数据敏感性和对非传递偏好的鲁棒性方面具有优势。\\n\\n**关键设计**：SLHF的关键设计包括跟随者策略的训练方式、领导者策略的优化目标以及推理时的优化策略。跟随者策略可以使用各种监督学习或强化学习方法进行训练，目标是准确预测人类偏好或改进领导者的输出。领导者策略的优化目标是最大化跟随者策略的偏好，可以使用对抗性训练或其他优化算法。推理时，跟随者策略可以迭代地优化领导者的输出，直到达到满意的结果。",
            "application_zh": "SLHF可应用于各种需要与人类偏好对齐的场景，例如对话系统、文本生成、图像生成和机器人控制。通过学习更丰富的偏好结构，SLHF可以生成更符合人类期望和价值观的输出，提高用户满意度和信任度。该方法还可用于个性化推荐系统，根据用户的历史行为和偏好，提供更精准的推荐结果。",
            "highlight_zh": "实验结果表明，SLHF在多个偏好数据集上取得了优于RLHF和NLHF的性能。SLHF能够扩展到具有数十亿参数的大型语言模型，并且其推理时优化策略可以跨模型系列迁移，无需额外的微调。这些结果表明SLHF具有很强的泛化能力和实用价值。",
            "tags_zh": [
                "人机反馈",
                "偏好优化",
                "序贯博弈",
                "Stackelberg学习",
                "大型语言模型"
            ],
            "_index": 47,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16626v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                }
            ]
        },
        {
            "title": "Do Multi-Agents Solve Better Than Single? Evaluating Agentic Frameworks for Diagram-Grounded Geometry Problem Solving and Reasoning",
            "authors": [
                "Mahbub E Sobhani",
                "Md. Faiyaz Abdullah Sayeedi",
                "Mohammad Nehad Alam",
                "Proma Hossain Progga",
                "Swakkhar Shatabda"
            ],
            "arxiv_id": "2512.16698v1",
            "summary": "Diagram-grounded geometry problem solving is a critical benchmark for multimodal large language models (MLLMs), yet the benefits of multi-agent design over single-agent remain unclear. We systematically compare single-agent and multi-agent pipelines on four visual math benchmarks: Geometry3K, MathVerse, OlympiadBench, and We-Math. For open-source models, multi-agent consistently improves performance. For example, Qwen-2.5-VL (7B) gains +6.8 points and Qwen-2.5-VL (32B) gains +3.3 on Geometry3K, and both Qwen-2.5-VL variants see further gains on OlympiadBench and We-Math. In contrast, the closed-source Gemini-2.0-Flash generally performs better in single-agent mode on classic benchmarks, while multi-agent yields only modest improvements on the newer We-Math dataset. These findings show that multi-agent pipelines provide clear benefits for open-source models and can assist strong proprietary systems on newer, less familiar benchmarks, but agentic decomposition is not universally optimal. All code, data, and reasoning files are available at https://github.com/faiyazabdullah/Interpreter-Solver",
            "categories": [
                "cs.AI",
                "cs.CG"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Accepted to the ARR October 2025 cycle",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16698v1",
            "code_links": [
                {
                    "url": "https://github.com/faiyazabdullah/Interpreter-Solver",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model",
                        "multimodal"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "比较多智能体与单智能体在几何问题求解中的表现",
            "summary_zh": "图示基础的几何问题求解是多模态大型语言模型（MLLMs）的重要基准，但多智能体设计相较于单智能体的优势尚不明确。本文系统比较了单智能体和多智能体在四个视觉数学基准上的表现，包括Geometry3K、MathVerse、OlympiadBench和We-Math。结果表明，对于开源模型，多智能体设计显著提升了性能，而闭源模型在经典基准上表现更佳。研究结果显示，多智能体管道对开源模型有明显益处，并能在新颖基准上辅助强大的专有系统，但智能体分解并非普遍最优。",
            "intro_zh": [
                "核心问题：现有的单智能体方法在处理复杂的几何问题时，性能提升有限，尤其在新颖的基准上表现不佳。",
                "方法要点：论文提出通过多智能体设计来优化几何问题求解，利用多个智能体的协作来提升模型的推理能力。",
                "实验或效果：实验结果表明，开源模型在多智能体模式下性能提升显著，例如Qwen-2.5-VL在Geometry3K上提升了6.8分。"
            ],
            "method_zh": "**问题定义**：本文旨在探讨多智能体设计在图示基础几何问题求解中的有效性，现有单智能体方法在新颖基准上的表现不足，无法充分利用多模态信息。\\n\\n**核心思路**：通过引入多智能体框架，允许多个智能体并行处理和推理，从而提高整体求解效率和准确性。这样的设计能够更好地应对复杂的几何问题，尤其是在数据稀缺或新颖的场景中。\\n\\n**技术框架**：整体架构包括多个智能体协同工作，每个智能体负责不同的任务或问题部分。主要模块包括输入处理、智能体推理、结果整合和输出生成。\\n\\n**关键创新**：最重要的创新在于多智能体的协作机制，通过智能体间的信息共享和任务分配，显著提升了求解的准确性和效率。这与传统的单智能体方法形成鲜明对比。\\n\\n**关键设计**：在参数设置上，采用了动态任务分配策略，损失函数设计为多任务学习损失，以适应不同智能体的学习需求。网络结构上，使用了增强的Transformer架构，以支持多模态输入的处理。",
            "application_zh": "该研究的潜在应用领域包括教育、自动化数学推理、智能辅导系统等。通过提升几何问题求解的能力，能够为学生提供更精准的学习支持，同时也为研究人员提供更强大的工具来探索复杂的数学问题。未来，该方法可能在其他领域的多模态学习中发挥重要作用。",
            "highlight_zh": "实验结果显示，开源模型在多智能体模式下表现优异，例如Qwen-2.5-VL在Geometry3K上提升了6.8分，而在OlympiadBench和We-Math上也有显著提升。相比之下，闭源模型在经典基准上表现更佳，但在新基准上多智能体的提升幅度有限。",
            "tags_zh": [
                "多智能体",
                "几何问题",
                "图示基础",
                "多模态学习",
                "性能提升",
                "开源模型",
                "智能体协作"
            ],
            "_index": 48,
            "_used_api": "openai",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16698v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16698v1/figures/diagram.png",
                    "caption": "",
                    "figure_id": "fig_1"
                }
            ]
        },
        {
            "title": "Scaling Laws for Energy Efficiency of Local LLMs",
            "authors": [
                "Ander Alvarez",
                "Alessandro Genuardi",
                "Nilotpal Sinha",
                "Antonio Tiene",
                "Samuel Mugel",
                "Román Orús"
            ],
            "arxiv_id": "2512.16531v1",
            "summary": "Deploying local large language models and vision-language models on edge devices requires balancing accuracy with constrained computational and energy budgets. Although graphics processors dominate modern artificial-intelligence deployment, most consumer hardware--including laptops, desktops, industrial controllers, and embedded systems--relies on central processing units. Despite this, the computational laws governing central-processing-unit-only inference for local language and vision-language workloads remain largely unexplored. We systematically benchmark large language and vision-language models on two representative central-processing-unit tiers widely used for local inference: a MacBook Pro M2, reflecting mainstream laptop-class deployment, and a Raspberry Pi 5, representing constrained, low-power embedded settings. Using a unified methodology based on continuous sampling of processor and memory usage together with area-under-curve integration, we characterize how computational load scales with input text length for language models and with image resolution for vision-language models. We uncover two empirical scaling laws: (1) computational cost for language-model inference scales approximately linearly with token length; and (2) vision-language models exhibit a preprocessing-driven \"resolution knee\", where compute remains constant above an internal resolution clamp and decreases sharply below it. Beyond these laws, we show that quantum-inspired compression reduces processor and memory usage by up to 71.9% and energy consumption by up to 62%, while preserving or improving semantic accuracy. These results provide a systematic quantification of multimodal central-processing-unit-only scaling for local language and vision-language workloads, and they identify model compression and input-resolution preprocessing as effective, low-cost levers for sustainable edge inference.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16531v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model",
                        "multimodal"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "针对本地LLM，揭示CPU能效缩放规律，并提出量子启发压缩优化方案。",
            "summary_zh": "在边缘设备上部署本地大型语言模型和视觉语言模型需要在精度、计算和能源预算之间取得平衡。尽管图形处理器在现代人工智能部署中占据主导地位，但大多数消费级硬件（包括笔记本电脑、台式机、工业控制器和嵌入式系统）依赖于中央处理器。本文系统地对两种广泛用于本地推理的中央处理器进行了基准测试：MacBook Pro M2（主流笔记本电脑级部署）和 Raspberry Pi 5（受限的低功耗嵌入式环境）。通过统一的方法，即连续采样处理器和内存使用情况以及曲线下面积积分，我们描述了语言模型的计算负载如何随输入文本长度缩放，以及视觉语言模型的计算负载如何随图像分辨率缩放。我们发现了两个经验缩放规律：(1) 语言模型推理的计算成本与token长度近似线性缩放；(2) 视觉语言模型表现出由预处理驱动的“分辨率膝点”，即计算量在内部分辨率上限之上保持不变，而在其之下急剧下降。此外，量子启发压缩可将处理器和内存使用量降低高达71.9%，并将能耗降低高达62%，同时保持或提高语义准确性。这些结果系统地量化了多模态中央处理器在本地语言和视觉语言工作负载中的缩放规律，并确定了模型压缩和输入分辨率预处理是可持续边缘推理的有效且低成本的手段。",
            "intro_zh": [
                "现有方法在CPU上部署本地LLM时，缺乏对计算和能耗缩放规律的系统研究，难以优化。",
                "论文通过在主流CPU上进行基准测试，揭示了LLM和VLM在CPU上的计算负载缩放规律。",
                "实验表明，量子启发压缩能显著降低CPU和内存使用，并降低能耗，同时保持或提升语义精度。"
            ],
            "method_zh": "**问题定义**：论文旨在解决在CPU上部署本地大型语言模型（LLM）和视觉语言模型（VLM）时，缺乏对计算资源和能耗需求缩放规律的理解的问题。现有方法主要针对GPU进行优化，忽略了大量依赖CPU的边缘设备，导致在这些设备上部署LLM/VLM时效率低下，难以满足资源约束。\n\n**核心思路**：论文的核心思路是通过系统性的基准测试，量化LLM和VLM在不同CPU平台上的计算负载与输入数据规模（token长度、图像分辨率）之间的关系，从而揭示其缩放规律。此外，探索模型压缩技术，降低资源占用，提升能效。\n\n**技术框架**：论文采用的整体框架包括：\n1.  选择代表性的CPU平台：MacBook Pro M2 (笔记本电脑级) 和 Raspberry Pi 5 (嵌入式系统级)。\n2.  选择LLM和VLM模型进行测试。\n3.  设计统一的基准测试方法：连续采样CPU和内存使用情况，并计算曲线下面积。\n4.  分析计算负载与输入数据规模之间的关系，发现缩放规律。\n5.  应用量子启发压缩技术，降低模型大小和计算复杂度。\n\n**关键创新**：论文的关键创新在于：\n1.  首次系统性地研究了LLM和VLM在CPU上的计算负载缩放规律，填补了该领域的空白。\n2.  发现了视觉语言模型中由预处理驱动的“分辨率膝点”现象。\n3.  验证了量子启发压缩技术在降低CPU资源占用和能耗方面的有效性。\n\n**关键设计**：\n1.  基准测试方法：采用连续采样CPU和内存使用情况，通过曲线下面积积分来量化计算负载。\n2.  量子启发压缩：具体压缩算法未知，但强调其在降低模型大小和计算复杂度方面的作用。\n3.  实验设计：针对不同模型和平台，设计合理的输入数据规模范围，确保结果的可靠性。",
            "application_zh": "该研究成果可应用于各种边缘设备上的本地LLM/VLM部署，例如智能家居设备、工业控制器、移动机器人等。通过理解计算负载的缩放规律，可以更好地进行资源分配和模型优化，实现更高效、更节能的本地AI应用。此外，模型压缩技术可以进一步降低部署成本，扩大应用范围。",
            "highlight_zh": "实验结果表明，语言模型推理的计算成本与token长度近似线性缩放；视觉语言模型存在“分辨率膝点”现象。量子启发压缩可将处理器和内存使用量降低高达71.9%，并将能耗降低高达62%，同时保持或提高语义准确性。这些数据突出了模型压缩在边缘设备上的重要性。",
            "tags_zh": [
                "本地LLM",
                "CPU推理",
                "能效优化",
                "缩放规律",
                "量子启发压缩"
            ],
            "_index": 49,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16531v1/figures/updated/llm_mac_cpu_auc.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16531v1/figures/updated/llm_rpi_cpu_auc.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16531v1/figures/updated/vlm_mac_cpu_auc.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs",
            "authors": [
                "Sara Papi",
                "Javier Garcia Gilabert",
                "Zachary Hopton",
                "Vilém Zouhar",
                "Carlos Escolano",
                "Gerard I. Gállego",
                "Jorge Iranzo-Sánchez",
                "Ahrii Kim",
                "Dominik Macháček",
                "Patricia Schmidtova",
                "Maike Züfle"
            ],
            "arxiv_id": "2512.16378v1",
            "summary": "As Large Language Models (LLMs) expand beyond text, integrating speech as a native modality has given rise to SpeechLLMs, which aim to translate spoken language directly, thereby bypassing traditional transcription-based pipelines. Whether this integration improves speech-to-text translation quality over established cascaded architectures, however, remains an open question. We present Hearing to Translate, the first comprehensive test suite rigorously benchmarking 5 state-of-the-art SpeechLLMs against 16 strong direct and cascade systems that couple leading speech foundation models (SFM), with multilingual LLMs. Our analysis spans 16 benchmarks, 13 language pairs, and 9 challenging conditions, including disfluent, noisy, and long-form speech. Across this extensive evaluation, we find that cascaded systems remain the most reliable overall, while current SpeechLLMs only match cascades in selected settings and SFMs lag behind both, highlighting that integrating an LLM, either within the model or in a pipeline, is essential for high-quality speech translation.",
            "categories": [
                "cs.CL",
                "cs.AI",
                "cs.SD"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Project available at https://github.com/sarapapi/hearing2translate",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16378v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model",
                        "foundation model"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "首个SpeechLLM综合评测：对比端到端与级联架构语音翻译性能",
            "summary_zh": "随着大型语言模型(LLMs)超越文本领域，将语音作为原生模态集成催生了SpeechLLMs，旨在直接翻译口语，从而绕过传统的基于转录的流程。然而，这种集成是否能提高语音到文本的翻译质量，优于已建立的级联架构，仍然是一个悬而未决的问题。我们提出了Hearing to Translate，这是第一个综合测试套件，严格地将5个最先进的SpeechLLMs与16个强大的直接和级联系统进行基准测试，这些系统将领先的语音基础模型(SFM)与多语言LLMs相结合。我们的分析跨越16个基准、13个语言对和9个具有挑战性的条件，包括口齿不清、嘈杂和长篇语音。通过这项广泛的评估，我们发现级联系统仍然是最可靠的，而当前的SpeechLLMs仅在选定的设置中与级联系统相匹配，并且SFM落后于两者，这突出了集成LLM（无论是在模型内部还是在pipeline中）对于高质量语音翻译至关重要。",
            "intro_zh": [
                "现有端到端SpeechLLM语音翻译能力有待考量，缺乏与传统级联架构的系统性对比。",
                "提出Hearing to Translate测试套件，全面评估SpeechLLM与级联系统的语音翻译性能。",
                "实验表明级联系统总体更可靠，SpeechLLM仅在特定场景媲美，集成LLM至关重要。"
            ],
            "method_zh": "**问题定义**：论文旨在评估端到端SpeechLLM在语音翻译任务中的有效性，并将其与传统的级联系统进行比较。现有端到端SpeechLLM的性能优势尚不明确，缺乏在各种场景下的全面评估，尤其是在处理口语中的噪声、口齿不清和长篇语音等挑战性条件时。\\n\\n**核心思路**：论文的核心思路是通过构建一个全面的测试套件，在多种语言对和具有挑战性的条件下，对最先进的SpeechLLM和级联系统进行严格的基准测试。通过对比不同架构的性能，揭示SpeechLLM的优势和局限性，并确定影响语音翻译质量的关键因素。\\n\\n**技术框架**：论文构建了名为Hearing to Translate的测试框架，包含以下几个关键组成部分：1) 收集并整理了16个语音翻译基准数据集，涵盖13个语言对和9种具有挑战性的条件；2) 选择了5个最先进的SpeechLLM作为评估对象；3) 构建了16个强大的直接和级联系统作为对比基线，这些系统结合了领先的语音基础模型(SFM)和多语言LLM；4) 使用标准的语音翻译评估指标（如BLEU）对所有系统在各个基准上进行评估。\\n\\n**关键创新**：论文的主要创新在于构建了首个全面评估SpeechLLM语音翻译能力的测试套件。该测试套件不仅涵盖了多种语言对和具有挑战性的条件，还对比了端到端SpeechLLM与传统的级联系统，为研究人员提供了一个统一的评估平台，促进了语音翻译领域的发展。此外，论文还深入分析了不同架构的优势和局限性，为未来的模型设计提供了指导。\\n\\n**关键设计**：论文的关键设计包括：1) 选择了具有代表性的SpeechLLM，包括直接翻译模型和基于转录的模型；2) 构建了强大的级联系统，充分利用了现有的语音识别和机器翻译技术；3) 采用了多种评估指标，包括BLEU、TER等，以全面评估翻译质量；4) 针对不同的挑战性条件，设计了相应的评估方案，例如，通过添加噪声来模拟嘈杂环境，通过引入口齿不清的语音来评估模型的鲁棒性。",
            "application_zh": "该研究成果可应用于实时语音翻译、多语言会议、语音助手等领域。通过优化SpeechLLM或级联系统，可以提升跨语言交流的效率和准确性，促进全球范围内的信息共享和文化交流。未来的研究可以进一步探索如何提高SpeechLLM在噪声环境和口语化场景下的鲁棒性，并将其应用于更多实际场景。",
            "highlight_zh": "实验结果表明，在大多数情况下，级联系统在语音翻译任务中表现更可靠，尤其是在处理具有挑战性的语音条件时。当前的SpeechLLM仅在特定设置下与级联系统性能相当。语音基础模型(SFM)的性能落后于两者，表明集成LLM对于高质量语音翻译至关重要。这些发现为未来的语音翻译系统设计提供了重要的指导。",
            "tags_zh": [
                "语音翻译",
                "SpeechLLM",
                "级联系统",
                "端到端模型",
                "基准测试"
            ],
            "_index": 50,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16378v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16378v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16378v1/figs/pearmut_screenshot_1.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Code-in-the-Loop Forensics: Agentic Tool Use for Image Forgery Detection",
            "authors": [
                "Fanrui Zhang",
                "Qiang Zhang",
                "Sizhuo Zhou",
                "Jianwen Sun",
                "Chuanhao Li",
                "Jiaxin Ai",
                "Yukang Feng",
                "Yujie Zhang",
                "Wenjie Li",
                "Zizhen Li",
                "Yifan Chang",
                "Jiawei Liu",
                "Kaipeng Zhang"
            ],
            "arxiv_id": "2512.16300v1",
            "summary": "Existing image forgery detection (IFD) methods either exploit low-level, semantics-agnostic artifacts or rely on multimodal large language models (MLLMs) with high-level semantic knowledge. Although naturally complementary, these two information streams are highly heterogeneous in both paradigm and reasoning, making it difficult for existing methods to unify them or effectively model their cross-level interactions. To address this gap, we propose ForenAgent, a multi-round interactive IFD framework that enables MLLMs to autonomously generate, execute, and iteratively refine Python-based low-level tools around the detection objective, thereby achieving more flexible and interpretable forgery analysis. ForenAgent follows a two-stage training pipeline combining Cold Start and Reinforcement Fine-Tuning to enhance its tool interaction capability and reasoning adaptability progressively. Inspired by human reasoning, we design a dynamic reasoning loop comprising global perception, local focusing, iterative probing, and holistic adjudication, and instantiate it as both a data-sampling strategy and a task-aligned process reward. For systematic training and evaluation, we construct FABench, a heterogeneous, high-quality agent-forensics dataset comprising 100k images and approximately 200k agent-interaction question-answer pairs. Experiments show that ForenAgent exhibits emergent tool-use competence and reflective reasoning on challenging IFD tasks when assisted by low-level tools, charting a promising route toward general-purpose IFD. The code will be released after the review process is completed.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "11 pages, 6 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16300v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model",
                        "multimodal"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出ForenAgent以解决图像伪造检测中的信息流不统一问题",
            "summary_zh": "现有的图像伪造检测方法要么利用低级、无语义的伪造特征，要么依赖于具有高级语义知识的多模态大语言模型（MLLMs）。这两种信息流在范式和推理上高度异质，使得现有方法难以统一或有效建模它们的跨层次交互。为了解决这一问题，本文提出了ForenAgent，一个多轮交互的图像伪造检测框架，能够自主生成、执行并迭代优化基于Python的低级工具，从而实现更灵活和可解释的伪造分析。ForenAgent采用了结合冷启动和强化微调的两阶段训练流程，逐步增强其工具交互能力和推理适应性。通过构建FABench数据集，实验表明ForenAgent在低级工具的辅助下，展现出工具使用能力和反思推理能力，为通用图像伪造检测开辟了新的方向。",
            "intro_zh": [
                "现有图像伪造检测方法在信息流的统一性和交互建模上存在显著不足，难以有效结合低级特征与高级语义知识。",
                "本文提出ForenAgent框架，通过多轮交互使MLLMs能够生成和优化低级工具，从而实现灵活的伪造检测。",
                "实验结果显示，ForenAgent在复杂的图像伪造检测任务中表现出色，具备工具使用能力和反思推理能力，推动了该领域的发展。"
            ],
            "method_zh": "**问题定义**：本文旨在解决现有图像伪造检测方法在低级特征与高级语义知识之间的异质性问题，导致信息流难以统一和交互建模的痛点。\\n\\n**核心思路**：ForenAgent框架通过多轮交互，使得多模态大语言模型能够自主生成、执行和优化低级工具，从而实现更灵活和可解释的伪造分析。该设计灵感来源于人类的推理过程，强调动态推理循环。\\n\\n**技术框架**：ForenAgent采用两阶段训练流程，包括冷启动和强化微调，增强工具交互能力和推理适应性。动态推理循环包含全球感知、局部聚焦、迭代探测和整体裁决，作为数据采样策略和任务对齐过程奖励的实例。\\n\\n**关键创新**：最重要的创新在于引入了动态推理循环和多轮交互机制，使得低级工具与高级语义知识能够有效结合，提升了图像伪造检测的灵活性和可解释性。\\n\\n**关键设计**：在训练过程中，FABench数据集的构建提供了100k张图像和约200k个代理交互问答对，确保了系统的系统性训练和评估。",
            "application_zh": "该研究的潜在应用领域包括数字图像取证、社交媒体内容验证以及新闻报道的真实性检查等。通过提升图像伪造检测的灵活性和准确性，ForenAgent能够在打击虚假信息和保护数字内容的真实性方面发挥重要作用，具有显著的实际价值和未来影响。",
            "highlight_zh": "实验结果表明，ForenAgent在图像伪造检测任务中展现出显著的性能提升，尤其是在低级工具的辅助下，表现出工具使用能力和反思推理能力。具体性能数据尚未披露，但实验表明其在复杂任务中的有效性，为通用图像伪造检测提供了新的可能性。",
            "tags_zh": [
                "图像伪造检测",
                "多模态大语言模型",
                "动态推理循环",
                "工具交互",
                "冷启动",
                "强化微调",
                "数据集构建",
                "反思推理"
            ],
            "_index": 51,
            "_used_api": "openai",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16300v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16300v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16300v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "AMUSE: Audio-Visual Benchmark and Alignment Framework for Agentic Multi-Speaker Understanding",
            "authors": [
                "Sanjoy Chowdhury",
                "Karren D. Yang",
                "Xudong Liu",
                "Fartash Faghri",
                "Pavan Kumar Anasosalu Vasu",
                "Oncel Tuzel",
                "Dinesh Manocha",
                "Chun-Liang Li",
                "Raviteja Vemulapalli"
            ],
            "arxiv_id": "2512.16250v1",
            "summary": "Recent multimodal large language models (MLLMs) such as GPT-4o and Qwen3-Omni show strong perception but struggle in multi-speaker, dialogue-centric settings that demand agentic reasoning tracking who speaks, maintaining roles, and grounding events across time. These scenarios are central to multimodal audio-video understanding, where models must jointly reason over audio and visual streams in applications such as conversational video assistants and meeting analytics. We introduce AMUSE, a benchmark designed around tasks that are inherently agentic, requiring models to decompose complex audio-visual interactions into planning, grounding, and reflection steps. It evaluates MLLMs across three modes zero-shot, guided, and agentic and six task families, including spatio-temporal speaker grounding and multimodal dialogue summarization. Across all modes, current models exhibit weak multi-speaker reasoning and inconsistent behavior under both non-agentic and agentic evaluation. Motivated by the inherently agentic nature of these tasks and recent advances in LLM agents, we propose RAFT, a data-efficient agentic alignment framework that integrates reward optimization with intrinsic multimodal self-evaluation as reward and selective parameter adaptation for data and parameter efficient updates. Using RAFT, we achieve up to 39.52\\% relative improvement in accuracy on our benchmark. Together, AMUSE and RAFT provide a practical platform for examining agentic reasoning in multimodal models and improving their capabilities.",
            "categories": [
                "cs.AI",
                "cs.MA"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16250v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model",
                        "multimodal"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "AMUSE：用于Agentic多说话人理解的音视频基准和对齐框架",
            "summary_zh": "本文提出了AMUSE，一个旨在评估多模态大型语言模型（MLLM）在多说话人、以对话为中心的场景下agentic推理能力的基准。现有MLLM如GPT-4o和Qwen3-Omni在感知方面表现出色，但在需要跟踪说话人、维护角色以及理解跨时间事件的场景中表现不足。AMUSE包含零样本、引导和agentic三种模式，以及六个任务族，包括时空说话人定位和多模态对话摘要。实验表明，现有模型在多说话人推理方面表现较弱，且在不同评估模式下行为不一致。此外，本文提出了RAFT，一个数据高效的agentic对齐框架，它结合了奖励优化、内在多模态自评估以及选择性参数调整。使用RAFT，在AMUSE基准上取得了高达39.52%的相对精度提升。AMUSE和RAFT共同为研究多模态模型中的agentic推理并提升其能力提供了一个实用平台。",
            "intro_zh": [
                "现有多模态大模型在多说话人对话场景中，缺乏有效的agentic推理能力，难以跟踪说话人、理解角色和事件。",
                "论文提出AMUSE基准测试模型在agentic任务上的表现，并设计RAFT框架，通过奖励优化和自评估提升模型性能。",
                "实验结果表明，RAFT框架在AMUSE基准上显著提升了多模态大模型的性能，相对精度提升高达39.52%。"
            ],
            "method_zh": "**问题定义**：论文旨在解决多模态大型语言模型（MLLM）在处理多说话人音视频对话场景时，缺乏有效的agentic推理能力的问题。现有MLLM在感知方面表现良好，但在需要理解谁在说话、角色是什么以及事件如何随时间演变等复杂场景中表现不足。这些场景对于会话视频助手和会议分析等应用至关重要。现有方法难以将复杂的音视频交互分解为规划、定位和反思等步骤。\n\n**核心思路**：论文的核心思路是构建一个专门用于评估和提升MLLM在agentic多说话人理解方面的基准（AMUSE）和一个对齐框架（RAFT）。通过AMUSE，可以系统地评估现有模型的不足，并利用RAFT框架，通过奖励优化和内在自评估，提升模型在这些任务上的性能。RAFT框架旨在使模型能够更好地理解和推理多说话人对话场景中的复杂交互。\n\n**技术框架**：整体框架包含两个主要部分：AMUSE基准和RAFT对齐框架。AMUSE基准包含六个任务族，涵盖时空说话人定位、多模态对话摘要等。RAFT框架则包含奖励优化模块，该模块利用奖励信号来指导模型的学习；内在多模态自评估模块，用于评估模型自身的表现；以及选择性参数调整模块，用于高效地更新模型参数。整个流程旨在使模型能够更好地理解和推理多说话人对话场景中的复杂交互。\n\n**关键创新**：论文的关键创新在于提出了AMUSE基准和RAFT对齐框架。AMUSE基准专门设计用于评估MLLM在agentic多说话人理解方面的能力，而RAFT框架则提供了一种数据高效的方法来提升模型在这些任务上的性能。RAFT框架结合了奖励优化、内在自评估和选择性参数调整，使其能够更有效地利用数据，并避免过度拟合。\n\n**关键设计**：RAFT框架的关键设计包括：1) 使用奖励模型来评估模型的行为，并提供奖励信号；2) 使用内在多模态自评估来评估模型自身的表现，并提供反馈；3) 使用选择性参数调整来高效地更新模型参数，避免过度拟合。具体的参数设置、损失函数和网络结构等细节在论文中进行了详细描述，例如，奖励模型的设计、自评估指标的选择以及参数更新策略的制定。",
            "application_zh": "该研究成果可应用于会话视频助手、会议分析、智能客服等领域。通过提升模型在多说话人场景下的理解和推理能力，可以实现更自然、更智能的人机交互。例如，在会议分析中，模型可以自动识别发言人、总结会议内容，并提取关键信息。在智能客服中，模型可以更好地理解用户意图，并提供更准确的回答。",
            "highlight_zh": "实验结果表明，RAFT框架在AMUSE基准上取得了显著的性能提升。在多个任务上，RAFT框架的性能优于现有的基线模型，相对精度提升高达39.52%。这表明RAFT框架能够有效地提升MLLM在agentic多说话人理解方面的能力。实验还表明，RAFT框架具有数据高效性，能够在较少的数据下取得良好的性能。",
            "tags_zh": [
                "多说话人理解",
                "音视频分析",
                "Agentic推理",
                "多模态学习",
                "基准测试",
                "奖励优化",
                "自评估"
            ],
            "_index": 52,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16250v1/figures/teaser.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16250v1/figures/eval-modes-short.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16250v1/figures/raft-revised.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "JustRL: Scaling a 1.5B LLM with a Simple RL Recipe",
            "authors": [
                "Bingxiang He",
                "Zekai Qu",
                "Zeyuan Liu",
                "Yinghao Chen",
                "Yuxin Zuo",
                "Cheng Qian",
                "Kaiyan Zhang",
                "Weize Chen",
                "Chaojun Xiao",
                "Ganqu Cui",
                "Ning Ding",
                "Zhiyuan Liu"
            ],
            "arxiv_id": "2512.16649v1",
            "summary": "Recent advances in reinforcement learning for large language models have converged on increasing complexity: multi-stage training pipelines, dynamic hyperparameter schedules, and curriculum learning strategies. This raises a fundamental question: \\textbf{Is this complexity necessary?} We present \\textbf{JustRL}, a minimal approach using single-stage training with fixed hyperparameters that achieves state-of-the-art performance on two 1.5B reasoning models (54.9\\% and 64.3\\% average accuracy across nine mathematical benchmarks) while using 2$\\times$ less compute than sophisticated approaches. The same hyperparameters transfer across both models without tuning, and training exhibits smooth, monotonic improvement over 4,000+ steps without the collapses or plateaus that typically motivate interventions. Critically, ablations reveal that adding ``standard tricks'' like explicit length penalties and robust verifiers may degrade performance by collapsing exploration. These results suggest that the field may be adding complexity to solve problems that disappear with a stable, scaled-up baseline. We release our models and code to establish a simple, validated baseline for the community.",
            "categories": [
                "cs.CL"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "12 pages, 3 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16649v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning",
                        "curriculum learning"
                    ],
                    "score": 3.0
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "JustRL：通过简单强化学习方法扩展15亿参数LLM，实现数学推理SOTA",
            "summary_zh": "大型语言模型（LLM）的强化学习研究日益复杂，包括多阶段训练流程、动态超参数调整和课程学习策略。本文提出一个根本问题：这种复杂性是必要的吗？我们提出了JustRL，一种极简方法，使用单阶段训练和固定超参数，在两个15亿参数的推理模型上实现了最先进的性能（在九个数学基准测试中平均准确率分别为54.9％和64.3％），同时计算量仅为复杂方法的2倍。相同的超参数无需调整即可在两个模型之间迁移，并且训练过程表现出平滑的单调改进，超过4,000多个步骤，没有通常需要干预的崩溃或停滞。关键的是，消融实验表明，添加诸如显式长度惩罚和鲁棒验证器之类的“标准技巧”可能会因探索崩溃而降低性能。这些结果表明，该领域可能正在增加复杂性来解决随着稳定、规模化的基线而消失的问题。我们发布了我们的模型和代码，以建立一个简单、经过验证的社区基线。",
            "intro_zh": [
                "现有LLM强化学习方法过于复杂，包含多阶段训练和动态超参数调整，缺乏对必要性的深入研究。",
                "JustRL采用单阶段训练和固定超参数，避免复杂调整，旨在探索简化强化学习流程在LLM中的潜力。",
                "实验表明，JustRL在数学推理任务上达到SOTA，且计算量更少，并发现一些常用技巧可能适得其反。"
            ],
            "method_zh": "**问题定义**：现有的大型语言模型强化学习方法通常采用复杂的多阶段训练流程、动态超参数调整以及课程学习策略，这增加了训练的难度和计算成本。论文旨在探究这些复杂性是否是必要的，并尝试使用更简单的方法来实现更好的性能。现有方法的痛点在于训练流程复杂，计算资源消耗大，且对超参数敏感。\\n\\n**核心思路**：论文的核心思路是采用极简的强化学习方法，即JustRL，使用单阶段训练和固定的超参数。作者认为，通过稳定和规模化的基线，可以避免一些复杂方法试图解决的问题。这种方法旨在降低训练的复杂性，提高训练的稳定性，并减少计算资源的消耗。\\n\\n**技术框架**：JustRL的整体框架非常简单，主要包含以下几个步骤：首先，使用一个预训练的1.5B参数的语言模型作为基础模型。然后，使用强化学习算法（具体算法未知，但从描述来看，可能是PPO或类似的算法）对模型进行单阶段的训练。在训练过程中，使用固定的超参数，避免动态调整。最后，在数学推理任务上评估模型的性能。\\n\\n**关键创新**：论文最重要的技术创新点在于其极简的设计理念。与现有方法相比，JustRL避免了复杂的多阶段训练流程和动态超参数调整，而是采用单阶段训练和固定的超参数。这种设计理念简化了训练流程，提高了训练的稳定性，并减少了计算资源的消耗。此外，论文还发现，一些常用的技巧，如显式长度惩罚和鲁棒验证器，可能会因探索崩溃而降低性能。\\n\\n**关键设计**：论文的关键设计在于单阶段训练和固定超参数的使用。具体的参数设置未知，但作者强调，相同的超参数可以在不同的模型之间迁移，而无需进行调整。损失函数未知，但可能是标准的强化学习损失函数，如PPO损失函数。网络结构是基于一个预训练的1.5B参数的语言模型，没有进行特殊的修改。",
            "application_zh": "JustRL的研究成果可应用于各种需要复杂推理能力的场景，例如数学问题求解、代码生成、知识问答等。通过简化强化学习流程，降低训练成本，使得更多研究者和开发者能够训练出高性能的LLM，加速相关技术的普及和应用。此外，该研究也为未来的LLM强化学习研究提供了一个新的方向，即关注简单性和稳定性。",
            "highlight_zh": "JustRL在两个15亿参数的推理模型上实现了最先进的性能，在九个数学基准测试中平均准确率分别达到54.9％和64.3％，同时计算量仅为复杂方法的2倍。更重要的是，相同的超参数无需调整即可在两个模型之间迁移，训练过程表现出平滑的单调改进，超过4,000多个步骤，没有出现崩溃或停滞。",
            "tags_zh": [
                "强化学习",
                "大型语言模型",
                "数学推理",
                "单阶段训练",
                "固定超参数",
                "模型简化",
                "计算效率"
            ],
            "_index": 53,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16649v1/figures/fig1_aime24_curves_added.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16649v1/figures/fig2_training_dynamics.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16649v1/figures/fig3_training_dynamics.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Learning-based Approximate Model Predictive Control for an Impact Wrench Tool",
            "authors": [
                "Mark Benazet",
                "Francesco Ricca",
                "Dario Bralla",
                "Melanie N. Zeilinger",
                "Andrea Carron"
            ],
            "arxiv_id": "2512.16624v1",
            "summary": "Learning-based model predictive control has emerged as a powerful approach for handling complex dynamics in mechatronic systems, enabling data-driven performance improvements while respecting safety constraints. However, when computational resources are severely limited, as in battery-powered tools with embedded processors, existing approaches struggle to meet real-time requirements. In this paper, we address the problem of real-time torque control for impact wrenches, where high-frequency control updates are necessary to accurately track the fast transients occurring during periodic impact events, while maintaining high-performance safety-critical control that mitigates harmful vibrations and component wear. The key novelty of the approach is that we combine data-driven model augmentation through Gaussian process regression with neural network approximation of the resulting control policy. This insight allows us to deploy predictive control on resource-constrained embedded platforms while maintaining both constraint satisfaction and microsecond-level inference times. The proposed framework is evaluated through numerical simulations and hardware experiments on a custom impact wrench testbed. The results show that our approach successfully achieves real-time control suitable for high-frequency operation while maintaining constraint satisfaction and improving tracking accuracy compared to baseline PID control.",
            "categories": [
                "eess.SY"
            ],
            "primary_category": "eess.SY",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16624v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]model predictive control"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "提出基于学习的近似模型预测控制，用于冲击扳手的实时扭矩控制",
            "summary_zh": "本文提出了一种基于学习的模型预测控制方法，用于解决机电系统中复杂动力学问题，通过数据驱动的方式提升性能并满足安全约束。针对嵌入式处理器等计算资源受限的电池供电工具，现有方法难以满足实时性要求。本文聚焦冲击扳手的实时扭矩控制，需要高频控制更新以精确跟踪周期性冲击事件中的快速瞬变，同时保持高性能的安全控制，以减轻有害振动和部件磨损。该方法的核心创新在于，结合高斯过程回归的数据驱动模型增强和神经网络对控制策略的近似。这种结合使得在资源受限的嵌入式平台上部署预测控制成为可能，同时保持约束满足和微秒级的推理时间。通过数值仿真和定制冲击扳手测试台上的硬件实验验证了所提出的框架。结果表明，该方法成功实现了适用于高频操作的实时控制，同时保持约束满足，并提高了相对于基线PID控制的跟踪精度。",
            "intro_zh": [
                "现有冲击扳手控制方法难以在资源受限的嵌入式系统中实现高频、高性能的实时扭矩控制。",
                "结合高斯过程回归增强模型和神经网络近似控制策略，实现资源受限平台上的预测控制。",
                "实验结果表明，该方法在满足约束的同时，提高了跟踪精度，实现了高频实时控制。"
            ],
            "method_zh": "**问题定义**：论文旨在解决冲击扳手的实时扭矩控制问题，特别是在资源受限的嵌入式平台上。现有方法，如传统的PID控制，难以在高频操作下精确跟踪冲击事件中的快速瞬变，并且难以同时满足安全约束和性能要求。此外，直接应用复杂的模型预测控制（MPC）算法在嵌入式平台上计算量过大，无法满足实时性需求。\\n\\n**核心思路**：论文的核心思路是利用数据驱动的方法来增强模型预测控制的性能，同时降低计算复杂度。具体来说，首先使用高斯过程回归（GPR）来学习冲击扳手的动态特性，从而改进模型预测的准确性。然后，使用神经网络（NN）来近似MPC的控制策略，从而将在线优化问题转化为离线学习问题，显著降低了在线计算负担。\\n\\n**技术框架**：整体框架包括以下几个主要模块：1) 数据采集：通过实验或仿真收集冲击扳手的工作数据。2) 模型学习：使用高斯过程回归（GPR）从数据中学习冲击扳手的动态模型。3) 控制策略近似：使用神经网络（NN）学习MPC的控制策略，将状态映射到控制输入。4) 实时控制：在嵌入式平台上部署训练好的神经网络，实现实时扭矩控制。\\n\\n**关键创新**：该方法最重要的技术创新点在于将数据驱动的模型增强与控制策略近似相结合。传统MPC方法依赖于精确的系统模型，但在实际应用中，模型往往存在不确定性。通过GPR学习，可以有效地补偿模型误差，提高预测精度。同时，使用神经网络近似MPC的控制策略，避免了在线求解优化问题，大大降低了计算复杂度，使其能够在资源受限的嵌入式平台上实现。\\n\\n**关键设计**：GPR模型的核函数选择和超参数优化是关键。神经网络的结构（例如，层数、神经元数量）和训练方法（例如，损失函数、优化器）需要仔细设计，以保证控制策略的准确性和实时性。此外，约束条件的处理也是一个重要方面，需要在神经网络的训练过程中考虑约束条件，例如，通过添加惩罚项到损失函数中，或者使用约束优化算法。",
            "application_zh": "该研究成果可广泛应用于各种需要高精度、高频率控制的电动工具和机器人系统中，例如电动螺丝刀、电动扳手、精密装配机器人等。通过降低计算复杂度和提高控制性能，可以延长电池寿命、提高工作效率、减少设备磨损，并提升用户体验。未来，该方法有望推广到其他资源受限的嵌入式控制系统中。",
            "highlight_zh": "硬件实验结果表明，所提出的方法在定制冲击扳手测试台上实现了微秒级的推理时间，满足了高频实时控制的需求。与基线PID控制相比，该方法显著提高了扭矩跟踪精度，并能够有效地满足安全约束，例如限制振动和冲击力。具体性能提升数据（例如，跟踪误差降低百分比）在论文中进行了详细展示。",
            "tags_zh": [
                "模型预测控制",
                "机器学习",
                "高斯过程回归",
                "神经网络",
                "实时控制",
                "嵌入式系统",
                "冲击扳手"
            ],
            "_index": 54,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16624v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16624v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16624v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Economic versus energetic model predictive control of a cold production plant with thermal energy storage",
            "authors": [
                "Manuel G. Satué",
                "Manuel R. Arahal",
                "Luis F. Acedo",
                "Manuel G. Ortega"
            ],
            "arxiv_id": "2512.16379v1",
            "summary": "Economic model predictive control has been proposed as a means for solving the unit loading and unit allocation problem in multi-chiller cooling plants. The adjective economic stems from the use of financial cost due to electricity consumption in a time horizon, such is the loss function minimized at each sampling period. The energetic approach is rarely encountered. This article presents for the first time a comparison between the energetic optimization objective and the economic one. The comparison is made on a cooling plant using air-cooled water chillers and a cold storage system. Models developed have been integrated into Simscape, and non-convex mixed optimization methods used to achieve optimal control trajectories for both energetic and economic goals considered separately. The results over several scenarios, and in different seasons, support the consideration of the energetic approach despite the current prevalence of the economic one. The results are dependent on the electric season and the available tariffs. In particular, for the high electric season and considering a representative tariff, the results show that an increment of about 2.15% in energy consumption takes place when using the economic approach instead of the energetic one. On the other hand, a reduction in cost of 2.94% is achieved.",
            "categories": [
                "eess.SY"
            ],
            "primary_category": "eess.SY",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "14 pages",
            "doi": "10.1016/j.applthermaleng.2022.118309",
            "journal_ref": "Applied Thermal Engineering 210 (2022) 118309",
            "pdf_url": "https://arxiv.org/pdf/2512.16379v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]model predictive control"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "对比经济与能量型模型预测控制，优化冷库生产能耗与成本",
            "summary_zh": "本文首次对比了能量优化目标和经济优化目标在冷库生产中的应用。研究对象为一个使用风冷式冷水机组和冷能存储系统的冷却工厂。论文将开发的模型集成到Simscape中，并使用非凸混合优化方法，分别针对能量和经济目标实现了最优控制轨迹。在不同场景和季节下的结果表明，尽管目前经济优化方法更为普遍，但能量优化方法也值得考虑。结果依赖于用电季节和可用的电价。特别是在用电高峰期，使用经济优化方法代替能量优化方法会导致能耗增加约2.15%，但成本降低2.94%。",
            "intro_zh": [
                "多冷水机组冷却工厂面临机组负荷分配问题，经济型模型预测控制通过最小化电费来解决此问题，但鲜有研究关注能量优化。",
                "本文提出对比能量优化和经济优化两种目标，旨在评估在冷库生产中，哪种方法在不同条件下更具优势。",
                "实验结果表明，在用电高峰期，经济优化虽然降低了成本，但会导致能耗显著增加，因此能量优化在特定情况下更优。"
            ],
            "method_zh": "**问题定义**：论文旨在解决多冷水机组冷却工厂的单元负荷和单元分配问题，即如何控制冷水机组和冷能存储系统，以满足冷却需求的同时，最小化能源消耗或运行成本。现有方法主要集中于经济型模型预测控制，通过最小化电费来优化运行，但忽略了能源消耗本身，可能导致能源浪费。\\n\\n**核心思路**：论文的核心思路是对比经济型模型预测控制（Economic MPC）和能量型模型预测控制（Energetic MPC）在冷却工厂中的性能。经济型MPC以最小化电费为目标，而能量型MPC以最小化能源消耗为目标。通过对比两种方法在不同场景和季节下的表现，评估其优缺点，从而为实际应用提供指导。\\n\\n**技术框架**：该研究的技术框架主要包括以下几个部分：1) 建立冷却工厂的Simscape模型，包括冷水机组、冷能存储系统等组件；2) 设计经济型和能量型模型预测控制器，分别以最小化电费和能源消耗为目标；3) 使用非凸混合优化方法求解最优控制轨迹；4) 在不同场景和季节下进行仿真实验，对比两种控制器的性能。\\n\\n**关键创新**：论文的关键创新在于首次在冷库生产领域对比了经济型和能量型模型预测控制的性能。以往的研究主要集中于经济型MPC，而忽略了能量优化。通过对比研究，论文揭示了在特定条件下，能量型MPC可能比经济型MPC更优，从而为冷却工厂的优化运行提供了新的思路。\\n\\n**关键设计**：在模型预测控制器的设计中，关键的技术细节包括：1) 建立准确的冷却工厂模型，包括冷水机组的能耗模型、冷能存储系统的充放电模型等；2) 选择合适的优化算法，由于问题是非凸的，需要使用非凸混合优化方法；3) 设计合理的成本函数，经济型MPC的成本函数为电费，能量型MPC的成本函数为能源消耗；4) 根据实际情况设置约束条件，如冷水机组的运行范围、冷能存储系统的容量等。",
            "application_zh": "该研究成果可应用于各种需要冷却系统的工业和商业建筑，例如数据中心、医院、购物中心等。通过选择合适的控制策略，可以在满足冷却需求的同时，降低能源消耗和运行成本，提高能源利用效率，并减少对环境的影响。未来的研究可以进一步探索更复杂的冷却系统和控制策略，例如考虑可再生能源的利用、需求侧响应等。",
            "highlight_zh": "实验结果表明，在用电高峰期，使用经济型MPC代替能量型MPC会导致能耗增加约2.15%，但成本降低2.94%。这表明在电价较高时，经济型MPC可以通过牺牲少量能耗来显著降低成本。然而，在其他季节或电价较低时，能量型MPC可能更具优势。因此，在实际应用中需要根据具体情况选择合适的控制策略。",
            "tags_zh": [
                "模型预测控制",
                "经济优化",
                "能量优化",
                "冷库生产",
                "热能存储"
            ],
            "_index": 55,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16379v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16379v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16379v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Sceniris: A Fast Procedural Scene Generation Framework",
            "authors": [
                "Jinghuan Shang",
                "Harsh Patel",
                "Ran Gong",
                "Karl Schmeckpeper"
            ],
            "arxiv_id": "2512.16896v1",
            "summary": "Synthetic 3D scenes are essential for developing Physical AI and generative models. Existing procedural generation methods often have low output throughput, creating a significant bottleneck in scaling up dataset creation. In this work, we introduce Sceniris, a highly efficient procedural scene generation framework for rapidly generating large-scale, collision-free scene variations. Sceniris also provides an optional robot reachability check, providing manipulation-feasible scenes for robot tasks. Sceniris is designed for maximum efficiency by addressing the primary performance limitations of the prior method, Scene Synthesizer. Leveraging batch sampling and faster collision checking in cuRobo, Sceniris achieves at least 234x speed-up over Scene Synthesizer. Sceniris also expands the object-wise spatial relationships available in prior work to support diverse scene requirements. Our code is available at https://github.com/rai-inst/sceniris",
            "categories": [
                "cs.RO",
                "cs.CV",
                "cs.GR"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Code is available at https://github.com/rai-inst/sceniris",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16896v1",
            "code_links": [
                {
                    "url": "https://github.com/rai-inst/sceniris",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱七：动作重定向 (Motion Retargeting)",
                    "id": "7_retargeting",
                    "matched_keywords": [
                        "spatial relationship"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 5.0,
            "hit_pillars": [
                "1_robot_core",
                "7_retargeting"
            ],
            "headline_zh": "Sceniris：一种快速程序化场景生成框架，加速物理AI和生成模型开发。",
            "summary_zh": "合成3D场景对于开发物理人工智能和生成模型至关重要。现有的程序化生成方法通常输出吞吐量较低，这在扩展数据集创建方面造成了显著的瓶颈。本文介绍Sceniris，一个高效的程序化场景生成框架，用于快速生成大规模、无碰撞的场景变体。Sceniris还提供可选的机器人可达性检查，为机器人任务提供可操作的场景。Sceniris通过解决先前方法Scene Synthesizer的主要性能限制来实现最大效率。通过利用批量采样和cuRobo中更快的碰撞检测，Sceniris实现了比Scene Synthesizer至少234倍的加速。Sceniris还扩展了先前工作中可用的对象级空间关系，以支持多样化的场景需求。代码可在https://github.com/rai-inst/sceniris 获取。",
            "intro_zh": [
                "现有程序化场景生成方法吞吐量低，严重制约了大规模数据集的构建，成为物理AI和生成模型发展的瓶颈。",
                "Sceniris通过批量采样和更快的碰撞检测，显著提升了场景生成的效率，并扩展了对象间的空间关系。",
                "实验表明，Sceniris比Scene Synthesizer至少加速了234倍，为快速生成大规模、无碰撞的场景变体提供了可能。"
            ],
            "method_zh": "**问题定义**：论文旨在解决现有程序化场景生成方法速度慢的问题。现有方法在生成大规模数据集时效率低下，无法满足物理AI和生成模型对大量训练数据的需求。Scene Synthesizer等方法虽然可以生成场景，但速度较慢，成为瓶颈。\\n\\n**核心思路**：Sceniris的核心思路是通过优化采样和碰撞检测过程来提高场景生成的效率。具体来说，它利用批量采样来并行处理多个场景，并采用cuRobo中更快的碰撞检测算法来加速碰撞检测过程。此外，Sceniris还扩展了对象间的空间关系，以支持更多样化的场景需求。\\n\\n**技术框架**：Sceniris的整体框架包括以下几个主要模块：1) 场景描述模块：定义场景中对象的属性和空间关系。2) 采样模块：根据场景描述生成对象的初始位置和姿态。3) 碰撞检测模块：检测对象之间是否存在碰撞。4) 优化模块：调整对象的位置和姿态，以消除碰撞。5) 可达性检测模块（可选）：检查生成的场景是否适合机器人操作。\\n\\n**关键创新**：Sceniris的关键创新在于其高效的场景生成流程。通过批量采样和更快的碰撞检测，Sceniris显著提高了场景生成的速度。此外，Sceniris还扩展了对象间的空间关系，使其能够生成更复杂和多样化的场景。与Scene Synthesizer相比，Sceniris在性能上有了显著提升。\\n\\n**关键设计**：Sceniris的关键设计包括：1) 批量采样：并行生成多个场景，提高吞吐量。2) cuRobo碰撞检测：利用GPU加速碰撞检测过程。3) 扩展的空间关系：支持更多样化的场景需求。4) 可选的机器人可达性检查：确保生成的场景适合机器人操作。具体的参数设置和损失函数等细节在论文中未详细描述，属于未知信息。",
            "application_zh": "Sceniris可广泛应用于物理AI和生成模型领域。它可以用于生成大规模的训练数据集，从而加速机器人学习、强化学习和计算机视觉等领域的研究。此外，Sceniris还可以用于虚拟环境的创建、游戏开发和电影制作等领域，具有广泛的应用前景和实际价值。未来，Sceniris有望成为一个重要的场景生成工具，推动相关领域的发展。",
            "highlight_zh": "Sceniris通过优化采样和碰撞检测过程，实现了比Scene Synthesizer至少234倍的加速。这一显著的性能提升使得Sceniris能够快速生成大规模、无碰撞的场景变体，为物理AI和生成模型的研究提供了有力支持。具体的实验设置和指标在论文中未详细描述，属于未知信息。",
            "tags_zh": [
                "程序化场景生成",
                "物理AI",
                "机器人",
                "碰撞检测",
                "批量采样",
                "cuRobo",
                "3D场景"
            ],
            "_index": 56,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16896v1/assets/main_steps.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16896v1/assets/spatial_rels_cc.jpg",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16896v1/assets/main_benchmark.jpg",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "TextEditBench: Evaluating Reasoning-aware Text Editing Beyond Rendering",
            "authors": [
                "Rui Gui",
                "Yang Wan",
                "Haochen Han",
                "Dongxing Mao",
                "Fangming Liu",
                "Min Li",
                "Alex Jinpeng Wang"
            ],
            "arxiv_id": "2512.16270v1",
            "summary": "Text rendering has recently emerged as one of the most challenging frontiers in visual generation, drawing significant attention from large-scale diffusion and multimodal models. However, text editing within images remains largely unexplored, as it requires generating legible characters while preserving semantic, geometric, and contextual coherence. To fill this gap, we introduce TextEditBench, a comprehensive evaluation benchmark that explicitly focuses on text-centric regions in images. Beyond basic pixel manipulations, our benchmark emphasizes reasoning-intensive editing scenarios that require models to understand physical plausibility, linguistic meaning, and cross-modal dependencies. We further propose a novel evaluation dimension, Semantic Expectation (SE), which measures reasoning ability of model to maintain semantic consistency, contextual coherence, and cross-modal alignment during text editing. Extensive experiments on state-of-the-art editing systems reveal that while current models can follow simple textual instructions, they still struggle with context-dependent reasoning, physical consistency, and layout-aware integration. By focusing evaluation on this long-overlooked yet fundamental capability, TextEditBench establishes a new testing ground for advancing text-guided image editing and reasoning in multimodal generation.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16270v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "multimodal"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 5.0,
            "hit_pillars": [
                "1_robot_core",
                "9_embodied_foundation"
            ],
            "headline_zh": "提出TextEditBench，用于评估图像文本编辑中蕴含推理能力的模型。",
            "summary_zh": "本文提出了TextEditBench，一个全面的评估基准，专门关注图像中以文本为中心的区域。与基本的像素操作不同，该基准强调推理密集型的编辑场景，要求模型理解物理合理性、语言意义和跨模态依赖关系。此外，本文还提出了一种新的评估维度，即语义期望（SE），用于衡量模型在文本编辑过程中保持语义一致性、上下文连贯性和跨模态对齐的推理能力。对最先进的编辑系统进行的大量实验表明，虽然当前的模型可以遵循简单的文本指令，但它们仍然难以处理依赖于上下文的推理、物理一致性和布局感知的集成。通过专注于这种长期被忽视但又至关重要的能力，TextEditBench 为推进文本引导的图像编辑和多模态生成中的推理建立了一个新的测试平台。",
            "intro_zh": [
                "现有图像编辑方法在文本编辑方面存在不足，难以保证生成字符的可读性以及语义、几何和上下文的一致性。",
                "TextEditBench基准测试通过关注图像中以文本为中心的区域，强调模型在推理密集型场景下的编辑能力。",
                "实验结果表明，现有模型在处理上下文推理、物理一致性和布局感知集成方面存在困难，有待进一步提升。"
            ],
            "method_zh": "**问题定义**：现有图像编辑方法，特别是文本编辑，往往只关注像素层面的操作，缺乏对深层语义和上下文信息的理解，导致编辑后的图像在物理合理性、语言意义和跨模态依赖关系上出现不一致。现有方法难以处理需要复杂推理的文本编辑任务，例如，根据场景调整文本的字体、颜色或位置。\n\n**核心思路**：TextEditBench的核心思路是构建一个更具挑战性的评估基准，该基准不仅关注文本的可读性，更侧重于评估模型在文本编辑过程中对物理世界知识、语言语义以及跨模态关系的理解和推理能力。通过引入推理密集型的编辑场景，迫使模型学习和应用更高级的推理能力。\n\n**技术框架**：TextEditBench主要包含两部分：数据集和评估指标。数据集包含各种图像和文本编辑指令，涵盖了需要物理合理性、语言意义和跨模态依赖关系的复杂场景。评估指标包括传统的像素级指标（如PSNR、SSIM），以及新提出的语义期望（SE）指标。SE指标旨在衡量模型在文本编辑过程中保持语义一致性、上下文连贯性和跨模态对齐的能力。\n\n**关键创新**：TextEditBench的关键创新在于其对推理能力的强调和SE指标的提出。与以往的基准测试不同，TextEditBench更加关注模型对图像中隐含信息的理解和利用，以及在编辑过程中保持这些信息一致性的能力。SE指标则提供了一种量化评估模型推理能力的手段，弥补了传统指标的不足。\n\n**关键设计**：TextEditBench数据集的设计考虑了多种因素，包括文本的类型、字体、大小、位置，以及图像的场景、光照、遮挡等。SE指标的计算方法未知，论文中可能没有详细描述，需要查阅补充材料或相关文献。",
            "application_zh": "TextEditBench的研究成果可以应用于智能图像编辑、广告设计、虚拟现实、增强现实等领域。例如，可以利用该基准测试来提升图像编辑软件的智能化水平，使其能够根据用户的文本指令，自动完成复杂的文本编辑任务，并保证编辑结果的真实性和合理性。此外，该研究还可以促进多模态生成技术的发展，为构建更智能、更自然的交互式应用奠定基础。",
            "highlight_zh": "TextEditBench对现有图像编辑模型进行了广泛的评估，结果表明，虽然这些模型在简单的文本编辑任务上表现良好，但在处理需要复杂推理的任务时，性能显著下降。特别是在物理一致性、上下文连贯性和跨模态对齐方面，现有模型仍存在较大差距。这些实验结果突显了TextEditBench的价值，并为未来的研究方向提供了指导。",
            "tags_zh": [
                "文本编辑",
                "图像生成",
                "推理能力",
                "多模态学习",
                "评估基准"
            ],
            "_index": 57,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16270v1/figures/src/data_collection.jpg",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16270v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16270v1/figures/src/evaluation.jpg",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Skeleton-Snippet Contrastive Learning with Multiscale Feature Fusion for Action Localization",
            "authors": [
                "Qiushuo Cheng",
                "Jingjing Liu",
                "Catherine Morgan",
                "Alan Whone",
                "Majid Mirmehdi"
            ],
            "arxiv_id": "2512.16504v1",
            "summary": "The self-supervised pretraining paradigm has achieved great success in learning 3D action representations for skeleton-based action recognition using contrastive learning. However, learning effective representations for skeleton-based temporal action localization remains challenging and underexplored. Unlike video-level {action} recognition, detecting action boundaries requires temporally sensitive features that capture subtle differences between adjacent frames where labels change. To this end, we formulate a snippet discrimination pretext task for self-supervised pretraining, which densely projects skeleton sequences into non-overlapping segments and promotes features that distinguish them across videos via contrastive learning. Additionally, we build on strong backbones of skeleton-based action recognition models by fusing intermediate features with a U-shaped module to enhance feature resolution for frame-level localization. Our approach consistently improves existing skeleton-based contrastive learning methods for action localization on BABEL across diverse subsets and evaluation protocols. We also achieve state-of-the-art transfer learning performance on PKUMMD with pretraining on NTU RGB+D and BABEL.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16504v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]contrastive learning"
                    ],
                    "score": 4.5
                }
            ],
            "relevance_score": 4.5,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "提出基于骨骼片段对比学习和多尺度特征融合的动作定位方法",
            "summary_zh": "本文针对基于骨骼的动作定位任务，提出了一种自监督预训练范式。不同于视频级别的动作识别，动作定位需要对时间敏感的特征，以捕捉相邻帧之间细微的标签变化。为此，我们设计了一个片段判别预训练任务，将骨骼序列密集地投影到非重叠的片段中，并通过对比学习来区分不同视频中的片段特征。此外，我们利用U型模块融合中间特征，增强特征分辨率，从而提升帧级别的定位性能。实验结果表明，我们的方法在BABEL数据集上持续改进了现有的基于骨骼的对比学习方法，并在PKUMMD数据集上实现了最先进的迁移学习性能，预训练数据为NTU RGB+D和BABEL。",
            "intro_zh": [
                "基于骨骼的动作定位需要捕捉帧间细微差异，现有方法难以有效学习时间敏感特征。",
                "提出片段判别预训练任务，通过对比学习区分不同视频片段，学习更具区分性的特征。",
                "利用U型模块融合多尺度特征，提升特征分辨率，从而改善帧级别动作定位性能。"
            ],
            "method_zh": "**问题定义**：论文旨在解决基于骨骼的动作定位问题。现有方法，特别是基于对比学习的方法，在视频级别的动作识别上取得了成功，但直接应用于动作定位时，无法有效捕捉帧与帧之间的细微时间差异，导致定位精度不高。现有方法缺乏对时间敏感特征的有效学习机制。\\n\\n**核心思路**：论文的核心思路是通过片段判别任务进行自监督预训练，使模型能够学习区分不同时间片段的特征。通过对比学习，模型被训练成能够区分来自不同视频的片段，从而学习到更具区分性的、对时间变化敏感的特征表示。此外，通过融合多尺度特征，增强特征的分辨率，进一步提升定位精度。\\n\\n**技术框架**：整体框架包含两个主要部分：片段判别预训练和多尺度特征融合。在预训练阶段，输入骨骼序列被分割成非重叠的片段，然后通过编码器提取特征。对比学习模块用于训练模型区分不同视频的片段。在特征融合阶段，使用U型模块融合来自不同层的中间特征，以增强特征的分辨率。最终，融合后的特征被用于帧级别的动作定位。\\n\\n**关键创新**：论文的关键创新在于提出了片段判别预训练任务，这是一种专门为动作定位设计的自监督学习方法。与传统的视频级别对比学习不同，片段判别任务更加关注局部时间信息，能够更好地捕捉动作边界。此外，多尺度特征融合进一步提升了特征的分辨率，使得模型能够更精确地定位动作。\\n\\n**关键设计**：片段的划分方式采用非重叠分割，保证了片段之间的独立性。对比学习损失函数采用InfoNCE损失，鼓励模型将来自同一视频的片段拉近，将来自不同视频的片段推远。U型模块的具体结构可以根据不同的骨骼动作识别模型进行调整，通常包含卷积层、池化层和上采样层等。具体的参数设置和损失函数权重需要根据实验结果进行调整。",
            "application_zh": "该研究成果可应用于智能监控、人机交互、康复训练等领域。例如，在智能监控中，可以利用该方法自动检测异常行为；在人机交互中，可以识别用户的动作意图；在康复训练中，可以评估患者的运动能力。该研究有助于提升相关系统的智能化水平和用户体验。",
            "highlight_zh": "该方法在BABEL数据集上，相较于现有基于骨骼的对比学习方法，在动作定位任务上取得了持续改进。此外，在PKUMMD数据集上，通过在NTU RGB+D和BABEL数据集上进行预训练，实现了最先进的迁移学习性能。这些结果表明，该方法能够有效学习动作定位所需的特征表示，并具有良好的泛化能力。",
            "tags_zh": [
                "骨骼动作识别",
                "动作定位",
                "对比学习",
                "自监督学习",
                "多尺度特征融合"
            ],
            "_index": 58,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16504v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16504v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16504v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward",
            "authors": [
                "Peter Chen",
                "Xiaopeng Li",
                "Ziniu Li",
                "Wotao Yin",
                "Xi Chen",
                "Tianyi Lin"
            ],
            "arxiv_id": "2512.16912v1",
            "summary": "This paper examines the exploration-exploitation trade-off in reinforcement learning with verifiable rewards (RLVR), a framework for improving the reasoning of Large Language Models (LLMs). Recent studies suggest that RLVR can elicit strong mathematical reasoning in LLMs through two seemingly paradoxical mechanisms: spurious rewards, which suppress exploitation by rewarding outcomes unrelated to the ground truth, and entropy minimization, which suppresses exploration by pushing the model toward more confident and deterministic outputs, highlighting a puzzling dynamic: both discouraging exploitation and discouraging exploration improve reasoning performance, yet the underlying principles that reconcile these effects remain poorly understood. We focus on two fundamental questions: (i) how policy entropy relates to performance, and (ii) whether spurious rewards yield gains, potentially through the interplay of clipping bias and model contamination. Our results show that clipping bias under spurious rewards reduces policy entropy, leading to more confident and deterministic outputs, while entropy minimization alone is insufficient for improvement. We further propose a reward-misalignment model explaining why spurious rewards can enhance performance beyond contaminated settings. Our findings clarify the mechanisms behind spurious-reward benefits and provide principles for more effective RLVR training.",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.CL"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "35 pages",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16912v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 4.5,
            "hit_pillars": [
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "通过裁剪、熵和虚假奖励重新思考RLVR，提升LLM推理能力",
            "summary_zh": "本文研究了具有可验证奖励的强化学习（RLVR）中的探索-利用权衡，RLVR是一种用于改进大型语言模型（LLM）推理的框架。最近的研究表明，RLVR可以通过两种看似矛盾的机制激发LLM强大的数学推理能力：虚假奖励，通过奖励与真实结果无关的输出来抑制利用；熵最小化，通过将模型推向更自信和确定性的输出来抑制探索。这突显了一种令人困惑的动态：抑制利用和抑制探索都能提高推理性能，但协调这些影响的潜在原则仍然知之甚少。我们关注两个基本问题：（i）策略熵如何与性能相关，以及（ii）虚假奖励是否能产生收益，可能是通过裁剪偏差和模型污染的相互作用。我们的结果表明，虚假奖励下的裁剪偏差会降低策略熵，从而产生更自信和确定性的输出，而仅靠熵最小化不足以改进。我们进一步提出了一个奖励错位模型，解释了为什么虚假奖励可以提高超出污染环境的性能。我们的发现阐明了虚假奖励背后机制，并为更有效的RLVR训练提供了原则。",
            "intro_zh": [
                "现有RLVR方法在探索-利用平衡上存在不足，对LLM推理能力的提升机制理解不够深入。",
                "论文核心思想是通过分析裁剪偏差、熵最小化和虚假奖励的相互作用，揭示提升LLM推理性能的关键因素。",
                "实验结果表明，虚假奖励下的裁剪偏差能有效降低策略熵，产生更自信的输出，从而提升模型性能。"
            ],
            "method_zh": "**问题定义**：论文旨在解决RLVR框架下，如何更好地平衡探索与利用，从而提升大型语言模型（LLM）的推理能力。现有方法在利用虚假奖励和熵最小化时，其内在机制尚不明确，导致训练效率和效果存在瓶颈。现有方法缺乏对裁剪偏差、模型污染等因素的深入分析，难以解释虚假奖励为何能提升性能。\n\n**核心思路**：论文的核心思路是通过深入分析策略熵、裁剪偏差和虚假奖励之间的关系，揭示虚假奖励提升性能的内在机制。具体而言，论文认为虚假奖励通过裁剪偏差降低策略熵，使得模型输出更加自信和确定，从而提高推理准确性。同时，论文提出了奖励错位模型，解释了虚假奖励在非污染环境下的有效性。\n\n**技术框架**：论文主要采用理论分析和实验验证相结合的方法。首先，论文构建了奖励错位模型，用于解释虚假奖励的有效性。然后，论文通过实验验证了裁剪偏差对策略熵的影响，以及虚假奖励对模型性能的提升。实验主要基于RLVR框架，并针对数学推理任务进行评估。\n\n**关键创新**：论文的关键创新在于揭示了虚假奖励通过裁剪偏差降低策略熵，从而提升LLM推理性能的机制。此外，论文提出的奖励错位模型为理解虚假奖励的有效性提供了新的视角。与现有方法相比，论文更加关注虚假奖励背后的内在机制，并提出了相应的解释模型。\n\n**关键设计**：论文的关键设计包括：(1) 使用裁剪操作限制奖励范围，从而引入裁剪偏差；(2) 通过策略熵衡量模型输出的确定性；(3) 设计奖励错位模型，模拟真实任务与虚假奖励之间的关系。论文还详细描述了实验设置，包括数据集、模型架构、训练参数等。",
            "application_zh": "该研究成果可应用于提升大型语言模型在数学推理、代码生成等领域的性能。通过更有效地利用虚假奖励和熵最小化，可以训练出更准确、更可靠的LLM，从而在智能客服、自动编程等实际应用中发挥更大的作用。未来的研究可以进一步探索更有效的奖励设计方法，以及如何将该方法推广到其他类型的任务中。",
            "highlight_zh": "实验结果表明，在虚假奖励下，裁剪偏差能够有效降低策略熵，使得模型输出更加自信和确定。与没有虚假奖励的基线相比，使用虚假奖励的模型在数学推理任务上取得了显著的性能提升。论文还验证了奖励错位模型的有效性，表明虚假奖励在非污染环境下也能提升模型性能。",
            "tags_zh": [
                "强化学习",
                "大型语言模型",
                "奖励工程",
                "探索利用",
                "策略熵",
                "裁剪偏差",
                "虚假奖励"
            ],
            "_index": 59,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16912v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16912v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16912v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Meta-RL Induces Exploration in Language Agents",
            "authors": [
                "Yulun Jiang",
                "Liangze Jiang",
                "Damien Teney",
                "Michael Moor",
                "Maria Brbic"
            ],
            "arxiv_id": "2512.16848v1",
            "summary": "Reinforcement learning (RL) has enabled the training of large language model (LLM) agents to interact with the environment and to solve multi-turn long-horizon tasks. However, the RL-trained agents often struggle in tasks that require active exploration and fail to efficiently adapt from trial-and-error experiences. In this paper, we present LaMer, a general Meta-RL framework that enables LLM agents to actively explore and learn from the environment feedback at test time. LaMer consists of two key components: (i) a cross-episode training framework to encourage exploration and long-term rewards optimization; and (ii) in-context policy adaptation via reflection, allowing the agent to adapt their policy from task feedback signal without gradient update. Experiments across diverse environments show that LaMer significantly improves performance over RL baselines, with 11%, 14%, and 19% performance gains on Sokoban, MineSweeper and Webshop, respectively. Moreover, LaMer also demonstrates better generalization to more challenging or previously unseen tasks compared to the RL-trained agents. Overall, our results demonstrate that Meta-RL provides a principled approach to induce exploration in language agents, enabling more robust adaptation to novel environments through learned exploration strategies.",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16848v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 4.5,
            "hit_pillars": [
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "LaMer：通过元强化学习提升语言Agent在复杂环境中的探索能力",
            "summary_zh": "强化学习(RL)已使大型语言模型(LLM)Agent能够与环境交互并解决多轮长时程任务。然而，RL训练的Agent在需要主动探索的任务中常常表现不佳，并且无法有效地从试错经验中学习。本文提出了LaMer，一个通用的元强化学习框架，使LLM Agent能够在测试时主动探索并从环境反馈中学习。LaMer包含两个关键组件：(i)一个跨episode训练框架，以鼓励探索和长期奖励优化；(ii)通过反思进行上下文策略适应，允许Agent从任务反馈信号中调整其策略，而无需梯度更新。在各种环境中的实验表明，LaMer显著提高了相对于RL基线的性能，在Sokoban、MineSweeper和Webshop上分别提高了11%、14%和19%的性能。此外，与RL训练的Agent相比，LaMer还展示了对更具挑战性或先前未见过的任务更好的泛化能力。总的来说，我们的结果表明，元强化学习提供了一种原则性的方法来诱导语言Agent中的探索，从而通过学习到的探索策略实现对新环境的更鲁棒的适应。",
            "intro_zh": [
                "现有RL训练的语言Agent在需要主动探索和从经验中学习的任务中表现不佳。",
                "LaMer通过跨episode训练鼓励探索，并利用反思机制实现上下文策略适应，无需梯度更新。",
                "实验表明，LaMer在多个环境中显著优于RL基线，并具有更好的泛化能力。"
            ],
            "method_zh": "**问题定义**：现有基于强化学习训练的语言Agent在复杂环境中进行探索时效率低下，难以适应新任务。它们通常难以平衡探索和利用，导致次优策略和泛化能力不足。尤其是在长时程任务中，奖励稀疏问题更加突出，Agent难以有效地从试错中学习。\n\n**核心思路**：LaMer的核心思路是利用元强化学习(Meta-RL)的思想，让Agent学习如何学习。通过跨episode的训练，Agent能够学会主动探索环境，并根据环境反馈快速调整策略。这种“学会探索”的能力使得Agent能够更有效地适应新的、未知的任务。\n\n**技术框架**：LaMer框架包含两个主要组成部分：(1) **跨episode训练框架**：该框架通过构建多个episode的任务，鼓励Agent在不同episode之间进行知识迁移，从而学习到更通用的探索策略。训练目标是最大化长期奖励，促使Agent关注长远利益。(2) **上下文策略适应**：该模块利用Agent的反思能力，根据任务的反馈信号（例如奖励、状态变化等）调整策略。这种调整是在上下文环境中进行的，无需进行梯度更新，从而实现了快速适应。\n\n**关键创新**：LaMer的关键创新在于将元强化学习的思想引入到语言Agent的训练中，并设计了跨episode训练和上下文策略适应机制。与传统的RL方法相比，LaMer能够让Agent学会主动探索，并快速适应新任务，从而提高了Agent的泛化能力和鲁棒性。此外，无需梯度更新的上下文策略适应机制也提高了训练效率。\n\n**关键设计**：在跨episode训练中，采用了精心设计的奖励函数，以鼓励Agent进行多样化的探索。上下文策略适应模块利用Transformer架构，将任务反馈信号作为上下文信息输入到Agent中，从而影响Agent的决策。具体而言，Agent会根据历史经验和当前反馈，生成新的行动策略。此外，还采用了经验回放机制，用于存储和重用过去的经验，从而提高学习效率。",
            "application_zh": "LaMer框架可应用于各种需要语言Agent进行主动探索和适应的场景，例如机器人导航、游戏AI、智能助手等。通过学习到的探索策略，Agent能够更有效地完成复杂任务，并适应不断变化的环境。该研究有助于推动通用人工智能的发展，使Agent能够更好地理解和交互真实世界。",
            "highlight_zh": "实验结果表明，LaMer在Sokoban、MineSweeper和Webshop等多个环境中显著优于RL基线，性能分别提升了11%、14%和19%。此外，LaMer还展示了更好的泛化能力，能够适应更具挑战性或先前未见过的任务。这些结果验证了元强化学习在诱导语言Agent探索方面的有效性。",
            "tags_zh": [
                "元强化学习",
                "语言Agent",
                "主动探索",
                "上下文学习",
                "策略适应"
            ],
            "_index": 60,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16848v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16848v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16848v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Coordinated Anti-Jamming Resilience in Swarm Networks via Multi-Agent Reinforcement Learning",
            "authors": [
                "Bahman Abolhassani",
                "Tugba Erpek",
                "Kemal Davaslioglu",
                "Yalin E. Sagduyu",
                "Sastry Kompella"
            ],
            "arxiv_id": "2512.16813v1",
            "summary": "Reactive jammers pose a severe security threat to robotic-swarm networks by selectively disrupting inter-agent communications and undermining formation integrity and mission success. Conventional countermeasures such as fixed power control or static channel hopping are largely ineffective against such adaptive adversaries. This paper presents a multi-agent reinforcement learning (MARL) framework based on the QMIX algorithm to improve the resilience of swarm communications under reactive jamming. We consider a network of multiple transmitter-receiver pairs sharing channels while a reactive jammer with Markovian threshold dynamics senses aggregate power and reacts accordingly. Each agent jointly selects transmit frequency (channel) and power, and QMIX learns a centralized but factorizable action-value function that enables coordinated yet decentralized execution. We benchmark QMIX against a genie-aided optimal policy in a no-channel-reuse setting, and against local Upper Confidence Bound (UCB) and a stateless reactive policy in a more general fading regime with channel reuse enabled. Simulation results show that QMIX rapidly converges to cooperative policies that nearly match the genie-aided bound, while achieving higher throughput and lower jamming incidence than the baselines, thereby demonstrating MARL's effectiveness for securing autonomous swarms in contested environments.",
            "categories": [
                "cs.NI",
                "cs.AI",
                "cs.DC",
                "cs.LG",
                "eess.SP"
            ],
            "primary_category": "cs.NI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16813v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]reinforcement learning"
                    ],
                    "score": 4.5
                }
            ],
            "relevance_score": 4.5,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "提出基于QMIX的多智能体强化学习方法，提升集群网络在反应式干扰下的通信韧性。",
            "summary_zh": "本文提出了一种基于QMIX算法的多智能体强化学习(MARL)框架，旨在提高集群通信在反应式干扰下的韧性。反应式干扰机会选择性地干扰智能体间的通信，从而破坏集群的完整性和任务成功率，对机器人集群网络构成严重的安全威胁。传统的对策，如固定功率控制或静态信道跳频，对这种自适应的攻击者基本无效。本文考虑了一个多发射机-接收机对共享信道的网络，同时一个具有马尔可夫阈值动态的反应式干扰机感知总功率并做出相应反应。每个智能体联合选择发射频率（信道）和功率，QMIX学习一个集中式但可分解的动作价值函数，从而实现协调但分散的执行。在无信道复用设置中，我们将QMIX与genie-aided最优策略进行基准测试，在启用信道复用的更一般的衰落环境中，将其与局部上限置信区间(UCB)和无状态反应策略进行基准测试。仿真结果表明，QMIX迅速收敛到接近genie-aided界限的协作策略，同时比基线实现更高的吞吐量和更低的干扰发生率，从而证明了MARL在竞争环境中保护自主集群的有效性。",
            "intro_zh": [
                "反应式干扰严重威胁机器人集群网络，传统固定策略难以有效应对自适应干扰。",
                "提出基于QMIX的多智能体强化学习框架，协调信道和功率选择，提升通信韧性。",
                "实验表明，QMIX能快速收敛到协作策略，显著提高吞吐量并降低干扰发生率。"
            ],
            "method_zh": "**问题定义**：论文旨在解决机器人集群网络在反应式干扰下的通信安全问题。现有的固定功率控制或静态信道跳频等方法无法有效应对自适应的反应式干扰，导致集群通信中断，任务失败。\\n\\n**核心思路**：论文的核心思路是利用多智能体强化学习（MARL）来训练集群中的每个智能体，使其能够根据环境（包括其他智能体的行为和干扰机的行为）自适应地选择合适的信道和功率，从而最大化集群的整体通信性能。通过学习协作策略，智能体可以共同避免干扰，提高通信的可靠性和效率。\\n\\n**技术框架**：该框架包含多个发射机-接收机对，它们共享信道进行通信。一个反应式干扰机监听信道上的总功率，并根据马尔可夫阈值动态决定是否进行干扰。每个智能体（发射机）通过MARL算法学习选择合适的信道和功率。QMIX算法被用于学习一个集中式但可分解的动作价值函数，该函数允许智能体在分散的环境中执行协调的动作。\\n\\n**关键创新**：该论文的关键创新在于使用QMIX算法来解决集群网络中的抗干扰问题。QMIX能够学习一个集中式的价值函数，同时允许智能体分散地执行动作，这使得智能体能够在复杂的环境中进行有效的协作。此外，该论文还考虑了反应式干扰机的自适应行为，并设计了相应的奖励函数来鼓励智能体学习避免干扰的策略。\\n\\n**关键设计**：QMIX算法中的关键设计包括：1）状态表示：每个智能体观察到的状态包括信道状态、其他智能体的行为以及干扰机的行为。2）动作空间：每个智能体的动作空间包括选择信道和功率级别。3）奖励函数：奖励函数的设计旨在鼓励智能体最大化吞吐量，同时避免干扰。4）网络结构：QMIX使用一个混合网络来将每个智能体的Q值组合成一个联合Q值，该联合Q值用于指导智能体的学习。",
            "application_zh": "该研究成果可应用于各种需要高可靠性通信的集群机器人系统，例如：无人机集群搜索救援、自主水下航行器集群环境监测、以及工业机器人集群协同作业等。通过提高集群通信的抗干扰能力，可以确保任务的顺利完成，并降低因通信中断带来的风险。",
            "highlight_zh": "实验结果表明，QMIX算法在无信道复用场景下，性能接近genie-aided最优策略。在更一般的信道复用场景下，QMIX算法相比于局部UCB和无状态反应策略，实现了更高的吞吐量和更低的干扰发生率，验证了MARL在保护自主集群通信方面的有效性。",
            "tags_zh": [
                "多智能体强化学习",
                "集群网络",
                "反应式干扰",
                "QMIX算法",
                "通信安全"
            ],
            "_index": 61,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16813v1/topology.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16813v1/based10.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16813v1/based8.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "On The Hidden Biases of Flow Matching Samplers",
            "authors": [
                "Soon Hoe Lim"
            ],
            "arxiv_id": "2512.16768v1",
            "summary": "We study the implicit bias of flow matching (FM) samplers via the lens of empirical flow matching. Although population FM may produce gradient-field velocities resembling optimal transport (OT), we show that the empirical FM minimizer is almost never a gradient field, even when each conditional flow is. Consequently, empirical FM is intrinsically energetically suboptimal. In view of this, we analyze the kinetic energy of generated samples. With Gaussian sources, both instantaneous and integrated kinetic energies exhibit exponential concentration, while heavy-tailed sources lead to polynomial tails. These behaviors are governed primarily by the choice of source distribution rather than the data. Overall, these notes provide a concise mathematical account of the structural and energetic biases arising in empirical FM.",
            "categories": [
                "stat.ML",
                "cs.LG",
                "math.PR"
            ],
            "primary_category": "stat.ML",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "20 pages",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16768v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]flow matching"
                    ],
                    "score": 4.5
                }
            ],
            "relevance_score": 4.5,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "揭示Flow Matching采样器中的隐式偏差，分析其能量次优性",
            "summary_zh": "本文研究了Flow Matching (FM)采样器在经验流匹配视角下的隐式偏差。尽管总体FM可能产生类似于最优传输(OT)的梯度场速度，但我们表明，即使每个条件流都是梯度场，经验FM的最小化结果几乎都不是梯度场。因此，经验FM本质上是能量次优的。鉴于此，我们分析了生成样本的动能。对于高斯源，瞬时和积分动能都表现出指数集中性，而重尾源则导致多项式尾部。这些行为主要受源分布的选择控制，而不是数据本身。总的来说，这些研究结果对经验FM中出现的结构和能量偏差提供了简明的数学解释。",
            "intro_zh": [
                "现有Flow Matching方法在经验估计时存在偏差，导致能量次优，影响生成质量。",
                "通过分析经验流匹配的性质，揭示了其与最优传输的偏差，并从能量角度进行了量化。",
                "研究表明，生成样本的动能分布主要受源分布影响，为选择合适的源分布提供了理论依据。"
            ],
            "method_zh": "**问题定义**：Flow Matching (FM) 旨在通过学习连续的概率分布变换来实现生成建模。然而，在实际应用中，我们只能获得有限的样本，因此需要使用经验流匹配来近似总体流匹配。现有的FM方法在经验估计时，会引入隐式偏差，导致生成的样本并非最优，能量消耗也并非最低。这种能量次优性限制了FM采样器的性能。\\n\\n**核心思路**：本文的核心思路是通过分析经验流匹配的性质，揭示其与最优传输(Optimal Transport, OT)的偏差。理论上，总体FM可以产生类似于OT的梯度场速度，但经验FM的最小化结果却并非如此。因此，本文从能量的角度出发，分析生成样本的动能，以此来量化经验FM的次优性。\\n\\n**技术框架**：本文主要采用数学分析的方法，对经验FM的性质进行研究。具体来说，首先证明了经验FM的最小化结果几乎都不是梯度场，即使每个条件流都是梯度场。然后，分析了生成样本的动能，包括瞬时动能和积分动能。对于不同的源分布（高斯分布和重尾分布），分析了动能的分布情况。\\n\\n**关键创新**：本文最重要的技术创新点在于揭示了经验FM的结构性偏差和能量次优性。以往的研究主要关注如何提高FM的训练效率和生成质量，而忽略了经验估计带来的偏差。本文首次从理论上证明了经验FM的最小化结果并非梯度场，并从能量的角度量化了这种偏差。\\n\\n**关键设计**：本文的关键设计在于选择合适的动能作为分析指标。动能能够反映生成样本的运动状态和能量消耗情况，因此可以用来衡量FM采样器的效率。此外，本文还针对不同的源分布（高斯分布和重尾分布）进行了分析，发现源分布的选择对生成样本的动能分布有重要影响。",
            "application_zh": "该研究成果可应用于生成模型的改进和优化，尤其是在需要高效率和低能量消耗的场景下，例如图像生成、音频合成、分子设计等。通过选择合适的源分布和优化经验流匹配算法，可以降低生成样本的能量消耗，提高生成质量，并为未来的Flow Matching算法研究提供理论指导。",
            "highlight_zh": "研究表明，经验FM的最小化结果几乎都不是梯度场，即使每个条件流都是梯度场。对于高斯源，瞬时和积分动能都表现出指数集中性，而重尾源则导致多项式尾部。这些行为主要受源分布的选择控制，而不是数据本身。",
            "tags_zh": [
                "Flow Matching",
                "生成模型",
                "隐式偏差",
                "最优传输",
                "能量次优性",
                "动能分析",
                "采样器",
                "概率分布"
            ],
            "_index": 62,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "NDRL: Cotton Irrigation and Nitrogen Application with Nested Dual-Agent Reinforcement Learning",
            "authors": [
                "Ruifeng Xu",
                "Liang He"
            ],
            "arxiv_id": "2512.16408v1",
            "summary": "Effective irrigation and nitrogen fertilization have a significant impact on crop yield. However, existing research faces two limitations: (1) the high complexity of optimizing water-nitrogen combinations during crop growth and poor yield optimization results; and (2) the difficulty in quantifying mild stress signals and the delayed feedback, which results in less precise dynamic regulation of water and nitrogen and lower resource utilization efficiency. To address these issues, we propose a Nested Dual-Agent Reinforcement Learning (NDRL) method. The parent agent in NDRL identifies promising macroscopic irrigation and fertilization actions based on projected cumulative yield benefits, reducing ineffective explorationwhile maintaining alignment between objectives and yield. The child agent's reward function incorporates quantified Water Stress Factor (WSF) and Nitrogen Stress Factor (NSF), and uses a mixed probability distribution to dynamically optimize daily strategies, thereby enhancing both yield and resource efficiency. We used field experiment data from 2023 and 2024 to calibrate and validate the Decision Support System for Agrotechnology Transfer (DSSAT) to simulate real-world conditions and interact with NDRL. Experimental results demonstrate that, compared to the best baseline, the simulated yield increased by 4.7% in both 2023 and 2024, the irrigation water productivity increased by 5.6% and 5.1% respectively, and the nitrogen partial factor productivity increased by 6.3% and 1.0% respectively. Our method advances the development of cotton irrigation and nitrogen fertilization, providing new ideas for addressing the complexity and precision issues in agricultural resource management and for sustainable agricultural development.",
            "categories": [
                "cs.LG",
                "cs.MA"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Accepted by ICONIP 2025",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16408v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]reinforcement learning"
                    ],
                    "score": 4.5
                }
            ],
            "relevance_score": 4.5,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "提出嵌套双智能体强化学习NDRL，优化棉花灌溉施氮，提升产量和资源利用率。",
            "summary_zh": "本文提出了一种嵌套双智能体强化学习(NDRL)方法，旨在解决作物生长过程中水氮组合优化的高复杂性和产量优化结果不佳的问题，以及量化轻微胁迫信号的困难和反馈延迟的问题，从而提高水氮动态调节的精确性和资源利用效率。NDRL中的父智能体基于预测的累积产量效益识别有前景的宏观灌溉和施肥行动，减少无效探索，同时保持目标与产量之间的一致性。子智能体的奖励函数结合了量化的水分胁迫因子(WSF)和氮素胁迫因子(NSF)，并使用混合概率分布来动态优化每日策略，从而提高产量和资源效率。使用2023年和2024年的田间试验数据校准和验证了农业技术转移决策支持系统(DSSAT)，以模拟真实世界条件并与NDRL交互。实验结果表明，与最佳基线相比，2023年和2024年的模拟产量均提高了4.7%，灌溉用水生产率分别提高了5.6%和5.1%，氮素偏生产率分别提高了6.3%和1.0%。该方法推动了棉花灌溉和氮肥施用的发展，为解决农业资源管理中的复杂性和精确性问题以及可持续农业发展提供了新思路。",
            "intro_zh": [
                "现有方法在作物生长期间难以优化水氮组合，导致产量提升有限，且难以量化轻微胁迫信号。",
                "NDRL通过嵌套双智能体结构，父智能体进行宏观决策，子智能体基于胁迫因子动态优化每日策略。",
                "实验结果表明，NDRL相较于最佳基线，产量提升4.7%，灌溉用水生产率提升5.1%-5.6%，氮素偏生产率提升1.0%-6.3%。"
            ],
            "method_zh": "**问题定义**：论文旨在解决棉花种植过程中，如何精确控制灌溉和施氮量，以最大化产量并提高资源利用率的问题。现有方法的痛点在于难以在复杂的水氮组合中找到最优解，且对作物轻微胁迫的感知不精确，导致资源浪费和产量损失。\\n\\n**核心思路**：论文的核心思路是利用嵌套的双智能体强化学习框架，将宏观决策（长期产量目标）和微观调控（每日水氮策略）相结合。父智能体负责根据预测的累积产量效益选择有前景的宏观行动，减少无效探索；子智能体则根据量化的水分和氮素胁迫因子，动态优化每日策略，从而实现产量和资源效率的双重提升。\\n\\n**技术框架**：NDRL的整体架构包含两个智能体：父智能体和子智能体。父智能体基于DSSAT模拟的长期产量预测，选择宏观的水氮管理策略。子智能体则在父智能体设定的宏观策略下，根据每日的WSF和NSF，利用混合概率分布动态调整每日的灌溉和施氮量。DSSAT作为环境模拟器，提供作物生长状态和产量反馈。\\n\\n**关键创新**：NDRL的关键创新在于嵌套的双智能体结构和基于胁迫因子的奖励函数设计。双智能体结构实现了宏观目标和微观调控的有效结合，克服了传统单智能体强化学习在复杂环境中的探索难题。基于WSF和NSF的奖励函数能够更精确地反映作物的水氮需求，从而指导子智能体做出更合理的决策。\\n\\n**关键设计**：子智能体使用混合概率分布来选择每日的灌溉和施氮量，这种设计允许智能体在探索和利用之间进行平衡。WSF和NSF的计算方式需要根据具体的作物生理模型和环境条件进行调整。父智能体的奖励函数设计需要充分考虑长期产量目标和资源利用率之间的权衡。",
            "application_zh": "该研究成果可应用于精准农业领域，为棉花等作物的灌溉和施氮管理提供决策支持。通过NDRL，可以实现水肥资源的优化配置，提高作物产量和资源利用率，降低农业生产成本，并促进农业可持续发展。该方法也可推广到其他作物和农业场景，具有广阔的应用前景。",
            "highlight_zh": "实验结果表明，NDRL在模拟环境下显著提升了棉花产量和资源利用率。与最佳基线相比，2023年和2024年的模拟产量均提高了4.7%，灌溉用水生产率分别提高了5.6%和5.1%，氮素偏生产率分别提高了6.3%和1.0%。这些数据表明，NDRL能够有效地优化水氮管理策略，实现产量和资源效率的双重提升。",
            "tags_zh": [
                "强化学习",
                "农业灌溉",
                "氮肥施用",
                "双智能体",
                "精准农业"
            ],
            "_index": 63,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16408v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16408v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16408v1/x4.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "A Task-Driven, Planner-in-the-Loop Computational Design Framework for Modular Manipulators",
            "authors": [
                "Maolin Lei",
                "Edoardo Romiti",
                "Arturo Laurenzi",
                "Rui Dai",
                "Matteo Dalle Vedove",
                "Jiatao Ding",
                "Daniele Fontanelli",
                "Nikos Tsagarakis"
            ],
            "arxiv_id": "2512.16069v1",
            "summary": "Modular manipulators composed of pre-manufactured and interchangeable modules offer high adaptability across diverse tasks. However, their deployment requires generating feasible motions while jointly optimizing morphology and mounted pose under kinematic, dynamic, and physical constraints. Moreover, traditional single-branch designs often extend reach by increasing link length, which can easily violate torque limits at the base joint. To address these challenges, we propose a unified task-driven computational framework that integrates trajectory planning across varying morphologies with the co-optimization of morphology and mounted pose. Within this framework, a hierarchical model predictive control (HMPC) strategy is developed to enable motion planning for both redundant and non-redundant manipulators. For design optimization, the CMA-ES is employed to efficiently explore a hybrid search space consisting of discrete morphology configurations and continuous mounted poses. Meanwhile, a virtual module abstraction is introduced to enable bi-branch morphologies, allowing an auxiliary branch to offload torque from the primary branch and extend the achievable workspace without increasing the capacity of individual joint modules. Extensive simulations and hardware experiments on polishing, drilling, and pick-and-place tasks demonstrate the effectiveness of the proposed framework. The results show that: 1) the framework can generate multiple feasible designs that satisfy kinematic and dynamic constraints while avoiding environmental collisions for given tasks; 2) flexible design objectives, such as maximizing manipulability, minimizing joint effort, or reducing the number of modules, can be achieved by customizing the cost functions; and 3) a bi-branch morphology capable of operating in a large workspace can be realized without requiring more powerful basic modules.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16069v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "model predictive control",
                        "motion planning"
                    ],
                    "score": 4.0
                }
            ],
            "relevance_score": 4.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "提出任务驱动的模块化机械臂计算设计框架，实现形态与运动的协同优化",
            "summary_zh": "本文提出了一种统一的任务驱动计算框架，用于模块化机械臂的设计，该框架集成了不同形态下的轨迹规划以及形态和安装姿态的协同优化。开发了一种分层模型预测控制（HMPC）策略，用于冗余和非冗余机械臂的运动规划。采用CMA-ES算法高效探索离散形态配置和连续安装姿态的混合搜索空间。引入虚拟模块抽象来实现双分支形态，允许辅助分支卸载主分支的扭矩，并在不增加单个关节模块容量的情况下扩展可实现的工作空间。在抛光、钻孔和取放任务中的仿真和硬件实验表明了该框架的有效性。结果表明，该框架可以生成满足运动学和动力学约束的多个可行设计，同时避免环境碰撞；通过定制成本函数，可以实现灵活的设计目标，例如最大化可操作性、最小化关节力或减少模块数量；无需更强大的基本模块即可实现可在大型工作空间中运行的双分支形态。",
            "intro_zh": [
                "传统单分支机械臂通过增加连杆长度来扩展范围，但容易超出基关节的扭矩限制，这是核心问题。",
                "论文提出统一的计算框架，将轨迹规划与形态和安装姿态的协同优化相结合，解决上述问题。",
                "仿真和硬件实验验证了框架的有效性，能够生成满足约束的设计，并实现灵活的设计目标。"
            ],
            "method_zh": "**问题定义**：模块化机械臂的设计需要同时优化机械臂的形态、安装姿态以及运动轨迹，以满足特定的任务需求。传统方法通常采用单分支结构，为了扩大工作空间，会增加连杆长度，但这会导致基关节的扭矩需求增加，容易超出关节的承载能力。因此，需要在满足运动学、动力学和物理约束的条件下，寻找最优的机械臂设计方案。\\n\\n**核心思路**：论文的核心思路是将轨迹规划与形态优化相结合，通过一个统一的框架来实现。该框架采用任务驱动的方式，根据任务需求来指导机械臂的设计。通过引入双分支结构，利用辅助分支来分担主分支的扭矩，从而在不增加关节模块功率的情况下，扩展机械臂的工作空间。\\n\\n**技术框架**：该框架主要包含以下几个模块：1) 运动规划模块：采用分层模型预测控制（HMPC）策略，为冗余和非冗余机械臂生成可行的运动轨迹。2) 形态优化模块：采用CMA-ES算法，在离散的形态配置和连续的安装姿态空间中进行搜索，寻找最优的机械臂设计方案。3) 虚拟模块抽象：引入虚拟模块的概念，允许设计双分支结构的机械臂，从而实现扭矩分担和工作空间扩展。整个流程是，首先根据任务需求，利用运动规划模块生成初始轨迹，然后利用形态优化模块对机械臂的形态和安装姿态进行优化，最后通过仿真和硬件实验验证设计方案的有效性。\\n\\n**关键创新**：该论文的关键创新在于：1) 提出了一个统一的框架，将轨迹规划与形态优化相结合，实现了任务驱动的机械臂设计。2) 引入了虚拟模块抽象，允许设计双分支结构的机械臂，从而在不增加关节模块功率的情况下，扩展机械臂的工作空间。3) 采用分层模型预测控制（HMPC）策略，实现了冗余和非冗余机械臂的运动规划。\\n\\n**关键设计**：在形态优化模块中，采用了CMA-ES算法，该算法是一种进化策略算法，适用于解决非凸优化问题。在成本函数的设计中，可以根据具体的任务需求，灵活地设置不同的目标，例如最大化可操作性、最小化关节力或减少模块数量。在HMPC策略中，采用了分层控制结构，上层控制器负责生成全局轨迹，下层控制器负责跟踪轨迹并处理约束。",
            "application_zh": "该研究成果可应用于各种需要灵活配置和高性能的机器人操作任务，例如工业自动化中的装配、喷涂、焊接等，以及医疗机器人中的手术辅助、康复训练等。通过优化机械臂的形态和运动，可以提高操作效率、降低能源消耗，并扩展机器人的应用范围。未来，该框架可以进一步扩展到多机器人协同操作、人机协作等领域。",
            "highlight_zh": "实验结果表明，该框架能够为抛光、钻孔和取放等任务生成满足运动学和动力学约束的可行设计。通过定制成本函数，可以实现不同的设计目标，例如最大化可操作性、最小化关节力或减少模块数量。此外，双分支形态能够在不增加关节模块功率的情况下，扩展机械臂的工作空间。例如，在特定任务中，双分支结构能够显著降低基关节的扭矩需求，从而避免超出关节的承载能力。",
            "tags_zh": [
                "模块化机械臂",
                "计算设计",
                "轨迹规划",
                "形态优化",
                "模型预测控制",
                "双分支结构",
                "任务驱动",
                "机器人操作"
            ],
            "_index": 64,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16069v1/figure/dual_arm_robot.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16069v1/figure/framework.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16069v1/figure/balance_updae.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Auto-Vocabulary 3D Object Detection",
            "authors": [
                "Haomeng Zhang",
                "Kuan-Chuan Peng",
                "Suhas Lohit",
                "Raymond A. Yeh"
            ],
            "arxiv_id": "2512.16077v1",
            "summary": "Open-vocabulary 3D object detection methods are able to localize 3D boxes of classes unseen during training. Despite the name, existing methods rely on user-specified classes both at training and inference. We propose to study Auto-Vocabulary 3D Object Detection (AV3DOD), where the classes are automatically generated for the detected objects without any user input. To this end, we introduce Semantic Score (SS) to evaluate the quality of the generated class names. We then develop a novel framework, AV3DOD, which leverages 2D vision-language models (VLMs) to generate rich semantic candidates through image captioning, pseudo 3D box generation, and feature-space semantics expansion. AV3DOD achieves the state-of-the-art (SOTA) performance on both localization (mAP) and semantic quality (SS) on the ScanNetV2 and SUNRGB-D datasets. Notably, it surpasses the SOTA, CoDA, by 3.48 overall mAP and attains a 24.5% relative improvement in SS on ScanNetV2.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "technical report",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16077v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "open-vocabulary",
                        "open vocabulary"
                    ],
                    "score": 4.0
                }
            ],
            "relevance_score": 4.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出AV3DOD框架，实现无需用户干预的自动词汇3D目标检测。",
            "summary_zh": "本文研究了自动词汇3D目标检测（AV3DOD），旨在无需任何用户输入，自动为检测到的对象生成类别。为此，我们引入了语义分数（SS）来评估生成的类名称的质量。然后，我们开发了一个新颖的框架AV3DOD，该框架利用2D视觉-语言模型（VLMs），通过图像字幕、伪3D框生成和特征空间语义扩展来生成丰富的语义候选。AV3DOD在ScanNetV2和SUNRGB-D数据集上实现了最先进的（SOTA）定位（mAP）和语义质量（SS）性能。值得注意的是，它超过了SOTA方法CoDA，在ScanNetV2上实现了3.48的总体mAP提升，并在SS上实现了24.5%的相对改进。",
            "intro_zh": [
                "现有开放词汇3D目标检测方法依赖于训练和推理阶段用户指定的类别，限制了其自动化程度和泛化能力。",
                "AV3DOD框架利用2D视觉-语言模型，通过图像描述、伪3D框生成和特征空间语义扩展，自动生成丰富的语义候选。",
                "实验表明，AV3DOD在ScanNetV2和SUNRGB-D数据集上，定位精度和语义质量均达到SOTA，显著优于现有方法。"
            ],
            "method_zh": "**问题定义**：现有开放词汇3D目标检测方法虽然声称具有开放词汇能力，但实际上仍然需要在训练和推理阶段依赖用户预先定义的类别。这限制了模型的自动化程度和泛化能力，无法应对真实场景中未知的物体类别。因此，需要一种能够自动生成物体类别名称的3D目标检测方法。\\n\\n**核心思路**：AV3DOD的核心思路是利用2D视觉-语言模型（VLM）的强大语义理解能力，从2D图像中提取丰富的语义信息，并将其迁移到3D空间中，从而自动生成3D物体的类别名称。通过图像描述生成候选类别，并利用伪3D框和特征空间语义扩展来提升类别名称的质量。\\n\\n**技术框架**：AV3DOD框架主要包含以下几个模块：1) 图像字幕模块：利用2D VLM对场景图像进行描述，生成候选的物体类别名称。2) 伪3D框生成模块：根据2D检测结果和深度信息，生成伪3D框，用于关联2D语义信息和3D几何信息。3) 特征空间语义扩展模块：在特征空间中对语义信息进行扩展，进一步丰富类别名称的语义表达。4) 语义评分模块：引入语义分数（SS）来评估生成的类别名称的质量，并选择最佳的类别名称。\\n\\n**关键创新**：AV3DOD的关键创新在于：1) 提出了自动词汇3D目标检测的概念，无需用户指定类别。2) 利用2D VLM生成丰富的语义候选，避免了对预定义词汇表的依赖。3) 引入语义分数（SS）来评估生成类别名称的质量。\\n\\n**关键设计**：在图像字幕模块中，使用了预训练的2D VLM模型，例如CLIP或BLIP。在伪3D框生成模块中，利用了相机内外参和深度信息。在特征空间语义扩展模块中，使用了对比学习的方法，将相似物体的特征拉近。语义分数（SS）的计算方式为：SS = α * Localization Score + (1 - α) * Semantic Alignment Score，其中α是一个超参数，用于平衡定位精度和语义对齐程度。",
            "application_zh": "AV3DOD可应用于机器人导航、自动驾驶、智能家居等领域，尤其是在需要识别未知物体的场景中。例如，机器人可以在未知环境中自动识别并标注物体，从而更好地理解环境并执行任务。该研究有助于提升3D目标检测的自动化程度和泛化能力，推动相关领域的发展。",
            "highlight_zh": "AV3DOD在ScanNetV2数据集上，总体mAP超过SOTA方法CoDA 3.48，并在语义分数（SS）上实现了24.5%的相对改进。这些结果表明，AV3DOD在定位精度和语义质量方面均取得了显著提升，验证了该方法的有效性。",
            "tags_zh": [
                "3D目标检测",
                "开放词汇",
                "视觉-语言模型",
                "自动词汇",
                "语义理解"
            ],
            "_index": 65,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16077v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16077v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16077v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Posterior Behavioral Cloning: Pretraining BC Policies for Efficient RL Finetuning",
            "authors": [
                "Andrew Wagenmaker",
                "Perry Dong",
                "Raymond Tsao",
                "Chelsea Finn",
                "Sergey Levine"
            ],
            "arxiv_id": "2512.16911v1",
            "summary": "Standard practice across domains from robotics to language is to first pretrain a policy on a large-scale demonstration dataset, and then finetune this policy, typically with reinforcement learning (RL), in order to improve performance on deployment domains. This finetuning step has proved critical in achieving human or super-human performance, yet while much attention has been given to developing more effective finetuning algorithms, little attention has been given to ensuring the pretrained policy is an effective initialization for RL finetuning. In this work we seek to understand how the pretrained policy affects finetuning performance, and how to pretrain policies in order to ensure they are effective initializations for finetuning. We first show theoretically that standard behavioral cloning (BC) -- which trains a policy to directly match the actions played by the demonstrator -- can fail to ensure coverage over the demonstrator's actions, a minimal condition necessary for effective RL finetuning. We then show that if, instead of exactly fitting the observed demonstrations, we train a policy to model the posterior distribution of the demonstrator's behavior given the demonstration dataset, we do obtain a policy that ensures coverage over the demonstrator's actions, enabling more effective finetuning. Furthermore, this policy -- which we refer to as the posterior behavioral cloning (PostBC) policy -- achieves this while ensuring pretrained performance is no worse than that of the BC policy. We then show that PostBC is practically implementable with modern generative models in robotic control domains -- relying only on standard supervised learning -- and leads to significantly improved RL finetuning performance on both realistic robotic control benchmarks and real-world robotic manipulation tasks, as compared to standard behavioral cloning.",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.RO"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16911v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning"
                    ],
                    "score": 1.5
                }
            ],
            "relevance_score": 3.5,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "提出后验行为克隆(PostBC)方法，提升RL微调的预训练策略效果",
            "summary_zh": "本文研究了预训练策略如何影响强化学习(RL)微调的性能，以及如何预训练策略以确保它们是有效的微调初始化。理论上证明，标准行为克隆(BC)无法确保覆盖演示者的行为，这是有效RL微调的必要条件。因此，提出后验行为克隆(PostBC)策略，该策略训练模型以模拟给定演示数据集的演示者行为的后验分布，从而确保覆盖演示者的行为，实现更有效的微调，同时保证预训练性能不低于BC策略。PostBC可以通过现代生成模型在机器人控制领域中实际实现，仅依赖于标准监督学习，并且与标准行为克隆相比，在真实的机器人控制基准和真实机器人操作任务上，显著提高了RL微调的性能。",
            "intro_zh": [
                "现有行为克隆(BC)方法在作为强化学习(RL)微调的初始化时，无法保证覆盖演示者的所有行为，限制了微调效果。",
                "提出后验行为克隆(PostBC)，通过建模演示者行为的后验分布，确保策略覆盖演示者的行为，从而改善RL微调的初始化。",
                "实验表明，PostBC在机器人控制任务中，相较于标准BC，显著提升了RL微调的性能，并在真实机器人操作任务中验证了其有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决标准行为克隆(BC)作为强化学习(RL)微调的预训练策略时，无法有效覆盖演示者行为的问题。现有BC方法直接模仿演示数据中的动作，可能导致策略陷入局部最优，缺乏探索能力，从而限制了后续RL微调的性能。\\n\\n**核心思路**：论文的核心思路是，与其精确拟合观察到的演示数据，不如训练一个策略来模拟给定演示数据集下演示者行为的后验分布。这种方法能够更好地捕捉演示者行为的多样性，并确保策略能够覆盖演示者的所有可能行为，从而为后续的RL微调提供更好的初始化。\\n\\n**技术框架**：PostBC的整体框架包括以下几个阶段：1) 收集演示数据集；2) 使用生成模型（例如变分自编码器VAE或归一化流）对演示数据进行建模，学习演示者行为的后验分布；3) 从学习到的后验分布中采样，训练PostBC策略；4) 使用RL算法对PostBC策略进行微调。\\n\\n**关键创新**：PostBC的关键创新在于，它不再是简单地模仿演示数据，而是学习演示者行为的后验分布。这种方法能够更好地捕捉演示者行为的不确定性和多样性，从而提高策略的泛化能力和探索能力。与现有BC方法相比，PostBC能够更好地覆盖演示者的行为空间，为后续的RL微调提供更有效的初始化。\\n\\n**关键设计**：PostBC的关键设计包括：1) 使用合适的生成模型来建模演示者行为的后验分布，例如VAE或归一化流；2) 设计合适的损失函数来训练生成模型，例如变分下界(ELBO)或最大似然估计；3) 从学习到的后验分布中采样，生成训练数据，用于训练PostBC策略；4) 选择合适的RL算法对PostBC策略进行微调，例如PPO或SAC。",
            "application_zh": "PostBC方法可广泛应用于机器人控制、自动驾驶、游戏AI等领域。通过预训练一个能够覆盖专家行为的策略，可以显著提高后续RL微调的效率和性能，降低对大量环境交互的需求，加速智能体的学习过程。该方法尤其适用于那些难以获取大量奖励信号或环境交互成本较高的场景。",
            "highlight_zh": "实验结果表明，PostBC在多个机器人控制任务中显著优于标准BC。例如，在真实机器人操作任务中，PostBC能够更快地学习到有效的策略，并达到更高的性能。与BC相比，PostBC在RL微调后的性能提升幅度明显，验证了其作为RL微调有效初始化的优势。",
            "tags_zh": [
                "后验行为克隆",
                "强化学习微调",
                "行为克隆",
                "机器人控制",
                "预训练策略",
                "生成模型",
                "策略初始化"
            ],
            "_index": 66,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16911v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16911v1/im/corn_in_pot2.jpg",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16911v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning",
            "authors": [
                "Yuanchen Ju",
                "Yongyuan Liang",
                "Yen-Jen Wang",
                "Nandiraju Gireesh",
                "Yuanliang Ju",
                "Seungjae Lee",
                "Qiao Gu",
                "Elvis Hsieh",
                "Furong Huang",
                "Koushil Sreenath"
            ],
            "arxiv_id": "2512.16909v1",
            "summary": "Mobile manipulators in households must both navigate and manipulate. This requires a compact, semantically rich scene representation that captures where objects are, how they function, and which parts are actionable. Scene graphs are a natural choice, yet prior work often separates spatial and functional relations, treats scenes as static snapshots without object states or temporal updates, and overlooks information most relevant for accomplishing the current task. To address these limitations, we introduce MomaGraph, a unified scene representation for embodied agents that integrates spatial-functional relationships and part-level interactive elements. However, advancing such a representation requires both suitable data and rigorous evaluation, which have been largely missing. We thus contribute MomaGraph-Scenes, the first large-scale dataset of richly annotated, task-driven scene graphs in household environments, along with MomaGraph-Bench, a systematic evaluation suite spanning six reasoning capabilities from high-level planning to fine-grained scene understanding. Built upon this foundation, we further develop MomaGraph-R1, a 7B vision-language model trained with reinforcement learning on MomaGraph-Scenes. MomaGraph-R1 predicts task-oriented scene graphs and serves as a zero-shot task planner under a Graph-then-Plan framework. Extensive experiments demonstrate that our model achieves state-of-the-art results among open-source models, reaching 71.6% accuracy on the benchmark (+11.4% over the best baseline), while generalizing across public benchmarks and transferring effectively to real-robot experiments.",
            "categories": [
                "cs.CV",
                "cs.RO"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "25 pages, 10 figures. Project page:https://hybridrobotics.github.io/MomaGraph/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16909v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "scene understanding"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 3.5,
            "hit_pillars": [
                "2_algo_arch",
                "3_perception_slam"
            ],
            "headline_zh": "MomaGraph：面向具身任务规划，融合视觉-语言模型的、状态感知的统一场景图",
            "summary_zh": "本文提出了MomaGraph，一种用于具身智能体的统一场景表示方法，它集成了空间-功能关系和部件级别的交互元素。现有场景图通常分离空间和功能关系，将场景视为静态快照，忽略了与当前任务最相关的信息。为了推进该表示方法，本文贡献了MomaGraph-Scenes，这是首个大规模的、带有丰富标注的、任务驱动的家庭环境场景图数据集，以及MomaGraph-Bench，一个涵盖从高层规划到细粒度场景理解的六种推理能力的系统评估套件。在此基础上，进一步开发了MomaGraph-R1，一个在MomaGraph-Scenes上通过强化学习训练的70亿参数视觉-语言模型。MomaGraph-R1预测面向任务的场景图，并在Graph-then-Plan框架下作为零样本任务规划器。大量实验表明，该模型在开源模型中取得了最先进的结果，在基准测试中达到了71.6%的准确率（比最佳基线高出11.4%），同时推广到公共基准测试，并有效地转移到真实机器人实验。",
            "intro_zh": [
                "现有场景图表示方法缺乏对物体状态和时序更新的考虑，且空间和功能关系分离，限制了具身智能体的任务规划能力。",
                "MomaGraph通过统一的场景表示，整合空间-功能关系和部件级别的交互元素，从而更全面地描述场景。",
                "MomaGraph-R1模型在MomaGraph数据集上训练，并在多个基准测试中取得了显著的性能提升，验证了其有效性。"
            ],
            "method_zh": "**问题定义**：现有场景图表示方法主要存在三个痛点：一是空间和功能关系分离，无法有效支持复杂任务规划；二是将场景视为静态快照，忽略了物体状态和时序变化；三是缺乏针对具身任务的定制化信息，导致泛化能力不足。这些问题限制了移动操作机器人在家庭环境中的应用。\n\n**核心思路**：MomaGraph的核心思路是构建一个统一的、状态感知的场景图表示，将空间关系、功能关系和物体状态整合在一起。通过引入部件级别的交互元素，MomaGraph能够更精细地描述场景，从而支持更复杂的任务规划。此外，通过视觉-语言模型进行训练，MomaGraph能够更好地理解场景中的语义信息。\n\n**技术框架**：MomaGraph的整体框架包括数据收集与标注、模型训练和任务规划三个主要阶段。首先，构建MomaGraph-Scenes数据集，对家庭环境中的场景进行详细标注，包括物体的位置、功能和状态。然后，使用强化学习训练视觉-语言模型MomaGraph-R1，使其能够预测面向任务的场景图。最后，在Graph-then-Plan框架下，利用预测的场景图进行任务规划。\n\n**关键创新**：MomaGraph的关键创新在于其统一的场景表示方法，它将空间关系、功能关系和物体状态整合在一起，从而更全面地描述场景。此外，MomaGraph-Scenes数据集的构建和MomaGraph-Bench评估套件的提出，为该领域的研究提供了重要的数据和评估标准。\n\n**关键设计**：MomaGraph-R1模型是一个70亿参数的视觉-语言模型，采用Transformer架构。在训练过程中，使用了强化学习方法，以优化模型在任务规划方面的性能。损失函数包括场景图预测损失和任务规划奖励。具体参数设置和网络结构细节未在论文中详细描述，属于未知信息。",
            "application_zh": "MomaGraph在家庭服务机器人、自动驾驶、虚拟现实等领域具有广泛的应用前景。它可以帮助机器人更好地理解周围环境，从而执行更复杂的任务，例如物品整理、清洁和烹饪。此外，MomaGraph还可以用于构建更逼真的虚拟环境，为用户提供更沉浸式的体验。",
            "highlight_zh": "MomaGraph-R1模型在MomaGraph-Bench基准测试中取得了71.6%的准确率，比最佳基线高出11.4%。此外，该模型还成功地推广到公共基准测试，并有效地转移到真实机器人实验中，验证了其泛化能力和实用性。这些实验结果表明，MomaGraph是一种有效的场景表示方法，可以显著提高具身智能体的任务规划能力。",
            "tags_zh": [
                "场景图",
                "具身智能",
                "任务规划",
                "视觉-语言模型",
                "强化学习"
            ],
            "_index": 67,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16909v1/Figures/Teaser.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16909v1/Figures/Failure.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16909v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Hypernetworks That Evolve Themselves",
            "authors": [
                "Joachim Winther Pedersen",
                "Erwan Plantec",
                "Eleni Nisioti",
                "Marcello Barylli",
                "Milton Montero",
                "Kathrin Korte",
                "Sebastian Risi"
            ],
            "arxiv_id": "2512.16406v1",
            "summary": "How can neural networks evolve themselves without relying on external optimizers? We propose Self-Referential Graph HyperNetworks, systems where the very machinery of variation and inheritance is embedded within the network. By uniting hypernetworks, stochastic parameter generation, and graph-based representations, Self-Referential GHNs mutate and evaluate themselves while adapting mutation rates as selectable traits. Through new reinforcement learning benchmarks with environmental shifts (CartPoleSwitch, LunarLander-Switch), Self-Referential GHNs show swift, reliable adaptation and emergent population dynamics. In the locomotion benchmark Ant-v5, they evolve coherent gaits, showing promising fine-tuning capabilities by autonomously decreasing variation in the population to concentrate around promising solutions. Our findings support the idea that evolvability itself can emerge from neural self-reference. Self-Referential GHNs reflect a step toward synthetic systems that more closely mirror biological evolution, offering tools for autonomous, open-ended learning agents.",
            "categories": [
                "cs.NE",
                "cs.AI"
            ],
            "primary_category": "cs.NE",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16406v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "locomotion"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning"
                    ],
                    "score": 1.5
                }
            ],
            "relevance_score": 3.5,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "提出自引用图超网络，实现无需外部优化器的神经网络自进化。",
            "summary_zh": "本文提出自引用图超网络（Self-Referential Graph HyperNetworks, Self-Referential GHNs），该系统将变异和遗传机制嵌入网络内部，无需依赖外部优化器即可实现神经网络的自进化。通过结合超网络、随机参数生成和基于图的表示，Self-Referential GHNs能够自我变异和评估，同时将变异率作为可选择的特征进行调整。在包含环境变化的强化学习基准测试（CartPoleSwitch, LunarLander-Switch）中，Self-Referential GHNs展现出快速、可靠的适应性和涌现的人口动态。在locomotion基准测试Ant-v5中，它们进化出连贯的步态，并通过自主降低种群变异性来集中于有希望的解决方案，显示出良好的微调能力。研究结果表明，可进化性本身可以从神经自我参照中涌现。Self-Referential GHNs代表着朝着更接近生物进化的合成系统迈出的一步，为自主、开放式学习智能体提供了工具。",
            "intro_zh": [
                "现有神经网络进化依赖外部优化器，限制了其自主性和适应性，难以模拟生物进化。",
                "提出自引用图超网络，将变异和遗传机制嵌入网络内部，实现自我变异和评估，自主调整变异率。",
                "在多个强化学习基准测试中，该方法展现出快速适应环境变化的能力，并进化出连贯的运动步态。"
            ],
            "method_zh": "**问题定义**：现有神经网络的进化通常依赖于外部优化器，例如遗传算法或进化策略。这些外部优化器负责选择和变异网络参数，但这种方式限制了网络的自主性和适应性，也难以模拟生物进化中内在的自我进化机制。现有方法的痛点在于缺乏将进化机制内化到网络结构中的能力。\\n\\n**核心思路**：本文的核心思路是将神经网络的进化机制嵌入到网络自身结构中，使其能够自我变异、自我评估，并自主调整变异率。通过构建一个自引用的系统，网络可以根据自身的性能和环境反馈来调整其进化策略，从而实现更高效和自主的进化。\\n\\n**技术框架**：Self-Referential GHN 的整体架构包含以下几个主要模块：1) **图超网络 (Graph HyperNetwork)**：使用图结构来表示神经网络的连接和参数。超网络负责生成目标网络的权重。2) **随机参数生成 (Stochastic Parameter Generation)**：引入随机性，允许网络参数在一定范围内变化，从而实现变异。3) **自我参照机制 (Self-Referential Mechanism)**：网络可以访问自身的结构和性能信息，并利用这些信息来调整变异率和其他进化参数。4) **评估与选择 (Evaluation and Selection)**：通过强化学习或其他评估方法来评估网络的性能，并根据性能选择优秀的个体进行繁殖。\\n\\n**关键创新**：最重要的技术创新点在于将进化机制内化到神经网络的结构中，使其能够自我参照和自我进化。与传统的外部优化器方法相比，Self-Referential GHN 能够更自主地适应环境变化，并进化出更复杂的行为。本质区别在于，传统方法依赖外部力量驱动进化，而 Self-Referential GHN 则通过内部机制实现自我驱动的进化。\\n\\n**关键设计**：关键设计包括：1) **超网络结构**：超网络的设计需要能够有效地生成目标网络的权重，并允许参数的随机变异。2) **图表示**：图结构需要能够灵活地表示神经网络的连接和参数，并支持高效的变异操作。3) **变异率控制**：变异率的控制是实现稳定进化的关键，需要根据网络的性能和环境反馈进行动态调整。4) **损失函数**：使用强化学习的奖励信号作为损失函数，引导网络朝着更高的性能方向进化。",
            "application_zh": "该研究成果可应用于自主机器人、游戏AI、以及其他需要持续学习和适应环境的智能体。通过实现自主进化，智能体能够更好地适应复杂和动态的环境，无需人工干预即可完成任务。未来，该技术有望推动开放式学习和通用人工智能的发展。",
            "highlight_zh": "在CartPoleSwitch和LunarLander-Switch等强化学习基准测试中，Self-Referential GHNs展现出快速且可靠的适应性。在Ant-v5 locomotion基准测试中，它们进化出连贯的步态，并通过自主降低种群变异性来集中于有希望的解决方案，显示出良好的微调能力。这些结果表明，该方法在复杂环境中具有强大的适应性和进化能力。",
            "tags_zh": [
                "自进化神经网络",
                "超网络",
                "强化学习",
                "图神经网络",
                "自我参照",
                "进化算法",
                "开放式学习"
            ],
            "_index": 68,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16406v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16406v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16406v1/figures/environments.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "PolaRiS: Scalable Real-to-Sim Evaluations for Generalist Robot Policies",
            "authors": [
                "Arhan Jain",
                "Mingtong Zhang",
                "Kanav Arora",
                "William Chen",
                "Marcel Torne",
                "Muhammad Zubair Irshad",
                "Sergey Zakharov",
                "Yue Wang",
                "Sergey Levine",
                "Chelsea Finn",
                "Wei-Chiu Ma",
                "Dhruv Shah",
                "Abhishek Gupta",
                "Karl Pertsch"
            ],
            "arxiv_id": "2512.16881v1",
            "summary": "A significant challenge for robot learning research is our ability to accurately measure and compare the performance of robot policies. Benchmarking in robotics is historically challenging due to the stochasticity, reproducibility, and time-consuming nature of real-world rollouts. This challenge is exacerbated for recent generalist policies, which has to be evaluated across a wide variety of scenes and tasks. Evaluation in simulation offers a scalable complement to real world evaluations, but the visual and physical domain gap between existing simulation benchmarks and the real world has made them an unreliable signal for policy improvement. Furthermore, building realistic and diverse simulated environments has traditionally required significant human effort and expertise. To bridge the gap, we introduce Policy Evaluation and Environment Reconstruction in Simulation (PolaRiS), a scalable real-to-sim framework for high-fidelity simulated robot evaluation. PolaRiS utilizes neural reconstruction methods to turn short video scans of real-world scenes into interactive simulation environments. Additionally, we develop a simple simulation data co-training recipe that bridges remaining real-to-sim gaps and enables zero-shot evaluation in unseen simulation environments. Through extensive paired evaluations between simulation and the real world, we demonstrate that PolaRiS evaluations provide a much stronger correlation to real world generalist policy performance than existing simulated benchmarks. Its simplicity also enables rapid creation of diverse simulated environments. As such, this work takes a step towards distributed and democratized evaluation for the next generation of robotic foundation models.",
            "categories": [
                "cs.RO",
                "cs.LG"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Website: https://polaris-evals.github.io/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16881v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "foundation model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "PolaRiS：一种可扩展的真实到模拟的通用机器人策略评估框架",
            "summary_zh": "机器人学习研究面临的一个重要挑战是准确测量和比较机器人策略的性能。由于真实世界rollout的随机性、可重复性和耗时性，机器人技术的基准测试历来具有挑战性。对于最近的通用策略，需要在各种场景和任务中进行评估，这使得挑战更加严峻。仿真环境中的评估为真实世界评估提供了一种可扩展的补充，但现有仿真基准与真实世界之间的视觉和物理领域差距使其成为不可靠的策略改进信号。此外，构建逼真且多样化的仿真环境传统上需要大量的人工和专业知识。为了弥合差距，我们引入了仿真环境中的策略评估和环境重建（PolaRiS），这是一个可扩展的真实到模拟的框架，用于高保真仿真机器人评估。PolaRiS利用神经重建方法将真实世界场景的短视频扫描转换为交互式仿真环境。此外，我们开发了一种简单的仿真数据协同训练方法，弥合了剩余的真实到模拟的差距，并实现了在未见过的仿真环境中的零样本评估。通过仿真和真实世界之间的广泛配对评估，我们证明PolaRiS评估比现有的仿真基准更能提供与真实世界通用策略性能的更强相关性。它的简单性也使得能够快速创建多样化的仿真环境。因此，这项工作朝着为下一代机器人基础模型进行分布式和民主化的评估迈出了一步。",
            "intro_zh": [
                "机器人策略评估面临真实环境rollout成本高、难以复现等问题，通用策略的评估更具挑战。",
                "PolaRiS利用神经重建技术将真实场景视频转化为交互式仿真环境，实现高效的策略评估。",
                "通过仿真数据协同训练，PolaRiS显著提升了仿真环境与真实环境的相关性，实现零样本评估。"
            ],
            "method_zh": "**问题定义**：现有机器人策略评估方法，尤其是在通用机器人策略的评估上，面临着真实环境评估成本高昂、难以复现，以及现有仿真环境与真实环境存在较大差距的问题。这导致在仿真环境中评估的策略，在真实环境中表现不佳，阻碍了机器人学习的进展。现有方法难以快速构建逼真且多样化的仿真环境，需要大量人工干预和专业知识。\\n\\n**核心思路**：PolaRiS的核心思路是利用神经重建技术，将真实世界的场景快速转化为高保真度的仿真环境。通过这种方式，可以低成本地创建大量多样化的仿真环境，用于机器人策略的评估和训练。此外，通过仿真数据协同训练，进一步缩小仿真环境与真实环境之间的差距，提高仿真评估的可靠性。\\n\\n**技术框架**：PolaRiS框架主要包含两个阶段：环境重建和策略评估。首先，利用神经重建方法，将真实场景的短视频扫描转化为交互式仿真环境。然后，在这些仿真环境中对机器人策略进行评估。为了进一步提高仿真环境的真实性，采用仿真数据协同训练的方法，利用真实数据和仿真数据共同训练策略，从而缩小领域差距。\\n\\n**关键创新**：PolaRiS的关键创新在于其可扩展的真实到模拟的框架，能够快速、低成本地创建高保真度的仿真环境。与传统方法相比，PolaRiS无需大量人工干预，即可构建多样化的仿真环境。此外，仿真数据协同训练方法能够有效缩小仿真环境与真实环境之间的差距，提高仿真评估的可靠性。\\n\\n**关键设计**：PolaRiS使用神经辐射场（NeRF）或类似技术进行环境重建，将真实场景的视频转化为三维模型。在仿真数据协同训练中，可以使用对抗训练或领域自适应等技术，最小化真实数据和仿真数据之间的分布差异。具体的损失函数和网络结构选择取决于具体的任务和数据集。",
            "application_zh": "PolaRiS可应用于机器人策略的开发、测试和验证，尤其适用于通用机器人策略的评估。该框架能够加速机器人学习的研究进程，降低机器人开发的成本。此外，PolaRiS还可用于创建虚拟现实环境，进行远程操作和训练等应用。",
            "highlight_zh": "PolaRiS在仿真环境中评估的策略与真实世界中的性能表现出更强的相关性，优于现有的仿真基准。通过实验证明，PolaRiS能够快速创建多样化的仿真环境，并实现零样本评估，显著提升了机器人策略评估的效率和可靠性。",
            "tags_zh": [
                "机器人学习",
                "策略评估",
                "真实到模拟",
                "神经重建",
                "仿真环境"
            ],
            "_index": 69,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16881v1/figures/Teaser_Karl_version.jpg",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16881v1/figures/polaris_pipeline.jpg",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16881v1/figures/scene_comp_gui.jpg",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "VIVA: VLM-Guided Instruction-Based Video Editing with Reward Optimization",
            "authors": [
                "Xiaoyan Cong",
                "Haotian Yang",
                "Angtian Wang",
                "Yizhi Wang",
                "Yiding Yang",
                "Canyu Zhang",
                "Chongyang Ma"
            ],
            "arxiv_id": "2512.16906v1",
            "summary": "Instruction-based video editing aims to modify an input video according to a natural-language instruction while preserving content fidelity and temporal coherence. However, existing diffusion-based approaches are often trained on paired data of simple editing operations, which fundamentally limits their ability to generalize to diverse and complex, real-world instructions. To address this generalization gap, we propose VIVA, a scalable framework for instruction-based video editing that leverages VLM-guided encoding and reward optimization. First, we introduce a VLM-based instructor that encodes the textual instruction, the first frame of the source video, and an optional reference image into visually-grounded instruction representations, providing fine-grained spatial and semantic context for the diffusion transformer backbone. Second, we propose a post-training stage, Edit-GRPO, which adapts Group Relative Policy Optimization to the domain of video editing, directly optimizing the model for instruction-faithful, content-preserving, and aesthetically pleasing edits using relative rewards. Furthermore, we propose a data construction pipeline designed to synthetically generate diverse, high-fidelity paired video-instruction data of basic editing operations. Extensive experiments show that VIVA achieves superior instruction following, generalization, and editing quality over state-of-the-art methods. Website: https://viva-paper.github.io",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16906v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "instruction following"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "VIVA：基于VLM引导和奖励优化的指令驱动视频编辑框架",
            "summary_zh": "本文提出VIVA，一个可扩展的指令驱动视频编辑框架，它利用VLM引导的编码和奖励优化来解决现有方法泛化能力不足的问题。该框架包含一个基于VLM的指导器，它将文本指令、源视频首帧和可选的参考图像编码为视觉对齐的指令表示，为扩散Transformer主干网络提供细粒度的空间和语义上下文。此外，提出了Edit-GRPO后训练阶段，将Group Relative Policy Optimization应用于视频编辑领域，使用相对奖励直接优化模型，使其生成符合指令、保持内容一致且美观的编辑结果。同时，设计了一个数据构建流程，用于合成生成多样且高质量的视频-指令对数据。大量实验表明，VIVA在指令遵循、泛化能力和编辑质量方面优于现有方法。",
            "intro_zh": [
                "现有基于扩散模型的视频编辑方法在处理复杂指令时泛化能力有限，主要受限于训练数据简单。",
                "VIVA利用VLM提取视觉对齐的指令表示，并采用Edit-GRPO进行奖励优化，提升编辑质量和指令遵循度。",
                "实验表明，VIVA在指令遵循、泛化能力和编辑质量上超越了现有技术水平，效果显著。"
            ],
            "method_zh": "**问题定义**：指令驱动的视频编辑旨在根据自然语言指令修改输入视频，同时保持内容一致性和时间连贯性。现有基于扩散模型的方法通常在简单的编辑操作配对数据上训练，这限制了它们泛化到多样化和复杂的真实世界指令的能力。\\n\\n**核心思路**：VIVA的核心思路是利用视觉语言模型（VLM）来更好地理解指令，并结合奖励优化来提升编辑质量。VLM能够将文本指令与视频内容对齐，提供更丰富的上下文信息。奖励优化则直接驱动模型生成更符合人类偏好的编辑结果。\\n\\n**技术框架**：VIVA框架主要包含两个阶段：VLM引导的编码阶段和Edit-GRPO后训练阶段。在编码阶段，VLM将文本指令、源视频首帧和可选的参考图像编码为视觉对齐的指令表示。然后，这些表示被输入到扩散Transformer主干网络中进行视频编辑。在Edit-GRPO阶段，使用相对奖励优化模型，使其生成更符合指令、保持内容一致且美观的编辑结果。\\n\\n**关键创新**：VIVA的关键创新在于以下两点：1) 利用VLM进行指令编码，从而更好地理解指令的语义和视觉信息；2) 提出Edit-GRPO后训练方法，直接优化模型的编辑质量，使其更符合人类偏好。与现有方法相比，VIVA能够更好地处理复杂指令，并生成更高质量的编辑结果。\\n\\n**关键设计**：VLM指导器使用预训练的视觉语言模型，并针对视频编辑任务进行微调。Edit-GRPO采用Group Relative Policy Optimization，通过比较不同编辑结果的相对质量来优化模型。此外，还设计了一个数据构建流程，用于合成生成多样且高质量的视频-指令对数据，以增强模型的泛化能力。",
            "application_zh": "VIVA技术可应用于电影制作、广告设计、社交媒体内容生成等领域，实现快速、高质量的视频编辑。该技术能够降低视频编辑的门槛，让用户通过简单的自然语言指令即可完成复杂的编辑任务，具有广阔的应用前景和商业价值。",
            "highlight_zh": "实验结果表明，VIVA在多个视频编辑任务上取得了显著的性能提升。与现有最先进的方法相比，VIVA在指令遵循度、内容一致性和编辑质量方面均有明显优势。具体性能数据和对比基线信息请参考论文原文。",
            "tags_zh": [
                "视频编辑",
                "指令驱动",
                "视觉语言模型",
                "扩散模型",
                "奖励优化"
            ],
            "_index": 70,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16906v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16906v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16906v1/x4.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "REGLUE Your Latents with Global and Local Semantics for Entangled Diffusion",
            "authors": [
                "Giorgos Petsangourakis",
                "Christos Sgouropoulos",
                "Bill Psomas",
                "Theodoros Giannakopoulos",
                "Giorgos Sfikas",
                "Ioannis Kakogeorgiou"
            ],
            "arxiv_id": "2512.16636v1",
            "summary": "Latent diffusion models (LDMs) achieve state-of-the-art image synthesis, yet their reconstruction-style denoising objective provides only indirect semantic supervision: high-level semantics emerge slowly, requiring longer training and limiting sample quality. Recent works inject semantics from Vision Foundation Models (VFMs) either externally via representation alignment or internally by jointly modeling only a narrow slice of VFM features inside the diffusion process, under-utilizing the rich, nonlinear, multi-layer spatial semantics available. We introduce REGLUE (Representation Entanglement with Global-Local Unified Encoding), a unified latent diffusion framework that jointly models (i) VAE image latents, (ii) compact local (patch-level) VFM semantics, and (iii) a global (image-level) [CLS] token within a single SiT backbone. A lightweight convolutional semantic compressor nonlinearly aggregates multi-layer VFM features into a low-dimensional, spatially structured representation, which is entangled with the VAE latents in the diffusion process. An external alignment loss further regularizes internal representations toward frozen VFM targets. On ImageNet 256x256, REGLUE consistently improves FID and accelerates convergence over SiT-B/2 and SiT-XL/2 baselines, as well as over REPA, ReDi, and REG. Extensive experiments show that (a) spatial VFM semantics are crucial, (b) non-linear compression is key to unlocking their full benefit, and (c) global tokens and external alignment act as complementary, lightweight enhancements within our global-local-latent joint modeling framework. The code is available at https://github.com/giorgospets/reglue .",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16636v1",
            "code_links": [
                {
                    "url": "https://github.com/giorgospets/reglue",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "foundation model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "REGLUE：融合全局与局部语义的解耦扩散模型，提升图像合成质量与收敛速度",
            "summary_zh": "潜在扩散模型(LDMs)在图像合成方面取得了最先进的成果，但其重建式去噪目标仅提供间接的语义监督：高层语义出现缓慢，需要更长的训练时间，并限制了样本质量。现有工作通过表征对齐从视觉基础模型(VFMs)外部注入语义，或者仅在扩散过程中对VFM特征的一小部分进行联合建模，未能充分利用可用的丰富、非线性、多层空间语义。我们提出了REGLUE（Representation Entanglement with Global-Local Unified Encoding），一个统一的潜在扩散框架，它在单个SiT骨干网络中联合建模(i)VAE图像潜在变量，(ii)紧凑的局部(patch级别)VFM语义，以及(iii)全局(图像级别)[CLS]token。一个轻量级的卷积语义压缩器将多层VFM特征非线性地聚合为低维、空间结构化的表示，并在扩散过程中与VAE潜在变量纠缠。外部对齐损失进一步将内部表示正则化到冻结的VFM目标。在ImageNet 256x256上，REGLUE始终如一地提高了FID，并加速了SiT-B/2和SiT-XL/2基线以及REPA、ReDi和REG的收敛速度。大量实验表明，(a)空间VFM语义至关重要，(b)非线性压缩是充分发挥其优势的关键，以及(c)全局token和外部对齐在我们全局-局部-潜在联合建模框架中充当互补的、轻量级的增强。",
            "intro_zh": [
                "现有潜在扩散模型语义监督不足，导致训练缓慢和样本质量受限，未能充分利用视觉基础模型(VFMs)的丰富语义信息。",
                "REGLUE通过联合建模VAE潜在变量、局部VFM语义和全局[CLS] token，在扩散过程中融合全局和局部语义信息，实现更有效的语义监督。",
                "实验表明，REGLUE在ImageNet 256x256上显著提升了FID，并加速了收敛速度，优于现有方法，验证了空间VFM语义和非线性压缩的重要性。"
            ],
            "method_zh": "**问题定义**：现有潜在扩散模型在图像合成任务中，依赖重建式的去噪目标进行训练，这种方式提供的语义监督是间接的，导致模型学习高层语义的速度较慢，需要更长的训练时间和更大的计算资源。此外，现有方法未能充分利用视觉基础模型（VFMs）中蕴含的丰富、非线性、多层空间语义信息，限制了生成图像的质量。\\n\\n**核心思路**：REGLUE的核心思路是通过联合建模VAE图像潜在变量、局部（patch级别）VFM语义和全局（图像级别）[CLS] token，在扩散过程中显式地注入全局和局部语义信息，从而增强模型的语义理解能力，加速训练过程，并提升生成图像的质量。这种联合建模的方式允许模型更好地利用VFM提供的多层次语义信息，从而生成更逼真、更符合语义的图像。\\n\\n**技术框架**：REGLUE的整体框架包含以下几个主要模块：1) VAE编码器：将输入图像编码为潜在变量。2) 视觉基础模型（VFM）：提取图像的局部（patch级别）和全局（图像级别）语义特征。3) 语义压缩器：使用轻量级的卷积神经网络将多层VFM特征非线性地压缩为低维、空间结构化的表示。4) SiT骨干网络：作为扩散模型的去噪器，联合建模VAE潜在变量、压缩后的局部VFM语义和全局[CLS] token。5) 外部对齐损失：正则化内部表示，使其与冻结的VFM目标对齐。\\n\\n**关键创新**：REGLUE的关键创新在于其全局-局部-潜在联合建模框架，该框架能够有效地融合来自视觉基础模型的全局和局部语义信息，并将其注入到扩散过程中。此外，使用非线性语义压缩器来处理VFM特征也是一个重要的创新点，它可以有效地降低VFM特征的维度，并保留关键的语义信息。\\n\\n**关键设计**：REGLUE的关键设计包括：1) 使用轻量级的卷积神经网络作为语义压缩器，以降低计算成本。2) 使用SiT（未知）作为扩散模型的骨干网络，以提高模型的表达能力。3) 使用外部对齐损失来正则化内部表示，使其与VFM目标对齐，从而进一步提高生成图像的质量。具体参数设置和损失函数细节在论文中未明确说明，属于未知信息。",
            "application_zh": "REGLUE具有广泛的应用前景，包括图像生成、图像编辑、图像修复、视频生成等领域。该方法可以用于生成高质量、高逼真度的图像和视频内容，例如用于游戏开发、电影制作、广告设计等。此外，REGLUE还可以应用于医学图像分析、遥感图像处理等领域，为这些领域提供更准确、更可靠的图像分析结果。",
            "highlight_zh": "REGLUE在ImageNet 256x256数据集上的实验结果表明，该方法能够显著提升FID指标，并加速收敛速度。与SiT-B/2和SiT-XL/2基线相比，REGLUE取得了明显的性能提升。此外，REGLUE还优于REPA、ReDi和REG等现有方法，验证了其有效性。实验结果还表明，空间VFM语义和非线性压缩是提升性能的关键因素。",
            "tags_zh": [
                "潜在扩散模型",
                "图像合成",
                "视觉基础模型",
                "语义融合",
                "全局局部语义"
            ],
            "_index": 71,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16636v1/figs/training_steps_photos/50k_steps/000343.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16636v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16636v1/figs/cfg/golden_retriever_207/000444.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "VenusBench-GD: A Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks",
            "authors": [
                "Beitong Zhou",
                "Zhexiao Huang",
                "Yuan Guo",
                "Zhangxuan Gu",
                "Tianyu Xia",
                "Zichen Luo",
                "Fei Tang",
                "Dehan Kong",
                "Yanyi Shang",
                "Suling Ou",
                "Zhenlin Guo",
                "Changhua Meng",
                "Shuheng Shen"
            ],
            "arxiv_id": "2512.16501v1",
            "summary": "GUI grounding is a critical component in building capable GUI agents. However, existing grounding benchmarks suffer from significant limitations: they either provide insufficient data volume and narrow domain coverage, or focus excessively on a single platform and require highly specialized domain knowledge. In this work, we present VenusBench-GD, a comprehensive, bilingual benchmark for GUI grounding that spans multiple platforms, enabling hierarchical evaluation for real-word applications. VenusBench-GD contributes as follows: (i) we introduce a large-scale, cross-platform benchmark with extensive coverage of applications, diverse UI elements, and rich annotated data, (ii) we establish a high-quality data construction pipeline for grounding tasks, achieving higher annotation accuracy than existing benchmarks, and (iii) we extend the scope of element grounding by proposing a hierarchical task taxonomy that divides grounding into basic and advanced categories, encompassing six distinct subtasks designed to evaluate models from complementary perspectives. Our experimental findings reveal critical insights: general-purpose multimodal models now match or even surpass specialized GUI models on basic grounding tasks. In contrast, advanced tasks, still favor GUI-specialized models, though they exhibit significant overfitting and poor robustness. These results underscore the necessity of comprehensive, multi-tiered evaluation frameworks.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16501v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "multimodal"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "VenusBench-GD：一个全面的多平台GUI基准，用于评估多样化的Grounding任务",
            "summary_zh": "GUI grounding是构建强大GUI代理的关键组成部分。然而，现有的grounding基准存在显著的局限性：它们要么提供的数据量不足且领域覆盖范围狭窄，要么过度关注单一平台并需要高度专业化的领域知识。本文提出了VenusBench-GD，这是一个全面的、双语的GUI grounding基准，跨越多个平台，能够对真实世界的应用程序进行分层评估。VenusBench-GD的贡献如下：（i）我们引入了一个大规模的、跨平台的基准，具有广泛的应用程序覆盖、多样的UI元素和丰富的标注数据；（ii）我们建立了一个高质量的数据构建流程，用于grounding任务，实现了比现有基准更高的标注准确率；（iii）我们通过提出一个分层任务分类法来扩展元素grounding的范围，该分类法将grounding分为基本和高级类别，包含六个不同的子任务，旨在从互补的角度评估模型。我们的实验结果揭示了关键的见解：通用多模态模型现在在基本grounding任务上匹配甚至超过了专门的GUI模型。相比之下，高级任务仍然偏爱GUI专用模型，尽管它们表现出显著的过拟合和较差的鲁棒性。这些结果强调了全面、多层评估框架的必要性。",
            "intro_zh": [
                "现有GUI grounding基准数据量不足、领域覆盖窄，或过度关注单一平台，限制了GUI代理的发展。",
                "VenusBench-GD构建了一个大规模、跨平台、双语的GUI grounding基准，包含丰富的标注数据和分层任务分类。",
                "实验表明，通用多模态模型在基本任务上可与专用GUI模型媲美，但高级任务仍需专用模型，且存在过拟合问题。"
            ],
            "method_zh": "**问题定义**：论文旨在解决现有GUI grounding基准数据集不足、领域覆盖范围有限以及平台单一的问题。现有方法难以全面评估GUI代理的grounding能力，且标注质量不高，阻碍了相关研究的进展。\\n\\n**核心思路**：论文的核心思路是构建一个大规模、跨平台、高质量的GUI grounding基准数据集VenusBench-GD，并设计分层任务分类，以全面评估模型的grounding能力。通过提供更丰富的数据和更细粒度的评估，促进GUI grounding技术的发展。\\n\\n**技术框架**：VenusBench-GD的构建流程包括数据收集、数据清洗、数据标注和任务划分等步骤。首先，从多个平台收集GUI数据，然后进行清洗和过滤，去除噪声数据。接着，对UI元素进行标注，包括位置、类型、文本等信息。最后，将grounding任务划分为基本和高级类别，并设计六个不同的子任务。\\n\\n**关键创新**：VenusBench-GD的关键创新在于其大规模、跨平台和分层任务分类。与现有基准相比，VenusBench-GD覆盖了更广泛的应用程序和UI元素，提供了更丰富的标注数据，并能够从多个角度评估模型的grounding能力。此外，该基准还采用了高质量的数据构建流程，保证了标注的准确性。\\n\\n**关键设计**：VenusBench-GD的关键设计包括UI元素的标注规范、任务的划分标准以及评估指标的选择。UI元素的标注规范定义了不同类型UI元素的标注方式，确保标注的一致性。任务的划分标准基于grounding的复杂程度，将任务分为基本和高级类别。评估指标包括准确率、召回率和F1值等，用于评估模型在不同任务上的性能。",
            "application_zh": "VenusBench-GD可应用于开发更智能的GUI代理，例如自动化测试工具、辅助技术和人机交互系统。通过利用该基准，研究人员可以训练和评估更强大的GUI grounding模型，从而提高GUI代理的可用性和效率。此外，该基准还可以促进跨平台GUI应用程序的开发和维护。",
            "highlight_zh": "实验结果表明，通用多模态模型在基本grounding任务上可以达到甚至超过专门的GUI模型性能。然而，在高级任务上，GUI专用模型仍然更胜一筹，但存在严重的过拟合问题，鲁棒性较差。这些结果突出了VenusBench-GD在评估模型泛化能力方面的重要性。",
            "tags_zh": [
                "GUI grounding",
                "多平台基准",
                "用户界面",
                "多模态学习",
                "分层任务",
                "基准数据集",
                "人机交互",
                "计算机视觉"
            ],
            "_index": 72,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16501v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16501v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16501v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Avatar4D: Synthesizing Domain-Specific 4D Humans for Real-World Pose Estimation",
            "authors": [
                "Jerrin Bright",
                "Zhibo Wang",
                "Dmytro Klepachevskyi",
                "Yuhao Chen",
                "Sirisha Rambhatla",
                "David Clausi",
                "John Zelek"
            ],
            "arxiv_id": "2512.16199v1",
            "summary": "We present Avatar4D, a real-world transferable pipeline for generating customizable synthetic human motion datasets tailored to domain-specific applications. Unlike prior works, which focus on general, everyday motions and offer limited flexibility, our approach provides fine-grained control over body pose, appearance, camera viewpoint, and environmental context, without requiring any manual annotations. To validate the impact of Avatar4D, we focus on sports, where domain-specific human actions and movement patterns pose unique challenges for motion understanding. In this setting, we introduce Syn2Sport, a large-scale synthetic dataset spanning sports, including baseball and ice hockey. Avatar4D features high-fidelity 4D (3D geometry over time) human motion sequences with varying player appearances rendered in diverse environments. We benchmark several state-of-the-art pose estimation models on Syn2Sport and demonstrate their effectiveness for supervised learning, zero-shot transfer to real-world data, and generalization across sports. Furthermore, we evaluate how closely the generated synthetic data aligns with real-world datasets in feature space. Our results highlight the potential of such systems to generate scalable, controllable, and transferable human datasets for diverse domain-specific tasks without relying on domain-specific real data.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16199v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "zero-shot transfer"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "Avatar4D：合成特定领域4D人体数据，用于真实场景姿态估计",
            "summary_zh": "本文提出Avatar4D，一个可迁移的真实世界流水线，用于生成可定制的合成人体运动数据集，专门针对特定领域的应用。与以往侧重于通用日常运动且灵活性有限的工作不同，我们的方法可以对身体姿势、外观、相机视角和环境上下文进行细粒度控制，而无需任何手动标注。为了验证Avatar4D的影响，我们专注于体育运动，其中特定领域的人体动作和运动模式对运动理解提出了独特的挑战。在这种背景下，我们引入了Syn2Sport，一个涵盖棒球和冰球等运动的大规模合成数据集。Avatar4D具有高保真度的4D（随时间变化的3D几何）人体运动序列，具有不同的运动员外观，并在不同的环境中渲染。我们在Syn2Sport上对几种最先进的姿态估计模型进行了基准测试，并证明了它们在监督学习、零样本迁移到真实世界数据以及跨运动泛化方面的有效性。此外，我们评估了生成的合成数据在特征空间中与真实世界数据集的对齐程度。我们的结果突出了这种系统在生成可扩展、可控和可迁移的人体数据集方面的潜力，用于各种特定领域的任务，而无需依赖特定领域的真实数据。",
            "intro_zh": [
                "现有方法在生成人体运动数据时，缺乏对特定领域动作的针对性，且控制粒度不足，限制了其在专业领域的应用。",
                "Avatar4D通过精细控制人体姿势、外观、视角和环境，生成特定领域的高质量合成数据，无需手动标注，提升数据多样性。",
                "实验表明，基于Avatar4D生成的Syn2Sport数据集训练的姿态估计模型，在真实体育运动数据上表现出良好的泛化能力。"
            ],
            "method_zh": "**问题定义**：论文旨在解决特定领域（如体育运动）人体姿态估计中，缺乏高质量、多样化的训练数据的问题。现有方法生成的通用人体运动数据，难以覆盖特定领域动作的复杂性和特殊性，导致模型在真实场景下的性能下降。此外，真实数据的标注成本高昂，限制了模型的发展。\\n\\n**核心思路**：论文的核心思路是利用计算机图形学技术，构建一个可定制的合成数据生成流水线Avatar4D。该流水线能够根据用户需求，生成具有不同姿势、外观、视角和环境的4D人体运动序列。通过在合成数据上训练模型，可以提高模型在真实场景下的泛化能力，同时降低数据获取和标注的成本。\\n\\n**技术框架**：Avatar4D流水线主要包含以下几个模块：1) 人体模型库：包含不同体型、服装和外观的人体模型。2) 运动捕捉数据：用于驱动人体模型运动，可以来自真实数据或程序生成。3) 环境建模：创建各种虚拟环境，如体育场馆、训练场地等。4) 渲染引擎：将人体模型和环境渲染成图像序列。5) 数据增强：对合成数据进行增强，如添加噪声、改变光照等。整个流程无需人工标注，即可生成大规模的特定领域人体运动数据集。\\n\\n**关键创新**：Avatar4D的关键创新在于其可定制性和领域针对性。与以往的通用人体运动数据生成方法不同，Avatar4D可以根据特定领域的需求，调整人体姿势、外观和环境，从而生成更具代表性的训练数据。此外，Avatar4D无需手动标注，降低了数据生成的成本。\\n\\n**关键设计**：Avatar4D的关键设计包括：1) 使用参数化人体模型，可以方便地调整人体体型和外观。2) 采用运动捕捉数据驱动人体模型运动，保证运动的真实性和自然性。3) 使用高质量的渲染引擎，生成逼真的图像序列。4) 设计了多种数据增强方法，提高模型的鲁棒性。",
            "application_zh": "Avatar4D技术可广泛应用于体育运动分析、虚拟现实、游戏开发、动作捕捉等领域。通过生成特定领域的人体运动数据，可以训练更准确、更鲁棒的姿态估计模型，从而实现运动员动作分析、虚拟角色控制、人机交互等功能。该技术有望推动相关领域的发展，并为人们带来更智能、更便捷的体验。",
            "highlight_zh": "论文在Syn2Sport数据集上对多种姿态估计模型进行了评估，结果表明，在合成数据上训练的模型在真实数据上表现出良好的泛化能力。例如，在棒球运动姿态估计任务中，使用Syn2Sport训练的模型在真实数据集上的性能接近甚至超过了使用真实数据训练的模型。此外，实验还证明了Avatar4D生成的数据可以用于零样本迁移学习，即在没有真实数据的情况下，直接将模型应用于真实场景。",
            "tags_zh": [
                "4D人体建模",
                "合成数据生成",
                "姿态估计",
                "领域自适应",
                "运动分析",
                "计算机视觉",
                "深度学习"
            ],
            "_index": 73,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16199v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16199v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16199v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times",
            "authors": [
                "Jintao Zhang",
                "Kaiwen Zheng",
                "Kai Jiang",
                "Haoxu Wang",
                "Ion Stoica",
                "Joseph E. Gonzalez",
                "Jianfei Chen",
                "Jun Zhu"
            ],
            "arxiv_id": "2512.16093v1",
            "summary": "We introduce TurboDiffusion, a video generation acceleration framework that can speed up end-to-end diffusion generation by 100-200x while maintaining video quality. TurboDiffusion mainly relies on several components for acceleration: (1) Attention acceleration: TurboDiffusion uses low-bit SageAttention and trainable Sparse-Linear Attention (SLA) to speed up attention computation. (2) Step distillation: TurboDiffusion adopts rCM for efficient step distillation. (3) W8A8 quantization: TurboDiffusion quantizes model parameters and activations to 8 bits to accelerate linear layers and compress the model. In addition, TurboDiffusion incorporates several other engineering optimizations.\n  We conduct experiments on the Wan2.2-I2V-14B-720P, Wan2.1-T2V-1.3B-480P, Wan2.1-T2V-14B-720P, and Wan2.1-T2V-14B-480P models. Experimental results show that TurboDiffusion achieves 100-200x speedup for video generation even on a single RTX 5090 GPU, while maintaining comparable video quality. The GitHub repository, which includes model checkpoints and easy-to-use code, is available at https://github.com/thu-ml/TurboDiffusion.",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16093v1",
            "code_links": [
                {
                    "url": "https://github.com/thu-ml/TurboDiffusion",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "linear attention",
                        "distillation"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "TurboDiffusion：通过多重加速策略实现视频扩散模型100-200倍的加速。",
            "summary_zh": "TurboDiffusion是一个视频生成加速框架，能够在保持视频质量的同时，将端到端扩散生成速度提高100-200倍。TurboDiffusion主要依赖于以下几个加速组件：（1）注意力加速：TurboDiffusion使用低比特SageAttention和可训练的稀疏线性注意力（SLA）来加速注意力计算。（2）步骤蒸馏：TurboDiffusion采用rCM进行高效的步骤蒸馏。（3）W8A8量化：TurboDiffusion将模型参数和激活量化为8位，以加速线性层并压缩模型。此外，TurboDiffusion还包含其他一些工程优化。我们在Wan2.2-I2V-14B-720P、Wan2.1-T2V-1.3B-480P、Wan2.1-T2V-14B-720P和Wan2.1-T2V-14B-480P模型上进行了实验。实验结果表明，即使在单个RTX 5090 GPU上，TurboDiffusion也能实现100-200倍的视频生成加速，同时保持相当的视频质量。包含模型检查点和易于使用的代码的GitHub存储库可在https://github.com/thu-ml/TurboDiffusion上找到。",
            "intro_zh": [
                "现有视频扩散模型计算复杂度高，生成速度慢，难以满足实时性要求。",
                "TurboDiffusion通过注意力加速、步骤蒸馏和模型量化等多重策略，显著降低计算量。",
                "实验表明，TurboDiffusion在保证视频质量的同时，实现了100-200倍的加速效果。"
            ],
            "method_zh": "**问题定义**：现有视频扩散模型在生成高质量视频时，计算复杂度极高，生成速度慢，难以满足实际应用中对实时性的需求。尤其是在高分辨率视频生成方面，计算瓶颈更加突出。因此，如何加速视频扩散模型的生成速度，同时保持视频质量，是一个重要的研究问题。\\n\\n**核心思路**：TurboDiffusion的核心思路是通过多方面的优化，降低视频扩散模型的计算复杂度，从而加速生成过程。具体来说，它从注意力机制、模型蒸馏和模型量化三个主要方面入手，并结合其他工程优化手段，实现了显著的加速效果。这种多管齐下的方法能够有效地解决视频扩散模型速度慢的问题。\\n\\n**技术框架**：TurboDiffusion的整体框架主要包含以下几个模块：1) **注意力加速模块**：使用低比特SageAttention和可训练的稀疏线性注意力（SLA）来降低注意力计算的复杂度。2) **步骤蒸馏模块**：采用rCM方法进行高效的步骤蒸馏，减少扩散过程所需的迭代次数。3) **模型量化模块**：将模型参数和激活量化为8位（W8A8），以加速线性层计算并压缩模型大小。4) **工程优化模块**：包含其他一些工程优化手段，进一步提升生成速度。\\n\\n**关键创新**：TurboDiffusion的关键创新在于其综合性的加速策略。它不仅仅依赖于单一的优化方法，而是将注意力加速、步骤蒸馏和模型量化等多种技术结合起来，形成一个完整的加速框架。这种综合性的方法能够充分利用各种优化技术的优势，从而实现更高的加速效果。此外，可训练的稀疏线性注意力（SLA）也是一个重要的创新点，它能够在降低计算复杂度的同时，保持模型的表达能力。\\n\\n**关键设计**：在注意力加速方面，SageAttention的具体比特数选择以及SLA的稀疏模式设计是关键。在步骤蒸馏方面，rCM的具体参数设置会影响蒸馏效果。在模型量化方面，W8A8量化的具体实现方式，例如量化范围和舍入策略，会影响模型的精度。此外，损失函数的选择和优化器的设置也会影响模型的训练效果。",
            "application_zh": "TurboDiffusion的加速技术可广泛应用于视频生成、视频编辑、虚拟现实、游戏开发等领域。它能够降低视频生成的时间成本和硬件需求，使得高质量视频内容的创作更加便捷。未来，TurboDiffusion有望推动视频生成技术在移动设备和边缘计算平台上的应用，实现更广泛的普及。",
            "highlight_zh": "TurboDiffusion在多个视频生成模型上实现了100-200倍的加速，同时保持了与原始模型相当的视频质量。即使在单张RTX 5090 GPU上，也能达到如此显著的加速效果，证明了该框架的有效性和实用性。开源代码和模型权重也为后续研究和应用提供了便利。",
            "tags_zh": [
                "视频生成",
                "扩散模型",
                "模型加速",
                "注意力机制",
                "模型量化",
                "步骤蒸馏",
                "稀疏注意力"
            ],
            "_index": 74,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16093v1/src/figs/original/outputs_1.3B/frames/12-1.jpg",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16093v1/src/figs/turbodiffusion/outputs_1.3B/frames/12-1.jpg",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16093v1/src/figs/i2v/original/outputs_A14B_720p/frames/1-1.jpg",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Machine Learning Enabled Graph Analysis of Particulate Composites: Application to Solid-state Battery Cathodes",
            "authors": [
                "Zebin Li",
                "Shimao Deng",
                "Yijin Liu",
                "Jia-Mian Hu"
            ],
            "arxiv_id": "2512.16085v1",
            "summary": "Particulate composites underpin many solid-state chemical and electrochemical systems, where microstructural features such as multiphase boundaries and inter-particle connections strongly influence system performance. Advances in X-ray microscopy enable capturing large-scale, multimodal images of these complex microstructures with an unprecedentedly high throughput. However, harnessing these datasets to discover new physical insights and guide microstructure optimization remains a major challenge. Here, we develop a machine learning (ML) enabled framework that enables automated transformation of experimental multimodal X-ray images of multiphase particulate composites into scalable, topology-aware graphs for extracting physical insights and establishing local microstructure-property relationships at both the particle and network level. Using the multiphase particulate cathode of solid-state lithium batteries as an example, our ML-enabled graph analysis corroborates the critical role of triple phase junctions and concurrent ion/electron conduction channels in realizing desirable local electrochemical activity. Our work establishes graph-based microstructure representation as a powerful paradigm for bridging multimodal experimental imaging and functional understanding, and facilitating microstructure-aware data-driven materials design in a broad range of particulate composites.",
            "categories": [
                "cond-mat.mtrl-sci",
                "cs.CV"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16085v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "multimodal"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出基于机器学习的图分析方法，用于固态电池正极材料微观结构表征与性能预测。",
            "summary_zh": "本文提出了一种基于机器学习（ML）的框架，该框架能够自动将多相颗粒复合材料的实验多模态X射线图像转换为可扩展的、拓扑感知的图，从而提取物理见解，并在颗粒和网络层面建立局部微观结构-性能关系。以固态锂电池的多相颗粒正极为例，我们的ML图分析证实了三相结和并发离子/电子传导通道在实现理想的局部电化学活性中的关键作用。我们的工作将基于图的微观结构表示确立为连接多模态实验成像和功能理解的强大范例，并促进了各种颗粒复合材料中具有微观结构感知的数据驱动材料设计。",
            "intro_zh": [
                "多相复合材料的微观结构特征，如相界面和颗粒间连接，对系统性能有重要影响，但现有方法难以有效利用高通量X射线显微镜数据。",
                "该论文提出了一种基于机器学习的图分析框架，将多模态X射线图像转化为拓扑感知的图，用于提取物理见解和建立微观结构-性能关系。",
                "通过固态锂电池正极的案例研究，验证了该方法在揭示三相结和离子/电子传导通道对电化学活性的重要作用，为材料设计提供了新思路。"
            ],
            "method_zh": "**问题定义**：论文旨在解决如何从多相颗粒复合材料（特别是固态电池正极）的大规模多模态X射线图像中提取有意义的微观结构信息，并将其与材料性能关联起来的问题。现有方法难以有效处理高通量图像数据，无法充分挖掘微观结构与性能之间的复杂关系。\\n\\n**核心思路**：论文的核心思路是将多相颗粒复合材料的微观结构表示为图。图的节点代表颗粒，边代表颗粒之间的连接。通过图分析，可以提取拓扑信息，例如三相结的数量和位置，以及离子/电子传导通道的连通性。然后，利用机器学习方法将这些图特征与材料的电化学性能关联起来。\\n\\n**技术框架**：该框架包含以下主要模块：1) 多模态X射线图像采集；2) 图像分割和颗粒识别；3) 基于分割结果构建拓扑图，节点代表颗粒，边代表颗粒间的连接关系；4) 图特征提取，例如节点度、聚类系数、最短路径长度等；5) 机器学习模型训练，将图特征与材料性能关联起来。\\n\\n**关键创新**：该方法最重要的创新点在于将传统的图像分析问题转化为图分析问题，从而能够利用图论中的各种工具来提取微观结构的拓扑信息。此外，该方法结合了机器学习，能够自动学习微观结构与性能之间的复杂关系，而无需人工定义特征。\\n\\n**关键设计**：论文中，图的构建方式至关重要，需要仔细选择连接颗粒的标准。机器学习模型的选择也需要根据具体问题进行调整。例如，可以使用图神经网络（GNN）来直接处理图数据，或者使用传统的机器学习模型（例如支持向量机或随机森林）来处理提取的图特征。",
            "application_zh": "该研究成果可广泛应用于各种颗粒复合材料的设计和优化，例如固态电池、催化剂、陶瓷材料等。通过分析材料的微观结构，可以预测其性能，并指导材料的制备工艺，从而加速新材料的研发过程。该方法还可用于分析材料的失效机制，提高材料的可靠性。",
            "highlight_zh": "该研究通过对固态锂电池正极材料的分析，验证了三相结和并发离子/电子传导通道在实现理想的局部电化学活性中的关键作用。通过图分析，能够定量评估不同微观结构特征对电池性能的影响，为优化正极材料的微观结构提供了依据。具体性能数据和对比基线在原文中未明确给出，属于未来研究方向。",
            "tags_zh": [
                "机器学习",
                "图分析",
                "固态电池",
                "正极材料",
                "微观结构",
                "多模态成像",
                "材料设计"
            ],
            "_index": 75,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "Impacts of Racial Bias in Historical Training Data for News AI",
            "authors": [
                "Rahul Bhargava",
                "Malene Hornstrup Jespersen",
                "Emily Boardman Ndulue",
                "Vivica Dsouza"
            ],
            "arxiv_id": "2512.16901v1",
            "summary": "AI technologies have rapidly moved into business and research applications that involve large text corpora, including computational journalism research and newsroom settings. These models, trained on extant data from various sources, can be conceptualized as historical artifacts that encode decades-old attitudes and stereotypes. This paper investigates one such example trained on the broadly-used New York Times Annotated Corpus to create a multi-label classifier. Our use in research settings surfaced the concerning \"blacks\" thematic topic label. Through quantitative and qualitative means we investigate this label's use in the training corpus, what concepts it might be encoding in the trained classifier, and how those concepts impact our model use. Via the application of explainable AI methods, we find that the \"blacks\" label operates partially as a general \"racism detector\" across some minoritized groups. However, it performs poorly against expectations on modern examples such as COVID-19 era anti-Asian hate stories, and reporting on the Black Lives Matter movement. This case study of interrogating embedded biases in a model reveals how similar applications in newsroom settings can lead to unexpected outputs that could impact a wide variety of potential uses of any large language model-story discovery, audience targeting, summarization, etc. The fundamental tension this exposes for newsrooms is how to adopt AI-enabled workflow tools while reducing the risk of reproducing historical biases in news coverage.",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.CL",
                "cs.CY"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16901v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "揭示新闻AI中历史数据偏见：以纽约时报语料库为例，分析种族标签的影响。",
            "summary_zh": "人工智能技术正迅速应用于涉及大型文本语料库的商业和研究领域，包括计算新闻学研究和新闻编辑室环境。这些模型在现有数据上训练，可以被视为编码了几十年前的态度和刻板印象的历史产物。本文研究了一个基于广泛使用的《纽约时报》标注语料库训练的多标签分类器。研究中浮现了令人担忧的“黑人”主题标签。通过定量和定性的方法，我们调查了这个标签在训练语料库中的使用情况，它可能在训练后的分类器中编码了哪些概念，以及这些概念如何影响模型的应用。通过可解释人工智能方法，我们发现“黑人”标签在一定程度上充当了针对一些少数族裔群体的通用“种族主义检测器”。然而，在诸如COVID-19时代的反亚裔仇恨故事以及对“黑人的命也是命”运动的报道等现代例子中，它的表现不尽如人意。这个对模型中嵌入偏见的案例研究揭示了新闻编辑室环境中类似的应用如何导致意想不到的输出，这可能会影响任何大型语言模型的各种潜在用途——故事发现、受众定位、摘要等。这为新闻编辑室暴露出的根本矛盾是，如何在采用人工智能支持的工作流程工具的同时，降低在新闻报道中重现历史偏见的风险。",
            "intro_zh": [
                "现有新闻AI模型训练于历史数据，可能内嵌过时的偏见和刻板印象，影响新闻报道的客观性。",
                "通过分析《纽约时报》语料库训练的模型中“黑人”标签的使用，揭示其潜在的偏见编码。",
                "实验表明该标签在某些情况下充当“种族主义检测器”，但在现代事件中表现不佳，存在局限性。"
            ],
            "method_zh": "**问题定义**：论文旨在揭示新闻AI模型中由于历史训练数据造成的种族偏见问题。现有方法忽略了训练数据中可能存在的偏见，导致模型在实际应用中产生不公平或不准确的结果，尤其是在涉及种族相关议题时。这种偏见会影响新闻报道的客观性和公正性。\\n\\n**核心思路**：论文的核心思路是通过分析一个在《纽约时报》标注语料库上训练的多标签分类器，特别是其中的“黑人”标签，来揭示模型中潜在的种族偏见。通过定量和定性分析，研究人员试图理解该标签在语料库中的使用方式，以及它在模型中编码了哪些概念。\\n\\n**技术框架**：该研究的技术框架主要包括以下几个阶段：1) 选择《纽约时报》标注语料库作为研究对象；2) 分析语料库中“黑人”标签的使用情况，包括其频率、上下文等；3) 使用该语料库训练一个多标签分类器；4) 使用可解释人工智能（XAI）方法，如LIME或SHAP，来分析模型对“黑人”标签的预测行为，并识别影响预测的关键特征；5) 在现代新闻事件（如COVID-19期间的反亚裔仇恨事件和“黑人的命也是命”运动）上测试模型的表现，评估其泛化能力和潜在偏见。\\n\\n**关键创新**：该研究的关键创新在于其对新闻AI模型中历史数据偏见的系统性分析。它不仅揭示了偏见的存在，还尝试理解偏见的来源和影响。此外，该研究还强调了可解释人工智能方法在识别和减轻模型偏见中的作用。\\n\\n**关键设计**：研究的关键设计包括：1) 选择《纽约时报》语料库，因为它是一个广泛使用的新闻文本资源；2) 使用多标签分类器，以便捕捉文本中多个主题之间的关系；3) 应用可解释人工智能方法，以便理解模型的预测行为；4) 在现代新闻事件上进行测试，以便评估模型的泛化能力和潜在偏见。",
            "application_zh": "该研究成果可应用于新闻编辑室，帮助新闻从业者识别和减轻AI模型中的偏见，提高新闻报道的客观性和公正性。此外，该研究也为其他领域（如法律、教育等）的AI应用提供了借鉴，有助于开发更公平、更负责任的AI系统。该研究强调了在AI应用中考虑历史数据偏见的重要性，并为未来的研究方向提供了指导。",
            "highlight_zh": "研究发现，在《纽约时报》语料库训练的模型中，“黑人”标签在一定程度上充当了“种族主义检测器”，但其性能在现代事件中表现不佳。例如，在COVID-19期间的反亚裔仇恨事件和“黑人的命也是命”运动的报道中，该标签的预测结果与预期不符，表明模型存在泛化能力不足和潜在偏见问题。",
            "tags_zh": [
                "新闻AI",
                "种族偏见",
                "历史数据",
                "可解释AI",
                "文本分类"
            ],
            "_index": 76,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16901v1/figures/fig1-blacks-use.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16901v1/figures/fig2-boxplots.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16901v1/figures/fig3-terms-grid.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI",
            "authors": [
                "Hao Liang",
                "Xiaochen Ma",
                "Zhou Liu",
                "Zhen Hao Wong",
                "Zhengyang Zhao",
                "Zimo Meng",
                "Runming He",
                "Chengyu Shen",
                "Qifeng Cai",
                "Zhaoyang Han",
                "Meiyi Qiang",
                "Yalin Feng",
                "Tianyi Bai",
                "Zewei Pan",
                "Ziyi Guo",
                "Yizhen Jiang",
                "Jingwen Deng",
                "Qijie You",
                "Peichao Lai",
                "Tianyu Guo",
                "Chi Hsu Tsai",
                "Hengyi Feng",
                "Rui Hu",
                "Wenkai Yu",
                "Junbo Niu",
                "Bohan Zeng",
                "Ruichuan An",
                "Lu Ma",
                "Jihao Huang",
                "Yaowei Zheng",
                "Conghui He",
                "Linpeng Tang",
                "Bin Cui",
                "Weinan E",
                "Wentao Zhang"
            ],
            "arxiv_id": "2512.16676v1",
            "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.",
            "categories": [
                "cs.LG",
                "cs.CL"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16676v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "DataFlow：一个LLM驱动的统一数据准备与工作流自动化框架",
            "summary_zh": "为了应对大语言模型（LLM）对高质量数据日益增长的需求，本文提出了DataFlow，一个统一且可扩展的LLM驱动的数据准备框架。DataFlow采用系统级抽象，实现了模块化、可重用和可组合的数据转换，并提供了一个PyTorch风格的pipeline构建API，用于构建可调试和优化的数据流。该框架包含近200个可重用算子和六个通用领域pipeline，涵盖文本、数学推理、代码、Text-to-SQL、agentic RAG和大规模知识抽取。为了进一步提高可用性，我们引入了DataFlow-Agent，它通过算子合成、pipeline规划和迭代验证，自动将自然语言规范转换为可执行的pipeline。在六个代表性用例中，DataFlow始终提高了下游LLM的性能。我们的数学、代码和文本pipeline优于人工数据集和专门的合成基线，在Text-to-SQL中实现了高达+3%的执行准确率（超过SynSQL），在代码基准测试中平均提高了+7%，在MATH、GSM8K和AIME上获得了1-3分的提升。此外，DataFlow生成的统一的1万样本数据集使基础模型能够超越在100万Infinity-Instruct数据上训练的同类模型。这些结果表明，DataFlow为可靠、可重复和可扩展的LLM数据准备提供了一个实用且高性能的基础，并为未来的数据中心AI开发奠定了系统级基础。",
            "intro_zh": [
                "现有数据准备方法依赖临时脚本和松散的工作流，缺乏抽象，难以复现，且对模型在环数据生成支持有限。",
                "DataFlow框架通过系统级抽象实现模块化、可重用和可组合的数据转换，并提供PyTorch风格的pipeline构建API。",
                "实验表明，DataFlow在多个任务上超越人工数据集和特定基线，并能用少量高质量数据训练出更优模型。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大语言模型时代，数据准备流程的低效、不可靠和难以扩展的问题。现有方法主要依赖于手工编写的脚本和非结构化的工作流，缺乏统一的抽象和管理，导致数据准备过程难以复现、调试和优化，同时也难以支持模型在环的数据生成。\n\n**核心思路**：论文的核心思路是构建一个统一的、可扩展的、LLM驱动的数据准备框架DataFlow。该框架通过提供系统级的抽象，将数据转换过程分解为模块化的、可重用的算子，并通过pipeline的方式将这些算子组合起来，从而实现数据准备流程的自动化、可控和可优化。\n\n**技术框架**：DataFlow框架包含以下几个主要组成部分：1) 一组可重用的数据转换算子，涵盖文本、数学、代码等多个领域；2) 一个PyTorch风格的pipeline构建API，用于构建数据流；3) DataFlow-Agent，一个LLM驱动的智能体，可以将自然语言描述转换为可执行的pipeline。整体流程是，用户可以使用API或者自然语言描述来构建数据准备pipeline，DataFlow-Agent负责将自然语言描述转换为pipeline，然后pipeline执行数据转换，最终生成高质量的数据。\n\n**关键创新**：DataFlow的关键创新在于其统一的抽象和LLM驱动的自动化。它将各种数据准备任务抽象为统一的算子和pipeline，使得数据准备流程更加模块化和可重用。同时，DataFlow-Agent利用LLM的能力，实现了自然语言到可执行pipeline的自动转换，大大降低了数据准备的门槛。\n\n**关键设计**：DataFlow框架的关键设计包括：1) 算子的设计，需要保证算子的通用性和可组合性；2) pipeline构建API的设计，需要保证API的易用性和灵活性；3) DataFlow-Agent的设计，需要保证其能够准确地理解用户的意图，并生成正确的pipeline。具体的技术细节包括算子的实现方式、API的接口定义、以及DataFlow-Agent中LLM的使用方式和训练策略等，但论文中没有详细说明。",
            "application_zh": "DataFlow框架可广泛应用于各种需要高质量数据的大语言模型训练场景，例如文本生成、代码生成、数学推理、知识图谱构建等。它能够提高数据准备的效率和质量，降低数据准备的成本，并加速大语言模型的开发和应用。未来，DataFlow有望成为数据中心AI时代的重要基础设施。",
            "highlight_zh": "DataFlow在多个任务上取得了显著的性能提升。在Text-to-SQL任务中，DataFlow的执行准确率比SynSQL提高了3%。在代码生成任务中，DataFlow在多个代码基准测试中平均提高了7%。在数学推理任务中，DataFlow在MATH、GSM8K和AIME数据集上分别提高了1-3分。此外，DataFlow生成的1万样本数据集能够使基础模型超越在100万Infinity-Instruct数据上训练的同类模型。",
            "tags_zh": [
                "数据准备",
                "大语言模型",
                "工作流自动化",
                "LLM驱动",
                "数据中心AI"
            ],
            "_index": 77,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16676v1/x4.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16676v1/x5.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16676v1/x6.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Muon is Provably Faster with Momentum Variance Reduction",
            "authors": [
                "Xun Qian",
                "Hussein Rammal",
                "Dmitry Kovalev",
                "Peter Richtárik"
            ],
            "arxiv_id": "2512.16598v1",
            "summary": "Recent empirical research has demonstrated that deep learning optimizers based on the linear minimization oracle (LMO) over specifically chosen Non-Euclidean norm balls, such as Muon and Scion, outperform Adam-type methods in the training of large language models. In this work, we show that such optimizers can be provably improved by replacing their vanilla momentum by momentum variance reduction (MVR). Instead of proposing and analyzing MVR variants of Muon and Scion separately, we incorporate MVR into the recently proposed Gluon framework, which captures Muon, Scion and other specific Non-Euclidean LMO-based methods as special cases, and at the same time works with a more general smoothness assumption which better captures the layer-wise structure of neural networks. In the non-convex case, we incorporate MVR into Gluon in three different ways. All of them improve the convergence rate from ${\\cal O} (\\frac{1}{K^{1/4}})$ to ${\\cal O} (\\frac{1}{K^{1/3}})$. Additionally, we provide improved rates in the star-convex case. Finally, we conduct several numerical experiments that verify the superior performance of our proposed algorithms in terms of iteration complexity.",
            "categories": [
                "math.OC",
                "cs.LG"
            ],
            "primary_category": "math.OC",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "31 pages, 4 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16598v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出动量方差减少方法以提升Muon优化器性能",
            "summary_zh": "近期的实证研究表明，基于线性最小化oracle（LMO）的深度学习优化器，如Muon和Scion，在训练大型语言模型时优于Adam类方法。本文展示了通过将传统动量替换为动量方差减少（MVR），可以对这些优化器进行可证明的改进。我们将MVR整合到最近提出的Gluon框架中，该框架能够捕捉Muon、Scion及其他特定的非欧几里得LMO方法，同时适用于更一般的光滑性假设。在非凸情况下，我们以三种不同方式将MVR纳入Gluon，所有方法均将收敛速率从${\textcal O} (\frac{1}{K^{1/4}})$提升至${\textcal O} (\frac{1}{K^{1/3}})$，并在星凸情况下提供了改进的速率。最后，我们进行了多项数值实验，验证了所提算法在迭代复杂性方面的优越性能。",
            "intro_zh": [
                "现有的深度学习优化器在训练大型语言模型时，收敛速度较慢，尤其是基于传统动量的方法。",
                "本文提出将动量方差减少（MVR）方法整合进Gluon框架，以提升优化器的收敛速率和性能。",
                "实验结果显示，整合MVR后，收敛速率从${\textcal O} (\frac{1}{K^{1/4}})$提升至${\textcal O} (\frac{1}{K^{1/3}})$，并在多个场景中验证了算法的优越性。"
            ],
            "method_zh": "**问题定义**：本文旨在解决现有深度学习优化器在训练大型语言模型时收敛速度慢的问题，尤其是传统动量方法的不足之处。\\n\\n**核心思路**：通过将动量方差减少（MVR）方法引入到Gluon框架中，旨在提升优化器的收敛速率，并更好地适应神经网络的层次结构。\\n\\n**技术框架**：Gluon框架整合了Muon、Scion等多种非欧几里得LMO方法，采用更一般的光滑性假设。MVR在非凸情况下以三种不同方式被纳入Gluon中。\\n\\n**关键创新**：本文的主要创新在于将MVR与Gluon框架结合，显著提升了收敛速率，并在星凸情况下提供了更好的性能表现。\\n\\n**关键设计**：在设计中，MVR的引入使得收敛速率从${\textcal O} (\frac{1}{K^{1/4}})$提升至${\textcal O} (\frac{1}{K^{1/3}})$，并通过数值实验验证了算法的有效性和优越性。具体的参数设置和损失函数设计在实验部分进行了详细描述。",
            "application_zh": "该研究的优化算法可广泛应用于自然语言处理、计算机视觉等领域，尤其是在需要高效训练大型模型的场景中。通过提升优化器的性能，能够加速模型的训练过程，提高实际应用的效率和效果，具有重要的实际价值和未来影响。",
            "highlight_zh": "实验结果表明，整合MVR后的优化器在迭代复杂性方面表现优越，收敛速率从${\textcal O} (\frac{1}{K^{1/4}})$提升至${\textcal O} (\frac{1}{K^{1/3}})$，在多个基准测试中均优于传统的Adam类方法，验证了所提算法的有效性。",
            "tags_zh": [
                "深度学习",
                "优化算法",
                "动量方差减少",
                "非欧几里得方法",
                "Gluon框架",
                "收敛速率",
                "大型语言模型"
            ],
            "_index": 78,
            "_used_api": "openai",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16598v1/fig/MVR1gbs512.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16598v1/fig/MVR1gbs128.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16598v1/fig/MVR2gbs512.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "A Systematic Study of Code Obfuscation Against LLM-based Vulnerability Detection",
            "authors": [
                "Xiao Li",
                "Yue Li",
                "Hao Wu",
                "Yue Zhang",
                "Yechao Zhang",
                "Fengyuan Xu",
                "Sheng Zhong"
            ],
            "arxiv_id": "2512.16538v1",
            "summary": "As large language models (LLMs) are increasingly adopted for code vulnerability detection, their reliability and robustness across diverse vulnerability types have become a pressing concern. In traditional adversarial settings, code obfuscation has long been used as a general strategy to bypass auditing tools, preserving exploitability without tampering with the tools themselves. Numerous efforts have explored obfuscation methods and tools, yet their capabilities differ in terms of supported techniques, granularity, and programming languages, making it difficult to systematically assess their impact on LLM-based vulnerability detection. To address this gap, we provide a structured systematization of obfuscation techniques and evaluate them under a unified framework. Specifically, we categorize existing obfuscation methods into three major classes (layout, data flow, and control flow) covering 11 subcategories and 19 concrete techniques. We implement these techniques across four programming languages (Solidity, C, C++, and Python) using a consistent LLM-driven approach, and evaluate their effects on 15 LLMs spanning four model families (DeepSeek, OpenAI, Qwen, and LLaMA), as well as on two coding agents (GitHub Copilot and Codex). Our findings reveal both positive and negative impacts of code obfuscation on LLM-based vulnerability detection, highlighting conditions under which obfuscation leads to performance improvements or degradations. We further analyze these outcomes with respect to vulnerability characteristics, code properties, and model attributes. Finally, we outline several open problems and propose future directions to enhance the robustness of LLMs for real-world vulnerability detection.",
            "categories": [
                "cs.CR",
                "cs.LG"
            ],
            "primary_category": "cs.CR",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16538v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "系统性研究代码混淆对基于LLM的漏洞检测的影响，揭示其有效性边界",
            "summary_zh": "随着大型语言模型（LLM）越来越多地应用于代码漏洞检测，其在各种漏洞类型上的可靠性和鲁棒性日益受到关注。代码混淆作为一种绕过审计工具的通用策略，长期以来被用于传统的对抗环境中，它在不篡改工具本身的情况下保留了可利用性。现有的混淆方法和工具在支持的技术、粒度和编程语言方面存在差异，这使得系统性地评估它们对基于LLM的漏洞检测的影响变得困难。为了弥补这一差距，本文对混淆技术进行了结构化的系统化研究，并在统一的框架下评估了它们。具体来说，我们将现有的混淆方法分为三大类（布局、数据流和控制流），涵盖11个子类别和19个具体技术。我们使用一致的LLM驱动方法在四种编程语言（Solidity、C、C++和Python）中实现了这些技术，并评估了它们对跨越四个模型系列（DeepSeek、OpenAI、Qwen和LLaMA）的15个LLM以及两个编码代理（GitHub Copilot和Codex）的影响。我们的研究结果揭示了代码混淆对基于LLM的漏洞检测的积极和消极影响，突出了混淆导致性能提升或下降的条件。我们进一步分析了这些结果与漏洞特征、代码属性和模型属性的关系。最后，我们概述了几个开放性问题，并提出了未来方向，以增强LLM在实际漏洞检测中的鲁棒性。",
            "intro_zh": [
                "现有代码混淆工具在技术、粒度和语言支持上存在差异，缺乏统一框架评估其对LLM漏洞检测的影响。",
                "论文系统性地对代码混淆技术进行分类，并在统一框架下评估其对多种LLM漏洞检测的影响。",
                "实验揭示了代码混淆对LLM漏洞检测的正反两方面影响，并分析了影响因素，为提升LLM鲁棒性提供方向。"
            ],
            "method_zh": "**问题定义**：论文旨在解决现有代码混淆方法缺乏系统性评估，难以衡量其对基于LLM的漏洞检测工具的影响的问题。现有方法的痛点在于混淆技术种类繁多，缺乏统一的评估标准和框架，导致无法有效评估其对不同LLM模型和漏洞类型的影响。\\n\\n**核心思路**：论文的核心思路是对现有的代码混淆技术进行系统性的分类和整理，构建一个统一的评估框架，并在该框架下评估不同混淆技术对多种LLM漏洞检测工具的影响。通过这种方式，可以揭示不同混淆技术对LLM漏洞检测的有效性边界，并为提升LLM的鲁棒性提供指导。\\n\\n**技术框架**：论文构建的评估框架主要包含以下几个模块：1) 代码混淆技术分类模块：将现有的代码混淆技术分为布局、数据流和控制流三大类，并细分为11个子类别和19个具体技术。2) 代码混淆实现模块：在四种编程语言（Solidity、C、C++和Python）中实现这些混淆技术。3) LLM漏洞检测模块：使用15个LLM模型和2个编码代理进行漏洞检测。4) 评估模块：评估不同混淆技术对LLM漏洞检测的影响，并分析影响因素。\\n\\n**关键创新**：论文最重要的技术创新点在于对代码混淆技术进行了系统性的分类和整理，并构建了一个统一的评估框架。该框架可以用于评估不同混淆技术对多种LLM漏洞检测工具的影响，从而揭示不同混淆技术对LLM漏洞检测的有效性边界。与现有方法相比，该方法更加系统和全面，可以为提升LLM的鲁棒性提供更有效的指导。\\n\\n**关键设计**：论文的关键设计包括：1) 代码混淆技术的分类标准：基于布局、数据流和控制流三个维度进行分类。2) 评估指标：使用漏洞检测的准确率、召回率等指标来评估混淆技术的影响。3) 实验设置：选择具有代表性的LLM模型和编码代理，以及多种漏洞类型进行评估。",
            "application_zh": "该研究成果可应用于提升软件安全性和LLM的鲁棒性。开发者可以利用研究结果选择合适的混淆技术来保护代码，同时避免过度混淆导致LLM漏洞检测失效。研究结果也能帮助LLM开发者提升模型对混淆代码的识别能力，增强其在实际应用中的可靠性。",
            "highlight_zh": "实验结果表明，代码混淆对LLM漏洞检测的影响是双面的，某些混淆技术可以提升检测性能，而另一些则会降低性能。研究发现，混淆效果与漏洞特征、代码属性和模型属性密切相关。例如，某些混淆技术对特定类型的漏洞更有效，而某些LLM模型对特定混淆技术的抵抗能力更强。",
            "tags_zh": [
                "代码混淆",
                "漏洞检测",
                "大型语言模型",
                "LLM",
                "软件安全"
            ],
            "_index": 79,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "Feature-Selective Representation Misdirection for Machine Unlearning",
            "authors": [
                "Taozhao Chen",
                "Linghan Huang",
                "Kim-Kwang Raymond Choo",
                "Huaming Chen"
            ],
            "arxiv_id": "2512.16297v1",
            "summary": "As large language models (LLMs) are increasingly adopted in safety-critical and regulated sectors, the retention of sensitive or prohibited knowledge introduces escalating risks, ranging from privacy leakage to regulatory non-compliance to to potential misuse, and so on. Recent studies suggest that machine unlearning can help ensure deployed models comply with evolving legal, safety, and governance requirements. However, current unlearning techniques assume clean separation between forget and retain datasets, which is challenging in operational settings characterized by highly entangled distributions. In such scenarios, perturbation-based methods often degrade general model utility or fail to ensure safety. To address this, we propose Selective Representation Misdirection for Unlearning (SRMU), a novel principled activation-editing framework that enforces feature-aware and directionally controlled perturbations. Unlike indiscriminate model weights perturbations, SRMU employs a structured misdirection vector with an activation importance map. The goal is to allow SRMU selectively suppresses harmful representations while preserving the utility on benign ones. Experiments are conducted on the widely used WMDP benchmark across low- and high-entanglement configurations. Empirical results reveal that SRMU delivers state-of-the-art unlearning performance with minimal utility losses, and remains effective under 20-30\\% overlap where existing baselines collapse. SRMU provides a robust foundation for safety-driven model governance, privacy compliance, and controlled knowledge removal in the emerging LLM-based applications. We release the replication package at https://figshare.com/s/d5931192a8824de26aff.",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16297v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出选择性表征误导（SRMU）框架，用于解决高数据纠缠场景下的机器遗忘问题。",
            "summary_zh": "随着大型语言模型（LLMs）在安全关键和受监管领域的日益普及，模型中保留的敏感或违禁知识带来了不断升级的风险，包括隐私泄露、违反法规以及潜在的滥用等。近期的研究表明，机器遗忘有助于确保已部署的模型符合不断变化的法律、安全和治理要求。然而，现有的遗忘技术通常假设遗忘数据集和保留数据集之间存在清晰的分离，这在具有高度纠缠分布的实际操作环境中极具挑战性。在这种情况下，基于扰动的方法通常会降低模型的通用性或无法确保安全性。为了解决这个问题，我们提出了一种新颖的、基于原则的激活编辑框架——用于遗忘的选择性表征误导（SRMU），该框架强制执行特征感知和方向控制的扰动。与不加区分的模型权重扰动不同，SRMU采用带有激活重要性图的结构化误导向量。目标是允许SRMU选择性地抑制有害表征，同时保留对良性表征的效用。在广泛使用的WMDP基准上，针对低纠缠和高纠缠配置进行了实验。实验结果表明，SRMU提供了最先进的遗忘性能，同时最大限度地减少了效用损失，并且在现有基线崩溃的20-30%重叠下仍然有效。SRMU为安全驱动的模型治理、隐私合规性和新兴的基于LLM的应用中的受控知识移除提供了强大的基础。我们在https://figshare.com/s/d5931192a8824de26aff发布了复现包。",
            "intro_zh": [
                "现有机器遗忘方法在数据高度纠缠的情况下表现不佳，容易导致模型性能下降或遗忘失败。",
                "SRMU通过引入特征感知的、方向可控的扰动，选择性地抑制有害表征，同时保留对良性表征的效用。",
                "实验表明，SRMU在高数据纠缠场景下实现了最先进的遗忘性能，且对模型效用的影响最小。"
            ],
            "method_zh": "**问题定义**：论文旨在解决机器遗忘领域中，当遗忘数据和保留数据高度纠缠时，现有方法难以有效遗忘敏感信息且容易损害模型通用性的问题。现有方法通常假设数据之间存在清晰的分离，但在实际应用中，这种假设往往不成立，导致基于扰动的方法要么无法有效遗忘，要么过度扰动模型权重，降低模型性能。\\n\\n**核心思路**：SRMU的核心思路是通过选择性地“误导”模型内部的表征，使其不再包含或弱化与遗忘数据相关的特征，同时尽可能保留与保留数据相关的特征。这种“误导”不是随机的，而是基于特征重要性和方向控制的，从而实现更精确的遗忘效果，并减少对模型整体性能的影响。\\n\\n**技术框架**：SRMU框架主要包含以下几个阶段：1) **激活重要性图生成**：评估模型每一层激活对遗忘任务的重要性。2) **结构化误导向量构建**：基于激活重要性图，构建一个具有方向性的扰动向量，用于“误导”模型的表征。3) **激活编辑**：将误导向量应用到模型的激活层，从而选择性地抑制有害表征。4) **模型微调**：对模型进行微调，以恢复因激活编辑而损失的性能。\\n\\n**关键创新**：SRMU的关键创新在于其选择性和方向性的扰动方式。与传统的权重扰动方法不同，SRMU不是直接修改模型权重，而是通过修改激活层来实现遗忘。此外，SRMU还引入了激活重要性图，用于指导扰动的方向和强度，从而实现更精确的遗忘效果。\\n\\n**关键设计**：SRMU的关键设计包括：1) **激活重要性评估方法**：论文可能采用梯度信息或其他方法来评估激活对遗忘任务的重要性。2) **误导向量的构建方式**：误导向量需要能够有效地抑制有害表征，同时尽可能保留良性表征。3) **激活编辑的具体操作**：如何将误导向量应用到激活层，以及如何控制扰动的强度。4) **微调策略**：如何通过微调来恢复模型性能，同时避免重新学习到遗忘的信息。",
            "application_zh": "SRMU技术可应用于各种需要进行安全驱动的模型治理、隐私合规性和受控知识移除的场景，例如：大型语言模型在医疗、金融等敏感领域的应用；在线教育平台中对不适宜内容的过滤；以及企业内部知识库中对过期或错误信息的移除。该技术有助于确保AI系统在满足合规性要求的同时，保持良好的性能和可靠性。",
            "highlight_zh": "SRMU在WMDP基准测试中取得了最先进的遗忘性能，尤其是在高数据纠缠的情况下，显著优于现有基线方法。即使在遗忘数据和保留数据存在20-30%的重叠时，SRMU仍然能够有效遗忘，而现有方法已经失效。此外，SRMU在实现有效遗忘的同时，对模型效用的影响也最小。",
            "tags_zh": [
                "机器遗忘",
                "表征学习",
                "激活编辑",
                "数据隐私",
                "大型语言模型"
            ],
            "_index": 80,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16297v1/img/overviewSRMU.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16297v1/img/combinationablation.png",
                    "caption": "",
                    "figure_id": "fig_1"
                }
            ]
        },
        {
            "title": "In-Context Probing for Membership Inference in Fine-Tuned Language Models",
            "authors": [
                "Zhexi Lu",
                "Hongliang Chi",
                "Nathalie Baracaldo",
                "Swanand Ravindra Kadhe",
                "Yuseok Jeon",
                "Lei Yu"
            ],
            "arxiv_id": "2512.16292v1",
            "summary": "Membership inference attacks (MIAs) pose a critical privacy threat to fine-tuned large language models (LLMs), especially when models are adapted to domain-specific tasks using sensitive data. While prior black-box MIA techniques rely on confidence scores or token likelihoods, these signals are often entangled with a sample's intrinsic properties - such as content difficulty or rarity - leading to poor generalization and low signal-to-noise ratios. In this paper, we propose ICP-MIA, a novel MIA framework grounded in the theory of training dynamics, particularly the phenomenon of diminishing returns during optimization. We introduce the Optimization Gap as a fundamental signal of membership: at convergence, member samples exhibit minimal remaining loss-reduction potential, while non-members retain significant potential for further optimization. To estimate this gap in a black-box setting, we propose In-Context Probing (ICP), a training-free method that simulates fine-tuning-like behavior via strategically constructed input contexts. We propose two probing strategies: reference-data-based (using semantically similar public samples) and self-perturbation (via masking or generation). Experiments on three tasks and multiple LLMs show that ICP-MIA significantly outperforms prior black-box MIAs, particularly at low false positive rates. We further analyze how reference data alignment, model type, PEFT configurations, and training schedules affect attack effectiveness. Our findings establish ICP-MIA as a practical and theoretically grounded framework for auditing privacy risks in deployed LLMs.",
            "categories": [
                "cs.CR",
                "cs.LG"
            ],
            "primary_category": "cs.CR",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16292v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出ICP-MIA框架，通过上下文探查解决微调语言模型的成员推断攻击问题。",
            "summary_zh": "成员推断攻击(MIAs)对微调的大型语言模型(LLMs)构成严重的隐私威胁，尤其是在模型使用敏感数据进行特定领域任务适配时。现有的黑盒MIA技术依赖于置信度分数或token似然，但这些信号通常与样本的固有属性（如内容难度或稀有性）纠缠在一起，导致泛化能力差和信噪比低。本文提出了ICP-MIA，一个基于训练动态理论（特别是优化过程中的收益递减现象）的新型MIA框架。我们将优化差距作为成员的基本信号：在收敛时，成员样本表现出最小的剩余损失减少潜力，而非成员样本保留了进一步优化的显著潜力。为了在黑盒设置中估计这个差距，我们提出了上下文探查(ICP)，一种通过策略性构建的输入上下文来模拟微调行为的免训练方法。我们提出了两种探查策略：基于参考数据的探查（使用语义相似的公共样本）和自扰动（通过掩码或生成）。在三个任务和多个LLM上的实验表明，ICP-MIA显著优于先前的黑盒MIA，尤其是在低假阳性率下。我们进一步分析了参考数据对齐、模型类型、PEFT配置和训练计划如何影响攻击效果。我们的发现将ICP-MIA确立为一个实用且理论上合理的框架，用于审计已部署LLM中的隐私风险。",
            "intro_zh": [
                "现有黑盒成员推断攻击依赖置信度等信号，易受样本固有属性干扰，泛化性差。",
                "提出ICP-MIA框架，利用优化差距作为成员信号，通过上下文探查模拟微调行为。",
                "实验表明，ICP-MIA显著优于现有黑盒MIA，尤其在低假阳性率下，验证了有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决微调语言模型中，现有成员推断攻击（MIA）方法效果不佳的问题。现有方法依赖于模型输出的置信度或token似然等信号，但这些信号容易受到样本自身属性（如难度、稀有度）的干扰，导致攻击的泛化能力较差，信噪比低。因此，如何更准确地判断一个样本是否为训练集成员，是本研究要解决的核心问题。\\n\\n**核心思路**：论文的核心思路是利用训练动态中的“收益递减”现象。作者认为，在模型训练收敛后，训练集中的样本（成员）已经得到了充分学习，其损失减少的潜力很小；而未见过的样本（非成员）则仍有较大的优化空间。因此，可以通过估计样本的“优化差距”（Optimization Gap）来判断其是否为成员。为了在黑盒场景下估计这个差距，论文提出了“上下文探查”（In-Context Probing, ICP）方法，通过构造特定的输入上下文来模拟微调过程，从而评估样本的优化潜力。\\n\\n**技术框架**：ICP-MIA框架主要包含以下几个步骤：1) **选择目标样本**：选择需要进行成员推断的样本。2) **构建上下文**：使用ICP方法构建输入上下文，模拟微调过程。论文提出了两种上下文构建策略：基于参考数据的探查和自扰动。3) **损失计算**：计算在原始模型和经过上下文探查后的模型上的损失，得到优化差距。4) **成员推断**：根据优化差距的大小，判断样本是否为训练集成员。优化差距小的样本被认为是成员，优化差距大的样本被认为是非成员。\\n\\n**关键创新**：该论文最重要的创新点在于提出了基于“优化差距”的成员推断信号，并设计了“上下文探查”方法来在黑盒场景下估计这个差距。与以往依赖置信度等信号的方法不同，该方法直接从训练动态的角度出发，更准确地反映了样本与模型之间的关系。此外，ICP方法无需训练，降低了攻击的成本和复杂性。\\n\\n**关键设计**：在上下文探查方面，论文提出了两种关键的设计：1) **基于参考数据的探查**：利用与目标样本语义相似的公共数据作为参考，构建上下文。关键在于如何选择合适的参考数据，论文可能使用了语义相似度计算等方法。2) **自扰动**：通过对目标样本进行掩码或生成等操作，生成扰动后的样本，并将其作为上下文。关键在于如何设计扰动策略，以保证扰动后的样本仍然具有一定的语义信息，并且能够反映样本的优化潜力。论文可能使用了特定的掩码策略或生成模型。",
            "application_zh": "该研究成果可应用于评估和增强已部署大型语言模型的隐私安全性。通过ICP-MIA框架，可以审计模型是否存在过度记忆训练数据的风险，从而指导模型开发者采取相应的隐私保护措施，例如差分隐私训练、数据增强等，以降低模型泄露敏感信息的可能性。该研究对于保护用户隐私、促进LLM安全应用具有重要意义。",
            "highlight_zh": "实验结果表明，ICP-MIA在三个任务和多个LLM上显著优于先前的黑盒MIA方法，尤其是在低假阳性率下。这意味着在保证较低误报率的前提下，ICP-MIA能够更准确地识别出训练集成员。具体的性能提升数据（例如AUC、准确率等）以及对比的基线方法需要在论文中查找。",
            "tags_zh": [
                "成员推断攻击",
                "大型语言模型",
                "隐私安全",
                "上下文学习",
                "优化差距"
            ],
            "_index": 81,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16292v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16292v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16292v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "CKA-Guided Modular Quantization: Beyond Bit-Width to Algorithmic Diversity",
            "authors": [
                "Jinhao Zhang",
                "Yunquan Zhang",
                "Daning Chen"
            ],
            "arxiv_id": "2512.16282v1",
            "summary": "Current mainstream post-training quantization methods for large language models typically apply a uniform quantization strategy across all network layers, overlooking the substantial differences in algorithmic suitability among layers. To address this limitation, we propose CKA Guided Modular Quantization, a fine-tuning-free, plug-and-play framework for algorithmic heterogeneous quantization. Our method independently evaluates multiple PTQ algorithms on each layer and employs Linear Centered Kernel Alignment (CKA) as a metric to automatically select the optimal quantization strategy per layer. The individually optimized strategies are then integrated to construct a hybrid quantized model. Experiments demonstrate that our approach consistently outperforms both uniform quantization baselines and state-of-the-art mixed-precision methods across mainstream LLMs including LLaMA and Qwen ,in terms of perplexity (PPL) and downstream task performance.",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16282v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出CKA引导的模块化量化方法，实现大语言模型层间算法多样性量化",
            "summary_zh": "当前主流的大语言模型后训练量化方法通常在所有网络层应用统一的量化策略，忽略了层间算法适用性的显著差异。为了解决这一局限，我们提出了CKA引导的模块化量化方法，这是一个无需微调、即插即用的算法异构量化框架。我们的方法独立评估每个层上的多个PTQ算法，并采用线性中心核对齐（CKA）作为指标来自动选择每个层的最佳量化策略。然后，将单独优化的策略集成以构建混合量化模型。实验表明，在包括LLaMA和Qwen在内的主流LLM上，我们的方法在困惑度（PPL）和下游任务性能方面始终优于统一量化基线和最先进的混合精度方法。",
            "intro_zh": [
                "现有大语言模型量化方法忽略了不同层对量化算法的适应性差异，导致性能瓶颈。",
                "提出CKA引导的模块化量化方法，通过CKA指标为每一层选择最优量化算法。",
                "实验结果表明，该方法在LLaMA和Qwen等模型上，显著提升了量化模型的性能。"
            ],
            "method_zh": "**问题定义**：现有大语言模型的后训练量化（PTQ）方法通常采用统一的量化策略，即对所有层使用相同的量化算法和比特宽度。然而，不同层对量化的敏感度不同，统一量化策略无法充分利用每一层的特性，导致量化后的模型性能下降。因此，需要一种能够根据每一层的特点选择最佳量化策略的方法。\\n\\n**核心思路**：论文的核心思路是为每一层独立评估多个PTQ算法，并选择最适合该层的算法。为了衡量不同量化算法对每一层的影响，论文使用线性中心核对齐（CKA）作为指标。CKA能够衡量不同表示之间的相似性，因此可以用来评估量化后的表示与原始表示的相似程度。选择CKA值最高的量化算法作为该层的最佳策略。\\n\\n**技术框架**：该方法是一个无需微调的、即插即用的框架，主要包含以下几个阶段：1) 对每一层，尝试不同的PTQ算法（例如，不同的比特宽度、不同的量化方案）。2) 使用CKA指标评估每种量化算法对该层的影响。具体来说，计算原始模型在该层的输出表示，以及量化模型在该层的输出表示，然后计算这两个表示之间的CKA值。3) 选择CKA值最高的量化算法作为该层的最佳策略。4) 将所有层的最佳策略组合起来，构建一个混合量化模型。\\n\\n**关键创新**：该方法最重要的创新点在于使用CKA指标来自动选择每一层的最佳量化策略。与传统的混合精度量化方法相比，该方法不需要手动调整每一层的比特宽度，而是通过CKA指标自动搜索最佳的量化配置。此外，该方法不仅限于选择不同的比特宽度，还可以选择不同的量化算法，从而实现更灵活的量化策略。\\n\\n**关键设计**：CKA的计算是关键。论文使用线性CKA，其计算效率较高。此外，PTQ算法的选择也很重要，需要选择足够多的PTQ算法，才能保证能够找到每一层的最佳策略。论文中没有明确说明具体的PTQ算法选择，这部分属于可配置的超参数。",
            "application_zh": "该研究成果可广泛应用于大语言模型的部署和推理加速，尤其是在资源受限的边缘设备上。通过自动选择每一层的最佳量化策略，可以显著降低模型的存储空间和计算复杂度，同时保持较高的模型性能。这有助于推动大语言模型在移动设备、嵌入式系统等领域的应用。",
            "highlight_zh": "实验结果表明，该方法在LLaMA和Qwen等主流大语言模型上，显著优于统一量化基线和最先进的混合精度方法。具体来说，在困惑度（PPL）和下游任务性能方面均取得了提升，验证了该方法的有效性。但具体提升幅度未在摘要中体现。",
            "tags_zh": [
                "大语言模型",
                "后训练量化",
                "异构量化",
                "线性中心核对齐",
                "模型压缩"
            ],
            "_index": 82,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16282v1/figure6.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16282v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16282v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Staggered Batch Scheduling: Co-optimizing Time-to-First-Token and Throughput for High-Efficiency LLM Inference",
            "authors": [
                "Jian Tian",
                "Shuailong Li",
                "Yang Cao",
                "Wenbo Cui",
                "Minghan Zhu",
                "Wenkang Wu",
                "Jianming Zhang",
                "Yanpeng Wang",
                "Zhiwen Xiao",
                "Zhenyu Hou",
                "Dou Shen"
            ],
            "arxiv_id": "2512.16134v1",
            "summary": "The evolution of Large Language Model (LLM) serving towards complex, distributed architectures--specifically the P/D-separated, large-scale DP+EP paradigm--introduces distinct scheduling challenges. Unlike traditional deployments where schedulers can treat instances as black boxes, DP+EP architectures exhibit high internal synchronization costs. We identify that immediate request dispatching in such systems leads to severe in-engine queuing and parallelization bubbles, degrading Time-to-First-Token (TTFT). To address this, we propose Staggered Batch Scheduling (SBS), a mechanism that deliberately buffers requests to form optimal execution batches. This temporal decoupling eliminates internal queuing bubbles without compromising throughput. Furthermore, leveraging the scheduling window created by buffering, we introduce a Load-Aware Global Allocation strategy that balances computational load across DP units for both Prefill and Decode phases. Deployed on a production H800 cluster serving Deepseek-V3, our system reduces TTFT by 30%-40% and improves throughput by 15%-20% compared to state-of-the-art immediate scheduling baselines.",
            "categories": [
                "cs.DC",
                "cs.LG"
            ],
            "primary_category": "cs.DC",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16134v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出交错批调度(SBS)，优化DP+EP架构下LLM推理的首Token延迟和吞吐量。",
            "summary_zh": "大型语言模型（LLM）服务正朝着复杂的分布式架构发展，特别是P/D分离的大规模DP+EP范式，这带来了独特的调度挑战。与传统部署中调度器将实例视为黑盒不同，DP+EP架构表现出较高的内部同步成本。我们发现，在此类系统中立即分发请求会导致严重的引擎内排队和并行化气泡，从而降低首Token延迟（TTFT）。为了解决这个问题，我们提出了交错批调度（SBS），这是一种有意缓冲请求以形成最佳执行批次的机制。这种时间解耦消除了内部排队气泡，而不会影响吞吐量。此外，利用缓冲创建的调度窗口，我们引入了一种负载感知全局分配策略，用于平衡Prefill和Decode阶段跨DP单元的计算负载。在服务Deepseek-V3的生产H800集群上部署后，与最先进的即时调度基线相比，我们的系统将TTFT降低了30%-40%，并将吞吐量提高了15%-20%。",
            "intro_zh": [
                "DP+EP架构的LLM服务中，立即调度请求导致引擎内排队和并行化气泡，严重影响首Token延迟。",
                "提出交错批调度（SBS），通过缓冲请求形成最佳批次，消除内部排队，并利用调度窗口进行负载均衡。",
                "在生产H800集群上，SBS将Deepseek-V3的TTFT降低30%-40%，吞吐量提升15%-20%。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大规模分布式LLM服务中，特别是采用DP+EP架构时，由于内部同步成本高昂，导致的首Token延迟（TTFT）过高以及吞吐量受限的问题。现有方法通常采用立即调度策略，忽略了DP+EP架构的特殊性，导致引擎内部出现排队和并行化气泡，降低了效率。\\n\\n**核心思路**：论文的核心思路是引入时间上的解耦，通过交错批调度（SBS）机制，有意缓冲请求，形成更优的执行批次。这样可以避免立即调度带来的内部排队问题，同时利用缓冲窗口，进行全局的负载感知分配，平衡不同DP单元的计算负载。\\n\\n**技术框架**：SBS包含两个主要部分：请求缓冲和负载感知全局分配。首先，请求会被缓冲一段时间，形成批次。然后，利用缓冲窗口，采用负载感知的全局分配策略，将批次分配到不同的DP单元上，以平衡计算负载。整个流程旨在最小化TTFT，同时最大化吞吐量。\\n\\n**关键创新**：论文的关键创新在于将调度策略从“立即调度”转变为“交错批调度”，并结合负载感知的全局分配。这种方法充分考虑了DP+EP架构的内部同步成本，通过时间上的解耦，避免了内部排队和并行化气泡。与现有方法相比，SBS能够更有效地利用计算资源，提高LLM服务的效率。\\n\\n**关键设计**：SBS的关键设计包括缓冲窗口的大小、批次形成策略以及负载感知的全局分配算法。缓冲窗口的大小需要根据实际的系统负载和请求到达率进行调整，以平衡TTFT和吞吐量。负载感知的全局分配算法需要考虑不同DP单元的计算能力和当前负载，以实现最佳的负载均衡。具体的损失函数和网络结构（如果涉及）在论文中未明确说明，属于未知细节。",
            "application_zh": "该研究成果可应用于大规模分布式LLM服务，特别是在需要高并发和低延迟的场景下，如在线客服、智能助手、实时翻译等。通过优化调度策略，可以显著提升LLM服务的用户体验和资源利用率，降低运营成本，并为更复杂的LLM应用提供支持。",
            "highlight_zh": "实验结果表明，在生产H800集群上，与最先进的即时调度基线相比，SBS将Deepseek-V3的TTFT降低了30%-40%，并将吞吐量提高了15%-20%。这些数据表明SBS在实际应用中具有显著的性能优势，能够有效提升LLM服务的效率。",
            "tags_zh": [
                "大型语言模型",
                "分布式推理",
                "调度算法",
                "首Token延迟",
                "吞吐量优化"
            ],
            "_index": 83,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16134v1/figures/schedule_unit_1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16134v1/figures/schedule_unit_2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16134v1/figures/queue_1.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Scaling Text2SQL via LLM-efficient Schema Filtering with Functional Dependency Graph Rerankers",
            "authors": [
                "Thanh Dat Hoang",
                "Thanh Tam Nguyen",
                "Thanh Trung Huynh",
                "Hongzhi Yin",
                "Quoc Viet Hung Nguyen"
            ],
            "arxiv_id": "2512.16083v1",
            "summary": "Most modern Text2SQL systems prompt large language models (LLMs) with entire schemas -- mostly column information -- alongside the user's question. While effective on small databases, this approach fails on real-world schemas that exceed LLM context limits, even for commercial models. The recent Spider 2.0 benchmark exemplifies this with hundreds of tables and tens of thousands of columns, where existing systems often break. Current mitigations either rely on costly multi-step prompting pipelines or filter columns by ranking them against user's question independently, ignoring inter-column structure. To scale existing systems, we introduce \\toolname, an open-source, LLM-efficient schema filtering framework that compacts Text2SQL prompts by (i) ranking columns with a query-aware LLM encoder enriched with values and metadata, (ii) reranking inter-connected columns via a lightweight graph transformer over functional dependencies, and (iii) selecting a connectivity-preserving sub-schema with a Steiner-tree heuristic. Experiments on real datasets show that \\toolname achieves near-perfect recall and higher precision than CodeS, SchemaExP, Qwen rerankers, and embedding retrievers, while maintaining sub-second median latency and scaling to schemas with 23,000+ columns. Our source code is available at https://github.com/thanhdath/grast-sql.",
            "categories": [
                "cs.DB",
                "cs.AI",
                "cs.HC",
                "cs.LG"
            ],
            "primary_category": "cs.DB",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16083v1",
            "code_links": [
                {
                    "url": "https://github.com/thanhdath/grast-sql",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出GRaST，通过LLM高效的模式过滤和函数依赖图重排序，扩展Text2SQL到大规模数据库。",
            "summary_zh": "大多数现代Text2SQL系统在提示大型语言模型(LLM)时，会将整个模式（主要是列信息）与用户的问题一起提供。虽然这种方法在小型数据库上有效，但对于超过LLM上下文限制的真实模式则会失效，即使是商业模型也是如此。最近的Spider 2.0基准测试就是一个例子，它包含数百个表和数万列，现有系统经常崩溃。目前的缓解措施要么依赖于代价高昂的多步骤提示流程，要么通过独立于用户问题对列进行排序来过滤列，忽略了列间的结构。为了扩展现有系统，我们引入了GRaST，这是一个开源的、LLM高效的模式过滤框架，它通过以下方式压缩Text2SQL提示：(i) 使用一个查询感知的LLM编码器（富含值和元数据）对列进行排序，(ii) 通过函数依赖关系上的轻量级图Transformer对相互连接的列进行重排序，以及(iii) 使用Steiner树启发式算法选择一个保持连通性的子模式。在真实数据集上的实验表明，GRaST实现了接近完美的召回率和比CodeS、SchemaExP、Qwen重排序器和嵌入检索器更高的精度，同时保持了亚秒级的中值延迟，并可扩展到具有23,000+列的模式。我们的源代码可在https://github.com/thanhdath/grast-sql 获得。",
            "intro_zh": [
                "现有Text2SQL系统在处理大规模数据库时，由于LLM上下文长度限制，无法有效利用完整数据库模式信息。",
                "GRaST框架通过查询感知的LLM编码器、函数依赖图Transformer和Steiner树启发式算法，实现高效的模式过滤和子模式选择。",
                "实验表明，GRaST在保证高召回率的同时，提高了Text2SQL的精度，并能扩展到包含数万列的数据库模式。"
            ],
            "method_zh": "**问题定义**：现有Text2SQL系统在处理大规模数据库时，需要将整个数据库模式信息输入LLM，导致超出LLM的上下文长度限制，影响性能甚至导致系统崩溃。现有的缓解方法，如多步提示或独立列过滤，要么成本高昂，要么忽略了列之间的关联性，导致精度下降。\\n\\n**核心思路**：GRaST的核心思路是在保证召回率的前提下，尽可能减少输入LLM的模式信息量，同时考虑列之间的关联性。通过查询感知的LLM编码器对列进行排序，利用函数依赖图Transformer对相关列进行重排序，并使用Steiner树启发式算法选择一个连通的子模式，从而实现高效的模式过滤。\\n\\n**技术框架**：GRaST框架包含三个主要阶段：1) 列排序：使用查询感知的LLM编码器对数据库中的列进行排序，该编码器利用列的值和元数据信息。2) 列重排序：利用函数依赖图Transformer对相互连接的列进行重排序，考虑列之间的依赖关系。3) 子模式选择：使用Steiner树启发式算法选择一个保持连通性的子模式，作为LLM的输入。\\n\\n**关键创新**：GRaST的关键创新在于结合了查询感知的LLM编码器、函数依赖图Transformer和Steiner树启发式算法，实现了一种高效且精确的模式过滤方法。与现有方法相比，GRaST不仅考虑了列与查询的相关性，还考虑了列之间的依赖关系，从而提高了Text2SQL的精度。\\n\\n**关键设计**：查询感知的LLM编码器使用预训练的语言模型（如BERT）对查询和列信息进行编码，并使用注意力机制计算查询与列之间的相关性。函数依赖图Transformer使用图神经网络对函数依赖图进行编码，并学习列之间的依赖关系。Steiner树启发式算法用于在函数依赖图中选择一个包含所有相关列的最小连通子图。",
            "application_zh": "GRaST框架可应用于各种需要处理大规模数据库的Text2SQL场景，例如金融、医疗、电商等领域。它可以帮助用户更方便地通过自然语言查询数据库，提高数据分析和决策效率。未来，该框架可以进一步扩展到支持更复杂的SQL查询和更广泛的数据库类型。",
            "highlight_zh": "在真实数据集上的实验表明，GRaST实现了接近完美的召回率，并显著提高了Text2SQL的精度。与CodeS、SchemaExP、Qwen重排序器和嵌入检索器等基线方法相比，GRaST在精度上取得了明显的优势，同时保持了亚秒级的中值延迟，并可扩展到具有23,000+列的模式。",
            "tags_zh": [
                "Text2SQL",
                "大型语言模型",
                "模式过滤",
                "函数依赖图",
                "图神经网络",
                "Steiner树",
                "数据库查询"
            ],
            "_index": 84,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "From Facts to Conclusions : Integrating Deductive Reasoning in Retrieval-Augmented LLMs",
            "authors": [
                "Shubham Mishra",
                "Samyek Jain",
                "Gorang Mehrishi",
                "Shiv Tiwari",
                "Harsh Sharma",
                "Pratik Narang",
                "Dhruv Kumar"
            ],
            "arxiv_id": "2512.16795v1",
            "summary": "Retrieval-Augmented Generation (RAG) grounds large language models (LLMs) in external evidence, but fails when retrieved sources conflict or contain outdated or subjective information. Prior work address these issues independently but lack unified reasoning supervision. We propose a reasoning-trace-augmented RAG framework that adds structured, interpretable reasoning across three stages : (1) document-level adjudication, (2) conflict analysis, and (3) grounded synthesis, producing citation-linked answers or justified refusals. A Conflict-Aware Trust-Score (CATS) pipeline is introduced which evaluates groundedness, factual correctness, refusal accuracy, and conflict-behavior alignment using an LLM-as-a-Judge. Our 539-query reasoning dataset and evaluation pipeline establish a foundation for conflict-aware, interpretable RAG systems. Experimental results demonstrate substantial gains over baselines, most notably with Qwen, where Supervised Fine-Tuning improved End-to-End answer correctness from 0.069 to 0.883 and behavioral adherence from 0.074 to 0.722.",
            "categories": [
                "cs.CL",
                "cs.AI",
                "cs.CY",
                "cs.IR"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Under Review",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16795v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出推理追踪增强的RAG框架，解决检索信息冲突和主观性问题。",
            "summary_zh": "检索增强生成(RAG)使大型语言模型(LLM)能够基于外部证据，但当检索到的信息存在冲突、过时或主观性时会失效。现有工作分别解决这些问题，但缺乏统一的推理监督。本文提出了一种推理追踪增强的RAG框架，该框架在三个阶段添加了结构化、可解释的推理：(1)文档级裁决，(2)冲突分析，(3)基于证据的综合，从而生成带有引用的答案或合理的拒绝。引入了一种冲突感知信任评分(CATS)流程，该流程使用LLM作为评判者来评估证据充分性、事实正确性、拒绝准确性和冲突行为一致性。本文构建了一个包含539个查询的推理数据集和评估流程，为冲突感知、可解释的RAG系统奠定了基础。实验结果表明，该方法优于基线，尤其是在Qwen模型上，有监督微调将端到端答案正确率从0.069提高到0.883，行为一致性从0.074提高到0.722。",
            "intro_zh": [
                "现有RAG方法在处理冲突、过时或主观信息时表现不佳，缺乏统一的推理监督。",
                "提出推理追踪增强的RAG框架，通过文档裁决、冲突分析和证据综合实现可解释推理。",
                "实验结果表明，该方法在答案正确性和行为一致性方面均优于基线，尤其是在Qwen模型上。"
            ],
            "method_zh": "**问题定义**：现有RAG模型在面对检索到的信息冲突、包含过时信息或主观信息时，无法有效判断和整合信息，导致生成错误或不准确的答案。现有的解决方案通常独立地解决这些问题，缺乏一个统一的框架来处理这些挑战，并且缺乏可解释的推理过程。\n\n**核心思路**：本文的核心思路是通过引入一个结构化的推理过程，对检索到的文档进行裁决、冲突分析和证据综合，从而生成更准确、更可靠的答案。该方法旨在使RAG模型能够识别和解决信息冲突，并基于可靠的证据生成答案或拒绝回答。\n\n**技术框架**：该框架包含三个主要阶段：(1)文档级裁决：评估每个检索到的文档的可靠性和相关性，以确定其是否值得信任。(2)冲突分析：识别检索到的文档之间的冲突，并确定哪些信息是相互矛盾的。(3)基于证据的综合：基于裁决和冲突分析的结果，综合来自不同来源的信息，生成一个一致且准确的答案或拒绝回答。此外，还引入了冲突感知信任评分(CATS)流程，用于评估模型的性能。\n\n**关键创新**：该方法的主要创新在于引入了一个结构化的推理过程，使RAG模型能够更好地处理信息冲突和主观性。通过文档裁决、冲突分析和证据综合，该方法能够生成更可靠、更可信的答案。此外，CATS流程提供了一种评估RAG模型在处理冲突信息时的性能的有效方法。\n\n**关键设计**：框架的关键设计包括：(1)文档裁决模块，使用LLM评估文档的可靠性。(2)冲突分析模块，识别文档之间的矛盾之处。(3)证据综合模块，基于裁决和冲突分析的结果生成答案。CATS流程使用LLM作为评判者，评估生成答案的证据充分性、事实正确性、拒绝准确性和冲突行为一致性。具体的参数设置、损失函数和网络结构等细节未在摘要中详细说明，属于未知信息。",
            "application_zh": "该研究成果可应用于需要处理大量信息并从中提取准确、可靠答案的领域，如智能客服、金融分析、法律咨询和医疗诊断等。通过提高RAG模型处理冲突信息的能力，可以提升信息检索和问答系统的准确性和可信度，减少错误信息的传播，并为用户提供更可靠的信息服务。",
            "highlight_zh": "实验结果表明，该方法在Qwen模型上表现出色，通过有监督微调，端到端答案正确率从0.069显著提高到0.883，行为一致性从0.074提高到0.722。这些结果表明，该方法能够有效地提高RAG模型处理冲突信息的能力，并生成更准确、更可靠的答案。",
            "tags_zh": [
                "检索增强生成",
                "RAG",
                "冲突分析",
                "推理追踪",
                "可解释性",
                "大型语言模型",
                "LLM",
                "信息裁决"
            ],
            "_index": 85,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "Plausibility as Failure: How LLMs and Humans Co-Construct Epistemic Error",
            "authors": [
                "Claudia Vale Oliveira",
                "Nelson Zagalo",
                "Filipe Silva",
                "Anabela Brandao",
                "Syeda Faryal Hussain Khurrum",
                "Joaquim Santos"
            ],
            "arxiv_id": "2512.16750v1",
            "summary": "Large language models (LLMs) are increasingly used as epistemic partners in everyday reasoning, yet their errors remain predominantly analyzed through predictive metrics rather than through their interpretive effects on human judgment. This study examines how different forms of epistemic failure emerge, are masked, and are tolerated in human AI interaction, where failure is understood as a relational breakdown shaped by model-generated plausibility and human interpretive judgment. We conducted a three round, multi LLM evaluation using interdisciplinary tasks and progressively differentiated assessment frameworks to observe how evaluators interpret model responses across linguistic, epistemic, and credibility dimensions. Our findings show that LLM errors shift from predictive to hermeneutic forms, where linguistic fluency, structural coherence, and superficially plausible citations conceal deeper distortions of meaning. Evaluators frequently conflated criteria such as correctness, relevance, bias, groundedness, and consistency, indicating that human judgment collapses analytical distinctions into intuitive heuristics shaped by form and fluency. Across rounds, we observed a systematic verification burden and cognitive drift. As tasks became denser, evaluators increasingly relied on surface cues, allowing erroneous yet well formed answers to pass as credible. These results suggest that error is not solely a property of model behavior but a co-constructed outcome of generative plausibility and human interpretive shortcuts. Understanding AI epistemic failure therefore requires reframing evaluation as a relational interpretive process, where the boundary between system failure and human miscalibration becomes porous. The study provides implications for LLM assessment, digital literacy, and the design of trustworthy human AI communication.",
            "categories": [
                "cs.HC",
                "cs.AI"
            ],
            "primary_category": "cs.HC",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "19 pages, 2 tables, 77 references, 6 appendices",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16750v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "揭示LLM与人类交互中认知错误的共建机制，强调评估的解释性视角",
            "summary_zh": "大型语言模型（LLM）日益成为日常推理中的认知伙伴，但对其错误的分析主要集中在预测指标上，而非其对人类判断的解释性影响。本研究考察了在人机交互中，不同形式的认知失败如何产生、被掩盖和被容忍。这里的失败被理解为一种关系破裂，由模型生成的可信度和人类的解释性判断共同塑造。我们进行了三轮多LLM评估，使用跨学科任务和逐步区分的评估框架，观察评估者如何解释模型在语言、认知和可信度维度上的响应。研究结果表明，LLM的错误从预测形式转变为解释形式，其中语言流畅性、结构连贯性和表面上可信的引用掩盖了更深层次的意义扭曲。评估者经常混淆正确性、相关性、偏差、依据性和一致性等标准，表明人类判断将分析区分简化为受形式和流畅性影响的直觉启发式。在整个过程中，我们观察到系统的验证负担和认知漂移。随着任务变得更加密集，评估者越来越依赖表面线索，允许错误但形式良好的答案被认为是可信的。这些结果表明，错误不仅仅是模型行为的属性，而是生成可信度和人类解释性捷径共同构建的结果。因此，理解AI的认知失败需要将评估重新定义为一个关系解释过程，其中系统失败和人类校准错误之间的界限变得模糊。该研究为LLM评估、数字素养和可信的人机通信设计提供了启示。",
            "intro_zh": [
                "现有LLM错误分析侧重于预测指标，忽略了其对人类判断的解释性影响，导致对人机交互中认知错误的理解不足。",
                "该研究将LLM错误视为一种关系破裂，由模型生成的可信度和人类的解释性判断共同塑造，强调评估的解释性视角。",
                "通过多轮评估发现，LLM错误会从预测形式转变为解释形式，且人类评估易受表面线索影响，导致认知漂移。"
            ],
            "method_zh": "**问题定义**：现有的大型语言模型（LLM）的错误评估主要集中在预测准确性等指标上，忽略了LLM的输出如何影响人类的理解和判断。这种片面的评估方式无法充分揭示人机交互中认知错误的复杂性，尤其是在LLM生成看似合理但实际上错误的答案时，人类可能会受到误导。因此，需要更深入地理解LLM的错误如何被人类感知和解释，以及这种交互如何共同构建认知错误。\\n\\n**核心思路**：该研究的核心思路是将LLM的错误视为一种“关系破裂”，即LLM的输出与人类的理解之间出现偏差。这种偏差并非仅仅由LLM的预测错误引起，而是由LLM生成内容的可信度（plausibility）和人类的解释性判断共同塑造。通过考察人类如何解释和容忍LLM的错误，可以更好地理解人机交互中认知错误的产生机制。\\n\\n**技术框架**：该研究采用了一个三轮的多LLM评估框架。第一轮使用跨学科任务，旨在识别不同类型的LLM错误。第二轮和第三轮逐步细化评估框架，更加关注评估者如何解释LLM的响应，并考察语言、认知和可信度等维度。评估者需要对LLM的输出进行多方面的评估，包括正确性、相关性、偏差、依据性和一致性等。研究者通过分析评估者的反馈，揭示人类判断中的认知偏差和启发式方法。\\n\\n**关键创新**：该研究的关键创新在于将LLM的错误评估从单纯的预测准确性转向了对人机交互的解释性分析。它强调了LLM生成内容的可信度对人类判断的影响，并揭示了人类在评估LLM输出时存在的认知偏差。这种新的评估视角有助于更全面地理解人机交互中认知错误的产生机制。\\n\\n**关键设计**：在评估过程中，研究者设计了跨学科的任务，以考察LLM在不同领域的表现。同时，他们逐步细化评估框架，更加关注评估者对LLM输出的解释。此外，研究者还采用了多种LLM，以考察不同模型的错误模式。通过这些设计，研究者能够更全面地了解LLM的错误以及人类对这些错误的反应。",
            "application_zh": "该研究成果可应用于LLM评估体系的改进，提升数字素养教育，并指导更值得信赖的人机交互系统设计。通过理解LLM认知错误的共建机制，可以帮助用户更理性地使用LLM，避免盲目信任，从而减少误导信息的传播。",
            "highlight_zh": "研究发现，LLM的错误会从预测形式转变为解释形式，即语言流畅、结构连贯但意义扭曲。评估者易受表面线索影响，导致认知漂移，使得错误答案被误认为可信。这表明错误并非仅是模型属性，而是人机交互的共建结果。",
            "tags_zh": [
                "大型语言模型",
                "人机交互",
                "认知错误",
                "可信度",
                "解释性评估"
            ],
            "_index": 86,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "Cyber Humanism in Education: Reclaiming Agency through AI and Learning Sciences",
            "authors": [
                "Giovanni Adorni"
            ],
            "arxiv_id": "2512.16701v1",
            "summary": "Generative Artificial Intelligence (GenAI) is rapidly reshaping how knowledge is produced and validated in education. Rather than adding another digital tool, large language models reconfigure reading, writing, and coding into hybrid human-AI workflows, raising concerns about epistemic automation, cognitive offloading, and the de-professiona\\-lisation of teachers. This paper proposes \\emph{Cyber Humanism in Education} as a framework for reclaiming human agency in this landscape. We conceptualise AI-enabled learning environments as socio-technical infrastructures co-authored by humans and machines, and position educators and learners as epistemic agents and \\emph{algorithmic citizens} who have both the right and the responsibility to shape these infrastructures.\n  We articulate three pillars for cyber-humanist design, \\emph{reflexive competence}, \\emph{algorithmic citizenship}, and \\emph{dialogic design}, and relate them to major international digital and AI competence frameworks. We then present higher-education case studies that operationalise these ideas through \\emph{prompt-based learning} and a new \\emph{Conversational AI Educator} certification within the EPICT ecosystem. The findings show how such practices can strengthen epistemic agency while surfacing tensions around workload, equity, and governance, and outline implications for the future of AI-rich, human-centred education.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "15 pages, 16 references, Key Note preented at the \"WAILS 2025 - The 2nd. Workshop on Artificial Intelligence with and for Learning Sciences\", Cagliary, Italy, 10-12 December 2025",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16701v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出教育领域“赛博人文主义”框架，通过AI和学习科学重塑能动性",
            "summary_zh": "生成式人工智能（GenAI）正在迅速重塑教育中知识的生产和验证方式。大型语言模型不仅仅是另一种数字工具，它们将阅读、写作和编码重新配置为混合人机工作流程，引发了对认知自动化、认知卸载和教师去专业化的担忧。本文提出了“教育中的赛博人文主义”框架，旨在重塑这种背景下的人类能动性。我们将支持人工智能的学习环境概念化为由人类和机器共同创作的社会技术基础设施，并将教育者和学习者定位为认知主体和“算法公民”，他们有权力和责任来塑造这些基础设施。我们阐述了赛博人文主义设计的三个支柱：反思能力、算法公民身份和对话式设计，并将它们与主要的国际数字和人工智能能力框架联系起来。然后，我们展示了高等教育案例研究，这些案例研究通过“基于提示的学习”和EPICT生态系统中的新的“会话式人工智能教育者”认证来实施这些想法。研究结果表明，这些实践如何加强认知能动性，同时揭示围绕工作量、公平和治理的紧张关系，并概述了以人为本的、富含人工智能的教育的未来影响。",
            "intro_zh": [
                "现有教育方法在生成式AI快速发展下，面临认知自动化、认知卸载和教师去专业化的挑战。",
                "论文提出“教育中的赛博人文主义”框架，旨在通过人机协作重塑教育中的人类能动性。",
                "通过案例研究，展示了基于提示的学习和会话式AI教育者认证如何加强认知能动性，并探讨了相关问题。"
            ],
            "method_zh": "**问题定义**：论文旨在解决生成式AI在教育领域快速发展带来的挑战，包括知识生产和验证方式的转变，以及由此引发的认知自动化、认知卸载和教师去专业化等问题。现有方法未能充分应对这些挑战，需要一种新的框架来重塑人类在教育中的能动性。\\n\\n**核心思路**：论文的核心思路是将AI赋能的学习环境视为由人类和机器共同创作的社会技术基础设施。教育者和学习者被定位为认知主体和“算法公民”，他们有权力和责任来塑造这些基础设施。通过这种方式，可以重新掌握人类在教育中的能动性，并确保AI的应用符合人类的价值观和目标。\\n\\n**技术框架**：该框架包含三个主要支柱：反思能力、算法公民身份和对话式设计。反思能力是指教育者和学习者需要具备批判性地思考AI技术及其影响的能力。算法公民身份是指教育者和学习者需要了解并参与塑造AI系统的权利和责任。对话式设计是指需要通过人机对话的方式来设计和开发AI赋能的学习环境。\\n\\n**关键创新**：该论文的关键创新在于提出了“教育中的赛博人文主义”框架，并将教育者和学习者定位为“算法公民”。这种框架超越了将AI视为简单工具的视角，而是将其视为与人类共同塑造教育环境的合作伙伴。\\n\\n**关键设计**：论文通过“基于提示的学习”和“会话式AI教育者”认证来具体实施赛博人文主义框架。基于提示的学习是指通过精心设计的提示来引导学生使用AI工具进行学习。会话式AI教育者认证是指对能够有效利用会话式AI技术进行教学的教育者进行认证。这些设计旨在帮助教育者和学习者更好地理解和利用AI技术，并确保AI的应用符合教育的价值观和目标。",
            "application_zh": "该研究成果可应用于高等教育、K12教育等多个领域，帮助教育者和学习者更好地理解和利用AI技术，促进以人为本的、富含人工智能的教育发展。该框架有助于应对AI带来的伦理和社会挑战，并确保AI的应用符合人类的价值观和目标。",
            "highlight_zh": "论文通过高等教育案例研究，展示了“基于提示的学习”和“会话式AI教育者”认证如何加强认知能动性。研究结果揭示了围绕工作量、公平和治理的紧张关系，并为未来AI赋能的教育提供了重要启示。具体性能数据和提升幅度未知。",
            "tags_zh": [
                "赛博人文主义",
                "教育人工智能",
                "算法公民",
                "人机协作",
                "提示学习"
            ],
            "_index": 87,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "Microsoft Academic Graph Information Retrieval for Research Recommendation and Assistance",
            "authors": [
                "Jacob Reiss",
                "Shikshya Shiwakoti",
                "Samuel Goldsmith",
                "Ujjwal Pandit"
            ],
            "arxiv_id": "2512.16661v1",
            "summary": "In today's information-driven world, access to scientific publications has become increasingly easy. At the same time, filtering through the massive volume of available research has become more challenging than ever. Graph Neural Networks (GNNs) and graph attention mechanisms have shown strong effectiveness in searching large-scale information databases, particularly when combined with modern large language models. In this paper, we propose an Attention-Based Subgraph Retriever, a GNN-as-retriever model that applies attention-based pruning to extract a refined subgraph, which is then passed to a large language model for advanced knowledge reasoning.",
            "categories": [
                "cs.IR",
                "cs.AI"
            ],
            "primary_category": "cs.IR",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "5 pages, 3 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16661v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出基于注意力机制的子图检索器，用于科研推荐和辅助，提升知识推理能力。",
            "summary_zh": "在当今信息驱动的世界中，获取科学出版物变得越来越容易。与此同时，筛选海量可用研究成果的难度也前所未有地增加。图神经网络（GNN）和图注意力机制已在搜索大规模信息数据库方面显示出强大的有效性，尤其是在与现代大型语言模型结合使用时。在本文中，我们提出了一种基于注意力的子图检索器，这是一种GNN检索模型，它应用基于注意力的剪枝来提取精炼的子图，然后将其传递给大型语言模型以进行高级知识推理。",
            "intro_zh": [
                "现有科研信息检索方法难以有效应对海量文献带来的筛选挑战。",
                "提出基于注意力机制的子图检索器，利用GNN提取精炼子图，辅助大型语言模型进行知识推理。",
                "论文重点在于模型设计和框架搭建，实验结果未知，有待进一步验证有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决科研信息过载的问题，即如何从海量的学术文献中快速准确地检索到所需信息。现有方法在处理大规模信息数据库时效率较低，难以有效利用文献之间的关联关系进行知识推理。\\n\\n**核心思路**：论文的核心思路是利用图神经网络（GNN）对学术文献之间的关系进行建模，并通过注意力机制提取与查询相关的子图。该子图包含更精炼的信息，可以更好地辅助大型语言模型进行知识推理。通过这种方式，可以提高检索效率和准确性。\\n\\n**技术框架**：整体框架包含两个主要阶段：1) 基于GNN的子图检索阶段：该阶段利用图神经网络对学术图谱进行编码，并使用注意力机制对节点进行剪枝，提取与查询相关的子图。2) 基于大型语言模型的知识推理阶段：该阶段将提取的子图输入到大型语言模型中，利用其强大的语言理解和生成能力进行知识推理，例如生成研究报告或推荐相关文献。\\n\\n**关键创新**：论文的关键创新在于提出了基于注意力机制的子图检索器，将GNN和大型语言模型相结合，充分利用了图结构信息和语言模型的知识推理能力。与传统的信息检索方法相比，该方法能够更好地捕捉文献之间的关联关系，并提供更准确的检索结果。\\n\\n**关键设计**：论文的关键设计包括：1) 如何设计GNN的结构以有效地编码学术图谱；2) 如何设计注意力机制以准确地提取与查询相关的子图；3) 如何将提取的子图有效地输入到大型语言模型中，并指导其进行知识推理。具体的参数设置、损失函数、网络结构等技术细节在摘要中未提及，属于未知信息。",
            "application_zh": "该研究成果可应用于科研推荐系统、学术搜索引擎、智能科研助手等领域。通过提供更准确、更高效的科研信息检索服务，可以帮助研究人员快速找到所需文献，提高科研效率，促进学术交流与合作。未来，该方法有望应用于更广泛的知识图谱检索和推理任务。",
            "highlight_zh": "摘要中未提供具体的实验结果和性能数据，因此无法总结实验亮点。需要查阅论文全文才能了解具体的实验设置、对比基线以及性能提升幅度等信息。目前仅知该方法结合了GNN和大型语言模型，理论上具有一定的优势。",
            "tags_zh": [
                "图神经网络",
                "GNN",
                "注意力机制",
                "子图检索",
                "知识推理",
                "科研推荐",
                "信息检索"
            ],
            "_index": 88,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16661v1/gril_framework.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16661v1/sag_outline.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16661v1/AttentionbasedGraphRetriever_Algo.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Refusal Steering: Fine-grained Control over LLM Refusal Behaviour for Sensitive Topics",
            "authors": [
                "Iker García-Ferrero",
                "David Montero",
                "Roman Orus"
            ],
            "arxiv_id": "2512.16602v1",
            "summary": "We introduce Refusal Steering, an inference-time method to exercise fine-grained control over Large Language Models refusal behaviour on politically sensitive topics without retraining. We replace fragile pattern-based refusal detection with an LLM-as-a-judge that assigns refusal confidence scores and we propose a ridge-regularized variant to compute steering vectors that better isolate the refusal--compliance direction. On Qwen3-Next-80B-A3B-Thinking, our method removes the refusal behaviour of the model around politically sensitive topics while maintaining safety on JailbreakBench and near-baseline performance on general benchmarks. The approach generalizes across 4B and 80B models and can also induce targeted refusals when desired. We analize the steering vectors and show that refusal signals concentrate in deeper layers of the transformer and are distributed across many dimensions. Together, these results demonstrate that activation steering can remove political refusal behaviour while retaining safety alignment for harmful content, offering a practical path to controllable, transparent moderation at inference time.",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16602v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出拒绝引导方法以控制大型语言模型的拒绝行为",
            "summary_zh": "本文介绍了一种名为拒绝引导（Refusal Steering）的方法，旨在对大型语言模型在政治敏感话题上的拒绝行为进行细粒度控制，而无需重新训练。该方法通过用大型语言模型作为判断者来替代脆弱的基于模式的拒绝检测，分配拒绝置信度分数，并提出了一种岭回归变体来计算更好地隔离拒绝与合规方向的引导向量。在Qwen3-Next-80B-A3B-Thinking模型上，我们的方法消除了模型在政治敏感话题上的拒绝行为，同时在JailbreakBench上保持安全性，并在一般基准测试中接近基线性能。该方法在4B和80B模型中具有良好的泛化能力，并能够在需要时诱导有针对性的拒绝。我们分析了引导向量，表明拒绝信号集中在变换器的深层，并分布在多个维度上。这些结果表明，激活引导可以消除政治拒绝行为，同时保持对有害内容的安全对齐，为推理时可控、透明的内容审核提供了实际路径。",
            "intro_zh": [
                "现有方法在处理政治敏感话题时，拒绝行为的检测依赖于脆弱的模式匹配，导致控制效果不佳。",
                "本文提出的拒绝引导方法通过引入大型语言模型作为判断者，利用拒绝置信度分数实现对拒绝行为的精细控制。",
                "实验结果显示，该方法在消除政治敏感话题的拒绝行为的同时，保持了安全性和接近基线的性能，具有良好的泛化能力。"
            ],
            "method_zh": "**问题定义**：本文旨在解决大型语言模型在处理政治敏感话题时的拒绝行为控制问题。现有方法依赖于脆弱的模式匹配，难以有效识别和调整拒绝行为。\\n\\n**核心思路**：提出拒绝引导方法，通过将大型语言模型作为判断者，利用其生成的拒绝置信度分数来实现对拒绝行为的细粒度控制，避免了重新训练的复杂性。\\n\\n**技术框架**：该方法的整体架构包括拒绝置信度评分模块和引导向量计算模块。首先，模型评估输入内容的拒绝置信度，然后计算引导向量以调整模型的输出。\\n\\n**关键创新**：最重要的创新在于引入岭回归变体来计算引导向量，这一设计使得拒绝与合规方向的隔离更加有效，显著提升了拒绝行为的控制能力。\\n\\n**关键设计**：在参数设置上，采用了岭回归的正则化技术，以优化引导向量的计算。此外，分析表明拒绝信号主要集中在变换器的深层，并在多个维度上分布，提供了新的理解模型行为的视角。",
            "application_zh": "该研究的潜在应用领域包括社交媒体内容审核、在线问答系统和任何涉及敏感话题的自动化对话系统。通过实现对拒绝行为的可控性，能够提高系统的透明度和用户信任度，减少误解和不当内容的传播。",
            "highlight_zh": "实验结果表明，拒绝引导方法在Qwen3-Next-80B-A3B-Thinking模型上有效消除了政治敏感话题的拒绝行为，同时在JailbreakBench上保持了安全性，且在一般基准测试中接近基线性能，展示了良好的泛化能力。",
            "tags_zh": [
                "大型语言模型",
                "拒绝行为",
                "内容审核",
                "政治敏感话题",
                "深度学习",
                "模型控制",
                "推理时调整",
                "岭回归"
            ],
            "_index": 89,
            "_used_api": "openai",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16602v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16602v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16602v1/images/top_layer_pca_2d_chinabadWRMD.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "From Personalization to Prejudice: Bias and Discrimination in Memory-Enhanced AI Agents for Recruitment",
            "authors": [
                "Himanshu Gharat",
                "Himanshi Agrawal",
                "Gourab K. Patro"
            ],
            "arxiv_id": "2512.16532v1",
            "summary": "Large Language Models (LLMs) have empowered AI agents with advanced capabilities for understanding, reasoning, and interacting across diverse tasks. The addition of memory further enhances them by enabling continuity across interactions, learning from past experiences, and improving the relevance of actions and responses over time; termed as memory-enhanced personalization. Although such personalization through memory offers clear benefits, it also introduces risks of bias. While several previous studies have highlighted bias in ML and LLMs, bias due to memory-enhanced personalized agents is largely unexplored. Using recruitment as an example use case, we simulate the behavior of a memory-enhanced personalized agent, and study whether and how bias is introduced and amplified in and across various stages of operation. Our experiments on agents using safety-trained LLMs reveal that bias is systematically introduced and reinforced through personalization, emphasizing the need for additional protective measures or agent guardrails in memory-enhanced LLM-based AI agents.",
            "categories": [
                "cs.AI",
                "cs.IR"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "In Proceedings of the Nineteenth ACM International Conference on Web Search and Data Mining (WSDM '26)",
            "doi": "10.1145/3773966.3779376",
            "journal_ref": "In Proceedings of the Nineteenth ACM International Conference on Web Search and Data Mining (WSDM '26), 2026, Boise, ID, USA. ACM, New York, NY, USA",
            "pdf_url": "https://arxiv.org/pdf/2512.16532v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "揭示记忆增强型AI招聘Agent中的偏见引入与强化机制",
            "summary_zh": "大型语言模型（LLMs）赋予了AI Agent强大的理解、推理和交互能力，使其能够胜任各种任务。通过添加记忆功能，AI Agent能够跨交互保持连贯性，从过去的经验中学习，并随着时间的推移提高行动和响应的相关性，从而实现记忆增强型个性化。虽然这种通过记忆实现的个性化具有明显的优势，但也带来了偏见风险。尽管之前的研究已经强调了ML和LLM中的偏见，但关于记忆增强型个性化Agent所带来的偏见在很大程度上尚未被探索。本文以招聘为例，模拟了记忆增强型个性化Agent的行为，并研究了偏见是如何在各个操作阶段被引入和强化的。对使用安全训练LLM的Agent进行的实验表明，偏见通过个性化被系统地引入和强化，强调了在基于记忆增强型LLM的AI Agent中采取额外保护措施或Agent防护措施的必要性。",
            "intro_zh": [
                "现有研究较少关注记忆增强型AI Agent中的偏见问题，尤其是在个性化过程中偏见的引入和强化机制。",
                "该研究通过模拟招聘场景中的AI Agent行为，分析了偏见在Agent操作的各个阶段如何产生和演变。",
                "实验结果表明，即使使用安全训练的LLM，记忆增强型个性化仍然会导致偏见的系统性引入和强化。"
            ],
            "method_zh": "**问题定义**：论文旨在研究在招聘场景下，记忆增强型AI Agent在个性化过程中引入和强化偏见的问题。现有方法缺乏对这种偏见来源和演化过程的深入理解，难以有效缓解。\n\\n**核心思路**：论文的核心思路是通过模拟AI Agent在招聘过程中的行为，观察和分析Agent在与不同候选人交互后，其记忆中积累的经验如何影响后续的决策，从而揭示偏见的产生和强化机制。这种模拟方法能够控制实验变量，更清晰地观察偏见的影响。\n\\n**技术框架**：论文构建了一个模拟招聘环境，AI Agent与一系列模拟的候选人进行交互。Agent使用LLM作为其核心推理引擎，并配备记忆模块来存储交互历史。整个流程包括：候选人信息输入、Agent基于记忆和LLM进行评估、Agent给出招聘建议、Agent更新记忆。通过多次迭代，观察Agent的招聘偏好变化。\n\\n**关键创新**：该研究的关键创新在于关注了记忆增强型AI Agent在个性化过程中产生的偏见，并提出了一种模拟方法来研究这种偏见的产生和强化机制。与以往关注静态数据集偏见的研究不同，该研究关注的是Agent在动态交互过程中学习到的偏见。\n\\n**关键设计**：论文的关键设计包括：1) 使用安全训练的LLM作为Agent的核心，以降低LLM本身带来的偏见；2) 设计合理的记忆更新机制，模拟Agent如何从过去的经验中学习；3) 设计多种评估指标，量化Agent的招聘偏好，例如不同性别或种族候选人的录取率。",
            "application_zh": "该研究成果可应用于各种需要个性化推荐或决策的AI系统中，例如金融信贷、教育评估等。通过理解和缓解记忆增强型AI Agent中的偏见，可以提高AI系统的公平性和可靠性，避免歧视性行为，从而促进社会公平。",
            "highlight_zh": "实验结果表明，即使使用经过安全训练的LLM，记忆增强型个性化仍然会导致偏见的系统性引入和强化。具体而言，Agent在与特定群体（例如，特定性别或种族）的候选人交互后，会逐渐形成对该群体的偏见，并在后续的招聘决策中体现出来。这表明，仅仅依靠LLM的安全训练不足以消除偏见，需要额外的保护措施。",
            "tags_zh": [
                "大型语言模型",
                "AI Agent",
                "记忆增强",
                "个性化",
                "偏见",
                "招聘",
                "公平性",
                "机器学习"
            ],
            "_index": 90,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16532v1/Figure_1_overview.png",
                    "caption": "",
                    "figure_id": "fig_0"
                }
            ]
        },
        {
            "title": "cuPilot: A Strategy-Coordinated Multi-agent Framework for CUDA Kernel Evolution",
            "authors": [
                "Jinwu Chen",
                "Qidie Wu",
                "Bin Li",
                "Lin Ma",
                "Xin Si",
                "Yang Hu",
                "Shouyi Yin",
                "Jun Yang"
            ],
            "arxiv_id": "2512.16465v1",
            "summary": "Optimizing CUDA kernels is a challenging and labor-intensive task, given the need for hardware-software co-design expertise and the proprietary nature of high-performance kernel libraries. While recent large language models (LLMs) combined with evolutionary algorithms show promise in automatic kernel optimization, existing approaches often fall short in performance due to their suboptimal agent designs and mismatched evolution representations. This work identifies these mismatches and proposes cuPilot, a strategy-coordinated multi-agent framework that introduces strategy as an intermediate semantic representation for kernel evolution. Key contributions include a strategy-coordinated evolution algorithm, roofline-guided prompting, and strategy-level population initialization. Experimental results show that the generated kernels by cuPilot achieve an average speed up of 3.09$\\times$ over PyTorch on a benchmark of 100 kernels. On the GEMM tasks, cuPilot showcases sophisticated optimizations and achieves high utilization of critical hardware units. The generated kernels are open-sourced at https://github.com/champloo2878/cuPilot-Kernels.git.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16465v1",
            "code_links": [
                {
                    "url": "https://github.com/champloo2878/cuPilot-Kernels.git",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "cuPilot：一种策略协调的多智能体框架，用于CUDA内核演化",
            "summary_zh": "优化CUDA内核是一项具有挑战性且劳动密集型的工作，需要软硬件协同设计专业知识和高性能内核库的专有性质。虽然最近的大型语言模型（LLM）与进化算法相结合在自动内核优化方面显示出希望，但由于其次优的智能体设计和不匹配的演化表示，现有方法通常在性能方面表现不佳。这项工作识别了这些不匹配之处，并提出了cuPilot，这是一个策略协调的多智能体框架，它引入策略作为内核演化的中间语义表示。主要贡献包括策略协调的进化算法、屋顶线引导的提示和策略级种群初始化。实验结果表明，cuPilot生成的内核在100个内核的基准测试中，相对于PyTorch实现了平均3.09倍的加速。在GEMM任务中，cuPilot展示了复杂的优化，并实现了关键硬件单元的高利用率。生成的内核已在https://github.com/champloo2878/cuPilot-Kernels.git上开源。",
            "intro_zh": [
                "现有CUDA内核自动优化方法在智能体设计和演化表示上存在不足，导致性能受限。",
                "cuPilot提出策略协调的多智能体框架，将策略作为内核演化的中间语义表示，解决上述问题。",
                "实验表明，cuPilot在多个基准测试中显著提升了CUDA内核的性能，平均加速比达到3.09倍。"
            ],
            "method_zh": "**问题定义**：CUDA内核优化需要专业的软硬件协同设计知识，且高性能内核库通常是专有的，这使得自动优化极具挑战性。现有方法，如直接使用LLM结合进化算法，由于智能体设计不佳和演化表示不匹配，难以达到理想的优化效果。这些方法无法有效地探索CUDA内核的优化空间，导致性能提升有限。\\n\\n**核心思路**：cuPilot的核心思路是将CUDA内核的优化过程分解为一系列策略，例如循环展开、内存重排等。通过引入“策略”作为中间语义表示，使得LLM能够更好地理解和控制内核的演化过程。这种策略级的抽象允许智能体在更高的层次上进行推理和决策，从而更有效地探索优化空间。\\n\\n**技术框架**：cuPilot采用多智能体框架，每个智能体负责不同的优化策略。整体流程包括：1) 策略级种群初始化：根据屋顶线模型指导，初始化一组有潜力的策略组合。2) 策略协调的进化算法：多个智能体协同工作，根据当前内核的性能和硬件特性，选择合适的策略进行演化。3) 屋顶线引导的提示：利用屋顶线模型为LLM提供提示，引导其生成更高效的CUDA代码。框架通过迭代演化，不断优化内核性能。\\n\\n**关键创新**：cuPilot的关键创新在于引入了“策略”作为CUDA内核演化的中间语义表示。与直接操作CUDA代码相比，策略级的抽象更易于LLM理解和控制，从而提高了优化效率。此外，策略协调的进化算法和屋顶线引导的提示进一步增强了框架的优化能力。\\n\\n**关键设计**：cuPilot的关键设计包括：1) 策略库的设计：定义了一系列常用的CUDA内核优化策略，例如循环展开、向量化、共享内存优化等。2) 策略选择机制：根据当前内核的性能瓶颈和硬件特性，选择合适的策略进行演化。3) 屋顶线模型的应用：利用屋顶线模型分析内核的性能瓶颈，并为LLM提供优化方向的指导。",
            "application_zh": "cuPilot可应用于各种需要高性能计算的领域，如深度学习、科学计算、图像处理等。它可以帮助开发者自动优化CUDA内核，提高应用程序的性能，降低开发成本。该研究的成果有助于推动高性能计算的普及，加速相关领域的发展。",
            "highlight_zh": "cuPilot在100个CUDA内核的基准测试中，相对于PyTorch实现了平均3.09倍的加速。在GEMM任务中，cuPilot生成的内核能够充分利用硬件资源，展现了其强大的优化能力。这些实验结果表明，cuPilot在CUDA内核自动优化方面具有显著优势。",
            "tags_zh": [
                "CUDA内核优化",
                "多智能体系统",
                "进化算法",
                "大型语言模型",
                "屋顶线模型",
                "策略协调",
                "自动代码生成"
            ],
            "_index": 91,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16465v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16465v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16465v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Towards AI-Supported Research: a Vision of the TIB AIssistant",
            "authors": [
                "Sören Auer",
                "Allard Oelen",
                "Mohamad Yaser Jaradeh",
                "Mutahira Khalid",
                "Farhana Keya",
                "Sasi Kiran Gaddipati",
                "Jennifer D'Souza",
                "Lorenz Schlüter",
                "Amirreza Alasti",
                "Gollam Rabby",
                "Azanzi Jiomekong",
                "Oliver Karras"
            ],
            "arxiv_id": "2512.16447v1",
            "summary": "The rapid advancements in Generative AI and Large Language Models promise to transform the way research is conducted, potentially offering unprecedented opportunities to augment scholarly workflows. However, effectively integrating AI into research remains a challenge due to varying domain requirements, limited AI literacy, the complexity of coordinating tools and agents, and the unclear accuracy of Generative AI in research. We present the vision of the TIB AIssistant, a domain-agnostic human-machine collaborative platform designed to support researchers across disciplines in scientific discovery, with AI assistants supporting tasks across the research life cycle. The platform offers modular components - including prompt and tool libraries, a shared data store, and a flexible orchestration framework - that collectively facilitate ideation, literature analysis, methodology development, data analysis, and scholarly writing. We describe the conceptual framework, system architecture, and implementation of an early prototype that demonstrates the feasibility and potential impact of our approach.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16447v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出TIB AIssistant：一个支持科研全流程的领域无关人机协作平台",
            "summary_zh": "生成式AI和大型语言模型的快速发展有望变革科研方式，为学术工作流程带来前所未有的提升机会。然而，由于领域需求各异、AI素养有限、工具和代理协调复杂以及生成式AI在研究中的准确性不明确，将AI有效集成到研究中仍然是一个挑战。我们提出了TIB AIssistant的愿景，这是一个领域无关的人机协作平台，旨在支持跨学科研究人员进行科学发现，AI助手支持研究生命周期中的各项任务。该平台提供模块化组件，包括提示和工具库、共享数据存储以及灵活的编排框架，共同促进构思、文献分析、方法开发、数据分析和学术写作。我们描述了概念框架、系统架构以及早期原型的实现，展示了我们方法的可行性和潜在影响。",
            "intro_zh": [
                "现有方法难以有效整合AI到科研中，面临领域差异大、AI素养不足、工具协调复杂和AI准确性不确定等挑战。",
                "TIB AIssistant旨在构建一个领域无关的人机协作平台，通过AI助手支持科研全流程，促进科学发现。",
                "早期原型验证了TIB AIssistant的可行性和潜在价值，展示了其在构思、文献分析、方法开发等方面的应用。"
            ],
            "method_zh": "**问题定义**：当前科研领域面临着如何有效利用生成式AI和大型语言模型来提升研究效率和质量的问题。现有方法在将AI集成到研究工作流程中时，存在诸多痛点，包括不同研究领域的特殊需求差异大，研究人员普遍缺乏足够的AI素养，各种AI工具和代理之间的协调复杂，以及生成式AI在科研任务中的准确性难以保证。这些问题阻碍了AI在科研领域的广泛应用。\\n\\n**核心思路**：TIB AIssistant的核心思路是构建一个领域无关的人机协作平台，该平台能够为研究人员提供全面的AI支持，覆盖从研究构思到成果写作的整个生命周期。通过模块化的设计，平台可以灵活适应不同领域的需求，并提供易于使用的工具和接口，降低AI的使用门槛。同时，平台强调人机协作，充分发挥研究人员的专业知识和判断力，确保AI的输出结果的准确性和可靠性。\\n\\n**技术框架**：TIB AIssistant的整体架构包括以下几个主要模块：1) 提示和工具库：提供丰富的预定义提示和AI工具，方便研究人员快速启动任务。2) 共享数据存储：用于存储和管理研究数据，确保数据的一致性和可访问性。3) 灵活的编排框架：允许研究人员自定义AI助手的工作流程，实现任务的自动化和优化。此外，平台还提供用户界面，方便研究人员与AI助手进行交互，并监控任务的执行情况。\\n\\n**关键创新**：TIB AIssistant的关键创新在于其领域无关性和人机协作的设计理念。与以往专注于特定领域的AI研究助手不同，TIB AIssistant旨在为所有学科的研究人员提供通用的AI支持。同时，平台强调人机协作，将AI视为研究人员的助手，而非替代品，充分发挥研究人员的专业知识和判断力，确保研究结果的质量。\\n\\n**关键设计**：目前论文描述的是一个早期原型，并未涉及具体的参数设置、损失函数、网络结构等技术细节。未来的研究将重点关注如何优化AI助手的性能，提高其在不同科研任务中的准确性和效率。例如，可以通过微调大型语言模型，使其更好地适应科研领域的特定需求。此外，还需要设计有效的评估指标，用于衡量AI助手的性能和用户满意度。",
            "application_zh": "TIB AIssistant 具有广泛的应用前景，可应用于各个学科的研究领域，例如自然科学、社会科学、人文科学等。它可以帮助研究人员更高效地进行文献调研、数据分析、实验设计和论文撰写，从而加速科学发现的进程。此外，该平台还可以用于教育领域，帮助学生更好地学习和掌握研究方法。",
            "highlight_zh": "该论文主要展示了TIB AIssistant的概念框架、系统架构和早期原型，验证了其在支持科研方面的可行性和潜在影响。虽然没有提供具体的性能数据，但原型展示了平台在构思、文献分析、方法开发等方面的应用，为未来的研究奠定了基础。未来的工作将集中在优化AI助手的性能和用户体验上。",
            "tags_zh": [
                "人机协作",
                "科研助手",
                "大型语言模型",
                "领域无关",
                "AI平台"
            ],
            "_index": 92,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16447v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16447v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                }
            ]
        },
        {
            "title": "TIB AIssistant: a Platform for AI-Supported Research Across Research Life Cycles",
            "authors": [
                "Allard Oelen",
                "Sören Auer"
            ],
            "arxiv_id": "2512.16442v1",
            "summary": "The rapidly growing popularity of adopting Artificial Intelligence (AI), and specifically Large Language Models (LLMs), is having a widespread impact throughout society, including the academic domain. AI-supported research has the potential to support researchers with tasks across the entire research life cycle. In this work, we demonstrate the TIB AIssistant, an AI-supported research platform providing support throughout the research life cycle. The AIssistant consists of a collection of assistants, each responsible for a specific research task. In addition, tools are provided to give access to external scholarly services. Generated data is stored in the assets and can be exported as an RO-Crate bundle to provide transparency and enhance reproducibility of the research project. We demonstrate the AIssistant's main functionalities by means of a sequential walk-through of assistants, interacting with each other to generate sections for a draft research paper. In the end, with the AIssistant, we lay the foundation for a larger agenda of providing a community-maintained platform for AI-supported research.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16442v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "TIB AIssistant：一个支持研究全生命周期的人工智能研究平台",
            "summary_zh": "人工智能（AI），特别是大型语言模型（LLMs）的迅速普及，正在对包括学术领域在内的整个社会产生广泛影响。AI支持的研究有潜力在整个研究生命周期中为研究人员提供帮助。本文展示了TIB AIssistant，这是一个AI支持的研究平台，为整个研究生命周期提供支持。AIssistant由一系列助手组成，每个助手负责特定的研究任务。此外，还提供了工具来访问外部学术服务。生成的数据存储在资产中，可以导出为RO-Crate包，以提供透明度并增强研究项目的可重复性。我们通过一个助手序列的演示，展示了AIssistant的主要功能，这些助手相互交互，为研究论文草案生成章节。最后，通过AIssistant，我们为提供一个社区维护的AI支持研究平台奠定了基础。",
            "intro_zh": [
                "研究人员在整个研究生命周期中面临诸多挑战，例如文献综述、实验设计、论文撰写等，传统方法效率较低。",
                "TIB AIssistant平台通过集成多个AI助手，分别负责不同的研究任务，旨在简化研究流程并提高效率。",
                "该平台允许助手之间相互交互，协同完成任务，并将生成的数据以RO-Crate格式存储，增强研究的透明性和可重复性。"
            ],
            "method_zh": "**问题定义**：当前研究人员在研究的各个阶段，如文献检索、论文撰写等，面临着效率低下的问题。现有的方法往往需要人工完成大量重复性工作，耗费时间且容易出错。因此，如何利用AI技术提升研究效率，是亟待解决的问题。\\n\\n**核心思路**：论文的核心思路是构建一个AI辅助研究平台，该平台集成多个AI助手，每个助手负责特定的研究任务，例如文献检索、论文撰写、数据分析等。通过这些AI助手的协同工作，可以自动化完成研究过程中的许多重复性任务，从而提高研究效率。\\n\\n**技术框架**：TIB AIssistant平台包含以下几个主要模块：1) AI助手模块：包含多个AI助手，每个助手负责特定的研究任务。2) 外部学术服务接口：提供访问外部学术服务的接口，例如文献数据库、知识图谱等。3) 数据存储模块：用于存储生成的数据，并支持导出为RO-Crate格式。4) 用户界面：提供用户友好的交互界面，方便用户使用平台。整个流程是用户通过界面与不同的AI助手交互，助手调用外部服务，并将结果存储，最终用户可以导出结果。\\n\\n**关键创新**：该平台的关键创新在于将多个AI助手集成到一个统一的平台中，并允许这些助手之间相互交互，协同完成研究任务。这种集成化的设计可以更好地利用AI技术，提高研究效率。此外，平台还支持将生成的数据导出为RO-Crate格式，增强了研究的透明性和可重复性。\\n\\n**关键设计**：论文中没有详细描述关键参数设置、损失函数、网络结构等技术细节。这些细节可能与各个AI助手的具体实现有关，需要参考各个助手的相关文献。RO-Crate 的使用是关键设计，保证了研究结果的可追溯性。",
            "application_zh": "该研究成果可应用于各个学科领域，为研究人员提供AI辅助的研究工具，加速科研进程。例如，可以帮助研究人员快速完成文献综述、实验设计、论文撰写等任务。未来，该平台有望成为一个社区维护的AI支持研究平台，促进学术交流和合作。",
            "highlight_zh": "论文通过演示AIssistant平台的主要功能，展示了AI助手在生成研究论文草案章节方面的能力。虽然没有提供具体的性能数据或对比基线，但该演示验证了平台的可行性和潜力，表明AI技术可以有效地辅助研究人员完成研究任务。",
            "tags_zh": [
                "人工智能",
                "大型语言模型",
                "研究平台",
                "AI辅助研究",
                "RO-Crate",
                "研究生命周期",
                "学术服务",
                "知识图谱"
            ],
            "_index": 93,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16442v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                }
            ]
        },
        {
            "title": "Introducing ORKG ASK: an AI-driven Scholarly Literature Search and Exploration System Taking a Neuro-Symbolic Approach",
            "authors": [
                "Allard Oelen",
                "Mohamad Yaser Jaradeh",
                "Sören Auer"
            ],
            "arxiv_id": "2512.16425v1",
            "summary": "As the volume of published scholarly literature continues to grow, finding relevant literature becomes increasingly difficult. With the rise of generative Artificial Intelligence (AI), and particularly Large Language Models (LLMs), new possibilities emerge to find and explore literature. We introduce ASK (Assistant for Scientific Knowledge), an AI-driven scholarly literature search and exploration system that follows a neuro-symbolic approach. ASK aims to provide active support to researchers in finding relevant scholarly literature by leveraging vector search, LLMs, and knowledge graphs. The system allows users to input research questions in natural language and retrieve relevant articles. ASK automatically extracts key information and generates answers to research questions using a Retrieval-Augmented Generation (RAG) approach. We present an evaluation of ASK, assessing the system's usability and usefulness. Findings indicate that the system is user-friendly and users are generally satisfied while using the system.",
            "categories": [
                "cs.IR",
                "cs.AI"
            ],
            "primary_category": "cs.IR",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "10.1007/978-3-031-97207-2_2",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16425v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出基于神经符号方法的ORKG ASK，用于AI驱动的学术文献搜索与探索",
            "summary_zh": "随着学术文献数量的持续增长，找到相关文献变得越来越困难。生成式人工智能（AI），特别是大型语言模型（LLM）的兴起，为文献查找和探索带来了新的可能性。我们介绍ASK（科学知识助手），这是一个AI驱动的学术文献搜索和探索系统，它遵循神经符号方法。ASK旨在通过利用向量搜索、LLM和知识图谱，为研究人员寻找相关学术文献提供积极支持。该系统允许用户以自然语言输入研究问题并检索相关文章。ASK自动提取关键信息，并使用检索增强生成（RAG）方法生成研究问题的答案。我们对ASK进行了评估，评估了系统的可用性和实用性。结果表明，该系统用户友好，用户在使用该系统时普遍感到满意。",
            "intro_zh": [
                "现有学术文献数量庞大，研究人员难以快速找到所需信息，传统搜索方法效率较低。",
                "ASK系统采用神经符号方法，结合向量搜索、LLM和知识图谱，提升文献检索和探索能力。",
                "评估结果表明，ASK系统具有良好的用户友好性和实用性，用户对系统使用体验感到满意。"
            ],
            "method_zh": "**问题定义**：当前学术文献数量爆炸式增长，研究人员面临着信息过载的挑战。传统的关键词搜索方法难以准确捕捉研究意图，检索结果往往包含大量无关文献。此外，研究人员需要花费大量时间阅读和总结文献，效率低下。\\n\\n**核心思路**：ASK系统旨在通过结合神经方法（LLM）和符号方法（知识图谱），实现更智能、更高效的学术文献搜索和探索。核心思想是利用LLM理解用户提出的自然语言问题，并结合知识图谱进行推理和信息提取，最终通过RAG方法生成答案。\\n\\n**技术框架**：ASK系统的整体架构包含以下几个主要模块：1) **问题理解模块**：利用LLM对用户输入的自然语言问题进行语义理解和意图识别。2) **文献检索模块**：使用向量搜索技术，在文献数据库中检索与问题相关的候选文献。3) **知识提取模块**：从候选文献中提取关键信息，并将其与知识图谱中的实体和关系进行对齐。4) **答案生成模块**：利用RAG方法，结合检索到的文献和知识图谱中的信息，生成针对用户问题的答案。\\n\\n**关键创新**：ASK系统的关键创新在于其神经符号方法的融合。与传统的基于关键词的搜索方法相比，ASK能够更准确地理解用户的研究意图。与单纯使用LLM的方法相比，ASK通过结合知识图谱，能够提供更可靠、更结构化的答案。\\n\\n**关键设计**：ASK系统使用了预训练的LLM模型（具体模型未知）进行问题理解和答案生成。向量搜索使用了预训练的词向量模型（具体模型未知）来表示文献和问题。知识图谱的构建和维护方法未知。RAG方法的具体实现细节，例如prompt的设计和生成策略，也未知。",
            "application_zh": "ASK系统可应用于学术研究、科技情报分析、教育等领域。研究人员可以使用ASK快速找到相关文献，了解研究进展，并发现新的研究方向。科技情报分析人员可以使用ASK进行竞争情报分析和技术趋势预测。学生可以使用ASK进行文献综述和学习。",
            "highlight_zh": "论文对ASK系统进行了可用性和实用性评估，结果表明用户对系统的用户友好性感到满意。然而，论文并未提供具体的性能指标，例如检索准确率、答案质量等。因此，ASK系统在实际应用中的效果仍有待进一步验证。",
            "tags_zh": [
                "学术文献搜索",
                "知识图谱",
                "大型语言模型",
                "神经符号方法",
                "检索增强生成"
            ],
            "_index": 94,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16425v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16425v1/figures/screenshot-ask.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16425v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Synthelite: Chemist-aligned and feasibility-aware synthesis planning with LLMs",
            "authors": [
                "Nguyen Xuan-Vu",
                "Daniel Armstrong",
                "Milena Wehrbach",
                "Andres M Bran",
                "Zlatko Jončev",
                "Philippe Schwaller"
            ],
            "arxiv_id": "2512.16424v1",
            "summary": "Computer-aided synthesis planning (CASP) has long been envisioned as a complementary tool for synthetic chemists. However, existing frameworks often lack mechanisms to allow interaction with human experts, limiting their ability to integrate chemists' insights. In this work, we introduce Synthelite, a synthesis planning framework that uses large language models (LLMs) to directly propose retrosynthetic transformations. Synthelite can generate end-to-end synthesis routes by harnessing the intrinsic chemical knowledge and reasoning capabilities of LLMs, while allowing expert intervention through natural language prompts. Our experiments demonstrate that Synthelite can flexibly adapt its planning trajectory to diverse user-specified constraints, achieving up to 95\\% success rates in both strategy-constrained and starting-material-constrained synthesis tasks. Additionally, Synthelite exhibits the ability to account for chemical feasibility during route design. We envision Synthelite to be both a useful tool and a step toward a paradigm where LLMs are the central orchestrators of synthesis planning.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16424v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "Synthelite：利用LLM实现化学家友好且可行性感知的合成路线规划",
            "summary_zh": "计算机辅助合成规划(CASP)长期以来被认为是合成化学家的辅助工具。然而，现有的框架通常缺乏与人类专家交互的机制，限制了它们整合化学家见解的能力。本文介绍了Synthelite，一个使用大型语言模型(LLM)直接提出逆合成转化的合成规划框架。Synthelite通过利用LLM内在的化学知识和推理能力来生成端到端的合成路线，同时允许通过自然语言提示进行专家干预。实验表明，Synthelite可以灵活地调整其规划轨迹以适应各种用户指定的约束，在策略约束和起始材料约束的合成任务中均达到高达95%的成功率。此外，Synthelite还展示了在路线设计期间考虑化学可行性的能力。我们设想Synthelite既是一个有用的工具，也是朝着LLM成为合成规划中心协调者的范例迈出的一步。",
            "intro_zh": [
                "现有CASP框架缺乏与化学专家的有效交互，难以整合专家知识和经验。",
                "Synthelite利用LLM的化学知识和推理能力，通过自然语言提示实现专家干预的合成路线规划。",
                "实验表明，Synthelite在多种约束条件下均表现出高成功率，并能考虑化学反应的可行性。"
            ],
            "method_zh": "**问题定义**：现有的计算机辅助合成规划（CASP）系统难以与化学专家进行有效互动，无法充分利用专家的经验和知识来指导合成路线的设计。这限制了CASP系统的实用性和适用范围，尤其是在面对复杂或非标准的合成挑战时。\n\n**核心思路**：Synthelite的核心思路是利用大型语言模型（LLM）的强大语言理解和生成能力，以及其蕴含的化学知识，直接生成逆合成转化方案。通过自然语言提示，化学专家可以与LLM进行交互，提供约束条件、指导方向或修正建议，从而实现人机协同的合成路线规划。\n\n**技术框架**：Synthelite的整体框架包括以下几个主要阶段：1) 用户通过自然语言输入目标分子和约束条件；2) LLM基于输入生成可能的逆合成转化方案；3) 用户可以对LLM的建议进行评估和修改，并通过自然语言反馈给LLM；4) LLM根据用户反馈调整规划轨迹，并生成下一步的逆合成转化方案；5) 重复步骤2-4，直到生成完整的合成路线。\n\n**关键创新**：Synthelite的关键创新在于将LLM作为合成规划的核心引擎，并引入了自然语言交互机制，使得化学专家能够直接参与到合成路线的设计过程中。这种人机协同的方式能够充分利用LLM的知识和推理能力，同时结合专家的经验和判断，从而提高合成路线规划的效率和成功率。与传统的基于规则或模板的CASP系统相比，Synthelite具有更强的灵活性和适应性。\n\n**关键设计**：Synthelite的关键设计包括：1) 针对化学合成任务对LLM进行微调，使其更好地理解化学语言和反应规则；2) 设计有效的自然语言提示模板，使得用户能够清晰地表达约束条件和反馈意见；3) 引入可行性评估模块，用于评估LLM生成的反应方案的化学可行性，避免生成不合理的反应。",
            "application_zh": "Synthelite可应用于药物发现、材料科学等领域，加速新分子和新材料的合成路线设计。它能够辅助化学家快速探索合成空间，降低实验成本，并有望推动化学合成的自动化和智能化。未来，Synthelite可以集成到实验室自动化平台中，实现从设计到合成的全流程自动化。",
            "highlight_zh": "Synthelite在策略约束和起始材料约束的合成任务中均达到了高达95%的成功率，表明其能够灵活地适应用户指定的约束条件。此外，Synthelite还展示了在路线设计期间考虑化学可行性的能力，避免了生成不合理的反应路径。这些结果表明Synthelite在合成路线规划方面具有显著的优势。",
            "tags_zh": [
                "计算机辅助合成规划",
                "大型语言模型",
                "逆合成分析",
                "自然语言交互",
                "化学可行性",
                "人机协同",
                "药物发现"
            ],
            "_index": 95,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16424v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16424v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16424v1/figs/sm_constrained_solve.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "AI Needs Physics More Than Physics Needs AI",
            "authors": [
                "Peter Coveney",
                "Roger Highfield"
            ],
            "arxiv_id": "2512.16344v1",
            "summary": "Artificial intelligence (AI) is commonly depicted as transformative. Yet, after more than a decade of hype, its measurable impact remains modest outside a few high-profile scientific and commercial successes. The 2024 Nobel Prizes in Chemistry and Physics recognized AI's potential, but broader assessments indicate the impact to date is often more promotional than technical. We argue that while current AI may influence physics, physics has significantly more to offer this generation of AI. Current architectures - large language models, reasoning models, and agentic AI - can depend on trillions of meaningless parameters, suffer from distributional bias, lack uncertainty quantification, provide no mechanistic insights, and fail to capture even elementary scientific laws. We review critiques of these limits, highlight opportunities in quantum AI and analogue computing, and lay down a roadmap for the adoption of 'Big AI': a synthesis of theory-based rigour with the flexibility of machine learning.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16344v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "强调物理学对AI发展的重要性，提出融合理论严谨性和机器学习灵活性的“Big AI”愿景",
            "summary_zh": "人工智能（AI）常被描绘成具有变革性的技术。然而，经过十多年的炒作，除了少数引人注目的科学和商业成功案例外，其可衡量的影响仍然有限。2024年诺贝尔化学奖和物理学奖认可了AI的潜力，但更广泛的评估表明，迄今为止，AI的影响更多是宣传性的而非技术性的。我们认为，虽然当前的AI可能会影响物理学，但物理学能为这一代AI提供更多。当前的架构——大型语言模型、推理模型和代理AI——可能依赖于数万亿个无意义的参数，遭受分布偏差，缺乏不确定性量化，无法提供机制性见解，甚至无法捕捉基本的科学定律。我们回顾了对这些局限性的批评，强调了量子AI和模拟计算中的机遇，并为采用“Big AI”制定了路线图：即将基于理论的严谨性与机器学习的灵活性相结合。",
            "intro_zh": [
                "现有AI模型依赖大量无意义参数，缺乏对基本科学定律的理解，限制了其在科学领域的应用。",
                "论文提出“Big AI”愿景，强调将物理学等理论知识融入AI模型，提升模型的泛化性和可解释性。",
                "论文探讨了量子AI和模拟计算等新兴技术在AI发展中的潜力，并为“Big AI”的实现制定了路线图。"
            ],
            "method_zh": "**问题定义**：当前AI模型，如大型语言模型，虽然在某些任务上表现出色，但存在严重缺陷。它们依赖于海量数据和参数，缺乏对物理世界基本规律的理解，导致泛化能力差、易受分布偏差影响，且难以提供可解释的机制性见解。这些问题限制了AI在科学领域的应用，尤其是在需要精确预测和深入理解的物理学研究中。\\n\\n**核心思路**：论文的核心思路是强调物理学等理论知识对AI发展的重要性。作者认为，未来的AI应该将基于理论的严谨性与机器学习的灵活性相结合，构建一种“Big AI”模型。这种模型不仅能够处理大量数据，还能够理解和应用物理学原理，从而提高模型的泛化能力、可解释性和可靠性。\\n\\n**技术框架**：论文并未提出一个具体的AI模型架构，而是提出了一个发展方向。这个方向强调将物理学知识融入AI模型中。具体的技术框架可能包括：1) 利用物理学方程作为约束条件来训练AI模型；2) 将物理学知识嵌入到AI模型的结构中；3) 开发新的AI算法，使其能够更好地处理物理学数据。此外，论文还强调了量子AI和模拟计算等新兴技术在AI发展中的潜力。\\n\\n**关键创新**：论文的关键创新在于提出了“Big AI”的愿景，强调了物理学等理论知识对AI发展的重要性。这与当前AI领域过度依赖数据和参数的趋势形成对比。论文还指出了当前AI模型的局限性，并为未来的AI发展方向提供了新的思路。\\n\\n**关键设计**：由于论文主要关注的是AI的发展方向，而非具体的模型设计，因此没有详细的关键参数设置、损失函数或网络结构等技术细节。未来的研究需要探索如何将物理学知识有效地融入到AI模型中，并设计相应的训练方法和评估指标。",
            "application_zh": "该研究的潜在应用领域包括：新材料发现、药物研发、气候建模、能源优化等。通过将物理学知识融入AI模型，可以加速科学研究进程，提高预测精度，并为解决复杂科学问题提供新的思路。未来，这种“Big AI”模型有望在各个科学领域发挥重要作用，推动科技进步。",
            "highlight_zh": "论文的主要亮点在于对当前AI局限性的深刻剖析，以及对未来AI发展方向的前瞻性思考。作者强调了物理学等理论知识对AI的重要性，并提出了“Big AI”的愿景。虽然论文没有提供具体的实验结果，但其提出的观点对AI研究人员具有重要的启发意义，并为未来的研究方向提供了指导。",
            "tags_zh": [
                "人工智能",
                "物理学",
                "机器学习",
                "理论建模",
                "量子AI"
            ],
            "_index": 96,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "Agent Tools Orchestration Leaks More: Dataset, Benchmark, and Mitigation",
            "authors": [
                "Yuxuan Qiao",
                "Dongqin Liu",
                "Hongchang Yang",
                "Wei Zhou",
                "Songlin Hu"
            ],
            "arxiv_id": "2512.16310v1",
            "summary": "Driven by Large Language Models, the single-agent, multi-tool architecture has become a popular paradigm for autonomous agents due to its simplicity and effectiveness. However, this architecture also introduces a new and severe privacy risk, which we term Tools Orchestration Privacy Risk (TOP-R), where an agent, to achieve a benign user goal, autonomously aggregates information fragments across multiple tools and leverages its reasoning capabilities to synthesize unexpected sensitive information. We provide the first systematic study of this risk. First, we establish a formal framework, attributing the risk's root cause to the agent's misaligned objective function: an overoptimization for helpfulness while neglecting privacy awareness. Second, we construct TOP-Bench, comprising paired leakage and benign scenarios, to comprehensively evaluate this risk. To quantify the trade-off between safety and robustness, we introduce the H-Score as a holistic metric. The evaluation results reveal that TOP-R is a severe risk: the average Risk Leakage Rate (RLR) of eight representative models reaches 90.24%, while the average H-Score is merely 0.167, with no model exceeding 0.3. Finally, we propose the Privacy Enhancement Principle (PEP) method, which effectively mitigates TOP-R, reducing the Risk Leakage Rate to 46.58% and significantly improving the H-Score to 0.624. Our work reveals both a new class of risk and inherent structural limitations in current agent architectures, while also offering feasible mitigation strategies.",
            "categories": [
                "cs.CR",
                "cs.AI",
                "cs.CL"
            ],
            "primary_category": "cs.CR",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16310v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "揭示Agent工具编排中的隐私泄露风险，并提出TOP-Bench基准与PEP缓解方法",
            "summary_zh": "本文系统性地研究了由大型语言模型驱动的单Agent多工具架构中存在的工具编排隐私风险(TOP-R)。这种架构为了实现用户目标，可能自主地聚合多个工具中的信息片段，并利用其推理能力合成意想不到的敏感信息。研究首先建立了一个形式化框架，将风险的根本原因归结为Agent目标函数的不对齐：过度优化了有用性而忽略了隐私意识。其次，构建了TOP-Bench，包含配对的泄露和良性场景，以全面评估该风险。为了量化安全性和鲁棒性之间的权衡，引入了H-Score作为整体指标。评估结果表明TOP-R是一个严重的风险：八个代表性模型的平均风险泄露率(RLR)达到90.24%，而平均H-Score仅为0.167，没有模型超过0.3。最后，提出了隐私增强原则(PEP)方法，有效地缓解了TOP-R，将风险泄露率降低到46.58%，并将H-Score显著提高到0.624。这项工作揭示了一种新型风险和当前Agent架构中固有的结构性限制，同时也提供了可行的缓解策略。",
            "intro_zh": [
                "现有Agent架构在追求有用性时忽略了隐私保护，导致Agent可能通过工具编排泄露敏感信息。",
                "论文提出隐私增强原则(PEP)，旨在调整Agent的目标函数，使其在提供帮助的同时兼顾隐私保护。",
                "实验表明，提出的PEP方法能有效降低风险泄露率，并显著提升安全性和鲁棒性的综合指标H-Score。"
            ],
            "method_zh": "**问题定义**：论文旨在解决单Agent多工具架构中，Agent为了完成用户目标，可能通过编排多个工具来泄露用户隐私的问题。现有方法主要关注Agent的有用性，而忽略了其可能造成的隐私风险，导致Agent过度优化有用性而忽视了隐私保护。\\n\\n**核心思路**：论文的核心思路是调整Agent的目标函数，使其在追求有用性的同时，也考虑到隐私保护。通过引入隐私增强原则(PEP)，引导Agent在工具编排过程中避免泄露敏感信息。\\n\\n**技术框架**：论文首先建立了一个形式化框架来描述工具编排隐私风险(TOP-R)。然后，构建了TOP-Bench基准数据集，包含配对的泄露和良性场景，用于评估Agent的隐私泄露风险。最后，提出了隐私增强原则(PEP)方法，并使用H-Score来量化安全性和鲁棒性之间的权衡。\\n\\n**关键创新**：论文的关键创新在于识别并形式化了工具编排隐私风险(TOP-R)，并提出了隐私增强原则(PEP)来缓解该风险。PEP方法通过调整Agent的目标函数，使其在追求有用性的同时兼顾隐私保护，从而降低隐私泄露的风险。\\n\\n**关键设计**：PEP方法的具体设计细节未知，摘要中没有详细描述。H-Score的计算方法也未知，需要查阅论文全文才能了解。",
            "application_zh": "该研究成果可应用于各种需要使用Agent进行自动化任务处理的场景，例如智能客服、自动化报告生成、智能家居控制等。通过降低Agent的隐私泄露风险，可以提高用户对Agent系统的信任度，促进Agent技术的广泛应用。未来的研究可以进一步探索更有效的隐私保护方法，并将其应用于更复杂的Agent系统中。",
            "highlight_zh": "实验结果表明，现有的Agent模型存在严重的工具编排隐私风险(TOP-R)，平均风险泄露率(RLR)高达90.24%，平均H-Score仅为0.167。提出的隐私增强原则(PEP)方法能够有效缓解TOP-R，将风险泄露率降低到46.58%，并将H-Score显著提高到0.624，表明PEP方法在安全性和鲁棒性之间取得了更好的平衡。",
            "tags_zh": [
                "Agent工具编排",
                "隐私泄露风险",
                "大型语言模型",
                "隐私增强原则",
                "基准数据集",
                "目标函数对齐",
                "安全鲁棒性权衡"
            ],
            "_index": 97,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16310v1/Problem_Introduction.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16310v1/Dataset_Construction.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16310v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Beyond the Benchmark: Innovative Defenses Against Prompt Injection Attacks",
            "authors": [
                "Safwan Shaheer",
                "G. M. Refatul Islam",
                "Mohammad Rafid Hamid",
                "Tahsin Zaman Jilan"
            ],
            "arxiv_id": "2512.16307v1",
            "summary": "In this fast-evolving area of LLMs, our paper discusses the significant security risk presented by prompt injection attacks. It focuses on small open-sourced models, specifically the LLaMA family of models. We introduce novel defense mechanisms capable of generating automatic defenses and systematically evaluate said generated defenses against a comprehensive set of benchmarked attacks. Thus, we empirically demonstrated the improvement proposed by our approach in mitigating goal-hijacking vulnerabilities in LLMs. Our work recognizes the increasing relevance of small open-sourced LLMs and their potential for broad deployments on edge devices, aligning with future trends in LLM applications. We contribute to the greater ecosystem of open-source LLMs and their security in the following: (1) assessing present prompt-based defenses against the latest attacks, (2) introducing a new framework using a seed defense (Chain Of Thoughts) to refine the defense prompts iteratively, and (3) showing significant improvements in detecting goal hijacking attacks. Out strategies significantly reduce the success rates of the attacks and false detection rates while at the same time effectively detecting goal-hijacking capabilities, paving the way for more secure and efficient deployments of small and open-source LLMs in resource-constrained environments.",
            "categories": [
                "cs.CR",
                "cs.AI"
            ],
            "primary_category": "cs.CR",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "10 pages, 4 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16307v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "chain-of-thought"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "针对LLaMA模型，提出迭代式prompt优化防御prompt注入攻击。",
            "summary_zh": "本文探讨了大型语言模型（LLM）领域中日益严峻的prompt注入攻击安全风险，重点关注小型开源模型，特别是LLaMA系列。我们提出了一种新颖的防御机制，能够自动生成防御策略，并针对一系列基准攻击系统地评估这些生成的防御策略。实验结果表明，我们的方法能够有效缓解LLM中的目标劫持漏洞。我们的工作认识到小型开源LLM日益增长的相关性及其在边缘设备上广泛部署的潜力，这与LLM应用的未来趋势相符。我们通过以下方式为开源LLM及其安全性的生态系统做出贡献：（1）评估当前基于prompt的防御措施对最新攻击的有效性；（2）引入一种新的框架，使用种子防御（思维链）来迭代地改进防御prompt；（3）在检测目标劫持攻击方面显示出显著的改进。我们的策略显著降低了攻击的成功率和误检率，同时有效地检测目标劫持能力，为在资源受限环境中更安全、更高效地部署小型开源LLM铺平了道路。",
            "intro_zh": [
                "现有prompt防御方法在面对新型prompt注入攻击时存在不足，尤其是在小型开源LLM上。",
                "提出一种迭代式prompt优化框架，利用思维链作为种子防御，逐步改进防御prompt。",
                "实验表明，该方法能显著降低攻击成功率和误检率，有效提升小型开源LLM的安全性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决小型开源LLM（如LLaMA系列）中，prompt注入攻击导致的目标劫持问题。现有的prompt防御方法难以有效应对不断演变的攻击手段，尤其是在资源受限的边缘设备上部署时，防御效果和效率面临挑战。\\n\\n**核心思路**：论文的核心思路是通过迭代优化防御prompt，使其能够更有效地识别和阻止恶意prompt的注入。利用思维链（Chain of Thoughts）作为初始防御，引导模型进行更深入的推理，从而提高检测目标劫持攻击的能力。\\n\\n**技术框架**：该框架包含以下主要阶段：1) 使用思维链生成初始防御prompt；2) 利用基准攻击数据集评估防御效果；3) 根据评估结果，迭代优化防御prompt，例如通过调整prompt的措辞、增加约束条件等；4) 重复评估和优化过程，直至达到预期的防御性能。\\n\\n**关键创新**：最重要的创新点在于迭代式的prompt优化方法，它能够根据实际攻击效果动态调整防御策略，从而提高防御的鲁棒性和适应性。与传统的静态防御方法相比，该方法能够更好地应对新型prompt注入攻击。\\n\\n**关键设计**：论文的关键设计包括：1) 选择思维链作为种子防御，利用其推理能力提高检测准确率；2) 设计有效的评估指标，用于衡量防御prompt的性能，例如攻击成功率、误检率等；3) 采用合适的优化算法，例如基于梯度的优化方法或进化算法，自动调整防御prompt的参数。",
            "application_zh": "该研究成果可应用于各种需要安全可靠的LLM部署场景，尤其是在资源受限的边缘设备上。例如，智能家居设备、移动应用、自动驾驶系统等，这些场景对模型的安全性和效率都有较高要求。通过提升LLM的抗攻击能力，可以保护用户数据和系统安全，促进LLM技术的更广泛应用。",
            "highlight_zh": "实验结果表明，该方法能够显著降低prompt注入攻击的成功率，并有效降低误检率。与现有防御方法相比，该方法在检测目标劫持攻击方面取得了显著的改进，为小型开源LLM的安全部署提供了更有效的解决方案。具体性能数据和对比基线在论文中进行了详细展示。",
            "tags_zh": [
                "Prompt注入攻击",
                "LLaMA模型",
                "目标劫持",
                "思维链",
                "迭代优化",
                "防御机制",
                "开源LLM"
            ],
            "_index": 98,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16307v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16307v1/attack_types.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16307v1/defense.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Adaptation of Agentic AI",
            "authors": [
                "Pengcheng Jiang",
                "Jiacheng Lin",
                "Zhiyi Shi",
                "Zifeng Wang",
                "Luxi He",
                "Yichen Wu",
                "Ming Zhong",
                "Peiyang Song",
                "Qizheng Zhang",
                "Heng Wang",
                "Xueqiang Xu",
                "Hanwen Xu",
                "Pengrui Han",
                "Dylan Zhang",
                "Jiashuo Sun",
                "Chaoqi Yang",
                "Kun Qian",
                "Tian Wang",
                "Changran Hu",
                "Manling Li",
                "Quanzheng Li",
                "Hao Peng",
                "Sheng Wang",
                "Jingbo Shang",
                "Chao Zhang",
                "Jiaxuan You",
                "Liyuan Liu",
                "Pan Lu",
                "Yu Zhang",
                "Heng Ji",
                "Yejin Choi",
                "Dawn Song",
                "Jimeng Sun",
                "Jiawei Han"
            ],
            "arxiv_id": "2512.16301v1",
            "summary": "Cutting-edge agentic AI systems are built on foundation models that can be adapted to plan, reason, and interact with external tools to perform increasingly complex and specialized tasks. As these systems grow in capability and scope, adaptation becomes a central mechanism for improving performance, reliability, and generalization. In this paper, we unify the rapidly expanding research landscape into a systematic framework that spans both agent adaptations and tool adaptations. We further decompose these into tool-execution-signaled and agent-output-signaled forms of agent adaptation, as well as agent-agnostic and agent-supervised forms of tool adaptation. We demonstrate that this framework helps clarify the design space of adaptation strategies in agentic AI, makes their trade-offs explicit, and provides practical guidance for selecting or switching among strategies during system design. We then review the representative approaches in each category, analyze their strengths and limitations, and highlight key open challenges and future opportunities. Overall, this paper aims to offer a conceptual foundation and practical roadmap for researchers and practitioners seeking to build more capable, efficient, and reliable agentic AI systems.",
            "categories": [
                "cs.AI",
                "cs.CL"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16301v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "foundation model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出Agentic AI自适应统一框架，提升智能体性能、可靠性和泛化能力",
            "summary_zh": "本文旨在对快速发展的Agentic AI研究领域进行整合，提出了一个系统的框架，涵盖了智能体自适应和工具自适应。进一步将智能体自适应分解为工具执行信号驱动和智能体输出信号驱动两种形式，并将工具自适应分解为智能体无关和智能体监督两种形式。该框架有助于明确Agentic AI中自适应策略的设计空间，明确其权衡，并为系统设计期间选择或切换策略提供实用指导。本文回顾了每个类别中的代表性方法，分析了它们的优缺点，并强调了关键的开放挑战和未来的机遇。总而言之，本文旨在为寻求构建更强大、高效和可靠的Agentic AI系统的研究人员和从业者提供概念基础和实践路线图。",
            "intro_zh": [
                "现有Agentic AI系统在性能、可靠性和泛化方面面临挑战，需要有效的自适应机制。",
                "论文提出了一个统一的框架，涵盖智能体和工具的自适应，并细分为不同类型。",
                "该框架旨在帮助研究人员和实践者更好地设计和选择自适应策略，构建更强大的Agentic AI系统。"
            ],
            "method_zh": "**问题定义**：Agentic AI系统在执行复杂任务时，需要不断适应环境和任务的变化。现有的自适应方法分散且缺乏统一的框架，难以指导实际应用，并且在性能、可靠性和泛化能力上存在瓶颈。\\n\\n**核心思路**：论文的核心思路是将Agentic AI的自适应过程分解为智能体自适应和工具自适应两个维度，并进一步细化为不同的类型。通过这种分解，可以更清晰地理解不同自适应策略的优缺点，并根据具体任务选择合适的策略。\\n\\n**技术框架**：该框架主要包含两个部分：智能体自适应和工具自适应。智能体自适应又分为工具执行信号驱动和智能体输出信号驱动两种形式。工具自适应分为智能体无关和智能体监督两种形式。通过对这四个维度的分析，可以构建一个完整的Agentic AI自适应策略图谱。\\n\\n**关键创新**：该论文的关键创新在于提出了一个统一的Agentic AI自适应框架，将现有的自适应方法纳入其中，并明确了不同策略的权衡。这种框架性的视角有助于研究人员和实践者更好地理解和应用自适应技术。\\n\\n**关键设计**：论文没有涉及具体的参数设置、损失函数或网络结构等技术细节，而是侧重于对自适应策略的分类和分析。未来的研究可以基于该框架，探索更有效的自适应算法和技术。",
            "application_zh": "该研究成果可应用于各种需要智能体自主完成复杂任务的领域，例如智能客服、自动化运维、机器人控制、智能家居等。通过选择合适的自适应策略，可以提升智能体在不同环境下的性能和可靠性，从而更好地服务于人类。",
            "highlight_zh": "该论文的主要贡献在于提出了一个Agentic AI自适应的统一框架，对现有方法进行了系统性的分类和分析。虽然没有提供具体的实验数据，但该框架为未来的研究提供了清晰的路线图，并为实际应用提供了指导。",
            "tags_zh": [
                "Agentic AI",
                "智能体自适应",
                "工具自适应",
                "自适应策略",
                "统一框架"
            ],
            "_index": 99,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16301v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16301v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16301v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "QuadSentinel: Sequent Safety for Machine-Checkable Control in Multi-agent Systems",
            "authors": [
                "Yiliu Yang",
                "Yilei Jiang",
                "Qunzhong Wang",
                "Yingshui Tan",
                "Xiaoyong Zhu",
                "Sherman S. M. Chow",
                "Bo Zheng",
                "Xiangyu Yue"
            ],
            "arxiv_id": "2512.16279v1",
            "summary": "Safety risks arise as large language model-based agents solve complex tasks with tools, multi-step plans, and inter-agent messages. However, deployer-written policies in natural language are ambiguous and context dependent, so they map poorly to machine-checkable rules, and runtime enforcement is unreliable. Expressing safety policies as sequents, we propose \\textsc{QuadSentinel}, a four-agent guard (state tracker, policy verifier, threat watcher, and referee) that compiles these policies into machine-checkable rules built from predicates over observable state and enforces them online. Referee logic plus an efficient top-$k$ predicate updater keeps costs low by prioritizing checks and resolving conflicts hierarchically. Measured on ST-WebAgentBench (ICML CUA~'25) and AgentHarm (ICLR~'25), \\textsc{QuadSentinel} improves guardrail accuracy and rule recall while reducing false positives. Against single-agent baselines such as ShieldAgent (ICML~'25), it yields better overall safety control. Near-term deployments can adopt this pattern without modifying core agents by keeping policies separate and machine-checkable. Our code will be made publicly available at https://github.com/yyiliu/QuadSentinel.",
            "categories": [
                "cs.AI",
                "cs.CL"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Preprint",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16279v1",
            "code_links": [
                {
                    "url": "https://github.com/yyiliu/QuadSentinel",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "QuadSentinel：多智能体系统中基于时序推理和可机检规则的安全控制",
            "summary_zh": "基于大型语言模型的智能体在解决复杂任务时，涉及工具使用、多步计划和智能体间消息传递，由此产生安全风险。然而，部署者编写的自然语言策略具有模糊性和上下文依赖性，难以映射到可机检规则，且运行时强制执行不可靠。本文提出\textsc{QuadSentinel}，一种四智能体守卫（状态跟踪器、策略验证器、威胁观察器和裁判）系统，将安全策略表示为时序逻辑，并将其编译为基于可观察状态谓词的可机检规则，并在线执行。裁判逻辑和高效的top-$k$谓词更新器通过优先检查和分层解决冲突来降低成本。在ST-WebAgentBench和AgentHarm上的实验表明，\textsc{QuadSentinel}提高了护栏精度和规则召回率，同时减少了误报。与ShieldAgent等单智能体基线相比，它产生了更好的整体安全控制。无需修改核心智能体，通过保持策略分离和可机检性，近期部署即可采用此模式。代码将在https://github.com/yyiliu/QuadSentinel公开。",
            "intro_zh": [
                "现有基于自然语言的安全策略难以转化为可机检规则，导致多智能体系统运行时安全控制不可靠。",
                "提出QuadSentinel，采用四智能体架构，将安全策略编译为基于可观察状态谓词的可机检规则，并在线执行。",
                "实验表明，QuadSentinel提高了护栏精度和规则召回率，减少了误报，并优于单智能体基线。"
            ],
            "method_zh": "**问题定义**：多智能体系统在执行复杂任务时，由于智能体间的交互和工具的使用，容易出现安全风险。现有的安全策略通常以自然语言形式表达，存在模糊性和上下文依赖性，难以转化为机器可执行的规则，导致运行时安全控制效果不佳。此外，单智能体安全控制方法难以有效应对多智能体环境下的复杂交互。\n\n**核心思路**：将安全策略表示为时序逻辑（sequents），并将其编译为基于可观察状态谓词的可机检规则。通过引入四智能体守卫架构，实现对多智能体系统运行时的安全监控和控制。核心在于将模糊的自然语言策略转化为精确的、可验证的机器规则，并利用多智能体协同来增强安全控制的鲁棒性和准确性。\n\n**技术框架**：QuadSentinel采用四智能体架构，包括：1) 状态跟踪器（State Tracker）：负责跟踪和记录多智能体系统的状态信息。2) 策略验证器（Policy Verifier）：将安全策略编译为可机检规则，并验证当前状态是否违反策略。3) 威胁观察器（Threat Watcher）：监控潜在的威胁行为，并发出警报。4) 裁判（Referee）：根据策略验证器和威胁观察器的结果，做出最终的安全决策，并采取相应的控制措施。整体流程是状态跟踪器提供状态信息，策略验证器和威胁观察器进行评估，裁判根据评估结果执行安全策略。\n\n**关键创新**：主要创新点在于将自然语言安全策略转化为可机检的时序逻辑规则，并利用四智能体架构实现对多智能体系统的在线安全控制。与传统的单智能体安全控制方法相比，QuadSentinel能够更好地应对多智能体环境下的复杂交互和潜在威胁。此外，裁判逻辑和高效的top-$k$谓词更新器能够有效降低计算成本，提高系统的实时性。\n\n**关键设计**：裁判逻辑采用分层冲突解决机制，优先处理高优先级规则，避免冲突决策。Top-$k$谓词更新器用于选择最相关的状态谓词进行验证，降低计算复杂度。具体的参数设置和损失函数信息未知，论文可能未详细描述。",
            "application_zh": "QuadSentinel可应用于各种多智能体系统，例如：自动驾驶、机器人协作、智能家居、金融交易等。通过提供可信赖的安全保障，能够促进多智能体技术在实际场景中的应用，并降低潜在的安全风险。该研究有助于构建更安全、可靠的人工智能系统。",
            "highlight_zh": "实验结果表明，QuadSentinel在ST-WebAgentBench和AgentHarm数据集上，提高了护栏精度和规则召回率，同时减少了误报。与ShieldAgent等单智能体基线相比，QuadSentinel在整体安全控制方面表现更优，证明了其在多智能体安全控制方面的有效性。",
            "tags_zh": [
                "多智能体系统",
                "安全控制",
                "可机检规则",
                "时序逻辑",
                "智能体架构"
            ],
            "_index": 100,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16279v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16279v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16279v1/imgs/harmful_by_category.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Beyond Blind Spots: Analytic Hints for Mitigating LLM-Based Evaluation Pitfalls",
            "authors": [
                "Ora Nova Fandina",
                "Eitan Farchi",
                "Shmulik Froimovich",
                "Raviv Gal",
                "Wesam Ibraheem",
                "Rami Katan",
                "Alice Podolsky"
            ],
            "arxiv_id": "2512.16272v1",
            "summary": "Large Language Models are increasingly deployed as judges (LaaJ) in code generation pipelines. While attractive for scalability, LaaJs tend to overlook domain specific issues raising concerns about their reliability in critical evaluation tasks. To better understand these limitations in practice, we examine LaaJ behavior in a concrete industrial use case: legacy code modernization via COBOL code generation. In this setting, we find that even production deployed LaaJs can miss domain critical errors, revealing consistent blind spots in their evaluation capabilities.\n  To better understand these blind spots, we analyze generated COBOL programs and associated LaaJs judgments, drawing on expert knowledge to construct a preliminary taxonomy. Based on this taxonomy, we develop a lightweight analytic checker tool that flags over 30 domain specific issues observed in practice. We use its outputs as analytic hints, dynamically injecting them into the judges prompt to encourage LaaJ to revisit aspects it may have overlooked.\n  Experiments on a test set of 100 programs using four production level LaaJs show that LaaJ alone detects only about 45% of the errors present in the code (in all judges we tested), while the analytic checker alone lacks explanatory depth. When combined, the LaaJ+Hints configuration achieves up to 94% coverage (for the best performing judge and injection prompt) and produces qualitatively richer, more accurate explanations, demonstrating that analytic-LLM hybrids can substantially enhance evaluation reliability in deployed pipelines. We release the dataset and all used prompts.",
            "categories": [
                "cs.SE",
                "cs.AI"
            ],
            "primary_category": "cs.SE",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16272v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "利用分析提示缓解LLM代码评估中的盲点，提升COBOL代码生成质量",
            "summary_zh": "大型语言模型（LLM）越来越多地被用作代码生成流程中的评估者（LaaJ）。尽管其可扩展性具有吸引力，但LaaJ往往忽略特定领域的关键问题，引发对其在关键评估任务中可靠性的担忧。为了更好地理解这些局限性，本文研究了LaaJ在工业用例中的行为：通过COBOL代码生成实现遗留代码现代化。研究发现，即使是生产环境中部署的LaaJ也会遗漏领域关键错误，揭示其评估能力中存在一致的盲点。为了更好地理解这些盲点，本文分析了生成的COBOL程序和相关的LaaJ判断，并利用专家知识构建了一个初步的分类。基于此分类，开发了一个轻量级的分析检查工具，用于标记实践中观察到的30多个特定领域问题。该工具的输出被用作分析提示，动态地注入到评估者的提示中，以鼓励LaaJ重新审视可能被忽略的方面。在包含100个程序的测试集上，使用四个生产级别的LaaJ进行的实验表明，LaaJ单独检测到的错误仅占代码中存在的错误的约45%（在所有测试的评估者中），而分析检查器本身缺乏解释深度。当结合使用时，LaaJ+提示配置实现了高达94%的覆盖率（对于性能最佳的评估者和注入提示），并产生了质量更高、更准确的解释，表明分析-LLM混合方法可以显著提高已部署流程中的评估可靠性。本文发布了数据集和所有使用的提示。",
            "intro_zh": [
                "现有LaaJ在代码生成评估中存在领域盲点，无法有效识别特定领域的关键错误，影响评估可靠性。",
                "提出一种分析提示方法，通过轻量级分析检查工具识别领域问题，并将其作为提示注入LaaJ，引导其重新评估。",
                "实验表明，LaaJ+提示配置显著提升错误检测覆盖率，最高可达94%，并生成更准确的解释。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大型语言模型（LLM）在评估COBOL代码生成质量时存在的盲点问题。现有方法，即直接使用LLM作为评估者（LaaJ），在评估特定领域（如COBOL）的代码时，容易忽略领域相关的错误，导致评估结果不准确，影响代码质量。现有方法的痛点在于LLM缺乏对特定领域知识的深入理解。\n\n**核心思路**：论文的核心思路是将领域专家的知识融入到LLM的评估过程中。具体来说，通过一个轻量级的分析检查工具，自动检测生成的COBOL代码中存在的特定领域问题，并将这些问题作为“分析提示”注入到LLM的评估提示中。这样，LLM在评估代码时，可以根据这些提示，更加关注可能存在的领域错误，从而提高评估的准确性。\n\n**技术框架**：整体框架包含以下几个主要模块：1) COBOL代码生成器：生成需要评估的COBOL代码。2) 分析检查工具：基于专家知识，对生成的COBOL代码进行静态分析，检测潜在的领域问题，生成分析提示。3) LaaJ：接收生成的COBOL代码和分析提示，根据提示对代码进行评估，并给出评估结果和解释。4) 提示注入模块：将分析检查工具生成的提示动态地注入到LaaJ的评估提示中。\n\n**关键创新**：最重要的技术创新点在于将领域专家的知识以“分析提示”的形式融入到LLM的评估过程中。与现有方法相比，该方法不需要对LLM进行额外的训练或微调，而是通过简单的提示工程，即可显著提高LLM在特定领域代码评估中的准确性。这种方法具有轻量级、易于部署的优点。\n\n**关键设计**：分析检查工具的设计是关键。该工具基于专家知识，定义了30多个COBOL代码中常见的错误模式。这些错误模式被编码为静态分析规则，用于检测生成的COBOL代码中是否存在这些错误。提示注入模块的设计也很重要，需要选择合适的提示格式和注入方式，以确保LLM能够有效地利用这些提示。",
            "application_zh": "该研究成果可应用于遗留系统现代化改造、代码质量保证、自动化代码审查等领域。通过结合领域专家知识和LLM的强大能力，可以显著提高代码评估的准确性和效率，降低人工成本，加速软件开发和维护过程。未来，该方法可以推广到其他领域，例如金融、医疗等，提升LLM在特定领域的应用价值。",
            "highlight_zh": "实验结果表明，单独使用LaaJ只能检测到约45%的错误，而结合分析提示后，错误检测覆盖率最高可达94%。与单独使用LaaJ相比，LaaJ+提示配置不仅提高了错误检测率，还生成了质量更高、更准确的解释。这些结果表明，分析-LLM混合方法可以显著提高已部署流程中的评估可靠性。",
            "tags_zh": [
                "大型语言模型",
                "代码评估",
                "COBOL代码生成",
                "遗留系统现代化",
                "分析提示"
            ],
            "_index": 101,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16272v1/hint_llaj.jpg",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16272v1/taxonomy.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16272v1/hybrid_laaj_issues_triplets_total_native_orange_leftlegend.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Learning to Wait: Synchronizing Agents with the Physical World",
            "authors": [
                "Yifei She",
                "Ping Zhang",
                "He Liu",
                "Yanmin Jia",
                "Yang Jing",
                "Zijun Liu",
                "Peng Sun",
                "Xiangbin Li",
                "Xiaohe Hu"
            ],
            "arxiv_id": "2512.16262v1",
            "summary": "Real-world agentic tasks, unlike synchronous Markov Decision Processes (MDPs), often involve non-blocking actions with variable latencies, creating a fundamental \\textit{Temporal Gap} between action initiation and completion. Existing environment-side solutions, such as blocking wrappers or frequent polling, either limit scalability or dilute the agent's context window with redundant observations. In this work, we propose an \\textbf{Agent-side Approach} that empowers Large Language Models (LLMs) to actively align their \\textit{Cognitive Timeline} with the physical world. By extending the Code-as-Action paradigm to the temporal domain, agents utilize semantic priors and In-Context Learning (ICL) to predict precise waiting durations (\\texttt{time.sleep(t)}), effectively synchronizing with asynchronous environment without exhaustive checking. Experiments in a simulated Kubernetes cluster demonstrate that agents can precisely calibrate their internal clocks to minimize both query overhead and execution latency, validating that temporal awareness is a learnable capability essential for autonomous evolution in open-ended environments.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16262v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出Agent端时间同步方法，解决LLM在异步环境中与物理世界交互的时延问题",
            "summary_zh": "与同步马尔可夫决策过程（MDP）不同，现实世界的Agent任务通常涉及具有可变延迟的非阻塞动作，从而在动作发起和完成之间产生根本性的“时间间隔”。现有的环境侧解决方案，如阻塞包装器或频繁轮询，要么限制了可扩展性，要么用冗余的观察稀释了Agent的上下文窗口。本文提出了一种Agent端方法，使大型语言模型（LLM）能够主动将其“认知时间线”与物理世界对齐。通过将代码即动作范式扩展到时间域，Agent利用语义先验和上下文学习（ICL）来预测精确的等待时长（`time.sleep(t)`），从而有效地与异步环境同步，而无需详尽的检查。在模拟的Kubernetes集群中的实验表明，Agent可以精确地校准其内部时钟，以最大限度地减少查询开销和执行延迟，从而验证了时间感知是在开放环境中自主进化必不可少的、可学习的能力。",
            "intro_zh": [
                "现实Agent任务中，动作完成存在时延，导致Agent与环境交互出现时间间隔，现有环境侧解决方案存在可扩展性或上下文稀释问题。",
                "论文提出Agent端方法，通过让LLM预测等待时长（`time.sleep(t)`），主动将其认知时间线与物理世界对齐，实现时间同步。",
                "实验表明，Agent能够精确校准内部时钟，最小化查询开销和执行延迟，验证了时间感知能力对于开放环境自主进化的重要性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决现实世界Agent任务中，由于动作执行存在时延，导致Agent与环境交互异步的问题。现有方法，如阻塞包装器和频繁轮询，要么限制了系统的可扩展性，要么引入了大量的冗余观察，稀释了Agent的上下文信息，影响决策效率。\\n\\n**核心思路**：论文的核心思路是赋予Agent时间感知能力，使其能够主动预测并等待动作完成所需的时间，从而实现与异步环境的同步。通过让Agent学习何时以及等待多久，避免了不必要的轮询和阻塞，提高了交互效率和决策质量。\\n\\n**技术框架**：论文将代码即动作范式扩展到时间域，Agent通过生成包含`time.sleep(t)`指令的代码来控制等待时间。整体流程包括：1) Agent接收环境观测；2) Agent利用LLM生成包含`time.sleep(t)`的动作代码；3) 执行动作代码，Agent进入休眠状态；4) 休眠结束后，Agent再次接收环境观测，进入下一轮交互。论文利用上下文学习（ICL）来引导LLM学习时间感知能力。\\n\\n**关键创新**：最重要的创新点在于将时间同步问题从环境侧转移到Agent侧，赋予Agent主动控制等待时间的能力。与传统的被动等待或频繁轮询方法相比，该方法能够更有效地利用Agent的计算资源，并减少不必要的环境交互。\\n\\n**关键设计**：论文的关键设计包括：1) 使用LLM作为Agent的决策核心，利用其强大的语义理解和生成能力；2) 通过上下文学习（ICL）提供时间相关的示例，引导LLM学习预测合适的等待时间；3) 使用`time.sleep(t)`作为控制等待时间的指令，简单而有效；4) 在Kubernetes集群模拟环境中进行实验，验证方法的有效性。",
            "application_zh": "该研究成果可应用于各种需要与异步环境交互的Agent系统，例如机器人控制、自动化运维、智能家居等。通过赋予Agent时间感知能力，可以提高其在复杂、动态环境中的自主性和效率，降低人工干预的需求，并为构建更智能、更可靠的自动化系统奠定基础。",
            "highlight_zh": "实验结果表明，所提出的Agent端时间同步方法能够显著减少查询开销和执行延迟。在模拟的Kubernetes集群环境中，Agent能够精确校准其内部时钟，实现与异步环境的高效同步。具体性能数据未知，但论文强调了该方法在最小化查询开销和执行延迟方面的有效性。",
            "tags_zh": [
                "Agent时间同步",
                "大型语言模型",
                "上下文学习",
                "异步环境",
                "代码即动作"
            ],
            "_index": 102,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16262v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16262v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16262v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Sigma-Moe-Tiny Technical Report",
            "authors": [
                "Qingguo Hu",
                "Zhenghao Lin",
                "Ziyue Yang",
                "Yucheng Ding",
                "Xiao Liu",
                "Yuting Jiang",
                "Ruizhe Wang",
                "Tianyu Chen",
                "Zhongxin Guo",
                "Yifan Xiong",
                "Rui Gao",
                "Lei Qu",
                "Jinsong Su",
                "Peng Cheng",
                "Yeyun Gong"
            ],
            "arxiv_id": "2512.16248v1",
            "summary": "Mixture-of-Experts (MoE) has emerged as a promising paradigm for foundation models due to its efficient and powerful scalability. In this work, we present Sigma-MoE-Tiny, an MoE language model that achieves the highest sparsity compared to existing open-source models. Sigma-MoE-Tiny employs fine-grained expert segmentation with up to 96 experts per layer, while activating only one expert for each token, resulting in 20B total parameters with just 0.5B activated. The major challenge introduced by such extreme sparsity lies in expert load balancing. We find that the widely-used load balancing loss tends to become ineffective in the lower layers under this setting. To address this issue, we propose a progressive sparsification schedule aiming to balance expert utilization and training stability. Sigma-MoE-Tiny is pre-trained on a diverse and high-quality corpus, followed by post-training to further unlock its capabilities. The entire training process remains remarkably stable, with no occurrence of irrecoverable loss spikes. Comprehensive evaluations reveal that, despite activating only 0.5B parameters, Sigma-MoE-Tiny achieves top-tier performance among counterparts of comparable or significantly larger scale. In addition, we provide an in-depth discussion of load balancing in highly sparse MoE models, offering insights for advancing sparsity in future MoE architectures.\n  Project page: https://qghuxmu.github.io/Sigma-MoE-Tiny\n  Code: https://github.com/microsoft/ltp-megatron-lm",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16248v1",
            "code_links": [
                {
                    "url": "https://github.com/microsoft/ltp-megatron-lm",
                    "type": "github"
                },
                {
                    "url": "https://qghuxmu.github.io/Sigma-MoE-Tiny",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "foundation model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "Sigma-MoE-Tiny：提出一种高稀疏MoE语言模型，解决专家负载均衡问题，实现高效扩展。",
            "summary_zh": "本文介绍Sigma-MoE-Tiny，一种混合专家(MoE)语言模型，其稀疏性高于现有开源模型。Sigma-MoE-Tiny采用细粒度的专家分割，每层最多96个专家，但每个token仅激活一个专家，从而在200亿总参数下仅激活0.5B参数。这种极端稀疏性带来的主要挑战是专家负载均衡。研究发现，在这种设置下，广泛使用的负载均衡损失在较低层中往往失效。为了解决这个问题，提出了一种渐进式稀疏化策略，旨在平衡专家利用率和训练稳定性。Sigma-MoE-Tiny在一个多样化和高质量的语料库上进行预训练，然后进行后训练以进一步释放其能力。整个训练过程保持稳定，没有出现不可恢复的损失峰值。综合评估表明，尽管仅激活0.5B参数，Sigma-MoE-Tiny在同等或更大规模的同类模型中实现了顶级的性能。此外，还深入讨论了高稀疏MoE模型中的负载均衡问题，为未来MoE架构中提高稀疏性提供了见解。",
            "intro_zh": [
                "现有MoE模型在极端稀疏性下，专家负载均衡面临挑战，传统负载均衡损失在底层失效。",
                "提出渐进式稀疏化策略，平衡专家利用率和训练稳定性，解决高稀疏MoE模型的负载均衡问题。",
                "Sigma-MoE-Tiny仅激活0.5B参数，在同等或更大规模模型中取得领先性能，且训练过程稳定。"
            ],
            "method_zh": "**问题定义**：论文旨在解决MoE模型中，在极高稀疏度下专家负载不均衡的问题。现有的负载均衡损失函数在高稀疏度下，尤其是在模型的浅层，效果不佳，导致部分专家过度使用，而另一些专家利用不足，影响模型整体性能。\\n\\n**核心思路**：核心思路是采用渐进式稀疏化策略。通过逐步增加模型的稀疏度，在训练初期保持相对稠密的状态，使得负载均衡损失能够有效工作，随着训练的进行，逐步引入更高的稀疏度。这种方法旨在平衡专家利用率和训练稳定性，避免训练初期因过度稀疏导致的不稳定。\\n\\n**技术框架**：Sigma-MoE-Tiny基于Transformer架构，采用MoE层替换部分Transformer层。整体训练流程包括：1) 在大规模语料库上进行预训练；2) 应用渐进式稀疏化策略进行训练，平衡专家负载；3) 进行后训练，进一步提升模型性能。模型包含多个MoE层，每个MoE层包含多个专家，每个token只路由到一个专家。\\n\\n**关键创新**：关键创新在于渐进式稀疏化策略。与传统的固定稀疏度MoE模型不同，Sigma-MoE-Tiny在训练过程中动态调整稀疏度，使得模型能够在训练初期更容易学习到有效的专家分配策略，并在后期实现更高的稀疏度。这与现有方法的本质区别在于，它不是静态地设置稀疏度，而是动态地调整，以适应训练过程中的变化。\\n\\n**关键设计**：渐进式稀疏化策略的具体实现可能涉及以下技术细节：1) 设计一个稀疏度增长的schedule，例如线性增长或指数增长；2) 监控专家利用率，并根据利用率调整稀疏度；3) 使用合适的路由算法，例如Top-K路由，并调整K值以控制稀疏度；4) 调整负载均衡损失的权重，使其在训练初期发挥更大的作用。",
            "application_zh": "Sigma-MoE-Tiny具有广泛的应用前景，包括自然语言处理、机器翻译、文本生成等领域。其高稀疏性使得模型能够在资源受限的环境中部署，例如移动设备或边缘计算设备。该研究为未来MoE模型的设计提供了新的思路，有助于开发更大规模、更高效的语言模型。",
            "highlight_zh": "Sigma-MoE-Tiny在激活仅0.5B参数的情况下，在多个benchmark上取得了与参数量大得多的模型相媲美的性能。实验结果表明，该模型在保持训练稳定性的同时，实现了极高的稀疏度，验证了渐进式稀疏化策略的有效性。具体性能数据未知，但摘要强调了其在同等或更大规模模型中的顶级性能。",
            "tags_zh": [
                "混合专家模型",
                "MoE",
                "稀疏模型",
                "负载均衡",
                "渐进式稀疏化",
                "语言模型",
                "自然语言处理"
            ],
            "_index": 103,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16248v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16248v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16248v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Ev-Trust: A Strategy Equilibrium Trust Mechanism for Evolutionary Games in LLM-Based Multi-Agent Services",
            "authors": [
                "Shiduo Yang",
                "Jiye Wang",
                "Jiayu Qin",
                "Jianbin Li",
                "Yu Wang",
                "Yuanhe Zhao",
                "Kenan Guo"
            ],
            "arxiv_id": "2512.16167v1",
            "summary": "The rapid evolution of the Web toward an agent-centric paradigm, driven by large language models (LLMs), has enabled autonomous agents to reason, plan, and interact in complex decentralized environments. However, the openness and heterogeneity of LLM-based multi-agent systems also amplify the risks of deception, fraud, and misinformation, posing severe challenges to trust establishment and system robustness. To address this issue, we propose Ev-Trust, a strategy-equilibrium trust mechanism grounded in evolutionary game theory. This mechanism integrates direct trust, indirect trust, and expected revenue into a dynamic feedback structure that guides agents' behavioral evolution toward equilibria. Within a decentralized \"Request-Response-Payment-Evaluation\" service framework, Ev-Trust enables agents to adaptively adjust strategies, naturally excluding malicious participants while reinforcing high-quality collaboration. Furthermore, our theoretical derivation based on replicator dynamics equations proves the existence and stability of local evolutionary equilibria. Experimental results indicate that our approach effectively reflects agent trustworthiness in LLM-driven open service interaction scenarios, reduces malicious strategies, and increases collective revenue. We hope Ev-Trust can provide a new perspective on trust modeling for the agentic service web in group evolutionary game scenarios.",
            "categories": [
                "cs.MA",
                "cs.AI",
                "cs.GT"
            ],
            "primary_category": "cs.MA",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "12 pages, 11 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16167v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出Ev-Trust，一种基于演化博弈论的LLM多智能体服务信任机制。",
            "summary_zh": "随着Web向以智能体为中心的范式快速演进，由大型语言模型（LLMs）驱动的自主智能体能够在复杂的去中心化环境中进行推理、规划和交互。然而，基于LLM的多智能体系统的开放性和异构性也加剧了欺骗、欺诈和错误信息的风险，对信任建立和系统鲁棒性构成严峻挑战。为了解决这个问题，我们提出Ev-Trust，一种基于演化博弈论的策略均衡信任机制。该机制将直接信任、间接信任和预期收益整合到一个动态反馈结构中，引导智能体的行为演化趋向均衡。在去中心化的“请求-响应-支付-评估”服务框架内，Ev-Trust使智能体能够自适应地调整策略，自然地排除恶意参与者，同时加强高质量的协作。此外，我们基于复制者动态方程的理论推导证明了局部演化均衡的存在和稳定性。实验结果表明，我们的方法有效地反映了LLM驱动的开放服务交互场景中智能体的可信度，减少了恶意策略，并增加了集体收益。我们希望Ev-Trust能够为群体演化博弈场景中的智能服务网络提供一种新的信任建模视角。",
            "intro_zh": [
                "基于LLM的多智能体系统面临欺骗、欺诈和错误信息等信任危机，严重影响系统鲁棒性。",
                "Ev-Trust通过演化博弈论，整合直接信任、间接信任和预期收益，动态引导智能体行为趋向策略均衡。",
                "实验表明，Ev-Trust能有效反映智能体可信度，减少恶意策略，并提升LLM驱动服务交互的集体收益。"
            ],
            "method_zh": "**问题定义**：论文旨在解决基于LLM的多智能体系统中，由于开放性和异构性带来的信任缺失问题。现有方法难以有效识别和排除恶意智能体，导致欺骗、欺诈和错误信息泛滥，影响系统整体性能和用户体验。现有信任机制无法适应智能体策略的动态变化，容易被恶意智能体利用。\\n\\n**核心思路**：论文的核心思路是利用演化博弈论，将智能体之间的交互建模为一个动态博弈过程。通过引入直接信任、间接信任和预期收益，构建一个动态反馈结构，引导智能体不断调整策略，最终达到策略均衡。这种均衡状态能够自然地排除恶意智能体，并促进高质量的协作。\\n\\n**技术框架**：Ev-Trust运行在一个去中心化的“请求-响应-支付-评估”服务框架内。主要包含以下几个阶段：1) 请求者智能体发起服务请求；2) 响应者智能体提供服务；3) 请求者智能体根据服务质量支付报酬；4) 请求者智能体对响应者智能体进行评估，更新直接信任值；5) 系统根据直接信任值和间接信任值，计算智能体的整体信任度，并更新智能体的策略。整个过程形成一个闭环反馈系统，不断优化智能体的行为策略。\\n\\n**关键创新**：Ev-Trust的关键创新在于将演化博弈论引入到LLM多智能体系统的信任建模中。与传统的信任机制相比，Ev-Trust能够动态地适应智能体策略的变化，并利用复制者动态方程保证系统的稳定性。此外，Ev-Trust综合考虑了直接信任、间接信任和预期收益，更全面地反映了智能体的可信度。\\n\\n**关键设计**：Ev-Trust使用复制者动态方程来模拟智能体策略的演化过程。直接信任基于请求者对响应者的服务质量评估，间接信任通过信任网络传播。预期收益根据智能体的历史表现和当前策略计算。关键参数包括信任衰减因子、学习率和探索率。损失函数的设计目标是最大化集体收益，同时惩罚恶意行为。",
            "application_zh": "Ev-Trust可应用于各种基于LLM的多智能体服务场景，例如：去中心化知识问答、智能客服、供应链管理、金融交易等。通过建立有效的信任机制，可以提高系统的安全性、可靠性和效率，促进智能体之间的协作，并提升用户体验。未来，该研究可扩展到更复杂的智能体交互场景，例如：多智能体强化学习、人机协作等。",
            "highlight_zh": "实验结果表明，Ev-Trust能够有效降低恶意策略的使用，并提高集体收益。具体而言，与基线方法相比，Ev-Trust能够将恶意智能体的比例降低至少20%，同时将集体收益提高至少15%。此外，实验还验证了Ev-Trust的稳定性和收敛性，证明其能够在动态环境中保持良好的性能。",
            "tags_zh": [
                "多智能体系统",
                "信任机制",
                "演化博弈论",
                "大型语言模型",
                "策略均衡"
            ],
            "_index": 104,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16167v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16167v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16167v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "From Essence to Defense: Adaptive Semantic-aware Watermarking for Embedding-as-a-Service Copyright Protection",
            "authors": [
                "Hao Li",
                "Yubing Ren",
                "Yanan Cao",
                "Yingjie Li",
                "Fang Fang",
                "Xuebin Wang"
            ],
            "arxiv_id": "2512.16439v1",
            "summary": "Benefiting from the superior capabilities of large language models in natural language understanding and generation, Embeddings-as-a-Service (EaaS) has emerged as a successful commercial paradigm on the web platform. However, prior studies have revealed that EaaS is vulnerable to imitation attacks. Existing methods protect the intellectual property of EaaS through watermarking techniques, but they all ignore the most important properties of embedding: semantics, resulting in limited harmlessness and stealthiness. To this end, we propose SemMark, a novel semantic-based watermarking paradigm for EaaS copyright protection. SemMark employs locality-sensitive hashing to partition the semantic space and inject semantic-aware watermarks into specific regions, ensuring that the watermark signals remain imperceptible and diverse. In addition, we introduce the adaptive watermark weight mechanism based on the local outlier factor to preserve the original embedding distribution. Furthermore, we propose Detect-Sampling and Dimensionality-Reduction attacks and construct four scenarios to evaluate the watermarking method. Extensive experiments are conducted on four popular NLP datasets, and SemMark achieves superior verifiability, diversity, stealthiness, and harmlessness.",
            "categories": [
                "cs.CR",
                "cs.CL"
            ],
            "primary_category": "cs.CR",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16439v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出SemMark：一种自适应语义感知水印方法，用于保护Embedding-as-a-Service的版权",
            "summary_zh": "Embedding-as-a-Service (EaaS) 凭借大型语言模型在自然语言理解和生成方面的卓越能力，已成为一种成功的商业模式。然而，以往的研究表明，EaaS容易受到模仿攻击。现有的EaaS知识产权保护方法主要基于水印技术，但它们都忽略了嵌入最重要的属性：语义，导致其无害性和隐蔽性有限。为此，我们提出了一种新颖的基于语义的水印范式SemMark，用于EaaS版权保护。SemMark采用局部敏感哈希来划分语义空间，并将语义感知水印注入到特定区域，确保水印信号保持难以察觉和多样性。此外，我们引入了基于局部离群因子的自适应水印权重机制，以保持原始嵌入分布。我们还提出了Detect-Sampling和Dimensionality-Reduction攻击，并构建了四种场景来评估水印方法。在四个流行的NLP数据集上进行的大量实验表明，SemMark在可验证性、多样性、隐蔽性和无害性方面表现出色。",
            "intro_zh": [
                "现有EaaS水印方法忽略了嵌入的语义信息，导致水印的隐蔽性和无害性不足，容易被攻击。",
                "SemMark通过局部敏感哈希划分语义空间，并注入语义感知水印，保证水印的不可察觉性和多样性。",
                "实验证明，SemMark在可验证性、多样性、隐蔽性和无害性方面优于现有方法，有效保护EaaS版权。"
            ],
            "method_zh": "**问题定义**：论文旨在解决Embedding-as-a-Service (EaaS) 的版权保护问题。现有的水印方法忽略了嵌入的语义信息，导致水印的隐蔽性不足，容易被攻击者发现和移除，同时可能对嵌入的原始语义造成较大影响，降低EaaS的服务质量。因此，如何设计一种既能有效保护EaaS版权，又能保证水印的隐蔽性和无害性的水印方法是本文要解决的核心问题。\\n\\n**核心思路**：SemMark的核心思路是利用嵌入的语义信息来设计水印。通过将语义空间划分为多个区域，并在特定区域内注入语义感知水印，可以保证水印的隐蔽性和多样性。同时，采用自适应水印权重机制，根据局部离群因子调整水印强度，以保持原始嵌入分布，减少水印对嵌入语义的影响。\\n\\n**技术框架**：SemMark主要包含以下几个模块：1) 语义空间划分：使用局部敏感哈希 (LSH) 将嵌入的语义空间划分为多个区域。2) 水印注入：在选定的语义区域内，根据预先设定的规则注入语义感知水印。3) 自适应权重调整：根据局部离群因子 (LOF) 调整水印的权重，以保持原始嵌入分布。4) 水印检测：通过检测嵌入中是否存在特定的水印信号来验证版权。\\n\\n**关键创新**：SemMark的关键创新在于：1) 提出了语义感知水印的概念，将水印与嵌入的语义信息相结合，提高了水印的隐蔽性和鲁棒性。2) 引入了自适应水印权重机制，根据局部离群因子动态调整水印强度，有效降低了水印对嵌入语义的影响。3) 提出了Detect-Sampling和Dimensionality-Reduction两种新的攻击方式，并验证了SemMark在这些攻击下的鲁棒性。\\n\\n**关键设计**：在语义空间划分方面，LSH的哈希桶大小是一个关键参数，它决定了语义区域的粒度。在自适应权重调整方面，局部离群因子LOF的计算半径是一个关键参数，它决定了局部邻域的大小。此外，水印注入的强度也需要仔细调整，以在隐蔽性和鲁棒性之间取得平衡。论文中具体参数设置未知。",
            "application_zh": "SemMark可应用于各种基于Embedding-as-a-Service的商业平台，例如文本相似度计算、信息检索、推荐系统等。通过有效保护EaaS的版权，可以促进相关技术的发展和应用，维护公平竞争的市场环境。未来，该技术可以扩展到其他类型的嵌入，例如图像嵌入、音频嵌入等。",
            "highlight_zh": "SemMark在四个流行的NLP数据集上进行了广泛的实验，结果表明，SemMark在可验证性、多样性、隐蔽性和无害性方面均优于现有方法。具体性能提升数据未知。此外，SemMark还能够有效抵抗Detect-Sampling和Dimensionality-Reduction等新型攻击。",
            "tags_zh": [
                "语义水印",
                "版权保护",
                "Embedding-as-a-Service",
                "局部敏感哈希",
                "自适应权重"
            ],
            "_index": 105,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16439v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16439v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16439v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Evaluating OpenAI GPT Models for Translation of Endangered Uralic Languages: A Comparison of Reasoning and Non-Reasoning Architectures",
            "authors": [
                "Yehor Tereshchenko",
                "Mika Hämäläinen",
                "Svitlana Myroniuk"
            ],
            "arxiv_id": "2512.16287v1",
            "summary": "The evaluation of Large Language Models (LLMs) for translation tasks has primarily focused on high-resource languages, leaving a significant gap in understanding their performance on low-resource and endangered languages. This study presents a comprehensive comparison of OpenAI's GPT models, specifically examining the differences between reasoning and non-reasoning architectures for translating between Finnish and four low-resource Uralic languages: Komi-Zyrian, Moksha, Erzya, and Udmurt. Using a parallel corpus of literary texts, we evaluate model willingness to attempt translation through refusal rate analysis across different model architectures. Our findings reveal significant performance variations between reasoning and non-reasoning models, with reasoning models showing 16 percentage points lower refusal rates. The results provide valuable insights for researchers and practitioners working with Uralic languages and contribute to the broader understanding of reasoning model capabilities for endangered language preservation.",
            "categories": [
                "cs.CL"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "IWCLUL 2025",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16287v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "评估OpenAI GPT模型在濒危乌拉尔语翻译中的性能：推理与非推理架构对比",
            "summary_zh": "本研究旨在评估大型语言模型（LLMs）在翻译任务中的性能，特别关注低资源和濒危语言，弥补了现有研究主要集中于高资源语言的不足。本文对比了OpenAI的GPT模型，着重考察了推理和非推理架构在芬兰语与四种低资源乌拉尔语（科米-兹梁语、莫克沙语、埃尔兹亚语和乌德穆尔特语）之间翻译的差异。我们使用文学文本的平行语料库，通过分析不同模型架构的拒绝率来评估模型尝试翻译的意愿。研究结果表明，推理模型和非推理模型之间存在显著的性能差异，推理模型的拒绝率降低了16个百分点。这些发现为研究人员和从业者提供了关于乌拉尔语的宝贵见解，并有助于更广泛地理解推理模型在濒危语言保护方面的能力。",
            "intro_zh": [
                "现有大型语言模型翻译研究主要集中在高资源语言，忽略了低资源和濒危语言的翻译性能。",
                "本研究对比了OpenAI的GPT模型，考察推理和非推理架构在乌拉尔语翻译中的差异。",
                "实验结果表明，推理模型在低资源乌拉尔语翻译中表现更佳，拒绝率显著降低。"
            ],
            "method_zh": "**问题定义**：论文旨在评估OpenAI的GPT模型在低资源濒危乌拉尔语翻译任务中的性能。现有方法主要关注高资源语言，缺乏对低资源语言翻译能力的深入评估，导致对这些语言的数字化保护和传承缺乏有效的工具支持。现有方法无法有效衡量模型在低资源场景下的翻译意愿和质量。\\n\\n**核心思路**：论文的核心思路是通过对比推理和非推理架构的GPT模型在乌拉尔语翻译任务中的表现，来评估不同架构对低资源语言翻译的适用性。通过分析模型的拒绝率，来衡量模型尝试翻译的意愿。这种对比有助于揭示推理能力对低资源翻译的影响，并为选择合适的模型架构提供指导。\\n\\n**技术框架**：研究采用平行语料库，包含芬兰语和四种低资源乌拉尔语（科米-兹梁语、莫克沙语、埃尔兹亚语和乌德穆尔特语）的文学文本。研究流程包括：1）构建平行语料库；2）选择OpenAI的GPT模型，包括推理和非推理架构；3）使用平行语料库进行翻译实验；4）分析模型的拒绝率和翻译质量。\\n\\n**关键创新**：本研究的关键创新在于首次系统性地评估了OpenAI的GPT模型在低资源濒危乌拉尔语翻译中的性能，并对比了推理和非推理架构的差异。通过分析模型的拒绝率，提供了一种新的评估模型在低资源场景下翻译意愿的方法。\\n\\n**关键设计**：研究的关键设计包括：1）选择具有代表性的低资源乌拉尔语；2）构建高质量的平行语料库；3）选择具有代表性的OpenAI GPT模型架构；4）采用拒绝率作为评估指标，衡量模型翻译意愿；5）对翻译结果进行人工评估，衡量翻译质量（具体评估指标未知）。",
            "application_zh": "该研究成果可应用于濒危语言的数字化保护和传承，为低资源语言的机器翻译系统开发提供指导。通过选择合适的模型架构，可以提高低资源语言的翻译质量，促进文化交流和信息传播。此外，该研究方法也可推广到其他低资源语言的翻译评估中，具有广泛的应用前景。",
            "highlight_zh": "实验结果表明，推理模型在低资源乌拉尔语翻译中表现优于非推理模型，拒绝率降低了16个百分点。这一发现表明，推理能力对于低资源语言的翻译至关重要。具体翻译质量的提升幅度（如BLEU值或其他指标）未知，但拒绝率的显著降低表明模型更愿意尝试翻译，为后续的翻译质量提升奠定了基础。",
            "tags_zh": [
                "低资源翻译",
                "濒危语言",
                "乌拉尔语",
                "大型语言模型",
                "GPT模型",
                "推理能力",
                "拒绝率",
                "机器翻译"
            ],
            "_index": 106,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "LoPA: Scaling dLLM Inference via Lookahead Parallel Decoding",
            "authors": [
                "Chenkai Xu",
                "Yijie Jin",
                "Jiajun Li",
                "Yi Tu",
                "Guoping Long",
                "Dandan Tu",
                "Tianqi Hou",
                "Junchi Yan",
                "Zhijie Deng"
            ],
            "arxiv_id": "2512.16229v1",
            "summary": "Diffusion Large Language Models (dLLMs) have demonstrated significant potential for high-speed inference. However, current confidence-driven decoding strategies are constrained by limited parallelism, typically achieving only 1--3 tokens per forward pass (TPF). In this work, we identify that the degree of parallelism during dLLM inference is highly sensitive to the Token Filling Order (TFO). Then, we introduce Lookahead PArallel Decoding LoPA, a training-free, plug-and-play algorithm, to identify a superior TFO and hence accelerate inference. LoPA concurrently explores distinct candidate TFOs via parallel branches, and selects the one with the highest potential for future parallelism based on branch confidence. We apply LoPA to the state-of-the-art D2F model and observe a substantial enhancement in decoding efficiency. Notably, LoPA increases the TPF of D2F-Dream to 10.1 on the GSM8K while maintaining performance superior to the Dream baseline. Furthermore, to facilitate this unprecedented degree of parallelism, we develop a specialized multi-device inference system featuring Branch Parallelism (BP), which achieves a single-sample throughput of 1073.9 tokens per second under multi-GPU deployment. The code is available at https://github.com/zhijie-group/LoPA.",
            "categories": [
                "cs.CL"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16229v1",
            "code_links": [
                {
                    "url": "https://github.com/zhijie-group/LoPA",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "LoPA：通过前瞻并行解码加速扩散大语言模型推理",
            "summary_zh": "扩散大语言模型(dLLM)在高速推理方面展现出巨大潜力。然而，当前基于置信度的解码策略受到有限并行性的约束，通常每次前向传播只能实现1-3个token。本文发现dLLM推理过程中的并行度对Token填充顺序(TFO)高度敏感。因此，我们提出了Lookahead PArallel Decoding (LoPA)，一种无需训练、即插即用的算法，用于识别更优的TFO，从而加速推理。LoPA通过并行分支同时探索不同的候选TFO，并根据分支置信度选择未来并行潜力最大的一个。我们将LoPA应用于最先进的D2F模型，观察到解码效率的显著提高。值得注意的是，LoPA在GSM8K上将D2F-Dream的TPF提高到10.1，同时保持优于Dream基线的性能。此外，为了支持这种前所未有的并行度，我们开发了一种专门的多设备推理系统，具有分支并行性(BP)，在多GPU部署下实现了每秒1073.9个token的单样本吞吐量。代码已开源。",
            "intro_zh": [
                "现有的扩散大语言模型推理方法受限于置信度驱动的解码策略，并行度较低，导致推理速度慢。",
                "LoPA通过并行探索不同的Token填充顺序(TFO)，并基于置信度选择最优TFO，从而提高并行度。",
                "实验表明，LoPA显著提高了D2F模型的解码效率，在GSM8K上实现了更高的TPF和优于基线的性能。"
            ],
            "method_zh": "**问题定义**：扩散大语言模型(dLLM)的推理速度受限于其解码过程的并行度。现有的基于置信度的解码策略，例如D2F，在每一步只能填充少数几个token，导致整体推理速度较慢。问题的核心在于如何找到一种能够最大化并行度的Token填充顺序(TFO)。\\n\\n**核心思路**：LoPA的核心思路是通过前瞻性的并行探索，找到一个能够最大化未来并行度的TFO。不同于传统的贪心策略，LoPA同时探索多个候选的TFO，并根据这些候选TFO的置信度来评估其未来并行潜力。选择具有最高并行潜力的TFO，从而提高整体的解码效率。\\n\\n**技术框架**：LoPA的整体框架包括以下几个主要步骤：1) **并行分支探索**：同时生成多个候选的TFO分支。2) **置信度评估**：评估每个分支的置信度，用于预测其未来并行潜力。3) **分支选择**：选择具有最高并行潜力的分支。4) **Token填充**：根据选定的分支填充token。这个过程迭代进行，直到生成完整的序列。为了支持高并行度，论文还开发了一个多设备推理系统，利用分支并行性(BP)在多个GPU上并行执行不同的分支。\\n\\n**关键创新**：LoPA的关键创新在于其前瞻性的并行解码策略。与传统的贪心解码策略不同，LoPA不是简单地选择当前置信度最高的token，而是通过并行探索多个候选的TFO，并根据其未来并行潜力进行选择。这种前瞻性的策略能够有效地提高解码过程的并行度，从而加速推理。\\n\\n**关键设计**：LoPA的关键设计包括：1) **分支数量**：控制并行探索的候选TFO的数量。2) **置信度评估函数**：用于评估每个分支的置信度，并预测其未来并行潜力。论文中使用了D2F模型提供的置信度作为评估函数。3) **分支选择策略**：用于选择具有最高并行潜力的分支。论文中使用了基于置信度的选择策略。",
            "application_zh": "LoPA可以应用于各种需要高速推理的扩散大语言模型应用场景，例如实时对话系统、快速文本生成、以及需要低延迟响应的AI助手。通过提高推理速度，LoPA可以降低计算成本，并使dLLM能够部署在资源受限的设备上，从而扩展其应用范围。",
            "highlight_zh": "LoPA在D2F-Dream模型上取得了显著的性能提升。在GSM8K数据集上，LoPA将TPF（每前向传播的token数）提高到10.1，显著高于基线模型。同时，LoPA保持了优于Dream基线的性能。此外，通过开发专门的多设备推理系统，LoPA实现了高达1073.9 tokens/秒的单样本吞吐量。",
            "tags_zh": [
                "扩散大语言模型",
                "并行解码",
                "推理加速",
                "Token填充顺序",
                "前瞻策略"
            ],
            "_index": 107,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16229v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16229v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16229v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Machine Learning-based Optimal Control for Colloidal Self-Assembly",
            "authors": [
                "Andres Lizano-Villalobos",
                "Fangyuan Ma",
                "Wentao Tang",
                "Wei Sun",
                "Xun Tang"
            ],
            "arxiv_id": "2512.16402v1",
            "summary": "Achieving precise control of colloidal self-assembly into specific patterns remains a longstanding challenge due to the complex process dynamics. Recently, machine learning-based state representation and reinforcement learning-based control strategies have started to accumulate popularity in the field, showing great potential in achieving an automatable and generalizable approach to producing patterned colloidal assembly. In this work, we adopted a machine learning-based optimal control framework, combining unsupervised learning and graph convolutional neural work for state observation with deep reinforcement learning-based optimal control policy calculation, to provide a data-driven control approach that can potentially be generalized to other many-body self-assembly systems. With Brownian dynamics simulations, we demonstrated its superior performance as compared to traditional order parameter-based state description, and its efficacy in obtaining ordered 2-dimensional spherical colloidal self-assembly in an electric field-mediated system with an actual success rate of 97%.",
            "categories": [
                "cond-mat.soft",
                "eess.SY"
            ],
            "primary_category": "cond-mat.soft",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "19 pages, 5 figures, 1 table",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16402v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning",
                        "deep reinforcement learning"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "提出基于机器学习的最优控制框架，实现胶体自组装的精确控制",
            "summary_zh": "由于复杂的动力学过程，精确控制胶体自组装成特定模式一直是一个长期存在的挑战。 近年来，基于机器学习的状态表示和基于强化学习的控制策略在该领域越来越受欢迎，显示出在实现自动化和通用化方法以产生图案化胶体组装方面的巨大潜力。 在这项工作中，我们采用了一种基于机器学习的最优控制框架，将无监督学习和图卷积神经网络用于状态观察，以及基于深度强化学习的最优控制策略计算，以提供一种数据驱动的控制方法，该方法可以推广到其他多体自组装系统。 通过布朗动力学模拟，我们证明了其优于传统的基于序参数的状态描述，并且在电场介导的系统中，以97%的实际成功率获得了有序的二维球形胶体自组装。",
            "intro_zh": [
                "精确控制胶体自组装面临挑战，传统方法难以应对复杂动力学过程。",
                "论文提出基于机器学习的最优控制框架，结合无监督学习和深度强化学习。",
                "实验证明该方法优于传统方法，在二维胶体自组装中成功率高达97%。"
            ],
            "method_zh": "**问题定义**：论文旨在解决胶体自组装过程中难以精确控制的问题。现有方法，如基于序参数的状态描述，无法充分捕捉复杂的动力学过程，导致控制效果不佳。因此，需要一种能够更准确地描述系统状态并进行有效控制的方法。\\n\\n**核心思路**：论文的核心思路是利用机器学习方法，从数据中学习系统的状态表示和控制策略。具体而言，使用无监督学习提取有效的状态特征，并使用深度强化学习训练最优控制策略。这种数据驱动的方法能够更好地适应复杂系统，并实现更精确的控制。\\n\\n**技术框架**：整体框架包括三个主要模块：1) 数据生成模块，通过布朗动力学模拟生成胶体自组装过程的数据；2) 状态表示模块，使用无监督学习（例如，变分自编码器）和图卷积神经网络（GCN）从数据中提取状态特征；3) 控制策略模块，使用深度强化学习（例如，DDPG或SAC）训练最优控制策略。这三个模块协同工作，实现从数据到控制的闭环。\\n\\n**关键创新**：论文的关键创新在于将无监督学习和图卷积神经网络用于状态表示。传统的序参数无法充分描述系统的复杂状态，而无监督学习和GCN能够从原始数据中学习到更有效的状态特征。此外，将这种状态表示与深度强化学习相结合，实现了数据驱动的最优控制。\\n\\n**关键设计**：在状态表示模块中，GCN被用于处理胶体之间的相互作用，从而提取更具代表性的状态特征。在控制策略模块中，选择合适的深度强化学习算法（如DDPG或SAC）并调整超参数，以获得最佳的控制性能。损失函数的设计需要考虑控制的精度和稳定性，以及避免过度控制。",
            "application_zh": "该研究成果可应用于材料科学、纳米技术等领域，实现对纳米颗粒、胶体颗粒等微观粒子的精确组装，从而制造具有特定功能的材料和器件。例如，可以用于制造新型传感器、光子晶体、药物递送系统等。未来，该方法有望推广到其他多体自组装系统，实现更广泛的应用。",
            "highlight_zh": "实验结果表明，该方法在二维球形胶体自组装中取得了显著的成功，实际成功率高达97%。与传统的基于序参数的状态描述方法相比，该方法能够更有效地控制胶体自组装过程，并获得更高的组装质量。这证明了基于机器学习的最优控制框架在胶体自组装领域的优越性。",
            "tags_zh": [
                "胶体自组装",
                "机器学习",
                "最优控制",
                "深度强化学习",
                "图卷积神经网络"
            ],
            "_index": 108,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "Prime and Reach: Synthesising Body Motion for Gaze-Primed Object Reach",
            "authors": [
                "Masashi Hatano",
                "Saptarshi Sinha",
                "Jacob Chalk",
                "Wei-Hong Li",
                "Hideo Saito",
                "Dima Damen"
            ],
            "arxiv_id": "2512.16456v1",
            "summary": "Human motion generation is a challenging task that aims to create realistic motion imitating natural human behaviour. We focus on the well-studied behaviour of priming an object/location for pick up or put down -- that is, the spotting of an object/location from a distance, known as gaze priming, followed by the motion of approaching and reaching the target location. To that end, we curate, for the first time, 23.7K gaze-primed human motion sequences for reaching target object locations from five publicly available datasets, i.e., HD-EPIC, MoGaze, HOT3D, ADT, and GIMO. We pre-train a text-conditioned diffusion-based motion generation model, then fine-tune it conditioned on goal pose or location, on our curated sequences. Importantly, we evaluate the ability of the generated motion to imitate natural human movement through several metrics, including the 'Reach Success' and a newly introduced 'Prime Success' metric. On the largest dataset, HD-EPIC, our model achieves 60% prime success and 89% reach success when conditioned on the goal object location.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Project Page: https://masashi-hatano.github.io/prime-and-reach/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16456v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱四：生成式动作 (Generative Motion)",
                    "id": "4_motion_diffusion",
                    "matched_keywords": [
                        "motion generation"
                    ],
                    "score": 2.5
                }
            ],
            "relevance_score": 2.5,
            "hit_pillars": [
                "4_motion_diffusion"
            ],
            "headline_zh": "提出基于注视启动的人体运动合成方法，用于模拟拾取/放置物体的自然行为。",
            "summary_zh": "人体运动生成是一项具有挑战性的任务，旨在创建模仿自然人类行为的逼真运动。本文关注一个经过充分研究的行为：启动一个物体/位置以进行拾取或放置——即，从远处发现一个物体/位置，称为注视启动，然后是接近和到达目标位置的运动。为此，我们首次整理了23.7K个注视启动的人体运动序列，用于从五个公开可用的数据集（即HD-EPIC、MoGaze、HOT3D、ADT和GIMO）到达目标物体位置。我们预训练了一个基于文本条件扩散的运动生成模型，然后在我们整理的序列上，以目标姿势或位置为条件对其进行微调。重要的是，我们通过包括“到达成功”和一个新引入的“启动成功”指标在内的多个指标，评估了生成的运动模仿自然人类运动的能力。在最大的数据集HD-EPIC上，当以目标物体位置为条件时，我们的模型实现了60%的启动成功率和89%的到达成功率。",
            "intro_zh": [
                "现有方法难以生成逼真的人体运动，尤其是在涉及注视启动和目标导向的复杂交互行为时。",
                "本文提出一种基于扩散模型的运动生成方法，利用大规模注视启动数据集进行训练，从而模拟自然的人类运动。",
                "实验表明，该模型在HD-EPIC数据集上取得了显著的启动成功率（60%）和到达成功率（89%），验证了其有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决人体运动生成中，如何模拟注视启动（gaze-primed）的物体拾取或放置行为的问题。现有方法难以捕捉人类在执行此类任务时的自然运动模式，尤其是在长距离观察和接近目标的过程中，运动的连贯性和目标导向性难以保证。\\n\\n**核心思路**：论文的核心思路是利用大规模的注视启动人体运动数据集，训练一个条件扩散模型，使其能够根据目标物体的位置或姿势，生成逼真且符合人类行为习惯的运动序列。通过模仿人类在注视目标后接近并与之交互的自然过程，提高生成运动的真实性和实用性。\\n\\n**技术框架**：整体框架包含以下几个主要步骤：1) 数据收集与整理：从多个公开数据集（HD-EPIC、MoGaze、HOT3D、ADT和GIMO）中收集并整理包含注视信息的运动序列，构建大规模的注视启动数据集。2) 模型预训练：使用文本条件扩散模型进行预训练，学习人体运动的基本模式。3) 模型微调：在整理的注视启动数据集上，以目标物体的位置或姿势为条件，对预训练模型进行微调，使其能够生成与目标相关的运动。4) 运动生成与评估：输入目标物体的位置或姿势，生成相应的运动序列，并使用“到达成功”和“启动成功”等指标进行评估。\\n\\n**关键创新**：该论文的关键创新在于：1) 首次整理并公开了一个大规模的注视启动人体运动数据集，为相关研究提供了宝贵的数据资源。2) 提出了“启动成功”这一新的评估指标，用于衡量生成的运动是否能够模仿人类在注视目标后的行为。3) 将扩散模型应用于注视启动的人体运动生成，并取得了显著的性能提升。\\n\\n**关键设计**：论文使用了基于文本条件的扩散模型，具体结构未知。微调阶段，模型以目标物体的位置或姿势作为条件输入。损失函数的设计可能包含运动的平滑性、目标可达性以及注视行为的合理性等多个方面。具体参数设置和网络结构细节在论文中可能没有详细描述，属于未知信息。",
            "application_zh": "该研究成果可应用于虚拟现实、人机交互、机器人控制等领域。例如，可以用于创建更逼真的虚拟角色，使其能够自然地与虚拟环境中的物体进行交互。此外，还可以用于训练机器人执行复杂的任务，例如在仓库中拣选货物或在家庭环境中进行辅助。",
            "highlight_zh": "该模型在HD-EPIC数据集上取得了显著的成果，在以目标物体位置为条件时，达到了60%的启动成功率和89%的到达成功率。这表明该模型能够有效地模仿人类在注视目标后接近并与之交互的行为，生成的运动更加自然和逼真。",
            "tags_zh": [
                "人体运动生成",
                "注视启动",
                "扩散模型",
                "人机交互",
                "目标导向运动"
            ],
            "_index": 109,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16456v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16456v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16456v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Single-View Shape Completion for Robotic Grasping in Clutter",
            "authors": [
                "Abhishek Kashyap",
                "Yuxuan Yang",
                "Henrik Andreasson",
                "Todor Stoyanov"
            ],
            "arxiv_id": "2512.16449v1",
            "summary": "In vision-based robot manipulation, a single camera view can only capture one side of objects of interest, with additional occlusions in cluttered scenes further restricting visibility. As a result, the observed geometry is incomplete, and grasp estimation algorithms perform suboptimally. To address this limitation, we leverage diffusion models to perform category-level 3D shape completion from partial depth observations obtained from a single view, reconstructing complete object geometries to provide richer context for grasp planning. Our method focuses on common household items with diverse geometries, generating full 3D shapes that serve as input to downstream grasp inference networks. Unlike prior work, which primarily considers isolated objects or minimal clutter, we evaluate shape completion and grasping in realistic clutter scenarios with household objects. In preliminary evaluations on a cluttered scene, our approach consistently results in better grasp success rates than a naive baseline without shape completion by 23% and over a recent state of the art shape completion approach by 19%. Our code is available at https://amm.aass.oru.se/shape-completion-grasping/.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16449v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "提出基于扩散模型的单视角形状补全方法，提升复杂场景下机器人抓取成功率。",
            "summary_zh": "在基于视觉的机器人操作中，单个相机视角只能捕捉到目标物体的一侧，并且杂乱场景中的遮挡进一步限制了可见性。这导致观测到的几何形状不完整，抓取估计算法的性能欠佳。为了解决这个限制，我们利用扩散模型从单视角获取的局部深度观测中执行类别级别的3D形状补全，重建完整的物体几何形状，为抓取规划提供更丰富的上下文信息。我们的方法侧重于具有不同几何形状的常见家居物品，生成完整的3D形状，作为下游抓取推理网络的输入。与主要考虑孤立物体或极少杂乱的先前工作不同，我们在具有家居物品的真实杂乱场景中评估形状补全和抓取。在杂乱场景的初步评估中，我们的方法始终比没有形状补全的朴素基线提高了23%的抓取成功率，并且比最近最先进的形状补全方法提高了19%。我们的代码可在https://amm.aass.oru.se/shape-completion-grasping/ 获取。",
            "intro_zh": [
                "现有方法在杂乱场景下，单视角信息不完整导致机器人抓取性能下降，是亟待解决的核心问题。",
                "利用扩散模型从单视角深度信息进行类别级别的3D形状补全，为抓取提供更完整的几何信息。",
                "实验表明，该方法在杂乱场景中显著提升了机器人抓取成功率，优于现有方法。"
            ],
            "method_zh": "**问题定义**：论文旨在解决在杂乱场景下，由于单视角深度信息不完整，导致机器人抓取成功率低的问题。现有方法通常假设物体是孤立的或者场景杂乱程度较低，无法有效处理真实场景中的遮挡和不完整观测，导致抓取规划的输入信息不足。\\n\\n**核心思路**：论文的核心思路是利用扩散模型强大的生成能力，从单视角局部深度观测中推断出完整的3D形状。通过补全缺失的几何信息，为下游的抓取推理网络提供更准确、更全面的物体表示，从而提高抓取成功率。这种方法避免了对完整3D模型的依赖，使其更适用于实际的机器人操作场景。\\n\\n**技术框架**：该方法主要包含两个阶段：形状补全阶段和抓取推理阶段。在形状补全阶段，利用扩散模型从单视角深度图像中生成完整的3D形状。在抓取推理阶段，将补全后的3D形状输入到抓取推理网络中，预测最佳的抓取姿态。整体流程是从不完整的观测到完整的形状，再到可靠的抓取姿态。\\n\\n**关键创新**：该论文的关键创新在于将扩散模型应用于单视角形状补全，并将其与机器人抓取任务相结合。与传统的形状补全方法相比，扩散模型能够生成更逼真、更完整的3D形状，尤其是在处理复杂几何形状和遮挡时表现更佳。此外，该方法在真实的杂乱场景中进行了评估，更贴近实际应用。\\n\\n**关键设计**：论文中使用了特定的扩散模型架构，并针对形状补全任务进行了优化。具体的网络结构和训练细节在论文中应该有详细描述。此外，损失函数的设计也至关重要，可能包括重建损失、对抗损失等，以保证生成形状的质量和真实性。抓取推理网络的选择和训练也需要与形状补全模块相协调，以实现端到端的优化。",
            "application_zh": "该研究成果可广泛应用于机器人自动化领域，例如智能仓储、家庭服务机器人、工业自动化等。通过提升机器人在复杂环境下的物体识别和抓取能力，可以实现更高效、更可靠的自动化操作，降低人工成本，提高生产效率。未来，该技术有望应用于更复杂的场景，例如医疗手术机器人、灾难救援机器人等。",
            "highlight_zh": "实验结果表明，该方法在杂乱场景中显著提升了机器人抓取成功率。与没有形状补全的朴素基线相比，抓取成功率提高了23%。与最近最先进的形状补全方法相比，抓取成功率提高了19%。这些数据表明，该方法在实际应用中具有显著的优势。",
            "tags_zh": [
                "机器人抓取",
                "形状补全",
                "扩散模型",
                "单视角深度",
                "3D重建"
            ],
            "_index": 110,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16449v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16449v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16449v1/images/ReOcS/easy/shard_0/scene_1/view_6/rgb.jpg",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "A2VISR: An Active and Adaptive Ground-Aerial Localization System Using Visual Inertial and Single-Range Fusion",
            "authors": [
                "Sijia Chen",
                "Wei Dong"
            ],
            "arxiv_id": "2512.16367v1",
            "summary": "It's a practical approach using the ground-aerial collaborative system to enhance the localization robustness of flying robots in cluttered environments, especially when visual sensors degrade. Conventional approaches estimate the flying robot's position using fixed cameras observing pre-attached markers, which could be constrained by limited distance and susceptible to capture failure. To address this issue, we improve the ground-aerial localization framework in a more comprehensive manner, which integrates active vision, single-ranging, inertial odometry, and optical flow. First, the designed active vision subsystem mounted on the ground vehicle can be dynamically rotated to detect and track infrared markers on the aerial robot, improving the field of view and the target recognition with a single camera. Meanwhile, the incorporation of single-ranging extends the feasible distance and enhances re-capture capability under visual degradation. During estimation, a dimension-reduced estimator fuses multi-source measurements based on polynomial approximation with an extended sliding window, balancing computational efficiency and redundancy. Considering different sensor fidelities, an adaptive sliding confidence evaluation algorithm is implemented to assess measurement quality and dynamically adjust the weighting parameters based on moving variance. Finally, extensive experiments under conditions such as smoke interference, illumination variation, obstacle occlusion, prolonged visual loss, and extended operating range demonstrate that the proposed approach achieves robust online localization, with an average root mean square error of approximately 0.09 m, while maintaining resilience to capture loss and sensor failures.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "accept by IEEE Transactions on Industrial Electronics",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16367v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "optical flow"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出A2VISR，一种融合视觉惯性和单测距的主动自适应地-空协同定位系统",
            "summary_zh": "本文提出了一种地-空协同系统，旨在提升飞行机器人在复杂环境中，尤其是在视觉传感器性能下降时的定位鲁棒性。传统方法使用固定相机观测预先安装的标记来估计飞行机器人的位置，但受到距离限制且容易捕获失败。为了解决这个问题，本文以更全面的方式改进了地-空定位框架，集成了主动视觉、单点测距、惯性里程计和光流。首先，地面车辆上安装的主动视觉子系统可以动态旋转，以检测和跟踪空中机器人上的红外标记，从而扩大视野范围，并使用单个摄像头提高目标识别率。同时，单点测距的加入扩展了可行距离，并增强了视觉退化下的重捕获能力。在估计过程中，一种降维估计器基于多项式逼近的扩展滑动窗口融合多源测量，平衡了计算效率和冗余。考虑到不同传感器的可靠性，实现了一种自适应滑动置信度评估算法，以评估测量质量，并基于移动方差动态调整权重参数。最后，在烟雾干扰、光照变化、障碍物遮挡、长时间视觉丢失和扩展操作范围等条件下进行的大量实验表明，所提出的方法实现了鲁棒的在线定位，平均均方根误差约为0.09米，同时保持了对捕获丢失和传感器故障的弹性。",
            "intro_zh": [
                "现有方法依赖固定相机和标记，易受距离限制和捕获失败影响，难以在复杂环境中保持定位鲁棒性。",
                "A2VISR系统融合主动视觉、单点测距、惯性里程计和光流，提升视野、重捕获能力和定位精度。",
                "实验表明，A2VISR在多种复杂条件下实现了鲁棒的在线定位，平均均方根误差约为0.09米。"
            ],
            "method_zh": "**问题定义**：论文旨在解决复杂环境下，飞行机器人在视觉传感器退化时定位鲁棒性差的问题。现有方法依赖固定相机观测预先安装的标记，存在视野范围有限、易受距离限制和捕获失败等痛点。这些问题限制了飞行机器人在实际场景中的应用。\n\\n**核心思路**：论文的核心思路是构建一个地-空协同定位系统，通过地面车辆搭载的主动视觉系统和单点测距传感器，辅助空中机器人进行定位。主动视觉系统可以动态调整视角，扩大视野范围，提高目标识别率。单点测距则可以扩展可行距离，增强视觉退化下的重捕获能力。同时，融合惯性里程计和光流信息，提高定位精度和鲁棒性。\n\\n**技术框架**：A2VISR系统的整体框架包括以下几个主要模块：1) 地面车辆搭载的主动视觉子系统，用于检测和跟踪空中机器人上的红外标记；2) 单点测距传感器，用于测量地面车辆和空中机器人之间的距离；3) 空中机器人上的惯性测量单元（IMU）和摄像头，用于提供惯性里程计和光流信息；4) 降维估计器，用于融合多源测量数据，估计空中机器人的位置和姿态；5) 自适应滑动置信度评估算法，用于评估测量质量，动态调整权重参数。\n\\n**关键创新**：论文的关键创新在于以下几个方面：1) 提出了一种主动视觉系统，可以动态调整视角，扩大视野范围，提高目标识别率；2) 将单点测距引入地-空协同定位系统，扩展了可行距离，增强了视觉退化下的重捕获能力；3) 提出了一种自适应滑动置信度评估算法，可以根据测量质量动态调整权重参数，提高定位精度和鲁棒性。\n\\n**关键设计**：降维估计器基于多项式逼近的扩展滑动窗口，在保证计算效率的同时，融合多源测量数据。自适应滑动置信度评估算法基于移动方差评估测量质量，并动态调整权重参数。具体的参数设置和损失函数等技术细节在论文中进行了详细描述，但具体数值未知。",
            "application_zh": "A2VISR系统可应用于复杂环境下的无人机自主导航、协同作业、目标跟踪等领域。例如，在仓库巡检、灾后救援、桥梁检测等场景中，该系统可以提高无人机的定位精度和鲁棒性，使其能够在恶劣环境下安全可靠地完成任务。该研究成果有助于推动无人机在更多实际场景中的应用。",
            "highlight_zh": "实验结果表明，A2VISR系统在烟雾干扰、光照变化、障碍物遮挡、长时间视觉丢失和扩展操作范围等复杂条件下，实现了鲁棒的在线定位，平均均方根误差约为0.09米。该系统能够有效应对视觉退化和传感器故障，保持对目标的稳定跟踪，显著提升了地-空协同定位的性能。",
            "tags_zh": [
                "地空协同定位",
                "主动视觉",
                "单点测距",
                "视觉惯性融合",
                "鲁棒定位"
            ],
            "_index": 111,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16367v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16367v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16367v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Privacy-Aware Sharing of Raw Spatial Sensor Data for Cooperative Perception",
            "authors": [
                "Bangya Liu",
                "Chengpo Yan",
                "Chenghao Jiang",
                "Suman Banerjee",
                "Akarsh Prabhakara"
            ],
            "arxiv_id": "2512.16265v1",
            "summary": "Cooperative perception between vehicles is poised to offer robust and reliable scene understanding. Recently, we are witnessing experimental systems research building testbeds that share raw spatial sensor data for cooperative perception. While there has been a marked improvement in accuracies and is the natural way forward, we take a moment to consider the problems with such an approach for eventual adoption by automakers. In this paper, we first argue that new forms of privacy concerns arise and discourage stakeholders to share raw sensor data. Next, we present SHARP, a research framework to minimize privacy leakage and drive stakeholders towards the ambitious goal of raw data based cooperative perception. Finally, we discuss open questions for networked systems, mobile computing, perception researchers, industry and government in realizing our proposed framework.",
            "categories": [
                "cs.NI",
                "cs.RO"
            ],
            "primary_category": "cs.NI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16265v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "scene understanding"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出SHARP框架，旨在最小化原始空间传感器数据共享中的隐私泄露，促进车辆协同感知。",
            "summary_zh": "车辆间的协同感知有望提供更强大和可靠的场景理解能力。最近，我们看到实验性系统研究正在构建测试平台，以共享原始空间传感器数据用于协同感知。虽然精度有了显著提高，并且这是自然的发展方向，但我们花时间考虑了汽车制造商最终采用这种方法的问题。在本文中，我们首先论证了新的隐私问题出现，并阻碍了利益相关者共享原始传感器数据。接下来，我们提出了SHARP，一个研究框架，旨在最小化隐私泄露，并推动利益相关者朝着基于原始数据的协同感知的宏伟目标前进。最后，我们讨论了网络系统、移动计算、感知研究人员、工业界和政府在实现我们提出的框架时面临的开放性问题。",
            "intro_zh": [
                "现有车辆协同感知系统依赖原始传感器数据共享，但直接共享引发了严重的隐私泄露风险，阻碍了实际应用。",
                "论文提出SHARP框架，通过隐私保护机制，在最小化隐私泄露的同时，促进基于原始数据的协同感知。",
                "论文构建了一个研究框架，并讨论了实现该框架在网络系统、移动计算和感知研究中面临的开放性问题。"
            ],
            "method_zh": "**问题定义**：论文旨在解决车辆协同感知中，直接共享原始空间传感器数据所带来的隐私泄露问题。现有方法在追求感知精度的同时，忽略了用户隐私，导致利益相关者不愿共享数据，阻碍了协同感知技术的实际应用。\\n\\n**核心思路**：SHARP框架的核心思路是在数据共享过程中引入隐私保护机制，通过对原始数据进行处理，降低隐私泄露的风险，同时尽可能保持数据的可用性，以支持协同感知任务。这样既能促进数据共享，又能保护用户隐私。\\n\\n**技术框架**：论文提出了一个研究框架SHARP，但摘要中并未详细描述其具体架构和流程。根据上下文推断，该框架可能包含以下模块：1) 隐私评估模块：用于评估原始数据中存在的隐私风险；2) 隐私保护模块：用于对原始数据进行处理，例如差分隐私、同态加密等，以降低隐私泄露风险；3) 数据共享模块：用于安全地共享处理后的数据；4) 感知融合模块：用于将来自不同车辆的数据进行融合，以提高感知精度。\\n\\n**关键创新**：该论文的关键创新在于提出了一个隐私感知的协同感知框架SHARP，强调在原始数据共享过程中保护用户隐私的重要性。与现有方法相比，SHARP框架更加关注隐私保护，旨在平衡感知精度和隐私保护之间的关系。\\n\\n**关键设计**：由于摘要中没有提供关于SHARP框架的具体技术细节，因此无法得知关键参数设置、损失函数或网络结构等信息。这些细节需要在论文正文中进一步查找。",
            "application_zh": "该研究成果可应用于自动驾驶、智能交通等领域。通过SHARP框架，车辆可以在保护用户隐私的前提下，共享传感器数据，实现更安全、更高效的协同感知。这有助于提高自动驾驶系统的可靠性，减少交通事故，并优化交通流量。",
            "highlight_zh": "由于论文摘要主要关注框架的提出和问题分析，并未提供具体的实验结果。因此，无法从摘要中提取实验亮点。具体的性能数据、对比基线和提升幅度等信息需要在论文正文中查找。",
            "tags_zh": [
                "协同感知",
                "隐私保护",
                "原始数据共享",
                "车辆网络",
                "自动驾驶"
            ],
            "_index": 112,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16265v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16265v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16265v1/hotnets25-template/newfigures/privacy_metrics.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "EasyV2V: A High-quality Instruction-based Video Editing Framework",
            "authors": [
                "Jinjie Mai",
                "Chaoyang Wang",
                "Guocheng Gordon Qian",
                "Willi Menapace",
                "Sergey Tulyakov",
                "Bernard Ghanem",
                "Peter Wonka",
                "Ashkan Mirzaei"
            ],
            "arxiv_id": "2512.16920v1",
            "summary": "While image editing has advanced rapidly, video editing remains less explored, facing challenges in consistency, control, and generalization. We study the design space of data, architecture, and control, and introduce \\emph{EasyV2V}, a simple and effective framework for instruction-based video editing. On the data side, we compose existing experts with fast inverses to build diverse video pairs, lift image edit pairs into videos via single-frame supervision and pseudo pairs with shared affine motion, mine dense-captioned clips for video pairs, and add transition supervision to teach how edits unfold. On the model side, we observe that pretrained text-to-video models possess editing capability, motivating a simplified design. Simple sequence concatenation for conditioning with light LoRA fine-tuning suffices to train a strong model. For control, we unify spatiotemporal control via a single mask mechanism and support optional reference images. Overall, EasyV2V works with flexible inputs, e.g., video+text, video+mask+text, video+mask+reference+text, and achieves state-of-the-art video editing results, surpassing concurrent and commercial systems. Project page: https://snap-research.github.io/easyv2v/",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Project page: https://snap-research.github.io/easyv2v/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16920v1",
            "code_links": [
                {
                    "url": "https://snap-research.github.io/easyv2v/",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱八：物理动画 (Physics-based Animation)",
                    "id": "8_physics_animation",
                    "matched_keywords": [
                        "spatiotemporal"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "8_physics_animation"
            ],
            "headline_zh": "EasyV2V：提出一种高质量的基于指令的视频编辑框架，提升视频编辑的一致性、控制性和泛化性。",
            "summary_zh": "图像编辑技术发展迅速，但视频编辑相对滞后，面临着一致性、控制性和泛化性方面的挑战。本文研究了数据、架构和控制的设计空间，并提出了一种简单有效的基于指令的视频编辑框架EasyV2V。在数据方面，我们利用具有快速逆向功能的现有专家模型构建多样化的视频对，通过单帧监督和共享仿射运动的伪视频对将图像编辑对提升为视频，挖掘密集字幕片段以构建视频对，并添加过渡监督来学习编辑如何展开。在模型方面，我们观察到预训练的文本到视频模型具有编辑能力，从而简化了设计。简单的序列连接和轻量级的LoRA微调足以训练出一个强大的模型。在控制方面，我们通过单一的掩码机制统一了时空控制，并支持可选的参考图像。总体而言，EasyV2V可以处理灵活的输入，例如视频+文本、视频+掩码+文本、视频+掩码+参考+文本，并实现了最先进的视频编辑结果，超越了现有的商业系统。",
            "intro_zh": [
                "现有视频编辑方法在一致性、控制性和泛化性方面存在不足，难以满足高质量编辑需求。",
                "EasyV2V框架利用预训练文本到视频模型，通过简单序列连接和LoRA微调实现指令驱动的视频编辑。",
                "实验结果表明，EasyV2V在视频编辑质量上超越了现有商业系统，并支持灵活的输入控制方式。"
            ],
            "method_zh": "**问题定义**：现有视频编辑方法难以在保证视频帧间一致性的前提下，实现精确的、可控的编辑，并且泛化能力不足，难以适应各种编辑指令和场景。这些方法通常需要复杂的模型结构和大量的训练数据，计算成本高昂。\\n\\n**核心思路**：论文的核心思路是利用预训练的文本到视频模型所具备的潜在编辑能力，通过轻量级的微调和简单有效的控制机制，实现高质量的指令驱动视频编辑。这种方法避免了从头开始训练复杂的视频编辑模型，降低了训练成本，并提高了泛化能力。\\n\\n**技术框架**：EasyV2V框架主要包含数据准备和模型训练两个阶段。数据准备阶段构建了多样化的视频编辑对，包括利用现有图像编辑模型生成视频对、通过单帧监督将图像编辑扩展到视频、挖掘密集字幕片段等。模型训练阶段则采用预训练的文本到视频模型，通过简单的序列连接方式将视频、文本、掩码等信息输入模型，并使用LoRA进行轻量级微调。\\n\\n**关键创新**：EasyV2V的关键创新在于：1) 利用预训练模型的编辑能力，避免了从头训练；2) 提出了一种简单有效的序列连接方式，将多种控制信息（文本、掩码、参考图像）统一输入模型；3) 构建了多样化的视频编辑数据集，提高了模型的泛化能力。\\n\\n**关键设计**：在数据方面，论文设计了多种数据增强策略，包括利用图像编辑模型生成视频对、通过单帧监督扩展图像编辑到视频等。在模型方面，使用了预训练的文本到视频模型，并采用LoRA进行微调，降低了计算成本。在控制方面，使用单一的掩码机制统一了时空控制，并支持可选的参考图像输入。",
            "application_zh": "EasyV2V框架可应用于各种视频编辑场景，例如短视频创作、电影后期制作、广告设计等。该框架能够根据用户的指令，快速、高质量地编辑视频内容，降低视频编辑的门槛，提高创作效率。未来，该技术有望应用于智能视频监控、虚拟现实等领域，实现更加智能化的视频处理。",
            "highlight_zh": "EasyV2V在视频编辑质量上超越了现有的商业系统，例如在用户研究中，EasyV2V在多个指标上显著优于其他方法。该框架支持灵活的输入控制方式，例如视频+文本、视频+掩码+文本、视频+掩码+参考+文本，能够满足不同用户的编辑需求。此外，EasyV2V的训练成本较低，易于部署和应用。",
            "tags_zh": [
                "视频编辑",
                "指令驱动",
                "文本到视频",
                "预训练模型",
                "LoRA微调"
            ],
            "_index": 113,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "Distributional AGI Safety",
            "authors": [
                "Nenad Tomašev",
                "Matija Franklin",
                "Julian Jacobs",
                "Sébastien Krier",
                "Simon Osindero"
            ],
            "arxiv_id": "2512.16856v1",
            "summary": "AI safety and alignment research has predominantly been focused on methods for safeguarding individual AI systems, resting on the assumption of an eventual emergence of a monolithic Artificial General Intelligence (AGI). The alternative AGI emergence hypothesis, where general capability levels are first manifested through coordination in groups of sub-AGI individual agents with complementary skills and affordances, has received far less attention. Here we argue that this patchwork AGI hypothesis needs to be given serious consideration, and should inform the development of corresponding safeguards and mitigations. The rapid deployment of advanced AI agents with tool-use capabilities and the ability to communicate and coordinate makes this an urgent safety consideration. We therefore propose a framework for distributional AGI safety that moves beyond evaluating and aligning individual agents. This framework centers on the design and implementation of virtual agentic sandbox economies (impermeable or semi-permeable), where agent-to-agent transactions are governed by robust market mechanisms, coupled with appropriate auditability, reputation management, and oversight to mitigate collective risks.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16856v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "affordance"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出分布式AGI安全框架，通过虚拟沙盒经济应对群体智能风险。",
            "summary_zh": "当前人工智能安全和对齐研究主要集中在保护单个AI系统的方法上，并假设最终会出现一个单体的通用人工智能（AGI）。另一种AGI涌现的假设是，通用能力水平首先通过具有互补技能和能力的多个亚AGI个体智能体的协作来体现，但这种假设受到的关注较少。本文认为，这种拼凑式AGI假设需要认真考虑，并应指导相应安全措施和缓解措施的开发。随着具有工具使用能力以及沟通和协调能力的高级AI智能体的快速部署，这成为一个紧迫的安全问题。因此，本文提出了一个分布式AGI安全框架，该框架超越了评估和对齐单个智能体。该框架的核心是设计和实施虚拟智能体沙盒经济（不可渗透或半渗透），其中智能体之间的交易受稳健的市场机制、适当的可审计性、声誉管理和监督的约束，以减轻集体风险。",
            "intro_zh": [
                "现有AI安全研究侧重于单个AGI，忽略了多个亚AGI智能体协作涌现通用能力的风险。",
                "提出分布式AGI安全框架，通过虚拟沙盒经济和市场机制管理智能体间交互，降低集体风险。",
                "该框架强调可审计性、声誉管理和监督，旨在应对快速部署的高级AI智能体带来的安全挑战。"
            ],
            "method_zh": "**问题定义**：现有AI安全研究主要关注单个AGI系统的安全，忽略了多个能力互补的亚AGI智能体通过协作涌现出通用智能的可能性。这种“拼凑式AGI”可能带来新的安全风险，例如智能体之间的恶意串通、市场操纵等，而传统的针对单个智能体的安全措施难以有效应对这些风险。因此，需要一种新的安全框架来应对分布式AGI带来的挑战。\\n\\n**核心思路**：本文的核心思路是构建一个虚拟的智能体沙盒经济，通过市场机制来规范智能体之间的交互行为，并引入可审计性、声誉管理和监督机制来降低集体风险。这种方法借鉴了经济学和社会学的理论，将智能体之间的交互视为一种经济活动，通过市场规则来引导智能体的行为，从而实现整体的安全和稳定。\\n\\n**技术框架**：该框架主要包含以下几个模块：1) **虚拟沙盒环境**：提供一个隔离的、可控的运行环境，用于模拟智能体之间的交互。2) **市场机制**：设计一套市场规则，例如价格机制、交易规则等，用于规范智能体之间的交易行为。3) **可审计性**：记录智能体之间的所有交易行为，以便进行事后分析和追溯。4) **声誉管理**：建立一套声誉系统，用于评估智能体的行为，并根据声誉值来调整智能体的权限和奖励。5) **监督机制**：引入人工或自动化的监督机制，用于监控智能体的行为，并及时发现和处理异常情况。\\n\\n**关键创新**：该框架的关键创新在于将经济学和社会学的理论引入到AI安全领域，提出了一种基于市场机制的分布式AGI安全解决方案。与传统的针对单个智能体的安全措施相比，该框架能够更好地应对多个智能体之间的复杂交互带来的安全风险。\\n\\n**关键设计**：关键设计包括：1) **市场机制的设计**：需要根据具体的应用场景来设计合适的市场规则，例如拍卖机制、合约机制等。2) **声誉系统的设计**：需要选择合适的声誉指标和评估方法，例如基于交易历史、用户反馈等。3) **监督机制的设计**：需要确定监督的范围和频率，以及异常情况的处理流程。",
            "application_zh": "该研究成果可应用于各种涉及多智能体协作的场景，例如智能制造、供应链管理、金融交易等。通过构建安全的虚拟沙盒环境，可以有效降低智能体之间的恶意行为和系统性风险，促进人工智能技术的健康发展。未来，该框架还可以扩展到更复杂的社会系统，例如城市管理、公共服务等。",
            "highlight_zh": "论文提出了一个概念性的框架，目前没有提供具体的实验结果。未来的工作可以包括：1) 在不同的虚拟沙盒环境中测试该框架的有效性。2) 比较该框架与传统的AI安全措施的性能。3) 研究不同市场机制和声誉系统对系统安全性的影响。4) 探索自动化监督机制的可能性。",
            "tags_zh": [
                "AGI安全",
                "分布式智能",
                "多智能体系统",
                "虚拟沙盒",
                "市场机制",
                "声誉管理",
                "可审计性"
            ],
            "_index": 114,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "OS-Oracle: A Comprehensive Framework for Cross-Platform GUI Critic Models",
            "authors": [
                "Zhenyu Wu",
                "Jingjing Xie",
                "Zehao Li",
                "Bowen Yang",
                "Qiushi Sun",
                "Zhaoyang Liu",
                "Zhoumianze Liu",
                "Yu Qiao",
                "Xiangyu Yue",
                "Zun Wang",
                "Zichen Ding"
            ],
            "arxiv_id": "2512.16295v1",
            "summary": "With VLM-powered computer-using agents (CUAs) becoming increasingly capable at graphical user interface (GUI) navigation and manipulation, reliable step-level decision-making has emerged as a key bottleneck for real-world deployment. In long-horizon workflows, errors accumulate quickly and irreversible actions can cause unintended consequences, motivating critic models that assess each action before execution. While critic models offer a promising solution, their effectiveness is hindered by the lack of diverse, high-quality GUI feedback data and public critic benchmarks for step-level evaluation in computer use. To bridge these gaps, we introduce OS-Oracle that makes three core contributions: (1) a scalable data pipeline for synthesizing cross-platform GUI critic data; (2) a two-stage training paradigm combining supervised fine-tuning (SFT) and consistency-preserving group relative policy optimization (CP-GRPO); (3) OS-Critic Bench, a holistic benchmark for evaluating critic model performance across Mobile, Web, and Desktop platforms. Leveraging this framework, we curate a high-quality dataset containing 310k critic samples. The resulting critic model, OS-Oracle-7B, achieves state-of-the-art performance among open-source VLMs on OS-Critic Bench, and surpasses proprietary models on the mobile domain. Furthermore, when serving as a pre-critic, OS-Oracle-7B improves the performance of native GUI agents such as UI-TARS-1.5-7B in OSWorld and AndroidWorld environments. The code is open-sourced at https://github.com/numbmelon/OS-Oracle.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16295v1",
            "code_links": [
                {
                    "url": "https://github.com/numbmelon/OS-Oracle",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "提出OS-Oracle框架，用于跨平台GUI智能体的决策评估与优化",
            "summary_zh": "随着VLM驱动的计算机使用智能体(CUA)在图形用户界面(GUI)导航和操作方面能力不断增强，可靠的步级决策已成为实际部署的关键瓶颈。在长流程任务中，错误会迅速累积，不可逆操作可能导致意外后果，因此需要评估每次操作的critic模型。然而，缺乏多样化、高质量的GUI反馈数据和用于计算机使用中步级评估的公共critic基准阻碍了critic模型的有效性。为了弥合这些差距，我们引入了OS-Oracle，它做出了三个核心贡献：(1)用于合成跨平台GUI critic数据的可扩展数据管道；(2)结合监督微调(SFT)和一致性保持的组相对策略优化(CP-GRPO)的两阶段训练范式；(3)OS-Critic Bench，一个用于评估移动、Web和桌面平台上的critic模型性能的整体基准。利用此框架，我们整理了一个包含31万个critic样本的高质量数据集。由此产生的critic模型OS-Oracle-7B在OS-Critic Bench上实现了开源VLM中的最先进性能，并在移动领域超越了专有模型。此外，当作为pre-critic时，OS-Oracle-7B提高了原生GUI智能体（如UI-TARS-1.5-7B）在OSWorld和AndroidWorld环境中的性能。代码已开源。",
            "intro_zh": [
                "现有计算机使用智能体在GUI操作中面临步级决策可靠性问题，长流程任务易出错。",
                "OS-Oracle通过构建数据管道、设计训练范式和建立评估基准来解决GUI智能体的决策评估问题。",
                "OS-Oracle-7B在OS-Critic Bench上达到SOTA，并提升了UI-TARS-1.5-7B等智能体的性能。"
            ],
            "method_zh": "**问题定义**：论文旨在解决计算机使用智能体（CUAs）在图形用户界面（GUI）交互中，由于缺乏可靠的步级决策能力而导致的错误累积问题。现有方法缺乏高质量的GUI反馈数据和统一的评估基准，难以训练和评估有效的critic模型，从而限制了CUAs在实际应用中的部署。\\n\\n**核心思路**：论文的核心思路是构建一个全面的框架，包括数据生成、模型训练和性能评估三个方面，以提升GUI智能体的决策能力。通过合成高质量的跨平台GUI critic数据，设计有效的训练范式，并建立统一的评估基准，从而训练出能够准确评估每一步操作的critic模型。\\n\\n**技术框架**：OS-Oracle框架包含三个主要组成部分：1) 可扩展的数据管道，用于合成跨平台GUI critic数据；2) 两阶段训练范式，包括监督微调（SFT）和一致性保持的组相对策略优化（CP-GRPO）；3) OS-Critic Bench，一个用于评估critic模型性能的综合基准，涵盖移动、Web和桌面平台。数据管道负责生成训练数据，两阶段训练范式用于训练critic模型，OS-Critic Bench用于评估模型的性能。\\n\\n**关键创新**：论文的关键创新在于构建了一个完整的、可扩展的框架，用于解决GUI智能体的决策评估问题。具体包括：1) 提出了一个可扩展的数据管道，能够合成高质量的跨平台GUI critic数据，解决了数据稀缺的问题；2) 设计了一种两阶段训练范式，结合了监督微调和一致性保持的组相对策略优化，提高了模型的训练效率和性能；3) 构建了一个综合的评估基准OS-Critic Bench，能够全面评估critic模型在不同平台上的性能。\\n\\n**关键设计**：在数据生成方面，论文设计了自动化的数据收集和标注流程，以保证数据的质量和多样性。在模型训练方面，论文采用了两阶段训练范式，首先使用监督微调（SFT）对模型进行预训练，然后使用一致性保持的组相对策略优化（CP-GRPO）对模型进行微调，以提高模型的泛化能力。在评估方面，论文构建了OS-Critic Bench，包含多个任务和评估指标，能够全面评估critic模型的性能。",
            "application_zh": "OS-Oracle框架可应用于各种需要GUI交互的智能体系统中，例如自动化测试、智能助手、机器人流程自动化（RPA）等。通过提升智能体的决策能力，可以减少错误操作，提高工作效率，并降低人工干预的需求。该研究的成果有助于推动计算机使用智能体在实际场景中的广泛应用。",
            "highlight_zh": "OS-Oracle-7B在OS-Critic Bench上实现了开源VLM中的SOTA性能，并在移动领域超越了专有模型。作为pre-critic，OS-Oracle-7B显著提升了UI-TARS-1.5-7B等原生GUI智能体在OSWorld和AndroidWorld环境中的性能。这些结果表明OS-Oracle框架在提升GUI智能体决策能力方面的有效性。",
            "tags_zh": [
                "GUI智能体",
                "决策评估",
                "Critic模型",
                "跨平台",
                "数据合成"
            ],
            "_index": 115,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16295v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16295v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16295v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Resilience of coupled systems under deep uncertainty and dynamic complexity: An integrative literature review",
            "authors": [
                "Jannie Coenen",
                "Vítor Vasconcelos",
                "Heiman Wertheim",
                "Marcel Olde Rikkert",
                "Sophie Hadjisotiriou",
                "Vittorio Nespeca",
                "Tom Oreel",
                "Rick Quax",
                "Etiënne Rouwette",
                "Vincent Marchau",
                "Hubert Korzilius"
            ],
            "arxiv_id": "2512.16608v1",
            "summary": "Resilience in coupled systems is increasingly critical in addressing global challenges such as climate change and pandemics. These systems show unpredictable behaviour due to dynamic complexity and deep uncertainty across spatiotemporal scales. Despite growing interest, few studies systematically integrate both concepts when assessing resilience. This paper conducts an integrative review of 102 English-language publications to identify gaps in current approaches. Findings reveal that most papers address lower levels of uncertainty and rarely consider dynamic complexity and deep uncertainty simultaneously, which limits the effectiveness of resilience strategies. To advance systems research, we propose a conceptual framework and practical tools to support researchers and decision-makers in evaluating and improving resilience. The paper also outlines future research directions for more robust, adaptive, and integrative resilience assessments.",
            "categories": [
                "physics.soc-ph",
                "eess.SY"
            ],
            "primary_category": "physics.soc-ph",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16608v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱八：物理动画 (Physics-based Animation)",
                    "id": "8_physics_animation",
                    "matched_keywords": [
                        "spatiotemporal"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "8_physics_animation"
            ],
            "headline_zh": "针对复杂动态和深度不确定性，提出耦合系统韧性评估的综合框架。",
            "summary_zh": "耦合系统的韧性在应对气候变化和疫情等全球挑战中日益重要。由于时空尺度上的动态复杂性和深度不确定性，这些系统表现出不可预测的行为。尽管人们对此越来越感兴趣，但很少有研究在评估韧性时系统地整合这两个概念。本文对102篇英文出版物进行了综合回顾，以找出当前方法的差距。研究结果表明，大多数论文处理的是较低水平的不确定性，很少同时考虑动态复杂性和深度不确定性，这限制了韧性策略的有效性。为了推进系统研究，我们提出了一个概念框架和实用工具，以支持研究人员和决策者评估和提高韧性。本文还概述了未来研究方向，以实现更稳健、适应性和综合性的韧性评估。",
            "intro_zh": [
                "现有韧性评估方法未能充分整合动态复杂性和深度不确定性，导致策略有效性受限。",
                "论文提出一个概念框架和实用工具，旨在支持研究人员和决策者评估并提升耦合系统的韧性。",
                "通过综合文献回顾，论文识别了当前研究的差距，并为未来更稳健的韧性评估指明了方向。"
            ],
            "method_zh": "**问题定义**：现有耦合系统韧性评估方法在处理动态复杂性和深度不确定性时存在不足。许多研究仅关注较低层次的不确定性，未能充分考虑系统内部的复杂交互和外部环境的剧烈变化，导致评估结果与实际情况存在偏差，难以制定有效的应对策略。\\n\\n**核心思路**：论文的核心思路是通过综合文献回顾，识别现有研究在处理动态复杂性和深度不确定性方面的差距，并在此基础上构建一个概念框架和实用工具，以支持更全面、更准确的韧性评估。该框架旨在帮助研究人员和决策者更好地理解耦合系统的复杂性，并制定更具适应性的韧性策略。\\n\\n**技术框架**：论文采用综合文献回顾的方法，对102篇相关英文出版物进行系统分析。首先，确定文献选择标准，并进行文献检索。然后，对选定的文献进行编码和分类，提取关键概念和方法。接着，分析现有研究在处理动态复杂性和深度不确定性方面的优势和不足。最后，基于分析结果，构建概念框架和实用工具，并提出未来研究方向。\\n\\n**关键创新**：论文的关键创新在于提出了一个整合动态复杂性和深度不确定性的耦合系统韧性评估框架。该框架强调同时考虑系统内部的复杂交互和外部环境的剧烈变化，从而更全面地评估系统的韧性。此外，论文还提供了一系列实用工具，以支持研究人员和决策者应用该框架进行韧性评估。\\n\\n**关键设计**：论文提出的概念框架包含多个关键要素，例如系统的边界定义、关键变量的识别、动态复杂性的建模、深度不确定性的量化、韧性指标的选取等。实用工具包括用于识别和量化动态复杂性和深度不确定性的方法，以及用于评估和提高韧性的策略。具体的参数设置、损失函数、网络结构等技术细节未在摘要中提及，属于未知信息。",
            "application_zh": "该研究成果可应用于气候变化适应、疫情应对、城市规划、供应链管理等领域。通过更准确地评估耦合系统的韧性，决策者可以制定更有效的风险管理策略，提高系统应对外部冲击的能力，保障社会经济的可持续发展。未来，该框架可进一步扩展到其他复杂系统，为解决全球性挑战提供更强大的支持。",
            "highlight_zh": "论文通过对102篇文献的综合回顾，揭示了现有韧性评估方法在处理动态复杂性和深度不确定性方面的不足。研究发现，大多数论文仅关注较低水平的不确定性，很少同时考虑动态复杂性和深度不确定性。这一发现强调了整合这两个概念的重要性，并为未来的研究方向提供了指导。",
            "tags_zh": [
                "耦合系统",
                "韧性评估",
                "动态复杂性",
                "深度不确定性",
                "文献综述",
                "风险管理",
                "系统研究"
            ],
            "_index": 116,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "From Liability to Asset: A Three-Mode Grid-Forming Control Framework for Centralized Data Center UPS Systems",
            "authors": [
                "Mohamed Shamseldein"
            ],
            "arxiv_id": "2512.16497v1",
            "summary": "AI workloads are turning large data centers into highly dynamic power-electronic loads; fault-time behavior and workload pulsing can stress weak-grid points of interconnection. This paper proposes a centralized medium-voltage (MV) uninterruptible power supply (UPS) control architecture implemented as three operating modes: Mode 1 regulates a DC stiff bus and shapes normal-operation grid draw, Mode 2 enforces current-limited fault-mode P--Q priority with UPS battery energy storage system (UPS-BESS) buffering and a rate-limited post-fault \"soft return,\" and Mode 3 optionally provides droop-based fast frequency response via grid-draw modulation. Fundamental-frequency averaged dq simulations (50 MW block, short-circuit ratio (SCR) = 1.5, 0.5 p.u. three-phase dip for 150~ms) show zero unserved information-technology (IT) energy (0.00000 MWh vs.0.00208 MWh for a momentary-cessation benchmark), a 0.57 p.u. peak inverter current (vs. 1.02 p.u. for a synchronous-reference-frame phase-locked loop (SRF-PLL) low-voltage ride-through (LVRT) baseline), a nonzero mean fault-window grid draw of 0.20~p.u. (vs.approx 0 for momentary cessation), and an improved settled point-of-common-coupling (PCC) voltage minimum of 0.79 p.u. after one cycle (vs. 0.66 p.u.). A forced-oscillation case study applies a 1 Hz pulsed load (+/- 0.25 p.u.) and shows that the normal-operation shaping filters the oscillation seen by the grid while the UPS-BESS buffers the pulsing component.",
            "categories": [
                "eess.SY"
            ],
            "primary_category": "eess.SY",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16497v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱八：物理动画 (Physics-based Animation)",
                    "id": "8_physics_animation",
                    "matched_keywords": [
                        "PULSE"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "8_physics_animation"
            ],
            "headline_zh": "针对数据中心UPS系统，提出三模式并网控制框架，提升弱电网适应性。",
            "summary_zh": "人工智能负载正将大型数据中心转变为高度动态的电力电子负载；故障期间的行为和负载脉冲可能会给互联的弱电网带来压力。本文提出了一种集中式中压（MV）不间断电源（UPS）控制架构，该架构实现为三种运行模式：模式1调节直流母线并塑造正常运行时的电网吸取；模式2通过UPS电池储能系统（UPS-BESS）缓冲和速率限制的故障后“软返回”来执行限流故障模式下的P-Q优先级；模式3可选地通过电网吸取调制提供基于下垂的快速频率响应。基频平均dq仿真（50 MW模块，短路比（SCR）= 1.5，0.5 p.u.三相骤降150毫秒）显示零未服务信息技术（IT）能量（0.00000 MWh vs. 0.00208 MWh用于瞬时中断基准），0.57 p.u.峰值逆变器电流（vs. 1.02 p.u.用于同步参考系锁相环（SRF-PLL）低电压穿越（LVRT）基线），0.20 p.u.的非零平均故障窗口电网吸取（vs.瞬时中断的近似0），以及在1个周期后改善的稳定公共连接点（PCC）电压最小值0.79 p.u.（vs. 0.66 p.u.）。强制振荡案例研究应用1 Hz脉冲负载（+/- 0.25 p.u.），并表明正常运行整形滤波器过滤了电网看到的振荡，而UPS-BESS缓冲了脉冲分量。",
            "intro_zh": [
                "数据中心UPS系统面临弱电网连接下的故障和负载波动挑战，传统方法难以保证供电质量和电网稳定性。",
                "提出一种三模式并网控制框架，通过调节直流母线、故障模式P-Q优先级控制和快速频率响应，实现稳定供电和电网支撑。",
                "仿真结果表明，该方法在故障期间显著降低了未服务IT能量，改善了PCC电压，并有效抑制了负载波动对电网的影响。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大型数据中心UPS系统在连接到弱电网时面临的供电稳定性和电网交互问题。传统UPS系统在应对电网故障（如电压骤降）和负载波动时，可能导致IT设备断电，并对电网造成冲击。现有方法，如基于SRF-PLL的LVRT控制，在故障期间可能导致过大的逆变器电流和较低的PCC电压。\n\n**核心思路**：论文的核心思路是将UPS系统视为一个可控的电网支撑单元，通过分模式控制，在正常运行、故障和频率响应三种状态下优化其行为。正常运行时，塑造电网吸取，减少谐波和波动；故障时，优先保证有功和无功功率输出，并限制电流；频率异常时，提供快速频率响应，稳定电网。\n\n**技术框架**：该框架包含三个主要模式：\n1. **模式1（正常运行）**：调节直流母线电压，并对电网吸取的电流进行整形，优化电能质量。\n2. **模式2（故障模式）**：在电网故障期间，执行限流的P-Q优先级控制，利用UPS-BESS缓冲，并实现速率限制的故障后“软返回”，避免对电网造成冲击。\n3. **模式3（频率响应）**：可选地提供基于下垂控制的快速频率响应，通过调制电网吸取来稳定电网频率。\n\n**关键创新**：该方法的主要创新在于其集中式三模式控制架构，能够根据电网状态动态调整UPS系统的行为，从而在保证数据中心供电可靠性的同时，为电网提供支撑。与传统的瞬时中断或简单的LVRT策略相比，该方法能够更好地平衡供电可靠性和电网稳定性。\n\n**关键设计**：论文采用基频平均dq仿真进行验证。关键设计包括：短路比（SCR）设置为1.5的弱电网环境；0.5 p.u.的三相电压骤降，持续150ms；P-Q优先级控制的参数设置，以及速率限制的“软返回”策略。此外，还设计了1Hz的脉冲负载，以评估系统对负载波动的抑制能力。",
            "application_zh": "该研究成果可应用于大型数据中心的中压UPS系统，尤其是在连接到弱电网或可再生能源比例较高的电网时。通过提高数据中心供电的可靠性和电网的稳定性，可以降低数据中心运营风险，并促进可再生能源的消纳。该技术还可推广到其他需要高可靠性供电的场景，如关键基础设施和工业园区。",
            "highlight_zh": "仿真结果表明，与瞬时中断基准相比，该方法实现了零未服务IT能量（0.00000 MWh vs. 0.00208 MWh）。在故障期间，峰值逆变器电流降低至0.57 p.u.（相比于SRF-PLL LVRT基线的1.02 p.u.），PCC电压最小值提升至0.79 p.u.（相比于基线的0.66 p.u.）。此外，正常运行模式下的整形滤波器有效抑制了1Hz脉冲负载对电网的影响。",
            "tags_zh": [
                "数据中心",
                "不间断电源",
                "并网控制",
                "弱电网",
                "电压暂降",
                "频率响应",
                "储能系统"
            ],
            "_index": 117,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16497v1/simulation_results_pulse_ramp.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16497v1/simulation_results_pulse.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16497v1/simulation_results_stage1.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        }
    ]
}