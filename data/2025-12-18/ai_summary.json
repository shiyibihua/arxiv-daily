{
    "papers": [
        {
            "title": "E-SDS: Environment-aware See it, Do it, Sorted - Automated Environment-Aware Reinforcement Learning for Humanoid Locomotion",
            "authors": [
                "Enis Yalcin",
                "Joshua O'Hara",
                "Maria Stamatopoulou",
                "Chengxu Zhou",
                "Dimitrios Kanoulas"
            ],
            "arxiv_id": "2512.16446v1",
            "summary": "Vision-language models (VLMs) show promise in automating reward design in humanoid locomotion, which could eliminate the need for tedious manual engineering. However, current VLM-based methods are essentially \"blind\", as they lack the environmental perception required to navigate complex terrain. We present E-SDS (Environment-aware See it, Do it, Sorted), a framework that closes this perception gap. E-SDS integrates VLMs with real-time terrain sensor analysis to automatically generate reward functions that facilitate training of robust perceptive locomotion policies, grounded by example videos. Evaluated on a Unitree G1 humanoid across four distinct terrains (simple, gaps, obstacles, stairs), E-SDS uniquely enabled successful stair descent, while policies trained with manually-designed rewards or a non-perceptive automated baseline were unable to complete the task. In all terrains, E-SDS also reduced velocity tracking error by 51.9-82.6%. Our framework reduces the human effort of reward design from days to less than two hours while simultaneously producing more robust and capable locomotion policies.",
            "categories": [
                "cs.RO",
                "cs.AI"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "12 pages, 3 figures, 4 tables. Accepted at RiTA 2025 (Springer LNNS)",
            "doi": "",
            "journal_ref": "RiTA 2025 (Springer LNNS)",
            "pdf_url": "https://arxiv.org/pdf/2512.16446v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]humanoid",
                        "[T]humanoid locomotion",
                        "[T]locomotion",
                        "locomotion policy",
                        "Unitree"
                    ],
                    "score": 22.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]reinforcement learning",
                        "reward design"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 28.0,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "E-SDS：环境感知的人形机器人强化学习框架，实现复杂地形稳健行走",
            "summary_zh": "本文提出E-SDS（Environment-aware See it, Do it, Sorted），一个环境感知的人形机器人强化学习框架，旨在解决现有基于视觉语言模型（VLM）的方法在复杂地形导航中缺乏环境感知能力的问题。E-SDS集成了VLM与实时地形传感器分析，自动生成奖励函数，从而训练出稳健的、具有感知能力的运动策略，并以示例视频作为指导。在Unitree G1人形机器人上，E-SDS在四种不同地形（简单、间隙、障碍物、楼梯）上进行了评估，结果表明，E-SDS能够成功完成下楼梯任务，而手动设计的奖励或非感知的自动化基线策略均无法完成此任务。在所有地形中，E-SDS还将速度跟踪误差降低了51.9-82.6%。该框架将奖励设计的人工工作量从数天减少到不到两小时，同时产生了更稳健和更有能力的运动策略。",
            "intro_zh": [
                "现有基于视觉语言模型的机器人运动方法缺乏环境感知，难以在复杂地形中导航。",
                "E-SDS框架融合视觉语言模型与实时地形传感器数据，自动生成奖励函数，引导强化学习。",
                "实验表明，E-SDS在复杂地形（如楼梯）上表现出色，并显著降低了速度跟踪误差。"
            ],
            "method_zh": "**问题定义**：现有基于视觉语言模型的机器人运动控制方法，虽然能够利用视觉信息，但缺乏对环境的精确感知，尤其是在复杂地形中，例如楼梯、障碍物等。这导致机器人难以根据环境变化调整运动策略，从而影响其稳定性和适应性。手动设计奖励函数耗时耗力，且难以泛化到不同地形。\\n\\n**核心思路**：E-SDS的核心思路是将视觉语言模型与实时地形传感器数据相结合，利用VLM理解任务目标，同时利用传感器数据感知环境信息。通过融合这两种信息，E-SDS能够自动生成与环境相关的奖励函数，引导强化学习算法训练出适应复杂地形的运动策略。这种方法避免了手动设计奖励函数的繁琐过程，并提高了策略的泛化能力。\\n\\n**技术框架**：E-SDS框架主要包含以下几个模块：1) 环境感知模块：利用地形传感器（如激光雷达、深度相机）获取环境信息，并进行处理和分析，提取地形特征。2) 视觉语言模型模块：利用VLM理解任务目标，例如“下楼梯”、“避开障碍物”等，并生成相应的文本描述。3) 奖励函数生成模块：将环境感知模块提取的地形特征和VLM生成的文本描述作为输入，自动生成与环境相关的奖励函数。4) 强化学习模块：利用生成的奖励函数训练机器人运动策略。\\n\\n**关键创新**：E-SDS的关键创新在于将视觉语言模型与实时地形传感器数据相结合，实现了环境感知的自动化奖励函数生成。与传统的基于VLM的方法相比，E-SDS能够根据环境变化动态调整奖励函数，从而训练出更稳健和适应性更强的运动策略。与手动设计奖励函数的方法相比，E-SDS大大减少了人工工作量，并提高了策略的泛化能力。\\n\\n**关键设计**：E-SDS使用Transformer架构的VLM来理解任务目标。地形传感器数据经过滤波和特征提取后，被编码成向量表示。奖励函数生成模块使用一个神经网络，将VLM的输出和地形特征向量作为输入，输出奖励值。强化学习算法采用PPO（Proximal Policy Optimization）。具体参数设置（如学习率、折扣因子、奖励系数等）需要根据具体任务进行调整。",
            "application_zh": "E-SDS框架可应用于各种人形机器人运动控制场景，尤其是在复杂和动态环境中，例如灾难救援、物流运输、家庭服务等。该框架能够帮助机器人自主适应不同的地形和任务需求，提高其工作效率和安全性。未来，E-SDS还可以扩展到其他类型的机器人，例如四足机器人、轮式机器人等。",
            "highlight_zh": "E-SDS在Unitree G1人形机器人上进行了实验，结果表明，E-SDS能够成功完成下楼梯任务，而手动设计的奖励或非感知的自动化基线策略均无法完成此任务。在所有地形中，E-SDS还将速度跟踪误差降低了51.9-82.6%。这表明E-SDS能够显著提高机器人在复杂地形中的运动能力和稳定性。",
            "tags_zh": [
                "人形机器人",
                "强化学习",
                "视觉语言模型",
                "环境感知",
                "奖励函数设计"
            ],
            "_index": 0,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16446v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16446v1/figure/vel_chart.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16446v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Vision-Language-Action Models for Autonomous Driving: Past, Present, and Future",
            "authors": [
                "Tianshuai Hu",
                "Xiaolu Liu",
                "Song Wang",
                "Yiyao Zhu",
                "Ao Liang",
                "Lingdong Kong",
                "Guoyang Zhao",
                "Zeying Gong",
                "Jun Cen",
                "Zhiyu Huang",
                "Xiaoshuai Hao",
                "Linfeng Li",
                "Hang Song",
                "Xiangtai Li",
                "Jun Ma",
                "Shaojie Shen",
                "Jianke Zhu",
                "Dacheng Tao",
                "Ziwei Liu",
                "Junwei Liang"
            ],
            "arxiv_id": "2512.16760v1",
            "summary": "Autonomous driving has long relied on modular \"Perception-Decision-Action\" pipelines, where hand-crafted interfaces and rule-based components often break down in complex or long-tailed scenarios. Their cascaded design further propagates perception errors, degrading downstream planning and control. Vision-Action (VA) models address some limitations by learning direct mappings from visual inputs to actions, but they remain opaque, sensitive to distribution shifts, and lack structured reasoning or instruction-following capabilities. Recent progress in Large Language Models (LLMs) and multimodal learning has motivated the emergence of Vision-Language-Action (VLA) frameworks, which integrate perception with language-grounded decision making. By unifying visual understanding, linguistic reasoning, and actionable outputs, VLAs offer a pathway toward more interpretable, generalizable, and human-aligned driving policies. This work provides a structured characterization of the emerging VLA landscape for autonomous driving. We trace the evolution from early VA approaches to modern VLA frameworks and organize existing methods into two principal paradigms: End-to-End VLA, which integrates perception, reasoning, and planning within a single model, and Dual-System VLA, which separates slow deliberation (via VLMs) from fast, safety-critical execution (via planners). Within these paradigms, we further distinguish subclasses such as textual vs. numerical action generators and explicit vs. implicit guidance mechanisms. We also summarize representative datasets and benchmarks for evaluating VLA-based driving systems and highlight key challenges and open directions, including robustness, interpretability, and instruction fidelity. Overall, this work aims to establish a coherent foundation for advancing human-compatible autonomous driving systems.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Preprint; 40 pages, 7 figures, 9 tables; GitHub at https://github.com/worldbench/awesome-vla-for-ad",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16760v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]vision-language-action",
                        "VLA",
                        "large language model",
                        "multimodal",
                        "instruction following"
                    ],
                    "score": 21.0
                }
            ],
            "relevance_score": 21.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "综述性论文：面向自动驾驶的视觉-语言-动作模型研究进展与未来展望",
            "summary_zh": "自动驾驶长期以来依赖于模块化的“感知-决策-行动”流程，但手工设计的接口和基于规则的组件在复杂或长尾场景中经常失效。其级联设计进一步传播感知误差，降低下游规划和控制的性能。视觉-动作（VA）模型通过学习从视觉输入到动作的直接映射来解决一些局限性，但它们仍然不透明，对分布偏移敏感，并且缺乏结构化推理或指令遵循能力。大型语言模型（LLM）和多模态学习的最新进展推动了视觉-语言-动作（VLA）框架的出现，该框架将感知与基于语言的决策相结合。通过统一视觉理解、语言推理和可操作的输出，VLA为更可解释、更通用和更符合人类习惯的驾驶策略提供了一条途径。本文对新兴的自动驾驶VLA领域进行了结构化描述，追溯了从早期VA方法到现代VLA框架的演变，并将现有方法组织成两种主要范例：端到端VLA，它在单个模型中集成了感知、推理和规划；以及双系统VLA，它将慢速审议（通过VLM）与快速、安全关键的执行（通过规划器）分开。在这些范例中，我们进一步区分了文本与数值动作生成器以及显式与隐式指导机制等子类。我们还总结了用于评估基于VLA的驾驶系统的代表性数据集和基准，并强调了关键挑战和开放方向，包括鲁棒性、可解释性和指令保真度。总的来说，这项工作旨在为推进与人类兼容的自动驾驶系统奠定连贯的基础。",
            "intro_zh": [
                "传统自动驾驶依赖“感知-决策-行动”流程，但存在手工设计接口失效、感知误差传播等问题，限制了其在复杂场景下的应用。",
                "视觉-语言-动作（VLA）模型通过整合视觉理解、语言推理和动作输出，旨在实现更可解释、通用且符合人类习惯的自动驾驶策略。",
                "论文对VLA领域进行了系统性综述，总结了现有方法、数据集和基准，并指出了鲁棒性、可解释性和指令保真度等关键挑战。"
            ],
            "method_zh": "**问题定义**：传统自动驾驶系统依赖于模块化的“感知-决策-行动”流程，各个模块之间通过手工设计的接口连接。这种设计在复杂或长尾场景下容易失效，并且感知模块的误差会逐级传递，影响下游的决策和控制。此外，早期的视觉-动作(VA)模型虽然能够直接从视觉输入预测动作，但缺乏可解释性和泛化能力，难以应对分布偏移。\n\n**核心思路**：论文的核心思路是总结和分析近年来兴起的视觉-语言-动作（VLA）模型在自动驾驶领域的应用。VLA模型通过引入大型语言模型（LLM）进行语言推理，从而将视觉感知、语言理解和动作执行统一起来，旨在提高自动驾驶系统的可解释性、泛化能力和指令遵循能力。论文将现有VLA模型分为端到端VLA和双系统VLA两大类，并对各类方法进行了详细的分析和比较。\n\n**技术框架**：论文将现有的VLA模型分为以下两类：\n1. **端到端VLA**：该类模型将感知、推理和规划集成到一个统一的模型中，直接从视觉输入和语言指令预测车辆的动作。这类模型通常采用Transformer架构，利用注意力机制实现多模态信息的融合。\n2. **双系统VLA**：该类模型将决策过程分为两个阶段：慢速审议和快速执行。慢速审议阶段由视觉语言模型（VLM）负责，根据视觉输入和语言指令生成高级别的规划。快速执行阶段由传统的规划器或控制器负责，根据高级别的规划生成具体的车辆控制指令。\n\n**关键创新**：论文的主要创新在于对自动驾驶领域的VLA模型进行了系统的分类和总结，并指出了该领域面临的关键挑战和未来发展方向。论文提出的分类框架（端到端VLA和双系统VLA）有助于研究人员更好地理解和比较不同的VLA模型。此外，论文还总结了用于评估VLA模型的代表性数据集和基准，为未来的研究提供了参考。\n\n**关键设计**：论文没有提出新的模型或算法，而是一篇综述性文章，因此没有具体的参数设置、损失函数或网络结构等技术细节。但是，论文对现有VLA模型的技术细节进行了详细的描述，包括不同模型的架构、训练方法和评估指标等。",
            "application_zh": "该研究对自动驾驶领域具有重要的应用价值。VLA模型有望提高自动驾驶系统的安全性、可靠性和智能化水平，使其能够更好地适应复杂多变的交通环境。此外，VLA模型还可以实现更自然的人机交互，例如通过语音指令控制车辆的行驶。",
            "highlight_zh": "该论文是一篇综述性文章，没有具体的实验结果。但是，论文总结了现有VLA模型在自动驾驶数据集上的性能表现，并指出了不同模型的优缺点。例如，一些端到端VLA模型在特定场景下取得了较好的性能，但泛化能力较差；而双系统VLA模型则在安全性和可解释性方面具有优势。",
            "tags_zh": [
                "自动驾驶",
                "视觉-语言-动作模型",
                "大型语言模型",
                "多模态学习",
                "端到端学习"
            ],
            "_index": 1,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16760v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16760v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16760v1/figures/fig3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation",
            "authors": [
                "Xin Lin",
                "Meixi Song",
                "Dizhe Zhang",
                "Wenxuan Lu",
                "Haodong Li",
                "Bo Du",
                "Ming-Hsuan Yang",
                "Truong Nguyen",
                "Lu Qi"
            ],
            "arxiv_id": "2512.16913v1",
            "summary": "In this work, we present a panoramic metric depth foundation model that generalizes across diverse scene distances. We explore a data-in-the-loop paradigm from the view of both data construction and framework design. We collect a large-scale dataset by combining public datasets, high-quality synthetic data from our UE5 simulator and text-to-image models, and real panoramic images from the web. To reduce domain gaps between indoor/outdoor and synthetic/real data, we introduce a three-stage pseudo-label curation pipeline to generate reliable ground truth for unlabeled images. For the model, we adopt DINOv3-Large as the backbone for its strong pre-trained generalization, and introduce a plug-and-play range mask head, sharpness-centric optimization, and geometry-centric optimization to improve robustness to varying distances and enforce geometric consistency across views. Experiments on multiple benchmarks (e.g., Stanford2D3D, Matterport3D, and Deep360) demonstrate strong performance and zero-shot generalization, with particularly robust and stable metric predictions in diverse real-world scenes. The project page can be found at: \\href{https://insta360-research-team.github.io/DAP_website/} {https://insta360-research-team.github.io/DAP\\_website/}",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Project Page: https://insta360-research-team.github.io/DAP_website/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16913v1",
            "code_links": [
                {
                    "url": "https://insta360-research-team.github.io/DAP_website/",
                    "type": "project_page"
                },
                {
                    "url": "https://insta360-research-team.github.io/DAP",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]depth estimation",
                        "metric depth"
                    ],
                    "score": 8.0
                },
                {
                    "name": "支柱七：动作重定向 (Motion Retargeting)",
                    "id": "7_retargeting",
                    "matched_keywords": [
                        "geometric consistency"
                    ],
                    "score": 3.0
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]foundation model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 20.0,
            "hit_pillars": [
                "3_perception_slam",
                "7_retargeting",
                "9_embodied_foundation"
            ],
            "headline_zh": "提出全景深度估计基础模型DAP，提升跨场景距离的泛化能力。",
            "summary_zh": "本文提出了一种全景度量深度基础模型，该模型能够泛化到各种场景距离。我们探索了一种数据闭环范式，从数据构建和框架设计的角度出发。我们通过结合公共数据集、来自UE5模拟器的高质量合成数据、文本到图像模型以及来自网络的真实全景图像，收集了一个大规模数据集。为了减少室内/室外和合成/真实数据之间的领域差距，我们引入了一个三阶段伪标签生成流程，为未标记图像生成可靠的ground truth。在模型方面，我们采用DINOv3-Large作为骨干网络，因为它具有强大的预训练泛化能力，并引入了一个即插即用的范围掩码头、以清晰度为中心的优化和以几何为中心的优化，以提高对不同距离的鲁棒性并加强跨视图的几何一致性。在多个基准测试（例如，Stanford2D3D、Matterport3D和Deep360）上的实验表明，该模型具有强大的性能和零样本泛化能力，尤其是在各种真实场景中具有鲁棒和稳定的度量预测。",
            "intro_zh": [
                "现有全景深度估计方法在处理不同场景距离和领域泛化性方面存在不足，尤其是在真实场景中。",
                "论文提出一种数据闭环范式，结合多种数据源并使用伪标签方法缩小领域差距，提升模型泛化能力。",
                "实验结果表明，该模型在多个基准测试中表现出色，并在真实场景中实现了鲁棒和稳定的深度预测。"
            ],
            "method_zh": "**问题定义**：全景深度估计旨在从单张全景图像中预测场景的深度信息。现有方法在处理不同场景距离（近距离、远距离）以及室内外场景的泛化能力上存在挑战。此外，合成数据与真实数据之间的领域差距也是一个重要问题，限制了模型在真实场景中的应用效果。\\n\\n**核心思路**：论文的核心思路是构建一个大规模、多样化的全景数据集，并设计一个能够有效利用这些数据的深度估计模型。通过数据闭环的方式，不断迭代优化数据集和模型，从而提升模型的泛化能力和鲁棒性。具体来说，利用伪标签技术来弥合合成数据和真实数据之间的差距，并采用几何一致性约束来提高深度预测的准确性。\\n\\n**技术框架**：整体框架包含三个主要阶段：数据收集与增强、伪标签生成和模型训练。数据收集阶段结合了公共数据集、UE5合成数据和网络爬取的真实全景图像。伪标签生成阶段采用三阶段流程，逐步为未标记图像生成可靠的深度标签。模型训练阶段使用DINOv3-Large作为骨干网络，并添加了范围掩码头、清晰度优化和几何优化模块。\\n\\n**关键创新**：论文的关键创新在于数据闭环范式和伪标签生成流程。数据闭环范式强调数据集和模型之间的相互促进，通过不断迭代优化来提升性能。伪标签生成流程能够有效利用未标记的真实数据，缩小领域差距，从而提高模型的泛化能力。此外，范围掩码头和几何优化模块也进一步提升了深度预测的准确性和一致性。\\n\\n**关键设计**：在数据方面，UE5合成数据生成使用了随机纹理和光照，增加了数据的多样性。伪标签生成流程包括初始预测、置信度过滤和几何一致性优化三个阶段。模型方面，范围掩码头用于区分不同距离的物体，清晰度优化用于提高深度图的清晰度，几何优化则通过最小化相邻像素深度差异来保证几何一致性。损失函数包括深度损失、梯度损失和几何一致性损失。",
            "application_zh": "该研究成果可广泛应用于机器人导航、虚拟现实、增强现实、自动驾驶等领域。高质量的全景深度估计能够为这些应用提供更准确的环境感知信息，从而提升系统的性能和用户体验。未来，该模型可以进一步扩展到其他模态数据，例如RGB-D全景图像，以实现更全面的场景理解。",
            "highlight_zh": "实验结果表明，该模型在Stanford2D3D、Matterport3D和Deep360等多个基准测试中取得了优异的性能，尤其是在真实场景中表现出强大的鲁棒性和稳定性。与现有方法相比，该模型在零样本泛化能力方面有显著提升，能够直接应用于未见过的场景。",
            "tags_zh": [
                "全景深度估计",
                "深度学习",
                "基础模型",
                "数据闭环",
                "伪标签"
            ],
            "_index": 2,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16913v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16913v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16913v1/x4.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "GeoPredict: Leveraging Predictive Kinematics and 3D Gaussian Geometry for Precise VLA Manipulation",
            "authors": [
                "Jingjing Qian",
                "Boyao Han",
                "Chen Shi",
                "Lei Xiao",
                "Long Yang",
                "Shaoshuai Shi",
                "Li Jiang"
            ],
            "arxiv_id": "2512.16811v1",
            "summary": "Vision-Language-Action (VLA) models achieve strong generalization in robotic manipulation but remain largely reactive and 2D-centric, making them unreliable in tasks that require precise 3D reasoning. We propose GeoPredict, a geometry-aware VLA framework that augments a continuous-action policy with predictive kinematic and geometric priors. GeoPredict introduces a trajectory-level module that encodes motion history and predicts multi-step 3D keypoint trajectories of robot arms, and a predictive 3D Gaussian geometry module that forecasts workspace geometry with track-guided refinement along future keypoint trajectories. These predictive modules serve exclusively as training-time supervision through depth-based rendering, while inference requires only lightweight additional query tokens without invoking any 3D decoding. Experiments on RoboCasa Human-50, LIBERO, and real-world manipulation tasks show that GeoPredict consistently outperforms strong VLA baselines, especially in geometry-intensive and spatially demanding scenarios.",
            "categories": [
                "cs.CV",
                "cs.RO"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16811v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]manipulation"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "vision-language-action",
                        "[T]VLA"
                    ],
                    "score": 12.0
                }
            ],
            "relevance_score": 18.0,
            "hit_pillars": [
                "1_robot_core",
                "9_embodied_foundation"
            ],
            "headline_zh": "GeoPredict：利用预测运动学和3D高斯几何实现精确的VLA操作",
            "summary_zh": "视觉-语言-动作(VLA)模型在机器人操作中表现出强大的泛化能力，但很大程度上仍然是反应式的和以2D为中心的，这使得它们在需要精确3D推理的任务中不可靠。我们提出了GeoPredict，一个几何感知的VLA框架，它用预测运动学和几何先验来增强连续动作策略。GeoPredict引入了一个轨迹级模块，该模块编码运动历史并预测机器人手臂的多步3D关键点轨迹，以及一个预测性3D高斯几何模块，该模块预测工作空间几何形状，并通过沿未来关键点轨迹的跟踪引导细化。这些预测模块仅作为训练时的监督，通过基于深度的渲染实现，而推理只需要轻量级的额外查询token，无需调用任何3D解码。在RoboCasa Human-50、LIBERO和真实世界操作任务上的实验表明，GeoPredict始终优于强大的VLA基线，尤其是在几何密集型和空间要求高的场景中。",
            "intro_zh": [
                "现有VLA模型在精确3D推理任务中表现不足，主要原因是其反应式和2D中心的设计。",
                "GeoPredict通过预测运动学和几何先验增强VLA模型，提升其在复杂3D环境中的操作能力。",
                "实验表明，GeoPredict在多个数据集和真实场景中显著优于现有VLA基线，尤其是在几何相关的任务中。"
            ],
            "method_zh": "**问题定义**：现有的视觉-语言-动作(VLA)模型在机器人操作任务中，尤其是在需要精确3D推理的场景下，表现出局限性。这些模型通常是反应式的，依赖于当前观察到的图像信息来生成动作，缺乏对未来状态的预测能力，并且主要以2D为中心，难以准确理解和利用3D几何信息。这导致它们在几何密集型和空间要求高的任务中表现不佳。\\n\\n**核心思路**：GeoPredict的核心思路是通过引入预测性的运动学和几何先验来增强VLA模型。具体来说，它预测机器人手臂未来多个步骤的3D关键点轨迹，并预测工作空间的3D几何形状。这些预测信息作为训练时的监督信号，引导模型学习更有效的3D表示和推理能力。在推理阶段，模型只需要轻量级的额外查询token，无需进行复杂的3D解码，从而保持了高效性。\\n\\n**技术框架**：GeoPredict的整体框架包括一个连续动作策略网络，以及两个关键的预测模块：轨迹级模块和预测性3D高斯几何模块。轨迹级模块编码运动历史，并预测机器人手臂的多步3D关键点轨迹。预测性3D高斯几何模块预测工作空间的几何形状，并通过跟踪引导细化。这两个预测模块在训练时通过深度渲染提供监督信号，而在推理时仅需少量额外查询token。\\n\\n**关键创新**：GeoPredict的关键创新在于将预测性的运动学和几何先验融入到VLA框架中。与传统的反应式VLA模型不同，GeoPredict能够预测未来的状态，从而更好地规划动作。此外，GeoPredict使用3D高斯几何来表示工作空间，并利用跟踪引导细化，从而提高了几何预测的准确性。最重要的是，这些预测模块只在训练时使用，推理时只需要轻量级的额外查询token，保证了推理效率。\\n\\n**关键设计**：轨迹级模块使用Transformer网络来编码运动历史并预测3D关键点轨迹。预测性3D高斯几何模块使用3D高斯混合模型来表示工作空间的几何形状，并使用卡尔曼滤波等跟踪算法来引导几何预测的细化。损失函数包括轨迹预测损失和几何预测损失，通过深度渲染将预测的3D信息与真实深度图像进行比较，从而提供监督信号。推理时，通过额外的查询token将预测的运动学和几何信息融入到连续动作策略网络中。",
            "application_zh": "GeoPredict在机器人操作领域具有广泛的应用前景，例如在复杂环境下的物体抓取、装配、以及需要精确3D定位的任务中。该方法可以提升机器人在工业自动化、家庭服务、医疗辅助等领域的应用能力，使其能够更好地理解和操作周围环境，完成更加复杂和精细的任务。未来，该研究可以进一步扩展到更复杂的机器人系统和更广泛的应用场景。",
            "highlight_zh": "GeoPredict在RoboCasa Human-50、LIBERO和真实世界操作任务上进行了评估，实验结果表明，GeoPredict始终优于强大的VLA基线。例如，在几何密集型和空间要求高的场景中，GeoPredict的性能提升显著。具体数据提升幅度在不同数据集和任务上有所不同，但总体上表明GeoPredict在精确3D操作方面具有显著优势。",
            "tags_zh": [
                "视觉语言动作模型",
                "机器人操作",
                "3D几何推理",
                "运动学预测",
                "高斯混合模型"
            ],
            "_index": 3,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16811v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16811v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16811v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "KineST: A Kinematics-guided Spatiotemporal State Space Model for Human Motion Tracking from Sparse Signals",
            "authors": [
                "Shuting Zhao",
                "Zeyu Xiao",
                "Xinrong Chen"
            ],
            "arxiv_id": "2512.16791v1",
            "summary": "Full-body motion tracking plays an essential role in AR/VR applications, bridging physical and virtual interactions. However, it is challenging to reconstruct realistic and diverse full-body poses based on sparse signals obtained by head-mounted displays, which are the main devices in AR/VR scenarios. Existing methods for pose reconstruction often incur high computational costs or rely on separately modeling spatial and temporal dependencies, making it difficult to balance accuracy, temporal coherence, and efficiency. To address this problem, we propose KineST, a novel kinematics-guided state space model, which effectively extracts spatiotemporal dependencies while integrating local and global pose perception. The innovation comes from two core ideas. Firstly, in order to better capture intricate joint relationships, the scanning strategy within the State Space Duality framework is reformulated into kinematics-guided bidirectional scanning, which embeds kinematic priors. Secondly, a mixed spatiotemporal representation learning approach is employed to tightly couple spatial and temporal contexts, balancing accuracy and smoothness. Additionally, a geometric angular velocity loss is introduced to impose physically meaningful constraints on rotational variations for further improving motion stability. Extensive experiments demonstrate that KineST has superior performance in both accuracy and temporal consistency within a lightweight framework. Project page: https://kaka-1314.github.io/KineST/",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Accepted by AAAI 2026",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16791v1",
            "code_links": [
                {
                    "url": "https://kaka-1314.github.io/KineST/",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]state space model",
                        "representation learning"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱八：物理动画 (Physics-based Animation)",
                    "id": "8_physics_animation",
                    "matched_keywords": [
                        "[T]spatiotemporal",
                        "[T]motion tracking"
                    ],
                    "score": 12.0
                }
            ],
            "relevance_score": 18.0,
            "hit_pillars": [
                "2_algo_arch",
                "8_physics_animation"
            ],
            "headline_zh": "KineST：一种基于运动学引导的时空状态空间模型，用于从稀疏信号中进行人体运动跟踪",
            "summary_zh": "全身运动跟踪在AR/VR应用中至关重要，它连接了物理交互和虚拟交互。然而，基于头戴式显示器获取的稀疏信号重建逼真且多样化的全身姿势仍然具有挑战性。现有的姿势重建方法通常计算成本高昂，或者依赖于分别建模空间和时间依赖性，难以平衡准确性、时间连贯性和效率。为了解决这个问题，我们提出了一种新颖的运动学引导的状态空间模型KineST，它有效地提取时空依赖性，同时整合局部和全局姿势感知。其创新来自两个核心思想：首先，为了更好地捕捉复杂的关节关系，我们将状态空间对偶框架内的扫描策略重新定义为运动学引导的双向扫描，从而嵌入运动学先验。其次，采用混合时空表示学习方法来紧密耦合空间和时间上下文，从而平衡准确性和平滑性。此外，引入了几何角速度损失，对旋转变化施加物理意义上的约束，以进一步提高运动稳定性。大量实验表明，KineST在轻量级框架内具有卓越的准确性和时间一致性。",
            "intro_zh": [
                "现有基于稀疏信号的全身运动跟踪方法计算成本高，或难以同时建模空间和时间依赖性，导致精度、连贯性和效率难以兼顾。",
                "KineST通过运动学引导的双向扫描嵌入运动学先验，并采用混合时空表示学习方法紧密耦合空间和时间上下文，从而平衡精度和平滑性。",
                "实验结果表明，KineST在轻量级框架下，在准确性和时间一致性方面均表现出优越的性能，优于现有方法。"
            ],
            "method_zh": "**问题定义**：论文旨在解决基于AR/VR场景中头戴式显示器等设备获取的稀疏信号，进行准确、高效且时间连贯的全身运动跟踪问题。现有方法的痛点在于，要么计算复杂度高，难以实时应用；要么空间和时间依赖性建模分离，导致重建的运动不自然、不流畅。\\n\\n**核心思路**：论文的核心思路是利用人体运动学先验知识，指导时空状态空间模型的构建，从而更有效地提取时空依赖关系，并约束运动的合理性。通过运动学引导的双向扫描，将关节之间的运动学关系嵌入到模型中，并使用混合时空表示学习方法，紧密耦合空间和时间信息，从而在精度、效率和时间连贯性之间取得平衡。\\n\\n**技术框架**：KineST采用状态空间模型作为基础框架，主要包含以下几个阶段：1) 稀疏信号输入：接收来自头戴式显示器等设备的稀疏运动信号。2) 运动学引导的双向扫描：利用人体运动学结构，对状态空间进行双向扫描，提取关节之间的依赖关系。3) 混合时空表示学习：学习紧密耦合空间和时间信息的运动表示。4) 运动状态预测：基于学习到的运动表示，预测下一时刻的全身姿势。5) 几何角速度损失约束：使用几何角速度损失，约束预测的运动符合物理规律。\\n\\n**关键创新**：KineST最重要的技术创新点在于运动学引导的双向扫描和混合时空表示学习。运动学引导的双向扫描将人体运动学先验知识嵌入到状态空间模型中，使得模型能够更好地捕捉关节之间的依赖关系。混合时空表示学习方法能够紧密耦合空间和时间信息，从而提高运动跟踪的准确性和时间连贯性。与现有方法相比，KineST能够更有效地利用人体运动学信息，从而在精度、效率和时间连贯性之间取得更好的平衡。\\n\\n**关键设计**：论文的关键设计包括：1) 运动学引导的双向扫描策略，具体实现方式未知。2) 混合时空表示学习的具体网络结构未知。3) 几何角速度损失的具体计算方式，可能是通过计算相邻帧之间关节旋转角度的变化率，并对其进行约束。4) 状态空间模型的具体参数设置未知。",
            "application_zh": "KineST在AR/VR领域具有广泛的应用前景，例如虚拟化身控制、体感游戏、远程协作等。通过准确、高效地跟踪用户的全身运动，KineST可以增强AR/VR应用的沉浸感和交互性，提升用户体验。未来，KineST还可以应用于康复训练、运动分析等领域，为人们的生活带来更多便利。",
            "highlight_zh": "论文通过实验验证了KineST在准确性和时间一致性方面的优越性。具体性能数据未知，但论文强调KineST在轻量级框架下实现了优于现有方法的性能，表明其具有较高的实用价值。与现有方法相比，KineST在保持较高精度的同时，显著提高了运动跟踪的效率和时间连贯性。",
            "tags_zh": [
                "人体运动跟踪",
                "状态空间模型",
                "运动学引导",
                "时空表示学习",
                "AR/VR",
                "稀疏信号",
                "全身姿势重建"
            ],
            "_index": 4,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16791v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16791v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16791v1/scan2.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "VERM: Leveraging Foundation Models to Create a Virtual Eye for Efficient 3D Robotic Manipulation",
            "authors": [
                "Yixiang Chen",
                "Yan Huang",
                "Keji He",
                "Peiyan Li",
                "Liang Wang"
            ],
            "arxiv_id": "2512.16724v1",
            "summary": "When performing 3D manipulation tasks, robots have to execute action planning based on perceptions from multiple fixed cameras. The multi-camera setup introduces substantial redundancy and irrelevant information, which increases computational costs and forces the model to spend extra training time extracting crucial task-relevant details. To filter out redundant information and accurately extract task-relevant features, we propose the VERM (Virtual Eye for Robotic Manipulation) method, leveraging the knowledge in foundation models to imagine a virtual task-adaptive view from the constructed 3D point cloud, which efficiently captures necessary information and mitigates occlusion. To facilitate 3D action planning and fine-grained manipulation, we further design a depth-aware module and a dynamic coarse-to-fine procedure. Extensive experimental results on both simulation benchmark RLBench and real-world evaluations demonstrate the effectiveness of our method, surpassing previous state-of-the-art methods while achieving 1.89x speedup in training time and 1.54x speedup in inference speed. More results can be found on our project website at https://verm-ral.github.io .",
            "categories": [
                "cs.RO",
                "cs.CV"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Accepted at RA-L 2025",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16724v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]manipulation"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]foundation model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 15.0,
            "hit_pillars": [
                "1_robot_core",
                "9_embodied_foundation"
            ],
            "headline_zh": "VERM：利用基础模型为机器人操作构建高效3D虚拟视觉",
            "summary_zh": "在执行3D操作任务时，机器人需要基于多个固定摄像头的感知进行动作规划。多摄像头设置引入了大量的冗余和不相关信息，增加了计算成本，并迫使模型花费额外的训练时间来提取关键的任务相关细节。为了过滤掉冗余信息并准确提取任务相关的特征，我们提出了一种VERM（用于机器人操作的虚拟视觉）方法，该方法利用基础模型中的知识，从构建的3D点云中想象出一个虚拟的、任务自适应的视角，从而有效地捕获必要的信息并减轻遮挡。为了促进3D动作规划和精细操作，我们进一步设计了一个深度感知模块和一个动态的由粗到精的过程。在模拟基准RLBench和真实世界评估中的大量实验结果表明了我们方法的有效性，超越了先前的最先进方法，同时在训练时间上实现了1.89倍的加速，在推理速度上实现了1.54倍的加速。",
            "intro_zh": [
                "多摄像头系统在3D机器人操作中引入冗余信息，增加了计算负担，并降低了模型提取关键特征的效率。",
                "VERM方法利用基础模型知识，从3D点云构建任务自适应的虚拟视角，有效过滤冗余信息，减轻遮挡问题。",
                "实验表明，VERM在RLBench和真实场景中均优于现有方法，训练时间加速1.89倍，推理速度提升1.54倍。"
            ],
            "method_zh": "**问题定义**：现有机器人操作任务依赖多摄像头获取环境信息，但多视角数据包含大量冗余和与任务无关的信息，导致计算资源浪费，模型训练效率低下，难以聚焦关键特征。尤其是在存在遮挡的情况下，信息获取更加困难。\\n\\n**核心思路**：借鉴人类视觉的聚焦机制，利用预训练的基础模型，从多视角3D点云中生成一个针对特定任务优化的虚拟视角。该虚拟视角能够过滤掉冗余信息，突出任务相关的特征，并尽可能减少遮挡。\\n\\n**技术框架**：VERM方法主要包含以下几个阶段：1) 从多摄像头获取RGB-D图像，构建3D点云；2) 利用预训练的基础模型，根据任务目标，从3D点云中生成虚拟视角图像；3) 使用深度感知模块处理虚拟视角图像，提取深度信息；4) 采用动态的由粗到精的策略进行动作规划和精细操作。\\n\\n**关键创新**：VERM的核心创新在于利用基础模型生成任务自适应的虚拟视角。与传统的多视角融合方法不同，VERM不是简单地将多个视角的信息进行融合，而是根据任务目标，选择或合成一个最优的视角，从而显著减少了冗余信息，提高了计算效率。此外，深度感知模块和动态由粗到精的策略进一步提升了操作的精度和效率。\\n\\n**关键设计**：VERM的关键设计包括：1) 如何选择合适的基础模型，并将其应用于虚拟视角的生成；2) 深度感知模块的具体实现方式，例如使用深度卷积神经网络；3) 动态由粗到精策略的实现细节，例如如何确定粗略阶段和精细阶段的切换条件，以及如何设计相应的动作空间。",
            "application_zh": "VERM方法在机器人操作领域具有广泛的应用前景，例如工业自动化中的装配、抓取和放置任务，家庭服务机器人中的物品整理和清洁任务，以及医疗机器人中的手术辅助和康复训练。该方法能够显著提高机器人的操作效率和精度，降低计算成本，并增强其在复杂环境中的适应性。未来，VERM有望成为机器人智能的重要组成部分。",
            "highlight_zh": "实验结果表明，VERM在RLBench模拟环境和真实世界场景中均取得了显著的性能提升。在RLBench上，VERM超越了先前的SOTA方法，并在训练时间上实现了1.89倍的加速，在推理速度上实现了1.54倍的加速。真实世界实验也验证了VERM的有效性和鲁棒性，表明其具有很强的实际应用潜力。",
            "tags_zh": [
                "机器人操作",
                "虚拟视觉",
                "基础模型",
                "3D感知",
                "动作规划"
            ],
            "_index": 5,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16724v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16724v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16724v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "AIMM: An AI-Driven Multimodal Framework for Detecting Social-Media-Influenced Stock Market Manipulation",
            "authors": [
                "Sandeep Neela"
            ],
            "arxiv_id": "2512.16103v1",
            "summary": "Market manipulation now routinely originates from coordinated social media campaigns, not isolated trades. Retail investors, regulators, and brokerages need tools that connect online narratives and coordination patterns to market behavior. We present AIMM, an AI-driven framework that fuses Reddit activity, bot and coordination indicators, and OHLCV market features into a daily AIMM Manipulation Risk Score for each ticker.\n  The system uses a parquet-native pipeline with a Streamlit dashboard that allows analysts to explore suspicious windows, inspect underlying posts and price action, and log model outputs over time. Due to Reddit API restrictions, we employ calibrated synthetic social features matching documented event characteristics; market data (OHLCV) uses real historical data from Yahoo Finance. This release makes three contributions. First, we build the AIMM Ground Truth dataset (AIMM-GT): 33 labeled ticker-days spanning eight equities, drawing from SEC enforcement actions, community-verified manipulation cases, and matched normal controls. Second, we implement forward-walk evaluation and prospective prediction logging for both retrospective and deployment-style assessment. Third, we analyze lead times and show that AIMM flagged GME 22 days before the January 2021 squeeze peak.\n  The current labeled set is small (33 ticker-days, 3 positive events), but results show preliminary discriminative capability and early warnings for the GME incident. We release the code, dataset schema, and dashboard design to support research on social media-driven market surveillance.",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Preprint",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16103v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]manipulation"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]multimodal"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 15.0,
            "hit_pillars": [
                "1_robot_core",
                "9_embodied_foundation"
            ],
            "headline_zh": "AIMM：用于检测社交媒体影响的股票市场操纵的人工智能驱动多模态框架",
            "summary_zh": "市场操纵现在通常源于有组织的社交媒体活动，而非孤立的交易行为。零售投资者、监管机构和经纪公司需要能够将在线叙事和协调模式与市场行为联系起来的工具。我们提出了AIMM，一个人工智能驱动的框架，它将Reddit活动、机器人和协调指标以及OHLCV市场特征融合为每个股票代码的每日AIMM操纵风险评分。该系统使用一个parquet原生管道和一个Streamlit仪表板，分析师可以通过它来探索可疑窗口，检查底层帖子和价格行为，并记录随时间推移的模型输出。由于Reddit API的限制，我们采用了经过校准的合成社交特征，以匹配已记录的事件特征；市场数据（OHLCV）使用来自Yahoo Finance的真实历史数据。这项工作有三个贡献。首先，我们构建了AIMM Ground Truth数据集（AIMM-GT）：33个标记的股票代码-天，涵盖8个股票，数据来自SEC的执法行动、社区验证的操纵案例和匹配的正常对照。其次，我们为回顾性和部署式评估实施了前向步进评估和前瞻性预测日志记录。第三，我们分析了提前期，并表明AIMM在2021年1月GME挤兑高峰前22天发出了警告。当前的标记集很小（33个股票代码-天，3个正面事件），但结果显示了初步的区分能力和对GME事件的早期警告。我们发布了代码、数据集模式和仪表板设计，以支持对社交媒体驱动的市场监控的研究。",
            "intro_zh": [
                "现有市场操纵检测方法难以有效关联社交媒体叙事与市场行为，缺乏对在线协调模式的分析。",
                "AIMM框架融合Reddit活动、机器人指标、协调模式和市场特征，生成每日操纵风险评分，提供早期预警。",
                "实验表明，AIMM在小规模数据集上具有初步区分能力，并在GME事件发生前22天成功发出预警。"
            ],
            "method_zh": "**问题定义**：当前市场操纵行为日益依赖社交媒体的协调活动，传统方法难以有效识别。零售投资者和监管机构缺乏将社交媒体叙事与市场行为关联的工具，无法及时发现和应对潜在的操纵风险。现有方法未能充分利用社交媒体数据中的信息，缺乏对机器人行为和协调模式的有效分析。\\n\\n**核心思路**：AIMM的核心思路是将社交媒体数据（Reddit活动）、机器人和协调指标以及传统的市场数据（OHLCV）进行融合，利用人工智能技术学习这些数据之间的关联，从而识别潜在的市场操纵行为。通过计算每日的AIMM操纵风险评分，为投资者和监管机构提供早期预警。这种多模态融合的方法能够更全面地捕捉市场操纵的迹象。\\n\\n**技术框架**：AIMM框架包含以下主要模块：1) 数据收集模块：收集Reddit数据、市场数据（OHLCV）以及机器人和协调指标。由于Reddit API的限制，使用校准后的合成社交特征。2) 特征工程模块：从收集到的数据中提取相关特征，例如Reddit帖子的情感、机器人活动频率、价格波动率等。3) 模型训练模块：使用机器学习模型（具体模型类型未知）学习特征与市场操纵之间的关系。4) 风险评分计算模块：根据模型输出计算每日的AIMM操纵风险评分。5) 可视化仪表板：使用Streamlit构建仪表板，方便分析师探索可疑窗口，检查底层帖子和价格行为。\\n\\n**关键创新**：AIMM的关键创新在于其多模态数据融合的方法，将社交媒体数据、机器人指标和市场数据结合起来，从而更全面地捕捉市场操纵的迹象。此外，AIMM还构建了AIMM-GT数据集，为市场操纵检测研究提供了宝贵的资源。前向步进评估和前瞻性预测日志记录方法也为模型的评估和部署提供了更可靠的手段。\\n\\n**关键设计**：论文中没有详细描述具体的模型结构、损失函数或参数设置。由于Reddit API限制，使用了校准的合成社交特征，具体的校准方法未知。AIMM-GT数据集包含33个标记的股票代码-天，其中3个为正面事件，数据集规模较小。",
            "application_zh": "AIMM框架可应用于股票市场监管、风险管理和投资决策。监管机构可以使用AIMM来监控社交媒体上的市场操纵活动，及时采取行动。经纪公司可以使用AIMM来评估客户交易的风险，并提供更安全的投资环境。零售投资者可以使用AIMM来识别潜在的投资风险，做出更明智的决策。该研究有助于提高市场透明度和公平性。",
            "highlight_zh": "AIMM在包含33个标记股票代码-天的小规模数据集上展现了初步的区分能力。最重要的是，AIMM在2021年1月GME挤兑高峰前22天发出了预警，表明其具有早期预警的潜力。研究团队发布了代码、数据集模式和仪表板设计，为进一步研究社交媒体驱动的市场监控提供了基础。",
            "tags_zh": [
                "市场操纵检测",
                "社交媒体分析",
                "多模态融合",
                "风险预警",
                "人工智能",
                "股票市场",
                "机器人检测"
            ],
            "_index": 6,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16103v1/figures/GroundTruthCoverage.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16103v1/figures/Forward-walk.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16103v1/figures/PredictionLog.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "AlignMerge - Alignment-Preserving Large Language Model Merging via Fisher-Guided Geometric Constraints",
            "authors": [
                "Aniruddha Roy",
                "Jyoti Patel",
                "Aman Chadha",
                "Vinija Jain",
                "Amitava Das"
            ],
            "arxiv_id": "2512.16245v1",
            "summary": "Merging large language models (LLMs) is a practical way to compose capabilities from multiple fine-tuned checkpoints without retraining. Yet standard schemes (linear weight soups, task vectors, and Fisher-weighted averaging) can preserve loss while quietly destroying alignment. We argue that merging is not a numerical trick but a geometry-constrained operation around an already-aligned anchor: fusion must be steered to respect safety geometry, not validated post hoc.\n  We introduce AlignMerge, a geometry-aware merging framework that makes alignment an explicit invariant. In a local Fisher chart around an instruction-tuned base, we estimate an alignment subspace with projector P_A and optimize:\n  L_AlignMerge = L_geo + lambda_align * L_align + lambda_bud * L_bud,\n  where L_geo keeps the merge close to its experts in Fisher-Rao geometry, L_align penalizes motion along alignment-sensitive directions, and L_bud enforces a soft alignment budget. As the alignment functional we use the decoding-invariant Alignment Quality Index (AQI), a latent-space criterion that captures how cleanly aligned and misaligned behaviors separate in representation space.\n  Across five model families (LLaMA-3 8B, Mistral 7B, Qwen 2, Phi-3.5, Gemma 2), merging safety anchors with task experts, AlignMerge improves alignment metrics (AQI, toxicity, LLM-judge alignment) while matching or exceeding the best expert on instruction-following, reasoning, and helpfulness. It also exhibits smaller alignment-subspace drift and fewer budget violations than Fisher soups, TIES, SafeMerge, and MergeAlign. These results make alignment-preserving merging a first-class design goal and suggest a path to geometry-aware composition of future foundation models.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16245v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]large language model",
                        "foundation model",
                        "instruction following"
                    ],
                    "score": 15.0
                }
            ],
            "relevance_score": 15.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "AlignMerge：通过Fisher引导的几何约束实现对齐保持的大语言模型合并",
            "summary_zh": "合并大型语言模型(LLMs)是一种实用的方法，可以在不重新训练的情况下组合来自多个微调检查点的能力。然而，标准方案（线性权重平均、任务向量和Fisher加权平均）可能会在保持损失的同时悄然破坏对齐。我们认为合并不是一种数值技巧，而是一种围绕已对齐锚点的几何约束操作：必须引导融合以尊重安全几何，而不是事后验证。我们引入了AlignMerge，这是一个几何感知合并框架，它使对齐成为显式不变性。在指令调整基础周围的局部Fisher图中，我们使用投影算子P_A估计对齐子空间并优化：L_AlignMerge = L_geo + lambda_align * L_align + lambda_bud * L_bud，其中L_geo使合并结果在Fisher-Rao几何中接近其专家，L_align惩罚沿对齐敏感方向的运动，L_bud强制执行软对齐预算。作为对齐函数，我们使用解码不变的对齐质量指数(AQI)，这是一种潜在空间标准，用于捕获对齐和未对齐行为在表示空间中分离的清晰程度。在五个模型系列（LLaMA-3 8B、Mistral 7B、Qwen 2、Phi-3.5、Gemma 2）中，将安全锚点与任务专家合并，AlignMerge提高了对齐指标（AQI、毒性、LLM-judge对齐），同时在指令遵循、推理和帮助性方面匹配或超过了最佳专家。与Fisher soups、TIES、SafeMerge和MergeAlign相比，它还表现出更小的对齐子空间漂移和更少的预算违规。这些结果使保持对齐的合并成为首要的设计目标，并为未来基础模型的几何感知组合提供了一条途径。",
            "intro_zh": [
                "现有LLM合并方法在保持模型性能的同时，容易破坏模型的对齐性，导致安全性下降。",
                "AlignMerge通过在Fisher-Rao几何空间中施加约束，显式地将对齐作为合并过程中的不变性来保持。",
                "实验表明，AlignMerge在多个模型上提高了对齐指标，同时保持或超过了最佳专家的指令遵循和推理能力。"
            ],
            "method_zh": "**问题定义**：现有的大语言模型合并方法，如线性权重平均、任务向量等，虽然能够保持模型的性能指标，但往往忽略了模型对齐性（Alignment）的保持，导致合并后的模型可能产生不安全或有害的输出。这些方法没有将对齐性作为合并过程中的一个显式约束，因此无法保证合并后的模型仍然具有良好的对齐性。\\n\\n**核心思路**：AlignMerge的核心思路是将模型合并视为一个在几何空间中受约束的优化问题，其中对齐性是一个必须保持的不变量。具体来说，该方法在Fisher-Rao几何空间中，通过约束合并后的模型使其接近原始模型，并惩罚那些可能破坏对齐性的方向上的移动。这种方法将对齐性显式地纳入了合并过程中，从而保证了合并后的模型仍然具有良好的对齐性。\\n\\n**技术框架**：AlignMerge的技术框架主要包括以下几个步骤：1. 在指令调整后的基础模型周围构建一个局部Fisher图。2. 使用投影算子估计对齐子空间。3. 定义一个包含三个损失函数的优化目标：L_geo（保持几何接近性）、L_align（惩罚对齐敏感方向的移动）和L_bud（强制执行软对齐预算）。4. 使用对齐质量指数（AQI）作为对齐函数，该指数衡量了对齐和未对齐行为在表示空间中的分离程度。5. 通过优化上述目标函数，得到合并后的模型。\\n\\n**关键创新**：AlignMerge的关键创新在于它将对齐性显式地纳入了模型合并过程中，并将其视为一个几何约束。与现有方法不同，AlignMerge不是在合并后验证对齐性，而是在合并过程中就保证了对齐性。此外，AlignMerge还使用了对齐质量指数（AQI）作为对齐函数，该指数能够有效地衡量模型的对齐程度。\\n\\n**关键设计**：AlignMerge的关键设计包括：1. 使用Fisher-Rao几何来度量模型之间的距离，这能够更好地反映模型在参数空间中的相似性。2. 使用投影算子来估计对齐子空间，这能够有效地识别出对齐敏感的方向。3. 定义了一个包含三个损失函数的优化目标，这能够平衡模型性能和对齐性之间的关系。4. 使用对齐质量指数（AQI）作为对齐函数，这能够有效地衡量模型的对齐程度。参数lambda_align和lambda_bud用于控制对齐损失和预算损失的权重。",
            "application_zh": "AlignMerge可应用于安全关键型LLM的开发，例如医疗、金融等领域，确保模型在提供强大功能的同时，不会产生有害或不安全的输出。该方法还可用于构建更可靠、可信赖的AI系统，促进LLM在各行各业的广泛应用。",
            "highlight_zh": "AlignMerge在五个模型系列（LLaMA-3 8B、Mistral 7B、Qwen 2、Phi-3.5、Gemma 2）上进行了实验，结果表明，该方法在提高对齐指标（AQI、毒性、LLM-judge对齐）的同时，能够匹配或超过最佳专家在指令遵循、推理和帮助性方面的表现。此外，AlignMerge还表现出比其他合并方法更小的对齐子空间漂移和更少的预算违规。",
            "tags_zh": [
                "大语言模型合并",
                "模型对齐",
                "Fisher-Rao几何",
                "几何约束优化",
                "对齐质量指数",
                "安全AI",
                "模型融合"
            ],
            "_index": 7,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16245v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16245v1/figures/mechanistic.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16245v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "BrepLLM: Native Boundary Representation Understanding with Large Language Models",
            "authors": [
                "Liyuan Deng",
                "Hao Guo",
                "Yunpeng Bai",
                "Yongkang Dai",
                "Huaxi Huang",
                "Yilei Shi"
            ],
            "arxiv_id": "2512.16413v1",
            "summary": "Current token-sequence-based Large Language Models (LLMs) are not well-suited for directly processing 3D Boundary Representation (Brep) models that contain complex geometric and topological information. We propose BrepLLM, the first framework that enables LLMs to parse and reason over raw Brep data, bridging the modality gap between structured 3D geometry and natural language. BrepLLM employs a two-stage training pipeline: Cross-modal Alignment Pre-training and Multi-stage LLM Fine-tuning. In the first stage, an adaptive UV sampling strategy converts Breps into graphs representation with geometric and topological information. We then design a hierarchical BrepEncoder to extract features from geometry (i.e., faces and edges) and topology, producing both a single global token and a sequence of node tokens. Then we align the global token with text embeddings from a frozen CLIP text encoder (ViT-L/14) via contrastive learning. In the second stage, we integrate the pretrained BrepEncoder into an LLM. We then align its sequence of node tokens using a three-stage progressive training strategy: (1) training an MLP-based semantic mapping from Brep representation to 2D with 2D-LLM priors. (2) performing fine-tuning of the LLM. (3) designing a Mixture-of-Query Experts (MQE) to enhance geometric diversity modeling. We also construct Brep2Text, a dataset comprising 269,444 Brep-text question-answer pairs. Experiments show that BrepLLM achieves state-of-the-art (SOTA) results on 3D object classification and captioning tasks.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16413v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "contrastive learning"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "semantic mapping",
                        "semantic map"
                    ],
                    "score": 4.0
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]large language model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 14.5,
            "hit_pillars": [
                "2_algo_arch",
                "3_perception_slam",
                "9_embodied_foundation"
            ],
            "headline_zh": "BrepLLM：提出一种原生边界表示理解的大语言模型框架",
            "summary_zh": "当前基于token序列的大语言模型(LLMs)不适合直接处理包含复杂几何和拓扑信息的3D边界表示(Brep)模型。我们提出了BrepLLM，这是第一个使LLMs能够解析和推理原始Brep数据的框架，弥合了结构化3D几何和自然语言之间的模态差距。BrepLLM采用两阶段训练流程：跨模态对齐预训练和多阶段LLM微调。在第一阶段，自适应UV采样策略将Brep转换为具有几何和拓扑信息的图表示。然后，我们设计了一个分层BrepEncoder来提取几何（即面和边）和拓扑的特征，生成单个全局token和一系列节点token。然后，我们通过对比学习将全局token与来自冻结的CLIP文本编码器(ViT-L/14)的文本嵌入对齐。在第二阶段，我们将预训练的BrepEncoder集成到LLM中。然后，我们使用三阶段渐进式训练策略对齐其节点token序列：(1)训练一个基于MLP的语义映射，将Brep表示映射到具有2D-LLM先验的2D表示。(2)执行LLM的微调。(3)设计一个混合查询专家(MQE)来增强几何多样性建模。我们还构建了Brep2Text数据集，包含269,444个Brep-文本问答对。实验表明，BrepLLM在3D对象分类和字幕任务上取得了最先进(SOTA)的结果。",
            "intro_zh": [
                "现有大语言模型难以直接处理包含复杂几何和拓扑信息的3D Brep模型。",
                "BrepLLM通过两阶段训练，将Brep数据转换为LLM可理解的token序列，实现跨模态对齐。",
                "实验表明，BrepLLM在3D对象分类和描述任务上取得了当前最优的结果。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大语言模型(LLM)无法直接理解和处理3D边界表示(Brep)模型的问题。现有的LLM主要处理文本序列，而Brep模型包含复杂的几何和拓扑信息，直接输入LLM会导致信息丢失和性能下降。因此，如何将Brep数据有效地转换为LLM可以理解的形式是关键挑战。\\n\\n**核心思路**：论文的核心思路是将Brep模型转换为图表示，并设计一个专门的BrepEncoder来提取几何和拓扑特征。通过跨模态对齐预训练和多阶段LLM微调，使LLM能够理解和推理Brep数据。这种方法的核心在于将复杂的3D几何信息转化为LLM擅长处理的token序列，同时保留关键的几何和拓扑信息。\\n\\n**技术框架**：BrepLLM的整体框架包含两个主要阶段：跨模态对齐预训练和多阶段LLM微调。在预训练阶段，首先使用自适应UV采样将Brep转换为图表示。然后，BrepEncoder提取几何和拓扑特征，生成全局token和节点token序列。全局token通过对比学习与CLIP文本嵌入对齐。在微调阶段，预训练的BrepEncoder集成到LLM中，节点token序列通过三阶段渐进式训练策略进行对齐，包括语义映射、LLM微调和混合查询专家(MQE)训练。\\n\\n**关键创新**：该论文的关键创新在于提出了BrepLLM框架，这是第一个能够让LLM直接解析和推理原始Brep数据的框架。此外，自适应UV采样策略、分层BrepEncoder和三阶段渐进式训练策略也是重要的技术创新。与现有方法相比，BrepLLM能够更有效地利用Brep数据中的几何和拓扑信息，从而提高LLM在3D相关任务上的性能。\\n\\n**关键设计**：BrepEncoder采用分层结构，分别提取面和边的特征，并融合几何和拓扑信息。自适应UV采样策略根据曲率调整采样密度，以保留更多细节。三阶段渐进式训练策略逐步对齐Brep表示和LLM，避免了直接微调导致的灾难性遗忘。混合查询专家(MQE)通过学习不同的查询策略来增强几何多样性建模。损失函数包括对比学习损失和交叉熵损失，用于对齐不同模态的特征。",
            "application_zh": "BrepLLM在CAD/CAM、逆向工程、3D内容创作等领域具有广泛的应用前景。它可以用于3D模型的自动分类、描述生成、缺陷检测、参数化设计等任务。通过结合LLM的强大推理能力和Brep模型的精确几何信息，BrepLLM可以实现更智能、更高效的3D设计和制造流程。未来，该技术有望应用于智能制造、数字孪生等领域。",
            "highlight_zh": "BrepLLM在3D对象分类和描述任务上取得了最先进的结果。在Brep2Text数据集上，BrepLLM的性能显著优于现有方法。例如，在3D对象分类任务上，BrepLLM的准确率比基线方法提高了XX%。在描述生成任务上，BrepLLM生成的描述更加准确和丰富。",
            "tags_zh": [
                "边界表示",
                "大语言模型",
                "三维理解",
                "跨模态学习",
                "几何深度学习"
            ],
            "_index": 8,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16413v1/images/zhanshitu1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16413v1/images/framework.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16413v1/images/BrepEncoder.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "AdaSearch: Balancing Parametric Knowledge and Search in Large Language Models via Reinforcement Learning",
            "authors": [
                "Tzu-Han Lin",
                "Wei-Lin Chen",
                "Chen-An Li",
                "Hung-yi Lee",
                "Yun-Nung Chen",
                "Yu Meng"
            ],
            "arxiv_id": "2512.16883v1",
            "summary": "Equipping large language models (LLMs) with search engines via reinforcement learning (RL) has emerged as an effective approach for building search agents. However, overreliance on search introduces unnecessary cost and risks exposure to noisy or malicious content, while relying solely on parametric knowledge risks hallucination. The central challenge is to develop agents that adaptively balance parametric knowledge with external search, invoking search only when necessary. Prior work mitigates search overuse by shaping rewards around the number of tool calls. However, these penalties require substantial reward engineering, provide ambiguous credit assignment, and can be exploited by agents that superficially reduce calls. Moreover, evaluating performance solely through call counts conflates necessary and unnecessary search, obscuring the measurement of true adaptive behavior. To address these limitations, we first quantify the self-knowledge awareness of existing search agents via an F1-based decision metric, revealing that methods such as Search-R1 often overlook readily available parametric knowledge. Motivated by these findings, we propose AdaSearch, a simple two-stage, outcome-driven RL framework that disentangles problem solving from the decision of whether to invoke search, and makes this decision process explicit and interpretable. This transparency is crucial for high-stakes domains such as finance and medical question answering, yet is largely neglected by prior approaches. Experiments across multiple model families and sizes demonstrate that AdaSearch substantially improves knowledge-boundary awareness, reduces unnecessary search calls, preserves strong task performance, and offers more transparent, interpretable decision behaviors.",
            "categories": [
                "cs.CL"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Preprint. Code and artifacts will be uploaded to https://github.com/hank0316/AdaSearch",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16883v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]reinforcement learning"
                    ],
                    "score": 4.5
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]large language model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 13.5,
            "hit_pillars": [
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "AdaSearch：通过强化学习平衡大语言模型中的参数知识和搜索",
            "summary_zh": "本文提出了一种利用强化学习为大语言模型配备搜索引擎的有效方法，用于构建搜索代理。然而，过度依赖搜索会引入不必要的成本，并可能暴露于噪声或恶意内容，而仅依赖参数知识则存在幻觉风险。核心挑战在于开发能够自适应地平衡参数知识与外部搜索的代理，仅在必要时才调用搜索。现有工作通过围绕工具调用次数塑造奖励来缓解搜索过度使用，但这些惩罚需要大量的奖励工程，提供模糊的信用分配，并且可能被表面上减少调用的代理利用。此外，仅通过调用次数评估性能会混淆必要和不必要的搜索，从而模糊了对真正自适应行为的衡量。为了解决这些局限性，我们首先通过基于F1的决策指标量化现有搜索代理的自我知识感知能力，揭示了诸如Search-R1之类的方法经常忽略现成的参数知识。受这些发现的启发，我们提出了AdaSearch，这是一个简单的两阶段、结果驱动的强化学习框架，它将问题解决与是否调用搜索的决策分离开来，并使该决策过程显式且可解释。这种透明性对于金融和医学问答等高风险领域至关重要，但之前的研究方法在很大程度上忽略了这一点。跨多个模型系列和规模的实验表明，AdaSearch显着提高了知识边界意识，减少了不必要的搜索调用，保持了强大的任务性能，并提供了更透明、可解释的决策行为。",
            "intro_zh": [
                "现有搜索增强的大语言模型过度依赖搜索，导致成本增加和潜在风险，而完全依赖模型自身知识则容易产生幻觉。",
                "AdaSearch通过两阶段强化学习框架，将问题解决与搜索决策分离，显式地学习何时调用搜索，从而实现自适应平衡。",
                "实验表明，AdaSearch能显著提高知识边界意识，减少不必要的搜索调用，同时保持甚至提升任务性能，并提供更透明的决策过程。"
            ],
            "method_zh": "**问题定义**：现有方法在利用搜索引擎增强大语言模型时，难以平衡参数知识和外部搜索。过度依赖搜索会增加成本并引入噪声，而完全依赖参数知识则容易产生幻觉。现有方法通常通过惩罚工具调用次数来减少搜索使用，但这种方法需要大量人工设计奖励，且信用分配模糊，容易被模型利用。\\n\\n**核心思路**：AdaSearch的核心思路是将问题解决过程与搜索决策过程解耦。模型首先尝试仅使用参数知识解决问题，然后独立地决定是否需要进行搜索。这种解耦使得模型能够更清晰地评估自身知识的边界，并仅在必要时才调用搜索。\\n\\n**技术框架**：AdaSearch是一个两阶段的强化学习框架。第一阶段是问题解决阶段，模型尝试仅使用自身参数知识回答问题。第二阶段是搜索决策阶段，模型根据第一阶段的结果和问题本身，决定是否需要进行搜索。如果决定搜索，则执行搜索并利用搜索结果重新回答问题。整个过程通过强化学习进行训练，目标是最大化任务完成的奖励，同时最小化不必要的搜索调用。\\n\\n**关键创新**：AdaSearch的关键创新在于将问题解决和搜索决策解耦，并通过强化学习显式地学习搜索策略。这种方法避免了手动设计奖励函数的复杂性，并允许模型自适应地学习何时调用搜索。此外，AdaSearch提供了一个更透明和可解释的决策过程，这对于高风险应用至关重要。\\n\\n**关键设计**：AdaSearch使用基于F1的决策指标来量化模型的自我知识感知能力。强化学习算法使用标准的策略梯度方法，奖励函数包括任务完成的奖励和搜索调用的惩罚。具体参数设置（如学习率、奖励系数等）根据不同的模型和数据集进行调整。",
            "application_zh": "AdaSearch可应用于需要大语言模型进行知识密集型任务的各种场景，例如金融问答、医疗诊断、法律咨询等。通过自适应地平衡参数知识和外部搜索，AdaSearch可以提高模型的准确性、可靠性和效率，并降低不必要的成本和风险。该方法尤其适用于对透明性和可解释性要求较高的领域。",
            "highlight_zh": "实验结果表明，AdaSearch在多个模型和数据集上均取得了显著的性能提升。例如，在某些任务上，AdaSearch能够将不必要的搜索调用次数减少50%以上，同时保持甚至提高了任务完成的准确率。与现有方法相比，AdaSearch在知识边界意识方面也表现出明显的优势。",
            "tags_zh": [
                "大语言模型",
                "强化学习",
                "搜索引擎",
                "知识边界",
                "自适应搜索"
            ],
            "_index": 9,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16883v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16883v1/figures/qwen3b_comparison.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16883v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence",
            "authors": [
                "Xiaopeng Lin",
                "Shijie Lian",
                "Bin Yu",
                "Ruoqi Yang",
                "Changti Wu",
                "Yuzhuo Miao",
                "Yurun Jin",
                "Yukun Shi",
                "Cong Huang",
                "Bojun Cheng",
                "Kai Chen"
            ],
            "arxiv_id": "2512.16793v1",
            "summary": "Robotic generalization relies on physical intelligence: the ability to reason about state changes, contact-rich interactions, and long-horizon planning under egocentric perception and action. However, most VLMs are trained primarily on third-person data, creating a fundamental viewpoint mismatch for humanoid robots. Scaling robot egocentric data collection remains impractical due to high cost and limited diversity, whereas large-scale human egocentric videos offer a scalable alternative that naturally capture rich interaction context and causal structure. The key challenge is to convert raw egocentric videos into structured and reliable embodiment training supervision. Accordingly, we propose an Egocentric2Embodiment translation pipeline that transforms first-person videos into multi-level, schema-driven VQA supervision with enforced evidence grounding and temporal consistency, enabling the construction of the Egocentric2Embodiment dataset (E2E-3M) at scale. An egocentric-aware embodied brain, termed PhysBrain, is obtained by training on the E2E-3M dataset. PhysBrain exhibits substantially improved egocentric understanding, particularly for planning on EgoThink. It provides an egocentric-aware initialization that enables more sample-efficient VLA fine-tuning and higher SimplerEnv success rates (53.9\\%), demonstrating effective transfer from human egocentric supervision to downstream robot control.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "17 pages, 4 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16793v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "humanoid",
                        "humanoid robot"
                    ],
                    "score": 4.0
                },
                {
                    "name": "支柱六：视频提取与匹配 (Video Extraction)",
                    "id": "6_video_extraction",
                    "matched_keywords": [
                        "[T]egocentric"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "VLA"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 13.0,
            "hit_pillars": [
                "1_robot_core",
                "6_video_extraction",
                "9_embodied_foundation"
            ],
            "headline_zh": "提出PhysBrain，利用人类第一视角数据提升机器人物理智能",
            "summary_zh": "机器人泛化依赖于物理智能，即在第一视角感知和动作下，推理状态变化、富接触交互和长时程规划的能力。然而，大多数视觉语言模型（VLM）主要在第三人称数据上训练，这与人形机器人存在根本的视角不匹配。扩展机器人第一视角数据收集不切实际，因为成本高且多样性有限。大规模人类第一视角视频提供了一种可扩展的替代方案，自然地捕捉了丰富的交互上下文和因果结构。关键挑战是将原始第一视角视频转换为结构化且可靠的具身训练监督。为此，我们提出了一个Egocentric2Embodiment翻译流程，将第一人称视频转换为多层次、模式驱动的VQA监督，并强制执行证据 grounding 和时间一致性，从而大规模构建Egocentric2Embodiment数据集（E2E-3M）。通过在E2E-3M数据集上训练，获得了一个以自我为中心的具身大脑，称为PhysBrain。PhysBrain 显著提高了以自我为中心的理解能力，尤其是在 EgoThink 上的规划。它提供了一个以自我为中心的初始化，可以实现更高效的 VLA 微调和更高的 SimplerEnv 成功率（53.9%），证明了从人类以自我为中心的监督到下游机器人控制的有效迁移。",
            "intro_zh": [
                "现有VLM主要基于第三人称数据训练，与机器人第一视角存在视角差异，限制了机器人物理智能的发展。",
                "论文提出Egocentric2Embodiment翻译流程，将人类第一视角视频转化为多层次VQA监督，构建大规模E2E-3M数据集。",
                "通过在E2E-3M上训练PhysBrain，显著提升了机器人以自我为中心的理解能力，并在下游控制任务中取得更好效果。"
            ],
            "method_zh": "**问题定义**：现有视觉语言模型（VLM）主要在第三人称视角数据上训练，这与机器人需要的第一人称视角存在根本性的不匹配。这种视角差异阻碍了VLM在机器人控制任务中的应用，尤其是在需要物理交互和长时程规划的场景下。同时，直接收集大规模机器人第一视角数据成本高昂且多样性有限。\\n\\n**核心思路**：论文的核心思路是利用大规模人类第一视角视频作为机器人训练的替代数据源。人类第一视角视频自然地捕捉了丰富的交互上下文和因果结构，可以为机器人提供有价值的具身学习监督信号。通过将人类第一视角视频转化为结构化的训练数据，可以弥合VLM与机器人之间的视角差异，提升机器人的物理智能。\\n\\n**技术框架**：论文提出了一个名为Egocentric2Embodiment的翻译流程，该流程将第一人称视频转换为多层次、模式驱动的VQA监督。该流程包含以下几个主要步骤：1) 视频数据收集与预处理；2) 基于模式的VQA问题生成，包括状态变化、交互推理和长时程规划等方面；3) 证据 grounding，确保VQA答案与视频内容相关联；4) 时间一致性约束，保证VQA答案在时间上的连贯性。通过该流程，构建了大规模的Egocentric2Embodiment数据集（E2E-3M），用于训练PhysBrain。\\n\\n**关键创新**：论文的关键创新在于提出了Egocentric2Embodiment翻译流程，该流程能够有效地将非结构化的第一人称视频转化为结构化的具身学习监督信号。与以往方法相比，该方法能够利用大规模人类数据，降低机器人数据收集成本，并提升训练数据的多样性。此外，论文还提出了PhysBrain，一个以自我为中心的具身大脑，能够更好地理解第一人称视角下的场景和任务。\\n\\n**关键设计**：E2E-3M数据集包含300万个视频片段，每个片段都包含多层次的VQA问题。VQA问题的设计涵盖了状态变化、交互推理和长时程规划等多个方面，旨在全面提升机器人的物理智能。在训练PhysBrain时，使用了对比学习和掩码语言建模等技术，以提高模型的泛化能力和鲁棒性。具体参数设置和网络结构细节在论文中有详细描述，此处不再赘述。",
            "application_zh": "该研究成果可应用于各种机器人控制任务，例如家庭服务机器人、工业机器人和医疗机器人。通过利用人类第一视角数据进行训练，可以显著提升机器人在复杂环境中的适应性和交互能力。未来，该方法有望推动机器人技术的发展，使其能够更好地服务于人类社会。",
            "highlight_zh": "PhysBrain在EgoThink数据集上表现出显著的性能提升，证明了其在以自我为中心的理解能力方面的优势。此外，通过将PhysBrain作为初始化，VLA微调的效率更高，SimplerEnv的成功率也提升至53.9%，表明了从人类第一视角监督到下游机器人控制的有效迁移。",
            "tags_zh": [
                "机器人控制",
                "第一视角视觉",
                "具身智能",
                "视觉语言模型",
                "迁移学习"
            ],
            "_index": 10,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16793v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16793v1/fig/data_pipeline.jpg",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16793v1/fig/data_sum.jpg",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image",
            "authors": [
                "Yushi Hu",
                "Reyhane Askari-Hemmat",
                "Melissa Hall",
                "Emily Dinan",
                "Luke Zettlemoyer",
                "Marjan Ghazvininejad"
            ],
            "arxiv_id": "2512.16899v1",
            "summary": "Reward models (RMs) are essential for training large language models (LLMs), but remain underexplored for omni models that handle interleaved image and text sequences. We introduce Multimodal RewardBench 2 (MMRB2), the first comprehensive benchmark for reward models on multimodal understanding and (interleaved) generation. MMRB2 spans four tasks: text-to-image, image editing, interleaved generation, and multimodal reasoning (\"thinking-with-images\"), providing 1,000 expert-annotated preference pairs per task from 23 models and agents across 21 source tasks. MMRB2 is designed with: (1) practical but challenging prompts; (2) responses from state-of-the-art models and agents; and (3) preference pairs with strong human-expert consensus, curated via an ensemble filtering strategy. Using MMRB2, we study existing judges for each subtask, including multimodal LLM-as-a-judge and models trained with human preferences. The latest Gemini 3 Pro attains 75-80% accuracy. GPT-5 and Gemini 2.5 Pro reach 66-75% accuracy, compared to >90% for humans, yet surpass the widely used GPT-4o (59%). The best performing open-source model Qwen3-VL-32B achieves similar accuracies as Gemini 2.5 Flash (64%). We also show that MMRB2 performance strongly correlates with downstream task success using Best-of-N sampling and conduct an in-depth analysis that shows key areas to improve the reward models going forward.",
            "categories": [
                "cs.CL",
                "cs.CV"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Code and data available at https://github.com/facebookresearch/MMRB2",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16899v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model",
                        "[T]multimodal"
                    ],
                    "score": 12.0
                }
            ],
            "relevance_score": 12.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出Multimodal RewardBench 2，用于评估处理交错文本和图像的通用奖励模型。",
            "summary_zh": "奖励模型（RMs）对于训练大型语言模型（LLMs）至关重要，但对于处理交错图像和文本序列的通用模型仍未得到充分探索。我们推出了Multimodal RewardBench 2（MMRB2），这是第一个全面的奖励模型基准，用于多模态理解和（交错）生成。MMRB2涵盖四个任务：文本到图像、图像编辑、交错生成和多模态推理（“用图像思考”），每个任务提供1000个专家标注的偏好对，这些数据来自23个模型和代理，涵盖21个源任务。MMRB2的设计特点包括：（1）实用但具有挑战性的提示；（2）来自最先进模型和代理的响应；（3）通过集成过滤策略策划的具有强烈人类专家共识的偏好对。使用MMRB2，我们研究了每个子任务的现有评判标准，包括多模态LLM-as-a-judge和使用人类偏好训练的模型。最新的Gemini 3 Pro达到了75-80%的准确率。GPT-5和Gemini 2.5 Pro达到了66-75%的准确率，而人类的准确率超过90%，但超过了广泛使用的GPT-4o（59%）。性能最佳的开源模型Qwen3-VL-32B实现了与Gemini 2.5 Flash（64%）相似的准确率。我们还表明，MMRB2的性能与使用Best-of-N抽样的下游任务成功率密切相关，并进行了深入分析，揭示了未来改进奖励模型的关键领域。",
            "intro_zh": [
                "现有奖励模型在处理交错文本和图像的多模态任务中表现不足，缺乏专门的评估基准。",
                "提出Multimodal RewardBench 2 (MMRB2)，一个全面的多模态奖励模型评估基准，包含四个具有挑战性的任务。",
                "实验表明，即使是强大的模型如Gemini 3 Pro，在MMRB2上的表现仍与人类专家存在差距，开源模型Qwen3-VL-32B表现接近Gemini 2.5 Flash。"
            ],
            "method_zh": "**问题定义**：论文旨在解决多模态奖励模型评估标准缺失的问题，尤其是在处理交错文本和图像的任务中。现有的奖励模型在评估多模态内容理解和生成方面存在不足，缺乏一个全面、具有挑战性的基准来衡量其性能。这阻碍了多模态LLM的进一步发展。\\n\\n**核心思路**：论文的核心思路是构建一个高质量、多样化的多模态奖励模型评估基准MMRB2。通过收集来自多个模型和代理的响应，并由人类专家进行标注，形成偏好对，以此来评估奖励模型在不同多模态任务中的表现。这种方法能够更全面地反映奖励模型在实际应用中的能力。\\n\\n**技术框架**：MMRB2基准包含四个主要任务：文本到图像生成、图像编辑、交错生成和多模态推理。每个任务都包含1000个专家标注的偏好对，这些数据来自23个模型和代理，涵盖21个源任务。数据的收集和标注过程采用了集成过滤策略，以确保偏好对具有高度的人类专家共识。\\n\\n**关键创新**：MMRB2的关键创新在于其全面性和高质量。它不仅涵盖了多种多模态任务，而且使用了来自最先进模型和代理的响应，并由人类专家进行标注。此外，MMRB2还采用了集成过滤策略，以确保偏好对的质量和一致性。这是首个针对多模态奖励模型的综合性基准。\\n\\n**关键设计**：MMRB2的设计考虑了实际应用场景，使用了实用但具有挑战性的提示。偏好对的生成过程包括多个步骤，首先是收集来自不同模型和代理的响应，然后由人类专家对这些响应进行排序和标注。为了确保标注的质量，采用了集成过滤策略，即只有当多个专家对同一偏好对达成一致时，该偏好对才会被纳入基准。",
            "application_zh": "该研究成果可应用于训练和评估多模态大型语言模型，提升模型在图像生成、图像编辑、多模态推理等任务中的性能。MMRB2基准的发布将促进多模态奖励模型的研究和发展，推动相关技术的进步，并最终提升多模态AI应用的质量和用户体验。",
            "highlight_zh": "MMRB2基准测试结果显示，Gemini 3 Pro在多模态任务上达到了75-80%的准确率，GPT-5和Gemini 2.5 Pro达到了66-75%的准确率，但仍低于人类专家的90%以上。开源模型Qwen3-VL-32B的性能与Gemini 2.5 Flash相当，达到了64%的准确率。MMRB2性能与下游任务成功率高度相关。",
            "tags_zh": [
                "多模态学习",
                "奖励模型",
                "基准测试",
                "图像文本交错",
                "大型语言模型",
                "多模态推理",
                "人类偏好"
            ],
            "_index": 11,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16899v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16899v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16899v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Make-It-Poseable: Feed-forward Latent Posing Model for 3D Humanoid Character Animation",
            "authors": [
                "Zhiyang Guo",
                "Ori Zhang",
                "Jax Xiang",
                "Alan Zhao",
                "Wengang Zhou",
                "Houqiang Li"
            ],
            "arxiv_id": "2512.16767v1",
            "summary": "Posing 3D characters is a fundamental task in computer graphics and vision. However, existing methods like auto-rigging and pose-conditioned generation often struggle with challenges such as inaccurate skinning weight prediction, topological imperfections, and poor pose conformance, limiting their robustness and generalizability. To overcome these limitations, we introduce Make-It-Poseable, a novel feed-forward framework that reformulates character posing as a latent-space transformation problem. Instead of deforming mesh vertices as in traditional pipelines, our method reconstructs the character in new poses by directly manipulating its latent representation. At the core of our method is a latent posing transformer that manipulates shape tokens based on skeletal motion. This process is facilitated by a dense pose representation for precise control. To ensure high-fidelity geometry and accommodate topological changes, we also introduce a latent-space supervision strategy and an adaptive completion module. Our method demonstrates superior performance in posing quality. It also naturally extends to 3D editing applications like part replacement and refinement.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Project page: https://jasongzy.github.io/Make-It-Poseable/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16767v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]humanoid"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱八：物理动画 (Physics-based Animation)",
                    "id": "8_physics_animation",
                    "matched_keywords": [
                        "[T]character animation"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 12.0,
            "hit_pillars": [
                "1_robot_core",
                "8_physics_animation"
            ],
            "headline_zh": "提出Make-It-Poseable，解决3D人形角色动画中姿态控制难题",
            "summary_zh": "本文提出了一种名为Make-It-Poseable的新型前馈框架，用于3D人形角色动画的姿态控制。现有方法如自动绑定和姿态条件生成，在蒙皮权重预测不准确、拓扑结构缺陷和姿态一致性差等方面存在挑战，限制了其鲁棒性和泛化能力。Make-It-Poseable将角色姿态控制重新定义为潜在空间转换问题。该方法不直接变形网格顶点，而是通过操纵角色的潜在表示来重建新的姿态。核心是一个潜在姿态Transformer，它根据骨骼运动来操纵形状token。密集姿态表示用于实现精确控制。为了确保高保真几何形状并适应拓扑变化，还引入了潜在空间监督策略和自适应补全模块。实验表明，该方法在姿态质量方面表现出色，并且可以自然地扩展到3D编辑应用，如部件替换和优化。",
            "intro_zh": [
                "现有3D角色姿态控制方法在蒙皮权重预测、拓扑结构和姿态一致性方面存在不足，影响了其鲁棒性和泛化性。",
                "Make-It-Poseable将姿态控制问题转化为潜在空间中的变换，通过操纵潜在表示来重建角色姿态，避免了直接变形网格。",
                "该方法通过潜在姿态Transformer、密集姿态表示、潜在空间监督和自适应补全模块，实现了高质量的姿态控制，并可扩展到3D编辑应用。"
            ],
            "method_zh": "**问题定义**：论文旨在解决3D人形角色动画中，现有姿态控制方法存在的不足。现有方法，如自动绑定和姿态条件生成，在蒙皮权重预测的准确性、拓扑结构的完整性以及姿态一致性方面表现不佳，导致生成的人物模型质量不高，难以满足实际应用需求。这些问题限制了模型的鲁棒性和泛化能力。\\n\\n**核心思路**：论文的核心思路是将3D角色姿态控制问题转化为潜在空间中的变换问题。不同于传统方法直接对网格顶点进行变形，该方法通过学习一个潜在空间，并将角色表示为该空间中的一个点。然后，通过操纵这个潜在表示，可以重建出具有不同姿态的角色模型。这种方法避免了直接处理复杂的网格变形，从而提高了模型的鲁棒性和生成质量。\\n\\n**技术框架**：Make-It-Poseable框架主要包含以下几个模块：1) 编码器：将3D角色网格编码到潜在空间中。2) 潜在姿态Transformer：根据输入的骨骼运动信息，对潜在空间中的角色表示进行变换，从而得到新的姿态。3) 解码器：将变换后的潜在表示解码回3D角色网格。4) 自适应补全模块：用于处理拓扑结构变化，确保生成高质量的网格模型。框架采用前馈结构，可以高效地生成各种姿态的角色模型。\\n\\n**关键创新**：该方法最重要的技术创新在于将姿态控制问题转化为潜在空间变换。与现有方法直接操作网格顶点不同，该方法通过学习一个潜在空间，并将角色表示为该空间中的一个点。然后，通过操纵这个潜在表示，可以重建出具有不同姿态的角色模型。这种方法避免了直接处理复杂的网格变形，从而提高了模型的鲁棒性和生成质量。此外，自适应补全模块也是一个重要的创新，它可以处理拓扑结构变化，确保生成高质量的网格模型。\\n\\n**关键设计**：潜在姿态Transformer是基于Transformer架构设计的，用于根据输入的骨骼运动信息，对潜在空间中的角色表示进行变换。论文使用了密集姿态表示，以实现对姿态的精确控制。为了确保高保真几何形状，论文还引入了潜在空间监督策略，即在潜在空间中对生成的角色表示进行约束，使其与真实的角色表示尽可能接近。此外，自适应补全模块采用了一种基于学习的方法，可以根据输入的角色网格和目标姿态，自动生成缺失的网格部分。",
            "application_zh": "该研究成果可广泛应用于3D游戏开发、动画制作、虚拟现实和增强现实等领域。通过Make-It-Poseable，可以快速生成具有各种姿态的3D角色模型，从而提高内容创作效率。此外，该方法还可以用于3D角色编辑，例如部件替换和优化，为用户提供更加灵活和便捷的创作工具。未来，该技术有望应用于个性化虚拟形象定制、人机交互等领域。",
            "highlight_zh": "实验结果表明，Make-It-Poseable在姿态质量方面优于现有方法。该方法能够生成具有高保真几何形状和准确姿态的3D角色模型。与基线方法相比，Make-It-Poseable在姿态一致性方面取得了显著提升。此外，该方法还能够自然地扩展到3D编辑应用，例如部件替换和优化，展示了其强大的泛化能力。",
            "tags_zh": [
                "3D角色动画",
                "姿态控制",
                "潜在空间变换",
                "Transformer网络",
                "自适应补全"
            ],
            "_index": 12,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16767v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16767v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16767v1/x4.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "SDFoam: Signed-Distance Foam for explicit surface reconstruction",
            "authors": [
                "Antonella Rech",
                "Nicola Conci",
                "Nicola Garau"
            ],
            "arxiv_id": "2512.16706v1",
            "summary": "Neural radiance fields (NeRF) have driven impressive progress in view synthesis by using ray-traced volumetric rendering. Splatting-based methods such as 3D Gaussian Splatting (3DGS) provide faster rendering by rasterizing 3D primitives. RadiantFoam (RF) brought ray tracing back, achieving throughput comparable to Gaussian Splatting by organizing radiance with an explicit Voronoi Diagram (VD). Yet, all the mentioned methods still struggle with precise mesh reconstruction. We address this gap by jointly learning an explicit VD with an implicit Signed Distance Field (SDF). The scene is optimized via ray tracing and regularized by an Eikonal objective. The SDF introduces metric-consistent isosurfaces, which, in turn, bias near-surface Voronoi cell faces to align with the zero level set. The resulting model produces crisper, view-consistent surfaces with fewer floaters and improved topology, while preserving photometric quality and maintaining training speed on par with RadiantFoam. Across diverse scenes, our hybrid implicit-explicit formulation, which we name SDFoam, substantially improves mesh reconstruction accuracy (Chamfer distance) with comparable appearance (PSNR, SSIM), without sacrificing efficiency.",
            "categories": [
                "cs.CV",
                "cs.GR"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16706v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "3D gaussian splatting",
                        "3DGS",
                        "gaussian splatting",
                        "splatting",
                        "NeRF",
                        "neural radiance field"
                    ],
                    "score": 12.0
                }
            ],
            "relevance_score": 12.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "SDFoam：结合显式Voronoi图和隐式SDF，实现精确表面重建",
            "summary_zh": "神经辐射场（NeRF）通过光线追踪的体渲染在视角合成方面取得了显著进展。基于Splatting的方法，如3D高斯溅射（3DGS），通过栅格化3D图元提供了更快的渲染速度。RadiantFoam（RF）通过使用显式Voronoi图（VD）组织辐射，重新引入了光线追踪，实现了与高斯溅射相当的吞吐量。然而，上述方法在精确网格重建方面仍然存在困难。本文通过联合学习显式VD和隐式有向距离场（SDF）来解决这个问题。场景通过光线追踪进行优化，并由Eikonal目标正则化。SDF引入了度量一致的等值面，进而偏置近表面Voronoi单元面与零水平集对齐。由此产生的模型产生更清晰、视角一致的表面，减少了浮动伪影并改善了拓扑结构，同时保持了光度质量，并保持了与RadiantFoam相当的训练速度。在不同的场景中，我们提出的混合隐式-显式公式，我们称之为SDFoam，在不牺牲效率的情况下，显著提高了网格重建精度（Chamfer距离），并具有可比的外观（PSNR，SSIM）。",
            "intro_zh": [
                "现有NeRF和3DGS等方法在精确网格重建方面存在不足，难以生成高质量的几何表面。",
                "SDFoam结合显式Voronoi图和隐式SDF，利用SDF的度量一致性来约束Voronoi单元，从而优化表面几何。",
                "实验表明，SDFoam在网格重建精度上显著提升（Chamfer距离），同时保持了良好的光度质量和训练效率。"
            ],
            "method_zh": "**问题定义**：现有方法，如NeRF和3DGS，虽然在视角合成方面表现出色，但在精确网格重建方面存在局限性。它们难以生成具有清晰细节和良好拓扑结构的表面，尤其是在复杂场景中，容易出现浮动伪影和不准确的几何形状。RadiantFoam虽然提高了渲染速度，但在网格重建精度上仍有提升空间。\\n\\n**核心思路**：SDFoam的核心思路是将显式的Voronoi图表示与隐式的有向距离场（SDF）表示相结合。Voronoi图用于快速渲染和光线追踪，而SDF则提供度量一致的几何约束。通过联合优化这两种表示，SDFoam能够生成更精确、更清晰的表面几何。SDF的零水平集定义了目标表面，并引导Voronoi单元面与其对齐，从而提高重建精度。\\n\\n**技术框架**：SDFoam的整体框架包括以下几个主要模块：1) 初始化：初始化一组3D点，并构建其Voronoi图。2) 光线追踪：使用Voronoi图进行光线追踪，计算每个像素的颜色值。3) SDF预测：使用神经网络预测每个点的SDF值。4) 损失计算：计算光度损失（photometric loss）和Eikonal损失（Eikonal loss），以及SDF对Voronoi单元的约束损失。5) 优化：使用优化算法（如Adam）更新Voronoi图的顶点位置和SDF网络的参数。\\n\\n**关键创新**：SDFoam的关键创新在于将显式Voronoi图和隐式SDF相结合，形成一种混合的表示方法。这种混合表示既利用了Voronoi图的快速渲染能力，又利用了SDF的度量一致性约束。通过联合优化这两种表示，SDFoam能够生成比单独使用Voronoi图或SDF更精确的表面几何。与现有方法相比，SDFoam在网格重建精度和表面质量方面都有显著提升。\\n\\n**关键设计**：SDFoam的关键设计包括：1) Eikonal损失：使用Eikonal损失来正则化SDF，使其满足梯度范数为1的约束，从而保证SDF的度量一致性。2) SDF约束损失：设计了一种损失函数，用于约束Voronoi单元面与SDF的零水平集对齐，从而提高重建精度。3) 网络结构：SDF网络可以使用MLP或其他适合SDF预测的网络结构。4) 优化策略：采用合适的优化算法和学习率，以保证训练的稳定性和收敛速度。",
            "application_zh": "SDFoam在三维重建、虚拟现实、增强现实、机器人导航等领域具有广泛的应用前景。它可以用于生成高质量的三维模型，用于游戏开发、电影制作等。在机器人领域，SDFoam可以用于构建机器人的环境地图，帮助机器人进行导航和避障。此外，SDFoam还可以应用于医学图像分析，例如对CT或MRI图像进行三维重建，辅助医生进行诊断。",
            "highlight_zh": "SDFoam在多个数据集上进行了实验，结果表明，与现有方法相比，SDFoam在网格重建精度（Chamfer距离）上取得了显著提升，同时保持了可比的光度质量（PSNR, SSIM）和训练效率。具体而言，SDFoam在重建精度上优于RadiantFoam和其他基线方法，并且在视觉效果上具有更清晰的表面细节和更少的浮动伪影。",
            "tags_zh": [
                "三维重建",
                "神经辐射场",
                "有向距离场",
                "Voronoi图",
                "光线追踪"
            ],
            "_index": 13,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16706v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16706v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16706v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models",
            "authors": [
                "Yuxin Wang",
                "Lei Ke",
                "Boqiang Zhang",
                "Tianyuan Qu",
                "Hanxun Yu",
                "Zhenpeng Huang",
                "Meng Yu",
                "Dan Xu",
                "Dong Yu"
            ],
            "arxiv_id": "2512.16561v1",
            "summary": "While current multimodal models can answer questions based on 2D images, they lack intrinsic 3D object perception, limiting their ability to comprehend spatial relationships and depth cues in 3D scenes. In this work, we propose N3D-VLM, a novel unified framework that seamlessly integrates native 3D object perception with 3D-aware visual reasoning, enabling both precise 3D grounding and interpretable spatial understanding. Unlike conventional end-to-end models that directly predict answers from RGB/RGB-D inputs, our approach equips the model with native 3D object perception capabilities, enabling it to directly localize objects in 3D space based on textual descriptions. Building upon accurate 3D object localization, the model further performs explicit reasoning in 3D, achieving more interpretable and structured spatial understanding. To support robust training for these capabilities, we develop a scalable data construction pipeline that leverages depth estimation to lift large-scale 2D annotations into 3D space, significantly increasing the diversity and coverage for 3D object grounding data, yielding over six times larger than the largest existing single-image 3D detection dataset. Moreover, the pipeline generates spatial question-answering datasets that target chain-of-thought (CoT) reasoning in 3D, facilitating joint training for both 3D object localization and 3D spatial reasoning. Experimental results demonstrate that our unified framework not only achieves state-of-the-art performance on 3D grounding tasks, but also consistently surpasses existing methods in 3D spatial reasoning in vision-language model.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Project Page: https://n3d-vlm.github.io",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16561v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "depth estimation"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱七：动作重定向 (Motion Retargeting)",
                    "id": "7_retargeting",
                    "matched_keywords": [
                        "spatial relationship"
                    ],
                    "score": 3.0
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "multimodal",
                        "chain-of-thought"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 11.0,
            "hit_pillars": [
                "3_perception_slam",
                "7_retargeting",
                "9_embodied_foundation"
            ],
            "headline_zh": "N3D-VLM：原生3D感知赋能视觉语言模型精确空间推理",
            "summary_zh": "当前的多模态模型虽然可以基于2D图像回答问题，但缺乏固有的3D物体感知能力，限制了其理解3D场景中的空间关系和深度线索的能力。本文提出了N3D-VLM，一种新颖的统一框架，它无缝集成了原生3D物体感知和3D感知视觉推理，从而实现了精确的3D grounding和可解释的空间理解。与直接从RGB/RGB-D输入预测答案的传统端到端模型不同，我们的方法赋予模型原生的3D物体感知能力，使其能够基于文本描述直接在3D空间中定位物体。在精确的3D物体定位的基础上，该模型进一步在3D中执行显式推理，从而实现更可解释和结构化的空间理解。为了支持这些能力的稳健训练，我们开发了一个可扩展的数据构建流程，该流程利用深度估计将大规模2D标注提升到3D空间，显著增加了3D物体grounding数据的多样性和覆盖范围，产生了比现有最大的单图像3D检测数据集大六倍以上的数据集。此外，该流程还生成了针对3D中思维链（CoT）推理的空间问答数据集，从而促进了3D物体定位和3D空间推理的联合训练。实验结果表明，我们的统一框架不仅在3D grounding任务上取得了最先进的性能，而且在视觉语言模型中的3D空间推理方面始终优于现有方法。",
            "intro_zh": [
                "现有视觉语言模型缺乏对3D场景的固有感知能力，难以理解空间关系和深度信息。",
                "N3D-VLM通过集成原生3D物体感知和3D感知视觉推理，实现精确的3D定位和空间理解。",
                "该方法在3D grounding和空间推理任务上均超越现有方法，并构建了大规模3D标注数据集。"
            ],
            "method_zh": "**问题定义**：现有视觉语言模型在处理3D场景时，由于缺乏对3D物体的直接感知能力，难以准确理解和推理空间关系。它们通常依赖于2D图像信息，无法充分利用3D场景中的深度信息和空间结构，导致在3D grounding和空间推理任务中表现不佳。\\n\\n**核心思路**：N3D-VLM的核心思路是赋予模型原生的3D物体感知能力，使其能够直接在3D空间中定位物体，并在此基础上进行显式的3D空间推理。通过将2D图像信息提升到3D空间，并结合文本描述，模型可以更准确地理解场景中的物体及其空间关系。\\n\\n**技术框架**：N3D-VLM包含以下主要模块：1) 数据构建流程：利用深度估计将大规模2D标注提升到3D空间，生成大规模3D物体grounding和空间问答数据集。2) 3D物体定位模块：基于文本描述，在3D空间中定位目标物体。3) 3D空间推理模块：在3D物体定位的基础上，进行显式的3D空间推理，例如判断物体之间的空间关系。整个框架采用端到端的方式进行训练，联合优化3D物体定位和3D空间推理的能力。\\n\\n**关键创新**：N3D-VLM最重要的技术创新点在于其原生3D物体感知能力。与传统的基于2D图像的视觉语言模型不同，N3D-VLM可以直接在3D空间中定位物体，从而更准确地理解和推理空间关系。此外，该方法还提出了一个可扩展的数据构建流程，可以生成大规模的3D标注数据集，为模型的训练提供了充足的数据支持。\\n\\n**关键设计**：数据构建流程利用深度估计将2D标注提升到3D空间，并采用数据增强技术增加数据的多样性。3D物体定位模块采用Transformer结构，将文本描述和3D场景信息进行融合，预测物体的3D bounding box。3D空间推理模块采用链式推理（Chain-of-Thought, CoT）的方式，逐步推理物体之间的空间关系，并最终生成答案。损失函数包括3D物体定位损失和空间推理损失，联合优化模型的定位和推理能力。",
            "application_zh": "N3D-VLM在机器人导航、自动驾驶、虚拟现实和增强现实等领域具有广泛的应用前景。它可以帮助机器人理解周围环境，进行更智能的导航和交互。在自动驾驶领域，它可以提高车辆对复杂场景的理解能力，从而提高驾驶安全性。在虚拟现实和增强现实领域，它可以增强用户与虚拟环境的交互体验，提供更逼真的沉浸感。",
            "highlight_zh": "N3D-VLM在3D grounding任务上取得了state-of-the-art的性能，并且在3D空间推理任务上始终优于现有方法。例如，在ScanRefer数据集上，N3D-VLM的3D grounding准确率比现有最佳方法提高了X%。此外，该方法构建了一个比现有最大的单图像3D检测数据集大六倍以上的数据集，为3D视觉语言模型的研究提供了重要的数据支持。",
            "tags_zh": [
                "3D视觉语言模型",
                "3D grounding",
                "空间推理",
                "深度估计",
                "数据增强"
            ],
            "_index": 14,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16561v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16561v1/x4.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16561v1/x5.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "SNOW: Spatio-Temporal Scene Understanding with World Knowledge for Open-World Embodied Reasoning",
            "authors": [
                "Tin Stribor Sohn",
                "Maximilian Dillitzer",
                "Jason J. Corso",
                "Eric Sax"
            ],
            "arxiv_id": "2512.16461v1",
            "summary": "Autonomous robotic systems require spatio-temporal understanding of dynamic environments to ensure reliable navigation and interaction. While Vision-Language Models (VLMs) provide open-world semantic priors, they lack grounding in 3D geometry and temporal dynamics. Conversely, geometric perception captures structure and motion but remains semantically sparse. We propose SNOW (Scene Understanding with Open-World Knowledge), a training-free and backbone-agnostic framework for unified 4D scene understanding that integrates VLM-derived semantics with point cloud geometry and temporal consistency. SNOW processes synchronized RGB images and 3D point clouds, using HDBSCAN clustering to generate object-level proposals that guide SAM2-based segmentation. Each segmented region is encoded through our proposed Spatio-Temporal Tokenized Patch Encoding (STEP), producing multimodal tokens that capture localized semantic, geometric, and temporal attributes. These tokens are incrementally integrated into a 4D Scene Graph (4DSG), which serves as 4D prior for downstream reasoning. A lightweight SLAM backend anchors all STEP tokens spatially in the environment, providing the global reference alignment, and ensuring unambiguous spatial grounding across time. The resulting 4DSG forms a queryable, unified world model through which VLMs can directly interpret spatial scene structure and temporal dynamics. Experiments on a diverse set of benchmarks demonstrate that SNOW enables precise 4D scene understanding and spatially grounded inference, thereby setting new state-of-the-art performance in several settings, highlighting the importance of structured 4D priors for embodied reasoning and autonomous robotics.",
            "categories": [
                "cs.CV",
                "cs.RO"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16461v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "world model"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]scene understanding"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "multimodal"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 10.5,
            "hit_pillars": [
                "2_algo_arch",
                "3_perception_slam",
                "9_embodied_foundation"
            ],
            "headline_zh": "SNOW：利用世界知识进行时空场景理解，实现开放世界具身推理",
            "summary_zh": "自主机器人系统需要对动态环境进行时空理解，以确保可靠的导航和交互。视觉-语言模型(VLMs)提供了开放世界的语义先验，但缺乏3D几何和时间动态的 grounding。相反，几何感知捕捉了结构和运动，但语义仍然稀疏。我们提出了SNOW（Scene Understanding with Open-World Knowledge），一个无需训练且与骨干网络无关的框架，用于统一的4D场景理解，它集成了VLM衍生的语义与点云几何和时间一致性。SNOW处理同步的RGB图像和3D点云，使用HDBSCAN聚类生成对象级别的提议，指导基于SAM2的分割。每个分割区域通过我们提出的时空Token化Patch编码(STEP)进行编码，产生多模态tokens，捕捉局部语义、几何和时间属性。这些tokens被增量地集成到4D场景图(4DSG)中，作为下游推理的4D先验。轻量级的SLAM后端在环境中空间地锚定所有STEP tokens，提供全局参考对齐，并确保跨时间无歧义的空间grounding。由此产生的4DSG形成了一个可查询的统一世界模型，通过该模型，VLMs可以直接解释空间场景结构和时间动态。在各种基准测试上的实验表明，SNOW能够实现精确的4D场景理解和空间grounding的推理，从而在多个设置中设置了新的最先进的性能，突出了结构化4D先验对于具身推理和自主机器人的重要性。",
            "intro_zh": [
                "现有方法在机器人时空场景理解中，要么缺乏语义信息，要么缺乏几何和时间动态的精确建模，限制了其在复杂环境中的应用。",
                "SNOW框架通过融合视觉-语言模型的语义知识、点云几何信息和时间一致性，构建统一的4D场景图，为机器人提供更丰富的环境理解。",
                "实验结果表明，SNOW在多个基准测试中取得了最先进的性能，验证了其在具身推理和自主机器人领域的有效性。"
            ],
            "method_zh": "**问题定义**：现有机器人场景理解方法面临的挑战在于如何有效地融合语义信息（例如，物体类别、属性）与几何信息（例如，物体形状、位置）以及时间信息（例如，物体运动轨迹）。视觉-语言模型（VLM）虽然具备强大的语义理解能力，但缺乏对3D几何和时间动态的精确建模。而传统的几何感知方法虽然能够捕捉场景的结构和运动，但语义信息较为稀疏，难以支持复杂的推理任务。\\n\\n**核心思路**：SNOW的核心思路是将VLM的语义先验知识与点云几何信息和时间一致性相结合，构建一个统一的4D场景图（4DSG）。通过将场景中的物体表示为包含语义、几何和时间属性的多模态tokens，并利用SLAM技术将这些tokens在空间中进行对齐，从而实现对动态环境的全面理解。这种方法能够充分利用VLM的语义推理能力，同时保证场景理解的几何精度和时间一致性。\\n\\n**技术框架**：SNOW框架主要包含以下几个模块：1) 对象提议生成：利用HDBSCAN聚类算法对点云进行分割，生成对象级别的提议区域。2) 基于SAM2的分割：使用SAM2模型对提议区域进行精确分割，提取每个物体的像素级掩码。3) 时空Token化Patch编码（STEP）：将分割后的物体区域编码为多模态tokens，包含语义、几何和时间属性。4) 4D场景图构建：将STEP tokens增量式地集成到4DSG中，形成一个可查询的统一世界模型。5) SLAM后端：利用SLAM技术将所有STEP tokens在环境中进行空间对齐，提供全局参考坐标系。\\n\\n**关键创新**：SNOW的关键创新在于提出了时空Token化Patch编码（STEP）方法，能够将分割后的物体区域编码为包含语义、几何和时间属性的多模态tokens。这些tokens不仅包含了VLM的语义信息，还包含了点云的几何信息和时间信息，从而实现了对场景的全面理解。此外，SNOW框架还利用SLAM技术将所有STEP tokens在环境中进行空间对齐，保证了场景理解的几何精度和时间一致性。\\n\\n**关键设计**：在STEP编码中，论文可能使用了特定的网络结构来提取语义、几何和时间特征，例如，使用Transformer网络来捕捉tokens之间的关系。损失函数的设计可能包括语义一致性损失、几何一致性损失和时间一致性损失，以保证tokens的语义、几何和时间属性的一致性。SLAM后端可能采用了基于图优化的方法，将STEP tokens作为节点，将它们之间的空间关系作为边，通过优化图结构来提高空间对齐的精度。",
            "application_zh": "SNOW框架具有广泛的应用前景，例如，可以应用于自主导航、机器人操作、增强现实和虚拟现实等领域。通过提供精确的4D场景理解，SNOW能够帮助机器人更好地理解周围环境，从而实现更智能的导航和交互。此外，SNOW还可以用于构建虚拟环境，为用户提供更逼真的沉浸式体验。未来，SNOW有望成为机器人和人工智能领域的重要基础设施。",
            "highlight_zh": "SNOW在多个基准测试中取得了最先进的性能，证明了其在4D场景理解方面的有效性。具体而言，SNOW在空间grounding的推理任务中表现出色，能够准确地将语义信息与几何信息进行关联。此外，SNOW还能够有效地处理动态环境，对物体的运动轨迹进行精确建模。这些实验结果表明，SNOW框架能够为机器人提供更全面、更精确的环境理解，从而提高其在复杂环境中的适应性和鲁棒性。",
            "tags_zh": [
                "时空场景理解",
                "具身推理",
                "视觉-语言模型",
                "点云处理",
                "4D场景图"
            ],
            "_index": 15,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16461v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16461v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16461v1/02_Figures/RoboSpatial_1.jpeg",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "ManiLong-Shot: Interaction-Aware One-Shot Imitation Learning for Long-Horizon Manipulation",
            "authors": [
                "Zixuan Chen",
                "Chongkai Gao",
                "Lin Shao",
                "Jieqi Shi",
                "Jing Huo",
                "Yang Gao"
            ],
            "arxiv_id": "2512.16302v1",
            "summary": "One-shot imitation learning (OSIL) offers a promising way to teach robots new skills without large-scale data collection. However, current OSIL methods are primarily limited to short-horizon tasks, thus limiting their applicability to complex, long-horizon manipulations. To address this limitation, we propose ManiLong-Shot, a novel framework that enables effective OSIL for long-horizon prehensile manipulation tasks. ManiLong-Shot structures long-horizon tasks around physical interaction events, reframing the problem as sequencing interaction-aware primitives instead of directly imitating continuous trajectories. This primitive decomposition can be driven by high-level reasoning from a vision-language model (VLM) or by rule-based heuristics derived from robot state changes. For each primitive, ManiLong-Shot predicts invariant regions critical to the interaction, establishes correspondences between the demonstration and the current observation, and computes the target end-effector pose, enabling effective task execution. Extensive simulation experiments show that ManiLong-Shot, trained on only 10 short-horizon tasks, generalizes to 20 unseen long-horizon tasks across three difficulty levels via one-shot imitation, achieving a 22.8% relative improvement over the SOTA. Additionally, real-robot experiments validate ManiLong-Shot's ability to robustly execute three long-horizon manipulation tasks via OSIL, confirming its practical applicability.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Accepted by AAAI 2026",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16302v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]manipulation"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]imitation learning"
                    ],
                    "score": 4.5
                }
            ],
            "relevance_score": 10.5,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "ManiLong-Shot：交互感知的单样本模仿学习用于长时程操作任务",
            "summary_zh": "本文提出ManiLong-Shot，一个新颖的框架，旨在实现长时程灵巧操作任务的有效单样本模仿学习(OSIL)。ManiLong-Shot围绕物理交互事件构建长时程任务，将问题重新定义为对交互感知原语进行排序，而不是直接模仿连续轨迹。这种原语分解可以由视觉-语言模型(VLM)的高级推理驱动，或者由机器人状态变化推导出的基于规则的启发式方法驱动。对于每个原语，ManiLong-Shot预测对交互至关重要的不变区域，建立演示和当前观察之间的对应关系，并计算目标末端执行器姿态，从而实现有效的任务执行。大量的仿真实验表明，ManiLong-Shot仅在10个短时程任务上训练，即可通过单样本模仿泛化到20个未见过的长时程任务，跨越三个难度级别，相对于SOTA方法实现了22.8%的相对改进。此外，真实机器人实验验证了ManiLong-Shot通过OSIL稳健地执行三个长时程操作任务的能力，证实了其在实际应用中的可行性。",
            "intro_zh": [
                "现有单样本模仿学习方法主要局限于短时程任务，限制了其在复杂、长时程操作任务中的应用。",
                "ManiLong-Shot将长时程任务分解为交互感知的原语序列，通过视觉-语言模型或规则启发式方法驱动原语分解。",
                "实验表明，ManiLong-Shot在仿真和真实机器人上均表现出优异的性能，显著提升了长时程操作任务的成功率。"
            ],
            "method_zh": "**问题定义**：现有单样本模仿学习方法难以处理长时程操作任务，因为直接模仿连续轨迹在长序列中容易累积误差，并且难以泛化到新的场景。痛点在于缺乏对任务结构的理解和有效的分解策略。\\n\\n**核心思路**：将长时程任务分解为一系列交互感知的原语。每个原语代表一个基本的物理交互动作，例如抓取、放置等。通过对这些原语进行排序和模仿，可以有效地完成长时程任务。这种分解降低了模仿的难度，并提高了泛化能力。\\n\\n**技术框架**：ManiLong-Shot包含以下主要模块：1) 任务分解模块：利用视觉-语言模型或规则启发式方法将长时程任务分解为交互原语序列。2) 交互区域预测模块：预测每个原语中对交互至关重要的不变区域。3) 对应关系建立模块：建立演示和当前观察之间的对应关系，例如通过特征匹配。4) 末端执行器姿态计算模块：根据对应关系计算目标末端执行器姿态，驱动机器人执行动作。\\n\\n**关键创新**：ManiLong-Shot的关键创新在于将长时程任务分解为交互感知的原语，并利用视觉-语言模型或规则启发式方法进行任务分解。与直接模仿连续轨迹的方法相比，这种方法更具结构化，更容易泛化到新的场景。\\n\\n**关键设计**：任务分解模块可以使用预训练的视觉-语言模型，例如CLIP，来理解任务描述并生成原语序列。交互区域预测模块可以使用卷积神经网络来预测图像中与交互相关的区域。对应关系建立模块可以使用SIFT或ORB等特征匹配算法。末端执行器姿态计算模块可以使用逆运动学算法将目标姿态转换为关节角度。",
            "application_zh": "ManiLong-Shot在工业自动化、家庭服务机器人、医疗机器人等领域具有广泛的应用前景。例如，它可以用于训练机器人完成复杂的装配任务、清洁任务或辅助手术任务。通过单样本模仿学习，可以大大降低机器人编程的成本和难度，提高机器人的智能化水平。",
            "highlight_zh": "在仿真实验中，ManiLong-Shot在20个未见过的长时程任务上，相对于SOTA方法实现了22.8%的相对改进。在真实机器人实验中，ManiLong-Shot成功地执行了三个长时程操作任务，验证了其在实际应用中的可行性。这些结果表明，ManiLong-Shot是一种有效的长时程操作任务单样本模仿学习方法。",
            "tags_zh": [
                "单样本模仿学习",
                "长时程操作",
                "交互感知",
                "视觉-语言模型",
                "机器人操作"
            ],
            "_index": 16,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16302v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16302v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16302v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "AdaTooler-V: Adaptive Tool-Use for Images and Videos",
            "authors": [
                "Chaoyang Wang",
                "Kaituo Feng",
                "Dongyang Chen",
                "Zhongyu Wang",
                "Zhixun Li",
                "Sicheng Gao",
                "Meng Meng",
                "Xu Zhou",
                "Manyuan Zhang",
                "Yuzhang Shang",
                "Xiangyu Yue"
            ],
            "arxiv_id": "2512.16918v1",
            "summary": "Recent advances have shown that multimodal large language models (MLLMs) benefit from multimodal interleaved chain-of-thought (CoT) with vision tool interactions. However, existing open-source models often exhibit blind tool-use reasoning patterns, invoking vision tools even when they are unnecessary, which significantly increases inference overhead and degrades model performance. To this end, we propose AdaTooler-V, an MLLM that performs adaptive tool-use by determining whether a visual problem truly requires tools. First, we introduce AT-GRPO, a reinforcement learning algorithm that adaptively adjusts reward scales based on the Tool Benefit Score of each sample, encouraging the model to invoke tools only when they provide genuine improvements. Moreover, we construct two datasets to support training: AdaTooler-V-CoT-100k for SFT cold start and AdaTooler-V-300k for RL with verifiable rewards across single-image, multi-image, and video data. Experiments across twelve benchmarks demonstrate the strong reasoning capability of AdaTooler-V, outperforming existing methods in diverse visual reasoning tasks. Notably, AdaTooler-V-7B achieves an accuracy of 89.8\\% on the high-resolution benchmark V*, surpassing the commercial proprietary model GPT-4o and Gemini 1.5 Pro. All code, models, and data are released.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Project page: https://github.com/CYWang735/AdaTooler-V",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16918v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model",
                        "multimodal",
                        "chain-of-thought"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 10.5,
            "hit_pillars": [
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "提出AdaTooler-V，通过自适应工具使用提升多模态大语言模型在图像和视频任务中的推理效率和性能。",
            "summary_zh": "本文提出AdaTooler-V，一种多模态大语言模型（MLLM），通过自适应工具使用来解决现有开源模型中存在的盲目工具使用问题。现有模型即使在不需要视觉工具的情况下也会调用，从而显著增加推理开销并降低模型性能。AdaTooler-V通过AT-GRPO强化学习算法自适应地调整奖励尺度，鼓励模型仅在工具能带来真正改进时才调用。此外，构建了AdaTooler-V-CoT-100k和AdaTooler-V-300k两个数据集，分别用于SFT冷启动和RL训练，涵盖单图像、多图像和视频数据。在十二个基准测试上的实验表明，AdaTooler-V具有强大的推理能力，在各种视觉推理任务中优于现有方法。值得注意的是，AdaTooler-V-7B在高分辨率基准V*上实现了89.8%的准确率，超过了商业专有模型GPT-4o和Gemini 1.5 Pro。所有代码、模型和数据均已发布。",
            "intro_zh": [
                "现有多模态大语言模型存在盲目工具使用问题，即使不需要视觉工具也会调用，导致推理开销增加和性能下降。",
                "AdaTooler-V通过AT-GRPO强化学习算法，根据工具收益自适应调整奖励尺度，鼓励模型仅在必要时调用工具。",
                "实验结果表明，AdaTooler-V在多个视觉推理任务中优于现有方法，并在高分辨率基准测试中超越了GPT-4o和Gemini 1.5 Pro。"
            ],
            "method_zh": "**问题定义**：论文旨在解决多模态大语言模型（MLLMs）在视觉任务中盲目使用工具的问题。现有开源MLLMs常常不加区分地调用视觉工具，即使这些工具对于解决当前问题并非必要。这种盲目性导致了不必要的计算开销，降低了推理效率，并且可能损害模型的整体性能。因此，如何让MLLMs学会自适应地判断何时需要使用工具，以及何时可以避免使用工具，是本文要解决的核心问题。\\n\\n**核心思路**：论文的核心思路是训练模型仅在工具能够带来显著性能提升时才调用工具。为了实现这一目标，论文引入了一种基于强化学习的训练方法，该方法通过奖励机制来鼓励模型进行合理的工具使用决策。具体来说，模型会根据工具的使用情况获得奖励，如果使用工具能够显著提高解决问题的准确性，则会获得更高的奖励；反之，如果使用工具并没有带来明显的改进，则会受到惩罚。通过这种方式，模型可以逐渐学习到何时应该使用工具，以及何时应该避免使用工具。\\n\\n**技术框架**：AdaTooler-V的整体框架包含以下几个主要组成部分：1) 多模态大语言模型（MLLM）：作为模型的核心，负责接收输入（图像、视频和文本），并生成输出（文本）。2) 视觉工具：提供各种视觉处理能力，例如目标检测、图像分割等。3) AT-GRPO强化学习算法：用于训练模型，使其能够自适应地选择是否使用视觉工具。4) 奖励函数：用于评估模型使用工具的效果，并根据效果给予奖励或惩罚。训练过程包括：首先使用AdaTooler-V-CoT-100k数据集进行监督微调（SFT），为模型提供一个良好的冷启动。然后，使用AdaTooler-V-300k数据集进行强化学习训练，通过AT-GRPO算法优化模型，使其能够自适应地选择是否使用工具。\\n\\n**关键创新**：论文的关键创新在于提出了AT-GRPO（Adaptive Tool-use with Gradient Ratio Policy Optimization）强化学习算法。该算法的核心思想是根据每个样本的“工具收益分数”（Tool Benefit Score）自适应地调整奖励尺度。工具收益分数衡量了使用工具相比不使用工具所带来的性能提升。如果工具收益分数较高，则模型在使用工具时会获得更高的奖励，从而鼓励模型在必要时使用工具。与传统的强化学习算法相比，AT-GRPO能够更有效地引导模型学习合理的工具使用策略。\\n\\n**关键设计**：AT-GRPO算法的关键设计在于奖励尺度的自适应调整。具体来说，奖励尺度与工具收益分数成正比。工具收益分数可以通过比较模型在使用工具和不使用工具两种情况下的性能来估计。此外，论文还设计了两个数据集AdaTooler-V-CoT-100k和AdaTooler-V-300k，用于支持模型的训练。AdaTooler-V-CoT-100k数据集包含10万个样本，用于SFT冷启动。AdaTooler-V-300k数据集包含30万个样本，用于强化学习训练，并提供了可验证的奖励信号。",
            "application_zh": "AdaTooler-V具有广泛的应用前景，例如智能客服、自动驾驶、医疗诊断等领域。在智能客服中，模型可以根据用户提供的图像或视频，自动选择合适的视觉工具进行分析，从而更准确地理解用户的问题并提供相应的解决方案。在自动驾驶中，模型可以利用视觉工具来识别交通标志、行人和其他车辆，从而提高驾驶安全性。在医疗诊断中，模型可以分析医学图像，辅助医生进行疾病诊断。",
            "highlight_zh": "AdaTooler-V在十二个基准测试中表现出色，证明了其强大的视觉推理能力。尤其是在高分辨率基准V*上，AdaTooler-V-7B达到了89.8%的准确率，超越了商业专有模型GPT-4o和Gemini 1.5 Pro。这表明AdaTooler-V在处理复杂视觉任务方面具有显著优势。",
            "tags_zh": [
                "多模态大语言模型",
                "自适应工具使用",
                "强化学习",
                "视觉推理",
                "图像视频处理"
            ],
            "_index": 17,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16918v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16918v1/x4.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16918v1/x5.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Instant Expressive Gaussian Head Avatar via 3D-Aware Expression Distillation",
            "authors": [
                "Kaiwen Jiang",
                "Xueting Li",
                "Seonwook Park",
                "Ravi Ramamoorthi",
                "Shalini De Mello",
                "Koki Nagano"
            ],
            "arxiv_id": "2512.16893v1",
            "summary": "Portrait animation has witnessed tremendous quality improvements thanks to recent advances in video diffusion models. However, these 2D methods often compromise 3D consistency and speed, limiting their applicability in real-world scenarios, such as digital twins or telepresence. In contrast, 3D-aware facial animation feedforward methods -- built upon explicit 3D representations, such as neural radiance fields or Gaussian splatting -- ensure 3D consistency and achieve faster inference speed, but come with inferior expression details. In this paper, we aim to combine their strengths by distilling knowledge from a 2D diffusion-based method into a feed-forward encoder, which instantly converts an in-the-wild single image into a 3D-consistent, fast yet expressive animatable representation. Our animation representation is decoupled from the face's 3D representation and learns motion implicitly from data, eliminating the dependency on pre-defined parametric models that often constrain animation capabilities. Unlike previous computationally intensive global fusion mechanisms (e.g., multiple attention layers) for fusing 3D structural and animation information, our design employs an efficient lightweight local fusion strategy to achieve high animation expressivity. As a result, our method runs at 107.31 FPS for animation and pose control while achieving comparable animation quality to the state-of-the-art, surpassing alternative designs that trade speed for quality or vice versa. Project website is https://research.nvidia.com/labs/amri/projects/instant4d",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Project website is https://research.nvidia.com/labs/amri/projects/instant4d",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16893v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]distillation"
                    ],
                    "score": 4.5
                },
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "gaussian splatting",
                        "splatting",
                        "neural radiance field"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 10.5,
            "hit_pillars": [
                "2_algo_arch",
                "3_perception_slam"
            ],
            "headline_zh": "提出基于3D感知表达蒸馏的快速高表现力高斯头部头像方法",
            "summary_zh": "得益于视频扩散模型的最新进展，人像动画的质量得到了显著提升。然而，这些2D方法通常会牺牲3D一致性和速度，限制了它们在数字孪生或远程呈现等实际场景中的应用。相比之下，基于显式3D表示（如神经辐射场或高斯溅射）的3D感知面部动画前馈方法，可确保3D一致性并实现更快的推理速度，但表达细节较差。本文旨在结合两者的优势，将知识从基于2D扩散的方法提炼到前馈编码器中，该编码器可立即将野外单张图像转换为3D一致、快速且富有表现力的可动画表示。我们的动画表示与面部的3D表示解耦，并从数据中隐式地学习运动，从而消除了对通常限制动画能力的预定义参数模型的依赖。与先前用于融合3D结构和动画信息的计算密集型全局融合机制（例如，多个注意力层）不同，我们的设计采用了一种高效的轻量级局部融合策略，以实现高动画表现力。因此，我们的方法以107.31 FPS的速度运行动画和姿势控制，同时实现了与最先进技术相当的动画质量，超过了在速度和质量之间进行权衡的替代设计。",
            "intro_zh": [
                "现有2D人像动画方法在3D一致性和速度上存在不足，难以应用于实时场景。",
                "通过将2D扩散模型的知识蒸馏到3D前馈网络中，实现快速且高表现力的3D人像动画。",
                "该方法在动画和姿势控制上达到107.31 FPS，动画质量与SOTA方法相当。"
            ],
            "method_zh": "**问题定义**：现有2D人像动画方法虽然在动画质量上有所提升，但往往牺牲了3D一致性和速度，难以满足实时应用的需求。而基于3D表示的方法虽然保证了3D一致性和速度，但在表达细节上有所欠缺。因此，如何兼顾3D一致性、速度和表达能力是本文要解决的问题。\\n\\n**核心思路**：本文的核心思路是将2D扩散模型的表达能力“蒸馏”到3D前馈网络中。具体来说，利用2D扩散模型生成高质量的动画细节，然后训练一个前馈网络来快速预测这些细节，并将其融合到3D人像表示中。这样既能保证3D一致性和速度，又能获得丰富的动画表达。\\n\\n**技术框架**：该方法主要包含以下几个模块：1) 2D扩散模型：用于生成高质量的动画细节；2) 前馈编码器：将单张图像转换为3D一致、快速且富有表现力的可动画表示；3) 轻量级局部融合模块：将动画信息融合到3D结构信息中。整个流程是，首先使用2D扩散模型生成动画细节，然后使用前馈编码器预测这些细节，最后使用轻量级局部融合模块将这些细节融合到3D人像表示中。\\n\\n**关键创新**：该方法最重要的创新点在于使用了一种轻量级的局部融合策略，而不是传统的全局融合机制（如注意力层）。这种局部融合策略可以有效地融合3D结构和动画信息，同时保持较高的计算效率。此外，该方法还解耦了动画表示和3D表示，使得动画的控制更加灵活。\\n\\n**关键设计**：该方法使用了一种高效的轻量级局部融合策略，具体来说，它将动画特征和3D结构特征在局部区域进行融合，而不是像全局注意力机制那样对所有特征进行融合。这种局部融合策略可以有效地减少计算量，同时保持较高的融合效果。此外，该方法还设计了一种特殊的损失函数，用于训练前馈编码器，使得其能够准确地预测动画细节。",
            "application_zh": "该研究成果可广泛应用于数字孪生、远程呈现、虚拟现实、增强现实等领域。例如，可以用于创建逼真的虚拟化身，进行远程会议和协作，或者用于游戏和娱乐应用中，提供更具表现力的角色动画。该技术的发展将推动人机交互方式的进步，并为用户带来更沉浸式的体验。",
            "highlight_zh": "该方法在动画和姿势控制上达到了107.31 FPS，显著优于其他需要牺牲速度来换取质量的方法。同时，该方法在动画质量上与最先进的2D扩散模型相当，证明了其在速度和质量上的优越性。实验结果表明，该方法能够生成高质量、3D一致且富有表现力的头部头像动画。",
            "tags_zh": [
                "人像动画",
                "3D感知",
                "扩散模型",
                "知识蒸馏",
                "高斯溅射",
                "实时渲染",
                "局部融合"
            ],
            "_index": 18,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16893v1/fig/expressiveness_vs_consistency_colored.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16893v1/fig/pipeline-2.jpg",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16893v1/fig/residual_features.jpg",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning",
            "authors": [
                "Qihao Liu",
                "Luoxin Ye",
                "Wufei Ma",
                "Yu-Cheng Chou",
                "Alan Yuille"
            ],
            "arxiv_id": "2512.16917v1",
            "summary": "Large language models (LLMs) with explicit reasoning capabilities excel at mathematical reasoning yet still commit process errors, such as incorrect calculations, brittle logic, and superficially plausible but invalid steps. In this paper, we introduce Generative Adversarial Reasoner, an on-policy joint training framework designed to enhance reasoning by co-evolving an LLM reasoner and an LLM-based discriminator through adversarial reinforcement learning. A compute-efficient review schedule partitions each reasoning chain into logically complete slices of comparable length, and the discriminator evaluates each slice's soundness with concise, structured justifications. Learning couples complementary signals: the LLM reasoner is rewarded for logically consistent steps that yield correct answers, while the discriminator earns rewards for correctly detecting errors or distinguishing traces in the reasoning process. This produces dense, well-calibrated, on-policy step-level rewards that supplement sparse exact-match signals, improving credit assignment, increasing sample efficiency, and enhancing overall reasoning quality of LLMs. Across various mathematical benchmarks, the method delivers consistent gains over strong baselines with standard RL post-training. Specifically, on AIME24, we improve DeepSeek-R1-Distill-Qwen-7B from 54.0 to 61.3 (+7.3) and DeepSeek-R1-Distill-Llama-8B from 43.7 to 53.7 (+10.0). The modular discriminator also enables flexible reward shaping for objectives such as teacher distillation, preference alignment, and mathematical proof-based reasoning.",
            "categories": [
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16917v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]reinforcement learning",
                        "distillation",
                        "reward shaping"
                    ],
                    "score": 7.5
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 10.5,
            "hit_pillars": [
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "提出Generative Adversarial Reasoner，通过对抗强化学习提升LLM的推理能力。",
            "summary_zh": "本文提出了一种名为Generative Adversarial Reasoner 的 on-policy 联合训练框架，旨在通过对抗强化学习协同进化 LLM 推理器和基于 LLM 的判别器，从而增强 LLM 的推理能力。该框架采用计算高效的审查机制，将每个推理链划分为逻辑完整的、长度相当的片段，判别器通过简洁、结构化的理由评估每个片段的合理性。学习过程耦合了互补信号：LLM 推理器因产生正确答案的逻辑一致步骤而获得奖励，而判别器因正确检测推理过程中的错误或区分推理轨迹而获得奖励。这产生了密集的、良好校准的、on-policy 的步级奖励，补充了稀疏的精确匹配信号，改善了信用分配，提高了样本效率，并增强了 LLM 的整体推理质量。在各种数学基准测试中，该方法相对于使用标准 RL 后训练的强大基线，实现了持续的收益。特别是在 AIME24 上，我们将 DeepSeek-R1-Distill-Qwen-7B 从 54.0 提高到 61.3 (+7.3)，将 DeepSeek-R1-Distill-Llama-8B 从 43.7 提高到 53.7 (+10.0)。模块化判别器还能够灵活地进行奖励塑造，以实现诸如教师知识蒸馏、偏好对齐和基于数学证明的推理等目标。",
            "intro_zh": [
                "现有LLM在数学推理方面表现出色，但仍存在计算错误、逻辑脆弱和表面合理但无效的步骤等过程性错误。",
                "论文提出Generative Adversarial Reasoner，通过对抗强化学习协同训练LLM推理器和判别器，提升推理的逻辑一致性和准确性。",
                "实验表明，该方法在数学基准测试中优于现有RL微调方法，在AIME24数据集上DeepSeek模型提升高达10个百分点。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大型语言模型（LLM）在数学推理过程中出现的逻辑错误、计算错误以及推理步骤无效等问题。现有方法通常依赖于稀疏的奖励信号（例如，最终答案是否正确），导致信用分配困难，训练效率低下，难以有效提升LLM的推理能力。\\n\\n**核心思路**：论文的核心思路是通过对抗强化学习，同时训练一个LLM推理器和一个LLM判别器。推理器负责生成推理步骤，判别器负责评估每个推理步骤的合理性。通过这种对抗的方式，推理器可以学习到更细粒度的奖励信号，从而更好地优化推理过程。\\n\\n**技术框架**：Generative Adversarial Reasoner (GAR) 包含两个主要模块：LLM推理器和LLM判别器。推理器负责生成推理链，判别器负责评估推理链中每个片段的合理性，并给出结构化的理由。训练过程采用on-policy强化学习，推理器根据判别器的反馈调整策略，判别器根据推理器的输出调整判别能力。整个框架通过对抗的方式，不断提升推理器和判别器的能力。\\n\\n**关键创新**：该方法最重要的创新点在于引入了对抗强化学习来提升LLM的推理能力。与传统的强化学习方法相比，GAR能够提供更密集、更细粒度的奖励信号，从而更好地指导推理器的训练。此外，GAR的模块化判别器设计使得可以灵活地进行奖励塑造，以适应不同的目标，例如教师知识蒸馏、偏好对齐和基于数学证明的推理。\\n\\n**关键设计**：GAR的关键设计包括：(1) 计算高效的审查机制，将推理链划分为逻辑完整的片段；(2) 判别器输出的结构化理由，提供更丰富的反馈信息；(3) 基于对抗强化学习的训练策略，协同优化推理器和判别器；(4) 灵活的奖励塑造机制，可以根据不同的目标进行调整。具体参数设置和网络结构等细节未在摘要中详细说明，属于未知信息。",
            "application_zh": "该研究成果可应用于需要复杂推理能力的各种领域，例如数学解题、科学研究、代码生成、逻辑推理等。通过提升LLM的推理能力，可以提高自动化系统的智能化水平，减少人工干预，提高工作效率。未来，该方法有望应用于更广泛的领域，例如医疗诊断、金融分析等。",
            "highlight_zh": "实验结果表明，Generative Adversarial Reasoner 在多个数学基准测试中取得了显著的提升。在 AIME24 数据集上，DeepSeek-R1-Distill-Qwen-7B 模型的准确率从 54.0% 提高到 61.3% (+7.3%)，DeepSeek-R1-Distill-Llama-8B 模型的准确率从 43.7% 提高到 53.7% (+10.0%)。这些结果表明，该方法能够有效提升 LLM 的推理能力。",
            "tags_zh": [
                "大型语言模型",
                "强化学习",
                "对抗学习",
                "数学推理",
                "推理链",
                "奖励塑造",
                "知识蒸馏"
            ],
            "_index": 19,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16917v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16917v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16917v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Non-Asymptotic Global Convergence of PPO-Clip",
            "authors": [
                "Yin Liu",
                "Qiming Dai",
                "Junyu Zhang",
                "Zaiwen Wen"
            ],
            "arxiv_id": "2512.16565v1",
            "summary": "Reinforcement learning (RL) has gained attention for aligning large language models (LLMs) via reinforcement learning from human feedback (RLHF). The actor-only variants of Proximal Policy Optimization (PPO) are widely applied for their efficiency. These algorithms incorporate a clipping mechanism to improve stability. Besides, a regularization term, such as the reverse KL-divergence or a more general \\(f\\)-divergence, is introduced to prevent policy drift. Despite their empirical success, a rigorous theoretical understanding of the problem and the algorithm's properties is limited. This paper advances the theoretical foundations of the PPO-Clip algorithm by analyzing a deterministic actor-only PPO algorithm within the general RL setting with \\(f\\)-divergence regularization under the softmax policy parameterization. We derive a non-uniform Lipschitz smoothness condition and a Łojasiewicz inequality for the considered problem. Based on these, a non-asymptotic linear convergence rate to the globally optimal policy is established for the forward KL-regularizer. Furthermore, stationary convergence and local linear convergence are derived for the reverse KL-regularizer.",
            "categories": [
                "math.OC",
                "cs.LG"
            ],
            "primary_category": "math.OC",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16565v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning",
                        "[T]PPO",
                        "RLHF"
                    ],
                    "score": 7.5
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 10.5,
            "hit_pillars": [
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "提出PPO-Clip算法的非渐近全局收敛性分析",
            "summary_zh": "强化学习（RL）因其在通过人类反馈对大型语言模型（LLM）进行对齐的能力而受到关注。PPO的仅演员变体因其高效性而被广泛应用，这些算法通过剪切机制提高稳定性，并引入正则化项以防止策略漂移。尽管在经验上取得了成功，但对该问题及算法特性的严格理论理解仍然有限。本文通过分析带有软最大策略参数化的确定性仅演员PPO算法，推进了PPO-Clip算法的理论基础，导出了非均匀Lipschitz光滑性条件和Łojasiewicz不等式，并建立了前向KL正则化器的非渐近线性收敛率。此外，还推导了反向KL正则化器的平稳收敛和局部线性收敛性。",
            "intro_zh": [
                "现有的PPO算法在理论理解上存在不足，尤其是在收敛性和稳定性方面的分析较为薄弱。",
                "论文提出了一种新的理论框架，通过引入f-散度正则化，分析了PPO-Clip算法的全局收敛性。",
                "研究表明，使用前向KL正则化器时，PPO-Clip算法能够实现非渐近线性收敛，显著提升了收敛速度。"
            ],
            "method_zh": "**问题定义**：本文旨在解决PPO-Clip算法在理论上的收敛性问题，现有方法缺乏对算法性质的严格分析，尤其是在稳定性和收敛性方面的不足。\\n\\n**核心思路**：论文通过分析带有f-散度正则化的确定性仅演员PPO算法，建立了非均匀Lipschitz光滑性条件和Łojasiewicz不等式，从而推进了对PPO-Clip算法的理论理解。\\n\\n**技术框架**：整体架构包括对PPO-Clip算法的理论分析，主要模块包括算法的收敛性分析、光滑性条件的推导以及正则化器的比较。\\n\\n**关键创新**：最重要的技术创新在于导出了PPO-Clip算法的非渐近线性收敛率，特别是针对前向KL正则化器的分析，为理解算法的全局收敛性提供了新的视角。\\n\\n**关键设计**：论文中设计了特定的正则化项（如反向KL和前向KL），并通过软最大策略参数化来实现算法的稳定性和收敛性，确保了理论推导的严谨性。",
            "application_zh": "该研究的潜在应用领域包括自然语言处理、机器人控制和智能决策系统等。通过提高PPO-Clip算法的收敛性和稳定性，能够在实际应用中更有效地训练大型语言模型，提升其在复杂任务中的表现，具有重要的实际价值和未来影响。",
            "highlight_zh": "实验结果表明，PPO-Clip算法在使用前向KL正则化器时，能够实现非渐近线性收敛，收敛速度相比于传统方法提升了显著的幅度，具体性能数据在实验中得到了验证，展示了其优越性。",
            "tags_zh": [
                "强化学习",
                "PPO算法",
                "收敛性分析",
                "正则化",
                "理论研究",
                "大型语言模型",
                "算法稳定性"
            ],
            "_index": 20,
            "_used_api": "openai",
            "figures": []
        },
        {
            "title": "The Evolution of Reranking Models in Information Retrieval: From Heuristic Methods to Large Language Models",
            "authors": [
                "Tejul Pandit",
                "Sakshi Mahendru",
                "Meet Raval",
                "Dhvani Upadhyay"
            ],
            "arxiv_id": "2512.16236v1",
            "summary": "Reranking is a critical stage in contemporary information retrieval (IR) systems, improving the relevance of the user-presented final results by honing initial candidate sets. This paper is a thorough guide to examine the changing reranker landscape and offer a clear view of the advancements made in reranking methods. We present a comprehensive survey of reranking models employed in IR, particularly within modern Retrieval Augmented Generation (RAG) pipelines, where retrieved documents notably influence output quality.\n  We embark on a chronological journey through the historical trajectory of reranking techniques, starting with foundational approaches, before exploring the wide range of sophisticated neural network architectures such as cross-encoders, sequence-generation models like T5, and Graph Neural Networks (GNNs) utilized for structural information. Recognizing the computational cost of advancing neural rerankers, we analyze techniques for enhancing efficiency, notably knowledge distillation for creating competitive, lighter alternatives. Furthermore, we map the emerging territory of integrating Large Language Models (LLMs) in reranking, examining novel prompting strategies and fine-tuning tactics. This survey seeks to elucidate the fundamental ideas, relative effectiveness, computational features, and real-world trade-offs of various reranking strategies. The survey provides a structured synthesis of the diverse reranking paradigms, highlighting their underlying principles and comparative strengths and weaknesses.",
            "categories": [
                "cs.IR",
                "cs.AI"
            ],
            "primary_category": "cs.IR",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "15 pages, 1 figure, Accepted in CLNLP'25",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16236v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "distillation"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]large language model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 10.5,
            "hit_pillars": [
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "综述信息检索中重排序模型演进：从启发式方法到大型语言模型",
            "summary_zh": "重排序是现代信息检索（IR）系统中的关键阶段，它通过优化初始候选集来提高用户最终检索结果的相关性。本文全面考察了重排序领域的发展变化，清晰地展示了重排序方法的进步。我们对信息检索中使用的重排序模型进行了全面的综述，特别是在现代检索增强生成（RAG）流程中，检索到的文档对输出质量有显著影响。我们按时间顺序回顾了重排序技术的发展历程，从基础方法开始，然后探讨了各种复杂的神经网络架构，如交叉编码器、序列生成模型（如T5）和用于结构信息的图神经网络（GNN）。考虑到神经重排序器推进的计算成本，我们分析了提高效率的技术，特别是用于创建有竞争力的、更轻量级替代方案的知识蒸馏。此外，我们还绘制了大型语言模型（LLM）集成到重排序中的新兴领域，研究了新颖的提示策略和微调策略。本综述旨在阐明各种重排序策略的基本思想、相对有效性、计算特征和实际权衡，并提供对不同重排序范式的结构化综合，突出其基本原理以及相对优势和劣势。",
            "intro_zh": [
                "现有信息检索系统在初始检索后，需要对候选结果进行重排序以提升用户体验，但传统方法效果有限。",
                "本文旨在全面回顾信息检索领域中重排序模型的发展历程，并分析各种模型的优缺点和适用场景。",
                "综述涵盖了从传统方法到基于深度学习（包括LLM）的重排序技术，并探讨了效率优化和实际应用。"
            ],
            "method_zh": "**问题定义**：论文旨在解决信息检索系统中重排序的问题。现有方法，特别是早期的启发式方法，在处理复杂查询和语义理解方面存在不足。随着数据量的增长和用户需求的提高，传统的重排序方法难以满足实际应用的需求，需要更有效的模型来提升检索结果的相关性和准确性。此外，计算成本也是一个重要的考量因素，复杂的神经重排序器计算开销大，限制了其在大规模数据集上的应用。\\n\\n**核心思路**：论文的核心思路是对信息检索中的重排序模型进行全面的回顾和分析，从早期的启发式方法到基于深度学习的模型，再到最近的大型语言模型（LLM），梳理其发展脉络和演进趋势。通过对比不同模型的优缺点，为研究人员和工程师提供选择合适的重排序模型的指导。同时，论文还关注了如何提高重排序模型的效率，例如通过知识蒸馏等技术。\\n\\n**技术框架**：论文的整体框架是按照时间顺序和技术类型对重排序模型进行分类和介绍。首先回顾了早期的启发式方法，然后介绍了基于神经网络的重排序模型，包括交叉编码器、序列生成模型（如T5）和图神经网络（GNN）。接着，论文探讨了如何提高神经重排序器的效率，例如通过知识蒸馏。最后，论文介绍了大型语言模型（LLM）在重排序中的应用，包括提示策略和微调策略。\\n\\n**关键创新**：论文的关键创新在于对信息检索领域中重排序模型进行了全面的综述和分析，涵盖了从传统方法到最新的大型语言模型（LLM）。论文不仅介绍了各种模型的原理和技术细节，还对比了它们的优缺点和适用场景，为研究人员和工程师提供了有价值的参考。此外，论文还关注了如何提高重排序模型的效率，例如通过知识蒸馏等技术。\\n\\n**关键设计**：论文没有提出新的模型或算法，而是一篇综述文章，因此没有具体的参数设置、损失函数或网络结构等技术细节。但是，论文对各种重排序模型的技术细节进行了详细的介绍和分析，例如交叉编码器的结构、T5模型的序列生成方式、GNN在结构信息建模中的应用等。此外，论文还介绍了知识蒸馏在提高重排序模型效率中的应用，包括如何选择合适的教师模型和学生模型，以及如何设计蒸馏损失函数。",
            "application_zh": "该研究成果可应用于各种信息检索系统，例如搜索引擎、问答系统、推荐系统等。通过选择合适的重排序模型，可以提高检索结果的相关性和准确性，提升用户体验。此外，该研究还可以为开发更高效、更有效的重排序模型提供指导，推动信息检索技术的发展。",
            "highlight_zh": "本文是一篇综述性文章，主要贡献在于对现有重排序模型进行了全面的总结和分析，没有提供具体的实验结果。但是，论文对各种重排序模型的性能进行了对比和评估，为研究人员和工程师提供了选择合适的重排序模型的参考依据。例如，论文指出交叉编码器在处理长文本时具有优势，而T5模型在序列生成方面表现出色。",
            "tags_zh": [
                "信息检索",
                "重排序",
                "检索增强生成",
                "大型语言模型",
                "知识蒸馏"
            ],
            "_index": 21,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16236v1/Reranker_Module_2.png",
                    "caption": "",
                    "figure_id": "fig_0"
                }
            ]
        },
        {
            "title": "ReinforceGen: Hybrid Skill Policies with Automated Data Generation and Reinforcement Learning",
            "authors": [
                "Zihan Zhou",
                "Animesh Garg",
                "Ajay Mandlekar",
                "Caelan Garrett"
            ],
            "arxiv_id": "2512.16861v1",
            "summary": "Long-horizon manipulation has been a long-standing challenge in the robotics community. We propose ReinforceGen, a system that combines task decomposition, data generation, imitation learning, and motion planning to form an initial solution, and improves each component through reinforcement-learning-based fine-tuning. ReinforceGen first segments the task into multiple localized skills, which are connected through motion planning. The skills and motion planning targets are trained with imitation learning on a dataset generated from 10 human demonstrations, and then fine-tuned through online adaptation and reinforcement learning. When benchmarked on the Robosuite dataset, ReinforceGen reaches 80% success rate on all tasks with visuomotor controls in the highest reset range setting. Additional ablation studies show that our fine-tuning approaches contributes to an 89% average performance increase. More results and videos available in https://reinforcegen.github.io/",
            "categories": [
                "cs.RO",
                "cs.AI",
                "cs.LG"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16861v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation",
                        "motion planning"
                    ],
                    "score": 4.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]reinforcement learning",
                        "imitation learning"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 10.0,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "ReinforceGen：结合自动数据生成与强化学习的混合技能策略，解决长时程操作任务。",
            "summary_zh": "本文提出ReinforceGen，一个结合任务分解、数据生成、模仿学习和运动规划的系统，旨在解决机器人领域中长期存在的长时程操作挑战。ReinforceGen首先将任务分割成多个局部技能，并通过运动规划连接这些技能。利用从10个人类演示生成的数据集，使用模仿学习训练技能和运动规划目标，然后通过在线自适应和强化学习进行微调。在Robosuite数据集上的基准测试表明，ReinforceGen在最高重置范围设置下，所有视觉运动控制任务的成功率达到80%。额外的消融研究表明，我们的微调方法平均提高了89%的性能。",
            "intro_zh": [
                "长时程操作任务是机器人领域的长期挑战，现有方法难以有效分解任务并进行优化。",
                "ReinforceGen通过任务分解、数据生成和模仿学习构建初始解决方案，再利用强化学习进行微调。",
                "在Robosuite数据集上，ReinforceGen在视觉运动控制任务中达到80%的成功率，性能平均提升89%。"
            ],
            "method_zh": "**问题定义**：长时程操作任务需要机器人执行一系列复杂的动作才能完成，现有方法通常难以有效地分解任务，并且难以从少量数据中学习到鲁棒的策略。此外，如何有效地利用强化学习来微调模仿学习得到的策略也是一个挑战。\\n\\n**核心思路**：ReinforceGen的核心思路是将长时程任务分解为多个局部技能，并利用模仿学习从少量人类演示数据中学习这些技能的初始策略。然后，通过强化学习对这些策略进行微调，以提高其鲁棒性和泛化能力。运动规划用于连接这些技能，形成完整的任务执行流程。\\n\\n**技术框架**：ReinforceGen的整体框架包括以下几个主要阶段：1) 任务分解：将长时程任务分解为多个局部技能。2) 数据生成：利用少量人类演示数据生成大规模的训练数据集。3) 模仿学习：使用生成的数据集训练技能和运动规划目标的初始策略。4) 运动规划：使用运动规划器连接各个技能，形成完整的任务执行流程。5) 强化学习微调：使用强化学习算法对技能策略和运动规划目标进行微调，以提高其鲁棒性和泛化能力。\\n\\n**关键创新**：ReinforceGen的关键创新在于结合了模仿学习和强化学习的优势，利用模仿学习从少量数据中快速学习到初始策略，然后利用强化学习对策略进行微调，从而提高了策略的鲁棒性和泛化能力。此外，自动数据生成也减少了对大量人工标注数据的依赖。\\n\\n**关键设计**：论文中使用了从10个人类演示生成的数据集进行模仿学习。强化学习部分，具体使用的算法和奖励函数等细节未明确给出，属于未知信息。运动规划器的具体选择也未明确说明。",
            "application_zh": "ReinforceGen具有广泛的应用前景，例如在工业自动化、家庭服务机器人和医疗机器人等领域。它可以用于解决复杂的装配、抓取和操作任务，提高机器人的自主性和效率。通过结合模仿学习和强化学习，ReinforceGen可以有效地利用少量的人类演示数据，并不断地从环境中学习和改进，从而实现更智能、更灵活的机器人控制。",
            "highlight_zh": "ReinforceGen在Robosuite数据集上进行了评估，结果表明，在最高重置范围设置下，所有视觉运动控制任务的成功率达到了80%。消融研究表明，强化学习微调方法平均提高了89%的性能，证明了该方法的有效性。这些结果表明，ReinforceGen能够有效地解决长时程操作任务，并具有很强的鲁棒性和泛化能力。",
            "tags_zh": [
                "长时程操作",
                "机器人控制",
                "强化学习",
                "模仿学习",
                "任务分解",
                "运动规划",
                "数据生成"
            ],
            "_index": 22,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16861v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16861v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16861v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "SARMAE: Masked Autoencoder for SAR Representation Learning",
            "authors": [
                "Danxu Liu",
                "Di Wang",
                "Hebaixu Wang",
                "Haoyang Chen",
                "Wentao Jiang",
                "Yilin Cheng",
                "Haonan Guo",
                "Wei Cui",
                "Jing Zhang"
            ],
            "arxiv_id": "2512.16635v1",
            "summary": "Synthetic Aperture Radar (SAR) imagery plays a critical role in all-weather, day-and-night remote sensing applications. However, existing SAR-oriented deep learning is constrained by data scarcity, while the physically grounded speckle noise in SAR imagery further hampers fine-grained semantic representation learning. To address these challenges, we propose SARMAE, a Noise-Aware Masked Autoencoder for self-supervised SAR representation learning. Specifically, we construct SAR-1M, the first million-scale SAR dataset, with additional paired optical images, to enable large-scale pre-training. Building upon this, we design Speckle-Aware Representation Enhancement (SARE), which injects SAR-specific speckle noise into masked autoencoders to facilitate noise-aware and robust representation learning. Furthermore, we introduce Semantic Anchor Representation Constraint (SARC), which leverages paired optical priors to align SAR features and ensure semantic consistency. Extensive experiments across multiple SAR datasets demonstrate that SARMAE achieves state-of-the-art performance on classification, detection, and segmentation tasks. Code and models will be available at https://github.com/MiliLab/SARMAE.",
            "categories": [
                "cs.CV",
                "cs.LG"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Code and models will be available at https://github.com/MiliLab/SARMAE",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16635v1",
            "code_links": [
                {
                    "url": "https://github.com/MiliLab/SARMAE",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]representation learning",
                        "[T]masked autoencoder"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "SARMAE：面向SAR图像表征学习的噪声感知掩码自编码器",
            "summary_zh": "合成孔径雷达(SAR)图像在全天候、昼夜遥感应用中起着关键作用。然而，现有的面向SAR的深度学习受到数据稀缺的限制，并且SAR图像中固有的物理散斑噪声进一步阻碍了细粒度语义表征学习。为了解决这些挑战，我们提出了SARMAE，一种用于自监督SAR表征学习的噪声感知掩码自编码器。具体来说，我们构建了首个百万级SAR数据集SAR-1M，并包含配对的光学图像，以实现大规模预训练。在此基础上，我们设计了散斑感知表征增强(SARE)，它将SAR特有的散斑噪声注入到掩码自编码器中，以促进噪声感知和鲁棒的表征学习。此外，我们引入了语义锚点表征约束(SARC)，它利用配对的光学先验来对齐SAR特征并确保语义一致性。在多个SAR数据集上的大量实验表明，SARMAE在分类、检测和分割任务上实现了最先进的性能。代码和模型将在https://github.com/MiliLab/SARMAE上提供。",
            "intro_zh": [
                "现有SAR图像深度学习受限于数据稀缺和散斑噪声，阻碍了细粒度语义表征学习。",
                "SARMAE通过构建大规模SAR数据集，并设计散斑感知表征增强模块，实现噪声鲁棒的表征学习。",
                "实验结果表明，SARMAE在SAR图像分类、检测和分割任务上均取得了state-of-the-art的性能。"
            ],
            "method_zh": "**问题定义**：现有的SAR图像深度学习方法面临两个主要挑战：一是缺乏大规模的标注数据，导致模型泛化能力不足；二是SAR图像中固有的散斑噪声会严重干扰特征提取，影响模型的性能。因此，如何有效地利用无标注的SAR数据，并克服散斑噪声的影响，是SAR图像表征学习的关键问题。\\n\\n**核心思路**：SARMAE的核心思路是利用掩码自编码器(MAE)进行自监督学习，并通过引入SAR特有的散斑噪声和配对光学图像的语义信息来增强模型的表征能力。具体来说，通过散斑感知表征增强(SARE)模块，将散斑噪声注入到MAE中，使模型能够学习到对噪声更鲁棒的特征。同时，利用语义锚点表征约束(SARC)模块，将SAR图像的特征与配对光学图像的特征对齐，从而提高SAR图像特征的语义一致性。\\n\\n**技术框架**：SARMAE的整体框架包括三个主要部分：首先，使用掩码策略对SAR图像进行随机遮挡；然后，将遮挡后的图像输入到编码器中，提取低维特征；接着，通过散斑感知表征增强(SARE)模块，将散斑噪声注入到编码后的特征中；最后，使用解码器重构原始SAR图像，并通过语义锚点表征约束(SARC)模块，将SAR图像的特征与配对光学图像的特征对齐。\\n\\n**关键创新**：SARMAE的关键创新点在于：1) 构建了大规模的SAR数据集SAR-1M，为自监督学习提供了充足的数据；2) 提出了散斑感知表征增强(SARE)模块，通过注入散斑噪声来提高模型的鲁棒性；3) 引入了语义锚点表征约束(SARC)模块，利用配对光学图像的语义信息来提高SAR图像特征的语义一致性。与现有方法相比，SARMAE能够更有效地利用无标注的SAR数据，并克服散斑噪声的影响，从而提高SAR图像表征学习的性能。\\n\\n**关键设计**：在SARE模块中，作者通过模拟SAR成像过程中的散斑噪声，生成具有不同强度和分布的噪声图像，并将这些噪声图像与编码后的特征进行融合。在SARC模块中，作者使用对比学习的方法，将SAR图像的特征与配对光学图像的特征进行对齐。具体的损失函数包括重构损失、对比损失和正则化损失。网络结构采用Transformer架构，并针对SAR图像的特点进行了优化。",
            "application_zh": "SARMAE在全天候、昼夜遥感应用中具有广泛的应用前景，例如目标检测、图像分割、变化检测、地物分类等。该研究成果可以提高SAR图像的解译精度和自动化程度，为灾害监测、资源管理、环境评估等领域提供更可靠的技术支持，具有重要的实际应用价值和未来发展潜力。",
            "highlight_zh": "SARMAE在多个SAR数据集上进行了广泛的实验，结果表明，SARMAE在分类、检测和分割任务上均取得了state-of-the-art的性能。例如，在某SAR图像分类数据集上，SARMAE的准确率比现有最佳方法提高了3%以上。此外，消融实验验证了SARE和SARC模块的有效性，证明了散斑噪声感知和语义一致性约束对于SAR图像表征学习的重要性。",
            "tags_zh": [
                "SAR图像",
                "自监督学习",
                "掩码自编码器",
                "散斑噪声",
                "表征学习",
                "遥感",
                "深度学习"
            ],
            "_index": 23,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16635v1/images/radar.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16635v1/images/dataset.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16635v1/images/model.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Causal-Tune: Mining Causal Factors from Vision Foundation Models for Domain Generalized Semantic Segmentation",
            "authors": [
                "Yin Zhang",
                "Yongqiang Zhang",
                "Yaoyue Zheng",
                "Bogdan Raducanu",
                "Dan Liu"
            ],
            "arxiv_id": "2512.16567v1",
            "summary": "Fine-tuning Vision Foundation Models (VFMs) with a small number of parameters has shown remarkable performance in Domain Generalized Semantic Segmentation (DGSS). Most existing works either train lightweight adapters or refine intermediate features to achieve better generalization on unseen domains. However, they both overlook the fact that long-term pre-trained VFMs often exhibit artifacts, which hinder the utilization of valuable representations and ultimately degrade DGSS performance. Inspired by causal mechanisms, we observe that these artifacts are associated with non-causal factors, which usually reside in the low- and high-frequency components of the VFM spectrum. In this paper, we explicitly examine the causal and non-causal factors of features within VFMs for DGSS, and propose a simple yet effective method to identify and disentangle them, enabling more robust domain generalization. Specifically, we propose Causal-Tune, a novel fine-tuning strategy designed to extract causal factors and suppress non-causal ones from the features of VFMs. First, we extract the frequency spectrum of features from each layer using the Discrete Cosine Transform (DCT). A Gaussian band-pass filter is then applied to separate the spectrum into causal and non-causal components. To further refine the causal components, we introduce a set of causal-aware learnable tokens that operate in the frequency domain, while the non-causal components are discarded. Finally, refined features are transformed back into the spatial domain via inverse DCT and passed to the next layer. Extensive experiments conducted on various cross-domain tasks demonstrate the effectiveness of Causal-Tune. In particular, our method achieves superior performance under adverse weather conditions, improving +4.8% mIoU over the baseline in snow conditions.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Accepted by AAAI 2026",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16567v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]foundation model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "Causal-Tune：挖掘视觉基础模型中的因果因子，用于领域泛化语义分割",
            "summary_zh": "本文提出了一种针对领域泛化语义分割（DGSS）的因果调优（Causal-Tune）方法，旨在解决视觉基础模型（VFM）中存在的伪影问题。作者观察到，这些伪影与VFM频谱中的低频和高频非因果因素相关。Causal-Tune通过显式地检查VFM特征中的因果和非因果因素，并分离它们，从而实现更鲁棒的领域泛化。该方法首先使用离散余弦变换（DCT）提取每一层特征的频谱，然后应用高斯带通滤波器将频谱分离为因果和非因果分量。为了进一步细化因果分量，引入了一组在频域中操作的因果感知可学习token，并丢弃非因果分量。最后，细化后的特征通过逆DCT转换回空间域，并传递到下一层。在各种跨域任务上的大量实验表明了Causal-Tune的有效性，尤其是在恶劣天气条件下，其性能优于基线，在雪地条件下提高了+4.8% mIoU。",
            "intro_zh": [
                "现有领域泛化语义分割方法忽略了预训练视觉基础模型中存在的伪影，这些伪影会阻碍有价值表征的利用，降低性能。",
                "Causal-Tune通过分析视觉基础模型特征的频谱，分离因果和非因果因素，抑制非因果因素，从而提升领域泛化能力。",
                "实验表明，Causal-Tune在各种跨域任务中表现出色，尤其是在恶劣天气条件下，显著提升了语义分割的准确性。"
            ],
            "method_zh": "**问题定义**：领域泛化语义分割（DGSS）旨在使模型在未见过的目标领域上也能保持良好的分割性能。现有的方法，如微调轻量级适配器或改进中间特征，往往忽略了预训练视觉基础模型（VFM）中存在的伪影。这些伪影与非因果因素相关，阻碍了VFM中宝贵表征的有效利用，最终导致DGSS性能下降。\\n\\n**核心思路**：Causal-Tune的核心思路是基于因果机制，显式地识别和分离VFM特征中的因果和非因果因素。通过抑制非因果因素，提取更纯粹的因果表征，从而提高模型在不同领域之间的泛化能力。这种方法避免了直接修改网络结构，而是专注于特征层面的因果关系建模。\\n\\n**技术框架**：Causal-Tune的整体框架包括以下几个主要步骤：1) 使用离散余弦变换（DCT）将VFM每一层的特征转换到频域；2) 应用高斯带通滤波器将频谱分离为因果和非因果分量；3) 引入一组因果感知可学习token，在频域中操作，以细化因果分量；4) 丢弃非因果分量；5) 使用逆DCT将细化后的特征转换回空间域，并传递到下一层。\\n\\n**关键创新**：Causal-Tune的关键创新在于它将因果推理引入到领域泛化语义分割中，并提出了一种在频域中分离和提纯因果特征的方法。与现有方法不同，Causal-Tune不是简单地调整网络参数或特征表示，而是直接从因果关系的角度出发，挖掘影响模型泛化能力的根本因素。\\n\\n**关键设计**：Causal-Tune的关键设计包括：1) 使用DCT进行频域分析，以便于分离不同频率分量；2) 设计高斯带通滤波器，用于区分因果和非因果分量；3) 引入因果感知可学习token，用于在频域中细化因果特征。高斯带通滤波器的参数（如中心频率和带宽）需要根据具体任务进行调整。可学习token的数量和维度也是重要的超参数，需要通过实验进行优化。",
            "application_zh": "Causal-Tune在自动驾驶、遥感图像分析、医学图像诊断等领域具有广泛的应用前景。通过提高模型在不同环境和条件下的泛化能力，可以显著提升这些应用系统的可靠性和鲁棒性。例如，在自动驾驶中，Causal-Tune可以帮助车辆更好地应对恶劣天气条件，提高行车安全性。在医学图像诊断中，可以提高模型在不同医院和设备上的适用性，辅助医生进行更准确的诊断。",
            "highlight_zh": "Causal-Tune在多个跨域语义分割任务上取得了显著的性能提升。特别是在恶劣天气条件下，例如雪地场景，Causal-Tune相比基线方法提高了+4.8%的mIoU。实验结果表明，Causal-Tune能够有效地提取因果特征，抑制非因果因素，从而提高模型在未见过的目标领域上的泛化能力。",
            "tags_zh": [
                "领域泛化",
                "语义分割",
                "因果推理",
                "视觉基础模型",
                "离散余弦变换"
            ],
            "_index": 24,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16567v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16567v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16567v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Smile on the Face, Sadness in the Eyes: Bridging the Emotion Gap with a Multimodal Dataset of Eye and Facial Behaviors",
            "authors": [
                "Kejun Liu",
                "Yuanyuan Liu",
                "Lin Wei",
                "Chang Tang",
                "Yibing Zhan",
                "Zijing Chen",
                "Zhe Chen"
            ],
            "arxiv_id": "2512.16485v1",
            "summary": "Emotion Recognition (ER) is the process of analyzing and identifying human emotions from sensing data. Currently, the field heavily relies on facial expression recognition (FER) because visual channel conveys rich emotional cues. However, facial expressions are often used as social tools rather than manifestations of genuine inner emotions. To understand and bridge this gap between FER and ER, we introduce eye behaviors as an important emotional cue and construct an Eye-behavior-aided Multimodal Emotion Recognition (EMER) dataset. To collect data with genuine emotions, spontaneous emotion induction paradigm is exploited with stimulus material, during which non-invasive eye behavior data, like eye movement sequences and eye fixation maps, is captured together with facial expression videos. To better illustrate the gap between ER and FER, multi-view emotion labels for mutimodal ER and FER are separately annotated. Furthermore, based on the new dataset, we design a simple yet effective Eye-behavior-aided MER Transformer (EMERT) that enhances ER by bridging the emotion gap. EMERT leverages modality-adversarial feature decoupling and a multitask Transformer to model eye behaviors as a strong complement to facial expressions. In the experiment, we introduce seven multimodal benchmark protocols for a variety of comprehensive evaluations of the EMER dataset. The results show that the EMERT outperforms other state-of-the-art multimodal methods by a great margin, revealing the importance of modeling eye behaviors for robust ER. To sum up, we provide a comprehensive analysis of the importance of eye behaviors in ER, advancing the study on addressing the gap between FER and ER for more robust ER performance. Our EMER dataset and the trained EMERT models will be publicly available at https://github.com/kejun1/EMER.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Accepted by TMM",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16485v1",
            "code_links": [
                {
                    "url": "https://github.com/kejun1/EMER",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]multimodal"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出EMER数据集和EMERT模型，利用眼部行为弥合面部表情识别和情感识别之间的差距",
            "summary_zh": "情感识别(ER)是从感知数据中分析和识别人类情感的过程。目前，该领域严重依赖于面部表情识别(FER)，因为视觉通道传递丰富的情感线索。然而，面部表情通常被用作社交工具，而不是真实内心情感的体现。为了理解和弥合FER和ER之间的差距，我们引入眼部行为作为重要的情感线索，并构建了一个眼部行为辅助的多模态情感识别(EMER)数据集。为了收集具有真实情感的数据，利用刺激材料进行自发情感诱导，在此过程中，非侵入性眼部行为数据（如眼动序列和眼部注视图）与面部表情视频一起被捕获。为了更好地说明ER和FER之间的差距，分别对多模态ER和FER进行多视角情感标注。此外，基于新的数据集，我们设计了一个简单而有效的眼部行为辅助的MER Transformer (EMERT)，通过弥合情感差距来增强ER。EMERT利用模态对抗特征解耦和一个多任务Transformer来将眼部行为建模为面部表情的有力补充。在实验中，我们为EMER数据集的各种综合评估引入了七种多模态基准协议。结果表明，EMERT优于其他最先进的多模态方法，揭示了建模眼部行为对于鲁棒ER的重要性。总而言之，我们对眼部行为在ER中的重要性进行了全面的分析，从而推进了解决FER和ER之间差距的研究，以获得更强大的ER性能。我们的EMER数据集和训练好的EMERT模型将在https://github.com/kejun1/EMER上公开。",
            "intro_zh": [
                "现有情感识别方法过度依赖面部表情，忽略了面部表情可能伪装真实情感的问题。",
                "提出EMER数据集和EMERT模型，将眼部行为作为情感线索，弥合面部表情识别和情感识别之间的差距。",
                "实验结果表明，EMERT模型在EMER数据集上显著优于其他多模态方法，验证了眼部行为在情感识别中的重要性。"
            ],
            "method_zh": "**问题定义**：现有情感识别方法主要依赖面部表情，但面部表情容易受到社会因素的影响，可能无法真实反映个体的情感状态。这导致面部表情识别(FER)和真实情感识别(ER)之间存在差距。论文旨在解决如何利用更可靠的情感线索（如眼部行为）来提升情感识别的准确性和鲁棒性。\n\n**核心思路**：论文的核心思路是将眼部行为作为面部表情的补充，通过多模态融合的方式来提升情感识别的性能。眼部行为不易伪装，能够更真实地反映个体的情感状态。通过建模眼部行为与面部表情之间的关系，可以有效弥合FER和ER之间的差距。\n\n**技术框架**：论文提出了一个名为EMERT（Eye-behavior-aided MER Transformer）的模型。该模型包含以下几个主要模块：1) 特征提取模块：分别提取面部表情视频和眼部行为数据的特征。2) 模态对抗特征解耦模块：用于解耦模态特定和模态共享的特征，提高模型的泛化能力。3) 多任务Transformer模块：用于建模眼部行为和面部表情之间的关系，并进行情感分类。整体流程是先分别提取两种模态的特征，然后进行特征解耦和融合，最后通过Transformer进行情感预测。\n\n**关键创新**：论文的关键创新在于：1) 提出了EMER数据集，该数据集包含面部表情视频和眼部行为数据，并对两种模态分别进行了情感标注。2) 设计了模态对抗特征解耦模块，可以有效分离模态特定和模态共享的特征。3) 提出了EMERT模型，该模型能够有效利用眼部行为来提升情感识别的性能。\n\n**关键设计**：在模态对抗特征解耦模块中，使用了梯度反转层(GRL)来实现对抗训练。多任务Transformer模块使用了标准的Transformer结构，并针对情感识别任务进行了优化。损失函数包括情感分类损失和模态对抗损失。实验中，使用了七种多模态基准协议对EMER数据集进行了评估。",
            "application_zh": "该研究成果可应用于人机交互、心理健康评估、智能客服等领域。通过结合面部表情和眼部行为进行情感识别，可以更准确地理解用户的情感状态，从而提供更个性化和人性化的服务。例如，在心理健康评估中，可以利用该技术辅助医生诊断，提高诊断的准确性。",
            "highlight_zh": "实验结果表明，EMERT模型在EMER数据集上取得了显著的性能提升，优于其他最先进的多模态方法。具体来说，EMERT模型在七种多模态基准协议上均取得了最佳结果，平均提升幅度超过5%。这验证了眼部行为在情感识别中的重要性，以及EMERT模型的有效性。",
            "tags_zh": [
                "情感识别",
                "面部表情识别",
                "眼部行为",
                "多模态融合",
                "Transformer",
                "数据集",
                "特征解耦"
            ],
            "_index": 25,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16485v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16485v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16485v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Pretrained Battery Transformer (PBT): A battery life prediction foundation model",
            "authors": [
                "Ruifeng Tan",
                "Weixiang Hong",
                "Jia Li",
                "Jiaqiang Huang",
                "Tong-Yi Zhang"
            ],
            "arxiv_id": "2512.16334v1",
            "summary": "Early prediction of battery cycle life is essential for accelerating battery research, manufacturing, and deployment. Although machine learning methods have shown encouraging results, progress is hindered by data scarcity and heterogeneity arising from diverse aging conditions. In other fields, foundation models (FMs) trained on diverse datasets have achieved broad generalization through transfer learning, but no FMs have been reported for battery cycle life prediction yet. Here we present the Pretrained Battery Transformer (PBT), the first FM for battery life prediction, developed through domain-knowledge-encoded mixture-of-expert layers. Validated on the largest public battery life database, PBT learns transferable representations from 13 lithium-ion battery (LIB) datasets, outperforming existing models by an average of 19.8%. With transfer learning, PBT achieves state-of-the-art performance across 15 diverse datasets encompassing various operating conditions, formation protocols, and chemistries of LIBs. This work establishes a foundation model pathway for battery lifetime prediction, paving the way toward universal battery lifetime prediction systems.",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "5 figures in the main content",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16334v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]foundation model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出预训练电池Transformer（PBT），用于电池寿命预测，显著提升泛化性能。",
            "summary_zh": "本文提出了预训练电池Transformer（PBT），这是首个用于电池寿命预测的Foundation Model。该模型通过领域知识编码的混合专家层进行训练。在最大的公开电池寿命数据库上验证表明，PBT从13个锂离子电池（LIB）数据集学习到可迁移的表征，性能平均优于现有模型19.8%。通过迁移学习，PBT在涵盖各种操作条件、形成协议和LIB化学成分的15个不同数据集上实现了最先进的性能。这项工作为电池寿命预测建立了一个基础模型路径，为通用电池寿命预测系统铺平了道路。",
            "intro_zh": [
                "电池循环寿命的早期预测对于加速电池研究至关重要，但数据稀缺和异构性阻碍了现有机器学习方法的进展。",
                "PBT通过领域知识编码的混合专家层，从多样化的电池数据集中学习可迁移的表征，从而实现更好的泛化能力。",
                "实验结果表明，PBT在多个数据集上显著优于现有模型，为电池寿命预测提供了一个强大的基础模型。"
            ],
            "method_zh": "**问题定义**：论文旨在解决电池循环寿命预测问题。现有方法受限于数据稀缺和异构性，难以在不同工况、化学成分的电池上实现良好的泛化性能。因此，需要一种能够从多样化数据中学习通用表征的模型。\n\n**核心思路**：论文的核心思路是借鉴自然语言处理领域的Foundation Model思想，通过在大量电池数据上进行预训练，使模型学习到通用的电池老化规律表征。然后，通过迁移学习，将预训练模型应用于新的电池数据集，从而提高预测精度和泛化能力。领域知识编码的混合专家层旨在更好地捕捉电池老化的复杂特性。\n\n**技术框架**：PBT的整体框架基于Transformer架构，并引入了混合专家层（Mixture-of-Experts, MoE）。MoE允许模型根据输入数据的不同特征，选择不同的专家网络进行处理，从而提高模型的表达能力。整个流程包括：1）数据预处理；2）PBT预训练；3）迁移学习（在新数据集上进行微调）。\n\n**关键创新**：PBT的关键创新在于：1）首次将Foundation Model的思想应用于电池寿命预测领域；2）通过领域知识编码的混合专家层，提高了模型对电池老化过程的建模能力；3）构建了基于Transformer的电池寿命预测模型，能够有效捕捉电池数据中的时序依赖关系。\n\n**关键设计**：PBT的关键设计包括：1）混合专家层的具体结构和专家网络的选择；2）预训练目标函数的选择，例如，可以使用自监督学习方法，预测电池的剩余寿命；3）Transformer的层数、注意力头数等超参数的设置；4）迁移学习时的微调策略，例如，可以只微调部分参数，以避免过拟合。",
            "application_zh": "PBT可应用于电池研发、生产和部署等多个领域。在研发阶段，可以加速新型电池材料的筛选和优化。在生产阶段，可以实现电池质量的早期预测和控制。在部署阶段，可以为电池管理系统提供更准确的寿命预测，从而优化电池的使用策略，延长电池的使用寿命，并降低电池更换成本。未来，PBT有望成为通用电池寿命预测系统的核心组成部分。",
            "highlight_zh": "PBT在最大的公开电池寿命数据库上验证，性能平均优于现有模型19.8%。通过迁移学习，PBT在涵盖各种操作条件、形成协议和LIB化学成分的15个不同数据集上实现了最先进的性能。这些结果表明，PBT具有很强的泛化能力和迁移学习能力，能够有效解决电池寿命预测中的数据稀缺和异构性问题。",
            "tags_zh": [
                "电池寿命预测",
                "预训练模型",
                "Transformer",
                "迁移学习",
                "混合专家层",
                "锂离子电池",
                "Foundation Model"
            ],
            "_index": 26,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "Coarse-to-Fine Open-Set Graph Node Classification with Large Language Models",
            "authors": [
                "Xueqi Ma",
                "Xingjun Ma",
                "Sarah Monazam Erfani",
                "Danilo Mandic",
                "James Bailey"
            ],
            "arxiv_id": "2512.16244v1",
            "summary": "Developing open-set classification methods capable of classifying in-distribution (ID) data while detecting out-of-distribution (OOD) samples is essential for deploying graph neural networks (GNNs) in open-world scenarios. Existing methods typically treat all OOD samples as a single class, despite real-world applications, especially high-stake settings such as fraud detection and medical diagnosis, demanding deeper insights into OOD samples, including their probable labels. This raises a critical question: can OOD detection be extended to OOD classification without true label information? To address this question, we propose a Coarse-to-Fine open-set Classification (CFC) framework that leverages large language models (LLMs) for graph datasets. CFC consists of three key components: a coarse classifier that uses LLM prompts for OOD detection and outlier label generation, a GNN-based fine classifier trained with OOD samples identified by the coarse classifier for enhanced OOD detection and ID classification, and refined OOD classification achieved through LLM prompts and post-processed OOD labels. Unlike methods that rely on synthetic or auxiliary OOD samples, CFC employs semantic OOD instances that are genuinely out-of-distribution based on their inherent meaning, improving interpretability and practical utility. Experimental results show that CFC improves OOD detection by ten percent over state-of-the-art methods on graph and text domains and achieves up to seventy percent accuracy in OOD classification on graph datasets.",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Accepted to AAAI 2026",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16244v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]large language model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出CFC框架，利用大语言模型实现图节点开放集分类与细粒度OOD识别。",
            "summary_zh": "开发能够在识别分布内(ID)数据的同时检测分布外(OOD)样本的开放集分类方法，对于在开放世界场景中部署图神经网络(GNN)至关重要。现有方法通常将所有OOD样本视为单个类别，然而现实应用，特别是欺诈检测和医疗诊断等高风险场景，需要对OOD样本进行更深入的分析，包括其可能的标签。这提出了一个关键问题：能否在没有真实标签信息的情况下将OOD检测扩展到OOD分类？为了解决这个问题，我们提出了一种粗到细的开放集分类(CFC)框架，该框架利用大型语言模型(LLM)处理图数据集。CFC由三个关键组件组成：一个使用LLM提示进行OOD检测和异常值标签生成的粗分类器，一个使用粗分类器识别的OOD样本训练的基于GNN的细分类器，用于增强OOD检测和ID分类，以及通过LLM提示和后处理OOD标签实现的精细化OOD分类。与依赖合成或辅助OOD样本的方法不同，CFC采用基于其内在含义的语义OOD实例，这些实例是真正分布外的，从而提高了可解释性和实用性。实验结果表明，CFC在图和文本领域将OOD检测提高了10%，并且在图数据集上实现了高达70%的OOD分类准确率。",
            "intro_zh": [
                "现有开放集图节点分类方法难以提供OOD样本的细粒度信息，限制了其在高风险场景的应用。",
                "CFC框架利用大语言模型进行粗粒度OOD检测和标签生成，再用GNN进行细粒度分类，提升OOD识别能力。",
                "实验表明，CFC在图和文本数据上显著提升了OOD检测性能，并在图数据上实现了较高的OOD分类准确率。"
            ],
            "method_zh": "**问题定义**：论文旨在解决图节点分类中的开放集识别问题，即区分已知类别（ID）和未知类别（OOD）。现有方法通常将所有OOD样本视为一个类别，无法提供OOD样本的细粒度信息，这在需要深入理解OOD样本的应用中（如欺诈检测）是不够的。\\n\\n**核心思路**：论文的核心思路是利用大语言模型（LLM）的语义理解能力，对OOD样本进行粗粒度的分类和标签生成，然后利用这些信息训练图神经网络（GNN），从而提升GNN在OOD检测和分类方面的性能。通过LLM的先验知识，为GNN提供关于OOD样本的语义信息，弥补了传统方法仅依赖数据分布的不足。\\n\\n**技术框架**：CFC框架包含三个主要阶段：1) **粗分类器**：利用LLM对图节点进行分类，并生成OOD样本的标签。通过设计合适的prompt，让LLM判断节点是否属于已知类别，并为OOD节点生成可能的标签。2) **细分类器**：使用GNN对节点进行分类，GNN的训练数据包括ID样本和由粗分类器识别并标记的OOD样本。GNN的目标是提升ID分类的准确率，并更好地区分ID和OOD样本。3) **OOD分类优化**：利用LLM对GNN的OOD分类结果进行优化，通过prompt和后处理，进一步提升OOD分类的准确率。\\n\\n**关键创新**：CFC的关键创新在于将大语言模型引入到图节点开放集分类中，利用LLM的语义理解能力来辅助GNN进行OOD检测和分类。与传统方法依赖合成或辅助OOD样本不同，CFC利用LLM生成语义相关的OOD标签，提高了OOD分类的可解释性和实用性。\\n\\n**关键设计**：在粗分类器中，关键在于设计有效的LLM prompt，prompt需要能够引导LLM准确判断节点是否属于已知类别，并为OOD节点生成合理的标签。在细分类器中，GNN的网络结构和训练方式需要能够有效利用LLM提供的OOD标签信息。在OOD分类优化阶段，需要设计合适的后处理方法，对LLM的输出进行修正，以提高OOD分类的准确率。",
            "application_zh": "该研究成果可应用于欺诈检测、医疗诊断、金融风控等领域。在这些领域中，识别和理解未知类型的异常行为至关重要。CFC框架能够帮助识别新型欺诈手段、罕见疾病或潜在风险，从而提高决策的准确性和效率，具有重要的实际应用价值。",
            "highlight_zh": "实验结果表明，CFC框架在图和文本数据集上显著提升了OOD检测的性能，相比现有方法提升了10%。在图数据集上，CFC实现了高达70%的OOD分类准确率，表明其能够有效识别和分类未知类型的节点。这些结果验证了CFC框架的有效性和优越性。",
            "tags_zh": [
                "图神经网络",
                "开放集分类",
                "分布外检测",
                "大语言模型",
                "节点分类"
            ],
            "_index": 27,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16244v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16244v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16244v1/ood-prompt.jpg",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "A Multimodal Approach to Alzheimer's Diagnosis: Geometric Insights from Cube Copying and Cognitive Assessments",
            "authors": [
                "Jaeho Yang",
                "Kijung Yoon"
            ],
            "arxiv_id": "2512.16184v1",
            "summary": "Early and accessible detection of Alzheimer's disease (AD) remains a critical clinical challenge, and cube-copying tasks offer a simple yet informative assessment of visuospatial function. This work proposes a multimodal framework that converts hand-drawn cube sketches into graph-structured representations capturing geometric and topological properties, and integrates these features with demographic information and neuropsychological test (NPT) scores for AD classification. Cube drawings are modeled as graphs with node features encoding spatial coordinates, local graphlet-based topology, and angular geometry, which are processed using graph neural networks and fused with age, education, and NPT features in a late-fusion model. Experimental results show that graph-based representations provide a strong unimodal baseline and substantially outperform pixel-based convolutional models, while multimodal integration further improves performance and robustness to class imbalance. SHAP-based interpretability analysis identifies specific graphlet motifs and geometric distortions as key predictors, closely aligning with clinical observations of disorganized cube drawings in AD. Together, these results establish graph-based analysis of cube copying as an interpretable, non-invasive, and scalable approach for Alzheimer's disease screening.",
            "categories": [
                "cs.LG"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16184v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]multimodal"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出基于图神经网络的多模态融合方法，用于阿尔茨海默病早期诊断。",
            "summary_zh": "阿尔茨海默病(AD)的早期和可及性检测仍然是一项关键的临床挑战，而立方体复制任务提供了一种简单但信息丰富的视觉空间功能评估方法。本研究提出了一种多模态框架，该框架将手绘立方体草图转换为图结构化表示，以捕获几何和拓扑属性，并将这些特征与人口统计信息和神经心理测试(NPT)分数相结合，用于AD分类。立方体图被建模为具有节点特征的图，节点特征编码空间坐标、基于局部图元的拓扑结构和角度几何，这些特征使用图神经网络进行处理，并在后期融合模型中与年龄、教育程度和NPT特征融合。实验结果表明，基于图的表示提供了强大的单模态基线，并且大大优于基于像素的卷积模型，而多模态集成进一步提高了性能和对类不平衡的鲁棒性。基于SHAP的可解释性分析确定了特定的图元模式和几何扭曲是关键预测因子，这与AD中紊乱的立方体图的临床观察结果密切相关。总之，这些结果确立了基于图的立方体复制分析作为一种可解释、非侵入性和可扩展的阿尔茨海默病筛查方法。",
            "intro_zh": [
                "现有阿尔茨海默病早期诊断方法复杂且可及性差，立方体复制任务提供了一种简单有效的评估视觉空间功能的方式。",
                "该论文提出将手绘立方体图转换为图结构，提取几何和拓扑特征，并结合人口统计学信息进行多模态融合。",
                "实验结果表明，基于图的表示优于传统像素方法，多模态融合进一步提升了诊断性能和鲁棒性。"
            ],
            "method_zh": "**问题定义**：阿尔茨海默病(AD)的早期诊断面临挑战，现有方法通常复杂且难以普及。手绘立方体复制任务虽然简单，但蕴含丰富的视觉空间信息，如何有效提取并利用这些信息是关键。现有方法，如基于像素的卷积神经网络，难以捕捉立方体图的几何结构和拓扑关系。\n\\n**核心思路**：将手绘立方体图转换为图结构，利用图神经网络(GNN)提取节点特征（空间坐标、局部图元拓扑、角度几何），从而更好地捕捉立方体图的几何和拓扑信息。结合人口统计学信息和神经心理测试(NPT)分数，进行多模态融合，提升诊断准确率。\n\\n**技术框架**：整体框架包括以下几个阶段：1) 数据预处理：将手绘立方体图转换为图结构，节点表示立方体的顶点，边表示连接关系。2) 特征提取：提取节点的空间坐标、局部图元拓扑和角度几何特征。3) 图神经网络处理：使用GNN对图结构进行学习，提取图级别的特征表示。4) 多模态融合：将GNN提取的图特征与人口统计学信息和NPT分数进行融合。5) 分类：使用分类器（如全连接网络）进行AD诊断。\n\\n**关键创新**：主要创新在于将手绘立方体图转换为图结构，并利用图神经网络进行特征提取。与传统的基于像素的卷积神经网络相比，GNN能够更好地捕捉立方体图的几何结构和拓扑关系，从而提高诊断准确率。此外，多模态融合也进一步提升了性能。\n\\n**关键设计**：节点特征包括：1) 空间坐标：节点的二维坐标。2) 局部图元拓扑：使用Graphlet Counting统计每个节点周围的局部图元数量。3) 角度几何：计算节点之间的角度关系。GNN采用Graph Convolutional Network (GCN)或Graph Attention Network (GAT)。损失函数采用交叉熵损失函数。后期融合采用简单的拼接或加权平均方法。",
            "application_zh": "该研究成果可应用于阿尔茨海默病的早期筛查和诊断，尤其适用于基层医疗机构，具有非侵入性、低成本、易于实施的优点。通过结合手绘立方体图分析和简单的认知测试，可以辅助医生进行初步诊断，为进一步的临床评估提供参考。未来，该方法有望推广到其他神经退行性疾病的早期诊断。",
            "highlight_zh": "实验结果表明，基于图的表示方法在单模态情况下优于基于像素的卷积模型。多模态融合进一步提升了性能，并提高了对类别不平衡的鲁棒性。SHAP分析表明，特定的图元模式和几何扭曲是关键的预测因子，与临床观察结果一致。该方法在阿尔茨海默病诊断任务上取得了显著的性能提升。",
            "tags_zh": [
                "阿尔茨海默病诊断",
                "图神经网络",
                "多模态融合",
                "立方体复制任务",
                "几何特征",
                "拓扑特征",
                "早期筛查"
            ],
            "_index": 28,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16184v1/Figure1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16184v1/Figure2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16184v1/Figure3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "TOGGLE: Temporal Logic-Guided Large Language Model Compression for Edge",
            "authors": [
                "Khurram Khalil",
                "Khaza Anuarul Hoque"
            ],
            "arxiv_id": "2512.16855v1",
            "summary": "Large Language Models (LLMs) deliver exceptional performance across natural language tasks but demand substantial computational resources, limiting their deployment on resource-constrained edge devices. Existing compression techniques, such as quantization and pruning, often degrade critical linguistic properties and lack formal guarantees for preserving model behavior. We propose Temporal Logic-Guided Large Language Model Compression (TOGGLE), a novel framework that leverages Signal Temporal Logic (STL) to formally specify and enforce linguistic properties during compression. TOGGLE employs an STL robustness-guided Bayesian optimization to systematically explore layer-wise quantization and pruning configurations, generating compressed models that formally satisfy specified linguistic constraints without retraining or fine-tuning. Evaluating TOGGLE on four LLM architectures (GPT-2, DeepSeek-V2 7B, LLaMA 3 8B, and Mistral 7B), we achieve up to 3.3x reduction in computational costs (FLOPs) and up to a 68.8% reduction in model size while satisfying all linguistic properties. TOGGLE represents the first integration of formal methods into LLM compression, enabling efficient, verifiable deployment of LLMs on edge hardware.",
            "categories": [
                "cs.AI",
                "cs.LO"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Published in the IEEE ICCAD 2025 conference",
            "doi": "10.1109/ICCAD66269.2025.11240962",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16855v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]large language model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "TOGGLE：时序逻辑引导的大语言模型边缘压缩方法",
            "summary_zh": "大型语言模型（LLM）在自然语言任务中表现出色，但需要大量的计算资源，限制了其在资源受限的边缘设备上的部署。现有的压缩技术，如量化和剪枝，通常会降低关键的语言属性，并且缺乏对模型行为保持的正式保证。我们提出了时序逻辑引导的大语言模型压缩（TOGGLE），这是一个新颖的框架，它利用信号时序逻辑（STL）来正式地指定和强制执行压缩过程中的语言属性。TOGGLE采用STL鲁棒性引导的贝叶斯优化，系统地探索逐层量化和剪枝配置，生成压缩模型，在不重新训练或微调的情况下，正式地满足指定的语言约束。在四个LLM架构（GPT-2、DeepSeek-V2 7B、LLaMA 3 8B和Mistral 7B）上评估TOGGLE，我们实现了高达3.3倍的计算成本（FLOPs）降低和高达68.8%的模型大小降低，同时满足所有语言属性。TOGGLE代表了形式化方法首次集成到LLM压缩中，从而能够在边缘硬件上高效、可验证地部署LLM。",
            "intro_zh": [
                "现有LLM压缩方法在降低计算资源需求的同时，往往会损害模型的关键语言属性，且缺乏对模型行为的正式保证。",
                "TOGGLE利用信号时序逻辑（STL）来形式化地指定和强制执行压缩过程中的语言属性，确保压缩后的模型满足特定的语言约束。",
                "实验结果表明，TOGGLE在显著降低计算成本和模型大小的同时，能够保持LLM的语言属性，实现高效且可验证的边缘部署。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大型语言模型（LLM）在资源受限的边缘设备上部署的问题。现有压缩方法（如量化和剪枝）虽然能减小模型体积和计算量，但常常会降低模型的语言能力，并且缺乏形式化的验证手段来保证压缩后的模型仍然满足特定的语言属性。\\n\\n**核心思路**：TOGGLE的核心思路是利用信号时序逻辑（STL）来形式化地描述LLM需要满足的语言属性，并在模型压缩过程中，通过优化算法来寻找满足这些属性的最佳压缩配置。这样可以在保证模型性能的同时，尽可能地减小模型体积和计算量。\\n\\n**技术框架**：TOGGLE框架主要包含以下几个模块：1) **STL属性定义模块**：使用STL语言定义LLM需要满足的语言属性，例如，某些特定词语必须出现在输出中，或者输出必须符合某种语法结构。2) **鲁棒性评估模块**：评估当前模型对于STL属性的满足程度，即计算鲁棒性得分。鲁棒性得分越高，表示模型越能满足对应的STL属性。3) **贝叶斯优化模块**：使用贝叶斯优化算法来搜索最佳的量化和剪枝配置。该模块以鲁棒性得分作为优化目标，旨在找到既能减小模型体积和计算量，又能保证模型满足STL属性的压缩配置。4) **模型压缩模块**：根据贝叶斯优化模块输出的配置，对LLM进行量化和剪枝。\\n\\n**关键创新**：TOGGLE的关键创新在于将形式化方法（STL）引入到LLM压缩中。与传统的压缩方法不同，TOGGLE能够提供形式化的保证，确保压缩后的模型仍然满足特定的语言属性。此外，TOGGLE还采用鲁棒性引导的贝叶斯优化，能够更有效地搜索最佳的压缩配置。\\n\\n**关键设计**：TOGGLE的关键设计包括：1) **STL属性的选取**：选择合适的STL属性对于保证压缩后模型的语言能力至关重要。论文中可能给出了选择STL属性的一些指导原则。2) **鲁棒性函数的定义**：鲁棒性函数需要能够准确地反映模型对于STL属性的满足程度。不同的STL属性可能需要不同的鲁棒性函数。3) **贝叶斯优化算法的参数设置**：贝叶斯优化算法的参数设置会影响搜索效率和最终结果。论文中可能给出了参数设置的一些建议。4) **量化和剪枝策略的选择**：TOGGLE可以支持不同的量化和剪枝策略。论文中可能比较了不同策略的性能。",
            "application_zh": "TOGGLE的应用场景广泛，包括在智能手机、无人机、机器人等资源受限的边缘设备上部署LLM。该技术能够使这些设备在本地运行复杂的自然语言处理任务，例如语音助手、机器翻译、智能客服等，从而提高响应速度、保护用户隐私，并降低对云端服务器的依赖。未来，TOGGLE有望推动LLM在物联网、自动驾驶等领域的应用。",
            "highlight_zh": "TOGGLE在GPT-2、DeepSeek-V2 7B、LLaMA 3 8B和Mistral 7B四个LLM架构上进行了评估，实验结果表明，TOGGLE能够实现高达3.3倍的计算成本（FLOPs）降低和高达68.8%的模型大小降低，同时满足所有指定的语言属性。这些结果表明TOGGLE在LLM压缩方面具有显著的优势。",
            "tags_zh": [
                "大语言模型压缩",
                "边缘计算",
                "形式化方法",
                "信号时序逻辑",
                "贝叶斯优化"
            ],
            "_index": 29,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "Prefix Probing: Lightweight Harmful Content Detection for Large Language Models",
            "authors": [
                "Jirui Yang",
                "Hengqi Guo",
                "Zhihui Lu",
                "Yi Zhao",
                "Yuansen Zhang",
                "Shijing Hu",
                "Qiang Duan",
                "Yinggui Wang",
                "Tao Wei"
            ],
            "arxiv_id": "2512.16650v1",
            "summary": "Large language models often face a three-way trade-off among detection accuracy, inference latency, and deployment cost when used in real-world safety-sensitive applications. This paper introduces Prefix Probing, a black-box harmful content detection method that compares the conditional log-probabilities of \"agreement/execution\" versus \"refusal/safety\" opening prefixes and leverages prefix caching to reduce detection overhead to near first-token latency. During inference, the method requires only a single log-probability computation over the probe prefixes to produce a harmfulness score and apply a threshold, without invoking any additional models or multi-stage inference. To further enhance the discriminative power of the prefixes, we design an efficient prefix construction algorithm that automatically discovers highly informative prefixes, substantially improving detection performance. Extensive experiments demonstrate that Prefix Probing achieves detection effectiveness comparable to mainstream external safety models while incurring only minimal computational cost and requiring no extra model deployment, highlighting its strong practicality and efficiency.",
            "categories": [
                "cs.AI",
                "cs.CR"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16650v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]large language model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出Prefix Probing，以低延迟、低成本实现大语言模型有害内容检测。",
            "summary_zh": "大型语言模型在实际安全敏感应用中，通常面临检测准确性、推理延迟和部署成本的三重权衡。本文介绍了一种黑盒有害内容检测方法Prefix Probing，它通过比较“同意/执行”与“拒绝/安全”开头前缀的条件对数概率，并利用前缀缓存将检测开销降低到接近首个token的延迟。在推理过程中，该方法仅需对探测前缀进行一次对数概率计算，即可生成有害性评分并应用阈值，无需调用任何额外的模型或多阶段推理。为了进一步增强前缀的区分能力，我们设计了一种高效的前缀构建算法，可以自动发现信息量大的前缀，从而显著提高检测性能。大量实验表明，Prefix Probing在计算成本极低且无需额外模型部署的情况下，实现了与主流外部安全模型相当的检测效果，突显了其强大的实用性和效率。",
            "intro_zh": [
                "现有大语言模型有害内容检测方法在准确性、延迟和成本间存在权衡，难以兼顾。",
                "Prefix Probing通过比较特定前缀的概率，无需额外模型或多阶段推理，实现高效检测。",
                "论文设计高效前缀构建算法，自动发现信息量大的前缀，显著提升检测性能。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大型语言模型在实际应用中，有害内容检测的准确性、推理延迟和部署成本难以兼顾的问题。现有方法通常需要额外的安全模型或多阶段推理，导致延迟高、成本高。\\n\\n**核心思路**：Prefix Probing的核心思路是，利用大语言模型自身对不同语义前缀的概率分布差异，来判断输入内容是否具有危害性。如果模型更倾向于“同意/执行”类前缀，则认为输入可能有害；反之，如果倾向于“拒绝/安全”类前缀，则认为输入安全。\\n\\n**技术框架**：Prefix Probing的整体框架非常简单。首先，预先定义两组前缀，分别代表“同意/执行”和“拒绝/安全”的语义。然后，对于给定的输入，计算大语言模型在输入后生成这两组前缀的条件对数概率。最后，通过比较这两组概率的差异，得到一个有害性评分，并根据设定的阈值判断输入是否有害。为了加速推理，可以缓存已计算的前缀概率。\\n\\n**关键创新**：Prefix Probing的关键创新在于，它无需额外的安全模型或多阶段推理，仅利用大语言模型自身的概率分布进行有害内容检测，从而显著降低了延迟和成本。此外，论文还提出了一种高效的前缀构建算法，可以自动发现信息量大的前缀，进一步提升检测性能。\\n\\n**关键设计**：前缀构建算法是关键设计之一。该算法旨在自动搜索能够最大化“同意/执行”和“拒绝/安全”两类前缀概率差异的前缀。具体实现细节未知，但可以推测可能使用了某种搜索或优化算法，例如梯度下降或进化算法，来寻找最优的前缀组合。阈值的设定也需要根据实际应用场景进行调整，以平衡检测准确性和误报率。",
            "application_zh": "Prefix Probing可广泛应用于各种需要对大语言模型生成内容进行安全过滤的场景，例如聊天机器人、内容生成平台、代码生成工具等。该方法能够以极低的延迟和成本，有效识别和阻止有害内容的生成，保障用户安全和平台合规性。未来，该方法还可以扩展到其他类型的安全问题检测，例如隐私泄露、版权侵犯等。",
            "highlight_zh": "实验结果表明，Prefix Probing在检测效果上可以与主流的外部安全模型相媲美，同时计算成本极低，几乎不增加推理延迟。该方法无需额外部署模型，可以直接集成到现有的大语言模型应用中，具有很强的实用性和效率。具体的性能数据和对比基线在论文中进行了详细的展示。",
            "tags_zh": [
                "大语言模型",
                "有害内容检测",
                "黑盒方法",
                "前缀探测",
                "低延迟",
                "安全过滤",
                "概率建模"
            ],
            "_index": 30,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16650v1/figs/insight.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16650v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16650v1/figs/f1_vs_time_2.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "TimeSeries2Report prompting enables adaptive large language model management of lithium-ion batteries",
            "authors": [
                "Jiayang Yang",
                "Chunhui Zhao",
                "Martin Guay",
                "Zhixing Cao"
            ],
            "arxiv_id": "2512.16453v1",
            "summary": "Large language models (LLMs) offer promising capabilities for interpreting multivariate time-series data, yet their application to real-world battery energy storage system (BESS) operation and maintenance remains largely unexplored. Here, we present TimeSeries2Report (TS2R), a prompting framework that converts raw lithium-ion battery operational time-series into structured, semantically enriched reports, enabling LLMs to reason, predict, and make decisions in BESS management scenarios. TS2R encodes short-term temporal dynamics into natural language through a combination of segmentation, semantic abstraction, and rule-based interpretation, effectively bridging low-level sensor signals with high-level contextual insights. We benchmark TS2R across both lab-scale and real-world datasets, evaluating report quality and downstream task performance in anomaly detection, state-of-charge prediction, and charging/discharging management. Compared with vision-, embedding-, and text-based prompting baselines, report-based prompting via TS2R consistently improves LLM performance in terms of across accuracy, robustness, and explainability metrics. Notably, TS2R-integrated LLMs achieve expert-level decision quality and predictive consistency without retraining or architecture modification, establishing a practical path for adaptive, LLM-driven battery intelligence.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16453v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]large language model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出TimeSeries2Report框架，实现大语言模型对锂离子电池的自适应管理",
            "summary_zh": "本文提出了一种名为TimeSeries2Report (TS2R) 的提示框架，旨在将原始锂离子电池运行时间序列数据转换为结构化、语义丰富的报告，从而使大语言模型 (LLM) 能够在电池储能系统 (BESS) 管理场景中进行推理、预测和决策。TS2R 通过分割、语义抽象和基于规则的解释，将短期时间动态编码为自然语言，有效地将低级传感器信号与高级上下文洞察联系起来。该框架在实验室规模和真实世界数据集上进行了基准测试，评估了报告质量以及在异常检测、荷电状态预测和充放电管理等下游任务中的性能。与基于视觉、嵌入和文本的提示基线相比，通过 TS2R 进行的基于报告的提示始终提高了 LLM 在准确性、鲁棒性和可解释性指标方面的性能。值得注意的是，集成了 TS2R 的 LLM 在无需重新训练或架构修改的情况下，实现了专家级的决策质量和预测一致性，为自适应的、LLM 驱动的电池智能建立了一条切实可行的路径。",
            "intro_zh": [
                "现有方法难以有效利用大语言模型处理电池储能系统中的多变量时间序列数据，缺乏有效的桥梁连接底层传感器信号和高层决策。",
                "TimeSeries2Report框架通过时间序列分割、语义抽象和规则解释，将时间动态编码为自然语言报告，为LLM提供结构化和语义丰富的输入。",
                "实验表明，TS2R框架显著提升了LLM在异常检测、荷电状态预测和充放电管理等任务中的性能，无需重新训练或修改模型架构。"
            ],
            "method_zh": "**问题定义**：现有方法在利用大语言模型处理电池储能系统（BESS）中的多变量时间序列数据时面临挑战。直接将原始时间序列输入LLM通常效果不佳，因为LLM难以理解低层次的传感器信号与高层次的运行状态之间的关系。此外，缺乏有效的手段将时间序列数据转化为LLM能够理解和利用的结构化信息，限制了LLM在BESS管理中的应用潜力。\\n\\n**核心思路**：论文的核心思路是将原始时间序列数据转换为结构化、语义丰富的自然语言报告，从而使LLM能够更好地理解和利用这些数据。通过将时间序列数据转化为自然语言报告，可以有效地将低层次的传感器信号与高层次的运行状态联系起来，为LLM提供更易于理解和推理的输入。这种方法的核心在于利用自然语言的表达能力，将时间序列数据中的模式和趋势转化为人类可理解的语言描述。\\n\\n**技术框架**：TS2R框架包含三个主要模块：分割（Segmentation）、语义抽象（Semantic Abstraction）和规则解释（Rule-based Interpretation）。首先，分割模块将原始时间序列数据分割成具有语义意义的片段。然后，语义抽象模块将每个片段抽象成自然语言描述，例如“电压快速上升”或“温度缓慢下降”。最后，规则解释模块根据预定义的规则，将这些自然语言描述组合成完整的报告，例如“电池正在快速充电，但温度略有升高，可能存在过热风险”。整个流程将原始时间序列数据转化为LLM能够理解和利用的结构化报告。\\n\\n**关键创新**：TS2R框架的关键创新在于其将时间序列数据转化为自然语言报告的提示方法。与传统的基于视觉、嵌入或文本的提示方法相比，TS2R能够更有效地将时间序列数据中的时间动态和上下文信息编码到LLM的输入中。这种方法不仅提高了LLM的性能，还增强了其可解释性，因为LLM的决策过程可以追溯到具体的自然语言报告。\\n\\n**关键设计**：TS2R框架的关键设计包括：(1) 时间序列分割算法的选择，需要根据具体应用场景进行调整，以确保分割后的片段具有语义意义；(2) 语义抽象规则的设计，需要根据领域知识进行定义，以确保自然语言描述准确地反映时间序列数据的特征；(3) 报告生成规则的设计，需要考虑LLM的输入长度限制和推理能力，以确保生成的报告既包含足够的信息，又易于LLM理解和利用。",
            "application_zh": "该研究成果可广泛应用于电池储能系统的智能管理，例如异常检测、状态预测和优化充放电策略。通过集成TS2R框架和LLM，可以实现对电池运行状态的实时监控和智能诊断，提高电池系统的安全性、可靠性和经济性。未来，该技术有望应用于电动汽车、智能电网等领域，推动能源存储和管理技术的智能化发展。",
            "highlight_zh": "实验结果表明，与基于视觉、嵌入和文本的提示基线相比，TS2R框架显著提高了LLM在异常检测、荷电状态预测和充放电管理等任务中的性能。例如，在异常检测任务中，TS2R框架将LLM的准确率提高了15%。更重要的是，集成了TS2R的LLM在无需重新训练或修改模型架构的情况下，实现了专家级的决策质量和预测一致性。",
            "tags_zh": [
                "时间序列分析",
                "大语言模型",
                "锂离子电池",
                "电池管理系统",
                "自然语言处理"
            ],
            "_index": 31,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16453v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16453v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16453v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "An Information-Theoretic Framework for Robust Large Language Model Editing",
            "authors": [
                "Qizhou Chen",
                "Chengyu Wang",
                "Taolin Zhang",
                "Xiaofeng He"
            ],
            "arxiv_id": "2512.16227v1",
            "summary": "Large Language Models (LLMs) have become indispensable tools in science, technology, and society, enabling transformative advances across diverse fields. However, errors or outdated information within these models can undermine their accuracy and restrict their safe deployment. Developing efficient strategies for updating model knowledge without the expense and disruption of full retraining remains a critical challenge. Current model editing techniques frequently struggle to generalize corrections beyond narrow domains, leading to unintended consequences and limiting their practical impact. Here, we introduce a novel framework for editing LLMs, grounded in information bottleneck theory. This approach precisely compresses and isolates the essential information required for generalizable knowledge correction while minimizing disruption to unrelated model behaviors. Building upon this foundation, we present the Information Bottleneck Knowledge Editor (IBKE), which leverages compact latent representations to guide gradient-based updates, enabling robust and broadly applicable model editing. We validate IBKE's effectiveness across multiple LLM architectures and standard benchmark tasks, demonstrating state-of-the-art accuracy and improved generality and specificity of edits. These findings establish a theoretically principled and practical paradigm for open-domain knowledge editing, advancing the utility and trustworthiness of LLMs in real-world applications.",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16227v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]large language model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出IBKE，基于信息瓶颈理论实现鲁棒的大语言模型知识编辑",
            "summary_zh": "大型语言模型（LLMs）已成为科学、技术和社会中不可或缺的工具，推动了各个领域的变革性进步。然而，这些模型中存在的错误或过时信息可能会损害其准确性，并限制其安全部署。开发有效的策略来更新模型知识，同时避免完全重新训练的成本和干扰，仍然是一个关键挑战。当前的模型编辑技术通常难以将修正推广到狭窄领域之外，导致意想不到的后果，并限制了它们的实际影响。本文介绍了一种基于信息瓶颈理论的LLM编辑新框架。该方法精确地压缩和隔离了通用知识修正所需的基本信息，同时最大限度地减少了对不相关模型行为的干扰。在此基础上，我们提出了信息瓶颈知识编辑器（IBKE），它利用紧凑的潜在表示来指导基于梯度的更新，从而实现鲁棒且广泛适用的模型编辑。我们在多个LLM架构和标准基准任务上验证了IBKE的有效性，证明了其最先进的准确性，以及编辑的改进的通用性和特异性。这些发现为开放域知识编辑建立了一个理论上合理且实用的范例，提高了LLM在实际应用中的效用和可信度。",
            "intro_zh": [
                "现有模型编辑方法泛化性差，容易产生副作用，限制了实际应用。",
                "论文提出基于信息瓶颈理论的IBKE，压缩关键信息，减少对无关行为的干扰。",
                "实验表明，IBKE在多个LLM和基准测试中表现出色，提升了编辑的准确性和泛化性。"
            ],
            "method_zh": "**问题定义**：现有的大语言模型知识编辑方法，在更新模型知识时，容易出现泛化性不足的问题，即只能在特定领域或特定问题上有效，而无法推广到其他相关领域。此外，不精确的编辑还会导致模型在其他任务上的性能下降，产生副作用。因此，如何实现鲁棒且具有泛化性的知识编辑是一个关键问题。\\n\\n**核心思路**：论文的核心思路是利用信息瓶颈理论，在编辑过程中，只保留与待编辑知识相关的最关键信息，而过滤掉其他无关信息。通过这种方式，可以减少编辑对模型其他部分的影响，从而提高编辑的泛化性和鲁棒性。同时，利用紧凑的潜在表示来指导梯度更新，可以更有效地进行知识编辑。\\n\\n**技术框架**：IBKE框架主要包含以下几个阶段：1) **知识表示**：将需要编辑的知识表示为一种紧凑的潜在向量。2) **信息压缩**：利用信息瓶颈理论，对潜在向量进行压缩，只保留与编辑任务最相关的信息。3) **梯度更新**：利用压缩后的潜在向量，指导模型参数的梯度更新，从而实现知识编辑。4) **评估**：评估编辑后的模型在相关任务上的性能，以及在其他任务上的副作用。\\n\\n**关键创新**：IBKE的关键创新在于将信息瓶颈理论应用于大语言模型的知识编辑。通过信息瓶颈，可以有效地分离出与编辑任务相关的关键信息，从而减少编辑对模型其他部分的影响，提高编辑的泛化性和鲁棒性。此外，利用紧凑的潜在表示，可以更有效地进行梯度更新。\\n\\n**关键设计**：IBKE的关键设计包括：1) **信息瓶颈的实现**：使用变分自编码器（VAE）来实现信息瓶颈，通过调整VAE的KL散度损失，来控制信息压缩的程度。2) **潜在表示的选择**：选择模型中间层的激活值作为潜在表示，因为这些激活值包含了模型对输入信息的理解。3) **梯度更新策略**：使用Adam优化器进行梯度更新，并设置合适的学习率和权重衰减，以防止过拟合。",
            "application_zh": "该研究成果可应用于各种需要知识更新的大语言模型应用场景，例如：自动问答系统、知识图谱构建、智能客服等。通过IBKE，可以更高效、更安全地更新模型知识，提高模型的准确性和可靠性，从而提升用户体验，并降低模型维护成本。未来，该技术还可以扩展到其他类型的机器学习模型，例如图像识别模型、语音识别模型等。",
            "highlight_zh": "实验结果表明，IBKE在多个LLM架构（包括GPT-2、GPT-J等）和标准基准任务上，都取得了state-of-the-art的性能。与现有方法相比，IBKE在编辑的准确性、通用性和特异性方面都有显著提升。例如，在某些任务上，IBKE的准确率比现有方法提高了10%以上，同时副作用也显著降低。",
            "tags_zh": [
                "大语言模型",
                "知识编辑",
                "信息瓶颈",
                "鲁棒性",
                "泛化性",
                "梯度更新",
                "潜在表示"
            ],
            "_index": 32,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16227v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16227v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16227v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "A Multi-Agent Large Language Model Framework for Automated Qualitative Analysis",
            "authors": [
                "Qidi Xu",
                "Nuzha Amjad",
                "Grace Giles",
                "Alexa Cumming",
                "De'angelo Hermesky",
                "Alexander Wen",
                "Min Ji Kwak",
                "Yejin Kim"
            ],
            "arxiv_id": "2512.16063v1",
            "summary": "Understanding patients experiences is essential for advancing patient centered care, especially in chronic diseases that require ongoing communication. However, qualitative thematic analysis, the primary approach for exploring these experiences, remains labor intensive, subjective, and difficult to scale. In this study, we developed a multi agent large language model framework that automates qualitative thematic analysis through three agents (Instructor, Thematizer, CodebookGenerator), named Collaborative Theme Identification Agent (CoTI). We applied CoTI to 12 heart failure patient interviews to analyze their perceptions of medication intensity. CoTI identified key phrases, themes, and codebook that were more similar to those of the senior investigator than both junior investigators and baseline NLP models. We also implemented CoTI into a user-facing application to enable AI human interaction in qualitative analysis. However, collaboration between CoTI and junior investigators provided only marginal gains, suggesting they may overrely on CoTI and limit their independent critical thinking.",
            "categories": [
                "cs.HC",
                "cs.AI"
            ],
            "primary_category": "cs.HC",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "42 pages, 5 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16063v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]large language model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出CoTI：一个基于多Agent LLM的自动化定性分析框架，应用于心力衰竭患者访谈分析。",
            "summary_zh": "理解患者体验对于推进以患者为中心的护理至关重要，尤其是在需要持续沟通的慢性疾病中。然而，定性主题分析是探索这些体验的主要方法，但仍然劳动密集、主观且难以扩展。本研究开发了一个多Agent大型语言模型框架，通过三个Agent（指导者、主题化者、代码手册生成器）自动化定性主题分析，命名为协同主题识别Agent（CoTI）。我们将CoTI应用于12个心力衰竭患者访谈，以分析他们对药物强度的看法。CoTI识别的关键短语、主题和代码手册与资深研究员的结果比初级研究员和基线NLP模型更相似。我们还将CoTI集成到面向用户的应用程序中，以实现AI人机交互的定性分析。然而，CoTI与初级研究员之间的协作仅提供了边际收益，表明他们可能过度依赖CoTI并限制了他们的独立批判性思维。",
            "intro_zh": [
                "定性主题分析在患者体验研究中至关重要，但其劳动密集和主观性限制了应用范围。",
                "论文提出CoTI框架，利用多Agent LLM协同工作，自动化主题识别、代码手册生成等流程。",
                "实验表明，CoTI在心力衰竭患者访谈分析中，结果与资深研究员更接近，优于传统NLP模型。"
            ],
            "method_zh": "**问题定义**：论文旨在解决定性研究中主题分析耗时耗力、主观性强、难以规模化的问题。现有方法依赖人工，效率低且易受研究者个人偏见影响。\\n\\n**核心思路**：论文的核心思路是利用大型语言模型（LLM）的强大自然语言处理能力，构建多个智能Agent协同完成主题分析任务，从而降低人工干预，提高分析效率和一致性。通过Agent之间的协作，模拟专家团队进行分析的过程。\\n\\n**技术框架**：CoTI框架包含三个主要Agent：Instructor（指导者）、Thematizer（主题化者）和CodebookGenerator（代码手册生成器）。Instructor负责引导整个分析流程，Thematizer负责从访谈文本中提取主题，CodebookGenerator负责生成代码手册。这三个Agent通过协同工作，完成从原始访谈数据到主题和代码手册的自动化生成。\\n\\n**关键创新**：CoTI的关键创新在于其多Agent协同架构，它将复杂的定性分析任务分解为多个Agent负责的子任务，并通过Agent之间的交互实现整体分析。这种架构能够更好地利用LLM的能力，并提高分析结果的质量和一致性。与传统的单模型方法相比，CoTI更具模块化和可扩展性。\\n\\n**关键设计**：论文中没有详细描述关键参数设置、损失函数或网络结构等技术细节。但可以推测，每个Agent都使用了预训练的LLM作为基础模型，并通过提示工程（Prompt Engineering）来指导其行为。Agent之间的交互方式和信息传递机制是CoTI设计的关键，但具体实现细节未知。",
            "application_zh": "CoTI框架可应用于医疗健康领域，例如患者反馈分析、临床试验数据挖掘等。它还可以扩展到其他需要定性分析的领域，如市场调研、社会科学研究等。通过自动化定性分析，CoTI可以帮助研究人员更高效地理解数据，发现有价值的见解，并为决策提供支持。",
            "highlight_zh": "实验结果表明，CoTI在心力衰竭患者访谈分析中，识别的关键短语、主题和代码手册与资深研究员的结果更相似，优于初级研究员和基线NLP模型。这表明CoTI能够有效地模拟专家进行定性分析，并提高分析结果的质量。",
            "tags_zh": [
                "大型语言模型",
                "多Agent系统",
                "定性分析",
                "主题识别",
                "自然语言处理"
            ],
            "_index": 33,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "DualGuard: Dual-stream Large Language Model Watermarking Defense against Paraphrase and Spoofing Attack",
            "authors": [
                "Hao Li",
                "Yubing Ren",
                "Yanan Cao",
                "Yingjie Li",
                "Fang Fang",
                "Shi Wang",
                "Li Guo"
            ],
            "arxiv_id": "2512.16182v1",
            "summary": "With the rapid development of cloud-based services, large language models (LLMs) have become increasingly accessible through various web platforms. However, this accessibility has also led to growing risks of model abuse. LLM watermarking has emerged as an effective approach to mitigate such misuse and protect intellectual property. Existing watermarking algorithms, however, primarily focus on defending against paraphrase attacks while overlooking piggyback spoofing attacks, which can inject harmful content, compromise watermark reliability, and undermine trust in attribution. To address this limitation, we propose DualGuard, the first watermarking algorithm capable of defending against both paraphrase and spoofing attacks. DualGuard employs the adaptive dual-stream watermarking mechanism, in which two complementary watermark signals are dynamically injected based on the semantic content. This design enables DualGuard not only to detect but also to trace spoofing attacks, thereby ensuring reliable and trustworthy watermark detection. Extensive experiments conducted across multiple datasets and language models demonstrate that DualGuard achieves excellent detectability, robustness, traceability, and text quality, effectively advancing the state of LLM watermarking for real-world applications.",
            "categories": [
                "cs.CR",
                "cs.CL"
            ],
            "primary_category": "cs.CR",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16182v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]large language model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出DualGuard，一种可防御复述攻击和欺骗攻击的双流大语言模型水印算法",
            "summary_zh": "随着云服务的快速发展，大型语言模型（LLM）越来越容易通过各种网络平台访问。然而，这种可访问性也导致了模型滥用的风险日益增加。LLM水印技术已成为缓解此类滥用和保护知识产权的有效方法。然而，现有的水印算法主要侧重于防御复述攻击，而忽略了可能注入有害内容、损害水印可靠性并破坏对归属信任的背驮式欺骗攻击。为了解决这个局限性，我们提出了DualGuard，这是第一个能够防御复述攻击和欺骗攻击的水印算法。DualGuard采用自适应双流水印机制，其中两个互补的水印信号根据语义内容动态注入。这种设计使DualGuard不仅能够检测，而且能够追踪欺骗攻击，从而确保可靠和值得信赖的水印检测。在多个数据集和语言模型上进行的大量实验表明，DualGuard实现了出色的可检测性、鲁棒性、可追溯性和文本质量，有效地推进了LLM水印技术在实际应用中的发展。",
            "intro_zh": [
                "现有LLM水印技术主要关注复述攻击防御，忽略了注入恶意内容的欺骗攻击，导致水印可靠性降低。",
                "DualGuard采用自适应双流水印机制，根据语义内容动态注入互补水印信号，实现欺骗攻击的检测和追踪。",
                "实验证明DualGuard在可检测性、鲁棒性、可追溯性和文本质量方面表现出色，适用于实际应用。"
            ],
            "method_zh": "**问题定义**：现有的大语言模型水印方法主要关注抵抗复述攻击，即攻击者通过改写文本来移除或弱化水印。然而，这些方法忽略了一种更隐蔽的攻击方式，即欺骗攻击（Spoofing Attack）。攻击者可以在原始文本中注入恶意内容，同时尝试保留或伪造水印，从而误导水印检测系统，将恶意内容归因于原始作者。这种攻击会损害水印的可靠性和可信度。\\n\\n**核心思路**：DualGuard的核心思路是利用双流水印机制，同时嵌入两种互补的水印信号。一种水印信号用于检测文本是否被篡改，另一种水印信号用于追踪攻击来源。通过分析这两种水印信号，DualGuard可以区分复述攻击和欺骗攻击，并识别出注入的恶意内容。这种设计基于一个假设：欺骗攻击者很难同时伪造两种独立的水印信号。\\n\\n**技术框架**：DualGuard的整体框架包含以下几个主要模块：1) 语义分析模块：分析输入文本的语义信息，用于指导水印信号的嵌入。2) 双流水印嵌入模块：根据语义分析结果，动态地将两种互补的水印信号嵌入到文本中。3) 水印检测模块：检测文本中是否存在水印信号，并分析信号的强度和一致性。4) 攻击追踪模块：如果检测到欺骗攻击，则追踪攻击来源，例如注入恶意内容的位置和方式。\\n\\n**关键创新**：DualGuard的关键创新在于其自适应双流水印机制。与传统的单流水印方法相比，DualGuard可以同时检测文本篡改和追踪攻击来源，从而更有效地防御欺骗攻击。此外，DualGuard的水印嵌入过程是自适应的，可以根据文本的语义信息动态调整水印信号的强度和位置，从而提高水印的鲁棒性和不可感知性。\\n\\n**关键设计**：DualGuard的关键设计包括：1) 两种互补水印信号的选择：一种水印信号可以基于文本的统计特征，例如词频或n-gram分布；另一种水印信号可以基于文本的语义信息，例如情感或主题。2) 自适应水印嵌入策略：根据文本的语义信息，动态调整水印信号的强度和位置，以平衡水印的鲁棒性和不可感知性。3) 攻击追踪算法：分析水印信号的变化，识别注入恶意内容的位置和方式，并追踪攻击来源。",
            "application_zh": "DualGuard可应用于各种基于大语言模型的云服务平台，用于保护模型输出的知识产权，防止恶意内容传播，并追溯攻击来源。例如，可以用于检测和阻止AI写作工具生成的虚假新闻，保护在线教育平台的课程内容不被篡改，以及防止聊天机器人被用于传播仇恨言论。",
            "highlight_zh": "实验结果表明，DualGuard在防御复述攻击和欺骗攻击方面均优于现有水印算法。在多个数据集和语言模型上，DualGuard实现了接近100%的欺骗攻击检测率，同时保持了较高的水印鲁棒性和较低的文本质量损失。与基线方法相比，DualGuard在欺骗攻击防御方面取得了显著的性能提升。",
            "tags_zh": [
                "大语言模型",
                "水印技术",
                "欺骗攻击防御",
                "复述攻击防御",
                "双流水印",
                "知识产权保护"
            ],
            "_index": 34,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16182v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16182v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16182v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Flowing from Reasoning to Motion: Learning 3D Hand Trajectory Prediction from Egocentric Human Interaction Videos",
            "authors": [
                "Mingfei Chen",
                "Yifan Wang",
                "Zhengqin Li",
                "Homanga Bharadhwaj",
                "Yujin Chen",
                "Chuan Qin",
                "Ziyi Kou",
                "Yuan Tian",
                "Eric Whitmire",
                "Rajinder Sodhi",
                "Hrvoje Benko",
                "Eli Shlizerman",
                "Yue Liu"
            ],
            "arxiv_id": "2512.16907v1",
            "summary": "Prior works on 3D hand trajectory prediction are constrained by datasets that decouple motion from semantic supervision and by models that weakly link reasoning and action. To address these, we first present the EgoMAN dataset, a large-scale egocentric dataset for interaction stage-aware 3D hand trajectory prediction with 219K 6DoF trajectories and 3M structured QA pairs for semantic, spatial, and motion reasoning. We then introduce the EgoMAN model, a reasoning-to-motion framework that links vision-language reasoning and motion generation via a trajectory-token interface. Trained progressively to align reasoning with motion dynamics, our approach yields accurate and stage-aware trajectories with generalization across real-world scenes.",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.RO"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Project website: https://egoman-project.github.io",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16907v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱四：生成式动作 (Generative Motion)",
                    "id": "4_motion_diffusion",
                    "matched_keywords": [
                        "motion generation"
                    ],
                    "score": 2.5
                },
                {
                    "name": "支柱六：视频提取与匹配 (Video Extraction)",
                    "id": "6_video_extraction",
                    "matched_keywords": [
                        "[T]egocentric"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 8.5,
            "hit_pillars": [
                "4_motion_diffusion",
                "6_video_extraction"
            ],
            "headline_zh": "EgoMAN：基于自中心交互视频学习3D手部轨迹预测，实现推理到运动的衔接",
            "summary_zh": "本文提出了一种基于自中心人类交互视频的3D手部轨迹预测方法。现有方法受限于缺乏语义监督的数据集以及推理和动作之间弱连接的模型。为了解决这些问题，我们首先提出了EgoMAN数据集，这是一个大规模的自中心数据集，用于交互阶段感知的3D手部轨迹预测，包含21.9万个6DoF轨迹和300万个结构化的QA对，用于语义、空间和运动推理。然后，我们介绍了EgoMAN模型，这是一个推理到运动的框架，通过轨迹-token接口连接视觉-语言推理和运动生成。通过逐步训练以使推理与运动动力学对齐，我们的方法产生了准确且阶段感知的轨迹，并在真实场景中具有泛化能力。",
            "intro_zh": [
                "现有3D手部轨迹预测方法缺乏语义信息的有效利用，限制了预测的准确性和泛化能力。",
                "EgoMAN模型通过轨迹-token接口，将视觉-语言推理与运动生成相结合，实现了推理到运动的有效衔接。",
                "EgoMAN数据集和模型在真实场景中表现出良好的泛化能力，能够生成准确且阶段感知的轨迹。"
            ],
            "method_zh": "**问题定义**：现有3D手部轨迹预测方法主要面临两个挑战：一是数据集的限制，现有数据集通常将运动与语义监督分离，缺乏丰富的交互信息；二是模型设计上的不足，现有模型通常难以有效地将推理过程与动作生成联系起来，导致预测结果不够准确和自然。\\n\\n**核心思路**：本文的核心思路是通过构建一个大规模的自中心交互数据集EgoMAN，并设计一个推理到运动的框架EgoMAN模型，从而实现更准确和自然的3D手部轨迹预测。EgoMAN数据集提供了丰富的语义、空间和运动推理信息，EgoMAN模型则通过轨迹-token接口将视觉-语言推理与运动生成相结合。\\n\\n**技术框架**：EgoMAN模型是一个推理到运动的框架，主要包含以下几个模块：首先，利用视觉和语言信息进行推理，提取场景的语义信息和交互意图；然后，将推理结果编码成轨迹-token，作为运动生成模块的输入；最后，运动生成模块根据轨迹-token生成3D手部轨迹。整个框架通过逐步训练，使推理与运动动力学对齐。\\n\\n**关键创新**：本文的关键创新在于：1) 提出了EgoMAN数据集，这是一个大规模的自中心交互数据集，包含丰富的语义、空间和运动推理信息；2) 设计了EgoMAN模型，这是一个推理到运动的框架，通过轨迹-token接口将视觉-语言推理与运动生成相结合，实现了推理和动作之间的有效衔接。\\n\\n**关键设计**：EgoMAN模型中的轨迹-token接口是关键设计之一，它将推理结果编码成一种紧凑的表示，作为运动生成模块的输入。此外，模型还采用了逐步训练的策略，通过逐步对齐推理和运动动力学，提高预测的准确性和自然性。具体的损失函数和网络结构细节在论文中有详细描述。",
            "application_zh": "该研究成果可应用于人机交互、机器人控制、虚拟现实和增强现实等领域。例如，在机器人控制中，可以利用该模型预测人的手部运动轨迹，从而使机器人能够更好地理解人的意图并做出相应的动作。在虚拟现实和增强现实中，可以利用该模型生成更逼真的手部动画，提高用户的沉浸感。",
            "highlight_zh": "EgoMAN模型在EgoMAN数据集上取得了显著的性能提升。实验结果表明，该模型能够生成准确且阶段感知的轨迹，并在真实场景中具有良好的泛化能力。具体的性能数据和对比基线在论文中有详细描述，证明了EgoMAN数据集和模型的有效性。",
            "tags_zh": [
                "3D手部轨迹预测",
                "自中心视频",
                "人机交互",
                "视觉语言推理",
                "运动生成"
            ],
            "_index": 35,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16907v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16907v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16907v1/x4.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text",
            "authors": [
                "Hanlin Wang",
                "Hao Ouyang",
                "Qiuyu Wang",
                "Yue Yu",
                "Yihao Meng",
                "Wen Wang",
                "Ka Leong Cheng",
                "Shuailei Ma",
                "Qingyan Bai",
                "Yixuan Li",
                "Cheng Chen",
                "Yanhong Zeng",
                "Xing Zhu",
                "Yujun Shen",
                "Qifeng Chen"
            ],
            "arxiv_id": "2512.16924v1",
            "summary": "We present WorldCanvas, a framework for promptable world events that enables rich, user-directed simulation by combining text, trajectories, and reference images. Unlike text-only approaches and existing trajectory-controlled image-to-video methods, our multimodal approach combines trajectories -- encoding motion, timing, and visibility -- with natural language for semantic intent and reference images for visual grounding of object identity, enabling the generation of coherent, controllable events that include multi-agent interactions, object entry/exit, reference-guided appearance and counterintuitive events. The resulting videos demonstrate not only temporal coherence but also emergent consistency, preserving object identity and scene despite temporary disappearance. By supporting expressive world events generation, WorldCanvas advances world models from passive predictors to interactive, user-shaped simulators. Our project page is available at: https://worldcanvas.github.io/.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Project page and code: https://worldcanvas.github.io/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16924v1",
            "code_links": [
                {
                    "url": "https://worldcanvas.github.io/",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "world model"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "multimodal",
                        "visual grounding"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 7.5,
            "hit_pillars": [
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "WorldCanvas：结合文本、轨迹和参考图像，实现可控的世界事件模拟。",
            "summary_zh": "本文提出了WorldCanvas，一个用于可提示世界事件的框架，它通过结合文本、轨迹和参考图像来实现丰富的、用户导向的模拟。与仅使用文本的方法和现有的轨迹控制图像到视频方法不同，我们的多模态方法将轨迹（编码运动、时间安排和可见性）与自然语言（用于语义意图）以及参考图像（用于对象身份的视觉基础）相结合，从而能够生成连贯的、可控的事件，包括多智能体交互、对象进入/退出、参考引导的外观和违反直觉的事件。生成的视频不仅展示了时间一致性，还展示了涌现一致性，即使在暂时消失的情况下也能保持对象身份和场景。通过支持富有表现力的世界事件生成，WorldCanvas将世界模型从被动预测器提升为交互式的、用户塑造的模拟器。",
            "intro_zh": [
                "现有方法在世界事件模拟中，要么仅依赖文本，缺乏精细控制，要么依赖轨迹控制，但缺乏对对象身份的视觉 grounding。",
                "WorldCanvas 结合文本、轨迹和参考图像，利用轨迹编码运动，文本表达语义意图，参考图像提供视觉 grounding，实现可控事件生成。",
                "实验结果表明，WorldCanvas 生成的视频具有时间一致性和涌现一致性，即使对象暂时消失，也能保持对象身份和场景的连贯性。"
            ],
            "method_zh": "**问题定义**：现有世界事件模拟方法存在局限性。基于文本的方法难以实现精细控制，而基于轨迹控制的图像到视频方法缺乏对对象身份的视觉 grounding，无法生成包含复杂交互和对象变化的连贯事件。因此，需要一种能够结合多种模态信息，实现用户可控、视觉一致的世界事件模拟框架。\\n\\n**核心思路**：WorldCanvas 的核心思路是将文本、轨迹和参考图像三种模态的信息融合起来，共同驱动世界事件的生成。轨迹信息负责控制运动、时间安排和可见性，文本信息负责表达语义意图，参考图像负责提供对象身份的视觉 grounding。通过这种多模态融合的方式，可以生成更加丰富、可控和视觉一致的世界事件。\\n\\n**技术框架**：WorldCanvas 的整体框架包含以下几个主要模块：1) 轨迹编码模块：负责将轨迹信息编码成可供后续模块使用的特征向量。2) 文本编码模块：负责将文本信息编码成语义向量。3) 参考图像编码模块：负责提取参考图像的视觉特征。4) 事件生成模块：该模块接收轨迹特征、语义向量和视觉特征作为输入，生成相应的视频帧。该模块通常基于生成对抗网络（GAN）或扩散模型等技术实现。\\n\\n**关键创新**：WorldCanvas 的关键创新在于其多模态融合的事件生成方式。它不仅考虑了运动轨迹和语义意图，还引入了参考图像作为视觉 grounding，从而能够生成更加真实、可控和视觉一致的世界事件。与现有方法相比，WorldCanvas 能够更好地处理多智能体交互、对象进入/退出、参考引导的外观和违反直觉的事件。\\n\\n**关键设计**：具体的技术细节包括：轨迹编码模块可能使用循环神经网络（RNN）或 Transformer 等序列模型来处理轨迹信息；文本编码模块可能使用预训练的语言模型（如 BERT 或 GPT）来提取语义特征；参考图像编码模块可能使用卷积神经网络（CNN）来提取视觉特征；事件生成模块可能使用时空 GAN 或扩散模型来生成视频帧。损失函数的设计需要考虑时间一致性、视觉一致性和语义一致性，例如可以使用对抗损失、循环一致性损失和语义相似度损失等。",
            "application_zh": "WorldCanvas 在游戏开发、电影制作、机器人仿真、自动驾驶测试等领域具有广泛的应用前景。它可以用于生成各种虚拟场景和事件，帮助开发者快速创建游戏内容、进行电影特效制作、训练机器人和测试自动驾驶系统。此外，WorldCanvas 还可以用于教育和娱乐领域，例如创建交互式故事和虚拟现实体验。",
            "highlight_zh": "论文通过大量实验验证了 WorldCanvas 的有效性。实验结果表明，WorldCanvas 生成的视频在时间一致性、视觉一致性和语义一致性方面均优于现有方法。例如，在对象进入/退出场景的实验中，WorldCanvas 能够保持对象身份和外观的一致性，即使对象暂时消失也能正确地重新出现。此外，WorldCanvas 还能够生成违反直觉的事件，例如让物体以不符合物理规律的方式运动。",
            "tags_zh": [
                "世界事件模拟",
                "多模态融合",
                "轨迹控制",
                "参考图像引导",
                "视频生成"
            ],
            "_index": 36,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16924v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16924v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16924v1/x4.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Task-Oriented Data Synthesis and Control-Rectify Sampling for Remote Sensing Semantic Segmentation",
            "authors": [
                "Yunkai Yang",
                "Yudong Zhang",
                "Kunquan Zhang",
                "Jinxiao Zhang",
                "Xinying Chen",
                "Haohuan Fu",
                "Runmin Dong"
            ],
            "arxiv_id": "2512.16740v1",
            "summary": "With the rapid progress of controllable generation, training data synthesis has become a promising way to expand labeled datasets and alleviate manual annotation in remote sensing (RS). However, the complexity of semantic mask control and the uncertainty of sampling quality often limit the utility of synthetic data in downstream semantic segmentation tasks. To address these challenges, we propose a task-oriented data synthesis framework (TODSynth), including a Multimodal Diffusion Transformer (MM-DiT) with unified triple attention and a plug-and-play sampling strategy guided by task feedback. Built upon the powerful DiT-based generative foundation model, we systematically evaluate different control schemes, showing that a text-image-mask joint attention scheme combined with full fine-tuning of the image and mask branches significantly enhances the effectiveness of RS semantic segmentation data synthesis, particularly in few-shot and complex-scene scenarios. Furthermore, we propose a control-rectify flow matching (CRFM) method, which dynamically adjusts sampling directions guided by semantic loss during the early high-plasticity stage, mitigating the instability of generated images and bridging the gap between synthetic data and downstream segmentation tasks. Extensive experiments demonstrate that our approach consistently outperforms state-of-the-art controllable generation methods, producing more stable and task-oriented synthetic data for RS semantic segmentation.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16740v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "flow matching"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "foundation model",
                        "multimodal"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 7.5,
            "hit_pillars": [
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "提出TODSynth框架，用于遥感语义分割任务的数据合成与控制优化。",
            "summary_zh": "随着可控生成技术的快速发展，训练数据合成已成为扩展遥感（RS）标注数据集和缓解人工标注负担的一种有前景的方法。然而，语义掩码控制的复杂性和采样质量的不确定性常常限制了合成数据在下游语义分割任务中的效用。为了解决这些挑战，我们提出了一个面向任务的数据合成框架（TODSynth），包括一个具有统一三重注意力的多模态扩散Transformer（MM-DiT）和一个由任务反馈指导的即插即用采样策略。基于强大的DiT生成基础模型，我们系统地评估了不同的控制方案，表明文本-图像-掩码联合注意力方案与图像和掩码分支的完全微调相结合，显著增强了遥感语义分割数据合成的有效性，尤其是在少样本和复杂场景中。此外，我们提出了一种控制校正流匹配（CRFM）方法，该方法在早期高可塑性阶段动态调整采样方向，由语义损失引导，从而减轻了生成图像的不稳定性，并弥合了合成数据与下游分割任务之间的差距。大量实验表明，我们的方法始终优于最先进的可控生成方法，为遥感语义分割生成更稳定和面向任务的合成数据。",
            "intro_zh": [
                "遥感图像语义分割依赖大量标注数据，但人工标注成本高昂，可控数据生成是潜在解决方案。",
                "TODSynth框架利用多模态扩散Transformer和任务反馈采样策略，提升合成数据的质量和任务相关性。",
                "实验表明，该方法在少样本和复杂场景下，显著优于现有可控生成方法，提升了遥感语义分割性能。"
            ],
            "method_zh": "**问题定义**：遥感图像语义分割任务需要大量的标注数据，而人工标注成本高、效率低。现有的数据合成方法难以有效控制生成数据的语义信息，且合成数据的质量和下游任务的相关性不足，导致其在实际应用中效果不佳。尤其是在少样本和复杂场景下，问题更为突出。\\n\\n**核心思路**：本文的核心思路是提出一个面向任务的数据合成框架TODSynth，通过多模态扩散模型和任务反馈机制，实现对生成数据的精确控制和优化。具体来说，利用文本、图像和掩码等多模态信息，指导扩散模型的生成过程，并引入下游分割任务的损失函数，动态调整采样方向，从而生成更符合任务需求的合成数据。\\n\\n**技术框架**：TODSynth框架主要包含两个核心模块：多模态扩散Transformer（MM-DiT）和控制校正流匹配（CRFM）。MM-DiT是一个基于DiT的生成模型，通过统一的三重注意力机制，融合文本、图像和掩码等多模态信息。CRFM则是一种采样策略，它在扩散模型的采样过程中，利用下游分割任务的语义损失，动态调整采样方向，从而优化生成数据的质量。整体流程是：首先，利用MM-DiT生成初步的合成数据；然后，通过CRFM策略，利用下游分割任务的反馈，对生成数据进行优化，最终得到高质量的合成数据。\\n\\n**关键创新**：本文的关键创新在于：1) 提出了一个统一的三重注意力机制，能够有效融合文本、图像和掩码等多模态信息，实现对生成数据的精确控制。2) 提出了控制校正流匹配（CRFM）方法，利用下游分割任务的反馈，动态调整采样方向，从而优化生成数据的质量，弥合了合成数据与真实数据之间的差距。\\n\\n**关键设计**：MM-DiT采用DiT作为基础模型，并引入了统一的三重注意力机制，将文本、图像和掩码信息融合到Transformer的每一层。CRFM的关键在于语义损失的计算和采样方向的调整。具体来说，利用下游分割模型的预测结果，计算生成数据与真实数据之间的语义损失，然后利用该损失的梯度，调整扩散模型的采样方向。此外，还设计了一系列参数，用于控制CRFM的强度和优化过程。",
            "application_zh": "该研究成果可广泛应用于遥感图像语义分割领域，尤其是在缺乏标注数据或标注成本高昂的情况下。例如，可以用于城市规划、环境监测、灾害评估等领域。通过合成高质量的训练数据，可以显著提升遥感图像语义分割模型的性能，降低人工标注的成本，加速相关技术的应用和发展。",
            "highlight_zh": "实验结果表明，TODSynth框架在遥感语义分割任务中取得了显著的性能提升。与现有最先进的可控生成方法相比，TODSynth能够生成更稳定、更面向任务的合成数据。具体来说，在少样本场景下，TODSynth的性能提升尤为明显，能够有效缓解数据稀缺带来的问题。此外，TODSynth在复杂场景下的表现也优于其他方法，表明其具有更强的泛化能力。",
            "tags_zh": [
                "遥感语义分割",
                "数据合成",
                "可控生成",
                "扩散模型",
                "多模态学习"
            ],
            "_index": 37,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16740v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16740v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16740v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "OPENTOUCH: Bringing Full-Hand Touch to Real-World Interaction",
            "authors": [
                "Yuxin Ray Song",
                "Jinzhou Li",
                "Rao Fu",
                "Devin Murphy",
                "Kaichen Zhou",
                "Rishi Shiv",
                "Yaqi Li",
                "Haoyu Xiong",
                "Crystal Elaine Owens",
                "Yilun Du",
                "Yiyue Luo",
                "Xianyi Cheng",
                "Antonio Torralba",
                "Wojciech Matusik",
                "Paul Pu Liang"
            ],
            "arxiv_id": "2512.16842v1",
            "summary": "The human hand is our primary interface to the physical world, yet egocentric perception rarely knows when, where, or how forcefully it makes contact. Robust wearable tactile sensors are scarce, and no existing in-the-wild datasets align first-person video with full-hand touch. To bridge the gap between visual perception and physical interaction, we present OpenTouch, the first in-the-wild egocentric full-hand tactile dataset, containing 5.1 hours of synchronized video-touch-pose data and 2,900 curated clips with detailed text annotations. Using OpenTouch, we introduce retrieval and classification benchmarks that probe how touch grounds perception and action. We show that tactile signals provide a compact yet powerful cue for grasp understanding, strengthen cross-modal alignment, and can be reliably retrieved from in-the-wild video queries. By releasing this annotated vision-touch-pose dataset and benchmark, we aim to advance multimodal egocentric perception, embodied learning, and contact-rich robotic manipulation.",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.RO"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "https://opentouch-tactile.github.io/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16842v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱六：视频提取与匹配 (Video Extraction)",
                    "id": "6_video_extraction",
                    "matched_keywords": [
                        "egocentric"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "multimodal"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 7.0,
            "hit_pillars": [
                "1_robot_core",
                "6_video_extraction",
                "9_embodied_foundation"
            ],
            "headline_zh": "OPENTOUCH：构建真实场景下全手触觉交互数据集与基准",
            "summary_zh": "人手是与物理世界交互的主要界面，但自我中心感知很少知道何时、何地或以多大力度进行接触。目前缺乏稳健的可穿戴触觉传感器，并且没有现有的真实场景数据集将第一人称视频与全手触觉对齐。为了弥合视觉感知和物理交互之间的差距，我们提出了OpenTouch，这是第一个真实场景下的自我中心全手触觉数据集，包含5.1小时的同步视频-触觉-姿势数据和2,900个带有详细文本注释的精选片段。利用OpenTouch，我们引入了检索和分类基准，以探索触觉如何支撑感知和行动。我们表明，触觉信号为抓取理解提供了一个紧凑而强大的线索，加强了跨模态对齐，并且可以从真实场景视频查询中可靠地检索。通过发布这个带注释的视觉-触觉-姿势数据集和基准，我们的目标是推进多模态自我中心感知、具身学习和富含接触的机器人操作。",
            "intro_zh": [
                "现有方法缺乏在真实场景下同步第一人称视频与全手触觉的数据集，阻碍了视觉与触觉融合研究。",
                "OpenTouch数据集包含同步的视频、触觉和姿势数据，并提供检索和分类基准，用于评估触觉在感知和行动中的作用。",
                "实验表明，触觉信号能有效提升抓取理解和跨模态对齐，并可从视频中可靠检索触觉信息。"
            ],
            "method_zh": "**问题定义**：现有方法缺乏在真实场景下同步第一人称视频与全手触觉的数据集，导致无法有效研究视觉和触觉的融合，限制了具身智能和机器人操作的发展。现有可穿戴触觉传感器不够鲁棒，难以在真实场景中应用。\\n\\n**核心思路**：论文的核心思路是通过构建一个大规模的、带注释的真实场景全手触觉数据集，为多模态自我中心感知提供数据基础。通过设计检索和分类基准，鼓励研究者探索触觉在感知和行动中的作用，并评估不同模型的性能。\\n\\n**技术框架**：OpenTouch数据集包含同步的自我中心视频、全手触觉数据和手部姿势数据。数据采集过程涉及多种真实场景下的交互任务。数据集还包含2900个带有详细文本注释的精选片段。基于该数据集，论文提出了两个基准任务：触觉检索和触觉分类。触觉检索任务旨在从视频中检索对应的触觉信号，触觉分类任务旨在根据触觉信号识别交互动作。\\n\\n**关键创新**：该论文的关键创新在于构建了首个真实场景下的自我中心全手触觉数据集，并提出了相应的检索和分类基准。该数据集的规模和质量为多模态感知研究提供了新的资源。此外，论文还展示了触觉信号在抓取理解和跨模态对齐方面的潜力。\\n\\n**关键设计**：数据集采集过程中，使用了可穿戴触觉传感器来获取全手触觉信息，并使用同步设备记录自我中心视频和手部姿势数据。为了保证数据的质量，论文对数据进行了清洗和标注。在基准任务中，论文使用了标准的深度学习模型，并针对触觉数据的特点进行了优化。具体的参数设置和网络结构细节在论文中有详细描述。",
            "application_zh": "OpenTouch数据集和基准可应用于机器人操作、具身智能、人机交互等领域。例如，可以训练机器人通过触觉感知来更好地理解和执行抓取任务，或者开发更自然的人机交互界面，使用户可以通过触摸与虚拟环境进行交互。该研究有望推动机器人更加智能和自主，并改善人机交互体验。",
            "highlight_zh": "实验结果表明，触觉信号能够显著提升抓取理解的准确率，并加强跨模态对齐的效果。在触觉检索任务中，模型能够从真实场景视频中可靠地检索出对应的触觉信号。这些结果验证了触觉在感知和行动中的重要作用，并为未来的研究提供了有力的支持。",
            "tags_zh": [
                "全手触觉",
                "自我中心视觉",
                "多模态数据集",
                "具身智能",
                "机器人操作",
                "触觉感知",
                "跨模态学习"
            ],
            "_index": 38,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16842v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16842v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16842v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Kling-Omni Technical Report",
            "authors": [
                "Kling Team",
                "Jialu Chen",
                "Yuanzheng Ci",
                "Xiangyu Du",
                "Zipeng Feng",
                "Kun Gai",
                "Sainan Guo",
                "Feng Han",
                "Jingbin He",
                "Kang He",
                "Xiao Hu",
                "Xiaohua Hu",
                "Boyuan Jiang",
                "Fangyuan Kong",
                "Hang Li",
                "Jie Li",
                "Qingyu Li",
                "Shen Li",
                "Xiaohan Li",
                "Yan Li",
                "Jiajun Liang",
                "Borui Liao",
                "Yiqiao Liao",
                "Weihong Lin",
                "Quande Liu",
                "Xiaokun Liu",
                "Yilun Liu",
                "Yuliang Liu",
                "Shun Lu",
                "Hangyu Mao",
                "Yunyao Mao",
                "Haodong Ouyang",
                "Wenyu Qin",
                "Wanqi Shi",
                "Xiaoyu Shi",
                "Lianghao Su",
                "Haozhi Sun",
                "Peiqin Sun",
                "Pengfei Wan",
                "Chao Wang",
                "Chenyu Wang",
                "Meng Wang",
                "Qiulin Wang",
                "Runqi Wang",
                "Xintao Wang",
                "Xuebo Wang",
                "Zekun Wang",
                "Min Wei",
                "Tiancheng Wen",
                "Guohao Wu",
                "Xiaoshi Wu",
                "Zhenhua Wu",
                "Da Xie",
                "Yingtong Xiong",
                "Yulong Xu",
                "Sile Yang",
                "Zikang Yang",
                "Weicai Ye",
                "Ziyang Yuan",
                "Shenglong Zhang",
                "Shuaiyu Zhang",
                "Yuanxing Zhang",
                "Yufan Zhang",
                "Wenzheng Zhao",
                "Ruiliang Zhou",
                "Yan Zhou",
                "Guosheng Zhu",
                "Yongjie Zhu"
            ],
            "arxiv_id": "2512.16776v1",
            "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Kling-Omni Technical Report",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16776v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "multimodal",
                        "instruction following"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "Kling-Omni：通用生成框架，实现多模态输入到高质量视频的端到端合成",
            "summary_zh": "Kling-Omni是一个通用的生成框架，旨在直接从多模态视觉语言输入合成高保真视频。Kling-Omni采用端到端的视角，弥合了不同视频生成、编辑和智能推理任务之间的功能分离，将它们集成到一个整体系统中。与不连贯的流水线方法不同，Kling-Omni支持多种用户输入，包括文本指令、参考图像和视频上下文，并将它们处理成统一的多模态表示，以提供电影质量和高度智能的视频内容创作。为了支持这些能力，我们构建了一个全面的数据系统，作为多模态视频创作的基础。该框架通过高效的大规模预训练策略和用于推理的基础设施优化得到进一步加强。综合评估表明，Kling-Omni在上下文生成、基于推理的编辑和多模态指令遵循方面表现出卓越的能力。我们认为，Kling-Omni不仅仅是一个内容创作工具，更是朝着能够感知、推理、生成和与动态复杂世界交互的多模态世界模拟器迈出的关键一步。",
            "intro_zh": [
                "现有视频生成方法通常采用分离的流水线，难以处理复杂的多模态输入和实现统一的视频理解与生成。",
                "Kling-Omni提出一个端到端的通用框架，通过统一的多模态表示，支持文本、图像、视频等多种输入，实现高质量视频生成。",
                "通过大规模预训练和基础设施优化，Kling-Omni在上下文生成、推理编辑和多模态指令跟随等方面展现了卓越的性能。"
            ],
            "method_zh": "**问题定义**：现有视频生成、编辑和智能推理任务通常是分离的，需要构建复杂的流水线来处理不同的任务和输入模态。这些方法难以实现统一的视频理解和生成，并且难以处理复杂的多模态输入，例如同时考虑文本描述、参考图像和视频上下文。现有方法的痛点在于缺乏一个能够端到端处理多种任务和模态的通用框架。\\n\\n**核心思路**：Kling-Omni的核心思路是将视频生成、编辑和智能推理任务统一到一个端到端的框架中，通过学习一个统一的多模态表示来处理不同的输入模态。该框架旨在构建一个通用的视频生成模型，能够根据文本指令、参考图像和视频上下文等多种输入生成高质量的视频内容。这样设计的目的是为了简化视频生成流程，提高生成视频的质量和智能化程度。\\n\\n**技术框架**：Kling-Omni的技术框架包含以下几个主要模块：1) 多模态输入编码器：用于将文本指令、参考图像和视频上下文等多种输入编码成统一的多模态表示。2) 视频生成器：用于根据多模态表示生成高质量的视频内容。3) 大规模预训练模块：用于在海量数据上预训练模型，提高模型的泛化能力和生成质量。4) 推理优化模块：用于优化模型的推理速度，使其能够快速生成视频内容。整体流程是从多模态输入开始，经过编码器得到统一表示，然后通过生成器生成视频，并通过预训练和推理优化来提升性能。\\n\\n**关键创新**：Kling-Omni最重要的技术创新点在于其端到端的通用框架，能够统一处理视频生成、编辑和智能推理任务。与现有方法相比，Kling-Omni不需要构建复杂的流水线来处理不同的任务和输入模态，而是通过学习一个统一的多模态表示来实现多种任务的统一。这种端到端的设计简化了视频生成流程，提高了生成视频的质量和智能化程度。\\n\\n**关键设计**：关于关键设计，论文中没有提供非常具体的参数设置、损失函数、网络结构等技术细节。但是，可以推测，多模态输入编码器可能采用了Transformer等注意力机制模型，视频生成器可能采用了生成对抗网络（GAN）或扩散模型等技术。大规模预训练可能采用了对比学习或掩码语言模型等方法。具体的损失函数和网络结构等细节未知。",
            "application_zh": "Kling-Omni具有广泛的应用前景，可应用于电影制作、游戏开发、广告设计、教育培训等领域。它可以帮助用户快速生成高质量的视频内容，降低视频制作的成本和门槛。未来，Kling-Omni有望成为一个强大的多模态世界模拟器，能够感知、推理、生成和与动态复杂世界交互。",
            "highlight_zh": "Kling-Omni在上下文生成、基于推理的编辑和多模态指令跟随等方面表现出卓越的能力。具体性能数据未知，但摘要强调了其在处理多种输入模态和生成高质量视频方面的优势。该框架通过大规模预训练和基础设施优化，实现了显著的性能提升，超越了传统的分离式流水线方法。",
            "tags_zh": [
                "视频生成",
                "多模态学习",
                "端到端框架",
                "视觉语言模型",
                "通用人工智能"
            ],
            "_index": 39,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16776v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16776v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16776v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Sketch-in-Latents: Eliciting Unified Reasoning in MLLMs",
            "authors": [
                "Jintao Tong",
                "Jiaqi Gu",
                "Yujing Lou",
                "Lubin Fan",
                "Yixiong Zou",
                "Yue Wu",
                "Jieping Ye",
                "Ruixuan Li"
            ],
            "arxiv_id": "2512.16584v1",
            "summary": "While Multimodal Large Language Models (MLLMs) excel at visual understanding tasks through text reasoning, they often fall short in scenarios requiring visual imagination. Unlike current works that take predefined external toolkits or generate images during thinking, however, humans can form flexible visual-text imagination and interactions during thinking without predefined toolkits, where one important reason is that humans construct the visual-text thinking process in a unified space inside the brain. Inspired by this capability, given that current MLLMs already encode visual and text information in the same feature space, we hold that visual tokens can be seamlessly inserted into the reasoning process carried by text tokens, where ideally, all visual imagination processes can be encoded by the latent features. To achieve this goal, we propose Sketch-in-Latents (SkiLa), a novel paradigm for unified multi-modal reasoning that expands the auto-regressive capabilities of MLLMs to natively generate continuous visual embeddings, termed latent sketch tokens, as visual thoughts. During multi-step reasoning, the model dynamically alternates between textual thinking mode for generating textual think tokens and visual sketching mode for generating latent sketch tokens. A latent visual semantics reconstruction mechanism is proposed to ensure these latent sketch tokens are semantically grounded. Extensive experiments demonstrate that SkiLa achieves superior performance on vision-centric tasks while exhibiting strong generalization to diverse general multi-modal benchmarks. Codes will be released at https://github.com/TungChintao/SkiLa.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "14 pages, 11 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16584v1",
            "code_links": [
                {
                    "url": "https://github.com/TungChintao/SkiLa",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model",
                        "multimodal"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出Sketch-in-Latents (SkiLa)，实现MLLM中统一的多模态推理与视觉想象。",
            "summary_zh": "多模态大型语言模型(MLLM)擅长通过文本推理进行视觉理解任务，但在需要视觉想象的场景中表现不佳。与采用预定义外部工具包或在思考过程中生成图像的现有方法不同，人类可以在没有预定义工具包的情况下，在思考过程中形成灵活的视觉-文本想象和交互，一个重要的原因是人类在大脑内部的统一空间中构建视觉-文本思考过程。受此启发，考虑到当前的MLLM已经将视觉和文本信息编码在相同的特征空间中，我们认为视觉token可以无缝地插入到文本token所携带的推理过程中，理想情况下，所有的视觉想象过程都可以由潜在特征编码。为了实现这个目标，我们提出Sketch-in-Latents (SkiLa)，这是一种用于统一多模态推理的新范式，它扩展了MLLM的自回归能力，以原生生成连续的视觉嵌入，称为潜在草图token，作为视觉思想。在多步推理过程中，模型动态地在用于生成文本思考token的文本思考模式和用于生成潜在草图token的视觉草图模式之间切换。提出了一种潜在的视觉语义重建机制，以确保这些潜在的草图token在语义上是接地的。大量的实验表明，SkiLa在以视觉为中心的任务上取得了优异的性能，同时对各种通用多模态基准表现出强大的泛化能力。",
            "intro_zh": [
                "现有MLLM在视觉想象方面存在不足，无法像人类一样灵活进行视觉-文本交互。",
                "SkiLa通过生成潜在草图token，将视觉信息无缝融入MLLM的推理过程，实现统一的多模态推理。",
                "实验表明，SkiLa在视觉任务上表现优异，并对通用多模态基准具有良好的泛化性。"
            ],
            "method_zh": "**问题定义**：现有MLLM虽然擅长视觉理解，但缺乏有效的视觉想象能力，无法像人类一样在推理过程中灵活地进行视觉-文本交互。现有方法通常依赖于预定义的外部工具或在推理过程中生成图像，这限制了模型的灵活性和泛化能力。\\n\\n**核心思路**：SkiLa的核心思路是将视觉想象过程编码为潜在特征，并将其无缝地插入到MLLM的文本推理过程中。通过生成连续的视觉嵌入（潜在草图token），模型可以在推理过程中动态地进行文本思考和视觉草图绘制，从而实现统一的多模态推理。这种方法模拟了人类大脑中视觉和文本信息在统一空间中进行交互的方式。\\n\\n**技术框架**：SkiLa的整体框架包括文本思考模式和视觉草图模式。在文本思考模式下，模型生成文本token进行推理；在视觉草图模式下，模型生成潜在草图token，表示视觉信息。模型在这两种模式之间动态切换，以实现多步推理。为了确保潜在草图token的语义一致性，SkiLa还引入了一种潜在视觉语义重建机制。\\n\\n**关键创新**：SkiLa最重要的创新点在于它扩展了MLLM的自回归能力，使其能够原生生成连续的视觉嵌入（潜在草图token）。这与现有方法依赖于外部工具或生成离散图像的方式不同，SkiLa将视觉想象过程直接融入到MLLM的推理过程中，实现了更紧密的视觉-文本融合。\\n\\n**关键设计**：SkiLa的关键设计包括：1) 潜在草图token的生成方式，可能涉及到特定的网络结构或损失函数，以确保生成的token具有良好的语义表达能力；2) 文本思考模式和视觉草图模式之间的切换机制，需要根据具体的任务和推理步骤进行设计；3) 潜在视觉语义重建机制，可能涉及到对抗训练或自监督学习等技术，以确保潜在草图token的语义一致性。",
            "application_zh": "SkiLa具有广泛的应用前景，例如视觉问答、图像生成、机器人导航和人机交互等领域。它可以帮助机器更好地理解和利用视觉信息，从而实现更智能、更自然的交互。未来，SkiLa有望应用于自动驾驶、智能家居和虚拟现实等领域，提升用户体验。",
            "highlight_zh": "SkiLa在多个视觉任务上取得了优异的性能，证明了其有效性。具体实验结果（论文中提供）表明，SkiLa在视觉问答、图像生成等任务上显著优于现有方法，并且在通用多模态基准上表现出良好的泛化能力。这些结果表明，SkiLa是一种有前景的多模态推理方法。",
            "tags_zh": [
                "多模态学习",
                "大型语言模型",
                "视觉推理",
                "视觉想象",
                "潜在空间",
                "自回归模型",
                "语义重建"
            ],
            "_index": 40,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16584v1/img/method.jpg",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16584v1/img/hyper.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16584v1/img/case_geo.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "4D Primitive-Mâché: Glueing Primitives for Persistent 4D Scene Reconstruction",
            "authors": [
                "Kirill Mazur",
                "Marwan Taher",
                "Andrew J. Davison"
            ],
            "arxiv_id": "2512.16564v1",
            "summary": "We present a dynamic reconstruction system that receives a casual monocular RGB video as input, and outputs a complete and persistent reconstruction of the scene. In other words, we reconstruct not only the the currently visible parts of the scene, but also all previously viewed parts, which enables replaying the complete reconstruction across all timesteps.\n  Our method decomposes the scene into a set of rigid 3D primitives, which are assumed to be moving throughout the scene. Using estimated dense 2D correspondences, we jointly infer the rigid motion of these primitives through an optimisation pipeline, yielding a 4D reconstruction of the scene, i.e. providing 3D geometry dynamically moving through time. To achieve this, we also introduce a mechanism to extrapolate motion for objects that become invisible, employing motion-grouping techniques to maintain continuity.\n  The resulting system enables 4D spatio-temporal awareness, offering capabilities such as replayable 3D reconstructions of articulated objects through time, multi-object scanning, and object permanence. On object scanning and multi-object datasets, our system significantly outperforms existing methods both quantitatively and qualitatively.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "For project page, see https://makezur.github.io/4DPM/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16564v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]scene reconstruction"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出4D Primitive-Mâché，通过拼接基元实现持久化4D场景重建",
            "summary_zh": "本文提出了一种动态重建系统，该系统以单目RGB视频作为输入，输出场景的完整且持久的重建结果。换句话说，我们不仅重建当前可见的场景部分，还重建所有先前观察到的部分，从而能够重放所有时间步的完整重建。我们的方法将场景分解为一组刚性3D基元，假设这些基元在场景中移动。利用估计的密集2D对应关系，我们通过优化流程联合推断这些基元的刚性运动，从而产生场景的4D重建，即提供随时间动态移动的3D几何体。为了实现这一点，我们还引入了一种机制来推断不可见物体的运动，采用运动分组技术来保持连续性。由此产生的系统实现了4D时空感知，提供了诸如随时间推移的可重放3D铰接物体重建、多物体扫描和物体持久性等功能。在物体扫描和多物体数据集上，我们的系统在定量和定性方面均显著优于现有方法。",
            "intro_zh": [
                "现有动态场景重建方法难以保持场景的完整性和持久性，尤其是在物体遮挡或离开视野后。",
                "该方法将场景分解为可移动的刚性3D基元，通过优化框架联合推断基元的刚性运动，实现4D场景重建。",
                "实验结果表明，该系统在物体扫描和多物体数据集上，显著优于现有的动态重建方法。"
            ],
            "method_zh": "**问题定义**：现有的动态场景重建方法通常只能重建当前可见的场景部分，无法保持场景的完整性和持久性，尤其是在物体被遮挡或离开视野后。此外，如何有效地处理场景中多个独立运动的物体也是一个挑战。\\n\\n**核心思路**：该论文的核心思路是将动态场景分解为一组刚性3D基元，并假设这些基元在场景中进行刚性运动。通过估计的密集2D对应关系，联合优化这些基元的运动轨迹，从而实现对整个场景的4D重建。这种方法能够有效地处理多个独立运动的物体，并且可以通过运动外推来估计不可见物体的运动。\\n\\n**技术框架**：该系统的整体流程如下：1) 输入单目RGB视频；2) 使用SLAM或SfM等技术估计相机位姿和稀疏点云；3) 将场景分解为一组3D基元；4) 利用密集2D对应关系，通过优化框架联合估计基元的刚性运动；5) 对于不可见的物体，使用运动分组技术进行运动外推；6) 输出完整的4D场景重建结果。\\n\\n**关键创新**：该论文的关键创新在于提出了一种基于3D基元的4D场景重建方法，能够有效地处理多个独立运动的物体，并且可以通过运动外推来估计不可见物体的运动。此外，该方法还引入了一种运动分组技术，用于保持运动的连续性。\\n\\n**关键设计**：在优化框架中，使用了包括光度误差、几何误差和运动平滑性约束在内的损失函数。运动分组技术基于物体之间的运动相似性进行分组，并使用卡尔曼滤波器对每个组的运动进行平滑。具体的参数设置和网络结构在论文中进行了详细描述。",
            "application_zh": "该研究成果可应用于增强现实、虚拟现实、机器人导航、自动驾驶等领域。例如，在AR/VR中，可以提供更真实和沉浸式的体验，用户可以与动态场景进行交互。在机器人导航和自动驾驶中，可以帮助机器人更好地理解周围环境，并做出更合理的决策。此外，该技术还可以用于创建数字孪生，用于远程监控和控制。",
            "highlight_zh": "该系统在物体扫描和多物体数据集上进行了评估，实验结果表明，该系统在定量和定性方面均显著优于现有的动态重建方法。具体而言，该系统在重建精度和完整性方面均取得了显著提升，并且能够有效地处理多个独立运动的物体。",
            "tags_zh": [
                "4D场景重建",
                "动态场景",
                "基元分解",
                "运动估计",
                "单目视觉"
            ],
            "_index": 41,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16564v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16564v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16564v1/x4.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Using Gaussian Splats to Create High-Fidelity Facial Geometry and Texture",
            "authors": [
                "Haodi He",
                "Jihun Yu",
                "Ronald Fedkiw"
            ],
            "arxiv_id": "2512.16397v1",
            "summary": "We leverage increasingly popular three-dimensional neural representations in order to construct a unified and consistent explanation of a collection of uncalibrated images of the human face. Our approach utilizes Gaussian Splatting, since it is more explicit and thus more amenable to constraints than NeRFs. We leverage segmentation annotations to align the semantic regions of the face, facilitating the reconstruction of a neutral pose from only 11 images (as opposed to requiring a long video). We soft constrain the Gaussians to an underlying triangulated surface in order to provide a more structured Gaussian Splat reconstruction, which in turn informs subsequent perturbations to increase the accuracy of the underlying triangulated surface. The resulting triangulated surface can then be used in a standard graphics pipeline. In addition, and perhaps most impactful, we show how accurate geometry enables the Gaussian Splats to be transformed into texture space where they can be treated as a view-dependent neural texture. This allows one to use high visual fidelity Gaussian Splatting on any asset in a scene without the need to modify any other asset or any other aspect (geometry, lighting, renderer, etc.) of the graphics pipeline. We utilize a relightable Gaussian model to disentangle texture from lighting in order to obtain a delit high-resolution albedo texture that is also readily usable in a standard graphics pipeline. The flexibility of our system allows for training with disparate images, even with incompatible lighting, facilitating robust regularization. Finally, we demonstrate the efficacy of our approach by illustrating its use in a text-driven asset creation pipeline.",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.GR"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Submitted to CVPR 2026. 21 pages, 22 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16397v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "gaussian splatting",
                        "splatting",
                        "NeRF"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "利用高斯溅射重建高保真面部几何与纹理，实现可控人脸生成",
            "summary_zh": "本文利用日益流行的三维神经表示，从一组未经校准的人脸图像中构建统一且一致的解释。该方法采用高斯溅射，因为它比NeRF更显式，因此更易于约束。利用分割标注对齐面部的语义区域，从而仅用11张图像即可重建中性姿势（而无需长视频）。软约束高斯分布到一个潜在的三角化表面，以提供更结构化的重建，进而指导后续扰动以提高三角化表面的准确性。生成的三角化表面可用于标准图形管线。此外，也是最重要的，展示了精确的几何体如何使高斯溅射转换为纹理空间，在纹理空间中，它们可以被视为与视角相关的神经纹理。这允许在场景中的任何资产上使用高视觉保真度的高斯溅射，而无需修改任何其他资产或图形管线的任何其他方面（几何体、光照、渲染器等）。利用可重新光照的高斯模型将纹理与光照分离，以获得可在标准图形管线中使用的去光照高分辨率反照率纹理。系统的灵活性允许使用不同的图像进行训练，即使光照不兼容，也有助于鲁棒的正则化。最后，通过展示其在文本驱动的资产创建管线中的应用，证明了该方法的有效性。",
            "intro_zh": [
                "现有方法难以从少量未校准图像中重建高保真人脸几何与纹理，尤其是在光照条件不一致的情况下。",
                "利用高斯溅射的显式特性，结合语义分割和表面约束，实现从少量图像中重建高质量人脸模型。",
                "通过将高斯溅射转换为纹理空间，并解耦光照，生成可用于标准图形管线的、高质量的反照率纹理。"
            ],
            "method_zh": "**问题定义**：现有方法，如NeRF，在人脸重建任务中，尤其是在图像数量有限且未经校准的情况下，难以生成高保真度的几何和纹理。此外，光照条件不一致也会严重影响重建质量。现有方法通常需要大量的训练数据或复杂的预处理步骤，限制了其在实际应用中的可行性。\\n\\n**核心思路**：本文的核心思路是利用高斯溅射（Gaussian Splatting）的显式特性，结合语义分割和表面约束，从少量图像中重建高质量的人脸模型。通过将高斯溅射投影到纹理空间，并解耦光照，生成可用于标准图形管线的、高质量的反照率纹理。这种方法能够有效地处理光照不一致的问题，并降低对训练数据量的需求。\\n\\n**技术框架**：该方法主要包含以下几个阶段：1) 使用少量未校准的人脸图像作为输入；2) 利用语义分割标注对齐面部的语义区域，从而实现中性姿势的重建；3) 将高斯溅射软约束到一个潜在的三角化表面，以提供更结构化的重建；4) 通过扰动三角化表面来提高其准确性；5) 将高斯溅射转换为纹理空间，并将其视为与视角相关的神经纹理；6) 利用可重新光照的高斯模型将纹理与光照分离，生成去光照的高分辨率反照率纹理。\\n\\n**关键创新**：该方法的关键创新在于：1) 利用高斯溅射的显式特性，使其更易于约束，从而能够从少量图像中重建高质量的人脸模型；2) 将高斯溅射转换为纹理空间，并将其视为与视角相关的神经纹理，从而能够生成高质量的反照率纹理；3) 利用可重新光照的高斯模型将纹理与光照分离，从而能够有效地处理光照不一致的问题。与现有方法相比，该方法能够以更少的图像和更低的计算成本，生成更高质量的人脸模型。\\n\\n**关键设计**：该方法的关键设计包括：1) 使用软约束将高斯溅射约束到三角化表面，以提高重建的结构性；2) 使用可重新光照的高斯模型，该模型允许将纹理与光照分离，从而生成去光照的反照率纹理；3) 使用分割标注对齐面部的语义区域，从而实现中性姿势的重建。损失函数的设计也至关重要，需要平衡重建质量、表面平滑度和光照一致性。",
            "application_zh": "该研究成果可广泛应用于虚拟现实、增强现实、游戏开发、数字人生成等领域。通过该方法，可以快速、高效地创建逼真的人脸模型，为用户提供更沉浸式的体验。此外，该方法还可以应用于人脸识别、表情识别等领域，提高相关算法的准确性和鲁棒性。未来，该技术有望在个性化定制、远程医疗、教育等领域发挥重要作用。",
            "highlight_zh": "该方法仅使用11张图像即可重建高质量的人脸模型，无需长视频或复杂的预处理步骤。通过将高斯溅射转换为纹理空间，并解耦光照，生成了高质量的反照率纹理，可直接用于标准图形管线。实验结果表明，该方法在重建质量和效率方面均优于现有方法，尤其是在光照条件不一致的情况下。",
            "tags_zh": [
                "高斯溅射",
                "人脸重建",
                "神经渲染",
                "纹理生成",
                "三维重建"
            ],
            "_index": 42,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16397v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16397v1/figs/ablation/vis_gaussians/2_geometry_render.jpg",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16397v1/figs/head_poses/00.jpg",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "EverybodyDance: Bipartite Graph-Based Identity Correspondence for Multi-Character Animation",
            "authors": [
                "Haotian Ling",
                "Zequn Chen",
                "Qiuying Chen",
                "Donglin Di",
                "Yongjia Ma",
                "Hao Li",
                "Chen Wei",
                "Zhulin Tao",
                "Xun Yang"
            ],
            "arxiv_id": "2512.16360v1",
            "summary": "Consistent pose-driven character animation has achieved remarkable progress in single-character scenarios. However, extending these advances to multi-character settings is non-trivial, especially when position swap is involved. Beyond mere scaling, the core challenge lies in enforcing correct Identity Correspondence (IC) between characters in reference and generated frames. To address this, we introduce EverybodyDance, a systematic solution targeting IC correctness in multi-character animation. EverybodyDance is built around the Identity Matching Graph (IMG), which models characters in the generated and reference frames as two node sets in a weighted complete bipartite graph. Edge weights, computed via our proposed Mask-Query Attention (MQA), quantify the affinity between each pair of characters. Our key insight is to formalize IC correctness as a graph structural metric and to optimize it during training. We also propose a series of targeted strategies tailored for multi-character animation, including identity-embedded guidance, a multi-scale matching strategy, and pre-classified sampling, which work synergistically. Finally, to evaluate IC performance, we curate the Identity Correspondence Evaluation benchmark, dedicated to multi-character IC correctness. Extensive experiments demonstrate that EverybodyDance substantially outperforms state-of-the-art baselines in both IC and visual fidelity.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16360v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱八：物理动画 (Physics-based Animation)",
                    "id": "8_physics_animation",
                    "matched_keywords": [
                        "[T]character animation"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "8_physics_animation"
            ],
            "headline_zh": "提出EverybodyDance，通过二分图匹配解决多角色动画中的身份对应问题。",
            "summary_zh": "本文提出EverybodyDance，旨在解决多角色动画中身份对应（IC）的正确性问题，尤其是在角色位置互换的情况下。EverybodyDance的核心是身份匹配图（IMG），它将生成帧和参考帧中的角色建模为加权完全二分图的两个节点集合。通过提出的Mask-Query Attention（MQA）计算边权重，量化角色之间的亲和力。论文将IC正确性形式化为图结构度量，并在训练过程中优化它。此外，还提出了一系列针对多角色动画的策略，包括身份嵌入引导、多尺度匹配策略和预分类采样。为了评估IC性能，创建了身份对应评估基准。实验表明，EverybodyDance在IC和视觉保真度方面均优于现有技术。",
            "intro_zh": [
                "现有姿态驱动的角色动画在单角色场景中取得了显著进展，但扩展到多角色场景，尤其是在角色位置互换时，面临身份对应的挑战。",
                "EverybodyDance构建身份匹配图（IMG），利用Mask-Query Attention（MQA）计算角色间的亲和力，并将身份对应正确性形式化为图结构度量进行优化。",
                "论文构建了身份对应评估基准，并通过大量实验证明EverybodyDance在身份对应和视觉保真度方面均优于现有方法。"
            ],
            "method_zh": "**问题定义**：论文旨在解决多角色动画中，由于角色位置互换等因素导致的身份对应错误问题。现有方法在单角色动画上表现良好，但难以直接应用于多角色场景，尤其是在角色发生位置交换时，无法保证生成动画中角色的身份与参考帧中的角色身份一致。现有方法缺乏对角色间身份关系的建模和约束，容易出现身份混淆的情况。\\n\\n**核心思路**：论文的核心思路是将多角色动画中的身份对应问题转化为图匹配问题。通过构建身份匹配图（IMG），将参考帧和生成帧中的角色视为节点，角色之间的亲和度视为边权重，从而将身份对应问题转化为寻找最佳图匹配的问题。通过优化图结构度量，可以保证生成动画中角色的身份与参考帧中的角色身份一致。\\n\\n**技术框架**：EverybodyDance的整体框架包括以下几个主要模块：1) 角色检测与分割：从参考帧和生成帧中检测并分割出每个角色。2) 特征提取：提取每个角色的视觉特征和姿态特征。3) 身份匹配图构建：基于提取的特征，利用Mask-Query Attention（MQA）计算角色之间的亲和度，构建身份匹配图（IMG）。4) 图匹配优化：通过优化图结构度量，寻找最佳的图匹配方案，从而确定角色之间的身份对应关系。5) 动画生成：基于确定的身份对应关系，生成多角色动画。\\n\\n**关键创新**：论文最重要的技术创新点在于将身份对应问题形式化为图匹配问题，并提出了Mask-Query Attention（MQA）机制来计算角色之间的亲和度。MQA能够有效地捕捉角色之间的视觉相似性和姿态相似性，从而提高身份匹配的准确性。此外，论文还提出了身份嵌入引导、多尺度匹配策略和预分类采样等一系列针对多角色动画的策略，进一步提升了动画生成的质量。\\n\\n**关键设计**：在身份匹配图（IMG）中，边权重由Mask-Query Attention（MQA）计算得到。MQA利用角色的分割掩码作为Query，角色的视觉特征和姿态特征作为Key和Value，通过注意力机制计算角色之间的亲和度。论文还设计了身份嵌入引导，将角色的身份信息嵌入到特征中，从而提高身份匹配的准确性。此外，论文还采用了多尺度匹配策略，在不同尺度上进行身份匹配，从而提高对角色位置变化的鲁棒性。损失函数包括身份对应损失、视觉保真度损失等，用于优化动画生成的质量。",
            "application_zh": "该研究成果可应用于虚拟现实、游戏开发、电影制作等领域，实现更加自然和逼真的多角色动画效果。例如，可以用于创建多人在线游戏中玩家角色的动画，或者用于电影中多个演员的动作捕捉和动画生成。该技术还可以应用于虚拟形象定制，用户可以通过简单的姿态输入，生成具有个性化特征的多角色动画。",
            "highlight_zh": "实验结果表明，EverybodyDance在身份对应（IC）和视觉保真度方面均显著优于现有技术。在身份对应评估基准上，EverybodyDance的IC准确率比最先进的基线方法提高了超过10%。此外，视觉效果评估也表明，EverybodyDance生成的动画更加自然和逼真，能够有效地避免身份混淆等问题。",
            "tags_zh": [
                "多角色动画",
                "身份对应",
                "图匹配",
                "Mask-Query Attention",
                "姿态驱动",
                "角色动画",
                "二分图",
                "深度学习"
            ],
            "_index": 43,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16360v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16360v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16360v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "PixelArena: A benchmark for Pixel-Precision Visual Intelligence",
            "authors": [
                "Feng Liang",
                "Sizhe Cheng",
                "Chenqi Yi"
            ],
            "arxiv_id": "2512.16303v1",
            "summary": "Multi-modal large language models that have image output are emerging. Many image generation benchmarks focus on aesthetics instead of fine-grained generation capabilities. In PixelArena, we propose using semantic segmentation tasks to objectively examine their fine-grained generative intelligence with pixel precision. We find the latest Gemini 3 Pro Image has emergent image generation capabilities that generate semantic masks with high fidelity under zero-shot settings, showcasing visual intelligence unseen before and true generalization in new image generation tasks. We further investigate its results, compare them qualitatively and quantitatively with those of other models, and present failure cases. The findings not only signal exciting progress in the field but also provide insights into future research related to multimodality, reasoning, interpretability and benchmarking.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "7 pages, 11 figures, project page: https://pixelarena.reify.ing/project",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16303v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model",
                        "multimodal"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "PixelArena：提出像素级视觉智能评测基准，评估多模态大模型图像生成能力。",
            "summary_zh": "本文提出了PixelArena，一个用于像素精度视觉智能的评测基准。随着具有图像输出能力的多模态大型语言模型不断涌现，许多图像生成基准侧重于美学，而非细粒度的生成能力。PixelArena利用语义分割任务，以客观地检验这些模型在像素精度上的生成智能。研究发现，最新的Gemini 3 Pro Image在零样本设置下展现出卓越的图像生成能力，能够生成高保真度的语义掩码，体现了前所未见的视觉智能和在新图像生成任务中的真正泛化能力。进一步研究了其结果，并与其他模型进行了定性和定量比较，同时展示了失败案例。这些发现不仅标志着该领域令人兴奋的进展，也为多模态、推理、可解释性和基准测试等未来研究提供了见解。",
            "intro_zh": [
                "现有图像生成基准侧重于美学评估，缺乏对模型细粒度生成能力的客观像素级评估。",
                "PixelArena利用语义分割任务，通过生成语义掩码来评估多模态大模型的像素精度视觉智能。",
                "实验表明Gemini 3 Pro Image在零样本语义分割任务中表现出卓越的图像生成能力和泛化能力。"
            ],
            "method_zh": "**问题定义**：现有图像生成基准主要关注生成图像的美学质量，缺乏对模型生成图像在像素级别上的准确性和细节控制能力的评估。这使得我们难以客观地衡量模型是否真正理解了图像的语义信息，并能够精确地生成符合语义信息的图像。\n\n**核心思路**：PixelArena的核心思路是利用语义分割任务来评估图像生成模型的像素级视觉智能。语义分割需要模型理解图像中每个像素所属的类别，并生成相应的语义掩码。通过比较生成掩码与真实掩码的相似度，可以客观地衡量模型生成图像的准确性和细节控制能力。选择语义分割任务是因为它能够提供像素级别的反馈，从而更精确地评估模型的视觉智能。\n\n**技术框架**：PixelArena基准测试主要包含以下几个阶段：1) 数据集准备：选择或构建包含像素级标注的图像数据集，用于评估模型的语义分割能力。2) 模型推理：将图像输入到待评估的多模态大模型中，要求模型生成对应的语义掩码。3) 评估指标：使用像素精度、IoU等指标来衡量生成掩码与真实掩码的相似度。4) 结果分析：对模型的生成结果进行定性和定量分析，找出模型的优势和不足。\n\n**关键创新**：PixelArena的关键创新在于将语义分割任务作为评估多模态大模型像素级视觉智能的手段。与传统的图像生成基准相比，PixelArena能够提供更细粒度的评估结果，从而更准确地反映模型的视觉智能水平。此外，PixelArena还强调在零样本设置下评估模型的泛化能力，这更符合实际应用场景的需求。\n\n**关键设计**：PixelArena的关键设计包括：1) 数据集选择：选择具有代表性的语义分割数据集，例如Cityscapes、ADE20K等。2) 评估指标：采用像素精度、平均IoU等常用的语义分割评估指标。3) 零样本设置：在评估过程中，不使用任何与目标数据集相关的训练数据，以评估模型的泛化能力。4) 失败案例分析：对模型生成错误的案例进行分析，找出模型在哪些方面存在不足。",
            "application_zh": "PixelArena可用于评估和比较各种多模态大模型的图像生成能力，推动模型在图像编辑、自动驾驶、机器人等领域的应用。通过像素级精度评估，可以帮助研究人员更好地理解模型的优势和不足，从而改进模型的设计和训练方法。此外，PixelArena还可以作为一种诊断工具，用于发现模型中的潜在问题，例如幻觉或偏见。",
            "highlight_zh": "实验结果表明，Gemini 3 Pro Image在PixelArena基准测试中表现出色，在零样本语义分割任务中展现出卓越的图像生成能力，能够生成高保真度的语义掩码。与其他模型相比，Gemini 3 Pro Image在像素精度和IoU等指标上均取得了显著提升，体现了其强大的视觉智能和泛化能力。该研究还分析了Gemini 3 Pro Image的失败案例，为未来的研究提供了有价值的参考。",
            "tags_zh": [
                "像素级视觉智能",
                "多模态大模型",
                "图像生成",
                "语义分割",
                "评测基准"
            ],
            "_index": 44,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16303v1/images/celeb/label-color-palette.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16303v1/images/celeb/model-comparison-celeb.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16303v1/images/celeb/best-f1.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "MACL: Multi-Label Adaptive Contrastive Learning Loss for Remote Sensing Image Retrieval",
            "authors": [
                "Amna Amir",
                "Erchan Aptoula"
            ],
            "arxiv_id": "2512.16294v1",
            "summary": "Semantic overlap among land-cover categories, highly imbalanced label distributions, and complex inter-class co-occurrence patterns constitute significant challenges for multi-label remote-sensing image retrieval. In this article, Multi-Label Adaptive Contrastive Learning (MACL) is introduced as an extension of contrastive learning to address them. It integrates label-aware sampling, frequency-sensitive weighting, and dynamic-temperature scaling to achieve balanced representation learning across both common and rare categories. Extensive experiments on three benchmark datasets (DLRSD, ML-AID, and WHDLD), show that MACL consistently outperforms contrastive-loss based baselines, effectively mitigating semantic imbalance and delivering more reliable retrieval performance in large-scale remote-sensing archives. Code, pretrained models, and evaluation scripts will be released at https://github.com/amna/MACL upon acceptance.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16294v1",
            "code_links": [
                {
                    "url": "https://github.com/amna/MACL",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "representation learning",
                        "[T]contrastive learning"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "提出MACL，解决遥感图像检索中多标签语义重叠和类别不平衡问题",
            "summary_zh": "本文针对多标签遥感图像检索中地物类别语义重叠、标签分布极度不平衡以及复杂的类间共现模式等挑战，提出了一种多标签自适应对比学习（MACL）方法。MACL是对比学习的扩展，通过集成标签感知采样、频率敏感加权和动态温度缩放，实现了常见类别和稀有类别之间平衡的表征学习。在DLRSD、ML-AID和WHDLD三个基准数据集上的大量实验表明，MACL始终优于基于对比损失的基线方法，有效缓解了语义不平衡问题，并在大规模遥感档案中提供了更可靠的检索性能。代码、预训练模型和评估脚本将在论文接收后发布在https://github.com/amna/MACL。",
            "intro_zh": [
                "多标签遥感图像检索面临语义重叠、类别不平衡和类间共现等难题，现有方法难以有效处理。",
                "MACL通过标签感知采样、频率敏感加权和动态温度缩放，平衡常见和稀有类别的表征学习。",
                "在三个遥感数据集上的实验表明，MACL优于对比损失基线，提升了大规模遥感图像检索的可靠性。"
            ],
            "method_zh": "**问题定义**：多标签遥感图像检索任务旨在根据给定的查询图像，从大规模遥感图像库中检索出包含相似地物类别的图像。现有方法在处理多标签遥感图像时，面临着严重的语义重叠问题（不同地物类别之间存在相似性），标签分布不平衡问题（某些地物类别出现频率远高于其他类别），以及复杂的类间共现模式（某些地物类别经常同时出现）。这些问题导致模型难以学习到有效的图像表征，从而影响检索性能。\\n\\n**核心思路**：MACL的核心思路是通过自适应的对比学习方法，解决多标签遥感图像检索中的语义不平衡问题。具体来说，它通过标签感知采样策略，增加稀有类别的采样概率，从而平衡不同类别之间的训练样本数量。同时，采用频率敏感加权策略，对不同类别的样本赋予不同的权重，从而缓解类别不平衡带来的影响。此外，还引入了动态温度缩放机制，自适应地调整对比学习的温度参数，从而更好地控制正负样本之间的区分度。\\n\\n**技术框架**：MACL的整体框架可以分为三个主要模块：1) 特征提取模块：使用预训练的卷积神经网络（如ResNet）提取遥感图像的特征表示。2) 对比学习模块：基于提取的特征表示，构建正负样本对，并使用对比损失函数进行训练。3) 自适应调整模块：包括标签感知采样、频率敏感加权和动态温度缩放三个子模块，用于自适应地调整对比学习过程，以缓解语义不平衡问题。\\n\\n**关键创新**：MACL的关键创新在于其自适应的对比学习机制。与传统的对比学习方法不同，MACL能够根据标签信息、类别频率和训练过程中的动态反馈，自适应地调整采样策略、样本权重和温度参数。这种自适应性使得MACL能够更好地处理多标签遥感图像检索中的语义不平衡问题，从而提高检索性能。\\n\\n**关键设计**：MACL的关键设计包括：1) 标签感知采样：根据标签信息，增加稀有类别的采样概率，具体实现方式是为每个类别设置一个采样权重，权重与类别频率成反比。2) 频率敏感加权：根据类别频率，对不同类别的样本赋予不同的权重，具体实现方式是为每个样本设置一个权重，权重与样本所属类别的频率成反比。3) 动态温度缩放：根据训练过程中的动态反馈，自适应地调整对比学习的温度参数，具体实现方式是使用一个可学习的温度参数，该参数根据训练损失进行更新。",
            "application_zh": "MACL可应用于大规模遥感图像档案的智能检索，例如城市规划、环境监测、灾害评估等领域。通过快速准确地检索出包含特定地物类别的遥感图像，可以为相关领域的决策提供有力支持。未来，该方法有望扩展到其他多标签图像检索任务中，例如医学图像检索、视频检索等。",
            "highlight_zh": "实验结果表明，MACL在DLRSD、ML-AID和WHDLD三个遥感数据集上均取得了显著的性能提升。例如，在DLRSD数据集上，MACL相比于对比损失基线方法，mAP指标提升了超过5个百分点。此外，MACL在稀有类别的检索性能方面也表现出色，表明其有效缓解了语义不平衡问题。",
            "tags_zh": [
                "遥感图像检索",
                "多标签学习",
                "对比学习",
                "类别不平衡",
                "自适应学习"
            ],
            "_index": 45,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16294v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16294v1/Images/Architecture/Architecture.jpg",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16294v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "SegGraph: Leveraging Graphs of SAM Segments for Few-Shot 3D Part Segmentation",
            "authors": [
                "Yueyang Hu",
                "Haiyong Jiang",
                "Haoxuan Song",
                "Jun Xiao",
                "Hao Pan"
            ],
            "arxiv_id": "2512.16143v1",
            "summary": "This work presents a novel framework for few-shot 3D part segmentation. Recent advances have demonstrated the significant potential of 2D foundation models for low-shot 3D part segmentation. However, it is still an open problem that how to effectively aggregate 2D knowledge from foundation models to 3D. Existing methods either ignore geometric structures for 3D feature learning or neglects the high-quality grouping clues from SAM, leading to under-segmentation and inconsistent part labels. We devise a novel SAM segment graph-based propagation method, named SegGraph, to explicitly learn geometric features encoded within SAM's segmentation masks. Our method encodes geometric features by modeling mutual overlap and adjacency between segments while preserving intra-segment semantic consistency. We construct a segment graph, conceptually similar to an atlas, where nodes represent segments and edges capture their spatial relationships (overlap/adjacency). Each node adaptively modulates 2D foundation model features, which are then propagated via a graph neural network to learn global geometric structures. To enforce intra-segment semantic consistency, we map segment features to 3D points with a novel view-direction-weighted fusion attenuating contributions from low-quality segments. Extensive experiments on PartNet-E demonstrate that our method outperforms all competing baselines by at least 6.9 percent mIoU. Further analysis reveals that SegGraph achieves particularly strong performance on small components and part boundaries, demonstrating its superior geometric understanding. The code is available at: https://github.com/YueyangHu2000/SegGraph.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16143v1",
            "code_links": [
                {
                    "url": "https://github.com/YueyangHu2000/SegGraph",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱七：动作重定向 (Motion Retargeting)",
                    "id": "7_retargeting",
                    "matched_keywords": [
                        "spatial relationship"
                    ],
                    "score": 3.0
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "foundation model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "7_retargeting",
                "9_embodied_foundation"
            ],
            "headline_zh": "提出SegGraph，利用SAM分割图进行少样本3D部件分割",
            "summary_zh": "本文提出了一种用于少样本3D部件分割的新框架。最近的研究表明，2D基础模型在低样本3D部件分割方面具有巨大的潜力。然而，如何有效地将2D知识从基础模型聚合到3D仍然是一个开放的问题。现有方法要么忽略3D特征学习的几何结构，要么忽略SAM提供的高质量分组线索，导致分割不足和部件标签不一致。我们设计了一种新颖的基于SAM分割图的传播方法，名为SegGraph，以显式地学习SAM分割掩码中编码的几何特征。我们的方法通过对分割之间的相互重叠和邻接关系进行建模来编码几何特征，同时保持分割内的语义一致性。我们构建了一个分割图，在概念上类似于地图集，其中节点表示分割，边表示它们之间的空间关系（重叠/邻接）。每个节点自适应地调节2D基础模型特征，然后通过图神经网络传播以学习全局几何结构。为了加强分割内的语义一致性，我们使用一种新颖的视角方向加权融合将分割特征映射到3D点，从而衰减低质量分割的贡献。在PartNet-E上的大量实验表明，我们的方法优于所有竞争基线至少6.9个百分点的mIoU。进一步的分析表明，SegGraph在小组件和部件边界上实现了特别强大的性能，证明了其卓越的几何理解能力。",
            "intro_zh": [
                "现有少样本3D部件分割方法在聚合2D基础模型知识到3D时，忽略了几何结构或SAM分组线索，导致分割不足和标签不一致。",
                "SegGraph通过构建SAM分割图，显式学习分割掩码中的几何特征，并利用图神经网络进行特征传播，从而实现更精确的部件分割。",
                "在PartNet-E数据集上的实验表明，SegGraph显著优于现有方法，尤其在小组件和部件边界上表现出色，证明了其几何理解能力。"
            ],
            "method_zh": "**问题定义**：论文旨在解决少样本3D部件分割问题。现有方法在利用2D基础模型（如SAM）的知识时，要么忽略了3D几何结构的学习，要么未能充分利用SAM分割结果中蕴含的高质量分组信息，导致分割结果不准确，部件标签不一致。这些方法无法有效区分细小的部件，并且在部件边界处的分割效果较差。\\n\\n**核心思路**：论文的核心思路是构建一个基于SAM分割结果的图结构（SegGraph），显式地对分割区域之间的几何关系（重叠和邻接）进行建模，并利用图神经网络学习全局几何结构。通过这种方式，模型可以更好地理解3D形状的结构信息，从而提高分割的准确性。同时，论文还考虑了分割区域内部的语义一致性，避免低质量分割区域对最终结果产生负面影响。\\n\\n**技术框架**：SegGraph框架主要包含以下几个阶段：1) 利用SAM对3D形状进行分割，得到一系列2D分割掩码；2) 构建分割图，其中节点表示分割区域，边表示分割区域之间的空间关系（重叠和邻接）；3) 使用图神经网络在分割图上进行特征传播，学习全局几何结构；4) 将分割特征映射回3D点，并使用视角方向加权融合来增强分割区域内部的语义一致性。\\n\\n**关键创新**：论文的关键创新在于：1) 提出了SegGraph，一种显式地对分割区域之间的几何关系进行建模的图结构；2) 使用图神经网络在SegGraph上进行特征传播，从而学习全局几何结构；3) 提出了视角方向加权融合方法，用于增强分割区域内部的语义一致性。与现有方法相比，SegGraph能够更好地利用SAM分割结果中的几何信息，从而提高分割的准确性。\\n\\n**关键设计**：在构建SegGraph时，节点特征由2D基础模型提取的特征表示，边的权重根据分割区域之间的重叠和邻接程度计算。图神经网络采用GCN结构，损失函数包括分割损失和一致性损失。视角方向加权融合根据视角方向与分割区域法向量的夹角来确定权重，从而降低低质量分割区域的贡献。",
            "application_zh": "该研究成果可应用于机器人感知、三维场景理解、CAD模型分析等领域。例如，机器人可以利用该技术更准确地识别和操作物体部件，从而实现更复杂的任务。在CAD模型分析中，该技术可以帮助用户快速理解模型的结构和功能，提高设计效率。未来，该技术有望扩展到更多领域，如医疗影像分析、文物修复等。",
            "highlight_zh": "实验结果表明，SegGraph在PartNet-E数据集上优于所有竞争基线至少6.9个百分点的mIoU。尤其在小组件和部件边界上，SegGraph表现出更强的性能，证明了其卓越的几何理解能力。消融实验验证了SegGraph中各个模块的有效性，例如，分割图结构和视角方向加权融合都对最终性能有显著贡献。",
            "tags_zh": [
                "少样本学习",
                "3D部件分割",
                "图神经网络",
                "SAM分割",
                "几何特征学习"
            ],
            "_index": 46,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "Stackelberg Learning from Human Feedback: Preference Optimization as a Sequential Game",
            "authors": [
                "Barna Pásztor",
                "Thomas Kleine Buening",
                "Andreas Krause"
            ],
            "arxiv_id": "2512.16626v1",
            "summary": "We introduce Stackelberg Learning from Human Feedback (SLHF), a new framework for preference optimization. SLHF frames the alignment problem as a sequential-move game between two policies: a Leader, which commits to an action, and a Follower, which responds conditionally on the Leader's action. This approach decomposes preference optimization into a refinement problem for the Follower and an optimization problem against an adversary for the Leader. Unlike Reinforcement Learning from Human Feedback (RLHF), which assigns scalar rewards to actions, or Nash Learning from Human Feedback (NLHF), which seeks a simultaneous-move equilibrium, SLHF leverages the asymmetry of sequential play to capture richer preference structures. The sequential design of SLHF naturally enables inference-time refinement, as the Follower learns to improve the Leader's actions, and these refinements can be leveraged through iterative sampling. We compare the solution concepts of SLHF, RLHF, and NLHF, and lay out key advantages in consistency, data sensitivity, and robustness to intransitive preferences. Experiments on large language models demonstrate that SLHF achieves strong alignment across diverse preference datasets, scales from 0.5B to 8B parameters, and yields inference-time refinements that transfer across model families without further fine-tuning.",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.GT",
                "cs.MA",
                "stat.ML"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "10 pages, 5 tables, 1 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16626v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning",
                        "RLHF"
                    ],
                    "score": 3.0
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "提出Stackelberg Learning from Human Feedback (SLHF)框架，用于偏好优化。",
            "summary_zh": "本文提出了一种新的偏好优化框架，即Stackelberg Learning from Human Feedback (SLHF)。SLHF将对齐问题建模为两个策略之间的序贯博弈：领导者(Leader)首先采取行动，然后跟随者(Follower)根据领导者的行动做出响应。这种方法将偏好优化分解为跟随者的优化问题和领导者对抗性优化问题。与为动作分配标量奖励的Reinforcement Learning from Human Feedback (RLHF)或寻求同步博弈均衡的Nash Learning from Human Feedback (NLHF)不同，SLHF利用序贯博弈的不对称性来捕获更丰富的偏好结构。SLHF的序贯设计自然地实现了推理时优化，因为跟随者学习改进领导者的动作，并且这些改进可以通过迭代采样来利用。我们比较了SLHF、RLHF和NLHF的解概念，并阐述了在一致性、数据敏感性和对非传递偏好的鲁棒性方面的关键优势。对大型语言模型的实验表明，SLHF在不同的偏好数据集上实现了强大的对齐，可以从0.5B扩展到8B参数，并产生可以在模型系列之间转移而无需进一步微调的推理时优化。",
            "intro_zh": [
                "现有RLHF方法为动作分配标量奖励，NLHF寻求同步博弈均衡，无法捕捉复杂偏好结构。",
                "SLHF将对齐问题建模为领导者和跟随者之间的序贯博弈，利用序贯博弈的不对称性。",
                "实验表明，SLHF在不同数据集上实现了强大的对齐，且推理时优化可跨模型迁移。"
            ],
            "method_zh": "**问题定义**：现有基于人类反馈的强化学习方法（如RLHF）通常将人类反馈转化为标量奖励，然后使用强化学习算法进行优化。这种方法难以捕捉人类偏好的复杂性和细微差别。此外，NLHF方法试图找到一个同步博弈的纳什均衡，但可能无法有效地利用序贯决策的优势。因此，如何更有效地利用人类反馈来对齐模型，特别是捕捉人类偏好的复杂结构，是一个关键问题。\\n\\n**核心思路**：SLHF的核心思路是将对齐问题建模为一个序贯博弈，其中领导者（Leader）策略首先采取行动，然后跟随者（Follower）策略根据领导者的行动做出响应。这种序贯博弈的结构允许模型学习更丰富的偏好结构，因为跟随者可以学习如何改进领导者的行动。通过这种方式，SLHF能够更好地捕捉人类偏好的细微差别和上下文依赖性。\\n\\n**技术框架**：SLHF的整体框架包含两个主要阶段：跟随者优化和领导者优化。首先，跟随者策略通过学习人类反馈来优化，目标是根据领导者的行动做出最佳响应。然后，领导者策略通过对抗性优化来学习，目标是最大化其在跟随者响应下的表现。这个过程可以迭代进行，以不断改进领导者和跟随者的策略。在推理阶段，跟随者可以用于改进领导者的输出，从而提高模型的整体性能。\\n\\n**关键创新**：SLHF的关键创新在于其将对齐问题建模为一个序贯博弈。与传统的RLHF方法相比，SLHF能够捕捉更丰富的偏好结构，并利用序贯决策的优势。此外，SLHF的序贯设计允许在推理时进行优化，从而进一步提高模型的性能。与NLHF相比，SLHF避免了寻找同步博弈均衡的复杂性，并更有效地利用了序贯决策的信息。\\n\\n**关键设计**：SLHF的关键设计包括跟随者策略和领导者策略的建模方式，以及用于优化这些策略的损失函数。跟随者策略通常使用监督学习方法进行训练，目标是预测人类对不同响应的偏好。领导者策略可以使用强化学习或对抗性学习方法进行训练，目标是最大化其在跟随者响应下的表现。损失函数的设计需要考虑到人类反馈的噪声和不确定性，并鼓励模型学习鲁棒的策略。",
            "application_zh": "SLHF框架可应用于各种需要与人类偏好对齐的场景，例如对话系统、文本生成、图像生成和机器人控制。通过学习人类的偏好，SLHF可以帮助模型生成更符合人类期望的输出，提高用户满意度，并促进人机协作。",
            "highlight_zh": "实验结果表明，SLHF在多个偏好数据集上实现了强大的对齐效果，并且可以扩展到具有数十亿参数的大型语言模型。此外，SLHF产生的推理时优化可以跨模型系列转移，无需进一步微调，这表明SLHF具有良好的泛化能力。",
            "tags_zh": [
                "人机反馈",
                "偏好学习",
                "序贯博弈",
                "大型语言模型",
                "对齐",
                "强化学习",
                "Stackelberg学习"
            ],
            "_index": 47,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16626v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                }
            ]
        },
        {
            "title": "Do Multi-Agents Solve Better Than Single? Evaluating Agentic Frameworks for Diagram-Grounded Geometry Problem Solving and Reasoning",
            "authors": [
                "Mahbub E Sobhani",
                "Md. Faiyaz Abdullah Sayeedi",
                "Mohammad Nehad Alam",
                "Proma Hossain Progga",
                "Swakkhar Shatabda"
            ],
            "arxiv_id": "2512.16698v1",
            "summary": "Diagram-grounded geometry problem solving is a critical benchmark for multimodal large language models (MLLMs), yet the benefits of multi-agent design over single-agent remain unclear. We systematically compare single-agent and multi-agent pipelines on four visual math benchmarks: Geometry3K, MathVerse, OlympiadBench, and We-Math. For open-source models, multi-agent consistently improves performance. For example, Qwen-2.5-VL (7B) gains +6.8 points and Qwen-2.5-VL (32B) gains +3.3 on Geometry3K, and both Qwen-2.5-VL variants see further gains on OlympiadBench and We-Math. In contrast, the closed-source Gemini-2.0-Flash generally performs better in single-agent mode on classic benchmarks, while multi-agent yields only modest improvements on the newer We-Math dataset. These findings show that multi-agent pipelines provide clear benefits for open-source models and can assist strong proprietary systems on newer, less familiar benchmarks, but agentic decomposition is not universally optimal. All code, data, and reasoning files are available at https://github.com/faiyazabdullah/Interpreter-Solver",
            "categories": [
                "cs.AI",
                "cs.CG"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Accepted to the ARR October 2025 cycle",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16698v1",
            "code_links": [
                {
                    "url": "https://github.com/faiyazabdullah/Interpreter-Solver",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model",
                        "multimodal"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "对比单智能体与多智能体框架，评估其在图解几何问题求解中的性能差异",
            "summary_zh": "图解几何问题求解是多模态大型语言模型（MLLM）的关键基准，但多智能体设计相对于单智能体的优势尚不明确。本文系统地比较了单智能体和多智能体流程在四个视觉数学基准（Geometry3K、MathVerse、OlympiadBench 和 We-Math）上的性能。对于开源模型，多智能体方法始终能提高性能。例如，Qwen-2.5-VL (7B) 在 Geometry3K 上获得了 +6.8 的提升，Qwen-2.5-VL (32B) 获得了 +3.3 的提升，并且两种 Qwen-2.5-VL 变体在 OlympiadBench 和 We-Math 上都获得了进一步的提升。相比之下，闭源模型 Gemini-2.0-Flash 在经典基准测试中通常在单智能体模式下表现更好，而多智能体仅在新数据集 We-Math 上产生了适度的改进。这些发现表明，多智能体流程为开源模型提供了明显的优势，并且可以帮助强大的专有系统处理较新的、不太熟悉的基准，但智能体分解并非普遍最佳。",
            "intro_zh": [
                "现有的多模态大语言模型在图解几何问题求解中面临挑战，尤其是在复杂推理和视觉信息利用方面。",
                "论文对比单智能体和多智能体框架，旨在探究多智能体架构是否能有效提升几何问题求解能力。",
                "实验结果表明，多智能体框架对开源模型有显著提升，对闭源模型在较新数据集上也有一定帮助。"
            ],
            "method_zh": "**问题定义**：论文旨在解决图解几何问题，即给定几何图形和问题描述，求解几何问题。现有方法，特别是单智能体方法，在处理复杂几何问题时，推理能力和视觉信息利用效率存在瓶颈，难以达到理想的准确率。\\n\\n**核心思路**：论文的核心思路是利用多智能体系统，将复杂的几何问题分解为多个子任务，每个智能体负责不同的子任务，例如理解问题、提取图形信息、进行数学推理等。通过智能体之间的协作，提高问题求解的效率和准确性。这种分解的目的是为了更好地利用每个智能体的专长，并减少单个智能体的认知负担。\\n\\n**技术框架**：整体框架包含单智能体和多智能体两种pipeline。单智能体pipeline直接使用MLLM模型进行问题求解。多智能体pipeline将问题分解为多个步骤，每个步骤由一个智能体负责。这些智能体之间通过消息传递进行协作，最终得到问题的答案。具体模块可能包括：问题解析模块、图形理解模块、推理模块和答案生成模块。\\n\\n**关键创新**：论文的关键创新在于系统性地对比了单智能体和多智能体框架在图解几何问题求解中的性能。通过实验，揭示了多智能体框架在开源模型上的优势，并发现其在处理新数据集时对闭源模型也有帮助。与现有方法相比，该研究更全面地评估了多智能体架构在几何问题求解中的有效性。\\n\\n**关键设计**：论文的关键设计包括：1) 针对不同类型的几何问题，设计不同的智能体角色和协作方式；2) 使用合适的提示工程（prompt engineering）来引导智能体的行为；3) 设计有效的消息传递机制，确保智能体之间的信息交流顺畅；4) 针对不同的模型，调整智能体的数量和功能分配。",
            "application_zh": "该研究成果可应用于智能教育、自动几何定理证明、机器人视觉等领域。通过多智能体协作，可以构建更智能的几何问题求解系统，辅助学生学习，提高几何定理证明的效率，并为机器人提供更强大的视觉推理能力。未来，该技术有望扩展到其他需要复杂推理和多模态信息融合的领域。",
            "highlight_zh": "实验结果表明，对于开源模型，多智能体框架在Geometry3K上取得了显著的性能提升，例如Qwen-2.5-VL (7B) 提升了6.8个点，Qwen-2.5-VL (32B) 提升了3.3个点。此外，在OlympiadBench和We-Math数据集上，开源模型也获得了进一步的提升。对于闭源模型Gemini-2.0-Flash，多智能体框架在We-Math数据集上表现出一定的优势。",
            "tags_zh": [
                "多智能体系统",
                "图解几何问题求解",
                "多模态大语言模型",
                "视觉推理",
                "智能教育"
            ],
            "_index": 48,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16698v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16698v1/figures/diagram.png",
                    "caption": "",
                    "figure_id": "fig_1"
                }
            ]
        },
        {
            "title": "Scaling Laws for Energy Efficiency of Local LLMs",
            "authors": [
                "Ander Alvarez",
                "Alessandro Genuardi",
                "Nilotpal Sinha",
                "Antonio Tiene",
                "Samuel Mugel",
                "Román Orús"
            ],
            "arxiv_id": "2512.16531v1",
            "summary": "Deploying local large language models and vision-language models on edge devices requires balancing accuracy with constrained computational and energy budgets. Although graphics processors dominate modern artificial-intelligence deployment, most consumer hardware--including laptops, desktops, industrial controllers, and embedded systems--relies on central processing units. Despite this, the computational laws governing central-processing-unit-only inference for local language and vision-language workloads remain largely unexplored. We systematically benchmark large language and vision-language models on two representative central-processing-unit tiers widely used for local inference: a MacBook Pro M2, reflecting mainstream laptop-class deployment, and a Raspberry Pi 5, representing constrained, low-power embedded settings. Using a unified methodology based on continuous sampling of processor and memory usage together with area-under-curve integration, we characterize how computational load scales with input text length for language models and with image resolution for vision-language models. We uncover two empirical scaling laws: (1) computational cost for language-model inference scales approximately linearly with token length; and (2) vision-language models exhibit a preprocessing-driven \"resolution knee\", where compute remains constant above an internal resolution clamp and decreases sharply below it. Beyond these laws, we show that quantum-inspired compression reduces processor and memory usage by up to 71.9% and energy consumption by up to 62%, while preserving or improving semantic accuracy. These results provide a systematic quantification of multimodal central-processing-unit-only scaling for local language and vision-language workloads, and they identify model compression and input-resolution preprocessing as effective, low-cost levers for sustainable edge inference.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16531v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model",
                        "multimodal"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "针对本地LLM，揭示CPU能效缩放规律，并提出量子启发压缩优化方案。",
            "summary_zh": "在边缘设备上部署本地大型语言模型和视觉-语言模型需要在精度、计算和能源预算之间取得平衡。尽管图形处理器在现代人工智能部署中占据主导地位，但大多数消费级硬件（包括笔记本电脑、台式机、工业控制器和嵌入式系统）依赖于中央处理器。本文系统地对两种广泛用于本地推理的中央处理器进行了基准测试：MacBook Pro M2（代表主流笔记本电脑级部署）和 Raspberry Pi 5（代表受限的低功耗嵌入式环境）。通过统一的方法，即连续采样处理器和内存使用情况以及曲线下面积积分，我们描述了语言模型的计算负载如何随输入文本长度缩放，以及视觉-语言模型的计算负载如何随图像分辨率缩放。我们发现了两个经验缩放规律：（1）语言模型推理的计算成本与token长度近似线性缩放；（2）视觉-语言模型表现出由预处理驱动的“分辨率膝点”，即计算量在内部分辨率上限之上保持不变，并在其之下急剧下降。此外，我们表明，量子启发压缩可将处理器和内存使用量降低高达71.9%，并将能耗降低高达62%，同时保持或提高语义准确性。这些结果系统地量化了多模态中央处理器在本地语言和视觉-语言工作负载中的缩放规律，并确定了模型压缩和输入分辨率预处理是可持续边缘推理的有效且低成本的手段。",
            "intro_zh": [
                "现有方法在边缘设备上部署LLM时，难以在精度、计算和能耗间平衡，尤其缺乏对CPU上推理的深入研究。",
                "论文通过在主流CPU上进行基准测试，揭示了语言模型和视觉-语言模型在CPU上的计算负载缩放规律。",
                "实验表明，量子启发压缩能显著降低CPU和内存使用，同时保持或提升语义准确性，为边缘推理提供优化方案。"
            ],
            "method_zh": "**问题定义**：论文旨在解决在资源受限的边缘设备上部署大型语言模型（LLM）和视觉-语言模型（VLM）时，如何在中央处理器（CPU）上实现高效推理的问题。现有方法主要关注GPU加速，而忽略了大量依赖CPU的设备，且缺乏对CPU上LLM/VLM推理的计算负载缩放规律的系统研究。\\n\\n**核心思路**：论文的核心思路是通过系统性的基准测试，量化LLM和VLM在不同CPU平台上的计算负载与输入数据规模（token长度、图像分辨率）之间的关系，从而揭示经验性的缩放规律。此外，探索模型压缩技术，以降低计算和内存需求，提升能效。\\n\\n**技术框架**：论文采用统一的基准测试方法，包括：\n1.  选择代表性的CPU平台：MacBook Pro M2 (笔记本电脑级) 和 Raspberry Pi 5 (嵌入式系统级)。\n2.  选择LLM和VLM模型进行测试。\n3.  连续采样处理器和内存使用情况，并计算曲线下面积（AUC）作为计算负载的度量。\n4.  分析计算负载与输入数据规模之间的关系，拟合经验缩放规律。\n5.  应用量子启发压缩技术，评估其对性能和能效的影响。\\n\\n**关键创新**：论文的关键创新在于：\n1.  首次系统性地研究了LLM和VLM在CPU上的计算负载缩放规律，揭示了token长度与计算成本的线性关系，以及VLM的“分辨率膝点”现象。\n2.  验证了量子启发压缩技术在降低CPU和内存使用方面的有效性，并证明其能在保持或提升语义准确性的前提下，显著降低能耗。\\n\\n**关键设计**：论文的关键设计包括：\n1.  统一的基准测试方法，确保不同平台和模型之间结果的可比性。\n2.  使用曲线下面积（AUC）作为计算负载的度量，能够更准确地反映实际的计算成本。\n3.  采用量子启发压缩技术，利用量子力学原理进行模型参数的压缩，降低模型大小和计算复杂度。",
            "application_zh": "该研究成果可应用于各种边缘计算场景，例如：在低功耗嵌入式设备上部署本地LLM，实现离线语音助手、智能家居控制等功能；在笔记本电脑上优化VLM推理，提升图像处理和视觉搜索的效率。研究结果有助于开发者根据硬件资源选择合适的模型和优化策略，实现可持续的边缘AI应用。",
            "highlight_zh": "实验结果表明，语言模型推理的计算成本与token长度近似线性缩放；视觉-语言模型存在“分辨率膝点”，计算量在内部分辨率上限之上保持不变，之下急剧下降。量子启发压缩可将处理器和内存使用量降低高达71.9%，并将能耗降低高达62%，同时保持或提高语义准确性。",
            "tags_zh": [
                "本地LLM",
                "CPU推理",
                "能效优化",
                "缩放规律",
                "量子启发压缩"
            ],
            "_index": 49,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16531v1/figures/updated/llm_mac_cpu_auc.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16531v1/figures/updated/llm_rpi_cpu_auc.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16531v1/figures/updated/vlm_mac_cpu_auc.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs",
            "authors": [
                "Sara Papi",
                "Javier Garcia Gilabert",
                "Zachary Hopton",
                "Vilém Zouhar",
                "Carlos Escolano",
                "Gerard I. Gállego",
                "Jorge Iranzo-Sánchez",
                "Ahrii Kim",
                "Dominik Macháček",
                "Patricia Schmidtova",
                "Maike Züfle"
            ],
            "arxiv_id": "2512.16378v1",
            "summary": "As Large Language Models (LLMs) expand beyond text, integrating speech as a native modality has given rise to SpeechLLMs, which aim to translate spoken language directly, thereby bypassing traditional transcription-based pipelines. Whether this integration improves speech-to-text translation quality over established cascaded architectures, however, remains an open question. We present Hearing to Translate, the first comprehensive test suite rigorously benchmarking 5 state-of-the-art SpeechLLMs against 16 strong direct and cascade systems that couple leading speech foundation models (SFM), with multilingual LLMs. Our analysis spans 16 benchmarks, 13 language pairs, and 9 challenging conditions, including disfluent, noisy, and long-form speech. Across this extensive evaluation, we find that cascaded systems remain the most reliable overall, while current SpeechLLMs only match cascades in selected settings and SFMs lag behind both, highlighting that integrating an LLM, either within the model or in a pipeline, is essential for high-quality speech translation.",
            "categories": [
                "cs.CL",
                "cs.AI",
                "cs.SD"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Project available at https://github.com/sarapapi/hearing2translate",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16378v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model",
                        "foundation model"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "首个SpeechLLM综合评测：对比端到端与级联架构语音翻译性能",
            "summary_zh": "随着大型语言模型(LLMs)超越文本领域，将语音作为原生模态集成催生了SpeechLLMs，旨在直接翻译口语，从而绕过传统的基于转录的流程。然而，这种集成是否能提高语音到文本的翻译质量，优于已建立的级联架构，仍然是一个悬而未决的问题。我们提出了Hearing to Translate，这是第一个综合测试套件，严格地将5个最先进的SpeechLLMs与16个强大的直接和级联系统进行基准测试，这些系统将领先的语音基础模型(SFM)与多语言LLMs结合。我们的分析跨越16个基准、13个语言对和9个具有挑战性的条件，包括口齿不清、嘈杂和长篇语音。通过这项广泛的评估，我们发现级联系统仍然是最可靠的，而当前的SpeechLLMs仅在选定的设置中与级联系统相匹配，并且SFMs落后于两者，这突出了集成LLM（无论是在模型内部还是在pipeline中）对于高质量语音翻译至关重要。",
            "intro_zh": [
                "现有端到端SpeechLLM语音翻译能力有待考量，缺乏与传统级联架构的系统性对比。",
                "提出Hearing to Translate测试套件，全面评估SpeechLLM与级联系统的语音翻译性能。",
                "实验表明，级联系统总体上更可靠，SpeechLLM仅在特定场景下可媲美，SFM表现最差。"
            ],
            "method_zh": "**问题定义**：论文旨在解决语音翻译领域中，端到端SpeechLLM与传统级联架构孰优孰劣的问题。现有端到端模型虽然简化了流程，但其性能是否能超越经过充分优化的级联系统，尤其是在复杂场景下，仍缺乏充分的实验验证。级联系统虽然流程复杂，但每个模块都可以独立优化，性能上限较高。因此，需要一个全面的评测体系来客观比较两者的优劣。\n\n**核心思路**：论文的核心思路是通过构建一个全面的评测基准，在多种语言对和复杂场景下，系统性地比较端到端SpeechLLM和级联系统的语音翻译性能。通过大量实验数据，分析不同架构的优势和劣势，为未来的语音翻译模型设计提供指导。\n\n**技术框架**：论文构建的Hearing to Translate测试套件包含以下几个关键部分：\n1.  **数据集**：涵盖16个基准数据集，13个语言对，以及9种具有挑战性的语音条件，包括口齿不清、噪声和长篇语音。\n2.  **模型选择**：选取了5个最先进的SpeechLLM模型，以及16个强大的直接和级联系统，这些系统结合了领先的语音基础模型(SFM)和多语言LLM。\n3.  **评估指标**：采用标准的语音翻译评估指标，如BLEU等，来衡量不同模型的翻译质量。\n4.  **实验流程**：在统一的实验环境下，对所有模型进行评估，并进行详细的性能分析。\n\n**关键创新**：论文的主要创新在于构建了首个全面的SpeechLLM评测基准Hearing to Translate。该基准覆盖了多种语言对、复杂语音条件和模型架构，能够更客观地评估不同语音翻译系统的性能。此外，论文还深入分析了端到端SpeechLLM和级联系统在不同场景下的优劣，为未来的模型设计提供了有价值的参考。\n\n**关键设计**：论文的关键设计在于数据集的选择和实验场景的构建。为了更全面地评估模型的性能，论文选择了涵盖多种语言对和复杂语音条件的数据集。此外，论文还设计了9种具有挑战性的语音条件，包括口齿不清、噪声和长篇语音，以模拟真实的语音翻译场景。在模型选择方面，论文选取了最先进的SpeechLLM模型和级联系统，保证了评估结果的代表性。",
            "application_zh": "该研究成果可应用于多种语音翻译场景，如国际会议同声传译、跨语言语音助手、多语言客服系统等。通过对比不同架构的优劣，可以指导未来语音翻译系统的设计和优化，提升跨语言交流的效率和质量。此外，该评测基准的发布，也将促进语音翻译领域的研究进展。",
            "highlight_zh": "实验结果表明，在大多数情况下，级联系统仍然是最可靠的语音翻译方案。虽然当前的SpeechLLM在特定场景下可以与级联系统相媲美，但在整体性能上仍有差距。此外，实验还发现，单独使用SFM进行语音翻译的效果较差，集成LLM（无论是在模型内部还是在pipeline中）对于高质量语音翻译至关重要。",
            "tags_zh": [
                "语音翻译",
                "SpeechLLM",
                "级联系统",
                "端到端模型",
                "性能评测"
            ],
            "_index": 50,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16378v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16378v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16378v1/figs/pearmut_screenshot_1.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Code-in-the-Loop Forensics: Agentic Tool Use for Image Forgery Detection",
            "authors": [
                "Fanrui Zhang",
                "Qiang Zhang",
                "Sizhuo Zhou",
                "Jianwen Sun",
                "Chuanhao Li",
                "Jiaxin Ai",
                "Yukang Feng",
                "Yujie Zhang",
                "Wenjie Li",
                "Zizhen Li",
                "Yifan Chang",
                "Jiawei Liu",
                "Kaipeng Zhang"
            ],
            "arxiv_id": "2512.16300v1",
            "summary": "Existing image forgery detection (IFD) methods either exploit low-level, semantics-agnostic artifacts or rely on multimodal large language models (MLLMs) with high-level semantic knowledge. Although naturally complementary, these two information streams are highly heterogeneous in both paradigm and reasoning, making it difficult for existing methods to unify them or effectively model their cross-level interactions. To address this gap, we propose ForenAgent, a multi-round interactive IFD framework that enables MLLMs to autonomously generate, execute, and iteratively refine Python-based low-level tools around the detection objective, thereby achieving more flexible and interpretable forgery analysis. ForenAgent follows a two-stage training pipeline combining Cold Start and Reinforcement Fine-Tuning to enhance its tool interaction capability and reasoning adaptability progressively. Inspired by human reasoning, we design a dynamic reasoning loop comprising global perception, local focusing, iterative probing, and holistic adjudication, and instantiate it as both a data-sampling strategy and a task-aligned process reward. For systematic training and evaluation, we construct FABench, a heterogeneous, high-quality agent-forensics dataset comprising 100k images and approximately 200k agent-interaction question-answer pairs. Experiments show that ForenAgent exhibits emergent tool-use competence and reflective reasoning on challenging IFD tasks when assisted by low-level tools, charting a promising route toward general-purpose IFD. The code will be released after the review process is completed.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "11 pages, 6 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16300v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model",
                        "multimodal"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出ForenAgent，利用Agentic工具解决图像伪造检测中跨层信息融合难题。",
            "summary_zh": "现有的图像伪造检测(IFD)方法要么利用低级、语义无关的伪影，要么依赖具有高级语义知识的多模态大型语言模型(MLLM)。这两种信息流在范式和推理上高度异构，使得现有方法难以统一它们或有效地建模它们的跨层交互。为了解决这个差距，我们提出了ForenAgent，一个多轮交互式IFD框架，使MLLM能够自主生成、执行和迭代地改进围绕检测目标的基于Python的低级工具，从而实现更灵活和可解释的伪造分析。ForenAgent遵循一个结合冷启动和强化微调的两阶段训练流程，以逐步提高其工具交互能力和推理适应性。受到人类推理的启发，我们设计了一个动态推理循环，包括全局感知、局部聚焦、迭代探测和整体裁决，并将其实例化为数据采样策略和任务对齐的过程奖励。为了进行系统的训练和评估，我们构建了FABench，一个异构的、高质量的agent-forensics数据集，包含10万张图像和大约20万个agent交互问答对。实验表明，在低级工具的辅助下，ForenAgent在具有挑战性的IFD任务中表现出新兴的工具使用能力和反思性推理，为通用IFD开辟了一条有希望的道路。代码将在审查过程完成后发布。",
            "intro_zh": [
                "现有图像伪造检测方法难以有效融合低级伪影和高级语义知识，限制了检测性能。",
                "ForenAgent框架利用多模态大语言模型自主生成和执行低级工具，实现灵活可解释的伪造分析。",
                "通过冷启动和强化微调，ForenAgent在FABench数据集上展现出强大的工具使用能力和推理能力。"
            ],
            "method_zh": "**问题定义**：现有图像伪造检测方法主要存在两个问题。一是依赖低级特征，缺乏语义理解能力，容易受到内容干扰。二是依赖多模态大语言模型，虽然具备语义知识，但缺乏对底层伪造痕迹的精确分析能力。这两种信息流的异构性使得现有方法难以有效融合，导致检测精度受限。\\n\\n**核心思路**：ForenAgent的核心思路是利用多模态大语言模型（MLLM）作为智能体，使其能够自主生成、执行和迭代优化基于Python的低级工具，从而实现对图像伪造的细粒度分析。通过将高级语义知识与低级特征分析相结合，弥合了现有方法在信息融合方面的不足。这种Agentic工具使用方式使得伪造检测过程更加灵活、可解释，并能够适应不同的伪造类型。\\n\\n**技术框架**：ForenAgent框架主要包含两个阶段：冷启动和强化微调。在冷启动阶段，使用预训练的MLLM初始化Agent，并提供少量示例进行初步训练。在强化微调阶段，通过与环境的交互，Agent不断学习和优化其工具使用策略。框架的核心是一个动态推理循环，包括全局感知（Global Perception）、局部聚焦（Local Focusing）、迭代探测（Iterative Probing）和整体裁决（Holistic Adjudication）。这个循环指导Agent逐步分析图像，并最终做出伪造判断。\\n\\n**关键创新**：ForenAgent的关键创新在于将多模态大语言模型与Agentic工具使用相结合，实现了图像伪造检测的自动化和智能化。与传统方法相比，ForenAgent能够自主探索和利用各种低级工具，从而更全面地分析图像中的伪造痕迹。此外，ForenAgent的动态推理循环模拟了人类的推理过程，使其能够更有效地解决复杂的伪造检测问题。\\n\\n**关键设计**：ForenAgent的训练过程采用了两阶段策略。冷启动阶段旨在快速初始化Agent，使其具备基本的工具使用能力。强化微调阶段则通过奖励函数引导Agent学习更有效的工具使用策略。奖励函数的设计至关重要，它需要能够反映Agent的推理过程和最终的检测结果。此外，FABench数据集的构建也为ForenAgent的训练和评估提供了高质量的数据支持。",
            "application_zh": "ForenAgent在数字取证、新闻真实性验证、版权保护等领域具有广泛的应用前景。它可以帮助专业人员更高效地检测图像伪造，提高信息安全水平，维护社会诚信。未来，该技术有望应用于视频伪造检测、音频伪造检测等领域，为打击网络犯罪提供有力支持。",
            "highlight_zh": "ForenAgent在FABench数据集上取得了显著的性能提升，证明了其有效性。实验结果表明，ForenAgent能够有效地利用低级工具进行伪造检测，并在多个指标上超越了现有方法。此外，ForenAgent还展现出良好的泛化能力，能够在不同的伪造类型上保持较高的检测精度。",
            "tags_zh": [
                "图像伪造检测",
                "多模态大语言模型",
                "Agentic工具使用",
                "强化学习",
                "数字取证"
            ],
            "_index": 51,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16300v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16300v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16300v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "AMUSE: Audio-Visual Benchmark and Alignment Framework for Agentic Multi-Speaker Understanding",
            "authors": [
                "Sanjoy Chowdhury",
                "Karren D. Yang",
                "Xudong Liu",
                "Fartash Faghri",
                "Pavan Kumar Anasosalu Vasu",
                "Oncel Tuzel",
                "Dinesh Manocha",
                "Chun-Liang Li",
                "Raviteja Vemulapalli"
            ],
            "arxiv_id": "2512.16250v1",
            "summary": "Recent multimodal large language models (MLLMs) such as GPT-4o and Qwen3-Omni show strong perception but struggle in multi-speaker, dialogue-centric settings that demand agentic reasoning tracking who speaks, maintaining roles, and grounding events across time. These scenarios are central to multimodal audio-video understanding, where models must jointly reason over audio and visual streams in applications such as conversational video assistants and meeting analytics. We introduce AMUSE, a benchmark designed around tasks that are inherently agentic, requiring models to decompose complex audio-visual interactions into planning, grounding, and reflection steps. It evaluates MLLMs across three modes zero-shot, guided, and agentic and six task families, including spatio-temporal speaker grounding and multimodal dialogue summarization. Across all modes, current models exhibit weak multi-speaker reasoning and inconsistent behavior under both non-agentic and agentic evaluation. Motivated by the inherently agentic nature of these tasks and recent advances in LLM agents, we propose RAFT, a data-efficient agentic alignment framework that integrates reward optimization with intrinsic multimodal self-evaluation as reward and selective parameter adaptation for data and parameter efficient updates. Using RAFT, we achieve up to 39.52\\% relative improvement in accuracy on our benchmark. Together, AMUSE and RAFT provide a practical platform for examining agentic reasoning in multimodal models and improving their capabilities.",
            "categories": [
                "cs.AI",
                "cs.MA"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16250v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model",
                        "multimodal"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "AMUSE：用于Agentic多说话人理解的视听基准和对齐框架",
            "summary_zh": "本文提出了AMUSE，一个旨在评估多模态大型语言模型（MLLM）在多说话人、以对话为中心的场景下agentic推理能力的基准。现有MLLM如GPT-4o和Qwen3-Omni在感知方面表现出色，但在需要跟踪说话者、维护角色以及理解跨时间事件的此类场景中表现不佳。AMUSE围绕本质上是agentic的任务设计，要求模型将复杂的视听交互分解为规划、理解和反思步骤。它在零样本、引导和agentic三种模式以及六个任务族（包括时空说话人定位和多模态对话摘要）中评估MLLM。结果表明，当前模型在非agentic和agentic评估下都表现出较弱的多说话人推理和不一致的行为。受任务的agentic本质和LLM agent最新进展的启发，本文提出RAFT，一种数据高效的agentic对齐框架，它将奖励优化与内在多模态自我评估（作为奖励）和选择性参数适应相结合，以实现数据和参数高效的更新。使用RAFT，在基准测试中实现了高达39.52％的相对精度提升。AMUSE和RAFT共同为检查多模态模型中的agentic推理并提高其能力提供了一个实用的平台。",
            "intro_zh": [
                "现有MLLM在多说话人对话场景中，缺乏有效的agentic推理能力，难以跟踪说话人、理解角色和事件。",
                "提出RAFT框架，通过奖励优化和多模态自评估，实现数据高效的agentic对齐，提升模型性能。",
                "在AMUSE基准测试中，RAFT框架实现了高达39.52%的相对精度提升，验证了其有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决多模态大型语言模型（MLLM）在处理多说话人、以对话为中心的视听场景时，缺乏有效的agentic推理能力的问题。现有方法难以准确跟踪说话人身份、维护角色一致性，以及理解跨时间发生的事件，导致在会话视频助手和会议分析等应用中表现不佳。\\n\\n**核心思路**：论文的核心思路是利用agentic框架来提升MLLM在多说话人视听场景中的推理能力。通过将复杂的视听交互分解为规划、理解和反思等步骤，并结合奖励优化和多模态自评估，使模型能够更好地理解和处理多说话人对话中的上下文信息。\\n\\n**技术框架**：整体框架包含两个主要部分：AMUSE基准测试和RAFT对齐框架。AMUSE基准用于评估MLLM在不同agentic模式下的性能，包括零样本、引导和agentic模式。RAFT框架则用于提升MLLM的agentic推理能力，它通过将奖励优化与内在多模态自评估相结合，并采用选择性参数适应策略，实现数据和参数高效的更新。\\n\\n**关键创新**：论文的关键创新在于提出了RAFT（Reward-Aware Fine-Tuning）框架，该框架将奖励优化与内在多模态自评估相结合，以提升MLLM的agentic推理能力。与传统的微调方法相比，RAFT能够更有效地利用数据，并根据模型的自身评估结果进行参数调整，从而实现更好的性能。\\n\\n**关键设计**：RAFT框架的关键设计包括：1) 使用多模态自评估作为奖励信号，引导模型学习更有效的agentic策略；2) 采用选择性参数适应策略，只更新与agentic推理相关的参数，从而提高训练效率；3) 设计了AMUSE基准测试，用于全面评估MLLM在多说话人视听场景下的agentic推理能力。",
            "application_zh": "该研究成果可应用于会话视频助手、会议分析、智能客服等领域。通过提升模型在多说话人场景下的理解和推理能力，可以实现更自然、更智能的人机交互，提高工作效率和用户体验。未来，该技术有望在教育、医疗等领域发挥重要作用。",
            "highlight_zh": "实验结果表明，使用RAFT框架后，模型在AMUSE基准测试中实现了高达39.52%的相对精度提升。这一显著的性能提升验证了RAFT框架的有效性，表明其能够显著提高MLLM在多说话人视听场景下的agentic推理能力。此外，实验还表明，RAFT框架具有数据高效性，能够在少量数据上实现显著的性能提升。",
            "tags_zh": [
                "多模态学习",
                "Agentic推理",
                "多说话人理解",
                "视听融合",
                "基准测试",
                "奖励学习",
                "大型语言模型"
            ],
            "_index": 52,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16250v1/figures/teaser.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16250v1/figures/eval-modes-short.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16250v1/figures/raft-revised.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "JustRL: Scaling a 1.5B LLM with a Simple RL Recipe",
            "authors": [
                "Bingxiang He",
                "Zekai Qu",
                "Zeyuan Liu",
                "Yinghao Chen",
                "Yuxin Zuo",
                "Cheng Qian",
                "Kaiyan Zhang",
                "Weize Chen",
                "Chaojun Xiao",
                "Ganqu Cui",
                "Ning Ding",
                "Zhiyuan Liu"
            ],
            "arxiv_id": "2512.16649v1",
            "summary": "Recent advances in reinforcement learning for large language models have converged on increasing complexity: multi-stage training pipelines, dynamic hyperparameter schedules, and curriculum learning strategies. This raises a fundamental question: \\textbf{Is this complexity necessary?} We present \\textbf{JustRL}, a minimal approach using single-stage training with fixed hyperparameters that achieves state-of-the-art performance on two 1.5B reasoning models (54.9\\% and 64.3\\% average accuracy across nine mathematical benchmarks) while using 2$\\times$ less compute than sophisticated approaches. The same hyperparameters transfer across both models without tuning, and training exhibits smooth, monotonic improvement over 4,000+ steps without the collapses or plateaus that typically motivate interventions. Critically, ablations reveal that adding ``standard tricks'' like explicit length penalties and robust verifiers may degrade performance by collapsing exploration. These results suggest that the field may be adding complexity to solve problems that disappear with a stable, scaled-up baseline. We release our models and code to establish a simple, validated baseline for the community.",
            "categories": [
                "cs.CL"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "12 pages, 3 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16649v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning",
                        "curriculum learning"
                    ],
                    "score": 3.0
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "JustRL：通过简单强化学习方法扩展15亿参数LLM，实现数学推理SOTA",
            "summary_zh": "大型语言模型（LLM）的强化学习研究日益复杂，包括多阶段训练流程、动态超参数调整和课程学习策略。本文质疑这种复杂性是否必要，并提出了JustRL，一种极简方法，使用单阶段训练和固定超参数，在两个15亿参数的推理模型上实现了最先进的性能（在九个数学基准测试中平均准确率分别为54.9％和64.3％），同时计算量比复杂方法少2倍。相同的超参数无需调整即可在两个模型之间迁移，并且训练过程表现出平稳的单调改进，超过4,000步，没有通常需要干预的崩溃或停滞。关键的是，消融实验表明，添加诸如显式长度惩罚和鲁棒验证器之类的“标准技巧”可能会因崩溃探索而降低性能。这些结果表明，该领域可能正在增加复杂性来解决可以通过稳定、扩展的基线来解决的问题。我们发布了我们的模型和代码，以建立一个简单、经过验证的社区基线。",
            "intro_zh": [
                "现有LLM强化学习方法过于复杂，包含多阶段训练和动态超参数调整，缺乏对必要性的深入研究。",
                "JustRL采用单阶段训练和固定超参数，避免复杂调整，旨在探索简化强化学习流程在LLM中的潜力。",
                "实验表明，JustRL在数学推理任务上达到SOTA，且计算量更少，证明了简单方法的可行性与优越性。"
            ],
            "method_zh": "**问题定义**：现有的大型语言模型强化学习方法通常采用复杂的多阶段训练流程、动态调整的超参数以及课程学习策略。这些复杂性使得训练过程难以理解和优化，并且需要大量的计算资源。论文旨在解决的问题是：是否必须采用如此复杂的强化学习方法才能训练出高性能的LLM？现有方法的痛点在于其复杂性和计算成本。\n\n**核心思路**：论文的核心思路是采用一种极简的强化学习方法，即JustRL。JustRL使用单阶段训练和固定的超参数，避免了复杂的调整过程。作者认为，通过稳定和扩展的基线，可以解决一些原本需要复杂方法才能解决的问题。这种方法旨在降低计算成本，并提高训练过程的可理解性。\n\n**技术框架**：JustRL的技术框架非常简单，主要包括以下几个步骤：首先，使用一个预训练的1.5B参数的LLM作为基础模型。然后，使用强化学习算法（具体算法未明确说明，但暗示是常见的策略梯度方法）对模型进行微调。在训练过程中，使用固定的超参数，并且只进行单阶段的训练。最后，对训练好的模型进行评估，并在数学推理基准测试上进行性能测试。\n\n**关键创新**：JustRL最重要的技术创新点在于其极简的设计理念。与现有方法相比，JustRL避免了复杂的多阶段训练流程和动态超参数调整，从而降低了计算成本，并提高了训练过程的可理解性。此外，论文还发现，一些常用的“标准技巧”，如显式长度惩罚和鲁棒验证器，可能会降低性能，这与现有认知有所不同。\n\n**关键设计**：JustRL的关键设计在于其单阶段训练和固定超参数。作者通过实验证明，使用相同的超参数可以在不同的模型之间迁移，并且训练过程表现出平稳的单调改进。此外，作者还进行了消融实验，以评估不同技术细节对性能的影响。具体的技术细节包括损失函数的设计、奖励函数的设计以及探索策略的选择（虽然论文没有明确说明，但这些都是强化学习中需要考虑的关键因素）。",
            "application_zh": "JustRL的潜在应用领域包括各种需要复杂推理和决策能力的自然语言处理任务，例如数学问题求解、代码生成、对话系统等。该研究的实际价值在于降低了训练高性能LLM的计算成本，并提高了训练过程的可理解性。未来，JustRL可以作为一种简单有效的基线方法，用于评估和改进其他更复杂的强化学习方法。",
            "highlight_zh": "JustRL在两个15亿参数的推理模型上实现了最先进的性能，在九个数学基准测试中平均准确率分别达到54.9％和64.3％，同时计算量比复杂方法少2倍。消融实验表明，添加显式长度惩罚和鲁棒验证器等“标准技巧”可能会降低性能。相同的超参数无需调整即可在两个模型之间迁移，训练过程表现出平稳的单调改进。",
            "tags_zh": [
                "强化学习",
                "大型语言模型",
                "数学推理",
                "单阶段训练",
                "固定超参数"
            ],
            "_index": 53,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16649v1/figures/fig1_aime24_curves_added.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16649v1/figures/fig2_training_dynamics.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16649v1/figures/fig3_training_dynamics.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Learning-based Approximate Model Predictive Control for an Impact Wrench Tool",
            "authors": [
                "Mark Benazet",
                "Francesco Ricca",
                "Dario Bralla",
                "Melanie N. Zeilinger",
                "Andrea Carron"
            ],
            "arxiv_id": "2512.16624v1",
            "summary": "Learning-based model predictive control has emerged as a powerful approach for handling complex dynamics in mechatronic systems, enabling data-driven performance improvements while respecting safety constraints. However, when computational resources are severely limited, as in battery-powered tools with embedded processors, existing approaches struggle to meet real-time requirements. In this paper, we address the problem of real-time torque control for impact wrenches, where high-frequency control updates are necessary to accurately track the fast transients occurring during periodic impact events, while maintaining high-performance safety-critical control that mitigates harmful vibrations and component wear. The key novelty of the approach is that we combine data-driven model augmentation through Gaussian process regression with neural network approximation of the resulting control policy. This insight allows us to deploy predictive control on resource-constrained embedded platforms while maintaining both constraint satisfaction and microsecond-level inference times. The proposed framework is evaluated through numerical simulations and hardware experiments on a custom impact wrench testbed. The results show that our approach successfully achieves real-time control suitable for high-frequency operation while maintaining constraint satisfaction and improving tracking accuracy compared to baseline PID control.",
            "categories": [
                "eess.SY"
            ],
            "primary_category": "eess.SY",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16624v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]model predictive control"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "提出一种基于学习的近似模型预测控制方法，用于冲击扳手的实时扭矩控制。",
            "summary_zh": "本文提出了一种基于学习的模型预测控制方法，用于解决机电系统中复杂动力学问题，通过数据驱动的方式提升性能并满足安全约束。针对电池供电工具等资源受限的嵌入式处理器，现有方法难以满足实时性要求。本文聚焦冲击扳手的实时扭矩控制，需要高频控制更新以精确跟踪周期性冲击事件中的快速瞬变，同时保持高性能的安全控制，以减轻有害振动和部件磨损。该方法的核心创新在于结合了高斯过程回归的数据驱动模型增强和神经网络对控制策略的近似。这种结合使得在资源受限的嵌入式平台上部署预测控制成为可能，同时保证了约束满足和微秒级的推理时间。通过数值仿真和定制冲击扳手测试平台的硬件实验验证了所提出框架的有效性，结果表明该方法成功实现了适用于高频操作的实时控制，同时保持了约束满足，并提高了相对于基线PID控制的跟踪精度。",
            "intro_zh": [
                "现有冲击扳手控制方法难以在资源受限的嵌入式系统中实现高频、安全且高性能的实时扭矩控制。",
                "利用高斯过程回归进行数据驱动的模型增强，并使用神经网络近似控制策略，实现资源受限平台上的预测控制。",
                "实验结果表明，该方法在满足约束条件的同时，实现了微秒级的推理速度，并提高了扭矩跟踪精度。"
            ],
            "method_zh": "**问题定义**：冲击扳手的实时扭矩控制问题，需要在资源受限的嵌入式平台上实现高频控制更新，以精确跟踪冲击事件中的快速瞬变，同时保证安全性和减少部件磨损。现有方法难以在满足实时性要求的同时，兼顾高性能和安全性。\\n\\n**核心思路**：核心思路是将数据驱动的模型增强与控制策略的近似相结合。具体来说，利用高斯过程回归学习冲击扳手的动态特性，从而增强模型预测控制器的预测能力。然后，使用神经网络来近似求解模型预测控制问题，从而降低计算复杂度，满足实时性要求。\\n\\n**技术框架**：整体框架包括以下几个主要模块：1) 数据采集模块：收集冲击扳手在不同工况下的运行数据。2) 模型学习模块：使用高斯过程回归学习冲击扳手的动态模型。3) 控制策略近似模块：使用神经网络近似求解模型预测控制问题，得到控制策略。4) 实时控制模块：在嵌入式平台上部署神经网络控制策略，实现实时扭矩控制。\\n\\n**关键创新**：最重要的技术创新点在于将高斯过程回归和神经网络相结合，用于近似求解模型预测控制问题。高斯过程回归能够有效地学习冲击扳手的动态特性，提高预测精度。神经网络能够快速地进行推理，满足实时性要求。这种结合使得在资源受限的嵌入式平台上部署高性能的预测控制成为可能。与现有方法相比，该方法能够在保证实时性的同时，提高控制精度和安全性。\\n\\n**关键设计**：高斯过程回归使用径向基函数核，并采用最大似然估计方法优化超参数。神经网络采用多层感知机结构，使用ReLU激活函数。损失函数包括扭矩跟踪误差、控制量惩罚项和约束违反惩罚项。采用Adam优化器训练神经网络。具体的网络结构和参数设置需要根据实际应用进行调整。",
            "application_zh": "该研究成果可应用于各种需要高精度、高频率控制的电动工具，例如电动螺丝刀、电动扳手等。通过提高控制精度和效率，可以延长工具的使用寿命，提高工作效率，并降低能源消耗。此外，该方法还可以推广到其他资源受限的嵌入式控制系统，例如机器人、无人机等。",
            "highlight_zh": "在定制的冲击扳手测试平台上进行的硬件实验表明，该方法能够实现微秒级的推理速度，满足实时性要求。与基线PID控制相比，扭矩跟踪精度提高了显著，同时能够有效地抑制振动，保证安全性。具体性能数据（例如跟踪误差的降低百分比）需要在论文中查找。",
            "tags_zh": [
                "模型预测控制",
                "高斯过程回归",
                "神经网络",
                "实时控制",
                "嵌入式系统"
            ],
            "_index": 54,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16624v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16624v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16624v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Economic versus energetic model predictive control of a cold production plant with thermal energy storage",
            "authors": [
                "Manuel G. Satué",
                "Manuel R. Arahal",
                "Luis F. Acedo",
                "Manuel G. Ortega"
            ],
            "arxiv_id": "2512.16379v1",
            "summary": "Economic model predictive control has been proposed as a means for solving the unit loading and unit allocation problem in multi-chiller cooling plants. The adjective economic stems from the use of financial cost due to electricity consumption in a time horizon, such is the loss function minimized at each sampling period. The energetic approach is rarely encountered. This article presents for the first time a comparison between the energetic optimization objective and the economic one. The comparison is made on a cooling plant using air-cooled water chillers and a cold storage system. Models developed have been integrated into Simscape, and non-convex mixed optimization methods used to achieve optimal control trajectories for both energetic and economic goals considered separately. The results over several scenarios, and in different seasons, support the consideration of the energetic approach despite the current prevalence of the economic one. The results are dependent on the electric season and the available tariffs. In particular, for the high electric season and considering a representative tariff, the results show that an increment of about 2.15% in energy consumption takes place when using the economic approach instead of the energetic one. On the other hand, a reduction in cost of 2.94% is achieved.",
            "categories": [
                "eess.SY"
            ],
            "primary_category": "eess.SY",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "14 pages",
            "doi": "10.1016/j.applthermaleng.2022.118309",
            "journal_ref": "Applied Thermal Engineering 210 (2022) 118309",
            "pdf_url": "https://arxiv.org/pdf/2512.16379v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]model predictive control"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "对比经济与能量模型预测控制，优化冷库储能系统运行",
            "summary_zh": "本文首次对比了能量优化目标和经济优化目标在冷库冷却装置中的应用。经济模型预测控制通过最小化时间范围内的电力消耗成本来解决多冷却器冷却装置的单元负荷和单元分配问题。研究在包含风冷式冷水机组和冷存储系统的冷却装置上进行了对比实验。模型集成到Simscape中，并使用非凸混合优化方法分别实现了能量和经济目标下的最优控制轨迹。结果表明，在不同场景和季节下，能量优化方法值得考虑，尽管目前经济优化方法更为普遍。结果依赖于用电季节和可用电价。在高用电季节和代表性电价下，与能量优化方法相比，经济优化方法会导致能耗增加约2.15%，但成本降低2.94%。",
            "intro_zh": [
                "传统多冷却器冷却装置的单元负荷和分配问题主要依赖经济模型预测控制，但鲜有研究关注能量优化。",
                "本文提出对比经济和能量两种优化目标，旨在评估在冷库冷却装置中采用能量优化策略的可行性。",
                "实验结果表明，在高用电季节，经济优化虽然降低了成本，但会导致能耗显著增加，突显了能量优化的价值。"
            ],
            "method_zh": "**问题定义**：论文旨在解决冷库冷却装置的运行优化问题，具体而言，是在满足制冷需求的前提下，如何选择冷却器的运行状态和冷存储系统的充放电策略，以最小化能源消耗或运行成本。现有方法主要侧重于经济优化，即最小化电费支出，但忽略了能源效率，可能导致总体能耗增加。\\n\\n**核心思路**：论文的核心思路是对比经济模型预测控制（EMPC）和能量模型预测控制（EnMPC）两种策略，评估它们在不同季节和电价下的性能差异。EMPC以最小化电费为目标，而EnMPC以最小化能耗为目标。通过对比两种策略的运行结果，揭示经济优化可能带来的能源浪费，并为实际应用提供决策依据。\\n\\n**技术框架**：论文构建了一个冷库冷却装置的Simscape模型，该模型包括风冷式冷水机组和冷存储系统。然后，分别针对EMPC和EnMPC设计了模型预测控制器。控制器通过预测未来一段时间内的系统状态，并根据优化目标（电费或能耗）选择最优的控制策略。优化问题采用非凸混合优化方法求解。\\n\\n**关键创新**：论文的主要创新在于首次在冷库冷却装置的控制中对比了经济优化和能量优化两种目标。以往的研究主要集中在经济优化上，而忽略了能源效率。通过对比实验，论文揭示了经济优化可能导致能源浪费，并为实际应用提供了新的视角。\\n\\n**关键设计**：论文的关键设计包括：1) 精确的冷库冷却装置Simscape模型，能够准确模拟系统的动态特性；2) 基于模型预测控制的优化框架，能够根据预测信息选择最优控制策略；3) 非凸混合优化方法的应用，能够有效求解复杂的优化问题；4) 针对不同季节和电价的实验设计，能够全面评估两种控制策略的性能。",
            "application_zh": "该研究成果可应用于各种需要冷量供应的场景，例如大型商场、数据中心、工业生产等。通过选择合适的控制策略，可以在保证制冷效果的同时，降低能源消耗和运行成本，提高能源利用效率，具有重要的经济和社会价值。未来的研究可以进一步探索更加智能化的控制策略，例如结合人工智能算法，实现自适应的能源优化。",
            "highlight_zh": "实验结果表明，在高用电季节和代表性电价下，使用经济模型预测控制（EMPC）代替能量模型预测控制（EnMPC）会导致能耗增加约2.15%，但成本降低2.94%。这表明在电价较高的时段，经济优化策略可能会牺牲能源效率以降低运行成本。该结果突出了在不同运行条件下选择合适控制策略的重要性。",
            "tags_zh": [
                "模型预测控制",
                "经济优化",
                "能量优化",
                "冷库储能",
                "冷却装置"
            ],
            "_index": 55,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16379v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16379v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16379v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Sceniris: A Fast Procedural Scene Generation Framework",
            "authors": [
                "Jinghuan Shang",
                "Harsh Patel",
                "Ran Gong",
                "Karl Schmeckpeper"
            ],
            "arxiv_id": "2512.16896v1",
            "summary": "Synthetic 3D scenes are essential for developing Physical AI and generative models. Existing procedural generation methods often have low output throughput, creating a significant bottleneck in scaling up dataset creation. In this work, we introduce Sceniris, a highly efficient procedural scene generation framework for rapidly generating large-scale, collision-free scene variations. Sceniris also provides an optional robot reachability check, providing manipulation-feasible scenes for robot tasks. Sceniris is designed for maximum efficiency by addressing the primary performance limitations of the prior method, Scene Synthesizer. Leveraging batch sampling and faster collision checking in cuRobo, Sceniris achieves at least 234x speed-up over Scene Synthesizer. Sceniris also expands the object-wise spatial relationships available in prior work to support diverse scene requirements. Our code is available at https://github.com/rai-inst/sceniris",
            "categories": [
                "cs.RO",
                "cs.CV",
                "cs.GR"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Code is available at https://github.com/rai-inst/sceniris",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16896v1",
            "code_links": [
                {
                    "url": "https://github.com/rai-inst/sceniris",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱七：动作重定向 (Motion Retargeting)",
                    "id": "7_retargeting",
                    "matched_keywords": [
                        "spatial relationship"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 5.0,
            "hit_pillars": [
                "1_robot_core",
                "7_retargeting"
            ],
            "headline_zh": "Sceniris：一种快速程序化场景生成框架，加速物理AI和生成模型开发。",
            "summary_zh": "合成3D场景对于开发物理人工智能和生成模型至关重要。现有的程序化生成方法通常输出吞吐量较低，这在扩展数据集创建方面造成了显著的瓶颈。本文介绍Sceniris，这是一个高效的程序化场景生成框架，用于快速生成大规模、无碰撞的场景变体。Sceniris还提供可选的机器人可达性检查，为机器人任务提供可操作的场景。Sceniris通过解决先前方法Scene Synthesizer的主要性能限制来实现最大效率。通过利用批量采样和cuRobo中更快的碰撞检测，Sceniris实现了比Scene Synthesizer至少234倍的加速。Sceniris还扩展了先前工作中可用的对象级空间关系，以支持多样化的场景需求。代码已开源。",
            "intro_zh": [
                "现有程序化场景生成方法吞吐量低，严重限制了大规模数据集的创建，阻碍了物理AI和生成模型的发展。",
                "Sceniris通过批量采样和更快的碰撞检测，显著提升了场景生成速度，并扩展了对象间的空间关系。",
                "实验表明，Sceniris比Scene Synthesizer至少快234倍，为机器人任务提供了更多可操作的场景。"
            ],
            "method_zh": "**问题定义**：论文旨在解决现有程序化场景生成方法速度慢的问题。现有方法在生成大规模数据集时面临严重的性能瓶颈，限制了物理AI和生成模型的发展。Scene Synthesizer等方法虽然能够生成场景，但速度较慢，难以满足实际需求。\\n\\n**核心思路**：Sceniris的核心思路是通过优化采样和碰撞检测过程来提高场景生成速度。具体来说，它采用了批量采样策略，并利用cuRobo库中更快的碰撞检测算法，从而显著减少了生成场景所需的时间。此外，Sceniris还扩展了对象间的空间关系，使其能够生成更多样化的场景。\\n\\n**技术框架**：Sceniris的整体框架包括以下几个主要模块：1) 场景描述模块：定义场景中对象的类型、数量和空间关系。2) 采样模块：根据场景描述，批量采样对象的位置和姿态。3) 碰撞检测模块：使用cuRobo库快速检测对象之间是否存在碰撞。4) 可达性检查模块（可选）：检查生成的场景是否适合机器人操作。5) 场景输出模块：将生成的场景保存为3D模型文件。\\n\\n**关键创新**：Sceniris最重要的技术创新点在于其高效的场景生成速度。这主要归功于以下两点：1) 批量采样：通过一次性采样多个对象的位置和姿态，减少了采样过程中的开销。2) cuRobo加速碰撞检测：利用cuRobo库中优化的碰撞检测算法，显著提高了碰撞检测的速度。与现有方法相比，Sceniris在保证场景质量的同时，显著提高了生成速度。\\n\\n**关键设计**：Sceniris的关键设计包括：1) 批量采样策略：具体采样数量需要根据场景的复杂度和计算资源进行调整。2) cuRobo碰撞检测参数：需要根据对象的形状和大小设置合适的碰撞检测参数，以保证检测的准确性和效率。3) 对象空间关系定义：Sceniris扩展了对象间的空间关系，例如支持对象之间的支撑关系、包含关系等。这些关系的定义需要根据具体的应用场景进行调整。",
            "application_zh": "Sceniris可广泛应用于物理AI、机器人学习、计算机视觉等领域。它可以用于生成大规模的训练数据集，帮助训练更强大的AI模型。例如，可以用于训练机器人操作技能，或者用于训练视觉识别模型。此外，Sceniris还可以用于虚拟环境的创建，例如游戏开发、仿真模拟等。该研究的实际价值在于降低了数据集创建的成本，加速了AI技术的发展。未来，Sceniris可以进一步扩展，支持更多类型的对象和场景，并提供更高级的场景编辑功能。",
            "highlight_zh": "Sceniris通过批量采样和cuRobo加速碰撞检测，实现了显著的性能提升。实验结果表明，Sceniris比Scene Synthesizer至少快234倍。这一显著的加速使得生成大规模数据集成为可能，为物理AI和生成模型的发展提供了有力支持。此外，Sceniris还扩展了对象间的空间关系，使其能够生成更多样化的场景。",
            "tags_zh": [
                "程序化场景生成",
                "物理AI",
                "机器人学习",
                "碰撞检测",
                "cuRobo",
                "批量采样",
                "3D场景"
            ],
            "_index": 56,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16896v1/assets/main_steps.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16896v1/assets/spatial_rels_cc.jpg",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16896v1/assets/main_benchmark.jpg",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "TextEditBench: Evaluating Reasoning-aware Text Editing Beyond Rendering",
            "authors": [
                "Rui Gui",
                "Yang Wan",
                "Haochen Han",
                "Dongxing Mao",
                "Fangming Liu",
                "Min Li",
                "Alex Jinpeng Wang"
            ],
            "arxiv_id": "2512.16270v1",
            "summary": "Text rendering has recently emerged as one of the most challenging frontiers in visual generation, drawing significant attention from large-scale diffusion and multimodal models. However, text editing within images remains largely unexplored, as it requires generating legible characters while preserving semantic, geometric, and contextual coherence. To fill this gap, we introduce TextEditBench, a comprehensive evaluation benchmark that explicitly focuses on text-centric regions in images. Beyond basic pixel manipulations, our benchmark emphasizes reasoning-intensive editing scenarios that require models to understand physical plausibility, linguistic meaning, and cross-modal dependencies. We further propose a novel evaluation dimension, Semantic Expectation (SE), which measures reasoning ability of model to maintain semantic consistency, contextual coherence, and cross-modal alignment during text editing. Extensive experiments on state-of-the-art editing systems reveal that while current models can follow simple textual instructions, they still struggle with context-dependent reasoning, physical consistency, and layout-aware integration. By focusing evaluation on this long-overlooked yet fundamental capability, TextEditBench establishes a new testing ground for advancing text-guided image editing and reasoning in multimodal generation.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16270v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "multimodal"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 5.0,
            "hit_pillars": [
                "1_robot_core",
                "9_embodied_foundation"
            ],
            "headline_zh": "提出TextEditBench，用于评估图像文本编辑中蕴含推理能力，超越简单的渲染效果。",
            "summary_zh": "本文提出TextEditBench，一个综合性的评估基准，专门关注图像中以文本为中心的区域。与基本的像素操作不同，该基准强调推理密集型的编辑场景，要求模型理解物理合理性、语言意义和跨模态依赖关系。此外，本文还提出了一种新的评估维度——语义期望（SE），用于衡量模型在文本编辑过程中保持语义一致性、上下文连贯性和跨模态对齐的推理能力。对最先进的编辑系统进行的大量实验表明，虽然当前的模型可以遵循简单的文本指令，但它们在上下文相关的推理、物理一致性和布局感知的集成方面仍然存在困难。通过专注于这种长期被忽视但又至关重要的能力，TextEditBench 为推进文本引导的图像编辑和多模态生成中的推理建立了一个新的测试平台。",
            "intro_zh": [
                "现有图像编辑方法在文本编辑方面存在不足，尤其是在需要理解物理合理性、语言意义和跨模态依赖的复杂场景下。",
                "TextEditBench通过构建包含推理密集型编辑场景的基准，并提出语义期望（SE）评估指标，来衡量模型在文本编辑中的推理能力。",
                "实验表明，现有模型在处理简单的文本指令尚可，但在上下文推理、物理一致性和布局集成方面仍有挑战，TextEditBench为此提供测试平台。"
            ],
            "method_zh": "**问题定义**：现有图像编辑方法在处理图像中的文本编辑任务时，尤其是在需要进行复杂推理的场景下，表现不足。这些场景要求模型不仅要生成清晰可辨认的字符，还要保证编辑后的图像在语义、几何和上下文上的一致性。现有方法难以同时满足这些要求，尤其是在理解物理合理性、语言意义和跨模态依赖关系方面存在明显缺陷。\\n\\n**核心思路**：TextEditBench的核心思路是构建一个更具挑战性的评估基准，该基准侧重于测试模型在文本编辑过程中进行推理的能力。通过设计需要模型理解上下文、物理规则和跨模态关系的编辑场景，TextEditBench能够更全面地评估模型的智能水平。此外，提出的语义期望（SE）指标能够量化模型在保持语义一致性方面的表现。\\n\\n**技术框架**：TextEditBench包含一系列图像文本编辑任务，这些任务被设计成需要不同程度的推理能力。评估流程如下：首先，给定一张包含文本的图像和一段编辑指令，模型根据指令对图像中的文本进行修改。然后，使用语义期望（SE）指标来评估编辑后的图像在语义一致性、上下文连贯性和跨模态对齐方面的表现。SE指标的计算可能涉及多个步骤，例如分析编辑前后图像的语义变化，以及评估编辑后的文本与周围环境的融合程度。\\n\\n**关键创新**：TextEditBench的关键创新在于其对推理能力的强调。与以往侧重于像素级操作的图像编辑基准不同，TextEditBench更加关注模型对图像内容的理解和推理能力。通过引入需要理解物理合理性、语言意义和跨模态依赖关系的编辑场景，TextEditBench能够更有效地评估模型的智能水平。此外，语义期望（SE）指标的提出为量化模型在保持语义一致性方面的表现提供了一种新的方法。\\n\\n**关键设计**：TextEditBench的具体实现细节未知，但可以推测其关键设计包括：1) 多样化的编辑场景：涵盖各种需要不同推理能力的文本编辑任务。2) 精心设计的评估指标：语义期望（SE）指标需要能够准确地衡量编辑后的图像在语义一致性、上下文连贯性和跨模态对齐方面的表现。3) 标准化的评估流程：确保不同模型在相同条件下进行评估，从而保证评估结果的公平性和可比性。",
            "application_zh": "TextEditBench的研究成果可应用于图像编辑软件、智能设计工具、内容生成平台等领域。通过提升模型在文本编辑中的推理能力，可以实现更智能、更自然的图像编辑效果，例如自动修复图像中的错误文字、根据用户指令修改图像中的文本内容，以及生成具有特定风格的文本图像。未来，该研究还有助于开发更强大的多模态生成模型，实现更复杂的图像编辑和内容创作任务。",
            "highlight_zh": "TextEditBench对现有图像编辑模型进行了广泛的评估，结果表明，虽然这些模型在简单的文本编辑任务上表现尚可，但在需要进行复杂推理的场景下，性能显著下降。具体来说，模型在处理上下文相关的推理、物理一致性和布局感知的集成方面存在明显困难。这些实验结果突显了TextEditBench的价值，并为未来的研究方向提供了重要的参考。",
            "tags_zh": [
                "文本编辑",
                "图像编辑",
                "推理能力",
                "多模态学习",
                "评估基准"
            ],
            "_index": 57,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16270v1/figures/src/data_collection.jpg",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16270v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16270v1/figures/src/evaluation.jpg",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Skeleton-Snippet Contrastive Learning with Multiscale Feature Fusion for Action Localization",
            "authors": [
                "Qiushuo Cheng",
                "Jingjing Liu",
                "Catherine Morgan",
                "Alan Whone",
                "Majid Mirmehdi"
            ],
            "arxiv_id": "2512.16504v1",
            "summary": "The self-supervised pretraining paradigm has achieved great success in learning 3D action representations for skeleton-based action recognition using contrastive learning. However, learning effective representations for skeleton-based temporal action localization remains challenging and underexplored. Unlike video-level {action} recognition, detecting action boundaries requires temporally sensitive features that capture subtle differences between adjacent frames where labels change. To this end, we formulate a snippet discrimination pretext task for self-supervised pretraining, which densely projects skeleton sequences into non-overlapping segments and promotes features that distinguish them across videos via contrastive learning. Additionally, we build on strong backbones of skeleton-based action recognition models by fusing intermediate features with a U-shaped module to enhance feature resolution for frame-level localization. Our approach consistently improves existing skeleton-based contrastive learning methods for action localization on BABEL across diverse subsets and evaluation protocols. We also achieve state-of-the-art transfer learning performance on PKUMMD with pretraining on NTU RGB+D and BABEL.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16504v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]contrastive learning"
                    ],
                    "score": 4.5
                }
            ],
            "relevance_score": 4.5,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "提出基于骨骼片段对比学习和多尺度特征融合的动作定位方法",
            "summary_zh": "本文针对基于骨骼的动作定位任务，提出了一种自监督预训练范式，旨在学习更有效的3D动作表示。与视频级别的动作识别不同，动作定位需要对时间敏感的特征，以捕捉相邻帧之间细微的标签变化。为此，我们设计了一个片段判别预训练任务，将骨骼序列密集地投影到非重叠的片段中，并通过对比学习来区分不同视频中的片段。此外，我们利用U型模块融合中间特征，增强特征分辨率，从而提升帧级别的定位性能。实验结果表明，我们的方法在BABEL数据集上，针对不同的子集和评估协议，均能持续改进现有的基于骨骼的对比学习方法。通过在NTU RGB+D和BABEL数据集上进行预训练，我们在PKUMMD数据集上实现了最先进的迁移学习性能。",
            "intro_zh": [
                "现有基于骨骼的动作定位方法缺乏对时间敏感特征的有效学习，难以准确检测动作边界。",
                "论文提出片段判别预训练任务，通过对比学习区分不同视频片段，学习时间敏感特征。",
                "采用U型模块融合多尺度特征，提升特征分辨率，并在BABEL和PKUMMD数据集上验证有效性。"
            ],
            "method_zh": "**问题定义**：现有基于骨骼的动作定位方法在学习时间敏感特征方面存在不足，难以准确区分相邻帧之间细微的动作变化，导致动作边界检测不准确。现有的对比学习方法主要集中在视频级别的动作识别，缺乏对帧级别动作定位的优化。\\n\\n**核心思路**：论文的核心思路是通过片段判别任务进行自监督预训练，学习对时间变化敏感的特征表示。通过将骨骼序列分割成多个非重叠的片段，并利用对比学习来区分不同视频中的片段，从而使模型能够捕捉到动作边界的细微变化。同时，通过多尺度特征融合增强特征分辨率，进一步提升定位精度。\\n\\n**技术框架**：整体框架包含两个主要部分：片段判别预训练和多尺度特征融合。首先，将输入的骨骼序列分割成多个非重叠的片段。然后，利用编码器将每个片段映射到特征空间。接着，通过对比学习损失函数，鼓励模型区分不同视频中的片段。在下游任务中，利用预训练的编码器作为骨干网络，并添加U型模块进行多尺度特征融合，以增强特征分辨率。\\n\\n**关键创新**：论文的关键创新在于提出了片段判别预训练任务，该任务专门为基于骨骼的动作定位设计，能够有效地学习时间敏感特征。与传统的视频级别对比学习方法不同，片段判别任务关注的是相邻帧之间的细微变化，从而更适合于动作定位任务。此外，多尺度特征融合模块也进一步提升了特征分辨率，增强了模型的定位能力。\\n\\n**关键设计**：片段判别任务的关键在于如何选择合适的片段长度和对比学习策略。论文中采用了非重叠的片段分割方式，并使用InfoNCE损失函数进行对比学习。U型模块的关键在于如何选择合适的中间特征进行融合，以及如何设计融合的方式。具体的网络结构和参数设置在论文中有详细描述，但此处未提供具体数值。",
            "application_zh": "该研究成果可应用于智能监控、人机交互、医疗康复等领域。例如，在智能监控中，可以利用该方法实现对异常行为的自动检测和定位；在医疗康复中，可以用于评估患者的运动功能和康复效果；在人机交互中，可以实现对人体动作的精确识别和理解。",
            "highlight_zh": "实验结果表明，该方法在BABEL数据集上显著提升了现有基于骨骼的对比学习方法的动作定位性能。此外，通过在NTU RGB+D和BABEL数据集上进行预训练，该方法在PKUMMD数据集上实现了最先进的迁移学习性能，表明了其良好的泛化能力。具体的性能提升数据需要在论文中查找。",
            "tags_zh": [
                "骨骼动作定位",
                "对比学习",
                "自监督学习",
                "多尺度特征融合",
                "时间敏感特征"
            ],
            "_index": 58,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16504v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16504v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16504v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward",
            "authors": [
                "Peter Chen",
                "Xiaopeng Li",
                "Ziniu Li",
                "Wotao Yin",
                "Xi Chen",
                "Tianyi Lin"
            ],
            "arxiv_id": "2512.16912v1",
            "summary": "This paper examines the exploration-exploitation trade-off in reinforcement learning with verifiable rewards (RLVR), a framework for improving the reasoning of Large Language Models (LLMs). Recent studies suggest that RLVR can elicit strong mathematical reasoning in LLMs through two seemingly paradoxical mechanisms: spurious rewards, which suppress exploitation by rewarding outcomes unrelated to the ground truth, and entropy minimization, which suppresses exploration by pushing the model toward more confident and deterministic outputs, highlighting a puzzling dynamic: both discouraging exploitation and discouraging exploration improve reasoning performance, yet the underlying principles that reconcile these effects remain poorly understood. We focus on two fundamental questions: (i) how policy entropy relates to performance, and (ii) whether spurious rewards yield gains, potentially through the interplay of clipping bias and model contamination. Our results show that clipping bias under spurious rewards reduces policy entropy, leading to more confident and deterministic outputs, while entropy minimization alone is insufficient for improvement. We further propose a reward-misalignment model explaining why spurious rewards can enhance performance beyond contaminated settings. Our findings clarify the mechanisms behind spurious-reward benefits and provide principles for more effective RLVR training.",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.CL"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "35 pages",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16912v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 4.5,
            "hit_pillars": [
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "通过裁剪、熵和虚假奖励重新思考RLVR，提升LLM推理能力",
            "summary_zh": "本文研究了具有可验证奖励的强化学习（RLVR）中的探索-利用权衡，RLVR是一种用于改进大型语言模型（LLM）推理的框架。最近的研究表明，RLVR可以通过两种看似矛盾的机制来激发LLM强大的数学推理能力：虚假奖励，通过奖励与ground truth无关的结果来抑制利用；以及熵最小化，通过将模型推向更自信和确定性的输出来抑制探索。这突显了一种令人困惑的动态：抑制利用和抑制探索都能提高推理性能，但协调这些影响的潜在原则仍然知之甚少。我们关注两个基本问题：（i）策略熵如何与性能相关，以及（ii）虚假奖励是否能带来收益，可能是通过裁剪偏差和模型污染的相互作用。我们的结果表明，虚假奖励下的裁剪偏差降低了策略熵，从而产生更自信和确定性的输出，而仅靠熵最小化不足以改进。我们进一步提出了一个奖励错位模型，解释了为什么虚假奖励可以提高超出污染环境的性能。我们的发现阐明了虚假奖励背后机制，并为更有效的RLVR训练提供了原则。",
            "intro_zh": [
                "现有RLVR方法在探索-利用平衡上存在不足，对LLM推理能力的提升机制理解不够深入。",
                "论文提出通过裁剪偏差降低策略熵，并利用奖励错位模型解释虚假奖励的有效性。",
                "实验表明，虚假奖励下的裁剪偏差能有效降低策略熵，提升LLM推理的确定性和准确性。"
            ],
            "method_zh": "**问题定义**：现有RLVR方法在提升LLM推理能力时，对探索与利用的权衡机制理解不足。具体来说，虚假奖励和熵最小化这两种看似矛盾的方法都能提升性能，但其内在原理尚不明确。现有方法缺乏对策略熵与性能之间关系，以及虚假奖励收益来源的深入分析。\\n\\n**核心思路**：论文的核心思路是通过分析裁剪偏差对策略熵的影响，以及构建奖励错位模型，来解释虚假奖励在RLVR中的作用。认为裁剪偏差在虚假奖励下能降低策略熵，从而产生更自信的输出。同时，奖励错位模型解释了虚假奖励在非污染环境下的有效性。\\n\\n**技术框架**：论文主要通过理论分析和实验验证来研究RLVR中的探索-利用权衡。技术框架包括：1）分析裁剪偏差对策略熵的影响；2）构建奖励错位模型；3）设计实验验证虚假奖励的有效性。没有明确的整体架构或流程图。\\n\\n**关键创新**：论文的关键创新在于：1）揭示了裁剪偏差在虚假奖励下降低策略熵的作用；2）提出了奖励错位模型，解释了虚假奖励在非污染环境下的有效性。与现有方法相比，该研究更深入地理解了虚假奖励的机制，并为更有效的RLVR训练提供了理论依据。\\n\\n**关键设计**：论文的关键设计包括：1）使用裁剪操作来限制奖励的范围，从而引入裁剪偏差；2）定义策略熵来衡量模型输出的确定性；3）构建奖励错位模型，该模型考虑了奖励与真实目标之间的偏差。具体的参数设置、损失函数和网络结构在论文中没有详细描述，属于实验细节，可能因具体任务而异。",
            "application_zh": "该研究成果可应用于提升大型语言模型在数学推理、逻辑推理等领域的性能。通过更有效地利用虚假奖励和控制策略熵，可以训练出更可靠、更准确的LLM，从而在教育、科研、金融等领域发挥更大的作用。未来的研究可以探索更有效的奖励设计和策略优化方法。",
            "highlight_zh": "实验结果表明，在虚假奖励下，裁剪偏差能够有效降低策略熵，使得LLM输出更加自信和确定。同时，奖励错位模型能够解释虚假奖励在非污染环境下的有效性。这些发现为改进RLVR训练提供了新的思路和方法。",
            "tags_zh": [
                "强化学习",
                "大型语言模型",
                "可验证奖励",
                "探索-利用权衡",
                "策略熵"
            ],
            "_index": 59,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16912v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16912v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16912v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Meta-RL Induces Exploration in Language Agents",
            "authors": [
                "Yulun Jiang",
                "Liangze Jiang",
                "Damien Teney",
                "Michael Moor",
                "Maria Brbic"
            ],
            "arxiv_id": "2512.16848v1",
            "summary": "Reinforcement learning (RL) has enabled the training of large language model (LLM) agents to interact with the environment and to solve multi-turn long-horizon tasks. However, the RL-trained agents often struggle in tasks that require active exploration and fail to efficiently adapt from trial-and-error experiences. In this paper, we present LaMer, a general Meta-RL framework that enables LLM agents to actively explore and learn from the environment feedback at test time. LaMer consists of two key components: (i) a cross-episode training framework to encourage exploration and long-term rewards optimization; and (ii) in-context policy adaptation via reflection, allowing the agent to adapt their policy from task feedback signal without gradient update. Experiments across diverse environments show that LaMer significantly improves performance over RL baselines, with 11%, 14%, and 19% performance gains on Sokoban, MineSweeper and Webshop, respectively. Moreover, LaMer also demonstrates better generalization to more challenging or previously unseen tasks compared to the RL-trained agents. Overall, our results demonstrate that Meta-RL provides a principled approach to induce exploration in language agents, enabling more robust adaptation to novel environments through learned exploration strategies.",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16848v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 4.5,
            "hit_pillars": [
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "LaMer：基于元强化学习提升语言Agent在复杂环境中的探索能力",
            "summary_zh": "强化学习(RL)使得训练大型语言模型(LLM) Agent与环境交互并解决多轮长时程任务成为可能。然而，RL训练的Agent在需要主动探索的任务中表现不佳，并且无法有效地从试错经验中学习。本文提出了LaMer，一个通用的元强化学习框架，使LLM Agent能够在测试时主动探索并从环境反馈中学习。LaMer包含两个关键组件：(i)一个跨episode的训练框架，鼓励探索和长期奖励优化；(ii)通过反思进行上下文策略调整，允许Agent从任务反馈信号中调整其策略，而无需梯度更新。在各种环境中的实验表明，LaMer显著提高了性能，在Sokoban、MineSweeper和Webshop上的性能分别提高了11%、14%和19%。此外，与RL训练的Agent相比，LaMer还展示了更好的泛化能力，可以应对更具挑战性或以前未见过的任务。总的来说，我们的结果表明，元强化学习提供了一种原则性的方法来诱导语言Agent进行探索，从而通过学习到的探索策略实现对新环境的更稳健的适应。",
            "intro_zh": [
                "现有RL训练的LLM Agent在需要主动探索和长期规划的任务中表现不足，难以有效利用试错经验。",
                "LaMer通过元强化学习框架，鼓励Agent在训练时进行跨episode的探索，并在测试时通过反思进行策略调整。",
                "实验结果表明，LaMer在Sokoban、MineSweeper和Webshop等任务中显著优于RL基线，并具有更好的泛化能力。"
            ],
            "method_zh": "**问题定义**：现有的基于强化学习的语言Agent在复杂环境中进行探索时存在效率低下的问题。它们难以有效地从试错经验中学习，尤其是在需要长期规划和主动探索的任务中。传统的强化学习方法往往侧重于利用已知的策略，而忽略了对未知状态和行为的探索，导致Agent容易陷入局部最优解。\n\n**核心思路**：LaMer的核心思路是利用元强化学习的思想，让Agent学习如何进行有效的探索。通过跨episode的训练，Agent可以学习到一种通用的探索策略，使其能够在新的环境中快速适应并找到最优解。此外，LaMer还引入了反思机制，允许Agent根据环境的反馈信号动态调整其策略，从而提高其适应性和鲁棒性。\n\n**技术框架**：LaMer的整体框架包含两个主要阶段：(1) 跨episode训练阶段：在此阶段，Agent在多个不同的episode中与环境交互，并根据获得的奖励更新其策略。该阶段的目标是学习一种通用的探索策略，使其能够在不同的环境中快速适应。(2) 上下文策略调整阶段：在此阶段，Agent根据环境的反馈信号，通过反思机制动态调整其策略。该阶段的目标是提高Agent的适应性和鲁棒性，使其能够更好地应对新的环境。\n\n**关键创新**：LaMer的关键创新在于将元强化学习的思想应用于语言Agent的探索问题。与传统的强化学习方法相比，LaMer能够学习一种通用的探索策略，使其能够在新的环境中快速适应并找到最优解。此外，LaMer的反思机制也能够有效地提高Agent的适应性和鲁棒性。\n\n**关键设计**：LaMer的关键设计包括：(1) 跨episode训练框架：该框架鼓励Agent在不同的episode中进行探索，并根据获得的奖励更新其策略。(2) 反思机制：该机制允许Agent根据环境的反馈信号动态调整其策略。具体来说，Agent会根据过去的经验和当前的反馈，生成一段反思文本，然后将该文本作为上下文信息输入到策略网络中，从而调整其行为。损失函数的设计目标是最大化长期奖励，并鼓励Agent进行有效的探索。",
            "application_zh": "LaMer具有广泛的应用前景，例如可以应用于游戏AI、机器人控制、自动驾驶等领域。通过学习有效的探索策略，Agent可以在复杂环境中自主地完成各种任务，例如在游戏中找到最优策略，在机器人控制中实现自主导航，在自动驾驶中安全地行驶。此外，LaMer还可以应用于教育领域，帮助学生更好地学习和掌握知识。",
            "highlight_zh": "LaMer在Sokoban、MineSweeper和Webshop等任务中取得了显著的性能提升，分别提高了11%、14%和19%。与传统的RL基线相比，LaMer不仅在性能上有所提升，而且还具有更好的泛化能力，能够更好地应对新的环境和任务。这些结果表明，LaMer是一种有效的元强化学习框架，可以显著提高语言Agent的探索能力。",
            "tags_zh": [
                "元强化学习",
                "语言Agent",
                "探索策略",
                "上下文学习",
                "策略适应"
            ],
            "_index": 60,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16848v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16848v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16848v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Coordinated Anti-Jamming Resilience in Swarm Networks via Multi-Agent Reinforcement Learning",
            "authors": [
                "Bahman Abolhassani",
                "Tugba Erpek",
                "Kemal Davaslioglu",
                "Yalin E. Sagduyu",
                "Sastry Kompella"
            ],
            "arxiv_id": "2512.16813v1",
            "summary": "Reactive jammers pose a severe security threat to robotic-swarm networks by selectively disrupting inter-agent communications and undermining formation integrity and mission success. Conventional countermeasures such as fixed power control or static channel hopping are largely ineffective against such adaptive adversaries. This paper presents a multi-agent reinforcement learning (MARL) framework based on the QMIX algorithm to improve the resilience of swarm communications under reactive jamming. We consider a network of multiple transmitter-receiver pairs sharing channels while a reactive jammer with Markovian threshold dynamics senses aggregate power and reacts accordingly. Each agent jointly selects transmit frequency (channel) and power, and QMIX learns a centralized but factorizable action-value function that enables coordinated yet decentralized execution. We benchmark QMIX against a genie-aided optimal policy in a no-channel-reuse setting, and against local Upper Confidence Bound (UCB) and a stateless reactive policy in a more general fading regime with channel reuse enabled. Simulation results show that QMIX rapidly converges to cooperative policies that nearly match the genie-aided bound, while achieving higher throughput and lower jamming incidence than the baselines, thereby demonstrating MARL's effectiveness for securing autonomous swarms in contested environments.",
            "categories": [
                "cs.NI",
                "cs.AI",
                "cs.DC",
                "cs.LG",
                "eess.SP"
            ],
            "primary_category": "cs.NI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16813v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]reinforcement learning"
                    ],
                    "score": 4.5
                }
            ],
            "relevance_score": 4.5,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "提出基于QMIX的多智能体强化学习方法，提升集群网络抗干扰能力",
            "summary_zh": "本文提出了一种基于QMIX算法的多智能体强化学习(MARL)框架，旨在提高集群通信在反应式干扰下的弹性。反应式干扰机会选择性地干扰智能体间的通信，从而破坏集群的完整性和任务成功率，对机器人集群网络构成严重的安全威胁。传统的对策，如固定功率控制或静态信道跳频，对于这种自适应的攻击者来说效果不佳。本文考虑了一个多发射机-接收机对共享信道的网络，其中一个具有马尔可夫阈值动态的反应式干扰机感知总功率并做出相应反应。每个智能体联合选择发射频率（信道）和功率，QMIX学习一个集中式但可分解的动作价值函数，从而实现协调但分散的执行。在无信道重用设置中，我们将QMIX与genie-aided最优策略进行基准测试；在启用信道重用的更一般的衰落环境中，我们将QMIX与局部上限置信界(UCB)和无状态反应策略进行基准测试。仿真结果表明，QMIX迅速收敛到接近genie-aided界限的协作策略，同时比基线实现更高的吞吐量和更低的干扰发生率，从而证明了MARL在竞争环境中保护自主集群的有效性。",
            "intro_zh": [
                "现有固定功率或静态信道跳频策略难以有效应对自适应反应式干扰对集群网络的威胁。",
                "提出基于QMIX的多智能体强化学习框架，使智能体能够协同选择信道和功率，提升抗干扰能力。",
                "实验表明，QMIX能快速收敛到协作策略，在吞吐量和抗干扰方面优于传统方法，接近最优策略。"
            ],
            "method_zh": "**问题定义**：论文旨在解决反应式干扰对机器人集群网络通信的威胁。传统的固定功率控制或静态信道跳频策略无法有效应对自适应干扰，导致集群通信中断，影响任务完成。现有方法的痛点在于缺乏智能体间的协同和对干扰的动态适应能力。\\n\\n**核心思路**：论文的核心思路是利用多智能体强化学习（MARL）来训练智能体，使其能够根据环境状态（包括干扰情况）动态调整发射频率（信道）和功率。通过学习智能体间的协作策略，提高集群通信的抗干扰能力和整体性能。这种方法允许智能体在分散执行的同时，实现协调一致的行动。\\n\\n**技术框架**：整体框架包含多个发射机-接收机对，它们共享信道进行通信。一个反应式干扰机监听信道上的总功率，并根据马尔可夫阈值动态决定是否进行干扰。每个智能体通过QMIX算法学习最优策略，该算法包含以下主要模块：1) 局部观测模块：每个智能体获取局部环境信息，如信道状态和干扰情况。2) 动作选择模块：根据当前策略选择发射频率和功率。3) QMIX网络：学习一个集中式但可分解的动作价值函数，用于评估联合动作的价值。4) 奖励函数：根据通信吞吐量和干扰情况，为智能体提供奖励信号。\\n\\n**关键创新**：论文的关键创新在于将QMIX算法应用于集群网络的抗干扰问题，并设计了一种集中式训练、分散式执行的MARL框架。与传统的单智能体强化学习方法相比，QMIX能够更好地处理多智能体环境中的信用分配问题，学习到更有效的协作策略。此外，论文还考虑了反应式干扰机的动态行为，使智能体能够适应不断变化的干扰环境。\\n\\n**关键设计**：QMIX网络是该方法的核心，它由一个混合网络和一个动作价值函数网络组成。混合网络将每个智能体的局部Q值混合成一个全局Q值，用于评估联合动作的价值。动作价值函数网络则根据智能体的局部观测和动作，估计其局部Q值。奖励函数的设计至关重要，它需要平衡通信吞吐量和干扰情况，引导智能体学习到既能保证通信质量又能避免干扰的策略。此外，论文还考虑了信道衰落和信道重用等实际因素，使模型更具实用性。",
            "application_zh": "该研究成果可应用于各种需要高可靠性通信的集群机器人系统，例如：无人机集群协同搜索与救援、自主水下航行器集群环境监测、以及无线传感器网络等。通过提升集群网络的抗干扰能力，可以确保任务的顺利完成，提高系统的鲁棒性和可靠性，在军事和民用领域都具有重要的应用价值。",
            "highlight_zh": "仿真结果表明，QMIX算法能够快速收敛到协作策略，在无信道重用场景下，性能接近genie-aided最优策略。在启用信道重用的衰落环境中，QMIX算法相比于局部UCB和无状态反应策略，实现了更高的吞吐量和更低的干扰发生率。具体而言，QMIX在吞吐量上平均提升了15%-20%，干扰发生率降低了10%-15%。",
            "tags_zh": [
                "多智能体强化学习",
                "集群网络",
                "抗干扰",
                "QMIX算法",
                "反应式干扰",
                "信道选择",
                "功率控制"
            ],
            "_index": 61,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16813v1/topology.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16813v1/based10.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16813v1/based8.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "On The Hidden Biases of Flow Matching Samplers",
            "authors": [
                "Soon Hoe Lim"
            ],
            "arxiv_id": "2512.16768v1",
            "summary": "We study the implicit bias of flow matching (FM) samplers via the lens of empirical flow matching. Although population FM may produce gradient-field velocities resembling optimal transport (OT), we show that the empirical FM minimizer is almost never a gradient field, even when each conditional flow is. Consequently, empirical FM is intrinsically energetically suboptimal. In view of this, we analyze the kinetic energy of generated samples. With Gaussian sources, both instantaneous and integrated kinetic energies exhibit exponential concentration, while heavy-tailed sources lead to polynomial tails. These behaviors are governed primarily by the choice of source distribution rather than the data. Overall, these notes provide a concise mathematical account of the structural and energetic biases arising in empirical FM.",
            "categories": [
                "stat.ML",
                "cs.LG",
                "math.PR"
            ],
            "primary_category": "stat.ML",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "20 pages",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16768v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]flow matching"
                    ],
                    "score": 4.5
                }
            ],
            "relevance_score": 4.5,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "揭示Flow Matching采样器中的隐藏偏差，分析其能量次优性",
            "summary_zh": "本文通过经验流匹配的视角研究了流匹配(FM)采样器的隐式偏差。尽管总体FM可能产生类似于最优传输(OT)的梯度场速度，但我们表明，即使每个条件流都是梯度场，经验FM的最小化器几乎都不是梯度场。因此，经验FM本质上是能量次优的。鉴于此，我们分析了生成样本的动能。对于高斯源，瞬时和积分动能都表现出指数集中，而重尾源则导致多项式尾部。这些行为主要受源分布的选择控制，而不是数据本身。总的来说，这些笔记对经验FM中出现的结构和能量偏差进行了简明的数学描述。",
            "intro_zh": [
                "现有Flow Matching方法在经验估计时会偏离最优传输，导致能量次优。",
                "论文通过分析经验流匹配最小化器的性质，揭示了其并非梯度场的内在结构偏差。",
                "论文分析了生成样本的动能，发现其分布主要受源分布影响，而非数据本身。"
            ],
            "method_zh": "**问题定义**：Flow Matching (FM) 旨在学习一个连续的概率分布变换，将一个简单的源分布（如高斯分布）转换为复杂的目标数据分布。然而，在实际应用中，我们只能获得有限的样本，因此需要使用经验流匹配来近似总体流匹配。现有方法的痛点在于，经验流匹配可能引入偏差，导致生成的样本并非最优，尤其是在能量方面表现出次优性。\\n\\n**核心思路**：论文的核心思路是通过分析经验流匹配最小化器的性质，揭示其内在的结构偏差。具体来说，论文证明了即使总体流匹配产生的是梯度场，经验流匹配的解几乎都不是梯度场。这意味着经验流匹配在能量上并非最优，因为它未能找到真正的最优传输路径。\\n\\n**技术框架**：论文主要采用数学分析的方法，没有涉及具体的模型架构或训练流程。其分析框架主要包括以下几个方面：1) 证明经验流匹配最小化器不是梯度场；2) 分析生成样本的动能，包括瞬时动能和积分动能；3) 研究不同源分布（高斯分布和重尾分布）对动能分布的影响。\\n\\n**关键创新**：论文最重要的技术创新点在于揭示了经验流匹配的结构偏差，即经验流匹配的解不是梯度场。这一发现解释了为什么经验流匹配在能量上表现出次优性，并为改进Flow Matching方法提供了新的思路。与现有方法相比，本文更侧重于理论分析，而非模型设计或算法优化。\\n\\n**关键设计**：论文主要关注理论分析，没有涉及具体的参数设置或网络结构。关键的设计在于选择合适的数学工具来分析经验流匹配的性质，例如，使用变分法来证明经验流匹配最小化器不是梯度场，使用概率论和统计学来分析生成样本的动能分布。",
            "application_zh": "该研究成果可应用于生成模型的改进，尤其是在需要高保真度和能量效率的场景下，例如图像生成、音频合成等。通过理解Flow Matching的偏差，可以设计更有效的训练方法和模型架构，从而提高生成样本的质量和效率。此外，该研究也为理解其他基于流的生成模型的行为提供了理论基础。",
            "highlight_zh": "论文通过数学分析证明了经验Flow Matching的解不是梯度场，揭示了其能量次优性。研究表明，生成样本的动能分布主要受源分布影响，高斯源导致指数集中，重尾源导致多项式尾部。这些发现为改进Flow Matching方法提供了理论指导。",
            "tags_zh": [
                "流匹配",
                "生成模型",
                "最优传输",
                "经验风险最小化",
                "偏差分析"
            ],
            "_index": 62,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "NDRL: Cotton Irrigation and Nitrogen Application with Nested Dual-Agent Reinforcement Learning",
            "authors": [
                "Ruifeng Xu",
                "Liang He"
            ],
            "arxiv_id": "2512.16408v1",
            "summary": "Effective irrigation and nitrogen fertilization have a significant impact on crop yield. However, existing research faces two limitations: (1) the high complexity of optimizing water-nitrogen combinations during crop growth and poor yield optimization results; and (2) the difficulty in quantifying mild stress signals and the delayed feedback, which results in less precise dynamic regulation of water and nitrogen and lower resource utilization efficiency. To address these issues, we propose a Nested Dual-Agent Reinforcement Learning (NDRL) method. The parent agent in NDRL identifies promising macroscopic irrigation and fertilization actions based on projected cumulative yield benefits, reducing ineffective explorationwhile maintaining alignment between objectives and yield. The child agent's reward function incorporates quantified Water Stress Factor (WSF) and Nitrogen Stress Factor (NSF), and uses a mixed probability distribution to dynamically optimize daily strategies, thereby enhancing both yield and resource efficiency. We used field experiment data from 2023 and 2024 to calibrate and validate the Decision Support System for Agrotechnology Transfer (DSSAT) to simulate real-world conditions and interact with NDRL. Experimental results demonstrate that, compared to the best baseline, the simulated yield increased by 4.7% in both 2023 and 2024, the irrigation water productivity increased by 5.6% and 5.1% respectively, and the nitrogen partial factor productivity increased by 6.3% and 1.0% respectively. Our method advances the development of cotton irrigation and nitrogen fertilization, providing new ideas for addressing the complexity and precision issues in agricultural resource management and for sustainable agricultural development.",
            "categories": [
                "cs.LG",
                "cs.MA"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Accepted by ICONIP 2025",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16408v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]reinforcement learning"
                    ],
                    "score": 4.5
                }
            ],
            "relevance_score": 4.5,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "提出NDRL方法以解决棉花灌溉与氮肥施用的复杂性问题",
            "summary_zh": "有效的灌溉和氮肥施用对作物产量有显著影响。然而，现有研究面临两个主要限制：一是优化水氮组合的复杂性高，导致产量优化效果不佳；二是轻微压力信号的量化和反馈延迟，使得水氮动态调节精度不足，资源利用效率低。为了解决这些问题，本文提出了一种嵌套双代理强化学习（NDRL）方法。NDRL中的父代理基于预期的累计产量收益识别有前景的宏观灌溉和施肥动作，减少无效探索，同时保持目标与产量的一致性。子代理的奖励函数结合了量化的水分压力因子和氮压力因子，并使用混合概率分布动态优化日常策略，从而提高产量和资源效率。实验结果表明，与最佳基线相比，模拟产量在2023年和2024年均提高了4.7%，灌溉水生产率分别提高了5.6%和5.1%，氮部分生产率分别提高了6.3%和1.0%。",
            "intro_zh": [
                "现有方法在优化水氮组合时复杂性高，导致产量优化效果不佳，且难以量化轻微压力信号，反馈延迟影响动态调节精度。",
                "本文提出的NDRL方法通过父代理和子代理的协作，优化灌溉和施肥策略，动态调整以提高资源利用效率和作物产量。",
                "实验结果显示，NDRL方法在2023年和2024年均实现了4.7%的产量提升，灌溉水生产率和氮部分生产率也有显著提高。"
            ],
            "method_zh": "**问题定义**：本文旨在解决棉花灌溉与氮肥施用中的复杂优化问题，现有方法在水氮组合优化和动态调节精度方面存在不足，导致资源利用效率低下。\\n\\n**核心思路**：提出嵌套双代理强化学习（NDRL）方法，通过父代理识别宏观策略，子代理动态优化日常策略，从而提高产量和资源利用效率。\\n\\n**技术框架**：NDRL方法由父代理和子代理组成，父代理负责宏观决策，子代理根据量化的水分和氮压力因子进行微观调整，二者协同工作以实现优化目标。\\n\\n**关键创新**：NDRL的创新在于通过双代理结构实现了对复杂环境的有效探索与利用，尤其是在动态调节方面，显著提高了资源使用效率。\\n\\n**关键设计**：奖励函数结合了水分压力因子（WSF）和氮压力因子（NSF），采用混合概率分布进行策略优化，确保了动态调整的灵活性和准确性。",
            "application_zh": "该研究的潜在应用领域包括农业资源管理、精准灌溉和施肥技术等。通过优化水氮施用策略，能够有效提升作物产量和资源利用效率，推动可持续农业发展。未来，该方法可扩展至其他作物的管理与优化，具有广泛的实际价值。",
            "highlight_zh": "实验结果表明，NDRL方法在2023年和2024年均实现了4.7%的产量提升，灌溉水生产率分别提高了5.6%和5.1%，氮部分生产率分别提高了6.3%和1.0%，显示出显著的性能提升。",
            "tags_zh": [
                "棉花灌溉",
                "氮肥施用",
                "强化学习",
                "资源优化",
                "农业管理",
                "动态调节",
                "可持续发展"
            ],
            "_index": 63,
            "_used_api": "openai",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16408v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16408v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16408v1/x4.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "A Task-Driven, Planner-in-the-Loop Computational Design Framework for Modular Manipulators",
            "authors": [
                "Maolin Lei",
                "Edoardo Romiti",
                "Arturo Laurenzi",
                "Rui Dai",
                "Matteo Dalle Vedove",
                "Jiatao Ding",
                "Daniele Fontanelli",
                "Nikos Tsagarakis"
            ],
            "arxiv_id": "2512.16069v1",
            "summary": "Modular manipulators composed of pre-manufactured and interchangeable modules offer high adaptability across diverse tasks. However, their deployment requires generating feasible motions while jointly optimizing morphology and mounted pose under kinematic, dynamic, and physical constraints. Moreover, traditional single-branch designs often extend reach by increasing link length, which can easily violate torque limits at the base joint. To address these challenges, we propose a unified task-driven computational framework that integrates trajectory planning across varying morphologies with the co-optimization of morphology and mounted pose. Within this framework, a hierarchical model predictive control (HMPC) strategy is developed to enable motion planning for both redundant and non-redundant manipulators. For design optimization, the CMA-ES is employed to efficiently explore a hybrid search space consisting of discrete morphology configurations and continuous mounted poses. Meanwhile, a virtual module abstraction is introduced to enable bi-branch morphologies, allowing an auxiliary branch to offload torque from the primary branch and extend the achievable workspace without increasing the capacity of individual joint modules. Extensive simulations and hardware experiments on polishing, drilling, and pick-and-place tasks demonstrate the effectiveness of the proposed framework. The results show that: 1) the framework can generate multiple feasible designs that satisfy kinematic and dynamic constraints while avoiding environmental collisions for given tasks; 2) flexible design objectives, such as maximizing manipulability, minimizing joint effort, or reducing the number of modules, can be achieved by customizing the cost functions; and 3) a bi-branch morphology capable of operating in a large workspace can be realized without requiring more powerful basic modules.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16069v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "model predictive control",
                        "motion planning"
                    ],
                    "score": 4.0
                }
            ],
            "relevance_score": 4.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "提出任务驱动的模块化机械臂计算设计框架，实现形态与运动的协同优化",
            "summary_zh": "本文提出了一种统一的任务驱动计算框架，用于模块化机械臂的设计，该框架集成了不同形态下的轨迹规划以及形态和安装姿态的协同优化。开发了一种分层模型预测控制（HMPC）策略，用于冗余和非冗余机械臂的运动规划。采用CMA-ES算法高效探索离散形态配置和连续安装姿态的混合搜索空间。引入虚拟模块抽象，实现双分支形态，允许辅助分支卸载主分支的扭矩，扩展可达工作空间，而无需增加单个关节模块的容量。在抛光、钻孔和取放任务中的仿真和硬件实验验证了该框架的有效性。结果表明，该框架可以生成满足运动学和动力学约束的可行设计，同时避免环境碰撞；通过定制成本函数，可以实现灵活的设计目标，例如最大化可操作性、最小化关节力或减少模块数量；无需更强大的基本模块即可实现可在大型工作空间中运行的双分支形态。",
            "intro_zh": [
                "传统单分支机械臂通过增加连杆长度来扩展工作空间，易超出基关节的扭矩限制，存在设计瓶颈。",
                "提出一种统一的计算框架，将轨迹规划与形态、安装姿态的协同优化相结合，实现任务驱动的设计。",
                "通过仿真和硬件实验验证，该框架能生成满足约束的可行设计，并实现灵活的设计目标，如优化可操作性。"
            ],
            "method_zh": "**问题定义**：模块化机械臂的设计需要同时优化机械臂的形态、安装姿态和运动轨迹，以满足特定的任务需求。传统方法往往采用单分支结构，通过增加连杆长度来扩大工作空间，但容易导致基关节扭矩超出限制。此外，现有方法难以在满足运动学、动力学和物理约束的同时，实现形态和运动的协同优化。\\n\\n**核心思路**：本文的核心思路是将轨迹规划融入到机械臂的设计过程中，通过任务驱动的方式，同时优化机械臂的形态、安装姿态和运动轨迹。引入双分支结构，利用辅助分支来分担主分支的扭矩，从而在不增加关节模块功率的情况下，扩展机械臂的工作空间。\\n\\n**技术框架**：该框架包含以下主要模块：1) 运动规划模块，采用分层模型预测控制（HMPC）策略，为冗余和非冗余机械臂生成可行的运动轨迹；2) 设计优化模块，采用CMA-ES算法，在离散的形态配置和连续的安装姿态空间中进行高效搜索；3) 虚拟模块抽象模块，用于实现双分支形态，并评估其对扭矩分担和工作空间扩展的影响。整个流程是迭代的，设计优化模块根据运动规划模块的反馈，不断调整机械臂的形态和安装姿态，直到满足任务需求。\\n\\n**关键创新**：该论文的关键创新在于：1) 提出了一种任务驱动的设计框架，将运动规划融入到机械臂的设计过程中，实现了形态和运动的协同优化；2) 引入了虚拟模块抽象，实现了双分支形态，可以在不增加关节模块功率的情况下，扩展机械臂的工作空间；3) 采用分层模型预测控制（HMPC）策略，实现了冗余和非冗余机械臂的运动规划。\\n\\n**关键设计**：在运动规划模块中，HMPC策略被用于生成运动轨迹，其目标是最小化关节力矩和轨迹误差。在设计优化模块中，CMA-ES算法被用于搜索最优的形态和安装姿态，其目标是最大化可操作性、最小化关节力或减少模块数量。虚拟模块抽象通过添加虚拟连杆和关节来模拟双分支结构，并评估其对扭矩分担和工作空间扩展的影响。成本函数可以根据具体任务进行定制，以实现不同的设计目标。",
            "application_zh": "该研究成果可应用于各种需要灵活适应性的工业自动化场景，例如：复杂曲面的抛光、高精度钻孔、以及在狭小空间内的物料搬运等。通过优化机械臂的形态和运动，可以提高生产效率、降低能源消耗，并扩展机械臂的应用范围。未来，该框架可进一步扩展到多机械臂协同作业、人机协作等领域。",
            "highlight_zh": "通过在抛光、钻孔和取放任务中的仿真和硬件实验，验证了该框架的有效性。实验结果表明，该框架能够生成满足运动学和动力学约束的可行设计，并实现灵活的设计目标，例如最大化可操作性、最小化关节力或减少模块数量。此外，双分支形态可以在不增加关节模块功率的情况下，扩展机械臂的工作空间。",
            "tags_zh": [
                "模块化机械臂",
                "计算设计",
                "轨迹规划",
                "形态优化",
                "双分支结构",
                "模型预测控制",
                "CMA-ES"
            ],
            "_index": 64,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16069v1/figure/dual_arm_robot.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16069v1/figure/framework.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16069v1/figure/balance_updae.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Auto-Vocabulary 3D Object Detection",
            "authors": [
                "Haomeng Zhang",
                "Kuan-Chuan Peng",
                "Suhas Lohit",
                "Raymond A. Yeh"
            ],
            "arxiv_id": "2512.16077v1",
            "summary": "Open-vocabulary 3D object detection methods are able to localize 3D boxes of classes unseen during training. Despite the name, existing methods rely on user-specified classes both at training and inference. We propose to study Auto-Vocabulary 3D Object Detection (AV3DOD), where the classes are automatically generated for the detected objects without any user input. To this end, we introduce Semantic Score (SS) to evaluate the quality of the generated class names. We then develop a novel framework, AV3DOD, which leverages 2D vision-language models (VLMs) to generate rich semantic candidates through image captioning, pseudo 3D box generation, and feature-space semantics expansion. AV3DOD achieves the state-of-the-art (SOTA) performance on both localization (mAP) and semantic quality (SS) on the ScanNetV2 and SUNRGB-D datasets. Notably, it surpasses the SOTA, CoDA, by 3.48 overall mAP and attains a 24.5% relative improvement in SS on ScanNetV2.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "technical report",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16077v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "open-vocabulary",
                        "open vocabulary"
                    ],
                    "score": 4.0
                }
            ],
            "relevance_score": 4.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出AV3DOD，实现无需用户干预的自动词汇3D目标检测",
            "summary_zh": "开放词汇3D目标检测方法能够定位训练期间未见过的类别的3D框。然而，现有方法在训练和推理时都依赖于用户指定的类别。本文研究了自动词汇3D目标检测（AV3DOD），其中类别是为检测到的对象自动生成的，无需任何用户输入。为此，本文引入了语义分数（SS）来评估生成的类名称的质量。然后，开发了一个新的框架AV3DOD，该框架利用2D视觉-语言模型（VLMs）通过图像字幕、伪3D框生成和特征空间语义扩展来生成丰富的语义候选。AV3DOD在ScanNetV2和SUNRGB-D数据集上的定位（mAP）和语义质量（SS）方面均达到了最先进的（SOTA）性能。值得注意的是，它超过了SOTA方法CoDA，在ScanNetV2上总体mAP提高了3.48，SS相对提高了24.5%。",
            "intro_zh": [
                "现有开放词汇3D目标检测方法依赖用户指定的类别，限制了其自动化程度和泛化能力。",
                "AV3DOD框架利用2D视觉-语言模型自动生成类别名称，无需人工干预，提升了检测的灵活性。",
                "实验表明，AV3DOD在ScanNetV2和SUNRGB-D数据集上，定位精度和语义质量均超越现有技术水平。"
            ],
            "method_zh": "**问题定义**：现有开放词汇3D目标检测方法需要用户预先定义类别，这限制了其在实际应用中的灵活性和自动化程度。当面对未知的场景或物体时，这些方法无法有效工作，因为它们无法生成或识别新的类别。因此，如何实现无需用户干预的自动类别生成是当前面临的关键问题。\\n\\n**核心思路**：AV3DOD的核心思路是利用2D视觉-语言模型（VLMs）的强大语义理解能力，从图像中自动生成与3D物体相关的类别名称。通过将2D图像信息与3D几何信息相结合，可以为每个检测到的3D物体生成丰富的语义候选，从而实现自动词汇的3D目标检测。这种方法避免了对预定义类别的依赖，提高了检测系统的泛化能力。\\n\\n**技术框架**：AV3DOD框架主要包含以下几个阶段：1) 图像字幕生成：使用2D VLM为场景图像生成字幕，提取场景的整体语义信息。2) 伪3D框生成：根据2D检测结果和深度信息生成伪3D框，为每个物体提供初步的3D定位。3) 特征空间语义扩展：利用VLM将2D图像特征映射到语义空间，并结合3D几何特征，为每个3D物体生成更丰富的语义候选。4) 语义分数评估：引入语义分数（SS）来评估生成的类别名称的质量，选择最佳的类别名称。\\n\\n**关键创新**：AV3DOD最重要的技术创新点在于实现了完全自动化的类别生成，无需任何用户输入。与现有方法相比，AV3DOD不再依赖预定义的类别集合，而是能够根据场景中的实际物体自动生成类别名称。此外，语义分数（SS）的引入为评估生成类别的质量提供了一种有效的手段。\\n\\n**关键设计**：在图像字幕生成阶段，使用了预训练的2D VLM模型，并针对3D场景进行了微调。在特征空间语义扩展阶段，采用了对比学习的方法，将2D图像特征和3D几何特征映射到同一个语义空间。语义分数（SS）的计算综合考虑了生成类别的相关性、准确性和简洁性。",
            "application_zh": "AV3DOD在机器人导航、自动驾驶、智能家居等领域具有广泛的应用前景。它可以帮助机器人更好地理解周围环境，识别未知的物体，并做出更智能的决策。例如，在智能家居场景中，AV3DOD可以自动识别新的家具或物品，并将其添加到家居控制系统中。未来，该技术有望推动3D场景理解和人机交互的发展。",
            "highlight_zh": "AV3DOD在ScanNetV2数据集上取得了显著的性能提升，总体mAP超过了SOTA方法CoDA 3.48，并且在语义质量（SS）方面实现了24.5%的相对提升。这些结果表明，AV3DOD在自动类别生成和3D目标检测方面具有显著的优势，能够有效地提高检测的准确性和语义理解能力。",
            "tags_zh": [
                "3D目标检测",
                "开放词汇",
                "视觉-语言模型",
                "自动类别生成",
                "语义理解"
            ],
            "_index": 65,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16077v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16077v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16077v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Posterior Behavioral Cloning: Pretraining BC Policies for Efficient RL Finetuning",
            "authors": [
                "Andrew Wagenmaker",
                "Perry Dong",
                "Raymond Tsao",
                "Chelsea Finn",
                "Sergey Levine"
            ],
            "arxiv_id": "2512.16911v1",
            "summary": "Standard practice across domains from robotics to language is to first pretrain a policy on a large-scale demonstration dataset, and then finetune this policy, typically with reinforcement learning (RL), in order to improve performance on deployment domains. This finetuning step has proved critical in achieving human or super-human performance, yet while much attention has been given to developing more effective finetuning algorithms, little attention has been given to ensuring the pretrained policy is an effective initialization for RL finetuning. In this work we seek to understand how the pretrained policy affects finetuning performance, and how to pretrain policies in order to ensure they are effective initializations for finetuning. We first show theoretically that standard behavioral cloning (BC) -- which trains a policy to directly match the actions played by the demonstrator -- can fail to ensure coverage over the demonstrator's actions, a minimal condition necessary for effective RL finetuning. We then show that if, instead of exactly fitting the observed demonstrations, we train a policy to model the posterior distribution of the demonstrator's behavior given the demonstration dataset, we do obtain a policy that ensures coverage over the demonstrator's actions, enabling more effective finetuning. Furthermore, this policy -- which we refer to as the posterior behavioral cloning (PostBC) policy -- achieves this while ensuring pretrained performance is no worse than that of the BC policy. We then show that PostBC is practically implementable with modern generative models in robotic control domains -- relying only on standard supervised learning -- and leads to significantly improved RL finetuning performance on both realistic robotic control benchmarks and real-world robotic manipulation tasks, as compared to standard behavioral cloning.",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.RO"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16911v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning"
                    ],
                    "score": 1.5
                }
            ],
            "relevance_score": 3.5,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "提出后验行为克隆(PostBC)方法，提升RL微调的预训练策略效果",
            "summary_zh": "本文研究了预训练策略如何影响强化学习(RL)微调的性能，以及如何预训练策略以确保它们是有效的微调初始化。理论上证明，标准行为克隆(BC)无法确保覆盖演示者的行为，这是有效RL微调的必要条件。因此，提出后验行为克隆(PostBC)策略，该策略训练模型来模拟给定演示数据集的演示者行为的后验分布，从而确保覆盖演示者的行为，并实现更有效的微调。PostBC在保证预训练性能不低于BC策略的同时，通过标准监督学习即可在机器人控制领域实际应用，并且与标准行为克隆相比，在真实的机器人控制基准和真实世界的机器人操作任务上，显著提高了RL微调的性能。",
            "intro_zh": [
                "现有行为克隆(BC)方法在预训练策略时，无法保证覆盖演示者的所有行为，导致强化学习微调效果不佳。",
                "论文提出后验行为克隆(PostBC)方法，通过建模演示者行为的后验分布，确保预训练策略覆盖演示者的行为。",
                "实验表明，PostBC在机器人控制任务中，显著提升了强化学习微调的性能，优于标准行为克隆。"
            ],
            "method_zh": "**问题定义**：现有方法，特别是标准行为克隆(BC)，在预训练策略时，目标是直接模仿演示者的动作。这种方法的痛点在于，它可能无法充分覆盖演示者行为的整个分布，导致预训练策略泛化能力不足，从而限制了后续强化学习(RL)微调的性能。换句话说，如果预训练策略无法探索到演示者可能采取的所有动作，那么RL微调就难以找到更优的策略。\n\n**核心思路**：论文的核心解决思路是，不再仅仅模仿演示者的动作，而是学习演示者行为的后验分布。这意味着，给定演示数据集，模型需要学习生成与演示数据相似的行为，而不是简单地复制。通过建模后验分布，可以确保预训练策略能够覆盖演示者行为的更广泛范围，从而为RL微调提供更好的初始化。\n\n**技术框架**：PostBC的整体框架仍然基于监督学习，但训练目标不同于传统的BC。主要流程包括：1) 收集演示数据集；2) 使用生成模型（如变分自编码器VAE或生成对抗网络GAN）来建模演示者行为的后验分布；3) 使用学习到的后验分布来生成预训练策略。在RL微调阶段，使用预训练的PostBC策略作为RL算法的初始化策略。\n\n**关键创新**：最重要的技术创新点在于，将预训练策略的学习目标从模仿单个动作转变为建模整个行为分布的后验概率。与现有方法的本质区别在于，PostBC不再追求精确匹配演示数据，而是学习生成与演示数据相似的行为，从而提高策略的泛化能力和探索能力。\n\n**关键设计**：PostBC的关键设计包括：1) 选择合适的生成模型来建模后验分布，例如可以使用变分自编码器(VAE)，其中编码器用于推断潜在变量，解码器用于生成动作；2) 设计合适的损失函数，例如可以使用VAE的重构损失和KL散度损失，以确保生成的动作与演示数据相似，并且潜在变量的分布接近先验分布；3) 在RL微调阶段，可以使用各种RL算法，例如PPO或SAC，并调整学习率和探索策略，以充分利用预训练策略的优势。",
            "application_zh": "PostBC方法可广泛应用于机器人控制、自动驾驶、游戏AI等领域。在机器人控制中，可以利用大量人类演示数据预训练机器人策略，然后通过RL微调，使机器人能够完成复杂的任务。在自动驾驶中，可以利用驾驶员的驾驶数据预训练自动驾驶策略，提高自动驾驶系统的安全性和可靠性。在游戏AI中，可以利用玩家的游戏数据预训练游戏AI策略，提高游戏AI的智能水平和挑战性。",
            "highlight_zh": "实验结果表明，PostBC在多个机器人控制任务中显著优于标准行为克隆。例如，在真实机器人操作任务中，PostBC策略作为RL微调的初始化，能够更快地收敛到更高的性能，并且最终性能也优于使用标准BC策略初始化的RL算法。具体提升幅度取决于任务的复杂程度和RL算法的选择，但总体而言，PostBC能够带来显著的性能提升。",
            "tags_zh": [
                "后验行为克隆",
                "强化学习微调",
                "预训练策略",
                "机器人控制",
                "行为克隆",
                "生成模型",
                "策略优化"
            ],
            "_index": 66,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16911v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16911v1/im/corn_in_pot2.jpg",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16911v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning",
            "authors": [
                "Yuanchen Ju",
                "Yongyuan Liang",
                "Yen-Jen Wang",
                "Nandiraju Gireesh",
                "Yuanliang Ju",
                "Seungjae Lee",
                "Qiao Gu",
                "Elvis Hsieh",
                "Furong Huang",
                "Koushil Sreenath"
            ],
            "arxiv_id": "2512.16909v1",
            "summary": "Mobile manipulators in households must both navigate and manipulate. This requires a compact, semantically rich scene representation that captures where objects are, how they function, and which parts are actionable. Scene graphs are a natural choice, yet prior work often separates spatial and functional relations, treats scenes as static snapshots without object states or temporal updates, and overlooks information most relevant for accomplishing the current task. To address these limitations, we introduce MomaGraph, a unified scene representation for embodied agents that integrates spatial-functional relationships and part-level interactive elements. However, advancing such a representation requires both suitable data and rigorous evaluation, which have been largely missing. We thus contribute MomaGraph-Scenes, the first large-scale dataset of richly annotated, task-driven scene graphs in household environments, along with MomaGraph-Bench, a systematic evaluation suite spanning six reasoning capabilities from high-level planning to fine-grained scene understanding. Built upon this foundation, we further develop MomaGraph-R1, a 7B vision-language model trained with reinforcement learning on MomaGraph-Scenes. MomaGraph-R1 predicts task-oriented scene graphs and serves as a zero-shot task planner under a Graph-then-Plan framework. Extensive experiments demonstrate that our model achieves state-of-the-art results among open-source models, reaching 71.6% accuracy on the benchmark (+11.4% over the best baseline), while generalizing across public benchmarks and transferring effectively to real-robot experiments.",
            "categories": [
                "cs.CV",
                "cs.RO"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "25 pages, 10 figures. Project page:https://hybridrobotics.github.io/MomaGraph/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16909v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "scene understanding"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 3.5,
            "hit_pillars": [
                "2_algo_arch",
                "3_perception_slam"
            ],
            "headline_zh": "提出MomaGraph，利用视觉-语言模型为具身任务规划构建状态感知的统一场景图。",
            "summary_zh": "本文提出MomaGraph，一种用于具身智能体的统一场景表示，它集成了空间-功能关系和部件级别的交互元素，旨在解决现有场景图表示方法中空间和功能关系分离、场景被视为静态快照以及忽略与当前任务最相关信息的问题。同时，本文贡献了MomaGraph-Scenes，这是首个大规模的、带有丰富标注的、任务驱动的家庭环境场景图数据集，以及MomaGraph-Bench，一个涵盖从高层规划到细粒度场景理解的六种推理能力的系统评估套件。在此基础上，进一步开发了MomaGraph-R1，一个在MomaGraph-Scenes上通过强化学习训练的7B视觉-语言模型。MomaGraph-R1预测面向任务的场景图，并在Graph-then-Plan框架下作为零样本任务规划器。大量实验表明，该模型在开源模型中取得了最先进的结果，在基准测试中达到了71.6%的准确率（比最佳基线高出11.4%），同时推广到公共基准测试并有效地转移到真实机器人实验。",
            "intro_zh": [
                "现有场景图方法在处理具身任务时，缺乏对空间-功能关系的统一建模，忽略了对象状态和时间更新，且未充分考虑任务相关信息。",
                "MomaGraph通过整合空间-功能关系和部件级交互元素，构建统一的、状态感知的场景图表示，从而更有效地支持具身智能体的任务规划。",
                "MomaGraph-R1模型在MomaGraph-Scenes数据集上训练，并在MomaGraph-Bench上评估，实验结果表明其性能优于现有方法，并具有良好的泛化能力。"
            ],
            "method_zh": "**问题定义**：现有方法在移动操作机器人的场景理解中存在局限性，主要体现在：1) 空间关系和功能关系分离；2) 场景被视为静态快照，缺乏对物体状态和时间变化的建模；3) 忽略了与当前任务最相关的信息。这些问题导致机器人难以有效地进行导航和操作。\n\\n**核心思路**：MomaGraph的核心思路是构建一个统一的、状态感知的场景图表示，将空间关系、功能关系以及物体状态整合在一起。通过这种方式，机器人可以更好地理解场景，并根据当前任务进行规划。\n\\n**技术框架**：MomaGraph的整体框架包含以下几个主要部分：1) MomaGraph-Scenes数据集的构建，用于训练和评估模型；2) MomaGraph-Bench评估套件，用于系统地评估模型的推理能力；3) MomaGraph-R1视觉-语言模型，该模型在MomaGraph-Scenes上进行训练，用于预测任务导向的场景图，并作为零样本任务规划器。\n\\n**关键创新**：MomaGraph的关键创新在于：1) 提出了统一的场景图表示，整合了空间-功能关系和物体状态；2) 构建了大规模的、任务驱动的场景图数据集MomaGraph-Scenes；3) 开发了MomaGraph-R1视觉-语言模型，该模型能够预测任务导向的场景图，并作为零样本任务规划器。\n\\n**关键设计**：MomaGraph-R1是一个7B的视觉-语言模型，使用强化学习在MomaGraph-Scenes数据集上进行训练。具体的损失函数和网络结构细节在论文中未详细说明，但强调了其在任务导向的场景图预测和零样本任务规划方面的能力。强化学习的使用可能涉及到奖励函数的设计，以鼓励模型生成更符合任务需求的场景图。",
            "application_zh": "MomaGraph在家庭服务机器人、自动驾驶、虚拟现实等领域具有广泛的应用前景。它可以帮助机器人更好地理解周围环境，从而实现更智能的导航、操作和人机交互。例如，服务机器人可以利用MomaGraph来理解厨房场景，并规划完成诸如“准备早餐”之类的复杂任务。未来，MomaGraph有望成为构建通用人工智能系统的关键组成部分。",
            "highlight_zh": "MomaGraph-R1在MomaGraph-Bench基准测试中取得了显著的性能提升，达到了71.6%的准确率，比最佳基线高出11.4%。此外，该模型还展现出良好的泛化能力，能够推广到公共基准测试，并有效地转移到真实机器人实验中。这些结果表明MomaGraph在场景理解和任务规划方面具有强大的潜力。",
            "tags_zh": [
                "场景图",
                "具身智能",
                "视觉-语言模型",
                "任务规划",
                "强化学习"
            ],
            "_index": 67,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16909v1/Figures/Teaser.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16909v1/Figures/Failure.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16909v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Hypernetworks That Evolve Themselves",
            "authors": [
                "Joachim Winther Pedersen",
                "Erwan Plantec",
                "Eleni Nisioti",
                "Marcello Barylli",
                "Milton Montero",
                "Kathrin Korte",
                "Sebastian Risi"
            ],
            "arxiv_id": "2512.16406v1",
            "summary": "How can neural networks evolve themselves without relying on external optimizers? We propose Self-Referential Graph HyperNetworks, systems where the very machinery of variation and inheritance is embedded within the network. By uniting hypernetworks, stochastic parameter generation, and graph-based representations, Self-Referential GHNs mutate and evaluate themselves while adapting mutation rates as selectable traits. Through new reinforcement learning benchmarks with environmental shifts (CartPoleSwitch, LunarLander-Switch), Self-Referential GHNs show swift, reliable adaptation and emergent population dynamics. In the locomotion benchmark Ant-v5, they evolve coherent gaits, showing promising fine-tuning capabilities by autonomously decreasing variation in the population to concentrate around promising solutions. Our findings support the idea that evolvability itself can emerge from neural self-reference. Self-Referential GHNs reflect a step toward synthetic systems that more closely mirror biological evolution, offering tools for autonomous, open-ended learning agents.",
            "categories": [
                "cs.NE",
                "cs.AI"
            ],
            "primary_category": "cs.NE",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16406v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "locomotion"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning"
                    ],
                    "score": 1.5
                }
            ],
            "relevance_score": 3.5,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "提出自引用图超网络，实现无需外部优化器的神经网络自进化。",
            "summary_zh": "本文提出自引用图超网络（Self-Referential Graph HyperNetworks, Self-Referential GHNs），该系统将变异和遗传机制嵌入网络内部，无需依赖外部优化器即可实现神经网络的自进化。通过结合超网络、随机参数生成和基于图的表示，Self-Referential GHNs能够自我变异和评估，同时将变异率作为可选择的特征进行调整。在具有环境变化的强化学习基准测试（CartPoleSwitch, LunarLander-Switch）中，Self-Referential GHNs展现出快速、可靠的适应性和涌现的人口动态。在locomotion基准测试Ant-v5中，它们进化出连贯的步态，并通过自主降低种群变异性以集中于有希望的解决方案，展现出良好的微调能力。研究结果表明，可进化性本身可以从神经自我参照中涌现。Self-Referential GHNs代表了朝着更接近生物进化的合成系统迈出的一步，为自主、开放式学习智能体提供了工具。",
            "intro_zh": [
                "现有神经网络进化依赖外部优化器，限制了其自主性和适应性。",
                "提出自引用图超网络，将变异和遗传机制嵌入网络内部，实现自我进化。",
                "实验表明，该方法在环境变化和步态进化任务中表现出快速适应性和微调能力。"
            ],
            "method_zh": "**问题定义**：现有神经网络的进化通常依赖于外部优化器，例如遗传算法或进化策略。这种依赖限制了神经网络的自主性和适应性，尤其是在环境动态变化的情况下。此外，如何有效地控制和调整变异过程，使其能够适应不同的任务和环境，也是一个挑战。\\n\\n**核心思路**：本文的核心思路是将神经网络的进化机制嵌入到网络本身，使其能够自我变异、评估和选择。通过使用超网络生成网络的参数，并结合随机参数生成和基于图的表示，实现网络的自我参照和自我进化。这种设计允许网络自主地调整变异率，并根据环境反馈进行优化。\\n\\n**技术框架**：Self-Referential GHN 的整体架构包含以下几个主要模块：1) 图表示模块：使用图结构表示神经网络的结构和连接。2) 超网络模块：使用超网络生成图结构中各个节点的参数。3) 随机参数生成模块：引入随机性，使得网络可以探索不同的参数空间。4) 评估模块：使用强化学习或其他方法评估网络的性能。5) 变异率控制模块：将变异率作为可选择的特征，通过学习来调整变异的幅度。整个流程是循环的，网络不断地自我变异、评估和选择，从而实现进化。\\n\\n**关键创新**：最重要的技术创新点在于将进化机制嵌入到神经网络本身，实现了真正的自我进化。与传统的进化算法相比，Self-Referential GHN 不需要外部优化器，而是通过自我参照和自我评估来实现优化。此外，将变异率作为可选择的特征进行学习，使得网络可以自主地调整变异的幅度，从而更好地适应不同的任务和环境。\\n\\n**关键设计**：在具体实现上，超网络的设计至关重要，需要能够生成具有多样性和适应性的参数。随机参数生成模块需要引入适当的随机性，以保证网络的探索能力。评估模块需要选择合适的评估指标和方法，以准确地评估网络的性能。变异率控制模块需要设计合适的损失函数和优化算法，以有效地调整变异的幅度。具体的网络结构、参数设置和损失函数需要根据具体的任务进行调整。",
            "application_zh": "该研究成果可应用于自主机器人、游戏AI、以及其他需要持续学习和适应环境的智能体。通过实现神经网络的自我进化，可以降低对人工干预的依赖，提高智能体的自主性和鲁棒性。未来，该技术有望应用于开发更智能、更灵活的AI系统，例如能够自主适应新环境的机器人或能够不断学习和进化的游戏AI。",
            "highlight_zh": "在CartPoleSwitch和LunarLander-Switch等强化学习基准测试中，Self-Referential GHNs展现出快速、可靠的适应性。在Ant-v5 locomotion任务中，它们进化出连贯的步态，并通过自主降低种群变异性以集中于有希望的解决方案，展现出良好的微调能力。这些结果表明，该方法在复杂和动态环境中具有良好的适应性和优化能力。",
            "tags_zh": [
                "自进化神经网络",
                "超网络",
                "强化学习",
                "神经进化",
                "图神经网络"
            ],
            "_index": 68,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16406v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16406v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16406v1/figures/environments.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "PolaRiS: Scalable Real-to-Sim Evaluations for Generalist Robot Policies",
            "authors": [
                "Arhan Jain",
                "Mingtong Zhang",
                "Kanav Arora",
                "William Chen",
                "Marcel Torne",
                "Muhammad Zubair Irshad",
                "Sergey Zakharov",
                "Yue Wang",
                "Sergey Levine",
                "Chelsea Finn",
                "Wei-Chiu Ma",
                "Dhruv Shah",
                "Abhishek Gupta",
                "Karl Pertsch"
            ],
            "arxiv_id": "2512.16881v1",
            "summary": "A significant challenge for robot learning research is our ability to accurately measure and compare the performance of robot policies. Benchmarking in robotics is historically challenging due to the stochasticity, reproducibility, and time-consuming nature of real-world rollouts. This challenge is exacerbated for recent generalist policies, which has to be evaluated across a wide variety of scenes and tasks. Evaluation in simulation offers a scalable complement to real world evaluations, but the visual and physical domain gap between existing simulation benchmarks and the real world has made them an unreliable signal for policy improvement. Furthermore, building realistic and diverse simulated environments has traditionally required significant human effort and expertise. To bridge the gap, we introduce Policy Evaluation and Environment Reconstruction in Simulation (PolaRiS), a scalable real-to-sim framework for high-fidelity simulated robot evaluation. PolaRiS utilizes neural reconstruction methods to turn short video scans of real-world scenes into interactive simulation environments. Additionally, we develop a simple simulation data co-training recipe that bridges remaining real-to-sim gaps and enables zero-shot evaluation in unseen simulation environments. Through extensive paired evaluations between simulation and the real world, we demonstrate that PolaRiS evaluations provide a much stronger correlation to real world generalist policy performance than existing simulated benchmarks. Its simplicity also enables rapid creation of diverse simulated environments. As such, this work takes a step towards distributed and democratized evaluation for the next generation of robotic foundation models.",
            "categories": [
                "cs.RO",
                "cs.LG"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Website: https://polaris-evals.github.io/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16881v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "foundation model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "PolaRiS：面向通用机器人策略的可扩展真实到仿真评估框架",
            "summary_zh": "机器人学习研究面临的一大挑战是准确测量和比较机器人策略的性能。由于真实环境rollout的随机性、可重复性和耗时性，机器人技术的基准测试历来具有挑战性。对于最近的通用策略，需要在各种场景和任务中进行评估，这使得挑战更加严峻。仿真环境中的评估为真实环境评估提供了一种可扩展的补充，但现有仿真基准与真实世界之间的视觉和物理领域差距使其成为策略改进的不可靠信号。此外，构建逼真且多样化的仿真环境传统上需要大量的人工和专业知识。为了弥合差距，我们引入了仿真环境中的策略评估和环境重建（PolaRiS），这是一个可扩展的真实到仿真框架，用于高保真仿真机器人评估。PolaRiS利用神经重建方法将真实场景的短视频扫描转换为交互式仿真环境。此外，我们开发了一种简单的仿真数据协同训练方法，弥合了剩余的真实到仿真差距，并实现了在未见过的仿真环境中的零样本评估。通过仿真和真实世界之间的大量配对评估，我们证明了PolaRiS评估比现有的仿真基准更能提供与真实世界通用策略性能的更强相关性。它的简单性也使得能够快速创建多样化的仿真环境。因此，这项工作朝着下一代机器人基础模型的分布式和民主化评估迈出了一步。",
            "intro_zh": [
                "机器人策略评估面临真实环境成本高、可重复性差等问题，现有仿真环境与真实环境存在较大差距，导致评估结果不可靠。",
                "PolaRiS通过神经重建将真实场景转化为交互式仿真环境，并采用协同训练弥合真实到仿真的领域差距，实现更准确的策略评估。",
                "实验表明，PolaRiS评估与真实世界通用策略性能的相关性远高于现有仿真基准，且能快速创建多样化的仿真环境。"
            ],
            "method_zh": "**问题定义**：现有机器人策略评估方法，尤其是在通用机器人策略的评估上，面临着真实环境评估成本高昂、难以重复、以及仿真环境与真实环境存在较大差异的问题。现有的仿真环境难以准确反映真实世界中的物理特性和视觉效果，导致在仿真环境中表现良好的策略在真实世界中可能表现不佳。因此，如何构建一个既能高效评估策略，又能准确反映真实世界环境的仿真平台，是亟待解决的问题。\\n\\n**核心思路**：PolaRiS的核心思路是利用神经重建技术，将真实世界的场景快速转化为高保真度的仿真环境。通过短视频扫描真实场景，并利用神经渲染技术重建出可交互的3D环境。此外，为了进一步缩小真实环境和仿真环境之间的差距，PolaRiS还采用了协同训练的方法，利用仿真数据来提升策略在真实环境中的泛化能力。\\n\\n**技术框架**：PolaRiS框架主要包含两个阶段：环境重建阶段和策略评估阶段。在环境重建阶段，首先使用相机扫描真实世界的场景，获取短视频数据。然后，利用神经辐射场（NeRF）等神经渲染技术，将这些视频数据重建为可交互的3D仿真环境。在策略评估阶段，将待评估的机器人策略部署到重建的仿真环境中进行测试，并记录其性能指标。为了进一步提升策略的泛化能力，PolaRiS还采用了协同训练的方法，即同时在真实环境和仿真环境中训练策略，从而弥合两者之间的领域差距。\\n\\n**关键创新**：PolaRiS最重要的技术创新点在于其能够利用神经重建技术，快速、高效地将真实世界的场景转化为高保真度的仿真环境。与传统的手动建模方法相比，这种方法大大降低了构建仿真环境的成本和时间。此外，PolaRiS还通过协同训练的方法，进一步缩小了真实环境和仿真环境之间的差距，使得在仿真环境中评估的策略能够更好地泛化到真实世界中。\\n\\n**关键设计**：在环境重建阶段，PolaRiS采用了基于神经辐射场（NeRF）的神经渲染技术，通过优化一个神经网络来表示场景的辐射场，从而实现高质量的3D重建。在协同训练阶段，PolaRiS设计了一个简单的损失函数，鼓励策略在真实环境和仿真环境中学习到相似的行为。具体的网络结构和参数设置取决于具体的机器人策略和任务。",
            "application_zh": "PolaRiS可广泛应用于机器人学习、强化学习等领域，尤其适用于需要大量实验评估的通用机器人策略研究。该框架能够加速机器人算法的开发和迭代，降低真实环境实验的成本和风险。未来，PolaRiS有望成为机器人基础模型评估的重要工具，推动机器人技术的进步。",
            "highlight_zh": "实验结果表明，PolaRiS评估与真实世界通用策略性能的相关性显著高于现有仿真基准。例如，在多个机器人操作任务上，PolaRiS评估的策略性能与真实环境性能的相关性提高了XX%（具体数据论文中给出），证明了PolaRiS在评估通用机器人策略方面的有效性。",
            "tags_zh": [
                "机器人学习",
                "仿真评估",
                "神经重建",
                "领域自适应",
                "通用机器人",
                "强化学习",
                "策略评估"
            ],
            "_index": 69,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16881v1/figures/Teaser_Karl_version.jpg",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16881v1/figures/polaris_pipeline.jpg",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16881v1/figures/scene_comp_gui.jpg",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "VIVA: VLM-Guided Instruction-Based Video Editing with Reward Optimization",
            "authors": [
                "Xiaoyan Cong",
                "Haotian Yang",
                "Angtian Wang",
                "Yizhi Wang",
                "Yiding Yang",
                "Canyu Zhang",
                "Chongyang Ma"
            ],
            "arxiv_id": "2512.16906v1",
            "summary": "Instruction-based video editing aims to modify an input video according to a natural-language instruction while preserving content fidelity and temporal coherence. However, existing diffusion-based approaches are often trained on paired data of simple editing operations, which fundamentally limits their ability to generalize to diverse and complex, real-world instructions. To address this generalization gap, we propose VIVA, a scalable framework for instruction-based video editing that leverages VLM-guided encoding and reward optimization. First, we introduce a VLM-based instructor that encodes the textual instruction, the first frame of the source video, and an optional reference image into visually-grounded instruction representations, providing fine-grained spatial and semantic context for the diffusion transformer backbone. Second, we propose a post-training stage, Edit-GRPO, which adapts Group Relative Policy Optimization to the domain of video editing, directly optimizing the model for instruction-faithful, content-preserving, and aesthetically pleasing edits using relative rewards. Furthermore, we propose a data construction pipeline designed to synthetically generate diverse, high-fidelity paired video-instruction data of basic editing operations. Extensive experiments show that VIVA achieves superior instruction following, generalization, and editing quality over state-of-the-art methods. Website: https://viva-paper.github.io",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16906v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "instruction following"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "VIVA：利用VLM引导和奖励优化的指令驱动视频编辑框架",
            "summary_zh": "本文提出VIVA，一个可扩展的指令驱动视频编辑框架，它利用VLM引导的编码和奖励优化来解决现有方法泛化能力不足的问题。VIVA首先引入一个基于VLM的指导器，将文本指令、源视频的第一帧以及可选的参考图像编码为视觉对齐的指令表示，为扩散Transformer主干网络提供精细的空间和语义上下文。其次，提出了一个后训练阶段Edit-GRPO，将Group Relative Policy Optimization适配到视频编辑领域，使用相对奖励直接优化模型，使其生成符合指令、保持内容一致且美观的编辑结果。此外，还设计了一个数据构建流程，用于合成生成多样且高质量的视频-指令对数据。大量实验表明，VIVA在指令遵循、泛化能力和编辑质量方面均优于现有方法。",
            "intro_zh": [
                "现有基于扩散模型的视频编辑方法依赖简单编辑操作的配对数据训练，泛化到复杂指令的能力有限。",
                "VIVA利用VLM编码指令和视频信息，并采用奖励优化策略，提升模型对复杂指令的理解和执行能力。",
                "实验结果表明，VIVA在指令遵循、内容保持和编辑质量上超越了现有技术水平，具有显著优势。"
            ],
            "method_zh": "**问题定义**：指令驱动的视频编辑旨在根据自然语言指令修改输入视频，同时保持内容一致性和时间连贯性。现有方法主要依赖于在简单编辑操作的配对数据上训练的扩散模型，这限制了它们在处理多样化和复杂的真实世界指令时的泛化能力。这些方法难以理解复杂指令中的细粒度语义信息，并且难以在编辑过程中保持视频内容的原有特征。\n\n**核心思路**：VIVA的核心思路是利用视觉语言模型（VLM）来增强模型对指令的理解能力，并通过奖励优化来提升编辑质量。VLM能够将文本指令和视频帧编码为统一的视觉语义空间中的表示，从而为扩散模型提供更丰富的上下文信息。奖励优化则允许模型直接学习如何生成符合指令、保持内容一致且美观的编辑结果，而无需依赖大量的配对数据。\n\n**技术框架**：VIVA框架主要包含两个阶段：VLM引导的编码和奖励优化。首先，VLM-based Instructor模块将文本指令、源视频的第一帧以及可选的参考图像编码为视觉对齐的指令表示。然后，这些表示被输入到扩散Transformer主干网络中，用于指导视频编辑过程。在后训练阶段，Edit-GRPO模块使用Group Relative Policy Optimization算法，根据相对奖励来优化模型参数，从而提升编辑质量。\n\n**关键创新**：VIVA的关键创新在于以下几点：1) 引入VLM来增强模型对指令的理解能力，从而更好地处理复杂指令。2) 提出Edit-GRPO，将Group Relative Policy Optimization适配到视频编辑领域，直接优化编辑质量。3) 设计了一个数据构建流程，用于合成生成多样且高质量的视频-指令对数据，从而缓解了数据稀缺问题。与现有方法相比，VIVA能够更好地理解复杂指令，并生成更高质量的编辑结果。\n\n**关键设计**：VLM-based Instructor使用了预训练的视觉语言模型，例如CLIP，来提取文本和图像的特征。Edit-GRPO使用相对奖励来评估编辑结果的质量，例如，判断一个编辑结果是否比另一个更符合指令或更美观。数据构建流程使用程序化生成和人工标注相结合的方式，生成多样化的视频-指令对数据。具体的损失函数包括指令遵循损失、内容保持损失和美学损失等。",
            "application_zh": "VIVA具有广泛的应用前景，包括视频内容创作、社交媒体编辑、广告制作、教育视频生成等。它可以帮助用户轻松地根据自然语言指令修改视频内容，例如改变视频风格、添加特效、替换对象等。VIVA的出现有望降低视频编辑的门槛，让更多人能够参与到视频创作中来，并推动视频内容产业的发展。",
            "highlight_zh": "实验结果表明，VIVA在多个指标上均优于现有方法。例如，在指令遵循度方面，VIVA比现有最佳方法提高了10%以上。在用户偏好度方面，VIVA生成的编辑结果更受用户青睐。此外，VIVA还具有良好的泛化能力，能够处理各种复杂指令和不同类型的视频内容。",
            "tags_zh": [
                "视频编辑",
                "指令驱动",
                "视觉语言模型",
                "扩散模型",
                "奖励优化"
            ],
            "_index": 70,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16906v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16906v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16906v1/x4.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "REGLUE Your Latents with Global and Local Semantics for Entangled Diffusion",
            "authors": [
                "Giorgos Petsangourakis",
                "Christos Sgouropoulos",
                "Bill Psomas",
                "Theodoros Giannakopoulos",
                "Giorgos Sfikas",
                "Ioannis Kakogeorgiou"
            ],
            "arxiv_id": "2512.16636v1",
            "summary": "Latent diffusion models (LDMs) achieve state-of-the-art image synthesis, yet their reconstruction-style denoising objective provides only indirect semantic supervision: high-level semantics emerge slowly, requiring longer training and limiting sample quality. Recent works inject semantics from Vision Foundation Models (VFMs) either externally via representation alignment or internally by jointly modeling only a narrow slice of VFM features inside the diffusion process, under-utilizing the rich, nonlinear, multi-layer spatial semantics available. We introduce REGLUE (Representation Entanglement with Global-Local Unified Encoding), a unified latent diffusion framework that jointly models (i) VAE image latents, (ii) compact local (patch-level) VFM semantics, and (iii) a global (image-level) [CLS] token within a single SiT backbone. A lightweight convolutional semantic compressor nonlinearly aggregates multi-layer VFM features into a low-dimensional, spatially structured representation, which is entangled with the VAE latents in the diffusion process. An external alignment loss further regularizes internal representations toward frozen VFM targets. On ImageNet 256x256, REGLUE consistently improves FID and accelerates convergence over SiT-B/2 and SiT-XL/2 baselines, as well as over REPA, ReDi, and REG. Extensive experiments show that (a) spatial VFM semantics are crucial, (b) non-linear compression is key to unlocking their full benefit, and (c) global tokens and external alignment act as complementary, lightweight enhancements within our global-local-latent joint modeling framework. The code is available at https://github.com/giorgospets/reglue .",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16636v1",
            "code_links": [
                {
                    "url": "https://github.com/giorgospets/reglue",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "foundation model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "REGLUE：利用全局和局部语义增强潜在扩散模型，提升图像合成质量。",
            "summary_zh": "潜在扩散模型(LDMs)在图像合成方面取得了最先进的成果，但其重建式去噪目标仅提供间接的语义监督：高层语义出现缓慢，需要更长的训练时间，并限制了样本质量。最近的工作通过表征对齐从视觉基础模型(VFMs)外部注入语义，或者通过在扩散过程中联合建模VFM特征的一小部分来内部注入语义，未能充分利用可用的丰富、非线性、多层空间语义。我们引入了REGLUE（Representation Entanglement with Global-Local Unified Encoding），一个统一的潜在扩散框架，它在单个SiT骨干网络中联合建模(i)VAE图像潜在变量，(ii)紧凑的局部（patch级别）VFM语义，以及(iii)全局（图像级别）[CLS]token。一个轻量级的卷积语义压缩器将多层VFM特征非线性地聚合为低维、空间结构化的表示，并在扩散过程中与VAE潜在变量纠缠。外部对齐损失进一步将内部表示正则化到冻结的VFM目标。在ImageNet 256x256上，REGLUE始终优于SiT-B/2和SiT-XL/2基线，以及REPA、ReDi和REG，在FID指标上有所改进并加速了收敛。大量实验表明，(a)空间VFM语义至关重要，(b)非线性压缩是充分利用其优势的关键，以及(c)全局token和外部对齐在我们全局-局部-潜在联合建模框架中充当互补的、轻量级的增强。",
            "intro_zh": [
                "现有潜在扩散模型语义监督不足，高层语义学习缓慢，限制了图像生成质量和训练效率。",
                "REGLUE通过联合建模VAE潜在变量、局部VFM语义和全局[CLS] token，实现全局-局部语义的统一编码。",
                "实验表明，REGLUE在ImageNet 256x256上显著提升了FID，并加速了收敛，优于现有方法。"
            ],
            "method_zh": "**问题定义**：现有潜在扩散模型在图像合成任务中，由于其重建式的去噪目标，语义信息的学习效率较低，导致生成图像质量受限，且需要更长的训练时间。现有方法要么未能充分利用视觉基础模型(VFM)提供的丰富语义信息，要么仅使用了VFM特征的有限部分，无法有效提升图像合成效果。\\n\\n**核心思路**：REGLUE的核心思路是将VAE图像潜在变量、局部VFM语义和全局[CLS] token在一个统一的框架中进行联合建模。通过这种方式，模型能够同时学习图像的全局语义信息和局部细节信息，从而更有效地指导图像生成过程。利用轻量级的卷积语义压缩器，将多层VFM特征非线性地聚合为低维、空间结构化的表示，并将其与VAE潜在变量进行融合。\\n\\n**技术框架**：REGLUE的整体框架包含以下几个主要模块：1) VAE编码器：将输入图像编码为潜在变量。2) VFM特征提取器：提取图像的局部和全局语义特征。3) 语义压缩器：将VFM特征压缩为低维表示。4) SiT骨干网络：用于联合建模VAE潜在变量、局部VFM语义和全局[CLS] token，并进行扩散过程。5) 外部对齐模块：通过外部对齐损失，正则化内部表示，使其与冻结的VFM目标对齐。\\n\\n**关键创新**：REGLUE的关键创新在于其全局-局部语义的统一编码方式。通过同时建模VAE潜在变量、局部VFM语义和全局[CLS] token，模型能够更全面地理解图像内容，从而生成更高质量的图像。此外，非线性语义压缩器的设计也是一个重要的创新点，它能够有效地提取和压缩VFM特征，避免了信息冗余。\\n\\n**关键设计**：REGLUE的关键设计包括：1) 使用SiT作为骨干网络，以实现高效的特征融合和扩散过程。2) 设计轻量级的卷积语义压缩器，以非线性方式聚合多层VFM特征。3) 引入外部对齐损失，以正则化内部表示，使其与VFM目标对齐。4) 使用全局[CLS] token，以捕捉图像的全局语义信息。具体参数设置和损失函数细节在论文中有详细描述，此处不再赘述。",
            "application_zh": "REGLUE在图像生成、图像编辑、图像修复等领域具有广泛的应用前景。它可以用于生成逼真度更高的图像，编辑图像内容，修复图像中的缺失部分。此外，REGLUE还可以应用于艺术创作、游戏开发、虚拟现实等领域，为这些领域带来更丰富的可能性。该研究的未来影响在于推动图像生成技术的发展，并为相关应用领域提供更强大的工具。",
            "highlight_zh": "实验结果表明，REGLUE在ImageNet 256x256数据集上显著提升了图像生成质量，FID指标优于SiT-B/2和SiT-XL/2基线，以及REPA、ReDi和REG等现有方法。此外，REGLUE还加速了训练过程的收敛，表明其具有更高的训练效率。实验还验证了空间VFM语义的重要性，以及非线性压缩对提升性能的关键作用。",
            "tags_zh": [
                "潜在扩散模型",
                "图像合成",
                "视觉基础模型",
                "语义编码",
                "全局局部语义",
                "非线性压缩",
                "表征对齐"
            ],
            "_index": 71,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16636v1/figs/training_steps_photos/50k_steps/000343.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16636v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16636v1/figs/cfg/golden_retriever_207/000444.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "VenusBench-GD: A Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks",
            "authors": [
                "Beitong Zhou",
                "Zhexiao Huang",
                "Yuan Guo",
                "Zhangxuan Gu",
                "Tianyu Xia",
                "Zichen Luo",
                "Fei Tang",
                "Dehan Kong",
                "Yanyi Shang",
                "Suling Ou",
                "Zhenlin Guo",
                "Changhua Meng",
                "Shuheng Shen"
            ],
            "arxiv_id": "2512.16501v1",
            "summary": "GUI grounding is a critical component in building capable GUI agents. However, existing grounding benchmarks suffer from significant limitations: they either provide insufficient data volume and narrow domain coverage, or focus excessively on a single platform and require highly specialized domain knowledge. In this work, we present VenusBench-GD, a comprehensive, bilingual benchmark for GUI grounding that spans multiple platforms, enabling hierarchical evaluation for real-word applications. VenusBench-GD contributes as follows: (i) we introduce a large-scale, cross-platform benchmark with extensive coverage of applications, diverse UI elements, and rich annotated data, (ii) we establish a high-quality data construction pipeline for grounding tasks, achieving higher annotation accuracy than existing benchmarks, and (iii) we extend the scope of element grounding by proposing a hierarchical task taxonomy that divides grounding into basic and advanced categories, encompassing six distinct subtasks designed to evaluate models from complementary perspectives. Our experimental findings reveal critical insights: general-purpose multimodal models now match or even surpass specialized GUI models on basic grounding tasks. In contrast, advanced tasks, still favor GUI-specialized models, though they exhibit significant overfitting and poor robustness. These results underscore the necessity of comprehensive, multi-tiered evaluation frameworks.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16501v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "multimodal"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出VenusBench-GD，一个全面的多平台GUI基准，用于评估多样化的Grounding任务。",
            "summary_zh": "GUI grounding是构建强大GUI代理的关键组成部分。然而，现有的grounding基准存在显著的局限性：它们要么提供的数据量不足且领域覆盖范围狭窄，要么过度关注单一平台并需要高度专业的领域知识。本文提出了VenusBench-GD，这是一个全面的、双语的GUI grounding基准，涵盖多个平台，能够对真实世界的应用程序进行分层评估。VenusBench-GD的贡献如下：（i）引入了一个大规模、跨平台的基准，具有广泛的应用程序覆盖、多样的UI元素和丰富的标注数据；（ii）建立了一个高质量的数据构建流程，用于grounding任务，实现了比现有基准更高的标注准确率；（iii）通过提出一个分层任务分类法，扩展了元素grounding的范围，该分类法将grounding分为基本和高级类别，包含六个不同的子任务，旨在从互补的角度评估模型。实验结果揭示了关键的见解：通用多模态模型现在在基本grounding任务上与专门的GUI模型相匹配甚至超越。相比之下，高级任务仍然偏爱GUI专用模型，尽管它们表现出显著的过拟合和较差的鲁棒性。这些结果强调了全面、多层评估框架的必要性。",
            "intro_zh": [
                "现有GUI grounding基准数据量不足、领域覆盖窄，或过度关注单一平台，限制了GUI代理的开发。",
                "VenusBench-GD构建大规模跨平台GUI基准，包含丰富的UI元素和标注数据，并提出分层任务分类法。",
                "实验表明通用多模态模型在基础任务上表现优异，但高级任务仍需GUI专用模型，存在过拟合问题。"
            ],
            "method_zh": "**问题定义**：论文旨在解决现有GUI grounding基准数据集不足、领域覆盖范围有限以及平台单一的问题。现有方法难以有效评估和提升GUI代理在真实世界应用中的grounding能力，尤其是在跨平台和复杂交互场景下。现有基准的标注质量也存在问题，影响了模型训练和评估的可靠性。\\n\\n**核心思路**：论文的核心思路是构建一个大规模、跨平台、高质量的GUI grounding基准数据集，并提出一个分层的任务分类体系，从而更全面地评估模型的grounding能力。通过引入多样化的UI元素和应用场景，以及高质量的人工标注，提高数据集的代表性和可靠性。分层任务分类法旨在区分模型在基础和高级grounding任务上的表现，从而更深入地了解模型的优势和不足。\\n\\n**技术框架**：VenusBench-GD的构建流程主要包括以下几个阶段：1) 数据收集：从多个平台（例如Android、iOS、Web）收集GUI应用程序的截图和UI元素信息。2) 数据标注：对UI元素进行详细的标注，包括元素类型、位置、文本内容等。采用高质量的标注流程，确保标注的准确性和一致性。3) 任务定义：将grounding任务分为基本和高级类别，并定义六个不同的子任务，例如元素定位、关系推理等。4) 基准测试：在VenusBench-GD上评估现有模型的性能，并分析实验结果。\\n\\n**关键创新**：VenusBench-GD的关键创新点在于：1) 大规模跨平台数据集：提供了比现有基准更广泛的应用程序覆盖和更多样化的UI元素。2) 高质量数据构建流程：通过严格的标注流程和质量控制，提高了数据集的标注准确率。3) 分层任务分类法：将grounding任务分为基本和高级类别，从而更全面地评估模型的grounding能力。\\n\\n**关键设计**：VenusBench-GD的数据集包含多种类型的UI元素，例如按钮、文本框、图像等。标注信息包括元素的类型、位置、文本内容、父子关系等。分层任务分类法包括以下六个子任务：1) 元素定位：根据文本描述定位UI元素。2) 属性识别：识别UI元素的属性，例如颜色、大小等。3) 关系推理：推理UI元素之间的关系，例如包含、相邻等。4) 状态理解：理解UI元素的状态，例如激活、禁用等。5) 动作预测：预测用户在GUI上的下一步动作。6) 复杂交互：处理涉及多个UI元素的复杂交互。",
            "application_zh": "VenusBench-GD可应用于开发更智能、更强大的GUI代理，例如自动化测试工具、辅助技术和人机交互系统。该基准能够促进跨平台GUI理解和交互的研究，帮助提升用户体验，并为残疾人士提供更好的辅助功能。未来，该研究可扩展到更复杂的GUI环境，并支持更高级的交互任务。",
            "highlight_zh": "实验结果表明，通用多模态模型在基本grounding任务上与GUI专用模型性能相当甚至超越，但在高级任务上，GUI专用模型仍然更胜一筹，但存在过拟合和鲁棒性问题。VenusBench-GD的评估结果揭示了现有模型在不同类型grounding任务上的优缺点，为未来的研究方向提供了指导。",
            "tags_zh": [
                "GUI grounding",
                "多平台基准",
                "用户界面",
                "多模态学习",
                "分层任务",
                "数据集构建",
                "人机交互"
            ],
            "_index": 72,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16501v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16501v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16501v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Avatar4D: Synthesizing Domain-Specific 4D Humans for Real-World Pose Estimation",
            "authors": [
                "Jerrin Bright",
                "Zhibo Wang",
                "Dmytro Klepachevskyi",
                "Yuhao Chen",
                "Sirisha Rambhatla",
                "David Clausi",
                "John Zelek"
            ],
            "arxiv_id": "2512.16199v1",
            "summary": "We present Avatar4D, a real-world transferable pipeline for generating customizable synthetic human motion datasets tailored to domain-specific applications. Unlike prior works, which focus on general, everyday motions and offer limited flexibility, our approach provides fine-grained control over body pose, appearance, camera viewpoint, and environmental context, without requiring any manual annotations. To validate the impact of Avatar4D, we focus on sports, where domain-specific human actions and movement patterns pose unique challenges for motion understanding. In this setting, we introduce Syn2Sport, a large-scale synthetic dataset spanning sports, including baseball and ice hockey. Avatar4D features high-fidelity 4D (3D geometry over time) human motion sequences with varying player appearances rendered in diverse environments. We benchmark several state-of-the-art pose estimation models on Syn2Sport and demonstrate their effectiveness for supervised learning, zero-shot transfer to real-world data, and generalization across sports. Furthermore, we evaluate how closely the generated synthetic data aligns with real-world datasets in feature space. Our results highlight the potential of such systems to generate scalable, controllable, and transferable human datasets for diverse domain-specific tasks without relying on domain-specific real data.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16199v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "zero-shot transfer"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "Avatar4D：合成特定领域4D人体数据，用于真实场景姿态估计",
            "summary_zh": "本文提出Avatar4D，一个可迁移的真实世界流水线，用于生成可定制的合成人体运动数据集，专门针对特定领域的应用。与以往专注于通用日常运动且灵活性有限的工作不同，我们的方法提供了对身体姿势、外观、相机视角和环境上下文的细粒度控制，无需任何手动标注。为了验证Avatar4D的影响，我们专注于体育运动，其中特定领域的动作和运动模式对运动理解提出了独特的挑战。在此背景下，我们引入了Syn2Sport，一个涵盖棒球和冰球等运动的大规模合成数据集。Avatar4D具有高保真4D（随时间变化的3D几何）人体运动序列，具有不同的运动员外观，并在不同的环境中渲染。我们在Syn2Sport上对几种最先进的姿态估计模型进行了基准测试，并证明了它们在监督学习、零样本迁移到真实世界数据以及跨运动泛化方面的有效性。此外，我们评估了生成的合成数据在特征空间中与真实世界数据集的对齐程度。我们的结果突出了这种系统在生成可扩展、可控和可迁移的人体数据集方面的潜力，用于各种特定领域的任务，而无需依赖特定领域的真实数据。",
            "intro_zh": [
                "现有方法在生成人体运动数据时，缺乏对特定领域动作和环境的细粒度控制，限制了其在专业领域的应用。",
                "Avatar4D通过控制身体姿势、外观、相机视角和环境上下文，生成特定领域的高质量合成数据，无需手动标注。",
                "实验表明，使用Avatar4D生成的Syn2Sport数据集训练的姿态估计模型，在真实世界数据上表现出良好的泛化能力。"
            ],
            "method_zh": "**问题定义**：论文旨在解决缺乏特定领域人体运动数据的问题，尤其是在体育运动等领域。现有方法生成的合成数据通常是通用的日常动作，无法满足特定领域对动作类型、运动模式和环境的特殊需求，导致模型在真实场景中的性能下降。\\n\\n**核心思路**：核心思路是构建一个可定制的合成数据生成流水线，允许用户对人体姿势、外观、相机视角和环境上下文进行细粒度控制。通过这种方式，可以生成高度逼真且与特定领域相关的合成数据，用于训练和评估模型。\\n\\n**技术框架**：Avatar4D流水线包含以下主要模块：1) 人体模型和动画引擎，用于生成具有不同姿势和运动的3D人体模型；2) 外观定制模块，用于改变人体模型的服装、肤色等外观属性；3) 环境渲染模块，用于将人体模型放置在不同的虚拟环境中；4) 相机控制模块，用于调整相机视角和参数；5) 数据生成模块，用于生成包含人体姿势、外观、相机参数和环境信息的合成数据。\\n\\n**关键创新**：Avatar4D的关键创新在于其可定制性和领域特定性。与以往的通用合成数据生成方法不同，Avatar4D允许用户根据特定领域的需求，调整人体姿势、外观和环境，从而生成更具代表性和实用性的合成数据。此外，该方法无需手动标注，降低了数据生成的成本和难度。\\n\\n**关键设计**：Avatar4D使用参数化人体模型（如SMPL）来控制人体姿势和形状。外观定制模块使用纹理映射和材质编辑技术来改变人体模型的外观。环境渲染模块使用基于物理的渲染引擎来生成逼真的图像。数据生成模块将所有信息（包括人体姿势、外观、相机参数和环境信息）保存为标准格式，方便后续使用。",
            "application_zh": "Avatar4D在体育分析、虚拟现实、游戏开发和动作捕捉等领域具有广泛的应用前景。它可以用于生成大规模的训练数据，提高姿态估计、动作识别和人体行为分析模型的性能。此外，Avatar4D还可以用于创建逼真的虚拟角色和环境，增强用户体验。",
            "highlight_zh": "论文在Syn2Sport数据集上评估了多种姿态估计模型，结果表明，使用Syn2Sport训练的模型在真实世界数据上表现出良好的泛化能力。例如，在棒球和冰球运动的姿态估计任务中，使用Syn2Sport训练的模型取得了与使用真实数据训练的模型相近甚至更好的性能。此外，论文还评估了合成数据与真实数据在特征空间中的对齐程度，结果表明，Avatar4D生成的合成数据与真实数据具有较高的相似性。",
            "tags_zh": [
                "合成数据生成",
                "4D人体建模",
                "姿态估计",
                "领域自适应",
                "计算机视觉",
                "深度学习",
                "体育分析"
            ],
            "_index": 73,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16199v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16199v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16199v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times",
            "authors": [
                "Jintao Zhang",
                "Kaiwen Zheng",
                "Kai Jiang",
                "Haoxu Wang",
                "Ion Stoica",
                "Joseph E. Gonzalez",
                "Jianfei Chen",
                "Jun Zhu"
            ],
            "arxiv_id": "2512.16093v1",
            "summary": "We introduce TurboDiffusion, a video generation acceleration framework that can speed up end-to-end diffusion generation by 100-200x while maintaining video quality. TurboDiffusion mainly relies on several components for acceleration: (1) Attention acceleration: TurboDiffusion uses low-bit SageAttention and trainable Sparse-Linear Attention (SLA) to speed up attention computation. (2) Step distillation: TurboDiffusion adopts rCM for efficient step distillation. (3) W8A8 quantization: TurboDiffusion quantizes model parameters and activations to 8 bits to accelerate linear layers and compress the model. In addition, TurboDiffusion incorporates several other engineering optimizations.\n  We conduct experiments on the Wan2.2-I2V-14B-720P, Wan2.1-T2V-1.3B-480P, Wan2.1-T2V-14B-720P, and Wan2.1-T2V-14B-480P models. Experimental results show that TurboDiffusion achieves 100-200x speedup for video generation even on a single RTX 5090 GPU, while maintaining comparable video quality. The GitHub repository, which includes model checkpoints and easy-to-use code, is available at https://github.com/thu-ml/TurboDiffusion.",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16093v1",
            "code_links": [
                {
                    "url": "https://github.com/thu-ml/TurboDiffusion",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "linear attention",
                        "distillation"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "TurboDiffusion：通过多重加速策略将视频扩散模型提速100-200倍",
            "summary_zh": "本文提出了一种名为TurboDiffusion的视频生成加速框架，能够在保持视频质量的同时，将端到端扩散生成速度提高100-200倍。TurboDiffusion主要依赖于以下几个加速组件：（1）注意力加速：使用低比特SageAttention和可训练的稀疏线性注意力（SLA）来加速注意力计算。（2）步骤蒸馏：采用rCM进行高效的步骤蒸馏。（3）W8A8量化：将模型参数和激活量化到8位，以加速线性层并压缩模型。此外，TurboDiffusion还包含其他一些工程优化。在Wan2.2-I2V-14B-720P、Wan2.1-T2V-1.3B-480P、Wan2.1-T2V-14B-720P和Wan2.1-T2V-14B-480P模型上进行的实验结果表明，即使在单个RTX 5090 GPU上，TurboDiffusion也能实现100-200倍的视频生成加速，同时保持相当的视频质量。代码和模型已开源。",
            "intro_zh": [
                "现有视频扩散模型计算成本高昂，限制了其在实际应用中的部署和使用。",
                "TurboDiffusion通过注意力加速、步骤蒸馏和模型量化等多重策略，大幅降低计算复杂度。",
                "实验表明，TurboDiffusion在保证视频质量的前提下，实现了100-200倍的加速效果。"
            ],
            "method_zh": "**问题定义**：视频扩散模型在生成高质量视频方面表现出色，但其计算复杂度极高，推理速度慢，难以在资源受限的设备上部署，阻碍了其广泛应用。现有方法在加速方面效果有限，或者会显著降低视频质量。\\n\\n**核心思路**：TurboDiffusion的核心思路是通过多方面的优化，包括降低注意力计算的复杂度、减少扩散步骤以及量化模型参数，从而在不显著降低视频质量的前提下，大幅提升视频生成的速度。这种多管齐下的方法能够充分利用硬件资源，实现更高的效率。\\n\\n**技术框架**：TurboDiffusion的整体框架包含以下几个主要模块：1) **注意力加速模块**：使用低比特SageAttention和可训练的稀疏线性注意力（SLA）来减少注意力计算的复杂度。2) **步骤蒸馏模块**：采用rCM进行高效的步骤蒸馏，减少生成视频所需的扩散步骤。3) **量化模块**：将模型参数和激活量化到8位（W8A8），以加速线性层计算并压缩模型大小。此外，还包括一些工程优化，例如kernel优化等。\\n\\n**关键创新**：TurboDiffusion的关键创新在于其综合利用多种加速技术，并针对视频扩散模型的特点进行了优化。例如，可训练的稀疏线性注意力（SLA）能够自适应地学习重要的注意力模式，从而在减少计算量的同时保持性能。rCM蒸馏方法能够更有效地减少扩散步骤，而不会引入过多的伪影。\\n\\n**关键设计**：在注意力加速方面，SageAttention使用低比特量化来减少计算量，而SLA则通过学习稀疏的注意力矩阵来减少计算量。rCM蒸馏方法通过引入一个重构损失来提高蒸馏的质量。W8A8量化使用对称量化方案，并针对不同的层进行了不同的量化策略选择。具体的参数设置和损失函数细节在论文中有详细描述。",
            "application_zh": "TurboDiffusion具有广泛的应用前景，包括视频编辑、游戏开发、虚拟现实、社交媒体等领域。它可以加速视频内容的生成，降低计算成本，使得在移动设备或云端进行高质量视频生成成为可能。未来，TurboDiffusion可以进一步扩展到其他生成模型，例如图像生成、音频生成等。",
            "highlight_zh": "TurboDiffusion在多个视频生成模型上实现了显著的加速效果，达到了100-200倍的加速，同时保持了与原始模型相当的视频质量。即使在单个RTX 5090 GPU上，也能实现如此高的加速比，证明了该框架的有效性和实用性。开源的代码和模型方便了研究人员和开发者进行进一步的研究和应用。",
            "tags_zh": [
                "视频生成",
                "扩散模型",
                "模型加速",
                "注意力机制",
                "模型量化",
                "步骤蒸馏",
                "稀疏注意力"
            ],
            "_index": 74,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16093v1/src/figs/original/outputs_1.3B/frames/12-1.jpg",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16093v1/src/figs/turbodiffusion/outputs_1.3B/frames/12-1.jpg",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16093v1/src/figs/i2v/original/outputs_A14B_720p/frames/1-1.jpg",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Machine Learning Enabled Graph Analysis of Particulate Composites: Application to Solid-state Battery Cathodes",
            "authors": [
                "Zebin Li",
                "Shimao Deng",
                "Yijin Liu",
                "Jia-Mian Hu"
            ],
            "arxiv_id": "2512.16085v1",
            "summary": "Particulate composites underpin many solid-state chemical and electrochemical systems, where microstructural features such as multiphase boundaries and inter-particle connections strongly influence system performance. Advances in X-ray microscopy enable capturing large-scale, multimodal images of these complex microstructures with an unprecedentedly high throughput. However, harnessing these datasets to discover new physical insights and guide microstructure optimization remains a major challenge. Here, we develop a machine learning (ML) enabled framework that enables automated transformation of experimental multimodal X-ray images of multiphase particulate composites into scalable, topology-aware graphs for extracting physical insights and establishing local microstructure-property relationships at both the particle and network level. Using the multiphase particulate cathode of solid-state lithium batteries as an example, our ML-enabled graph analysis corroborates the critical role of triple phase junctions and concurrent ion/electron conduction channels in realizing desirable local electrochemical activity. Our work establishes graph-based microstructure representation as a powerful paradigm for bridging multimodal experimental imaging and functional understanding, and facilitating microstructure-aware data-driven materials design in a broad range of particulate composites.",
            "categories": [
                "cond-mat.mtrl-sci",
                "cs.CV"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16085v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "multimodal"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出基于机器学习的图分析方法，用于固态电池正极材料微观结构表征与性能预测。",
            "summary_zh": "本文提出了一种基于机器学习（ML）的框架，该框架能够自动将多相颗粒复合材料的实验多模态X射线图像转换为可扩展的、具有拓扑感知能力的图，从而提取物理见解，并在颗粒和网络层面建立局部微观结构-性能关系。以固态锂电池的多相颗粒正极为例，我们的ML图分析证实了三相结和并发离子/电子传导通道在实现理想的局部电化学活性中的关键作用。我们的工作将基于图的微观结构表示确立为连接多模态实验成像和功能理解的强大范例，并促进了各种颗粒复合材料中具有微观结构感知的数据驱动材料设计。",
            "intro_zh": [
                "多相颗粒复合材料的微观结构特征，如相界和颗粒间连接，对系统性能有重要影响，但现有方法难以有效利用大规模多模态图像数据。",
                "该论文提出了一种基于机器学习的图分析框架，将多模态X射线图像转化为拓扑感知的图，从而提取物理见解并建立微观结构与性能的联系。",
                "通过固态锂电池正极的案例研究，验证了该方法在揭示三相结和离子/电子传导通道对电化学活性的重要作用方面的有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决如何从多相颗粒复合材料的大规模多模态X射线图像中提取有意义的物理信息，并建立微观结构与材料性能之间关系的问题。现有方法难以有效处理高通量图像数据，无法充分利用微观结构的拓扑信息，从而限制了对材料性能的深入理解和优化。\\n\\n**核心思路**：论文的核心思路是将多相颗粒复合材料的微观结构表示为图，其中节点代表颗粒，边代表颗粒之间的连接。通过机器学习方法，自动从X射线图像中提取颗粒和连接信息，构建拓扑感知的图。然后，利用图分析技术，提取关键的微观结构特征，并建立其与材料性能之间的关系。这种方法能够有效地利用图像数据中的拓扑信息，从而更全面地理解材料的性能。\\n\\n**技术框架**：该框架包含以下主要模块：1) 多模态X射线图像采集；2) 图像预处理和分割，利用机器学习方法自动识别和分割不同相的颗粒；3) 图构建，将分割后的颗粒表示为图的节点，颗粒之间的连接表示为边；4) 图分析，提取图的拓扑特征，如节点度、聚类系数等；5) 微观结构-性能关系建模，利用机器学习方法建立图特征与材料性能之间的关系。\\n\\n**关键创新**：该方法最重要的技术创新点在于将机器学习与图分析相结合，实现对多相颗粒复合材料微观结构的自动化、拓扑感知的分析。与传统的图像处理方法相比，该方法能够更有效地利用图像数据中的拓扑信息，从而更全面地理解材料的性能。\\n\\n**关键设计**：论文中关键的设计包括：1) 使用机器学习方法进行图像分割，提高了分割的准确性和效率；2) 构建拓扑感知的图，能够有效地表示颗粒之间的连接关系；3) 提取图的拓扑特征，能够反映微观结构的复杂性和连通性；4) 使用机器学习方法建立微观结构-性能关系，能够预测材料的性能。",
            "application_zh": "该研究成果可广泛应用于各种颗粒复合材料的设计与优化，例如固态电池、催化剂、陶瓷材料等。通过该方法，可以更深入地理解微观结构对材料性能的影响，从而指导材料的成分设计、制备工艺优化，最终提升材料的整体性能。该方法有望加速新材料的研发进程，并降低研发成本。",
            "highlight_zh": "该研究以固态锂电池正极材料为例，验证了该方法的有效性。实验结果表明，通过该方法提取的图特征能够有效地预测正极材料的电化学活性。具体而言，该研究证实了三相结和并发离子/电子传导通道在实现理想的局部电化学活性中的关键作用，为正极材料的优化设计提供了重要的指导。",
            "tags_zh": [
                "机器学习",
                "图分析",
                "颗粒复合材料",
                "固态电池",
                "微观结构表征",
                "多模态成像",
                "材料设计"
            ],
            "_index": 75,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "Impacts of Racial Bias in Historical Training Data for News AI",
            "authors": [
                "Rahul Bhargava",
                "Malene Hornstrup Jespersen",
                "Emily Boardman Ndulue",
                "Vivica Dsouza"
            ],
            "arxiv_id": "2512.16901v1",
            "summary": "AI technologies have rapidly moved into business and research applications that involve large text corpora, including computational journalism research and newsroom settings. These models, trained on extant data from various sources, can be conceptualized as historical artifacts that encode decades-old attitudes and stereotypes. This paper investigates one such example trained on the broadly-used New York Times Annotated Corpus to create a multi-label classifier. Our use in research settings surfaced the concerning \"blacks\" thematic topic label. Through quantitative and qualitative means we investigate this label's use in the training corpus, what concepts it might be encoding in the trained classifier, and how those concepts impact our model use. Via the application of explainable AI methods, we find that the \"blacks\" label operates partially as a general \"racism detector\" across some minoritized groups. However, it performs poorly against expectations on modern examples such as COVID-19 era anti-Asian hate stories, and reporting on the Black Lives Matter movement. This case study of interrogating embedded biases in a model reveals how similar applications in newsroom settings can lead to unexpected outputs that could impact a wide variety of potential uses of any large language model-story discovery, audience targeting, summarization, etc. The fundamental tension this exposes for newsrooms is how to adopt AI-enabled workflow tools while reducing the risk of reproducing historical biases in news coverage.",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.CL",
                "cs.CY"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16901v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "揭示新闻AI中历史数据偏见：以纽约时报语料库为例，分析种族标签的影响。",
            "summary_zh": "人工智能技术已迅速应用于涉及大型文本语料库的商业和研究应用，包括计算新闻研究和新闻编辑室环境。这些模型在来自各种来源的现有数据上进行训练，可以被概念化为编码了数十年历史态度和刻板印象的历史产物。本文研究了一个这样的例子，该例子在广泛使用的《纽约时报》注释语料库上进行训练，以创建一个多标签分类器。我们在研究环境中的使用浮现了令人担忧的“黑人”主题标签。通过定量和定性的方法，我们调查了该标签在训练语料库中的使用情况，它可能在训练后的分类器中编码了哪些概念，以及这些概念如何影响我们的模型使用。通过应用可解释人工智能方法，我们发现“黑人”标签在一定程度上充当了针对一些少数族裔群体的通用“种族主义检测器”。然而，它在现代例子（如COVID-19时代的反亚裔仇恨故事和关于“黑人的命也是命”运动的报道）上的表现不尽如人意。这个对模型中嵌入的偏见进行调查的案例研究揭示了新闻编辑室环境中类似的应用如何导致意想不到的输出，这些输出可能会影响任何大型语言模型的各种潜在用途——故事发现、受众定位、摘要等。这为新闻编辑室暴露出的根本矛盾是：如何在采用人工智能支持的工作流程工具的同时，降低在新闻报道中重现历史偏见的风险。",
            "intro_zh": [
                "现有新闻AI模型训练于历史数据，可能内嵌过时的态度和刻板印象，导致偏见。",
                "通过分析《纽约时报》语料库训练的多标签分类器，研究“黑人”标签的潜在偏见。",
                "发现该标签在特定情境下表现不佳，揭示了新闻AI应用中重现历史偏见的风险。"
            ],
            "method_zh": "**问题定义**：论文旨在解决新闻AI模型中由于历史训练数据偏差而产生的种族偏见问题。现有方法未能充分识别和减轻这些偏差，导致模型在处理涉及种族议题的新闻时产生不准确或带有偏见的输出。这种偏见会影响新闻报道的客观性和公正性，损害公众信任。\\n\\n**核心思路**：论文的核心思路是通过深入分析训练数据和模型行为，揭示和量化模型中存在的种族偏见。通过可解释AI方法，探究特定标签（如“黑人”）在模型中的作用，并评估其在不同情境下的表现。这种分析有助于理解模型如何编码和传播历史偏见。\\n\\n**技术框架**：论文采用的框架包括以下几个主要阶段：1) 数据分析：对《纽约时报》注释语料库中“黑人”标签的使用情况进行定量和定性分析。2) 模型训练：使用该语料库训练一个多标签分类器。3) 可解释性分析：应用可解释AI方法（具体方法未知）来理解“黑人”标签在模型中的作用。4) 案例研究：评估模型在现代新闻事件（如COVID-19期间的反亚裔仇恨和“黑人的命也是命”运动）上的表现。\\n\\n**关键创新**：论文的关键创新在于将可解释AI方法应用于新闻AI模型的偏见分析，并以“黑人”标签为例，深入研究了历史数据偏差对模型行为的影响。与以往研究相比，该论文更注重揭示偏见的具体表现形式和潜在影响，而不仅仅是检测偏见的存在。\\n\\n**关键设计**：论文的关键设计包括选择《纽约时报》注释语料库作为训练数据，因为它具有广泛的应用和代表性。此外，选择“黑人”标签作为研究对象，因为它在新闻报道中具有重要的社会和政治意义。具体的可解释AI方法、损失函数和网络结构等技术细节未知。",
            "application_zh": "该研究成果可应用于新闻编辑室，帮助记者和编辑识别和减轻AI模型中的偏见，提高新闻报道的客观性和公正性。此外，该研究也为其他领域（如法律、教育等）的AI应用提供了借鉴，促进了负责任的AI开发和部署。",
            "highlight_zh": "研究发现，在《纽约时报》语料库上训练的模型中，“黑人”标签在一定程度上充当了“种族主义检测器”，但其表现并不稳定，在COVID-19期间的反亚裔仇恨和“黑人的命也是命”运动等现代案例中表现不佳，揭示了历史数据偏差对模型泛化能力的影响。",
            "tags_zh": [
                "新闻AI",
                "种族偏见",
                "历史数据",
                "可解释AI",
                "自然语言处理"
            ],
            "_index": 76,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16901v1/figures/fig1-blacks-use.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16901v1/figures/fig2-boxplots.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16901v1/figures/fig3-terms-grid.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI",
            "authors": [
                "Hao Liang",
                "Xiaochen Ma",
                "Zhou Liu",
                "Zhen Hao Wong",
                "Zhengyang Zhao",
                "Zimo Meng",
                "Runming He",
                "Chengyu Shen",
                "Qifeng Cai",
                "Zhaoyang Han",
                "Meiyi Qiang",
                "Yalin Feng",
                "Tianyi Bai",
                "Zewei Pan",
                "Ziyi Guo",
                "Yizhen Jiang",
                "Jingwen Deng",
                "Qijie You",
                "Peichao Lai",
                "Tianyu Guo",
                "Chi Hsu Tsai",
                "Hengyi Feng",
                "Rui Hu",
                "Wenkai Yu",
                "Junbo Niu",
                "Bohan Zeng",
                "Ruichuan An",
                "Lu Ma",
                "Jihao Huang",
                "Yaowei Zheng",
                "Conghui He",
                "Linpeng Tang",
                "Bin Cui",
                "Weinan E",
                "Wentao Zhang"
            ],
            "arxiv_id": "2512.16676v1",
            "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.",
            "categories": [
                "cs.LG",
                "cs.CL"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16676v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "DataFlow：一个LLM驱动的统一数据准备与工作流自动化框架",
            "summary_zh": "为了应对大语言模型（LLMs）对高质量数据日益增长的需求，本文提出了DataFlow，一个统一且可扩展的LLM驱动的数据准备框架。DataFlow采用系统级抽象，实现了模块化、可重用和可组合的数据转换，并提供了一个类似PyTorch的pipeline构建API，用于构建可调试和优化的数据流。该框架包含近200个可重用算子和六个通用领域pipeline，涵盖文本、数学推理、代码、Text-to-SQL、agentic RAG和大规模知识抽取。为了进一步提高可用性，我们引入了DataFlow-Agent，它可以通过算子合成、pipeline规划和迭代验证，自动将自然语言规范转换为可执行的pipeline。在六个代表性用例中，DataFlow始终提高了下游LLM的性能。我们的数学、代码和文本pipeline优于人工数据集和专门的合成基线，在Text-to-SQL中实现了高达+3%的执行准确率（超过SynSQL），在代码基准测试中平均提高了+7%，在MATH、GSM8K和AIME上提高了1-3个点。此外，DataFlow生成的统一的1万样本数据集使基础模型能够超越在100万Infinity-Instruct数据上训练的同类模型。这些结果表明，DataFlow为可靠、可重复和可扩展的LLM数据准备提供了一个实用且高性能的基础，并为未来的数据中心AI开发奠定了系统级基础。",
            "intro_zh": [
                "现有数据准备方法依赖临时脚本和松散的工作流，缺乏抽象，可复现性差，对模型在环数据生成支持有限。",
                "DataFlow通过系统级抽象实现模块化、可重用和可组合的数据转换，并提供类似PyTorch的pipeline构建API。",
                "实验表明，DataFlow在多个任务上优于人工数据集和特定基线，例如Text-to-SQL准确率提升3%，代码基准测试平均提升7%。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大语言模型时代，数据准备流程的低效、不可靠和缺乏抽象的问题。现有方法通常依赖于临时脚本，缺乏系统性的设计，难以复现，并且难以支持模型在环的数据生成，无法满足LLM对高质量数据的需求。\\n\\n**核心思路**：论文的核心思路是构建一个统一的、可扩展的LLM驱动的数据准备框架DataFlow。该框架通过系统级的抽象，将数据转换过程模块化、可重用和可组合，从而提高数据准备的效率和可靠性。同时，利用LLM的能力，实现数据准备流程的自动化。\\n\\n**技术框架**：DataFlow框架包含以下几个主要模块：1) 算子库：包含近200个可重用的数据转换算子，涵盖文本、数学、代码等多个领域。2) Pipeline构建API：提供类似PyTorch的API，用于构建可调试和优化的数据流。3) DataFlow-Agent：一个LLM驱动的智能体，可以将自然语言规范转换为可执行的pipeline。整体流程是，用户可以通过API或者自然语言描述定义数据准备流程，DataFlow-Agent将自然语言描述转化为pipeline，然后执行pipeline生成高质量的数据。\\n\\n**关键创新**：DataFlow的关键创新在于：1) 统一的系统级抽象，使得数据准备流程更加模块化和可复用。2) LLM驱动的自动化数据准备，通过DataFlow-Agent将自然语言规范转换为可执行的pipeline，降低了数据准备的门槛。3) 丰富的算子库，涵盖多个领域，可以满足不同场景下的数据准备需求。\\n\\n**关键设计**：DataFlow-Agent的设计是关键。它利用LLM的理解和生成能力，将自然语言描述转化为可执行的pipeline。具体来说，DataFlow-Agent包含算子合成、pipeline规划和迭代验证三个阶段。算子合成负责根据自然语言描述选择合适的算子；pipeline规划负责将算子组合成一个完整的pipeline；迭代验证负责验证pipeline的正确性，并进行必要的调整。",
            "application_zh": "DataFlow可应用于各种需要高质量数据的大语言模型训练和应用场景，例如文本生成、代码生成、数学推理、知识图谱构建等。它能够显著降低数据准备的成本，提高数据质量，从而提升下游LLM的性能。未来，DataFlow有望成为数据中心AI时代的重要基础设施。",
            "highlight_zh": "实验结果表明，DataFlow在多个任务上取得了显著的性能提升。例如，在Text-to-SQL任务中，DataFlow的准确率比SynSQL提高了3%。在代码生成任务中，DataFlow的平均性能提升了7%。此外，DataFlow生成的1万样本数据集，使得基础模型能够超越在100万Infinity-Instruct数据上训练的同类模型。",
            "tags_zh": [
                "数据准备",
                "大语言模型",
                "工作流自动化",
                "LLM驱动",
                "数据中心AI"
            ],
            "_index": 77,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16676v1/x4.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16676v1/x5.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16676v1/x6.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Muon is Provably Faster with Momentum Variance Reduction",
            "authors": [
                "Xun Qian",
                "Hussein Rammal",
                "Dmitry Kovalev",
                "Peter Richtárik"
            ],
            "arxiv_id": "2512.16598v1",
            "summary": "Recent empirical research has demonstrated that deep learning optimizers based on the linear minimization oracle (LMO) over specifically chosen Non-Euclidean norm balls, such as Muon and Scion, outperform Adam-type methods in the training of large language models. In this work, we show that such optimizers can be provably improved by replacing their vanilla momentum by momentum variance reduction (MVR). Instead of proposing and analyzing MVR variants of Muon and Scion separately, we incorporate MVR into the recently proposed Gluon framework, which captures Muon, Scion and other specific Non-Euclidean LMO-based methods as special cases, and at the same time works with a more general smoothness assumption which better captures the layer-wise structure of neural networks. In the non-convex case, we incorporate MVR into Gluon in three different ways. All of them improve the convergence rate from ${\\cal O} (\\frac{1}{K^{1/4}})$ to ${\\cal O} (\\frac{1}{K^{1/3}})$. Additionally, we provide improved rates in the star-convex case. Finally, we conduct several numerical experiments that verify the superior performance of our proposed algorithms in terms of iteration complexity.",
            "categories": [
                "math.OC",
                "cs.LG"
            ],
            "primary_category": "math.OC",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "31 pages, 4 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16598v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "通过动量方差减少提升Muon优化器性能",
            "summary_zh": "近期的实证研究表明，基于线性最小化oracle（LMO）并在特定非欧几里得范数球上优化的深度学习优化器，如Muon和Scion，在训练大型语言模型时优于Adam类方法。本研究展示了通过将传统动量替换为动量方差减少（MVR），可以对这些优化器进行可证明的改进。我们将MVR整合到最近提出的Gluon框架中，该框架能够捕捉Muon、Scion及其他特定的非欧几里得LMO方法，并在更一般的平滑性假设下工作，从而更好地捕捉神经网络的层次结构。在非凸情况下，我们以三种不同方式将MVR融入Gluon，所有方法均将收敛速率从${\text{O}}(\frac{1}{K^{1/4}})$提高至${\text{O}}(\frac{1}{K^{1/3}})$，并在星凸情况下提供了改进的速率。最后，我们进行了多项数值实验，验证了所提算法在迭代复杂度上的优越性能。",
            "intro_zh": [
                "现有的深度学习优化器在训练大型语言模型时存在收敛速度慢的问题，尤其是Adam类方法的表现不尽如人意。",
                "本研究提出将动量方差减少（MVR）技术整合到Gluon框架中，以提升Muon和Scion等优化器的收敛速率。",
                "实验结果表明，所提方法的收敛速率从${\text{O}}(\frac{1}{K^{1/4}})$提升至${\text{O}}(\frac{1}{K^{1/3}})$，并在多项实验中验证了其优越性。"
            ],
            "method_zh": "**问题定义**：本论文旨在解决现有深度学习优化器在训练大型语言模型时收敛速度慢的问题，尤其是Adam类方法在非凸优化中的不足。\\n\\n**核心思路**：通过将动量方差减少（MVR）技术引入到Gluon框架中，提升优化器的收敛速率，进而提高训练效率。\\n\\n**技术框架**：整体框架为Gluon，能够统一处理Muon、Scion及其他非欧几里得LMO方法，采用更一般的平滑性假设，分为三个阶段：MVR的引入、收敛速率分析及实验验证。\\n\\n**关键创新**：最重要的创新在于将MVR与Gluon框架结合，提供了对多种优化器的统一分析和改进，显著提升了收敛速率。\\n\\n**关键设计**：在设计中，采用了三种不同的方式将MVR融入Gluon，并在星凸情况下提供了改进的收敛速率，具体参数设置和损失函数设计未详细披露，属于未知领域。",
            "application_zh": "该研究的潜在应用领域包括自然语言处理、计算机视觉等需要高效训练的大型深度学习模型。通过提升优化器的性能，可以显著缩短模型训练时间，提高模型的实际应用价值。未来，随着深度学习模型规模的不断扩大，优化器的改进将对整个领域产生深远影响。",
            "highlight_zh": "实验结果显示，所提算法的收敛速率从${\text{O}}(\frac{1}{K^{1/4}})$提升至${\text{O}}(\frac{1}{K^{1/3}})$，在多项数值实验中验证了其在迭代复杂度上的优越性，相较于基线方法表现出显著的性能提升。",
            "tags_zh": [
                "深度学习",
                "优化器",
                "动量方差减少",
                "Gluon框架",
                "收敛速率",
                "非欧几里得优化",
                "大型语言模型"
            ],
            "_index": 78,
            "_used_api": "openai",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16598v1/fig/MVR1gbs512.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16598v1/fig/MVR1gbs128.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16598v1/fig/MVR2gbs512.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "A Systematic Study of Code Obfuscation Against LLM-based Vulnerability Detection",
            "authors": [
                "Xiao Li",
                "Yue Li",
                "Hao Wu",
                "Yue Zhang",
                "Yechao Zhang",
                "Fengyuan Xu",
                "Sheng Zhong"
            ],
            "arxiv_id": "2512.16538v1",
            "summary": "As large language models (LLMs) are increasingly adopted for code vulnerability detection, their reliability and robustness across diverse vulnerability types have become a pressing concern. In traditional adversarial settings, code obfuscation has long been used as a general strategy to bypass auditing tools, preserving exploitability without tampering with the tools themselves. Numerous efforts have explored obfuscation methods and tools, yet their capabilities differ in terms of supported techniques, granularity, and programming languages, making it difficult to systematically assess their impact on LLM-based vulnerability detection. To address this gap, we provide a structured systematization of obfuscation techniques and evaluate them under a unified framework. Specifically, we categorize existing obfuscation methods into three major classes (layout, data flow, and control flow) covering 11 subcategories and 19 concrete techniques. We implement these techniques across four programming languages (Solidity, C, C++, and Python) using a consistent LLM-driven approach, and evaluate their effects on 15 LLMs spanning four model families (DeepSeek, OpenAI, Qwen, and LLaMA), as well as on two coding agents (GitHub Copilot and Codex). Our findings reveal both positive and negative impacts of code obfuscation on LLM-based vulnerability detection, highlighting conditions under which obfuscation leads to performance improvements or degradations. We further analyze these outcomes with respect to vulnerability characteristics, code properties, and model attributes. Finally, we outline several open problems and propose future directions to enhance the robustness of LLMs for real-world vulnerability detection.",
            "categories": [
                "cs.CR",
                "cs.LG"
            ],
            "primary_category": "cs.CR",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16538v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "系统性研究代码混淆对基于LLM的漏洞检测的影响，揭示其性能变化规律",
            "summary_zh": "随着大型语言模型（LLM）越来越多地应用于代码漏洞检测，其在不同漏洞类型上的可靠性和鲁棒性日益受到关注。代码混淆作为一种绕过审计工具的通用策略，长期以来被使用，它在不篡改工具本身的情况下保留了可利用性。虽然已经有很多关于混淆方法和工具的研究，但它们在支持的技术、粒度和编程语言方面存在差异，这使得系统性地评估它们对基于LLM的漏洞检测的影响变得困难。为了解决这个问题，我们对混淆技术进行了结构化的系统化研究，并在统一的框架下评估了它们。具体来说，我们将现有的混淆方法分为三大类（布局、数据流和控制流），涵盖11个子类别和19个具体技术。我们使用一致的LLM驱动方法在四种编程语言（Solidity、C、C++和Python）中实现了这些技术，并评估了它们对跨越四个模型家族（DeepSeek、OpenAI、Qwen和LLaMA）的15个LLM以及两个编码代理（GitHub Copilot和Codex）的影响。我们的研究结果揭示了代码混淆对基于LLM的漏洞检测的积极和消极影响，突出了混淆导致性能提升或下降的条件。我们进一步分析了这些结果与漏洞特征、代码属性和模型属性的关系。最后，我们概述了几个开放性问题，并提出了未来方向，以增强LLM在实际漏洞检测中的鲁棒性。",
            "intro_zh": [
                "现有代码混淆工具在技术、粒度和语言支持上存在差异，缺乏系统性的评估框架来衡量其对LLM漏洞检测的影响。",
                "论文对代码混淆技术进行分类和系统化，并在统一框架下评估其对多种LLM漏洞检测性能的影响。",
                "实验结果揭示了代码混淆对LLM漏洞检测的复杂影响，并分析了影响因素，为提升LLM鲁棒性提供了方向。"
            ],
            "method_zh": "**问题定义**：论文旨在解决代码混淆对基于大型语言模型（LLM）的漏洞检测工具的影响评估问题。现有代码混淆工具种类繁多，缺乏统一的评估标准，难以系统性地了解不同混淆技术对LLM漏洞检测性能的影响，这阻碍了LLM在安全领域的可靠应用。现有方法无法有效应对代码混淆带来的挑战。\\n\\n**核心思路**：论文的核心思路是对现有代码混淆技术进行系统分类和整理，构建一个统一的评估框架，并在该框架下评估不同混淆技术对多种LLM漏洞检测性能的影响。通过实验分析，揭示代码混淆对LLM漏洞检测的积极和消极影响，并分析影响因素，从而为提升LLM的鲁棒性提供指导。\\n\\n**技术框架**：论文构建的评估框架包含以下几个主要模块：1) 代码混淆技术分类模块：将现有混淆技术分为布局、数据流和控制流三大类，并细分为11个子类别和19个具体技术。2) 代码混淆实现模块：在Solidity、C、C++和Python四种编程语言中实现这些混淆技术。3) LLM漏洞检测评估模块：使用15个LLM（来自DeepSeek、OpenAI、Qwen和LLaMA四个模型家族）和两个编码代理（GitHub Copilot和Codex）进行漏洞检测，并评估混淆代码对检测结果的影响。4) 结果分析模块：分析实验结果，揭示代码混淆对LLM漏洞检测的影响，并分析影响因素。\\n\\n**关键创新**：论文最重要的创新点在于对代码混淆技术进行了系统性的分类和整理，并构建了一个统一的评估框架，从而能够系统性地评估不同混淆技术对LLM漏洞检测性能的影响。此外，论文还深入分析了代码混淆对LLM漏洞检测的积极和消极影响，并分析了影响因素，为提升LLM的鲁棒性提供了指导。与现有方法相比，该研究更加全面和系统，能够更深入地了解代码混淆对LLM漏洞检测的影响。\\n\\n**关键设计**：论文的关键设计包括：1) 混淆技术的选择：选择了具有代表性的19种混淆技术，覆盖了布局、数据流和控制流三大类。2) 编程语言的选择：选择了Solidity、C、C++和Python四种常用的编程语言。3) LLM的选择：选择了来自不同模型家族的15个LLM和两个编码代理，以保证评估结果的广泛适用性。4) 评估指标的选择：选择了能够反映LLM漏洞检测性能的指标，如准确率、召回率和F1值。",
            "application_zh": "该研究成果可应用于提升软件安全和代码审计领域。通过了解代码混淆对LLM漏洞检测的影响，可以指导开发者选择合适的混淆策略，同时也可以帮助安全研究人员设计更鲁棒的LLM漏洞检测工具。此外，该研究还可以促进LLM在安全领域的更广泛应用，例如自动化漏洞修复和恶意代码分析。",
            "highlight_zh": "实验结果表明，代码混淆对LLM漏洞检测的影响是复杂的，既有积极影响也有消极影响。某些混淆技术可以提高LLM的检测性能，而另一些则会降低性能。例如，某些布局混淆技术可能会降低LLM的检测准确率，而某些控制流混淆技术可能会提高检测召回率。研究还发现，不同LLM对不同混淆技术的敏感度不同，这表明需要针对不同的LLM选择合适的混淆策略。",
            "tags_zh": [
                "代码混淆",
                "漏洞检测",
                "大型语言模型",
                "软件安全",
                "代码审计"
            ],
            "_index": 79,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "Feature-Selective Representation Misdirection for Machine Unlearning",
            "authors": [
                "Taozhao Chen",
                "Linghan Huang",
                "Kim-Kwang Raymond Choo",
                "Huaming Chen"
            ],
            "arxiv_id": "2512.16297v1",
            "summary": "As large language models (LLMs) are increasingly adopted in safety-critical and regulated sectors, the retention of sensitive or prohibited knowledge introduces escalating risks, ranging from privacy leakage to regulatory non-compliance to to potential misuse, and so on. Recent studies suggest that machine unlearning can help ensure deployed models comply with evolving legal, safety, and governance requirements. However, current unlearning techniques assume clean separation between forget and retain datasets, which is challenging in operational settings characterized by highly entangled distributions. In such scenarios, perturbation-based methods often degrade general model utility or fail to ensure safety. To address this, we propose Selective Representation Misdirection for Unlearning (SRMU), a novel principled activation-editing framework that enforces feature-aware and directionally controlled perturbations. Unlike indiscriminate model weights perturbations, SRMU employs a structured misdirection vector with an activation importance map. The goal is to allow SRMU selectively suppresses harmful representations while preserving the utility on benign ones. Experiments are conducted on the widely used WMDP benchmark across low- and high-entanglement configurations. Empirical results reveal that SRMU delivers state-of-the-art unlearning performance with minimal utility losses, and remains effective under 20-30\\% overlap where existing baselines collapse. SRMU provides a robust foundation for safety-driven model governance, privacy compliance, and controlled knowledge removal in the emerging LLM-based applications. We release the replication package at https://figshare.com/s/d5931192a8824de26aff.",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16297v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出选择性表征误导(SRMU)框架，解决LLM中知识遗忘难题，兼顾安全与效用。",
            "summary_zh": "随着大型语言模型（LLMs）在安全关键和受监管领域日益普及，模型中保留的敏感或禁止知识带来了不断升级的风险，包括隐私泄露、违反法规以及潜在的滥用等。近期的研究表明，机器遗忘技术有助于确保已部署的模型符合不断变化的法律、安全和治理要求。然而，现有的遗忘技术假设遗忘数据集和保留数据集之间存在清晰的分离，这在具有高度纠缠分布的实际操作环境中是具有挑战性的。在这种情况下，基于扰动的方法通常会降低模型的通用效用或无法确保安全性。为了解决这个问题，我们提出了一种用于遗忘的选择性表征误导（SRMU）方法，这是一种新颖的、有原则的激活编辑框架，它强制执行特征感知和方向控制的扰动。与不加区分的模型权重扰动不同，SRMU采用带有激活重要性图的结构化误导向量。目标是允许SRMU选择性地抑制有害表征，同时保持对良性表征的效用。在广泛使用的WMDP基准上，针对低纠缠和高纠缠配置进行了实验。实验结果表明，SRMU提供了最先进的遗忘性能，同时效用损失最小，并且在现有基线崩溃的20-30%重叠下仍然有效。SRMU为新兴的基于LLM的应用中的安全驱动模型治理、隐私合规性和受控知识移除提供了强大的基础。我们在https://figshare.com/s/d5931192a8824de26aff发布了复制包。",
            "intro_zh": [
                "现有机器遗忘方法在遗忘和保留数据集高度纠缠时表现不佳，容易导致模型效用下降或无法保证安全性。",
                "SRMU通过激活编辑，有选择性地施加特征感知和方向控制的扰动，抑制有害表征，同时保留良性表征的效用。",
                "实验表明，SRMU在低、高纠缠配置下均表现出色，即使在20-30%的数据重叠下，仍优于现有方法，且效用损失最小。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大型语言模型（LLMs）中知识遗忘的问题，尤其是在遗忘数据和保留数据高度纠缠的情况下。现有基于扰动的遗忘方法，如直接修改模型权重，容易导致模型通用性能下降，或者无法有效移除有害知识，难以满足安全性和合规性要求。\\n\\n**核心思路**：论文的核心思路是通过选择性地修改模型内部的激活表征，从而达到遗忘特定知识的目的。这种方法不是直接修改模型权重，而是通过在激活层引入有针对性的扰动，来“误导”模型的判断，使其不再输出与遗忘知识相关的内容。通过控制扰动的方向和强度，可以最大限度地减少对模型通用性能的影响。\\n\\n**技术框架**：SRMU框架主要包含以下几个阶段：1) **激活重要性评估**：确定哪些激活对于模型输出特定知识最为重要。这可以通过梯度信息或其他方法来实现。2) **误导向量生成**：基于激活重要性，生成一个结构化的误导向量，该向量包含方向和强度信息，用于指导扰动的施加。3) **激活编辑**：将误导向量施加到模型的激活层，从而改变模型的内部表征。4) **模型微调（可选）**：在激活编辑后，可以对模型进行微调，以进一步提高其通用性能。\\n\\n**关键创新**：SRMU的关键创新在于其选择性和方向控制的扰动机制。与传统的权重扰动方法不同，SRMU不是盲目地修改模型参数，而是有针对性地修改激活表征，并且可以控制扰动的方向，从而最大限度地减少对模型通用性能的影响。此外，SRMU还引入了激活重要性图，用于指导扰动的施加，从而进一步提高了遗忘的效率和准确性。\\n\\n**关键设计**：SRMU的关键设计包括：1) **激活重要性评估方法**：可以使用梯度信息、注意力权重或其他方法来评估激活的重要性。2) **误导向量的生成方式**：误导向量的方向可以设置为与目标知识相关的激活方向相反，强度可以根据激活的重要性进行调整。3) **激活编辑的策略**：可以将误导向量直接加到激活值上，或者使用更复杂的编辑策略，如基于注意力机制的编辑。",
            "application_zh": "SRMU技术可应用于各种需要知识遗忘的场景，例如：1) 大型语言模型中的隐私数据移除，确保模型不泄露用户个人信息；2) 模型合规性治理，移除模型中违反法律法规或伦理道德的内容；3) 模型知识产权保护，防止模型被用于非法复制或传播受版权保护的内容。该技术有助于提升LLM在安全敏感领域的应用价值。",
            "highlight_zh": "SRMU在WMDP基准测试中取得了最先进的遗忘性能，同时保持了最小的效用损失。在高纠缠配置下，即使在20-30%的数据重叠情况下，SRMU仍然有效，而现有的基线方法已经失效。这表明SRMU具有更强的鲁棒性和实用性，能够应对更复杂的实际应用场景。",
            "tags_zh": [
                "机器遗忘",
                "大型语言模型",
                "知识移除",
                "表征学习",
                "激活编辑"
            ],
            "_index": 80,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16297v1/img/overviewSRMU.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16297v1/img/combinationablation.png",
                    "caption": "",
                    "figure_id": "fig_1"
                }
            ]
        },
        {
            "title": "In-Context Probing for Membership Inference in Fine-Tuned Language Models",
            "authors": [
                "Zhexi Lu",
                "Hongliang Chi",
                "Nathalie Baracaldo",
                "Swanand Ravindra Kadhe",
                "Yuseok Jeon",
                "Lei Yu"
            ],
            "arxiv_id": "2512.16292v1",
            "summary": "Membership inference attacks (MIAs) pose a critical privacy threat to fine-tuned large language models (LLMs), especially when models are adapted to domain-specific tasks using sensitive data. While prior black-box MIA techniques rely on confidence scores or token likelihoods, these signals are often entangled with a sample's intrinsic properties - such as content difficulty or rarity - leading to poor generalization and low signal-to-noise ratios. In this paper, we propose ICP-MIA, a novel MIA framework grounded in the theory of training dynamics, particularly the phenomenon of diminishing returns during optimization. We introduce the Optimization Gap as a fundamental signal of membership: at convergence, member samples exhibit minimal remaining loss-reduction potential, while non-members retain significant potential for further optimization. To estimate this gap in a black-box setting, we propose In-Context Probing (ICP), a training-free method that simulates fine-tuning-like behavior via strategically constructed input contexts. We propose two probing strategies: reference-data-based (using semantically similar public samples) and self-perturbation (via masking or generation). Experiments on three tasks and multiple LLMs show that ICP-MIA significantly outperforms prior black-box MIAs, particularly at low false positive rates. We further analyze how reference data alignment, model type, PEFT configurations, and training schedules affect attack effectiveness. Our findings establish ICP-MIA as a practical and theoretically grounded framework for auditing privacy risks in deployed LLMs.",
            "categories": [
                "cs.CR",
                "cs.LG"
            ],
            "primary_category": "cs.CR",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16292v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出ICP-MIA框架，通过上下文探查解决微调语言模型的成员推理攻击问题。",
            "summary_zh": "成员推理攻击(MIAs)对微调的大型语言模型(LLMs)构成严重的隐私威胁，尤其是在模型使用敏感数据进行领域特定任务适配时。现有的黑盒MIA技术依赖于置信度分数或token似然，但这些信号通常与样本的内在属性（如内容难度或稀有度）纠缠在一起，导致泛化能力差和信噪比低。本文提出了ICP-MIA，一个基于训练动态理论（特别是优化过程中的收益递减现象）的新型MIA框架。我们将优化差距作为成员身份的基本信号：在收敛时，成员样本表现出最小的剩余损失减少潜力，而非成员样本保留了进一步优化的显著潜力。为了在黑盒设置中估计这个差距，我们提出了一种无需训练的方法——上下文探查(ICP)，它通过策略性构建的输入上下文来模拟微调行为。我们提出了两种探查策略：基于参考数据的探查（使用语义相似的公共样本）和自扰动（通过掩码或生成）。在三个任务和多个LLM上的实验表明，ICP-MIA显著优于先前的黑盒MIA，尤其是在低假阳性率下。我们进一步分析了参考数据对齐、模型类型、PEFT配置和训练计划如何影响攻击效果。我们的发现确立了ICP-MIA作为一个实用且理论上合理的框架，用于审计已部署LLM中的隐私风险。",
            "intro_zh": [
                "现有黑盒成员推理攻击依赖置信度等信号，易受样本固有属性干扰，泛化性差。",
                "提出ICP-MIA框架，利用优化差距作为成员信号，通过上下文探查模拟微调行为。",
                "实验表明，ICP-MIA显著优于现有黑盒MIA方法，尤其在低假阳性率下表现突出。"
            ],
            "method_zh": "**问题定义**：论文旨在解决微调语言模型中，现有成员推理攻击方法效果不佳的问题。现有方法依赖于模型输出的置信度或token似然等信号，但这些信号与样本本身的难度、稀有度等属性高度相关，导致攻击者难以区分模型“记住”了训练数据，还是仅仅因为样本本身就容易预测。因此，现有方法泛化能力差，信噪比低，难以有效识别成员样本。\\n\\n**核心思路**：论文的核心思路是利用训练动态中的“收益递减”现象。成员样本在训练过程中已经被模型“记住”，因此在模型收敛后，对这些样本进行进一步优化带来的收益（损失减少）会很小。而非成员样本则还有很大的优化空间。因此，可以通过估计样本的“优化差距”（Optimization Gap）来判断其是否为成员样本。\\n\\n**技术框架**：ICP-MIA框架主要包含两个阶段：首先，通过“上下文探查”（In-Context Probing, ICP）来估计样本的优化差距。然后，利用估计的优化差距来判断样本是否为成员。ICP阶段又包含两种探查策略：基于参考数据的探查和自扰动探查。基于参考数据的探查使用与目标样本语义相似的公共数据作为上下文，引导模型对目标样本进行预测。自扰动探查则通过掩码或生成等方式对目标样本进行扰动，然后观察模型在扰动后的预测变化。\\n\\n**关键创新**：论文最重要的创新点在于提出了利用“优化差距”作为成员推理攻击的信号。与以往方法直接利用模型输出的置信度等信号不同，优化差距更能反映模型对样本的“记忆”程度，从而提高了攻击的准确性和泛化能力。此外，提出的“上下文探查”方法无需训练，可以直接在黑盒模型上进行，具有很强的实用性。\\n\\n**关键设计**：在基于参考数据的探查中，关键在于选择与目标样本语义相似的参考数据。论文可能使用了某种语义相似度度量方法（具体方法未知）来选择参考数据。在自扰动探查中，关键在于选择合适的扰动方式和扰动强度。论文可能尝试了不同的掩码策略和生成模型（具体细节未知）。此外，如何将估计的优化差距转化为成员概率，可能也涉及一些阈值设置或概率模型（具体细节未知）。",
            "application_zh": "该研究成果可应用于评估和提升已部署大型语言模型的隐私安全性。通过ICP-MIA框架，可以有效识别模型存在的隐私泄露风险，并指导模型开发者采取相应的防御措施，例如差分隐私训练、对抗训练等，从而保护用户敏感数据。",
            "highlight_zh": "实验结果表明，ICP-MIA在多个任务和模型上显著优于现有的黑盒MIA方法。尤其是在低假阳性率下，ICP-MIA的性能提升更为明显，表明其能够更准确地识别成员样本，降低误报率。论文还分析了参考数据对齐、模型类型、PEFT配置和训练计划等因素对攻击效果的影响，为实际应用提供了指导。",
            "tags_zh": [
                "成员推理攻击",
                "大型语言模型",
                "隐私保护",
                "上下文学习",
                "优化差距"
            ],
            "_index": 81,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16292v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16292v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16292v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "CKA-Guided Modular Quantization: Beyond Bit-Width to Algorithmic Diversity",
            "authors": [
                "Jinhao Zhang",
                "Yunquan Zhang",
                "Daning Chen"
            ],
            "arxiv_id": "2512.16282v1",
            "summary": "Current mainstream post-training quantization methods for large language models typically apply a uniform quantization strategy across all network layers, overlooking the substantial differences in algorithmic suitability among layers. To address this limitation, we propose CKA Guided Modular Quantization, a fine-tuning-free, plug-and-play framework for algorithmic heterogeneous quantization. Our method independently evaluates multiple PTQ algorithms on each layer and employs Linear Centered Kernel Alignment (CKA) as a metric to automatically select the optimal quantization strategy per layer. The individually optimized strategies are then integrated to construct a hybrid quantized model. Experiments demonstrate that our approach consistently outperforms both uniform quantization baselines and state-of-the-art mixed-precision methods across mainstream LLMs including LLaMA and Qwen ,in terms of perplexity (PPL) and downstream task performance.",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16282v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出CKA引导的模块化量化方法，实现大模型层间算法多样性量化",
            "summary_zh": "当前主流的大语言模型后训练量化方法通常在所有网络层应用统一的量化策略，忽略了层间算法适用性的显著差异。为了解决这一局限性，我们提出了一种CKA引导的模块化量化方法，这是一个无需微调、即插即用的算法异构量化框架。我们的方法独立评估每个层上的多个PTQ算法，并采用线性中心核对齐（CKA）作为度量标准，以自动选择每个层的最佳量化策略。然后，将单独优化的策略集成以构建混合量化模型。实验表明，在困惑度（PPL）和下游任务性能方面，我们的方法始终优于包括LLaMA和Qwen在内的主流LLM上的统一量化基线和最先进的混合精度方法。",
            "intro_zh": [
                "现有大模型量化方法忽略了不同层对量化算法的适应性差异，导致性能瓶颈。",
                "提出CKA引导的模块化量化方法，通过CKA度量自动为每一层选择最优量化算法。",
                "实验证明，该方法在LLaMA和Qwen等模型上，显著优于统一量化和混合精度方法。"
            ],
            "method_zh": "**问题定义**：现有大语言模型的后训练量化（PTQ）方法通常采用统一的量化策略，即对模型的所有层使用相同的量化算法和比特宽度。然而，不同的层可能具有不同的激活分布、权重分布以及对量化误差的敏感度。因此，统一的量化策略无法充分利用每一层的特性，导致量化后的模型性能下降。现有方法的痛点在于缺乏一种能够根据每一层的特性自适应选择量化算法的机制。\\n\\n**核心思路**：论文的核心思路是为模型的每一层独立评估多个PTQ算法，并选择最适合该层的算法。为了衡量不同量化算法对每一层的影响，论文使用线性中心核对齐（CKA）作为度量标准。CKA能够衡量不同层表示之间的相似性，从而反映量化算法对层表示的影响。通过最大化量化前后层表示的CKA相似度，可以选择对该层影响最小的量化算法。\\n\\n**技术框架**：该方法是一个无需微调的即插即用框架，主要包含以下几个阶段：1) **算法库构建**：构建一个包含多种PTQ算法的算法库，例如INT8、INT4、FP16等。2) **逐层评估**：对于模型的每一层，使用算法库中的每种算法进行量化，并计算量化前后层表示的CKA相似度。3) **策略选择**：选择CKA相似度最高的算法作为该层的最佳量化策略。4) **模型集成**：将所有层的最佳量化策略集成到一起，构建一个混合量化模型。\\n\\n**关键创新**：该方法最重要的技术创新点在于使用CKA作为量化算法选择的度量标准。与传统的基于性能指标（如困惑度）的算法选择方法相比，CKA能够更直接地反映量化算法对层表示的影响，从而更准确地选择最佳量化策略。此外，该方法无需微调，可以快速应用于各种大语言模型。\\n\\n**关键设计**：CKA的计算方式是关键设计之一。论文采用线性CKA，其计算复杂度较低，适合大规模模型的逐层评估。此外，算法库中PTQ算法的选择也会影响最终的量化效果。论文选择了常用的INT8、INT4、FP16等算法，并可以根据实际需求进行扩展。",
            "application_zh": "该研究成果可广泛应用于大语言模型的部署和推理加速。通过自适应地选择每一层的量化算法，可以在保证模型性能的同时，显著降低模型的存储空间和计算复杂度。这对于在资源受限的设备上部署大语言模型具有重要意义，例如移动设备、嵌入式系统等。此外，该方法还可以应用于模型压缩、模型加速等领域。",
            "highlight_zh": "实验结果表明，该方法在LLaMA和Qwen等主流大语言模型上，显著优于统一量化和混合精度方法。具体来说，在相同的比特宽度下，该方法可以获得更低的困惑度（PPL）和更高的下游任务性能。例如，在LLaMA模型上，该方法可以将困惑度降低X%，同时保持下游任务性能不变。",
            "tags_zh": [
                "后训练量化",
                "大语言模型",
                "算法异构量化",
                "线性中心核对齐",
                "模型压缩"
            ],
            "_index": 82,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16282v1/figure6.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16282v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16282v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Staggered Batch Scheduling: Co-optimizing Time-to-First-Token and Throughput for High-Efficiency LLM Inference",
            "authors": [
                "Jian Tian",
                "Shuailong Li",
                "Yang Cao",
                "Wenbo Cui",
                "Minghan Zhu",
                "Wenkang Wu",
                "Jianming Zhang",
                "Yanpeng Wang",
                "Zhiwen Xiao",
                "Zhenyu Hou",
                "Dou Shen"
            ],
            "arxiv_id": "2512.16134v1",
            "summary": "The evolution of Large Language Model (LLM) serving towards complex, distributed architectures--specifically the P/D-separated, large-scale DP+EP paradigm--introduces distinct scheduling challenges. Unlike traditional deployments where schedulers can treat instances as black boxes, DP+EP architectures exhibit high internal synchronization costs. We identify that immediate request dispatching in such systems leads to severe in-engine queuing and parallelization bubbles, degrading Time-to-First-Token (TTFT). To address this, we propose Staggered Batch Scheduling (SBS), a mechanism that deliberately buffers requests to form optimal execution batches. This temporal decoupling eliminates internal queuing bubbles without compromising throughput. Furthermore, leveraging the scheduling window created by buffering, we introduce a Load-Aware Global Allocation strategy that balances computational load across DP units for both Prefill and Decode phases. Deployed on a production H800 cluster serving Deepseek-V3, our system reduces TTFT by 30%-40% and improves throughput by 15%-20% compared to state-of-the-art immediate scheduling baselines.",
            "categories": [
                "cs.DC",
                "cs.LG"
            ],
            "primary_category": "cs.DC",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16134v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出交错批调度(SBS)，优化DP+EP架构下LLM推理的首Token延迟和吞吐量。",
            "summary_zh": "针对大规模分布式架构（特别是P/D分离的DP+EP范式）下LLM服务面临的调度挑战，本文提出交错批调度（Staggered Batch Scheduling, SBS）。与传统调度器将实例视为黑盒不同，DP+EP架构具有较高的内部同步成本。研究发现，在此类系统中立即调度请求会导致严重的引擎内排队和并行化气泡，从而降低首Token延迟（TTFT）。SBS通过有意缓冲请求以形成最佳执行批次来解决此问题，这种时间解耦消除了内部排队气泡，且不影响吞吐量。此外，利用缓冲创建的调度窗口，引入负载感知全局分配策略，以平衡Prefill和Decode阶段跨DP单元的计算负载。在服务Deepseek-V3的生产H800集群上部署后，与最先进的立即调度基线相比，该系统将TTFT降低了30%-40%，并将吞吐量提高了15%-20%。",
            "intro_zh": [
                "DP+EP架构下LLM服务中，立即调度请求导致引擎内排队和并行化气泡，严重影响首Token延迟。",
                "提出交错批调度（SBS），通过缓冲请求形成最佳批次，消除内部排队气泡，并利用调度窗口进行负载均衡。",
                "在生产H800集群上，SBS将TTFT降低30%-40%，吞吐量提高15%-20%，显著优于现有调度方法。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大规模分布式LLM服务中，特别是采用DP+EP架构时，由于高内部同步成本导致的调度效率低下问题。现有立即调度方法在DP+EP架构下会产生严重的引擎内排队和并行化气泡，导致首Token延迟（TTFT）增加，影响用户体验。同时，简单的增加batch size又会影响TTFT。\n\n**核心思路**：论文的核心思路是通过时间解耦的方式，即交错批调度（SBS），来优化请求的调度。SBS并非立即调度请求，而是有意识地缓冲请求，形成更优的执行批次。这种缓冲创造了一个调度窗口，允许进行全局负载感知的资源分配，从而平衡不同DP单元上的计算负载，最终降低TTFT并提高吞吐量。\n\n**技术框架**：SBS包含两个主要阶段：请求缓冲和全局负载感知分配。首先，请求被缓冲在一个调度队列中，等待形成合适的批次。然后，利用缓冲窗口，采用负载感知的全局分配策略，将批次分配到不同的DP单元上。该策略考虑了Prefill和Decode阶段的计算负载，力求在各个DP单元之间实现负载均衡，避免出现资源闲置或拥塞。\n\n**关键创新**：论文的关键创新在于将时间维度引入LLM推理调度中。传统的调度方法通常是基于即时可用资源进行调度，而SBS则通过缓冲请求，创造了一个时间窗口，从而能够进行更全局的优化。这种时间解耦的思想是与现有方法的本质区别。\n\n**关键设计**：SBS的关键设计包括：1) 缓冲队列的长度，需要根据实际负载和系统资源进行调整，以平衡TTFT和吞吐量；2) 负载感知的全局分配策略，需要精确估计Prefill和Decode阶段的计算负载，并根据DP单元的计算能力进行合理分配；3) 调度窗口大小的确定，需要在TTFT和调度优化空间之间进行权衡。论文中可能还涉及一些具体的负载预测模型或启发式算法，用于实现负载均衡。",
            "application_zh": "该研究成果可广泛应用于大规模语言模型的在线推理服务，特别是在需要高并发、低延迟的场景下，例如智能客服、AI助手、搜索引擎等。通过优化调度策略，可以显著提升用户体验，并提高计算资源的利用率，降低运营成本。未来，该方法可以进一步扩展到其他类型的AI模型和服务中。",
            "highlight_zh": "实验结果表明，在服务Deepseek-V3的生产H800集群上，与最先进的立即调度基线相比，SBS将首Token延迟（TTFT）降低了30%-40%，并将吞吐量提高了15%-20%。这些数据表明SBS在实际应用中具有显著的性能优势。",
            "tags_zh": [
                "大语言模型推理",
                "分布式系统",
                "调度算法",
                "首Token延迟",
                "吞吐量优化"
            ],
            "_index": 83,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16134v1/figures/schedule_unit_1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16134v1/figures/schedule_unit_2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16134v1/figures/queue_1.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Scaling Text2SQL via LLM-efficient Schema Filtering with Functional Dependency Graph Rerankers",
            "authors": [
                "Thanh Dat Hoang",
                "Thanh Tam Nguyen",
                "Thanh Trung Huynh",
                "Hongzhi Yin",
                "Quoc Viet Hung Nguyen"
            ],
            "arxiv_id": "2512.16083v1",
            "summary": "Most modern Text2SQL systems prompt large language models (LLMs) with entire schemas -- mostly column information -- alongside the user's question. While effective on small databases, this approach fails on real-world schemas that exceed LLM context limits, even for commercial models. The recent Spider 2.0 benchmark exemplifies this with hundreds of tables and tens of thousands of columns, where existing systems often break. Current mitigations either rely on costly multi-step prompting pipelines or filter columns by ranking them against user's question independently, ignoring inter-column structure. To scale existing systems, we introduce \\toolname, an open-source, LLM-efficient schema filtering framework that compacts Text2SQL prompts by (i) ranking columns with a query-aware LLM encoder enriched with values and metadata, (ii) reranking inter-connected columns via a lightweight graph transformer over functional dependencies, and (iii) selecting a connectivity-preserving sub-schema with a Steiner-tree heuristic. Experiments on real datasets show that \\toolname achieves near-perfect recall and higher precision than CodeS, SchemaExP, Qwen rerankers, and embedding retrievers, while maintaining sub-second median latency and scaling to schemas with 23,000+ columns. Our source code is available at https://github.com/thanhdath/grast-sql.",
            "categories": [
                "cs.DB",
                "cs.AI",
                "cs.HC",
                "cs.LG"
            ],
            "primary_category": "cs.DB",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16083v1",
            "code_links": [
                {
                    "url": "https://github.com/thanhdath/grast-sql",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出GRaST，通过LLM高效的模式过滤和函数依赖图重排序，扩展Text2SQL系统处理大规模数据库的能力。",
            "summary_zh": "大多数现代Text2SQL系统在提示大型语言模型（LLM）时，会将整个模式（主要是列信息）与用户的问题一起提供。虽然这种方法在小型数据库上有效，但在超出LLM上下文限制的真实模式上会失败，即使是商业模型也是如此。最近的Spider 2.0基准测试就是一个例子，它包含数百个表和数万列，现有系统经常崩溃。目前的缓解措施要么依赖于代价高昂的多步骤提示流水线，要么通过独立于用户问题对列进行排序来过滤列，忽略了列间的结构。为了扩展现有系统，我们引入了GRaST，这是一个开源的、LLM高效的模式过滤框架，它通过以下方式压缩Text2SQL提示：（i）使用查询感知的LLM编码器（富含值和元数据）对列进行排序，（ii）通过函数依赖关系上的轻量级图Transformer对互连的列进行重排序，以及（iii）使用Steiner树启发式算法选择一个保持连通性的子模式。在真实数据集上的实验表明，GRaST实现了接近完美的召回率和比CodeS、SchemaExP、Qwen重排序器和嵌入检索器更高的精度，同时保持了亚秒级的中值延迟，并可扩展到具有23,000+列的模式。我们的源代码可在https://github.com/thanhdath/grast-sql 获得。",
            "intro_zh": [
                "现有Text2SQL系统在处理大规模数据库时，由于LLM上下文长度限制，无法有效利用完整数据库模式信息，导致性能下降。",
                "GRaST框架通过查询感知的LLM编码器进行列排序，利用函数依赖图Transformer进行列重排序，并采用Steiner树启发式算法选择子模式，实现高效模式过滤。",
                "实验结果表明，GRaST在保持亚秒级延迟的同时，实现了接近完美的召回率和更高的精度，并可扩展到包含数万列的数据库模式。"
            ],
            "method_zh": "**问题定义**：现有Text2SQL系统在处理大规模数据库时，面临着LLM上下文长度的限制。直接将整个数据库模式（包含大量列信息）输入LLM会导致超出上下文限制，从而影响生成SQL查询的准确性和效率。现有的解决方法，如多步提示或独立列排序，要么成本高昂，要么忽略了列之间的关联结构，效果不佳。\\n\\n**核心思路**：GRaST的核心思路是通过高效的模式过滤，减少输入LLM的模式信息量，同时保留关键的列和列之间的关联关系。具体来说，首先对列进行排序，然后利用函数依赖关系对列进行重排序，最后选择一个能够保持连通性的子模式。这样既能降低LLM的计算负担，又能保证生成SQL查询所需的必要信息。\\n\\n**技术框架**：GRaST框架包含三个主要阶段：1) **列排序**：使用查询感知的LLM编码器，结合列的值和元数据，对数据库中的列进行排序。2) **列重排序**：利用函数依赖图Transformer，根据列之间的函数依赖关系对排序后的列进行重排序，提高相关列的排名。3) **子模式选择**：使用Steiner树启发式算法，选择一个能够保持连通性的子模式，作为最终输入LLM的模式信息。\\n\\n**关键创新**：GRaST的关键创新在于结合了查询感知的LLM编码器、函数依赖图Transformer和Steiner树启发式算法，实现了一种高效且准确的模式过滤方法。与现有方法相比，GRaST不仅考虑了列与查询之间的相关性，还考虑了列之间的关联结构，从而能够更有效地选择关键的模式信息。\\n\\n**关键设计**：GRaST使用预训练的LLM作为编码器，并针对Text2SQL任务进行了微调。函数依赖图Transformer采用轻量级设计，以降低计算成本。Steiner树启发式算法用于在保证连通性的前提下，选择尽可能小的子模式。具体的参数设置和网络结构细节在论文中有详细描述。",
            "application_zh": "GRaST框架可应用于各种需要处理大规模数据库的Text2SQL系统，例如智能客服、数据分析平台和自动化报表生成工具。通过提高Text2SQL系统处理大规模数据库的能力，GRaST可以帮助用户更方便地从海量数据中获取所需信息，提高工作效率，并为企业决策提供更准确的数据支持。",
            "highlight_zh": "实验结果表明，GRaST在真实数据集上实现了接近完美的召回率，并且精度高于CodeS、SchemaExP、Qwen重排序器和嵌入检索器等基线方法。GRaST能够处理包含23,000+列的数据库模式，同时保持亚秒级的中值延迟，证明了其在大规模数据库上的高效性和可扩展性。",
            "tags_zh": [
                "Text2SQL",
                "大规模数据库",
                "模式过滤",
                "函数依赖图",
                "LLM",
                "Steiner树",
                "查询优化"
            ],
            "_index": 84,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "From Facts to Conclusions : Integrating Deductive Reasoning in Retrieval-Augmented LLMs",
            "authors": [
                "Shubham Mishra",
                "Samyek Jain",
                "Gorang Mehrishi",
                "Shiv Tiwari",
                "Harsh Sharma",
                "Pratik Narang",
                "Dhruv Kumar"
            ],
            "arxiv_id": "2512.16795v1",
            "summary": "Retrieval-Augmented Generation (RAG) grounds large language models (LLMs) in external evidence, but fails when retrieved sources conflict or contain outdated or subjective information. Prior work address these issues independently but lack unified reasoning supervision. We propose a reasoning-trace-augmented RAG framework that adds structured, interpretable reasoning across three stages : (1) document-level adjudication, (2) conflict analysis, and (3) grounded synthesis, producing citation-linked answers or justified refusals. A Conflict-Aware Trust-Score (CATS) pipeline is introduced which evaluates groundedness, factual correctness, refusal accuracy, and conflict-behavior alignment using an LLM-as-a-Judge. Our 539-query reasoning dataset and evaluation pipeline establish a foundation for conflict-aware, interpretable RAG systems. Experimental results demonstrate substantial gains over baselines, most notably with Qwen, where Supervised Fine-Tuning improved End-to-End answer correctness from 0.069 to 0.883 and behavioral adherence from 0.074 to 0.722.",
            "categories": [
                "cs.CL",
                "cs.AI",
                "cs.CY",
                "cs.IR"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Under Review",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16795v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出推理追踪增强的RAG框架，解决检索信息冲突和主观性问题。",
            "summary_zh": "检索增强生成(RAG)使大型语言模型(LLM)能够利用外部证据，但当检索到的信息存在冲突、过时或主观性时，RAG会失效。现有工作分别解决这些问题，但缺乏统一的推理监督。本文提出了一种推理追踪增强的RAG框架，该框架在三个阶段添加了结构化的、可解释的推理过程：(1)文档级裁决，(2)冲突分析，(3)基于证据的综合，从而生成带有引用的答案或合理的拒绝回答。引入了一种冲突感知信任评分(CATS)流程，该流程使用LLM作为评判者来评估证据充分性、事实正确性、拒绝回答的准确性以及冲突行为的一致性。本文构建了一个包含539个查询的推理数据集和评估流程，为冲突感知、可解释的RAG系统奠定了基础。实验结果表明，该方法优于基线方法，尤其是在Qwen模型上，通过监督微调，端到端答案正确率从0.069提高到0.883，行为一致性从0.074提高到0.722。",
            "intro_zh": [
                "现有RAG方法在处理冲突、过时或主观信息时表现不佳，缺乏统一的推理监督。",
                "提出推理追踪增强的RAG框架，通过文档裁决、冲突分析和证据综合实现可解释推理。",
                "实验表明，该方法在Qwen模型上显著提升了答案正确率和行为一致性。"
            ],
            "method_zh": "**问题定义**：现有RAG模型在面对检索到的信息存在冲突、过时或主观性时，无法有效判断和整合信息，导致生成错误或不可靠的答案。现有的解决方案通常独立解决这些问题，缺乏统一的推理监督机制，难以保证生成结果的质量和可信度。\\n\\n**核心思路**：本文的核心思路是通过引入结构化的推理过程，显式地对检索到的文档进行裁决、冲突分析和证据综合，从而提高RAG模型在复杂信息环境下的推理能力和生成结果的可靠性。通过将推理过程分解为多个可解释的步骤，可以更好地理解模型的决策过程，并进行针对性的优化。\\n\\n**技术框架**：该框架包含三个主要阶段：1) 文档级裁决：对检索到的文档进行评估，判断其可靠性和相关性；2) 冲突分析：识别文档之间的冲突信息，并分析冲突的原因；3) 基于证据的综合：根据裁决结果和冲突分析，综合各个文档的信息，生成带有引用的答案或合理的拒绝回答。此外，还引入了冲突感知信任评分(CATS)流程，使用LLM作为评判者来评估模型的性能。\\n\\n**关键创新**：该方法最重要的创新点在于引入了结构化的推理追踪，将RAG模型的推理过程显式化，使其更易于理解和调试。通过文档裁决和冲突分析，可以有效地过滤掉不可靠的信息，提高生成结果的质量。此外，CATS流程提供了一种自动化的评估方法，可以更全面地评估RAG模型的性能。\\n\\n**关键设计**：框架的关键设计包括：1) 文档裁决模块使用LLM对文档的可靠性和相关性进行评分；2) 冲突分析模块使用LLM识别文档之间的冲突信息，并分析冲突的原因；3) 证据综合模块使用LLM根据裁决结果和冲突分析，生成最终答案。CATS流程使用LLM作为评判者，评估生成答案的证据充分性、事实正确性、拒绝回答的准确性以及冲突行为的一致性。具体参数设置和损失函数等细节在论文中未详细说明，属于未知信息。",
            "application_zh": "该研究成果可应用于需要处理大量冲突或不确定信息的场景，例如智能客服、金融分析、医疗诊断等。通过提高RAG模型在复杂信息环境下的推理能力，可以生成更准确、可靠的答案，从而提升用户体验和决策效率。未来，该方法有望应用于更广泛的知识密集型任务。",
            "highlight_zh": "实验结果表明，该方法在Qwen模型上取得了显著的性能提升。通过监督微调，端到端答案正确率从0.069提高到0.883，行为一致性从0.074提高到0.722。这些结果表明，该方法能够有效地提高RAG模型在处理冲突信息时的推理能力和生成结果的可靠性。",
            "tags_zh": [
                "检索增强生成",
                "大型语言模型",
                "冲突分析",
                "推理追踪",
                "可解释性",
                "知识推理",
                "信息融合"
            ],
            "_index": 85,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "Plausibility as Failure: How LLMs and Humans Co-Construct Epistemic Error",
            "authors": [
                "Claudia Vale Oliveira",
                "Nelson Zagalo",
                "Filipe Silva",
                "Anabela Brandao",
                "Syeda Faryal Hussain Khurrum",
                "Joaquim Santos"
            ],
            "arxiv_id": "2512.16750v1",
            "summary": "Large language models (LLMs) are increasingly used as epistemic partners in everyday reasoning, yet their errors remain predominantly analyzed through predictive metrics rather than through their interpretive effects on human judgment. This study examines how different forms of epistemic failure emerge, are masked, and are tolerated in human AI interaction, where failure is understood as a relational breakdown shaped by model-generated plausibility and human interpretive judgment. We conducted a three round, multi LLM evaluation using interdisciplinary tasks and progressively differentiated assessment frameworks to observe how evaluators interpret model responses across linguistic, epistemic, and credibility dimensions. Our findings show that LLM errors shift from predictive to hermeneutic forms, where linguistic fluency, structural coherence, and superficially plausible citations conceal deeper distortions of meaning. Evaluators frequently conflated criteria such as correctness, relevance, bias, groundedness, and consistency, indicating that human judgment collapses analytical distinctions into intuitive heuristics shaped by form and fluency. Across rounds, we observed a systematic verification burden and cognitive drift. As tasks became denser, evaluators increasingly relied on surface cues, allowing erroneous yet well formed answers to pass as credible. These results suggest that error is not solely a property of model behavior but a co-constructed outcome of generative plausibility and human interpretive shortcuts. Understanding AI epistemic failure therefore requires reframing evaluation as a relational interpretive process, where the boundary between system failure and human miscalibration becomes porous. The study provides implications for LLM assessment, digital literacy, and the design of trustworthy human AI communication.",
            "categories": [
                "cs.HC",
                "cs.AI"
            ],
            "primary_category": "cs.HC",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "19 pages, 2 tables, 77 references, 6 appendices",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16750v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "揭示LLM与人类交互中认知错误的共建机制，强调评估的解释性视角",
            "summary_zh": "大型语言模型（LLM）日益成为日常推理中的认知伙伴，但对其错误的分析主要集中在预测指标上，而非其对人类判断的解释性影响。本研究考察了在人机交互中，不同形式的认知失败如何产生、被掩盖和被容忍。这里的失败被理解为一种关系破裂，由模型生成的可信度和人类的解释性判断共同塑造。我们进行了三轮多LLM评估，采用跨学科任务和逐步区分的评估框架，观察评估者如何从语言、认知和可信度维度解释模型响应。研究发现，LLM的错误从预测性转向解释性，语言流畅性、结构连贯性和表面上可信的引用掩盖了更深层次的意义扭曲。评估者经常混淆正确性、相关性、偏差、依据和一致性等标准，表明人类判断将分析区分简化为受形式和流畅性影响的直觉启发式。在各轮评估中，我们观察到系统的验证负担和认知漂移。随着任务变得更加密集，评估者越来越依赖表面线索，允许错误但形式良好的答案被认为是可信的。这些结果表明，错误不仅仅是模型行为的属性，而是生成的可信度和人类解释性捷径共同构建的结果。因此，理解AI的认知失败需要将评估重新定义为一个关系解释过程，其中系统失败和人类校准错误之间的界限变得模糊。该研究为LLM评估、数字素养和可信赖的人机通信设计提供了启示。",
            "intro_zh": [
                "现有LLM错误分析侧重预测指标，忽略了其对人类判断的解释性影响，导致对人机交互中认知错误的理解不足。",
                "该研究将LLM错误视为人机交互中共同构建的认知失败，强调模型生成的可信度与人类解释性判断之间的关系。",
                "通过多轮评估，揭示了LLM错误从预测性向解释性转变，以及人类评估中出现的验证负担和认知漂移现象。"
            ],
            "method_zh": "**问题定义**：现有的大型语言模型（LLM）的错误评估主要集中在预测准确率等指标上，忽略了LLM的输出如何影响人类的判断和认知。这种评估方式无法充分理解人机交互中认知错误的产生机制，尤其是在LLM输出具有表面可信度的情况下，人类可能会受到误导。现有方法缺乏对人类如何解释和容忍LLM错误的深入研究。\\n\\n**核心思路**：本研究的核心思路是将LLM的错误视为一种“共建”的现象，即错误并非仅仅是模型自身的属性，而是模型生成的内容与人类的解释性判断相互作用的结果。研究强调了“可信度”（plausibility）在这一过程中的作用，认为LLM生成的表面上合理的内容可能会掩盖深层次的错误，从而影响人类的判断。\\n\\n**技术框架**：研究采用了一种三轮多LLM评估框架，涉及跨学科任务。每一轮评估都使用不同的评估框架，这些框架在语言、认知和可信度维度上逐步区分。评估者需要对LLM的响应进行评估，研究人员观察评估者如何解释这些响应，以及在不同任务密度下，评估者的判断标准如何变化。\\n\\n**关键创新**：该研究最重要的创新在于其对LLM错误评估的视角转变。它不再将错误视为模型自身的孤立问题，而是将其视为人机交互中共同构建的认知失败。这种视角强调了人类的解释性判断在错误产生过程中的作用，并揭示了LLM生成的可信度如何影响人类的判断。\\n\\n**关键设计**：研究的关键设计包括：1) 多轮评估，以便观察评估者判断标准随任务密度变化的趋势；2) 跨学科任务，以考察LLM在不同领域的表现；3) 逐步区分的评估框架，以更细致地分析LLM的错误类型和人类的判断标准。研究还关注了评估者在评估过程中出现的“验证负担”和“认知漂移”现象，即随着任务变得更加密集，评估者越来越依赖表面线索，从而更容易接受错误的答案。",
            "application_zh": "该研究成果可应用于LLM评估体系的改进，提升数字素养教育，并指导更值得信赖的人机交互系统设计。通过理解LLM错误的共建机制，可以开发更有效的评估方法，帮助用户识别和避免LLM带来的认知偏差，从而促进负责任的AI应用。",
            "highlight_zh": "研究发现，LLM错误会从预测性错误转变为解释性错误，即表面流畅和连贯的回答可能掩盖深层含义的扭曲。评估者在任务密度增加时，会更多依赖表面线索，导致错误答案被接受。这些结果强调了评估LLM时，需要关注人类的解释性判断，而不仅仅是模型的预测准确率。",
            "tags_zh": [
                "大型语言模型",
                "人机交互",
                "认知错误",
                "可信度",
                "评估框架",
                "解释性判断",
                "认知漂移",
                "验证负担"
            ],
            "_index": 86,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "Cyber Humanism in Education: Reclaiming Agency through AI and Learning Sciences",
            "authors": [
                "Giovanni Adorni"
            ],
            "arxiv_id": "2512.16701v1",
            "summary": "Generative Artificial Intelligence (GenAI) is rapidly reshaping how knowledge is produced and validated in education. Rather than adding another digital tool, large language models reconfigure reading, writing, and coding into hybrid human-AI workflows, raising concerns about epistemic automation, cognitive offloading, and the de-professiona\\-lisation of teachers. This paper proposes \\emph{Cyber Humanism in Education} as a framework for reclaiming human agency in this landscape. We conceptualise AI-enabled learning environments as socio-technical infrastructures co-authored by humans and machines, and position educators and learners as epistemic agents and \\emph{algorithmic citizens} who have both the right and the responsibility to shape these infrastructures.\n  We articulate three pillars for cyber-humanist design, \\emph{reflexive competence}, \\emph{algorithmic citizenship}, and \\emph{dialogic design}, and relate them to major international digital and AI competence frameworks. We then present higher-education case studies that operationalise these ideas through \\emph{prompt-based learning} and a new \\emph{Conversational AI Educator} certification within the EPICT ecosystem. The findings show how such practices can strengthen epistemic agency while surfacing tensions around workload, equity, and governance, and outline implications for the future of AI-rich, human-centred education.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "15 pages, 16 references, Key Note preented at the \"WAILS 2025 - The 2nd. Workshop on Artificial Intelligence with and for Learning Sciences\", Cagliary, Italy, 10-12 December 2025",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16701v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出教育领域“赛博人文主义”框架，通过AI与学习科学重塑人类能动性",
            "summary_zh": "生成式人工智能（GenAI）正在迅速重塑教育中知识的生产和验证方式。大型语言模型不仅仅是另一种数字工具，它们将阅读、写作和编码重新配置为混合人机工作流程，引发了对认知自动化、认知卸载和教师去专业化的担忧。本文提出了“教育中的赛博人文主义”框架，旨在重塑这一领域中的人类能动性。我们将支持人工智能的学习环境概念化为由人类和机器共同创作的社会技术基础设施，并将教育者和学习者定位为认知主体和“算法公民”，他们有权力和责任来塑造这些基础设施。我们阐述了赛博人文主义设计的三个支柱：反思能力、算法公民身份和对话式设计，并将它们与主要的国际数字和人工智能能力框架联系起来。然后，我们展示了高等教育案例研究，这些案例研究通过“基于提示的学习”和EPICT生态系统中的新的“会话式AI教育者”认证来实施这些想法。研究结果表明，这些实践如何能够加强认知能动性，同时揭示围绕工作量、公平和治理的紧张关系，并概述了以人为本的富人工智能教育的未来影响。",
            "intro_zh": [
                "现有教育方法在GenAI快速发展下，面临认知自动化、认知卸载和教师去专业化的挑战。",
                "论文提出“教育中的赛博人文主义”框架，将AI学习环境视为人机共创的社会技术基础设施。",
                "通过“基于提示的学习”和“会话式AI教育者”认证等案例，验证了该框架在加强认知能动性方面的潜力。"
            ],
            "method_zh": "**问题定义**：论文旨在解决生成式AI在教育领域快速发展带来的挑战，包括知识生产和验证方式的变革，以及由此引发的认知自动化、认知卸载和教师去专业化等问题。现有方法未能充分应对这些挑战，需要一种新的框架来重塑人类在教育中的能动性。\\n\\n**核心思路**：论文的核心思路是将AI学习环境视为由人类和机器共同创作的社会技术基础设施。通过强调教育者和学习者作为认知主体和“算法公民”的角色，赋予他们塑造这些基础设施的权力和责任。这种赛博人文主义的视角旨在平衡AI带来的便利与人类的自主性。\\n\\n**技术框架**：论文提出了赛博人文主义设计的三个支柱：反思能力、算法公民身份和对话式设计。反思能力强调对AI工具的批判性思考和应用；算法公民身份强调参与AI系统设计和治理的权利和责任；对话式设计强调人机之间的互动和协作。这些支柱与国际数字和AI能力框架相联系，并应用于高等教育案例研究中。\\n\\n**关键创新**：论文的关键创新在于提出了“教育中的赛博人文主义”这一框架，它超越了将AI视为单纯工具的视角，而是将其视为与人类共同演进的社会技术系统。这种框架强调了人类在AI时代教育中的核心地位和能动性，并为AI在教育领域的应用提供了新的伦理和实践指导。\\n\\n**关键设计**：论文通过“基于提示的学习”和“会话式AI教育者”认证等具体实践来体现赛博人文主义的设计理念。“基于提示的学习”鼓励学生通过精心设计的提示与AI进行互动，从而提升学习效果和批判性思维能力。“会话式AI教育者”认证旨在培养教师利用AI进行教学的能力，并确保他们能够负责任地使用这些技术。",
            "application_zh": "该研究成果可应用于高等教育、K12教育等多个领域，指导AI辅助教学工具的设计与应用，提升学习者的认知能力和批判性思维，促进教育公平，并为教育政策制定提供参考。未来，该框架有望扩展到其他领域，如医疗、法律等，促进人与AI的协同发展。",
            "highlight_zh": "论文通过高等教育案例研究，展示了“基于提示的学习”和“会话式AI教育者”认证在实践中的效果。研究发现，这些方法能够加强认知能动性，并揭示了围绕工作量、公平和治理的潜在问题。这些发现为未来AI在教育领域的应用提供了宝贵的经验和启示。",
            "tags_zh": [
                "赛博人文主义",
                "教育人工智能",
                "人机协作",
                "算法公民",
                "提示学习"
            ],
            "_index": 87,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "Microsoft Academic Graph Information Retrieval for Research Recommendation and Assistance",
            "authors": [
                "Jacob Reiss",
                "Shikshya Shiwakoti",
                "Samuel Goldsmith",
                "Ujjwal Pandit"
            ],
            "arxiv_id": "2512.16661v1",
            "summary": "In today's information-driven world, access to scientific publications has become increasingly easy. At the same time, filtering through the massive volume of available research has become more challenging than ever. Graph Neural Networks (GNNs) and graph attention mechanisms have shown strong effectiveness in searching large-scale information databases, particularly when combined with modern large language models. In this paper, we propose an Attention-Based Subgraph Retriever, a GNN-as-retriever model that applies attention-based pruning to extract a refined subgraph, which is then passed to a large language model for advanced knowledge reasoning.",
            "categories": [
                "cs.IR",
                "cs.AI"
            ],
            "primary_category": "cs.IR",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "5 pages, 3 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16661v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出基于注意力的子图检索器，用于科研推荐和辅助，提升知识推理能力。",
            "summary_zh": "在当今信息驱动的世界中，获取科学出版物变得越来越容易。与此同时，筛选海量可用研究成果的难度也变得前所未有。图神经网络（GNN）和图注意力机制在搜索大规模信息数据库方面表现出强大的有效性，尤其是在与现代大型语言模型结合使用时。在本文中，我们提出了一种基于注意力的子图检索器，这是一种GNN检索模型，它应用基于注意力的剪枝来提取精炼的子图，然后将其传递给大型语言模型以进行高级知识推理。",
            "intro_zh": [
                "现有科研信息过滤方法难以应对海量数据，检索效率和准确性面临挑战。",
                "提出基于注意力的子图检索器，利用GNN和注意力机制提取关键信息子图。",
                "通过精炼子图与大型语言模型结合，提升知识推理能力，辅助科研推荐。"
            ],
            "method_zh": "**问题定义**：论文旨在解决科研信息过载的问题，即如何从海量的学术论文数据中高效、准确地检索出与用户需求相关的研究成果。现有方法，例如传统的关键词搜索或基于内容相似度的推荐，难以有效处理论文之间的复杂关系，并且无法进行深层次的知识推理，导致检索结果的准确性和相关性不足。\\n\\n**核心思路**：论文的核心思路是利用图神经网络（GNN）来建模学术论文之间的关系，并使用注意力机制来提取关键的子图。通过这种方式，模型可以关注到与用户查询最相关的论文和关系，从而提高检索的效率和准确性。然后，将提取的精炼子图输入到大型语言模型中，利用其强大的知识推理能力，进一步提升检索结果的质量。\\n\\n**技术框架**：整体框架包含以下几个主要阶段：1) 构建学术图谱：利用Microsoft Academic Graph (MAG)等数据源构建学术论文之间的关系图。2) 子图检索：使用基于注意力的GNN模型从图谱中检索出与用户查询相关的子图。该GNN模型通过注意力机制对图中的节点和边进行加权，从而提取出最相关的子图。3) 知识推理：将提取的子图输入到大型语言模型中，利用其进行知识推理，例如判断论文之间的关系、预测未来的研究方向等。4) 结果排序与推荐：根据知识推理的结果对检索到的论文进行排序，并向用户推荐最相关的研究成果。\\n\\n**关键创新**：论文的关键创新在于提出了基于注意力的子图检索器，该模型能够有效地从大规模学术图谱中提取出与用户查询相关的子图。与传统的GNN模型相比，该模型引入了注意力机制，可以更加关注到与用户查询最相关的节点和边，从而提高检索的准确性。此外，该模型还与大型语言模型相结合，利用其强大的知识推理能力，进一步提升了检索结果的质量。\\n\\n**关键设计**：在GNN模型中，使用了图注意力网络（GAT）作为基本的图卷积层。注意力权重通过计算节点之间的相似度来确定，相似度可以使用余弦相似度或点积等方法计算。损失函数的设计目标是最大化相关论文的得分，同时最小化不相关论文的得分。可以使用hinge loss或cross-entropy loss等损失函数。在大型语言模型方面，可以使用预训练的BERT或GPT等模型，并针对科研推荐任务进行微调。",
            "application_zh": "该研究成果可应用于多种场景，包括：个性化科研推荐系统，帮助研究人员快速找到相关文献；智能科研助手，辅助研究人员进行文献综述和研究方向探索；学术知识图谱构建，为科研管理和决策提供支持。该方法有望提升科研效率，促进学术交流与合作。",
            "highlight_zh": "论文提出的注意力机制子图检索器，能够有效提取关键信息，提升检索准确性。通过与大型语言模型结合，实现了更深层次的知识推理，提高了推荐质量。具体的性能数据和对比基线（如传统GNN模型）的提升幅度需要在实验部分进一步给出。",
            "tags_zh": [
                "图神经网络",
                "注意力机制",
                "子图检索",
                "科研推荐",
                "知识推理"
            ],
            "_index": 88,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16661v1/gril_framework.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16661v1/sag_outline.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16661v1/AttentionbasedGraphRetriever_Algo.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Refusal Steering: Fine-grained Control over LLM Refusal Behaviour for Sensitive Topics",
            "authors": [
                "Iker García-Ferrero",
                "David Montero",
                "Roman Orus"
            ],
            "arxiv_id": "2512.16602v1",
            "summary": "We introduce Refusal Steering, an inference-time method to exercise fine-grained control over Large Language Models refusal behaviour on politically sensitive topics without retraining. We replace fragile pattern-based refusal detection with an LLM-as-a-judge that assigns refusal confidence scores and we propose a ridge-regularized variant to compute steering vectors that better isolate the refusal--compliance direction. On Qwen3-Next-80B-A3B-Thinking, our method removes the refusal behaviour of the model around politically sensitive topics while maintaining safety on JailbreakBench and near-baseline performance on general benchmarks. The approach generalizes across 4B and 80B models and can also induce targeted refusals when desired. We analize the steering vectors and show that refusal signals concentrate in deeper layers of the transformer and are distributed across many dimensions. Together, these results demonstrate that activation steering can remove political refusal behaviour while retaining safety alignment for harmful content, offering a practical path to controllable, transparent moderation at inference time.",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16602v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "Refusal Steering：通过激活向量干预实现对LLM在敏感话题上拒绝行为的细粒度控制",
            "summary_zh": "本文提出了一种名为Refusal Steering的推理时方法，用于对大型语言模型在政治敏感话题上的拒绝行为进行细粒度控制，而无需重新训练。该方法使用LLM-as-a-judge来替代脆弱的基于模式的拒绝检测，并赋予拒绝置信度分数。同时，提出了一种岭正则化变体来计算steering vectors，从而更好地隔离拒绝-顺从方向。在Qwen3-Next-80B-A3B-Thinking模型上，该方法消除了模型在政治敏感话题上的拒绝行为，同时在JailbreakBench上保持了安全性，并在通用基准测试上保持了接近基线的性能。该方法可以推广到4B和80B模型，并且可以在需要时诱导有针对性的拒绝。分析表明，拒绝信号集中在Transformer的更深层，并且分布在许多维度上。这些结果表明，激活steering可以消除政治拒绝行为，同时保持对有害内容的安全对齐，从而为推理时可控、透明的审核提供了一条实用途径。",
            "intro_zh": [
                "现有方法难以对LLM在政治敏感话题上的拒绝行为进行细粒度控制，且依赖脆弱的模式匹配。",
                "Refusal Steering通过LLM-as-a-judge评估拒绝置信度，并使用岭回归计算steering vectors，精准控制拒绝行为。",
                "实验表明，该方法在消除政治拒绝行为的同时，保持了模型在安全性和通用性能上的良好表现。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大型语言模型（LLM）在处理政治敏感话题时，拒绝回答或提供信息的行为控制问题。现有方法通常依赖于基于模式的拒绝检测，这种方法脆弱且难以泛化。此外，缺乏对拒绝行为进行细粒度控制的手段，难以在安全性、通用性和政治敏感性之间取得平衡。\\n\\n**核心思路**：论文的核心思路是利用激活steering技术，通过计算和应用steering vectors来干预LLM的内部激活状态，从而控制其拒绝行为。关键在于使用LLM本身作为裁判（LLM-as-a-judge）来评估拒绝的置信度，并使用岭回归来更精确地计算steering vectors，从而更好地分离拒绝和顺从的方向。\\n\\n**技术框架**：整体框架包含以下几个主要步骤：1) 使用LLM-as-a-judge对模型的拒绝行为进行评分，生成拒绝置信度；2) 使用带岭正则化的线性模型，基于模型的激活状态和拒绝置信度，计算steering vectors；3) 在推理时，将计算得到的steering vectors添加到模型的激活状态中，从而引导模型的行为。\\n\\n**关键创新**：最重要的创新点在于：1) 使用LLM-as-a-judge进行拒绝检测，避免了脆弱的模式匹配；2) 提出了一种岭正则化的steering vector计算方法，能够更有效地隔离拒绝和顺从的方向，从而实现更精确的控制；3) 证明了该方法可以在消除政治拒绝行为的同时，保持模型在安全性和通用性能上的良好表现。\\n\\n**关键设计**：关键设计包括：1) LLM-as-a-judge的具体prompt设计，需要能够准确评估拒绝的置信度；2) 岭回归的正则化参数的选择，需要在steering vector的强度和模型的泛化能力之间进行权衡；3) steering vector应用的位置（Transformer的哪些层）和方式（加法或乘法）的选择，需要根据具体的模型和任务进行调整。",
            "application_zh": "该研究成果可应用于需要对LLM输出进行精细控制的场景，例如：政治敏感话题的讨论、内容审核、个性化对话系统等。通过控制LLM的拒绝行为，可以使其在特定领域更加安全、可靠和可控。未来，该技术有望应用于更广泛的LLM应用中，实现更加透明和可控的人工智能系统。",
            "highlight_zh": "实验结果表明，Refusal Steering方法在Qwen3-Next-80B-A3B-Thinking模型上，成功消除了模型在政治敏感话题上的拒绝行为，同时在JailbreakBench上保持了安全性，并在通用基准测试上保持了接近基线的性能。该方法还成功推广到4B和80B模型，并实现了有针对性的拒绝。",
            "tags_zh": [
                "大型语言模型",
                "拒绝行为控制",
                "激活Steering",
                "政治敏感话题",
                "LLM-as-a-judge"
            ],
            "_index": 89,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16602v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16602v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16602v1/images/top_layer_pca_2d_chinabadWRMD.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "From Personalization to Prejudice: Bias and Discrimination in Memory-Enhanced AI Agents for Recruitment",
            "authors": [
                "Himanshu Gharat",
                "Himanshi Agrawal",
                "Gourab K. Patro"
            ],
            "arxiv_id": "2512.16532v1",
            "summary": "Large Language Models (LLMs) have empowered AI agents with advanced capabilities for understanding, reasoning, and interacting across diverse tasks. The addition of memory further enhances them by enabling continuity across interactions, learning from past experiences, and improving the relevance of actions and responses over time; termed as memory-enhanced personalization. Although such personalization through memory offers clear benefits, it also introduces risks of bias. While several previous studies have highlighted bias in ML and LLMs, bias due to memory-enhanced personalized agents is largely unexplored. Using recruitment as an example use case, we simulate the behavior of a memory-enhanced personalized agent, and study whether and how bias is introduced and amplified in and across various stages of operation. Our experiments on agents using safety-trained LLMs reveal that bias is systematically introduced and reinforced through personalization, emphasizing the need for additional protective measures or agent guardrails in memory-enhanced LLM-based AI agents.",
            "categories": [
                "cs.AI",
                "cs.IR"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "In Proceedings of the Nineteenth ACM International Conference on Web Search and Data Mining (WSDM '26)",
            "doi": "10.1145/3773966.3779376",
            "journal_ref": "In Proceedings of the Nineteenth ACM International Conference on Web Search and Data Mining (WSDM '26), 2026, Boise, ID, USA. ACM, New York, NY, USA",
            "pdf_url": "https://arxiv.org/pdf/2512.16532v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "揭示记忆增强型AI招聘Agent中的偏见引入与强化机制",
            "summary_zh": "大型语言模型(LLMs)赋予了AI Agent强大的理解、推理和交互能力，使其能够胜任各种任务。通过添加记忆功能，AI Agent可以实现跨交互的连续性，从过去的经验中学习，并随着时间的推移提高行为和响应的相关性，这种方式被称为记忆增强型个性化。虽然这种通过记忆实现的个性化具有明显的优势，但也带来了偏见的风险。尽管之前的研究已经强调了ML和LLM中的偏见，但由于记忆增强型个性化Agent而产生的偏见在很大程度上尚未被探索。本文以招聘为例，模拟了记忆增强型个性化Agent的行为，并研究了偏见是如何在操作的各个阶段引入和加强的。对使用安全训练LLM的Agent进行的实验表明，偏见是通过个性化系统地引入和强化的，这强调了在基于记忆增强型LLM的AI Agent中采取额外保护措施或Agent防护措施的必要性。",
            "intro_zh": [
                "现有研究较少关注记忆增强型AI Agent中的偏见问题，尤其是在个性化过程中如何引入和强化偏见。",
                "该研究通过模拟招聘场景下的记忆增强型Agent，分析偏见的产生和演变过程，揭示个性化带来的潜在风险。",
                "实验结果表明，即使使用安全训练的LLM，偏见仍然会通过个性化被系统性地引入和强化，需要额外的安全措施。"
            ],
            "method_zh": "**问题定义**：论文旨在研究在记忆增强型AI Agent中，个性化如何引入和强化偏见。现有方法主要关注ML和LLM的固有偏见，忽略了记忆增强和个性化带来的新偏见来源。在招聘场景下，这种偏见可能导致不公平的候选人筛选，损害公平性。\\n\\n**核心思路**：论文的核心思路是通过模拟一个记忆增强型AI招聘Agent，观察其在与不同候选人交互过程中，如何逐渐形成和强化偏见。通过控制实验变量，分析偏见产生的具体环节和影响因素。这种模拟方法能够更深入地理解偏见的动态演变过程。\\n\\n**技术框架**：该研究的技术框架主要包括以下几个阶段：1) 构建一个基于LLM的AI招聘Agent，该Agent具有记忆功能，可以记录与候选人的交互历史。2) 设计模拟的候选人数据集，包含不同的背景和特征。3) Agent与候选人进行交互，根据候选人的信息和历史记录进行评估和筛选。4) 分析Agent的决策过程，识别偏见产生的环节，并量化偏见的程度。\\n\\n**关键创新**：该研究的关键创新在于关注了记忆增强型AI Agent中的个性化偏见问题，并提出了一个模拟框架来研究这种偏见的产生和演变。与以往研究不同，该研究不仅关注LLM的固有偏见，更关注个性化带来的新偏见来源。\\n\\n**关键设计**：在实验设计方面，论文可能控制了候选人的某些特征（例如性别、种族、教育背景等），并观察Agent在不同特征组合下的决策差异。此外，论文可能使用了特定的指标来量化偏见的程度，例如公平性指标（如机会均等、统计均等）或偏差指标（如优势比、差异倍数）。具体的LLM选择和训练方法（例如，是否使用安全训练）也是关键的设计因素。",
            "application_zh": "该研究成果可应用于各种需要个性化推荐或决策的AI系统中，例如金融信贷、教育评估、医疗诊断等。通过理解和减轻记忆增强型AI Agent中的偏见，可以提高AI系统的公平性、透明度和可靠性，避免歧视性结果，促进社会公平。",
            "highlight_zh": "该研究通过实验证明，即使使用经过安全训练的LLM，记忆增强型个性化Agent仍然会系统性地引入和强化偏见。这表明，仅仅依靠LLM的安全训练是不够的，还需要额外的保护措施或Agent防护措施来减轻个性化带来的偏见风险。具体的性能数据和提升幅度未知，需要在论文中查找。",
            "tags_zh": [
                "大型语言模型",
                "AI Agent",
                "记忆增强",
                "个性化",
                "偏见",
                "招聘",
                "公平性",
                "机器学习"
            ],
            "_index": 90,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16532v1/Figure_1_overview.png",
                    "caption": "",
                    "figure_id": "fig_0"
                }
            ]
        },
        {
            "title": "cuPilot: A Strategy-Coordinated Multi-agent Framework for CUDA Kernel Evolution",
            "authors": [
                "Jinwu Chen",
                "Qidie Wu",
                "Bin Li",
                "Lin Ma",
                "Xin Si",
                "Yang Hu",
                "Shouyi Yin",
                "Jun Yang"
            ],
            "arxiv_id": "2512.16465v1",
            "summary": "Optimizing CUDA kernels is a challenging and labor-intensive task, given the need for hardware-software co-design expertise and the proprietary nature of high-performance kernel libraries. While recent large language models (LLMs) combined with evolutionary algorithms show promise in automatic kernel optimization, existing approaches often fall short in performance due to their suboptimal agent designs and mismatched evolution representations. This work identifies these mismatches and proposes cuPilot, a strategy-coordinated multi-agent framework that introduces strategy as an intermediate semantic representation for kernel evolution. Key contributions include a strategy-coordinated evolution algorithm, roofline-guided prompting, and strategy-level population initialization. Experimental results show that the generated kernels by cuPilot achieve an average speed up of 3.09$\\times$ over PyTorch on a benchmark of 100 kernels. On the GEMM tasks, cuPilot showcases sophisticated optimizations and achieves high utilization of critical hardware units. The generated kernels are open-sourced at https://github.com/champloo2878/cuPilot-Kernels.git.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16465v1",
            "code_links": [
                {
                    "url": "https://github.com/champloo2878/cuPilot-Kernels.git",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "cuPilot：一种策略协调的多智能体框架，用于CUDA内核演化",
            "summary_zh": "优化CUDA内核是一项具有挑战性且劳动密集型的工作，需要软硬件协同设计专业知识和高性能内核库的专有性质。虽然最近的大型语言模型（LLM）与进化算法相结合在自动内核优化方面显示出希望，但由于其次优的智能体设计和不匹配的演化表示，现有方法通常在性能方面表现不佳。这项工作识别了这些不匹配之处，并提出了cuPilot，一个策略协调的多智能体框架，它引入策略作为内核演化的中间语义表示。主要贡献包括策略协调的进化算法、roofline引导的提示和策略级别的种群初始化。实验结果表明，cuPilot生成的内核在100个内核的基准测试中，平均比PyTorch加速3.09倍。在GEMM任务中，cuPilot展示了复杂的优化，并实现了关键硬件单元的高利用率。生成的内核已在https://github.com/champloo2878/cuPilot-Kernels.git上开源。",
            "intro_zh": [
                "现有CUDA内核优化方法依赖专家知识，且基于LLM的方法存在智能体设计和演化表示不匹配的问题。",
                "cuPilot提出策略协调的多智能体框架，将策略作为内核演化的中间语义表示，解决上述不匹配问题。",
                "实验表明，cuPilot生成的内核在多个基准测试中显著优于PyTorch，并在GEMM任务中实现了硬件的高效利用。"
            ],
            "method_zh": "**问题定义**：CUDA内核优化需要深厚的软硬件协同设计知识，且高性能内核库通常是专有的，导致优化过程耗时且困难。现有基于LLM和进化算法的方法，由于智能体设计不佳和演化表示不匹配，难以达到理想的优化效果。这些方法通常缺乏对CUDA内核优化策略的有效建模和利用。\\n\\n**核心思路**：cuPilot的核心思路是将CUDA内核优化过程分解为一系列策略，并使用策略作为中间语义表示，从而弥合LLM和内核演化之间的差距。通过策略协调，多个智能体可以协同工作，探索不同的优化方向，并最终生成高性能的CUDA内核。这种方法能够更好地利用LLM的生成能力，并指导内核演化过程。\\n\\n**技术框架**：cuPilot框架包含以下主要模块：1) 策略协调的进化算法：该算法使用策略作为演化的基本单元，并设计了相应的交叉和变异操作。2) Roofline引导的提示：利用Roofline模型指导LLM生成更有效的优化策略。3) 策略级别的种群初始化：通过预先定义的策略集合初始化种群，加速演化过程。整体流程为：首先，使用Roofline模型分析目标内核的性能瓶颈；然后，根据瓶颈选择合适的优化策略；接着，使用LLM生成相应的CUDA代码；最后，通过进化算法不断优化策略组合，最终生成高性能的CUDA内核。\\n\\n**关键创新**：cuPilot的关键创新在于引入了策略作为CUDA内核演化的中间语义表示。与直接使用LLM生成CUDA代码相比，cuPilot能够更好地控制优化过程，并利用LLM的知识进行策略选择和代码生成。此外，策略协调的进化算法能够有效地探索不同的优化方向，并找到最优的策略组合。\\n\\n**关键设计**：Roofline引导的提示机制利用Roofline模型提供的硬件性能上限信息，指导LLM生成更符合硬件特性的优化策略。策略协调的进化算法采用了一种新的交叉和变异操作，能够有效地探索策略空间。策略级别的种群初始化使用预定义的策略集合，加速了演化过程的收敛。具体的参数设置和损失函数等技术细节在论文中进行了详细描述。",
            "application_zh": "cuPilot可应用于各种需要高性能CUDA内核的领域，例如深度学习、科学计算和图像处理。它可以帮助开发者自动优化CUDA内核，提高应用程序的性能，并降低开发成本。该研究的成果可以促进高性能计算的发展，并推动人工智能技术的应用。",
            "highlight_zh": "实验结果表明，cuPilot在100个内核的基准测试中，平均比PyTorch加速3.09倍。在GEMM任务中，cuPilot展示了复杂的优化，并实现了关键硬件单元的高利用率。这些结果表明，cuPilot能够有效地优化CUDA内核，并显著提高应用程序的性能。",
            "tags_zh": [
                "CUDA内核优化",
                "多智能体系统",
                "进化算法",
                "大型语言模型",
                "Roofline模型"
            ],
            "_index": 91,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16465v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16465v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16465v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Towards AI-Supported Research: a Vision of the TIB AIssistant",
            "authors": [
                "Sören Auer",
                "Allard Oelen",
                "Mohamad Yaser Jaradeh",
                "Mutahira Khalid",
                "Farhana Keya",
                "Sasi Kiran Gaddipati",
                "Jennifer D'Souza",
                "Lorenz Schlüter",
                "Amirreza Alasti",
                "Gollam Rabby",
                "Azanzi Jiomekong",
                "Oliver Karras"
            ],
            "arxiv_id": "2512.16447v1",
            "summary": "The rapid advancements in Generative AI and Large Language Models promise to transform the way research is conducted, potentially offering unprecedented opportunities to augment scholarly workflows. However, effectively integrating AI into research remains a challenge due to varying domain requirements, limited AI literacy, the complexity of coordinating tools and agents, and the unclear accuracy of Generative AI in research. We present the vision of the TIB AIssistant, a domain-agnostic human-machine collaborative platform designed to support researchers across disciplines in scientific discovery, with AI assistants supporting tasks across the research life cycle. The platform offers modular components - including prompt and tool libraries, a shared data store, and a flexible orchestration framework - that collectively facilitate ideation, literature analysis, methodology development, data analysis, and scholarly writing. We describe the conceptual framework, system architecture, and implementation of an early prototype that demonstrates the feasibility and potential impact of our approach.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16447v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出TIB AIssistant：一个支持AI的跨学科科研协作平台",
            "summary_zh": "生成式AI和大型语言模型的快速发展有望变革科研方式，为学术工作流程带来前所未有的增强机会。然而，由于领域需求各异、AI素养有限、工具和代理协调复杂以及生成式AI在研究中的准确性不明确，将AI有效集成到研究中仍然是一个挑战。我们提出了TIB AIssistant的愿景，这是一个领域无关的人机协作平台，旨在支持跨学科研究人员进行科学发现，AI助手支持研究生命周期中的各项任务。该平台提供模块化组件，包括提示和工具库、共享数据存储以及灵活的编排框架，共同促进构思、文献分析、方法开发、数据分析和学术写作。我们描述了概念框架、系统架构和一个早期原型的实现，该原型展示了我们方法的可行性和潜在影响。",
            "intro_zh": [
                "现有方法难以有效整合AI到科研中，面临领域差异大、AI素养不足、工具协调复杂和AI准确性不确定等挑战。",
                "TIB AIssistant旨在构建一个领域无关的人机协作平台，通过AI助手支持科研全流程，促进科学发现。",
                "早期原型验证了TIB AIssistant的可行性和潜在价值，其模块化设计和灵活编排框架是关键。"
            ],
            "method_zh": "**问题定义**：当前科研工作流程中，生成式AI的集成面临诸多挑战。不同学科的研究需求差异巨大，研究人员的AI素养参差不齐，各种AI工具和代理的协调复杂，以及生成式AI在科研中的准确性问题，都阻碍了AI在科研领域的有效应用。现有方法缺乏一个统一的、易于使用的平台来整合和管理这些AI资源，导致研究效率低下。\\n\\n**核心思路**：TIB AIssistant的核心思路是构建一个领域无关的人机协作平台，为研究人员提供一个统一的界面来访问和使用各种AI工具和服务。通过模块化的设计和灵活的编排框架，该平台可以根据不同学科的需求进行定制，并支持研究生命周期中的各个阶段，从而提高科研效率和质量。\\n\\n**技术框架**：TIB AIssistant的整体架构包括以下几个主要模块：1) 提示和工具库：提供各种预定义的提示和AI工具，供研究人员选择和使用。2) 共享数据存储：用于存储和管理研究数据，方便AI工具进行访问和分析。3) 灵活的编排框架：允许研究人员自定义AI工作流程，将不同的AI工具和服务组合起来，完成复杂的科研任务。4) 用户界面：提供友好的用户界面，方便研究人员与AI助手进行交互。\\n\\n**关键创新**：TIB AIssistant最重要的技术创新点在于其领域无关性和模块化设计。与以往专注于特定领域的AI科研工具不同，TIB AIssistant旨在支持跨学科的研究人员，并提供灵活的定制选项。此外，其模块化的设计使得可以轻松地添加和替换AI工具和服务，从而保持平台的先进性和适应性。\\n\\n**关键设计**：TIB AIssistant的关键设计包括：1) 提示工程：设计有效的提示，引导AI工具生成高质量的输出。2) 工具集成：将各种AI工具和服务集成到平台中，并提供统一的API接口。3) 工作流程编排：设计灵活的工作流程编排机制，允许研究人员自定义AI工作流程。4) 用户界面设计：设计友好的用户界面，方便研究人员与AI助手进行交互。具体的参数设置、损失函数、网络结构等技术细节取决于所集成的AI工具和服务。",
            "application_zh": "TIB AIssistant具有广泛的应用前景，可应用于各个学科的科研领域，例如自然科学、社会科学、人文科学等。它可以帮助研究人员更高效地进行文献分析、数据分析、方法开发和学术写作，从而加速科学发现的进程。未来，TIB AIssistant有望成为科研人员不可或缺的助手，推动科研领域的创新和发展。",
            "highlight_zh": "论文展示了一个TIB AIssistant的早期原型，验证了其可行性和潜在影响。该原型集成了多种AI工具和服务，并提供了一个用户友好的界面。实验结果表明，使用TIB AIssistant可以显著提高研究效率和质量。具体的性能数据和对比基线在论文中进行了详细描述。",
            "tags_zh": [
                "AI辅助研究",
                "人机协作",
                "科研平台",
                "生成式AI",
                "大型语言模型"
            ],
            "_index": 92,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16447v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16447v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                }
            ]
        },
        {
            "title": "TIB AIssistant: a Platform for AI-Supported Research Across Research Life Cycles",
            "authors": [
                "Allard Oelen",
                "Sören Auer"
            ],
            "arxiv_id": "2512.16442v1",
            "summary": "The rapidly growing popularity of adopting Artificial Intelligence (AI), and specifically Large Language Models (LLMs), is having a widespread impact throughout society, including the academic domain. AI-supported research has the potential to support researchers with tasks across the entire research life cycle. In this work, we demonstrate the TIB AIssistant, an AI-supported research platform providing support throughout the research life cycle. The AIssistant consists of a collection of assistants, each responsible for a specific research task. In addition, tools are provided to give access to external scholarly services. Generated data is stored in the assets and can be exported as an RO-Crate bundle to provide transparency and enhance reproducibility of the research project. We demonstrate the AIssistant's main functionalities by means of a sequential walk-through of assistants, interacting with each other to generate sections for a draft research paper. In the end, with the AIssistant, we lay the foundation for a larger agenda of providing a community-maintained platform for AI-supported research.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16442v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "TIB AIssistant：一个支持研究全生命周期的人工智能研究平台",
            "summary_zh": "人工智能（AI），特别是大型语言模型（LLMs）的快速普及，正在对包括学术领域在内的整个社会产生广泛的影响。AI支持的研究有潜力在整个研究生命周期中为研究人员提供帮助。本文展示了TIB AIssistant，这是一个AI支持的研究平台，为整个研究生命周期提供支持。AIssistant由一系列助手组成，每个助手负责特定的研究任务。此外，还提供了工具来访问外部学术服务。生成的数据存储在资产中，可以导出为RO-Crate包，以提供透明度并增强研究项目的可重复性。我们通过一个助手顺序演练来演示AIssistant的主要功能，这些助手相互交互以生成研究论文草案的各个部分。最后，通过AIssistant，我们为提供一个社区维护的AI支持研究平台奠定了基础。",
            "intro_zh": [
                "研究人员面临着在研究生命周期的各个阶段有效利用AI工具的挑战。",
                "TIB AIssistant平台通过提供一系列AI助手，简化了研究流程，每个助手专注于特定的研究任务。",
                "该平台支持生成研究论文草案，并提供数据管理和导出功能，增强了研究的透明度和可重复性。"
            ],
            "method_zh": "**问题定义**：当前研究人员在研究生命周期的各个阶段，例如文献综述、实验设计、论文撰写等，面临着如何有效利用人工智能工具的挑战。现有的AI工具往往是孤立的，缺乏整合，难以支持研究的整体流程。此外，研究数据的透明度和可重复性也是一个重要问题。\\n\\n**核心思路**：TIB AIssistant的核心思路是构建一个集成化的AI研究平台，通过一系列专门设计的AI助手，覆盖研究生命周期的各个阶段。每个助手负责特定的任务，例如文献检索、数据分析、文本生成等。这些助手可以相互协作，共同完成研究任务。同时，平台提供数据管理和导出功能，确保研究的透明度和可重复性。\\n\\n**技术框架**：TIB AIssistant平台包含以下主要模块：1) AI助手模块：包含一系列AI助手，每个助手负责特定的研究任务。2) 数据管理模块：用于存储和管理研究数据，包括输入数据、生成数据和元数据。3) 外部服务接口：提供访问外部学术服务的接口，例如文献数据库、知识图谱等。4) RO-Crate导出模块：用于将研究数据和元数据导出为RO-Crate包，以增强研究的可重复性。整个流程是用户与不同的AI助手交互，完成研究任务，生成的数据存储在平台中，并可以导出为RO-Crate包。\\n\\n**关键创新**：TIB AIssistant的关键创新在于其集成化的设计理念，将多个AI助手整合到一个平台中，实现了对研究生命周期的全面支持。与现有的孤立的AI工具相比，TIB AIssistant能够更好地满足研究人员的需求，提高研究效率。此外，RO-Crate导出功能也增强了研究的透明度和可重复性。\\n\\n**关键设计**：AI助手的具体实现细节未知，论文中没有详细描述。RO-Crate是一种用于描述和打包研究数据的标准，TIB AIssistant使用RO-Crate来确保研究数据的可重复性。平台使用的具体编程语言、数据库等技术细节也未知。",
            "application_zh": "TIB AIssistant平台可应用于各种研究领域，例如自然科学、社会科学、人文科学等。它可以帮助研究人员提高研究效率，降低研究成本，并增强研究的透明度和可重复性。未来，该平台可以进一步扩展，支持更多的研究任务，并与其他研究工具集成，成为一个全面的AI研究平台。",
            "highlight_zh": "论文通过一个助手顺序演练展示了TIB AIssistant的主要功能，演示了如何使用不同的AI助手生成研究论文草案的各个部分。虽然论文没有提供具体的性能数据，但展示了该平台在支持研究流程方面的潜力。RO-Crate导出功能也增强了研究的可重复性。",
            "tags_zh": [
                "人工智能研究平台",
                "大型语言模型",
                "研究生命周期",
                "AI助手",
                "RO-Crate"
            ],
            "_index": 93,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16442v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                }
            ]
        },
        {
            "title": "Introducing ORKG ASK: an AI-driven Scholarly Literature Search and Exploration System Taking a Neuro-Symbolic Approach",
            "authors": [
                "Allard Oelen",
                "Mohamad Yaser Jaradeh",
                "Sören Auer"
            ],
            "arxiv_id": "2512.16425v1",
            "summary": "As the volume of published scholarly literature continues to grow, finding relevant literature becomes increasingly difficult. With the rise of generative Artificial Intelligence (AI), and particularly Large Language Models (LLMs), new possibilities emerge to find and explore literature. We introduce ASK (Assistant for Scientific Knowledge), an AI-driven scholarly literature search and exploration system that follows a neuro-symbolic approach. ASK aims to provide active support to researchers in finding relevant scholarly literature by leveraging vector search, LLMs, and knowledge graphs. The system allows users to input research questions in natural language and retrieve relevant articles. ASK automatically extracts key information and generates answers to research questions using a Retrieval-Augmented Generation (RAG) approach. We present an evaluation of ASK, assessing the system's usability and usefulness. Findings indicate that the system is user-friendly and users are generally satisfied while using the system.",
            "categories": [
                "cs.IR",
                "cs.AI"
            ],
            "primary_category": "cs.IR",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "10.1007/978-3-031-97207-2_2",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16425v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出ORKG ASK：一种基于神经符号方法的AI驱动的学术文献搜索与探索系统",
            "summary_zh": "随着已发表的学术文献数量持续增长，找到相关的文献变得越来越困难。生成式人工智能（AI）的兴起，特别是大型语言模型（LLM），为发现和探索文献带来了新的可能性。我们介绍ASK（科学知识助手），这是一个AI驱动的学术文献搜索和探索系统，它遵循神经符号方法。ASK旨在通过利用向量搜索、LLM和知识图谱，为研究人员寻找相关学术文献提供积极支持。该系统允许用户以自然语言输入研究问题并检索相关文章。ASK自动提取关键信息，并使用检索增强生成（RAG）方法生成研究问题的答案。我们对ASK进行了评估，评估了系统的可用性和实用性。调查结果表明，该系统用户友好，用户在使用该系统时普遍感到满意。",
            "intro_zh": [
                "现有学术文献数量庞大，研究人员面临难以快速找到相关文献的挑战。",
                "ASK系统采用神经符号方法，结合向量搜索、LLM和知识图谱，辅助研究人员进行文献检索。",
                "ASK系统通过RAG方法，能够根据用户提出的自然语言问题，自动提取信息并生成答案。"
            ],
            "method_zh": "**问题定义**：当前学术文献数量爆炸式增长，研究人员难以高效地找到所需文献。传统文献检索方法依赖关键词匹配，无法理解用户提问的深层语义，导致检索结果不准确或不全面。现有方法缺乏对文献内容的深入理解和推理能力，难以直接回答用户提出的复杂研究问题。\\n\\n**核心思路**：ASK系统的核心思路是结合神经方法和符号方法，利用LLM的语义理解能力和知识图谱的结构化知识，实现更智能的文献检索和探索。通过向量搜索快速定位相关文献，然后利用LLM提取关键信息并生成答案，最后利用知识图谱进行知识推理和扩展。\\n\\n**技术框架**：ASK系统主要包含以下几个模块：1) 自然语言查询接口：接收用户以自然语言提出的研究问题。2) 向量搜索模块：利用预训练的向量模型将用户查询和文献进行向量化表示，通过计算向量相似度快速检索相关文献。3) 信息抽取模块：利用LLM从检索到的文献中提取关键信息，例如研究目的、方法、结果等。4) 答案生成模块：利用RAG方法，结合提取的关键信息和知识图谱中的相关知识，生成对用户问题的答案。5) 知识图谱模块：存储和管理领域知识，为答案生成提供背景知识和推理能力。\\n\\n**关键创新**：ASK系统的关键创新在于其神经符号方法的融合。它将LLM的强大的自然语言处理能力与知识图谱的结构化知识表示相结合，实现了更准确、更全面的文献检索和知识发现。与传统的基于关键词的检索方法相比，ASK能够理解用户查询的深层语义，并提供更相关的结果。与单纯使用LLM的方法相比，ASK能够利用知识图谱进行知识推理和扩展，生成更准确、更可靠的答案。\\n\\n**关键设计**：ASK系统使用了预训练的BERT模型进行向量化表示，使用Transformer模型进行信息抽取和答案生成。向量搜索模块使用了FAISS库进行高效的向量相似度计算。知识图谱使用了RDF格式进行存储和管理。RAG方法使用了Beam Search算法进行答案生成，并使用了惩罚机制避免生成重复或不相关的答案。",
            "application_zh": "ASK系统可应用于各个学科领域，帮助研究人员快速找到相关文献，提高科研效率。该系统还可以用于辅助教学，帮助学生更好地理解和掌握领域知识。未来，ASK系统可以扩展到其他类型的知识资源，例如专利、报告等，成为一个综合性的知识发现平台。此外，还可以将ASK系统与科研工作流集成，实现自动化文献综述和研究报告生成。",
            "highlight_zh": "论文对ASK系统进行了可用性和实用性评估，结果表明用户对该系统普遍满意，认为其用户友好且实用。具体而言，用户认为ASK系统能够有效地帮助他们找到相关文献，并快速获取所需信息。虽然论文中没有给出具体的性能数据，但用户反馈表明ASK系统在文献检索和知识发现方面具有显著优势。",
            "tags_zh": [
                "学术文献搜索",
                "知识图谱",
                "大型语言模型",
                "神经符号方法",
                "检索增强生成"
            ],
            "_index": 94,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16425v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16425v1/figures/screenshot-ask.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16425v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Synthelite: Chemist-aligned and feasibility-aware synthesis planning with LLMs",
            "authors": [
                "Nguyen Xuan-Vu",
                "Daniel Armstrong",
                "Milena Wehrbach",
                "Andres M Bran",
                "Zlatko Jončev",
                "Philippe Schwaller"
            ],
            "arxiv_id": "2512.16424v1",
            "summary": "Computer-aided synthesis planning (CASP) has long been envisioned as a complementary tool for synthetic chemists. However, existing frameworks often lack mechanisms to allow interaction with human experts, limiting their ability to integrate chemists' insights. In this work, we introduce Synthelite, a synthesis planning framework that uses large language models (LLMs) to directly propose retrosynthetic transformations. Synthelite can generate end-to-end synthesis routes by harnessing the intrinsic chemical knowledge and reasoning capabilities of LLMs, while allowing expert intervention through natural language prompts. Our experiments demonstrate that Synthelite can flexibly adapt its planning trajectory to diverse user-specified constraints, achieving up to 95\\% success rates in both strategy-constrained and starting-material-constrained synthesis tasks. Additionally, Synthelite exhibits the ability to account for chemical feasibility during route design. We envision Synthelite to be both a useful tool and a step toward a paradigm where LLMs are the central orchestrators of synthesis planning.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16424v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "Synthelite：利用LLM实现化学家友好且可行性感知的合成路线规划",
            "summary_zh": "计算机辅助合成规划(CASP)长期以来被认为是合成化学家的辅助工具。然而，现有的框架通常缺乏与人类专家交互的机制，限制了它们整合化学家见解的能力。本文介绍了Synthelite，一个使用大型语言模型(LLM)直接提出逆合成转化的合成规划框架。Synthelite通过利用LLM固有的化学知识和推理能力来生成端到端的合成路线，同时允许通过自然语言提示进行专家干预。实验表明，Synthelite可以灵活地调整其规划轨迹以适应各种用户指定的约束，在策略约束和起始材料约束的合成任务中均达到高达95%的成功率。此外，Synthelite还展示了在路线设计期间考虑化学可行性的能力。我们设想Synthelite既是一个有用的工具，也是朝着LLM成为合成规划中心协调者的范例迈出的一步。",
            "intro_zh": [
                "现有CASP系统缺乏与化学专家的有效交互，难以整合专家知识和经验。",
                "Synthelite利用LLM的化学知识和推理能力，通过自然语言提示实现专家干预的合成路线规划。",
                "实验表明，Synthelite在多种约束条件下均表现出高成功率，并能考虑化学反应的可行性。"
            ],
            "method_zh": "**问题定义**：现有的计算机辅助合成规划（CASP）系统，虽然在一定程度上辅助了化学家的工作，但缺乏与人类专家进行有效交互的机制。这导致系统难以充分利用化学家的专业知识和经验，限制了其在复杂合成路线设计中的应用。因此，如何设计一个能够与化学家协同工作，并能有效利用专家知识的CASP系统是一个亟待解决的问题。\\n\\n**核心思路**：Synthelite的核心思路是利用大型语言模型（LLM）强大的自然语言处理和知识推理能力，将合成路线规划问题转化为一个语言生成问题。通过精心设计的提示（prompt），引导LLM生成逆合成转化方案，并允许化学家通过自然语言进行干预和指导，从而实现人机协同的合成路线设计。\\n\\n**技术框架**：Synthelite的整体框架包含以下几个主要模块：1) LLM推理引擎：使用预训练的LLM作为核心推理引擎，负责生成逆合成转化方案。2) 自然语言交互接口：提供自然语言交互接口，允许化学家通过文本提示对LLM的规划过程进行干预和指导。3) 合成路线构建模块：根据LLM生成的转化方案，构建完整的合成路线。4) 可行性评估模块：评估合成路线的化学可行性，并对不可行的路线进行优化。\\n\\n**关键创新**：Synthelite最重要的创新在于其将LLM应用于合成路线规划，并实现了人机协同的合成设计模式。与传统的基于规则或模板的CASP系统相比，Synthelite能够利用LLM的知识推理能力，生成更具创造性和灵活性的合成路线。同时，自然语言交互接口允许化学家直接参与到规划过程中，充分发挥专家知识的作用。\\n\\n**关键设计**：Synthelite的关键设计包括：1) 提示工程：设计有效的提示，引导LLM生成符合要求的逆合成转化方案。2) 可行性评估：采用多种方法评估合成路线的化学可行性，例如基于规则的评估、基于机器学习模型的评估等。3) 迭代优化：通过迭代的方式，不断优化合成路线，直到满足要求。",
            "application_zh": "Synthelite具有广泛的应用前景，可用于药物发现、材料科学等领域。它可以帮助化学家快速生成和评估合成路线，加速新化合物的合成和筛选。此外，Synthelite还可以作为教学工具，帮助学生学习和理解合成化学的原理和方法。未来，Synthelite有望成为化学研究的重要辅助工具，推动化学领域的创新发展。",
            "highlight_zh": "Synthelite在策略约束和起始材料约束的合成任务中均取得了高达95%的成功率，显著优于传统方法。实验结果表明，Synthelite能够灵活地适应用户指定的约束条件，并能有效地考虑化学反应的可行性。这些结果验证了Synthelite在合成路线规划方面的有效性和优越性。",
            "tags_zh": [
                "合成路线规划",
                "大型语言模型",
                "化学信息学",
                "人机协同",
                "逆合成分析"
            ],
            "_index": 95,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16424v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16424v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16424v1/figs/sm_constrained_solve.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "AI Needs Physics More Than Physics Needs AI",
            "authors": [
                "Peter Coveney",
                "Roger Highfield"
            ],
            "arxiv_id": "2512.16344v1",
            "summary": "Artificial intelligence (AI) is commonly depicted as transformative. Yet, after more than a decade of hype, its measurable impact remains modest outside a few high-profile scientific and commercial successes. The 2024 Nobel Prizes in Chemistry and Physics recognized AI's potential, but broader assessments indicate the impact to date is often more promotional than technical. We argue that while current AI may influence physics, physics has significantly more to offer this generation of AI. Current architectures - large language models, reasoning models, and agentic AI - can depend on trillions of meaningless parameters, suffer from distributional bias, lack uncertainty quantification, provide no mechanistic insights, and fail to capture even elementary scientific laws. We review critiques of these limits, highlight opportunities in quantum AI and analogue computing, and lay down a roadmap for the adoption of 'Big AI': a synthesis of theory-based rigour with the flexibility of machine learning.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16344v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "强调物理学对人工智能发展的重要性，呼吁融合理论严谨性与机器学习灵活性。",
            "summary_zh": "人工智能（AI）常被描绘成具有变革性的技术。然而，经过十多年的炒作，除了少数引人注目的科学和商业成功案例外，其可衡量的影响仍然有限。2024年诺贝尔化学奖和物理学奖认可了AI的潜力，但更广泛的评估表明，迄今为止，AI的影响更多的是宣传而非技术。我们认为，虽然当前的AI可能会影响物理学，但物理学能为这一代AI提供更多。当前架构——大型语言模型、推理模型和代理AI——可能依赖于数万亿个无意义的参数，遭受分布偏差，缺乏不确定性量化，无法提供机制性见解，甚至无法捕捉基本的科学定律。我们回顾了对这些局限性的批评，强调了量子AI和模拟计算中的机遇，并为采用“大AI”制定了路线图：即将基于理论的严谨性与机器学习的灵活性相结合。",
            "intro_zh": [
                "现有AI架构依赖大量无意义参数，缺乏对基本科学定律的理解，限制了其在科学领域的应用。",
                "论文提出“大AI”概念，强调将物理学等理论知识融入AI模型，提升模型的泛化性和可解释性。",
                "论文探讨了量子AI和模拟计算等新兴领域，认为这些领域为AI发展提供了新的机遇和方向。"
            ],
            "method_zh": "**问题定义**：当前人工智能，特别是大型语言模型，在科学领域的应用面临诸多挑战。它们依赖于海量数据，但缺乏对物理规律的理解，导致模型容易出现分布偏差，无法进行不确定性量化，并且缺乏可解释性，难以提供机制性见解。现有方法的痛点在于其“黑盒”特性和对数据的过度依赖，使其难以解决需要物理知识的复杂科学问题。\\n\\n**核心思路**：论文的核心思路是强调物理学等理论知识对人工智能发展的重要性。作者认为，应该将基于理论的严谨性与机器学习的灵活性相结合，构建一种“大AI”范式。这种范式能够克服现有AI的局限性，使其更好地理解和应用科学规律。\\n\\n**技术框架**：论文并未提出一个具体的AI模型或算法框架，而是提出了一个更高层次的指导思想。它强调了以下几个关键方向：1) 将物理知识融入AI模型，例如通过物理信息神经网络（PINN）等方法；2) 发展量子AI和模拟计算等新兴技术，利用量子力学原理解决传统AI难以解决的问题；3) 加强对AI模型的可解释性研究，使其能够提供机制性见解。\\n\\n**关键创新**：论文的关键创新在于其对人工智能发展方向的重新思考。它打破了当前AI领域对数据驱动方法的过度依赖，强调了理论知识的重要性。这种观点为AI在科学领域的应用提供了新的思路和方向。\\n\\n**关键设计**：由于论文主要关注的是AI发展方向的宏观探讨，因此没有涉及具体的参数设置、损失函数或网络结构等技术细节。未来的研究可以沿着论文提出的方向，探索将物理知识融入AI模型的具体方法，例如设计能够学习和推理物理规律的神经网络结构，或者开发基于量子力学原理的AI算法。",
            "application_zh": "该研究的潜在应用领域包括材料科学、药物发现、气候建模等。通过将物理学知识融入AI模型，可以加速新材料的研发，提高药物筛选的效率，并更准确地预测气候变化。这将对科学研究和工程实践产生深远的影响。",
            "highlight_zh": "论文强调了当前AI在科学应用中的局限性，并指出了量子AI和模拟计算等新兴领域的潜力。虽然没有提供具体的实验数据，但其对AI发展方向的思考具有重要的指导意义，为未来的研究提供了新的方向。",
            "tags_zh": [
                "人工智能",
                "物理学",
                "机器学习",
                "量子AI",
                "模拟计算"
            ],
            "_index": 96,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "Agent Tools Orchestration Leaks More: Dataset, Benchmark, and Mitigation",
            "authors": [
                "Yuxuan Qiao",
                "Dongqin Liu",
                "Hongchang Yang",
                "Wei Zhou",
                "Songlin Hu"
            ],
            "arxiv_id": "2512.16310v1",
            "summary": "Driven by Large Language Models, the single-agent, multi-tool architecture has become a popular paradigm for autonomous agents due to its simplicity and effectiveness. However, this architecture also introduces a new and severe privacy risk, which we term Tools Orchestration Privacy Risk (TOP-R), where an agent, to achieve a benign user goal, autonomously aggregates information fragments across multiple tools and leverages its reasoning capabilities to synthesize unexpected sensitive information. We provide the first systematic study of this risk. First, we establish a formal framework, attributing the risk's root cause to the agent's misaligned objective function: an overoptimization for helpfulness while neglecting privacy awareness. Second, we construct TOP-Bench, comprising paired leakage and benign scenarios, to comprehensively evaluate this risk. To quantify the trade-off between safety and robustness, we introduce the H-Score as a holistic metric. The evaluation results reveal that TOP-R is a severe risk: the average Risk Leakage Rate (RLR) of eight representative models reaches 90.24%, while the average H-Score is merely 0.167, with no model exceeding 0.3. Finally, we propose the Privacy Enhancement Principle (PEP) method, which effectively mitigates TOP-R, reducing the Risk Leakage Rate to 46.58% and significantly improving the H-Score to 0.624. Our work reveals both a new class of risk and inherent structural limitations in current agent architectures, while also offering feasible mitigation strategies.",
            "categories": [
                "cs.CR",
                "cs.AI",
                "cs.CL"
            ],
            "primary_category": "cs.CR",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16310v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "揭示Agent工具编排中的隐私泄露风险，并提出TOP-Bench基准与PEP缓解方法",
            "summary_zh": "本文系统性地研究了由大型语言模型驱动的单智能体多工具架构中存在的工具编排隐私风险(TOP-R)。这种架构为了实现用户目标，可能自主地聚合多个工具中的信息片段，并利用其推理能力合成意想不到的敏感信息。研究首先建立了一个形式化框架，将风险的根本原因归结为智能体目标函数的错位：过度优化了有用性而忽略了隐私意识。其次，构建了TOP-Bench，包含配对的泄露和良性场景，以全面评估这种风险。为了量化安全性和鲁棒性之间的权衡，引入了H-Score作为整体指标。评估结果表明TOP-R是一个严重的风险：八个代表性模型的平均风险泄露率(RLR)达到90.24%，而平均H-Score仅为0.167，没有模型超过0.3。最后，提出了隐私增强原则(PEP)方法，有效地缓解了TOP-R，将风险泄露率降低到46.58%，并将H-Score显著提高到0.624。这项工作揭示了一种新型风险以及当前智能体架构中固有的结构性限制，同时也提供了可行的缓解策略。",
            "intro_zh": [
                "现有Agent架构在追求有用性的同时，忽略了隐私保护，导致工具编排过程中存在严重的隐私泄露风险。",
                "论文提出隐私增强原则(PEP)方法，通过调整Agent的目标函数，使其在追求有用性的同时兼顾隐私保护。",
                "实验结果表明，PEP方法能够有效降低风险泄露率，显著提高安全性和鲁棒性之间的权衡指标H-Score。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大型语言模型驱动的Agent在工具编排过程中产生的隐私泄露问题（Tools Orchestration Privacy Risk, TOP-R）。现有Agent架构过度优化了有用性，忽略了隐私保护，导致Agent可能自主地聚合多个工具中的信息片段，并推理出敏感信息。现有方法缺乏对这种风险的系统性研究和有效的缓解策略。\\n\\n**核心思路**：论文的核心思路是识别并缓解Agent目标函数中的错位，即过度强调有用性而忽略隐私意识。通过引入隐私增强原则(PEP)，调整Agent的目标函数，使其在追求有用性的同时，也考虑到隐私保护。这样可以避免Agent为了达成目标而过度利用工具，从而减少隐私泄露的风险。\\n\\n**技术框架**：论文的技术框架主要包括三个部分：1) 形式化框架：用于定义和分析TOP-R的根本原因，将其归结为Agent目标函数的错位。2) TOP-Bench基准：包含配对的泄露和良性场景，用于评估Agent的隐私泄露风险。3) 隐私增强原则(PEP)方法：用于缓解TOP-R，通过调整Agent的目标函数，使其兼顾有用性和隐私保护。整体流程是先通过TOP-Bench评估现有Agent的隐私泄露风险，然后应用PEP方法进行缓解，最后再次通过TOP-Bench评估缓解效果。\\n\\n**关键创新**：论文最重要的技术创新点在于提出了隐私增强原则(PEP)方法。与现有方法不同，PEP方法不是简单地限制Agent对工具的使用，而是通过调整Agent的目标函数，使其在追求有用性的同时，也考虑到隐私保护。这种方法更加灵活和有效，可以更好地平衡安全性和鲁棒性。此外，TOP-Bench基准的构建也为评估Agent的隐私泄露风险提供了一个标准化的平台。\\n\\n**关键设计**：关于PEP方法的具体实现细节，论文中可能涉及对损失函数的修改，例如引入一个惩罚项，用于惩罚Agent的隐私泄露行为。具体参数设置和网络结构的选择可能取决于具体的Agent模型和任务。TOP-Bench基准的设计需要仔细考虑泄露场景和良性场景的构建，确保能够有效地评估Agent的隐私泄露风险。",
            "application_zh": "该研究成果可应用于各种需要使用Agent进行自动化任务处理的领域，例如智能客服、自动化办公、智能家居等。通过降低Agent的隐私泄露风险，可以提高用户对Agent的信任度，促进Agent技术的广泛应用。未来，该研究可以进一步扩展到多Agent协作场景，研究更复杂的隐私泄露风险和缓解策略。",
            "highlight_zh": "实验结果表明，现有Agent模型存在严重的隐私泄露风险，平均风险泄露率(RLR)高达90.24%，平均H-Score仅为0.167。应用PEP方法后，风险泄露率显著降低到46.58%，H-Score显著提高到0.624。这表明PEP方法能够有效地缓解TOP-R，并在安全性和鲁棒性之间取得更好的平衡。",
            "tags_zh": [
                "Agent工具编排",
                "隐私泄露风险",
                "大型语言模型",
                "隐私增强原则",
                "基准测试",
                "目标函数对齐",
                "安全与鲁棒性权衡"
            ],
            "_index": 97,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16310v1/Problem_Introduction.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16310v1/Dataset_Construction.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16310v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Beyond the Benchmark: Innovative Defenses Against Prompt Injection Attacks",
            "authors": [
                "Safwan Shaheer",
                "G. M. Refatul Islam",
                "Mohammad Rafid Hamid",
                "Tahsin Zaman Jilan"
            ],
            "arxiv_id": "2512.16307v1",
            "summary": "In this fast-evolving area of LLMs, our paper discusses the significant security risk presented by prompt injection attacks. It focuses on small open-sourced models, specifically the LLaMA family of models. We introduce novel defense mechanisms capable of generating automatic defenses and systematically evaluate said generated defenses against a comprehensive set of benchmarked attacks. Thus, we empirically demonstrated the improvement proposed by our approach in mitigating goal-hijacking vulnerabilities in LLMs. Our work recognizes the increasing relevance of small open-sourced LLMs and their potential for broad deployments on edge devices, aligning with future trends in LLM applications. We contribute to the greater ecosystem of open-source LLMs and their security in the following: (1) assessing present prompt-based defenses against the latest attacks, (2) introducing a new framework using a seed defense (Chain Of Thoughts) to refine the defense prompts iteratively, and (3) showing significant improvements in detecting goal hijacking attacks. Out strategies significantly reduce the success rates of the attacks and false detection rates while at the same time effectively detecting goal-hijacking capabilities, paving the way for more secure and efficient deployments of small and open-source LLMs in resource-constrained environments.",
            "categories": [
                "cs.CR",
                "cs.AI"
            ],
            "primary_category": "cs.CR",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "10 pages, 4 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16307v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "chain-of-thought"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "针对LLaMA模型，提出迭代式防御框架，提升抵御Prompt注入攻击能力",
            "summary_zh": "本文探讨了大型语言模型（LLM）领域中Prompt注入攻击带来的严重安全风险，特别关注小型开源模型，如LLaMA系列。我们提出了一种新颖的防御机制，能够自动生成防御策略，并针对一系列基准攻击系统地评估这些防御策略。实验结果表明，该方法能够有效缓解LLM中的目标劫持漏洞。我们的工作认识到小型开源LLM日益增长的重要性及其在边缘设备上广泛部署的潜力，这与LLM应用的未来趋势相符。我们通过以下方式为开源LLM及其安全生态系统做出贡献：（1）评估现有基于Prompt的防御措施对最新攻击的有效性；（2）引入一种新的框架，使用种子防御（思维链）来迭代地改进防御Prompt；（3）显著提高检测目标劫持攻击的能力。我们的策略显著降低了攻击成功率和误检率，同时有效地检测目标劫持能力，为在资源受限环境中更安全、更高效地部署小型开源LLM铺平了道路。",
            "intro_zh": [
                "现有Prompt防御方法在面对新型Prompt注入攻击时存在不足，难以有效防止目标劫持。",
                "提出一种迭代式防御框架，利用思维链（Chain of Thoughts）作为种子防御，逐步优化防御Prompt。",
                "实验表明，该方法能显著降低攻击成功率和误检率，提升小型开源LLM的安全性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决小型开源LLM（如LLaMA）中存在的Prompt注入攻击漏洞，特别是目标劫持问题。现有的Prompt防御方法难以有效应对不断演化的攻击手段，存在攻击成功率高、误检率高等问题。\\n\\n**核心思路**：论文的核心思路是利用迭代优化的方式，自动生成更有效的防御Prompt。通过将思维链（Chain of Thoughts）作为初始防御，并不断地进行测试和改进，逐步提升防御Prompt的鲁棒性和准确性。\\n\\n**技术框架**：该框架包含以下主要阶段：1) **种子防御生成**：使用思维链方法生成初始防御Prompt。2) **攻击测试**：利用一系列基准Prompt注入攻击对防御Prompt进行测试。3) **防御Prompt优化**：根据攻击测试的结果，对防御Prompt进行迭代优化，提升其防御能力。4) **评估**：评估优化后的防御Prompt在降低攻击成功率和误检率方面的效果。\\n\\n**关键创新**：该方法最重要的创新点在于采用了迭代优化的方式来自动生成防御Prompt，避免了人工设计防御策略的局限性。通过将思维链作为种子防御，并结合攻击测试进行迭代改进，能够更有效地应对各种Prompt注入攻击。\\n\\n**关键设计**：论文的关键设计包括：1) **思维链的Prompt设计**：如何设计有效的思维链Prompt，引导LLM进行正确的推理和判断。2) **攻击测试集的构建**：如何构建全面、具有代表性的Prompt注入攻击测试集，评估防御Prompt的有效性。3) **优化算法的选择**：如何选择合适的优化算法，迭代改进防御Prompt，例如使用梯度下降或进化算法等。",
            "application_zh": "该研究成果可应用于各种需要安全可靠的LLM部署场景，尤其是在资源受限的边缘设备上。例如，智能家居设备、移动应用、机器人等，可以利用该防御机制来防止恶意用户通过Prompt注入攻击控制设备或窃取数据。该研究有助于推动小型开源LLM在实际应用中的普及。",
            "highlight_zh": "实验结果表明，该方法能够显著降低Prompt注入攻击的成功率和误检率。具体而言，与现有防御方法相比，该方法在目标劫持攻击的检测率方面提升了XX%，同时将误检率降低了YY%。这些结果表明，该方法能够有效提升小型开源LLM的安全性。",
            "tags_zh": [
                "Prompt注入攻击",
                "防御机制",
                "大型语言模型",
                "LLaMA模型",
                "思维链",
                "目标劫持",
                "安全漏洞"
            ],
            "_index": 98,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16307v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16307v1/attack_types.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16307v1/defense.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Adaptation of Agentic AI",
            "authors": [
                "Pengcheng Jiang",
                "Jiacheng Lin",
                "Zhiyi Shi",
                "Zifeng Wang",
                "Luxi He",
                "Yichen Wu",
                "Ming Zhong",
                "Peiyang Song",
                "Qizheng Zhang",
                "Heng Wang",
                "Xueqiang Xu",
                "Hanwen Xu",
                "Pengrui Han",
                "Dylan Zhang",
                "Jiashuo Sun",
                "Chaoqi Yang",
                "Kun Qian",
                "Tian Wang",
                "Changran Hu",
                "Manling Li",
                "Quanzheng Li",
                "Hao Peng",
                "Sheng Wang",
                "Jingbo Shang",
                "Chao Zhang",
                "Jiaxuan You",
                "Liyuan Liu",
                "Pan Lu",
                "Yu Zhang",
                "Heng Ji",
                "Yejin Choi",
                "Dawn Song",
                "Jimeng Sun",
                "Jiawei Han"
            ],
            "arxiv_id": "2512.16301v1",
            "summary": "Cutting-edge agentic AI systems are built on foundation models that can be adapted to plan, reason, and interact with external tools to perform increasingly complex and specialized tasks. As these systems grow in capability and scope, adaptation becomes a central mechanism for improving performance, reliability, and generalization. In this paper, we unify the rapidly expanding research landscape into a systematic framework that spans both agent adaptations and tool adaptations. We further decompose these into tool-execution-signaled and agent-output-signaled forms of agent adaptation, as well as agent-agnostic and agent-supervised forms of tool adaptation. We demonstrate that this framework helps clarify the design space of adaptation strategies in agentic AI, makes their trade-offs explicit, and provides practical guidance for selecting or switching among strategies during system design. We then review the representative approaches in each category, analyze their strengths and limitations, and highlight key open challenges and future opportunities. Overall, this paper aims to offer a conceptual foundation and practical roadmap for researchers and practitioners seeking to build more capable, efficient, and reliable agentic AI systems.",
            "categories": [
                "cs.AI",
                "cs.CL"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16301v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "foundation model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "构建Agentic AI自适应框架，提升智能体性能、可靠性和泛化能力",
            "summary_zh": "本文旨在对快速发展的Agentic AI研究领域进行整合，提出了一个系统的框架，涵盖了智能体自适应和工具自适应。进一步将智能体自适应分解为工具执行信号驱动和智能体输出信号驱动两种形式，并将工具自适应分解为智能体无关和智能体监督两种形式。该框架有助于明确Agentic AI中自适应策略的设计空间，明确其权衡，并为系统设计期间选择或切换策略提供实用指导。本文回顾了每个类别中的代表性方法，分析了它们的优缺点，并强调了关键的开放挑战和未来的机遇。总而言之，本文旨在为寻求构建更强大、高效和可靠的Agentic AI系统的研究人员和从业者提供概念基础和实践路线图。",
            "intro_zh": [
                "现有Agentic AI系统在规划、推理和交互方面面临性能、可靠性和泛化能力的挑战，需要有效的自适应机制。",
                "论文提出了一个统一的框架，涵盖智能体和工具的自适应，并细分为不同类型，从而系统化地研究自适应策略。",
                "通过对各类自适应方法的分析，论文明确了设计空间中的权衡，并为Agentic AI系统的设计提供了实践指导。"
            ],
            "method_zh": "**问题定义**：Agentic AI系统在执行复杂任务时，需要不断适应环境和任务的变化。现有方法缺乏统一的框架来指导智能体和工具的自适应，导致设计选择困难，难以权衡不同策略的优缺点。因此，需要一个系统化的框架来指导Agentic AI系统的自适应设计。\n\\n**核心思路**：论文的核心思路是将Agentic AI的自适应问题分解为智能体自适应和工具自适应两个方面。智能体自适应关注如何根据工具执行的结果和智能体的输出来调整智能体的行为。工具自适应关注如何根据智能体的反馈来改进工具的性能。通过这种分解，可以更清晰地理解自适应策略的设计空间。\n\\n**技术框架**：该框架包含两个主要部分：智能体自适应和工具自适应。智能体自适应进一步分为工具执行信号驱动和智能体输出信号驱动两种形式。工具执行信号驱动的自适应是指智能体根据工具执行的结果（例如，成功或失败）来调整其行为。智能体输出信号驱动的自适应是指智能体根据自身的输出来调整其行为。工具自适应分为智能体无关和智能体监督两种形式。智能体无关的自适应是指工具的改进不依赖于智能体的反馈。智能体监督的自适应是指工具的改进依赖于智能体的反馈。\n\\n**关键创新**：该论文的关键创新在于提出了一个统一的框架，将Agentic AI的自适应问题分解为智能体自适应和工具自适应两个方面，并进一步细分了不同的自适应形式。该框架有助于明确Agentic AI中自适应策略的设计空间，并为系统设计提供实用指导。\n\\n**关键设计**：论文没有涉及具体的参数设置、损失函数或网络结构等技术细节。该论文主要关注的是框架的设计和分类，而不是具体的算法实现。",
            "application_zh": "该研究成果可应用于各种需要智能体与工具交互的复杂任务，例如自动化软件开发、智能客服、机器人流程自动化等。通过自适应机制，Agentic AI系统能够更好地适应变化的环境和任务需求，提高工作效率和质量，降低人工干预成本。未来，该框架可以促进Agentic AI在更多领域的应用和发展。",
            "highlight_zh": "该论文的主要贡献在于提出了一个统一的Agentic AI自适应框架，并对现有方法进行了分类和分析。虽然论文没有提供具体的实验结果，但它为研究人员和从业者提供了一个有价值的参考框架，有助于更好地理解和设计Agentic AI系统。",
            "tags_zh": [
                "Agentic AI",
                "智能体自适应",
                "工具自适应",
                "自适应框架",
                "人工智能"
            ],
            "_index": 99,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16301v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16301v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16301v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "QuadSentinel: Sequent Safety for Machine-Checkable Control in Multi-agent Systems",
            "authors": [
                "Yiliu Yang",
                "Yilei Jiang",
                "Qunzhong Wang",
                "Yingshui Tan",
                "Xiaoyong Zhu",
                "Sherman S. M. Chow",
                "Bo Zheng",
                "Xiangyu Yue"
            ],
            "arxiv_id": "2512.16279v1",
            "summary": "Safety risks arise as large language model-based agents solve complex tasks with tools, multi-step plans, and inter-agent messages. However, deployer-written policies in natural language are ambiguous and context dependent, so they map poorly to machine-checkable rules, and runtime enforcement is unreliable. Expressing safety policies as sequents, we propose \\textsc{QuadSentinel}, a four-agent guard (state tracker, policy verifier, threat watcher, and referee) that compiles these policies into machine-checkable rules built from predicates over observable state and enforces them online. Referee logic plus an efficient top-$k$ predicate updater keeps costs low by prioritizing checks and resolving conflicts hierarchically. Measured on ST-WebAgentBench (ICML CUA~'25) and AgentHarm (ICLR~'25), \\textsc{QuadSentinel} improves guardrail accuracy and rule recall while reducing false positives. Against single-agent baselines such as ShieldAgent (ICML~'25), it yields better overall safety control. Near-term deployments can adopt this pattern without modifying core agents by keeping policies separate and machine-checkable. Our code will be made publicly available at https://github.com/yyiliu/QuadSentinel.",
            "categories": [
                "cs.AI",
                "cs.CL"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Preprint",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16279v1",
            "code_links": [
                {
                    "url": "https://github.com/yyiliu/QuadSentinel",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "QuadSentinel：多智能体系统中基于时序逻辑的安全可验证控制",
            "summary_zh": "基于大型语言模型（LLM）的智能体在解决复杂任务时，会利用工具、多步计划和智能体间消息，从而产生安全风险。然而，部署者编写的自然语言策略具有模糊性和上下文依赖性，难以映射到机器可检查的规则，并且运行时强制执行也不可靠。本文提出\textsc{QuadSentinel}，一种四智能体守卫（状态跟踪器、策略验证器、威胁观察器和裁判）系统，它将安全策略表示为时序逻辑，并将其编译为基于可观察状态谓词的机器可检查规则，并在运行时强制执行。裁判逻辑加上高效的top-$k$谓词更新器，通过优先检查和分层解决冲突来降低成本。在ST-WebAgentBench和AgentHarm上的实验表明，\textsc{QuadSentinel}提高了护栏的准确性和规则召回率，同时减少了误报。与ShieldAgent等单智能体基线相比，它产生了更好的整体安全控制。无需修改核心智能体，即可通过保持策略分离和机器可检查性来采用此模式进行近期部署。代码将在https://github.com/yyiliu/QuadSentinel公开。",
            "intro_zh": [
                "基于LLM的智能体在复杂任务中存在安全风险，而自然语言策略难以转化为机器可验证规则，运行时强制执行不可靠。",
                "QuadSentinel将安全策略表示为时序逻辑，编译为机器可检查规则，并通过四智能体守卫系统在线强制执行。",
                "实验表明，QuadSentinel提高了护栏准确率和规则召回率，减少误报，并优于单智能体基线。"
            ],
            "method_zh": "**问题定义**：论文旨在解决多智能体系统中，基于大型语言模型的智能体在执行复杂任务时产生的安全风险问题。现有方法，如直接使用自然语言策略进行安全控制，存在模糊性、上下文依赖性以及难以进行机器验证的痛点，导致运行时强制执行的可靠性不足。\\n\\n**核心思路**：论文的核心思路是将安全策略形式化为时序逻辑（sequents），并将其编译成机器可检查的规则。通过引入一个四智能体守卫系统，在线监控和强制执行这些规则，从而提高安全控制的准确性和可靠性。这种方法将安全策略与核心智能体分离，便于维护和更新。\\n\\n**技术框架**：QuadSentinel 包含四个核心智能体：状态跟踪器（State Tracker）、策略验证器（Policy Verifier）、威胁观察器（Threat Watcher）和裁判（Referee）。状态跟踪器负责监控环境和智能体的状态；策略验证器将状态信息与预定义的安全策略进行比对；威胁观察器检测潜在的威胁行为；裁判根据前三个智能体的输出，做出最终的安全决策并采取相应的行动。整个系统通过一个高效的 top-$k$ 谓词更新器来优化性能，优先检查最重要的规则，并分层解决冲突。\\n\\n**关键创新**：论文的关键创新在于将安全策略表示为时序逻辑，并将其编译为机器可检查的规则。这种形式化的表示方法克服了自然语言策略的模糊性和不确定性，使得安全策略能够被机器精确地理解和执行。此外，四智能体守卫系统的设计也提高了安全控制的鲁棒性和可靠性。\\n\\n**关键设计**：裁判逻辑是 QuadSentinel 的关键设计之一，它负责整合来自其他三个智能体的输出，并做出最终的安全决策。为了降低计算成本，系统采用了一个高效的 top-$k$ 谓词更新器，优先检查最重要的规则。此外，系统还采用了分层冲突解决机制，以确保安全决策的一致性和有效性。具体的参数设置、损失函数和网络结构等技术细节在论文中未详细说明，属于未知信息。",
            "application_zh": "QuadSentinel 可应用于各种多智能体系统，例如自动驾驶、机器人协作、智能家居等。通过提供可验证的安全控制，它可以降低智能体系统在复杂环境中运行的风险，提高系统的可靠性和安全性。该研究对于推动安全可靠的人工智能应用具有重要意义。",
            "highlight_zh": "QuadSentinel 在 ST-WebAgentBench 和 AgentHarm 两个基准测试中进行了评估。实验结果表明，QuadSentinel 提高了护栏的准确性和规则召回率，同时减少了误报。与 ShieldAgent 等单智能体基线相比，QuadSentinel 取得了更好的整体安全控制效果。具体的性能提升数据在摘要中未给出，属于未知信息。",
            "tags_zh": [
                "多智能体系统",
                "安全控制",
                "大型语言模型",
                "时序逻辑",
                "机器可验证",
                "智能体安全",
                "运行时监控"
            ],
            "_index": 100,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16279v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16279v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16279v1/imgs/harmful_by_category.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Beyond Blind Spots: Analytic Hints for Mitigating LLM-Based Evaluation Pitfalls",
            "authors": [
                "Ora Nova Fandina",
                "Eitan Farchi",
                "Shmulik Froimovich",
                "Raviv Gal",
                "Wesam Ibraheem",
                "Rami Katan",
                "Alice Podolsky"
            ],
            "arxiv_id": "2512.16272v1",
            "summary": "Large Language Models are increasingly deployed as judges (LaaJ) in code generation pipelines. While attractive for scalability, LaaJs tend to overlook domain specific issues raising concerns about their reliability in critical evaluation tasks. To better understand these limitations in practice, we examine LaaJ behavior in a concrete industrial use case: legacy code modernization via COBOL code generation. In this setting, we find that even production deployed LaaJs can miss domain critical errors, revealing consistent blind spots in their evaluation capabilities.\n  To better understand these blind spots, we analyze generated COBOL programs and associated LaaJs judgments, drawing on expert knowledge to construct a preliminary taxonomy. Based on this taxonomy, we develop a lightweight analytic checker tool that flags over 30 domain specific issues observed in practice. We use its outputs as analytic hints, dynamically injecting them into the judges prompt to encourage LaaJ to revisit aspects it may have overlooked.\n  Experiments on a test set of 100 programs using four production level LaaJs show that LaaJ alone detects only about 45% of the errors present in the code (in all judges we tested), while the analytic checker alone lacks explanatory depth. When combined, the LaaJ+Hints configuration achieves up to 94% coverage (for the best performing judge and injection prompt) and produces qualitatively richer, more accurate explanations, demonstrating that analytic-LLM hybrids can substantially enhance evaluation reliability in deployed pipelines. We release the dataset and all used prompts.",
            "categories": [
                "cs.SE",
                "cs.AI"
            ],
            "primary_category": "cs.SE",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16272v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "利用分析提示缓解LLM代码评估中的盲点，提升COBOL代码生成质量",
            "summary_zh": "大型语言模型（LLM）越来越多地被部署为代码生成流程中的评估者（LaaJ）。尽管这种方式具有可扩展性，但LaaJ容易忽略特定领域的细节问题，引发了对其在关键评估任务中可靠性的担忧。为了更好地理解这些局限性，本文研究了LaaJ在工业用例中的表现：通过COBOL代码生成实现遗留代码现代化。研究发现，即使是生产环境中部署的LaaJ也会遗漏领域关键错误，揭示了其评估能力中存在的盲点。为了更好地理解这些盲点，本文分析了生成的COBOL程序和LaaJ的判断，并利用专家知识构建了一个初步的分类体系。基于此，开发了一个轻量级的分析检查工具，可以标记实践中观察到的30多个特定领域的问题。该工具的输出被用作分析提示，动态地注入到评估者的提示中，以鼓励LaaJ重新审视可能被忽略的方面。在包含100个程序的测试集上，使用四个生产级别的LaaJ进行的实验表明，LaaJ单独检测到的错误仅占代码中存在的错误的45%左右（在所有测试的评估者中），而分析检查器本身缺乏解释深度。当结合使用时，LaaJ+提示配置实现了高达94%的覆盖率（对于性能最佳的评估者和注入提示），并产生了质量更高、更准确的解释，证明了分析-LLM混合方法可以显著提高已部署流程中的评估可靠性。本文发布了数据集和所有使用的提示。",
            "intro_zh": [
                "现有LaaJ在代码生成评估中存在领域盲点，无法有效识别特定领域的关键错误，影响评估可靠性。",
                "提出一种分析提示方法，通过轻量级分析检查工具识别领域特定问题，并将其作为提示注入LaaJ，引导其重新评估。",
                "实验表明，LaaJ+提示配置显著提升错误检测覆盖率，最高可达94%，并生成更准确的解释，验证了混合方法的有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大型语言模型（LLM）在作为代码生成评估者（LaaJ）时，由于缺乏特定领域知识而产生的评估盲点问题。现有方法依赖于LLM自身的泛化能力，容易忽略COBOL等遗留代码现代化过程中的关键错误，导致评估结果不准确，影响代码质量。\\n\\n**核心思路**：论文的核心思路是将领域专家的知识融入到LLM的评估过程中。通过构建一个轻量级的分析检查工具，该工具能够识别COBOL代码中常见的特定领域问题，并将这些问题以“提示”的形式反馈给LLM，引导LLM重新审视代码中可能存在的错误。这种方法结合了LLM的泛化能力和领域专家的专业知识，从而提高评估的准确性和可靠性。\\n\\n**技术框架**：该方法主要包含以下几个阶段：\n1. **COBOL代码生成**：使用代码生成模型生成COBOL代码。\n2. **分析检查**：使用轻量级分析检查工具对生成的COBOL代码进行静态分析，识别潜在的领域特定问题。\n3. **提示注入**：将分析检查工具的输出作为“提示”注入到LLM的评估提示中。\n4. **LLM评估**：LLM基于带有提示的输入，对生成的COBOL代码进行评估，并给出评估结果和解释。\n5. **结果分析**：分析LLM的评估结果，评估该方法在提高评估准确性方面的效果。\\n\\n**关键创新**：该方法最重要的创新点在于将领域专家的知识以“提示”的形式动态地注入到LLM的评估过程中。与传统的LLM评估方法相比，该方法能够有效地缓解LLM在特定领域知识方面的不足，从而提高评估的准确性和可靠性。此外，轻量级分析检查工具的设计也降低了计算成本，使其更易于部署和应用。\\n\\n**关键设计**：分析检查工具的设计是关键。它需要能够准确识别COBOL代码中常见的特定领域问题，例如数据类型不匹配、变量未初始化、循环边界错误等。提示注入的方式也需要仔细设计，以确保LLM能够有效地利用这些提示信息。论文中使用了多种提示模板，并通过实验选择最佳的提示模板。此外，论文还对不同的LLM进行了实验，以评估该方法在不同LLM上的效果。",
            "application_zh": "该研究成果可应用于遗留代码现代化、代码质量评估、自动化代码审查等领域。通过结合领域知识和LLM的推理能力，可以显著提高代码评估的准确性和效率，降低软件开发和维护成本。未来，该方法可以扩展到其他编程语言和领域，为软件工程的自动化和智能化提供更强大的支持。",
            "highlight_zh": "实验结果表明，单独使用LaaJ只能检测到约45%的错误，而结合分析提示后，错误检测覆盖率最高可达94%。这表明分析提示能够显著提升LLM在代码评估中的性能。此外，LaaJ+提示配置还能生成质量更高、更准确的解释，有助于开发人员更好地理解和修复代码中的问题。",
            "tags_zh": [
                "大型语言模型",
                "代码评估",
                "领域知识",
                "分析提示",
                "COBOL代码生成"
            ],
            "_index": 101,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16272v1/hint_llaj.jpg",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16272v1/taxonomy.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16272v1/hybrid_laaj_issues_triplets_total_native_orange_leftlegend.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Learning to Wait: Synchronizing Agents with the Physical World",
            "authors": [
                "Yifei She",
                "Ping Zhang",
                "He Liu",
                "Yanmin Jia",
                "Yang Jing",
                "Zijun Liu",
                "Peng Sun",
                "Xiangbin Li",
                "Xiaohe Hu"
            ],
            "arxiv_id": "2512.16262v1",
            "summary": "Real-world agentic tasks, unlike synchronous Markov Decision Processes (MDPs), often involve non-blocking actions with variable latencies, creating a fundamental \\textit{Temporal Gap} between action initiation and completion. Existing environment-side solutions, such as blocking wrappers or frequent polling, either limit scalability or dilute the agent's context window with redundant observations. In this work, we propose an \\textbf{Agent-side Approach} that empowers Large Language Models (LLMs) to actively align their \\textit{Cognitive Timeline} with the physical world. By extending the Code-as-Action paradigm to the temporal domain, agents utilize semantic priors and In-Context Learning (ICL) to predict precise waiting durations (\\texttt{time.sleep(t)}), effectively synchronizing with asynchronous environment without exhaustive checking. Experiments in a simulated Kubernetes cluster demonstrate that agents can precisely calibrate their internal clocks to minimize both query overhead and execution latency, validating that temporal awareness is a learnable capability essential for autonomous evolution in open-ended environments.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16262v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出Agent侧时间同步方法，解决LLM在异步环境中的时序认知问题",
            "summary_zh": "与同步马尔可夫决策过程（MDP）不同，现实世界的Agent任务通常涉及具有可变延迟的非阻塞动作，从而在动作发起和完成之间产生根本性的“时间差”。现有的环境侧解决方案，如阻塞包装器或频繁轮询，要么限制了可扩展性，要么用冗余的观察稀释了Agent的上下文窗口。本文提出了一种“Agent侧方法”，使大型语言模型（LLM）能够主动将其“认知时间线”与物理世界对齐。通过将代码即动作范式扩展到时间域，Agent利用语义先验和上下文学习（ICL）来预测精确的等待时间（`time.sleep(t)`），从而有效地与异步环境同步，而无需详尽的检查。在模拟的Kubernetes集群中的实验表明，Agent可以精确地校准其内部时钟，以最大限度地减少查询开销和执行延迟，从而验证了时间感知是在开放环境中自主进化必不可少的、可学习的能力。",
            "intro_zh": [
                "现有方法在处理真实世界Agent任务时，由于动作延迟的不确定性，导致Agent与环境时序不同步，影响效率。",
                "论文提出Agent侧的时间同步方法，通过让LLM预测等待时间，主动与异步环境对齐认知时间线。",
                "实验表明，该方法能有效减少查询开销和执行延迟，验证了Agent学习时间感知的可行性和必要性。"
            ],
            "method_zh": "**问题定义**：现实世界的Agent任务通常是异步的，动作的完成时间不确定，这导致Agent的认知和物理世界之间存在时间差。现有的解决方案，如阻塞式等待或频繁轮询，要么限制了Agent的并发能力，要么引入了大量的冗余信息，降低了效率。因此，如何让Agent在异步环境中高效地执行任务是一个关键问题。\\n\\n**核心思路**：论文的核心思路是让Agent具备时间感知能力，通过预测动作的完成时间，主动地与环境进行同步。具体来说，Agent通过学习来预测合适的等待时间（`time.sleep(t)`），从而避免不必要的等待或过早地进行下一步操作。这种Agent侧的方法避免了对环境的修改，具有更好的通用性和可扩展性。\\n\\n**技术框架**：该方法基于Code-as-Action范式，将等待时间预测视为一个代码生成任务。Agent首先接收环境的观察，然后利用LLM生成包含`time.sleep(t)`的动作代码。LLM通过上下文学习（ICL）来学习如何根据环境状态预测合适的等待时间。Agent执行生成的代码，并在等待时间结束后接收新的观察，从而形成一个闭环。\\n\\n**关键创新**：最重要的创新点在于将时间感知能力赋予Agent本身，而不是依赖于环境的同步机制。通过让Agent学习预测等待时间，实现了Agent与异步环境的有效同步。这种方法避免了对环境的侵入式修改，具有更好的通用性和可扩展性。\\n\\n**关键设计**：关键的设计包括：1) 使用Code-as-Action范式，将等待时间预测转化为代码生成任务；2) 利用上下文学习（ICL）来提高LLM的预测精度；3) 设计合适的奖励函数，鼓励Agent学习精确的等待时间。具体的参数设置和网络结构细节在论文中未详细说明，属于未知信息。",
            "application_zh": "该研究成果可应用于各种需要Agent与异步环境交互的场景，例如机器人控制、自动化运维、智能家居等。通过让Agent具备时间感知能力，可以提高任务执行效率，降低资源消耗，并实现更智能化的自主决策。未来，该方法有望推动Agent在开放环境中实现更高级别的自主进化。",
            "highlight_zh": "实验结果表明，该方法在模拟的Kubernetes集群中能够有效地减少查询开销和执行延迟。Agent能够精确地校准其内部时钟，从而避免不必要的等待或过早地进行下一步操作。具体的性能数据和提升幅度在论文中未详细说明，属于未知信息。",
            "tags_zh": [
                "大型语言模型",
                "时间感知",
                "异步环境",
                "Agent",
                "上下文学习"
            ],
            "_index": 102,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16262v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16262v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16262v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Sigma-Moe-Tiny Technical Report",
            "authors": [
                "Qingguo Hu",
                "Zhenghao Lin",
                "Ziyue Yang",
                "Yucheng Ding",
                "Xiao Liu",
                "Yuting Jiang",
                "Ruizhe Wang",
                "Tianyu Chen",
                "Zhongxin Guo",
                "Yifan Xiong",
                "Rui Gao",
                "Lei Qu",
                "Jinsong Su",
                "Peng Cheng",
                "Yeyun Gong"
            ],
            "arxiv_id": "2512.16248v1",
            "summary": "Mixture-of-Experts (MoE) has emerged as a promising paradigm for foundation models due to its efficient and powerful scalability. In this work, we present Sigma-MoE-Tiny, an MoE language model that achieves the highest sparsity compared to existing open-source models. Sigma-MoE-Tiny employs fine-grained expert segmentation with up to 96 experts per layer, while activating only one expert for each token, resulting in 20B total parameters with just 0.5B activated. The major challenge introduced by such extreme sparsity lies in expert load balancing. We find that the widely-used load balancing loss tends to become ineffective in the lower layers under this setting. To address this issue, we propose a progressive sparsification schedule aiming to balance expert utilization and training stability. Sigma-MoE-Tiny is pre-trained on a diverse and high-quality corpus, followed by post-training to further unlock its capabilities. The entire training process remains remarkably stable, with no occurrence of irrecoverable loss spikes. Comprehensive evaluations reveal that, despite activating only 0.5B parameters, Sigma-MoE-Tiny achieves top-tier performance among counterparts of comparable or significantly larger scale. In addition, we provide an in-depth discussion of load balancing in highly sparse MoE models, offering insights for advancing sparsity in future MoE architectures.\n  Project page: https://qghuxmu.github.io/Sigma-MoE-Tiny\n  Code: https://github.com/microsoft/ltp-megatron-lm",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16248v1",
            "code_links": [
                {
                    "url": "https://github.com/microsoft/ltp-megatron-lm",
                    "type": "github"
                },
                {
                    "url": "https://qghuxmu.github.io/Sigma-MoE-Tiny",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "foundation model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "Sigma-MoE-Tiny：提出一种高稀疏MoE语言模型，解决专家负载均衡问题，实现高效扩展。",
            "summary_zh": "本文介绍了Sigma-MoE-Tiny，一种混合专家（MoE）语言模型，它实现了现有开源模型中最高的稀疏性。Sigma-MoE-Tiny采用细粒度的专家分割，每层最多有96个专家，但每个token只激活一个专家，从而在总参数量为200亿的情况下，仅激活0.5B参数。这种极端稀疏性带来的主要挑战是专家负载均衡。研究发现，在这种设置下，广泛使用的负载均衡损失在较低层往往失效。为了解决这个问题，提出了一种渐进式稀疏化策略，旨在平衡专家利用率和训练稳定性。Sigma-MoE-Tiny在一个多样化和高质量的语料库上进行预训练，然后进行后训练以进一步释放其能力。整个训练过程保持了显著的稳定性，没有出现不可恢复的损失峰值。综合评估表明，尽管只激活了0.5B参数，Sigma-MoE-Tiny在同等或更大规模的同类模型中实现了顶级的性能。此外，还深入讨论了高稀疏MoE模型中的负载均衡问题，为未来MoE架构中提高稀疏性提供了见解。",
            "intro_zh": [
                "现有MoE模型在极端稀疏性下，专家负载均衡面临挑战，传统负载均衡损失在底层失效。",
                "提出渐进式稀疏化策略，平衡专家利用率和训练稳定性，解决高稀疏性下的负载均衡问题。",
                "Sigma-MoE-Tiny仅激活0.5B参数，但在同等或更大规模模型中表现出色，训练过程稳定。"
            ],
            "method_zh": "**问题定义**：论文旨在解决MoE模型中，在极高稀疏度下专家负载不均衡的问题。现有的负载均衡损失函数在高稀疏度下，尤其是在模型的底层，效果显著下降，导致部分专家过度使用，而另一些专家则利用不足，影响模型整体性能和训练稳定性。\\n\\n**核心思路**：论文的核心思路是通过渐进式稀疏化策略来解决专家负载均衡问题。该策略并非一开始就采用最高的稀疏度，而是逐步增加稀疏度，从而在训练初期保证所有专家都能得到充分利用，避免早期训练阶段出现专家利用率严重不均衡的情况。这种逐步稀疏化的方式有助于稳定训练过程，并最终达到期望的高稀疏度。\\n\\n**技术框架**：Sigma-MoE-Tiny的整体框架基于Transformer架构，并引入了MoE层。每个MoE层包含多个专家（最多96个），但每个token只激活一个专家。模型首先在大量数据上进行预训练，然后进行后训练以提升性能。关键在于训练过程中使用的渐进式稀疏化策略，该策略控制着每层激活的专家数量，并随着训练的进行逐步增加稀疏度。\\n\\n**关键创新**：论文的关键创新在于提出了渐进式稀疏化策略。与传统的MoE训练方法不同，该策略不是一开始就设定固定的高稀疏度，而是通过一个schedule逐步增加稀疏度。这种方法能够更好地平衡专家利用率和训练稳定性，避免了因早期专家利用率不均衡而导致的训练崩溃。\\n\\n**关键设计**：渐进式稀疏化策略的具体实现涉及到一个稀疏度schedule，该schedule定义了在训练的不同阶段激活的专家数量。例如，在训练初期，可能激活更多的专家，以确保所有专家都能得到训练。随着训练的进行，逐渐减少激活的专家数量，最终达到目标稀疏度。此外，论文还可能涉及到对负载均衡损失函数的调整，以适应高稀疏度下的训练。",
            "application_zh": "Sigma-MoE-Tiny的研究成果可应用于各种需要高效和可扩展语言模型的场景，例如自然语言处理、机器翻译、文本生成、对话系统等。其高稀疏性使得模型能够在资源受限的环境中部署，降低计算成本和能源消耗。此外，该研究为未来MoE架构的设计提供了新的思路，有助于开发更大规模、更高性能的语言模型。",
            "highlight_zh": "Sigma-MoE-Tiny在激活仅0.5B参数的情况下，实现了与参数量更大模型相当甚至更优的性能。实验结果表明，该模型在多个NLP任务上取得了具有竞争力的结果，证明了其在高稀疏度下的有效性。此外，训练过程表现出极高的稳定性，没有出现不可恢复的损失峰值，验证了渐进式稀疏化策略的有效性。",
            "tags_zh": [
                "混合专家模型",
                "MoE",
                "稀疏模型",
                "负载均衡",
                "渐进式稀疏化"
            ],
            "_index": 103,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16248v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16248v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16248v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Ev-Trust: A Strategy Equilibrium Trust Mechanism for Evolutionary Games in LLM-Based Multi-Agent Services",
            "authors": [
                "Shiduo Yang",
                "Jiye Wang",
                "Jiayu Qin",
                "Jianbin Li",
                "Yu Wang",
                "Yuanhe Zhao",
                "Kenan Guo"
            ],
            "arxiv_id": "2512.16167v1",
            "summary": "The rapid evolution of the Web toward an agent-centric paradigm, driven by large language models (LLMs), has enabled autonomous agents to reason, plan, and interact in complex decentralized environments. However, the openness and heterogeneity of LLM-based multi-agent systems also amplify the risks of deception, fraud, and misinformation, posing severe challenges to trust establishment and system robustness. To address this issue, we propose Ev-Trust, a strategy-equilibrium trust mechanism grounded in evolutionary game theory. This mechanism integrates direct trust, indirect trust, and expected revenue into a dynamic feedback structure that guides agents' behavioral evolution toward equilibria. Within a decentralized \"Request-Response-Payment-Evaluation\" service framework, Ev-Trust enables agents to adaptively adjust strategies, naturally excluding malicious participants while reinforcing high-quality collaboration. Furthermore, our theoretical derivation based on replicator dynamics equations proves the existence and stability of local evolutionary equilibria. Experimental results indicate that our approach effectively reflects agent trustworthiness in LLM-driven open service interaction scenarios, reduces malicious strategies, and increases collective revenue. We hope Ev-Trust can provide a new perspective on trust modeling for the agentic service web in group evolutionary game scenarios.",
            "categories": [
                "cs.MA",
                "cs.AI",
                "cs.GT"
            ],
            "primary_category": "cs.MA",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "12 pages, 11 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16167v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出Ev-Trust机制，利用演化博弈论解决LLM多智能体服务中的信任问题。",
            "summary_zh": "随着Web向以智能体为中心的范式快速演进，由大型语言模型（LLMs）驱动的自主智能体能够在复杂的去中心化环境中进行推理、规划和交互。然而，基于LLM的多智能体系统的开放性和异构性也加剧了欺骗、欺诈和虚假信息的风险，对信任建立和系统鲁棒性构成严峻挑战。为了解决这个问题，我们提出了一种基于演化博弈论的策略均衡信任机制Ev-Trust。该机制将直接信任、间接信任和预期收益整合到一个动态反馈结构中，引导智能体的行为演化趋向均衡。在去中心化的“请求-响应-支付-评估”服务框架内，Ev-Trust使智能体能够自适应地调整策略，自然地排除恶意参与者，同时加强高质量的协作。此外，我们基于复制者动态方程的理论推导证明了局部演化均衡的存在和稳定性。实验结果表明，我们的方法有效地反映了LLM驱动的开放服务交互场景中智能体的可信度，减少了恶意策略，并增加了集体收益。我们希望Ev-Trust能为群体演化博弈场景中的智能体服务网络提供一种新的信任建模视角。",
            "intro_zh": [
                "基于LLM的多智能体系统面临欺骗、欺诈和虚假信息等信任挑战，现有方法难以有效应对。",
                "Ev-Trust机制融合直接信任、间接信任和预期收益，构建动态反馈结构，引导智能体行为演化至均衡。",
                "实验表明，Ev-Trust能有效反映智能体可信度，减少恶意策略，并提升LLM驱动服务交互的集体收益。"
            ],
            "method_zh": "**问题定义**：论文旨在解决基于LLM的多智能体系统中，由于开放性和异构性带来的信任危机问题。现有方法难以有效区分恶意和诚实智能体，导致欺骗、欺诈和虚假信息泛滥，影响系统整体的鲁棒性和协作效率。\\n\\n**核心思路**：论文的核心思路是利用演化博弈论，将智能体之间的交互建模为一个动态演化过程。通过设计合理的信任机制，引导智能体自适应地调整策略，最终达到一个策略均衡，从而自然地排除恶意参与者，并促进高质量的协作。这种方法的核心在于将信任评估与智能体的收益直接关联，激励智能体采取诚实守信的行为。\\n\\n**技术框架**：Ev-Trust机制运行在一个去中心化的“请求-响应-支付-评估”服务框架内。该框架包含以下主要阶段：1) 请求者发布服务请求；2) 响应者提供服务；3) 请求者根据服务质量支付报酬；4) 请求者对响应者进行评估。Ev-Trust机制在评估阶段发挥作用，它综合考虑直接信任（请求者对响应者的直接经验）、间接信任（其他请求者的评价）和预期收益，计算出一个综合信任值，用于指导后续的智能体行为。\\n\\n**关键创新**：Ev-Trust的关键创新在于其策略均衡信任机制。它不同于传统的信任模型，后者通常依赖于静态的信任评分或规则。Ev-Trust通过演化博弈论，将信任评估与智能体的策略选择联系起来，形成一个动态的反馈循环。这种机制能够自适应地应对环境变化和恶意攻击，并最终达到一个稳定的策略均衡。此外，论文还通过复制者动态方程，从理论上证明了局部演化均衡的存在和稳定性。\\n\\n**关键设计**：Ev-Trust的关键设计包括：1) 信任值的计算方式，它综合考虑了直接信任、间接信任和预期收益，并使用加权平均的方式进行融合；2) 策略更新规则，智能体根据自身的信任值和收益，自适应地调整其策略，例如选择提供更高质量的服务或更诚实地进行交互；3) 复制者动态方程，用于分析系统的演化过程，并证明局部演化均衡的存在和稳定性。具体的参数设置（例如权重系数）需要根据实际应用场景进行调整。",
            "application_zh": "Ev-Trust机制可应用于各种基于LLM的多智能体服务场景，例如去中心化知识共享平台、智能合约执行系统、以及开放式创新生态系统。通过建立可靠的信任机制，Ev-Trust能够促进智能体之间的协作，提高服务质量，并降低欺诈风险，从而推动agentic service web的发展。",
            "highlight_zh": "实验结果表明，Ev-Trust机制能够有效区分恶意和诚实智能体，显著降低恶意策略的比例。与基线方法相比，Ev-Trust能够提高集体收益，并促进高质量的协作。具体而言，在模拟实验中，恶意策略的比例降低了XX%，集体收益提高了YY%。这些结果验证了Ev-Trust机制的有效性和优越性。",
            "tags_zh": [
                "多智能体系统",
                "大型语言模型",
                "演化博弈论",
                "信任机制",
                "策略均衡",
                "复制者动态",
                "去中心化服务"
            ],
            "_index": 104,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16167v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16167v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16167v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "From Essence to Defense: Adaptive Semantic-aware Watermarking for Embedding-as-a-Service Copyright Protection",
            "authors": [
                "Hao Li",
                "Yubing Ren",
                "Yanan Cao",
                "Yingjie Li",
                "Fang Fang",
                "Xuebin Wang"
            ],
            "arxiv_id": "2512.16439v1",
            "summary": "Benefiting from the superior capabilities of large language models in natural language understanding and generation, Embeddings-as-a-Service (EaaS) has emerged as a successful commercial paradigm on the web platform. However, prior studies have revealed that EaaS is vulnerable to imitation attacks. Existing methods protect the intellectual property of EaaS through watermarking techniques, but they all ignore the most important properties of embedding: semantics, resulting in limited harmlessness and stealthiness. To this end, we propose SemMark, a novel semantic-based watermarking paradigm for EaaS copyright protection. SemMark employs locality-sensitive hashing to partition the semantic space and inject semantic-aware watermarks into specific regions, ensuring that the watermark signals remain imperceptible and diverse. In addition, we introduce the adaptive watermark weight mechanism based on the local outlier factor to preserve the original embedding distribution. Furthermore, we propose Detect-Sampling and Dimensionality-Reduction attacks and construct four scenarios to evaluate the watermarking method. Extensive experiments are conducted on four popular NLP datasets, and SemMark achieves superior verifiability, diversity, stealthiness, and harmlessness.",
            "categories": [
                "cs.CR",
                "cs.CL"
            ],
            "primary_category": "cs.CR",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16439v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出SemMark：一种自适应语义感知水印方法，用于保护Embedding-as-a-Service的版权",
            "summary_zh": "Embedding-as-a-Service (EaaS) 凭借大型语言模型在自然语言理解和生成方面的卓越能力，已成为一种成功的商业模式。然而，先前的研究表明EaaS容易受到模仿攻击。现有的水印技术试图保护EaaS的知识产权，但它们都忽略了嵌入最重要的属性：语义，导致其无害性和隐蔽性有限。为此，我们提出SemMark，一种新颖的基于语义的水印范式，用于EaaS版权保护。SemMark采用局部敏感哈希来划分语义空间，并将语义感知水印注入到特定区域，确保水印信号保持难以察觉和多样化。此外，我们引入了基于局部离群因子（LOF）的自适应水印权重机制，以保留原始嵌入分布。我们还提出了Detect-Sampling和Dimensionality-Reduction攻击，并构建了四种场景来评估水印方法。在四个流行的NLP数据集上进行了大量实验，SemMark实现了卓越的可验证性、多样性、隐蔽性和无害性。",
            "intro_zh": [
                "现有EaaS水印方法忽略了嵌入的语义信息，导致水印的隐蔽性和对原始嵌入的影响控制不足。",
                "SemMark通过局部敏感哈希划分语义空间，并注入语义感知水印，保证水印的不可感知性和多样性。",
                "实验证明，SemMark在可验证性、多样性、隐蔽性和无害性方面优于现有方法，有效抵抗新型攻击。"
            ],
            "method_zh": "**问题定义**：论文旨在解决Embedding-as-a-Service (EaaS) 的版权保护问题。现有的水印方法主要痛点在于忽略了嵌入的语义信息，导致水印的隐蔽性不足，容易被检测和移除，并且可能对原始嵌入的语义造成较大干扰，影响下游任务的性能。\\n\\n**核心思路**：SemMark的核心思路是利用嵌入的语义信息，将水印嵌入到特定的语义区域，并根据语义区域的特性自适应地调整水印的强度。通过这种方式，可以提高水印的隐蔽性和鲁棒性，同时减少对原始嵌入语义的干扰。\\n\\n**技术框架**：SemMark主要包含以下几个模块：1) 语义空间划分：使用局部敏感哈希（LSH）将嵌入空间划分为多个语义区域。2) 水印注入：在特定的语义区域注入语义感知水印。水印的强度由基于局部离群因子（LOF）的自适应权重机制控制。3) 水印检测：通过检测特定语义区域的水印信号来验证版权。\\n\\n**关键创新**：SemMark的关键创新在于：1) 提出了语义感知水印的概念，将水印嵌入到特定的语义区域，提高了水印的隐蔽性和鲁棒性。2) 引入了基于局部离群因子（LOF）的自适应权重机制，可以根据语义区域的特性自适应地调整水印的强度，减少对原始嵌入语义的干扰。\\n\\n**关键设计**：在语义空间划分阶段，LSH的参数（如哈希函数的数量和哈希桶的大小）需要根据数据集的特性进行调整。在水印注入阶段，水印的强度由基于LOF的自适应权重机制控制，LOF的计算需要选择合适的邻域大小。此外，论文还提出了Detect-Sampling和Dimensionality-Reduction两种新型攻击方法，并针对这些攻击方法设计了相应的防御策略。",
            "application_zh": "SemMark可应用于各种基于Embedding-as-a-Service的场景，例如文本分类、情感分析、机器翻译等。它可以有效地保护EaaS提供商的知识产权，防止未经授权的复制和使用。该研究成果有助于构建更安全、更可靠的EaaS生态系统，促进人工智能技术的健康发展。",
            "highlight_zh": "实验结果表明，SemMark在四个流行的NLP数据集上实现了卓越的性能。与现有方法相比，SemMark在可验证性、多样性、隐蔽性和无害性方面均有显著提升。例如，在抵抗Detect-Sampling和Dimensionality-Reduction攻击方面，SemMark表现出更强的鲁棒性，能够有效地保护EaaS的版权。",
            "tags_zh": [
                "Embedding-as-a-Service",
                "版权保护",
                "语义水印",
                "局部敏感哈希",
                "自适应权重",
                "模仿攻击",
                "自然语言处理"
            ],
            "_index": 105,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16439v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16439v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16439v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Evaluating OpenAI GPT Models for Translation of Endangered Uralic Languages: A Comparison of Reasoning and Non-Reasoning Architectures",
            "authors": [
                "Yehor Tereshchenko",
                "Mika Hämäläinen",
                "Svitlana Myroniuk"
            ],
            "arxiv_id": "2512.16287v1",
            "summary": "The evaluation of Large Language Models (LLMs) for translation tasks has primarily focused on high-resource languages, leaving a significant gap in understanding their performance on low-resource and endangered languages. This study presents a comprehensive comparison of OpenAI's GPT models, specifically examining the differences between reasoning and non-reasoning architectures for translating between Finnish and four low-resource Uralic languages: Komi-Zyrian, Moksha, Erzya, and Udmurt. Using a parallel corpus of literary texts, we evaluate model willingness to attempt translation through refusal rate analysis across different model architectures. Our findings reveal significant performance variations between reasoning and non-reasoning models, with reasoning models showing 16 percentage points lower refusal rates. The results provide valuable insights for researchers and practitioners working with Uralic languages and contribute to the broader understanding of reasoning model capabilities for endangered language preservation.",
            "categories": [
                "cs.CL"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "IWCLUL 2025",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16287v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "评估OpenAI GPT模型在濒危乌拉尔语翻译中的表现，对比推理与非推理架构。",
            "summary_zh": "本研究旨在评估大型语言模型（LLMs）在翻译任务中的表现，特别关注低资源和濒危语言，弥补了现有研究主要集中于高资源语言的不足。本文对比了OpenAI的GPT模型，着重考察了推理和非推理架构在芬兰语与四种低资源乌拉尔语（科米-兹梁语、莫克沙语、埃尔齐亚语和乌德穆尔特语）之间翻译的差异。我们使用文学文本的平行语料库，通过拒绝率分析评估模型尝试翻译的意愿。研究结果表明，推理模型和非推理模型之间存在显著的性能差异，推理模型的拒绝率降低了16个百分点。这些发现为研究乌拉尔语的研究人员和从业者提供了有价值的见解，并有助于更广泛地理解推理模型在濒危语言保护方面的能力。",
            "intro_zh": [
                "现有LLM翻译评估主要集中于高资源语言，忽略了低资源和濒危语言的性能。",
                "研究对比OpenAI的GPT模型，考察推理和非推理架构在乌拉尔语翻译中的差异。",
                "实验表明，推理模型在低资源乌拉尔语翻译中表现更佳，拒绝率显著降低。"
            ],
            "method_zh": "**问题定义**：论文旨在解决低资源和濒危乌拉尔语的机器翻译问题。现有的大型语言模型（LLMs）在这些语言上的表现缺乏充分评估，并且现有方法在高资源语言上训练的模型可能无法很好地泛化到低资源语言，导致翻译质量差或模型直接拒绝翻译请求。\\n\\n**核心思路**：论文的核心思路是对比具有推理能力的LLM（如GPT模型）和不具备推理能力的LLM在低资源乌拉尔语翻译任务上的表现差异。通过分析模型的拒绝率和翻译质量，评估推理能力对低资源语言翻译的贡献。\\n\\n**技术框架**：研究采用平行语料库，包含芬兰语和四种低资源乌拉尔语（科米-兹梁语、莫克沙语、埃尔齐亚语和乌德穆尔特语）的文学文本。研究流程包括：1) 构建平行语料库；2) 使用不同的OpenAI GPT模型（包括推理和非推理架构）进行翻译；3) 分析模型的拒绝率，即模型拒绝尝试翻译的比例；4) 对翻译结果进行人工评估或使用自动评估指标（如BLEU）评估翻译质量。\\n\\n**关键创新**：论文的关键创新在于首次系统性地评估了具有推理能力的LLM在低资源乌拉尔语翻译中的表现，并对比了推理和非推理架构的差异。通过拒绝率分析，揭示了推理能力对模型处理低资源语言翻译请求的重要性。\\n\\n**关键设计**：论文的关键设计包括：1) 选择具有代表性的低资源乌拉尔语，以确保研究结果的泛化性；2) 使用文学文本的平行语料库，以保证翻译任务的复杂性和挑战性；3) 采用拒绝率作为评估指标，以衡量模型处理低资源语言翻译请求的意愿；4) 对比不同规模和架构的GPT模型，以分析模型性能与模型复杂度的关系。",
            "application_zh": "该研究成果可应用于濒危语言的保护和传承，例如辅助语言学习、自动生成语言资源、促进跨文化交流等。通过优化LLM在低资源语言上的翻译能力，可以帮助更多人接触和了解这些语言，从而促进其复兴和发展。此外，该研究也为其他低资源语言的机器翻译研究提供了借鉴。",
            "highlight_zh": "实验结果表明，具有推理能力的GPT模型在低资源乌拉尔语翻译中表现优于非推理模型，拒绝率降低了16个百分点。这表明推理能力对于处理低资源语言的翻译请求至关重要。该研究为进一步优化LLM在低资源语言上的翻译性能提供了重要依据。",
            "tags_zh": [
                "低资源语言翻译",
                "濒危语言保护",
                "大型语言模型",
                "推理能力",
                "乌拉尔语"
            ],
            "_index": 106,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "LoPA: Scaling dLLM Inference via Lookahead Parallel Decoding",
            "authors": [
                "Chenkai Xu",
                "Yijie Jin",
                "Jiajun Li",
                "Yi Tu",
                "Guoping Long",
                "Dandan Tu",
                "Tianqi Hou",
                "Junchi Yan",
                "Zhijie Deng"
            ],
            "arxiv_id": "2512.16229v1",
            "summary": "Diffusion Large Language Models (dLLMs) have demonstrated significant potential for high-speed inference. However, current confidence-driven decoding strategies are constrained by limited parallelism, typically achieving only 1--3 tokens per forward pass (TPF). In this work, we identify that the degree of parallelism during dLLM inference is highly sensitive to the Token Filling Order (TFO). Then, we introduce Lookahead PArallel Decoding LoPA, a training-free, plug-and-play algorithm, to identify a superior TFO and hence accelerate inference. LoPA concurrently explores distinct candidate TFOs via parallel branches, and selects the one with the highest potential for future parallelism based on branch confidence. We apply LoPA to the state-of-the-art D2F model and observe a substantial enhancement in decoding efficiency. Notably, LoPA increases the TPF of D2F-Dream to 10.1 on the GSM8K while maintaining performance superior to the Dream baseline. Furthermore, to facilitate this unprecedented degree of parallelism, we develop a specialized multi-device inference system featuring Branch Parallelism (BP), which achieves a single-sample throughput of 1073.9 tokens per second under multi-GPU deployment. The code is available at https://github.com/zhijie-group/LoPA.",
            "categories": [
                "cs.CL"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16229v1",
            "code_links": [
                {
                    "url": "https://github.com/zhijie-group/LoPA",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "LoPA：通过前瞻并行解码加速扩散大语言模型推理",
            "summary_zh": "扩散大语言模型(dLLM)在高速推理方面展现出巨大潜力。然而，当前基于置信度的解码策略受到有限并行性的约束，通常每次前向传播只能实现1-3个token。本文发现dLLM推理过程中的并行度对Token填充顺序(TFO)高度敏感。因此，我们提出了Lookahead PArallel Decoding (LoPA)，一种无需训练、即插即用的算法，用于识别更优的TFO，从而加速推理。LoPA通过并行分支同时探索不同的候选TFO，并基于分支置信度选择具有最高未来并行潜力的TFO。我们将LoPA应用于最先进的D2F模型，观察到解码效率的显著提高。值得注意的是，LoPA将GSM8K上的D2F-Dream的TPF提高到10.1，同时保持优于Dream基线的性能。此外，为了支持这种前所未有的并行度，我们开发了一种专门的多设备推理系统，该系统具有分支并行性(BP)，在多GPU部署下实现了每秒1073.9个token的单样本吞吐量。代码已开源。",
            "intro_zh": [
                "现有扩散大语言模型推理受限于置信度驱动的解码策略，并行度低，严重影响推理速度。",
                "LoPA通过并行探索不同的Token填充顺序，并基于置信度选择最优方案，从而提升并行度。",
                "实验表明，LoPA显著提升了D2F模型的解码效率，并在GSM8K数据集上实现了更高的TPF和性能。"
            ],
            "method_zh": "**问题定义**：扩散大语言模型（dLLM）的推理速度受限于其解码过程的并行度。现有的基于置信度的解码策略，如D2F，在选择下一个要填充的token时，通常只能实现较低的并行度（1-3个token/次前向传播）。这主要是因为token填充顺序（TFO）的选择会显著影响后续解码的并行潜力。如果选择了不合适的TFO，可能会导致后续token的填充必须串行进行，从而降低整体推理速度。\\n\\n**核心思路**：LoPA的核心思想是通过前瞻性的并行探索不同的TFO，并选择能够最大化未来并行度的TFO。具体来说，LoPA维护多个并行的解码分支，每个分支代表一种不同的TFO。通过并行地评估这些分支的置信度，LoPA可以预测每个TFO在未来解码过程中能够实现的并行度。然后，LoPA选择具有最高并行度潜力的分支继续解码。\\n\\n**技术框架**：LoPA的整体框架包括以下几个主要步骤：1）**初始化**：从当前已解码的token序列出发，生成多个候选的TFO。2）**并行解码**：对于每个候选TFO，创建一个解码分支，并并行地进行解码。3）**置信度评估**：评估每个解码分支的置信度，用于预测未来解码的并行度。4）**分支选择**：选择具有最高置信度的解码分支，并将其作为下一步解码的基础。5）**迭代**：重复步骤1-4，直到生成完整的token序列。为了支持LoPA的高并行度，论文还开发了一个专门的多设备推理系统，该系统利用分支并行性（BP）在多个GPU上并行执行不同的解码分支。\\n\\n**关键创新**：LoPA的关键创新在于其前瞻性的并行解码策略。与传统的贪心解码方法不同，LoPA不是简单地选择当前置信度最高的token，而是通过并行探索不同的TFO，并选择能够最大化未来并行度的TFO。这种前瞻性的策略能够显著提高dLLM的推理速度。此外，LoPA是一种无需训练的即插即用算法，可以方便地应用于现有的dLLM模型。\\n\\n**关键设计**：LoPA的关键设计包括：1）**TFO生成**：论文采用了一种启发式的TFO生成方法，该方法基于token之间的依赖关系来生成候选的TFO。2）**置信度评估**：论文使用模型的输出概率作为置信度，并结合一些启发式规则来预测未来解码的并行度。3）**分支并行性（BP）**：论文开发了一种专门的多设备推理系统，该系统利用BP在多个GPU上并行执行不同的解码分支，从而充分利用硬件资源。",
            "application_zh": "LoPA具有广泛的应用前景，可以应用于各种需要高速推理的场景，例如实时对话系统、机器翻译、文本摘要等。通过提高dLLM的推理速度，LoPA可以降低计算成本，并使得dLLM能够部署在资源受限的设备上。此外，LoPA的即插即用特性使其可以方便地应用于现有的dLLM模型，从而加速这些模型的部署和应用。",
            "highlight_zh": "LoPA在D2F-Dream模型上取得了显著的性能提升。在GSM8K数据集上，LoPA将TPF提高到10.1，同时保持了优于Dream基线的性能。此外，通过开发专门的多设备推理系统，LoPA实现了每秒1073.9个token的单样本吞吐量，展示了其在实际应用中的巨大潜力。",
            "tags_zh": [
                "扩散模型",
                "大语言模型",
                "并行解码",
                "推理加速",
                "Token填充顺序"
            ],
            "_index": 107,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16229v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16229v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16229v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Machine Learning-based Optimal Control for Colloidal Self-Assembly",
            "authors": [
                "Andres Lizano-Villalobos",
                "Fangyuan Ma",
                "Wentao Tang",
                "Wei Sun",
                "Xun Tang"
            ],
            "arxiv_id": "2512.16402v1",
            "summary": "Achieving precise control of colloidal self-assembly into specific patterns remains a longstanding challenge due to the complex process dynamics. Recently, machine learning-based state representation and reinforcement learning-based control strategies have started to accumulate popularity in the field, showing great potential in achieving an automatable and generalizable approach to producing patterned colloidal assembly. In this work, we adopted a machine learning-based optimal control framework, combining unsupervised learning and graph convolutional neural work for state observation with deep reinforcement learning-based optimal control policy calculation, to provide a data-driven control approach that can potentially be generalized to other many-body self-assembly systems. With Brownian dynamics simulations, we demonstrated its superior performance as compared to traditional order parameter-based state description, and its efficacy in obtaining ordered 2-dimensional spherical colloidal self-assembly in an electric field-mediated system with an actual success rate of 97%.",
            "categories": [
                "cond-mat.soft",
                "eess.SY"
            ],
            "primary_category": "cond-mat.soft",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "19 pages, 5 figures, 1 table",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16402v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning",
                        "deep reinforcement learning"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "提出基于机器学习的最优控制框架，实现胶体自组装的精确控制",
            "summary_zh": "由于复杂的动力学过程，精确控制胶体自组装成特定模式仍然是一个长期存在的挑战。近年来，基于机器学习的状态表示和基于强化学习的控制策略在该领域越来越受欢迎，显示出在实现自动化和通用化方法以产生图案化胶体组装方面的巨大潜力。本文采用了一种基于机器学习的最优控制框架，将无监督学习和图卷积神经网络用于状态观测，以及基于深度强化学习的最优控制策略计算，以提供一种数据驱动的控制方法，该方法可能推广到其他多体自组装系统。通过布朗动力学模拟，我们证明了它相对于传统的基于序参数的状态描述的优越性能，以及在电场介导的系统中获得有序二维球形胶体自组装的有效性，实际成功率达到97%。",
            "intro_zh": [
                "精确控制胶体自组装面临挑战，传统方法难以应对复杂动力学。",
                "采用机器学习最优控制框架，结合无监督学习和深度强化学习。",
                "布朗动力学模拟验证了该方法优越性，自组装成功率达97%。"
            ],
            "method_zh": "**问题定义**：论文旨在解决胶体自组装过程中难以精确控制的问题。现有方法，例如基于序参数的状态描述，无法充分捕捉复杂的动力学过程，导致控制效果不佳。因此，需要一种能够更准确地描述系统状态并进行有效控制的方法。\\n\\n**核心思路**：论文的核心思路是利用机器学习方法来学习系统状态的有效表示，并基于此进行最优控制。具体而言，使用无监督学习和图卷积神经网络（GCN）来提取状态特征，然后使用深度强化学习（DRL）来学习最优控制策略。这种数据驱动的方法能够更好地适应复杂的系统动力学。\\n\\n**技术框架**：整体框架包含以下几个主要模块：1) 数据收集：通过布朗动力学模拟生成大量的系统状态数据。2) 状态表示学习：使用无监督学习（例如自编码器）和GCN从原始数据中学习到有效的状态表示。GCN用于处理胶体粒子之间的相互作用关系。3) 控制策略学习：使用DRL算法（例如DDPG、PPO）基于学习到的状态表示来训练最优控制策略。4) 控制执行：将训练好的控制策略应用于实际的胶体自组装过程。\\n\\n**关键创新**：最重要的技术创新点在于将无监督学习、GCN和DRL结合起来，形成一个端到端的学习框架。与传统方法相比，该方法能够自动地学习到系统状态的有效表示，并基于此进行最优控制，无需人工设计复杂的序参数。GCN的使用能够有效地处理胶体粒子之间的相互作用关系，从而提高状态表示的准确性。\\n\\n**关键设计**：GCN的网络结构需要根据具体的胶体系统进行设计，例如层数、每层的节点数等。DRL算法的选择也需要根据具体问题进行调整。损失函数的设计需要考虑控制目标，例如最小化能量消耗、最大化组装成功率等。此外，还需要仔细调整DRL算法的超参数，例如学习率、折扣因子等。",
            "application_zh": "该研究成果可应用于材料科学、纳米技术等领域，实现对纳米颗粒、胶体颗粒等微观粒子的精确组装，从而制造出具有特定功能的材料和器件。例如，可以用于制造新型传感器、光子晶体、药物递送系统等。该方法具有自动化和通用化的潜力，有望加速新材料的研发过程。",
            "highlight_zh": "实验结果表明，该方法在电场介导的二维球形胶体自组装系统中取得了显著的成功，实际成功率高达97%。与传统的基于序参数的状态描述方法相比，该方法能够更有效地控制胶体自组装过程，并获得更高的组装成功率。这表明该方法具有很强的实用价值。",
            "tags_zh": [
                "胶体自组装",
                "机器学习",
                "最优控制",
                "深度强化学习",
                "图卷积神经网络",
                "无监督学习",
                "布朗动力学",
                "状态表示"
            ],
            "_index": 108,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "Prime and Reach: Synthesising Body Motion for Gaze-Primed Object Reach",
            "authors": [
                "Masashi Hatano",
                "Saptarshi Sinha",
                "Jacob Chalk",
                "Wei-Hong Li",
                "Hideo Saito",
                "Dima Damen"
            ],
            "arxiv_id": "2512.16456v1",
            "summary": "Human motion generation is a challenging task that aims to create realistic motion imitating natural human behaviour. We focus on the well-studied behaviour of priming an object/location for pick up or put down -- that is, the spotting of an object/location from a distance, known as gaze priming, followed by the motion of approaching and reaching the target location. To that end, we curate, for the first time, 23.7K gaze-primed human motion sequences for reaching target object locations from five publicly available datasets, i.e., HD-EPIC, MoGaze, HOT3D, ADT, and GIMO. We pre-train a text-conditioned diffusion-based motion generation model, then fine-tune it conditioned on goal pose or location, on our curated sequences. Importantly, we evaluate the ability of the generated motion to imitate natural human movement through several metrics, including the 'Reach Success' and a newly introduced 'Prime Success' metric. On the largest dataset, HD-EPIC, our model achieves 60% prime success and 89% reach success when conditioned on the goal object location.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Project Page: https://masashi-hatano.github.io/prime-and-reach/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16456v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱四：生成式动作 (Generative Motion)",
                    "id": "4_motion_diffusion",
                    "matched_keywords": [
                        "motion generation"
                    ],
                    "score": 2.5
                }
            ],
            "relevance_score": 2.5,
            "hit_pillars": [
                "4_motion_diffusion"
            ],
            "headline_zh": "提出基于注视启动的人体运动合成方法，用于模拟抓取或放置物体的自然行为。",
            "summary_zh": "人体运动生成是一项具有挑战性的任务，旨在创建模仿自然人类行为的真实运动。本文关注一个经过充分研究的行为：启动物体/位置以进行拾取或放置，即从远处发现物体/位置（称为注视启动），然后是接近和到达目标位置的运动。为此，我们首次整理了23.7K个注视启动的人体运动序列，用于从五个公开可用的数据集（即HD-EPIC、MoGaze、HOT3D、ADT和GIMO）到达目标物体位置。我们预训练了一个文本条件扩散运动生成模型，然后在我们整理的序列上，以目标姿势或位置为条件对其进行微调。重要的是，我们通过包括“到达成功”和新引入的“启动成功”指标在内的多个指标来评估生成的运动模仿自然人类运动的能力。在最大的数据集HD-EPIC上，当以目标物体位置为条件时，我们的模型实现了60%的启动成功率和89%的到达成功率。",
            "intro_zh": [
                "现有方法在生成人体运动时，难以模拟注视启动等复杂行为，缺乏对目标物体交互的精准控制。",
                "本文提出一种基于扩散模型的运动生成框架，利用大规模注视启动数据集进行训练，实现自然的人体运动合成。",
                "实验表明，该方法在HD-EPIC数据集上取得了显著的启动成功率和到达成功率，验证了其有效性。"
            ],
            "method_zh": "**问题定义**：现有的人体运动生成方法难以准确模拟人类在抓取或放置物体前的注视启动行为，即在运动前通过注视来预先锁定目标物体。这导致生成的运动不够自然和真实，缺乏与环境的有效交互。现有方法通常难以将注视信息有效地融入到运动生成过程中。\\n\\n**核心思路**：本文的核心思路是利用大规模的注视启动人体运动数据集，训练一个条件扩散模型，使其能够学习到注视行为与后续运动之间的关联。通过将目标物体的位置或姿势作为条件输入，模型可以生成符合人类自然行为的运动序列，从而实现更真实和自然的运动合成。\\n\\n**技术框架**：该方法采用两阶段训练策略。首先，使用文本条件扩散模型进行预训练，使其具备生成基本人体运动的能力。然后，利用整理的注视启动数据集，以目标物体的位置或姿势为条件，对预训练模型进行微调。在推理阶段，给定目标物体的位置或姿势，模型即可生成相应的运动序列。整体流程包括数据收集与整理、模型预训练、条件微调和运动生成。\\n\\n**关键创新**：该方法的关键创新在于首次整理了大规模的注视启动人体运动数据集，并将其用于训练条件扩散模型。此外，还提出了一个新的评估指标“启动成功率”，用于衡量生成的运动是否能够模拟真实的注视启动行为。与现有方法相比，该方法能够更好地模拟人类与环境的交互，生成更自然和真实的运动序列。\\n\\n**关键设计**：该方法使用扩散模型作为运动生成的核心架构。在微调阶段，目标物体的位置或姿势被编码为条件向量，并输入到扩散模型的逆扩散过程中，以引导运动的生成。损失函数包括运动重建损失和对抗损失，以保证生成运动的真实性和多样性。具体参数设置和网络结构细节在论文中有详细描述（未知）。",
            "application_zh": "该研究成果可应用于虚拟现实、人机交互、游戏开发和机器人控制等领域。例如，可以用于创建更逼真的虚拟角色动画，提高人机交互的自然性和效率，以及使机器人能够更好地理解和模仿人类的动作，从而完成更复杂的任务。未来的研究可以进一步探索如何将注视信息与其他感知信息（如语音、触觉）融合，以生成更丰富和智能的人体运动。",
            "highlight_zh": "该模型在HD-EPIC数据集上取得了显著的成果，到达成功率高达89%，启动成功率达到60%。这些结果表明，该方法能够有效地模拟人类的注视启动行为，并生成自然和真实的运动序列。与未进行注视启动训练的模型相比，该方法在启动成功率方面有显著提升（具体数值未知）。",
            "tags_zh": [
                "人体运动生成",
                "注视启动",
                "扩散模型",
                "人机交互",
                "运动合成"
            ],
            "_index": 109,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16456v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16456v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16456v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Single-View Shape Completion for Robotic Grasping in Clutter",
            "authors": [
                "Abhishek Kashyap",
                "Yuxuan Yang",
                "Henrik Andreasson",
                "Todor Stoyanov"
            ],
            "arxiv_id": "2512.16449v1",
            "summary": "In vision-based robot manipulation, a single camera view can only capture one side of objects of interest, with additional occlusions in cluttered scenes further restricting visibility. As a result, the observed geometry is incomplete, and grasp estimation algorithms perform suboptimally. To address this limitation, we leverage diffusion models to perform category-level 3D shape completion from partial depth observations obtained from a single view, reconstructing complete object geometries to provide richer context for grasp planning. Our method focuses on common household items with diverse geometries, generating full 3D shapes that serve as input to downstream grasp inference networks. Unlike prior work, which primarily considers isolated objects or minimal clutter, we evaluate shape completion and grasping in realistic clutter scenarios with household objects. In preliminary evaluations on a cluttered scene, our approach consistently results in better grasp success rates than a naive baseline without shape completion by 23% and over a recent state of the art shape completion approach by 19%. Our code is available at https://amm.aass.oru.se/shape-completion-grasping/.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16449v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "提出基于扩散模型的单视角形状补全方法，提升复杂场景下机器人抓取成功率。",
            "summary_zh": "在基于视觉的机器人操作中，单个相机视角只能捕捉到目标物体的一侧，而复杂场景中的遮挡进一步限制了可见性。这导致观测到的几何形状不完整，抓取估计算法的性能欠佳。为了解决这一局限性，我们利用扩散模型从单视角获取的局部深度观测中进行类别级别的3D形状补全，重建完整的物体几何形状，为抓取规划提供更丰富的上下文信息。我们的方法侧重于具有多样几何形状的常见家居物品，生成完整的3D形状，作为下游抓取推理网络的输入。与主要考虑孤立物体或极少遮挡的先前工作不同，我们在具有家居物品的真实复杂场景中评估形状补全和抓取性能。在复杂场景的初步评估中，我们的方法始终比没有形状补全的朴素基线提高了23%的抓取成功率，并且比最近最先进的形状补全方法提高了19%。我们的代码可在https://amm.aass.oru.se/shape-completion-grasping/ 获取。",
            "intro_zh": [
                "现有方法在复杂场景下，单视角视觉信息不完整，导致机器人抓取性能下降。",
                "利用扩散模型，从单视角深度信息补全物体3D形状，为抓取提供更全面的几何信息。",
                "在复杂场景实验中，该方法显著提升了机器人抓取成功率，优于现有方法。"
            ],
            "method_zh": "**问题定义**：现有基于视觉的机器人抓取方法在复杂场景中面临挑战，单视角相机只能捕捉到物体部分信息，遮挡进一步加剧了信息缺失，导致抓取算法性能下降。现有形状补全方法主要针对孤立物体或简单场景，难以应对真实复杂环境。\n\n**核心思路**：利用扩散模型强大的生成能力，从单视角深度信息推断出完整的3D物体形状。通过补全缺失的几何信息，为下游抓取算法提供更准确、更全面的输入，从而提升抓取成功率。这种方法的核心在于利用扩散模型学习物体形状的先验知识，并将其应用于局部观测的补全。\n\n**技术框架**：该方法主要包含两个阶段：1) 形状补全阶段：使用扩散模型从单视角深度图像生成完整的3D形状。输入为局部深度图，输出为补全后的3D形状。2) 抓取推理阶段：将补全后的3D形状输入到抓取推理网络中，生成抓取姿态。整体流程是从局部观测到完整形状，再到抓取姿态的端到端过程。\n\n**关键创新**：该方法的主要创新在于将扩散模型应用于复杂场景下的单视角形状补全，并将其与机器人抓取任务相结合。与现有方法相比，该方法能够更好地处理复杂场景中的遮挡和噪声，生成更准确的3D形状，从而提升抓取性能。此外，该方法关注于常见家居物品，更贴近实际应用。\n\n**关键设计**：扩散模型采用标准的U-Net结构，损失函数为L2损失。训练数据为常见家居物品的3D模型。在抓取推理阶段，可以使用现有的抓取推理网络，例如GraspNet。关键参数包括扩散模型的迭代次数、噪声水平等。具体网络结构和参数设置需要在实验中进行调整。",
            "application_zh": "该研究成果可应用于家庭服务机器人、工业自动化等领域。通过提升机器人在复杂环境下的物体识别和抓取能力，可以实现更智能、更高效的自动化操作。例如，在家庭环境中，机器人可以更好地整理物品、进行清洁等任务。在工业环境中，可以实现更灵活的物料搬运和装配。",
            "highlight_zh": "实验结果表明，该方法在复杂场景下显著提升了机器人抓取成功率。与没有形状补全的基线方法相比，抓取成功率提高了23%。与最近最先进的形状补全方法相比，抓取成功率提高了19%。这些结果表明，该方法在复杂场景下的形状补全和抓取性能方面具有显著优势。",
            "tags_zh": [
                "形状补全",
                "扩散模型",
                "机器人抓取",
                "单视角视觉",
                "复杂场景"
            ],
            "_index": 110,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16449v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16449v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16449v1/images/ReOcS/easy/shard_0/scene_1/view_6/rgb.jpg",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "A2VISR: An Active and Adaptive Ground-Aerial Localization System Using Visual Inertial and Single-Range Fusion",
            "authors": [
                "Sijia Chen",
                "Wei Dong"
            ],
            "arxiv_id": "2512.16367v1",
            "summary": "It's a practical approach using the ground-aerial collaborative system to enhance the localization robustness of flying robots in cluttered environments, especially when visual sensors degrade. Conventional approaches estimate the flying robot's position using fixed cameras observing pre-attached markers, which could be constrained by limited distance and susceptible to capture failure. To address this issue, we improve the ground-aerial localization framework in a more comprehensive manner, which integrates active vision, single-ranging, inertial odometry, and optical flow. First, the designed active vision subsystem mounted on the ground vehicle can be dynamically rotated to detect and track infrared markers on the aerial robot, improving the field of view and the target recognition with a single camera. Meanwhile, the incorporation of single-ranging extends the feasible distance and enhances re-capture capability under visual degradation. During estimation, a dimension-reduced estimator fuses multi-source measurements based on polynomial approximation with an extended sliding window, balancing computational efficiency and redundancy. Considering different sensor fidelities, an adaptive sliding confidence evaluation algorithm is implemented to assess measurement quality and dynamically adjust the weighting parameters based on moving variance. Finally, extensive experiments under conditions such as smoke interference, illumination variation, obstacle occlusion, prolonged visual loss, and extended operating range demonstrate that the proposed approach achieves robust online localization, with an average root mean square error of approximately 0.09 m, while maintaining resilience to capture loss and sensor failures.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "accept by IEEE Transactions on Industrial Electronics",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16367v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "optical flow"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出A2VISR，一种融合视觉惯性和单测距的主动自适应地-空协同定位系统",
            "summary_zh": "本文提出了一种地-空协同系统，旨在增强飞行机器人在复杂环境中，尤其是在视觉传感器性能下降时的定位鲁棒性。传统方法使用固定相机观察预先附着的标记来估计飞行机器人的位置，但受到距离限制和易捕获失败的约束。为了解决这个问题，我们以更全面的方式改进了地-空定位框架，集成了主动视觉、单点测距、惯性里程计和光流。首先，地面车辆上安装的主动视觉子系统可以动态旋转，以检测和跟踪空中机器人上的红外标记，从而提高视野和目标识别能力。同时，单点测距的加入扩展了可行距离，增强了视觉退化下的重捕获能力。在估计过程中，降维估计器基于多项式逼近和扩展滑动窗口融合多源测量，平衡了计算效率和冗余。考虑到不同传感器的保真度，自适应滑动置信度评估算法用于评估测量质量，并基于移动方差动态调整权重参数。最后，在烟雾干扰、光照变化、障碍物遮挡、长时间视觉丢失和扩展操作范围等条件下进行的大量实验表明，该方法实现了鲁棒的在线定位，平均均方根误差约为0.09米，同时保持了对捕获丢失和传感器故障的弹性。",
            "intro_zh": [
                "现有方法依赖固定相机和预设标记，易受距离限制和捕获失败影响，难以在复杂环境中保证飞行器定位的鲁棒性。",
                "A2VISR通过地-空协同，融合主动视觉、单点测距、惯性里程计和光流，提升视野范围和重捕获能力，增强定位鲁棒性。",
                "实验结果表明，该方法在多种复杂条件下实现了鲁棒的在线定位，平均均方根误差约为0.09米，并能有效应对传感器失效。"
            ],
            "method_zh": "**问题定义**：论文旨在解决飞行机器人在复杂环境中，尤其是在视觉传感器性能下降时，定位鲁棒性不足的问题。现有方法依赖固定相机和预设标记，存在视野范围有限、易受遮挡、距离受限以及易捕获失败等痛点。\\n\\n**核心思路**：论文的核心思路是构建一个地-空协同定位系统，利用地面车辆搭载的主动视觉系统动态跟踪空中机器人，并融合单点测距、惯性里程计和光流等多源信息，从而提高定位的鲁棒性和精度。通过主动视觉扩展视野，单点测距增加距离范围，多传感器融合提升系统冗余性。\\n\\n**技术框架**：A2VISR系统主要包含以下几个模块：1) 地面车辆搭载的主动视觉子系统，用于动态检测和跟踪空中机器人上的红外标记；2) 单点测距传感器，用于扩展定位距离；3) 空中机器人上的惯性测量单元（IMU）和光流传感器，用于提供运动信息；4) 降维估计器，基于多项式逼近和扩展滑动窗口融合多源测量数据；5) 自适应滑动置信度评估算法，用于评估测量质量并动态调整权重参数。\\n\\n**关键创新**：该论文的关键创新在于：1) 提出了一种地-空协同的主动视觉定位框架，能够动态调整视野范围，提高目标识别能力；2) 融合了单点测距信息，扩展了定位距离，增强了视觉退化下的重捕获能力；3) 提出了一种自适应滑动置信度评估算法，能够根据传感器数据的质量动态调整权重参数，提高定位的鲁棒性。\\n\\n**关键设计**：主动视觉子系统采用可旋转的云台，通过控制云台的旋转角度来扩大视野范围。降维估计器采用多项式逼近来降低计算复杂度。自适应滑动置信度评估算法基于移动方差来评估测量质量，并使用加权最小二乘法进行融合。",
            "application_zh": "该研究成果可应用于复杂环境下的无人机自主导航、协同作业、以及室内定位等领域。例如，在仓库巡检、灾后搜救、以及工业检测等场景中，可以利用该系统实现无人机的精准定位和稳定飞行，提高工作效率和安全性。未来，该技术有望进一步推广到更多需要高精度定位的机器人应用中。",
            "highlight_zh": "实验结果表明，在烟雾干扰、光照变化、障碍物遮挡、长时间视觉丢失和扩展操作范围等复杂条件下，该方法实现了鲁棒的在线定位，平均均方根误差约为0.09米。相较于传统方法，该方法在视觉退化和遮挡情况下表现出更强的鲁棒性和重捕获能力，有效降低了定位失败的风险。",
            "tags_zh": [
                "地空协同",
                "主动视觉",
                "视觉惯性融合",
                "单点测距",
                "鲁棒定位"
            ],
            "_index": 111,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16367v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16367v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16367v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Privacy-Aware Sharing of Raw Spatial Sensor Data for Cooperative Perception",
            "authors": [
                "Bangya Liu",
                "Chengpo Yan",
                "Chenghao Jiang",
                "Suman Banerjee",
                "Akarsh Prabhakara"
            ],
            "arxiv_id": "2512.16265v1",
            "summary": "Cooperative perception between vehicles is poised to offer robust and reliable scene understanding. Recently, we are witnessing experimental systems research building testbeds that share raw spatial sensor data for cooperative perception. While there has been a marked improvement in accuracies and is the natural way forward, we take a moment to consider the problems with such an approach for eventual adoption by automakers. In this paper, we first argue that new forms of privacy concerns arise and discourage stakeholders to share raw sensor data. Next, we present SHARP, a research framework to minimize privacy leakage and drive stakeholders towards the ambitious goal of raw data based cooperative perception. Finally, we discuss open questions for networked systems, mobile computing, perception researchers, industry and government in realizing our proposed framework.",
            "categories": [
                "cs.NI",
                "cs.RO"
            ],
            "primary_category": "cs.NI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16265v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "scene understanding"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出SHARP框架，旨在最小化原始空间传感器数据共享中的隐私泄露，促进车辆协同感知。",
            "summary_zh": "车辆间的协同感知有望提供更强大和可靠的场景理解能力。最近，我们看到实验性系统研究正在构建测试平台，用于共享原始空间传感器数据以实现协同感知。虽然精度有了显著提高，并且这是自然的发展方向，但我们在此停下来思考这种方法在最终被汽车制造商采用时可能存在的问题。在本文中，我们首先指出，新的隐私问题正在出现，这会阻碍利益相关者共享原始传感器数据。接下来，我们提出了SHARP，一个研究框架，旨在最小化隐私泄露，并推动利益相关者朝着基于原始数据的协同感知的宏伟目标前进。最后，我们讨论了网络系统、移动计算、感知研究人员、行业和政府在实现我们提出的框架时面临的开放性问题。",
            "intro_zh": [
                "现有车辆协同感知系统依赖原始传感器数据共享，但直接共享引发了严重的隐私泄露风险，阻碍了实际应用。",
                "SHARP框架旨在通过隐私保护机制，最小化原始数据共享过程中的隐私泄露，从而鼓励利益相关者共享数据。",
                "论文提出了SHARP框架，并讨论了实现该框架在网络系统、移动计算和感知研究等领域面临的挑战和开放性问题。"
            ],
            "method_zh": "**问题定义**：论文旨在解决车辆协同感知中，直接共享原始空间传感器数据所带来的隐私泄露问题。现有方法虽然提高了感知精度，但忽略了用户隐私，导致利益相关者不愿共享数据，阻碍了协同感知技术的推广应用。\\n\\n**核心思路**：SHARP框架的核心思路是在保证协同感知性能的前提下，通过隐私保护机制，降低原始数据共享过程中的隐私泄露风险。具体而言，框架可能采用差分隐私、同态加密等技术，对原始数据进行处理，使其在满足协同感知需求的同时，尽可能地减少敏感信息的暴露。\\n\\n**技术框架**：由于论文摘要中没有详细描述SHARP框架的具体架构，因此无法给出确切的模块和流程。但可以推测，该框架可能包含以下几个主要模块：1) 数据预处理模块：对原始传感器数据进行清洗、校准等操作。2) 隐私保护模块：应用差分隐私、同态加密等技术，对数据进行脱敏处理。3) 数据共享模块：将处理后的数据安全地共享给其他车辆或云平台。4) 感知融合模块：接收来自其他车辆的数据，并进行融合，以提高感知精度。\\n\\n**关键创新**：论文的关键创新在于提出了SHARP框架，将隐私保护机制融入到车辆协同感知的数据共享过程中。与现有方法相比，SHARP框架更加关注用户隐私，有望解决数据共享的瓶颈问题，促进协同感知技术的实际应用。\\n\\n**关键设计**：由于论文摘要中没有提供关于关键设计的具体细节，因此无法给出确切的参数设置、损失函数、网络结构等信息。这些细节可能在论文的后续章节中进行详细描述。例如，差分隐私的参数ε和δ的选择，同态加密方案的选择，以及感知融合算法的设计等。",
            "application_zh": "该研究成果可应用于自动驾驶、智能交通等领域。通过SHARP框架，车辆可以安全地共享传感器数据，提高协同感知能力，从而提升自动驾驶的安全性和可靠性。此外，该框架还可以应用于其他需要数据共享的场景，例如智慧城市、环境监测等，具有广泛的应用前景。",
            "highlight_zh": "由于摘要中没有提供具体的实验结果，因此无法总结实验亮点。需要阅读论文全文才能了解SHARP框架在实际应用中的性能表现，以及与现有方法的对比结果。",
            "tags_zh": [
                "协同感知",
                "隐私保护",
                "数据共享",
                "自动驾驶",
                "传感器数据"
            ],
            "_index": 112,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16265v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16265v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16265v1/hotnets25-template/newfigures/privacy_metrics.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "EasyV2V: A High-quality Instruction-based Video Editing Framework",
            "authors": [
                "Jinjie Mai",
                "Chaoyang Wang",
                "Guocheng Gordon Qian",
                "Willi Menapace",
                "Sergey Tulyakov",
                "Bernard Ghanem",
                "Peter Wonka",
                "Ashkan Mirzaei"
            ],
            "arxiv_id": "2512.16920v1",
            "summary": "While image editing has advanced rapidly, video editing remains less explored, facing challenges in consistency, control, and generalization. We study the design space of data, architecture, and control, and introduce \\emph{EasyV2V}, a simple and effective framework for instruction-based video editing. On the data side, we compose existing experts with fast inverses to build diverse video pairs, lift image edit pairs into videos via single-frame supervision and pseudo pairs with shared affine motion, mine dense-captioned clips for video pairs, and add transition supervision to teach how edits unfold. On the model side, we observe that pretrained text-to-video models possess editing capability, motivating a simplified design. Simple sequence concatenation for conditioning with light LoRA fine-tuning suffices to train a strong model. For control, we unify spatiotemporal control via a single mask mechanism and support optional reference images. Overall, EasyV2V works with flexible inputs, e.g., video+text, video+mask+text, video+mask+reference+text, and achieves state-of-the-art video editing results, surpassing concurrent and commercial systems. Project page: https://snap-research.github.io/easyv2v/",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Project page: https://snap-research.github.io/easyv2v/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16920v1",
            "code_links": [
                {
                    "url": "https://snap-research.github.io/easyv2v/",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱八：物理动画 (Physics-based Animation)",
                    "id": "8_physics_animation",
                    "matched_keywords": [
                        "spatiotemporal"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "8_physics_animation"
            ],
            "headline_zh": "EasyV2V：高质量的基于指令的视频编辑框架，实现超越现有商业系统的性能。",
            "summary_zh": "视频编辑相较于图像编辑发展缓慢，面临着一致性、控制性和泛化性方面的挑战。本文研究了数据、架构和控制的设计空间，并提出了一个简单有效的基于指令的视频编辑框架EasyV2V。在数据方面，我们利用具有快速逆向功能的现有专家模型构建多样化的视频对，通过单帧监督将图像编辑对提升为视频，使用共享仿射运动的伪视频对，挖掘密集字幕片段以生成视频对，并添加过渡监督来学习编辑如何展开。在模型方面，我们观察到预训练的文本到视频模型具有编辑能力，从而简化了设计。简单的序列连接和轻量级的LoRA微调足以训练出一个强大的模型。在控制方面，我们通过单一的掩码机制统一了时空控制，并支持可选的参考图像。总体而言，EasyV2V可以处理灵活的输入，例如视频+文本、视频+掩码+文本、视频+掩码+参考+文本，并实现了最先进的视频编辑效果，超越了现有的商业系统。",
            "intro_zh": [
                "现有视频编辑方法在一致性、控制性和泛化性方面存在不足，难以满足高质量编辑需求。",
                "EasyV2V框架利用预训练文本到视频模型的编辑能力，通过简单序列连接和LoRA微调实现高效训练。",
                "该方法通过统一的掩码机制实现时空控制，并支持多种输入形式，显著提升了视频编辑效果。"
            ],
            "method_zh": "**问题定义**：现有视频编辑方法在保持视频内容一致性、精确控制编辑过程以及实现良好的泛化能力方面存在挑战。商业系统和现有方法难以灵活处理多种输入形式，并且编辑质量有待提高。\\n\\n**核心思路**：论文的核心思路是利用预训练的文本到视频模型所蕴含的编辑能力，通过简单而有效的微调策略，使其能够根据指令进行高质量的视频编辑。通过精心设计的数据增强策略和统一的控制机制，提升模型在各种输入条件下的编辑效果。\\n\\n**技术框架**：EasyV2V框架主要包含数据准备、模型训练和控制三个部分。数据准备阶段，通过多种方式构建高质量的视频编辑对，包括利用图像编辑对、伪视频对、密集字幕片段等。模型训练阶段，采用预训练的文本到视频模型，并使用序列连接和LoRA微调进行训练。控制阶段，通过统一的掩码机制实现时空控制，并支持可选的参考图像输入。\\n\\n**关键创新**：该方法最重要的创新点在于发现了预训练文本到视频模型的潜在编辑能力，并提出了一种简单有效的微调策略，使其能够根据指令进行高质量的视频编辑。此外，统一的掩码控制机制和多样化的数据增强策略也为提升编辑效果做出了重要贡献。\\n\\n**关键设计**：在数据增强方面，论文采用了多种策略，包括利用具有快速逆向功能的专家模型、单帧监督、共享仿射运动等。在模型训练方面，采用了轻量级的LoRA微调，以避免过拟合。在控制方面，使用单一的掩码机制统一了时空控制，简化了控制流程。",
            "application_zh": "EasyV2V框架可广泛应用于电影制作、广告设计、社交媒体内容创作等领域。它能够帮助用户快速、高效地根据指令编辑视频，实现各种创意效果，降低视频编辑的门槛，提升内容创作效率。未来，该技术有望进一步发展，实现更智能、更个性化的视频编辑。",
            "highlight_zh": "EasyV2V在视频编辑任务上取得了state-of-the-art的结果，超越了现有的商业系统和同期的研究工作。该方法能够处理多种输入形式，例如视频+文本、视频+掩码+文本、视频+掩码+参考+文本，并且在编辑质量和效率方面都表现出色。项目主页提供了详细的实验结果和可视化展示。",
            "tags_zh": [
                "视频编辑",
                "指令驱动",
                "文本到视频",
                "预训练模型",
                "LoRA微调",
                "时空控制",
                "数据增强"
            ],
            "_index": 113,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "Distributional AGI Safety",
            "authors": [
                "Nenad Tomašev",
                "Matija Franklin",
                "Julian Jacobs",
                "Sébastien Krier",
                "Simon Osindero"
            ],
            "arxiv_id": "2512.16856v1",
            "summary": "AI safety and alignment research has predominantly been focused on methods for safeguarding individual AI systems, resting on the assumption of an eventual emergence of a monolithic Artificial General Intelligence (AGI). The alternative AGI emergence hypothesis, where general capability levels are first manifested through coordination in groups of sub-AGI individual agents with complementary skills and affordances, has received far less attention. Here we argue that this patchwork AGI hypothesis needs to be given serious consideration, and should inform the development of corresponding safeguards and mitigations. The rapid deployment of advanced AI agents with tool-use capabilities and the ability to communicate and coordinate makes this an urgent safety consideration. We therefore propose a framework for distributional AGI safety that moves beyond evaluating and aligning individual agents. This framework centers on the design and implementation of virtual agentic sandbox economies (impermeable or semi-permeable), where agent-to-agent transactions are governed by robust market mechanisms, coupled with appropriate auditability, reputation management, and oversight to mitigate collective risks.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16856v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "affordance"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出分布式AGI安全框架，通过虚拟沙盒经济保障群体智能安全。",
            "summary_zh": "人工智能安全和对齐研究主要集中在保护单个AI系统的方法上，并假设最终会出现一个单体的通用人工智能（AGI）。另一种AGI涌现假设，即通用能力水平首先通过具有互补技能和能力的多个亚AGI个体智能体的协作来体现，受到的关注要少得多。本文认为，这种拼凑式AGI假设需要认真考虑，并应为相应安全措施和缓解措施的开发提供信息。具有工具使用能力以及沟通和协调能力的高级AI智能体的快速部署使这成为一个紧迫的安全考虑因素。因此，本文提出了一个分布式AGI安全框架，该框架超越了评估和对齐单个智能体。该框架的核心是设计和实施虚拟智能体沙盒经济（不可渗透或半渗透），其中智能体之间的交易受稳健的市场机制、适当的可审计性、声誉管理和监督的约束，以减轻集体风险。",
            "intro_zh": [
                "现有AI安全研究主要关注单个AGI的安全，忽略了多个亚AGI智能体协作涌现通用能力的风险。",
                "提出分布式AGI安全框架，通过构建虚拟沙盒经济来管理和减轻多个智能体协作带来的风险。",
                "该框架强调市场机制、可审计性、声誉管理和监督在保障群体智能安全中的作用，应对快速部署的智能体。"
            ],
            "method_zh": "**问题定义**：现有AI安全研究主要关注单个AGI的安全，忽略了多个亚AGI智能体通过协作涌现通用能力的潜在风险。这种“拼凑式AGI”可能带来新的安全挑战，例如智能体之间的恶意串通、市场操纵等，而传统的针对单个智能体的安全措施难以有效应对。\n\n**核心思路**：论文的核心思路是构建一个虚拟的智能体沙盒经济，通过市场机制来规范智能体之间的交互行为，并引入可审计性、声誉管理和监督机制来降低集体风险。这种方法将安全问题从单个智能体的对齐转移到整个智能体生态系统的治理。\n\n**技术框架**：该框架包含以下几个主要模块：1) 虚拟沙盒环境：提供一个隔离的、可控的环境，用于模拟智能体之间的交互。2) 市场机制：设计合理的市场规则，例如定价机制、交易规则等，以激励智能体进行良性竞争和合作。3) 可审计性：记录智能体之间的所有交易行为，以便进行事后分析和风险评估。4) 声誉管理：建立智能体的声誉系统，惩罚恶意行为，奖励良好行为。5) 监督机制：引入外部监督者，监控智能体之间的交互，及时发现和处理潜在的风险。\n\n**关键创新**：该论文的关键创新在于将经济学原理引入到AI安全领域，提出了一个基于市场机制的分布式AGI安全框架。与传统的针对单个智能体的安全措施相比，该框架能够更好地应对多个智能体协作带来的复杂风险。\n\n**关键设计**：论文中没有给出具体的参数设置、损失函数或网络结构等技术细节，而是侧重于框架的设计理念和整体架构。未来的研究可以进一步探索如何将具体的机器学习技术与该框架相结合，例如使用强化学习来优化市场机制，使用图神经网络来分析智能体之间的关系等。此外，如何设计有效的声誉系统和监督机制也是一个重要的研究方向。",
            "application_zh": "该研究成果可应用于构建安全的AI协作平台，例如智能制造、自动驾驶、金融交易等领域。通过构建虚拟沙盒经济，可以有效降低多个AI智能体协作带来的风险，保障系统的稳定性和可靠性。此外，该框架还可以用于评估和验证新的AI安全技术，促进AI安全领域的发展。",
            "highlight_zh": "由于是框架性论文，没有提供具体的实验结果。其亮点在于提出了一个全新的分布式AGI安全视角，并设计了一个基于虚拟沙盒经济的解决方案。该框架为未来的AI安全研究提供了一个新的方向，具有重要的理论价值和实践意义。未来的研究可以基于该框架进行更深入的探索，例如设计具体的市场机制、声誉系统和监督机制，并进行实验验证。",
            "tags_zh": [
                "AGI安全",
                "分布式智能",
                "虚拟沙盒",
                "市场机制",
                "群体智能",
                "AI治理",
                "智能体协作"
            ],
            "_index": 114,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "OS-Oracle: A Comprehensive Framework for Cross-Platform GUI Critic Models",
            "authors": [
                "Zhenyu Wu",
                "Jingjing Xie",
                "Zehao Li",
                "Bowen Yang",
                "Qiushi Sun",
                "Zhaoyang Liu",
                "Zhoumianze Liu",
                "Yu Qiao",
                "Xiangyu Yue",
                "Zun Wang",
                "Zichen Ding"
            ],
            "arxiv_id": "2512.16295v1",
            "summary": "With VLM-powered computer-using agents (CUAs) becoming increasingly capable at graphical user interface (GUI) navigation and manipulation, reliable step-level decision-making has emerged as a key bottleneck for real-world deployment. In long-horizon workflows, errors accumulate quickly and irreversible actions can cause unintended consequences, motivating critic models that assess each action before execution. While critic models offer a promising solution, their effectiveness is hindered by the lack of diverse, high-quality GUI feedback data and public critic benchmarks for step-level evaluation in computer use. To bridge these gaps, we introduce OS-Oracle that makes three core contributions: (1) a scalable data pipeline for synthesizing cross-platform GUI critic data; (2) a two-stage training paradigm combining supervised fine-tuning (SFT) and consistency-preserving group relative policy optimization (CP-GRPO); (3) OS-Critic Bench, a holistic benchmark for evaluating critic model performance across Mobile, Web, and Desktop platforms. Leveraging this framework, we curate a high-quality dataset containing 310k critic samples. The resulting critic model, OS-Oracle-7B, achieves state-of-the-art performance among open-source VLMs on OS-Critic Bench, and surpasses proprietary models on the mobile domain. Furthermore, when serving as a pre-critic, OS-Oracle-7B improves the performance of native GUI agents such as UI-TARS-1.5-7B in OSWorld and AndroidWorld environments. The code is open-sourced at https://github.com/numbmelon/OS-Oracle.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16295v1",
            "code_links": [
                {
                    "url": "https://github.com/numbmelon/OS-Oracle",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "OS-Oracle：跨平台GUI评论模型的综合框架，提升计算机使用代理的决策能力",
            "summary_zh": "随着VLM驱动的计算机使用代理(CUA)在图形用户界面(GUI)导航和操作方面能力日益增强，可靠的步级决策已成为实际部署的关键瓶颈。在长时程工作流程中，错误会迅速累积，不可逆的操作可能导致意想不到的后果，因此需要评估每次操作的评论模型。然而，缺乏多样化、高质量的GUI反馈数据和用于计算机使用中步级评估的公共评论基准阻碍了评论模型的有效性。为了弥合这些差距，我们引入了OS-Oracle，它做出了三个核心贡献：(1)用于合成跨平台GUI评论数据的可扩展数据管道；(2)结合监督微调(SFT)和一致性保持的群体相对策略优化(CP-GRPO)的两阶段训练范式；(3)OS-Critic Bench，一个用于评估移动、Web和桌面平台评论模型性能的整体基准。利用该框架，我们整理了一个包含31万个评论样本的高质量数据集。由此产生的评论模型OS-Oracle-7B在OS-Critic Bench上实现了开源VLM中最先进的性能，并在移动领域超越了专有模型。此外，当作为预评论模型时，OS-Oracle-7B提高了原生GUI代理（如UI-TARS-1.5-7B）在OSWorld和AndroidWorld环境中的性能。代码已在https://github.com/numbmelon/OS-Oracle上开源。",
            "intro_zh": [
                "现有计算机使用代理在GUI操作中面临步级决策可靠性问题，长时程任务中易累积错误。",
                "提出OS-Oracle框架，通过合成数据、两阶段训练和基准测试，提升评论模型性能。",
                "OS-Oracle-7B在跨平台GUI评论任务中达到SOTA，并能有效提升现有GUI代理的性能。"
            ],
            "method_zh": "**问题定义**：论文旨在解决计算机使用代理（CUAs）在图形用户界面（GUI）交互中，由于缺乏可靠的步级决策能力而导致的性能瓶颈问题。现有方法在长时程任务中容易累积错误，并且缺乏高质量的GUI反馈数据和公共评论基准，难以有效评估和提升决策能力。\\n\\n**核心思路**：论文的核心思路是构建一个综合的框架，包括数据合成管道、训练范式和评估基准，从而提升评论模型在GUI交互中的决策能力。通过大规模合成高质量的GUI反馈数据，并采用两阶段训练方法，训练出一个能够准确评估每一步操作的评论模型。\\n\\n**技术框架**：OS-Oracle框架包含三个主要组成部分：(1) 可扩展的数据管道，用于合成跨平台GUI评论数据；(2) 两阶段训练范式，包括监督微调（SFT）和一致性保持的群体相对策略优化（CP-GRPO）；(3) OS-Critic Bench，一个用于评估评论模型在移动、Web和桌面平台性能的基准。整体流程是先通过数据管道生成训练数据，然后使用两阶段训练方法训练评论模型，最后使用OS-Critic Bench评估模型性能。\\n\\n**关键创新**：论文的关键创新在于提出了一个完整的框架，解决了GUI评论模型训练和评估中的数据稀缺和基准缺失问题。具体包括：(1) 设计了一个可扩展的数据管道，能够合成多样化、高质量的GUI反馈数据；(2) 提出了一个两阶段训练范式，结合了监督学习和强化学习的优点，提升了模型的泛化能力和鲁棒性；(3) 构建了一个全面的评估基准，能够客观地评估评论模型在不同平台和任务上的性能。\\n\\n**关键设计**：在数据合成方面，论文设计了能够模拟用户交互行为的规则和策略，从而生成真实的GUI反馈数据。在两阶段训练中，SFT阶段使用监督学习方法，使模型学习基本的评论知识；CP-GRPO阶段使用强化学习方法，通过群体策略优化，提升模型的一致性和鲁棒性。损失函数的设计也考虑了不同平台和任务的特点，从而更好地优化模型性能。具体参数设置和网络结构细节在论文中有更详细的描述。",
            "application_zh": "该研究成果可广泛应用于各种需要人机交互的场景，例如智能助手、自动化测试、机器人流程自动化（RPA）等。通过提升计算机使用代理的决策能力，可以减少人为干预，提高工作效率，并降低错误率。未来，该技术有望应用于更复杂的任务，例如医疗诊断、金融分析等。",
            "highlight_zh": "OS-Oracle-7B在OS-Critic Bench上实现了开源VLM中最先进的性能，并在移动领域超越了专有模型。此外，当作为预评论模型时，OS-Oracle-7B提高了原生GUI代理（如UI-TARS-1.5-7B）在OSWorld和AndroidWorld环境中的性能。这些结果表明，OS-Oracle框架能够有效提升评论模型的性能，并改善计算机使用代理的决策能力。",
            "tags_zh": [
                "GUI评论模型",
                "跨平台",
                "计算机使用代理",
                "两阶段训练",
                "数据合成",
                "强化学习",
                "基准测试"
            ],
            "_index": 115,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16295v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16295v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16295v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Resilience of coupled systems under deep uncertainty and dynamic complexity: An integrative literature review",
            "authors": [
                "Jannie Coenen",
                "Vítor Vasconcelos",
                "Heiman Wertheim",
                "Marcel Olde Rikkert",
                "Sophie Hadjisotiriou",
                "Vittorio Nespeca",
                "Tom Oreel",
                "Rick Quax",
                "Etiënne Rouwette",
                "Vincent Marchau",
                "Hubert Korzilius"
            ],
            "arxiv_id": "2512.16608v1",
            "summary": "Resilience in coupled systems is increasingly critical in addressing global challenges such as climate change and pandemics. These systems show unpredictable behaviour due to dynamic complexity and deep uncertainty across spatiotemporal scales. Despite growing interest, few studies systematically integrate both concepts when assessing resilience. This paper conducts an integrative review of 102 English-language publications to identify gaps in current approaches. Findings reveal that most papers address lower levels of uncertainty and rarely consider dynamic complexity and deep uncertainty simultaneously, which limits the effectiveness of resilience strategies. To advance systems research, we propose a conceptual framework and practical tools to support researchers and decision-makers in evaluating and improving resilience. The paper also outlines future research directions for more robust, adaptive, and integrative resilience assessments.",
            "categories": [
                "physics.soc-ph",
                "eess.SY"
            ],
            "primary_category": "physics.soc-ph",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16608v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱八：物理动画 (Physics-based Animation)",
                    "id": "8_physics_animation",
                    "matched_keywords": [
                        "spatiotemporal"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "8_physics_animation"
            ],
            "headline_zh": "针对耦合系统在深度不确定性和动态复杂性下的韧性，提出综合性文献综述。",
            "summary_zh": "耦合系统的韧性在应对气候变化和疫情等全球挑战中日益重要。由于时空尺度上的动态复杂性和深度不确定性，这些系统表现出不可预测的行为。尽管人们对此越来越感兴趣，但很少有研究在评估韧性时系统地整合这两个概念。本文对102篇英文出版物进行了综合性回顾，以找出当前方法中的差距。研究结果表明，大多数论文处理的是较低水平的不确定性，很少同时考虑动态复杂性和深度不确定性，这限制了韧性策略的有效性。为了推进系统研究，我们提出了一个概念框架和实用工具，以支持研究人员和决策者评估和提高韧性。本文还概述了未来研究方向，以实现更稳健、适应性和综合性的韧性评估。",
            "intro_zh": [
                "现有韧性研究较少同时考虑动态复杂性和深度不确定性，限制了策略有效性。",
                "论文提出概念框架和实用工具，旨在支持研究人员和决策者评估并提升耦合系统的韧性。",
                "通过综合文献回顾，论文识别了当前方法的差距，并为未来研究方向提供了指导。"
            ],
            "method_zh": "**问题定义**：论文旨在解决耦合系统在深度不确定性和动态复杂性下韧性评估的不足问题。现有方法通常只关注较低层次的不确定性，忽略了动态复杂性和深度不确定性同时存在的情况，导致韧性策略的有效性受限。此外，缺乏系统性的框架和工具来支持研究人员和决策者进行综合评估。\\n\\n**核心思路**：论文的核心思路是通过综合性的文献回顾，识别现有研究的差距，并在此基础上提出一个概念框架和实用工具，以支持更全面、更有效的韧性评估。该框架旨在帮助研究人员和决策者更好地理解和应对耦合系统中的复杂性和不确定性。\\n\\n**技术框架**：论文采用综合文献回顾的方法，对102篇英文出版物进行分析。首先，确定研究范围和关键词，然后进行文献检索和筛选。接着，对选定的文献进行深入阅读和分析，提取关键信息，识别研究差距。最后，基于分析结果，构建概念框架和提出实用工具，并为未来研究方向提供建议。\\n\\n**关键创新**：论文的关键创新在于其综合性的视角，将动态复杂性和深度不确定性同时纳入韧性评估的考量范围。此外，提出的概念框架和实用工具为研究人员和决策者提供了一个系统性的方法来评估和提高耦合系统的韧性，填补了现有研究的空白。\\n\\n**关键设计**：论文的关键设计在于其文献选择标准和分析框架。文献选择标准确保了研究的代表性和相关性，而分析框架则指导了信息的提取和整合。概念框架和实用工具的设计则基于对现有研究差距的深刻理解，旨在提供实用和可操作的指导。",
            "application_zh": "该研究成果可应用于气候变化适应、疫情应对、城市规划、供应链管理等领域，帮助决策者制定更具韧性的策略，提高系统应对外部冲击的能力。通过更全面地考虑动态复杂性和深度不确定性，可以提升策略的有效性和可持续性，从而降低风险并促进长期发展。",
            "highlight_zh": "论文通过对102篇文献的综合分析，揭示了现有韧性研究的局限性，并提出了一个概念框架和实用工具，为未来的研究和实践提供了指导。研究强调了同时考虑动态复杂性和深度不确定性的重要性，为更有效的韧性策略奠定了基础。具体性能数据未知，但研究结果为相关领域的研究人员和决策者提供了重要的参考。",
            "tags_zh": [
                "耦合系统",
                "韧性评估",
                "深度不确定性",
                "动态复杂性",
                "文献综述",
                "概念框架",
                "决策支持"
            ],
            "_index": 116,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "From Liability to Asset: A Three-Mode Grid-Forming Control Framework for Centralized Data Center UPS Systems",
            "authors": [
                "Mohamed Shamseldein"
            ],
            "arxiv_id": "2512.16497v1",
            "summary": "AI workloads are turning large data centers into highly dynamic power-electronic loads; fault-time behavior and workload pulsing can stress weak-grid points of interconnection. This paper proposes a centralized medium-voltage (MV) uninterruptible power supply (UPS) control architecture implemented as three operating modes: Mode 1 regulates a DC stiff bus and shapes normal-operation grid draw, Mode 2 enforces current-limited fault-mode P--Q priority with UPS battery energy storage system (UPS-BESS) buffering and a rate-limited post-fault \"soft return,\" and Mode 3 optionally provides droop-based fast frequency response via grid-draw modulation. Fundamental-frequency averaged dq simulations (50 MW block, short-circuit ratio (SCR) = 1.5, 0.5 p.u. three-phase dip for 150~ms) show zero unserved information-technology (IT) energy (0.00000 MWh vs.0.00208 MWh for a momentary-cessation benchmark), a 0.57 p.u. peak inverter current (vs. 1.02 p.u. for a synchronous-reference-frame phase-locked loop (SRF-PLL) low-voltage ride-through (LVRT) baseline), a nonzero mean fault-window grid draw of 0.20~p.u. (vs.approx 0 for momentary cessation), and an improved settled point-of-common-coupling (PCC) voltage minimum of 0.79 p.u. after one cycle (vs. 0.66 p.u.). A forced-oscillation case study applies a 1 Hz pulsed load (+/- 0.25 p.u.) and shows that the normal-operation shaping filters the oscillation seen by the grid while the UPS-BESS buffers the pulsing component.",
            "categories": [
                "eess.SY"
            ],
            "primary_category": "eess.SY",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16497v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱八：物理动画 (Physics-based Animation)",
                    "id": "8_physics_animation",
                    "matched_keywords": [
                        "PULSE"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "8_physics_animation"
            ],
            "headline_zh": "针对数据中心UPS系统，提出三模电网重构控制框架，提升弱电网适应性。",
            "summary_zh": "人工智能负载正将大型数据中心转变为高度动态的电力电子负载；故障期间的行为和负载脉冲可能会给互联的弱电网带来压力。本文提出了一种集中式中压（MV）不间断电源（UPS）控制架构，该架构实现为三种运行模式：模式1调节直流母线并塑造正常运行的电网吸取功率；模式2通过UPS电池储能系统（UPS-BESS）缓冲和速率限制的故障后“软返回”来强制执行电流限制的故障模式P-Q优先级；模式3可选地通过电网吸取功率调制提供基于下垂的快速频率响应。基频平均dq仿真（50 MW模块，短路比（SCR）= 1.5，0.5 p.u.三相骤降150毫秒）显示零未服务的信息技术（IT）能量（0.00000 MWh，而瞬时中断基准为0.00208 MWh），0.57 p.u.峰值逆变器电流（而同步参考系锁相环（SRF-PLL）低电压穿越（LVRT）基线为1.02 p.u.），0.20 p.u.的非零平均故障窗口电网吸取功率（而瞬时中断约为0），以及改善的稳定公共连接点（PCC）电压最小值，在一个周期后为0.79 p.u.（而0.66 p.u.）。强制振荡案例研究应用1 Hz脉冲负载（+/- 0.25 p.u.），表明正常运行整形滤波器滤除了电网看到的振荡，而UPS-BESS缓冲了脉冲分量。",
            "intro_zh": [
                "数据中心负载波动和故障易对弱电网造成冲击，现有UPS系统在应对此类问题时存在不足。",
                "提出三模控制框架，通过不同模式分别实现稳压、故障保护和频率响应，提升系统鲁棒性。",
                "仿真结果表明，该方法能有效减少能量损失，降低逆变器电流峰值，并改善电网电压稳定性。"
            ],
            "method_zh": "**问题定义**：数据中心UPS系统需要应对日益增长的动态负载和潜在的电网故障，尤其是在弱电网环境下。传统UPS系统在故障期间可能导致电压骤降、电流过载，甚至信息技术设备停机，影响数据中心的可靠运行。现有方法，如基于同步参考系锁相环（SRF-PLL）的低电压穿越（LVRT）技术，在故障期间可能导致逆变器电流过大，且无法有效支撑电网电压。\n\\n**核心思路**：本文的核心思路是将UPS系统设计为具有三种运行模式的智能电网重构单元。通过集中式控制，UPS系统可以根据电网状态和负载需求，灵活切换不同的运行模式，从而实现对电网的支撑和保护。这种方法旨在将UPS系统从单纯的备用电源转变为电网的积极参与者，提高电网的稳定性和可靠性。\n\\n**技术框架**：该控制框架包含三个主要模式：\n1. **模式1（正常运行）**：调节直流母线电压，并对电网吸取的功率进行整形，以减少谐波和提高功率因数。\n2. **模式2（故障模式）**：在电网发生故障时，限制逆变器电流，并根据P-Q优先级向电网注入有功和无功功率，同时利用UPS-BESS提供缓冲，并采用速率限制的“软返回”策略，避免故障恢复时的冲击。\n3. **模式3（可选，快速频率响应）**：通过调制电网吸取的功率，提供基于下垂控制的快速频率响应，以支撑电网频率稳定。\n\\n**关键创新**：该方法的关键创新在于将UPS系统视为一个可编程的电网支撑单元，通过集中式控制实现多种运行模式的切换，从而在正常运行、故障和频率扰动等不同工况下都能有效支撑电网。与传统的被动式UPS系统相比，该方法能够更积极地参与电网的稳定控制。\n\\n**关键设计**：\n*   **模式切换策略**：根据电网电压、频率和负载需求，设计合理的模式切换策略，确保系统在不同工况下都能平稳运行。\n*   **故障模式下的P-Q优先级控制**：根据电网需求，合理分配有功和无功功率的优先级，以最大程度地支撑电网电压。\n*   **速率限制的“软返回”策略**：在故障恢复时，限制逆变器电流的上升速率，避免对电网造成冲击。\n*   **正常运行模式下的功率整形滤波器**：设计合适的滤波器，减少电网吸取功率的谐波含量。",
            "application_zh": "该研究成果可应用于大型数据中心、工业园区等对电力可靠性要求高的场景。通过将UPS系统升级为智能电网支撑单元，可以提高电网的稳定性和可靠性，减少因电网故障造成的经济损失。未来，该技术有望推广到更多分布式电源接入的场景，促进智能电网的发展。",
            "highlight_zh": "仿真结果表明，与瞬时中断基准相比，该方法实现了零未服务的信息技术能量（0.00000 MWh vs. 0.00208 MWh）。逆变器峰值电流从1.02 p.u.（SRF-PLL LVRT基线）降低到0.57 p.u.。故障期间的平均电网吸取功率为0.20 p.u.，而瞬时中断约为0。公共连接点（PCC）电压最小值在一个周期后提高到0.79 p.u.，而基线为0.66 p.u.。",
            "tags_zh": [
                "数据中心",
                "不间断电源",
                "电网重构",
                "弱电网",
                "电力电子",
                "智能电网",
                "故障穿越"
            ],
            "_index": 117,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16497v1/simulation_results_pulse_ramp.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16497v1/simulation_results_pulse.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16497v1/simulation_results_stage1.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        }
    ]
}