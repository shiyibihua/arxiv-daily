{
  "count": 118,
  "papers": [
    {
      "title": "E-SDS: Environment-aware See it, Do it, Sorted - Automated Environment-Aware Reinforcement Learning for Humanoid Locomotion",
      "authors": [
        "Enis Yalcin",
        "Joshua O'Hara",
        "Maria Stamatopoulou",
        "Chengxu Zhou",
        "Dimitrios Kanoulas"
      ],
      "arxiv_id": "2512.16446v1",
      "summary": "Vision-language models (VLMs) show promise in automating reward design in humanoid locomotion, which could eliminate the need for tedious manual engineering. However, current VLM-based methods are essentially \"blind\", as they lack the environmental perception required to navigate complex terrain. We present E-SDS (Environment-aware See it, Do it, Sorted), a framework that closes this perception gap. E-SDS integrates VLMs with real-time terrain sensor analysis to automatically generate reward functions that facilitate training of robust perceptive locomotion policies, grounded by example videos. Evaluated on a Unitree G1 humanoid across four distinct terrains (simple, gaps, obstacles, stairs), E-SDS uniquely enabled successful stair descent, while policies trained with manually-designed rewards or a non-perceptive automated baseline were unable to complete the task. In all terrains, E-SDS also reduced velocity tracking error by 51.9-82.6%. Our framework reduces the human effort of reward design from days to less than two hours while simultaneously producing more robust and capable locomotion policies.",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "12 pages, 3 figures, 4 tables. Accepted at RiTA 2025 (Springer LNNS)",
      "doi": "",
      "journal_ref": "RiTA 2025 (Springer LNNS)",
      "pdf_url": "https://arxiv.org/pdf/2512.16446v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "[T]humanoid locomotion",
            "[T]locomotion",
            "locomotion policy",
            "Unitree"
          ],
          "score": 22.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning",
            "reward design"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 28.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "Vision-Language-Action Models for Autonomous Driving: Past, Present, and Future",
      "authors": [
        "Tianshuai Hu",
        "Xiaolu Liu",
        "Song Wang",
        "Yiyao Zhu",
        "Ao Liang",
        "Lingdong Kong",
        "Guoyang Zhao",
        "Zeying Gong",
        "Jun Cen",
        "Zhiyu Huang",
        "Xiaoshuai Hao",
        "Linfeng Li",
        "Hang Song",
        "Xiangtai Li",
        "Jun Ma",
        "Shaojie Shen",
        "Jianke Zhu",
        "Dacheng Tao",
        "Ziwei Liu",
        "Junwei Liang"
      ],
      "arxiv_id": "2512.16760v1",
      "summary": "Autonomous driving has long relied on modular \"Perception-Decision-Action\" pipelines, where hand-crafted interfaces and rule-based components often break down in complex or long-tailed scenarios. Their cascaded design further propagates perception errors, degrading downstream planning and control. Vision-Action (VA) models address some limitations by learning direct mappings from visual inputs to actions, but they remain opaque, sensitive to distribution shifts, and lack structured reasoning or instruction-following capabilities. Recent progress in Large Language Models (LLMs) and multimodal learning has motivated the emergence of Vision-Language-Action (VLA) frameworks, which integrate perception with language-grounded decision making. By unifying visual understanding, linguistic reasoning, and actionable outputs, VLAs offer a pathway toward more interpretable, generalizable, and human-aligned driving policies. This work provides a structured characterization of the emerging VLA landscape for autonomous driving. We trace the evolution from early VA approaches to modern VLA frameworks and organize existing methods into two principal paradigms: End-to-End VLA, which integrates perception, reasoning, and planning within a single model, and Dual-System VLA, which separates slow deliberation (via VLMs) from fast, safety-critical execution (via planners). Within these paradigms, we further distinguish subclasses such as textual vs. numerical action generators and explicit vs. implicit guidance mechanisms. We also summarize representative datasets and benchmarks for evaluating VLA-based driving systems and highlight key challenges and open directions, including robustness, interpretability, and instruction fidelity. Overall, this work aims to establish a coherent foundation for advancing human-compatible autonomous driving systems.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "Preprint; 40 pages, 7 figures, 9 tables; GitHub at https://github.com/worldbench/awesome-vla-for-ad",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16760v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "VLA",
            "large language model",
            "multimodal",
            "instruction following"
          ],
          "score": 21.0
        }
      ],
      "relevance_score": 21.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation",
      "authors": [
        "Xin Lin",
        "Meixi Song",
        "Dizhe Zhang",
        "Wenxuan Lu",
        "Haodong Li",
        "Bo Du",
        "Ming-Hsuan Yang",
        "Truong Nguyen",
        "Lu Qi"
      ],
      "arxiv_id": "2512.16913v1",
      "summary": "In this work, we present a panoramic metric depth foundation model that generalizes across diverse scene distances. We explore a data-in-the-loop paradigm from the view of both data construction and framework design. We collect a large-scale dataset by combining public datasets, high-quality synthetic data from our UE5 simulator and text-to-image models, and real panoramic images from the web. To reduce domain gaps between indoor/outdoor and synthetic/real data, we introduce a three-stage pseudo-label curation pipeline to generate reliable ground truth for unlabeled images. For the model, we adopt DINOv3-Large as the backbone for its strong pre-trained generalization, and introduce a plug-and-play range mask head, sharpness-centric optimization, and geometry-centric optimization to improve robustness to varying distances and enforce geometric consistency across views. Experiments on multiple benchmarks (e.g., Stanford2D3D, Matterport3D, and Deep360) demonstrate strong performance and zero-shot generalization, with particularly robust and stable metric predictions in diverse real-world scenes. The project page can be found at: \\href{https://insta360-research-team.github.io/DAP_website/} {https://insta360-research-team.github.io/DAP\\_website/}",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "Project Page: https://insta360-research-team.github.io/DAP_website/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16913v1",
      "code_links": [
        {
          "url": "https://insta360-research-team.github.io/DAP_website/",
          "type": "project_page"
        },
        {
          "url": "https://insta360-research-team.github.io/DAP",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]depth estimation",
            "metric depth"
          ],
          "score": 8.0
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "geometric consistency"
          ],
          "score": 3.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]foundation model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 20.0,
      "hit_pillars": [
        "3_perception_slam",
        "7_retargeting",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "GeoPredict: Leveraging Predictive Kinematics and 3D Gaussian Geometry for Precise VLA Manipulation",
      "authors": [
        "Jingjing Qian",
        "Boyao Han",
        "Chen Shi",
        "Lei Xiao",
        "Long Yang",
        "Shaoshuai Shi",
        "Li Jiang"
      ],
      "arxiv_id": "2512.16811v1",
      "summary": "Vision-Language-Action (VLA) models achieve strong generalization in robotic manipulation but remain largely reactive and 2D-centric, making them unreliable in tasks that require precise 3D reasoning. We propose GeoPredict, a geometry-aware VLA framework that augments a continuous-action policy with predictive kinematic and geometric priors. GeoPredict introduces a trajectory-level module that encodes motion history and predicts multi-step 3D keypoint trajectories of robot arms, and a predictive 3D Gaussian geometry module that forecasts workspace geometry with track-guided refinement along future keypoint trajectories. These predictive modules serve exclusively as training-time supervision through depth-based rendering, while inference requires only lightweight additional query tokens without invoking any 3D decoding. Experiments on RoboCasa Human-50, LIBERO, and real-world manipulation tasks show that GeoPredict consistently outperforms strong VLA baselines, especially in geometry-intensive and spatially demanding scenarios.",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16811v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "vision-language-action",
            "[T]VLA"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "KineST: A Kinematics-guided Spatiotemporal State Space Model for Human Motion Tracking from Sparse Signals",
      "authors": [
        "Shuting Zhao",
        "Zeyu Xiao",
        "Xinrong Chen"
      ],
      "arxiv_id": "2512.16791v1",
      "summary": "Full-body motion tracking plays an essential role in AR/VR applications, bridging physical and virtual interactions. However, it is challenging to reconstruct realistic and diverse full-body poses based on sparse signals obtained by head-mounted displays, which are the main devices in AR/VR scenarios. Existing methods for pose reconstruction often incur high computational costs or rely on separately modeling spatial and temporal dependencies, making it difficult to balance accuracy, temporal coherence, and efficiency. To address this problem, we propose KineST, a novel kinematics-guided state space model, which effectively extracts spatiotemporal dependencies while integrating local and global pose perception. The innovation comes from two core ideas. Firstly, in order to better capture intricate joint relationships, the scanning strategy within the State Space Duality framework is reformulated into kinematics-guided bidirectional scanning, which embeds kinematic priors. Secondly, a mixed spatiotemporal representation learning approach is employed to tightly couple spatial and temporal contexts, balancing accuracy and smoothness. Additionally, a geometric angular velocity loss is introduced to impose physically meaningful constraints on rotational variations for further improving motion stability. Extensive experiments demonstrate that KineST has superior performance in both accuracy and temporal consistency within a lightweight framework. Project page: https://kaka-1314.github.io/KineST/",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "Accepted by AAAI 2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16791v1",
      "code_links": [
        {
          "url": "https://kaka-1314.github.io/KineST/",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]state space model",
            "representation learning"
          ],
          "score": 6.0
        },
        {
          "name": "支柱八：物理动画 (Physics-based Animation)",
          "id": "8_physics_animation",
          "matched_keywords": [
            "[T]spatiotemporal",
            "[T]motion tracking"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "2_algo_arch",
        "8_physics_animation"
      ]
    },
    {
      "title": "VERM: Leveraging Foundation Models to Create a Virtual Eye for Efficient 3D Robotic Manipulation",
      "authors": [
        "Yixiang Chen",
        "Yan Huang",
        "Keji He",
        "Peiyan Li",
        "Liang Wang"
      ],
      "arxiv_id": "2512.16724v1",
      "summary": "When performing 3D manipulation tasks, robots have to execute action planning based on perceptions from multiple fixed cameras. The multi-camera setup introduces substantial redundancy and irrelevant information, which increases computational costs and forces the model to spend extra training time extracting crucial task-relevant details. To filter out redundant information and accurately extract task-relevant features, we propose the VERM (Virtual Eye for Robotic Manipulation) method, leveraging the knowledge in foundation models to imagine a virtual task-adaptive view from the constructed 3D point cloud, which efficiently captures necessary information and mitigates occlusion. To facilitate 3D action planning and fine-grained manipulation, we further design a depth-aware module and a dynamic coarse-to-fine procedure. Extensive experimental results on both simulation benchmark RLBench and real-world evaluations demonstrate the effectiveness of our method, surpassing previous state-of-the-art methods while achieving 1.89x speedup in training time and 1.54x speedup in inference speed. More results can be found on our project website at https://verm-ral.github.io .",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "Accepted at RA-L 2025",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16724v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]foundation model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 15.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "AIMM: An AI-Driven Multimodal Framework for Detecting Social-Media-Influenced Stock Market Manipulation",
      "authors": [
        "Sandeep Neela"
      ],
      "arxiv_id": "2512.16103v1",
      "summary": "Market manipulation now routinely originates from coordinated social media campaigns, not isolated trades. Retail investors, regulators, and brokerages need tools that connect online narratives and coordination patterns to market behavior. We present AIMM, an AI-driven framework that fuses Reddit activity, bot and coordination indicators, and OHLCV market features into a daily AIMM Manipulation Risk Score for each ticker.\n  The system uses a parquet-native pipeline with a Streamlit dashboard that allows analysts to explore suspicious windows, inspect underlying posts and price action, and log model outputs over time. Due to Reddit API restrictions, we employ calibrated synthetic social features matching documented event characteristics; market data (OHLCV) uses real historical data from Yahoo Finance. This release makes three contributions. First, we build the AIMM Ground Truth dataset (AIMM-GT): 33 labeled ticker-days spanning eight equities, drawing from SEC enforcement actions, community-verified manipulation cases, and matched normal controls. Second, we implement forward-walk evaluation and prospective prediction logging for both retrospective and deployment-style assessment. Third, we analyze lead times and show that AIMM flagged GME 22 days before the January 2021 squeeze peak.\n  The current labeled set is small (33 ticker-days, 3 positive events), but results show preliminary discriminative capability and early warnings for the GME incident. We release the code, dataset schema, and dashboard design to support research on social media-driven market surveillance.",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "Preprint",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16103v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]multimodal"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 15.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "AlignMerge - Alignment-Preserving Large Language Model Merging via Fisher-Guided Geometric Constraints",
      "authors": [
        "Aniruddha Roy",
        "Jyoti Patel",
        "Aman Chadha",
        "Vinija Jain",
        "Amitava Das"
      ],
      "arxiv_id": "2512.16245v1",
      "summary": "Merging large language models (LLMs) is a practical way to compose capabilities from multiple fine-tuned checkpoints without retraining. Yet standard schemes (linear weight soups, task vectors, and Fisher-weighted averaging) can preserve loss while quietly destroying alignment. We argue that merging is not a numerical trick but a geometry-constrained operation around an already-aligned anchor: fusion must be steered to respect safety geometry, not validated post hoc.\n  We introduce AlignMerge, a geometry-aware merging framework that makes alignment an explicit invariant. In a local Fisher chart around an instruction-tuned base, we estimate an alignment subspace with projector P_A and optimize:\n  L_AlignMerge = L_geo + lambda_align * L_align + lambda_bud * L_bud,\n  where L_geo keeps the merge close to its experts in Fisher-Rao geometry, L_align penalizes motion along alignment-sensitive directions, and L_bud enforces a soft alignment budget. As the alignment functional we use the decoding-invariant Alignment Quality Index (AQI), a latent-space criterion that captures how cleanly aligned and misaligned behaviors separate in representation space.\n  Across five model families (LLaMA-3 8B, Mistral 7B, Qwen 2, Phi-3.5, Gemma 2), merging safety anchors with task experts, AlignMerge improves alignment metrics (AQI, toxicity, LLM-judge alignment) while matching or exceeding the best expert on instruction-following, reasoning, and helpfulness. It also exhibits smaller alignment-subspace drift and fewer budget violations than Fisher soups, TIES, SafeMerge, and MergeAlign. These results make alignment-preserving merging a first-class design goal and suggest a path to geometry-aware composition of future foundation models.",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16245v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model",
            "foundation model",
            "instruction following"
          ],
          "score": 15.0
        }
      ],
      "relevance_score": 15.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "BrepLLM: Native Boundary Representation Understanding with Large Language Models",
      "authors": [
        "Liyuan Deng",
        "Hao Guo",
        "Yunpeng Bai",
        "Yongkang Dai",
        "Huaxi Huang",
        "Yilei Shi"
      ],
      "arxiv_id": "2512.16413v1",
      "summary": "Current token-sequence-based Large Language Models (LLMs) are not well-suited for directly processing 3D Boundary Representation (Brep) models that contain complex geometric and topological information. We propose BrepLLM, the first framework that enables LLMs to parse and reason over raw Brep data, bridging the modality gap between structured 3D geometry and natural language. BrepLLM employs a two-stage training pipeline: Cross-modal Alignment Pre-training and Multi-stage LLM Fine-tuning. In the first stage, an adaptive UV sampling strategy converts Breps into graphs representation with geometric and topological information. We then design a hierarchical BrepEncoder to extract features from geometry (i.e., faces and edges) and topology, producing both a single global token and a sequence of node tokens. Then we align the global token with text embeddings from a frozen CLIP text encoder (ViT-L/14) via contrastive learning. In the second stage, we integrate the pretrained BrepEncoder into an LLM. We then align its sequence of node tokens using a three-stage progressive training strategy: (1) training an MLP-based semantic mapping from Brep representation to 2D with 2D-LLM priors. (2) performing fine-tuning of the LLM. (3) designing a Mixture-of-Query Experts (MQE) to enhance geometric diversity modeling. We also construct Brep2Text, a dataset comprising 269,444 Brep-text question-answer pairs. Experiments show that BrepLLM achieves state-of-the-art (SOTA) results on 3D object classification and captioning tasks.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16413v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "contrastive learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "semantic mapping",
            "semantic map"
          ],
          "score": 4.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 14.5,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "AdaSearch: Balancing Parametric Knowledge and Search in Large Language Models via Reinforcement Learning",
      "authors": [
        "Tzu-Han Lin",
        "Wei-Lin Chen",
        "Chen-An Li",
        "Hung-yi Lee",
        "Yun-Nung Chen",
        "Yu Meng"
      ],
      "arxiv_id": "2512.16883v1",
      "summary": "Equipping large language models (LLMs) with search engines via reinforcement learning (RL) has emerged as an effective approach for building search agents. However, overreliance on search introduces unnecessary cost and risks exposure to noisy or malicious content, while relying solely on parametric knowledge risks hallucination. The central challenge is to develop agents that adaptively balance parametric knowledge with external search, invoking search only when necessary. Prior work mitigates search overuse by shaping rewards around the number of tool calls. However, these penalties require substantial reward engineering, provide ambiguous credit assignment, and can be exploited by agents that superficially reduce calls. Moreover, evaluating performance solely through call counts conflates necessary and unnecessary search, obscuring the measurement of true adaptive behavior. To address these limitations, we first quantify the self-knowledge awareness of existing search agents via an F1-based decision metric, revealing that methods such as Search-R1 often overlook readily available parametric knowledge. Motivated by these findings, we propose AdaSearch, a simple two-stage, outcome-driven RL framework that disentangles problem solving from the decision of whether to invoke search, and makes this decision process explicit and interpretable. This transparency is crucial for high-stakes domains such as finance and medical question answering, yet is largely neglected by prior approaches. Experiments across multiple model families and sizes demonstrate that AdaSearch substantially improves knowledge-boundary awareness, reduces unnecessary search calls, preserves strong task performance, and offers more transparent, interpretable decision behaviors.",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "Preprint. Code and artifacts will be uploaded to https://github.com/hank0316/AdaSearch",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16883v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 13.5,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence",
      "authors": [
        "Xiaopeng Lin",
        "Shijie Lian",
        "Bin Yu",
        "Ruoqi Yang",
        "Changti Wu",
        "Yuzhuo Miao",
        "Yurun Jin",
        "Yukun Shi",
        "Cong Huang",
        "Bojun Cheng",
        "Kai Chen"
      ],
      "arxiv_id": "2512.16793v1",
      "summary": "Robotic generalization relies on physical intelligence: the ability to reason about state changes, contact-rich interactions, and long-horizon planning under egocentric perception and action. However, most VLMs are trained primarily on third-person data, creating a fundamental viewpoint mismatch for humanoid robots. Scaling robot egocentric data collection remains impractical due to high cost and limited diversity, whereas large-scale human egocentric videos offer a scalable alternative that naturally capture rich interaction context and causal structure. The key challenge is to convert raw egocentric videos into structured and reliable embodiment training supervision. Accordingly, we propose an Egocentric2Embodiment translation pipeline that transforms first-person videos into multi-level, schema-driven VQA supervision with enforced evidence grounding and temporal consistency, enabling the construction of the Egocentric2Embodiment dataset (E2E-3M) at scale. An egocentric-aware embodied brain, termed PhysBrain, is obtained by training on the E2E-3M dataset. PhysBrain exhibits substantially improved egocentric understanding, particularly for planning on EgoThink. It provides an egocentric-aware initialization that enables more sample-efficient VLA fine-tuning and higher SimplerEnv success rates (53.9\\%), demonstrating effective transfer from human egocentric supervision to downstream robot control.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "17 pages, 4 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16793v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "humanoid",
            "humanoid robot"
          ],
          "score": 4.0
        },
        {
          "name": "支柱六：视频提取与匹配 (Video Extraction)",
          "id": "6_video_extraction",
          "matched_keywords": [
            "[T]egocentric"
          ],
          "score": 6.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "VLA"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 13.0,
      "hit_pillars": [
        "1_robot_core",
        "6_video_extraction",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image",
      "authors": [
        "Yushi Hu",
        "Reyhane Askari-Hemmat",
        "Melissa Hall",
        "Emily Dinan",
        "Luke Zettlemoyer",
        "Marjan Ghazvininejad"
      ],
      "arxiv_id": "2512.16899v1",
      "summary": "Reward models (RMs) are essential for training large language models (LLMs), but remain underexplored for omni models that handle interleaved image and text sequences. We introduce Multimodal RewardBench 2 (MMRB2), the first comprehensive benchmark for reward models on multimodal understanding and (interleaved) generation. MMRB2 spans four tasks: text-to-image, image editing, interleaved generation, and multimodal reasoning (\"thinking-with-images\"), providing 1,000 expert-annotated preference pairs per task from 23 models and agents across 21 source tasks. MMRB2 is designed with: (1) practical but challenging prompts; (2) responses from state-of-the-art models and agents; and (3) preference pairs with strong human-expert consensus, curated via an ensemble filtering strategy. Using MMRB2, we study existing judges for each subtask, including multimodal LLM-as-a-judge and models trained with human preferences. The latest Gemini 3 Pro attains 75-80% accuracy. GPT-5 and Gemini 2.5 Pro reach 66-75% accuracy, compared to >90% for humans, yet surpass the widely used GPT-4o (59%). The best performing open-source model Qwen3-VL-32B achieves similar accuracies as Gemini 2.5 Flash (64%). We also show that MMRB2 performance strongly correlates with downstream task success using Best-of-N sampling and conduct an in-depth analysis that shows key areas to improve the reward models going forward.",
      "categories": [
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "Code and data available at https://github.com/facebookresearch/MMRB2",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16899v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model",
            "[T]multimodal"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Make-It-Poseable: Feed-forward Latent Posing Model for 3D Humanoid Character Animation",
      "authors": [
        "Zhiyang Guo",
        "Ori Zhang",
        "Jax Xiang",
        "Alan Zhao",
        "Wengang Zhou",
        "Houqiang Li"
      ],
      "arxiv_id": "2512.16767v1",
      "summary": "Posing 3D characters is a fundamental task in computer graphics and vision. However, existing methods like auto-rigging and pose-conditioned generation often struggle with challenges such as inaccurate skinning weight prediction, topological imperfections, and poor pose conformance, limiting their robustness and generalizability. To overcome these limitations, we introduce Make-It-Poseable, a novel feed-forward framework that reformulates character posing as a latent-space transformation problem. Instead of deforming mesh vertices as in traditional pipelines, our method reconstructs the character in new poses by directly manipulating its latent representation. At the core of our method is a latent posing transformer that manipulates shape tokens based on skeletal motion. This process is facilitated by a dense pose representation for precise control. To ensure high-fidelity geometry and accommodate topological changes, we also introduce a latent-space supervision strategy and an adaptive completion module. Our method demonstrates superior performance in posing quality. It also naturally extends to 3D editing applications like part replacement and refinement.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "Project page: https://jasongzy.github.io/Make-It-Poseable/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16767v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid"
          ],
          "score": 6.0
        },
        {
          "name": "支柱八：物理动画 (Physics-based Animation)",
          "id": "8_physics_animation",
          "matched_keywords": [
            "[T]character animation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "1_robot_core",
        "8_physics_animation"
      ]
    },
    {
      "title": "SDFoam: Signed-Distance Foam for explicit surface reconstruction",
      "authors": [
        "Antonella Rech",
        "Nicola Conci",
        "Nicola Garau"
      ],
      "arxiv_id": "2512.16706v1",
      "summary": "Neural radiance fields (NeRF) have driven impressive progress in view synthesis by using ray-traced volumetric rendering. Splatting-based methods such as 3D Gaussian Splatting (3DGS) provide faster rendering by rasterizing 3D primitives. RadiantFoam (RF) brought ray tracing back, achieving throughput comparable to Gaussian Splatting by organizing radiance with an explicit Voronoi Diagram (VD). Yet, all the mentioned methods still struggle with precise mesh reconstruction. We address this gap by jointly learning an explicit VD with an implicit Signed Distance Field (SDF). The scene is optimized via ray tracing and regularized by an Eikonal objective. The SDF introduces metric-consistent isosurfaces, which, in turn, bias near-surface Voronoi cell faces to align with the zero level set. The resulting model produces crisper, view-consistent surfaces with fewer floaters and improved topology, while preserving photometric quality and maintaining training speed on par with RadiantFoam. Across diverse scenes, our hybrid implicit-explicit formulation, which we name SDFoam, substantially improves mesh reconstruction accuracy (Chamfer distance) with comparable appearance (PSNR, SSIM), without sacrificing efficiency.",
      "categories": [
        "cs.CV",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16706v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "3D gaussian splatting",
            "3DGS",
            "gaussian splatting",
            "splatting",
            "NeRF",
            "neural radiance field"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models",
      "authors": [
        "Yuxin Wang",
        "Lei Ke",
        "Boqiang Zhang",
        "Tianyuan Qu",
        "Hanxun Yu",
        "Zhenpeng Huang",
        "Meng Yu",
        "Dan Xu",
        "Dong Yu"
      ],
      "arxiv_id": "2512.16561v1",
      "summary": "While current multimodal models can answer questions based on 2D images, they lack intrinsic 3D object perception, limiting their ability to comprehend spatial relationships and depth cues in 3D scenes. In this work, we propose N3D-VLM, a novel unified framework that seamlessly integrates native 3D object perception with 3D-aware visual reasoning, enabling both precise 3D grounding and interpretable spatial understanding. Unlike conventional end-to-end models that directly predict answers from RGB/RGB-D inputs, our approach equips the model with native 3D object perception capabilities, enabling it to directly localize objects in 3D space based on textual descriptions. Building upon accurate 3D object localization, the model further performs explicit reasoning in 3D, achieving more interpretable and structured spatial understanding. To support robust training for these capabilities, we develop a scalable data construction pipeline that leverages depth estimation to lift large-scale 2D annotations into 3D space, significantly increasing the diversity and coverage for 3D object grounding data, yielding over six times larger than the largest existing single-image 3D detection dataset. Moreover, the pipeline generates spatial question-answering datasets that target chain-of-thought (CoT) reasoning in 3D, facilitating joint training for both 3D object localization and 3D spatial reasoning. Experimental results demonstrate that our unified framework not only achieves state-of-the-art performance on 3D grounding tasks, but also consistently surpasses existing methods in 3D spatial reasoning in vision-language model.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "Project Page: https://n3d-vlm.github.io",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16561v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "depth estimation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "spatial relationship"
          ],
          "score": 3.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "multimodal",
            "chain-of-thought"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 11.0,
      "hit_pillars": [
        "3_perception_slam",
        "7_retargeting",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "SNOW: Spatio-Temporal Scene Understanding with World Knowledge for Open-World Embodied Reasoning",
      "authors": [
        "Tin Stribor Sohn",
        "Maximilian Dillitzer",
        "Jason J. Corso",
        "Eric Sax"
      ],
      "arxiv_id": "2512.16461v1",
      "summary": "Autonomous robotic systems require spatio-temporal understanding of dynamic environments to ensure reliable navigation and interaction. While Vision-Language Models (VLMs) provide open-world semantic priors, they lack grounding in 3D geometry and temporal dynamics. Conversely, geometric perception captures structure and motion but remains semantically sparse. We propose SNOW (Scene Understanding with Open-World Knowledge), a training-free and backbone-agnostic framework for unified 4D scene understanding that integrates VLM-derived semantics with point cloud geometry and temporal consistency. SNOW processes synchronized RGB images and 3D point clouds, using HDBSCAN clustering to generate object-level proposals that guide SAM2-based segmentation. Each segmented region is encoded through our proposed Spatio-Temporal Tokenized Patch Encoding (STEP), producing multimodal tokens that capture localized semantic, geometric, and temporal attributes. These tokens are incrementally integrated into a 4D Scene Graph (4DSG), which serves as 4D prior for downstream reasoning. A lightweight SLAM backend anchors all STEP tokens spatially in the environment, providing the global reference alignment, and ensuring unambiguous spatial grounding across time. The resulting 4DSG forms a queryable, unified world model through which VLMs can directly interpret spatial scene structure and temporal dynamics. Experiments on a diverse set of benchmarks demonstrate that SNOW enables precise 4D scene understanding and spatially grounded inference, thereby setting new state-of-the-art performance in several settings, highlighting the importance of structured 4D priors for embodied reasoning and autonomous robotics.",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16461v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "world model"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]scene understanding"
          ],
          "score": 6.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "multimodal"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 10.5,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "ManiLong-Shot: Interaction-Aware One-Shot Imitation Learning for Long-Horizon Manipulation",
      "authors": [
        "Zixuan Chen",
        "Chongkai Gao",
        "Lin Shao",
        "Jieqi Shi",
        "Jing Huo",
        "Yang Gao"
      ],
      "arxiv_id": "2512.16302v1",
      "summary": "One-shot imitation learning (OSIL) offers a promising way to teach robots new skills without large-scale data collection. However, current OSIL methods are primarily limited to short-horizon tasks, thus limiting their applicability to complex, long-horizon manipulations. To address this limitation, we propose ManiLong-Shot, a novel framework that enables effective OSIL for long-horizon prehensile manipulation tasks. ManiLong-Shot structures long-horizon tasks around physical interaction events, reframing the problem as sequencing interaction-aware primitives instead of directly imitating continuous trajectories. This primitive decomposition can be driven by high-level reasoning from a vision-language model (VLM) or by rule-based heuristics derived from robot state changes. For each primitive, ManiLong-Shot predicts invariant regions critical to the interaction, establishes correspondences between the demonstration and the current observation, and computes the target end-effector pose, enabling effective task execution. Extensive simulation experiments show that ManiLong-Shot, trained on only 10 short-horizon tasks, generalizes to 20 unseen long-horizon tasks across three difficulty levels via one-shot imitation, achieving a 22.8% relative improvement over the SOTA. Additionally, real-robot experiments validate ManiLong-Shot's ability to robustly execute three long-horizon manipulation tasks via OSIL, confirming its practical applicability.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "Accepted by AAAI 2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16302v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]imitation learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 10.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "AdaTooler-V: Adaptive Tool-Use for Images and Videos",
      "authors": [
        "Chaoyang Wang",
        "Kaituo Feng",
        "Dongyang Chen",
        "Zhongyu Wang",
        "Zhixun Li",
        "Sicheng Gao",
        "Meng Meng",
        "Xu Zhou",
        "Manyuan Zhang",
        "Yuzhang Shang",
        "Xiangyu Yue"
      ],
      "arxiv_id": "2512.16918v1",
      "summary": "Recent advances have shown that multimodal large language models (MLLMs) benefit from multimodal interleaved chain-of-thought (CoT) with vision tool interactions. However, existing open-source models often exhibit blind tool-use reasoning patterns, invoking vision tools even when they are unnecessary, which significantly increases inference overhead and degrades model performance. To this end, we propose AdaTooler-V, an MLLM that performs adaptive tool-use by determining whether a visual problem truly requires tools. First, we introduce AT-GRPO, a reinforcement learning algorithm that adaptively adjusts reward scales based on the Tool Benefit Score of each sample, encouraging the model to invoke tools only when they provide genuine improvements. Moreover, we construct two datasets to support training: AdaTooler-V-CoT-100k for SFT cold start and AdaTooler-V-300k for RL with verifiable rewards across single-image, multi-image, and video data. Experiments across twelve benchmarks demonstrate the strong reasoning capability of AdaTooler-V, outperforming existing methods in diverse visual reasoning tasks. Notably, AdaTooler-V-7B achieves an accuracy of 89.8\\% on the high-resolution benchmark V*, surpassing the commercial proprietary model GPT-4o and Gemini 1.5 Pro. All code, models, and data are released.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "Project page: https://github.com/CYWang735/AdaTooler-V",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16918v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model",
            "multimodal",
            "chain-of-thought"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 10.5,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Instant Expressive Gaussian Head Avatar via 3D-Aware Expression Distillation",
      "authors": [
        "Kaiwen Jiang",
        "Xueting Li",
        "Seonwook Park",
        "Ravi Ramamoorthi",
        "Shalini De Mello",
        "Koki Nagano"
      ],
      "arxiv_id": "2512.16893v1",
      "summary": "Portrait animation has witnessed tremendous quality improvements thanks to recent advances in video diffusion models. However, these 2D methods often compromise 3D consistency and speed, limiting their applicability in real-world scenarios, such as digital twins or telepresence. In contrast, 3D-aware facial animation feedforward methods -- built upon explicit 3D representations, such as neural radiance fields or Gaussian splatting -- ensure 3D consistency and achieve faster inference speed, but come with inferior expression details. In this paper, we aim to combine their strengths by distilling knowledge from a 2D diffusion-based method into a feed-forward encoder, which instantly converts an in-the-wild single image into a 3D-consistent, fast yet expressive animatable representation. Our animation representation is decoupled from the face's 3D representation and learns motion implicitly from data, eliminating the dependency on pre-defined parametric models that often constrain animation capabilities. Unlike previous computationally intensive global fusion mechanisms (e.g., multiple attention layers) for fusing 3D structural and animation information, our design employs an efficient lightweight local fusion strategy to achieve high animation expressivity. As a result, our method runs at 107.31 FPS for animation and pose control while achieving comparable animation quality to the state-of-the-art, surpassing alternative designs that trade speed for quality or vice versa. Project website is https://research.nvidia.com/labs/amri/projects/instant4d",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "Project website is https://research.nvidia.com/labs/amri/projects/instant4d",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16893v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]distillation"
          ],
          "score": 4.5
        },
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "gaussian splatting",
            "splatting",
            "neural radiance field"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 10.5,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning",
      "authors": [
        "Qihao Liu",
        "Luoxin Ye",
        "Wufei Ma",
        "Yu-Cheng Chou",
        "Alan Yuille"
      ],
      "arxiv_id": "2512.16917v1",
      "summary": "Large language models (LLMs) with explicit reasoning capabilities excel at mathematical reasoning yet still commit process errors, such as incorrect calculations, brittle logic, and superficially plausible but invalid steps. In this paper, we introduce Generative Adversarial Reasoner, an on-policy joint training framework designed to enhance reasoning by co-evolving an LLM reasoner and an LLM-based discriminator through adversarial reinforcement learning. A compute-efficient review schedule partitions each reasoning chain into logically complete slices of comparable length, and the discriminator evaluates each slice's soundness with concise, structured justifications. Learning couples complementary signals: the LLM reasoner is rewarded for logically consistent steps that yield correct answers, while the discriminator earns rewards for correctly detecting errors or distinguishing traces in the reasoning process. This produces dense, well-calibrated, on-policy step-level rewards that supplement sparse exact-match signals, improving credit assignment, increasing sample efficiency, and enhancing overall reasoning quality of LLMs. Across various mathematical benchmarks, the method delivers consistent gains over strong baselines with standard RL post-training. Specifically, on AIME24, we improve DeepSeek-R1-Distill-Qwen-7B from 54.0 to 61.3 (+7.3) and DeepSeek-R1-Distill-Llama-8B from 43.7 to 53.7 (+10.0). The modular discriminator also enables flexible reward shaping for objectives such as teacher distillation, preference alignment, and mathematical proof-based reasoning.",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16917v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning",
            "distillation",
            "reward shaping"
          ],
          "score": 7.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 10.5,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Non-Asymptotic Global Convergence of PPO-Clip",
      "authors": [
        "Yin Liu",
        "Qiming Dai",
        "Junyu Zhang",
        "Zaiwen Wen"
      ],
      "arxiv_id": "2512.16565v1",
      "summary": "Reinforcement learning (RL) has gained attention for aligning large language models (LLMs) via reinforcement learning from human feedback (RLHF). The actor-only variants of Proximal Policy Optimization (PPO) are widely applied for their efficiency. These algorithms incorporate a clipping mechanism to improve stability. Besides, a regularization term, such as the reverse KL-divergence or a more general \\(f\\)-divergence, is introduced to prevent policy drift. Despite their empirical success, a rigorous theoretical understanding of the problem and the algorithm's properties is limited. This paper advances the theoretical foundations of the PPO-Clip algorithm by analyzing a deterministic actor-only PPO algorithm within the general RL setting with \\(f\\)-divergence regularization under the softmax policy parameterization. We derive a non-uniform Lipschitz smoothness condition and a Łojasiewicz inequality for the considered problem. Based on these, a non-asymptotic linear convergence rate to the globally optimal policy is established for the forward KL-regularizer. Furthermore, stationary convergence and local linear convergence are derived for the reverse KL-regularizer.",
      "categories": [
        "math.OC",
        "cs.LG"
      ],
      "primary_category": "math.OC",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16565v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "[T]PPO",
            "RLHF"
          ],
          "score": 7.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 10.5,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "The Evolution of Reranking Models in Information Retrieval: From Heuristic Methods to Large Language Models",
      "authors": [
        "Tejul Pandit",
        "Sakshi Mahendru",
        "Meet Raval",
        "Dhvani Upadhyay"
      ],
      "arxiv_id": "2512.16236v1",
      "summary": "Reranking is a critical stage in contemporary information retrieval (IR) systems, improving the relevance of the user-presented final results by honing initial candidate sets. This paper is a thorough guide to examine the changing reranker landscape and offer a clear view of the advancements made in reranking methods. We present a comprehensive survey of reranking models employed in IR, particularly within modern Retrieval Augmented Generation (RAG) pipelines, where retrieved documents notably influence output quality.\n  We embark on a chronological journey through the historical trajectory of reranking techniques, starting with foundational approaches, before exploring the wide range of sophisticated neural network architectures such as cross-encoders, sequence-generation models like T5, and Graph Neural Networks (GNNs) utilized for structural information. Recognizing the computational cost of advancing neural rerankers, we analyze techniques for enhancing efficiency, notably knowledge distillation for creating competitive, lighter alternatives. Furthermore, we map the emerging territory of integrating Large Language Models (LLMs) in reranking, examining novel prompting strategies and fine-tuning tactics. This survey seeks to elucidate the fundamental ideas, relative effectiveness, computational features, and real-world trade-offs of various reranking strategies. The survey provides a structured synthesis of the diverse reranking paradigms, highlighting their underlying principles and comparative strengths and weaknesses.",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "15 pages, 1 figure, Accepted in CLNLP'25",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16236v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "distillation"
          ],
          "score": 1.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 10.5,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "ReinforceGen: Hybrid Skill Policies with Automated Data Generation and Reinforcement Learning",
      "authors": [
        "Zihan Zhou",
        "Animesh Garg",
        "Ajay Mandlekar",
        "Caelan Garrett"
      ],
      "arxiv_id": "2512.16861v1",
      "summary": "Long-horizon manipulation has been a long-standing challenge in the robotics community. We propose ReinforceGen, a system that combines task decomposition, data generation, imitation learning, and motion planning to form an initial solution, and improves each component through reinforcement-learning-based fine-tuning. ReinforceGen first segments the task into multiple localized skills, which are connected through motion planning. The skills and motion planning targets are trained with imitation learning on a dataset generated from 10 human demonstrations, and then fine-tuned through online adaptation and reinforcement learning. When benchmarked on the Robosuite dataset, ReinforceGen reaches 80% success rate on all tasks with visuomotor controls in the highest reset range setting. Additional ablation studies show that our fine-tuning approaches contributes to an 89% average performance increase. More results and videos available in https://reinforcegen.github.io/",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16861v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "motion planning"
          ],
          "score": 4.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning",
            "imitation learning"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 10.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "SARMAE: Masked Autoencoder for SAR Representation Learning",
      "authors": [
        "Danxu Liu",
        "Di Wang",
        "Hebaixu Wang",
        "Haoyang Chen",
        "Wentao Jiang",
        "Yilin Cheng",
        "Haonan Guo",
        "Wei Cui",
        "Jing Zhang"
      ],
      "arxiv_id": "2512.16635v1",
      "summary": "Synthetic Aperture Radar (SAR) imagery plays a critical role in all-weather, day-and-night remote sensing applications. However, existing SAR-oriented deep learning is constrained by data scarcity, while the physically grounded speckle noise in SAR imagery further hampers fine-grained semantic representation learning. To address these challenges, we propose SARMAE, a Noise-Aware Masked Autoencoder for self-supervised SAR representation learning. Specifically, we construct SAR-1M, the first million-scale SAR dataset, with additional paired optical images, to enable large-scale pre-training. Building upon this, we design Speckle-Aware Representation Enhancement (SARE), which injects SAR-specific speckle noise into masked autoencoders to facilitate noise-aware and robust representation learning. Furthermore, we introduce Semantic Anchor Representation Constraint (SARC), which leverages paired optical priors to align SAR features and ensure semantic consistency. Extensive experiments across multiple SAR datasets demonstrate that SARMAE achieves state-of-the-art performance on classification, detection, and segmentation tasks. Code and models will be available at https://github.com/MiliLab/SARMAE.",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "Code and models will be available at https://github.com/MiliLab/SARMAE",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16635v1",
      "code_links": [
        {
          "url": "https://github.com/MiliLab/SARMAE",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]representation learning",
            "[T]masked autoencoder"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Causal-Tune: Mining Causal Factors from Vision Foundation Models for Domain Generalized Semantic Segmentation",
      "authors": [
        "Yin Zhang",
        "Yongqiang Zhang",
        "Yaoyue Zheng",
        "Bogdan Raducanu",
        "Dan Liu"
      ],
      "arxiv_id": "2512.16567v1",
      "summary": "Fine-tuning Vision Foundation Models (VFMs) with a small number of parameters has shown remarkable performance in Domain Generalized Semantic Segmentation (DGSS). Most existing works either train lightweight adapters or refine intermediate features to achieve better generalization on unseen domains. However, they both overlook the fact that long-term pre-trained VFMs often exhibit artifacts, which hinder the utilization of valuable representations and ultimately degrade DGSS performance. Inspired by causal mechanisms, we observe that these artifacts are associated with non-causal factors, which usually reside in the low- and high-frequency components of the VFM spectrum. In this paper, we explicitly examine the causal and non-causal factors of features within VFMs for DGSS, and propose a simple yet effective method to identify and disentangle them, enabling more robust domain generalization. Specifically, we propose Causal-Tune, a novel fine-tuning strategy designed to extract causal factors and suppress non-causal ones from the features of VFMs. First, we extract the frequency spectrum of features from each layer using the Discrete Cosine Transform (DCT). A Gaussian band-pass filter is then applied to separate the spectrum into causal and non-causal components. To further refine the causal components, we introduce a set of causal-aware learnable tokens that operate in the frequency domain, while the non-causal components are discarded. Finally, refined features are transformed back into the spatial domain via inverse DCT and passed to the next layer. Extensive experiments conducted on various cross-domain tasks demonstrate the effectiveness of Causal-Tune. In particular, our method achieves superior performance under adverse weather conditions, improving +4.8% mIoU over the baseline in snow conditions.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "Accepted by AAAI 2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16567v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]foundation model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Smile on the Face, Sadness in the Eyes: Bridging the Emotion Gap with a Multimodal Dataset of Eye and Facial Behaviors",
      "authors": [
        "Kejun Liu",
        "Yuanyuan Liu",
        "Lin Wei",
        "Chang Tang",
        "Yibing Zhan",
        "Zijing Chen",
        "Zhe Chen"
      ],
      "arxiv_id": "2512.16485v1",
      "summary": "Emotion Recognition (ER) is the process of analyzing and identifying human emotions from sensing data. Currently, the field heavily relies on facial expression recognition (FER) because visual channel conveys rich emotional cues. However, facial expressions are often used as social tools rather than manifestations of genuine inner emotions. To understand and bridge this gap between FER and ER, we introduce eye behaviors as an important emotional cue and construct an Eye-behavior-aided Multimodal Emotion Recognition (EMER) dataset. To collect data with genuine emotions, spontaneous emotion induction paradigm is exploited with stimulus material, during which non-invasive eye behavior data, like eye movement sequences and eye fixation maps, is captured together with facial expression videos. To better illustrate the gap between ER and FER, multi-view emotion labels for mutimodal ER and FER are separately annotated. Furthermore, based on the new dataset, we design a simple yet effective Eye-behavior-aided MER Transformer (EMERT) that enhances ER by bridging the emotion gap. EMERT leverages modality-adversarial feature decoupling and a multitask Transformer to model eye behaviors as a strong complement to facial expressions. In the experiment, we introduce seven multimodal benchmark protocols for a variety of comprehensive evaluations of the EMER dataset. The results show that the EMERT outperforms other state-of-the-art multimodal methods by a great margin, revealing the importance of modeling eye behaviors for robust ER. To sum up, we provide a comprehensive analysis of the importance of eye behaviors in ER, advancing the study on addressing the gap between FER and ER for more robust ER performance. Our EMER dataset and the trained EMERT models will be publicly available at https://github.com/kejun1/EMER.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "Accepted by TMM",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16485v1",
      "code_links": [
        {
          "url": "https://github.com/kejun1/EMER",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]multimodal"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Pretrained Battery Transformer (PBT): A battery life prediction foundation model",
      "authors": [
        "Ruifeng Tan",
        "Weixiang Hong",
        "Jia Li",
        "Jiaqiang Huang",
        "Tong-Yi Zhang"
      ],
      "arxiv_id": "2512.16334v1",
      "summary": "Early prediction of battery cycle life is essential for accelerating battery research, manufacturing, and deployment. Although machine learning methods have shown encouraging results, progress is hindered by data scarcity and heterogeneity arising from diverse aging conditions. In other fields, foundation models (FMs) trained on diverse datasets have achieved broad generalization through transfer learning, but no FMs have been reported for battery cycle life prediction yet. Here we present the Pretrained Battery Transformer (PBT), the first FM for battery life prediction, developed through domain-knowledge-encoded mixture-of-expert layers. Validated on the largest public battery life database, PBT learns transferable representations from 13 lithium-ion battery (LIB) datasets, outperforming existing models by an average of 19.8%. With transfer learning, PBT achieves state-of-the-art performance across 15 diverse datasets encompassing various operating conditions, formation protocols, and chemistries of LIBs. This work establishes a foundation model pathway for battery lifetime prediction, paving the way toward universal battery lifetime prediction systems.",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "5 figures in the main content",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16334v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]foundation model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Coarse-to-Fine Open-Set Graph Node Classification with Large Language Models",
      "authors": [
        "Xueqi Ma",
        "Xingjun Ma",
        "Sarah Monazam Erfani",
        "Danilo Mandic",
        "James Bailey"
      ],
      "arxiv_id": "2512.16244v1",
      "summary": "Developing open-set classification methods capable of classifying in-distribution (ID) data while detecting out-of-distribution (OOD) samples is essential for deploying graph neural networks (GNNs) in open-world scenarios. Existing methods typically treat all OOD samples as a single class, despite real-world applications, especially high-stake settings such as fraud detection and medical diagnosis, demanding deeper insights into OOD samples, including their probable labels. This raises a critical question: can OOD detection be extended to OOD classification without true label information? To address this question, we propose a Coarse-to-Fine open-set Classification (CFC) framework that leverages large language models (LLMs) for graph datasets. CFC consists of three key components: a coarse classifier that uses LLM prompts for OOD detection and outlier label generation, a GNN-based fine classifier trained with OOD samples identified by the coarse classifier for enhanced OOD detection and ID classification, and refined OOD classification achieved through LLM prompts and post-processed OOD labels. Unlike methods that rely on synthetic or auxiliary OOD samples, CFC employs semantic OOD instances that are genuinely out-of-distribution based on their inherent meaning, improving interpretability and practical utility. Experimental results show that CFC improves OOD detection by ten percent over state-of-the-art methods on graph and text domains and achieves up to seventy percent accuracy in OOD classification on graph datasets.",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "Accepted to AAAI 2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16244v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "A Multimodal Approach to Alzheimer's Diagnosis: Geometric Insights from Cube Copying and Cognitive Assessments",
      "authors": [
        "Jaeho Yang",
        "Kijung Yoon"
      ],
      "arxiv_id": "2512.16184v1",
      "summary": "Early and accessible detection of Alzheimer's disease (AD) remains a critical clinical challenge, and cube-copying tasks offer a simple yet informative assessment of visuospatial function. This work proposes a multimodal framework that converts hand-drawn cube sketches into graph-structured representations capturing geometric and topological properties, and integrates these features with demographic information and neuropsychological test (NPT) scores for AD classification. Cube drawings are modeled as graphs with node features encoding spatial coordinates, local graphlet-based topology, and angular geometry, which are processed using graph neural networks and fused with age, education, and NPT features in a late-fusion model. Experimental results show that graph-based representations provide a strong unimodal baseline and substantially outperform pixel-based convolutional models, while multimodal integration further improves performance and robustness to class imbalance. SHAP-based interpretability analysis identifies specific graphlet motifs and geometric distortions as key predictors, closely aligning with clinical observations of disorganized cube drawings in AD. Together, these results establish graph-based analysis of cube copying as an interpretable, non-invasive, and scalable approach for Alzheimer's disease screening.",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16184v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]multimodal"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "TOGGLE: Temporal Logic-Guided Large Language Model Compression for Edge",
      "authors": [
        "Khurram Khalil",
        "Khaza Anuarul Hoque"
      ],
      "arxiv_id": "2512.16855v1",
      "summary": "Large Language Models (LLMs) deliver exceptional performance across natural language tasks but demand substantial computational resources, limiting their deployment on resource-constrained edge devices. Existing compression techniques, such as quantization and pruning, often degrade critical linguistic properties and lack formal guarantees for preserving model behavior. We propose Temporal Logic-Guided Large Language Model Compression (TOGGLE), a novel framework that leverages Signal Temporal Logic (STL) to formally specify and enforce linguistic properties during compression. TOGGLE employs an STL robustness-guided Bayesian optimization to systematically explore layer-wise quantization and pruning configurations, generating compressed models that formally satisfy specified linguistic constraints without retraining or fine-tuning. Evaluating TOGGLE on four LLM architectures (GPT-2, DeepSeek-V2 7B, LLaMA 3 8B, and Mistral 7B), we achieve up to 3.3x reduction in computational costs (FLOPs) and up to a 68.8% reduction in model size while satisfying all linguistic properties. TOGGLE represents the first integration of formal methods into LLM compression, enabling efficient, verifiable deployment of LLMs on edge hardware.",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "Published in the IEEE ICCAD 2025 conference",
      "doi": "10.1109/ICCAD66269.2025.11240962",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16855v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Prefix Probing: Lightweight Harmful Content Detection for Large Language Models",
      "authors": [
        "Jirui Yang",
        "Hengqi Guo",
        "Zhihui Lu",
        "Yi Zhao",
        "Yuansen Zhang",
        "Shijing Hu",
        "Qiang Duan",
        "Yinggui Wang",
        "Tao Wei"
      ],
      "arxiv_id": "2512.16650v1",
      "summary": "Large language models often face a three-way trade-off among detection accuracy, inference latency, and deployment cost when used in real-world safety-sensitive applications. This paper introduces Prefix Probing, a black-box harmful content detection method that compares the conditional log-probabilities of \"agreement/execution\" versus \"refusal/safety\" opening prefixes and leverages prefix caching to reduce detection overhead to near first-token latency. During inference, the method requires only a single log-probability computation over the probe prefixes to produce a harmfulness score and apply a threshold, without invoking any additional models or multi-stage inference. To further enhance the discriminative power of the prefixes, we design an efficient prefix construction algorithm that automatically discovers highly informative prefixes, substantially improving detection performance. Extensive experiments demonstrate that Prefix Probing achieves detection effectiveness comparable to mainstream external safety models while incurring only minimal computational cost and requiring no extra model deployment, highlighting its strong practicality and efficiency.",
      "categories": [
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.AI",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16650v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "TimeSeries2Report prompting enables adaptive large language model management of lithium-ion batteries",
      "authors": [
        "Jiayang Yang",
        "Chunhui Zhao",
        "Martin Guay",
        "Zhixing Cao"
      ],
      "arxiv_id": "2512.16453v1",
      "summary": "Large language models (LLMs) offer promising capabilities for interpreting multivariate time-series data, yet their application to real-world battery energy storage system (BESS) operation and maintenance remains largely unexplored. Here, we present TimeSeries2Report (TS2R), a prompting framework that converts raw lithium-ion battery operational time-series into structured, semantically enriched reports, enabling LLMs to reason, predict, and make decisions in BESS management scenarios. TS2R encodes short-term temporal dynamics into natural language through a combination of segmentation, semantic abstraction, and rule-based interpretation, effectively bridging low-level sensor signals with high-level contextual insights. We benchmark TS2R across both lab-scale and real-world datasets, evaluating report quality and downstream task performance in anomaly detection, state-of-charge prediction, and charging/discharging management. Compared with vision-, embedding-, and text-based prompting baselines, report-based prompting via TS2R consistently improves LLM performance in terms of across accuracy, robustness, and explainability metrics. Notably, TS2R-integrated LLMs achieve expert-level decision quality and predictive consistency without retraining or architecture modification, establishing a practical path for adaptive, LLM-driven battery intelligence.",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16453v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "An Information-Theoretic Framework for Robust Large Language Model Editing",
      "authors": [
        "Qizhou Chen",
        "Chengyu Wang",
        "Taolin Zhang",
        "Xiaofeng He"
      ],
      "arxiv_id": "2512.16227v1",
      "summary": "Large Language Models (LLMs) have become indispensable tools in science, technology, and society, enabling transformative advances across diverse fields. However, errors or outdated information within these models can undermine their accuracy and restrict their safe deployment. Developing efficient strategies for updating model knowledge without the expense and disruption of full retraining remains a critical challenge. Current model editing techniques frequently struggle to generalize corrections beyond narrow domains, leading to unintended consequences and limiting their practical impact. Here, we introduce a novel framework for editing LLMs, grounded in information bottleneck theory. This approach precisely compresses and isolates the essential information required for generalizable knowledge correction while minimizing disruption to unrelated model behaviors. Building upon this foundation, we present the Information Bottleneck Knowledge Editor (IBKE), which leverages compact latent representations to guide gradient-based updates, enabling robust and broadly applicable model editing. We validate IBKE's effectiveness across multiple LLM architectures and standard benchmark tasks, demonstrating state-of-the-art accuracy and improved generality and specificity of edits. These findings establish a theoretically principled and practical paradigm for open-domain knowledge editing, advancing the utility and trustworthiness of LLMs in real-world applications.",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16227v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "A Multi-Agent Large Language Model Framework for Automated Qualitative Analysis",
      "authors": [
        "Qidi Xu",
        "Nuzha Amjad",
        "Grace Giles",
        "Alexa Cumming",
        "De'angelo Hermesky",
        "Alexander Wen",
        "Min Ji Kwak",
        "Yejin Kim"
      ],
      "arxiv_id": "2512.16063v1",
      "summary": "Understanding patients experiences is essential for advancing patient centered care, especially in chronic diseases that require ongoing communication. However, qualitative thematic analysis, the primary approach for exploring these experiences, remains labor intensive, subjective, and difficult to scale. In this study, we developed a multi agent large language model framework that automates qualitative thematic analysis through three agents (Instructor, Thematizer, CodebookGenerator), named Collaborative Theme Identification Agent (CoTI). We applied CoTI to 12 heart failure patient interviews to analyze their perceptions of medication intensity. CoTI identified key phrases, themes, and codebook that were more similar to those of the senior investigator than both junior investigators and baseline NLP models. We also implemented CoTI into a user-facing application to enable AI human interaction in qualitative analysis. However, collaboration between CoTI and junior investigators provided only marginal gains, suggesting they may overrely on CoTI and limit their independent critical thinking.",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "42 pages, 5 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16063v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "DualGuard: Dual-stream Large Language Model Watermarking Defense against Paraphrase and Spoofing Attack",
      "authors": [
        "Hao Li",
        "Yubing Ren",
        "Yanan Cao",
        "Yingjie Li",
        "Fang Fang",
        "Shi Wang",
        "Li Guo"
      ],
      "arxiv_id": "2512.16182v1",
      "summary": "With the rapid development of cloud-based services, large language models (LLMs) have become increasingly accessible through various web platforms. However, this accessibility has also led to growing risks of model abuse. LLM watermarking has emerged as an effective approach to mitigate such misuse and protect intellectual property. Existing watermarking algorithms, however, primarily focus on defending against paraphrase attacks while overlooking piggyback spoofing attacks, which can inject harmful content, compromise watermark reliability, and undermine trust in attribution. To address this limitation, we propose DualGuard, the first watermarking algorithm capable of defending against both paraphrase and spoofing attacks. DualGuard employs the adaptive dual-stream watermarking mechanism, in which two complementary watermark signals are dynamically injected based on the semantic content. This design enables DualGuard not only to detect but also to trace spoofing attacks, thereby ensuring reliable and trustworthy watermark detection. Extensive experiments conducted across multiple datasets and language models demonstrate that DualGuard achieves excellent detectability, robustness, traceability, and text quality, effectively advancing the state of LLM watermarking for real-world applications.",
      "categories": [
        "cs.CR",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16182v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Flowing from Reasoning to Motion: Learning 3D Hand Trajectory Prediction from Egocentric Human Interaction Videos",
      "authors": [
        "Mingfei Chen",
        "Yifan Wang",
        "Zhengqin Li",
        "Homanga Bharadhwaj",
        "Yujin Chen",
        "Chuan Qin",
        "Ziyi Kou",
        "Yuan Tian",
        "Eric Whitmire",
        "Rajinder Sodhi",
        "Hrvoje Benko",
        "Eli Shlizerman",
        "Yue Liu"
      ],
      "arxiv_id": "2512.16907v1",
      "summary": "Prior works on 3D hand trajectory prediction are constrained by datasets that decouple motion from semantic supervision and by models that weakly link reasoning and action. To address these, we first present the EgoMAN dataset, a large-scale egocentric dataset for interaction stage-aware 3D hand trajectory prediction with 219K 6DoF trajectories and 3M structured QA pairs for semantic, spatial, and motion reasoning. We then introduce the EgoMAN model, a reasoning-to-motion framework that links vision-language reasoning and motion generation via a trajectory-token interface. Trained progressively to align reasoning with motion dynamics, our approach yields accurate and stage-aware trajectories with generalization across real-world scenes.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "Project website: https://egoman-project.github.io",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16907v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱四：生成式动作 (Generative Motion)",
          "id": "4_motion_diffusion",
          "matched_keywords": [
            "motion generation"
          ],
          "score": 2.5
        },
        {
          "name": "支柱六：视频提取与匹配 (Video Extraction)",
          "id": "6_video_extraction",
          "matched_keywords": [
            "[T]egocentric"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 8.5,
      "hit_pillars": [
        "4_motion_diffusion",
        "6_video_extraction"
      ]
    },
    {
      "title": "The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text",
      "authors": [
        "Hanlin Wang",
        "Hao Ouyang",
        "Qiuyu Wang",
        "Yue Yu",
        "Yihao Meng",
        "Wen Wang",
        "Ka Leong Cheng",
        "Shuailei Ma",
        "Qingyan Bai",
        "Yixuan Li",
        "Cheng Chen",
        "Yanhong Zeng",
        "Xing Zhu",
        "Yujun Shen",
        "Qifeng Chen"
      ],
      "arxiv_id": "2512.16924v1",
      "summary": "We present WorldCanvas, a framework for promptable world events that enables rich, user-directed simulation by combining text, trajectories, and reference images. Unlike text-only approaches and existing trajectory-controlled image-to-video methods, our multimodal approach combines trajectories -- encoding motion, timing, and visibility -- with natural language for semantic intent and reference images for visual grounding of object identity, enabling the generation of coherent, controllable events that include multi-agent interactions, object entry/exit, reference-guided appearance and counterintuitive events. The resulting videos demonstrate not only temporal coherence but also emergent consistency, preserving object identity and scene despite temporary disappearance. By supporting expressive world events generation, WorldCanvas advances world models from passive predictors to interactive, user-shaped simulators. Our project page is available at: https://worldcanvas.github.io/.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "Project page and code: https://worldcanvas.github.io/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16924v1",
      "code_links": [
        {
          "url": "https://worldcanvas.github.io/",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "world model"
          ],
          "score": 1.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "multimodal",
            "visual grounding"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 7.5,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Task-Oriented Data Synthesis and Control-Rectify Sampling for Remote Sensing Semantic Segmentation",
      "authors": [
        "Yunkai Yang",
        "Yudong Zhang",
        "Kunquan Zhang",
        "Jinxiao Zhang",
        "Xinying Chen",
        "Haohuan Fu",
        "Runmin Dong"
      ],
      "arxiv_id": "2512.16740v1",
      "summary": "With the rapid progress of controllable generation, training data synthesis has become a promising way to expand labeled datasets and alleviate manual annotation in remote sensing (RS). However, the complexity of semantic mask control and the uncertainty of sampling quality often limit the utility of synthetic data in downstream semantic segmentation tasks. To address these challenges, we propose a task-oriented data synthesis framework (TODSynth), including a Multimodal Diffusion Transformer (MM-DiT) with unified triple attention and a plug-and-play sampling strategy guided by task feedback. Built upon the powerful DiT-based generative foundation model, we systematically evaluate different control schemes, showing that a text-image-mask joint attention scheme combined with full fine-tuning of the image and mask branches significantly enhances the effectiveness of RS semantic segmentation data synthesis, particularly in few-shot and complex-scene scenarios. Furthermore, we propose a control-rectify flow matching (CRFM) method, which dynamically adjusts sampling directions guided by semantic loss during the early high-plasticity stage, mitigating the instability of generated images and bridging the gap between synthetic data and downstream segmentation tasks. Extensive experiments demonstrate that our approach consistently outperforms state-of-the-art controllable generation methods, producing more stable and task-oriented synthetic data for RS semantic segmentation.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16740v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "flow matching"
          ],
          "score": 1.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "foundation model",
            "multimodal"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 7.5,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "OPENTOUCH: Bringing Full-Hand Touch to Real-World Interaction",
      "authors": [
        "Yuxin Ray Song",
        "Jinzhou Li",
        "Rao Fu",
        "Devin Murphy",
        "Kaichen Zhou",
        "Rishi Shiv",
        "Yaqi Li",
        "Haoyu Xiong",
        "Crystal Elaine Owens",
        "Yilun Du",
        "Yiyue Luo",
        "Xianyi Cheng",
        "Antonio Torralba",
        "Wojciech Matusik",
        "Paul Pu Liang"
      ],
      "arxiv_id": "2512.16842v1",
      "summary": "The human hand is our primary interface to the physical world, yet egocentric perception rarely knows when, where, or how forcefully it makes contact. Robust wearable tactile sensors are scarce, and no existing in-the-wild datasets align first-person video with full-hand touch. To bridge the gap between visual perception and physical interaction, we present OpenTouch, the first in-the-wild egocentric full-hand tactile dataset, containing 5.1 hours of synchronized video-touch-pose data and 2,900 curated clips with detailed text annotations. Using OpenTouch, we introduce retrieval and classification benchmarks that probe how touch grounds perception and action. We show that tactile signals provide a compact yet powerful cue for grasp understanding, strengthen cross-modal alignment, and can be reliably retrieved from in-the-wild video queries. By releasing this annotated vision-touch-pose dataset and benchmark, we aim to advance multimodal egocentric perception, embodied learning, and contact-rich robotic manipulation.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "https://opentouch-tactile.github.io/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16842v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱六：视频提取与匹配 (Video Extraction)",
          "id": "6_video_extraction",
          "matched_keywords": [
            "egocentric"
          ],
          "score": 2.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "multimodal"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 7.0,
      "hit_pillars": [
        "1_robot_core",
        "6_video_extraction",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Kling-Omni Technical Report",
      "authors": [
        "Kling Team",
        "Jialu Chen",
        "Yuanzheng Ci",
        "Xiangyu Du",
        "Zipeng Feng",
        "Kun Gai",
        "Sainan Guo",
        "Feng Han",
        "Jingbin He",
        "Kang He",
        "Xiao Hu",
        "Xiaohua Hu",
        "Boyuan Jiang",
        "Fangyuan Kong",
        "Hang Li",
        "Jie Li",
        "Qingyu Li",
        "Shen Li",
        "Xiaohan Li",
        "Yan Li",
        "Jiajun Liang",
        "Borui Liao",
        "Yiqiao Liao",
        "Weihong Lin",
        "Quande Liu",
        "Xiaokun Liu",
        "Yilun Liu",
        "Yuliang Liu",
        "Shun Lu",
        "Hangyu Mao",
        "Yunyao Mao",
        "Haodong Ouyang",
        "Wenyu Qin",
        "Wanqi Shi",
        "Xiaoyu Shi",
        "Lianghao Su",
        "Haozhi Sun",
        "Peiqin Sun",
        "Pengfei Wan",
        "Chao Wang",
        "Chenyu Wang",
        "Meng Wang",
        "Qiulin Wang",
        "Runqi Wang",
        "Xintao Wang",
        "Xuebo Wang",
        "Zekun Wang",
        "Min Wei",
        "Tiancheng Wen",
        "Guohao Wu",
        "Xiaoshi Wu",
        "Zhenhua Wu",
        "Da Xie",
        "Yingtong Xiong",
        "Yulong Xu",
        "Sile Yang",
        "Zikang Yang",
        "Weicai Ye",
        "Ziyang Yuan",
        "Shenglong Zhang",
        "Shuaiyu Zhang",
        "Yuanxing Zhang",
        "Yufan Zhang",
        "Wenzheng Zhao",
        "Ruiliang Zhou",
        "Yan Zhou",
        "Guosheng Zhu",
        "Yongjie Zhu"
      ],
      "arxiv_id": "2512.16776v1",
      "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "Kling-Omni Technical Report",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16776v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "multimodal",
            "instruction following"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Sketch-in-Latents: Eliciting Unified Reasoning in MLLMs",
      "authors": [
        "Jintao Tong",
        "Jiaqi Gu",
        "Yujing Lou",
        "Lubin Fan",
        "Yixiong Zou",
        "Yue Wu",
        "Jieping Ye",
        "Ruixuan Li"
      ],
      "arxiv_id": "2512.16584v1",
      "summary": "While Multimodal Large Language Models (MLLMs) excel at visual understanding tasks through text reasoning, they often fall short in scenarios requiring visual imagination. Unlike current works that take predefined external toolkits or generate images during thinking, however, humans can form flexible visual-text imagination and interactions during thinking without predefined toolkits, where one important reason is that humans construct the visual-text thinking process in a unified space inside the brain. Inspired by this capability, given that current MLLMs already encode visual and text information in the same feature space, we hold that visual tokens can be seamlessly inserted into the reasoning process carried by text tokens, where ideally, all visual imagination processes can be encoded by the latent features. To achieve this goal, we propose Sketch-in-Latents (SkiLa), a novel paradigm for unified multi-modal reasoning that expands the auto-regressive capabilities of MLLMs to natively generate continuous visual embeddings, termed latent sketch tokens, as visual thoughts. During multi-step reasoning, the model dynamically alternates between textual thinking mode for generating textual think tokens and visual sketching mode for generating latent sketch tokens. A latent visual semantics reconstruction mechanism is proposed to ensure these latent sketch tokens are semantically grounded. Extensive experiments demonstrate that SkiLa achieves superior performance on vision-centric tasks while exhibiting strong generalization to diverse general multi-modal benchmarks. Codes will be released at https://github.com/TungChintao/SkiLa.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "14 pages, 11 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16584v1",
      "code_links": [
        {
          "url": "https://github.com/TungChintao/SkiLa",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model",
            "multimodal"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "4D Primitive-Mâché: Glueing Primitives for Persistent 4D Scene Reconstruction",
      "authors": [
        "Kirill Mazur",
        "Marwan Taher",
        "Andrew J. Davison"
      ],
      "arxiv_id": "2512.16564v1",
      "summary": "We present a dynamic reconstruction system that receives a casual monocular RGB video as input, and outputs a complete and persistent reconstruction of the scene. In other words, we reconstruct not only the the currently visible parts of the scene, but also all previously viewed parts, which enables replaying the complete reconstruction across all timesteps.\n  Our method decomposes the scene into a set of rigid 3D primitives, which are assumed to be moving throughout the scene. Using estimated dense 2D correspondences, we jointly infer the rigid motion of these primitives through an optimisation pipeline, yielding a 4D reconstruction of the scene, i.e. providing 3D geometry dynamically moving through time. To achieve this, we also introduce a mechanism to extrapolate motion for objects that become invisible, employing motion-grouping techniques to maintain continuity.\n  The resulting system enables 4D spatio-temporal awareness, offering capabilities such as replayable 3D reconstructions of articulated objects through time, multi-object scanning, and object permanence. On object scanning and multi-object datasets, our system significantly outperforms existing methods both quantitatively and qualitatively.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "For project page, see https://makezur.github.io/4DPM/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16564v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]scene reconstruction"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Using Gaussian Splats to Create High-Fidelity Facial Geometry and Texture",
      "authors": [
        "Haodi He",
        "Jihun Yu",
        "Ronald Fedkiw"
      ],
      "arxiv_id": "2512.16397v1",
      "summary": "We leverage increasingly popular three-dimensional neural representations in order to construct a unified and consistent explanation of a collection of uncalibrated images of the human face. Our approach utilizes Gaussian Splatting, since it is more explicit and thus more amenable to constraints than NeRFs. We leverage segmentation annotations to align the semantic regions of the face, facilitating the reconstruction of a neutral pose from only 11 images (as opposed to requiring a long video). We soft constrain the Gaussians to an underlying triangulated surface in order to provide a more structured Gaussian Splat reconstruction, which in turn informs subsequent perturbations to increase the accuracy of the underlying triangulated surface. The resulting triangulated surface can then be used in a standard graphics pipeline. In addition, and perhaps most impactful, we show how accurate geometry enables the Gaussian Splats to be transformed into texture space where they can be treated as a view-dependent neural texture. This allows one to use high visual fidelity Gaussian Splatting on any asset in a scene without the need to modify any other asset or any other aspect (geometry, lighting, renderer, etc.) of the graphics pipeline. We utilize a relightable Gaussian model to disentangle texture from lighting in order to obtain a delit high-resolution albedo texture that is also readily usable in a standard graphics pipeline. The flexibility of our system allows for training with disparate images, even with incompatible lighting, facilitating robust regularization. Finally, we demonstrate the efficacy of our approach by illustrating its use in a text-driven asset creation pipeline.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "Submitted to CVPR 2026. 21 pages, 22 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16397v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "gaussian splatting",
            "splatting",
            "NeRF"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "EverybodyDance: Bipartite Graph-Based Identity Correspondence for Multi-Character Animation",
      "authors": [
        "Haotian Ling",
        "Zequn Chen",
        "Qiuying Chen",
        "Donglin Di",
        "Yongjia Ma",
        "Hao Li",
        "Chen Wei",
        "Zhulin Tao",
        "Xun Yang"
      ],
      "arxiv_id": "2512.16360v1",
      "summary": "Consistent pose-driven character animation has achieved remarkable progress in single-character scenarios. However, extending these advances to multi-character settings is non-trivial, especially when position swap is involved. Beyond mere scaling, the core challenge lies in enforcing correct Identity Correspondence (IC) between characters in reference and generated frames. To address this, we introduce EverybodyDance, a systematic solution targeting IC correctness in multi-character animation. EverybodyDance is built around the Identity Matching Graph (IMG), which models characters in the generated and reference frames as two node sets in a weighted complete bipartite graph. Edge weights, computed via our proposed Mask-Query Attention (MQA), quantify the affinity between each pair of characters. Our key insight is to formalize IC correctness as a graph structural metric and to optimize it during training. We also propose a series of targeted strategies tailored for multi-character animation, including identity-embedded guidance, a multi-scale matching strategy, and pre-classified sampling, which work synergistically. Finally, to evaluate IC performance, we curate the Identity Correspondence Evaluation benchmark, dedicated to multi-character IC correctness. Extensive experiments demonstrate that EverybodyDance substantially outperforms state-of-the-art baselines in both IC and visual fidelity.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16360v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱八：物理动画 (Physics-based Animation)",
          "id": "8_physics_animation",
          "matched_keywords": [
            "[T]character animation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "8_physics_animation"
      ]
    },
    {
      "title": "PixelArena: A benchmark for Pixel-Precision Visual Intelligence",
      "authors": [
        "Feng Liang",
        "Sizhe Cheng",
        "Chenqi Yi"
      ],
      "arxiv_id": "2512.16303v1",
      "summary": "Multi-modal large language models that have image output are emerging. Many image generation benchmarks focus on aesthetics instead of fine-grained generation capabilities. In PixelArena, we propose using semantic segmentation tasks to objectively examine their fine-grained generative intelligence with pixel precision. We find the latest Gemini 3 Pro Image has emergent image generation capabilities that generate semantic masks with high fidelity under zero-shot settings, showcasing visual intelligence unseen before and true generalization in new image generation tasks. We further investigate its results, compare them qualitatively and quantitatively with those of other models, and present failure cases. The findings not only signal exciting progress in the field but also provide insights into future research related to multimodality, reasoning, interpretability and benchmarking.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "7 pages, 11 figures, project page: https://pixelarena.reify.ing/project",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16303v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model",
            "multimodal"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "MACL: Multi-Label Adaptive Contrastive Learning Loss for Remote Sensing Image Retrieval",
      "authors": [
        "Amna Amir",
        "Erchan Aptoula"
      ],
      "arxiv_id": "2512.16294v1",
      "summary": "Semantic overlap among land-cover categories, highly imbalanced label distributions, and complex inter-class co-occurrence patterns constitute significant challenges for multi-label remote-sensing image retrieval. In this article, Multi-Label Adaptive Contrastive Learning (MACL) is introduced as an extension of contrastive learning to address them. It integrates label-aware sampling, frequency-sensitive weighting, and dynamic-temperature scaling to achieve balanced representation learning across both common and rare categories. Extensive experiments on three benchmark datasets (DLRSD, ML-AID, and WHDLD), show that MACL consistently outperforms contrastive-loss based baselines, effectively mitigating semantic imbalance and delivering more reliable retrieval performance in large-scale remote-sensing archives. Code, pretrained models, and evaluation scripts will be released at https://github.com/amna/MACL upon acceptance.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16294v1",
      "code_links": [
        {
          "url": "https://github.com/amna/MACL",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "representation learning",
            "[T]contrastive learning"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "SegGraph: Leveraging Graphs of SAM Segments for Few-Shot 3D Part Segmentation",
      "authors": [
        "Yueyang Hu",
        "Haiyong Jiang",
        "Haoxuan Song",
        "Jun Xiao",
        "Hao Pan"
      ],
      "arxiv_id": "2512.16143v1",
      "summary": "This work presents a novel framework for few-shot 3D part segmentation. Recent advances have demonstrated the significant potential of 2D foundation models for low-shot 3D part segmentation. However, it is still an open problem that how to effectively aggregate 2D knowledge from foundation models to 3D. Existing methods either ignore geometric structures for 3D feature learning or neglects the high-quality grouping clues from SAM, leading to under-segmentation and inconsistent part labels. We devise a novel SAM segment graph-based propagation method, named SegGraph, to explicitly learn geometric features encoded within SAM's segmentation masks. Our method encodes geometric features by modeling mutual overlap and adjacency between segments while preserving intra-segment semantic consistency. We construct a segment graph, conceptually similar to an atlas, where nodes represent segments and edges capture their spatial relationships (overlap/adjacency). Each node adaptively modulates 2D foundation model features, which are then propagated via a graph neural network to learn global geometric structures. To enforce intra-segment semantic consistency, we map segment features to 3D points with a novel view-direction-weighted fusion attenuating contributions from low-quality segments. Extensive experiments on PartNet-E demonstrate that our method outperforms all competing baselines by at least 6.9 percent mIoU. Further analysis reveals that SegGraph achieves particularly strong performance on small components and part boundaries, demonstrating its superior geometric understanding. The code is available at: https://github.com/YueyangHu2000/SegGraph.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16143v1",
      "code_links": [
        {
          "url": "https://github.com/YueyangHu2000/SegGraph",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "spatial relationship"
          ],
          "score": 3.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "foundation model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "7_retargeting",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Stackelberg Learning from Human Feedback: Preference Optimization as a Sequential Game",
      "authors": [
        "Barna Pásztor",
        "Thomas Kleine Buening",
        "Andreas Krause"
      ],
      "arxiv_id": "2512.16626v1",
      "summary": "We introduce Stackelberg Learning from Human Feedback (SLHF), a new framework for preference optimization. SLHF frames the alignment problem as a sequential-move game between two policies: a Leader, which commits to an action, and a Follower, which responds conditionally on the Leader's action. This approach decomposes preference optimization into a refinement problem for the Follower and an optimization problem against an adversary for the Leader. Unlike Reinforcement Learning from Human Feedback (RLHF), which assigns scalar rewards to actions, or Nash Learning from Human Feedback (NLHF), which seeks a simultaneous-move equilibrium, SLHF leverages the asymmetry of sequential play to capture richer preference structures. The sequential design of SLHF naturally enables inference-time refinement, as the Follower learns to improve the Leader's actions, and these refinements can be leveraged through iterative sampling. We compare the solution concepts of SLHF, RLHF, and NLHF, and lay out key advantages in consistency, data sensitivity, and robustness to intransitive preferences. Experiments on large language models demonstrate that SLHF achieves strong alignment across diverse preference datasets, scales from 0.5B to 8B parameters, and yields inference-time refinements that transfer across model families without further fine-tuning.",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.GT",
        "cs.MA",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "10 pages, 5 tables, 1 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16626v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "RLHF"
          ],
          "score": 3.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Do Multi-Agents Solve Better Than Single? Evaluating Agentic Frameworks for Diagram-Grounded Geometry Problem Solving and Reasoning",
      "authors": [
        "Mahbub E Sobhani",
        "Md. Faiyaz Abdullah Sayeedi",
        "Mohammad Nehad Alam",
        "Proma Hossain Progga",
        "Swakkhar Shatabda"
      ],
      "arxiv_id": "2512.16698v1",
      "summary": "Diagram-grounded geometry problem solving is a critical benchmark for multimodal large language models (MLLMs), yet the benefits of multi-agent design over single-agent remain unclear. We systematically compare single-agent and multi-agent pipelines on four visual math benchmarks: Geometry3K, MathVerse, OlympiadBench, and We-Math. For open-source models, multi-agent consistently improves performance. For example, Qwen-2.5-VL (7B) gains +6.8 points and Qwen-2.5-VL (32B) gains +3.3 on Geometry3K, and both Qwen-2.5-VL variants see further gains on OlympiadBench and We-Math. In contrast, the closed-source Gemini-2.0-Flash generally performs better in single-agent mode on classic benchmarks, while multi-agent yields only modest improvements on the newer We-Math dataset. These findings show that multi-agent pipelines provide clear benefits for open-source models and can assist strong proprietary systems on newer, less familiar benchmarks, but agentic decomposition is not universally optimal. All code, data, and reasoning files are available at https://github.com/faiyazabdullah/Interpreter-Solver",
      "categories": [
        "cs.AI",
        "cs.CG"
      ],
      "primary_category": "cs.AI",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "Accepted to the ARR October 2025 cycle",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16698v1",
      "code_links": [
        {
          "url": "https://github.com/faiyazabdullah/Interpreter-Solver",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model",
            "multimodal"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Scaling Laws for Energy Efficiency of Local LLMs",
      "authors": [
        "Ander Alvarez",
        "Alessandro Genuardi",
        "Nilotpal Sinha",
        "Antonio Tiene",
        "Samuel Mugel",
        "Román Orús"
      ],
      "arxiv_id": "2512.16531v1",
      "summary": "Deploying local large language models and vision-language models on edge devices requires balancing accuracy with constrained computational and energy budgets. Although graphics processors dominate modern artificial-intelligence deployment, most consumer hardware--including laptops, desktops, industrial controllers, and embedded systems--relies on central processing units. Despite this, the computational laws governing central-processing-unit-only inference for local language and vision-language workloads remain largely unexplored. We systematically benchmark large language and vision-language models on two representative central-processing-unit tiers widely used for local inference: a MacBook Pro M2, reflecting mainstream laptop-class deployment, and a Raspberry Pi 5, representing constrained, low-power embedded settings. Using a unified methodology based on continuous sampling of processor and memory usage together with area-under-curve integration, we characterize how computational load scales with input text length for language models and with image resolution for vision-language models. We uncover two empirical scaling laws: (1) computational cost for language-model inference scales approximately linearly with token length; and (2) vision-language models exhibit a preprocessing-driven \"resolution knee\", where compute remains constant above an internal resolution clamp and decreases sharply below it. Beyond these laws, we show that quantum-inspired compression reduces processor and memory usage by up to 71.9% and energy consumption by up to 62%, while preserving or improving semantic accuracy. These results provide a systematic quantification of multimodal central-processing-unit-only scaling for local language and vision-language workloads, and they identify model compression and input-resolution preprocessing as effective, low-cost levers for sustainable edge inference.",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16531v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model",
            "multimodal"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs",
      "authors": [
        "Sara Papi",
        "Javier Garcia Gilabert",
        "Zachary Hopton",
        "Vilém Zouhar",
        "Carlos Escolano",
        "Gerard I. Gállego",
        "Jorge Iranzo-Sánchez",
        "Ahrii Kim",
        "Dominik Macháček",
        "Patricia Schmidtova",
        "Maike Züfle"
      ],
      "arxiv_id": "2512.16378v1",
      "summary": "As Large Language Models (LLMs) expand beyond text, integrating speech as a native modality has given rise to SpeechLLMs, which aim to translate spoken language directly, thereby bypassing traditional transcription-based pipelines. Whether this integration improves speech-to-text translation quality over established cascaded architectures, however, remains an open question. We present Hearing to Translate, the first comprehensive test suite rigorously benchmarking 5 state-of-the-art SpeechLLMs against 16 strong direct and cascade systems that couple leading speech foundation models (SFM), with multilingual LLMs. Our analysis spans 16 benchmarks, 13 language pairs, and 9 challenging conditions, including disfluent, noisy, and long-form speech. Across this extensive evaluation, we find that cascaded systems remain the most reliable overall, while current SpeechLLMs only match cascades in selected settings and SFMs lag behind both, highlighting that integrating an LLM, either within the model or in a pipeline, is essential for high-quality speech translation.",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "cs.CL",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "Project available at https://github.com/sarapapi/hearing2translate",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16378v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model",
            "foundation model"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Code-in-the-Loop Forensics: Agentic Tool Use for Image Forgery Detection",
      "authors": [
        "Fanrui Zhang",
        "Qiang Zhang",
        "Sizhuo Zhou",
        "Jianwen Sun",
        "Chuanhao Li",
        "Jiaxin Ai",
        "Yukang Feng",
        "Yujie Zhang",
        "Wenjie Li",
        "Zizhen Li",
        "Yifan Chang",
        "Jiawei Liu",
        "Kaipeng Zhang"
      ],
      "arxiv_id": "2512.16300v1",
      "summary": "Existing image forgery detection (IFD) methods either exploit low-level, semantics-agnostic artifacts or rely on multimodal large language models (MLLMs) with high-level semantic knowledge. Although naturally complementary, these two information streams are highly heterogeneous in both paradigm and reasoning, making it difficult for existing methods to unify them or effectively model their cross-level interactions. To address this gap, we propose ForenAgent, a multi-round interactive IFD framework that enables MLLMs to autonomously generate, execute, and iteratively refine Python-based low-level tools around the detection objective, thereby achieving more flexible and interpretable forgery analysis. ForenAgent follows a two-stage training pipeline combining Cold Start and Reinforcement Fine-Tuning to enhance its tool interaction capability and reasoning adaptability progressively. Inspired by human reasoning, we design a dynamic reasoning loop comprising global perception, local focusing, iterative probing, and holistic adjudication, and instantiate it as both a data-sampling strategy and a task-aligned process reward. For systematic training and evaluation, we construct FABench, a heterogeneous, high-quality agent-forensics dataset comprising 100k images and approximately 200k agent-interaction question-answer pairs. Experiments show that ForenAgent exhibits emergent tool-use competence and reflective reasoning on challenging IFD tasks when assisted by low-level tools, charting a promising route toward general-purpose IFD. The code will be released after the review process is completed.",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "11 pages, 6 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16300v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model",
            "multimodal"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "AMUSE: Audio-Visual Benchmark and Alignment Framework for Agentic Multi-Speaker Understanding",
      "authors": [
        "Sanjoy Chowdhury",
        "Karren D. Yang",
        "Xudong Liu",
        "Fartash Faghri",
        "Pavan Kumar Anasosalu Vasu",
        "Oncel Tuzel",
        "Dinesh Manocha",
        "Chun-Liang Li",
        "Raviteja Vemulapalli"
      ],
      "arxiv_id": "2512.16250v1",
      "summary": "Recent multimodal large language models (MLLMs) such as GPT-4o and Qwen3-Omni show strong perception but struggle in multi-speaker, dialogue-centric settings that demand agentic reasoning tracking who speaks, maintaining roles, and grounding events across time. These scenarios are central to multimodal audio-video understanding, where models must jointly reason over audio and visual streams in applications such as conversational video assistants and meeting analytics. We introduce AMUSE, a benchmark designed around tasks that are inherently agentic, requiring models to decompose complex audio-visual interactions into planning, grounding, and reflection steps. It evaluates MLLMs across three modes zero-shot, guided, and agentic and six task families, including spatio-temporal speaker grounding and multimodal dialogue summarization. Across all modes, current models exhibit weak multi-speaker reasoning and inconsistent behavior under both non-agentic and agentic evaluation. Motivated by the inherently agentic nature of these tasks and recent advances in LLM agents, we propose RAFT, a data-efficient agentic alignment framework that integrates reward optimization with intrinsic multimodal self-evaluation as reward and selective parameter adaptation for data and parameter efficient updates. Using RAFT, we achieve up to 39.52\\% relative improvement in accuracy on our benchmark. Together, AMUSE and RAFT provide a practical platform for examining agentic reasoning in multimodal models and improving their capabilities.",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16250v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model",
            "multimodal"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "JustRL: Scaling a 1.5B LLM with a Simple RL Recipe",
      "authors": [
        "Bingxiang He",
        "Zekai Qu",
        "Zeyuan Liu",
        "Yinghao Chen",
        "Yuxin Zuo",
        "Cheng Qian",
        "Kaiyan Zhang",
        "Weize Chen",
        "Chaojun Xiao",
        "Ganqu Cui",
        "Ning Ding",
        "Zhiyuan Liu"
      ],
      "arxiv_id": "2512.16649v1",
      "summary": "Recent advances in reinforcement learning for large language models have converged on increasing complexity: multi-stage training pipelines, dynamic hyperparameter schedules, and curriculum learning strategies. This raises a fundamental question: \\textbf{Is this complexity necessary?} We present \\textbf{JustRL}, a minimal approach using single-stage training with fixed hyperparameters that achieves state-of-the-art performance on two 1.5B reasoning models (54.9\\% and 64.3\\% average accuracy across nine mathematical benchmarks) while using 2$\\times$ less compute than sophisticated approaches. The same hyperparameters transfer across both models without tuning, and training exhibits smooth, monotonic improvement over 4,000+ steps without the collapses or plateaus that typically motivate interventions. Critically, ablations reveal that adding ``standard tricks'' like explicit length penalties and robust verifiers may degrade performance by collapsing exploration. These results suggest that the field may be adding complexity to solve problems that disappear with a stable, scaled-up baseline. We release our models and code to establish a simple, validated baseline for the community.",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "12 pages, 3 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16649v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "curriculum learning"
          ],
          "score": 3.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Learning-based Approximate Model Predictive Control for an Impact Wrench Tool",
      "authors": [
        "Mark Benazet",
        "Francesco Ricca",
        "Dario Bralla",
        "Melanie N. Zeilinger",
        "Andrea Carron"
      ],
      "arxiv_id": "2512.16624v1",
      "summary": "Learning-based model predictive control has emerged as a powerful approach for handling complex dynamics in mechatronic systems, enabling data-driven performance improvements while respecting safety constraints. However, when computational resources are severely limited, as in battery-powered tools with embedded processors, existing approaches struggle to meet real-time requirements. In this paper, we address the problem of real-time torque control for impact wrenches, where high-frequency control updates are necessary to accurately track the fast transients occurring during periodic impact events, while maintaining high-performance safety-critical control that mitigates harmful vibrations and component wear. The key novelty of the approach is that we combine data-driven model augmentation through Gaussian process regression with neural network approximation of the resulting control policy. This insight allows us to deploy predictive control on resource-constrained embedded platforms while maintaining both constraint satisfaction and microsecond-level inference times. The proposed framework is evaluated through numerical simulations and hardware experiments on a custom impact wrench testbed. The results show that our approach successfully achieves real-time control suitable for high-frequency operation while maintaining constraint satisfaction and improving tracking accuracy compared to baseline PID control.",
      "categories": [
        "eess.SY"
      ],
      "primary_category": "eess.SY",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16624v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]model predictive control"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Economic versus energetic model predictive control of a cold production plant with thermal energy storage",
      "authors": [
        "Manuel G. Satué",
        "Manuel R. Arahal",
        "Luis F. Acedo",
        "Manuel G. Ortega"
      ],
      "arxiv_id": "2512.16379v1",
      "summary": "Economic model predictive control has been proposed as a means for solving the unit loading and unit allocation problem in multi-chiller cooling plants. The adjective economic stems from the use of financial cost due to electricity consumption in a time horizon, such is the loss function minimized at each sampling period. The energetic approach is rarely encountered. This article presents for the first time a comparison between the energetic optimization objective and the economic one. The comparison is made on a cooling plant using air-cooled water chillers and a cold storage system. Models developed have been integrated into Simscape, and non-convex mixed optimization methods used to achieve optimal control trajectories for both energetic and economic goals considered separately. The results over several scenarios, and in different seasons, support the consideration of the energetic approach despite the current prevalence of the economic one. The results are dependent on the electric season and the available tariffs. In particular, for the high electric season and considering a representative tariff, the results show that an increment of about 2.15% in energy consumption takes place when using the economic approach instead of the energetic one. On the other hand, a reduction in cost of 2.94% is achieved.",
      "categories": [
        "eess.SY"
      ],
      "primary_category": "eess.SY",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "14 pages",
      "doi": "10.1016/j.applthermaleng.2022.118309",
      "journal_ref": "Applied Thermal Engineering 210 (2022) 118309",
      "pdf_url": "https://arxiv.org/pdf/2512.16379v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]model predictive control"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Sceniris: A Fast Procedural Scene Generation Framework",
      "authors": [
        "Jinghuan Shang",
        "Harsh Patel",
        "Ran Gong",
        "Karl Schmeckpeper"
      ],
      "arxiv_id": "2512.16896v1",
      "summary": "Synthetic 3D scenes are essential for developing Physical AI and generative models. Existing procedural generation methods often have low output throughput, creating a significant bottleneck in scaling up dataset creation. In this work, we introduce Sceniris, a highly efficient procedural scene generation framework for rapidly generating large-scale, collision-free scene variations. Sceniris also provides an optional robot reachability check, providing manipulation-feasible scenes for robot tasks. Sceniris is designed for maximum efficiency by addressing the primary performance limitations of the prior method, Scene Synthesizer. Leveraging batch sampling and faster collision checking in cuRobo, Sceniris achieves at least 234x speed-up over Scene Synthesizer. Sceniris also expands the object-wise spatial relationships available in prior work to support diverse scene requirements. Our code is available at https://github.com/rai-inst/sceniris",
      "categories": [
        "cs.RO",
        "cs.CV",
        "cs.GR"
      ],
      "primary_category": "cs.RO",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "Code is available at https://github.com/rai-inst/sceniris",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16896v1",
      "code_links": [
        {
          "url": "https://github.com/rai-inst/sceniris",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "spatial relationship"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 5.0,
      "hit_pillars": [
        "1_robot_core",
        "7_retargeting"
      ]
    },
    {
      "title": "TextEditBench: Evaluating Reasoning-aware Text Editing Beyond Rendering",
      "authors": [
        "Rui Gui",
        "Yang Wan",
        "Haochen Han",
        "Dongxing Mao",
        "Fangming Liu",
        "Min Li",
        "Alex Jinpeng Wang"
      ],
      "arxiv_id": "2512.16270v1",
      "summary": "Text rendering has recently emerged as one of the most challenging frontiers in visual generation, drawing significant attention from large-scale diffusion and multimodal models. However, text editing within images remains largely unexplored, as it requires generating legible characters while preserving semantic, geometric, and contextual coherence. To fill this gap, we introduce TextEditBench, a comprehensive evaluation benchmark that explicitly focuses on text-centric regions in images. Beyond basic pixel manipulations, our benchmark emphasizes reasoning-intensive editing scenarios that require models to understand physical plausibility, linguistic meaning, and cross-modal dependencies. We further propose a novel evaluation dimension, Semantic Expectation (SE), which measures reasoning ability of model to maintain semantic consistency, contextual coherence, and cross-modal alignment during text editing. Extensive experiments on state-of-the-art editing systems reveal that while current models can follow simple textual instructions, they still struggle with context-dependent reasoning, physical consistency, and layout-aware integration. By focusing evaluation on this long-overlooked yet fundamental capability, TextEditBench establishes a new testing ground for advancing text-guided image editing and reasoning in multimodal generation.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16270v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "multimodal"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 5.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Skeleton-Snippet Contrastive Learning with Multiscale Feature Fusion for Action Localization",
      "authors": [
        "Qiushuo Cheng",
        "Jingjing Liu",
        "Catherine Morgan",
        "Alan Whone",
        "Majid Mirmehdi"
      ],
      "arxiv_id": "2512.16504v1",
      "summary": "The self-supervised pretraining paradigm has achieved great success in learning 3D action representations for skeleton-based action recognition using contrastive learning. However, learning effective representations for skeleton-based temporal action localization remains challenging and underexplored. Unlike video-level {action} recognition, detecting action boundaries requires temporally sensitive features that capture subtle differences between adjacent frames where labels change. To this end, we formulate a snippet discrimination pretext task for self-supervised pretraining, which densely projects skeleton sequences into non-overlapping segments and promotes features that distinguish them across videos via contrastive learning. Additionally, we build on strong backbones of skeleton-based action recognition models by fusing intermediate features with a U-shaped module to enhance feature resolution for frame-level localization. Our approach consistently improves existing skeleton-based contrastive learning methods for action localization on BABEL across diverse subsets and evaluation protocols. We also achieve state-of-the-art transfer learning performance on PKUMMD with pretraining on NTU RGB+D and BABEL.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16504v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]contrastive learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward",
      "authors": [
        "Peter Chen",
        "Xiaopeng Li",
        "Ziniu Li",
        "Wotao Yin",
        "Xi Chen",
        "Tianyi Lin"
      ],
      "arxiv_id": "2512.16912v1",
      "summary": "This paper examines the exploration-exploitation trade-off in reinforcement learning with verifiable rewards (RLVR), a framework for improving the reasoning of Large Language Models (LLMs). Recent studies suggest that RLVR can elicit strong mathematical reasoning in LLMs through two seemingly paradoxical mechanisms: spurious rewards, which suppress exploitation by rewarding outcomes unrelated to the ground truth, and entropy minimization, which suppresses exploration by pushing the model toward more confident and deterministic outputs, highlighting a puzzling dynamic: both discouraging exploitation and discouraging exploration improve reasoning performance, yet the underlying principles that reconcile these effects remain poorly understood. We focus on two fundamental questions: (i) how policy entropy relates to performance, and (ii) whether spurious rewards yield gains, potentially through the interplay of clipping bias and model contamination. Our results show that clipping bias under spurious rewards reduces policy entropy, leading to more confident and deterministic outputs, while entropy minimization alone is insufficient for improvement. We further propose a reward-misalignment model explaining why spurious rewards can enhance performance beyond contaminated settings. Our findings clarify the mechanisms behind spurious-reward benefits and provide principles for more effective RLVR training.",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "35 pages",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16912v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Meta-RL Induces Exploration in Language Agents",
      "authors": [
        "Yulun Jiang",
        "Liangze Jiang",
        "Damien Teney",
        "Michael Moor",
        "Maria Brbic"
      ],
      "arxiv_id": "2512.16848v1",
      "summary": "Reinforcement learning (RL) has enabled the training of large language model (LLM) agents to interact with the environment and to solve multi-turn long-horizon tasks. However, the RL-trained agents often struggle in tasks that require active exploration and fail to efficiently adapt from trial-and-error experiences. In this paper, we present LaMer, a general Meta-RL framework that enables LLM agents to actively explore and learn from the environment feedback at test time. LaMer consists of two key components: (i) a cross-episode training framework to encourage exploration and long-term rewards optimization; and (ii) in-context policy adaptation via reflection, allowing the agent to adapt their policy from task feedback signal without gradient update. Experiments across diverse environments show that LaMer significantly improves performance over RL baselines, with 11%, 14%, and 19% performance gains on Sokoban, MineSweeper and Webshop, respectively. Moreover, LaMer also demonstrates better generalization to more challenging or previously unseen tasks compared to the RL-trained agents. Overall, our results demonstrate that Meta-RL provides a principled approach to induce exploration in language agents, enabling more robust adaptation to novel environments through learned exploration strategies.",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16848v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Coordinated Anti-Jamming Resilience in Swarm Networks via Multi-Agent Reinforcement Learning",
      "authors": [
        "Bahman Abolhassani",
        "Tugba Erpek",
        "Kemal Davaslioglu",
        "Yalin E. Sagduyu",
        "Sastry Kompella"
      ],
      "arxiv_id": "2512.16813v1",
      "summary": "Reactive jammers pose a severe security threat to robotic-swarm networks by selectively disrupting inter-agent communications and undermining formation integrity and mission success. Conventional countermeasures such as fixed power control or static channel hopping are largely ineffective against such adaptive adversaries. This paper presents a multi-agent reinforcement learning (MARL) framework based on the QMIX algorithm to improve the resilience of swarm communications under reactive jamming. We consider a network of multiple transmitter-receiver pairs sharing channels while a reactive jammer with Markovian threshold dynamics senses aggregate power and reacts accordingly. Each agent jointly selects transmit frequency (channel) and power, and QMIX learns a centralized but factorizable action-value function that enables coordinated yet decentralized execution. We benchmark QMIX against a genie-aided optimal policy in a no-channel-reuse setting, and against local Upper Confidence Bound (UCB) and a stateless reactive policy in a more general fading regime with channel reuse enabled. Simulation results show that QMIX rapidly converges to cooperative policies that nearly match the genie-aided bound, while achieving higher throughput and lower jamming incidence than the baselines, thereby demonstrating MARL's effectiveness for securing autonomous swarms in contested environments.",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.DC",
        "cs.LG",
        "eess.SP"
      ],
      "primary_category": "cs.NI",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16813v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "On The Hidden Biases of Flow Matching Samplers",
      "authors": [
        "Soon Hoe Lim"
      ],
      "arxiv_id": "2512.16768v1",
      "summary": "We study the implicit bias of flow matching (FM) samplers via the lens of empirical flow matching. Although population FM may produce gradient-field velocities resembling optimal transport (OT), we show that the empirical FM minimizer is almost never a gradient field, even when each conditional flow is. Consequently, empirical FM is intrinsically energetically suboptimal. In view of this, we analyze the kinetic energy of generated samples. With Gaussian sources, both instantaneous and integrated kinetic energies exhibit exponential concentration, while heavy-tailed sources lead to polynomial tails. These behaviors are governed primarily by the choice of source distribution rather than the data. Overall, these notes provide a concise mathematical account of the structural and energetic biases arising in empirical FM.",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.PR"
      ],
      "primary_category": "stat.ML",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "20 pages",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16768v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]flow matching"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "NDRL: Cotton Irrigation and Nitrogen Application with Nested Dual-Agent Reinforcement Learning",
      "authors": [
        "Ruifeng Xu",
        "Liang He"
      ],
      "arxiv_id": "2512.16408v1",
      "summary": "Effective irrigation and nitrogen fertilization have a significant impact on crop yield. However, existing research faces two limitations: (1) the high complexity of optimizing water-nitrogen combinations during crop growth and poor yield optimization results; and (2) the difficulty in quantifying mild stress signals and the delayed feedback, which results in less precise dynamic regulation of water and nitrogen and lower resource utilization efficiency. To address these issues, we propose a Nested Dual-Agent Reinforcement Learning (NDRL) method. The parent agent in NDRL identifies promising macroscopic irrigation and fertilization actions based on projected cumulative yield benefits, reducing ineffective explorationwhile maintaining alignment between objectives and yield. The child agent's reward function incorporates quantified Water Stress Factor (WSF) and Nitrogen Stress Factor (NSF), and uses a mixed probability distribution to dynamically optimize daily strategies, thereby enhancing both yield and resource efficiency. We used field experiment data from 2023 and 2024 to calibrate and validate the Decision Support System for Agrotechnology Transfer (DSSAT) to simulate real-world conditions and interact with NDRL. Experimental results demonstrate that, compared to the best baseline, the simulated yield increased by 4.7% in both 2023 and 2024, the irrigation water productivity increased by 5.6% and 5.1% respectively, and the nitrogen partial factor productivity increased by 6.3% and 1.0% respectively. Our method advances the development of cotton irrigation and nitrogen fertilization, providing new ideas for addressing the complexity and precision issues in agricultural resource management and for sustainable agricultural development.",
      "categories": [
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.LG",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "Accepted by ICONIP 2025",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16408v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "A Task-Driven, Planner-in-the-Loop Computational Design Framework for Modular Manipulators",
      "authors": [
        "Maolin Lei",
        "Edoardo Romiti",
        "Arturo Laurenzi",
        "Rui Dai",
        "Matteo Dalle Vedove",
        "Jiatao Ding",
        "Daniele Fontanelli",
        "Nikos Tsagarakis"
      ],
      "arxiv_id": "2512.16069v1",
      "summary": "Modular manipulators composed of pre-manufactured and interchangeable modules offer high adaptability across diverse tasks. However, their deployment requires generating feasible motions while jointly optimizing morphology and mounted pose under kinematic, dynamic, and physical constraints. Moreover, traditional single-branch designs often extend reach by increasing link length, which can easily violate torque limits at the base joint. To address these challenges, we propose a unified task-driven computational framework that integrates trajectory planning across varying morphologies with the co-optimization of morphology and mounted pose. Within this framework, a hierarchical model predictive control (HMPC) strategy is developed to enable motion planning for both redundant and non-redundant manipulators. For design optimization, the CMA-ES is employed to efficiently explore a hybrid search space consisting of discrete morphology configurations and continuous mounted poses. Meanwhile, a virtual module abstraction is introduced to enable bi-branch morphologies, allowing an auxiliary branch to offload torque from the primary branch and extend the achievable workspace without increasing the capacity of individual joint modules. Extensive simulations and hardware experiments on polishing, drilling, and pick-and-place tasks demonstrate the effectiveness of the proposed framework. The results show that: 1) the framework can generate multiple feasible designs that satisfy kinematic and dynamic constraints while avoiding environmental collisions for given tasks; 2) flexible design objectives, such as maximizing manipulability, minimizing joint effort, or reducing the number of modules, can be achieved by customizing the cost functions; and 3) a bi-branch morphology capable of operating in a large workspace can be realized without requiring more powerful basic modules.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16069v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "model predictive control",
            "motion planning"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Auto-Vocabulary 3D Object Detection",
      "authors": [
        "Haomeng Zhang",
        "Kuan-Chuan Peng",
        "Suhas Lohit",
        "Raymond A. Yeh"
      ],
      "arxiv_id": "2512.16077v1",
      "summary": "Open-vocabulary 3D object detection methods are able to localize 3D boxes of classes unseen during training. Despite the name, existing methods rely on user-specified classes both at training and inference. We propose to study Auto-Vocabulary 3D Object Detection (AV3DOD), where the classes are automatically generated for the detected objects without any user input. To this end, we introduce Semantic Score (SS) to evaluate the quality of the generated class names. We then develop a novel framework, AV3DOD, which leverages 2D vision-language models (VLMs) to generate rich semantic candidates through image captioning, pseudo 3D box generation, and feature-space semantics expansion. AV3DOD achieves the state-of-the-art (SOTA) performance on both localization (mAP) and semantic quality (SS) on the ScanNetV2 and SUNRGB-D datasets. Notably, it surpasses the SOTA, CoDA, by 3.48 overall mAP and attains a 24.5% relative improvement in SS on ScanNetV2.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "technical report",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16077v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "open-vocabulary",
            "open vocabulary"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Posterior Behavioral Cloning: Pretraining BC Policies for Efficient RL Finetuning",
      "authors": [
        "Andrew Wagenmaker",
        "Perry Dong",
        "Raymond Tsao",
        "Chelsea Finn",
        "Sergey Levine"
      ],
      "arxiv_id": "2512.16911v1",
      "summary": "Standard practice across domains from robotics to language is to first pretrain a policy on a large-scale demonstration dataset, and then finetune this policy, typically with reinforcement learning (RL), in order to improve performance on deployment domains. This finetuning step has proved critical in achieving human or super-human performance, yet while much attention has been given to developing more effective finetuning algorithms, little attention has been given to ensuring the pretrained policy is an effective initialization for RL finetuning. In this work we seek to understand how the pretrained policy affects finetuning performance, and how to pretrain policies in order to ensure they are effective initializations for finetuning. We first show theoretically that standard behavioral cloning (BC) -- which trains a policy to directly match the actions played by the demonstrator -- can fail to ensure coverage over the demonstrator's actions, a minimal condition necessary for effective RL finetuning. We then show that if, instead of exactly fitting the observed demonstrations, we train a policy to model the posterior distribution of the demonstrator's behavior given the demonstration dataset, we do obtain a policy that ensures coverage over the demonstrator's actions, enabling more effective finetuning. Furthermore, this policy -- which we refer to as the posterior behavioral cloning (PostBC) policy -- achieves this while ensuring pretrained performance is no worse than that of the BC policy. We then show that PostBC is practically implementable with modern generative models in robotic control domains -- relying only on standard supervised learning -- and leads to significantly improved RL finetuning performance on both realistic robotic control benchmarks and real-world robotic manipulation tasks, as compared to standard behavioral cloning.",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16911v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 3.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning",
      "authors": [
        "Yuanchen Ju",
        "Yongyuan Liang",
        "Yen-Jen Wang",
        "Nandiraju Gireesh",
        "Yuanliang Ju",
        "Seungjae Lee",
        "Qiao Gu",
        "Elvis Hsieh",
        "Furong Huang",
        "Koushil Sreenath"
      ],
      "arxiv_id": "2512.16909v1",
      "summary": "Mobile manipulators in households must both navigate and manipulate. This requires a compact, semantically rich scene representation that captures where objects are, how they function, and which parts are actionable. Scene graphs are a natural choice, yet prior work often separates spatial and functional relations, treats scenes as static snapshots without object states or temporal updates, and overlooks information most relevant for accomplishing the current task. To address these limitations, we introduce MomaGraph, a unified scene representation for embodied agents that integrates spatial-functional relationships and part-level interactive elements. However, advancing such a representation requires both suitable data and rigorous evaluation, which have been largely missing. We thus contribute MomaGraph-Scenes, the first large-scale dataset of richly annotated, task-driven scene graphs in household environments, along with MomaGraph-Bench, a systematic evaluation suite spanning six reasoning capabilities from high-level planning to fine-grained scene understanding. Built upon this foundation, we further develop MomaGraph-R1, a 7B vision-language model trained with reinforcement learning on MomaGraph-Scenes. MomaGraph-R1 predicts task-oriented scene graphs and serves as a zero-shot task planner under a Graph-then-Plan framework. Extensive experiments demonstrate that our model achieves state-of-the-art results among open-source models, reaching 71.6% accuracy on the benchmark (+11.4% over the best baseline), while generalizing across public benchmarks and transferring effectively to real-robot experiments.",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "25 pages, 10 figures. Project page:https://hybridrobotics.github.io/MomaGraph/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16909v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "scene understanding"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 3.5,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "Hypernetworks That Evolve Themselves",
      "authors": [
        "Joachim Winther Pedersen",
        "Erwan Plantec",
        "Eleni Nisioti",
        "Marcello Barylli",
        "Milton Montero",
        "Kathrin Korte",
        "Sebastian Risi"
      ],
      "arxiv_id": "2512.16406v1",
      "summary": "How can neural networks evolve themselves without relying on external optimizers? We propose Self-Referential Graph HyperNetworks, systems where the very machinery of variation and inheritance is embedded within the network. By uniting hypernetworks, stochastic parameter generation, and graph-based representations, Self-Referential GHNs mutate and evaluate themselves while adapting mutation rates as selectable traits. Through new reinforcement learning benchmarks with environmental shifts (CartPoleSwitch, LunarLander-Switch), Self-Referential GHNs show swift, reliable adaptation and emergent population dynamics. In the locomotion benchmark Ant-v5, they evolve coherent gaits, showing promising fine-tuning capabilities by autonomously decreasing variation in the population to concentrate around promising solutions. Our findings support the idea that evolvability itself can emerge from neural self-reference. Self-Referential GHNs reflect a step toward synthetic systems that more closely mirror biological evolution, offering tools for autonomous, open-ended learning agents.",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16406v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "locomotion"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 3.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "PolaRiS: Scalable Real-to-Sim Evaluations for Generalist Robot Policies",
      "authors": [
        "Arhan Jain",
        "Mingtong Zhang",
        "Kanav Arora",
        "William Chen",
        "Marcel Torne",
        "Muhammad Zubair Irshad",
        "Sergey Zakharov",
        "Yue Wang",
        "Sergey Levine",
        "Chelsea Finn",
        "Wei-Chiu Ma",
        "Dhruv Shah",
        "Abhishek Gupta",
        "Karl Pertsch"
      ],
      "arxiv_id": "2512.16881v1",
      "summary": "A significant challenge for robot learning research is our ability to accurately measure and compare the performance of robot policies. Benchmarking in robotics is historically challenging due to the stochasticity, reproducibility, and time-consuming nature of real-world rollouts. This challenge is exacerbated for recent generalist policies, which has to be evaluated across a wide variety of scenes and tasks. Evaluation in simulation offers a scalable complement to real world evaluations, but the visual and physical domain gap between existing simulation benchmarks and the real world has made them an unreliable signal for policy improvement. Furthermore, building realistic and diverse simulated environments has traditionally required significant human effort and expertise. To bridge the gap, we introduce Policy Evaluation and Environment Reconstruction in Simulation (PolaRiS), a scalable real-to-sim framework for high-fidelity simulated robot evaluation. PolaRiS utilizes neural reconstruction methods to turn short video scans of real-world scenes into interactive simulation environments. Additionally, we develop a simple simulation data co-training recipe that bridges remaining real-to-sim gaps and enables zero-shot evaluation in unseen simulation environments. Through extensive paired evaluations between simulation and the real world, we demonstrate that PolaRiS evaluations provide a much stronger correlation to real world generalist policy performance than existing simulated benchmarks. Its simplicity also enables rapid creation of diverse simulated environments. As such, this work takes a step towards distributed and democratized evaluation for the next generation of robotic foundation models.",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "Website: https://polaris-evals.github.io/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16881v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "foundation model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "VIVA: VLM-Guided Instruction-Based Video Editing with Reward Optimization",
      "authors": [
        "Xiaoyan Cong",
        "Haotian Yang",
        "Angtian Wang",
        "Yizhi Wang",
        "Yiding Yang",
        "Canyu Zhang",
        "Chongyang Ma"
      ],
      "arxiv_id": "2512.16906v1",
      "summary": "Instruction-based video editing aims to modify an input video according to a natural-language instruction while preserving content fidelity and temporal coherence. However, existing diffusion-based approaches are often trained on paired data of simple editing operations, which fundamentally limits their ability to generalize to diverse and complex, real-world instructions. To address this generalization gap, we propose VIVA, a scalable framework for instruction-based video editing that leverages VLM-guided encoding and reward optimization. First, we introduce a VLM-based instructor that encodes the textual instruction, the first frame of the source video, and an optional reference image into visually-grounded instruction representations, providing fine-grained spatial and semantic context for the diffusion transformer backbone. Second, we propose a post-training stage, Edit-GRPO, which adapts Group Relative Policy Optimization to the domain of video editing, directly optimizing the model for instruction-faithful, content-preserving, and aesthetically pleasing edits using relative rewards. Furthermore, we propose a data construction pipeline designed to synthetically generate diverse, high-fidelity paired video-instruction data of basic editing operations. Extensive experiments show that VIVA achieves superior instruction following, generalization, and editing quality over state-of-the-art methods. Website: https://viva-paper.github.io",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16906v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "instruction following"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "REGLUE Your Latents with Global and Local Semantics for Entangled Diffusion",
      "authors": [
        "Giorgos Petsangourakis",
        "Christos Sgouropoulos",
        "Bill Psomas",
        "Theodoros Giannakopoulos",
        "Giorgos Sfikas",
        "Ioannis Kakogeorgiou"
      ],
      "arxiv_id": "2512.16636v1",
      "summary": "Latent diffusion models (LDMs) achieve state-of-the-art image synthesis, yet their reconstruction-style denoising objective provides only indirect semantic supervision: high-level semantics emerge slowly, requiring longer training and limiting sample quality. Recent works inject semantics from Vision Foundation Models (VFMs) either externally via representation alignment or internally by jointly modeling only a narrow slice of VFM features inside the diffusion process, under-utilizing the rich, nonlinear, multi-layer spatial semantics available. We introduce REGLUE (Representation Entanglement with Global-Local Unified Encoding), a unified latent diffusion framework that jointly models (i) VAE image latents, (ii) compact local (patch-level) VFM semantics, and (iii) a global (image-level) [CLS] token within a single SiT backbone. A lightweight convolutional semantic compressor nonlinearly aggregates multi-layer VFM features into a low-dimensional, spatially structured representation, which is entangled with the VAE latents in the diffusion process. An external alignment loss further regularizes internal representations toward frozen VFM targets. On ImageNet 256x256, REGLUE consistently improves FID and accelerates convergence over SiT-B/2 and SiT-XL/2 baselines, as well as over REPA, ReDi, and REG. Extensive experiments show that (a) spatial VFM semantics are crucial, (b) non-linear compression is key to unlocking their full benefit, and (c) global tokens and external alignment act as complementary, lightweight enhancements within our global-local-latent joint modeling framework. The code is available at https://github.com/giorgospets/reglue .",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16636v1",
      "code_links": [
        {
          "url": "https://github.com/giorgospets/reglue",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "foundation model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "VenusBench-GD: A Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks",
      "authors": [
        "Beitong Zhou",
        "Zhexiao Huang",
        "Yuan Guo",
        "Zhangxuan Gu",
        "Tianyu Xia",
        "Zichen Luo",
        "Fei Tang",
        "Dehan Kong",
        "Yanyi Shang",
        "Suling Ou",
        "Zhenlin Guo",
        "Changhua Meng",
        "Shuheng Shen"
      ],
      "arxiv_id": "2512.16501v1",
      "summary": "GUI grounding is a critical component in building capable GUI agents. However, existing grounding benchmarks suffer from significant limitations: they either provide insufficient data volume and narrow domain coverage, or focus excessively on a single platform and require highly specialized domain knowledge. In this work, we present VenusBench-GD, a comprehensive, bilingual benchmark for GUI grounding that spans multiple platforms, enabling hierarchical evaluation for real-word applications. VenusBench-GD contributes as follows: (i) we introduce a large-scale, cross-platform benchmark with extensive coverage of applications, diverse UI elements, and rich annotated data, (ii) we establish a high-quality data construction pipeline for grounding tasks, achieving higher annotation accuracy than existing benchmarks, and (iii) we extend the scope of element grounding by proposing a hierarchical task taxonomy that divides grounding into basic and advanced categories, encompassing six distinct subtasks designed to evaluate models from complementary perspectives. Our experimental findings reveal critical insights: general-purpose multimodal models now match or even surpass specialized GUI models on basic grounding tasks. In contrast, advanced tasks, still favor GUI-specialized models, though they exhibit significant overfitting and poor robustness. These results underscore the necessity of comprehensive, multi-tiered evaluation frameworks.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16501v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "multimodal"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Avatar4D: Synthesizing Domain-Specific 4D Humans for Real-World Pose Estimation",
      "authors": [
        "Jerrin Bright",
        "Zhibo Wang",
        "Dmytro Klepachevskyi",
        "Yuhao Chen",
        "Sirisha Rambhatla",
        "David Clausi",
        "John Zelek"
      ],
      "arxiv_id": "2512.16199v1",
      "summary": "We present Avatar4D, a real-world transferable pipeline for generating customizable synthetic human motion datasets tailored to domain-specific applications. Unlike prior works, which focus on general, everyday motions and offer limited flexibility, our approach provides fine-grained control over body pose, appearance, camera viewpoint, and environmental context, without requiring any manual annotations. To validate the impact of Avatar4D, we focus on sports, where domain-specific human actions and movement patterns pose unique challenges for motion understanding. In this setting, we introduce Syn2Sport, a large-scale synthetic dataset spanning sports, including baseball and ice hockey. Avatar4D features high-fidelity 4D (3D geometry over time) human motion sequences with varying player appearances rendered in diverse environments. We benchmark several state-of-the-art pose estimation models on Syn2Sport and demonstrate their effectiveness for supervised learning, zero-shot transfer to real-world data, and generalization across sports. Furthermore, we evaluate how closely the generated synthetic data aligns with real-world datasets in feature space. Our results highlight the potential of such systems to generate scalable, controllable, and transferable human datasets for diverse domain-specific tasks without relying on domain-specific real data.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16199v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "zero-shot transfer"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times",
      "authors": [
        "Jintao Zhang",
        "Kaiwen Zheng",
        "Kai Jiang",
        "Haoxu Wang",
        "Ion Stoica",
        "Joseph E. Gonzalez",
        "Jianfei Chen",
        "Jun Zhu"
      ],
      "arxiv_id": "2512.16093v1",
      "summary": "We introduce TurboDiffusion, a video generation acceleration framework that can speed up end-to-end diffusion generation by 100-200x while maintaining video quality. TurboDiffusion mainly relies on several components for acceleration: (1) Attention acceleration: TurboDiffusion uses low-bit SageAttention and trainable Sparse-Linear Attention (SLA) to speed up attention computation. (2) Step distillation: TurboDiffusion adopts rCM for efficient step distillation. (3) W8A8 quantization: TurboDiffusion quantizes model parameters and activations to 8 bits to accelerate linear layers and compress the model. In addition, TurboDiffusion incorporates several other engineering optimizations.\n  We conduct experiments on the Wan2.2-I2V-14B-720P, Wan2.1-T2V-1.3B-480P, Wan2.1-T2V-14B-720P, and Wan2.1-T2V-14B-480P models. Experimental results show that TurboDiffusion achieves 100-200x speedup for video generation even on a single RTX 5090 GPU, while maintaining comparable video quality. The GitHub repository, which includes model checkpoints and easy-to-use code, is available at https://github.com/thu-ml/TurboDiffusion.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16093v1",
      "code_links": [
        {
          "url": "https://github.com/thu-ml/TurboDiffusion",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "linear attention",
            "distillation"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Machine Learning Enabled Graph Analysis of Particulate Composites: Application to Solid-state Battery Cathodes",
      "authors": [
        "Zebin Li",
        "Shimao Deng",
        "Yijin Liu",
        "Jia-Mian Hu"
      ],
      "arxiv_id": "2512.16085v1",
      "summary": "Particulate composites underpin many solid-state chemical and electrochemical systems, where microstructural features such as multiphase boundaries and inter-particle connections strongly influence system performance. Advances in X-ray microscopy enable capturing large-scale, multimodal images of these complex microstructures with an unprecedentedly high throughput. However, harnessing these datasets to discover new physical insights and guide microstructure optimization remains a major challenge. Here, we develop a machine learning (ML) enabled framework that enables automated transformation of experimental multimodal X-ray images of multiphase particulate composites into scalable, topology-aware graphs for extracting physical insights and establishing local microstructure-property relationships at both the particle and network level. Using the multiphase particulate cathode of solid-state lithium batteries as an example, our ML-enabled graph analysis corroborates the critical role of triple phase junctions and concurrent ion/electron conduction channels in realizing desirable local electrochemical activity. Our work establishes graph-based microstructure representation as a powerful paradigm for bridging multimodal experimental imaging and functional understanding, and facilitating microstructure-aware data-driven materials design in a broad range of particulate composites.",
      "categories": [
        "cond-mat.mtrl-sci",
        "cs.CV"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16085v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "multimodal"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Impacts of Racial Bias in Historical Training Data for News AI",
      "authors": [
        "Rahul Bhargava",
        "Malene Hornstrup Jespersen",
        "Emily Boardman Ndulue",
        "Vivica Dsouza"
      ],
      "arxiv_id": "2512.16901v1",
      "summary": "AI technologies have rapidly moved into business and research applications that involve large text corpora, including computational journalism research and newsroom settings. These models, trained on extant data from various sources, can be conceptualized as historical artifacts that encode decades-old attitudes and stereotypes. This paper investigates one such example trained on the broadly-used New York Times Annotated Corpus to create a multi-label classifier. Our use in research settings surfaced the concerning \"blacks\" thematic topic label. Through quantitative and qualitative means we investigate this label's use in the training corpus, what concepts it might be encoding in the trained classifier, and how those concepts impact our model use. Via the application of explainable AI methods, we find that the \"blacks\" label operates partially as a general \"racism detector\" across some minoritized groups. However, it performs poorly against expectations on modern examples such as COVID-19 era anti-Asian hate stories, and reporting on the Black Lives Matter movement. This case study of interrogating embedded biases in a model reveals how similar applications in newsroom settings can lead to unexpected outputs that could impact a wide variety of potential uses of any large language model-story discovery, audience targeting, summarization, etc. The fundamental tension this exposes for newsrooms is how to adopt AI-enabled workflow tools while reducing the risk of reproducing historical biases in news coverage.",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16901v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI",
      "authors": [
        "Hao Liang",
        "Xiaochen Ma",
        "Zhou Liu",
        "Zhen Hao Wong",
        "Zhengyang Zhao",
        "Zimo Meng",
        "Runming He",
        "Chengyu Shen",
        "Qifeng Cai",
        "Zhaoyang Han",
        "Meiyi Qiang",
        "Yalin Feng",
        "Tianyi Bai",
        "Zewei Pan",
        "Ziyi Guo",
        "Yizhen Jiang",
        "Jingwen Deng",
        "Qijie You",
        "Peichao Lai",
        "Tianyu Guo",
        "Chi Hsu Tsai",
        "Hengyi Feng",
        "Rui Hu",
        "Wenkai Yu",
        "Junbo Niu",
        "Bohan Zeng",
        "Ruichuan An",
        "Lu Ma",
        "Jihao Huang",
        "Yaowei Zheng",
        "Conghui He",
        "Linpeng Tang",
        "Bin Cui",
        "Weinan E",
        "Wentao Zhang"
      ],
      "arxiv_id": "2512.16676v1",
      "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16676v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Muon is Provably Faster with Momentum Variance Reduction",
      "authors": [
        "Xun Qian",
        "Hussein Rammal",
        "Dmitry Kovalev",
        "Peter Richtárik"
      ],
      "arxiv_id": "2512.16598v1",
      "summary": "Recent empirical research has demonstrated that deep learning optimizers based on the linear minimization oracle (LMO) over specifically chosen Non-Euclidean norm balls, such as Muon and Scion, outperform Adam-type methods in the training of large language models. In this work, we show that such optimizers can be provably improved by replacing their vanilla momentum by momentum variance reduction (MVR). Instead of proposing and analyzing MVR variants of Muon and Scion separately, we incorporate MVR into the recently proposed Gluon framework, which captures Muon, Scion and other specific Non-Euclidean LMO-based methods as special cases, and at the same time works with a more general smoothness assumption which better captures the layer-wise structure of neural networks. In the non-convex case, we incorporate MVR into Gluon in three different ways. All of them improve the convergence rate from ${\\cal O} (\\frac{1}{K^{1/4}})$ to ${\\cal O} (\\frac{1}{K^{1/3}})$. Additionally, we provide improved rates in the star-convex case. Finally, we conduct several numerical experiments that verify the superior performance of our proposed algorithms in terms of iteration complexity.",
      "categories": [
        "math.OC",
        "cs.LG"
      ],
      "primary_category": "math.OC",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "31 pages, 4 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16598v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "A Systematic Study of Code Obfuscation Against LLM-based Vulnerability Detection",
      "authors": [
        "Xiao Li",
        "Yue Li",
        "Hao Wu",
        "Yue Zhang",
        "Yechao Zhang",
        "Fengyuan Xu",
        "Sheng Zhong"
      ],
      "arxiv_id": "2512.16538v1",
      "summary": "As large language models (LLMs) are increasingly adopted for code vulnerability detection, their reliability and robustness across diverse vulnerability types have become a pressing concern. In traditional adversarial settings, code obfuscation has long been used as a general strategy to bypass auditing tools, preserving exploitability without tampering with the tools themselves. Numerous efforts have explored obfuscation methods and tools, yet their capabilities differ in terms of supported techniques, granularity, and programming languages, making it difficult to systematically assess their impact on LLM-based vulnerability detection. To address this gap, we provide a structured systematization of obfuscation techniques and evaluate them under a unified framework. Specifically, we categorize existing obfuscation methods into three major classes (layout, data flow, and control flow) covering 11 subcategories and 19 concrete techniques. We implement these techniques across four programming languages (Solidity, C, C++, and Python) using a consistent LLM-driven approach, and evaluate their effects on 15 LLMs spanning four model families (DeepSeek, OpenAI, Qwen, and LLaMA), as well as on two coding agents (GitHub Copilot and Codex). Our findings reveal both positive and negative impacts of code obfuscation on LLM-based vulnerability detection, highlighting conditions under which obfuscation leads to performance improvements or degradations. We further analyze these outcomes with respect to vulnerability characteristics, code properties, and model attributes. Finally, we outline several open problems and propose future directions to enhance the robustness of LLMs for real-world vulnerability detection.",
      "categories": [
        "cs.CR",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16538v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Feature-Selective Representation Misdirection for Machine Unlearning",
      "authors": [
        "Taozhao Chen",
        "Linghan Huang",
        "Kim-Kwang Raymond Choo",
        "Huaming Chen"
      ],
      "arxiv_id": "2512.16297v1",
      "summary": "As large language models (LLMs) are increasingly adopted in safety-critical and regulated sectors, the retention of sensitive or prohibited knowledge introduces escalating risks, ranging from privacy leakage to regulatory non-compliance to to potential misuse, and so on. Recent studies suggest that machine unlearning can help ensure deployed models comply with evolving legal, safety, and governance requirements. However, current unlearning techniques assume clean separation between forget and retain datasets, which is challenging in operational settings characterized by highly entangled distributions. In such scenarios, perturbation-based methods often degrade general model utility or fail to ensure safety. To address this, we propose Selective Representation Misdirection for Unlearning (SRMU), a novel principled activation-editing framework that enforces feature-aware and directionally controlled perturbations. Unlike indiscriminate model weights perturbations, SRMU employs a structured misdirection vector with an activation importance map. The goal is to allow SRMU selectively suppresses harmful representations while preserving the utility on benign ones. Experiments are conducted on the widely used WMDP benchmark across low- and high-entanglement configurations. Empirical results reveal that SRMU delivers state-of-the-art unlearning performance with minimal utility losses, and remains effective under 20-30\\% overlap where existing baselines collapse. SRMU provides a robust foundation for safety-driven model governance, privacy compliance, and controlled knowledge removal in the emerging LLM-based applications. We release the replication package at https://figshare.com/s/d5931192a8824de26aff.",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16297v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "In-Context Probing for Membership Inference in Fine-Tuned Language Models",
      "authors": [
        "Zhexi Lu",
        "Hongliang Chi",
        "Nathalie Baracaldo",
        "Swanand Ravindra Kadhe",
        "Yuseok Jeon",
        "Lei Yu"
      ],
      "arxiv_id": "2512.16292v1",
      "summary": "Membership inference attacks (MIAs) pose a critical privacy threat to fine-tuned large language models (LLMs), especially when models are adapted to domain-specific tasks using sensitive data. While prior black-box MIA techniques rely on confidence scores or token likelihoods, these signals are often entangled with a sample's intrinsic properties - such as content difficulty or rarity - leading to poor generalization and low signal-to-noise ratios. In this paper, we propose ICP-MIA, a novel MIA framework grounded in the theory of training dynamics, particularly the phenomenon of diminishing returns during optimization. We introduce the Optimization Gap as a fundamental signal of membership: at convergence, member samples exhibit minimal remaining loss-reduction potential, while non-members retain significant potential for further optimization. To estimate this gap in a black-box setting, we propose In-Context Probing (ICP), a training-free method that simulates fine-tuning-like behavior via strategically constructed input contexts. We propose two probing strategies: reference-data-based (using semantically similar public samples) and self-perturbation (via masking or generation). Experiments on three tasks and multiple LLMs show that ICP-MIA significantly outperforms prior black-box MIAs, particularly at low false positive rates. We further analyze how reference data alignment, model type, PEFT configurations, and training schedules affect attack effectiveness. Our findings establish ICP-MIA as a practical and theoretically grounded framework for auditing privacy risks in deployed LLMs.",
      "categories": [
        "cs.CR",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16292v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "CKA-Guided Modular Quantization: Beyond Bit-Width to Algorithmic Diversity",
      "authors": [
        "Jinhao Zhang",
        "Yunquan Zhang",
        "Daning Chen"
      ],
      "arxiv_id": "2512.16282v1",
      "summary": "Current mainstream post-training quantization methods for large language models typically apply a uniform quantization strategy across all network layers, overlooking the substantial differences in algorithmic suitability among layers. To address this limitation, we propose CKA Guided Modular Quantization, a fine-tuning-free, plug-and-play framework for algorithmic heterogeneous quantization. Our method independently evaluates multiple PTQ algorithms on each layer and employs Linear Centered Kernel Alignment (CKA) as a metric to automatically select the optimal quantization strategy per layer. The individually optimized strategies are then integrated to construct a hybrid quantized model. Experiments demonstrate that our approach consistently outperforms both uniform quantization baselines and state-of-the-art mixed-precision methods across mainstream LLMs including LLaMA and Qwen ,in terms of perplexity (PPL) and downstream task performance.",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16282v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Staggered Batch Scheduling: Co-optimizing Time-to-First-Token and Throughput for High-Efficiency LLM Inference",
      "authors": [
        "Jian Tian",
        "Shuailong Li",
        "Yang Cao",
        "Wenbo Cui",
        "Minghan Zhu",
        "Wenkang Wu",
        "Jianming Zhang",
        "Yanpeng Wang",
        "Zhiwen Xiao",
        "Zhenyu Hou",
        "Dou Shen"
      ],
      "arxiv_id": "2512.16134v1",
      "summary": "The evolution of Large Language Model (LLM) serving towards complex, distributed architectures--specifically the P/D-separated, large-scale DP+EP paradigm--introduces distinct scheduling challenges. Unlike traditional deployments where schedulers can treat instances as black boxes, DP+EP architectures exhibit high internal synchronization costs. We identify that immediate request dispatching in such systems leads to severe in-engine queuing and parallelization bubbles, degrading Time-to-First-Token (TTFT). To address this, we propose Staggered Batch Scheduling (SBS), a mechanism that deliberately buffers requests to form optimal execution batches. This temporal decoupling eliminates internal queuing bubbles without compromising throughput. Furthermore, leveraging the scheduling window created by buffering, we introduce a Load-Aware Global Allocation strategy that balances computational load across DP units for both Prefill and Decode phases. Deployed on a production H800 cluster serving Deepseek-V3, our system reduces TTFT by 30%-40% and improves throughput by 15%-20% compared to state-of-the-art immediate scheduling baselines.",
      "categories": [
        "cs.DC",
        "cs.LG"
      ],
      "primary_category": "cs.DC",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16134v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Scaling Text2SQL via LLM-efficient Schema Filtering with Functional Dependency Graph Rerankers",
      "authors": [
        "Thanh Dat Hoang",
        "Thanh Tam Nguyen",
        "Thanh Trung Huynh",
        "Hongzhi Yin",
        "Quoc Viet Hung Nguyen"
      ],
      "arxiv_id": "2512.16083v1",
      "summary": "Most modern Text2SQL systems prompt large language models (LLMs) with entire schemas -- mostly column information -- alongside the user's question. While effective on small databases, this approach fails on real-world schemas that exceed LLM context limits, even for commercial models. The recent Spider 2.0 benchmark exemplifies this with hundreds of tables and tens of thousands of columns, where existing systems often break. Current mitigations either rely on costly multi-step prompting pipelines or filter columns by ranking them against user's question independently, ignoring inter-column structure. To scale existing systems, we introduce \\toolname, an open-source, LLM-efficient schema filtering framework that compacts Text2SQL prompts by (i) ranking columns with a query-aware LLM encoder enriched with values and metadata, (ii) reranking inter-connected columns via a lightweight graph transformer over functional dependencies, and (iii) selecting a connectivity-preserving sub-schema with a Steiner-tree heuristic. Experiments on real datasets show that \\toolname achieves near-perfect recall and higher precision than CodeS, SchemaExP, Qwen rerankers, and embedding retrievers, while maintaining sub-second median latency and scaling to schemas with 23,000+ columns. Our source code is available at https://github.com/thanhdath/grast-sql.",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.DB",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16083v1",
      "code_links": [
        {
          "url": "https://github.com/thanhdath/grast-sql",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "From Facts to Conclusions : Integrating Deductive Reasoning in Retrieval-Augmented LLMs",
      "authors": [
        "Shubham Mishra",
        "Samyek Jain",
        "Gorang Mehrishi",
        "Shiv Tiwari",
        "Harsh Sharma",
        "Pratik Narang",
        "Dhruv Kumar"
      ],
      "arxiv_id": "2512.16795v1",
      "summary": "Retrieval-Augmented Generation (RAG) grounds large language models (LLMs) in external evidence, but fails when retrieved sources conflict or contain outdated or subjective information. Prior work address these issues independently but lack unified reasoning supervision. We propose a reasoning-trace-augmented RAG framework that adds structured, interpretable reasoning across three stages : (1) document-level adjudication, (2) conflict analysis, and (3) grounded synthesis, producing citation-linked answers or justified refusals. A Conflict-Aware Trust-Score (CATS) pipeline is introduced which evaluates groundedness, factual correctness, refusal accuracy, and conflict-behavior alignment using an LLM-as-a-Judge. Our 539-query reasoning dataset and evaluation pipeline establish a foundation for conflict-aware, interpretable RAG systems. Experimental results demonstrate substantial gains over baselines, most notably with Qwen, where Supervised Fine-Tuning improved End-to-End answer correctness from 0.069 to 0.883 and behavioral adherence from 0.074 to 0.722.",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "Under Review",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16795v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Plausibility as Failure: How LLMs and Humans Co-Construct Epistemic Error",
      "authors": [
        "Claudia Vale Oliveira",
        "Nelson Zagalo",
        "Filipe Silva",
        "Anabela Brandao",
        "Syeda Faryal Hussain Khurrum",
        "Joaquim Santos"
      ],
      "arxiv_id": "2512.16750v1",
      "summary": "Large language models (LLMs) are increasingly used as epistemic partners in everyday reasoning, yet their errors remain predominantly analyzed through predictive metrics rather than through their interpretive effects on human judgment. This study examines how different forms of epistemic failure emerge, are masked, and are tolerated in human AI interaction, where failure is understood as a relational breakdown shaped by model-generated plausibility and human interpretive judgment. We conducted a three round, multi LLM evaluation using interdisciplinary tasks and progressively differentiated assessment frameworks to observe how evaluators interpret model responses across linguistic, epistemic, and credibility dimensions. Our findings show that LLM errors shift from predictive to hermeneutic forms, where linguistic fluency, structural coherence, and superficially plausible citations conceal deeper distortions of meaning. Evaluators frequently conflated criteria such as correctness, relevance, bias, groundedness, and consistency, indicating that human judgment collapses analytical distinctions into intuitive heuristics shaped by form and fluency. Across rounds, we observed a systematic verification burden and cognitive drift. As tasks became denser, evaluators increasingly relied on surface cues, allowing erroneous yet well formed answers to pass as credible. These results suggest that error is not solely a property of model behavior but a co-constructed outcome of generative plausibility and human interpretive shortcuts. Understanding AI epistemic failure therefore requires reframing evaluation as a relational interpretive process, where the boundary between system failure and human miscalibration becomes porous. The study provides implications for LLM assessment, digital literacy, and the design of trustworthy human AI communication.",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "19 pages, 2 tables, 77 references, 6 appendices",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16750v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Cyber Humanism in Education: Reclaiming Agency through AI and Learning Sciences",
      "authors": [
        "Giovanni Adorni"
      ],
      "arxiv_id": "2512.16701v1",
      "summary": "Generative Artificial Intelligence (GenAI) is rapidly reshaping how knowledge is produced and validated in education. Rather than adding another digital tool, large language models reconfigure reading, writing, and coding into hybrid human-AI workflows, raising concerns about epistemic automation, cognitive offloading, and the de-professiona\\-lisation of teachers. This paper proposes \\emph{Cyber Humanism in Education} as a framework for reclaiming human agency in this landscape. We conceptualise AI-enabled learning environments as socio-technical infrastructures co-authored by humans and machines, and position educators and learners as epistemic agents and \\emph{algorithmic citizens} who have both the right and the responsibility to shape these infrastructures.\n  We articulate three pillars for cyber-humanist design, \\emph{reflexive competence}, \\emph{algorithmic citizenship}, and \\emph{dialogic design}, and relate them to major international digital and AI competence frameworks. We then present higher-education case studies that operationalise these ideas through \\emph{prompt-based learning} and a new \\emph{Conversational AI Educator} certification within the EPICT ecosystem. The findings show how such practices can strengthen epistemic agency while surfacing tensions around workload, equity, and governance, and outline implications for the future of AI-rich, human-centred education.",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "15 pages, 16 references, Key Note preented at the \"WAILS 2025 - The 2nd. Workshop on Artificial Intelligence with and for Learning Sciences\", Cagliary, Italy, 10-12 December 2025",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16701v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Microsoft Academic Graph Information Retrieval for Research Recommendation and Assistance",
      "authors": [
        "Jacob Reiss",
        "Shikshya Shiwakoti",
        "Samuel Goldsmith",
        "Ujjwal Pandit"
      ],
      "arxiv_id": "2512.16661v1",
      "summary": "In today's information-driven world, access to scientific publications has become increasingly easy. At the same time, filtering through the massive volume of available research has become more challenging than ever. Graph Neural Networks (GNNs) and graph attention mechanisms have shown strong effectiveness in searching large-scale information databases, particularly when combined with modern large language models. In this paper, we propose an Attention-Based Subgraph Retriever, a GNN-as-retriever model that applies attention-based pruning to extract a refined subgraph, which is then passed to a large language model for advanced knowledge reasoning.",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "5 pages, 3 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16661v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Refusal Steering: Fine-grained Control over LLM Refusal Behaviour for Sensitive Topics",
      "authors": [
        "Iker García-Ferrero",
        "David Montero",
        "Roman Orus"
      ],
      "arxiv_id": "2512.16602v1",
      "summary": "We introduce Refusal Steering, an inference-time method to exercise fine-grained control over Large Language Models refusal behaviour on politically sensitive topics without retraining. We replace fragile pattern-based refusal detection with an LLM-as-a-judge that assigns refusal confidence scores and we propose a ridge-regularized variant to compute steering vectors that better isolate the refusal--compliance direction. On Qwen3-Next-80B-A3B-Thinking, our method removes the refusal behaviour of the model around politically sensitive topics while maintaining safety on JailbreakBench and near-baseline performance on general benchmarks. The approach generalizes across 4B and 80B models and can also induce targeted refusals when desired. We analize the steering vectors and show that refusal signals concentrate in deeper layers of the transformer and are distributed across many dimensions. Together, these results demonstrate that activation steering can remove political refusal behaviour while retaining safety alignment for harmful content, offering a practical path to controllable, transparent moderation at inference time.",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16602v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "From Personalization to Prejudice: Bias and Discrimination in Memory-Enhanced AI Agents for Recruitment",
      "authors": [
        "Himanshu Gharat",
        "Himanshi Agrawal",
        "Gourab K. Patro"
      ],
      "arxiv_id": "2512.16532v1",
      "summary": "Large Language Models (LLMs) have empowered AI agents with advanced capabilities for understanding, reasoning, and interacting across diverse tasks. The addition of memory further enhances them by enabling continuity across interactions, learning from past experiences, and improving the relevance of actions and responses over time; termed as memory-enhanced personalization. Although such personalization through memory offers clear benefits, it also introduces risks of bias. While several previous studies have highlighted bias in ML and LLMs, bias due to memory-enhanced personalized agents is largely unexplored. Using recruitment as an example use case, we simulate the behavior of a memory-enhanced personalized agent, and study whether and how bias is introduced and amplified in and across various stages of operation. Our experiments on agents using safety-trained LLMs reveal that bias is systematically introduced and reinforced through personalization, emphasizing the need for additional protective measures or agent guardrails in memory-enhanced LLM-based AI agents.",
      "categories": [
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "In Proceedings of the Nineteenth ACM International Conference on Web Search and Data Mining (WSDM '26)",
      "doi": "10.1145/3773966.3779376",
      "journal_ref": "In Proceedings of the Nineteenth ACM International Conference on Web Search and Data Mining (WSDM '26), 2026, Boise, ID, USA. ACM, New York, NY, USA",
      "pdf_url": "https://arxiv.org/pdf/2512.16532v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "cuPilot: A Strategy-Coordinated Multi-agent Framework for CUDA Kernel Evolution",
      "authors": [
        "Jinwu Chen",
        "Qidie Wu",
        "Bin Li",
        "Lin Ma",
        "Xin Si",
        "Yang Hu",
        "Shouyi Yin",
        "Jun Yang"
      ],
      "arxiv_id": "2512.16465v1",
      "summary": "Optimizing CUDA kernels is a challenging and labor-intensive task, given the need for hardware-software co-design expertise and the proprietary nature of high-performance kernel libraries. While recent large language models (LLMs) combined with evolutionary algorithms show promise in automatic kernel optimization, existing approaches often fall short in performance due to their suboptimal agent designs and mismatched evolution representations. This work identifies these mismatches and proposes cuPilot, a strategy-coordinated multi-agent framework that introduces strategy as an intermediate semantic representation for kernel evolution. Key contributions include a strategy-coordinated evolution algorithm, roofline-guided prompting, and strategy-level population initialization. Experimental results show that the generated kernels by cuPilot achieve an average speed up of 3.09$\\times$ over PyTorch on a benchmark of 100 kernels. On the GEMM tasks, cuPilot showcases sophisticated optimizations and achieves high utilization of critical hardware units. The generated kernels are open-sourced at https://github.com/champloo2878/cuPilot-Kernels.git.",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16465v1",
      "code_links": [
        {
          "url": "https://github.com/champloo2878/cuPilot-Kernels.git",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Towards AI-Supported Research: a Vision of the TIB AIssistant",
      "authors": [
        "Sören Auer",
        "Allard Oelen",
        "Mohamad Yaser Jaradeh",
        "Mutahira Khalid",
        "Farhana Keya",
        "Sasi Kiran Gaddipati",
        "Jennifer D'Souza",
        "Lorenz Schlüter",
        "Amirreza Alasti",
        "Gollam Rabby",
        "Azanzi Jiomekong",
        "Oliver Karras"
      ],
      "arxiv_id": "2512.16447v1",
      "summary": "The rapid advancements in Generative AI and Large Language Models promise to transform the way research is conducted, potentially offering unprecedented opportunities to augment scholarly workflows. However, effectively integrating AI into research remains a challenge due to varying domain requirements, limited AI literacy, the complexity of coordinating tools and agents, and the unclear accuracy of Generative AI in research. We present the vision of the TIB AIssistant, a domain-agnostic human-machine collaborative platform designed to support researchers across disciplines in scientific discovery, with AI assistants supporting tasks across the research life cycle. The platform offers modular components - including prompt and tool libraries, a shared data store, and a flexible orchestration framework - that collectively facilitate ideation, literature analysis, methodology development, data analysis, and scholarly writing. We describe the conceptual framework, system architecture, and implementation of an early prototype that demonstrates the feasibility and potential impact of our approach.",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16447v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "TIB AIssistant: a Platform for AI-Supported Research Across Research Life Cycles",
      "authors": [
        "Allard Oelen",
        "Sören Auer"
      ],
      "arxiv_id": "2512.16442v1",
      "summary": "The rapidly growing popularity of adopting Artificial Intelligence (AI), and specifically Large Language Models (LLMs), is having a widespread impact throughout society, including the academic domain. AI-supported research has the potential to support researchers with tasks across the entire research life cycle. In this work, we demonstrate the TIB AIssistant, an AI-supported research platform providing support throughout the research life cycle. The AIssistant consists of a collection of assistants, each responsible for a specific research task. In addition, tools are provided to give access to external scholarly services. Generated data is stored in the assets and can be exported as an RO-Crate bundle to provide transparency and enhance reproducibility of the research project. We demonstrate the AIssistant's main functionalities by means of a sequential walk-through of assistants, interacting with each other to generate sections for a draft research paper. In the end, with the AIssistant, we lay the foundation for a larger agenda of providing a community-maintained platform for AI-supported research.",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16442v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Introducing ORKG ASK: an AI-driven Scholarly Literature Search and Exploration System Taking a Neuro-Symbolic Approach",
      "authors": [
        "Allard Oelen",
        "Mohamad Yaser Jaradeh",
        "Sören Auer"
      ],
      "arxiv_id": "2512.16425v1",
      "summary": "As the volume of published scholarly literature continues to grow, finding relevant literature becomes increasingly difficult. With the rise of generative Artificial Intelligence (AI), and particularly Large Language Models (LLMs), new possibilities emerge to find and explore literature. We introduce ASK (Assistant for Scientific Knowledge), an AI-driven scholarly literature search and exploration system that follows a neuro-symbolic approach. ASK aims to provide active support to researchers in finding relevant scholarly literature by leveraging vector search, LLMs, and knowledge graphs. The system allows users to input research questions in natural language and retrieve relevant articles. ASK automatically extracts key information and generates answers to research questions using a Retrieval-Augmented Generation (RAG) approach. We present an evaluation of ASK, assessing the system's usability and usefulness. Findings indicate that the system is user-friendly and users are generally satisfied while using the system.",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "10.1007/978-3-031-97207-2_2",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16425v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Synthelite: Chemist-aligned and feasibility-aware synthesis planning with LLMs",
      "authors": [
        "Nguyen Xuan-Vu",
        "Daniel Armstrong",
        "Milena Wehrbach",
        "Andres M Bran",
        "Zlatko Jončev",
        "Philippe Schwaller"
      ],
      "arxiv_id": "2512.16424v1",
      "summary": "Computer-aided synthesis planning (CASP) has long been envisioned as a complementary tool for synthetic chemists. However, existing frameworks often lack mechanisms to allow interaction with human experts, limiting their ability to integrate chemists' insights. In this work, we introduce Synthelite, a synthesis planning framework that uses large language models (LLMs) to directly propose retrosynthetic transformations. Synthelite can generate end-to-end synthesis routes by harnessing the intrinsic chemical knowledge and reasoning capabilities of LLMs, while allowing expert intervention through natural language prompts. Our experiments demonstrate that Synthelite can flexibly adapt its planning trajectory to diverse user-specified constraints, achieving up to 95\\% success rates in both strategy-constrained and starting-material-constrained synthesis tasks. Additionally, Synthelite exhibits the ability to account for chemical feasibility during route design. We envision Synthelite to be both a useful tool and a step toward a paradigm where LLMs are the central orchestrators of synthesis planning.",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16424v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "AI Needs Physics More Than Physics Needs AI",
      "authors": [
        "Peter Coveney",
        "Roger Highfield"
      ],
      "arxiv_id": "2512.16344v1",
      "summary": "Artificial intelligence (AI) is commonly depicted as transformative. Yet, after more than a decade of hype, its measurable impact remains modest outside a few high-profile scientific and commercial successes. The 2024 Nobel Prizes in Chemistry and Physics recognized AI's potential, but broader assessments indicate the impact to date is often more promotional than technical. We argue that while current AI may influence physics, physics has significantly more to offer this generation of AI. Current architectures - large language models, reasoning models, and agentic AI - can depend on trillions of meaningless parameters, suffer from distributional bias, lack uncertainty quantification, provide no mechanistic insights, and fail to capture even elementary scientific laws. We review critiques of these limits, highlight opportunities in quantum AI and analogue computing, and lay down a roadmap for the adoption of 'Big AI': a synthesis of theory-based rigour with the flexibility of machine learning.",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16344v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Agent Tools Orchestration Leaks More: Dataset, Benchmark, and Mitigation",
      "authors": [
        "Yuxuan Qiao",
        "Dongqin Liu",
        "Hongchang Yang",
        "Wei Zhou",
        "Songlin Hu"
      ],
      "arxiv_id": "2512.16310v1",
      "summary": "Driven by Large Language Models, the single-agent, multi-tool architecture has become a popular paradigm for autonomous agents due to its simplicity and effectiveness. However, this architecture also introduces a new and severe privacy risk, which we term Tools Orchestration Privacy Risk (TOP-R), where an agent, to achieve a benign user goal, autonomously aggregates information fragments across multiple tools and leverages its reasoning capabilities to synthesize unexpected sensitive information. We provide the first systematic study of this risk. First, we establish a formal framework, attributing the risk's root cause to the agent's misaligned objective function: an overoptimization for helpfulness while neglecting privacy awareness. Second, we construct TOP-Bench, comprising paired leakage and benign scenarios, to comprehensively evaluate this risk. To quantify the trade-off between safety and robustness, we introduce the H-Score as a holistic metric. The evaluation results reveal that TOP-R is a severe risk: the average Risk Leakage Rate (RLR) of eight representative models reaches 90.24%, while the average H-Score is merely 0.167, with no model exceeding 0.3. Finally, we propose the Privacy Enhancement Principle (PEP) method, which effectively mitigates TOP-R, reducing the Risk Leakage Rate to 46.58% and significantly improving the H-Score to 0.624. Our work reveals both a new class of risk and inherent structural limitations in current agent architectures, while also offering feasible mitigation strategies.",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16310v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Beyond the Benchmark: Innovative Defenses Against Prompt Injection Attacks",
      "authors": [
        "Safwan Shaheer",
        "G. M. Refatul Islam",
        "Mohammad Rafid Hamid",
        "Tahsin Zaman Jilan"
      ],
      "arxiv_id": "2512.16307v1",
      "summary": "In this fast-evolving area of LLMs, our paper discusses the significant security risk presented by prompt injection attacks. It focuses on small open-sourced models, specifically the LLaMA family of models. We introduce novel defense mechanisms capable of generating automatic defenses and systematically evaluate said generated defenses against a comprehensive set of benchmarked attacks. Thus, we empirically demonstrated the improvement proposed by our approach in mitigating goal-hijacking vulnerabilities in LLMs. Our work recognizes the increasing relevance of small open-sourced LLMs and their potential for broad deployments on edge devices, aligning with future trends in LLM applications. We contribute to the greater ecosystem of open-source LLMs and their security in the following: (1) assessing present prompt-based defenses against the latest attacks, (2) introducing a new framework using a seed defense (Chain Of Thoughts) to refine the defense prompts iteratively, and (3) showing significant improvements in detecting goal hijacking attacks. Out strategies significantly reduce the success rates of the attacks and false detection rates while at the same time effectively detecting goal-hijacking capabilities, paving the way for more secure and efficient deployments of small and open-source LLMs in resource-constrained environments.",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "10 pages, 4 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16307v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "chain-of-thought"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Adaptation of Agentic AI",
      "authors": [
        "Pengcheng Jiang",
        "Jiacheng Lin",
        "Zhiyi Shi",
        "Zifeng Wang",
        "Luxi He",
        "Yichen Wu",
        "Ming Zhong",
        "Peiyang Song",
        "Qizheng Zhang",
        "Heng Wang",
        "Xueqiang Xu",
        "Hanwen Xu",
        "Pengrui Han",
        "Dylan Zhang",
        "Jiashuo Sun",
        "Chaoqi Yang",
        "Kun Qian",
        "Tian Wang",
        "Changran Hu",
        "Manling Li",
        "Quanzheng Li",
        "Hao Peng",
        "Sheng Wang",
        "Jingbo Shang",
        "Chao Zhang",
        "Jiaxuan You",
        "Liyuan Liu",
        "Pan Lu",
        "Yu Zhang",
        "Heng Ji",
        "Yejin Choi",
        "Dawn Song",
        "Jimeng Sun",
        "Jiawei Han"
      ],
      "arxiv_id": "2512.16301v1",
      "summary": "Cutting-edge agentic AI systems are built on foundation models that can be adapted to plan, reason, and interact with external tools to perform increasingly complex and specialized tasks. As these systems grow in capability and scope, adaptation becomes a central mechanism for improving performance, reliability, and generalization. In this paper, we unify the rapidly expanding research landscape into a systematic framework that spans both agent adaptations and tool adaptations. We further decompose these into tool-execution-signaled and agent-output-signaled forms of agent adaptation, as well as agent-agnostic and agent-supervised forms of tool adaptation. We demonstrate that this framework helps clarify the design space of adaptation strategies in agentic AI, makes their trade-offs explicit, and provides practical guidance for selecting or switching among strategies during system design. We then review the representative approaches in each category, analyze their strengths and limitations, and highlight key open challenges and future opportunities. Overall, this paper aims to offer a conceptual foundation and practical roadmap for researchers and practitioners seeking to build more capable, efficient, and reliable agentic AI systems.",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16301v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "foundation model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "QuadSentinel: Sequent Safety for Machine-Checkable Control in Multi-agent Systems",
      "authors": [
        "Yiliu Yang",
        "Yilei Jiang",
        "Qunzhong Wang",
        "Yingshui Tan",
        "Xiaoyong Zhu",
        "Sherman S. M. Chow",
        "Bo Zheng",
        "Xiangyu Yue"
      ],
      "arxiv_id": "2512.16279v1",
      "summary": "Safety risks arise as large language model-based agents solve complex tasks with tools, multi-step plans, and inter-agent messages. However, deployer-written policies in natural language are ambiguous and context dependent, so they map poorly to machine-checkable rules, and runtime enforcement is unreliable. Expressing safety policies as sequents, we propose \\textsc{QuadSentinel}, a four-agent guard (state tracker, policy verifier, threat watcher, and referee) that compiles these policies into machine-checkable rules built from predicates over observable state and enforces them online. Referee logic plus an efficient top-$k$ predicate updater keeps costs low by prioritizing checks and resolving conflicts hierarchically. Measured on ST-WebAgentBench (ICML CUA~'25) and AgentHarm (ICLR~'25), \\textsc{QuadSentinel} improves guardrail accuracy and rule recall while reducing false positives. Against single-agent baselines such as ShieldAgent (ICML~'25), it yields better overall safety control. Near-term deployments can adopt this pattern without modifying core agents by keeping policies separate and machine-checkable. Our code will be made publicly available at https://github.com/yyiliu/QuadSentinel.",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "Preprint",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16279v1",
      "code_links": [
        {
          "url": "https://github.com/yyiliu/QuadSentinel",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Beyond Blind Spots: Analytic Hints for Mitigating LLM-Based Evaluation Pitfalls",
      "authors": [
        "Ora Nova Fandina",
        "Eitan Farchi",
        "Shmulik Froimovich",
        "Raviv Gal",
        "Wesam Ibraheem",
        "Rami Katan",
        "Alice Podolsky"
      ],
      "arxiv_id": "2512.16272v1",
      "summary": "Large Language Models are increasingly deployed as judges (LaaJ) in code generation pipelines. While attractive for scalability, LaaJs tend to overlook domain specific issues raising concerns about their reliability in critical evaluation tasks. To better understand these limitations in practice, we examine LaaJ behavior in a concrete industrial use case: legacy code modernization via COBOL code generation. In this setting, we find that even production deployed LaaJs can miss domain critical errors, revealing consistent blind spots in their evaluation capabilities.\n  To better understand these blind spots, we analyze generated COBOL programs and associated LaaJs judgments, drawing on expert knowledge to construct a preliminary taxonomy. Based on this taxonomy, we develop a lightweight analytic checker tool that flags over 30 domain specific issues observed in practice. We use its outputs as analytic hints, dynamically injecting them into the judges prompt to encourage LaaJ to revisit aspects it may have overlooked.\n  Experiments on a test set of 100 programs using four production level LaaJs show that LaaJ alone detects only about 45% of the errors present in the code (in all judges we tested), while the analytic checker alone lacks explanatory depth. When combined, the LaaJ+Hints configuration achieves up to 94% coverage (for the best performing judge and injection prompt) and produces qualitatively richer, more accurate explanations, demonstrating that analytic-LLM hybrids can substantially enhance evaluation reliability in deployed pipelines. We release the dataset and all used prompts.",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16272v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Learning to Wait: Synchronizing Agents with the Physical World",
      "authors": [
        "Yifei She",
        "Ping Zhang",
        "He Liu",
        "Yanmin Jia",
        "Yang Jing",
        "Zijun Liu",
        "Peng Sun",
        "Xiangbin Li",
        "Xiaohe Hu"
      ],
      "arxiv_id": "2512.16262v1",
      "summary": "Real-world agentic tasks, unlike synchronous Markov Decision Processes (MDPs), often involve non-blocking actions with variable latencies, creating a fundamental \\textit{Temporal Gap} between action initiation and completion. Existing environment-side solutions, such as blocking wrappers or frequent polling, either limit scalability or dilute the agent's context window with redundant observations. In this work, we propose an \\textbf{Agent-side Approach} that empowers Large Language Models (LLMs) to actively align their \\textit{Cognitive Timeline} with the physical world. By extending the Code-as-Action paradigm to the temporal domain, agents utilize semantic priors and In-Context Learning (ICL) to predict precise waiting durations (\\texttt{time.sleep(t)}), effectively synchronizing with asynchronous environment without exhaustive checking. Experiments in a simulated Kubernetes cluster demonstrate that agents can precisely calibrate their internal clocks to minimize both query overhead and execution latency, validating that temporal awareness is a learnable capability essential for autonomous evolution in open-ended environments.",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16262v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Sigma-Moe-Tiny Technical Report",
      "authors": [
        "Qingguo Hu",
        "Zhenghao Lin",
        "Ziyue Yang",
        "Yucheng Ding",
        "Xiao Liu",
        "Yuting Jiang",
        "Ruizhe Wang",
        "Tianyu Chen",
        "Zhongxin Guo",
        "Yifan Xiong",
        "Rui Gao",
        "Lei Qu",
        "Jinsong Su",
        "Peng Cheng",
        "Yeyun Gong"
      ],
      "arxiv_id": "2512.16248v1",
      "summary": "Mixture-of-Experts (MoE) has emerged as a promising paradigm for foundation models due to its efficient and powerful scalability. In this work, we present Sigma-MoE-Tiny, an MoE language model that achieves the highest sparsity compared to existing open-source models. Sigma-MoE-Tiny employs fine-grained expert segmentation with up to 96 experts per layer, while activating only one expert for each token, resulting in 20B total parameters with just 0.5B activated. The major challenge introduced by such extreme sparsity lies in expert load balancing. We find that the widely-used load balancing loss tends to become ineffective in the lower layers under this setting. To address this issue, we propose a progressive sparsification schedule aiming to balance expert utilization and training stability. Sigma-MoE-Tiny is pre-trained on a diverse and high-quality corpus, followed by post-training to further unlock its capabilities. The entire training process remains remarkably stable, with no occurrence of irrecoverable loss spikes. Comprehensive evaluations reveal that, despite activating only 0.5B parameters, Sigma-MoE-Tiny achieves top-tier performance among counterparts of comparable or significantly larger scale. In addition, we provide an in-depth discussion of load balancing in highly sparse MoE models, offering insights for advancing sparsity in future MoE architectures.\n  Project page: https://qghuxmu.github.io/Sigma-MoE-Tiny\n  Code: https://github.com/microsoft/ltp-megatron-lm",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16248v1",
      "code_links": [
        {
          "url": "https://github.com/microsoft/ltp-megatron-lm",
          "type": "github"
        },
        {
          "url": "https://qghuxmu.github.io/Sigma-MoE-Tiny",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "foundation model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Ev-Trust: A Strategy Equilibrium Trust Mechanism for Evolutionary Games in LLM-Based Multi-Agent Services",
      "authors": [
        "Shiduo Yang",
        "Jiye Wang",
        "Jiayu Qin",
        "Jianbin Li",
        "Yu Wang",
        "Yuanhe Zhao",
        "Kenan Guo"
      ],
      "arxiv_id": "2512.16167v1",
      "summary": "The rapid evolution of the Web toward an agent-centric paradigm, driven by large language models (LLMs), has enabled autonomous agents to reason, plan, and interact in complex decentralized environments. However, the openness and heterogeneity of LLM-based multi-agent systems also amplify the risks of deception, fraud, and misinformation, posing severe challenges to trust establishment and system robustness. To address this issue, we propose Ev-Trust, a strategy-equilibrium trust mechanism grounded in evolutionary game theory. This mechanism integrates direct trust, indirect trust, and expected revenue into a dynamic feedback structure that guides agents' behavioral evolution toward equilibria. Within a decentralized \"Request-Response-Payment-Evaluation\" service framework, Ev-Trust enables agents to adaptively adjust strategies, naturally excluding malicious participants while reinforcing high-quality collaboration. Furthermore, our theoretical derivation based on replicator dynamics equations proves the existence and stability of local evolutionary equilibria. Experimental results indicate that our approach effectively reflects agent trustworthiness in LLM-driven open service interaction scenarios, reduces malicious strategies, and increases collective revenue. We hope Ev-Trust can provide a new perspective on trust modeling for the agentic service web in group evolutionary game scenarios.",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.GT"
      ],
      "primary_category": "cs.MA",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "12 pages, 11 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16167v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "From Essence to Defense: Adaptive Semantic-aware Watermarking for Embedding-as-a-Service Copyright Protection",
      "authors": [
        "Hao Li",
        "Yubing Ren",
        "Yanan Cao",
        "Yingjie Li",
        "Fang Fang",
        "Xuebin Wang"
      ],
      "arxiv_id": "2512.16439v1",
      "summary": "Benefiting from the superior capabilities of large language models in natural language understanding and generation, Embeddings-as-a-Service (EaaS) has emerged as a successful commercial paradigm on the web platform. However, prior studies have revealed that EaaS is vulnerable to imitation attacks. Existing methods protect the intellectual property of EaaS through watermarking techniques, but they all ignore the most important properties of embedding: semantics, resulting in limited harmlessness and stealthiness. To this end, we propose SemMark, a novel semantic-based watermarking paradigm for EaaS copyright protection. SemMark employs locality-sensitive hashing to partition the semantic space and inject semantic-aware watermarks into specific regions, ensuring that the watermark signals remain imperceptible and diverse. In addition, we introduce the adaptive watermark weight mechanism based on the local outlier factor to preserve the original embedding distribution. Furthermore, we propose Detect-Sampling and Dimensionality-Reduction attacks and construct four scenarios to evaluate the watermarking method. Extensive experiments are conducted on four popular NLP datasets, and SemMark achieves superior verifiability, diversity, stealthiness, and harmlessness.",
      "categories": [
        "cs.CR",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16439v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Evaluating OpenAI GPT Models for Translation of Endangered Uralic Languages: A Comparison of Reasoning and Non-Reasoning Architectures",
      "authors": [
        "Yehor Tereshchenko",
        "Mika Hämäläinen",
        "Svitlana Myroniuk"
      ],
      "arxiv_id": "2512.16287v1",
      "summary": "The evaluation of Large Language Models (LLMs) for translation tasks has primarily focused on high-resource languages, leaving a significant gap in understanding their performance on low-resource and endangered languages. This study presents a comprehensive comparison of OpenAI's GPT models, specifically examining the differences between reasoning and non-reasoning architectures for translating between Finnish and four low-resource Uralic languages: Komi-Zyrian, Moksha, Erzya, and Udmurt. Using a parallel corpus of literary texts, we evaluate model willingness to attempt translation through refusal rate analysis across different model architectures. Our findings reveal significant performance variations between reasoning and non-reasoning models, with reasoning models showing 16 percentage points lower refusal rates. The results provide valuable insights for researchers and practitioners working with Uralic languages and contribute to the broader understanding of reasoning model capabilities for endangered language preservation.",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "IWCLUL 2025",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16287v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "LoPA: Scaling dLLM Inference via Lookahead Parallel Decoding",
      "authors": [
        "Chenkai Xu",
        "Yijie Jin",
        "Jiajun Li",
        "Yi Tu",
        "Guoping Long",
        "Dandan Tu",
        "Tianqi Hou",
        "Junchi Yan",
        "Zhijie Deng"
      ],
      "arxiv_id": "2512.16229v1",
      "summary": "Diffusion Large Language Models (dLLMs) have demonstrated significant potential for high-speed inference. However, current confidence-driven decoding strategies are constrained by limited parallelism, typically achieving only 1--3 tokens per forward pass (TPF). In this work, we identify that the degree of parallelism during dLLM inference is highly sensitive to the Token Filling Order (TFO). Then, we introduce Lookahead PArallel Decoding LoPA, a training-free, plug-and-play algorithm, to identify a superior TFO and hence accelerate inference. LoPA concurrently explores distinct candidate TFOs via parallel branches, and selects the one with the highest potential for future parallelism based on branch confidence. We apply LoPA to the state-of-the-art D2F model and observe a substantial enhancement in decoding efficiency. Notably, LoPA increases the TPF of D2F-Dream to 10.1 on the GSM8K while maintaining performance superior to the Dream baseline. Furthermore, to facilitate this unprecedented degree of parallelism, we develop a specialized multi-device inference system featuring Branch Parallelism (BP), which achieves a single-sample throughput of 1073.9 tokens per second under multi-GPU deployment. The code is available at https://github.com/zhijie-group/LoPA.",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16229v1",
      "code_links": [
        {
          "url": "https://github.com/zhijie-group/LoPA",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Machine Learning-based Optimal Control for Colloidal Self-Assembly",
      "authors": [
        "Andres Lizano-Villalobos",
        "Fangyuan Ma",
        "Wentao Tang",
        "Wei Sun",
        "Xun Tang"
      ],
      "arxiv_id": "2512.16402v1",
      "summary": "Achieving precise control of colloidal self-assembly into specific patterns remains a longstanding challenge due to the complex process dynamics. Recently, machine learning-based state representation and reinforcement learning-based control strategies have started to accumulate popularity in the field, showing great potential in achieving an automatable and generalizable approach to producing patterned colloidal assembly. In this work, we adopted a machine learning-based optimal control framework, combining unsupervised learning and graph convolutional neural work for state observation with deep reinforcement learning-based optimal control policy calculation, to provide a data-driven control approach that can potentially be generalized to other many-body self-assembly systems. With Brownian dynamics simulations, we demonstrated its superior performance as compared to traditional order parameter-based state description, and its efficacy in obtaining ordered 2-dimensional spherical colloidal self-assembly in an electric field-mediated system with an actual success rate of 97%.",
      "categories": [
        "cond-mat.soft",
        "eess.SY"
      ],
      "primary_category": "cond-mat.soft",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "19 pages, 5 figures, 1 table",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16402v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "deep reinforcement learning"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Prime and Reach: Synthesising Body Motion for Gaze-Primed Object Reach",
      "authors": [
        "Masashi Hatano",
        "Saptarshi Sinha",
        "Jacob Chalk",
        "Wei-Hong Li",
        "Hideo Saito",
        "Dima Damen"
      ],
      "arxiv_id": "2512.16456v1",
      "summary": "Human motion generation is a challenging task that aims to create realistic motion imitating natural human behaviour. We focus on the well-studied behaviour of priming an object/location for pick up or put down -- that is, the spotting of an object/location from a distance, known as gaze priming, followed by the motion of approaching and reaching the target location. To that end, we curate, for the first time, 23.7K gaze-primed human motion sequences for reaching target object locations from five publicly available datasets, i.e., HD-EPIC, MoGaze, HOT3D, ADT, and GIMO. We pre-train a text-conditioned diffusion-based motion generation model, then fine-tune it conditioned on goal pose or location, on our curated sequences. Importantly, we evaluate the ability of the generated motion to imitate natural human movement through several metrics, including the 'Reach Success' and a newly introduced 'Prime Success' metric. On the largest dataset, HD-EPIC, our model achieves 60% prime success and 89% reach success when conditioned on the goal object location.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "Project Page: https://masashi-hatano.github.io/prime-and-reach/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16456v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱四：生成式动作 (Generative Motion)",
          "id": "4_motion_diffusion",
          "matched_keywords": [
            "motion generation"
          ],
          "score": 2.5
        }
      ],
      "relevance_score": 2.5,
      "hit_pillars": [
        "4_motion_diffusion"
      ]
    },
    {
      "title": "Single-View Shape Completion for Robotic Grasping in Clutter",
      "authors": [
        "Abhishek Kashyap",
        "Yuxuan Yang",
        "Henrik Andreasson",
        "Todor Stoyanov"
      ],
      "arxiv_id": "2512.16449v1",
      "summary": "In vision-based robot manipulation, a single camera view can only capture one side of objects of interest, with additional occlusions in cluttered scenes further restricting visibility. As a result, the observed geometry is incomplete, and grasp estimation algorithms perform suboptimally. To address this limitation, we leverage diffusion models to perform category-level 3D shape completion from partial depth observations obtained from a single view, reconstructing complete object geometries to provide richer context for grasp planning. Our method focuses on common household items with diverse geometries, generating full 3D shapes that serve as input to downstream grasp inference networks. Unlike prior work, which primarily considers isolated objects or minimal clutter, we evaluate shape completion and grasping in realistic clutter scenarios with household objects. In preliminary evaluations on a cluttered scene, our approach consistently results in better grasp success rates than a naive baseline without shape completion by 23% and over a recent state of the art shape completion approach by 19%. Our code is available at https://amm.aass.oru.se/shape-completion-grasping/.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16449v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "A2VISR: An Active and Adaptive Ground-Aerial Localization System Using Visual Inertial and Single-Range Fusion",
      "authors": [
        "Sijia Chen",
        "Wei Dong"
      ],
      "arxiv_id": "2512.16367v1",
      "summary": "It's a practical approach using the ground-aerial collaborative system to enhance the localization robustness of flying robots in cluttered environments, especially when visual sensors degrade. Conventional approaches estimate the flying robot's position using fixed cameras observing pre-attached markers, which could be constrained by limited distance and susceptible to capture failure. To address this issue, we improve the ground-aerial localization framework in a more comprehensive manner, which integrates active vision, single-ranging, inertial odometry, and optical flow. First, the designed active vision subsystem mounted on the ground vehicle can be dynamically rotated to detect and track infrared markers on the aerial robot, improving the field of view and the target recognition with a single camera. Meanwhile, the incorporation of single-ranging extends the feasible distance and enhances re-capture capability under visual degradation. During estimation, a dimension-reduced estimator fuses multi-source measurements based on polynomial approximation with an extended sliding window, balancing computational efficiency and redundancy. Considering different sensor fidelities, an adaptive sliding confidence evaluation algorithm is implemented to assess measurement quality and dynamically adjust the weighting parameters based on moving variance. Finally, extensive experiments under conditions such as smoke interference, illumination variation, obstacle occlusion, prolonged visual loss, and extended operating range demonstrate that the proposed approach achieves robust online localization, with an average root mean square error of approximately 0.09 m, while maintaining resilience to capture loss and sensor failures.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "accept by IEEE Transactions on Industrial Electronics",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16367v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "optical flow"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Privacy-Aware Sharing of Raw Spatial Sensor Data for Cooperative Perception",
      "authors": [
        "Bangya Liu",
        "Chengpo Yan",
        "Chenghao Jiang",
        "Suman Banerjee",
        "Akarsh Prabhakara"
      ],
      "arxiv_id": "2512.16265v1",
      "summary": "Cooperative perception between vehicles is poised to offer robust and reliable scene understanding. Recently, we are witnessing experimental systems research building testbeds that share raw spatial sensor data for cooperative perception. While there has been a marked improvement in accuracies and is the natural way forward, we take a moment to consider the problems with such an approach for eventual adoption by automakers. In this paper, we first argue that new forms of privacy concerns arise and discourage stakeholders to share raw sensor data. Next, we present SHARP, a research framework to minimize privacy leakage and drive stakeholders towards the ambitious goal of raw data based cooperative perception. Finally, we discuss open questions for networked systems, mobile computing, perception researchers, industry and government in realizing our proposed framework.",
      "categories": [
        "cs.NI",
        "cs.RO"
      ],
      "primary_category": "cs.NI",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16265v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "scene understanding"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "EasyV2V: A High-quality Instruction-based Video Editing Framework",
      "authors": [
        "Jinjie Mai",
        "Chaoyang Wang",
        "Guocheng Gordon Qian",
        "Willi Menapace",
        "Sergey Tulyakov",
        "Bernard Ghanem",
        "Peter Wonka",
        "Ashkan Mirzaei"
      ],
      "arxiv_id": "2512.16920v1",
      "summary": "While image editing has advanced rapidly, video editing remains less explored, facing challenges in consistency, control, and generalization. We study the design space of data, architecture, and control, and introduce \\emph{EasyV2V}, a simple and effective framework for instruction-based video editing. On the data side, we compose existing experts with fast inverses to build diverse video pairs, lift image edit pairs into videos via single-frame supervision and pseudo pairs with shared affine motion, mine dense-captioned clips for video pairs, and add transition supervision to teach how edits unfold. On the model side, we observe that pretrained text-to-video models possess editing capability, motivating a simplified design. Simple sequence concatenation for conditioning with light LoRA fine-tuning suffices to train a strong model. For control, we unify spatiotemporal control via a single mask mechanism and support optional reference images. Overall, EasyV2V works with flexible inputs, e.g., video+text, video+mask+text, video+mask+reference+text, and achieves state-of-the-art video editing results, surpassing concurrent and commercial systems. Project page: https://snap-research.github.io/easyv2v/",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "Project page: https://snap-research.github.io/easyv2v/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16920v1",
      "code_links": [
        {
          "url": "https://snap-research.github.io/easyv2v/",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱八：物理动画 (Physics-based Animation)",
          "id": "8_physics_animation",
          "matched_keywords": [
            "spatiotemporal"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "8_physics_animation"
      ]
    },
    {
      "title": "Distributional AGI Safety",
      "authors": [
        "Nenad Tomašev",
        "Matija Franklin",
        "Julian Jacobs",
        "Sébastien Krier",
        "Simon Osindero"
      ],
      "arxiv_id": "2512.16856v1",
      "summary": "AI safety and alignment research has predominantly been focused on methods for safeguarding individual AI systems, resting on the assumption of an eventual emergence of a monolithic Artificial General Intelligence (AGI). The alternative AGI emergence hypothesis, where general capability levels are first manifested through coordination in groups of sub-AGI individual agents with complementary skills and affordances, has received far less attention. Here we argue that this patchwork AGI hypothesis needs to be given serious consideration, and should inform the development of corresponding safeguards and mitigations. The rapid deployment of advanced AI agents with tool-use capabilities and the ability to communicate and coordinate makes this an urgent safety consideration. We therefore propose a framework for distributional AGI safety that moves beyond evaluating and aligning individual agents. This framework centers on the design and implementation of virtual agentic sandbox economies (impermeable or semi-permeable), where agent-to-agent transactions are governed by robust market mechanisms, coupled with appropriate auditability, reputation management, and oversight to mitigate collective risks.",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16856v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "affordance"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "OS-Oracle: A Comprehensive Framework for Cross-Platform GUI Critic Models",
      "authors": [
        "Zhenyu Wu",
        "Jingjing Xie",
        "Zehao Li",
        "Bowen Yang",
        "Qiushi Sun",
        "Zhaoyang Liu",
        "Zhoumianze Liu",
        "Yu Qiao",
        "Xiangyu Yue",
        "Zun Wang",
        "Zichen Ding"
      ],
      "arxiv_id": "2512.16295v1",
      "summary": "With VLM-powered computer-using agents (CUAs) becoming increasingly capable at graphical user interface (GUI) navigation and manipulation, reliable step-level decision-making has emerged as a key bottleneck for real-world deployment. In long-horizon workflows, errors accumulate quickly and irreversible actions can cause unintended consequences, motivating critic models that assess each action before execution. While critic models offer a promising solution, their effectiveness is hindered by the lack of diverse, high-quality GUI feedback data and public critic benchmarks for step-level evaluation in computer use. To bridge these gaps, we introduce OS-Oracle that makes three core contributions: (1) a scalable data pipeline for synthesizing cross-platform GUI critic data; (2) a two-stage training paradigm combining supervised fine-tuning (SFT) and consistency-preserving group relative policy optimization (CP-GRPO); (3) OS-Critic Bench, a holistic benchmark for evaluating critic model performance across Mobile, Web, and Desktop platforms. Leveraging this framework, we curate a high-quality dataset containing 310k critic samples. The resulting critic model, OS-Oracle-7B, achieves state-of-the-art performance among open-source VLMs on OS-Critic Bench, and surpasses proprietary models on the mobile domain. Furthermore, when serving as a pre-critic, OS-Oracle-7B improves the performance of native GUI agents such as UI-TARS-1.5-7B in OSWorld and AndroidWorld environments. The code is open-sourced at https://github.com/numbmelon/OS-Oracle.",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16295v1",
      "code_links": [
        {
          "url": "https://github.com/numbmelon/OS-Oracle",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Resilience of coupled systems under deep uncertainty and dynamic complexity: An integrative literature review",
      "authors": [
        "Jannie Coenen",
        "Vítor Vasconcelos",
        "Heiman Wertheim",
        "Marcel Olde Rikkert",
        "Sophie Hadjisotiriou",
        "Vittorio Nespeca",
        "Tom Oreel",
        "Rick Quax",
        "Etiënne Rouwette",
        "Vincent Marchau",
        "Hubert Korzilius"
      ],
      "arxiv_id": "2512.16608v1",
      "summary": "Resilience in coupled systems is increasingly critical in addressing global challenges such as climate change and pandemics. These systems show unpredictable behaviour due to dynamic complexity and deep uncertainty across spatiotemporal scales. Despite growing interest, few studies systematically integrate both concepts when assessing resilience. This paper conducts an integrative review of 102 English-language publications to identify gaps in current approaches. Findings reveal that most papers address lower levels of uncertainty and rarely consider dynamic complexity and deep uncertainty simultaneously, which limits the effectiveness of resilience strategies. To advance systems research, we propose a conceptual framework and practical tools to support researchers and decision-makers in evaluating and improving resilience. The paper also outlines future research directions for more robust, adaptive, and integrative resilience assessments.",
      "categories": [
        "physics.soc-ph",
        "eess.SY"
      ],
      "primary_category": "physics.soc-ph",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16608v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱八：物理动画 (Physics-based Animation)",
          "id": "8_physics_animation",
          "matched_keywords": [
            "spatiotemporal"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "8_physics_animation"
      ]
    },
    {
      "title": "From Liability to Asset: A Three-Mode Grid-Forming Control Framework for Centralized Data Center UPS Systems",
      "authors": [
        "Mohamed Shamseldein"
      ],
      "arxiv_id": "2512.16497v1",
      "summary": "AI workloads are turning large data centers into highly dynamic power-electronic loads; fault-time behavior and workload pulsing can stress weak-grid points of interconnection. This paper proposes a centralized medium-voltage (MV) uninterruptible power supply (UPS) control architecture implemented as three operating modes: Mode 1 regulates a DC stiff bus and shapes normal-operation grid draw, Mode 2 enforces current-limited fault-mode P--Q priority with UPS battery energy storage system (UPS-BESS) buffering and a rate-limited post-fault \"soft return,\" and Mode 3 optionally provides droop-based fast frequency response via grid-draw modulation. Fundamental-frequency averaged dq simulations (50 MW block, short-circuit ratio (SCR) = 1.5, 0.5 p.u. three-phase dip for 150~ms) show zero unserved information-technology (IT) energy (0.00000 MWh vs.0.00208 MWh for a momentary-cessation benchmark), a 0.57 p.u. peak inverter current (vs. 1.02 p.u. for a synchronous-reference-frame phase-locked loop (SRF-PLL) low-voltage ride-through (LVRT) baseline), a nonzero mean fault-window grid draw of 0.20~p.u. (vs.approx 0 for momentary cessation), and an improved settled point-of-common-coupling (PCC) voltage minimum of 0.79 p.u. after one cycle (vs. 0.66 p.u.). A forced-oscillation case study applies a 1 Hz pulsed load (+/- 0.25 p.u.) and shows that the normal-operation shaping filters the oscillation seen by the grid while the UPS-BESS buffers the pulsing component.",
      "categories": [
        "eess.SY"
      ],
      "primary_category": "eess.SY",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16497v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱八：物理动画 (Physics-based Animation)",
          "id": "8_physics_animation",
          "matched_keywords": [
            "PULSE"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "8_physics_animation"
      ]
    }
  ],
  "filtered": true
}