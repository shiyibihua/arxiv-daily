{
    "papers": [
        {
            "title": "Moment-Based 3D Gaussian Splatting: Resolving Volumetric Occlusion with Order-Independent Transmittance",
            "authors": [
                "Jan U. Müller",
                "Robin Tim Landsgesell",
                "Leif Van Holland",
                "Patrick Stotko",
                "Reinhard Klein"
            ],
            "arxiv_id": "2512.11800v1",
            "summary": "The recent success of 3D Gaussian Splatting (3DGS) has reshaped novel view synthesis by enabling fast optimization and real-time rendering of high-quality radiance fields. However, it relies on simplified, order-dependent alpha blending and coarse approximations of the density integral within the rasterizer, thereby limiting its ability to render complex, overlapping semi-transparent objects. In this paper, we extend rasterization-based rendering of 3D Gaussian representations with a novel method for high-fidelity transmittance computation, entirely avoiding the need for ray tracing or per-pixel sample sorting. Building on prior work in moment-based order-independent transparency, our key idea is to characterize the density distribution along each camera ray with a compact and continuous representation based on statistical moments. To this end, we analytically derive and compute a set of per-pixel moments from all contributing 3D Gaussians. From these moments, a continuous transmittance function is reconstructed for each ray, which is then independently sampled within each Gaussian. As a result, our method bridges the gap between rasterization and physical accuracy by modeling light attenuation in complex translucent media, significantly improving overall reconstruction and rendering quality.",
            "headline_zh": "提出基于矩的3D高斯溅射方法，以解决复杂半透明物体渲染中的体积遮挡问题。",
            "intro_zh": [
                "3D高斯溅射依赖简化混合，难以渲染重叠半透明物体。",
                "利用统计矩表征密度分布，实现无排序的高精度透射率计算。",
                "方法提升重建和渲染质量，无需光线追踪或像素排序。"
            ],
            "tags_zh": [
                "3D高斯溅射",
                "体积渲染",
                "矩方法",
                "透射率计算",
                "光栅化渲染",
                "半透明物体"
            ],
            "_index": 0
        },
        {
            "title": "V-RGBX: Video Editing with Accurate Controls over Intrinsic Properties",
            "authors": [
                "Ye Fang",
                "Tong Wu",
                "Valentin Deschaintre",
                "Duygu Ceylan",
                "Iliyan Georgiev",
                "Chun-Hao Paul Huang",
                "Yiwei Hu",
                "Xuelin Chen",
                "Tuanfeng Yang Wang"
            ],
            "arxiv_id": "2512.11799v1",
            "summary": "Large-scale video generation models have shown remarkable potential in modeling photorealistic appearance and lighting interactions in real-world scenes. However, a closed-loop framework that jointly understands intrinsic scene properties (e.g., albedo, normal, material, and irradiance), leverages them for video synthesis, and supports editable intrinsic representations remains unexplored. We present V-RGBX, the first end-to-end framework for intrinsic-aware video editing. V-RGBX unifies three key capabilities: (1) video inverse rendering into intrinsic channels, (2) photorealistic video synthesis from these intrinsic representations, and (3) keyframe-based video editing conditioned on intrinsic channels. At the core of V-RGBX is an interleaved conditioning mechanism that enables intuitive, physically grounded video editing through user-selected keyframes, supporting flexible manipulation of any intrinsic modality. Extensive qualitative and quantitative results show that V-RGBX produces temporally consistent, photorealistic videos while propagating keyframe edits across sequences in a physically plausible manner. We demonstrate its effectiveness in diverse applications, including object appearance editing and scene-level relighting, surpassing the performance of prior methods.",
            "headline_zh": "提出V-RGBX框架，实现基于内在属性的视频编辑，支持关键帧编辑与物理一致性传播。",
            "intro_zh": [
                "核心问题：现有视频生成模型缺乏联合理解与编辑内在场景属性的闭环框架。",
                "方法要点：通过视频逆渲染、合成与关键帧条件编辑，统一内在属性感知的视频编辑能力。",
                "实验或效果：在物体外观编辑和场景重照明等应用中，生成时间一致、逼真的视频，超越先前方法。"
            ],
            "tags_zh": [
                "视频编辑",
                "内在属性感知",
                "逆渲染",
                "关键帧编辑",
                "物理一致性",
                "场景重照明"
            ],
            "_index": 1
        },
        {
            "title": "Particulate: Feed-Forward 3D Object Articulation",
            "authors": [
                "Ruining Li",
                "Yuxin Yao",
                "Chuanxia Zheng",
                "Christian Rupprecht",
                "Joan Lasenby",
                "Shangzhe Wu",
                "Andrea Vedaldi"
            ],
            "arxiv_id": "2512.11798v1",
            "summary": "We present Particulate, a feed-forward approach that, given a single static 3D mesh of an everyday object, directly infers all attributes of the underlying articulated structure, including its 3D parts, kinematic structure, and motion constraints. At its core is a transformer network, Part Articulation Transformer, which processes a point cloud of the input mesh using a flexible and scalable architecture to predict all the aforementioned attributes with native multi-joint support. We train the network end-to-end on a diverse collection of articulated 3D assets from public datasets. During inference, Particulate lifts the network's feed-forward prediction to the input mesh, yielding a fully articulated 3D model in seconds, much faster than prior approaches that require per-object optimization. Particulate can also accurately infer the articulated structure of AI-generated 3D assets, enabling full-fledged extraction of articulated 3D objects from a single (real or synthetic) image when combined with an off-the-shelf image-to-3D generator. We further introduce a new challenging benchmark for 3D articulation estimation curated from high-quality public 3D assets, and redesign the evaluation protocol to be more consistent with human preferences. Quantitative and qualitative results show that Particulate significantly outperforms state-of-the-art approaches.",
            "headline_zh": "提出Particulate前馈方法，从单静态3D网格直接推断日常物体的完整铰接结构。",
            "intro_zh": [
                "核心问题：从单静态3D网格自动推断物体的铰接结构，包括部件、运动学和约束。",
                "方法要点：使用Part Articulation Transformer处理点云，前馈预测多关节属性，无需逐对象优化。",
                "实验或效果：在公共数据集上训练，推理速度快，优于现有方法，支持AI生成资产。"
            ],
            "tags_zh": [
                "3D铰接估计",
                "前馈网络",
                "Transformer架构",
                "点云处理",
                "运动学推断",
                "单图像到3D"
            ],
            "_index": 2
        },
        {
            "title": "AnchorDream: Repurposing Video Diffusion for Embodiment-Aware Robot Data Synthesis",
            "authors": [
                "Junjie Ye",
                "Rong Xue",
                "Basile Van Hoorick",
                "Pavel Tokmakov",
                "Muhammad Zubair Irshad",
                "Yue Wang",
                "Vitor Guizilini"
            ],
            "arxiv_id": "2512.11797v1",
            "summary": "The collection of large-scale and diverse robot demonstrations remains a major bottleneck for imitation learning, as real-world data acquisition is costly and simulators offer limited diversity and fidelity with pronounced sim-to-real gaps. While generative models present an attractive solution, existing methods often alter only visual appearances without creating new behaviors, or suffer from embodiment inconsistencies that yield implausible motions. To address these limitations, we introduce AnchorDream, an embodiment-aware world model that repurposes pretrained video diffusion models for robot data synthesis. AnchorDream conditions the diffusion process on robot motion renderings, anchoring the embodiment to prevent hallucination while synthesizing objects and environments consistent with the robot's kinematics. Starting from only a handful of human teleoperation demonstrations, our method scales them into large, diverse, high-quality datasets without requiring explicit environment modeling. Experiments show that the generated data leads to consistent improvements in downstream policy learning, with relative gains of 36.4% in simulator benchmarks and nearly double performance in real-world studies. These results suggest that grounding generative world models in robot motion provides a practical path toward scaling imitation learning.",
            "headline_zh": "提出AnchorDream，通过视频扩散模型合成机器人数据以解决模仿学习数据瓶颈问题。",
            "intro_zh": [
                "核心问题：机器人模仿学习面临大规模多样化数据获取成本高、仿真器多样性有限且存在仿真到现实差距。",
                "方法要点：利用预训练视频扩散模型，以机器人运动渲染为条件，合成与机器人运动学一致的对象和环境数据。",
                "实验或效果：生成数据提升下游策略学习性能，在仿真基准中相对增益36.4%，真实世界性能近翻倍。"
            ],
            "tags_zh": [
                "机器人数据合成",
                "视频扩散模型",
                "模仿学习",
                "仿真到现实迁移",
                "生成模型"
            ],
            "_index": 3
        },
        {
            "title": "A General Algorithm for Detecting Higher-Order Interactions via Random Sequential Additions",
            "authors": [
                "Ahmad Shamail",
                "Claire McWhite"
            ],
            "arxiv_id": "2512.11793v1",
            "summary": "Many systems exhibit complex interactions between their components: some features or actions amplify each other's effects, others provide redundant information, and some contribute independently. We present a simple geometric method for discovering interactions and redundancies: when elements are added in random sequential orders and their contributions plotted over many trials, characteristic L-shaped patterns emerge that directly reflect interaction structure. The approach quantifies how the contribution of each element depends on those added before it, revealing patterns that distinguish interaction, independence, and redundancy on a unified scale. When pairwise contributions are visualized as two--dimensional point clouds, redundant pairs form L--shaped patterns where only the first-added element contributes, while synergistic pairs form L--shaped patterns where only elements contribute together. Independent elements show order--invariant distributions. We formalize this with the L--score, a continuous measure ranging from $-1$ (perfect synergy, e.g. $Y=X_1X_2$) to $0$ (independence) to $+1$ (perfect redundancy, $X_1 \\approx X_2$). The relative scaling of the L--shaped arms reveals feature dominance in which element consistently provides more information. Although computed only from pairwise measurements, higher--order interactions among three or more elements emerge naturally through consistent cross--pair relationships (e.g. AB, AC, BC). The method is metric--agnostic and broadly applicable to any domain where performance can be evaluated incrementally over non-repeating element sequences, providing a unified geometric approach to uncovering interaction structure.",
            "headline_zh": "提出基于随机顺序添加的几何方法，以检测特征间的高阶交互与冗余",
            "intro_zh": [
                "核心问题：系统组件间存在复杂交互，如协同、冗余或独立，需量化检测",
                "方法要点：通过随机顺序添加元素并绘制贡献图，利用L形模式量化交互结构",
                "实验或效果：定义L分数从-1到+1连续度量交互，适用于任何可增量评估性能的领域"
            ],
            "tags_zh": [
                "交互检测",
                "几何方法",
                "高阶交互",
                "冗余分析",
                "随机顺序添加",
                "L分数"
            ],
            "_index": 4
        },
        {
            "title": "Structure From Tracking: Distilling Structure-Preserving Motion for Video Generation",
            "authors": [
                "Yang Fei",
                "George Stoica",
                "Jingyuan Liu",
                "Qifeng Chen",
                "Ranjay Krishna",
                "Xiaojuan Wang",
                "Benlin Liu"
            ],
            "arxiv_id": "2512.11792v1",
            "summary": "Reality is a dance between rigid constraints and deformable structures. For video models, that means generating motion that preserves fidelity as well as structure. Despite progress in diffusion models, producing realistic structure-preserving motion remains challenging, especially for articulated and deformable objects such as humans and animals. Scaling training data alone, so far, has failed to resolve physically implausible transitions. Existing approaches rely on conditioning with noisy motion representations, such as optical flow or skeletons extracted using an external imperfect model. To address these challenges, we introduce an algorithm to distill structure-preserving motion priors from an autoregressive video tracking model (SAM2) into a bidirectional video diffusion model (CogVideoX). With our method, we train SAM2VideoX, which contains two innovations: (1) a bidirectional feature fusion module that extracts global structure-preserving motion priors from a recurrent model like SAM2; (2) a Local Gram Flow loss that aligns how local features move together. Experiments on VBench and in human studies show that SAM2VideoX delivers consistent gains (+2.60\\% on VBench, 21-22\\% lower FVD, and 71.4\\% human preference) over prior baselines. Specifically, on VBench, we achieve 95.51\\%, surpassing REPA (92.91\\%) by 2.60\\%, and reduce FVD to 360.57, a 21.20\\% and 22.46\\% improvement over REPA- and LoRA-finetuning, respectively. The project website can be found at https://sam2videox.github.io/ .",
            "headline_zh": "提出SAM2VideoX方法，通过蒸馏结构保持运动先验以提升视频生成质量",
            "intro_zh": [
                "核心问题：现有视频生成模型难以产生结构保持的逼真运动，尤其在关节和可变形物体上。",
                "方法要点：从自回归视频跟踪模型SAM2蒸馏运动先验，结合双向特征融合模块和局部Gram流损失。",
                "实验效果：在VBench上提升2.60%，FVD降低21-22%，人类偏好率达71.4%。"
            ],
            "tags_zh": [
                "视频生成",
                "结构保持运动",
                "蒸馏训练",
                "扩散模型",
                "运动先验"
            ],
            "_index": 5
        },
        {
            "title": "Uncertainty-Aware Domain Adaptation for Vitiligo Segmentation in Clinical Photographs",
            "authors": [
                "Wentao Jiang",
                "Vamsi Varra",
                "Caitlin Perez-Stable",
                "Harrison Zhu",
                "Meredith Apicella",
                "Nicole Nyamongo"
            ],
            "arxiv_id": "2512.11791v1",
            "summary": "Accurately quantifying vitiligo extent in routine clinical photographs is crucial for longitudinal monitoring of treatment response. We propose a trustworthy, frequency-aware segmentation framework built on three synergistic pillars: (1) a data-efficient training strategy combining domain-adaptive pre-training on the ISIC 2019 dataset with an ROI-constrained dual-task loss to suppress background noise; (2) an architectural refinement via a ConvNeXt V2-based encoder enhanced with a novel High-Frequency Spectral Gating (HFSG) module and stem-skip connections to capture subtle textures; and (3) a clinical trust mechanism employing K-fold ensemble and Test-Time Augmentation (TTA) to generate pixel-wise uncertainty maps. Extensive validation on an expert-annotated clinical cohort demonstrates superior performance, achieving a Dice score of 85.05% and significantly reducing boundary error (95% Hausdorff Distance improved from 44.79 px to 29.95 px), consistently outperforming strong CNN (ResNet-50 and UNet++) and Transformer (MiT-B5) baselines. Notably, our framework demonstrates high reliability with zero catastrophic failures and provides interpretable entropy maps to identify ambiguous regions for clinician review. Our approach suggests that the proposed framework establishes a robust and reliable standard for automated vitiligo assessment.",
            "headline_zh": "提出不确定性感知域适应框架，用于临床照片中白癜风分割，以提升量化准确性和可靠性。",
            "intro_zh": [
                "核心问题：临床照片中白癜风区域分割受背景噪声和纹理细微变化影响，需高精度量化以监测治疗反应。",
                "方法要点：结合域适应预训练、高频谱门控模块和双任务损失，增强模型对细微纹理的捕捉并抑制噪声。",
                "实验或效果：在专家标注数据集上验证，Dice分数达85.05%，边界误差显著降低，优于CNN和Transformer基线，提供不确定性地图增强临床信任。"
            ],
            "tags_zh": [
                "医学图像分割",
                "域适应",
                "不确定性估计",
                "高频特征提取",
                "临床照片分析",
                "白癜风量化"
            ],
            "_index": 6
        },
        {
            "title": "Toward a Decision Support System for Energy-Efficient Ferry Operation on Lake Constance based on Optimal Control",
            "authors": [
                "Hannes Homburger",
                "Bastian Jäckl",
                "Stefan Wirtensohn",
                "Christian Stopp",
                "Maximilian T. Fischer",
                "Moritz Diehl",
                "Daniel A. Keim",
                "Johannes Reuter"
            ],
            "arxiv_id": "2512.11786v1",
            "summary": "The maritime sector is undergoing a disruptive technological change driven by three main factors: autonomy, decarbonization, and digital transformation. Addressing these factors necessitates a reassessment of inland vessel operations. This paper presents the design and development of a decision support system for ferry operations based on a shrinking-horizon optimal control framework. The problem formulation incorporates a mathematical model of the ferry's dynamics and environmental disturbances, specifically water currents and wind, which can significantly influence the dynamics. Real-world data and illustrative scenarios demonstrate the potential of the proposed system to effectively support ferry crews by providing real-time guidance. This enables enhanced operational efficiency while maintaining predefined maneuver durations. The findings suggest that optimal control applications hold substantial promise for advancing future ferry operations on inland waters. A video of the real-world ferry MS Insel Mainau operating on Lake Constance is available at: https://youtu.be/i1MjCdbEQyE",
            "headline_zh": "提出基于收缩时域最优控制的决策支持系统，以优化康斯坦茨湖渡轮能效操作。",
            "intro_zh": [
                "核心问题：海事领域面临自主化、脱碳和数字化转型，需重新评估内河船舶操作效率。",
                "方法要点：采用收缩时域最优控制框架，结合渡轮动力学和环境扰动（水流和风）的数学模型。",
                "实验或效果：基于真实数据和场景，系统能提供实时指导，提升操作效率并保持预定航行时间。"
            ],
            "tags_zh": [
                "最优控制",
                "决策支持系统",
                "渡轮操作",
                "能效优化",
                "内河航运"
            ],
            "_index": 7
        },
        {
            "title": "Softmax as Linear Attention in the Large-Prompt Regime: a Measure-based Perspective",
            "authors": [
                "Etienne Boursier",
                "Claire Boyer"
            ],
            "arxiv_id": "2512.11784v1",
            "summary": "Softmax attention is a central component of transformer architectures, yet its nonlinear structure poses significant challenges for theoretical analysis. We develop a unified, measure-based framework for studying single-layer softmax attention under both finite and infinite prompts. For i.i.d. Gaussian inputs, we lean on the fact that the softmax operator converges in the infinite-prompt limit to a linear operator acting on the underlying input-token measure. Building on this insight, we establish non-asymptotic concentration bounds for the output and gradient of softmax attention, quantifying how rapidly the finite-prompt model approaches its infinite-prompt counterpart, and prove that this concentration remains stable along the entire training trajectory in general in-context learning settings with sub-Gaussian tokens. In the case of in-context linear regression, we use the tractable infinite-prompt dynamics to analyze training at finite prompt length. Our results allow optimization analyses developed for linear attention to transfer directly to softmax attention when prompts are sufficiently long, showing that large-prompt softmax attention inherits the analytical structure of its linear counterpart. This, in turn, provides a principled and broadly applicable toolkit for studying the training dynamics and statistical behavior of softmax attention layers in large prompt regimes.",
            "headline_zh": "提出基于测度的框架，证明长提示下Softmax注意力收敛为线性算子，便于理论分析。",
            "intro_zh": [
                "核心问题：Softmax注意力的非线性结构导致理论分析困难，尤其在长提示场景下。",
                "方法要点：基于测度框架，证明在无限提示极限下Softmax收敛为线性算子，并建立非渐近浓度界。",
                "实验或效果：在上下文线性回归中，利用无限提示动态分析有限提示训练，展示长提示下Softmax继承线性结构。"
            ],
            "tags_zh": [
                "Softmax注意力",
                "线性注意力",
                "测度理论",
                "长提示分析",
                "上下文学习",
                "训练动态"
            ],
            "_index": 8
        },
        {
            "title": "Super Suffixes: Bypassing Text Generation Alignment and Guard Models Simultaneously",
            "authors": [
                "Andrew Adiletta",
                "Kathryn Adiletta",
                "Kemal Derya",
                "Berk Sunar"
            ],
            "arxiv_id": "2512.11783v1",
            "summary": "The rapid deployment of Large Language Models (LLMs) has created an urgent need for enhanced security and privacy measures in Machine Learning (ML). LLMs are increasingly being used to process untrusted text inputs and even generate executable code, often while having access to sensitive system controls. To address these security concerns, several companies have introduced guard models, which are smaller, specialized models designed to protect text generation models from adversarial or malicious inputs. In this work, we advance the study of adversarial inputs by introducing Super Suffixes, suffixes capable of overriding multiple alignment objectives across various models with different tokenization schemes. We demonstrate their effectiveness, along with our joint optimization technique, by successfully bypassing the protection mechanisms of Llama Prompt Guard 2 on five different text generation models for malicious text and code generation. To the best of our knowledge, this is the first work to reveal that Llama Prompt Guard 2 can be compromised through joint optimization.\n  Additionally, by analyzing the changing similarity of a model's internal state to specific concept directions during token sequence processing, we propose an effective and lightweight method to detect Super Suffix attacks. We show that the cosine similarity between the residual stream and certain concept directions serves as a distinctive fingerprint of model intent. Our proposed countermeasure, DeltaGuard, significantly improves the detection of malicious prompts generated through Super Suffixes. It increases the non-benign classification rate to nearly 100%, making DeltaGuard a valuable addition to the guard model stack and enhancing robustness against adversarial prompt attacks.",
            "headline_zh": "提出Super Suffixes以绕过文本生成对齐和防护模型，并设计DeltaGuard进行检测",
            "intro_zh": [
                "核心问题：大型语言模型面临对抗性输入威胁，防护模型如Llama Prompt Guard 2可能被绕过",
                "方法要点：引入Super Suffixes后缀，通过联合优化技术同时绕过多种对齐目标和防护机制",
                "实验或效果：在五个文本生成模型上成功绕过Llama Prompt Guard 2，DeltaGuard检测率提升至近100%"
            ],
            "tags_zh": [
                "对抗性攻击",
                "文本生成安全",
                "防护模型",
                "联合优化",
                "恶意检测"
            ],
            "_index": 9
        },
        {
            "title": "MatAnyone 2: Scaling Video Matting via a Learned Quality Evaluator",
            "authors": [
                "Peiqing Yang",
                "Shangchen Zhou",
                "Kai Hao",
                "Qingyi Tao"
            ],
            "arxiv_id": "2512.11782v1",
            "summary": "Video matting remains limited by the scale and realism of existing datasets. While leveraging segmentation data can enhance semantic stability, the lack of effective boundary supervision often leads to segmentation-like mattes lacking fine details. To this end, we introduce a learned Matting Quality Evaluator (MQE) that assesses semantic and boundary quality of alpha mattes without ground truth. It produces a pixel-wise evaluation map that identifies reliable and erroneous regions, enabling fine-grained quality assessment. The MQE scales up video matting in two ways: (1) as an online matting-quality feedback during training to suppress erroneous regions, providing comprehensive supervision, and (2) as an offline selection module for data curation, improving annotation quality by combining the strengths of leading video and image matting models. This process allows us to build a large-scale real-world video matting dataset, VMReal, containing 28K clips and 2.4M frames. To handle large appearance variations in long videos, we introduce a reference-frame training strategy that incorporates long-range frames beyond the local window for effective training. Our MatAnyone 2 achieves state-of-the-art performance on both synthetic and real-world benchmarks, surpassing prior methods across all metrics.",
            "headline_zh": "提出学习型抠图质量评估器以扩展视频抠图，通过质量反馈和数据集构建提升性能。",
            "intro_zh": [
                "视频抠图受限于数据集规模和真实性，缺乏有效边界监督导致细节缺失。",
                "引入学习型抠图质量评估器，无需真值评估语义和边界质量，提供像素级评估图。",
                "构建大规模真实视频抠图数据集VMReal，结合参考帧训练策略，在基准测试中达到最优性能。"
            ],
            "tags_zh": [
                "视频抠图",
                "质量评估",
                "数据集构建",
                "参考帧训练",
                "语义稳定性",
                "边界监督"
            ],
            "_index": 10
        },
        {
            "title": "Agile Flight Emerges from Multi-Agent Competitive Racing",
            "authors": [
                "Vineet Pasumarti",
                "Lorenzo Bianchi",
                "Antonio Loquercio"
            ],
            "arxiv_id": "2512.11781v1",
            "summary": "Through multi-agent competition and the sparse high-level objective of winning a race, we find that both agile flight (e.g., high-speed motion pushing the platform to its physical limits) and strategy (e.g., overtaking or blocking) emerge from agents trained with reinforcement learning. We provide evidence in both simulation and the real world that this approach outperforms the common paradigm of training agents in isolation with rewards that prescribe behavior, e.g., progress on the raceline, in particular when the complexity of the environment increases, e.g., in the presence of obstacles. Moreover, we find that multi-agent competition yields policies that transfer more reliably to the real world than policies trained with a single-agent progress-based reward, despite the two methods using the same simulation environment, randomization strategy, and hardware. In addition to improved sim-to-real transfer, the multi-agent policies also exhibit some degree of generalization to opponents unseen at training time. Overall, our work, following in the tradition of multi-agent competitive game-play in digital domains, shows that sparse task-level rewards are sufficient for training agents capable of advanced low-level control in the physical world.\n  Code: https://github.com/Jirl-upenn/AgileFlight_MultiAgent",
            "headline_zh": "提出多智能体竞争强化学习以训练无人机实现敏捷飞行与策略，超越单智能体训练范式。",
            "intro_zh": [
                "核心问题：传统单智能体训练依赖行为奖励，难以在复杂环境中实现高效敏捷飞行与策略。",
                "方法要点：通过多智能体竞争和稀疏高层目标（赢得比赛），训练强化学习智能体，无需详细行为奖励。",
                "实验或效果：在仿真和真实世界中验证，多智能体方法在复杂环境、仿真到真实迁移和泛化性方面表现更优。"
            ],
            "tags_zh": [
                "多智能体强化学习",
                "敏捷飞行",
                "仿真到真实迁移",
                "无人机控制",
                "竞争训练",
                "稀疏奖励"
            ],
            "_index": 11
        },
        {
            "title": "Conditional Coverage Diagnostics for Conformal Prediction",
            "authors": [
                "Sacha Braun",
                "David Holzmüller",
                "Michael I. Jordan",
                "Francis Bach"
            ],
            "arxiv_id": "2512.11779v1",
            "summary": "Evaluating conditional coverage remains one of the most persistent challenges in assessing the reliability of predictive systems. Although conformal methods can give guarantees on marginal coverage, no method can guarantee to produce sets with correct conditional coverage, leaving practitioners without a clear way to interpret local deviations. To overcome sample-inefficiency and overfitting issues of existing metrics, we cast conditional coverage estimation as a classification problem. Conditional coverage is violated if and only if any classifier can achieve lower risk than the target coverage. Through the choice of a (proper) loss function, the resulting risk difference gives a conservative estimate of natural miscoverage measures such as L1 and L2 distance, and can even separate the effects of over- and under-coverage, and non-constant target coverages. We call the resulting family of metrics excess risk of the target coverage (ERT). We show experimentally that the use of modern classifiers provides much higher statistical power than simple classifiers underlying established metrics like CovGap. Additionally, we use our metric to benchmark different conformal prediction methods. Finally, we release an open-source package for ERT as well as previous conditional coverage metrics. Together, these contributions provide a new lens for understanding, diagnosing, and improving the conditional reliability of predictive systems.",
            "headline_zh": "提出ERT指标以解决共形预测中条件覆盖评估的样本低效和过拟合问题。",
            "intro_zh": [
                "核心问题：共形预测方法无法保证条件覆盖，现有评估指标存在样本低效和过拟合。",
                "方法要点：将条件覆盖估计转化为分类问题，通过风险差计算保守估计的ERT指标。",
                "实验或效果：使用现代分类器提高统计功效，并用于基准测试不同共形预测方法。"
            ],
            "tags_zh": [
                "共形预测",
                "条件覆盖",
                "分类问题",
                "风险差",
                "统计评估",
                "开源工具"
            ],
            "_index": 12
        },
        {
            "title": "The Adaptive Vekua Cascade: A Differentiable Spectral-Analytic Solver for Physics-Informed Representation",
            "authors": [
                "Vladimer Khasia"
            ],
            "arxiv_id": "2512.11776v1",
            "summary": "Coordinate-based neural networks have emerged as a powerful tool for representing continuous physical fields, yet they face two fundamental pathologies: spectral bias, which hinders the learning of high-frequency dynamics, and the curse of dimensionality, which causes parameter explosion in discrete feature grids. We propose the Adaptive Vekua Cascade (AVC), a hybrid architecture that bridges deep learning and classical approximation theory. AVC decouples manifold learning from function approximation by using a deep network to learn a diffeomorphic warping of the physical domain, projecting complex spatiotemporal dynamics onto a latent manifold where the solution is represented by a basis of generalized analytic functions. Crucially, we replace the standard gradient-descent output layer with a differentiable linear solver, allowing the network to optimally resolve spectral coefficients in a closed form during the forward pass. We evaluate AVC on a suite of five rigorous physics benchmarks, including high-frequency Helmholtz wave propagation, sparse medical reconstruction, and unsteady 3D Navier-Stokes turbulence. Our results demonstrate that AVC achieves state-of-the-art accuracy while reducing parameter counts by orders of magnitude (e.g., 840 parameters vs. 4.2 million for 3D grids) and converging 2-3x faster than implicit neural representations. This work establishes a new paradigm for memory-efficient, spectrally accurate scientific machine learning. The code is available at https://github.com/VladimerKhasia/vecua.",
            "headline_zh": "提出自适应Vekua级联以解决坐标神经网络中的谱偏差和维度灾难问题",
            "intro_zh": [
                "核心问题：坐标神经网络存在谱偏差和维度灾难，影响高频率学习和参数效率",
                "方法要点：使用深度网络学习物理域变形，结合可微分线性求解器优化谱系数",
                "实验或效果：在物理基准测试中实现高精度，参数减少数量级，收敛速度提升2-3倍"
            ],
            "tags_zh": [
                "坐标神经网络",
                "谱偏差",
                "维度灾难",
                "可微分求解器",
                "物理信息表示",
                "自适应Vekua级联"
            ],
            "_index": 13
        },
        {
            "title": "ProbeMDE: Uncertainty-Guided Active Proprioception for Monocular Depth Estimation in Surgical Robotics",
            "authors": [
                "Britton Jordan",
                "Jordan Thompson",
                "Jesse F. d'Almeida",
                "Hao Li",
                "Nithesh Kumar",
                "Susheela Sharma Stern",
                "Ipek Oguz",
                "Robert J. Webster",
                "Daniel Brown",
                "Alan Kuntz",
                "James Ferguson"
            ],
            "arxiv_id": "2512.11773v1",
            "summary": "Monocular depth estimation (MDE) provides a useful tool for robotic perception, but its predictions are often uncertain and inaccurate in challenging environments such as surgical scenes where textureless surfaces, specular reflections, and occlusions are common. To address this, we propose ProbeMDE, a cost-aware active sensing framework that combines RGB images with sparse proprioceptive measurements for MDE. Our approach utilizes an ensemble of MDE models to predict dense depth maps conditioned on both RGB images and on a sparse set of known depth measurements obtained via proprioception, where the robot has touched the environment in a known configuration. We quantify predictive uncertainty via the ensemble's variance and measure the gradient of the uncertainty with respect to candidate measurement locations. To prevent mode collapse while selecting maximally informative locations to propriocept (touch), we leverage Stein Variational Gradient Descent (SVGD) over this gradient map. We validate our method in both simulated and physical experiments on central airway obstruction surgical phantoms. Our results demonstrate that our approach outperforms baseline methods across standard depth estimation metrics, achieving higher accuracy while minimizing the number of required proprioceptive measurements.",
            "headline_zh": "提出ProbeMDE框架，结合RGB图像与稀疏本体感知测量以提升手术机器人单目深度估计精度",
            "intro_zh": [
                "核心问题：单目深度估计在手术场景中因纹理缺失、镜面反射和遮挡导致预测不确定和不准确",
                "方法要点：利用模型集成预测密集深度图，基于不确定性梯度通过SVGD选择最优本体感知测量位置",
                "实验或效果：在模拟和物理实验中验证，优于基线方法，提高精度并减少所需测量次数"
            ],
            "tags_zh": [
                "单目深度估计",
                "手术机器人",
                "不确定性引导",
                "主动感知",
                "本体感知测量",
                "模型集成"
            ],
            "_index": 14
        },
        {
            "title": "Smudged Fingerprints: A Systematic Evaluation of the Robustness of AI Image Fingerprints",
            "authors": [
                "Kai Yao",
                "Marc Juarez"
            ],
            "arxiv_id": "2512.11771v1",
            "summary": "Model fingerprint detection techniques have emerged as a promising approach for attributing AI-generated images to their source models, but their robustness under adversarial conditions remains largely unexplored. We present the first systematic security evaluation of these techniques, formalizing threat models that encompass both white- and black-box access and two attack goals: fingerprint removal, which erases identifying traces to evade attribution, and fingerprint forgery, which seeks to cause misattribution to a target model. We implement five attack strategies and evaluate 14 representative fingerprinting methods across RGB, frequency, and learned-feature domains on 12 state-of-the-art image generators. Our experiments reveal a pronounced gap between clean and adversarial performance. Removal attacks are highly effective, often achieving success rates above 80% in white-box settings and over 50% under constrained black-box access. While forgery is more challenging than removal, its success significantly varies across targeted models. We also identify a utility-robustness trade-off: methods with the highest attribution accuracy are often vulnerable to attacks. Although some techniques exhibit robustness in specific settings, none achieves high robustness and accuracy across all evaluated threat models. These findings highlight the need for techniques balancing robustness and accuracy, and identify the most promising approaches for advancing this goal.",
            "headline_zh": "系统评估AI图像指纹的鲁棒性，揭示其在对抗攻击下的脆弱性",
            "intro_zh": [
                "核心问题：AI图像指纹检测技术在对抗条件下的鲁棒性未知，威胁模型包括白盒和黑盒访问。",
                "方法要点：提出五种攻击策略，评估14种指纹方法在RGB、频率和特征域的鲁棒性。",
                "实验或效果：移除攻击在白盒下成功率超80%，黑盒下超50%；指纹伪造成功率因目标模型而异，存在准确性与鲁棒性权衡。"
            ],
            "tags_zh": [
                "AI图像指纹",
                "对抗攻击",
                "鲁棒性评估",
                "模型归因",
                "安全威胁模型"
            ],
            "_index": 15
        },
        {
            "title": "BLURR: A Boosted Low-Resource Inference for Vision-Language-Action Models",
            "authors": [
                "Xiaoyu Ma",
                "Zhengqing Yuan",
                "Zheyuan Zhang",
                "Kaiwen Shi",
                "Lichao Sun",
                "Yanfang Ye"
            ],
            "arxiv_id": "2512.11769v1",
            "summary": "Vision-language-action (VLA) models enable impressive zero shot manipulation, but their inference stacks are often too heavy for responsive web demos or high frequency robot control on commodity GPUs. We present BLURR, a lightweight inference wrapper that can be plugged into existing VLA controllers without retraining or changing model checkpoints. Instantiated on the pi-zero VLA controller, BLURR keeps the original observation interfaces and accelerates control by combining an instruction prefix key value cache, mixed precision execution, and a single step rollout schedule that reduces per step computation. In our SimplerEnv based evaluation, BLURR maintains task success rates comparable to the original controller while significantly lowering effective FLOPs and wall clock latency. We also build an interactive web demo that allows users to switch between controllers and toggle inference options in real time while watching manipulation episodes. This highlights BLURR as a practical approach for deploying modern VLA policies under tight compute budgets.",
            "headline_zh": "提出BLURR轻量推理包装器，以在有限计算资源下加速视觉-语言-动作模型部署。",
            "intro_zh": [
                "核心问题：VLA模型推理栈过重，难以在普通GPU上实现响应式Web演示或高频机器人控制。",
                "方法要点：通过指令前缀键值缓存、混合精度执行和单步展开调度，无需重训练即可加速现有VLA控制器。",
                "实验或效果：在SimplerEnv评估中保持任务成功率，显著降低FLOPs和延迟，并构建交互式Web演示。"
            ],
            "tags_zh": [
                "视觉-语言-动作模型",
                "轻量推理",
                "键值缓存",
                "混合精度",
                "机器人控制",
                "Web部署"
            ],
            "_index": 16
        },
        {
            "title": "Learning Minimal Representations of Fermionic Ground States",
            "authors": [
                "Felix Frohnert",
                "Emiel Koridon",
                "Stefano Polla"
            ],
            "arxiv_id": "2512.11767v1",
            "summary": "We introduce an unsupervised machine-learning framework that discovers optimally compressed representations of quantum many-body ground states. Using an autoencoder neural network architecture on data from $L$-site Fermi-Hubbard models, we identify minimal latent spaces with a sharp reconstruction quality threshold at $L-1$ latent dimensions, matching the system's intrinsic degrees of freedom. We demonstrate the use of the trained decoder as a differentiable variational ansatz to minimize energy directly within the latent space. Crucially, this approach circumvents the $N$-representability problem, as the learned manifold implicitly restricts the optimization to physically valid quantum states.",
            "headline_zh": "提出无监督机器学习框架以发现费米子多体基态的最优压缩表示",
            "intro_zh": [
                "核心问题：如何压缩量子多体基态表示，避免N-可表示性问题",
                "方法要点：使用自编码器神经网络在费米-哈伯德模型数据上学习最小潜在空间",
                "实验或效果：潜在空间维度为L-1时重建质量阈值尖锐，匹配系统内在自由度"
            ],
            "tags_zh": [
                "量子多体系统",
                "自编码器",
                "费米-哈伯德模型",
                "变分ansatz",
                "无监督学习",
                "N-可表示性"
            ],
            "_index": 17
        },
        {
            "title": "Reducing Domain Gap with Diffusion-Based Domain Adaptation for Cell Counting",
            "authors": [
                "Mohammad Dehghanmanshadi",
                "Wallapak Tavanapong"
            ],
            "arxiv_id": "2512.11763v1",
            "summary": "Generating realistic synthetic microscopy images is critical for training deep learning models in label-scarce environments, such as cell counting with many cells per image. However, traditional domain adaptation methods often struggle to bridge the domain gap when synthetic images lack the complex textures and visual patterns of real samples. In this work, we adapt the Inversion-Based Style Transfer (InST) framework originally designed for artistic style transfer to biomedical microscopy images. Our method combines latent-space Adaptive Instance Normalization with stochastic inversion in a diffusion model to transfer the style from real fluorescence microscopy images to synthetic ones, while weakly preserving content structure.\n  We evaluate the effectiveness of our InST-based synthetic dataset for downstream cell counting by pre-training and fine-tuning EfficientNet-B0 models on various data sources, including real data, hard-coded synthetic data, and the public Cell200-s dataset. Models trained with our InST-synthesized images achieve up to 37\\% lower Mean Absolute Error (MAE) compared to models trained on hard-coded synthetic data, and a 52\\% reduction in MAE compared to models trained on Cell200-s (from 53.70 to 25.95 MAE). Notably, our approach also outperforms models trained on real data alone (25.95 vs. 27.74 MAE). Further improvements are achieved when combining InST-synthesized data with lightweight domain adaptation techniques such as DACS with CutMix. These findings demonstrate that InST-based style transfer most effectively reduces the domain gap between synthetic and real microscopy data. Our approach offers a scalable path for enhancing cell counting performance while minimizing manual labeling effort. The source code and resources are publicly available at: https://github.com/MohammadDehghan/InST-Microscopy.",
            "headline_zh": "提出基于扩散模型的InST风格迁移方法，以降低细胞计数中合成与真实显微图像的域差距。",
            "intro_zh": [
                "核心问题：传统域适应方法难以处理合成显微图像缺乏真实纹理和视觉模式的问题。",
                "方法要点：结合潜在空间自适应实例归一化和扩散模型中的随机反转，将真实荧光显微图像风格迁移至合成图像。",
                "实验或效果：在细胞计数任务中，InST合成数据训练模型比硬编码合成数据降低37% MAE，比Cell200-s降低52% MAE。"
            ],
            "tags_zh": [
                "域适应",
                "扩散模型",
                "风格迁移",
                "细胞计数",
                "显微图像"
            ],
            "_index": 18
        },
        {
            "title": "SpectralKrum: A Spectral-Geometric Defense Against Byzantine Attacks in Federated Learning",
            "authors": [
                "Aditya Tripathi",
                "Karan Sharma",
                "Rahul Mishra",
                "Tapas Kumar Maiti"
            ],
            "arxiv_id": "2512.11760v1",
            "summary": "Federated Learning (FL) distributes model training across clients who retain their data locally, but this architecture exposes a fundamental vulnerability: Byzantine clients can inject arbitrarily corrupted updates that degrade or subvert the global model. While robust aggregation methods (including Krum, Bulyan, and coordinate-wise defenses) offer theoretical guarantees under idealized assumptions, their effectiveness erodes substantially when client data distributions are heterogeneous (non-IID) and adversaries can observe or approximate the defense mechanism.\n  This paper introduces SpectralKrum, a defense that fuses spectral subspace estimation with geometric neighbor-based selection. The core insight is that benign optimization trajectories, despite per-client heterogeneity, concentrate near a low-dimensional manifold that can be estimated from historical aggregates. SpectralKrum projects incoming updates into this learned subspace, applies Krum selection in compressed coordinates, and filters candidates whose orthogonal residual energy exceeds a data-driven threshold. The method requires no auxiliary data, operates entirely on model updates, and preserves FL privacy properties.\n  We evaluate SpectralKrum against eight robust baselines across seven attack scenarios on CIFAR-10 with Dirichlet-distributed non-IID partitions (alpha = 0.1). Experiments spanning over 56,000 training rounds show that SpectralKrum is competitive against directional and subspace-aware attacks (adaptive-steer, buffer-drift), but offers limited advantage under label-flip and min-max attacks where malicious updates remain spectrally indistinguishable from benign ones.",
            "headline_zh": "提出SpectralKrum防御方法，结合谱子空间估计与几何邻居选择，以应对联邦学习中的拜占庭攻击。",
            "intro_zh": [
                "核心问题：联邦学习在非独立同分布数据下，现有鲁棒聚合方法对拜占庭攻击的防御效果显著下降。",
                "方法要点：通过历史聚合估计低维流形，将更新投影到子空间，结合Krum选择和残差能量阈值过滤。",
                "实验或效果：在CIFAR-10非独立同分布数据上评估，对方向性和子空间感知攻击有效，但对标签翻转和最小最大攻击优势有限。"
            ],
            "tags_zh": [
                "联邦学习",
                "拜占庭攻击防御",
                "谱子空间估计",
                "鲁棒聚合",
                "非独立同分布数据",
                "模型更新过滤"
            ],
            "_index": 19
        },
        {
            "title": "LUCID: Learning-Enabled Uncertainty-Aware Certification of Stochastic Dynamical Systems",
            "authors": [
                "Ernesto Casablanca",
                "Oliver Schön",
                "Paolo Zuliani",
                "Sadegh Soudjani"
            ],
            "arxiv_id": "2512.11750v1",
            "summary": "Ensuring the safety of AI-enabled systems, particularly in high-stakes domains such as autonomous driving and healthcare, has become increasingly critical. Traditional formal verification tools fall short when faced with systems that embed both opaque, black-box AI components and complex stochastic dynamics. To address these challenges, we introduce LUCID (Learning-enabled Uncertainty-aware Certification of stochastIc Dynamical systems), a verification engine for certifying safety of black-box stochastic dynamical systems from a finite dataset of random state transitions. As such, LUCID is the first known tool capable of establishing quantified safety guarantees for such systems. Thanks to its modular architecture and extensive documentation, LUCID is designed for easy extensibility. LUCID employs a data-driven methodology rooted in control barrier certificates, which are learned directly from system transition data, to ensure formal safety guarantees. We use conditional mean embeddings to embed data into a reproducing kernel Hilbert space (RKHS), where an RKHS ambiguity set is constructed that can be inflated to robustify the result to out-of-distribution behavior. A key innovation within LUCID is its use of a finite Fourier kernel expansion to reformulate a semi-infinite non-convex optimization problem into a tractable linear program. The resulting spectral barrier allows us to leverage the fast Fourier transform to generate the relaxed problem efficiently, offering a scalable yet distributionally robust framework for verifying safety. LUCID thus offers a robust and efficient verification framework, able to handle the complexities of modern black-box systems while providing formal guarantees of safety. These unique capabilities are demonstrated on challenging benchmarks.",
            "headline_zh": "提出LUCID以解决黑盒随机动态系统的安全验证问题",
            "intro_zh": [
                "核心问题：传统形式化验证工具难以处理嵌入不透明AI组件和复杂随机动态的系统",
                "方法要点：基于数据驱动学习控制屏障证书，利用RKHS嵌入和傅里叶核展开将非凸优化转化为线性规划",
                "实验或效果：在挑战性基准测试中展示其提供形式化安全保证的能力"
            ],
            "tags_zh": [
                "随机动态系统",
                "安全验证",
                "控制屏障证书",
                "RKHS嵌入",
                "傅里叶核展开",
                "线性规划"
            ],
            "_index": 20
        },
        {
            "title": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder",
            "authors": [
                "Minglei Shi",
                "Haolin Wang",
                "Borui Zhang",
                "Wenzhao Zheng",
                "Bohan Zeng",
                "Ziyang Yuan",
                "Xiaoshi Wu",
                "Yuanxing Zhang",
                "Huan Yang",
                "Xintao Wang",
                "Pengfei Wan",
                "Kun Gai",
                "Jie Zhou",
                "Jiwen Lu"
            ],
            "arxiv_id": "2512.11749v1",
            "summary": "Visual generation grounded in Visual Foundation Model (VFM) representations offers a highly promising unified pathway for integrating visual understanding, perception, and generation. Despite this potential, training large-scale text-to-image diffusion models entirely within the VFM representation space remains largely unexplored. To bridge this gap, we scale the SVG (Self-supervised representations for Visual Generation) framework, proposing SVG-T2I to support high-quality text-to-image synthesis directly in the VFM feature domain. By leveraging a standard text-to-image diffusion pipeline, SVG-T2I achieves competitive performance, reaching 0.75 on GenEval and 85.78 on DPG-Bench. This performance validates the intrinsic representational power of VFMs for generative tasks. We fully open-source the project, including the autoencoder and generation model, together with their training, inference, evaluation pipelines, and pre-trained weights, to facilitate further research in representation-driven visual generation.",
            "headline_zh": "提出SVG-T2I以在视觉基础模型表示空间中实现高质量文本到图像合成",
            "intro_zh": [
                "核心问题：在视觉基础模型表示空间中训练大规模文本到图像扩散模型尚未充分探索",
                "方法要点：扩展SVG框架，直接在VFM特征域使用标准文本到图像扩散流程",
                "实验或效果：在GenEval和DPG-Bench上达到竞争性性能，验证VFM表示能力"
            ],
            "tags_zh": [
                "文本到图像生成",
                "视觉基础模型",
                "扩散模型",
                "表示学习",
                "开源框架"
            ],
            "_index": 21
        },
        {
            "title": "Generative Parametric Design (GPD): A framework for real-time geometry generation and on-the-fly multiparametric approximation",
            "authors": [
                "Mohammed El Fallaki Idrissi",
                "Jad Mounayer",
                "Sebastian Rodriguez",
                "Fodil Meraghni",
                "Francisco Chinesta"
            ],
            "arxiv_id": "2512.11748v1",
            "summary": "This paper presents a novel paradigm in simulation-based engineering sciences by introducing a new framework called Generative Parametric Design (GPD). The GPD framework enables the generation of new designs along with their corresponding parametric solutions given as a reduced basis. To achieve this, two Rank Reduction Autoencoders (RRAEs) are employed, one for encoding and generating the design or geometry, and the other for encoding the sparse Proper Generalized Decomposition (sPGD) mode solutions. These models are linked in the latent space using regression techniques, allowing efficient transitions between design and their associated sPGD modes. By empowering design exploration and optimization, this framework also advances digital and hybrid twin development, enhancing predictive modeling and real-time decision-making in engineering applications. The developed framework is demonstrated on two-phase microstructures, in which the multiparametric solutions account for variations in two key material parameters.",
            "headline_zh": "提出生成参数化设计框架，通过双自编码器实现实时几何生成与多参数近似。",
            "intro_zh": [
                "核心问题：仿真工程中设计生成与参数化解决方案的高效关联问题。",
                "方法要点：使用两个秩降自编码器分别编码几何和稀疏PGD模式，通过潜空间回归链接。",
                "实验或效果：在两相微结构上演示，支持材料参数变化的多参数解。"
            ],
            "tags_zh": [
                "生成参数化设计",
                "秩降自编码器",
                "稀疏PGD",
                "实时几何生成",
                "多参数近似",
                "数字孪生"
            ],
            "_index": 22
        },
        {
            "title": "The Influence of Human-like Appearance on Expected Robot Explanations",
            "authors": [
                "Hana Kopecka",
                "Jose Such"
            ],
            "arxiv_id": "2512.11746v1",
            "summary": "A robot's appearance is a known factor influencing user's mental model and human-robot interaction, that has not been studied in the context of its influence in expected robot explanations. In this study, we investigate whether and to what extent the human-like appearance of robots elicits anthropomorphism, which is conceptualised as an attribution of mental capacities, and how the level of anthropomorphism is revealed in explanations that people expect to receive. We designed a between-subject study comprising conditions with visual stimuli of three domestic service robots with varying human-like appearance, and we prompted respondents to provide explanations they would expect to receive from the robot for the same robot actions. We found that most explanations were anthropomorphic across all conditions. However, there is a positive correlation between the anthropomorphic explanations and human-like appearance. We also report on more nuanced trends observed in non-anthropomorphic explanations and trends in robot descriptions.",
            "headline_zh": "研究机器人外观对人类化解释期望的影响，发现外观越像人，解释越具人类化特征。",
            "intro_zh": [
                "核心问题：机器人外观如何影响用户对其解释的期望，特别是人类化程度。",
                "方法要点：设计组间实验，使用三种不同人类化外观的机器人视觉刺激，收集用户期望的解释。",
                "实验或效果：发现解释普遍人类化，且人类化外观与人类化解释呈正相关，并观察到非人类化解释的细微趋势。"
            ],
            "tags_zh": [
                "机器人外观",
                "人类化解释",
                "人机交互",
                "期望解释",
                "视觉刺激"
            ],
            "_index": 23
        },
        {
            "title": "mViSE: A Visual Search Engine for Analyzing Multiplex IHC Brain Tissue Images",
            "authors": [
                "Liqiang Huang",
                "Rachel W. Mills",
                "Saikiran Mandula",
                "Lin Bai",
                "Mahtab Jeyhani",
                "John Redell",
                "Hien Van Nguyen",
                "Saurabh Prasad",
                "Dragan Maric",
                "Badrinath Roysam"
            ],
            "arxiv_id": "2512.11745v1",
            "summary": "Whole-slide multiplex imaging of brain tissue generates massive information-dense images that are challenging to analyze and require custom software. We present an alternative query-driven programming-free strategy using a multiplex visual search engine (mViSE) that learns the multifaceted brain tissue chemoarchitecture, cytoarchitecture, and myeloarchitecture. Our divide-and-conquer strategy organizes the data into panels of related molecular markers and uses self-supervised learning to train a multiplex encoder for each panel with explicit visual confirmation of successful learning. Multiple panels can be combined to process visual queries for retrieving similar communities of individual cells or multicellular niches using information-theoretic methods. The retrievals can be used for diverse purposes including tissue exploration, delineating brain regions and cortical cell layers, profiling and comparing brain regions without computer programming. We validated mViSE's ability to retrieve single cells, proximal cell pairs, tissue patches, delineate cortical layers, brain regions and sub-regions. mViSE is provided as an open-source QuPath plug-in.",
            "headline_zh": "提出mViSE视觉搜索引擎以分析脑组织多重免疫组化图像，无需编程实现查询驱动分析。",
            "intro_zh": [
                "核心问题：脑组织全切片多重成像产生信息密集图像，分析困难且需定制软件。",
                "方法要点：采用分治策略组织数据，结合自监督学习训练多重编码器，支持信息论方法检索相似细胞群落。",
                "实验或效果：验证了检索单细胞、组织区域及划分皮层和脑区的能力，提供开源QuPath插件。"
            ],
            "tags_zh": [
                "脑组织图像分析",
                "多重免疫组化",
                "视觉搜索引擎",
                "自监督学习",
                "信息论检索",
                "QuPath插件"
            ],
            "_index": 24
        },
        {
            "title": "CogniSNN: Enabling Neuron-Expandability, Pathway-Reusability, and Dynamic-Configurability with Random Graph Architectures in Spiking Neural Networks",
            "authors": [
                "Yongsheng Huang",
                "Peibo Duan",
                "Yujie Wu",
                "Kai Sun",
                "Zhipeng Liu",
                "Changsheng Zhang",
                "Bin Zhang",
                "Mingkun Xu"
            ],
            "arxiv_id": "2512.11743v1",
            "summary": "Spiking neural networks (SNNs), regarded as the third generation of artificial neural networks, are expected to bridge the gap between artificial intelligence and computational neuroscience. However, most mainstream SNN research directly adopts the rigid, chain-like hierarchical architecture of traditional artificial neural networks (ANNs), ignoring key structural characteristics of the brain. Biological neurons are stochastically interconnected, forming complex neural pathways that exhibit Neuron-Expandability, Pathway-Reusability, and Dynamic-Configurability. In this paper, we introduce a new SNN paradigm, named Cognition-aware SNN (CogniSNN), by incorporating Random Graph Architecture (RGA). Furthermore, we address the issues of network degradation and dimensional mismatch in deep pathways by introducing an improved pure spiking residual mechanism alongside an adaptive pooling strategy. Then, we design a Key Pathway-based Learning without Forgetting (KP-LwF) approach, which selectively reuses critical neural pathways while retaining historical knowledge, enabling efficient multi-task transfer. Finally, we propose a Dynamic Growth Learning (DGL) algorithm that allows neurons and synapses to grow dynamically along the internal temporal dimension. Extensive experiments demonstrate that CogniSNN achieves performance comparable to, or even surpassing, current state-of-the-art SNNs on neuromorphic datasets and Tiny-ImageNet. The Pathway-Reusability enhances the network's continuous learning capability across different scenarios, while the dynamic growth algorithm improves robustness against interference and mitigates the fixed-timestep constraints during neuromorphic chip deployment. This work demonstrates the potential of SNNs with random graph structures in advancing brain-inspired intelligence and lays the foundation for their practical application on neuromorphic hardware.",
            "headline_zh": "提出CogniSNN，通过随机图架构实现神经元可扩展性、通路可重用性和动态可配置性，以提升脉冲神经网络性能。",
            "intro_zh": [
                "问题：主流SNN采用链式架构，忽略大脑随机连接特性，导致性能受限。",
                "方法：引入随机图架构、改进残差机制、自适应池化、KP-LwF和DGL算法。",
                "效果：在神经形态数据集和Tiny-ImageNet上达到或超越先进SNN，增强连续学习和鲁棒性。"
            ],
            "tags_zh": [
                "脉冲神经网络",
                "随机图架构",
                "通路可重用性",
                "动态增长学习",
                "神经形态计算"
            ],
            "_index": 25
        },
        {
            "title": "Bench-Push: Benchmarking Pushing-based Navigation and Manipulation Tasks for Mobile Robots",
            "authors": [
                "Ninghan Zhong",
                "Steven Caro",
                "Megnath Ramesh",
                "Rishi Bhatnagar",
                "Avraiem Iskandar",
                "Stephen L. Smith"
            ],
            "arxiv_id": "2512.11736v1",
            "summary": "Mobile robots are increasingly deployed in cluttered environments with movable objects, posing challenges for traditional methods that prohibit interaction. In such settings, the mobile robot must go beyond traditional obstacle avoidance, leveraging pushing or nudging strategies to accomplish its goals. While research in pushing-based robotics is growing, evaluations rely on ad hoc setups, limiting reproducibility and cross-comparison. To address this, we present Bench-Push, the first unified benchmark for pushing-based mobile robot navigation and manipulation tasks. Bench-Push includes multiple components: 1) a comprehensive range of simulated environments that capture the fundamental challenges in pushing-based tasks, including navigating a maze with movable obstacles, autonomous ship navigation in ice-covered waters, box delivery, and area clearing, each with varying levels of complexity; 2) novel evaluation metrics to capture efficiency, interaction effort, and partial task completion; and 3) demonstrations using Bench-Push to evaluate example implementations of established baselines across environments. Bench-Push is open-sourced as a Python library with a modular design. The code, documentation, and trained models can be found at https://github.com/IvanIZ/BenchNPIN.",
            "headline_zh": "提出Bench-Push基准以解决移动机器人在杂乱环境中基于推动的导航与操作任务评估问题",
            "intro_zh": [
                "核心问题：移动机器人在杂乱环境中需推动物体，但现有评估方法依赖临时设置，缺乏可重复性和跨比较性。",
                "方法要点：提供统一基准，包括模拟环境、评估指标和开源Python库，支持推动任务的标准化测试。",
                "实验或效果：通过示例实现评估基线方法，涵盖迷宫导航、冰区航行等任务，并开源代码和模型。"
            ],
            "tags_zh": [
                "移动机器人",
                "推动操作",
                "基准测试",
                "导航任务",
                "模拟环境",
                "开源库"
            ],
            "_index": 26
        },
        {
            "title": "ECCO: Leveraging Cross-Camera Correlations for Efficient Live Video Continuous Learning",
            "authors": [
                "Yuze He",
                "Ferdi Kossmann",
                "Srinivasan Seshan",
                "Peter Steenkiste"
            ],
            "arxiv_id": "2512.11727v1",
            "summary": "Recent advances in video analytics address real-time data drift by continuously retraining specialized, lightweight DNN models for individual cameras. However, the current practice of retraining a separate model for each camera suffers from high compute and communication costs, making it unscalable. We present ECCO, a new video analytics framework designed for resource-efficient continuous learning. The key insight is that the data drift, which necessitates model retraining, often shows temporal and spatial correlations across nearby cameras. By identifying cameras that experience similar drift and retraining a shared model for them, ECCO can substantially reduce the associated compute and communication costs. Specifically, ECCO introduces: (i) a lightweight grouping algorithm that dynamically forms and updates camera groups; (ii) a GPU allocator that dynamically assigns GPU resources across different groups to improve retraining accuracy and ensure fairness; and (iii) a transmission controller at each camera that configures frame sampling and coordinates bandwidth sharing with other cameras based on its assigned GPU resources. We conducted extensive evaluations on three distinctive datasets for two vision tasks. Compared to leading baselines, ECCO improves retraining accuracy by 6.7%-18.1% using the same compute and communication resources, or supports 3.3 times more concurrent cameras at the same accuracy.",
            "headline_zh": "提出ECCO框架，利用跨摄像头相关性实现高效视频连续学习",
            "intro_zh": [
                "核心问题：单摄像头独立重训练模型导致计算和通信成本高，难以扩展",
                "方法要点：动态分组摄像头共享模型，优化GPU分配和传输控制",
                "实验或效果：在相同资源下提升重训练精度6.7%-18.1%，或支持3.3倍并发摄像头"
            ],
            "tags_zh": [
                "视频分析",
                "连续学习",
                "摄像头分组",
                "资源优化",
                "动态重训练"
            ],
            "_index": 27
        },
        {
            "title": "From Signal to Turn: Interactional Friction in Modular Speech-to-Speech Pipelines",
            "authors": [
                "Titaya Mairittha",
                "Tanakon Sawanglok",
                "Panuwit Raden",
                "Jirapast Buntub",
                "Thanapat Warunee",
                "Napat Asawachaisuvikrom",
                "Thanaphum Saiwongin"
            ],
            "arxiv_id": "2512.11724v1",
            "summary": "While voice-based AI systems have achieved remarkable generative capabilities, their interactions often feel conversationally broken. This paper examines the interactional friction that emerges in modular Speech-to-Speech Retrieval-Augmented Generation (S2S-RAG) pipelines. By analyzing a representative production system, we move beyond simple latency metrics to identify three recurring patterns of conversational breakdown: (1) Temporal Misalignment, where system delays violate user expectations of conversational rhythm; (2) Expressive Flattening, where the loss of paralinguistic cues leads to literal, inappropriate responses; and (3) Repair Rigidity, where architectural gating prevents users from correcting errors in real-time. Through system-level analysis, we demonstrate that these friction points should not be understood as defects or failures, but as structural consequences of a modular design that prioritizes control over fluidity. We conclude that building natural spoken AI is an infrastructure design challenge, requiring a shift from optimizing isolated components to carefully choreographing the seams between them.",
            "headline_zh": "分析模块化语音到语音检索增强生成管道中的交互摩擦，揭示其结构性根源。",
            "intro_zh": [
                "核心问题：模块化语音AI系统在交互中产生对话断裂，表现为时间错位、表达扁平化和修复僵化。",
                "方法要点：通过系统级分析，识别三种常见对话断裂模式，强调摩擦是模块化设计优先控制而非流畅性的结果。",
                "实验或效果：基于代表性生产系统分析，提出构建自然语音AI需从优化组件转向协调模块间衔接。"
            ],
            "tags_zh": [
                "语音到语音检索增强生成",
                "交互摩擦",
                "模块化系统设计",
                "对话断裂分析",
                "系统级分析"
            ],
            "_index": 28
        },
        {
            "title": "Weak-to-Strong Generalization Enables Fully Automated De Novo Training of Multi-head Mask-RCNN Model for Segmenting Densely Overlapping Cell Nuclei in Multiplex Whole-slice Brain Images",
            "authors": [
                "Lin Bai",
                "Xiaoyang Li",
                "Liqiang Huang",
                "Quynh Nguyen",
                "Hien Van Nguyen",
                "Saurabh Prasad",
                "Dragan Maric",
                "John Redell",
                "Pramod Dash",
                "Badrinath Roysam"
            ],
            "arxiv_id": "2512.11722v1",
            "summary": "We present a weak to strong generalization methodology for fully automated training of a multi-head extension of the Mask-RCNN method with efficient channel attention for reliable segmentation of overlapping cell nuclei in multiplex cyclic immunofluorescent (IF) whole-slide images (WSI), and present evidence for pseudo-label correction and coverage expansion, the key phenomena underlying weak to strong generalization. This method can learn to segment de novo a new class of images from a new instrument and/or a new imaging protocol without the need for human annotations. We also present metrics for automated self-diagnosis of segmentation quality in production environments, where human visual proofreading of massive WSI images is unaffordable. Our method was benchmarked against five current widely used methods and showed a significant improvement. The code, sample WSI images, and high-resolution segmentation results are provided in open form for community adoption and adaptation.",
            "headline_zh": "提出弱到强泛化方法，实现全自动训练多头Mask-RCNN，用于分割多重全切片脑图像中密集重叠细胞核。",
            "intro_zh": [
                "核心问题：多重全切片脑图像中密集重叠细胞核的自动化分割，无需人工标注。",
                "方法要点：基于弱到强泛化，结合多头Mask-RCNN与高效通道注意力，支持伪标签校正和覆盖扩展。",
                "实验或效果：在基准测试中优于五种现有方法，提供代码和样本供社区使用。"
            ],
            "tags_zh": [
                "细胞核分割",
                "弱到强泛化",
                "多头Mask-RCNN",
                "全切片图像",
                "自动化训练",
                "伪标签校正"
            ],
            "_index": 29
        },
        {
            "title": "Reframing Music-Driven 2D Dance Pose Generation as Multi-Channel Image Generation",
            "authors": [
                "Yan Zhang",
                "Han Zou",
                "Lincong Feng",
                "Cong Xie",
                "Ruiqi Yu",
                "Zhenpeng Zhan"
            ],
            "arxiv_id": "2512.11720v1",
            "summary": "Recent pose-to-video models can translate 2D pose sequences into photorealistic, identity-preserving dance videos, so the key challenge is to generate temporally coherent, rhythm-aligned 2D poses from music, especially under complex, high-variance in-the-wild distributions. We address this by reframing music-to-dance generation as a music-token-conditioned multi-channel image synthesis problem: 2D pose sequences are encoded as one-hot images, compressed by a pretrained image VAE, and modeled with a DiT-style backbone, allowing us to inherit architectural and training advances from modern text-to-image models and better capture high-variance 2D pose distributions. On top of this formulation, we introduce (i) a time-shared temporal indexing scheme that explicitly synchronizes music tokens and pose latents over time and (ii) a reference-pose conditioning strategy that preserves subject-specific body proportions and on-screen scale while enabling long-horizon segment-and-stitch generation. Experiments on a large in-the-wild 2D dance corpus and the calibrated AIST++2D benchmark show consistent improvements over representative music-to-dance methods in pose- and video-space metrics and human preference, and ablations validate the contributions of the representation, temporal indexing, and reference conditioning. See supplementary videos at https://hot-dance.github.io",
            "headline_zh": "提出基于多通道图像生成的音乐驱动2D舞蹈姿态生成方法，以解决复杂分布下的时序一致性和节奏对齐问题。",
            "intro_zh": [
                "核心问题：从音乐生成时序一致、节奏对齐的2D舞蹈姿态，尤其在复杂高方差分布下。",
                "方法要点：将姿态序列编码为独热图像，使用预训练VAE压缩和DiT风格骨干建模，引入时间共享索引和参考姿态条件。",
                "实验效果：在野外舞蹈数据集和AIST++2D基准上，姿态和视频指标及人类偏好优于现有方法。"
            ],
            "tags_zh": [
                "音乐驱动舞蹈生成",
                "多通道图像生成",
                "时序一致性",
                "姿态序列编码",
                "参考姿态条件",
                "野外数据集"
            ],
            "_index": 30
        },
        {
            "title": "Referring Change Detection in Remote Sensing Imagery",
            "authors": [
                "Yilmaz Korkmaz",
                "Jay N. Paranjape",
                "Celso M. de Melo",
                "Vishal M. Patel"
            ],
            "arxiv_id": "2512.11719v1",
            "summary": "Change detection in remote sensing imagery is essential for applications such as urban planning, environmental monitoring, and disaster management. Traditional change detection methods typically identify all changes between two temporal images without distinguishing the types of transitions, which can lead to results that may not align with specific user needs. Although semantic change detection methods have attempted to address this by categorizing changes into predefined classes, these methods rely on rigid class definitions and fixed model architectures, making it difficult to mix datasets with different label sets or reuse models across tasks, as the output channels are tightly coupled with the number and type of semantic classes. To overcome these limitations, we introduce Referring Change Detection (RCD), which leverages natural language prompts to detect specific classes of changes in remote sensing images. By integrating language understanding with visual analysis, our approach allows users to specify the exact type of change they are interested in. However, training models for RCD is challenging due to the limited availability of annotated data and severe class imbalance in existing datasets. To address this, we propose a two-stage framework consisting of (I) \\textbf{RCDNet}, a cross-modal fusion network designed for referring change detection, and (II) \\textbf{RCDGen}, a diffusion-based synthetic data generation pipeline that produces realistic post-change images and change maps for a specified category using only pre-change image, without relying on semantic segmentation masks and thereby significantly lowering the barrier to scalable data creation. Experiments across multiple datasets show that our framework enables scalable and targeted change detection. Project website is here: https://yilmazkorkmaz1.github.io/RCD.",
            "headline_zh": "提出基于自然语言提示的遥感图像指代变化检测框架，以解决传统方法无法针对特定变化类型的问题。",
            "intro_zh": [
                "传统遥感变化检测方法识别所有变化，不区分类型，难以满足用户特定需求。",
                "引入指代变化检测，通过自然语言提示指定变化类别，结合跨模态融合网络和扩散模型生成数据。",
                "在多个数据集上实验，框架支持可扩展和针对性的变化检测，降低数据创建门槛。"
            ],
            "tags_zh": [
                "遥感图像变化检测",
                "指代变化检测",
                "自然语言提示",
                "跨模态融合",
                "扩散模型数据生成"
            ],
            "_index": 31
        },
        {
            "title": "EditMGT: Unleashing Potentials of Masked Generative Transformers in Image Editing",
            "authors": [
                "Wei Chow",
                "Linfeng Li",
                "Lingdong Kong",
                "Zefeng Li",
                "Qi Xu",
                "Hang Song",
                "Tian Ye",
                "Xian Wang",
                "Jinbin Bai",
                "Shilin Xu",
                "Xiangtai Li",
                "Junting Pan",
                "Shaoteng Liu",
                "Ran Zhou",
                "Tianshu Yang",
                "Songhua Liu"
            ],
            "arxiv_id": "2512.11715v1",
            "summary": "Recent advances in diffusion models (DMs) have achieved exceptional visual quality in image editing tasks. However, the global denoising dynamics of DMs inherently conflate local editing targets with the full-image context, leading to unintended modifications in non-target regions. In this paper, we shift our attention beyond DMs and turn to Masked Generative Transformers (MGTs) as an alternative approach to tackle this challenge. By predicting multiple masked tokens rather than holistic refinement, MGTs exhibit a localized decoding paradigm that endows them with the inherent capacity to explicitly preserve non-relevant regions during the editing process. Building upon this insight, we introduce the first MGT-based image editing framework, termed EditMGT. We first demonstrate that MGT's cross-attention maps provide informative localization signals for localizing edit-relevant regions and devise a multi-layer attention consolidation scheme that refines these maps to achieve fine-grained and precise localization. On top of these adaptive localization results, we introduce region-hold sampling, which restricts token flipping within low-attention areas to suppress spurious edits, thereby confining modifications to the intended target regions and preserving the integrity of surrounding non-target areas. To train EditMGT, we construct CrispEdit-2M, a high-resolution dataset spanning seven diverse editing categories. Without introducing additional parameters, we adapt a pre-trained text-to-image MGT into an image editing model through attention injection. Extensive experiments across four standard benchmarks demonstrate that, with fewer than 1B parameters, our model achieves similarity performance while enabling 6 times faster editing. Moreover, it delivers comparable or superior editing quality, with improvements of 3.6% and 17.6% on style change and style transfer tasks, respectively.",
            "headline_zh": "提出EditMGT框架，利用掩码生成Transformer解决图像编辑中非目标区域意外修改问题。",
            "intro_zh": [
                "扩散模型全局去噪导致非目标区域意外修改，转向掩码生成Transformer的局部解码范式。",
                "基于交叉注意力图精确定位编辑区域，引入区域保持采样限制修改范围。",
                "在四个基准测试中，模型参数少于10亿，实现相似性能且编辑速度提升6倍。"
            ],
            "tags_zh": [
                "图像编辑",
                "掩码生成Transformer",
                "局部解码",
                "注意力机制",
                "区域保持采样",
                "高分辨率数据集"
            ],
            "_index": 32
        },
        {
            "title": "Two-dimensional Decompositions of High-dimensional Configurations for Efficient Multi-vehicle Coordination at Intelligent Intersections",
            "authors": [
                "Amirreza Akbari",
                "Johan Thunberg"
            ],
            "arxiv_id": "2512.11713v1",
            "summary": "For multi-vehicle complex traffic scenarios in shared spaces such as intelligent intersections, safe coordination and trajectory planning is challenging due to computational complexity. To meet this challenge, we introduce a computationally efficient method for generating collision-free trajectories along predefined vehicle paths. We reformulate a constrained minimum-time trajectory planning problem as a problem in a high-dimensional configuration space, where conflict zones are modeled by high-dimensional polyhedra constructed from two-dimensional rectangles. Still, in such a formulation, as the number of vehicles involved increases, the computational complexity increases significantly. To address this, we propose two algorithms for near-optimal local optimization that significantly reduce the computational complexity by decomposing the high-dimensional problem into a sequence of 2D graph search problems. The resulting trajectories are then incorporated into a Nonlinear Model Predictive Control (NMPC) framework to ensure safe and smooth vehicle motion. We furthermore show in numerical evaluation that this approach significantly outperforms existing MILP-based time-scheduling; both in terms of objective-value and computational time.",
            "headline_zh": "提出基于二维分解的高维配置空间方法，以高效解决智能交叉口多车辆协调问题",
            "intro_zh": [
                "核心问题：多车辆在智能交叉口等共享空间中的安全协调与轨迹规划计算复杂度高",
                "方法要点：将高维配置空间问题分解为序列二维图搜索，结合非线性模型预测控制确保平滑运动",
                "实验或效果：数值评估显示在目标值和计算时间上显著优于现有基于混合整数线性规划的方法"
            ],
            "tags_zh": [
                "多车辆协调",
                "轨迹规划",
                "配置空间分解",
                "非线性模型预测控制",
                "智能交叉口"
            ],
            "_index": 33
        },
        {
            "title": "High-Dimensional Surrogate Modeling for Closed-Loop Learning of Neural-Network-Parameterized Model Predictive Control",
            "authors": [
                "Sebastian Hirt",
                "Valentinus Suwanto",
                "Hendrik Alsmeier",
                "Maik Pfefferkorn",
                "Rolf Findeisen"
            ],
            "arxiv_id": "2512.11705v1",
            "summary": "Learning controller parameters from closed-loop data has been shown to improve closed-loop performance. Bayesian optimization, a widely used black-box and sample-efficient learning method, constructs a probabilistic surrogate of the closed-loop performance from few experiments and uses it to select informative controller parameters. However, it typically struggles with dense high-dimensional controller parameterizations, as they may appear, for example, in tuning model predictive controllers, because standard surrogate models fail to capture the structure of such spaces. This work suggests that the use of Bayesian neural networks as surrogate models may help to mitigate this limitation. Through a comparison between Gaussian processes with Matern kernels, finite-width Bayesian neural networks, and infinite-width Bayesian neural networks on a cart-pole task, we find that Bayesian neural network surrogate models achieve faster and more reliable convergence of the closed-loop cost and enable successful optimization of parameterizations with hundreds of dimensions. Infinite-width Bayesian neural networks also maintain performance in settings with more than one thousand parameters, whereas Matern-kernel Gaussian processes rapidly lose effectiveness. These results indicate that Bayesian neural network surrogate models may be suitable for learning dense high-dimensional controller parameterizations and offer practical guidance for selecting surrogate models in learning-based controller design.",
            "headline_zh": "提出贝叶斯神经网络代理模型以解决高维控制器参数学习中的收敛问题",
            "intro_zh": [
                "核心问题：贝叶斯优化在密集高维控制器参数化中因标准代理模型失效而收敛困难",
                "方法要点：使用贝叶斯神经网络作为代理模型，包括有限宽和无限宽变体",
                "实验或效果：在cart-pole任务中，贝叶斯神经网络实现更快收敛，支持超千维参数优化"
            ],
            "tags_zh": [
                "贝叶斯优化",
                "代理建模",
                "模型预测控制",
                "高维参数学习",
                "贝叶斯神经网络"
            ],
            "_index": 34
        },
        {
            "title": "Particle Image Velocimetry Refinement via Consensus ADMM",
            "authors": [
                "Alan Bonomi",
                "Francesco Banelli",
                "Antonio Terpin"
            ],
            "arxiv_id": "2512.11695v1",
            "summary": "Particle Image Velocimetry (PIV) is an imaging technique in experimental fluid dynamics that quantifies flow fields around bluff bodies by analyzing the displacement of neutrally buoyant tracer particles immersed in the fluid. Traditional PIV approaches typically depend on tuning parameters specific to the imaging setup, making the performance sensitive to variations in illumination, flow conditions, and seeding density. On the other hand, even state-of-the-art machine learning methods for flow quantification are fragile outside their training set. In our experiments, we observed that flow quantification would improve if different tunings (or algorithms) were applied to different regions of the same image pair. In this work, we parallelize the instantaneous flow quantification with multiple algorithms and adopt a consensus framework based on the alternating direction method of multipliers, seamlessly incorporating priors such as smoothness and incompressibility. We perform several numerical experiments to demonstrate the benefits of this approach. For instance, we achieve a decrease in end-point-error of up to 20% of a dense-inverse-search estimator at an inference rate of 60Hz, and we show how this performance boost can be increased further with outlier rejection. Our method is implemented in JAX, effectively exploiting hardware acceleration, and integrated in Flow Gym, enabling (i) reproducible comparisons with the state-of-the-art, (ii) testing different base algorithms, (iii) straightforward deployment for active fluids control applications.",
            "headline_zh": "提出基于共识ADMM的粒子图像测速优化方法，提升流体场量化精度与鲁棒性。",
            "intro_zh": [
                "传统PIV方法依赖调参，性能易受成像条件影响，机器学习方法泛化性差。",
                "采用多算法并行量化流场，结合ADMM共识框架融入平滑性和不可压缩性先验。",
                "实验显示端点误差降低达20%，推理速率60Hz，集成于Flow Gym支持可复现比较。"
            ],
            "tags_zh": [
                "粒子图像测速",
                "交替方向乘子法",
                "流体场量化",
                "共识优化",
                "硬件加速",
                "主动流体控制"
            ],
            "_index": 35
        },
        {
            "title": "Text images processing system using artificial intelligence models",
            "authors": [
                "Aya Kaysan Bahjat"
            ],
            "arxiv_id": "2512.11691v1",
            "summary": "This is to present a text image classifier device that identifies textual content in images and then categorizes each image into one of four predefined categories, including Invoice, Form, Letter, or Report. The device supports a gallery mode, in which users browse files on flash disks, hard disk drives, or microSD cards, and a live mode which renders feeds of cameras connected to it. Its design is specifically aimed at addressing pragmatic challenges, such as changing light, random orientation, curvature or partial coverage of text, low resolution, and slightly visible text. The steps of the processing process are divided into four steps: image acquisition and preprocessing, textual elements detection with the help of DBNet++ (Differentiable Binarization Network Plus) model, BART (Bidirectional Auto-Regressive Transformers) model that classifies detected textual elements, and the presentation of the results through a user interface written in Python and PyQt5. All the stages are connected in such a way that they form a smooth workflow. The system achieved a text recognition rate of about 94.62% when tested over ten hours on the mentioned Total-Text dataset, that includes high resolution images, created so as to represent a wide range of problematic conditions. These experimental results support the effectiveness of the suggested methodology to practice, mixed-source text categorization, even in uncontrolled imaging conditions.",
            "headline_zh": "提出基于DBNet++和BART的文本图像处理系统，用于在复杂条件下分类Invoice、Form、Letter或Report。",
            "intro_zh": [
                "核心问题：解决图像中文本识别与分类的挑战，如光照变化、低分辨率、文本部分覆盖等。",
                "方法要点：采用DBNet++检测文本元素，BART模型进行分类，集成Python/PyQt5用户界面。",
                "实验或效果：在Total-Text数据集上测试10小时，文本识别率约94.62%，验证了方法的有效性。"
            ],
            "tags_zh": [
                "文本图像分类",
                "DBNet++",
                "BART模型",
                "复杂条件处理",
                "用户界面集成"
            ],
            "_index": 36
        },
        {
            "title": "Stable spectral neural operator for learning stiff PDE systems from limited data",
            "authors": [
                "Rui Zhang",
                "Han Wan",
                "Yang Liu",
                "Hao Sun"
            ],
            "arxiv_id": "2512.11686v1",
            "summary": "Accurate modeling of spatiotemporal dynamics is crucial to understanding complex phenomena across science and engineering. However, this task faces a fundamental challenge when the governing equations are unknown and observational data are sparse. System stiffness, the coupling of multiple time-scales, further exacerbates this problem and hinders long-term prediction. Existing methods fall short: purely data-driven methods demand massive datasets, whereas physics-aware approaches are constrained by their reliance on known equations and fine-grained time steps. To overcome these limitations, we introduce an equation-free learning framework, namely, the Stable Spectral Neural Operator (SSNO), for modeling stiff partial differential equation (PDE) systems based on limited data. Instead of encoding specific equation terms, SSNO embeds spectrally inspired structures in its architecture, yielding strong inductive biases for learning the underlying physics. It automatically learns local and global spatial interactions in the frequency domain, while handling system stiffness with a robust integrating factor time-stepping scheme. Demonstrated across multiple 2D and 3D benchmarks in Cartesian and spherical geometries, SSNO achieves prediction errors one to two orders of magnitude lower than leading models. Crucially, it shows remarkable data efficiency, requiring only very few (2--5) training trajectories for robust generalization to out-of-distribution conditions. This work offers a robust and generalizable approach to learning stiff spatiotemporal dynamics from limited data without explicit \\textit{a priori} knowledge of PDE terms.",
            "headline_zh": "提出稳定谱神经算子以从有限数据学习刚性偏微分方程系统",
            "intro_zh": [
                "核心问题：未知方程和稀疏数据下，系统刚性阻碍时空动力学建模。",
                "方法要点：嵌入谱结构，在频域学习空间交互，采用积分因子时间步进处理刚性。",
                "实验或效果：在2D/3D基准测试中，预测误差降低1-2个数量级，仅需2-5条轨迹实现泛化。"
            ],
            "tags_zh": [
                "谱神经算子",
                "刚性偏微分方程",
                "有限数据学习",
                "时空动力学建模",
                "积分因子时间步进"
            ],
            "_index": 37
        },
        {
            "title": "Depth-Copy-Paste: Multimodal and Depth-Aware Compositing for Robust Face Detection",
            "authors": [
                "Qiushi Guo"
            ],
            "arxiv_id": "2512.11683v1",
            "summary": "Data augmentation is crucial for improving the robustness of face detection systems, especially under challenging conditions such as occlusion, illumination variation, and complex environments. Traditional copy paste augmentation often produces unrealistic composites due to inaccurate foreground extraction, inconsistent scene geometry, and mismatched background semantics. To address these limitations, we propose Depth Copy Paste, a multimodal and depth aware augmentation framework that generates diverse and physically consistent face detection training samples by copying full body person instances and pasting them into semantically compatible scenes. Our approach first employs BLIP and CLIP to jointly assess semantic and visual coherence, enabling automatic retrieval of the most suitable background images for the given foreground person. To ensure high quality foreground masks that preserve facial details, we integrate SAM3 for precise segmentation and Depth-Anything to extract only the non occluded visible person regions, preventing corrupted facial textures from being used in augmentation. For geometric realism, we introduce a depth guided sliding window placement mechanism that searches over the background depth map to identify paste locations with optimal depth continuity and scale alignment. The resulting composites exhibit natural depth relationships and improved visual plausibility. Extensive experiments show that Depth Copy Paste provides more diverse and realistic training data, leading to significant performance improvements in downstream face detection tasks compared with traditional copy paste and depth free augmentation methods.",
            "headline_zh": "提出Depth-Copy-Paste以解决传统复制粘贴增强在面部检测中不真实的问题",
            "intro_zh": [
                "传统复制粘贴增强因前景提取不准和几何不一致导致不真实合成",
                "方法结合BLIP、CLIP、SAM3和Depth-Anything实现语义兼容和深度感知的增强",
                "实验显示该方法提升面部检测性能，优于传统方法"
            ],
            "tags_zh": [
                "数据增强",
                "面部检测",
                "深度感知",
                "语义兼容",
                "复制粘贴"
            ],
            "_index": 38
        },
        {
            "title": "MedAI: Evaluating TxAgent's Therapeutic Agentic Reasoning in the NeurIPS CURE-Bench Competition",
            "authors": [
                "Tim Cofala",
                "Christian Kalfar",
                "Jingge Xiao",
                "Johanna Schrader",
                "Michelle Tang",
                "Wolfgang Nejdl"
            ],
            "arxiv_id": "2512.11682v1",
            "summary": "Therapeutic decision-making in clinical medicine constitutes a high-stakes domain in which AI guidance interacts with complex interactions among patient characteristics, disease processes, and pharmacological agents. Tasks such as drug recommendation, treatment planning, and adverse-effect prediction demand robust, multi-step reasoning grounded in reliable biomedical knowledge. Agentic AI methods, exemplified by TxAgent, address these challenges through iterative retrieval-augmented generation (RAG). TxAgent employs a fine-tuned Llama-3.1-8B model that dynamically generates and executes function calls to a unified biomedical tool suite (ToolUniverse), integrating FDA Drug API, OpenTargets, and Monarch resources to ensure access to current therapeutic information. In contrast to general-purpose RAG systems, medical applications impose stringent safety constraints, rendering the accuracy of both the reasoning trace and the sequence of tool invocations critical. These considerations motivate evaluation protocols treating token-level reasoning and tool-usage behaviors as explicit supervision signals. This work presents insights derived from our participation in the CURE-Bench NeurIPS 2025 Challenge, which benchmarks therapeutic-reasoning systems using metrics that assess correctness, tool utilization, and reasoning quality. We analyze how retrieval quality for function (tool) calls influences overall model performance and demonstrate performance gains achieved through improved tool-retrieval strategies. Our work was awarded the Excellence Award in Open Science. Complete information can be found at https://curebench.ai/.",
            "headline_zh": "提出TxAgent的代理推理方法，在CURE-Bench竞赛中评估医疗决策性能",
            "intro_zh": [
                "医疗AI需处理患者、疾病和药物的复杂交互，要求多步推理和可靠知识",
                "TxAgent基于微调Llama-3.1-8B模型，通过迭代RAG动态调用生物医学工具",
                "在CURE-Bench竞赛中分析工具检索质量对性能的影响，获得Open Science Excellence奖"
            ],
            "tags_zh": [
                "医疗决策AI",
                "代理推理",
                "检索增强生成",
                "生物医学工具集成",
                "CURE-Bench竞赛"
            ],
            "_index": 39
        },
        {
            "title": "Cross-modal Context-aware Learning for Visual Prompt Guided Multimodal Image Understanding in Remote Sensing",
            "authors": [
                "Xu Zhang",
                "Jiabin Fang",
                "Zhuoming Ding",
                "Jin Yuan",
                "Xuan Liu",
                "Qianjun Zhang",
                "Zhiyong Li"
            ],
            "arxiv_id": "2512.11680v1",
            "summary": "Recent advances in image understanding have enabled methods that leverage large language models for multimodal reasoning in remote sensing. However, existing approaches still struggle to steer models to the user-relevant regions when only simple, generic text prompts are available. Moreover, in large-scale aerial imagery many objects exhibit highly similar visual appearances and carry rich inter-object relationships, which further complicates accurate recognition. To address these challenges, we propose Cross-modal Context-aware Learning for Visual Prompt-Guided Multimodal Image Understanding (CLV-Net). CLV-Net lets users supply a simple visual cue, a bounding box, to indicate a region of interest, and uses that cue to guide the model to generate correlated segmentation masks and captions that faithfully reflect user intent. Central to our design is a Context-Aware Mask Decoder that models and integrates inter-object relationships to strengthen target representations and improve mask quality. In addition, we introduce a Semantic and Relationship Alignment module: a Cross-modal Semantic Consistency Loss enhances fine-grained discrimination among visually similar targets, while a Relationship Consistency Loss enforces alignment between textual relations and visual interactions. Comprehensive experiments on two benchmark datasets show that CLV-Net outperforms existing methods and establishes new state-of-the-art results. The model effectively captures user intent and produces precise, intention-aligned multimodal outputs.",
            "headline_zh": "提出CLV-Net以解决遥感图像中视觉提示引导的多模态理解问题",
            "intro_zh": [
                "现有方法在简单文本提示下难以引导模型关注用户相关区域",
                "CLV-Net通过视觉提示和上下文感知解码器增强目标表示与掩码质量",
                "在基准数据集上超越现有方法，实现用户意图对齐的多模态输出"
            ],
            "tags_zh": [
                "遥感图像理解",
                "视觉提示引导",
                "上下文感知学习",
                "多模态对齐",
                "目标分割",
                "关系建模"
            ],
            "_index": 40
        },
        {
            "title": "Stochastics of shapes and Kunita flows",
            "authors": [
                "Stefan Sommer",
                "Gefan Yang",
                "Elizabeth Louise Baker"
            ],
            "arxiv_id": "2512.11676v1",
            "summary": "Stochastic processes of evolving shapes are used in applications including evolutionary biology, where morphology changes stochastically as a function of evolutionary processes. Due to the non-linear and often infinite-dimensional nature of shape spaces, the mathematical construction of suitable stochastic shape processes is far from immediate. We define and formalize properties that stochastic shape processes should ideally satisfy to be compatible with the shape structure, and we link this to Kunita flows that, when acting on shape spaces, induce stochastic processes that satisfy these criteria by their construction. We couple this with a survey of other relevant shape stochastic processes and show how bridge sampling techniques can be used to condition shape stochastic processes on observed data thereby allowing for statistical inference of parameters of the stochastic dynamics.",
            "headline_zh": "提出基于Kunita流的随机形状过程，以支持进化生物学等领域的统计推断",
            "intro_zh": [
                "核心问题：形状空间非线性且无限维，构建兼容结构的随机形状过程困难",
                "方法要点：定义理想属性，链接Kunita流以自动满足这些标准",
                "实验或效果：结合桥采样技术，基于观测数据条件化过程，实现参数推断"
            ],
            "tags_zh": [
                "随机形状过程",
                "Kunita流",
                "形状空间",
                "统计推断",
                "桥采样"
            ],
            "_index": 41
        },
        {
            "title": "Bridging Streaming Continual Learning via In-Context Large Tabular Models",
            "authors": [
                "Afonso Lourenço",
                "João Gama",
                "Eric P. Xing",
                "Goreti Marreiros"
            ],
            "arxiv_id": "2512.11668v1",
            "summary": "In streaming scenarios, models must learn continuously, adapting to concept drifts without erasing previously acquired knowledge. However, existing research communities address these challenges in isolation. Continual Learning (CL) focuses on long-term retention and mitigating catastrophic forgetting, often without strict real-time constraints. Stream Learning (SL) emphasizes rapid, efficient adaptation to high-frequency data streams, but typically neglects forgetting. Recent efforts have tried to combine these paradigms, yet no clear algorithmic overlap exists. We argue that large in-context tabular models (LTMs) provide a natural bridge for Streaming Continual Learning (SCL). In our view, unbounded streams should be summarized on-the-fly into compact sketches that can be consumed by LTMs. This recovers the classical SL motivation of compressing massive streams with fixed-size guarantees, while simultaneously aligning with the experience-replay desiderata of CL. To clarify this bridge, we show how the SL and CL communities implicitly adopt a divide-to-conquer strategy to manage the tension between plasticity (performing well on the current distribution) and stability (retaining past knowledge), while also imposing a minimal complexity constraint that motivates diversification (avoiding redundancy in what is stored) and retrieval (re-prioritizing past information when needed). Within this perspective, we propose structuring SCL with LTMs around two core principles of data selection for in-context learning: (1) distribution matching, which balances plasticity and stability, and (2) distribution compression, which controls memory size through diversification and retrieval mechanisms.",
            "headline_zh": "提出基于大上下文表格模型的流式持续学习框架，以桥接流学习与持续学习",
            "intro_zh": [
                "核心问题：流式场景中模型需连续学习，但现有研究孤立处理流学习与持续学习，缺乏算法重叠",
                "方法要点：利用大上下文表格模型，将无界流数据实时压缩为紧凑摘要，平衡可塑性与稳定性",
                "实验或效果：通过分布匹配和压缩原则，实现数据选择，控制内存大小并避免冗余"
            ],
            "tags_zh": [
                "流式持续学习",
                "大上下文表格模型",
                "概念漂移",
                "灾难性遗忘",
                "数据压缩",
                "经验回放"
            ],
            "_index": 42
        },
        {
            "title": "From Verification Burden to Trusted Collaboration: Design Goals for LLM-Assisted Literature Reviews",
            "authors": [
                "Brenda Nogueira",
                "Werner Geyer",
                "Andrew Anderson",
                "Toby Jia-Jun Li",
                "Dongwhi Kim",
                "Nuno Moniz",
                "Nitesh V. Chawla"
            ],
            "arxiv_id": "2512.11661v1",
            "summary": "Large Language Models (LLMs) are increasingly embedded in academic writing practices. Although numerous studies have explored how researchers employ these tools for scientific writing, their concrete implementation, limitations, and design challenges within the literature review process remain underexplored. In this paper, we report a user study with researchers across multiple disciplines to characterize current practices, benefits, and \\textit{pain points} in using LLMs to investigate related work. We identified three recurring gaps: (i) lack of trust in outputs, (ii) persistent verification burden, and (iii) requiring multiple tools. This motivates our proposal of six design goals and a high-level framework that operationalizes them through improved related papers visualization, verification at every step, and human-feedback alignment with generation-guided explanations. Overall, by grounding our work in the practical, day-to-day needs of researchers, we designed a framework that addresses these limitations and models real-world LLM-assisted writing, advancing trust through verifiable actions and fostering practical collaboration between researchers and AI systems.",
            "headline_zh": "提出六项设计目标与框架，以减轻文献综述中LLM辅助的验证负担并增强信任",
            "intro_zh": [
                "核心问题：LLM在文献综述中缺乏信任、验证负担重、需多工具协作",
                "方法要点：通过用户研究识别痛点，提出可视化、逐步验证、人机反馈对齐的设计目标",
                "实验或效果：基于研究者日常需求设计框架，促进可验证行动与AI系统协作"
            ],
            "tags_zh": [
                "文献综述",
                "大型语言模型",
                "用户研究",
                "设计目标",
                "信任增强",
                "人机协作"
            ],
            "_index": 43
        },
        {
            "title": "Kinetic Mining in Context: Few-Shot Action Synthesis via Text-to-Motion Distillation",
            "authors": [
                "Luca Cazzola",
                "Ahed Alboody"
            ],
            "arxiv_id": "2512.11654v1",
            "summary": "The acquisition cost for large, annotated motion datasets remains a critical bottleneck for skeletal-based Human Activity Recognition (HAR). Although Text-to-Motion (T2M) generative models offer a compelling, scalable source of synthetic data, their training objectives, which emphasize general artistic motion, and dataset structures fundamentally differ from HAR's requirements for kinematically precise, class-discriminative actions. This disparity creates a significant domain gap, making generalist T2M models ill-equipped for generating motions suitable for HAR classifiers. To address this challenge, we propose KineMIC (Kinetic Mining In Context), a transfer learning framework for few-shot action synthesis. KineMIC adapts a T2M diffusion model to an HAR domain by hypothesizing that semantic correspondences in the text encoding space can provide soft supervision for kinematic distillation. We operationalize this via a kinetic mining strategy that leverages CLIP text embeddings to establish correspondences between sparse HAR labels and T2M source data. This process guides fine-tuning, transforming the generalist T2M backbone into a specialized few-shot Action-to-Motion generator. We validate KineMIC using HumanML3D as the source T2M dataset and a subset of NTU RGB+D 120 as the target HAR domain, randomly selecting just 10 samples per action class. Our approach generates significantly more coherent motions, providing a robust data augmentation source that delivers a +23.1% accuracy points improvement. Animated illustrations and supplementary materials are available at (https://lucazzola.github.io/publications/kinemic).",
            "headline_zh": "提出KineMIC框架，通过文本到运动蒸馏解决少样本动作合成问题。",
            "intro_zh": [
                "核心问题：通用文本到运动模型生成的动作不适合骨骼动作识别，存在领域差距。",
                "方法要点：利用CLIP文本嵌入建立语义对应，指导扩散模型微调为动作到运动生成器。",
                "实验或效果：在NTU RGB+D 120子集上，仅用每类10样本，提升准确率23.1%。"
            ],
            "tags_zh": [
                "少样本动作合成",
                "文本到运动蒸馏",
                "骨骼动作识别",
                "扩散模型微调",
                "数据增强"
            ],
            "_index": 44
        },
        {
            "title": "Causal Inference in Energy Demand Prediction",
            "authors": [
                "Chutian Ma",
                "Grigorii Pomazkin",
                "Giacinto Paolo Saggese",
                "Paul Smith"
            ],
            "arxiv_id": "2512.11653v1",
            "summary": "Energy demand prediction is critical for grid operators, industrial energy\n  consumers, and service providers. Energy demand is influenced by multiple\n  factors, including weather conditions (e.g. temperature, humidity, wind\n  speed, solar radiation), and calendar information (e.g. hour of day and\n  month of year), which further affect daily work and life schedules. These\n  factors are causally interdependent, making the problem more complex than\n  simple correlation-based learning techniques satisfactorily allow for. We\n  propose a structural causal model that explains the causal relationship\n  between these variables. A full analysis is performed to validate our causal\n  beliefs, also revealing important insights consistent with prior studies.\n  For example, our causal model reveals that energy demand responds to\n  temperature fluctuations with season-dependent sensitivity. Additionally, we\n  find that energy demand exhibits lower variance in winter due to the\n  decoupling effect between temperature changes and daily activity patterns.\n  We then build a Bayesian model, which takes advantage of the causal insights\n  we learned as prior knowledge. The model is trained and tested on unseen\n  data and yields state-of-the-art performance in the form of a 3.84 percent MAPE on\n  the test set. The model also demonstrates strong robustness, as the\n  cross-validation across two years of data yields an average MAPE of 3.88 percent.",
            "headline_zh": "提出结构因果模型与贝叶斯模型，以提升能源需求预测的准确性和鲁棒性。",
            "intro_zh": [
                "核心问题：能源需求受天气和日历因素因果互依影响，传统相关学习方法难以充分处理。",
                "方法要点：构建结构因果模型揭示变量间因果关系，并基于此先验知识开发贝叶斯预测模型。",
                "实验或效果：在未见数据上测试，平均绝对百分比误差为3.84%，跨年交叉验证平均误差为3.88%。"
            ],
            "tags_zh": [
                "因果推断",
                "能源需求预测",
                "结构因果模型",
                "贝叶斯模型",
                "时间序列分析"
            ],
            "_index": 45
        },
        {
            "title": "FactorPortrait: Controllable Portrait Animation via Disentangled Expression, Pose, and Viewpoint",
            "authors": [
                "Jiapeng Tang",
                "Kai Li",
                "Chengxiang Yin",
                "Liuhao Ge",
                "Fei Jiang",
                "Jiu Xu",
                "Matthias Nießner",
                "Christian Häne",
                "Timur Bagautdinov",
                "Egor Zakharov",
                "Peihong Guo"
            ],
            "arxiv_id": "2512.11645v1",
            "summary": "We introduce FactorPortrait, a video diffusion method for controllable portrait animation that enables lifelike synthesis from disentangled control signals of facial expressions, head movement, and camera viewpoints. Given a single portrait image, a driving video, and camera trajectories, our method animates the portrait by transferring facial expressions and head movements from the driving video while simultaneously enabling novel view synthesis from arbitrary viewpoints. We utilize a pre-trained image encoder to extract facial expression latents from the driving video as control signals for animation generation. Such latents implicitly capture nuanced facial expression dynamics with identity and pose information disentangled, and they are efficiently injected into the video diffusion transformer through our proposed expression controller. For camera and head pose control, we employ Plücker ray maps and normal maps rendered from 3D body mesh tracking. To train our model, we curate a large-scale synthetic dataset containing diverse combinations of camera viewpoints, head poses, and facial expression dynamics. Extensive experiments demonstrate that our method outperforms existing approaches in realism, expressiveness, control accuracy, and view consistency.",
            "headline_zh": "提出FactorPortrait，通过解耦控制实现单张肖像动画与视角合成",
            "intro_zh": [
                "核心问题：如何从单张肖像生成可控动画，同时解耦表情、姿态和视角",
                "方法要点：使用预训练编码器提取表情潜变量，结合Plücker射线图控制相机和姿态",
                "实验或效果：在合成数据集上训练，实验显示在真实感、控制精度和视角一致性上优于现有方法"
            ],
            "tags_zh": [
                "肖像动画",
                "视频扩散",
                "解耦控制",
                "视角合成",
                "表情迁移"
            ],
            "_index": 46
        },
        {
            "title": "Automating Historical Insight Extraction from Large-Scale Newspaper Archives via Neural Topic Modeling",
            "authors": [
                "Keerthana Murugaraj",
                "Salima Lamsiyah",
                "Marten During",
                "Martin Theobald"
            ],
            "arxiv_id": "2512.11635v1",
            "summary": "Extracting coherent and human-understandable themes from large collections of unstructured historical newspaper archives presents significant challenges due to topic evolution, Optical Character Recognition (OCR) noise, and the sheer volume of text. Traditional topic-modeling methods, such as Latent Dirichlet Allocation (LDA), often fall short in capturing the complexity and dynamic nature of discourse in historical texts. To address these limitations, we employ BERTopic. This neural topic-modeling approach leverages transformerbased embeddings to extract and classify topics, which, despite its growing popularity, still remains underused in historical research. Our study focuses on articles published between 1955 and 2018, specifically examining discourse on nuclear power and nuclear safety. We analyze various topic distributions across the corpus and trace their temporal evolution to uncover long-term trends and shifts in public discourse. This enables us to more accurately explore patterns in public discourse, including the co-occurrence of themes related to nuclear power and nuclear weapons and their shifts in topic importance over time. Our study demonstrates the scalability and contextual sensitivity of BERTopic as an alternative to traditional approaches, offering richer insights into historical discourses extracted from newspaper archives. These findings contribute to historical, nuclear, and social-science research while reflecting on current limitations and proposing potential directions for future work.",
            "headline_zh": "应用BERTopic从大规模历史报纸档案中自动化提取主题，以分析核能话语演变",
            "intro_zh": [
                "核心问题：传统主题建模方法难以处理历史报纸档案中的主题演变、OCR噪声和大规模文本复杂性",
                "方法要点：采用基于Transformer嵌入的BERTopic进行神经主题建模，以提升主题提取的上下文敏感性和可扩展性",
                "实验或效果：分析1955-2018年核能与核安全相关文章，揭示主题分布、时间演变及核能与核武器主题的共现模式"
            ],
            "tags_zh": [
                "神经主题建模",
                "历史文本分析",
                "BERTopic",
                "核能话语",
                "主题演变",
                "大规模档案处理"
            ],
            "_index": 47
        },
        {
            "title": "Fast and Explicit: Slice-to-Volume Reconstruction via 3D Gaussian Primitives with Analytic Point Spread Function Modeling",
            "authors": [
                "Maik Dannecker",
                "Steven Jia",
                "Nil Stolt-Ansó",
                "Nadine Girard",
                "Guillaume Auzias",
                "François Rousseau",
                "Daniel Rueckert"
            ],
            "arxiv_id": "2512.11624v1",
            "summary": "Recovering high-fidelity 3D images from sparse or degraded 2D images is a fundamental challenge in medical imaging, with broad applications ranging from 3D ultrasound reconstruction to MRI super-resolution. In the context of fetal MRI, high-resolution 3D reconstruction of the brain from motion-corrupted low-resolution 2D acquisitions is a prerequisite for accurate neurodevelopmental diagnosis. While implicit neural representations (INRs) have recently established state-of-the-art performance in self-supervised slice-to-volume reconstruction (SVR), they suffer from a critical computational bottleneck: accurately modeling the image acquisition physics requires expensive stochastic Monte Carlo sampling to approximate the point spread function (PSF). In this work, we propose a shift from neural network based implicit representations to Gaussian based explicit representations. By parameterizing the HR 3D image volume as a field of anisotropic Gaussian primitives, we leverage the property of Gaussians being closed under convolution and thus derive a \\textit{closed-form analytical solution} for the forward model. This formulation reduces the previously intractable acquisition integral to an exact covariance addition ($\\mathbfΣ_{obs} = \\mathbfΣ_{HR} + \\mathbfΣ_{PSF}$), effectively bypassing the need for compute-intensive stochastic sampling while ensuring exact gradient propagation. We demonstrate that our approach matches the reconstruction quality of self-supervised state-of-the-art SVR frameworks while delivering a 5$\\times$--10$\\times$ speed-up on neonatal and fetal data. With convergence often reached in under 30 seconds, our framework paves the way towards translation into clinical routine of real-time fetal 3D MRI. Code will be public at {https://github.com/m-dannecker/Gaussian-Primitives-for-Fast-SVR}.",
            "headline_zh": "提出基于3D高斯基元的显式表示方法，以解决医学成像中自监督切片到体积重建的计算瓶颈问题。",
            "intro_zh": [
                "核心问题：隐式神经表示在建模点扩散函数时需昂贵蒙特卡洛采样，导致计算瓶颈。",
                "方法要点：使用各向异性高斯基元参数化3D图像，通过闭式解析解实现精确前向模型，避免随机采样。",
                "实验或效果：在新生儿和胎儿数据上匹配SOTA重建质量，速度提升5-10倍，收敛时间常低于30秒。"
            ],
            "tags_zh": [
                "切片到体积重建",
                "3D高斯基元",
                "点扩散函数建模",
                "医学成像",
                "自监督学习",
                "计算加速"
            ],
            "_index": 48
        },
        {
            "title": "Architecting Large Action Models for Human-in-the-Loop Intelligent Robots",
            "authors": [
                "Kanisorn Sangchai",
                "Methasit Boonpun",
                "Withawin Kraipetchara",
                "Paulo Garcia"
            ],
            "arxiv_id": "2512.11620v1",
            "summary": "The realization of intelligent robots, operating autonomously and interacting with other intelligent agents, human or artificial, requires the integration of environment perception, reasoning, and action. Classic Artificial Intelligence techniques for this purpose, focusing on symbolic approaches, have long-ago hit the scalability wall on compute and memory costs. Advances in Large Language Models in the past decade (neural approaches) have resulted in unprecedented displays of capability, at the cost of control, explainability, and interpretability. Large Action Models aim at extending Large Language Models to encompass the full perception, reasoning, and action cycle; however, they typically require substantially more comprehensive training and suffer from the same deficiencies in reliability. Here, we show it is possible to build competent Large Action Models by composing off-the-shelf foundation models, and that their control, interpretability, and explainability can be effected by incorporating symbolic wrappers and associated verification on their outputs, achieving verifiable neuro-symbolic solutions for intelligent robots. Our experiments on a multi-modal robot demonstrate that Large Action Model intelligence does not require massive end-to-end training, but can be achieved by integrating efficient perception models with a logic-driven core. We find that driving action execution through the generation of Planning Domain Definition Language (PDDL) code enables a human-in-the-loop verification stage that effectively mitigates action hallucinations. These results can support practitioners in the design and development of robotic Large Action Models across novel industries, and shed light on the ongoing challenges that must be addressed to ensure safety in the field.",
            "headline_zh": "提出基于现成基础模型组合构建大型动作模型，通过符号包装和验证实现人机协作智能机器人。",
            "intro_zh": [
                "核心问题：大型动作模型训练成本高且可靠性不足，难以控制与解释。",
                "方法要点：组合现成基础模型，集成符号包装和PDDL代码生成以支持人机验证。",
                "实验或效果：多模态机器人实验显示无需大规模端到端训练，可有效减少动作幻觉。"
            ],
            "tags_zh": [
                "大型动作模型",
                "神经符号系统",
                "人机协作",
                "PDDL规划",
                "机器人智能",
                "模型组合"
            ],
            "_index": 49
        },
        {
            "title": "A Fast Interpretable Fuzzy Tree Learner",
            "authors": [
                "Javier Fumanal-Idocin",
                "Raquel Fernandez-Peralta",
                "Javier Andreu-Perez"
            ],
            "arxiv_id": "2512.11616v1",
            "summary": "Fuzzy rule-based systems have been mostly used in interpretable decision-making because of their interpretable linguistic rules. However, interpretability requires both sensible linguistic partitions and small rule-base sizes, which are not guaranteed by many existing fuzzy rule-mining algorithms. Evolutionary approaches can produce high-quality models but suffer from prohibitive computational costs, while neural-based methods like ANFIS have problems retaining linguistic interpretations. In this work, we propose an adaptation of classical tree-based splitting algorithms from crisp rules to fuzzy trees, combining the computational efficiency of greedy algoritms with the interpretability advantages of fuzzy logic. This approach achieves interpretable linguistic partitions and substantially improves running time compared to evolutionary-based approaches while maintaining competitive predictive performance. Our experiments on tabular classification benchmarks proof that our method achieves comparable accuracy to state-of-the-art fuzzy classifiers with significantly lower computational cost and produces more interpretable rule bases with constrained complexity. Code is available in: https://github.com/Fuminides/fuzzy_greedy_tree_public",
            "headline_zh": "提出模糊贪婪树算法，以高效生成可解释模糊规则，用于表格分类任务。",
            "intro_zh": [
                "核心问题：现有模糊规则挖掘算法难以同时保证可解释性（合理语言划分和小规则库）与计算效率。",
                "方法要点：将经典树分裂算法从清晰规则扩展到模糊树，结合贪婪算法效率和模糊逻辑可解释性优势。",
                "实验或效果：在表格分类基准上，相比进化方法显著降低计算成本，保持竞争性预测性能，生成更可解释规则库。"
            ],
            "tags_zh": [
                "模糊规则系统",
                "可解释机器学习",
                "贪婪树算法",
                "表格分类",
                "计算效率"
            ],
            "_index": 50
        },
        {
            "title": "Bounding Hallucinations: Information-Theoretic Guarantees for RAG Systems via Merlin-Arthur Protocols",
            "authors": [
                "Björn Deiseroth",
                "Max Henning Höth",
                "Kristian Kersting",
                "Letitia Parcalabescu"
            ],
            "arxiv_id": "2512.11614v1",
            "summary": "Retrieval-augmented generation (RAG) models rely on retrieved evidence to guide large language model (LLM) generators, yet current systems treat retrieval as a weak heuristic rather than verifiable evidence. As a result, LLMs answer without support, hallucinate under incomplete or misleading context, and rely on spurious evidence. We introduce a training framework that treats the entire RAG pipeline -- both the retriever and the generator -- as an interactive proof system via an adaptation of the Merlin-Arthur (M/A) protocol. Arthur (the generator LLM) trains on questions of unkown provenance: Merlin provides helpful evidence, while Morgana injects adversarial, misleading context. Both use a linear-time XAI method to identify and modify the evidence most influential to Arthur. Consequently, Arthur learns to (i) answer when the context support the answer, (ii) reject when evidence is insufficient, and (iii) rely on the specific context spans that truly ground the answer. We further introduce a rigorous evaluation framework to disentangle explanation fidelity from baseline predictive errors. This allows us to introduce and measure the Explained Information Fraction (EIF), which normalizes M/A certified mutual-information guarantees relative to model capacity and imperfect benchmarks. Across three RAG datasets and two model families of varying sizes, M/A-trained LLMs show improved groundedness, completeness, soundness, and reject behavior, as well as reduced hallucinations -- without needing manually annotated unanswerable questions. The retriever likewise improves recall and MRR through automatically generated M/A hard positives and negatives. Our results demonstrate that autonomous interactive-proof-style supervision provides a principled and practical path toward reliable RAG systems that treat retrieved documents not as suggestions, but as verifiable evidence.",
            "headline_zh": "提出基于Merlin-Arthur协议的训练框架，以提升检索增强生成系统的可验证性与可靠性。",
            "intro_zh": [
                "核心问题：当前RAG系统将检索视为启发式而非可验证证据，导致模型幻觉、依赖虚假证据。",
                "方法要点：将RAG管道建模为交互证明系统，通过Merlin提供证据、Morgana注入误导上下文，训练生成器基于证据回答、拒绝或依赖具体上下文。",
                "实验或效果：在多个数据集和模型上，M/A训练改善了模型的基础性、完整性和拒绝行为，减少了幻觉，并提升了检索器性能。"
            ],
            "tags_zh": [
                "检索增强生成",
                "交互证明系统",
                "可解释人工智能",
                "幻觉减少",
                "信息论保证",
                "自动监督"
            ],
            "_index": 51
        },
        {
            "title": "Embodied Image Compression",
            "authors": [
                "Chunyi Li",
                "Rui Qing",
                "Jianbo Zhang",
                "Yuan Tian",
                "Xiangyang Zhu",
                "Zicheng Zhang",
                "Xiaohong Liu",
                "Weisi Lin",
                "Guangtao Zhai"
            ],
            "arxiv_id": "2512.11612v1",
            "summary": "Image Compression for Machines (ICM) has emerged as a pivotal research direction in the field of visual data compression. However, with the rapid evolution of machine intelligence, the target of compression has shifted from task-specific virtual models to Embodied agents operating in real-world environments. To address the communication constraints of Embodied AI in multi-agent systems and ensure real-time task execution, this paper introduces, for the first time, the scientific problem of Embodied Image Compression. We establish a standardized benchmark, EmbodiedComp, to facilitate systematic evaluation under ultra-low bitrate conditions in a closed-loop setting. Through extensive empirical studies in both simulated and real-world settings, we demonstrate that existing Vision-Language-Action models (VLAs) fail to reliably perform even simple manipulation tasks when compressed below the Embodied bitrate threshold. We anticipate that EmbodiedComp will catalyze the development of domain-specific compression tailored for Embodied agents , thereby accelerating the Embodied AI deployment in the Real-world.",
            "headline_zh": "提出具身图像压缩以解决多智能体系统中具身AI的通信约束与实时任务执行问题",
            "intro_zh": [
                "核心问题：首次定义具身图像压缩，针对具身智能体在真实环境中的超低码率压缩需求",
                "方法要点：建立标准化基准EmbodiedComp，在闭环设置下系统评估超低码率性能",
                "实验或效果：实证显示现有视觉-语言-动作模型在低于具身码率阈值时无法可靠执行简单操作任务"
            ],
            "tags_zh": [
                "具身图像压缩",
                "超低码率压缩",
                "多智能体系统",
                "视觉-语言-动作模型",
                "闭环评估"
            ],
            "_index": 52
        },
        {
            "title": "Using GUI Agent for Electronic Design Automation",
            "authors": [
                "Chunyi Li",
                "Longfei Li",
                "Zicheng Zhang",
                "Xiaohong Liu",
                "Min Tang",
                "Weisi Lin",
                "Guangtao Zhai"
            ],
            "arxiv_id": "2512.11611v1",
            "summary": "Graphical User Interface (GUI) agents adopt an end-to-end paradigm that maps a screenshot to an action sequence, thereby automating repetitive tasks in virtual environments. However, existing GUI agents are evaluated almost exclusively on commodity software such as Microsoft Word and Excel. Professional Computer-Aided Design (CAD) suites promise an order-of-magnitude higher economic return, yet remain the weakest performance domain for existing agents and are still far from replacing expert Electronic-Design-Automation (EDA) engineers. We therefore present the first systematic study that deploys GUI agents for EDA workflows. Our contributions are: (1) a large-scale dataset named GUI-EDA, including 5 CAD tools and 5 physical domains, comprising 2,000+ high-quality screenshot-answer-action pairs recorded by EDA scientists and engineers during real-world component design; (2) a comprehensive benchmark that evaluates 30+ mainstream GUI agents, demonstrating that EDA tasks constitute a major, unsolved challenge; and (3) an EDA-specialized metric named EDAgent, equipped with a reflection mechanism that achieves reliable performance on industrial CAD software and, for the first time, outperforms Ph.D. students majored in Electrical Engineering. This work extends GUI agents from generic office automation to specialized, high-value engineering domains and offers a new avenue for advancing EDA productivity. The dataset will be released at: https://github.com/aiben-ch/GUI-EDA.",
            "headline_zh": "提出GUI-EDA数据集与EDAgent方法，将GUI代理应用于电子设计自动化以提升工程效率。",
            "intro_zh": [
                "现有GUI代理在专业CAD软件中性能不足，无法替代EDA工程师。",
                "构建大规模GUI-EDA数据集，包含5种CAD工具和2000+真实截图-动作对。",
                "EDAgent方法在工业CAD软件中首次超越电气工程博士生，解决EDA任务挑战。"
            ],
            "tags_zh": [
                "GUI代理",
                "电子设计自动化",
                "CAD软件",
                "数据集构建",
                "自动化评估",
                "工程应用"
            ],
            "_index": 53
        },
        {
            "title": "UniBYD: A Unified Framework for Learning Robotic Manipulation Across Embodiments Beyond Imitation of Human Demonstrations",
            "authors": [
                "Tingyu Yuan",
                "Biaoliang Guan",
                "Wen Ye",
                "Ziyan Tian",
                "Yi Yang",
                "Weijie Zhou",
                "Yan Huang",
                "Peng Wang",
                "Chaoyang Zhao",
                "Jinqiao Wang"
            ],
            "arxiv_id": "2512.11609v1",
            "summary": "In embodied intelligence, the embodiment gap between robotic and human hands brings significant challenges for learning from human demonstrations. Although some studies have attempted to bridge this gap using reinforcement learning, they remain confined to merely reproducing human manipulation, resulting in limited task performance. In this paper, we propose UniBYD, a unified framework that uses a dynamic reinforcement learning algorithm to discover manipulation policies aligned with the robot's physical characteristics. To enable consistent modeling across diverse robotic hand morphologies, UniBYD incorporates a unified morphological representation (UMR). Building on UMR, we design a dynamic PPO with an annealed reward schedule, enabling reinforcement learning to transition from imitation of human demonstrations to explore policies adapted to diverse robotic morphologies better, thereby going beyond mere imitation of human hands. To address the frequent failures of learning human priors in the early training stage, we design a hybrid Markov-based shadow engine that enables reinforcement learning to imitate human manipulations in a fine-grained manner. To evaluate UniBYD comprehensively, we propose UniManip, the first benchmark encompassing robotic manipulation tasks spanning multiple hand morphologies. Experiments demonstrate a 67.90% improvement in success rate over the current state-of-the-art. Upon acceptance of the paper, we will release our code and benchmark at https://github.com/zhanheng-creator/UniBYD.",
            "headline_zh": "提出UniBYD统一框架，通过动态强化学习超越人类演示模仿，适应多样机器人手形态。",
            "intro_zh": [
                "核心问题：机器人手与人类手间的形态差异阻碍从人类演示中学习，现有方法局限于模仿，性能受限。",
                "方法要点：引入统一形态表示（UMR）和动态PPO算法，结合混合马尔可夫影子引擎，实现从模仿到探索机器人适应策略的过渡。",
                "实验或效果：在UniManip基准上，成功率比当前最优方法提升67.90%，并计划开源代码和基准。"
            ],
            "tags_zh": [
                "机器人操作学习",
                "统一形态表示",
                "动态强化学习",
                "超越模仿",
                "多形态基准",
                "混合马尔可夫引擎"
            ],
            "_index": 54
        },
        {
            "title": "Neural Network-based Partial-Linear Single-Index Models for Environmental Mixtures Analysis",
            "authors": [
                "Hyungrok Do",
                "Yuyan Wang",
                "Mengling Liu",
                "Myeonggyun Lee"
            ],
            "arxiv_id": "2512.11593v1",
            "summary": "Evaluating the health effects of complex environmental mixtures remains a central challenge in environmental health research. Existing approaches vary in their flexibility, interpretability, scalability, and support for diverse outcome types, often limiting their utility in real-world applications. To address these limitations, we propose a neural network-based partial-linear single-index (NeuralPLSI) modeling framework that bridges semiparametric regression modeling interpretability with the expressive power of deep learning. The NeuralPLSI model constructs an interpretable exposure index via a learnable projection and models its relationship with the outcome through a flexible neural network. The framework accommodates continuous, binary, and time-to-event outcomes, and supports inference through a bootstrap-based procedure that yields confidence intervals for key model parameters. We evaluated NeuralPLSI through simulation studies under a range of scenarios and applied it to data from the National Health and Nutrition Examination Survey (NHANES) to demonstrate its practical utility. Together, our contributions establish NeuralPLSI as a scalable, interpretable, and versatile modeling tool for mixture analysis. To promote adoption and reproducibility, we release a user-friendly open-source software package that implements the proposed methodology and supports downstream visualization and inference (\\texttt{https://github.com/hyungrok-do/NeuralPLSI}).",
            "headline_zh": "提出基于神经网络的偏线性单指数模型，用于环境混合物健康效应分析。",
            "intro_zh": [
                "核心问题：评估复杂环境混合物的健康效应，现有方法在灵活性、可解释性和适用性上存在局限。",
                "方法要点：结合半参数回归的可解释性与深度学习的表达能力，通过可学习投影构建暴露指数，并用神经网络建模其与结局的关系。",
                "实验或效果：通过模拟研究和NHANES数据应用验证模型，提供开源软件包支持下游可视化和推断。"
            ],
            "tags_zh": [
                "环境混合物分析",
                "神经网络建模",
                "半参数回归",
                "可解释性",
                "健康效应评估",
                "开源软件"
            ],
            "_index": 55
        },
        {
            "title": "AI Benchmark Democratization and Carpentry",
            "authors": [
                "Gregor von Laszewski",
                "Wesley Brewer",
                "Jeyan Thiyagalingam",
                "Juri Papay",
                "Armstrong Foundjem",
                "Piotr Luszczek",
                "Murali Emani",
                "Shirley V. Moore",
                "Vijay Janapa Reddi",
                "Matthew D. Sinclair",
                "Sebastian Lobentanzer",
                "Sujata Goswami",
                "Benjamin Hawks",
                "Marco Colombo",
                "Nhan Tran",
                "Christine R. Kirkpatrick",
                "Abdulkareem Alsudais",
                "Gregg Barrett",
                "Tianhao Li",
                "Kirsten Morehouse",
                "Shivaram Venkataraman",
                "Rutwik Jain",
                "Kartik Mathur",
                "Victor Lu",
                "Tejinder Singh",
                "Khojasteh Z. Mirza",
                "Kongtao Chen",
                "Sasidhar Kunapuli",
                "Gavin Farrell",
                "Renato Umeton",
                "Geoffrey C. Fox"
            ],
            "arxiv_id": "2512.11588v1",
            "summary": "Benchmarks are a cornerstone of modern machine learning, enabling reproducibility, comparison, and scientific progress. However, AI benchmarks are increasingly complex, requiring dynamic, AI-focused workflows. Rapid evolution in model architectures, scale, datasets, and deployment contexts makes evaluation a moving target. Large language models often memorize static benchmarks, causing a gap between benchmark results and real-world performance.\n  Beyond traditional static benchmarks, continuous adaptive benchmarking frameworks are needed to align scientific assessment with deployment risks. This calls for skills and education in AI Benchmark Carpentry. From our experience with MLCommons, educational initiatives, and programs like the DOE's Trillion Parameter Consortium, key barriers include high resource demands, limited access to specialized hardware, lack of benchmark design expertise, and uncertainty in relating results to application domains. Current benchmarks often emphasize peak performance on top-tier hardware, offering limited guidance for diverse, real-world scenarios.\n  Benchmarking must become dynamic, incorporating evolving models, updated data, and heterogeneous platforms while maintaining transparency, reproducibility, and interpretability. Democratization requires both technical innovation and systematic education across levels, building sustained expertise in benchmark design and use. Benchmarks should support application-relevant comparisons, enabling informed, context-sensitive decisions. Dynamic, inclusive benchmarking will ensure evaluation keeps pace with AI evolution and supports responsible, reproducible, and accessible AI deployment. Community efforts can provide a foundation for AI Benchmark Carpentry.",
            "headline_zh": "提出AI基准民主化与基准木工概念，以应对动态AI评估挑战",
            "intro_zh": [
                "核心问题：AI基准日益复杂，静态基准易被大模型记忆，导致评估与现实性能脱节",
                "方法要点：倡导动态自适应基准框架，结合技术革新与系统教育，提升基准设计和使用技能",
                "实验或效果：基于MLCommons和DOE万亿参数联盟经验，识别资源需求高、硬件访问有限等关键障碍"
            ],
            "tags_zh": [
                "AI基准民主化",
                "动态自适应基准",
                "基准木工",
                "模型评估",
                "可复现性",
                "AI部署"
            ],
            "_index": 56
        },
        {
            "title": "Gradient Descent as a Perceptron Algorithm: Understanding Dynamics and Implicit Acceleration",
            "authors": [
                "Alexander Tyurin"
            ],
            "arxiv_id": "2512.11587v1",
            "summary": "Even for the gradient descent (GD) method applied to neural network training, understanding its optimization dynamics, including convergence rate, iterate trajectories, function value oscillations, and especially its implicit acceleration, remains a challenging problem. We analyze nonlinear models with the logistic loss and show that the steps of GD reduce to those of generalized perceptron algorithms (Rosenblatt, 1958), providing a new perspective on the dynamics. This reduction yields significantly simpler algorithmic steps, which we analyze using classical linear algebra tools. Using these tools, we demonstrate on a minimalistic example that the nonlinearity in a two-layer model can provably yield a faster iteration complexity $\\tilde{O}(\\sqrt{d})$ compared to $Ω(d)$ achieved by linear models, where $d$ is the number of features. This helps explain the optimization dynamics and the implicit acceleration phenomenon observed in neural networks. The theoretical results are supported by extensive numerical experiments. We believe that this alternative view will further advance research on the optimization of neural networks.",
            "headline_zh": "将梯度下降简化为感知机算法，解释神经网络优化动态与隐式加速",
            "intro_zh": [
                "分析梯度下降在神经网络训练中的动态，如收敛率与隐式加速问题",
                "通过逻辑损失将梯度下降步骤简化为广义感知机算法，简化分析",
                "理论证明非线性模型可加速迭代复杂度，实验支持结果"
            ],
            "tags_zh": [
                "梯度下降",
                "感知机算法",
                "优化动态",
                "隐式加速",
                "神经网络训练",
                "迭代复杂度"
            ],
            "_index": 57
        },
        {
            "title": "Atomic Action Slicing: Planner-Aligned Options for Generalist VLA Agents",
            "authors": [
                "Stefan Tabakov",
                "Asen Popov",
                "Dimitar Dimitrov",
                "S. Ensiye Kiyamousavi",
                "Vladimir Hristov",
                "Boris Kraychev"
            ],
            "arxiv_id": "2512.11584v1",
            "summary": "Current vision-language-action (VLA) models generalize poorly, particularly when tasks require new compositions of skills or objects. We introduce Atomic Action Slicing (AAS), a planner-aligned approach that decomposes long-horizon demonstrations into short, typed atomic actions that are easier for planners to use and policies to learn. Using LIBERO demonstrations, AAS produces a validated dataset of 2,124 atomic segments labeled with action type, temporal span, and confidence. A stronger segmenter (Gemini 2.5 Pro) closely matches planner-defined plans and remains robust under keyframe jitter, while smaller models perform worse on multi-object tasks. Fine-tuning CLIP-RT+ on our atomic dataset improves task success from 94.2% to 95.3% on LIBERO-Goal and 83.8% to 88.8% on LIBERO-Long. We publicly release the GATE-VLAP dataset on HuggingFace(https://huggingface.co/datasets/gate-institute/GATE-VLAP-datasets)",
            "headline_zh": "提出原子动作切片方法以提升通用视觉语言动作代理的长时任务泛化能力",
            "intro_zh": [
                "当前视觉语言动作模型在技能或对象新组合任务中泛化能力差",
                "通过分解长时演示为短类型化原子动作，对齐规划器需求",
                "在LIBERO数据集上验证，微调CLIP-RT+模型提升任务成功率"
            ],
            "tags_zh": [
                "原子动作切片",
                "视觉语言动作模型",
                "长时任务规划",
                "动作分解",
                "泛化能力提升"
            ],
            "_index": 58
        },
        {
            "title": "Brain-Semantoks: Learning Semantic Tokens of Brain Dynamics with a Self-Distilled Foundation Model",
            "authors": [
                "Sam Gijsen",
                "Marc-Andre Schulz",
                "Kerstin Ritter"
            ],
            "arxiv_id": "2512.11582v1",
            "summary": "The development of foundation models for functional magnetic resonance imaging (fMRI) time series holds significant promise for predicting phenotypes related to disease and cognition. Current models, however, are often trained using a mask-and-reconstruct objective on small brain regions. This focus on low-level information leads to representations that are sensitive to noise and temporal fluctuations, necessitating extensive fine-tuning for downstream tasks. We introduce Brain-Semantoks, a self-supervised framework designed specifically to learn abstract representations of brain dynamics. Its architecture is built on two core innovations: a semantic tokenizer that aggregates noisy regional signals into robust tokens representing functional networks, and a self-distillation objective that enforces representational stability across time. We show that this objective is stabilized through a novel training curriculum, ensuring the model robustly learns meaningful features from low signal-to-noise time series. We demonstrate that learned representations enable strong performance on a variety of downstream tasks even when only using a linear probe. Furthermore, we provide comprehensive scaling analyses indicating more unlabeled data reliably results in out-of-distribution performance gains without domain adaptation.",
            "headline_zh": "提出Brain-Semantoks框架以学习脑功能磁共振成像时间序列的抽象表示，提升下游任务性能。",
            "intro_zh": [
                "当前fMRI基础模型常基于小区域掩码重建训练，导致表示对噪声敏感，需大量微调。",
                "引入语义分词器聚合区域信号为功能网络令牌，结合自蒸馏目标增强时间稳定性。",
                "学习表示在线性探针下实现强下游任务性能，缩放分析显示无标签数据提升分布外性能。"
            ],
            "tags_zh": [
                "功能磁共振成像",
                "自监督学习",
                "语义分词器",
                "自蒸馏",
                "脑动力学",
                "基础模型"
            ],
            "_index": 59
        },
        {
            "title": "Safe Bayesian optimization across noise models via scenario programming",
            "authors": [
                "Abdullah Tokmak",
                "Thomas B. Schön",
                "Dominik Baumann"
            ],
            "arxiv_id": "2512.11580v1",
            "summary": "Safe Bayesian optimization (BO) with Gaussian processes is an effective tool for tuning control policies in safety-critical real-world systems, specifically due to its sample efficiency and safety guarantees. However, most safe BO algorithms assume homoscedastic sub-Gaussian measurement noise, an assumption that does not hold in many relevant applications. In this article, we propose a straightforward yet rigorous approach for safe BO across noise models, including homoscedastic sub-Gaussian and heteroscedastic heavy-tailed distributions. We provide a high-probability bound on the measurement noise via the scenario approach, integrate these bounds into high probability confidence intervals, and prove safety and optimality for our proposed safe BO algorithm. We deploy our algorithm in synthetic examples and in tuning a controller for the Franka Emika manipulator in simulation.",
            "headline_zh": "提出基于场景编程的安全贝叶斯优化方法，以处理多种噪声模型",
            "intro_zh": [
                "核心问题：现有安全贝叶斯优化算法假设同方差次高斯噪声，不适用于异方差重尾分布等实际场景。",
                "方法要点：通过场景方法提供测量噪声的高概率界，并集成到置信区间中，确保算法安全性和最优性。",
                "实验或效果：在合成示例和Franka Emika机械臂控制器调优仿真中部署验证。"
            ],
            "tags_zh": [
                "安全贝叶斯优化",
                "高斯过程",
                "场景编程",
                "噪声模型",
                "控制器调优"
            ],
            "_index": 60
        },
        {
            "title": "In-Context Learning for Seismic Data Processing",
            "authors": [
                "Fabian Fuchs",
                "Mario Ruben Fernandez",
                "Norman Ettrich",
                "Janis Keuper"
            ],
            "arxiv_id": "2512.11575v1",
            "summary": "Seismic processing transforms raw data into subsurface images essential for geophysical applications. Traditional methods face challenges, such as noisy data, and manual parameter tuning, among others. Recently deep learning approaches have proposed alternative solutions to some of these problems. However, important challenges of existing deep learning approaches are spatially inconsistent results across neighboring seismic gathers and lack of user-control. We address these limitations by introducing ContextSeisNet, an in-context learning model, to seismic demultiple processing. Our approach conditions predictions on a support set of spatially related example pairs: neighboring common-depth point gathers from the same seismic line and their corresponding labels. This allows the model to learn task-specific processing behavior at inference time by observing how similar gathers should be processed, without any retraining. This method provides both flexibility through user-defined examples and improved lateral consistency across seismic lines. On synthetic data, ContextSeisNet outperforms a U-Net baseline quantitatively and demonstrates enhanced spatial coherence between neighboring gathers. On field data, our model achieves superior lateral consistency compared to both traditional Radon demultiple and the U-Net baseline. Relative to the U-Net, ContextSeisNet also delivers improved near-offset performance and more complete multiple removal. Notably, ContextSeisNet achieves comparable field data performance despite being trained on 90% less data, demonstrating substantial data efficiency. These results establish ContextSeisNet as a practical approach for spatially consistent seismic demultiple with potential applicability to other seismic processing tasks.",
            "headline_zh": "提出ContextSeisNet，通过上下文学习解决地震数据去多次波中的空间不一致和用户控制不足问题。",
            "intro_zh": [
                "核心问题：传统和深度学习地震处理方法存在空间结果不一致和缺乏用户控制。",
                "方法要点：引入上下文学习模型，基于空间相关示例对进行预测，无需重新训练。",
                "实验或效果：在合成和现场数据上优于基线，提升空间一致性和数据效率。"
            ],
            "tags_zh": [
                "地震数据处理",
                "上下文学习",
                "去多次波",
                "空间一致性",
                "数据效率"
            ],
            "_index": 61
        },
        {
            "title": "Evaluating Foundation Models' 3D Understanding Through Multi-View Correspondence Analysis",
            "authors": [
                "Valentina Lilova",
                "Toyesh Chakravorty",
                "Julian I. Bibo",
                "Emma Boccaletti",
                "Brandon Li",
                "Lívia Baxová",
                "Cees G. M. Snoek",
                "Mohammadreza Salehi"
            ],
            "arxiv_id": "2512.11574v1",
            "summary": "Benchmarking 3D spatial understanding of foundation models is essential for real-world applications such as robotics and autonomous driving. Existing evaluations often rely on downstream finetuning with linear heads or task-specific decoders, making it difficult to isolate the intrinsic 3D reasoning ability of pretrained encoders. In this work, we introduce a novel benchmark for in-context 3D scene understanding that requires no finetuning and directly probes the quality of dense visual features. Building on the Hummingbird framework, which evaluates in-context 2D scene understanding, we extend the setup to the 3D Multi-View ImageNet (MVImgNet) dataset. Given a set of images from objects in specific angles (keys), we benchmark the performance of segmenting novel views (queries) and report the scores in 4 categories of easy, medium, hard, and extreme based on the key-query view contrast. We benchmark 8 state-of-the-art foundation models and show DINO-based encoders remain competitive across large viewpoint shifts, while 3D-aware models like VGGT require dedicated multi-view adjustments. Our code is publicly available at https://github.com/ToyeshC/open-hummingbird-3d-eval .",
            "headline_zh": "提出无需微调的基准以评估基础模型在3D多视角对应中的内在理解能力",
            "intro_zh": [
                "现有评估依赖下游微调，难以隔离预训练编码器的3D推理能力",
                "基于Hummingbird框架扩展至3D场景，使用MVImgNet数据集进行多视角分割基准测试",
                "评估8个模型，显示DINO编码器在大视角变化下保持竞争力，3D感知模型需调整"
            ],
            "tags_zh": [
                "3D场景理解",
                "基础模型评估",
                "多视角对应",
                "无需微调基准",
                "MVImgNet数据集",
                "视觉特征质量"
            ],
            "_index": 62
        },
        {
            "title": "Visualizing token importance for black-box language models",
            "authors": [
                "Paulius Rauba",
                "Qiyao Wei",
                "Mihaela van der Schaar"
            ],
            "arxiv_id": "2512.11573v1",
            "summary": "We consider the problem of auditing black-box large language models (LLMs) to ensure they behave reliably when deployed in production settings, particularly in high-stakes domains such as legal, medical, and regulatory compliance. Existing approaches for LLM auditing often focus on isolated aspects of model behavior, such as detecting specific biases or evaluating fairness. We are interested in a more general question -- can we understand how the outputs of black-box LLMs depend on each input token? There is a critical need to have such tools in real-world applications that rely on inaccessible API endpoints to language models. However, this is a highly non-trivial problem, as LLMs are stochastic functions (i.e. two outputs will be different by chance), while computing prompt-level gradients to approximate input sensitivity is infeasible. To address this, we propose Distribution-Based Sensitivity Analysis (DBSA), a lightweight model-agnostic procedure to evaluate the sensitivity of the output of a language model for each input token, without making any distributional assumptions about the LLM. DBSA is developed as a practical tool for practitioners, enabling quick, plug-and-play visual exploration of LLMs reliance on specific input tokens. Through illustrative examples, we demonstrate how DBSA can enable users to inspect LLM inputs and find sensitivities that may be overlooked by existing LLM interpretability methods.",
            "headline_zh": "提出分布敏感性分析以评估黑盒语言模型输入令牌重要性",
            "intro_zh": [
                "核心问题：审计黑盒LLM输出对输入令牌的依赖，确保高风险领域可靠性",
                "方法要点：开发轻量级模型无关的DBSA，无需分布假设，可视化令牌敏感性",
                "实验或效果：通过示例展示DBSA能发现现有方法忽略的敏感性，支持快速探索"
            ],
            "tags_zh": [
                "黑盒语言模型",
                "令牌重要性",
                "敏感性分析",
                "模型审计",
                "可解释性"
            ],
            "_index": 63
        },
        {
            "title": "Cross-Entropy Optimization of Physically Grounded Task and Motion Plans",
            "authors": [
                "Andreu Matoses Gimenez",
                "Nils Wilde",
                "Chris Pek",
                "Javier Alonso-Mora"
            ],
            "arxiv_id": "2512.11571v1",
            "summary": "Autonomously performing tasks often requires robots to plan high-level discrete actions and continuous low-level motions to realize them. Previous TAMP algorithms have focused mainly on computational performance, completeness, or optimality by making the problem tractable through simplifications and abstractions. However, this comes at the cost of the resulting plans potentially failing to account for the dynamics or complex contacts necessary to reliably perform the task when object manipulation is required. Additionally, approaches that ignore effects of the low-level controllers may not obtain optimal or feasible plan realizations for the real system. We investigate the use of a GPU-parallelized physics simulator to compute realizations of plans with motion controllers, explicitly accounting for dynamics, and considering contacts with the environment. Using cross-entropy optimization, we sample the parameters of the controllers, or actions, to obtain low-cost solutions. Since our approach uses the same controllers as the real system, the robot can directly execute the computed plans. We demonstrate our approach for a set of tasks where the robot is able to exploit the environment's geometry to move an object. Website and code: https://andreumatoses.github.io/research/parallel-realization",
            "headline_zh": "提出基于交叉熵优化的物理仿真方法，以解决机器人任务与运动规划中的动态和接触问题。",
            "intro_zh": [
                "核心问题：传统TAMP算法因简化可能忽略动态和接触，导致规划不可靠或不可行。",
                "方法要点：使用GPU并行物理仿真计算规划实现，结合交叉熵优化采样控制器参数以获取低成本解。",
                "实验或效果：在机器人利用环境几何移动物体的任务中，规划可直接执行，提升可靠性。"
            ],
            "tags_zh": [
                "任务与运动规划",
                "物理仿真",
                "交叉熵优化",
                "GPU并行计算",
                "机器人控制"
            ],
            "_index": 64
        },
        {
            "title": "Fully Inductive Node Representation Learning via Graph View Transformation",
            "authors": [
                "Dooho Lee",
                "Myeong Kong",
                "Minho Jeong",
                "Jaemin Yoo"
            ],
            "arxiv_id": "2512.11561v1",
            "summary": "Generalizing a pretrained model to unseen datasets without retraining is an essential step toward a foundation model. However, achieving such cross-dataset, fully inductive inference is difficult in graph-structured data where feature spaces vary widely in both dimensionality and semantics. Any transformation in the feature space can easily violate the inductive applicability to unseen datasets, strictly limiting the design space of a graph model. In this work, we introduce the view space, a novel representational axis in which arbitrary graphs can be naturally encoded in a unified manner. We then propose Graph View Transformation (GVT), a node- and feature-permutation-equivariant mapping in the view space. GVT serves as the building block for Recurrent GVT, a fully inductive model for node representation learning. Pretrained on OGBN-Arxiv and evaluated on 27 node-classification benchmarks, Recurrent GVT outperforms GraphAny, the prior fully inductive graph model, by +8.93% and surpasses 12 individually tuned GNNs by at least +3.30%. These results establish the view space as a principled and effective ground for fully inductive node representation learning.",
            "headline_zh": "提出图视图变换以实现跨数据集的完全归纳节点表示学习",
            "intro_zh": [
                "核心问题：图数据特征空间差异大，阻碍预训练模型跨数据集归纳推理",
                "方法要点：引入视图空间，设计节点和特征置换等变的图视图变换作为构建块",
                "实验或效果：在27个节点分类基准上，超越现有完全归纳模型和个体调优GNN"
            ],
            "tags_zh": [
                "图神经网络",
                "完全归纳学习",
                "节点表示学习",
                "视图空间",
                "跨数据集泛化",
                "图视图变换"
            ],
            "_index": 65
        },
        {
            "title": "Multi-temporal Calving Front Segmentation",
            "authors": [
                "Marcel Dreier",
                "Nora Gourmelon",
                "Dakota Pyles",
                "Fei Wu",
                "Matthias Braun",
                "Thorsten Seehaus",
                "Andreas Maier",
                "Vincent Christlein"
            ],
            "arxiv_id": "2512.11560v1",
            "summary": "The calving fronts of marine-terminating glaciers undergo constant changes. These changes significantly affect the glacier's mass and dynamics, demanding continuous monitoring. To address this need, deep learning models were developed that can automatically delineate the calving front in Synthetic Aperture Radar imagery. However, these models often struggle to correctly classify areas affected by seasonal conditions such as ice melange or snow-covered surfaces. To address this issue, we propose to process multiple frames from a satellite image time series of the same glacier in parallel and exchange temporal information between the corresponding feature maps to stabilize each prediction. We integrate our approach into the current state-of-the-art architecture Tyrion and accomplish a new state-of-the-art performance on the CaFFe benchmark dataset. In particular, we achieve a Mean Distance Error of 184.4 m and a mean Intersection over Union of 83.6.",
            "headline_zh": "提出多时相并行处理与特征交换方法，以提升合成孔径雷达影像中冰架崩解前缘分割的稳定性。",
            "intro_zh": [
                "核心问题：现有深度学习模型在季节性条件如冰混合物或积雪覆盖区域分类困难。",
                "方法要点：并行处理同一冰川的卫星图像时间序列，并在特征图间交换时相信息以稳定预测。",
                "实验或效果：在CaFFe基准数据集上实现新最优性能，平均距离误差184.4米，平均交并比83.6%。"
            ],
            "tags_zh": [
                "冰架崩解前缘分割",
                "合成孔径雷达影像",
                "多时相处理",
                "特征交换",
                "深度学习模型",
                "冰川监测"
            ],
            "_index": 66
        },
        {
            "title": "DentalGPT: Incentivizing Multimodal Complex Reasoning in Dentistry",
            "authors": [
                "Zhenyang Cai",
                "Jiaming Zhang",
                "Junjie Zhao",
                "Ziyi Zeng",
                "Yanchao Li",
                "Jingyi Liang",
                "Junying Chen",
                "Yunjin Yang",
                "Jiajun You",
                "Shuzhi Deng",
                "Tongfei Wang",
                "Wanting Chen",
                "Chunxiu Hao",
                "Ruiqi Xie",
                "Zhenwei Wen",
                "Xiangyi Feng",
                "Zou Ting",
                "Jin Zou Lin",
                "Jianquan Li",
                "Guangjun Yu",
                "Liangyi Chen",
                "Junwen Wang",
                "Shan Jiang",
                "Benyou Wang"
            ],
            "arxiv_id": "2512.11558v1",
            "summary": "Reliable interpretation of multimodal data in dentistry is essential for automated oral healthcare, yet current multimodal large language models (MLLMs) struggle to capture fine-grained dental visual details and lack sufficient reasoning ability for precise diagnosis. To address these limitations, we present DentalGPT, a specialized dental MLLM developed through high-quality domain knowledge injection and reinforcement learning. Specifically, the largest annotated multimodal dataset for dentistry to date was constructed by aggregating over 120k dental images paired with detailed descriptions that highlight diagnostically relevant visual features, making it the multimodal dataset with the most extensive collection of dental images to date. Training on this dataset significantly enhances the MLLM's visual understanding of dental conditions, while the subsequent reinforcement learning stage further strengthens its capability for multimodal complex reasoning. Comprehensive evaluations on intraoral and panoramic benchmarks, along with dental subsets of medical VQA benchmarks, show that DentalGPT achieves superior performance in disease classification and dental VQA tasks, outperforming many state-of-the-art MLLMs despite having only 7B parameters. These results demonstrate that high-quality dental data combined with staged adaptation provides an effective pathway for building capable and domain-specialized dental MLLMs.",
            "headline_zh": "提出DentalGPT以解决牙科多模态数据精细视觉理解与复杂推理不足的问题",
            "intro_zh": [
                "当前多模态大语言模型在牙科领域难以捕捉细粒度视觉细节且推理能力不足",
                "通过高质量领域知识注入和强化学习构建专用牙科多模态大语言模型",
                "在牙科基准测试中表现优异，优于许多先进模型，参数仅7B"
            ],
            "tags_zh": [
                "牙科多模态大语言模型",
                "领域知识注入",
                "强化学习",
                "牙科视觉问答",
                "疾病分类",
                "多模态数据集"
            ],
            "_index": 67
        },
        {
            "title": "3DTeethSAM: Taming SAM2 for 3D Teeth Segmentation",
            "authors": [
                "Zhiguo Lu",
                "Jianwen Lou",
                "Mingjun Ma",
                "Hairong Jin",
                "Youyi Zheng",
                "Kun Zhou"
            ],
            "arxiv_id": "2512.11557v1",
            "summary": "3D teeth segmentation, involving the localization of tooth instances and their semantic categorization in 3D dental models, is a critical yet challenging task in digital dentistry due to the complexity of real-world dentition. In this paper, we propose 3DTeethSAM, an adaptation of the Segment Anything Model 2 (SAM2) for 3D teeth segmentation. SAM2 is a pretrained foundation model for image and video segmentation, demonstrating a strong backbone in various downstream scenarios. To adapt SAM2 for 3D teeth data, we render images of 3D teeth models from predefined views, apply SAM2 for 2D segmentation, and reconstruct 3D results using 2D-3D projections. Since SAM2's performance depends on input prompts and its initial outputs often have deficiencies, and given its class-agnostic nature, we introduce three light-weight learnable modules: (1) a prompt embedding generator to derive prompt embeddings from image embeddings for accurate mask decoding, (2) a mask refiner to enhance SAM2's initial segmentation results, and (3) a mask classifier to categorize the generated masks. Additionally, we incorporate Deformable Global Attention Plugins (DGAP) into SAM2's image encoder. The DGAP enhances both the segmentation accuracy and the speed of the training process. Our method has been validated on the 3DTeethSeg benchmark, achieving an IoU of 91.90% on high-resolution 3D teeth meshes, establishing a new state-of-the-art in the field.",
            "headline_zh": "提出3DTeethSAM以解决3D牙齿分割问题，通过适配SAM2并引入轻量模块提升性能。",
            "intro_zh": [
                "核心问题：3D牙齿分割在数字牙科中因真实牙列复杂性而具挑战性，需定位实例并语义分类。",
                "方法要点：从预定义视图渲染3D牙齿模型图像，应用SAM2进行2D分割，通过2D-3D投影重建结果，并引入提示嵌入生成器、掩码精炼器和分类器等轻量模块。",
                "实验或效果：在3DTeethSeg基准测试中，高分辨率3D牙齿网格上达到91.90% IoU，创下新最优性能。"
            ],
            "tags_zh": [
                "3D牙齿分割",
                "SAM2适配",
                "轻量模块",
                "2D-3D投影",
                "数字牙科",
                "基准测试"
            ],
            "_index": 68
        },
        {
            "title": "CarlaNCAP: A Framework for Quantifying the Safety of Vulnerable Road Users in Infrastructure-Assisted Collective Perception Using EuroNCAP Scenarios",
            "authors": [
                "Jörg Gamerdinger",
                "Sven Teufel",
                "Simon Roller",
                "Oliver Bringmann"
            ],
            "arxiv_id": "2512.11551v1",
            "summary": "The growing number of road users has significantly increased the risk of accidents in recent years. Vulnerable Road Users (VRUs) are particularly at risk, especially in urban environments where they are often occluded by parked vehicles or buildings. Autonomous Driving (AD) and Collective Perception (CP) are promising solutions to mitigate these risks. In particular, infrastructure-assisted CP, where sensor units are mounted on infrastructure elements such as traffic lights or lamp posts, can help overcome perceptual limitations by providing enhanced points of view, which significantly reduces occlusions. To encourage decision makers to adopt this technology, comprehensive studies and datasets demonstrating safety improvements for VRUs are essential. In this paper, we propose a framework for evaluating the safety improvement by infrastructure-based CP specifically targeted at VRUs including a dataset with safety-critical EuroNCAP scenarios (CarlaNCAP) with 11k frames. Using this dataset, we conduct an in-depth simulation study and demonstrate that infrastructure-assisted CP can significantly reduce accident rates in safety-critical scenarios, achieving up to 100% accident avoidance compared to a vehicle equipped with sensors with only 33%. Code is available at https://github.com/ekut-es/carla_ncap",
            "headline_zh": "提出CarlaNCAP框架以量化基础设施辅助集体感知在EuroNCAP场景中对弱势道路用户的安全提升",
            "intro_zh": [
                "核心问题：城市环境中弱势道路用户常被遮挡，导致事故风险高，需量化基础设施辅助集体感知的安全效益",
                "方法要点：基于EuroNCAP安全关键场景构建数据集，通过模拟评估基础设施传感器对事故避免的贡献",
                "实验或效果：在11k帧数据集中，基础设施辅助集体感知可将事故避免率从33%提升至100%，显著降低事故率"
            ],
            "tags_zh": [
                "基础设施辅助集体感知",
                "弱势道路用户安全",
                "EuroNCAP场景",
                "模拟评估",
                "事故避免"
            ],
            "_index": 69
        },
        {
            "title": "SSL-MedSAM2: A Semi-supervised Medical Image Segmentation Framework Powered by Few-shot Learning of SAM2",
            "authors": [
                "Zhendi Gong",
                "Xin Chen"
            ],
            "arxiv_id": "2512.11548v1",
            "summary": "Despite the success of deep learning based models in medical image segmentation, most state-of-the-art (SOTA) methods perform fully-supervised learning, which commonly rely on large scale annotated training datasets. However, medical image annotation is highly time-consuming, hindering its clinical applications. Semi-supervised learning (SSL) has been emerged as an appealing strategy in training with limited annotations, largely reducing the labelling cost. We propose a novel SSL framework SSL-MedSAM2, which contains a training-free few-shot learning branch TFFS-MedSAM2 based on the pretrained large foundation model Segment Anything Model 2 (SAM2) for pseudo label generation, and an iterative fully-supervised learning branch FSL-nnUNet based on nnUNet for pseudo label refinement. The results on MICCAI2025 challenge CARE-LiSeg (Liver Segmentation) demonstrate an outstanding performance of SSL-MedSAM2 among other methods. The average dice scores on the test set in GED4 and T1 MRI are 0.9710 and 0.9648 respectively, and the Hausdorff distances are 20.07 and 21.97 respectively. The code is available via https://github.com/naisops/SSL-MedSAM2/tree/main.",
            "headline_zh": "提出SSL-MedSAM2半监督医学图像分割框架，结合SAM2少样本学习与nnUNet迭代训练以降低标注成本。",
            "intro_zh": [
                "核心问题：医学图像标注耗时，全监督方法依赖大规模标注数据，限制临床应用。",
                "方法要点：基于SAM2的免训练少样本分支生成伪标签，结合nnUNet迭代分支进行伪标签精炼。",
                "实验或效果：在CARE-LiSeg挑战中，GED4和T1 MRI测试集Dice分数分别为0.9710和0.9648，表现优异。"
            ],
            "tags_zh": [
                "半监督学习",
                "医学图像分割",
                "少样本学习",
                "伪标签生成",
                "SAM2",
                "nnUNet"
            ],
            "_index": 70
        },
        {
            "title": "Elastic-Net Multiple Kernel Learning: Combining Multiple Data Sources for Prediction",
            "authors": [
                "Janaina Mourão-Miranda",
                "Zakria Hussain",
                "Konstantinos Tsirlis",
                "Christophe Phillips",
                "John Shawe-Taylor"
            ],
            "arxiv_id": "2512.11547v1",
            "summary": "Multiple Kernel Learning (MKL) models combine several kernels in supervised and unsupervised settings to integrate multiple data representations or sources, each represented by a different kernel. MKL seeks an optimal linear combination of base kernels that maximizes a generalized performance measure under a regularization constraint. Various norms have been used to regularize the kernel weights, including $l1$, $l2$ and $lp$, as well as the \"elastic-net\" penalty, which combines $l1$- and $l2$-norm to promote both sparsity and the selection of correlated kernels. This property makes elastic-net regularized MKL (ENMKL) especially valuable when model interpretability is critical and kernels capture correlated information, such as in neuroimaging. Previous ENMKL methods have followed a two-stage procedure: fix kernel weights, train a support vector machine (SVM) with the weighted kernel, and then update the weights via gradient descent, cutting-plane methods, or surrogate functions. Here, we introduce an alternative ENMKL formulation that yields a simple analytical update for the kernel weights. We derive explicit algorithms for both SVM and kernel ridge regression (KRR) under this framework, and implement them in the open-source Pattern Recognition for Neuroimaging Toolbox (PRoNTo). We evaluate these ENMKL algorithms against $l1$-norm MKL and against SVM (or KRR) trained on the unweighted sum of kernels across three neuroimaging applications. Our results show that ENMKL matches or outperforms $l1$-norm MKL in all tasks and only underperforms standard SVM in one scenario. Crucially, ENMKL produces sparser, more interpretable models by selectively weighting correlated kernels.",
            "headline_zh": "提出弹性网多核学习以结合相关核进行稀疏可解释预测，应用于神经影像学。",
            "intro_zh": [
                "多核学习整合多数据源，弹性网正则化促进稀疏性和相关核选择。",
                "新方法提供核权重的解析更新，支持SVM和核岭回归算法实现。",
                "在神经影像应用中，性能优于或匹配l1正则化，模型更稀疏可解释。"
            ],
            "tags_zh": [
                "多核学习",
                "弹性网正则化",
                "稀疏模型",
                "神经影像学",
                "支持向量机",
                "核岭回归"
            ],
            "_index": 71
        },
        {
            "title": "Optimizing the Training Diet: Data Mixture Search for Robust Time Series Forecasting",
            "authors": [
                "Federico Pennino",
                "Maurizio Gabbrielli"
            ],
            "arxiv_id": "2512.11546v1",
            "summary": "The standard paradigm for training deep learning models on sensor data assumes that more data is always better. However, raw sensor streams are often imbalanced and contain significant redundancy, meaning that not all data points contribute equally to model generalization. In this paper, we show that, in some cases, \"less is more\" when considering datasets. We do this by reframing the data selection problem: rather than tuning model hyperparameters, we fix the model and optimize the composition of the training data itself. We introduce a framework for discovering the optimal \"training diet\" from a large, unlabeled time series corpus. Our framework first uses a large-scale encoder and k-means clustering to partition the dataset into distinct, behaviorally consistent clusters. These clusters represent the fundamental 'ingredients' available for training. We then employ the Optuna optimization framework to search the high-dimensional space of possible data mixtures. For each trial, Optuna proposes a specific sampling ratio for each cluster, and a new training set is constructed based on this recipe. A smaller target model is then trained and evaluated. Our experiments reveal that this data-centric search consistently discovers data mixtures that yield models with significantly higher performance compared to baselines trained on the entire dataset. Specifically - evaluated on PMSM dataset - our method improved performance from a baseline MSE of 1.70 to 1.37, a 19.41% improvement.",
            "headline_zh": "提出数据混合搜索框架以优化时间序列预测模型的训练数据组成",
            "intro_zh": [
                "核心问题：标准训练范式假设数据越多越好，但传感器数据常不平衡且冗余，影响模型泛化。",
                "方法要点：使用编码器和聚类划分数据为行为一致簇，通过Optuna优化搜索最佳数据混合比例。",
                "实验或效果：在PMSM数据集上，MSE从1.70提升至1.37，性能改善19.41%。"
            ],
            "tags_zh": [
                "时间序列预测",
                "数据选择优化",
                "聚类分析",
                "Optuna优化",
                "传感器数据"
            ],
            "_index": 72
        },
        {
            "title": "Graph Embedding with Mel-spectrograms for Underwater Acoustic Target Recognition",
            "authors": [
                "Sheng Feng",
                "Shuqing Ma",
                "Xiaoqian Zhu"
            ],
            "arxiv_id": "2512.11545v1",
            "summary": "Underwater acoustic target recognition (UATR) is extremely challenging due to the complexity of ship-radiated noise and the variability of ocean environments. Although deep learning (DL) approaches have achieved promising results, most existing models implicitly assume that underwater acoustic data lie in a Euclidean space. This assumption, however, is unsuitable for the inherently complex topology of underwater acoustic signals, which exhibit non-stationary, non-Gaussian, and nonlinear characteristics. To overcome this limitation, this paper proposes the UATR-GTransformer, a non-Euclidean DL model that integrates Transformer architectures with graph neural networks (GNNs). The model comprises three key components: a Mel patchify block, a GTransformer block, and a classification head. The Mel patchify block partitions the Mel-spectrogram into overlapping patches, while the GTransformer block employs a Transformer Encoder to capture mutual information between split patches to generate Mel-graph embeddings. Subsequently, a GNN enhances these embeddings by modeling local neighborhood relationships, and a feed-forward network (FFN) further performs feature transformation. Experiments results based on two widely used benchmark datasets demonstrate that the UATR-GTransformer achieves performance competitive with state-of-the-art methods. In addition, interpretability analysis reveals that the proposed model effectively extracts rich frequency-domain information, highlighting its potential for applications in ocean engineering.",
            "headline_zh": "提出UATR-GTransformer，结合Transformer与图神经网络，用于水下声学目标识别。",
            "intro_zh": [
                "核心问题：水下声学信号非平稳、非线性，传统欧氏空间假设不适用。",
                "方法要点：将梅尔频谱图分块，用Transformer编码器生成图嵌入，再经GNN增强特征。",
                "实验或效果：在两个基准数据集上性能达到先进水平，可解释性分析显示有效提取频域信息。"
            ],
            "tags_zh": [
                "水下声学目标识别",
                "图神经网络",
                "Transformer",
                "梅尔频谱图",
                "非欧氏深度学习"
            ],
            "_index": 73
        },
        {
            "title": "AI-MASLD Metabolic Dysfunction and Information Steatosis of Large Language Models in Unstructured Clinical Narratives",
            "authors": [
                "Yuan Shen",
                "Xiaojun Wu",
                "Linghua Yu"
            ],
            "arxiv_id": "2512.11544v1",
            "summary": "This study aims to simulate real-world clinical scenarios to systematically evaluate the ability of Large Language Models (LLMs) to extract core medical information from patient chief complaints laden with noise and redundancy, and to verify whether they exhibit a functional decline analogous to Metabolic Dysfunction-Associated Steatotic Liver Disease (MASLD). We employed a cross-sectional analysis design based on standardized medical probes, selecting four mainstream LLMs as research subjects: GPT-4o, Gemini 2.5, DeepSeek 3.1, and Qwen3-Max. An evaluation system comprising twenty medical probes across five core dimensions was used to simulate a genuine clinical communication environment. All probes had gold-standard answers defined by clinical experts and were assessed via a double-blind, inverse rating scale by two independent clinicians. The results show that all tested models exhibited functional defects to varying degrees, with Qwen3-Max demonstrating the best overall performance and Gemini 2.5 the worst. Under conditions of extreme noise, most models experienced a functional collapse. Notably, GPT-4o made a severe misjudgment in the risk assessment for pulmonary embolism (PE) secondary to deep vein thrombosis (DVT). This research is the first to empirically confirm that LLMs exhibit features resembling metabolic dysfunction when processing clinical information, proposing the innovative concept of \"AI-Metabolic Dysfunction-Associated Steatotic Liver Disease (AI-MASLD)\". These findings offer a crucial safety warning for the application of Artificial Intelligence (AI) in healthcare, emphasizing that current LLMs must be used as auxiliary tools under human expert supervision, as there remains a significant gap between their theoretical knowledge and practical clinical application.",
            "headline_zh": "提出AI-MASLD概念，揭示大语言模型在嘈杂临床叙述中提取信息时功能下降",
            "intro_zh": [
                "核心问题：评估大语言模型从含噪声的临床主诉中提取核心医学信息的能力，并验证其是否表现出类似代谢功能障碍的功能衰退",
                "方法要点：采用横断面分析设计，基于标准化医学探针，测试GPT-4o等四款主流模型，模拟真实临床沟通环境",
                "实验或效果：所有模型均出现不同程度功能缺陷，极端噪声下多数模型功能崩溃，GPT-4o在肺栓塞风险评估中严重误判"
            ],
            "tags_zh": [
                "大语言模型评估",
                "临床信息提取",
                "AI-MASLD",
                "医疗人工智能安全",
                "噪声鲁棒性"
            ],
            "_index": 74
        },
        {
            "title": "Infinity and Beyond: Compositional Alignment in VAR and Diffusion T2I Models",
            "authors": [
                "Hossein Shahabadi",
                "Niki Sepasian",
                "Arash Marioriyad",
                "Ali Sharifi-Zarchi",
                "Mahdieh Soleymani Baghshah"
            ],
            "arxiv_id": "2512.11542v1",
            "summary": "Achieving compositional alignment between textual descriptions and generated images - covering objects, attributes, and spatial relationships - remains a core challenge for modern text-to-image (T2I) models. Although diffusion-based architectures have been widely studied, the compositional behavior of emerging Visual Autoregressive (VAR) models is still largely unexamined. We benchmark six diverse T2I systems - SDXL, PixArt-$α$, Flux-Dev, Flux-Schnell, Infinity-2B, and Infinity-8B - across the full T2I-CompBench++ and GenEval suites, evaluating alignment in color and attribute binding, spatial relations, numeracy, and complex multi-object prompts. Across both benchmarks, Infinity-8B achieves the strongest overall compositional alignment, while Infinity-2B also matches or exceeds larger diffusion models in several categories, highlighting favorable efficiency-performance trade-offs. In contrast, SDXL and PixArt-$α$ show persistent weaknesses in attribute-sensitive and spatial tasks. These results provide the first systematic comparison of VAR and diffusion approaches to compositional alignment and establish unified baselines for the future development of the T2I model.",
            "headline_zh": "评估VAR与扩散T2I模型在组合对齐上的性能，Infinity-8B表现最佳",
            "intro_zh": [
                "核心问题：文本到图像模型在对象、属性和空间关系的组合对齐上仍面临挑战",
                "方法要点：系统比较六种T2I模型，包括VAR和扩散架构，使用T2I-CompBench++和GenEval基准",
                "实验或效果：Infinity-8B在组合对齐上整体最强，Infinity-2B在效率-性能权衡中表现优异"
            ],
            "tags_zh": [
                "文本到图像模型",
                "组合对齐",
                "VAR模型",
                "扩散模型",
                "基准测试",
                "性能评估"
            ],
            "_index": 75
        },
        {
            "title": "A Multi-Criteria Automated MLOps Pipeline for Cost-Effective Cloud-Based Classifier Retraining in Response to Data Distribution Shifts",
            "authors": [
                "Emmanuel K. Katalay",
                "David O. Dimandja",
                "Jordan F. Masakuna"
            ],
            "arxiv_id": "2512.11541v1",
            "summary": "The performance of machine learning (ML) models often deteriorates when the underlying data distribution changes over time, a phenomenon known as data distribution drift. When this happens, ML models need to be retrained and redeployed. ML Operations (MLOps) is often manual, i.e., humans trigger the process of model retraining and redeployment. In this work, we present an automated MLOps pipeline designed to address neural network classifier retraining in response to significant data distribution changes. Our MLOps pipeline employs multi-criteria statistical techniques to detect distribution shifts and triggers model updates only when necessary, ensuring computational efficiency and resource optimization. We demonstrate the effectiveness of our framework through experiments on several benchmark anomaly detection data sets, showing significant improvements in model accuracy and robustness compared to traditional retraining strategies. Our work provides a foundation for deploying more reliable and adaptive ML systems in dynamic real-world settings, where data distribution changes are common.",
            "headline_zh": "提出多标准自动化MLOps管道，以应对数据分布漂移，实现云端分类器高效重训练。",
            "intro_zh": [
                "核心问题：数据分布漂移导致机器学习模型性能下降，传统MLOps依赖人工触发重训练。",
                "方法要点：采用多标准统计技术检测分布变化，仅在必要时自动触发模型更新，优化计算资源。",
                "实验或效果：在多个异常检测数据集上验证，相比传统策略，模型准确性和鲁棒性显著提升。"
            ],
            "tags_zh": [
                "MLOps自动化",
                "数据分布漂移检测",
                "分类器重训练",
                "云端资源优化",
                "多标准统计方法",
                "异常检测"
            ],
            "_index": 76
        },
        {
            "title": "HFS: Holistic Query-Aware Frame Selection for Efficient Video Reasoning",
            "authors": [
                "Yiqing Yang",
                "Kin-Man Lam"
            ],
            "arxiv_id": "2512.11534v1",
            "summary": "Key frame selection in video understanding presents significant challenges. Traditional top-K selection methods, which score frames independently, often fail to optimize the selection as a whole. This independent scoring frequently results in selecting frames that are temporally clustered and visually redundant. Additionally, training lightweight selectors using pseudo labels generated offline by Multimodal Large Language Models (MLLMs) prevents the supervisory signal from dynamically adapting to task objectives. To address these limitations, we propose an end-to-end trainable, task-adaptive framework for frame selection. A Chain-of-Thought approach guides a Small Language Model (SLM) to generate task-specific implicit query vectors, which are combined with multimodal features to enable dynamic frame scoring. We further define a continuous set-level objective function that incorporates relevance, coverage, and redundancy, enabling differentiable optimization via Gumbel-Softmax to select optimal frame combinations at the set level. Finally, student-teacher mutual learning is employed, where the student selector (SLM) and teacher reasoner (MLLM) are trained to align their frame importance distributions via KL divergence. Combined with cross-entropy loss, this enables end-to-end optimization, eliminating reliance on static pseudo labels. Experiments across various benchmarks, including Video-MME, LongVideoBench, MLVU, and NExT-QA, demonstrate that our method significantly outperforms existing approaches.",
            "headline_zh": "提出端到端可训练的任务自适应框架HFS，通过整体优化解决视频推理中关键帧选择问题。",
            "intro_zh": [
                "核心问题：传统独立评分方法导致帧选择冗余且无法整体优化，离线伪标签训练限制任务适应性。",
                "方法要点：结合链式思维生成任务特定查询向量，定义连续集级目标函数，通过师生互学习实现端到端优化。",
                "实验或效果：在多个基准测试中显著优于现有方法，验证了框架的有效性。"
            ],
            "tags_zh": [
                "视频理解",
                "关键帧选择",
                "端到端训练",
                "任务自适应",
                "师生互学习",
                "整体优化"
            ],
            "_index": 77
        },
        {
            "title": "Parallax: Runtime Parallelization for Operator Fallbacks in Heterogeneous Edge Systems",
            "authors": [
                "Chong Tang",
                "Hao Dai",
                "Jagmohan Chauhan"
            ],
            "arxiv_id": "2512.11532v1",
            "summary": "The growing demand for real-time DNN applications on edge devices necessitates faster inference of increasingly complex models. Although many devices include specialized accelerators (e.g., mobile GPUs), dynamic control-flow operators and unsupported kernels often fall back to CPU execution. Existing frameworks handle these fallbacks poorly, leaving CPU cores idle and causing high latency and memory spikes. We introduce Parallax, a framework that accelerates mobile DNN inference without model refactoring or custom operator implementations. Parallax first partitions the computation DAG to expose parallelism, then employs branch-aware memory management with dedicated arenas and buffer reuse to reduce runtime footprint. An adaptive scheduler executes branches according to device memory constraints, meanwhile, fine-grained subgraph control enables heterogeneous inference of dynamic models. By evaluating on five representative DNNs across three different mobile devices, Parallax achieves up to 46% latency reduction, maintains controlled memory overhead (26.5% on average), and delivers up to 30% energy savings compared with state-of-the-art frameworks, offering improvements aligned with the responsiveness demands of real-time mobile inference.",
            "headline_zh": "提出Parallax框架以加速移动设备上动态DNN推理的算子回退问题",
            "intro_zh": [
                "核心问题：移动设备上动态控制流算子回退CPU执行导致高延迟和内存峰值",
                "方法要点：通过计算图分区、分支感知内存管理和自适应调度实现并行化",
                "实验或效果：在三种移动设备上评估，实现最高46%延迟降低和平均26.5%内存开销"
            ],
            "tags_zh": [
                "移动DNN推理",
                "算子回退",
                "并行化框架",
                "内存管理",
                "异构边缘系统"
            ],
            "_index": 78
        },
        {
            "title": "Parametric Numerical Integration with (Differential) Machine Learning",
            "authors": [
                "Álvaro Leitao",
                "Jonatan Ráfales"
            ],
            "arxiv_id": "2512.11530v1",
            "summary": "In this work, we introduce a machine/deep learning methodology to solve parametric integrals. Besides classical machine learning approaches, we consider a differential learning framework that incorporates derivative information during training, emphasizing its advantageous properties. Our study covers three representative problem classes: statistical functionals (including moments and cumulative distribution functions), approximation of functions via Chebyshev expansions, and integrals arising directly from differential equations. These examples range from smooth closed-form benchmarks to challenging numerical integrals. Across all cases, the differential machine learning-based approach consistently outperforms standard architectures, achieving lower mean squared error, enhanced scalability, and improved sample efficiency.",
            "headline_zh": "提出微分机器学习方法以解决参数积分计算问题",
            "intro_zh": [
                "核心问题：参数积分计算在统计函数、切比雪夫展开和微分方程中具有挑战性",
                "方法要点：结合导数信息的微分学习框架，提升训练效率和准确性",
                "实验或效果：在多种问题中优于标准架构，降低均方误差并增强可扩展性"
            ],
            "tags_zh": [
                "参数积分计算",
                "微分机器学习",
                "统计函数",
                "切比雪夫展开",
                "微分方程积分"
            ],
            "_index": 79
        },
        {
            "title": "xGR: Efficient Generative Recommendation Serving at Scale",
            "authors": [
                "Qingxiao Sun",
                "Tongxuan Liu",
                "Shen Zhang",
                "Siyu Wu",
                "Peijun Yang",
                "Haotian Liang",
                "Menxin Li",
                "Xiaolong Ma",
                "Zhiwei Liang",
                "Ziyi Ren",
                "Minchao Zhang",
                "Xinyu Liu",
                "Ke Zhang",
                "Depei Qian",
                "Hailong Yang"
            ],
            "arxiv_id": "2512.11529v1",
            "summary": "Recommendation system delivers substantial economic benefits by providing personalized predictions. Generative recommendation (GR) integrates LLMs to enhance the understanding of long user-item sequences. Despite employing attention-based architectures, GR's workload differs markedly from that of LLM serving. GR typically processes long prompt while producing short, fixed-length outputs, yet the computational cost of each decode phase is especially high due to the large beam width. In addition, since the beam search involves a vast item space, the sorting overhead becomes particularly time-consuming. We propose xGR, a GR-oriented serving system that meets strict low-latency requirements under highconcurrency scenarios. First, xGR unifies the processing of prefill and decode phases through staged computation and separated KV cache. Second, xGR enables early sorting termination and mask-based item filtering with data structure reuse. Third, xGR reconstructs the overall pipeline to exploit multilevel overlap and multi-stream parallelism. Our experiments with real-world recommendation service datasets demonstrate that xGR achieves at least 3.49x throughput compared to the state-of-the-art baseline under strict latency constraints.",
            "headline_zh": "提出xGR以解决高并发下生成式推荐系统低延迟服务问题",
            "intro_zh": [
                "核心问题：生成式推荐处理长提示、短输出，但解码阶段计算和排序开销大，难以满足高并发低延迟需求。",
                "方法要点：通过阶段化计算和分离KV缓存统一预填充与解码，利用早期排序终止和基于掩码的过滤优化排序，重构流水线实现多级重叠和多流并行。",
                "实验或效果：在真实推荐数据集上，xGR在严格延迟约束下比现有基线至少提升3.49倍吞吐量。"
            ],
            "tags_zh": [
                "生成式推荐",
                "低延迟服务",
                "KV缓存优化",
                "排序加速",
                "流水线并行"
            ],
            "_index": 80
        },
        {
            "title": "Contrastive Time Series Forecasting with Anomalies",
            "authors": [
                "Joel Ekstrand",
                "Zahra Taghiyarrenani",
                "Slawomir Nowaczyk"
            ],
            "arxiv_id": "2512.11526v1",
            "summary": "Time series forecasting predicts future values from past data. In real-world settings, some anomalous events have lasting effects and influence the forecast, while others are short-lived and should be ignored. Standard forecasting models fail to make this distinction, often either overreacting to noise or missing persistent shifts. We propose Co-TSFA (Contrastive Time Series Forecasting with Anomalies), a regularization framework that learns when to ignore anomalies and when to respond. Co-TSFA generates input-only and input-output augmentations to model forecast-irrelevant and forecast-relevant anomalies, and introduces a latent-output alignment loss that ties representation changes to forecast changes. This encourages invariance to irrelevant perturbations while preserving sensitivity to meaningful distributional shifts. Experiments on the Traffic and Electricity benchmarks, as well as on a real-world cash-demand dataset, demonstrate that Co-TSFA improves performance under anomalous conditions while maintaining accuracy on normal data. An anonymized GitHub repository with the implementation of Co-TSFA is provided and will be made public upon acceptance.",
            "headline_zh": "提出Co-TSFA框架以解决时间序列预测中异常事件区分问题",
            "intro_zh": [
                "核心问题：标准模型难以区分异常事件对预测的持久或短暂影响，导致过反应或漏检。",
                "方法要点：通过输入-输出增强和潜在输出对齐损失，学习忽略无关异常并响应相关分布变化。",
                "实验或效果：在交通、电力和现金需求数据集上验证，提升异常条件下性能并保持正常数据准确性。"
            ],
            "tags_zh": [
                "时间序列预测",
                "异常处理",
                "对比学习",
                "正则化框架",
                "分布偏移"
            ],
            "_index": 81
        },
        {
            "title": "NeuralOGCM: Differentiable Ocean Modeling with Learnable Physics",
            "authors": [
                "Hao Wu",
                "Yuan Gao",
                "Fan Xu",
                "Fan Zhang",
                "Guangliang Liu",
                "Yuxuan Liang",
                "Xiaomeng Huang"
            ],
            "arxiv_id": "2512.11525v1",
            "summary": "High-precision scientific simulation faces a long-standing trade-off between computational efficiency and physical fidelity. To address this challenge, we propose NeuralOGCM, an ocean modeling framework that fuses differentiable programming with deep learning. At the core of NeuralOGCM is a fully differentiable dynamical solver, which leverages physics knowledge as its core inductive bias. The learnable physics integration captures large-scale, deterministic physical evolution, and transforms key physical parameters (e.g., diffusion coefficients) into learnable parameters, enabling the model to autonomously optimize its physical core via end-to-end training. Concurrently, a deep neural network learns to correct for subgrid-scale processes and discretization errors not captured by the physics model. Both components work in synergy, with their outputs integrated by a unified ODE solver. Experiments demonstrate that NeuralOGCM maintains long-term stability and physical consistency, significantly outperforming traditional numerical models in speed and pure AI baselines in accuracy. Our work paves a new path for building fast, stable, and physically-plausible models for scientific computing.",
            "headline_zh": "提出NeuralOGCM框架，融合可微分编程与深度学习以解决海洋建模中计算效率与物理保真度的权衡问题。",
            "intro_zh": [
                "核心问题：高精度科学模拟面临计算效率与物理保真度的长期权衡。",
                "方法要点：结合可微分动力学求解器与深度神经网络，通过端到端训练优化物理参数并校正子网格过程。",
                "实验或效果：模型保持长期稳定性和物理一致性，在速度和准确性上优于传统数值模型与纯AI基线。"
            ],
            "tags_zh": [
                "可微分海洋建模",
                "学习物理",
                "深度学习校正",
                "科学计算",
                "端到端训练"
            ],
            "_index": 82
        },
        {
            "title": "Super-Resolved Canopy Height Mapping from Sentinel-2 Time Series Using LiDAR HD Reference Data across Metropolitan France",
            "authors": [
                "Ekaterina Kalinicheva",
                "Florian Helen",
                "Stéphane Mermoz",
                "Florian Mouret",
                "Milena Planells"
            ],
            "arxiv_id": "2512.11524v1",
            "summary": "Fine-scale forest monitoring is essential for understanding canopy structure and its dynamics, which are key indicators of carbon stocks, biodiversity, and forest health. Deep learning is particularly effective for this task, as it integrates spectral, temporal, and spatial signals that jointly reflect the canopy structure. To address this need, we introduce THREASURE-Net, a novel end-to-end framework for Tree Height Regression And Super-Resolution. The model is trained on Sentinel-2 time series using reference height metrics derived from LiDAR HD data at multiple spatial resolutions over Metropolitan France to produce annual height maps. We evaluate three model variants, producing tree-height predictions at 2.5 m, 5 m, and 10 m resolution. THREASURE-Net does not rely on any pretrained model nor on reference very high resolution optical imagery to train its super-resolution module; instead, it learns solely from LiDAR-derived height information. Our approach outperforms existing state-of-the-art methods based on Sentinel data and is competitive with methods based on very high resolution imagery. It can be deployed to generate high-precision annual canopy-height maps, achieving mean absolute errors of 2.62 m, 2.72 m, and 2.88 m at 2.5 m, 5 m, and 10 m resolution, respectively. These results highlight the potential of THREASURE-Net for scalable and cost-effective structural monitoring of temperate forests using only freely available satellite data. The source code for THREASURE-Net is available at: https://github.com/Global-Earth-Observation/threasure-net.",
            "headline_zh": "提出THREASURE-Net框架，利用Sentinel-2时序数据和LiDAR参考实现森林冠层高度超分辨率制图",
            "intro_zh": [
                "核心问题：精细尺度森林监测需高分辨率冠层高度图，以评估碳储量、生物多样性和森林健康。",
                "方法要点：THREASURE-Net为端到端深度学习框架，整合光谱、时空信号，从LiDAR高度数据学习超分辨率，无需预训练模型或高分辨率光学影像。",
                "实验或效果：在法国大都市区评估，生成2.5米、5米和10米分辨率高度图，平均绝对误差分别为2.62米、2.72米和2.88米，优于现有方法。"
            ],
            "tags_zh": [
                "森林冠层高度制图",
                "超分辨率",
                "深度学习",
                "Sentinel-2时序数据",
                "LiDAR参考数据",
                "端到端框架"
            ],
            "_index": 83
        },
        {
            "title": "Reconstruction as a Bridge for Event-Based Visual Question Answering",
            "authors": [
                "Hanyue Lou",
                "Jiayi Zhou",
                "Yang Zhang",
                "Boyu Li",
                "Yi Wang",
                "Guangnan Ye",
                "Boxin Shi"
            ],
            "arxiv_id": "2512.11510v1",
            "summary": "Integrating event cameras with Multimodal Large Language Models (MLLMs) promises general scene understanding in challenging visual conditions, yet requires navigating a trade-off between preserving the unique advantages of event data and ensuring compatibility with frame-based models. We address this challenge by using reconstruction as a bridge, proposing a straightforward Frame-based Reconstruction and Tokenization (FRT) method and designing an efficient Adaptive Reconstruction and Tokenization (ART) method that leverages event sparsity. For robust evaluation, we introduce EvQA, the first objective, real-world benchmark for event-based MLLMs, comprising 1,000 event-Q&A pairs from 22 public datasets. Our experiments demonstrate that our methods achieve state-of-the-art performance on EvQA, highlighting the significant potential of MLLMs in event-based vision.",
            "headline_zh": "提出基于重建的方法以解决事件相机与多模态大语言模型融合中的权衡问题",
            "intro_zh": [
                "核心问题：事件相机与帧基模型融合需平衡事件数据优势与兼容性",
                "方法要点：设计FRT和ART方法，利用重建作为桥梁，ART利用事件稀疏性提升效率",
                "实验或效果：在EvQA基准上实现最优性能，验证MLLMs在事件视觉中的潜力"
            ],
            "tags_zh": [
                "事件相机",
                "多模态大语言模型",
                "视觉问答",
                "重建方法",
                "稀疏性利用",
                "基准评测"
            ],
            "_index": 84
        },
        {
            "title": "Does Less Hallucination Mean Less Creativity? An Empirical Investigation in LLMs",
            "authors": [
                "Mohor Banerjee",
                "Nadya Yuki Wangsajaya",
                "Syed Ali Redha Alsagoff",
                "Min Sen Tan",
                "Zachary Choy Kit Chun",
                "Alvin Chan Guo Wei"
            ],
            "arxiv_id": "2512.11509v1",
            "summary": "Large Language Models (LLMs) exhibit remarkable capabilities in natural language understanding and reasoning, but suffer from hallucination: the generation of factually incorrect content. While numerous methods have been developed to reduce hallucinations, their impact on creative generations remains unexplored. This gap is particularly critical for AI-assisted scientific discovery, which requires both factual accuracy and creative hypothesis generation. We investigate how three hallucination-reduction techniques: Chain of Verification (CoVe), Decoding by Contrasting Layers (DoLa), and Retrieval-Augmented Generation (RAG), affect creativity in LLMs. Evaluating multiple model families (LLaMA, Qwen, Mistral) at varying scales (1B - 70B parameters) on two creativity benchmarks (NeoCoder and CS4), we find that these methods have opposing effects on divergent creativity. CoVe enhances divergent thinking, DoLa suppresses it, and RAG shows minimal impact. Our findings provide guidance for selecting appropriate hallucination-reduction methods in scientific applications, where the balance between factual accuracy and creative exploration is crucial.",
            "headline_zh": "探究幻觉减少技术对LLMs创造力的影响，为科学应用提供方法选择指导",
            "intro_zh": [
                "核心问题：幻觉减少技术如何影响LLMs的创造力，尤其在科学发现中需平衡事实准确性与创造性",
                "方法要点：评估CoVe、DoLa和RAG三种技术对LLaMA、Qwen、Mistral等模型在NeoCoder和CS4基准上的创造力影响",
                "实验或效果：CoVe增强发散思维，DoLa抑制发散思维，RAG影响最小，提供方法选择依据"
            ],
            "tags_zh": [
                "幻觉减少",
                "创造力评估",
                "大语言模型",
                "科学发现",
                "发散思维"
            ],
            "_index": 85
        },
        {
            "title": "On Geometric Understanding and Learned Data Priors in VGGT",
            "authors": [
                "Jelena Bratulić",
                "Sudhanshu Mittal",
                "Thomas Brox",
                "Christian Rupprecht"
            ],
            "arxiv_id": "2512.11508v1",
            "summary": "The Visual Geometry Grounded Transformer (VGGT) is a 3D foundation model that infers camera geometry and scene structure in a single feed-forward pass. Trained in a supervised, single-step fashion on large datasets, VGGT raises a key question: does it build upon geometric concepts like traditional multi-view methods, or does it rely primarily on learned appearance-based data-driven priors? In this work, we conduct a systematic analysis of VGGT's internal mechanisms to uncover whether geometric understanding emerges within its representations. By probing intermediate features, analyzing attention patterns, and performing interventions, we examine how the model implements its functionality. Our findings reveal that VGGT implicitly performs correspondence matching within its global attention layers and encodes epipolar geometry, despite being trained without explicit geometric constraints. We further investigate VGGT's dependence on its learned data priors. Using spatial input masking and perturbation experiments, we assess its robustness to occlusions, appearance variations, and camera configurations, comparing it with classical multi-stage pipelines. Together, these insights highlight how VGGT internalizes geometric structure while using learned data-driven priors.",
            "headline_zh": "分析VGGT模型内部机制以揭示其几何理解与数据先验的依赖关系",
            "intro_zh": [
                "核心问题：VGGT是否基于几何概念或主要依赖学习的数据驱动先验",
                "方法要点：通过探测中间特征、分析注意力模式和干预实验来研究模型功能实现",
                "实验或效果：发现VGGT隐式执行对应匹配并编码极线几何，评估其对遮挡和相机配置的鲁棒性"
            ],
            "tags_zh": [
                "视觉几何基础模型",
                "注意力机制分析",
                "几何理解",
                "数据驱动先验",
                "鲁棒性评估",
                "多视图方法"
            ],
            "_index": 86
        },
        {
            "title": "SSA3D: Text-Conditioned Assisted Self-Supervised Framework for Automatic Dental Abutment Design",
            "authors": [
                "Mianjie Zheng",
                "Xinquan Yang",
                "Along He",
                "Xuguang Li",
                "Feilie Zhong",
                "Xuefen Liu",
                "Kun Tang",
                "Zhicheng Zhang",
                "Linlin Shen"
            ],
            "arxiv_id": "2512.11507v1",
            "summary": "Abutment design is a critical step in dental implant restoration. However, manual design involves tedious measurement and fitting, and research on automating this process with AI is limited, due to the unavailability of large annotated datasets. Although self-supervised learning (SSL) can alleviate data scarcity, its need for pre-training and fine-tuning results in high computational costs and long training times. In this paper, we propose a Self-supervised assisted automatic abutment design framework (SS$A^3$D), which employs a dual-branch architecture with a reconstruction branch and a regression branch. The reconstruction branch learns to restore masked intraoral scan data and transfers the learned structural information to the regression branch. The regression branch then predicts the abutment parameters under supervised learning, which eliminates the separate pre-training and fine-tuning process. We also design a Text-Conditioned Prompt (TCP) module to incorporate clinical information (such as implant location, system, and series) into SS$A^3$D. This guides the network to focus on relevant regions and constrains the parameter predictions. Extensive experiments on a collected dataset show that SS$A^3$D saves half of the training time and achieves higher accuracy than traditional SSL methods. It also achieves state-of-the-art performance compared to other methods, significantly improving the accuracy and efficiency of automated abutment design.",
            "headline_zh": "提出SSA3D框架，通过双分支架构和文本条件提示，实现高效自动牙科基台设计。",
            "intro_zh": [
                "核心问题：牙科基台设计依赖人工，AI自动化因标注数据稀缺和自监督学习计算成本高而受限。",
                "方法要点：采用重建与回归双分支架构，结合文本条件提示模块，整合临床信息以指导网络预测。",
                "实验或效果：在收集数据集上，SSA3D节省一半训练时间，精度优于传统自监督方法，达到先进水平。"
            ],
            "tags_zh": [
                "牙科基台设计",
                "自监督学习",
                "双分支架构",
                "文本条件提示",
                "自动参数预测"
            ],
            "_index": 87
        },
        {
            "title": "EmeraldMind: A Knowledge Graph-Augmented Framework for Greenwashing Detection",
            "authors": [
                "Georgios Kaoukis",
                "Ioannis Aris Koufopoulos",
                "Psaroudaki Eleni",
                "Danae Pla Karidi",
                "Evaggelia Pitoura",
                "George Papastefanatos",
                "Panayiotis Tsaparas"
            ],
            "arxiv_id": "2512.11506v1",
            "summary": "As AI and web agents become pervasive in decision-making, it is critical to design intelligent systems that not only support sustainability efforts but also guard against misinformation. Greenwashing, i.e., misleading corporate sustainability claims, poses a major challenge to environmental progress. To address this challenge, we introduce EmeraldMind, a fact-centric framework integrating a domain-specific knowledge graph with retrieval-augmented generation to automate greenwashing detection. EmeraldMind builds the EmeraldGraph from diverse corporate ESG (environmental, social, and governance) reports, surfacing verifiable evidence, often missing in generic knowledge bases, and supporting large language models in claim assessment. The framework delivers justification-centric classifications, presenting transparent, evidence-backed verdicts and abstaining responsibly when claims cannot be verified. Experiments on a new greenwashing claims dataset demonstrate that EmeraldMind achieves competitive accuracy, greater coverage, and superior explanation quality compared to generic LLMs, without the need for fine-tuning or retraining.",
            "headline_zh": "提出EmeraldMind框架，结合知识图谱与检索增强生成，以自动化检测企业漂绿行为。",
            "intro_zh": [
                "核心问题：企业漂绿（误导性可持续性声明）阻碍环境进展，需自动化检测系统。",
                "方法要点：构建EmeraldGraph知识图谱，整合企业ESG报告，支持检索增强生成进行声明评估。",
                "实验或效果：在新数据集上验证，EmeraldMind在准确性、覆盖范围和解释质量上优于通用大语言模型。"
            ],
            "tags_zh": [
                "漂绿检测",
                "知识图谱",
                "检索增强生成",
                "ESG报告",
                "自动化评估",
                "解释性AI"
            ],
            "_index": 88
        },
        {
            "title": "BAID: A Benchmark for Bias Assessment of AI Detectors",
            "authors": [
                "Priyam Basu",
                "Yunfeng Zhang",
                "Vipul Raheja"
            ],
            "arxiv_id": "2512.11505v1",
            "summary": "AI-generated text detectors have recently gained adoption in educational and professional contexts. Prior research has uncovered isolated cases of bias, particularly against English Language Learners (ELLs) however, there is a lack of systematic evaluation of such systems across broader sociolinguistic factors. In this work, we propose BAID, a comprehensive evaluation framework for AI detectors across various types of biases. As a part of the framework, we introduce over 200k samples spanning 7 major categories: demographics, age, educational grade level, dialect, formality, political leaning, and topic. We also generated synthetic versions of each sample with carefully crafted prompts to preserve the original content while reflecting subgroup-specific writing styles. Using this, we evaluate four open-source state-of-the-art AI text detectors and find consistent disparities in detection performance, particularly low recall rates for texts from underrepresented groups. Our contributions provide a scalable, transparent approach for auditing AI detectors and emphasize the need for bias-aware evaluation before these tools are deployed for public use.",
            "headline_zh": "提出BAID基准以系统评估AI文本检测器在广泛社会语言因素中的偏见问题。",
            "intro_zh": [
                "核心问题：AI文本检测器缺乏对人口统计、方言等社会语言偏见的系统性评估。",
                "方法要点：构建包含20万样本的框架，覆盖7类偏见，并生成保留内容但反映子群写作风格的合成文本。",
                "实验或效果：评估四个开源检测器，发现对少数群体文本的召回率低，强调部署前需偏见感知评估。"
            ],
            "tags_zh": [
                "AI文本检测器",
                "偏见评估",
                "社会语言因素",
                "基准测试",
                "合成文本生成"
            ],
            "_index": 89
        },
        {
            "title": "TSkel-Mamba: Temporal Dynamic Modeling via State Space Model for Human Skeleton-based Action Recognition",
            "authors": [
                "Yanan Liu",
                "Jun Liu",
                "Hao Zhang",
                "Dan Xu",
                "Hossein Rahmani",
                "Mohammed Bennamoun",
                "Qiuhong Ke"
            ],
            "arxiv_id": "2512.11503v1",
            "summary": "Skeleton-based action recognition has garnered significant attention in the computer vision community. Inspired by the recent success of the selective state-space model (SSM) Mamba in modeling 1D temporal sequences, we propose TSkel-Mamba, a hybrid Transformer-Mamba framework that effectively captures both spatial and temporal dynamics. In particular, our approach leverages Spatial Transformer for spatial feature learning while utilizing Mamba for temporal modeling. Mamba, however, employs separate SSM blocks for individual channels, which inherently limits its ability to model inter-channel dependencies. To better adapt Mamba for skeleton data and enhance Mamba`s ability to model temporal dependencies, we introduce a Temporal Dynamic Modeling (TDM) block, which is a versatile plug-and-play component that integrates a novel Multi-scale Temporal Interaction (MTI) module. The MTI module employs multi-scale Cycle operators to capture cross-channel temporal interactions, a critical factor in action recognition. Extensive experiments on NTU-RGB+D 60, NTU-RGB+D 120, NW-UCLA and UAV-Human datasets demonstrate that TSkel-Mamba achieves state-of-the-art performance while maintaining low inference time, making it both efficient and highly effective.",
            "headline_zh": "提出TSkel-Mamba框架，通过状态空间模型增强骨架动作识别中的时空动态建模。",
            "intro_zh": [
                "核心问题：Mamba在骨架动作识别中建模通道间依赖能力有限，影响时间动态捕捉。",
                "方法要点：结合Transformer处理空间特征，引入TDM块和MTI模块以多尺度循环算子增强跨通道时间交互。",
                "实验或效果：在多个数据集上实现最优性能，同时保持低推理时间，高效有效。"
            ],
            "tags_zh": [
                "骨架动作识别",
                "状态空间模型",
                "时空建模",
                "Transformer-Mamba混合框架",
                "多尺度时间交互"
            ],
            "_index": 90
        },
        {
            "title": "FRQI Pairs method for image classification using Quantum Recurrent Neural Network",
            "authors": [
                "Rafał Potempa",
                "Michał Kordasz",
                "Sundas Naqeeb Khan",
                "Krzysztof Werner",
                "Kamil Wereszczyński",
                "Krzysztof Simiński",
                "Krzysztof A. Cyran"
            ],
            "arxiv_id": "2512.11499v1",
            "summary": "This study aims to introduce the FRQI Pairs method to a wider audience, a novel approach to image classification using Quantum Recurrent Neural Networks (QRNN) with Flexible Representation for Quantum Images (FRQI).\n  The study highlights an innovative approach to use quantum encoded data for an image classification task, suggesting that such quantum-based approaches could significantly reduce the complexity of quantum algorithms. Comparison of the FRQI Pairs method with contemporary techniques underscores the promise of integrating quantum computing principles with neural network architectures for the development of quantum machine learning.",
            "headline_zh": "提出FRQI Pairs方法，结合量子循环神经网络进行图像分类，以降低量子算法复杂度。",
            "intro_zh": [
                "核心问题：探索量子编码数据在图像分类中的应用，以提升量子机器学习效率。",
                "方法要点：采用FRQI Pairs方法，将图像编码为量子态，并集成量子循环神经网络架构。",
                "实验或效果：与现有技术比较，显示该方法在降低量子算法复杂度方面具有潜力。"
            ],
            "tags_zh": [
                "量子图像分类",
                "量子循环神经网络",
                "FRQI编码",
                "量子机器学习",
                "算法复杂度降低"
            ],
            "_index": 91
        },
        {
            "title": "VLM2GeoVec: Toward Universal Multimodal Embeddings for Remote Sensing",
            "authors": [
                "Emanuel Sánchez Aimar",
                "Gulnaz Zhambulova",
                "Fahad Shahbaz Khan",
                "Yonghao Xu",
                "Michael Felsberg"
            ],
            "arxiv_id": "2512.11490v1",
            "summary": "Satellite imagery differs fundamentally from natural images: its aerial viewpoint, very high resolution, diverse scale variations, and abundance of small objects demand both region-level spatial reasoning and holistic scene understanding. Current remote-sensing approaches remain fragmented between dual-encoder retrieval models, which excel at large-scale cross-modal search but cannot interleave modalities, and generative assistants, which support region-level interpretation but lack scalable retrieval capabilities. We propose $\\textbf{VLM2GeoVec}$, an instruction-following, single-encoder vision-language model trained contrastively to embed interleaved inputs (images, text, bounding boxes, and geographic coordinates) in a unified vector space. Our single encoder interleaves all inputs into one joint embedding trained with a contrastive loss, eliminating multi-stage pipelines and task-specific modules. To evaluate its versatility, we introduce $\\textbf{RSMEB}$, a novel benchmark covering key remote-sensing embedding applications: scene classification; cross-modal search; compositional retrieval; visual-question answering; visual grounding and region-level reasoning; and semantic geospatial retrieval. On RSMEB, it achieves $\\textbf{26.6%}$ P@1 on region-caption retrieval (+25 pp vs. dual-encoder baselines), $\\textbf{32.5%}$ P@1 on referring-expression retrieval (+19 pp), and $\\textbf{17.8%}$ P@1 on semantic geo-localization retrieval (over $3\\times$ prior best), while matching or exceeding specialized baselines on conventional tasks such as scene classification and cross-modal retrieval. VLM2GeoVec unifies scalable retrieval with region-level spatial reasoning, enabling cohesive multimodal analysis in remote sensing. We will publicly release the code, checkpoints, and data upon acceptance.",
            "headline_zh": "提出VLM2GeoVec，通过单编码器统一嵌入多模态遥感数据，实现可扩展检索与区域级空间推理。",
            "intro_zh": [
                "遥感图像与自然图像差异显著，现有方法在检索与生成任务间存在割裂。",
                "VLM2GeoVec采用单编码器处理交错输入，通过对比损失训练统一向量空间。",
                "在RSMEB基准上，区域级检索任务性能显著提升，同时保持传统任务竞争力。"
            ],
            "tags_zh": [
                "遥感视觉语言模型",
                "多模态嵌入",
                "对比学习",
                "区域级推理",
                "地理空间检索"
            ],
            "_index": 92
        },
        {
            "title": "Towards Privacy-Preserving Code Generation: Differentially Private Code Language Models",
            "authors": [
                "Melih Catal",
                "Pooja Rani",
                "Harald C. Gall"
            ],
            "arxiv_id": "2512.11482v1",
            "summary": "Large language models specialized for code (CodeLLMs) have demonstrated remarkable capabilities in generating code snippets, documentation, and test cases. However, despite their promising capabilities, CodeLLMs can inadvertently memorize and reproduce snippets from their training data, which poses risks of privacy breaches and intellectual property violations. These risks restrict the deployment of CodeLLMs in sensitive domains and limit their training datasets to publicly available sources. To mitigate the memorization risk without compromising their task performance, we apply Differential Privacy (DP) to CodeLLMs. To the best of our knowledge, this is the first comprehensive study that systematically evaluates the effectiveness of DP in CodeLLMs. DP adds calibrated noise to the training process to protect individual data points while still allowing the model to learn useful patterns. To this end, we first identify and understand the driving reasons of the memorization behaviour of the CodeLLMs during their fine-tuning. Then, to address this issue, we empirically evaluate the effect of DP on mitigating memorization while preserving code generation capabilities. Our findings show that DP substantially reduces memorization in CodeLLMs across all the tested snippet types. The snippet types most prone to memorization are also the most effectively mitigated by DP. Furthermore, we observe that DP slightly increases perplexity but preserves, and can even enhance, the code generation capabilities of CodeLLMs, which makes it feasible to apply DP in practice without significantly compromising model utility. Finally, we analyze the impact of DP on training efficiency and energy consumption, finding that DP does not significantly affect training time or energy usage, making it a practical choice for privacy-preserving CodeLLMs training.",
            "headline_zh": "应用差分隐私于代码语言模型以缓解记忆风险并保持生成能力",
            "intro_zh": [
                "代码语言模型在训练中可能记忆并泄露敏感代码片段，引发隐私和知识产权风险",
                "通过差分隐私在训练过程中添加校准噪声，有效减少记忆行为而不显著损害模型性能",
                "实验显示差分隐私大幅降低记忆率，轻微增加困惑度，且对训练效率和能耗影响小"
            ],
            "tags_zh": [
                "差分隐私",
                "代码语言模型",
                "隐私保护",
                "代码生成",
                "模型训练",
                "记忆缓解"
            ],
            "_index": 93
        },
        {
            "title": "CADMorph: Geometry-Driven Parametric CAD Editing via a Plan-Generate-Verify Loop",
            "authors": [
                "Weijian Ma",
                "Shizhao Sun",
                "Ruiyu Wang",
                "Jiang Bian"
            ],
            "arxiv_id": "2512.11480v1",
            "summary": "A Computer-Aided Design (CAD) model encodes an object in two coupled forms: a parametric construction sequence and its resulting visible geometric shape. During iterative design, adjustments to the geometric shape inevitably require synchronized edits to the underlying parametric sequence, called geometry-driven parametric CAD editing. The task calls for 1) preserving the original sequence's structure, 2) ensuring each edit's semantic validity, and 3) maintaining high shape fidelity to the target shape, all under scarce editing data triplets. We present CADMorph, an iterative plan-generate-verify framework that orchestrates pretrained domain-specific foundation models during inference: a parameter-to-shape (P2S) latent diffusion model and a masked-parameter-prediction (MPP) model. In the planning stage, cross-attention maps from the P2S model pinpoint the segments that need modification and offer editing masks. The MPP model then infills these masks with semantically valid edits in the generation stage. During verification, the P2S model embeds each candidate sequence in shape-latent space, measures its distance to the target shape, and selects the closest one. The three stages leverage the inherent geometric consciousness and design knowledge in pretrained priors, and thus tackle structure preservation, semantic validity, and shape fidelity respectively. Besides, both P2S and MPP models are trained without triplet data, bypassing the data-scarcity bottleneck. CADMorph surpasses GPT-4o and specialized CAD baselines, and supports downstream applications such as iterative editing and reverse-engineering enhancement.",
            "headline_zh": "提出CADMorph框架，通过计划-生成-验证循环解决几何驱动参数化CAD编辑问题",
            "intro_zh": [
                "核心问题：几何驱动参数化CAD编辑需在数据稀缺下同步调整几何形状与参数序列，保持结构、语义和保真度",
                "方法要点：利用预训练P2S和MPP模型，通过计划、生成、验证三阶段迭代编辑，无需三元组数据训练",
                "实验或效果：超越GPT-4o和专用基线，支持迭代编辑和逆向工程增强等应用"
            ],
            "tags_zh": [
                "参数化CAD编辑",
                "几何驱动设计",
                "计划-生成-验证框架",
                "预训练模型",
                "形状保真度",
                "数据稀缺处理"
            ],
            "_index": 94
        },
        {
            "title": "General-purpose AI models can generate actionable knowledge on agroecological crop protection",
            "authors": [
                "Kris A. G. Wyckhuys"
            ],
            "arxiv_id": "2512.11474v1",
            "summary": "Generative artificial intelligence (AI) offers potential for democratizing scientific knowledge and converting this to clear, actionable information, yet its application in agri-food science remains unexplored. Here, we verify the scientific knowledge on agroecological crop protection that is generated by either web-grounded or non-grounded large language models (LLMs), i.e., DeepSeek versus the free-tier version of ChatGPT. For nine globally limiting pests, weeds, and plant diseases, we assessed the factual accuracy, data consistency, and breadth of knowledge or data completeness of each LLM. Overall, DeepSeek consistently screened a 4.8-49.7-fold larger literature corpus and reported 1.6-2.4-fold more biological control agents or management solutions than ChatGPT. As a result, DeepSeek reported 21.6% higher efficacy estimates, exhibited greater laboratory-to-field data consistency, and showed more realistic effects of pest identity and management tactics. However, both models hallucinated, i.e., fabricated fictitious agents or references, reported on implausible ecological interactions or outcomes, confused old and new scientific nomenclatures, and omitted data on key agents or solutions. Despite these shortcomings, both LLMs correctly reported low-resolution efficacy trends. Overall, when paired with rigorous human oversight, LLMs may pose a powerful tool to support farm-level decision-making and unleash scientific creativity.",
            "headline_zh": "评估通用AI模型在农业生态作物保护中生成可操作知识的潜力",
            "intro_zh": [
                "核心问题：AI在农业食品科学中生成科学知识的应用未充分探索，需验证其准确性与实用性。",
                "方法要点：比较基于网络与非网络的LLMs（DeepSeek与ChatGPT）在九种全球性病虫害上的知识生成能力。",
                "实验或效果：DeepSeek在文献覆盖、解决方案数量和一致性上优于ChatGPT，但两者均存在幻觉和遗漏问题。"
            ],
            "tags_zh": [
                "农业生态作物保护",
                "大型语言模型",
                "知识生成",
                "幻觉检测",
                "决策支持",
                "科学验证"
            ],
            "_index": 95
        },
        {
            "title": "Mirror Skin: In Situ Visualization of Robot Touch Intent on Robotic Skin",
            "authors": [
                "David Wagmann",
                "Matti Krüger",
                "Chao Wang",
                "Jürgen Steimle"
            ],
            "arxiv_id": "2512.11472v1",
            "summary": "Effective communication of robotic touch intent is a key factor in promoting safe and predictable physical human-robot interaction (pHRI). While intent communication has been widely studied, existing approaches lack the spatial specificity and semantic depth necessary to convey robot touch actions. We present Mirror Skin, a cephalopod-inspired concept that utilizes high-resolution, mirror-like visual feedback on robotic skin. By mapping in-situ visual representations of a human's body parts onto the corresponding robot's touch region, Mirror Skin communicates who shall initiate touch, where it will occur, and when it is imminent. To inform the design of Mirror Skin, we conducted a structured design exploration with experts in virtual reality (VR), iteratively refining six key dimensions. A subsequent controlled user study demonstrated that Mirror Skin significantly enhances accuracy and reduces response times for interpreting touch intent. These findings highlight the potential of visual feedback on robotic skin to communicate human-robot touch interactions.",
            "headline_zh": "提出Mirror Skin以通过机器人皮肤上的高分辨率视觉反馈增强人机触摸意图的沟通",
            "intro_zh": [
                "核心问题：现有机器人触摸意图沟通方法缺乏空间特异性和语义深度，影响人机交互的安全性和可预测性。",
                "方法要点：采用头足类动物启发的概念，在机器人皮肤上映射人体部位的视觉表示，以原位显示谁、哪里、何时进行触摸。",
                "实验或效果：通过专家设计探索和用户研究，Mirror Skin显著提高了意图解释的准确性和减少了响应时间。"
            ],
            "tags_zh": [
                "机器人皮肤",
                "视觉反馈",
                "人机交互",
                "触摸意图",
                "原位可视化"
            ],
            "_index": 96
        },
        {
            "title": "Rethinking Expert Trajectory Utilization in LLM Post-training",
            "authors": [
                "Bowen Ding",
                "Yuhan Chen",
                "Jiayang Lv",
                "Jiyao Yuan",
                "Qi Zhu",
                "Shuangshuang Tian",
                "Dantong Zhu",
                "Futing Wang",
                "Heyuan Deng",
                "Fei Mi",
                "Lifeng Shang",
                "Tao Lin"
            ],
            "arxiv_id": "2512.11470v1",
            "summary": "While effective post-training integrates Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), the optimal mechanism for utilizing expert trajectories remains unresolved. We propose the Plasticity-Ceiling Framework to theoretically ground this landscape, decomposing performance into foundational SFT performance and the subsequent RL plasticity. Through extensive benchmarking, we establish the Sequential SFT-then-RL pipeline as the superior standard, overcoming the stability deficits of synchronized approaches. Furthermore, we derive precise scaling guidelines: (1) Transitioning to RL at the SFT Stable or Mild Overfitting Sub-phase maximizes the final ceiling by securing foundational SFT performance without compromising RL plasticity; (2) Refuting ``Less is More'' in the context of SFT-then-RL scaling, we demonstrate that Data Scale determines the primary post-training potential, while Trajectory Difficulty acts as a performance multiplier; and (3) Identifying that the Minimum SFT Validation Loss serves as a robust indicator for selecting the expert trajectories that maximize the final performance ceiling. Our findings provide actionable guidelines for maximizing the value extracted from expert trajectories.",
            "headline_zh": "提出塑性-上限框架以优化大语言模型后训练中专家轨迹的利用",
            "intro_zh": [
                "核心问题：专家轨迹在后训练中的最佳利用机制未解决，影响SFT与RL的整合效果。",
                "方法要点：通过塑性-上限框架理论分析，分解性能为基础SFT表现与后续RL塑性。",
                "实验或效果：基准测试确立顺序SFT-then-RL为优标准，提供数据规模和轨迹难度的缩放指南。"
            ],
            "tags_zh": [
                "大语言模型后训练",
                "专家轨迹利用",
                "塑性-上限框架",
                "SFT-then-RL",
                "缩放指南",
                "性能优化"
            ],
            "_index": 97
        },
        {
            "title": "Three methods, one problem: Classical and AI approaches to no-three-in-line",
            "authors": [
                "Pranav Ramanathan",
                "Thomas Prellberg",
                "Matthew Lewis",
                "Prathamesh Dinesh Joshi",
                "Raj Abhijit Dandekar",
                "Rajat Dandekar",
                "Sreedath Panat"
            ],
            "arxiv_id": "2512.11469v1",
            "summary": "The No-Three-In-Line problem asks for the maximum number of points that can be placed on an n by n grid with no three collinear, representing a famous problem in combinatorial geometry. While classical methods like Integer Linear Programming (ILP) guarantee optimal solutions, they face exponential scaling with grid size, and recent advances in machine learning offer promising alternatives for pattern-based approximation. This paper presents the first systematic comparison of classical optimization and AI approaches to this problem, evaluating their performance against traditional algorithms. We apply PatternBoost transformer learning and reinforcement learning (PPO) to this problem for the first time, comparing them against ILP. ILP achieves provably optimal solutions up to 19 by 19 grids, while PatternBoost matches optimal performance up to 14 by 14 grids with 96% test loss reduction. PPO achieves perfect solutions on 10 by 10 grids but fails at 11 by 11 grids, where constraint violations prevent valid configurations. These results demonstrate that classical optimization remains essential for exact solutions while AI methods offer competitive performance on smaller instances, with hybrid approaches presenting the most promising direction for scaling to larger problem sizes.",
            "headline_zh": "比较经典优化与AI方法解决无三点共线问题，提出混合方法以扩展规模",
            "intro_zh": [
                "研究无三点共线问题，寻求n×n网格中最大非共线点集",
                "应用整数线性规划、PatternBoost变换器学习和PPO强化学习三种方法",
                "ILP在19×19网格内最优，PatternBoost在14×14网格内匹配最优，PPO在10×10网格内完美但11×11失败"
            ],
            "tags_zh": [
                "无三点共线问题",
                "整数线性规划",
                "变换器学习",
                "强化学习",
                "组合几何",
                "混合优化"
            ],
            "_index": 98
        },
        {
            "title": "DOS: Distilling Observable Softmaps of Zipfian Prototypes for Self-Supervised Point Representation",
            "authors": [
                "Mohamed Abdelsamad",
                "Michael Ulrich",
                "Bin Yang",
                "Miao Zhang",
                "Yakov Miron",
                "Abhinav Valada"
            ],
            "arxiv_id": "2512.11465v1",
            "summary": "Recent advances in self-supervised learning (SSL) have shown tremendous potential for learning 3D point cloud representations without human annotations. However, SSL for 3D point clouds still faces critical challenges due to irregular geometry, shortcut-prone reconstruction, and unbalanced semantics distribution. In this work, we propose DOS (Distilling Observable Softmaps), a novel SSL framework that self-distills semantic relevance softmaps only at observable (unmasked) points. This strategy prevents information leakage from masked regions and provides richer supervision than discrete token-to-prototype assignments. To address the challenge of unbalanced semantics in an unsupervised setting, we introduce Zipfian prototypes and incorporate them using a modified Sinkhorn-Knopp algorithm, Zipf-Sinkhorn, which enforces a power-law prior over prototype usage and modulates the sharpness of the target softmap during training. DOS outperforms current state-of-the-art methods on semantic segmentation and 3D object detection across multiple benchmarks, including nuScenes, Waymo, SemanticKITTI, ScanNet, and ScanNet200, without relying on extra data or annotations. Our results demonstrate that observable-point softmaps distillation offers a scalable and effective paradigm for learning robust 3D representations.",
            "headline_zh": "提出DOS框架，通过可观测点软图蒸馏和Zipfian原型解决3D点云自监督学习中的语义不平衡和信息泄露问题。",
            "intro_zh": [
                "核心问题：3D点云自监督学习面临几何不规则、重建易走捷径和语义分布不平衡的挑战。",
                "方法要点：仅蒸馏可观测点的语义相关性软图，引入Zipfian原型并使用Zipf-Sinkhorn算法强制原型使用的幂律先验。",
                "实验或效果：在多个基准测试中优于当前最先进方法，无需额外数据或标注，验证了可扩展性和有效性。"
            ],
            "tags_zh": [
                "3D点云表示",
                "自监督学习",
                "软图蒸馏",
                "Zipfian原型",
                "语义分割",
                "3D目标检测"
            ],
            "_index": 99
        },
        {
            "title": "Exploring MLLM-Diffusion Information Transfer with MetaCanvas",
            "authors": [
                "Han Lin",
                "Xichen Pan",
                "Ziqi Huang",
                "Ji Hou",
                "Jialiang Wang",
                "Weifeng Chen",
                "Zecheng He",
                "Felix Juefei-Xu",
                "Junzhe Sun",
                "Zhipeng Fan",
                "Ali Thabet",
                "Mohit Bansal",
                "Chu Wang"
            ],
            "arxiv_id": "2512.11464v1",
            "summary": "Multimodal learning has rapidly advanced visual understanding, largely via multimodal large language models (MLLMs) that use powerful LLMs as cognitive cores. In visual generation, however, these powerful core models are typically reduced to global text encoders for diffusion models, leaving most of their reasoning and planning ability unused. This creates a gap: current multimodal LLMs can parse complex layouts, attributes, and knowledge-intensive scenes, yet struggle to generate images or videos with equally precise and structured control. We propose MetaCanvas, a lightweight framework that lets MLLMs reason and plan directly in spatial and spatiotemporal latent spaces and interface tightly with diffusion generators. We empirically implement MetaCanvas on three different diffusion backbones and evaluate it across six tasks, including text-to-image generation, text/image-to-video generation, image/video editing, and in-context video generation, each requiring precise layouts, robust attribute binding, and reasoning-intensive control. MetaCanvas consistently outperforms global-conditioning baselines, suggesting that treating MLLMs as latent-space planners is a promising direction for narrowing the gap between multimodal understanding and generation.",
            "headline_zh": "提出MetaCanvas框架，让多模态大语言模型在潜在空间中规划，以提升扩散模型生成图像的精确控制能力。",
            "intro_zh": [
                "当前多模态大语言模型在视觉理解中能解析复杂布局，但在生成图像时仅用作全局文本编码器，未充分利用其推理能力。",
                "MetaCanvas是一个轻量级框架，使多模态大语言模型能在空间和时空潜在空间中直接进行推理和规划，并与扩散生成器紧密接口。",
                "在三种扩散骨干网络上实现，并在六项任务中评估，包括文本到图像生成和视频编辑，均优于全局条件基线。"
            ],
            "tags_zh": [
                "多模态大语言模型",
                "扩散模型",
                "潜在空间规划",
                "图像生成",
                "视频生成",
                "精确控制"
            ],
            "_index": 100
        },
        {
            "title": "Boosting Skeleton-based Zero-Shot Action Recognition with Training-Free Test-Time Adaptation",
            "authors": [
                "Jingmin Zhu",
                "Anqi Zhu",
                "Hossein Rahmani",
                "Jun Liu",
                "Mohammed Bennamoun",
                "Qiuhong Ke"
            ],
            "arxiv_id": "2512.11458v1",
            "summary": "We introduce Skeleton-Cache, the first training-free test-time adaptation framework for skeleton-based zero-shot action recognition (SZAR), aimed at improving model generalization to unseen actions during inference. Skeleton-Cache reformulates inference as a lightweight retrieval process over a non-parametric cache that stores structured skeleton representations, combining both global and fine-grained local descriptors. To guide the fusion of descriptor-wise predictions, we leverage the semantic reasoning capabilities of large language models (LLMs) to assign class-specific importance weights. By integrating these structured descriptors with LLM-guided semantic priors, Skeleton-Cache dynamically adapts to unseen actions without any additional training or access to training data. Extensive experiments on NTU RGB+D 60/120 and PKU-MMD II demonstrate that Skeleton-Cache consistently boosts the performance of various SZAR backbones under both zero-shot and generalized zero-shot settings. The code is publicly available at https://github.com/Alchemist0754/Skeleton-Cache.",
            "headline_zh": "提出Skeleton-Cache框架，通过无训练测试时适应提升骨架零样本动作识别的泛化能力。",
            "intro_zh": [
                "针对骨架零样本动作识别中未见动作泛化不足的问题，提出首个无训练测试时适应框架。",
                "方法结合全局与局部骨架描述符，并利用大语言模型语义推理指导预测融合，实现动态适应。",
                "在NTU RGB+D 60/120和PKU-MMD II数据集上验证，能提升多种骨干网络的零样本和广义零样本性能。"
            ],
            "tags_zh": [
                "骨架动作识别",
                "零样本学习",
                "测试时适应",
                "大语言模型",
                "非参数缓存"
            ],
            "_index": 101
        },
        {
            "title": "Hyperbolic Gaussian Blurring Mean Shift: A Statistical Mode-Seeking Framework for Clustering in Curved Spaces",
            "authors": [
                "Arghya Pratihar",
                "Arnab Seal",
                "Swagatam Das",
                "Inesh Chattopadhyay"
            ],
            "arxiv_id": "2512.11448v1",
            "summary": "Clustering is a fundamental unsupervised learning task for uncovering patterns in data. While Gaussian Blurring Mean Shift (GBMS) has proven effective for identifying arbitrarily shaped clusters in Euclidean space, it struggles with datasets exhibiting hierarchical or tree-like structures. In this work, we introduce HypeGBMS, a novel extension of GBMS to hyperbolic space. Our method replaces Euclidean computations with hyperbolic distances and employs Möbius-weighted means to ensure that all updates remain consistent with the geometry of the space. HypeGBMS effectively captures latent hierarchies while retaining the density-seeking behavior of GBMS. We provide theoretical insights into convergence and computational complexity, along with empirical results that demonstrate improved clustering quality in hierarchical datasets. This work bridges classical mean-shift clustering and hyperbolic representation learning, offering a principled approach to density-based clustering in curved spaces. Extensive experimental evaluations on $11$ real-world datasets demonstrate that HypeGBMS significantly outperforms conventional mean-shift clustering methods in non-Euclidean settings, underscoring its robustness and effectiveness.",
            "headline_zh": "提出HypeGBMS以解决层次结构数据在双曲空间中的聚类问题",
            "intro_zh": [
                "核心问题：传统GBMS在欧氏空间处理层次结构数据效果不佳",
                "方法要点：扩展GBMS至双曲空间，使用双曲距离和Möbius加权均值",
                "实验或效果：在11个真实数据集上验证，显著优于传统方法"
            ],
            "tags_zh": [
                "双曲空间聚类",
                "均值漂移算法",
                "层次结构数据",
                "密度估计",
                "非欧几何"
            ],
            "_index": 102
        },
        {
            "title": "YawDD+: Frame-level Annotations for Accurate Yawn Prediction",
            "authors": [
                "Ahmed Mujtaba",
                "Gleb Radchenko",
                "Marc Masana",
                "Radu Prodan"
            ],
            "arxiv_id": "2512.11446v1",
            "summary": "Driver fatigue remains a leading cause of road accidents, with 24\\% of crashes involving drowsy drivers. While yawning serves as an early behavioral indicator of fatigue, existing machine learning approaches face significant challenges due to video-annotated datasets that introduce systematic noise from coarse temporal annotations. We develop a semi-automated labeling pipeline with human-in-the-loop verification, which we apply to YawDD, enabling more accurate model training. Training the established MNasNet classifier and YOLOv11 detector architectures on YawDD+ improves frame accuracy by up to 6\\% and mAP by 5\\% over video-level supervision, achieving 99.34\\% classification accuracy and 95.69\\% detection mAP. The resulting approach deliver up to 59.8 FPS on edge AI hardware (NVIDIA Jetson Nano), confirming that enhanced data quality alone supports on-device yawning monitoring without server-side computation.",
            "headline_zh": "提出YawDD+数据集以解决驾驶员疲劳监测中视频级标注噪声问题，提升打哈欠预测准确性。",
            "intro_zh": [
                "核心问题：驾驶员疲劳导致事故，现有视频级标注数据集引入系统噪声，影响模型训练准确性。",
                "方法要点：开发半自动标注流程，结合人工验证，生成帧级标注的YawDD+数据集，用于改进模型训练。",
                "实验或效果：在YawDD+上训练MNasNet和YOLOv11，帧准确率提升6%，mAP提升5%，达到99.34%分类准确率和95.69%检测mAP，边缘硬件实现59.8 FPS。"
            ],
            "tags_zh": [
                "驾驶员疲劳监测",
                "打哈欠预测",
                "帧级标注",
                "半自动标注",
                "边缘AI硬件",
                "视频分析"
            ],
            "_index": 103
        },
        {
            "title": "Flowception: Temporally Expansive Flow Matching for Video Generation",
            "authors": [
                "Tariq Berrada Ifriqi",
                "John Nguyen",
                "Karteek Alahari",
                "Jakob Verbeek",
                "Ricky T. Q. Chen"
            ],
            "arxiv_id": "2512.11438v1",
            "summary": "We present Flowception, a novel non-autoregressive and variable-length video generation framework. Flowception learns a probability path that interleaves discrete frame insertions with continuous frame denoising. Compared to autoregressive methods, Flowception alleviates error accumulation/drift as the frame insertion mechanism during sampling serves as an efficient compression mechanism to handle long-term context. Compared to full-sequence flows, our method reduces FLOPs for training three-fold, while also being more amenable to local attention variants, and allowing to learn the length of videos jointly with their content. Quantitative experimental results show improved FVD and VBench metrics over autoregressive and full-sequence baselines, which is further validated with qualitative results. Finally, by learning to insert and denoise frames in a sequence, Flowception seamlessly integrates different tasks such as image-to-video generation and video interpolation.",
            "headline_zh": "提出Flowception框架，通过交织离散帧插入与连续帧去噪实现非自回归变长视频生成。",
            "intro_zh": [
                "核心问题：解决自回归方法中的误差累积/漂移问题，以及全序列流方法的高计算成本。",
                "方法要点：学习概率路径，结合离散帧插入作为压缩机制，减少训练FLOPs三倍，并支持联合学习视频长度与内容。",
                "实验或效果：在FVD和VBench指标上优于基线，支持图像到视频生成和视频插值等任务。"
            ],
            "tags_zh": [
                "视频生成",
                "非自回归模型",
                "流匹配",
                "变长序列",
                "帧插入去噪"
            ],
            "_index": 104
        },
        {
            "title": "Back to the Baseline: Examining Baseline Effects on Explainability Metrics",
            "authors": [
                "Agustin Martin Picard",
                "Thibaut Boissin",
                "Varshini Subhash",
                "Rémi Cadène",
                "Thomas Fel"
            ],
            "arxiv_id": "2512.11433v1",
            "summary": "Attribution methods are among the most prevalent techniques in Explainable Artificial Intelligence (XAI) and are usually evaluated and compared using Fidelity metrics, with Insertion and Deletion being the most popular. These metrics rely on a baseline function to alter the pixels of the input image that the attribution map deems most important. In this work, we highlight a critical problem with these metrics: the choice of a given baseline will inevitably favour certain attribution methods over others. More concerningly, even a simple linear model with commonly used baselines contradicts itself by designating different optimal methods. A question then arises: which baseline should we use? We propose to study this problem through two desirable properties of a baseline: (i) that it removes information and (ii) that it does not produce overly out-of-distribution (OOD) images. We first show that none of the tested baselines satisfy both criteria, and there appears to be a trade-off among current baselines: either they remove information or they produce a sequence of OOD images. Finally, we introduce a novel baseline by leveraging recent work in feature visualisation to artificially produce a model-dependent baseline that removes information without being overly OOD, thus improving on the trade-off when compared to other existing baselines. Our code is available at https://github.com/deel-ai-papers/Back-to-the-Baseline",
            "headline_zh": "提出模型依赖基线以解决可解释性评估中基线选择偏差问题",
            "intro_zh": [
                "揭示基线选择对归因方法评估的偏差影响，导致不同基线得出矛盾最优方法",
                "提出基线应具备移除信息且不过度偏离分布的双重属性，但现有基线存在权衡",
                "引入基于特征可视化的模型依赖基线，在移除信息与分布偏移间取得更好平衡"
            ],
            "tags_zh": [
                "可解释人工智能",
                "归因方法",
                "基线选择",
                "评估指标",
                "特征可视化"
            ],
            "_index": 105
        },
        {
            "title": "AgentBalance: Backbone-then-Topology Design for Cost-Effective Multi-Agent Systems under Budget Constraints",
            "authors": [
                "Shuowei Cai",
                "Yansong Ning",
                "Hao Liu"
            ],
            "arxiv_id": "2512.11426v1",
            "summary": "Large Language Model (LLM)-based multi-agent systems (MAS) are becoming indispensable building blocks for web-scale applications such as web search, social network analytics, and online customer support, where cost-effectiveness is increasingly the primary constraint for large-scale deployment. While recent work improves MAS cost-effectiveness by shaping inter-agent communication topologies and selecting agent backbones, it rarely models and optimizes under explicit token-cost and latency budgets that reflect deployment constraints. This often leads to topology-first designs and suboptimal cost-effectiveness when budgets are binding. We present AgentBalance, a framework for constructing cost-effective MAS under explicit token-cost and latency budgets via a backbone-then-topology design. AgentBalance first performs backbone-oriented agent generation, constructing agents with heterogeneous backbones through LLM pool construction, pool selection, and role-backbone matching. It then performs adaptive MAS topology generation, guiding inter-agent communication via agent representation learning, gating, and latency-aware topology synthesis. Experiments on benchmarks with 14 candidate LLM backbones show that AgentBalance achieves up to 10% and 22% performance gains under matched token-cost and latency budgets, respectively, and yields strong AUC on performance-versus-budget curves across benchmarks. AgentBalance also functions as a plug-in for existing MAS, improving performance under the same token-cost and latency constraints, and it generalizes well to unseen LLMs for practical, budget-aware deployment. Code: https://github.com/usail-hkust/AgentBalance",
            "headline_zh": "提出AgentBalance框架，通过先骨干后拓扑设计，在预算约束下构建成本效益高的多智能体系统。",
            "intro_zh": [
                "核心问题：现有多智能体系统在显式令牌成本和延迟预算下设计不足，导致成本效益低。",
                "方法要点：先基于LLM池构建异构骨干智能体，再通过表示学习和延迟感知合成自适应通信拓扑。",
                "实验效果：在匹配预算下性能提升达10%和22%，并作为插件提升现有系统性能。"
            ],
            "tags_zh": [
                "多智能体系统",
                "成本效益优化",
                "预算约束",
                "骨干选择",
                "拓扑设计",
                "延迟感知"
            ],
            "_index": 106
        },
        {
            "title": "JoyAvatar: Real-time and Infinite Audio-Driven Avatar Generation with Autoregressive Diffusion",
            "authors": [
                "Chaochao Li",
                "Ruikui Wang",
                "Liangbo Zhou",
                "Jinheng Feng",
                "Huaishao Luo",
                "Huan Zhang",
                "Youzheng Wu",
                "Xiaodong He"
            ],
            "arxiv_id": "2512.11423v1",
            "summary": "Existing DiT-based audio-driven avatar generation methods have achieved considerable progress, yet their broader application is constrained by limitations such as high computational overhead and the inability to synthesize long-duration videos. Autoregressive methods address this problem by applying block-wise autoregressive diffusion methods. However, these methods suffer from the problem of error accumulation and quality degradation. To address this, we propose JoyAvatar, an audio-driven autoregressive model capable of real-time inference and infinite-length video generation with the following contributions: (1) Progressive Step Bootstrapping (PSB), which allocates more denoising steps to initial frames to stabilize generation and reduce error accumulation; (2) Motion Condition Injection (MCI), enhancing temporal coherence by injecting noise-corrupted previous frames as motion condition; and (3) Unbounded RoPE via Cache-Resetting (URCR), enabling infinite-length generation through dynamic positional encoding. Our 1.3B-parameter causal model achieves 16 FPS on a single GPU and achieves competitive results in visual quality, temporal consistency, and lip synchronization.",
            "headline_zh": "提出JoyAvatar以解决音频驱动虚拟人生成中的实时性和长视频合成问题",
            "intro_zh": [
                "现有DiT方法计算开销高且无法生成长视频，自回归方法存在误差累积和质量下降问题",
                "引入渐进步引导、运动条件注入和缓存重置无界RoPE，提升稳定性和时间一致性",
                "1.3B参数模型在单GPU上实现16 FPS实时推理，视觉质量、时间一致性和唇同步效果竞争性强"
            ],
            "tags_zh": [
                "音频驱动虚拟人生成",
                "自回归扩散模型",
                "实时推理",
                "长视频合成",
                "时间一致性",
                "唇同步"
            ],
            "_index": 107
        },
        {
            "title": "Towards Trustworthy Multi-Turn LLM Agents via Behavioral Guidance",
            "authors": [
                "Gonca Gürsun"
            ],
            "arxiv_id": "2512.11421v1",
            "summary": "Large Language Models demonstrate strong reasoning and generation abilities, yet their behavior in multi-turn tasks often lacks reliability and verifiability. We present a task completion framework that enables LLM-based agents to act under explicit behavioral guidance in environments described by reinforcement learning formalisms with defined observation, action, and reward signals.\n  The framework integrates three components: a lightweight task profiler that selects reasoning and generation strategies, a reasoning module that learns verifiable observation - action mappings, and a generation module that enforces constraint-compliant outputs through validation or deterministic synthesis. We show that as the agent interacts with the environment, these components co-evolve, yielding trustworthy behavior.",
            "headline_zh": "提出基于强化学习形式的多轮LLM代理框架，通过行为指导提升可信度",
            "intro_zh": [
                "核心问题：多轮任务中LLM行为缺乏可靠性和可验证性",
                "方法要点：集成任务分析器、推理模块和生成模块，实现行为指导与约束",
                "实验或效果：组件协同进化，代理在交互中展现可信行为"
            ],
            "tags_zh": [
                "多轮LLM代理",
                "行为指导",
                "强化学习框架",
                "可验证推理",
                "约束生成"
            ],
            "_index": 108
        },
        {
            "title": "Emergence of Nonequilibrium Latent Cycles in Unsupervised Generative Modeling",
            "authors": [
                "Marco Baiesi",
                "Alberto Rosso"
            ],
            "arxiv_id": "2512.11415v1",
            "summary": "We show that nonequilibrium dynamics can play a constructive role in unsupervised machine learning by inducing the spontaneous emergence of latent-state cycles. We introduce a model in which visible and hidden variables interact through two independently parametrized transition matrices, defining a Markov chain whose steady state is intrinsically out of equilibrium. Likelihood maximization drives this system toward nonequilibrium steady states with finite entropy production, reduced self-transition probabilities, and persistent probability currents in the latent space. These cycles are not imposed by the architecture but arise from training, and models that develop them avoid the low-log-likelihood regime associated with nearly reversible dynamics while more faithfully reproducing the empirical distribution of data classes. Compared with equilibrium approaches such as restricted Boltzmann machines, our model breaks the detailed balance between the forward and backward conditional transitions and relies on a log-likelihood gradient that depends explicitly on the last two steps of the Markov chain. Hence, this exploration of the interface between nonequilibrium statistical physics and modern machine learning suggests that introducing irreversibility into latent-variable models can enhance generative performance.",
            "headline_zh": "提出非平衡隐变量循环模型以增强无监督生成性能",
            "intro_zh": [
                "核心问题：传统平衡模型如受限玻尔兹曼机在生成任务中可能因可逆动态导致低似然。",
                "方法要点：引入双参数化转移矩阵的马尔可夫链，打破详细平衡，驱动隐空间形成自发循环。",
                "实验或效果：模型通过最大化似然实现非平衡稳态，提高数据类分布拟合度，避免低似然区域。"
            ],
            "tags_zh": [
                "非平衡统计物理",
                "无监督生成模型",
                "隐变量循环",
                "马尔可夫链",
                "熵产生"
            ],
            "_index": 109
        },
        {
            "title": "Task-Specific Sparse Feature Masks for Molecular Toxicity Prediction with Chemical Language Models",
            "authors": [
                "Kwun Sy Lee",
                "Jiawei Chen",
                "Fuk Sheng Ford Chung",
                "Tianyu Zhao",
                "Zhenyuan Chen",
                "Debby D. Wang"
            ],
            "arxiv_id": "2512.11412v1",
            "summary": "Reliable in silico molecular toxicity prediction is a cornerstone of modern drug discovery, offering a scalable alternative to experimental screening. However, the black-box nature of state-of-the-art models remains a significant barrier to adoption, as high-stakes safety decisions demand verifiable structural insights alongside predictive performance. To address this, we propose a novel multi-task learning (MTL) framework designed to jointly enhance accuracy and interpretability. Our architecture integrates a shared chemical language model with task-specific attention modules. By imposing an L1 sparsity penalty on these modules, the framework is constrained to focus on a minimal set of salient molecular fragments for each distinct toxicity endpoint. The resulting framework is trained end-to-end and is readily adaptable to various transformer-based backbones. Evaluated on the ClinTox, SIDER, and Tox21 benchmark datasets, our approach consistently outperforms both single-task and standard MTL baselines. Crucially, the sparse attention weights provide chemically intuitive visualizations that reveal the specific fragments influencing predictions, thereby enhancing insight into the model's decision-making process.",
            "headline_zh": "提出任务特定稀疏特征掩码的多任务学习框架，以提升分子毒性预测的准确性和可解释性。",
            "intro_zh": [
                "核心问题：现有分子毒性预测模型为黑盒，缺乏可验证的结构洞察，阻碍高安全决策应用。",
                "方法要点：结合共享化学语言模型与任务特定注意力模块，通过L1稀疏惩罚聚焦关键分子片段。",
                "实验或效果：在ClinTox、SIDER和Tox21数据集上优于单任务和标准多任务基线，并提供化学直观可视化。"
            ],
            "tags_zh": [
                "分子毒性预测",
                "多任务学习",
                "化学语言模型",
                "稀疏注意力",
                "可解释性",
                "药物发现"
            ],
            "_index": 110
        },
        {
            "title": "Sliced ReLU attention: Quasi-linear contextual expressivity via sorting",
            "authors": [
                "Siwan Boufadène",
                "François-Xavier Vialard"
            ],
            "arxiv_id": "2512.11411v1",
            "summary": "We introduce sliced ReLU attention, a new attention mechanism that departs structurally from both softmax and ReLU-based alternatives. Instead of applying a nonlinearity to pairwise dot products, we operate on one-dimensional projections of key--query differences and leverage sorting to obtain quasi-linear complexity. This construction yields a differentiable, non-symmetric kernel that can be computed in O(n log(n)) through a sorting procedure, making it suitable for very long contexts. Beyond computational benefits, the model retains strong theoretical expressive power: we establish two in-context expressivity results, previously known for softmax attention, showing that sliced ReLU attention preserves the ability to perform nontrivial sequence-to-sequence disentangling tasks and satisfies a contextual universal approximation property. Finally, we illustrate the potential practical interest of this kernel in small-scale experiments.",
            "headline_zh": "提出切片ReLU注意力机制，通过排序实现准线性复杂度，适用于长上下文序列处理。",
            "intro_zh": [
                "核心问题：传统注意力机制如softmax和ReLU变体在长上下文序列中计算复杂度高，影响效率。",
                "方法要点：基于键-查询差异的一维投影，利用排序操作构建可微非对称核，实现O(n log(n))复杂度。",
                "实验或效果：理论证明保持强表达力，支持序列解耦任务和上下文通用逼近性质，小规模实验展示实用潜力。"
            ],
            "tags_zh": [
                "注意力机制",
                "长上下文处理",
                "准线性复杂度",
                "排序算法",
                "可微核",
                "序列解耦"
            ],
            "_index": 111
        },
        {
            "title": "REMODEL-LLM: Transforming C code to Java using LLMs",
            "authors": [
                "Aryan Gupta",
                "Y. Raghu Reddy"
            ],
            "arxiv_id": "2512.11402v1",
            "summary": "The automated translation of C code to Java code is a notoriously difficult task, fraught with challenges stemming from fundamental paradigm shifts (procedural vs. Object Oriented), memory models (manual pointers vs. Garbage Collection), and incompatible data types. This paper investigates the efficacy of 19 small, quantized LLMs (under 20 billion parameters) for the C to Java translation task. We use a novel, hybrid pipeline that leverages Abstract Syntax Trees (ASTs) for semantic decomposition and employs a highly constrained, rule based prompting strategy. The results are stark: a clear multi tiered performance divide emerged. The vast majority of models (Tier 3, e.g., llama3.1, gemma3, starcoder2) failed 100\\% of the tests, proving incapable of generating even basic, runnable Java boilerplate. A small middle tier (Tier 2, e.g., mistral-nemo and mistral) produced runnable code but was plagued by dangerous semantic failures and wrong translations. Only three models (Tier 1: phi4, deepseek-coder-v2, codeqwen) proved viable, passing over 50\\% of the test suite. Even these top models failed on the most complex C concepts, such as function pointers, sizeof, and enum logic, revealing a hard ceiling for the reasoning capabilities of current quantized models.",
            "headline_zh": "提出基于AST和规则提示的混合管道，评估小量化LLM在C到Java代码翻译中的性能",
            "intro_zh": [
                "研究C到Java代码自动翻译的挑战，涉及范式、内存模型和数据类型差异",
                "采用AST语义分解和约束规则提示的混合方法，测试19个小量化LLM",
                "结果显示仅三个模型通过超50%测试，但复杂概念如函数指针仍失败"
            ],
            "tags_zh": [
                "代码翻译",
                "量化语言模型",
                "抽象语法树",
                "规则提示",
                "C到Java转换",
                "性能评估"
            ],
            "_index": 112
        },
        {
            "title": "Collaborative Reconstruction and Repair for Multi-class Industrial Anomaly Detection",
            "authors": [
                "Qishan Wang",
                "Haofeng Wang",
                "Shuyong Gao",
                "Jia Guo",
                "Li Xiong",
                "Jiaqi Li",
                "Dengxuan Bai",
                "Wenqiang Zhang"
            ],
            "arxiv_id": "2512.11401v1",
            "summary": "Industrial anomaly detection is a challenging open-set task that aims to identify unknown anomalous patterns deviating from normal data distribution. To avoid the significant memory consumption and limited generalizability brought by building separate models per class, we focus on developing a unified framework for multi-class anomaly detection. However, under this challenging setting, conventional reconstruction-based networks often suffer from an identity mapping problem, where they directly replicate input features regardless of whether they are normal or anomalous, resulting in detection failures. To address this issue, this study proposes a novel framework termed Collaborative Reconstruction and Repair (CRR), which transforms the reconstruction to repairation. First, we optimize the decoder to reconstruct normal samples while repairing synthesized anomalies. Consequently, it generates distinct representations for anomalous regions and similar representations for normal areas compared to the encoder's output. Second, we implement feature-level random masking to ensure that the representations from decoder contain sufficient local information. Finally, to minimize detection errors arising from the discrepancies between feature representations from the encoder and decoder, we train a segmentation network supervised by synthetic anomaly masks, thereby enhancing localization performance. Extensive experiments on industrial datasets that CRR effectively mitigates the identity mapping issue and achieves state-of-the-art performance in multi-class industrial anomaly detection.",
            "headline_zh": "提出协作重建与修复框架以解决多类工业异常检测中的身份映射问题",
            "intro_zh": [
                "核心问题：多类统一框架下重建网络易发生身份映射，导致异常检测失败。",
                "方法要点：通过重建正常样本并修复合成异常，结合特征掩码和分割网络优化表示差异。",
                "实验或效果：在工业数据集上验证有效缓解身份映射，实现先进检测性能。"
            ],
            "tags_zh": [
                "工业异常检测",
                "多类统一框架",
                "协作重建与修复",
                "身份映射问题",
                "特征掩码",
                "异常定位"
            ],
            "_index": 113
        },
        {
            "title": "Minimal Clips, Maximum Salience: Long Video Summarization via Key Moment Extraction",
            "authors": [
                "Galann Pennec",
                "Zhengyuan Liu",
                "Nicholas Asher",
                "Philippe Muller",
                "Nancy F. Chen"
            ],
            "arxiv_id": "2512.11399v1",
            "summary": "Vision-Language Models (VLMs) are able to process increasingly longer videos. Yet, important visual information is easily lost throughout the entire context and missed by VLMs. Also, it is important to design tools that enable cost-effective analysis of lengthy video content. In this paper, we propose a clip selection method that targets key video moments to be included in a multimodal summary. We divide the video into short clips and generate compact visual descriptions of each using a lightweight video captioning model. These are then passed to a large language model (LLM), which selects the K clips containing the most relevant visual information for a multimodal summary. We evaluate our approach on reference clips for the task, automatically derived from full human-annotated screenplays and summaries in the MovieSum dataset. We further show that these reference clips (less than 6% of the movie) are sufficient to build a complete multimodal summary of the movies in MovieSum. Using our clip selection method, we achieve a summarization performance close to that of these reference clips while capturing substantially more relevant video information than random clip selection. Importantly, we maintain low computational cost by relying on a lightweight captioning model.",
            "headline_zh": "提出基于关键片段提取的长视频多模态摘要方法，以低成本捕获重要视觉信息。",
            "intro_zh": [
                "问题：长视频中视觉信息易丢失，需低成本分析工具。",
                "方法：使用轻量视频描述模型生成片段描述，LLM选择关键片段。",
                "效果：在MovieSum数据集上接近参考片段性能，计算成本低。"
            ],
            "tags_zh": [
                "长视频摘要",
                "关键片段提取",
                "视觉语言模型",
                "多模态摘要",
                "低成本分析"
            ],
            "_index": 114
        },
        {
            "title": "FlowDC: Flow-Based Decoupling-Decay for Complex Image Editing",
            "authors": [
                "Yilei Jiang",
                "Zhen Wang",
                "Yanghao Wang",
                "Jun Yu",
                "Yueting Zhuang",
                "Jun Xiao",
                "Long Chen"
            ],
            "arxiv_id": "2512.11395v1",
            "summary": "With the surge of pre-trained text-to-image flow matching models, text-based image editing performance has gained remarkable improvement, especially for \\underline{simple editing} that only contains a single editing target. To satisfy the exploding editing requirements, the \\underline{complex editing} which contains multiple editing targets has posed as a more challenging task. However, current complex editing solutions: single-round and multi-round editing are limited by long text following and cumulative inconsistency, respectively. Thus, they struggle to strike a balance between semantic alignment and source consistency. In this paper, we propose \\textbf{FlowDC}, which decouples the complex editing into multiple sub-editing effects and superposes them in parallel during the editing process. Meanwhile, we observed that the velocity quantity that is orthogonal to the editing displacement harms the source structure preserving. Thus, we decompose the velocity and decay the orthogonal part for better source consistency. To evaluate the effectiveness of complex editing settings, we construct a complex editing benchmark: Complex-PIE-Bench. On two benchmarks, FlowDC shows superior results compared with existing methods. We also detail the ablations of our module designs.",
            "headline_zh": "提出FlowDC方法，通过解耦-衰减机制解决复杂图像编辑中的语义对齐与源一致性平衡问题。",
            "intro_zh": [
                "核心问题：现有复杂图像编辑方法在单轮和多轮编辑中面临长文本跟随和累积不一致性挑战，难以平衡语义对齐与源一致性。",
                "方法要点：将复杂编辑解耦为多个子编辑效果，并行叠加；分解速度并衰减正交部分以提升源结构保持。",
                "实验或效果：构建Complex-PIE-Bench基准，FlowDC在多个基准上优于现有方法，并通过消融实验验证模块设计。"
            ],
            "tags_zh": [
                "复杂图像编辑",
                "流匹配模型",
                "解耦编辑",
                "源一致性",
                "文本到图像编辑",
                "基准构建"
            ],
            "_index": 115
        },
        {
            "title": "The N-Body Problem: Parallel Execution from Single-Person Egocentric Video",
            "authors": [
                "Zhifan Zhu",
                "Yifei Huang",
                "Yoichi Sato",
                "Dima Damen"
            ],
            "arxiv_id": "2512.11393v1",
            "summary": "Humans can intuitively parallelise complex activities, but can a model learn this from observing a single person? Given one egocentric video, we introduce the N-Body Problem: how N individuals, can hypothetically perform the same set of tasks observed in this video. The goal is to maximise speed-up, but naive assignment of video segments to individuals often violates real-world constraints, leading to physically impossible scenarios like two people using the same object or occupying the same space. To address this, we formalise the N-Body Problem and propose a suite of metrics to evaluate both performance (speed-up, task coverage) and feasibility (spatial collisions, object conflicts and causal constraints). We then introduce a structured prompting strategy that guides a Vision-Language Model (VLM) to reason about the 3D environment, object usage, and temporal dependencies to produce a viable parallel execution. On 100 videos from EPIC-Kitchens and HD-EPIC, our method for N = 2 boosts action coverage by 45% over a baseline prompt for Gemini 2.5 Pro, while simultaneously slashing collision rates, object and causal conflicts by 55%, 45% and 55% respectively.",
            "headline_zh": "提出N-Body问题与结构化提示策略，从单人第一人称视频学习多人并行执行任务",
            "intro_zh": [
                "核心问题：如何从单人第一人称视频中学习多人并行执行任务，最大化加速但避免物理冲突",
                "方法要点：使用结构化提示引导视觉语言模型推理3D环境、物体使用和时序依赖，生成可行并行执行方案",
                "实验或效果：在EPIC-Kitchens和HD-EPIC数据集上，N=2时动作覆盖率提升45%，冲突率显著降低"
            ],
            "tags_zh": [
                "第一人称视频理解",
                "并行任务执行",
                "视觉语言模型",
                "物理约束推理",
                "结构化提示"
            ],
            "_index": 116
        },
        {
            "title": "Bhargava Cube--Inspired Quadratic Regularization for Structured Neural Embeddings",
            "authors": [
                "S Sairam",
                "Prateek P Kulkarni"
            ],
            "arxiv_id": "2512.11392v1",
            "summary": "We present a novel approach to neural representation learning that incorporates algebraic constraints inspired by Bhargava cubes from number theory. Traditional deep learning methods learn representations in unstructured latent spaces lacking interpretability and mathematical consistency. Our framework maps input data to constrained 3-dimensional latent spaces where embeddings are regularized to satisfy learned quadratic relationships derived from Bhargava's combinatorial structures. The architecture employs a differentiable auxiliary loss function operating independently of classification objectives, guiding models toward mathematically structured representations. We evaluate on MNIST, achieving 99.46% accuracy while producing interpretable 3D embeddings that naturally cluster by digit class and satisfy learned quadratic constraints. Unlike existing manifold learning approaches requiring explicit geometric supervision, our method imposes weak algebraic priors through differentiable constraints, ensuring compatibility with standard optimization. This represents the first application of number-theoretic constructs to neural representation learning, establishing a foundation for incorporating structured mathematical priors in neural networks.",
            "headline_zh": "提出基于Bhargava立方体的二次正则化方法，用于结构化神经嵌入学习",
            "intro_zh": [
                "传统深度学习的潜在空间缺乏可解释性和数学一致性，导致表示学习受限",
                "通过引入Bhargava立方体启发的代数约束，在三维潜在空间中正则化嵌入以满足二次关系",
                "在MNIST上实现99.46%准确率，生成可解释的聚类嵌入并兼容标准优化"
            ],
            "tags_zh": [
                "结构化表示学习",
                "二次正则化",
                "Bhargava立方体",
                "可解释嵌入",
                "代数约束",
                "神经嵌入"
            ],
            "_index": 117
        },
        {
            "title": "Mitigating the Safety Alignment Tax with Null-Space Constrained Policy Optimization",
            "authors": [
                "Yifan Niu",
                "Han Xiao",
                "Dongyi Liu",
                "Nuo Chen",
                "Jia Li"
            ],
            "arxiv_id": "2512.11391v1",
            "summary": "As Large Language Models (LLMs) are increasingly deployed in real-world applications, it is important to ensure their behaviors align with human values, societal norms, and ethical principles. However, safety alignment under Reinforcement Learning (RL) often suffers from forgetting learned general abilities, which is also known as the alignment tax. To address this issue, we introduce Null-Space constrained Policy Optimization (NSPO), a novel RL framework for LLM safety alignment while preserving their core abilities. The safety policy gradients are geometrically projected into the null space of general tasks, thereby mitigating the safety alignment tax. In addition, we theoretically prove that NSPO preserves the model's original core capabilities, while still guaranteeing a descent direction for effective safety alignment. Extensive experiments demonstrate that NSPO outperforms existing methods by a large margin, achieving state-of-the-art safety performance without sacrificing accuracy on general tasks, including math, code, and instruction-following tasks. Notably, NSPO is data-efficient and only requires 40% of public human-annotated safety data from PKU-SafeRLHF to achieve promising safety performance, without a large amount of mixed general tasks data in existing alignment methods.",
            "headline_zh": "提出零空间约束策略优化以缓解大语言模型安全对齐中的能力遗忘问题",
            "intro_zh": [
                "核心问题：强化学习安全对齐导致大语言模型遗忘通用能力，即对齐税",
                "方法要点：将安全策略梯度几何投影到通用任务的零空间，以保留核心能力",
                "实验或效果：在数学、代码等任务上实现最优安全性能，且数据效率高"
            ],
            "tags_zh": [
                "大语言模型",
                "安全对齐",
                "强化学习",
                "零空间投影",
                "策略优化",
                "对齐税"
            ],
            "_index": 118
        },
        {
            "title": "Out-of-Distribution Segmentation via Wasserstein-Based Evidential Uncertainty",
            "authors": [
                "Arnold Brosch",
                "Abdelrahman Eldesokey",
                "Michael Felsberg",
                "Kira Maag"
            ],
            "arxiv_id": "2512.11373v1",
            "summary": "Deep neural networks achieve superior performance in semantic segmentation, but are limited to a predefined set of classes, which leads to failures when they encounter unknown objects in open-world scenarios. Recognizing and segmenting these out-of-distribution (OOD) objects is crucial for safety-critical applications such as automated driving. In this work, we present an evidence segmentation framework using a Wasserstein loss, which captures distributional distances while respecting the probability simplex geometry. Combined with Kullback-Leibler regularization and Dice structural consistency terms, our approach leads to improved OOD segmentation performance compared to uncertainty-based approaches.",
            "headline_zh": "提出基于Wasserstein损失的证据分割框架，以改进开放世界场景中的未知物体分割性能。",
            "intro_zh": [
                "核心问题：深度神经网络在语义分割中局限于预定义类别，遇到未知物体时易失败，影响自动驾驶等安全关键应用。",
                "方法要点：使用Wasserstein损失捕捉分布距离并尊重概率单纯形几何，结合Kullback-Leibler正则化和Dice结构一致性项。",
                "实验或效果：相比基于不确定性的方法，本方法提升了未知物体分割性能，具体指标未知。"
            ],
            "tags_zh": [
                "未知物体分割",
                "证据分割",
                "Wasserstein损失",
                "Kullback-Leibler正则化",
                "Dice损失",
                "开放世界场景"
            ],
            "_index": 119
        },
        {
            "title": "Assisted Refinement Network Based on Channel Information Interaction for Camouflaged and Salient Object Detection",
            "authors": [
                "Kuan Wang",
                "Yanjun Qin",
                "Mengge Lu",
                "Liejun Wang",
                "Xiaoming Tao"
            ],
            "arxiv_id": "2512.11369v1",
            "summary": "Camouflaged Object Detection (COD) stands as a significant challenge in computer vision, dedicated to identifying and segmenting objects visually highly integrated with their backgrounds. Current mainstream methods have made progress in cross-layer feature fusion, but two critical issues persist during the decoding stage. The first is insufficient cross-channel information interaction within the same-layer features, limiting feature expressiveness. The second is the inability to effectively co-model boundary and region information, making it difficult to accurately reconstruct complete regions and sharp boundaries of objects. To address the first issue, we propose the Channel Information Interaction Module (CIIM), which introduces a horizontal-vertical integration mechanism in the channel dimension. This module performs feature reorganization and interaction across channels to effectively capture complementary cross-channel information. To address the second issue, we construct a collaborative decoding architecture guided by prior knowledge. This architecture generates boundary priors and object localization maps through Boundary Extraction (BE) and Region Extraction (RE) modules, then employs hybrid attention to collaboratively calibrate decoded features, effectively overcoming semantic ambiguity and imprecise boundaries. Additionally, the Multi-scale Enhancement (MSE) module enriches contextual feature representations. Extensive experiments on four COD benchmark datasets validate the effectiveness and state-of-the-art performance of the proposed model. We further transferred our model to the Salient Object Detection (SOD) task and demonstrated its adaptability across downstream tasks, including polyp segmentation, transparent object detection, and industrial and road defect detection. Code and experimental results are publicly available at: https://github.com/akuan1234/ARNet-v2.",
            "headline_zh": "提出基于通道信息交互的辅助精炼网络，以解决伪装与显著目标检测中的特征表达与边界重建问题。",
            "intro_zh": [
                "核心问题：解码阶段跨通道信息交互不足，边界与区域信息协同建模困难。",
                "方法要点：引入通道信息交互模块和先验知识引导的协同解码架构，增强特征表达。",
                "实验或效果：在多个基准数据集上验证有效性，并展示下游任务适应性。"
            ],
            "tags_zh": [
                "伪装目标检测",
                "显著目标检测",
                "通道信息交互",
                "协同解码",
                "多尺度增强"
            ],
            "_index": 120
        },
        {
            "title": "Maritime object classification with SAR imagery using quantum kernel methods",
            "authors": [
                "John Tanner",
                "Nicholas Davies",
                "Pascal Elahi",
                "Casey R. Myers",
                "Du Huynh",
                "Wei Liu",
                "Mark Reynolds",
                "Jingbo Wang"
            ],
            "arxiv_id": "2512.11367v1",
            "summary": "Illegal, unreported, and unregulated (IUU) fishing causes global economic losses of \\$10-25 billion annually and undermines marine sustainability and governance. Synthetic Aperture Radar (SAR) provides reliable maritime surveillance under all weather and lighting conditions, but classifying small maritime objects in SAR imagery remains challenging. We investigate quantum machine learning for this task, focusing on Quantum Kernel Methods (QKMs) applied to real and complex SAR chips extracted from the SARFish dataset. We tackle two binary classification problems, the first for distinguishing vessels from non-vessels, and the second for distinguishing fishing vessels from other types of vessels. We compare QKMs applied to real and complex SAR chips against classical Laplacian, RBF, and linear kernels applied to real SAR chips. Using noiseless numerical simulations of the quantum kernels, we find that QKMs are capable of obtaining equal or better performance than the classical kernel on these tasks in the best case, but do not demonstrate a clear advantage for the complex SAR data. This work presents the first application of QKMs to maritime classification in SAR imagery and offers insight into the potential and current limitations of quantum-enhanced learning for maritime surveillance.",
            "headline_zh": "提出量子核方法用于SAR图像海事目标分类，评估其在船舶与非船舶及渔船分类中的性能。",
            "intro_zh": [
                "核心问题：SAR图像中非法捕鱼等海事目标分类困难，需全天候监测。",
                "方法要点：应用量子核方法处理SARFish数据集中的实数和复数SAR芯片。",
                "实验或效果：量子核方法在无噪声模拟中性能与经典核相当或更优，但复数数据未显优势。"
            ],
            "tags_zh": [
                "量子机器学习",
                "合成孔径雷达",
                "海事目标分类",
                "量子核方法",
                "SARFish数据集"
            ],
            "_index": 121
        },
        {
            "title": "An Anatomy of Vision-Language-Action Models: From Modules to Milestones and Challenges",
            "authors": [
                "Chao Xu",
                "Suyu Zhang",
                "Yang Liu",
                "Baigui Sun",
                "Weihong Chen",
                "Bo Xu",
                "Qi Liu",
                "Juncheng Wang",
                "Shujun Wang",
                "Shan Luo",
                "Jan Peters",
                "Athanasios V. Vasilakos",
                "Stefanos Zafeiriou",
                "Jiankang Deng"
            ],
            "arxiv_id": "2512.11362v1",
            "summary": "Vision-Language-Action (VLA) models are driving a revolution in robotics, enabling machines to understand instructions and interact with the physical world. This field is exploding with new models and datasets, making it both exciting and challenging to keep pace with. This survey offers a clear and structured guide to the VLA landscape. We design it to follow the natural learning path of a researcher: we start with the basic Modules of any VLA model, trace the history through key Milestones, and then dive deep into the core Challenges that define recent research frontier. Our main contribution is a detailed breakdown of the five biggest challenges in: (1) Representation, (2) Execution, (3) Generalization, (4) Safety, and (5) Dataset and Evaluation. This structure mirrors the developmental roadmap of a generalist agent: establishing the fundamental perception-action loop, scaling capabilities across diverse embodiments and environments, and finally ensuring trustworthy deployment-all supported by the essential data infrastructure. For each of them, we review existing approaches and highlight future opportunities. We position this paper as both a foundational guide for newcomers and a strategic roadmap for experienced researchers, with the dual aim of accelerating learning and inspiring new ideas in embodied intelligence. A live version of this survey, with continuous updates, is maintained on our \\href{https://suyuz1.github.io/Survery/}{project page}.",
            "headline_zh": "综述视觉-语言-动作模型，从模块到里程碑与挑战，为机器人交互提供结构化指南",
            "intro_zh": [
                "核心问题：VLA模型在机器人领域快速发展，但模型和数据集激增导致跟踪研究进展困难",
                "方法要点：基于研究者学习路径，从基本模块、关键里程碑到核心挑战进行结构化分析",
                "实验或效果：详细分解五大挑战（表示、执行、泛化、安全、数据集与评估），并回顾现有方法及未来机会"
            ],
            "tags_zh": [
                "视觉-语言-动作模型",
                "机器人交互",
                "模块化分析",
                "研究挑战",
                "数据集评估",
                "泛化能力"
            ],
            "_index": 122
        },
        {
            "title": "Reliable Detection of Minute Targets in High-Resolution Aerial Imagery across Temporal Shifts",
            "authors": [
                "Mohammad Sadegh Gholizadeh",
                "Amir Arsalan Rezapour",
                "Hamidreza Shayegh",
                "Ehsan Pazouki"
            ],
            "arxiv_id": "2512.11360v1",
            "summary": "Efficient crop detection via Unmanned Aerial Vehicles is critical for scaling precision agriculture, yet it remains challenging due to the small scale of targets and environmental variability. This paper addresses the detection of rice seedlings in paddy fields by leveraging a Faster R-CNN architecture initialized via transfer learning. To overcome the specific difficulties of detecting minute objects in high-resolution aerial imagery, we curate a significant UAV dataset for training and rigorously evaluate the model's generalization capabilities. Specifically, we validate performance across three distinct test sets acquired at different temporal intervals, thereby assessing robustness against varying imaging conditions. Our empirical results demonstrate that transfer learning not only facilitates the rapid convergence of object detection models in agricultural contexts but also yields consistent performance despite domain shifts in image acquisition.",
            "headline_zh": "提出基于迁移学习的Faster R-CNN方法，以解决高分辨率航拍图像中水稻幼苗微小目标检测的挑战。",
            "intro_zh": [
                "核心问题：高分辨率航拍图像中微小目标检测困难，且环境变化影响模型泛化能力。",
                "方法要点：利用迁移学习初始化Faster R-CNN，并构建大规模无人机数据集进行训练。",
                "实验或效果：在三个不同时间间隔的测试集上验证，模型在域偏移下保持稳定性能。"
            ],
            "tags_zh": [
                "微小目标检测",
                "高分辨率航拍图像",
                "迁移学习",
                "Faster R-CNN",
                "农业视觉",
                "域泛化"
            ],
            "_index": 123
        },
        {
            "title": "Attacking and Securing Community Detection: A Game-Theoretic Framework",
            "authors": [
                "Yifan Niu",
                "Aochuan Chen",
                "Tingyang Xu",
                "Jia Li"
            ],
            "arxiv_id": "2512.11359v1",
            "summary": "It has been demonstrated that adversarial graphs, i.e., graphs with imperceptible perturbations, can cause deep graph models to fail on classification tasks. In this work, we extend the concept of adversarial graphs to the community detection problem, which is more challenging. We propose novel attack and defense techniques for community detection problem, with the objective of hiding targeted individuals from detection models and enhancing the robustness of community detection models, respectively. These techniques have many applications in real-world scenarios, for example, protecting personal privacy in social networks and understanding camouflage patterns in transaction networks. To simulate interactive attack and defense behaviors, we further propose a game-theoretic framework, called CD-GAME. One player is a graph attacker, while the other player is a Rayleigh Quotient defender. The CD-GAME models the mutual influence and feedback mechanisms between the attacker and the defender, revealing the dynamic evolutionary process of the game. Both players dynamically update their strategies until they reach the Nash equilibrium. Extensive experiments demonstrate the effectiveness of our proposed attack and defense methods, and both outperform existing baselines by a significant margin. Furthermore, CD-GAME provides valuable insights for understanding interactive attack and defense scenarios in community detection problems. We found that in traditional single-step attack or defense, attacker tends to employ strategies that are most effective, but are easily detected and countered by defender. When the interactive game reaches a Nash equilibrium, attacker adopts more imperceptible strategies that can still achieve satisfactory attack effectiveness even after defense.",
            "headline_zh": "提出CD-GAME博弈框架，攻击与防御社区检测以保护隐私和增强鲁棒性。",
            "intro_zh": [
                "扩展对抗图概念至社区检测，提出新攻击与防御技术。",
                "构建CD-GAME博弈框架模拟攻击者与防御者交互，动态更新策略至纳什均衡。",
                "实验显示方法显著优于基线，揭示交互场景中攻击策略的演变。"
            ],
            "tags_zh": [
                "社区检测",
                "对抗攻击",
                "博弈论",
                "隐私保护",
                "鲁棒性增强"
            ],
            "_index": 124
        },
        {
            "title": "Prior-Enhanced Gaussian Splatting for Dynamic Scene Reconstruction from Casual Video",
            "authors": [
                "Meng-Li Shih",
                "Ying-Huan Chen",
                "Yu-Lun Liu",
                "Brian Curless"
            ],
            "arxiv_id": "2512.11356v1",
            "summary": "We introduce a fully automatic pipeline for dynamic scene reconstruction from casually captured monocular RGB videos. Rather than designing a new scene representation, we enhance the priors that drive Dynamic Gaussian Splatting. Video segmentation combined with epipolar-error maps yields object-level masks that closely follow thin structures; these masks (i) guide an object-depth loss that sharpens the consistent video depth, and (ii) support skeleton-based sampling plus mask-guided re-identification to produce reliable, comprehensive 2-D tracks. Two additional objectives embed the refined priors in the reconstruction stage: a virtual-view depth loss removes floaters, and a scaffold-projection loss ties motion nodes to the tracks, preserving fine geometry and coherent motion. The resulting system surpasses previous monocular dynamic scene reconstruction methods and delivers visibly superior renderings",
            "headline_zh": "提出先验增强高斯泼溅方法，用于从随意拍摄的单目视频重建动态场景",
            "intro_zh": [
                "核心问题：从随意拍摄的单目RGB视频自动重建动态场景，需处理薄结构和运动一致性",
                "方法要点：结合视频分割与极线误差图生成对象级掩码，指导深度损失和跟踪，并引入虚拟视图深度损失和骨架投影损失",
                "实验或效果：系统超越先前单目动态场景重建方法，渲染质量显著提升"
            ],
            "tags_zh": [
                "动态场景重建",
                "高斯泼溅",
                "单目视频",
                "先验增强",
                "对象跟踪",
                "深度优化"
            ],
            "_index": 125
        },
        {
            "title": "A Multi-Mode Structured Light 3D Imaging System with Multi-Source Information Fusion for Underwater Pipeline Detection",
            "authors": [
                "Qinghan Hu",
                "Haijiang Zhu",
                "Na Sun",
                "Lei Chen",
                "Zhengqiang Fan",
                "Zhiqing Li"
            ],
            "arxiv_id": "2512.11354v1",
            "summary": "Underwater pipelines are highly susceptible to corrosion, which not only shorten their service life but also pose significant safety risks. Compared with manual inspection, the intelligent real-time imaging system for underwater pipeline detection has become a more reliable and practical solution. Among various underwater imaging techniques, structured light 3D imaging can restore the sufficient spatial detail for precise defect characterization. Therefore, this paper develops a multi-mode underwater structured light 3D imaging system for pipeline detection (UW-SLD system) based on multi-source information fusion. First, a rapid distortion correction (FDC) method is employed for efficient underwater image rectification. To overcome the challenges of extrinsic calibration among underwater sensors, a factor graph-based parameter optimization method is proposed to estimate the transformation matrix between the structured light and acoustic sensors. Furthermore, a multi-mode 3D imaging strategy is introduced to adapt to the geometric variability of underwater pipelines. Given the presence of numerous disturbances in underwater environments, a multi-source information fusion strategy and an adaptive extended Kalman filter (AEKF) are designed to ensure stable pose estimation and high-accuracy measurements. In particular, an edge detection-based ICP (ED-ICP) algorithm is proposed. This algorithm integrates pipeline edge detection network with enhanced point cloud registration to achieve robust and high-fidelity reconstruction of defect structures even under variable motion conditions. Extensive experiments are conducted under different operation modes, velocities, and depths. The results demonstrate that the developed system achieves superior accuracy, adaptability and robustness, providing a solid foundation for autonomous underwater pipeline detection.",
            "headline_zh": "提出多模式水下结构光三维成像系统，融合多源信息以检测水下管道缺陷。",
            "intro_zh": [
                "核心问题：水下管道易腐蚀，需高精度三维成像系统进行实时检测。",
                "方法要点：采用多模式成像策略、多源信息融合和自适应扩展卡尔曼滤波，提升稳定性和精度。",
                "实验或效果：在不同操作模式、速度和深度下实验，系统展现出高准确性、适应性和鲁棒性。"
            ],
            "tags_zh": [
                "水下三维成像",
                "结构光",
                "多源信息融合",
                "点云配准",
                "管道检测",
                "自适应滤波"
            ],
            "_index": 126
        },
        {
            "title": "CAT: Can Trust be Predicted with Context-Awareness in Dynamic Heterogeneous Networks?",
            "authors": [
                "Jie Wang",
                "Zheng Yan",
                "Jiahe Lan",
                "Xuyan Li",
                "Elisa Bertino"
            ],
            "arxiv_id": "2512.11352v1",
            "summary": "Trust prediction provides valuable support for decision-making, risk mitigation, and system security enhancement. Recently, Graph Neural Networks (GNNs) have emerged as a promising approach for trust prediction, owing to their ability to learn expressive node representations that capture intricate trust relationships within a network. However, current GNN-based trust prediction models face several limitations: (i) Most of them fail to capture trust dynamicity, leading to questionable inferences. (ii) They rarely consider the heterogeneous nature of real-world networks, resulting in a loss of rich semantics. (iii) None of them support context-awareness, a basic property of trust, making prediction results coarse-grained.\n  To this end, we propose CAT, the first Context-Aware GNN-based Trust prediction model that supports trust dynamicity and accurately represents real-world heterogeneity. CAT consists of a graph construction layer, an embedding layer, a heterogeneous attention layer, and a prediction layer. It handles dynamic graphs using continuous-time representations and captures temporal information through a time encoding function. To model graph heterogeneity and leverage semantic information, CAT employs a dual attention mechanism that identifies the importance of different node types and nodes within each type. For context-awareness, we introduce a new notion of meta-paths to extract contextual features. By constructing context embeddings and integrating a context-aware aggregator, CAT can predict both context-aware trust and overall trust. Extensive experiments on three real-world datasets demonstrate that CAT outperforms five groups of baselines in trust prediction, while exhibiting strong scalability to large-scale graphs and robustness against both trust-oriented and GNN-oriented attacks.",
            "headline_zh": "提出CAT模型以解决动态异构网络中上下文感知的信任预测问题",
            "intro_zh": [
                "核心问题：现有GNN信任预测模型忽略动态性、异构性和上下文感知，导致预测粗糙",
                "方法要点：CAT结合连续时间表示、异构注意力机制和元路径提取上下文特征，实现动态异构网络的上下文感知信任预测",
                "实验或效果：在三个真实数据集上优于五组基线，展现强可扩展性和对攻击的鲁棒性"
            ],
            "tags_zh": [
                "信任预测",
                "图神经网络",
                "动态图",
                "异构网络",
                "上下文感知",
                "注意力机制"
            ],
            "_index": 127
        },
        {
            "title": "Incremental Validation of Automated Driving Functions using Generic Volumes in Micro- Operational Design Domains",
            "authors": [
                "Steffen Schäfer",
                "Martin Cichon"
            ],
            "arxiv_id": "2512.11351v1",
            "summary": "The validation of highly automated, perception-based driving systems must ensure that they function correctly under the full range of real-world conditions. Scenario-based testing is a prominent approach to addressing this challenge, as it involves the systematic simulation of objects and environments. Operational Design Domains (ODDs) are usually described using a taxonomy of qualitative designations for individual objects. However, the process of transitioning from taxonomy to concrete test cases remains unstructured, and completeness is theoretical. This paper introduces a structured method of subdividing the ODD into manageable sections, termed micro-ODDs (mODDs), and deriving test cases with abstract object representations. This concept is demonstrated using a one-dimensional, laterally guided manoeuvre involving a shunting locomotive within a constrained ODD. In this example, mODDs are defined and refined into narrow taxonomies that enable test case generation. Obstacles are represented as generic cubes of varying sizes, providing a simplified yet robust means of evaluating perception performance. A series of tests were conducted in a closed-loop, co-simulated virtual environment featuring photorealistic rendering and simulated LiDAR, GNSS and camera sensors. The results demonstrate how edge cases in obstacle detection can be systematically explored and how perception quality can be evaluated based on observed vehicle behaviour, using crash versus safe stop as the outcome metrics. These findings support the development of a standardised framework for safety argumentation and offer a practical step towards the validation and authorisation of automated driving functions.",
            "headline_zh": "提出基于微操作设计域和通用立方体的结构化方法，以系统验证自动驾驶感知功能。",
            "intro_zh": [
                "核心问题：从操作设计域到具体测试用例的转换过程缺乏结构化，完整性难以保证。",
                "方法要点：将操作设计域细分为微操作设计域，使用通用立方体抽象表示障碍物生成测试用例。",
                "实验或效果：在闭环协同仿真环境中测试，以碰撞与安全停止为指标，系统探索感知边缘案例。"
            ],
            "tags_zh": [
                "自动驾驶验证",
                "微操作设计域",
                "场景测试",
                "感知评估",
                "闭环仿真",
                "安全论证"
            ],
            "_index": 128
        },
        {
            "title": "Surveillance Video-Based Traffic Accident Detection Using Transformer Architecture",
            "authors": [
                "Tanu Singh",
                "Pranamesh Chakraborty",
                "Long T. Truong"
            ],
            "arxiv_id": "2512.11350v1",
            "summary": "Road traffic accidents represent a leading cause of mortality globally, with incidence rates rising due to increasing population, urbanization, and motorization. Rising accident rates raise concerns about traffic surveillance effectiveness. Traditional computer vision methods for accident detection struggle with limited spatiotemporal understanding and poor cross-domain generalization. Recent advances in transformer architectures excel at modeling global spatial-temporal dependencies and parallel computation. However, applying these models to automated traffic accident detection is limited by small, non-diverse datasets, hindering the development of robust, generalizable systems. To address this gap, we curated a comprehensive and balanced dataset that captures a wide spectrum of traffic environments, accident types, and contextual variations. Utilizing the curated dataset, we propose an accident detection model based on a transformer architecture using pre-extracted spatial video features. The architecture employs convolutional layers to extract local correlations across diverse patterns within a frame, while leveraging transformers to capture sequential-temporal dependencies among the retrieved features. Moreover, most existing studies neglect the integration of motion cues, which are essential for understanding dynamic scenes, especially during accidents. These approaches typically rely on static features or coarse temporal information. In this study, multiple methods for incorporating motion cues were evaluated to identify the most effective strategy. Among the tested input approaches, concatenating RGB features with optical flow achieved the highest accuracy at 88.3%. The results were further compared with vision language models (VLM) such as GPT, Gemini, and LLaVA-NeXT-Video to assess the effectiveness of the proposed method.",
            "headline_zh": "提出基于Transformer的监控视频交通事故检测方法，结合运动线索提升准确性。",
            "intro_zh": [
                "核心问题：传统方法在时空理解和跨域泛化方面不足，且数据集小、多样性差。",
                "方法要点：使用Transformer架构捕获时空依赖，并评估多种运动线索融合策略。",
                "实验或效果：结合RGB与光流特征达到88.3%最高准确率，并与VLM模型对比验证。"
            ],
            "tags_zh": [
                "交通事故检测",
                "Transformer架构",
                "监控视频分析",
                "运动线索融合",
                "时空依赖建模"
            ],
            "_index": 129
        },
        {
            "title": "Symmetry-Aware Steering of Equivariant Diffusion Policies: Benefits and Limits",
            "authors": [
                "Minwoo Park",
                "Junwoo Chang",
                "Jongeun Choi",
                "Roberto Horowitz"
            ],
            "arxiv_id": "2512.11345v1",
            "summary": "Equivariant diffusion policies (EDPs) combine the generative expressivity of diffusion models with the strong generalization and sample efficiency afforded by geometric symmetries. While steering these policies with reinforcement learning (RL) offers a promising mechanism for fine-tuning beyond demonstration data, directly applying standard (non-equivariant) RL can be sample-inefficient and unstable, as it ignores the symmetries that EDPs are designed to exploit. In this paper, we theoretically establish that the diffusion process of an EDP is equivariant, which in turn induces a group-invariant latent-noise MDP that is well-suited for equivariant diffusion steering. Building on this theory, we introduce a principled symmetry-aware steering framework and compare standard, equivariant, and approximately equivariant RL strategies through comprehensive experiments across tasks with varying degrees of symmetry. While we identify the practical boundaries of strict equivariance under symmetry breaking, we show that exploiting symmetry during the steering process yields substantial benefits-enhancing sample efficiency, preventing value divergence, and achieving strong policy improvements even when EDPs are trained from extremely limited demonstrations.",
            "headline_zh": "提出对称感知引导框架以提升等变扩散策略的强化学习效率与稳定性",
            "intro_zh": [
                "核心问题：标准强化学习引导等变扩散策略时忽略对称性，导致样本效率低且不稳定",
                "方法要点：理论证明等变扩散过程的等变性，构建群不变潜在噪声MDP，提出对称感知引导框架",
                "实验或效果：在对称性程度不同的任务中验证，对称感知引导显著提升样本效率、防止价值发散、改善策略"
            ],
            "tags_zh": [
                "等变扩散策略",
                "对称感知引导",
                "强化学习",
                "样本效率",
                "几何对称性",
                "策略改进"
            ],
            "_index": 130
        },
        {
            "title": "DAPO: Design Structure-Aware Pass Ordering in High-Level Synthesis with Graph Contrastive and Reinforcement Learning",
            "authors": [
                "Jinming Ge",
                "Linfeng Du",
                "Likith Anaparty",
                "Shangkun Li",
                "Tingyuan Liang",
                "Afzal Ahmad",
                "Vivek Chaturvedi",
                "Sharad Sinha",
                "Zhiyao Xie",
                "Jiang Xu",
                "Wei Zhang"
            ],
            "arxiv_id": "2512.11342v1",
            "summary": "High-Level Synthesis (HLS) tools are widely adopted in FPGA-based domain-specific accelerator design. However, existing tools rely on fixed optimization strategies inherited from software compilations, limiting their effectiveness. Tailoring optimization strategies to specific designs requires deep semantic understanding, accurate hardware metric estimation, and advanced search algorithms -- capabilities that current approaches lack.\n  We propose DAPO, a design structure-aware pass ordering framework that extracts program semantics from control and data flow graphs, employs contrastive learning to generate rich embeddings, and leverages an analytical model for accurate hardware metric estimation. These components jointly guide a reinforcement learning agent to discover design-specific optimization strategies. Evaluations on classic HLS designs demonstrate that our end-to-end flow delivers a 2.36 speedup over Vitis HLS on average.",
            "headline_zh": "提出DAPO框架，通过图对比和强化学习实现设计结构感知的HLS优化策略定制。",
            "intro_zh": [
                "现有HLS工具依赖固定优化策略，缺乏针对特定设计的语义理解和硬件指标估计能力。",
                "DAPO从控制流和数据流图提取语义，利用对比学习生成嵌入，结合分析模型指导强化学习代理。",
                "在经典HLS设计上评估，平均比Vitis HLS加速2.36倍。"
            ],
            "tags_zh": [
                "高层次综合",
                "图对比学习",
                "强化学习",
                "硬件优化",
                "FPGA加速器"
            ],
            "_index": 131
        },
        {
            "title": "Task-Specific Distance Correlation Matching for Few-Shot Action Recognition",
            "authors": [
                "Fei Long",
                "Yao Zhang",
                "Jiaming Lv",
                "Jiangtao Xie",
                "Peihua Li"
            ],
            "arxiv_id": "2512.11340v1",
            "summary": "Few-shot action recognition (FSAR) has recently made notable progress through set matching and efficient adaptation of large-scale pre-trained models. However, two key limitations persist. First, existing set matching metrics typically rely on cosine similarity to measure inter-frame linear dependencies and then perform matching with only instance-level information, thus failing to capture more complex patterns such as nonlinear relationships and overlooking task-specific cues. Second, for efficient adaptation of CLIP to FSAR, recent work performing fine-tuning via skip-fusion layers (which we refer to as side layers) has significantly reduced memory cost. However, the newly introduced side layers are often difficult to optimize under limited data conditions. To address these limitations, we propose TS-FSAR, a framework comprising three components: (1) a visual Ladder Side Network (LSN) for efficient CLIP fine-tuning; (2) a metric called Task-Specific Distance Correlation Matching (TS-DCM), which uses $α$-distance correlation to model both linear and nonlinear inter-frame dependencies and leverages a task prototype to enable task-specific matching; and (3) a Guiding LSN with Adapted CLIP (GLAC) module, which regularizes LSN using the adapted frozen CLIP to improve training for better $α$-distance correlation estimation under limited supervision. Extensive experiments on five widely-used benchmarks demonstrate that our TS-FSAR yields superior performance compared to prior state-of-the-arts.",
            "headline_zh": "提出TS-FSAR框架，通过任务特定距离相关匹配和引导侧网络解决少样本动作识别中的非线性依赖和优化难题。",
            "intro_zh": [
                "现有方法依赖余弦相似度，难以捕捉非线性帧间依赖和任务特定线索。",
                "引入α-距离相关匹配和任务原型，建模线性与非线性依赖，实现任务特定匹配。",
                "在五个基准测试中表现优于先前方法，验证了框架的有效性。"
            ],
            "tags_zh": [
                "少样本动作识别",
                "距离相关匹配",
                "CLIP微调",
                "非线性依赖建模",
                "任务特定学习"
            ],
            "_index": 132
        },
        {
            "title": "UFVideo: Towards Unified Fine-Grained Video Cooperative Understanding with Large Language Models",
            "authors": [
                "Hewen Pan",
                "Cong Wei",
                "Dashuang Liang",
                "Zepeng Huang",
                "Pengfei Gao",
                "Ziqi Zhou",
                "Lulu Xue",
                "Pengfei Yan",
                "Xiaoming Wei",
                "Minghui Li",
                "Shengshan Hu"
            ],
            "arxiv_id": "2512.11336v1",
            "summary": "With the advancement of multi-modal Large Language Models (LLMs), Video LLMs have been further developed to perform on holistic and specialized video understanding. However, existing works are limited to specialized video understanding tasks, failing to achieve a comprehensive and multi-grained video perception. To bridge this gap, we introduce UFVideo, the first Video LLM with unified multi-grained cooperative understanding capabilities. Specifically, we design unified visual-language guided alignment to flexibly handle video understanding across global, pixel and temporal scales within a single model. UFVideo dynamically encodes the visual and text inputs of different tasks and generates the textual response, temporal localization, or grounded mask. Additionally, to evaluate challenging multi-grained video understanding tasks, we construct the UFVideo-Bench consisting of three distinct collaborative tasks within the scales, which demonstrates UFVideo's flexibility and advantages over GPT-4o. Furthermore, we validate the effectiveness of our model across 9 public benchmarks covering various common video understanding tasks, providing valuable insights for future Video LLMs.",
            "headline_zh": "提出UFVideo，首个统一多粒度协同理解的视频大语言模型，以解决现有模型局限于专项任务的问题。",
            "intro_zh": [
                "现有视频大语言模型局限于专项理解任务，缺乏全面多粒度感知能力。",
                "设计统一视觉-语言引导对齐方法，在单一模型中灵活处理全局、像素和时间尺度视频理解。",
                "构建UFVideo-Bench评估多粒度协同任务，并在9个公共基准上验证模型有效性，优于GPT-4o。"
            ],
            "tags_zh": [
                "视频大语言模型",
                "多粒度协同理解",
                "统一视觉-语言对齐",
                "视频理解基准",
                "时间定位",
                "掩码生成"
            ],
            "_index": 133
        },
        {
            "title": "FreqDINO: Frequency-Guided Adaptation for Generalized Boundary-Aware Ultrasound Image Segmentation",
            "authors": [
                "Yixuan Zhang",
                "Qing Xu",
                "Yue Li",
                "Xiangjian He",
                "Qian Zhang",
                "Mainul Haque",
                "Rong Qu",
                "Wenting Duan",
                "Zhen Chen"
            ],
            "arxiv_id": "2512.11335v1",
            "summary": "Ultrasound image segmentation is pivotal for clinical diagnosis, yet challenged by speckle noise and imaging artifacts. Recently, DINOv3 has shown remarkable promise in medical image segmentation with its powerful representation capabilities. However, DINOv3, pre-trained on natural images, lacks sensitivity to ultrasound-specific boundary degradation. To address this limitation, we propose FreqDINO, a frequency-guided segmentation framework that enhances boundary perception and structural consistency. Specifically, we devise a Multi-scale Frequency Extraction and Alignment (MFEA) strategy to separate low-frequency structures and multi-scale high-frequency boundary details, and align them via learnable attention. We also introduce a Frequency-Guided Boundary Refinement (FGBR) module that extracts boundary prototypes from high-frequency components and refines spatial features. Furthermore, we design a Multi-task Boundary-Guided Decoder (MBGD) to ensure spatial coherence between boundary and semantic predictions. Extensive experiments demonstrate that FreqDINO surpasses state-of-the-art methods with superior achieves remarkable generalization capability. The code is at https://github.com/MingLang-FD/FreqDINO.",
            "headline_zh": "提出FreqDINO以解决超声图像分割中边界退化问题",
            "intro_zh": [
                "核心问题：DINOv3预训练于自然图像，对超声图像边界退化不敏感。",
                "方法要点：设计多尺度频率提取对齐策略和频率引导边界细化模块。",
                "实验或效果：在实验中超越先进方法，展现优异泛化能力。"
            ],
            "tags_zh": [
                "超声图像分割",
                "频率引导",
                "边界感知",
                "多尺度特征",
                "DINOv3适应"
            ],
            "_index": 134
        },
        {
            "title": "Spectral entropy prior-guided deep feature fusion architecture for magnetic core loss",
            "authors": [
                "Cong Yao",
                "Chunye Gong",
                "Jin Zhang"
            ],
            "arxiv_id": "2512.11334v1",
            "summary": "Accurate core loss modeling is critical for the design of high-efficiency power electronic systems. Traditional core loss modeling methods have limitations in prediction accuracy. To advance this field, the IEEE Power Electronics Society launched the MagNet Challenge in 2023, the first international competition focused on data-driven power electronics design methods, aiming to uncover complex loss patterns in magnetic components through a data-driven paradigm. Although purely data-driven models demonstrate strong fitting performance, their interpretability and cross-distribution generalization capabilities remain limited. To address these issues, this paper proposes a hybrid model, SEPI-TFPNet, which integrates empirical models with deep learning. The physical-prior submodule employs a spectral entropy discrimination mechanism to select the most suitable empirical model under different excitation waveforms. The data-driven submodule incorporates convolutional neural networks, multi-head attention mechanisms, and bidirectional long short-term memory networks to extract flux-density time-series features. An adaptive feature fusion module is introduced to improve multimodal feature interaction and integration. Using the MagNet dataset containing various magnetic materials, this paper evaluates the proposed method and compares it with 21 representative models from the 2023 challenge and three advanced methods from 2024-2025. The results show that the proposed method achieves improved modeling accuracy and robustness.",
            "headline_zh": "提出SEPI-TFPNet混合模型以提升磁芯损耗建模的准确性与鲁棒性",
            "intro_zh": [
                "核心问题：传统磁芯损耗建模方法预测精度有限，纯数据驱动模型可解释性与跨分布泛化能力不足",
                "方法要点：结合经验模型与深度学习，通过谱熵判别机制选择经验模型，并利用CNN、多头注意力和BiLSTM提取特征",
                "实验或效果：在MagNet数据集上评估，相比2023年挑战赛21个模型及2024-2025年三个先进方法，建模精度和鲁棒性提升"
            ],
            "tags_zh": [
                "磁芯损耗建模",
                "混合模型",
                "谱熵先验",
                "特征融合",
                "深度学习",
                "电力电子"
            ],
            "_index": 135
        },
        {
            "title": "Pace: Physics-Aware Attentive Temporal Convolutional Network for Battery Health Estimation",
            "authors": [
                "Sara Sameer",
                "Wei Zhang",
                "Kannan Dhivya Dharshini",
                "Xin Lou",
                "Yulin Gao",
                "Terence Goh",
                "Qingyu Yan"
            ],
            "arxiv_id": "2512.11332v1",
            "summary": "Batteries are critical components in modern energy systems such as electric vehicles and power grid energy storage. Effective battery health management is essential for battery system safety, cost-efficiency, and sustainability. In this paper, we propose Pace, a physics-aware attentive temporal convolutional network for battery health estimation. Pace integrates raw sensor measurements with battery physics features derived from the equivalent circuit model. We develop three battery-specific modules, including dilated temporal blocks for efficient temporal encoding, chunked attention blocks for context modeling, and a dual-head output block for fusing short- and long-term battery degradation patterns. Together, the modules enable Pace to predict battery health accurately and efficiently in various battery usage conditions. In a large public dataset, Pace performs much better than existing models, achieving an average performance improvement of 6.5 and 2.0x compared to two best-performing baseline models. We further demonstrate its practical viability with a real-time edge deployment on a Raspberry Pi. These results establish Pace as a practical and high-performance solution for battery health analytics.",
            "headline_zh": "提出Pace物理感知注意力时序卷积网络，用于电池健康估计",
            "intro_zh": [
                "核心问题：电池健康管理对现代能源系统安全与效率至关重要",
                "方法要点：结合原始传感器数据与等效电路模型物理特征，设计电池专用模块",
                "实验或效果：在公开数据集上优于基线模型，并在树莓派上实现实时边缘部署"
            ],
            "tags_zh": [
                "电池健康估计",
                "物理感知模型",
                "时序卷积网络",
                "注意力机制",
                "边缘计算"
            ],
            "_index": 136
        },
        {
            "title": "Physics-Informed Video Flare Synthesis and Removal Leveraging Motion Independence between Flare and Scene",
            "authors": [
                "Junqiao Wang",
                "Yuanfei Huang",
                "Hua Huang"
            ],
            "arxiv_id": "2512.11327v1",
            "summary": "Lens flare is a degradation phenomenon caused by strong light sources. Existing researches on flare removal have mainly focused on images, while the spatiotemporal characteristics of video flare remain largely unexplored. Video flare synthesis and removal pose significantly greater challenges than in image, owing to the complex and mutually independent motion of flare, light sources, and scene content. This motion independence further affects restoration performance, often resulting in flicker and artifacts. To address this issue, we propose a physics-informed dynamic flare synthesis pipeline, which simulates light source motion using optical flow and models the temporal behaviors of both scattering and reflective flares. Meanwhile, we design a video flare removal network that employs an attention module to spatially suppress flare regions and incorporates a Mamba-based temporal modeling component to capture long range spatio-temporal dependencies. This motion-independent spatiotemporal representation effectively eliminates the need for multi-frame alignment, alleviating temporal aliasing between flares and scene content and thereby improving video flare removal performance. Building upon this, we construct the first video flare dataset to comprehensively evaluate our method, which includes a large set of synthetic paired videos and additional real-world videos collected from the Internet to assess generalization capability. Extensive experiments demonstrate that our method consistently outperforms existing video-based restoration and image-based flare removal methods on both real and synthetic videos, effectively removing dynamic flares while preserving light source integrity and maintaining spatiotemporal consistency of scene.",
            "headline_zh": "提出物理引导的视频光晕合成与去除方法，利用光晕与场景运动独立性提升视频恢复性能。",
            "intro_zh": [
                "核心问题：视频光晕去除因光晕、光源和场景运动独立而复杂，导致闪烁和伪影。",
                "方法要点：设计物理引导的动态光晕合成流程和基于注意力与Mamba的视频去除网络，无需多帧对齐。",
                "实验或效果：构建首个视频光晕数据集，实验显示在真实和合成视频上优于现有方法，保持时空一致性。"
            ],
            "tags_zh": [
                "视频光晕去除",
                "物理引导合成",
                "时空建模",
                "注意力机制",
                "Mamba网络",
                "光晕数据集"
            ],
            "_index": 137
        },
        {
            "title": "MLLM Machine Unlearning via Visual Knowledge Distillation",
            "authors": [
                "Yuhang Wang",
                "Zhenxing Niu",
                "Haoxuan Ji",
                "Guangyu He",
                "Haichang Gao",
                "Gang Hua"
            ],
            "arxiv_id": "2512.11325v1",
            "summary": "Recently, machine unlearning approaches have been proposed to remove sensitive information from well-trained large models. However, most existing methods are tailored for LLMs, while MLLM-oriented unlearning remains at its early stage. Inspired by recent studies exploring the internal mechanisms of MLLMs, we propose to disentangle the visual and textual knowledge embedded within MLLMs and introduce a dedicated approach to selectively erase target visual knowledge while preserving textual knowledge. Unlike previous unlearning methods that rely on output-level supervision, our approach introduces a Visual Knowledge Distillation (VKD) scheme, which leverages intermediate visual representations within the MLLM as supervision signals. This design substantially enhances both unlearning effectiveness and model utility. Moreover, since our method only fine-tunes the visual components of the MLLM, it offers significant efficiency advantages. Extensive experiments demonstrate that our approach outperforms state-of-the-art unlearning methods in terms of both effectiveness and efficiency. Moreover, we are the first to evaluate the robustness of MLLM unlearning against relearning attacks.",
            "headline_zh": "提出视觉知识蒸馏方法以解决多模态大模型中的视觉知识选择性遗忘问题",
            "intro_zh": [
                "核心问题：现有遗忘方法主要针对LLMs，多模态大模型遗忘研究处于早期阶段，需选择性移除视觉知识。",
                "方法要点：引入视觉知识蒸馏方案，利用模型内部视觉表示作为监督信号，仅微调视觉组件以提升效率。",
                "实验或效果：实验表明方法在有效性和效率上优于现有技术，并首次评估了遗忘对再学习攻击的鲁棒性。"
            ],
            "tags_zh": [
                "多模态大模型",
                "机器遗忘",
                "视觉知识蒸馏",
                "选择性遗忘",
                "模型效率",
                "鲁棒性评估"
            ],
            "_index": 138
        },
        {
            "title": "CAPTURE: A Benchmark and Evaluation for LVLMs in CAPTCHA Resolving",
            "authors": [
                "Jianyi Zhang",
                "Ziyin Zhou",
                "Xu Ji",
                "Shizhao Liu",
                "Zhangchi Zhao"
            ],
            "arxiv_id": "2512.11323v1",
            "summary": "Benefiting from strong and efficient multi-modal alignment strategies, Large Visual Language Models (LVLMs) are able to simulate human visual and reasoning capabilities, such as solving CAPTCHAs. However, existing benchmarks based on visual CAPTCHAs still face limitations. Previous studies, when designing benchmarks and datasets, customized them according to their research objectives. Consequently, these benchmarks cannot comprehensively cover all CAPTCHA types. Notably, there is a dearth of dedicated benchmarks for LVLMs. To address this problem, we introduce a novel CAPTCHA benchmark for the first time, named CAPTURE CAPTCHA for Testing Under Real-world Experiments, specifically for LVLMs. Our benchmark encompasses 4 main CAPTCHA types and 25 sub-types from 31 vendors. The diversity enables a multi-dimensional and thorough evaluation of LVLM performance. CAPTURE features extensive class variety, large-scale data, and unique LVLM-tailored labels, filling the gaps in previous research in terms of data comprehensiveness and labeling pertinence. When evaluated by this benchmark, current LVLMs demonstrate poor performance in solving CAPTCHAs.",
            "headline_zh": "提出CAPTURE基准以评估大型视觉语言模型在验证码解析中的性能",
            "intro_zh": [
                "现有验证码基准覆盖不全，缺乏针对LVLMs的专用评估工具",
                "CAPTURE包含4类25子类验证码，数据多样且标注针对LVLMs",
                "评估显示当前LVLMs在验证码解析上表现不佳"
            ],
            "tags_zh": [
                "大型视觉语言模型",
                "验证码解析",
                "多模态基准",
                "性能评估",
                "数据集构建"
            ],
            "_index": 139
        },
        {
            "title": "KeyframeFace: From Text to Expressive Facial Keyframes",
            "authors": [
                "Jingchao Wu",
                "Zejian Kang",
                "Haibo Liu",
                "Yuanchen Fei",
                "Xiangru Huang"
            ],
            "arxiv_id": "2512.11321v1",
            "summary": "Generating dynamic 3D facial animation from natural language requires understanding both temporally structured semantics and fine-grained expression changes. Existing datasets and methods mainly focus on speech-driven animation or unstructured expression sequences and therefore lack the semantic grounding and temporal structures needed for expressive human performance generation. In this work, we introduce KeyframeFace, a large-scale multimodal dataset designed for text-to-animation research through keyframe-level supervision. KeyframeFace provides 2,100 expressive scripts paired with monocular videos, per-frame ARKit coefficients, contextual backgrounds, complex emotions, manually defined keyframes, and multi-perspective annotations based on ARKit coefficients and images via Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs). Beyond the dataset, we propose the first text-to-animation framework that explicitly leverages LLM priors for interpretable facial motion synthesis. This design aligns the semantic understanding capabilities of LLMs with the interpretable structure of ARKit's coefficients, enabling high-fidelity expressive animation. KeyframeFace and our LLM-based framework together establish a new foundation for interpretable, keyframe-guided, and context-aware text-to-animation. Code and data are available at https://github.com/wjc12345123/KeyframeFace.",
            "headline_zh": "提出KeyframeFace数据集与LLM框架，以解决文本到表情动画的语义与时间结构问题。",
            "intro_zh": [
                "核心问题：现有方法缺乏语义基础和时间结构，难以从文本生成动态3D面部动画。",
                "方法要点：构建大规模多模态数据集，并利用LLM先验实现可解释的面部运动合成。",
                "实验或效果：通过关键帧监督和ARKit系数，实现高保真、上下文感知的动画生成。"
            ],
            "tags_zh": [
                "文本到动画",
                "面部动画",
                "关键帧监督",
                "多模态数据集",
                "LLM先验",
                "ARKit系数"
            ],
            "_index": 140
        },
        {
            "title": "SATMapTR: Satellite Image Enhanced Online HD Map Construction",
            "authors": [
                "Bingyuan Huang",
                "Guanyi Zhao",
                "Qian Xu",
                "Yang Lou",
                "Yung-Hui Li",
                "Jianping Wang"
            ],
            "arxiv_id": "2512.11319v1",
            "summary": "High-definition (HD) maps are evolving from pre-annotated to real-time construction to better support autonomous driving in diverse scenarios. However, this process is hindered by low-quality input data caused by onboard sensors limited capability and frequent occlusions, leading to incomplete, noisy, or missing data, and thus reduced mapping accuracy and robustness. Recent efforts have introduced satellite images as auxiliary input, offering a stable, wide-area view to complement the limited ego perspective. However, satellite images in Bird's Eye View are often degraded by shadows and occlusions from vegetation and buildings. Prior methods using basic feature extraction and fusion remain ineffective. To address these challenges, we propose SATMapTR, a novel online map construction model that effectively fuses satellite image through two key components: (1) a gated feature refinement module that adaptively filters satellite image features by integrating high-level semantics with low-level structural cues to extract high signal-to-noise ratio map-relevant representations; and (2) a geometry-aware fusion module that consistently fuse satellite and BEV features at a grid-to-grid level, minimizing interference from irrelevant regions and low-quality inputs. Experimental results on the nuScenes dataset show that SATMapTR achieves the highest mean average precision (mAP) of 73.8, outperforming state-of-the-art satellite-enhanced models by up to 14.2 mAP. It also shows lower mAP degradation under adverse weather and sensor failures, and achieves nearly 3 times higher mAP at extended perception ranges.",
            "headline_zh": "提出SATMapTR模型，通过融合卫星图像增强在线高精地图构建，以解决车载传感器数据质量低的问题。",
            "intro_zh": [
                "核心问题：车载传感器数据因遮挡和能力限制导致高精地图构建不完整、噪声大，影响自动驾驶。",
                "方法要点：引入门控特征细化模块和几何感知融合模块，自适应过滤卫星图像特征并高效融合BEV特征。",
                "实验或效果：在nuScenes数据集上达到73.8 mAP，优于现有卫星增强模型，并在恶劣条件下表现更稳健。"
            ],
            "tags_zh": [
                "高精地图构建",
                "卫星图像融合",
                "在线地图生成",
                "自动驾驶感知",
                "特征融合",
                "BEV视角"
            ],
            "_index": 141
        },
        {
            "title": "Condensation-Concatenation Framework for Dynamic Graph Continual Learning",
            "authors": [
                "Tingxu Yan",
                "Ye Yuan"
            ],
            "arxiv_id": "2512.11317v1",
            "summary": "Dynamic graphs are prevalent in real-world scenarios, where continuous structural changes induce catastrophic forgetting in graph neural networks (GNNs). While continual learning has been extended to dynamic graphs, existing methods overlook the effects of topological changes on existing nodes. To address it, we propose a novel framework for continual learning on dynamic graphs, named Condensation-Concatenation-based Continual Learning (CCC). Specifically, CCC first condenses historical graph snapshots into compact semantic representations while aiming to preserve the original label distribution and topological properties. Then it concatenates these historical embeddings with current graph representations selectively. Moreover, we refine the forgetting measure (FM) to better adapt to dynamic graph scenarios by quantifying the predictive performance degradation of existing nodes caused by structural updates. CCC demonstrates superior performance over state-of-the-art baselines across four real-world datasets in extensive experiments.",
            "headline_zh": "提出Condensation-Concatenation框架以解决动态图持续学习中的灾难性遗忘问题",
            "intro_zh": [
                "核心问题：动态图结构变化导致图神经网络对现有节点的灾难性遗忘",
                "方法要点：通过压缩历史图快照为语义表示，并选择性拼接当前图表示",
                "实验或效果：在四个真实数据集上优于现有基线，并改进了遗忘度量"
            ],
            "tags_zh": [
                "动态图持续学习",
                "灾难性遗忘",
                "图神经网络",
                "拓扑变化",
                "语义压缩",
                "遗忘度量"
            ],
            "_index": 142
        },
        {
            "title": "Benchmarking the Generality of Vision-Language-Action Models",
            "authors": [
                "Pranav Guruprasad",
                "Sudipta Chowdhury",
                "Harsh Sikka",
                "Mridul Sharma",
                "Helen Lu",
                "Sean Rivera",
                "Aryan Khurana",
                "Hangliang Ren",
                "Yangyue Wang"
            ],
            "arxiv_id": "2512.11315v1",
            "summary": "Generalist multimodal agents are expected to unify perception, language, and control - operating robustly across diverse real world domains. However, current evaluation practices remain fragmented across isolated benchmarks, making it difficult to assess whether today's foundation models truly generalize beyond their training distributions. We introduce MultiNet v1.0, a unified benchmark for measuring the cross domain generality of vision language models (VLMs) and vision language action models (VLAs) across six foundational capability regimes. Visual grounding, spatial reasoning, tool use, physical commonsense, multi agent coordination, and continuous robot control. Evaluating GPT 5, Pi0, and Magma, we find that no model demonstrates consistent generality. All exhibit substantial degradation on unseen domains, unfamiliar modalities, or cross domain task shifts despite strong performance within their training distributions.These failures manifest as modality misalignment, output format instability, and catastrophic knowledge degradation under domain transfer.Our findings reveal a persistent gap between the aspiration of generalist intelligence and the actual capabilities of current foundation models.MultiNet v1.0 provides a standardized evaluation substrate for diagnosing these gaps and guiding the development of future generalist agents.Code, data, and leaderboards are publicly available.",
            "headline_zh": "提出MultiNet v1.0基准以评估视觉语言模型在跨领域通用性中的表现。",
            "intro_zh": [
                "核心问题：当前评估方法碎片化，难以衡量基础模型在训练分布外的通用性。",
                "方法要点：引入统一基准MultiNet v1.0，覆盖六种能力领域以标准化评估。",
                "实验或效果：评估GPT-5等模型发现通用性不足，存在模态错位和知识退化问题。"
            ],
            "tags_zh": [
                "视觉语言模型评估",
                "跨领域通用性",
                "基准测试",
                "模态对齐",
                "机器人控制",
                "多模态智能"
            ],
            "_index": 143
        },
        {
            "title": "QGEC : Quantum Golay Code Error Correction",
            "authors": [
                "Hideo Mukai",
                "Hoshitaro Ohnishi"
            ],
            "arxiv_id": "2512.11307v1",
            "summary": "Quantum computers have the possibility of a much reduced calculation load compared with classical computers in specific problems. Quantum error correction (QEC) is vital for handling qubits, which are vulnerable to external noise. In QEC, actual errors are predicted from the results of syndrome measurements by stabilizer generators, in place of making direct measurements of the data qubits. Here, we propose Quantum Golay code Error Correction (QGEC), a QEC method using Golay code, which is an efficient coding method in classical information theory. We investigated our method's ability in decoding calculations with the Transformer. We evaluated the accuracy of the decoder in a code space defined by the generative polynomials with three different weights sets and three noise models with different correlations of bit-flip error and phase-flip error. Furthermore, under a noise model following a discrete uniform distribution, we compared the decoding performance of Transformer decoders with identical architectures trained respectively on Golay and toric codes. The results showed that the noise model with the smaller correlation gave better accuracy, while the weights of the generative polynomials had little effect on the accuracy of the decoder. In addition, they showed that Golay code requiring 23 data qubits and having a code distance of 7 achieved higher decoding accuracy than toric code which requiring 50 data qubits and having a code distance of 5. This suggests that implementing quantum error correction using a Transformer may enable the Golay code to realize fault-tolerant quantum computation more efficiently.",
            "headline_zh": "提出基于Golay码的量子纠错方法QGEC，利用Transformer解码提升容错量子计算效率",
            "intro_zh": [
                "量子纠错是量子计算的关键，需通过稳定子测量预测错误而非直接测量数据量子比特",
                "QGEC采用经典信息论中高效的Golay码，结合Transformer进行解码计算，评估不同噪声模型和权重集下的解码精度",
                "实验表明，Golay码（23数据量子比特，码距7）比toric码（50数据量子比特，码距5）解码精度更高，可能更高效实现容错量子计算"
            ],
            "tags_zh": [
                "量子纠错",
                "Golay码",
                "Transformer解码",
                "噪声模型",
                "容错量子计算"
            ],
            "_index": 144
        },
        {
            "title": "MultiEgo: A Multi-View Egocentric Video Dataset for 4D Scene Reconstruction",
            "authors": [
                "Bate Li",
                "Houqiang Zhong",
                "Zhengxue Cheng",
                "Qiang Hu",
                "Qiang Wang",
                "Li Song",
                "Wenjun Zhang"
            ],
            "arxiv_id": "2512.11301v1",
            "summary": "Multi-view egocentric dynamic scene reconstruction holds significant research value for applications in holographic documentation of social interactions. However, existing reconstruction datasets focus on static multi-view or single-egocentric view setups, lacking multi-view egocentric datasets for dynamic scene reconstruction. Therefore, we present MultiEgo, the first multi-view egocentric dataset for 4D dynamic scene reconstruction. The dataset comprises five canonical social interaction scenes: meetings, performances, and a presentation. Each scene provides five authentic egocentric videos captured by participants wearing AR glasses. We design a hardware-based data acquisition system and processing pipeline, achieving sub-millisecond temporal synchronization across views, coupled with accurate pose annotations. Experiment validation demonstrates the practical utility and effectiveness of our dataset for free-viewpoint video (FVV) applications, establishing MultiEgo as a foundational resource for advancing multi-view egocentric dynamic scene reconstruction research.",
            "headline_zh": "提出首个多视角第一人称视频数据集MultiEgo，用于4D动态场景重建，覆盖社交互动场景。",
            "intro_zh": [
                "核心问题：现有重建数据集缺乏多视角第一人称动态场景数据，限制社交互动全息记录研究。",
                "方法要点：设计硬件采集系统，实现亚毫秒级时间同步和精确姿态标注，包含五个社交互动场景。",
                "实验或效果：验证数据集在自由视点视频应用中的实用性，为多视角第一人称动态场景重建提供基础资源。"
            ],
            "tags_zh": [
                "多视角第一人称视频",
                "4D动态场景重建",
                "社交互动数据集",
                "自由视点视频",
                "时间同步",
                "姿态标注"
            ],
            "_index": 145
        },
        {
            "title": "SRLR: Symbolic Regression based Logic Recovery to Counter Programmable Logic Controller Attacks",
            "authors": [
                "Hao Zhou",
                "Suman Sourav",
                "Binbin Chen",
                "Ke Yu"
            ],
            "arxiv_id": "2512.11298v1",
            "summary": "Programmable Logic Controllers (PLCs) are critical components in Industrial Control Systems (ICSs). Their potential exposure to external world makes them susceptible to cyber-attacks. Existing detection methods against controller logic attacks use either specification-based or learnt models. However, specification-based models require experts' manual efforts or access to PLC's source code, while machine learning-based models often fall short of providing explanation for their decisions. We design SRLR -- a it Symbolic Regression based Logic Recovery} solution to identify the logic of a PLC based only on its inputs and outputs. The recovered logic is used to generate explainable rules for detecting controller logic attacks. SRLR enhances the latest deep symbolic regression methods using the following ICS-specific properties: (1) some important ICS control logic is best represented in frequency domain rather than time domain; (2) an ICS controller can operate in multiple modes, each using different logic, where mode switches usually do not happen frequently; (3) a robust controller usually filters out outlier inputs as ICS sensor data can be noisy; and (4) with the above factors captured, the degree of complexity of the formulas is reduced, making effective search possible. Thanks to these enhancements, SRLR consistently outperforms all existing methods in a variety of ICS settings that we evaluate. In terms of the recovery accuracy, SRLR's gain can be as high as 39% in some challenging environment. We also evaluate SRLR on a distribution grid containing hundreds of voltage regulators, demonstrating its stability in handling large-scale, complex systems with varied configurations.",
            "headline_zh": "提出基于符号回归的逻辑恢复方法SRLR，以检测工业控制系统中的可编程逻辑控制器攻击。",
            "intro_zh": [
                "核心问题：现有PLC攻击检测方法依赖专家规范或机器学习模型，前者成本高，后者解释性差。",
                "方法要点：SRLR仅基于输入输出恢复PLC逻辑，利用ICS特性（如频域表示、多模式操作）增强符号回归。",
                "实验或效果：在多种ICS设置中，SRLR恢复准确率最高提升39%，并在大规模电网中验证稳定性。"
            ],
            "tags_zh": [
                "符号回归",
                "工业控制系统安全",
                "可编程逻辑控制器",
                "逻辑恢复",
                "攻击检测"
            ],
            "_index": 146
        },
        {
            "title": "Few-Shot VLM-Based G-Code and HMI Verification in CNC Machining",
            "authors": [
                "Yasaman Hashem Pour",
                "Nazanin Mahjourian",
                "Vinh Nguyen"
            ],
            "arxiv_id": "2512.11296v1",
            "summary": "Manual generation of G-code is important for learning the operation of CNC machines. Prior work in G-code verification uses Large-Language Models (LLMs), which primarily examine errors in the written programming. However, CNC machining requires extensive use and knowledge of the Human-Machine Interface (HMI), which displays machine status and errors. LLMs currently lack the capability to leverage knowledge of HMIs due to their inability to access the vision modality. This paper proposes a few-shot VLM-based verification approach that simultaneously evaluates the G-code and the HMI display for errors and safety status. The input dataset includes paired G-code text and associated HMI screenshots from a 15-slant-PRO lathe, including both correct and error-prone cases. To enable few-shot learning, the VLM is provided with a structured JSON schema based on prior heuristic knowledge. After determining the prompts, instances of G-code and HMI that either contain errors or are error free are used as few-shot examples to guide the VLM. The model was then evaluated in comparison to a zero-shot VLM through multiple scenarios of incorrect G-code and HMI errors with respect to per-slot accuracy. The VLM showed that few-shot prompting led to overall enhancement of detecting HMI errors and discrepancies with the G-code for more comprehensive debugging. Therefore, the proposed framework was demonstrated to be suitable for verification of manually generated G-code that is typically developed in CNC training.",
            "headline_zh": "提出基于少样本视觉语言模型的G代码和HMI验证方法，用于CNC加工中的综合调试。",
            "intro_zh": [
                "核心问题：CNC加工中G代码验证需结合HMI视觉信息，但现有LLM方法无法处理视觉模态。",
                "方法要点：使用少样本VLM，基于结构化JSON提示，同时评估G代码文本和HMI截图中的错误。",
                "实验或效果：在15-slant-PRO车床数据集上，少样本提示相比零-shot提升了HMI错误检测和G代码不一致性识别。"
            ],
            "tags_zh": [
                "少样本学习",
                "视觉语言模型",
                "G代码验证",
                "人机界面",
                "CNC加工",
                "综合调试"
            ],
            "_index": 147
        },
        {
            "title": "AI Autonomy or Human Dependency? Defining the Boundary in Responsible AI with the $α$-Coefficient",
            "authors": [
                "Nattaya Mairittha",
                "Gabriel Phorncharoenmusikul",
                "Sorawit Worapradidth"
            ],
            "arxiv_id": "2512.11295v1",
            "summary": "The integrity of contemporary AI systems is undermined by a critical design flaw: the misappropriation of Human-in-the-Loop (HITL) models to mask systems that are fundamentally reliant on human labor. We term this structural reliance Human-Instead-of-AI (HISOAI). HISOAI systems represent an ethical failure and an unsustainable economic dependency, where human workers function as hidden operational fallbacks rather than strategic collaborators. To rectify this, we propose the AI-First, Human-Empowered (AFHE) paradigm. AFHE mandates a technological design where the AI component must achieve a minimum, quantifiable level of functional independence prior to deployment. This standard is formalized through the AI Autonomy Coefficient (alpha), a metric that determines the proportion of tasks that the AI successfully processes without mandatory human substitution. We introduce the AFHE Deployment Algorithm, an algorithmic gate that requires the system to meet a specified alpha threshold across both offline and shadow testing. By enforcing this structural separation, the AFHE framework redefines the human's role to focus exclusively on high-value tasks, including ethical oversight, boundary pushing, and strategic model tuning, thereby ensuring true system transparency and operational independence. This work advocates for a critical shift toward metric-driven, structurally sound AI architecture, moving the industry beyond deceptive human dependency toward verifiable autonomy.",
            "headline_zh": "提出AI自主系数α和AFHE框架以解决AI系统过度依赖人类劳动的伦理与经济问题。",
            "intro_zh": [
                "核心问题：AI系统设计缺陷导致人类替代AI（HISOAI），掩盖对人工的依赖，引发伦理和经济不可持续性。",
                "方法要点：引入AI自主系数α量化AI功能独立性，提出AFHE范式要求AI在部署前达到α阈值，确保系统透明和自主。",
                "实验或效果：未知，但通过AFHE部署算法强制离线与影子测试，以验证α标准并重构人类角色为高价值任务。"
            ],
            "tags_zh": [
                "AI自主性",
                "人机协作",
                "伦理AI",
                "系统透明度",
                "AFHE框架",
                "α系数"
            ],
            "_index": 148
        },
        {
            "title": "Autoregressive Video Autoencoder with Decoupled Temporal and Spatial Context",
            "authors": [
                "Cuifeng Shen",
                "Lumin Xu",
                "Xingguo Zhu",
                "Gengdai Liu"
            ],
            "arxiv_id": "2512.11293v1",
            "summary": "Video autoencoders compress videos into compact latent representations for efficient reconstruction, playing a vital role in enhancing the quality and efficiency of video generation. However, existing video autoencoders often entangle spatial and temporal information, limiting their ability to capture temporal consistency and leading to suboptimal performance. To address this, we propose Autoregressive Video Autoencoder (ARVAE), which compresses and reconstructs each frame conditioned on its predecessor in an autoregressive manner, allowing flexible processing of videos with arbitrary lengths. ARVAE introduces a temporal-spatial decoupled representation that combines downsampled flow field for temporal coherence with spatial relative compensation for newly emerged content, achieving high compression efficiency without information loss. Specifically, the encoder compresses the current and previous frames into the temporal motion and spatial supplement, while the decoder reconstructs the original frame from the latent representations given the preceding frame. A multi-stage training strategy is employed to progressively optimize the model. Extensive experiments demonstrate that ARVAE achieves superior reconstruction quality with extremely lightweight models and small-scale training data. Moreover, evaluations on video generation tasks highlight its strong potential for downstream applications.",
            "headline_zh": "提出自回归视频自编码器，通过解耦时空上下文以提升视频重建质量与效率。",
            "intro_zh": [
                "现有视频自编码器常纠缠时空信息，导致时间一致性差和性能受限。",
                "ARVAE采用自回归方式逐帧处理，结合流场和空间补偿实现高效无损压缩。",
                "实验显示模型轻量、数据需求小，重建质量优，下游生成任务潜力强。"
            ],
            "tags_zh": [
                "视频自编码器",
                "时空解耦",
                "自回归模型",
                "视频压缩",
                "视频生成"
            ],
            "_index": 149
        },
        {
            "title": "RcAE: Recursive Reconstruction Framework for Unsupervised Industrial Anomaly Detection",
            "authors": [
                "Rongcheng Wu",
                "Hao Zhu",
                "Shiying Zhang",
                "Mingzhe Wang",
                "Zhidong Li",
                "Hui Li",
                "Jianlong Zhou",
                "Jiangtao Cui",
                "Fang Chen",
                "Pingyang Sun",
                "Qiyu Liao",
                "Ye Lin"
            ],
            "arxiv_id": "2512.11284v1",
            "summary": "Unsupervised industrial anomaly detection requires accurately identifying defects without labeled data. Traditional autoencoder-based methods often struggle with incomplete anomaly suppression and loss of fine details, as their single-pass decoding fails to effectively handle anomalies with varying severity and scale. We propose a recursive architecture for autoencoder (RcAE), which performs reconstruction iteratively to progressively suppress anomalies while refining normal structures. Unlike traditional single-pass models, this recursive design naturally produces a sequence of reconstructions, progressively exposing suppressed abnormal patterns. To leverage this reconstruction dynamics, we introduce a Cross Recursion Detection (CRD) module that tracks inconsistencies across recursion steps, enhancing detection of both subtle and large-scale anomalies. Additionally, we incorporate a Detail Preservation Network (DPN) to recover high-frequency textures typically lost during reconstruction. Extensive experiments demonstrate that our method significantly outperforms existing non-diffusion methods, and achieves performance on par with recent diffusion models with only 10% of their parameters and offering substantially faster inference. These results highlight the practicality and efficiency of our approach for real-world applications.",
            "headline_zh": "提出递归自编码器框架RcAE，通过迭代重建渐进抑制工业异常，实现高效无监督检测",
            "intro_zh": [
                "传统自编码器单次解码难以有效处理不同严重程度和尺度的异常，导致异常抑制不完整和细节丢失",
                "RcAE采用递归架构迭代重建，逐步抑制异常并细化正常结构，结合跨递归检测模块和细节保留网络提升检测能力",
                "实验表明该方法显著优于现有非扩散方法，性能与扩散模型相当但参数量仅10%，推理速度大幅提升"
            ],
            "tags_zh": [
                "无监督异常检测",
                "工业视觉",
                "递归自编码器",
                "渐进重建",
                "细节保留",
                "高效推理"
            ],
            "_index": 150
        },
        {
            "title": "CIP: A Plug-and-Play Causal Prompting Framework for Mitigating Hallucinations under Long-Context Noise",
            "authors": [
                "Qingsen Ma",
                "Dianyun Wang",
                "Ran Jing",
                "Yujun Sun",
                "Zhenbo Xu"
            ],
            "arxiv_id": "2512.11282v1",
            "summary": "Large language models often hallucinate when processing long and noisy retrieval contexts because they rely on spurious correlations rather than genuine causal relationships. We propose CIP, a lightweight and plug-and-play causal prompting framework that mitigates hallucinations at the input stage. CIP constructs a causal relation sequence among entities, actions, and events and injects it into the prompt to guide reasoning toward causally relevant evidence. Through causal intervention and counterfactual reasoning, CIP suppresses non causal reasoning paths, improving factual grounding and interpretability. Experiments across seven mainstream language models, including GPT-4o, Gemini 2.0 Flash, and Llama 3.1, show that CIP consistently enhances reasoning quality and reliability, achieving 2.6 points improvement in Attributable Rate, 0.38 improvement in Causal Consistency Score, and a fourfold increase in effective information density. API level profiling further shows that CIP accelerates contextual understanding and reduces end to end response latency by up to 55.1 percent. These results suggest that causal reasoning may serve as a promising paradigm for improving the explainability, stability, and efficiency of large language models.",
            "headline_zh": "提出CIP因果提示框架以缓解长上下文噪声下的幻觉问题",
            "intro_zh": [
                "核心问题：大语言模型在长噪声检索上下文中因依赖伪相关而产生幻觉",
                "方法要点：构建实体-动作-事件因果序列并注入提示，引导因果推理",
                "实验或效果：在七种主流模型中提升归因率和因果一致性，降低响应延迟"
            ],
            "tags_zh": [
                "因果推理",
                "幻觉缓解",
                "长上下文处理",
                "提示工程",
                "大语言模型"
            ],
            "_index": 151
        },
        {
            "title": "When Actions Teach You to Think: Reasoning-Action Synergy via Reinforcement Learning in Conversational Agents",
            "authors": [
                "Mrinal Rawat",
                "Arkajyoti Chakraborty",
                "Neha Gupta",
                "Roberto Pieraccini"
            ],
            "arxiv_id": "2512.11277v1",
            "summary": "Supervised fine-tuning (SFT) has emerged as one of the most effective ways to improve the performance of large language models (LLMs) in downstream tasks. However, SFT can have difficulty generalizing when the underlying data distribution changes, even when the new data does not fall completely outside the training domain. Recent reasoning-focused models such as o1 and R1 have demonstrated consistent gains over their non-reasoning counterparts, highlighting the importance of reasoning for improved generalization and reliability. However, collecting high-quality reasoning traces for SFT remains challenging -- annotations are costly, subjective, and difficult to scale. To address this limitation, we leverage Reinforcement Learning (RL) to enable models to learn reasoning strategies directly from task outcomes. We propose a pipeline in which LLMs generate reasoning steps that guide both the invocation of tools (e.g., function calls) and the final answer generation for conversational agents. Our method employs Group Relative Policy Optimization (GRPO) with rewards designed around tool accuracy and answer correctness, allowing the model to iteratively refine its reasoning and actions. Experimental results demonstrate that our approach improves both the quality of reasoning and the precision of tool invocations, achieving a 1.5% relative improvement over the SFT model (trained without explicit thinking) and a 40% gain compared to the base of the vanilla Qwen3-1.7B model. These findings demonstrate the promise of unifying reasoning and action learning through RL to build more capable and generalizable conversational agents.",
            "headline_zh": "提出基于强化学习的推理-行动协同方法，以提升对话代理的泛化能力与工具调用精度。",
            "intro_zh": [
                "核心问题：监督微调在数据分布变化时泛化困难，高质量推理标注成本高且难扩展。",
                "方法要点：利用强化学习，通过奖励机制（工具准确性和答案正确性）迭代优化推理步骤与工具调用。",
                "实验或效果：相比无显式推理的监督微调模型，相对提升1.5%；相比基础模型，增益达40%。"
            ],
            "tags_zh": [
                "强化学习",
                "推理增强",
                "对话代理",
                "工具调用",
                "泛化能力",
                "策略优化"
            ],
            "_index": 152
        },
        {
            "title": "Words to Describe What I'm Feeling: Exploring the Potential of AI Agents for High Subjectivity Decisions in Advance Care Planning",
            "authors": [
                "Kellie Yu Hui Sim",
                "Pin Sym Foong",
                "Chenyu Zhao",
                "Melanie Yi Ning Quek",
                "Swarangi Subodh Mehta",
                "Kenny Tsu Wei Choo"
            ],
            "arxiv_id": "2512.11276v1",
            "summary": "Serious illness can deprive patients of the capacity to speak for themselves. As populations age and caregiver networks shrink, the need for reliable support in Advance Care Planning (ACP) grows. To probe this fraught design space of using proxy agents for high-risk, high-subjectivity decisions, we built an experience prototype (\\acpagent{}) and asked 15 participants in 4 workshops to train it to be their personal proxy in ACP decisions. We analysed their coping strategies and feature requests and mapped the results onto axes of agent autonomy and human control. Our findings argue for a potential new role of AI in ACP where agents act as personal advocates for individuals, building mutual intelligibility over time. We conclude with design recommendations to balance the risks and benefits of such an agent.",
            "headline_zh": "提出AI代理作为个人倡导者，以支持高风险高主观性的预先护理规划决策。",
            "intro_zh": [
                "核心问题：严重疾病患者可能失去表达能力，需在预先护理规划中处理高风险高主观性决策。",
                "方法要点：构建经验原型，通过工作坊让参与者训练AI代理作为个人代理，分析应对策略和功能需求。",
                "实验或效果：研究发现AI代理可随时间建立相互理解，提出设计建议以平衡风险与益处。"
            ],
            "tags_zh": [
                "AI代理",
                "预先护理规划",
                "高主观性决策",
                "经验原型",
                "设计建议"
            ],
            "_index": 153
        },
        {
            "title": "Towards Logic-Aware Manipulation: A Knowledge Primitive for VLM-Based Assistants in Smart Manufacturing",
            "authors": [
                "Suchang Chen",
                "Daqiang Guo"
            ],
            "arxiv_id": "2512.11275v1",
            "summary": "Existing pipelines for vision-language models (VLMs) in robotic manipulation prioritize broad semantic generalization from images and language, but typically omit execution-critical parameters required for contact-rich actions in manufacturing cells. We formalize an object-centric manipulation-logic schema, serialized as an eight-field tuple τ, which exposes object, interface, trajectory, tolerance, and force/impedance information as a first-class knowledge signal between human operators, VLM-based assistants, and robot controllers. We instantiate τ and a small knowledge base (KB) on a 3D-printer spool-removal task in a collaborative cell, and analyze τ-conditioned VLM planning using plan-quality metrics adapted from recent VLM/LLM planning benchmarks, while demonstrating how the same schema supports taxonomy-tagged data augmentation at training time and logic-aware retrieval-augmented prompting at test time as a building block for assistant systems in smart manufacturing enterprises.",
            "headline_zh": "提出面向智能制造的物体中心操作逻辑模式，以增强视觉语言模型在机器人操作中的执行参数感知能力。",
            "intro_zh": [
                "现有视觉语言模型在机器人操作中缺乏执行关键参数，如力和轨迹，影响制造场景下的接触丰富动作。",
                "定义八字段元组τ作为知识原语，显式编码物体、接口、轨迹、容差和力/阻抗信息，连接操作员、助手和控制器。",
                "在3D打印机线轴移除任务中实例化τ和知识库，评估τ条件下的规划质量，并展示其在训练数据增强和测试时逻辑感知提示中的应用。"
            ],
            "tags_zh": [
                "视觉语言模型",
                "机器人操作",
                "智能制造",
                "知识原语",
                "逻辑感知规划",
                "数据增强"
            ],
            "_index": 154
        },
        {
            "title": "FilmWeaver: Weaving Consistent Multi-Shot Videos with Cache-Guided Autoregressive Diffusion",
            "authors": [
                "Xiangyang Luo",
                "Qingyu Li",
                "Xiaokun Liu",
                "Wenyu Qin",
                "Miao Yang",
                "Meng Wang",
                "Pengfei Wan",
                "Di Zhang",
                "Kun Gai",
                "Shao-Lun Huang"
            ],
            "arxiv_id": "2512.11274v1",
            "summary": "Current video generation models perform well at single-shot synthesis but struggle with multi-shot videos, facing critical challenges in maintaining character and background consistency across shots and flexibly generating videos of arbitrary length and shot count. To address these limitations, we introduce \\textbf{FilmWeaver}, a novel framework designed to generate consistent, multi-shot videos of arbitrary length. First, it employs an autoregressive diffusion paradigm to achieve arbitrary-length video generation. To address the challenge of consistency, our key insight is to decouple the problem into inter-shot consistency and intra-shot coherence. We achieve this through a dual-level cache mechanism: a shot memory caches keyframes from preceding shots to maintain character and scene identity, while a temporal memory retains a history of frames from the current shot to ensure smooth, continuous motion. The proposed framework allows for flexible, multi-round user interaction to create multi-shot videos. Furthermore, due to this decoupled design, our method demonstrates high versatility by supporting downstream tasks such as multi-concept injection and video extension. To facilitate the training of our consistency-aware method, we also developed a comprehensive pipeline to construct a high-quality multi-shot video dataset. Extensive experimental results demonstrate that our method surpasses existing approaches on metrics for both consistency and aesthetic quality, opening up new possibilities for creating more consistent, controllable, and narrative-driven video content. Project Page: https://filmweaver.github.io",
            "headline_zh": "提出FilmWeaver框架，通过缓存引导的自回归扩散生成一致的多镜头视频",
            "intro_zh": [
                "核心问题：现有视频生成模型难以保持多镜头视频中角色和背景的一致性，且无法灵活生成长度和镜头数任意的视频。",
                "方法要点：采用自回归扩散范式实现任意长度生成，通过双级缓存机制（镜头记忆和时序记忆）解耦镜头间一致性和镜头内连贯性。",
                "实验或效果：在一致性和美学质量指标上超越现有方法，支持多概念注入和视频扩展等下游任务，并构建了高质量多镜头视频数据集。"
            ],
            "tags_zh": [
                "多镜头视频生成",
                "一致性保持",
                "自回归扩散模型",
                "缓存机制",
                "视频数据集构建"
            ],
            "_index": 155
        },
        {
            "title": "Integrated Prediction and Multi-period Portfolio Optimization",
            "authors": [
                "Qi Deng",
                "Yuxuan Linghu",
                "Zhiyuan Liu"
            ],
            "arxiv_id": "2512.11273v1",
            "summary": "Multi-period portfolio optimization is important for real portfolio management, as it accounts for transaction costs, path-dependent risks, and the intertemporal structure of trading decisions that single-period models cannot capture. Classical methods usually follow a two-stage framework: machine learning algorithms are employed to produce forecasts that closely fit the realized returns, and the predicted values are then used in a downstream portfolio optimization problem to determine the asset weights. This separation leads to a fundamental misalignment between predictions and decision outcomes, while also ignoring the impact of transaction costs. To bridge this gap, recent studies have proposed the idea of end-to-end learning, integrating the two stages into a single pipeline. This paper introduces IPMO (Integrated Prediction and Multi-period Portfolio Optimization), a model for multi-period mean-variance portfolio optimization with turnover penalties. The predictor generates multi-period return forecasts that parameterize a differentiable convex optimization layer, which in turn drives learning via portfolio performance. For scalability, we introduce a mirror-descent fixed-point (MDFP) differentiation scheme that avoids factorizing the Karush-Kuhn-Tucker (KKT) systems, which thus yields stable implicit gradients and nearly scale-insensitive runtime as the decision horizon grows. In experiments with real market data and two representative time-series prediction models, the IPMO method consistently outperforms the two-stage benchmarks in risk-adjusted performance net of transaction costs and achieves more coherent allocation paths. Our results show that integrating machine learning prediction with optimization in the multi-period setting improves financial outcomes and remains computationally tractable.",
            "headline_zh": "提出IPMO模型以解决多期投资组合优化中预测与决策脱节问题",
            "intro_zh": [
                "核心问题：传统两阶段方法预测与决策脱节，忽略交易成本影响",
                "方法要点：集成预测与优化，采用可微凸优化层和MDFP微分方案提升可扩展性",
                "实验或效果：在真实市场数据上，IPMO在风险调整后净收益和分配路径上优于基准"
            ],
            "tags_zh": [
                "多期投资组合优化",
                "端到端学习",
                "可微优化",
                "交易成本",
                "风险调整收益",
                "时间序列预测"
            ],
            "_index": 156
        },
        {
            "title": "TriFlow: A Progressive Multi-Agent Framework for Intelligent Trip Planning",
            "authors": [
                "Yuxing Chen",
                "Basem Suleiman",
                "Qifan Chen"
            ],
            "arxiv_id": "2512.11271v1",
            "summary": "Real-world trip planning requires transforming open-ended user requests into executable itineraries under strict spatial, temporal, and budgetary constraints while aligning with user preferences. Existing LLM-based agents struggle with constraint satisfaction, tool coordination, and efficiency, often producing infeasible or costly plans. To address these limitations, we present TriFlow, a progressive multi-agent framework that unifies structured reasoning and language-based flexibility through a three-stage pipeline of retrieval, planning, and governance. By this design, TriFlow progressively narrows the search space, assembles constraint-consistent itineraries via rule-LLM collaboration, and performs bounded iterative refinement to ensure global feasibility and personalisation. Evaluations on TravelPlanner and TripTailor benchmarks demonstrated state-of-the-art results, achieving 91.1% and 97% final pass rates, respectively, with over 10x runtime efficiency improvement compared to current SOTA.",
            "headline_zh": "提出TriFlow多智能体框架以解决旅行规划中约束满足和效率问题",
            "intro_zh": [
                "核心问题：现有LLM智能体在旅行规划中难以满足时空预算约束，导致计划不可行或低效",
                "方法要点：采用检索-规划-治理三阶段流水线，结合规则与LLM协作渐进优化行程",
                "实验或效果：在TravelPlanner和TripTailor基准上达到91.1%和97%通过率，运行效率提升超10倍"
            ],
            "tags_zh": [
                "旅行规划",
                "多智能体框架",
                "约束优化",
                "LLM协作",
                "渐进式推理"
            ],
            "_index": 157
        },
        {
            "title": "A-LAMP: Agentic LLM-Based Framework for Automated MDP Modeling and Policy Generation",
            "authors": [
                "Hong Je-Gal",
                "Chan-Bin Yi",
                "Hyun-Suk Lee"
            ],
            "arxiv_id": "2512.11270v1",
            "summary": "Applying reinforcement learning (RL) to real-world tasks requires converting informal descriptions into a formal Markov decision process (MDP), implementing an executable environment, and training a policy agent. Automating this process is challenging due to modeling errors, fragile code, and misaligned objectives, which often impede policy training. We introduce an agentic large language model (LLM)-based framework for automated MDP modeling and policy generation (A-LAMP), that automatically translates free-form natural language task descriptions into an MDP formulation and trained policy. The framework decomposes modeling, coding, and training into verifiable stages, ensuring semantic alignment throughout the pipeline. Across both classic control and custom RL domains, A-LAMP consistently achieves higher policy generation capability than a single state-of-the-art LLM model. Notably, even its lightweight variant, which is built on smaller language models, approaches the performance of much larger models. Failure analysis reveals why these improvements occur. In addition, a case study also demonstrates that A-LAMP generates environments and policies that preserve the task's optimality, confirming its correctness and reliability.",
            "headline_zh": "提出A-LAMP框架，基于智能体化大语言模型自动将自然语言任务描述转化为MDP和训练策略。",
            "intro_zh": [
                "核心问题：强化学习应用中，从非正式描述到MDP建模、环境实现和策略训练的自动化过程易受建模错误、代码脆弱和目标错位阻碍。",
                "方法要点：采用智能体化LLM框架，将建模、编码和训练分解为可验证阶段，确保语义对齐，自动生成MDP和训练策略。",
                "实验或效果：在经典控制和自定义RL领域，A-LAMP比单一先进LLM模型表现更优，轻量版接近大模型性能，案例研究确认其正确性和可靠性。"
            ],
            "tags_zh": [
                "强化学习自动化",
                "MDP建模",
                "智能体化LLM",
                "策略生成",
                "语义对齐",
                "环境生成"
            ],
            "_index": 158
        },
        {
            "title": "A Scalable Multi-GPU Framework for Encrypted Large-Model Inference",
            "authors": [
                "Siddharth Jayashankar",
                "Joshua Kim",
                "Michael B. Sullivan",
                "Wenting Zheng",
                "Dimitrios Skarlatos"
            ],
            "arxiv_id": "2512.11269v1",
            "summary": "Encrypted AI using fully homomorphic encryption (FHE) provides strong privacy guarantees; but its slow performance has limited practical deployment. Recent works proposed ASICs to accelerate FHE, but require expensive advanced manufacturing processes that constrain their accessibility. GPUs are a far more accessible platform, but achieving ASIC-level performance using GPUs has remained elusive. Furthermore, state-of-the-art approaches primarily focus on small models that fit comfortably within a single device. Supporting large models such as LLMs in FHE introduces a dramatic increase in computational complexity that requires optimized GPU kernels, along with managing terabyte-scale memory footprints that far exceed the capacity of a single GPU. This paper presents Cerium, a multi-GPU framework for FHE inference on large models. Cerium integrates a domain-specific language, an optimizing compiler, and a runtime system to automatically generate high-performance GPU kernels, manage terabyte-scale memory footprints, and parallelize computation across multiple GPUs. It introduces new IR constructs, compiler passes, sparse polynomial representations, memory-efficient data layouts, and communication-aware parallelization techniques that together enable encrypted inference for models ranging from small CNNs to Llama3-8B. We build Cerium on NVIDIA GPUs and demonstrate significant performance gains. For small models, Cerium outperforms expert-written hand-optimized GPU libraries by up to 2.25 times. Cerium achieves performance competitive with state-of-the-art FHE ASICs, outright matching prior FHE ASIC CraterLake. It is the first GPU system to execute bootstrapping in under 10 milliseconds, achieving 7.5 milliseconds, and is the first to demonstrate encrypted inference for BERT-Base and Llama3-8B in 8 seconds and 134 seconds, respectively.",
            "headline_zh": "提出Cerium多GPU框架以解决全同态加密大模型推理的性能与内存挑战",
            "intro_zh": [
                "核心问题：全同态加密推理性能慢，大模型内存需求远超单GPU容量，GPU平台难以匹敌ASIC性能",
                "方法要点：集成领域特定语言、优化编译器和运行时系统，自动生成GPU内核，管理TB级内存，多GPU并行计算",
                "实验或效果：性能超越手工优化库2.25倍，匹配先进FHE ASIC，首次实现BERT-Base和Llama3-8B加密推理"
            ],
            "tags_zh": [
                "全同态加密",
                "大模型推理",
                "多GPU框架",
                "编译器优化",
                "内存管理",
                "并行计算"
            ],
            "_index": 159
        },
        {
            "title": "Evaluating the Efficacy of Sentinel-2 versus Aerial Imagery in Serrated Tussock Classification",
            "authors": [
                "Rezwana Sultana",
                "Manzur Murshed",
                "Kathryn Sheffield",
                "Singarayer Florentine",
                "Tsz-Kwan Lee",
                "Shyh Wei Teng"
            ],
            "arxiv_id": "2512.11267v1",
            "summary": "Invasive species pose major global threats to ecosystems and agriculture. Serrated tussock (\\textit{Nassella trichotoma}) is a highly competitive invasive grass species that disrupts native grasslands, reduces pasture productivity, and increases land management costs. In Victoria, Australia, it presents a major challenge due to its aggressive spread and ecological impact. While current ground surveys and subsequent management practices are effective at small scales, they are not feasible for landscape-scale monitoring. Although aerial imagery offers high spatial resolution suitable for detailed classification, its high cost limits scalability. Satellite-based remote sensing provides a more cost-effective and scalable alternative, though often with lower spatial resolution. This study evaluates whether multi-temporal Sentinel-2 imagery, despite its lower spatial resolution, can provide a comparable and cost-effective alternative for landscape-scale monitoring of serrated tussock by leveraging its higher spectral resolution and seasonal phenological information. A total of eleven models have been developed using various combinations of spectral bands, texture features, vegetation indices, and seasonal data. Using a random forest classifier, the best-performing Sentinel-2 model (M76*) has achieved an Overall Accuracy (OA) of 68\\% and an Overall Kappa (OK) of 0.55, slightly outperforming the best-performing aerial imaging model's OA of 67\\% and OK of 0.52 on the same dataset. These findings highlight the potential of multi-seasonal feature-enhanced satellite-based models for scalable invasive species classification.",
            "headline_zh": "评估Sentinel-2与航空影像在锯齿丛草分类中的效能，证明多时相卫星模型可提供可扩展的替代方案。",
            "intro_zh": [
                "核心问题：锯齿丛草作为入侵物种，地面监测成本高，需可扩展的景观尺度分类方法。",
                "方法要点：利用Sentinel-2多时相影像，结合光谱、纹理和植被指数，开发随机森林分类模型。",
                "实验或效果：最佳Sentinel-2模型总体准确率68%，略优于航空影像模型，突显卫星模型的成本效益潜力。"
            ],
            "tags_zh": [
                "入侵物种分类",
                "遥感监测",
                "多时相分析",
                "随机森林分类",
                "景观尺度评估"
            ],
            "_index": 160
        },
        {
            "title": "Features Emerge as Discrete States: The First Application of SAEs to 3D Representations",
            "authors": [
                "Albert Miao",
                "Chenliang Zhou",
                "Jiawei Zhou",
                "Cengiz Oztireli"
            ],
            "arxiv_id": "2512.11263v1",
            "summary": "Sparse Autoencoders (SAEs) are a powerful dictionary learning technique for decomposing neural network activations, translating the hidden state into human ideas with high semantic value despite no external intervention or guidance. However, this technique has rarely been applied outside of the textual domain, limiting theoretical explorations of feature decomposition. We present the \\textbf{first application of SAEs to the 3D domain}, analyzing the features used by a state-of-the-art 3D reconstruction VAE applied to 53k 3D models from the Objaverse dataset. We observe that the network encodes discrete rather than continuous features, leading to our key finding: \\textbf{such models approximate a discrete state space, driven by phase-like transitions from feature activations}. Through this state transition framework, we address three otherwise unintuitive behaviors -- the inclination of the reconstruction model towards positional encoding representations, the sigmoidal behavior of reconstruction loss from feature ablation, and the bimodality in the distribution of phase transition points. This final observation suggests the model \\textbf{redistributes the interference caused by superposition to prioritize the saliency of different features}. Our work not only compiles and explains unexpected phenomena regarding feature decomposition, but also provides a framework to explain the model's feature learning dynamics. The code and dataset of encoded 3D objects will be available on release.",
            "headline_zh": "首次将稀疏自编码器应用于3D表示，揭示特征以离散状态涌现并解释模型行为。",
            "intro_zh": [
                "核心问题：稀疏自编码器在文本域外应用有限，阻碍特征分解理论探索。",
                "方法要点：分析3D重建VAE在Objaverse数据集上的特征，发现离散而非连续编码。",
                "实验或效果：观察到相变驱动离散状态空间，解释位置编码偏好和损失行为等现象。"
            ],
            "tags_zh": [
                "稀疏自编码器",
                "3D表示学习",
                "特征分解",
                "离散状态空间",
                "相变分析",
                "VAE重建"
            ],
            "_index": 161
        },
        {
            "title": "Do We Need Reformer for Vision? An Experimental Comparison with Vision Transformers",
            "authors": [
                "Ali El Bellaj",
                "Mohammed-Amine Cheddadi",
                "Rhassan Berber"
            ],
            "arxiv_id": "2512.11260v1",
            "summary": "Transformers have recently demonstrated strong performance in computer vision, with Vision Transformers (ViTs) leveraging self-attention to capture both low-level and high-level image features. However, standard ViTs remain computationally expensive, since global self-attention scales quadratically with the number of tokens, which limits their practicality for high-resolution inputs and resource-constrained settings.\n  In this work, we investigate the Reformer architecture as an alternative vision backbone. By combining patch-based tokenization with locality-sensitive hashing (LSH) attention, our model approximates global self-attention while reducing its theoretical time complexity from $\\mathcal{O}(n^2)$ to $\\mathcal{O}(n \\log n)$ in the sequence length $n$. We evaluate the proposed Reformer-based vision model on CIFAR-10 to assess its behavior on small-scale datasets, on ImageNet-100 to study its accuracy--efficiency trade-off in a more realistic setting, and on a high-resolution medical imaging dataset to evaluate the model under longer token sequences.\n  While the Reformer achieves higher accuracy on CIFAR-10 compared to our ViT-style baseline, the ViT model consistently outperforms the Reformer in our experiments in terms of practical efficiency and end-to-end computation time across the larger and higher-resolution settings. These results suggest that, despite the theoretical advantages of LSH-based attention, meaningful computation gains require sequence lengths substantially longer than those produced by typical high-resolution images.",
            "headline_zh": "比较Reformer与Vision Transformer在视觉任务中的效率与性能",
            "intro_zh": [
                "核心问题：标准Vision Transformer因全局自注意力计算复杂度高，限制高分辨率输入应用。",
                "方法要点：采用Reformer架构，结合局部敏感哈希注意力降低理论复杂度至O(n log n)。",
                "实验或效果：在CIFAR-10上Reformer更准确，但在更大规模和高分辨率设置中ViT实际效率更高。"
            ],
            "tags_zh": [
                "视觉Transformer",
                "Reformer架构",
                "局部敏感哈希注意力",
                "计算效率",
                "高分辨率图像",
                "实验比较"
            ],
            "_index": 162
        },
        {
            "title": "Multi-Intent Spoken Language Understanding: Methods, Trends, and Challenges",
            "authors": [
                "Di Wu",
                "Ruiyu Fang",
                "Liting Jiang",
                "Shuangyong Song",
                "Xiaomeng Huang",
                "Shiquan Wang",
                "Zhongqiu Li",
                "Lingling Shi",
                "Mengjiao Bao",
                "Yongxiang Li",
                "Hao Huang"
            ],
            "arxiv_id": "2512.11258v1",
            "summary": "Multi-intent spoken language understanding (SLU) involves two tasks: multiple intent detection and slot filling, which jointly handle utterances containing more than one intent. Owing to this characteristic, which closely reflects real-world applications, the task has attracted increasing research attention, and substantial progress has been achieved. However, there remains a lack of a comprehensive and systematic review of existing studies on multi-intent SLU. To this end, this paper presents a survey of recent advances in multi-intent SLU. We provide an in-depth overview of previous research from two perspectives: decoding paradigms and modeling approaches. On this basis, we further compare the performance of representative models and analyze their strengths and limitations. Finally, we discuss the current challenges and outline promising directions for future research. We hope this survey will offer valuable insights and serve as a useful reference for advancing research in multi-intent SLU.",
            "headline_zh": "综述多意图口语理解的方法、趋势与挑战，提供系统回顾与未来方向",
            "intro_zh": [
                "核心问题：多意图口语理解涉及多意图检测与槽填充，处理含多个意图的语句，反映真实应用需求",
                "方法要点：从解码范式和建模方法两个角度深入概述现有研究，比较代表性模型性能并分析优劣",
                "实验或效果：未知具体实验细节，但强调任务已取得显著进展，并讨论当前挑战与未来研究方向"
            ],
            "tags_zh": [
                "多意图口语理解",
                "意图检测",
                "槽填充",
                "解码范式",
                "建模方法",
                "综述研究"
            ],
            "_index": 163
        },
        {
            "title": "A Simple Generalisation of the Implicit Dynamics of In-Context Learning",
            "authors": [
                "Francesco Innocenti",
                "El Mehdi Achour"
            ],
            "arxiv_id": "2512.11255v1",
            "summary": "In-context learning (ICL) refers to the ability of a model to learn new tasks from examples in its input without any parameter updates. In contrast to previous theories of ICL relying on toy models and data settings, recently it has been shown that an abstraction of a transformer block can be seen as implicitly updating the weights of its feedforward network according to the context (Dherin et al., 2025). Here, we provide a simple generalisation of this result for (i) all sequence positions beyond the last, (ii) any transformer block beyond the first, and (iii) more realistic residual blocks including layer normalisation. We empirically verify our theory on simple in-context linear regression tasks and investigate the relationship between the implicit updates related to different tokens within and between blocks. These results help to bring the theory of Dherin et al. (2025) even closer to practice, with potential for validation on large-scale models.",
            "headline_zh": "提出上下文学习隐式动态的简单泛化，扩展至所有序列位置、任意Transformer块及更现实的残差块。",
            "intro_zh": [
                "核心问题：上下文学习（ICL）中模型如何从输入示例学习新任务，无需参数更新。",
                "方法要点：泛化Dherin等人（2025）的理论，涵盖所有序列位置、任意Transformer块和包含层归一化的残差块。",
                "实验或效果：在简单上下文线性回归任务上实证验证，并探究块内和块间不同令牌的隐式更新关系。"
            ],
            "tags_zh": [
                "上下文学习",
                "Transformer",
                "隐式动态",
                "线性回归",
                "残差块",
                "层归一化"
            ],
            "_index": 164
        },
        {
            "title": "PersonaLive! Expressive Portrait Image Animation for Live Streaming",
            "authors": [
                "Zhiyuan Li",
                "Chi-Man Pun",
                "Chen Fang",
                "Jue Wang",
                "Xiaodong Cun"
            ],
            "arxiv_id": "2512.11253v1",
            "summary": "Current diffusion-based portrait animation models predominantly focus on enhancing visual quality and expression realism, while overlooking generation latency and real-time performance, which restricts their application range in the live streaming scenario. We propose PersonaLive, a novel diffusion-based framework towards streaming real-time portrait animation with multi-stage training recipes. Specifically, we first adopt hybrid implicit signals, namely implicit facial representations and 3D implicit keypoints, to achieve expressive image-level motion control. Then, a fewer-step appearance distillation strategy is proposed to eliminate appearance redundancy in the denoising process, greatly improving inference efficiency. Finally, we introduce an autoregressive micro-chunk streaming generation paradigm equipped with a sliding training strategy and a historical keyframe mechanism to enable low-latency and stable long-term video generation. Extensive experiments demonstrate that PersonaLive achieves state-of-the-art performance with up to 7-22x speedup over prior diffusion-based portrait animation models.",
            "headline_zh": "提出PersonaLive框架，通过混合隐式信号和蒸馏策略实现直播场景下的实时肖像动画。",
            "intro_zh": [
                "当前扩散模型在肖像动画中忽视生成延迟和实时性能，限制直播应用。",
                "采用混合隐式信号和较少步数外观蒸馏，提升运动控制和推理效率。",
                "实验显示PersonaLive在性能上达到先进水平，速度提升7-22倍。"
            ],
            "tags_zh": [
                "肖像动画",
                "扩散模型",
                "实时生成",
                "隐式表示",
                "蒸馏训练",
                "直播应用"
            ],
            "_index": 165
        },
        {
            "title": "Insight Miner: A Time Series Analysis Dataset for Cross-Domain Alignment with Natural Language",
            "authors": [
                "Yunkai Zhang",
                "Yawen Zhang",
                "Ming Zheng",
                "Kezhen Chen",
                "Chongyang Gao",
                "Ruian Ge",
                "Siyuan Teng",
                "Amine Jelloul",
                "Jinmeng Rao",
                "Xiaoyuan Guo",
                "Chiang-Wei Fang",
                "Zeyu Zheng",
                "Jie Yang"
            ],
            "arxiv_id": "2512.11251v1",
            "summary": "Time-series data is critical across many scientific and industrial domains, including environmental analysis, agriculture, transportation, and finance. However, mining insights from this data typically requires deep domain expertise, a process that is both time-consuming and labor-intensive. In this paper, we propose \\textbf{Insight Miner}, a large-scale multimodal model (LMM) designed to generate high-quality, comprehensive time-series descriptions enriched with domain-specific knowledge. To facilitate this, we introduce \\textbf{TS-Insights}\\footnote{Available at \\href{https://huggingface.co/datasets/zhykoties/time-series-language-alignment}{https://huggingface.co/datasets/zhykoties/time-series-language-alignment}.}, the first general-domain dataset for time series and language alignment. TS-Insights contains 100k time-series windows sampled from 20 forecasting datasets. We construct this dataset using a novel \\textbf{agentic workflow}, where we use statistical tools to extract features from raw time series before synthesizing them into coherent trend descriptions with GPT-4. Following instruction tuning on TS-Insights, Insight Miner outperforms state-of-the-art multimodal models, such as LLaVA \\citep{liu2023llava} and GPT-4, in generating time-series descriptions and insights. Our findings suggest a promising direction for leveraging LMMs in time series analysis, and serve as a foundational step toward enabling LLMs to interpret time series as a native input modality.",
            "headline_zh": "提出Insight Miner模型与TS-Insights数据集，以解决跨领域时间序列分析中依赖专家知识的问题。",
            "intro_zh": [
                "核心问题：时间序列分析需深度领域知识，过程耗时费力。",
                "方法要点：构建TS-Insights数据集，通过代理工作流合成时间序列与语言对齐数据。",
                "实验或效果：Insight Miner在指令调优后，在生成描述和洞察方面优于LLaVA和GPT-4等模型。"
            ],
            "tags_zh": [
                "时间序列分析",
                "多模态模型",
                "语言对齐",
                "数据集构建",
                "指令调优",
                "跨领域应用"
            ],
            "_index": 166
        },
        {
            "title": "Optimal Control and Structurally-Informed Gradient Optimization of a Custom 4-DOF Rigid-Body Manipulator",
            "authors": [
                "Brock Marcinczyk",
                "Logan E. Beaver"
            ],
            "arxiv_id": "2512.11250v1",
            "summary": "This work develops a control-centric framework for a custom 4-DOF rigid-body manipulator by coupling a reduced-order Pontryagin's Maximum Principle (PMP) controller with a physics-informed Gradient Descent stage. The reduced PMP model provides a closed-form optimal control law for the joint accelerations, while the Gradient Descent module determines the corresponding time horizons by minimizing a cost functional built directly from the full Rigid-Body Dynamics. Structural-mechanics reaction analysis is used only to initialize feasible joint velocities-most critically the azimuthal component-ensuring that the optimizer begins in a physically admissible region. The resulting kinematic trajectories and dynamically consistent time horizons are then supplied to the symbolic Euler-Lagrange model to yield closed-form inverse-dynamics inputs. This pipeline preserves a strict control-theoretic structure while embedding the physical constraints and loading behavior of the manipulator in a computationally efficient way.",
            "headline_zh": "提出结合降阶PMP控制器与物理梯度下降的框架，优化4自由度刚性机械臂控制。",
            "intro_zh": [
                "核心问题：为定制4自由度刚性机械臂开发控制框架，确保物理可行性和计算效率。",
                "方法要点：使用降阶PMP提供关节加速度最优控制律，梯度下降优化时间范围，基于结构力学初始化。",
                "实验或效果：生成运动轨迹和时间范围，输入符号欧拉-拉格朗日模型，得到闭式逆动力学输入。"
            ],
            "tags_zh": [
                "刚性机械臂控制",
                "最优控制",
                "梯度下降优化",
                "物理约束嵌入",
                "逆动力学"
            ],
            "_index": 167
        },
        {
            "title": "Elevation Aware 2D/3D Co-simulation Framework for Large-scale Traffic Flow and High-fidelity Vehicle Dynamics",
            "authors": [
                "Chandra Raskoti",
                "Weizi Li"
            ],
            "arxiv_id": "2512.11249v1",
            "summary": "Reliable testing of autonomous driving systems requires simulation environments that combine large-scale traffic modeling with realistic 3D perception and terrain. Existing tools rarely capture real-world elevation, limiting their usefulness in cities with complex topography. This paper presents an automated, elevation-aware co-simulation framework that integrates SUMO with CARLA using a pipeline that fuses OpenStreetMap road networks and USGS elevation data into physically consistent 3D environments. The system generates smooth elevation profiles, validates geometric accuracy, and enables synchronized 2D-3D simulation across platforms. Demonstrations on multiple regions of San Francisco show the framework's scalability and ability to reproduce steep and irregular terrain. The result is a practical foundation for high-fidelity autonomous vehicle testing in realistic, elevation-rich urban settings.",
            "headline_zh": "提出自动化高程感知协同仿真框架，以支持复杂地形下自动驾驶系统的高保真测试。",
            "intro_zh": [
                "核心问题：现有仿真工具常忽略真实高程，限制在复杂地形城市中的自动驾驶测试可靠性。",
                "方法要点：集成SUMO与CARLA，融合OpenStreetMap路网和USGS高程数据，生成物理一致的3D环境。",
                "实验或效果：在旧金山多个区域演示，验证了框架的可扩展性和再现陡峭不规则地形的能力。"
            ],
            "tags_zh": [
                "自动驾驶仿真",
                "高程感知",
                "协同仿真",
                "SUMO-CARLA集成",
                "地形建模",
                "高保真测试"
            ],
            "_index": 168
        },
        {
            "title": "Multi-Objective Reinforcement Learning for Large-Scale Mixed Traffic Control",
            "authors": [
                "Iftekharul Islam",
                "Weizi Li"
            ],
            "arxiv_id": "2512.11247v1",
            "summary": "Effective mixed traffic control requires balancing efficiency, fairness, and safety. Existing approaches excel at optimizing efficiency and enforcing safety constraints but lack mechanisms to ensure equitable service, resulting in systematic starvation of vehicles on low-demand approaches. We propose a hierarchical framework combining multi-objective reinforcement learning for local intersection control with strategic routing for network-level coordination. Our approach introduces a Conflict Threat Vector that provides agents with explicit risk signals for proactive conflict avoidance, and a queue parity penalty that ensures equitable service across all traffic streams. Extensive experiments on a real-world network across different robot vehicle (RV) penetration rates demonstrate substantial improvements: up to 53% reductions in average wait time, up to 86% reductions in maximum starvation, and up to 86\\% reduction in conflict rate compared to baselines, while maintaining fuel efficiency. Our analysis reveals that strategic routing effectiveness scales with RV penetration, becoming increasingly valuable at higher autonomy levels. The results demonstrate that multi-objective optimization through well-curated reward functions paired with strategic RV routing yields significant benefits in fairness and safety metrics critical for equitable mixed-autonomy deployment.",
            "headline_zh": "提出多目标强化学习与战略路由的混合交通控制框架，以提升公平性、安全性和效率。",
            "intro_zh": [
                "核心问题：现有方法缺乏公平性机制，导致低需求车辆服务不足。",
                "方法要点：结合多目标强化学习进行局部控制，引入冲突威胁向量和队列均等惩罚。",
                "实验或效果：在真实网络中显著减少等待时间、饥饿和冲突率，同时保持燃油效率。"
            ],
            "tags_zh": [
                "混合交通控制",
                "多目标强化学习",
                "战略路由",
                "公平性优化",
                "冲突避免"
            ],
            "_index": 169
        },
        {
            "title": "Task-Aware Multi-Expert Architecture For Lifelong Deep Learning",
            "authors": [
                "Jianyu Wang",
                "Jacob Nean-Hua Sheikh",
                "Cat P. Le",
                "Hoda Bidkhori"
            ],
            "arxiv_id": "2512.11243v1",
            "summary": "Lifelong deep learning (LDL) trains neural networks to learn sequentially across tasks while preserving prior knowledge. We propose Task-Aware Multi-Expert (TAME), a continual learning algorithm that leverages task similarity to guide expert selection and knowledge transfer. TAME maintains a pool of pretrained neural networks and activates the most relevant expert for each new task. A shared dense layer integrates features from the chosen expert to generate predictions. To reduce catastrophic forgetting, TAME uses a replay buffer that stores representative samples and embeddings from previous tasks and reuses them during training. An attention mechanism further prioritizes the most relevant stored information for each prediction. Together, these components allow TAME to adapt flexibly while retaining important knowledge across evolving task sequences. Experiments on binary classification tasks derived from CIFAR-100 show that TAME improves accuracy on new tasks while sustaining performance on earlier ones, highlighting its effectiveness in balancing adaptation and retention in lifelong learning settings.",
            "headline_zh": "提出任务感知多专家架构以解决终身深度学习中的灾难性遗忘问题",
            "intro_zh": [
                "核心问题：终身深度学习需顺序学习任务同时避免遗忘旧知识",
                "方法要点：基于任务相似性选择专家，结合重播缓冲和注意力机制",
                "实验或效果：在CIFAR-100二分类任务上提升新任务准确率并维持旧任务性能"
            ],
            "tags_zh": [
                "终身深度学习",
                "灾难性遗忘",
                "多专家架构",
                "任务相似性",
                "重播缓冲",
                "注意力机制"
            ],
            "_index": 170
        },
        {
            "title": "Cross-modal Prompting for Balanced Incomplete Multi-modal Emotion Recognition",
            "authors": [
                "Wen-Jue He",
                "Xiaofeng Zhu",
                "Zheng Zhang"
            ],
            "arxiv_id": "2512.11239v1",
            "summary": "Incomplete multi-modal emotion recognition (IMER) aims at understanding human intentions and sentiments by comprehensively exploring the partially observed multi-source data. Although the multi-modal data is expected to provide more abundant information, the performance gap and modality under-optimization problem hinder effective multi-modal learning in practice, and are exacerbated in the confrontation of the missing data. To address this issue, we devise a novel Cross-modal Prompting (ComP) method, which emphasizes coherent information by enhancing modality-specific features and improves the overall recognition accuracy by boosting each modality's performance. Specifically, a progressive prompt generation module with a dynamic gradient modulator is proposed to produce concise and consistent modality semantic cues. Meanwhile, cross-modal knowledge propagation selectively amplifies the consistent information in modality features with the delivered prompts to enhance the discrimination of the modality-specific output. Additionally, a coordinator is designed to dynamically re-weight the modality outputs as a complement to the balance strategy to improve the model's efficacy. Extensive experiments on 4 datasets with 7 SOTA methods under different missing rates validate the effectiveness of our proposed method.",
            "headline_zh": "提出跨模态提示方法以解决不完整多模态情感识别中的性能差距和模态欠优化问题。",
            "intro_zh": [
                "核心问题：不完整多模态数据导致性能差距和模态欠优化，阻碍情感识别效果。",
                "方法要点：设计跨模态提示方法，通过渐进提示生成和动态梯度调制增强模态特征一致性。",
                "实验或效果：在4个数据集上验证，优于7种先进方法，提升识别准确率。"
            ],
            "tags_zh": [
                "不完整多模态情感识别",
                "跨模态提示",
                "动态梯度调制",
                "模态特征增强",
                "知识传播",
                "平衡策略"
            ],
            "_index": 171
        },
        {
            "title": "WildCap: Facial Appearance Capture in the Wild via Hybrid Inverse Rendering",
            "authors": [
                "Yuxuan Han",
                "Xin Ming",
                "Tianxiao Li",
                "Zhuofan Shen",
                "Qixuan Zhang",
                "Lan Xu",
                "Feng Xu"
            ],
            "arxiv_id": "2512.11237v1",
            "summary": "Existing methods achieve high-quality facial appearance capture under controllable lighting, which increases capture cost and limits usability. We propose WildCap, a novel method for high-quality facial appearance capture from a smartphone video recorded in the wild. To disentangle high-quality reflectance from complex lighting effects in in-the-wild captures, we propose a novel hybrid inverse rendering framework. Specifically, we first apply a data-driven method, i.e., SwitchLight, to convert the captured images into more constrained conditions and then adopt model-based inverse rendering. However, unavoidable local artifacts in network predictions, such as shadow-baking, are non-physical and thus hinder accurate inverse rendering of lighting and material. To address this, we propose a novel texel grid lighting model to explain non-physical effects as clean albedo illuminated by local physical lighting. During optimization, we jointly sample a diffusion prior for reflectance maps and optimize the lighting, effectively resolving scale ambiguity between local lights and albedo. Our method achieves significantly better results than prior arts in the same capture setup, closing the quality gap between in-the-wild and controllable recordings by a large margin. Our code will be released \\href{https://yxuhan.github.io/WildCap/index.html}{\\textcolor{magenta}{here}}.",
            "headline_zh": "提出WildCap方法，通过混合逆渲染在野外智能手机视频中实现高质量面部外观捕捉。",
            "intro_zh": [
                "现有方法在可控光照下实现高质量面部捕捉，但野外捕获成本高且受限。",
                "采用混合逆渲染框架，先数据驱动转换图像，再模型优化，解决光照与材质分离问题。",
                "提出纹理网格光照模型，结合扩散先验优化，显著提升野外捕获质量，缩小与可控记录的差距。"
            ],
            "tags_zh": [
                "面部外观捕捉",
                "混合逆渲染",
                "野外捕获",
                "智能手机视频",
                "光照分离",
                "纹理网格模型"
            ],
            "_index": 172
        },
        {
            "title": "RoomPilot: Controllable Synthesis of Interactive Indoor Environments via Multimodal Semantic Parsing",
            "authors": [
                "Wentang Chen",
                "Shougao Zhang",
                "Yiman Zhang",
                "Tianhao Zhou",
                "Ruihui Li"
            ],
            "arxiv_id": "2512.11234v1",
            "summary": "Generating controllable and interactive indoor scenes is fundamental to applications in game development, architectural visualization, and embodied AI training. Yet existing approaches either handle a narrow range of input modalities or rely on stochastic processes that hinder controllability. To overcome these limitations, we introduce RoomPilot, a unified framework that parses diverse multi-modal inputs--textual descriptions or CAD floor plans--into an Indoor Domain-Specific Language (IDSL) for indoor structured scene generation. The key insight is that a well-designed IDSL can act as a shared semantic representation, enabling coherent, high-quality scene synthesis from any single modality while maintaining interaction semantics. In contrast to conventional procedural methods that produce visually plausible but functionally inert layouts, RoomPilot leverages a curated dataset of interaction-annotated assets to synthesize environments exhibiting realistic object behaviors. Extensive experiments further validate its strong multi-modal understanding, fine-grained controllability in scene generation, and superior physical consistency and visual fidelity, marking a significant step toward general-purpose controllable 3D indoor scene generation.",
            "headline_zh": "提出RoomPilot框架，通过多模态语义解析实现可控交互式室内场景合成",
            "intro_zh": [
                "核心问题：现有方法输入模态有限或依赖随机过程，难以生成可控交互式室内场景。",
                "方法要点：设计室内领域特定语言（IDSL）作为共享语义表示，解析文本或CAD平面图以生成结构化场景。",
                "实验或效果：验证了多模态理解能力、细粒度可控性，以及物理一致性和视觉保真度的提升。"
            ],
            "tags_zh": [
                "室内场景生成",
                "多模态语义解析",
                "可控合成",
                "交互式环境",
                "领域特定语言"
            ],
            "_index": 173
        },
        {
            "title": "REST: Diffusion-based Real-time End-to-end Streaming Talking Head Generation via ID-Context Caching and Asynchronous Streaming Distillation",
            "authors": [
                "Haotian Wang",
                "Yuzhe Weng",
                "Xinyi Yu",
                "Jun Du",
                "Haoran Xu",
                "Xiaoyan Wu",
                "Shan He",
                "Bing Yin",
                "Cong Liu",
                "Qingfeng Liu"
            ],
            "arxiv_id": "2512.11229v1",
            "summary": "Diffusion models have significantly advanced the field of talking head generation. However, the slow inference speeds and non-autoregressive paradigms severely constrain the application of diffusion-based THG models. In this study, we propose REST, the first diffusion-based, real-time, end-to-end streaming audio-driven talking head generation framework. To support real-time end-to-end generation, a compact video latent space is first learned through high spatiotemporal VAE compression. Additionally, to enable autoregressive streaming within the compact video latent space, we introduce an ID-Context Cache mechanism, which integrates ID-Sink and Context-Cache principles to key-value caching for maintaining temporal consistency and identity coherence during long-time streaming generation. Furthermore, an Asynchronous Streaming Distillation (ASD) training strategy is proposed to mitigate error accumulation in autoregressive generation and enhance temporal consistency, which leverages a non-streaming teacher with an asynchronous noise schedule to supervise the training of the streaming student model. REST bridges the gap between autoregressive and diffusion-based approaches, demonstrating substantial value for applications requiring real-time talking head generation. Experimental results demonstrate that REST outperforms state-of-the-art methods in both generation speed and overall performance.",
            "headline_zh": "提出REST框架，通过ID-Context缓存和异步流蒸馏实现基于扩散模型的实时端到端流式说话头生成。",
            "intro_zh": [
                "扩散模型在说话头生成中推理慢且非自回归，限制实时应用。",
                "引入ID-Context缓存机制和异步流蒸馏训练策略，提升时序一致性和身份连贯性。",
                "实验显示REST在生成速度和整体性能上优于现有方法，支持实时流式生成。"
            ],
            "tags_zh": [
                "说话头生成",
                "扩散模型",
                "实时流式生成",
                "ID-Context缓存",
                "异步蒸馏训练",
                "视频潜在空间压缩"
            ],
            "_index": 174
        },
        {
            "title": "FutureX: Enhance End-to-End Autonomous Driving via Latent Chain-of-Thought World Model",
            "authors": [
                "Hongbin Lin",
                "Yiming Yang",
                "Yifan Zhang",
                "Chaoda Zheng",
                "Jie Feng",
                "Sheng Wang",
                "Zhennan Wang",
                "Shijia Chen",
                "Boyang Wang",
                "Yu Zhang",
                "Xianming Liu",
                "Shuguang Cui",
                "Zhen Li"
            ],
            "arxiv_id": "2512.11226v1",
            "summary": "In autonomous driving, end-to-end planners learn scene representations from raw sensor data and utilize them to generate a motion plan or control actions. However, exclusive reliance on the current scene for motion planning may result in suboptimal responses in highly dynamic traffic environments where ego actions further alter the future scene. To model the evolution of future scenes, we leverage the World Model to represent how the ego vehicle and its environment interact and change over time, which entails complex reasoning. The Chain of Thought (CoT) offers a promising solution by forecasting a sequence of future thoughts that subsequently guide trajectory refinement. In this paper, we propose FutureX, a CoT-driven pipeline that enhances end-to-end planners to perform complex motion planning via future scene latent reasoning and trajectory refinement. Specifically, the Auto-think Switch examines the current scene and decides whether additional reasoning is required to yield a higher-quality motion plan. Once FutureX enters the Thinking mode, the Latent World Model conducts a CoT-guided rollout to predict future scene representation, enabling the Summarizer Module to further refine the motion plan. Otherwise, FutureX operates in an Instant mode to generate motion plans in a forward pass for relatively simple scenes. Extensive experiments demonstrate that FutureX enhances existing methods by producing more rational motion plans and fewer collisions without compromising efficiency, thereby achieving substantial overall performance gains, e.g., 6.2 PDMS improvement for TransFuser on NAVSIM. Code will be released.",
            "headline_zh": "提出FutureX以增强端到端自动驾驶规划，通过潜在思维链世界模型进行未来场景推理和轨迹优化。",
            "intro_zh": [
                "核心问题：端到端规划器仅依赖当前场景，在动态交通环境中可能导致次优响应。",
                "方法要点：引入自动思维开关和潜在世界模型，通过思维链预测未来场景表示以优化轨迹。",
                "实验或效果：在NAVSIM上提升TransFuser 6.2 PDMS，减少碰撞且不牺牲效率。"
            ],
            "tags_zh": [
                "自动驾驶规划",
                "世界模型",
                "思维链推理",
                "端到端学习",
                "轨迹优化"
            ],
            "_index": 175
        },
        {
            "title": "VFMF: World Modeling by Forecasting Vision Foundation Model Features",
            "authors": [
                "Gabrijel Boduljak",
                "Yushi Lan",
                "Christian Rupprecht",
                "Andrea Vedaldi"
            ],
            "arxiv_id": "2512.11225v1",
            "summary": "Forecasting from partial observations is central to world modeling. Many recent methods represent the world through images, and reduce forecasting to stochastic video generation. Although such methods excel at realism and visual fidelity, predicting pixels is computationally intensive and not directly useful in many applications, as it requires translating RGB into signals useful for decision making. An alternative approach uses features from vision foundation models (VFMs) as world representations, performing deterministic regression to predict future world states. These features can be directly translated into actionable signals such as semantic segmentation and depth, while remaining computationally efficient. However, deterministic regression averages over multiple plausible futures, undermining forecast accuracy by failing to capture uncertainty. To address this crucial limitation, we introduce a generative forecaster that performs autoregressive flow matching in VFM feature space. Our key insight is that generative modeling in this space requires encoding VFM features into a compact latent space suitable for diffusion. We show that this latent space preserves information more effectively than previously used PCA-based alternatives, both for forecasting and other applications, such as image generation. Our latent predictions can be easily decoded into multiple useful and interpretable output modalities: semantic segmentation, depth, surface normals, and even RGB. With matched architecture and compute, our method produces sharper and more accurate predictions than regression across all modalities. Our results suggest that stochastic conditional generation of VFM features offers a promising and scalable foundation for future world models.",
            "headline_zh": "提出VFMF方法，通过预测视觉基础模型特征进行世界建模，以解决确定性回归中不确定性捕获不足的问题。",
            "intro_zh": [
                "核心问题：确定性回归在预测视觉基础模型特征时平均化多种可能未来，导致不确定性捕获不足，影响预测准确性。",
                "方法要点：采用自回归流匹配在视觉基础模型特征空间进行生成式预测，将特征编码到紧凑潜在空间以支持扩散模型。",
                "实验或效果：在匹配架构和计算下，相比回归方法，在所有输出模态（如语义分割、深度）上产生更清晰和准确的预测。"
            ],
            "tags_zh": [
                "世界建模",
                "特征预测",
                "生成式模型",
                "视觉基础模型",
                "不确定性捕获",
                "多模态输出"
            ],
            "_index": 176
        },
        {
            "title": "Adaptive Soft Rolling KV Freeze with Entropy-Guided Recovery: Sublinear Memory Growth for Efficient LLM Inference",
            "authors": [
                "Adilet Metinov",
                "Gulida M. Kudakeeva",
                "Bolotbek uulu Nursultan",
                "Gulnara D. Kabaeva"
            ],
            "arxiv_id": "2512.11221v1",
            "summary": "We present Adaptive Soft Rolling KV Freeze with Entropy-Guided Recovery (ASR-KF-EGR), a training-free inference-time framework for efficient large language model generation. Our method introduces a reversible soft-freeze mechanism that temporarily suspends key-value (KV) updates for low-importance tokens identified within a sliding attention window. Unlike eviction-based approaches that permanently discard context, ASR-KF-EGR preserves all tokens in off-GPU storage and restores them on demand. We extend the framework with sublinear freeze scheduling, where freeze duration grows sublinearly with repeated low-importance detections, preventing over-aggressive compression. Preliminary experiments on LLaMA-3 8B demonstrate 55-67% reduction in active KV cache size while maintaining generation quality and passing needle-in-haystack retrieval tests. The method is architecture-agnostic, requires no fine-tuning, and provides a practical solution for memory-constrained deployment of long-context LLMs.",
            "headline_zh": "提出自适应软滚动KV冻结与熵引导恢复框架，以解决长上下文LLM推理中的内存效率问题。",
            "intro_zh": [
                "核心问题：长上下文LLM推理时KV缓存内存增长快，影响部署效率。",
                "方法要点：基于滑动注意力窗口识别低重要性token，可逆软冻结其KV更新，结合熵引导恢复和次线性调度。",
                "实验或效果：在LLaMA-3 8B上，主动KV缓存大小减少55-67%，保持生成质量并通过检索测试。"
            ],
            "tags_zh": [
                "KV缓存优化",
                "推理效率",
                "长上下文处理",
                "训练无关方法",
                "内存管理",
                "注意力机制"
            ],
            "_index": 177
        },
        {
            "title": "Latent Variable Causal Discovery under Selection Bias",
            "authors": [
                "Haoyue Dai",
                "Yiwen Qiu",
                "Ignavier Ng",
                "Xinshuai Dong",
                "Peter Spirtes",
                "Kun Zhang"
            ],
            "arxiv_id": "2512.11219v1",
            "summary": "Addressing selection bias in latent variable causal discovery is important yet underexplored, largely due to a lack of suitable statistical tools: While various tools beyond basic conditional independencies have been developed to handle latent variables, none have been adapted for selection bias. We make an attempt by studying rank constraints, which, as a generalization to conditional independence constraints, exploits the ranks of covariance submatrices in linear Gaussian models. We show that although selection can significantly complicate the joint distribution, interestingly, the ranks in the biased covariance matrices still preserve meaningful information about both causal structures and selection mechanisms. We provide a graph-theoretic characterization of such rank constraints. Using this tool, we demonstrate that the one-factor model, a classical latent variable model, can be identified under selection bias. Simulations and real-world experiments confirm the effectiveness of using our rank constraints.",
            "headline_zh": "提出秩约束方法以解决选择偏差下的潜变量因果发现",
            "intro_zh": [
                "核心问题：选择偏差在潜变量因果发现中未被充分探索，缺乏统计工具。",
                "方法要点：利用秩约束作为条件独立性的泛化，分析线性高斯模型中协方差子矩阵的秩。",
                "实验或效果：通过模拟和真实实验验证秩约束的有效性，识别经典潜变量模型。"
            ],
            "tags_zh": [
                "因果发现",
                "选择偏差",
                "潜变量",
                "秩约束",
                "线性高斯模型"
            ],
            "_index": 178
        },
        {
            "title": "Seeing to Act, Prompting to Specify: A Bayesian Factorization of Vision Language Action Policy",
            "authors": [
                "Kechun Xu",
                "Zhenjie Zhu",
                "Anzhe Chen",
                "Shuqi Zhao",
                "Qing Huang",
                "Yifei Yang",
                "Haojian Lu",
                "Rong Xiong",
                "Masayoshi Tomizuka",
                "Yue Wang"
            ],
            "arxiv_id": "2512.11218v1",
            "summary": "The pursuit of out-of-distribution generalization in Vision-Language-Action (VLA) models is often hindered by catastrophic forgetting of the Vision-Language Model (VLM) backbone during fine-tuning. While co-training with external reasoning data helps, it requires experienced tuning and data-related overhead. Beyond such external dependencies, we identify an intrinsic cause within VLA datasets: modality imbalance, where language diversity is much lower than visual and action diversity. This imbalance biases the model toward visual shortcuts and language forgetting. To address this, we introduce BayesVLA, a Bayesian factorization that decomposes the policy into a visual-action prior, supporting seeing-to-act, and a language-conditioned likelihood, enabling prompt-to-specify. This inherently preserves generalization and promotes instruction following. We further incorporate pre- and post-contact phases to better leverage pre-trained foundation models. Information-theoretic analysis formally validates our effectiveness in mitigating shortcut learning. Extensive experiments show superior generalization to unseen instructions, objects, and environments compared to existing methods. Project page is available at: https://xukechun.github.io/papers/BayesVLA.",
            "headline_zh": "提出BayesVLA贝叶斯分解方法，解决视觉-语言-动作模型中模态不平衡导致的泛化问题。",
            "intro_zh": [
                "核心问题：VLA模型微调时因模态不平衡（语言多样性低）导致视觉捷径和语言遗忘，阻碍分布外泛化。",
                "方法要点：通过贝叶斯分解将策略分解为视觉-动作先验和语言条件似然，支持“看到即行动”和“提示指定”，保留泛化能力。",
                "实验或效果：在未见指令、对象和环境上优于现有方法，信息论分析验证缓解捷径学习的有效性。"
            ],
            "tags_zh": [
                "视觉-语言-动作模型",
                "贝叶斯分解",
                "模态不平衡",
                "泛化能力",
                "信息论分析",
                "预训练模型利用"
            ],
            "_index": 179
        },
        {
            "title": "SmokeBench: Evaluating Multimodal Large Language Models for Wildfire Smoke Detection",
            "authors": [
                "Tianye Qi",
                "Weihao Li",
                "Nick Barnes"
            ],
            "arxiv_id": "2512.11215v1",
            "summary": "Wildfire smoke is transparent, amorphous, and often visually confounded with clouds, making early-stage detection particularly challenging. In this work, we introduce a benchmark, called SmokeBench, to evaluate the ability of multimodal large language models (MLLMs) to recognize and localize wildfire smoke in images. The benchmark consists of four tasks: (1) smoke classification, (2) tile-based smoke localization, (3) grid-based smoke localization, and (4) smoke detection. We evaluate several MLLMs, including Idefics2, Qwen2.5-VL, InternVL3, Unified-IO 2, Grounding DINO, GPT-4o, and Gemini-2.5 Pro. Our results show that while some models can classify the presence of smoke when it covers a large area, all models struggle with accurate localization, especially in the early stages. Further analysis reveals that smoke volume is strongly correlated with model performance, whereas contrast plays a comparatively minor role. These findings highlight critical limitations of current MLLMs for safety-critical wildfire monitoring and underscore the need for methods that improve early-stage smoke localization.",
            "headline_zh": "提出SmokeBench基准以评估多模态大语言模型在野火烟雾检测中的性能",
            "intro_zh": [
                "野火烟雾透明、无定形，易与云混淆，早期检测困难",
                "基准包含分类、基于瓦片/网格的定位和检测四项任务",
                "评估显示模型在烟雾大面积时分类尚可，但定位能力普遍不足"
            ],
            "tags_zh": [
                "野火烟雾检测",
                "多模态大语言模型",
                "基准评估",
                "图像定位",
                "早期检测",
                "安全监控"
            ],
            "_index": 180
        },
        {
            "title": "FutureWeaver: Planning Test-Time Compute for Multi-Agent Systems with Modularized Collaboration",
            "authors": [
                "Dongwon Jung",
                "Peng Shi",
                "Yi Zhang"
            ],
            "arxiv_id": "2512.11213v1",
            "summary": "Scaling test-time computation improves large language model performance without additional training. Recent work demonstrates that techniques such as repeated sampling, self-verification, and self-reflection can significantly enhance task success by allocating more inference-time compute. However, applying these techniques across multiple agents in a multi-agent system is difficult: there does not exist principled mechanisms to allocate compute to foster collaboration among agents, to extend test-time scaling to collaborative interactions, or to distribute compute across agents under explicit budget constraints. To address this gap, we propose FutureWeaver, a framework for planning and optimizing test-time compute allocation in multi-agent systems under fixed budgets. FutureWeaver introduces modularized collaboration, formalized as callable functions that encapsulate reusable multi-agent workflows. These modules are automatically derived through self-play reflection by abstracting recurring interaction patterns from past trajectories. Building on these modules, FutureWeaver employs a dual-level planning architecture that optimizes compute allocation by reasoning over the current task state while also speculating on future steps. Experiments on complex agent benchmarks demonstrate that FutureWeaver consistently outperforms baselines across diverse budget settings, validating its effectiveness for multi-agent collaboration in inference-time optimization.",
            "headline_zh": "提出FutureWeaver框架以优化多智能体系统在固定预算下的测试时计算分配",
            "intro_zh": [
                "核心问题：多智能体系统中缺乏原则性机制来分配计算以促进协作，或扩展测试时计算到交互中",
                "方法要点：引入模块化协作，通过自玩反思抽象可重用工作流，并采用双级规划架构优化计算分配",
                "实验或效果：在复杂智能体基准测试中，FutureWeaver在不同预算设置下均优于基线，验证其有效性"
            ],
            "tags_zh": [
                "多智能体系统",
                "测试时计算",
                "模块化协作",
                "计算分配优化",
                "自玩反思",
                "双级规划"
            ],
            "_index": 181
        },
        {
            "title": "AutoRefiner: Improving Autoregressive Video Diffusion Models via Reflective Refinement Over the Stochastic Sampling Path",
            "authors": [
                "Zhengyang Yu",
                "Akio Hayakawa",
                "Masato Ishii",
                "Qingtao Yu",
                "Takashi Shibuya",
                "Jing Zhang",
                "Yuki Mitsufuji"
            ],
            "arxiv_id": "2512.11203v1",
            "summary": "Autoregressive video diffusion models (AR-VDMs) show strong promise as scalable alternatives to bidirectional VDMs, enabling real-time and interactive applications. Yet there remains room for improvement in their sample fidelity. A promising solution is inference-time alignment, which optimizes the noise space to improve sample fidelity without updating model parameters. Yet, optimization- or search-based methods are computationally impractical for AR-VDMs. Recent text-to-image (T2I) works address this via feedforward noise refiners that modulate sampled noises in a single forward pass. Can such noise refiners be extended to AR-VDMs? We identify the failure of naively extending T2I noise refiners to AR-VDMs and propose AutoRefiner-a noise refiner tailored for AR-VDMs, with two key designs: pathwise noise refinement and a reflective KV-cache. Experiments demonstrate that AutoRefiner serves as an efficient plug-in for AR-VDMs, effectively enhancing sample fidelity by refining noise along stochastic denoising paths.",
            "headline_zh": "提出AutoRefiner以改进自回归视频扩散模型的样本保真度",
            "intro_zh": [
                "自回归视频扩散模型样本保真度不足，推理时对齐方法计算成本高",
                "AutoRefiner通过路径噪声精炼和反射KV缓存，在单次前向传递中调制噪声",
                "实验显示AutoRefiner作为高效插件，有效提升样本保真度"
            ],
            "tags_zh": [
                "自回归视频扩散模型",
                "噪声精炼",
                "推理优化",
                "样本保真度",
                "KV缓存"
            ],
            "_index": 182
        },
        {
            "title": "amc: The Automated Mission Classifier for Telescope Bibliographies",
            "authors": [
                "John F. Wu",
                "Joshua E. G. Peek",
                "Sophie J. Miller",
                "Jenny Novacescu",
                "Achu J. Usha",
                "Christopher A. Wilkinson"
            ],
            "arxiv_id": "2512.11202v1",
            "summary": "Telescope bibliographies record the pulse of astronomy research by capturing publication statistics and citation metrics for telescope facilities. Robust and scalable bibliographies ensure that we can measure the scientific impact of our facilities and archives. However, the growing rate of publications threatens to outpace our ability to manually label astronomical literature. We therefore present the Automated Mission Classifier (amc), a tool that uses large language models (LLMs) to identify and categorize telescope references by processing large quantities of paper text. A modified version of amc performs well on the TRACS Kaggle challenge, achieving a macro $F_1$ score of 0.84 on the held-out test set. amc is valuable for other telescopes beyond TRACS; we developed the initial software for identifying papers that featured scientific results by NASA missions. Additionally, we investigate how amc can also be used to interrogate historical datasets and surface potential label errors. Our work demonstrates that LLM-based applications offer powerful and scalable assistance for library sciences.",
            "headline_zh": "提出自动化任务分类器以解决望远镜文献手动标注瓶颈",
            "intro_zh": [
                "核心问题：天文文献增长快，手动标注望远镜参考文献难以扩展。",
                "方法要点：利用大语言模型处理大量论文文本，自动识别和分类望远镜引用。",
                "实验或效果：在TRACS挑战中宏F1分数达0.84，可应用于NASA任务等望远镜。"
            ],
            "tags_zh": [
                "望远镜文献分类",
                "大语言模型应用",
                "自动化标注",
                "天文图书馆学",
                "TRACS挑战"
            ],
            "_index": 183
        },
        {
            "title": "Fast EXP3 Algorithms",
            "authors": [
                "Ryoma Sato",
                "Shinji Ito"
            ],
            "arxiv_id": "2512.11201v1",
            "summary": "We point out that EXP3 can be implemented in constant time per round, propose more practical algorithms, and analyze the trade-offs between the regret bounds and time complexities of these algorithms.",
            "headline_zh": "提出快速EXP3算法，实现每轮常数时间，分析遗憾界与时间复杂度的权衡。",
            "intro_zh": [
                "核心问题：EXP3算法在每轮的时间复杂度较高，影响实际应用效率。",
                "方法要点：设计常数时间每轮的EXP3实现，并提出更实用的算法变体。",
                "实验或效果：分析算法在遗憾界和时间复杂度之间的权衡关系，提升实用性。"
            ],
            "tags_zh": [
                "EXP3算法",
                "在线学习",
                "遗憾界",
                "时间复杂度",
                "常数时间算法",
                "权衡分析"
            ],
            "_index": 184
        },
        {
            "title": "Theoretical Foundations of GPU-Native Compilation for Rapid Code Iteration",
            "authors": [
                "Adilet Metinov",
                "Gulida M. Kudakeeva",
                "Gulnara D. Kabaeva"
            ],
            "arxiv_id": "2512.11200v1",
            "summary": "Current AI code generation systems suffer from significant latency bottlenecks due to CPU-GPU data transfers during compilation, execution, and testing phases. We establish theoretical foundations for three complementary approaches to GPU-native compilation that eliminate these transfers: (1) parallel traditional compilation adapted for GPU execution, (2) neural compilation using learned sequence-to-sequence translation with probabilistic verification, and (3) hybrid architectures combining both strategies. We derive latency and energy bounds demonstrating potential speedups of 10-100x for code iteration cycles. Our analysis shows that traditional GPU compilation provides 2-5x improvements through transfer elimination, neural compilation achieves 10-100x speedups via massive parallelism, and hybrid approaches offer practical deployment paths with guaranteed correctness. We formalize the probabilistic verification framework that enables trading compilation accuracy for parallel exploration, and discuss implications for self-improving AI systems and future analog computing substrates.",
            "headline_zh": "提出GPU原生编译理论以消除CPU-GPU数据传输，加速代码迭代",
            "intro_zh": [
                "核心问题：AI代码生成系统因CPU-GPU数据传输导致编译、执行和测试延迟瓶颈",
                "方法要点：建立三种GPU原生编译理论方法：并行传统编译、神经编译和混合架构",
                "实验或效果：理论分析显示潜在10-100倍加速，传统编译提升2-5倍，神经编译提升10-100倍"
            ],
            "tags_zh": [
                "GPU原生编译",
                "代码迭代加速",
                "神经编译",
                "并行编译",
                "数据传输消除",
                "概率验证"
            ],
            "_index": 185
        },
        {
            "title": "CADKnitter: Compositional CAD Generation from Text and Geometry Guidance",
            "authors": [
                "Tri Le",
                "Khang Nguyen",
                "Baoru Huang",
                "Tung D. Ta",
                "Anh Nguyen"
            ],
            "arxiv_id": "2512.11199v1",
            "summary": "Crafting computer-aided design (CAD) models has long been a painstaking and time-intensive task, demanding both precision and expertise from designers. With the emergence of 3D generation, this task has undergone a transformative impact, shifting not only from visual fidelity to functional utility but also enabling editable CAD designs. Prior works have achieved early success in single-part CAD generation, which is not well-suited for real-world applications, as multiple parts need to be assembled under semantic and geometric constraints. In this paper, we propose CADKnitter, a compositional CAD generation framework with a geometry-guided diffusion sampling strategy. CADKnitter is able to generate a complementary CAD part that follows both the geometric constraints of the given CAD model and the semantic constraints of the desired design text prompt. We also curate a dataset, so-called KnitCAD, containing over 310,000 samples of CAD models, along with textual prompts and assembly metadata that provide semantic and geometric constraints. Intensive experiments demonstrate that our proposed method outperforms other state-of-the-art baselines by a clear margin.",
            "headline_zh": "提出CADKnitter框架，通过几何引导扩散采样实现从文本和几何约束的组合式CAD生成。",
            "intro_zh": [
                "核心问题：现有单部件CAD生成方法不适用于需语义和几何约束的多部件组装实际应用。",
                "方法要点：采用几何引导扩散采样策略，生成符合给定CAD模型几何约束和设计文本语义约束的互补部件。",
                "实验或效果：构建KnitCAD数据集，包含超31万样本，实验显示方法明显优于其他先进基线。"
            ],
            "tags_zh": [
                "CAD生成",
                "组合式设计",
                "扩散模型",
                "几何约束",
                "文本引导",
                "3D建模"
            ],
            "_index": 186
        }
    ]
}