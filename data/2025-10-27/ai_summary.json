{
    "papers": [
        {
            "title": "Concerto: Joint 2D-3D Self-Supervised Learning Emerges Spatial Representations",
            "authors": [
                "Yujia Zhang",
                "Xiaoyang Wu",
                "Yixing Lao",
                "Chengyao Wang",
                "Zhuotao Tian",
                "Naiyan Wang",
                "Hengshuang Zhao"
            ],
            "arxiv_id": "2510.23607v1",
            "summary": "Humans learn abstract concepts through multisensory synergy, and once formed,\nsuch representations can often be recalled from a single modality. Inspired by\nthis principle, we introduce Concerto, a minimalist simulation of human concept\nlearning for spatial cognition, combining 3D intra-modal self-distillation with\n2D-3D cross-modal joint embedding. Despite its simplicity, Concerto learns more\ncoherent and informative spatial features, as demonstrated by zero-shot\nvisualizations. It outperforms both standalone SOTA 2D and 3D self-supervised\nmodels by 14.2% and 4.8%, respectively, as well as their feature concatenation,\nin linear probing for 3D scene perception. With full fine-tuning, Concerto sets\nnew SOTA results across multiple scene understanding benchmarks (e.g., 80.7%\nmIoU on ScanNet). We further present a variant of Concerto tailored for\nvideo-lifted point cloud spatial understanding, and a translator that linearly\nprojects Concerto representations into CLIP's language space, enabling\nopen-world perception. These results highlight that Concerto emerges spatial\nrepresentations with superior fine-grained geometric and semantic consistency.",
            "headline_zh": "提出Concerto联合2D-3D自监督学习，以增强空间认知表示",
            "intro_zh": [
                "核心问题：如何通过多模态学习模拟人类空间概念形成，提升单模态表示能力",
                "方法要点：结合3D模态内自蒸馏与2D-3D跨模态联合嵌入，实现简单高效训练",
                "实验或效果：在3D场景感知线性探测中超越SOTA模型，并在多个基准上创下新记录"
            ],
            "tags_zh": [
                "自监督学习",
                "2D-3D联合嵌入",
                "空间表示学习",
                "多模态学习",
                "场景理解"
            ],
            "_index": 0
        },
        {
            "title": "Track, Inpaint, Resplat: Subject-driven 3D and 4D Generation with Progressive Texture Infilling",
            "authors": [
                "Shuhong Zheng",
                "Ashkan Mirzaei",
                "Igor Gilitschenski"
            ],
            "arxiv_id": "2510.23605v1",
            "summary": "Current 3D/4D generation methods are usually optimized for photorealism,\nefficiency, and aesthetics. However, they often fail to preserve the semantic\nidentity of the subject across different viewpoints. Adapting generation\nmethods with one or few images of a specific subject (also known as\nPersonalization or Subject-driven generation) allows generating visual content\nthat align with the identity of the subject. However, personalized 3D/4D\ngeneration is still largely underexplored. In this work, we introduce TIRE\n(Track, Inpaint, REsplat), a novel method for subject-driven 3D/4D generation.\nIt takes an initial 3D asset produced by an existing 3D generative model as\ninput and uses video tracking to identify the regions that need to be modified.\nThen, we adopt a subject-driven 2D inpainting model for progressively infilling\nthe identified regions. Finally, we resplat the modified 2D multi-view\nobservations back to 3D while still maintaining consistency. Extensive\nexperiments demonstrate that our approach significantly improves identity\npreservation in 3D/4D generation compared to state-of-the-art methods. Our\nproject website is available at\nhttps://zsh2000.github.io/track-inpaint-resplat.github.io/.",
            "headline_zh": "提出TIRE方法以解决主题驱动3D/4D生成中的身份保持问题",
            "intro_zh": [
                "现有3D/4D生成方法难以在不同视角下保持主题的语义身份",
                "通过跟踪、修复和重投影，渐进填充纹理以提升身份一致性",
                "实验显示在身份保持方面优于现有先进方法"
            ],
            "tags_zh": [
                "主题驱动生成",
                "3D生成",
                "4D生成",
                "纹理修复",
                "视频跟踪",
                "多视图一致性"
            ],
            "_index": 1
        },
        {
            "title": "PixelRefer: A Unified Framework for Spatio-Temporal Object Referring with Arbitrary Granularity",
            "authors": [
                "Yuqian Yuan",
                "Wenqiao Zhang",
                "Xin Li",
                "Shihao Wang",
                "Kehan Li",
                "Wentong Li",
                "Jun Xiao",
                "Lei Zhang",
                "Beng Chin Ooi"
            ],
            "arxiv_id": "2510.23603v1",
            "summary": "Multimodal large language models (MLLMs) have demonstrated strong\ngeneral-purpose capabilities in open-world visual comprehension. However, most\nexisting MLLMs primarily focus on holistic, scene-level understanding, often\noverlooking the need for fine-grained, object-centric reasoning. In this paper,\nwe present PixelRefer, a unified region-level MLLM framework that enables\nadvanced fine-grained understanding over user-specified regions across both\nimages and videos. Motivated by the observation that LLM attention\npredominantly focuses on object-level tokens, we propose a Scale-Adaptive\nObject Tokenizer (SAOT) to generate compact and semantically rich object\nrepresentations from free-form regions. Our analysis reveals that global visual\ntokens contribute mainly in early LLM layers, inspiring the design of\nPixelRefer-Lite, an efficient variant that employs an Object-Centric Infusion\nmodule to pre-fuse global context into object tokens. This yields a lightweight\nObject-Only Framework that substantially reduces computational cost while\nmaintaining high semantic fidelity. To facilitate fine-grained instruction\ntuning, we curate PixelRefer-2.2M, a high-quality object-centric instruction\ndataset. Extensive experiments across a range of benchmarks validate that\nPixelRefer achieves leading performance with fewer training samples, while\nPixelRefer-Lite offers competitive accuracy with notable gains in efficiency.",
            "headline_zh": "提出PixelRefer统一框架，实现图像和视频中任意粒度对象的细粒度理解",
            "intro_zh": [
                "现有MLLMs多关注场景级理解，缺乏对象级细粒度推理能力",
                "引入Scale-Adaptive Object Tokenizer生成对象表示，并设计高效变体PixelRefer-Lite",
                "实验验证在多个基准上领先性能，PixelRefer-Lite在保持精度下显著提升效率"
            ],
            "tags_zh": [
                "多模态大语言模型",
                "细粒度对象理解",
                "区域级视觉推理",
                "高效计算框架",
                "指令调优数据集"
            ],
            "_index": 2
        },
        {
            "title": "PRISM-Bench: A Benchmark of Puzzle-Based Visual Tasks with CoT Error Detection",
            "authors": [
                "Yusu Qian",
                "Cheng Wan",
                "Chao Jia",
                "Yinfei Yang",
                "Qingyu Zhao",
                "Zhe Gan"
            ],
            "arxiv_id": "2510.23594v1",
            "summary": "We introduce \\textbf{PRISM-Bench}, a benchmark of puzzle-based visual\nchallenges designed to evaluate not only whether models can solve problems, but\nhow their reasoning unfolds. Unlike prior evaluations that measure only\nfinal-answer accuracy, PRISM-Bench introduces a diagnostic task: given a visual\npuzzle and a step-by-step chain-of-thought (CoT) containing exactly one error,\nmodels must identify the first incorrect step. This setting enables\nfine-grained assessment of logical consistency, error detection, and visual\nreasoning. The puzzles in PRISM-Bench require multi-step symbolic, geometric,\nand analogical reasoning, resisting shortcuts based on superficial pattern\nmatching. Evaluations across state-of-the-art MLLMs reveal a persistent gap\nbetween fluent generation and faithful reasoning: models that produce plausible\nCoTs often fail to locate simple logical faults. By disentangling answer\ngeneration from reasoning verification, PRISM-Bench offers a sharper lens on\nmultimodal reasoning competence and underscores the need for diagnostic\nevaluation protocols in the development of trustworthy MLLMs.",
            "headline_zh": "提出PRISM-Bench基准，通过视觉谜题和思维链错误检测评估多模态推理能力。",
            "intro_zh": [
                "核心问题：现有模型在视觉推理中流畅生成与忠实推理间存在差距，难以检测逻辑错误。",
                "方法要点：设计包含单步错误的思维链任务，要求模型识别首个错误步骤。",
                "实验效果：评估显示先进模型在错误检测上表现不佳，突显诊断评估的必要性。"
            ],
            "tags_zh": [
                "视觉推理基准",
                "思维链错误检测",
                "多模态大语言模型",
                "逻辑一致性评估",
                "诊断评估协议"
            ],
            "_index": 3
        },
        {
            "title": "InFlux: A Benchmark for Self-Calibration of Dynamic Intrinsics of Video Cameras",
            "authors": [
                "Erich Liang",
                "Roma Bhattacharjee",
                "Sreemanti Dey",
                "Rafael Moschopoulos",
                "Caitlin Wang",
                "Michel Liao",
                "Grace Tan",
                "Andrew Wang",
                "Karhan Kayan",
                "Stamatis Alexandropoulos",
                "Jia Deng"
            ],
            "arxiv_id": "2510.23589v1",
            "summary": "Accurately tracking camera intrinsics is crucial for achieving 3D\nunderstanding from 2D video. However, most 3D algorithms assume that camera\nintrinsics stay constant throughout a video, which is often not true for many\nreal-world in-the-wild videos. A major obstacle in this field is a lack of\ndynamic camera intrinsics benchmarks--existing benchmarks typically offer\nlimited diversity in scene content and intrinsics variation, and none provide\nper-frame intrinsic changes for consecutive video frames. In this paper, we\npresent Intrinsics in Flux (InFlux), a real-world benchmark that provides\nper-frame ground truth intrinsics annotations for videos with dynamic\nintrinsics. Compared to prior benchmarks, InFlux captures a wider range of\nintrinsic variations and scene diversity, featuring 143K+ annotated frames from\n386 high-resolution indoor and outdoor videos with dynamic camera intrinsics.\nTo ensure accurate per-frame intrinsics, we build a comprehensive lookup table\nof calibration experiments and extend the Kalibr toolbox to improve its\naccuracy and robustness. Using our benchmark, we evaluate existing baseline\nmethods for predicting camera intrinsics and find that most struggle to achieve\naccurate predictions on videos with dynamic intrinsics. For the dataset, code,\nvideos, and submission, please visit https://influx.cs.princeton.edu/.",
            "headline_zh": "提出InFlux基准以解决动态相机内参自校准的评估问题",
            "intro_zh": [
                "核心问题：现有3D算法假设相机内参恒定，但真实视频中内参常变，缺乏动态内参基准。",
                "方法要点：构建InFlux基准，提供逐帧真实内参标注，扩展Kalibr工具提升精度。",
                "实验或效果：评估基线方法，发现多数在动态内参视频上预测不准，基准含143K+帧。"
            ],
            "tags_zh": [
                "相机内参自校准",
                "动态内参基准",
                "视频3D理解",
                "Kalibr工具扩展",
                "逐帧标注"
            ],
            "_index": 4
        },
        {
            "title": "FARMER: Flow AutoRegressive Transformer over Pixels",
            "authors": [
                "Guangting Zheng",
                "Qinyu Zhao",
                "Tao Yang",
                "Fei Xiao",
                "Zhijie Lin",
                "Jie Wu",
                "Jiajun Deng",
                "Yanyong Zhang",
                "Rui Zhu"
            ],
            "arxiv_id": "2510.23588v1",
            "summary": "Directly modeling the explicit likelihood of the raw data distribution is key\ntopic in the machine learning area, which achieves the scaling successes in\nLarge Language Models by autoregressive modeling. However, continuous AR\nmodeling over visual pixel data suffer from extremely long sequences and\nhigh-dimensional spaces. In this paper, we present FARMER, a novel end-to-end\ngenerative framework that unifies Normalizing Flows (NF) and Autoregressive\n(AR) models for tractable likelihood estimation and high-quality image\nsynthesis directly from raw pixels. FARMER employs an invertible autoregressive\nflow to transform images into latent sequences, whose distribution is modeled\nimplicitly by an autoregressive model. To address the redundancy and complexity\nin pixel-level modeling, we propose a self-supervised dimension reduction\nscheme that partitions NF latent channels into informative and redundant\ngroups, enabling more effective and efficient AR modeling. Furthermore, we\ndesign a one-step distillation scheme to significantly accelerate inference\nspeed and introduce a resampling-based classifier-free guidance algorithm to\nboost image generation quality. Extensive experiments demonstrate that FARMER\nachieves competitive performance compared to existing pixel-based generative\nmodels while providing exact likelihoods and scalable training.",
            "headline_zh": "提出FARMER框架，结合归一化流与自回归模型以解决像素级图像生成中的似然估计难题。",
            "intro_zh": [
                "核心问题：连续自回归建模在视觉像素数据中面临序列过长和高维空间挑战。",
                "方法要点：使用可逆自回归流将图像转为隐序列，并通过自回归模型建模其分布。",
                "实验或效果：在像素级生成模型中实现竞争性能，提供精确似然和可扩展训练。"
            ],
            "tags_zh": [
                "归一化流",
                "自回归模型",
                "图像生成",
                "似然估计",
                "维度缩减",
                "推理加速"
            ],
            "_index": 5
        },
        {
            "title": "Lookahead Anchoring: Preserving Character Identity in Audio-Driven Human Animation",
            "authors": [
                "Junyoung Seo",
                "Rodrigo Mira",
                "Alexandros Haliassos",
                "Stella Bounareli",
                "Honglie Chen",
                "Linh Tran",
                "Seungryong Kim",
                "Zoe Landgraf",
                "Jie Shen"
            ],
            "arxiv_id": "2510.23581v1",
            "summary": "Audio-driven human animation models often suffer from identity drift during\ntemporal autoregressive generation, where characters gradually lose their\nidentity over time. One solution is to generate keyframes as intermediate\ntemporal anchors that prevent degradation, but this requires an additional\nkeyframe generation stage and can restrict natural motion dynamics. To address\nthis, we propose Lookahead Anchoring, which leverages keyframes from future\ntimesteps ahead of the current generation window, rather than within it. This\ntransforms keyframes from fixed boundaries into directional beacons: the model\ncontinuously pursues these future anchors while responding to immediate audio\ncues, maintaining consistent identity through persistent guidance. This also\nenables self-keyframing, where the reference image serves as the lookahead\ntarget, eliminating the need for keyframe generation entirely. We find that the\ntemporal lookahead distance naturally controls the balance between expressivity\nand consistency: larger distances allow for greater motion freedom, while\nsmaller ones strengthen identity adherence. When applied to three recent human\nanimation models, Lookahead Anchoring achieves superior lip synchronization,\nidentity preservation, and visual quality, demonstrating improved temporal\nconditioning across several different architectures. Video results are\navailable at the following link: https://lookahead-anchoring.github.io.",
            "headline_zh": "提出前瞻锚定方法以解决音频驱动人体动画中的身份漂移问题",
            "intro_zh": [
                "核心问题：时序自回归生成中角色身份随时间逐渐漂移",
                "方法要点：利用未来时间步的关键帧作为方向性引导，无需额外关键帧生成",
                "实验或效果：在多个模型上提升唇同步、身份保持和视觉质量"
            ],
            "tags_zh": [
                "音频驱动动画",
                "身份保持",
                "时序建模",
                "关键帧引导",
                "自回归生成"
            ],
            "_index": 6
        },
        {
            "title": "UrbanVLA: A Vision-Language-Action Model for Urban Micromobility",
            "authors": [
                "Anqi Li",
                "Zhiyong Wang",
                "Jiazhao Zhang",
                "Minghan Li",
                "Yunpeng Qi",
                "Zhibo Chen",
                "Zhizheng Zhang",
                "He Wang"
            ],
            "arxiv_id": "2510.23576v1",
            "summary": "Urban micromobility applications, such as delivery robots, demand reliable\nnavigation across large-scale urban environments while following long-horizon\nroute instructions. This task is particularly challenging due to the dynamic\nand unstructured nature of real-world city areas, yet most existing navigation\nmethods remain tailored to short-scale and controllable scenarios. Effective\nurban micromobility requires two complementary levels of navigation skills:\nlow-level capabilities such as point-goal reaching and obstacle avoidance, and\nhigh-level capabilities, such as route-visual alignment. To this end, we\npropose UrbanVLA, a route-conditioned Vision-Language-Action (VLA) framework\ndesigned for scalable urban navigation. Our method explicitly aligns noisy\nroute waypoints with visual observations during execution, and subsequently\nplans trajectories to drive the robot. To enable UrbanVLA to master both levels\nof navigation, we employ a two-stage training pipeline. The process begins with\nSupervised Fine-Tuning (SFT) using simulated environments and trajectories\nparsed from web videos. This is followed by Reinforcement Fine-Tuning (RFT) on\na mixture of simulation and real-world data, which enhances the model's safety\nand adaptability in real-world settings. Experiments demonstrate that UrbanVLA\nsurpasses strong baselines by more than 55% in the SocialNav task on MetaUrban.\nFurthermore, UrbanVLA achieves reliable real-world navigation, showcasing both\nscalability to large-scale urban environments and robustness against real-world\nuncertainties.",
            "headline_zh": "提出UrbanVLA模型以解决城市微移动中的长距离导航问题",
            "intro_zh": [
                "核心问题：城市微移动在动态、非结构化环境中实现可靠长距离导航的挑战",
                "方法要点：采用两阶段训练，结合监督和强化微调，对齐路径点与视觉观察",
                "实验或效果：在MetaUrban上超越基线55%，实现大规模城市环境的可靠导航"
            ],
            "tags_zh": [
                "城市微移动导航",
                "视觉-语言-动作模型",
                "两阶段训练",
                "路径-视觉对齐",
                "强化微调",
                "大规模环境导航"
            ],
            "_index": 7
        },
        {
            "title": "More Than Generation: Unifying Generation and Depth Estimation via Text-to-Image Diffusion Models",
            "authors": [
                "Hongkai Lin",
                "Dingkang Liang",
                "Mingyang Du",
                "Xin Zhou",
                "Xiang Bai"
            ],
            "arxiv_id": "2510.23574v1",
            "summary": "Generative depth estimation methods leverage the rich visual priors stored in\npre-trained text-to-image diffusion models, demonstrating astonishing zero-shot\ncapability. However, parameter updates during training lead to catastrophic\ndegra- dation in the image generation capability of the pre-trained model. We\nintroduce MERGE, a unified model for image generation and depth estimation,\nstarting from a fixed pre-trained text-to-image model. MERGE demonstrates that\nthe pre-trained text-to-image model can do more than image generation, but also\nexpand to depth estimation effortlessly. Specifically, MERGE introduces a play-\nand-plug framework that enables seamless switching between image generation and\ndepth estimation modes through simple and pluggable converters. Meanwhile, we\npropose a Group Reuse Mechanism to encourage parameter reuse and im- prove the\nutilization of the additional learnable parameters. MERGE unleashes the\npowerful depth estimation capability of the pre-trained text-to-image model\nwhile preserving its original image generation ability. Compared to other\nunified models for image generation and depth estimation, MERGE achieves\nstate-of- the-art performance across multiple depth estimation benchmarks. The\ncode will be made available at https://github.com/H-EmbodVis/MERGE",
            "headline_zh": "提出MERGE模型，统一图像生成与深度估计，基于预训练扩散模型",
            "intro_zh": [
                "问题：训练参数更新导致预训练模型图像生成能力退化",
                "方法：采用即插即用框架和组重用机制，实现模式切换与参数高效利用",
                "效果：在多个深度估计基准上达到最优，保持原始图像生成能力"
            ],
            "tags_zh": [
                "文本到图像扩散模型",
                "深度估计",
                "统一模型",
                "零样本能力",
                "参数重用"
            ],
            "_index": 8
        },
        {
            "title": "RobotArena $\\infty$: Scalable Robot Benchmarking via Real-to-Sim Translation",
            "authors": [
                "Yash Jangir",
                "Yidi Zhang",
                "Kashu Yamazaki",
                "Chenyu Zhang",
                "Kuan-Hsun Tu",
                "Tsung-Wei Ke",
                "Lei Ke",
                "Yonatan Bisk",
                "Katerina Fragkiadaki"
            ],
            "arxiv_id": "2510.23571v1",
            "summary": "The pursuit of robot generalists - instructable agents capable of performing\ndiverse tasks across diverse environments - demands rigorous and scalable\nevaluation. Yet real-world testing of robot policies remains fundamentally\nconstrained: it is labor-intensive, slow, unsafe at scale, and difficult to\nreproduce. Existing simulation benchmarks are similarly limited, as they train\nand test policies within the same synthetic domains and cannot assess models\ntrained from real-world demonstrations or alternative simulation environments.\nAs policies expand in scope and complexity, these barriers only intensify,\nsince defining \"success\" in robotics often hinges on nuanced human judgments of\nexecution quality. In this paper, we introduce a new benchmarking framework\nthat overcomes these challenges by shifting VLA evaluation into large-scale\nsimulated environments augmented with online human feedback. Leveraging\nadvances in vision-language models, 2D-to-3D generative modeling, and\ndifferentiable rendering, our approach automatically converts video\ndemonstrations from widely used robot datasets into simulated counterparts.\nWithin these digital twins, we assess VLA policies using both automated\nVLM-guided scoring and scalable human preference judgments collected from\ncrowdworkers, transforming human involvement from tedious scene setup,\nresetting, and safety supervision into lightweight preference comparisons. To\nmeasure robustness, we systematically perturb simulated environments along\nmultiple axes, such as textures and object placements, stress-testing policy\ngeneralization under controlled variation. The result is a continuously\nevolving, reproducible, and scalable benchmark for real-world trained robot\nmanipulation policies, addressing a critical missing capability in today's\nrobotics landscape.",
            "headline_zh": "提出RobotArena ∞框架，通过真实到模拟转换实现可扩展机器人基准测试",
            "intro_zh": [
                "核心问题：真实世界机器人策略测试受限，难以大规模、安全、可重复评估",
                "方法要点：利用视觉语言模型和生成建模，自动将视频演示转换为模拟环境",
                "实验或效果：结合自动评分和人类偏好，系统扰动测试策略鲁棒性"
            ],
            "tags_zh": [
                "机器人基准测试",
                "真实到模拟转换",
                "视觉语言模型",
                "可扩展评估",
                "人类反馈"
            ],
            "_index": 9
        },
        {
            "title": "EgoThinker: Unveiling Egocentric Reasoning with Spatio-Temporal CoT",
            "authors": [
                "Baoqi Pei",
                "Yifei Huang",
                "Jilan Xu",
                "Yuping He",
                "Guo Chen",
                "Fei Wu",
                "Yu Qiao",
                "Jiangmiao Pang"
            ],
            "arxiv_id": "2510.23569v1",
            "summary": "Egocentric video reasoning centers on an unobservable agent behind the camera\nwho dynamically shapes the environment, requiring inference of hidden\nintentions and recognition of fine-grained interactions. This core challenge\nlimits current multimodal large language models MLLMs, which excel at visible\nevent reasoning but lack embodied, first-person understanding. To bridge this\ngap, we introduce EgoThinker, a novel framework that endows MLLMs with robust\negocentric reasoning capabilities through spatio-temporal chain-of-thought\nsupervision and a two-stage learning curriculum. First, we introduce EgoRe-5M,\na large-scale egocentric QA dataset constructed from 13M diverse egocentric\nvideo clips. This dataset features multi-minute segments annotated with\ndetailed CoT rationales and dense hand-object grounding. Second, we employ SFT\non EgoRe-5M to instill reasoning skills, followed by reinforcement fine-tuning\nRFT to further enhance spatio-temporal localization. Experimental results show\nthat EgoThinker outperforms existing methods across multiple egocentric\nbenchmarks, while achieving substantial improvements in fine-grained\nspatio-temporal localization tasks. Full code and data are released at\nhttps://github.com/InternRobotics/EgoThinker.",
            "headline_zh": "提出EgoThinker框架，通过时空链式思维监督增强多模态大语言模型在自我中心视频推理中的能力。",
            "intro_zh": [
                "核心问题：自我中心视频推理需推断隐藏意图和细粒度交互，现有MLLMs缺乏第一人称理解能力。",
                "方法要点：构建EgoRe-5M数据集，采用两阶段学习（SFT和RFT）提升时空定位和推理。",
                "实验效果：在多个自我中心基准测试中表现优异，细粒度时空定位任务有显著改进。"
            ],
            "tags_zh": [
                "自我中心视频推理",
                "多模态大语言模型",
                "时空链式思维",
                "两阶段学习",
                "细粒度定位",
                "EgoRe-5M数据集"
            ],
            "_index": 10
        },
        {
            "title": "Revising Second Order Terms in Deep Animation Video Coding",
            "authors": [
                "Konstantin Schmidt",
                "Thomas Richter"
            ],
            "arxiv_id": "2510.23561v1",
            "summary": "First Order Motion Model is a generative model that animates human heads\nbased on very little motion information derived from keypoints. It is a\npromising solution for video communication because first it operates at very\nlow bitrate and second its computational complexity is moderate compared to\nother learning based video codecs. However, it has strong limitations by\ndesign. Since it generates facial animations by warping source-images, it fails\nto recreate videos with strong head movements. This works concentrates on one\nspecific kind of head movements, namely head rotations. We show that replacing\nthe Jacobian transformations in FOMM by a global rotation helps the system to\nperform better on items with head-rotations while saving 40% to 80% of bitrate\non P-frames. Moreover, we apply state-of-the-art normalization techniques to\nthe discriminator to stabilize the adversarial training which is essential for\ngenerating visually appealing videos. We evaluate the performance by the\nlearned metics LPIPS and DISTS to show the success our optimizations.",
            "headline_zh": "提出全局旋转替换Jacobian变换以改进头部旋转动画视频编码",
            "intro_zh": [
                "核心问题：一阶运动模型在强头部旋转时生成失败，因依赖图像扭曲",
                "方法要点：用全局旋转替换Jacobian变换，并应用归一化技术稳定对抗训练",
                "实验或效果：在P帧节省40%-80%比特率，LPIPS和DISTS指标显示优化成功"
            ],
            "tags_zh": [
                "视频编码",
                "生成模型",
                "头部动画",
                "对抗训练",
                "比特率优化"
            ],
            "_index": 11
        },
        {
            "title": "A U-Net and Transformer Pipeline for Multilingual Image Translation",
            "authors": [
                "Siddharth Sahay",
                "Radhika Agarwal"
            ],
            "arxiv_id": "2510.23554v1",
            "summary": "This paper presents an end-to-end multilingual translation pipeline that\nintegrates a custom U-Net for text detection, the Tesseract engine for text\nrecognition, and a from-scratch sequence-to-sequence (Seq2Seq) Transformer for\nNeural Machine Translation (NMT). Our approach first utilizes a U-Net model,\ntrained on a synthetic dataset , to accurately segment and detect text regions\nfrom an image. These detected regions are then processed by Tesseract to\nextract the source text. This extracted text is fed into a custom Transformer\nmodel trained from scratch on a multilingual parallel corpus spanning 5\nlanguages. Unlike systems reliant on monolithic pre-trained models, our\narchitecture emphasizes full customization and adaptability. The system is\nevaluated on its text detection accuracy, text recognition quality, and\ntranslation performance via BLEU scores. The complete pipeline demonstrates\npromising results, validating the viability of a custom-built system for\ntranslating text directly from images.",
            "headline_zh": "提出基于U-Net和Transformer的多语言图像翻译管道，用于从图像中直接翻译文本。",
            "intro_zh": [
                "核心问题：从图像中检测、识别并翻译多语言文本，避免依赖预训练模型。",
                "方法要点：使用U-Net检测文本区域，Tesseract识别文本，自定义Transformer进行翻译。",
                "实验或效果：评估文本检测准确性、识别质量和翻译BLEU分数，结果表现良好。"
            ],
            "tags_zh": [
                "图像文本检测",
                "多语言机器翻译",
                "U-Net模型",
                "Transformer架构",
                "端到端管道"
            ],
            "_index": 12
        },
        {
            "title": "JanusCoder: Towards a Foundational Visual-Programmatic Interface for Code Intelligence",
            "authors": [
                "Qiushi Sun",
                "Jingyang Gong",
                "Yang Liu",
                "Qiaosheng Chen",
                "Lei Li",
                "Kai Chen",
                "Qipeng Guo",
                "Ben Kao",
                "Fei Yuan"
            ],
            "arxiv_id": "2510.23538v1",
            "summary": "The scope of neural code intelligence is rapidly expanding beyond text-based\nsource code to encompass the rich visual outputs that programs generate. This\nvisual dimension is critical for advanced applications like flexible content\ngeneration and precise, program-driven editing of visualizations. However,\nprogress has been impeded by the scarcity of high-quality multimodal code data,\na bottleneck stemming from challenges in synthesis and quality assessment. To\naddress these challenges, we make contributions from both a data and modeling\nperspective. We first introduce a complete synthesis toolkit that leverages\nreciprocal synergies between data modalities to efficiently produce a\nlarge-scale, high-quality corpus spanning from standard charts to complex\ninteractive web UIs and code-driven animations. Leveraging this toolkit, we\nconstruct JanusCode-800K, the largest multimodal code corpus to date. This\npowers the training of our models, JanusCoder and JanusCoderV, which establish\na visual-programmatic interface for generating code from textual instructions,\nvisual inputs, or a combination of both. Our unified model is a departure from\nexisting approaches that build specialized models for isolated tasks. Extensive\nexperiments on both text-centric and vision-centric coding tasks demonstrate\nthe superior performance of the JanusCoder series, with our 7B to 14B scale\nmodels approaching or even exceeding the performance of commercial models.\nFurthermore, extensive analysis provides key insights into harmonizing\nprogrammatic logic with its visual expression. Our code and checkpoints will\nare available at https://github.com/InternLM/JanusCoder.",
            "headline_zh": "提出JanusCoder系列模型，构建视觉-程序接口以解决多模态代码数据稀缺问题",
            "intro_zh": [
                "核心问题：高质量多模态代码数据稀缺，阻碍视觉化代码智能应用发展",
                "方法要点：开发合成工具包构建JanusCode-800K语料库，训练统一模型处理文本和视觉输入",
                "实验或效果：7B至14B模型在文本和视觉编码任务中表现优异，接近或超越商业模型"
            ],
            "tags_zh": [
                "多模态代码智能",
                "视觉-程序接口",
                "代码生成",
                "数据合成",
                "统一模型",
                "代码语料库"
            ],
            "_index": 13
        },
        {
            "title": "DPGLA: Bridging the Gap between Synthetic and Real Data for Unsupervised Domain Adaptation in 3D LiDAR Semantic Segmentation",
            "authors": [
                "Wanmeng Li",
                "Simone Mosco",
                "Daniel Fusaro",
                "Alberto Pretto"
            ],
            "arxiv_id": "2510.23525v1",
            "summary": "Annotating real-world LiDAR point clouds for use in intelligent autonomous\nsystems is costly. To overcome this limitation, self-training-based\nUnsupervised Domain Adaptation (UDA) has been widely used to improve point\ncloud semantic segmentation by leveraging synthetic point cloud data. However,\nwe argue that existing methods do not effectively utilize unlabeled data, as\nthey either rely on predefined or fixed confidence thresholds, resulting in\nsuboptimal performance. In this paper, we propose a Dynamic Pseudo-Label\nFiltering (DPLF) scheme to enhance real data utilization in point cloud UDA\nsemantic segmentation. Additionally, we design a simple and efficient\nPrior-Guided Data Augmentation Pipeline (PG-DAP) to mitigate domain shift\nbetween synthetic and real-world point clouds. Finally, we utilize data mixing\nconsistency loss to push the model to learn context-free representations. We\nimplement and thoroughly evaluate our approach through extensive comparisons\nwith state-of-the-art methods. Experiments on two challenging synthetic-to-real\npoint cloud semantic segmentation tasks demonstrate that our approach achieves\nsuperior performance. Ablation studies confirm the effectiveness of the DPLF\nand PG-DAP modules. We release the code of our method in this paper.",
            "headline_zh": "提出动态伪标签过滤与先验引导数据增强以提升3D LiDAR语义分割的无监督域适应性能",
            "intro_zh": [
                "核心问题：现有方法依赖固定置信阈值，未充分利用未标记数据，导致性能不佳。",
                "方法要点：引入动态伪标签过滤和先导数据增强管道，减少合成与真实点云间的域偏移。",
                "实验效果：在合成到真实点云分割任务中表现优异，消融研究验证模块有效性。"
            ],
            "tags_zh": [
                "3D LiDAR语义分割",
                "无监督域适应",
                "动态伪标签过滤",
                "数据增强",
                "点云处理",
                "合成到真实迁移"
            ],
            "_index": 14
        },
        {
            "title": "Explicit Memory through Online 3D Gaussian Splatting Improves Class-Agnostic Video Segmentation",
            "authors": [
                "Anthony Opipari",
                "Aravindhan K Krishnan",
                "Shreekant Gayaka",
                "Min Sun",
                "Cheng-Hao Kuo",
                "Arnie Sen",
                "Odest Chadwicke Jenkins"
            ],
            "arxiv_id": "2510.23521v1",
            "summary": "Remembering where object segments were predicted in the past is useful for\nimproving the accuracy and consistency of class-agnostic video segmentation\nalgorithms. Existing video segmentation algorithms typically use either no\nobject-level memory (e.g. FastSAM) or they use implicit memories in the form of\nrecurrent neural network features (e.g. SAM2). In this paper, we augment both\ntypes of segmentation models using an explicit 3D memory and show that the\nresulting models have more accurate and consistent predictions. For this, we\ndevelop an online 3D Gaussian Splatting (3DGS) technique to store predicted\nobject-level segments generated throughout the duration of a video. Based on\nthis 3DGS representation, a set of fusion techniques are developed, named\nFastSAM-Splat and SAM2-Splat, that use the explicit 3DGS memory to improve\ntheir respective foundation models' predictions. Ablation experiments are used\nto validate the proposed techniques' design and hyperparameter settings.\nResults from both real-world and simulated benchmarking experiments show that\nmodels which use explicit 3D memories result in more accurate and consistent\npredictions than those which use no memory or only implicit neural network\nmemories. Project Page: https://topipari.com/projects/FastSAM-Splat/",
            "headline_zh": "提出在线3D高斯泼溅显式记忆以提升类无关视频分割的准确性与一致性",
            "intro_zh": [
                "现有视频分割算法缺乏对象级显式记忆，导致预测不一致。",
                "开发在线3D高斯泼溅技术存储对象片段，并融合FastSAM和SAM2模型。",
                "实验验证显式记忆优于无记忆或隐式记忆，提升分割准确性和一致性。"
            ],
            "tags_zh": [
                "视频分割",
                "3D高斯泼溅",
                "显式记忆",
                "类无关分割",
                "在线学习"
            ],
            "_index": 15
        },
        {
            "title": "FreeFuse: Multi-Subject LoRA Fusion via Auto Masking at Test Time",
            "authors": [
                "Yaoli Liu",
                "Yao-Xiang Ding",
                "Kun Zhou"
            ],
            "arxiv_id": "2510.23515v1",
            "summary": "This paper proposes FreeFuse, a novel training-free approach for\nmulti-subject text-to-image generation through automatic fusion of multiple\nsubject LoRAs. In contrast to existing methods that either focus on\npre-inference LoRA weight merging or rely on segmentation models and complex\ntechniques like noise blending to isolate LoRA outputs, our key insight is that\ncontext-aware dynamic subject masks can be automatically derived from\ncross-attention layer weights. Mathematical analysis shows that directly\napplying these masks to LoRA outputs during inference well approximates the\ncase where the subject LoRA is integrated into the diffusion model and used\nindividually for the masked region. FreeFuse demonstrates superior practicality\nand efficiency as it requires no additional training, no modification to LoRAs,\nno auxiliary models, and no user-defined prompt templates or region\nspecifications. Alternatively, it only requires users to provide the LoRA\nactivation words for seamless integration into standard workflows. Extensive\nexperiments validate that FreeFuse outperforms existing approaches in both\ngeneration quality and usability under the multi-subject generation tasks. The\nproject page is at https://future-item.github.io/FreeFuse/",
            "headline_zh": "提出FreeFuse以通过自动掩码融合多主题LoRA实现免训练多主题图像生成",
            "intro_zh": [
                "核心问题：现有方法依赖预融合或复杂分割技术，难以高效生成多主题图像。",
                "方法要点：利用交叉注意力权重自动生成动态掩码，在推理时直接应用于LoRA输出。",
                "实验或效果：在生成质量和可用性上优于现有方法，无需额外训练或辅助模型。"
            ],
            "tags_zh": [
                "多主题图像生成",
                "LoRA融合",
                "免训练方法",
                "动态掩码",
                "交叉注意力",
                "文本到图像生成"
            ],
            "_index": 16
        },
        {
            "title": "Localising under the drape: proprioception in the era of distributed surgical robotic system",
            "authors": [
                "Martin Huber",
                "Nicola A. Cavalcanti",
                "Ayoob Davoodi",
                "Ruixuan Li",
                "Christopher E. Mower",
                "Fabio Carrillo",
                "Christoph J. Laux",
                "Francois Teyssere",
                "Thibault Chandanson",
                "Antoine Harlé",
                "Elie Saghbiny",
                "Mazda Farshad",
                "Guillaume Morel",
                "Emmanuel Vander Poorten",
                "Philipp Fürnstahl",
                "Sébastien Ourselin",
                "Christos Bergeles",
                "Tom Vercauteren"
            ],
            "arxiv_id": "2510.23512v1",
            "summary": "Despite their mechanical sophistication, surgical robots remain blind to\ntheir surroundings. This lack of spatial awareness causes collisions, system\nrecoveries, and workflow disruptions, issues that will intensify with the\nintroduction of distributed robots with independent interacting arms. Existing\ntracking systems rely on bulky infrared cameras and reflective markers,\nproviding only limited views of the surgical scene and adding hardware burden\nin crowded operating rooms. We present a marker-free proprioception method that\nenables precise localisation of surgical robots under their sterile draping\ndespite associated obstruction of visual cues. Our method solely relies on\nlightweight stereo-RGB cameras and novel transformer-based deep learning\nmodels. It builds on the largest multi-centre spatial robotic surgery dataset\nto date (1.4M self-annotated images from human cadaveric and preclinical in\nvivo studies). By tracking the entire robot and surgical scene, rather than\nindividual markers, our approach provides a holistic view robust to occlusions,\nsupporting surgical scene understanding and context-aware control. We\ndemonstrate an example of potential clinical benefits during in vivo breathing\ncompensation with access to tissue dynamics, unobservable under state of the\nart tracking, and accurately locate in multi-robot systems for future\nintelligent interaction. In addition, and compared with existing systems, our\nmethod eliminates markers and improves tracking visibility by 25%. To our\nknowledge, this is the first demonstration of marker-free proprioception for\nfully draped surgical robots, reducing setup complexity, enhancing safety, and\npaving the way toward modular and autonomous robotic surgery.",
            "headline_zh": "提出无标记本体感知方法，以解决分布式手术机器人定位问题。",
            "intro_zh": [
                "核心问题：手术机器人缺乏空间感知，易碰撞且硬件负担重。",
                "方法要点：使用轻量立体RGB相机和Transformer模型，无需标记。",
                "实验或效果：基于大规模数据集，提升跟踪可见度25%，支持多机器人交互。"
            ],
            "tags_zh": [
                "手术机器人定位",
                "无标记感知",
                "立体视觉",
                "Transformer模型",
                "多机器人系统",
                "手术场景理解"
            ],
            "_index": 17
        },
        {
            "title": "Dexbotic: Open-Source Vision-Language-Action Toolbox",
            "authors": [
                "Bin Xie",
                "Erjin Zhou",
                "Fan Jia",
                "Hao Shi",
                "Haoqiang Fan",
                "Haowei Zhang",
                "Hebei Li",
                "Jianjian Sun",
                "Jie Bin",
                "Junwen Huang",
                "Kai Liu",
                "Kaixin Liu",
                "Kefan Gu",
                "Lin Sun",
                "Meng Zhang",
                "Peilong Han",
                "Ruitao Hao",
                "Ruitao Zhang",
                "Saike Huang",
                "Songhan Xie",
                "Tiancai Wang",
                "Tianle Liu",
                "Wenbin Tang",
                "Wenqi Zhu",
                "Yang Chen",
                "Yingfei Liu",
                "Yizhuang Zhou",
                "Yu Liu",
                "Yucheng Zhao",
                "Yunchao Ma",
                "Yunfei Wei",
                "Yuxiang Chen",
                "Ze Chen",
                "Zeming Li",
                "Zhao Wu",
                "Ziheng Zhang",
                "Ziming Liu",
                "Ziwei Yan",
                "Ziyu Zhang"
            ],
            "arxiv_id": "2510.23511v1",
            "summary": "In this paper, we present Dexbotic, an open-source Vision-Language-Action\n(VLA) model toolbox based on PyTorch. It aims to provide a one-stop VLA\nresearch service for professionals in the field of embodied intelligence. It\noffers a codebase that supports multiple mainstream VLA policies\nsimultaneously, allowing users to reproduce various VLA methods with just a\nsingle environment setup. The toolbox is experiment-centric, where the users\ncan quickly develop new VLA experiments by simply modifying the Exp script.\nMoreover, we provide much stronger pretrained models to achieve great\nperformance improvements for state-of-the-art VLA policies. Dexbotic will\ncontinuously update to include more of the latest pre-trained foundation models\nand cutting-edge VLA models in the industry.",
            "headline_zh": "提出开源视觉-语言-动作工具箱Dexbotic，支持多策略VLA研究。",
            "intro_zh": [
                "核心问题：为具身智能领域提供一站式VLA研究服务。",
                "方法要点：基于PyTorch，支持多主流VLA策略，简化环境设置。",
                "实验或效果：提供更强预训练模型，提升SOTA VLA策略性能。"
            ],
            "tags_zh": [
                "视觉-语言-动作模型",
                "开源工具箱",
                "具身智能",
                "PyTorch框架",
                "预训练模型",
                "实验脚本"
            ],
            "_index": 18
        },
        {
            "title": "Deductive Chain-of-Thought Augmented Socially-aware Robot Navigation World Model",
            "authors": [
                "Weizheng Wang",
                "Obi Ike",
                "Soyun Choi",
                "Sungeun Hong",
                "Byung-Cheol Min"
            ],
            "arxiv_id": "2510.23509v1",
            "summary": "Social robot navigation increasingly relies on large language models for\nreasoning, path planning, and enabling movement in dynamic human spaces.\nHowever, relying solely on LLMs for planning often leads to unpredictable and\nunsafe behaviors, especially in dynamic human spaces, due to limited physical\ngrounding and weak logical consistency. In this work, we introduce NaviWM, a\nsocially-aware robot Navigation World Model that augments LLM reasoning with a\nstructured world model and a logic-driven chain-of-thought process. NaviWM\nconsists of two main components: (1) a spatial-temporal world model that\ncaptures the positions, velocities, and activities of agents in the\nenvironment, and (2) a deductive reasoning module that guides LLMs through a\nmulti-step, logic-based inference process. This integration enables the robot\nto generate navigation decisions that are both socially compliant and\nphysically safe, under well-defined constraints such as personal space,\ncollision avoidance, and timing. Unlike previous methods based on prompting or\nfine-tuning, NaviWM encodes social norms as first-order logic, enabling\ninterpretable and verifiable reasoning. Experiments show that NaviWM improves\nsuccess rates and reduces social violations, particularly in crowded\nenvironments. These results demonstrate the benefit of combining formal\nreasoning with LLMs for robust social navigation. Additional experimental\ndetails and demo videos for this work can be found at:\nhttps://sites.google.com/view/NaviWM.",
            "headline_zh": "提出NaviWM世界模型以增强社交机器人导航的安全性和合规性",
            "intro_zh": [
                "核心问题：LLM在社交导航中因缺乏物理基础和逻辑一致性导致行为不可预测",
                "方法要点：结合空间-时间世界模型和演绎推理模块，使用一阶逻辑编码社会规范",
                "实验或效果：在拥挤环境中提高成功率并减少社交违规，验证了方法的有效性"
            ],
            "tags_zh": [
                "社交机器人导航",
                "世界模型",
                "演绎推理",
                "LLM增强",
                "安全导航",
                "逻辑编码"
            ],
            "_index": 19
        },
        {
            "title": "iPac: Incorporating Intra-image Patch Context into Graph Neural Networks for Medical Image Classification",
            "authors": [
                "Usama Zidan",
                "Mohamed Gaber",
                "Mohammed M. Abdelsamea"
            ],
            "arxiv_id": "2510.23504v1",
            "summary": "Graph neural networks have emerged as a promising paradigm for image\nprocessing, yet their performance in image classification tasks is hindered by\na limited consideration of the underlying structure and relationships among\nvisual entities. This work presents iPac, a novel approach to introduce a new\ngraph representation of images to enhance graph neural network image\nclassification by recognizing the importance of underlying structure and\nrelationships in medical image classification. iPac integrates various stages,\nincluding patch partitioning, feature extraction, clustering, graph\nconstruction, and graph-based learning, into a unified network to advance graph\nneural network image classification. By capturing relevant features and\norganising them into clusters, we construct a meaningful graph representation\nthat effectively encapsulates the semantics of the image. Experimental\nevaluation on diverse medical image datasets demonstrates the efficacy of iPac,\nexhibiting an average accuracy improvement of up to 5% over baseline methods.\nOur approach offers a versatile and generic solution for image classification,\nparticularly in the realm of medical images, by leveraging the graph\nrepresentation and accounting for the inherent structure and relationships\namong visual entities.",
            "headline_zh": "提出iPac方法，通过整合图像内补丁上下文增强图神经网络，以改进医学图像分类。",
            "intro_zh": [
                "核心问题：图神经网络在图像分类中忽视视觉实体间结构和关系，限制性能。",
                "方法要点：结合补丁划分、特征提取、聚类、图构建和学习，构建语义图表示。",
                "实验或效果：在多个医学图像数据集上，平均准确率提升最高达5%。"
            ],
            "tags_zh": [
                "图神经网络",
                "医学图像分类",
                "图像补丁上下文",
                "图表示学习",
                "特征聚类"
            ],
            "_index": 20
        },
        {
            "title": "VOLD: Reasoning Transfer from LLMs to Vision-Language Models via On-Policy Distillation",
            "authors": [
                "Walid Bousselham",
                "Hilde Kuehne",
                "Cordelia Schmid"
            ],
            "arxiv_id": "2510.23497v1",
            "summary": "Training vision-language models (VLMs) for complex reasoning remains a\nchallenging task, i.a. due to the scarcity of high-quality image-text reasoning\ndata. Conversely, text-based reasoning resources are abundant and scalable, but\nit is still an open question how to leveraging them for VLM reasoning. To\naddress this problem, we propose VOLD, a framework to transfer reasoning\ncapabilities from text-only teacher models to VLM student models. To this end,\nVOLD combines reinforcement learning via Group Relative Policy Optimization\n(GRPO) with on-policy distillation, which allows the student reasoning traces\nto be guided by the teacher model, resulting in a significant gain over using\nGRPO alone. We further show that a cold-start alignment is essential for an\neffective transfer during the online training phase in this scenario and that\nwithout sufficient distributional alignment between teacher and student,\non-policy distillation fails to provide meaningful guidance. We evaluate VOLD\nacross diverse benchmarks including MMMU-Pro, MathVision, MathVista, and\nLogicVista, showing that VOLD outperforms the baseline model significantly and\nimproves over the state of the art by a margin. Our ablation shows the\nimportance of a cold-start alignment via SFT for on-policy distillation with a\ntext-only teacher.",
            "headline_zh": "提出VOLD框架，通过在线策略蒸馏将LLMs推理能力迁移至VLMs，解决高质量图像-文本推理数据稀缺问题。",
            "intro_zh": [
                "核心问题：高质量图像-文本推理数据稀缺，阻碍视觉语言模型复杂推理能力发展。",
                "方法要点：结合强化学习与在线策略蒸馏，利用文本教师模型指导学生模型推理轨迹。",
                "实验效果：在多个基准测试中显著超越基线模型，并提升当前最优性能。"
            ],
            "tags_zh": [
                "视觉语言模型",
                "推理迁移",
                "在线策略蒸馏",
                "强化学习",
                "分布对齐",
                "教师-学生模型"
            ],
            "_index": 21
        },
        {
            "title": "COOPERA: Continual Open-Ended Human-Robot Assistance",
            "authors": [
                "Chenyang Ma",
                "Kai Lu",
                "Ruta Desai",
                "Xavier Puig",
                "Andrew Markham",
                "Niki Trigoni"
            ],
            "arxiv_id": "2510.23495v1",
            "summary": "To understand and collaborate with humans, robots must account for individual\nhuman traits, habits, and activities over time. However, most robotic\nassistants lack these abilities, as they primarily focus on predefined tasks in\nstructured environments and lack a human model to learn from. This work\nintroduces COOPERA, a novel framework for COntinual, OPen-Ended human-Robot\nAssistance, where simulated humans, driven by psychological traits and\nlong-term intentions, interact with robots in complex environments. By\nintegrating continuous human feedback, our framework, for the first time,\nenables the study of long-term, open-ended human-robot collaboration (HRC) in\ndifferent collaborative tasks across various time-scales. Within COOPERA, we\nintroduce a benchmark and an approach to personalize the robot's collaborative\nactions by learning human traits and context-dependent intents. Experiments\nvalidate the extent to which our simulated humans reflect realistic human\nbehaviors and demonstrate the value of inferring and personalizing to human\nintents for open-ended and long-term HRC. Project Page:\nhttps://dannymcy.github.io/coopera/",
            "headline_zh": "提出COOPERA框架以解决长期开放人机协作中个性化适应问题",
            "intro_zh": [
                "核心问题：机器人缺乏长期学习和适应人类特质与习惯的能力",
                "方法要点：集成连续人类反馈，学习人类特质和情境意图以个性化协作",
                "实验或效果：验证模拟人类行为真实性，展示意图推断对长期协作的价值"
            ],
            "tags_zh": [
                "人机协作",
                "持续学习",
                "个性化适应",
                "模拟人类",
                "意图推断",
                "开放任务"
            ],
            "_index": 22
        },
        {
            "title": "Yesnt: Are Diffusion Relighting Models Ready for Capture Stage Compositing? A Hybrid Alternative to Bridge the Gap",
            "authors": [
                "Elisabeth Jüttner",
                "Leona Krath",
                "Stefan Korfhage",
                "Hannah Dröge",
                "Matthias B. Hullin",
                "Markus Plack"
            ],
            "arxiv_id": "2510.23494v1",
            "summary": "Volumetric video relighting is essential for bringing captured performances\ninto virtual worlds, but current approaches struggle to deliver temporally\nstable, production-ready results. Diffusion-based intrinsic decomposition\nmethods show promise for single frames, yet suffer from stochastic noise and\ninstability when extended to sequences, while video diffusion models remain\nconstrained by memory and scale. We propose a hybrid relighting framework that\ncombines diffusion-derived material priors with temporal regularization and\nphysically motivated rendering. Our method aggregates multiple stochastic\nestimates of per-frame material properties into temporally consistent shading\ncomponents, using optical-flow-guided regularization. For indirect effects such\nas shadows and reflections, we extract a mesh proxy from Gaussian Opacity\nFields and render it within a standard graphics pipeline. Experiments on real\nand synthetic captures show that this hybrid strategy achieves substantially\nmore stable relighting across sequences than diffusion-only baselines, while\nscaling beyond the clip lengths feasible for video diffusion. These results\nindicate that hybrid approaches, which balance learned priors with physically\ngrounded constraints, are a practical step toward production-ready volumetric\nvideo relighting.",
            "headline_zh": "提出混合重光照框架以解决体积视频重光照的时序不稳定问题",
            "intro_zh": [
                "核心问题：扩散模型在序列重光照中产生随机噪声和不稳定性，视频扩散模型受限于内存和规模",
                "方法要点：结合扩散先验与时间正则化，使用光流引导聚合材料属性，并基于高斯不透明度场渲染间接效果",
                "实验或效果：在真实和合成捕获上实现比纯扩散基线更稳定的重光照，并扩展到更长序列"
            ],
            "tags_zh": [
                "体积视频重光照",
                "扩散模型",
                "时序稳定性",
                "混合框架",
                "高斯不透明度场",
                "光流引导"
            ],
            "_index": 23
        },
        {
            "title": "T-REGS: Minimum Spanning Tree Regularization for Self-Supervised Learning",
            "authors": [
                "Julie Mordacq",
                "David Loiseaux",
                "Vicky Kalogeiton",
                "Steve Oudot"
            ],
            "arxiv_id": "2510.23484v1",
            "summary": "Self-supervised learning (SSL) has emerged as a powerful paradigm for\nlearning representations without labeled data, often by enforcing invariance to\ninput transformations such as rotations or blurring. Recent studies have\nhighlighted two pivotal properties for effective representations: (i) avoiding\ndimensional collapse-where the learned features occupy only a low-dimensional\nsubspace, and (ii) enhancing uniformity of the induced distribution. In this\nwork, we introduce T-REGS, a simple regularization framework for SSL based on\nthe length of the Minimum Spanning Tree (MST) over the learned representation.\nWe provide theoretical analysis demonstrating that T-REGS simultaneously\nmitigates dimensional collapse and promotes distribution uniformity on\narbitrary compact Riemannian manifolds. Several experiments on synthetic data\nand on classical SSL benchmarks validate the effectiveness of our approach at\nenhancing representation quality.",
            "headline_zh": "提出T-REGS正则化框架，通过最小生成树长度缓解自监督学习中的维度坍缩和分布不均匀问题。",
            "intro_zh": [
                "自监督学习中存在维度坍缩和分布不均匀问题，影响表示质量。",
                "T-REGS利用最小生成树长度作为正则项，理论上在紧致黎曼流形上同时缓解坍缩和促进均匀性。",
                "在合成数据和经典基准测试中验证了T-REGS提升表示质量的有效性。"
            ],
            "tags_zh": [
                "自监督学习",
                "正则化方法",
                "最小生成树",
                "维度坍缩",
                "分布均匀性",
                "表示学习"
            ],
            "_index": 24
        },
        {
            "title": "On the Faithfulness of Visual Thinking: Measurement and Enhancement",
            "authors": [
                "Zujing Liu",
                "Junwen Pan",
                "Qi She",
                "Yuan Gao",
                "Guisong Xia"
            ],
            "arxiv_id": "2510.23482v1",
            "summary": "Recent large vision-language models (LVLMs) can generate vision-text\nmultimodal chain-of-thought (MCoT) traces after reinforcement fine-tuning\n(RFT). However, we observe that the visual information incorporated in MCoT is\noften inaccurate, though still yield correct answers, indicating a lack of\nfaithfulness in the MCoT reasoning process. We attribute this unfaithfulness to\nthe RL reward in RFT, which solely incentivizes the format of interleaved\nvision-text cues, ie, it encourages the model to incorporate visual information\ninto its text reasoning steps without considering the correctness of the visual\ninformation. In this paper, we first probe the faithfulness of MCoT by\nmeasuring how much the prediction changes when its visual and textual thoughts\nare intervened. Surprisingly, the model's predictions remain nearly unchanged\nunder visual intervention but change significantly under textual intervention,\nindicating that the visual evidence is largely ignored. To further analyze\nvisual information, we introduce an automated LVLM-based evaluation metric that\nquantifies the faithfulness of visual cues from two perspectives: reliability\nand sufficiency. Our evaluation reveals that the visual information in current\nMCoT traces is simultaneously unreliable and insufficient. To address this\nissue, we propose a novel MCoT learning strategy termed Sufficient-Component\nCause Model (SCCM) learning. This approach encourages the MCoT to generate\nsufficient yet minimal visual components that are independently capable of\nleading to correct answers. We note that the proposed SCCM is annotation-free\nand compatible with various RFT for MCoT in a plug-and-play manner. Empirical\nresults demonstrate that SCCM consistently improves the visual faithfulness\nacross a suite of fine-grained perception and reasoning benchmarks. Code is\navailable at https://github.com/EugeneLiu01/Faithful_Thinking_with_Image.",
            "headline_zh": "提出SCCM学习策略以增强视觉-语言模型多模态思维链的忠实性",
            "intro_zh": [
                "核心问题：MCoT中视觉信息不忠实，常被忽略，导致推理过程不可靠",
                "方法要点：引入SCCM学习，鼓励生成充分且最小视觉组件，无需额外标注",
                "实验或效果：在细粒度感知和推理基准上，SCCM一致提升视觉忠实性"
            ],
            "tags_zh": [
                "多模态思维链",
                "视觉忠实性",
                "强化微调",
                "SCCM学习",
                "自动评估",
                "视觉-语言模型"
            ],
            "_index": 25
        },
        {
            "title": "MergeMix: A Unified Augmentation Paradigm for Visual and Multi-Modal Understanding",
            "authors": [
                "Xin Jin",
                "Siyuan Li",
                "Siyong Jian",
                "Kai Yu",
                "Huan Wang"
            ],
            "arxiv_id": "2510.23479v1",
            "summary": "Vision-language alignment in multi-modal large language models (MLLMs)\ntypically relies on supervised fine-tuning (SFT) or reinforcement learning\n(RL). SFT is stable and efficient but requires large-scale human annotations\nand cannot capture subtle preferences, while RL brings in a reward signal for\ntraining, but suffers from overhead and instability. These limitations\nhighlight a trade-off between scalability, robustness, and alignment quality.\nTo address this, we propose MergeMix, a training-time augmentation paradigm\nthat bridges SFT and RL. It first applies an attention-aware image mixing via\ntoken merge with more cluster representation and spatial context, and then\npresents a preference-driven training paradigm for MLLMs by building preference\npairs with mixed images and raw images, and optimizing via SimPO loss. As a\nmixup augmentation, MergeMix enhances attention consistency and efficiency,\nsurpassing other heuristic-based methods in classification. Extensive\nexperiments demonstrate that MergeMix achieves competitive accuracy with\nimproved efficiency, providing a scalable approach to preference alignment in\nclassification and MLLMs.",
            "headline_zh": "提出MergeMix以解决多模态大语言模型中的对齐质量与效率权衡问题",
            "intro_zh": [
                "核心问题：SFT依赖大量人工标注且无法捕捉细微偏好，RL效率低且不稳定。",
                "方法要点：通过注意力感知图像混合和偏好驱动训练，结合SimPO损失优化。",
                "实验或效果：在分类任务中实现竞争性准确率，提升效率与对齐质量。"
            ],
            "tags_zh": [
                "多模态大语言模型",
                "训练增强",
                "偏好对齐",
                "图像混合",
                "SimPO损失"
            ],
            "_index": 26
        },
        {
            "title": "UrbanIng-V2X: A Large-Scale Multi-Vehicle, Multi-Infrastructure Dataset Across Multiple Intersections for Cooperative Perception",
            "authors": [
                "Karthikeyan Chandra Sekaran",
                "Markus Geisler",
                "Dominik Rößle",
                "Adithya Mohan",
                "Daniel Cremers",
                "Wolfgang Utschick",
                "Michael Botsch",
                "Werner Huber",
                "Torsten Schön"
            ],
            "arxiv_id": "2510.23478v1",
            "summary": "Recent cooperative perception datasets have played a crucial role in\nadvancing smart mobility applications by enabling information exchange between\nintelligent agents, helping to overcome challenges such as occlusions and\nimproving overall scene understanding. While some existing real-world datasets\nincorporate both vehicle-to-vehicle and vehicle-to-infrastructure interactions,\nthey are typically limited to a single intersection or a single vehicle. A\ncomprehensive perception dataset featuring multiple connected vehicles and\ninfrastructure sensors across several intersections remains unavailable,\nlimiting the benchmarking of algorithms in diverse traffic environments.\nConsequently, overfitting can occur, and models may demonstrate misleadingly\nhigh performance due to similar intersection layouts and traffic participant\nbehavior. To address this gap, we introduce UrbanIng-V2X, the first\nlarge-scale, multi-modal dataset supporting cooperative perception involving\nvehicles and infrastructure sensors deployed across three urban intersections\nin Ingolstadt, Germany. UrbanIng-V2X consists of 34 temporally aligned and\nspatially calibrated sensor sequences, each lasting 20 seconds. All sequences\ncontain recordings from one of three intersections, involving two vehicles and\nup to three infrastructure-mounted sensor poles operating in coordinated\nscenarios. In total, UrbanIng-V2X provides data from 12 vehicle-mounted RGB\ncameras, 2 vehicle LiDARs, 17 infrastructure thermal cameras, and 12\ninfrastructure LiDARs. All sequences are annotated at a frequency of 10 Hz with\n3D bounding boxes spanning 13 object classes, resulting in approximately 712k\nannotated instances across the dataset. We provide comprehensive evaluations\nusing state-of-the-art cooperative perception methods and publicly release the\ncodebase, dataset, HD map, and a digital twin of the complete data collection\nenvironment.",
            "headline_zh": "提出UrbanIng-V2X数据集以解决多交叉路口协同感知基准缺失问题",
            "intro_zh": [
                "现有协同感知数据集局限于单交叉路口或单车，易导致算法过拟合",
                "UrbanIng-V2X包含三交叉路口多车辆与基础设施传感器数据，支持多模态感知",
                "提供712k 3D标注实例，并基于先进方法进行综合评估与代码发布"
            ],
            "tags_zh": [
                "协同感知",
                "多模态数据集",
                "车辆到一切",
                "3D目标检测",
                "城市交叉路口",
                "数字孪生"
            ],
            "_index": 27
        },
        {
            "title": "Video-Thinker: Sparking \"Thinking with Videos\" via Reinforcement Learning",
            "authors": [
                "Shijian Wang",
                "Jiarui Jin",
                "Xingjian Wang",
                "Linxin Song",
                "Runhao Fu",
                "Hecheng Wang",
                "Zongyuan Ge",
                "Yuan Lu",
                "Xuelian Cheng"
            ],
            "arxiv_id": "2510.23473v1",
            "summary": "Recent advances in image reasoning methods, particularly \"Thinking with\nImages\", have demonstrated remarkable success in Multimodal Large Language\nModels (MLLMs); however, this dynamic reasoning paradigm has not yet been\nextended to video reasoning tasks. In this paper, we propose Video-Thinker,\nwhich empowers MLLMs to think with videos by autonomously leveraging their\nintrinsic \"grounding\" and \"captioning\" capabilities to generate reasoning clues\nthroughout the inference process. To spark this capability, we construct\nVideo-Thinker-10K, a curated dataset featuring autonomous tool usage within\nchain-of-thought reasoning sequences. Our training strategy begins with\nSupervised Fine-Tuning (SFT) to learn the reasoning format, followed by Group\nRelative Policy Optimization (GRPO) to strengthen this reasoning capability.\nThrough this approach, Video-Thinker enables MLLMs to autonomously navigate\ngrounding and captioning tasks for video reasoning, eliminating the need for\nconstructing and calling external tools. Extensive experiments demonstrate that\nVideo-Thinker achieves significant performance gains on both in-domain tasks\nand challenging out-of-domain video reasoning benchmarks, including\nVideo-Holmes, CG-Bench-Reasoning, and VRBench. Our Video-Thinker-7B\nsubstantially outperforms existing baselines such as Video-R1 and establishes\nstate-of-the-art performance among 7B-sized MLLMs.",
            "headline_zh": "提出Video-Thinker，通过强化学习实现多模态大语言模型在视频推理中自主思考",
            "intro_zh": [
                "核心问题：视频推理中缺乏动态推理范式，无法自主利用模型能力生成推理线索",
                "方法要点：构建Video-Thinker-10K数据集，结合监督微调和GRPO强化训练，实现自主工具使用",
                "实验或效果：在多个视频推理基准上显著超越基线，7B模型达到最优性能"
            ],
            "tags_zh": [
                "视频推理",
                "多模态大语言模型",
                "强化学习",
                "自主工具使用",
                "链式思维推理"
            ],
            "_index": 28
        },
        {
            "title": "Omni-Reward: Towards Generalist Omni-Modal Reward Modeling with Free-Form Preferences",
            "authors": [
                "Zhuoran Jin",
                "Hongbang Yuan",
                "Kejian Zhu",
                "Jiachun Li",
                "Pengfei Cao",
                "Yubo Chen",
                "Kang Liu",
                "Jun Zhao"
            ],
            "arxiv_id": "2510.23451v1",
            "summary": "Reward models (RMs) play a critical role in aligning AI behaviors with human\npreferences, yet they face two fundamental challenges: (1) Modality Imbalance,\nwhere most RMs are mainly focused on text and image modalities, offering\nlimited support for video, audio, and other modalities; and (2) Preference\nRigidity, where training on fixed binary preference pairs fails to capture the\ncomplexity and diversity of personalized preferences. To address the above\nchallenges, we propose Omni-Reward, a step toward generalist omni-modal reward\nmodeling with support for free-form preferences, consisting of: (1) Evaluation:\nWe introduce Omni-RewardBench, the first omni-modal RM benchmark with free-form\npreferences, covering nine tasks across five modalities including text, image,\nvideo, audio, and 3D; (2) Data: We construct Omni-RewardData, a multimodal\npreference dataset comprising 248K general preference pairs and 69K\ninstruction-tuning pairs for training generalist omni-modal RMs; (3) Model: We\npropose Omni-RewardModel, which includes both discriminative and generative\nRMs, and achieves strong performance on Omni-RewardBench as well as other\nwidely used reward modeling benchmarks.",
            "headline_zh": "提出Omni-Reward以解决多模态奖励模型中的模态不平衡和偏好僵化问题",
            "intro_zh": [
                "核心问题：现有奖励模型模态不平衡，偏好僵化，难以处理个性化偏好",
                "方法要点：构建多模态基准、数据集和模型，支持自由形式偏好",
                "实验或效果：在Omni-RewardBench等基准上表现优异，覆盖五种模态"
            ],
            "tags_zh": [
                "多模态奖励建模",
                "自由形式偏好",
                "奖励模型基准",
                "多模态数据集",
                "生成式奖励模型"
            ],
            "_index": 29
        },
        {
            "title": "FRBNet: Revisiting Low-Light Vision through Frequency-Domain Radial Basis Network",
            "authors": [
                "Fangtong Sun",
                "Congyu Li",
                "Ke Yang",
                "Yuchen Pan",
                "Hanwen Yu",
                "Xichuan Zhang",
                "Yiying Li"
            ],
            "arxiv_id": "2510.23444v1",
            "summary": "Low-light vision remains a fundamental challenge in computer vision due to\nsevere illumination degradation, which significantly affects the performance of\ndownstream tasks such as detection and segmentation. While recent\nstate-of-the-art methods have improved performance through invariant feature\nlearning modules, they still fall short due to incomplete modeling of low-light\nconditions. Therefore, we revisit low-light image formation and extend the\nclassical Lambertian model to better characterize low-light conditions. By\nshifting our analysis to the frequency domain, we theoretically prove that the\nfrequency-domain channel ratio can be leveraged to extract\nillumination-invariant features via a structured filtering process. We then\npropose a novel and end-to-end trainable module named \\textbf{F}requency-domain\n\\textbf{R}adial \\textbf{B}asis \\textbf{Net}work (\\textbf{FRBNet}), which\nintegrates the frequency-domain channel ratio operation with a learnable\nfrequency domain filter for the overall illumination-invariant feature\nenhancement. As a plug-and-play module, FRBNet can be integrated into existing\nnetworks for low-light downstream tasks without modifying loss functions.\nExtensive experiments across various downstream tasks demonstrate that FRBNet\nachieves superior performance, including +2.2 mAP for dark object detection and\n+2.9 mIoU for nighttime segmentation. Code is available at:\nhttps://github.com/Sing-Forevet/FRBNet.",
            "headline_zh": "提出FRBNet以解决低光视觉中的光照退化问题",
            "intro_zh": [
                "核心问题：低光条件下图像退化严重，影响检测和分割等下游任务性能",
                "方法要点：在频域中利用通道比提取光照不变特征，集成可学习滤波器",
                "实验或效果：在多个任务中表现优异，如目标检测mAP提升2.2"
            ],
            "tags_zh": [
                "低光视觉",
                "频域分析",
                "光照不变特征",
                "目标检测",
                "图像分割"
            ],
            "_index": 30
        },
        {
            "title": "CURVETE: Curriculum Learning and Progressive Self-supervised Training for Medical Image Classification",
            "authors": [
                "Asmaa Abbas",
                "Mohamed Gaber",
                "Mohammed M. Abdelsamea"
            ],
            "arxiv_id": "2510.23442v1",
            "summary": "Identifying high-quality and easily accessible annotated samples poses a\nnotable challenge in medical image analysis. Transfer learning techniques,\nleveraging pre-training data, offer a flexible solution to this issue. However,\nthe impact of fine-tuning diminishes when the dataset exhibits an irregular\ndistribution between classes. This paper introduces a novel deep convolutional\nneural network, named Curriculum Learning and Progressive Self-supervised\nTraining (CURVETE). CURVETE addresses challenges related to limited samples,\nenhances model generalisability, and improves overall classification\nperformance. It achieves this by employing a curriculum learning strategy based\non the granularity of sample decomposition during the training of generic\nunlabelled samples. Moreover, CURVETE address the challenge of irregular class\ndistribution by incorporating a class decomposition approach in the downstream\ntask. The proposed method undergoes evaluation on three distinct medical image\ndatasets: brain tumour, digital knee x-ray, and Mini-DDSM datasets. We\ninvestigate the classification performance using a generic self-supervised\nsample decomposition approach with and without the curriculum learning\ncomponent in training the pretext task. Experimental results demonstrate that\nthe CURVETE model achieves superior performance on test sets with an accuracy\nof 96.60% on the brain tumour dataset, 75.60% on the digital knee x-ray\ndataset, and 93.35% on the Mini-DDSM dataset using the baseline ResNet-50.\nFurthermore, with the baseline DenseNet-121, it achieved accuracies of 95.77%,\n80.36%, and 93.22% on the brain tumour, digital knee x-ray, and Mini-DDSM\ndatasets, respectively, outperforming other training strategies.",
            "headline_zh": "提出CURVETE方法，通过课程学习和自监督训练解决医学图像分类中的样本不足和类别分布不均问题。",
            "intro_zh": [
                "核心问题：医学图像分析中样本标注困难且类别分布不均，影响模型性能。",
                "方法要点：结合课程学习和自监督训练，使用样本分解策略提升泛化能力。",
                "实验效果：在脑肿瘤、膝关节X光和Mini-DDSM数据集上，准确率最高达96.60%，优于基线方法。"
            ],
            "tags_zh": [
                "医学图像分类",
                "课程学习",
                "自监督训练",
                "样本分解",
                "类别分布不均",
                "深度学习"
            ],
            "_index": 31
        },
        {
            "title": "MiCADangelo: Fine-Grained Reconstruction of Constrained CAD Models from 3D Scans",
            "authors": [
                "Ahmet Serdar Karadeniz",
                "Dimitrios Mallis",
                "Danila Rukhovich",
                "Kseniya Cherenkova",
                "Anis Kacem",
                "Djamila Aouada"
            ],
            "arxiv_id": "2510.23429v1",
            "summary": "Computer-Aided Design (CAD) plays a foundational role in modern manufacturing\nand product development, often requiring designers to modify or build upon\nexisting models. Converting 3D scans into parametric CAD representations--a\nprocess known as CAD reverse engineering--remains a significant challenge due\nto the high precision and structural complexity of CAD models. Existing deep\nlearning-based approaches typically fall into two categories: bottom-up,\ngeometry-driven methods, which often fail to produce fully parametric outputs,\nand top-down strategies, which tend to overlook fine-grained geometric details.\nMoreover, current methods neglect an essential aspect of CAD modeling:\nsketch-level constraints. In this work, we introduce a novel approach to CAD\nreverse engineering inspired by how human designers manually perform the task.\nOur method leverages multi-plane cross-sections to extract 2D patterns and\ncapture fine parametric details more effectively. It enables the reconstruction\nof detailed and editable CAD models, outperforming state-of-the-art methods\nand, for the first time, incorporating sketch constraints directly into the\nreconstruction process.",
            "headline_zh": "提出多平面截面方法以解决3D扫描重建参数化CAD模型的精细细节和约束问题",
            "intro_zh": [
                "核心问题：现有方法难以从3D扫描重建高精度、参数化CAD模型，忽略草图级约束。",
                "方法要点：利用多平面截面提取2D模式，捕捉精细参数细节，并集成草图约束。",
                "实验或效果：优于现有方法，首次实现约束重建，生成可编辑CAD模型。"
            ],
            "tags_zh": [
                "CAD逆向工程",
                "3D扫描重建",
                "参数化建模",
                "草图约束",
                "多平面截面"
            ],
            "_index": 32
        },
        {
            "title": "Quality-controlled registration of urban MLS point clouds reducing drift effects by adaptive fragmentation",
            "authors": [
                "Marco Antonio Ortiz Rincon",
                "Yihui Yang",
                "Christoph Holst"
            ],
            "arxiv_id": "2510.23416v1",
            "summary": "This study presents a novel workflow designed to efficiently and accurately\nregister large-scale mobile laser scanning (MLS) point clouds to a target model\npoint cloud in urban street scenarios. This workflow specifically targets the\ncomplexities inherent in urban environments and adeptly addresses the\nchallenges of integrating point clouds that vary in density, noise\ncharacteristics, and occlusion scenarios, which are common in bustling city\ncenters. Two methodological advancements are introduced. First, the proposed\nSemi-sphere Check (SSC) preprocessing technique optimally fragments MLS\ntrajectory data by identifying mutually orthogonal planar surfaces. This step\nreduces the impact of MLS drift on the accuracy of the entire point cloud\nregistration, while ensuring sufficient geometric features within each fragment\nto avoid local minima. Second, we propose Planar Voxel-based Generalized\nIterative Closest Point (PV-GICP), a fine registration method that selectively\nutilizes planar surfaces within voxel partitions. This pre-process strategy not\nonly improves registration accuracy but also reduces computation time by more\nthan 50% compared to conventional point-to-plane ICP methods. Experiments on\nreal-world datasets from Munich's inner city demonstrate that our workflow\nachieves sub-0.01 m average registration accuracy while significantly\nshortening processing times. The results underscore the potential of the\nproposed methods to advance automated 3D urban modeling and updating, with\ndirect applications in urban planning, infrastructure management, and dynamic\ncity monitoring.",
            "headline_zh": "提出自适应分块与PV-GICP方法以解决城市MLS点云配准中的漂移问题",
            "intro_zh": [
                "核心问题：城市移动激光扫描点云配准中漂移、密度不均和遮挡导致精度下降",
                "方法要点：SSC预处理识别正交平面分块，PV-GICP在体素内选择性使用平面进行精细配准",
                "实验效果：在慕尼黑数据集上实现亚0.01米平均精度，计算时间减少超50%"
            ],
            "tags_zh": [
                "点云配准",
                "移动激光扫描",
                "城市建模",
                "漂移校正",
                "迭代最近点"
            ],
            "_index": 33
        },
        {
            "title": "Towards Generalisable Foundation Models for 3D Brain MRI",
            "authors": [
                "Moona Mazher",
                "Geoff J. M. Parker",
                "Daniel C. Alexander"
            ],
            "arxiv_id": "2510.23415v1",
            "summary": "Foundation models in artificial intelligence (AI) are transforming medical\nimaging by enabling general-purpose feature learning from large-scale,\nunlabeled datasets. In this work, we introduce BrainFound, a self-supervised\nfoundation model for brain MRI, built by extending DINO-v2, a vision\ntransformer originally designed for 2D natural images. BrainFound adapts\nDINO-v2 to model full 3D brain anatomy by incorporating volumetric information\nfrom sequential MRI slices, moving beyond conventional single-slice paradigms.\nIt supports both single- and multimodal inputs, enabling a broad range of\ndownstream tasks, including disease detection and image segmentation, while\ngeneralising across varied imaging protocols and clinical scenarios. We show\nthat BrainFound consistently outperforms existing self-supervised pretraining\nstrategies and supervised baselines, particularly in label-scarce and\nmulti-contrast settings. By integrating information from diverse 3D MRI\nmodalities (e.g., T1, T2, FLAIR), it enhances diagnostic accuracy and reduces\ndependency on extensive expert annotations. This flexibility makes BrainFound a\nscalable and practical solution for 3D neuroimaging pipelines, with significant\npotential for clinical deployment and research innovation.",
            "headline_zh": "提出BrainFound基础模型以提升3D脑MRI的泛化性和诊断准确性",
            "intro_zh": [
                "核心问题：传统方法依赖单切片和大量标注，难以泛化到不同成像协议和临床场景。",
                "方法要点：基于DINO-v2扩展，自监督学习3D脑解剖，支持单模态和多模态输入。",
                "实验或效果：在标签稀缺和多对比度设置中优于现有方法，提高诊断准确性和减少标注依赖。"
            ],
            "tags_zh": [
                "3D脑MRI",
                "自监督学习",
                "基础模型",
                "多模态输入",
                "疾病检测",
                "图像分割"
            ],
            "_index": 34
        },
        {
            "title": "Symmetria: A Synthetic Dataset for Learning in Point Clouds",
            "authors": [
                "Ivan Sipiran",
                "Gustavo Santelices",
                "Lucas Oyarzún",
                "Andrea Ranieri",
                "Chiara Romanengo",
                "Silvia Biasotti",
                "Bianca Falcidieno"
            ],
            "arxiv_id": "2510.23414v1",
            "summary": "Unlike image or text domains that benefit from an abundance of large-scale\ndatasets, point cloud learning techniques frequently encounter limitations due\nto the scarcity of extensive datasets. To overcome this limitation, we present\nSymmetria, a formula-driven dataset that can be generated at any arbitrary\nscale. By construction, it ensures the absolute availability of precise ground\ntruth, promotes data-efficient experimentation by requiring fewer samples,\nenables broad generalization across diverse geometric settings, and offers easy\nextensibility to new tasks and modalities. Using the concept of symmetry, we\ncreate shapes with known structure and high variability, enabling neural\nnetworks to learn point cloud features effectively. Our results demonstrate\nthat this dataset is highly effective for point cloud self-supervised\npre-training, yielding models with strong performance in downstream tasks such\nas classification and segmentation, which also show good few-shot learning\ncapabilities. Additionally, our dataset can support fine-tuning models to\nclassify real-world objects, highlighting our approach's practical utility and\napplication. We also introduce a challenging task for symmetry detection and\nprovide a benchmark for baseline comparisons. A significant advantage of our\napproach is the public availability of the dataset, the accompanying code, and\nthe ability to generate very large collections, promoting further research and\ninnovation in point cloud learning.",
            "headline_zh": "提出Symmetria合成数据集以解决点云学习数据稀缺问题",
            "intro_zh": [
                "点云学习因缺乏大规模数据集而受限",
                "基于对称性生成可变形状，提供精确标注和可扩展性",
                "实验显示在自监督预训练和下游任务中表现优异"
            ],
            "tags_zh": [
                "点云学习",
                "合成数据集",
                "对称性生成",
                "自监督预训练",
                "下游任务评估"
            ],
            "_index": 35
        },
        {
            "title": "Color and Frequency Correction for Image Colorization",
            "authors": [
                "Yun Kai Zhuang"
            ],
            "arxiv_id": "2510.23399v1",
            "summary": "The project has carried out the re-optimization of image coloring in\naccordance with the existing Autocolorization direction model DDColor. For the\nexperiments on the existing weights of DDColor, we found that it has\nlimitations in some frequency bands and the color cast problem caused by\ninsufficient input dimension. We construct two optimization schemes and combine\nthem, which achieves the performance improvement of indicators such as PSNR and\nSSIM of the images after DDColor.",
            "headline_zh": "提出颜色和频率校正方案以优化DDColor图像着色模型性能",
            "intro_zh": [
                "核心问题：DDColor模型存在频段限制和输入维度不足导致的色偏问题",
                "方法要点：构建两种优化方案并组合，针对频率和颜色进行校正",
                "实验或效果：在PSNR和SSIM等指标上实现性能提升"
            ],
            "tags_zh": [
                "图像着色",
                "颜色校正",
                "频率优化",
                "DDColor模型",
                "性能提升"
            ],
            "_index": 36
        },
        {
            "title": "VideoTG-R1: Boosting Video Temporal Grounding via Curriculum Reinforcement Learning on Reflected Boundary Annotations",
            "authors": [
                "Lu Dong",
                "Haiyu Zhang",
                "Han Lin",
                "Ziang Yan",
                "Xiangyu Zeng",
                "Hongjie Zhang",
                "Yifei Huang",
                "Yi Wang",
                "Zhen-Hua Ling",
                "Limin Wang",
                "Yali Wang"
            ],
            "arxiv_id": "2510.23397v1",
            "summary": "Video temporal grounding (VTG) aims to locate precise segments in videos\nbased on language queries, which is a fundamental challenge in video\nunderstanding. While recent Multimodal Large Language Models (MLLMs) have shown\npromise in tackling VTG through reinforcement learning (RL), they overlook the\nchallenges arising from both the quality and difficulty of training samples.\n(1) Partially annotated samples. Many samples contain relevant segments beyond\nthe annotated interval, introducing ambiguous supervision. (2) Hard-to-ground\nsamples. Samples with poor zero-shot performance produce consistently low and\nindistinguishable rewards during RL training, exhibiting no clear preference\namong multiple outputs and thus hindering learning efficiency. To address these\nchallenges, we propose VideoTG-R1, a novel curriculum RL framework with\nreflected boundary annotations, enabling data-efficient training. Specifically,\nwe propose a Boundary Reflection Agent that utilizes MLLMs to predict\nquery-relevant timestamps outside the annotated intervals, allowing us to\nidentify and filter out partially annotated samples, thereby reducing\nambiguity. Furthermore, we introduce a Difficulty Estimation Agent to assess\nthe training difficulty of each sample and design a curriculum RL strategy that\ndynamically masks the videos of hard-to-ground samples according to the\ntraining steps, easing the training difficulty and providing clearer\npreference. Experiments on the VTG and grounded VideoQA tasks demonstrate the\neffectiveness of our method. Remarkably, with only 10% of the training samples\nand 21% of the computational budget, VideoTG-R1 outperforms full-data\ncounterparts under both group relative policy optimization (GRPO) and\nsupervised fine-tuning (SFT). The code is available at\nhttps://github.com/ldong1111/VideoTG-R1.",
            "headline_zh": "提出VideoTG-R1课程强化学习框架，通过边界反射和难度估计提升视频时序定位性能。",
            "intro_zh": [
                "核心问题：视频时序定位中部分标注样本引入歧义，难定位样本在强化学习中奖励低且无偏好。",
                "方法要点：使用边界反射代理过滤部分标注样本，难度估计代理动态屏蔽难样本以优化训练。",
                "实验效果：仅用10%训练数据和21%计算预算，在VTG和VideoQA任务中超越全数据方法。"
            ],
            "tags_zh": [
                "视频时序定位",
                "强化学习",
                "课程学习",
                "边界标注",
                "多模态大语言模型",
                "视频问答"
            ],
            "_index": 37
        },
        {
            "title": "Full-Dynamics Real-Time Nonlinear Model Predictive Control of Heavy-Duty Hydraulic Manipulator for Trajectory Tracking Tasks",
            "authors": [
                "Alvaro Paz",
                "Mahdi Hejrati",
                "Pauli Mustalahti",
                "Jouni Mattila"
            ],
            "arxiv_id": "2510.23386v1",
            "summary": "Heavy-duty hydraulic manipulators (HHMs) operate under strict physical and\nsafety-critical constraints due to their large size, high power, and complex\nnonlinear dynamics. Ensuring that both joint-level and end-effector\ntrajectories remain compliant with actuator capabilities, such as force,\nvelocity, and position limits, is essential for safe and reliable operation,\nyet remains largely underexplored in real-time control frameworks. This paper\npresents a nonlinear model predictive control (NMPC) framework designed to\nguarantee constraint satisfaction throughout the full nonlinear dynamics of\nHHMs, while running at a real-time control frequency of 1 kHz. The proposed\nmethod combines a multiple-shooting strategy with real-time sensor feedback,\nand is supported by a robust low-level controller based on virtual\ndecomposition control (VDC) for precise joint tracking. Experimental validation\non a full-scale hydraulic manipulator shows that the NMPC framework not only\nenforces actuator constraints at the joint level, but also ensures\nconstraint-compliant motion in Cartesian space for the end-effector. These\nresults demonstrate the method's capability to deliver high-accuracy trajectory\ntracking while strictly respecting safety-critical limits, setting a new\nbenchmark for real-time control in large-scale hydraulic systems.",
            "headline_zh": "提出非线性模型预测控制框架，用于重型液压机械臂实时轨迹跟踪，确保约束满足。",
            "intro_zh": [
                "重型液压机械臂在严格物理约束下操作，实时控制中约束满足问题未充分探索。",
                "结合多射击策略与实时传感器反馈，采用虚拟分解控制实现精确关节跟踪。",
                "实验验证框架在关节和笛卡尔空间均满足约束，实现高精度轨迹跟踪。"
            ],
            "tags_zh": [
                "非线性模型预测控制",
                "重型液压机械臂",
                "轨迹跟踪",
                "约束满足",
                "实时控制",
                "虚拟分解控制"
            ],
            "_index": 38
        },
        {
            "title": "An Efficient Remote Sensing Super Resolution Method Exploring Diffusion Priors and Multi-Modal Constraints for Crop Type Mapping",
            "authors": [
                "Songxi Yang",
                "Tang Sui",
                "Qunying Huang"
            ],
            "arxiv_id": "2510.23382v1",
            "summary": "Super resolution offers a way to harness medium even lowresolution but\nhistorically valuable remote sensing image archives. Generative models,\nespecially diffusion models, have recently been applied to remote sensing super\nresolution (RSSR), yet several challenges exist. First, diffusion models are\neffective but require expensive training from scratch resources and have slow\ninference speeds. Second, current methods have limited utilization of auxiliary\ninformation as real-world constraints to reconstruct scientifically realistic\nimages. Finally, most current methods lack evaluation on downstream tasks. In\nthis study, we present a efficient LSSR framework for RSSR, supported by a new\nmultimodal dataset of paired 30 m Landsat 8 and 10 m Sentinel 2 imagery. Built\non frozen pretrained Stable Diffusion, LSSR integrates crossmodal attention\nwith auxiliary knowledge (Digital Elevation Model, land cover, month) and\nSynthetic Aperture Radar guidance, enhanced by adapters and a tailored Fourier\nNDVI loss to balance spatial details and spectral fidelity. Extensive\nexperiments demonstrate that LSSR significantly improves crop boundary\ndelineation and recovery, achieving state-of-the-art performance with Peak\nSignal-to-Noise Ratio/Structural Similarity Index Measure of 32.63/0.84 (RGB)\nand 23.99/0.78 (IR), and the lowest NDVI Mean Squared Error (0.042), while\nmaintaining efficient inference (0.39 sec/image). Moreover, LSSR transfers\neffectively to NASA Harmonized Landsat and Sentinel (HLS) super resolution,\nyielding more reliable crop classification (F1: 0.86) than Sentinel-2 (F1:\n0.85). These results highlight the potential of RSSR to advance precision\nagriculture.",
            "headline_zh": "提出高效遥感超分辨率方法LSSR，利用扩散先验和多模态约束提升作物类型制图",
            "intro_zh": [
                "核心问题：扩散模型在遥感超分辨率中训练资源高、推理慢，且辅助信息利用不足。",
                "方法要点：基于预训练Stable Diffusion，集成多模态注意力与适配器，优化傅里叶NDVI损失。",
                "实验效果：在作物边界恢复和分类任务中达到SOTA，推理高效且下游任务表现优异。"
            ],
            "tags_zh": [
                "遥感超分辨率",
                "扩散模型",
                "多模态约束",
                "作物类型制图",
                "高效推理",
                "傅里叶损失"
            ],
            "_index": 39
        },
        {
            "title": "PlanarTrack: A high-quality and challenging benchmark for large-scale planar object tracking",
            "authors": [
                "Yifan Jiao",
                "Xinran Liu",
                "Xiaoqiong Liu",
                "Xiaohui Yuan",
                "Heng Fan",
                "Libo Zhang"
            ],
            "arxiv_id": "2510.23368v1",
            "summary": "Planar tracking has drawn increasing interest owing to its key roles in\nrobotics and augmented reality. Despite recent great advancement, further\ndevelopment of planar tracking, particularly in the deep learning era, is\nlargely limited compared to generic tracking due to the lack of large-scale\nplatforms. To mitigate this, we propose PlanarTrack, a large-scale high-quality\nand challenging benchmark for planar tracking. Specifically, PlanarTrack\nconsists of 1,150 sequences with over 733K frames, including 1,000 short-term\nand 150 new long-term videos, which enables comprehensive evaluation of short-\nand long-term tracking performance. All videos in PlanarTrack are recorded in\nunconstrained conditions from the wild, which makes PlanarTrack challenging but\nmore realistic for real-world applications. To ensure high-quality annotations,\neach video frame is manually annotated by four corner points with multi-round\nmeticulous inspection and refinement. To enhance target diversity of\nPlanarTrack, we only capture a unique target in one sequence, which is\ndifferent from existing benchmarks. To our best knowledge, PlanarTrack is by\nfar the largest and most diverse and challenging dataset dedicated to planar\ntracking. To understand performance of existing methods on PlanarTrack and to\nprovide a comparison for future research, we evaluate 10 representative planar\ntrackers with extensive comparison and in-depth analysis. Our evaluation\nreveals that, unsurprisingly, the top planar trackers heavily degrade on the\nchallenging PlanarTrack, which indicates more efforts are required for\nimproving planar tracking. Our data and results will be released at\nhttps://github.com/HengLan/PlanarTrack",
            "headline_zh": "提出PlanarTrack基准以解决平面跟踪缺乏大规模平台的问题",
            "intro_zh": [
                "平面跟踪在机器人和增强现实中应用广泛，但缺乏大规模数据集限制发展",
                "构建包含1150个序列、733K帧的大规模高质量基准，支持短长期跟踪评估",
                "评估10种现有方法，显示性能显著下降，表明需进一步改进平面跟踪技术"
            ],
            "tags_zh": [
                "平面目标跟踪",
                "大规模基准数据集",
                "长短期跟踪评估",
                "真实世界应用",
                "手动标注质量"
            ],
            "_index": 40
        },
        {
            "title": "Interpretable Tile-Based Classification of Paclitaxel Exposure",
            "authors": [
                "Sean Fletcher",
                "Gabby Scott",
                "Douglas Currie",
                "Xin Zhang",
                "Yuqi Song",
                "Bruce MacLeod"
            ],
            "arxiv_id": "2510.23363v1",
            "summary": "Medical image analysis is central to drug discovery and preclinical\nevaluation, where scalable, objective readouts can accelerate decision-making.\nWe address classification of paclitaxel (Taxol) exposure from phase-contrast\nmicroscopy of C6 glioma cells -- a task with subtle dose differences that\nchallenges full-image models. We propose a simple tiling-and-aggregation\npipeline that operates on local patches and combines tile outputs into an image\nlabel, achieving state-of-the-art accuracy on the benchmark dataset and\nimproving over the published baseline by around 20 percentage points, with\ntrends confirmed by cross-validation. To understand why tiling is effective, we\nfurther apply Grad-CAM and Score-CAM and attention analyses, which enhance\nmodel interpretability and point toward robustness-oriented directions for\nfuture medical image research. Code is released to facilitate reproduction and\nextension.",
            "headline_zh": "提出基于分块与聚合的管道，用于紫杉醇暴露分类，提升准确性与可解释性。",
            "intro_zh": [
                "核心问题：从C6胶质瘤细胞相位对比显微镜图像中分类紫杉醇暴露，剂量差异细微，挑战全图像模型。",
                "方法要点：采用简单分块与聚合流程，处理局部图像块并整合输出，实现图像级分类。",
                "实验或效果：在基准数据集上达到最先进准确率，比基线提升约20个百分点，并通过交叉验证确认趋势。"
            ],
            "tags_zh": [
                "医学图像分析",
                "分块分类",
                "紫杉醇暴露",
                "模型可解释性",
                "Grad-CAM",
                "Score-CAM"
            ],
            "_index": 41
        },
        {
            "title": "T-ESKF: Transformed Error-State Kalman Filter for Consistent Visual-Inertial Navigation",
            "authors": [
                "Chungeng Tian",
                "Ning Hao",
                "Fenghua He"
            ],
            "arxiv_id": "2510.23359v1",
            "summary": "This paper presents a novel approach to address the inconsistency problem\ncaused by observability mismatch in visual-inertial navigation systems (VINS).\nThe key idea involves applying a linear time-varying transformation to the\nerror-state within the Error-State Kalman Filter (ESKF). This transformation\nensures that \\textrr{the unobservable subspace of the transformed error-state\nsystem} becomes independent of the state, thereby preserving the correct\nobservability of the transformed system against variations in linearization\npoints. We introduce the Transformed ESKF (T-ESKF), a consistent VINS estimator\nthat performs state estimation using the transformed error-state system.\nFurthermore, we develop an efficient propagation technique to accelerate the\ncovariance propagation based on the transformation relationship between the\ntransition and accumulated matrices of T-ESKF and ESKF. We validate the\nproposed method through extensive simulations and experiments, demonstrating\nbetter (or competitive at least) performance compared to state-of-the-art\nmethods. The code is available at github.com/HITCSC/T-ESKF.",
            "headline_zh": "提出变换误差状态卡尔曼滤波器以解决视觉-惯性导航中的不一致性问题",
            "intro_zh": [
                "核心问题：视觉-惯性导航系统因可观测性失配导致估计不一致",
                "方法要点：在线性时变变换下变换误差状态，确保可观测子空间与状态无关",
                "实验或效果：仿真与实验验证性能优于或至少与先进方法相当"
            ],
            "tags_zh": [
                "视觉-惯性导航",
                "误差状态卡尔曼滤波",
                "可观测性分析",
                "状态估计",
                "传感器融合"
            ],
            "_index": 42
        },
        {
            "title": "Large language model-based task planning for service robots: A review",
            "authors": [
                "Shaohan Bian",
                "Ying Zhang",
                "Guohui Tian",
                "Zhiqiang Miao",
                "Edmond Q. Wu",
                "Simon X. Yang",
                "Changchun Hua"
            ],
            "arxiv_id": "2510.23357v1",
            "summary": "With the rapid advancement of large language models (LLMs) and robotics,\nservice robots are increasingly becoming an integral part of daily life,\noffering a wide range of services in complex environments. To deliver these\nservices intelligently and efficiently, robust and accurate task planning\ncapabilities are essential. This paper presents a comprehensive overview of the\nintegration of LLMs into service robotics, with a particular focus on their\nrole in enhancing robotic task planning. First, the development and\nfoundational techniques of LLMs, including pre-training, fine-tuning,\nretrieval-augmented generation (RAG), and prompt engineering, are reviewed. We\nthen explore the application of LLMs as the cognitive core-`brain'-of service\nrobots, discussing how LLMs contribute to improved autonomy and\ndecision-making. Furthermore, recent advancements in LLM-driven task planning\nacross various input modalities are analyzed, including text, visual, audio,\nand multimodal inputs. Finally, we summarize key challenges and limitations in\ncurrent research and propose future directions to advance the task planning\ncapabilities of service robots in complex, unstructured domestic environments.\nThis review aims to serve as a valuable reference for researchers and\npractitioners in the fields of artificial intelligence and robotics.",
            "headline_zh": "综述大语言模型在服务机器人任务规划中的应用与挑战",
            "intro_zh": [
                "核心问题：服务机器人在复杂环境中需要智能任务规划以实现高效服务。",
                "方法要点：回顾LLM预训练、微调、RAG和提示工程等基础技术。",
                "实验或效果：分析LLM在多模态输入下的任务规划进展，并总结未来方向。"
            ],
            "tags_zh": [
                "大语言模型",
                "服务机器人",
                "任务规划",
                "多模态输入",
                "认知核心",
                "综述研究"
            ],
            "_index": 43
        },
        {
            "title": "Transferable Deep Reinforcement Learning for Cross-Domain Navigation: from Farmland to the Moon",
            "authors": [
                "Shreya Santra",
                "Thomas Robbins",
                "Kazuya Yoshida"
            ],
            "arxiv_id": "2510.23329v1",
            "summary": "Autonomous navigation in unstructured environments is essential for field and\nplanetary robotics, where robots must efficiently reach goals while avoiding\nobstacles under uncertain conditions. Conventional algorithmic approaches often\nrequire extensive environment-specific tuning, limiting scalability to new\ndomains. Deep Reinforcement Learning (DRL) provides a data-driven alternative,\nallowing robots to acquire navigation strategies through direct interactions\nwith their environment. This work investigates the feasibility of DRL policy\ngeneralization across visually and topographically distinct simulated domains,\nwhere policies are trained in terrestrial settings and validated in a zero-shot\nmanner in extraterrestrial environments. A 3D simulation of an agricultural\nrover is developed and trained using Proximal Policy Optimization (PPO) to\nachieve goal-directed navigation and obstacle avoidance in farmland settings.\nThe learned policy is then evaluated in a lunar-like simulated environment to\nassess transfer performance. The results indicate that policies trained under\nterrestrial conditions retain a high level of effectiveness, achieving close to\n50\\% success in lunar simulations without the need for additional training and\nfine-tuning. This underscores the potential of cross-domain DRL-based policy\ntransfer as a promising approach to developing adaptable and efficient\nautonomous navigation for future planetary exploration missions, with the added\nbenefit of minimizing retraining costs.",
            "headline_zh": "提出可迁移深度强化学习策略，实现从农田到月球的跨域自主导航。",
            "intro_zh": [
                "核心问题：传统导航方法需环境特定调优，难以适应新领域。",
                "方法要点：使用PPO算法训练DRL策略，在农田模拟中学习导航与避障。",
                "实验效果：零样本迁移至月球模拟，成功率近50%，无需额外训练。"
            ],
            "tags_zh": [
                "深度强化学习",
                "跨域导航",
                "策略迁移",
                "自主机器人",
                "模拟验证"
            ],
            "_index": 44
        },
        {
            "title": "Multitask Multimodal Self-Supervised Learning for Medical Images",
            "authors": [
                "Cristian Simionescu"
            ],
            "arxiv_id": "2510.23325v1",
            "summary": "This thesis works to address a pivotal challenge in medical image analysis:\nthe reliance on extensive labeled datasets, which are often limited due to the\nneed for expert annotation and constrained by privacy and legal issues. By\nfocusing on the development of self-supervised learning techniques and domain\nadaptation methods, this research aims to circumvent these limitations,\npresenting a novel approach to enhance the utility and efficacy of deep\nlearning in medical imaging.\n  Central to this thesis is the development of the Medformer, an innovative\nneural network architecture designed for multitask learning and deep domain\nadaptation. This model is adept at pre-training on diverse medical image\ndatasets, handling varying sizes and modalities, and is equipped with a dynamic\ninput-output adaptation mechanism. This enables efficient processing and\nintegration of a wide range of medical image types, from 2D X-rays to complex\n3D MRIs, thus mitigating the dependency on large labeled datasets.\n  Further, the thesis explores the current state of self-supervised learning in\nmedical imaging. It introduces novel pretext tasks that are capable of\nextracting meaningful information from unlabeled data, significantly advancing\nthe model's interpretative abilities. This approach is validated through\nrigorous experimentation, including the use of the MedMNIST dataset,\ndemonstrating the model's proficiency in learning generalized features\napplicable to various downstream tasks.\n  In summary, this thesis contributes to the advancement of medical image\nanalysis by offering a scalable, adaptable framework that reduces reliance on\nlabeled data. It paves the way for more accurate, efficient diagnostic tools in\nhealthcare, signifying a major step forward in the application of deep learning\nin medical imaging.",
            "headline_zh": "提出Medformer架构以解决医学图像分析中标注数据稀缺问题",
            "intro_zh": [
                "核心问题：医学图像分析依赖大量标注数据，但标注成本高且受隐私限制",
                "方法要点：开发Medformer架构，支持多任务学习和动态输入输出适应",
                "实验或效果：使用MedMNIST数据集验证，模型能学习通用特征用于下游任务"
            ],
            "tags_zh": [
                "自监督学习",
                "医学图像分析",
                "多任务学习",
                "领域适应",
                "Medformer架构"
            ],
            "_index": 45
        },
        {
            "title": "ReconViaGen: Towards Accurate Multi-view 3D Object Reconstruction via Generation",
            "authors": [
                "Jiahao Chang",
                "Chongjie Ye",
                "Yushuang Wu",
                "Yuantao Chen",
                "Yidan Zhang",
                "Zhongjin Luo",
                "Chenghong Li",
                "Yihao Zhi",
                "Xiaoguang Han"
            ],
            "arxiv_id": "2510.23306v1",
            "summary": "Existing multi-view 3D object reconstruction methods heavily rely on\nsufficient overlap between input views, where occlusions and sparse coverage in\npractice frequently yield severe reconstruction incompleteness. Recent\nadvancements in diffusion-based 3D generative techniques offer the potential to\naddress these limitations by leveraging learned generative priors to\nhallucinate invisible parts of objects, thereby generating plausible 3D\nstructures. However, the stochastic nature of the inference process limits the\naccuracy and reliability of generation results, preventing existing\nreconstruction frameworks from integrating such 3D generative priors. In this\nwork, we comprehensively analyze the reasons why diffusion-based 3D generative\nmethods fail to achieve high consistency, including (a) the insufficiency in\nconstructing and leveraging cross-view connections when extracting multi-view\nimage features as conditions, and (b) the poor controllability of iterative\ndenoising during local detail generation, which easily leads to plausible but\ninconsistent fine geometric and texture details with inputs. Accordingly, we\npropose ReconViaGen to innovatively integrate reconstruction priors into the\ngenerative framework and devise several strategies that effectively address\nthese issues. Extensive experiments demonstrate that our ReconViaGen can\nreconstruct complete and accurate 3D models consistent with input views in both\nglobal structure and local details.Project page:\nhttps://jiahao620.github.io/reconviagen.",
            "headline_zh": "提出ReconViaGen以解决多视角3D重建中因遮挡和稀疏覆盖导致的不完整问题",
            "intro_zh": [
                "核心问题：现有方法依赖视图重叠，但遮挡和稀疏覆盖导致重建不完整，扩散生成方法因随机性限制准确性",
                "方法要点：整合重建先验到生成框架，改进跨视图连接提取和迭代去噪可控性",
                "实验或效果：实验显示能重建完整且准确的3D模型，全局结构和局部细节与输入一致"
            ],
            "tags_zh": [
                "多视角3D重建",
                "扩散生成模型",
                "重建先验",
                "跨视图连接",
                "迭代去噪控制"
            ],
            "_index": 46
        },
        {
            "title": "MDReID: Modality-Decoupled Learning for Any-to-Any Multi-Modal Object Re-Identification",
            "authors": [
                "Yingying Feng",
                "Jie Li",
                "Jie Hu",
                "Yukang Zhang",
                "Lei Tan",
                "Jiayi Ji"
            ],
            "arxiv_id": "2510.23301v1",
            "summary": "Real-world object re-identification (ReID) systems often face modality\ninconsistencies, where query and gallery images come from different sensors\n(e.g., RGB, NIR, TIR). However, most existing methods assume modality-matched\nconditions, which limits their robustness and scalability in practical\napplications. To address this challenge, we propose MDReID, a flexible\nany-to-any image-level ReID framework designed to operate under both\nmodality-matched and modality-mismatched scenarios. MDReID builds on the\ninsight that modality information can be decomposed into two components:\nmodality-shared features that are predictable and transferable, and\nmodality-specific features that capture unique, modality-dependent\ncharacteristics. To effectively leverage this, MDReID introduces two key\ncomponents: the Modality Decoupling Learning (MDL) and Modality-aware Metric\nLearning (MML). Specifically, MDL explicitly decomposes modality features into\nmodality-shared and modality-specific representations, enabling effective\nretrieval in both modality-aligned and mismatched scenarios. MML, a tailored\nmetric learning strategy, further enforces orthogonality and complementarity\nbetween the two components to enhance discriminative power across modalities.\nExtensive experiments conducted on three challenging multi-modality ReID\nbenchmarks (RGBNT201, RGBNT100, MSVR310) consistently demonstrate the\nsuperiority of MDReID. Notably, MDReID achieves significant mAP improvements of\n9.8\\%, 3.0\\%, and 11.5\\% in general modality-matched scenarios, and average\ngains of 3.4\\%, 11.8\\%, and 10.9\\% in modality-mismatched scenarios,\nrespectively. The code is available at:\n\\textcolor{magenta}{https://github.com/stone96123/MDReID}.",
            "headline_zh": "提出MDReID框架，通过模态解耦学习解决多模态物体重识别中的模态不一致问题。",
            "intro_zh": [
                "核心问题：现实物体重识别常面临模态不一致，如RGB与NIR图像查询，现有方法假设模态匹配，限制实用性。",
                "方法要点：引入模态解耦学习和模态感知度量学习，分解特征为共享与特定部分，增强跨模态检索能力。",
                "实验效果：在多个基准测试中，模态匹配和不匹配场景下均取得显著mAP提升，最高达11.5%。"
            ],
            "tags_zh": [
                "多模态物体重识别",
                "模态解耦学习",
                "模态感知度量学习",
                "跨模态检索",
                "特征分解"
            ],
            "_index": 47
        },
        {
            "title": "MMSD3.0: A Multi-Image Benchmark for Real-World Multimodal Sarcasm Detection",
            "authors": [
                "Haochen Zhao",
                "Yuyao Kong",
                "Yongxiu Xu",
                "Gaopeng Gou",
                "Hongbo Xu",
                "Yubin Wang",
                "Haoliang Zhang"
            ],
            "arxiv_id": "2510.23299v1",
            "summary": "Despite progress in multimodal sarcasm detection, existing datasets and\nmethods predominantly focus on single-image scenarios, overlooking potential\nsemantic and affective relations across multiple images. This leaves a gap in\nmodeling cases where sarcasm is triggered by multi-image cues in real-world\nsettings. To bridge this gap, we introduce MMSD3.0, a new benchmark composed\nentirely of multi-image samples curated from tweets and Amazon reviews. We\nfurther propose the Cross-Image Reasoning Model (CIRM), which performs targeted\ncross-image sequence modeling to capture latent inter-image connections. In\naddition, we introduce a relevance-guided, fine-grained cross-modal fusion\nmechanism based on text-image correspondence to reduce information loss during\nintegration. We establish a comprehensive suite of strong and representative\nbaselines and conduct extensive experiments, showing that MMSD3.0 is an\neffective and reliable benchmark that better reflects real-world conditions.\nMoreover, CIRM demonstrates state-of-the-art performance across MMSD, MMSD2.0\nand MMSD3.0, validating its effectiveness in both single-image and multi-image\nscenarios.",
            "headline_zh": "提出MMSD3.0多图像基准和CIRM模型以解决真实世界多模态讽刺检测问题",
            "intro_zh": [
                "现有方法多关注单图像场景，忽略多图像间的语义和情感关系，导致真实世界讽刺检测不足",
                "引入跨图像推理模型CIRM，通过序列建模和相关性引导的跨模态融合捕获图像间连接",
                "在MMSD系列基准上实验，CIRM实现先进性能，验证其在单图像和多图像场景的有效性"
            ],
            "tags_zh": [
                "多模态讽刺检测",
                "多图像基准",
                "跨图像推理",
                "跨模态融合",
                "真实世界数据集"
            ],
            "_index": 48
        },
        {
            "title": "Payload trajectory tracking control for aerial transportation systems with cable length online optimization",
            "authors": [
                "Hai Yu",
                "Zhichao Yang",
                "Wei He",
                "Jianda Han",
                "Yongchun Fang",
                "Xiao Liang"
            ],
            "arxiv_id": "2510.23296v1",
            "summary": "Cable-suspended aerial transportation systems are employed extensively across\nvarious industries. The capability to flexibly adjust the relative position\nbetween the multirotor and the payload has spurred growing interest in the\nsystem equipped with variable-length cable, promising broader application\npotential. Compared to systems with fixed-length cables, introducing the\nvariable-length cable adds a new degree of freedom. However, it also results in\nincreased nonlinearity and more complex dynamic coupling among the multirotor,\nthe cable and the payload, posing significant challenges in control design.\nThis paper introduces a backstepping control strategy tailored for aerial\ntransportation systems with variable-length cable, designed to precisely track\nthe payload trajectory while dynamically adjusting cable length. Then, a cable\nlength generator has been developed that achieves online optimization of the\ncable length while satisfying state constraints, thus balancing the\nmultirotor's motion and cable length changes without the need for manual\ntrajectory planning. The asymptotic stability of the closed-loop system is\nguaranteed through Lyapunov techniques and the growth restriction condition.\nFinally, simulation results confirm the efficacy of the proposed method in\nmanaging trajectory tracking and cable length adjustments effectively.",
            "headline_zh": "提出基于反步控制与缆长在线优化的方法，用于可变缆长空中运输系统的载荷轨迹跟踪。",
            "intro_zh": [
                "核心问题：可变缆长引入非线性与动态耦合，增加控制设计难度。",
                "方法要点：采用反步控制策略，结合缆长生成器在线优化缆长。",
                "实验或效果：仿真验证方法有效管理轨迹跟踪与缆长调整。"
            ],
            "tags_zh": [
                "空中运输系统",
                "反步控制",
                "缆长优化",
                "轨迹跟踪",
                "非线性控制",
                "动态耦合"
            ],
            "_index": 49
        },
        {
            "title": "Precise Time Delay Measurement and Compensation for Tightly Coupled Underwater SINS/piUSBL Navigation",
            "authors": [
                "Jin Huang",
                "Yingqiang Wang",
                "Haoda Li",
                "Zichen Liu",
                "Zhikun Wang",
                "Ying Chen"
            ],
            "arxiv_id": "2510.23286v1",
            "summary": "In multi-sensor systems, time synchronization between sensors is a\nsignificant challenge, and this issue is particularly pronounced in underwater\nintegrated navigation systems incorporating acoustic positioning. Such systems\nare highly susceptible to time delay, which can significantly degrade accuracy\nwhen measurement and fusion moments are misaligned. To address this challenge,\nthis paper introduces a tightly coupled navigation framework that integrates a\npassive inverted ultra-short baseline (piUSBL) acoustic positioning system, a\nstrapdown inertial navigation system (SINS), and a depth gauge under precise\ntime synchronization. The framework fuses azimuth and slant range from the\npiUSBL with depth data, thereby avoiding poor vertical-angle observability in\nplanar arrays. A novel delay measurement strategy is introduced, combining\nsynchronized timing with acoustic signal processing, which redefines\ndelay-traditionally an unobservable error-into a quantifiable parameter,\nenabling explicit estimation of both acoustic propagation and system processing\ndelays. Simulations and field experiments confirm the feasibility of the\nproposed method, with delay-compensated navigation reducing RMSE by 40.45% and\nmaximum error by 32.55%. These findings show that precise delay measurement and\ncompensation not only enhance underwater navigation accuracy but also establish\na generalizable framework for acoustic positioning integration, offering\nvaluable insights into time alignment and data fusion in latency-sensitive\nmulti-sensor systems.",
            "headline_zh": "提出紧密耦合水下SINS/piUSBL导航框架，通过精确时延测量与补偿提升精度",
            "intro_zh": [
                "核心问题：水下集成导航系统中传感器时间不同步，导致测量融合误差显著。",
                "方法要点：结合同步定时与声信号处理，将时延量化为可估计参数。",
                "实验或效果：仿真与现场实验显示，时延补偿使RMSE降低40.45%，最大误差减少32.55%。"
            ],
            "tags_zh": [
                "水下导航",
                "时延补偿",
                "紧密耦合",
                "声学定位",
                "多传感器融合",
                "惯性导航系统"
            ],
            "_index": 50
        },
        {
            "title": "Adaptive Stochastic Coefficients for Accelerating Diffusion Sampling",
            "authors": [
                "Ruoyu Wang",
                "Beier Zhu",
                "Junzhi Li",
                "Liangyu Yuan",
                "Chi Zhang"
            ],
            "arxiv_id": "2510.23285v1",
            "summary": "Diffusion-based generative processes, formulated as differential equation\nsolving, frequently balance computational speed with sample quality. Our\ntheoretical investigation of ODE- and SDE-based solvers reveals complementary\nweaknesses: ODE solvers accumulate irreducible gradient error along\ndeterministic trajectories, while SDE methods suffer from amplified\ndiscretization errors when the step budget is limited. Building upon this\ninsight, we introduce AdaSDE, a novel single-step SDE solver that aims to unify\nthe efficiency of ODEs with the error resilience of SDEs. Specifically, we\nintroduce a single per-step learnable coefficient, estimated via lightweight\ndistillation, which dynamically regulates the error correction strength to\naccelerate diffusion sampling. Notably, our framework can be integrated with\nexisting solvers to enhance their capabilities. Extensive experiments\ndemonstrate state-of-the-art performance: at 5 NFE, AdaSDE achieves FID scores\nof 4.18 on CIFAR-10, 8.05 on FFHQ and 6.96 on LSUN Bedroom. Codes are available\nin https://github.com/WLU-wry02/AdaSDE.",
            "headline_zh": "提出AdaSDE以加速扩散采样，平衡ODE与SDE的误差问题",
            "intro_zh": [
                "核心问题：ODE和SDE求解器在扩散采样中存在梯度误差和离散化误差的互补弱点",
                "方法要点：引入可学习系数动态调节误差修正强度，通过轻量蒸馏估计",
                "实验或效果：在5 NFE下，CIFAR-10 FID为4.18，FFHQ为8.05，LSUN Bedroom为6.96"
            ],
            "tags_zh": [
                "扩散模型",
                "采样加速",
                "误差修正",
                "轻量蒸馏",
                "SDE求解器"
            ],
            "_index": 51
        },
        {
            "title": "hYOLO Model: Enhancing Object Classification with Hierarchical Context in YOLOv8",
            "authors": [
                "Veska Tsenkova",
                "Peter Stanchev",
                "Daniel Petrov",
                "Deyan Lazarov"
            ],
            "arxiv_id": "2510.23278v1",
            "summary": "Current convolution neural network (CNN) classification methods are\npredominantly focused on flat classification which aims solely to identify a\nspecified object within an image. However, real-world objects often possess a\nnatural hierarchical organization that can significantly help classification\ntasks. Capturing the presence of relations between objects enables better\ncontextual understanding as well as control over the severity of mistakes.\nConsidering these aspects, this paper proposes an end-to-end hierarchical model\nfor image detection and classification built upon the YOLO model family. A\nnovel hierarchical architecture, a modified loss function, and a performance\nmetric tailored to the hierarchical nature of the model are introduced. The\nproposed model is trained and evaluated on two different hierarchical\ncategorizations of the same dataset: a systematic categorization that\ndisregards visual similarities between objects and a categorization accounting\nfor common visual characteristics across classes. The results illustrate how\nthe suggested methodology addresses the inherent hierarchical structure present\nin real-world objects, which conventional flat classification algorithms often\noverlook.",
            "headline_zh": "提出hYOLO模型，在YOLOv8中引入层次结构以增强物体分类",
            "intro_zh": [
                "核心问题：传统CNN分类忽略物体层次关系，影响分类准确性。",
                "方法要点：设计层次架构、修改损失函数和性能指标，实现端到端检测。",
                "实验或效果：在两种层次分类数据集上验证，提升上下文理解和错误控制。"
            ],
            "tags_zh": [
                "层次分类",
                "YOLO模型",
                "损失函数优化",
                "物体检测",
                "上下文建模"
            ],
            "_index": 52
        },
        {
            "title": "Deep Active Inference with Diffusion Policy and Multiple Timescale World Model for Real-World Exploration and Navigation",
            "authors": [
                "Riko Yokozawa",
                "Kentaro Fujii",
                "Yuta Nomura",
                "Shingo Murata"
            ],
            "arxiv_id": "2510.23258v1",
            "summary": "Autonomous robotic navigation in real-world environments requires exploration\nto acquire environmental information as well as goal-directed navigation in\norder to reach specified targets. Active inference (AIF) based on the\nfree-energy principle provides a unified framework for these behaviors by\nminimizing the expected free energy (EFE), thereby combining epistemic and\nextrinsic values. To realize this practically, we propose a deep AIF framework\nthat integrates a diffusion policy as the policy model and a multiple timescale\nrecurrent state-space model (MTRSSM) as the world model. The diffusion policy\ngenerates diverse candidate actions while the MTRSSM predicts their\nlong-horizon consequences through latent imagination, enabling action selection\nthat minimizes EFE. Real-world navigation experiments demonstrated that our\nframework achieved higher success rates and fewer collisions compared with the\nbaselines, particularly in exploration-demanding scenarios. These results\nhighlight how AIF based on EFE minimization can unify exploration and\ngoal-directed navigation in real-world robotic settings.",
            "headline_zh": "提出基于扩散策略和多时间尺度世界模型的深度主动推理框架，用于真实世界机器人探索与导航。",
            "intro_zh": [
                "核心问题：真实世界机器人导航需平衡探索获取环境信息与目标导向导航。",
                "方法要点：结合扩散策略生成多样动作，多时间尺度世界模型预测长期后果以最小化期望自由能。",
                "实验效果：真实世界实验显示，在探索需求高场景中，成功率更高且碰撞更少。"
            ],
            "tags_zh": [
                "主动推理",
                "扩散策略",
                "多时间尺度世界模型",
                "机器人导航",
                "期望自由能最小化"
            ],
            "_index": 53
        },
        {
            "title": "A Video Is Not Worth a Thousand Words",
            "authors": [
                "Sam Pollard",
                "Michael Wray"
            ],
            "arxiv_id": "2510.23253v1",
            "summary": "As we become increasingly dependent on vision language models (VLMs) to\nanswer questions about the world around us, there is a significant amount of\nresearch devoted to increasing both the difficulty of video question answering\n(VQA) datasets, and the context lengths of the models that they evaluate. The\nreliance on large language models as backbones has lead to concerns about\npotential text dominance, and the exploration of interactions between\nmodalities is underdeveloped. How do we measure whether we're heading in the\nright direction, with the complexity that multi-modal models introduce? We\npropose a joint method of computing both feature attributions and modality\nscores based on Shapley values, where both the features and modalities are\narbitrarily definable. Using these metrics, we compare $6$ VLM models of\nvarying context lengths on $4$ representative datasets, focusing on\nmultiple-choice VQA. In particular, we consider video frames and whole textual\nelements as equal features in the hierarchy, and the multiple-choice VQA task\nas an interaction between three modalities: video, question and answer. Our\nresults demonstrate a dependence on text and show that the multiple-choice VQA\ntask devolves into a model's ability to ignore distractors. Code available at\nhttps://github.com/sjpollard/a-video-is-not-worth-a-thousand-words.",
            "headline_zh": "提出基于Shapley值的特征归因与模态评分方法，评估多模态模型在视频问答中的文本依赖问题。",
            "intro_zh": [
                "核心问题：多模态模型在视频问答中可能过度依赖文本，忽略视频模态的交互。",
                "方法要点：使用Shapley值计算特征归因和模态分数，支持任意定义特征和模态。",
                "实验或效果：比较6个模型在4个数据集上，发现模型倾向于忽略干扰项，依赖文本。"
            ],
            "tags_zh": [
                "视频问答",
                "多模态模型",
                "Shapley值",
                "特征归因",
                "模态交互",
                "文本依赖"
            ],
            "_index": 54
        },
        {
            "title": "Progressive Growing of Patch Size: Curriculum Learning for Accelerated and Improved Medical Image Segmentation",
            "authors": [
                "Stefan M. Fischer",
                "Johannes Kiechle",
                "Laura Daza",
                "Lina Felsner",
                "Richard Osuala",
                "Daniel M. Lang",
                "Karim Lekadir",
                "Jan C. Peeken",
                "Julia A. Schnabel"
            ],
            "arxiv_id": "2510.23241v1",
            "summary": "In this work, we introduce Progressive Growing of Patch Size, an automatic\ncurriculum learning approach for 3D medical image segmentation. Our approach\nprogressively increases the patch size during model training, resulting in an\nimproved class balance for smaller patch sizes and accelerated convergence of\nthe training process. We evaluate our curriculum approach in two settings: a\nresource-efficient mode and a performance mode, both regarding Dice score\nperformance and computational costs across 15 diverse and popular 3D medical\nimage segmentation tasks. The resource-efficient mode matches the Dice score\nperformance of the conventional constant patch size sampling baseline with a\nnotable reduction in training time to only 44%. The performance mode improves\nupon constant patch size segmentation results, achieving a statistically\nsignificant relative mean performance gain of 1.28% in Dice Score. Remarkably,\nacross all 15 tasks, our proposed performance mode manages to surpass the\nconstant patch size baseline in Dice Score performance, while simultaneously\nreducing training time to only 89%. The benefits are particularly pronounced\nfor highly imbalanced tasks such as lesion segmentation tasks. Rigorous\nexperiments demonstrate that our performance mode not only improves mean\nsegmentation performance but also reduces performance variance, yielding more\ntrustworthy model comparison. Furthermore, our findings reveal that the\nproposed curriculum sampling is not tied to a specific architecture but\nrepresents a broadly applicable strategy that consistently boosts performance\nacross diverse segmentation models, including UNet, UNETR, and SwinUNETR. In\nsummary, we show that this simple yet elegant transformation on input data\nsubstantially improves both Dice Score performance and training runtime, while\nbeing compatible across diverse segmentation backbones.",
            "headline_zh": "提出渐进式补丁大小增长方法，以加速和改进3D医学图像分割。",
            "intro_zh": [
                "核心问题：3D医学图像分割中类不平衡和训练效率低的问题。",
                "方法要点：训练中逐步增大补丁大小，改善类平衡并加速收敛。",
                "实验或效果：在15个任务中，性能模式提升Dice分数1.28%，训练时间减少至89%。"
            ],
            "tags_zh": [
                "3D医学图像分割",
                "课程学习",
                "渐进式训练",
                "补丁大小优化",
                "计算效率提升"
            ],
            "_index": 55
        },
        {
            "title": "Autoregressive Styled Text Image Generation, but Make it Reliable",
            "authors": [
                "Carmine Zaccagnino",
                "Fabio Quattrini",
                "Vittorio Pippi",
                "Silvia Cascianelli",
                "Alessio Tonioni",
                "Rita Cucchiara"
            ],
            "arxiv_id": "2510.23240v1",
            "summary": "Generating faithful and readable styled text images (especially for Styled\nHandwritten Text generation - HTG) is an open problem with several possible\napplications across graphic design, document understanding, and image editing.\nA lot of research effort in this task is dedicated to developing strategies\nthat reproduce the stylistic characteristics of a given writer, with promising\nresults in terms of style fidelity and generalization achieved by the recently\nproposed Autoregressive Transformer paradigm for HTG. However, this method\nrequires additional inputs, lacks a proper stop mechanism, and might end up in\nrepetition loops, generating visual artifacts. In this work, we rethink the\nautoregressive formulation by framing HTG as a multimodal prompt-conditioned\ngeneration task, and tackle the content controllability issues by introducing\nspecial textual input tokens for better alignment with the visual ones.\nMoreover, we devise a Classifier-Free-Guidance-based strategy for our\nautoregressive model. Through extensive experimental validation, we demonstrate\nthat our approach, dubbed Eruku, compared to previous solutions requires fewer\ninputs, generalizes better to unseen styles, and follows more faithfully the\ntextual prompt, improving content adherence.",
            "headline_zh": "提出Eruku方法以解决自回归风格文本图像生成中的内容可控性问题",
            "intro_zh": [
                "核心问题：自回归风格文本图像生成存在内容对齐差、缺乏停止机制和视觉伪影问题",
                "方法要点：将生成任务重构为多模态提示条件生成，引入特殊文本令牌和分类器自由引导策略",
                "实验或效果：Eruku减少输入需求，提升未见风格泛化能力和文本提示遵循度"
            ],
            "tags_zh": [
                "风格文本图像生成",
                "自回归模型",
                "多模态提示",
                "分类器自由引导",
                "内容对齐",
                "手写文本生成"
            ],
            "_index": 56
        },
        {
            "title": "Optimal Dimensioning of Elastic-Link Manipulators regarding Lifetime Estimation",
            "authors": [
                "Klaus Zauner",
                "Hubert Gattringer",
                "Andreas Mueller"
            ],
            "arxiv_id": "2510.23234v1",
            "summary": "Resourceful operation and design of robots is key for sustainable industrial\nautomation. This will be enabled by lightweight design along with time and\nenergy optimal control of robotic manipulators. Design and control of such\nsystems is intertwined as the control must take into account inherent\nmechanical compliance while the design must accommodate the dynamic\nrequirements demanded by the control. As basis for such design optimization, a\nmethod for estimating the lifetime of elastic link robotic manipulators is\npresented. This is applied to the geometry optimization of flexible serial\nmanipulators performing pick-and-place operations, where the optimization\nobjective is a combination of overall weight and vibration amplitudes. The\nlifetime estimation draws from a fatigue analysis combining the rainflow\ncounting algorithm and the method of critical cutting plane. Tresca hypothesis\nis used to formulate an equivalent stress, and linear damage accumulation is\nassumed. The final robot geometry is selected from a Pareto front as a tradeoff\nof lifetime and vibration characteristic. The method is illustrated for a three\ndegrees of freedom articulated robotic manipulator.",
            "headline_zh": "提出弹性连杆机械臂寿命估计方法，用于几何优化以平衡重量和振动。",
            "intro_zh": [
                "核心问题：弹性机械臂在轻量化和动态控制下的寿命估计与几何优化。",
                "方法要点：结合雨流计数和临界切割平面法进行疲劳分析，使用Tresca假设和线性损伤累积。",
                "实验或效果：应用于三自由度机械臂，通过Pareto前沿选择几何，权衡寿命与振动特性。"
            ],
            "tags_zh": [
                "弹性连杆机械臂",
                "寿命估计",
                "几何优化",
                "疲劳分析",
                "Pareto优化",
                "振动控制"
            ],
            "_index": 57
        },
        {
            "title": "Workspace Registration and Collision Detection for Industrial Robotics Applications",
            "authors": [
                "Klaus Zauner",
                "Josef El Dib",
                "Hubert Gattringer",
                "Andreas Mueller"
            ],
            "arxiv_id": "2510.23227v1",
            "summary": "Motion planning for robotic manipulators relies on precise knowledge of the\nenvironment in order to be able to define restricted areas and to take\ncollision objects into account. To capture the workspace, point clouds of the\nenvironment are acquired using various sensors. The collision objects are\nidentified by region growing segmentation and VCCS algorithm. Subsequently the\npoint clusters are approximated. The aim of the present paper is to compare\ndifferent sensors, to illustrate the process from detection to the finished\ncollision environment and to detect collisions between the robot and this\nenvironment.",
            "headline_zh": "比较传感器与流程，实现工业机器人工作空间注册与碰撞检测",
            "intro_zh": [
                "核心问题：机器人运动规划需精确环境知识，以定义限制区域和考虑碰撞对象。",
                "方法要点：使用点云采集、区域生长分割和VCCS算法识别碰撞对象并近似点簇。",
                "实验或效果：比较不同传感器，展示从检测到完整碰撞环境的流程，检测机器人与环境碰撞。"
            ],
            "tags_zh": [
                "工业机器人",
                "工作空间注册",
                "碰撞检测",
                "点云处理",
                "区域生长分割",
                "VCCS算法"
            ],
            "_index": 58
        },
        {
            "title": "Through the Lens: Benchmarking Deepfake Detectors Against Moiré-Induced Distortions",
            "authors": [
                "Razaib Tariq",
                "Minji Heo",
                "Simon S. Woo",
                "Shahroz Tariq"
            ],
            "arxiv_id": "2510.23225v1",
            "summary": "Deepfake detection remains a pressing challenge, particularly in real-world\nsettings where smartphone-captured media from digital screens often introduces\nMoir\\'e artifacts that can distort detection outcomes. This study\nsystematically evaluates state-of-the-art (SOTA) deepfake detectors on\nMoir\\'e-affected videos, an issue that has received little attention. We\ncollected a dataset of 12,832 videos, spanning 35.64 hours, from the Celeb-DF,\nDFD, DFDC, UADFV, and FF++ datasets, capturing footage under diverse real-world\nconditions, including varying screens, smartphones, lighting setups, and camera\nangles. To further examine the influence of Moir\\'e patterns on deepfake\ndetection, we conducted additional experiments using our DeepMoir\\'eFake,\nreferred to as (DMF) dataset and two synthetic Moir\\'e generation techniques.\nAcross 15 top-performing detectors, our results show that Moir\\'e artifacts\ndegrade performance by as much as 25.4%, while synthetically generated Moir\\'e\npatterns lead to a 21.4% drop in accuracy. Surprisingly, demoir\\'eing methods,\nintended as a mitigation approach, instead worsened the problem, reducing\naccuracy by up to 17.2%. These findings underscore the urgent need for\ndetection models that can robustly handle Moir\\'e distortions alongside other\nrealworld challenges, such as compression, sharpening, and blurring. By\nintroducing the DMF dataset, we aim to drive future research toward closing the\ngap between controlled experiments and practical deepfake detection.",
            "headline_zh": "评估深度伪造检测器在莫尔纹失真下的性能，揭示其显著下降并提出DMF数据集。",
            "intro_zh": [
                "核心问题：智能手机拍摄数字屏幕引入莫尔纹，导致深度伪造检测性能下降。",
                "方法要点：使用真实和合成莫尔纹数据集，系统评估15种先进检测器。",
                "实验或效果：莫尔纹使检测准确率下降达25.4%，去莫尔纹方法反而恶化问题。"
            ],
            "tags_zh": [
                "深度伪造检测",
                "莫尔纹失真",
                "数据集构建",
                "性能评估",
                "真实世界挑战"
            ],
            "_index": 59
        },
        {
            "title": "Accurate and Scalable Multimodal Pathology Retrieval via Attentive Vision-Language Alignment",
            "authors": [
                "Hongyi Wang",
                "Zhengjie Zhu",
                "Jiabo Ma",
                "Fang Wang",
                "Yue Shi",
                "Bo Luo",
                "Jili Wang",
                "Qiuyu Cai",
                "Xiuming Zhang",
                "Yen-Wei Chen",
                "Lanfen Lin",
                "Hao Chen"
            ],
            "arxiv_id": "2510.23224v1",
            "summary": "The rapid digitization of histopathology slides has opened up new\npossibilities for computational tools in clinical and research workflows. Among\nthese, content-based slide retrieval stands out, enabling pathologists to\nidentify morphologically and semantically similar cases, thereby supporting\nprecise diagnoses, enhancing consistency across observers, and assisting\nexample-based education. However, effective retrieval of whole slide images\n(WSIs) remains challenging due to their gigapixel scale and the difficulty of\ncapturing subtle semantic differences amid abundant irrelevant content. To\novercome these challenges, we present PathSearch, a retrieval framework that\nunifies fine-grained attentive mosaic representations with global-wise slide\nembeddings aligned through vision-language contrastive learning. Trained on a\ncorpus of 6,926 slide-report pairs, PathSearch captures both fine-grained\nmorphological cues and high-level semantic patterns to enable accurate and\nflexible retrieval. The framework supports two key functionalities: (1)\nmosaic-based image-to-image retrieval, ensuring accurate and efficient slide\nresearch; and (2) multi-modal retrieval, where text queries can directly\nretrieve relevant slides. PathSearch was rigorously evaluated on four public\npathology datasets and three in-house cohorts, covering tasks including\nanatomical site retrieval, tumor subtyping, tumor vs. non-tumor discrimination,\nand grading across diverse organs such as breast, lung, kidney, liver, and\nstomach. External results show that PathSearch outperforms traditional\nimage-to-image retrieval frameworks. A multi-center reader study further\ndemonstrates that PathSearch improves diagnostic accuracy, boosts confidence,\nand enhances inter-observer agreement among pathologists in real clinical\nscenarios. These results establish PathSearch as a scalable and generalizable\nretrieval solution for digital pathology.",
            "headline_zh": "提出PathSearch框架以解决数字病理学中全切片图像检索的挑战",
            "intro_zh": [
                "核心问题：全切片图像规模巨大且语义差异细微，导致检索困难。",
                "方法要点：结合细粒度注意力马赛克表示和全局嵌入，通过视觉-语言对比学习对齐。",
                "实验或效果：在多个数据集上验证，提升检索准确性、诊断信心和观察者间一致性。"
            ],
            "tags_zh": [
                "数字病理学",
                "图像检索",
                "视觉-语言对齐",
                "注意力机制",
                "对比学习",
                "多模态检索"
            ],
            "_index": 60
        },
        {
            "title": "VR-Drive: Viewpoint-Robust End-to-End Driving with Feed-Forward 3D Gaussian Splatting",
            "authors": [
                "Hoonhee Cho",
                "Jae-Young Kang",
                "Giwon Lee",
                "Hyemin Yang",
                "Heejun Park",
                "Seokwoo Jung",
                "Kuk-Jin Yoon"
            ],
            "arxiv_id": "2510.23205v1",
            "summary": "End-to-end autonomous driving (E2E-AD) has emerged as a promising paradigm\nthat unifies perception, prediction, and planning into a holistic, data-driven\nframework. However, achieving robustness to varying camera viewpoints, a common\nreal-world challenge due to diverse vehicle configurations, remains an open\nproblem. In this work, we propose VR-Drive, a novel E2E-AD framework that\naddresses viewpoint generalization by jointly learning 3D scene reconstruction\nas an auxiliary task to enable planning-aware view synthesis. Unlike prior\nscene-specific synthesis approaches, VR-Drive adopts a feed-forward inference\nstrategy that supports online training-time augmentation from sparse views\nwithout additional annotations. To further improve viewpoint consistency, we\nintroduce a viewpoint-mixed memory bank that facilitates temporal interaction\nacross multiple viewpoints and a viewpoint-consistent distillation strategy\nthat transfers knowledge from original to synthesized views. Trained in a fully\nend-to-end manner, VR-Drive effectively mitigates synthesis-induced noise and\nimproves planning under viewpoint shifts. In addition, we release a new\nbenchmark dataset to evaluate E2E-AD performance under novel camera viewpoints,\nenabling comprehensive analysis. Our results demonstrate that VR-Drive is a\nscalable and robust solution for the real-world deployment of end-to-end\nautonomous driving systems.",
            "headline_zh": "提出VR-Drive以解决端到端自动驾驶中相机视角变化的鲁棒性问题",
            "intro_zh": [
                "核心问题：端到端自动驾驶在多样化相机视角下缺乏鲁棒性，影响实际部署。",
                "方法要点：联合学习3D场景重建作为辅助任务，支持前馈推理和视角混合记忆库。",
                "实验或效果：在新型视角基准数据集上验证，提升规划性能并减少合成噪声。"
            ],
            "tags_zh": [
                "端到端自动驾驶",
                "3D高斯泼溅",
                "视角鲁棒性",
                "视图合成",
                "蒸馏策略",
                "基准数据集"
            ],
            "_index": 61
        },
        {
            "title": "If They Disagree, Will You Conform? Exploring the Role of Robots' Value Awareness in a Decision-Making Task",
            "authors": [
                "Giulia Pusceddu",
                "Giulio Antonio Abbo",
                "Francesco Rea",
                "Tony Belpaeme",
                "Alessandra Sciutti"
            ],
            "arxiv_id": "2510.23204v1",
            "summary": "This study investigates whether the opinions of robotic agents are more\nlikely to influence human decision-making when the robots are perceived as\nvalue-aware (i.e., when they display an understanding of human principles). We\ndesigned an experiment in which participants interacted with two Furhat robots\n- one programmed to be Value-Aware and the other Non-Value-Aware - during a\nlabeling task for images representing human values. Results indicate that\nparticipants distinguished the Value-Aware robot from the Non-Value-Aware one.\nAlthough their explicit choices did not indicate a clear preference for one\nrobot over the other, participants directed their gaze more toward the\nValue-Aware robot. Additionally, the Value-Aware robot was perceived as more\nloyal, suggesting that value awareness in a social robot may enhance its\nperceived commitment to the group. Finally, when both robots disagreed with the\nparticipant, conformity occurred in about one out of four trials, and\nparticipants took longer to confirm their responses, suggesting that two robots\nexpressing dissent may introduce hesitation in decision-making. On one hand,\nthis highlights the potential risk that robots, if misused, could manipulate\nusers for unethical purposes. On the other hand, it reinforces the idea that\nsocial robots might encourage reflection in ambiguous situations and help users\navoid scams.",
            "headline_zh": "探索机器人价值意识对人类决策的影响，通过标签任务实验揭示其潜在风险与益处。",
            "intro_zh": [
                "核心问题：机器人价值意识是否增强其对人类决策的影响力。",
                "方法要点：使用价值意识和非价值意识机器人进行图像标签任务交互。",
                "实验效果：参与者注视价值意识机器人更多，感知其更忠诚，且机器人分歧引发犹豫。"
            ],
            "tags_zh": [
                "社会机器人",
                "价值意识",
                "决策影响",
                "人机交互",
                "实验研究",
                "伦理风险"
            ],
            "_index": 62
        },
        {
            "title": "DecoDINO: 3D Human-Scene Contact Prediction with Semantic Classification",
            "authors": [
                "Lukas Bierling",
                "Davide Pasero",
                "Fleur Dolmans",
                "Helia Ghasemi",
                "Angelo Broere"
            ],
            "arxiv_id": "2510.23203v1",
            "summary": "Accurate vertex-level contact prediction between humans and surrounding\nobjects is a prerequisite for high fidelity human object interaction models\nused in robotics, AR/VR, and behavioral simulation. DECO was the first in the\nwild estimator for this task but is limited to binary contact maps and\nstruggles with soft surfaces, occlusions, children, and false-positive foot\ncontacts. We address these issues and introduce DecoDINO, a three-branch\nnetwork based on DECO's framework. It uses two DINOv2 ViT-g/14 encoders,\nclass-balanced loss weighting to reduce bias, and patch-level cross-attention\nfor improved local reasoning. Vertex features are finally passed through a\nlightweight MLP with a softmax to assign semantic contact labels. We also\ntested a vision-language model (VLM) to integrate text features, but the\nsimpler architecture performed better and was used instead. On the DAMON\nbenchmark, DecoDINO (i) raises the binary-contact F1 score by 7$\\%$, (ii)\nhalves the geodesic error, and (iii) augments predictions with object-level\nsemantic labels. Ablation studies show that LoRA fine-tuning and the dual\nencoders are key to these improvements. DecoDINO outperformed the challenge\nbaseline in both tasks of the DAMON Challenge. Our code is available at\nhttps://github.com/DavidePasero/deco/tree/main.",
            "headline_zh": "提出DecoDINO以改进人-场景接触预测，提升精度与语义分类能力",
            "intro_zh": [
                "核心问题：现有方法在软表面、遮挡等场景下接触预测精度不足，且缺乏语义信息",
                "方法要点：基于DECO框架，采用双DINOv2编码器与补丁级交叉注意力，优化局部推理",
                "实验或效果：在DAMON基准上，F1分数提升7%，测地误差减半，并添加对象级语义标签"
            ],
            "tags_zh": [
                "3D人-场景接触预测",
                "语义分类",
                "DINOv2编码器",
                "补丁级交叉注意力",
                "LoRA微调",
                "DAMON基准"
            ],
            "_index": 63
        },
        {
            "title": "Evaluation of Vision-LLMs in Surveillance Video",
            "authors": [
                "Pascal Benschop",
                "Cristian Meo",
                "Justin Dauwels",
                "Jelte P. Mense"
            ],
            "arxiv_id": "2510.23190v1",
            "summary": "The widespread use of cameras in our society has created an overwhelming\namount of video data, far exceeding the capacity for human monitoring. This\npresents a critical challenge for public safety and security, as the timely\ndetection of anomalous or criminal events is crucial for effective response and\nprevention. The ability for an embodied agent to recognize unexpected events is\nfundamentally tied to its capacity for spatial reasoning. This paper\ninvestigates the spatial reasoning of vision-language models (VLMs) by framing\nanomalous action recognition as a zero-shot, language-grounded task, addressing\nthe embodied perception challenge of interpreting dynamic 3D scenes from sparse\n2D video. Specifically, we investigate whether small, pre-trained vision--LLMs\ncan act as spatially-grounded, zero-shot anomaly detectors by converting video\ninto text descriptions and scoring labels via textual entailment. We evaluate\nfour open models on UCF-Crime and RWF-2000 under prompting and\nprivacy-preserving conditions. Few-shot exemplars can improve accuracy for some\nmodels, but may increase false positives, and privacy filters -- especially\nfull-body GAN transforms -- introduce inconsistencies that degrade accuracy.\nThese results chart where current vision--LLMs succeed (simple, spatially\nsalient events) and where they falter (noisy spatial cues, identity\nobfuscation). Looking forward, we outline concrete paths to strengthen spatial\ngrounding without task-specific training: structure-aware prompts, lightweight\nspatial memory across clips, scene-graph or 3D-pose priors during description,\nand privacy methods that preserve action-relevant geometry. This positions\nzero-shot, language-grounded pipelines as adaptable building blocks for\nembodied, real-world video understanding. Our implementation for evaluating\nVLMs is publicly available at:\nhttps://github.com/pascalbenschopTU/VLLM_AnomalyRecognition",
            "headline_zh": "评估视觉-语言模型在监控视频中的零样本异常检测能力",
            "intro_zh": [
                "核心问题：监控视频数据量庞大，需自动检测异常事件以提升公共安全。",
                "方法要点：将视频转为文本描述，通过文本蕴含评分实现零样本异常识别。",
                "实验效果：在UCF-Crime和RWF-2000数据集上测试，隐私过滤可能降低准确性。"
            ],
            "tags_zh": [
                "视觉-语言模型",
                "零样本学习",
                "异常检测",
                "监控视频",
                "空间推理",
                "隐私保护"
            ],
            "_index": 64
        },
        {
            "title": "Finding 3D Scene Analogies with Multimodal Foundation Models",
            "authors": [
                "Junho Kim",
                "Young Min Kim"
            ],
            "arxiv_id": "2510.23184v1",
            "summary": "Connecting current observations with prior experiences helps robots adapt and\nplan in new, unseen 3D environments. Recently, 3D scene analogies have been\nproposed to connect two 3D scenes, which are smooth maps that align scene\nregions with common spatial relationships. These maps enable detailed transfer\nof trajectories or waypoints, potentially supporting demonstration transfer for\nimitation learning or task plan transfer across scenes. However, existing\nmethods for the task require additional training and fixed object vocabularies.\nIn this work, we propose to use multimodal foundation models for finding 3D\nscene analogies in a zero-shot, open-vocabulary setting. Central to our\napproach is a hybrid neural representation of scenes that consists of a sparse\ngraph based on vision-language model features and a feature field derived from\n3D shape foundation models. 3D scene analogies are then found in a\ncoarse-to-fine manner, by first aligning the graph and refining the\ncorrespondence with feature fields. Our method can establish accurate\ncorrespondences between complex scenes, and we showcase applications in\ntrajectory and waypoint transfer.",
            "headline_zh": "提出基于多模态基础模型的3D场景类比方法，实现零样本开放词汇场景对应",
            "intro_zh": [
                "核心问题：现有3D场景类比方法需额外训练和固定对象词汇，限制泛化能力。",
                "方法要点：使用混合神经表示，结合视觉语言模型图和3D形状模型特征场，进行粗到精对齐。",
                "实验或效果：能建立复杂场景间准确对应，应用于轨迹和路径点转移。"
            ],
            "tags_zh": [
                "3D场景类比",
                "多模态基础模型",
                "零样本学习",
                "开放词汇",
                "轨迹转移",
                "混合神经表示"
            ],
            "_index": 65
        },
        {
            "title": "TARC: Time-Adaptive Robotic Control",
            "authors": [
                "Arnav Sukhija",
                "Lenart Treven",
                "Jin Cheng",
                "Florian Dörfler",
                "Stelian Coros",
                "Andreas Krause"
            ],
            "arxiv_id": "2510.23176v1",
            "summary": "Fixed-frequency control in robotics imposes a trade-off between the\nefficiency of low-frequency control and the robustness of high-frequency\ncontrol, a limitation not seen in adaptable biological systems. We address this\nwith a reinforcement learning approach in which policies jointly select control\nactions and their application durations, enabling robots to autonomously\nmodulate their control frequency in response to situational demands. We\nvalidate our method with zero-shot sim-to-real experiments on two distinct\nhardware platforms: a high-speed RC car and a quadrupedal robot. Our method\nmatches or outperforms fixed-frequency baselines in terms of rewards while\nsignificantly reducing the control frequency and exhibiting adaptive frequency\ncontrol under real-world conditions.",
            "headline_zh": "提出时间自适应机器人控制方法，通过强化学习联合选择控制动作与持续时间，解决固定频率控制的效率与鲁棒性权衡问题。",
            "intro_zh": [
                "固定频率控制导致效率与鲁棒性权衡，限制机器人适应动态环境。",
                "采用强化学习策略，联合优化控制动作及其应用时长，实现频率自适应。",
                "零样本仿真到真实实验验证，在高速RC车和四足机器人上优于固定频率基线。"
            ],
            "tags_zh": [
                "机器人控制",
                "强化学习",
                "自适应频率",
                "仿真到真实迁移",
                "零样本学习"
            ],
            "_index": 66
        },
        {
            "title": "AG-Fusion: adaptive gated multimodal fusion for 3d object detection in complex scenes",
            "authors": [
                "Sixian Liu",
                "Chen Xu",
                "Qiang Wang",
                "Donghai Shi",
                "Yiwen Li"
            ],
            "arxiv_id": "2510.23151v1",
            "summary": "Multimodal camera-LiDAR fusion technology has found extensive application in\n3D object detection, demonstrating encouraging performance. However, existing\nmethods exhibit significant performance degradation in challenging scenarios\ncharacterized by sensor degradation or environmental disturbances. We propose a\nnovel Adaptive Gated Fusion (AG-Fusion) approach that selectively integrates\ncross-modal knowledge by identifying reliable patterns for robust detection in\ncomplex scenes. Specifically, we first project features from each modality into\na unified BEV space and enhance them using a window-based attention mechanism.\nSubsequently, an adaptive gated fusion module based on cross-modal attention is\ndesigned to integrate these features into reliable BEV representations robust\nto challenging environments. Furthermore, we construct a new dataset named\nExcavator3D (E3D) focusing on challenging excavator operation scenarios to\nbenchmark performance in complex conditions. Our method not only achieves\ncompetitive performance on the standard KITTI dataset with 93.92% accuracy, but\nalso significantly outperforms the baseline by 24.88% on the challenging E3D\ndataset, demonstrating superior robustness to unreliable modal information in\ncomplex industrial scenes.",
            "headline_zh": "提出自适应门控融合方法以提升复杂场景中3D目标检测的鲁棒性",
            "intro_zh": [
                "核心问题：现有多模态融合方法在传感器退化或环境干扰场景中性能显著下降",
                "方法要点：在BEV空间使用窗口注意力和跨模态注意力门控融合特征",
                "实验或效果：在KITTI数据集达93.92%精度，E3D数据集比基线提升24.88%"
            ],
            "tags_zh": [
                "3D目标检测",
                "多模态融合",
                "自适应门控",
                "BEV表示",
                "鲁棒性",
                "复杂场景"
            ],
            "_index": 67
        },
        {
            "title": "Implicit Modeling for Transferability Estimation of Vision Foundation Models",
            "authors": [
                "Yaoyan Zheng",
                "Huiqun Wang",
                "Nan Zhou",
                "Di Huang"
            ],
            "arxiv_id": "2510.23145v1",
            "summary": "Transferability estimation identifies the best pre-trained models for\ndownstream tasks without incurring the high computational cost of full\nfine-tuning. This capability facilitates deployment and advances the\npre-training and fine-tuning paradigm. However, existing methods often struggle\nto accurately assess transferability for emerging pre-trained models with\ndiverse architectures, training strategies, and task alignments. In this work,\nwe propose Implicit Transferability Modeling (ITM), a novel framework that\nimplicitly models each model's intrinsic transferability, coupled with a\nDivide-and-Conquer Variational Approximation (DVA) strategy to efficiently\napproximate embedding space evolution. This design enables generalization\nacross a broader range of models and downstream tasks. Extensive experiments on\na comprehensive benchmark--spanning extensive training regimes and a wider\nvariety of model types--demonstrate that ITM consistently outperforms existing\nmethods in terms of stability, effectiveness, and efficiency.",
            "headline_zh": "提出隐式建模框架以提升视觉基础模型迁移性估计的泛化能力",
            "intro_zh": [
                "现有方法难以准确评估新兴预训练模型的迁移性",
                "引入隐式建模和分治变分近似策略高效近似嵌入空间演化",
                "实验表明在稳定性和效率上优于现有方法"
            ],
            "tags_zh": [
                "迁移性估计",
                "隐式建模",
                "变分近似",
                "视觉基础模型",
                "下游任务"
            ],
            "_index": 68
        },
        {
            "title": "DQ3D: Depth-guided Query for Transformer-Based 3D Object Detection in Traffic Scenarios",
            "authors": [
                "Ziyu Wang",
                "Wenhao Li",
                "Ji Wu"
            ],
            "arxiv_id": "2510.23144v1",
            "summary": "3D object detection from multi-view images in traffic scenarios has garnered\nsignificant attention in recent years. Many existing approaches rely on object\nqueries that are generated from 3D reference points to localize objects.\nHowever, a limitation of these methods is that some reference points are often\nfar from the target object, which can lead to false positive detections. In\nthis paper, we propose a depth-guided query generator for 3D object detection\n(DQ3D) that leverages depth information and 2D detections to ensure that\nreference points are sampled from the surface or interior of the object.\nFurthermore, to address partially occluded objects in current frame, we\nintroduce a hybrid attention mechanism that fuses historical detection results\nwith depth-guided queries, thereby forming hybrid queries. Evaluation on the\nnuScenes dataset demonstrates that our method outperforms the baseline by 6.3\\%\nin terms of mean Average Precision (mAP) and 4.3\\% in the NuScenes Detection\nScore (NDS).",
            "headline_zh": "提出深度引导查询生成器以解决交通场景中3D物体检测的误检问题",
            "intro_zh": [
                "核心问题：现有方法中3D参考点可能远离目标物体，导致误检。",
                "方法要点：利用深度信息和2D检测，从物体表面或内部采样参考点。",
                "实验或效果：在nuScenes数据集上，mAP和NDS分别提升6.3%和4.3%。"
            ],
            "tags_zh": [
                "3D物体检测",
                "深度引导查询",
                "交通场景",
                "混合注意力机制",
                "nuScenes数据集"
            ],
            "_index": 69
        },
        {
            "title": "Fast Voxel-Wise Kinetic Modeling in Dynamic PET using a Physics-Informed CycleGAN",
            "authors": [
                "Christian Salomonsen",
                "Samuel Kuttner",
                "Michael Kampffmeyer",
                "Robert Jenssen",
                "Kristoffer Wickstrøm",
                "Jong Chul Ye",
                "Elisabeth Wetzer"
            ],
            "arxiv_id": "2510.23140v1",
            "summary": "Tracer kinetic modeling serves a vital role in diagnosis, treatment planning,\ntracer development and oncology, but burdens practitioners with complex and\ninvasive arterial input function estimation (AIF). We adopt a physics-informed\nCycleGAN showing promise in DCE-MRI quantification to dynamic PET\nquantification. Our experiments demonstrate sound AIF predictions and parameter\nmaps closely resembling the reference.",
            "headline_zh": "提出物理信息循环GAN以解决动态PET中动脉输入函数估计的复杂性",
            "intro_zh": [
                "核心问题：动态PET中动脉输入函数估计复杂且具侵入性，阻碍动力学建模应用。",
                "方法要点：采用物理信息循环GAN，从动态PET数据预测动脉输入函数和参数图。",
                "实验或效果：实验显示预测的动脉输入函数和参数图与参考结果高度相似。"
            ],
            "tags_zh": [
                "动态PET",
                "动力学建模",
                "动脉输入函数估计",
                "物理信息循环GAN",
                "参数图预测"
            ],
            "_index": 70
        },
        {
            "title": "Note on the Construction of Structure Tensor",
            "authors": [
                "Josef Bigun",
                "Fernado Alonso-Fernandez"
            ],
            "arxiv_id": "2510.23137v1",
            "summary": "This note presents a theoretical discussion of two structure tensor\nconstructions: one proposed by Bigun and Granlund 1987, and the other by\nGranlund and Knutsson 1995. At first glance, these approaches may appear quite\ndifferent--the former is implemented by averaging outer products of gradient\nfilter responses, while the latter constructs the tensor from weighted outer\nproducts of tune-in frequency vectors of quadrature filters. We argue that when\nboth constructions are viewed through the common lens of Total Least Squares\n(TLS) line fitting to the power spectrum, they can be reconciled to a large\nextent, and additional benefits emerge. From this perspective, the correction\nterm introduced in Granlund and Knutsson 1995 becomes unnecessary. Omitting it\nensures that the resulting tensor remains positive semi-definite, thereby\nsimplifying the interpretation of its eigenvalues. Furthermore, this\ninterpretation allows fitting more than a single 0rientation to the input by\nreinterpreting quadrature filter responses without relying on a structure\ntensor. It also removes the constraint that responses must originate strictly\nfrom quadrature filters, allowing the use of alternative filter types and\nnon-angular tessellations. These alternatives include Gabor filters--which,\nalthough not strictly quadrature, are still suitable for structure tensor\nconstruction--even when they tessellate the spectrum in a Cartesian fashion,\nprovided they are sufficiently concentrated.",
            "headline_zh": "统一结构张量构造方法，基于TLS线拟合提升灵活性与简化性",
            "intro_zh": [
                "核心问题：比较两种结构张量构造方法，探讨其差异与统一性。",
                "方法要点：通过TLS线拟合视角统一方法，移除校正项确保正半定性。",
                "实验或效果：简化特征值解释，支持多种滤波器与非角镶嵌。"
            ],
            "tags_zh": [
                "结构张量",
                "总最小二乘",
                "滤波器设计",
                "图像处理",
                "特征提取"
            ],
            "_index": 71
        },
        {
            "title": "Combining High Level Scheduling and Low Level Control to Manage Fleets of Mobile Robots",
            "authors": [
                "Sabino Francesco Roselli",
                "Ze Zhang",
                "Knut Åkesson"
            ],
            "arxiv_id": "2510.23129v1",
            "summary": "The deployment of mobile robots for material handling in industrial\nenvironments requires scalable coordination of large fleets in dynamic\nsettings. This paper presents a two-layer framework that combines high-level\nscheduling with low-level control. Tasks are assigned and scheduled using the\ncompositional algorithm ComSat, which generates time-parameterized routes for\neach robot. These schedules are then used by a distributed Model Predictive\nControl (MPC) system in real time to compute local reference trajectories,\naccounting for static and dynamic obstacles. The approach ensures safe,\ncollision-free operation, and supports rapid rescheduling in response to\ndisruptions such as robot failures or environmental changes. We evaluate the\nmethod in simulated 2D environments with varying road capacities and traffic\nconditions, demonstrating high task completion rates and robust behavior even\nunder congestion. The modular structure of the framework allows for\ncomputational tractability and flexibility, making it suitable for deployment\nin complex, real-world industrial scenarios.",
            "headline_zh": "提出两层框架结合高层调度与低层控制以管理工业移动机器人车队",
            "intro_zh": [
                "核心问题：工业环境中大规模移动机器人车队的可扩展协调与动态障碍处理。",
                "方法要点：使用ComSat算法调度任务，分布式MPC实时控制轨迹，确保安全无碰撞。",
                "实验效果：在模拟2D环境中评估，高任务完成率，拥堵下鲁棒性强。"
            ],
            "tags_zh": [
                "移动机器人调度",
                "模型预测控制",
                "工业自动化",
                "车队管理",
                "实时控制"
            ],
            "_index": 72
        },
        {
            "title": "DeepSalt: Bridging Laboratory and Satellite Spectra through Domain Adaptation and Knowledge Distillation for Large-Scale Soil Salinity Estimation",
            "authors": [
                "Rupasree Dey",
                "Abdul Matin",
                "Everett Lewark",
                "Tanjim Bin Faruk",
                "Andrei Bachinin",
                "Sam Leuthold",
                "M. Francesca Cotrufo",
                "Shrideep Pallickara",
                "Sangmi Lee Pallickara"
            ],
            "arxiv_id": "2510.23124v1",
            "summary": "Soil salinization poses a significant threat to both ecosystems and\nagriculture because it limits plants' ability to absorb water and, in doing so,\nreduces crop productivity. This phenomenon alters the soil's spectral\nproperties, creating a measurable relationship between salinity and light\nreflectance that enables remote monitoring. While laboratory spectroscopy\nprovides precise measurements, its reliance on in-situ sampling limits\nscalability to regional or global levels. Conversely, hyperspectral satellite\nimagery enables wide-area observation but lacks the fine-grained\ninterpretability of laboratory instruments. To bridge this gap, we introduce\nDeepSalt, a deep-learning-based spectral transfer framework that leverages\nknowledge distillation and a novel Spectral Adaptation Unit to transfer\nhigh-resolution spectral insights from laboratory-based spectroscopy to\nsatellite-based hyperspectral sensing. Our approach eliminates the need for\nextensive ground sampling while enabling accurate, large-scale salinity\nestimation, as demonstrated through comprehensive empirical benchmarks.\nDeepSalt achieves significant performance gains over methods without explicit\ndomain adaptation, underscoring the impact of the proposed Spectral Adaptation\nUnit and the knowledge distillation strategy. The model also effectively\ngeneralized to unseen geographic regions, explaining a substantial portion of\nthe salinity variance.",
            "headline_zh": "提出DeepSalt框架，通过领域适应和知识蒸馏解决实验室与卫星光谱差异，实现大规模土壤盐度估计。",
            "intro_zh": [
                "土壤盐化影响生态系统和农业，实验室光谱精确但难扩展，卫星图像覆盖广但精度低。",
                "使用知识蒸馏和光谱适应单元，将实验室高分辨率光谱知识迁移到卫星数据。",
                "实验显示性能优于无领域适应方法，能泛化到未知区域，解释盐度方差。"
            ],
            "tags_zh": [
                "土壤盐度估计",
                "领域适应",
                "知识蒸馏",
                "光谱迁移",
                "深度学习",
                "遥感监测"
            ],
            "_index": 73
        },
        {
            "title": "Reliable Robotic Task Execution in the Face of Anomalies",
            "authors": [
                "Bharath Santhanam",
                "Alex Mitrevski",
                "Santosh Thoduka",
                "Sebastian Houben",
                "Teena Hassan"
            ],
            "arxiv_id": "2510.23121v1",
            "summary": "Learned robot policies have consistently been shown to be versatile, but they\ntypically have no built-in mechanism for handling the complexity of open\nenvironments, making them prone to execution failures; this implies that\ndeploying policies without the ability to recognise and react to failures may\nlead to unreliable and unsafe robot behaviour. In this paper, we present a\nframework that couples a learned policy with a method to detect visual\nanomalies during policy deployment and to perform recovery behaviours when\nnecessary, thereby aiming to prevent failures. Specifically, we train an\nanomaly detection model using data collected during nominal executions of a\ntrained policy. This model is then integrated into the online policy execution\nprocess, so that deviations from the nominal execution can trigger a\nthree-level sequential recovery process that consists of (i) pausing the\nexecution temporarily, (ii) performing a local perturbation of the robot's\nstate, and (iii) resetting the robot to a safe state by sampling from a learned\nexecution success model. We verify our proposed method in two different\nscenarios: (i) a door handle reaching task with a Kinova Gen3 arm using a\npolicy trained in simulation and transferred to the real robot, and (ii) an\nobject placing task with a UFactory xArm 6 using a general-purpose policy\nmodel. Our results show that integrating policy execution with anomaly\ndetection and recovery increases the execution success rate in environments\nwith various anomalies, such as trajectory deviations and adversarial human\ninterventions.",
            "headline_zh": "提出结合异常检测与恢复的框架，提升学习策略在开放环境中的任务执行可靠性。",
            "intro_zh": [
                "学习策略在开放环境中易因异常导致执行失败，缺乏内置处理机制。",
                "训练异常检测模型，集成在线策略执行，触发三级顺序恢复过程。",
                "在门把手到达和物体放置任务中验证，提高异常环境下的执行成功率。"
            ],
            "tags_zh": [
                "机器人策略执行",
                "视觉异常检测",
                "恢复行为",
                "任务可靠性",
                "仿真到真实迁移"
            ],
            "_index": 74
        },
        {
            "title": "OmniDexGrasp: Generalizable Dexterous Grasping via Foundation Model and Force Feedback",
            "authors": [
                "Yi-Lin Wei",
                "Zhexi Luo",
                "Yuhao Lin",
                "Mu Lin",
                "Zhizhao Liang",
                "Shuoyu Chen",
                "Wei-Shi Zheng"
            ],
            "arxiv_id": "2510.23119v1",
            "summary": "Enabling robots to dexterously grasp and manipulate objects based on human\ncommands is a promising direction in robotics. However, existing approaches are\nchallenging to generalize across diverse objects or tasks due to the limited\nscale of semantic dexterous grasp datasets. Foundation models offer a new way\nto enhance generalization, yet directly leveraging them to generate feasible\nrobotic actions remains challenging due to the gap between abstract model\nknowledge and physical robot execution. To address these challenges, we propose\nOmniDexGrasp, a generalizable framework that achieves omni-capabilities in user\nprompting, dexterous embodiment, and grasping tasks by combining foundation\nmodels with the transfer and control strategies. OmniDexGrasp integrates three\nkey modules: (i) foundation models are used to enhance generalization by\ngenerating human grasp images supporting omni-capability of user prompt and\ntask; (ii) a human-image-to-robot-action transfer strategy converts human\ndemonstrations into executable robot actions, enabling omni dexterous\nembodiment; (iii) force-aware adaptive grasp strategy ensures robust and stable\ngrasp execution. Experiments in simulation and on real robots validate the\neffectiveness of OmniDexGrasp on diverse user prompts, grasp task and dexterous\nhands, and further results show its extensibility to dexterous manipulation\ntasks.",
            "headline_zh": "提出OmniDexGrasp框架，结合基础模型与力反馈实现通用灵巧抓取",
            "intro_zh": [
                "核心问题：现有方法因语义灵巧抓取数据集有限，难以泛化到多样物体或任务",
                "方法要点：集成基础模型生成人类抓取图像，并转换演示为机器人可执行动作",
                "实验或效果：仿真与真实机器人实验验证其在多样提示、任务和灵巧手上的有效性"
            ],
            "tags_zh": [
                "灵巧抓取",
                "基础模型",
                "力反馈控制",
                "机器人泛化",
                "动作转移"
            ],
            "_index": 75
        },
        {
            "title": "Task-Agnostic Fusion of Time Series and Imagery for Earth Observation",
            "authors": [
                "Gianfranco Basile",
                "Johannes Jakubik",
                "Benedikt Blumenstiel",
                "Thomas Brunschwiler",
                "Juan Bernabe Moreno"
            ],
            "arxiv_id": "2510.23118v1",
            "summary": "We propose a task-agnostic framework for multimodal fusion of time series and\nsingle timestamp images, enabling cross-modal generation and robust downstream\nperformance. Our approach explores deterministic and learned strategies for\ntime series quantization and then leverages a masked correlation learning\nobjective, aligning discrete image and time series tokens in a unified\nrepresentation space. Instantiated in the Earth observation domain, the\npretrained model generates consistent global temperature profiles from\nsatellite imagery and is validated through counterfactual experiments. Across\ndownstream tasks, our task-agnostic pretraining outperforms task-specific\nfusion by 6\\% in R$^2$ and 2\\% in RMSE on average, and exceeds baseline methods\nby 50\\% in R$^2$ and 12\\% in RMSE. Finally, we analyze gradient sensitivity\nacross modalities, providing insights into model robustness. Code, data, and\nweights will be released under a permissive license.",
            "headline_zh": "提出任务无关框架融合时间序列与图像，用于地球观测中的跨模态生成与下游任务。",
            "intro_zh": [
                "核心问题：如何任务无关地融合时间序列和单时间点图像，提升地球观测的鲁棒性。",
                "方法要点：采用时间序列量化与掩码相关学习，对齐图像和时间序列标记于统一表示空间。",
                "实验或效果：预训练模型在R²和RMSE指标上优于任务特定融合和基线方法，并验证了模型鲁棒性。"
            ],
            "tags_zh": [
                "多模态融合",
                "时间序列量化",
                "掩码相关学习",
                "地球观测",
                "任务无关预训练",
                "跨模态生成"
            ],
            "_index": 76
        },
        {
            "title": "Seeing Structural Failure Before it Happens: An Image-Based Physics-Informed Neural Network (PINN) for Spaghetti Bridge Load Prediction",
            "authors": [
                "Omer Jauhar Khan",
                "Sudais Khan",
                "Hafeez Anwar"
            ],
            "arxiv_id": "2510.23117v1",
            "summary": "Physics Informed Neural Networks (PINNs) are gaining attention for their\nability to embed physical laws into deep learning models, which is particularly\nuseful in structural engineering tasks with limited data. This paper aims to\nexplore the use of PINNs to predict the weight of small scale spaghetti\nbridges, a task relevant to understanding load limits and potential failure\nmodes in simplified structural models. Our proposed framework incorporates\nphysics-based constraints to the prediction model for improved performance. In\naddition to standard PINNs, we introduce a novel architecture named Physics\nInformed Kolmogorov Arnold Network (PIKAN), which blends universal function\napproximation theory with physical insights. The structural parameters provided\nas input to the model are collected either manually or through computer vision\nmethods. Our dataset includes 15 real bridges, augmented to 100 samples, and\nour best model achieves an $R^2$ score of 0.9603 and a mean absolute error\n(MAE) of 10.50 units. From applied perspective, we also provide a web based\ninterface for parameter entry and prediction. These results show that PINNs can\noffer reliable estimates of structural weight, even with limited data, and may\nhelp inform early stage failure analysis in lightweight bridge designs.\n  The complete data and code are available at\nhttps://github.com/OmerJauhar/PINNS-For-Spaghetti-Bridges.",
            "headline_zh": "提出物理信息神经网络以预测意大利面桥梁承重，辅助结构失效分析。",
            "intro_zh": [
                "核心问题：在数据有限下预测简化结构模型的承重和失效模式。",
                "方法要点：结合物理约束与深度学习，引入新型PIKAN架构。",
                "实验或效果：在15座桥梁数据集上，R²达0.9603，MAE为10.50单位。"
            ],
            "tags_zh": [
                "物理信息神经网络",
                "结构工程预测",
                "意大利面桥梁",
                "计算机视觉",
                "失效分析",
                "深度学习"
            ],
            "_index": 77
        },
        {
            "title": "Residual Diffusion Bridge Model for Image Restoration",
            "authors": [
                "Hebaixu Wang",
                "Jing Zhang",
                "Haoyang Chen",
                "Haonan Guo",
                "Di Wang",
                "Jiayi Ma",
                "Bo Du"
            ],
            "arxiv_id": "2510.23116v1",
            "summary": "Diffusion bridge models establish probabilistic paths between arbitrary\npaired distributions and exhibit great potential for universal image\nrestoration. Most existing methods merely treat them as simple variants of\nstochastic interpolants, lacking a unified analytical perspective. Besides,\nthey indiscriminately reconstruct images through global noise injection and\nremoval, inevitably distorting undegraded regions due to imperfect\nreconstruction. To address these challenges, we propose the Residual Diffusion\nBridge Model (RDBM). Specifically, we theoretically reformulate the stochastic\ndifferential equations of generalized diffusion bridge and derive the\nanalytical formulas of its forward and reverse processes. Crucially, we\nleverage the residuals from given distributions to modulate the noise injection\nand removal, enabling adaptive restoration of degraded regions while preserving\nintact others. Moreover, we unravel the fundamental mathematical essence of\nexisting bridge models, all of which are special cases of RDBM and empirically\ndemonstrate the optimality of our proposed models. Extensive experiments are\nconducted to demonstrate the state-of-the-art performance of our method both\nqualitatively and quantitatively across diverse image restoration tasks. Code\nis publicly available at https://github.com/MiliLab/RDBM.",
            "headline_zh": "提出残差扩散桥模型以解决图像修复中未退化区域失真问题",
            "intro_zh": [
                "核心问题：现有扩散桥模型缺乏统一分析视角，全局噪声注入导致未退化区域失真。",
                "方法要点：理论推导扩散桥随机微分方程，利用残差自适应调制噪声注入与去除。",
                "实验或效果：在多种图像修复任务中实现最优性能，代码已公开。"
            ],
            "tags_zh": [
                "图像修复",
                "扩散桥模型",
                "残差调制",
                "随机微分方程",
                "自适应恢复"
            ],
            "_index": 78
        },
        {
            "title": "An Automated Tape Laying System Employing a Uniaxial Force Control Device",
            "authors": [
                "Bernhard Rameder",
                "Hubert Gattringer",
                "Ronald Naderer",
                "Andreas Mueller"
            ],
            "arxiv_id": "2510.23109v1",
            "summary": "This paper deals with the design of a cost effective automated tape laying\nsystem (ATL system) with integrated uniaxial force control to ensure the\nnecessary compaction forces as well as with an accurate temperature control to\nguarantee the used tape being melted appropriate. It is crucial to control the\nsubstrate and the oncoming tape onto a specific temperature level to ensure an\noptimal consolidation between the different layers of the product. Therefore,\nit takes several process steps from the spooled tape on the coil until it is\nfinally tacked onto the desired mold. The different modules are divided into\nthe tape storage spool, a tape-guiding roller, a tape processing unit, a\nheating zone and the consolidation unit. Moreover, a special robot control\nconcept for testing the ATL system is presented. In contrast to many other\nsystems, with this approach, the tape laying device is spatially fixed and the\nshape is moved accordingly by the robot, which allows for handling of rather\ncompact and complex shapes. The functionality of the subsystems and the taping\nprocess itself was finally approved in experimental results using a carbon\nfiber reinforced HDPE tape.",
            "headline_zh": "提出集成单轴力控和温度控制的自动化铺带系统，用于复合材料制造。",
            "intro_zh": [
                "核心问题：复合材料铺带中需控制压实力和温度以确保层间固结。",
                "方法要点：系统包括储带、导带、处理、加热和固结单元，采用固定铺带装置。",
                "实验或效果：使用碳纤维增强HDPE带进行实验验证系统功能。"
            ],
            "tags_zh": [
                "自动化铺带系统",
                "单轴力控制",
                "温度控制",
                "复合材料制造",
                "机器人控制",
                "碳纤维增强带"
            ],
            "_index": 79
        },
        {
            "title": "Revisiting Multimodal Positional Encoding in Vision-Language Models",
            "authors": [
                "Jie Huang",
                "Xuejing Liu",
                "Sibo Song",
                "Ruibing Hou",
                "Hong Chang",
                "Junyang Lin",
                "Shuai Bai"
            ],
            "arxiv_id": "2510.23095v1",
            "summary": "Multimodal position encoding is essential for vision-language models, yet\nthere has been little systematic investigation into multimodal position\nencoding. We conduct a comprehensive analysis of multimodal Rotary Positional\nEmbedding (RoPE) by examining its two core components: position design and\nfrequency allocation. Through extensive experiments, we identify three key\nguidelines: positional coherence, full frequency utilization, and preservation\nof textual priors-ensuring unambiguous layout, rich representation, and\nfaithful transfer from the pre-trained LLM. Based on these insights, we propose\nMulti-Head RoPE (MHRoPE) and MRoPE-Interleave (MRoPE-I), two simple and\nplug-and-play variants that require no architectural changes. Our methods\nconsistently outperform existing approaches across diverse benchmarks, with\nsignificant improvements in both general and fine-grained multimodal\nunderstanding. Code will be avaliable at\nhttps://github.com/JJJYmmm/Multimodal-RoPEs.",
            "headline_zh": "提出MHRoPE和MRoPE-I以改进视觉语言模型中的多模态位置编码",
            "intro_zh": [
                "核心问题：多模态位置编码缺乏系统研究，影响模型布局理解和表示能力",
                "方法要点：基于位置设计和频率分配分析，提出无需架构更改的即插即用变体",
                "实验或效果：在多个基准测试中显著提升通用和细粒度多模态理解性能"
            ],
            "tags_zh": [
                "多模态位置编码",
                "旋转位置嵌入",
                "视觉语言模型",
                "即插即用方法",
                "多模态理解"
            ],
            "_index": 80
        },
        {
            "title": "EndoWave: Rational-Wavelet 4D Gaussian Splatting for Endoscopic Reconstruction",
            "authors": [
                "Taoyu Wu",
                "Yiyi Miao",
                "Jiaxin Guo",
                "Ziyan Chen",
                "Sihang Zhao",
                "Zhuoxiao Li",
                "Zhe Tang",
                "Baoru Huang",
                "Limin Yu"
            ],
            "arxiv_id": "2510.23087v1",
            "summary": "In robot-assisted minimally invasive surgery, accurate 3D reconstruction from\nendoscopic video is vital for downstream tasks and improved outcomes. However,\nendoscopic scenarios present unique challenges, including photometric\ninconsistencies, non-rigid tissue motion, and view-dependent highlights. Most\n3DGS-based methods that rely solely on appearance constraints for optimizing\n3DGS are often insufficient in this context, as these dynamic visual artifacts\ncan mislead the optimization process and lead to inaccurate reconstructions. To\naddress these limitations, we present EndoWave, a unified spatio-temporal\nGaussian Splatting framework by incorporating an optical flow-based geometric\nconstraint and a multi-resolution rational wavelet supervision. First, we adopt\na unified spatio-temporal Gaussian representation that directly optimizes\nprimitives in a 4D domain. Second, we propose a geometric constraint derived\nfrom optical flow to enhance temporal coherence and effectively constrain the\n3D structure of the scene. Third, we propose a multi-resolution rational\northogonal wavelet as a constraint, which can effectively separate the details\nof the endoscope and enhance the rendering performance. Extensive evaluations\non two real surgical datasets, EndoNeRF and StereoMIS, demonstrate that our\nmethod EndoWave achieves state-of-the-art reconstruction quality and visual\naccuracy compared to the baseline method.",
            "headline_zh": "提出EndoWave框架，结合光流与多分辨率小波监督，优化内窥镜4D重建。",
            "intro_zh": [
                "内窥镜场景存在光度不一致、非刚性运动和视点高光，误导3DGS优化。",
                "采用4D高斯表示，引入光流几何约束和多分辨率有理小波监督。",
                "在EndoNeRF和StereoMIS数据集上，实现SOTA重建质量和视觉精度。"
            ],
            "tags_zh": [
                "内窥镜重建",
                "4D高斯溅射",
                "光流约束",
                "多分辨率小波",
                "时空一致性"
            ],
            "_index": 81
        },
        {
            "title": "Breaking the Circle: An Autonomous Control-Switching Strategy for Stable Orographic Soaring in MAVs",
            "authors": [
                "Sunyou Hwang",
                "Christophe De Wagter",
                "Bart Remes",
                "Guido de Croon"
            ],
            "arxiv_id": "2510.23084v1",
            "summary": "Orographic soaring can significantly extend the endurance of micro aerial\nvehicles (MAVs), but circling behavior, arising from control conflicts between\nthe longitudinal and vertical axes, increases energy consumption and the risk\nof divergence. We propose a control switching method, named SAOS: Switched\nControl for Autonomous Orographic Soaring, which mitigates circling behavior by\nselectively controlling either the horizontal or vertical axis, effectively\ntransforming the system from underactuated to fully actuated during soaring.\nAdditionally, the angle of attack is incorporated into the INDI controller to\nimprove force estimation. Simulations with randomized initial positions and\nwind tunnel experiments on two MAVs demonstrate that the SAOS improves position\nconvergence, reduces throttle usage, and mitigates roll oscillations caused by\npitch-roll coupling. These improvements enhance energy efficiency and flight\nstability in constrained soaring environments.",
            "headline_zh": "提出SAOS控制切换策略以解决MAV地形翱翔中的盘旋行为问题",
            "intro_zh": [
                "核心问题：MAV地形翱翔中纵向与垂直轴控制冲突导致盘旋，增加能耗与发散风险。",
                "方法要点：通过选择性控制水平或垂直轴，将系统从欠驱动转为全驱动翱翔。",
                "实验或效果：仿真与风洞实验显示SAOS改善位置收敛、减少油门使用并抑制滚转振荡。"
            ],
            "tags_zh": [
                "微型飞行器",
                "地形翱翔",
                "控制切换",
                "INDI控制器",
                "能量效率",
                "飞行稳定性"
            ],
            "_index": 82
        },
        {
            "title": "Strategies for Robust Deep Learning Based Deformable Registration",
            "authors": [
                "Joel Honkamaa",
                "Pekka Marttinen"
            ],
            "arxiv_id": "2510.23079v1",
            "summary": "Deep learning based deformable registration methods have become popular in\nrecent years. However, their ability to generalize beyond training data\ndistribution can be poor, significantly hindering their usability. LUMIR brain\nregistration challenge for Learn2Reg 2025 aims to advance the field by\nevaluating the performance of the registration on contrasts and modalities\ndifferent from those included in the training set. Here we describe our\nsubmission to the challenge, which proposes a very simple idea for\nsignificantly improving robustness by transforming the images into MIND feature\nspace before feeding them into the model. In addition, a special ensembling\nstrategy is proposed that shows a small but consistent improvement.",
            "headline_zh": "提出MIND特征空间变换和集成策略以提升深度学习形变配准的鲁棒性",
            "intro_zh": [
                "核心问题：深度学习形变配准方法泛化能力差，难以处理训练数据分布外的图像对比度和模态",
                "方法要点：将图像转换到MIND特征空间后输入模型，并采用特殊集成策略",
                "实验或效果：在LUMIR脑配准挑战中验证，显著提升鲁棒性，集成策略带来小幅稳定改进"
            ],
            "tags_zh": [
                "深度学习形变配准",
                "鲁棒性提升",
                "MIND特征空间",
                "图像配准挑战",
                "集成策略"
            ],
            "_index": 83
        },
        {
            "title": "Awakening Facial Emotional Expressions in Human-Robot",
            "authors": [
                "Yongtong Zhu",
                "Lei Li",
                "Iggy Qian",
                "WenBin Zhou",
                "Ye Yuan",
                "Qingdu Li",
                "Na Liu",
                "Jianwei Zhang"
            ],
            "arxiv_id": "2510.23059v1",
            "summary": "The facial expression generation capability of humanoid social robots is\ncritical for achieving natural and human-like interactions, playing a vital\nrole in enhancing the fluidity of human-robot interactions and the accuracy of\nemotional expression. Currently, facial expression generation in humanoid\nsocial robots still relies on pre-programmed behavioral patterns, which are\nmanually coded at high human and time costs. To enable humanoid robots to\nautonomously acquire generalized expressive capabilities, they need to develop\nthe ability to learn human-like expressions through self-training. To address\nthis challenge, we have designed a highly biomimetic robotic face with\nphysical-electronic animated facial units and developed an end-to-end learning\nframework based on KAN (Kolmogorov-Arnold Network) and attention mechanisms.\nUnlike previous humanoid social robots, we have also meticulously designed an\nautomated data collection system based on expert strategies of facial motion\nprimitives to construct the dataset. Notably, to the best of our knowledge,\nthis is the first open-source facial dataset for humanoid social robots.\nComprehensive evaluations indicate that our approach achieves accurate and\ndiverse facial mimicry across different test subjects.",
            "headline_zh": "提出基于KAN和注意力机制的端到端学习框架，以解决人形社交机器人面部表情生成的自主性问题。",
            "intro_zh": [
                "核心问题：人形社交机器人依赖预编程表情，成本高且缺乏自主性。",
                "方法要点：设计仿生机器人脸，结合KAN和注意力机制实现端到端学习。",
                "实验或效果：评估显示方法能准确、多样地模仿不同测试对象的面部表情。"
            ],
            "tags_zh": [
                "人形社交机器人",
                "面部表情生成",
                "KAN网络",
                "注意力机制",
                "端到端学习",
                "仿生机器人脸"
            ],
            "_index": 84
        },
        {
            "title": "Seq-DeepIPC: Sequential Sensing for End-to-End Control in Legged Robot Navigation",
            "authors": [
                "Oskar Natan",
                "Jun Miura"
            ],
            "arxiv_id": "2510.23057v1",
            "summary": "We present Seq-DeepIPC, a sequential end-to-end perception-to-control model\nfor legged robot navigation in realworld environments. Seq-DeepIPC advances\nintelligent sensing for autonomous legged navigation by tightly integrating\nmulti-modal perception (RGB-D + GNSS) with temporal fusion and control. The\nmodel jointly predicts semantic segmentation and depth estimation, giving\nricher spatial features for planning and control. For efficient deployment on\nedge devices, we use EfficientNet-B0 as the encoder, reducing computation while\nmaintaining accuracy. Heading estimation is simplified by removing the noisy\nIMU and instead computing the bearing angle directly from consecutive GNSS\npositions. We collected a larger and more diverse dataset that includes both\nroad and grass terrains, and validated Seq-DeepIPC on a robot dog. Comparative\nand ablation studies show that sequential inputs improve perception and control\nin our models, while other baselines do not benefit. Seq-DeepIPC achieves\ncompetitive or better results with reasonable model size; although GNSS-only\nheading is less reliable near tall buildings, it is robust in open areas.\nOverall, Seq-DeepIPC extends end-to-end navigation beyond wheeled robots to\nmore versatile and temporally-aware systems. To support future research, we\nwill release the codes to our GitHub repository at\nhttps://github.com/oskarnatan/Seq-DeepIPC.",
            "headline_zh": "提出Seq-DeepIPC以实现腿式机器人在真实环境中的端到端导航控制",
            "intro_zh": [
                "核心问题：腿式机器人导航中感知与控制的高效集成与实时部署。",
                "方法要点：使用多模态感知与序列融合，联合预测语义分割和深度估计。",
                "实验或效果：在机器人狗上验证，序列输入提升性能，模型尺寸合理。"
            ],
            "tags_zh": [
                "腿式机器人导航",
                "端到端控制",
                "多模态感知",
                "序列融合",
                "语义分割",
                "深度估计"
            ],
            "_index": 85
        },
        {
            "title": "HieraMamba: Video Temporal Grounding via Hierarchical Anchor-Mamba Pooling",
            "authors": [
                "Joungbin An",
                "Kristen Grauman"
            ],
            "arxiv_id": "2510.23043v1",
            "summary": "Video temporal grounding, the task of localizing the start and end times of a\nnatural language query in untrimmed video, requires capturing both global\ncontext and fine-grained temporal detail. This challenge is particularly\npronounced in long videos, where existing methods often compromise temporal\nfidelity by over-downsampling or relying on fixed windows. We present\nHieraMamba, a hierarchical architecture that preserves temporal structure and\nsemantic richness across scales. At its core are Anchor-MambaPooling (AMP)\nblocks, which utilize Mamba's selective scanning to produce compact anchor\ntokens that summarize video content at multiple granularities. Two\ncomplementary objectives, anchor-conditioned and segment-pooled contrastive\nlosses, encourage anchors to retain local detail while remaining globally\ndiscriminative. HieraMamba sets a new state-of-the-art on Ego4D-NLQ, MAD, and\nTACoS, demonstrating precise, temporally faithful localization in long,\nuntrimmed videos.",
            "headline_zh": "提出HieraMamba架构以解决长视频中语言查询的时序定位问题",
            "intro_zh": [
                "核心问题：长视频时序定位需平衡全局上下文与细粒度时序细节，现有方法易因过度下采样或固定窗口而损失精度。",
                "方法要点：采用分层Anchor-MambaPooling块，通过选择性扫描生成多粒度锚点令牌，结合对比损失保留局部细节与全局区分性。",
                "实验或效果：在Ego4D-NLQ、MAD和TACoS数据集上达到新SOTA，实现长未修剪视频的精确时序定位。"
            ],
            "tags_zh": [
                "视频时序定位",
                "分层架构",
                "锚点令牌",
                "对比学习",
                "长视频理解",
                "Mamba模型"
            ],
            "_index": 86
        },
        {
            "title": "Nested AutoRegressive Models",
            "authors": [
                "Hongyu Wu",
                "Xuhui Fan",
                "Zhangkai Wu",
                "Longbing Cao"
            ],
            "arxiv_id": "2510.23028v1",
            "summary": "AutoRegressive (AR) models have demonstrated competitive performance in image\ngeneration, achieving results comparable to those of diffusion models. However,\ntheir token-by-token image generation mechanism remains computationally\nintensive and existing solutions such as VAR often lead to limited sample\ndiversity. In this work, we propose a Nested AutoRegressive~(NestAR) model,\nwhich proposes nested AutoRegressive architectures in generating images. NestAR\ndesigns multi-scale modules in a hierarchical order. These different scaled\nmodules are constructed in an AR architecture, where one larger-scale module is\nconditioned on outputs from its previous smaller-scale module. Within each\nmodule, NestAR uses another AR structure to generate ``patches'' of tokens. The\nproposed nested AR architecture reduces the overall complexity from\n$\\mathcal{O}(n)$ to $\\mathcal{O}(\\log n)$ in generating $n$ image tokens, as\nwell as increases image diversities. NestAR further incorporates flow matching\nloss to use continuous tokens, and develops objectives to coordinate these\nmulti-scale modules in model training. NestAR achieves competitive image\ngeneration performance while significantly lowering computational cost.",
            "headline_zh": "提出嵌套自回归模型以降低图像生成计算成本并提升多样性",
            "intro_zh": [
                "自回归模型图像生成计算密集且样本多样性受限",
                "采用多尺度模块嵌套自回归架构，复杂度降至O(log n)",
                "结合流匹配损失，实现高效且多样化的图像生成"
            ],
            "tags_zh": [
                "图像生成",
                "自回归模型",
                "多尺度架构",
                "计算效率",
                "样本多样性"
            ],
            "_index": 87
        },
        {
            "title": "Mixed Density Diffuser: Efficient Planning with Non-uniform Temporal Resolution",
            "authors": [
                "Crimson Stambaugh",
                "Rajesh P. N. Rao"
            ],
            "arxiv_id": "2510.23026v1",
            "summary": "Recent studies demonstrate that diffusion planners benefit from sparse-step\nplanning over single-step planning. Training models to skip steps in their\ntrajectories helps capture long-term dependencies without additional or memory\ncomputational cost. However, predicting excessively sparse plans degrades\nperformance. We hypothesize this temporal density threshold is non-uniform\nacross a temporal horizon and that certain parts of a planned trajectory should\nbe more densely planned. We propose Mixed Density Diffuser (MDD), a diffusion\nplanner where the densities throughout the horizon are tunable hyperparameters.\nMDD achieves a new SOTA across the Maze2D, Franka Kitchen, and Antmaze D4RL\ntask domains.",
            "headline_zh": "提出混合密度扩散器以在规划任务中实现非均匀时间分辨率",
            "intro_zh": [
                "核心问题：扩散规划中均匀稀疏步长预测可能导致性能下降",
                "方法要点：引入可调超参数控制轨迹不同部分的时间密度",
                "实验或效果：在Maze2D等D4RL任务中达到新SOTA性能"
            ],
            "tags_zh": [
                "扩散规划",
                "非均匀时间分辨率",
                "轨迹优化",
                "强化学习",
                "D4RL基准"
            ],
            "_index": 88
        },
        {
            "title": "UniAIDet: A Unified and Universal Benchmark for AI-Generated Image Content Detection and Localization",
            "authors": [
                "Huixuan Zhang",
                "Xiaojun Wan"
            ],
            "arxiv_id": "2510.23023v1",
            "summary": "With the rapid proliferation of image generative models, the authenticity of\ndigital images has become a significant concern. While existing studies have\nproposed various methods for detecting AI-generated content, current benchmarks\nare limited in their coverage of diverse generative models and image\ncategories, often overlooking end-to-end image editing and artistic images. To\naddress these limitations, we introduce UniAIDet, a unified and comprehensive\nbenchmark that includes both photographic and artistic images. UniAIDet covers\na wide range of generative models, including text-to-image, image-to-image,\nimage inpainting, image editing, and deepfake models. Using UniAIDet, we\nconduct a comprehensive evaluation of various detection methods and answer\nthree key research questions regarding generalization capability and the\nrelation between detection and localization. Our benchmark and analysis provide\na robust foundation for future research.",
            "headline_zh": "提出UniAIDet基准以解决AI生成图像检测与定位的多样性不足问题",
            "intro_zh": [
                "核心问题：现有基准在生成模型和图像类别覆盖上不足，忽略端到端编辑和艺术图像",
                "方法要点：构建统一基准，涵盖文本到图像、图像到图像等多种模型和图像类型",
                "实验或效果：评估多种检测方法，回答泛化能力和检测与定位关系的研究问题"
            ],
            "tags_zh": [
                "AI生成图像检测",
                "基准构建",
                "图像定位",
                "生成模型多样性",
                "图像真实性评估"
            ],
            "_index": 89
        },
        {
            "title": "Planning Oriented Integrated Sensing and Communication",
            "authors": [
                "Xibin Jin",
                "Guoliang Li",
                "Shuai Wang",
                "Fan Liu",
                "Miaowen Wen",
                "Huseyin Arslan",
                "Derrick Wing Kwan Ng",
                "Chengzhong Xu"
            ],
            "arxiv_id": "2510.23021v1",
            "summary": "Integrated sensing and communication (ISAC) enables simultaneous\nlocalization, environment perception, and data exchange for connected\nautonomous vehicles. However, most existing ISAC designs prioritize sensing\naccuracy and communication throughput, treating all targets uniformly and\noverlooking the impact of critical obstacles on motion efficiency. To overcome\nthis limitation, we propose a planning-oriented ISAC (PISAC) framework that\nreduces the sensing uncertainty of planning-bottleneck obstacles and expands\nthe safe navigable path for the ego-vehicle, thereby bridging the gap between\nphysical-layer optimization and motion-level planning. The core of PISAC lies\nin deriving a closed-form safety bound that explicitly links ISAC transmit\npower to sensing uncertainty, based on the Cram\\'er-Rao Bound and occupancy\ninflation principles. Using this model, we formulate a bilevel power allocation\nand motion planning (PAMP) problem, where the inner layer optimizes the ISAC\nbeam power distribution and the outer layer computes a collision-free\ntrajectory under uncertainty-aware safety constraints. Comprehensive\nsimulations in high-fidelity urban driving environments demonstrate that PISAC\nachieves up to 40% higher success rates and over 5% shorter traversal times\nthan existing ISAC-based and communication-oriented benchmarks, validating its\neffectiveness in enhancing both safety and efficiency.",
            "headline_zh": "提出规划导向集成感知通信框架，以提升自动驾驶车辆的安全与效率",
            "intro_zh": [
                "现有ISAC设计忽视关键障碍对运动效率的影响，导致感知与规划脱节",
                "基于Cramér-Rao Bound和占用膨胀原理，推导安全边界并构建双层优化问题",
                "仿真显示PISAC在成功率和通行时间上优于基准方法，验证其有效性"
            ],
            "tags_zh": [
                "集成感知与通信",
                "自动驾驶规划",
                "功率分配优化",
                "安全边界建模",
                "双层优化问题"
            ],
            "_index": 90
        },
        {
            "title": "M$^{3}$T2IBench: A Large-Scale Multi-Category, Multi-Instance, Multi-Relation Text-to-Image Benchmark",
            "authors": [
                "Huixuan Zhang",
                "Xiaojun Wan"
            ],
            "arxiv_id": "2510.23020v1",
            "summary": "Text-to-image models are known to struggle with generating images that\nperfectly align with textual prompts. Several previous studies have focused on\nevaluating image-text alignment in text-to-image generation. However, these\nevaluations either address overly simple scenarios, especially overlooking the\ndifficulty of prompts with multiple different instances belonging to the same\ncategory, or they introduce metrics that do not correlate well with human\nevaluation. In this study, we introduce M$^3$T2IBench, a large-scale,\nmulti-category, multi-instance, multi-relation along with an\nobject-detection-based evaluation metric, $AlignScore$, which aligns closely\nwith human evaluation. Our findings reveal that current open-source\ntext-to-image models perform poorly on this challenging benchmark.\nAdditionally, we propose the Revise-Then-Enforce approach to enhance image-text\nalignment. This training-free post-editing method demonstrates improvements in\nimage-text alignment across a broad range of diffusion models. \\footnote{Our\ncode and data has been released in supplementary material and will be made\npublicly available after the paper is accepted.}",
            "headline_zh": "提出M³T2IBench基准和AlignScore指标以评估多实例文本-图像对齐问题。",
            "intro_zh": [
                "核心问题：现有文本-图像模型在多实例、多类别提示下图像-文本对齐不佳。",
                "方法要点：引入大规模多类别、多实例、多关系基准及基于目标检测的AlignScore指标。",
                "实验或效果：提出Revise-Then-Enforce方法，在扩散模型中提升对齐效果。"
            ],
            "tags_zh": [
                "文本-图像生成",
                "多实例评估",
                "图像-文本对齐",
                "基准数据集",
                "目标检测指标"
            ],
            "_index": 91
        },
        {
            "title": "ManiDP: Manipulability-Aware Diffusion Policy for Posture-Dependent Bimanual Manipulation",
            "authors": [
                "Zhuo Li",
                "Junjia Liu",
                "Dianxi Li",
                "Tao Teng",
                "Miao Li",
                "Sylvain Calinon",
                "Darwin Caldwell",
                "Fei Chen"
            ],
            "arxiv_id": "2510.23016v1",
            "summary": "Recent work has demonstrated the potential of diffusion models in robot\nbimanual skill learning. However, existing methods ignore the learning of\nposture-dependent task features, which are crucial for adapting dual-arm\nconfigurations to meet specific force and velocity requirements in dexterous\nbimanual manipulation. To address this limitation, we propose\nManipulability-Aware Diffusion Policy (ManiDP), a novel imitation learning\nmethod that not only generates plausible bimanual trajectories, but also\noptimizes dual-arm configurations to better satisfy posture-dependent task\nrequirements. ManiDP achieves this by extracting bimanual manipulability from\nexpert demonstrations and encoding the encapsulated posture features using\nRiemannian-based probabilistic models. These encoded posture features are then\nincorporated into a conditional diffusion process to guide the generation of\ntask-compatible bimanual motion sequences. We evaluate ManiDP on six real-world\nbimanual tasks, where the experimental results demonstrate a 39.33$\\%$ increase\nin average manipulation success rate and a 0.45 improvement in task\ncompatibility compared to baseline methods. This work highlights the importance\nof integrating posture-relevant robotic priors into bimanual skill diffusion to\nenable human-like adaptability and dexterity.",
            "headline_zh": "提出ManiDP以解决双手机器人操作中姿势依赖任务特征学习不足的问题",
            "intro_zh": [
                "现有方法忽略姿势依赖任务特征，影响双臂配置适应力和速度需求",
                "ManiDP提取双手机动性，使用黎曼概率模型编码姿势特征，指导扩散过程生成运动序列",
                "在六项真实任务中，平均成功率提升39.33%，任务兼容性提高0.45"
            ],
            "tags_zh": [
                "双手机器人操作",
                "扩散策略",
                "姿势依赖特征",
                "模仿学习",
                "黎曼概率模型",
                "任务兼容性"
            ],
            "_index": 92
        },
        {
            "title": "UGAE: Unified Geometry and Attribute Enhancement for G-PCC Compressed Point Clouds",
            "authors": [
                "Pan Zhao",
                "Hui Yuan",
                "Chongzhen Tian",
                "Tian Guo",
                "Raouf Hamzaoui",
                "Zhigeng Pan"
            ],
            "arxiv_id": "2510.23009v1",
            "summary": "Lossy compression of point clouds reduces storage and transmission costs;\nhowever, it inevitably leads to irreversible distortion in geometry structure\nand attribute information. To address these issues, we propose a unified\ngeometry and attribute enhancement (UGAE) framework, which consists of three\ncore components: post-geometry enhancement (PoGE), pre-attribute enhancement\n(PAE), and post-attribute enhancement (PoAE). In PoGE, a Transformer-based\nsparse convolutional U-Net is used to reconstruct the geometry structure with\nhigh precision by predicting voxel occupancy probabilities. Building on the\nrefined geometry structure, PAE introduces an innovative enhanced\ngeometry-guided recoloring strategy, which uses a detail-aware K-Nearest\nNeighbors (DA-KNN) method to achieve accurate recoloring and effectively\npreserve high-frequency details before attribute compression. Finally, at the\ndecoder side, PoAE uses an attribute residual prediction network with a\nweighted mean squared error (W-MSE) loss to enhance the quality of\nhigh-frequency regions while maintaining the fidelity of low-frequency regions.\nUGAE significantly outperformed existing methods on three benchmark datasets:\n8iVFB, Owlii, and MVUB. Compared to the latest G-PCC test model (TMC13v29),\nUGAE achieved an average BD-PSNR gain of 9.98 dB and 90.98% BD-bitrate savings\nfor geometry under the D1 metric, as well as a 3.67 dB BD-PSNR improvement with\n56.88% BD-bitrate savings for attributes on the Y component. Additionally, it\nimproved perceptual quality significantly.",
            "headline_zh": "提出UGAE框架以增强G-PCC压缩点云的几何结构和属性质量",
            "intro_zh": [
                "点云有损压缩导致几何结构和属性信息不可逆失真",
                "UGAE包含PoGE、PAE和PoAE组件，分别处理几何重建、预属性增强和后属性增强",
                "在8iVFB等数据集上，几何和属性质量显著提升，BD-PSNR增益和比特率节省突出"
            ],
            "tags_zh": [
                "点云压缩",
                "几何增强",
                "属性增强",
                "Transformer网络",
                "G-PCC标准",
                "感知质量"
            ],
            "_index": 93
        },
        {
            "title": "CoMo: Compositional Motion Customization for Text-to-Video Generation",
            "authors": [
                "Youcan Xu",
                "Zhen Wang",
                "Jiaxin Shi",
                "Kexin Li",
                "Feifei Shao",
                "Jun Xiao",
                "Yi Yang",
                "Jun Yu",
                "Long Chen"
            ],
            "arxiv_id": "2510.23007v1",
            "summary": "While recent text-to-video models excel at generating diverse scenes, they\nstruggle with precise motion control, particularly for complex, multi-subject\nmotions. Although methods for single-motion customization have been developed\nto address this gap, they fail in compositional scenarios due to two primary\nchallenges: motion-appearance entanglement and ineffective multi-motion\nblending. This paper introduces CoMo, a novel framework for\n$\\textbf{compositional motion customization}$ in text-to-video generation,\nenabling the synthesis of multiple, distinct motions within a single video.\nCoMo addresses these issues through a two-phase approach. First, in the\nsingle-motion learning phase, a static-dynamic decoupled tuning paradigm\ndisentangles motion from appearance to learn a motion-specific module. Second,\nin the multi-motion composition phase, a plug-and-play divide-and-merge\nstrategy composes these learned motions without additional training by\nspatially isolating their influence during the denoising process. To facilitate\nresearch in this new domain, we also introduce a new benchmark and a novel\nevaluation metric designed to assess multi-motion fidelity and blending.\nExtensive experiments demonstrate that CoMo achieves state-of-the-art\nperformance, significantly advancing the capabilities of controllable video\ngeneration. Our project page is at https://como6.github.io/.",
            "headline_zh": "提出CoMo框架以解决文本到视频生成中的多运动定制问题",
            "intro_zh": [
                "核心问题：现有方法难以控制复杂多主体运动，存在运动-外观纠缠和多运动混合无效问题。",
                "方法要点：采用两阶段方法，包括静态-动态解耦调谐和即插即用分合策略，实现多运动合成。",
                "实验或效果：在引入的新基准上，CoMo实现最先进性能，提升可控视频生成能力。"
            ],
            "tags_zh": [
                "文本到视频生成",
                "运动定制",
                "多运动合成",
                "解耦学习",
                "可控生成"
            ],
            "_index": 94
        },
        {
            "title": "An Intelligent Water-Saving Irrigation System Based on Multi-Sensor Fusion and Visual Servoing Control",
            "authors": [
                "ZhengKai Huang",
                "YiKun Wang",
                "ChenYu Hui",
                "XiaoCheng"
            ],
            "arxiv_id": "2510.23003v1",
            "summary": "This paper introduces an intelligent water-saving irrigation system designed\nto address critical challenges in precision agriculture, such as inefficient\nwater use and poor terrain adaptability. The system integrates advanced\ncomputer vision, robotic control, and real-time stabilization technologies via\na multi-sensor fusion approach. A lightweight YOLO model, deployed on an\nembedded vision processor (K210), enables real-time plant container detection\nwith over 96% accuracy under varying lighting conditions. A simplified hand-eye\ncalibration algorithm-designed for 'handheld camera' robot arm\nconfigurations-ensures that the end effector can be precisely positioned, with\na success rate exceeding 90%. The active leveling system, driven by the\nSTM32F103ZET6 main control chip and JY901S inertial measurement data, can\nstabilize the irrigation platform on slopes up to 10 degrees, with a response\ntime of 1.8 seconds. Experimental results across three simulated agricultural\nenvironments (standard greenhouse, hilly terrain, complex lighting) demonstrate\na 30-50% reduction in water consumption compared to conventional flood\nirrigation, with water use efficiency exceeding 92% in all test cases.",
            "headline_zh": "提出基于多传感器融合与视觉伺服的智能节水灌溉系统，以解决精准农业中水资源浪费和地形适应性问题。",
            "intro_zh": [
                "核心问题：精准农业中水资源利用效率低和地形适应性差。",
                "方法要点：集成轻量YOLO模型、手眼校准和主动调平系统，实现实时检测与稳定控制。",
                "实验效果：在模拟环境中节水30-50%，用水效率超92%。"
            ],
            "tags_zh": [
                "多传感器融合",
                "视觉伺服控制",
                "轻量YOLO模型",
                "手眼校准",
                "主动调平系统",
                "节水灌溉"
            ],
            "_index": 95
        },
        {
            "title": "LoMix: Learnable Weighted Multi-Scale Logits Mixing for Medical Image Segmentation",
            "authors": [
                "Md Mostafijur Rahman",
                "Radu Marculescu"
            ],
            "arxiv_id": "2510.22995v1",
            "summary": "U-shaped networks output logits at multiple spatial scales, each capturing a\ndifferent blend of coarse context and fine detail. Yet, training still treats\nthese logits in isolation - either supervising only the final,\nhighest-resolution logits or applying deep supervision with identical loss\nweights at every scale - without exploring mixed-scale combinations.\nConsequently, the decoder output misses the complementary cues that arise only\nwhen coarse and fine predictions are fused. To address this issue, we introduce\nLoMix (Logits Mixing), a NAS-inspired, differentiable plug-and-play module that\ngenerates new mixed-scale outputs and learns how exactly each of them should\nguide the training process. More precisely, LoMix mixes the multi-scale decoder\nlogits with four lightweight fusion operators: addition, multiplication,\nconcatenation, and attention-based weighted fusion, yielding a rich set of\nsynthetic mutant maps. Every original or mutant map is given a softplus loss\nweight that is co-optimized with network parameters, mimicking a one-step\narchitecture search that automatically discovers the most useful scales,\nmixtures, and operators. Plugging LoMix into recent U-shaped architectures\n(i.e., PVT-V2-B2 backbone with EMCAD decoder) on Synapse 8-organ dataset\nimproves DICE by +4.2% over single-output supervision, +2.2% over deep\nsupervision, and +1.5% over equally weighted additive fusion, all with zero\ninference overhead. When training data are scarce (e.g., one or two labeled\nscans), the advantage grows to +9.23%, underscoring LoMix's data efficiency.\nAcross four benchmarks and diverse U-shaped networks, LoMiX improves DICE by up\nto +13.5% over single-output supervision, confirming that learnable weighted\nmixed-scale fusion generalizes broadly while remaining data efficient, fully\ninterpretable, and overhead-free at inference. Our code is available at\nhttps://github.com/SLDGroup/LoMix.",
            "headline_zh": "提出LoMix模块，通过可学习加权多尺度logits融合提升医学图像分割性能。",
            "intro_zh": [
                "U形网络多尺度logits训练时孤立处理，未充分利用粗-细预测融合互补信息。",
                "LoMix使用四种轻量融合算子混合logits，并学习软加损失权重，实现零推理开销。",
                "在多个基准测试中，DICE提升达+13.5%，数据稀缺时优势更显著。"
            ],
            "tags_zh": [
                "医学图像分割",
                "多尺度融合",
                "可学习权重",
                "U形网络",
                "零推理开销"
            ],
            "_index": 96
        },
        {
            "title": "SceneDecorator: Towards Scene-Oriented Story Generation with Scene Planning and Scene Consistency",
            "authors": [
                "Quanjian Song",
                "Donghao Zhou",
                "Jingyu Lin",
                "Fei Shen",
                "Jiaze Wang",
                "Xiaowei Hu",
                "Cunjian Chen",
                "Pheng-Ann Heng"
            ],
            "arxiv_id": "2510.22994v1",
            "summary": "Recent text-to-image models have revolutionized image generation, but they\nstill struggle with maintaining concept consistency across generated images.\nWhile existing works focus on character consistency, they often overlook the\ncrucial role of scenes in storytelling, which restricts their creativity in\npractice. This paper introduces scene-oriented story generation, addressing two\nkey challenges: (i) scene planning, where current methods fail to ensure\nscene-level narrative coherence by relying solely on text descriptions, and\n(ii) scene consistency, which remains largely unexplored in terms of\nmaintaining scene consistency across multiple stories. We propose\nSceneDecorator, a training-free framework that employs VLM-Guided Scene\nPlanning to ensure narrative coherence across different scenes in a\n``global-to-local'' manner, and Long-Term Scene-Sharing Attention to maintain\nlong-term scene consistency and subject diversity across generated stories.\nExtensive experiments demonstrate the superior performance of SceneDecorator,\nhighlighting its potential to unleash creativity in the fields of arts, films,\nand games.",
            "headline_zh": "提出SceneDecorator框架以解决场景导向故事生成中的场景规划和一致性挑战",
            "intro_zh": [
                "核心问题：现有方法忽视场景在故事生成中的作用，导致叙事连贯性和场景一致性不足",
                "方法要点：采用VLM引导场景规划确保全局到局部的叙事连贯，长时场景共享注意力维持一致性",
                "实验或效果：广泛实验显示SceneDecorator性能优越，在艺术、电影和游戏领域有应用潜力"
            ],
            "tags_zh": [
                "场景导向故事生成",
                "场景规划",
                "场景一致性",
                "VLM引导",
                "长时注意力",
                "训练无关框架"
            ],
            "_index": 97
        },
        {
            "title": "USF-MAE: Ultrasound Self-Supervised Foundation Model with Masked Autoencoding",
            "authors": [
                "Youssef Megahed",
                "Robin Ducharme",
                "Mark Walker",
                "Steven Hawken",
                "Adrian D. C. Chan"
            ],
            "arxiv_id": "2510.22990v1",
            "summary": "Ultrasound imaging is one of the most widely used diagnostic modalities,\noffering real-time, radiation-free assessment across diverse clinical domains.\nHowever, interpretation of ultrasound images remains challenging due to high\nnoise levels, operator dependence, and limited field of view, resulting in\nsubstantial inter-observer variability. Current Deep Learning approaches are\nhindered by the scarcity of large labeled datasets and the domain gap between\ngeneral and sonographic images, which limits the transferability of models\npretrained on non-medical data. To address these challenges, we introduce the\nUltrasound Self-Supervised Foundation Model with Masked Autoencoding (USF-MAE),\nthe first large-scale self-supervised MAE framework pretrained exclusively on\nultrasound data. The model was pre-trained on 370,000 2D and 3D ultrasound\nimages curated from 46 open-source datasets, collectively termed OpenUS-46,\nspanning over twenty anatomical regions. This curated dataset has been made\npublicly available to facilitate further research and reproducibility. Using a\nVision Transformer encoder-decoder architecture, USF-MAE reconstructs masked\nimage patches, enabling it to learn rich, modality-specific representations\ndirectly from unlabeled data. The pretrained encoder was fine-tuned on three\npublic downstream classification benchmarks: BUS-BRA (breast cancer), MMOTU-2D\n(ovarian tumors), and GIST514-DB (gastrointestinal stromal tumors). Across all\ntasks, USF-MAE consistently outperformed conventional CNN and ViT baselines,\nachieving F1-scores of 81.6%, 79.6%, and 82.4%, respectively. Despite not using\nlabels during pretraining, USF-MAE approached the performance of the supervised\nfoundation model UltraSam on breast cancer classification and surpassed it on\nthe other tasks, demonstrating strong cross-anatomical generalization.",
            "headline_zh": "提出USF-MAE自监督基础模型，通过掩码自编码解决超声图像标注稀缺问题。",
            "intro_zh": [
                "超声图像解释困难，因噪声高、操作依赖性强，导致观察者间差异大。",
                "使用ViT架构，在37万超声图像上预训练，通过重建掩码补丁学习模态特定表示。",
                "在三个分类任务中微调，F1分数达81.6%、79.6%和82.4%，优于基线模型。"
            ],
            "tags_zh": [
                "超声图像分析",
                "自监督学习",
                "掩码自编码",
                "视觉变换器",
                "医学影像分类"
            ],
            "_index": 98
        },
        {
            "title": "Exploring Semantic-constrained Adversarial Example with Instruction Uncertainty Reduction",
            "authors": [
                "Jin Hu",
                "Jiakai Wang",
                "Linna Jing",
                "Haolin Li",
                "Haodong Liu",
                "Haotong Qin",
                "Aishan Liu",
                "Ke Xu",
                "Xianglong Liu"
            ],
            "arxiv_id": "2510.22981v1",
            "summary": "Recently, semantically constrained adversarial examples (SemanticAE), which\nare directly generated from natural language instructions, have become a\npromising avenue for future research due to their flexible attacking forms. To\ngenerate SemanticAEs, current methods fall short of satisfactory attacking\nability as the key underlying factors of semantic uncertainty in human\ninstructions, such as referring diversity, descriptive incompleteness, and\nboundary ambiguity, have not been fully investigated. To tackle the issues,\nthis paper develops a multi-dimensional instruction uncertainty reduction\n(InSUR) framework to generate more satisfactory SemanticAE, i.e., transferable,\nadaptive, and effective. Specifically, in the dimension of the sampling method,\nwe propose the residual-driven attacking direction stabilization to alleviate\nthe unstable adversarial optimization caused by the diversity of language\nreferences. By coarsely predicting the language-guided sampling process, the\noptimization process will be stabilized by the designed ResAdv-DDIM sampler,\ntherefore releasing the transferable and robust adversarial capability of\nmulti-step diffusion models. In task modeling, we propose the context-encoded\nattacking scenario constraint to supplement the missing knowledge from\nincomplete human instructions. Guidance masking and renderer integration are\nproposed to regulate the constraints of 2D/3D SemanticAE, activating stronger\nscenario-adapted attacks. Moreover, in the dimension of generator evaluation,\nwe propose the semantic-abstracted attacking evaluation enhancement by\nclarifying the evaluation boundary, facilitating the development of more\neffective SemanticAE generators. Extensive experiments demonstrate the\nsuperiority of the transfer attack performance of InSUR. Moreover, we realize\nthe reference-free generation of semantically constrained 3D adversarial\nexamples for the first time.",
            "headline_zh": "提出多维度指令不确定性减少框架以提升语义约束对抗样本的攻击能力",
            "intro_zh": [
                "核心问题：现有方法因指令语义不确定性（如指代多样性和描述不完整）导致攻击能力不足",
                "方法要点：通过残差驱动采样、上下文编码约束和语义抽象评估来减少不确定性",
                "实验或效果：广泛实验显示InSUR在转移攻击性能上优越，并首次实现无参考3D对抗样本生成"
            ],
            "tags_zh": [
                "语义约束对抗样本",
                "指令不确定性减少",
                "扩散模型攻击",
                "3D对抗生成",
                "转移攻击性能"
            ],
            "_index": 99
        },
        {
            "title": "VoMP: Predicting Volumetric Mechanical Property Fields",
            "authors": [
                "Rishit Dagli",
                "Donglai Xiang",
                "Vismay Modi",
                "Charles Loop",
                "Clement Fuji Tsang",
                "Anka He Chen",
                "Anita Hu",
                "Gavriel State",
                "David I. W. Levin",
                "Maria Shugrina"
            ],
            "arxiv_id": "2510.22975v1",
            "summary": "Physical simulation relies on spatially-varying mechanical properties, often\nlaboriously hand-crafted. VoMP is a feed-forward method trained to predict\nYoung's modulus ($E$), Poisson's ratio ($\\nu$), and density ($\\rho$) throughout\nthe volume of 3D objects, in any representation that can be rendered and\nvoxelized. VoMP aggregates per-voxel multi-view features and passes them to our\ntrained Geometry Transformer to predict per-voxel material latent codes. These\nlatents reside on a manifold of physically plausible materials, which we learn\nfrom a real-world dataset, guaranteeing the validity of decoded per-voxel\nmaterials. To obtain object-level training data, we propose an annotation\npipeline combining knowledge from segmented 3D datasets, material databases,\nand a vision-language model, along with a new benchmark. Experiments show that\nVoMP estimates accurate volumetric properties, far outperforming prior art in\naccuracy and speed.",
            "headline_zh": "提出VoMP方法以预测3D物体的体积力学属性，替代手工设计。",
            "intro_zh": [
                "物理模拟依赖空间变化的力学属性，传统方法需手工设计，效率低。",
                "VoMP使用前馈网络聚合多视图特征，通过几何Transformer预测体素材料潜码。",
                "实验显示VoMP在准确性和速度上远超现有方法，基于真实数据集保证材料有效性。"
            ],
            "tags_zh": [
                "体积力学属性预测",
                "3D物体表示",
                "几何Transformer",
                "材料潜码学习",
                "多视图特征聚合"
            ],
            "_index": 100
        },
        {
            "title": "Scaling Up Occupancy-centric Driving Scene Generation: Dataset and Method",
            "authors": [
                "Bohan Li",
                "Xin Jin",
                "Hu Zhu",
                "Hongsi Liu",
                "Ruikai Li",
                "Jiazhe Guo",
                "Kaiwen Cai",
                "Chao Ma",
                "Yueming Jin",
                "Hao Zhao",
                "Xiaokang Yang",
                "Wenjun Zeng"
            ],
            "arxiv_id": "2510.22973v1",
            "summary": "Driving scene generation is a critical domain for autonomous driving,\nenabling downstream applications, including perception and planning evaluation.\nOccupancy-centric methods have recently achieved state-of-the-art results by\noffering consistent conditioning across frames and modalities; however, their\nperformance heavily depends on annotated occupancy data, which still remains\nscarce. To overcome this limitation, we curate Nuplan-Occ, the largest semantic\noccupancy dataset to date, constructed from the widely used Nuplan benchmark.\nIts scale and diversity facilitate not only large-scale generative modeling but\nalso autonomous driving downstream applications. Based on this dataset, we\ndevelop a unified framework that jointly synthesizes high-quality semantic\noccupancy, multi-view videos, and LiDAR point clouds. Our approach incorporates\na spatio-temporal disentangled architecture to support high-fidelity spatial\nexpansion and temporal forecasting of 4D dynamic occupancy. To bridge modal\ngaps, we further propose two novel techniques: a Gaussian splatting-based\nsparse point map rendering strategy that enhances multi-view video generation,\nand a sensor-aware embedding strategy that explicitly models LiDAR sensor\nproperties for realistic multi-LiDAR simulation. Extensive experiments\ndemonstrate that our method achieves superior generation fidelity and\nscalability compared to existing approaches, and validates its practical value\nin downstream tasks. Repo:\nhttps://github.com/Arlo0o/UniScene-Unified-Occupancy-centric-Driving-Scene-Generation/tree/v2",
            "headline_zh": "提出统一框架与Nuplan-Occ数据集，以解决驾驶场景生成中占用数据稀缺问题。",
            "intro_zh": [
                "核心问题：占用中心方法依赖稀缺的标注占用数据，限制驾驶场景生成性能。",
                "方法要点：开发统一框架，联合生成语义占用、多视角视频和LiDAR点云。",
                "实验或效果：在Nuplan-Occ数据集上验证，生成保真度和可扩展性优于现有方法。"
            ],
            "tags_zh": [
                "驾驶场景生成",
                "语义占用",
                "多模态合成",
                "LiDAR模拟",
                "时空解耦架构"
            ],
            "_index": 101
        },
        {
            "title": "VALA: Learning Latent Anchors for Training-Free and Temporally Consistent",
            "authors": [
                "Zhangkai Wu",
                "Xuhui Fan",
                "Zhongyuan Xie",
                "Kaize Shi",
                "Longbing Cao"
            ],
            "arxiv_id": "2510.22970v1",
            "summary": "Recent advances in training-free video editing have enabled lightweight and\nprecise cross-frame generation by leveraging pre-trained text-to-image\ndiffusion models. However, existing methods often rely on heuristic frame\nselection to maintain temporal consistency during DDIM inversion, which\nintroduces manual bias and reduces the scalability of end-to-end inference. In\nthis paper, we propose~\\textbf{VALA} (\\textbf{V}ariational \\textbf{A}lignment\nfor \\textbf{L}atent \\textbf{A}nchors), a variational alignment module that\nadaptively selects key frames and compresses their latent features into\nsemantic anchors for consistent video editing. To learn meaningful assignments,\nVALA propose a variational framework with a contrastive learning objective.\nTherefore, it can transform cross-frame latent representations into compressed\nlatent anchors that preserve both content and temporal coherence. Our method\ncan be fully integrated into training-free text-to-image based video editing\nmodels. Extensive experiments on real-world video editing benchmarks show that\nVALA achieves state-of-the-art performance in inversion fidelity, editing\nquality, and temporal consistency, while offering improved efficiency over\nprior methods.",
            "headline_zh": "提出VALA以解决训练免费视频编辑中的时间一致性问题",
            "intro_zh": [
                "现有方法依赖启发式帧选择，导致手动偏差和可扩展性差",
                "VALA使用变分对齐模块自适应选择关键帧并压缩潜在特征为语义锚点",
                "实验显示在真实视频编辑基准上实现高保真、高质量和高效性能"
            ],
            "tags_zh": [
                "训练免费视频编辑",
                "变分对齐",
                "潜在锚点",
                "时间一致性",
                "对比学习",
                "DDIM反演"
            ],
            "_index": 102
        },
        {
            "title": "Survey of Multimodal Geospatial Foundation Models: Techniques, Applications, and Challenges",
            "authors": [
                "Liling Yang",
                "Ning Chen",
                "Jun Yue",
                "Yidan Liu",
                "Jiayi Ma",
                "Pedram Ghamisi",
                "Antonio Plaza",
                "Leyuan Fang"
            ],
            "arxiv_id": "2510.22964v1",
            "summary": "Foundation models have transformed natural language processing and computer\nvision, and their impact is now reshaping remote sensing image analysis. With\npowerful generalization and transfer learning capabilities, they align\nnaturally with the multimodal, multi-resolution, and multi-temporal\ncharacteristics of remote sensing data. To address unique challenges in the\nfield, multimodal geospatial foundation models (GFMs) have emerged as a\ndedicated research frontier. This survey delivers a comprehensive review of\nmultimodal GFMs from a modality-driven perspective, covering five core visual\nand vision-language modalities. We examine how differences in imaging physics\nand data representation shape interaction design, and we analyze key techniques\nfor alignment, integration, and knowledge transfer to tackle modality\nheterogeneity, distribution shifts, and semantic gaps. Advances in training\nparadigms, architectures, and task-specific adaptation strategies are\nsystematically assessed alongside a wealth of emerging benchmarks.\nRepresentative multimodal visual and vision-language GFMs are evaluated across\nten downstream tasks, with insights into their architectures, performance, and\napplication scenarios. Real-world case studies, spanning land cover mapping,\nagricultural monitoring, disaster response, climate studies, and geospatial\nintelligence, demonstrate the practical potential of GFMs. Finally, we outline\npressing challenges in domain generalization, interpretability, efficiency, and\nprivacy, and chart promising avenues for future research.",
            "headline_zh": "综述多模态地理空间基础模型技术、应用与挑战，以应对遥感数据分析需求",
            "intro_zh": [
                "核心问题：遥感数据多模态、多分辨率、多时间特性带来的模态异质性和语义鸿沟。",
                "方法要点：从模态驱动视角，分析对齐、集成和知识转移等关键技术。",
                "实验或效果：评估模型在十项下游任务中的性能，并通过案例展示实际应用潜力。"
            ],
            "tags_zh": [
                "多模态基础模型",
                "地理空间分析",
                "遥感图像处理",
                "模态对齐",
                "下游任务评估",
                "领域挑战"
            ],
            "_index": 103
        },
        {
            "title": "FAME: Fairness-aware Attention-modulated Video Editing",
            "authors": [
                "Zhangkai Wu",
                "Xuhui Fan",
                "Zhongyuan Xie",
                "Kaize Shi",
                "Zhidong Li",
                "Longbing Cao"
            ],
            "arxiv_id": "2510.22960v1",
            "summary": "Training-free video editing (VE) models tend to fall back on gender\nstereotypes when rendering profession-related prompts. We propose \\textbf{FAME}\nfor \\textit{Fairness-aware Attention-modulated Video Editing} that mitigates\nprofession-related gender biases while preserving prompt alignment and temporal\nconsistency for coherent VE. We derive fairness embeddings from existing\nminority representations by softly injecting debiasing tokens into the text\nencoder. Simultaneously, FAME integrates fairness modulation into both temporal\nself attention and prompt-to-region cross attention to mitigate the motion\ncorruption and temporal inconsistency caused by directly introducing fairness\ncues. For temporal self attention, FAME introduces a region constrained\nattention mask combined with time decay weighting, which enhances intra-region\ncoherence while suppressing irrelevant inter-region interactions. For cross\nattention, it reweights tokens to region matching scores by incorporating\nfairness sensitive similarity masks derived from debiasing prompt embeddings.\nTogether, these modulations keep fairness-sensitive semantics tied to the right\nvisual regions and prevent temporal drift across frames. Extensive experiments\non new VE fairness-oriented benchmark \\textit{FairVE} demonstrate that FAME\nachieves stronger fairness alignment and semantic fidelity, surpassing existing\nVE baselines.",
            "headline_zh": "提出FAME方法以缓解视频编辑中的职业相关性别偏见，同时保持提示对齐和时间一致性。",
            "intro_zh": [
                "核心问题：无训练视频编辑模型在渲染职业相关提示时易陷入性别刻板印象。",
                "方法要点：通过软注入去偏标记和调制注意力机制，增强公平性并减少时间不一致。",
                "实验或效果：在FairVE基准上，FAME在公平对齐和语义保真度上优于现有基线。"
            ],
            "tags_zh": [
                "视频编辑",
                "公平性",
                "注意力机制",
                "时间一致性",
                "去偏技术"
            ],
            "_index": 104
        },
        {
            "title": "End-to-End Design and Validation of a Low-Cost Stewart Platform with Nonlinear Estimation and Control",
            "authors": [
                "Benedictus C. G. Cinun",
                "Tua A. Tamba",
                "Immanuel R. Santjoko",
                "Xiaofeng Wang",
                "Michael A. Gunarso",
                "Bin Hu"
            ],
            "arxiv_id": "2510.22949v1",
            "summary": "This paper presents the complete design, control, and experimental validation\nof a low-cost Stewart platform prototype developed as an affordable yet capable\nrobotic testbed for research and education. The platform combines off the shelf\ncomponents with 3D printed and custom fabricated parts to deliver full six\ndegrees of freedom motions using six linear actuators connecting a moving\nplatform to a fixed base. The system software integrates dynamic modeling, data\nacquisition, and real time control within a unified framework. A robust\ntrajectory tracking controller based on feedback linearization, augmented with\nan LQR scheme, compensates for the platform's nonlinear dynamics to achieve\nprecise motion control. In parallel, an Extended Kalman Filter fuses IMU and\nactuator encoder feedback to provide accurate and reliable state estimation\nunder sensor noise and external disturbances. Unlike prior efforts that\nemphasize only isolated aspects such as modeling or control, this work delivers\na complete hardware-software platform validated through both simulation and\nexperiments on static and dynamic trajectories. Results demonstrate effective\ntrajectory tracking and real-time state estimation, highlighting the platform's\npotential as a cost effective and versatile tool for advanced research and\neducational applications.",
            "headline_zh": "提出低成本Stewart平台，结合非线性估计与控制，用于研究和教育。",
            "intro_zh": [
                "核心问题：开发低成本六自由度Stewart平台，作为研究和教育测试平台。",
                "方法要点：集成反馈线性化与LQR控制，使用扩展卡尔曼滤波融合IMU和编码器数据。",
                "实验或效果：通过仿真和实验验证轨迹跟踪和状态估计，展示平台成本效益和多功能性。"
            ],
            "tags_zh": [
                "Stewart平台",
                "非线性控制",
                "状态估计",
                "低成本机器人",
                "实时控制"
            ],
            "_index": 105
        },
        {
            "title": "LightBagel: A Light-weighted, Double Fusion Framework for Unified Multimodal Understanding and Generation",
            "authors": [
                "Zeyu Wang",
                "Zilong Chen",
                "Chenhui Gou",
                "Feng Li",
                "Chaorui Deng",
                "Deyao Zhu",
                "Kunchang Li",
                "Weihao Yu",
                "Haoqin Tu",
                "Haoqi Fan",
                "Cihang Xie"
            ],
            "arxiv_id": "2510.22946v1",
            "summary": "Unified multimodal models have recently shown remarkable gains in both\ncapability and versatility, yet most leading systems are still trained from\nscratch and require substantial computational resources. In this paper, we show\nthat competitive performance can be obtained far more efficiently by\nstrategically fusing publicly available models specialized for either\ngeneration or understanding. Our key design is to retain the original blocks\nwhile additionally interleaving multimodal self-attention blocks throughout the\nnetworks. This double fusion mechanism (1) effectively enables rich multi-modal\nfusion while largely preserving the original strengths of the base models, and\n(2) catalyzes synergistic fusion of high-level semantic representations from\nthe understanding encoder with low-level spatial signals from the generation\nencoder. By training with only ~ 35B tokens, this approach achieves strong\nresults across multiple benchmarks: 0.91 on GenEval for compositional\ntext-to-image generation, 82.16 on DPG-Bench for complex text-to-image\ngeneration, 6.06 on GEditBench, and 3.77 on ImgEdit-Bench for image editing. By\nfully releasing the entire suite of code, model weights, and datasets, we hope\nto support future research on unified multimodal modeling.",
            "headline_zh": "提出LightBagel框架，通过双融合机制高效统一多模态理解与生成。",
            "intro_zh": [
                "问题：统一多模态模型训练资源消耗大，需从零开始构建。",
                "方法：保留原模型块，插入多模态自注意力块实现双融合。",
                "效果：仅用35B tokens训练，在多个基准测试中取得强结果。"
            ],
            "tags_zh": [
                "多模态融合",
                "轻量级框架",
                "自注意力机制",
                "统一模型",
                "高效训练"
            ],
            "_index": 106
        },
        {
            "title": "Switchable Token-Specific Codebook Quantization For Face Image Compression",
            "authors": [
                "Yongbo Wang",
                "Haonan Wang",
                "Guodong Mu",
                "Ruixin Zhang",
                "Jiaqi Chen",
                "Jingyun Zhang",
                "Jun Wang",
                "Yuan Xie",
                "Zhizhong Zhang",
                "Shouhong Ding"
            ],
            "arxiv_id": "2510.22943v1",
            "summary": "With the ever-increasing volume of visual data, the efficient and lossless\ntransmission, along with its subsequent interpretation and understanding, has\nbecome a critical bottleneck in modern information systems. The emerged\ncodebook-based solution utilize a globally shared codebook to quantize and\ndequantize each token, controlling the bpp by adjusting the number of tokens or\nthe codebook size. However, for facial images, which are rich in attributes,\nsuch global codebook strategies overlook both the category-specific\ncorrelations within images and the semantic differences among tokens, resulting\nin suboptimal performance, especially at low bpp. Motivated by these\nobservations, we propose a Switchable Token-Specific Codebook Quantization for\nface image compression, which learns distinct codebook groups for different\nimage categories and assigns an independent codebook to each token. By\nrecording the codebook group to which each token belongs with a small number of\nbits, our method can reduce the loss incurred when decreasing the size of each\ncodebook group. This enables a larger total number of codebooks under a lower\noverall bpp, thereby enhancing the expressive capability and improving\nreconstruction performance. Owing to its generalizable design, our method can\nbe integrated into any existing codebook-based representation learning approach\nand has demonstrated its effectiveness on face recognition datasets, achieving\nan average accuracy of 93.51% for reconstructed images at 0.05 bpp.",
            "headline_zh": "提出可切换令牌特定码本量化方法以提升人脸图像压缩性能",
            "intro_zh": [
                "人脸图像压缩中全局码本忽略类别相关性和令牌语义差异，导致低比特率性能不佳",
                "方法为不同图像类别学习码本组，并为每个令牌分配独立码本，减少码本大小损失",
                "在0.05 bpp下重构图像平均识别准确率达93.51%，可集成现有码本方法"
            ],
            "tags_zh": [
                "人脸图像压缩",
                "码本量化",
                "令牌特定码本",
                "图像重构",
                "低比特率压缩"
            ],
            "_index": 107
        },
        {
            "title": "Bi-Encoder Contrastive Learning for Fingerprint and Iris Biometrics",
            "authors": [
                "Matthew So",
                "Judah Goldfeder",
                "Mark Lis",
                "Hod Lipson"
            ],
            "arxiv_id": "2510.22937v1",
            "summary": "There has been a historic assumption that the biometrics of an individual are\nstatistically uncorrelated. We test this assumption by training Bi-Encoder\nnetworks on three verification tasks, including fingerprint-to-fingerprint\nmatching, iris-to-iris matching, and cross-modal fingerprint-to-iris matching\nusing 274 subjects with $\\sim$100k fingerprints and 7k iris images. We trained\nResNet-50 and Vision Transformer backbones in Bi-Encoder architectures such\nthat the contrastive loss between images sampled from the same individual is\nminimized. The iris ResNet architecture reaches 91 ROC AUC score for\niris-to-iris matching, providing clear evidence that the left and right irises\nof an individual are correlated. Fingerprint models reproduce the positive\nintra-subject suggested by prior work in this space. This is the first work\nattempting to use Vision Transformers for this matching. Cross-modal matching\nrises only slightly above chance, which suggests that more data and a more\nsophisticated pipeline is needed to obtain compelling results. These findings\ncontinue challenge independence assumptions of biometrics and we plan to extend\nthis work to other biometrics in the future. Code available:\nhttps://github.com/MatthewSo/bio_fingerprints_iris.",
            "headline_zh": "提出双编码器对比学习以验证指纹和虹膜生物特征相关性",
            "intro_zh": [
                "核心问题：测试个体生物特征统计独立性的历史假设",
                "方法要点：使用ResNet-50和Vision Transformer训练双编码器，最小化同个体图像对比损失",
                "实验或效果：虹膜匹配ROC AUC达91%，指纹结果与先前一致，跨模态匹配接近随机"
            ],
            "tags_zh": [
                "双编码器网络",
                "对比学习",
                "生物特征验证",
                "指纹匹配",
                "虹膜匹配",
                "跨模态匹配"
            ],
            "_index": 108
        },
        {
            "title": "Positional Preservation Embedding for Multimodal Large Language Models",
            "authors": [
                "Mouxiao Huang",
                "Borui Jiang",
                "Dehua Zheng",
                "Hailin Hu",
                "Kai Han",
                "Xinghao Chen"
            ],
            "arxiv_id": "2510.22936v1",
            "summary": "Multimodal large language models (MLLMs) have achieved strong performance on\nvision-language tasks, yet often suffer from inefficiencies due to redundant\nvisual tokens. Existing token merging methods reduce sequence length but\nfrequently disrupt spatial layouts and temporal continuity by disregarding\npositional relationships. In this work, we propose a novel encoding operator\ndubbed as \\textbf{P}ositional \\textbf{P}reservation \\textbf{E}mbedding\n(\\textbf{PPE}), which has the main hallmark of preservation of spatiotemporal\nstructure during visual token compression. PPE explicitly introduces the\ndisentangled encoding of 3D positions in the token dimension, enabling each\ncompressed token to encapsulate different positions from multiple original\ntokens. Furthermore, we show that PPE can effectively support cascade\nclustering -- a progressive token compression strategy that leads to better\nperformance retention. PPE is a parameter-free and generic operator that can be\nseamlessly integrated into existing token merging methods without any\nadjustments. Applied to state-of-the-art token merging framework, PPE achieves\nconsistent improvements of $2\\%\\sim5\\%$ across multiple vision-language\nbenchmarks, including MMBench (general vision understanding), TextVQA (layout\nunderstanding) and VideoMME (temporal understanding). These results demonstrate\nthat preserving positional cues is critical for efficient and effective MLLM\nreasoning.",
            "headline_zh": "提出位置保持嵌入以解决多模态大模型中视觉令牌压缩时的空间布局破坏问题",
            "intro_zh": [
                "核心问题：现有令牌合并方法在减少序列长度时忽略位置关系，破坏空间布局和时间连续性",
                "方法要点：PPE通过解耦编码3D位置，使压缩令牌封装多个原始令牌的不同位置",
                "实验或效果：在多个基准测试中实现2%~5%性能提升，支持级联聚类策略"
            ],
            "tags_zh": [
                "多模态大语言模型",
                "视觉令牌压缩",
                "位置保持嵌入",
                "空间布局保留",
                "时间连续性",
                "级联聚类"
            ],
            "_index": 109
        },
        {
            "title": "Gen-LangSplat: Generalized Language Gaussian Splatting with Pre-Trained Feature Compression",
            "authors": [
                "Pranav Saxena"
            ],
            "arxiv_id": "2510.22930v1",
            "summary": "Modeling open-vocabulary language fields in 3D is essential for intuitive\nhuman-AI interaction and querying within physical environments.\nState-of-the-art approaches, such as LangSplat, leverage 3D Gaussian Splatting\nto efficiently construct these language fields, encoding features distilled\nfrom high-dimensional models like CLIP. However, this efficiency is currently\noffset by the requirement to train a scene-specific language autoencoder for\nfeature compression, introducing a costly, per-scene optimization bottleneck\nthat hinders deployment scalability. In this work, we introduce Gen-LangSplat,\nthat eliminates this requirement by replacing the scene-wise autoencoder with a\ngeneralized autoencoder, pre-trained extensively on the large-scale ScanNet\ndataset. This architectural shift enables the use of a fixed, compact latent\nspace for language features across any new scene without any scene-specific\ntraining. By removing this dependency, our entire language field construction\nprocess achieves a efficiency boost while delivering querying performance\ncomparable to, or exceeding, the original LangSplat method. To validate our\ndesign choice, we perform a thorough ablation study empirically determining the\noptimal latent embedding dimension and quantifying representational fidelity\nusing Mean Squared Error and cosine similarity between the original and\nreprojected 512-dimensional CLIP embeddings. Our results demonstrate that\ngeneralized embeddings can efficiently and accurately support open-vocabulary\nquerying in novel 3D scenes, paving the way for scalable, real-time interactive\n3D AI applications.",
            "headline_zh": "提出Gen-LangSplat以消除3D语言场构建中的场景特定训练瓶颈",
            "intro_zh": [
                "核心问题：现有方法需为每个场景训练语言自编码器，导致部署扩展性差。",
                "方法要点：使用预训练通用自编码器替代场景特定模型，实现跨场景固定潜在空间。",
                "实验或效果：在ScanNet数据集验证，性能与原方法相当或更优，提升效率。"
            ],
            "tags_zh": [
                "3D高斯泼溅",
                "语言场建模",
                "预训练特征压缩",
                "开放词汇查询",
                "自编码器优化"
            ],
            "_index": 110
        },
        {
            "title": "HyPerNav: Hybrid Perception for Object-Oriented Navigation in Unknown Environment",
            "authors": [
                "Zecheng Yin",
                "Hao Zhao",
                "Zhen Li"
            ],
            "arxiv_id": "2510.22917v1",
            "summary": "Objective-oriented navigation(ObjNav) enables robot to navigate to target\nobject directly and autonomously in an unknown environment. Effective\nperception in navigation in unknown environment is critical for autonomous\nrobots. While egocentric observations from RGB-D sensors provide abundant local\ninformation, real-time top-down maps offer valuable global context for ObjNav.\nNevertheless, the majority of existing studies focus on a single source, seldom\nintegrating these two complementary perceptual modalities, despite the fact\nthat humans naturally attend to both. With the rapid advancement of\nVision-Language Models(VLMs), we propose Hybrid Perception Navigation\n(HyPerNav), leveraging VLMs' strong reasoning and vision-language understanding\ncapabilities to jointly perceive both local and global information to enhance\nthe effectiveness and intelligence of navigation in unknown environments. In\nboth massive simulation evaluation and real-world validation, our methods\nachieved state-of-the-art performance against popular baselines. Benefiting\nfrom hybrid perception approach, our method captures richer cues and finds the\nobjects more effectively, by simultaneously leveraging information\nunderstanding from egocentric observations and the top-down map. Our ablation\nstudy further proved that either of the hybrid perception contributes to the\nnavigation performance.",
            "headline_zh": "提出HyPerNav方法，利用视觉语言模型融合局部与全局感知，提升未知环境中目标导向导航性能。",
            "intro_zh": [
                "核心问题：未知环境中目标导向导航依赖单一感知源，缺乏局部与全局信息融合。",
                "方法要点：结合视觉语言模型，同时处理RGB-D传感器局部观测和实时俯视图全局上下文。",
                "实验或效果：在仿真和真实世界测试中，性能优于基线，消融研究验证混合感知有效性。"
            ],
            "tags_zh": [
                "目标导向导航",
                "视觉语言模型",
                "混合感知",
                "未知环境导航",
                "RGB-D传感器",
                "俯视图融合"
            ],
            "_index": 111
        },
        {
            "title": "Estimating Pasture Biomass from Top-View Images: A Dataset for Precision Agriculture",
            "authors": [
                "Qiyu Liao",
                "Dadong Wang",
                "Rebecca Haling",
                "Jiajun Liu",
                "Xun Li",
                "Martyna Plomecka",
                "Andrew Robson",
                "Matthew Pringle",
                "Rhys Pirie",
                "Megan Walker",
                "Joshua Whelan"
            ],
            "arxiv_id": "2510.22916v1",
            "summary": "Accurate estimation of pasture biomass is important for decision-making in\nlivestock production systems. Estimates of pasture biomass can be used to\nmanage stocking rates to maximise pasture utilisation, while minimising the\nrisk of overgrazing and promoting overall system health. We present a\ncomprehensive dataset of 1,162 annotated top-view images of pastures collected\nacross 19 locations in Australia. The images were taken across multiple seasons\nand include a range of temperate pasture species. Each image captures a 70cm *\n30cm quadrat and is paired with on-ground measurements including biomass sorted\nby component (green, dead, and legume fraction), vegetation height, and\nNormalized Difference Vegetation Index (NDVI) from Active Optical Sensors\n(AOS). The multidimensional nature of the data, which combines visual,\nspectral, and structural information, opens up new possibilities for advancing\nthe use of precision grazing management. The dataset is released and hosted in\na Kaggle competition that challenges the international Machine Learning\ncommunity with the task of pasture biomass estimation. The dataset is available\non the official Kaggle webpage:\nhttps://www.kaggle.com/competitions/csiro-biomass",
            "headline_zh": "提出基于俯视图像的牧场生物量数据集，以支持精准放牧管理。",
            "intro_zh": [
                "核心问题：准确估计牧场生物量对优化牲畜生产系统决策至关重要。",
                "方法要点：构建包含1162张俯视图像的多维数据集，结合视觉、光谱和结构信息。",
                "实验或效果：数据集通过Kaggle竞赛发布，推动机器学习在生物量估计中的应用。"
            ],
            "tags_zh": [
                "牧场生物量估计",
                "俯视图像数据集",
                "精准农业",
                "多维数据融合",
                "Kaggle竞赛"
            ],
            "_index": 112
        },
        {
            "title": "Clinic-Oriented Feasibility of a Sensor-Fused Wearable for Upper-Limb Function",
            "authors": [
                "Thanyanee Srichaisak",
                "Arissa Ieochai",
                "Aueaphum Aueawattthanaphisut"
            ],
            "arxiv_id": "2510.22913v1",
            "summary": "Background: Upper-limb weakness and tremor (4--12 Hz) limit activities of\ndaily living (ADL) and reduce adherence to home rehabilitation. Objective: To\nassess technical feasibility and clinician-relevant signals of a sensor-fused\nwearable targeting the triceps brachii and extensor pollicis brevis. Methods: A\nlightweight node integrates surface EMG (1 kHz), IMU (100--200 Hz), and\nflex/force sensors with on-device INT8 inference (Tiny 1D-CNN/Transformer) and\na safety-bounded assist policy (angle/torque/jerk limits; stall/time-out).\nHealthy adults (n = 12) performed three ADL-like tasks. Primary outcomes:\nTremor Index (TI), range of motion (ROM), repetitions (Reps min$^{-1}$).\nSecondary: EMG median-frequency slope (fatigue trend), closed-loop latency,\nsession completion, and device-related adverse events. Analyses used\nsubject-level paired medians with BCa 95\\% CIs; exact Wilcoxon $p$-values are\nreported in the Results. Results: Assistance was associated with lower tremor\nprominence and improved task throughput: TI decreased by $-0.092$ (95\\% CI\n[$-0.102$, $-0.079$]), ROM increased by $+12.65\\%$ (95\\% CI [$+8.43$,\n$+13.89$]), and Reps rose by $+2.99$ min$^{-1}$ (95\\% CI [$+2.61$, $+3.35$]).\nMedian on-device latency was 8.7 ms at a 100 Hz loop rate; all sessions were\ncompleted with no device-related adverse events. Conclusions: Multimodal\nsensing with low-latency, safety-bounded assistance produced improved movement\nquality (TI $\\downarrow$) and throughput (ROM, Reps $\\uparrow$) in a pilot\ntechnical-feasibility setting, supporting progression to IRB-approved patient\nstudies. Trial registration: Not applicable (pilot non-clinical).",
            "headline_zh": "提出传感器融合可穿戴设备，用于改善上肢功能康复的震颤和活动范围。",
            "intro_zh": [
                "核心问题：上肢无力和震颤限制日常生活活动，降低家庭康复依从性。",
                "方法要点：集成表面肌电、IMU和弯曲/力传感器，采用低延迟设备端推理和安全辅助策略。",
                "实验或效果：在健康成人中测试，震颤指数降低，活动范围和重复次数增加，无不良事件。"
            ],
            "tags_zh": [
                "传感器融合",
                "上肢康复",
                "震颤检测",
                "设备端推理",
                "安全辅助策略",
                "技术可行性"
            ],
            "_index": 113
        },
        {
            "title": "Never Too Rigid to Reach: Adaptive Virtual Model Control with LLM- and Lyapunov-Based Reinforcement Learning",
            "authors": [
                "Jingzehua Xu",
                "Yangyang Li",
                "Yangfei Chen",
                "Guanwen Xie",
                "Shuai Zhang"
            ],
            "arxiv_id": "2510.22892v1",
            "summary": "Robotic arms are increasingly deployed in uncertain environments, yet\nconventional control pipelines often become rigid and brittle when exposed to\nperturbations or incomplete information. Virtual Model Control (VMC) enables\ncompliant behaviors by embedding virtual forces and mapping them into joint\ntorques, but its reliance on fixed parameters and limited coordination among\nvirtual components constrains adaptability and may undermine stability as task\nobjectives evolve. To address these limitations, we propose Adaptive VMC with\nLarge Language Model (LLM)- and Lyapunov-Based Reinforcement Learning (RL),\nwhich preserves the physical interpretability of VMC while supporting\nstability-guaranteed online adaptation. The LLM provides structured priors and\nhigh-level reasoning that enhance coordination among virtual components,\nimprove sample efficiency, and facilitate flexible adjustment to varying task\nrequirements. Complementarily, Lyapunov-based RL enforces theoretical stability\nconstraints, ensuring safe and reliable adaptation under uncertainty. Extensive\nsimulations on a 7-DoF Panda arm demonstrate that our approach effectively\nbalances competing objectives in dynamic tasks, achieving superior performance\nwhile highlighting the synergistic benefits of LLM guidance and\nLyapunov-constrained adaptation.",
            "headline_zh": "提出自适应虚拟模型控制，结合LLM与Lyapunov强化学习，提升机械臂在不确定环境中的适应性与稳定性。",
            "intro_zh": [
                "核心问题：传统虚拟模型控制参数固定、组件协调不足，在扰动下易失稳且适应性差。",
                "方法要点：LLM提供先验与推理增强协调，Lyapunov强化学习确保理论稳定性约束。",
                "实验或效果：在7自由度Panda臂仿真中，动态任务表现优异，平衡目标并保证安全。"
            ],
            "tags_zh": [
                "虚拟模型控制",
                "强化学习",
                "大语言模型",
                "Lyapunov稳定性",
                "机器人控制",
                "自适应控制"
            ],
            "_index": 114
        }
    ]
}