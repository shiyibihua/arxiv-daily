{
  "count": 115,
  "papers": [
    {
      "title": "Concerto: Joint 2D-3D Self-Supervised Learning Emerges Spatial Representations",
      "authors": [
        "Yujia Zhang",
        "Xiaoyang Wu",
        "Yixing Lao",
        "Chengyao Wang",
        "Zhuotao Tian",
        "Naiyan Wang",
        "Hengshuang Zhao"
      ],
      "arxiv_id": "2510.23607v1",
      "summary": "Humans learn abstract concepts through multisensory synergy, and once formed,\nsuch representations can often be recalled from a single modality. Inspired by\nthis principle, we introduce Concerto, a minimalist simulation of human concept\nlearning for spatial cognition, combining 3D intra-modal self-distillation with\n2D-3D cross-modal joint embedding. Despite its simplicity, Concerto learns more\ncoherent and informative spatial features, as demonstrated by zero-shot\nvisualizations. It outperforms both standalone SOTA 2D and 3D self-supervised\nmodels by 14.2% and 4.8%, respectively, as well as their feature concatenation,\nin linear probing for 3D scene perception. With full fine-tuning, Concerto sets\nnew SOTA results across multiple scene understanding benchmarks (e.g., 80.7%\nmIoU on ScanNet). We further present a variant of Concerto tailored for\nvideo-lifted point cloud spatial understanding, and a translator that linearly\nprojects Concerto representations into CLIP's language space, enabling\nopen-world perception. These results highlight that Concerto emerges spatial\nrepresentations with superior fine-grained geometric and semantic consistency."
    },
    {
      "title": "Track, Inpaint, Resplat: Subject-driven 3D and 4D Generation with Progressive Texture Infilling",
      "authors": [
        "Shuhong Zheng",
        "Ashkan Mirzaei",
        "Igor Gilitschenski"
      ],
      "arxiv_id": "2510.23605v1",
      "summary": "Current 3D/4D generation methods are usually optimized for photorealism,\nefficiency, and aesthetics. However, they often fail to preserve the semantic\nidentity of the subject across different viewpoints. Adapting generation\nmethods with one or few images of a specific subject (also known as\nPersonalization or Subject-driven generation) allows generating visual content\nthat align with the identity of the subject. However, personalized 3D/4D\ngeneration is still largely underexplored. In this work, we introduce TIRE\n(Track, Inpaint, REsplat), a novel method for subject-driven 3D/4D generation.\nIt takes an initial 3D asset produced by an existing 3D generative model as\ninput and uses video tracking to identify the regions that need to be modified.\nThen, we adopt a subject-driven 2D inpainting model for progressively infilling\nthe identified regions. Finally, we resplat the modified 2D multi-view\nobservations back to 3D while still maintaining consistency. Extensive\nexperiments demonstrate that our approach significantly improves identity\npreservation in 3D/4D generation compared to state-of-the-art methods. Our\nproject website is available at\nhttps://zsh2000.github.io/track-inpaint-resplat.github.io/."
    },
    {
      "title": "PixelRefer: A Unified Framework for Spatio-Temporal Object Referring with Arbitrary Granularity",
      "authors": [
        "Yuqian Yuan",
        "Wenqiao Zhang",
        "Xin Li",
        "Shihao Wang",
        "Kehan Li",
        "Wentong Li",
        "Jun Xiao",
        "Lei Zhang",
        "Beng Chin Ooi"
      ],
      "arxiv_id": "2510.23603v1",
      "summary": "Multimodal large language models (MLLMs) have demonstrated strong\ngeneral-purpose capabilities in open-world visual comprehension. However, most\nexisting MLLMs primarily focus on holistic, scene-level understanding, often\noverlooking the need for fine-grained, object-centric reasoning. In this paper,\nwe present PixelRefer, a unified region-level MLLM framework that enables\nadvanced fine-grained understanding over user-specified regions across both\nimages and videos. Motivated by the observation that LLM attention\npredominantly focuses on object-level tokens, we propose a Scale-Adaptive\nObject Tokenizer (SAOT) to generate compact and semantically rich object\nrepresentations from free-form regions. Our analysis reveals that global visual\ntokens contribute mainly in early LLM layers, inspiring the design of\nPixelRefer-Lite, an efficient variant that employs an Object-Centric Infusion\nmodule to pre-fuse global context into object tokens. This yields a lightweight\nObject-Only Framework that substantially reduces computational cost while\nmaintaining high semantic fidelity. To facilitate fine-grained instruction\ntuning, we curate PixelRefer-2.2M, a high-quality object-centric instruction\ndataset. Extensive experiments across a range of benchmarks validate that\nPixelRefer achieves leading performance with fewer training samples, while\nPixelRefer-Lite offers competitive accuracy with notable gains in efficiency."
    },
    {
      "title": "PRISM-Bench: A Benchmark of Puzzle-Based Visual Tasks with CoT Error Detection",
      "authors": [
        "Yusu Qian",
        "Cheng Wan",
        "Chao Jia",
        "Yinfei Yang",
        "Qingyu Zhao",
        "Zhe Gan"
      ],
      "arxiv_id": "2510.23594v1",
      "summary": "We introduce \\textbf{PRISM-Bench}, a benchmark of puzzle-based visual\nchallenges designed to evaluate not only whether models can solve problems, but\nhow their reasoning unfolds. Unlike prior evaluations that measure only\nfinal-answer accuracy, PRISM-Bench introduces a diagnostic task: given a visual\npuzzle and a step-by-step chain-of-thought (CoT) containing exactly one error,\nmodels must identify the first incorrect step. This setting enables\nfine-grained assessment of logical consistency, error detection, and visual\nreasoning. The puzzles in PRISM-Bench require multi-step symbolic, geometric,\nand analogical reasoning, resisting shortcuts based on superficial pattern\nmatching. Evaluations across state-of-the-art MLLMs reveal a persistent gap\nbetween fluent generation and faithful reasoning: models that produce plausible\nCoTs often fail to locate simple logical faults. By disentangling answer\ngeneration from reasoning verification, PRISM-Bench offers a sharper lens on\nmultimodal reasoning competence and underscores the need for diagnostic\nevaluation protocols in the development of trustworthy MLLMs."
    },
    {
      "title": "InFlux: A Benchmark for Self-Calibration of Dynamic Intrinsics of Video Cameras",
      "authors": [
        "Erich Liang",
        "Roma Bhattacharjee",
        "Sreemanti Dey",
        "Rafael Moschopoulos",
        "Caitlin Wang",
        "Michel Liao",
        "Grace Tan",
        "Andrew Wang",
        "Karhan Kayan",
        "Stamatis Alexandropoulos",
        "Jia Deng"
      ],
      "arxiv_id": "2510.23589v1",
      "summary": "Accurately tracking camera intrinsics is crucial for achieving 3D\nunderstanding from 2D video. However, most 3D algorithms assume that camera\nintrinsics stay constant throughout a video, which is often not true for many\nreal-world in-the-wild videos. A major obstacle in this field is a lack of\ndynamic camera intrinsics benchmarks--existing benchmarks typically offer\nlimited diversity in scene content and intrinsics variation, and none provide\nper-frame intrinsic changes for consecutive video frames. In this paper, we\npresent Intrinsics in Flux (InFlux), a real-world benchmark that provides\nper-frame ground truth intrinsics annotations for videos with dynamic\nintrinsics. Compared to prior benchmarks, InFlux captures a wider range of\nintrinsic variations and scene diversity, featuring 143K+ annotated frames from\n386 high-resolution indoor and outdoor videos with dynamic camera intrinsics.\nTo ensure accurate per-frame intrinsics, we build a comprehensive lookup table\nof calibration experiments and extend the Kalibr toolbox to improve its\naccuracy and robustness. Using our benchmark, we evaluate existing baseline\nmethods for predicting camera intrinsics and find that most struggle to achieve\naccurate predictions on videos with dynamic intrinsics. For the dataset, code,\nvideos, and submission, please visit https://influx.cs.princeton.edu/."
    },
    {
      "title": "FARMER: Flow AutoRegressive Transformer over Pixels",
      "authors": [
        "Guangting Zheng",
        "Qinyu Zhao",
        "Tao Yang",
        "Fei Xiao",
        "Zhijie Lin",
        "Jie Wu",
        "Jiajun Deng",
        "Yanyong Zhang",
        "Rui Zhu"
      ],
      "arxiv_id": "2510.23588v1",
      "summary": "Directly modeling the explicit likelihood of the raw data distribution is key\ntopic in the machine learning area, which achieves the scaling successes in\nLarge Language Models by autoregressive modeling. However, continuous AR\nmodeling over visual pixel data suffer from extremely long sequences and\nhigh-dimensional spaces. In this paper, we present FARMER, a novel end-to-end\ngenerative framework that unifies Normalizing Flows (NF) and Autoregressive\n(AR) models for tractable likelihood estimation and high-quality image\nsynthesis directly from raw pixels. FARMER employs an invertible autoregressive\nflow to transform images into latent sequences, whose distribution is modeled\nimplicitly by an autoregressive model. To address the redundancy and complexity\nin pixel-level modeling, we propose a self-supervised dimension reduction\nscheme that partitions NF latent channels into informative and redundant\ngroups, enabling more effective and efficient AR modeling. Furthermore, we\ndesign a one-step distillation scheme to significantly accelerate inference\nspeed and introduce a resampling-based classifier-free guidance algorithm to\nboost image generation quality. Extensive experiments demonstrate that FARMER\nachieves competitive performance compared to existing pixel-based generative\nmodels while providing exact likelihoods and scalable training."
    },
    {
      "title": "Lookahead Anchoring: Preserving Character Identity in Audio-Driven Human Animation",
      "authors": [
        "Junyoung Seo",
        "Rodrigo Mira",
        "Alexandros Haliassos",
        "Stella Bounareli",
        "Honglie Chen",
        "Linh Tran",
        "Seungryong Kim",
        "Zoe Landgraf",
        "Jie Shen"
      ],
      "arxiv_id": "2510.23581v1",
      "summary": "Audio-driven human animation models often suffer from identity drift during\ntemporal autoregressive generation, where characters gradually lose their\nidentity over time. One solution is to generate keyframes as intermediate\ntemporal anchors that prevent degradation, but this requires an additional\nkeyframe generation stage and can restrict natural motion dynamics. To address\nthis, we propose Lookahead Anchoring, which leverages keyframes from future\ntimesteps ahead of the current generation window, rather than within it. This\ntransforms keyframes from fixed boundaries into directional beacons: the model\ncontinuously pursues these future anchors while responding to immediate audio\ncues, maintaining consistent identity through persistent guidance. This also\nenables self-keyframing, where the reference image serves as the lookahead\ntarget, eliminating the need for keyframe generation entirely. We find that the\ntemporal lookahead distance naturally controls the balance between expressivity\nand consistency: larger distances allow for greater motion freedom, while\nsmaller ones strengthen identity adherence. When applied to three recent human\nanimation models, Lookahead Anchoring achieves superior lip synchronization,\nidentity preservation, and visual quality, demonstrating improved temporal\nconditioning across several different architectures. Video results are\navailable at the following link: https://lookahead-anchoring.github.io."
    },
    {
      "title": "UrbanVLA: A Vision-Language-Action Model for Urban Micromobility",
      "authors": [
        "Anqi Li",
        "Zhiyong Wang",
        "Jiazhao Zhang",
        "Minghan Li",
        "Yunpeng Qi",
        "Zhibo Chen",
        "Zhizheng Zhang",
        "He Wang"
      ],
      "arxiv_id": "2510.23576v1",
      "summary": "Urban micromobility applications, such as delivery robots, demand reliable\nnavigation across large-scale urban environments while following long-horizon\nroute instructions. This task is particularly challenging due to the dynamic\nand unstructured nature of real-world city areas, yet most existing navigation\nmethods remain tailored to short-scale and controllable scenarios. Effective\nurban micromobility requires two complementary levels of navigation skills:\nlow-level capabilities such as point-goal reaching and obstacle avoidance, and\nhigh-level capabilities, such as route-visual alignment. To this end, we\npropose UrbanVLA, a route-conditioned Vision-Language-Action (VLA) framework\ndesigned for scalable urban navigation. Our method explicitly aligns noisy\nroute waypoints with visual observations during execution, and subsequently\nplans trajectories to drive the robot. To enable UrbanVLA to master both levels\nof navigation, we employ a two-stage training pipeline. The process begins with\nSupervised Fine-Tuning (SFT) using simulated environments and trajectories\nparsed from web videos. This is followed by Reinforcement Fine-Tuning (RFT) on\na mixture of simulation and real-world data, which enhances the model's safety\nand adaptability in real-world settings. Experiments demonstrate that UrbanVLA\nsurpasses strong baselines by more than 55% in the SocialNav task on MetaUrban.\nFurthermore, UrbanVLA achieves reliable real-world navigation, showcasing both\nscalability to large-scale urban environments and robustness against real-world\nuncertainties."
    },
    {
      "title": "More Than Generation: Unifying Generation and Depth Estimation via Text-to-Image Diffusion Models",
      "authors": [
        "Hongkai Lin",
        "Dingkang Liang",
        "Mingyang Du",
        "Xin Zhou",
        "Xiang Bai"
      ],
      "arxiv_id": "2510.23574v1",
      "summary": "Generative depth estimation methods leverage the rich visual priors stored in\npre-trained text-to-image diffusion models, demonstrating astonishing zero-shot\ncapability. However, parameter updates during training lead to catastrophic\ndegra- dation in the image generation capability of the pre-trained model. We\nintroduce MERGE, a unified model for image generation and depth estimation,\nstarting from a fixed pre-trained text-to-image model. MERGE demonstrates that\nthe pre-trained text-to-image model can do more than image generation, but also\nexpand to depth estimation effortlessly. Specifically, MERGE introduces a play-\nand-plug framework that enables seamless switching between image generation and\ndepth estimation modes through simple and pluggable converters. Meanwhile, we\npropose a Group Reuse Mechanism to encourage parameter reuse and im- prove the\nutilization of the additional learnable parameters. MERGE unleashes the\npowerful depth estimation capability of the pre-trained text-to-image model\nwhile preserving its original image generation ability. Compared to other\nunified models for image generation and depth estimation, MERGE achieves\nstate-of- the-art performance across multiple depth estimation benchmarks. The\ncode will be made available at https://github.com/H-EmbodVis/MERGE"
    },
    {
      "title": "RobotArena $\\infty$: Scalable Robot Benchmarking via Real-to-Sim Translation",
      "authors": [
        "Yash Jangir",
        "Yidi Zhang",
        "Kashu Yamazaki",
        "Chenyu Zhang",
        "Kuan-Hsun Tu",
        "Tsung-Wei Ke",
        "Lei Ke",
        "Yonatan Bisk",
        "Katerina Fragkiadaki"
      ],
      "arxiv_id": "2510.23571v1",
      "summary": "The pursuit of robot generalists - instructable agents capable of performing\ndiverse tasks across diverse environments - demands rigorous and scalable\nevaluation. Yet real-world testing of robot policies remains fundamentally\nconstrained: it is labor-intensive, slow, unsafe at scale, and difficult to\nreproduce. Existing simulation benchmarks are similarly limited, as they train\nand test policies within the same synthetic domains and cannot assess models\ntrained from real-world demonstrations or alternative simulation environments.\nAs policies expand in scope and complexity, these barriers only intensify,\nsince defining \"success\" in robotics often hinges on nuanced human judgments of\nexecution quality. In this paper, we introduce a new benchmarking framework\nthat overcomes these challenges by shifting VLA evaluation into large-scale\nsimulated environments augmented with online human feedback. Leveraging\nadvances in vision-language models, 2D-to-3D generative modeling, and\ndifferentiable rendering, our approach automatically converts video\ndemonstrations from widely used robot datasets into simulated counterparts.\nWithin these digital twins, we assess VLA policies using both automated\nVLM-guided scoring and scalable human preference judgments collected from\ncrowdworkers, transforming human involvement from tedious scene setup,\nresetting, and safety supervision into lightweight preference comparisons. To\nmeasure robustness, we systematically perturb simulated environments along\nmultiple axes, such as textures and object placements, stress-testing policy\ngeneralization under controlled variation. The result is a continuously\nevolving, reproducible, and scalable benchmark for real-world trained robot\nmanipulation policies, addressing a critical missing capability in today's\nrobotics landscape."
    },
    {
      "title": "EgoThinker: Unveiling Egocentric Reasoning with Spatio-Temporal CoT",
      "authors": [
        "Baoqi Pei",
        "Yifei Huang",
        "Jilan Xu",
        "Yuping He",
        "Guo Chen",
        "Fei Wu",
        "Yu Qiao",
        "Jiangmiao Pang"
      ],
      "arxiv_id": "2510.23569v1",
      "summary": "Egocentric video reasoning centers on an unobservable agent behind the camera\nwho dynamically shapes the environment, requiring inference of hidden\nintentions and recognition of fine-grained interactions. This core challenge\nlimits current multimodal large language models MLLMs, which excel at visible\nevent reasoning but lack embodied, first-person understanding. To bridge this\ngap, we introduce EgoThinker, a novel framework that endows MLLMs with robust\negocentric reasoning capabilities through spatio-temporal chain-of-thought\nsupervision and a two-stage learning curriculum. First, we introduce EgoRe-5M,\na large-scale egocentric QA dataset constructed from 13M diverse egocentric\nvideo clips. This dataset features multi-minute segments annotated with\ndetailed CoT rationales and dense hand-object grounding. Second, we employ SFT\non EgoRe-5M to instill reasoning skills, followed by reinforcement fine-tuning\nRFT to further enhance spatio-temporal localization. Experimental results show\nthat EgoThinker outperforms existing methods across multiple egocentric\nbenchmarks, while achieving substantial improvements in fine-grained\nspatio-temporal localization tasks. Full code and data are released at\nhttps://github.com/InternRobotics/EgoThinker."
    },
    {
      "title": "Revising Second Order Terms in Deep Animation Video Coding",
      "authors": [
        "Konstantin Schmidt",
        "Thomas Richter"
      ],
      "arxiv_id": "2510.23561v1",
      "summary": "First Order Motion Model is a generative model that animates human heads\nbased on very little motion information derived from keypoints. It is a\npromising solution for video communication because first it operates at very\nlow bitrate and second its computational complexity is moderate compared to\nother learning based video codecs. However, it has strong limitations by\ndesign. Since it generates facial animations by warping source-images, it fails\nto recreate videos with strong head movements. This works concentrates on one\nspecific kind of head movements, namely head rotations. We show that replacing\nthe Jacobian transformations in FOMM by a global rotation helps the system to\nperform better on items with head-rotations while saving 40% to 80% of bitrate\non P-frames. Moreover, we apply state-of-the-art normalization techniques to\nthe discriminator to stabilize the adversarial training which is essential for\ngenerating visually appealing videos. We evaluate the performance by the\nlearned metics LPIPS and DISTS to show the success our optimizations."
    },
    {
      "title": "A U-Net and Transformer Pipeline for Multilingual Image Translation",
      "authors": [
        "Siddharth Sahay",
        "Radhika Agarwal"
      ],
      "arxiv_id": "2510.23554v1",
      "summary": "This paper presents an end-to-end multilingual translation pipeline that\nintegrates a custom U-Net for text detection, the Tesseract engine for text\nrecognition, and a from-scratch sequence-to-sequence (Seq2Seq) Transformer for\nNeural Machine Translation (NMT). Our approach first utilizes a U-Net model,\ntrained on a synthetic dataset , to accurately segment and detect text regions\nfrom an image. These detected regions are then processed by Tesseract to\nextract the source text. This extracted text is fed into a custom Transformer\nmodel trained from scratch on a multilingual parallel corpus spanning 5\nlanguages. Unlike systems reliant on monolithic pre-trained models, our\narchitecture emphasizes full customization and adaptability. The system is\nevaluated on its text detection accuracy, text recognition quality, and\ntranslation performance via BLEU scores. The complete pipeline demonstrates\npromising results, validating the viability of a custom-built system for\ntranslating text directly from images."
    },
    {
      "title": "JanusCoder: Towards a Foundational Visual-Programmatic Interface for Code Intelligence",
      "authors": [
        "Qiushi Sun",
        "Jingyang Gong",
        "Yang Liu",
        "Qiaosheng Chen",
        "Lei Li",
        "Kai Chen",
        "Qipeng Guo",
        "Ben Kao",
        "Fei Yuan"
      ],
      "arxiv_id": "2510.23538v1",
      "summary": "The scope of neural code intelligence is rapidly expanding beyond text-based\nsource code to encompass the rich visual outputs that programs generate. This\nvisual dimension is critical for advanced applications like flexible content\ngeneration and precise, program-driven editing of visualizations. However,\nprogress has been impeded by the scarcity of high-quality multimodal code data,\na bottleneck stemming from challenges in synthesis and quality assessment. To\naddress these challenges, we make contributions from both a data and modeling\nperspective. We first introduce a complete synthesis toolkit that leverages\nreciprocal synergies between data modalities to efficiently produce a\nlarge-scale, high-quality corpus spanning from standard charts to complex\ninteractive web UIs and code-driven animations. Leveraging this toolkit, we\nconstruct JanusCode-800K, the largest multimodal code corpus to date. This\npowers the training of our models, JanusCoder and JanusCoderV, which establish\na visual-programmatic interface for generating code from textual instructions,\nvisual inputs, or a combination of both. Our unified model is a departure from\nexisting approaches that build specialized models for isolated tasks. Extensive\nexperiments on both text-centric and vision-centric coding tasks demonstrate\nthe superior performance of the JanusCoder series, with our 7B to 14B scale\nmodels approaching or even exceeding the performance of commercial models.\nFurthermore, extensive analysis provides key insights into harmonizing\nprogrammatic logic with its visual expression. Our code and checkpoints will\nare available at https://github.com/InternLM/JanusCoder."
    },
    {
      "title": "DPGLA: Bridging the Gap between Synthetic and Real Data for Unsupervised Domain Adaptation in 3D LiDAR Semantic Segmentation",
      "authors": [
        "Wanmeng Li",
        "Simone Mosco",
        "Daniel Fusaro",
        "Alberto Pretto"
      ],
      "arxiv_id": "2510.23525v1",
      "summary": "Annotating real-world LiDAR point clouds for use in intelligent autonomous\nsystems is costly. To overcome this limitation, self-training-based\nUnsupervised Domain Adaptation (UDA) has been widely used to improve point\ncloud semantic segmentation by leveraging synthetic point cloud data. However,\nwe argue that existing methods do not effectively utilize unlabeled data, as\nthey either rely on predefined or fixed confidence thresholds, resulting in\nsuboptimal performance. In this paper, we propose a Dynamic Pseudo-Label\nFiltering (DPLF) scheme to enhance real data utilization in point cloud UDA\nsemantic segmentation. Additionally, we design a simple and efficient\nPrior-Guided Data Augmentation Pipeline (PG-DAP) to mitigate domain shift\nbetween synthetic and real-world point clouds. Finally, we utilize data mixing\nconsistency loss to push the model to learn context-free representations. We\nimplement and thoroughly evaluate our approach through extensive comparisons\nwith state-of-the-art methods. Experiments on two challenging synthetic-to-real\npoint cloud semantic segmentation tasks demonstrate that our approach achieves\nsuperior performance. Ablation studies confirm the effectiveness of the DPLF\nand PG-DAP modules. We release the code of our method in this paper."
    },
    {
      "title": "Explicit Memory through Online 3D Gaussian Splatting Improves Class-Agnostic Video Segmentation",
      "authors": [
        "Anthony Opipari",
        "Aravindhan K Krishnan",
        "Shreekant Gayaka",
        "Min Sun",
        "Cheng-Hao Kuo",
        "Arnie Sen",
        "Odest Chadwicke Jenkins"
      ],
      "arxiv_id": "2510.23521v1",
      "summary": "Remembering where object segments were predicted in the past is useful for\nimproving the accuracy and consistency of class-agnostic video segmentation\nalgorithms. Existing video segmentation algorithms typically use either no\nobject-level memory (e.g. FastSAM) or they use implicit memories in the form of\nrecurrent neural network features (e.g. SAM2). In this paper, we augment both\ntypes of segmentation models using an explicit 3D memory and show that the\nresulting models have more accurate and consistent predictions. For this, we\ndevelop an online 3D Gaussian Splatting (3DGS) technique to store predicted\nobject-level segments generated throughout the duration of a video. Based on\nthis 3DGS representation, a set of fusion techniques are developed, named\nFastSAM-Splat and SAM2-Splat, that use the explicit 3DGS memory to improve\ntheir respective foundation models' predictions. Ablation experiments are used\nto validate the proposed techniques' design and hyperparameter settings.\nResults from both real-world and simulated benchmarking experiments show that\nmodels which use explicit 3D memories result in more accurate and consistent\npredictions than those which use no memory or only implicit neural network\nmemories. Project Page: https://topipari.com/projects/FastSAM-Splat/"
    },
    {
      "title": "FreeFuse: Multi-Subject LoRA Fusion via Auto Masking at Test Time",
      "authors": [
        "Yaoli Liu",
        "Yao-Xiang Ding",
        "Kun Zhou"
      ],
      "arxiv_id": "2510.23515v1",
      "summary": "This paper proposes FreeFuse, a novel training-free approach for\nmulti-subject text-to-image generation through automatic fusion of multiple\nsubject LoRAs. In contrast to existing methods that either focus on\npre-inference LoRA weight merging or rely on segmentation models and complex\ntechniques like noise blending to isolate LoRA outputs, our key insight is that\ncontext-aware dynamic subject masks can be automatically derived from\ncross-attention layer weights. Mathematical analysis shows that directly\napplying these masks to LoRA outputs during inference well approximates the\ncase where the subject LoRA is integrated into the diffusion model and used\nindividually for the masked region. FreeFuse demonstrates superior practicality\nand efficiency as it requires no additional training, no modification to LoRAs,\nno auxiliary models, and no user-defined prompt templates or region\nspecifications. Alternatively, it only requires users to provide the LoRA\nactivation words for seamless integration into standard workflows. Extensive\nexperiments validate that FreeFuse outperforms existing approaches in both\ngeneration quality and usability under the multi-subject generation tasks. The\nproject page is at https://future-item.github.io/FreeFuse/"
    },
    {
      "title": "Localising under the drape: proprioception in the era of distributed surgical robotic system",
      "authors": [
        "Martin Huber",
        "Nicola A. Cavalcanti",
        "Ayoob Davoodi",
        "Ruixuan Li",
        "Christopher E. Mower",
        "Fabio Carrillo",
        "Christoph J. Laux",
        "Francois Teyssere",
        "Thibault Chandanson",
        "Antoine Harlé",
        "Elie Saghbiny",
        "Mazda Farshad",
        "Guillaume Morel",
        "Emmanuel Vander Poorten",
        "Philipp Fürnstahl",
        "Sébastien Ourselin",
        "Christos Bergeles",
        "Tom Vercauteren"
      ],
      "arxiv_id": "2510.23512v1",
      "summary": "Despite their mechanical sophistication, surgical robots remain blind to\ntheir surroundings. This lack of spatial awareness causes collisions, system\nrecoveries, and workflow disruptions, issues that will intensify with the\nintroduction of distributed robots with independent interacting arms. Existing\ntracking systems rely on bulky infrared cameras and reflective markers,\nproviding only limited views of the surgical scene and adding hardware burden\nin crowded operating rooms. We present a marker-free proprioception method that\nenables precise localisation of surgical robots under their sterile draping\ndespite associated obstruction of visual cues. Our method solely relies on\nlightweight stereo-RGB cameras and novel transformer-based deep learning\nmodels. It builds on the largest multi-centre spatial robotic surgery dataset\nto date (1.4M self-annotated images from human cadaveric and preclinical in\nvivo studies). By tracking the entire robot and surgical scene, rather than\nindividual markers, our approach provides a holistic view robust to occlusions,\nsupporting surgical scene understanding and context-aware control. We\ndemonstrate an example of potential clinical benefits during in vivo breathing\ncompensation with access to tissue dynamics, unobservable under state of the\nart tracking, and accurately locate in multi-robot systems for future\nintelligent interaction. In addition, and compared with existing systems, our\nmethod eliminates markers and improves tracking visibility by 25%. To our\nknowledge, this is the first demonstration of marker-free proprioception for\nfully draped surgical robots, reducing setup complexity, enhancing safety, and\npaving the way toward modular and autonomous robotic surgery."
    },
    {
      "title": "Dexbotic: Open-Source Vision-Language-Action Toolbox",
      "authors": [
        "Bin Xie",
        "Erjin Zhou",
        "Fan Jia",
        "Hao Shi",
        "Haoqiang Fan",
        "Haowei Zhang",
        "Hebei Li",
        "Jianjian Sun",
        "Jie Bin",
        "Junwen Huang",
        "Kai Liu",
        "Kaixin Liu",
        "Kefan Gu",
        "Lin Sun",
        "Meng Zhang",
        "Peilong Han",
        "Ruitao Hao",
        "Ruitao Zhang",
        "Saike Huang",
        "Songhan Xie",
        "Tiancai Wang",
        "Tianle Liu",
        "Wenbin Tang",
        "Wenqi Zhu",
        "Yang Chen",
        "Yingfei Liu",
        "Yizhuang Zhou",
        "Yu Liu",
        "Yucheng Zhao",
        "Yunchao Ma",
        "Yunfei Wei",
        "Yuxiang Chen",
        "Ze Chen",
        "Zeming Li",
        "Zhao Wu",
        "Ziheng Zhang",
        "Ziming Liu",
        "Ziwei Yan",
        "Ziyu Zhang"
      ],
      "arxiv_id": "2510.23511v1",
      "summary": "In this paper, we present Dexbotic, an open-source Vision-Language-Action\n(VLA) model toolbox based on PyTorch. It aims to provide a one-stop VLA\nresearch service for professionals in the field of embodied intelligence. It\noffers a codebase that supports multiple mainstream VLA policies\nsimultaneously, allowing users to reproduce various VLA methods with just a\nsingle environment setup. The toolbox is experiment-centric, where the users\ncan quickly develop new VLA experiments by simply modifying the Exp script.\nMoreover, we provide much stronger pretrained models to achieve great\nperformance improvements for state-of-the-art VLA policies. Dexbotic will\ncontinuously update to include more of the latest pre-trained foundation models\nand cutting-edge VLA models in the industry."
    },
    {
      "title": "Deductive Chain-of-Thought Augmented Socially-aware Robot Navigation World Model",
      "authors": [
        "Weizheng Wang",
        "Obi Ike",
        "Soyun Choi",
        "Sungeun Hong",
        "Byung-Cheol Min"
      ],
      "arxiv_id": "2510.23509v1",
      "summary": "Social robot navigation increasingly relies on large language models for\nreasoning, path planning, and enabling movement in dynamic human spaces.\nHowever, relying solely on LLMs for planning often leads to unpredictable and\nunsafe behaviors, especially in dynamic human spaces, due to limited physical\ngrounding and weak logical consistency. In this work, we introduce NaviWM, a\nsocially-aware robot Navigation World Model that augments LLM reasoning with a\nstructured world model and a logic-driven chain-of-thought process. NaviWM\nconsists of two main components: (1) a spatial-temporal world model that\ncaptures the positions, velocities, and activities of agents in the\nenvironment, and (2) a deductive reasoning module that guides LLMs through a\nmulti-step, logic-based inference process. This integration enables the robot\nto generate navigation decisions that are both socially compliant and\nphysically safe, under well-defined constraints such as personal space,\ncollision avoidance, and timing. Unlike previous methods based on prompting or\nfine-tuning, NaviWM encodes social norms as first-order logic, enabling\ninterpretable and verifiable reasoning. Experiments show that NaviWM improves\nsuccess rates and reduces social violations, particularly in crowded\nenvironments. These results demonstrate the benefit of combining formal\nreasoning with LLMs for robust social navigation. Additional experimental\ndetails and demo videos for this work can be found at:\nhttps://sites.google.com/view/NaviWM."
    },
    {
      "title": "iPac: Incorporating Intra-image Patch Context into Graph Neural Networks for Medical Image Classification",
      "authors": [
        "Usama Zidan",
        "Mohamed Gaber",
        "Mohammed M. Abdelsamea"
      ],
      "arxiv_id": "2510.23504v1",
      "summary": "Graph neural networks have emerged as a promising paradigm for image\nprocessing, yet their performance in image classification tasks is hindered by\na limited consideration of the underlying structure and relationships among\nvisual entities. This work presents iPac, a novel approach to introduce a new\ngraph representation of images to enhance graph neural network image\nclassification by recognizing the importance of underlying structure and\nrelationships in medical image classification. iPac integrates various stages,\nincluding patch partitioning, feature extraction, clustering, graph\nconstruction, and graph-based learning, into a unified network to advance graph\nneural network image classification. By capturing relevant features and\norganising them into clusters, we construct a meaningful graph representation\nthat effectively encapsulates the semantics of the image. Experimental\nevaluation on diverse medical image datasets demonstrates the efficacy of iPac,\nexhibiting an average accuracy improvement of up to 5% over baseline methods.\nOur approach offers a versatile and generic solution for image classification,\nparticularly in the realm of medical images, by leveraging the graph\nrepresentation and accounting for the inherent structure and relationships\namong visual entities."
    },
    {
      "title": "VOLD: Reasoning Transfer from LLMs to Vision-Language Models via On-Policy Distillation",
      "authors": [
        "Walid Bousselham",
        "Hilde Kuehne",
        "Cordelia Schmid"
      ],
      "arxiv_id": "2510.23497v1",
      "summary": "Training vision-language models (VLMs) for complex reasoning remains a\nchallenging task, i.a. due to the scarcity of high-quality image-text reasoning\ndata. Conversely, text-based reasoning resources are abundant and scalable, but\nit is still an open question how to leveraging them for VLM reasoning. To\naddress this problem, we propose VOLD, a framework to transfer reasoning\ncapabilities from text-only teacher models to VLM student models. To this end,\nVOLD combines reinforcement learning via Group Relative Policy Optimization\n(GRPO) with on-policy distillation, which allows the student reasoning traces\nto be guided by the teacher model, resulting in a significant gain over using\nGRPO alone. We further show that a cold-start alignment is essential for an\neffective transfer during the online training phase in this scenario and that\nwithout sufficient distributional alignment between teacher and student,\non-policy distillation fails to provide meaningful guidance. We evaluate VOLD\nacross diverse benchmarks including MMMU-Pro, MathVision, MathVista, and\nLogicVista, showing that VOLD outperforms the baseline model significantly and\nimproves over the state of the art by a margin. Our ablation shows the\nimportance of a cold-start alignment via SFT for on-policy distillation with a\ntext-only teacher."
    },
    {
      "title": "COOPERA: Continual Open-Ended Human-Robot Assistance",
      "authors": [
        "Chenyang Ma",
        "Kai Lu",
        "Ruta Desai",
        "Xavier Puig",
        "Andrew Markham",
        "Niki Trigoni"
      ],
      "arxiv_id": "2510.23495v1",
      "summary": "To understand and collaborate with humans, robots must account for individual\nhuman traits, habits, and activities over time. However, most robotic\nassistants lack these abilities, as they primarily focus on predefined tasks in\nstructured environments and lack a human model to learn from. This work\nintroduces COOPERA, a novel framework for COntinual, OPen-Ended human-Robot\nAssistance, where simulated humans, driven by psychological traits and\nlong-term intentions, interact with robots in complex environments. By\nintegrating continuous human feedback, our framework, for the first time,\nenables the study of long-term, open-ended human-robot collaboration (HRC) in\ndifferent collaborative tasks across various time-scales. Within COOPERA, we\nintroduce a benchmark and an approach to personalize the robot's collaborative\nactions by learning human traits and context-dependent intents. Experiments\nvalidate the extent to which our simulated humans reflect realistic human\nbehaviors and demonstrate the value of inferring and personalizing to human\nintents for open-ended and long-term HRC. Project Page:\nhttps://dannymcy.github.io/coopera/"
    },
    {
      "title": "Yesnt: Are Diffusion Relighting Models Ready for Capture Stage Compositing? A Hybrid Alternative to Bridge the Gap",
      "authors": [
        "Elisabeth Jüttner",
        "Leona Krath",
        "Stefan Korfhage",
        "Hannah Dröge",
        "Matthias B. Hullin",
        "Markus Plack"
      ],
      "arxiv_id": "2510.23494v1",
      "summary": "Volumetric video relighting is essential for bringing captured performances\ninto virtual worlds, but current approaches struggle to deliver temporally\nstable, production-ready results. Diffusion-based intrinsic decomposition\nmethods show promise for single frames, yet suffer from stochastic noise and\ninstability when extended to sequences, while video diffusion models remain\nconstrained by memory and scale. We propose a hybrid relighting framework that\ncombines diffusion-derived material priors with temporal regularization and\nphysically motivated rendering. Our method aggregates multiple stochastic\nestimates of per-frame material properties into temporally consistent shading\ncomponents, using optical-flow-guided regularization. For indirect effects such\nas shadows and reflections, we extract a mesh proxy from Gaussian Opacity\nFields and render it within a standard graphics pipeline. Experiments on real\nand synthetic captures show that this hybrid strategy achieves substantially\nmore stable relighting across sequences than diffusion-only baselines, while\nscaling beyond the clip lengths feasible for video diffusion. These results\nindicate that hybrid approaches, which balance learned priors with physically\ngrounded constraints, are a practical step toward production-ready volumetric\nvideo relighting."
    },
    {
      "title": "T-REGS: Minimum Spanning Tree Regularization for Self-Supervised Learning",
      "authors": [
        "Julie Mordacq",
        "David Loiseaux",
        "Vicky Kalogeiton",
        "Steve Oudot"
      ],
      "arxiv_id": "2510.23484v1",
      "summary": "Self-supervised learning (SSL) has emerged as a powerful paradigm for\nlearning representations without labeled data, often by enforcing invariance to\ninput transformations such as rotations or blurring. Recent studies have\nhighlighted two pivotal properties for effective representations: (i) avoiding\ndimensional collapse-where the learned features occupy only a low-dimensional\nsubspace, and (ii) enhancing uniformity of the induced distribution. In this\nwork, we introduce T-REGS, a simple regularization framework for SSL based on\nthe length of the Minimum Spanning Tree (MST) over the learned representation.\nWe provide theoretical analysis demonstrating that T-REGS simultaneously\nmitigates dimensional collapse and promotes distribution uniformity on\narbitrary compact Riemannian manifolds. Several experiments on synthetic data\nand on classical SSL benchmarks validate the effectiveness of our approach at\nenhancing representation quality."
    },
    {
      "title": "On the Faithfulness of Visual Thinking: Measurement and Enhancement",
      "authors": [
        "Zujing Liu",
        "Junwen Pan",
        "Qi She",
        "Yuan Gao",
        "Guisong Xia"
      ],
      "arxiv_id": "2510.23482v1",
      "summary": "Recent large vision-language models (LVLMs) can generate vision-text\nmultimodal chain-of-thought (MCoT) traces after reinforcement fine-tuning\n(RFT). However, we observe that the visual information incorporated in MCoT is\noften inaccurate, though still yield correct answers, indicating a lack of\nfaithfulness in the MCoT reasoning process. We attribute this unfaithfulness to\nthe RL reward in RFT, which solely incentivizes the format of interleaved\nvision-text cues, ie, it encourages the model to incorporate visual information\ninto its text reasoning steps without considering the correctness of the visual\ninformation. In this paper, we first probe the faithfulness of MCoT by\nmeasuring how much the prediction changes when its visual and textual thoughts\nare intervened. Surprisingly, the model's predictions remain nearly unchanged\nunder visual intervention but change significantly under textual intervention,\nindicating that the visual evidence is largely ignored. To further analyze\nvisual information, we introduce an automated LVLM-based evaluation metric that\nquantifies the faithfulness of visual cues from two perspectives: reliability\nand sufficiency. Our evaluation reveals that the visual information in current\nMCoT traces is simultaneously unreliable and insufficient. To address this\nissue, we propose a novel MCoT learning strategy termed Sufficient-Component\nCause Model (SCCM) learning. This approach encourages the MCoT to generate\nsufficient yet minimal visual components that are independently capable of\nleading to correct answers. We note that the proposed SCCM is annotation-free\nand compatible with various RFT for MCoT in a plug-and-play manner. Empirical\nresults demonstrate that SCCM consistently improves the visual faithfulness\nacross a suite of fine-grained perception and reasoning benchmarks. Code is\navailable at https://github.com/EugeneLiu01/Faithful_Thinking_with_Image."
    },
    {
      "title": "MergeMix: A Unified Augmentation Paradigm for Visual and Multi-Modal Understanding",
      "authors": [
        "Xin Jin",
        "Siyuan Li",
        "Siyong Jian",
        "Kai Yu",
        "Huan Wang"
      ],
      "arxiv_id": "2510.23479v1",
      "summary": "Vision-language alignment in multi-modal large language models (MLLMs)\ntypically relies on supervised fine-tuning (SFT) or reinforcement learning\n(RL). SFT is stable and efficient but requires large-scale human annotations\nand cannot capture subtle preferences, while RL brings in a reward signal for\ntraining, but suffers from overhead and instability. These limitations\nhighlight a trade-off between scalability, robustness, and alignment quality.\nTo address this, we propose MergeMix, a training-time augmentation paradigm\nthat bridges SFT and RL. It first applies an attention-aware image mixing via\ntoken merge with more cluster representation and spatial context, and then\npresents a preference-driven training paradigm for MLLMs by building preference\npairs with mixed images and raw images, and optimizing via SimPO loss. As a\nmixup augmentation, MergeMix enhances attention consistency and efficiency,\nsurpassing other heuristic-based methods in classification. Extensive\nexperiments demonstrate that MergeMix achieves competitive accuracy with\nimproved efficiency, providing a scalable approach to preference alignment in\nclassification and MLLMs."
    },
    {
      "title": "UrbanIng-V2X: A Large-Scale Multi-Vehicle, Multi-Infrastructure Dataset Across Multiple Intersections for Cooperative Perception",
      "authors": [
        "Karthikeyan Chandra Sekaran",
        "Markus Geisler",
        "Dominik Rößle",
        "Adithya Mohan",
        "Daniel Cremers",
        "Wolfgang Utschick",
        "Michael Botsch",
        "Werner Huber",
        "Torsten Schön"
      ],
      "arxiv_id": "2510.23478v1",
      "summary": "Recent cooperative perception datasets have played a crucial role in\nadvancing smart mobility applications by enabling information exchange between\nintelligent agents, helping to overcome challenges such as occlusions and\nimproving overall scene understanding. While some existing real-world datasets\nincorporate both vehicle-to-vehicle and vehicle-to-infrastructure interactions,\nthey are typically limited to a single intersection or a single vehicle. A\ncomprehensive perception dataset featuring multiple connected vehicles and\ninfrastructure sensors across several intersections remains unavailable,\nlimiting the benchmarking of algorithms in diverse traffic environments.\nConsequently, overfitting can occur, and models may demonstrate misleadingly\nhigh performance due to similar intersection layouts and traffic participant\nbehavior. To address this gap, we introduce UrbanIng-V2X, the first\nlarge-scale, multi-modal dataset supporting cooperative perception involving\nvehicles and infrastructure sensors deployed across three urban intersections\nin Ingolstadt, Germany. UrbanIng-V2X consists of 34 temporally aligned and\nspatially calibrated sensor sequences, each lasting 20 seconds. All sequences\ncontain recordings from one of three intersections, involving two vehicles and\nup to three infrastructure-mounted sensor poles operating in coordinated\nscenarios. In total, UrbanIng-V2X provides data from 12 vehicle-mounted RGB\ncameras, 2 vehicle LiDARs, 17 infrastructure thermal cameras, and 12\ninfrastructure LiDARs. All sequences are annotated at a frequency of 10 Hz with\n3D bounding boxes spanning 13 object classes, resulting in approximately 712k\nannotated instances across the dataset. We provide comprehensive evaluations\nusing state-of-the-art cooperative perception methods and publicly release the\ncodebase, dataset, HD map, and a digital twin of the complete data collection\nenvironment."
    },
    {
      "title": "Video-Thinker: Sparking \"Thinking with Videos\" via Reinforcement Learning",
      "authors": [
        "Shijian Wang",
        "Jiarui Jin",
        "Xingjian Wang",
        "Linxin Song",
        "Runhao Fu",
        "Hecheng Wang",
        "Zongyuan Ge",
        "Yuan Lu",
        "Xuelian Cheng"
      ],
      "arxiv_id": "2510.23473v1",
      "summary": "Recent advances in image reasoning methods, particularly \"Thinking with\nImages\", have demonstrated remarkable success in Multimodal Large Language\nModels (MLLMs); however, this dynamic reasoning paradigm has not yet been\nextended to video reasoning tasks. In this paper, we propose Video-Thinker,\nwhich empowers MLLMs to think with videos by autonomously leveraging their\nintrinsic \"grounding\" and \"captioning\" capabilities to generate reasoning clues\nthroughout the inference process. To spark this capability, we construct\nVideo-Thinker-10K, a curated dataset featuring autonomous tool usage within\nchain-of-thought reasoning sequences. Our training strategy begins with\nSupervised Fine-Tuning (SFT) to learn the reasoning format, followed by Group\nRelative Policy Optimization (GRPO) to strengthen this reasoning capability.\nThrough this approach, Video-Thinker enables MLLMs to autonomously navigate\ngrounding and captioning tasks for video reasoning, eliminating the need for\nconstructing and calling external tools. Extensive experiments demonstrate that\nVideo-Thinker achieves significant performance gains on both in-domain tasks\nand challenging out-of-domain video reasoning benchmarks, including\nVideo-Holmes, CG-Bench-Reasoning, and VRBench. Our Video-Thinker-7B\nsubstantially outperforms existing baselines such as Video-R1 and establishes\nstate-of-the-art performance among 7B-sized MLLMs."
    },
    {
      "title": "Omni-Reward: Towards Generalist Omni-Modal Reward Modeling with Free-Form Preferences",
      "authors": [
        "Zhuoran Jin",
        "Hongbang Yuan",
        "Kejian Zhu",
        "Jiachun Li",
        "Pengfei Cao",
        "Yubo Chen",
        "Kang Liu",
        "Jun Zhao"
      ],
      "arxiv_id": "2510.23451v1",
      "summary": "Reward models (RMs) play a critical role in aligning AI behaviors with human\npreferences, yet they face two fundamental challenges: (1) Modality Imbalance,\nwhere most RMs are mainly focused on text and image modalities, offering\nlimited support for video, audio, and other modalities; and (2) Preference\nRigidity, where training on fixed binary preference pairs fails to capture the\ncomplexity and diversity of personalized preferences. To address the above\nchallenges, we propose Omni-Reward, a step toward generalist omni-modal reward\nmodeling with support for free-form preferences, consisting of: (1) Evaluation:\nWe introduce Omni-RewardBench, the first omni-modal RM benchmark with free-form\npreferences, covering nine tasks across five modalities including text, image,\nvideo, audio, and 3D; (2) Data: We construct Omni-RewardData, a multimodal\npreference dataset comprising 248K general preference pairs and 69K\ninstruction-tuning pairs for training generalist omni-modal RMs; (3) Model: We\npropose Omni-RewardModel, which includes both discriminative and generative\nRMs, and achieves strong performance on Omni-RewardBench as well as other\nwidely used reward modeling benchmarks."
    },
    {
      "title": "FRBNet: Revisiting Low-Light Vision through Frequency-Domain Radial Basis Network",
      "authors": [
        "Fangtong Sun",
        "Congyu Li",
        "Ke Yang",
        "Yuchen Pan",
        "Hanwen Yu",
        "Xichuan Zhang",
        "Yiying Li"
      ],
      "arxiv_id": "2510.23444v1",
      "summary": "Low-light vision remains a fundamental challenge in computer vision due to\nsevere illumination degradation, which significantly affects the performance of\ndownstream tasks such as detection and segmentation. While recent\nstate-of-the-art methods have improved performance through invariant feature\nlearning modules, they still fall short due to incomplete modeling of low-light\nconditions. Therefore, we revisit low-light image formation and extend the\nclassical Lambertian model to better characterize low-light conditions. By\nshifting our analysis to the frequency domain, we theoretically prove that the\nfrequency-domain channel ratio can be leveraged to extract\nillumination-invariant features via a structured filtering process. We then\npropose a novel and end-to-end trainable module named \\textbf{F}requency-domain\n\\textbf{R}adial \\textbf{B}asis \\textbf{Net}work (\\textbf{FRBNet}), which\nintegrates the frequency-domain channel ratio operation with a learnable\nfrequency domain filter for the overall illumination-invariant feature\nenhancement. As a plug-and-play module, FRBNet can be integrated into existing\nnetworks for low-light downstream tasks without modifying loss functions.\nExtensive experiments across various downstream tasks demonstrate that FRBNet\nachieves superior performance, including +2.2 mAP for dark object detection and\n+2.9 mIoU for nighttime segmentation. Code is available at:\nhttps://github.com/Sing-Forevet/FRBNet."
    },
    {
      "title": "CURVETE: Curriculum Learning and Progressive Self-supervised Training for Medical Image Classification",
      "authors": [
        "Asmaa Abbas",
        "Mohamed Gaber",
        "Mohammed M. Abdelsamea"
      ],
      "arxiv_id": "2510.23442v1",
      "summary": "Identifying high-quality and easily accessible annotated samples poses a\nnotable challenge in medical image analysis. Transfer learning techniques,\nleveraging pre-training data, offer a flexible solution to this issue. However,\nthe impact of fine-tuning diminishes when the dataset exhibits an irregular\ndistribution between classes. This paper introduces a novel deep convolutional\nneural network, named Curriculum Learning and Progressive Self-supervised\nTraining (CURVETE). CURVETE addresses challenges related to limited samples,\nenhances model generalisability, and improves overall classification\nperformance. It achieves this by employing a curriculum learning strategy based\non the granularity of sample decomposition during the training of generic\nunlabelled samples. Moreover, CURVETE address the challenge of irregular class\ndistribution by incorporating a class decomposition approach in the downstream\ntask. The proposed method undergoes evaluation on three distinct medical image\ndatasets: brain tumour, digital knee x-ray, and Mini-DDSM datasets. We\ninvestigate the classification performance using a generic self-supervised\nsample decomposition approach with and without the curriculum learning\ncomponent in training the pretext task. Experimental results demonstrate that\nthe CURVETE model achieves superior performance on test sets with an accuracy\nof 96.60% on the brain tumour dataset, 75.60% on the digital knee x-ray\ndataset, and 93.35% on the Mini-DDSM dataset using the baseline ResNet-50.\nFurthermore, with the baseline DenseNet-121, it achieved accuracies of 95.77%,\n80.36%, and 93.22% on the brain tumour, digital knee x-ray, and Mini-DDSM\ndatasets, respectively, outperforming other training strategies."
    },
    {
      "title": "MiCADangelo: Fine-Grained Reconstruction of Constrained CAD Models from 3D Scans",
      "authors": [
        "Ahmet Serdar Karadeniz",
        "Dimitrios Mallis",
        "Danila Rukhovich",
        "Kseniya Cherenkova",
        "Anis Kacem",
        "Djamila Aouada"
      ],
      "arxiv_id": "2510.23429v1",
      "summary": "Computer-Aided Design (CAD) plays a foundational role in modern manufacturing\nand product development, often requiring designers to modify or build upon\nexisting models. Converting 3D scans into parametric CAD representations--a\nprocess known as CAD reverse engineering--remains a significant challenge due\nto the high precision and structural complexity of CAD models. Existing deep\nlearning-based approaches typically fall into two categories: bottom-up,\ngeometry-driven methods, which often fail to produce fully parametric outputs,\nand top-down strategies, which tend to overlook fine-grained geometric details.\nMoreover, current methods neglect an essential aspect of CAD modeling:\nsketch-level constraints. In this work, we introduce a novel approach to CAD\nreverse engineering inspired by how human designers manually perform the task.\nOur method leverages multi-plane cross-sections to extract 2D patterns and\ncapture fine parametric details more effectively. It enables the reconstruction\nof detailed and editable CAD models, outperforming state-of-the-art methods\nand, for the first time, incorporating sketch constraints directly into the\nreconstruction process."
    },
    {
      "title": "Quality-controlled registration of urban MLS point clouds reducing drift effects by adaptive fragmentation",
      "authors": [
        "Marco Antonio Ortiz Rincon",
        "Yihui Yang",
        "Christoph Holst"
      ],
      "arxiv_id": "2510.23416v1",
      "summary": "This study presents a novel workflow designed to efficiently and accurately\nregister large-scale mobile laser scanning (MLS) point clouds to a target model\npoint cloud in urban street scenarios. This workflow specifically targets the\ncomplexities inherent in urban environments and adeptly addresses the\nchallenges of integrating point clouds that vary in density, noise\ncharacteristics, and occlusion scenarios, which are common in bustling city\ncenters. Two methodological advancements are introduced. First, the proposed\nSemi-sphere Check (SSC) preprocessing technique optimally fragments MLS\ntrajectory data by identifying mutually orthogonal planar surfaces. This step\nreduces the impact of MLS drift on the accuracy of the entire point cloud\nregistration, while ensuring sufficient geometric features within each fragment\nto avoid local minima. Second, we propose Planar Voxel-based Generalized\nIterative Closest Point (PV-GICP), a fine registration method that selectively\nutilizes planar surfaces within voxel partitions. This pre-process strategy not\nonly improves registration accuracy but also reduces computation time by more\nthan 50% compared to conventional point-to-plane ICP methods. Experiments on\nreal-world datasets from Munich's inner city demonstrate that our workflow\nachieves sub-0.01 m average registration accuracy while significantly\nshortening processing times. The results underscore the potential of the\nproposed methods to advance automated 3D urban modeling and updating, with\ndirect applications in urban planning, infrastructure management, and dynamic\ncity monitoring."
    },
    {
      "title": "Towards Generalisable Foundation Models for 3D Brain MRI",
      "authors": [
        "Moona Mazher",
        "Geoff J. M. Parker",
        "Daniel C. Alexander"
      ],
      "arxiv_id": "2510.23415v1",
      "summary": "Foundation models in artificial intelligence (AI) are transforming medical\nimaging by enabling general-purpose feature learning from large-scale,\nunlabeled datasets. In this work, we introduce BrainFound, a self-supervised\nfoundation model for brain MRI, built by extending DINO-v2, a vision\ntransformer originally designed for 2D natural images. BrainFound adapts\nDINO-v2 to model full 3D brain anatomy by incorporating volumetric information\nfrom sequential MRI slices, moving beyond conventional single-slice paradigms.\nIt supports both single- and multimodal inputs, enabling a broad range of\ndownstream tasks, including disease detection and image segmentation, while\ngeneralising across varied imaging protocols and clinical scenarios. We show\nthat BrainFound consistently outperforms existing self-supervised pretraining\nstrategies and supervised baselines, particularly in label-scarce and\nmulti-contrast settings. By integrating information from diverse 3D MRI\nmodalities (e.g., T1, T2, FLAIR), it enhances diagnostic accuracy and reduces\ndependency on extensive expert annotations. This flexibility makes BrainFound a\nscalable and practical solution for 3D neuroimaging pipelines, with significant\npotential for clinical deployment and research innovation."
    },
    {
      "title": "Symmetria: A Synthetic Dataset for Learning in Point Clouds",
      "authors": [
        "Ivan Sipiran",
        "Gustavo Santelices",
        "Lucas Oyarzún",
        "Andrea Ranieri",
        "Chiara Romanengo",
        "Silvia Biasotti",
        "Bianca Falcidieno"
      ],
      "arxiv_id": "2510.23414v1",
      "summary": "Unlike image or text domains that benefit from an abundance of large-scale\ndatasets, point cloud learning techniques frequently encounter limitations due\nto the scarcity of extensive datasets. To overcome this limitation, we present\nSymmetria, a formula-driven dataset that can be generated at any arbitrary\nscale. By construction, it ensures the absolute availability of precise ground\ntruth, promotes data-efficient experimentation by requiring fewer samples,\nenables broad generalization across diverse geometric settings, and offers easy\nextensibility to new tasks and modalities. Using the concept of symmetry, we\ncreate shapes with known structure and high variability, enabling neural\nnetworks to learn point cloud features effectively. Our results demonstrate\nthat this dataset is highly effective for point cloud self-supervised\npre-training, yielding models with strong performance in downstream tasks such\nas classification and segmentation, which also show good few-shot learning\ncapabilities. Additionally, our dataset can support fine-tuning models to\nclassify real-world objects, highlighting our approach's practical utility and\napplication. We also introduce a challenging task for symmetry detection and\nprovide a benchmark for baseline comparisons. A significant advantage of our\napproach is the public availability of the dataset, the accompanying code, and\nthe ability to generate very large collections, promoting further research and\ninnovation in point cloud learning."
    },
    {
      "title": "Color and Frequency Correction for Image Colorization",
      "authors": [
        "Yun Kai Zhuang"
      ],
      "arxiv_id": "2510.23399v1",
      "summary": "The project has carried out the re-optimization of image coloring in\naccordance with the existing Autocolorization direction model DDColor. For the\nexperiments on the existing weights of DDColor, we found that it has\nlimitations in some frequency bands and the color cast problem caused by\ninsufficient input dimension. We construct two optimization schemes and combine\nthem, which achieves the performance improvement of indicators such as PSNR and\nSSIM of the images after DDColor."
    },
    {
      "title": "VideoTG-R1: Boosting Video Temporal Grounding via Curriculum Reinforcement Learning on Reflected Boundary Annotations",
      "authors": [
        "Lu Dong",
        "Haiyu Zhang",
        "Han Lin",
        "Ziang Yan",
        "Xiangyu Zeng",
        "Hongjie Zhang",
        "Yifei Huang",
        "Yi Wang",
        "Zhen-Hua Ling",
        "Limin Wang",
        "Yali Wang"
      ],
      "arxiv_id": "2510.23397v1",
      "summary": "Video temporal grounding (VTG) aims to locate precise segments in videos\nbased on language queries, which is a fundamental challenge in video\nunderstanding. While recent Multimodal Large Language Models (MLLMs) have shown\npromise in tackling VTG through reinforcement learning (RL), they overlook the\nchallenges arising from both the quality and difficulty of training samples.\n(1) Partially annotated samples. Many samples contain relevant segments beyond\nthe annotated interval, introducing ambiguous supervision. (2) Hard-to-ground\nsamples. Samples with poor zero-shot performance produce consistently low and\nindistinguishable rewards during RL training, exhibiting no clear preference\namong multiple outputs and thus hindering learning efficiency. To address these\nchallenges, we propose VideoTG-R1, a novel curriculum RL framework with\nreflected boundary annotations, enabling data-efficient training. Specifically,\nwe propose a Boundary Reflection Agent that utilizes MLLMs to predict\nquery-relevant timestamps outside the annotated intervals, allowing us to\nidentify and filter out partially annotated samples, thereby reducing\nambiguity. Furthermore, we introduce a Difficulty Estimation Agent to assess\nthe training difficulty of each sample and design a curriculum RL strategy that\ndynamically masks the videos of hard-to-ground samples according to the\ntraining steps, easing the training difficulty and providing clearer\npreference. Experiments on the VTG and grounded VideoQA tasks demonstrate the\neffectiveness of our method. Remarkably, with only 10% of the training samples\nand 21% of the computational budget, VideoTG-R1 outperforms full-data\ncounterparts under both group relative policy optimization (GRPO) and\nsupervised fine-tuning (SFT). The code is available at\nhttps://github.com/ldong1111/VideoTG-R1."
    },
    {
      "title": "Full-Dynamics Real-Time Nonlinear Model Predictive Control of Heavy-Duty Hydraulic Manipulator for Trajectory Tracking Tasks",
      "authors": [
        "Alvaro Paz",
        "Mahdi Hejrati",
        "Pauli Mustalahti",
        "Jouni Mattila"
      ],
      "arxiv_id": "2510.23386v1",
      "summary": "Heavy-duty hydraulic manipulators (HHMs) operate under strict physical and\nsafety-critical constraints due to their large size, high power, and complex\nnonlinear dynamics. Ensuring that both joint-level and end-effector\ntrajectories remain compliant with actuator capabilities, such as force,\nvelocity, and position limits, is essential for safe and reliable operation,\nyet remains largely underexplored in real-time control frameworks. This paper\npresents a nonlinear model predictive control (NMPC) framework designed to\nguarantee constraint satisfaction throughout the full nonlinear dynamics of\nHHMs, while running at a real-time control frequency of 1 kHz. The proposed\nmethod combines a multiple-shooting strategy with real-time sensor feedback,\nand is supported by a robust low-level controller based on virtual\ndecomposition control (VDC) for precise joint tracking. Experimental validation\non a full-scale hydraulic manipulator shows that the NMPC framework not only\nenforces actuator constraints at the joint level, but also ensures\nconstraint-compliant motion in Cartesian space for the end-effector. These\nresults demonstrate the method's capability to deliver high-accuracy trajectory\ntracking while strictly respecting safety-critical limits, setting a new\nbenchmark for real-time control in large-scale hydraulic systems."
    },
    {
      "title": "An Efficient Remote Sensing Super Resolution Method Exploring Diffusion Priors and Multi-Modal Constraints for Crop Type Mapping",
      "authors": [
        "Songxi Yang",
        "Tang Sui",
        "Qunying Huang"
      ],
      "arxiv_id": "2510.23382v1",
      "summary": "Super resolution offers a way to harness medium even lowresolution but\nhistorically valuable remote sensing image archives. Generative models,\nespecially diffusion models, have recently been applied to remote sensing super\nresolution (RSSR), yet several challenges exist. First, diffusion models are\neffective but require expensive training from scratch resources and have slow\ninference speeds. Second, current methods have limited utilization of auxiliary\ninformation as real-world constraints to reconstruct scientifically realistic\nimages. Finally, most current methods lack evaluation on downstream tasks. In\nthis study, we present a efficient LSSR framework for RSSR, supported by a new\nmultimodal dataset of paired 30 m Landsat 8 and 10 m Sentinel 2 imagery. Built\non frozen pretrained Stable Diffusion, LSSR integrates crossmodal attention\nwith auxiliary knowledge (Digital Elevation Model, land cover, month) and\nSynthetic Aperture Radar guidance, enhanced by adapters and a tailored Fourier\nNDVI loss to balance spatial details and spectral fidelity. Extensive\nexperiments demonstrate that LSSR significantly improves crop boundary\ndelineation and recovery, achieving state-of-the-art performance with Peak\nSignal-to-Noise Ratio/Structural Similarity Index Measure of 32.63/0.84 (RGB)\nand 23.99/0.78 (IR), and the lowest NDVI Mean Squared Error (0.042), while\nmaintaining efficient inference (0.39 sec/image). Moreover, LSSR transfers\neffectively to NASA Harmonized Landsat and Sentinel (HLS) super resolution,\nyielding more reliable crop classification (F1: 0.86) than Sentinel-2 (F1:\n0.85). These results highlight the potential of RSSR to advance precision\nagriculture."
    },
    {
      "title": "PlanarTrack: A high-quality and challenging benchmark for large-scale planar object tracking",
      "authors": [
        "Yifan Jiao",
        "Xinran Liu",
        "Xiaoqiong Liu",
        "Xiaohui Yuan",
        "Heng Fan",
        "Libo Zhang"
      ],
      "arxiv_id": "2510.23368v1",
      "summary": "Planar tracking has drawn increasing interest owing to its key roles in\nrobotics and augmented reality. Despite recent great advancement, further\ndevelopment of planar tracking, particularly in the deep learning era, is\nlargely limited compared to generic tracking due to the lack of large-scale\nplatforms. To mitigate this, we propose PlanarTrack, a large-scale high-quality\nand challenging benchmark for planar tracking. Specifically, PlanarTrack\nconsists of 1,150 sequences with over 733K frames, including 1,000 short-term\nand 150 new long-term videos, which enables comprehensive evaluation of short-\nand long-term tracking performance. All videos in PlanarTrack are recorded in\nunconstrained conditions from the wild, which makes PlanarTrack challenging but\nmore realistic for real-world applications. To ensure high-quality annotations,\neach video frame is manually annotated by four corner points with multi-round\nmeticulous inspection and refinement. To enhance target diversity of\nPlanarTrack, we only capture a unique target in one sequence, which is\ndifferent from existing benchmarks. To our best knowledge, PlanarTrack is by\nfar the largest and most diverse and challenging dataset dedicated to planar\ntracking. To understand performance of existing methods on PlanarTrack and to\nprovide a comparison for future research, we evaluate 10 representative planar\ntrackers with extensive comparison and in-depth analysis. Our evaluation\nreveals that, unsurprisingly, the top planar trackers heavily degrade on the\nchallenging PlanarTrack, which indicates more efforts are required for\nimproving planar tracking. Our data and results will be released at\nhttps://github.com/HengLan/PlanarTrack"
    },
    {
      "title": "Interpretable Tile-Based Classification of Paclitaxel Exposure",
      "authors": [
        "Sean Fletcher",
        "Gabby Scott",
        "Douglas Currie",
        "Xin Zhang",
        "Yuqi Song",
        "Bruce MacLeod"
      ],
      "arxiv_id": "2510.23363v1",
      "summary": "Medical image analysis is central to drug discovery and preclinical\nevaluation, where scalable, objective readouts can accelerate decision-making.\nWe address classification of paclitaxel (Taxol) exposure from phase-contrast\nmicroscopy of C6 glioma cells -- a task with subtle dose differences that\nchallenges full-image models. We propose a simple tiling-and-aggregation\npipeline that operates on local patches and combines tile outputs into an image\nlabel, achieving state-of-the-art accuracy on the benchmark dataset and\nimproving over the published baseline by around 20 percentage points, with\ntrends confirmed by cross-validation. To understand why tiling is effective, we\nfurther apply Grad-CAM and Score-CAM and attention analyses, which enhance\nmodel interpretability and point toward robustness-oriented directions for\nfuture medical image research. Code is released to facilitate reproduction and\nextension."
    },
    {
      "title": "T-ESKF: Transformed Error-State Kalman Filter for Consistent Visual-Inertial Navigation",
      "authors": [
        "Chungeng Tian",
        "Ning Hao",
        "Fenghua He"
      ],
      "arxiv_id": "2510.23359v1",
      "summary": "This paper presents a novel approach to address the inconsistency problem\ncaused by observability mismatch in visual-inertial navigation systems (VINS).\nThe key idea involves applying a linear time-varying transformation to the\nerror-state within the Error-State Kalman Filter (ESKF). This transformation\nensures that \\textrr{the unobservable subspace of the transformed error-state\nsystem} becomes independent of the state, thereby preserving the correct\nobservability of the transformed system against variations in linearization\npoints. We introduce the Transformed ESKF (T-ESKF), a consistent VINS estimator\nthat performs state estimation using the transformed error-state system.\nFurthermore, we develop an efficient propagation technique to accelerate the\ncovariance propagation based on the transformation relationship between the\ntransition and accumulated matrices of T-ESKF and ESKF. We validate the\nproposed method through extensive simulations and experiments, demonstrating\nbetter (or competitive at least) performance compared to state-of-the-art\nmethods. The code is available at github.com/HITCSC/T-ESKF."
    },
    {
      "title": "Large language model-based task planning for service robots: A review",
      "authors": [
        "Shaohan Bian",
        "Ying Zhang",
        "Guohui Tian",
        "Zhiqiang Miao",
        "Edmond Q. Wu",
        "Simon X. Yang",
        "Changchun Hua"
      ],
      "arxiv_id": "2510.23357v1",
      "summary": "With the rapid advancement of large language models (LLMs) and robotics,\nservice robots are increasingly becoming an integral part of daily life,\noffering a wide range of services in complex environments. To deliver these\nservices intelligently and efficiently, robust and accurate task planning\ncapabilities are essential. This paper presents a comprehensive overview of the\nintegration of LLMs into service robotics, with a particular focus on their\nrole in enhancing robotic task planning. First, the development and\nfoundational techniques of LLMs, including pre-training, fine-tuning,\nretrieval-augmented generation (RAG), and prompt engineering, are reviewed. We\nthen explore the application of LLMs as the cognitive core-`brain'-of service\nrobots, discussing how LLMs contribute to improved autonomy and\ndecision-making. Furthermore, recent advancements in LLM-driven task planning\nacross various input modalities are analyzed, including text, visual, audio,\nand multimodal inputs. Finally, we summarize key challenges and limitations in\ncurrent research and propose future directions to advance the task planning\ncapabilities of service robots in complex, unstructured domestic environments.\nThis review aims to serve as a valuable reference for researchers and\npractitioners in the fields of artificial intelligence and robotics."
    },
    {
      "title": "Transferable Deep Reinforcement Learning for Cross-Domain Navigation: from Farmland to the Moon",
      "authors": [
        "Shreya Santra",
        "Thomas Robbins",
        "Kazuya Yoshida"
      ],
      "arxiv_id": "2510.23329v1",
      "summary": "Autonomous navigation in unstructured environments is essential for field and\nplanetary robotics, where robots must efficiently reach goals while avoiding\nobstacles under uncertain conditions. Conventional algorithmic approaches often\nrequire extensive environment-specific tuning, limiting scalability to new\ndomains. Deep Reinforcement Learning (DRL) provides a data-driven alternative,\nallowing robots to acquire navigation strategies through direct interactions\nwith their environment. This work investigates the feasibility of DRL policy\ngeneralization across visually and topographically distinct simulated domains,\nwhere policies are trained in terrestrial settings and validated in a zero-shot\nmanner in extraterrestrial environments. A 3D simulation of an agricultural\nrover is developed and trained using Proximal Policy Optimization (PPO) to\nachieve goal-directed navigation and obstacle avoidance in farmland settings.\nThe learned policy is then evaluated in a lunar-like simulated environment to\nassess transfer performance. The results indicate that policies trained under\nterrestrial conditions retain a high level of effectiveness, achieving close to\n50\\% success in lunar simulations without the need for additional training and\nfine-tuning. This underscores the potential of cross-domain DRL-based policy\ntransfer as a promising approach to developing adaptable and efficient\nautonomous navigation for future planetary exploration missions, with the added\nbenefit of minimizing retraining costs."
    },
    {
      "title": "Multitask Multimodal Self-Supervised Learning for Medical Images",
      "authors": [
        "Cristian Simionescu"
      ],
      "arxiv_id": "2510.23325v1",
      "summary": "This thesis works to address a pivotal challenge in medical image analysis:\nthe reliance on extensive labeled datasets, which are often limited due to the\nneed for expert annotation and constrained by privacy and legal issues. By\nfocusing on the development of self-supervised learning techniques and domain\nadaptation methods, this research aims to circumvent these limitations,\npresenting a novel approach to enhance the utility and efficacy of deep\nlearning in medical imaging.\n  Central to this thesis is the development of the Medformer, an innovative\nneural network architecture designed for multitask learning and deep domain\nadaptation. This model is adept at pre-training on diverse medical image\ndatasets, handling varying sizes and modalities, and is equipped with a dynamic\ninput-output adaptation mechanism. This enables efficient processing and\nintegration of a wide range of medical image types, from 2D X-rays to complex\n3D MRIs, thus mitigating the dependency on large labeled datasets.\n  Further, the thesis explores the current state of self-supervised learning in\nmedical imaging. It introduces novel pretext tasks that are capable of\nextracting meaningful information from unlabeled data, significantly advancing\nthe model's interpretative abilities. This approach is validated through\nrigorous experimentation, including the use of the MedMNIST dataset,\ndemonstrating the model's proficiency in learning generalized features\napplicable to various downstream tasks.\n  In summary, this thesis contributes to the advancement of medical image\nanalysis by offering a scalable, adaptable framework that reduces reliance on\nlabeled data. It paves the way for more accurate, efficient diagnostic tools in\nhealthcare, signifying a major step forward in the application of deep learning\nin medical imaging."
    },
    {
      "title": "ReconViaGen: Towards Accurate Multi-view 3D Object Reconstruction via Generation",
      "authors": [
        "Jiahao Chang",
        "Chongjie Ye",
        "Yushuang Wu",
        "Yuantao Chen",
        "Yidan Zhang",
        "Zhongjin Luo",
        "Chenghong Li",
        "Yihao Zhi",
        "Xiaoguang Han"
      ],
      "arxiv_id": "2510.23306v1",
      "summary": "Existing multi-view 3D object reconstruction methods heavily rely on\nsufficient overlap between input views, where occlusions and sparse coverage in\npractice frequently yield severe reconstruction incompleteness. Recent\nadvancements in diffusion-based 3D generative techniques offer the potential to\naddress these limitations by leveraging learned generative priors to\nhallucinate invisible parts of objects, thereby generating plausible 3D\nstructures. However, the stochastic nature of the inference process limits the\naccuracy and reliability of generation results, preventing existing\nreconstruction frameworks from integrating such 3D generative priors. In this\nwork, we comprehensively analyze the reasons why diffusion-based 3D generative\nmethods fail to achieve high consistency, including (a) the insufficiency in\nconstructing and leveraging cross-view connections when extracting multi-view\nimage features as conditions, and (b) the poor controllability of iterative\ndenoising during local detail generation, which easily leads to plausible but\ninconsistent fine geometric and texture details with inputs. Accordingly, we\npropose ReconViaGen to innovatively integrate reconstruction priors into the\ngenerative framework and devise several strategies that effectively address\nthese issues. Extensive experiments demonstrate that our ReconViaGen can\nreconstruct complete and accurate 3D models consistent with input views in both\nglobal structure and local details.Project page:\nhttps://jiahao620.github.io/reconviagen."
    },
    {
      "title": "MDReID: Modality-Decoupled Learning for Any-to-Any Multi-Modal Object Re-Identification",
      "authors": [
        "Yingying Feng",
        "Jie Li",
        "Jie Hu",
        "Yukang Zhang",
        "Lei Tan",
        "Jiayi Ji"
      ],
      "arxiv_id": "2510.23301v1",
      "summary": "Real-world object re-identification (ReID) systems often face modality\ninconsistencies, where query and gallery images come from different sensors\n(e.g., RGB, NIR, TIR). However, most existing methods assume modality-matched\nconditions, which limits their robustness and scalability in practical\napplications. To address this challenge, we propose MDReID, a flexible\nany-to-any image-level ReID framework designed to operate under both\nmodality-matched and modality-mismatched scenarios. MDReID builds on the\ninsight that modality information can be decomposed into two components:\nmodality-shared features that are predictable and transferable, and\nmodality-specific features that capture unique, modality-dependent\ncharacteristics. To effectively leverage this, MDReID introduces two key\ncomponents: the Modality Decoupling Learning (MDL) and Modality-aware Metric\nLearning (MML). Specifically, MDL explicitly decomposes modality features into\nmodality-shared and modality-specific representations, enabling effective\nretrieval in both modality-aligned and mismatched scenarios. MML, a tailored\nmetric learning strategy, further enforces orthogonality and complementarity\nbetween the two components to enhance discriminative power across modalities.\nExtensive experiments conducted on three challenging multi-modality ReID\nbenchmarks (RGBNT201, RGBNT100, MSVR310) consistently demonstrate the\nsuperiority of MDReID. Notably, MDReID achieves significant mAP improvements of\n9.8\\%, 3.0\\%, and 11.5\\% in general modality-matched scenarios, and average\ngains of 3.4\\%, 11.8\\%, and 10.9\\% in modality-mismatched scenarios,\nrespectively. The code is available at:\n\\textcolor{magenta}{https://github.com/stone96123/MDReID}."
    },
    {
      "title": "MMSD3.0: A Multi-Image Benchmark for Real-World Multimodal Sarcasm Detection",
      "authors": [
        "Haochen Zhao",
        "Yuyao Kong",
        "Yongxiu Xu",
        "Gaopeng Gou",
        "Hongbo Xu",
        "Yubin Wang",
        "Haoliang Zhang"
      ],
      "arxiv_id": "2510.23299v1",
      "summary": "Despite progress in multimodal sarcasm detection, existing datasets and\nmethods predominantly focus on single-image scenarios, overlooking potential\nsemantic and affective relations across multiple images. This leaves a gap in\nmodeling cases where sarcasm is triggered by multi-image cues in real-world\nsettings. To bridge this gap, we introduce MMSD3.0, a new benchmark composed\nentirely of multi-image samples curated from tweets and Amazon reviews. We\nfurther propose the Cross-Image Reasoning Model (CIRM), which performs targeted\ncross-image sequence modeling to capture latent inter-image connections. In\naddition, we introduce a relevance-guided, fine-grained cross-modal fusion\nmechanism based on text-image correspondence to reduce information loss during\nintegration. We establish a comprehensive suite of strong and representative\nbaselines and conduct extensive experiments, showing that MMSD3.0 is an\neffective and reliable benchmark that better reflects real-world conditions.\nMoreover, CIRM demonstrates state-of-the-art performance across MMSD, MMSD2.0\nand MMSD3.0, validating its effectiveness in both single-image and multi-image\nscenarios."
    },
    {
      "title": "Payload trajectory tracking control for aerial transportation systems with cable length online optimization",
      "authors": [
        "Hai Yu",
        "Zhichao Yang",
        "Wei He",
        "Jianda Han",
        "Yongchun Fang",
        "Xiao Liang"
      ],
      "arxiv_id": "2510.23296v1",
      "summary": "Cable-suspended aerial transportation systems are employed extensively across\nvarious industries. The capability to flexibly adjust the relative position\nbetween the multirotor and the payload has spurred growing interest in the\nsystem equipped with variable-length cable, promising broader application\npotential. Compared to systems with fixed-length cables, introducing the\nvariable-length cable adds a new degree of freedom. However, it also results in\nincreased nonlinearity and more complex dynamic coupling among the multirotor,\nthe cable and the payload, posing significant challenges in control design.\nThis paper introduces a backstepping control strategy tailored for aerial\ntransportation systems with variable-length cable, designed to precisely track\nthe payload trajectory while dynamically adjusting cable length. Then, a cable\nlength generator has been developed that achieves online optimization of the\ncable length while satisfying state constraints, thus balancing the\nmultirotor's motion and cable length changes without the need for manual\ntrajectory planning. The asymptotic stability of the closed-loop system is\nguaranteed through Lyapunov techniques and the growth restriction condition.\nFinally, simulation results confirm the efficacy of the proposed method in\nmanaging trajectory tracking and cable length adjustments effectively."
    },
    {
      "title": "Precise Time Delay Measurement and Compensation for Tightly Coupled Underwater SINS/piUSBL Navigation",
      "authors": [
        "Jin Huang",
        "Yingqiang Wang",
        "Haoda Li",
        "Zichen Liu",
        "Zhikun Wang",
        "Ying Chen"
      ],
      "arxiv_id": "2510.23286v1",
      "summary": "In multi-sensor systems, time synchronization between sensors is a\nsignificant challenge, and this issue is particularly pronounced in underwater\nintegrated navigation systems incorporating acoustic positioning. Such systems\nare highly susceptible to time delay, which can significantly degrade accuracy\nwhen measurement and fusion moments are misaligned. To address this challenge,\nthis paper introduces a tightly coupled navigation framework that integrates a\npassive inverted ultra-short baseline (piUSBL) acoustic positioning system, a\nstrapdown inertial navigation system (SINS), and a depth gauge under precise\ntime synchronization. The framework fuses azimuth and slant range from the\npiUSBL with depth data, thereby avoiding poor vertical-angle observability in\nplanar arrays. A novel delay measurement strategy is introduced, combining\nsynchronized timing with acoustic signal processing, which redefines\ndelay-traditionally an unobservable error-into a quantifiable parameter,\nenabling explicit estimation of both acoustic propagation and system processing\ndelays. Simulations and field experiments confirm the feasibility of the\nproposed method, with delay-compensated navigation reducing RMSE by 40.45% and\nmaximum error by 32.55%. These findings show that precise delay measurement and\ncompensation not only enhance underwater navigation accuracy but also establish\na generalizable framework for acoustic positioning integration, offering\nvaluable insights into time alignment and data fusion in latency-sensitive\nmulti-sensor systems."
    },
    {
      "title": "Adaptive Stochastic Coefficients for Accelerating Diffusion Sampling",
      "authors": [
        "Ruoyu Wang",
        "Beier Zhu",
        "Junzhi Li",
        "Liangyu Yuan",
        "Chi Zhang"
      ],
      "arxiv_id": "2510.23285v1",
      "summary": "Diffusion-based generative processes, formulated as differential equation\nsolving, frequently balance computational speed with sample quality. Our\ntheoretical investigation of ODE- and SDE-based solvers reveals complementary\nweaknesses: ODE solvers accumulate irreducible gradient error along\ndeterministic trajectories, while SDE methods suffer from amplified\ndiscretization errors when the step budget is limited. Building upon this\ninsight, we introduce AdaSDE, a novel single-step SDE solver that aims to unify\nthe efficiency of ODEs with the error resilience of SDEs. Specifically, we\nintroduce a single per-step learnable coefficient, estimated via lightweight\ndistillation, which dynamically regulates the error correction strength to\naccelerate diffusion sampling. Notably, our framework can be integrated with\nexisting solvers to enhance their capabilities. Extensive experiments\ndemonstrate state-of-the-art performance: at 5 NFE, AdaSDE achieves FID scores\nof 4.18 on CIFAR-10, 8.05 on FFHQ and 6.96 on LSUN Bedroom. Codes are available\nin https://github.com/WLU-wry02/AdaSDE."
    },
    {
      "title": "hYOLO Model: Enhancing Object Classification with Hierarchical Context in YOLOv8",
      "authors": [
        "Veska Tsenkova",
        "Peter Stanchev",
        "Daniel Petrov",
        "Deyan Lazarov"
      ],
      "arxiv_id": "2510.23278v1",
      "summary": "Current convolution neural network (CNN) classification methods are\npredominantly focused on flat classification which aims solely to identify a\nspecified object within an image. However, real-world objects often possess a\nnatural hierarchical organization that can significantly help classification\ntasks. Capturing the presence of relations between objects enables better\ncontextual understanding as well as control over the severity of mistakes.\nConsidering these aspects, this paper proposes an end-to-end hierarchical model\nfor image detection and classification built upon the YOLO model family. A\nnovel hierarchical architecture, a modified loss function, and a performance\nmetric tailored to the hierarchical nature of the model are introduced. The\nproposed model is trained and evaluated on two different hierarchical\ncategorizations of the same dataset: a systematic categorization that\ndisregards visual similarities between objects and a categorization accounting\nfor common visual characteristics across classes. The results illustrate how\nthe suggested methodology addresses the inherent hierarchical structure present\nin real-world objects, which conventional flat classification algorithms often\noverlook."
    },
    {
      "title": "Deep Active Inference with Diffusion Policy and Multiple Timescale World Model for Real-World Exploration and Navigation",
      "authors": [
        "Riko Yokozawa",
        "Kentaro Fujii",
        "Yuta Nomura",
        "Shingo Murata"
      ],
      "arxiv_id": "2510.23258v1",
      "summary": "Autonomous robotic navigation in real-world environments requires exploration\nto acquire environmental information as well as goal-directed navigation in\norder to reach specified targets. Active inference (AIF) based on the\nfree-energy principle provides a unified framework for these behaviors by\nminimizing the expected free energy (EFE), thereby combining epistemic and\nextrinsic values. To realize this practically, we propose a deep AIF framework\nthat integrates a diffusion policy as the policy model and a multiple timescale\nrecurrent state-space model (MTRSSM) as the world model. The diffusion policy\ngenerates diverse candidate actions while the MTRSSM predicts their\nlong-horizon consequences through latent imagination, enabling action selection\nthat minimizes EFE. Real-world navigation experiments demonstrated that our\nframework achieved higher success rates and fewer collisions compared with the\nbaselines, particularly in exploration-demanding scenarios. These results\nhighlight how AIF based on EFE minimization can unify exploration and\ngoal-directed navigation in real-world robotic settings."
    },
    {
      "title": "A Video Is Not Worth a Thousand Words",
      "authors": [
        "Sam Pollard",
        "Michael Wray"
      ],
      "arxiv_id": "2510.23253v1",
      "summary": "As we become increasingly dependent on vision language models (VLMs) to\nanswer questions about the world around us, there is a significant amount of\nresearch devoted to increasing both the difficulty of video question answering\n(VQA) datasets, and the context lengths of the models that they evaluate. The\nreliance on large language models as backbones has lead to concerns about\npotential text dominance, and the exploration of interactions between\nmodalities is underdeveloped. How do we measure whether we're heading in the\nright direction, with the complexity that multi-modal models introduce? We\npropose a joint method of computing both feature attributions and modality\nscores based on Shapley values, where both the features and modalities are\narbitrarily definable. Using these metrics, we compare $6$ VLM models of\nvarying context lengths on $4$ representative datasets, focusing on\nmultiple-choice VQA. In particular, we consider video frames and whole textual\nelements as equal features in the hierarchy, and the multiple-choice VQA task\nas an interaction between three modalities: video, question and answer. Our\nresults demonstrate a dependence on text and show that the multiple-choice VQA\ntask devolves into a model's ability to ignore distractors. Code available at\nhttps://github.com/sjpollard/a-video-is-not-worth-a-thousand-words."
    },
    {
      "title": "Progressive Growing of Patch Size: Curriculum Learning for Accelerated and Improved Medical Image Segmentation",
      "authors": [
        "Stefan M. Fischer",
        "Johannes Kiechle",
        "Laura Daza",
        "Lina Felsner",
        "Richard Osuala",
        "Daniel M. Lang",
        "Karim Lekadir",
        "Jan C. Peeken",
        "Julia A. Schnabel"
      ],
      "arxiv_id": "2510.23241v1",
      "summary": "In this work, we introduce Progressive Growing of Patch Size, an automatic\ncurriculum learning approach for 3D medical image segmentation. Our approach\nprogressively increases the patch size during model training, resulting in an\nimproved class balance for smaller patch sizes and accelerated convergence of\nthe training process. We evaluate our curriculum approach in two settings: a\nresource-efficient mode and a performance mode, both regarding Dice score\nperformance and computational costs across 15 diverse and popular 3D medical\nimage segmentation tasks. The resource-efficient mode matches the Dice score\nperformance of the conventional constant patch size sampling baseline with a\nnotable reduction in training time to only 44%. The performance mode improves\nupon constant patch size segmentation results, achieving a statistically\nsignificant relative mean performance gain of 1.28% in Dice Score. Remarkably,\nacross all 15 tasks, our proposed performance mode manages to surpass the\nconstant patch size baseline in Dice Score performance, while simultaneously\nreducing training time to only 89%. The benefits are particularly pronounced\nfor highly imbalanced tasks such as lesion segmentation tasks. Rigorous\nexperiments demonstrate that our performance mode not only improves mean\nsegmentation performance but also reduces performance variance, yielding more\ntrustworthy model comparison. Furthermore, our findings reveal that the\nproposed curriculum sampling is not tied to a specific architecture but\nrepresents a broadly applicable strategy that consistently boosts performance\nacross diverse segmentation models, including UNet, UNETR, and SwinUNETR. In\nsummary, we show that this simple yet elegant transformation on input data\nsubstantially improves both Dice Score performance and training runtime, while\nbeing compatible across diverse segmentation backbones."
    },
    {
      "title": "Autoregressive Styled Text Image Generation, but Make it Reliable",
      "authors": [
        "Carmine Zaccagnino",
        "Fabio Quattrini",
        "Vittorio Pippi",
        "Silvia Cascianelli",
        "Alessio Tonioni",
        "Rita Cucchiara"
      ],
      "arxiv_id": "2510.23240v1",
      "summary": "Generating faithful and readable styled text images (especially for Styled\nHandwritten Text generation - HTG) is an open problem with several possible\napplications across graphic design, document understanding, and image editing.\nA lot of research effort in this task is dedicated to developing strategies\nthat reproduce the stylistic characteristics of a given writer, with promising\nresults in terms of style fidelity and generalization achieved by the recently\nproposed Autoregressive Transformer paradigm for HTG. However, this method\nrequires additional inputs, lacks a proper stop mechanism, and might end up in\nrepetition loops, generating visual artifacts. In this work, we rethink the\nautoregressive formulation by framing HTG as a multimodal prompt-conditioned\ngeneration task, and tackle the content controllability issues by introducing\nspecial textual input tokens for better alignment with the visual ones.\nMoreover, we devise a Classifier-Free-Guidance-based strategy for our\nautoregressive model. Through extensive experimental validation, we demonstrate\nthat our approach, dubbed Eruku, compared to previous solutions requires fewer\ninputs, generalizes better to unseen styles, and follows more faithfully the\ntextual prompt, improving content adherence."
    },
    {
      "title": "Optimal Dimensioning of Elastic-Link Manipulators regarding Lifetime Estimation",
      "authors": [
        "Klaus Zauner",
        "Hubert Gattringer",
        "Andreas Mueller"
      ],
      "arxiv_id": "2510.23234v1",
      "summary": "Resourceful operation and design of robots is key for sustainable industrial\nautomation. This will be enabled by lightweight design along with time and\nenergy optimal control of robotic manipulators. Design and control of such\nsystems is intertwined as the control must take into account inherent\nmechanical compliance while the design must accommodate the dynamic\nrequirements demanded by the control. As basis for such design optimization, a\nmethod for estimating the lifetime of elastic link robotic manipulators is\npresented. This is applied to the geometry optimization of flexible serial\nmanipulators performing pick-and-place operations, where the optimization\nobjective is a combination of overall weight and vibration amplitudes. The\nlifetime estimation draws from a fatigue analysis combining the rainflow\ncounting algorithm and the method of critical cutting plane. Tresca hypothesis\nis used to formulate an equivalent stress, and linear damage accumulation is\nassumed. The final robot geometry is selected from a Pareto front as a tradeoff\nof lifetime and vibration characteristic. The method is illustrated for a three\ndegrees of freedom articulated robotic manipulator."
    },
    {
      "title": "Workspace Registration and Collision Detection for Industrial Robotics Applications",
      "authors": [
        "Klaus Zauner",
        "Josef El Dib",
        "Hubert Gattringer",
        "Andreas Mueller"
      ],
      "arxiv_id": "2510.23227v1",
      "summary": "Motion planning for robotic manipulators relies on precise knowledge of the\nenvironment in order to be able to define restricted areas and to take\ncollision objects into account. To capture the workspace, point clouds of the\nenvironment are acquired using various sensors. The collision objects are\nidentified by region growing segmentation and VCCS algorithm. Subsequently the\npoint clusters are approximated. The aim of the present paper is to compare\ndifferent sensors, to illustrate the process from detection to the finished\ncollision environment and to detect collisions between the robot and this\nenvironment."
    },
    {
      "title": "Through the Lens: Benchmarking Deepfake Detectors Against Moiré-Induced Distortions",
      "authors": [
        "Razaib Tariq",
        "Minji Heo",
        "Simon S. Woo",
        "Shahroz Tariq"
      ],
      "arxiv_id": "2510.23225v1",
      "summary": "Deepfake detection remains a pressing challenge, particularly in real-world\nsettings where smartphone-captured media from digital screens often introduces\nMoir\\'e artifacts that can distort detection outcomes. This study\nsystematically evaluates state-of-the-art (SOTA) deepfake detectors on\nMoir\\'e-affected videos, an issue that has received little attention. We\ncollected a dataset of 12,832 videos, spanning 35.64 hours, from the Celeb-DF,\nDFD, DFDC, UADFV, and FF++ datasets, capturing footage under diverse real-world\nconditions, including varying screens, smartphones, lighting setups, and camera\nangles. To further examine the influence of Moir\\'e patterns on deepfake\ndetection, we conducted additional experiments using our DeepMoir\\'eFake,\nreferred to as (DMF) dataset and two synthetic Moir\\'e generation techniques.\nAcross 15 top-performing detectors, our results show that Moir\\'e artifacts\ndegrade performance by as much as 25.4%, while synthetically generated Moir\\'e\npatterns lead to a 21.4% drop in accuracy. Surprisingly, demoir\\'eing methods,\nintended as a mitigation approach, instead worsened the problem, reducing\naccuracy by up to 17.2%. These findings underscore the urgent need for\ndetection models that can robustly handle Moir\\'e distortions alongside other\nrealworld challenges, such as compression, sharpening, and blurring. By\nintroducing the DMF dataset, we aim to drive future research toward closing the\ngap between controlled experiments and practical deepfake detection."
    },
    {
      "title": "Accurate and Scalable Multimodal Pathology Retrieval via Attentive Vision-Language Alignment",
      "authors": [
        "Hongyi Wang",
        "Zhengjie Zhu",
        "Jiabo Ma",
        "Fang Wang",
        "Yue Shi",
        "Bo Luo",
        "Jili Wang",
        "Qiuyu Cai",
        "Xiuming Zhang",
        "Yen-Wei Chen",
        "Lanfen Lin",
        "Hao Chen"
      ],
      "arxiv_id": "2510.23224v1",
      "summary": "The rapid digitization of histopathology slides has opened up new\npossibilities for computational tools in clinical and research workflows. Among\nthese, content-based slide retrieval stands out, enabling pathologists to\nidentify morphologically and semantically similar cases, thereby supporting\nprecise diagnoses, enhancing consistency across observers, and assisting\nexample-based education. However, effective retrieval of whole slide images\n(WSIs) remains challenging due to their gigapixel scale and the difficulty of\ncapturing subtle semantic differences amid abundant irrelevant content. To\novercome these challenges, we present PathSearch, a retrieval framework that\nunifies fine-grained attentive mosaic representations with global-wise slide\nembeddings aligned through vision-language contrastive learning. Trained on a\ncorpus of 6,926 slide-report pairs, PathSearch captures both fine-grained\nmorphological cues and high-level semantic patterns to enable accurate and\nflexible retrieval. The framework supports two key functionalities: (1)\nmosaic-based image-to-image retrieval, ensuring accurate and efficient slide\nresearch; and (2) multi-modal retrieval, where text queries can directly\nretrieve relevant slides. PathSearch was rigorously evaluated on four public\npathology datasets and three in-house cohorts, covering tasks including\nanatomical site retrieval, tumor subtyping, tumor vs. non-tumor discrimination,\nand grading across diverse organs such as breast, lung, kidney, liver, and\nstomach. External results show that PathSearch outperforms traditional\nimage-to-image retrieval frameworks. A multi-center reader study further\ndemonstrates that PathSearch improves diagnostic accuracy, boosts confidence,\nand enhances inter-observer agreement among pathologists in real clinical\nscenarios. These results establish PathSearch as a scalable and generalizable\nretrieval solution for digital pathology."
    },
    {
      "title": "VR-Drive: Viewpoint-Robust End-to-End Driving with Feed-Forward 3D Gaussian Splatting",
      "authors": [
        "Hoonhee Cho",
        "Jae-Young Kang",
        "Giwon Lee",
        "Hyemin Yang",
        "Heejun Park",
        "Seokwoo Jung",
        "Kuk-Jin Yoon"
      ],
      "arxiv_id": "2510.23205v1",
      "summary": "End-to-end autonomous driving (E2E-AD) has emerged as a promising paradigm\nthat unifies perception, prediction, and planning into a holistic, data-driven\nframework. However, achieving robustness to varying camera viewpoints, a common\nreal-world challenge due to diverse vehicle configurations, remains an open\nproblem. In this work, we propose VR-Drive, a novel E2E-AD framework that\naddresses viewpoint generalization by jointly learning 3D scene reconstruction\nas an auxiliary task to enable planning-aware view synthesis. Unlike prior\nscene-specific synthesis approaches, VR-Drive adopts a feed-forward inference\nstrategy that supports online training-time augmentation from sparse views\nwithout additional annotations. To further improve viewpoint consistency, we\nintroduce a viewpoint-mixed memory bank that facilitates temporal interaction\nacross multiple viewpoints and a viewpoint-consistent distillation strategy\nthat transfers knowledge from original to synthesized views. Trained in a fully\nend-to-end manner, VR-Drive effectively mitigates synthesis-induced noise and\nimproves planning under viewpoint shifts. In addition, we release a new\nbenchmark dataset to evaluate E2E-AD performance under novel camera viewpoints,\nenabling comprehensive analysis. Our results demonstrate that VR-Drive is a\nscalable and robust solution for the real-world deployment of end-to-end\nautonomous driving systems."
    },
    {
      "title": "If They Disagree, Will You Conform? Exploring the Role of Robots' Value Awareness in a Decision-Making Task",
      "authors": [
        "Giulia Pusceddu",
        "Giulio Antonio Abbo",
        "Francesco Rea",
        "Tony Belpaeme",
        "Alessandra Sciutti"
      ],
      "arxiv_id": "2510.23204v1",
      "summary": "This study investigates whether the opinions of robotic agents are more\nlikely to influence human decision-making when the robots are perceived as\nvalue-aware (i.e., when they display an understanding of human principles). We\ndesigned an experiment in which participants interacted with two Furhat robots\n- one programmed to be Value-Aware and the other Non-Value-Aware - during a\nlabeling task for images representing human values. Results indicate that\nparticipants distinguished the Value-Aware robot from the Non-Value-Aware one.\nAlthough their explicit choices did not indicate a clear preference for one\nrobot over the other, participants directed their gaze more toward the\nValue-Aware robot. Additionally, the Value-Aware robot was perceived as more\nloyal, suggesting that value awareness in a social robot may enhance its\nperceived commitment to the group. Finally, when both robots disagreed with the\nparticipant, conformity occurred in about one out of four trials, and\nparticipants took longer to confirm their responses, suggesting that two robots\nexpressing dissent may introduce hesitation in decision-making. On one hand,\nthis highlights the potential risk that robots, if misused, could manipulate\nusers for unethical purposes. On the other hand, it reinforces the idea that\nsocial robots might encourage reflection in ambiguous situations and help users\navoid scams."
    },
    {
      "title": "DecoDINO: 3D Human-Scene Contact Prediction with Semantic Classification",
      "authors": [
        "Lukas Bierling",
        "Davide Pasero",
        "Fleur Dolmans",
        "Helia Ghasemi",
        "Angelo Broere"
      ],
      "arxiv_id": "2510.23203v1",
      "summary": "Accurate vertex-level contact prediction between humans and surrounding\nobjects is a prerequisite for high fidelity human object interaction models\nused in robotics, AR/VR, and behavioral simulation. DECO was the first in the\nwild estimator for this task but is limited to binary contact maps and\nstruggles with soft surfaces, occlusions, children, and false-positive foot\ncontacts. We address these issues and introduce DecoDINO, a three-branch\nnetwork based on DECO's framework. It uses two DINOv2 ViT-g/14 encoders,\nclass-balanced loss weighting to reduce bias, and patch-level cross-attention\nfor improved local reasoning. Vertex features are finally passed through a\nlightweight MLP with a softmax to assign semantic contact labels. We also\ntested a vision-language model (VLM) to integrate text features, but the\nsimpler architecture performed better and was used instead. On the DAMON\nbenchmark, DecoDINO (i) raises the binary-contact F1 score by 7$\\%$, (ii)\nhalves the geodesic error, and (iii) augments predictions with object-level\nsemantic labels. Ablation studies show that LoRA fine-tuning and the dual\nencoders are key to these improvements. DecoDINO outperformed the challenge\nbaseline in both tasks of the DAMON Challenge. Our code is available at\nhttps://github.com/DavidePasero/deco/tree/main."
    },
    {
      "title": "Evaluation of Vision-LLMs in Surveillance Video",
      "authors": [
        "Pascal Benschop",
        "Cristian Meo",
        "Justin Dauwels",
        "Jelte P. Mense"
      ],
      "arxiv_id": "2510.23190v1",
      "summary": "The widespread use of cameras in our society has created an overwhelming\namount of video data, far exceeding the capacity for human monitoring. This\npresents a critical challenge for public safety and security, as the timely\ndetection of anomalous or criminal events is crucial for effective response and\nprevention. The ability for an embodied agent to recognize unexpected events is\nfundamentally tied to its capacity for spatial reasoning. This paper\ninvestigates the spatial reasoning of vision-language models (VLMs) by framing\nanomalous action recognition as a zero-shot, language-grounded task, addressing\nthe embodied perception challenge of interpreting dynamic 3D scenes from sparse\n2D video. Specifically, we investigate whether small, pre-trained vision--LLMs\ncan act as spatially-grounded, zero-shot anomaly detectors by converting video\ninto text descriptions and scoring labels via textual entailment. We evaluate\nfour open models on UCF-Crime and RWF-2000 under prompting and\nprivacy-preserving conditions. Few-shot exemplars can improve accuracy for some\nmodels, but may increase false positives, and privacy filters -- especially\nfull-body GAN transforms -- introduce inconsistencies that degrade accuracy.\nThese results chart where current vision--LLMs succeed (simple, spatially\nsalient events) and where they falter (noisy spatial cues, identity\nobfuscation). Looking forward, we outline concrete paths to strengthen spatial\ngrounding without task-specific training: structure-aware prompts, lightweight\nspatial memory across clips, scene-graph or 3D-pose priors during description,\nand privacy methods that preserve action-relevant geometry. This positions\nzero-shot, language-grounded pipelines as adaptable building blocks for\nembodied, real-world video understanding. Our implementation for evaluating\nVLMs is publicly available at:\nhttps://github.com/pascalbenschopTU/VLLM_AnomalyRecognition"
    },
    {
      "title": "Finding 3D Scene Analogies with Multimodal Foundation Models",
      "authors": [
        "Junho Kim",
        "Young Min Kim"
      ],
      "arxiv_id": "2510.23184v1",
      "summary": "Connecting current observations with prior experiences helps robots adapt and\nplan in new, unseen 3D environments. Recently, 3D scene analogies have been\nproposed to connect two 3D scenes, which are smooth maps that align scene\nregions with common spatial relationships. These maps enable detailed transfer\nof trajectories or waypoints, potentially supporting demonstration transfer for\nimitation learning or task plan transfer across scenes. However, existing\nmethods for the task require additional training and fixed object vocabularies.\nIn this work, we propose to use multimodal foundation models for finding 3D\nscene analogies in a zero-shot, open-vocabulary setting. Central to our\napproach is a hybrid neural representation of scenes that consists of a sparse\ngraph based on vision-language model features and a feature field derived from\n3D shape foundation models. 3D scene analogies are then found in a\ncoarse-to-fine manner, by first aligning the graph and refining the\ncorrespondence with feature fields. Our method can establish accurate\ncorrespondences between complex scenes, and we showcase applications in\ntrajectory and waypoint transfer."
    },
    {
      "title": "TARC: Time-Adaptive Robotic Control",
      "authors": [
        "Arnav Sukhija",
        "Lenart Treven",
        "Jin Cheng",
        "Florian Dörfler",
        "Stelian Coros",
        "Andreas Krause"
      ],
      "arxiv_id": "2510.23176v1",
      "summary": "Fixed-frequency control in robotics imposes a trade-off between the\nefficiency of low-frequency control and the robustness of high-frequency\ncontrol, a limitation not seen in adaptable biological systems. We address this\nwith a reinforcement learning approach in which policies jointly select control\nactions and their application durations, enabling robots to autonomously\nmodulate their control frequency in response to situational demands. We\nvalidate our method with zero-shot sim-to-real experiments on two distinct\nhardware platforms: a high-speed RC car and a quadrupedal robot. Our method\nmatches or outperforms fixed-frequency baselines in terms of rewards while\nsignificantly reducing the control frequency and exhibiting adaptive frequency\ncontrol under real-world conditions."
    },
    {
      "title": "AG-Fusion: adaptive gated multimodal fusion for 3d object detection in complex scenes",
      "authors": [
        "Sixian Liu",
        "Chen Xu",
        "Qiang Wang",
        "Donghai Shi",
        "Yiwen Li"
      ],
      "arxiv_id": "2510.23151v1",
      "summary": "Multimodal camera-LiDAR fusion technology has found extensive application in\n3D object detection, demonstrating encouraging performance. However, existing\nmethods exhibit significant performance degradation in challenging scenarios\ncharacterized by sensor degradation or environmental disturbances. We propose a\nnovel Adaptive Gated Fusion (AG-Fusion) approach that selectively integrates\ncross-modal knowledge by identifying reliable patterns for robust detection in\ncomplex scenes. Specifically, we first project features from each modality into\na unified BEV space and enhance them using a window-based attention mechanism.\nSubsequently, an adaptive gated fusion module based on cross-modal attention is\ndesigned to integrate these features into reliable BEV representations robust\nto challenging environments. Furthermore, we construct a new dataset named\nExcavator3D (E3D) focusing on challenging excavator operation scenarios to\nbenchmark performance in complex conditions. Our method not only achieves\ncompetitive performance on the standard KITTI dataset with 93.92% accuracy, but\nalso significantly outperforms the baseline by 24.88% on the challenging E3D\ndataset, demonstrating superior robustness to unreliable modal information in\ncomplex industrial scenes."
    },
    {
      "title": "Implicit Modeling for Transferability Estimation of Vision Foundation Models",
      "authors": [
        "Yaoyan Zheng",
        "Huiqun Wang",
        "Nan Zhou",
        "Di Huang"
      ],
      "arxiv_id": "2510.23145v1",
      "summary": "Transferability estimation identifies the best pre-trained models for\ndownstream tasks without incurring the high computational cost of full\nfine-tuning. This capability facilitates deployment and advances the\npre-training and fine-tuning paradigm. However, existing methods often struggle\nto accurately assess transferability for emerging pre-trained models with\ndiverse architectures, training strategies, and task alignments. In this work,\nwe propose Implicit Transferability Modeling (ITM), a novel framework that\nimplicitly models each model's intrinsic transferability, coupled with a\nDivide-and-Conquer Variational Approximation (DVA) strategy to efficiently\napproximate embedding space evolution. This design enables generalization\nacross a broader range of models and downstream tasks. Extensive experiments on\na comprehensive benchmark--spanning extensive training regimes and a wider\nvariety of model types--demonstrate that ITM consistently outperforms existing\nmethods in terms of stability, effectiveness, and efficiency."
    },
    {
      "title": "DQ3D: Depth-guided Query for Transformer-Based 3D Object Detection in Traffic Scenarios",
      "authors": [
        "Ziyu Wang",
        "Wenhao Li",
        "Ji Wu"
      ],
      "arxiv_id": "2510.23144v1",
      "summary": "3D object detection from multi-view images in traffic scenarios has garnered\nsignificant attention in recent years. Many existing approaches rely on object\nqueries that are generated from 3D reference points to localize objects.\nHowever, a limitation of these methods is that some reference points are often\nfar from the target object, which can lead to false positive detections. In\nthis paper, we propose a depth-guided query generator for 3D object detection\n(DQ3D) that leverages depth information and 2D detections to ensure that\nreference points are sampled from the surface or interior of the object.\nFurthermore, to address partially occluded objects in current frame, we\nintroduce a hybrid attention mechanism that fuses historical detection results\nwith depth-guided queries, thereby forming hybrid queries. Evaluation on the\nnuScenes dataset demonstrates that our method outperforms the baseline by 6.3\\%\nin terms of mean Average Precision (mAP) and 4.3\\% in the NuScenes Detection\nScore (NDS)."
    },
    {
      "title": "Fast Voxel-Wise Kinetic Modeling in Dynamic PET using a Physics-Informed CycleGAN",
      "authors": [
        "Christian Salomonsen",
        "Samuel Kuttner",
        "Michael Kampffmeyer",
        "Robert Jenssen",
        "Kristoffer Wickstrøm",
        "Jong Chul Ye",
        "Elisabeth Wetzer"
      ],
      "arxiv_id": "2510.23140v1",
      "summary": "Tracer kinetic modeling serves a vital role in diagnosis, treatment planning,\ntracer development and oncology, but burdens practitioners with complex and\ninvasive arterial input function estimation (AIF). We adopt a physics-informed\nCycleGAN showing promise in DCE-MRI quantification to dynamic PET\nquantification. Our experiments demonstrate sound AIF predictions and parameter\nmaps closely resembling the reference."
    },
    {
      "title": "Note on the Construction of Structure Tensor",
      "authors": [
        "Josef Bigun",
        "Fernado Alonso-Fernandez"
      ],
      "arxiv_id": "2510.23137v1",
      "summary": "This note presents a theoretical discussion of two structure tensor\nconstructions: one proposed by Bigun and Granlund 1987, and the other by\nGranlund and Knutsson 1995. At first glance, these approaches may appear quite\ndifferent--the former is implemented by averaging outer products of gradient\nfilter responses, while the latter constructs the tensor from weighted outer\nproducts of tune-in frequency vectors of quadrature filters. We argue that when\nboth constructions are viewed through the common lens of Total Least Squares\n(TLS) line fitting to the power spectrum, they can be reconciled to a large\nextent, and additional benefits emerge. From this perspective, the correction\nterm introduced in Granlund and Knutsson 1995 becomes unnecessary. Omitting it\nensures that the resulting tensor remains positive semi-definite, thereby\nsimplifying the interpretation of its eigenvalues. Furthermore, this\ninterpretation allows fitting more than a single 0rientation to the input by\nreinterpreting quadrature filter responses without relying on a structure\ntensor. It also removes the constraint that responses must originate strictly\nfrom quadrature filters, allowing the use of alternative filter types and\nnon-angular tessellations. These alternatives include Gabor filters--which,\nalthough not strictly quadrature, are still suitable for structure tensor\nconstruction--even when they tessellate the spectrum in a Cartesian fashion,\nprovided they are sufficiently concentrated."
    },
    {
      "title": "Combining High Level Scheduling and Low Level Control to Manage Fleets of Mobile Robots",
      "authors": [
        "Sabino Francesco Roselli",
        "Ze Zhang",
        "Knut Åkesson"
      ],
      "arxiv_id": "2510.23129v1",
      "summary": "The deployment of mobile robots for material handling in industrial\nenvironments requires scalable coordination of large fleets in dynamic\nsettings. This paper presents a two-layer framework that combines high-level\nscheduling with low-level control. Tasks are assigned and scheduled using the\ncompositional algorithm ComSat, which generates time-parameterized routes for\neach robot. These schedules are then used by a distributed Model Predictive\nControl (MPC) system in real time to compute local reference trajectories,\naccounting for static and dynamic obstacles. The approach ensures safe,\ncollision-free operation, and supports rapid rescheduling in response to\ndisruptions such as robot failures or environmental changes. We evaluate the\nmethod in simulated 2D environments with varying road capacities and traffic\nconditions, demonstrating high task completion rates and robust behavior even\nunder congestion. The modular structure of the framework allows for\ncomputational tractability and flexibility, making it suitable for deployment\nin complex, real-world industrial scenarios."
    },
    {
      "title": "DeepSalt: Bridging Laboratory and Satellite Spectra through Domain Adaptation and Knowledge Distillation for Large-Scale Soil Salinity Estimation",
      "authors": [
        "Rupasree Dey",
        "Abdul Matin",
        "Everett Lewark",
        "Tanjim Bin Faruk",
        "Andrei Bachinin",
        "Sam Leuthold",
        "M. Francesca Cotrufo",
        "Shrideep Pallickara",
        "Sangmi Lee Pallickara"
      ],
      "arxiv_id": "2510.23124v1",
      "summary": "Soil salinization poses a significant threat to both ecosystems and\nagriculture because it limits plants' ability to absorb water and, in doing so,\nreduces crop productivity. This phenomenon alters the soil's spectral\nproperties, creating a measurable relationship between salinity and light\nreflectance that enables remote monitoring. While laboratory spectroscopy\nprovides precise measurements, its reliance on in-situ sampling limits\nscalability to regional or global levels. Conversely, hyperspectral satellite\nimagery enables wide-area observation but lacks the fine-grained\ninterpretability of laboratory instruments. To bridge this gap, we introduce\nDeepSalt, a deep-learning-based spectral transfer framework that leverages\nknowledge distillation and a novel Spectral Adaptation Unit to transfer\nhigh-resolution spectral insights from laboratory-based spectroscopy to\nsatellite-based hyperspectral sensing. Our approach eliminates the need for\nextensive ground sampling while enabling accurate, large-scale salinity\nestimation, as demonstrated through comprehensive empirical benchmarks.\nDeepSalt achieves significant performance gains over methods without explicit\ndomain adaptation, underscoring the impact of the proposed Spectral Adaptation\nUnit and the knowledge distillation strategy. The model also effectively\ngeneralized to unseen geographic regions, explaining a substantial portion of\nthe salinity variance."
    },
    {
      "title": "Reliable Robotic Task Execution in the Face of Anomalies",
      "authors": [
        "Bharath Santhanam",
        "Alex Mitrevski",
        "Santosh Thoduka",
        "Sebastian Houben",
        "Teena Hassan"
      ],
      "arxiv_id": "2510.23121v1",
      "summary": "Learned robot policies have consistently been shown to be versatile, but they\ntypically have no built-in mechanism for handling the complexity of open\nenvironments, making them prone to execution failures; this implies that\ndeploying policies without the ability to recognise and react to failures may\nlead to unreliable and unsafe robot behaviour. In this paper, we present a\nframework that couples a learned policy with a method to detect visual\nanomalies during policy deployment and to perform recovery behaviours when\nnecessary, thereby aiming to prevent failures. Specifically, we train an\nanomaly detection model using data collected during nominal executions of a\ntrained policy. This model is then integrated into the online policy execution\nprocess, so that deviations from the nominal execution can trigger a\nthree-level sequential recovery process that consists of (i) pausing the\nexecution temporarily, (ii) performing a local perturbation of the robot's\nstate, and (iii) resetting the robot to a safe state by sampling from a learned\nexecution success model. We verify our proposed method in two different\nscenarios: (i) a door handle reaching task with a Kinova Gen3 arm using a\npolicy trained in simulation and transferred to the real robot, and (ii) an\nobject placing task with a UFactory xArm 6 using a general-purpose policy\nmodel. Our results show that integrating policy execution with anomaly\ndetection and recovery increases the execution success rate in environments\nwith various anomalies, such as trajectory deviations and adversarial human\ninterventions."
    },
    {
      "title": "OmniDexGrasp: Generalizable Dexterous Grasping via Foundation Model and Force Feedback",
      "authors": [
        "Yi-Lin Wei",
        "Zhexi Luo",
        "Yuhao Lin",
        "Mu Lin",
        "Zhizhao Liang",
        "Shuoyu Chen",
        "Wei-Shi Zheng"
      ],
      "arxiv_id": "2510.23119v1",
      "summary": "Enabling robots to dexterously grasp and manipulate objects based on human\ncommands is a promising direction in robotics. However, existing approaches are\nchallenging to generalize across diverse objects or tasks due to the limited\nscale of semantic dexterous grasp datasets. Foundation models offer a new way\nto enhance generalization, yet directly leveraging them to generate feasible\nrobotic actions remains challenging due to the gap between abstract model\nknowledge and physical robot execution. To address these challenges, we propose\nOmniDexGrasp, a generalizable framework that achieves omni-capabilities in user\nprompting, dexterous embodiment, and grasping tasks by combining foundation\nmodels with the transfer and control strategies. OmniDexGrasp integrates three\nkey modules: (i) foundation models are used to enhance generalization by\ngenerating human grasp images supporting omni-capability of user prompt and\ntask; (ii) a human-image-to-robot-action transfer strategy converts human\ndemonstrations into executable robot actions, enabling omni dexterous\nembodiment; (iii) force-aware adaptive grasp strategy ensures robust and stable\ngrasp execution. Experiments in simulation and on real robots validate the\neffectiveness of OmniDexGrasp on diverse user prompts, grasp task and dexterous\nhands, and further results show its extensibility to dexterous manipulation\ntasks."
    },
    {
      "title": "Task-Agnostic Fusion of Time Series and Imagery for Earth Observation",
      "authors": [
        "Gianfranco Basile",
        "Johannes Jakubik",
        "Benedikt Blumenstiel",
        "Thomas Brunschwiler",
        "Juan Bernabe Moreno"
      ],
      "arxiv_id": "2510.23118v1",
      "summary": "We propose a task-agnostic framework for multimodal fusion of time series and\nsingle timestamp images, enabling cross-modal generation and robust downstream\nperformance. Our approach explores deterministic and learned strategies for\ntime series quantization and then leverages a masked correlation learning\nobjective, aligning discrete image and time series tokens in a unified\nrepresentation space. Instantiated in the Earth observation domain, the\npretrained model generates consistent global temperature profiles from\nsatellite imagery and is validated through counterfactual experiments. Across\ndownstream tasks, our task-agnostic pretraining outperforms task-specific\nfusion by 6\\% in R$^2$ and 2\\% in RMSE on average, and exceeds baseline methods\nby 50\\% in R$^2$ and 12\\% in RMSE. Finally, we analyze gradient sensitivity\nacross modalities, providing insights into model robustness. Code, data, and\nweights will be released under a permissive license."
    },
    {
      "title": "Seeing Structural Failure Before it Happens: An Image-Based Physics-Informed Neural Network (PINN) for Spaghetti Bridge Load Prediction",
      "authors": [
        "Omer Jauhar Khan",
        "Sudais Khan",
        "Hafeez Anwar"
      ],
      "arxiv_id": "2510.23117v1",
      "summary": "Physics Informed Neural Networks (PINNs) are gaining attention for their\nability to embed physical laws into deep learning models, which is particularly\nuseful in structural engineering tasks with limited data. This paper aims to\nexplore the use of PINNs to predict the weight of small scale spaghetti\nbridges, a task relevant to understanding load limits and potential failure\nmodes in simplified structural models. Our proposed framework incorporates\nphysics-based constraints to the prediction model for improved performance. In\naddition to standard PINNs, we introduce a novel architecture named Physics\nInformed Kolmogorov Arnold Network (PIKAN), which blends universal function\napproximation theory with physical insights. The structural parameters provided\nas input to the model are collected either manually or through computer vision\nmethods. Our dataset includes 15 real bridges, augmented to 100 samples, and\nour best model achieves an $R^2$ score of 0.9603 and a mean absolute error\n(MAE) of 10.50 units. From applied perspective, we also provide a web based\ninterface for parameter entry and prediction. These results show that PINNs can\noffer reliable estimates of structural weight, even with limited data, and may\nhelp inform early stage failure analysis in lightweight bridge designs.\n  The complete data and code are available at\nhttps://github.com/OmerJauhar/PINNS-For-Spaghetti-Bridges."
    },
    {
      "title": "Residual Diffusion Bridge Model for Image Restoration",
      "authors": [
        "Hebaixu Wang",
        "Jing Zhang",
        "Haoyang Chen",
        "Haonan Guo",
        "Di Wang",
        "Jiayi Ma",
        "Bo Du"
      ],
      "arxiv_id": "2510.23116v1",
      "summary": "Diffusion bridge models establish probabilistic paths between arbitrary\npaired distributions and exhibit great potential for universal image\nrestoration. Most existing methods merely treat them as simple variants of\nstochastic interpolants, lacking a unified analytical perspective. Besides,\nthey indiscriminately reconstruct images through global noise injection and\nremoval, inevitably distorting undegraded regions due to imperfect\nreconstruction. To address these challenges, we propose the Residual Diffusion\nBridge Model (RDBM). Specifically, we theoretically reformulate the stochastic\ndifferential equations of generalized diffusion bridge and derive the\nanalytical formulas of its forward and reverse processes. Crucially, we\nleverage the residuals from given distributions to modulate the noise injection\nand removal, enabling adaptive restoration of degraded regions while preserving\nintact others. Moreover, we unravel the fundamental mathematical essence of\nexisting bridge models, all of which are special cases of RDBM and empirically\ndemonstrate the optimality of our proposed models. Extensive experiments are\nconducted to demonstrate the state-of-the-art performance of our method both\nqualitatively and quantitatively across diverse image restoration tasks. Code\nis publicly available at https://github.com/MiliLab/RDBM."
    },
    {
      "title": "An Automated Tape Laying System Employing a Uniaxial Force Control Device",
      "authors": [
        "Bernhard Rameder",
        "Hubert Gattringer",
        "Ronald Naderer",
        "Andreas Mueller"
      ],
      "arxiv_id": "2510.23109v1",
      "summary": "This paper deals with the design of a cost effective automated tape laying\nsystem (ATL system) with integrated uniaxial force control to ensure the\nnecessary compaction forces as well as with an accurate temperature control to\nguarantee the used tape being melted appropriate. It is crucial to control the\nsubstrate and the oncoming tape onto a specific temperature level to ensure an\noptimal consolidation between the different layers of the product. Therefore,\nit takes several process steps from the spooled tape on the coil until it is\nfinally tacked onto the desired mold. The different modules are divided into\nthe tape storage spool, a tape-guiding roller, a tape processing unit, a\nheating zone and the consolidation unit. Moreover, a special robot control\nconcept for testing the ATL system is presented. In contrast to many other\nsystems, with this approach, the tape laying device is spatially fixed and the\nshape is moved accordingly by the robot, which allows for handling of rather\ncompact and complex shapes. The functionality of the subsystems and the taping\nprocess itself was finally approved in experimental results using a carbon\nfiber reinforced HDPE tape."
    },
    {
      "title": "Revisiting Multimodal Positional Encoding in Vision-Language Models",
      "authors": [
        "Jie Huang",
        "Xuejing Liu",
        "Sibo Song",
        "Ruibing Hou",
        "Hong Chang",
        "Junyang Lin",
        "Shuai Bai"
      ],
      "arxiv_id": "2510.23095v1",
      "summary": "Multimodal position encoding is essential for vision-language models, yet\nthere has been little systematic investigation into multimodal position\nencoding. We conduct a comprehensive analysis of multimodal Rotary Positional\nEmbedding (RoPE) by examining its two core components: position design and\nfrequency allocation. Through extensive experiments, we identify three key\nguidelines: positional coherence, full frequency utilization, and preservation\nof textual priors-ensuring unambiguous layout, rich representation, and\nfaithful transfer from the pre-trained LLM. Based on these insights, we propose\nMulti-Head RoPE (MHRoPE) and MRoPE-Interleave (MRoPE-I), two simple and\nplug-and-play variants that require no architectural changes. Our methods\nconsistently outperform existing approaches across diverse benchmarks, with\nsignificant improvements in both general and fine-grained multimodal\nunderstanding. Code will be avaliable at\nhttps://github.com/JJJYmmm/Multimodal-RoPEs."
    },
    {
      "title": "EndoWave: Rational-Wavelet 4D Gaussian Splatting for Endoscopic Reconstruction",
      "authors": [
        "Taoyu Wu",
        "Yiyi Miao",
        "Jiaxin Guo",
        "Ziyan Chen",
        "Sihang Zhao",
        "Zhuoxiao Li",
        "Zhe Tang",
        "Baoru Huang",
        "Limin Yu"
      ],
      "arxiv_id": "2510.23087v1",
      "summary": "In robot-assisted minimally invasive surgery, accurate 3D reconstruction from\nendoscopic video is vital for downstream tasks and improved outcomes. However,\nendoscopic scenarios present unique challenges, including photometric\ninconsistencies, non-rigid tissue motion, and view-dependent highlights. Most\n3DGS-based methods that rely solely on appearance constraints for optimizing\n3DGS are often insufficient in this context, as these dynamic visual artifacts\ncan mislead the optimization process and lead to inaccurate reconstructions. To\naddress these limitations, we present EndoWave, a unified spatio-temporal\nGaussian Splatting framework by incorporating an optical flow-based geometric\nconstraint and a multi-resolution rational wavelet supervision. First, we adopt\na unified spatio-temporal Gaussian representation that directly optimizes\nprimitives in a 4D domain. Second, we propose a geometric constraint derived\nfrom optical flow to enhance temporal coherence and effectively constrain the\n3D structure of the scene. Third, we propose a multi-resolution rational\northogonal wavelet as a constraint, which can effectively separate the details\nof the endoscope and enhance the rendering performance. Extensive evaluations\non two real surgical datasets, EndoNeRF and StereoMIS, demonstrate that our\nmethod EndoWave achieves state-of-the-art reconstruction quality and visual\naccuracy compared to the baseline method."
    },
    {
      "title": "Breaking the Circle: An Autonomous Control-Switching Strategy for Stable Orographic Soaring in MAVs",
      "authors": [
        "Sunyou Hwang",
        "Christophe De Wagter",
        "Bart Remes",
        "Guido de Croon"
      ],
      "arxiv_id": "2510.23084v1",
      "summary": "Orographic soaring can significantly extend the endurance of micro aerial\nvehicles (MAVs), but circling behavior, arising from control conflicts between\nthe longitudinal and vertical axes, increases energy consumption and the risk\nof divergence. We propose a control switching method, named SAOS: Switched\nControl for Autonomous Orographic Soaring, which mitigates circling behavior by\nselectively controlling either the horizontal or vertical axis, effectively\ntransforming the system from underactuated to fully actuated during soaring.\nAdditionally, the angle of attack is incorporated into the INDI controller to\nimprove force estimation. Simulations with randomized initial positions and\nwind tunnel experiments on two MAVs demonstrate that the SAOS improves position\nconvergence, reduces throttle usage, and mitigates roll oscillations caused by\npitch-roll coupling. These improvements enhance energy efficiency and flight\nstability in constrained soaring environments."
    },
    {
      "title": "Strategies for Robust Deep Learning Based Deformable Registration",
      "authors": [
        "Joel Honkamaa",
        "Pekka Marttinen"
      ],
      "arxiv_id": "2510.23079v1",
      "summary": "Deep learning based deformable registration methods have become popular in\nrecent years. However, their ability to generalize beyond training data\ndistribution can be poor, significantly hindering their usability. LUMIR brain\nregistration challenge for Learn2Reg 2025 aims to advance the field by\nevaluating the performance of the registration on contrasts and modalities\ndifferent from those included in the training set. Here we describe our\nsubmission to the challenge, which proposes a very simple idea for\nsignificantly improving robustness by transforming the images into MIND feature\nspace before feeding them into the model. In addition, a special ensembling\nstrategy is proposed that shows a small but consistent improvement."
    },
    {
      "title": "Awakening Facial Emotional Expressions in Human-Robot",
      "authors": [
        "Yongtong Zhu",
        "Lei Li",
        "Iggy Qian",
        "WenBin Zhou",
        "Ye Yuan",
        "Qingdu Li",
        "Na Liu",
        "Jianwei Zhang"
      ],
      "arxiv_id": "2510.23059v1",
      "summary": "The facial expression generation capability of humanoid social robots is\ncritical for achieving natural and human-like interactions, playing a vital\nrole in enhancing the fluidity of human-robot interactions and the accuracy of\nemotional expression. Currently, facial expression generation in humanoid\nsocial robots still relies on pre-programmed behavioral patterns, which are\nmanually coded at high human and time costs. To enable humanoid robots to\nautonomously acquire generalized expressive capabilities, they need to develop\nthe ability to learn human-like expressions through self-training. To address\nthis challenge, we have designed a highly biomimetic robotic face with\nphysical-electronic animated facial units and developed an end-to-end learning\nframework based on KAN (Kolmogorov-Arnold Network) and attention mechanisms.\nUnlike previous humanoid social robots, we have also meticulously designed an\nautomated data collection system based on expert strategies of facial motion\nprimitives to construct the dataset. Notably, to the best of our knowledge,\nthis is the first open-source facial dataset for humanoid social robots.\nComprehensive evaluations indicate that our approach achieves accurate and\ndiverse facial mimicry across different test subjects."
    },
    {
      "title": "Seq-DeepIPC: Sequential Sensing for End-to-End Control in Legged Robot Navigation",
      "authors": [
        "Oskar Natan",
        "Jun Miura"
      ],
      "arxiv_id": "2510.23057v1",
      "summary": "We present Seq-DeepIPC, a sequential end-to-end perception-to-control model\nfor legged robot navigation in realworld environments. Seq-DeepIPC advances\nintelligent sensing for autonomous legged navigation by tightly integrating\nmulti-modal perception (RGB-D + GNSS) with temporal fusion and control. The\nmodel jointly predicts semantic segmentation and depth estimation, giving\nricher spatial features for planning and control. For efficient deployment on\nedge devices, we use EfficientNet-B0 as the encoder, reducing computation while\nmaintaining accuracy. Heading estimation is simplified by removing the noisy\nIMU and instead computing the bearing angle directly from consecutive GNSS\npositions. We collected a larger and more diverse dataset that includes both\nroad and grass terrains, and validated Seq-DeepIPC on a robot dog. Comparative\nand ablation studies show that sequential inputs improve perception and control\nin our models, while other baselines do not benefit. Seq-DeepIPC achieves\ncompetitive or better results with reasonable model size; although GNSS-only\nheading is less reliable near tall buildings, it is robust in open areas.\nOverall, Seq-DeepIPC extends end-to-end navigation beyond wheeled robots to\nmore versatile and temporally-aware systems. To support future research, we\nwill release the codes to our GitHub repository at\nhttps://github.com/oskarnatan/Seq-DeepIPC."
    },
    {
      "title": "HieraMamba: Video Temporal Grounding via Hierarchical Anchor-Mamba Pooling",
      "authors": [
        "Joungbin An",
        "Kristen Grauman"
      ],
      "arxiv_id": "2510.23043v1",
      "summary": "Video temporal grounding, the task of localizing the start and end times of a\nnatural language query in untrimmed video, requires capturing both global\ncontext and fine-grained temporal detail. This challenge is particularly\npronounced in long videos, where existing methods often compromise temporal\nfidelity by over-downsampling or relying on fixed windows. We present\nHieraMamba, a hierarchical architecture that preserves temporal structure and\nsemantic richness across scales. At its core are Anchor-MambaPooling (AMP)\nblocks, which utilize Mamba's selective scanning to produce compact anchor\ntokens that summarize video content at multiple granularities. Two\ncomplementary objectives, anchor-conditioned and segment-pooled contrastive\nlosses, encourage anchors to retain local detail while remaining globally\ndiscriminative. HieraMamba sets a new state-of-the-art on Ego4D-NLQ, MAD, and\nTACoS, demonstrating precise, temporally faithful localization in long,\nuntrimmed videos."
    },
    {
      "title": "Nested AutoRegressive Models",
      "authors": [
        "Hongyu Wu",
        "Xuhui Fan",
        "Zhangkai Wu",
        "Longbing Cao"
      ],
      "arxiv_id": "2510.23028v1",
      "summary": "AutoRegressive (AR) models have demonstrated competitive performance in image\ngeneration, achieving results comparable to those of diffusion models. However,\ntheir token-by-token image generation mechanism remains computationally\nintensive and existing solutions such as VAR often lead to limited sample\ndiversity. In this work, we propose a Nested AutoRegressive~(NestAR) model,\nwhich proposes nested AutoRegressive architectures in generating images. NestAR\ndesigns multi-scale modules in a hierarchical order. These different scaled\nmodules are constructed in an AR architecture, where one larger-scale module is\nconditioned on outputs from its previous smaller-scale module. Within each\nmodule, NestAR uses another AR structure to generate ``patches'' of tokens. The\nproposed nested AR architecture reduces the overall complexity from\n$\\mathcal{O}(n)$ to $\\mathcal{O}(\\log n)$ in generating $n$ image tokens, as\nwell as increases image diversities. NestAR further incorporates flow matching\nloss to use continuous tokens, and develops objectives to coordinate these\nmulti-scale modules in model training. NestAR achieves competitive image\ngeneration performance while significantly lowering computational cost."
    },
    {
      "title": "Mixed Density Diffuser: Efficient Planning with Non-uniform Temporal Resolution",
      "authors": [
        "Crimson Stambaugh",
        "Rajesh P. N. Rao"
      ],
      "arxiv_id": "2510.23026v1",
      "summary": "Recent studies demonstrate that diffusion planners benefit from sparse-step\nplanning over single-step planning. Training models to skip steps in their\ntrajectories helps capture long-term dependencies without additional or memory\ncomputational cost. However, predicting excessively sparse plans degrades\nperformance. We hypothesize this temporal density threshold is non-uniform\nacross a temporal horizon and that certain parts of a planned trajectory should\nbe more densely planned. We propose Mixed Density Diffuser (MDD), a diffusion\nplanner where the densities throughout the horizon are tunable hyperparameters.\nMDD achieves a new SOTA across the Maze2D, Franka Kitchen, and Antmaze D4RL\ntask domains."
    },
    {
      "title": "UniAIDet: A Unified and Universal Benchmark for AI-Generated Image Content Detection and Localization",
      "authors": [
        "Huixuan Zhang",
        "Xiaojun Wan"
      ],
      "arxiv_id": "2510.23023v1",
      "summary": "With the rapid proliferation of image generative models, the authenticity of\ndigital images has become a significant concern. While existing studies have\nproposed various methods for detecting AI-generated content, current benchmarks\nare limited in their coverage of diverse generative models and image\ncategories, often overlooking end-to-end image editing and artistic images. To\naddress these limitations, we introduce UniAIDet, a unified and comprehensive\nbenchmark that includes both photographic and artistic images. UniAIDet covers\na wide range of generative models, including text-to-image, image-to-image,\nimage inpainting, image editing, and deepfake models. Using UniAIDet, we\nconduct a comprehensive evaluation of various detection methods and answer\nthree key research questions regarding generalization capability and the\nrelation between detection and localization. Our benchmark and analysis provide\na robust foundation for future research."
    },
    {
      "title": "Planning Oriented Integrated Sensing and Communication",
      "authors": [
        "Xibin Jin",
        "Guoliang Li",
        "Shuai Wang",
        "Fan Liu",
        "Miaowen Wen",
        "Huseyin Arslan",
        "Derrick Wing Kwan Ng",
        "Chengzhong Xu"
      ],
      "arxiv_id": "2510.23021v1",
      "summary": "Integrated sensing and communication (ISAC) enables simultaneous\nlocalization, environment perception, and data exchange for connected\nautonomous vehicles. However, most existing ISAC designs prioritize sensing\naccuracy and communication throughput, treating all targets uniformly and\noverlooking the impact of critical obstacles on motion efficiency. To overcome\nthis limitation, we propose a planning-oriented ISAC (PISAC) framework that\nreduces the sensing uncertainty of planning-bottleneck obstacles and expands\nthe safe navigable path for the ego-vehicle, thereby bridging the gap between\nphysical-layer optimization and motion-level planning. The core of PISAC lies\nin deriving a closed-form safety bound that explicitly links ISAC transmit\npower to sensing uncertainty, based on the Cram\\'er-Rao Bound and occupancy\ninflation principles. Using this model, we formulate a bilevel power allocation\nand motion planning (PAMP) problem, where the inner layer optimizes the ISAC\nbeam power distribution and the outer layer computes a collision-free\ntrajectory under uncertainty-aware safety constraints. Comprehensive\nsimulations in high-fidelity urban driving environments demonstrate that PISAC\nachieves up to 40% higher success rates and over 5% shorter traversal times\nthan existing ISAC-based and communication-oriented benchmarks, validating its\neffectiveness in enhancing both safety and efficiency."
    },
    {
      "title": "M$^{3}$T2IBench: A Large-Scale Multi-Category, Multi-Instance, Multi-Relation Text-to-Image Benchmark",
      "authors": [
        "Huixuan Zhang",
        "Xiaojun Wan"
      ],
      "arxiv_id": "2510.23020v1",
      "summary": "Text-to-image models are known to struggle with generating images that\nperfectly align with textual prompts. Several previous studies have focused on\nevaluating image-text alignment in text-to-image generation. However, these\nevaluations either address overly simple scenarios, especially overlooking the\ndifficulty of prompts with multiple different instances belonging to the same\ncategory, or they introduce metrics that do not correlate well with human\nevaluation. In this study, we introduce M$^3$T2IBench, a large-scale,\nmulti-category, multi-instance, multi-relation along with an\nobject-detection-based evaluation metric, $AlignScore$, which aligns closely\nwith human evaluation. Our findings reveal that current open-source\ntext-to-image models perform poorly on this challenging benchmark.\nAdditionally, we propose the Revise-Then-Enforce approach to enhance image-text\nalignment. This training-free post-editing method demonstrates improvements in\nimage-text alignment across a broad range of diffusion models. \\footnote{Our\ncode and data has been released in supplementary material and will be made\npublicly available after the paper is accepted.}"
    },
    {
      "title": "ManiDP: Manipulability-Aware Diffusion Policy for Posture-Dependent Bimanual Manipulation",
      "authors": [
        "Zhuo Li",
        "Junjia Liu",
        "Dianxi Li",
        "Tao Teng",
        "Miao Li",
        "Sylvain Calinon",
        "Darwin Caldwell",
        "Fei Chen"
      ],
      "arxiv_id": "2510.23016v1",
      "summary": "Recent work has demonstrated the potential of diffusion models in robot\nbimanual skill learning. However, existing methods ignore the learning of\nposture-dependent task features, which are crucial for adapting dual-arm\nconfigurations to meet specific force and velocity requirements in dexterous\nbimanual manipulation. To address this limitation, we propose\nManipulability-Aware Diffusion Policy (ManiDP), a novel imitation learning\nmethod that not only generates plausible bimanual trajectories, but also\noptimizes dual-arm configurations to better satisfy posture-dependent task\nrequirements. ManiDP achieves this by extracting bimanual manipulability from\nexpert demonstrations and encoding the encapsulated posture features using\nRiemannian-based probabilistic models. These encoded posture features are then\nincorporated into a conditional diffusion process to guide the generation of\ntask-compatible bimanual motion sequences. We evaluate ManiDP on six real-world\nbimanual tasks, where the experimental results demonstrate a 39.33$\\%$ increase\nin average manipulation success rate and a 0.45 improvement in task\ncompatibility compared to baseline methods. This work highlights the importance\nof integrating posture-relevant robotic priors into bimanual skill diffusion to\nenable human-like adaptability and dexterity."
    },
    {
      "title": "UGAE: Unified Geometry and Attribute Enhancement for G-PCC Compressed Point Clouds",
      "authors": [
        "Pan Zhao",
        "Hui Yuan",
        "Chongzhen Tian",
        "Tian Guo",
        "Raouf Hamzaoui",
        "Zhigeng Pan"
      ],
      "arxiv_id": "2510.23009v1",
      "summary": "Lossy compression of point clouds reduces storage and transmission costs;\nhowever, it inevitably leads to irreversible distortion in geometry structure\nand attribute information. To address these issues, we propose a unified\ngeometry and attribute enhancement (UGAE) framework, which consists of three\ncore components: post-geometry enhancement (PoGE), pre-attribute enhancement\n(PAE), and post-attribute enhancement (PoAE). In PoGE, a Transformer-based\nsparse convolutional U-Net is used to reconstruct the geometry structure with\nhigh precision by predicting voxel occupancy probabilities. Building on the\nrefined geometry structure, PAE introduces an innovative enhanced\ngeometry-guided recoloring strategy, which uses a detail-aware K-Nearest\nNeighbors (DA-KNN) method to achieve accurate recoloring and effectively\npreserve high-frequency details before attribute compression. Finally, at the\ndecoder side, PoAE uses an attribute residual prediction network with a\nweighted mean squared error (W-MSE) loss to enhance the quality of\nhigh-frequency regions while maintaining the fidelity of low-frequency regions.\nUGAE significantly outperformed existing methods on three benchmark datasets:\n8iVFB, Owlii, and MVUB. Compared to the latest G-PCC test model (TMC13v29),\nUGAE achieved an average BD-PSNR gain of 9.98 dB and 90.98% BD-bitrate savings\nfor geometry under the D1 metric, as well as a 3.67 dB BD-PSNR improvement with\n56.88% BD-bitrate savings for attributes on the Y component. Additionally, it\nimproved perceptual quality significantly."
    },
    {
      "title": "CoMo: Compositional Motion Customization for Text-to-Video Generation",
      "authors": [
        "Youcan Xu",
        "Zhen Wang",
        "Jiaxin Shi",
        "Kexin Li",
        "Feifei Shao",
        "Jun Xiao",
        "Yi Yang",
        "Jun Yu",
        "Long Chen"
      ],
      "arxiv_id": "2510.23007v1",
      "summary": "While recent text-to-video models excel at generating diverse scenes, they\nstruggle with precise motion control, particularly for complex, multi-subject\nmotions. Although methods for single-motion customization have been developed\nto address this gap, they fail in compositional scenarios due to two primary\nchallenges: motion-appearance entanglement and ineffective multi-motion\nblending. This paper introduces CoMo, a novel framework for\n$\\textbf{compositional motion customization}$ in text-to-video generation,\nenabling the synthesis of multiple, distinct motions within a single video.\nCoMo addresses these issues through a two-phase approach. First, in the\nsingle-motion learning phase, a static-dynamic decoupled tuning paradigm\ndisentangles motion from appearance to learn a motion-specific module. Second,\nin the multi-motion composition phase, a plug-and-play divide-and-merge\nstrategy composes these learned motions without additional training by\nspatially isolating their influence during the denoising process. To facilitate\nresearch in this new domain, we also introduce a new benchmark and a novel\nevaluation metric designed to assess multi-motion fidelity and blending.\nExtensive experiments demonstrate that CoMo achieves state-of-the-art\nperformance, significantly advancing the capabilities of controllable video\ngeneration. Our project page is at https://como6.github.io/."
    },
    {
      "title": "An Intelligent Water-Saving Irrigation System Based on Multi-Sensor Fusion and Visual Servoing Control",
      "authors": [
        "ZhengKai Huang",
        "YiKun Wang",
        "ChenYu Hui",
        "XiaoCheng"
      ],
      "arxiv_id": "2510.23003v1",
      "summary": "This paper introduces an intelligent water-saving irrigation system designed\nto address critical challenges in precision agriculture, such as inefficient\nwater use and poor terrain adaptability. The system integrates advanced\ncomputer vision, robotic control, and real-time stabilization technologies via\na multi-sensor fusion approach. A lightweight YOLO model, deployed on an\nembedded vision processor (K210), enables real-time plant container detection\nwith over 96% accuracy under varying lighting conditions. A simplified hand-eye\ncalibration algorithm-designed for 'handheld camera' robot arm\nconfigurations-ensures that the end effector can be precisely positioned, with\na success rate exceeding 90%. The active leveling system, driven by the\nSTM32F103ZET6 main control chip and JY901S inertial measurement data, can\nstabilize the irrigation platform on slopes up to 10 degrees, with a response\ntime of 1.8 seconds. Experimental results across three simulated agricultural\nenvironments (standard greenhouse, hilly terrain, complex lighting) demonstrate\na 30-50% reduction in water consumption compared to conventional flood\nirrigation, with water use efficiency exceeding 92% in all test cases."
    },
    {
      "title": "LoMix: Learnable Weighted Multi-Scale Logits Mixing for Medical Image Segmentation",
      "authors": [
        "Md Mostafijur Rahman",
        "Radu Marculescu"
      ],
      "arxiv_id": "2510.22995v1",
      "summary": "U-shaped networks output logits at multiple spatial scales, each capturing a\ndifferent blend of coarse context and fine detail. Yet, training still treats\nthese logits in isolation - either supervising only the final,\nhighest-resolution logits or applying deep supervision with identical loss\nweights at every scale - without exploring mixed-scale combinations.\nConsequently, the decoder output misses the complementary cues that arise only\nwhen coarse and fine predictions are fused. To address this issue, we introduce\nLoMix (Logits Mixing), a NAS-inspired, differentiable plug-and-play module that\ngenerates new mixed-scale outputs and learns how exactly each of them should\nguide the training process. More precisely, LoMix mixes the multi-scale decoder\nlogits with four lightweight fusion operators: addition, multiplication,\nconcatenation, and attention-based weighted fusion, yielding a rich set of\nsynthetic mutant maps. Every original or mutant map is given a softplus loss\nweight that is co-optimized with network parameters, mimicking a one-step\narchitecture search that automatically discovers the most useful scales,\nmixtures, and operators. Plugging LoMix into recent U-shaped architectures\n(i.e., PVT-V2-B2 backbone with EMCAD decoder) on Synapse 8-organ dataset\nimproves DICE by +4.2% over single-output supervision, +2.2% over deep\nsupervision, and +1.5% over equally weighted additive fusion, all with zero\ninference overhead. When training data are scarce (e.g., one or two labeled\nscans), the advantage grows to +9.23%, underscoring LoMix's data efficiency.\nAcross four benchmarks and diverse U-shaped networks, LoMiX improves DICE by up\nto +13.5% over single-output supervision, confirming that learnable weighted\nmixed-scale fusion generalizes broadly while remaining data efficient, fully\ninterpretable, and overhead-free at inference. Our code is available at\nhttps://github.com/SLDGroup/LoMix."
    },
    {
      "title": "SceneDecorator: Towards Scene-Oriented Story Generation with Scene Planning and Scene Consistency",
      "authors": [
        "Quanjian Song",
        "Donghao Zhou",
        "Jingyu Lin",
        "Fei Shen",
        "Jiaze Wang",
        "Xiaowei Hu",
        "Cunjian Chen",
        "Pheng-Ann Heng"
      ],
      "arxiv_id": "2510.22994v1",
      "summary": "Recent text-to-image models have revolutionized image generation, but they\nstill struggle with maintaining concept consistency across generated images.\nWhile existing works focus on character consistency, they often overlook the\ncrucial role of scenes in storytelling, which restricts their creativity in\npractice. This paper introduces scene-oriented story generation, addressing two\nkey challenges: (i) scene planning, where current methods fail to ensure\nscene-level narrative coherence by relying solely on text descriptions, and\n(ii) scene consistency, which remains largely unexplored in terms of\nmaintaining scene consistency across multiple stories. We propose\nSceneDecorator, a training-free framework that employs VLM-Guided Scene\nPlanning to ensure narrative coherence across different scenes in a\n``global-to-local'' manner, and Long-Term Scene-Sharing Attention to maintain\nlong-term scene consistency and subject diversity across generated stories.\nExtensive experiments demonstrate the superior performance of SceneDecorator,\nhighlighting its potential to unleash creativity in the fields of arts, films,\nand games."
    },
    {
      "title": "USF-MAE: Ultrasound Self-Supervised Foundation Model with Masked Autoencoding",
      "authors": [
        "Youssef Megahed",
        "Robin Ducharme",
        "Mark Walker",
        "Steven Hawken",
        "Adrian D. C. Chan"
      ],
      "arxiv_id": "2510.22990v1",
      "summary": "Ultrasound imaging is one of the most widely used diagnostic modalities,\noffering real-time, radiation-free assessment across diverse clinical domains.\nHowever, interpretation of ultrasound images remains challenging due to high\nnoise levels, operator dependence, and limited field of view, resulting in\nsubstantial inter-observer variability. Current Deep Learning approaches are\nhindered by the scarcity of large labeled datasets and the domain gap between\ngeneral and sonographic images, which limits the transferability of models\npretrained on non-medical data. To address these challenges, we introduce the\nUltrasound Self-Supervised Foundation Model with Masked Autoencoding (USF-MAE),\nthe first large-scale self-supervised MAE framework pretrained exclusively on\nultrasound data. The model was pre-trained on 370,000 2D and 3D ultrasound\nimages curated from 46 open-source datasets, collectively termed OpenUS-46,\nspanning over twenty anatomical regions. This curated dataset has been made\npublicly available to facilitate further research and reproducibility. Using a\nVision Transformer encoder-decoder architecture, USF-MAE reconstructs masked\nimage patches, enabling it to learn rich, modality-specific representations\ndirectly from unlabeled data. The pretrained encoder was fine-tuned on three\npublic downstream classification benchmarks: BUS-BRA (breast cancer), MMOTU-2D\n(ovarian tumors), and GIST514-DB (gastrointestinal stromal tumors). Across all\ntasks, USF-MAE consistently outperformed conventional CNN and ViT baselines,\nachieving F1-scores of 81.6%, 79.6%, and 82.4%, respectively. Despite not using\nlabels during pretraining, USF-MAE approached the performance of the supervised\nfoundation model UltraSam on breast cancer classification and surpassed it on\nthe other tasks, demonstrating strong cross-anatomical generalization."
    },
    {
      "title": "Exploring Semantic-constrained Adversarial Example with Instruction Uncertainty Reduction",
      "authors": [
        "Jin Hu",
        "Jiakai Wang",
        "Linna Jing",
        "Haolin Li",
        "Haodong Liu",
        "Haotong Qin",
        "Aishan Liu",
        "Ke Xu",
        "Xianglong Liu"
      ],
      "arxiv_id": "2510.22981v1",
      "summary": "Recently, semantically constrained adversarial examples (SemanticAE), which\nare directly generated from natural language instructions, have become a\npromising avenue for future research due to their flexible attacking forms. To\ngenerate SemanticAEs, current methods fall short of satisfactory attacking\nability as the key underlying factors of semantic uncertainty in human\ninstructions, such as referring diversity, descriptive incompleteness, and\nboundary ambiguity, have not been fully investigated. To tackle the issues,\nthis paper develops a multi-dimensional instruction uncertainty reduction\n(InSUR) framework to generate more satisfactory SemanticAE, i.e., transferable,\nadaptive, and effective. Specifically, in the dimension of the sampling method,\nwe propose the residual-driven attacking direction stabilization to alleviate\nthe unstable adversarial optimization caused by the diversity of language\nreferences. By coarsely predicting the language-guided sampling process, the\noptimization process will be stabilized by the designed ResAdv-DDIM sampler,\ntherefore releasing the transferable and robust adversarial capability of\nmulti-step diffusion models. In task modeling, we propose the context-encoded\nattacking scenario constraint to supplement the missing knowledge from\nincomplete human instructions. Guidance masking and renderer integration are\nproposed to regulate the constraints of 2D/3D SemanticAE, activating stronger\nscenario-adapted attacks. Moreover, in the dimension of generator evaluation,\nwe propose the semantic-abstracted attacking evaluation enhancement by\nclarifying the evaluation boundary, facilitating the development of more\neffective SemanticAE generators. Extensive experiments demonstrate the\nsuperiority of the transfer attack performance of InSUR. Moreover, we realize\nthe reference-free generation of semantically constrained 3D adversarial\nexamples for the first time."
    },
    {
      "title": "VoMP: Predicting Volumetric Mechanical Property Fields",
      "authors": [
        "Rishit Dagli",
        "Donglai Xiang",
        "Vismay Modi",
        "Charles Loop",
        "Clement Fuji Tsang",
        "Anka He Chen",
        "Anita Hu",
        "Gavriel State",
        "David I. W. Levin",
        "Maria Shugrina"
      ],
      "arxiv_id": "2510.22975v1",
      "summary": "Physical simulation relies on spatially-varying mechanical properties, often\nlaboriously hand-crafted. VoMP is a feed-forward method trained to predict\nYoung's modulus ($E$), Poisson's ratio ($\\nu$), and density ($\\rho$) throughout\nthe volume of 3D objects, in any representation that can be rendered and\nvoxelized. VoMP aggregates per-voxel multi-view features and passes them to our\ntrained Geometry Transformer to predict per-voxel material latent codes. These\nlatents reside on a manifold of physically plausible materials, which we learn\nfrom a real-world dataset, guaranteeing the validity of decoded per-voxel\nmaterials. To obtain object-level training data, we propose an annotation\npipeline combining knowledge from segmented 3D datasets, material databases,\nand a vision-language model, along with a new benchmark. Experiments show that\nVoMP estimates accurate volumetric properties, far outperforming prior art in\naccuracy and speed."
    },
    {
      "title": "Scaling Up Occupancy-centric Driving Scene Generation: Dataset and Method",
      "authors": [
        "Bohan Li",
        "Xin Jin",
        "Hu Zhu",
        "Hongsi Liu",
        "Ruikai Li",
        "Jiazhe Guo",
        "Kaiwen Cai",
        "Chao Ma",
        "Yueming Jin",
        "Hao Zhao",
        "Xiaokang Yang",
        "Wenjun Zeng"
      ],
      "arxiv_id": "2510.22973v1",
      "summary": "Driving scene generation is a critical domain for autonomous driving,\nenabling downstream applications, including perception and planning evaluation.\nOccupancy-centric methods have recently achieved state-of-the-art results by\noffering consistent conditioning across frames and modalities; however, their\nperformance heavily depends on annotated occupancy data, which still remains\nscarce. To overcome this limitation, we curate Nuplan-Occ, the largest semantic\noccupancy dataset to date, constructed from the widely used Nuplan benchmark.\nIts scale and diversity facilitate not only large-scale generative modeling but\nalso autonomous driving downstream applications. Based on this dataset, we\ndevelop a unified framework that jointly synthesizes high-quality semantic\noccupancy, multi-view videos, and LiDAR point clouds. Our approach incorporates\na spatio-temporal disentangled architecture to support high-fidelity spatial\nexpansion and temporal forecasting of 4D dynamic occupancy. To bridge modal\ngaps, we further propose two novel techniques: a Gaussian splatting-based\nsparse point map rendering strategy that enhances multi-view video generation,\nand a sensor-aware embedding strategy that explicitly models LiDAR sensor\nproperties for realistic multi-LiDAR simulation. Extensive experiments\ndemonstrate that our method achieves superior generation fidelity and\nscalability compared to existing approaches, and validates its practical value\nin downstream tasks. Repo:\nhttps://github.com/Arlo0o/UniScene-Unified-Occupancy-centric-Driving-Scene-Generation/tree/v2"
    },
    {
      "title": "VALA: Learning Latent Anchors for Training-Free and Temporally Consistent",
      "authors": [
        "Zhangkai Wu",
        "Xuhui Fan",
        "Zhongyuan Xie",
        "Kaize Shi",
        "Longbing Cao"
      ],
      "arxiv_id": "2510.22970v1",
      "summary": "Recent advances in training-free video editing have enabled lightweight and\nprecise cross-frame generation by leveraging pre-trained text-to-image\ndiffusion models. However, existing methods often rely on heuristic frame\nselection to maintain temporal consistency during DDIM inversion, which\nintroduces manual bias and reduces the scalability of end-to-end inference. In\nthis paper, we propose~\\textbf{VALA} (\\textbf{V}ariational \\textbf{A}lignment\nfor \\textbf{L}atent \\textbf{A}nchors), a variational alignment module that\nadaptively selects key frames and compresses their latent features into\nsemantic anchors for consistent video editing. To learn meaningful assignments,\nVALA propose a variational framework with a contrastive learning objective.\nTherefore, it can transform cross-frame latent representations into compressed\nlatent anchors that preserve both content and temporal coherence. Our method\ncan be fully integrated into training-free text-to-image based video editing\nmodels. Extensive experiments on real-world video editing benchmarks show that\nVALA achieves state-of-the-art performance in inversion fidelity, editing\nquality, and temporal consistency, while offering improved efficiency over\nprior methods."
    },
    {
      "title": "Survey of Multimodal Geospatial Foundation Models: Techniques, Applications, and Challenges",
      "authors": [
        "Liling Yang",
        "Ning Chen",
        "Jun Yue",
        "Yidan Liu",
        "Jiayi Ma",
        "Pedram Ghamisi",
        "Antonio Plaza",
        "Leyuan Fang"
      ],
      "arxiv_id": "2510.22964v1",
      "summary": "Foundation models have transformed natural language processing and computer\nvision, and their impact is now reshaping remote sensing image analysis. With\npowerful generalization and transfer learning capabilities, they align\nnaturally with the multimodal, multi-resolution, and multi-temporal\ncharacteristics of remote sensing data. To address unique challenges in the\nfield, multimodal geospatial foundation models (GFMs) have emerged as a\ndedicated research frontier. This survey delivers a comprehensive review of\nmultimodal GFMs from a modality-driven perspective, covering five core visual\nand vision-language modalities. We examine how differences in imaging physics\nand data representation shape interaction design, and we analyze key techniques\nfor alignment, integration, and knowledge transfer to tackle modality\nheterogeneity, distribution shifts, and semantic gaps. Advances in training\nparadigms, architectures, and task-specific adaptation strategies are\nsystematically assessed alongside a wealth of emerging benchmarks.\nRepresentative multimodal visual and vision-language GFMs are evaluated across\nten downstream tasks, with insights into their architectures, performance, and\napplication scenarios. Real-world case studies, spanning land cover mapping,\nagricultural monitoring, disaster response, climate studies, and geospatial\nintelligence, demonstrate the practical potential of GFMs. Finally, we outline\npressing challenges in domain generalization, interpretability, efficiency, and\nprivacy, and chart promising avenues for future research."
    },
    {
      "title": "FAME: Fairness-aware Attention-modulated Video Editing",
      "authors": [
        "Zhangkai Wu",
        "Xuhui Fan",
        "Zhongyuan Xie",
        "Kaize Shi",
        "Zhidong Li",
        "Longbing Cao"
      ],
      "arxiv_id": "2510.22960v1",
      "summary": "Training-free video editing (VE) models tend to fall back on gender\nstereotypes when rendering profession-related prompts. We propose \\textbf{FAME}\nfor \\textit{Fairness-aware Attention-modulated Video Editing} that mitigates\nprofession-related gender biases while preserving prompt alignment and temporal\nconsistency for coherent VE. We derive fairness embeddings from existing\nminority representations by softly injecting debiasing tokens into the text\nencoder. Simultaneously, FAME integrates fairness modulation into both temporal\nself attention and prompt-to-region cross attention to mitigate the motion\ncorruption and temporal inconsistency caused by directly introducing fairness\ncues. For temporal self attention, FAME introduces a region constrained\nattention mask combined with time decay weighting, which enhances intra-region\ncoherence while suppressing irrelevant inter-region interactions. For cross\nattention, it reweights tokens to region matching scores by incorporating\nfairness sensitive similarity masks derived from debiasing prompt embeddings.\nTogether, these modulations keep fairness-sensitive semantics tied to the right\nvisual regions and prevent temporal drift across frames. Extensive experiments\non new VE fairness-oriented benchmark \\textit{FairVE} demonstrate that FAME\nachieves stronger fairness alignment and semantic fidelity, surpassing existing\nVE baselines."
    },
    {
      "title": "End-to-End Design and Validation of a Low-Cost Stewart Platform with Nonlinear Estimation and Control",
      "authors": [
        "Benedictus C. G. Cinun",
        "Tua A. Tamba",
        "Immanuel R. Santjoko",
        "Xiaofeng Wang",
        "Michael A. Gunarso",
        "Bin Hu"
      ],
      "arxiv_id": "2510.22949v1",
      "summary": "This paper presents the complete design, control, and experimental validation\nof a low-cost Stewart platform prototype developed as an affordable yet capable\nrobotic testbed for research and education. The platform combines off the shelf\ncomponents with 3D printed and custom fabricated parts to deliver full six\ndegrees of freedom motions using six linear actuators connecting a moving\nplatform to a fixed base. The system software integrates dynamic modeling, data\nacquisition, and real time control within a unified framework. A robust\ntrajectory tracking controller based on feedback linearization, augmented with\nan LQR scheme, compensates for the platform's nonlinear dynamics to achieve\nprecise motion control. In parallel, an Extended Kalman Filter fuses IMU and\nactuator encoder feedback to provide accurate and reliable state estimation\nunder sensor noise and external disturbances. Unlike prior efforts that\nemphasize only isolated aspects such as modeling or control, this work delivers\na complete hardware-software platform validated through both simulation and\nexperiments on static and dynamic trajectories. Results demonstrate effective\ntrajectory tracking and real-time state estimation, highlighting the platform's\npotential as a cost effective and versatile tool for advanced research and\neducational applications."
    },
    {
      "title": "LightBagel: A Light-weighted, Double Fusion Framework for Unified Multimodal Understanding and Generation",
      "authors": [
        "Zeyu Wang",
        "Zilong Chen",
        "Chenhui Gou",
        "Feng Li",
        "Chaorui Deng",
        "Deyao Zhu",
        "Kunchang Li",
        "Weihao Yu",
        "Haoqin Tu",
        "Haoqi Fan",
        "Cihang Xie"
      ],
      "arxiv_id": "2510.22946v1",
      "summary": "Unified multimodal models have recently shown remarkable gains in both\ncapability and versatility, yet most leading systems are still trained from\nscratch and require substantial computational resources. In this paper, we show\nthat competitive performance can be obtained far more efficiently by\nstrategically fusing publicly available models specialized for either\ngeneration or understanding. Our key design is to retain the original blocks\nwhile additionally interleaving multimodal self-attention blocks throughout the\nnetworks. This double fusion mechanism (1) effectively enables rich multi-modal\nfusion while largely preserving the original strengths of the base models, and\n(2) catalyzes synergistic fusion of high-level semantic representations from\nthe understanding encoder with low-level spatial signals from the generation\nencoder. By training with only ~ 35B tokens, this approach achieves strong\nresults across multiple benchmarks: 0.91 on GenEval for compositional\ntext-to-image generation, 82.16 on DPG-Bench for complex text-to-image\ngeneration, 6.06 on GEditBench, and 3.77 on ImgEdit-Bench for image editing. By\nfully releasing the entire suite of code, model weights, and datasets, we hope\nto support future research on unified multimodal modeling."
    },
    {
      "title": "Switchable Token-Specific Codebook Quantization For Face Image Compression",
      "authors": [
        "Yongbo Wang",
        "Haonan Wang",
        "Guodong Mu",
        "Ruixin Zhang",
        "Jiaqi Chen",
        "Jingyun Zhang",
        "Jun Wang",
        "Yuan Xie",
        "Zhizhong Zhang",
        "Shouhong Ding"
      ],
      "arxiv_id": "2510.22943v1",
      "summary": "With the ever-increasing volume of visual data, the efficient and lossless\ntransmission, along with its subsequent interpretation and understanding, has\nbecome a critical bottleneck in modern information systems. The emerged\ncodebook-based solution utilize a globally shared codebook to quantize and\ndequantize each token, controlling the bpp by adjusting the number of tokens or\nthe codebook size. However, for facial images, which are rich in attributes,\nsuch global codebook strategies overlook both the category-specific\ncorrelations within images and the semantic differences among tokens, resulting\nin suboptimal performance, especially at low bpp. Motivated by these\nobservations, we propose a Switchable Token-Specific Codebook Quantization for\nface image compression, which learns distinct codebook groups for different\nimage categories and assigns an independent codebook to each token. By\nrecording the codebook group to which each token belongs with a small number of\nbits, our method can reduce the loss incurred when decreasing the size of each\ncodebook group. This enables a larger total number of codebooks under a lower\noverall bpp, thereby enhancing the expressive capability and improving\nreconstruction performance. Owing to its generalizable design, our method can\nbe integrated into any existing codebook-based representation learning approach\nand has demonstrated its effectiveness on face recognition datasets, achieving\nan average accuracy of 93.51% for reconstructed images at 0.05 bpp."
    },
    {
      "title": "Bi-Encoder Contrastive Learning for Fingerprint and Iris Biometrics",
      "authors": [
        "Matthew So",
        "Judah Goldfeder",
        "Mark Lis",
        "Hod Lipson"
      ],
      "arxiv_id": "2510.22937v1",
      "summary": "There has been a historic assumption that the biometrics of an individual are\nstatistically uncorrelated. We test this assumption by training Bi-Encoder\nnetworks on three verification tasks, including fingerprint-to-fingerprint\nmatching, iris-to-iris matching, and cross-modal fingerprint-to-iris matching\nusing 274 subjects with $\\sim$100k fingerprints and 7k iris images. We trained\nResNet-50 and Vision Transformer backbones in Bi-Encoder architectures such\nthat the contrastive loss between images sampled from the same individual is\nminimized. The iris ResNet architecture reaches 91 ROC AUC score for\niris-to-iris matching, providing clear evidence that the left and right irises\nof an individual are correlated. Fingerprint models reproduce the positive\nintra-subject suggested by prior work in this space. This is the first work\nattempting to use Vision Transformers for this matching. Cross-modal matching\nrises only slightly above chance, which suggests that more data and a more\nsophisticated pipeline is needed to obtain compelling results. These findings\ncontinue challenge independence assumptions of biometrics and we plan to extend\nthis work to other biometrics in the future. Code available:\nhttps://github.com/MatthewSo/bio_fingerprints_iris."
    },
    {
      "title": "Positional Preservation Embedding for Multimodal Large Language Models",
      "authors": [
        "Mouxiao Huang",
        "Borui Jiang",
        "Dehua Zheng",
        "Hailin Hu",
        "Kai Han",
        "Xinghao Chen"
      ],
      "arxiv_id": "2510.22936v1",
      "summary": "Multimodal large language models (MLLMs) have achieved strong performance on\nvision-language tasks, yet often suffer from inefficiencies due to redundant\nvisual tokens. Existing token merging methods reduce sequence length but\nfrequently disrupt spatial layouts and temporal continuity by disregarding\npositional relationships. In this work, we propose a novel encoding operator\ndubbed as \\textbf{P}ositional \\textbf{P}reservation \\textbf{E}mbedding\n(\\textbf{PPE}), which has the main hallmark of preservation of spatiotemporal\nstructure during visual token compression. PPE explicitly introduces the\ndisentangled encoding of 3D positions in the token dimension, enabling each\ncompressed token to encapsulate different positions from multiple original\ntokens. Furthermore, we show that PPE can effectively support cascade\nclustering -- a progressive token compression strategy that leads to better\nperformance retention. PPE is a parameter-free and generic operator that can be\nseamlessly integrated into existing token merging methods without any\nadjustments. Applied to state-of-the-art token merging framework, PPE achieves\nconsistent improvements of $2\\%\\sim5\\%$ across multiple vision-language\nbenchmarks, including MMBench (general vision understanding), TextVQA (layout\nunderstanding) and VideoMME (temporal understanding). These results demonstrate\nthat preserving positional cues is critical for efficient and effective MLLM\nreasoning."
    },
    {
      "title": "Gen-LangSplat: Generalized Language Gaussian Splatting with Pre-Trained Feature Compression",
      "authors": [
        "Pranav Saxena"
      ],
      "arxiv_id": "2510.22930v1",
      "summary": "Modeling open-vocabulary language fields in 3D is essential for intuitive\nhuman-AI interaction and querying within physical environments.\nState-of-the-art approaches, such as LangSplat, leverage 3D Gaussian Splatting\nto efficiently construct these language fields, encoding features distilled\nfrom high-dimensional models like CLIP. However, this efficiency is currently\noffset by the requirement to train a scene-specific language autoencoder for\nfeature compression, introducing a costly, per-scene optimization bottleneck\nthat hinders deployment scalability. In this work, we introduce Gen-LangSplat,\nthat eliminates this requirement by replacing the scene-wise autoencoder with a\ngeneralized autoencoder, pre-trained extensively on the large-scale ScanNet\ndataset. This architectural shift enables the use of a fixed, compact latent\nspace for language features across any new scene without any scene-specific\ntraining. By removing this dependency, our entire language field construction\nprocess achieves a efficiency boost while delivering querying performance\ncomparable to, or exceeding, the original LangSplat method. To validate our\ndesign choice, we perform a thorough ablation study empirically determining the\noptimal latent embedding dimension and quantifying representational fidelity\nusing Mean Squared Error and cosine similarity between the original and\nreprojected 512-dimensional CLIP embeddings. Our results demonstrate that\ngeneralized embeddings can efficiently and accurately support open-vocabulary\nquerying in novel 3D scenes, paving the way for scalable, real-time interactive\n3D AI applications."
    },
    {
      "title": "HyPerNav: Hybrid Perception for Object-Oriented Navigation in Unknown Environment",
      "authors": [
        "Zecheng Yin",
        "Hao Zhao",
        "Zhen Li"
      ],
      "arxiv_id": "2510.22917v1",
      "summary": "Objective-oriented navigation(ObjNav) enables robot to navigate to target\nobject directly and autonomously in an unknown environment. Effective\nperception in navigation in unknown environment is critical for autonomous\nrobots. While egocentric observations from RGB-D sensors provide abundant local\ninformation, real-time top-down maps offer valuable global context for ObjNav.\nNevertheless, the majority of existing studies focus on a single source, seldom\nintegrating these two complementary perceptual modalities, despite the fact\nthat humans naturally attend to both. With the rapid advancement of\nVision-Language Models(VLMs), we propose Hybrid Perception Navigation\n(HyPerNav), leveraging VLMs' strong reasoning and vision-language understanding\ncapabilities to jointly perceive both local and global information to enhance\nthe effectiveness and intelligence of navigation in unknown environments. In\nboth massive simulation evaluation and real-world validation, our methods\nachieved state-of-the-art performance against popular baselines. Benefiting\nfrom hybrid perception approach, our method captures richer cues and finds the\nobjects more effectively, by simultaneously leveraging information\nunderstanding from egocentric observations and the top-down map. Our ablation\nstudy further proved that either of the hybrid perception contributes to the\nnavigation performance."
    },
    {
      "title": "Estimating Pasture Biomass from Top-View Images: A Dataset for Precision Agriculture",
      "authors": [
        "Qiyu Liao",
        "Dadong Wang",
        "Rebecca Haling",
        "Jiajun Liu",
        "Xun Li",
        "Martyna Plomecka",
        "Andrew Robson",
        "Matthew Pringle",
        "Rhys Pirie",
        "Megan Walker",
        "Joshua Whelan"
      ],
      "arxiv_id": "2510.22916v1",
      "summary": "Accurate estimation of pasture biomass is important for decision-making in\nlivestock production systems. Estimates of pasture biomass can be used to\nmanage stocking rates to maximise pasture utilisation, while minimising the\nrisk of overgrazing and promoting overall system health. We present a\ncomprehensive dataset of 1,162 annotated top-view images of pastures collected\nacross 19 locations in Australia. The images were taken across multiple seasons\nand include a range of temperate pasture species. Each image captures a 70cm *\n30cm quadrat and is paired with on-ground measurements including biomass sorted\nby component (green, dead, and legume fraction), vegetation height, and\nNormalized Difference Vegetation Index (NDVI) from Active Optical Sensors\n(AOS). The multidimensional nature of the data, which combines visual,\nspectral, and structural information, opens up new possibilities for advancing\nthe use of precision grazing management. The dataset is released and hosted in\na Kaggle competition that challenges the international Machine Learning\ncommunity with the task of pasture biomass estimation. The dataset is available\non the official Kaggle webpage:\nhttps://www.kaggle.com/competitions/csiro-biomass"
    },
    {
      "title": "Clinic-Oriented Feasibility of a Sensor-Fused Wearable for Upper-Limb Function",
      "authors": [
        "Thanyanee Srichaisak",
        "Arissa Ieochai",
        "Aueaphum Aueawattthanaphisut"
      ],
      "arxiv_id": "2510.22913v1",
      "summary": "Background: Upper-limb weakness and tremor (4--12 Hz) limit activities of\ndaily living (ADL) and reduce adherence to home rehabilitation. Objective: To\nassess technical feasibility and clinician-relevant signals of a sensor-fused\nwearable targeting the triceps brachii and extensor pollicis brevis. Methods: A\nlightweight node integrates surface EMG (1 kHz), IMU (100--200 Hz), and\nflex/force sensors with on-device INT8 inference (Tiny 1D-CNN/Transformer) and\na safety-bounded assist policy (angle/torque/jerk limits; stall/time-out).\nHealthy adults (n = 12) performed three ADL-like tasks. Primary outcomes:\nTremor Index (TI), range of motion (ROM), repetitions (Reps min$^{-1}$).\nSecondary: EMG median-frequency slope (fatigue trend), closed-loop latency,\nsession completion, and device-related adverse events. Analyses used\nsubject-level paired medians with BCa 95\\% CIs; exact Wilcoxon $p$-values are\nreported in the Results. Results: Assistance was associated with lower tremor\nprominence and improved task throughput: TI decreased by $-0.092$ (95\\% CI\n[$-0.102$, $-0.079$]), ROM increased by $+12.65\\%$ (95\\% CI [$+8.43$,\n$+13.89$]), and Reps rose by $+2.99$ min$^{-1}$ (95\\% CI [$+2.61$, $+3.35$]).\nMedian on-device latency was 8.7 ms at a 100 Hz loop rate; all sessions were\ncompleted with no device-related adverse events. Conclusions: Multimodal\nsensing with low-latency, safety-bounded assistance produced improved movement\nquality (TI $\\downarrow$) and throughput (ROM, Reps $\\uparrow$) in a pilot\ntechnical-feasibility setting, supporting progression to IRB-approved patient\nstudies. Trial registration: Not applicable (pilot non-clinical)."
    },
    {
      "title": "Never Too Rigid to Reach: Adaptive Virtual Model Control with LLM- and Lyapunov-Based Reinforcement Learning",
      "authors": [
        "Jingzehua Xu",
        "Yangyang Li",
        "Yangfei Chen",
        "Guanwen Xie",
        "Shuai Zhang"
      ],
      "arxiv_id": "2510.22892v1",
      "summary": "Robotic arms are increasingly deployed in uncertain environments, yet\nconventional control pipelines often become rigid and brittle when exposed to\nperturbations or incomplete information. Virtual Model Control (VMC) enables\ncompliant behaviors by embedding virtual forces and mapping them into joint\ntorques, but its reliance on fixed parameters and limited coordination among\nvirtual components constrains adaptability and may undermine stability as task\nobjectives evolve. To address these limitations, we propose Adaptive VMC with\nLarge Language Model (LLM)- and Lyapunov-Based Reinforcement Learning (RL),\nwhich preserves the physical interpretability of VMC while supporting\nstability-guaranteed online adaptation. The LLM provides structured priors and\nhigh-level reasoning that enhance coordination among virtual components,\nimprove sample efficiency, and facilitate flexible adjustment to varying task\nrequirements. Complementarily, Lyapunov-based RL enforces theoretical stability\nconstraints, ensuring safe and reliable adaptation under uncertainty. Extensive\nsimulations on a 7-DoF Panda arm demonstrate that our approach effectively\nbalances competing objectives in dynamic tasks, achieving superior performance\nwhile highlighting the synergistic benefits of LLM guidance and\nLyapunov-constrained adaptation."
    }
  ]
}