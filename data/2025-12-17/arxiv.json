{
  "count": 98,
  "papers": [
    {
      "title": "VLA-AN: An Efficient and Onboard Vision-Language-Action Framework for Aerial Navigation in Complex Environments",
      "authors": [
        "Yuze Wu",
        "Mo Zhu",
        "Xingxing Li",
        "Yuheng Du",
        "Yuxin Fan",
        "Wenjun Li",
        "Xin Zhou",
        "Fei Gao"
      ],
      "arxiv_id": "2512.15258v1",
      "summary": "This paper proposes VLA-AN, an efficient and onboard Vision-Language-Action (VLA) framework dedicated to autonomous drone navigation in complex environments. VLA-AN addresses four major limitations of existing large aerial navigation models: the data domain gap, insufficient temporal navigation with reasoning, safety issues with generative action policies, and onboard deployment constraints. First, we construct a high-fidelity dataset utilizing 3D Gaussian Splatting (3D-GS) to effectively bridge the domain gap. Second, we introduce a progressive three-stage training framework that sequentially reinforces scene comprehension, core flight skills, and complex navigation capabilities. Third, we design a lightweight, real-time action module coupled with geometric safety correction. This module ensures fast, collision-free, and stable command generation, mitigating the safety risks inherent in stochastic generative policies. Finally, through deep optimization of the onboard deployment pipeline, VLA-AN achieves a robust real-time 8.3x improvement in inference throughput on resource-constrained UAVs. Extensive experiments demonstrate that VLA-AN significantly improves spatial grounding, scene reasoning, and long-horizon navigation, achieving a maximum single-task success rate of 98.1%, and providing an efficient, practical solution for realizing full-chain closed-loop autonomy in lightweight aerial robots.",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15258v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "3D gaussian splatting",
            "gaussian splatting",
            "splatting"
          ],
          "score": 6.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "[T]VLA"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 24.0,
      "hit_pillars": [
        "3_perception_slam",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Off The Grid: Detection of Primitives for Feed-Forward 3D Gaussian Splatting",
      "authors": [
        "Arthur Moreau",
        "Richard Shaw",
        "Michal Nazarczuk",
        "Jisu Shin",
        "Thomas Tanay",
        "Zhensong Zhang",
        "Songcen Xu",
        "Eduardo Pérez-Pellitero"
      ],
      "arxiv_id": "2512.15508v1",
      "summary": "Feed-forward 3D Gaussian Splatting (3DGS) models enable real-time scene generation but are hindered by suboptimal pixel-aligned primitive placement, which relies on a dense, rigid grid and limits both quality and efficiency. We introduce a new feed-forward architecture that detects 3D Gaussian primitives at a sub-pixel level, replacing the pixel grid with an adaptive, \"Off The Grid\" distribution. Inspired by keypoint detection, our multi-resolution decoder learns to distribute primitives across image patches. This module is trained end-to-end with a 3D reconstruction backbone using self-supervised learning. Our resulting pose-free model generates photorealistic scenes in seconds, achieving state-of-the-art novel view synthesis for feed-forward models. It outperforms competitors while using far fewer primitives, demonstrating a more accurate and efficient allocation that captures fine details and reduces artifacts. Moreover, we observe that by learning to render 3D Gaussians, our 3D reconstruction backbone improves camera pose estimation, suggesting opportunities to train these foundational models without labels.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15508v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]3D gaussian splatting",
            "3DGS",
            "[T]gaussian splatting",
            "[T]splatting"
          ],
          "score": 20.0
        }
      ],
      "relevance_score": 20.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Explainable Action Form Assessment by Exploiting Multimodal Chain-of-Thoughts Reasoning",
      "authors": [
        "Mengshi Qi",
        "Yeteng Wu",
        "Xianlin Zhang",
        "Huadong Ma"
      ],
      "arxiv_id": "2512.15153v1",
      "summary": "Evaluating whether human action is standard or not and providing reasonable feedback to improve action standardization is very crucial but challenging in real-world scenarios. However, current video understanding methods are mainly concerned with what and where the action is, which is unable to meet the requirements. Meanwhile, most of the existing datasets lack the labels indicating the degree of action standardization, and the action quality assessment datasets lack explainability and detailed feedback. Therefore, we define a new Human Action Form Assessment (AFA) task, and introduce a new diverse dataset CoT-AFA, which contains a large scale of fitness and martial arts videos with multi-level annotations for comprehensive video analysis. We enrich the CoT-AFA dataset with a novel Chain-of-Thought explanation paradigm. Instead of offering isolated feedback, our explanations provide a complete reasoning process--from identifying an action step to analyzing its outcome and proposing a concrete solution. Furthermore, we propose a framework named Explainable Fitness Assessor, which can not only judge an action but also explain why and provide a solution. This framework employs two parallel processing streams and a dynamic gating mechanism to fuse visual and semantic information, thereby boosting its analytical capabilities. The experimental results demonstrate that our method has achieved improvements in explanation generation (e.g., +16.0% in CIDEr), action classification (+2.7% in accuracy) and quality assessment (+2.1% in accuracy), revealing great potential of CoT-AFA for future studies. Our dataset and source code is available at https://github.com/MICLAB-BUPT/EFA.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15153v1",
      "code_links": [
        {
          "url": "https://github.com/MICLAB-BUPT/EFA",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]multimodal",
            "[T]chain-of-thought"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "SGM: Safety Glasses for Multimodal Large Language Models via Neuron-Level Detoxification",
      "authors": [
        "Hongbo Wang",
        "MaungMaung AprilPyone",
        "Isao Echizen"
      ],
      "arxiv_id": "2512.15052v1",
      "summary": "Disclaimer: Samples in this paper may be harmful and cause discomfort.\n  Multimodal large language models (MLLMs) enable multimodal generation but inherit toxic, biased, and NSFW signals from weakly curated pretraining corpora, causing safety risks, especially under adversarial triggers that late, opaque training-free detoxification methods struggle to handle. We propose SGM, a white-box neuron-level multimodal intervention that acts like safety glasses for toxic neurons: it selectively recalibrates a small set of toxic expert neurons via expertise-weighted soft suppression, neutralizing harmful cross-modal activations without any parameter updates. We establish MM-TOXIC-QA, a multimodal toxicity evaluation framework, and compare SGM with existing detoxification techniques. Experiments on open-source MLLMs show that SGM mitigates toxicity in standard and adversarial conditions, cutting harmful rates from 48.2\\% to 2.5\\% while preserving fluency and multimodal reasoning. SGM is extensible, and its combined defenses, denoted as SGM*, integrate with existing detoxification methods for stronger safety performance, providing an interpretable, low-cost solution for toxicity-controlled multimodal generation.",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "Under Review for ACL 2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15052v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model",
            "[T]multimodal"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "mimic-video: Video-Action Models for Generalizable Robot Control Beyond VLAs",
      "authors": [
        "Jonas Pai",
        "Liam Achenbach",
        "Victoriano Montesinos",
        "Benedek Forrai",
        "Oier Mees",
        "Elvis Nava"
      ],
      "arxiv_id": "2512.15692v1",
      "summary": "Prevailing Vision-Language-Action Models (VLAs) for robotic manipulation are built upon vision-language backbones pretrained on large-scale, but disconnected static web data. As a result, despite improved semantic generalization, the policy must implicitly infer complex physical dynamics and temporal dependencies solely from robot trajectories. This reliance creates an unsustainable data burden, necessitating continuous, large-scale expert data collection to compensate for the lack of innate physical understanding. We contend that while vision-language pretraining effectively captures semantic priors, it remains blind to physical causality. A more effective paradigm leverages video to jointly capture semantics and visual dynamics during pretraining, thereby isolating the remaining task of low-level control. To this end, we introduce \\model, a novel Video-Action Model (VAM) that pairs a pretrained Internet-scale video model with a flow matching-based action decoder conditioned on its latent representations. The decoder serves as an Inverse Dynamics Model (IDM), generating low-level robot actions from the latent representation of video-space action plans. Our extensive evaluation shows that our approach achieves state-of-the-art performance on simulated and real-world robotic manipulation tasks, improving sample efficiency by 10x and convergence speed by 2x compared to traditional VLA architectures.",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15692v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "flow matching"
          ],
          "score": 1.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "vision-language-action",
            "[T]VLA"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 15.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "QoS-Aware Hierarchical Reinforcement Learning for Joint Link Selection and Trajectory Optimization in SAGIN-Supported UAV Mobility Management",
      "authors": [
        "Jiayang Wan",
        "Ke He",
        "Yafei Wang",
        "Fan Liu",
        "Wenjin Wang",
        "Shi Jin"
      ],
      "arxiv_id": "2512.15119v1",
      "summary": "Due to the significant variations in unmanned aerial vehicle (UAV) altitude and horizontal mobility, it becomes difficult for any single network to ensure continuous and reliable threedimensional coverage. Towards that end, the space-air-ground integrated network (SAGIN) has emerged as an essential architecture for enabling ubiquitous UAV connectivity. To address the pronounced disparities in coverage and signal characteristics across heterogeneous networks, this paper formulates UAV mobility management in SAGIN as a constrained multi-objective joint optimization problem. The formulation couples discrete link selection with continuous trajectory optimization. Building on this, we propose a two-level multi-agent hierarchical deep reinforcement learning (HDRL) framework that decomposes the problem into two alternately solvable subproblems. To map complex link selection decisions into a compact discrete action space, we conceive a double deep Q-network (DDQN) algorithm in the top-level, which achieves stable and high-quality policy learning through double Q-value estimation. To handle the continuous trajectory action space while satisfying quality of service (QoS) constraints, we integrate the maximum-entropy mechanism of the soft actor-critic (SAC) and employ a Lagrangian-based constrained SAC (CSAC) algorithm in the lower-level that dynamically adjusts the Lagrange multipliers to balance constraint satisfaction and policy optimization. Moreover, the proposed algorithm can be extended to multi-UAV scenarios under the centralized training and decentralized execution (CTDE) paradigm, which enables more generalizable policies. Simulation results demonstrate that the proposed scheme substantially outperforms existing benchmarks in throughput, link switching frequency and QoS satisfaction.",
      "categories": [
        "eess.SP",
        "cs.AI"
      ],
      "primary_category": "eess.SP",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "This work has been submitted to the IEEE for possible publication",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15119v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]trajectory optimization"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning",
            "deep reinforcement learning",
            "policy learning",
            "SAC"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 15.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "In Pursuit of Pixel Supervision for Visual Pre-training",
      "authors": [
        "Lihe Yang",
        "Shang-Wen Li",
        "Yang Li",
        "Xinjie Lei",
        "Dong Wang",
        "Abdelrahman Mohamed",
        "Hengshuang Zhao",
        "Hu Xu"
      ],
      "arxiv_id": "2512.15715v1",
      "summary": "At the most basic level, pixels are the source of the visual information through which we perceive the world. Pixels contain information at all levels, ranging from low-level attributes to high-level concepts. Autoencoders represent a classical and long-standing paradigm for learning representations from pixels or other raw inputs. In this work, we demonstrate that autoencoder-based self-supervised learning remains competitive today and can produce strong representations for downstream tasks, while remaining simple, stable, and efficient. Our model, codenamed \"Pixio\", is an enhanced masked autoencoder (MAE) with more challenging pre-training tasks and more capable architectures. The model is trained on 2B web-crawled images with a self-curation strategy with minimal human curation. Pixio performs competitively across a wide range of downstream tasks in the wild, including monocular depth estimation (e.g., Depth Anything), feed-forward 3D reconstruction (i.e., MapAnything), semantic segmentation, and robot learning, outperforming or matching DINOv3 trained at similar scales. Our results suggest that pixel-space self-supervised learning can serve as a promising alternative and a complement to latent-space approaches.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "Project page: https://github.com/facebookresearch/pixio",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15715v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "masked autoencoder",
            "MAE",
            "[T]visual pre-training"
          ],
          "score": 7.5
        },
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "depth estimation",
            "monocular depth",
            "Depth Anything"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 13.5,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "EagleVision: A Dual-Stage Framework with BEV-grounding-based Chain-of-Thought for Spatial Intelligence",
      "authors": [
        "Jiaxu Wan",
        "Xu Wang",
        "Mengwei Xie",
        "Hang Zhang",
        "Mu Xu",
        "Yang Han",
        "Hong Zhang",
        "Ding Yuan",
        "Yifan Yang"
      ],
      "arxiv_id": "2512.15160v1",
      "summary": "Recent spatial intelligence approaches typically attach 3D cues to 2D reasoning pipelines or couple MLLMs with black-box reconstruction modules, leading to weak spatial consistency, limited viewpoint diversity, and evidence chains that cannot be traced back to supporting views. Frameworks for \"thinking with images\" (e.g., ChatGPT-o3 and DeepEyes) show that stepwise multimodal reasoning can emerge by interleaving hypothesis formation with active acquisition of visual evidence, but they do not address three key challenges in spatial Chain-of-Thought (CoT): building global space perception under strict token budgets, explicitly associating 3D hypotheses with video frames for verification, and designing spatially grounded rewards for reinforcement learning. To address these issues, we present EagleVision, a dual-stage framework for progressive spatial cognition through macro perception and micro verification. In the macro perception stage, EagleVision employs a semantics-perspective-fusion determinantal point process (SPF-DPP) to select a compact set of geometry- and semantics-aware keyframes from long videos under a fixed token budget. In the micro verification stage, we formalize spatial CoT as BEV-grounded pose querying: the agent iteratively predicts poses on a BEV plane, retrieves the nearest real frames, and is trained purely by reinforcement learning with a spatial grounding reward that scores the consistency between predicted poses and observed views. On VSI-Bench, EagleVision achieves state-of-the-art performance among open-source vision-language models, demonstrating strong and generalizable spatial understanding.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "13 pages, 7 figures, 6 tables",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15160v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "multimodal",
            "[T]chain-of-thought"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 13.5,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Beyond Fast and Slow: Cognitive-Inspired Elastic Reasoning for Large Language Models",
      "authors": [
        "Jinwu Hu",
        "Dongjin Yang",
        "Langyu Bian",
        "Zhiquan Wen",
        "Yufeng Wang",
        "Yaofo Chen",
        "Bin Xiao",
        "Yuanqing Li",
        "Mingkui Tan"
      ],
      "arxiv_id": "2512.15089v1",
      "summary": "Large language models (LLMs) have demonstrated impressive performance across various language tasks. However, existing LLM reasoning strategies mainly rely on the LLM itself with fast or slow mode (like o1 thinking) and thus struggle to balance reasoning efficiency and accuracy across queries of varying difficulties. In this paper, we propose Cognitive-Inspired Elastic Reasoning (CogER), a framework inspired by human hierarchical reasoning that dynamically selects the most suitable reasoning strategy for each query. Specifically, CogER first assesses the complexity of incoming queries and assigns them to one of several predefined levels, each corresponding to a tailored processing strategy, thereby addressing the challenge of unobservable query difficulty. To achieve automatic strategy selection, we model the process as a Markov Decision Process and train a CogER-Agent using reinforcement learning. The agent is guided by a reward function that balances solution quality and computational cost, ensuring resource-efficient reasoning. Moreover, for queries requiring external tools, we introduce Cognitive Tool-Assisted Reasoning, which enables the LLM to autonomously invoke external tools within its chain-of-thought. Extensive experiments demonstrate that CogER outperforms state-of-the-art Test-Time scaling methods, achieving at least a 13% relative improvement in average exact match on In-Domain tasks and an 8% relative gain on Out-of-Domain tasks.",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "under review",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15089v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model",
            "chain-of-thought"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 13.5,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Multi-View Foundation Models",
      "authors": [
        "Leo Segre",
        "Or Hirschorn",
        "Shai Avidan"
      ],
      "arxiv_id": "2512.15708v1",
      "summary": "Foundation models are vital tools in various Computer Vision applications. They take as input a single RGB image and output a deep feature representation that is useful for various applications. However, in case we have multiple views of the same 3D scene, they operate on each image independently and do not always produce consistent features for the same 3D point. We propose a way to convert a Foundation Model into a Multi-View Foundation Model. Such a model takes as input a set of images and outputs a feature map for each image such that the features of corresponding points are as consistent as possible. This approach bypasses the need to build a consistent 3D model of the features and allows direct manipulation in the image space. Specifically, we show how to augment Transformers-based foundation models (i.e., DINO, SAM, CLIP) with intermediate 3D-aware attention layers that help match features across different views. As leading examples, we show surface normal estimation and multi-view segmentation tasks. Quantitative experiments show that our method improves feature matching considerably compared to current foundation models.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15708v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱六：视频提取与匹配 (Video Extraction)",
          "id": "6_video_extraction",
          "matched_keywords": [
            "feature matching"
          ],
          "score": 2.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]foundation model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 13.0,
      "hit_pillars": [
        "1_robot_core",
        "6_video_extraction",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "OMCL: Open-vocabulary Monte Carlo Localization",
      "authors": [
        "Evgenii Kruzhkov",
        "Raphael Memmesheimer",
        "Sven Behnke"
      ],
      "arxiv_id": "2512.15557v1",
      "summary": "Robust robot localization is an important prerequisite for navigation planning. If the environment map was created from different sensors, robot measurements must be robustly associated with map features. In this work, we extend Monte Carlo Localization using vision-language features. These open-vocabulary features enable to robustly compute the likelihood of visual observations, given a camera pose and a 3D map created from posed RGB-D images or aligned point clouds. The abstract vision-language features enable to associate observations and map elements from different modalities. Global localization can be initialized by natural language descriptions of the objects present in the vicinity of locations. We evaluate our approach using Matterport3D and Replica for indoor scenes and demonstrate generalization on SemanticKITTI for outdoor scenes.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "Accepted to IEEE RA-L",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15557v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]open-vocabulary",
            "[T]open vocabulary"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "MiVLA: Towards Generalizable Vision-Language-Action Model with Human-Robot Mutual Imitation Pre-training",
      "authors": [
        "Zhenhan Yin",
        "Xuanhan Wang",
        "Jiahao Jiang",
        "Kaiyuan Deng",
        "Pengqi Chen",
        "Shuangle Li",
        "Chong Liu",
        "Xing Xu",
        "ingkuan Song",
        "Lianli Gao",
        "Heng Tao Shen"
      ],
      "arxiv_id": "2512.15411v1",
      "summary": "While leveraging abundant human videos and simulated robot data poses a scalable solution to the scarcity of real-world robot data, the generalization capability of existing vision-language-action models (VLAs) remains limited by mismatches in camera views, visual appearance, and embodiment morphologies. To overcome this limitation, we propose MiVLA, a generalizable VLA empowered by human-robot mutual imitation pre-training, which leverages inherent behavioral similarity between human hands and robotic arms to build a foundation of strong behavioral priors for both human actions and robotic control. Specifically, our method utilizes kinematic rules with left/right hand coordinate systems for bidirectional alignment between human and robot action spaces. Given human or simulated robot demonstrations, MiVLA is trained to forecast behavior trajectories for one embodiment, and imitate behaviors for another one unseen in the demonstration. Based on this mutual imitation, it integrates the behavioral fidelity of real-world human data with the manipulative diversity of simulated robot data into a unified model, thereby enhancing the generalization capability for downstream tasks. Extensive experiments conducted on both simulation and real-world platforms with three robots (ARX, PiPer and LocoMan), demonstrate that MiVLA achieves strong improved generalization capability, outperforming state-of-the-art VLAs (e.g., $\\boldsymbolπ_{0}$, $\\boldsymbolπ_{0.5}$ and H-RDT) by 25% in simulation, and 14% in real-world robot control tasks.",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15411v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "VLA"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "NAP3D: NeRF Assisted 3D-3D Pose Alignment for Autonomous Vehicles",
      "authors": [
        "Gaurav Bansal"
      ],
      "arxiv_id": "2512.15080v1",
      "summary": "Accurate localization is essential for autonomous vehicles, yet sensor noise and drift over time can lead to significant pose estimation errors, particularly in long-horizon environments. A common strategy for correcting accumulated error is visual loop closure in SLAM, which adjusts the pose graph when the agent revisits previously mapped locations. These techniques typically rely on identifying visual mappings between the current view and previously observed scenes and often require fusing data from multiple sensors.\n  In contrast, this work introduces NeRF-Assisted 3D-3D Pose Alignment (NAP3D), a complementary approach that leverages 3D-3D correspondences between the agent's current depth image and a pre-trained Neural Radiance Field (NeRF). By directly aligning 3D points from the observed scene with synthesized points from the NeRF, NAP3D refines the estimated pose even from novel viewpoints, without relying on revisiting previously observed locations.\n  This robust 3D-3D formulation provides advantages over conventional 2D-3D localization methods while remaining comparable in accuracy and applicability. Experiments demonstrate that NAP3D achieves camera pose correction within 5 cm on a custom dataset, robustly outperforming a 2D-3D Perspective-N-Point baseline. On TUM RGB-D, NAP3D consistently improves 3D alignment RMSE by approximately 6 cm compared to this baseline given varying noise, despite PnP achieving lower raw rotation and translation parameter error in some regimes, highlighting NAP3D's improved geometric consistency in 3D space. By providing a lightweight, dataset-agnostic tool, NAP3D complements existing SLAM and localization pipelines when traditional loop closure is unavailable.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "10 pages, 5 figures, 2 tables",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15080v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]NeRF",
            "neural radiance field"
          ],
          "score": 8.0
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "geometric consistency"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 11.0,
      "hit_pillars": [
        "3_perception_slam",
        "7_retargeting"
      ]
    },
    {
      "title": "MVGSR: Multi-View Consistent 3D Gaussian Super-Resolution via Epipolar Guidance",
      "authors": [
        "Kaizhe Zhang",
        "Shinan Chen",
        "Qian Zhao",
        "Weizhan Zhang",
        "Caixia Yan",
        "Yudeng Xin"
      ],
      "arxiv_id": "2512.15048v1",
      "summary": "Scenes reconstructed by 3D Gaussian Splatting (3DGS) trained on low-resolution (LR) images are unsuitable for high-resolution (HR) rendering. Consequently, a 3DGS super-resolution (SR) method is needed to bridge LR inputs and HR rendering. Early 3DGS SR methods rely on single-image SR networks, which lack cross-view consistency and fail to fuse complementary information across views. More recent video-based SR approaches attempt to address this limitation but require strictly sequential frames, limiting their applicability to unstructured multi-view datasets. In this work, we introduce Multi-View Consistent 3D Gaussian Splatting Super-Resolution (MVGSR), a framework that focuses on integrating multi-view information for 3DGS rendering with high-frequency details and enhanced consistency. We first propose an Auxiliary View Selection Method based on camera poses, making our method adaptable for arbitrarily organized multi-view datasets without the need of temporal continuity or data reordering. Furthermore, we introduce, for the first time, an epipolar-constrained multi-view attention mechanism into 3DGS SR, which serves as the core of our proposed multi-view SR network. This design enables the model to selectively aggregate consistent information from auxiliary views, enhancing the geometric consistency and detail fidelity of 3DGS representations. Extensive experiments demonstrate that our method achieves state-of-the-art performance on both object-centric and scene-level 3DGS SR benchmarks.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "9 pages, 7 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15048v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "3D gaussian splatting",
            "3DGS",
            "gaussian splatting",
            "splatting"
          ],
          "score": 8.0
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "geometric consistency"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 11.0,
      "hit_pillars": [
        "3_perception_slam",
        "7_retargeting"
      ]
    },
    {
      "title": "How Many Heads Make an SSM? A Unified Framework for Attention and State Space Models",
      "authors": [
        "Ali Ghodsi"
      ],
      "arxiv_id": "2512.15115v1",
      "summary": "Sequence modeling has produced diverse architectures -- from classical recurrent neural networks to modern Transformers and state space models (SSMs) -- yet a unified theoretical understanding of expressivity and trainability trade-offs remains limited. We introduce a unified framework that represents a broad class of sequence maps via an input-dependent effective interaction operator $W_{ij}(X)$, making explicit two recurring construction patterns: (i) the Unified Factorized Framework (Explicit) (attention-style mixing), in which $W_{ij}(X)$ varies through scalar coefficients applied to shared value maps, and (ii) Structured Dynamics (Implicit) (state-space recurrences), in which $W_{ij}$ is induced by a latent dynamical system. Using this framework, we derive three theoretical results. First, we establish the Interaction Rank Gap: models in the Unified Factorized Framework, such as single-head attention, are constrained to a low-dimensional operator span and cannot represent certain structured dynamical maps. Second, we prove an Equivalence (Head-Count) Theorem showing that, within our multi-head factorized class, representing a linear SSM whose lag operators span a $k$-dimensional subspace on length-$n$ sequences requires and is achievable with $H=k$ heads. Third, we prove a Gradient Highway Result, showing that attention layers admit inputs with distance-independent gradient paths, whereas stable linear dynamics exhibit distance-dependent gradient attenuation. Together, these results formalize a fundamental trade-off between algebraic expressivity (interaction/operator span) and long-range gradient propagation, providing theoretical grounding for modern sequence architecture design.",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15115v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "latent dynamics",
            "[T]SSM",
            "[T]state space model"
          ],
          "score": 10.5
        }
      ],
      "relevance_score": 10.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "ISS Policy : Scalable Diffusion Policy with Implicit Scene Supervision",
      "authors": [
        "Wenlong Xia",
        "Jinhao Zhang",
        "Ce Zhang",
        "Yaojia Wang",
        "Youmin Gong",
        "Jie Mei"
      ],
      "arxiv_id": "2512.15020v1",
      "summary": "Vision-based imitation learning has enabled impressive robotic manipulation skills, but its reliance on object appearance while ignoring the underlying 3D scene structure leads to low training efficiency and poor generalization. To address these challenges, we introduce \\emph{Implicit Scene Supervision (ISS) Policy}, a 3D visuomotor DiT-based diffusion policy that predicts sequences of continuous actions from point cloud observations. We extend DiT with a novel implicit scene supervision module that encourages the model to produce outputs consistent with the scene's geometric evolution, thereby improving the performance and robustness of the policy. Notably, ISS Policy achieves state-of-the-art performance on both single-arm manipulation tasks (MetaWorld) and dexterous hand manipulation (Adroit). In real-world experiments, it also demonstrates strong generalization and robustness. Additional ablation studies show that our method scales effectively with both data and parameters. Code and videos will be released.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15020v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "dexterous hand"
          ],
          "score": 4.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "imitation learning",
            "[T]diffusion policy"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 10.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "Can LLMs Guide Their Own Exploration? Gradient-Guided Reinforcement Learning for LLM Reasoning",
      "authors": [
        "Zhenwen Liang",
        "Sidi Lu",
        "Wenhao Yu",
        "Kishan Panaganti",
        "Yujun Zhou",
        "Haitao Mi",
        "Dong Yu"
      ],
      "arxiv_id": "2512.15687v1",
      "summary": "Reinforcement learning has become essential for strengthening the reasoning abilities of large language models, yet current exploration mechanisms remain fundamentally misaligned with how these models actually learn. Entropy bonuses and external semantic comparators encourage surface level variation but offer no guarantee that sampled trajectories differ in the update directions that shape optimization. We propose G2RL, a gradient guided reinforcement learning framework in which exploration is driven not by external heuristics but by the model own first order update geometry. For each response, G2RL constructs a sequence level feature from the model final layer sensitivity, obtainable at negligible cost from a standard forward pass, and measures how each trajectory would reshape the policy by comparing these features within a sampled group. Trajectories that introduce novel gradient directions receive a bounded multiplicative reward scaler, while redundant or off manifold updates are deemphasized, yielding a self referential exploration signal that is naturally aligned with PPO style stability and KL control. Across math and general reasoning benchmarks (MATH500, AMC, AIME24, AIME25, GPQA, MMLUpro) on Qwen3 base 1.7B and 4B models, G2RL consistently improves pass@1, maj@16, and pass@k over entropy based GRPO and external embedding methods. Analyzing the induced geometry, we find that G2RL expands exploration into substantially more orthogonal and often opposing gradient directions while maintaining semantic coherence, revealing that a policy own update space provides a far more faithful and effective basis for guiding exploration in large language model reinforcement learning.",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15687v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning",
            "PPO"
          ],
          "score": 6.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Evaluating Large Language Models in Scientific Discovery",
      "authors": [
        "Zhangde Song",
        "Jieyu Lu",
        "Yuanqi Du",
        "Botao Yu",
        "Thomas M. Pruyn",
        "Yue Huang",
        "Kehan Guo",
        "Xiuzhe Luo",
        "Yuanhao Qu",
        "Yi Qu",
        "Yinkai Wang",
        "Haorui Wang",
        "Jeff Guo",
        "Jingru Gan",
        "Parshin Shojaee",
        "Di Luo",
        "Andres M Bran",
        "Gen Li",
        "Qiyuan Zhao",
        "Shao-Xiong Lennon Luo",
        "Yuxuan Zhang",
        "Xiang Zou",
        "Wanru Zhao",
        "Yifan F. Zhang",
        "Wucheng Zhang",
        "Shunan Zheng",
        "Saiyang Zhang",
        "Sartaaj Takrim Khan",
        "Mahyar Rajabi-Kochi",
        "Samantha Paradi-Maropakis",
        "Tony Baltoiu",
        "Fengyu Xie",
        "Tianyang Chen",
        "Kexin Huang",
        "Weiliang Luo",
        "Meijing Fang",
        "Xin Yang",
        "Lixue Cheng",
        "Jiajun He",
        "Soha Hassoun",
        "Xiangliang Zhang",
        "Wei Wang",
        "Chandan K. Reddy",
        "Chao Zhang",
        "Zhiling Zheng",
        "Mengdi Wang",
        "Le Cong",
        "Carla P. Gomes",
        "Chang-Yu Hsieh",
        "Aditya Nandy",
        "Philippe Schwaller",
        "Heather J. Kulik",
        "Haojun Jia",
        "Huan Sun",
        "Seyed Mohamad Moosavi",
        "Chenru Duan"
      ],
      "arxiv_id": "2512.15567v1",
      "summary": "Large language models (LLMs) are increasingly applied to scientific research, yet prevailing science benchmarks probe decontextualized knowledge and overlook the iterative reasoning, hypothesis generation, and observation interpretation that drive scientific discovery. We introduce a scenario-grounded benchmark that evaluates LLMs across biology, chemistry, materials, and physics, where domain experts define research projects of genuine interest and decompose them into modular research scenarios from which vetted questions are sampled. The framework assesses models at two levels: (i) question-level accuracy on scenario-tied items and (ii) project-level performance, where models must propose testable hypotheses, design simulations or experiments, and interpret results. Applying this two-phase scientific discovery evaluation (SDE) framework to state-of-the-art LLMs reveals a consistent performance gap relative to general science benchmarks, diminishing return of scaling up model sizes and reasoning, and systematic weaknesses shared across top-tier models from different providers. Large performance variation in research scenarios leads to changing choices of the best performing model on scientific discovery projects evaluated, suggesting all current LLMs are distant to general scientific \"superintelligence\". Nevertheless, LLMs already demonstrate promise in a great variety of scientific discovery projects, including cases where constituent scenario scores are low, highlighting the role of guided exploration and serendipity in discovery. This SDE framework offers a reproducible benchmark for discovery-relevant evaluation of LLMs and charts practical paths to advance their development toward scientific discovery.",
      "categories": [
        "cs.AI",
        "cond-mat.mtrl-sci",
        "cs.LG",
        "physics.chem-ph"
      ],
      "primary_category": "cs.AI",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15567v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Autonomous Pressure Control in MuVacAS via Deep Reinforcement Learning and Deep Learning Surrogate Models",
      "authors": [
        "Guillermo Rodriguez-Llorente",
        "Galo Gallardo",
        "Rodrigo Morant Navascués",
        "Nikita Khvatkin Petrovsky",
        "Anderson Sabogal",
        "Roberto Gómez-Espinosa Martín"
      ],
      "arxiv_id": "2512.15521v1",
      "summary": "The development of nuclear fusion requires materials that can withstand extreme conditions. The IFMIF-DONES facility, a high-power particle accelerator, is being designed to qualify these materials. A critical testbed for its development is the MuVacAS prototype, which replicates the final segment of the accelerator beamline. Precise regulation of argon gas pressure within its ultra-high vacuum chamber is vital for this task. This work presents a fully data-driven approach for autonomous pressure control. A Deep Learning Surrogate Model, trained on real operational data, emulates the dynamics of the argon injection system. This high-fidelity digital twin then serves as a fast-simulation environment to train a Deep Reinforcement Learning agent. The results demonstrate that the agent successfully learns a control policy that maintains gas pressure within strict operational limits despite dynamic disturbances. This approach marks a significant step toward the intelligent, autonomous control systems required for the demanding next-generation particle accelerator facilities.",
      "categories": [
        "physics.acc-ph",
        "cs.LG"
      ],
      "primary_category": "physics.acc-ph",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "13 pages, 7 figures, included in Machine Learning and the Physical Sciences Workshop @ NeurIPS 2025",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15521v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning",
            "[T]deep reinforcement learning"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Copyright Infringement Risk Reduction via Chain-of-Thought and Task Instruction Prompting",
      "authors": [
        "Neeraj Sarna",
        "Yuanyuan Li",
        "Michael von Gablenz"
      ],
      "arxiv_id": "2512.15442v1",
      "summary": "Large scale text-to-image generation models can memorize and reproduce their training dataset. Since the training dataset often contains copyrighted material, reproduction of training dataset poses a copyright infringement risk, which could result in legal liabilities and financial losses for both the AI user and the developer. The current works explores the potential of chain-of-thought and task instruction prompting in reducing copyrighted content generation. To this end, we present a formulation that combines these two techniques with two other copyright mitigation strategies: a) negative prompting, and b) prompt re-writing. We study the generated images in terms their similarity to a copyrighted image and their relevance of the user input. We present numerical experiments on a variety of models and provide insights on the effectiveness of the aforementioned techniques for varying model complexity.",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15442v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]chain-of-thought"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Explaining the Reasoning of Large Language Models Using Attribution Graphs",
      "authors": [
        "Chase Walker",
        "Rickard Ewetz"
      ],
      "arxiv_id": "2512.15663v1",
      "summary": "Large language models (LLMs) exhibit remarkable capabilities, yet their reasoning remains opaque, raising safety and trust concerns. Attribution methods, which assign credit to input features, have proven effective for explaining the decision making of computer vision models. From these, context attributions have emerged as a promising approach for explaining the behavior of autoregressive LLMs. However, current context attributions produce incomplete explanations by directly relating generated tokens to the prompt, discarding inter-generational influence in the process. To overcome these shortcomings, we introduce the Context Attribution via Graph Explanations (CAGE) framework. CAGE introduces an attribution graph: a directed graph that quantifies how each generation is influenced by both the prompt and all prior generations. The graph is constructed to preserve two properties-causality and row stochasticity. The attribution graph allows context attributions to be computed by marginalizing intermediate contributions along paths in the graph. Across multiple models, datasets, metrics, and methods, CAGE improves context attribution faithfulness, achieving average gains of up to 40%.",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15663v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "MCP-SafetyBench: A Benchmark for Safety Evaluation of Large Language Models with Real-World MCP Servers",
      "authors": [
        "Xuanjun Zong",
        "Zhiqi Shen",
        "Lei Wang",
        "Yunshi Lan",
        "Chao Yang"
      ],
      "arxiv_id": "2512.15163v1",
      "summary": "Large language models (LLMs) are evolving into agentic systems that reason, plan, and operate external tools. The Model Context Protocol (MCP) is a key enabler of this transition, offering a standardized interface for connecting LLMs with heterogeneous tools and services. Yet MCP's openness and multi-server workflows introduce new safety risks that existing benchmarks fail to capture, as they focus on isolated attacks or lack real-world coverage. We present MCP-SafetyBench, a comprehensive benchmark built on real MCP servers that supports realistic multi-turn evaluation across five domains: browser automation, financial analysis, location navigation, repository management, and web search. It incorporates a unified taxonomy of 20 MCP attack types spanning server, host, and user sides, and includes tasks requiring multi-step reasoning and cross-server coordination under uncertainty. Using MCP-SafetyBench, we systematically evaluate leading open- and closed-source LLMs, revealing large disparities in safety performance and escalating vulnerabilities as task horizons and server interactions grow. Our results highlight the urgent need for stronger defenses and establish MCP-SafetyBench as a foundation for diagnosing and mitigating safety risks in real-world MCP deployments.",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "Our benchmark is available at https://github.com/xjzzzzzzzz/MCPSafety",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15163v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Beyond Accuracy: A Geometric Stability Analysis of Large Language Models in Chess Evaluation",
      "authors": [
        "Xidan Song",
        "Weiqi Wang",
        "Ruifeng Cao",
        "Qingya Hu"
      ],
      "arxiv_id": "2512.15033v1",
      "summary": "The evaluation of Large Language Models (LLMs) in complex reasoning domains typically relies on performance alignment with ground-truth oracles. In the domain of chess, this standard manifests as accuracy benchmarks against strong engines like Stockfish. However, high scalar accuracy does not necessarily imply robust conceptual understanding. This paper argues that standard accuracy metrics fail to distinguish between genuine geometric reasoning and the superficial memorization of canonical board states. To address this gap, we propose a Geometric Stability Framework, a novel evaluation methodology that rigorously tests model consistency under invariant transformations-including board rotation, mirror symmetry, color inversion, and format conversion. We applied this framework to a comparative analysis of six state-of-the-art LLMs including GPT-5.1, Claude Sonnet 4.5, and Kimi K2 Turbo, utilizing a dataset of approximately 3,000 positions. Our results reveal a significant Accuracy-Stability Paradox. While models such as GPT-5.1 achieve near-optimal accuracy on standard positions, they exhibit catastrophic degradation under geometric perturbation, specifically in rotation tasks where error rates surge by over 600%. This disparity suggests a reliance on pattern matching over abstract spatial logic. Conversely, Claude Sonnet 4.5 and Kimi K2 Turbo demonstrate superior dual robustness, maintaining high consistency across all transformation axes. Furthermore, we analyze the trade-off between helpfulness and safety, identifying Gemini 2.5 Flash as the leader in illegal state rejection (96.0%). We conclude that geometric stability provides an orthogonal and essential metric for AI evaluation, offering a necessary proxy for disentangling reasoning capabilities from data contamination and overfitting in large-scale models.",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15033v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Exact Learning of Linear Model Predictive Control Laws using Oblique Decision Trees with Linear Predictions",
      "authors": [
        "Jiayang Ren",
        "Qiangqiang Mao",
        "Tianwei Zhao",
        "Yankai Cao"
      ],
      "arxiv_id": "2512.15568v1",
      "summary": "Model Predictive Control (MPC) is a powerful strategy for constrained multivariable systems but faces computational challenges in real-time deployment due to its online optimization requirements. While explicit MPC and neural network approximations mitigate this burden, they suffer from scalability issues or lack interpretability, limiting their applicability in safety-critical systems. This work introduces a data-driven framework that directly learns the Linear MPC control law from sampled state-action pairs using Oblique Decision Trees with Linear Predictions (ODT-LP), achieving both computational efficiency and interpretability. By leveraging the piecewise affine structure of Linear MPC, we prove that the Linear MPC control law can be replicated by finite-depth ODT-LP models. We develop a gradient-based training algorithm using smooth approximations of tree routing functions to learn this structure from grid-sampled Linear MPC solutions, enabling end-to-end optimization. Input-to-state stability is established under bounded approximation errors, with explicit error decomposition into learning inaccuracies and sampling errors to inform model design. Numerical experiments demonstrate that ODT-LP controllers match MPC's closed-loop performance while reducing online evaluation time by orders of magnitude compared to MPC, explicit MPC, neural network, and random forest counterparts. The transparent tree structure enables formal verification of control logic, bridging the gap between computational efficiency and certifiable reliability for safety-critical systems.",
      "categories": [
        "math.OC",
        "eess.SY"
      ],
      "primary_category": "math.OC",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "6 pages, 4 figures, accepted by and presented at the 64th IEEE Conference on Decision and Control (CDC) in December 2025",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15568v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "MPC",
            "[T]model predictive control"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Preserving Marker Specificity with Lightweight Channel-Independent Representation Learning",
      "authors": [
        "Simon Gutwein",
        "Arthur Longuefosse",
        "Jun Seita",
        "Sabine Taschner-Mandl",
        "Roxane Licandro"
      ],
      "arxiv_id": "2512.15410v1",
      "summary": "Multiplexed tissue imaging measures dozens of protein markers per cell, yet most deep learning models still apply early channel fusion, assuming shared structure across markers. We investigate whether preserving marker independence, combined with deliberately shallow architectures, provides a more suitable inductive bias for self-supervised representation learning in multiplex data than increasing model scale. Using a Hodgkin lymphoma CODEX dataset with 145,000 cells and 49 markers, we compare standard early-fusion CNNs with channel-separated architectures, including a marker-aware baseline and our novel shallow Channel-Independent Model (CIM-S) with 5.5K parameters. After contrastive pretraining and linear evaluation, early-fusion models show limited ability to retain marker-specific information and struggle particularly with rare-cell discrimination. Channel-independent architectures, and CIM-S in particular, achieve substantially stronger representations despite their compact size. These findings are consistent across multiple self-supervised frameworks, remain stable across augmentation settings, and are reproducible across both the 49-marker and reduced 18-marker settings. These results show that lightweight, channel-independent architectures can match or surpass deep early-fusion CNNs and foundation models for multiplex representation learning. Code is available at https://github.com/SimonBon/CIM-S.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "16 pages, 9 figures, MIDL 2026 conference",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15410v1",
      "code_links": [
        {
          "url": "https://github.com/SimonBon/CIM-S",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]representation learning"
          ],
          "score": 4.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "foundation model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 7.5,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "MMMamba: A Versatile Cross-Modal In Context Fusion Framework for Pan-Sharpening and Zero-Shot Image Enhancement",
      "authors": [
        "Yingying Wang",
        "Xuanhua He",
        "Chen Wu",
        "Jialing Huang",
        "Suiyun Zhang",
        "Rui Liu",
        "Xinghao Ding",
        "Haoxuan Che"
      ],
      "arxiv_id": "2512.15261v1",
      "summary": "Pan-sharpening aims to generate high-resolution multispectral (HRMS) images by integrating a high-resolution panchromatic (PAN) image with its corresponding low-resolution multispectral (MS) image. To achieve effective fusion, it is crucial to fully exploit the complementary information between the two modalities. Traditional CNN-based methods typically rely on channel-wise concatenation with fixed convolutional operators, which limits their adaptability to diverse spatial and spectral variations. While cross-attention mechanisms enable global interactions, they are computationally inefficient and may dilute fine-grained correspondences, making it difficult to capture complex semantic relationships. Recent advances in the Multimodal Diffusion Transformer (MMDiT) architecture have demonstrated impressive success in image generation and editing tasks. Unlike cross-attention, MMDiT employs in-context conditioning to facilitate more direct and efficient cross-modal information exchange. In this paper, we propose MMMamba, a cross-modal in-context fusion framework for pan-sharpening, with the flexibility to support image super-resolution in a zero-shot manner. Built upon the Mamba architecture, our design ensures linear computational complexity while maintaining strong cross-modal interaction capacity. Furthermore, we introduce a novel multimodal interleaved (MI) scanning mechanism that facilitates effective information exchange between the PAN and MS modalities. Extensive experiments demonstrate the superior performance of our method compared to existing state-of-the-art (SOTA) techniques across multiple tasks and benchmarks.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "\\link{Code}{https://github.com/Gracewangyy/MMMamba}",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15261v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]Mamba"
          ],
          "score": 4.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "multimodal"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 7.5,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Well Begun, Half Done: Reinforcement Learning with Prefix Optimization for LLM Reasoning",
      "authors": [
        "Yiliu Sun",
        "Zicheng Zhao",
        "Yang Wei",
        "Yanfang Zhang",
        "Chen Gong"
      ],
      "arxiv_id": "2512.15274v1",
      "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) significantly enhances the reasoning capability of Large Language Models (LLMs). Current RLVR approaches typically conduct training across all generated tokens, but neglect to explore which tokens (e.g., prefix tokens) actually contribute to reasoning. This uniform training strategy spends substantial effort on optimizing low-return tokens, which in turn impedes the potential improvement from high-return tokens and reduces overall training effectiveness. To address this issue, we propose a novel RLVR approach called Progressive Prefix-token Policy Optimization (PPPO), which highlights the significance of the prefix segment of generated outputs. Specifically, inspired by the well-established human thinking theory of Path Dependence, where early-stage thoughts substantially constrain subsequent thinking trajectory, we identify an analogous phenomenon in LLM reasoning termed Beginning Lock-in Effect (BLE). PPPO leverages this finding by focusing its optimization objective on the prefix reasoning process of LLMs. This targeted optimization strategy can positively influence subsequent reasoning processes, and ultimately improve final results. To improve the learning effectiveness of LLMs on how to start reasoning with high quality, PPPO introduces two training strategies: (a) Progressive Prefix Retention, which shapes a progressive learning process by increasing the proportion of retained prefix tokens during training; (b) Continuation Accumulated Reward, which mitigates reward bias by sampling multiple continuations for one prefix token sequence, and accumulating their scores as the reward signal. Extensive experimental results on various reasoning tasks demonstrate that our proposed PPPO outperforms representative RLVR methods, with the accuracy improvements of 18.02% on only 26.17% training tokens.",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "Accepted by AAAI 2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15274v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 7.5,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Characterizing Mamba's Selective Memory using Auto-Encoders",
      "authors": [
        "Tamanna Hossain",
        "Robert L. Logan",
        "Ganesh Jagadeesan",
        "Sameer Singh",
        "Joel Tetreault",
        "Alejandro Jaimes"
      ],
      "arxiv_id": "2512.15653v1",
      "summary": "State space models (SSMs) are a promising alternative to transformers for language modeling because they use fixed memory during inference. However, this fixed memory usage requires some information loss in the hidden state when processing long sequences. While prior work has studied the sequence length at which this information loss occurs, it does not characterize the types of information SSM language models (LMs) tend to forget. In this paper, we address this knowledge gap by identifying the types of tokens (e.g., parts of speech, named entities) and sequences (e.g., code, math problems) that are more frequently forgotten by SSM LMs. We achieve this by training an auto-encoder to reconstruct sequences from the SSM's hidden state, and measure information loss by comparing inputs with their reconstructions. We perform experiments using the Mamba family of SSM LMs (130M--1.4B) on sequences ranging from 4--256 tokens. Our results show significantly higher rates of information loss on math-related tokens (e.g., numbers, variables), mentions of organization entities, and alternative dialects to Standard American English. We then examine the frequency that these tokens appear in Mamba's pretraining data and find that less prevalent tokens tend to be the ones Mamba is most likely to forget. By identifying these patterns, our work provides clear direction for future research to develop methods that better control Mamba's ability to retain important information.",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "AACL 2025. Oral Presentation",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15653v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]Mamba",
            "SSM",
            "state space model"
          ],
          "score": 7.5
        }
      ],
      "relevance_score": 7.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Beyond Majority Voting: Towards Fine-grained and More Reliable Reward Signal for Test-Time Reinforcement Learning",
      "authors": [
        "Weiqin Wang",
        "Yile Wang",
        "Kehao Chen",
        "Hui Huang"
      ],
      "arxiv_id": "2512.15146v1",
      "summary": "Test-time reinforcement learning mitigates the reliance on annotated data by using majority voting results as pseudo-labels, emerging as a complementary direction to reinforcement learning with verifiable rewards (RLVR) for improving reasoning ability of large language models (LLMs). However, this voting strategy often induces confirmation bias and suffers from sparse rewards, limiting the overall performance. In this work, we propose subgroup-specific step-wise confidence-weighted pseudo-label estimation (SCOPE), a framework integrating model confidence and dynamic subgroup partitioning to address these issues. Specifically, SCOPE integrates the proposed step-wise confidence into pseudo label deduction, prioritizing high-quality reasoning paths over simple frequency count. Furthermore, it dynamically partitions the candidate outputs pool into independent subgroups by balancing reasoning quality against exploration diversity. By deriving local consensus via repeat sampling for each sub group, SCOPE provides diverse supervision targets to encourage broader exploration. We conduct experiments across various models and benchmarks, experimental results show that SCOPE consistently outperforms recent baselines. Notably, SCOPE achieving relative improvements of 13.1\\% on challenging AIME 2025 and 8.1\\% on AMC. The code is released at \\href{https://github.com/szu-tera/SCOPE}{https://github.com/szu-tera/SCOPE}.",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15146v1",
      "code_links": [
        {
          "url": "https://github.com/szu-tera/SCOPE",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 7.5,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Photorealistic Phantom Roads in Real Scenes: Disentangling 3D Hallucinations from Physical Geometry",
      "authors": [
        "Hoang Nguyen",
        "Xiaohao Xu",
        "Xiaonan Huang"
      ],
      "arxiv_id": "2512.15423v1",
      "summary": "Monocular depth foundation models achieve remarkable generalization by learning large-scale semantic priors, but this creates a critical vulnerability: they hallucinate illusory 3D structures from geometrically planar but perceptually ambiguous inputs. We term this failure the 3D Mirage. This paper introduces the first end-to-end framework to probe, quantify, and tame this unquantified safety risk. To probe, we present 3D-Mirage, the first benchmark of real-world illusions (e.g., street art) with precise planar-region annotations and context-restricted crops. To quantify, we propose a Laplacian-based evaluation framework with two metrics: the Deviation Composite Score (DCS) for spurious non-planarity and the Confusion Composite Score (CCS) for contextual instability. To tame this failure, we introduce Grounded Self-Distillation, a parameter-efficient strategy that surgically enforces planarity on illusion ROIs while using a frozen teacher to preserve background knowledge, thus avoiding catastrophic forgetting. Our work provides the essential tools to diagnose and mitigate this phenomenon, urging a necessary shift in MDE evaluation from pixel-wise accuracy to structural and contextual robustness. Our code and benchmark will be publicly available to foster this exciting research direction.",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15423v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "distillation"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "monocular depth"
          ],
          "score": 2.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "foundation model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 6.5,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "LADY: Linear Attention for Autonomous Driving Efficiency without Transformers",
      "authors": [
        "Jihao Huang",
        "Xi Xia",
        "Zhiyuan Li",
        "Tianle Liu",
        "Jingke Wang",
        "Junbo Chen",
        "Tengju Ye"
      ],
      "arxiv_id": "2512.15038v1",
      "summary": "End-to-end paradigms have demonstrated great potential for autonomous driving. Additionally, most existing methods are built upon Transformer architectures. However, transformers incur a quadratic attention cost, limiting their ability to model long spatial and temporal sequences-particularly on resource-constrained edge platforms. As autonomous driving inherently demands efficient temporal modeling, this challenge severely limits their deployment and real-time performance. Recently, linear attention mechanisms have gained increasing attention due to their superior spatiotemporal complexity. However, existing linear attention architectures are limited to self-attention, lacking support for cross-modal and cross-temporal interactions-both crucial for autonomous driving. In this work, we propose LADY, the first fully linear attention-based generative model for end-to-end autonomous driving. LADY enables fusion of long-range temporal context at inference with constant computational and memory costs, regardless of the history length of camera and LiDAR features. Additionally, we introduce a lightweight linear cross-attention mechanism that enables effective cross-modal information exchange. Experiments on the NAVSIM and Bench2Drive benchmarks demonstrate that LADY achieves state-of-the-art performance with constant-time and memory complexity, offering improved planning performance and significantly reduced computational cost. Additionally, the model has been deployed and validated on edge devices, demonstrating its practicality in resource-limited scenarios.",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "Under review",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15038v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]linear attention"
          ],
          "score": 4.5
        },
        {
          "name": "支柱八：物理动画 (Physics-based Animation)",
          "id": "8_physics_animation",
          "matched_keywords": [
            "spatiotemporal"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 6.5,
      "hit_pillars": [
        "2_algo_arch",
        "8_physics_animation"
      ]
    },
    {
      "title": "GuangMing-Explorer: A Four-Legged Robot Platform for Autonomous Exploration in General Environments",
      "authors": [
        "Kai Zhang",
        "Shoubin Chen",
        "Dong Li",
        "Baiyang Zhang",
        "Tao Huang",
        "Zehao Wu",
        "Jiasheng Chen",
        "Bo Zhang"
      ],
      "arxiv_id": "2512.15309v1",
      "summary": "Autonomous exploration is a fundamental capability that tightly integrates perception, planning, control, and motion execution. It plays a critical role in a wide range of applications, including indoor target search, mapping of extreme environments, resource exploration, etc. Despite significant progress in individual components, a holistic and practical description of a completely autonomous exploration system, encompassing both hardware and software, remains scarce. In this paper, we present GuangMing-Explorer, a fully integrated autonomous exploration platform designed for robust operation across diverse environments. We provide a comprehensive overview of the system architecture, including hardware design, software stack, algorithm deployment, and experimental configuration. Extensive real-world experiments demonstrate the platform's effectiveness and efficiency in executing autonomous exploration tasks, highlighting its potential for practical deployment in complex and unstructured environments.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "6 pages, published in ICUS2025",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15309v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]legged robot"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "BEV-Patch-PF: Particle Filtering with BEV-Aerial Feature Matching for Off-Road Geo-Localization",
      "authors": [
        "Dongmyeong Lee",
        "Jesse Quattrociocchi",
        "Christian Ellis",
        "Rwik Rana",
        "Amanda Adkins",
        "Adam Uccello",
        "Garrett Warnell",
        "Joydeep Biswas"
      ],
      "arxiv_id": "2512.15111v1",
      "summary": "We propose BEV-Patch-PF, a GPS-free sequential geo-localization system that integrates a particle filter with learned bird's-eye-view (BEV) and aerial feature maps. From onboard RGB and depth images, we construct a BEV feature map. For each 3-DoF particle pose hypothesis, we crop the corresponding patch from an aerial feature map computed from a local aerial image queried around the approximate location. BEV-Patch-PF computes a per-particle log-likelihood by matching the BEV feature to the aerial patch feature. On two real-world off-road datasets, our method achieves 7.5x lower absolute trajectory error (ATE) on seen routes and 7.0x lower ATE on unseen routes than a retrieval-based baseline, while maintaining accuracy under dense canopy and shadow. The system runs in real time at 10 Hz on an NVIDIA Tesla T4, enabling practical robot deployment.",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15111v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱六：视频提取与匹配 (Video Extraction)",
          "id": "6_video_extraction",
          "matched_keywords": [
            "[T]feature matching"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "6_video_extraction"
      ]
    },
    {
      "title": "Gaussian Pixel Codec Avatars: A Hybrid Representation for Efficient Rendering",
      "authors": [
        "Divam Gupta",
        "Anuj Pahuja",
        "Nemanja Bartolovic",
        "Tomas Simon",
        "Forrest Iandola",
        "Giljoo Nam"
      ],
      "arxiv_id": "2512.15711v1",
      "summary": "We present Gaussian Pixel Codec Avatars (GPiCA), photorealistic head avatars that can be generated from multi-view images and efficiently rendered on mobile devices. GPiCA utilizes a unique hybrid representation that combines a triangle mesh and anisotropic 3D Gaussians. This combination maximizes memory and rendering efficiency while maintaining a photorealistic appearance. The triangle mesh is highly efficient in representing surface areas like facial skin, while the 3D Gaussians effectively handle non-surface areas such as hair and beard. To this end, we develop a unified differentiable rendering pipeline that treats the mesh as a semi-transparent layer within the volumetric rendering paradigm of 3D Gaussian Splatting. We train neural networks to decode a facial expression code into three components: a 3D face mesh, an RGBA texture, and a set of 3D Gaussians. These components are rendered simultaneously in a unified rendering engine. The networks are trained using multi-view image supervision. Our results demonstrate that GPiCA achieves the realism of purely Gaussian-based avatars while matching the rendering performance of mesh-based avatars.",
      "categories": [
        "cs.CV",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "Tech report",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15711v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "3D gaussian splatting",
            "gaussian splatting",
            "splatting"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Skyra: AI-Generated Video Detection via Grounded Artifact Reasoning",
      "authors": [
        "Yifei Li",
        "Wenzhao Zheng",
        "Yanran Zhang",
        "Runze Sun",
        "Yu Zheng",
        "Lei Chen",
        "Jie Zhou",
        "Jiwen Lu"
      ],
      "arxiv_id": "2512.15693v1",
      "summary": "The misuse of AI-driven video generation technologies has raised serious social concerns, highlighting the urgent need for reliable AI-generated video detectors. However, most existing methods are limited to binary classification and lack the necessary explanations for human interpretation. In this paper, we present Skyra, a specialized multimodal large language model (MLLM) that identifies human-perceivable visual artifacts in AI-generated videos and leverages them as grounded evidence for both detection and explanation. To support this objective, we construct ViF-CoT-4K for Supervised Fine-Tuning (SFT), which represents the first large-scale AI-generated video artifact dataset with fine-grained human annotations. We then develop a two-stage training strategy that systematically enhances our model's spatio-temporal artifact perception, explanation capability, and detection accuracy. To comprehensively evaluate Skyra, we introduce ViF-Bench, a benchmark comprising 3K high-quality samples generated by over ten state-of-the-art video generators. Extensive experiments demonstrate that Skyra surpasses existing methods across multiple benchmarks, while our evaluation yields valuable insights for advancing explainable AI-generated video detection.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "Project Page: https://github.com/JoeLeelyf/Skyra",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15693v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model",
            "multimodal"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "GRAN-TED: Generating Robust, Aligned, and Nuanced Text Embedding for Diffusion Models",
      "authors": [
        "Bozhou Li",
        "Sihan Yang",
        "Yushuo Guan",
        "Ruichuan An",
        "Xinlong Chen",
        "Yang Shi",
        "Pengfei Wan",
        "Wentao Zhang",
        "Yuanxing zhang"
      ],
      "arxiv_id": "2512.15560v1",
      "summary": "The text encoder is a critical component of text-to-image and text-to-video diffusion models, fundamentally determining the semantic fidelity of the generated content. However, its development has been hindered by two major challenges: the lack of an efficient evaluation framework that reliably predicts downstream generation performance, and the difficulty of effectively adapting pretrained language models for visual synthesis. To address these issues, we introduce GRAN-TED, a paradigm to Generate Robust, Aligned, and Nuanced Text Embeddings for Diffusion models. Our contribution is twofold. First, we propose TED-6K, a novel text-only benchmark that enables efficient and robust assessment of an encoder's representational quality without requiring costly end-to-end model training. We demonstrate that performance on TED-6K, standardized via a lightweight, unified adapter, strongly correlates with an encoder's effectiveness in downstream generation tasks. Second, guided by this validated framework, we develop a superior text encoder using a novel two-stage training paradigm. This process involves an initial fine-tuning stage on a Multimodal Large Language Model for better visual representation, followed by a layer-wise weighting method to extract more nuanced and potent text features. Our experiments show that the resulting GRAN-TED encoder not only achieves state-of-the-art performance on TED-6K but also leads to demonstrable performance gains in text-to-image and text-to-video generation. Our code is available at the following link: https://anonymous.4open.science/r/GRAN-TED-4FCC/.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15560v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model",
            "multimodal"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "EmoCaliber: Advancing Reliable Visual Emotion Comprehension via Confidence Verbalization and Calibration",
      "authors": [
        "Daiqing Wu",
        "Dongbao Yang",
        "Can Ma. Yu Zhou"
      ],
      "arxiv_id": "2512.15528v1",
      "summary": "Visual Emotion Comprehension (VEC) aims to infer sentiment polarities or emotion categories from affective cues embedded in images. In recent years, Multimodal Large Language Models (MLLMs) have established a popular paradigm in VEC, leveraging their generalizability to unify VEC tasks defined under diverse emotion taxonomies. While this paradigm achieves notable success, it typically formulates VEC as a deterministic task, requiring the model to output a single, definitive emotion label for each image. Such a formulation insufficiently accounts for the inherent subjectivity of emotion perception, overlooking alternative interpretations that may be equally plausible to different viewers. To address this limitation, we propose equipping MLLMs with capabilities to verbalize their confidence in emotion predictions. This additional signal provides users with an estimate of both the plausibility of alternative interpretations and the MLLMs' self-assessed competence, thereby enhancing reliability in practice. Building on this insight, we introduce a three-stage training framework that progressively endows with structured reasoning, teaches to verbalize confidence, and calibrates confidence expression, culminating in EmoCaliber, a confidence-aware MLLM for VEC. Through fair and comprehensive evaluations on the unified benchmark VECBench, EmoCaliber demonstrates overall superiority against existing methods in both emotion prediction and confidence estimation. These results validate the effectiveness of our approach and mark a feasible step toward more reliable VEC systems. Project page: https://github.com/wdqqdw/EmoCaliber.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15528v1",
      "code_links": [
        {
          "url": "https://github.com/wdqqdw/EmoCaliber",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model",
            "multimodal"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "VAAS: Vision-Attention Anomaly Scoring for Image Manipulation Detection in Digital Forensics",
      "authors": [
        "Opeyemi Bamigbade",
        "Mark Scanlon",
        "John Sheppard"
      ],
      "arxiv_id": "2512.15512v1",
      "summary": "Recent advances in AI-driven image generation have introduced new challenges for verifying the authenticity of digital evidence in forensic investigations. Modern generative models can produce visually consistent forgeries that evade traditional detectors based on pixel or compression artefacts. Most existing approaches also lack an explicit measure of anomaly intensity, which limits their ability to quantify the severity of manipulation. This paper introduces Vision-Attention Anomaly Scoring (VAAS), a novel dual-module framework that integrates global attention-based anomaly estimation using Vision Transformers (ViT) with patch-level self-consistency scoring derived from SegFormer embeddings. The hybrid formulation provides a continuous and interpretable anomaly score that reflects both the location and degree of manipulation. Evaluations on the DF2023 and CASIA v2.0 datasets demonstrate that VAAS achieves competitive F1 and IoU performance, while enhancing visual explainability through attention-guided anomaly maps. The framework bridges quantitative detection with human-understandable reasoning, supporting transparent and reliable image integrity assessment. The source code for all experiments and corresponding materials for reproducing the results are available open source.",
      "categories": [
        "cs.CV",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15512v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "ST-DETrack: Identity-Preserving Branch Tracking in Entangled Plant Canopies via Dual Spatiotemporal Evidence",
      "authors": [
        "Yueqianji Chen",
        "Kevin Williams",
        "John H. Doonan",
        "Paolo Remagnino",
        "Jo Hepworth"
      ],
      "arxiv_id": "2512.15445v1",
      "summary": "Automated extraction of individual plant branches from time-series imagery is essential for high-throughput phenotyping, yet it remains computationally challenging due to non-rigid growth dynamics and severe identity fragmentation within entangled canopies. To overcome these stage-dependent ambiguities, we propose ST-DETrack, a spatiotemporal-fusion dual-decoder network designed to preserve branch identity from budding to flowering. Our architecture integrates a spatial decoder, which leverages geometric priors such as position and angle for early-stage tracking, with a temporal decoder that exploits motion consistency to resolve late-stage occlusions. Crucially, an adaptive gating mechanism dynamically shifts reliance between these spatial and temporal cues, while a biological constraint based on negative gravitropism mitigates vertical growth ambiguities. Validated on a Brassica napus dataset, ST-DETrack achieves a Branch Matching Accuracy (BMA) of 93.6%, significantly outperforming spatial and temporal baselines by 28.9 and 3.3 percentage points, respectively. These results demonstrate the method's robustness in maintaining long-term identity consistency amidst complex, dynamic plant architectures.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "Under Review at IEEE Transactions on Image Processing",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15445v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱八：物理动画 (Physics-based Animation)",
          "id": "8_physics_animation",
          "matched_keywords": [
            "[T]spatiotemporal"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "8_physics_animation"
      ]
    },
    {
      "title": "Step-GUI Technical Report",
      "authors": [
        "Haolong Yan",
        "Jia Wang",
        "Xin Huang",
        "Yeqing Shen",
        "Ziyang Meng",
        "Zhimin Fan",
        "Kaijun Tan",
        "Jin Gao",
        "Lieyu Shi",
        "Mi Yang",
        "Shiliang Yang",
        "Zhirui Wang",
        "Brian Li",
        "Kang An",
        "Chenyang Li",
        "Lei Lei",
        "Mengmeng Duan",
        "Danxun Liang",
        "Guodong Liu",
        "Hang Cheng",
        "Hao Wu",
        "Jie Dong",
        "Junhao Huang",
        "Mei Chen",
        "Renjie Yu",
        "Shunshan Li",
        "Xu Zhou",
        "Yiting Dai",
        "Yineng Deng",
        "Yingdan Liang",
        "Zelin Chen",
        "Wen Sun",
        "Chengxu Yan",
        "Chunqin Xu",
        "Dong Li",
        "Fengqiong Xiao",
        "Guanghao Fan",
        "Guopeng Li",
        "Guozhen Peng",
        "Hongbing Li",
        "Hang Li",
        "Hongming Chen",
        "Jingjing Xie",
        "Jianyong Li",
        "Jingyang Zhang",
        "Jiaju Ren",
        "Jiayu Yuan",
        "Jianpeng Yin",
        "Kai Cao",
        "Liang Zhao",
        "Liguo Tan",
        "Liying Shi",
        "Mengqiang Ren",
        "Min Xu",
        "Manjiao Liu",
        "Mao Luo",
        "Mingxin Wan",
        "Na Wang",
        "Nan Wu",
        "Ning Wang",
        "Peiyao Ma",
        "Qingzhou Zhang",
        "Qiao Wang",
        "Qinlin Zeng",
        "Qiong Gao",
        "Qiongyao Li",
        "Shangwu Zhong",
        "Shuli Gao",
        "Shaofan Liu",
        "Shisi Gao",
        "Shuang Luo",
        "Xingbin Liu",
        "Xiaojia Liu",
        "Xiaojie Hou",
        "Xin Liu",
        "Xuanti Feng",
        "Xuedan Cai",
        "Xuan Wen",
        "Xianwei Zhu",
        "Xin Liang",
        "Xin Liu",
        "Xin Zhou",
        "Yingxiu Zhao",
        "Yukang Shi",
        "Yunfang Xu",
        "Yuqing Zeng",
        "Yixun Zhang",
        "Zejia Weng",
        "Zhonghao Yan",
        "Zhiguo Huang",
        "Zhuoyu Wang",
        "Zheng Ge",
        "Jing Li",
        "Yibo Zhu",
        "Binxing Jiao",
        "Xiangyu Zhang",
        "Daxin Jiang"
      ],
      "arxiv_id": "2512.15431v1",
      "summary": "Recent advances in multimodal large language models unlock unprecedented opportunities for GUI automation. However, a fundamental challenge remains: how to efficiently acquire high-quality training data while maintaining annotation reliability? We introduce a self-evolving training pipeline powered by the Calibrated Step Reward System, which converts model-generated trajectories into reliable training signals through trajectory-level calibration, achieving >90% annotation accuracy with 10-100x lower cost. Leveraging this pipeline, we introduce Step-GUI, a family of models (4B/8B) that achieves state-of-the-art GUI performance (8B: 80.2% AndroidWorld, 48.5% OSWorld, 62.6% ScreenShot-Pro) while maintaining robust general capabilities. As GUI agent capabilities improve, practical deployment demands standardized interfaces across heterogeneous devices while protecting user privacy. To this end, we propose GUI-MCP, the first Model Context Protocol for GUI automation with hierarchical architecture that combines low-level atomic operations and high-level task delegation to local specialist models, enabling high-privacy execution where sensitive data stays on-device. Finally, to assess whether agents can handle authentic everyday usage, we introduce AndroidDaily, a benchmark grounded in real-world mobile usage patterns with 3146 static actions and 235 end-to-end tasks across high-frequency daily scenarios (8B: static 89.91%, end-to-end 52.50%). Our work advances the development of practical GUI agents and demonstrates strong potential for real-world deployment in everyday digital interactions.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "41 pages, 26 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15431v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model",
            "multimodal"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Autoregressive Language Models are Secretly Energy-Based Models: Insights into the Lookahead Capabilities of Next-Token Prediction",
      "authors": [
        "Mathieu Blondel",
        "Michael E. Sander",
        "Germain Vivier-Ardisson",
        "Tianlin Liu",
        "Vincent Roulet"
      ],
      "arxiv_id": "2512.15605v1",
      "summary": "Autoregressive models (ARMs) currently constitute the dominant paradigm for large language models (LLMs). Energy-based models (EBMs) represent another class of models, which have historically been less prevalent in LLM development, yet naturally characterize the optimal policy in post-training alignment. In this paper, we provide a unified view of these two model classes. Taking the chain rule of probability as a starting point, we establish an explicit bijection between ARMs and EBMs in function space, which we show to correspond to a special case of the soft Bellman equation in maximum entropy reinforcement learning. Building upon this bijection, we derive the equivalence between supervised learning of ARMs and EBMs. Furthermore, we analyze the distillation of EBMs into ARMs by providing theoretical error bounds. Our results provide insights into the ability of ARMs to plan ahead, despite being based on the next-token prediction paradigm.",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15605v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "distillation"
          ],
          "score": 3.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Automatic Reward Shaping from Multi-Objective Human Heuristics",
      "authors": [
        "Yuqing Xie",
        "Jiayu Chen",
        "Wenhao Tang",
        "Ya Zhang",
        "Chao Yu",
        "Yu Wang"
      ],
      "arxiv_id": "2512.15120v1",
      "summary": "Designing effective reward functions remains a central challenge in reinforcement learning, especially in multi-objective environments. In this work, we propose Multi-Objective Reward Shaping with Exploration (MORSE), a general framework that automatically combines multiple human-designed heuristic rewards into a unified reward function. MORSE formulates the shaping process as a bi-level optimization problem: the inner loop trains a policy to maximize the current shaped reward, while the outer loop updates the reward function to optimize task performance. To encourage exploration in the reward space and avoid suboptimal local minima, MORSE introduces stochasticity into the shaping process, injecting noise guided by task performance and the prediction error of a fixed, randomly initialized neural network. Experimental results in MuJoCo and Isaac Sim environments show that MORSE effectively balances multiple objectives across various robotic tasks, achieving task performance comparable to those obtained with manually tuned reward functions.",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15120v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "[T]reward shaping"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "ChatGPT and Gemini participated in the Korean College Scholastic Ability Test -- Earth Science I",
      "authors": [
        "Seok-Hyun Ga",
        "Chun-Yen Chang"
      ],
      "arxiv_id": "2512.15298v1",
      "summary": "The rapid development of Generative AI is bringing innovative changes to education and assessment. As the prevalence of students utilizing AI for assignments increases, concerns regarding academic integrity and the validity of assessments are growing. This study utilizes the Earth Science I section of the 2025 Korean College Scholastic Ability Test (CSAT) to deeply analyze the multimodal scientific reasoning capabilities and cognitive limitations of state-of-the-art Large Language Models (LLMs), including GPT-4o, Gemini 2.5 Flash, and Gemini 2.5 Pro. Three experimental conditions (full-page input, individual item input, and optimized multimodal input) were designed to evaluate model performance across different data structures. Quantitative results indicated that unstructured inputs led to significant performance degradation due to segmentation and Optical Character Recognition (OCR) failures. Even under optimized conditions, models exhibited fundamental reasoning flaws. Qualitative analysis revealed that \"Perception Errors\" were dominant, highlighting a \"Perception-Cognition Gap\" where models failed to interpret symbolic meanings in schematic diagrams despite recognizing visual data. Furthermore, models demonstrated a \"Calculation-Conceptualization Discrepancy,\" successfully performing calculations while failing to apply the underlying scientific concepts, and \"Process Hallucination,\" where models skipped visual verification in favor of plausible but unfounded background knowledge. Addressing the challenge of unauthorized AI use in coursework, this study provides actionable cues for designing \"AI-resistant questions\" that target these specific cognitive vulnerabilities. By exploiting AI's weaknesses, such as the gap between perception and cognition, educators can distinguish genuine student competency from AI-generated responses, thereby ensuring assessment fairness.",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "23 pages, 9 tables, 1 figure",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15298v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model",
            "multimodal"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Dual-Density Inference for Efficient Language Model Reasoning",
      "authors": [
        "Zhengyi Zhao",
        "Shubo Zhang",
        "Yuxi Zhang",
        "Huimin Wang",
        "Binyang Li",
        "Kam-Fai Wong"
      ],
      "arxiv_id": "2512.15358v1",
      "summary": "Large Language Models (LLMs) have shown impressive capabilities in complex reasoning tasks. However, current approaches employ uniform language density for both intermediate reasoning and final answers, leading to computational inefficiency. Our observation found that reasoning process serves a computational function for the model itself, while answering serves a communicative function for human understanding. This distinction enables the use of compressed, symbol-rich language for intermediate computations while maintaining human-readable final explanations. To address this inefficiency, we present Denser: \\underline{D}ual-d\\underline{ens}ity inf\\underline{er}ence, a novel framework that optimizes information density separately for reasoning and answering phases. Our framework implements this through three components: a query processing module that analyzes input problems, a high-density compressed reasoning mechanism for efficient intermediate computations, and an answer generation component that translates compressed reasoning into human-readable solutions. Experimental evaluation across multiple reasoning question answering benchmarks demonstrates that Denser reduces token consumption by up to 62\\% compared to standard Chain-of-Thought methods while preserving or improving accuracy. These efficiency gains are particularly significant for complex multi-step reasoning problems where traditional methods generate extensive explanations.",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15358v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model",
            "chain-of-thought"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Enhancing industrial microalgae production through Economic Model Predictive Control",
      "authors": [
        "Pablo Otálora",
        "Sigurd Skogestad",
        "José Luis Guzmán",
        "Manuel Berenguel"
      ],
      "arxiv_id": "2512.15668v1",
      "summary": "The industrial production of microalgae is an important and sustainable process, but its actual competitiveness is closely related to its optimization. The biological nature of the process hinders this task, mainly due to the high nonlinearity of the process along with its changing nature, features that make its modeling, control and optimization remarkably challenging. This paper presents an economic optimization framework aiming to enhance the operation of such systems. An Economic Model Predictive Controller is proposed, centralizing the decision making and achieving the theoretical optimal operation. Different scenarios with changing climate conditions are presented, and a comparison with the typical, non-optimized industrial process operation is established. The obtained results achieve economic optimization and dynamic stability of the process, while providing some insight into the priorities during process operation at industrial level, and justifying the use of optimal controllers over traditional operation.",
      "categories": [
        "eess.SY"
      ],
      "primary_category": "eess.SY",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15668v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]model predictive control"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "GateFusion: Hierarchical Gated Cross-Modal Fusion for Active Speaker Detection",
      "authors": [
        "Yu Wang",
        "Juhyung Ha",
        "Frangil M. Ramirez",
        "Yuchen Wang",
        "David J. Crandall"
      ],
      "arxiv_id": "2512.15707v1",
      "summary": "Active Speaker Detection (ASD) aims to identify who is currently speaking in each frame of a video. Most state-of-the-art approaches rely on late fusion to combine visual and audio features, but late fusion often fails to capture fine-grained cross-modal interactions, which can be critical for robust performance in unconstrained scenarios. In this paper, we introduce GateFusion, a novel architecture that combines strong pretrained unimodal encoders with a Hierarchical Gated Fusion Decoder (HiGate). HiGate enables progressive, multi-depth fusion by adaptively injecting contextual features from one modality into the other at multiple layers of the Transformer backbone, guided by learnable, bimodally-conditioned gates. To further strengthen multimodal learning, we propose two auxiliary objectives: Masked Alignment Loss (MAL) to align unimodal outputs with multimodal predictions, and Over-Positive Penalty (OPP) to suppress spurious video-only activations. GateFusion establishes new state-of-the-art results on several challenging ASD benchmarks, achieving 77.8% mAP (+9.4%), 86.1% mAP (+2.9%), and 96.1% mAP (+0.5%) on Ego4D-ASD, UniTalk, and WASD benchmarks, respectively, and delivering competitive performance on AVA-ActiveSpeaker. Out-of-domain experiments demonstrate the generalization of our model, while comprehensive ablations show the complementary benefits of each component.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "accepted by WACV 2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15707v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱六：视频提取与匹配 (Video Extraction)",
          "id": "6_video_extraction",
          "matched_keywords": [
            "Ego4D"
          ],
          "score": 2.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "multimodal"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 5.0,
      "hit_pillars": [
        "6_video_extraction",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "IC-Effect: Precise and Efficient Video Effects Editing via In-Context Learning",
      "authors": [
        "Yuanhang Li",
        "Yiren Song",
        "Junzhe Bai",
        "Xinran Liang",
        "Hu Yang",
        "Libiao Jin",
        "Qi Mao"
      ],
      "arxiv_id": "2512.15635v1",
      "summary": "We propose \\textbf{IC-Effect}, an instruction-guided, DiT-based framework for few-shot video VFX editing that synthesizes complex effects (\\eg flames, particles and cartoon characters) while strictly preserving spatial and temporal consistency. Video VFX editing is highly challenging because injected effects must blend seamlessly with the background, the background must remain entirely unchanged, and effect patterns must be learned efficiently from limited paired data. However, existing video editing models fail to satisfy these requirements. IC-Effect leverages the source video as clean contextual conditions, exploiting the contextual learning capability of DiT models to achieve precise background preservation and natural effect injection. A two-stage training strategy, consisting of general editing adaptation followed by effect-specific learning via Effect-LoRA, ensures strong instruction following and robust effect modeling. To further improve efficiency, we introduce spatiotemporal sparse tokenization, enabling high fidelity with substantially reduced computation. We also release a paired VFX editing dataset spanning $15$ high-quality visual styles. Extensive experiments show that IC-Effect delivers high-quality, controllable, and temporally consistent VFX editing, opening new possibilities for video creation.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15635v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱八：物理动画 (Physics-based Animation)",
          "id": "8_physics_animation",
          "matched_keywords": [
            "spatiotemporal"
          ],
          "score": 2.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "instruction following"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 5.0,
      "hit_pillars": [
        "8_physics_animation",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "IMKD: Intensity-Aware Multi-Level Knowledge Distillation for Camera-Radar Fusion",
      "authors": [
        "Shashank Mishra",
        "Karan Patil",
        "Didier Stricker",
        "Jason Rambach"
      ],
      "arxiv_id": "2512.15581v1",
      "summary": "High-performance Radar-Camera 3D object detection can be achieved by leveraging knowledge distillation without using LiDAR at inference time. However, existing distillation methods typically transfer modality-specific features directly to each sensor, which can distort their unique characteristics and degrade their individual strengths. To address this, we introduce IMKD, a radar-camera fusion framework based on multi-level knowledge distillation that preserves each sensor's intrinsic characteristics while amplifying their complementary strengths. IMKD applies a three-stage, intensity-aware distillation strategy to enrich the fused representation across the architecture: (1) LiDAR-to-Radar intensity-aware feature distillation to enhance radar representations with fine-grained structural cues, (2) LiDAR-to-Fused feature intensity-guided distillation to selectively highlight useful geometry and depth information at the fusion level, fostering complementarity between the modalities rather than forcing them to align, and (3) Camera-Radar intensity-guided fusion mechanism that facilitates effective feature alignment and calibration. Extensive experiments on the nuScenes benchmark show that IMKD reaches 67.0% NDS and 61.0% mAP, outperforming all prior distillation-based radar-camera fusion methods. Our code and models are available at https://github.com/dfki-av/IMKD/.",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "Accepted at IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 2026. 22 pages, 8 figures. Includes supplementary material",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15581v1",
      "code_links": [
        {
          "url": "https://github.com/dfki-av/IMKD/",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]distillation"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "MoonSeg3R: Monocular Online Zero-Shot Segment Anything in 3D with Reconstructive Foundation Priors",
      "authors": [
        "Zhipeng Du",
        "Duolikun Danier",
        "Jan Eric Lenssen",
        "Hakan Bilen"
      ],
      "arxiv_id": "2512.15577v1",
      "summary": "In this paper, we focus on online zero-shot monocular 3D instance segmentation, a novel practical setting where existing approaches fail to perform because they rely on posed RGB-D sequences. To overcome this limitation, we leverage CUT3R, a recent Reconstructive Foundation Model (RFM), to provide reliable geometric priors from a single RGB stream. We propose MoonSeg3R, which introduces three key components: (1) a self-supervised query refinement module with spatial-semantic distillation that transforms segmentation masks from 2D visual foundation models (VFMs) into discriminative 3D queries; (2) a 3D query index memory that provides temporal consistency by retrieving contextual queries; and (3) a state-distribution token from CUT3R that acts as a mask identity descriptor to strengthen cross-frame fusion. Experiments on ScanNet200 and SceneNN show that MoonSeg3R is the first method to enable online monocular 3D segmentation and achieves performance competitive with state-of-the-art RGB-D-based systems. Code and models will be released.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15577v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "distillation"
          ],
          "score": 1.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "foundation model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "SMART: Semantic Matching Contrastive Learning for Partially View-Aligned Clustering",
      "authors": [
        "Liang Peng",
        "Yixuan Ye",
        "Cheng Liu",
        "Hangjun Che",
        "Fei Wang",
        "Zhiwen Yu",
        "Si Wu",
        "Hau-San Wong"
      ],
      "arxiv_id": "2512.15396v1",
      "summary": "Multi-view clustering has been empirically shown to improve learning performance by leveraging the inherent complementary information across multiple views of data. However, in real-world scenarios, collecting strictly aligned views is challenging, and learning from both aligned and unaligned data becomes a more practical solution. Partially View-aligned Clustering aims to learn correspondences between misaligned view samples to better exploit the potential consistency and complementarity across views, including both aligned and unaligned data. However, most existing PVC methods fail to leverage unaligned data to capture the shared semantics among samples from the same cluster. Moreover, the inherent heterogeneity of multi-view data induces distributional shifts in representations, leading to inaccuracies in establishing meaningful correspondences between cross-view latent features and, consequently, impairing learning effectiveness. To address these challenges, we propose a Semantic MAtching contRasTive learning model (SMART) for PVC. The main idea of our approach is to alleviate the influence of cross-view distributional shifts, thereby facilitating semantic matching contrastive learning to fully exploit semantic relationships in both aligned and unaligned data. Extensive experiments on eight benchmark datasets demonstrate that our method consistently outperforms existing approaches on the PVC problem.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15396v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]contrastive learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "A Teacher-Student Perspective on the Dynamics of Learning Near the Optimal Point",
      "authors": [
        "Carlos Couto",
        "José Mourão",
        "Mário A. T. Figueiredo",
        "Pedro Ribeiro"
      ],
      "arxiv_id": "2512.15606v1",
      "summary": "Near an optimal learning point of a neural network, the learning performance of gradient descent dynamics is dictated by the Hessian matrix of the loss function with respect to the network parameters. We characterize the Hessian eigenspectrum for some classes of teacher-student problems, when the teacher and student networks have matching weights, showing that the smaller eigenvalues of the Hessian determine long-time learning performance. For linear networks, we analytically establish that for large networks the spectrum asymptotically follows a convolution of a scaled chi-square distribution with a scaled Marchenko-Pastur distribution. We numerically analyse the Hessian spectrum for polynomial and other non-linear networks. Furthermore, we show that the rank of the Hessian matrix can be seen as an effective number of parameters for networks using polynomial activation functions. For a generic non-linear activation function, such as the error function, we empirically observe that the Hessian matrix is always full rank.",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "25 pages, 9 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15606v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]teacher-student"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "EUBRL: Epistemic Uncertainty Directed Bayesian Reinforcement Learning",
      "authors": [
        "Jianfei Ma",
        "Wee Sun Lee"
      ],
      "arxiv_id": "2512.15405v1",
      "summary": "At the boundary between the known and the unknown, an agent inevitably confronts the dilemma of whether to explore or to exploit. Epistemic uncertainty reflects such boundaries, representing systematic uncertainty due to limited knowledge. In this paper, we propose a Bayesian reinforcement learning (RL) algorithm, $\\texttt{EUBRL}$, which leverages epistemic guidance to achieve principled exploration. This guidance adaptively reduces per-step regret arising from estimation errors. We establish nearly minimax-optimal regret and sample complexity guarantees for a class of sufficiently expressive priors in infinite-horizon discounted MDPs. Empirically, we evaluate $\\texttt{EUBRL}$ on tasks characterized by sparse rewards, long horizons, and stochasticity. Results demonstrate that $\\texttt{EUBRL}$ achieves superior sample efficiency, scalability, and consistency.",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15405v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Distillation-Guided Structural Transfer for Continual Learning Beyond Sparse Distributed Memory",
      "authors": [
        "Huiyan Xue",
        "Xuming Ran",
        "Yaxin Li",
        "Qi Xu",
        "Enhui Li",
        "Yi Xu",
        "Qiang Zhang"
      ],
      "arxiv_id": "2512.15267v1",
      "summary": "Sparse neural systems are gaining traction for efficient continual learning due to their modularity and low interference. Architectures such as Sparse Distributed Memory Multi-Layer Perceptrons (SDMLP) construct task-specific subnetworks via Top-K activation and have shown resilience against catastrophic forgetting. However, their rigid modularity limits cross-task knowledge reuse and leads to performance degradation under high sparsity. We propose Selective Subnetwork Distillation (SSD), a structurally guided continual learning framework that treats distillation not as a regularizer but as a topology-aligned information conduit. SSD identifies neurons with high activation frequency and selectively distills knowledge within previous Top-K subnetworks and output logits, without requiring replay or task labels. This enables structural realignment while preserving sparse modularity. Experiments on Split CIFAR-10, CIFAR-100, and MNIST demonstrate that SSD improves accuracy, retention, and representation coverage, offering a structurally grounded solution for sparse continual learning.",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15267v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]distillation"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "TrajSyn: Privacy-Preserving Dataset Distillation from Federated Model Trajectories for Server-Side Adversarial Training",
      "authors": [
        "Mukur Gupta",
        "Niharika Gupta",
        "Saifur Rahman",
        "Shantanu Pal",
        "Chandan Karmakar"
      ],
      "arxiv_id": "2512.15123v1",
      "summary": "Deep learning models deployed on edge devices are increasingly used in safety-critical applications. However, their vulnerability to adversarial perturbations poses significant risks, especially in Federated Learning (FL) settings where identical models are distributed across thousands of clients. While adversarial training is a strong defense, it is difficult to apply in FL due to strict client-data privacy constraints and the limited compute available on edge devices. In this work, we introduce TrajSyn, a privacy-preserving framework that enables effective server-side adversarial training by synthesizing a proxy dataset from the trajectories of client model updates, without accessing raw client data. We show that TrajSyn consistently improves adversarial robustness on image classification benchmarks with no extra compute burden on the client device.",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15123v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]distillation"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Feature-Centric Unsupervised Node Representation Learning Without Homophily Assumption",
      "authors": [
        "Sunwoo Kim",
        "Soo Yong Lee",
        "Kyungho Kim",
        "Hyunjin Hwang",
        "Jaemin Yoo",
        "Kijung Shin"
      ],
      "arxiv_id": "2512.15112v1",
      "summary": "Unsupervised node representation learning aims to obtain meaningful node embeddings without relying on node labels. To achieve this, graph convolution, which aggregates information from neighboring nodes, is commonly employed to encode node features and graph topology. However, excessive reliance on graph convolution can be suboptimal-especially in non-homophilic graphs-since it may yield unduly similar embeddings for nodes that differ in their features or topological properties. As a result, adjusting the degree of graph convolution usage has been actively explored in supervised learning settings, whereas such approaches remain underexplored in unsupervised scenarios. To tackle this, we propose FUEL, which adaptively learns the adequate degree of graph convolution usage by aiming to enhance intra-class similarity and inter-class separability in the embedding space. Since classes are unknown, FUEL leverages node features to identify node clusters and treats these clusters as proxies for classes. Through extensive experiments using 15 baseline methods and 14 benchmark datasets, we demonstrate the effectiveness of FUEL in downstream tasks, achieving state-of-the-art performance across graphs with diverse levels of homophily.",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "Published in AAAI 2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15112v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]representation learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Spectral Representation-based Reinforcement Learning",
      "authors": [
        "Chenxiao Gao",
        "Haotian Sun",
        "Na Li",
        "Dale Schuurmans",
        "Bo Dai"
      ],
      "arxiv_id": "2512.15036v1",
      "summary": "In real-world applications with large state and action spaces, reinforcement learning (RL) typically employs function approximations to represent core components like the policies, value functions, and dynamics models. Although powerful approximations such as neural networks offer great expressiveness, they often present theoretical ambiguities, suffer from optimization instability and exploration difficulty, and incur substantial computational costs in practice. In this paper, we introduce the perspective of spectral representations as a solution to address these difficulties in RL. Stemming from the spectral decomposition of the transition operator, this framework yields an effective abstraction of the system dynamics for subsequent policy optimization while also providing a clear theoretical characterization. We reveal how to construct spectral representations for transition operators that possess latent variable structures or energy-based structures, which implies different learning methods to extract spectral representations from data. Notably, each of these learning methods realizes an effective RL algorithm under this framework. We also provably extend this spectral view to partially observable MDPs. Finally, we validate these algorithms on over 20 challenging tasks from the DeepMind Control Suite, where they achieve performances comparable or superior to current state-of-the-art model-free and model-based baselines.",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15036v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Stepwise Think-Critique: A Unified Framework for Robust and Interpretable LLM Reasoning",
      "authors": [
        "Jiaqi Xu",
        "Cuiling Lan",
        "Xuejin Chen",
        "Yan LU"
      ],
      "arxiv_id": "2512.15662v1",
      "summary": "Human beings solve complex problems through critical thinking, where reasoning and evaluation are intertwined to converge toward correct solutions. However, most existing large language models (LLMs) decouple reasoning from verification: they either generate reasoning without explicit self-checking or rely on external verifiers to detect errors post hoc. The former lacks immediate feedback, while the latter increases system complexity and hinders synchronized learning. Motivated by human critical thinking, we propose Stepwise Think-Critique (STC), a unified framework that interleaves reasoning and self-critique at each step within a single model. STC is trained with a hybrid reinforcement learning objective combining reasoning rewards and critique-consistency rewards to jointly optimize reasoning quality and self-evaluation. Experiments on mathematical reasoning benchmarks show that STC demonstrates strong critic-thinking capabilities and produces more interpretable reasoning traces, representing a step toward LLMs with built-in critical thinking.",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "Under Review",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15662v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Nemotron-Math: Efficient Long-Context Distillation of Mathematical Reasoning from Multi-Mode Supervision",
      "authors": [
        "Wei Du",
        "Shubham Toshniwal",
        "Branislav Kisacanin",
        "Sadegh Mahdavi",
        "Ivan Moshkov",
        "George Armstrong",
        "Stephen Ge",
        "Edgar Minasyan",
        "Feng Chen",
        "Igor Gitman"
      ],
      "arxiv_id": "2512.15489v1",
      "summary": "High-quality mathematical reasoning supervision requires diverse reasoning styles, long-form traces, and effective tool integration, capabilities that existing datasets provide only in limited form. Leveraging the multi-mode generation ability of gpt-oss-120b, we introduce Nemotron-Math, a large-scale mathematical reasoning dataset containing 7.5M solution traces across high, medium, and low reasoning modes, each available both with and without Python tool-integrated reasoning (TIR).\n  The dataset integrates 85K curated AoPS problems with 262K community-sourced StackExchange-Math problems, combining structured competition tasks with diverse real-world mathematical queries. We conduct controlled evaluations to assess the dataset quality.\n  Nemotron-Math consistently outperforms the original OpenMathReasoning on matched AoPS problems. Incorporating StackExchange-Math substantially improves robustness and generalization, especially on HLE-Math, while preserving accuracy on math competition benchmarks.\n  To support efficient long-context training, we develop a sequential bucketed strategy that accelerates 128K context-length fine-tuning by 2--3$\\times$ without significant accuracy loss. Overall, Nemotron-Math enables state-of-the-art performance, including 100\\% maj@16 accuracy on AIME 2024 and 2025 with Python TIR.",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15489v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]distillation"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Graph Contextual Reinforcement Learning for Efficient Directed Controller Synthesis",
      "authors": [
        "Toshihide Ubukata",
        "Enhong Mu",
        "Takuto Yamauchi",
        "Mingyue Zhang",
        "Jialong Li",
        "Kenji Tei"
      ],
      "arxiv_id": "2512.15295v1",
      "summary": "Controller synthesis is a formal method approach for automatically generating Labeled Transition System (LTS) controllers that satisfy specified properties. The efficiency of the synthesis process, however, is critically dependent on exploration policies. These policies often rely on fixed rules or strategies learned through reinforcement learning (RL) that consider only a limited set of current features. To address this limitation, this paper introduces GCRL, an approach that enhances RL-based methods by integrating Graph Neural Networks (GNNs). GCRL encodes the history of LTS exploration into a graph structure, allowing it to capture a broader, non-current-based context. In a comparative experiment against state-of-the-art methods, GCRL exhibited superior learning efficiency and generalization across four out of five benchmark domains, except one particular domain characterized by high symmetry and strictly local interactions.",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15295v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "SoFlow: Solution Flow Models for One-Step Generative Modeling",
      "authors": [
        "Tianze Luo",
        "Haotian Yuan",
        "Zhuang Liu"
      ],
      "arxiv_id": "2512.15657v1",
      "summary": "The multi-step denoising process in diffusion and Flow Matching models causes major efficiency issues, which motivates research on few-step generation. We present Solution Flow Models (SoFlow), a framework for one-step generation from scratch. By analyzing the relationship between the velocity function and the solution function of the velocity ordinary differential equation (ODE), we propose a Flow Matching loss and a solution consistency loss to train our models. The Flow Matching loss allows our models to provide estimated velocity fields for Classifier-Free Guidance (CFG) during training, which improves generation performance. Notably, our consistency loss does not require the calculation of the Jacobian-vector product (JVP), a common requirement in recent works that is not well-optimized in deep learning frameworks like PyTorch. Experimental results indicate that, when trained from scratch using the same Diffusion Transformer (DiT) architecture and an equal number of training epochs, our models achieve better FID-50K scores than MeanFlow models on the ImageNet 256x256 dataset.",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "Our code is available at https://github.com/zlab-princeton/SoFlow",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15657v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "flow matching"
          ],
          "score": 1.5
        },
        {
          "name": "支柱四：生成式动作 (Generative Motion)",
          "id": "4_motion_diffusion",
          "matched_keywords": [
            "classifier-free guidance"
          ],
          "score": 2.5
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "2_algo_arch",
        "4_motion_diffusion"
      ]
    },
    {
      "title": "Ising Machines for Model Predictive Path Integral-Based Optimal Control",
      "authors": [
        "Lorin Werthen-Brabants",
        "Pieter Simoens"
      ],
      "arxiv_id": "2512.15533v1",
      "summary": "We present a sampling-based Model Predictive Control (MPC) method that implements Model Predictive Path Integral (MPPI) as an \\emph{Ising machine}, suitable for novel forms of probabilistic computing. By expressing the control problem as a Quadratic Unconstrained Binary Optimization (QUBO) problem, we map MPC onto an energy landscape suitable for Gibbs sampling from an Ising model. This formulation enables efficient exploration of (near-)optimal control trajectories. We demonstrate that the approach achieves accurate trajectory tracking compared to a reference MPPI implementation, highlighting the potential of Ising-based MPPI for real-time control in robotics and autonomous systems.",
      "categories": [
        "eess.SY"
      ],
      "primary_category": "eess.SY",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15533v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "MPC",
            "model predictive control"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models",
      "authors": [
        "Lunbin Zeng",
        "Jingfeng Yao",
        "Bencheng Liao",
        "Hongyuan Tao",
        "Wenyu Liu",
        "Xinggang Wang"
      ],
      "arxiv_id": "2512.15713v1",
      "summary": "In recent multimodal research, the diffusion paradigm has emerged as a promising alternative to the autoregressive paradigm (AR), owing to its unique decoding advantages. However, due to the capability limitations of the base diffusion language model, the performance of the diffusion vision language model (dVLM) still lags significantly behind that of mainstream models. This leads to a simple yet fundamental question: Is it possible to construct dVLMs based on existing powerful AR models? In response, we propose DiffusionVL, a dVLM family that could be translated from any powerful AR models. Through simple fine-tuning, we successfully adapt AR pre-trained models into the diffusion paradigm. This approach yields two key observations: (1) The paradigm shift from AR-based multimodal models to diffusion is remarkably effective. (2) Direct conversion of an AR language model to a dVLM is also feasible, achieving performance competitive with LLaVA-style visual-instruction-tuning. Further, we introduce a block-decoding design into dVLMs that supports arbitrary-length generation and KV cache reuse, achieving a significant inference speedup. We conduct a large number of experiments. Despite training with less than 5% of the data required by prior methods, DiffusionVL achieves a comprehensive performance improvement-a 34.4% gain on the MMMU-Pro (vision) bench and 37.5% gain on the MME (Cog.) bench-alongside a 2x inference speedup. The model and code are released at https://github.com/hustvl/DiffusionVL.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "11 pages, 5 figures, conference or other essential info",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15713v1",
      "code_links": [
        {
          "url": "https://github.com/hustvl/DiffusionVL",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "multimodal"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Towards Seamless Interaction: Causal Turn-Level Modeling of Interactive 3D Conversational Head Dynamics",
      "authors": [
        "Junjie Chen",
        "Fei Wang",
        "Zhihao Huang",
        "Qing Zhou",
        "Kun Li",
        "Dan Guo",
        "Linfeng Zhang",
        "Xun Yang"
      ],
      "arxiv_id": "2512.15340v1",
      "summary": "Human conversation involves continuous exchanges of speech and nonverbal cues such as head nods, gaze shifts, and facial expressions that convey attention and emotion. Modeling these bidirectional dynamics in 3D is essential for building expressive avatars and interactive robots. However, existing frameworks often treat talking and listening as independent processes or rely on non-causal full-sequence modeling, hindering temporal coherence across turns. We present TIMAR (Turn-level Interleaved Masked AutoRegression), a causal framework for 3D conversational head generation that models dialogue as interleaved audio-visual contexts. It fuses multimodal information within each turn and applies turn-level causal attention to accumulate conversational history, while a lightweight diffusion head predicts continuous 3D head dynamics that captures both coordination and expressive variability. Experiments on the DualTalk benchmark show that TIMAR reduces Fréchet Distance and MSE by 15-30% on the test set, and achieves similar gains on out-of-distribution data. The source code will be released in the GitHub repository https://github.com/CoderChen01/towards-seamleass-interaction.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15340v1",
      "code_links": [
        {
          "url": "https://github.com/CoderChen01/towards-seamleass-interaction",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "multimodal"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Assessing the Visual Enumeration Abilities of Specialized Counting Architectures and Vision-Language Models",
      "authors": [
        "Kuinan Hou",
        "Jing Mi",
        "Marco Zorzi",
        "Lamberto Ballan",
        "Alberto Testolin"
      ],
      "arxiv_id": "2512.15254v1",
      "summary": "Counting the number of items in a visual scene remains a fundamental yet challenging task in computer vision. Traditional approaches to solving this problem rely on domain-specific counting architectures, which are trained using datasets annotated with a predefined set of object categories. However, recent progress in creating large-scale multimodal vision-language models (VLMs) suggests that these domain-general architectures may offer a flexible alternative for open-set object counting. In this study, we therefore systematically compare the performance of state-of-the-art specialized counting architectures against VLMs on two popular counting datasets, as well as on a novel benchmark specifically created to have a finer-grained control over the visual properties of test images. Our findings show that most VLMs can approximately enumerate the number of items in a visual scene, matching or even surpassing the performance of specialized computer vision architectures. Notably, enumeration accuracy significantly improves when VLMs are prompted to generate intermediate representations (i.e., locations and verbal labels) of each object to be counted. Nevertheless, none of the models can reliably count the number of objects in complex visual scenes, showing that further research is still needed to create AI systems that can reliably deploy counting procedures in realistic environments.",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15254v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "multimodal"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Uni-Parser Technical Report",
      "authors": [
        "Xi Fang",
        "Haoyi Tao",
        "Shuwen Yang",
        "Suyang Zhong",
        "Haocheng Lu",
        "Han Lyu",
        "Chaozheng Huang",
        "Xinyu Li",
        "Linfeng Zhang",
        "Guolin Ke"
      ],
      "arxiv_id": "2512.15098v1",
      "summary": "This technical report introduces Uni-Parser, an industrial-grade document parsing engine tailored for scientific literature and patents, delivering high throughput, robust accuracy, and cost efficiency. Unlike pipeline-based document parsing methods, Uni-Parser employs a modular, loosely coupled multi-expert architecture that preserves fine-grained cross-modal alignments across text, equations, tables, figures, and chemical structures, while remaining easily extensible to emerging modalities. The system incorporates adaptive GPU load balancing, distributed inference, dynamic module orchestration, and configurable modes that support either holistic or modality-specific parsing. Optimized for large-scale cloud deployment, Uni-Parser achieves a processing rate of up to 20 PDF pages per second on 8 x NVIDIA RTX 4090D GPUs, enabling cost-efficient inference across billions of pages. This level of scalability facilitates a broad spectrum of downstream applications, ranging from literature retrieval and summarization to the extraction of chemical structures, reaction schemes, and bioactivity data, as well as the curation of large-scale corpora for training next-generation large language models and AI4Science models.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15098v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "PMMD: A pose-guided multi-view multi-modal diffusion for person generation",
      "authors": [
        "Ziyu Shang",
        "Haoran Liu",
        "Rongchao Zhang",
        "Zhiqian Wei",
        "Tongtong Feng"
      ],
      "arxiv_id": "2512.15069v1",
      "summary": "Generating consistent human images with controllable pose and appearance is essential for applications in virtual try on, image editing, and digital human creation. Current methods often suffer from occlusions, garment style drift, and pose misalignment. We propose Pose-guided Multi-view Multimodal Diffusion (PMMD), a diffusion framework that synthesizes photorealistic person images conditioned on multi-view references, pose maps, and text prompts. A multimodal encoder jointly models visual views, pose features, and semantic descriptions, which reduces cross modal discrepancy and improves identity fidelity. We further design a ResCVA module to enhance local detail while preserving global structure, and a cross modal fusion module that integrates image semantics with text throughout the denoising pipeline. Experiments on the DeepFashion MultiModal dataset show that PMMD outperforms representative baselines in consistency, detail preservation, and controllability. Project page and code are available at https://github.com/ZANMANGLOOPYE/PMMD.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15069v1",
      "code_links": [
        {
          "url": "https://github.com/ZANMANGLOOPYE/PMMD",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "multimodal"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Dynamic Rebatching for Efficient Early-Exit Inference with DREX",
      "authors": [
        "Xuting Liu",
        "Daniel Alexander",
        "Siva Kesava Reddy Kakarla",
        "Behnaz Arzani",
        "Vincent Liu"
      ],
      "arxiv_id": "2512.15705v1",
      "summary": "Early-Exit (EE) is a Large Language Model (LLM) architecture that accelerates inference by allowing easier tokens to be generated using only a subset of the model's layers. However, traditional batching frameworks are ill-suited for EE LLMs, as not all requests in a batch may be ready to exit at the same time. Existing solutions either force a uniform decision on the batch, which overlooks EE opportunities, or degrade output quality by forcing premature exits. We propose Dynamic Rebatching, a solution where we dynamically reorganize the batch at each early-exit point. Requests that meet the exit criteria are immediately processed, while those that continue are held in a buffer, re-grouped into a new batch, and forwarded to deeper layers. We introduce DREX, an early-exit inference system that implements Dynamic Rebatching with two key optimizations: 1) a copy-free rebatching buffer that avoids physical data movement, and 2) an EE and SLA-aware scheduler that analytically predicts whether a given rebatching operation will be profitable. DREX also efficiently handles the missing KV cache from skipped layers using memory-efficient state-copying. Our evaluation shows that DREX improves throughput by 2-12% compared to baseline approaches while maintaining output quality. Crucially, DREX completely eliminates involuntary exits, providing a key guarantee for preserving the output quality intended by the EE model.",
      "categories": [
        "cs.DC",
        "cs.LG"
      ],
      "primary_category": "cs.DC",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15705v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Activation Oracles: Training and Evaluating LLMs as General-Purpose Activation Explainers",
      "authors": [
        "Adam Karvonen",
        "James Chua",
        "Clément Dumas",
        "Kit Fraser-Taliente",
        "Subhash Kantamneni",
        "Julian Minder",
        "Euan Ong",
        "Arnab Sen Sharma",
        "Daniel Wen",
        "Owain Evans",
        "Samuel Marks"
      ],
      "arxiv_id": "2512.15674v1",
      "summary": "Large language model (LLM) activations are notoriously difficult to understand, with most existing techniques using complex, specialized methods for interpreting them. Recent work has proposed a simpler approach known as LatentQA: training LLMs to directly accept LLM activations as inputs and answer arbitrary questions about them in natural language. However, prior work has focused on narrow task settings for both training and evaluation. In this paper, we instead take a generalist perspective. We evaluate LatentQA-trained models, which we call Activation Oracles (AOs), in far out-of-distribution settings and examine how performance scales with training data diversity. We find that AOs can recover information fine-tuned into a model (e.g., biographical knowledge or malign propensities) that does not appear in the input text, despite never being trained with activations from a fine-tuned model. Our main evaluations are four downstream tasks where we can compare to prior white- and black-box techniques. We find that even narrowly-trained LatentQA models can generalize well, and that adding additional training datasets (such as classification tasks and a self-supervised context prediction task) yields consistent further improvements. Overall, our best AOs match or exceed prior white-box baselines on all four tasks and are the best method on 3 out of 4. These results suggest that diversified training to answer natural-language queries imparts a general capability to verbalize information about LLM activations.",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "36 pages",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15674v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "How Much is Too Much? Exploring LoRA Rank Trade-offs for Retaining Knowledge and Domain Robustness",
      "authors": [
        "Darshita Rathore",
        "Vineet Kumar",
        "Chetna Bansal",
        "Anindya Moitra"
      ],
      "arxiv_id": "2512.15634v1",
      "summary": "Large language models are increasingly adapted to downstream tasks through fine-tuning. Full supervised fine-tuning (SFT) and parameter-efficient fine-tuning (PEFT) methods, such as Low-Rank Adaptation (LoRA), are two dominant approaches. While PEFT methods are widely used for their computational efficiency, the implications of their configurations (e.g., rank) remain under-explored in downstream Q&A tasks and generalisation. In this work, we perform a comprehensive evaluation across multiple reasoning and recall datasets, conducting a rank sweep to quantify the trade-off between SFT and PEFT. We also compare the accuracy of PEFT and SFT models across in-domain and out-of-domain adaptation, highlighting distinct generalisation behaviour and task-specific forgetting. We demonstrate that LoRA achieves competitive and in some cases superior performance compared to SFT, particularly on reasoning tasks at specific rank values. Additionally, we analyze the internal representations via spectral features and layer-wise attention structures, offering insights into representational drift and structural changes in attention patterns.",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "Accepted at AACL IJCNLP 2025",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15634v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Behavior Tokens Speak Louder: Disentangled Explainable Recommendation with Behavior Vocabulary",
      "authors": [
        "Xinshun Feng",
        "Mingzhe Liu",
        "Yi Qiao",
        "Tongyu Zhu",
        "Leilei Sun",
        "Shuai Wang"
      ],
      "arxiv_id": "2512.15614v1",
      "summary": "Recent advances in explainable recommendations have explored the integration of language models to analyze natural language rationales for user-item interactions. Despite their potential, existing methods often rely on ID-based representations that obscure semantic meaning and impose structural constraints on language models, thereby limiting their applicability in open-ended scenarios. These challenges are intensified by the complex nature of real-world interactions, where diverse user intents are entangled and collaborative signals rarely align with linguistic semantics. To overcome these limitations, we propose BEAT, a unified and transferable framework that tokenizes user and item behaviors into discrete, interpretable sequences. We construct a behavior vocabulary via a vector-quantized autoencoding process that disentangles macro-level interests and micro-level intentions from graph-based representations. We then introduce multi-level semantic supervision to bridge the gap between behavioral signals and language space. A semantic alignment regularization mechanism is designed to embed behavior tokens directly into the input space of frozen language models. Experiments on three public datasets show that BEAT improves zero-shot recommendation performance while generating coherent and informative explanations. Further analysis demonstrates that our behavior tokens capture fine-grained semantics and offer a plug-and-play interface for integrating complex behavior patterns into large language models.",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "accepted by AAAI 2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15614v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "DEER: Draft with Diffusion, Verify with Autoregressive Models",
      "authors": [
        "Zicong Cheng",
        "Guo-Wei Yang",
        "Jia Li",
        "Zhijie Deng",
        "Meng-Hao Guo",
        "Shi-Min Hu"
      ],
      "arxiv_id": "2512.15176v1",
      "summary": "Efficiency, as a critical practical challenge for LLM-driven agentic and reasoning systems, is increasingly constrained by the inherent latency of autoregressive (AR) decoding. Speculative decoding mitigates this cost through a draft-verify scheme, yet existing approaches rely on AR draft models (a.k.a., drafters), which introduce two fundamental issues: (1) step-wise uncertainty accumulation leads to a progressive collapse of trust between the target model and the drafter, and (2) inherently sequential decoding of AR drafters. Together, these factors cause limited speedups. In this paper, we show that a diffusion large language model (dLLM) drafters can naturally overcome these issues through its fundamentally different probabilistic modeling and efficient parallel decoding strategy. Building on this insight, we introduce DEER, an efficient speculative decoding framework that drafts with diffusion and verifies with AR models. To enable high-quality drafting, DEER employs a two-stage training pipeline to align the dLLM-based drafters with the target AR model, and further adopts single-step decoding to generate long draft segments. Experiments show DEER reaches draft acceptance lengths of up to 32 tokens, far surpassing the 10 tokens achieved by EAGLE-3. Moreover, on HumanEval with Qwen3-30B-A3B, DEER attains a 5.54x speedup, while EAGLE-3 achieves only 2.41x. Code, model, demo, etc, will be available at https://czc726.github.io/DEER/",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "Homepage : https://czc726.github.io/DEER/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15176v1",
      "code_links": [
        {
          "url": "https://czc726.github.io/DEER/",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "The Semantic Architect: How FEAML Bridges Structured Data and LLMs for Multi-Label Tasks",
      "authors": [
        "Wanfu Gao",
        "Zebin He",
        "Jun Gao"
      ],
      "arxiv_id": "2512.15082v1",
      "summary": "Existing feature engineering methods based on large language models (LLMs) have not yet been applied to multi-label learning tasks. They lack the ability to model complex label dependencies and are not specifically adapted to the characteristics of multi-label tasks. To address the above issues, we propose Feature Engineering Automation for Multi-Label Learning (FEAML), an automated feature engineering method for multi-label classification which leverages the code generation capabilities of LLMs. By utilizing metadata and label co-occurrence matrices, LLMs are guided to understand the relationships between data features and task objectives, based on which high-quality features are generated. The newly generated features are evaluated in terms of model accuracy to assess their effectiveness, while Pearson correlation coefficients are used to detect redundancy. FEAML further incorporates the evaluation results as feedback to drive LLMs to continuously optimize code generation in subsequent iterations. By integrating LLMs with a feedback mechanism, FEAML realizes an efficient, interpretable and self-improving feature engineering paradigm. Empirical results on various multi-label datasets demonstrate that our FEAML outperforms other feature engineering methods.",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15082v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "The Meta-Prompting Protocol: Orchestrating LLMs via Adversarial Feedback Loops",
      "authors": [
        "Fanzhe Fu"
      ],
      "arxiv_id": "2512.15053v1",
      "summary": "The transition of Large Language Models (LLMs) from stochastic chat interfaces to reliable software components necessitates a fundamental re-engineering of interaction paradigms. Current methodologies, predominantly heuristic-based \"prompt engineering,\" fail to provide the deterministic guarantees required for mission-critical applications. We introduce the Meta-Prompting Protocol, a rigorous theoretical framework that formalizes the orchestration of LLMs as a programmable, self-optimizing system. Central to this protocol is the Adversarial Trinity, a tripartite topology comprising a Generator (P), an Auditor (A), and an Optimizer (O). By treating natural language instructions as differentiable variables within a semantic computation graph and utilizing textual critiques as gradients, this architecture mitigates hallucination and prevents model collapse. We demonstrate the theoretical viability of this approach using declarative programming paradigms (DSPy) and automatic textual differentiation (TextGrad), establishing a foundation for \"Observable Software Engineering\" in the era of probabilistic computing.",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.SE"
      ],
      "primary_category": "cs.CL",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "6 pages, 2 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15053v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "SeBERTis: A Framework for Producing Classifiers of Security-Related Issue Reports",
      "authors": [
        "Sogol Masoumzadeh",
        "Yufei Li",
        "Shane McIntosh",
        "Dániel Varró",
        "Lili Wei"
      ],
      "arxiv_id": "2512.15003v1",
      "summary": "Monitoring issue tracker submissions is a crucial software maintenance activity. A key goal is the prioritization of high risk, security-related bugs. If such bugs can be recognized early, the risk of propagation to dependent products and endangerment of stakeholder benefits can be mitigated. To assist triage engineers with this task, several automatic detection techniques, from Machine Learning (ML) models to prompting Large Language Models (LLMs), have been proposed. Although promising to some extent, prior techniques often memorize lexical cues as decision shortcuts, yielding low detection rate specifically for more complex submissions. As such, these classifiers do not yet reach the practical expectations of a real-time detector of security-related issues. To address these limitations, we propose SEBERTIS, a framework to train Deep Neural Networks (DNNs) as classifiers independent of lexical cues, so that they can confidently detect fully unseen security-related issues. SEBERTIS capitalizes on fine-tuning bidirectional transformer architectures as Masked Language Models (MLMs) on a series of semantically equivalent vocabulary to prediction labels (which we call Semantic Surrogates) when they have been replaced with a mask. Our SEBERTIS-trained classifier achieves a 0.9880 F1-score in detecting security-related issues of a curated corpus of 10,000 GitHub issue reports, substantially outperforming state-of-the-art issue classifiers, with 14.44%-96.98%, 15.40%-93.07%, and 14.90%-94.72% higher detection precision, recall, and F1-score over ML-based baselines. Our classifier also substantially surpasses LLM baselines, with an improvement of 23.20%-63.71%, 36.68%-85.63%, and 39.49%-74.53% for precision, recall, and F1-score.",
      "categories": [
        "cs.CR",
        "cs.LG",
        "cs.SE"
      ],
      "primary_category": "cs.CR",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "This is the author pre-print. The manuscript has been accepted for publication at SANER 2026!",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15003v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "DreamPRM-Code: Function-as-Step Process Reward Model with Label Correction for LLM Coding",
      "authors": [
        "Ruiyi Zhang",
        "Peijia Qin",
        "Qi Cao",
        "Pengtao Xie"
      ],
      "arxiv_id": "2512.15000v1",
      "summary": "Process Reward Models (PRMs) have become essential for improving Large Language Models (LLMs) via test-time scaling, yet their effectiveness in coding remains limited due to the lack of meaningful step decompositions in code and the noise of Monte-Carlo-generated partial labels. We propose DreamPRM-Code, a coding-focused PRM that treats functions as reasoning steps using a Chain-of-Function prompting strategy to induce modular code generation, enabling PRM training and application analogous to mathematical reasoning tasks. To address label noise, DreamPRM-Code introduces a meta-learning-based correction mechanism that leverages clean final-solution unit-test labels and performs bi-level optimization to refine intermediate labels. Applying on test-time scaling, DreamPRM-Code achieved state-of-the-art performance on LiveCodeBench with 80.9 pass@1 rate, surpassing OpenAI o4-mini.",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15000v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Evaluating Metrics for Safety with LLM-as-Judges",
      "authors": [
        "Kester Clegg",
        "Richard Hawkins",
        "Ibrahim Habli",
        "Tom Lawton"
      ],
      "arxiv_id": "2512.15617v1",
      "summary": "LLMs (Large Language Models) are increasingly used in text processing pipelines to intelligently respond to a variety of inputs and generation tasks. This raises the possibility of replacing human roles that bottleneck existing information flows, either due to insufficient staff or process complexity. However, LLMs make mistakes and some processing roles are safety critical. For example, triaging post-operative care to patients based on hospital referral letters, or updating site access schedules in nuclear facilities for work crews. If we want to introduce LLMs into critical information flows that were previously performed by humans, how can we make them safe and reliable? Rather than make performative claims about augmented generation frameworks or graph-based techniques, this paper argues that the safety argument should focus on the type of evidence we get from evaluation points in LLM processes, particularly in frameworks that employ LLM-as-Judges (LaJ) evaluators. This paper argues that although we cannot get deterministic evaluations from many natural language processing tasks, by adopting a basket of weighted metrics it may be possible to lower the risk of errors within an evaluation, use context sensitivity to define error severity and design confidence thresholds that trigger human review of critical LaJ judgments when concordance across evaluators is low.",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15617v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "How Do Semantically Equivalent Code Transformations Impact Membership Inference on LLMs for Code?",
      "authors": [
        "Hua Yang",
        "Alejandro Velasco",
        "Thanh Le-Cong",
        "Md Nazmul Haque",
        "Bowen Xu",
        "Denys Poshyvanyk"
      ],
      "arxiv_id": "2512.15468v1",
      "summary": "The success of large language models for code relies on vast amounts of code data, including public open-source repositories, such as GitHub, and private, confidential code from companies. This raises concerns about intellectual property compliance and the potential unauthorized use of license-restricted code. While membership inference (MI) techniques have been proposed to detect such unauthorized usage, their effectiveness can be undermined by semantically equivalent code transformation techniques, which modify code syntax while preserving semantic.\n  In this work, we systematically investigate whether semantically equivalent code transformation rules might be leveraged to evade MI detection. The results reveal that model accuracy drops by only 1.5% in the worst case for each rule, demonstrating that transformed datasets can effectively serve as substitutes for fine-tuning. Additionally, we find that one of the rules (RenameVariable) reduces MI success by 10.19%, highlighting its potential to obscure the presence of restricted code. To validate these findings, we conduct a causal analysis confirming that variable renaming has the strongest causal effect in disrupting MI detection. Notably, we find that combining multiple transformations does not further reduce MI effectiveness. Our results expose a critical loophole in license compliance enforcement for training large language models for code, showing that MI detection can be substantially weakened by transformation-based obfuscation techniques.",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.SE",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "13 pages, 3 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15468v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "On Assessing the Relevance of Code Reviews Authored by Generative Models",
      "authors": [
        "Robert Heumüller",
        "Frank Ortmeier"
      ],
      "arxiv_id": "2512.15466v1",
      "summary": "The use of large language models like ChatGPT in code review offers promising efficiency gains but also raises concerns about correctness and safety. Existing evaluation methods for code review generation either rely on automatic comparisons to a single ground truth, which fails to capture the variability of human perspectives, or on subjective assessments of \"usefulness\", a highly ambiguous concept. We propose a novel evaluation approach based on what we call multi-subjective ranking. Using a dataset of 280 self-contained code review requests and corresponding comments from CodeReview StackExchange, multiple human judges ranked the quality of ChatGPT-generated comments alongside the top human responses from the platform. Results show that ChatGPT's comments were ranked significantly better than human ones, even surpassing StackExchange's accepted answers. Going further, our proposed method motivates and enables more meaningful assessments of generative AI's performance in code review, while also raising awareness of potential risks of unchecked integration into review processes.",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "Replication Package: https://github.com/robert-heumueller-ovgu/repl-generative-review-relevance",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15466v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Bilateral Spatial Reasoning about Street Networks: Graph-based RAG with Qualitative Spatial Representations",
      "authors": [
        "Reinhard Moratz",
        "Niklas Daute",
        "James Ondieki",
        "Markus Kattenbeck",
        "Mario Krajina",
        "Ioannis Giannopoulos"
      ],
      "arxiv_id": "2512.15388v1",
      "summary": "This paper deals with improving the capabilities of Large Language Models (LLM) to provide route instructions for pedestrian wayfinders by means of qualitative spatial relations.",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15388v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "SCOPE: Prompt Evolution for Enhancing Agent Effectiveness",
      "authors": [
        "Zehua Pei",
        "Hui-Ling Zhen",
        "Shixiong Kai",
        "Sinno Jialin Pan",
        "Yunhe Wang",
        "Mingxuan Yuan",
        "Bei Yu"
      ],
      "arxiv_id": "2512.15374v1",
      "summary": "Large Language Model (LLM) agents are increasingly deployed in environments that generate massive, dynamic contexts. However, a critical bottleneck remains: while agents have access to this context, their static prompts lack the mechanisms to manage it effectively, leading to recurring Corrective and Enhancement failures. To address this capability gap, we introduce \\textbf{SCOPE} (Self-evolving Context Optimization via Prompt Evolution). SCOPE frames context management as an \\textit{online optimization} problem, synthesizing guidelines from execution traces to automatically evolve the agent's prompt. We propose a Dual-Stream mechanism that balances tactical specificity (resolving immediate errors) with strategic generality (evolving long-term principles). Furthermore, we introduce Perspective-Driven Exploration to maximize strategy coverage, increasing the likelihood that the agent has the correct strategy for any given task. Experiments on the HLE benchmark show that SCOPE improves task success rates from 14.23\\% to 38.64\\% without human intervention. We make our code publicly available at https://github.com/JarvisPei/SCOPE.",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15374v1",
      "code_links": [
        {
          "url": "https://github.com/JarvisPei/SCOPE",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Exploring User Acceptance and Concerns toward LLM-powered Conversational Agents in Immersive Extended Reality",
      "authors": [
        "Efe Bozkir",
        "Enkelejda Kasneci"
      ],
      "arxiv_id": "2512.15343v1",
      "summary": "The rapid development of generative artificial intelligence (AI) and large language models (LLMs), and the availability of services that make them accessible, have led the general public to begin incorporating them into everyday life. The extended reality (XR) community has also sought to integrate LLMs, particularly in the form of conversational agents, to enhance user experience and task efficiency. When interacting with such conversational agents, users may easily disclose sensitive information due to the naturalistic flow of the conversations, and combining such conversational data with fine-grained sensor data may lead to novel privacy issues. To address these issues, a user-centric understanding of technology acceptance and concerns is essential. Therefore, to this end, we conducted a large-scale crowdsourcing study with 1036 participants, examining user decision-making processes regarding LLM-powered conversational agents in XR, across factors of XR setting type, speech interaction type, and data processing location. We found that while users generally accept these technologies, they express concerns related to security, privacy, social implications, and trust. Our results suggest that familiarity plays a crucial role, as daily generative AI use is associated with greater acceptance. In contrast, previous ownership of XR devices is linked to less acceptance, possibly due to existing familiarity with the settings. We also found that men report higher acceptance with fewer concerns than women. Regarding data type sensitivity, location data elicited the most significant concern, while body temperature and virtual object states were considered least sensitive. Overall, our study highlights the importance of practitioners effectively communicating their measures to users, who may remain distrustful. We conclude with implications and recommendations for LLM-powered XR.",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15343v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Evaluating LLMs for Zeolite Synthesis Event Extraction (ZSEE): A Systematic Analysis of Prompting Strategies",
      "authors": [
        "Charan Prakash Rathore",
        "Saumi Ray",
        "Dhruv Kumar"
      ],
      "arxiv_id": "2512.15312v1",
      "summary": "Extracting structured information from zeolite synthesis experimental procedures is critical for materials discovery, yet existing methods have not systematically evaluated Large Language Models (LLMs) for this domain-specific task. This work addresses a fundamental question: what is the efficacy of different prompting strategies when applying LLMs to scientific information extraction? We focus on four key subtasks: event type classification (identifying synthesis steps), trigger text identification (locating event mentions), argument role extraction (recognizing parameter types), and argument text extraction (extracting parameter values). We evaluate four prompting strategies - zero-shot, few-shot, event-specific, and reflection-based - across six state-of-the-art LLMs (Gemma-3-12b-it, GPT-5-mini, O4-mini, Claude-Haiku-3.5, DeepSeek reasoning and non-reasoning) using the ZSEE dataset of 1,530 annotated sentences. Results demonstrate strong performance on event type classification (80-90\\% F1) but modest performance on fine-grained extraction tasks, particularly argument role and argument text extraction (50-65\\% F1). GPT-5-mini exhibits extreme prompt sensitivity with 11-79\\% F1 variation. Notably, advanced prompting strategies provide minimal improvements over zero-shot approaches, revealing fundamental architectural limitations. Error analysis identifies systematic hallucination, over-generalization, and inability to capture synthesis-specific nuances. Our findings demonstrate that while LLMs achieve high-level understanding, precise extraction of experimental parameters requires domain-adapted models, providing quantitative benchmarks for scientific information extraction.",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "Under Review",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15312v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Yes-MT's Submission to the Low-Resource Indic Language Translation Shared Task in WMT 2024",
      "authors": [
        "Yash Bhaskar",
        "Parameswari Krishnamurthy"
      ],
      "arxiv_id": "2512.15226v1",
      "summary": "This paper presents the systems submitted by the Yes-MT team for the Low-Resource Indic Language Translation Shared Task at WMT 2024 (Pakray et al., 2024), focusing on translating between English and the Assamese, Mizo, Khasi, and Manipuri languages. The experiments explored various approaches, including fine-tuning pre-trained models like mT5 (Xue et al., 2020) and IndicBart (Dabre et al., 2021) in both multilingual and monolingual settings, LoRA (Hu et al., 2021) fine-tuning IndicTrans2 (Gala et al., 2023), zero-shot and few-shot prompting (Brown, 2020) with large language models (LLMs) like Llama 3 (Dubey et al., 2024) and Mixtral 8x7b (Jiang et al., 2024), LoRA supervised fine-tuning of Llama 3 (Mecklenburg et al., 2024), and training Transformer models (Vaswani, 2017) from scratch. The results were evaluated on the WMT23 Low-Resource Indic Language Translation Shared Task test data using SacreBLEU (Post, 2018) and CHRF (Popovic, 2015), highlighting the challenges of low-resource translation and the potential of LLMs for these tasks, particularly with fine-tuning.",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "Accepted at WMT 2024",
      "doi": "10.18653/v1/2024.wmt-1.71",
      "journal_ref": "In Proceedings of the Ninth Conference on Machine Translation (WMT 2024), pages 788-792, 2024",
      "pdf_url": "https://arxiv.org/pdf/2512.15226v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "RFKG-CoT: Relation-Driven Adaptive Hop-count Selection and Few-Shot Path Guidance for Knowledge-Aware QA",
      "authors": [
        "Chao Zhang",
        "Minghan Li",
        "Tianrui Lv",
        "Guodong Zhou"
      ],
      "arxiv_id": "2512.15219v1",
      "summary": "Large language models (LLMs) often generate hallucinations in knowledge-intensive QA due to parametric knowledge limitations. While existing methods like KG-CoT improve reliability by integrating knowledge graph (KG) paths, they suffer from rigid hop-count selection (solely question-driven) and underutilization of reasoning paths (lack of guidance). To address this, we propose RFKG-CoT: First, it replaces the rigid hop-count selector with a relation-driven adaptive hop-count selector that dynamically adjusts reasoning steps by activating KG relations (e.g., 1-hop for direct \"brother\" relations, 2-hop for indirect \"father-son\" chains), formalized via a relation mask. Second, it introduces a few-shot in-context learning path guidance mechanism with CoT (think) that constructs examples in a \"question-paths-answer\" format to enhance LLMs' ability to understand reasoning paths. Experiments on four KGQA benchmarks show RFKG-CoT improves accuracy by up to 14.7 pp (Llama2-7B on WebQSP) over KG-CoT. Ablations confirm the hop-count selector and the path prompt are complementary, jointly transforming KG evidence into more faithful answers.",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "9pages, 5 figures, accepted by AAAI 2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15219v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Offline Multi-Task Multi-Objective Data-Driven Evolutionary Algorithm with Language Surrogate Model and Implicit Q-Learning",
      "authors": [
        "Xian-Rong Zhang",
        "Yue-Jiao Gong",
        "Zeyuan Ma",
        "Jun Zhang"
      ],
      "arxiv_id": "2512.15149v1",
      "summary": "Data-driven evolutionary algorithms has shown surprising results in addressing expensive optimization problems through robust surrogate modeling. Though promising, existing surrogate modeling schemes may encounter limitations in complex optimization problems with many sub-objectives, which rely on repeated and tedious approximation. To address such technical gap, we propose Q-MetaSur as a plug-and-play surrogate modeling scheme capable of providing unified and generalized surrogate learning. Specifically, we consider multi-task-multi-objective optimization~(MTMOO) in offline setting. Several key designs are proposed: 1) we transform objective approximation into sequence-to-sequence modeling where MTMOO problem can be represented by tenxual tokenization. To operate under such auto-regressive modeling, we introduce a Large Language Model-based surrogate model that first encodes a MTMOO instance and then decodes objective values of unseen decision variables. To ensure stability in training the proposed model, we propose a two-stage offline training strategy that operates as a synergy of supervised tuning and RL fine-tuning, which first exploits offline dataset to fit existing knowledge and then leverages RL to enhance model's generalization performance. Extensive empirical results on the CEC2019 benchmark demonstrate that Q-MetaSur not only outperforms representative surrogate baselines in objective approximation accuracy, but also helps underlying evolutionary algorithms achieve both desired optimization convergence and improved pareto optimality.",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "16 pages",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15149v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "CTkvr: KV Cache Retrieval for Long-Context LLMs via Centroid then Token Indexing",
      "authors": [
        "Kuan Lu",
        "Shuhang Lin",
        "Sai Wu",
        "Yichen Yao",
        "Junhan Yang",
        "Huan Li",
        "Wei Chu",
        "Xu Yinghui",
        "Yuan Qi",
        "Gang Chen"
      ],
      "arxiv_id": "2512.15550v1",
      "summary": "Large language models (LLMs) are increasingly applied in long-context scenarios such as multi-turn conversations. However, long contexts pose significant challenges for inference efficiency, including high memory overhead from Key-Value (KV) cache and increased latency due to excessive memory accesses. Recent methods for dynamic KV selection struggle with trade-offs: block-level indexing degrades accuracy by retrieving irrelevant KV entries, while token-level indexing incurs high latency from inefficient retrieval mechanisms. In this paper, we propose CTKVR, a novel centroid-then-token KV retrieval scheme that addresses these limitations. CTKVR leverages a key observation: query vectors adjacent in position exhibit high similarity after Rotary Position Embedding (RoPE) and share most of their top-k KV cache entries. Based on this insight, CTKVR employs a two-stage retrieval strategy: lightweight centroids are precomputed during prefilling for centroid-grained indexing, followed by token-level refinement for precise KV retrieval. This approach balances retrieval efficiency and accuracy. To further enhance performance, we implement an optimized system for indexing construction and search using CPU-GPU co-execution. Experimentally, CTKVR achieves superior performance across multiple benchmarks with less than 1% accuracy degradation. Meanwhile, CTKVR delivers 3 times and 4 times throughput speedups on Llama-3-8B and Yi-9B at 96K context length across diverse GPU hardware.",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15550v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Toward expert-level motivational interviewing for health behavior improvement with LLMs",
      "authors": [
        "Run-ze Hu",
        "Yang Yang",
        "Yi-hang Yang",
        "Jing-qi Kong",
        "Jia-hui Luo",
        "Wen-yu Yang",
        "Jing Chen",
        "Jing-yao Liu",
        "Hui-qun Zeng",
        "Lei Zhang",
        "Zheng Liu"
      ],
      "arxiv_id": "2512.15446v1",
      "summary": "Background: Motivational interviewing (MI) is an effective counseling approach for promoting health behavior change, but its impact is constrained by the need for highly trained human counselors. Objective: This study aimed to explore a scalable alternative by developing and evaluating Large Language Models for Motivational Interviewing (MI-LLMs). Methods: We first curated five Chinese psychological counseling corpora and, using GPT-4 with an MI-informed prompt, transcribed multi-turn dialogues from the two highest-quality datasets (CPsyCounD and PsyDTCorpus) into 2,040 MI-style counseling conversations, of which 2,000 were used for training and 40 for testing. Three Chinese-capable open-source LLMs (Baichuan2-7B-Chat, ChatGLM-4-9B-Chat and Llama-3-8B-Chinese-Chat-v2) were fine-tuned on this corpus and were named as MI-LLMs. We evaluated MI-LLMs using round-based automatic metrics and expert manual coding with the Motivational Interviewing Treatment Integrity (MITI) Coding Manual 4.2.1. Results: Across all three models, fine-tuning substantially improved BLEU-4 and ROUGE scores compared with the base models, and manual coding showed that MI-LLMs achieved technical and relational global scores, and MI-adherent ratios that approached those of real MI dialogues, although complex reflections and reflection-to-question ratios remained less frequent. Conclusions: These findings provide initial evidence that MI-oriented fine-tuning can endow general-purpose LLMs with core MI-consistent counseling behaviors, suggesting a scalable pathway toward AI-assisted health behavior change support while underscoring the need for further work on data scale, complex MI skills and real-world intervention trials.",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "26 pages, 3 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15446v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Towards Proactive Personalization through Profile Customization for Individual Users in Dialogues",
      "authors": [
        "Xiaotian Zhang",
        "Yuan Wang",
        "Ruizhe Chen",
        "Zeya Wang",
        "Runchen Hou",
        "Zuozhu Liu"
      ],
      "arxiv_id": "2512.15302v1",
      "summary": "The deployment of Large Language Models (LLMs) in interactive systems necessitates a deep alignment with the nuanced and dynamic preferences of individual users. Current alignment techniques predominantly address universal human values or static, single-turn preferences, thereby failing to address the critical needs of long-term personalization and the initial user cold-start problem. To bridge this gap, we propose PersonalAgent, a novel user-centric lifelong agent designed to continuously infer and adapt to user preferences. PersonalAgent constructs and dynamically refines a unified user profile by decomposing dialogues into single-turn interactions, framing preference inference as a sequential decision-making task. Experiments show that PersonalAgent achieves superior performance over strong prompt-based and policy optimization baselines, not only in idealized but also in noisy conversational contexts, while preserving cross-session preference consistency. Furthermore, human evaluation confirms that PersonalAgent excels at capturing user preferences naturally and coherently. Our findings underscore the importance of lifelong personalization for developing more inclusive and adaptive conversational agents. Our code is available here.",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15302v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "The Moralization Corpus: Frame-Based Annotation and Analysis of Moralizing Speech Acts across Diverse Text Genres",
      "authors": [
        "Maria Becker",
        "Mirko Sommer",
        "Lars Tapken",
        "Yi Wan Teh",
        "Bruno Brocai"
      ],
      "arxiv_id": "2512.15248v1",
      "summary": "Moralizations - arguments that invoke moral values to justify demands or positions - are a yet underexplored form of persuasive communication. We present the Moralization Corpus, a novel multi-genre dataset designed to analyze how moral values are strategically used in argumentative discourse. Moralizations are pragmatically complex and often implicit, posing significant challenges for both human annotators and NLP systems. We develop a frame-based annotation scheme that captures the constitutive elements of moralizations - moral values, demands, and discourse protagonists - and apply it to a diverse set of German texts, including political debates, news articles, and online discussions. The corpus enables fine-grained analysis of moralizing language across communicative formats and domains. We further evaluate several large language models (LLMs) under varied prompting conditions for the task of moralization detection and moralization component extraction and compare it to human annotations in order to investigate the challenges of automatic and manual analysis of moralizations. Results show that detailed prompt instructions has a greater effect than few-shot or explanation-based prompting, and that moralization remains a highly subjective and context-sensitive task. We release all data, annotation guidelines, and code to foster future interdisciplinary research on moral discourse and moral reasoning in NLP.",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15248v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "DeX-Portrait: Disentangled and Expressive Portrait Animation via Explicit and Latent Motion Representations",
      "authors": [
        "Yuxiang Shi",
        "Zhe Li",
        "Yanwen Wang",
        "Hao Zhu",
        "Xun Cao",
        "Ligang Liu"
      ],
      "arxiv_id": "2512.15524v1",
      "summary": "Portrait animation from a single source image and a driving video is a long-standing problem. Recent approaches tend to adopt diffusion-based image/video generation models for realistic and expressive animation. However, none of these diffusion models realizes high-fidelity disentangled control between the head pose and facial expression, hindering applications like expression-only or pose-only editing and animation. To address this, we propose DeX-Portrait, a novel approach capable of generating expressive portrait animation driven by disentangled pose and expression signals. Specifically, we represent the pose as an explicit global transformation and the expression as an implicit latent code. First, we design a powerful motion trainer to learn both pose and expression encoders for extracting precise and decomposed driving signals. Then we propose to inject the pose transformation into the diffusion model through a dual-branch conditioning mechanism, and the expression latent through cross attention. Finally, we design a progressive hybrid classifier-free guidance for more faithful identity consistency. Experiments show that our method outperforms state-of-the-art baselines on both animation quality and disentangled controllability.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "Projectpage: https://syx132.github.io/DeX-Portrait/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15524v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱四：生成式动作 (Generative Motion)",
          "id": "4_motion_diffusion",
          "matched_keywords": [
            "classifier-free guidance"
          ],
          "score": 2.5
        }
      ],
      "relevance_score": 2.5,
      "hit_pillars": [
        "4_motion_diffusion"
      ]
    },
    {
      "title": "Robustness Evaluation of Machine Learning Models for Fault Classification and Localization In Power System Protection",
      "authors": [
        "Julian Oelhaf",
        "Mehran Pashaei",
        "Georg Kordowich",
        "Christian Bergler",
        "Andreas Maier",
        "Johann Jäger",
        "Siming Bayer"
      ],
      "arxiv_id": "2512.15385v1",
      "summary": "The growing penetration of renewable and distributed generation is transforming power systems and challenging conventional protection schemes that rely on fixed settings and local measurements. Machine learning (ML) offers a data-driven alternative for centralized fault classification (FC) and fault localization (FL), enabling faster and more adaptive decision-making. However, practical deployment critically depends on robustness. Protection algorithms must remain reliable even when confronted with missing, noisy, or degraded sensor data. This work introduces a unified framework for systematically evaluating the robustness of ML models in power system protection.\n  High-fidelity EMT simulations are used to model realistic degradation scenarios, including sensor outages, reduced sampling rates, and transient communication losses. The framework provides a consistent methodology for benchmarking models, quantifying the impact of limited observability, and identifying critical measurement channels required for resilient operation. Results show that FC remains highly stable under most degradation types but drops by about 13% under single-phase loss, while FL is more sensitive overall, with voltage loss increasing localization error by over 150%. These findings offer actionable guidance for robustness-aware design of future ML-assisted protection systems.",
      "categories": [
        "cs.LG",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "This paper is a postprint of a paper submitted to and accepted for publication in the 20th IET International Conference on Developments in Power System Protection (DPSP Global 2026) and is subject to Institution of Engineering and Technology Copyright. The copy of record is available at the IET Digital Library",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15385v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱四：生成式动作 (Generative Motion)",
          "id": "4_motion_diffusion",
          "matched_keywords": [
            "penetration"
          ],
          "score": 2.5
        }
      ],
      "relevance_score": 2.5,
      "hit_pillars": [
        "4_motion_diffusion"
      ]
    },
    {
      "title": "Service-Oriented Fast Frequency Response from Flexible Loads and Energy Storage in Low-Inertia Power Systems",
      "authors": [
        "Xiaojie Tao",
        "Rajit Gadh"
      ],
      "arxiv_id": "2512.15677v1",
      "summary": "The increasing penetration of inverter-based renewable generation has significantly reduced system inertia, making modern power grids more vulnerable to rapid frequency deviations following disturbances. While a wide range of flexible resources-including electric vehicles (EVs), data centers, and battery energy storage systems (BESS)-have demonstrated the physical capability to provide fast frequency response (FFR), existing studies primarily focus on individual resource performance or controller-level designs. A systematic framework that translates heterogeneous FFR capabilities into deployable, system-level frequency services remains largely unexplored. This paper proposes a service-oriented coordination framework for fast frequency response from flexible loads and energy storage, bridging the gap between physical capability assessment and grid-operational utilization. The framework decomposes frequency support into multiple time-critical service layers based on response speed, power capacity, and energy sustainability, and dynamically allocates FFR responsibilities among heterogeneous resources accordingly. By explicitly accounting for response latency, saturation limits, and energy constraints, the proposed approach enables coordinated dispatch that prioritizes ultra-fast resources for initial frequency arrest while leveraging slower but energy-rich resources to sustain recovery.",
      "categories": [
        "eess.SY"
      ],
      "primary_category": "eess.SY",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15677v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱四：生成式动作 (Generative Motion)",
          "id": "4_motion_diffusion",
          "matched_keywords": [
            "penetration"
          ],
          "score": 2.5
        }
      ],
      "relevance_score": 2.5,
      "hit_pillars": [
        "4_motion_diffusion"
      ]
    },
    {
      "title": "QuantGraph: A Receding-Horizon Quantum Graph Solver",
      "authors": [
        "Pranav Vaidhyanathan",
        "Aristotelis Papatheodorou",
        "David R. M. Arvidsson-Shukur",
        "Mark T. Mitchison",
        "Natalia Ares",
        "Ioannis Havoutis"
      ],
      "arxiv_id": "2512.15476v1",
      "summary": "Dynamic programming is a cornerstone of graph-based optimization. While effective, it scales unfavorably with problem size. In this work, we present QuantGraph, a two-stage quantum-enhanced framework that casts local and global graph-optimization problems as quantum searches over discrete trajectory spaces. The solver is designed to operate efficiently by first finding a sequence of locally optimal transitions in the graph (local stage), without considering full trajectories. The accumulated cost of these transitions acts as a threshold that prunes the search space (up to 60% reduction for certain examples). The subsequent global stage, based on this threshold, refines the solution. Both stages utilize variants of the Grover-adaptive-search algorithm. To achieve scalability and robustness, we draw on principles from control theory and embed QuantGraph's global stage within a receding-horizon model-predictive-control scheme. This classical layer stabilizes and guides the quantum search, improving precision and reducing computational burden. In practice, the resulting closed-loop system exhibits robust behavior and lower overall complexity. Notably, for a fixed query budget, QuantGraph attains a 2x increase in control-discretization precision while still benefiting from Grover-search's inherent quadratic speedup compared to classical methods.",
      "categories": [
        "quant-ph",
        "cs.RO",
        "eess.SY",
        "physics.comp-ph"
      ],
      "primary_category": "quant-ph",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "P.Vaidhyanathan and A. Papatheodorou contributed equally to this work. 11 pages, 4 figures, 1 table, 2 algorithms",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15476v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "model predictive control"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Load-Based Variable Transmission Mechanism for Robotic Applications",
      "authors": [
        "Sinan Emre",
        "Victor Barasuol",
        "Matteo Villa",
        "Claudio Semini"
      ],
      "arxiv_id": "2512.15448v1",
      "summary": "This paper presents a Load-Based Variable Transmission (LBVT) mechanism designed to enhance robotic actuation by dynamically adjusting the transmission ratio in response to external torque demands. Unlike existing variable transmission systems that require additional actuators for active control, the proposed LBVT mechanism leverages a pre-tensioned spring and a four-bar linkage to passively modify the transmission ratio, thereby reducing the complexity of robot joint actuation systems. The effectiveness of the LBVT mechanism is evaluated through simulation-based analyses. The results confirm that the system achieves up to a 40 percent increase in transmission ratio upon reaching a predefined torque threshold, effectively amplifying joint torque when required without additional actuation. Furthermore, the simulations demonstrate a torque amplification effect triggered when the applied force exceeds 18 N, highlighting the system ability to autonomously respond to varying load conditions. This research contributes to the development of lightweight, efficient, and adaptive transmission systems for robotic applications, particularly in legged robots where dynamic torque adaptation is critical.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "22nd International Conference on Advanced Robotics (ICAR 2025)",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15448v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "legged robot"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "HERO: Hierarchical Traversable 3D Scene Graphs for Embodied Navigation Among Movable Obstacles",
      "authors": [
        "Yunheng Wang",
        "Yixiao Feng",
        "Yuetong Fang",
        "Shuning Zhang",
        "Tan Jing",
        "Jian Li",
        "Xiangrui Jiang",
        "Renjing Xu"
      ],
      "arxiv_id": "2512.15047v1",
      "summary": "3D Scene Graphs (3DSGs) constitute a powerful representation of the physical world, distinguished by their abilities to explicitly model the complex spatial, semantic, and functional relationships between entities, rendering a foundational understanding that enables agents to interact intelligently with their environment and execute versatile behaviors. Embodied navigation, as a crucial component of such capabilities, leverages the compact and expressive nature of 3DSGs to enable long-horizon reasoning and planning in complex, large-scale environments. However, prior works rely on a static-world assumption, defining traversable space solely based on static spatial layouts and thereby treating interactable obstacles as non-traversable. This fundamental limitation severely undermines their effectiveness in real-world scenarios, leading to limited reachability, low efficiency, and inferior extensibility. To address these issues, we propose HERO, a novel framework for constructing Hierarchical Traversable 3DSGs, that redefines traversability by modeling operable obstacles as pathways, capturing their physical interactivity, functional semantics, and the scene's relational hierarchy. The results show that, relative to its baseline, HERO reduces PL by 35.1% in partially obstructed environments and increases SR by 79.4% in fully obstructed ones, demonstrating substantially higher efficiency and reachability.",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15047v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "traversability"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Spatia: Video Generation with Updatable Spatial Memory",
      "authors": [
        "Jinjing Zhao",
        "Fangyun Wei",
        "Zhening Liu",
        "Hongyang Zhang",
        "Chang Xu",
        "Yan Lu"
      ],
      "arxiv_id": "2512.15716v1",
      "summary": "Existing video generation models struggle to maintain long-term spatial and temporal consistency due to the dense, high-dimensional nature of video signals. To overcome this limitation, we propose Spatia, a spatial memory-aware video generation framework that explicitly preserves a 3D scene point cloud as persistent spatial memory. Spatia iteratively generates video clips conditioned on this spatial memory and continuously updates it through visual SLAM. This dynamic-static disentanglement design enhances spatial consistency throughout the generation process while preserving the model's ability to produce realistic dynamic entities. Furthermore, Spatia enables applications such as explicit camera control and 3D-aware interactive editing, providing a geometrically grounded framework for scalable, memory-driven video generation.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "Project page: https://zhaojingjing713.github.io/Spatia/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15716v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "visual SLAM"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Asynchronous Event Stream Noise Filtering for High-frequency Structure Deformation Measurement",
      "authors": [
        "Yifei Bian",
        "Banglei Guan",
        "Zibin Liu",
        "Ang Su",
        "Shiyao Zhu",
        "Yang Shang",
        "Qifeng Yu"
      ],
      "arxiv_id": "2512.15055v1",
      "summary": "Large-scale structures suffer high-frequency deformations due to complex loads. However, harsh lighting conditions and high equipment costs limit measurement methods based on traditional high-speed cameras. This paper proposes a method to measure high-frequency deformations by exploiting an event camera and LED markers. Firstly, observation noise is filtered based on the characteristics of the event stream generated by LED markers blinking and spatiotemporal correlation. Then, LED markers are extracted from the event stream after differentiating between motion-induced events and events from LED blinking, which enables the extraction of high-speed moving LED markers. Ultimately, high-frequency planar deformations are measured by a monocular event camera. Experimental results confirm the accuracy of our method in measuring high-frequency planar deformations.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "13 pages, 12 figures",
      "doi": "",
      "journal_ref": "Applied Optics, 2024, Vol.63(35): 8936-8943",
      "pdf_url": "https://arxiv.org/pdf/2512.15055v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱八：物理动画 (Physics-based Animation)",
          "id": "8_physics_animation",
          "matched_keywords": [
            "spatiotemporal"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "8_physics_animation"
      ]
    },
    {
      "title": "PIP$^2$ Net: Physics-informed Partition Penalty Deep Operator Network",
      "authors": [
        "Hongjin Mi",
        "Huiqiang Lun",
        "Changhong Mou",
        "Yeyu Zhang"
      ],
      "arxiv_id": "2512.15086v1",
      "summary": "Operator learning has become a powerful tool for accelerating the solution of parameterized partial differential equations (PDEs), enabling rapid prediction of full spatiotemporal fields for new initial conditions or forcing functions. Existing architectures such as DeepONet and the Fourier Neural Operator (FNO) show strong empirical performance but often require large training datasets, lack explicit physical structure, and may suffer from instability in their trunk-network features, where mode imbalance or collapse can hinder accurate operator approximation. Motivated by the stability and locality of classical partition-of-unity (PoU) methods, we investigate PoU-based regularization techniques for operator learning and develop a revised formulation of the existing POU--PI--DeepONet framework. The resulting \\emph{P}hysics-\\emph{i}nformed \\emph{P}artition \\emph{P}enalty Deep Operator Network (PIP$^{2}$ Net) introduces a simplified and more principled partition penalty that improved the coordinated trunk outputs that leads to more expressiveness without sacrificing the flexibility of DeepONet. We evaluate PIP$^{2}$ Net on three nonlinear PDEs: the viscous Burgers equation, the Allen--Cahn equation, and a diffusion--reaction system. The results show that it consistently outperforms DeepONet, PI-DeepONet, and POU-DeepONet in prediction accuracy and robustness.",
      "categories": [
        "cs.LG",
        "physics.comp-ph"
      ],
      "primary_category": "cs.LG",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15086v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱八：物理动画 (Physics-based Animation)",
          "id": "8_physics_animation",
          "matched_keywords": [
            "spatiotemporal"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "8_physics_animation"
      ]
    }
  ],
  "filtered": true
}