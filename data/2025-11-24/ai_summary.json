{
    "papers": [
        {
            "title": "LumiTex: Towards High-Fidelity PBR Texture Generation with Illumination Context",
            "authors": [
                "Jingzhi Bao",
                "Hongze Chen",
                "Lingting Zhu",
                "Chenyu Liu",
                "Runze Zhang",
                "Keyang Luo",
                "Zeyu Hu",
                "Weikai Chen",
                "Yingda Yin",
                "Xin Wang",
                "Zehong Lin",
                "Jun Zhang",
                "Xiaoguang Han"
            ],
            "arxiv_id": "2511.19437v1",
            "summary": "Physically-based rendering (PBR) provides a principled standard for realistic material-lighting interactions in computer graphics. Despite recent advances in generating PBR textures, existing methods fail to address two fundamental challenges: 1) materials decomposition from image prompts under limited illumination cues, and 2) seamless and view-consistent texture completion. To this end, we propose LumiTex, an end-to-end framework that comprises three key components: (1) a multi-branch generation scheme that disentangles albedo and metallic-roughness under shared illumination priors for robust material understanding, (2) a lighting-aware material attention mechanism that injects illumination context into the decoding process for physically grounded generation of albedo, metallic, and roughness maps, and (3) a geometry-guided inpainting module based on a large view synthesis model that enriches texture coverage and ensures seamless, view-consistent UV completion. Extensive experiments demonstrate that LumiTex achieves state-of-the-art performance in texture quality, surpassing both existing open-source and commercial methods.",
            "headline_zh": "提出LumiTex框架以解决PBR纹理生成中的材料分解和纹理完成问题",
            "intro_zh": [
                "核心问题：现有方法难以从图像提示中分解材料并实现无缝纹理完成",
                "方法要点：采用多分支生成、光照感知注意力和几何引导修复模块",
                "实验或效果：在纹理质量上超越现有开源和商业方法，达到先进水平"
            ],
            "tags_zh": [
                "PBR纹理生成",
                "材料分解",
                "光照感知",
                "纹理完成",
                "多分支生成",
                "几何引导修复"
            ],
            "_index": 0
        },
        {
            "title": "VDC-Agent: When Video Detailed Captioners Evolve Themselves via Agentic Self-Reflection",
            "authors": [
                "Qiang Wang",
                "Xinyuan Gao",
                "SongLin Dong",
                "Jizhou Han",
                "Jiangyang Li",
                "Yuhang He",
                "Yihong Gong"
            ],
            "arxiv_id": "2511.19436v1",
            "summary": "We present VDC-Agent, a self-evolving framework for Video Detailed Captioning that requires neither human annotations nor larger teacher models. The agent forms a closed loop of caption generation, principle-guided scoring (score and textual suggestions), and prompt refinement. When caption quality regresses, a self-reflection path leverages the previous chain-of-thought to amend the update. Running this process on unlabeled videos produces trajectories of (caption, score) pairs. We convert the trajectories into preference tuples and filter out samples with JSON parsing errors, resulting in VDC-Agent-19K, which contains 18,886 automatically constructed pairs. We then fine-tune the base MLLM on this dataset using an easy-to-hard curriculum direct preference optimization. Built on Qwen2.5-VL-7B-Instruct, our VDC-Agent-7B attains state-of-the-art performance on the VDC benchmark with 49.08% average accuracy and 2.50 score, surpassing specialized video captioners and improving over the base model by +5.13% accuracy and +0.27 score at similar inference cost.",
            "headline_zh": "提出VDC-Agent自进化框架，实现无需人工标注的视频详细描述生成。",
            "intro_zh": [
                "核心问题：视频详细描述任务依赖人工标注或大模型，成本高且效率低。",
                "方法要点：构建闭环系统，包括描述生成、原则评分、提示优化和自我反思。",
                "实验效果：在VDC基准上达到SOTA，准确率49.08%，超越基模型5.13%。"
            ],
            "tags_zh": [
                "视频详细描述",
                "自进化框架",
                "无监督学习",
                "直接偏好优化",
                "多模态大语言模型"
            ],
            "_index": 1
        },
        {
            "title": "Are Image-to-Video Models Good Zero-Shot Image Editors?",
            "authors": [
                "Zechuan Zhang",
                "Zhenyuan Chen",
                "Zongxin Yang",
                "Yi Yang"
            ],
            "arxiv_id": "2511.19435v1",
            "summary": "Large-scale video diffusion models show strong world simulation and temporal reasoning abilities, but their use as zero-shot image editors remains underexplored. We introduce IF-Edit, a tuning-free framework that repurposes pretrained image-to-video diffusion models for instruction-driven image editing. IF-Edit addresses three key challenges: prompt misalignment, redundant temporal latents, and blurry late-stage frames. It includes (1) a chain-of-thought prompt enhancement module that transforms static editing instructions into temporally grounded reasoning prompts; (2) a temporal latent dropout strategy that compresses frame latents after the expert-switch point, accelerating denoising while preserving semantic and temporal coherence; and (3) a self-consistent post-refinement step that sharpens late-stage frames using a short still-video trajectory. Experiments on four public benchmarks, covering non-rigid editing, physical and temporal reasoning, and general instruction edits, show that IF-Edit performs strongly on reasoning-centric tasks while remaining competitive on general-purpose edits. Our study provides a systematic view of video diffusion models as image editors and highlights a simple recipe for unified video-image generative reasoning.",
            "headline_zh": "提出IF-Edit框架，利用图像到视频扩散模型实现零样本图像编辑",
            "intro_zh": [
                "核心问题：视频扩散模型在零样本图像编辑中存在提示错位、冗余时间潜在变量和模糊后期帧问题",
                "方法要点：包括思维链提示增强、时间潜在变量丢弃和自一致后精炼步骤",
                "实验或效果：在多个基准测试中，在推理任务上表现优异，通用编辑任务保持竞争力"
            ],
            "tags_zh": [
                "图像编辑",
                "视频扩散模型",
                "零样本学习",
                "推理增强",
                "潜在变量压缩"
            ],
            "_index": 2
        },
        {
            "title": "Breaking the Likelihood-Quality Trade-off in Diffusion Models by Merging Pretrained Experts",
            "authors": [
                "Yasin Esfandiari",
                "Stefan Bauer",
                "Sebastian U. Stich",
                "Andrea Dittadi"
            ],
            "arxiv_id": "2511.19434v1",
            "summary": "Diffusion models for image generation often exhibit a trade-off between perceptual sample quality and data likelihood: training objectives emphasizing high-noise denoising steps yield realistic images but poor likelihoods, whereas likelihood-oriented training overweights low-noise steps and harms visual fidelity. We introduce a simple plug-and-play sampling method that combines two pretrained diffusion experts by switching between them along the denoising trajectory. Specifically, we apply an image-quality expert at high noise levels to shape global structure, then switch to a likelihood expert at low noise levels to refine pixel statistics. The approach requires no retraining or fine-tuning -- only the choice of an intermediate switching step. On CIFAR-10 and ImageNet32, the merged model consistently matches or outperforms its base components, improving or preserving both likelihood and sample quality relative to each expert alone. These results demonstrate that expert switching across noise levels is an effective way to break the likelihood-quality trade-off in image diffusion models.",
            "headline_zh": "提出专家切换采样方法以解决扩散模型中似然与图像质量权衡问题",
            "intro_zh": [
                "扩散模型存在似然与图像质量权衡：高噪声训练提升质量但损害似然，反之亦然",
                "方法结合预训练专家：高噪声用质量专家，低噪声用似然专家，无需重训练",
                "在CIFAR-10和ImageNet32上，合并模型优于单专家，改善或保持似然与质量"
            ],
            "tags_zh": [
                "扩散模型",
                "图像生成",
                "似然优化",
                "采样方法",
                "专家合并",
                "噪声切换"
            ],
            "_index": 3
        },
        {
            "title": "Mixture of Horizons in Action Chunking",
            "authors": [
                "Dong Jing",
                "Gang Wang",
                "Jiaqi Liu",
                "Weiliang Tang",
                "Zelong Sun",
                "Yunchao Yao",
                "Zhenyu Wei",
                "Yunhui Liu",
                "Zhiwu Lu",
                "Mingyu Ding"
            ],
            "arxiv_id": "2511.19433v1",
            "summary": "Vision-language-action (VLA) models have shown remarkable capabilities in robotic manipulation, but their performance is sensitive to the $\\textbf{action chunk length}$ used during training, termed $\\textbf{horizon}$. Our empirical study reveals an inherent trade-off: longer horizons provide stronger global foresight but degrade fine-grained accuracy, while shorter ones sharpen local control yet struggle on long-term tasks, implying fixed choice of single horizons being suboptimal. To mitigate the trade-off, we propose a $\\textbf{mixture of horizons (MoH)}$ strategy. MoH rearranges the action chunk into several segments with different horizons, processes them in parallel with a shared action transformer, and fuses outputs with a light linear gate. It has three appealing benefits. 1) MoH exploits long-term foresight and short-term precision jointly within a single model, improving both performance and generalizability to complex tasks. 2) MoH is plug-and-play for full-attention action modules with minimal training or inference overhead. 3) MoH enables dynamic inference with adaptive horizons, which selects stable actions through cross-horizon consensus, achieving 2.5$\\times$ higher throughput than baselines while preserving superior performance. Extensive experiments over flow-based policies $π_0$, $π_{0.5}$, and one-step regression policy $π_{\\text{reg}}$ demonstrate that MoH yields consistent and significant gains on both simulations and real-world tasks. Notably, under mixed-task setting, $π_{0.5}$ with MoH reaches a new state-of-the-art with 99$\\%$ average success rate on LIBERO after only $30k$ training iterations. Project page: https://github.com/Timsty1/MixtureOfHorizons",
            "headline_zh": "提出混合视野策略以解决机器人操作中动作块长度权衡问题",
            "intro_zh": [
                "核心问题：固定动作块长度在机器人操作中导致长期任务与精细控制间的权衡",
                "方法要点：将动作块分段并行处理，使用共享变换器和线性门融合输出",
                "实验或效果：在混合任务设置中达到99%平均成功率，提升吞吐量2.5倍"
            ],
            "tags_zh": [
                "机器人操作",
                "动作块长度",
                "混合视野",
                "并行处理",
                "动态推理"
            ],
            "_index": 4
        },
        {
            "title": "Cloud4D",
            "authors": [
                "Jacob Lin",
                "Edward Gryspeerdt",
                "Ronald Clark"
            ],
            "arxiv_id": "2511.19431v1",
            "summary": "There has been great progress in improving numerical weather prediction and climate models using machine learning. However, most global models act at a kilometer-scale, making it challenging to model individual clouds and factors such as extreme precipitation, wind gusts, turbulence, and surface irradiance. Therefore, there is a need to move towards higher-resolution models, which in turn require high-resolution real-world observations that current instruments struggle to obtain. We present Cloud4D, the first learning-based framework that reconstructs a physically consistent, four-dimensional cloud state using only synchronized ground-based cameras. Leveraging a homography-guided 2D-to-3D transformer, Cloud4D infers the full 3D distribution of liquid water content at 25 m spatial and 5 s temporal resolution. By tracking the 3D liquid water content retrievals over time, Cloud4D additionally estimates horizontal wind vectors. Across a two-month deployment comprising six skyward cameras, our system delivers an order-of-magnitude improvement in space-time resolution relative to state-of-the-art satellite measurements, while retaining single-digit relative error ($<10\\%$) against collocated radar measurements. Code and data are available on our project page https://cloud4d.jacob-lin.com/.",
            "headline_zh": "提出Cloud4D框架，利用地面相机重建四维云状态以解决高分辨率观测难题",
            "intro_zh": [
                "核心问题：全球模型分辨率低，难以建模单个云和极端天气，需高分辨率观测",
                "方法要点：基于同形变换的2D到3D转换器，从同步地面相机推断3D液态水含量",
                "实验或效果：部署两月，时空分辨率比卫星高一个数量级，相对误差低于10%"
            ],
            "tags_zh": [
                "云状态重建",
                "四维建模",
                "地面相机观测",
                "液态水含量估计",
                "风向量估计",
                "高分辨率气象"
            ],
            "_index": 5
        },
        {
            "title": "Cook and Clean Together: Teaching Embodied Agents for Parallel Task Execution",
            "authors": [
                "Dingkang Liang",
                "Cheng Zhang",
                "Xiaopeng Xu",
                "Jianzhong Ju",
                "Zhenbo Luo",
                "Xiang Bai"
            ],
            "arxiv_id": "2511.19430v1",
            "summary": "Task scheduling is critical for embodied AI, enabling agents to follow natural language instructions and execute actions efficiently in 3D physical worlds. However, existing datasets often simplify task planning by ignoring operations research (OR) knowledge and 3D spatial grounding. In this work, we propose Operations Research knowledge-based 3D Grounded Task Scheduling (ORS3D), a new task that requires the synergy of language understanding, 3D grounding, and efficiency optimization. Unlike prior settings, ORS3D demands that agents minimize total completion time by leveraging parallelizable subtasks, e.g., cleaning the sink while the microwave operates. To facilitate research on ORS3D, we construct ORS3D-60K, a large-scale dataset comprising 60K composite tasks across 4K real-world scenes. Furthermore, we propose GRANT, an embodied multi-modal large language model equipped with a simple yet effective scheduling token mechanism to generate efficient task schedules and grounded actions. Extensive experiments on ORS3D-60K validate the effectiveness of GRANT across language understanding, 3D grounding, and scheduling efficiency. The code is available at https://github.com/H-EmbodVis/GRANT",
            "headline_zh": "提出ORS3D任务与GRANT模型，以优化具身AI在3D环境中的并行任务调度效率",
            "intro_zh": [
                "核心问题：现有数据集忽略运筹学知识与3D空间基础，限制具身AI任务调度效率",
                "方法要点：构建ORS3D-60K数据集，并开发GRANT模型，集成调度令牌机制生成高效计划",
                "实验或效果：在ORS3D-60K上验证GRANT在语言理解、3D基础与调度效率的有效性"
            ],
            "tags_zh": [
                "具身AI",
                "任务调度",
                "3D基础",
                "运筹学",
                "多模态大语言模型",
                "并行执行"
            ],
            "_index": 6
        },
        {
            "title": "Flow Map Distillation Without Data",
            "authors": [
                "Shangyuan Tong",
                "Nanye Ma",
                "Saining Xie",
                "Tommi Jaakkola"
            ],
            "arxiv_id": "2511.19428v1",
            "summary": "State-of-the-art flow models achieve remarkable quality but require slow, iterative sampling. To accelerate this, flow maps can be distilled from pre-trained teachers, a procedure that conventionally requires sampling from an external dataset. We argue that this data-dependency introduces a fundamental risk of Teacher-Data Mismatch, as a static dataset may provide an incomplete or even misaligned representation of the teacher's full generative capabilities. This leads us to question whether this reliance on data is truly necessary for successful flow map distillation. In this work, we explore a data-free alternative that samples only from the prior distribution, a distribution the teacher is guaranteed to follow by construction, thereby circumventing the mismatch risk entirely. To demonstrate the practical viability of this philosophy, we introduce a principled framework that learns to predict the teacher's sampling path while actively correcting for its own compounding errors to ensure high fidelity. Our approach surpasses all data-based counterparts and establishes a new state-of-the-art by a significant margin. Specifically, distilling from SiT-XL/2+REPA, our method reaches an impressive FID of 1.45 on ImageNet 256x256, and 1.49 on ImageNet 512x512, both with only 1 sampling step. We hope our work establishes a more robust paradigm for accelerating generative models and motivates the broader adoption of flow map distillation without data.",
            "headline_zh": "提出无数据流映射蒸馏方法，避免教师-数据不匹配风险。",
            "intro_zh": [
                "核心问题：传统流映射蒸馏依赖外部数据，易导致教师-数据不匹配。",
                "方法要点：仅从先验分布采样，主动纠正误差，确保高保真度。",
                "实验效果：在ImageNet上FID达1.45，超越所有数据方法。"
            ],
            "tags_zh": [
                "流映射蒸馏",
                "无数据学习",
                "生成模型加速",
                "教师-数据不匹配",
                "先验分布采样"
            ],
            "_index": 7
        },
        {
            "title": "Ref-SAM3D: Bridging SAM3D with Text for Reference 3D Reconstruction",
            "authors": [
                "Yun Zhou",
                "Yaoting Wang",
                "Guangquan Jie",
                "Jinyu Liu",
                "Henghui Ding"
            ],
            "arxiv_id": "2511.19426v1",
            "summary": "SAM3D has garnered widespread attention for its strong 3D object reconstruction capabilities. However, a key limitation remains: SAM3D cannot reconstruct specific objects referred to by textual descriptions, a capability that is essential for practical applications such as 3D editing, game development, and virtual environments. To address this gap, we introduce Ref-SAM3D, a simple yet effective extension to SAM3D that incorporates textual descriptions as a high-level prior, enabling text-guided 3D reconstruction from a single RGB image. Through extensive qualitative experiments, we show that Ref-SAM3D, guided only by natural language and a single 2D view, delivers competitive and high-fidelity zero-shot reconstruction performance. Our results demonstrate that Ref-SAM3D effectively bridges the gap between 2D visual cues and 3D geometric understanding, offering a more flexible and accessible paradigm for reference-guided 3D reconstruction. Code is available at: https://github.com/FudanCVL/Ref-SAM3D.",
            "headline_zh": "提出Ref-SAM3D以解决SAM3D无法基于文本描述重建特定3D对象的问题",
            "intro_zh": [
                "核心问题：SAM3D缺乏基于文本描述重建特定3D对象的能力，限制实际应用",
                "方法要点：扩展SAM3D，引入文本描述作为先验，实现单RGB图像的文本引导3D重建",
                "实验或效果：零样本重建性能竞争且高保真，仅需自然语言和单2D视图"
            ],
            "tags_zh": [
                "3D重建",
                "文本引导",
                "零样本学习",
                "单视图重建",
                "SAM3D扩展"
            ],
            "_index": 8
        },
        {
            "title": "SAM3-Adapter: Efficient Adaptation of Segment Anything 3 for Camouflage Object Segmentation, Shadow Detection, and Medical Image Segmentation",
            "authors": [
                "Tianrun Chen",
                "Runlong Cao",
                "Xinda Yu",
                "Lanyun Zhu",
                "Chaotao Ding",
                "Deyi Ji",
                "Cheng Chen",
                "Qi Zhu",
                "Chunyan Xu",
                "Papa Mao",
                "Ying Zang"
            ],
            "arxiv_id": "2511.19425v1",
            "summary": "The rapid rise of large-scale foundation models has reshaped the landscape of image segmentation, with models such as Segment Anything achieving unprecedented versatility across diverse vision tasks. However, previous generations-including SAM and its successor-still struggle with fine-grained, low-level segmentation challenges such as camouflaged object detection, medical image segmentation, cell image segmentation, and shadow detection. To address these limitations, we originally proposed SAM-Adapter in 2023, demonstrating substantial gains on these difficult scenarios. With the emergence of Segment Anything 3 (SAM3)-a more efficient and higher-performing evolution with a redesigned architecture and improved training pipeline-we revisit these long-standing challenges. In this work, we present SAM3-Adapter, the first adapter framework tailored for SAM3 that unlocks its full segmentation capability. SAM3-Adapter not only reduces computational overhead but also consistently surpasses both SAM and SAM2-based solutions, establishing new state-of-the-art results across multiple downstream tasks, including medical imaging, camouflaged (concealed) object segmentation, and shadow detection. Built upon the modular and composable design philosophy of the original SAM-Adapter, SAM3-Adapter provides stronger generalizability, richer task adaptability, and significantly improved segmentation precision. Extensive experiments confirm that integrating SAM3 with our adapter yields superior accuracy, robustness, and efficiency compared to all prior SAM-based adaptations. We hope SAM3-Adapter can serve as a foundation for future research and practical segmentation applications. Code, pre-trained models, and data processing pipelines are available.",
            "headline_zh": "提出SAM3-Adapter以高效适配Segment Anything 3，提升伪装物体分割、阴影检测和医学图像分割性能",
            "intro_zh": [
                "核心问题：基础模型在细粒度分割任务如伪装物体检测和医学图像分割中表现不足",
                "方法要点：设计适配器框架，降低计算开销并增强任务适应性和分割精度",
                "实验或效果：在多个下游任务中超越先前方法，实现新的最优结果"
            ],
            "tags_zh": [
                "图像分割",
                "适配器框架",
                "伪装物体检测",
                "阴影检测",
                "医学图像分割",
                "基础模型适配"
            ],
            "_index": 9
        },
        {
            "title": "Chain-of-Visual-Thought: Teaching VLMs to See and Think Better with Continuous Visual Tokens",
            "authors": [
                "Yiming Qin",
                "Bomin Wei",
                "Jiaxin Ge",
                "Konstantinos Kallidromitis",
                "Stephanie Fu",
                "Trevor Darrell",
                "Xudong Wang"
            ],
            "arxiv_id": "2511.19418v1",
            "summary": "Vision-Language Models (VLMs) excel at reasoning in linguistic space but struggle with perceptual understanding that requires dense visual perception, e.g., spatial reasoning and geometric awareness. This limitation stems from the fact that current VLMs have limited mechanisms to capture dense visual information across spatial dimensions. We introduce Chain-of-Visual-Thought (COVT), a framework that enables VLMs to reason not only in words but also through continuous visual tokens-compact latent representations that encode rich perceptual cues. Within a small budget of roughly 20 tokens, COVT distills knowledge from lightweight vision experts, capturing complementary properties such as 2D appearance, 3D geometry, spatial layout, and edge structure. During training, the VLM with COVT autoregressively predicts these visual tokens to reconstruct dense supervision signals (e.g., depth, segmentation, edges, and DINO features). At inference, the model reasons directly in the continuous visual token space, preserving efficiency while optionally decoding dense predictions for interpretability. Evaluated across more than ten diverse perception benchmarks, including CV-Bench, MMVP, RealWorldQA, MMStar, WorldMedQA, and HRBench, integrating COVT into strong VLMs such as Qwen2.5-VL and LLaVA consistently improves performance by 3% to 16% and demonstrates that compact continuous visual thinking enables more precise, grounded, and interpretable multimodal intelligence.",
            "headline_zh": "提出Chain-of-Visual-Thought框架，通过连续视觉令牌增强视觉语言模型的密集感知能力",
            "intro_zh": [
                "当前视觉语言模型在空间推理等密集视觉感知任务上表现不佳",
                "COVT使用约20个连续视觉令牌编码外观、几何等属性，并自回归预测以重构密集监督信号",
                "在多个基准测试中，集成COVT的模型性能提升3%至16%，实现更精确和可解释的多模态推理"
            ],
            "tags_zh": [
                "视觉语言模型",
                "连续视觉令牌",
                "密集感知",
                "自回归训练",
                "多模态推理",
                "性能提升"
            ],
            "_index": 10
        },
        {
            "title": "UniGame: Turning a Unified Multimodal Model Into Its Own Adversary",
            "authors": [
                "Zhaolong Su",
                "Wang Lu",
                "Hao Chen",
                "Sharon Li",
                "Jindong Wang"
            ],
            "arxiv_id": "2511.19413v1",
            "summary": "Unified Multimodal Models (UMMs) have shown impressive performance in both understanding and generation with a single architecture. However, UMMs still exhibit a fundamental inconsistency: understanding favors compact embeddings, whereas generation favors reconstruction-rich representations. This structural trade-off produces misaligned decision boundaries, degraded cross-modal coherence, and heightened vulnerability under distributional and adversarial shifts. In this paper, we present UniGame, a self-adversarial post-training framework that directly targets the inconsistencies. By applying a lightweight perturber at the shared token interface, UniGame enables the generation branch to actively seek and challenge fragile understanding, turning the model itself into its own adversary. Experiments demonstrate that UniGame significantly improves the consistency (+4.6%). Moreover, it also achieves substantial improvements in understanding (+3.6%), generation (+0.02), out-of-distribution and adversarial robustness (+4.8% and +6.2% on NaturalBench and AdVQA). The framework is architecture-agnostic, introduces less than 1% additional parameters, and is complementary to existing post-training methods. These results position adversarial self-play as a general and effective principle for enhancing the coherence, stability, and unified competence of future multimodal foundation models. The official code is available at: https://github.com/AIFrontierLab/UniGame",
            "headline_zh": "提出UniGame自对抗后训练框架以解决统一多模态模型中的不一致性问题",
            "intro_zh": [
                "核心问题：统一多模态模型在理解与生成间存在嵌入表示不一致，导致决策边界错位和鲁棒性下降",
                "方法要点：在共享令牌接口应用轻量扰动器，使生成分支主动挑战理解分支以提升一致性",
                "实验或效果：显著提升一致性、理解、生成及分布外和对抗鲁棒性，参数增加小于1%"
            ],
            "tags_zh": [
                "统一多模态模型",
                "自对抗训练",
                "多模态一致性",
                "后训练框架",
                "鲁棒性增强"
            ],
            "_index": 11
        },
        {
            "title": "In-Video Instructions: Visual Signals as Generative Control",
            "authors": [
                "Gongfan Fang",
                "Xinyin Ma",
                "Xinchao Wang"
            ],
            "arxiv_id": "2511.19401v1",
            "summary": "Large-scale video generative models have recently demonstrated strong visual capabilities, enabling the prediction of future frames that adhere to the logical and physical cues in the current observation. In this work, we investigate whether such capabilities can be harnessed for controllable image-to-video generation by interpreting visual signals embedded within the frames as instructions, a paradigm we term In-Video Instruction. In contrast to prompt-based control, which provides textual descriptions that are inherently global and coarse, In-Video Instruction encodes user guidance directly into the visual domain through elements such as overlaid text, arrows, or trajectories. This enables explicit, spatial-aware, and unambiguous correspondences between visual subjects and their intended actions by assigning distinct instructions to different objects. Extensive experiments on three state-of-the-art generators, including Veo 3.1, Kling 2.5, and Wan 2.2, show that video models can reliably interpret and execute such visually embedded instructions, particularly in complex multi-object scenarios.",
            "headline_zh": "提出视频内指令方法，利用视觉信号实现可控图像到视频生成。",
            "intro_zh": [
                "核心问题：如何实现细粒度可控的图像到视频生成，避免文本提示的全局性和模糊性。",
                "方法要点：在视频帧中嵌入视觉元素如文本、箭头作为指令，实现空间感知的物体动作对应。",
                "实验效果：在多个先进生成器上验证，模型能可靠解析并执行复杂多对象场景的视觉指令。"
            ],
            "tags_zh": [
                "可控视频生成",
                "视觉指令",
                "图像到视频",
                "多对象场景",
                "空间感知控制"
            ],
            "_index": 12
        },
        {
            "title": "Real-Time Object Tracking with On-Device Deep Learning for Adaptive Beamforming in Dynamic Acoustic Environments",
            "authors": [
                "Jorge Ortigoso-Narro",
                "Jose A. Belloch",
                "Adrian Amor-Martin",
                "Sandra Roger",
                "Maximo Cobos"
            ],
            "arxiv_id": "2511.19396v1",
            "summary": "Advances in object tracking and acoustic beamforming are driving new capabilities in surveillance, human-computer interaction, and robotics. This work presents an embedded system that integrates deep learning-based tracking with beamforming to achieve precise sound source localization and directional audio capture in dynamic environments. The approach combines single-camera depth estimation and stereo vision to enable accurate 3D localization of moving objects. A planar concentric circular microphone array constructed with MEMS microphones provides a compact, energy-efficient platform supporting 2D beam steering across azimuth and elevation. Real-time tracking outputs continuously adapt the array's focus, synchronizing the acoustic response with the target's position. By uniting learned spatial awareness with dynamic steering, the system maintains robust performance in the presence of multiple or moving sources. Experimental evaluation demonstrates significant gains in signal-to-interference ratio, making the design well-suited for teleconferencing, smart home devices, and assistive technologies.",
            "headline_zh": "提出嵌入式系统集成深度学习跟踪与波束成形，实现动态环境中声源精确定位与定向音频捕获。",
            "intro_zh": [
                "核心问题：动态声学环境中多源或移动声源的精确跟踪与定向音频捕获。",
                "方法要点：结合单相机深度估计与立体视觉，使用平面同心圆麦克风阵列进行实时波束转向。",
                "实验或效果：实验评估显示信干比显著提升，适用于视频会议和智能家居。"
            ],
            "tags_zh": [
                "实时目标跟踪",
                "声学波束成形",
                "嵌入式深度学习",
                "3D声源定位",
                "动态环境适应"
            ],
            "_index": 13
        },
        {
            "title": "BackSplit: The Importance of Sub-dividing the Background in Biomedical Lesion Segmentation",
            "authors": [
                "Rachit Saluja",
                "Asli Cihangir",
                "Ruining Deng",
                "Johannes C. Paetzold",
                "Fengbei Liu",
                "Mert R. Sabuncu"
            ],
            "arxiv_id": "2511.19394v1",
            "summary": "Segmenting small lesions in medical images remains notoriously difficult. Most prior work tackles this challenge by either designing better architectures, loss functions, or data augmentation schemes; and collecting more labeled data. We take a different view, arguing that part of the problem lies in how the background is modeled. Common lesion segmentation collapses all non-lesion pixels into a single \"background\" class, ignoring the rich anatomical context in which lesions appear. In reality, the background is highly heterogeneous-composed of tissues, organs, and other structures that can now be labeled manually or inferred automatically using existing segmentation models.\n  In this paper, we argue that training with fine-grained labels that sub-divide the background class, which we call BackSplit, is a simple yet powerful paradigm that can offer a significant performance boost without increasing inference costs. From an information theoretic standpoint, we prove that BackSplit increases the expected Fisher Information relative to conventional binary training, leading to tighter asymptotic bounds and more stable optimization. With extensive experiments across multiple datasets and architectures, we empirically show that BackSplit consistently boosts small-lesion segmentation performance, even when auxiliary labels are generated automatically using pretrained segmentation models. Additionally, we demonstrate that auxiliary labels derived from interactive segmentation frameworks exhibit the same beneficial effect, demonstrating its robustness, simplicity, and broad applicability.",
            "headline_zh": "提出BackSplit方法，通过细分背景提升医学图像小病灶分割性能。",
            "intro_zh": [
                "核心问题：传统方法将非病灶像素归为单一背景类，忽略解剖异质性。",
                "方法要点：使用细粒度标签细分背景，无需增加推理成本。",
                "实验效果：多数据集验证，BackSplit稳定提升小病灶分割精度。"
            ],
            "tags_zh": [
                "医学图像分割",
                "背景细分",
                "小病灶检测",
                "Fisher信息",
                "辅助标签",
                "优化稳定性"
            ],
            "_index": 14
        },
        {
            "title": "UISearch: Graph-Based Embeddings for Multimodal Enterprise UI Screenshots Retrieval",
            "authors": [
                "Maroun Ayli",
                "Youssef Bakouny",
                "Tushar Sharma",
                "Nader Jalloul",
                "Hani Seifeddine",
                "Rima Kilany"
            ],
            "arxiv_id": "2511.19380v1",
            "summary": "Enterprise software companies maintain thousands of user interface screens across products and versions, creating critical challenges for design consistency, pattern discovery, and compliance check. Existing approaches rely on visual similarity or text semantics, lacking explicit modeling of structural properties fundamental to user interface (UI) composition. We present a novel graph-based representation that converts UI screenshots into attributed graphs encoding hierarchical relationships and spatial arrangements, potentially generalizable to document layouts, architectural diagrams, and other structured visual domains. A contrastive graph autoencoder learns embeddings preserving multi-level similarity across visual, structural, and semantic properties. The comprehensive analysis demonstrates that our structural embeddings achieve better discriminative power than state-of-the-art Vision Encoders, representing a fundamental advance in the expressiveness of the UI representation. We implement this representation in UISearch, a multi-modal search framework that combines structural embeddings with semantic search through a composable query language. On 20,396 financial software UIs, UISearch achieves 0.92 Top-5 accuracy with 47.5ms median latency (P95: 124ms), scaling to 20,000+ screens. The hybrid indexing architecture enables complex queries and supports fine-grained UI distinction impossible with vision-only approaches.",
            "headline_zh": "提出基于图的嵌入方法UISearch，以解决企业UI截图多模态检索中的结构建模不足问题。",
            "intro_zh": [
                "企业UI截图数量庞大，现有方法缺乏对UI结构属性的显式建模。",
                "将UI截图转换为属性图，使用对比图自编码器学习多模态嵌入。",
                "在金融软件UI数据集上，Top-5准确率达0.92，延迟低至47.5ms。"
            ],
            "tags_zh": [
                "UI检索",
                "图嵌入",
                "多模态学习",
                "企业软件",
                "对比学习",
                "结构建模"
            ],
            "_index": 15
        },
        {
            "title": "Deployment Dynamics and Optimization of Novel Space Antenna Deployable Mechanism",
            "authors": [
                "Mamoon Aamir",
                "Mariyam Sattar",
                "Naveed Ur Rehman Junejo",
                "Aqsa Zafar Abbasi"
            ],
            "arxiv_id": "2511.19377v1",
            "summary": "Given the increasing need for large aperture antennas in space missions, the difficulty of fitting such structures into small launch vehicles has prompted the design of deployable antenna systems. The thesis introduces a new Triple Scissors Deployable Truss Mechanism (TSDTM) for space antenna missions. The new mechanism is to be stowed during launch and efficiently deploy in orbit, offering maximum aperture size while taking up minimal launch volume. The thesis covers the entire design process from geometric modeling, kinematic analysis with screw theory and Newtonian approaches, dynamic analysis by eigenvalue and simulation methods, and verification with SolidWorks. In addition, optimization routines were coded based on Support Vector Machines for material choice in LEO environments and machine learning method for geometric setup. The TSDTM presented has enhanced structural dynamics with good comparison between simulation and analytical predictions. The structure optimized proved highly accurate, with a deviation of just 1.94% between machine learning-predicted and simulated natural frequencies, demonstrating the potential of incorporating AI-based methods in space structural design.",
            "headline_zh": "提出三重剪式可展开桁架机制以解决空间大孔径天线发射体积限制问题",
            "intro_zh": [
                "空间任务中大型天线难以装入小型运载火箭，需可展开机制",
                "采用螺旋理论和牛顿法进行运动学与动力学分析，并利用SolidWorks验证",
                "基于支持向量机和机器学习的优化方法，预测与模拟频率偏差仅1.94%"
            ],
            "tags_zh": [
                "可展开天线机制",
                "空间结构动力学",
                "机器学习优化",
                "螺旋理论分析",
                "SolidWorks仿真"
            ],
            "_index": 16
        },
        {
            "title": "An Anatomy Aware Hybrid Deep Learning Framework for Lung Cancer Tumor Stage Classification",
            "authors": [
                "Saniah Kayenat Chowdhury",
                "Rusab Sarmun",
                "Muhammad E. H. Chowdhury",
                "Sohaib Bassam Zoghoul",
                "Israa Al-Hashimi",
                "Adam Mushtak",
                "Amith Khandakar"
            ],
            "arxiv_id": "2511.19367v1",
            "summary": "Accurate lung cancer tumor staging is crucial for prognosis and treatment planning. However, it remains challenging for end-to-end deep learning approaches, as such approaches often overlook spatial and anatomical information that are central to the tumor-node-metastasis system. The tumor stage depends on multiple quantitative criteria, including the tumor size and its proximity to the nearest anatomical structures, and small variations can alter the staging outcome. We propose a medically grounded hybrid pipeline that performs staging by explicitly measuring the tumor's size and distance properties rather than treating it as a pure image classification task. Our method employs specialized encoder-decoder networks to precisely segment the lung and adjacent anatomy, including the lobes, tumor, mediastinum, and diaphragm. Subsequently, we extract the necessary tumor properties, i.e. measure the largest tumor dimension and calculate the distance between the tumor and neighboring anatomical structures by a quantitative analysis of the segmentation masks. Finally, we apply rule-based tumor staging aligned with the medical guidelines. This novel framework has been evaluated on the Lung-PET-CT-Dx dataset, demonstrating superior performance compared to traditional deep learning models, achieving an overall classification accuracy of 91.36%. We report the per-stage F1-scores of 0.93 (T1), 0.89 (T2), 0.96 (T3), and 0.90 (T4), a critical evaluation aspect often omitted in prior literature. To our knowledge, this is the first study that embeds explicit clinical context into tumor stage classification. Unlike standard convolutional neural networks that operate in an uninterpretable \"black box\" manner, our method offers both state-of-the-art performance and transparent decision support.",
            "headline_zh": "提出基于解剖感知的混合深度学习框架，用于肺癌肿瘤分期分类。",
            "intro_zh": [
                "核心问题：端到端深度学习方法常忽略肿瘤-淋巴结-转移系统的空间和解剖信息，影响分期准确性。",
                "方法要点：使用编码器-解码器网络分割解剖结构，提取肿瘤尺寸和距离属性，结合规则进行分期。",
                "实验或效果：在Lung-PET-CT-Dx数据集上达到91.36%准确率，各阶段F1分数为0.93至0.96。"
            ],
            "tags_zh": [
                "肺癌分期",
                "解剖分割",
                "混合深度学习",
                "规则分类",
                "医学影像分析"
            ],
            "_index": 17
        },
        {
            "title": "DeCo: Frequency-Decoupled Pixel Diffusion for End-to-End Image Generation",
            "authors": [
                "Zehong Ma",
                "Longhui Wei",
                "Shuai Wang",
                "Shiliang Zhang",
                "Qi Tian"
            ],
            "arxiv_id": "2511.19365v1",
            "summary": "Pixel diffusion aims to generate images directly in pixel space in an end-to-end fashion. This approach avoids the limitations of VAE in the two-stage latent diffusion, offering higher model capacity. Existing pixel diffusion models suffer from slow training and inference, as they usually model both high-frequency signals and low-frequency semantics within a single diffusion transformer (DiT). To pursue a more efficient pixel diffusion paradigm, we propose the frequency-DeCoupled pixel diffusion framework. With the intuition to decouple the generation of high and low frequency components, we leverage a lightweight pixel decoder to generate high-frequency details conditioned on semantic guidance from the DiT. This thus frees the DiT to specialize in modeling low-frequency semantics. In addition, we introduce a frequency-aware flow-matching loss that emphasizes visually salient frequencies while suppressing insignificant ones. Extensive experiments show that DeCo achieves superior performance among pixel diffusion models, attaining FID of 1.62 (256x256) and 2.22 (512x512) on ImageNet, closing the gap with latent diffusion methods. Furthermore, our pretrained text-to-image model achieves a leading overall score of 0.86 on GenEval in system-level comparison. Codes are publicly available at https://github.com/Zehong-Ma/DeCo.",
            "headline_zh": "提出频率解耦像素扩散框架以提升端到端图像生成效率",
            "intro_zh": [
                "现有像素扩散模型在单一DiT中建模高低频信号，导致训练和推理缓慢",
                "使用轻量像素解码器生成高频细节，DiT专注低频语义，并引入频率感知流匹配损失",
                "在ImageNet上FID达1.62（256x256），文本到图像模型在GenEval得分0.86领先"
            ],
            "tags_zh": [
                "像素扩散",
                "频率解耦",
                "端到端图像生成",
                "扩散变换器",
                "流匹配损失"
            ],
            "_index": 18
        },
        {
            "title": "Growing with the Generator: Self-paced GRPO for Video Generation",
            "authors": [
                "Rui Li",
                "Yuanzhi Liang",
                "Ziqi Ni",
                "Haibing Huang",
                "Chi Zhang",
                "Xuelong Li"
            ],
            "arxiv_id": "2511.19356v1",
            "summary": "Group Relative Policy Optimization (GRPO) has emerged as a powerful reinforcement learning paradigm for post-training video generation models. However, existing GRPO pipelines rely on static, fixed-capacity reward models whose evaluation behavior is frozen during training. Such rigid rewards introduce distributional bias, saturate quickly as the generator improves, and ultimately limit the stability and effectiveness of reinforcement-based alignment. We propose Self-Paced GRPO, a competence-aware GRPO framework in which reward feedback co-evolves with the generator. Our method introduces a progressive reward mechanism that automatically shifts its emphasis from coarse visual fidelity to temporal coherence and fine-grained text-video semantic alignment as generation quality increases. This self-paced curriculum alleviates reward-policy mismatch, mitigates reward exploitation, and yields more stable optimization. Experiments on VBench across multiple video generation backbones demonstrate consistent improvements in both visual quality and semantic alignment over GRPO baselines with static rewards, validating the effectiveness and generality of Self-Paced GRPO.",
            "headline_zh": "提出自步GRPO以解决视频生成中静态奖励模型导致的分布偏差和优化不稳定问题",
            "intro_zh": [
                "核心问题：静态奖励模型在训练中行为固定，导致分布偏差、奖励饱和和优化不稳定",
                "方法要点：引入渐进奖励机制，随生成质量提升从视觉保真度转向时间一致性和语义对齐",
                "实验或效果：在VBench上验证，相比静态奖励GRPO，视觉质量和语义对齐均提升"
            ],
            "tags_zh": [
                "视频生成",
                "强化学习",
                "奖励模型",
                "策略优化",
                "自步学习",
                "语义对齐"
            ],
            "_index": 19
        },
        {
            "title": "Leveraging LLMs for reward function design in reinforcement learning control tasks",
            "authors": [
                "Franklin Cardenoso",
                "Wouter Caarls"
            ],
            "arxiv_id": "2511.19355v1",
            "summary": "The challenge of designing effective reward functions in reinforcement learning (RL) represents a significant bottleneck, often requiring extensive human expertise and being time-consuming. Previous work and recent advancements in large language models (LLMs) have demonstrated their potential for automating the generation of reward functions. However, existing methodologies often require preliminary evaluation metrics, human-engineered feedback for the refinement process, or the use of environmental source code as context. To address these limitations, this paper introduces LEARN-Opt (LLM-based Evaluator and Analyzer for Reward functioN Optimization). This LLM-based, fully autonomous, and model-agnostic framework eliminates the need for preliminary metrics and environmental source code as context to generate, execute, and evaluate reward function candidates from textual descriptions of systems and task objectives. LEARN-Opt's main contribution lies in its ability to autonomously derive performance metrics directly from the system description and the task objective, enabling unsupervised evaluation and selection of reward functions. Our experiments indicate that LEARN-Opt achieves performance comparable to or better to that of state-of-the-art methods, such as EUREKA, while requiring less prior knowledge. We find that automated reward design is a high-variance problem, where the average-case candidate fails, requiring a multi-run approach to find the best candidates. Finally, we show that LEARN-Opt can unlock the potential of low-cost LLMs to find high-performing candidates that are comparable to, or even better than, those of larger models. This demonstrated performance affirms its potential to generate high-quality reward functions without requiring any preliminary human-defined metrics, thereby reducing engineering overhead and enhancing generalizability.",
            "headline_zh": "提出LEARN-Opt框架，以自动化强化学习中的奖励函数设计",
            "intro_zh": [
                "强化学习中奖励函数设计依赖专家知识，耗时且低效",
                "LEARN-Opt基于LLM自动生成和评估奖励函数，无需预定义指标",
                "实验显示性能媲美或优于现有方法，降低先验知识需求"
            ],
            "tags_zh": [
                "强化学习",
                "奖励函数设计",
                "大语言模型",
                "自动化框架",
                "无监督评估"
            ],
            "_index": 20
        },
        {
            "title": "CellFMCount: A Fluorescence Microscopy Dataset, Benchmark, and Methods for Cell Counting",
            "authors": [
                "Abdurahman Ali Mohammed",
                "Catherine Fonder",
                "Ying Wei",
                "Wallapak Tavanapong",
                "Donald S Sakaguchi",
                "Qi Li",
                "Surya K. Mallapragada"
            ],
            "arxiv_id": "2511.19351v1",
            "summary": "Accurate cell counting is essential in various biomedical research and clinical applications, including cancer diagnosis, stem cell research, and immunology. Manual counting is labor-intensive and error-prone, motivating automation through deep learning techniques. However, training reliable deep learning models requires large amounts of high-quality annotated data, which is difficult and time-consuming to produce manually. Consequently, existing cell-counting datasets are often limited, frequently containing fewer than $500$ images. In this work, we introduce a large-scale annotated dataset comprising $3{,}023$ images from immunocytochemistry experiments related to cellular differentiation, containing over $430{,}000$ manually annotated cell locations. The dataset presents significant challenges: high cell density, overlapping and morphologically diverse cells, a long-tailed distribution of cell count per image, and variation in staining protocols. We benchmark three categories of existing methods: regression-based, crowd-counting, and cell-counting techniques on a test set with cell counts ranging from $10$ to $2{,}126$ cells per image. We also evaluate how the Segment Anything Model (SAM) can be adapted for microscopy cell counting using only dot-annotated datasets. As a case study, we implement a density-map-based adaptation of SAM (SAM-Counter) and report a mean absolute error (MAE) of $22.12$, which outperforms existing approaches (second-best MAE of $27.46$). Our results underscore the value of the dataset and the benchmarking framework for driving progress in automated cell counting and provide a robust foundation for future research and development.",
            "headline_zh": "提出CellFMCount数据集与SAM-Counter方法以解决荧光显微镜细胞计数难题",
            "intro_zh": [
                "细胞计数在生物医学中关键但手动计数费时易错，需自动化方法",
                "引入大规模数据集和基准测试，并适配SAM模型用于细胞计数",
                "SAM-Counter方法在测试集上MAE为22.12，优于现有方法"
            ],
            "tags_zh": [
                "细胞计数",
                "荧光显微镜数据集",
                "密度图方法",
                "SAM模型适配",
                "基准测试"
            ],
            "_index": 21
        },
        {
            "title": "Syn-GRPO: Self-Evolving Data Synthesis for MLLM Perception Reasoning",
            "authors": [
                "Qihan Huang",
                "Haofei Zhang",
                "Rong Wei",
                "Yi Wang",
                "Rui Tang",
                "Mingli Song",
                "Jie Song"
            ],
            "arxiv_id": "2511.19343v1",
            "summary": "RL (reinforcement learning) methods (e.g., GRPO) for MLLM (Multimodal LLM) perception ability has attracted wide research interest owing to its remarkable generalization ability. Nevertheless, existing reinforcement learning methods still face the problem of low data quality, where data samples cannot elicit diverse responses from MLLMs, thus restricting the exploration scope for MLLM reinforcement learning. Some methods attempt to mitigate this problem by imposing constraints on entropy, but none address it at its root. Therefore, to tackle this problem, this work proposes Syn-GRPO (Synthesis-GRPO), which employs an online data generator to synthesize high-quality training data with diverse responses in GRPO training. Specifically, Syn-GRPO consists of two components: (1) data server; (2) GRPO workflow. The data server synthesizes new samples from existing ones using an image generation model, featuring a decoupled and asynchronous scheme to achieve high generation efficiency. The GRPO workflow provides the data server with the new image descriptions, and it leverages a diversity reward to supervise the MLLM to predict image descriptions for synthesizing samples with diverse responses. Experiment results across three visual perception tasks demonstrate that Syn-GRPO improves the data quality by a large margin, achieving significant superior performance to existing MLLM perception methods, and Syn-GRPO presents promising potential for scaling long-term self-evolving RL. Our code is available at https://github.com/hqhQAQ/Syn-GRPO.",
            "headline_zh": "提出Syn-GRPO以解决MLLM强化学习中数据质量低的问题",
            "intro_zh": [
                "核心问题：现有强化学习方法数据质量低，样本无法激发MLLM多样响应，限制探索范围。",
                "方法要点：使用在线数据生成器合成高质量训练数据，结合数据服务器和GRPO工作流提升多样性。",
                "实验或效果：在三个视觉感知任务中显著提升数据质量和性能，优于现有方法。"
            ],
            "tags_zh": [
                "多模态大语言模型",
                "强化学习",
                "数据合成",
                "视觉感知",
                "多样性奖励",
                "自演化强化学习"
            ],
            "_index": 22
        },
        {
            "title": "POUR: A Provably Optimal Method for Unlearning Representations via Neural Collapse",
            "authors": [
                "Anjie Le",
                "Can Peng",
                "Yuyuan Liu",
                "J. Alison Noble"
            ],
            "arxiv_id": "2511.19339v1",
            "summary": "In computer vision, machine unlearning aims to remove the influence of specific visual concepts or training images without retraining from scratch. Studies show that existing approaches often modify the classifier while leaving internal representations intact, resulting in incomplete forgetting. In this work, we extend the notion of unlearning to the representation level, deriving a three-term interplay between forgetting efficacy, retention fidelity, and class separation. Building on Neural Collapse theory, we show that the orthogonal projection of a simplex Equiangular Tight Frame (ETF) remains an ETF in a lower dimensional space, yielding a provably optimal forgetting operator. We further introduce the Representation Unlearning Score (RUS) to quantify representation-level forgetting and retention fidelity. Building on this, we introduce POUR (Provably Optimal Unlearning of Representations), a geometric projection method with closed-form (POUR-P) and a feature-level unlearning variant under a distillation scheme (POUR-D). Experiments on CIFAR-10/100 and PathMNIST demonstrate that POUR achieves effective unlearning while preserving retained knowledge, outperforming state-of-the-art unlearning methods on both classification-level and representation-level metrics.",
            "headline_zh": "提出POUR方法以在计算机视觉中实现表示级别的可证明最优遗忘",
            "intro_zh": [
                "核心问题：现有遗忘方法仅修改分类器，导致表示层面遗忘不完整。",
                "方法要点：基于神经崩溃理论，设计几何投影算子实现最优表示遗忘。",
                "实验效果：在CIFAR和PathMNIST数据集上，POUR在分类和表示级别指标优于现有方法。"
            ],
            "tags_zh": [
                "机器遗忘",
                "表示学习",
                "神经崩溃",
                "几何投影",
                "蒸馏训练",
                "分类性能"
            ],
            "_index": 23
        },
        {
            "title": "MonoMSK: Monocular 3D Musculoskeletal Dynamics Estimation",
            "authors": [
                "Farnoosh Koleini",
                "Hongfei Xue",
                "Ahmed Helmy",
                "Pu Wang"
            ],
            "arxiv_id": "2511.19326v1",
            "summary": "Reconstructing biomechanically realistic 3D human motion - recovering both kinematics (motion) and kinetics (forces) - is a critical challenge. While marker-based systems are lab-bound and slow, popular monocular methods use oversimplified, anatomically inaccurate models (e.g., SMPL) and ignore physics, fundamentally limiting their biomechanical fidelity. In this work, we introduce MonoMSK, a hybrid framework that bridges data-driven learning and physics-based simulation for biomechanically realistic 3D human motion estimation from monocular video. MonoMSK jointly recovers both kinematics (motions) and kinetics (forces and torques) through an anatomically accurate musculoskeletal model. By integrating transformer-based inverse dynamics with differentiable forward kinematics and dynamics layers governed by ODE-based simulation, MonoMSK establishes a physics-regulated inverse-forward loop that enforces biomechanical causality and physical plausibility. A novel forward-inverse consistency loss further aligns motion reconstruction with the underlying kinetic reasoning. Experiments on BML-MoVi, BEDLAM, and OpenCap show that MonoMSK significantly outperforms state-of-the-art methods in kinematic accuracy, while for the first time enabling precise monocular kinetics estimation.",
            "headline_zh": "提出MonoMSK框架，通过单目视频估计生物力学真实的3D人体运动与力。",
            "intro_zh": [
                "核心问题：现有单目方法使用简化模型忽略物理，限制生物力学保真度。",
                "方法要点：结合数据驱动与物理模拟，使用解剖准确模型联合估计运动与力。",
                "实验效果：在多个数据集上显著提升运动精度，首次实现精确单目力估计。"
            ],
            "tags_zh": [
                "单目3D估计",
                "生物力学建模",
                "物理模拟",
                "运动与力联合估计",
                "变换器逆动力学"
            ],
            "_index": 24
        },
        {
            "title": "SteadyDancer: Harmonized and Coherent Human Image Animation with First-Frame Preservation",
            "authors": [
                "Jiaming Zhang",
                "Shengming Cao",
                "Rui Li",
                "Xiaotong Zhao",
                "Yutao Cui",
                "Xinglin Hou",
                "Gangshan Wu",
                "Haolan Chen",
                "Yu Xu",
                "Limin Wang",
                "Kai Ma"
            ],
            "arxiv_id": "2511.19320v1",
            "summary": "Preserving first-frame identity while ensuring precise motion control is a fundamental challenge in human image animation. The Image-to-Motion Binding process of the dominant Reference-to-Video (R2V) paradigm overlooks critical spatio-temporal misalignments common in real-world applications, leading to failures such as identity drift and visual artifacts. We introduce SteadyDancer, an Image-to-Video (I2V) paradigm-based framework that achieves harmonized and coherent animation and is the first to ensure first-frame preservation robustly. Firstly, we propose a Condition-Reconciliation Mechanism to harmonize the two conflicting conditions, enabling precise control without sacrificing fidelity. Secondly, we design Synergistic Pose Modulation Modules to generate an adaptive and coherent pose representation that is highly compatible with the reference image. Finally, we employ a Staged Decoupled-Objective Training Pipeline that hierarchically optimizes the model for motion fidelity, visual quality, and temporal coherence. Experiments demonstrate that SteadyDancer achieves state-of-the-art performance in both appearance fidelity and motion control, while requiring significantly fewer training resources than comparable methods.",
            "headline_zh": "提出SteadyDancer框架以解决人类图像动画中的身份漂移和运动控制问题",
            "intro_zh": [
                "核心问题：现有方法在图像到视频动画中忽视时空错位，导致身份漂移和视觉伪影",
                "方法要点：引入条件调和机制和协同姿态调制模块，确保第一帧身份保留和运动精确控制",
                "实验或效果：在保真度和运动控制上达到先进水平，且训练资源需求较低"
            ],
            "tags_zh": [
                "人类图像动画",
                "第一帧保留",
                "条件调和",
                "姿态调制",
                "分阶段训练",
                "时空一致性"
            ],
            "_index": 25
        },
        {
            "title": "SyncMV4D: Synchronized Multi-view Joint Diffusion of Appearance and Motion for Hand-Object Interaction Synthesis",
            "authors": [
                "Lingwei Dang",
                "Zonghan Li",
                "Juntong Li",
                "Hongwen Zhang",
                "Liang An",
                "Yebin Liu",
                "Qingyao Wu"
            ],
            "arxiv_id": "2511.19319v1",
            "summary": "Hand-Object Interaction (HOI) generation plays a critical role in advancing applications across animation and robotics. Current video-based methods are predominantly single-view, which impedes comprehensive 3D geometry perception and often results in geometric distortions or unrealistic motion patterns. While 3D HOI approaches can generate dynamically plausible motions, their dependence on high-quality 3D data captured in controlled laboratory settings severely limits their generalization to real-world scenarios. To overcome these limitations, we introduce SyncMV4D, the first model that jointly generates synchronized multi-view HOI videos and 4D motions by unifying visual prior, motion dynamics, and multi-view geometry. Our framework features two core innovations: (1) a Multi-view Joint Diffusion (MJD) model that co-generates HOI videos and intermediate motions, and (2) a Diffusion Points Aligner (DPA) that refines the coarse intermediate motion into globally aligned 4D metric point tracks. To tightly couple 2D appearance with 4D dynamics, we establish a closed-loop, mutually enhancing cycle. During the diffusion denoising process, the generated video conditions the refinement of the 4D motion, while the aligned 4D point tracks are reprojected to guide next-step joint generation. Experimentally, our method demonstrates superior performance to state-of-the-art alternatives in visual realism, motion plausibility, and multi-view consistency.",
            "headline_zh": "提出SyncMV4D模型，联合生成同步多视角手物交互视频与4D运动，以解决单视角几何失真和3D方法泛化差问题。",
            "intro_zh": [
                "核心问题：单视角HOI生成易导致几何失真，3D方法依赖实验室数据，泛化能力差。",
                "方法要点：采用多视角联合扩散模型和扩散点对齐器，耦合2D外观与4D动态。",
                "实验效果：在视觉真实感、运动合理性和多视角一致性上优于现有方法。"
            ],
            "tags_zh": [
                "手物交互合成",
                "多视角视频生成",
                "4D运动生成",
                "联合扩散模型",
                "点云对齐"
            ],
            "_index": 26
        },
        {
            "title": "Evaluating Dataset Watermarking for Fine-tuning Traceability of Customized Diffusion Models: A Comprehensive Benchmark and Removal Approach",
            "authors": [
                "Xincheng Wang",
                "Hanchi Sun",
                "Wenjun Sun",
                "Kejun Xue",
                "Wangqiu Zhou",
                "Jianbo Zhang",
                "Wei Sun",
                "Dandan Zhu",
                "Xiongkuo Min",
                "Jun Jia",
                "Zhijun Fang"
            ],
            "arxiv_id": "2511.19316v1",
            "summary": "Recent fine-tuning techniques for diffusion models enable them to reproduce specific image sets, such as particular faces or artistic styles, but also introduce copyright and security risks. Dataset watermarking has been proposed to ensure traceability by embedding imperceptible watermarks into training images, which remain detectable in outputs even after fine-tuning. However, current methods lack a unified evaluation framework. To address this, this paper establishes a general threat model and introduces a comprehensive evaluation framework encompassing Universality, Transmissibility, and Robustness. Experiments show that existing methods perform well in universality and transmissibility, and exhibit some robustness against common image processing operations, yet still fall short under real-world threat scenarios. To reveal these vulnerabilities, the paper further proposes a practical watermark removal method that fully eliminates dataset watermarks without affecting fine-tuning, highlighting a key challenge for future research.",
            "headline_zh": "提出数据集水印评估框架与移除方法以解决定制扩散模型微调溯源问题",
            "intro_zh": [
                "核心问题：扩散模型微调可复制特定图像集，但带来版权和安全风险，缺乏统一水印评估标准。",
                "方法要点：建立通用威胁模型和评估框架，涵盖通用性、可传递性和鲁棒性指标。",
                "实验或效果：现有方法在通用性和可传递性表现良好，但对真实威胁鲁棒性不足，提出有效移除方法。"
            ],
            "tags_zh": [
                "数据集水印",
                "扩散模型",
                "微调溯源",
                "评估框架",
                "水印移除",
                "版权保护"
            ],
            "_index": 27
        },
        {
            "title": "Rethinking Intermediate Representation for VLM-based Robot Manipulation",
            "authors": [
                "Weiliang Tang",
                "Jialin Gao",
                "Jia-Hui Pan",
                "Gang Wang",
                "Li Erran Li",
                "Yunhui Liu",
                "Mingyu Ding",
                "Pheng-Ann Heng",
                "Chi-Wing Fu"
            ],
            "arxiv_id": "2511.19315v1",
            "summary": "Vision-Language Model (VLM) is an important component to enable robust robot manipulation. Yet, using it to translate human instructions into an action-resolvable intermediate representation often needs a tradeoff between VLM-comprehensibility and generalizability. Inspired by context-free grammar, we design the Semantic Assembly representation named SEAM, by decomposing the intermediate representation into vocabulary and grammar. Doing so leads us to a concise vocabulary of semantically-rich operations and a VLM-friendly grammar for handling diverse unseen tasks. In addition, we design a new open-vocabulary segmentation paradigm with a retrieval-augmented few-shot learning strategy to localize fine-grained object parts for manipulation, effectively with the shortest inference time over all state-of-the-art parallel works. Also, we formulate new metrics for action-generalizability and VLM-comprehensibility, demonstrating the compelling performance of SEAM over mainstream representations on both aspects. Extensive real-world experiments further manifest its SOTA performance under varying settings and tasks.",
            "headline_zh": "提出SEAM中间表示以提升VLM机器人操作的可理解性与泛化性",
            "intro_zh": [
                "核心问题：VLM中间表示在可理解性与泛化性间存在权衡",
                "方法要点：基于上下文无关语法设计SEAM表示，分解为词汇与语法",
                "实验或效果：在真实世界实验中实现SOTA性能，并定义新评估指标"
            ],
            "tags_zh": [
                "视觉语言模型",
                "机器人操作",
                "中间表示",
                "开放词汇分割",
                "检索增强学习",
                "语义组装"
            ],
            "_index": 28
        },
        {
            "title": "Dual-Granularity Semantic Prompting for Language Guidance Infrared Small Target Detection",
            "authors": [
                "Zixuan Wang",
                "Haoran Sun",
                "Jiaming Lu",
                "Wenxuan Wang",
                "Zhongling Huang",
                "Dingwen Zhang",
                "Xuelin Qian",
                "Junwei Han"
            ],
            "arxiv_id": "2511.19306v1",
            "summary": "Infrared small target detection remains challenging due to limited feature representation and severe background interference, resulting in sub-optimal performance. While recent CLIP-inspired methods attempt to leverage textual guidance for detection, they are hindered by inaccurate text descriptions and reliance on manual annotations. To overcome these limitations, we propose DGSPNet, an end-to-end language prompt-driven framework. Our approach integrates dual-granularity semantic prompts: coarse-grained textual priors (e.g., 'infrared image', 'small target') and fine-grained personalized semantic descriptions derived through visual-to-textual mapping within the image space. This design not only facilitates learning fine-grained semantic information but also can inherently leverage language prompts during inference without relying on any annotation requirements. By fully leveraging the precision and conciseness of text descriptions, we further introduce a text-guide channel attention (TGCA) mechanism and text-guide spatial attention (TGSA) mechanism that enhances the model's sensitivity to potential targets across both low- and high-level feature spaces. Extensive experiments demonstrate that our method significantly improves detection accuracy and achieves state-of-the-art performance on three benchmark datasets.",
            "headline_zh": "提出DGSPNet以解决红外小目标检测中特征表示不足和背景干扰问题",
            "intro_zh": [
                "核心问题：红外小目标检测因特征表示有限和背景干扰严重导致性能不佳",
                "方法要点：集成双粒度语义提示，包括粗粒度文本先验和细粒度视觉到文本映射",
                "实验或效果：在三个基准数据集上显著提升检测精度，达到最先进水平"
            ],
            "tags_zh": [
                "红外小目标检测",
                "语义提示",
                "语言引导",
                "注意力机制",
                "端到端框架"
            ],
            "_index": 29
        },
        {
            "title": "IDEAL-M3D: Instance Diversity-Enriched Active Learning for Monocular 3D Detection",
            "authors": [
                "Johannes Meier",
                "Florian Günther",
                "Riccardo Marin",
                "Oussema Dhaouadi",
                "Jacques Kaiser",
                "Daniel Cremers"
            ],
            "arxiv_id": "2511.19301v1",
            "summary": "Monocular 3D detection relies on just a single camera and is therefore easy to deploy. Yet, achieving reliable 3D understanding from monocular images requires substantial annotation, and 3D labels are especially costly. To maximize performance under constrained labeling budgets, it is essential to prioritize annotating samples expected to deliver the largest performance gains. This prioritization is the focus of active learning. Curiously, we observed two significant limitations in active learning algorithms for 3D monocular object detection. First, previous approaches select entire images, which is inefficient, as non-informative instances contained in the same image also need to be labeled. Secondly, existing methods rely on uncertainty-based selection, which in monocular 3D object detection creates a bias toward depth ambiguity. Consequently, distant objects are selected, while nearby objects are overlooked.\n  To address these limitations, we propose IDEAL-M3D, the first instance-level pipeline for monocular 3D detection. For the first time, we demonstrate that an explicitly diverse, fast-to-train ensemble improves diversity-driven active learning for monocular 3D. We induce diversity with heterogeneous backbones and task-agnostic features, loss weight perturbation, and time-dependent bagging. IDEAL-M3D shows superior performance and significant resource savings: with just 60% of the annotations, we achieve similar or better AP3D on KITTI validation and test set results compared to training the same detector on the whole dataset.",
            "headline_zh": "提出IDEAL-M3D以解决单目3D检测中主动学习的实例多样性和标注效率问题",
            "intro_zh": [
                "核心问题：现有主动学习方法选择整张图像，效率低，且偏向深度模糊的远距离物体",
                "方法要点：采用实例级管道，通过异构骨干网络和任务无关特征增强多样性",
                "实验或效果：在KITTI数据集上，仅用60%标注达到相似或更高AP3D，节省资源"
            ],
            "tags_zh": [
                "单目3D检测",
                "主动学习",
                "实例多样性",
                "标注效率",
                "深度估计",
                "KITTI数据集"
            ],
            "_index": 30
        },
        {
            "title": "DensifyBeforehand: LiDAR-assisted Content-aware Densification for Efficient and Quality 3D Gaussian Splatting",
            "authors": [
                "Phurtivilai Patt",
                "Leyang Huang",
                "Yinqiang Zhang",
                "Yang Lei"
            ],
            "arxiv_id": "2511.19294v1",
            "summary": "This paper addresses the limitations of existing 3D Gaussian Splatting (3DGS) methods, particularly their reliance on adaptive density control, which can lead to floating artifacts and inefficient resource usage. We propose a novel densify beforehand approach that enhances the initialization of 3D scenes by combining sparse LiDAR data with monocular depth estimation from corresponding RGB images. Our ROI-aware sampling scheme prioritizes semantically and geometrically important regions, yielding a dense point cloud that improves visual fidelity and computational efficiency. This densify beforehand approach bypasses the adaptive density control that may introduce redundant Gaussians in the original pipeline, allowing the optimization to focus on the other attributes of 3D Gaussian primitives, reducing overlap while enhancing visual quality. Our method achieves comparable results to state-of-the-art techniques while significantly lowering resource consumption and training time. We validate our approach through extensive comparisons and ablation studies on four newly collected datasets, showcasing its effectiveness in preserving regions of interest in complex scenes.",
            "headline_zh": "提出LiDAR辅助内容感知预稠密化方法，以提升3D高斯泼溅的效率与质量",
            "intro_zh": [
                "核心问题：现有3D高斯泼溅依赖自适应密度控制，易产生漂浮伪影和资源浪费",
                "方法要点：结合稀疏LiDAR和单目深度估计，采用ROI感知采样预稠密化场景",
                "实验或效果：在四个新数据集上验证，降低资源消耗和训练时间，保持视觉质量"
            ],
            "tags_zh": [
                "3D高斯泼溅",
                "LiDAR辅助",
                "内容感知稠密化",
                "ROI感知采样",
                "计算效率优化"
            ],
            "_index": 31
        },
        {
            "title": "ReMatch: Boosting Representation through Matching for Multimodal Retrieval",
            "authors": [
                "Qianying Liu",
                "Xiao Liang",
                "Zhiqiang Zhang",
                "Yibo Chen",
                "Xu Tang",
                "Zhongfei Qing",
                "Fengfan Zhou",
                "Yao Hu",
                "Paul Henderson"
            ],
            "arxiv_id": "2511.19278v1",
            "summary": "We present ReMatch, a framework that leverages the generative strength of MLLMs for multimodal retrieval. Previous approaches treated an MLLM as a simple encoder, ignoring its generative nature, and under-utilising its compositional reasoning and world knowledge. We instead train the embedding MLLM end-to-end with a chat-style generative matching stage. The matching stage uses the same MLLM to autoregressively decide relevance from multi-view inputs, including both raw data and its own projected embeddings for each query and document. It provides instance-wise discrimination supervision that complements a standard contrastive loss, offering stronger gradients on hard negatives and preserving the compositional strengths of the original MLLM. To obtain semantically richer multimodal embeddings, we use multiple learnable tokens to augment each input, generating fine-grained contextual, mutually orthogonal embeddings with low inference cost. Leveraging our established high-performance baseline,we assemble the ideas mentioned above into a powerful training recipe and achieve a new state-of-the-art on the Massive Multimodal Embedding Benchmark (MMEB). Our experiments show particularly strong zero-shot generalization results on five datasets, highlighting the robustness and transferability of ReMatch.",
            "headline_zh": "提出ReMatch框架，利用MLLM的生成能力增强多模态检索性能。",
            "intro_zh": [
                "核心问题：现有方法将MLLM仅用作编码器，忽略其生成特性，导致推理能力利用不足。",
                "方法要点：端到端训练嵌入MLLM，结合聊天式生成匹配阶段，提供实例级判别监督。",
                "实验效果：在MMEB基准上实现新SOTA，零样本泛化在五个数据集上表现优异。"
            ],
            "tags_zh": [
                "多模态检索",
                "生成匹配",
                "零样本泛化",
                "MLLM训练",
                "嵌入增强"
            ],
            "_index": 32
        },
        {
            "title": "Diffusion Reconstruction-based Data Likelihood Estimation for Core-Set Selection",
            "authors": [
                "Mingyang Chen",
                "Jiawei Du",
                "Bo Huang",
                "Yi Wang",
                "Xiaobo Zhang",
                "Wei Wang"
            ],
            "arxiv_id": "2511.19274v1",
            "summary": "Existing core-set selection methods predominantly rely on heuristic scoring signals such as training dynamics or model uncertainty, lacking explicit modeling of data likelihood. This omission may hinder the constructed subset from capturing subtle yet critical distributional structures that underpin effective model training. In this work, we propose a novel, theoretically grounded approach that leverages diffusion models to estimate data likelihood via reconstruction deviation induced by partial reverse denoising. Specifically, we establish a formal connection between reconstruction error and data likelihood, grounded in the Evidence Lower Bound (ELBO) of Markovian diffusion processes, thereby enabling a principled, distribution-aware scoring criterion for data selection. Complementarily, we introduce an efficient information-theoretic method to identify the optimal reconstruction timestep, ensuring that the deviation provides a reliable signal indicative of underlying data likelihood. Extensive experiments on ImageNet demonstrate that reconstruction deviation offers an effective scoring criterion, consistently outperforming existing baselines across selection ratios, and closely matching full-data training using only 50% of the data. Further analysis shows that the likelihood-informed nature of our score reveals informative insights in data selection, shedding light on the interplay between data distributional characteristics and model learning preferences.",
            "headline_zh": "提出基于扩散重建的数据似然估计方法以优化核心集选择",
            "intro_zh": [
                "现有核心集选择方法依赖启发式评分，缺乏数据似然显式建模",
                "利用扩散模型通过部分反向去噪重建偏差估计数据似然",
                "在ImageNet上实验，仅用50%数据接近全数据训练效果"
            ],
            "tags_zh": [
                "核心集选择",
                "扩散模型",
                "数据似然估计",
                "重建偏差",
                "信息理论优化"
            ],
            "_index": 33
        },
        {
            "title": "BideDPO: Conditional Image Generation with Simultaneous Text and Condition Alignment",
            "authors": [
                "Dewei Zhou",
                "Mingwei Li",
                "Zongxin Yang",
                "Yu Lu",
                "Yunqiu Xu",
                "Zhizhong Wang",
                "Zeyi Huang",
                "Yi Yang"
            ],
            "arxiv_id": "2511.19268v1",
            "summary": "Conditional image generation enhances text-to-image synthesis with structural, spatial, or stylistic priors, but current methods face challenges in handling conflicts between sources. These include 1) input-level conflicts, where the conditioning image contradicts the text prompt, and 2) model-bias conflicts, where generative biases disrupt alignment even when conditions match the text. Addressing these conflicts requires nuanced solutions, which standard supervised fine-tuning struggles to provide. Preference-based optimization techniques like Direct Preference Optimization (DPO) show promise but are limited by gradient entanglement between text and condition signals and lack disentangled training data for multi-constraint tasks. To overcome this, we propose a bidirectionally decoupled DPO framework (BideDPO). Our method creates two disentangled preference pairs-one for the condition and one for the text-to reduce gradient entanglement. The influence of pairs is managed using an Adaptive Loss Balancing strategy for balanced optimization. We introduce an automated data pipeline to sample model outputs and generate conflict-aware data. This process is embedded in an iterative optimization strategy that refines both the model and the data. We construct a DualAlign benchmark to evaluate conflict resolution between text and condition. Experiments show BideDPO significantly improves text success rates (e.g., +35%) and condition adherence. We also validate our approach using the COCO dataset. Project Pages: https://limuloo.github.io/BideDPO/.",
            "headline_zh": "提出BideDPO框架以解决条件图像生成中的文本与条件冲突问题",
            "intro_zh": [
                "核心问题：条件图像生成中文本与条件源冲突，包括输入级和模型偏置冲突",
                "方法要点：使用双向解耦偏好对和自适应损失平衡策略减少梯度纠缠",
                "实验或效果：在DualAlign基准上显著提升文本成功率和条件遵循度"
            ],
            "tags_zh": [
                "条件图像生成",
                "偏好优化",
                "梯度解耦",
                "冲突解决",
                "自适应损失平衡"
            ],
            "_index": 34
        },
        {
            "title": "LAST: LeArning to Think in Space and Time for Generalist Vision-Language Models",
            "authors": [
                "Shuai Wang",
                "Daoan Zhang",
                "Tianyi Bai",
                "Shitong Shao",
                "Jiebo Luo",
                "Jiaheng Wei"
            ],
            "arxiv_id": "2511.19261v1",
            "summary": "Humans can perceive and understand 3D space and long videos from sequential visual observations. But do vision-language models (VLMs) can? Recent work demonstrates that even state-of-the-art VLMs still struggle to understand 3D space and long videos, although they are powerful in typical vision-language tasks. Current methods often rely on specialized architectural designs to improve performance for 3D tasks and video understanding tasks separately. In contrast, we propose LAST, short for LeArn to Think in Space and Time, to jointly improve 3D spatial and long video understanding for general VLMs with only a set of 2D images as inputs. LAST makes VLMs think in space and time rather than only with text before giving the final answer, building visual thinking trajectories in 3D space and temporal dimension. We demonstrate the effectiveness of LAST in two scenarios: 1) zero-shot, where we directly prompt proprietary models; and 2) fine-tuning general VLMs with data that include thinking trajectories in 3D space and time. We show that LAST brings substantial gains in various benchmarks, including 3 spatial understanding, 4 video understanding, and 3 image understanding tasks. Notably, 15.8% gains on EgoSchema with GPT-4o in a zero-shot manner and 8.3 gains on VSI-Bench compared with Qwen2.5-VL-7B.",
            "headline_zh": "提出LAST方法，通过空间与时间思维轨迹提升通用视觉语言模型在3D空间和长视频理解能力",
            "intro_zh": [
                "核心问题：当前视觉语言模型难以理解3D空间和长视频，依赖专用架构",
                "方法要点：使用2D图像输入，构建视觉思维轨迹，联合优化空间与时间理解",
                "实验或效果：在零样本和微调场景下，多任务基准显著提升，如EgoSchema增益15.8%"
            ],
            "tags_zh": [
                "视觉语言模型",
                "3D空间理解",
                "长视频理解",
                "思维轨迹",
                "零样本学习",
                "微调优化"
            ],
            "_index": 35
        },
        {
            "title": "Adversarial Patch Attacks on Vision-Based Cargo Occupancy Estimation via Differentiable 3D Simulation",
            "authors": [
                "Mohamed Rissal Hedna",
                "Sesugh Samuel Nder"
            ],
            "arxiv_id": "2511.19254v1",
            "summary": "Computer vision systems are increasingly adopted in modern logistics operations, including the estimation of trailer occupancy for planning, routing, and billing. Although effective, such systems may be vulnerable to physical adversarial attacks, particularly adversarial patches that can be printed and placed on interior surfaces. In this work, we study the feasibility of such attacks on a convolutional cargo-occupancy classifier using fully simulated 3D environments. Using Mitsuba 3 for differentiable rendering, we optimize patch textures across variations in geometry, lighting, and viewpoint, and compare their effectiveness to a 2D compositing baseline. Our experiments demonstrate that 3D-optimized patches achieve high attack success rates, especially in a denial-of-service scenario (empty to full), where success reaches 84.94 percent. Concealment attacks (full to empty) prove more challenging but still reach 30.32 percent. We analyze the factors influencing attack success, discuss implications for the security of automated logistics pipelines, and highlight directions for strengthening physical robustness. To our knowledge, this is the first study to investigate adversarial patch attacks for cargo-occupancy estimation in physically realistic, fully simulated 3D scenes.",
            "headline_zh": "提出基于可微分3D模拟的对抗补丁攻击方法，评估物流视觉系统安全性",
            "intro_zh": [
                "核心问题：物流视觉系统易受物理对抗攻击，如补丁导致分类错误",
                "方法要点：使用Mitsuba 3进行可微分渲染，优化补丁纹理适应3D环境变化",
                "实验或效果：3D优化补丁在拒绝服务攻击中成功率84.94%，隐蔽攻击为30.32%"
            ],
            "tags_zh": [
                "对抗补丁攻击",
                "可微分渲染",
                "3D模拟",
                "物流视觉系统",
                "物理对抗攻击"
            ],
            "_index": 36
        },
        {
            "title": "FedPoisonTTP: A Threat Model and Poisoning Attack for Federated Test-Time Personalization",
            "authors": [
                "Md Akil Raihan Iftee",
                "Syed Md. Ahnaf Hasan",
                "Amin Ahsan Ali",
                "AKM Mahbubur Rahman",
                "Sajib Mistry",
                "Aneesh Krishna"
            ],
            "arxiv_id": "2511.19248v1",
            "summary": "Test-time personalization in federated learning enables models at clients to adjust online to local domain shifts, enhancing robustness and personalization in deployment. Yet, existing federated learning work largely overlooks the security risks that arise when local adaptation occurs at test time. Heterogeneous domain arrivals, diverse adaptation algorithms, and limited cross-client visibility create vulnerabilities where compromised participants can craft poisoned inputs and submit adversarial updates that undermine both global and per-client performance. To address this threat, we introduce FedPoisonTTP, a realistic grey-box attack framework that explores test-time data poisoning in the federated adaptation setting. FedPoisonTTP distills a surrogate model from adversarial queries, synthesizes in-distribution poisons using feature-consistency, and optimizes attack objectives to generate high-entropy or class-confident poisons that evade common adaptation filters. These poisons are injected during local adaptation and spread through collaborative updates, leading to broad degradation. Extensive experiments on corrupted vision benchmarks show that compromised participants can substantially diminish overall test-time performance.",
            "headline_zh": "提出FedPoisonTTP威胁模型与攻击方法，以解决联邦测试时个性化中的安全风险",
            "intro_zh": [
                "核心问题：联邦测试时个性化中，本地适应易受中毒攻击，威胁全局与客户端性能",
                "方法要点：利用代理模型蒸馏和特征一致性合成中毒输入，优化攻击目标以规避过滤",
                "实验或效果：在视觉基准测试中，中毒攻击显著降低测试时性能"
            ],
            "tags_zh": [
                "联邦学习",
                "测试时个性化",
                "中毒攻击",
                "威胁模型",
                "对抗性更新",
                "安全漏洞"
            ],
            "_index": 37
        },
        {
            "title": "SENTINEL: A Fully End-to-End Language-Action Model for Humanoid Whole Body Control",
            "authors": [
                "Yuxuan Wang",
                "Haobin Jiang",
                "Shiqing Yao",
                "Ziluo Ding",
                "Zongqing Lu"
            ],
            "arxiv_id": "2511.19236v1",
            "summary": "Existing humanoid control systems often rely on teleoperation or modular generation pipelines that separate language understanding from physical execution. However, the former is entirely human-driven, and the latter lacks tight alignment between language commands and physical behaviors. In this paper, we present SENTINEL, a fully end-to-end language-action model for humanoid whole-body control. We construct a large-scale dataset by tracking human motions in simulation using a pretrained whole body controller, combined with their text annotations. The model directly maps language commands and proprioceptive inputs to low-level actions without any intermediate representation. The model generates action chunks using flow matching, which can be subsequently refined by a residual action head for real-world deployment. Our method exhibits strong semantic understanding and stable execution on humanoid robots in both simulation and real-world deployment, and also supports multi-modal extensions by converting inputs into texts.",
            "headline_zh": "提出SENTINEL端到端语言-动作模型以解决人形机器人全身控制中语言与行为对齐问题",
            "intro_zh": [
                "现有系统依赖遥操作或模块化管道，导致语言理解与物理执行分离",
                "模型直接映射语言命令和本体感觉输入到低级动作，使用流匹配生成动作块",
                "在仿真和真实部署中展示强语义理解和稳定执行，支持多模态扩展"
            ],
            "tags_zh": [
                "人形机器人控制",
                "端到端学习",
                "语言-动作映射",
                "流匹配",
                "全身控制",
                "多模态扩展"
            ],
            "_index": 38
        },
        {
            "title": "IDSplat: Instance-Decomposed 3D Gaussian Splatting for Driving Scenes",
            "authors": [
                "Carl Lindström",
                "Mahan Rafidashti",
                "Maryam Fatemi",
                "Lars Hammarstrand",
                "Martin R. Oswald",
                "Lennart Svensson"
            ],
            "arxiv_id": "2511.19235v1",
            "summary": "Reconstructing dynamic driving scenes is essential for developing autonomous systems through sensor-realistic simulation. Although recent methods achieve high-fidelity reconstructions, they either rely on costly human annotations for object trajectories or use time-varying representations without explicit object-level decomposition, leading to intertwined static and dynamic elements that hinder scene separation. We present IDSplat, a self-supervised 3D Gaussian Splatting framework that reconstructs dynamic scenes with explicit instance decomposition and learnable motion trajectories, without requiring human annotations. Our key insight is to model dynamic objects as coherent instances undergoing rigid transformations, rather than unstructured time-varying primitives. For instance decomposition, we employ zero-shot, language-grounded video tracking anchored to 3D using lidar, and estimate consistent poses via feature correspondences. We introduce a coordinated-turn smoothing scheme to obtain temporally and physically consistent motion trajectories, mitigating pose misalignments and tracking failures, followed by joint optimization of object poses and Gaussian parameters. Experiments on the Waymo Open Dataset demonstrate that our method achieves competitive reconstruction quality while maintaining instance-level decomposition and generalizes across diverse sequences and view densities without retraining, making it practical for large-scale autonomous driving applications. Code will be released.",
            "headline_zh": "提出IDSplat以自监督重建动态驾驶场景，实现实例分解与运动轨迹学习",
            "intro_zh": [
                "核心问题：动态驾驶场景重建中，静态与动态元素交织，依赖人工标注或缺乏实例分解",
                "方法要点：使用零-shot语言跟踪与激光雷达锚定，建模动态对象为刚性变换实例",
                "实验或效果：在Waymo数据集上实现竞争性重建质量，无需重训练泛化多序列"
            ],
            "tags_zh": [
                "3D高斯泼溅",
                "实例分解",
                "自监督学习",
                "动态场景重建",
                "自动驾驶仿真",
                "运动轨迹优化"
            ],
            "_index": 39
        },
        {
            "title": "Learning Plug-and-play Memory for Guiding Video Diffusion Models",
            "authors": [
                "Selena Song",
                "Ziming Xu",
                "Zijun Zhang",
                "Kun Zhou",
                "Jiaxian Guo",
                "Lianhui Qin",
                "Biwei Huang"
            ],
            "arxiv_id": "2511.19229v1",
            "summary": "Diffusion Transformer(DiT) based video generation models have recently achieved impressive visual quality and temporal coherence, but they still frequently violate basic physical laws and commonsense dynamics, revealing a lack of explicit world knowledge. In this work, we explore how to equip them with a plug-and-play memory that injects useful world knowledge. Motivated by in-context memory in Transformer-based LLMs, we conduct empirical studies to show that DiT can be steered via interventions on its hidden states, and simple low-pass and high-pass filters in the embedding space naturally disentangle low-level appearance and high-level physical/semantic cues, enabling targeted guidance. Building on these observations, we propose a learnable memory encoder DiT-Mem, composed of stacked 3D CNNs, low-/high-pass filters, and self-attention layers. The encoder maps reference videos into a compact set of memory tokens, which are concatenated as the memory within the DiT self-attention layers. During training, we keep the diffusion backbone frozen, and only optimize the memory encoder. It yields a rather efficient training process on few training parameters (150M) and 10K data samples, and enables plug-and-play usage at inference time. Extensive experiments on state-of-the-art models demonstrate the effectiveness of our method in improving physical rule following and video fidelity. Our code and data are publicly released here: https://thrcle421.github.io/DiT-Mem-Web/.",
            "headline_zh": "提出可插拔记忆模块以提升视频扩散模型的世界知识注入能力",
            "intro_zh": [
                "核心问题：视频扩散模型常违反物理规律和常识动态，缺乏显式世界知识",
                "方法要点：设计可学习记忆编码器，通过干预隐藏状态注入参考视频知识",
                "实验或效果：在少量数据和参数下高效训练，提升视频物理规则遵循和保真度"
            ],
            "tags_zh": [
                "视频扩散模型",
                "可插拔记忆",
                "世界知识注入",
                "Transformer干预",
                "高效训练",
                "物理规则遵循"
            ],
            "_index": 40
        },
        {
            "title": "Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving",
            "authors": [
                "Jianhua Han",
                "Meng Tian",
                "Jiangtong Zhu",
                "Fan He",
                "Huixin Zhang",
                "Sitong Guo",
                "Dechang Zhu",
                "Hao Tang",
                "Pei Xu",
                "Yuze Guo",
                "Minzhe Niu",
                "Haojie Zhu",
                "Qichao Dong",
                "Xuechao Yan",
                "Siyuan Dong",
                "Lu Hou",
                "Qingqiu Huang",
                "Xiaosong Jia",
                "Hang Xu"
            ],
            "arxiv_id": "2511.19221v1",
            "summary": "Autonomous driving heavily relies on accurate and robust spatial perception. Many failures arise from inaccuracies and instability, especially in long-tail scenarios and complex interactions. However, current vision-language models are weak at spatial grounding and understanding, and VLA systems built on them therefore show limited perception and localization ability. To address these challenges, we introduce Percept-WAM, a perception-enhanced World-Awareness-Action Model that is the first to implicitly integrate 2D/3D scene understanding abilities within a single vision-language model (VLM). Instead of relying on QA-style spatial reasoning, Percept-WAM unifies 2D/3D perception tasks into World-PV and World-BEV tokens, which encode both spatial coordinates and confidence. We propose a grid-conditioned prediction mechanism for dense object perception, incorporating IoU-aware scoring and parallel autoregressive decoding, improving stability in long-tail, far-range, and small-object scenarios. Additionally, Percept-WAM leverages pretrained VLM parameters to retain general intelligence (e.g., logical reasoning) and can output perception results and trajectory control outputs directly. Experiments show that Percept-WAM matches or surpasses classical detectors and segmenters on downstream perception benchmarks, achieving 51.7/58.9 mAP on COCO 2D detection and nuScenes BEV 3D detection. When integrated with trajectory decoders, it further improves planning performance on nuScenes and NAVSIM, e.g., surpassing DiffusionDrive by 2.1 in PMDS on NAVSIM. Qualitative results further highlight its strong open-vocabulary and long-tail generalization.",
            "headline_zh": "提出Percept-WAM模型以增强端到端自动驾驶的感知鲁棒性",
            "intro_zh": [
                "核心问题：自动驾驶中空间感知不准确和不稳定，尤其在长尾场景和复杂交互中。",
                "方法要点：统一2D/3D感知任务于单一视觉语言模型，使用World-PV和World-BEV令牌编码空间信息。",
                "实验或效果：在COCO和nuScenes基准上超越经典检测器，集成轨迹解码器提升规划性能。"
            ],
            "tags_zh": [
                "自动驾驶",
                "视觉语言模型",
                "2D/3D感知",
                "长尾场景",
                "轨迹规划"
            ],
            "_index": 41
        },
        {
            "title": "Are Large Vision Language Models Truly Grounded in Medical Images? Evidence from Italian Clinical Visual Question Answering",
            "authors": [
                "Federico Felizzi",
                "Olivia Riccomi",
                "Michele Ferramola",
                "Francesco Andrea Causio",
                "Manuel Del Medico",
                "Vittorio De Vita",
                "Lorenzo De Mori",
                "Alessandra Piscitelli Pietro Eric Risuleo",
                "Bianca Destro Castaniti",
                "Antonio Cristiano Alessia Longo",
                "Luigi De Angelis",
                "Mariapia Vassalli",
                "Marcello Di Pumpo"
            ],
            "arxiv_id": "2511.19220v1",
            "summary": "Large vision language models (VLMs) have achieved impressive performance on medical visual question answering benchmarks, yet their reliance on visual information remains unclear. We investigate whether frontier VLMs demonstrate genuine visual grounding when answering Italian medical questions by testing four state-of-the-art models: Claude Sonnet 4.5, GPT-4o, GPT-5-mini, and Gemini 2.0 flash exp. Using 60 questions from the EuropeMedQA Italian dataset that explicitly require image interpretation, we substitute correct medical images with blank placeholders to test whether models truly integrate visual and textual information. Our results reveal striking variability in visual dependency: GPT-4o shows the strongest visual grounding with a 27.9pp accuracy drop (83.2% [74.6%, 91.7%] to 55.3% [44.1%, 66.6%]), while GPT-5-mini, Gemini, and Claude maintain high accuracy with modest drops of 8.5pp, 2.4pp, and 5.6pp respectively. Analysis of model-generated reasoning reveals confident explanations for fabricated visual interpretations across all models, suggesting varying degrees of reliance on textual shortcuts versus genuine visual analysis. These findings highlight critical differences in model robustness and the need for rigorous evaluation before clinical deployment.",
            "headline_zh": "评估大型视觉语言模型在意大利医学视觉问答中的视觉依赖性，揭示模型差异。",
            "intro_zh": [
                "核心问题：大型视觉语言模型是否真正依赖医学图像进行视觉问答。",
                "方法要点：使用空白图像替换测试模型视觉依赖性，分析四种前沿模型。",
                "实验效果：GPT-4o视觉依赖性最强，其他模型依赖文本捷径，准确率下降不一。"
            ],
            "tags_zh": [
                "视觉语言模型",
                "医学视觉问答",
                "视觉依赖性评估",
                "模型鲁棒性",
                "意大利数据集"
            ],
            "_index": 42
        },
        {
            "title": "ReAlign: Text-to-Motion Generation via Step-Aware Reward-Guided Alignment",
            "authors": [
                "Wanjiang Weng",
                "Xiaofeng Tan",
                "Junbo Wang",
                "Guo-Sen Xie",
                "Pan Zhou",
                "Hongsong Wang"
            ],
            "arxiv_id": "2511.19217v1",
            "summary": "Text-to-motion generation, which synthesizes 3D human motions from text inputs, holds immense potential for applications in gaming, film, and robotics. Recently, diffusion-based methods have been shown to generate more diversity and realistic motion. However, there exists a misalignment between text and motion distributions in diffusion models, which leads to semantically inconsistent or low-quality motions. To address this limitation, we propose Reward-guided sampling Alignment (ReAlign), comprising a step-aware reward model to assess alignment quality during the denoising sampling and a reward-guided strategy that directs the diffusion process toward an optimally aligned distribution. This reward model integrates step-aware tokens and combines a text-aligned module for semantic consistency and a motion-aligned module for realism, refining noisy motions at each timestep to balance probability density and alignment. Extensive experiments of both motion generation and retrieval tasks demonstrate that our approach significantly improves text-motion alignment and motion quality compared to existing state-of-the-art methods.",
            "headline_zh": "提出ReAlign方法，通过奖励引导对齐解决文本到运动生成中的语义不一致问题",
            "intro_zh": [
                "核心问题：扩散模型中文本与运动分布不匹配，导致语义不一致或低质量运动",
                "方法要点：引入步感知奖励模型和奖励引导策略，优化去噪过程以提升对齐",
                "实验或效果：在生成和检索任务中显著改进文本-运动对齐和运动质量"
            ],
            "tags_zh": [
                "文本到运动生成",
                "扩散模型",
                "奖励引导对齐",
                "步感知奖励",
                "语义一致性",
                "运动质量优化"
            ],
            "_index": 43
        },
        {
            "title": "Soft pneumatic grippers: Topology optimization, 3D-printing and experimental validation",
            "authors": [
                "Prabhat Kumar",
                "Chandra Prakash",
                "Josh Pinskier",
                "David Howard",
                "Matthijs Langelaar"
            ],
            "arxiv_id": "2511.19211v1",
            "summary": "This paper presents a systematic topology optimization framework for designing a soft pneumatic gripper (SPG), explicitly considering the design-dependent nature of the actuating load. The load is modeled using Darcy's law with an added drainage term. A 2D soft arm unit is optimized by formulating it as a compliant mechanism design problem using the robust formulation. The problem is posed as a min-max optimization, where the output deformations of blueprint and eroded designs are considered. A volume constraint is imposed on the blueprint part, while a strain-energy constraint is enforced on the eroded part. The MMA is employed to solve the optimization problem and obtain the optimized soft unit. Finite element analysis with the Ogden material model confirms that the optimized 2D unit outperforms a conventional rectangular design under pneumatic loading. The optimized 2D unit is extruded to obtain a 3D module, and ten such units are assembled to create a soft arm. Deformation profiles of the optimized arm are analysed under different pressure loads. Four arms are 3D-printed and integrated with a supporting structure to realize the proposed SPG. The gripping performance of the SPG is demonstrated on objects with different weights, sizes, stiffness, and shapes.",
            "headline_zh": "提出拓扑优化框架以设计软气动抓取器，提升抓取性能。",
            "intro_zh": [
                "核心问题：软气动抓取器设计中需处理依赖设计的驱动负载。",
                "方法要点：采用稳健拓扑优化，结合Darcy定律和应变能约束。",
                "实验或效果：优化单元优于传统设计，3D打印抓取器验证多对象抓取。"
            ],
            "tags_zh": [
                "拓扑优化",
                "软气动抓取器",
                "3D打印",
                "有限元分析",
                "稳健设计"
            ],
            "_index": 44
        },
        {
            "title": "Reference-Free Sampling-Based Model Predictive Control",
            "authors": [
                "Fabian Schramm",
                "Pierre Fabre",
                "Nicolas Perrin-Gilbert",
                "Justin Carpentier"
            ],
            "arxiv_id": "2511.19204v1",
            "summary": "We present a sampling-based model predictive control (MPC) framework that enables emergent locomotion without relying on handcrafted gait patterns or predefined contact sequences. Our method discovers diverse motion patterns, ranging from trotting to galloping, robust standing policies, jumping, and handstand balancing, purely through the optimization of high-level objectives. Building on model predictive path integral (MPPI), we propose a dual-space spline parameterization that operates on position and velocity control points. Our approach enables contact-making and contact-breaking strategies that adapt automatically to task requirements, requiring only a limited number of sampled trajectories. This sample efficiency allows us to achieve real-time control on standard CPU hardware, eliminating the need for GPU acceleration typically required by other state-of-the-art MPPI methods. We validate our approach on the Go2 quadrupedal robot, demonstrating various emergent gaits and basic jumping capabilities. In simulation, we further showcase more complex behaviors, such as backflips, dynamic handstand balancing and locomotion on a Humanoid, all without requiring reference tracking or offline pre-training.",
            "headline_zh": "提出无参考采样模型预测控制框架，实现四足机器人涌现运动",
            "intro_zh": [
                "核心问题：模型预测控制依赖预设步态或接触序列，限制运动多样性。",
                "方法要点：基于MPPI，采用双空间样条参数化，优化高层目标自动发现运动。",
                "实验效果：在Go2机器人上验证涌现步态和跳跃，仿真中实现后空翻等复杂行为。"
            ],
            "tags_zh": [
                "模型预测控制",
                "采样优化",
                "四足机器人",
                "涌现运动",
                "实时控制"
            ],
            "_index": 45
        },
        {
            "title": "NVGS: Neural Visibility for Occlusion Culling in 3D Gaussian Splatting",
            "authors": [
                "Brent Zoomers",
                "Florian Hahlbohm",
                "Joni Vanherck",
                "Lode Jorissen",
                "Marcus Magnor",
                "Nick Michiels"
            ],
            "arxiv_id": "2511.19202v1",
            "summary": "3D Gaussian Splatting can exploit frustum culling and level-of-detail strategies to accelerate rendering of scenes containing a large number of primitives. However, the semi-transparent nature of Gaussians prevents the application of another highly effective technique: occlusion culling. We address this limitation by proposing a novel method to learn the viewpoint-dependent visibility function of all Gaussians in a trained model using a small, shared MLP across instances of an asset in a scene. By querying it for Gaussians within the viewing frustum prior to rasterization, our method can discard occluded primitives during rendering. Leveraging Tensor Cores for efficient computation, we integrate these neural queries directly into a novel instanced software rasterizer. Our approach outperforms the current state of the art for composed scenes in terms of VRAM usage and image quality, utilizing a combination of our instanced rasterizer and occlusion culling MLP, and exhibits complementary properties to existing LoD techniques.",
            "headline_zh": "提出神经可见性方法以解决3D高斯泼溅中遮挡剔除问题",
            "intro_zh": [
                "核心问题：高斯半透明特性阻碍遮挡剔除，影响渲染效率。",
                "方法要点：使用共享MLP学习高斯可见性函数，集成到实例化光栅器中。",
                "实验或效果：在组合场景中，降低VRAM使用并提升图像质量。"
            ],
            "tags_zh": [
                "3D高斯泼溅",
                "遮挡剔除",
                "神经可见性",
                "实例化渲染",
                "MLP查询"
            ],
            "_index": 46
        },
        {
            "title": "Efficient Optimization of a Permanent Magnet Array for a Stable 2D Trap",
            "authors": [
                "Ann-Sophia Müller",
                "Moonkwang Jeong",
                "Jiyuan Tian",
                "Meng Zhang",
                "Tian Qiu"
            ],
            "arxiv_id": "2511.19201v1",
            "summary": "Untethered magnetic manipulation of biomedical millirobots has a high potential for minimally invasive surgical applications. However, it is still challenging to exert high actuation forces on the small robots over a large distance. Permanent magnets offer stronger magnetic torques and forces than electromagnetic coils, however, feedback control is more difficult. As proven by Earnshaw's theorem, it is not possible to achieve a stable magnetic trap in 3D by static permanent magnets. Here, we report a stable 2D magnetic force trap by an array of permanent magnets to control a millirobot. The trap is located in an open space with a tunable distance to the magnet array in the range of 20 - 120mm, which is relevant to human anatomical scales. The design is achieved by a novel GPU-accelerated optimization algorithm that uses mean squared error (MSE) and Adam optimizer to efficiently compute the optimal angles for any number of magnets in the array. The algorithm is verified using numerical simulation and physical experiments with an array of two magnets. A millirobot is successfully trapped and controlled to follow a complex trajectory. The algorithm demonstrates high scalability by optimizing the angles for 100 magnets in under three seconds. Moreover, the optimization workflow can be adapted to optimize a permanent magnet array to achieve the desired force vector fields.",
            "headline_zh": "提出基于GPU加速优化算法的永磁体阵列，实现稳定二维磁力陷阱以控制医疗微型机器人。",
            "intro_zh": [
                "核心问题：静态永磁体无法在三维空间实现稳定磁陷阱，且微型机器人在大距离下难以施加高驱动力。",
                "方法要点：使用均方误差和Adam优化器，GPU加速计算永磁体阵列中磁体的最优角度。",
                "实验或效果：通过仿真和物理实验验证，成功捕获并控制微型机器人沿复杂轨迹运动。"
            ],
            "tags_zh": [
                "永磁体阵列",
                "磁力陷阱",
                "GPU加速优化",
                "微型机器人控制",
                "Adam优化器"
            ],
            "_index": 47
        },
        {
            "title": "Can Modern Vision Models Understand the Difference Between an Object and a Look-alike?",
            "authors": [
                "Itay Cohen",
                "Ethan Fetaya",
                "Amir Rosenfeld"
            ],
            "arxiv_id": "2511.19200v1",
            "summary": "Recent advances in computer vision have yielded models with strong performance on recognition benchmarks; however, significant gaps remain in comparison to human perception. One subtle ability is to judge whether an image looks like a given object without being an instance of that object. We study whether vision-language models such as CLIP capture this distinction. We curated a dataset named RoLA (Real or Lookalike) of real and lookalike exemplars (e.g., toys, statues, drawings, pareidolia) across multiple categories, and first evaluate a prompt-based baseline with paired \"real\"/\"lookalike\" prompts. We then estimate a direction in CLIP's embedding space that moves representations between real and lookalike. Applying this direction to image and text embeddings improves discrimination in cross-modal retrieval on Conceptual12M, and also enhances captions produced by a CLIP prefix captioner.",
            "headline_zh": "提出RoLA数据集和嵌入方向方法，以评估视觉语言模型区分真实物体与相似物的能力。",
            "intro_zh": [
                "核心问题：视觉语言模型能否区分真实物体与相似物（如玩具、雕像），弥补与人类感知的差距。",
                "方法要点：构建RoLA数据集，估计CLIP嵌入空间中真实与相似物的方向，并应用于跨模态检索和图像描述。",
                "实验或效果：该方法在Conceptual12M上提升检索性能，并改进CLIP前缀描述器的描述质量。"
            ],
            "tags_zh": [
                "视觉语言模型",
                "相似物识别",
                "CLIP嵌入",
                "跨模态检索",
                "图像描述增强"
            ],
            "_index": 48
        },
        {
            "title": "CLASH: A Benchmark for Cross-Modal Contradiction Detection",
            "authors": [
                "Teodora Popordanoska",
                "Jiameng Li",
                "Matthew B. Blaschko"
            ],
            "arxiv_id": "2511.19199v1",
            "summary": "Contradictory multimodal inputs are common in real-world settings, yet existing benchmarks typically assume input consistency and fail to evaluate cross-modal contradiction detection - a fundamental capability for preventing hallucinations and ensuring reliability. We introduce CLASH, a novel benchmark for multimodal contradiction detection, featuring COCO images paired with contradictory captions containing controlled object-level or attribute-level contradictions. The samples include targeted questions evaluated in both multiple-choice and open-ended formats. The benchmark provides an extensive fine-tuning set filtered through automated quality checks, alongside a smaller human-verified diagnostic set. Our analysis of state-of-the-art models reveals substantial limitations in recognizing cross-modal conflicts, exposing systematic modality biases and category-specific weaknesses. Furthermore, we empirically demonstrate that targeted fine-tuning on CLASH substantially enhances conflict detection capabilities.",
            "headline_zh": "提出CLASH基准以解决多模态输入中矛盾检测的评估问题",
            "intro_zh": [
                "现实多模态输入常含矛盾，现有基准假设一致性，无法评估矛盾检测能力",
                "CLASH基准使用COCO图像与矛盾字幕，包含对象或属性级矛盾，提供多格式问题",
                "分析显示先进模型存在模态偏见，针对性微调可显著提升矛盾检测性能"
            ],
            "tags_zh": [
                "多模态基准",
                "矛盾检测",
                "图像字幕",
                "模型评估",
                "微调优化"
            ],
            "_index": 49
        },
        {
            "title": "Three-Dimensional Anatomical Data Generation Based on Artificial Neural Networks",
            "authors": [
                "Ann-Sophia Müller",
                "Moonkwang Jeong",
                "Meng Zhang",
                "Jiyuan Tian",
                "Arkadiusz Miernik",
                "Stefanie Speidel",
                "Tian Qiu"
            ],
            "arxiv_id": "2511.19198v1",
            "summary": "Surgical planning and training based on machine learning requires a large amount of 3D anatomical models reconstructed from medical imaging, which is currently one of the major bottlenecks. Obtaining these data from real patients and during surgery is very demanding, if even possible, due to legal, ethical, and technical challenges. It is especially difficult for soft tissue organs with poor imaging contrast, such as the prostate. To overcome these challenges, we present a novel workflow for automated 3D anatomical data generation using data obtained from physical organ models. We additionally use a 3D Generative Adversarial Network (GAN) to obtain a manifold of 3D models useful for other downstream machine learning tasks that rely on 3D data. We demonstrate our workflow using an artificial prostate model made of biomimetic hydrogels with imaging contrast in multiple zones. This is used to physically simulate endoscopic surgery. For evaluation and 3D data generation, we place it into a customized ultrasound scanner that records the prostate before and after the procedure. A neural network is trained to segment the recorded ultrasound images, which outperforms conventional, non-learning-based computer vision techniques in terms of intersection over union (IoU). Based on the segmentations, a 3D mesh model is reconstructed, and performance feedback is provided.",
            "headline_zh": "提出基于物理模型和3D GAN的自动化3D解剖数据生成工作流，以解决手术规划中数据获取瓶颈。",
            "intro_zh": [
                "核心问题：手术规划依赖3D解剖模型，但真实患者数据获取面临法律、伦理和技术挑战，尤其对低对比度软组织器官如前列腺。",
                "方法要点：使用物理器官模型和3D GAN生成3D数据，训练神经网络分割超声图像，并重建3D网格模型。",
                "实验或效果：在人工前列腺模型上验证，神经网络分割的IoU优于传统计算机视觉方法，并提供性能反馈。"
            ],
            "tags_zh": [
                "3D解剖数据生成",
                "生成对抗网络",
                "医学图像分割",
                "手术规划",
                "超声成像",
                "物理模型仿真"
            ],
            "_index": 50
        },
        {
            "title": "SpectraNet: FFT-assisted Deep Learning Classifier for Deepfake Face Detection",
            "authors": [
                "Nithira Jayarathne",
                "Naveen Basnayake",
                "Keshawa Jayasundara",
                "Pasindu Dodampegama",
                "Praveen Wijesinghe",
                "Hirushika Pelagewatta",
                "Kavishka Abeywardana",
                "Sandushan Ranaweera",
                "Chamira Edussooriya"
            ],
            "arxiv_id": "2511.19187v1",
            "summary": "Detecting deepfake images is crucial in combating misinformation. We present a lightweight, generalizable binary classification model based on EfficientNet-B6, fine-tuned with transformation techniques to address severe class imbalances. By leveraging robust preprocessing, oversampling, and optimization strategies, our model achieves high accuracy, stability, and generalization. While incorporating Fourier transform-based phase and amplitude features showed minimal impact, our proposed framework helps non-experts to effectively identify deepfake images, making significant strides toward accessible and reliable deepfake detection.",
            "headline_zh": "提出基于EfficientNet-B6的轻量级深度伪造人脸检测模型，以应对类别不平衡问题。",
            "intro_zh": [
                "核心问题：检测深度伪造图像以对抗错误信息，面临严重类别不平衡挑战。",
                "方法要点：采用EfficientNet-B6微调，结合预处理、过采样和优化策略提升模型性能。",
                "实验或效果：模型实现高准确率、稳定性和泛化能力，但傅里叶变换特征影响未知。"
            ],
            "tags_zh": [
                "深度伪造检测",
                "类别不平衡处理",
                "EfficientNet微调",
                "轻量级模型",
                "图像分类"
            ],
            "_index": 51
        },
        {
            "title": "nnActive: A Framework for Evaluation of Active Learning in 3D Biomedical Segmentation",
            "authors": [
                "Carsten T. Lüth",
                "Jeremias Traub",
                "Kim-Celine Kahl",
                "Till J. Bungert",
                "Lukas Klein",
                "Lars Krämer",
                "Paul F. Jaeger",
                "Fabian Isensee",
                "Klaus Maier-Hein"
            ],
            "arxiv_id": "2511.19183v1",
            "summary": "Semantic segmentation is crucial for various biomedical applications, yet its reliance on large annotated datasets presents a bottleneck due to the high cost and specialized expertise required for manual labeling. Active Learning (AL) aims to mitigate this challenge by querying only the most informative samples, thereby reducing annotation effort. However, in the domain of 3D biomedical imaging, there is no consensus on whether AL consistently outperforms Random sampling. Four evaluation pitfalls hinder the current methodological assessment. These are (1) restriction to too few datasets and annotation budgets, (2) using 2D models on 3D images without partial annotations, (3) Random baseline not being adapted to the task, and (4) measuring annotation cost only in voxels. In this work, we introduce nnActive, an open-source AL framework that overcomes these pitfalls by (1) means of a large scale study spanning four biomedical imaging datasets and three label regimes, (2) extending nnU-Net by using partial annotations for training with 3D patch-based query selection, (3) proposing Foreground Aware Random sampling strategies tackling the foreground-background class imbalance of medical images and (4) propose the foreground efficiency metric, which captures the low annotation cost of background-regions. We reveal the following findings: (A) while all AL methods outperform standard Random sampling, none reliably surpasses an improved Foreground Aware Random sampling; (B) benefits of AL depend on task specific parameters; (C) Predictive Entropy is overall the best performing AL method, but likely requires the most annotation effort; (D) AL performance can be improved with more compute intensive design choices. As a holistic, open-source framework, nnActive can serve as a catalyst for research and application of AL in 3D biomedical imaging. Code is at: https://github.com/MIC-DKFZ/nnActive",
            "headline_zh": "提出nnActive框架以解决3D生物医学分割中主动学习评估的缺陷",
            "intro_zh": [
                "核心问题：3D生物医学分割依赖昂贵标注，主动学习评估存在四个常见缺陷，如数据集限制和基线不当。",
                "方法要点：nnActive扩展nnU-Net，使用部分标注和3D补丁查询，改进随机采样策略和效率指标。",
                "实验或效果：在四个数据集上，主动学习优于标准随机采样，但未可靠超越改进的随机采样。"
            ],
            "tags_zh": [
                "主动学习",
                "3D生物医学分割",
                "nnU-Net扩展",
                "标注效率",
                "随机采样策略",
                "开源框架"
            ],
            "_index": 52
        },
        {
            "title": "Evaluating Deep Learning and Traditional Approaches Used in Source Camera Identification",
            "authors": [
                "Mansur Ozaman"
            ],
            "arxiv_id": "2511.19180v1",
            "summary": "One of the most important tasks in computer vision is identifying the device using which the image was taken, useful for facilitating further comprehensive analysis of the image. This paper presents comparative analysis of three techniques used in source camera identification (SCI): Photo Response Non-Uniformity (PRNU), JPEG compression artifact analysis, and convolutional neural networks (CNNs). It evaluates each method in terms of device classification accuracy. Furthermore, the research discusses the possible scientific development needed for the implementation of the methods in real-life scenarios.",
            "headline_zh": "比较PRNU、JPEG压缩和CNN在源相机识别中的设备分类准确率",
            "intro_zh": [
                "核心问题：识别图像拍摄设备以支持图像深度分析",
                "方法要点：评估PRNU、JPEG压缩伪影分析和CNN三种技术",
                "实验或效果：比较各方法的设备分类准确率，讨论实际应用需求"
            ],
            "tags_zh": [
                "源相机识别",
                "PRNU",
                "JPEG压缩伪影",
                "卷积神经网络",
                "设备分类",
                "计算机视觉"
            ],
            "_index": 53
        },
        {
            "title": "MetroGS: Efficient and Stable Reconstruction of Geometrically Accurate High-Fidelity Large-Scale Scenes",
            "authors": [
                "Kehua Chen",
                "Tianlu Mao",
                "Zhuxin Ma",
                "Hao Jiang",
                "Zehao Li",
                "Zihan Liu",
                "Shuqi Gao",
                "Honglong Zhao",
                "Feng Dai",
                "Yucheng Zhang",
                "Zhaoqi Wang"
            ],
            "arxiv_id": "2511.19172v1",
            "summary": "Recently, 3D Gaussian Splatting and its derivatives have achieved significant breakthroughs in large-scale scene reconstruction. However, how to efficiently and stably achieve high-quality geometric fidelity remains a core challenge. To address this issue, we introduce MetroGS, a novel Gaussian Splatting framework for efficient and robust reconstruction in complex urban environments. Our method is built upon a distributed 2D Gaussian Splatting representation as the core foundation, serving as a unified backbone for subsequent modules. To handle potential sparse regions in complex scenes, we propose a structured dense enhancement scheme that utilizes SfM priors and a pointmap model to achieve a denser initialization, while incorporating a sparsity compensation mechanism to improve reconstruction completeness. Furthermore, we design a progressive hybrid geometric optimization strategy that organically integrates monocular and multi-view optimization to achieve efficient and accurate geometric refinement. Finally, to address the appearance inconsistency commonly observed in large-scale scenes, we introduce a depth-guided appearance modeling approach that learns spatial features with 3D consistency, facilitating effective decoupling between geometry and appearance and further enhancing reconstruction stability. Experiments on large-scale urban datasets demonstrate that MetroGS achieves superior geometric accuracy, rendering quality, offering a unified solution for high-fidelity large-scale scene reconstruction.",
            "headline_zh": "提出MetroGS框架以高效稳定重建高保真大规模城市场景",
            "intro_zh": [
                "核心问题：大规模场景重建中几何保真度低、效率与稳定性不足",
                "方法要点：采用分布式2D高斯泼溅表示，结合密集增强与混合几何优化",
                "实验效果：在城市场景数据集上实现高几何精度与渲染质量"
            ],
            "tags_zh": [
                "3D高斯泼溅",
                "大规模场景重建",
                "几何优化",
                "外观建模",
                "分布式表示"
            ],
            "_index": 54
        },
        {
            "title": "Test-Time Preference Optimization for Image Restoration",
            "authors": [
                "Bingchen Li",
                "Xin Li",
                "Jiaqi Xu",
                "Jiaming Guo",
                "Wenbo Li",
                "Renjing Pei",
                "Zhibo Chen"
            ],
            "arxiv_id": "2511.19169v1",
            "summary": "Image restoration (IR) models are typically trained to recover high-quality images using L1 or LPIPS loss. To handle diverse unknown degradations, zero-shot IR methods have also been introduced. However, existing pre-trained and zero-shot IR approaches often fail to align with human preferences, resulting in restored images that may not be favored. This highlights the critical need to enhance restoration quality and adapt flexibly to various image restoration tasks or backbones without requiring model retraining and ideally without labor-intensive preference data collection. In this paper, we propose the first Test-Time Preference Optimization (TTPO) paradigm for image restoration, which enhances perceptual quality, generates preference data on-the-fly, and is compatible with any IR model backbone. Specifically, we design a training-free, three-stage pipeline: (i) generate candidate preference images online using diffusion inversion and denoising based on the initially restored image; (ii) select preferred and dispreferred images using automated preference-aligned metrics or human feedback; and (iii) use the selected preference images as reward signals to guide the diffusion denoising process, optimizing the restored image to better align with human preferences. Extensive experiments across various image restoration tasks and models demonstrate the effectiveness and flexibility of the proposed pipeline.",
            "headline_zh": "提出测试时偏好优化范式以提升图像恢复质量并适应多种任务",
            "intro_zh": [
                "现有图像恢复方法常与人类偏好不一致，导致恢复图像质量不佳",
                "设计无训练三阶段流程：在线生成候选图像、选择偏好图像、指导扩散去噪优化",
                "实验证明在多种图像恢复任务和模型中有效提升感知质量与灵活性"
            ],
            "tags_zh": [
                "图像恢复",
                "测试时优化",
                "偏好对齐",
                "扩散模型",
                "无训练方法",
                "感知质量"
            ],
            "_index": 55
        },
        {
            "title": "First-order Sobolev Reinforcement Learning",
            "authors": [
                "Fabian Schramm",
                "Nicolas Perrin-Gilbert",
                "Justin Carpentier"
            ],
            "arxiv_id": "2511.19165v1",
            "summary": "We propose a refinement of temporal-difference learning that enforces first-order Bellman consistency: the learned value function is trained to match not only the Bellman targets in value but also their derivatives with respect to states and actions. By differentiating the Bellman backup through differentiable dynamics, we obtain analytically consistent gradient targets. Incorporating these into the critic objective using a Sobolev-type loss encourages the critic to align with both the value and local geometry of the target function. This first-order TD matching principle can be seamlessly integrated into existing algorithms, such as Q-learning or actor-critic methods (e.g., DDPG, SAC), potentially leading to faster critic convergence and more stable policy gradients without altering their overall structure.",
            "headline_zh": "提出一阶Sobolev强化学习，通过一阶贝尔曼一致性改进TD学习，提升收敛与稳定性。",
            "intro_zh": [
                "核心问题：传统TD学习仅匹配值函数，忽略导数一致性，可能影响收敛与稳定性。",
                "方法要点：通过可微动态微分贝尔曼备份，使用Sobolev损失函数对齐值与导数。",
                "实验或效果：可集成现有算法如DDPG、SAC，潜在加速收敛并稳定策略梯度。"
            ],
            "tags_zh": [
                "强化学习",
                "时间差分学习",
                "Sobolev损失",
                "贝尔曼方程",
                "策略梯度",
                "值函数逼近"
            ],
            "_index": 56
        },
        {
            "title": "From Pixels to Posts: Retrieval-Augmented Fashion Captioning and Hashtag Generation",
            "authors": [
                "Moazzam Umer Gondal",
                "Hamad Ul Qudous",
                "Daniya Siddiqui",
                "Asma Ahmad Farhan"
            ],
            "arxiv_id": "2511.19149v1",
            "summary": "This paper introduces the retrieval-augmented framework for automatic fashion caption and hashtag generation, combining multi-garment detection, attribute reasoning, and Large Language Model (LLM) prompting. The system aims to produce visually grounded, descriptive, and stylistically interesting text for fashion imagery, overcoming the limitations of end-to-end captioners that have problems with attribute fidelity and domain generalization. The pipeline combines a YOLO-based detector for multi-garment localization, k-means clustering for dominant color extraction, and a CLIP-FAISS retrieval module for fabric and gender attribute inference based on a structured product index. These attributes, together with retrieved style examples, create a factual evidence pack that is used to guide an LLM to generate human-like captions and contextually rich hashtags. A fine-tuned BLIP model is used as a supervised baseline model for comparison. Experimental results show that the YOLO detector is able to obtain a mean Average Precision (mAP@0.5) of 0.71 for nine categories of garments. The RAG-LLM pipeline generates expressive attribute-aligned captions and achieves mean attribute coverage of 0.80 with full coverage at the 50% threshold in hashtag generation, whereas BLIP gives higher lexical overlap and lower generalization. The retrieval-augmented approach exhibits better factual grounding, less hallucination, and great potential for scalable deployment in various clothing domains. These results demonstrate the use of retrieval-augmented generation as an effective and interpretable paradigm for automated and visually grounded fashion content generation.",
            "headline_zh": "提出检索增强框架以解决时尚图像描述和标签生成中的属性保真度与泛化问题",
            "intro_zh": [
                "核心问题：端到端方法在时尚图像描述中属性保真度低、领域泛化差，易产生幻觉。",
                "方法要点：结合多服装检测、属性推理和LLM提示，构建事实证据包指导文本生成。",
                "实验或效果：RAG-LLM在属性覆盖率达0.80，优于BLIP，减少幻觉，提升可扩展性。"
            ],
            "tags_zh": [
                "检索增强生成",
                "时尚图像描述",
                "多服装检测",
                "属性推理",
                "LLM提示",
                "事实证据包"
            ],
            "_index": 57
        },
        {
            "title": "Collaborative Learning with Multiple Foundation Models for Source-Free Domain Adaptation",
            "authors": [
                "Huisoo Lee",
                "Jisu Han",
                "Hyunsouk Cho",
                "Wonjun Hwang"
            ],
            "arxiv_id": "2511.19147v1",
            "summary": "Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained source model to an unlabeled target domain without access to source data. Recent advances in Foundation Models (FMs) have introduced new opportunities for leveraging external semantic knowledge to guide SFDA. However, relying on a single FM is often insufficient, as it tends to bias adaptation toward a restricted semantic coverage, failing to capture diverse contextual cues under domain shift. To overcome this limitation, we propose a Collaborative Multi-foundation Adaptation (CoMA) framework that jointly leverages two different FMs (e.g., CLIP and BLIP) with complementary properties to capture both global semantics and local contextual cues. Specifically, we employ a bidirectional adaptation mechanism that (1) aligns different FMs with the target model for task adaptation while maintaining their semantic distinctiveness, and (2) transfers complementary knowledge from the FMs to the target model. To ensure stable adaptation under mini-batch training, we introduce Decomposed Mutual Information (DMI) that selectively enhances true dependencies while suppressing false dependencies arising from incomplete class coverage. Extensive experiments demonstrate that our method consistently outperforms existing state-of-the-art SFDA methods across four benchmarks, including Office-31, Office-Home, DomainNet-126, and VisDA, under the closed-set setting, while also achieving best results on partial-set and open-set variants.",
            "headline_zh": "提出CoMA框架，利用互补基础模型解决源自由域适应中的语义覆盖不足问题",
            "intro_zh": [
                "源自由域适应中，单一基础模型易导致语义覆盖受限，无法捕捉多样上下文线索",
                "采用双向适应机制，结合CLIP和BLIP等模型，传递互补知识并保持语义独特性",
                "在多个基准测试中，CoMA在闭集、部分集和开放集设置下均优于现有方法"
            ],
            "tags_zh": [
                "源自由域适应",
                "基础模型协作",
                "双向适应机制",
                "分解互信息",
                "语义知识传递",
                "目标域适应"
            ],
            "_index": 58
        },
        {
            "title": "ABM-LoRA: Activation Boundary Matching for Fast Convergence in Low-Rank Adaptation",
            "authors": [
                "Dongha Lee",
                "Jinhee Park",
                "Minjun Kim",
                "Junseok Kwon"
            ],
            "arxiv_id": "2511.19145v1",
            "summary": "We propose Activation Boundary Matching for Low-Rank Adaptation (ABM-LoRA), a principled initialization strategy that substantially accelerates the convergence of low-rank adapters. While LoRA offers high parameter efficiency, its random initialization restricts gradient updates to a mismatched tangent space, causing significant information loss and hindering early convergence. Our ABM-LoRA addresses this by aligning the adapter's activation boundaries with those of the pretrained model before downstream training, thereby maximizing the projection of full-parameter gradients into the adapter subspace. This alignment sharply reduces information loss at initialization, yields a lower starting loss, and accelerates convergence. We demonstrate ABM-LoRA's effectiveness across diverse architectures and tasks: language understanding (T5-Base on GLUE), dialogue generation (LLaMA2-7B on WizardLM), and vision recognition (ViT-B/16 on VTAB-1K). On VTAB-1K, it achieves the highest accuracy among all methods, with strong gains on structured reasoning tasks requiring geometric understanding.",
            "headline_zh": "提出ABM-LoRA以加速低秩适配器收敛，通过激活边界匹配优化初始化。",
            "intro_zh": [
                "LoRA随机初始化导致梯度更新不匹配，造成信息损失和收敛缓慢。",
                "ABM-LoRA在训练前对齐适配器与预训练模型的激活边界，最大化梯度投影。",
                "在语言理解、对话生成和视觉识别任务中，ABM-LoRA显著提升收敛速度和准确率。"
            ],
            "tags_zh": [
                "低秩适配",
                "初始化策略",
                "梯度投影",
                "激活边界匹配",
                "模型微调",
                "收敛加速"
            ],
            "_index": 59
        },
        {
            "title": "FilmSceneDesigner: Chaining Set Design for Procedural Film Scene Generation",
            "authors": [
                "Zhifeng Xie",
                "Keyi Zhang",
                "Yiye Yan",
                "Yuling Guo",
                "Fan Yang",
                "Jiting Zhou",
                "Mengtian Li"
            ],
            "arxiv_id": "2511.19137v1",
            "summary": "Film set design plays a pivotal role in cinematic storytelling and shaping the visual atmosphere. However, the traditional process depends on expert-driven manual modeling, which is labor-intensive and time-consuming. To address this issue, we introduce FilmSceneDesigner, an automated scene generation system that emulates professional film set design workflow. Given a natural language description, including scene type, historical period, and style, we design an agent-based chaining framework to generate structured parameters aligned with film set design workflow, guided by prompt strategies that ensure parameter accuracy and coherence. On the other hand, we propose a procedural generation pipeline which executes a series of dedicated functions with the structured parameters for floorplan and structure generation, material assignment, door and window placement, and object retrieval and layout, ultimately constructing a complete film scene from scratch. Moreover, to enhance cinematic realism and asset diversity, we construct SetDepot-Pro, a curated dataset of 6,862 film-specific 3D assets and 733 materials. Experimental results and human evaluations demonstrate that our system produces structurally sound scenes with strong cinematic fidelity, supporting downstream tasks such as virtual previs, construction drawing and mood board creation.",
            "headline_zh": "提出FilmSceneDesigner系统以自动化电影场景设计，解决传统手动建模效率低的问题。",
            "intro_zh": [
                "核心问题：传统电影场景设计依赖专家手动建模，劳动密集且耗时。",
                "方法要点：基于代理链框架生成结构化参数，并通过程序化流程构建完整场景。",
                "实验或效果：系统生成结构合理、电影保真度高的场景，支持虚拟预演等下游任务。"
            ],
            "tags_zh": [
                "电影场景生成",
                "程序化生成",
                "代理链框架",
                "3D资产数据集",
                "自然语言处理",
                "虚拟预演"
            ],
            "_index": 60
        },
        {
            "title": "Autonomous Docking of Multi-Rotor UAVs on Blimps under the Influence of Wind Gusts",
            "authors": [
                "Pascal Goldschmid",
                "Aamir Ahmad"
            ],
            "arxiv_id": "2511.19135v1",
            "summary": "Multi-rotor UAVs face limited flight time due to battery constraints. Autonomous docking on blimps with onboard battery recharging and data offloading offers a promising solution for extended UAV missions. However, the vulnerability of blimps to wind gusts causes trajectory deviations, requiring precise, obstacle-aware docking strategies. To this end, this work introduces two key novelties: (i) a temporal convolutional network that predicts blimp responses to wind gusts, enabling rapid gust detection and estimation of points where the wind gust effect has subsided; (ii) a model predictive controller (MPC) that leverages these predictions to compute collision-free trajectories for docking, enabled by a novel obstacle avoidance method for close-range manoeuvres near the blimp. Simulation results show our method outperforms a baseline constant-velocity model of the blimp significantly across different scenarios. We further validate the approach in real-world experiments, demonstrating the first autonomous multi-rotor docking control strategy on blimps shown outside simulation. Source code is available here https://github.com/robot-perception-group/multi_rotor_airship_docking.",
            "headline_zh": "提出基于TCN和MPC的自主对接方法，解决多旋翼无人机在风扰下对接飞艇问题。",
            "intro_zh": [
                "核心问题：风扰导致飞艇轨迹偏移，影响无人机自主对接的精确性和安全性。",
                "方法要点：使用TCN预测飞艇对风扰的响应，MPC结合预测计算无碰撞对接轨迹。",
                "实验或效果：仿真和真实实验验证方法优于基线，首次实现仿真外自主对接。"
            ],
            "tags_zh": [
                "自主对接",
                "多旋翼无人机",
                "飞艇",
                "风扰预测",
                "模型预测控制",
                "障碍物避免"
            ],
            "_index": 61
        },
        {
            "title": "MambaRefine-YOLO: A Dual-Modality Small Object Detector for UAV Imagery",
            "authors": [
                "Shuyu Cao",
                "Minxin Chen",
                "Yucheng Song",
                "Zhaozhong Chen",
                "Xinyou Zhang"
            ],
            "arxiv_id": "2511.19134v1",
            "summary": "Small object detection in Unmanned Aerial Vehicle (UAV) imagery is a persistent challenge, hindered by low resolution and background clutter. While fusing RGB and infrared (IR) data offers a promising solution, existing methods often struggle with the trade-off between effective cross-modal interaction and computational efficiency. In this letter, we introduce MambaRefine-YOLO. Its core contributions are a Dual-Gated Complementary Mamba fusion module (DGC-MFM) that adaptively balances RGB and IR modalities through illumination-aware and difference-aware gating mechanisms, and a Hierarchical Feature Aggregation Neck (HFAN) that uses a ``refine-then-fuse'' strategy to enhance multi-scale features. Our comprehensive experiments validate this dual-pronged approach. On the dual-modality DroneVehicle dataset, the full model achieves a state-of-the-art mAP of 83.2%, an improvement of 7.9% over the baseline. On the single-modality VisDrone dataset, a variant using only the HFAN also shows significant gains, demonstrating its general applicability. Our work presents a superior balance between accuracy and speed, making it highly suitable for real-world UAV applications.",
            "headline_zh": "提出MambaRefine-YOLO以解决无人机图像中小物体检测的挑战",
            "intro_zh": [
                "核心问题：无人机图像中小物体检测受低分辨率和背景杂波影响，现有方法难以平衡跨模态交互与计算效率。",
                "方法要点：引入DGC-MFM模块通过光照和差异感知门控机制自适应融合RGB和红外模态，HFAN采用“精炼后融合”策略增强多尺度特征。",
                "实验或效果：在DroneVehicle数据集上mAP达83.2%，提升7.9%；VisDrone数据集上仅用HFAN也显著改进，平衡精度与速度。"
            ],
            "tags_zh": [
                "小物体检测",
                "无人机图像",
                "双模态融合",
                "YOLO改进",
                "特征增强",
                "实时应用"
            ],
            "_index": 62
        },
        {
            "title": "When Semantics Regulate: Rethinking Patch Shuffle and Internal Bias for Generated Image Detection with CLIP",
            "authors": [
                "Beilin Chu",
                "Weike You",
                "Mengtao Li",
                "Tingting Zheng",
                "Kehan Zhao",
                "Xuan Xu",
                "Zhigao Lu",
                "Jia Song",
                "Moxuan Xu",
                "Linna Zhou"
            ],
            "arxiv_id": "2511.19126v1",
            "summary": "The rapid progress of GANs and Diffusion Models poses new challenges for detecting AI-generated images. Although CLIP-based detectors exhibit promising generalization, they often rely on semantic cues rather than generator artifacts, leading to brittle performance under distribution shifts. In this work, we revisit the nature of semantic bias and uncover that Patch Shuffle provides an unusually strong benefit for CLIP, that disrupts global semantic continuity while preserving local artifact cues, which reduces semantic entropy and homogenizes feature distributions between natural and synthetic images. Through a detailed layer-wise analysis, we further show that CLIP's deep semantic structure functions as a regulator that stabilizes cross-domain representations once semantic bias is suppressed. Guided by these findings, we propose SemAnti, a semantic-antagonistic fine-tuning paradigm that freezes the semantic subspace and adapts only artifact-sensitive layers under shuffled semantics. Despite its simplicity, SemAnti achieves state-of-the-art cross-domain generalization on AIGCDetectBenchmark and GenImage, demonstrating that regulating semantics is key to unlocking CLIP's full potential for robust AI-generated image detection.",
            "headline_zh": "提出SemAnti语义对抗微调范式，以提升CLIP在AI生成图像检测中的跨域鲁棒性。",
            "intro_zh": [
                "核心问题：CLIP检测器依赖语义线索，在分布偏移下性能脆弱。",
                "方法要点：通过Patch Shuffle抑制语义偏差，仅微调伪影敏感层。",
                "实验或效果：在AIGCDetectBenchmark和GenImage上实现SOTA跨域泛化。"
            ],
            "tags_zh": [
                "AI生成图像检测",
                "CLIP模型",
                "语义偏差",
                "Patch Shuffle",
                "跨域泛化",
                "微调范式"
            ],
            "_index": 63
        },
        {
            "title": "MonoSR: Open-Vocabulary Spatial Reasoning from Monocular Images",
            "authors": [
                "Qirui Wang",
                "Jingyi He",
                "Yining Pan",
                "Si Yong Yeo",
                "Xulei Yang",
                "Shijie Li"
            ],
            "arxiv_id": "2511.19119v1",
            "summary": "Spatial reasoning (SR), the ability to infer 3D spatial information from 2D inputs, is essential for real-world applications such as embodied AI and autonomous driving. However, existing research primarily focuses on indoor environments and typically relies on multi-view observations, which limits their generalizability to outdoor scenarios and constrains their applicability to monocular images, the most common real-world setting. In this work, we propose MonoSR, a large-scale monocular spatial reasoning dataset that spans diverse scenarios including indoor, outdoor, and object-centric settings, and supports multiple question types. MonoSR provides a path toward open-world monocular spatial reasoning. Beyond introducing the dataset, we evaluate advanced vision-language models to reveal their limitations on this challenging task. We further analyze whether auxiliary information is crucial for monocular spatial reasoning and offer practical guidance for designing future models. These contributions collectively establish a foundation for advancing monocular spatial reasoning in real-world, open-world environments.",
            "headline_zh": "提出MonoSR数据集以解决单目图像开放词汇空间推理问题",
            "intro_zh": [
                "核心问题：现有空间推理研究依赖多视图，难以泛化到单目图像和室外场景。",
                "方法要点：构建大规模单目空间推理数据集，涵盖室内、室外和物体中心场景。",
                "实验或效果：评估先进模型局限性，分析辅助信息重要性，提供未来模型设计指导。"
            ],
            "tags_zh": [
                "单目图像",
                "空间推理",
                "开放词汇",
                "数据集构建",
                "模型评估"
            ],
            "_index": 64
        },
        {
            "title": "3M-TI: High-Quality Mobile Thermal Imaging via Calibration-free Multi-Camera Cross-Modal Diffusion",
            "authors": [
                "Minchong Chen",
                "Xiaoyun Yuan",
                "Junzhe Wan",
                "Jianing Zhang",
                "Jun Zhang"
            ],
            "arxiv_id": "2511.19117v1",
            "summary": "The miniaturization of thermal sensors for mobile platforms inherently limits their spatial resolution and textural fidelity, leading to blurry and less informative images. Existing thermal super-resolution (SR) methods can be grouped into single-image and RGB-guided approaches: the former struggles to recover fine structures from limited information, while the latter relies on accurate and laborious cross-camera calibration, which hinders practical deployment and robustness. Here, we propose 3M-TI, a calibration-free Multi-camera cross-Modality diffusion framework for Mobile Thermal Imaging. At its core, 3M-TI integrates a cross-modal self-attention module (CSM) into the diffusion UNet, replacing the original self-attention layers to adaptively align thermal and RGB features throughout the denoising process, without requiring explicit camera calibration. This design enables the diffusion network to leverage its generative prior to enhance spatial resolution, structural fidelity, and texture detail in the super-resolved thermal images. Extensive evaluations on real-world mobile thermal cameras and public benchmarks validate our superior performance, achieving state-of-the-art results in both visual quality and quantitative metrics. More importantly, the thermal images enhanced by 3M-TI lead to substantial gains in critical downstream tasks like object detection and segmentation, underscoring its practical value for robust mobile thermal perception systems. More materials: https://github.com/work-submit/3MTI.",
            "headline_zh": "提出3M-TI框架以解决移动热成像分辨率低和纹理模糊问题",
            "intro_zh": [
                "移动热传感器小型化导致图像分辨率低和纹理模糊，现有方法依赖校准或信息不足",
                "引入跨模态自注意力模块，在扩散过程中对齐热和RGB特征，无需相机校准",
                "在真实移动设备和基准测试中实现SOTA性能，提升下游任务如检测和分割效果"
            ],
            "tags_zh": [
                "热成像超分辨率",
                "跨模态扩散",
                "移动视觉",
                "无校准对齐",
                "下游任务增强"
            ],
            "_index": 65
        },
        {
            "title": "DiffSeg30k: A Multi-Turn Diffusion Editing Benchmark for Localized AIGC Detection",
            "authors": [
                "Hai Ci",
                "Ziheng Peng",
                "Pei Yang",
                "Yingxin Xuan",
                "Mike Zheng Shou"
            ],
            "arxiv_id": "2511.19111v1",
            "summary": "Diffusion-based editing enables realistic modification of local image regions, making AI-generated content harder to detect. Existing AIGC detection benchmarks focus on classifying entire images, overlooking the localization of diffusion-based edits. We introduce DiffSeg30k, a publicly available dataset of 30k diffusion-edited images with pixel-level annotations, designed to support fine-grained detection. DiffSeg30k features: 1) In-the-wild images--we collect images or image prompts from COCO to reflect real-world content diversity; 2) Diverse diffusion models--local edits using eight SOTA diffusion models; 3) Multi-turn editing--each image undergoes up to three sequential edits to mimic real-world sequential editing; and 4) Realistic editing scenarios--a vision-language model (VLM)-based pipeline automatically identifies meaningful regions and generates context-aware prompts covering additions, removals, and attribute changes. DiffSeg30k shifts AIGC detection from binary classification to semantic segmentation, enabling simultaneous localization of edits and identification of the editing models. We benchmark three baseline segmentation approaches, revealing significant challenges in semantic segmentation tasks, particularly concerning robustness to image distortions. Experiments also reveal that segmentation models, despite being trained for pixel-level localization, emerge as highly reliable whole-image classifiers of diffusion edits, outperforming established forgery classifiers while showing great potential in cross-generator generalization. We believe DiffSeg30k will advance research in fine-grained localization of AI-generated content by demonstrating the promise and limitations of segmentation-based methods. DiffSeg30k is released at: https://huggingface.co/datasets/Chaos2629/Diffseg30k",
            "headline_zh": "提出DiffSeg30k数据集以解决扩散编辑局部化检测的基准缺失问题",
            "intro_zh": [
                "核心问题：现有AIGC检测基准忽略扩散编辑的局部化，难以应对真实编辑场景。",
                "方法要点：构建30k扩散编辑图像数据集，含像素级标注、多轮编辑和多样化模型。",
                "实验或效果：基准测试显示分割方法在局部化检测中具挑战，但整体分类性能优越。"
            ],
            "tags_zh": [
                "扩散编辑检测",
                "语义分割基准",
                "像素级标注",
                "多轮编辑",
                "AIGC检测",
                "跨生成器泛化"
            ],
            "_index": 66
        },
        {
            "title": "HABIT: Human Action Benchmark for Interactive Traffic in CARLA",
            "authors": [
                "Mohan Ramesh",
                "Mark Azer",
                "Fabian B. Flohr"
            ],
            "arxiv_id": "2511.19109v1",
            "summary": "Current autonomous driving (AD) simulations are critically limited by their inadequate representation of realistic and diverse human behavior, which is essential for ensuring safety and reliability. Existing benchmarks often simplify pedestrian interactions, failing to capture complex, dynamic intentions and varied responses critical for robust system deployment. To overcome this, we introduce HABIT (Human Action Benchmark for Interactive Traffic), a high-fidelity simulation benchmark. HABIT integrates real-world human motion, sourced from mocap and videos, into CARLA (Car Learning to Act, a full autonomous driving simulator) via a modular, extensible, and physically consistent motion retargeting pipeline. From an initial pool of approximately 30,000 retargeted motions, we curate 4,730 traffic-compatible pedestrian motions, standardized in SMPL format for physically consistent trajectories. HABIT seamlessly integrates with CARLA's Leaderboard, enabling automated scenario generation and rigorous agent evaluation. Our safety metrics, including Abbreviated Injury Scale (AIS) and False Positive Braking Rate (FPBR), reveal critical failure modes in state-of-the-art AD agents missed by prior evaluations. Evaluating three state-of-the-art autonomous driving agents, InterFuser, TransFuser, and BEVDriver, demonstrates how HABIT exposes planner weaknesses that remain hidden in scripted simulations. Despite achieving close or equal to zero collisions per kilometer on the CARLA Leaderboard, the autonomous agents perform notably worse on HABIT, with up to 7.43 collisions/km and a 12.94% AIS 3+ injury risk, and they brake unnecessarily in up to 33% of cases. All components are publicly released to support reproducible, pedestrian-aware AI research.",
            "headline_zh": "提出HABIT基准以解决自动驾驶模拟中人类行为真实性问题",
            "intro_zh": [
                "核心问题：现有自动驾驶模拟缺乏真实多样的人类行为，影响系统安全评估。",
                "方法要点：集成真实人类运动数据到CARLA，通过模块化运动重定向管道。",
                "实验或效果：评估显示先进AD代理在HABIT中碰撞率高达7.43次/公里，暴露隐藏弱点。"
            ],
            "tags_zh": [
                "自动驾驶模拟",
                "人类行为基准",
                "运动重定向",
                "安全评估",
                "CARLA集成"
            ],
            "_index": 67
        },
        {
            "title": "Graph-based 3D Human Pose Estimation using WiFi Signals",
            "authors": [
                "Jichao Chen",
                "YangYang Qu",
                "Ruibo Tang",
                "Dirk Slock"
            ],
            "arxiv_id": "2511.19105v1",
            "summary": "WiFi-based human pose estimation (HPE) has attracted increasing attention due to its resilience to occlusion and privacy-preserving compared to camera-based methods. However, existing WiFi-based HPE approaches often employ regression networks that directly map WiFi channel state information (CSI) to 3D joint coordinates, ignoring the inherent topological relationships among human joints. In this paper, we present GraphPose-Fi, a graph-based framework that explicitly models skeletal topology for WiFi-based 3D HPE. Our framework comprises a CNN encoder shared across antennas for subcarrier-time feature extraction, a lightweight attention module that adaptively reweights features over time and across antennas, and a graph-based regression head that combines GCN layers with self-attention to capture local topology and global dependencies. Our proposed method significantly outperforms existing methods on the MM-Fi dataset in various settings. The source code is available at: https://github.com/Cirrick/GraphPose-Fi.",
            "headline_zh": "提出GraphPose-Fi框架，利用WiFi信号和图结构建模骨骼拓扑以改进3D人体姿态估计",
            "intro_zh": [
                "现有WiFi方法忽略关节拓扑关系，直接回归坐标导致精度不足",
                "方法结合CNN编码器、注意力模块和图卷积网络，捕捉局部与全局依赖",
                "在MM-Fi数据集上显著优于现有方法，代码已开源"
            ],
            "tags_zh": [
                "WiFi姿态估计",
                "图卷积网络",
                "3D人体建模",
                "注意力机制",
                "信道状态信息"
            ],
            "_index": 68
        },
        {
            "title": "Analysis of Deep-Learning Methods in an ISO/TS 15066-Compliant Human-Robot Safety Framework",
            "authors": [
                "David Bricher",
                "Andreas Mueller"
            ],
            "arxiv_id": "2511.19094v1",
            "summary": "Over the last years collaborative robots have gained great success in manufacturing applications where human and robot work together in close proximity. However, current ISO/TS-15066-compliant implementations often limit the efficiency of collaborative tasks due to conservative speed restrictions. For this reason, this paper introduces a deep-learning-based human-robot-safety framework (HRSF) that aims at a dynamical adaptation of robot velocities depending on the separation distance between human and robot while respecting maximum biomechanical force and pressure limits. The applicability of the framework was investigated for four different deep learning approaches that can be used for human body extraction: human body recognition, human body segmentation, human pose estimation, and human body part segmentation. Unlike conventional industrial safety systems, the proposed HRSF differentiates individual human body parts from other objects, enabling optimized robot process execution. Experiments demonstrated a quantitative reduction in cycle time of up to 15% compared to conventional safety technology.",
            "headline_zh": "提出基于深度学习的HRSF框架，动态调整机器人速度以提升协作效率。",
            "intro_zh": [
                "核心问题：ISO/TS 15066标准导致协作机器人速度受限，降低任务效率。",
                "方法要点：使用深度学习提取人体信息，区分身体部位以优化机器人执行。",
                "实验或效果：实验显示相比传统安全技术，周期时间最多减少15%。"
            ],
            "tags_zh": [
                "人机协作安全",
                "深度学习应用",
                "人体提取方法",
                "机器人速度控制",
                "ISO/TS 15066标准"
            ],
            "_index": 69
        },
        {
            "title": "Towards Generalizable Deepfake Detection via Forgery-aware Audio-Visual Adaptation: A Variational Bayesian Approach",
            "authors": [
                "Fan Nie",
                "Jiangqun Ni",
                "Jian Zhang",
                "Bin Zhang",
                "Weizhe Zhang",
                "Bin Li"
            ],
            "arxiv_id": "2511.19080v1",
            "summary": "The widespread application of AIGC contents has brought not only unprecedented opportunities, but also potential security concerns, e.g., audio-visual deepfakes. Therefore, it is of great importance to develop an effective and generalizable method for multi-modal deepfake detection. Typically, the audio-visual correlation learning could expose subtle cross-modal inconsistencies, e.g., audio-visual misalignment, which serve as crucial clues in deepfake detection. In this paper, we reformulate the correlation learning with variational Bayesian estimation, where audio-visual correlation is approximated as a Gaussian distributed latent variable, and thus develop a novel framework for deepfake detection, i.e., Forgery-aware Audio-Visual Adaptation with Variational Bayes (FoVB). Specifically, given the prior knowledge of pre-trained backbones, we adopt two core designs to estimate audio-visual correlations effectively. First, we exploit various difference convolutions and a high-pass filter to discern local and global forgery traces from both modalities. Second, with the extracted forgery-aware features, we estimate the latent Gaussian variable of audio-visual correlation via variational Bayes. Then, we factorize the variable into modality-specific and correlation-specific ones with orthogonality constraint, allowing them to better learn intra-modal and cross-modal forgery traces with less entanglement. Extensive experiments demonstrate that our FoVB outperforms other state-of-the-art methods in various benchmarks.",
            "headline_zh": "提出FoVB框架以提升音视频深度伪造检测的泛化性",
            "intro_zh": [
                "核心问题：多模态深度伪造检测中，音视频不一致性难以泛化识别。",
                "方法要点：采用变分贝叶斯估计音视频相关性，分解模态特定与相关性变量。",
                "实验效果：在多个基准测试中优于现有方法，验证了泛化性能。"
            ],
            "tags_zh": [
                "深度伪造检测",
                "音视频相关性学习",
                "变分贝叶斯估计",
                "多模态学习",
                "泛化性提升"
            ],
            "_index": 70
        },
        {
            "title": "DEAP-3DSAM: Decoder Enhanced and Auto Prompt SAM for 3D Medical Image Segmentation",
            "authors": [
                "Fangda Chen",
                "Jintao Tang",
                "Pancheng Wang",
                "Ting Wang",
                "Shasha Li",
                "Ting Deng"
            ],
            "arxiv_id": "2511.19071v1",
            "summary": "The Segment Anything Model (SAM) has recently demonstrated significant potential in medical image segmentation. Although SAM is primarily trained on 2D images, attempts have been made to apply it to 3D medical image segmentation. However, the pseudo 3D processing used to adapt SAM results in spatial feature loss, limiting its performance. Additionally, most SAM-based methods still rely on manual prompts, which are challenging to implement in real-world scenarios and require extensive external expert knowledge. To address these limitations, we introduce the Decoder Enhanced and Auto Prompt SAM (DEAP-3DSAM) to tackle these limitations. Specifically, we propose a Feature Enhanced Decoder that fuses the original image features with rich and detailed spatial information to enhance spatial features. We also design a Dual Attention Prompter to automatically obtain prompt information through Spatial Attention and Channel Attention. We conduct comprehensive experiments on four public abdominal tumor segmentation datasets. The results indicate that our DEAP-3DSAM achieves state-of-the-art performance in 3D image segmentation, outperforming or matching existing manual prompt methods. Furthermore, both quantitative and qualitative ablation studies confirm the effectiveness of our proposed modules.",
            "headline_zh": "提出DEAP-3DSAM以解决3D医学图像分割中的空间特征损失和手动提示依赖问题",
            "intro_zh": [
                "核心问题：SAM在3D医学图像分割中因伪3D处理导致空间特征损失，且依赖手动提示",
                "方法要点：设计特征增强解码器融合图像特征，并引入双注意力提示器自动生成提示",
                "实验或效果：在四个腹部肿瘤数据集上实现SOTA性能，验证模块有效性"
            ],
            "tags_zh": [
                "3D医学图像分割",
                "特征增强",
                "自动提示生成",
                "双注意力机制",
                "腹部肿瘤分割"
            ],
            "_index": 71
        },
        {
            "title": "DynaMix: Generalizable Person Re-identification via Dynamic Relabeling and Mixed Data Sampling",
            "authors": [
                "Timur Mamedov",
                "Anton Konushin",
                "Vadim Konushin"
            ],
            "arxiv_id": "2511.19067v1",
            "summary": "Generalizable person re-identification (Re-ID) aims to recognize individuals across unseen cameras and environments. While existing methods rely heavily on limited labeled multi-camera data, we propose DynaMix, a novel method that effectively combines manually labeled multi-camera and large-scale pseudo-labeled single-camera data. Unlike prior works, DynaMix dynamically adapts to the structure and noise of the training data through three core components: (1) a Relabeling Module that refines pseudo-labels of single-camera identities on-the-fly; (2) an Efficient Centroids Module that maintains robust identity representations under a large identity space; and (3) a Data Sampling Module that carefully composes mixed data mini-batches to balance learning complexity and intra-batch diversity. All components are specifically designed to operate efficiently at scale, enabling effective training on millions of images and hundreds of thousands of identities. Extensive experiments demonstrate that DynaMix consistently outperforms state-of-the-art methods in generalizable person Re-ID.",
            "headline_zh": "提出DynaMix方法，结合多相机标注和单相机伪标注数据，提升行人重识别泛化能力",
            "intro_zh": [
                "核心问题：行人重识别在未见相机和环境下的泛化能力不足，依赖有限多相机标注数据",
                "方法要点：动态重标伪标签、高效质心模块和混合数据采样，适应数据结构和噪声",
                "实验或效果：在广泛实验中，DynaMix一致优于现有最先进方法"
            ],
            "tags_zh": [
                "行人重识别",
                "泛化学习",
                "伪标签优化",
                "数据采样",
                "身份表示学习"
            ],
            "_index": 72
        },
        {
            "title": "Understanding, Accelerating, and Improving MeanFlow Training",
            "authors": [
                "Jin-Young Kim",
                "Hyojun Go",
                "Lea Bogensperger",
                "Julius Erbach",
                "Nikolai Kalischek",
                "Federico Tombari",
                "Konrad Schindler",
                "Dominik Narnhofer"
            ],
            "arxiv_id": "2511.19065v1",
            "summary": "MeanFlow promises high-quality generative modeling in few steps, by jointly learning instantaneous and average velocity fields. Yet, the underlying training dynamics remain unclear. We analyze the interaction between the two velocities and find: (i) well-established instantaneous velocity is a prerequisite for learning average velocity; (ii) learning of instantaneous velocity benefits from average velocity when the temporal gap is small, but degrades as the gap increases; and (iii) task-affinity analysis indicates that smooth learning of large-gap average velocities, essential for one-step generation, depends on the prior formation of accurate instantaneous and small-gap average velocities. Guided by these observations, we design an effective training scheme that accelerates the formation of instantaneous velocity, then shifts emphasis from short- to long-interval average velocity. Our enhanced MeanFlow training yields faster convergence and significantly better few-step generation: With the same DiT-XL backbone, our method reaches an impressive FID of 2.87 on 1-NFE ImageNet 256x256, compared to 3.43 for the conventional MeanFlow baseline. Alternatively, our method matches the performance of the MeanFlow baseline with 2.5x shorter training time, or with a smaller DiT-L backbone.",
            "headline_zh": "提出改进MeanFlow训练方案以加速收敛并提升少步生成质量",
            "intro_zh": [
                "分析MeanFlow中瞬时与平均速度场交互，揭示学习依赖关系与退化条件",
                "设计训练策略，先加速瞬时速度形成，再转向长间隔平均速度学习",
                "实验显示FID降至2.87，训练时间缩短2.5倍，或使用更小骨干网络"
            ],
            "tags_zh": [
                "生成模型",
                "速度场学习",
                "训练加速",
                "少步生成",
                "图像合成"
            ],
            "_index": 73
        },
        {
            "title": "Granular Computing-driven SAM: From Coarse-to-Fine Guidance for Prompt-Free Segmentation",
            "authors": [
                "Qiyang Yu",
                "Yu Fang",
                "Tianrui Li",
                "Xuemei Cao",
                "Yan Chen",
                "Jianghao Li",
                "Fan Min",
                "Yi Zhang"
            ],
            "arxiv_id": "2511.19062v1",
            "summary": "Prompt-free image segmentation aims to generate accurate masks without manual guidance. Typical pre-trained models, notably Segmentation Anything Model (SAM), generate prompts directly at a single granularity level. However, this approach has two limitations: (1) Localizability, lacking mechanisms for autonomous region localization; (2) Scalability, limited fine-grained modeling at high resolution. To address these challenges, we introduce Granular Computing-driven SAM (Grc-SAM), a coarse-to-fine framework motivated by Granular Computing (GrC). First, the coarse stage adaptively extracts high-response regions from features to achieve precise foreground localization and reduce reliance on external prompts. Second, the fine stage applies finer patch partitioning with sparse local swin-style attention to enhance detail modeling and enable high-resolution segmentation. Third, refined masks are encoded as latent prompt embeddings for the SAM decoder, replacing handcrafted prompts with an automated reasoning process. By integrating multi-granularity attention, Grc-SAM bridges granular computing with vision transformers. Extensive experimental results demonstrate Grc-SAM outperforms baseline methods in both accuracy and scalability. It offers a unique granular computational perspective for prompt-free segmentation.",
            "headline_zh": "提出Grc-SAM框架以解决无提示图像分割中的定位和细节建模问题",
            "intro_zh": [
                "核心问题：SAM模型缺乏自主区域定位机制和在高分辨率下精细建模能力",
                "方法要点：采用粗到细框架，结合粒度计算和多粒度注意力实现自动分割",
                "实验或效果：实验显示Grc-SAM在准确性和可扩展性上优于基线方法"
            ],
            "tags_zh": [
                "无提示图像分割",
                "粒度计算",
                "粗到细框架",
                "多粒度注意力",
                "高分辨率分割"
            ],
            "_index": 74
        },
        {
            "title": "LAA3D: A Benchmark of Detecting and Tracking Low-Altitude Aircraft in 3D Space",
            "authors": [
                "Hai Wu",
                "Shuai Tang",
                "Jiale Wang",
                "Longkun Zou",
                "Mingyue Guo",
                "Rongqin Liang",
                "Ke Chen",
                "Yaowei Wang"
            ],
            "arxiv_id": "2511.19057v1",
            "summary": "Perception of Low-Altitude Aircraft (LAA) in 3D space enables precise 3D object localization and behavior understanding. However, datasets tailored for 3D LAA perception remain scarce. To address this gap, we present LAA3D, a large-scale dataset designed to advance 3D detection and tracking of low-altitude aerial vehicles. LAA3D contains 15,000 real images and 600,000 synthetic frames, captured across diverse scenarios, including urban and suburban environments. It covers multiple aerial object categories, including electric Vertical Take-Off and Landing (eVTOL) aircraft, Micro Aerial Vehicles (MAVs), and Helicopters. Each instance is annotated with 3D bounding box, class label, and instance identity, supporting tasks such as 3D object detection, 3D multi-object tracking (MOT), and 6-DoF pose estimation. Besides, we establish the LAA3D Benchmark, integrating multiple tasks and methods with unified evaluation protocols for comparison. Furthermore, we propose MonoLAA, a monocular 3D detection baseline, achieving robust 3D localization from zoom cameras with varying focal lengths. Models pretrained on synthetic images transfer effectively to real-world data with fine-tuning, demonstrating strong sim-to-real generalization. Our LAA3D provides a comprehensive foundation for future research in low-altitude 3D object perception.",
            "headline_zh": "提出LAA3D基准以解决低空飞行器3D感知数据集稀缺问题",
            "intro_zh": [
                "核心问题：低空飞行器3D感知数据集稀缺，阻碍精确定位与行为理解。",
                "方法要点：构建大规模数据集，含真实与合成图像，支持3D检测与跟踪任务。",
                "实验或效果：提出MonoLAA基线，合成数据预训练后微调，实现强sim-to-real泛化。"
            ],
            "tags_zh": [
                "3D目标检测",
                "低空飞行器跟踪",
                "合成数据生成",
                "sim-to-real泛化",
                "单目3D定位",
                "多任务基准"
            ],
            "_index": 75
        },
        {
            "title": "Beyond Reward Margin: Rethinking and Resolving Likelihood Displacement in Diffusion Models via Video Generation",
            "authors": [
                "Ruojun Xu",
                "Yu Kai",
                "Xuhua Ren",
                "Jiaxiang Cheng",
                "Bing Ma",
                "Tianxiang Zheng",
                "Qinhlin Lu"
            ],
            "arxiv_id": "2511.19049v1",
            "summary": "Direct Preference Optimization (DPO) has shown promising results in aligning generative outputs with human preferences by distinguishing between chosen and rejected samples. However, a critical limitation of DPO is likelihood displacement, where the probabilities of chosen samples paradoxically decrease during training, undermining the quality of generation. Although this issue has been investigated in autoregressive models, its impact within diffusion-based models remains largely unexplored. This gap leads to suboptimal performance in tasks involving video generation. To address this, we conduct a formal analysis of DPO loss through updating policy within the diffusion framework, which describes how the updating of specific training samples influences the model's predictions on other samples. Using this tool, we identify two main failure modes: (1) Optimization Conflict, which arises from small reward margins between chosen and rejected samples, and (2) Suboptimal Maximization, caused by large reward margins. Informed by these insights, we introduce a novel solution named Policy-Guided DPO (PG-DPO), combining Adaptive Rejection Scaling (ARS) and Implicit Preference Regularization (IPR) to effectively mitigate likelihood displacement. Experiments show that PG-DPO outperforms existing methods in both quantitative metrics and qualitative evaluations, offering a robust solution for improving preference alignment in video generation tasks.",
            "headline_zh": "提出PG-DPO以解决扩散模型中似然位移问题，提升视频生成偏好对齐",
            "intro_zh": [
                "核心问题：DPO在扩散模型中导致似然位移，降低生成质量",
                "方法要点：引入PG-DPO，结合ARS和IPR缓解优化冲突与次优最大化",
                "实验或效果：PG-DPO在定量和定性评估中优于现有方法"
            ],
            "tags_zh": [
                "扩散模型",
                "直接偏好优化",
                "似然位移",
                "视频生成",
                "偏好对齐",
                "优化冲突"
            ],
            "_index": 76
        },
        {
            "title": "MedSAM3: Delving into Segment Anything with Medical Concepts",
            "authors": [
                "Anglin Liu",
                "Rundong Xue",
                "Xu R. Cao",
                "Yifan Shen",
                "Yi Lu",
                "Xiang Li",
                "Qianqian Chen",
                "Jintai Chen"
            ],
            "arxiv_id": "2511.19046v1",
            "summary": "Medical image segmentation is fundamental for biomedical discovery. Existing methods lack generalizability and demand extensive, time-consuming manual annotation for new clinical application. Here, we propose MedSAM-3, a text promptable medical segmentation model for medical image and video segmentation. By fine-tuning the Segment Anything Model (SAM) 3 architecture on medical images paired with semantic conceptual labels, our MedSAM-3 enables medical Promptable Concept Segmentation (PCS), allowing precise targeting of anatomical structures via open-vocabulary text descriptions rather than solely geometric prompts. We further introduce the MedSAM-3 Agent, a framework that integrates Multimodal Large Language Models (MLLMs) to perform complex reasoning and iterative refinement in an agent-in-the-loop workflow. Comprehensive experiments across diverse medical imaging modalities, including X-ray, MRI, Ultrasound, CT, and video, demonstrate that our approach significantly outperforms existing specialist and foundation models. We will release our code and model at https://github.com/Joey-S-Liu/MedSAM3.",
            "headline_zh": "提出MedSAM-3模型，通过文本提示实现医学图像分割，解决泛化性不足问题。",
            "intro_zh": [
                "医学图像分割泛化性差，需大量手动标注。",
                "基于SAM-3微调，支持开放词汇文本提示分割。",
                "在多种模态实验中，性能优于现有模型。"
            ],
            "tags_zh": [
                "医学图像分割",
                "文本提示分割",
                "多模态大语言模型",
                "开放词汇",
                "代理框架"
            ],
            "_index": 77
        },
        {
            "title": "CSD: Change Semantic Detection with only Semantic Change Masks for Damage Assessment in Conflict Zones",
            "authors": [
                "Kai Zhenga",
                "Zhenkai Wu",
                "Fupeng Wei",
                "Miaolan Zhou",
                "Kai Lie",
                "Haitao Guo",
                "Lei Ding",
                "Wei Zhang",
                "Hang-Cheng Dong"
            ],
            "arxiv_id": "2511.19035v1",
            "summary": "Accurately and swiftly assessing damage from conflicts is crucial for humanitarian aid and regional stability. In conflict zones, damaged zones often share similar architectural styles, with damage typically covering small areas and exhibiting blurred boundaries. These characteristics lead to limited data, annotation difficulties, and significant recognition challenges, including high intra-class similarity and ambiguous semantic changes. To address these issues, we introduce a pre-trained DINOv3 model and propose a multi-scale cross-attention difference siamese network (MC-DiSNet). The powerful visual representation capability of the DINOv3 backbone enables robust and rich feature extraction from bi-temporal remote sensing images. We also release a new Gaza-change dataset containing high-resolution satellite image pairs from 2023-2024 with pixel-level semantic change annotations. It is worth emphasizing that our annotations only include semantic pixels of changed areas. Unlike conventional semantic change detection (SCD), our approach eliminates the need for large-scale semantic annotations of bi-temporal images, instead focusing directly on the changed regions. We term this new task change semantic detection (CSD). The CSD task represents a direct extension of binary change detection (BCD). Due to the limited spatial extent of semantic regions, it presents greater challenges than traditional SCD tasks. We evaluated our method under the CSD framework on both the Gaza-Change and SECOND datasets. Experimental results demonstrate that our proposed approach effectively addresses the CSD task, and its outstanding performance paves the way for practical applications in rapid damage assessment across conflict zones.",
            "headline_zh": "提出MC-DiSNet以解决冲突区域损伤评估中的语义变化检测问题",
            "intro_zh": [
                "核心问题：冲突区域损伤评估数据有限、标注困难，语义变化模糊且区域小。",
                "方法要点：使用预训练DINOv3和多尺度交叉注意力孪生网络提取特征。",
                "实验或效果：在Gaza-Change和SECOND数据集上验证，性能优异，支持实际应用。"
            ],
            "tags_zh": [
                "语义变化检测",
                "损伤评估",
                "多尺度网络",
                "DINOv3",
                "冲突区域遥感"
            ],
            "_index": 78
        },
        {
            "title": "ReEXplore: Improving MLLMs for Embodied Exploration with Contextualized Retrospective Experience Replay",
            "authors": [
                "Gengyuan Zhang",
                "Mingcong Ding",
                "Jingpei Wu",
                "Ruotong Liao",
                "Volker Tresp"
            ],
            "arxiv_id": "2511.19033v1",
            "summary": "Embodied exploration is a target-driven process that requires embodied agents to possess fine-grained perception and knowledge-enhanced decision making. While recent attempts leverage MLLMs for exploration due to their strong perceptual and reasoning abilities, we find that MLLM-based embodied agents remain suboptimal in exploring new environments: (i) they rely on profound but stale pre-trained knowledge, (ii) training-based approaches such as imitation learning or reinforcement learning are expensive for long-horizon tasks with sparse outcome rewards, and (iii) frontier-based exploration yields a large, visually nuanced action space that is difficult for MLLMs to make reliable decisions. We address these challenges with ReEXplore, a training-free framework that performs retrospective experience replay to inject distilled, abstract experience at inference time, and hierarchical frontier selection to decompose frontier ranking into coarse-to-fine decisions. Our approach enables robust, traceable, and efficient exploration. Across multiple embodied exploration benchmarks, ReEXplore yields great improvements over strong MLLM baselines, up to 3x higher performance in both success rate and in navigation efficiency under open-source backbones.",
            "headline_zh": "提出ReEXplore框架以改进MLLM在具身探索中的性能",
            "intro_zh": [
                "核心问题：MLLM在具身探索中依赖过时知识、训练成本高且难以处理复杂动作空间",
                "方法要点：采用无训练框架，结合回顾经验回放和分层边界选择",
                "实验或效果：在多个基准测试中，性能提升高达3倍，导航效率显著提高"
            ],
            "tags_zh": [
                "具身探索",
                "多模态大语言模型",
                "经验回放",
                "分层决策",
                "无训练框架"
            ],
            "_index": 79
        },
        {
            "title": "Benchmarking Corruption Robustness of LVLMs: A Discriminative Benchmark and Robustness Alignment Metric",
            "authors": [
                "Xiangjie Sui",
                "Songyang Li",
                "Hanwei Zhu",
                "Baoliang Chen",
                "Yuming Fang",
                "Xin Sun"
            ],
            "arxiv_id": "2511.19032v1",
            "summary": "Despite the remarkable reasoning abilities of large vision-language models (LVLMs), their robustness under visual corruptions remains insufficiently studied. Existing evaluation paradigms exhibit two major limitations: 1) the dominance of low-discriminative samples in current datasets masks the real robustness gap between models; and 2) conventional accuracy-based metric fail to capture the degradation of the underlying prediction structure. To bridge these gaps, we introduce Bench-C, a comprehensive benchmark emphasizing discriminative samples for assessing corruption robustness, where a selection strategy is proposed to jointly consider the prediction inconsistency under corruption and the semantic diversity. Furthermore, we propose the Robustness Alignment Score (RAS), a unified metric that measures degradation in logit-level prediction structure by considering the shifts in prediction uncertainty and calibration alignment. Comprehensive experiments and analysis reveal several interesting findings: 1) model behaviors exhibit distinguish patterns under corruptions, such as erroneous confidence and hesitation; 2) despite subtle corruption may lead to a slight accuracy gain, the overall prediction structure still degrades; 3) by decomposing corruption robustness into destructive and corrective components, the distinct failure and recovery patterns across models can be revealed.",
            "headline_zh": "提出Bench-C基准和RAS指标以评估LVLM在视觉损坏下的鲁棒性",
            "intro_zh": [
                "现有评估范式存在样本低区分度和指标不全面问题，掩盖模型鲁棒性差距",
                "引入Bench-C基准强调区分性样本，并设计RAS指标量化预测结构退化",
                "实验揭示模型在损坏下行为模式，如错误置信和预测结构整体退化"
            ],
            "tags_zh": [
                "视觉语言模型",
                "鲁棒性评估",
                "基准构建",
                "预测结构分析",
                "视觉损坏",
                "不确定性度量"
            ],
            "_index": 80
        },
        {
            "title": "Multi-Agent Monocular Dense SLAM With 3D Reconstruction Priors",
            "authors": [
                "Haihang Wu",
                "Yuchen Zhou"
            ],
            "arxiv_id": "2511.19031v1",
            "summary": "Monocular Simultaneous Localization and Mapping (SLAM) aims to estimate a robot's pose while simultaneously reconstructing an unknown 3D scene using a single camera. While existing monocular SLAM systems generate detailed 3D geometry through dense scene representations, they are computationally expensive due to the need for iterative optimization. To address this challenge, MASt3R-SLAM utilizes learned 3D reconstruction priors, enabling more efficient and accurate estimation of both 3D structures and camera poses. However, MASt3R-SLAM is limited to single-agent operation. In this paper, we extend MASt3R-SLAM to introduce the first multi-agent monocular dense SLAM system. Each agent performs local SLAM using a 3D reconstruction prior, and their individual maps are fused into a globally consistent map through a loop-closure-based map fusion mechanism. Our approach improves computational efficiency compared to state-of-the-art methods, while maintaining similar mapping accuracy when evaluated on real-world datasets.",
            "headline_zh": "提出多智能体单目稠密SLAM系统，利用3D重建先验提升计算效率与地图一致性",
            "intro_zh": [
                "核心问题：单目稠密SLAM计算成本高，且现有方法仅支持单智能体操作",
                "方法要点：每个智能体使用3D重建先验进行局部SLAM，通过闭环机制融合全局地图",
                "实验效果：在真实数据集上保持类似精度，计算效率优于现有方法"
            ],
            "tags_zh": [
                "多智能体SLAM",
                "单目稠密SLAM",
                "3D重建先验",
                "地图融合",
                "计算效率优化"
            ],
            "_index": 81
        },
        {
            "title": "Life-IQA: Boosting Blind Image Quality Assessment through GCN-enhanced Layer Interaction and MoE-based Feature Decoupling",
            "authors": [
                "Long Tang",
                "Guoquan Zhen",
                "Jie Hao",
                "Jianbo Zhang",
                "Huiyu Duan",
                "Liang Yuan",
                "Guangtao Zhai"
            ],
            "arxiv_id": "2511.19024v1",
            "summary": "Blind image quality assessment (BIQA) plays a crucial role in evaluating and optimizing visual experience. Most existing BIQA approaches fuse shallow and deep features extracted from backbone networks, while overlooking the unequal contributions to quality prediction. Moreover, while various vision encoder backbones are widely adopted in BIQA, the effective quality decoding architectures remain underexplored. To address these limitations, this paper investigates the contributions of shallow and deep features to BIQA, and proposes a effective quality feature decoding framework via GCN-enhanced \\underline{l}ayer\\underline{i}nteraction and MoE-based \\underline{f}eature d\\underline{e}coupling, termed \\textbf{(Life-IQA)}. Specifically, the GCN-enhanced layer interaction module utilizes the GCN-enhanced deepest-layer features as query and the penultimate-layer features as key, value, then performs cross-attention to achieve feature interaction. Moreover, a MoE-based feature decoupling module is proposed to decouple fused representations though different experts specialized for specific distortion types or quality dimensions. Extensive experiments demonstrate that Life-IQA shows more favorable balance between accuracy and cost than a vanilla Transformer decoder and achieves state-of-the-art performance on multiple BIQA benchmarks.The code is available at: \\href{https://github.com/TANGLONG2/Life-IQA/tree/main}{\\texttt{Life-IQA}}.",
            "headline_zh": "提出Life-IQA框架，通过GCN增强层交互和MoE特征解耦提升盲图像质量评估性能",
            "intro_zh": [
                "核心问题：现有BIQA方法忽视浅层和深层特征对质量预测的不平等贡献，且质量解码架构探索不足",
                "方法要点：使用GCN增强层交互模块进行跨注意力，MoE模块解耦特征以处理不同失真类型",
                "实验或效果：在多个BIQA基准上实现SOTA性能，平衡准确性与成本优于Transformer解码器"
            ],
            "tags_zh": [
                "盲图像质量评估",
                "图卷积网络",
                "专家混合模型",
                "特征交互",
                "特征解耦",
                "视觉编码器"
            ],
            "_index": 82
        },
        {
            "title": "Dynamic Granularity Matters: Rethinking Vision Transformers Beyond Fixed Patch Splitting",
            "authors": [
                "Qiyang Yu",
                "Yu Fang",
                "Tianrui Li",
                "Xuemei Cao",
                "Yan Chen",
                "Jianghao Li",
                "Fan Min"
            ],
            "arxiv_id": "2511.19021v1",
            "summary": "Vision Transformers (ViTs) have demonstrated strong capabilities in capturing global dependencies but often struggle to efficiently represent fine-grained local details. Existing multi-scale approaches alleviate this issue by integrating hierarchical or hybrid features; however, they rely on fixed patch sizes and introduce redundant computation. To address these limitations, we propose Granularity-driven Vision Transformer (Grc-ViT), a dynamic coarse-to-fine framework that adaptively adjusts visual granularity based on image complexity. It comprises two key stages: (1) Coarse Granularity Evaluation module, which assesses visual complexity using edge density, entropy, and frequency-domain cues to estimate suitable patch and window sizes; (2) Fine-grained Refinement module, which refines attention computation according to the selected granularity, enabling efficient and precise feature learning. Two learnable parameters, α and \\b{eta}, are optimized end-to-end to balance global reasoning and local perception. Comprehensive evaluations demonstrate that Grc-ViT enhances fine-grained discrimination while achieving a superior trade-off between accuracy and computational efficiency.",
            "headline_zh": "提出Grc-ViT以动态调整视觉粒度，提升Vision Transformers的细粒度识别与计算效率",
            "intro_zh": [
                "Vision Transformers在捕捉全局依赖时，难以高效表示细粒度局部细节，且现有多尺度方法依赖固定补丁大小并引入冗余计算",
                "Grc-ViT通过粗粒度评估模块分析图像复杂度，并利用细粒度精炼模块动态调整补丁和窗口大小，优化注意力计算",
                "实验表明，Grc-ViT在准确性和计算效率间实现优越平衡，增强细粒度判别能力"
            ],
            "tags_zh": [
                "视觉Transformer",
                "动态粒度调整",
                "细粒度识别",
                "计算效率优化",
                "注意力机制",
                "图像复杂度评估"
            ],
            "_index": 83
        },
        {
            "title": "End-to-end Autonomous Vehicle Following System using Monocular Fisheye Camera",
            "authors": [
                "Jiale Zhang",
                "Yeqiang Qian",
                "Tong Qin",
                "Mingyang Jiang",
                "Siyuan Chen",
                "Ming Yang"
            ],
            "arxiv_id": "2511.19011v1",
            "summary": "The increase in vehicle ownership has led to increased traffic congestion, more accidents, and higher carbon emissions. Vehicle platooning is a promising solution to address these issues by improving road capacity and reducing fuel consumption. However, existing platooning systems face challenges such as reliance on lane markings and expensive high-precision sensors, which limits their general applicability. To address these issues, we propose a vehicle following framework that expands its capability from restricted scenarios to general scenario applications using only a camera. This is achieved through our newly proposed end-to-end method, which improves overall driving performance. The method incorporates a semantic mask to address causal confusion in multi-frame data fusion. Additionally, we introduce a dynamic sampling mechanism to precisely track the trajectories of preceding vehicles. Extensive closed-loop validation in real-world vehicle experiments demonstrates the system's ability to follow vehicles in various scenarios, outperforming traditional multi-stage algorithms. This makes it a promising solution for cost-effective autonomous vehicle platooning. A complete real-world vehicle experiment is available at https://youtu.be/zL1bcVb9kqQ.",
            "headline_zh": "提出端到端车辆跟随框架，使用单目鱼眼相机提升通用场景性能。",
            "intro_zh": [
                "核心问题：现有车辆编队系统依赖车道线和昂贵传感器，通用性受限。",
                "方法要点：引入语义掩码解决多帧数据因果混淆，动态采样机制精确跟踪前车轨迹。",
                "实验或效果：真实世界闭环验证显示，在多种场景下优于传统多阶段算法。"
            ],
            "tags_zh": [
                "车辆编队",
                "端到端学习",
                "单目鱼眼相机",
                "语义掩码",
                "动态采样",
                "闭环验证"
            ],
            "_index": 84
        },
        {
            "title": "A Self-Conditioned Representation Guided Diffusion Model for Realistic Text-to-LiDAR Scene Generation",
            "authors": [
                "Wentao Qu",
                "Guofeng Mei",
                "Yang Wu",
                "Yongshun Gong",
                "Xiaoshui Huang",
                "Liang Xiao"
            ],
            "arxiv_id": "2511.19004v1",
            "summary": "Text-to-LiDAR generation can customize 3D data with rich structures and diverse scenes for downstream tasks. However, the scarcity of Text-LiDAR pairs often causes insufficient training priors, generating overly smooth 3D scenes. Moreover, low-quality text descriptions may degrade generation quality and controllability. In this paper, we propose a Text-to-LiDAR Diffusion Model for scene generation, named T2LDM, with a Self-Conditioned Representation Guidance (SCRG). Specifically, SCRG, by aligning to the real representations, provides the soft supervision with reconstruction details for the Denoising Network (DN) in training, while decoupled in inference. In this way, T2LDM can perceive rich geometric structures from data distribution, generating detailed objects in scenes. Meanwhile, we construct a content-composable Text-LiDAR benchmark, T2nuScenes, along with a controllability metric. Based on this, we analyze the effects of different text prompts for LiDAR generation quality and controllability, providing practical prompt paradigms and insights. Furthermore, a directional position prior is designed to mitigate street distortion, further improving scene fidelity. Additionally, by learning a conditional encoder via frozen DN, T2LDM can support multiple conditional tasks, including Sparse-to-Dense, Dense-to-Sparse, and Semantic-to-LiDAR generation. Extensive experiments in unconditional and conditional generation demonstrate that T2LDM outperforms existing methods, achieving state-of-the-art scene generation.",
            "headline_zh": "提出自条件表示引导扩散模型，用于文本到LiDAR场景生成，提升细节与可控性。",
            "intro_zh": [
                "文本-LiDAR数据稀缺导致生成场景过于平滑，低质量文本描述影响生成质量。",
                "引入自条件表示引导，在训练中对去噪网络提供软监督，推理时解耦以感知几何结构。",
                "构建T2nuScenes基准，实验显示模型在无条件与条件生成中优于现有方法。"
            ],
            "tags_zh": [
                "文本到LiDAR生成",
                "扩散模型",
                "自条件表示引导",
                "场景生成",
                "可控性评估",
                "多条件任务"
            ],
            "_index": 85
        },
        {
            "title": "AuViRe: Audio-visual Speech Representation Reconstruction for Deepfake Temporal Localization",
            "authors": [
                "Christos Koutlis",
                "Symeon Papadopoulos"
            ],
            "arxiv_id": "2511.18993v1",
            "summary": "With the rapid advancement of sophisticated synthetic audio-visual content, e.g., for subtle malicious manipulations, ensuring the integrity of digital media has become paramount. This work presents a novel approach to temporal localization of deepfakes by leveraging Audio-Visual Speech Representation Reconstruction (AuViRe). Specifically, our approach reconstructs speech representations from one modality (e.g., lip movements) based on the other (e.g., audio waveform). Cross-modal reconstruction is significantly more challenging in manipulated video segments, leading to amplified discrepancies, thereby providing robust discriminative cues for precise temporal forgery localization. AuViRe outperforms the state of the art by +8.9 AP@0.95 on LAV-DF, +9.6 AP@0.5 on AV-Deepfake1M, and +5.1 AUC on an in-the-wild experiment. Code available at https://github.com/mever-team/auvire.",
            "headline_zh": "提出AuViRe方法，通过音频-视觉语音表示重建实现深度伪造视频的时间定位",
            "intro_zh": [
                "核心问题：深度伪造视频的精确时间定位，以应对恶意音频-视觉内容操纵。",
                "方法要点：利用跨模态重建语音表示，从一模态预测另一模态，放大伪造段差异。",
                "实验或效果：在多个数据集上优于现有方法，如LAV-DF上AP@0.95提升8.9。"
            ],
            "tags_zh": [
                "音频-视觉表示重建",
                "深度伪造定位",
                "跨模态学习",
                "时间定位",
                "语音表示"
            ],
            "_index": 86
        },
        {
            "title": "View-Consistent Diffusion Representations for 3D-Consistent Video Generation",
            "authors": [
                "Duolikun Danier",
                "Ge Gao",
                "Steven McDonagh",
                "Changjian Li",
                "Hakan Bilen",
                "Oisin Mac Aodha"
            ],
            "arxiv_id": "2511.18991v1",
            "summary": "Video generation models have made significant progress in generating realistic content, enabling applications in simulation, gaming, and film making. However, current generated videos still contain visual artifacts arising from 3D inconsistencies, e.g., objects and structures deforming under changes in camera pose, which can undermine user experience and simulation fidelity. Motivated by recent findings on representation alignment for diffusion models, we hypothesize that improving the multi-view consistency of video diffusion representations will yield more 3D-consistent video generation. Through detailed analysis on multiple recent camera-controlled video diffusion models we reveal strong correlations between 3D-consistent representations and videos. We also propose ViCoDR, a new approach for improving the 3D consistency of video models by learning multi-view consistent diffusion representations. We evaluate ViCoDR on camera controlled image-to-video, text-to-video, and multi-view generation models, demonstrating significant improvements in the 3D consistency of the generated videos. Project page: https://danier97.github.io/ViCoDR.",
            "headline_zh": "提出ViCoDR方法以解决视频生成中的3D不一致性问题",
            "intro_zh": [
                "核心问题：视频生成模型存在3D不一致性，导致物体和结构在相机姿态变化时变形",
                "方法要点：通过改进视频扩散模型的多视角一致性表示，学习视图一致的扩散表示",
                "实验或效果：在相机控制图像到视频、文本到视频和多视角生成模型中，显著提升3D一致性"
            ],
            "tags_zh": [
                "视频生成",
                "3D一致性",
                "扩散模型",
                "多视角表示",
                "视图一致性"
            ],
            "_index": 87
        },
        {
            "title": "Rethinking Plant Disease Diagnosis: Bridging the Academic-Practical Gap with Vision Transformers and Zero-Shot Learning",
            "authors": [
                "Wassim Benabbas",
                "Mohammed Brahimi",
                "Samir Akhrouf",
                "Bilal Fortas"
            ],
            "arxiv_id": "2511.18989v1",
            "summary": "Recent advances in deep learning have enabled significant progress in plant disease classification using leaf images. Much of the existing research in this field has relied on the PlantVillage dataset, which consists of well-centered plant images captured against uniform, uncluttered backgrounds. Although models trained on this dataset achieve high accuracy, they often fail to generalize to real-world field images, such as those submitted by farmers to plant diagnostic systems. This has created a significant gap between published studies and practical application requirements, highlighting the necessity of investigating and addressing this issue. In this study, we investigate whether attention-based architectures and zero-shot learning approaches can bridge the gap between curated academic datasets and real-world agricultural conditions in plant disease classification. We evaluate three model categories: Convolutional Neural Networks (CNNs), Vision Transformers, and Contrastive Language-Image Pre-training (CLIP)-based zero-shot models. While CNNs exhibit limited robustness under domain shift, Vision Transformers demonstrate stronger generalization by capturing global contextual features. Most notably, CLIP models classify diseases directly from natural language descriptions without any task-specific training, offering strong adaptability and interpretability. These findings highlight the potential of zero-shot learning as a practical and scalable domain adaptation strategy for plant health diagnosis in diverse field environments.",
            "headline_zh": "探索视觉变换器与零样本学习以弥合植物病害分类的学术-实践差距",
            "intro_zh": [
                "核心问题：基于PlantVillage数据集的模型在真实农田图像上泛化能力差，存在学术-实践鸿沟。",
                "方法要点：评估CNN、视觉变换器和基于CLIP的零样本模型，后者无需任务训练即可分类。",
                "实验或效果：视觉变换器泛化更强，CLIP模型提供高适应性和可解释性，零样本学习潜力显著。"
            ],
            "tags_zh": [
                "植物病害分类",
                "视觉变换器",
                "零样本学习",
                "领域适应",
                "CLIP模型",
                "泛化能力"
            ],
            "_index": 88
        },
        {
            "title": "UMCL: Unimodal-generated Multimodal Contrastive Learning for Cross-compression-rate Deepfake Detection",
            "authors": [
                "Ching-Yi Lai",
                "Chih-Yu Jian",
                "Pei-Cheng Chuang",
                "Chia-Ming Lee",
                "Chih-Chung Hsu",
                "Chiou-Ting Hsu",
                "Chia-Wen Lin"
            ],
            "arxiv_id": "2511.18983v1",
            "summary": "In deepfake detection, the varying degrees of compression employed by social media platforms pose significant challenges for model generalization and reliability. Although existing methods have progressed from single-modal to multimodal approaches, they face critical limitations: single-modal methods struggle with feature degradation under data compression in social media streaming, while multimodal approaches require expensive data collection and labeling and suffer from inconsistent modal quality or accessibility in real-world scenarios. To address these challenges, we propose a novel Unimodal-generated Multimodal Contrastive Learning (UMCL) framework for robust cross-compression-rate (CCR) deepfake detection. In the training stage, our approach transforms a single visual modality into three complementary features: compression-robust rPPG signals, temporal landmark dynamics, and semantic embeddings from pre-trained vision-language models. These features are explicitly aligned through an affinity-driven semantic alignment (ASA) strategy, which models inter-modal relationships through affinity matrices and optimizes their consistency through contrastive learning. Subsequently, our cross-quality similarity learning (CQSL) strategy enhances feature robustness across compression rates. Extensive experiments demonstrate that our method achieves superior performance across various compression rates and manipulation types, establishing a new benchmark for robust deepfake detection. Notably, our approach maintains high detection accuracy even when individual features degrade, while providing interpretable insights into feature relationships through explicit alignment.",
            "headline_zh": "提出UMCL框架以解决社交媒体压缩下深度伪造检测的泛化问题",
            "intro_zh": [
                "社交媒体压缩导致深度伪造检测模型泛化性差，现有方法难以应对压缩变化",
                "从单模态生成多模态特征，通过对比学习对齐压缩鲁棒信号、动态和语义嵌入",
                "实验显示方法在多种压缩率和伪造类型下性能优越，提供可解释特征关系"
            ],
            "tags_zh": [
                "深度伪造检测",
                "多模态对比学习",
                "压缩鲁棒性",
                "特征对齐",
                "跨压缩率检测"
            ],
            "_index": 89
        },
        {
            "title": "Zero-shot segmentation of skin tumors in whole-slide images with vision-language foundation models",
            "authors": [
                "Santiago Moreno",
                "Pablo Meseguer",
                "Rocío del Amor",
                "Valery Naranjo"
            ],
            "arxiv_id": "2511.18978v1",
            "summary": "Accurate annotation of cutaneous neoplasm biopsies represents a major challenge due to their wide morphological variability, overlapping histological patterns, and the subtle distinctions between benign and malignant lesions. Vision-language foundation models (VLMs), pre-trained on paired image-text corpora, learn joint representations that bridge visual features and diagnostic terminology, enabling zero-shot localization and classification of tissue regions without pixel-level labels. However, most existing VLM applications in histopathology remain limited to slide-level tasks or rely on coarse interactive prompts, and they struggle to produce fine-grained segmentations across gigapixel whole-slide images (WSIs). In this work, we introduce a zero-shot visual-language segmentation pipeline for whole-slide images (ZEUS), a fully automated, zero-shot segmentation framework that leverages class-specific textual prompt ensembles and frozen VLM encoders to generate high-resolution tumor masks in WSIs. By partitioning each WSI into overlapping patches, extracting visual embeddings, and computing cosine similarities against text prompts, we generate a final segmentation mask. We demonstrate competitive performance on two in-house datasets, primary spindle cell neoplasms and cutaneous metastases, highlighting the influence of prompt design, domain shifts, and institutional variability in VLMs for histopathology. ZEUS markedly reduces annotation burden while offering scalable, explainable tumor delineation for downstream diagnostic workflows.",
            "headline_zh": "提出ZEUS零样本视觉语言分割框架，用于全切片图像中的皮肤肿瘤分割。",
            "intro_zh": [
                "皮肤肿瘤活检注释困难，因形态多变、良恶性区分细微。",
                "方法使用文本提示集成和冻结VLM编码器，生成高分辨率分割掩码。",
                "在内部数据集上表现竞争性，评估提示设计、领域偏移和机构变异性影响。"
            ],
            "tags_zh": [
                "零样本分割",
                "视觉语言模型",
                "全切片图像",
                "皮肤肿瘤",
                "文本提示集成",
                "组织病理学"
            ],
            "_index": 90
        },
        {
            "title": "Peregrine: One-Shot Fine-Tuning for FHE Inference of General Deep CNNs",
            "authors": [
                "Huaming Ling",
                "Ying Wang",
                "Si Chen",
                "Junfeng Fan"
            ],
            "arxiv_id": "2511.18976v1",
            "summary": "We address two fundamental challenges in adapting general deep CNNs for FHE-based inference: approximating non-linear activations such as ReLU with low-degree polynomials while minimizing accuracy degradation, and overcoming the ciphertext capacity barrier that constrains high-resolution image processing on FHE inference. Our contributions are twofold: (1) a single-stage fine-tuning (SFT) strategy that directly converts pre-trained CNNs into FHE-friendly forms using low-degree polynomials, achieving competitive accuracy with minimal training overhead; and (2) a generalized interleaved packing (GIP) scheme that is compatible with feature maps of virtually arbitrary spatial resolutions, accompanied by a suite of carefully designed homomorphic operators that preserve the GIP-form encryption throughout computation. These advances enable efficient, end-to-end FHE inference across diverse CNN architectures. Experiments on CIFAR-10, ImageNet, and MS COCO demonstrate that the FHE-friendly CNNs obtained via our SFT strategy achieve accuracy comparable to baselines using ReLU or SiLU activations. Moreover, this work presents the first demonstration of FHE-based inference for YOLO architectures in object detection leveraging low-degree polynomial activations.",
            "headline_zh": "提出单阶段微调和广义交错打包方案，以解决全同态加密下CNN推理的激活近似和容量限制问题。",
            "intro_zh": [
                "核心问题：全同态加密推理中，ReLU等非线性激活的近似和密文容量限制高分辨率图像处理。",
                "方法要点：使用低阶多项式近似激活，单阶段微调直接转换预训练CNN，减少训练开销。",
                "实验效果：在CIFAR-10、ImageNet和MS COCO上，FHE友好CNN达到与ReLU/SiLU基线相当的精度。"
            ],
            "tags_zh": [
                "全同态加密推理",
                "CNN微调",
                "多项式激活近似",
                "交错打包",
                "对象检测",
                "低阶多项式"
            ],
            "_index": 91
        },
        {
            "title": "CataractCompDetect: Intraoperative Complication Detection in Cataract Surgery",
            "authors": [
                "Bhuvan Sachdeva",
                "Sneha Kumari",
                "Rudransh Agarwal",
                "Shalaka Kumaraswamy",
                "Niharika Singri Prasad",
                "Simon Mueller",
                "Raphael Lechtenboehmer",
                "Maximilian W. M. Wintergerst",
                "Thomas Schultz",
                "Kaushik Murali",
                "Mohit Jain"
            ],
            "arxiv_id": "2511.18968v1",
            "summary": "Cataract surgery is one of the most commonly performed surgeries worldwide, yet intraoperative complications such as iris prolapse, posterior capsule rupture (PCR), and vitreous loss remain major causes of adverse outcomes. Automated detection of such events could enable early warning systems and objective training feedback. In this work, we propose CataractCompDetect, a complication detection framework that combines phase-aware localization, SAM 2-based tracking, complication-specific risk scoring, and vision-language reasoning for final classification. To validate CataractCompDetect, we curate CataComp, the first cataract surgery video dataset annotated for intraoperative complications, comprising 53 surgeries, including 23 with clinical complications. On CataComp, CataractCompDetect achieves an average F1 score of 70.63%, with per-complication performance of 81.8% (Iris Prolapse), 60.87% (PCR), and 69.23% (Vitreous Loss). These results highlight the value of combining structured surgical priors with vision-language reasoning for recognizing rare but high-impact intraoperative events. Our dataset and code will be publicly released upon acceptance.",
            "headline_zh": "提出CataractCompDetect框架以检测白内障手术中的术中并发症",
            "intro_zh": [
                "核心问题：白内障手术中虹膜脱垂、后囊破裂和玻璃体丢失等并发症导致不良后果",
                "方法要点：结合阶段感知定位、SAM 2跟踪、风险评分和视觉语言推理进行分类",
                "实验或效果：在CataComp数据集上平均F1得分70.63%，各并发症检测性能达60.87%-81.8%"
            ],
            "tags_zh": [
                "白内障手术",
                "并发症检测",
                "视觉语言推理",
                "手术视频分析",
                "风险评分"
            ],
            "_index": 92
        },
        {
            "title": "AVA-VLA: Improving Vision-Language-Action models with Active Visual Attention",
            "authors": [
                "Lei Xiao",
                "Jifeng Li",
                "Juntao Gao",
                "Feiyang Ye",
                "Yan Jin",
                "Jingjing Qian",
                "Jing Zhang",
                "Yong Wu",
                "Xiaoyuan Yu"
            ],
            "arxiv_id": "2511.18960v1",
            "summary": "Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in embodied AI tasks. However, existing VLA models, often built upon Vision-Language Models (VLMs), typically process dense visual inputs independently at each timestep. This approach implicitly models the task as a Markov Decision Process (MDP). However, this history-agnostic design is suboptimal for effective visual token processing in dynamic sequential decision-making, as it fails to leverage the context of history. To address this limitation, we reformulate the problem from a Partially Observable Markov Decision Process (POMDP) perspective and propose a novel framework named AVA-VLA. Inspired by the POMDP that the action generation should be conditioned on the belief state. AVA-VLA introduces Active Visual Attention (AVA) to dynamically modulate visual processing. It achieves this by leveraging the recurrent state, which is a neural approximation of the agent's belief state derived from the previous decision step. Specifically, the AVA module uses the recurrent state to compute the soft weights to actively process task-relevant visual tokens based on its historical context. Comprehensive evaluations demonstrate that AVA-VLA achieves state-of-the-art performance across popular robotic benchmarks, including LIBERO and CALVIN. Furthermore, real-world deployments on a dual-arm robot platform validate the framework's practical applicability and robust sim-to-real transferability.",
            "headline_zh": "提出AVA-VLA框架，通过主动视觉注意力改进视觉-语言-动作模型在动态决策中的性能。",
            "intro_zh": [
                "现有VLA模型在动态序列决策中忽略历史上下文，导致视觉处理效率低下。",
                "引入主动视觉注意力模块，利用循环状态动态调制视觉令牌处理。",
                "在LIBERO和CALVIN基准测试中达到最优，并在真实机器人平台验证实用性。"
            ],
            "tags_zh": [
                "视觉-语言-动作模型",
                "主动视觉注意力",
                "部分可观测马尔可夫决策过程",
                "机器人基准测试",
                "模拟到真实迁移"
            ],
            "_index": 93
        },
        {
            "title": "Eevee: Towards Close-up High-resolution Video-based Virtual Try-on",
            "authors": [
                "Jianhao Zeng",
                "Yancheng Bai",
                "Ruidong Chen",
                "Xuanpu Zhang",
                "Lei Sun",
                "Dongyang Jin",
                "Ryan Xu",
                "Nannan Zhang",
                "Dan Song",
                "Xiangxiang Chu"
            ],
            "arxiv_id": "2511.18957v1",
            "summary": "Video virtual try-on technology provides a cost-effective solution for creating marketing videos in fashion e-commerce. However, its practical adoption is hindered by two critical limitations. First, the reliance on a single garment image as input in current virtual try-on datasets limits the accurate capture of realistic texture details. Second, most existing methods focus solely on generating full-shot virtual try-on videos, neglecting the business's demand for videos that also provide detailed close-ups. To address these challenges, we introduce a high-resolution dataset for video-based virtual try-on. This dataset offers two key features. First, it provides more detailed information on the garments, which includes high-fidelity images with detailed close-ups and textual descriptions; Second, it uniquely includes full-shot and close-up try-on videos of real human models. Furthermore, accurately assessing consistency becomes significantly more critical for the close-up videos, which demand high-fidelity preservation of garment details. To facilitate such fine-grained evaluation, we propose a new garment consistency metric VGID (Video Garment Inception Distance) that quantifies the preservation of both texture and structure. Our experiments validate these contributions. We demonstrate that by utilizing the detailed images from our dataset, existing video generation models can extract and incorporate texture features, significantly enhancing the realism and detail fidelity of virtual try-on results. Furthermore, we conduct a comprehensive benchmark of recent models. The benchmark effectively identifies the texture and structural preservation problems among current methods.",
            "headline_zh": "提出高分辨率视频虚拟试穿数据集与VGID指标，以提升服装细节真实性与一致性。",
            "intro_zh": [
                "核心问题：现有虚拟试穿依赖单张服装图像，无法准确捕捉纹理细节，且缺乏近景视频。",
                "方法要点：构建包含高清图像、文本描述及全/近景视频的数据集，并设计VGID指标评估一致性。",
                "实验或效果：实验验证数据集提升模型纹理提取能力，基准测试揭示现有方法在细节保留上的不足。"
            ],
            "tags_zh": [
                "视频虚拟试穿",
                "高分辨率数据集",
                "服装一致性评估",
                "近景视频生成",
                "纹理细节保留"
            ],
            "_index": 94
        },
        {
            "title": "Compressor-VLA: Instruction-Guided Visual Token Compression for Efficient Robotic Manipulation",
            "authors": [
                "Juntao Gao",
                "Feiyang Ye",
                "Jing Zhang",
                "Wenjing Qian"
            ],
            "arxiv_id": "2511.18950v1",
            "summary": "Vision-Language-Action (VLA) models have emerged as a powerful paradigm in Embodied AI. However, the significant computational overhead of processing redundant visual tokens remains a critical bottleneck for real-time robotic deployment. While standard token pruning techniques can alleviate this, these task-agnostic methods struggle to preserve task-critical visual information. To address this challenge, simultaneously preserving both the holistic context and fine-grained details for precise action, we propose Compressor-VLA, a novel hybrid instruction-conditioned token compression framework designed for efficient, task-oriented compression of visual information in VLA models. The proposed Compressor-VLA framework consists of two token compression modules: a Semantic Task Compressor (STC) that distills holistic, task-relevant context, and a Spatial Refinement Compressor (SRC) that preserves fine-grained spatial details. This compression is dynamically modulated by the natural language instruction, allowing for the adaptive condensation of task-relevant visual information. Experimentally, extensive evaluations demonstrate that Compressor-VLA achieves a competitive success rate on the LIBERO benchmark while reducing FLOPs by 59% and the visual token count by over 3x compared to its baseline. The real-robot deployments on a dual-arm robot platform validate the model's sim-to-real transferability and practical applicability. Moreover, qualitative analyses reveal that our instruction guidance dynamically steers the model's perceptual focus toward task-relevant objects, thereby validating the effectiveness of our approach.",
            "headline_zh": "提出Compressor-VLA以解决视觉-语言-动作模型中视觉令牌冗余问题",
            "intro_zh": [
                "核心问题：视觉令牌冗余导致计算开销大，阻碍机器人实时部署",
                "方法要点：结合语义任务压缩器和空间细化压缩器，动态调制压缩",
                "实验或效果：在LIBERO基准上成功率高，FLOPs减少59%，令牌数降3倍以上"
            ],
            "tags_zh": [
                "视觉-语言-动作模型",
                "令牌压缩",
                "机器人操作",
                "指令引导",
                "计算效率",
                "sim-to-real迁移"
            ],
            "_index": 95
        },
        {
            "title": "Leveraging Adversarial Learning for Pathological Fidelity in Virtual Staining",
            "authors": [
                "José Teixeira",
                "Pascal Klöckner",
                "Diana Montezuma",
                "Melis Erdal Cesur",
                "João Fraga",
                "Hugo M. Horlings",
                "Jaime S. Cardoso",
                "Sara P. Oliveira"
            ],
            "arxiv_id": "2511.18946v1",
            "summary": "In addition to evaluating tumor morphology using H&E staining, immunohistochemistry is used to assess the presence of specific proteins within the tissue. However, this is a costly and labor-intensive technique, for which virtual staining, as an image-to-image translation task, offers a promising alternative. Although recent, this is an emerging field of research with 64% of published studies just in 2024. Most studies use publicly available datasets of H&E-IHC pairs from consecutive tissue sections. Recognizing the training challenges, many authors develop complex virtual staining models based on conditional Generative Adversarial Networks, but ignore the impact of adversarial loss on the quality of virtual staining. Furthermore, overlooking the issues of model evaluation, they claim improved performance based on metrics such as SSIM and PSNR, which are not sufficiently robust to evaluate the quality of virtually stained images. In this paper, we developed CSSP2P GAN, which we demonstrate to achieve heightened pathological fidelity through a blind pathological expert evaluation. Furthermore, while iteratively developing our model, we study the impact of the adversarial loss and demonstrate its crucial role in the quality of virtually stained images. Finally, while comparing our model with reference works in the field, we underscore the limitations of the currently used evaluation metrics and demonstrate the superior performance of CSSP2P GAN.",
            "headline_zh": "提出CSSP2P GAN以提升虚拟染色病理保真度",
            "intro_zh": [
                "虚拟染色可替代昂贵免疫组化，但现有方法忽略对抗损失影响",
                "开发CSSP2P GAN，通过对抗学习增强病理保真度",
                "盲法专家评估显示模型优于现有方法，并揭示指标局限性"
            ],
            "tags_zh": [
                "虚拟染色",
                "生成对抗网络",
                "病理保真度",
                "图像翻译",
                "对抗损失",
                "模型评估"
            ],
            "_index": 96
        },
        {
            "title": "VeCoR - Velocity Contrastive Regularization for Flow Matching",
            "authors": [
                "Zong-Wei Hong",
                "Jing-lun Li",
                "Lin-Ze Li",
                "Shen Zhang",
                "Yao Tang"
            ],
            "arxiv_id": "2511.18942v1",
            "summary": "Flow Matching (FM) has recently emerged as a principled and efficient alternative to diffusion models. Standard FM encourages the learned velocity field to follow a target direction; however, it may accumulate errors along the trajectory and drive samples off the data manifold, leading to perceptual degradation, especially in lightweight or low-step configurations.\n  To enhance stability and generalization, we extend FM into a balanced attract-repel scheme that provides explicit guidance on both \"where to go\" and \"where not to go.\" To be formal, we propose \\textbf{Velocity Contrastive Regularization (VeCoR)}, a complementary training scheme for flow-based generative modeling that augments the standard FM objective with contrastive, two-sided supervision. VeCoR not only aligns the predicted velocity with a stable reference direction (positive supervision) but also pushes it away from inconsistent, off-manifold directions (negative supervision). This contrastive formulation transforms FM from a purely attractive, one-sided objective into a two-sided training signal, regularizing trajectory evolution and improving perceptual fidelity across datasets and backbones.\n  On ImageNet-1K 256$\\times$256, VeCoR yields 22\\% and 35\\% relative FID reductions on SiT-XL/2 and REPA-SiT-XL/2 backbones, respectively, and achieves further FID gains (32\\% relative) on MS-COCO text-to-image generation, demonstrating consistent improvements in stability, convergence, and image quality, particularly in low-step and lightweight settings. Project page: https://p458732.github.io/VeCoR_Project_Page/",
            "headline_zh": "提出VeCoR以增强流匹配的稳定性和图像质量，适用于轻量或低步设置。",
            "intro_zh": [
                "标准流匹配可能累积误差并偏离数据流形，导致感知退化。",
                "VeCoR通过对比正则化，提供正负监督以平衡吸引和排斥。",
                "在ImageNet和MS-COCO上显著降低FID，提升收敛和图像质量。"
            ],
            "tags_zh": [
                "流匹配",
                "对比正则化",
                "生成模型",
                "图像生成",
                "稳定训练"
            ],
            "_index": 97
        },
        {
            "title": "Human-Centric Open-Future Task Discovery: Formulation, Benchmark, and Scalable Tree-Based Search",
            "authors": [
                "Zijian Song",
                "Xiaoxin Lin",
                "Tao Pu",
                "Zhenlong Yuan",
                "Guangrun Wang",
                "Liang Lin"
            ],
            "arxiv_id": "2511.18929v1",
            "summary": "Recent progress in robotics and embodied AI is largely driven by Large Multimodal Models (LMMs). However, a key challenge remains underexplored: how can we advance LMMs to discover tasks that directly assist humans in open-future scenarios, where human intentions are highly concurrent and dynamic. In this work, we formalize the problem of Human-centric Open-future Task Discovery (HOTD), focusing particularly on identifying tasks that reduce human effort across multiple plausible futures. To facilitate this study, we propose an HOTD-Bench, which features over 2K real-world videos, a semi-automated annotation pipeline, and a simulation-based protocol tailored for open-set future evaluation. Additionally, we propose the Collaborative Multi-Agent Search Tree (CMAST) framework, which decomposes the complex reasoning through a multi-agent system and structures the reasoning process through a scalable search tree module. In our experiments, CMAST achieves the best performance on the HOTD-Bench, significantly surpassing existing LMMs. It also integrates well with existing LMMs, consistently improving performance.",
            "headline_zh": "提出CMAST框架以解决开放未来场景中人类中心任务发现问题",
            "intro_zh": [
                "核心问题：开放未来场景中人类意图并发动态，如何发现减少人类努力的任务。",
                "方法要点：使用多代理系统和可扩展搜索树分解复杂推理过程。",
                "实验或效果：在HOTD-Bench上性能最佳，显著超越现有LMMs。"
            ],
            "tags_zh": [
                "人类中心任务发现",
                "开放未来场景",
                "多代理系统",
                "搜索树模块",
                "LMM增强"
            ],
            "_index": 98
        },
        {
            "title": "FineXtrol: Controllable Motion Generation via Fine-Grained Text",
            "authors": [
                "Keming Shen",
                "Bizhu Wu",
                "Junliang Chen",
                "Xiaoqin Wang",
                "Linlin Shen"
            ],
            "arxiv_id": "2511.18927v1",
            "summary": "Recent works have sought to enhance the controllability and precision of text-driven motion generation. Some approaches leverage large language models (LLMs) to produce more detailed texts, while others incorporate global 3D coordinate sequences as additional control signals. However, the former often introduces misaligned details and lacks explicit temporal cues, and the latter incurs significant computational cost when converting coordinates to standard motion representations. To address these issues, we propose FineXtrol, a novel control framework for efficient motion generation guided by temporally-aware, precise, user-friendly, and fine-grained textual control signals that describe specific body part movements over time. In support of this framework, we design a hierarchical contrastive learning module that encourages the text encoder to produce more discriminative embeddings for our novel control signals, thereby improving motion controllability. Quantitative results show that FineXtrol achieves strong performance in controllable motion generation, while qualitative analysis demonstrates its flexibility in directing specific body part movements.",
            "headline_zh": "提出FineXtrol框架，通过细粒度文本控制解决运动生成中的精度和效率问题",
            "intro_zh": [
                "核心问题：现有方法细节错位、缺乏时间线索或计算成本高",
                "方法要点：使用时间感知细粒度文本信号，结合分层对比学习提升嵌入判别性",
                "实验或效果：定量结果强，定性分析显示灵活控制身体部位运动"
            ],
            "tags_zh": [
                "运动生成",
                "细粒度文本控制",
                "分层对比学习",
                "时间感知信号",
                "计算效率"
            ],
            "_index": 99
        },
        {
            "title": "AttenDence: Maximizing Attention Confidence for Test Time Adaptation",
            "authors": [
                "Yash Mali"
            ],
            "arxiv_id": "2511.18925v1",
            "summary": "Test-time adaptation (TTA) enables models to adapt to distribution shifts at inference time. While entropy minimization over the output distribution has proven effective for TTA, transformers offer an additional unsupervised learning signal through their attention mechanisms. We propose minimizing the entropy of attention distributions from the CLS token to image patches as a novel TTA objective.This approach encourages the model to attend more confidently to relevant image regions under distribution shift and is effective even when only a single test image is available. We demonstrate that attention entropy minimization improves robustness across diverse corruption types while not hurting performance on clean data on a single sample stream of images at test time.",
            "headline_zh": "提出最小化注意力分布熵以增强测试时适应中模型对相关图像区域的关注置信度",
            "intro_zh": [
                "核心问题：测试时分布偏移下模型适应能力不足，传统方法依赖输出分布熵最小化",
                "方法要点：利用Transformer注意力机制，最小化CLS令牌到图像补丁的注意力分布熵",
                "实验或效果：在多种损坏类型下提升鲁棒性，单测试样本有效，不影响干净数据性能"
            ],
            "tags_zh": [
                "测试时适应",
                "注意力机制",
                "熵最小化",
                "分布偏移",
                "Transformer模型"
            ],
            "_index": 100
        },
        {
            "title": "One4D: Unified 4D Generation and Reconstruction via Decoupled LoRA Control",
            "authors": [
                "Zhenxing Mi",
                "Yuxin Wang",
                "Dan Xu"
            ],
            "arxiv_id": "2511.18922v1",
            "summary": "We present One4D, a unified framework for 4D generation and reconstruction that produces dynamic 4D content as synchronized RGB frames and pointmaps. By consistently handling varying sparsities of conditioning frames through a Unified Masked Conditioning (UMC) mechanism, One4D can seamlessly transition between 4D generation from a single image, 4D reconstruction from a full video, and mixed generation and reconstruction from sparse frames. Our framework adapts a powerful video generation model for joint RGB and pointmap generation, with carefully designed network architectures. The commonly used diffusion finetuning strategies for depthmap or pointmap reconstruction often fail on joint RGB and pointmap generation, quickly degrading the base video model. To address this challenge, we introduce Decoupled LoRA Control (DLC), which employs two modality-specific LoRA adapters to form decoupled computation branches for RGB frames and pointmaps, connected by lightweight, zero-initialized control links that gradually learn mutual pixel-level consistency. Trained on a mixture of synthetic and real 4D datasets under modest computational budgets, One4D produces high-quality RGB frames and accurate pointmaps across both generation and reconstruction tasks. This work represents a step toward general, high-quality geometry-based 4D world modeling using video diffusion models. Project page: https://mizhenxing.github.io/One4D",
            "headline_zh": "提出One4D框架，通过解耦LoRA控制统一4D生成与重建",
            "intro_zh": [
                "核心问题：传统扩散微调在联合RGB和点云生成时易导致模型退化",
                "方法要点：使用模态特定LoRA适配器和零初始化控制链接实现解耦计算",
                "实验效果：在合成与真实数据集上生成高质量RGB帧和精确点云"
            ],
            "tags_zh": [
                "4D生成",
                "点云重建",
                "LoRA适配器",
                "视频扩散模型",
                "统一框架"
            ],
            "_index": 101
        },
        {
            "title": "BackdoorVLM: A Benchmark for Backdoor Attacks on Vision-Language Models",
            "authors": [
                "Juncheng Li",
                "Yige Li",
                "Hanxun Huang",
                "Yunhao Chen",
                "Xin Wang",
                "Yixu Wang",
                "Xingjun Ma",
                "Yu-Gang Jiang"
            ],
            "arxiv_id": "2511.18921v1",
            "summary": "Backdoor attacks undermine the reliability and trustworthiness of machine learning systems by injecting hidden behaviors that can be maliciously activated at inference time. While such threats have been extensively studied in unimodal settings, their impact on multimodal foundation models, particularly vision-language models (VLMs), remains largely underexplored. In this work, we introduce \\textbf{BackdoorVLM}, the first comprehensive benchmark for systematically evaluating backdoor attacks on VLMs across a broad range of settings. It adopts a unified perspective that injects and analyzes backdoors across core vision-language tasks, including image captioning and visual question answering. BackdoorVLM organizes multimodal backdoor threats into 5 representative categories: targeted refusal, malicious injection, jailbreak, concept substitution, and perceptual hijack. Each category captures a distinct pathway through which an adversary can manipulate a model's behavior. We evaluate these threats using 12 representative attack methods spanning text, image, and bimodal triggers, tested on 2 open-source VLMs and 3 multimodal datasets. Our analysis reveals that VLMs exhibit strong sensitivity to textual instructions, and in bimodal backdoors the text trigger typically overwhelms the image trigger when forming the backdoor mapping. Notably, backdoors involving the textual modality remain highly potent, with poisoning rates as low as 1\\% yielding over 90\\% success across most tasks. These findings highlight significant, previously underexplored vulnerabilities in current VLMs. We hope that BackdoorVLM can serve as a useful benchmark for analyzing and mitigating multimodal backdoor threats. Code is available at: https://github.com/bin015/BackdoorVLM .",
            "headline_zh": "提出BackdoorVLM基准以评估视觉语言模型中的后门攻击",
            "intro_zh": [
                "核心问题：多模态基础模型中的后门攻击威胁尚未充分探索，影响模型可靠性。",
                "方法要点：构建统一基准，涵盖5类后门威胁，包括目标拒绝和恶意注入等。",
                "实验或效果：在12种攻击方法上测试，文本触发主导，1%投毒率可达90%成功率。"
            ],
            "tags_zh": [
                "后门攻击",
                "视觉语言模型",
                "多模态基准",
                "文本触发",
                "模型安全"
            ],
            "_index": 102
        },
        {
            "title": "EventSTU: Event-Guided Efficient Spatio-Temporal Understanding for Video Large Language Models",
            "authors": [
                "Wenhao Xu",
                "Xin Dong",
                "Yue Li",
                "Haoyuan Shi",
                "Zhiwei Xiong"
            ],
            "arxiv_id": "2511.18920v1",
            "summary": "Video large language models have demonstrated strong video understanding capabilities but suffer from high inference costs due to the massive number of tokens in long videos. Inspired by event-based vision, we propose an event-guided, training-free framework for efficient spatio-temporal understanding, named EventSTU. In the temporal domain, we design a coarse-to-fine keyframe sampling algorithm that exploits the change-triggered property of event cameras to eliminate redundant frames. In the spatial domain, we design an adaptive token pruning algorithm that leverages the visual saliency of events as a zero-cost prior to guide spatial reduction. From a holistic spatio-temporal perspective, we further integrate question relevance from keyframe sampling to adaptively allocate token pruning budgets. To facilitate evaluation, we construct EventBench, the first event-inclusive, human-annotated multimodal benchmark that covers diverse real-world scenarios. Beyond physical event cameras, EventSTU also supports general video understanding using simulated events. Comprehensive experiments show that EventSTU achieves 3.01x FLOPs reduction and 3.10x prefilling speedup over the strongest baseline while still improving performance.",
            "headline_zh": "提出EventSTU框架以解决视频大语言模型推理成本高的问题",
            "intro_zh": [
                "核心问题：视频大语言模型在长视频中因token数量多导致推理成本高。",
                "方法要点：利用事件相机特性进行训练无关的粗到细关键帧采样和自适应token剪枝。",
                "实验或效果：实现3.01倍FLOPs减少和3.10倍预填充加速，同时提升性能。"
            ],
            "tags_zh": [
                "事件引导视频理解",
                "高效时空理解",
                "关键帧采样",
                "token剪枝",
                "事件基准",
                "视频大语言模型"
            ],
            "_index": 103
        },
        {
            "title": "Learning What to Trust: Bayesian Prior-Guided Optimization for Visual Generation",
            "authors": [
                "Ruiying Liu",
                "Yuanzhi Liang",
                "Haibin Huang",
                "Tianshu Yu",
                "Chi Zhang"
            ],
            "arxiv_id": "2511.18919v1",
            "summary": "Group Relative Policy Optimization (GRPO) has emerged as an effective and lightweight framework for post-training visual generative models. However, its performance is fundamentally limited by the ambiguity of textual visual correspondence: a single prompt may validly describe diverse visual outputs, and a single image or video may support multiple equally correct interpretations. This many to many relationship leads reward models to generate uncertain and weakly discriminative signals, causing GRPO to underutilize reliable feedback and overfit noisy ones. We introduce Bayesian Prior-Guided Optimization (BPGO), a novel extension of GRPO that explicitly models reward uncertainty through a semantic prior anchor. BPGO adaptively modulates optimization trust at two levels: inter-group Bayesian trust allocation emphasizes updates from groups consistent with the prior while down-weighting ambiguous ones, and intra-group prior-anchored renormalization sharpens sample distinctions by expanding confident deviations and compressing uncertain scores. Across both image and video generation tasks, BPGO delivers consistently stronger semantic alignment, enhanced perceptual fidelity, and faster convergence than standard GRPO and recent variants.",
            "headline_zh": "提出贝叶斯先验引导优化以解决视觉生成中奖励不确定性问题",
            "intro_zh": [
                "核心问题：文本与视觉对应关系模糊，导致奖励信号不确定和弱区分性",
                "方法要点：通过语义先验锚建模奖励不确定性，自适应调节组间和组内信任",
                "实验或效果：在图像和视频生成中，语义对齐、感知保真度和收敛速度均提升"
            ],
            "tags_zh": [
                "视觉生成",
                "贝叶斯优化",
                "奖励模型",
                "语义对齐",
                "后训练优化"
            ],
            "_index": 104
        },
        {
            "title": "An Efficient Closed-Form Solution to Full Visual-Inertial State Initialization",
            "authors": [
                "Samuel Cerezo",
                "Seong Hun Lee",
                "Javier Civera"
            ],
            "arxiv_id": "2511.18910v1",
            "summary": "In this letter, we present a closed-form initialization method that recovers the full visual-inertial state without nonlinear optimization. Unlike previous approaches that rely on iterative solvers, our formulation yields analytical, easy-to-implement, and numerically stable solutions for reliable start-up. Our method builds on small-rotation and constant-velocity approximations, which keep the formulation compact while preserving the essential coupling between motion and inertial measurements. We further propose an observability-driven, two-stage initialization scheme that balances accuracy with initialization latency. Extensive experiments on the EuRoC dataset validate our assumptions: our method achieves 10-20% lower initialization error than optimization-based approaches, while using 4x shorter initialization windows and reducing computational cost by 5x.",
            "headline_zh": "提出闭式视觉-惯性状态初始化方法，避免非线性优化，提高效率与稳定性。",
            "intro_zh": [
                "核心问题：视觉-惯性状态初始化依赖迭代求解器，计算成本高且不稳定。",
                "方法要点：基于小旋转和恒定速度近似，实现解析解，耦合运动与惯性测量。",
                "实验效果：在EuRoC数据集上，初始化误差降低10-20%，计算成本减少5倍。"
            ],
            "tags_zh": [
                "视觉-惯性状态初始化",
                "闭式解",
                "小旋转近似",
                "恒定速度假设",
                "EuRoC数据集",
                "计算效率"
            ],
            "_index": 105
        },
        {
            "title": "MatMart: Material Reconstruction of 3D Objects via Diffusion",
            "authors": [
                "Xiuchao Wu",
                "Pengfei Zhu",
                "Jiangjing Lyu",
                "Xinguo Liu",
                "Jie Guo",
                "Yanwen Guo",
                "Weiwei Xu",
                "Chengfei Lyu"
            ],
            "arxiv_id": "2511.18900v1",
            "summary": "Applying diffusion models to physically-based material estimation and generation has recently gained prominence. In this paper, we propose \\ttt, a novel material reconstruction framework for 3D objects, offering the following advantages. First, \\ttt\\ adopts a two-stage reconstruction, starting with accurate material prediction from inputs and followed by prior-guided material generation for unobserved views, yielding high-fidelity results. Second, by utilizing progressive inference alongside the proposed view-material cross-attention (VMCA), \\ttt\\ enables reconstruction from an arbitrary number of input images, demonstrating strong scalability and flexibility. Finally, \\ttt\\ achieves both material prediction and generation capabilities through end-to-end optimization of a single diffusion model, without relying on additional pre-trained models, thereby exhibiting enhanced stability across various types of objects. Extensive experiments demonstrate that \\ttt\\ achieves superior performance in material reconstruction compared to existing methods.",
            "headline_zh": "提出MatMart框架，通过扩散模型实现3D物体的材料重建。",
            "intro_zh": [
                "核心问题：从输入图像中估计和生成3D物体的物理材料。",
                "方法要点：采用两阶段重建，结合视图-材料交叉注意力和渐进推理。",
                "实验或效果：在材料重建中优于现有方法，具有高保真度和灵活性。"
            ],
            "tags_zh": [
                "材料重建",
                "扩散模型",
                "3D物体",
                "视图-材料交叉注意力",
                "渐进推理"
            ],
            "_index": 106
        },
        {
            "title": "MetaDCSeg: Robust Medical Image Segmentation via Meta Dynamic Center Weighting",
            "authors": [
                "Chenyu Mu",
                "Guihai Chen",
                "Xun Yang",
                "Erkun Yang",
                "Cheng Deng"
            ],
            "arxiv_id": "2511.18894v1",
            "summary": "Medical image segmentation is crucial for clinical applications, but it is frequently disrupted by noisy annotations and ambiguous anatomical boundaries, which lead to instability in model training. Existing methods typically rely on global noise assumptions or confidence-based sample selection, which inadequately mitigate the performance degradation caused by annotation noise, especially in challenging boundary regions. To address this issue, we propose MetaDCSeg, a robust framework that dynamically learns optimal pixel-wise weights to suppress the influence of noisy ground-truth labels while preserving reliable annotations. By explicitly modeling boundary uncertainty through a Dynamic Center Distance (DCD) mechanism, our approach utilizes weighted feature distances for foreground, background, and boundary centers, directing the model's attention toward hard-to-segment pixels near ambiguous boundaries. This strategy enables more precise handling of structural boundaries, which are often overlooked by existing methods, and significantly enhances segmentation performance. Extensive experiments across four benchmark datasets with varying noise levels demonstrate that MetaDCSeg consistently outperforms existing state-of-the-art methods.",
            "headline_zh": "提出MetaDCSeg以解决医学图像分割中噪声标注和模糊边界问题",
            "intro_zh": [
                "核心问题：医学图像分割易受噪声标注和模糊边界干扰，导致模型训练不稳定",
                "方法要点：通过动态中心距离机制学习像素级权重，抑制噪声并聚焦边界区域",
                "实验或效果：在多个基准数据集上优于现有方法，提升分割性能"
            ],
            "tags_zh": [
                "医学图像分割",
                "噪声标注处理",
                "边界不确定性建模",
                "动态中心加权",
                "元学习框架"
            ],
            "_index": 107
        },
        {
            "title": "MFmamba: A Multi-function Network for Panchromatic Image Resolution Restoration Based on State-Space Model",
            "authors": [
                "Qian Jiang",
                "Qianqian Wang",
                "Xin Jin",
                "Michal Wozniak",
                "Shaowen Yao",
                "Wei Zhou"
            ],
            "arxiv_id": "2511.18888v1",
            "summary": "Remote sensing images are becoming increasingly widespread in military, earth resource exploration. Because of the limitation of a single sensor, we can obtain high spatial resolution grayscale panchromatic (PAN) images and low spatial resolution color multispectral (MS) images. Therefore, an important issue is to obtain a color image with high spatial resolution when there is only a PAN image at the input. The existing methods improve spatial resolution using super-resolution (SR) technology and spectral recovery using colorization technology. However, the SR technique cannot improve the spectral resolution, and the colorization technique cannot improve the spatial resolution. Moreover, the pansharpening method needs two registered inputs and can not achieve SR. As a result, an integrated approach is expected. To solve the above problems, we designed a novel multi-function model (MFmamba) to realize the tasks of SR, spectral recovery, joint SR and spectral recovery through three different inputs. Firstly, MFmamba utilizes UNet++ as the backbone, and a Mamba Upsample Block (MUB) is combined with UNet++. Secondly, a Dual Pool Attention (DPA) is designed to replace the skip connection in UNet++. Finally, a Multi-scale Hybrid Cross Block (MHCB) is proposed for initial feature extraction. Many experiments show that MFmamba is competitive in evaluation metrics and visual results and performs well in the three tasks when only the input PAN image is used.",
            "headline_zh": "提出MFmamba网络，基于状态空间模型实现全色图像分辨率恢复与光谱恢复",
            "intro_zh": [
                "核心问题：仅输入高空间分辨率灰度全色图像时，如何同时提升空间与光谱分辨率",
                "方法要点：结合UNet++与Mamba上采样块，设计双池注意力和多尺度混合交叉块",
                "实验或效果：在多个任务中评估指标和视觉结果具有竞争力，仅需单输入"
            ],
            "tags_zh": [
                "遥感图像处理",
                "超分辨率",
                "光谱恢复",
                "状态空间模型",
                "多任务学习"
            ],
            "_index": 108
        },
        {
            "title": "MagicWorld: Interactive Geometry-driven Video World Exploration",
            "authors": [
                "Guangyuan Li",
                "Siming Zheng",
                "Shuolin Xu",
                "Jinwei Chen",
                "Bo Li",
                "Xiaobin Hu",
                "Lei Zhao",
                "Peng-Tao Jiang"
            ],
            "arxiv_id": "2511.18886v1",
            "summary": "Recent interactive video world model methods generate scene evolution conditioned on user instructions. Although they achieve impressive results, two key limitations remain. First, they fail to fully exploit the correspondence between instruction-driven scene motion and the underlying 3D geometry, which results in structural instability under viewpoint changes. Second, they easily forget historical information during multi-step interaction, resulting in error accumulation and progressive drift in scene semantics and structure. To address these issues, we propose MagicWorld, an interactive video world model that integrates 3D geometric priors and historical retrieval. MagicWorld starts from a single scene image, employs user actions to drive dynamic scene evolution, and autoregressively synthesizes continuous scenes. We introduce the Action-Guided 3D Geometry Module (AG3D), which constructs a point cloud from the first frame of each interaction and the corresponding action, providing explicit geometric constraints for viewpoint transitions and thereby improving structural consistency. We further propose History Cache Retrieval (HCR) mechanism, which retrieves relevant historical frames during generation and injects them as conditioning signals, helping the model utilize past scene information and mitigate error accumulation. Experimental results demonstrate that MagicWorld achieves notable improvements in scene stability and continuity across interaction iterations.",
            "headline_zh": "提出MagicWorld集成3D几何先验与历史检索以提升交互视频世界模型的稳定性与连续性",
            "intro_zh": [
                "核心问题：现有方法忽视指令驱动运动与3D几何对应，导致视角变化时结构不稳定，且多步交互中易遗忘历史信息",
                "方法要点：引入AG3D模块构建点云提供几何约束，并采用HCR机制检索历史帧注入条件信号",
                "实验或效果：实验显示MagicWorld在交互迭代中显著改善场景稳定性和连续性"
            ],
            "tags_zh": [
                "交互视频世界模型",
                "3D几何先验",
                "历史检索机制",
                "场景稳定性",
                "多步交互",
                "点云构建"
            ],
            "_index": 109
        },
        {
            "title": "Facade Segmentation for Solar Photovoltaic Suitability",
            "authors": [
                "Ayca Duran",
                "Christoph Waibel",
                "Bernd Bickel",
                "Iro Armeni",
                "Arno Schlueter"
            ],
            "arxiv_id": "2511.18882v1",
            "summary": "Building integrated photovoltaic (BIPV) facades represent a promising pathway towards urban decarbonization, especially where roof areas are insufficient and ground-mounted arrays are infeasible. Although machine learning-based approaches to support photovoltaic (PV) planning on rooftops are well researched, automated approaches for facades still remain scarce and oversimplified. This paper therefore presents a pipeline that integrates detailed information on the architectural composition of the facade to automatically identify suitable surfaces for PV application and estimate the solar energy potential. The pipeline fine-tunes SegFormer-B5 on the CMP Facades dataset and converts semantic predictions into facade-level PV suitability masks and PV panel layouts considering module sizes and clearances. Applied to a dataset of 373 facades with known dimensions from ten cities, the results show that installable BIPV potential is significantly lower than theoretical potential, thus providing valuable insights for reliable urban energy planning. With the growing availability of facade imagery, the proposed pipeline can be scaled to support BIPV planning in cities worldwide.",
            "headline_zh": "提出建筑立面光伏适用性分割管道，以支持城市能源规划。",
            "intro_zh": [
                "核心问题：建筑立面光伏潜力评估方法稀缺且过于简化，阻碍城市脱碳。",
                "方法要点：基于SegFormer-B5微调，结合立面语义分割生成光伏布局。",
                "实验或效果：在373个立面数据集上验证，显示可安装潜力远低于理论值。"
            ],
            "tags_zh": [
                "建筑立面分割",
                "光伏适用性评估",
                "语义分割",
                "城市能源规划",
                "SegFormer微调"
            ],
            "_index": 110
        },
        {
            "title": "Accelerating Reinforcement Learning via Error-Related Human Brain Signals",
            "authors": [
                "Suzie Kim",
                "Hye-Bin Shin",
                "Hyo-Jeong Jang"
            ],
            "arxiv_id": "2511.18878v1",
            "summary": "In this work, we investigate how implicit neural feed back can accelerate reinforcement learning in complex robotic manipulation settings. While prior electroencephalogram (EEG) guided reinforcement learning studies have primarily focused on navigation or low-dimensional locomotion tasks, we aim to understand whether such neural evaluative signals can improve policy learning in high-dimensional manipulation tasks involving obstacles and precise end-effector control. We integrate error related potentials decoded from offline-trained EEG classifiers into reward shaping and systematically evaluate the impact of human-feedback weighting. Experiments on a 7-DoF manipulator in an obstacle-rich reaching environment show that neural feedback accelerates reinforcement learning and, depending on the human-feedback weighting, can yield task success rates that at times exceed those of sparse-reward baselines. Moreover, when applying the best-performing feedback weighting across all sub jects, we observe consistent acceleration of reinforcement learning relative to the sparse-reward setting. Furthermore, leave-one subject-out evaluations confirm that the proposed framework remains robust despite the intrinsic inter-individual variability in EEG decodability. Our findings demonstrate that EEG-based reinforcement learning can scale beyond locomotion tasks and provide a viable pathway for human-aligned manipulation skill acquisition.",
            "headline_zh": "提出基于脑电误差信号的强化学习框架，加速高维机器人操作任务学习",
            "intro_zh": [
                "核心问题：高维机器人操作任务中强化学习效率低，脑电反馈应用局限于导航任务",
                "方法要点：解码脑电误差电位，集成到奖励塑形中，并系统评估反馈权重",
                "实验或效果：在7自由度机械臂环境中，神经反馈加速学习，成功率有时超过稀疏奖励基线"
            ],
            "tags_zh": [
                "强化学习",
                "脑电信号",
                "机器人操作",
                "奖励塑形",
                "误差相关电位",
                "人机交互"
            ],
            "_index": 111
        },
        {
            "title": "Parallel Vision Token Scheduling for Fast and Accurate Multimodal LMMs Inference",
            "authors": [
                "Wengyi Zhan",
                "Mingbao Lin",
                "Zhihang Lin",
                "Rongrong Ji"
            ],
            "arxiv_id": "2511.18875v1",
            "summary": "Multimodal large language models (MLLMs) deliver impressive vision-language reasoning but suffer steep inference latency because self-attention scales quadratically with sequence length and thousands of visual tokens contributed by high-resolution images. Naively pruning less-informative visual tokens reduces this burden, yet indiscriminate removal can strip away contextual cues essential for background or fine-grained questions, undermining accuracy. In this paper, we present ParVTS (Parallel Vision Token Scheduling), a training-free scheduling framework that partitions visual tokens into subject and non-subject groups, processes them in parallel to transfer their semantics into question tokens, and discards the non-subject path mid-inference to reduce computation. This scheduling reduces computational complexity, requires no heuristics or additional modules, and is compatible with diverse existing MLLM architectures. Experiments across multiple MLLM backbones show that ParVTS prunes up to 88.9% of visual tokens with minimal performance drop, achieving 1.77x speedup and 70% FLOPs reduction.",
            "headline_zh": "提出并行视觉令牌调度以加速多模态大语言模型推理",
            "intro_zh": [
                "多模态大语言模型推理延迟高，因自注意力随序列长度平方增长和视觉令牌过多",
                "将视觉令牌分为主体和非主体组并行处理，转移语义后丢弃非主体路径以减少计算",
                "实验显示可剪枝88.9%视觉令牌，性能损失小，实现1.77倍加速和70%FLOPs减少"
            ],
            "tags_zh": [
                "多模态大语言模型",
                "视觉令牌调度",
                "推理加速",
                "计算复杂度降低",
                "训练无关方法"
            ],
            "_index": 112
        },
        {
            "title": "GContextFormer: A global context-aware hybrid multi-head attention approach with scaled additive aggregation for multimodal trajectory prediction",
            "authors": [
                "Yuzhi Chen",
                "Yuanchang Xie",
                "Lei Zhao",
                "Pan Liu",
                "Yajie Zou",
                "Chen Wang"
            ],
            "arxiv_id": "2511.18874v1",
            "summary": "Multimodal trajectory prediction generates multiple plausible future trajectories to address vehicle motion uncertainty from intention ambiguity and execution variability. However, HD map-dependent models suffer from costly data acquisition, delayed updates, and vulnerability to corrupted inputs, causing prediction failures. Map-free approaches lack global context, with pairwise attention over-amplifying straight patterns while suppressing transitional patterns, resulting in motion-intention misalignment. This paper proposes GContextFormer, a plug-and-play encoder-decoder architecture with global context-aware hybrid attention and scaled additive aggregation achieving intention-aligned multimodal prediction without map reliance. The Motion-Aware Encoder builds scene-level intention prior via bounded scaled additive aggregation over mode-embedded trajectory tokens and refines per-mode representations under shared global context, mitigating inter-mode suppression and promoting intention alignment. The Hierarchical Interaction Decoder decomposes social reasoning into dual-pathway cross-attention: a standard pathway ensures uniform geometric coverage over agent-mode pairs while a neighbor-context-enhanced pathway emphasizes salient interactions, with gating module mediating their contributions to maintain coverage-focus balance. Experiments on eight highway-ramp scenarios from TOD-VT dataset show GContextFormer outperforms state-of-the-art baselines. Compared to existing transformer models, GContextFormer achieves greater robustness and concentrated improvements in high-curvature and transition zones via spatial distributions. Interpretability is achieved through motion mode distinctions and neighbor context modulation exposing reasoning attribution. The modular architecture supports extensibility toward cross-domain multimodal reasoning tasks. Source: https://fenghy-chen.github.io/sources/.",
            "headline_zh": "提出GContextFormer以解决无地图多模态轨迹预测中的全局上下文缺失问题",
            "intro_zh": [
                "核心问题：无地图方法缺乏全局上下文，注意力机制放大直线模式抑制过渡模式，导致运动意图错位",
                "方法要点：采用全局上下文感知混合注意力和缩放加法聚合，实现意图对齐的多模态预测",
                "实验或效果：在TOD-VT数据集上超越现有方法，在高曲率和过渡区表现更稳健"
            ],
            "tags_zh": [
                "轨迹预测",
                "多模态学习",
                "注意力机制",
                "全局上下文",
                "意图对齐",
                "无地图预测"
            ],
            "_index": 113
        },
        {
            "title": "Neural Texture Splatting: Expressive 3D Gaussian Splatting for View Synthesis, Geometry, and Dynamic Reconstruction",
            "authors": [
                "Yiming Wang",
                "Shaofei Wang",
                "Marko Mihajlovic",
                "Siyu Tang"
            ],
            "arxiv_id": "2511.18873v1",
            "summary": "3D Gaussian Splatting (3DGS) has emerged as a leading approach for high-quality novel view synthesis, with numerous variants extending its applicability to a broad spectrum of 3D and 4D scene reconstruction tasks. Despite its success, the representational capacity of 3DGS remains limited by the use of 3D Gaussian kernels to model local variations. Recent works have proposed to augment 3DGS with additional per-primitive capacity, such as per-splat textures, to enhance its expressiveness. However, these per-splat texture approaches primarily target dense novel view synthesis with a reduced number of Gaussian primitives, and their effectiveness tends to diminish when applied to more general reconstruction scenarios. In this paper, we aim to achieve concrete performance improvement over state-of-the-art 3DGS variants across a wide range of reconstruction tasks, including novel view synthesis, geometry and dynamic reconstruction, under both sparse and dense input settings. To this end, we introduce Neural Texture Splatting (NTS). At the core of our approach is a global neural field (represented as a hybrid of a tri-plane and a neural decoder) that predicts local appearance and geometric fields for each primitive. By leveraging this shared global representation that models local texture fields across primitives, we significantly reduce model size and facilitate efficient global information exchange, demonstrating strong generalization across tasks. Furthermore, our neural modeling of local texture fields introduces expressive view- and time-dependent effects, a critical aspect that existing methods fail to account for. Extensive experiments show that Neural Texture Splatting consistently improves models and achieves state-of-the-art results across multiple benchmarks.",
            "headline_zh": "提出神经纹理溅射以增强3D高斯溅射在视图合成、几何和动态重建中的表达力",
            "intro_zh": [
                "3D高斯溅射表示能力受限，难以处理一般重建任务",
                "引入全局神经场预测局部外观和几何，减少模型大小并促进信息交换",
                "实验显示在稀疏和密集输入下，多基准测试中达到最先进性能"
            ],
            "tags_zh": [
                "3D高斯溅射",
                "神经纹理溅射",
                "视图合成",
                "几何重建",
                "动态重建",
                "全局神经场"
            ],
            "_index": 114
        },
        {
            "title": "HunyuanVideo 1.5 Technical Report",
            "authors": [
                "Bing Wu",
                "Chang Zou",
                "Changlin Li",
                "Duojun Huang",
                "Fang Yang",
                "Hao Tan",
                "Jack Peng",
                "Jianbing Wu",
                "Jiangfeng Xiong",
                "Jie Jiang",
                "Linus",
                "Patrol",
                "Peizhen Zhang",
                "Peng Chen",
                "Penghao Zhao",
                "Qi Tian",
                "Songtao Liu",
                "Weijie Kong",
                "Weiyan Wang",
                "Xiao He",
                "Xin Li",
                "Xinchi Deng",
                "Xuefei Zhe",
                "Yang Li",
                "Yanxin Long",
                "Yuanbo Peng",
                "Yue Wu",
                "Yuhong Liu",
                "Zhenyu Wang",
                "Zuozhuo Dai",
                "Bo Peng",
                "Coopers Li",
                "Gu Gong",
                "Guojian Xiao",
                "Jiahe Tian",
                "Jiaxin Lin",
                "Jie Liu",
                "Jihong Zhang",
                "Jiesong Lian",
                "Kaihang Pan",
                "Lei Wang",
                "Lin Niu",
                "Mingtao Chen",
                "Mingyang Chen",
                "Mingzhe Zheng",
                "Miles Yang",
                "Qiangqiang Hu",
                "Qi Yang",
                "Qiuyong Xiao",
                "Runzhou Wu",
                "Ryan Xu",
                "Rui Yuan",
                "Shanshan Sang",
                "Shisheng Huang",
                "Siruis Gong",
                "Shuo Huang",
                "Weiting Guo",
                "Xiang Yuan",
                "Xiaojia Chen",
                "Xiawei Hu",
                "Wenzhi Sun",
                "Xiele Wu",
                "Xianshun Ren",
                "Xiaoyan Yuan",
                "Xiaoyue Mi",
                "Yepeng Zhang",
                "Yifu Sun",
                "Yiting Lu",
                "Yitong Li",
                "You Huang",
                "Yu Tang",
                "Yixuan Li",
                "Yuhang Deng",
                "Yuan Zhou",
                "Zhichao Hu",
                "Zhiguang Liu",
                "Zhihe Yang",
                "Zilin Yang",
                "Zhenzhi Lu",
                "Zixiang Zhou",
                "Zhao Zhong"
            ],
            "arxiv_id": "2511.18870v1",
            "summary": "We present HunyuanVideo 1.5, a lightweight yet powerful open-source video generation model that achieves state-of-the-art visual quality and motion coherence with only 8.3 billion parameters, enabling efficient inference on consumer-grade GPUs. This achievement is built upon several key components, including meticulous data curation, an advanced DiT architecture featuring selective and sliding tile attention (SSTA), enhanced bilingual understanding through glyph-aware text encoding, progressive pre-training and post-training, and an efficient video super-resolution network. Leveraging these designs, we developed a unified framework capable of high-quality text-to-video and image-to-video generation across multiple durations and resolutions.Extensive experiments demonstrate that this compact and proficient model establishes a new state-of-the-art among open-source video generation models. By releasing the code and model weights, we provide the community with a high-performance foundation that lowers the barrier to video creation and research, making advanced video generation accessible to a broader audience. All open-source assets are publicly available at https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5.",
            "headline_zh": "提出HunyuanVideo 1.5轻量视频生成模型，实现高质量文本/图像到视频生成。",
            "intro_zh": [
                "核心问题：轻量模型在视频生成中平衡视觉质量与计算效率。",
                "方法要点：采用DiT架构、选择性滑动瓦片注意力和双语文本编码。",
                "实验效果：在开源模型中达到SOTA，支持多时长和分辨率生成。"
            ],
            "tags_zh": [
                "视频生成",
                "扩散变换器",
                "轻量模型",
                "文本编码",
                "超分辨率网络"
            ],
            "_index": 115
        },
        {
            "title": "DualGazeNet: A Biologically Inspired Dual-Gaze Query Network for Salient Object Detection",
            "authors": [
                "Yu Zhang",
                "Haoan Ping",
                "Yuchen Li",
                "Zhenshan Bing",
                "Fuchun Sun",
                "Alois Knoll"
            ],
            "arxiv_id": "2511.18865v1",
            "summary": "Recent salient object detection (SOD) methods aim to improve performance in four key directions: semantic enhancement, boundary refinement, auxiliary task supervision, and multi-modal fusion. In pursuit of continuous gains, these approaches have evolved toward increasingly sophisticated architectures with multi-stage pipelines, specialized fusion modules, edge-guided learning, and elaborate attention mechanisms. However, this complexity paradoxically introduces feature redundancy and cross-component interference that obscure salient cues, ultimately reaching performance bottlenecks. In contrast, human vision achieves efficient salient object identification without such architectural complexity. This contrast raises a fundamental question: can we design a biologically grounded yet architecturally simple SOD framework that dispenses with most of this engineering complexity, while achieving state-of-the-art accuracy, computational efficiency, and interpretability? In this work, we answer this question affirmatively by introducing DualGazeNet, a biologically inspired pure Transformer framework that models the dual biological principles of robust representation learning and magnocellular-parvocellular dual-pathway processing with cortical attention modulation in the human visual system. Extensive experiments on five RGB SOD benchmarks show that DualGazeNet consistently surpasses 25 state-of-the-art CNN- and Transformer-based methods. On average, DualGazeNet achieves about 60\\% higher inference speed and 53.4\\% fewer FLOPs than four Transformer-based baselines of similar capacity (VST++, MDSAM, Sam2unet, and BiRefNet). Moreover, DualGazeNet exhibits strong cross-domain generalization, achieving leading or highly competitive performance on camouflaged and underwater SOD benchmarks without relying on additional modalities.",
            "headline_zh": "提出DualGazeNet以简化显著目标检测架构并提升性能",
            "intro_zh": [
                "核心问题：复杂SOD方法导致特征冗余和性能瓶颈",
                "方法要点：基于Transformer模拟人类视觉双通路处理机制",
                "实验或效果：在多个基准上超越25种方法，速度和效率显著提升"
            ],
            "tags_zh": [
                "显著目标检测",
                "Transformer架构",
                "生物启发模型",
                "计算效率",
                "跨域泛化"
            ],
            "_index": 116
        },
        {
            "title": "Robust and Generalizable GNN Fine-Tuning via Uncertainty-aware Adapter Learning",
            "authors": [
                "Bo Jiang",
                "Weijun Zhao",
                "Beibei Wang",
                "Xiao Wang",
                "Jin Tang"
            ],
            "arxiv_id": "2511.18859v1",
            "summary": "Recently, fine-tuning large-scale pre-trained GNNs has yielded remarkable attention in adapting pre-trained GNN models for downstream graph learning tasks. One representative fine-tuning method is to exploit adapter (termed AdapterGNN) which aims to 'augment' the pre-trained model by inserting a lightweight module to make the 'augmented' model better adapt to the downstream tasks. However, graph data may contain various types of noise in downstream tasks, such as noisy edges and ambiguous node attributes. Existing AdapterGNNs are often prone to graph noise and exhibit limited generalizability. How to enhance the robustness and generalization ability of GNNs' fine tuning remains an open problem. In this paper, we show that the above problem can be well addressed by integrating uncertainty learning into the GNN adapter. We propose the Uncertainty-aware Adapter (UAdapterGNN) that fortifies pre-trained GNN models against noisy graph data in the fine-tuning process. Specifically, in contrast to regular AdapterGNN, our UAdapterGNN exploits Gaussian probabilistic adapter to augment the pre-trained GNN model. In this way, when the graph contains various noises,our method can automatically absorb the effects of changes in the variances of the Gaussian distribution, thereby significantly enhancing the model's robustness. Also, UAdapterGNN can further improve the generalization ability of the model on the downstream tasks. Extensive experiments on several benchmarks demonstrate the effectiveness, robustness and high generalization ability of the proposed UAdapterGNN method.",
            "headline_zh": "提出不确定性感知适配器以增强图神经网络在噪声数据下的鲁棒性和泛化能力",
            "intro_zh": [
                "图数据在微调中易受噪声影响，现有适配器方法鲁棒性和泛化性不足",
                "引入高斯概率适配器，通过方差变化自动吸收噪声，提升模型适应性",
                "多基准实验验证方法在噪声图数据下的有效性、鲁棒性和高泛化能力"
            ],
            "tags_zh": [
                "图神经网络微调",
                "不确定性学习",
                "适配器模块",
                "噪声鲁棒性",
                "高斯概率模型",
                "泛化能力"
            ],
            "_index": 117
        },
        {
            "title": "Rethinking Long-tailed Dataset Distillation: A Uni-Level Framework with Unbiased Recovery and Relabeling",
            "authors": [
                "Xiao Cui",
                "Yulei Qin",
                "Xinyue Li",
                "Wengang Zhou",
                "Hongsheng Li",
                "Houqiang Li"
            ],
            "arxiv_id": "2511.18858v1",
            "summary": "Dataset distillation creates a small distilled set that enables efficient training by capturing key information from the full dataset. While existing dataset distillation methods perform well on balanced datasets, they struggle under long-tailed distributions, where imbalanced class frequencies induce biased model representations and corrupt statistical estimates such as Batch Normalization (BN) statistics. In this paper, we rethink long-tailed dataset distillation by revisiting the limitations of trajectory-based methods, and instead adopt the statistical alignment perspective to jointly mitigate model bias and restore fair supervision. To this end, we introduce three dedicated components that enable unbiased recovery of distilled images and soft relabeling: (1) enhancing expert models (an observer model for recovery and a teacher model for relabeling) to enable reliable statistics estimation and soft-label generation; (2) recalibrating BN statistics via a full forward pass with dynamically adjusted momentum to reduce representation skew; (3) initializing synthetic images by incrementally selecting high-confidence and diverse augmentations via a multi-round mechanism that promotes coverage and diversity. Extensive experiments on four long-tailed benchmarks show consistent improvements over state-of-the-art methods across varying degrees of class imbalance.Notably, our approach improves top-1 accuracy by 15.6% on CIFAR-100-LT and 11.8% on Tiny-ImageNet-LT under IPC=10 and IF=10.",
            "headline_zh": "提出统一框架，通过无偏恢复和软重标解决长尾数据集蒸馏问题",
            "intro_zh": [
                "长尾分布导致模型表示偏差和统计估计错误，影响蒸馏效果",
                "方法包括增强专家模型、重校准BN统计和多轮初始化合成图像",
                "在多个长尾基准测试中，准确率显著提升，如CIFAR-100-LT提高15.6%"
            ],
            "tags_zh": [
                "数据集蒸馏",
                "长尾分布",
                "无偏恢复",
                "软重标",
                "统计对齐",
                "模型偏差"
            ],
            "_index": 118
        },
        {
            "title": "AutoOdom: Learning Auto-regressive Proprioceptive Odometry for Legged Locomotion",
            "authors": [
                "Changsheng Luo",
                "Yushi Wang",
                "Wenhan Cai",
                "Mingguo Zhao"
            ],
            "arxiv_id": "2511.18857v1",
            "summary": "Accurate proprioceptive odometry is fundamental for legged robot navigation in GPS-denied and visually degraded environments where conventional visual odometry systems fail. Current approaches face critical limitations: analytical filtering methods suffer from modeling uncertainties and cumulative drift, hybrid learning-filtering approaches remain constrained by their analytical components, while pure learning-based methods struggle with simulation-to-reality transfer and demand extensive real-world data collection. This paper introduces AutoOdom, a novel autoregressive proprioceptive odometry system that overcomes these challenges through an innovative two-stage training paradigm. Stage 1 employs large-scale simulation data to learn complex nonlinear dynamics and rapidly changing contact states inherent in legged locomotion, while Stage 2 introduces an autoregressive enhancement mechanism using limited real-world data to effectively bridge the sim-to-real gap. The key innovation lies in our autoregressive training approach, where the model learns from its own predictions to develop resilience against sensor noise and improve robustness in highly dynamic environments. Comprehensive experimental validation on the Booster T1 humanoid robot demonstrates that AutoOdom significantly outperforms state-of-the-art methods across all evaluation metrics, achieving 57.2% improvement in absolute trajectory error, 59.2% improvement in Umeyama-aligned error, and 36.2% improvement in relative pose error compared to the Legolas baseline. Extensive ablation studies provide critical insights into sensor modality selection and temporal modeling, revealing counterintuitive findings about IMU acceleration data and validating our systematic design choices for robust proprioceptive odometry in challenging locomotion scenarios.",
            "headline_zh": "提出AutoOdom自回归本体感知里程计，解决腿式机器人在GPS缺失和视觉退化环境中的定位问题",
            "intro_zh": [
                "核心问题：传统里程计在GPS缺失和视觉退化环境中失效，现有方法存在建模不确定性、仿真到现实差距和漂移问题",
                "方法要点：采用两阶段训练，先仿真学习非线性动态和接触状态，再自回归增强以弥合仿真到现实差距",
                "实验或效果：在Booster T1人形机器人上验证，绝对轨迹误差等指标显著优于基线方法，提升达57.2%"
            ],
            "tags_zh": [
                "腿式机器人定位",
                "自回归学习",
                "仿真到现实迁移",
                "本体感知里程计",
                "两阶段训练",
                "传感器模态选择"
            ],
            "_index": 119
        },
        {
            "title": "Deep Hybrid Model for Region of Interest Detection in Omnidirectional Videos",
            "authors": [
                "Sana Alamgeer"
            ],
            "arxiv_id": "2511.18856v1",
            "summary": "The main goal of the project is to design a new model that predicts regions of interest in 360$^{\\circ}$ videos. The region of interest (ROI) plays an important role in 360$^{\\circ}$ video streaming. For example, ROIs are used to predict view-ports, intelligently cut the videos for live streaming, etc so that less bandwidth is used. Detecting view-ports in advance helps reduce the movement of the head while streaming and watching a video via the head-mounted device. Whereas, intelligent cuts of the videos help improve the efficiency of streaming the video to users and enhance the quality of their viewing experience. This report illustrates the secondary task to identify ROIs, in which, we design, train, and test a hybrid saliency model. In this work, we refer to saliency regions to represent the regions of interest. The method includes the processes as follows: preprocessing the video to obtain frames, developing a hybrid saliency model for predicting the region of interest, and finally post-processing the output predictions of the hybrid saliency model to obtain the output region of interest for each frame. Then, we compare the performance of the proposed method with the subjective annotations of the 360RAT dataset.",
            "headline_zh": "提出混合显著性模型以预测360度视频中的感兴趣区域，优化流媒体带宽与观看体验。",
            "intro_zh": [
                "核心问题：360度视频中感兴趣区域检测对带宽优化和头戴设备观看体验至关重要。",
                "方法要点：设计混合显著性模型，包括预处理、模型预测和后处理步骤。",
                "实验或效果：在360RAT数据集上评估模型性能，并与主观标注进行比较。"
            ],
            "tags_zh": [
                "360度视频",
                "感兴趣区域检测",
                "混合显著性模型",
                "视频流媒体优化",
                "头戴设备观看"
            ],
            "_index": 120
        },
        {
            "title": "Robust Long-term Test-Time Adaptation for 3D Human Pose Estimation through Motion Discretization",
            "authors": [
                "Yilin Wen",
                "Kechuan Dong",
                "Yusuke Sugano"
            ],
            "arxiv_id": "2511.18851v1",
            "summary": "Online test-time adaptation addresses the train-test domain gap by adapting the model on unlabeled streaming test inputs before making the final prediction. However, online adaptation for 3D human pose estimation suffers from error accumulation when relying on self-supervision with imperfect predictions, leading to degraded performance over time. To mitigate this fundamental challenge, we propose a novel solution that highlights the use of motion discretization. Specifically, we employ unsupervised clustering in the latent motion representation space to derive a set of anchor motions, whose regularity aids in supervising the human pose estimator and enables efficient self-replay. Additionally, we introduce an effective and efficient soft-reset mechanism by reverting the pose estimator to its exponential moving average during continuous adaptation. We examine long-term online adaptation by continuously adapting to out-of-domain streaming test videos of the same individual, which allows for the capture of consistent personal shape and motion traits throughout the streaming observation. By mitigating error accumulation, our solution enables robust exploitation of these personal traits for enhanced accuracy. Experiments demonstrate that our solution outperforms previous online test-time adaptation methods and validate our design choices.",
            "headline_zh": "提出基于运动离散化的长时测试时适应方法，以解决3D人体姿态估计中的误差累积问题。",
            "intro_zh": [
                "核心问题：在线测试时适应中，依赖不完美预测的自监督导致误差累积，性能随时间下降。",
                "方法要点：通过无监督聚类获取锚定运动，利用其规律性监督姿态估计器并实现高效自回放。",
                "实验或效果：在长时在线适应实验中，方法优于先前方法，验证了设计有效性。"
            ],
            "tags_zh": [
                "3D人体姿态估计",
                "测试时适应",
                "运动离散化",
                "误差累积缓解",
                "在线学习",
                "自监督学习"
            ],
            "_index": 121
        },
        {
            "title": "Personalized Federated Segmentation with Shared Feature Aggregation and Boundary-Focused Calibration",
            "authors": [
                "Ishmam Tashdeed",
                "Md. Atiqur Rahman",
                "Sabrina Islam",
                "Md. Azam Hossain"
            ],
            "arxiv_id": "2511.18847v1",
            "summary": "Personalized federated learning (PFL) possesses the unique capability of preserving data confidentiality among clients while tackling the data heterogeneity problem of non-independent and identically distributed (Non-IID) data. Its advantages have led to widespread adoption in domains such as medical image segmentation. However, the existing approaches mostly overlook the potential benefits of leveraging shared features across clients, where each client contains segmentation data of different organs. In this work, we introduce a novel personalized federated approach for organ agnostic tumor segmentation (FedOAP), that utilizes cross-attention to model long-range dependencies among the shared features of different clients and a boundary-aware loss to improve segmentation consistency. FedOAP employs a decoupled cross-attention (DCA), which enables each client to retain local queries while attending to globally shared key-value pairs aggregated from all clients, thereby capturing long-range inter-organ feature dependencies. Additionally, we introduce perturbed boundary loss (PBL) which focuses on the inconsistencies of the predicted mask's boundary for each client, forcing the model to localize the margins more precisely. We evaluate FedOAP on diverse tumor segmentation tasks spanning different organs. Extensive experiments demonstrate that FedOAP consistently outperforms existing state-of-the-art federated and personalized segmentation methods.",
            "headline_zh": "提出FedOAP方法，利用共享特征聚合和边界聚焦校准解决非IID数据下的个性化联邦分割问题。",
            "intro_zh": [
                "核心问题：现有方法忽略跨客户端共享特征，难以处理非IID数据异质性。",
                "方法要点：采用解耦交叉注意力聚合共享特征，并引入扰动边界损失提升分割一致性。",
                "实验或效果：在多器官肿瘤分割任务中，FedOAP优于现有联邦和个性化分割方法。"
            ],
            "tags_zh": [
                "个性化联邦学习",
                "医学图像分割",
                "特征聚合",
                "边界损失",
                "非IID数据",
                "肿瘤分割"
            ],
            "_index": 122
        },
        {
            "title": "Enhancing Multi-Label Thoracic Disease Diagnosis with Deep Ensemble-Based Uncertainty Quantification",
            "authors": [
                "Yasiru Laksara",
                "Uthayasanker Thayasivam"
            ],
            "arxiv_id": "2511.18839v1",
            "summary": "The utility of deep learning models, such as CheXNet, in high stakes clinical settings is fundamentally constrained by their purely deterministic nature, failing to provide reliable measures of predictive confidence. This project addresses this critical gap by integrating robust Uncertainty Quantification (UQ) into a high performance diagnostic platform for 14 common thoracic diseases on the NIH ChestX-ray14 dataset. Initial architectural development failed to stabilize performance and calibration using Monte Carlo Dropout (MCD), yielding an unacceptable Expected Calibration Error (ECE) of 0.7588. This technical failure necessitated a rigorous architectural pivot to a high diversity, 9-member Deep Ensemble (DE). This resulting DE successfully stabilized performance and delivered superior reliability, achieving a State-of-the-Art (SOTA) average Area Under the Receiver Operating Characteristic Curve (AUROC) of 0.8559 and an average F1 Score of 0.3857. Crucially, the DE demonstrated superior calibration (Mean ECE of 0.0728 and Negative Log-Likelihood (NLL) of 0.1916) and enabled the reliable decomposition of total uncertainty into its Aleatoric (irreducible data noise) and Epistemic (reducible model knowledge) components, with a mean Epistemic Uncertainty (EU) of 0.0240. These results establish the Deep Ensemble as a trustworthy and explainable platform, transforming the model from a probabilistic tool into a reliable clinical decision support system.",
            "headline_zh": "提出基于深度集成的不确定性量化方法，以增强多标签胸部疾病诊断的可靠性",
            "intro_zh": [
                "核心问题：深度学习模型在临床应用中缺乏可靠的不确定性度量，影响决策可信度",
                "方法要点：采用9成员深度集成替代蒙特卡洛Dropout，实现不确定性分解与性能稳定",
                "实验或效果：在NIH ChestX-ray14数据集上达到SOTA AUROC 0.8559，平均ECE 0.0728"
            ],
            "tags_zh": [
                "不确定性量化",
                "深度集成",
                "胸部X光诊断",
                "多标签分类",
                "模型校准",
                "临床决策支持"
            ],
            "_index": 123
        },
        {
            "title": "FVAR: Visual Autoregressive Modeling via Next Focus Prediction",
            "authors": [
                "Xiaofan Li",
                "Chenming Wu",
                "Yanpeng Sun",
                "Jiaming Zhou",
                "Delin Qu",
                "Yansong Qu",
                "Weihao Bo",
                "Haibao Yu",
                "Dingkang Liang"
            ],
            "arxiv_id": "2511.18838v1",
            "summary": "Visual autoregressive models achieve remarkable generation quality through next-scale predictions across multi-scale token pyramids. However, the conventional method uses uniform scale downsampling to build these pyramids, leading to aliasing artifacts that compromise fine details and introduce unwanted jaggies and moiré patterns. To tackle this issue, we present \\textbf{FVAR}, which reframes the paradigm from \\emph{next-scale prediction} to \\emph{next-focus prediction}, mimicking the natural process of camera focusing from blur to clarity. Our approach introduces three key innovations: \\textbf{1) Next-Focus Prediction Paradigm} that transforms multi-scale autoregression by progressively reducing blur rather than simply downsampling; \\textbf{2) Progressive Refocusing Pyramid Construction} that uses physics-consistent defocus kernels to build clean, alias-free multi-scale representations; and \\textbf{3) High-Frequency Residual Learning} that employs a specialized residual teacher network to effectively incorporate alias information during training while maintaining deployment simplicity. Specifically, we construct optical low-pass views using defocus point spread function (PSF) kernels with decreasing radius, creating smooth blur-to-clarity transitions that eliminate aliasing at its source. To further enhance detail generation, we introduce a High-Frequency Residual Teacher that learns from both clean structure and alias residuals, distilling this knowledge to a vanilla VAR deployment network for seamless inference. Extensive experiments on ImageNet demonstrate that FVAR substantially reduces aliasing artifacts, improves fine detail preservation, and enhances text readability, achieving superior performance with perfect compatibility to existing VAR frameworks.",
            "headline_zh": "提出FVAR通过下一焦点预测解决视觉自回归模型中的混叠伪影问题",
            "intro_zh": [
                "核心问题：传统多尺度自回归模型因均匀下采样产生混叠伪影，损害细节和引入锯齿",
                "方法要点：引入下一焦点预测范式，使用物理一致散焦核构建无混叠金字塔，结合高频残差学习",
                "实验或效果：在ImageNet上显著减少混叠，提升细节保留和文本可读性，兼容现有框架"
            ],
            "tags_zh": [
                "视觉自回归建模",
                "混叠伪影消除",
                "多尺度表示",
                "焦点预测",
                "高频残差学习",
                "图像生成"
            ],
            "_index": 124
        },
        {
            "title": "FlowSteer: Guiding Few-Step Image Synthesis with Authentic Trajectories",
            "authors": [
                "Lei Ke",
                "Hubery Yin",
                "Gongye Liu",
                "Zhengyao Lv",
                "Jingcai Guo",
                "Chen Li",
                "Wenhan Luo",
                "Yujiu Yang",
                "Jing Lyu"
            ],
            "arxiv_id": "2511.18834v1",
            "summary": "With the success of flow matching in visual generation, sampling efficiency remains a critical bottleneck for its practical application. Among flow models' accelerating methods, ReFlow has been somehow overlooked although it has theoretical consistency with flow matching. This is primarily due to its suboptimal performance in practical scenarios compared to consistency distillation and score distillation. In this work, we investigate this issue within the ReFlow framework and propose FlowSteer, a method unlocks the potential of ReFlow-based distillation by guiding the student along teacher's authentic generation trajectories. We first identify that Piecewised ReFlow's performance is hampered by a critical distribution mismatch during the training and propose Online Trajectory Alignment(OTA) to resolve it. Then, we introduce a adversarial distillation objective applied directly on the ODE trajectory, improving the student's adherence to the teacher's generation trajectory. Furthermore, we find and fix a previously undiscovered flaw in the widely-used FlowMatchEulerDiscreteScheduler that largely degrades few-step inference quality. Our experiment result on SD3 demonstrates our method's efficacy.",
            "headline_zh": "提出FlowSteer方法，通过真实轨迹引导提升少步图像合成效率",
            "intro_zh": [
                "核心问题：ReFlow方法在流匹配中采样效率低，实际性能不如一致性蒸馏和分数蒸馏",
                "方法要点：引入在线轨迹对齐和对抗蒸馏目标，优化学生模型沿教师轨迹生成",
                "实验或效果：在SD3上验证方法有效性，修复FlowMatchEulerDiscreteScheduler缺陷"
            ],
            "tags_zh": [
                "流匹配",
                "图像合成",
                "蒸馏训练",
                "轨迹对齐",
                "少步推理"
            ],
            "_index": 125
        },
        {
            "title": "PrismAudio: Decomposed Chain-of-Thoughts and Multi-dimensional Rewards for Video-to-Audio Generation",
            "authors": [
                "Huadai Liu",
                "Kaicheng Luo",
                "Wen Wang",
                "Qian Chen",
                "Peiwen Sun",
                "Rongjie Huang",
                "Xiangang Li",
                "Jieping Ye",
                "Wei Xue"
            ],
            "arxiv_id": "2511.18833v1",
            "summary": "Video-to-Audio (V2A) generation requires balancing four critical perceptual dimensions: semantic consistency, audio-visual temporal synchrony, aesthetic quality, and spatial accuracy; yet existing methods suffer from objective entanglement that conflates competing goals in single loss functions and lack human preference alignment. We introduce PrismAudio, the first framework to integrate Reinforcement Learning into V2A generation with specialized Chain-of-Thought (CoT) planning. Our approach decomposes monolithic reasoning into four specialized CoT modules (Semantic, Temporal, Aesthetic, and Spatial CoT), each paired with targeted reward functions. This CoT-reward correspondence enables multidimensional RL optimization that guides the model to jointly generate better reasoning across all perspectives, solving the objective entanglement problem while preserving interpretability. To make this optimization computationally practical, we propose Fast-GRPO, which employs hybrid ODE-SDE sampling that dramatically reduces the training overhead compared to existing GRPO implementations. We also introduce AudioCanvas, a rigorous benchmark that is more distributionally balanced and covers more realistically diverse and challenging scenarios than existing datasets, with 300 single-event classes and 501 multi-event samples. Experimental results demonstrate that PrismAudio achieves state-of-the-art performance across all four perceptual dimensions on both the in-domain VGGSound test set and out-of-domain AudioCanvas benchmark. The project page is available at https://PrismAudio-Project.github.io.",
            "headline_zh": "提出PrismAudio框架，通过分解思维链和多维奖励解决视频到音频生成的客观纠缠问题。",
            "intro_zh": [
                "核心问题：现有方法在视频到音频生成中存在客观纠缠，混淆多个感知维度的目标。",
                "方法要点：引入分解思维链和对应奖励函数，结合强化学习进行多维优化。",
                "实验或效果：在VGGSound和AudioCanvas基准上实现所有四个感知维度的最先进性能。"
            ],
            "tags_zh": [
                "视频到音频生成",
                "强化学习",
                "思维链分解",
                "多维奖励优化",
                "AudioCanvas基准"
            ],
            "_index": 126
        },
        {
            "title": "VideoCompressa: Data-Efficient Video Understanding via Joint Temporal Compression and Spatial Reconstruction",
            "authors": [
                "Shaobo Wang",
                "Tianle Niu",
                "Runkang Yang",
                "Deshan Liu",
                "Xu He",
                "Zichen Wen",
                "Conghui He",
                "Xuming Hu",
                "Linfeng Zhang"
            ],
            "arxiv_id": "2511.18831v1",
            "summary": "The scalability of video understanding models is increasingly limited by the prohibitive storage and computational costs of large-scale video datasets. While data synthesis has improved data efficiency in the image domain, its extension to video remains challenging due to pervasive temporal redundancy and complex spatiotemporal dynamics. In this work, we uncover a critical insight: the primary source of inefficiency in video datasets is not inter-sample redundancy, but intra-sample frame-level redundancy. To leverage this insight, we introduce VideoCompressa, a novel framework for video data synthesis that reframes the problem as dynamic latent compression. Specifically, VideoCompressa jointly optimizes a differentiable keyframe selector-implemented as a lightweight ConvNet with Gumbel-Softmax sampling-to identify the most informative frames, and a pretrained, frozen Variational Autoencoder (VAE) to compress these frames into compact, semantically rich latent codes. These latent representations are then fed into a compression network, enabling end-to-end backpropagation. Crucially, the keyframe selector and synthetic latent codes are co-optimized to maximize retention of task-relevant information. Experiments show that our method achieves unprecedented data efficiency: on UCF101 with ConvNets, VideoCompressa surpasses full-data training by 2.34\\% points using only 0.13\\% of the original data, with over 5800x speedup compared to traditional synthesis method. Moreover, when fine-tuning Qwen2.5-7B-VL on HMDB51, VideoCompressa matches full-data performance using just 0.41\\% of the training data-outperforming zero-shot baseline by 10.61\\%.",
            "headline_zh": "提出VideoCompressa框架，通过联合时间压缩和空间重建解决视频数据效率问题",
            "intro_zh": [
                "核心问题：视频数据效率低源于帧级冗余，而非样本间冗余",
                "方法要点：联合优化可微分关键帧选择器和VAE，压缩帧为语义丰富潜码",
                "实验效果：在UCF101上仅用0.13%数据超越全数据训练，速度提升5800倍"
            ],
            "tags_zh": [
                "视频数据合成",
                "时间压缩",
                "空间重建",
                "数据效率",
                "关键帧选择",
                "变分自编码器"
            ],
            "_index": 127
        },
        {
            "title": "Leveraging Metaheuristic Approaches to Improve Deep Learning Systems for Anxiety Disorder Detection",
            "authors": [
                "Mohammadreza Amiri",
                "Monireh Hosseini"
            ],
            "arxiv_id": "2511.18827v1",
            "summary": "Despite being among the most common psychological disorders, anxiety-related conditions are still primarily identified through subjective assessments, such as clinical interviews and self-evaluation questionnaires. These conventional methods often require significant time and may vary depending on the evaluator. However, the emergence of advanced artificial intelligence techniques has created new opportunities for detecting anxiety in a more consistent and automated manner. To address the limitations of traditional approaches, this study introduces a comprehensive model that integrates deep learning architectures with optimization strategies inspired by swarm intelligence. Using multimodal and wearable-sensor datasets, the framework analyzes physiological, emotional, and behavioral signals. Swarm intelligence techniques including genetic algorithms and particle swarm optimization are incorporated to refine the feature space and optimize hyperparameters. Meanwhile, deep learning components are tasked with deriving layered and discriminative representations from sequential, multi-source inputs. Our evaluation shows that the fusion of these two computational paradigms significantly enhances detection performance compared with using deep networks alone. The hybrid model achieves notable improvements in accuracy and demonstrates stronger generalization across various individuals. Overall, the results highlight the potential of combining metaheuristic optimization with deep learning to develop scalable, objective, and clinically meaningful solutions for assessing anxiety disorders",
            "headline_zh": "提出结合元启发式优化与深度学习的混合模型以改进焦虑障碍检测",
            "intro_zh": [
                "核心问题：传统焦虑检测方法主观且耗时，依赖临床访谈和问卷。",
                "方法要点：集成深度学习与群体智能优化，优化特征和超参数。",
                "实验或效果：混合模型在准确性和泛化性上优于纯深度学习。"
            ],
            "tags_zh": [
                "焦虑障碍检测",
                "深度学习",
                "元启发式优化",
                "群体智能",
                "多模态数据",
                "超参数优化"
            ],
            "_index": 128
        },
        {
            "title": "Uncertainty-Aware Dual-Student Knowledge Distillation for Efficient Image Classification",
            "authors": [
                "Aakash Gore",
                "Anoushka Dey",
                "Aryan Mishra"
            ],
            "arxiv_id": "2511.18826v1",
            "summary": "Knowledge distillation has emerged as a powerful technique for model compression, enabling the transfer of knowledge from large teacher networks to compact student models. However, traditional knowledge distillation methods treat all teacher predictions equally, regardless of the teacher's confidence in those predictions. This paper proposes an uncertainty-aware dual-student knowledge distillation framework that leverages teacher prediction uncertainty to selectively guide student learning. We introduce a peer-learning mechanism where two heterogeneous student architectures, specifically ResNet-18 and MobileNetV2, learn collaboratively from both the teacher network and each other. Experimental results on ImageNet-100 demonstrate that our approach achieves superior performance compared to baseline knowledge distillation methods, with ResNet-18 achieving 83.84\\% top-1 accuracy and MobileNetV2 achieving 81.46\\% top-1 accuracy, representing improvements of 2.04\\% and 0.92\\% respectively over traditional single-student distillation approaches.",
            "headline_zh": "提出不确定性感知双学生知识蒸馏框架以提升图像分类效率",
            "intro_zh": [
                "传统知识蒸馏方法忽视教师预测不确定性，影响学生模型学习效果",
                "引入双学生架构，通过不确定性感知和同伴学习机制优化知识传递",
                "在ImageNet-100上验证，ResNet-18和MobileNetV2准确率分别提升2.04%和0.92%"
            ],
            "tags_zh": [
                "知识蒸馏",
                "不确定性感知",
                "双学生架构",
                "图像分类",
                "模型压缩"
            ],
            "_index": 129
        },
        {
            "title": "Q-Save: Towards Scoring and Attribution for Generated Video Evaluation",
            "authors": [
                "Xiele Wu",
                "Zicheng Zhang",
                "Mingtao Chen",
                "Yixian Liu",
                "Yiming Liu",
                "Shushi Wang",
                "Zhichao Hu",
                "Yuhong Liu",
                "Guangtao Zhai",
                "Xiaohong Liu"
            ],
            "arxiv_id": "2511.18825v1",
            "summary": "We present Q-Save, a new benchmark dataset and model for holistic and explainable evaluation of AI-generated video (AIGV) quality. The dataset contains near 10000 videos, each annotated with a scalar mean opinion score (MOS) and fine-grained attribution labels along three core dimensions: visual quality, dynamic quality, and text-video alignment. These multi-aspect annotations enable both accurate quality assessment and interpretable reasoning behind the scores. To leverage this data, we propose a unified evaluation model that jointly performs quality scoring and attribution-based explanation. The model adopts the SlowFast framework to distinguish between fast frames and slow frames - slow frames are processed with high resolution while fast frames use low resolution, balancing evaluation accuracy and computational efficiency. For training, we use data formatted in Chain-of-Thought (COT) style and employ a multi-stage strategy: we first conduct Supervised Fine-Tuning (SFT), then further enhance the model with Grouped Relative Policy Optimization (GRPO), and finally perform SFT again to improve model stability. Experimental results demonstrate that our model achieves state-of-the-art performance in video quality prediction while also providing human-aligned, interpretable justifications. Our dataset and model establish a strong foundation for explainable evaluation in generative video research, contributing to the development of multimodal generation and trustworthy AI. Code and dataset will be released upon publication.",
            "headline_zh": "提出Q-Save基准数据集与模型，用于AI生成视频的全面可解释评估。",
            "intro_zh": [
                "核心问题：AI生成视频缺乏全面且可解释的质量评估基准。",
                "方法要点：构建多维度标注数据集，并基于SlowFast框架联合评分与归因。",
                "实验或效果：模型在视频质量预测上达到SOTA，并提供人类对齐的解释。"
            ],
            "tags_zh": [
                "AI生成视频评估",
                "多维度标注",
                "SlowFast框架",
                "可解释AI",
                "质量评分与归因"
            ],
            "_index": 130
        },
        {
            "title": "Assessing the alignment between infants' visual and linguistic experience using multimodal language models",
            "authors": [
                "Alvin Wei Ming Tan",
                "Jane Yang",
                "Tarun Sepuri",
                "Khai Loong Aw",
                "Robert Z. Sparks",
                "Zi Yin",
                "Virginia A. Marchman",
                "Michael C. Frank",
                "Bria Long"
            ],
            "arxiv_id": "2511.18824v1",
            "summary": "Figuring out which objects or concepts words refer to is a central language learning challenge for young children. Most models of this process posit that children learn early object labels from co-occurrences of words and their referents that occur when someone around them talks about an object in the immediate physical environment. But how aligned in time are children's visual and linguistic experiences during everyday learning? To date, answers to this question have been limited by the need for labor-intensive manual annotations of vision-language co-occurrences. Here, we evaluate the use of contrastive language-image pretraining (CLIP) models to automatically characterize vision-language alignment in egocentric videos taken from the infant perspective in home environments. After validating CLIP alignment scores using human alignment judgments, we apply this metric to a large corpus of infant-perspective videos. We show that idealized aligned moments for learning (e.g., \"look at the ball\" with a ball present in the child's view) are relatively rare in children's everyday experiences compared to modern machine learning datasets, and highlight variability in alignment both within and across children. These findings suggest that infrequent alignment is a constraint for models describing early word learning and offer a new method for investigating children's multimodal environment.",
            "headline_zh": "提出使用CLIP模型自动评估婴儿视角视频中视觉与语言对齐，揭示学习对齐时刻稀少。",
            "intro_zh": [
                "核心问题：婴儿日常学习中视觉与语言经验的时间对齐程度未知。",
                "方法要点：利用CLIP模型自动分析婴儿视角视频中的视觉-语言对齐。",
                "实验或效果：验证CLIP分数与人工判断一致，发现对齐时刻在真实数据中罕见。"
            ],
            "tags_zh": [
                "婴儿语言学习",
                "视觉语言对齐",
                "CLIP模型",
                "多模态学习",
                "第一人称视频分析"
            ],
            "_index": 131
        },
        {
            "title": "VideoPerceiver: Enhancing Fine-Grained Temporal Perception in Video Multimodal Large Language Models",
            "authors": [
                "Fufangchen Zhao",
                "Liao Zhang",
                "Daiqi Shi",
                "Yuanjun Gao",
                "Chen Ye",
                "Yang Cai",
                "Jian Gao",
                "Danfeng Yan"
            ],
            "arxiv_id": "2511.18823v1",
            "summary": "We propose VideoPerceiver, a novel video multimodal large language model (VMLLM) that enhances fine-grained perception in video understanding, addressing VMLLMs' limited ability to reason about brief actions in short clips or rare transient events in long videos. VideoPerceiver adopts a two-stage training framework. During supervised fine-tuning (SFT), we construct \"key-information-missing\" videos by extracting event-action keywords from captions, identifying corresponding key frames, and replacing them with adjacent frames. We jointly encode original and modified video tokens with text tokens, aligning intermediate visual representations with keywords via an auxiliary contrastive loss to enhance sensitivity to fine-grained motion cues. In reinforcement learning (RL), both video variants are fed into the model to generate descriptions, and a novel relative reward ensures responses from complete videos outperform those from degraded inputs, explicitly training the model to recover temporally precise action details. We also curate a dataset of 80,000 videos with fine-grained actions and transient events. Experiments show VideoPerceiver substantially outperforms state-of-the-art VMLLMs on fine-grained action understanding and rare event captioning benchmarks, while maintaining strong performance on standard tasks. By prioritizing task-relevant visual features, our work redefines video-language model training for fine-grained perception.",
            "headline_zh": "提出VideoPerceiver以增强视频多模态大语言模型在细粒度时间感知中的能力",
            "intro_zh": [
                "核心问题：VMLLMs在短片段中推理短暂动作或长视频中罕见瞬态事件的能力有限",
                "方法要点：采用两阶段训练框架，包括SFT中的关键信息缺失视频构建和RL中的相对奖励机制",
                "实验或效果：在细粒度动作理解和罕见事件描述基准上显著优于现有VMLLMs，同时保持标准任务性能"
            ],
            "tags_zh": [
                "视频多模态大语言模型",
                "细粒度时间感知",
                "两阶段训练",
                "关键信息缺失",
                "相对奖励机制",
                "罕见事件描述"
            ],
            "_index": 132
        },
        {
            "title": "DiP: Taming Diffusion Models in Pixel Space",
            "authors": [
                "Zhennan Chen",
                "Junwei Zhu",
                "Xu Chen",
                "Jiangning Zhang",
                "Xiaobin Hu",
                "Hanzhen Zhao",
                "Chengjie Wang",
                "Jian Yang",
                "Ying Tai"
            ],
            "arxiv_id": "2511.18822v1",
            "summary": "Diffusion models face a fundamental trade-off between generation quality and computational efficiency. Latent Diffusion Models (LDMs) offer an efficient solution but suffer from potential information loss and non-end-to-end training. In contrast, existing pixel space models bypass VAEs but are computationally prohibitive for high-resolution synthesis. To resolve this dilemma, we propose DiP, an efficient pixel space diffusion framework. DiP decouples generation into a global and a local stage: a Diffusion Transformer (DiT) backbone operates on large patches for efficient global structure construction, while a co-trained lightweight Patch Detailer Head leverages contextual features to restore fine-grained local details. This synergistic design achieves computational efficiency comparable to LDMs without relying on a VAE. DiP is accomplished with up to 10$\\times$ faster inference speeds than previous method while increasing the total number of parameters by only 0.3%, and achieves an 1.90 FID score on ImageNet 256$\\times$256.",
            "headline_zh": "提出DiP框架以解决扩散模型在像素空间的计算效率与质量权衡问题",
            "intro_zh": [
                "扩散模型面临生成质量与计算效率的根本权衡，潜在扩散模型效率高但信息损失",
                "DiP将生成解耦为全局和局部阶段，使用DiT构建结构，轻量头恢复细节",
                "实验显示DiP推理速度提升10倍，FID达1.90，参数仅增0.3%"
            ],
            "tags_zh": [
                "扩散模型",
                "像素空间生成",
                "计算效率优化",
                "全局局部解耦",
                "轻量细节恢复"
            ],
            "_index": 133
        },
        {
            "title": "Disc3D: Automatic Curation of High-Quality 3D Dialog Data via Discriminative Object Referring",
            "authors": [
                "Siyuan Wei",
                "Chunjie Wang",
                "Xiao Liu",
                "Xiaosheng Yan",
                "Zhishan Zhou",
                "Rui Huang"
            ],
            "arxiv_id": "2511.18817v1",
            "summary": "3D Multi-modal Large Language Models (MLLMs) still lag behind their 2D peers, largely because large-scale, high-quality 3D scene-dialogue datasets remain scarce. Prior efforts hinge on expensive human annotation and leave two key ambiguities unresolved: viewpoint ambiguity, where spatial language presumes unknown camera poses, and object referring ambiguity, where non-exclusive descriptions blur the line between targets and distractors. We therefore present a fully automated pipeline that converts raw 3D scans into unambiguous, high-quality dialogue data at a fraction of the previous cost. By synergizing rule-based constraints with 2D MLLMs and LLMs, the pipeline enables controllable, scalable generation without human intervention. The pipeline comprises four stages: (1) meta-annotation collection harvesting object-, frame-, and scene-level captions, (2) scene graph construction with relation correction to capture proximal object relations, (3) discriminative object referring that generates exclusive and compact descriptions, and (4) multi-task data generation synthesizing diverse dialogues. Our pipeline systematically mitigates inherent flaws in source datasets and produces the final Disc3D dataset, over 2 million samples in 25K hybrid 3D scenes, spanning scene, view, and object captioning, visual grounding, and five object-centric QA tasks. Extensive experiments demonstrate that training with Disc3D yields consistent, significant improvements on both public benchmarks and our multifaceted Disc3D-QA tasks. Code, data, and models will be publicly available.",
            "headline_zh": "提出Disc3D自动管道以解决3D多模态大模型数据稀缺与歧义问题",
            "intro_zh": [
                "核心问题：3D多模态大模型因高质量对话数据稀缺和视角、对象指代歧义而落后于2D模型",
                "方法要点：结合规则约束与2D MLLMs/LLMs，自动生成无歧义、高质量3D场景对话数据",
                "实验或效果：在公共基准和Disc3D-QA任务上训练模型，实现一致显著性能提升"
            ],
            "tags_zh": [
                "3D多模态大模型",
                "自动数据生成",
                "对象指代消歧",
                "场景图构建",
                "多任务对话合成",
                "大规模数据集"
            ],
            "_index": 134
        },
        {
            "title": "SupLID: Geometrical Guidance for Out-of-Distribution Detection in Semantic Segmentation",
            "authors": [
                "Nimeshika Udayangani",
                "Sarah Erfani",
                "Christopher Leckie"
            ],
            "arxiv_id": "2511.18816v1",
            "summary": "Out-of-Distribution (OOD) detection in semantic segmentation aims to localize anomalous regions at the pixel level, advancing beyond traditional image-level OOD techniques to better suit real-world applications such as autonomous driving. Recent literature has successfully explored the adaptation of commonly used image-level OOD methods--primarily based on classifier-derived confidence scores (e.g., energy or entropy)--for this pixel-precise task. However, these methods inherit a set of limitations, including vulnerability to overconfidence. In this work, we introduce SupLID, a novel framework that effectively guides classifier-derived OOD scores by exploiting the geometrical structure of the underlying semantic space, particularly using Linear Intrinsic Dimensionality (LID). While LID effectively characterizes the local structure of high-dimensional data by analyzing distance distributions, its direct application at the pixel level remains challenging. To overcome this, SupLID constructs a geometrical coreset that captures the intrinsic structure of the in-distribution (ID) subspace. It then computes OOD scores at the superpixel level, enabling both efficient real-time inference and improved spatial smoothness. We demonstrate that geometrical cues derived from SupLID serve as a complementary signal to traditional classifier confidence, enhancing the model's ability to detect diverse OOD scenarios. Designed as a post-hoc scoring method, SupLID can be seamlessly integrated with any semantic segmentation classifier at deployment time. Our results demonstrate that SupLID significantly enhances existing classifier-based OOD scores, achieving state-of-the-art performance across key evaluation metrics, including AUR, FPR, and AUP. Code is available at https://github.com/hdnugit/SupLID.",
            "headline_zh": "提出SupLID框架，利用几何结构增强语义分割中的分布外检测",
            "intro_zh": [
                "核心问题：语义分割中像素级分布外检测易受分类器过度自信影响",
                "方法要点：构建几何核心集，在超像素级别计算分布外分数",
                "实验或效果：显著提升现有方法性能，在AUR等指标上达到最优"
            ],
            "tags_zh": [
                "语义分割",
                "分布外检测",
                "几何结构",
                "超像素处理",
                "后处理评分"
            ],
            "_index": 135
        },
        {
            "title": "DetAny4D: Detect Anything 4D Temporally in a Streaming RGB Video",
            "authors": [
                "Jiawei Hou",
                "Shenghao Zhang",
                "Can Wang",
                "Zheng Gu",
                "Yonggen Ling",
                "Taiping Zeng",
                "Xiangyang Xue",
                "Jingbo Zhang"
            ],
            "arxiv_id": "2511.18814v1",
            "summary": "Reliable 4D object detection, which refers to 3D object detection in streaming video, is crucial for perceiving and understanding the real world. Existing open-set 4D object detection methods typically make predictions on a frame-by-frame basis without modeling temporal consistency, or rely on complex multi-stage pipelines that are prone to error propagation across cascaded stages. Progress in this area has been hindered by the lack of large-scale datasets that capture continuous reliable 3D bounding box (b-box) annotations. To overcome these challenges, we first introduce DA4D, a large-scale 4D detection dataset containing over 280k sequences with high-quality b-box annotations collected under diverse conditions. Building on DA4D, we propose DetAny4D, an open-set end-to-end framework that predicts 3D b-boxes directly from sequential inputs. DetAny4D fuses multi-modal features from pre-trained foundational models and designs a geometry-aware spatiotemporal decoder to effectively capture both spatial and temporal dynamics. Furthermore, it adopts a multi-task learning architecture coupled with a dedicated training strategy to maintain global consistency across sequences of varying lengths. Extensive experiments show that DetAny4D achieves competitive detection accuracy and significantly improves temporal stability, effectively addressing long-standing issues of jitter and inconsistency in 4D object detection. Data and code will be released upon acceptance.",
            "headline_zh": "提出DetAny4D端到端框架以解决流视频中4D物体检测的时序一致性问题",
            "intro_zh": [
                "核心问题：现有4D检测方法缺乏时序建模，且依赖多阶段流程易导致误差传播",
                "方法要点：融合多模态特征，设计几何感知时空解码器，采用多任务学习策略",
                "实验或效果：在DA4D数据集上验证，检测精度高且显著提升时序稳定性"
            ],
            "tags_zh": [
                "4D物体检测",
                "流视频分析",
                "时空建模",
                "多模态融合",
                "端到端学习",
                "时序一致性"
            ],
            "_index": 136
        },
        {
            "title": "Mitigating Long-Tail Bias in HOI Detection via Adaptive Diversity Cache",
            "authors": [
                "Yuqiu Jiang",
                "Xiaozhen Qiao",
                "Tianyu Mei",
                "Haojian Huang",
                "Yifan Chen",
                "Ye Zheng",
                "Zhe Sun"
            ],
            "arxiv_id": "2511.18811v1",
            "summary": "Human-Object Interaction (HOI) detection is a fundamental task in computer vision, empowering machines to comprehend human-object relationships in diverse real-world scenarios. Recent advances in VLMs have significantly improved HOI detection by leveraging rich cross-modal representations. However, most existing VLM-based approaches rely heavily on additional training or prompt tuning, resulting in substantial computational overhead and limited scalability, particularly in long-tailed scenarios where rare interactions are severely underrepresented. In this paper, we propose the Adaptive Diversity Cache (ADC) module, a novel training-free and plug-and-play mechanism designed to mitigate long-tail bias in HOI detection. ADC constructs class-specific caches that accumulate high-confidence and diverse feature representations during inference. The method incorporates frequency-aware cache adaptation that favors rare categories and is designed to enable robust prediction calibration without requiring additional training or fine-tuning. Extensive experiments on HICO-DET and V-COCO datasets show that ADC consistently improves existing HOI detectors, achieving up to +8.57\\% mAP gain on rare categories and +4.39\\% on the full dataset, demonstrating its effectiveness in mitigating long-tail bias while preserving overall performance.",
            "headline_zh": "提出自适应多样性缓存以缓解HOI检测中的长尾偏差",
            "intro_zh": [
                "HOI检测中长尾场景下稀有交互样本不足，导致模型偏差",
                "ADC模块无需训练，构建类特定缓存并采用频率感知适应策略",
                "在HICO-DET和V-COCO数据集上，稀有类别mAP提升达8.57%"
            ],
            "tags_zh": [
                "人-物交互检测",
                "长尾分布",
                "自适应缓存",
                "训练无关方法",
                "特征多样性"
            ],
            "_index": 137
        },
        {
            "title": "MergeVLA: Cross-Skill Model Merging Toward a Generalist Vision-Language-Action Agent",
            "authors": [
                "Yuxia Fu",
                "Zhizhen Zhang",
                "Yuqi Zhang",
                "Zijian Wang",
                "Zi Huang",
                "Yadan Luo"
            ],
            "arxiv_id": "2511.18810v1",
            "summary": "Recent Vision-Language-Action (VLA) models reformulate vision-language models by tuning them with millions of robotic demonstrations. While they perform well when fine-tuned for a single embodiment or task family, extending them to multi-skill settings remains challenging: directly merging VLA experts trained on different tasks results in near-zero success rates. This raises a fundamental question: what prevents VLAs from mastering multiple skills within one model? With an empirical decomposition of learnable parameters during VLA fine-tuning, we identify two key sources of non-mergeability: (1) Finetuning drives LoRA adapters in the VLM backbone toward divergent, task-specific directions beyond the capacity of existing merging methods to unify. (2) Action experts develop inter-block dependencies through self-attention feedback, causing task information to spread across layers and preventing modular recombination. To address these challenges, we present MergeVLA, a merging-oriented VLA architecture that preserves mergeability by design. MergeVLA introduces sparsely activated LoRA adapters via task masks to retain consistent parameters and reduce irreconcilable conflicts in the VLM. Its action expert replaces self-attention with cross-attention-only blocks to keep specialization localized and composable. When the task is unknown, it uses a test-time task router to adaptively select the appropriate task mask and expert head from the initial observation, enabling unsupervised task inference. Across LIBERO, LIBERO-Plus, RoboTwin, and multi-task experiments on the real SO101 robotic arm, MergeVLA achieves performance comparable to or even exceeding individually finetuned experts, demonstrating robust generalization across tasks, embodiments, and environments.",
            "headline_zh": "提出MergeVLA架构以解决视觉-语言-动作模型多技能合并难题",
            "intro_zh": [
                "核心问题：VLA模型微调后参数分歧与自注意力依赖阻碍多技能合并",
                "方法要点：使用稀疏激活LoRA适配器和仅交叉注意力块保持参数一致性与模块化",
                "实验或效果：在多个数据集和真实机器人上实现与专家模型相当或更优性能"
            ],
            "tags_zh": [
                "模型合并",
                "视觉-语言-动作模型",
                "稀疏激活",
                "交叉注意力",
                "任务路由",
                "机器人学习"
            ],
            "_index": 138
        },
        {
            "title": "TPG-INR: Target Prior-Guided Implicit 3D CT Reconstruction for Enhanced Sparse-view Imaging",
            "authors": [
                "Qinglei Cao",
                "Ziyao Tang",
                "Xiaoqin Tang"
            ],
            "arxiv_id": "2511.18806v1",
            "summary": "X-ray imaging, based on penetration, enables detailed visualization of internal structures. Building on this capability, existing implicit 3D reconstruction methods have adapted the NeRF model and its variants for internal CT reconstruction. However, these approaches often neglect the significance of objects' anatomical priors for implicit learning, limiting both reconstruction precision and learning efficiency, particularly in ultra-sparse view scenarios. To address these challenges, we propose a novel 3D CT reconstruction framework that employs a 'target prior' derived from the object's projection data to enhance implicit learning. Our approach integrates positional and structural encoding to facilitate voxel-wise implicit reconstruction, utilizing the target prior to guide voxel sampling and enrich structural encoding. This dual strategy significantly boosts both learning efficiency and reconstruction quality. Additionally, we introduce a CUDA-based algorithm for rapid estimation of high-quality 3D target priors from sparse-view projections. Experiments utilizing projection data from a complex abdominal dataset demonstrate that the proposed model substantially enhances learning efficiency, outperforming the current leading model, NAF, by a factor of ten. In terms of reconstruction quality, it also exceeds the most accurate model, NeRP, achieving PSNR improvements of 3.57 dB, 5.42 dB, and 5.70 dB with 10, 20, and 30 projections, respectively. The code is available at https://github.com/qlcao171/TPG-INR.",
            "headline_zh": "提出目标先验引导隐式3D CT重建框架，提升稀疏视图成像效率与质量",
            "intro_zh": [
                "核心问题：现有隐式3D CT重建方法忽略解剖先验，导致稀疏视图下精度和效率不足",
                "方法要点：利用投影数据生成目标先验，结合位置和结构编码指导体素采样与重建",
                "实验或效果：在腹部数据集上，学习效率提升10倍，PSNR优于NeRP达3.57-5.70 dB"
            ],
            "tags_zh": [
                "CT重建",
                "隐式神经表示",
                "稀疏视图成像",
                "目标先验",
                "体素采样",
                "学习效率"
            ],
            "_index": 139
        },
        {
            "title": "PartDiffuser: Part-wise 3D Mesh Generation via Discrete Diffusion",
            "authors": [
                "Yichen Yang",
                "Hong Li",
                "Haodong Zhu",
                "Linin Yang",
                "Guojun Lei",
                "Sheng Xu",
                "Baochang Zhang"
            ],
            "arxiv_id": "2511.18801v1",
            "summary": "Existing autoregressive (AR) methods for generating artist-designed meshes struggle to balance global structural consistency with high-fidelity local details, and are susceptible to error accumulation. To address this, we propose PartDiffuser, a novel semi-autoregressive diffusion framework for point-cloud-to-mesh generation. The method first performs semantic segmentation on the mesh and then operates in a \"part-wise\" manner: it employs autoregression between parts to ensure global topology, while utilizing a parallel discrete diffusion process within each semantic part to precisely reconstruct high-frequency geometric features. PartDiffuser is based on the DiT architecture and introduces a part-aware cross-attention mechanism, using point clouds as hierarchical geometric conditioning to dynamically control the generation process, thereby effectively decoupling the global and local generation tasks. Experiments demonstrate that this method significantly outperforms state-of-the-art (SOTA) models in generating 3D meshes with rich detail, exhibiting exceptional detail representation suitable for real-world applications.",
            "headline_zh": "提出PartDiffuser以解决3D网格生成中全局结构与局部细节的平衡问题",
            "intro_zh": [
                "现有自回归方法难以平衡全局结构一致性与高保真局部细节，易产生误差累积",
                "采用半自回归扩散框架，部分间自回归确保拓扑，部分内并行扩散重建细节",
                "实验显示在生成细节丰富的3D网格方面显著优于SOTA模型，适合实际应用"
            ],
            "tags_zh": [
                "3D网格生成",
                "离散扩散",
                "半自回归框架",
                "语义分割",
                "点云条件生成"
            ],
            "_index": 140
        },
        {
            "title": "ChronoGS: Disentangling Invariants and Changes in Multi-Period Scenes",
            "authors": [
                "Zhongtao Wang",
                "Jiaqi Dai",
                "Qingtian Zhu",
                "Yilong Li",
                "Mai Su",
                "Fei Zhu",
                "Meng Gai",
                "Shaorong Wang",
                "Chengwei Pan",
                "Yisong Chen",
                "Guoping Wang"
            ],
            "arxiv_id": "2511.18794v1",
            "summary": "Multi-period image collections are common in real-world applications. Cities are re-scanned for mapping, construction sites are revisited for progress tracking, and natural regions are monitored for environmental change. Such data form multi-period scenes, where geometry and appearance evolve. Reconstructing such scenes is an important yet underexplored problem. Existing pipelines rely on incompatible assumptions: static and in-the-wild methods enforce a single geometry, while dynamic ones assume smooth motion, both failing under long-term, discontinuous changes. To solve this problem, we introduce ChronoGS, a temporally modulated Gaussian representation that reconstructs all periods within a unified anchor scaffold. It's also designed to disentangle stable and evolving components, achieving temporally consistent reconstruction of multi-period scenes. To catalyze relevant research, we release ChronoScene dataset, a benchmark of real and synthetic multi-period scenes, capturing geometric and appearance variation. Experiments demonstrate that ChronoGS consistently outperforms baselines in reconstruction quality and temporal consistency. Our code and the ChronoScene dataset are publicly available at https://github.com/ZhongtaoWang/ChronoGS.",
            "headline_zh": "提出ChronoGS以解决多时期场景重建中几何与外观变化的问题",
            "intro_zh": [
                "核心问题：多时期图像集合中几何和外观演化，现有方法在长期不连续变化下失效",
                "方法要点：使用时间调制高斯表示，在统一锚架中重建所有时期并解耦稳定与演化组件",
                "实验或效果：在ChronoScene数据集上，重建质量和时间一致性优于基线"
            ],
            "tags_zh": [
                "多时期场景重建",
                "时间调制高斯表示",
                "几何外观解耦",
                "ChronoScene数据集",
                "时间一致性"
            ],
            "_index": 141
        },
        {
            "title": "Scale What Counts, Mask What Matters: Evaluating Foundation Models for Zero-Shot Cross-Domain Wi-Fi Sensing",
            "authors": [
                "Cheng Jiang",
                "Yihe Yan",
                "Yanxiang Wang",
                "Chun Tung Chou",
                "Wen Hu"
            ],
            "arxiv_id": "2511.18792v1",
            "summary": "While Wi-Fi sensing offers a compelling, privacy-preserving alternative to cameras, its practical utility has been fundamentally undermined by a lack of robustness across domains. Models trained in one setup fail to generalize to new environments, hardware, or users, a critical \"domain shift\" problem exacerbated by modest, fragmented public datasets. We shift from this limited paradigm and apply a foundation model approach, leveraging Masked Autoencoding (MAE) style pretraining on the largest and most heterogeneous Wi-Fi CSI datasets collection assembled to date. Our study pretrains and evaluates models on over 1.3 million samples extracted from 14 datasets, collected using 4 distinct devices across the 2.4/5/6 GHz bands and bandwidths from 20 to 160 MHz. Our large-scale evaluation is the first to systematically disentangle the impacts of data diversity versus model capacity on cross-domain performance. The results establish scaling trends on Wi-Fi CSI sensing. First, our experiments show log-linear improvements in unseen domain performance as the amount of pretraining data increases, suggesting that data scale and diversity are key to domain generalization. Second, based on the current data volume, larger model can only provide marginal gains for cross-domain performance, indicating that data, rather than model capacity, is the current bottleneck for Wi-Fi sensing generalization. Finally, we conduct a series of cross-domain evaluations on human activity recognition, human gesture recognition and user identification tasks. The results show that the large-scale pretraining improves cross-domain accuracy ranging from 2.2% to 15.7%, compared to the supervised learning baseline. Overall, our findings provide insightful direction for designing future Wi-Fi sensing systems that can eventually be robust enough for real-world deployment.",
            "headline_zh": "应用基础模型与掩码自编码提升Wi-Fi感知的跨域零样本性能",
            "intro_zh": [
                "Wi-Fi感知面临域偏移问题，模型在新环境、硬件或用户下泛化能力差",
                "采用掩码自编码预训练，利用大规模异构数据集提升数据多样性和规模",
                "实验显示数据规模是关键瓶颈，预训练在跨域任务中提升准确率2.2%至15.7%"
            ],
            "tags_zh": [
                "Wi-Fi感知",
                "跨域泛化",
                "掩码自编码",
                "基础模型",
                "零样本学习",
                "信道状态信息"
            ],
            "_index": 142
        },
        {
            "title": "StereoDETR: Stereo-based Transformer for 3D Object Detection",
            "authors": [
                "Shiyi Mu",
                "Zichong Gu",
                "Zhiqi Ai",
                "Anqi Liu",
                "Yilin Gao",
                "Shugong Xu"
            ],
            "arxiv_id": "2511.18788v1",
            "summary": "Compared to monocular 3D object detection, stereo-based 3D methods offer significantly higher accuracy but still suffer from high computational overhead and latency. The state-of-the-art stereo 3D detection method achieves twice the accuracy of monocular approaches, yet its inference speed is only half as fast. In this paper, we propose StereoDETR, an efficient stereo 3D object detection framework based on DETR. StereoDETR consists of two branches: a monocular DETR branch and a stereo branch. The DETR branch is built upon 2D DETR with additional channels for predicting object scale, orientation, and sampling points. The stereo branch leverages low-cost multi-scale disparity features to predict object-level depth maps. These two branches are coupled solely through a differentiable depth sampling strategy. To handle occlusion, we introduce a constrained supervision strategy for sampling points without requiring extra annotations. StereoDETR achieves real-time inference and is the first stereo-based method to surpass monocular approaches in speed. It also achieves competitive accuracy on the public KITTI benchmark, setting new state-of-the-art results on pedestrian and cyclist subsets. The code is available at https://github.com/shiyi-mu/StereoDETR-OPEN.",
            "headline_zh": "提出StereoDETR以高效实现基于立体视觉的3D物体检测",
            "intro_zh": [
                "立体3D检测精度高但计算开销大，推理速度慢于单目方法",
                "结合单目DETR分支和立体分支，通过可微分深度采样耦合",
                "在KITTI基准上实现实时推理，精度竞争领先，速度超越单目方法"
            ],
            "tags_zh": [
                "立体视觉",
                "3D物体检测",
                "Transformer",
                "实时推理",
                "深度采样",
                "KITTI基准"
            ],
            "_index": 143
        },
        {
            "title": "Understanding Task Transfer in Vision-Language Models",
            "authors": [
                "Bhuvan Sachdeva",
                "Karan Uppal",
                "Abhinav Java",
                "Vineeth N. Balasubramanian"
            ],
            "arxiv_id": "2511.18787v1",
            "summary": "Vision-Language Models (VLMs) perform well on multimodal benchmarks but lag behind humans and specialized models on visual perception tasks like depth estimation or object counting. Finetuning on one task can unpredictably affect performance on others, making task-specific finetuning challenging. In this paper, we address this challenge through a systematic study of task transferability. We examine how finetuning a VLM on one perception task affects its zero-shot performance on others. To quantify these effects, we introduce Perfection Gap Factor (PGF), a metric that captures both the breadth and magnitude of transfer. Using three open-weight VLMs evaluated across 13 perception tasks, we construct a task-transfer graph that reveals previously unobserved relationships among perception tasks. Our analysis uncovers patterns of positive and negative transfer, identifies groups of tasks that mutually influence each other, organizes tasks into personas based on their transfer behavior and demonstrates how PGF can guide data selection for more efficient training. These findings highlight both opportunities for positive transfer and risks of negative interference, offering actionable guidance for advancing VLMs.",
            "headline_zh": "提出Perfection Gap Factor以量化视觉语言模型任务迁移性",
            "intro_zh": [
                "核心问题：视觉语言模型在视觉感知任务中表现不佳，微调后任务间性能影响不可预测",
                "方法要点：引入PGF指标，构建任务迁移图，分析正负迁移模式",
                "实验或效果：评估三个模型在13个任务上，揭示任务间关系，指导数据选择"
            ],
            "tags_zh": [
                "视觉语言模型",
                "任务迁移性",
                "Perfection Gap Factor",
                "正负迁移",
                "任务迁移图",
                "数据选择"
            ],
            "_index": 144
        },
        {
            "title": "STCDiT: Spatio-Temporally Consistent Diffusion Transformer for High-Quality Video Super-Resolution",
            "authors": [
                "Junyang Chen",
                "Jiangxin Dong",
                "Long Sun",
                "Yixin Yang",
                "Jinshan Pan"
            ],
            "arxiv_id": "2511.18786v1",
            "summary": "We present STCDiT, a video super-resolution framework built upon a pre-trained video diffusion model, aiming to restore structurally faithful and temporally stable videos from degraded inputs, even under complex camera motions. The main challenges lie in maintaining temporal stability during reconstruction and preserving structural fidelity during generation. To address these challenges, we first develop a motion-aware VAE reconstruction method that performs segment-wise reconstruction, with each segment clip exhibiting uniform motion characteristic, thereby effectively handling videos with complex camera motions. Moreover, we observe that the first-frame latent extracted by the VAE encoder in each clip, termed the anchor-frame latent, remains unaffected by temporal compression and retains richer spatial structural information than subsequent frame latents. We further develop an anchor-frame guidance approach that leverages structural information from anchor frames to constrain the generation process and improve structural fidelity of video features. Coupling these two designs enables the video diffusion model to achieve high-quality video super-resolution. Extensive experiments show that STCDiT outperforms state-of-the-art methods in terms of structural fidelity and temporal consistency.",
            "headline_zh": "提出STCDiT框架以解决复杂相机运动下视频超分辨率的时空一致性问题",
            "intro_zh": [
                "核心问题：视频超分辨率中保持时间稳定性和结构保真度，尤其在复杂相机运动场景下",
                "方法要点：采用运动感知VAE分段重建和锚帧引导，提升结构保真和生成稳定性",
                "实验或效果：广泛实验显示在结构保真和时间一致性上优于现有先进方法"
            ],
            "tags_zh": [
                "视频超分辨率",
                "扩散变换器",
                "时空一致性",
                "运动感知重建",
                "锚帧引导"
            ],
            "_index": 145
        },
        {
            "title": "A Novel Dual-Stream Framework for dMRI Tractography Streamline Classification with Joint dMRI and fMRI Data",
            "authors": [
                "Haotian Yan",
                "Bocheng Guo",
                "Jianzhong He",
                "Nir A. Sochen",
                "Ofer Pasternak",
                "Lauren J O'Donnell",
                "Fan Zhang"
            ],
            "arxiv_id": "2511.18781v1",
            "summary": "Streamline classification is essential to identify anatomically meaningful white matter tracts from diffusion MRI (dMRI) tractography. However, current streamline classification methods rely primarily on the geometric features of the streamline trajectory, failing to distinguish between functionally distinct fiber tracts with similar pathways. To address this, we introduce a novel dual-stream streamline classification framework that jointly analyzes dMRI and functional MRI (fMRI) data to enhance the functional coherence of tract parcellation. We design a novel network that performs streamline classification using a pretrained backbone model for full streamline trajectories, while augmenting with an auxiliary network that processes fMRI signals from fiber endpoint regions. We demonstrate our method by parcellating the corticospinal tract (CST) into its four somatotopic subdivisions. Experimental results from ablation studies and comparisons with state-of-the-art methods demonstrate our approach's superior performance.",
            "headline_zh": "提出双流框架联合dMRI和fMRI数据以增强白质束流线分类的功能一致性",
            "intro_zh": [
                "核心问题：现有方法依赖几何特征，无法区分功能不同但路径相似的纤维束",
                "方法要点：设计双流网络，主网络处理全流线轨迹，辅助网络处理fMRI端点信号",
                "实验或效果：在皮质脊髓束分区中，通过消融实验和对比显示性能优越"
            ],
            "tags_zh": [
                "扩散MRI",
                "功能MRI",
                "白质束分类",
                "双流网络",
                "流线分析"
            ],
            "_index": 146
        },
        {
            "title": "ConceptGuard: Proactive Safety in Text-and-Image-to-Video Generation through Multimodal Risk Detection",
            "authors": [
                "Ruize Ma",
                "Minghong Cai",
                "Yilei Jiang",
                "Jiaming Han",
                "Yi Feng",
                "Yingshui Tan",
                "Xiaoyong Zhu",
                "Bo Zhang",
                "Bo Zheng",
                "Xiangyu Yue"
            ],
            "arxiv_id": "2511.18780v1",
            "summary": "Recent progress in video generative models has enabled the creation of high-quality videos from multimodal prompts that combine text and images. While these systems offer enhanced controllability, they also introduce new safety risks, as harmful content can emerge from individual modalities or their interaction. Existing safety methods are often text-only, require prior knowledge of the risk category, or operate as post-generation auditors, struggling to proactively mitigate such compositional, multimodal risks. To address this challenge, we present ConceptGuard, a unified safeguard framework for proactively detecting and mitigating unsafe semantics in multimodal video generation. ConceptGuard operates in two stages: First, a contrastive detection module identifies latent safety risks by projecting fused image-text inputs into a structured concept space; Second, a semantic suppression mechanism steers the generative process away from unsafe concepts by intervening in the prompt's multimodal conditioning. To support the development and rigorous evaluation of this framework, we introduce two novel benchmarks: ConceptRisk, a large-scale dataset for training on multimodal risks, and T2VSafetyBench-TI2V, the first benchmark adapted from T2VSafetyBench for the Text-and-Image-to-Video (TI2V) safety setting. Comprehensive experiments on both benchmarks show that ConceptGuard consistently outperforms existing baselines, achieving state-of-the-art results in both risk detection and safe video generation.",
            "headline_zh": "提出ConceptGuard框架以主动检测和缓解多模态视频生成中的安全风险",
            "intro_zh": [
                "核心问题：多模态视频生成中文本和图像交互可能产生有害内容，现有方法难以主动应对。",
                "方法要点：采用对比检测模块识别潜在风险，并通过语义抑制机制干预生成过程。",
                "实验或效果：在ConceptRisk和T2VSafetyBench-TI2V基准上实现最优风险检测和安全生成效果。"
            ],
            "tags_zh": [
                "多模态视频生成",
                "安全风险检测",
                "对比学习",
                "语义抑制",
                "基准数据集",
                "主动安全框架"
            ],
            "_index": 147
        },
        {
            "title": "Rethinking Garment Conditioning in Diffusion-based Virtual Try-On",
            "authors": [
                "Kihyun Na",
                "Jinyoung Choi",
                "Injung Kim"
            ],
            "arxiv_id": "2511.18775v1",
            "summary": "Virtual Try-On (VTON) is the task of synthesizing an image of a person wearing a target garment, conditioned on a person image and a garment image. While diffusion-based VTON models featuring a Dual UNet architecture demonstrate superior fidelity compared to single UNet models, they incur substantial computational and memory overhead due to their heavy structure. In this study, through visualization analysis and theoretical analysis, we derived three hypotheses regarding the learning of context features to condition the denoising process. Based on these hypotheses, we developed Re-CatVTON, an efficient single UNet model that achieves high performance. We further enhance the model by introducing a modified classifier-free guidance strategy tailored for VTON's spatial concatenation conditioning, and by directly injecting the ground-truth garment latent derived from the clean garment latent to prevent the accumulation of prediction error. The proposed Re-CatVTON significantly improves performance compared to its predecessor (CatVTON) and requires less computation and memory than the high-performance Dual UNet model, Leffa. Our results demonstrate improved FID, KID, and LPIPS scores, with only a marginal decrease in SSIM, establishing a new efficiency-performance trade-off for single UNet VTON models.",
            "headline_zh": "提出Re-CatVTON以解决扩散式虚拟试衣中计算效率与性能的平衡问题",
            "intro_zh": [
                "核心问题：双UNet扩散模型在虚拟试衣中计算和内存开销大，影响效率。",
                "方法要点：基于假设开发单UNet模型，改进分类器自由引导和直接注入真实服装潜在。",
                "实验或效果：性能优于CatVTON，计算内存需求低于Leffa，FID等指标提升。"
            ],
            "tags_zh": [
                "虚拟试衣",
                "扩散模型",
                "单UNet架构",
                "计算效率优化",
                "分类器自由引导"
            ],
            "_index": 148
        },
        {
            "title": "Sampling Control for Imbalanced Calibration in Semi-Supervised Learning",
            "authors": [
                "Senmao Tian",
                "Xiang Wei",
                "Shunli Zhang"
            ],
            "arxiv_id": "2511.18773v1",
            "summary": "Class imbalance remains a critical challenge in semi-supervised learning (SSL), especially when distributional mismatches between labeled and unlabeled data lead to biased classification. Although existing methods address this issue by adjusting logits based on the estimated class distribution of unlabeled data, they often handle model imbalance in a coarse-grained manner, conflating data imbalance with bias arising from varying class-specific learning difficulties. To address this issue, we propose a unified framework, SC-SSL, which suppresses model bias through decoupled sampling control. During training, we identify the key variables for sampling control under ideal conditions. By introducing a classifier with explicit expansion capability and adaptively adjusting sampling probabilities across different data distributions, SC-SSL mitigates feature-level imbalance for minority classes. In the inference phase, we further analyze the weight imbalance of the linear classifier and apply post-hoc sampling control with an optimization bias vector to directly calibrate the logits. Extensive experiments across various benchmark datasets and distribution settings validate the consistency and state-of-the-art performance of SC-SSL.",
            "headline_zh": "提出SC-SSL框架，通过解耦采样控制解决半监督学习中的类别不平衡问题",
            "intro_zh": [
                "核心问题：类别不平衡与分布不匹配导致半监督学习分类偏差，现有方法处理粗糙",
                "方法要点：引入解耦采样控制，自适应调整采样概率，并应用优化偏置向量校准logits",
                "实验或效果：在多个基准数据集上验证一致性和先进性能"
            ],
            "tags_zh": [
                "半监督学习",
                "类别不平衡",
                "采样控制",
                "模型校准",
                "分布不匹配"
            ],
            "_index": 149
        },
        {
            "title": "Unsupervised Multi-View Visual Anomaly Detection via Progressive Homography-Guided Alignment",
            "authors": [
                "Xintao Chen",
                "Xiaohao Xu",
                "Bozhong Zheng",
                "Yun Liu",
                "Yingna Wu"
            ],
            "arxiv_id": "2511.18766v1",
            "summary": "Unsupervised visual anomaly detection from multi-view images presents a significant challenge: distinguishing genuine defects from benign appearance variations caused by viewpoint changes. Existing methods, often designed for single-view inputs, treat multiple views as a disconnected set of images, leading to inconsistent feature representations and a high false-positive rate. To address this, we introduce ViewSense-AD (VSAD), a novel framework that learns viewpoint-invariant representations by explicitly modeling geometric consistency across views. At its core is our Multi-View Alignment Module (MVAM), which leverages homography to project and align corresponding feature regions between neighboring views. We integrate MVAM into a View-Align Latent Diffusion Model (VALDM), enabling progressive and multi-stage alignment during the denoising process. This allows the model to build a coherent and holistic understanding of the object's surface from coarse to fine scales. Furthermore, a lightweight Fusion Refiner Module (FRM) enhances the global consistency of the aligned features, suppressing noise and improving discriminative power. Anomaly detection is performed by comparing multi-level features from the diffusion model against a learned memory bank of normal prototypes. Extensive experiments on the challenging RealIAD and MANTA datasets demonstrate that VSAD sets a new state-of-the-art, significantly outperforming existing methods in pixel, view, and sample-level visual anomaly proving its robustness to large viewpoint shifts and complex textures.",
            "headline_zh": "提出VSAD框架以解决多视角图像中异常检测的视角变化问题",
            "intro_zh": [
                "核心问题：多视角图像中视角变化导致良性外观变化与真实缺陷难以区分",
                "方法要点：使用同形变换对齐多视角特征，结合扩散模型进行渐进式对齐",
                "实验或效果：在RealIAD和MANTA数据集上实现新SOTA，显著降低误报率"
            ],
            "tags_zh": [
                "多视角异常检测",
                "同形变换对齐",
                "扩散模型",
                "特征一致性",
                "无监督学习"
            ],
            "_index": 150
        },
        {
            "title": "NI-Tex: Non-isometric Image-based Garment Texture Generation",
            "authors": [
                "Hui Shan",
                "Ming Li",
                "Haitao Yang",
                "Kai Zheng",
                "Sizhe Zheng",
                "Yanwei Fu",
                "Xiangru Huang"
            ],
            "arxiv_id": "2511.18765v1",
            "summary": "Existing industrial 3D garment meshes already cover most real-world clothing geometries, yet their texture diversity remains limited. To acquire more realistic textures, generative methods are often used to extract Physically-based Rendering (PBR) textures and materials from large collections of wild images and project them back onto garment meshes. However, most image-conditioned texture generation approaches require strict topological consistency between the input image and the input 3D mesh, or rely on accurate mesh deformation to match to the image poses, which significantly constrains the texture generation quality and flexibility. To address the challenging problem of non-isometric image-based garment texture generation, we construct 3D Garment Videos, a physically simulated, garment-centric dataset that provides consistent geometry and material supervision across diverse deformations, enabling robust cross-pose texture learning. We further employ Nano Banana for high-quality non-isometric image editing, achieving reliable cross-topology texture generation between non-isometric image-geometry pairs. Finally, we propose an iterative baking method via uncertainty-guided view selection and reweighting that fuses multi-view predictions into seamless, production-ready PBR textures. Through extensive experiments, we demonstrate that our feedforward dual-branch architecture generates versatile and spatially aligned PBR materials suitable for industry-level 3D garment design.",
            "headline_zh": "提出NI-Tex方法以解决非等距图像到3D服装纹理生成的挑战",
            "intro_zh": [
                "核心问题：现有方法需严格拓扑一致或精确变形，限制纹理生成质量与灵活性。",
                "方法要点：构建3D Garment Videos数据集，使用Nano Banana编辑，实现跨拓扑纹理生成。",
                "实验或效果：通过迭代烘焙生成无缝PBR纹理，适用于工业级3D服装设计。"
            ],
            "tags_zh": [
                "非等距纹理生成",
                "3D服装设计",
                "PBR纹理",
                "图像编辑",
                "多视图融合"
            ],
            "_index": 151
        },
        {
            "title": "VAOT: Vessel-Aware Optimal Transport for Retinal Fundus Enhancement",
            "authors": [
                "Xuanzhao Dong",
                "Wenhui Zhu",
                "Yujian Xiong",
                "Xiwen Chen",
                "Hao Wang",
                "Xin Li",
                "Jiajun Cheng",
                "Zhipeng Wang",
                "Shao Tang",
                "Oana Dumitrascu",
                "Yalin Wang"
            ],
            "arxiv_id": "2511.18763v1",
            "summary": "Color fundus photography (CFP) is central to diagnosing and monitoring retinal disease, yet its acquisition variability (e.g., illumination changes) often degrades image quality, which motivates robust enhancement methods. Unpaired enhancement pipelines are typically GAN-based, however, they can distort clinically critical vasculature, altering vessel topology and endpoint integrity. Motivated by these structural alterations, we propose Vessel-Aware Optimal Transport (\\textbf{VAOT}), a framework that combines an optimal-transport objective with two structure-preserving regularizers: (i) a skeleton-based loss to maintain global vascular connectivity and (ii) an endpoint-aware loss to stabilize local termini. These constraints guide learning in the unpaired setting, reducing noise while preserving vessel structure. Experimental results on synthetic degradation benchmark and downstream evaluations in vessel and lesion segmentation demonstrate the superiority of the proposed methods against several state-of-the art baselines. The code is available at https://github.com/Retinal-Research/VAOT",
            "headline_zh": "提出VAOT框架以解决视网膜眼底图像增强中血管结构失真问题",
            "intro_zh": [
                "核心问题：无配对增强方法易扭曲血管拓扑和端点完整性，影响临床诊断。",
                "方法要点：结合最优传输目标与骨架和端点感知正则化，保持血管结构。",
                "实验或效果：在合成退化基准和下游分割任务中优于现有方法，代码已开源。"
            ],
            "tags_zh": [
                "视网膜眼底增强",
                "最优传输",
                "血管结构保持",
                "无配对学习",
                "图像分割"
            ],
            "_index": 152
        },
        {
            "title": "From Features to Reference Points: Lightweight and Adaptive Fusion for Cooperative Autonomous Driving",
            "authors": [
                "Yongqi Zhu",
                "Morui Zhu",
                "Qi Chen",
                "Deyuan Qu",
                "Song Fu",
                "Qing Yang"
            ],
            "arxiv_id": "2511.18757v1",
            "summary": "We present RefPtsFusion, a lightweight and interpretable framework for cooperative autonomous driving. Instead of sharing large feature maps or query embeddings, vehicles exchange compact reference points, e.g., objects' positions, velocities, and size information. This approach shifts the focus from \"what is seen\" to \"where to see\", creating a sensor- and model-independent interface that works well across vehicles with heterogeneous perception models while greatly reducing communication bandwidth. To enhance the richness of shared information, we further develop a selective Top-K query fusion that selectively adds high-confidence queries from the sender. It thus achieves a strong balance between accuracy and communication cost. Experiments on the M3CAD dataset show that RefPtsFusion maintains stable perception performance while reducing communication overhead by five orders of magnitude, dropping from hundreds of MB/s to only a few KB/s at 5 FPS (frame per second), compared to traditional feature-level fusion methods. Extensive experiments also demonstrate RefPtsFusion's strong robustness and consistent transmission behavior, highlighting its potential for scalable, real-time cooperative driving systems.",
            "headline_zh": "提出RefPtsFusion框架，通过交换参考点实现轻量级协同自动驾驶。",
            "intro_zh": [
                "核心问题：传统协同感知方法通信开销大，难以适应异构车辆模型。",
                "方法要点：车辆交换对象位置等参考点，并采用选择性Top-K查询融合。",
                "实验效果：在M3CAD数据集上，通信开销降低五个数量级，性能稳定。"
            ],
            "tags_zh": [
                "协同自动驾驶",
                "轻量级融合",
                "参考点交换",
                "通信优化",
                "异构感知"
            ],
            "_index": 153
        },
        {
            "title": "SP-VINS: A Hybrid Stereo Visual Inertial Navigation System based on Implicit Environmental Map",
            "authors": [
                "Xueyu Du",
                "Lilian Zhang",
                "Fuan Duan",
                "Xincan Luo",
                "Maosong Wang",
                "Wenqi Wu",
                "JunMao"
            ],
            "arxiv_id": "2511.18756v1",
            "summary": "Filter-based visual inertial navigation system (VINS) has attracted mobile-robot researchers for the good balance between accuracy and efficiency, but its limited mapping quality hampers long-term high-accuracy state estimation. To this end, we first propose a novel filter-based stereo VINS, differing from traditional simultaneous localization and mapping (SLAM) systems based on 3D map, which performs efficient loop closure constraints with implicit environmental map composed of keyframes and 2D keypoints. Secondly, we proposed a hybrid residual filter framework that combines landmark reprojection and ray constraints to construct a unified Jacobian matrix for measurement updates. Finally, considering the degraded environment, we incorporated the camera-IMU extrinsic parameters into visual description to achieve online calibration. Benchmark experiments demonstrate that the proposed SP-VINS achieves high computational efficiency while maintaining long-term high-accuracy localization performance, and is superior to existing state-of-the-art (SOTA) methods.",
            "headline_zh": "提出SP-VINS混合立体视觉惯性导航系统，基于隐式环境地图解决长期高精度定位问题",
            "intro_zh": [
                "核心问题：基于滤波的VINS在长期高精度状态估计中受限于地图质量不足",
                "方法要点：结合地标重投影和射线约束的混合残差滤波框架，实现高效闭环",
                "实验效果：在基准测试中，SP-VINS在计算效率和定位精度上优于现有SOTA方法"
            ],
            "tags_zh": [
                "立体视觉惯性导航",
                "隐式环境地图",
                "混合残差滤波",
                "在线校准",
                "状态估计"
            ],
            "_index": 154
        },
        {
            "title": "Any4D: Open-Prompt 4D Generation from Natural Language and Images",
            "authors": [
                "Hao Li",
                "Qiao Sun"
            ],
            "arxiv_id": "2511.18746v1",
            "summary": "While video-generation-based embodied world models have gained increasing attention, their reliance on large-scale embodied interaction data remains a key bottleneck. The scarcity, difficulty of collection, and high dimensionality of embodied data fundamentally limit the alignment granularity between language and actions and exacerbate the challenge of long-horizon video generation--hindering generative models from achieving a \\textit{\"GPT moment\"} in the embodied domain. There is a naive observation: \\textit{the diversity of embodied data far exceeds the relatively small space of possible primitive motions}. Based on this insight, we propose \\textbf{Primitive Embodied World Models} (PEWM), which restricts video generation to fixed shorter horizons, our approach \\textit{1) enables} fine-grained alignment between linguistic concepts and visual representations of robotic actions, \\textit{2) reduces} learning complexity, \\textit{3) improves} data efficiency in embodied data collection, and \\textit{4) decreases} inference latency. By equipping with a modular Vision-Language Model (VLM) planner and a Start-Goal heatmap Guidance mechanism (SGG), PEWM further enables flexible closed-loop control and supports compositional generalization of primitive-level policies over extended, complex tasks. Our framework leverages the spatiotemporal vision priors in video models and the semantic awareness of VLMs to bridge the gap between fine-grained physical interaction and high-level reasoning, paving the way toward scalable, interpretable, and general-purpose embodied intelligence.",
            "headline_zh": "提出Primitive Embodied World Models以解决具身数据稀缺与长视频生成难题",
            "intro_zh": [
                "核心问题：具身数据稀缺、高维和收集困难，限制语言与动作的细粒度对齐和长视频生成。",
                "方法要点：限制视频生成为短时域，结合VLM规划器和SGG机制，实现细粒度对齐和闭环控制。",
                "实验或效果：提高数据效率、降低推理延迟，支持原始策略在复杂任务中的组合泛化。"
            ],
            "tags_zh": [
                "具身世界模型",
                "视频生成",
                "语言-动作对齐",
                "闭环控制",
                "数据效率",
                "推理优化"
            ],
            "_index": 155
        },
        {
            "title": "ProxT2I: Efficient Reward-Guided Text-to-Image Generation via Proximal Diffusion",
            "authors": [
                "Zhenghan Fang",
                "Jian Zheng",
                "Qiaozi Gao",
                "Xiaofeng Gao",
                "Jeremias Sulam"
            ],
            "arxiv_id": "2511.18742v1",
            "summary": "Diffusion models have emerged as a dominant paradigm for generative modeling across a wide range of domains, including prompt-conditional generation. The vast majority of samplers, however, rely on forward discretization of the reverse diffusion process and use score functions that are learned from data. Such forward and explicit discretizations can be slow and unstable, requiring a large number of sampling steps to produce good-quality samples. In this work we develop a text-to-image (T2I) diffusion model based on backward discretizations, dubbed ProxT2I, relying on learned and conditional proximal operators instead of score functions. We further leverage recent advances in reinforcement learning and policy optimization to optimize our samplers for task-specific rewards. Additionally, we develop a new large-scale and open-source dataset comprising 15 million high-quality human images with fine-grained captions, called LAION-Face-T2I-15M, for training and evaluation. Our approach consistently enhances sampling efficiency and human-preference alignment compared to score-based baselines, and achieves results on par with existing state-of-the-art and open-source text-to-image models while requiring lower compute and smaller model size, offering a lightweight yet performant solution for human text-to-image generation.",
            "headline_zh": "提出ProxT2I方法，通过近端扩散和奖励优化提升文本到图像生成效率与质量。",
            "intro_zh": [
                "核心问题：基于前向离散化的扩散模型采样慢且不稳定，影响生成质量。",
                "方法要点：使用后向离散化和条件近端算子替代分数函数，结合强化学习优化奖励。",
                "实验或效果：在采样效率和人类偏好对齐上优于基线，模型更轻量且性能相当。"
            ],
            "tags_zh": [
                "文本到图像生成",
                "扩散模型",
                "近端算子",
                "奖励优化",
                "高效采样",
                "轻量模型"
            ],
            "_index": 156
        },
        {
            "title": "Thinking Ahead: Foresight Intelligence in MLLMs and World Models",
            "authors": [
                "Zhantao Gong",
                "Liaoyuan Fan",
                "Qing Guo",
                "Xun Xu",
                "Xulei Yang",
                "Shijie Li"
            ],
            "arxiv_id": "2511.18735v1",
            "summary": "In this work, we define Foresight Intelligence as the capability to anticipate and interpret future events-an ability essential for applications such as autonomous driving, yet largely overlooked by existing research. To bridge this gap, we introduce FSU-QA, a new Visual Question-Answering (VQA) dataset specifically designed to elicit and evaluate Foresight Intelligence. Using FSU-QA, we conduct the first comprehensive study of state-of-the-art Vision-Language Models (VLMs) under foresight-oriented tasks, revealing that current models still struggle to reason about future situations. Beyond serving as a benchmark, FSU-QA also enables the assessment of world models by measuring the semantic coherence of their generated predictions, quantified through performance gains when VLMs are augmented with such outputs. Our experiments further demonstrate that FSU-QA can effectively enhance foresight reasoning: even small VLMs fine-tuned on FSU-QA surpass much larger, advanced models by a substantial margin. Together, these findings position FSU-QA as a principled foundation for developing next-generation models capable of truly anticipating and understanding future events.",
            "headline_zh": "提出FSU-QA数据集以评估和增强多模态大语言模型的预见智能",
            "intro_zh": [
                "核心问题：现有视觉语言模型在预见未来事件方面能力不足，影响自动驾驶等应用。",
                "方法要点：引入FSU-QA视觉问答数据集，专门设计用于激发和评估预见智能。",
                "实验或效果：微调小模型在FSU-QA上表现超越大模型，有效提升预见推理能力。"
            ],
            "tags_zh": [
                "预见智能",
                "视觉问答数据集",
                "多模态大语言模型",
                "世界模型评估",
                "自动驾驶应用"
            ],
            "_index": 157
        },
        {
            "title": "Yo'City: Personalized and Boundless 3D Realistic City Scene Generation via Self-Critic Expansion",
            "authors": [
                "Keyang Lu",
                "Sifan Zhou",
                "Hongbin Xu",
                "Gang Xu",
                "Zhifei Yang",
                "Yikai Wang",
                "Zhen Xiao",
                "Jieyi Long",
                "Ming Li"
            ],
            "arxiv_id": "2511.18734v1",
            "summary": "Realistic 3D city generation is fundamental to a wide range of applications, including virtual reality and digital twins. However, most existing methods rely on training a single diffusion model, which limits their ability to generate personalized and boundless city-scale scenes. In this paper, we present Yo'City, a novel agentic framework that enables user-customized and infinitely expandable 3D city generation by leveraging the reasoning and compositional capabilities of off-the-shelf large models. Specifically, Yo'City first conceptualize the city through a top-down planning strategy that defines a hierarchical \"City-District-Grid\" structure. The Global Planner determines the overall layout and potential functional districts, while the Local Designer further refines each district with detailed grid-level descriptions. Subsequently, the grid-level 3D generation is achieved through a \"produce-refine-evaluate\" isometric image synthesis loop, followed by image-to-3D generation. To simulate continuous city evolution, Yo'City further introduces a user-interactive, relationship-guided expansion mechanism, which performs scene graph-based distance- and semantics-aware layout optimization, ensuring spatially coherent city growth. To comprehensively evaluate our method, we construct a diverse benchmark dataset and design six multi-dimensional metrics that assess generation quality from the perspectives of semantics, geometry, texture, and layout. Extensive experiments demonstrate that Yo'City consistently outperforms existing state-of-the-art methods across all evaluation aspects.",
            "headline_zh": "提出Yo'City框架，实现个性化无限扩展的3D城市场景生成",
            "intro_zh": [
                "现有方法依赖单一扩散模型，难以生成个性化无限城市场景",
                "采用分层规划与图像合成循环，结合场景图优化实现城市扩展",
                "构建多维度评估基准，实验显示在语义、几何等方面优于现有方法"
            ],
            "tags_zh": [
                "3D城市生成",
                "个性化场景",
                "无限扩展",
                "分层规划",
                "图像合成",
                "场景图优化"
            ],
            "_index": 158
        },
        {
            "title": "GuideFlow: Constraint-Guided Flow Matching for Planning in End-to-End Autonomous Driving",
            "authors": [
                "Lin Liu",
                "Caiyan Jia",
                "Guanyi Yu",
                "Ziying Song",
                "JunQiao Li",
                "Feiyang Jia",
                "Peiliang Wu",
                "Xiaoshuai Hao",
                "Yandan Luo"
            ],
            "arxiv_id": "2511.18729v1",
            "summary": "Driving planning is a critical component of end-to-end (E2E) autonomous driving. However, prevailing Imitative E2E Planners often suffer from multimodal trajectory mode collapse, failing to produce diverse trajectory proposals. Meanwhile, Generative E2E Planners struggle to incorporate crucial safety and physical constraints directly into the generative process, necessitating an additional optimization stage to refine their outputs. In this paper, we propose \\textit{\\textbf{GuideFlow}}, a novel planning framework that leverages Constrained Flow Matching. Concretely, \\textit{\\textbf{GuideFlow}} explicitly models the flow matching process, which inherently mitigates mode collapse and allows for flexible guidance from various conditioning signals. Our core contribution lies in directly enforcing explicit constraints within the flow matching generation process, rather than relying on implicit constraint encoding. Crucially, \\textit{\\textbf{GuideFlow}} unifies the training of the flow matching with the Energy-Based Model (EBM) to enhance the model's autonomous optimization capability to robustly satisfy physical constraints. Secondly, \\textit{\\textbf{GuideFlow}} parameterizes driving aggressiveness as a control signal during generation, enabling precise manipulation of trajectory style. Extensive evaluations on major driving benchmarks (Bench2Drive, NuScenes, NavSim and ADV-NuScenes) validate the effectiveness of \\textit{\\textbf{GuideFlow}}. Notably, on the NavSim test hard split (Navhard), \\textit{\\textbf{GuideFlow}} achieved SOTA with an EPDMS score of 43.0. The code will be released.",
            "headline_zh": "提出GuideFlow框架，通过约束引导流匹配解决端到端自动驾驶规划中的多模态轨迹生成问题",
            "intro_zh": [
                "核心问题：模仿式端到端规划器易出现多模态轨迹模式坍塌，生成式规划器难以直接整合安全与物理约束",
                "方法要点：在流匹配生成过程中直接施加显式约束，结合能量模型增强自主优化能力",
                "实验或效果：在多个驾驶基准测试中验证有效性，NavSim测试中达到SOTA性能"
            ],
            "tags_zh": [
                "自动驾驶规划",
                "流匹配",
                "约束引导生成",
                "多模态轨迹",
                "端到端学习"
            ],
            "_index": 159
        },
        {
            "title": "Neural B-Frame Coding: Tackling Domain Shift Issues with Lightweight Online Motion Resolution Adaptation",
            "authors": [
                "Sang NguyenQuang",
                "Xiem HoangVan",
                "Wen-Hsiao Peng"
            ],
            "arxiv_id": "2511.18724v1",
            "summary": "Learned B-frame codecs with hierarchical temporal prediction often encounter the domain-shift issue due to mismatches between the Group-of-Pictures (GOP) sizes for training and testing, leading to inaccurate motion estimates, particularly for large motion. A common solution is to turn large motion into small motion by downsampling video frames during motion estimation. However, determining the optimal downsampling factor typically requires costly rate-distortion optimization. This work introduces lightweight classifiers to predict downsampling factors. These classifiers leverage simple state signals from current and reference frames to balance rate-distortion performance with computational cost. Three variants are proposed: (1) a binary classifier (Bi-Class) trained with Focal Loss to choose between high and low resolutions, (2) a multi-class classifier (Mu-Class) trained with novel soft labels based on rate-distortion costs, and (3) a co-class approach (Co-Class) that combines the predictive capability of the multi-class classifier with the selective search of the binary classifier. All classifier methods can work seamlessly with existing B-frame codecs without requiring codec retraining. Experimental results show that they achieve coding performance comparable to exhaustive search methods while significantly reducing computational complexity. The code is available at: https://github.com/NYCU-MAPL/Fast-OMRA.git.",
            "headline_zh": "提出轻量级分类器以解决B帧编码中域偏移问题，实现在线运动分辨率自适应。",
            "intro_zh": [
                "核心问题：B帧编码域偏移源于训练与测试GOP大小不匹配，导致大运动估计不准确。",
                "方法要点：设计二进制、多类和协同分类器，利用帧状态信号预测下采样因子。",
                "实验或效果：性能接近穷举搜索，显著降低计算复杂度，无需重新训练编解码器。"
            ],
            "tags_zh": [
                "B帧编码",
                "域偏移",
                "运动估计",
                "轻量级分类器",
                "分辨率自适应",
                "计算复杂度优化"
            ],
            "_index": 160
        },
        {
            "title": "Seeing What Matters: Visual Preference Policy Optimization for Visual Generation",
            "authors": [
                "Ziqi Ni",
                "Yuanzhi Liang",
                "Rui Li",
                "Yi Zhou",
                "Haibing Huang",
                "Chi Zhang",
                "Xuelong Li"
            ],
            "arxiv_id": "2511.18719v1",
            "summary": "Reinforcement learning (RL) has become a powerful tool for post-training visual generative models, with Group Relative Policy Optimization (GRPO) increasingly used to align generators with human preferences. However, existing GRPO pipelines rely on a single scalar reward per sample, treating each image or video as a holistic entity and ignoring the rich spatial and temporal structure of visual content. This coarse supervision hinders the correction of localized artifacts and the modeling of fine-grained perceptual cues. We introduce Visual Preference Policy Optimization (ViPO), a GRPO variant that lifts scalar feedback into structured, pixel-level advantages. ViPO employs a Perceptual Structuring Module that uses pretrained vision backbones to construct spatially and temporally aware advantage maps, redistributing optimization pressure toward perceptually important regions while preserving the stability of standard GRPO. Across both image and video benchmarks, ViPO consistently outperforms vanilla GRPO, improving in-domain alignment with human-preference rewards and enhancing generalization on out-of-domain evaluations. The method is architecture-agnostic, lightweight, and fully compatible with existing GRPO training pipelines, providing a more expressive and informative learning signal for visual generation.",
            "headline_zh": "提出ViPO方法以解决视觉生成中GRPO忽略空间结构的问题",
            "intro_zh": [
                "核心问题：GRPO依赖单标量奖励，忽略视觉内容的空间和时间结构",
                "方法要点：使用感知结构模块构建像素级优势图，优化重要区域",
                "实验或效果：在图像和视频基准上优于GRPO，提升对齐和泛化能力"
            ],
            "tags_zh": [
                "视觉生成",
                "强化学习",
                "策略优化",
                "像素级优势",
                "感知结构模块"
            ],
            "_index": 161
        },
        {
            "title": "AIRHILT: A Human-in-the-Loop Testbed for Multimodal Conflict Detection in Aviation",
            "authors": [
                "Omar Garib",
                "Jayaprakash D. Kambhampaty",
                "Olivia J. Pinon Fischer",
                "Dimitri N. Mavris"
            ],
            "arxiv_id": "2511.18718v1",
            "summary": "We introduce AIRHILT (Aviation Integrated Reasoning, Human-in-the-Loop Testbed), a modular and lightweight simulation environment designed to evaluate multimodal pilot and air traffic control (ATC) assistance systems for aviation conflict detection. Built on the open-source Godot engine, AIRHILT synchronizes pilot and ATC radio communications, visual scene understanding from camera streams, and ADS-B surveillance data within a unified, scalable platform. The environment supports pilot- and controller-in-the-loop interactions, providing a comprehensive scenario suite covering both terminal area and en route operational conflicts, including communication errors and procedural mistakes. AIRHILT offers standardized JSON-based interfaces that enable researchers to easily integrate, swap, and evaluate automatic speech recognition (ASR), visual detection, decision-making, and text-to-speech (TTS) models. We demonstrate AIRHILT through a reference pipeline incorporating fine-tuned Whisper ASR, YOLO-based visual detection, ADS-B-based conflict logic, and GPT-OSS-20B structured reasoning, and present preliminary results from representative runway-overlap scenarios, where the assistant achieves an average time-to-first-warning of approximately 7.7 s, with average ASR and vision latencies of approximately 5.9 s and 0.4 s, respectively. The AIRHILT environment and scenario suite are openly available, supporting reproducible research on multimodal situational awareness and conflict detection in aviation; code and scenarios are available at https://github.com/ogarib3/airhilt.",
            "headline_zh": "提出AIRHILT测试平台，用于评估航空多模态冲突检测系统",
            "intro_zh": [
                "核心问题：航空中多模态冲突检测缺乏统一评估环境，涉及通信和视觉数据整合。",
                "方法要点：基于Godot引擎构建模块化测试平台，支持人机交互和标准化接口集成。",
                "实验或效果：初步测试显示平均预警时间约7.7秒，ASR和视觉延迟分别为5.9秒和0.4秒。"
            ],
            "tags_zh": [
                "多模态冲突检测",
                "人机交互测试平台",
                "航空安全仿真",
                "标准化接口",
                "开源环境"
            ],
            "_index": 162
        },
        {
            "title": "GRIT-LP: Graph Transformer with Long-Range Skip Connection and Partitioned Spatial Graphs for Accurate Ice Layer Thickness Prediction",
            "authors": [
                "Zesheng Liu",
                "Maryam Rahnemoonfar"
            ],
            "arxiv_id": "2511.18716v1",
            "summary": "Graph transformers have demonstrated remarkable capability on complex spatio-temporal tasks, yet their depth is often limited by oversmoothing and weak long-range dependency modeling. To address these challenges, we introduce GRIT-LP, a graph transformer explicitly designed for polar ice-layer thickness estimation from polar radar imagery. Accurately estimating ice layer thickness is critical for understanding snow accumulation, reconstructing past climate patterns and reducing uncertainties in projections of future ice sheet evolution and sea level rise. GRIT-LP combines an inductive geometric graph learning framework with self-attention mechanism, and introduces two major innovations that jointly address challenges in modeling the spatio-temporal patterns of ice layers: a partitioned spatial graph construction strategy that forms overlapping, fully connected local neighborhoods to preserve spatial coherence and suppress noise from irrelevant long-range links, and a long-range skip connection mechanism within the transformer that improves information flow and mitigates oversmoothing in deeper attention layers. We conducted extensive experiments, demonstrating that GRIT-LP outperforms current state-of-the-art methods with a 24.92\\% improvement in root mean squared error. These results highlight the effectiveness of graph transformers in modeling spatiotemporal patterns by capturing both localized structural features and long-range dependencies across internal ice layers, and demonstrate their potential to advance data-driven understanding of cryospheric processes.",
            "headline_zh": "提出GRIT-LP图变换器，结合分区空间图与长程跳跃连接，以提升极地冰层厚度预测精度",
            "intro_zh": [
                "核心问题：图变换器在深度建模中易出现过平滑和长程依赖弱化，影响冰层厚度估计。",
                "方法要点：采用分区空间图构建策略和长程跳跃连接机制，增强空间一致性和信息流动。",
                "实验或效果：在根均方误差上优于现有方法24.92%，验证了模型在时空模式建模中的有效性。"
            ],
            "tags_zh": [
                "图变换器",
                "冰层厚度预测",
                "长程跳跃连接",
                "分区空间图",
                "时空模式建模",
                "极地雷达图像"
            ],
            "_index": 163
        },
        {
            "title": "DriveFlow: Rectified Flow Adaptation for Robust 3D Object Detection in Autonomous Driving",
            "authors": [
                "Hongbin Lin",
                "Yiming Yang",
                "Chaoda Zheng",
                "Yifan Zhang",
                "Shuaicheng Niu",
                "Zilu Guo",
                "Yafeng Li",
                "Gui Gui",
                "Shuguang Cui",
                "Zhen Li"
            ],
            "arxiv_id": "2511.18713v1",
            "summary": "In autonomous driving, vision-centric 3D object detection recognizes and localizes 3D objects from RGB images. However, due to high annotation costs and diverse outdoor scenes, training data often fails to cover all possible test scenarios, known as the out-of-distribution (OOD) issue. Training-free image editing offers a promising solution for improving model robustness by training data enhancement without any modifications to pre-trained diffusion models. Nevertheless, inversion-based methods often suffer from limited effectiveness and inherent inaccuracies, while recent rectified-flow-based approaches struggle to preserve objects with accurate 3D geometry. In this paper, we propose DriveFlow, a Rectified Flow Adaptation method for training data enhancement in autonomous driving based on pre-trained Text-to-Image flow models. Based on frequency decomposition, DriveFlow introduces two strategies to adapt noise-free editing paths derived from text-conditioned velocities. 1) High-Frequency Foreground Preservation: DriveFlow incorporates a high-frequency alignment loss for foreground to maintain precise 3D object geometry. 2) Dual-Frequency Background Optimization: DriveFlow also conducts dual-frequency optimization for background, balancing editing flexibility and semantic consistency. Comprehensive experiments validate the effectiveness and efficiency of DriveFlow, demonstrating comprehensive performance improvements on all categories across OOD scenarios. Code is available at https://github.com/Hongbin98/DriveFlow.",
            "headline_zh": "提出DriveFlow方法，通过整流流适应增强训练数据，以解决自动驾驶中3D物体检测的分布外鲁棒性问题。",
            "intro_zh": [
                "核心问题：自动驾驶视觉3D检测中，训练数据覆盖不足导致分布外场景性能下降。",
                "方法要点：基于频率分解，引入高频前景保持和双频背景优化策略，适配无噪声编辑路径。",
                "实验或效果：在分布外场景中，所有类别均实现全面性能提升，验证方法有效高效。"
            ],
            "tags_zh": [
                "自动驾驶",
                "3D物体检测",
                "整流流",
                "训练数据增强",
                "分布外鲁棒性",
                "频率分解"
            ],
            "_index": 164
        },
        {
            "title": "Head Stabilization for Wheeled Bipedal Robots via Force-Estimation-Based Admittance Control",
            "authors": [
                "Tianyu Wang",
                "Chunxiang Yan",
                "Xuanhong Liao",
                "Tao Zhang",
                "Ping Wang",
                "Cong Wen",
                "Dingchuan Liu",
                "Haowen Yu",
                "Ximin Lyu"
            ],
            "arxiv_id": "2511.18712v1",
            "summary": "Wheeled bipedal robots are emerging as flexible platforms for field exploration. However, head instability induced by uneven terrain can degrade the accuracy of onboard sensors or damage fragile payloads. Existing research primarily focuses on stabilizing the mobile platform but overlooks active stabilization of the head in the world frame, resulting in vertical oscillations that undermine overall stability. To address this challenge, we developed a model-based ground force estimation method for our 6-degree-of-freedom wheeled bipedal robot. Leveraging these force estimates, we implemented an admittance control algorithm to enhance terrain adaptability. Simulation experiments validated the real-time performance of the force estimator and the robot's robustness when traversing uneven terrain.",
            "headline_zh": "提出基于力估计的导纳控制以解决轮式双足机器人头部在不平地形中的稳定性问题",
            "intro_zh": [
                "核心问题：轮式双足机器人在不平地形中头部不稳定，影响传感器精度和载荷安全",
                "方法要点：开发基于模型的接地力估计方法，并应用导纳控制算法增强地形适应性",
                "实验或效果：仿真实验验证了力估计器的实时性能和机器人在不平地形中的鲁棒性"
            ],
            "tags_zh": [
                "轮式双足机器人",
                "头部稳定性",
                "力估计",
                "导纳控制",
                "地形适应性",
                "仿真实验"
            ],
            "_index": 165
        },
        {
            "title": "Modality-Collaborative Low-Rank Decomposers for Few-Shot Video Domain Adaptation",
            "authors": [
                "Yuyang Wanyan",
                "Xiaoshan Yang",
                "Weiming Dong",
                "Changsheng Xu"
            ],
            "arxiv_id": "2511.18711v1",
            "summary": "In this paper, we study the challenging task of Few-Shot Video Domain Adaptation (FSVDA). The multimodal nature of videos introduces unique challenges, necessitating the simultaneous consideration of both domain alignment and modality collaboration in a few-shot scenario, which is ignored in previous literature. We observe that, under the influence of domain shift, the generalization performance on the target domain of each individual modality, as well as that of fused multimodal features, is constrained. Because each modality is comprised of coupled features with multiple components that exhibit different domain shifts. This variability increases the complexity of domain adaptation, thereby reducing the effectiveness of multimodal feature integration. To address these challenges, we introduce a novel framework of Modality-Collaborative LowRank Decomposers (MC-LRD) to decompose modality-unique and modality-shared features with different domain shift levels from each modality that are more friendly for domain alignment. The MC-LRD comprises multiple decomposers for each modality and Multimodal Decomposition Routers (MDR). Each decomposer has progressively shared parameters across different modalities. The MDR is leveraged to selectively activate the decomposers to produce modality-unique and modality-shared features. To ensure efficient decomposition, we apply orthogonal decorrelation constraints separately to decomposers and subrouters, enhancing their diversity. Furthermore, we propose a cross-domain activation consistency loss to guarantee that target and source samples of the same category exhibit consistent activation preferences of the decomposers, thereby facilitating domain alignment. Extensive experimental results on three public benchmarks demonstrate that our model achieves significant improvements over existing methods.",
            "headline_zh": "提出模态协作低秩分解器以解决少样本视频域适应问题",
            "intro_zh": [
                "核心问题：视频多模态特征在少样本域适应中，域偏移影响单模态与融合特征的泛化性能。",
                "方法要点：使用低秩分解器分离模态独有与共享特征，并引入路由器和一致性损失优化对齐。",
                "实验或效果：在三个公开基准测试中，模型性能显著优于现有方法。"
            ],
            "tags_zh": [
                "少样本学习",
                "视频域适应",
                "多模态融合",
                "低秩分解",
                "域对齐",
                "特征分解"
            ],
            "_index": 166
        },
        {
            "title": "Autonomous Surface Selection For Manipulator-Based UV Disinfection In Hospitals Using Foundation Models",
            "authors": [
                "Xueyan Oh",
                "Jonathan Her",
                "Zhixiang Ong",
                "Brandon Koh",
                "Yun Hann Tan",
                "U-Xuan Tan"
            ],
            "arxiv_id": "2511.18709v1",
            "summary": "Ultraviolet (UV) germicidal radiation is an established non-contact method for surface disinfection in medical environments. Traditional approaches require substantial human intervention to define disinfection areas, complicating automation, while deep learning-based methods often need extensive fine-tuning and large datasets, which can be impractical for large-scale deployment. Additionally, these methods often do not address scene understanding for partial surface disinfection, which is crucial for avoiding unintended UV exposure. We propose a solution that leverages foundation models to simplify surface selection for manipulator-based UV disinfection, reducing human involvement and removing the need for model training. Additionally, we propose a VLM-assisted segmentation refinement to detect and exclude thin and small non-target objects, showing that this reduces mis-segmentation errors. Our approach achieves over 92\\% success rate in correctly segmenting target and non-target surfaces, and real-world experiments with a manipulator and simulated UV light demonstrate its practical potential for real-world applications.",
            "headline_zh": "提出基于基础模型的自主表面选择方法以简化医院机械臂UV消毒",
            "intro_zh": [
                "核心问题：传统UV消毒方法依赖人工定义消毒区域，自动化困难且缺乏部分表面理解。",
                "方法要点：利用基础模型简化表面选择，无需模型训练，并引入VLM辅助分割精炼。",
                "实验或效果：分割成功率超92%，真实实验验证了实际应用潜力。"
            ],
            "tags_zh": [
                "UV消毒",
                "基础模型",
                "表面分割",
                "机械臂控制",
                "视觉语言模型"
            ],
            "_index": 167
        },
        {
            "title": "GVD-TG: Topological Graph based on Fast Hierarchical GVD Sampling for Robot Exploration",
            "authors": [
                "Yanbin Li",
                "Canran Xiao",
                "Shenghai Yuan",
                "Peilai Yu",
                "Ziruo Li",
                "Zhiguo Zhang",
                "Wenzheng Chi",
                "Wei Zhang"
            ],
            "arxiv_id": "2511.18708v1",
            "summary": "Topological maps are more suitable than metric maps for robotic exploration tasks. However, real-time updating of accurate and detail-rich environmental topological maps remains a challenge. This paper presents a topological map updating method based on the Generalized Voronoi Diagram (GVD). First, the newly observed areas are denoised to avoid low-efficiency GVD nodes misleading the topological structure. Subsequently, a multi-granularity hierarchical GVD generation method is designed to control the sampling granularity at both global and local levels. This not only ensures the accuracy of the topological structure but also enhances the ability to capture detail features, reduces the probability of path backtracking, and ensures no overlap between GVDs through the maintenance of a coverage map, thereby improving GVD utilization efficiency. Second, a node clustering method with connectivity constraints and a connectivity method based on a switching mechanism are designed to avoid the generation of unreachable nodes and erroneous nodes caused by obstacle attraction. A special cache structure is used to store all connectivity information, thereby improving exploration efficiency. Finally, to address the issue of frontiers misjudgment caused by obstacles within the scope of GVD units, a frontiers extraction method based on morphological dilation is designed to effectively ensure the reachability of frontiers. On this basis, a lightweight cost function is used to assess and switch to the next viewpoint in real time. This allows the robot to quickly adjust its strategy when signs of path backtracking appear, thereby escaping the predicament and increasing exploration flexibility. And the performance of system for exploration task is verified through comparative tests with SOTA methods.",
            "headline_zh": "提出基于分层GVD采样的拓扑图方法以提升机器人探索效率",
            "intro_zh": [
                "核心问题：实时更新准确且细节丰富的环境拓扑图在机器人探索中仍具挑战",
                "方法要点：采用多粒度分层GVD生成、节点聚类与连通性约束，避免无效节点生成",
                "实验或效果：通过对比测试验证系统性能优于SOTA方法，提高探索灵活性"
            ],
            "tags_zh": [
                "机器人探索",
                "拓扑图",
                "广义Voronoi图",
                "分层采样",
                "连通性约束",
                "前沿提取"
            ],
            "_index": 168
        },
        {
            "title": "CoD: A Diffusion Foundation Model for Image Compression",
            "authors": [
                "Zhaoyang Jia",
                "Zihan Zheng",
                "Naifu Xue",
                "Jiahao Li",
                "Bin Li",
                "Zongyu Guo",
                "Xiaoyi Zhang",
                "Houqiang Li",
                "Yan Lu"
            ],
            "arxiv_id": "2511.18706v1",
            "summary": "Existing diffusion codecs typically build on text-to-image diffusion foundation models like Stable Diffusion. However, text conditioning is suboptimal from a compression perspective, hindering the potential of downstream diffusion codecs, particularly at ultra-low bitrates. To address it, we introduce \\textbf{CoD}, the first \\textbf{Co}mpression-oriented \\textbf{D}iffusion foundation model, trained from scratch to enable end-to-end optimization of both compression and generation. CoD is not a fixed codec but a general foundation model designed for various diffusion-based codecs. It offers several advantages: \\textbf{High compression efficiency}, replacing Stable Diffusion with CoD in downstream codecs like DiffC achieves SOTA results, especially at ultra-low bitrates (e.g., 0.0039 bpp); \\textbf{Low-cost and reproducible training}, 300$\\times$ faster training than Stable Diffusion ($\\sim$ 20 vs. $\\sim$ 6,250 A100 GPU days) on entirely open image-only datasets; \\textbf{Providing new insights}, e.g., We find pixel-space diffusion can achieve VTM-level PSNR with high perceptual quality and can outperform GAN-based codecs using fewer parameters. We hope CoD lays the foundation for future diffusion codec research. Codes will be released.",
            "headline_zh": "提出CoD扩散基础模型以优化图像压缩，提升超低码率性能",
            "intro_zh": [
                "现有扩散编解码器依赖文本条件，压缩效率受限，尤其在超低码率下",
                "CoD从零训练，专为压缩设计，支持端到端优化，训练成本低且可复现",
                "实验显示CoD在超低码率下达到SOTA，像素扩散可媲美VTM并优于GAN"
            ],
            "tags_zh": [
                "图像压缩",
                "扩散模型",
                "基础模型",
                "超低码率",
                "端到端优化"
            ],
            "_index": 169
        },
        {
            "title": "Asynchronous Distributed Multi-Robot Motion Planning Under Imperfect Communication",
            "authors": [
                "Ardalan Tajbakhsh",
                "Augustinos Saravanos",
                "James Zhu",
                "Evangelos A. Theodorou",
                "Lorenz T. Biegler",
                "Aaron M. Johnson"
            ],
            "arxiv_id": "2511.18703v1",
            "summary": "This paper addresses the challenge of coordinating multi-robot systems under realistic communication delays using distributed optimization. We focus on consensus ADMM as a scalable framework for generating collision-free, dynamically feasible motion plans in both trajectory optimization and receding-horizon control settings. In practice, however, these algorithms are sensitive to penalty tuning or adaptation schemes (e.g. residual balancing and adaptive parameter heuristics) that do not explicitly consider delays. To address this, we introduce a Delay-Aware ADMM (DA-ADMM) variant that adapts penalty parameters based on real-time delay statistics, allowing agents to down-weight stale information and prioritize recent updates during consensus and dual updates. Through extensive simulations in 2D and 3D environments with double-integrator, Dubins-car, and drone dynamics, we show that DA-ADMM significantly improves robustness, success rate, and solution quality compared to fixed-parameter, residual-balancing, and fixed-constraint baselines. Our results highlight that performance degradation is not solely determined by delay length or frequency, but by the optimizer's ability to contextually reason over delayed information. The proposed DA-ADMM achieves consistently better coordination performance across a wide range of delay conditions, offering a principled and efficient mechanism for resilient multi-robot motion planning under imperfect communication.",
            "headline_zh": "提出延迟感知ADMM以提升多机器人运动规划在通信延迟下的鲁棒性",
            "intro_zh": [
                "核心问题：多机器人系统在通信延迟下协调运动，现有方法对延迟敏感",
                "方法要点：引入DA-ADMM，基于实时延迟统计自适应调整惩罚参数",
                "实验或效果：在多种动态模型中，DA-ADMM显著提高成功率和解质量"
            ],
            "tags_zh": [
                "多机器人系统",
                "分布式优化",
                "运动规划",
                "通信延迟",
                "ADMM算法",
                "鲁棒控制"
            ],
            "_index": 170
        },
        {
            "title": "CNN-Based Camera Pose Estimation and Localisation of Scan Images for Aircraft Visual Inspection",
            "authors": [
                "Xueyan Oh",
                "Leonard Loh",
                "Shaohui Foong",
                "Zhong Bao Andy Koh",
                "Kow Leong Ng",
                "Poh Kang Tan",
                "Pei Lin Pearlin Toh",
                "U-Xuan Tan"
            ],
            "arxiv_id": "2511.18702v1",
            "summary": "General Visual Inspection is a manual inspection process regularly used to detect and localise obvious damage on the exterior of commercial aircraft. There has been increasing demand to perform this process at the boarding gate to minimise the downtime of the aircraft and automating this process is desired to reduce the reliance on human labour. Automating this typically requires estimating a camera's pose with respect to the aircraft for initialisation but most existing localisation methods require infrastructure, which is very challenging in uncontrolled outdoor environments and within the limited turnover time (approximately 2 hours) on an airport tarmac. Additionally, many airlines and airports do not allow contact with the aircraft's surface or using UAVs for inspection between flights, and restrict access to commercial aircraft. Hence, this paper proposes an on-site method that is infrastructure-free and easy to deploy for estimating a pan-tilt-zoom camera's pose and localising scan images. This method initialises using the same pan-tilt-zoom camera used for the inspection task by utilising a Deep Convolutional Neural Network fine-tuned on only synthetic images to predict its own pose. We apply domain randomisation to generate the dataset for fine-tuning the network and modify its loss function by leveraging aircraft geometry to improve accuracy. We also propose a workflow for initialisation, scan path planning, and precise localisation of images captured from a pan-tilt-zoom camera. We evaluate and demonstrate our approach through experiments with real aircraft, achieving root-mean-square camera pose estimation errors of less than 0.24 m and 2 degrees for all real scenes.",
            "headline_zh": "提出基于CNN的相机姿态估计方法，用于飞机视觉检查中的图像定位。",
            "intro_zh": [
                "核心问题：飞机视觉检查中，无基础设施的相机姿态估计在户外环境受限。",
                "方法要点：使用合成图像微调CNN，结合几何损失函数预测相机姿态。",
                "实验或效果：真实飞机实验显示姿态估计误差小于0.24米和2度。"
            ],
            "tags_zh": [
                "相机姿态估计",
                "卷积神经网络",
                "飞机视觉检查",
                "图像定位",
                "合成数据训练"
            ],
            "_index": 171
        },
        {
            "title": "ObjectAlign: Neuro-Symbolic Object Consistency Verification and Correction",
            "authors": [
                "Mustafa Munir",
                "Harsh Goel",
                "Xiwen Wei",
                "Minkyu Choi",
                "Sahil Shah",
                "Kartikeya Bhardwaj",
                "Paul Whatmough",
                "Sandeep Chinchali",
                "Radu Marculescu"
            ],
            "arxiv_id": "2511.18701v1",
            "summary": "Video editing and synthesis often introduce object inconsistencies, such as frame flicker and identity drift that degrade perceptual quality. To address these issues, we introduce ObjectAlign, a novel framework that seamlessly blends perceptual metrics with symbolic reasoning to detect, verify, and correct object-level and temporal inconsistencies in edited video sequences. The novel contributions of ObjectAlign are as follows: First, we propose learnable thresholds for metrics characterizing object consistency (i.e. CLIP-based semantic similarity, LPIPS perceptual distance, histogram correlation, and SAM-derived object-mask IoU). Second, we introduce a neuro-symbolic verifier that combines two components: (a) a formal, SMT-based check that operates on masked object embeddings to provably guarantee that object identity does not drift, and (b) a temporal fidelity check that uses a probabilistic model checker to verify the video's formal representation against a temporal logic specification. A frame transition is subsequently deemed \"consistent\" based on a single logical assertion that requires satisfying both the learned metric thresholds and this unified neuro-symbolic constraint, ensuring both low-level stability and high-level temporal correctness. Finally, for each contiguous block of flagged frames, we propose a neural network based interpolation for adaptive frame repair, dynamically choosing the interpolation depth based on the number of frames to be corrected. This enables reconstruction of the corrupted frames from the last valid and next valid keyframes. Our results show up to 1.4 point improvement in CLIP Score and up to 6.1 point improvement in warp error compared to SOTA baselines on the DAVIS and Pexels video datasets.",
            "headline_zh": "提出ObjectAlign框架，通过神经符号方法检测和修正视频编辑中的对象不一致问题。",
            "intro_zh": [
                "视频编辑常导致对象闪烁和身份漂移，降低感知质量。",
                "结合可学习指标阈值与神经符号验证器，确保对象一致性和时间保真度。",
                "在DAVIS和Pexels数据集上，CLIP分数和warp误差显著优于基线方法。"
            ],
            "tags_zh": [
                "视频编辑一致性",
                "神经符号验证",
                "对象检测修正",
                "时间逻辑规范",
                "自适应帧修复"
            ],
            "_index": 172
        },
        {
            "title": "Dendritic Convolution for Noise Image Recognition",
            "authors": [
                "Jiarui Xue",
                "Dongjian Yang",
                "Ye Sun",
                "Gang Liu"
            ],
            "arxiv_id": "2511.18699v1",
            "summary": "In real-world scenarios of image recognition, there exists substantial noise interference. Existing works primarily focus on methods such as adjusting networks or training strategies to address noisy image recognition, and the anti-noise performance has reached a bottleneck. However, little is known about the exploration of anti-interference solutions from a neuronal perspective.This paper proposes an anti-noise neuronal convolution. This convolution mimics the dendritic structure of neurons, integrates the neighborhood interaction computation logic of dendrites into the underlying design of convolutional operations, and simulates the XOR logic preprocessing function of biological dendrites through nonlinear interactions between input features, thereby fundamentally reconstructing the mathematical paradigm of feature extraction. Unlike traditional convolution where noise directly interferes with feature extraction and exerts a significant impact, DDC mitigates the influence of noise by focusing on the interaction of neighborhood information. Experimental results demonstrate that in image classification tasks (using YOLOv11-cls, VGG16, and EfficientNet-B0) and object detection tasks (using YOLOv11, YOLOv8, and YOLOv5), after replacing traditional convolution with the dendritic convolution, the accuracy of the EfficientNet-B0 model on noisy datasets is relatively improved by 11.23%, and the mean Average Precision (mAP) of YOLOv8 is increased by 19.80%. The consistency between the computation method of this convolution and the dendrites of biological neurons enables it to perform significantly better than traditional convolution in complex noisy environments.",
            "headline_zh": "提出树突卷积以解决噪声图像识别问题",
            "intro_zh": [
                "图像识别中噪声干扰严重，现有方法抗噪性能已达瓶颈",
                "模仿神经元树突结构，通过非线性交互重构特征提取数学范式",
                "在分类和检测任务中，准确率和mAP分别提升11.23%和19.80%"
            ],
            "tags_zh": [
                "树突卷积",
                "噪声图像识别",
                "特征提取",
                "生物启发计算",
                "图像分类",
                "目标检测"
            ],
            "_index": 173
        },
        {
            "title": "Multimodal Real-Time Anomaly Detection and Industrial Applications",
            "authors": [
                "Aman Verma",
                "Keshav Samdani",
                "Mohd. Samiuddin Shafi"
            ],
            "arxiv_id": "2511.18698v1",
            "summary": "This paper presents the design, implementation, and evolution of a comprehensive multimodal room-monitoring system that integrates synchronized video and audio processing for real-time activity recognition and anomaly detection. We describe two iterations of the system: an initial lightweight implementation using YOLOv8, ByteTrack, and the Audio Spectrogram Transformer (AST), and an advanced version that incorporates multi-model audio ensembles, hybrid object detection, bidirectional cross-modal attention, and multi-method anomaly detection. The evolution demonstrates significant improvements in accuracy, robustness, and industrial applicability. The advanced system combines three audio models (AST, Wav2Vec2, and HuBERT) for comprehensive audio understanding, dual object detectors (YOLO and DETR) for improved accuracy, and sophisticated fusion mechanisms for enhanced cross-modal learning. Experimental evaluation shows the system's effectiveness in general monitoring scenarios as well as specialized industrial safety applications, achieving real-time performance on standard hardware while maintaining high accuracy.",
            "headline_zh": "提出多模态实时异常检测系统，用于工业安全与通用监控场景。",
            "intro_zh": [
                "核心问题：实时多模态活动识别与异常检测在工业应用中的准确性和鲁棒性需求。",
                "方法要点：集成视频和音频处理，采用双向跨模态注意力和多模型融合提升性能。",
                "实验或效果：在标准硬件上实现高精度实时检测，适用于工业安全场景。"
            ],
            "tags_zh": [
                "多模态异常检测",
                "实时视频处理",
                "音频识别",
                "工业安全监控",
                "跨模态融合"
            ],
            "_index": 174
        },
        {
            "title": "Exploring Surround-View Fisheye Camera 3D Object Detection",
            "authors": [
                "Changcai Li",
                "Wenwei Lin",
                "Zuoxun Hou",
                "Gang Chen",
                "Wei Zhang",
                "Huihui Zhou",
                "Weishi Zheng"
            ],
            "arxiv_id": "2511.18695v1",
            "summary": "In this work, we explore the technical feasibility of implementing end-to-end 3D object detection (3DOD) with surround-view fisheye camera system. Specifically, we first investigate the performance drop incurred when transferring classic pinhole-based 3D object detectors to fisheye imagery. To mitigate this, we then develop two methods that incorporate the unique geometry of fisheye images into mainstream detection frameworks: one based on the bird's-eye-view (BEV) paradigm, named FisheyeBEVDet, and the other on the query-based paradigm, named FisheyePETR. Both methods adopt spherical spatial representations to effectively capture fisheye geometry. In light of the lack of dedicated evaluation benchmarks, we release Fisheye3DOD, a new open dataset synthesized using CARLA and featuring both standard pinhole and fisheye camera arrays. Experiments on Fisheye3DOD show that our fisheye-compatible modeling improves accuracy by up to 6.2% over baseline methods.",
            "headline_zh": "提出FisheyeBEVDet和FisheyePETR方法，提升环视鱼眼相机3D目标检测精度",
            "intro_zh": [
                "核心问题：经典针孔相机3D检测器在鱼眼图像上性能下降",
                "方法要点：采用球面空间表示整合鱼眼几何到BEV和查询框架",
                "实验效果：在Fisheye3DOD数据集上精度提升最高6.2%"
            ],
            "tags_zh": [
                "鱼眼相机3D检测",
                "环视系统",
                "球面表示",
                "BEV检测",
                "查询检测",
                "合成数据集"
            ],
            "_index": 175
        },
        {
            "title": "Stable Multi-Drone GNSS Tracking System for Marine Robots",
            "authors": [
                "Shuo Wen",
                "Edwin Meriaux",
                "Mariana Sosa Guzmán",
                "Zhizun Wang",
                "Junming Shi",
                "Gregory Dudek"
            ],
            "arxiv_id": "2511.18694v1",
            "summary": "Accurate localization is essential for marine robotics, yet Global Navigation Satellite System (GNSS) signals are unreliable or unavailable even at a very short distance below the water surface. Traditional alternatives, such as inertial navigation, Doppler Velocity Loggers (DVL), SLAM, and acoustic methods, suffer from error accumulation, high computational demands, or infrastructure dependence. In this work, we present a scalable multi-drone GNSS-based tracking system for surface and near-surface marine robots. Our approach combines efficient visual detection, lightweight multi-object tracking, GNSS-based triangulation, and a confidence-weighted Extended Kalman Filter (EKF) to provide stable GNSS estimation in real time. We further introduce a cross-drone tracking ID alignment algorithm that enforces global consistency across views, enabling robust multi-robot tracking with redundant aerial coverage. We validate our system in diversified complex settings to show the scalability and robustness of the proposed algorithm.",
            "headline_zh": "提出多无人机GNSS跟踪系统以解决海洋机器人水下定位不可靠问题",
            "intro_zh": [
                "核心问题：GNSS信号在水下不可靠，传统方法存在误差累积或依赖基础设施",
                "方法要点：结合视觉检测、多目标跟踪、GNSS三角定位和置信度加权EKF",
                "实验或效果：在多样化复杂环境中验证了系统的可扩展性和鲁棒性"
            ],
            "tags_zh": [
                "海洋机器人定位",
                "多无人机跟踪",
                "GNSS三角定位",
                "扩展卡尔曼滤波",
                "视觉检测",
                "跨无人机ID对齐"
            ],
            "_index": 176
        },
        {
            "title": "VLM in a flash: I/O-Efficient Sparsification of Vision-Language Model via Neuron Chunking",
            "authors": [
                "Kichang Yang",
                "Seonjun Kim",
                "Minjae Kim",
                "Nairan Zhang",
                "Chi Zhang",
                "Youngki Lee"
            ],
            "arxiv_id": "2511.18692v1",
            "summary": "Edge deployment of large Vision-Language Models (VLMs) increasingly relies on flash-based weight offloading, where activation sparsification is used to reduce I/O overhead. However, conventional sparsification remains model-centric, selecting neurons solely by activation magnitude and neglecting how access patterns influence flash performance. We present Neuron Chunking, an I/O-efficient sparsification strategy that operates on chunks (i.e., groups of contiguous neurons in memory) and couples neuron importance with storage access cost. The method models I/O latency through a lightweight abstraction of access contiguity and selects chunks with high utility, defined as neuron importance normalized by estimated latency. By aligning sparsification decisions with the underlying storage behavior, Neuron Chunking improves I/O efficiency by up to 4.65x and 5.76x on Jetson Orin Nano and Jetson AGX Orin, respectively.",
            "headline_zh": "提出神经元分块方法以优化视觉语言模型在边缘设备上的I/O效率",
            "intro_zh": [
                "核心问题：传统激活稀疏化仅基于神经元重要性，忽略存储访问模式对闪存性能的影响。",
                "方法要点：通过分块操作，结合神经元重要性和访问连续性，选择高效用块以减少I/O开销。",
                "实验或效果：在Jetson设备上，I/O效率提升最高达4.65倍和5.76倍。"
            ],
            "tags_zh": [
                "视觉语言模型",
                "I/O优化",
                "激活稀疏化",
                "边缘计算",
                "神经元分块"
            ],
            "_index": 177
        },
        {
            "title": "EVCC: Enhanced Vision Transformer-ConvNeXt-CoAtNet Fusion for Classification",
            "authors": [
                "Kazi Reyazul Hasan",
                "Md Nafiu Rahman",
                "Wasif Jalal",
                "Sadif Ahmed",
                "Shahriar Raj",
                "Mubasshira Musarrat",
                "Muhammad Abdullah Adnan"
            ],
            "arxiv_id": "2511.18691v1",
            "summary": "Hybrid vision architectures combining Transformers and CNNs have significantly advanced image classification, but they usually do so at significant computational cost. We introduce EVCC (Enhanced Vision Transformer-ConvNeXt-CoAtNet), a novel multi-branch architecture integrating the Vision Transformer, lightweight ConvNeXt, and CoAtNet through key innovations: (1) adaptive token pruning with information preservation, (2) gated bidirectional cross-attention for enhanced feature refinement, (3) auxiliary classification heads for multi-task learning, and (4) a dynamic router gate employing context-aware confidence-driven weighting. Experiments across the CIFAR-100, Tobacco3482, CelebA, and Brain Cancer datasets demonstrate EVCC's superiority over powerful models like DeiT-Base, MaxViT-Base, and CrossViT-Base by consistently achieving state-of-the-art accuracy with improvements of up to 2 percentage points, while reducing FLOPs by 25 to 35%. Our adaptive architecture adjusts computational demands to deployment needs by dynamically reducing token count, efficiently balancing the accuracy-efficiency trade-off while combining global context, local details, and hierarchical features for real-world applications. The source code of our implementation is available at https://anonymous.4open.science/r/EVCC.",
            "headline_zh": "提出EVCC融合架构以高效结合Transformer与CNN，提升图像分类精度并降低计算成本。",
            "intro_zh": [
                "混合视觉架构计算成本高，难以平衡精度与效率。",
                "集成ViT、ConvNeXt和CoAtNet，采用自适应令牌剪枝和门控交叉注意力。",
                "在多个数据集上实现SOTA精度，FLOPs减少25-35%，提升达2个百分点。"
            ],
            "tags_zh": [
                "图像分类",
                "混合架构",
                "自适应令牌剪枝",
                "门控交叉注意力",
                "多任务学习",
                "计算效率"
            ],
            "_index": 178
        },
        {
            "title": "Beyond Description: Cognitively Benchmarking Fine-Grained Action for Embodied Agents",
            "authors": [
                "Dayong Liu",
                "Chao Xu",
                "Weihong Chen",
                "Suyu Zhang",
                "Juncheng Wang",
                "Jiankang Deng",
                "Baigui Sun",
                "Yang Liu"
            ],
            "arxiv_id": "2511.18685v1",
            "summary": "Multimodal Large Language Models (MLLMs) show promising results as decision-making engines for embodied agents operating in complex, physical environments. However, existing benchmarks often prioritize high-level planning or spatial reasoning, leaving the fine-grained action intelligence required for embodied physical interaction underexplored. To address this gap, we introduce CFG-Bench, a new benchmark designed to systematically evaluate this crucial capability. CFG-Bench consists of 1,368 curated videos paired with 19,562 three-modalities question-answer pairs targeting four cognitive abilities: 1) Physical Interaction, 2) Temporal-Causal Relation, 3) Intentional Understanding, and 4) Evaluative Judgment. Together, these dimensions provide a systematic framework for assessing a model's ability to translate visual observations into actionable knowledge, moving beyond mere surface-level recognition. Our comprehensive evaluation on CFG-Bench reveals that leading MLLMs struggle to produce detailed instructions for physical interactions and exhibit profound limitations in the higher-order reasoning of intention and evaluation. Moreover, supervised fine-tuning (SFT) on our data demonstrates that teaching an MLLMs to articulate fine-grained actions directly translates to significant performance gains on established embodied benchmarks. Our analysis highlights these limitations and offers insights for developing more capable and grounded embodied agents.",
            "headline_zh": "提出CFG-Bench基准以评估具身代理的细粒度动作认知能力",
            "intro_zh": [
                "现有基准忽视细粒度动作智能，聚焦高层规划或空间推理",
                "构建多模态问答对，评估物理交互、时序因果、意图理解和评价判断",
                "实验显示MLLMs在细粒度动作生成和高阶推理方面存在显著局限"
            ],
            "tags_zh": [
                "具身代理",
                "细粒度动作",
                "多模态基准",
                "认知能力评估",
                "监督微调"
            ],
            "_index": 179
        },
        {
            "title": "Now You See It, Now You Don't - Instant Concept Erasure for Safe Text-to-Image and Video Generation",
            "authors": [
                "Shristi Das Biswas",
                "Arani Roy",
                "Kaushik Roy"
            ],
            "arxiv_id": "2511.18684v1",
            "summary": "Robust concept removal for text-to-image (T2I) and text-to-video (T2V) models is essential for their safe deployment. Existing methods, however, suffer from costly retraining, inference overhead, or vulnerability to adversarial attacks. Crucially, they rarely model the latent semantic overlap between the target erase concept and surrounding content -- causing collateral damage post-erasure -- and even fewer methods work reliably across both T2I and T2V domains. We introduce Instant Concept Erasure (ICE), a training-free, modality-agnostic, one-shot weight modification approach that achieves precise, persistent unlearning with zero overhead. ICE defines erase and preserve subspaces using anisotropic energy-weighted scaling, then explicitly regularises against their intersection using a unique, closed-form overlap projector. We pose a convex and Lipschitz-bounded Spectral Unlearning Objective, balancing erasure fidelity and intersection preservation, that admits a stable and unique analytical solution. This solution defines a dissociation operator that is translated to the model's text-conditioning layers, making the edit permanent and runtime-free. Across targeted removals of artistic styles, objects, identities, and explicit content, ICE efficiently achieves strong erasure with improved robustness to red-teaming, all while causing only minimal degradation of original generative abilities in both T2I and T2V models.",
            "headline_zh": "提出ICE方法以解决文本到图像和视频生成中的概念安全擦除问题",
            "intro_zh": [
                "现有概念擦除方法存在重训练成本高、推理开销大或易受攻击等问题",
                "ICE使用各向异性能量加权缩放和重叠投影器实现无训练、模态无关的权重修改",
                "在多种目标移除任务中，ICE实现强擦除且保持生成能力，适用于T2I和T2V模型"
            ],
            "tags_zh": [
                "概念擦除",
                "文本到图像生成",
                "文本到视频生成",
                "权重修改",
                "安全部署",
                "模态无关方法"
            ],
            "_index": 180
        },
        {
            "title": "Online Learning-Enhanced Lie Algebraic MPC for Robust Trajectory Tracking of Autonomous Surface Vehicles",
            "authors": [
                "Yinan Dong",
                "Ziyu Xu",
                "Tsimafei Lazouski",
                "Sangli Teng",
                "Maani Ghaffari"
            ],
            "arxiv_id": "2511.18683v1",
            "summary": "Autonomous surface vehicles (ASVs) are easily influenced by environmental disturbances such as wind and waves, making accurate trajectory tracking a persistent challenge in dynamic marine conditions. In this paper, we propose an efficient controller for trajectory tracking of marine vehicles under unknown disturbances by combining a convex error-state MPC on the Lie group with an online learning module to compensate for these disturbances in real time. This design enables adaptive and robust control while maintaining computational efficiency. Extensive evaluations in numerical simulations, the Virtual RobotX (VRX) simulator, and real-world field experiments demonstrate that our method achieves superior tracking accuracy under various disturbance scenarios compared with existing approaches.",
            "headline_zh": "提出结合李群MPC与在线学习的控制器，以解决自主水面艇在未知扰动下的轨迹跟踪问题。",
            "intro_zh": [
                "核心问题：自主水面艇易受风浪等环境扰动影响，轨迹跟踪在动态海洋条件下困难。",
                "方法要点：结合李群上的凸误差状态MPC与在线学习模块，实时补偿未知扰动。",
                "实验或效果：在仿真和真实实验中，相比现有方法，在各种扰动场景下实现更高跟踪精度。"
            ],
            "tags_zh": [
                "自主水面艇",
                "轨迹跟踪",
                "李群MPC",
                "在线学习",
                "扰动补偿",
                "鲁棒控制"
            ],
            "_index": 181
        },
        {
            "title": "Hierarchical GraphCut Phase Unwrapping based on Invariance of Diffeomorphisms Framework",
            "authors": [
                "Xiang Gao",
                "Xinmu Wang",
                "Zhou Zhao",
                "Junqi Huang",
                "Xianfeng David Gu"
            ],
            "arxiv_id": "2511.18682v1",
            "summary": "Recent years have witnessed rapid advancements in 3D scanning technologies, with applications spanning VR/AR, digital human creation, and medical imaging. Structured-light scanning with phase-shifting techniques is preferred for its use of low-intensity visible light and high accuracy, making it well suited for capturing 4D facial dynamics. A key step is phase unwrapping, which recovers continuous phase values from measurements wrapped modulo 2pi. The goal is to estimate the unwrapped phase count k in the equation Phi = phi + 2pi k, where phi is the wrapped phase and Phi is the true phase. Noise, occlusions, and complex 3D geometry make recovering the true phase challenging because phase unwrapping is ill-posed: measurements only provide modulo 2pi values, and estimating k requires assumptions about surface continuity. Existing methods trade speed for accuracy: fast approaches lack precision, while accurate algorithms are too slow for real-time use. To overcome these limitations, this work proposes a phase unwrapping framework that reformulates GraphCut-based unwrapping as a pixel-labeling problem. This framework improves the estimation of the unwrapped phase count k through the invariance property of diffeomorphisms applied in image space via conformal and optimal transport (OT) maps. An odd number of diffeomorphisms are precomputed from the input phase data, and a hierarchical GraphCut algorithm is applied in each domain. The resulting label maps are fused via majority voting to robustly estimate k at each pixel. Experimental results demonstrate a 45.5x speedup and lower L2 error in real experiments and simulations, showing potential for real-time applications.",
            "headline_zh": "提出基于微分同胚不变性的分层GraphCut相位展开框架，以提升实时3D扫描精度与速度",
            "intro_zh": [
                "核心问题：相位展开因噪声和遮挡而病态，需从模2π值估计真实相位，现有方法难以兼顾速度与精度。",
                "方法要点：将GraphCut重构为像素标记问题，利用微分同胚不变性，通过多数投票融合分层结果。",
                "实验或效果：实验显示速度提升45.5倍且L2误差降低，适用于实时应用如4D面部动态捕捉。"
            ],
            "tags_zh": [
                "相位展开",
                "GraphCut算法",
                "微分同胚不变性",
                "实时3D扫描",
                "像素标记",
                "多数投票融合"
            ],
            "_index": 182
        },
        {
            "title": "Inverse Rendering for High-Genus Surface Meshes from Multi-View Images",
            "authors": [
                "Xiang Gao",
                "Xinmu Wang",
                "Xiaolong Wu",
                "Jiazhi Li",
                "Jingyu Shi",
                "Yu Guo",
                "Yuanpeng Liu",
                "Xiyun Song",
                "Heather Yu",
                "Zongfang Lin",
                "Xianfeng David Gu"
            ],
            "arxiv_id": "2511.18680v1",
            "summary": "We present a topology-informed inverse rendering approach for reconstructing high-genus surface meshes from multi-view images. Compared to 3D representations like voxels and point clouds, mesh-based representations are preferred as they enable the application of differential geometry theory and are optimized for modern graphics pipelines. However, existing inverse rendering methods often fail catastrophically on high-genus surfaces, leading to the loss of key topological features, and tend to oversmooth low-genus surfaces, resulting in the loss of surface details. This failure stems from their overreliance on Adam-based optimizers, which can lead to vanishing and exploding gradients. To overcome these challenges, we introduce an adaptive V-cycle remeshing scheme in conjunction with a re-parametrized Adam optimizer to enhance topological and geometric awareness. By periodically coarsening and refining the deforming mesh, our method informs mesh vertices of their current topology and geometry before optimization, mitigating gradient issues while preserving essential topological features. Additionally, we enforce topological consistency by constructing topological primitives with genus numbers that match those of ground truth using Gauss-Bonnet theorem. Experimental results demonstrate that our inverse rendering approach outperforms the current state-of-the-art method, achieving significant improvements in Chamfer Distance and Volume IoU, particularly for high-genus surfaces, while also enhancing surface details for low-genus surfaces.",
            "headline_zh": "提出拓扑感知逆渲染方法以重建高亏格表面网格",
            "intro_zh": [
                "现有逆渲染方法在高亏格表面易丢失拓扑特征，低亏格表面过度平滑",
                "采用自适应V循环重网格和重参数化Adam优化器，增强拓扑与几何感知",
                "实验显示在Chamfer距离和体积IoU上优于现有方法，提升表面细节"
            ],
            "tags_zh": [
                "逆渲染",
                "高亏格表面",
                "网格重建",
                "拓扑优化",
                "多视图图像"
            ],
            "_index": 183
        },
        {
            "title": "Neural Geometry Image-Based Representations with Optimal Transport (OT)",
            "authors": [
                "Xiang Gao",
                "Yuanpeng Liu",
                "Xinmu Wang",
                "Jiazhi Li",
                "Minghao Guo",
                "Yu Guo",
                "Xiyun Song",
                "Heather Yu",
                "Zhiqiang Lao",
                "Xianfeng David Gu"
            ],
            "arxiv_id": "2511.18679v1",
            "summary": "Neural representations for 3D meshes are emerging as an effective solution for compact storage and efficient processing. Existing methods often rely on neural overfitting, where a coarse mesh is stored and progressively refined through multiple decoder networks. While this can restore high-quality surfaces, it is computationally expensive due to successive decoding passes and the irregular structure of mesh data. In contrast, images have a regular structure that enables powerful super-resolution and restoration frameworks, but applying these advantages to meshes is difficult because their irregular connectivity demands complex encoder-decoder architectures. Our key insight is that a geometry image-based representation transforms irregular meshes into a regular image grid, making efficient image-based neural processing directly applicable. Building on this idea, we introduce our neural geometry image-based representation, which is decoder-free, storage-efficient, and naturally suited for neural processing. It stores a low-resolution geometry-image mipmap of the surface, from which high-quality meshes are restored in a single forward pass. To construct geometry images, we leverage Optimal Transport (OT), which resolves oversampling in flat regions and undersampling in feature-rich regions, and enables continuous levels of detail (LoD) through geometry-image mipmapping. Experimental results demonstrate state-of-the-art storage efficiency and restoration accuracy, measured by compression ratio (CR), Chamfer distance (CD), and Hausdorff distance (HD).",
            "headline_zh": "提出基于最优传输的神经几何图像表示，以高效存储和恢复3D网格。",
            "intro_zh": [
                "核心问题：现有3D网格神经表示依赖复杂解码，计算成本高且结构不规则。",
                "方法要点：利用最优传输构建几何图像，实现单次前向解码和连续细节层次。",
                "实验或效果：在压缩比、Chamfer距离和Hausdorff距离上达到先进水平。"
            ],
            "tags_zh": [
                "神经表示",
                "几何图像",
                "最优传输",
                "3D网格压缩",
                "细节层次"
            ],
            "_index": 184
        },
        {
            "title": "A Theory-Inspired Framework for Few-Shot Cross-Modal Sketch Person Re-Identification",
            "authors": [
                "Yunpeng Gong",
                "Yongjie Hou",
                "Jiangming Shi",
                "Kim Long Diep",
                "Min Jiang"
            ],
            "arxiv_id": "2511.18677v1",
            "summary": "Sketch based person re-identification aims to match hand-drawn sketches with RGB surveillance images, but remains challenging due to significant modality gaps and limited annotated data. To address this, we introduce KTCAA, a theoretically grounded framework for few-shot cross-modal generalization. Motivated by generalization theory, we identify two key factors influencing target domain risk: (1) domain discrepancy, which quantifies the alignment difficulty between source and target distributions; and (2) perturbation invariance, which evaluates the model's robustness to modality shifts. Based on these insights, we propose two components: (1) Alignment Augmentation (AA), which applies localized sketch-style transformations to simulate target distributions and facilitate progressive alignment; and (2) Knowledge Transfer Catalyst (KTC), which enhances invariance by introducing worst-case perturbations and enforcing consistency. These modules are jointly optimized under a meta-learning paradigm that transfers alignment knowledge from data-rich RGB domains to sketch-based scenarios. Experiments on multiple benchmarks demonstrate that KTCAA achieves state-of-the-art performance, particularly in data-scarce conditions.",
            "headline_zh": "提出KTCAA框架以解决少样本跨模态草图行人重识别问题",
            "intro_zh": [
                "核心问题：草图与RGB图像模态差异大且标注数据少，导致重识别困难。",
                "方法要点：基于泛化理论，设计对齐增强和知识转移催化剂模块提升对齐与鲁棒性。",
                "实验或效果：在多个基准测试中实现最先进性能，尤其在数据稀缺条件下表现突出。"
            ],
            "tags_zh": [
                "草图行人重识别",
                "跨模态学习",
                "少样本学习",
                "泛化理论",
                "元学习",
                "对齐增强"
            ],
            "_index": 185
        },
        {
            "title": "MedVision: Dataset and Benchmark for Quantitative Medical Image Analysis",
            "authors": [
                "Yongcheng Yao",
                "Yongshuo Zong",
                "Raman Dutt",
                "Yongxin Yang",
                "Sotirios A Tsaftaris",
                "Timothy Hospedales"
            ],
            "arxiv_id": "2511.18676v1",
            "summary": "Current vision-language models (VLMs) in medicine are primarily designed for categorical question answering (e.g., \"Is this normal or abnormal?\") or qualitative descriptive tasks. However, clinical decision-making often relies on quantitative assessments, such as measuring the size of a tumor or the angle of a joint, from which physicians draw their own diagnostic conclusions. This quantitative reasoning capability remains underexplored and poorly supported in existing VLMs. In this work, we introduce MedVision, a large-scale dataset and benchmark specifically designed to evaluate and improve VLMs on quantitative medical image analysis. MedVision spans 22 public datasets covering diverse anatomies and modalities, with 30.8 million image-annotation pairs. We focus on three representative quantitative tasks: (1) detection of anatomical structures and abnormalities, (2) tumor/lesion (T/L) size estimation, and (3) angle/distance (A/D) measurement. Our benchmarks show that current off-the-shelf VLMs perform poorly on these tasks. However, with supervised fine-tuning on MedVision, we significantly enhance their performance across detection, T/L estimation, and A/D measurement, demonstrating reduced error rates and improved precision. This work provides a foundation for developing VLMs with robust quantitative reasoning capabilities in medical imaging. Code and data are available at https://medvision-vlm.github.io.",
            "headline_zh": "提出MedVision数据集与基准以增强医学视觉语言模型的定量分析能力",
            "intro_zh": [
                "当前医学视觉语言模型缺乏定量推理能力，如肿瘤大小测量",
                "构建大规模数据集，覆盖22个公共数据集，含3080万图像-标注对",
                "通过监督微调显著提升检测、大小估计和角度测量任务的性能"
            ],
            "tags_zh": [
                "医学视觉语言模型",
                "定量图像分析",
                "数据集构建",
                "监督微调",
                "肿瘤大小估计",
                "角度测量"
            ],
            "_index": 186
        },
        {
            "title": "Edit2Perceive: Image Editing Diffusion Models Are Strong Dense Perceivers",
            "authors": [
                "Yiqing Shi",
                "Yiren Song",
                "Mike Zheng Shou"
            ],
            "arxiv_id": "2511.18673v1",
            "summary": "Recent advances in diffusion transformers have shown remarkable generalization in visual synthesis, yet most dense perception methods still rely on text-to-image (T2I) generators designed for stochastic generation. We revisit this paradigm and show that image editing diffusion models are inherently image-to-image consistent, providing a more suitable foundation for dense perception task. We introduce Edit2Perceive, a unified diffusion framework that adapts editing models for depth, normal, and matting. Built upon the FLUX.1 Kontext architecture, our approach employs full-parameter fine-tuning and a pixel-space consistency loss to enforce structure-preserving refinement across intermediate denoising states. Moreover, our single-step deterministic inference yields up to faster runtime while training on relatively small datasets. Extensive experiments demonstrate comprehensive state-of-the-art results across all three tasks, revealing the strong potential of editing-oriented diffusion transformers for geometry-aware perception.",
            "headline_zh": "提出Edit2Perceive框架，利用图像编辑扩散模型进行密集感知任务",
            "intro_zh": [
                "核心问题：传统密集感知方法依赖文本到图像生成器，缺乏图像一致性。",
                "方法要点：基于FLUX.1 Kontext架构，采用全参数微调和像素空间一致性损失。",
                "实验或效果：在深度、法线和抠图任务中实现SOTA，推理速度提升。"
            ],
            "tags_zh": [
                "扩散模型",
                "密集感知",
                "图像编辑",
                "几何感知",
                "一致性损失"
            ],
            "_index": 187
        },
        {
            "title": "Sphinx: Efficiently Serving Novel View Synthesis using Regression-Guided Selective Refinement",
            "authors": [
                "Yuchen Xia",
                "Souvik Kundu",
                "Mosharaf Chowdhury",
                "Nishil Talati"
            ],
            "arxiv_id": "2511.18672v1",
            "summary": "Novel View Synthesis (NVS) is the task of generating new images of a scene from viewpoints that were not part of the original input. Diffusion-based NVS can generate high-quality, temporally consistent images, however, remains computationally prohibitive. Conversely, regression-based NVS offers suboptimal generation quality despite requiring significantly lower compute; leaving the design objective of a high-quality, inference-efficient NVS framework an open challenge. To close this critical gap, we present Sphinx, a training-free hybrid inference framework that achieves diffusion-level fidelity at a significantly lower compute. Sphinx proposes to use regression-based fast initialization to guide and reduce the denoising workload for the diffusion model. Additionally, it integrates selective refinement with adaptive noise scheduling, allowing more compute to uncertain regions and frames. This enables Sphinx to provide flexible navigation of the performance-quality trade-off, allowing adaptation to latency and fidelity requirements for dynamically changing inference scenarios. Our evaluation shows that Sphinx achieves an average 1.8x speedup over diffusion model inference with negligible perceptual degradation of less than 5%, establishing a new Pareto frontier between quality and latency in NVS serving.",
            "headline_zh": "提出Sphinx框架以高效服务新视角合成，实现高质量与低延迟平衡",
            "intro_zh": [
                "核心问题：扩散模型NVS计算成本高，回归模型质量差，需兼顾质量与效率",
                "方法要点：使用回归初始化引导扩散去噪，结合选择性细化和自适应噪声调度",
                "实验或效果：平均加速1.8倍，感知退化低于5%，建立新帕累托前沿"
            ],
            "tags_zh": [
                "新视角合成",
                "扩散模型",
                "回归引导",
                "选择性细化",
                "自适应噪声调度",
                "高效推理"
            ],
            "_index": 188
        }
    ]
}