{
  "count": 68,
  "papers": [
    {
      "title": "Open-vocabulary object 6D pose estimation",
      "authors": [
        "Jaime Corsetti",
        "Davide Boscaini",
        "Changjae Oh",
        "Andrea Cavallaro",
        "Fabio Poiesi"
      ],
      "arxiv_id": "2312.00690v4",
      "summary": "We introduce the new setting of open-vocabulary object 6D pose estimation, in which a textual prompt is used to specify the object of interest. In contrast to existing approaches, in our setting (i) the object of interest is specified solely through the textual prompt, (ii) no object model (e.g., CAD or video sequence) is required at inference, and (iii) the object is imaged from two RGBD viewpoints of different scenes. To operate in this setting, we introduce a novel approach that leverages a Vision-Language Model to segment the object of interest from the scenes and to estimate its relative 6D pose. The key of our approach is a carefully devised strategy to fuse object-level information provided by the prompt with local image features, resulting in a feature space that can generalize to novel concepts. We validate our approach on a new benchmark based on two popular datasets, REAL275 and Toyota-Light, which collectively encompass 34 object instances appearing in four thousand image pairs. The results demonstrate that our approach outperforms both a well-established hand-crafted method and a recent deep learning-based baseline in estimating the relative 6D pose of objects in different scenes. Code and dataset are available at https://jcorsetti.github.io/oryon.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2023-12-01",
      "updated": "2024-06-25",
      "comment": "Camera ready version (CVPR 2024, poster highlight). New Oryon version: arXiv:2406.16384",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00690v4",
      "code_links": [
        {
          "url": "https://jcorsetti.github.io/oryon",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]open-vocabulary",
            "[T]open vocabulary",
            "[T]6D pose estimation"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "FSGS: Real-Time Few-shot View Synthesis using Gaussian Splatting",
      "authors": [
        "Zehao Zhu",
        "Zhiwen Fan",
        "Yifan Jiang",
        "Zhangyang Wang"
      ],
      "arxiv_id": "2312.00451v2",
      "summary": "Novel view synthesis from limited observations remains an important and persistent task. However, high efficiency in existing NeRF-based few-shot view synthesis is often compromised to obtain an accurate 3D representation. To address this challenge, we propose a few-shot view synthesis framework based on 3D Gaussian Splatting that enables real-time and photo-realistic view synthesis with as few as three training views. The proposed method, dubbed FSGS, handles the extremely sparse initialized SfM points with a thoughtfully designed Gaussian Unpooling process. Our method iteratively distributes new Gaussians around the most representative locations, subsequently infilling local details in vacant areas. We also integrate a large-scale pre-trained monocular depth estimator within the Gaussians optimization process, leveraging online augmented views to guide the geometric optimization towards an optimal solution. Starting from sparse points observed from limited input viewpoints, our FSGS can accurately grow into unseen regions, comprehensively covering the scene and boosting the rendering quality of novel views. Overall, FSGS achieves state-of-the-art performance in both accuracy and rendering efficiency across diverse datasets, including LLFF, Mip-NeRF360, and Blender. Project website: https://zehaozhu.github.io/FSGS/.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2023-12-01",
      "updated": "2024-06-16",
      "comment": "Project page: https://zehaozhu.github.io/FSGS/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00451v2",
      "code_links": [
        {
          "url": "https://zehaozhu.github.io/FSGS/",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "monocular depth",
            "3D gaussian splatting",
            "[T]gaussian splatting",
            "[T]splatting",
            "NeRF"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "NeuSG: Neural Implicit Surface Reconstruction with 3D Gaussian Splatting Guidance",
      "authors": [
        "Hanlin Chen",
        "Chen Li",
        "Yunsong Wang",
        "Gim Hee Lee"
      ],
      "arxiv_id": "2312.00846v2",
      "summary": "Existing neural implicit surface reconstruction methods have achieved impressive performance in multi-view 3D reconstruction by leveraging explicit geometry priors such as depth maps or point clouds as regularization. However, the reconstruction results still lack fine details because of the over-smoothed depth map or sparse point cloud. In this work, we propose a neural implicit surface reconstruction pipeline with guidance from 3D Gaussian Splatting to recover highly detailed surfaces. The advantage of 3D Gaussian Splatting is that it can generate dense point clouds with detailed structure. Nonetheless, a naive adoption of 3D Gaussian Splatting can fail since the generated points are the centers of 3D Gaussians that do not necessarily lie on the surface. We thus introduce a scale regularizer to pull the centers close to the surface by enforcing the 3D Gaussians to be extremely thin. Moreover, we propose to refine the point cloud from 3D Gaussians Splatting with the normal priors from the surface predicted by neural implicit models instead of using a fixed set of points as guidance. Consequently, the quality of surface reconstruction improves from the guidance of the more accurate 3D Gaussian splatting. By jointly optimizing the 3D Gaussian Splatting and the neural implicit model, our approach benefits from both representations and generates complete surfaces with intricate details. Experiments on Tanks and Temples verify the effectiveness of our proposed method.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2023-12-01",
      "updated": "2025-03-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00846v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]3D gaussian splatting",
            "[T]gaussian splatting",
            "[T]splatting"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "FreeZe: Training-free zero-shot 6D pose estimation with geometric and vision foundation models",
      "authors": [
        "Andrea Caraffa",
        "Davide Boscaini",
        "Amir Hamza",
        "Fabio Poiesi"
      ],
      "arxiv_id": "2312.00947v3",
      "summary": "Estimating the 6D pose of objects unseen during training is highly desirable yet challenging. Zero-shot object 6D pose estimation methods address this challenge by leveraging additional task-specific supervision provided by large-scale, photo-realistic synthetic datasets. However, their performance heavily depends on the quality and diversity of rendered data and they require extensive training. In this work, we show how to tackle the same task but without training on specific data. We propose FreeZe, a novel solution that harnesses the capabilities of pre-trained geometric and vision foundation models. FreeZe leverages 3D geometric descriptors learned from unrelated 3D point clouds and 2D visual features learned from web-scale 2D images to generate discriminative 3D point-level descriptors. We then estimate the 6D pose of unseen objects by 3D registration based on RANSAC. We also introduce a novel algorithm to solve ambiguous cases due to geometrically symmetric objects that is based on visual features. We comprehensively evaluate FreeZe across the seven core datasets of the BOP Benchmark, which include over a hundred 3D objects and 20,000 images captured in various scenarios. FreeZe consistently outperforms all state-of-the-art approaches, including competitors extensively trained on synthetic 6D pose estimation data. Code will be publicly available at https://andreacaraffa.github.io/freeze.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2023-12-01",
      "updated": "2025-01-08",
      "comment": "Accepted to ECCV 2024. Project page: https://andreacaraffa.github.io/freeze",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00947v3",
      "code_links": [
        {
          "url": "https://andreacaraffa.github.io/freeze",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]6D pose estimation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]foundation model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 15.0,
      "hit_pillars": [
        "3_perception_slam",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "ViP-LLaVA: Making Large Multimodal Models Understand Arbitrary Visual Prompts",
      "authors": [
        "Mu Cai",
        "Haotian Liu",
        "Dennis Park",
        "Siva Karthik Mustikovela",
        "Gregory P. Meyer",
        "Yuning Chai",
        "Yong Jae Lee"
      ],
      "arxiv_id": "2312.00784v2",
      "summary": "While existing large vision-language multimodal models focus on whole image understanding, there is a prominent gap in achieving region-specific comprehension. Current approaches that use textual coordinates or spatial encodings often fail to provide a user-friendly interface for visual prompting. To address this challenge, we introduce a novel multimodal model capable of decoding arbitrary visual prompts. This allows users to intuitively mark images and interact with the model using natural cues like a \"red bounding box\" or \"pointed arrow\". Our simple design directly overlays visual markers onto the RGB image, eliminating the need for complex region encodings, yet achieves state-of-the-art performance on region-understanding tasks like Visual7W, PointQA, and Visual Commonsense Reasoning benchmark. Furthermore, we present ViP-Bench, a comprehensive benchmark to assess the capability of models in understanding visual prompts across multiple dimensions, enabling future research in this domain. Code, data, and model are publicly available.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "published": "2023-12-01",
      "updated": "2024-04-27",
      "comment": "Accepted to CVPR2024. Project page: https://vip-llava.github.io/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00784v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]VIP"
          ],
          "score": 4.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]multimodal"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 13.5,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback",
      "authors": [
        "Tianyu Yu",
        "Yuan Yao",
        "Haoye Zhang",
        "Taiwen He",
        "Yifeng Han",
        "Ganqu Cui",
        "Jinyi Hu",
        "Zhiyuan Liu",
        "Hai-Tao Zheng",
        "Maosong Sun",
        "Tat-Seng Chua"
      ],
      "arxiv_id": "2312.00849v2",
      "summary": "Multimodal Large Language Models (MLLMs) have recently demonstrated impressive capabilities in multimodal understanding, reasoning, and interaction. However, existing MLLMs prevalently suffer from serious hallucination problems, generating text that is not factually grounded in associated images. The problem makes existing MLLMs untrustworthy and thus impractical in real-world (especially high-stakes) applications. To address the challenge, we present RLHF-V, which enhances MLLM trustworthiness via behavior alignment from fine-grained correctional human feedback. Specifically, RLHF-V collects human preference in the form of segment-level corrections on hallucinations, and performs dense direct preference optimization over the human feedback. Comprehensive experiments on five benchmarks in both automatic and human evaluation show that, RLHF-V can enable substantially more trustworthy MLLM behaviors with promising data and computation efficiency. Remarkably, using 1.4k annotated data samples, RLHF-V significantly reduces the hallucination rate of the base MLLM by 34.8%, outperforming the concurrent LLaVA-RLHF trained on 10k annotated data. The final model achieves state-of-the-art performance in trustworthiness among open-source MLLMs, and shows better robustness than GPT-4V in preventing hallucinations aroused from over-generalization. We open-source our code, model, and data at https://github.com/RLHF-V/RLHF-V.",
      "categories": [
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "published": "2023-12-01",
      "updated": "2024-03-08",
      "comment": "Accepted by CVPR 2024",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00849v2",
      "code_links": [
        {
          "url": "https://github.com/RLHF-V/RLHF-V",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]RLHF",
            "direct preference optimization"
          ],
          "score": 6.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model",
            "multimodal"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Dolphins: Multimodal Language Model for Driving",
      "authors": [
        "Yingzi Ma",
        "Yulong Cao",
        "Jiachen Sun",
        "Marco Pavone",
        "Chaowei Xiao"
      ],
      "arxiv_id": "2312.00438v1",
      "summary": "The quest for fully autonomous vehicles (AVs) capable of navigating complex real-world scenarios with human-like understanding and responsiveness. In this paper, we introduce Dolphins, a novel vision-language model architected to imbibe human-like abilities as a conversational driving assistant. Dolphins is adept at processing multimodal inputs comprising video (or image) data, text instructions, and historical control signals to generate informed outputs corresponding to the provided instructions. Building upon the open-sourced pretrained Vision-Language Model, OpenFlamingo, we first enhance Dolphins's reasoning capabilities through an innovative Grounded Chain of Thought (GCoT) process. Then we tailored Dolphins to the driving domain by constructing driving-specific instruction data and conducting instruction tuning. Through the utilization of the BDD-X dataset, we designed and consolidated four distinct AV tasks into Dolphins to foster a holistic understanding of intricate driving scenarios. As a result, the distinctive features of Dolphins are characterized into two dimensions: (1) the ability to provide a comprehensive understanding of complex and long-tailed open-world driving scenarios and solve a spectrum of AV tasks, and (2) the emergence of human-like capabilities including gradient-free instant adaptation via in-context learning and error recovery via reflection.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2023-12-01",
      "updated": "2023-12-01",
      "comment": "The project page is available at https://vlm-driver.github.io/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00438v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]multimodal",
            "chain-of-thought"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "authors": [
        "Albert Gu",
        "Tri Dao"
      ],
      "arxiv_id": "2312.00752v2",
      "summary": "Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "published": "2023-12-01",
      "updated": "2024-05-31",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00752v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]Mamba",
            "SSM",
            "state space model",
            "linear attention"
          ],
          "score": 9.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "foundation model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Exploring the Robustness of Decentralized Training for Large Language Models",
      "authors": [
        "Lin Lu",
        "Chenxi Dai",
        "Wangcheng Tao",
        "Binhang Yuan",
        "Yanan Sun",
        "Pan Zhou"
      ],
      "arxiv_id": "2312.00843v1",
      "summary": "Decentralized training of large language models has emerged as an effective way to democratize this technology. However, the potential threats associated with this approach have not been carefully discussed, which would hinder the development of decentralized training infrastructures. This paper aims to initiate discussion towards this end by exploring the robustness of decentralized training from three main perspectives. First, we demonstrate the vulnerabilities inherent in decentralized training frameworks in terms of hardware, data, and models. Second, we highlight the fundamental difference between decentralized foundation model training and vanilla federated learning, where the security techniques employed in federated learning cannot be applied directly. Third, we discuss the essential components required for a robust and efficient decentralized training framework and present a case study by modeling a concrete threat model. Our objective in this vision paper is to emphasize the importance of addressing security concerns in the context of decentralized training for large language models.",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "published": "2023-12-01",
      "updated": "2023-12-01",
      "comment": "6 pages, 3 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00843v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model",
            "foundation model"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "SeaLLMs -- Large Language Models for Southeast Asia",
      "authors": [
        "Xuan-Phi Nguyen",
        "Wenxuan Zhang",
        "Xin Li",
        "Mahani Aljunied",
        "Zhiqiang Hu",
        "Chenhui Shen",
        "Yew Ken Chia",
        "Xingxuan Li",
        "Jianyu Wang",
        "Qingyu Tan",
        "Liying Cheng",
        "Guanzheng Chen",
        "Yue Deng",
        "Sen Yang",
        "Chaoqun Liu",
        "Hang Zhang",
        "Lidong Bing"
      ],
      "arxiv_id": "2312.00738v2",
      "summary": "Despite the remarkable achievements of large language models (LLMs) in various tasks, there remains a linguistic bias that favors high-resource languages, such as English, often at the expense of low-resource and regional languages. To address this imbalance, we introduce SeaLLMs, an innovative series of language models that specifically focuses on Southeast Asian (SEA) languages. SeaLLMs are built upon the Llama-2 model and further advanced through continued pre-training with an extended vocabulary, specialized instruction and alignment tuning to better capture the intricacies of regional languages. This allows them to respect and reflect local cultural norms, customs, stylistic preferences, and legal considerations. Our comprehensive evaluation demonstrates that SeaLLM-13b models exhibit superior performance across a wide spectrum of linguistic tasks and assistant-style instruction-following capabilities relative to comparable open-source models. Moreover, they outperform ChatGPT-3.5 in non-Latin languages, such as Thai, Khmer, Lao, and Burmese, by large margins while remaining lightweight and cost-effective to operate.",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "published": "2023-12-01",
      "updated": "2024-07-01",
      "comment": "Technical report, ACL 2024 DEMO TRACK",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00738v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model",
            "instruction following"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Generalized Robot 3D Vision-Language Model with Fast Rendering and Pre-Training Vision-Language Alignment",
      "authors": [
        "Kangcheng Liu",
        "Yong-Jin Liu",
        "Baoquan Chen"
      ],
      "arxiv_id": "2312.00663v2",
      "summary": "Deep neural network models have achieved remarkable progress in 3D scene understanding while trained in the closed-set setting and with full labels. However, the major bottleneck is that these models do not have the capacity to recognize any unseen novel classes beyond the training categories in diverse real-world applications. Therefore, we are in urgent need of a framework that can simultaneously be applicable to both 3D point cloud segmentation and detection, particularly in the circumstances where the labels are rather scarce. This work presents a generalized and straightforward framework for dealing with 3D scene understanding when the labeled scenes are quite limited. To extract knowledge for novel categories from the pre-trained vision-language models, we propose a hierarchical feature-aligned pre-training and knowledge distillation strategy to extract and distill meaningful information from large-scale vision-language models, which helps benefit the open-vocabulary scene understanding tasks. To encourage latent instance discrimination and to guarantee efficiency, we propose the unsupervised region-level semantic contrastive learning scheme for point clouds, using confident predictions of the neural network to discriminate the intermediate feature embeddings at multiple stages. In the limited reconstruction case, our proposed approach, termed WS3D++, ranks 1st on the large-scale ScanNet benchmark on both the task of semantic segmentation and instance segmentation. Extensive experiments with both indoor and outdoor scenes demonstrated the effectiveness of our approach in both data-efficient learning and open-world few-shot learning. The code is made publicly available at: https://drive.google.com/drive/folders/1M58V-PtR8DBEwD296zJkNg_m2qq-MTAP?usp=sharing.",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "published": "2023-12-01",
      "updated": "2025-02-19",
      "comment": "IEEE Transactions on Pattern Analysis and Machine Intelligence, Manuscript Info: 17 Pages, 13 Figures, and 6 Tables",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00663v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "contrastive learning",
            "distillation"
          ],
          "score": 3.0
        },
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "scene understanding",
            "open-vocabulary",
            "open vocabulary"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "Omni-SMoLA: Boosting Generalist Multimodal Models with Soft Mixture of Low-rank Experts",
      "authors": [
        "Jialin Wu",
        "Xia Hu",
        "Yaqing Wang",
        "Bo Pang",
        "Radu Soricut"
      ],
      "arxiv_id": "2312.00968v2",
      "summary": "Large multi-modal models (LMMs) exhibit remarkable performance across numerous tasks. However, generalist LMMs often suffer from performance degradation when tuned over a large collection of tasks. Recent research suggests that Mixture of Experts (MoE) architectures are useful for instruction tuning, but for LMMs of parameter size around O(50-100B), the prohibitive cost of replicating and storing the expert models severely limits the number of experts we can use. We propose Omni-SMoLA, an architecture that uses the Soft MoE approach to (softly) mix many multimodal low rank experts, and avoids introducing a significant number of new parameters compared to conventional MoE models. The core intuition here is that the large model provides a foundational backbone, while different lightweight experts residually learn specialized knowledge, either per-modality or multimodally. Extensive experiments demonstrate that the SMoLA approach helps improve the generalist performance across a broad range of generative vision-and-language tasks, achieving new SoTA generalist performance that often matches or outperforms single specialized LMM baselines, as well as new SoTA specialist performance.",
      "categories": [
        "cs.CV",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "published": "2023-12-01",
      "updated": "2024-04-02",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00968v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]multimodal"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Efficient Multimodal Semantic Segmentation via Dual-Prompt Learning",
      "authors": [
        "Shaohua Dong",
        "Yunhe Feng",
        "Qing Yang",
        "Yan Huang",
        "Dongfang Liu",
        "Heng Fan"
      ],
      "arxiv_id": "2312.00360v2",
      "summary": "Multimodal (e.g., RGB-Depth/RGB-Thermal) fusion has shown great potential for improving semantic segmentation in complex scenes (e.g., indoor/low-light conditions). Existing approaches often fully fine-tune a dual-branch encoder-decoder framework with a complicated feature fusion strategy for achieving multimodal semantic segmentation, which is training-costly due to the massive parameter updates in feature extraction and fusion. To address this issue, we propose a surprisingly simple yet effective dual-prompt learning network (dubbed DPLNet) for training-efficient multimodal (e.g., RGB-D/T) semantic segmentation. The core of DPLNet is to directly adapt a frozen pre-trained RGB model to multimodal semantic segmentation, reducing parameter updates. For this purpose, we present two prompt learning modules, comprising multimodal prompt generator (MPG) and multimodal feature adapter (MFA). MPG works to fuse the features from different modalities in a compact manner and is inserted from shadow to deep stages to generate the multi-level multimodal prompts that are injected into the frozen backbone, while MPG adapts prompted multimodal features in the frozen backbone for better multimodal semantic segmentation. Since both the MPG and MFA are lightweight, only a few trainable parameters (3.88M, 4.4% of the pre-trained backbone parameters) are introduced for multimodal feature fusion and learning. Using a simple decoder (3.27M parameters), DPLNet achieves new state-of-the-art performance or is on a par with other complex approaches on four RGB-D/T semantic segmentation datasets while satisfying parameter efficiency. Moreover, we show that DPLNet is general and applicable to other multimodal tasks such as salient object detection and video semantic segmentation. Without special design, DPLNet outperforms many complicated models. Our code will be available at github.com/ShaohuaDong2021/DPLNet.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2023-12-01",
      "updated": "2023-12-04",
      "comment": "11 pages, 4 figures, 9 tables",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00360v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]multimodal"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Understanding Unimodal Bias in Multimodal Deep Linear Networks",
      "authors": [
        "Yedi Zhang",
        "Peter E. Latham",
        "Andrew Saxe"
      ],
      "arxiv_id": "2312.00935v2",
      "summary": "Using multiple input streams simultaneously to train multimodal neural networks is intuitively advantageous but practically challenging. A key challenge is unimodal bias, where a network overly relies on one modality and ignores others during joint training. We develop a theory of unimodal bias with multimodal deep linear networks to understand how architecture and data statistics influence this bias. This is the first work to calculate the duration of the unimodal phase in learning as a function of the depth at which modalities are fused within the network, dataset statistics, and initialization. We show that the deeper the layer at which fusion occurs, the longer the unimodal phase. A long unimodal phase can lead to a generalization deficit and permanent unimodal bias in the overparametrized regime. Our results, derived for multimodal linear networks, extend to nonlinear networks in certain settings. Taken together, this work illuminates pathologies of multimodal learning under joint training, showing that late and intermediate fusion architectures can give rise to long unimodal phases and permanent unimodal bias. Our code is available at: https://yedizhang.github.io/unimodal-bias.html.",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "published": "2023-12-01",
      "updated": "2024-06-02",
      "comment": "ICML 2024 camera ready",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00935v2",
      "code_links": [
        {
          "url": "https://yedizhang.github.io/unimodal-bias.html",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]multimodal"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Latent Space Explorer: Visual Analytics for Multimodal Latent Space Exploration",
      "authors": [
        "Bum Chul Kwon",
        "Samuel Friedman",
        "Kai Xu",
        "Steven A Lubitz",
        "Anthony Philippakis",
        "Puneet Batra",
        "Patrick T Ellinor",
        "Kenney Ng"
      ],
      "arxiv_id": "2312.00857v1",
      "summary": "Machine learning models built on training data with multiple modalities can reveal new insights that are not accessible through unimodal datasets. For example, cardiac magnetic resonance images (MRIs) and electrocardiograms (ECGs) are both known to capture useful information about subjects' cardiovascular health status. A multimodal machine learning model trained from large datasets can potentially predict the onset of heart-related diseases and provide novel medical insights about the cardiovascular system. Despite the potential benefits, it is difficult for medical experts to explore multimodal representation models without visual aids and to test the predictive performance of the models on various subpopulations. To address the challenges, we developed a visual analytics system called Latent Space Explorer. Latent Space Explorer provides interactive visualizations that enable users to explore the multimodal representation of subjects, define subgroups of interest, interactively decode data with different modalities with the selected subjects, and inspect the accuracy of the embedding in downstream prediction tasks. A user study was conducted with medical experts and their feedback provided useful insights into how Latent Space Explorer can help their analysis and possible new direction for further development in the medical domain.",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.HC",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "published": "2023-12-01",
      "updated": "2023-12-01",
      "comment": "7 pages, 5 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00857v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]multimodal"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Spatial-Temporal-Decoupled Masked Pre-training for Spatiotemporal Forecasting",
      "authors": [
        "Haotian Gao",
        "Renhe Jiang",
        "Zheng Dong",
        "Jinliang Deng",
        "Yuxin Ma",
        "Xuan Song"
      ],
      "arxiv_id": "2312.00516v3",
      "summary": "Spatiotemporal forecasting techniques are significant for various domains such as transportation, energy, and weather. Accurate prediction of spatiotemporal series remains challenging due to the complex spatiotemporal heterogeneity. In particular, current end-to-end models are limited by input length and thus often fall into spatiotemporal mirage, i.e., similar input time series followed by dissimilar future values and vice versa. To address these problems, we propose a novel self-supervised pre-training framework Spatial-Temporal-Decoupled Masked Pre-training (STD-MAE) that employs two decoupled masked autoencoders to reconstruct spatiotemporal series along the spatial and temporal dimensions. Rich-context representations learned through such reconstruction could be seamlessly integrated by downstream predictors with arbitrary architectures to augment their performances. A series of quantitative and qualitative evaluations on six widely used benchmarks (PEMS03, PEMS04, PEMS07, PEMS08, METR-LA, and PEMS-BAY) are conducted to validate the state-of-the-art performance of STD-MAE. Codes are available at https://github.com/Jimmy-7664/STD-MAE.",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "published": "2023-12-01",
      "updated": "2024-04-28",
      "comment": "Accepted at IJCAI-2024 Main Track",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00516v3",
      "code_links": [
        {
          "url": "https://github.com/Jimmy-7664/STD-MAE",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "masked autoencoder",
            "MAE"
          ],
          "score": 3.0
        },
        {
          "name": "支柱八：物理动画 (Physics-based Animation)",
          "id": "8_physics_animation",
          "matched_keywords": [
            "[T]spatiotemporal"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "2_algo_arch",
        "8_physics_animation"
      ]
    },
    {
      "title": "Towards a Psychological Generalist AI: A Survey of Current Applications of Large Language Models and Future Prospects",
      "authors": [
        "Tianyu He",
        "Guanghui Fu",
        "Yijing Yu",
        "Fan Wang",
        "Jianqiang Li",
        "Qing Zhao",
        "Changwei Song",
        "Hongzhi Qi",
        "Dan Luo",
        "Huijing Zou",
        "Bing Xiang Yang"
      ],
      "arxiv_id": "2312.04578v1",
      "summary": "The complexity of psychological principles underscore a significant societal challenge, given the vast social implications of psychological problems. Bridging the gap between understanding these principles and their actual clinical and real-world applications demands rigorous exploration and adept implementation. In recent times, the swift advancement of highly adaptive and reusable artificial intelligence (AI) models has emerged as a promising way to unlock unprecedented capabilities in the realm of psychology. This paper emphasizes the importance of performance validation for these large-scale AI models, emphasizing the need to offer a comprehensive assessment of their verification from diverse perspectives. Moreover, we review the cutting-edge advancements and practical implementations of these expansive models in psychology, highlighting pivotal work spanning areas such as social media analytics, clinical nursing insights, vigilant community monitoring, and the nuanced exploration of psychological theories. Based on our review, we project an acceleration in the progress of psychological fields, driven by these large-scale AI models. These future generalist AI models harbor the potential to substantially curtail labor costs and alleviate social stress. However, this forward momentum will not be without its set of challenges, especially when considering the paradigm changes and upgrades required for medical instrumentation and related applications.",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "published": "2023-12-01",
      "updated": "2023-12-01",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.04578v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "LinguaLinked: A Distributed Large Language Model Inference System for Mobile Devices",
      "authors": [
        "Junchen Zhao",
        "Yurun Song",
        "Simeng Liu",
        "Ian G. Harris",
        "Sangeetha Abdu Jyothi"
      ],
      "arxiv_id": "2312.00388v1",
      "summary": "Deploying Large Language Models (LLMs) locally on mobile devices presents a significant challenge due to their extensive memory requirements. In this paper, we introduce LinguaLinked, a system for decentralized, distributed LLM inference on mobile devices. LinguaLinked enables collaborative execution of the inference task across multiple trusted devices. LinguaLinked ensures data privacy by processing information locally. LinguaLinked uses three key strategies. First, an optimized model assignment technique segments LLMs and uses linear optimization to align segments with each device's capabilities. Second, an optimized data transmission mechanism ensures efficient and structured data flow between model segments while also maintaining the integrity of the original model structure. Finally, LinguaLinked incorporates a runtime load balancer that actively monitors and redistributes tasks among mobile devices to prevent bottlenecks, enhancing the system's overall efficiency and responsiveness. We demonstrate that LinguaLinked facilitates efficient LLM inference while maintaining consistent throughput and minimal latency through extensive testing across various mobile devices, from high-end to low-end Android devices. In our evaluations, compared to the baseline, LinguaLinked achieves an inference performance acceleration of $1.11\\times$ to $1.61\\times$ in single-threaded settings, $1.73\\times$ to $2.65\\times$ with multi-threading. Additionally, runtime load balancing yields an overall inference acceleration of $1.29\\times$ to $1.32\\times$.",
      "categories": [
        "cs.LG",
        "cs.DC",
        "cs.NI"
      ],
      "primary_category": "cs.LG",
      "published": "2023-12-01",
      "updated": "2023-12-01",
      "comment": "16 pages, 8 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00388v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Age-Based Scheduling for Mobile Edge Computing: A Deep Reinforcement Learning Approach",
      "authors": [
        "Xingqiu He",
        "Chaoqun You",
        "Tony Q. S. Quek"
      ],
      "arxiv_id": "2312.00279v2",
      "summary": "With the rapid development of Mobile Edge Computing (MEC), various real-time applications have been deployed to benefit people's daily lives. The performance of these applications relies heavily on the freshness of collected environmental information, which can be quantified by its Age of Information (AoI). In the traditional definition of AoI, it is assumed that the status information can be actively sampled and directly used. However, for many MEC-enabled applications, the desired status information is updated in an event-driven manner and necessitates data processing. To better serve these applications, we propose a new definition of AoI and, based on the redefined AoI, we formulate an online AoI minimization problem for MEC systems. Notably, the problem can be interpreted as a Markov Decision Process (MDP), thus enabling its solution through Reinforcement Learning (RL) algorithms. Nevertheless, the traditional RL algorithms are designed for MDPs with completely unknown system dynamics and hence usually suffer long convergence times. To accelerate the learning process, we introduce Post-Decision States (PDSs) to exploit the partial knowledge of the system's dynamics. We also combine PDSs with deep RL to further improve the algorithm's applicability, scalability, and robustness. Numerical results demonstrate that our algorithm outperforms the benchmarks under various scenarios.",
      "categories": [
        "cs.LG",
        "cs.NI"
      ],
      "primary_category": "cs.LG",
      "published": "2023-12-01",
      "updated": "2024-02-23",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00279v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning",
            "[T]deep reinforcement learning"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "LLM-TAKE: Theme Aware Keyword Extraction Using Large Language Models",
      "authors": [
        "Reza Yousefi Maragheh",
        "Chenhao Fang",
        "Charan Chand Irugu",
        "Parth Parikh",
        "Jason Cho",
        "Jianpeng Xu",
        "Saranyan Sukumar",
        "Malay Patel",
        "Evren Korpeoglu",
        "Sushant Kumar",
        "Kannan Achan"
      ],
      "arxiv_id": "2312.00909v1",
      "summary": "Keyword extraction is one of the core tasks in natural language processing. Classic extraction models are notorious for having a short attention span which make it hard for them to conclude relational connections among the words and sentences that are far from each other. This, in turn, makes their usage prohibitive for generating keywords that are inferred from the context of the whole text. In this paper, we explore using Large Language Models (LLMs) in generating keywords for items that are inferred from the items textual metadata. Our modeling framework includes several stages to fine grain the results by avoiding outputting keywords that are non informative or sensitive and reduce hallucinations common in LLM. We call our LLM-based framework Theme-Aware Keyword Extraction (LLM TAKE). We propose two variations of framework for generating extractive and abstractive themes for products in an E commerce setting. We perform an extensive set of experiments on three real data sets and show that our modeling framework can enhance accuracy based and diversity based metrics when compared with benchmark models.",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "published": "2023-12-01",
      "updated": "2023-12-01",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00909v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Questioning Biases in Case Judgment Summaries: Legal Datasets or Large Language Models?",
      "authors": [
        "Aniket Deroy",
        "Subhankar Maity"
      ],
      "arxiv_id": "2312.00554v1",
      "summary": "The evolution of legal datasets and the advent of large language models (LLMs) have significantly transformed the legal field, particularly in the generation of case judgment summaries. However, a critical concern arises regarding the potential biases embedded within these summaries. This study scrutinizes the biases present in case judgment summaries produced by legal datasets and large language models. The research aims to analyze the impact of biases on legal decision making. By interrogating the accuracy, fairness, and implications of biases in these summaries, this study contributes to a better understanding of the role of technology in legal contexts and the implications for justice systems worldwide. In this study, we investigate biases wrt Gender-related keywords, Race-related keywords, Keywords related to crime against women, Country names and religious keywords. The study shows interesting evidences of biases in the outputs generated by the large language models and pre-trained abstractive summarization models. The reasoning behind these biases needs further studies.",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "published": "2023-12-01",
      "updated": "2023-12-01",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00554v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "On Exploring the Reasoning Capability of Large Language Models with Knowledge Graphs",
      "authors": [
        "Pei-Chi Lo",
        "Yi-Hang Tsai",
        "Ee-Peng Lim",
        "San-Yih Hwang"
      ],
      "arxiv_id": "2312.00353v1",
      "summary": "This paper examines the capacity of LLMs to reason with knowledge graphs using their internal knowledge graph, i.e., the knowledge graph they learned during pre-training. Two research questions are formulated to investigate the accuracy of LLMs in recalling information from pre-training knowledge graphs and their ability to infer knowledge graph relations from context. To address these questions, we employ LLMs to perform four distinct knowledge graph reasoning tasks. Furthermore, we identify two types of hallucinations that may occur during knowledge reasoning with LLMs: content and ontology hallucination. Our experimental results demonstrate that LLMs can successfully tackle both simple and complex knowledge graph reasoning tasks from their own memory, as well as infer from input context.",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "published": "2023-12-01",
      "updated": "2023-12-01",
      "comment": "Presented at the Generative-IR Workshop during SIGIR 2023. https://coda.io/@sigir/gen-ir",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00353v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Conceptual Engineering Using Large Language Models",
      "authors": [
        "Bradley P. Allen"
      ],
      "arxiv_id": "2312.03749v2",
      "summary": "We describe a method, based on Jennifer Nado's proposal for classification procedures as targets of conceptual engineering, that implements such procedures by prompting a large language model. We apply this method, using data from the Wikidata knowledge graph, to evaluate stipulative definitions related to two paradigmatic conceptual engineering projects: the International Astronomical Union's redefinition of PLANET and Haslanger's ameliorative analysis of WOMAN. Our results show that classification procedures built using our approach can exhibit good classification performance and, through the generation of rationales for their classifications, can contribute to the identification of issues in either the definitions or the data against which they are being evaluated. We consider objections to this method, and discuss implications of this work for three aspects of theory and practice of conceptual engineering: the definition of its targets, empirical methods for their investigation, and their practical roles. The data and code used for our experiments, together with the experimental results, are available in a Github repository.",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "published": "2023-12-01",
      "updated": "2024-11-02",
      "comment": "22 pages, 2 figures, to appear in Vincent C. Müller, Aliya R. Dewey, Leonard Dung & Guido Löhr (eds.), Philosophy of Artificial Intelligence: The State of the Art. Berlin: SpringerNature (forthcoming), for associated code and data see https://github.com/bradleypallen/zero-shot-classifiers-for-conceptual-engineering",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.03749v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Hyperparameter Optimization for Large Language Model Instruction-Tuning",
      "authors": [
        "Christophe Tribes",
        "Sacha Benarroch-Lelong",
        "Peng Lu",
        "Ivan Kobyzev"
      ],
      "arxiv_id": "2312.00949v2",
      "summary": "The fine-tuning of Large Language Models (LLMs) has enabled them to recently achieve milestones in natural language processing applications. The emergence of ever larger LLMs has paved the way for more efficient fine-tuning methods. Among these, the Low-Rank Adaptation (LoRA) method keeps most of the weights of the pre-trained LLM frozen while introducing a low-rank decomposition of the weight matrix, enabling the tuning of only a very small proportion of the network. The performance on downstream tasks of models fine-tuned with LoRA heavily relies on a set of hyperparameters including the rank of the decomposition. In this work, we investigate the choice of these hyperparameters through two main blackbox optimization (BBO) techniques. We examine the whole pipeline of performing fine-tuning and validation on a pre-trained LLM as a blackbox and efficiently explore the space of hyperparameters with the \\nomad algorithm, achieving a boost in performance and human alignment of the tuned model.",
      "categories": [
        "cs.CL",
        "math.OC"
      ],
      "primary_category": "cs.CL",
      "published": "2023-12-01",
      "updated": "2024-01-30",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00949v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "The Efficiency Spectrum of Large Language Models: An Algorithmic Survey",
      "authors": [
        "Tianyu Ding",
        "Tianyi Chen",
        "Haidong Zhu",
        "Jiachen Jiang",
        "Yiqi Zhong",
        "Jinxin Zhou",
        "Guangzhi Wang",
        "Zhihui Zhu",
        "Ilya Zharkov",
        "Luming Liang"
      ],
      "arxiv_id": "2312.00678v2",
      "summary": "The rapid growth of Large Language Models (LLMs) has been a driving force in transforming various domains, reshaping the artificial general intelligence landscape. However, the increasing computational and memory demands of these models present substantial challenges, hindering both academic research and practical applications. To address these issues, a wide array of methods, including both algorithmic and hardware solutions, have been developed to enhance the efficiency of LLMs. This survey delivers a comprehensive review of algorithmic advancements aimed at improving LLM efficiency. Unlike other surveys that typically focus on specific areas such as training or model compression, this paper examines the multi-faceted dimensions of efficiency essential for the end-to-end algorithmic development of LLMs. Specifically, it covers various topics related to efficiency, including scaling laws, data utilization, architectural innovations, training and tuning strategies, and inference techniques. This paper aims to serve as a valuable resource for researchers and practitioners, laying the groundwork for future innovations in this critical research area. Our repository of relevant references is maintained at url{https://github.com/tding1/Efficient-LLM-Survey}.",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "published": "2023-12-01",
      "updated": "2024-04-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00678v2",
      "code_links": [
        {
          "url": "https://github.com/tding1/Efficient-LLM-Survey",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "CoLLiE: Collaborative Training of Large Language Models in an Efficient Way",
      "authors": [
        "Kai Lv",
        "Shuo Zhang",
        "Tianle Gu",
        "Shuhao Xing",
        "Jiawei Hong",
        "Keyu Chen",
        "Xiaoran Liu",
        "Yuqing Yang",
        "Honglin Guo",
        "Tengxiao Liu",
        "Yu Sun",
        "Qipeng Guo",
        "Hang Yan",
        "Xipeng Qiu"
      ],
      "arxiv_id": "2312.00407v1",
      "summary": "Large language models (LLMs) are increasingly pivotal in a wide range of natural language processing tasks. Access to pre-trained models, courtesy of the open-source community, has made it possible to adapt these models to specific applications for enhanced performance. However, the substantial resources required for training these models necessitate efficient solutions. This paper introduces CoLLiE, an efficient library that facilitates collaborative training of large language models using 3D parallelism, parameter-efficient fine-tuning (PEFT) methods, and optimizers such as Lion, Adan, Sophia, LOMO and AdaLomo. With its modular design and comprehensive functionality, CoLLiE offers a balanced blend of efficiency, ease of use, and customization. CoLLiE has proven superior training efficiency in comparison with prevalent solutions in pre-training and fine-tuning scenarios. Furthermore, we provide an empirical evaluation of the correlation between model size and GPU memory consumption under different optimization methods, as well as an analysis of the throughput. Lastly, we carry out a comprehensive comparison of various optimizers and PEFT methods within the instruction-tuning context. CoLLiE is available at https://github.com/OpenLMLab/collie.",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "published": "2023-12-01",
      "updated": "2023-12-01",
      "comment": "To appear at EMNLP 2023 Demo; Code is available at https://github.com/OpenLMLab/collie",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00407v1",
      "code_links": [
        {
          "url": "https://github.com/OpenLMLab/collie",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Gaussian Grouping: Segment and Edit Anything in 3D Scenes",
      "authors": [
        "Mingqiao Ye",
        "Martin Danelljan",
        "Fisher Yu",
        "Lei Ke"
      ],
      "arxiv_id": "2312.00732v2",
      "summary": "The recent Gaussian Splatting achieves high-quality and real-time novel-view synthesis of the 3D scenes. However, it is solely concentrated on the appearance and geometry modeling, while lacking in fine-grained object-level scene understanding. To address this issue, we propose Gaussian Grouping, which extends Gaussian Splatting to jointly reconstruct and segment anything in open-world 3D scenes. We augment each Gaussian with a compact Identity Encoding, allowing the Gaussians to be grouped according to their object instance or stuff membership in the 3D scene. Instead of resorting to expensive 3D labels, we supervise the Identity Encodings during the differentiable rendering by leveraging the 2D mask predictions by Segment Anything Model (SAM), along with introduced 3D spatial consistency regularization. Compared to the implicit NeRF representation, we show that the discrete and grouped 3D Gaussians can reconstruct, segment and edit anything in 3D with high visual quality, fine granularity and efficiency. Based on Gaussian Grouping, we further propose a local Gaussian Editing scheme, which shows efficacy in versatile scene editing applications, including 3D object removal, inpainting, colorization, style transfer and scene recomposition. Our code and models are at https://github.com/lkeab/gaussian-grouping.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2023-12-01",
      "updated": "2024-07-08",
      "comment": "ECCV 2024. Gaussian Grouping extends Gaussian Splatting to fine-grained open-world 3D scene understanding. Github: https://github.com/lkeab/gaussian-grouping",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00732v2",
      "code_links": [
        {
          "url": "https://github.com/lkeab/gaussian-grouping",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "gaussian splatting",
            "splatting",
            "NeRF",
            "scene understanding"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Nash Learning from Human Feedback",
      "authors": [
        "Rémi Munos",
        "Michal Valko",
        "Daniele Calandriello",
        "Mohammad Gheshlaghi Azar",
        "Mark Rowland",
        "Zhaohan Daniel Guo",
        "Yunhao Tang",
        "Matthieu Geist",
        "Thomas Mesnard",
        "Andrea Michi",
        "Marco Selvi",
        "Sertan Girgin",
        "Nikola Momchev",
        "Olivier Bachem",
        "Daniel J. Mankowitz",
        "Doina Precup",
        "Bilal Piot"
      ],
      "arxiv_id": "2312.00886v4",
      "summary": "Reinforcement learning from human feedback (RLHF) has emerged as the main paradigm for aligning large language models (LLMs) with human preferences. Typically, RLHF involves the initial step of learning a reward model from human feedback, often expressed as preferences between pairs of text generations produced by a pre-trained LLM. Subsequently, the LLM's policy is fine-tuned by optimizing it to maximize the reward model through a reinforcement learning algorithm. However, an inherent limitation of current reward models is their inability to fully represent the richness of human preferences and their dependency on the sampling distribution.\n  In this study, we introduce an alternative pipeline for the fine-tuning of LLMs using pairwise human feedback. Our approach entails the initial learning of a preference model, which is conditioned on two inputs given a prompt, followed by the pursuit of a policy that consistently generates responses preferred over those generated by any competing policy, thus defining the Nash equilibrium of this preference model. We term this approach Nash learning from human feedback (NLHF).\n  In the context of a tabular policy representation, we present a novel algorithmic solution, Nash-MD, founded on the principles of mirror descent. This algorithm produces a sequence of policies, with the last iteration converging to the regularized Nash equilibrium. Additionally, we explore parametric representations of policies and introduce gradient descent algorithms for deep-learning architectures. To demonstrate the effectiveness of our approach, we present experimental results involving the fine-tuning of a LLM for a text summarization task. We believe NLHF offers a compelling avenue for preference learning and policy optimization with the potential of advancing the field of aligning LLMs with human preferences.",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.GT",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "stat.ML",
      "published": "2023-12-01",
      "updated": "2024-06-11",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00886v4",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "preference learning",
            "RLHF"
          ],
          "score": 4.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 7.5,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Enhancing Diffusion Models with 3D Perspective Geometry Constraints",
      "authors": [
        "Rishi Upadhyay",
        "Howard Zhang",
        "Yunhao Ba",
        "Ethan Yang",
        "Blake Gella",
        "Sicheng Jiang",
        "Alex Wong",
        "Achuta Kadambi"
      ],
      "arxiv_id": "2312.00944v1",
      "summary": "While perspective is a well-studied topic in art, it is generally taken for granted in images. However, for the recent wave of high-quality image synthesis methods such as latent diffusion models, perspective accuracy is not an explicit requirement. Since these methods are capable of outputting a wide gamut of possible images, it is difficult for these synthesized images to adhere to the principles of linear perspective. We introduce a novel geometric constraint in the training process of generative models to enforce perspective accuracy. We show that outputs of models trained with this constraint both appear more realistic and improve performance of downstream models trained on generated images. Subjective human trials show that images generated with latent diffusion models trained with our constraint are preferred over images from the Stable Diffusion V2 model 70% of the time. SOTA monocular depth estimation models such as DPT and PixelFormer, fine-tuned on our images, outperform the original models trained on real images by up to 7.03% in RMSE and 19.3% in SqRel on the KITTI test set for zero-shot transfer.",
      "categories": [
        "cs.CV",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "published": "2023-12-01",
      "updated": "2023-12-01",
      "comment": "Project Webpage: http://visual.ee.ucla.edu/diffusionperspective.htm/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00944v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "depth estimation",
            "monocular depth"
          ],
          "score": 4.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "zero-shot transfer"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 7.0,
      "hit_pillars": [
        "3_perception_slam",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Grounding Everything: Emerging Localization Properties in Vision-Language Transformers",
      "authors": [
        "Walid Bousselham",
        "Felix Petersen",
        "Vittorio Ferrari",
        "Hilde Kuehne"
      ],
      "arxiv_id": "2312.00878v3",
      "summary": "Vision-language foundation models have shown remarkable performance in various zero-shot settings such as image retrieval, classification, or captioning. But so far, those models seem to fall behind when it comes to zero-shot localization of referential expressions and objects in images. As a result, they need to be fine-tuned for this task. In this paper, we show that pretrained vision-language (VL) models allow for zero-shot open-vocabulary object localization without any fine-tuning. To leverage those capabilities, we propose a Grounding Everything Module (GEM) that generalizes the idea of value-value attention introduced by CLIPSurgery to a self-self attention path. We show that the concept of self-self attention corresponds to clustering, thus enforcing groups of tokens arising from the same object to be similar while preserving the alignment with the language space. To further guide the group formation, we propose a set of regularizations that allows the model to finally generalize across datasets and backbones. We evaluate the proposed GEM framework on various benchmark tasks and datasets for semantic segmentation. It shows that GEM not only outperforms other training-free open-vocabulary localization methods, but also achieves state-of-the-art results on the recently proposed OpenImagesV7 large-scale segmentation benchmark.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2023-12-01",
      "updated": "2023-12-14",
      "comment": "Code available at https://github.com/WalBouss/GEM",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00878v3",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "open-vocabulary",
            "open vocabulary"
          ],
          "score": 4.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "foundation model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 7.0,
      "hit_pillars": [
        "3_perception_slam",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Adversarial Score Distillation: When score distillation meets GAN",
      "authors": [
        "Min Wei",
        "Jingkai Zhou",
        "Junyao Sun",
        "Xuesong Zhang"
      ],
      "arxiv_id": "2312.00739v2",
      "summary": "Existing score distillation methods are sensitive to classifier-free guidance (CFG) scale: manifested as over-smoothness or instability at small CFG scales, while over-saturation at large ones. To explain and analyze these issues, we revisit the derivation of Score Distillation Sampling (SDS) and decipher existing score distillation with the Wasserstein Generative Adversarial Network (WGAN) paradigm. With the WGAN paradigm, we find that existing score distillation either employs a fixed sub-optimal discriminator or conducts incomplete discriminator optimization, resulting in the scale-sensitive issue. We propose the Adversarial Score Distillation (ASD), which maintains an optimizable discriminator and updates it using the complete optimization objective. Experiments show that the proposed ASD performs favorably in 2D distillation and text-to-3D tasks against existing methods. Furthermore, to explore the generalization ability of our WGAN paradigm, we extend ASD to the image editing task, which achieves competitive results. The project page and code are at https://github.com/2y7c3/ASD.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2023-12-01",
      "updated": "2024-09-10",
      "comment": "CVPR 2024",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00739v2",
      "code_links": [
        {
          "url": "https://github.com/2y7c3/ASD",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]distillation"
          ],
          "score": 4.5
        },
        {
          "name": "支柱四：生成式动作 (Generative Motion)",
          "id": "4_motion_diffusion",
          "matched_keywords": [
            "classifier-free guidance"
          ],
          "score": 2.5
        }
      ],
      "relevance_score": 7.0,
      "hit_pillars": [
        "2_algo_arch",
        "4_motion_diffusion"
      ]
    },
    {
      "title": "TRC: Trust Region Conditional Value at Risk for Safe Reinforcement Learning",
      "authors": [
        "Dohyeong Kim",
        "Songhwai Oh"
      ],
      "arxiv_id": "2312.00344v1",
      "summary": "As safety is of paramount importance in robotics, reinforcement learning that reflects safety, called safe RL, has been studied extensively. In safe RL, we aim to find a policy which maximizes the desired return while satisfying the defined safety constraints. There are various types of constraints, among which constraints on conditional value at risk (CVaR) effectively lower the probability of failures caused by high costs since CVaR is a conditional expectation obtained above a certain percentile. In this paper, we propose a trust region-based safe RL method with CVaR constraints, called TRC. We first derive the upper bound on CVaR and then approximate the upper bound in a differentiable form in a trust region. Using this approximation, a subproblem to get policy gradients is formulated, and policies are trained by iteratively solving the subproblem. TRC is evaluated through safe navigation tasks in simulations with various robots and a sim-to-real environment with a Jackal robot from Clearpath. Compared to other safe RL methods, the performance is improved by 1.93 times while the constraints are satisfied in all experiments.",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "published": "2023-12-01",
      "updated": "2023-12-01",
      "comment": "RA-L and ICRA 2022",
      "doi": "10.1109/LRA.2022.3141829",
      "journal_ref": "IEEE Robotics and Automation Letters, vol. 7, no. 2, pp. 2621-2628, April 2022",
      "pdf_url": "https://arxiv.org/pdf/2312.00344v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "sim-to-real"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 6.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "Towards Generalizable Zero-Shot Manipulation via Translating Human Interaction Plans",
      "authors": [
        "Homanga Bharadhwaj",
        "Abhinav Gupta",
        "Vikash Kumar",
        "Shubham Tulsiani"
      ],
      "arxiv_id": "2312.00775v1",
      "summary": "We pursue the goal of developing robots that can interact zero-shot with generic unseen objects via a diverse repertoire of manipulation skills and show how passive human videos can serve as a rich source of data for learning such generalist robots. Unlike typical robot learning approaches which directly learn how a robot should act from interaction data, we adopt a factorized approach that can leverage large-scale human videos to learn how a human would accomplish a desired task (a human plan), followed by translating this plan to the robots embodiment. Specifically, we learn a human plan predictor that, given a current image of a scene and a goal image, predicts the future hand and object configurations. We combine this with a translation module that learns a plan-conditioned robot manipulation policy, and allows following humans plans for generic manipulation tasks in a zero-shot manner with no deployment-time training. Importantly, while the plan predictor can leverage large-scale human videos for learning, the translation module only requires a small amount of in-domain data, and can generalize to tasks not seen during training. We show that our learned system can perform over 16 manipulation skills that generalize to 40 objects, encompassing 100 real-world tasks for table-top manipulation and diverse in-the-wild manipulation. https://homangab.github.io/hopman/",
      "categories": [
        "cs.RO",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "published": "2023-12-01",
      "updated": "2023-12-01",
      "comment": "Preprint. Under Review",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00775v1",
      "code_links": [
        {
          "url": "https://homangab.github.io/hopman/",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "A bilevel optimal motion planning (BOMP) model with application to autonomous parking",
      "authors": [
        "Shenglei Shi",
        "Youlun Xiong",
        "Jiankui Chen",
        "Caihua Xiong"
      ],
      "arxiv_id": "2312.00314v1",
      "summary": "In this paper, we present a bilevel optimal motion planning (BOMP) model for autonomous parking. The BOMP model treats motion planning as an optimal control problem, in which the upper level is designed for vehicle nonlinear dynamics, and the lower level is for geometry collision-free constraints. The significant feature of the BOMP model is that the lower level is a linear programming problem that serves as a constraint for the upper-level problem. That is, an optimal control problem contains an embedded optimization problem as constraints. Traditional optimal control methods cannot solve the BOMP problem directly. Therefore, the modified approximate Karush-Kuhn-Tucker theory is applied to generate a general nonlinear optimal control problem. Then the pseudospectral optimal control method solves the converted problem. Particularly, the lower level is the $J_2$-function that acts as a distance function between convex polyhedron objects. Polyhedrons can approximate vehicles in higher precision than spheres or ellipsoids. Besides, the modified $J_2$-function (MJ) and the active-points based modified $J_2$-function (APMJ) are proposed to reduce the variables number and time complexity. As a result, an iteirative two-stage BOMP algorithm for autonomous parking concerning dynamical feasibility and collision-free property is proposed. The MJ function is used in the initial stage to find an initial collision-free approximate optimal trajectory and the active points, then the APMJ function in the final stage finds out the optimal trajectory. Simulation results and experiment on Turtlebot3 validate the BOMP model, and demonstrate that the computation speed increases almost two orders of magnitude compared with the area criterion based collision avoidance method.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2023-12-01",
      "updated": "2023-12-01",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00314v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]motion planning"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Zero-Shot Video Question Answering with Procedural Programs",
      "authors": [
        "Rohan Choudhury",
        "Koichiro Niinuma",
        "Kris M. Kitani",
        "László A. Jeni"
      ],
      "arxiv_id": "2312.00937v1",
      "summary": "We propose to answer zero-shot questions about videos by generating short procedural programs that derive a final answer from solving a sequence of visual subtasks. We present Procedural Video Querying (ProViQ), which uses a large language model to generate such programs from an input question and an API of visual modules in the prompt, then executes them to obtain the output. Recent similar procedural approaches have proven successful for image question answering, but videos remain challenging: we provide ProViQ with modules intended for video understanding, allowing it to generalize to a wide variety of videos. This code generation framework additionally enables ProViQ to perform other video tasks in addition to question answering, such as multi-object tracking or basic video editing. ProViQ achieves state-of-the-art results on a diverse range of benchmarks, with improvements of up to 25% on short, long, open-ended, and multimodal video question-answering datasets. Our project page is at https://rccchoudhury.github.io/proviq2023.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2023-12-01",
      "updated": "2023-12-01",
      "comment": "16 pages, 7 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00937v1",
      "code_links": [
        {
          "url": "https://rccchoudhury.github.io/proviq2023",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model",
            "multimodal"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "DISTWAR: Fast Differentiable Rendering on Raster-based Rendering Pipelines",
      "authors": [
        "Sankeerth Durvasula",
        "Adrian Zhao",
        "Fan Chen",
        "Ruofan Liang",
        "Pawan Kumar Sanjaya",
        "Nandita Vijaykumar"
      ],
      "arxiv_id": "2401.05345v1",
      "summary": "Differentiable rendering is a technique used in an important emerging class of visual computing applications that involves representing a 3D scene as a model that is trained from 2D images using gradient descent. Recent works (e.g. 3D Gaussian Splatting) use a rasterization pipeline to enable rendering high quality photo-realistic imagery at high speeds from these learned 3D models. These methods have been demonstrated to be very promising, providing state-of-art quality for many important tasks. However, training a model to represent a scene is still a time-consuming task even when using powerful GPUs. In this work, we observe that the gradient computation phase during training is a significant bottleneck on GPUs due to the large number of atomic operations that need to be processed. These atomic operations overwhelm atomic units in the L2 partitions causing stalls. To address this challenge, we leverage the observations that during the gradient computation: (1) for most warps, all threads atomically update the same memory locations; and (2) warps generate varying amounts of atomic traffic (since some threads may be inactive). We propose DISTWAR, a software-approach to accelerate atomic operations based on two key ideas: First, we enable warp-level reduction of threads at the SM sub-cores using registers to leverage the locality in intra-warp atomic updates. Second, we distribute the atomic computation between the warp-level reduction at the SM and the L2 atomic units to increase the throughput of atomic computation. Warps with many threads performing atomic updates to the same memory locations are scheduled at the SM, and the rest using L2 atomic units. We implement DISTWAR using existing warp-level primitives. We evaluate DISTWAR on widely used raster-based differentiable rendering workloads. We demonstrate significant speedups of 2.44x on average (up to 5.7x).",
      "categories": [
        "cs.CV",
        "cs.GR",
        "cs.PF"
      ],
      "primary_category": "cs.CV",
      "published": "2023-12-01",
      "updated": "2023-12-01",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2401.05345v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "3D gaussian splatting",
            "gaussian splatting",
            "splatting"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Segment Any 3D Gaussians",
      "authors": [
        "Jiazhong Cen",
        "Jiemin Fang",
        "Chen Yang",
        "Lingxi Xie",
        "Xiaopeng Zhang",
        "Wei Shen",
        "Qi Tian"
      ],
      "arxiv_id": "2312.00860v3",
      "summary": "This paper presents SAGA (Segment Any 3D GAussians), a highly efficient 3D promptable segmentation method based on 3D Gaussian Splatting (3D-GS). Given 2D visual prompts as input, SAGA can segment the corresponding 3D target represented by 3D Gaussians within 4 ms. This is achieved by attaching an scale-gated affinity feature to each 3D Gaussian to endow it a new property towards multi-granularity segmentation. Specifically, a scale-aware contrastive training strategy is proposed for the scale-gated affinity feature learning. It 1) distills the segmentation capability of the Segment Anything Model (SAM) from 2D masks into the affinity features and 2) employs a soft scale gate mechanism to deal with multi-granularity ambiguity in 3D segmentation through adjusting the magnitude of each feature channel according to a specified 3D physical scale. Evaluations demonstrate that SAGA achieves real-time multi-granularity segmentation with quality comparable to state-of-the-art methods. As one of the first methods addressing promptable segmentation in 3D-GS, the simplicity and effectiveness of SAGA pave the way for future advancements in this field. Our code will be released.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2023-12-01",
      "updated": "2025-02-05",
      "comment": "AAAI-25. Project page: https://jumpat.github.io/SAGA",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00860v3",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "3D gaussian splatting",
            "gaussian splatting",
            "splatting"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Spatiotemporal Transformer for Imputing Sparse Data: A Deep Learning Approach",
      "authors": [
        "Kehui Yao",
        "Jingyi Huang",
        "Jun Zhu"
      ],
      "arxiv_id": "2312.00963v1",
      "summary": "Effective management of environmental resources and agricultural sustainability heavily depends on accurate soil moisture data. However, datasets like the SMAP/Sentinel-1 soil moisture product often contain missing values across their spatiotemporal grid, which poses a significant challenge. This paper introduces a novel Spatiotemporal Transformer model (ST-Transformer) specifically designed to address the issue of missing values in sparse spatiotemporal datasets, particularly focusing on soil moisture data. The ST-Transformer employs multiple spatiotemporal attention layers to capture the complex spatiotemporal correlations in the data and can integrate additional spatiotemporal covariates during the imputation process, thereby enhancing its accuracy. The model is trained using a self-supervised approach, enabling it to autonomously predict missing values from observed data points. Our model's efficacy is demonstrated through its application to the SMAP 1km soil moisture data over a 36 x 36 km grid in Texas. It showcases superior accuracy compared to well-known imputation methods. Additionally, our simulation studies on other datasets highlight the model's broader applicability in various spatiotemporal imputation tasks.",
      "categories": [
        "cs.LG",
        "stat.ME"
      ],
      "primary_category": "cs.LG",
      "published": "2023-12-01",
      "updated": "2023-12-01",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00963v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱八：物理动画 (Physics-based Animation)",
          "id": "8_physics_animation",
          "matched_keywords": [
            "[T]spatiotemporal"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "8_physics_animation"
      ]
    },
    {
      "title": "Which Augmentation Should I Use? An Empirical Investigation of Augmentations for Self-Supervised Phonocardiogram Representation Learning",
      "authors": [
        "Aristotelis Ballas",
        "Vasileios Papapanagiotou",
        "Christos Diou"
      ],
      "arxiv_id": "2312.00502v6",
      "summary": "Despite recent advancements in deep learning, its application in real-world medical settings, such as phonocardiogram (PCG) classification, remains limited. A significant barrier is the lack of high-quality annotated datasets, which hampers the development of robust, generalizable models that can perform well on newly collected, out-of-distribution (OOD) data. Self-Supervised Learning (SSL) contrastive learning, has shown promise in mitigating the issue of data scarcity by using unlabeled data to enhance model robustness. Even though SSL methods have been proposed and researched in other domains, works focusing on the impact of data augmentations on model robustness for PCG classification are limited. In particular, while augmentations are a key component in SSL, selecting the most suitable policy during training is highly challenging. Improper augmentations can lead to substantial performance degradation and even hinder a network's ability to learn meaningful representations. Addressing this gap, our research aims to explore and evaluate a wide range of audio-based augmentations and uncover combinations that enhance SSL model performance in PCG classification. We conduct a comprehensive comparative analysis across multiple datasets, assessing the impact of various augmentations on model performance. Our findings reveal that depending on the training distribution, augmentation choice significantly influences model robustness, with fully-supervised models experiencing up to a 32\\% drop in effectiveness when evaluated on unseen data, while SSL models demonstrate greater resilience, losing only 10\\% or even improving in some cases. This study also highlights the most promising and appropriate augmentations for PCG signal processing, by calculating their effect size on training. These insights equip researchers with valuable guidelines for developing reliable models in PCG signal processing.",
      "categories": [
        "cs.LG",
        "cs.SD",
        "eess.AS",
        "q-bio.QM"
      ],
      "primary_category": "cs.LG",
      "published": "2023-12-01",
      "updated": "2025-01-04",
      "comment": "Accepted in IEEE ACCESS: https://doi.org/10.1109/ACCESS.2024.3519297",
      "doi": "10.1109/ACCESS.2024.3519297",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00502v6",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]representation learning",
            "contrastive learning"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Tracking Object Positions in Reinforcement Learning: A Metric for Keypoint Detection (extended version)",
      "authors": [
        "Emma Cramer",
        "Jonas Reiher",
        "Sebastian Trimpe"
      ],
      "arxiv_id": "2312.00592v3",
      "summary": "Reinforcement learning (RL) for robot control typically requires a detailed representation of the environment state, including information about task-relevant objects not directly measurable. Keypoint detectors, such as spatial autoencoders (SAEs), are a common approach to extracting a low-dimensional representation from high-dimensional image data. SAEs aim at spatial features such as object positions, which are often useful representations in robotic RL. However, whether an SAE is actually able to track objects in the scene and thus yields a spatial state representation well suited for RL tasks has rarely been examined due to a lack of established metrics. In this paper, we propose to assess the performance of an SAE instance by measuring how well keypoints track ground truth objects in images. We present a computationally lightweight metric and use it to evaluate common baseline SAE architectures on image data from a simulated robot task. We find that common SAEs differ substantially in their spatial extraction capability. Furthermore, we validate that SAEs that perform well in our metric achieve superior performance when used in downstream RL. Thus, our metric is an effective and lightweight indicator of RL performance before executing expensive RL training. Building on these insights, we identify three key modifications of SAE architectures to improve tracking performance.",
      "categories": [
        "cs.LG",
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "published": "2023-12-01",
      "updated": "2024-07-02",
      "comment": "19 pages, 12 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00592v3",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Domain Adaptive Imitation Learning with Visual Observation",
      "authors": [
        "Sungho Choi",
        "Seungyul Han",
        "Woojun Kim",
        "Jongseong Chae",
        "Whiyoung Jung",
        "Youngchul Sung"
      ],
      "arxiv_id": "2312.00548v1",
      "summary": "In this paper, we consider domain-adaptive imitation learning with visual observation, where an agent in a target domain learns to perform a task by observing expert demonstrations in a source domain. Domain adaptive imitation learning arises in practical scenarios where a robot, receiving visual sensory data, needs to mimic movements by visually observing other robots from different angles or observing robots of different shapes. To overcome the domain shift in cross-domain imitation learning with visual observation, we propose a novel framework for extracting domain-independent behavioral features from input observations that can be used to train the learner, based on dual feature extraction and image reconstruction. Empirical results demonstrate that our approach outperforms previous algorithms for imitation learning from visual observation with domain shift.",
      "categories": [
        "cs.LG",
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "published": "2023-12-01",
      "updated": "2023-12-01",
      "comment": "Accepted to NeurIPS 2023",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00548v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]imitation learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Improve Supervised Representation Learning with Masked Image Modeling",
      "authors": [
        "Kaifeng Chen",
        "Daniel Salz",
        "Huiwen Chang",
        "Kihyuk Sohn",
        "Dilip Krishnan",
        "Mojtaba Seyedhosseini"
      ],
      "arxiv_id": "2312.00950v1",
      "summary": "Training visual embeddings with labeled data supervision has been the de facto setup for representation learning in computer vision. Inspired by recent success of adopting masked image modeling (MIM) in self-supervised representation learning, we propose a simple yet effective setup that can easily integrate MIM into existing supervised training paradigms. In our design, in addition to the original classification task applied to a vision transformer image encoder, we add a shallow transformer-based decoder on top of the encoder and introduce an MIM task which tries to reconstruct image tokens based on masked image inputs. We show with minimal change in architecture and no overhead in inference that this setup is able to improve the quality of the learned representations for downstream tasks such as classification, image retrieval, and semantic segmentation. We conduct a comprehensive study and evaluation of our setup on public benchmarks. On ImageNet-1k, our ViT-B/14 model achieves 81.72% validation accuracy, 2.01% higher than the baseline model. On K-Nearest-Neighbor image retrieval evaluation with ImageNet-1k, the same model outperforms the baseline by 1.32%. We also show that this setup can be easily scaled to larger models and datasets. Code and checkpoints will be released.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2023-12-01",
      "updated": "2023-12-01",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00950v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]representation learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "EfficientSAM: Leveraged Masked Image Pretraining for Efficient Segment Anything",
      "authors": [
        "Yunyang Xiong",
        "Bala Varadarajan",
        "Lemeng Wu",
        "Xiaoyu Xiang",
        "Fanyi Xiao",
        "Chenchen Zhu",
        "Xiaoliang Dai",
        "Dilin Wang",
        "Fei Sun",
        "Forrest Iandola",
        "Raghuraman Krishnamoorthi",
        "Vikas Chandra"
      ],
      "arxiv_id": "2312.00863v1",
      "summary": "Segment Anything Model (SAM) has emerged as a powerful tool for numerous vision applications. A key component that drives the impressive performance for zero-shot transfer and high versatility is a super large Transformer model trained on the extensive high-quality SA-1B dataset. While beneficial, the huge computation cost of SAM model has limited its applications to wider real-world applications. To address this limitation, we propose EfficientSAMs, light-weight SAM models that exhibits decent performance with largely reduced complexity. Our idea is based on leveraging masked image pretraining, SAMI, which learns to reconstruct features from SAM image encoder for effective visual representation learning. Further, we take SAMI-pretrained light-weight image encoders and mask decoder to build EfficientSAMs, and finetune the models on SA-1B for segment anything task. We perform evaluations on multiple vision tasks including image classification, object detection, instance segmentation, and semantic object detection, and find that our proposed pretraining method, SAMI, consistently outperforms other masked image pretraining methods. On segment anything task such as zero-shot instance segmentation, our EfficientSAMs with SAMI-pretrained lightweight image encoders perform favorably with a significant gain (e.g., ~4 AP on COCO/LVIS) over other fast SAM models.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2023-12-01",
      "updated": "2023-12-01",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00863v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "representation learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "zero-shot transfer"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Virtual Fusion with Contrastive Learning for Single Sensor-based Activity Recognition",
      "authors": [
        "Duc-Anh Nguyen",
        "Cuong Pham",
        "Nhien-An Le-Khac"
      ],
      "arxiv_id": "2312.02185v1",
      "summary": "Various types of sensors can be used for Human Activity Recognition (HAR), and each of them has different strengths and weaknesses. Sometimes a single sensor cannot fully observe the user's motions from its perspective, which causes wrong predictions. While sensor fusion provides more information for HAR, it comes with many inherent drawbacks like user privacy and acceptance, costly set-up, operation, and maintenance. To deal with this problem, we propose Virtual Fusion - a new method that takes advantage of unlabeled data from multiple time-synchronized sensors during training, but only needs one sensor for inference. Contrastive learning is adopted to exploit the correlation among sensors. Virtual Fusion gives significantly better accuracy than training with the same single sensor, and in some cases, it even surpasses actual fusion using multiple sensors at test time. We also extend this method to a more general version called Actual Fusion within Virtual Fusion (AFVF), which uses a subset of training sensors during inference. Our method achieves state-of-the-art accuracy and F1-score on UCI-HAR and PAMAP2 benchmark datasets. Implementation is available upon request.",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "published": "2023-12-01",
      "updated": "2023-12-01",
      "comment": "",
      "doi": "10.1109/JSEN.2024.3412397",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.02185v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]contrastive learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Dancing with Still Images: Video Distillation via Static-Dynamic Disentanglement",
      "authors": [
        "Ziyu Wang",
        "Yue Xu",
        "Cewu Lu",
        "Yong-Lu Li"
      ],
      "arxiv_id": "2312.00362v2",
      "summary": "Recently, dataset distillation has paved the way towards efficient machine learning, especially for image datasets. However, the distillation for videos, characterized by an exclusive temporal dimension, remains an underexplored domain. In this work, we provide the first systematic study of video distillation and introduce a taxonomy to categorize temporal compression. Our investigation reveals that the temporal information is usually not well learned during distillation, and the temporal dimension of synthetic data contributes little. The observations motivate our unified framework of disentangling the dynamic and static information in the videos. It first distills the videos into still images as static memory and then compensates the dynamic and motion information with a learnable dynamic memory block. Our method achieves state-of-the-art on video datasets at different scales, with a notably smaller memory storage budget. Our code is available at https://github.com/yuz1wan/video_distillation.",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "published": "2023-12-01",
      "updated": "2024-04-15",
      "comment": "CVPR 2024, project page: https://mvig-rhos.com/video-distill",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00362v2",
      "code_links": [
        {
          "url": "https://github.com/yuz1wan/video_distillation",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]distillation"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Spectral Temporal Contrastive Learning",
      "authors": [
        "Sacha Morin",
        "Somjit Nath",
        "Samira Ebrahimi Kahou",
        "Guy Wolf"
      ],
      "arxiv_id": "2312.00966v2",
      "summary": "Learning useful data representations without requiring labels is a cornerstone of modern deep learning. Self-supervised learning methods, particularly contrastive learning (CL), have proven successful by leveraging data augmentations to define positive pairs. This success has prompted a number of theoretical studies to better understand CL and investigate theoretical bounds for downstream linear probing tasks. This work is concerned with the temporal contrastive learning (TCL) setting where the sequential structure of the data is used instead to define positive pairs, which is more commonly used in RL and robotics contexts. In this paper, we adapt recent work on Spectral CL to formulate Spectral Temporal Contrastive Learning (STCL). We discuss a population loss based on a state graph derived from a time-homogeneous reversible Markov chain with uniform stationary distribution. The STCL loss enables to connect the linear probing performance to the spectral properties of the graph, and can be estimated by considering previously observed data sequences as an ensemble of MCMC chains.",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "published": "2023-12-01",
      "updated": "2023-12-07",
      "comment": "Accepted to Self-Supervised Learning - Theory and Practice, NeurIPS Workshop, 2023",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00966v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]contrastive learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Extreme Event Prediction with Multi-agent Reinforcement Learning-based Parametrization of Atmospheric and Oceanic Turbulence",
      "authors": [
        "Rambod Mojgani",
        "Daniel Waelchli",
        "Yifei Guan",
        "Petros Koumoutsakos",
        "Pedram Hassanzadeh"
      ],
      "arxiv_id": "2312.00907v1",
      "summary": "Global climate models (GCMs) are the main tools for understanding and predicting climate change. However, due to limited numerical resolutions, these models suffer from major structural uncertainties; e.g., they cannot resolve critical processes such as small-scale eddies in atmospheric and oceanic turbulence. Thus, such small-scale processes have to be represented as a function of the resolved scales via closures (parametrization). The accuracy of these closures is particularly important for capturing climate extremes. Traditionally, such closures are based on heuristics and simplifying assumptions about the unresolved physics. Recently, supervised-learned closures, trained offline on high-fidelity data, have been shown to outperform the classical physics-based closures. However, this approach requires a significant amount of high-fidelity training data and can also lead to instabilities. Reinforcement learning is emerging as a potent alternative for developing such closures as it requires only low-order statistics and leads to stable closures. In Scientific Multi-Agent Reinforcement Learning (SMARL) computational elements serve a dual role of discretization points and learning agents. We leverage SMARL and fundamentals of turbulence physics to learn closures for prototypes of atmospheric and oceanic turbulence. The policy is trained using only the enstrophy spectrum, which is nearly invariant and can be estimated from a few high-fidelity samples (these few samples are far from enough for supervised/offline learning). We show that these closures lead to stable low-resolution simulations that, at a fraction of the cost, can reproduce the high-fidelity simulations' statistics, including the tails of the probability density functions. The results demonstrate the high potential of SMARL for closure modeling for GCMs, especially in the regime of scarce data and indirect observations.",
      "categories": [
        "cs.LG",
        "cs.CE",
        "physics.ao-ph",
        "physics.comp-ph",
        "physics.flu-dyn"
      ],
      "primary_category": "cs.LG",
      "published": "2023-12-01",
      "updated": "2023-12-01",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00907v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Safe Reinforcement Learning in Tensor Reproducing Kernel Hilbert Space",
      "authors": [
        "Xiaoyuan Cheng",
        "Boli Chen",
        "Liz Varga",
        "Yukun Hu"
      ],
      "arxiv_id": "2312.00727v1",
      "summary": "This paper delves into the problem of safe reinforcement learning (RL) in a partially observable environment with the aim of achieving safe-reachability objectives. In traditional partially observable Markov decision processes (POMDP), ensuring safety typically involves estimating the belief in latent states. However, accurately estimating an optimal Bayesian filter in POMDP to infer latent states from observations in a continuous state space poses a significant challenge, largely due to the intractable likelihood. To tackle this issue, we propose a stochastic model-based approach that guarantees RL safety almost surely in the face of unknown system dynamics and partial observation environments. We leveraged the Predictive State Representation (PSR) and Reproducing Kernel Hilbert Space (RKHS) to represent future multi-step observations analytically, and the results in this context are provable. Furthermore, we derived essential operators from the kernel Bayes' rule, enabling the recursive estimation of future observations using various operators. Under the assumption of \\textit{undercompleness}, a polynomial sample complexity is established for the RL algorithm for the infinite size of observation and action spaces, ensuring an $ε-$suboptimal safe policy guarantee.",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "published": "2023-12-01",
      "updated": "2023-12-01",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00727v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Optimal Sample Complexity of Contrastive Learning",
      "authors": [
        "Noga Alon",
        "Dmitrii Avdiukhin",
        "Dor Elboim",
        "Orr Fischer",
        "Grigory Yaroslavtsev"
      ],
      "arxiv_id": "2312.00379v1",
      "summary": "Contrastive learning is a highly successful technique for learning representations of data from labeled tuples, specifying the distance relations within the tuple. We study the sample complexity of contrastive learning, i.e. the minimum number of labeled tuples sufficient for getting high generalization accuracy. We give tight bounds on the sample complexity in a variety of settings, focusing on arbitrary distance functions, both general $\\ell_p$-distances, and tree metrics. Our main result is an (almost) optimal bound on the sample complexity of learning $\\ell_p$-distances for integer $p$. For any $p \\ge 1$ we show that $\\tilde Θ(\\min(nd,n^2))$ labeled tuples are necessary and sufficient for learning $d$-dimensional representations of $n$-point datasets. Our results hold for an arbitrary distribution of the input samples and are based on giving the corresponding bounds on the Vapnik-Chervonenkis/Natarajan dimension of the associated problems. We further show that the theoretical bounds on sample complexity obtained via VC/Natarajan dimension can have strong predictive power for experimental results, in contrast with the folklore belief about a substantial gap between the statistical learning theory and the practice of deep learning.",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "published": "2023-12-01",
      "updated": "2023-12-01",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00379v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]contrastive learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Efficient Off-Policy Safe Reinforcement Learning Using Trust Region Conditional Value at Risk",
      "authors": [
        "Dohyeong Kim",
        "Songhwai Oh"
      ],
      "arxiv_id": "2312.00342v1",
      "summary": "This paper aims to solve a safe reinforcement learning (RL) problem with risk measure-based constraints. As risk measures, such as conditional value at risk (CVaR), focus on the tail distribution of cost signals, constraining risk measures can effectively prevent a failure in the worst case. An on-policy safe RL method, called TRC, deals with a CVaR-constrained RL problem using a trust region method and can generate policies with almost zero constraint violations with high returns. However, to achieve outstanding performance in complex environments and satisfy safety constraints quickly, RL methods are required to be sample efficient. To this end, we propose an off-policy safe RL method with CVaR constraints, called off-policy TRC. If off-policy data from replay buffers is directly used to train TRC, the estimation error caused by the distributional shift results in performance degradation. To resolve this issue, we propose novel surrogate functions, in which the effect of the distributional shift can be reduced, and introduce an adaptive trust-region constraint to ensure a policy not to deviate far from replay buffers. The proposed method has been evaluated in simulation and real-world environments and satisfied safety constraints within a few steps while achieving high returns even in complex robotic tasks.",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "published": "2023-12-01",
      "updated": "2023-12-01",
      "comment": "RA-L and IROS 2022",
      "doi": "10.1109/LRA.2022.3184793",
      "journal_ref": "IEEE Robotics and Automation Letters, vol. 7, no. 3, pp. 7644-7651, July 2022",
      "pdf_url": "https://arxiv.org/pdf/2312.00342v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Hypergraph Node Representation Learning with One-Stage Message Passing",
      "authors": [
        "Shilin Qu",
        "Weiqing Wang",
        "Yuan-Fang Li",
        "Xin Zhou",
        "Fajie Yuan"
      ],
      "arxiv_id": "2312.00336v1",
      "summary": "Hypergraphs as an expressive and general structure have attracted considerable attention from various research domains. Most existing hypergraph node representation learning techniques are based on graph neural networks, and thus adopt the two-stage message passing paradigm (i.e. node -> hyperedge -> node). This paradigm only focuses on local information propagation and does not effectively take into account global information, resulting in less optimal representations. Our theoretical analysis of representative two-stage message passing methods shows that, mathematically, they model different ways of local message passing through hyperedges, and can be unified into one-stage message passing (i.e. node -> node). However, they still only model local information. Motivated by this theoretical analysis, we propose a novel one-stage message passing paradigm to model both global and local information propagation for hypergraphs. We integrate this paradigm into HGraphormer, a Transformer-based framework for hypergraph node representation learning. HGraphormer injects the hypergraph structure information (local information) into Transformers (global information) by combining the attention matrix and hypergraph Laplacian. Extensive experiments demonstrate that HGraphormer outperforms recent hypergraph learning methods on five representative benchmark datasets on the semi-supervised hypernode classification task, setting new state-of-the-art performance, with accuracy improvements between 2.52% and 6.70%. Our code and datasets are available.",
      "categories": [
        "cs.LG",
        "cs.IR"
      ],
      "primary_category": "cs.LG",
      "published": "2023-12-01",
      "updated": "2023-12-01",
      "comment": "11 pages",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00336v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]representation learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "TrackDiffusion: Tracklet-Conditioned Video Generation via Diffusion Models",
      "authors": [
        "Pengxiang Li",
        "Kai Chen",
        "Zhili Liu",
        "Ruiyuan Gao",
        "Lanqing Hong",
        "Guo Zhou",
        "Hua Yao",
        "Dit-Yan Yeung",
        "Huchuan Lu",
        "Xu Jia"
      ],
      "arxiv_id": "2312.00651v2",
      "summary": "Despite remarkable achievements in video synthesis, achieving granular control over complex dynamics, such as nuanced movement among multiple interacting objects, still presents a significant hurdle for dynamic world modeling, compounded by the necessity to manage appearance and disappearance, drastic scale changes, and ensure consistency for instances across frames. These challenges hinder the development of video generation that can faithfully mimic real-world complexity, limiting utility for applications requiring high-level realism and controllability, including advanced scene simulation and training of perception systems. To address that, we propose TrackDiffusion, a novel video generation framework affording fine-grained trajectory-conditioned motion control via diffusion models, which facilitates the precise manipulation of the object trajectories and interactions, overcoming the prevalent limitation of scale and continuity disruptions. A pivotal component of TrackDiffusion is the instance enhancer, which explicitly ensures inter-frame consistency of multiple objects, a critical factor overlooked in the current literature. Moreover, we demonstrate that generated video sequences by our TrackDiffusion can be used as training data for visual perception models. To the best of our knowledge, this is the first work to apply video diffusion models with tracklet conditions and demonstrate that generated frames can be beneficial for improving the performance of object trackers.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2023-12-01",
      "updated": "2024-03-20",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00651v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "world model"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 3.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "WeGeFT: Weight-Generative Fine-Tuning for Multi-Faceted Efficient Adaptation of Large Models",
      "authors": [
        "Chinmay Savadikar",
        "Xi Song",
        "Tianfu Wu"
      ],
      "arxiv_id": "2312.00700v5",
      "summary": "Fine-tuning large pretrained Transformer models can focus on either introducing a small number of new learnable parameters (parameter efficiency) or editing representations of a small number of tokens using lightweight modules (representation efficiency). While the pioneering method LoRA (Low-Rank Adaptation) inherently balances parameter, compute, and memory efficiency, many subsequent variants trade off compute and memory efficiency and/or performance to further reduce fine-tuning parameters. To address this limitation and unify parameter-efficient and representation-efficient fine-tuning, we propose Weight-Generative Fine-Tuning (WeGeFT, pronounced wee-gift), a novel approach that learns to generate fine-tuning weights directly from the pretrained weights. WeGeFT employs a simple low-rank formulation consisting of two linear layers, either shared across multiple layers of the pretrained model or individually learned for different layers. This design achieves multi-faceted efficiency in parameters, representations, compute, and memory, while maintaining or exceeding the performance of LoRA and its variants. Extensive experiments on commonsense reasoning, arithmetic reasoning, instruction following, code generation, and visual recognition verify the effectiveness of our proposed WeGeFT. Our code is available at https://github.com/savadikarc/wegeft",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "published": "2023-12-01",
      "updated": "2025-07-13",
      "comment": "Accepted to ICML25",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00700v5",
      "code_links": [
        {
          "url": "https://github.com/savadikarc/wegeft",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "instruction following"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "RadioGalaxyNET: Dataset and Novel Computer Vision Algorithms for the Detection of Extended Radio Galaxies and Infrared Hosts",
      "authors": [
        "Nikhel Gupta",
        "Zeeshan Hayder",
        "Ray P. Norris",
        "Minh Huynh",
        "Lars Petersson"
      ],
      "arxiv_id": "2312.00306v1",
      "summary": "Creating radio galaxy catalogues from next-generation deep surveys requires automated identification of associated components of extended sources and their corresponding infrared hosts. In this paper, we introduce RadioGalaxyNET, a multimodal dataset, and a suite of novel computer vision algorithms designed to automate the detection and localization of multi-component extended radio galaxies and their corresponding infrared hosts. The dataset comprises 4,155 instances of galaxies in 2,800 images with both radio and infrared channels. Each instance provides information about the extended radio galaxy class, its corresponding bounding box encompassing all components, the pixel-level segmentation mask, and the keypoint position of its corresponding infrared host galaxy. RadioGalaxyNET is the first dataset to include images from the highly sensitive Australian Square Kilometre Array Pathfinder (ASKAP) radio telescope, corresponding infrared images, and instance-level annotations for galaxy detection. We benchmark several object detection algorithms on the dataset and propose a novel multimodal approach to simultaneously detect radio galaxies and the positions of infrared hosts.",
      "categories": [
        "astro-ph.IM",
        "astro-ph.CO",
        "astro-ph.GA",
        "cs.CV"
      ],
      "primary_category": "astro-ph.IM",
      "published": "2023-12-01",
      "updated": "2023-12-01",
      "comment": "Accepted for publication in PASA. The paper has 17 pages, 6 figures, 5 tables",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00306v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "multimodal"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "The Cost of Compression: Investigating the Impact of Compression on Parametric Knowledge in Language Models",
      "authors": [
        "Satya Sai Srinath Namburi",
        "Makesh Sreedhar",
        "Srinath Srinivasan",
        "Frederic Sala"
      ],
      "arxiv_id": "2312.00960v1",
      "summary": "Compressing large language models (LLMs), often consisting of billions of parameters, provides faster inference, smaller memory footprints, and enables local deployment. Two standard compression techniques are pruning and quantization, with the former eliminating redundant connections in model layers and the latter representing model parameters with fewer bits. The key tradeoff is between the degree of compression and the impact on the quality of the compressed model. Existing research on LLM compression primarily focuses on performance in terms of general metrics like perplexity or downstream task accuracy. More fine-grained metrics, such as those measuring parametric knowledge, remain significantly underexplored. To help bridge this gap, we present a comprehensive analysis across multiple model families (ENCODER, ENCODER-DECODER, and DECODER) using the LAMA and LM-HARNESS benchmarks in order to systematically quantify the effect of commonly employed compression techniques on model performance. A particular focus is on tradeoffs involving parametric knowledge, with the goal of providing practitioners with practical insights to help make informed decisions on compression. We release our codebase1 to enable further research.",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "published": "2023-12-01",
      "updated": "2023-12-01",
      "comment": "Accepted to EMNLP 2023 Findings",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00960v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Nonparametric Variational Regularisation of Pretrained Transformers",
      "authors": [
        "Fabio Fehr",
        "James Henderson"
      ],
      "arxiv_id": "2312.00662v1",
      "summary": "The current paradigm of large-scale pre-training and fine-tuning Transformer large language models has lead to significant improvements across the board in natural language processing. However, such large models are susceptible to overfitting to their training data, and as a result the models perform poorly when the domain changes. Also, due to the model's scale, the cost of fine-tuning the model to the new domain is large. Nonparametric Variational Information Bottleneck (NVIB) has been proposed as a regulariser for training cross-attention in Transformers, potentially addressing the overfitting problem. We extend the NVIB framework to replace all types of attention functions in Transformers, and show that existing pretrained Transformers can be reinterpreted as Nonparametric Variational (NV) models using a proposed identity initialisation. We then show that changing the initialisation introduces a novel, information-theoretic post-training regularisation in the attention mechanism, which improves out-of-domain generalisation without any training. This success supports the hypothesis that pretrained Transformers are implicitly NV Bayesian models.",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "published": "2023-12-01",
      "updated": "2023-12-01",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00662v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Pathway to a fully data-driven geotechnics: lessons from materials informatics",
      "authors": [
        "Stephen Wu",
        "Yu Otake",
        "Yosuke Higo",
        "Ikumasa Yoshida"
      ],
      "arxiv_id": "2312.00581v1",
      "summary": "This paper elucidates the challenges and opportunities inherent in integrating data-driven methodologies into geotechnics, drawing inspiration from the success of materials informatics. Highlighting the intricacies of soil complexity, heterogeneity, and the lack of comprehensive data, the discussion underscores the pressing need for community-driven database initiatives and open science movements. By leveraging the transformative power of deep learning, particularly in feature extraction from high-dimensional data and the potential of transfer learning, we envision a paradigm shift towards a more collaborative and innovative geotechnics field. The paper concludes with a forward-looking stance, emphasizing the revolutionary potential brought about by advanced computational tools like large language models in reshaping geotechnics informatics.",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "published": "2023-12-01",
      "updated": "2023-12-01",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00581v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "A Bayesian approach for prompt optimization in pre-trained language models",
      "authors": [
        "Antonio Sabbatella",
        "Andrea Ponti",
        "Antonio Candelieri",
        "Ilaria Giordani",
        "Francesco Archetti"
      ],
      "arxiv_id": "2312.00471v1",
      "summary": "A prompt is a sequence of symbol or tokens, selected from a vocabulary according to some rule, which is prepended/concatenated to a textual query. A key problem is how to select the sequence of tokens: in this paper we formulate it as a combinatorial optimization problem. The high dimensionality of the token space com-pounded by the length of the prompt sequence requires a very efficient solution. In this paper we propose a Bayesian optimization method, executed in a continuous em-bedding of the combinatorial space. In this paper we focus on hard prompt tuning (HPT) which directly searches for discrete tokens to be added to the text input with-out requiring access to the large language model (LLM) and can be used also when LLM is available only as a black-box. This is critically important if LLMs are made available in the Model as a Service (MaaS) manner as in GPT-4. The current manu-script is focused on the optimization of discrete prompts for classification tasks. The discrete prompts give rise to difficult combinatorial optimization problem which easily become intractable given the dimension of the token space in realistic applications. The optimization method considered in this paper is Bayesian optimization (BO) which has become the dominant approach in black-box optimization for its sample efficiency along with its modular structure and versatility. In this paper we use BoTorch, a library for Bayesian optimization research built on top of pyTorch. Albeit preliminary and obtained using a 'vanilla' version of BO, the experiments on RoB-ERTa on six benchmarks, show a good performance across a variety of tasks and enable an analysis of the tradeoff between size of the search space, accuracy and wall clock time.",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "published": "2023-12-01",
      "updated": "2023-12-01",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00471v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "PEFTDebias : Capturing debiasing information using PEFTs",
      "authors": [
        "Sumit Agarwal",
        "Aditya Srikanth Veerubhotla",
        "Srijan Bansal"
      ],
      "arxiv_id": "2312.00434v1",
      "summary": "The increasing use of foundation models highlights the urgent need to address and eliminate implicit biases present in them that arise during pretraining. In this paper, we introduce PEFTDebias, a novel approach that employs parameter-efficient fine-tuning (PEFT) to mitigate the biases within foundation models. PEFTDebias consists of two main phases: an upstream phase for acquiring debiasing parameters along a specific bias axis, and a downstream phase where these parameters are incorporated into the model and frozen during the fine-tuning process. By evaluating on four datasets across two bias axes namely gender and race, we find that downstream biases can be effectively reduced with PEFTs. In addition, we show that these parameters possess axis-specific debiasing characteristics, enabling their effective transferability in mitigating biases in various downstream tasks. To ensure reproducibility, we release the code to do our experiments.",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "published": "2023-12-01",
      "updated": "2023-12-01",
      "comment": "EMNLP 2023",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00434v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "foundation model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "GFN-SR: Symbolic Regression with Generative Flow Networks",
      "authors": [
        "Sida Li",
        "Ioana Marinescu",
        "Sebastian Musslick"
      ],
      "arxiv_id": "2312.00396v1",
      "summary": "Symbolic regression (SR) is an area of interpretable machine learning that aims to identify mathematical expressions, often composed of simple functions, that best fit in a given set of covariates $X$ and response $y$. In recent years, deep symbolic regression (DSR) has emerged as a popular method in the field by leveraging deep reinforcement learning to solve the complicated combinatorial search problem. In this work, we propose an alternative framework (GFN-SR) to approach SR with deep learning. We model the construction of an expression tree as traversing through a directed acyclic graph (DAG) so that GFlowNet can learn a stochastic policy to generate such trees sequentially. Enhanced with an adaptive reward baseline, our method is capable of generating a diverse set of best-fitting expressions. Notably, we observe that GFN-SR outperforms other SR algorithms in noisy data regimes, owing to its ability to learn a distribution of rewards over a space of candidate solutions.",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "published": "2023-12-01",
      "updated": "2023-12-01",
      "comment": "Accepted by the NeurIPS 2023 AI4Science Workshop",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00396v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "deep reinforcement learning"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Deciphering Digital Detectives: Understanding LLM Behaviors and Capabilities in Multi-Agent Mystery Games",
      "authors": [
        "Dekun Wu",
        "Haochen Shi",
        "Zhiyuan Sun",
        "Bang Liu"
      ],
      "arxiv_id": "2312.00746v2",
      "summary": "In this study, we explore the application of Large Language Models (LLMs) in \\textit{Jubensha}, a Chinese detective role-playing game and a novel area in Artificial Intelligence (AI) driven gaming. We introduce the first dataset specifically for Jubensha, including character scripts and game rules, to foster AI agent development in this complex narrative environment. Our work also presents a unique multi-agent interaction framework using LLMs, allowing AI agents to autonomously engage in this game. To evaluate the gaming performance of these AI agents, we developed novel methods measuring their mastery of case information and reasoning skills. Furthermore, we incorporated the latest advancements in in-context learning to improve the agents' performance in information gathering, murderer identification, and logical reasoning. The experimental results validate the effectiveness of our proposed methods. This work aims to offer a novel perspective on understanding LLM capabilities and establish a new benchmark for evaluating large language model-based agents.",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "published": "2023-12-01",
      "updated": "2024-02-29",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00746v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Agent-OM: Leveraging LLM Agents for Ontology Matching",
      "authors": [
        "Zhangcheng Qiang",
        "Weiqing Wang",
        "Kerry Taylor"
      ],
      "arxiv_id": "2312.00326v23",
      "summary": "Ontology matching (OM) enables semantic interoperability between different ontologies and resolves their conceptual heterogeneity by aligning related entities. OM systems currently have two prevailing design paradigms: conventional knowledge-based expert systems and newer machine learning-based predictive systems. While large language models (LLMs) and LLM agents have revolutionised data engineering and have been applied creatively in many domains, their potential for OM remains underexplored. This study introduces a novel agent-powered LLM-based design paradigm for OM systems. With consideration of several specific challenges in leveraging LLM agents for OM, we propose a generic framework, namely Agent-OM (Agent for Ontology Matching), consisting of two Siamese agents for retrieval and matching, with a set of OM tools. Our framework is implemented in a proof-of-concept system. Evaluations of three Ontology Alignment Evaluation Initiative (OAEI) tracks over state-of-the-art OM systems show that our system can achieve results very close to the long-standing best performance on simple OM tasks and can significantly improve the performance on complex and few-shot OM tasks.",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "published": "2023-12-01",
      "updated": "2025-12-09",
      "comment": "31 pages",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00326v23",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Mark My Words: Analyzing and Evaluating Language Model Watermarks",
      "authors": [
        "Julien Piet",
        "Chawin Sitawarin",
        "Vivian Fang",
        "Norman Mu",
        "David Wagner"
      ],
      "arxiv_id": "2312.00273v3",
      "summary": "The capabilities of large language models have grown significantly in recent years and so too have concerns about their misuse. It is important to be able to distinguish machine-generated text from human-authored content. Prior works have proposed numerous schemes to watermark text, which would benefit from a systematic evaluation framework. This work focuses on LLM output watermarking techniques - as opposed to image or model watermarks - and proposes Mark My Words, a comprehensive benchmark for them under different natural language tasks. We focus on three main metrics: quality, size (i.e., the number of tokens needed to detect a watermark), and tamper resistance (i.e., the ability to detect a watermark after perturbing marked text). Current watermarking techniques are nearly practical enough for real-world use: Kirchenbauer et al. [33]'s scheme can watermark models like Llama 2 7B-chat or Mistral-7B-Instruct with no perceivable loss in quality on natural language tasks, the watermark can be detected with fewer than 100 tokens, and their scheme offers good tamper resistance to simple perturbations. However, they struggle to efficiently watermark code generations. We publicly release our benchmark (https://github.com/wagner-group/MarkMyWords).",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "published": "2023-12-01",
      "updated": "2024-10-11",
      "comment": "22 pages, 18 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00273v3",
      "code_links": [
        {
          "url": "https://github.com/wagner-group/MarkMyWords",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Instruction-tuning Aligns LLMs to the Human Brain",
      "authors": [
        "Khai Loong Aw",
        "Syrielle Montariol",
        "Badr AlKhamissi",
        "Martin Schrimpf",
        "Antoine Bosselut"
      ],
      "arxiv_id": "2312.00575v2",
      "summary": "Instruction-tuning is a widely adopted finetuning method that enables large language models (LLMs) to generate output that more closely resembles human responses. However, no studies have shown that instruction-tuning actually teaches LLMs to process language in a similar manner as humans. We investigate the effect of instruction-tuning on aligning LLM and human language processing mechanisms in two ways: (1) brain alignment, the similarity of LLM internal representations to neural activity in the human language system, and (2) behavioral alignment, the similarity of LLM and human behavior on a reading task. We assess 25 vanilla and instruction-tuned LLMs on three datasets involving humans reading naturalistic stories and sentences, and find that instruction-tuning generally enhances brain alignment (~6%), but has no similar effect on behavioral alignment. To identify factors underlying this improvement in brain alignment, we compute correlations between brain alignment and various LLM properties, such as model size, problem-solving, and world knowledge understanding. Notably, we find a strong positive correlation between brain alignment and model size (r = 0.95), as well as performance on tasks requiring world knowledge (r = 0.81). Our results demonstrate that instruction-tuning LLMs improves both world knowledge representations and brain alignment, suggesting that the mechanisms that encode world knowledge in LLMs also improve representational alignment to the human brain.",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "published": "2023-12-01",
      "updated": "2024-08-09",
      "comment": "COLM 2024",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00575v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Improving Unsupervised Relation Extraction by Augmenting Diverse Sentence Pairs",
      "authors": [
        "Qing Wang",
        "Kang Zhou",
        "Qiao Qiao",
        "Yuepei Li",
        "Qi Li"
      ],
      "arxiv_id": "2312.00552v1",
      "summary": "Unsupervised relation extraction (URE) aims to extract relations between named entities from raw text without requiring manual annotations or pre-existing knowledge bases. In recent studies of URE, researchers put a notable emphasis on contrastive learning strategies for acquiring relation representations. However, these studies often overlook two important aspects: the inclusion of diverse positive pairs for contrastive learning and the exploration of appropriate loss functions. In this paper, we propose AugURE with both within-sentence pairs augmentation and augmentation through cross-sentence pairs extraction to increase the diversity of positive pairs and strengthen the discriminative power of contrastive learning. We also identify the limitation of noise-contrastive estimation (NCE) loss for relation representation learning and propose to apply margin loss for sentence pairs. Experiments on NYT-FB and TACRED datasets demonstrate that the proposed relation representation learning and a simple K-Means clustering achieves state-of-the-art performance.",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "published": "2023-12-01",
      "updated": "2023-12-01",
      "comment": "Accepted by EMNLP 2023 Main Conference",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00552v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "representation learning",
            "contrastive learning"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Dense Optical Tracking: Connecting the Dots",
      "authors": [
        "Guillaume Le Moing",
        "Jean Ponce",
        "Cordelia Schmid"
      ],
      "arxiv_id": "2312.00786v3",
      "summary": "Recent approaches to point tracking are able to recover the trajectory of any scene point through a large portion of a video despite the presence of occlusions. They are, however, too slow in practice to track every point observed in a single frame in a reasonable amount of time. This paper introduces DOT, a novel, simple and efficient method for solving this problem. It first extracts a small set of tracks from key regions at motion boundaries using an off-the-shelf point tracking algorithm. Given source and target frames, DOT then computes rough initial estimates of a dense flow field and visibility mask through nearest-neighbor interpolation, before refining them using a learnable optical flow estimator that explicitly handles occlusions and can be trained on synthetic data with ground-truth correspondences. We show that DOT is significantly more accurate than current optical flow techniques, outperforms sophisticated \"universal\" trackers like OmniMotion, and is on par with, or better than, the best point tracking algorithms like CoTracker while being at least two orders of magnitude faster. Quantitative and qualitative experiments with synthetic and real videos validate the promise of the proposed approach. Code, data, and videos showcasing the capabilities of our approach are available in the project webpage: https://16lemoing.github.io/dot .",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2023-12-01",
      "updated": "2024-03-04",
      "comment": "Accepted to CVPR 2024",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00786v3",
      "code_links": [
        {
          "url": "https://16lemoing.github.io/dot",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "optical flow"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "MorpheuS: Neural Dynamic 360° Surface Reconstruction from Monocular RGB-D Video",
      "authors": [
        "Hengyi Wang",
        "Jingwen Wang",
        "Lourdes Agapito"
      ],
      "arxiv_id": "2312.00778v2",
      "summary": "Neural rendering has demonstrated remarkable success in dynamic scene reconstruction. Thanks to the expressiveness of neural representations, prior works can accurately capture the motion and achieve high-fidelity reconstruction of the target object. Despite this, real-world video scenarios often feature large unobserved regions where neural representations struggle to achieve realistic completion. To tackle this challenge, we introduce MorpheuS, a framework for dynamic 360° surface reconstruction from a casually captured RGB-D video. Our approach models the target scene as a canonical field that encodes its geometry and appearance, in conjunction with a deformation field that warps points from the current frame to the canonical space. We leverage a view-dependent diffusion prior and distill knowledge from it to achieve realistic completion of unobserved regions. Experimental results on various real-world and synthetic datasets show that our method can achieve high-fidelity 360° surface reconstruction of a deformable object from a monocular RGB-D video.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2023-12-01",
      "updated": "2024-04-04",
      "comment": "CVPR2024. Project page: https://hengyiwang.github.io/projects/morpheus",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00778v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "scene reconstruction"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "A Data-Driven Safety Preserving Control Architecture for Constrained Cyber-Physical Systems",
      "authors": [
        "Mehran Attar",
        "Walter Lucia"
      ],
      "arxiv_id": "2312.00658v2",
      "summary": "In this paper, we propose a data-driven networked control architecture for unknown and constrained cyber-physical systems capable of detecting networked false-data-injection attacks and ensuring plant's safety. In particular, on the controller's side, we design a novel robust anomaly detector that can discover the presence of network attacks using a data-driven outer approximation of the expected robust one-step reachable set. On the other hand, on the plant's side, we design a data-driven safety verification module, which resorts to worst-case arguments to determine if the received control input is safe for the plant's evolution. Whenever necessary, the same module is in charge of replacing the networked controller with a local data-driven set-theoretic model predictive controller, whose objective is to keep the plant's trajectory in a pre-established safe configuration until an attack-free condition is recovered. Numerical simulations involving a two-tank water system illustrate the features and capabilities of the proposed control architecture.",
      "categories": [
        "eess.SY"
      ],
      "primary_category": "eess.SY",
      "published": "2023-12-01",
      "updated": "2024-02-21",
      "comment": "Preprint submitted to the International Journal of Robust and Nonlinear Control",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2312.00658v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "model predictive control"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    }
  ],
  "query_info": {
    "tags": [
      "cs.RO",
      "cs.CV",
      "cs.GR",
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "eess.SY"
    ],
    "date_ranges": [
      [
        "2023-11-30T20:00:00-05:00",
        "2023-12-01T20:00:00-05:00",
        "2023-12-01"
      ]
    ],
    "filtered": true
  }
}