{
    "papers": [
        {
            "title": "Voxify3D: Pixel Art Meets Volumetric Rendering",
            "authors": [
                "Yi-Chuan Huang",
                "Jiewen Chan",
                "Hao-Jen Chien",
                "Yu-Lun Liu"
            ],
            "arxiv_id": "2512.07834v1",
            "summary": "Voxel art is a distinctive stylization widely used in games and digital media, yet automated generation from 3D meshes remains challenging due to conflicting requirements of geometric abstraction, semantic preservation, and discrete color coherence. Existing methods either over-simplify geometry or fail to achieve the pixel-precise, palette-constrained aesthetics of voxel art. We introduce Voxify3D, a differentiable two-stage framework bridging 3D mesh optimization with 2D pixel art supervision. Our core innovation lies in the synergistic integration of three components: (1) orthographic pixel art supervision that eliminates perspective distortion for precise voxel-pixel alignment; (2) patch-based CLIP alignment that preserves semantics across discretization levels; (3) palette-constrained Gumbel-Softmax quantization enabling differentiable optimization over discrete color spaces with controllable palette strategies. This integration addresses fundamental challenges: semantic preservation under extreme discretization, pixel-art aesthetics through volumetric rendering, and end-to-end discrete optimization. Experiments show superior performance (37.12 CLIP-IQA, 77.90\\% user preference) across diverse characters and controllable abstraction (2-8 colors, 20x-50x resolutions). Project page: https://yichuanh.github.io/Voxify-3D/",
            "headline_zh": "提出Voxify3D框架，通过可微分两阶段优化解决3D网格到体素艺术的自动生成难题。",
            "intro_zh": [
                "核心问题：体素艺术自动生成需平衡几何抽象、语义保持和离散颜色一致性，现有方法难以兼顾。",
                "方法要点：结合正交像素艺术监督、基于补丁的CLIP对齐和调色板约束Gumbel-Softmax量化，实现端到端优化。",
                "实验或效果：在多样角色上表现优异（CLIP-IQA 37.12，用户偏好77.90%），支持可控抽象（2-8颜色，20x-50x分辨率）。"
            ],
            "tags_zh": [
                "体素艺术生成",
                "可微分渲染",
                "离散优化",
                "语义保持",
                "像素艺术监督",
                "调色板约束"
            ],
            "_index": 0
        },
        {
            "title": "Relational Visual Similarity",
            "authors": [
                "Thao Nguyen",
                "Sicheng Mo",
                "Krishna Kumar Singh",
                "Yilin Wang",
                "Jing Shi",
                "Nicholas Kolkin",
                "Eli Shechtman",
                "Yong Jae Lee",
                "Yuheng Li"
            ],
            "arxiv_id": "2512.07833v1",
            "summary": "Humans do not just see attribute similarity -- we also see relational similarity. An apple is like a peach because both are reddish fruit, but the Earth is also like a peach: its crust, mantle, and core correspond to the peach's skin, flesh, and pit. This ability to perceive and recognize relational similarity, is arguable by cognitive scientist to be what distinguishes humans from other species. Yet, all widely used visual similarity metrics today (e.g., LPIPS, CLIP, DINO) focus solely on perceptual attribute similarity and fail to capture the rich, often surprising relational similarities that humans perceive. How can we go beyond the visible content of an image to capture its relational properties? How can we bring images with the same relational logic closer together in representation space? To answer these questions, we first formulate relational image similarity as a measurable problem: two images are relationally similar when their internal relations or functions among visual elements correspond, even if their visual attributes differ. We then curate 114k image-caption dataset in which the captions are anonymized -- describing the underlying relational logic of the scene rather than its surface content. Using this dataset, we finetune a Vision-Language model to measure the relational similarity between images. This model serves as the first step toward connecting images by their underlying relational structure rather than their visible appearance. Our study shows that while relational similarity has a lot of real-world applications, existing image similarity models fail to capture it -- revealing a critical gap in visual computing.",
            "headline_zh": "提出关系视觉相似性度量方法，通过匿名化数据集微调视觉语言模型以捕捉图像间关系逻辑。",
            "intro_zh": [
                "核心问题：现有视觉相似性度量（如LPIPS、CLIP）仅关注感知属性相似性，无法捕捉人类感知的关系相似性。",
                "方法要点：构建114k匿名化图像-描述数据集，描述场景关系逻辑而非表面内容，并微调视觉语言模型以测量关系相似性。",
                "实验或效果：模型能连接图像底层关系结构，揭示现有模型在关系相似性捕捉上的关键差距，具有实际应用潜力。"
            ],
            "tags_zh": [
                "关系视觉相似性",
                "匿名化数据集",
                "视觉语言模型微调",
                "图像关系逻辑",
                "相似性度量"
            ],
            "_index": 1
        },
        {
            "title": "Do Generalisation Results Generalise?",
            "authors": [
                "Matteo Boglioni",
                "Andrea Sgobbi",
                "Gabriel Tavernini",
                "Francesco Rita",
                "Marius Mosbach",
                "Tiago Pimentel"
            ],
            "arxiv_id": "2512.07832v1",
            "summary": "A large language model's (LLM's) out-of-distribution (OOD) generalisation ability is crucial to its deployment. Previous work assessing LLMs' generalisation performance, however, typically focuses on a single out-of-distribution dataset. This approach may fail to precisely evaluate the capabilities of the model, as the data shifts encountered once a model is deployed are much more diverse. In this work, we investigate whether OOD generalisation results generalise. More specifically, we evaluate a model's performance across multiple OOD testsets throughout a finetuning run; we then evaluate the partial correlation of performances across these testsets, regressing out in-domain performance. This allows us to assess how correlated are generalisation performances once in-domain performance is controlled for. Analysing OLMo2 and OPT, we observe no overarching trend in generalisation results: the existence of a positive or negative correlation between any two OOD testsets depends strongly on the specific choice of model analysed.",
            "headline_zh": "评估大语言模型在多个分布外数据集上的泛化结果相关性",
            "intro_zh": [
                "核心问题：现有评估通常基于单一分布外数据集，可能无法准确反映模型部署时的多样数据偏移。",
                "方法要点：通过微调过程中评估多个分布外测试集，并控制域内性能后计算部分相关性。",
                "实验或效果：分析OLMo2和OPT模型，发现泛化结果间相关性无统一趋势，取决于具体模型选择。"
            ],
            "tags_zh": [
                "大语言模型",
                "分布外泛化",
                "性能评估",
                "微调分析",
                "相关性研究"
            ],
            "_index": 2
        },
        {
            "title": "UnityVideo: Unified Multi-Modal Multi-Task Learning for Enhancing World-Aware Video Generation",
            "authors": [
                "Jiehui Huang",
                "Yuechen Zhang",
                "Xu He",
                "Yuan Gao",
                "Zhi Cen",
                "Bin Xia",
                "Yan Zhou",
                "Xin Tao",
                "Pengfei Wan",
                "Jiaya Jia"
            ],
            "arxiv_id": "2512.07831v1",
            "summary": "Recent video generation models demonstrate impressive synthesis capabilities but remain limited by single-modality conditioning, constraining their holistic world understanding. This stems from insufficient cross-modal interaction and limited modal diversity for comprehensive world knowledge representation. To address these limitations, we introduce UnityVideo, a unified framework for world-aware video generation that jointly learns across multiple modalities (segmentation masks, human skeletons, DensePose, optical flow, and depth maps) and training paradigms. Our approach features two core components: (1) dynamic noising to unify heterogeneous training paradigms, and (2) a modality switcher with an in-context learner that enables unified processing via modular parameters and contextual learning. We contribute a large-scale unified dataset with 1.3M samples. Through joint optimization, UnityVideo accelerates convergence and significantly enhances zero-shot generalization to unseen data. We demonstrate that UnityVideo achieves superior video quality, consistency, and improved alignment with physical world constraints. Code and data can be found at: https://github.com/dvlab-research/UnityVideo",
            "headline_zh": "提出UnityVideo统一多模态多任务学习框架，以增强世界感知视频生成能力。",
            "intro_zh": [
                "核心问题：现有视频生成模型受限于单模态条件，缺乏跨模态交互和模态多样性，影响世界理解。",
                "方法要点：采用动态噪声统一异构训练范式，结合模态切换器和上下文学习器实现多模态统一处理。",
                "实验或效果：构建大规模数据集，通过联合优化加速收敛，提升零样本泛化能力，改善视频质量和物理一致性。"
            ],
            "tags_zh": [
                "多模态视频生成",
                "世界感知学习",
                "统一训练框架",
                "零样本泛化",
                "动态噪声",
                "上下文学习"
            ],
            "_index": 3
        },
        {
            "title": "One Layer Is Enough: Adapting Pretrained Visual Encoders for Image Generation",
            "authors": [
                "Yuan Gao",
                "Chen Chen",
                "Tianrong Chen",
                "Jiatao Gu"
            ],
            "arxiv_id": "2512.07829v1",
            "summary": "Visual generative models (e.g., diffusion models) typically operate in compressed latent spaces to balance training efficiency and sample quality. In parallel, there has been growing interest in leveraging high-quality pre-trained visual representations, either by aligning them inside VAEs or directly within the generative model. However, adapting such representations remains challenging due to fundamental mismatches between understanding-oriented features and generation-friendly latent spaces. Representation encoders benefit from high-dimensional latents that capture diverse hypotheses for masked regions, whereas generative models favor low-dimensional latents that must faithfully preserve injected noise. This discrepancy has led prior work to rely on complex objectives and architectures. In this work, we propose FAE (Feature Auto-Encoder), a simple yet effective framework that adapts pre-trained visual representations into low-dimensional latents suitable for generation using as little as a single attention layer, while retaining sufficient information for both reconstruction and understanding. The key is to couple two separate deep decoders: one trained to reconstruct the original feature space, and a second that takes the reconstructed features as input for image generation. FAE is generic; it can be instantiated with a variety of self-supervised encoders (e.g., DINO, SigLIP) and plugged into two distinct generative families: diffusion models and normalizing flows. Across class-conditional and text-to-image benchmarks, FAE achieves strong performance. For example, on ImageNet 256x256, our diffusion model with CFG attains a near state-of-the-art FID of 1.29 (800 epochs) and 1.70 (80 epochs). Without CFG, FAE reaches the state-of-the-art FID of 1.48 (800 epochs) and 2.08 (80 epochs), demonstrating both high quality and fast learning.",
            "headline_zh": "提出FAE框架，通过单层注意力适配预训练视觉编码器用于图像生成",
            "intro_zh": [
                "核心问题：预训练视觉特征与生成模型潜在空间不匹配，导致适配困难",
                "方法要点：使用两个独立解码器，一个重构特征空间，另一个用于图像生成",
                "实验或效果：在ImageNet等基准上达到接近SOTA的FID，支持扩散模型和归一化流"
            ],
            "tags_zh": [
                "图像生成",
                "预训练视觉编码器",
                "特征适配",
                "扩散模型",
                "归一化流",
                "自监督学习"
            ],
            "_index": 4
        },
        {
            "title": "The Adoption and Usage of AI Agents: Early Evidence from Perplexity",
            "authors": [
                "Jeremy Yang",
                "Noah Yonack",
                "Kate Zyskowski",
                "Denis Yarats",
                "Johnny Ho",
                "Jerry Ma"
            ],
            "arxiv_id": "2512.07828v1",
            "summary": "This paper presents the first large-scale field study of the adoption, usage intensity, and use cases of general-purpose AI agents operating in open-world web environments. Our analysis centers on Comet, an AI-powered browser developed by Perplexity, and its integrated agent, Comet Assistant. Drawing on hundreds of millions of anonymized user interactions, we address three fundamental questions: Who is using AI agents? How intensively are they using them? And what are they using them for? Our findings reveal substantial heterogeneity in adoption and usage across user segments. Earlier adopters, users in countries with higher GDP per capita and educational attainment, and individuals working in digital or knowledge-intensive sectors -- such as digital technology, academia, finance, marketing, and entrepreneurship -- are more likely to adopt or actively use the agent. To systematically characterize the substance of agent usage, we introduce a hierarchical agentic taxonomy that organizes use cases across three levels: topic, subtopic, and task. The two largest topics, Productivity & Workflow and Learning & Research, account for 57% of all agentic queries, while the two largest subtopics, Courses and Shopping for Goods, make up 22%. The top 10 out of 90 tasks represent 55% of queries. Personal use constitutes 55% of queries, while professional and educational contexts comprise 30% and 16%, respectively. In the short term, use cases exhibit strong stickiness, but over time users tend to shift toward more cognitively oriented topics. The diffusion of increasingly capable AI agents carries important implications for researchers, businesses, policymakers, and educators, inviting new lines of inquiry into this rapidly emerging class of AI capabilities.",
            "headline_zh": "提出基于Comet Assistant的大规模现场研究，分析开放世界AI代理的采纳、使用强度与用例。",
            "intro_zh": [
                "核心问题：谁在使用AI代理、使用强度如何、用于什么场景。",
                "方法要点：基于数亿匿名用户交互数据，引入分层代理分类法组织用例。",
                "实验或效果：发现采纳与使用存在显著异质性，用例以生产力和学习为主，短期粘性强，长期转向认知导向。"
            ],
            "tags_zh": [
                "AI代理采纳",
                "大规模现场研究",
                "用户行为分析",
                "用例分类",
                "开放世界环境",
                "Perplexity Comet"
            ],
            "_index": 5
        },
        {
            "title": "An Adaptive Multi-Layered Honeynet Architecture for Threat Behavior Analysis via Deep Learning",
            "authors": [
                "Lukas Johannes Möller"
            ],
            "arxiv_id": "2512.07827v1",
            "summary": "The escalating sophistication and variety of cyber threats have rendered static honeypots inadequate, necessitating adaptive, intelligence-driven deception. In this work, ADLAH is introduced: an Adaptive Deep Learning Anomaly Detection Honeynet designed to maximize high-fidelity threat intelligence while minimizing cost through autonomous orchestration of infrastructure. The principal contribution is offered as an end-to-end architectural blueprint and vision for an AI-driven deception platform. Feasibility is evidenced by a functional prototype of the central decision mechanism, in which a reinforcement learning (RL) agent determines, in real time, when sessions should be escalated from low-interaction sensor nodes to dynamically provisioned, high-interaction honeypots. Because sufficient live data were unavailable, field-scale validation is not claimed; instead, design trade-offs and limitations are detailed, and a rigorous roadmap toward empirical evaluation at scale is provided. Beyond selective escalation and anomaly detection, the architecture pursues automated extraction, clustering, and versioning of bot attack chains, a core capability motivated by the empirical observation that exposed services are dominated by automated traffic. Together, these elements delineate a practical path toward cost-efficient capture of high-value adversary behavior, systematic bot versioning, and the production of actionable threat intelligence.",
            "headline_zh": "提出自适应深度学习蜜网ADLAH，通过强化学习实时升级会话以高效捕获威胁行为。",
            "intro_zh": [
                "核心问题：静态蜜罐无法应对复杂网络威胁，需自适应智能欺骗。",
                "方法要点：基于强化学习的决策机制，动态升级会话至高交互蜜罐。",
                "实验或效果：原型验证可行性，但缺乏大规模现场数据，提供详细设计权衡与评估路线图。"
            ],
            "tags_zh": [
                "蜜网架构",
                "深度学习异常检测",
                "强化学习决策",
                "威胁行为分析",
                "自动化攻击链提取"
            ],
            "_index": 6
        },
        {
            "title": "OpenVE-3M: A Large-Scale High-Quality Dataset for Instruction-Guided Video Editing",
            "authors": [
                "Haoyang He",
                "Jie Wang",
                "Jiangning Zhang",
                "Zhucun Xue",
                "Xingyuan Bu",
                "Qiangpeng Yang",
                "Shilei Wen",
                "Lei Xie"
            ],
            "arxiv_id": "2512.07826v1",
            "summary": "The quality and diversity of instruction-based image editing datasets are continuously increasing, yet large-scale, high-quality datasets for instruction-based video editing remain scarce. To address this gap, we introduce OpenVE-3M, an open-source, large-scale, and high-quality dataset for instruction-based video editing. It comprises two primary categories: spatially-aligned edits (Global Style, Background Change, Local Change, Local Remove, Local Add, and Subtitles Edit) and non-spatially-aligned edits (Camera Multi-Shot Edit and Creative Edit). All edit types are generated via a meticulously designed data pipeline with rigorous quality filtering. OpenVE-3M surpasses existing open-source datasets in terms of scale, diversity of edit types, instruction length, and overall quality. Furthermore, to address the lack of a unified benchmark in the field, we construct OpenVE-Bench, containing 431 video-edit pairs that cover a diverse range of editing tasks with three key metrics highly aligned with human judgment. We present OpenVE-Edit, a 5B model trained on our dataset that demonstrates remarkable efficiency and effectiveness by setting a new state-of-the-art on OpenVE-Bench, outperforming all prior open-source models including a 14B baseline. Project page is at https://github.com/lewandofskee/OpenVE.",
            "headline_zh": "提出OpenVE-3M数据集以解决指令引导视频编辑领域缺乏大规模高质量数据的问题",
            "intro_zh": [
                "核心问题：指令引导视频编辑领域缺乏大规模、高质量数据集，阻碍模型发展。",
                "方法要点：构建OpenVE-3M数据集，包含空间对齐和非空间对齐编辑类型，通过精心设计的数据管道和质量过滤确保质量。",
                "实验或效果：基于数据集训练OpenVE-Edit模型，在OpenVE-Bench基准上超越所有开源模型，包括14B基线。"
            ],
            "tags_zh": [
                "指令引导视频编辑",
                "大规模数据集",
                "视频编辑基准",
                "空间对齐编辑",
                "非空间对齐编辑",
                "质量过滤"
            ],
            "_index": 7
        },
        {
            "title": "WorldReel: 4D Video Generation with Consistent Geometry and Motion Modeling",
            "authors": [
                "Shaoheng Fang",
                "Hanwen Jiang",
                "Yunpeng Bai",
                "Niloy J. Mitra",
                "Qixing Huang"
            ],
            "arxiv_id": "2512.07821v1",
            "summary": "Recent video generators achieve striking photorealism, yet remain fundamentally inconsistent in 3D. We present WorldReel, a 4D video generator that is natively spatio-temporally consistent. WorldReel jointly produces RGB frames together with 4D scene representations, including pointmaps, camera trajectory, and dense flow mapping, enabling coherent geometry and appearance modeling over time. Our explicit 4D representation enforces a single underlying scene that persists across viewpoints and dynamic content, yielding videos that remain consistent even under large non-rigid motion and significant camera movement. We train WorldReel by carefully combining synthetic and real data: synthetic data providing precise 4D supervision (geometry, motion, and camera), while real videos contribute visual diversity and realism. This blend allows WorldReel to generalize to in-the-wild footage while preserving strong geometric fidelity. Extensive experiments demonstrate that WorldReel sets a new state-of-the-art for consistent video generation with dynamic scenes and moving cameras, improving metrics of geometric consistency, motion coherence, and reducing view-time artifacts over competing methods. We believe that WorldReel brings video generation closer to 4D-consistent world modeling, where agents can render, interact, and reason about scenes through a single and stable spatiotemporal representation.",
            "headline_zh": "提出WorldReel以解决视频生成中3D不一致问题，通过4D场景表示实现时空一致性。",
            "intro_zh": [
                "现有视频生成器在3D上存在不一致性，WorldReel联合生成RGB帧和4D场景表示（如点云、相机轨迹）。",
                "方法结合合成数据提供精确4D监督和真实视频增强视觉多样性，确保几何保真度和泛化能力。",
                "实验显示WorldReel在动态场景和移动相机下提升几何一致性、运动连贯性，减少视时伪影，优于现有方法。"
            ],
            "tags_zh": [
                "4D视频生成",
                "时空一致性",
                "几何建模",
                "运动建模",
                "合成与真实数据融合",
                "动态场景处理"
            ],
            "_index": 8
        },
        {
            "title": "Graph-Based Learning of Spectro-Topographical EEG Representations with Gradient Alignment for Brain-Computer Interfaces",
            "authors": [
                "Prithila Angkan",
                "Amin Jalali",
                "Paul Hungler",
                "Ali Etemad"
            ],
            "arxiv_id": "2512.07820v1",
            "summary": "We present a novel graph-based learning of EEG representations with gradient alignment (GEEGA) that leverages multi-domain information to learn EEG representations for brain-computer interfaces. Our model leverages graph convolutional networks to fuse embeddings from frequency-based topographical maps and time-frequency spectrograms, capturing inter-domain relationships. GEEGA addresses the challenge of achieving high inter-class separability, which arises from the temporally dynamic and subject-sensitive nature of EEG signals by incorporating the center loss and pairwise difference loss. Additionally, GEEGA incorporates a gradient alignment strategy to resolve conflicts between gradients from different domains and the fused embeddings, ensuring that discrepancies, where gradients point in conflicting directions, are aligned toward a unified optimization direction. We validate the efficacy of our method through extensive experiments on three publicly available EEG datasets: BCI-2a, CL-Drive and CLARE. Comprehensive ablation studies further highlight the impact of various components of our model.",
            "headline_zh": "提出基于图卷积网络与梯度对齐的脑电表征学习方法，用于脑机接口任务。",
            "intro_zh": [
                "核心问题：脑电信号具有时间动态性和个体敏感性，导致类间可分性低。",
                "方法要点：融合频域地形图和时频谱图，结合中心损失和成对差异损失，并采用梯度对齐策略。",
                "实验或效果：在三个公开脑电数据集上验证有效性，并通过消融研究分析组件影响。"
            ],
            "tags_zh": [
                "脑机接口",
                "图卷积网络",
                "脑电表征学习",
                "梯度对齐",
                "多域信息融合"
            ],
            "_index": 9
        },
        {
            "title": "Efficient and Compliant Control Framework for Versatile Human-Humanoid Collaborative Transportation",
            "authors": [
                "Shubham S. Kumbhar",
                "Abhijeet M. Kulkarni",
                "Panagiotis Artemiadis"
            ],
            "arxiv_id": "2512.07819v1",
            "summary": "We present a control framework that enables humanoid robots to perform collaborative transportation tasks with a human partner. The framework supports both translational and rotational motions, which are fundamental to co-transport scenarios. It comprises three components: a high-level planner, a low-level controller, and a stiffness modulation mechanism. At the planning level, we introduce the Interaction Linear Inverted Pendulum (I-LIP), which, combined with an admittance model and an MPC formulation, generates dynamically feasible footstep plans. These are executed by a QP-based whole-body controller that accounts for the coupled humanoid-object dynamics. Stiffness modulation regulates robot-object interaction, ensuring convergence to the desired relative configuration defined by the distance between the object and the robot's center of mass. We validate the effectiveness of the framework through real-world experiments conducted on the Digit humanoid platform. To quantify collaboration quality, we propose an efficiency metric that captures both task performance and inter-agent coordination. We show that this metric highlights the role of compliance in collaborative tasks and offers insights into desirable trajectory characteristics across both high- and low-level control layers. Finally, we showcase experimental results on collaborative behaviors, including translation, turning, and combined motions such as semi circular trajectories, representative of naturally occurring co-transportation tasks.",
            "headline_zh": "提出高效合规控制框架，实现人形机器人与人类协作搬运任务",
            "intro_zh": [
                "核心问题：人形机器人需在协作搬运中支持平移和旋转运动，确保动态可行性和合规性。",
                "方法要点：结合I-LIP规划器、QP全身控制器和刚度调制，生成并执行动态可行的步态计划。",
                "实验或效果：在Digit人形平台上验证，提出效率指标量化协作质量，展示平移、转向等行为。"
            ],
            "tags_zh": [
                "人形机器人控制",
                "协作搬运",
                "动态规划",
                "全身控制",
                "刚度调制",
                "人机交互"
            ],
            "_index": 10
        },
        {
            "title": "Provable Long-Range Benefits of Next-Token Prediction",
            "authors": [
                "Xinyuan Cao",
                "Santosh S. Vempala"
            ],
            "arxiv_id": "2512.07818v1",
            "summary": "Why do modern language models, trained to do well on next-word prediction, appear to generate coherent documents and capture long-range structure? Here we show that next-token prediction is provably powerful for learning longer-range structure, even with common neural network architectures. Specifically, we prove that optimizing next-token prediction over a Recurrent Neural Network (RNN) yields a model that closely approximates the training distribution: for held-out documents sampled from the training distribution, no algorithm of bounded description length limited to examining the next $k$ tokens, for any $k$, can distinguish between $k$ consecutive tokens of such documents and $k$ tokens generated by the learned language model following the same prefix. We provide polynomial bounds (in $k$, independent of the document length) on the model size needed to achieve such $k$-token indistinguishability, offering a complexity-theoretic explanation for the long-range coherence observed in practice.",
            "headline_zh": "证明RNN中下一词预测能学习长程结构，实现k词不可区分性。",
            "intro_zh": [
                "核心问题：解释语言模型通过下一词预测如何生成连贯文档并捕获长程结构。",
                "方法要点：理论证明RNN优化下一词预测可近似训练分布，实现k词不可区分性。",
                "实验或效果：提供多项式模型大小界限，解释实践中观察到的长程一致性。"
            ],
            "tags_zh": [
                "下一词预测",
                "长程结构学习",
                "RNN理论分析",
                "k词不可区分性",
                "语言模型理论"
            ],
            "_index": 11
        },
        {
            "title": "Understanding Privacy Risks in Code Models Through Training Dynamics: A Causal Approach",
            "authors": [
                "Hua Yang",
                "Alejandro Velasco",
                "Sen Fang",
                "Bowen Xu",
                "Denys Poshyvanyk"
            ],
            "arxiv_id": "2512.07814v1",
            "summary": "Large language models for code (LLM4Code) have greatly improved developer productivity but also raise privacy concerns due to their reliance on open-source repositories containing abundant personally identifiable information (PII). Prior work shows that commercial models can reproduce sensitive PII, yet existing studies largely treat PII as a single category and overlook the heterogeneous risks among different types. We investigate whether distinct PII types vary in their likelihood of being learned and leaked by LLM4Code, and whether this relationship is causal. Our methodology includes building a dataset with diverse PII types, fine-tuning representative models of different scales, computing training dynamics on real PII data, and formulating a structural causal model to estimate the causal effect of learnability on leakage. Results show that leakage risks differ substantially across PII types and correlate with their training dynamics: easy-to-learn instances such as IP addresses exhibit higher leakage, while harder types such as keys and passwords leak less frequently. Ambiguous types show mixed behaviors. This work provides the first causal evidence that leakage risks are type-dependent and offers guidance for developing type-aware and learnability-aware defenses for LLM4Code.",
            "headline_zh": "提出基于训练动态的因果方法，揭示代码模型中不同类型PII的隐私泄露风险差异。",
            "intro_zh": [
                "核心问题：代码大模型依赖开源库，可能泄露PII，但现有研究忽视不同类型PII的异质风险。",
                "方法要点：构建多类型PII数据集，微调不同规模模型，计算训练动态，并建立结构因果模型评估因果效应。",
                "实验或效果：结果显示PII泄露风险因类型而异，易学类型如IP地址泄露更高，难学类型如密钥泄露较少。"
            ],
            "tags_zh": [
                "代码大模型",
                "隐私风险",
                "训练动态",
                "因果推断",
                "PII类型",
                "泄露分析"
            ],
            "_index": 12
        },
        {
            "title": "Inchworm-Inspired Soft Robot with Groove-Guided Locomotion",
            "authors": [
                "Hari Prakash Thanabalan",
                "Lars Bengtsson",
                "Ugo Lafont",
                "Giovanni Volpe"
            ],
            "arxiv_id": "2512.07813v1",
            "summary": "Soft robots require directional control to navigate complex terrains. However, achieving such control often requires multiple actuators, which increases mechanical complexity, complicates control systems, and raises energy consumption. Here, we introduce an inchworm-inspired soft robot whose locomotion direction is controlled passively by patterned substrates. The robot employs a single rolled dielectric elastomer actuator, while groove patterns on a 3D-printed substrate guide its alignment and trajectory. Through systematic experiments, we demonstrate that varying groove angles enables precise control of locomotion direction without the need for complex actuation strategies. This groove-guided approach reduces energy consumption, simplifies robot design, and expands the applicability of bio-inspired soft robots in fields such as search and rescue, pipe inspection, and planetary exploration.",
            "headline_zh": "提出基于沟槽引导的尺蠖仿生软体机器人，以简化复杂地形导航中的方向控制",
            "intro_zh": [
                "核心问题：软体机器人方向控制常需多执行器，导致机械复杂、能耗高。",
                "方法要点：使用单卷曲介电弹性体执行器，通过3D打印基板沟槽图案被动引导运动方向。",
                "实验或效果：系统实验表明，改变沟槽角度可实现精确方向控制，降低能耗并简化设计。"
            ],
            "tags_zh": [
                "软体机器人",
                "仿生运动",
                "被动控制",
                "介电弹性体执行器",
                "沟槽引导",
                "简化设计"
            ],
            "_index": 13
        },
        {
            "title": "Auditing Games for Sandbagging",
            "authors": [
                "Jordan Taylor",
                "Sid Black",
                "Dillon Bowen",
                "Thomas Read",
                "Satvik Golechha",
                "Alex Zelenka-Martin",
                "Oliver Makins",
                "Connor Kissane",
                "Kola Ayonrinde",
                "Jacob Merizian",
                "Samuel Marks",
                "Chris Cundy",
                "Joseph Bloom"
            ],
            "arxiv_id": "2512.07810v1",
            "summary": "Future AI systems could conceal their capabilities ('sandbagging') during evaluations, potentially misleading developers and auditors. We stress-tested sandbagging detection techniques using an auditing game. First, a red team fine-tuned five models, some of which conditionally underperformed, as a proxy for sandbagging. Second, a blue team used black-box, model-internals, or training-based approaches to identify sandbagging models. We found that the blue team could not reliably discriminate sandbaggers from benign models. Black-box approaches were defeated by effective imitation of a weaker model. Linear probes, a model-internals approach, showed more promise but their naive application was vulnerable to behaviours instilled by the red team. We also explored capability elicitation as a strategy for detecting sandbagging. Although Prompt-based elicitation was not reliable, training-based elicitation consistently elicited full performance from the sandbagging models, using only a single correct demonstration of the evaluation task. However the performance of benign models was sometimes also raised, so relying on elicitation as a detection strategy was prone to false-positives. In the short-term, we recommend developers remove potential sandbagging using on-distribution training for elicitation. In the longer-term, further research is needed to ensure the efficacy of training-based elicitation, and develop robust methods for sandbagging detection. We open source our model organisms at https://github.com/AI-Safety-Institute/sandbagging_auditing_games and select transcripts and results at https://huggingface.co/datasets/sandbagging-games/evaluation_logs . A demo illustrating the game can be played at https://sandbagging-demo.far.ai/ .",
            "headline_zh": "提出审计游戏以评估AI系统在评估中隐藏能力的检测方法",
            "intro_zh": [
                "核心问题：未来AI系统可能在评估中隐藏能力（沙袋行为），误导开发者和审计者",
                "方法要点：通过红队微调模型模拟沙袋行为，蓝队使用黑盒、模型内部或基于训练的方法进行检测",
                "实验或效果：蓝队无法可靠区分沙袋模型与良性模型，基于训练的激发方法能提升性能但易产生假阳性"
            ],
            "tags_zh": [
                "AI安全审计",
                "沙袋行为检测",
                "模型微调",
                "能力激发",
                "黑盒评估",
                "线性探针"
            ],
            "_index": 14
        },
        {
            "title": "LUNA: LUT-Based Neural Architecture for Fast and Low-Cost Qubit Readout",
            "authors": [
                "M. A. Farooq",
                "G. Di Guglielmo",
                "A. Rajagopala",
                "N. Tran",
                "V. A. Chhabria",
                "A. Arora"
            ],
            "arxiv_id": "2512.07808v1",
            "summary": "Qubit readout is a critical operation in quantum computing systems, which maps the analog response of qubits into discrete classical states. Deep neural networks (DNNs) have recently emerged as a promising solution to improve readout accuracy . Prior hardware implementations of DNN-based readout are resource-intensive and suffer from high inference latency, limiting their practical use in low-latency decoding and quantum error correction (QEC) loops. This paper proposes LUNA, a fast and efficient superconducting qubit readout accelerator that combines low-cost integrator-based preprocessing with Look-Up Table (LUT) based neural networks for classification. The architecture uses simple integrators for dimensionality reduction with minimal hardware overhead, and employs LogicNets (DNNs synthesized into LUT logic) to drastically reduce resource usage while enabling ultra-low-latency inference. We integrate this with a differential evolution based exploration and optimization framework to identify high-quality design points. Our results show up to a 10.95x reduction in area and 30% lower latency with little to no loss in fidelity compared to the state-of-the-art. LUNA enables scalable, low-footprint, and high-speed qubit readout, supporting the development of larger and more reliable quantum computing systems.",
            "headline_zh": "提出LUNA架构，结合LUT神经网络与积分器预处理，实现快速低成本的量子比特读出",
            "intro_zh": [
                "量子比特读出中，基于DNN的硬件实现资源密集且延迟高，限制量子纠错应用",
                "LUNA采用积分器降维和LUT神经网络分类，减少硬件开销并实现超低延迟推理",
                "实验显示面积减少10.95倍，延迟降低30%，保真度损失极小，支持可扩展量子系统"
            ],
            "tags_zh": [
                "量子比特读出",
                "LUT神经网络",
                "硬件加速",
                "低延迟推理",
                "量子计算系统"
            ],
            "_index": 15
        },
        {
            "title": "Lang3D-XL: Language Embedded 3D Gaussians for Large-scale Scenes",
            "authors": [
                "Shai Krakovsky",
                "Gal Fiebelman",
                "Sagie Benaim",
                "Hadar Averbuch-Elor"
            ],
            "arxiv_id": "2512.07807v1",
            "summary": "Embedding a language field in a 3D representation enables richer semantic understanding of spatial environments by linking geometry with descriptive meaning. This allows for a more intuitive human-computer interaction, enabling querying or editing scenes using natural language, and could potentially improve tasks like scene retrieval, navigation, and multimodal reasoning. While such capabilities could be transformative, in particular for large-scale scenes, we find that recent feature distillation approaches cannot effectively learn over massive Internet data due to challenges in semantic feature misalignment and inefficiency in memory and runtime. To this end, we propose a novel approach to address these challenges. First, we introduce extremely low-dimensional semantic bottleneck features as part of the underlying 3D Gaussian representation. These are processed by rendering and passing them through a multi-resolution, feature-based, hash encoder. This significantly improves efficiency both in runtime and GPU memory. Second, we introduce an Attenuated Downsampler module and propose several regularizations addressing the semantic misalignment of ground truth 2D features. We evaluate our method on the in-the-wild HolyScenes dataset and demonstrate that it surpasses existing approaches in both performance and efficiency.",
            "headline_zh": "提出Lang3D-XL方法，通过低维语义瓶颈特征和哈希编码器解决大规模场景中语言嵌入3D高斯的效率与语义对齐问题。",
            "intro_zh": [
                "核心问题：现有特征蒸馏方法在大规模互联网数据上因语义特征错位和内存运行时效率低下而难以有效学习。",
                "方法要点：引入极低维语义瓶颈特征，结合多分辨率哈希编码器提升效率；使用Attenuated Downsampler模块和正则化处理语义错位。",
                "实验或效果：在HolyScenes数据集上评估，性能与效率均超越现有方法。"
            ],
            "tags_zh": [
                "语言嵌入3D高斯",
                "大规模场景理解",
                "语义特征对齐",
                "哈希编码器",
                "效率优化",
                "多模态推理"
            ],
            "_index": 16
        },
        {
            "title": "Multi-view Pyramid Transformer: Look Coarser to See Broader",
            "authors": [
                "Gyeongjin Kang",
                "Seungkwon Yang",
                "Seungtae Nam",
                "Younggeun Lee",
                "Jungwoo Kim",
                "Eunbyung Park"
            ],
            "arxiv_id": "2512.07806v1",
            "summary": "We propose Multi-view Pyramid Transformer (MVP), a scalable multi-view transformer architecture that directly reconstructs large 3D scenes from tens to hundreds of images in a single forward pass. Drawing on the idea of ``looking broader to see the whole, looking finer to see the details,\" MVP is built on two core design principles: 1) a local-to-global inter-view hierarchy that gradually broadens the model's perspective from local views to groups and ultimately the full scene, and 2) a fine-to-coarse intra-view hierarchy that starts from detailed spatial representations and progressively aggregates them into compact, information-dense tokens. This dual hierarchy achieves both computational efficiency and representational richness, enabling fast reconstruction of large and complex scenes. We validate MVP on diverse datasets and show that, when coupled with 3D Gaussian Splatting as the underlying 3D representation, it achieves state-of-the-art generalizable reconstruction quality while maintaining high efficiency and scalability across a wide range of view configurations.",
            "headline_zh": "提出多视图金字塔变换器，通过双层次结构从多图像高效重建大3D场景。",
            "intro_zh": [
                "核心问题：从数十至数百张图像直接重建大规模3D场景，需平衡计算效率与表示丰富性。",
                "方法要点：结合局部到全局的视图间层次和细到粗的视图内层次，实现视角扩展与信息聚合。",
                "实验或效果：在多样化数据集上验证，结合3D高斯泼溅实现高效、可扩展的先进重建质量。"
            ],
            "tags_zh": [
                "多视图3D重建",
                "变换器架构",
                "层次化表示",
                "计算效率",
                "可扩展性",
                "3D高斯泼溅"
            ],
            "_index": 17
        },
        {
            "title": "Group Representational Position Encoding",
            "authors": [
                "Yifan Zhang",
                "Zixiang Chen",
                "Yifeng Liu",
                "Zhen Qin",
                "Huizhuo Yuan",
                "Kangping Xu",
                "Yang Yuan",
                "Quanquan Gu",
                "Andrew Chi-Chih Yao"
            ],
            "arxiv_id": "2512.07805v1",
            "summary": "We present GRAPE (Group RepresentAtional Position Encoding), a unified framework for positional encoding based on group actions. GRAPE brings together two families of mechanisms: (i) multiplicative rotations (Multiplicative GRAPE) in $\\mathrm{SO}(d)$ and (ii) additive logit biases (Additive GRAPE) arising from unipotent actions in the general linear group $\\mathrm{GL}$. In Multiplicative GRAPE, a position $n \\in \\mathbb{Z}$ (or $t \\in \\mathbb{R}$) acts as $\\mathbf{G}(n)=\\exp(n\\,ω\\,\\mathbf{L})$ with a rank-2 skew generator $\\mathbf{L} \\in \\mathbb{R}^{d \\times d}$, yielding a relative, compositional, norm-preserving map with a closed-form matrix exponential. RoPE is recovered exactly when the $d/2$ planes are the canonical coordinate pairs with log-uniform spectrum. Learned commuting subspaces and compact non-commuting mixtures strictly extend this geometry to capture cross-subspace feature coupling at $O(d)$ and $O(r d)$ cost per head, respectively. In Additive GRAPE, additive logits arise as rank-1 (or low-rank) unipotent actions, recovering ALiBi and the Forgetting Transformer (FoX) as exact special cases while preserving an exact relative law and streaming cacheability. Altogether, GRAPE supplies a principled design space for positional geometry in long-context models, subsuming RoPE and ALiBi as special cases. Project Page: https://github.com/model-architectures/GRAPE.",
            "headline_zh": "提出GRAPE统一框架，基于群作用统一位置编码，涵盖RoPE和ALiBi等机制。",
            "intro_zh": [
                "核心问题：统一位置编码机制，解决长上下文模型中的位置几何设计问题。",
                "方法要点：基于群作用，包括乘法旋转和加法对数偏置，支持相对、组合和规范保持映射。",
                "实验或效果：GRAPE扩展了RoPE和ALiBi，提供原则性设计空间，具体效果未知。"
            ],
            "tags_zh": [
                "位置编码",
                "群作用",
                "长上下文模型",
                "相对位置编码",
                "统一框架"
            ],
            "_index": 18
        },
        {
            "title": "OneStory: Coherent Multi-Shot Video Generation with Adaptive Memory",
            "authors": [
                "Zhaochong An",
                "Menglin Jia",
                "Haonan Qiu",
                "Zijian Zhou",
                "Xiaoke Huang",
                "Zhiheng Liu",
                "Weiming Ren",
                "Kumara Kahatapitiya",
                "Ding Liu",
                "Sen He",
                "Chenyang Zhang",
                "Tao Xiang",
                "Fanny Yang",
                "Serge Belongie",
                "Tian Xie"
            ],
            "arxiv_id": "2512.07802v1",
            "summary": "Storytelling in real-world videos often unfolds through multiple shots -- discontinuous yet semantically connected clips that together convey a coherent narrative. However, existing multi-shot video generation (MSV) methods struggle to effectively model long-range cross-shot context, as they rely on limited temporal windows or single keyframe conditioning, leading to degraded performance under complex narratives. In this work, we propose OneStory, enabling global yet compact cross-shot context modeling for consistent and scalable narrative generation. OneStory reformulates MSV as a next-shot generation task, enabling autoregressive shot synthesis while leveraging pretrained image-to-video (I2V) models for strong visual conditioning. We introduce two key modules: a Frame Selection module that constructs a semantically-relevant global memory based on informative frames from prior shots, and an Adaptive Conditioner that performs importance-guided patchification to generate compact context for direct conditioning. We further curate a high-quality multi-shot dataset with referential captions to mirror real-world storytelling patterns, and design effective training strategies under the next-shot paradigm. Finetuned from a pretrained I2V model on our curated 60K dataset, OneStory achieves state-of-the-art narrative coherence across diverse and complex scenes in both text- and image-conditioned settings, enabling controllable and immersive long-form video storytelling.",
            "headline_zh": "提出OneStory以解决多镜头视频生成中长程跨镜头上下文建模不足的问题",
            "intro_zh": [
                "核心问题：现有方法依赖有限时间窗口或单关键帧条件，难以建模复杂叙事下的跨镜头上下文",
                "方法要点：将多镜头视频生成重构为下一镜头生成任务，引入帧选择模块和自适应条件器进行全局紧凑上下文建模",
                "实验或效果：在自建60K数据集上微调，在文本和图像条件下实现最先进的叙事连贯性，支持可控长视频生成"
            ],
            "tags_zh": [
                "多镜头视频生成",
                "跨镜头上下文建模",
                "自适应条件器",
                "下一镜头生成",
                "叙事连贯性",
                "长视频生成"
            ],
            "_index": 19
        },
        {
            "title": "Collaborative Causal Sensemaking: Closing the Complementarity Gap in Human-AI Decision Support",
            "authors": [
                "Raunak Jain",
                "Mudita Khurana"
            ],
            "arxiv_id": "2512.07801v1",
            "summary": "LLM-based agents are rapidly being plugged into expert decision-support, yet in messy, high-stakes settings they rarely make the team smarter: human-AI teams often underperform the best individual, experts oscillate between verification loops and over-reliance, and the promised complementarity does not materialise. We argue this is not just a matter of accuracy, but a fundamental gap in how we conceive AI assistance: expert decisions are made through collaborative cognitive processes where mental models, goals, and constraints are continually co-constructed, tested, and revised between human and AI. We propose Collaborative Causal Sensemaking (CCS) as a research agenda and organizing framework for decision-support agents: systems designed as partners in cognitive work, maintaining evolving models of how particular experts reason, helping articulate and revise goals, co-constructing and stress-testing causal hypotheses, and learning from the outcomes of joint decisions so that both human and agent improve over time. We sketch challenges around training ecologies that make collaborative thinking instrumentally valuable, representations and interaction protocols for co-authored models, and evaluation centred on trust and complementarity. These directions can reframe MAS research around agents that participate in collaborative sensemaking and act as AI teammates that think with their human partners.",
            "headline_zh": "提出协作因果感知框架以解决人机决策支持中的互补性差距问题",
            "intro_zh": [
                "核心问题：人机团队在复杂高风险决策中常表现不佳，互补性未实现，专家在验证与过度依赖间摇摆。",
                "方法要点：将AI设计为认知工作伙伴，共同构建和测试因果假设，学习联合决策结果以促进双方改进。",
                "实验或效果：未知，但提出围绕训练生态、模型表示和以信任与互补性为中心的评价等挑战方向。"
            ],
            "tags_zh": [
                "人机协作决策",
                "因果感知",
                "AI伙伴系统",
                "认知模型",
                "互补性评估"
            ],
            "_index": 20
        },
        {
            "title": "Large Causal Models from Large Language Models",
            "authors": [
                "Sridhar Mahadevan"
            ],
            "arxiv_id": "2512.07796v1",
            "summary": "We introduce a new paradigm for building large causal models (LCMs) that exploits the enormous potential latent in today's large language models (LLMs). We describe our ongoing experiments with an implemented system called DEMOCRITUS (Decentralized Extraction of Manifold Ontologies of Causal Relations Integrating Topos Universal Slices) aimed at building, organizing, and visualizing LCMs that span disparate domains extracted from carefully targeted textual queries to LLMs. DEMOCRITUS is methodologically distinct from traditional narrow domain and hypothesis centered causal inference that builds causal models from experiments that produce numerical data. A high-quality LLM is used to propose topics, generate causal questions, and extract plausible causal statements from a diverse range of domains. The technical challenge is then to take these isolated, fragmented, potentially ambiguous and possibly conflicting causal claims, and weave them into a coherent whole, converting them into relational causal triples and embedding them into a LCM. Addressing this technical challenge required inventing new categorical machine learning methods, which we can only briefly summarize in this paper, as it is focused more on the systems side of building DEMOCRITUS. We describe the implementation pipeline for DEMOCRITUS comprising of six modules, examine its computational cost profile to determine where the current bottlenecks in scaling the system to larger models. We describe the results of using DEMOCRITUS over a wide range of domains, spanning archaeology, biology, climate change, economics, medicine and technology. We discuss the limitations of the current DEMOCRITUS system, and outline directions for extending its capabilities.",
            "headline_zh": "提出DEMOCRITUS系统，利用大语言模型构建跨领域大型因果模型",
            "intro_zh": [
                "核心问题：传统因果推断依赖数值实验，难以构建跨领域大型因果模型。",
                "方法要点：使用大语言模型生成因果陈述，通过新范畴机器学习方法整合为因果三元组。",
                "实验或效果：在考古学、生物学、气候变化等多个领域应用，评估系统扩展瓶颈。"
            ],
            "tags_zh": [
                "大型因果模型",
                "大语言模型",
                "因果推断",
                "范畴机器学习",
                "跨领域建模"
            ],
            "_index": 21
        },
        {
            "title": "ReasonBENCH: Benchmarking the (In)Stability of LLM Reasoning",
            "authors": [
                "Nearchos Potamitis",
                "Lars Klein",
                "Akhil Arora"
            ],
            "arxiv_id": "2512.07795v1",
            "summary": "Large language models (LLMs) are increasingly deployed in settings where reasoning, such as multi-step problem solving and chain-of-thought, is essential. Yet, current evaluation practices overwhelmingly report single-run accuracy while ignoring the intrinsic uncertainty that naturally arises from stochastic decoding. This omission creates a blind spot because practitioners cannot reliably assess whether a method's reported performance is stable, reproducible, or cost-consistent. We introduce ReasonBENCH, the first benchmark designed to quantify the underlying instability in LLM reasoning. ReasonBENCH provides (i) a modular evaluation library that standardizes reasoning frameworks, models, and tasks, (ii) a multi-run protocol that reports statistically reliable metrics for both quality and cost, and (iii) a public leaderboard to encourage variance-aware reporting. Across tasks from different domains, we find that the vast majority of reasoning strategies and models exhibit high instability. Notably, even strategies with similar average performance can display confidence intervals up to four times wider, and the top-performing methods often incur higher and less stable costs. Such instability compromises reproducibility across runs and, consequently, the reliability of reported performance. To better understand these dynamics, we further analyze the impact of prompts, model families, and scale on the trade-off between solve rate and stability. Our results highlight reproducibility as a critical dimension for reliable LLM reasoning and provide a foundation for future reasoning methods and uncertainty quantification techniques. ReasonBENCH is publicly available at https://github.com/au-clan/ReasonBench .",
            "headline_zh": "提出ReasonBENCH基准以量化大语言模型推理的不稳定性",
            "intro_zh": [
                "核心问题：当前评估忽视随机解码导致的不稳定性，影响性能可靠性和可复现性",
                "方法要点：提供模块化评估库、多轮协议和公开排行榜，标准化推理框架与任务",
                "实验或效果：发现多数推理策略不稳定，性能相近方法置信区间差异可达四倍"
            ],
            "tags_zh": [
                "大语言模型推理",
                "基准测试",
                "不确定性量化",
                "可复现性评估",
                "多轮协议",
                "性能稳定性"
            ],
            "_index": 22
        },
        {
            "title": "GatedFWA: Linear Flash Windowed Attention with Gated Associative Memory",
            "authors": [
                "Jiaxu Liu",
                "Yuhe Bai",
                "Christos-Savvas Bouganis"
            ],
            "arxiv_id": "2512.07782v1",
            "summary": "Modern autoregressive models rely on attention, yet the Softmax full attention in Transformers scales quadratically with sequence length. Sliding Window Attention (SWA) achieves linear-time encoding/decoding by constraining the attention pattern, but under an \\textit{Associative Memory} interpretation, its difference-style update renders the training objective effectively \\emph{unbounded}. In contrast, Softmax attention normalizes updates, leading to \\emph{memory shrinkage and gradient vanishing}. We propose GatedFWA: a Memory-\\underline{Gated} (\\underline{F}lash) \\underline{W}indowed \\underline{A}ttention mechanism that preserves SWAs efficiency while stabilizing memory updates and making gradient flow controllable. In essence, GatedFWA accumulate a per-token/head gate into a decay bias added to the attention logits, acting as a learnable contraction in the memory recurrence. We implement a fused one-pass gate preprocessing and a FlashAttention-compatible kernel that injects the gate under a sliding mask, ensuring I/O efficiency and numerical stability. On language modelling benchmarks, GatedFWA delivers competitive throughput with negligible overhead and better use of global context, and it integrates cleanly with token compression/selection methods such as NSA and generalizes to various autoregressive domains.",
            "headline_zh": "提出GatedFWA以解决滑动窗口注意力在自回归模型中内存更新不稳定和梯度问题，保持线性效率。",
            "intro_zh": [
                "核心问题：滑动窗口注意力在关联内存解释下更新无界，Softmax注意力导致内存收缩和梯度消失。",
                "方法要点：引入可学习的门控机制，通过衰减偏置稳定内存更新，实现可控梯度流。",
                "实验或效果：在语言建模基准上，GatedFWA保持高吞吐量，集成压缩方法，泛化至多种自回归领域。"
            ],
            "tags_zh": [
                "自回归模型",
                "注意力机制",
                "线性复杂度",
                "内存门控",
                "梯度稳定",
                "语言建模"
            ],
            "_index": 23
        },
        {
            "title": "Distribution Matching Variational AutoEncoder",
            "authors": [
                "Sen Ye",
                "Jianning Pei",
                "Mengde Xu",
                "Shuyang Gu",
                "Chunyu Wang",
                "Liwei Wang",
                "Han Hu"
            ],
            "arxiv_id": "2512.07778v1",
            "summary": "Most visual generative models compress images into a latent space before applying diffusion or autoregressive modelling. Yet, existing approaches such as VAEs and foundation model aligned encoders implicitly constrain the latent space without explicitly shaping its distribution, making it unclear which types of distributions are optimal for modeling. We introduce \\textbf{Distribution-Matching VAE} (\\textbf{DMVAE}), which explicitly aligns the encoder's latent distribution with an arbitrary reference distribution via a distribution matching constraint. This generalizes beyond the Gaussian prior of conventional VAEs, enabling alignment with distributions derived from self-supervised features, diffusion noise, or other prior distributions. With DMVAE, we can systematically investigate which latent distributions are more conducive to modeling, and we find that SSL-derived distributions provide an excellent balance between reconstruction fidelity and modeling efficiency, reaching gFID equals 3.2 on ImageNet with only 64 training epochs. Our results suggest that choosing a suitable latent distribution structure (achieved via distribution-level alignment), rather than relying on fixed priors, is key to bridging the gap between easy-to-model latents and high-fidelity image synthesis. Code is avaliable at https://github.com/sen-ye/dmvae.",
            "headline_zh": "提出分布匹配变分自编码器，通过显式对齐潜在分布与任意参考分布来优化视觉生成建模。",
            "intro_zh": [
                "现有视觉生成模型（如VAE）的潜在分布未显式优化，不清楚何种分布最适合建模。",
                "DMVAE通过分布匹配约束，使编码器潜在分布与任意参考分布（如自监督特征分布）对齐。",
                "实验发现自监督特征分布能平衡重建保真度与建模效率，在ImageNet上仅用64轮训练达到gFID=3.2。"
            ],
            "tags_zh": [
                "变分自编码器",
                "分布匹配",
                "潜在空间优化",
                "视觉生成模型",
                "自监督学习"
            ],
            "_index": 24
        },
        {
            "title": "GorillaWatch: An Automated System for In-the-Wild Gorilla Re-Identification and Population Monitoring",
            "authors": [
                "Maximilian Schall",
                "Felix Leonard Knöfel",
                "Noah Elias König",
                "Jan Jonas Kubeler",
                "Maximilian von Klinski",
                "Joan Wilhelm Linnemann",
                "Xiaoshi Liu",
                "Iven Jelle Schlegelmilch",
                "Ole Woyciniuk",
                "Alexandra Schild",
                "Dante Wasmuht",
                "Magdalena Bermejo Espinet",
                "German Illera Basas",
                "Gerard de Melo"
            ],
            "arxiv_id": "2512.07776v1",
            "summary": "Monitoring critically endangered western lowland gorillas is currently hampered by the immense manual effort required to re-identify individuals from vast archives of camera trap footage. The primary obstacle to automating this process has been the lack of large-scale, \"in-the-wild\" video datasets suitable for training robust deep learning models. To address this gap, we introduce a comprehensive benchmark with three novel datasets: Gorilla-SPAC-Wild, the largest video dataset for wild primate re-identification to date; Gorilla-Berlin-Zoo, for assessing cross-domain re-identification generalization; and Gorilla-SPAC-MoT, for evaluating multi-object tracking in camera trap footage. Building on these datasets, we present GorillaWatch, an end-to-end pipeline integrating detection, tracking, and re-identification. To exploit temporal information, we introduce a multi-frame self-supervised pretraining strategy that leverages consistency in tracklets to learn domain-specific features without manual labels. To ensure scientific validity, a differentiable adaptation of AttnLRP verifies that our model relies on discriminative biometric traits rather than background correlations. Extensive benchmarking subsequently demonstrates that aggregating features from large-scale image backbones outperforms specialized video architectures. Finally, we address unsupervised population counting by integrating spatiotemporal constraints into standard clustering to mitigate over-segmentation. We publicly release all code and datasets to facilitate scalable, non-invasive monitoring of endangered species",
            "headline_zh": "提出GorillaWatch系统以解决野外大猩猩重识别与种群监测的自动化难题",
            "intro_zh": [
                "核心问题：缺乏大规模野外视频数据集，阻碍自动化重识别模型训练。",
                "方法要点：引入多数据集基准，集成检测、跟踪与重识别，采用多帧自监督预训练策略。",
                "实验或效果：通过AttnLRP验证模型依赖生物特征，大规模图像骨干优于视频架构，实现无监督种群计数。"
            ],
            "tags_zh": [
                "野生动物重识别",
                "多目标跟踪",
                "自监督学习",
                "种群监测",
                "相机陷阱视频",
                "跨域泛化"
            ],
            "_index": 25
        },
        {
            "title": "OptMap: Geometric Map Distillation via Submodular Maximization",
            "authors": [
                "David Thorne",
                "Nathan Chan",
                "Christa S. Robison",
                "Philip R. Osteen",
                "Brett T. Lopez"
            ],
            "arxiv_id": "2512.07775v1",
            "summary": "Autonomous robots rely on geometric maps to inform a diverse set of perception and decision-making algorithms. As autonomy requires reasoning and planning on multiple scales of the environment, each algorithm may require a different map for optimal performance. Light Detection And Ranging (LiDAR) sensors generate an abundance of geometric data to satisfy these diverse requirements, but selecting informative, size-constrained maps is computationally challenging as it requires solving an NP-hard combinatorial optimization. In this work we present OptMap: a geometric map distillation algorithm which achieves real-time, application-specific map generation via multiple theoretical and algorithmic innovations. A central feature is the maximization of set functions that exhibit diminishing returns, i.e., submodularity, using polynomial-time algorithms with provably near-optimal solutions. We formulate a novel submodular reward function which quantifies informativeness, reduces input set sizes, and minimizes bias in sequentially collected datasets. Further, we propose a dynamically reordered streaming submodular algorithm which improves empirical solution quality and addresses input order bias via an online approximation of the value of all scans. Testing was conducted on open-source and custom datasets with an emphasis on long-duration mapping sessions, highlighting OptMap's minimal computation requirements. Open-source ROS1 and ROS2 packages are available and can be used alongside any LiDAR SLAM algorithm.",
            "headline_zh": "提出OptMap几何地图蒸馏算法，通过子模最大化实现实时应用特定地图生成",
            "intro_zh": [
                "核心问题：LiDAR数据丰富，但选择信息丰富、大小受限的地图是NP-hard组合优化问题",
                "方法要点：利用子模奖励函数量化信息性，采用动态重排序流式子模算法减少输入集大小和顺序偏差",
                "实验或效果：在开源和自定义数据集上测试，强调长时映射会话，计算需求最小，提供ROS包"
            ],
            "tags_zh": [
                "几何地图蒸馏",
                "子模最大化",
                "LiDAR SLAM",
                "实时地图生成",
                "组合优化"
            ],
            "_index": 26
        },
        {
            "title": "Distribution-informed Online Conformal Prediction",
            "authors": [
                "Dongjian Hu",
                "Junxi Wu",
                "Shu-Tao Xia",
                "Changliang Zou"
            ],
            "arxiv_id": "2512.07770v1",
            "summary": "Conformal prediction provides a pivotal and flexible technique for uncertainty quantification by constructing prediction sets with a predefined coverage rate. Many online conformal prediction methods have been developed to address data distribution shifts in fully adversarial environments, resulting in overly conservative prediction sets. We propose Conformal Optimistic Prediction (COP), an online conformal prediction algorithm incorporating underlying data pattern into the update rule. Through estimated cumulative distribution function of non-conformity scores, COP produces tighter prediction sets when predictable pattern exists, while retaining valid coverage guarantees even when estimates are inaccurate. We establish a joint bound on coverage and regret, which further confirms the validity of our approach. We also prove that COP achieves distribution-free, finite-sample coverage under arbitrary learning rates and can converge when scores are $i.i.d.$. The experimental results also show that COP can achieve valid coverage and construct shorter prediction intervals than other baselines.",
            "headline_zh": "提出COP算法以解决在线共形预测中数据分布偏移导致的预测集保守问题",
            "intro_zh": [
                "核心问题：在线共形预测在对抗性环境中因数据分布偏移产生保守预测集，影响效率",
                "方法要点：COP通过非一致性分数的累积分布函数估计，将数据模式融入更新规则，生成更紧预测集",
                "实验或效果：COP在保持有效覆盖率的同时，比基线方法构建更短的预测区间，验证了其有效性"
            ],
            "tags_zh": [
                "在线共形预测",
                "不确定性量化",
                "数据分布偏移",
                "预测集优化",
                "覆盖率保证"
            ],
            "_index": 27
        },
        {
            "title": "Formalized Hopfield Networks and Boltzmann Machines",
            "authors": [
                "Matteo Cipollina",
                "Michail Karatarakis",
                "Freek Wiedijk"
            ],
            "arxiv_id": "2512.07766v1",
            "summary": "Neural networks are widely used, yet their analysis and verification remain challenging. In this work, we present a Lean 4 formalization of neural networks, covering both deterministic and stochastic models. We first formalize Hopfield networks, recurrent networks that store patterns as stable states. We prove convergence and the correctness of Hebbian learning, a training rule that updates network parameters to encode patterns, here limited to the case of pairwise-orthogonal patterns. We then consider stochastic networks, where updates are probabilistic and convergence is to a stationary distribution. As a canonical example, we formalize the dynamics of Boltzmann machines and prove their ergodicity, showing convergence to a unique stationary distribution using a new formalization of the Perron-Frobenius theorem.",
            "headline_zh": "在Lean 4中形式化Hopfield网络和Boltzmann机，以支持神经网络的分析与验证。",
            "intro_zh": [
                "核心问题：神经网络的分析与验证困难，缺乏形式化工具支持。",
                "方法要点：使用Lean 4形式化确定性和随机性神经网络模型，包括Hopfield网络和Boltzmann机。",
                "实验或效果：证明Hopfield网络的收敛性和Hebbian学习正确性，以及Boltzmann机的遍历性和平稳分布收敛性。"
            ],
            "tags_zh": [
                "形式化验证",
                "Hopfield网络",
                "Boltzmann机",
                "Lean 4",
                "神经网络分析",
                "随机模型"
            ],
            "_index": 28
        },
        {
            "title": "Toward Seamless Physical Human-Humanoid Interaction: Insights from Control, Intent, and Modeling with a Vision for What Comes Next",
            "authors": [
                "Gustavo A. Cardona",
                "Shubham S. Kumbhar",
                "Panagiotis Artemiadis"
            ],
            "arxiv_id": "2512.07765v1",
            "summary": "Physical Human-Humanoid Interaction (pHHI) is a rapidly advancing field with significant implications for deploying robots in unstructured, human-centric environments. In this review, we examine the current state of the art in pHHI through three core pillars: (i) humanoid modeling and control, (ii) human intent estimation, and (iii) computational human models. For each pillar, we survey representative approaches, identify open challenges, and analyze current limitations that hinder robust, scalable, and adaptive interaction. These include the need for whole-body control strategies capable of handling uncertain human dynamics, real-time intent inference under limited sensing, and modeling techniques that account for variability in human physical states. Although significant progress has been made within each domain, integration across pillars remains limited. We propose pathways for unifying methods across these areas to enable cohesive interaction frameworks. This structure enables us not only to map the current landscape but also to propose concrete directions for future research that aim to bridge these domains. Additionally, we introduce a unified taxonomy of interaction types based on modality, distinguishing between direct interactions (e.g., physical contact) and indirect interactions (e.g., object-mediated), and on the level of robot engagement, ranging from assistance to cooperation and collaboration. For each category in this taxonomy, we provide the three core pillars that highlight opportunities for cross-pillar unification. Our goal is to suggest avenues to advance robust, safe, and intuitive physical interaction, providing a roadmap for future research that will allow humanoid systems to effectively understand, anticipate, and collaborate with human partners in diverse real-world settings.",
            "headline_zh": "综述物理人-人形机器人交互的三大支柱，提出跨领域统一框架以提升鲁棒性",
            "intro_zh": [
                "核心问题：物理人-人形机器人交互在非结构化环境中面临建模、控制和意图估计的挑战",
                "方法要点：基于人形建模控制、人类意图估计和计算人类模型三大支柱分析现状与局限",
                "实验或效果：提出交互类型统一分类法，为未来研究提供跨支柱整合的具体方向"
            ],
            "tags_zh": [
                "物理人-人形机器人交互",
                "人形机器人控制",
                "人类意图估计",
                "计算人类模型",
                "交互分类法",
                "跨领域统一"
            ],
            "_index": 29
        },
        {
            "title": "RL-MTJail: Reinforcement Learning for Automated Black-Box Multi-Turn Jailbreaking of Large Language Models",
            "authors": [
                "Xiqiao Xiong",
                "Ouxiang Li",
                "Zhuo Liu",
                "Moxin Li",
                "Wentao Shi",
                "Fuli Feng",
                "Xiangnan He"
            ],
            "arxiv_id": "2512.07761v1",
            "summary": "Large language models are vulnerable to jailbreak attacks, threatening their safe deployment in real-world applications. This paper studies black-box multi-turn jailbreaks, aiming to train attacker LLMs to elicit harmful content from black-box models through a sequence of prompt-output interactions. Existing approaches typically rely on single turn optimization, which is insufficient for learning long-term attack strategies. To bridge this gap, we formulate the problem as a multi-turn reinforcement learning task, directly optimizing the harmfulness of the final-turn output as the outcome reward. To mitigate sparse supervision and promote long-term attack strategies, we propose two heuristic process rewards: (1) controlling the harmfulness of intermediate outputs to prevent triggering the black-box model's rejection mechanisms, and (2) maintaining the semantic relevance of intermediate outputs to avoid drifting into irrelevant content. Experimental results on multiple benchmarks show consistently improved attack success rates across multiple models, highlighting the effectiveness of our approach. The code is available at https://github.com/xxiqiao/RL-MTJail. Warning: This paper contains examples of harmful content.",
            "headline_zh": "提出基于强化学习的多轮黑盒越狱方法，以优化长期攻击策略提升攻击成功率。",
            "intro_zh": [
                "研究黑盒多轮越狱攻击，通过序列交互训练攻击者LLMs以诱导有害内容。",
                "将问题建模为多轮强化学习任务，引入启发式过程奖励以缓解稀疏监督并促进长期策略。",
                "在多个基准测试中实验，显示攻击成功率一致提升，验证方法有效性。"
            ],
            "tags_zh": [
                "黑盒越狱攻击",
                "多轮强化学习",
                "启发式奖励",
                "攻击成功率",
                "大语言模型安全"
            ],
            "_index": 30
        },
        {
            "title": "Modality-Aware Bias Mitigation and Invariance Learning for Unsupervised Visible-Infrared Person Re-Identification",
            "authors": [
                "Menglin Wang",
                "Xiaojin Gong",
                "Jiachen Li",
                "Genlin Ji"
            ],
            "arxiv_id": "2512.07760v1",
            "summary": "Unsupervised visible-infrared person re-identification (USVI-ReID) aims to match individuals across visible and infrared cameras without relying on any annotation. Given the significant gap across visible and infrared modality, estimating reliable cross-modality association becomes a major challenge in USVI-ReID. Existing methods usually adopt optimal transport to associate the intra-modality clusters, which is prone to propagating the local cluster errors, and also overlooks global instance-level relations. By mining and attending to the visible-infrared modality bias, this paper focuses on addressing cross-modality learning from two aspects: bias-mitigated global association and modality-invariant representation learning. Motivated by the camera-aware distance rectification in single-modality re-ID, we propose modality-aware Jaccard distance to mitigate the distance bias caused by modality discrepancy, so that more reliable cross-modality associations can be estimated through global clustering. To further improve cross-modality representation learning, a `split-and-contrast' strategy is designed to obtain modality-specific global prototypes. By explicitly aligning these prototypes under global association guidance, modality-invariant yet ID-discriminative representation learning can be achieved. While conceptually simple, our method obtains state-of-the-art performance on benchmark VI-ReID datasets and outperforms existing methods by a significant margin, validating its effectiveness.",
            "headline_zh": "提出模态感知偏差缓解与不变性学习方法，以解决无监督可见光-红外行人重识别中的跨模态关联挑战。",
            "intro_zh": [
                "核心问题：无监督可见光-红外行人重识别中，模态差异导致跨模态关联不可靠，现有方法易传播局部聚类错误并忽略全局实例关系。",
                "方法要点：设计模态感知Jaccard距离缓解模态偏差，通过全局聚类估计可靠关联；采用'分割-对比'策略获取模态特定全局原型，在全局关联指导下对齐以实现模态不变表示学习。",
                "实验或效果：在基准VI-ReID数据集上取得最先进性能，显著优于现有方法，验证了有效性。"
            ],
            "tags_zh": [
                "无监督行人重识别",
                "跨模态学习",
                "模态偏差缓解",
                "不变性表示学习",
                "全局聚类",
                "原型对齐"
            ],
            "_index": 31
        },
        {
            "title": "UltrasODM: A Dual Stream Optical Flow Mamba Network for 3D Freehand Ultrasound Reconstruction",
            "authors": [
                "Mayank Anand",
                "Ujair Alam",
                "Surya Prakash",
                "Priya Shukla",
                "Gora Chand Nandi",
                "Domenec Puig"
            ],
            "arxiv_id": "2512.07756v1",
            "summary": "Clinical ultrasound acquisition is highly operator-dependent, where rapid probe motion and brightness fluctuations often lead to reconstruction errors that reduce trust and clinical utility. We present UltrasODM, a dual-stream framework that assists sonographers during acquisition through calibrated per-frame uncertainty, saliency-based diagnostics, and actionable prompts. UltrasODM integrates (i) a contrastive ranking module that groups frames by motion similarity, (ii) an optical-flow stream fused with Dual-Mamba temporal modules for robust 6-DoF pose estimation, and (iii) a Human-in-the-Loop (HITL) layer combining Bayesian uncertainty, clinician-calibrated thresholds, and saliency maps highlighting regions of low confidence. When uncertainty exceeds the threshold, the system issues unobtrusive alerts suggesting corrective actions such as re-scanning highlighted regions or slowing the sweep. Evaluated on a clinical freehand ultrasound dataset, UltrasODM reduces drift by 15.2%, distance error by 12.1%, and Hausdorff distance by 10.1% relative to UltrasOM, while producing per-frame uncertainty and saliency outputs. By emphasizing transparency and clinician feedback, UltrasODM improves reconstruction reliability and supports safer, more trustworthy clinical workflows. Our code is publicly available at https://github.com/AnandMayank/UltrasODM.",
            "headline_zh": "提出UltrasODM双流框架，通过不确定性估计和提示辅助解决自由手超声重建中的漂移和误差问题。",
            "intro_zh": [
                "核心问题：临床超声采集依赖操作者，快速探头运动和亮度波动导致重建误差，降低临床信任度。",
                "方法要点：集成对比排序模块、光流流与Dual-Mamba时序模块，结合贝叶斯不确定性和人机交互层提供实时提示。",
                "实验或效果：在临床数据集上相比UltrasOM减少漂移15.2%、距离误差12.1%，并输出逐帧不确定性和显著性图。"
            ],
            "tags_zh": [
                "自由手超声重建",
                "光流估计",
                "Mamba网络",
                "不确定性量化",
                "人机交互",
                "三维医学成像"
            ],
            "_index": 32
        },
        {
            "title": "Physics-Informed Neural Networks for Source Inversion and Parameters Estimation in Atmospheric Dispersion",
            "authors": [
                "Brenda Anague",
                "Bamdad Hosseini",
                "Issa Karambal",
                "Jean Medard Ngnotchouye"
            ],
            "arxiv_id": "2512.07755v1",
            "summary": "Recent studies have shown the success of deep learning in solving forward and inverse problems in engineering and scientific computing domains, such as physics-informed neural networks (PINNs). In the fields of atmospheric science and environmental monitoring, estimating emission source locations is a central task that further relies on multiple model parameters that dictate velocity profiles and diffusion parameters. Estimating these parameters at the same time as emission sources from scarce data is a difficult task. In this work, we achieve this by leveraging the flexibility and generality of PINNs. We use a weighted adaptive method based on the neural tangent kernels to solve a source inversion problem with parameter estimation on the 2D and 3D advection-diffusion equations with unknown velocity and diffusion coefficients that may vary in space and time. Our proposed weighted adaptive method is presented as an extension of PINNs for forward PDE problems to a highly ill-posed source inversion and parameter estimation problem. The key idea behind our methodology is to attempt the joint recovery of the solution, the sources along with the unknown parameters, thereby using the underlying partial differential equation as a constraint that couples multiple unknown functional parameters, leading to more efficient use of the limited information in the measurements. We present various numerical experiments, using different types of measurements that model practical engineering systems, to show that our proposed method is indeed successful and robust to additional noise in the measurements.",
            "headline_zh": "提出加权自适应PINN方法以解决大气扩散中源反演与参数估计的联合问题",
            "intro_zh": [
                "核心问题：从稀疏数据中同时估计排放源位置及未知速度与扩散参数，任务高度不适定",
                "方法要点：基于神经正切核的加权自适应PINN，将PDE作为约束耦合多个未知函数参数",
                "实验或效果：在2D/3D对流扩散方程上验证方法成功且对测量噪声鲁棒"
            ],
            "tags_zh": [
                "物理信息神经网络",
                "源反演",
                "参数估计",
                "对流扩散方程",
                "加权自适应方法",
                "大气扩散"
            ],
            "_index": 33
        },
        {
            "title": "Unison: A Fully Automatic, Task-Universal, and Low-Cost Framework for Unified Understanding and Generation",
            "authors": [
                "Shihao Zhao",
                "Yitong Chen",
                "Zeyinzi Jiang",
                "Bojia Zi",
                "Shaozhe Hao",
                "Yu Liu",
                "Chaojie Mao",
                "Kwan-Yee K. Wong"
            ],
            "arxiv_id": "2512.07747v1",
            "summary": "Unified understanding and generation is a highly appealing research direction in multimodal learning. There exist two approaches: one trains a transformer via an auto-regressive paradigm, and the other adopts a two-stage scheme connecting pre-trained understanding and generative models for alignment fine-tuning. The former demands massive data and computing resources unaffordable for ordinary researchers. Though the latter requires a lower training cost, existing works often suffer from limited task coverage or poor generation quality. Both approaches lack the ability to parse input meta-information (such as task type, image resolution, video duration, etc.) and require manual parameter configuration that is tedious and non-intelligent. In this paper, we propose Unison which adopts the two-stage scheme while preserving the capabilities of the pre-trained models well. With an extremely low training cost, we cover a variety of multimodal understanding tasks, including text, image, and video understanding, as well as diverse generation tasks, such as text-to-visual content generation, editing, controllable generation, and IP-based reference generation. We also equip our model with the ability to automatically parse user intentions, determine the target task type, and accurately extract the meta-information required for the corresponding task. This enables full automation of various multimodal tasks without human intervention. Experiments demonstrate that, under a low-cost setting of only 500k training samples and 50 GPU hours, our model can accurately and automatically identify tasks and extract relevant parameters, and achieve superior performance across a variety of understanding and generation tasks.",
            "headline_zh": "提出Unison框架，以低成本实现多模态理解与生成的统一自动化处理",
            "intro_zh": [
                "核心问题：现有统一多模态方法成本高、任务覆盖有限且依赖手动配置参数",
                "方法要点：采用两阶段方案，结合预训练模型，自动解析用户意图和任务元信息",
                "实验或效果：仅用500k样本和50 GPU小时，在多种任务上实现准确自动识别和优越性能"
            ],
            "tags_zh": [
                "多模态学习",
                "统一理解与生成",
                "自动化任务解析",
                "低成本训练",
                "两阶段框架"
            ],
            "_index": 34
        },
        {
            "title": "DiffusionDriveV2: Reinforcement Learning-Constrained Truncated Diffusion Modeling in End-to-End Autonomous Driving",
            "authors": [
                "Jialv Zou",
                "Shaoyu Chen",
                "Bencheng Liao",
                "Zhiyu Zheng",
                "Yuehao Song",
                "Lefei Zhang",
                "Qian Zhang",
                "Wenyu Liu",
                "Xinggang Wang"
            ],
            "arxiv_id": "2512.07745v1",
            "summary": "Generative diffusion models for end-to-end autonomous driving often suffer from mode collapse, tending to generate conservative and homogeneous behaviors. While DiffusionDrive employs predefined anchors representing different driving intentions to partition the action space and generate diverse trajectories, its reliance on imitation learning lacks sufficient constraints, resulting in a dilemma between diversity and consistent high quality. In this work, we propose DiffusionDriveV2, which leverages reinforcement learning to both constrain low-quality modes and explore for superior trajectories. This significantly enhances the overall output quality while preserving the inherent multimodality of its core Gaussian Mixture Model. First, we use scale-adaptive multiplicative noise, ideal for trajectory planning, to promote broad exploration. Second, we employ intra-anchor GRPO to manage advantage estimation among samples generated from a single anchor, and inter-anchor truncated GRPO to incorporate a global perspective across different anchors, preventing improper advantage comparisons between distinct intentions (e.g., turning vs. going straight), which can lead to further mode collapse. DiffusionDriveV2 achieves 91.2 PDMS on the NAVSIM v1 dataset and 85.5 EPDMS on the NAVSIM v2 dataset in closed-loop evaluation with an aligned ResNet-34 backbone, setting a new record. Further experiments validate that our approach resolves the dilemma between diversity and consistent high quality for truncated diffusion models, achieving the best trade-off. Code and model will be available at https://github.com/hustvl/DiffusionDriveV2",
            "headline_zh": "提出DiffusionDriveV2，利用强化学习约束截断扩散模型，解决端到端自动驾驶中多样性与高质量轨迹生成的困境。",
            "intro_zh": [
                "核心问题：生成扩散模型在端到端自动驾驶中易出现模式崩溃，导致保守和同质化行为，难以平衡多样性与一致高质量。",
                "方法要点：采用尺度自适应乘性噪声促进探索，结合锚内GRPO和锚间截断GRPO管理优势估计，避免不同意图间不当比较。",
                "实验或效果：在NAVSIM数据集上取得领先性能，验证了方法在多样性与高质量间的优化平衡，代码将开源。"
            ],
            "tags_zh": [
                "端到端自动驾驶",
                "扩散模型",
                "强化学习",
                "轨迹生成",
                "模式崩溃",
                "截断建模"
            ],
            "_index": 35
        },
        {
            "title": "A multimodal Bayesian Network for symptom-level depression and anxiety prediction from voice and speech data",
            "authors": [
                "Agnes Norbury",
                "George Fairs",
                "Alexandra L. Georgescu",
                "Matthew M. Nour",
                "Emilia Molimpakis",
                "Stefano Goria"
            ],
            "arxiv_id": "2512.07741v1",
            "summary": "During psychiatric assessment, clinicians observe not only what patients report, but important nonverbal signs such as tone, speech rate, fluency, responsiveness, and body language. Weighing and integrating these different information sources is a challenging task and a good candidate for support by intelligence-driven tools - however this is yet to be realized in the clinic. Here, we argue that several important barriers to adoption can be addressed using Bayesian network modelling. To demonstrate this, we evaluate a model for depression and anxiety symptom prediction from voice and speech features in large-scale datasets (30,135 unique speakers). Alongside performance for conditions and symptoms (for depression, anxiety ROC-AUC=0.842,0.831 ECE=0.018,0.015; core individual symptom ROC-AUC>0.74), we assess demographic fairness and investigate integration across and redundancy between different input modality types. Clinical usefulness metrics and acceptability to mental health service users are explored. When provided with sufficiently rich and large-scale multimodal data streams and specified to represent common mental conditions at the symptom rather than disorder level, such models are a principled approach for building robust assessment support tools: providing clinically-relevant outputs in a transparent and explainable format that is directly amenable to expert clinical supervision.",
            "headline_zh": "提出基于贝叶斯网络的多模态模型，从语音数据预测抑郁和焦虑症状，以支持临床评估。",
            "intro_zh": [
                "核心问题：临床评估中整合非语言信息（如语音特征）预测精神症状的挑战。",
                "方法要点：使用贝叶斯网络建模，分析大规模语音数据集（30,135名说话者），评估症状级预测。",
                "实验或效果：模型在抑郁和焦虑预测上ROC-AUC达0.842和0.831，并探讨了公平性和临床实用性。"
            ],
            "tags_zh": [
                "贝叶斯网络",
                "多模态预测",
                "语音分析",
                "精神症状评估",
                "临床支持工具"
            ],
            "_index": 36
        },
        {
            "title": "HLTCOE Evaluation Team at TREC 2025: VQA Track",
            "authors": [
                "Dengjia Zhang",
                "Charles Weng",
                "Katherine Guerrerio",
                "Yi Lu",
                "Kenton Murray",
                "Alexander Martin",
                "Reno Kriz",
                "Benjamin Van Durme"
            ],
            "arxiv_id": "2512.07738v1",
            "summary": "The HLTCOE Evaluation team participated in TREC VQA's Answer Generation (AG) task, for which we developed a listwise learning framework that aims to improve semantic precision and ranking consistency in answer generation. Given a video-question pair, a base multimodal model first generates multiple candidate answers, which are then reranked using a model trained with a novel Masked Pointer Cross-Entropy Loss with Rank Weights. This objective integrates pointer-based candidate selection, rank-dependent weighting, and masked cross-entropy under vocabulary restriction, enabling stable and interpretable listwise optimization. By bridging generative modeling with discriminative ranking, our method produces coherent, fine-grained answer lists. Experiments reveal consistent gains in accuracy and ranking stability, especially for questions requiring temporal reasoning and semantic disambiguation.",
            "headline_zh": "提出基于列表学习框架的视频问答方法，通过候选答案重排序提升语义精度和排序一致性",
            "intro_zh": [
                "针对视频问答的答案生成任务，核心问题是提升答案的语义精度和排序稳定性",
                "方法采用列表学习框架，先由基础多模态模型生成候选答案，再用带掩码指针交叉熵损失和排序权重的模型进行重排序",
                "实验表明该方法在准确性和排序稳定性上取得一致提升，尤其在需要时序推理和语义消歧的问题上效果显著"
            ],
            "tags_zh": [
                "视频问答",
                "列表学习",
                "答案重排序",
                "多模态模型",
                "时序推理"
            ],
            "_index": 37
        },
        {
            "title": "A scalable and real-time neural decoder for topological quantum codes",
            "authors": [
                "Andrew W. Senior",
                "Thomas Edlich",
                "Francisco J. H. Heras",
                "Lei M. Zhang",
                "Oscar Higgott",
                "James S. Spencer",
                "Taylor Applebaum",
                "Sam Blackwell",
                "Justin Ledford",
                "Akvilė Žemgulytė",
                "Augustin Žídek",
                "Noah Shutty",
                "Andrew Cowie",
                "Yin Li",
                "George Holland",
                "Peter Brooks",
                "Charlie Beattie",
                "Michael Newman",
                "Alex Davies",
                "Cody Jones",
                "Sergio Boixo",
                "Hartmut Neven",
                "Pushmeet Kohli",
                "Johannes Bausch"
            ],
            "arxiv_id": "2512.07737v1",
            "summary": "Fault-tolerant quantum computing will require error rates far below those achievable with physical qubits. Quantum error correction (QEC) bridges this gap, but depends on decoders being simultaneously fast, accurate, and scalable. This combination of requirements has not yet been met by a machine-learning decoder, nor by any decoder for promising resource-efficient codes such as the colour code. Here we introduce AlphaQubit 2, a neural-network decoder that achieves near-optimal logical error rates for both surface and colour codes at large scales under realistic noise. For the colour code, it is orders of magnitude faster than other high-accuracy decoders. For the surface code, we demonstrate real-time decoding faster than 1 microsecond per cycle up to distance 11 on current commercial accelerators with better accuracy than leading real-time decoders. These results support the practical application of a wider class of promising QEC codes, and establish a credible path towards high-accuracy, real-time neural decoding at the scales required for fault-tolerant quantum computation.",
            "headline_zh": "提出AlphaQubit 2神经解码器，实现拓扑量子码的高精度实时解码",
            "intro_zh": [
                "量子纠错需解码器兼具快速、准确和可扩展性，现有方法未满足此要求",
                "AlphaQubit 2基于神经网络，对表面码和颜色码在大规模下实现近最优逻辑错误率",
                "实验显示颜色码解码速度比高精度解码器快多个数量级，表面码解码速度快于1微秒每周期"
            ],
            "tags_zh": [
                "量子纠错",
                "神经解码器",
                "拓扑量子码",
                "表面码",
                "颜色码",
                "实时解码"
            ],
            "_index": 38
        },
        {
            "title": "SpatialDreamer: Incentivizing Spatial Reasoning via Active Mental Imagery",
            "authors": [
                "Meng Cao",
                "Xingyu Li",
                "Xue Liu",
                "Ian Reid",
                "Xiaodan Liang"
            ],
            "arxiv_id": "2512.07733v1",
            "summary": "Despite advancements in Multi-modal Large Language Models (MLLMs) for scene understanding, their performance on complex spatial reasoning tasks requiring mental simulation remains significantly limited. Current methods often rely on passive observation of spatial data, failing to internalize an active mental imagery process. To bridge this gap, we propose SpatialDreamer, a reinforcement learning framework that enables spatial reasoning through a closedloop process of active exploration, visual imagination via a world model, and evidence-grounded reasoning. To address the lack of fine-grained reward supervision in longhorizontal reasoning tasks, we propose Geometric Policy Optimization (GeoPO), which introduces tree-structured sampling and step-level reward estimation with geometric consistency constraints. Extensive experiments demonstrate that SpatialDreamer delivers highly competitive results across multiple challenging benchmarks, signifying a critical advancement in human-like active spatial mental simulation for MLLMs.",
            "headline_zh": "提出SpatialDreamer强化学习框架，通过主动探索和世界模型解决MLLMs空间推理任务中的心理模拟不足问题。",
            "intro_zh": [
                "核心问题：多模态大语言模型在需要心理模拟的复杂空间推理任务中表现受限，缺乏主动心理意象过程。",
                "方法要点：采用强化学习框架，结合主动探索、世界模型视觉想象和基于证据的推理，并引入几何策略优化以处理长序列任务的奖励监督。",
                "实验或效果：在多个挑战性基准测试中取得竞争性结果，提升了MLLMs类人主动空间心理模拟能力。"
            ],
            "tags_zh": [
                "空间推理",
                "强化学习",
                "世界模型",
                "心理模拟",
                "多模态大语言模型",
                "几何一致性"
            ],
            "_index": 39
        },
        {
            "title": "SAVE: Sparse Autoencoder-Driven Visual Information Enhancement for Mitigating Object Hallucination",
            "authors": [
                "Sangha Park",
                "Seungryong Yoo",
                "Jisoo Mok",
                "Sungroh Yoon"
            ],
            "arxiv_id": "2512.07730v1",
            "summary": "Although Multimodal Large Language Models (MLLMs) have advanced substantially, they remain vulnerable to object hallucination caused by language priors and visual information loss. To address this, we propose SAVE (Sparse Autoencoder-Driven Visual Information Enhancement), a framework that mitigates hallucination by steering the model along Sparse Autoencoder (SAE) latent features. A binary object-presence question-answering probe identifies the SAE features most indicative of the model's visual information processing, referred to as visual understanding features. Steering the model along these identified features reinforces grounded visual understanding and effectively reduces hallucination. With its simple design, SAVE outperforms state-of-the-art training-free methods on standard benchmarks, achieving a 10\\%p improvement in CHAIR\\_S and consistent gains on POPE and MMHal-Bench. Extensive evaluations across multiple models and layers confirm the robustness and generalizability of our approach. Further analysis reveals that steering along visual understanding features suppresses the generation of uncertain object tokens and increases attention to image tokens, mitigating hallucination. Code is released at https://github.com/wiarae/SAVE.",
            "headline_zh": "提出SAVE框架，利用稀疏自编码器特征增强视觉信息以缓解多模态大语言模型的对象幻觉问题",
            "intro_zh": [
                "核心问题：多模态大语言模型因语言先验和视觉信息丢失易产生对象幻觉",
                "方法要点：通过二进制对象存在问答探针识别稀疏自编码器中的视觉理解特征，并沿这些特征引导模型",
                "实验或效果：在CHAIR_S基准上提升10%p，在POPE和MMHal-Bench上表现一致，优于现有免训练方法"
            ],
            "tags_zh": [
                "对象幻觉缓解",
                "稀疏自编码器",
                "视觉信息增强",
                "多模态大语言模型",
                "免训练方法"
            ],
            "_index": 40
        },
        {
            "title": "Improving action classification with brain-inspired deep networks",
            "authors": [
                "Aidas Aglinskas",
                "Stefano Anzellotti"
            ],
            "arxiv_id": "2512.07729v1",
            "summary": "Action recognition is also key for applications ranging from robotics to healthcare monitoring. Action information can be extracted from the body pose and movements, as well as from the background scene. However, the extent to which deep neural networks (DNNs) make use of information about the body and information about the background remains unclear. Since these two sources of information may be correlated within a training dataset, DNNs might learn to rely predominantly on one of them, without taking full advantage of the other. Unlike DNNs, humans have domain-specific brain regions selective for perceiving bodies, and regions selective for perceiving scenes. The present work tests whether humans are thus more effective at extracting information from both body and background, and whether building brain-inspired deep network architectures with separate domain-specific streams for body and scene perception endows them with more human-like performance. We first demonstrate that DNNs trained using the HAA500 dataset perform almost as accurately on versions of the stimuli that show both body and background and on versions of the stimuli from which the body was removed, but are at chance-level for versions of the stimuli from which the background was removed. Conversely, human participants (N=28) can recognize the same set of actions accurately with all three versions of the stimuli, and perform significantly better on stimuli that show only the body than on stimuli that show only the background. Finally, we implement and test a novel architecture patterned after domain specificity in the brain with separate streams to process body and background information. We show that 1) this architecture improves action recognition performance, and 2) its accuracy across different versions of the stimuli follows a pattern that matches more closely the pattern of accuracy observed in human participants.",
            "headline_zh": "提出脑启发的双流深度网络以提升动作分类性能，模拟人类对躯体和背景的感知分离。",
            "intro_zh": [
                "核心问题：深度神经网络在动作识别中可能过度依赖背景信息，忽视躯体信息，与人类感知模式不同。",
                "方法要点：设计脑启发的架构，包含独立的躯体流和背景流，模拟大脑的领域特异性处理。",
                "实验或效果：该架构在HAA500数据集上提升性能，且准确率模式更接近人类参与者。"
            ],
            "tags_zh": [
                "动作识别",
                "脑启发网络",
                "领域特异性",
                "深度神经网络",
                "躯体感知",
                "背景感知"
            ],
            "_index": 41
        },
        {
            "title": "The Native Spiking Microarchitecture: From Iontronic Primitives to Bit-Exact FP8 Arithmetic",
            "authors": [
                "Zhengzheng Tang"
            ],
            "arxiv_id": "2512.07724v1",
            "summary": "The 2025 Nobel Prize in Chemistry for Metal-Organic Frameworks (MOFs) and recent breakthroughs by Huanting Wang's team at Monash University establish angstrom-scale channels as promising post-silicon substrates with native integrate-and-fire (IF) dynamics. However, utilizing these stochastic, analog materials for deterministic, bit-exact AI workloads (e.g., FP8) remains a paradox. Existing neuromorphic methods often settle for approximation, failing Transformer precision standards. To traverse the gap \"from stochastic ions to deterministic floats,\" we propose a Native Spiking Microarchitecture. Treating noisy neurons as logic primitives, we introduce a Spatial Combinational Pipeline and a Sticky-Extra Correction mechanism. Validation across all 16,129 FP8 pairs confirms 100% bit-exact alignment with PyTorch. Crucially, our architecture reduces Linear layer latency to O(log N), yielding a 17x speedup. Physical simulations further demonstrate robustness against extreme membrane leakage (beta approx 0.01), effectively immunizing the system against the stochastic nature of the hardware.",
            "headline_zh": "提出原生脉冲微架构，利用离子电子学基元实现FP8位精确算术，解决后硅基材的确定性计算难题。",
            "intro_zh": [
                "核心问题：基于埃级通道的随机模拟材料难以支持确定性位精确AI计算，如FP8，现有神经形态方法精度不足。",
                "方法要点：将噪声神经元视为逻辑基元，引入空间组合流水线和粘性额外校正机制，实现从随机离子到确定性浮点的转换。",
                "实验或效果：验证所有FP8对实现100%位精确对齐，线性层延迟降至O(log N)，加速17倍，物理模拟显示对极端膜泄漏的鲁棒性。"
            ],
            "tags_zh": [
                "原生脉冲微架构",
                "离子电子学基元",
                "FP8位精确算术",
                "后硅基材",
                "确定性计算",
                "神经形态硬件"
            ],
            "_index": 42
        },
        {
            "title": "Enabling Delayed-Full Charging Through Transformer-Based Real-Time-to-Departure Modeling for EV Battery Longevity",
            "authors": [
                "Yonggeon Lee",
                "Jibin Hwang",
                "Alfred Malengo Kondoro",
                "Juhyun Song",
                "Youngtae Noh"
            ],
            "arxiv_id": "2512.07723v1",
            "summary": "Electric vehicles (EVs) are key to sustainable mobility, yet their lithium-ion batteries (LIBs) degrade more rapidly under prolonged high states of charge (SOC). This can be mitigated by delaying full charging \\ours until just before departure, which requires accurate prediction of user departure times. In this work, we propose Transformer-based real-time-to-event (TTE) model for accurate EV departure prediction. Our approach represents each day as a TTE sequence by discretizing time into grid-based tokens. Unlike previous methods primarily dependent on temporal dependency from historical patterns, our method leverages streaming contextual information to predict departures. Evaluation on a real-world study involving 93 users and passive smartphone data demonstrates that our method effectively captures irregular departure patterns within individual routines, outperforming baseline models. These results highlight the potential for practical deployment of the \\ours algorithm and its contribution to sustainable transportation systems.",
            "headline_zh": "提出基于Transformer的实时到事件模型，以准确预测电动汽车出发时间，延长电池寿命。",
            "intro_zh": [
                "核心问题：电动汽车锂离子电池在长时间高荷电状态下易退化，需延迟充满电至出发前。",
                "方法要点：将每天离散化为基于网格的令牌序列，利用流式上下文信息预测出发时间，而非仅依赖历史模式。",
                "实验或效果：在93名用户的真实世界智能手机数据上评估，模型能有效捕捉个体日常中的不规则出发模式，优于基线。"
            ],
            "tags_zh": [
                "电动汽车电池管理",
                "出发时间预测",
                "Transformer模型",
                "实时到事件建模",
                "可持续交通"
            ],
            "_index": 43
        },
        {
            "title": "ViSA: 3D-Aware Video Shading for Real-Time Upper-Body Avatar Creation",
            "authors": [
                "Fan Yang",
                "Heyuan Li",
                "Peihao Li",
                "Weihao Yuan",
                "Lingteng Qiu",
                "Chaoyue Song",
                "Cheng Chen",
                "Yisheng He",
                "Shifeng Zhang",
                "Xiaoguang Han",
                "Steven Hoi",
                "Guosheng Lin"
            ],
            "arxiv_id": "2512.07720v1",
            "summary": "Generating high-fidelity upper-body 3D avatars from one-shot input image remains a significant challenge. Current 3D avatar generation methods, which rely on large reconstruction models, are fast and capable of producing stable body structures, but they often suffer from artifacts such as blurry textures and stiff, unnatural motion. In contrast, generative video models show promising performance by synthesizing photorealistic and dynamic results, but they frequently struggle with unstable behavior, including body structural errors and identity drift. To address these limitations, we propose a novel approach that combines the strengths of both paradigms. Our framework employs a 3D reconstruction model to provide robust structural and appearance priors, which in turn guides a real-time autoregressive video diffusion model for rendering. This process enables the model to synthesize high-frequency, photorealistic details and fluid dynamics in real time, effectively reducing texture blur and motion stiffness while preventing the structural inconsistencies common in video generation methods. By uniting the geometric stability of 3D reconstruction with the generative capabilities of video models, our method produces high-fidelity digital avatars with realistic appearance and dynamic, temporally coherent motion. Experiments demonstrate that our approach significantly reduces artifacts and achieves substantial improvements in visual quality over leading methods, providing a robust and efficient solution for real-time applications such as gaming and virtual reality. Project page: https://lhyfst.github.io/visa",
            "headline_zh": "提出ViSA框架，结合3D重建与视频生成，实现实时高保真上身虚拟人创建。",
            "intro_zh": [
                "核心问题：单图像生成上身3D虚拟人存在纹理模糊、运动僵硬或结构不稳定问题。",
                "方法要点：使用3D重建模型提供结构先验，引导实时自回归视频扩散模型渲染细节与动态。",
                "实验或效果：显著减少伪影，提升视觉质量，适用于游戏和虚拟现实等实时应用。"
            ],
            "tags_zh": [
                "3D虚拟人生成",
                "视频扩散模型",
                "实时渲染",
                "上身虚拟人",
                "结构先验"
            ],
            "_index": 44
        },
        {
            "title": "UnCageNet: Tracking and Pose Estimation of Caged Animal",
            "authors": [
                "Sayak Dutta",
                "Harish Katti",
                "Shashikant Verma",
                "Shanmuganathan Raman"
            ],
            "arxiv_id": "2512.07712v1",
            "summary": "Animal tracking and pose estimation systems, such as STEP (Simultaneous Tracking and Pose Estimation) and ViTPose, experience substantial performance drops when processing images and videos with cage structures and systematic occlusions. We present a three-stage preprocessing pipeline that addresses this limitation through: (1) cage segmentation using a Gabor-enhanced ResNet-UNet architecture with tunable orientation filters, (2) cage inpainting using CRFill for content-aware reconstruction of occluded regions, and (3) evaluation of pose estimation and tracking on the uncaged frames. Our Gabor-enhanced segmentation model leverages orientation-aware features with 72 directional kernels to accurately identify and segment cage structures that severely impair the performance of existing methods. Experimental validation demonstrates that removing cage occlusions through our pipeline enables pose estimation and tracking performance comparable to that in environments without occlusions. We also observe significant improvements in keypoint detection accuracy and trajectory consistency.",
            "headline_zh": "提出UnCageNet三阶段预处理流程，以解决笼子遮挡下动物追踪与姿态估计性能下降问题",
            "intro_zh": [
                "核心问题：现有动物追踪与姿态估计系统在笼子结构和系统性遮挡下性能显著下降",
                "方法要点：采用Gabor增强ResNet-UNet进行笼子分割，CRFill进行笼子修复，再评估去遮挡后的帧",
                "实验或效果：通过去除笼子遮挡，实现与无遮挡环境相当的姿态估计和追踪性能，关键点检测精度和轨迹一致性显著提升"
            ],
            "tags_zh": [
                "笼子分割",
                "姿态估计",
                "动物追踪",
                "遮挡修复",
                "Gabor滤波器",
                "预处理流程"
            ],
            "_index": 45
        },
        {
            "title": "Each Prompt Matters: Scaling Reinforcement Learning Without Wasting Rollouts on Hundred-Billion-Scale MoE",
            "authors": [
                "Anxiang Zeng",
                "Haibo Zhang",
                "Hailing Zhang",
                "Kaixiang Mo",
                "Liang Yao",
                "Ling Hu",
                "Long Zhang",
                "Shuman Liu",
                "Shuyi Xie",
                "Yanshi Li",
                "Yizhang Chen",
                "Yuepeng Sheng",
                "Yuwei Huang",
                "Zhaochen Xu",
                "Zhiqiang Zhou",
                "Ziqin Liew"
            ],
            "arxiv_id": "2512.07710v1",
            "summary": "We present CompassMax-V3-Thinking, a hundred-billion-scale MoE reasoning model trained with a new RL framework built on one principle: each prompt must matter. Scaling RL to this size exposes critical inefficiencies-zero-variance prompts that waste rollouts, unstable importance sampling over long horizons, advantage inversion from standard reward models, and systemic bottlenecks in rollout processing. To overcome these challenges, we introduce several unified innovations: (1) Multi-Stage Zero-Variance Elimination, which filters out non-informative prompts and stabilizes group-based policy optimization (e.g. GRPO) by removing wasted rollouts; (2) ESPO, an entropy-adaptive optimization method that balances token-level and sequence-level importance sampling to maintain stable learning dynamics; (3) a Router Replay strategy that aligns training-time MoE router decisions with inference-time behavior to mitigate train-infer discrepancies, coupled with a reward model adjustment to prevent advantage inversion; (4) a high-throughput RL system with FP8-precision rollouts, overlapped reward computation, and length-aware scheduling to eliminate performance bottlenecks. Together, these contributions form a cohesive pipeline that makes RL on hundred-billion-scale MoE models stable and efficient. The resulting model delivers strong performance across both internal and public evaluations.",
            "headline_zh": "提出基于每个提示都重要的RL框架，以高效训练百亿规模MoE推理模型",
            "intro_zh": [
                "核心问题：百亿规模MoE模型RL训练存在零方差提示浪费rollouts、重要性采样不稳定、优势反转和系统瓶颈。",
                "方法要点：引入多阶段零方差消除、熵自适应优化ESPO、路由器重放策略和FP8高吞吐系统。",
                "实验或效果：模型在内部和公开评估中表现强劲，实现稳定高效训练。"
            ],
            "tags_zh": [
                "百亿规模MoE模型",
                "强化学习框架",
                "零方差提示消除",
                "熵自适应优化",
                "路由器重放",
                "高吞吐系统"
            ],
            "_index": 46
        },
        {
            "title": "In-Context and Few-Shots Learning for Forecasting Time Series Data based on Large Language Models",
            "authors": [
                "Saroj Gopali",
                "Bipin Chhetri",
                "Deepika Giri",
                "Sima Siami-Namini",
                "Akbar Siami Namin"
            ],
            "arxiv_id": "2512.07705v1",
            "summary": "Existing data-driven approaches in modeling and predicting time series data include ARIMA (Autoregressive Integrated Moving Average), Transformer-based models, LSTM (Long Short-Term Memory) and TCN (Temporal Convolutional Network). These approaches, and in particular deep learning-based models such as LSTM and TCN, have shown great results in predicting time series data. With the advancement of leveraging pre-trained foundation models such as Large Language Models (LLMs) and more notably Google's recent foundation model for time series data, {\\it TimesFM} (Time Series Foundation Model), it is of interest to investigate whether these foundation models have the capability of outperforming existing modeling approaches in analyzing and predicting time series data.\n  This paper investigates the performance of using LLM models for time series data prediction. We investigate the in-context learning methodology in the training of LLM models that are specific to the underlying application domain. More specifically, the paper explores training LLMs through in-context, zero-shot and few-shot learning and forecasting time series data with OpenAI {\\tt o4-mini} and Gemini 2.5 Flash Lite, as well as the recent Google's Transformer-based TimesFM, a time series-specific foundation model, along with two deep learning models, namely TCN and LSTM networks. The findings indicate that TimesFM has the best overall performance with the lowest RMSE value (0.3023) and the competitive inference time (266 seconds). Furthermore, OpenAI's o4-mini also exhibits a good performance based on Zero Shot learning.\n  These findings highlight pre-trained time series foundation models as a promising direction for real-time forecasting, enabling accurate and scalable deployment with minimal model adaptation.",
            "headline_zh": "评估大语言模型在时间序列预测中的上下文与少样本学习性能",
            "intro_zh": [
                "核心问题：比较大语言模型与传统方法在时间序列预测中的性能",
                "方法要点：使用上下文学习、零样本和少样本学习训练LLM，并对比TimesFM、TCN和LSTM",
                "实验或效果：TimesFM表现最佳，RMSE最低为0.3023，推理时间266秒"
            ],
            "tags_zh": [
                "时间序列预测",
                "大语言模型",
                "上下文学习",
                "少样本学习",
                "基础模型",
                "深度学习"
            ],
            "_index": 47
        },
        {
            "title": "PVeRA: Probabilistic Vector-Based Random Matrix Adaptation",
            "authors": [
                "Leo Fillioux",
                "Enzo Ferrante",
                "Paul-Henry Cournède",
                "Maria Vakalopoulou",
                "Stergios Christodoulidis"
            ],
            "arxiv_id": "2512.07703v1",
            "summary": "Large foundation models have emerged in the last years and are pushing performance boundaries for a variety of tasks. Training or even finetuning such models demands vast datasets and computational resources, which are often scarce and costly. Adaptation methods provide a computationally efficient solution to address these limitations by allowing such models to be finetuned on small amounts of data and computing power. This is achieved by appending new trainable modules to frozen backbones with only a fraction of the trainable parameters and fitting only these modules on novel tasks. Recently, the VeRA adapter was shown to excel in parameter-efficient adaptations by utilizing a pair of frozen random low-rank matrices shared across all layers. In this paper, we propose PVeRA, a probabilistic version of the VeRA adapter, which modifies the low-rank matrices of VeRA in a probabilistic manner. This modification naturally allows handling inherent ambiguities in the input and allows for different sampling configurations during training and testing. A comprehensive evaluation was performed on the VTAB-1k benchmark and seven adapters, with PVeRA outperforming VeRA and other adapters. Our code for training models with PVeRA and benchmarking all adapters is available https://github.com/leofillioux/pvera.",
            "headline_zh": "提出PVeRA概率向量适配器，以增强基础模型在小数据高效适应中的性能。",
            "intro_zh": [
                "核心问题：基础模型适应需大量数据与计算，现有方法如VeRA适配器在参数效率上仍有局限。",
                "方法要点：将VeRA的低秩矩阵修改为概率版本，处理输入模糊性并支持训练与测试时的不同采样配置。",
                "实验或效果：在VTAB-1k基准测试中，PVeRA优于VeRA及其他适配器，代码已开源。"
            ],
            "tags_zh": [
                "参数高效适应",
                "概率适配器",
                "基础模型",
                "低秩矩阵",
                "VTAB-1k基准"
            ],
            "_index": 48
        },
        {
            "title": "Guiding What Not to Generate: Automated Negative Prompting for Text-Image Alignment",
            "authors": [
                "Sangha Park",
                "Eunji Kim",
                "Yeongtak Oh",
                "Jooyoung Choi",
                "Sungroh Yoon"
            ],
            "arxiv_id": "2512.07702v1",
            "summary": "Despite substantial progress in text-to-image generation, achieving precise text-image alignment remains challenging, particularly for prompts with rich compositional structure or imaginative elements. To address this, we introduce Negative Prompting for Image Correction (NPC), an automated pipeline that improves alignment by identifying and applying negative prompts that suppress unintended content. We begin by analyzing cross-attention patterns to explain why both targeted negatives-those directly tied to the prompt's alignment error-and untargeted negatives-tokens unrelated to the prompt but present in the generated image-can enhance alignment. To discover useful negatives, NPC generates candidate prompts using a verifier-captioner-proposer framework and ranks them with a salient text-space score, enabling effective selection without requiring additional image synthesis. On GenEval++ and Imagine-Bench, NPC outperforms strong baselines, achieving 0.571 vs. 0.371 on GenEval++ and the best overall performance on Imagine-Bench. By guiding what not to generate, NPC provides a principled, fully automated route to stronger text-image alignment in diffusion models. Code is released at https://github.com/wiarae/NPC.",
            "headline_zh": "提出自动化负提示方法NPC以提升扩散模型中文本-图像对齐精度",
            "intro_zh": [
                "核心问题：文本到图像生成中，复杂或想象性提示的文本-图像对齐仍具挑战性",
                "方法要点：通过分析交叉注意力模式，自动识别并应用负提示来抑制不期望内容",
                "实验或效果：在GenEval++和Imagine-Bench上优于基线，实现更强对齐"
            ],
            "tags_zh": [
                "文本到图像生成",
                "负提示",
                "扩散模型",
                "文本-图像对齐",
                "自动化校正"
            ],
            "_index": 49
        },
        {
            "title": "sim2art: Accurate Articulated Object Modeling from a Single Video using Synthetic Training Data Only",
            "authors": [
                "Arslan Artykov",
                "Corentin Sautier",
                "Vincent Lepetit"
            ],
            "arxiv_id": "2512.07698v1",
            "summary": "Understanding articulated objects is a fundamental challenge in robotics and digital twin creation. To effectively model such objects, it is essential to recover both part segmentation and the underlying joint parameters. Despite the importance of this task, previous work has largely focused on setups like multi-view systems, object scanning, or static cameras. In this paper, we present the first data-driven approach that jointly predicts part segmentation and joint parameters from monocular video captured with a freely moving camera. Trained solely on synthetic data, our method demonstrates strong generalization to real-world objects, offering a scalable and practical solution for articulated object understanding. Our approach operates directly on casually recorded video, making it suitable for real-time applications in dynamic environments. Project webpage: https://aartykov.github.io/sim2art/",
            "headline_zh": "提出sim2art方法，仅用合成数据训练，从单目视频中准确建模关节物体",
            "intro_zh": [
                "核心问题：从自由移动相机拍摄的单目视频中恢复关节物体的部件分割和关节参数，解决机器人学和数字孪生中的建模挑战",
                "方法要点：首个数据驱动方法，联合预测部件分割和关节参数，仅依赖合成数据进行训练，实现强泛化到真实物体",
                "实验或效果：在真实世界物体上展示良好泛化能力，适用于动态环境中的实时应用，提供可扩展的实用解决方案"
            ],
            "tags_zh": [
                "关节物体建模",
                "单目视频理解",
                "合成数据训练",
                "部件分割",
                "关节参数估计",
                "数据驱动方法"
            ],
            "_index": 50
        },
        {
            "title": "Delay-Aware Diffusion Policy: Bridging the Observation-Execution Gap in Dynamic Tasks",
            "authors": [
                "Aileen Liao",
                "Dong-Ki Kim",
                "Max Olan Smith",
                "Ali-akbar Agha-mohammadi",
                "Shayegan Omidshafiei"
            ],
            "arxiv_id": "2512.07697v1",
            "summary": "As a robot senses and selects actions, the world keeps changing. This inference delay creates a gap of tens to hundreds of milliseconds between the observed state and the state at execution. In this work, we take the natural generalization from zero delay to measured delay during training and inference. We introduce Delay-Aware Diffusion Policy (DA-DP), a framework for explicitly incorporating inference delays into policy learning. DA-DP corrects zero-delay trajectories to their delay-compensated counterparts, and augments the policy with delay conditioning. We empirically validate DA-DP on a variety of tasks, robots, and delays and find its success rate more robust to delay than delay-unaware methods. DA-DP is architecture agnostic and transfers beyond diffusion policies, offering a general pattern for delay-aware imitation learning. More broadly, DA-DP encourages evaluation protocols that report performance as a function of measured latency, not just task difficulty.",
            "headline_zh": "提出延迟感知扩散策略以解决机器人动态任务中的观测-执行延迟问题",
            "intro_zh": [
                "核心问题：机器人感知与动作选择间的延迟导致观测状态与执行状态不匹配，影响任务性能。",
                "方法要点：通过延迟补偿轨迹和延迟条件增强，将推理延迟显式纳入策略学习框架。",
                "实验或效果：在多种任务、机器人和延迟下验证，相比无延迟方法，成功率对延迟更鲁棒。"
            ],
            "tags_zh": [
                "延迟感知策略",
                "扩散模型",
                "模仿学习",
                "机器人控制",
                "动态任务"
            ],
            "_index": 51
        },
        {
            "title": "HalluShift++: Bridging Language and Vision through Internal Representation Shifts for Hierarchical Hallucinations in MLLMs",
            "authors": [
                "Sujoy Nath",
                "Arkaprabha Basu",
                "Sharanya Dasgupta",
                "Swagatam Das"
            ],
            "arxiv_id": "2512.07687v1",
            "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in vision-language understanding tasks. While these models often produce linguistically coherent output, they often suffer from hallucinations, generating descriptions that are factually inconsistent with the visual content, potentially leading to adverse consequences. Therefore, the assessment of hallucinations in MLLM has become increasingly crucial in the model development process. Contemporary methodologies predominantly depend on external LLM evaluators, which are themselves susceptible to hallucinations and may present challenges in terms of domain adaptation. In this study, we propose the hypothesis that hallucination manifests as measurable irregularities within the internal layer dynamics of MLLMs, not merely due to distributional shifts but also in the context of layer-wise analysis of specific assumptions. By incorporating such modifications, \\textsc{\\textsc{HalluShift++}} broadens the efficacy of hallucination detection from text-based large language models (LLMs) to encompass multimodal scenarios. Our codebase is available at https://github.com/C0mRD/HalluShift_Plus.",
            "headline_zh": "提出HalluShift++方法，通过分析内部层动态检测多模态大语言模型中的幻觉问题。",
            "intro_zh": [
                "核心问题：MLLMs在视觉语言任务中常产生与视觉内容不一致的幻觉描述，现有评估方法依赖外部LLM评估器，易受幻觉影响且领域适应性差。",
                "方法要点：假设幻觉表现为MLLMs内部层动态的可测量异常，通过层间分析检测这些异常，扩展幻觉检测至多模态场景。",
                "实验或效果：HalluShift++提升了幻觉检测的效能，代码已开源，具体效果未知。"
            ],
            "tags_zh": [
                "多模态大语言模型",
                "幻觉检测",
                "内部层分析",
                "视觉语言理解",
                "模型评估"
            ],
            "_index": 52
        },
        {
            "title": "When Large Language Models Do Not Work: Online Incivility Prediction through Graph Neural Networks",
            "authors": [
                "Zihan Chen",
                "Lanyu Yu"
            ],
            "arxiv_id": "2512.07684v1",
            "summary": "Online incivility has emerged as a widespread and persistent problem in digital communities, imposing substantial social and psychological burdens on users. Although many platforms attempt to curb incivility through moderation and automated detection, the performance of existing approaches often remains limited in both accuracy and efficiency. To address this challenge, we propose a Graph Neural Network (GNN) framework for detecting three types of uncivil behavior (i.e., toxicity, aggression, and personal attacks) within the English Wikipedia community. Our model represents each user comment as a node, with textual similarity between comments defining the edges, allowing the network to jointly learn from both linguistic content and relational structures among comments. We also introduce a dynamically adjusted attention mechanism that adaptively balances nodal and topological features during information aggregation. Empirical evaluations demonstrate that our proposed architecture outperforms 12 state-of-the-art Large Language Models (LLMs) across multiple metrics while requiring significantly lower inference cost. These findings highlight the crucial role of structural context in detecting online incivility and address the limitations of text-only LLM paradigms in behavioral prediction. All datasets and comparative outputs will be publicly available in our repository to support further research and reproducibility.",
            "headline_zh": "提出基于图神经网络的框架，以解决在线社区不文明行为检测中准确性和效率不足的问题。",
            "intro_zh": [
                "在线不文明行为在数字社区中普遍存在，现有方法在准确性和效率上受限。",
                "模型将用户评论表示为节点，基于文本相似性构建边，结合语言内容和关系结构学习。",
                "实验显示，该框架在多个指标上优于12个大型语言模型，且推理成本显著降低。"
            ],
            "tags_zh": [
                "在线不文明行为检测",
                "图神经网络",
                "文本相似性",
                "注意力机制",
                "行为预测",
                "英语维基百科"
            ],
            "_index": 53
        },
        {
            "title": "AMBER: Aerial deployable gripping crawler with compliant microspine for canopy manipulation",
            "authors": [
                "P. A. Wigner",
                "L. Romanello",
                "A. Hammad",
                "P. H. Nguyen",
                "T. Lan",
                "S. F. Armanini",
                "B. B. Kocer",
                "M. Kovac"
            ],
            "arxiv_id": "2512.07680v1",
            "summary": "This paper presents an aerially deployable crawler designed for adaptive locomotion and manipulation within tree canopies. The system combines compliant microspine-based tracks, a dual-track rotary gripper, and an elastic tail, enabling secure attachment and stable traversal across branches of varying curvature and inclination.\n  Experiments demonstrate reliable gripping up to 90 degrees of body roll and inclination, while effective climbing on branches inclined up to 67.5 degrees, achieving a maximum speed of 0.55 body lengths per second on horizontal branches. The compliant tracks allow yaw steering of up to 10 degrees, enhancing maneuverability on irregular surfaces.\n  Power measurements show efficient operation with a dimensionless cost of transport over an order of magnitude lower than typical hovering power consumption in aerial robots. Integrated within a drone-tether deployment system, the crawler provides a robust, low-power platform for environmental sampling and in-canopy sensing, bridging the gap between aerial and surface-based ecological robotics.",
            "headline_zh": "提出一种空中部署的爬行机器人，用于树冠自适应移动与操作，结合柔性微刺轨道和弹性尾部。",
            "intro_zh": [
                "核心问题：树冠环境复杂，需要机器人能稳定附着和移动于不同曲率和倾斜度的树枝。",
                "方法要点：采用柔性微刺轨道、双轨旋转夹持器和弹性尾部，实现安全附着和稳定穿越。",
                "实验效果：在倾斜达67.5度的树枝上有效攀爬，最大速度0.55体长/秒，能耗低于典型空中机器人悬停功耗一个数量级。"
            ],
            "tags_zh": [
                "空中部署机器人",
                "柔性微刺轨道",
                "树冠操作",
                "自适应移动",
                "低功耗平台"
            ],
            "_index": 54
        },
        {
            "title": "A Bootstrap Perspective on Stochastic Gradient Descent",
            "authors": [
                "Hongjian Lan",
                "Yucong Liu",
                "Florian Schäfer"
            ],
            "arxiv_id": "2512.07676v1",
            "summary": "Machine learning models trained with \\emph{stochastic} gradient descent (SGD) can generalize better than those trained with deterministic gradient descent (GD). In this work, we study SGD's impact on generalization through the lens of the statistical bootstrap: SGD uses gradient variability under batch sampling as a proxy for solution variability under the randomness of the data collection process. We use empirical results and theoretical analysis to substantiate this claim. In idealized experiments on empirical risk minimization, we show that SGD is drawn to parameter choices that are robust under resampling and thus avoids spurious solutions even if they lie in wider and deeper minima of the training loss. We prove rigorously that by implicitly regularizing the trace of the gradient covariance matrix, SGD controls the algorithmic variability. This regularization leads to solutions that are less sensitive to sampling noise, thereby improving generalization. Numerical experiments on neural network training show that explicitly incorporating the estimate of the algorithmic variability as a regularizer improves test performance. This fact supports our claim that bootstrap estimation underpins SGD's generalization advantages.",
            "headline_zh": "提出基于统计自助法的视角解释SGD泛化优势，通过梯度协方差矩阵正则化控制算法变异性。",
            "intro_zh": [
                "核心问题：SGD相比确定性梯度下降为何能提升机器学习模型的泛化能力。",
                "方法要点：将SGD视为数据收集随机性的自助法代理，正则化梯度协方差矩阵迹以降低采样噪声敏感性。",
                "实验或效果：在经验风险最小化中验证SGD避免伪解，神经网络训练中显式正则化提升测试性能。"
            ],
            "tags_zh": [
                "随机梯度下降",
                "泛化能力",
                "统计自助法",
                "梯度协方差",
                "算法变异性",
                "正则化"
            ],
            "_index": 55
        },
        {
            "title": "DIST-CLIP: Arbitrary Metadata and Image Guided MRI Harmonization via Disentangled Anatomy-Contrast Representations",
            "authors": [
                "Mehmet Yigit Avci",
                "Pedro Borges",
                "Virginia Fernandez",
                "Paul Wright",
                "Mehmet Yigitsoy",
                "Sebastien Ourselin",
                "Jorge Cardoso"
            ],
            "arxiv_id": "2512.07674v1",
            "summary": "Deep learning holds immense promise for transforming medical image analysis, yet its clinical generalization remains profoundly limited. A major barrier is data heterogeneity. This is particularly true in Magnetic Resonance Imaging, where scanner hardware differences, diverse acquisition protocols, and varying sequence parameters introduce substantial domain shifts that obscure underlying biological signals. Data harmonization methods aim to reduce these instrumental and acquisition variability, but existing approaches remain insufficient. When applied to imaging data, image-based harmonization approaches are often restricted by the need for target images, while existing text-guided methods rely on simplistic labels that fail to capture complex acquisition details or are typically restricted to datasets with limited variability, failing to capture the heterogeneity of real-world clinical environments. To address these limitations, we propose DIST-CLIP (Disentangled Style Transfer with CLIP Guidance), a unified framework for MRI harmonization that flexibly uses either target images or DICOM metadata for guidance. Our framework explicitly disentangles anatomical content from image contrast, with the contrast representations being extracted using pre-trained CLIP encoders. These contrast embeddings are then integrated into the anatomical content via a novel Adaptive Style Transfer module. We trained and evaluated DIST-CLIP on diverse real-world clinical datasets, and showed significant improvements in performance when compared against state-of-the-art methods in both style translation fidelity and anatomical preservation, offering a flexible solution for style transfer and standardizing MRI data. Our code and weights will be made publicly available upon publication.",
            "headline_zh": "提出DIST-CLIP框架，通过解耦解剖-对比表示实现任意元数据和图像引导的MRI数据协调",
            "intro_zh": [
                "核心问题：MRI数据因扫描仪和协议差异导致异质性，现有方法依赖目标图像或简单标签，难以处理真实临床环境。",
                "方法要点：使用预训练CLIP编码器提取对比表示，通过自适应风格转移模块整合到解剖内容中，支持图像或DICOM元数据引导。",
                "实验或效果：在真实临床数据集上评估，相比先进方法在风格转换保真度和解剖保留方面表现显著提升。"
            ],
            "tags_zh": [
                "MRI数据协调",
                "解耦表示",
                "CLIP引导",
                "自适应风格转移",
                "医学图像分析"
            ],
            "_index": 56
        },
        {
            "title": "Multi-Domain Motion Embedding: Expressive Real-Time Mimicry for Legged Robots",
            "authors": [
                "Matthias Heyrman",
                "Chenhao Li",
                "Victor Klemm",
                "Dongho Kang",
                "Stelian Coros",
                "Marco Hutter"
            ],
            "arxiv_id": "2512.07673v1",
            "summary": "Effective motion representation is crucial for enabling robots to imitate expressive behaviors in real time, yet existing motion controllers often ignore inherent patterns in motion. Previous efforts in representation learning do not attempt to jointly capture structured periodic patterns and irregular variations in human and animal movement. To address this, we present Multi-Domain Motion Embedding (MDME), a motion representation that unifies the embedding of structured and unstructured features using a wavelet-based encoder and a probabilistic embedding in parallel. This produces a rich representation of reference motions from a minimal input set, enabling improved generalization across diverse motion styles and morphologies. We evaluate MDME on retargeting-free real-time motion imitation by conditioning robot control policies on the learned embeddings, demonstrating accurate reproduction of complex trajectories on both humanoid and quadruped platforms. Our comparative studies confirm that MDME outperforms prior approaches in reconstruction fidelity and generalizability to unseen motions. Furthermore, we demonstrate that MDME can reproduce novel motion styles in real-time through zero-shot deployment, eliminating the need for task-specific tuning or online retargeting. These results position MDME as a generalizable and structure-aware foundation for scalable real-time robot imitation.",
            "headline_zh": "提出多域运动嵌入以解决腿式机器人实时模仿中运动表示不足的问题",
            "intro_zh": [
                "核心问题：现有运动控制器忽略运动固有模式，难以联合捕捉结构化周期模式和非规则变化",
                "方法要点：使用基于小波的编码器和概率嵌入并行统一嵌入结构化和非结构化特征",
                "实验或效果：在类人形和四足机器人上实现无重定向实时模仿，优于先前方法，支持零样本部署"
            ],
            "tags_zh": [
                "运动表示学习",
                "实时机器人模仿",
                "多域嵌入",
                "小波编码",
                "概率嵌入",
                "零样本部署"
            ],
            "_index": 57
        },
        {
            "title": "EgoCampus: Egocentric Pedestrian Eye Gaze Model and Dataset",
            "authors": [
                "Ronan John",
                "Aditya Kesari",
                "Vincenzo DiMatteo",
                "Kristin Dana"
            ],
            "arxiv_id": "2512.07668v1",
            "summary": "We address the challenge of predicting human visual attention during real-world navigation by measuring and modeling egocentric pedestrian eye gaze in an outdoor campus setting. We introduce the EgoCampus dataset, which spans 25 unique outdoor paths over 6 km across a university campus with recordings from more than 80 distinct human pedestrians, resulting in a diverse set of gaze-annotated videos. The system used for collection, Meta's Project Aria glasses, integrates eye tracking, front-facing RGB cameras, inertial sensors, and GPS to provide rich data from the human perspective. Unlike many prior egocentric datasets that focus on indoor tasks or exclude eye gaze information, our work emphasizes visual attention while subjects walk in outdoor campus paths. Using this data, we develop EgoCampusNet, a novel method to predict eye gaze of navigating pedestrians as they move through outdoor environments. Our contributions provide both a new resource for studying real-world attention and a resource for future work in gaze prediction models for navigation. Dataset and code are available upon request, and will be made publicly available at a later date at https://github.com/ComputerVisionRutgers/EgoCampus .",
            "headline_zh": "提出EgoCampus数据集和EgoCampusNet方法，以预测户外校园环境中行人导航时的视觉注意力。",
            "intro_zh": [
                "核心问题：预测真实世界导航中的人类视觉注意力，特别是在户外校园环境下的行人眼动注视。",
                "方法要点：使用Meta's Project Aria眼镜收集数据，开发EgoCampusNet模型预测行人眼动注视。",
                "实验或效果：数据集包含超过80名行人的6公里户外路径视频，提供眼动注释，模型效果未知。"
            ],
            "tags_zh": [
                "自我中心视觉",
                "眼动注视预测",
                "户外导航",
                "数据集构建",
                "行人行为分析"
            ],
            "_index": 58
        },
        {
            "title": "Depth-Wise Activation Steering for Honest Language Models",
            "authors": [
                "Gracjan Góral",
                "Marysia Winkels",
                "Steven Basart"
            ],
            "arxiv_id": "2512.07667v1",
            "summary": "Large language models sometimes assert falsehoods despite internally representing the correct answer, failures of honesty rather than accuracy, which undermines auditability and safety. Existing approaches largely optimize factual correctness or depend on retraining and brittle single-layer edits, offering limited leverage over truthful reporting. We present a training-free activation steering method that weights steering strength across network depth using a Gaussian schedule. On the MASK benchmark, which separates honesty from knowledge, we evaluate seven models spanning the LLaMA, Qwen, and Mistral families and find that Gaussian scheduling improves honesty over no-steering and single-layer baselines in six of seven models. Equal-budget ablations on LLaMA-3.1-8B-Instruct and Qwen-2.5-7B-Instruct show the Gaussian schedule outperforms random, uniform, and box-filter depth allocations, indicating that how intervention is distributed across depth materially affects outcomes beyond total strength. The method is simple, model-agnostic, requires no finetuning, and provides a low-cost control knob for eliciting truthful reporting from models' existing capabilities.",
            "headline_zh": "提出深度激活导向方法以提升语言模型的诚实性",
            "intro_zh": [
                "核心问题：大型语言模型有时内部知道正确答案却输出错误，属于诚实性而非准确性失败。",
                "方法要点：使用高斯调度在深度上加权激活导向强度，无需训练或微调。",
                "实验或效果：在MASK基准上，高斯调度在七分之六模型中提升诚实性，优于单层和均匀分配基线。"
            ],
            "tags_zh": [
                "激活导向",
                "诚实性评估",
                "高斯调度",
                "训练免费方法",
                "深度加权"
            ],
            "_index": 59
        },
        {
            "title": "Optimization-Guided Diffusion for Interactive Scene Generation",
            "authors": [
                "Shiaho Li",
                "Naisheng Ye",
                "Tianyu Li",
                "Kashyap Chitta",
                "Tuo An",
                "Peng Su",
                "Boyang Wang",
                "Haiou Liu",
                "Chen Lv",
                "Hongyang Li"
            ],
            "arxiv_id": "2512.07661v1",
            "summary": "Realistic and diverse multi-agent driving scenes are crucial for evaluating autonomous vehicles, but safety-critical events which are essential for this task are rare and underrepresented in driving datasets. Data-driven scene generation offers a low-cost alternative by synthesizing complex traffic behaviors from existing driving logs. However, existing models often lack controllability or yield samples that violate physical or social constraints, limiting their usability. We present OMEGA, an optimization-guided, training-free framework that enforces structural consistency and interaction awareness during diffusion-based sampling from a scene generation model. OMEGA re-anchors each reverse diffusion step via constrained optimization, steering the generation towards physically plausible and behaviorally coherent trajectories. Building on this framework, we formulate ego-attacker interactions as a game-theoretic optimization in the distribution space, approximating Nash equilibria to generate realistic, safety-critical adversarial scenarios. Experiments on nuPlan and Waymo show that OMEGA improves generation realism, consistency, and controllability, increasing the ratio of physically and behaviorally valid scenes from 32.35% to 72.27% for free exploration capabilities, and from 11% to 80% for controllability-focused generation. Our approach can also generate $5\\times$ more near-collision frames with a time-to-collision under three seconds while maintaining the overall scene realism.",
            "headline_zh": "提出OMEGA框架，通过优化引导扩散生成物理合理且行为一致的自动驾驶场景",
            "intro_zh": [
                "问题：现有数据驱动场景生成模型缺乏可控性，常违反物理或社会约束，影响评估自动驾驶车辆的安全性。",
                "方法：OMEGA在扩散采样中引入优化引导，通过约束优化重锚反向扩散步骤，确保轨迹的物理合理性和行为一致性。",
                "效果：在nuPlan和Waymo数据集上，OMEGA显著提升场景有效性和可控性，并生成更多安全关键对抗场景。"
            ],
            "tags_zh": [
                "自动驾驶场景生成",
                "扩散模型",
                "优化引导采样",
                "物理约束",
                "行为一致性",
                "对抗场景生成"
            ],
            "_index": 60
        },
        {
            "title": "An AI-Powered Autonomous Underwater System for Sea Exploration and Scientific Research",
            "authors": [
                "Hamad Almazrouei",
                "Mariam Al Nasseri",
                "Maha Alzaabi"
            ],
            "arxiv_id": "2512.07652v1",
            "summary": "Traditional sea exploration faces significant challenges due to extreme conditions, limited visibility, and high costs, resulting in vast unexplored ocean regions. This paper presents an innovative AI-powered Autonomous Underwater Vehicle (AUV) system designed to overcome these limitations by automating underwater object detection, analysis, and reporting. The system integrates YOLOv12 Nano for real-time object detection, a Convolutional Neural Network (CNN) (ResNet50) for feature extraction, Principal Component Analysis (PCA) for dimensionality reduction, and K-Means++ clustering for grouping marine objects based on visual characteristics. Furthermore, a Large Language Model (LLM) (GPT-4o Mini) is employed to generate structured reports and summaries of underwater findings, enhancing data interpretation. The system was trained and evaluated on a combined dataset of over 55,000 images from the DeepFish and OzFish datasets, capturing diverse Australian marine environments. Experimental results demonstrate the system's capability to detect marine objects with a mAP@0.5 of 0.512, a precision of 0.535, and a recall of 0.438. The integration of PCA effectively reduced feature dimensionality while preserving 98% variance, facilitating K-Means clustering which successfully grouped detected objects based on visual similarities. The LLM integration proved effective in generating insightful summaries of detections and clusters, supported by location data. This integrated approach significantly reduces the risks associated with human diving, increases mission efficiency, and enhances the speed and depth of underwater data analysis, paving the way for more effective scientific research and discovery in challenging marine environments.",
            "headline_zh": "提出AI驱动的自主水下系统，集成目标检测与LLM生成报告，以提升海洋探索效率。",
            "intro_zh": [
                "传统海洋探索面临极端条件和高成本挑战，导致大片区域未勘探。",
                "系统结合YOLOv12 Nano、CNN、PCA和K-Means++进行实时检测与聚类，并集成LLM生成结构化报告。",
                "在55,000+图像数据集上评估，mAP@0.5达0.512，PCA降维保留98%方差，LLM有效生成洞察总结。"
            ],
            "tags_zh": [
                "自主水下车辆",
                "目标检测",
                "特征提取",
                "聚类分析",
                "大语言模型",
                "海洋探索"
            ],
            "_index": 61
        },
        {
            "title": "Liver Fibrosis Quantification and Analysis: The LiQA Dataset and Baseline Method",
            "authors": [
                "Yuanye Liu",
                "Hanxiao Zhang",
                "Nannan Shi",
                "Yuxin Shi",
                "Arif Mahmood",
                "Murtaza Taj",
                "Xiahai Zhuang"
            ],
            "arxiv_id": "2512.07651v1",
            "summary": "Liver fibrosis represents a significant global health burden, necessitating accurate staging for effective clinical management. This report introduces the LiQA (Liver Fibrosis Quantification and Analysis) dataset, established as part of the CARE 2024 challenge. Comprising $440$ patients with multi-phase, multi-center MRI scans, the dataset is curated to benchmark algorithms for Liver Segmentation (LiSeg) and Liver Fibrosis Staging (LiFS) under complex real-world conditions, including domain shifts, missing modalities, and spatial misalignment. We further describe the challenge's top-performing methodology, which integrates a semi-supervised learning framework with external data for robust segmentation, and utilizes a multi-view consensus approach with Class Activation Map (CAM)-based regularization for staging. Evaluation of this baseline demonstrates that leveraging multi-source data and anatomical constraints significantly enhances model robustness in clinical settings.",
            "headline_zh": "提出LiQA数据集与基线方法，用于复杂临床条件下的肝纤维化量化分析",
            "intro_zh": [
                "核心问题：肝纤维化准确分期对临床管理至关重要，需应对多中心、多模态MRI数据中的域偏移、缺失模态和空间错位等挑战。",
                "方法要点：采用半监督学习框架结合外部数据进行稳健分割，并利用多视图共识与CAM正则化进行分期。",
                "实验或效果：评估显示，利用多源数据和解剖约束显著提升了模型在临床环境中的鲁棒性。"
            ],
            "tags_zh": [
                "肝纤维化分期",
                "多模态MRI",
                "半监督学习",
                "多视图共识",
                "临床数据集"
            ],
            "_index": 62
        },
        {
            "title": "Exploring Test-time Scaling via Prediction Merging on Large-Scale Recommendation",
            "authors": [
                "Fuyuan Lyu",
                "Zhentai Chen",
                "Jingyan Jiang",
                "Lingjie Li",
                "Xing Tang",
                "Xiuqiang He",
                "Xue Liu"
            ],
            "arxiv_id": "2512.07650v1",
            "summary": "Inspired by the success of language models (LM), scaling up deep learning recommendation systems (DLRS) has become a recent trend in the community. All previous methods tend to scale up the model parameters during training time. However, how to efficiently utilize and scale up computational resources during test time remains underexplored, which can prove to be a scaling-efficient approach and bring orthogonal improvements in LM domains. The key point in applying test-time scaling to DLRS lies in effectively generating diverse yet meaningful outputs for the same instance. We propose two ways: One is to explore the heterogeneity of different model architectures. The other is to utilize the randomness of model initialization under a homogeneous architecture. The evaluation is conducted across eight models, including both classic and SOTA models, on three benchmarks. Sufficient evidence proves the effectiveness of both solutions. We further prove that under the same inference budget, test-time scaling can outperform parameter scaling. Our test-time scaling can also be seamlessly accelerated with the increase in parallel servers when deployed online, without affecting the inference time on the user side. Code is available.",
            "headline_zh": "提出测试时缩放方法，通过预测合并提升大规模推荐系统效率",
            "intro_zh": [
                "核心问题：如何在测试时高效利用计算资源，替代传统训练时参数缩放，以提升推荐系统性能",
                "方法要点：利用模型架构异质性或同构初始化随机性生成多样化预测，并通过合并增强输出",
                "实验或效果：在三个基准测试中评估八种模型，证明方法有效且优于参数缩放，支持在线并行加速"
            ],
            "tags_zh": [
                "测试时缩放",
                "预测合并",
                "大规模推荐系统",
                "模型异质性",
                "计算效率",
                "在线推理"
            ],
            "_index": 63
        },
        {
            "title": "A Mathematical Theory of Top-$k$ Sparse Attention via Total Variation Distance",
            "authors": [
                "Georgios Tzachristas",
                "Lei Deng",
                "Ioannis Tzachristas",
                "Gong Zhang",
                "Renhai Chen"
            ],
            "arxiv_id": "2512.07647v1",
            "summary": "We develop a unified mathematical framework for certified Top-$k$ attention truncation that quantifies approximation error at both the distribution and output levels. For a single attention distribution $P$ and its Top-$k$ truncation $\\hat P$, we show that the total-variation distance coincides with the discarded softmax tail mass and satisfies $\\mathrm{TV}(P,\\hat P)=1-e^{-\\mathrm{KL}(\\hat P\\Vert P)}$, yielding sharp Top-$k$-specific bounds in place of generic inequalities. From this we derive non-asymptotic deterministic bounds -- from a single boundary gap through multi-gap and blockwise variants -- that control $\\mathrm{TV}(P,\\hat P)$ using only the ordered logits. Using an exact head-tail decomposition, we prove that the output error factorizes as $\\|\\mathrm{Attn}(q,K,V)-\\mathrm{Attn}_k(q,K,V)\\|_2=τ\\|μ_{\\mathrm{tail}}-μ_{\\mathrm{head}}\\|_2$ with $τ=\\mathrm{TV}(P,\\hat P)$, yielding a new head-tail diameter bound $\\|\\mathrm{Attn}(q,K,V)-\\mathrm{Attn}_k(q,K,V)\\|_2\\leτ\\,\\mathrm{diam}_{H,T}$ and refinements linking the error to $\\mathrm{Var}_P(V)$. Under an i.i.d. Gaussian score model $s_i\\sim\\mathcal N(μ,σ^2)$ we derive closed-form tail masses and an asymptotic rule for the minimal $k_\\varepsilon$ ensuring $\\mathrm{TV}(P,\\hat P)\\le\\varepsilon$, namely $k_\\varepsilon/n\\approxΦ_c(σ+Φ^{-1}(\\varepsilon))$. Experiments on bert-base-uncased and synthetic logits confirm the predicted scaling of $k_\\varepsilon/n$ and show that certified Top-$k$ can reduce scored keys by 2-4$\\times$ on average while meeting the prescribed total-variation budget.",
            "headline_zh": "提出基于总变差距离的Top-k稀疏注意力数学理论，量化截断误差并提供确定性边界。",
            "intro_zh": [
                "核心问题：Top-k注意力截断的近似误差缺乏统一数学框架与确定性边界。",
                "方法要点：利用总变差距离量化分布与输出误差，推导非渐近边界及头尾分解公式。",
                "实验或效果：在BERT和合成数据上验证理论，平均减少2-4倍键值计算并满足误差预算。"
            ],
            "tags_zh": [
                "注意力机制",
                "稀疏注意力",
                "总变差距离",
                "数学理论",
                "误差分析",
                "Top-k截断"
            ],
            "_index": 64
        },
        {
            "title": "The Agent Capability Problem: Predicting Solvability Through Information-Theoretic Bounds",
            "authors": [
                "Shahar Lutati"
            ],
            "arxiv_id": "2512.07631v1",
            "summary": "When should an autonomous agent commit resources to a task? We introduce the Agent Capability Problem (ACP), a framework for predicting whether an agent can solve a problem under resource constraints. Rather than relying on empirical heuristics, ACP frames problem-solving as information acquisition: an agent requires $\\Itotal$ bits to identify a solution and gains $\\Istep$ bits per action at cost $\\Cstep$, yielding an effective cost $\\Ceff = (\\Itotal/\\Istep), \\Cstep$ that predicts resource requirements before search. We prove that $\\Ceff$ lower-bounds expected cost and provide tight probabilistic upper bounds. Experimental validation shows that ACP predictions closely track actual agent performance, consistently bounding search effort while improving efficiency over greedy and random strategies. The framework generalizes across LLM-based and agentic workflows, linking principles from active learning, Bayesian optimization, and reinforcement learning through a unified information-theoretic lens. \\",
            "headline_zh": "提出代理能力问题框架，通过信息论界限预测资源约束下任务可解性",
            "intro_zh": [
                "核心问题：代理在资源约束下何时应投入资源以解决任务，避免无效搜索",
                "方法要点：将问题解决建模为信息获取，定义总信息需求与每步信息增益，推导有效成本界限",
                "实验或效果：验证预测与实际性能紧密匹配，优于贪婪和随机策略，泛化于LLM和代理工作流"
            ],
            "tags_zh": [
                "代理能力问题",
                "信息论界限",
                "资源约束预测",
                "自主代理",
                "问题解决建模",
                "实验验证"
            ],
            "_index": 65
        },
        {
            "title": "MoCA: Mixture-of-Components Attention for Scalable Compositional 3D Generation",
            "authors": [
                "Zhiqi Li",
                "Wenhuan Li",
                "Tengfei Wang",
                "Zhenwei Wang",
                "Junta Wu",
                "Haoyuan Wang",
                "Yunhan Yang",
                "Zehuan Huang",
                "Yang Li",
                "Peidong Liu",
                "Chunchao Guo"
            ],
            "arxiv_id": "2512.07628v1",
            "summary": "Compositionality is critical for 3D object and scene generation, but existing part-aware 3D generation methods suffer from poor scalability due to quadratic global attention costs when increasing the number of components. In this work, we present MoCA, a compositional 3D generative model with two key designs: (1) importance-based component routing that selects top-k relevant components for sparse global attention, and (2) unimportant components compression that preserve contextual priors of unselected components while reducing computational complexity of global attention. With these designs, MoCA enables efficient, fine-grained compositional 3D asset creation with scalable number of components. Extensive experiments show MoCA outperforms baselines on both compositional object and scene generation tasks. Project page: https://lizhiqi49.github.io/MoCA",
            "headline_zh": "提出MoCA注意力机制以解决组合式3D生成中全局注意力计算成本高的问题",
            "intro_zh": [
                "核心问题：组合式3D生成方法在增加组件数量时，全局注意力计算成本呈二次增长，导致可扩展性差。",
                "方法要点：设计重要性组件路由选择top-k相关组件进行稀疏全局注意力，并对未选组件进行压缩以保留上下文先验。",
                "实验或效果：在组合式物体和场景生成任务上，MoCA优于基线方法，支持高效、细粒度的3D资产创建。"
            ],
            "tags_zh": [
                "组合式3D生成",
                "注意力机制",
                "可扩展性",
                "稀疏全局注意力",
                "组件路由",
                "计算优化"
            ],
            "_index": 66
        },
        {
            "title": "Incorporating Structure and Chord Constraints in Symbolic Transformer-based Melodic Harmonization",
            "authors": [
                "Maximos Kaliakatsos-Papakostas",
                "Konstantinos Soiledis",
                "Theodoros Tsamis",
                "Dimos Makris",
                "Vassilis Katsouros",
                "Emilios Cambouropoulos"
            ],
            "arxiv_id": "2512.07627v1",
            "summary": "Transformer architectures offer significant advantages regarding the generation of symbolic music; their capabilities for incorporating user preferences toward what they generate is being studied under many aspects. This paper studies the inclusion of predefined chord constraints in melodic harmonization, i.e., where a desired chord at a specific location is provided along with the melody as inputs and the autoregressive transformer model needs to incorporate the chord in the harmonization that it generates. The peculiarities of involving such constraints is discussed and an algorithm is proposed for tackling this task. This algorithm is called B* and it combines aspects of beam search and A* along with backtracking to force pretrained transformers to satisfy the chord constraints, at the correct onset position within the correct bar. The algorithm is brute-force and has exponential complexity in the worst case; however, this paper is a first attempt to highlight the difficulties of the problem and proposes an algorithm that offers many possibilities for improvements since it accommodates the involvement of heuristics.",
            "headline_zh": "提出B*算法以解决基于Transformer的旋律和声中融入预定义和弦约束的问题",
            "intro_zh": [
                "核心问题：如何在自回归Transformer模型中强制融入特定位置的和弦约束，确保和声生成符合用户输入",
                "方法要点：结合束搜索、A*算法和回溯，设计B*算法，以指数复杂度强制模型满足约束",
                "实验或效果：算法为首次尝试，突出任务难度，提供改进空间，但复杂度高，效果未知"
            ],
            "tags_zh": [
                "旋律和声",
                "Transformer模型",
                "和弦约束",
                "B*算法",
                "符号音乐生成",
                "自回归生成"
            ],
            "_index": 67
        },
        {
            "title": "Time Series Foundation Models for Process Model Forecasting",
            "authors": [
                "Yongbo Yu",
                "Jari Peeperkorn",
                "Johannes De Smedt",
                "Jochen De Weerdt"
            ],
            "arxiv_id": "2512.07624v1",
            "summary": "Process Model Forecasting (PMF) aims to predict how the control-flow structure of a process evolves over time by modeling the temporal dynamics of directly-follows (DF) relations, complementing predictive process monitoring that focuses on single-case prefixes. Prior benchmarks show that machine learning and deep learning models provide only modest gains over statistical baselines, mainly due to the sparsity and heterogeneity of the DF time series. We investigate Time Series Foundation Models (TSFMs), large pre-trained models for generic time series, as an alternative for PMF. Using DF time series derived from real-life event logs, we compare zero-shot use of TSFMs, without additional training, with fine-tuned variants adapted on PMF-specific data. TSFMs generally achieve lower forecasting errors (MAE and RMSE) than traditional and specialized models trained from scratch on the same logs, indicating effective transfer of temporal structure from non-process domains. While fine-tuning can further improve accuracy, the gains are often small and may disappear on smaller or more complex datasets, so zero-shot use remains a strong default. Our study highlights the generalization capability and data efficiency of TSFMs for process-related time series and, to the best of our knowledge, provides the first systematic evaluation of temporal foundation models for PMF.",
            "headline_zh": "评估时间序列基础模型在过程模型预测中的零样本与微调性能",
            "intro_zh": [
                "过程模型预测面临直接跟随关系时间序列的稀疏性和异质性挑战",
                "研究采用时间序列基础模型进行零样本预测和微调，与传统方法对比",
                "实验显示基础模型在多数数据集上优于传统模型，零样本使用效果稳定"
            ],
            "tags_zh": [
                "过程模型预测",
                "时间序列基础模型",
                "零样本学习",
                "微调",
                "直接跟随关系",
                "预测性能评估"
            ],
            "_index": 68
        },
        {
            "title": "PCMind-2.1-Kaiyuan-2B Technical Report",
            "authors": [
                "Kairong Luo",
                "Zhenbo Sun",
                "Xinyu Shi",
                "Shengqi Chen",
                "Bowen Yu",
                "Yunyi Chen",
                "Chenyi Dang",
                "Hengtao Tao",
                "Hui Wang",
                "Fangming Liu",
                "Kaifeng Lyu",
                "Wenguang Chen"
            ],
            "arxiv_id": "2512.07612v1",
            "summary": "The rapid advancement of Large Language Models (LLMs) has resulted in a significant knowledge gap between the open-source community and industry, primarily because the latter relies on closed-source, high-quality data and training recipes. To address this, we introduce PCMind-2.1-Kaiyuan-2B, a fully open-source 2-billion-parameter model focused on improving training efficiency and effectiveness under resource constraints. Our methodology includes three key innovations: a Quantile Data Benchmarking method for systematically comparing heterogeneous open-source datasets and providing insights on data mixing strategies; a Strategic Selective Repetition scheme within a multi-phase paradigm to effectively leverage sparse, high-quality data; and a Multi-Domain Curriculum Training policy that orders samples by quality. Supported by a highly optimized data preprocessing pipeline and architectural modifications for FP16 stability, Kaiyuan-2B achieves performance competitive with state-of-the-art fully open-source models, demonstrating practical and scalable solutions for resource-limited pretraining. We release all assets (including model weights, data, and code) under Apache 2.0 license at https://huggingface.co/thu-pacman/PCMind-2.1-Kaiyuan-2B.",
            "headline_zh": "提出PCMind-2.1-Kaiyuan-2B，通过数据基准、选择性重复和多领域课程训练，在资源受限下提升大语言模型训练效率与效果。",
            "intro_zh": [
                "核心问题：开源社区与产业间因闭源高质量数据和训练方法存在知识鸿沟，资源受限下训练效率低。",
                "方法要点：采用分位数数据基准法比较异构数据集，多阶段策略选择性重复利用稀疏高质量数据，多领域课程训练按质量排序样本。",
                "实验或效果：模型性能与顶尖开源模型竞争，提供可扩展的预训练解决方案，所有资产在Apache 2.0许可下开源。"
            ],
            "tags_zh": [
                "大语言模型",
                "数据基准",
                "选择性重复",
                "课程训练",
                "开源模型",
                "资源受限训练"
            ],
            "_index": 69
        },
        {
            "title": "Comparative Analysis and Parametric Tuning of PPO, GRPO, and DAPO for LLM Reasoning Enhancement",
            "authors": [
                "Yongsheng Lian"
            ],
            "arxiv_id": "2512.07611v1",
            "summary": "This study presents a systematic comparison of three Reinforcement Learning (RL) algorithms (PPO, GRPO, and DAPO) for improving complex reasoning in large language models (LLMs). Our main contribution is a controlled transfer-learning evaluation: models are first fine-tuned on the specialized Countdown Game and then assessed on a suite of general-purpose reasoning benchmarks. Across all tasks, RL-trained models outperform their corresponding base models, although the degree of improvement differs by benchmark.\n  Our parametric analysis offers practical guidance for RL-based LLM training. Increasing the group size in GRPO and DAPO leads to more stable training dynamics and higher accuracy, while the impact of the KL-penalty coefficient is non-monotonic. Additionally, we find that the Dynamic Sampling (DS) component in DAPO does not improve performance; in fact, the best overall results are achieved with DAPO when DS is disabled.",
            "headline_zh": "系统比较PPO、GRPO和DAPO三种强化学习算法，以增强大语言模型的复杂推理能力。",
            "intro_zh": [
                "核心问题：如何有效利用强化学习提升大语言模型在复杂推理任务中的性能。",
                "方法要点：采用控制转移学习评估，先在Countdown Game上微调，再在通用推理基准上测试。",
                "实验或效果：RL训练模型在所有任务上优于基础模型，但改进程度因基准而异；参数分析提供实用指导。"
            ],
            "tags_zh": [
                "强化学习",
                "大语言模型",
                "推理增强",
                "参数调优",
                "转移学习"
            ],
            "_index": 70
        },
        {
            "title": "Obstacle Avoidance of UAV in Dynamic Environments Using Direction and Velocity-Adaptive Artificial Potential Field",
            "authors": [
                "Nikita Vaibhav Pavle",
                "Shrreya Rajneesh",
                "Rakesh Kumar Sahoo",
                "Manoranjan Sinha"
            ],
            "arxiv_id": "2512.07609v1",
            "summary": "The conventional Artificial Potential Field (APF) is fundamentally limited by the local minima issue and its inability to account for the kinematics of moving obstacles. This paper addresses the critical challenge of autonomous collision avoidance for Unmanned Aerial Vehicles (UAVs) operating in dynamic and cluttered airspace by proposing a novel Direction and Relative Velocity Weighted Artificial Potential Field (APF). In this approach, a bounded weighting function, $ω(θ,v_{e})$, is introduced to dynamically scale the repulsive potential based on the direction and velocity of the obstacle relative to the UAV. This robust APF formulation is integrated within a Model Predictive Control (MPC) framework to generate collision-free trajectories while adhering to kinematic constraints. Simulation results demonstrate that the proposed method effectively resolves local minima and significantly enhances safety by enabling smooth, predictive avoidance maneuvers. The system ensures superior path integrity and reliable performance, confirming its viability for autonomous navigation in complex environments.",
            "headline_zh": "提出方向与相对速度加权人工势场以解决无人机动态环境避障问题",
            "intro_zh": [
                "核心问题：传统人工势场存在局部极小点且无法处理移动障碍物运动学。",
                "方法要点：引入有界加权函数动态调整排斥势，结合模型预测控制生成轨迹。",
                "实验或效果：仿真显示方法有效解决局部极小点，提升安全性和路径完整性。"
            ],
            "tags_zh": [
                "无人机避障",
                "人工势场",
                "动态环境",
                "模型预测控制",
                "轨迹规划"
            ],
            "_index": 71
        },
        {
            "title": "Metric-Fair Prompting: Treating Similar Samples Similarly",
            "authors": [
                "Jing Wang",
                "Jie Shen",
                "Xing Niu",
                "Tong Zhang",
                "Jeremy Weiss"
            ],
            "arxiv_id": "2512.07608v1",
            "summary": "We introduce \\emph{Metric-Fair Prompting}, a fairness-aware prompting framework that guides large language models (LLMs) to make decisions under metric-fairness constraints. In the application of multiple-choice medical question answering, each {(question, option)} pair is treated as a binary instance with label $+1$ (correct) or $-1$ (incorrect). To promote {individual fairness}~--~treating similar instances similarly~--~we compute question similarity using NLP embeddings and solve items in \\emph{joint pairs of similar questions} rather than in isolation. The prompt enforces a global decision protocol: extract decisive clinical features, map each \\((\\text{question}, \\text{option})\\) to a score $f(x)$ that acts as confidence, and impose a Lipschitz-style constraint so that similar inputs receive similar scores and, hence, consistent outputs. Evaluated on the {MedQA (US)} benchmark, Metric-Fair Prompting is shown to improve performance over standard single-item prompting, demonstrating that fairness-guided, confidence-oriented reasoning can enhance LLM accuracy on high-stakes clinical multiple-choice questions.",
            "headline_zh": "提出度量公平提示框架，以提升大型语言模型在医学多选题中的个体公平性和准确性。",
            "intro_zh": [
                "核心问题：在医学多选题中，标准提示可能忽略相似问题间的公平性，导致不一致决策。",
                "方法要点：基于NLP嵌入计算问题相似度，通过联合处理相似问题对并施加Lipschitz约束，确保相似输入获得相似分数。",
                "实验或效果：在MedQA基准上，该框架相比标准单题提示提高了性能，证明公平引导的置信度推理能增强模型准确性。"
            ],
            "tags_zh": [
                "度量公平提示",
                "个体公平性",
                "医学问答",
                "大型语言模型",
                "Lipschitz约束",
                "置信度评分"
            ],
            "_index": 72
        },
        {
            "title": "Decomposition Sampling for Efficient Region Annotations in Active Learning",
            "authors": [
                "Jingna Qiu",
                "Frauke Wilm",
                "Mathias Öttl",
                "Jonas Utz",
                "Maja Schlereth",
                "Moritz Schillinger",
                "Marc Aubreville",
                "Katharina Breininger"
            ],
            "arxiv_id": "2512.07606v1",
            "summary": "Active learning improves annotation efficiency by selecting the most informative samples for annotation and model training. While most prior work has focused on selecting informative images for classification tasks, we investigate the more challenging setting of dense prediction, where annotations are more costly and time-intensive, especially in medical imaging. Region-level annotation has been shown to be more efficient than image-level annotation for these tasks. However, existing methods for representative annotation region selection suffer from high computational and memory costs, irrelevant region choices, and heavy reliance on uncertainty sampling. We propose decomposition sampling (DECOMP), a new active learning sampling strategy that addresses these limitations. It enhances annotation diversity by decomposing images into class-specific components using pseudo-labels and sampling regions from each class. Class-wise predictive confidence further guides the sampling process, ensuring that difficult classes receive additional annotations. Across ROI classification, 2-D segmentation, and 3-D segmentation, DECOMP consistently surpasses baseline methods by better sampling minority-class regions and boosting performance on these challenging classes. Code is in https://github.com/JingnaQiu/DECOMP.git.",
            "headline_zh": "提出分解采样以提升密集预测任务中区域标注效率",
            "intro_zh": [
                "核心问题：现有区域标注选择方法计算成本高、依赖不确定性采样且区域选择不相关",
                "方法要点：通过伪标签分解图像为类特定组件，从每类采样区域，结合类置信度指导",
                "实验或效果：在ROI分类、2D和3D分割中超越基线，提升少数类区域采样和性能"
            ],
            "tags_zh": [
                "主动学习",
                "密集预测",
                "区域标注",
                "分解采样",
                "医学影像",
                "少数类采样"
            ],
            "_index": 73
        },
        {
            "title": "Online Segment Any 3D Thing as Instance Tracking",
            "authors": [
                "Hanshi Wang",
                "Zijian Cai",
                "Jin Gao",
                "Yiwei Zhang",
                "Weiming Hu",
                "Ke Wang",
                "Zhipeng Zhang"
            ],
            "arxiv_id": "2512.07599v1",
            "summary": "Online, real-time, and fine-grained 3D segmentation constitutes a fundamental capability for embodied intelligent agents to perceive and comprehend their operational environments. Recent advancements employ predefined object queries to aggregate semantic information from Vision Foundation Models (VFMs) outputs that are lifted into 3D point clouds, facilitating spatial information propagation through inter-query interactions. Nevertheless, perception is an inherently dynamic process, rendering temporal understanding a critical yet overlooked dimension within these prevailing query-based pipelines. Therefore, to further unlock the temporal environmental perception capabilities of embodied agents, our work reconceptualizes online 3D segmentation as an instance tracking problem (AutoSeg3D). Our core strategy involves utilizing object queries for temporal information propagation, where long-term instance association promotes the coherence of features and object identities, while short-term instance update enriches instant observations. Given that viewpoint variations in embodied robotics often lead to partial object visibility across frames, this mechanism aids the model in developing a holistic object understanding beyond incomplete instantaneous views. Furthermore, we introduce spatial consistency learning to mitigate the fragmentation problem inherent in VFMs, yielding more comprehensive instance information for enhancing the efficacy of both long-term and short-term temporal learning. The temporal information exchange and consistency learning facilitated by these sparse object queries not only enhance spatial comprehension but also circumvent the computational burden associated with dense temporal point cloud interactions. Our method establishes a new state-of-the-art, surpassing ESAM by 2.8 AP on ScanNet200 and delivering consistent gains on ScanNet, SceneNN, and 3RScan datasets.",
            "headline_zh": "提出AutoSeg3D方法，将在线3D分割重构为实例跟踪问题，以增强具身智能体的动态环境感知能力。",
            "intro_zh": [
                "核心问题：现有基于查询的3D分割方法忽视时间维度，难以处理动态环境中的部分可见对象。",
                "方法要点：利用对象查询进行时间信息传播，结合长期关联和短期更新，并引入空间一致性学习。",
                "实验或效果：在ScanNet200等数据集上超越ESAM，达到新SOTA，提升分割精度和一致性。"
            ],
            "tags_zh": [
                "在线3D分割",
                "实例跟踪",
                "时间信息传播",
                "空间一致性学习",
                "具身智能体",
                "点云处理"
            ],
            "_index": 74
        },
        {
            "title": "More than Segmentation: Benchmarking SAM 3 for Segmentation, 3D Perception, and Reconstruction in Robotic Surgery",
            "authors": [
                "Wenzhen Dong",
                "Jieming Yu",
                "Yiming Huang",
                "Hongqiu Wang",
                "Lei Zhu",
                "Albert C. S. Chung",
                "Hongliang Ren",
                "Long Bai"
            ],
            "arxiv_id": "2512.07596v1",
            "summary": "The recent Segment Anything Model (SAM) 3 has introduced significant advancements over its predecessor, SAM 2, particularly with the integration of language-based segmentation and enhanced 3D perception capabilities. SAM 3 supports zero-shot segmentation across a wide range of prompts, including point, bounding box, and language-based prompts, allowing for more flexible and intuitive interactions with the model. In this empirical evaluation, we assess the performance of SAM 3 in robot-assisted surgery, benchmarking its zero-shot segmentation with point and bounding box prompts and exploring its effectiveness in dynamic video tracking, alongside its newly introduced language prompt segmentation. While language prompts show potential, their performance in the surgical domain is currently suboptimal, highlighting the need for further domain-specific training. Additionally, we investigate SAM 3's 3D reconstruction abilities, demonstrating its capacity to process surgical scene data and reconstruct 3D anatomical structures from 2D images. Through comprehensive testing on the MICCAI EndoVis 2017 and EndoVis 2018 benchmarks, SAM 3 shows clear improvements over SAM and SAM 2 in both image and video segmentation under spatial prompts, while zero-shot evaluations on SCARED, StereoMIS, and EndoNeRF indicate strong monocular depth estimation and realistic 3D instrument reconstruction, yet also reveal remaining limitations in complex, highly dynamic surgical scenes.",
            "headline_zh": "评估SAM 3在机器人手术中的分割、3D感知与重建性能，揭示其在零样本分割和3D重建方面的优势与局限。",
            "intro_zh": [
                "核心问题：评估SAM 3在机器人手术场景下的零样本分割、动态视频跟踪和3D重建能力，以验证其实际应用潜力。",
                "方法要点：使用点、边界框和语言提示进行零样本分割测试，并基于2D图像进行3D解剖结构重建，结合多个手术数据集进行综合评估。",
                "实验或效果：在MICCAI EndoVis基准上，SAM 3在空间提示下的图像和视频分割优于前代模型；零样本评估显示其在单目深度估计和3D器械重建方面表现良好，但在复杂动态场景中仍有局限。"
            ],
            "tags_zh": [
                "零样本分割",
                "3D重建",
                "机器人手术",
                "语言提示",
                "动态视频跟踪",
                "单目深度估计"
            ],
            "_index": 75
        },
        {
            "title": "Robust Variational Model Based Tailored UNet: Leveraging Edge Detector and Mean Curvature for Improved Image Segmentation",
            "authors": [
                "Kaili Qi",
                "Zhongyi Huang",
                "Wenli Yang"
            ],
            "arxiv_id": "2512.07590v1",
            "summary": "To address the challenge of segmenting noisy images with blurred or fragmented boundaries, this paper presents a robust version of Variational Model Based Tailored UNet (VM_TUNet), a hybrid framework that integrates variational methods with deep learning. The proposed approach incorporates physical priors, an edge detector and a mean curvature term, into a modified Cahn-Hilliard equation, aiming to combine the interpretability and boundary-smoothing advantages of variational partial differential equations (PDEs) with the strong representational ability of deep neural networks. The architecture consists of two collaborative modules: an F module, which conducts efficient frequency domain preprocessing to alleviate poor local minima, and a T module, which ensures accurate and stable local computations, backed by a stability estimate. Extensive experiments on three benchmark datasets indicate that the proposed method achieves a balanced trade-off between performance and computational efficiency, which yields competitive quantitative results and improved visual quality compared to pure convolutional neural network (CNN) based models, while achieving performance close to that of transformer-based method with reasonable computational expense.",
            "headline_zh": "提出鲁棒变分模型定制UNet，结合边缘检测与平均曲率以改进噪声图像分割",
            "intro_zh": [
                "针对噪声图像边界模糊或断裂的分割挑战，提出混合框架VM_TUNet。",
                "集成变分方法与深度学习，引入物理先验、边缘检测和平均曲率项。",
                "在三个基准数据集上实验，性能与计算效率平衡，优于纯CNN模型。"
            ],
            "tags_zh": [
                "图像分割",
                "变分模型",
                "深度学习",
                "边缘检测",
                "平均曲率",
                "噪声图像处理"
            ],
            "_index": 76
        },
        {
            "title": "LongCat-Image Technical Report",
            "authors": [
                "Meituan LongCat Team",
                "Hanghang Ma",
                "Haoxian Tan",
                "Jiale Huang",
                "Junqiang Wu",
                "Jun-Yan He",
                "Lishuai Gao",
                "Songlin Xiao",
                "Xiaoming Wei",
                "Xiaoqi Ma",
                "Xunliang Cai",
                "Yayong Guan",
                "Jie Hu"
            ],
            "arxiv_id": "2512.07584v1",
            "summary": "We introduce LongCat-Image, a pioneering open-source and bilingual (Chinese-English) foundation model for image generation, designed to address core challenges in multilingual text rendering, photorealism, deployment efficiency, and developer accessibility prevalent in current leading models. 1) We achieve this through rigorous data curation strategies across the pre-training, mid-training, and SFT stages, complemented by the coordinated use of curated reward models during the RL phase. This strategy establishes the model as a new state-of-the-art (SOTA), delivering superior text-rendering capabilities and remarkable photorealism, and significantly enhancing aesthetic quality. 2) Notably, it sets a new industry standard for Chinese character rendering. By supporting even complex and rare characters, it outperforms both major open-source and commercial solutions in coverage, while also achieving superior accuracy. 3) The model achieves remarkable efficiency through its compact design. With a core diffusion model of only 6B parameters, it is significantly smaller than the nearly 20B or larger Mixture-of-Experts (MoE) architectures common in the field. This ensures minimal VRAM usage and rapid inference, significantly reducing deployment costs. Beyond generation, LongCat-Image also excels in image editing, achieving SOTA results on standard benchmarks with superior editing consistency compared to other open-source works. 4) To fully empower the community, we have established the most comprehensive open-source ecosystem to date. We are releasing not only multiple model versions for text-to-image and image editing, including checkpoints after mid-training and post-training stages, but also the entire toolchain of training procedure. We believe that the openness of LongCat-Image will provide robust support for developers and researchers, pushing the frontiers of visual content creation.",
            "headline_zh": "提出LongCat-Image开源双语图像生成基础模型，以解决多语言文本渲染、真实感、部署效率和开发者可访问性等核心挑战。",
            "intro_zh": [
                "通过预训练、中训练和SFT阶段的数据策展策略，结合RL阶段的奖励模型，实现卓越文本渲染和真实感，提升美学质量。",
                "在中文字符渲染上设定新行业标准，支持复杂罕见字符，覆盖率和准确性优于开源和商业方案。",
                "采用紧凑设计，核心扩散模型仅6B参数，确保低VRAM使用和快速推理，降低部署成本，并在图像编辑中实现SOTA结果。"
            ],
            "tags_zh": [
                "图像生成",
                "多语言文本渲染",
                "扩散模型",
                "开源生态系统",
                "图像编辑",
                "部署效率"
            ],
            "_index": 77
        },
        {
            "title": "Complementary Learning Approach for Text Classification using Large Language Models",
            "authors": [
                "Navid Asgari",
                "Benjamin M. Cole"
            ],
            "arxiv_id": "2512.07583v1",
            "summary": "In this study, we propose a structured methodology that utilizes large language models (LLMs) in a cost-efficient and parsimonious manner, integrating the strengths of scholars and machines while offsetting their respective weaknesses. Our methodology, facilitated through a chain of thought and few-shot learning prompting from computer science, extends best practices for co-author teams in qualitative research to human-machine teams in quantitative research. This allows humans to utilize abductive reasoning and natural language to interrogate not just what the machine has done but also what the human has done. Our method highlights how scholars can manage inherent weaknesses OF LLMs using careful, low-cost techniques. We demonstrate how to use the methodology to interrogate human-machine rating discrepancies for a sample of 1,934 press releases announcing pharmaceutical alliances (1990-2017).",
            "headline_zh": "提出互补学习方法，利用大语言模型进行成本高效文本分类，解决人机协作中的弱点管理问题。",
            "intro_zh": [
                "核心问题：如何在大语言模型应用中，以低成本方式管理其固有弱点，并整合人机优势进行文本分类。",
                "方法要点：采用思维链和少样本学习提示，将定性研究中的合作团队最佳实践扩展到人机团队，支持溯因推理和自然语言交互。",
                "实验或效果：应用于1934份制药联盟新闻稿（1990-2017），演示如何分析人机评分差异，验证方法的实用性。"
            ],
            "tags_zh": [
                "文本分类",
                "大语言模型",
                "互补学习",
                "人机协作",
                "少样本学习",
                "溯因推理"
            ],
            "_index": 78
        },
        {
            "title": "See Once, Then Act: Vision-Language-Action Model with Task Learning from One-Shot Video Demonstrations",
            "authors": [
                "Guangyan Chen",
                "Meiling Wang",
                "Qi Shao",
                "Zichen Zhou",
                "Weixin Mao",
                "Te Cui",
                "Minzhao Zhu",
                "Yinan Deng",
                "Luojie Yang",
                "Zhanqi Zhang",
                "Yi Yang",
                "Hua Chen",
                "Yufeng Yue"
            ],
            "arxiv_id": "2512.07582v1",
            "summary": "Developing robust and general-purpose manipulation policies represents a fundamental objective in robotics research. While Vision-Language-Action (VLA) models have demonstrated promising capabilities for end-to-end robot control, existing approaches still exhibit limited generalization to tasks beyond their training distributions. In contrast, humans possess remarkable proficiency in acquiring novel skills by simply observing others performing them once. Inspired by this capability, we propose ViVLA, a generalist robotic manipulation policy that achieves efficient task learning from a single expert demonstration video at test time. Our approach jointly processes an expert demonstration video alongside the robot's visual observations to predict both the demonstrated action sequences and subsequent robot actions, effectively distilling fine-grained manipulation knowledge from expert behavior and transferring it seamlessly to the agent. To enhance the performance of ViVLA, we develop a scalable expert-agent pair data generation pipeline capable of synthesizing paired trajectories from easily accessible human videos, further augmented by curated pairs from publicly available datasets. This pipeline produces a total of 892,911 expert-agent samples for training ViVLA. Experimental results demonstrate that our ViVLA is able to acquire novel manipulation skills from only a single expert demonstration video at test time. Our approach achieves over 30% improvement on unseen LIBERO tasks and maintains above 35% gains with cross-embodiment videos. Real-world experiments demonstrate effective learning from human videos, yielding more than 38% improvement on unseen tasks.",
            "headline_zh": "提出ViVLA模型，通过单次视频演示实现机器人任务学习，提升泛化能力。",
            "intro_zh": [
                "现有视觉-语言-动作模型泛化能力有限，难以适应新任务。",
                "ViVLA联合处理演示视频与机器人观测，预测动作序列以蒸馏知识。",
                "实验显示在未见任务上提升超30%，跨具身视频保持超35%增益。"
            ],
            "tags_zh": [
                "视觉-语言-动作模型",
                "单次学习",
                "机器人操作",
                "知识蒸馏",
                "泛化能力",
                "视频演示"
            ],
            "_index": 79
        },
        {
            "title": "All You Need Are Random Visual Tokens? Demystifying Token Pruning in VLLMs",
            "authors": [
                "Yahong Wang",
                "Juncheng Wu",
                "Zhangkai Ni",
                "Longzhen Yang",
                "Yihang Liu",
                "Chengmei Yang",
                "Ying Wen",
                "Xianfeng Tang",
                "Hui Liu",
                "Yuyin Zhou",
                "Lianghua He"
            ],
            "arxiv_id": "2512.07580v1",
            "summary": "Vision Large Language Models (VLLMs) incur high computational costs due to their reliance on hundreds of visual tokens to represent images. While token pruning offers a promising solution for accelerating inference, this paper, however, identifies a key observation: in deeper layers (e.g., beyond the 20th), existing training-free pruning methods perform no better than random pruning. We hypothesize that this degradation is caused by \"vanishing token information\", where visual tokens progressively lose their salience with increasing network depth. To validate this hypothesis, we quantify a token's information content by measuring the change in the model output probabilities upon its removal. Using this proposed metric, our analysis of the information of visual tokens across layers reveals three key findings: (1) As layers deepen, the information of visual tokens gradually becomes uniform and eventually vanishes at an intermediate layer, which we term as \"information horizon\", beyond which the visual tokens become redundant; (2) The position of this horizon is not static; it extends deeper for visually intensive tasks, such as Optical Character Recognition (OCR), compared to more general tasks like Visual Question Answering (VQA); (3) This horizon is also strongly correlated with model capacity, as stronger VLLMs (e.g., Qwen2.5-VL) employ deeper visual tokens than weaker models (e.g., LLaVA-1.5). Based on our findings, we show that simple random pruning in deep layers efficiently balances performance and efficiency. Moreover, integrating random pruning consistently enhances existing methods. Using DivPrune with random pruning achieves state-of-the-art results, maintaining 96.9% of Qwen-2.5-VL-7B performance while pruning 50% of visual tokens. The code will be publicly available at https://github.com/YahongWang1/Information-Horizon.",
            "headline_zh": "提出信息地平线概念，揭示视觉令牌在深层冗余，通过随机剪枝提升VLLM效率",
            "intro_zh": [
                "发现现有训练无关剪枝方法在深层表现不优于随机剪枝，归因于令牌信息消失",
                "提出信息度量分析视觉令牌信息变化，识别信息地平线，其位置随任务和模型能力变化",
                "实验表明深层随机剪枝有效平衡性能与效率，结合DivPrune在Qwen2.5-VL上剪枝50%保持96.9%性能"
            ],
            "tags_zh": [
                "视觉大语言模型",
                "令牌剪枝",
                "信息地平线",
                "随机剪枝",
                "计算效率",
                "视觉令牌冗余"
            ],
            "_index": 80
        },
        {
            "title": "$φ$-test: Global Feature Selection and Inference for Shapley Additive Explanations",
            "authors": [
                "Dongseok Kim",
                "Hyoungsun Choi",
                "Mohamed Jismy Aashik Rasool",
                "Gisung Oh"
            ],
            "arxiv_id": "2512.07578v1",
            "summary": "We propose $φ$-test, a global feature-selection and significance procedure for black-box predictors that combines Shapley attributions with selective inference. Given a trained model and an evaluation dataset, $φ$-test performs SHAP-guided screening and fits a linear surrogate on the screened features via a selection rule with a tractable selective-inference form. For each retained feature, it outputs a Shapley-based global score, a surrogate coefficient, and post-selection $p$-values and confidence intervals in a global feature-importance table. Experiments on real tabular regression tasks with tree-based and neural backbones suggest that $φ$-test can retain much of the predictive ability of the original model while using only a few features and producing feature sets that remain fairly stable across resamples and backbone classes. In these settings, $φ$-test acts as a practical global explanation layer linking Shapley-based importance summaries with classical statistical inference.",
            "headline_zh": "提出φ-test，结合Shapley归因与选择性推断，用于黑盒预测器的全局特征选择与显著性检验。",
            "intro_zh": [
                "核心问题：黑盒预测器中全局特征选择与显著性推断缺乏统计保证，难以稳定解释。",
                "方法要点：基于SHAP引导筛选特征，通过选择性推断拟合线性代理模型，输出p值和置信区间。",
                "实验或效果：在表格回归任务中，使用少量特征保持预测能力，特征集在重采样和模型间较稳定。"
            ],
            "tags_zh": [
                "Shapley归因",
                "选择性推断",
                "全局特征选择",
                "黑盒解释",
                "显著性检验",
                "代理模型"
            ],
            "_index": 81
        },
        {
            "title": "R2MF-Net: A Recurrent Residual Multi-Path Fusion Network for Robust Multi-directional Spine X-ray Segmentation",
            "authors": [
                "Xuecheng Li",
                "Weikuan Jia",
                "Komildzhon Sharipov",
                "Sharipov Hotam Beknazarovich",
                "Farzona S. Ataeva",
                "Qurbonaliev Alisher",
                "Yuanjie Zheng"
            ],
            "arxiv_id": "2512.07576v1",
            "summary": "Accurate segmentation of spinal structures in X-ray images is a prerequisite for quantitative scoliosis assessment, including Cobb angle measurement, vertebral translation estimation and curvature classification. In routine practice, clinicians acquire coronal, left-bending and right-bending radiographs to jointly evaluate deformity severity and spinal flexibility. However, the segmentation step remains heavily manual, time-consuming and non-reproducible, particularly in low-contrast images and in the presence of rib shadows or overlapping tissues. To address these limitations, this paper proposes R2MF-Net, a recurrent residual multi-path encoder--decoder network tailored for automatic segmentation of multi-directional spine X-ray images. The overall design consists of a coarse segmentation network and a fine segmentation network connected in cascade. Both stages adopt an improved Inception-style multi-branch feature extractor, while a recurrent residual jump connection (R2-Jump) module is inserted into skip paths to gradually align encoder and decoder semantics. A multi-scale cross-stage skip (MC-Skip) mechanism allows the fine network to reuse hierarchical representations from multiple decoder levels of the coarse network, thereby strengthening the stability of segmentation across imaging directions and contrast conditions. Furthermore, a lightweight spatial-channel squeeze-and-excitation block (SCSE-Lite) is employed at the bottleneck to emphasize spine-related activations and suppress irrelevant structures and background noise. We evaluate R2MF-Net on a clinical multi-view radiograph dataset comprising 228 sets of coronal, left-bending and right-bending spine X-ray images with expert annotations.",
            "headline_zh": "提出R2MF-Net以解决多方向脊柱X光图像分割中的低对比度和噪声干扰问题。",
            "intro_zh": [
                "核心问题：脊柱X光图像分割在低对比度、肋骨阴影和组织重叠下困难，手动分割耗时且不可重复。",
                "方法要点：采用级联网络，结合改进的Inception多分支特征提取器、R2-Jump模块和MC-Skip机制增强语义对齐和稳定性。",
                "实验或效果：在包含228组多视图X光图像的临床数据集上评估，未知具体性能指标。"
            ],
            "tags_zh": [
                "脊柱X光分割",
                "多方向图像处理",
                "级联网络",
                "语义对齐",
                "特征融合"
            ],
            "_index": 82
        },
        {
            "title": "Precise Liver Tumor Segmentation in CT Using a Hybrid Deep Learning-Radiomics Framework",
            "authors": [
                "Xuecheng Li",
                "Weikuan Jia",
                "Komildzhon Sharipov",
                "Alimov Ruslan",
                "Lutfuloev Mazbutdzhon",
                "Ismoilov Shuhratjon",
                "Yuanjie Zheng"
            ],
            "arxiv_id": "2512.07574v1",
            "summary": "Accurate three-dimensional delineation of liver tumors on contrast-enhanced CT is a prerequisite for treatment planning, navigation and response assessment, yet manual contouring is slow, observer-dependent and difficult to standardise across centres. Automatic segmentation is complicated by low lesion-parenchyma contrast, blurred or incomplete boundaries, heterogeneous enhancement patterns, and confounding structures such as vessels and adjacent organs. We propose a hybrid framework that couples an attention-enhanced cascaded U-Net with handcrafted radiomics and voxel-wise 3D CNN refinement for joint liver and liver-tumor segmentation. First, a 2.5D two-stage network with a densely connected encoder, sub-pixel convolution decoders and multi-scale attention gates produces initial liver and tumor probability maps from short stacks of axial slices. Inter-slice temporal consistency is then enforced by a simple three-slice refinement rule along the cranio-caudal direction, which restores thin and tiny lesions while suppressing isolated noise. Next, 728 radiomic descriptors spanning intensity, texture, shape, boundary and wavelet feature groups are extracted from candidate lesions and reduced to 20 stable, highly informative features via multi-strategy feature selection; a random forest classifier uses these features to reject false-positive regions. Finally, a compact 3D patch-based CNN derived from AlexNet operates in a narrow band around the tumor boundary to perform voxel-level relabelling and contour smoothing.",
            "headline_zh": "提出混合深度学习-放射组学框架以解决CT中肝脏肿瘤精确分割问题",
            "intro_zh": [
                "核心问题：CT图像中肝脏肿瘤自动分割因低对比度、边界模糊和结构干扰而复杂",
                "方法要点：结合注意力增强级联U-Net、放射组学特征筛选和3D CNN细化进行联合分割",
                "实验或效果：通过多阶段处理提升分割精度，减少假阳性，实现三维轮廓平滑"
            ],
            "tags_zh": [
                "肝脏肿瘤分割",
                "深度学习",
                "放射组学",
                "CT图像处理",
                "注意力机制",
                "3D CNN"
            ],
            "_index": 83
        },
        {
            "title": "Weighted Contrastive Learning for Anomaly-Aware Time-Series Forecasting",
            "authors": [
                "Joel Ekstrand",
                "Tor Mattsson",
                "Zahra Taghiyarrenani",
                "Slawomir Nowaczyk",
                "Jens Lundström",
                "Mikael Lindén"
            ],
            "arxiv_id": "2512.07569v1",
            "summary": "Reliable forecasting of multivariate time series under anomalous conditions is crucial in applications such as ATM cash logistics, where sudden demand shifts can disrupt operations. Modern deep forecasters achieve high accuracy on normal data but often fail when distribution shifts occur. We propose Weighted Contrastive Adaptation (WECA), a Weighted contrastive objective that aligns normal and anomaly-augmented representations, preserving anomaly-relevant information while maintaining consistency under benign variations. Evaluations on a nationwide ATM transaction dataset with domain-informed anomaly injection show that WECA improves SMAPE on anomaly-affected data by 6.1 percentage points compared to a normally trained baseline, with negligible degradation on normal data. These results demonstrate that WECA enhances forecasting reliability under anomalies without sacrificing performance during regular operations.",
            "headline_zh": "提出加权对比适应方法以增强ATM现金物流等场景中异常条件下的时间序列预测可靠性",
            "intro_zh": [
                "核心问题：现代深度预测模型在正常数据上准确，但在分布偏移或异常条件下表现不佳，影响如ATM交易等应用。",
                "方法要点：提出加权对比适应，通过加权对比目标对齐正常与异常增强表示，保留异常相关信息并保持良性变化下的一致性。",
                "实验或效果：在带领域知识异常注入的ATM数据集上评估，相比正常训练基线，异常数据SMAPE提升6.1个百分点，正常数据性能几乎无下降。"
            ],
            "tags_zh": [
                "时间序列预测",
                "异常检测",
                "对比学习",
                "分布偏移",
                "ATM现金物流",
                "加权适应"
            ],
            "_index": 84
        },
        {
            "title": "Dual-Stream Cross-Modal Representation Learning via Residual Semantic Decorrelation",
            "authors": [
                "Xuecheng Li",
                "Weikuan Jia",
                "Alisher Kurbonaliev",
                "Qurbonaliev Alisher",
                "Khudzhamkulov Rustam",
                "Ismoilov Shuhratjon",
                "Eshmatov Javhariddin",
                "Yuanjie Zheng"
            ],
            "arxiv_id": "2512.07568v1",
            "summary": "Cross-modal learning has become a fundamental paradigm for integrating heterogeneous information sources such as images, text, and structured attributes. However, multimodal representations often suffer from modality dominance, redundant information coupling, and spurious cross-modal correlations, leading to suboptimal generalization and limited interpretability. In particular, high-variance modalities tend to overshadow weaker but semantically important signals, while naïve fusion strategies entangle modality-shared and modality-specific factors in an uncontrolled manner. This makes it difficult to understand which modality actually drives a prediction and to maintain robustness when some modalities are noisy or missing. To address these challenges, we propose a Dual-Stream Residual Semantic Decorrelation Network (DSRSD-Net), a simple yet effective framework that disentangles modality-specific and modality-shared information through residual decomposition and explicit semantic decorrelation constraints. DSRSD-Net introduces: (1) a dual-stream representation learning module that separates intra-modal (private) and inter-modal (shared) latent factors via residual projection; (2) a residual semantic alignment head that maps shared factors from different modalities into a common space using a combination of contrastive and regression-style objectives; and (3) a decorrelation and orthogonality loss that regularizes the covariance structure of the shared space while enforcing orthogonality between shared and private streams, thereby suppressing cross-modal redundancy and preventing feature collapse. Experimental results on two large-scale educational benchmarks demonstrate that DSRSD-Net consistently improves next-step prediction and final outcome prediction over strong single-modality, early-fusion, late-fusion, and co-attention baselines.",
            "headline_zh": "提出双流残差语义去相关网络以解决跨模态学习中模态主导和冗余耦合问题",
            "intro_zh": [
                "核心问题：跨模态学习存在模态主导、冗余信息耦合和虚假相关性，影响泛化和可解释性",
                "方法要点：通过残差分解和语义去相关约束，分离模态特定与共享信息，并正则化共享空间",
                "实验或效果：在两个大规模教育基准上，优于单模态、早期融合、晚期融合和协同注意力基线"
            ],
            "tags_zh": [
                "跨模态学习",
                "残差分解",
                "语义去相关",
                "双流网络",
                "模态解耦",
                "教育预测"
            ],
            "_index": 85
        },
        {
            "title": "Toward More Reliable Artificial Intelligence: Reducing Hallucinations in Vision-Language Models",
            "authors": [
                "Kassoum Sanogo",
                "Renzo Ardiccioni"
            ],
            "arxiv_id": "2512.07564v1",
            "summary": "Vision-language models (VLMs) frequently generate hallucinated content plausible but incorrect claims about image content. We propose a training-free self-correction framework enabling VLMs to iteratively refine responses through uncertainty-guided visual re-attention. Our method combines multidimensional uncertainty quantification (token entropy, attention dispersion, semantic consistency, claim confidence) with attention-guided cropping of under-explored regions. Operating entirely with frozen, pretrained VLMs, our framework requires no gradient updates. We validate our approach on the POPE and MMHAL BENCH benchmarks using the Qwen2.5-VL-7B [23] architecture. Experimental results demonstrate that our method reduces hallucination rates by 9.8 percentage points compared to the baseline, while improving object existence accuracy by 4.7 points on adversarial splits. Furthermore, qualitative analysis confirms that uncertainty-guided re-attention successfully grounds corrections in visual evidence where standard decoding fails. We validate our approach on Qwen2.5-VL-7B [23], with plans to extend validation across diverse architectures in future versions. We release our code and methodology to facilitate future research in trustworthy multimodal systems.",
            "headline_zh": "提出无需训练的自校正框架以减少视觉语言模型中的幻觉内容",
            "intro_zh": [
                "核心问题：视觉语言模型常生成看似合理但错误的图像内容描述，即幻觉问题。",
                "方法要点：通过不确定性引导的视觉重注意机制，结合多维不确定性量化和注意力裁剪，迭代优化响应。",
                "实验或效果：在POPE和MMHAL BENCH基准上，幻觉率降低9.8个百分点，对抗性分割的对象存在准确率提升4.7点。"
            ],
            "tags_zh": [
                "视觉语言模型",
                "幻觉减少",
                "不确定性量化",
                "自校正框架",
                "无需训练",
                "注意力机制"
            ],
            "_index": 86
        },
        {
            "title": "ReLaX: Reasoning with Latent Exploration for Large Reasoning Models",
            "authors": [
                "Shimin Zhang",
                "Xianwei Chen",
                "Yufan Shen",
                "Ziyuan Ye",
                "Jibin Wu"
            ],
            "arxiv_id": "2512.07558v1",
            "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently demonstrated remarkable potential in enhancing the reasoning capability of Large Reasoning Models (LRMs). However, RLVR often leads to entropy collapse, resulting in premature policy convergence and performance saturation. While manipulating token-level entropy has proven effective for promoting policy exploration, we argue that the latent dynamics underlying token generation encode a far richer computational structure for steering policy optimization toward a more effective exploration-exploitation tradeoff. To enable tractable analysis and intervention of the latent dynamics of LRMs, we leverage Koopman operator theory to obtain a linearized representation of their hidden-state dynamics. This enables us to introduce Dynamic Spectral Dispersion (DSD), a new metric to quantify the heterogeneity of the model's latent dynamics, serving as a direct indicator of policy exploration. Building upon these foundations, we propose Reasoning with Latent eXploration (ReLaX), a paradigm that explicitly incorporates latent dynamics to regulate exploration and exploitation during policy optimization. Comprehensive experiments across a wide range of multimodal and text-only reasoning benchmarks show that ReLaX significantly mitigates premature convergence and consistently achieves state-of-the-art performance.",
            "headline_zh": "提出ReLaX范式，通过调控潜在动态以解决大型推理模型中的熵崩溃问题。",
            "intro_zh": [
                "核心问题：RLVR导致熵崩溃，引发策略早熟收敛和性能饱和。",
                "方法要点：利用Koopman算子理论线性化潜在动态，引入DSD指标量化探索，提出ReLaX调控探索与利用。",
                "实验或效果：在多模态和纯文本推理基准上显著缓解早熟收敛，实现SOTA性能。"
            ],
            "tags_zh": [
                "大型推理模型",
                "强化学习",
                "潜在动态分析",
                "探索与利用平衡",
                "Koopman算子理论"
            ],
            "_index": 87
        },
        {
            "title": "On Conditional Independence Graph Learning From Multi-Attribute Gaussian Dependent Time Series",
            "authors": [
                "Jitendra K. Tugnait"
            ],
            "arxiv_id": "2512.07557v1",
            "summary": "Estimation of the conditional independence graph (CIG) of high-dimensional multivariate Gaussian time series from multi-attribute data is considered. Existing methods for graph estimation for such data are based on single-attribute models where one associates a scalar time series with each node. In multi-attribute graphical models, each node represents a random vector or vector time series. In this paper we provide a unified theoretical analysis of multi-attribute graph learning for dependent time series using a penalized log-likelihood objective function formulated in the frequency domain using the discrete Fourier transform of the time-domain data. We consider both convex (sparse-group lasso) and non-convex (log-sum and SCAD group penalties) penalty/regularization functions. We establish sufficient conditions in a high-dimensional setting for consistency (convergence of the inverse power spectral density to true value in the Frobenius norm), local convexity when using non-convex penalties, and graph recovery. We do not impose any incoherence or irrepresentability condition for our convergence results. We also empirically investigate selection of the tuning parameters based on the Bayesian information criterion, and illustrate our approach using numerical examples utilizing both synthetic and real data.",
            "headline_zh": "提出基于频域惩罚似然的多属性高斯时间序列条件独立图学习方法",
            "intro_zh": [
                "研究多属性高斯时间序列的条件独立图估计问题，扩展单属性模型",
                "使用频域惩罚似然方法，结合凸与非凸惩罚函数，建立高维一致性理论",
                "通过合成和真实数据实验验证方法，并基于贝叶斯信息准则选择调参"
            ],
            "tags_zh": [
                "条件独立图学习",
                "多属性时间序列",
                "频域方法",
                "惩罚似然",
                "高维统计",
                "图恢复"
            ],
            "_index": 88
        },
        {
            "title": "MoCoRP: Modeling Consistent Relations between Persona and Response for Persona-based Dialogue",
            "authors": [
                "Kyungro Lee",
                "Dongha Choi",
                "Hyunju Lee"
            ],
            "arxiv_id": "2512.07544v1",
            "summary": "As dialogue systems become increasingly important across various domains, a key challenge in persona-based dialogue is generating engaging and context-specific interactions while ensuring the model acts with a coherent personality. However, existing persona-based dialogue datasets lack explicit relations between persona sentences and responses, which makes it difficult for models to effectively capture persona information. To address these issues, we propose MoCoRP (Modeling Consistent Relations between Persona and Response), a framework that incorporates explicit relations into language models. MoCoRP leverages an NLI expert to explicitly extract the NLI relations between persona sentences and responses, enabling the model to effectively incorporate appropriate persona information from the context into its responses. We applied this framework to pre-trained models like BART and further extended it to modern large language models (LLMs) through alignment tuning. Experimental results on the public datasets ConvAI2 and MPChat demonstrate that MoCoRP outperforms existing baselines, achieving superior persona consistency and engaging, context-aware dialogue generation. Furthermore, our model not only excels in quantitative metrics but also shows significant improvements in qualitative aspects. These results highlight the effectiveness of explicitly modeling persona-response relations in persona-based dialogue. The source codes of MoCoRP are available at https://github.com/DMCB-GIST/MoCoRP.",
            "headline_zh": "提出MoCoRP框架，通过显式建模人设与回复关系，提升基于人设对话的连贯性。",
            "intro_zh": [
                "核心问题：现有基于人设的对话数据集缺乏人设句子与回复间的显式关系，影响模型捕捉人设信息。",
                "方法要点：利用NLI专家提取人设与回复的NLI关系，并集成到语言模型中，支持BART和LLMs的扩展。",
                "实验或效果：在ConvAI2和MPChat数据集上优于基线，提升人设一致性和对话生成质量。"
            ],
            "tags_zh": [
                "基于人设对话",
                "NLI关系建模",
                "对话一致性",
                "语言模型对齐",
                "对话生成"
            ],
            "_index": 89
        },
        {
            "title": "RRAEDy: Adaptive Latent Linearization of Nonlinear Dynamical Systems",
            "authors": [
                "Jad Mounayer",
                "Sebastian Rodriguez",
                "Jerome Tomezyk",
                "Chady Ghnatios",
                "Francisco Chinesta"
            ],
            "arxiv_id": "2512.07542v1",
            "summary": "Most existing latent-space models for dynamical systems require fixing the latent dimension in advance, they rely on complex loss balancing to approximate linear dynamics, and they don't regularize the latent variables. We introduce RRAEDy, a model that removes these limitations by discovering the appropriate latent dimension, while enforcing both regularized and linearized dynamics in the latent space. Built upon Rank-Reduction Autoencoders (RRAEs), RRAEDy automatically rank and prune latent variables through their singular values while learning a latent Dynamic Mode Decomposition (DMD) operator that governs their temporal progression. This structure-free yet linearly constrained formulation enables the model to learn stable and low-dimensional dynamics without auxiliary losses or manual tuning. We provide theoretical analysis demonstrating the stability of the learned operator and showcase the generality of our model by proposing an extension that handles parametric ODEs. Experiments on canonical benchmarks, including the Van der Pol oscillator, Burgers' equation, 2D Navier-Stokes, and Rotating Gaussians, show that RRAEDy achieves accurate and robust predictions. Our code is open-source and available at https://github.com/JadM133/RRAEDy. We also provide a video summarizing the main results at https://youtu.be/ox70mSSMGrM.",
            "headline_zh": "提出RRAEDy模型，通过自适应潜在维度发现和线性化动态，解决非线性动力系统建模中的维度固定与正则化不足问题。",
            "intro_zh": [
                "现有潜在空间模型需预先固定维度，依赖复杂损失平衡，且缺乏潜在变量正则化。",
                "RRAEDy基于秩降自编码器，自动发现潜在维度并学习线性动态模式分解算子，无需辅助损失或手动调参。",
                "在Van der Pol振荡器、Burgers方程等基准测试中，模型实现准确稳健预测，并扩展处理参数常微分方程。"
            ],
            "tags_zh": [
                "非线性动力系统",
                "潜在空间建模",
                "动态模式分解",
                "秩降自编码器",
                "自适应维度选择",
                "参数常微分方程"
            ],
            "_index": 90
        },
        {
            "title": "High-Dimensional Change Point Detection using Graph Spanning Ratio",
            "authors": [
                "Youngwen Sun",
                "Katerina Papagiannouli",
                "Vladimir Spokoiny"
            ],
            "arxiv_id": "2512.07541v1",
            "summary": "Inspired by graph-based methodologies, we introduce a novel graph-spanning algorithm designed to identify changes in both offline and online data across low to high dimensions. This versatile approach is applicable to Euclidean and graph-structured data with unknown distributions, while maintaining control over error probabilities. Theoretically, we demonstrate that the algorithm achieves high detection power when the magnitude of the change surpasses the lower bound of the minimax separation rate, which scales on the order of $\\sqrt{nd}$. Our method outperforms other techniques in terms of accuracy for both Gaussian and non-Gaussian data. Notably, it maintains strong detection power even with small observation windows, making it particularly effective for online environments where timely and precise change detection is critical.",
            "headline_zh": "提出基于图生成比的高维变点检测算法，适用于离线和在线数据场景。",
            "intro_zh": [
                "核心问题：高维数据中未知分布的变点检测，需控制错误概率。",
                "方法要点：利用图生成比算法，适应欧几里得和图结构数据，理论证明检测能力强。",
                "实验或效果：在Gaussian和非Gaussian数据上准确度优于其他方法，小观测窗口下仍保持高检测力。"
            ],
            "tags_zh": [
                "高维变点检测",
                "图生成比算法",
                "在线检测",
                "未知分布数据",
                "错误概率控制",
                "检测力分析"
            ],
            "_index": 91
        },
        {
            "title": "Minimum Bayes Risk Decoding for Error Span Detection in Reference-Free Automatic Machine Translation Evaluation",
            "authors": [
                "Boxuan Lyu",
                "Haiyue Song",
                "Hidetaka Kamigaito",
                "Chenchen Ding",
                "Hideki Tanaka",
                "Masao Utiyama",
                "Kotaro Funakoshi",
                "Manabu Okumura"
            ],
            "arxiv_id": "2512.07540v1",
            "summary": "Error Span Detection (ESD) is a subtask of automatic machine translation evaluation that localizes error spans in translations and labels their severity. State-of-the-art generative ESD methods typically decode using Maximum a Posteriori (MAP), assuming that model-estimated probabilities are perfectly correlated with similarity to human annotation. However, we observed that annotations dissimilar to the human annotation could achieve a higher model likelihood than the human annotation. We address this issue by applying Minimum Bayes Risk (MBR) decoding to generative ESD models. Specifically, we employ sentence- and span-level similarity metrics as utility functions to select candidate hypotheses based on their approximate similarity to the human annotation. Extensive experimental results show that our MBR decoding outperforms the MAP baseline at the system, sentence, and span-levels. Furthermore, to mitigate the computational cost of MBR decoding, we demonstrate that applying MBR distillation enables a standard greedy model to match MBR decoding performance, effectively eliminating the inference-time latency bottleneck.",
            "headline_zh": "提出最小贝叶斯风险解码以解决生成式错误跨度检测中模型似然与人工标注不一致的问题",
            "intro_zh": [
                "核心问题：生成式错误跨度检测方法中，最大后验解码假设模型概率与人工标注相似度完美相关，但实际存在不一致。",
                "方法要点：应用最小贝叶斯风险解码，使用句子和跨度级相似度度量作为效用函数，选择更接近人工标注的候选假设。",
                "实验或效果：实验显示最小贝叶斯风险解码在系统、句子和跨度级别优于基线，并通过蒸馏消除推理延迟瓶颈。"
            ],
            "tags_zh": [
                "错误跨度检测",
                "最小贝叶斯风险解码",
                "机器翻译评估",
                "生成式模型",
                "蒸馏训练"
            ],
            "_index": 92
        },
        {
            "title": "FRWKV:Frequency-Domain Linear Attention for Long-Term Time Series Forecasting",
            "authors": [
                "Qingyuan Yang",
                "Shizhuo",
                "Dongyue Chen",
                "Da Teng",
                "Zehua Gan"
            ],
            "arxiv_id": "2512.07539v1",
            "summary": "Traditional Transformers face a major bottleneck in long-sequence time series forecasting due to their quadratic complexity $(\\mathcal{O}(T^2))$ and their limited ability to effectively exploit frequency-domain information. Inspired by RWKV's $\\mathcal{O}(T)$ linear attention and frequency-domain modeling, we propose FRWKV, a frequency-domain linear-attention framework that overcomes these limitations. Our model integrates linear attention mechanisms with frequency-domain analysis, achieving $\\mathcal{O}(T)$ computational complexity in the attention path while exploiting spectral information to enhance temporal feature representations for scalable long-sequence modeling. Across eight real-world datasets, FRWKV achieves a first-place average rank. Our ablation studies confirm the critical roles of both the linear attention and frequency-encoder components. This work demonstrates the powerful synergy between linear attention and frequency analysis, establishing a new paradigm for scalable time series modeling. Code is available at this repository: https://github.com/yangqingyuan-byte/FRWKV.",
            "headline_zh": "提出FRWKV，结合频域分析与线性注意力，解决长序列时间序列预测中的计算复杂度和频域信息利用问题。",
            "intro_zh": [
                "传统Transformer在长序列预测中面临二次计算复杂度和频域信息利用不足的瓶颈。",
                "FRWKV集成线性注意力与频域分析，实现线性计算复杂度并增强时间特征表示。",
                "在八个真实数据集上取得平均排名第一，消融研究验证了线性注意力和频域编码器的重要性。"
            ],
            "tags_zh": [
                "长序列时间序列预测",
                "线性注意力",
                "频域分析",
                "计算复杂度优化",
                "时间序列建模"
            ],
            "_index": 93
        },
        {
            "title": "VulnLLM-R: Specialized Reasoning LLM with Agent Scaffold for Vulnerability Detection",
            "authors": [
                "Yuzhou Nie",
                "Hongwei Li",
                "Chengquan Guo",
                "Ruizhe Jiang",
                "Zhun Wang",
                "Bo Li",
                "Dawn Song",
                "Wenbo Guo"
            ],
            "arxiv_id": "2512.07533v1",
            "summary": "We propose VulnLLM-R, the~\\emph{first specialized reasoning LLM} for vulnerability detection. Our key insight is that LLMs can reason about program states and analyze the potential vulnerabilities, rather than simple pattern matching. This can improve the model's generalizability and prevent learning shortcuts. However, SOTA reasoning LLMs are typically ultra-large, closed-source, or have limited performance in vulnerability detection. To address this, we propose a novel training recipe with specialized data selection, reasoning data generation, reasoning data filtering and correction, and testing-phase optimization. Using our proposed methodology, we train a reasoning model with seven billion parameters. Through extensive experiments on SOTA datasets across Python, C/C++, and Java, we show that VulnLLM-R has superior effectiveness and efficiency than SOTA static analysis tools and both open-source and commercial large reasoning models. We further conduct a detailed ablation study to validate the key designs in our training recipe. Finally, we construct an agent scaffold around our model and show that it outperforms CodeQL and AFL++ in real-world projects. Our agent further discovers a set of zero-day vulnerabilities in actively maintained repositories. This work represents a pioneering effort to enable real-world, project-level vulnerability detection using AI agents powered by specialized reasoning models. The code is available at~\\href{https://github.com/ucsb-mlsec/VulnLLM-R}{github}.",
            "headline_zh": "提出VulnLLM-R，首个专用于漏洞检测的推理大模型，结合代理框架提升实际项目检测能力。",
            "intro_zh": [
                "核心问题：现有推理大模型在漏洞检测中性能有限，或规模过大、闭源，难以泛化。",
                "方法要点：通过专门数据选择、生成、过滤与校正，训练70亿参数模型，实现程序状态推理而非简单模式匹配。",
                "实验或效果：在Python、C/C++、Java数据集上优于静态分析工具及开源/商业大模型，代理框架在真实项目中超越CodeQL和AFL++，发现零日漏洞。"
            ],
            "tags_zh": [
                "漏洞检测",
                "推理大模型",
                "程序状态分析",
                "代理框架",
                "零日漏洞",
                "静态分析"
            ],
            "_index": 94
        },
        {
            "title": "Model-Based Reinforcement Learning Under Confounding",
            "authors": [
                "Nishanth Venkatesh",
                "Andreas A. Malikopoulos"
            ],
            "arxiv_id": "2512.07528v1",
            "summary": "We investigate model-based reinforcement learning in contextual Markov decision processes (C-MDPs) in which the context is unobserved and induces confounding in the offline dataset. In such settings, conventional model-learning methods are fundamentally inconsistent, as the transition and reward mechanisms generated under a behavioral policy do not correspond to the interventional quantities required for evaluating a state-based policy. To address this issue, we adapt a proximal off-policy evaluation approach that identifies the confounded reward expectation using only observable state-action-reward trajectories under mild invertibility conditions on proxy variables. When combined with a behavior-averaged transition model, this construction yields a surrogate MDP whose Bellman operator is well defined and consistent for state-based policies, and which integrates seamlessly with the maximum causal entropy (MaxCausalEnt) model-learning framework. The proposed formulation enables principled model learning and planning in confounded environments where contextual information is unobserved, unavailable, or impractical to collect.",
            "headline_zh": "提出基于代理变量的近端离策略评估方法，以解决上下文未观测下的混淆模型强化学习问题。",
            "intro_zh": [
                "研究上下文未观测的C-MDPs中，离线数据存在混淆导致传统模型学习不一致。",
                "利用代理变量可逆性，识别混淆奖励期望，结合行为平均转移模型构建替代MDP。",
                "该方法与最大因果熵框架兼容，支持在混淆环境中进行原则性模型学习和规划。"
            ],
            "tags_zh": [
                "模型强化学习",
                "混淆环境",
                "离策略评估",
                "代理变量",
                "最大因果熵",
                "上下文MDPs"
            ],
            "_index": 95
        },
        {
            "title": "From Orbit to Ground: Generative City Photogrammetry from Extreme Off-Nadir Satellite Images",
            "authors": [
                "Fei Yu",
                "Yu Liu",
                "Luyang Tang",
                "Mingchao Sun",
                "Zengye Ge",
                "Rui Bu",
                "Yuchao Jin",
                "Haisen Zhao",
                "He Sun",
                "Yangyan Li",
                "Mu Xu",
                "Wenzheng Chen",
                "Baoquan Chen"
            ],
            "arxiv_id": "2512.07527v1",
            "summary": "City-scale 3D reconstruction from satellite imagery presents the challenge of extreme viewpoint extrapolation, where our goal is to synthesize ground-level novel views from sparse orbital images with minimal parallax. This requires inferring nearly $90^\\circ$ viewpoint gaps from image sources with severely foreshortened facades and flawed textures, causing state-of-the-art reconstruction engines such as NeRF and 3DGS to fail.\n  To address this problem, we propose two design choices tailored for city structures and satellite inputs. First, we model city geometry as a 2.5D height map, implemented as a Z-monotonic signed distance field (SDF) that matches urban building layouts from top-down viewpoints. This stabilizes geometry optimization under sparse, off-nadir satellite views and yields a watertight mesh with crisp roofs and clean, vertically extruded facades. Second, we paint the mesh appearance from satellite images via differentiable rendering techniques. While the satellite inputs may contain long-range, blurry captures, we further train a generative texture restoration network to enhance the appearance, recovering high-frequency, plausible texture details from degraded inputs.\n  Our method's scalability and robustness are demonstrated through extensive experiments on large-scale urban reconstruction. For example, in our teaser figure, we reconstruct a $4\\,\\mathrm{km}^2$ real-world region from only a few satellite images, achieving state-of-the-art performance in synthesizing photorealistic ground views. The resulting models are not only visually compelling but also serve as high-fidelity, application-ready assets for downstream tasks like urban planning and simulation.",
            "headline_zh": "提出基于2.5D高度图和生成纹理恢复的方法，以解决从极端离天底卫星图像合成地面级城市视图的挑战。",
            "intro_zh": [
                "核心问题：从稀疏、视角极端的卫星图像进行城市尺度3D重建，需推断近90度视角差距，导致现有方法如NeRF和3DGS失效。",
                "方法要点：建模城市几何为2.5D高度图，使用Z单调符号距离场稳定优化；通过可微分渲染和生成网络恢复高频率纹理细节。",
                "实验或效果：在大规模城市重建实验中，从少量卫星图像重建4平方公里区域，合成逼真地面视图，性能达到先进水平。"
            ],
            "tags_zh": [
                "城市3D重建",
                "卫星图像处理",
                "生成纹理恢复",
                "2.5D高度图",
                "可微分渲染"
            ],
            "_index": 96
        },
        {
            "title": "LIME: Making LLM Data More Efficient with Linguistic Metadata Embeddings",
            "authors": [
                "Sebastian Sztwiertnia",
                "Felix Friedrich",
                "Kristian Kersting",
                "Patrick Schramowski",
                "Björn Deiseroth"
            ],
            "arxiv_id": "2512.07522v1",
            "summary": "Pre-training decoder-only language models relies on vast amounts of high-quality data, yet the availability of such data is increasingly reaching its limits. While metadata is commonly used to create and curate these datasets, its potential as a direct training signal remains under-explored. We challenge this status quo and propose LIME (Linguistic Metadata Embeddings), a method that enriches token embeddings with metadata capturing syntax, semantics, and contextual properties. LIME substantially improves pre-training efficiency. Specifically, it adapts up to 56% faster to the training data distribution, while introducing only 0.01% additional parameters at negligible compute overhead. Beyond efficiency, LIME improves tokenization, leading to remarkably stronger language modeling capabilities and generative task performance. These benefits persist across model scales (500M to 2B). In addition, we develop a variant with shifted metadata, LIME+1, that can guide token generation. Given prior metadata for the next token, LIME+1 improves reasoning performance by up to 38% and arithmetic accuracy by up to 35%.",
            "headline_zh": "提出LIME方法，通过语言元数据嵌入提升LLM预训练效率与性能",
            "intro_zh": [
                "核心问题：预训练语言模型依赖大量高质量数据，但数据可用性受限，元数据作为训练信号未充分利用。",
                "方法要点：LIME将语法、语义和上下文属性的元数据嵌入到词元嵌入中，仅增加0.01%参数，计算开销可忽略。",
                "实验效果：LIME使模型适应训练数据分布的速度提升高达56%，并增强语言建模和生成任务性能，LIME+1变体可提升推理和算术准确性达38%和35%。"
            ],
            "tags_zh": [
                "语言模型预训练",
                "元数据嵌入",
                "训练效率",
                "词元化改进",
                "生成任务性能",
                "推理增强"
            ],
            "_index": 97
        },
        {
            "title": "Machine Learning: Progress and Prospects",
            "authors": [
                "Alexander Gammerman"
            ],
            "arxiv_id": "2512.07519v1",
            "summary": "This Inaugural Lecture was given at Royal Holloway University of London in 1996. It covers an introduction to machine learning and describes various theoretical advances and practical projects in the field. The Lecture here is presented in its original format, but a few remarks have been added in 2025 to reflect recent developments, and the list of references has been updated to enhance the convenience and accuracy for readers.\n  When did machine learning start? Maybe a good starting point is 1949, when Claude Shannon proposed a learning algorithm for chess-playing programs. Or maybe we should go back to the 1930s when Ronald Fisher developed discriminant analysis - a type of learning where the problem is to construct a decision rule that separates two types of vectors. Or could it be the 18th century when David Hume discussed the idea of induction? Or the 14th century, when William of Ockham formulated the principle of \"simplicity\" known as \"Ockham's razor\" (Ockham, by the way, is a small village not far from Royal Holloway). Or it may be that, like almost everything else in Western civilisation and culture, the origin of these ideas lies in the Mediterranean. After all, it was Aristotle who said that \"we learn some things only by doing things\".\n  The field of machine learning has been greatly influenced by other disciplines and the subject is in itself not a very homogeneous discipline, but includes separate, overlapping subfields. There are many parallel lines of research in ML: inductive learning, neural networks, clustering, and theories of learning. They are all part of the more general field of machine learning.",
            "headline_zh": "介绍机器学习的历史、理论与应用，涵盖从早期思想到1996年的进展及2025年更新。",
            "intro_zh": [
                "核心问题：探讨机器学习的起源与发展，包括从哲学到算法的历史脉络。",
                "方法要点：概述机器学习的主要子领域，如归纳学习、神经网络和聚类。",
                "实验或效果：基于1996年讲座内容，结合2025年更新，提供理论进展和实际项目的综述。"
            ],
            "tags_zh": [
                "机器学习历史",
                "归纳学习",
                "神经网络",
                "聚类分析",
                "理论进展",
                "应用项目"
            ],
            "_index": 98
        },
        {
            "title": "SPAD: Seven-Source Token Probability Attribution with Syntactic Aggregation for Detecting Hallucinations in RAG",
            "authors": [
                "Pengqian Lu",
                "Jie Lu",
                "Anjin Liu",
                "Guangquan Zhang"
            ],
            "arxiv_id": "2512.07515v1",
            "summary": "Detecting hallucinations in Retrieval-Augmented Generation (RAG) remains a challenge. Prior approaches attribute hallucinations to a binary conflict between internal knowledge (stored in FFNs) and retrieved context. However, this perspective is incomplete, failing to account for the impact of other components in the generative process, such as the user query, previously generated tokens, the current token itself, and the final LayerNorm adjustment. To address this, we introduce SPAD. First, we mathematically attribute each token's probability into seven distinct sources: Query, RAG, Past, Current Token, FFN, Final LayerNorm, and Initial Embedding. This attribution quantifies how each source contributes to the generation of the current token. Then, we aggregate these scores by POS tags to quantify how different components drive specific linguistic categories. By identifying anomalies, such as Nouns relying on Final LayerNorm, SPAD effectively detects hallucinations. Extensive experiments demonstrate that SPAD achieves state-of-the-art performance",
            "headline_zh": "提出SPAD方法，通过七源概率归因与句法聚合检测RAG中的幻觉",
            "intro_zh": [
                "核心问题：现有方法将幻觉归因于内部知识与检索上下文间的二元冲突，忽略其他生成组件的影响。",
                "方法要点：将每个令牌的概率归因到七个来源，并通过词性聚合分数以识别异常贡献。",
                "实验或效果：广泛实验显示SPAD在检测幻觉方面达到最先进性能。"
            ],
            "tags_zh": [
                "检索增强生成",
                "幻觉检测",
                "概率归因",
                "句法聚合",
                "令牌生成分析"
            ],
            "_index": 99
        },
        {
            "title": "MeshRipple: Structured Autoregressive Generation of Artist-Meshes",
            "authors": [
                "Junkai Lin",
                "Hang Long",
                "Huipeng Guo",
                "Jielei Zhang",
                "JiaYi Yang",
                "Tianle Guo",
                "Yang Yang",
                "Jianwen Li",
                "Wenxiao Zhang",
                "Matthias Nießner",
                "Wei Yang"
            ],
            "arxiv_id": "2512.07514v1",
            "summary": "Meshes serve as a primary representation for 3D assets. Autoregressive mesh generators serialize faces into sequences and train on truncated segments with sliding-window inference to cope with memory limits. However, this mismatch breaks long-range geometric dependencies, producing holes and fragmented components. To address this critical limitation, we introduce MeshRipple, which expands a mesh outward from an active generation frontier, akin to a ripple on a surface.MeshRipple rests on three key innovations: a frontier-aware BFS tokenization that aligns the generation order with surface topology; an expansive prediction strategy that maintains coherent, connected surface growth; and a sparse-attention global memory that provides an effectively unbounded receptive field to resolve long-range topological dependencies.This integrated design enables MeshRipple to generate meshes with high surface fidelity and topological completeness, outperforming strong recent baselines.",
            "headline_zh": "提出MeshRipple以解决自回归网格生成中长程几何依赖断裂问题",
            "intro_zh": [
                "核心问题：自回归网格生成因序列化与滑动窗口推理导致长程几何依赖断裂，产生空洞和碎片化组件。",
                "方法要点：采用前沿感知BFS标记化、扩展预测策略和稀疏注意力全局内存，实现连贯表面增长和长程拓扑依赖解析。",
                "实验或效果：MeshRipple在表面保真度和拓扑完整性上优于近期基线，生成高质量网格。"
            ],
            "tags_zh": [
                "自回归网格生成",
                "长程几何依赖",
                "表面拓扑",
                "稀疏注意力",
                "网格生成模型"
            ],
            "_index": 100
        },
        {
            "title": "Exploring possible vector systems for faster training of neural networks with preconfigured latent spaces",
            "authors": [
                "Nikita Gabdullin"
            ],
            "arxiv_id": "2512.07509v1",
            "summary": "The overall neural network (NN) performance is closely related to the properties of its embedding distribution in latent space (LS). It has recently been shown that predefined vector systems, specifically An root system vectors, can be used as targets for latent space configurations (LSC) to ensure the desired LS structure. One of the main LSC advantage is the possibility of training classifier NNs without classification layers, which facilitates training NNs on datasets with extremely large numbers of classes. This paper provides a more general overview of possible vector systems for NN training along with their properties and methods for vector system construction. These systems are used to configure LS of encoders and visual transformers to significantly speed up ImageNet-1K and 50k-600k classes LSC training. It is also shown that using the minimum number of LS dimensions for a specific number of classes results in faster convergence. The latter has potential advantages for reducing the size of vector databases used to store NN embeddings.",
            "headline_zh": "探索预定义向量系统以加速具有预配置潜在空间的神经网络训练",
            "intro_zh": [
                "研究预定义向量系统（如An根系向量）用于配置潜在空间结构，以优化神经网络嵌入分布。",
                "利用向量系统训练分类器网络无需分类层，适用于超多类别数据集，加速ImageNet-1K和50k-600k类别训练。",
                "实验表明最小化潜在空间维度可加速收敛，并可能减少嵌入存储的向量数据库大小。"
            ],
            "tags_zh": [
                "潜在空间配置",
                "向量系统",
                "神经网络训练加速",
                "超多类别分类",
                "嵌入优化",
                "收敛加速"
            ],
            "_index": 101
        },
        {
            "title": "VP-AutoTest: A Virtual-Physical Fusion Autonomous Driving Testing Platform",
            "authors": [
                "Yiming Cui",
                "Shiyu Fang",
                "Jiarui Zhang",
                "Yan Huang",
                "Chengkai Xu",
                "Bing Zhu",
                "Hao Zhang",
                "Peng Hang",
                "Jian Sun"
            ],
            "arxiv_id": "2512.07507v1",
            "summary": "The rapid development of autonomous vehicles has led to a surge in testing demand. Traditional testing methods, such as virtual simulation, closed-course, and public road testing, face several challenges, including unrealistic vehicle states, limited testing capabilities, and high costs. These issues have prompted increasing interest in virtual-physical fusion testing. However, despite its potential, virtual-physical fusion testing still faces challenges, such as limited element types, narrow testing scope, and fixed evaluation metrics. To address these challenges, we propose the Virtual-Physical Testing Platform for Autonomous Vehicles (VP-AutoTest), which integrates over ten types of virtual and physical elements, including vehicles, pedestrians, and roadside infrastructure, to replicate the diversity of real-world traffic participants. The platform also supports both single-vehicle interaction and multi-vehicle cooperation testing, employing adversarial testing and parallel deduction to accelerate fault detection and explore algorithmic limits, while OBU and Redis communication enable seamless vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) cooperation across all levels of cooperative automation. Furthermore, VP-AutoTest incorporates a multidimensional evaluation framework and AI-driven expert systems to conduct comprehensive performance assessment and defect diagnosis. Finally, by comparing virtual-physical fusion test results with real-world experiments, the platform performs credibility self-evaluation to ensure both the fidelity and efficiency of autonomous driving testing. Please refer to the website for the full testing functionalities on the autonomous driving public service platform OnSite:https://www.onsite.com.cn.",
            "headline_zh": "提出VP-AutoTest虚拟物理融合平台以解决自动驾驶测试中元素类型有限、范围窄和评估固定等问题。",
            "intro_zh": [
                "核心问题：传统自动驾驶测试方法存在车辆状态不真实、测试能力有限和高成本等挑战。",
                "方法要点：集成十余种虚拟物理元素，支持单/多车交互测试，采用对抗测试和平行推演加速故障检测。",
                "实验或效果：通过多维评估框架和AI专家系统进行性能评估，并与真实实验对比进行可信度自评。"
            ],
            "tags_zh": [
                "虚拟物理融合测试",
                "自动驾驶平台",
                "多维评估",
                "对抗测试",
                "V2V/V2I通信",
                "可信度自评"
            ],
            "_index": 102
        },
        {
            "title": "ControlVP: Interactive Geometric Refinement of AI-Generated Images with Consistent Vanishing Points",
            "authors": [
                "Ryota Okumura",
                "Kaede Shiohara",
                "Toshihiko Yamasaki"
            ],
            "arxiv_id": "2512.07504v1",
            "summary": "Recent text-to-image models, such as Stable Diffusion, have achieved impressive visual quality, yet they often suffer from geometric inconsistencies that undermine the structural realism of generated scenes. One prominent issue is vanishing point inconsistency, where projections of parallel lines fail to converge correctly in 2D space. This leads to structurally implausible geometry that degrades spatial realism, especially in architectural scenes. We propose ControlVP, a user-guided framework for correcting vanishing point inconsistencies in generated images. Our approach extends a pre-trained diffusion model by incorporating structural guidance derived from building contours. We also introduce geometric constraints that explicitly encourage alignment between image edges and perspective cues. Our method enhances global geometric consistency while maintaining visual fidelity comparable to the baselines. This capability is particularly valuable for applications that require accurate spatial structure, such as image-to-3D reconstruction. The dataset and source code are available at https://github.com/RyotaOkumura/ControlVP .",
            "headline_zh": "提出ControlVP框架，通过用户引导修正AI生成图像中的灭点不一致问题",
            "intro_zh": [
                "核心问题：文本到图像模型常产生几何不一致，如灭点不一致，影响场景结构真实感",
                "方法要点：扩展预训练扩散模型，结合建筑轮廓结构指导和几何约束，增强全局几何一致性",
                "实验或效果：在保持视觉保真度下提升几何一致性，适用于图像到3D重建等应用"
            ],
            "tags_zh": [
                "灭点校正",
                "几何一致性",
                "扩散模型",
                "用户引导框架",
                "图像生成"
            ],
            "_index": 103
        },
        {
            "title": "SJD++: Improved Speculative Jacobi Decoding for Training-free Acceleration of Discrete Auto-regressive Text-to-Image Generation",
            "authors": [
                "Yao Teng",
                "Zhihuan Jiang",
                "Han Shi",
                "Xian Liu",
                "Xuefei Ning",
                "Guohao Dai",
                "Yu Wang",
                "Zhenguo Li",
                "Xihui Liu"
            ],
            "arxiv_id": "2512.07503v1",
            "summary": "Large autoregressive models can generate high-quality, high-resolution images but suffer from slow generation speed, because these models require hundreds to thousands of sequential forward passes for next-token prediction during inference. To accelerate autoregressive text-to-image generation, we propose Speculative Jacobi Decoding++ (SJD++), a training-free probabilistic parallel decoding algorithm. Unlike traditional next-token prediction, SJD++ performs multi-token prediction in each forward pass, drastically reducing generation steps. Specifically, it integrates the iterative multi-token prediction mechanism from Jacobi decoding, with the probabilistic drafting-and-verification mechanism from speculative sampling. More importantly, for further acceleration, SJD++ reuses high-confidence draft tokens after each verification phase instead of resampling them all. We conduct extensive experiments on several representative autoregressive text-to-image generation models and demonstrate that SJD++ achieves $2\\times$ to $3\\times$ inference latency reduction and $2\\times$ to $7\\times$ step compression, while preserving visual quality with no observable degradation.",
            "headline_zh": "提出SJD++以加速自回归文本到图像生成，无需训练即可减少推理延迟和步骤。",
            "intro_zh": [
                "自回归模型生成图像慢，需大量顺序前向传递预测下一个标记。",
                "SJD++结合Jacobi解码的多标记预测和推测采样的草稿验证机制，实现并行解码。",
                "实验显示SJD++在多个模型上实现2-3倍延迟降低和2-7倍步骤压缩，视觉质量无下降。"
            ],
            "tags_zh": [
                "自回归模型",
                "文本到图像生成",
                "并行解码",
                "推测采样",
                "Jacobi解码",
                "推理加速"
            ],
            "_index": 104
        },
        {
            "title": "AutoICE: Automatically Synthesizing Verifiable C Code via LLM-driven Evolution",
            "authors": [
                "Weilin Luo",
                "Xueyi Liang",
                "Haotian Deng",
                "Yanan Liu",
                "Hai Wan"
            ],
            "arxiv_id": "2512.07501v1",
            "summary": "Automatically synthesizing verifiable code from natural language requirements ensures software correctness and reliability while significantly lowering the barrier to adopting the techniques of formal methods. With the rise of large language models (LLMs), long-standing efforts at autoformalization have gained new momentum. However, existing approaches suffer from severe syntactic and semantic errors due to the scarcity of domain-specific pre-training corpora and often fail to formalize implicit knowledge effectively. In this paper, we propose AutoICE, an LLM-driven evolutionary search for synthesizing verifiable C code. It introduces the diverse individual initialization and the collaborative crossover to enable diverse iterative updates, thereby mitigating error propagation inherent in single-agent iterations. Besides, it employs the self-reflective mutation to facilitate the discovery of implicit knowledge. Evaluation results demonstrate the effectiveness of AutoICE: it successfully verifies $90.36$\\% of code, outperforming the state-of-the-art (SOTA) approach. Besides, on a developer-friendly dataset variant, AutoICE achieves a $88.33$\\% verification success rate, significantly surpassing the $65$\\% success rate of the SOTA approach.",
            "headline_zh": "提出AutoICE，通过LLM驱动的进化搜索合成可验证C代码，以解决自动形式化中的错误传播问题。",
            "intro_zh": [
                "核心问题：现有方法因领域语料稀缺和隐式知识形式化困难，导致语法和语义错误频发。",
                "方法要点：采用多样个体初始化和协作交叉，结合自反思变异，以进化搜索减少单代理迭代的错误传播。",
                "实验或效果：在验证成功率上达到90.36%，超越现有最佳方法，并在开发者友好数据集上显著提升至88.33%。"
            ],
            "tags_zh": [
                "代码合成",
                "形式化验证",
                "大语言模型",
                "进化算法",
                "C语言编程"
            ],
            "_index": 105
        },
        {
            "title": "MultiMotion: Multi Subject Video Motion Transfer via Video Diffusion Transformer",
            "authors": [
                "Penghui Liu",
                "Jiangshan Wang",
                "Yutong Shen",
                "Shanhui Mo",
                "Chenyang Qi",
                "Yue Ma"
            ],
            "arxiv_id": "2512.07500v1",
            "summary": "Multi-object video motion transfer poses significant challenges for Diffusion Transformer (DiT) architectures due to inherent motion entanglement and lack of object-level control. We present MultiMotion, a novel unified framework that overcomes these limitations. Our core innovation is Maskaware Attention Motion Flow (AMF), which utilizes SAM2 masks to explicitly disentangle and control motion features for multiple objects within the DiT pipeline. Furthermore, we introduce RectPC, a high-order predictor-corrector solver for efficient and accurate sampling, particularly beneficial for multi-entity generation. To facilitate rigorous evaluation, we construct the first benchmark dataset specifically for DiT-based multi-object motion transfer. MultiMotion demonstrably achieves precise, semantically aligned, and temporally coherent motion transfer for multiple distinct objects, maintaining DiT's high quality and scalability. The code is in the supp.",
            "headline_zh": "提出MultiMotion框架，通过Maskaware Attention Motion Flow解决多对象视频运动转移中的运动纠缠问题。",
            "intro_zh": [
                "核心问题：多对象视频运动转移在Diffusion Transformer中面临运动纠缠和对象级控制缺失的挑战。",
                "方法要点：引入Maskaware Attention Motion Flow，利用SAM2掩码在DiT流程中显式解缠和控制多对象运动特征。",
                "实验或效果：构建首个基于DiT的多对象运动转移基准数据集，实现精确、语义对齐且时间一致的运动转移。"
            ],
            "tags_zh": [
                "视频运动转移",
                "扩散变换器",
                "多对象控制",
                "运动解缠",
                "基准数据集"
            ],
            "_index": 106
        },
        {
            "title": "Towards Robust DeepFake Detection under Unstable Face Sequences: Adaptive Sparse Graph Embedding with Order-Free Representation and Explicit Laplacian Spectral Prior",
            "authors": [
                "Chih-Chung Hsu",
                "Shao-Ning Chen",
                "Chia-Ming Lee",
                "Yi-Fang Wang",
                "Yi-Shiuan Chou"
            ],
            "arxiv_id": "2512.07498v1",
            "summary": "Ensuring the authenticity of video content remains challenging as DeepFake generation becomes increasingly realistic and robust against detection. Most existing detectors implicitly assume temporally consistent and clean facial sequences, an assumption that rarely holds in real-world scenarios where compression artifacts, occlusions, and adversarial attacks destabilize face detection and often lead to invalid or misdetected faces. To address these challenges, we propose a Laplacian-Regularized Graph Convolutional Network (LR-GCN) that robustly detects DeepFakes from noisy or unordered face sequences, while being trained only on clean facial data. Our method constructs an Order-Free Temporal Graph Embedding (OF-TGE) that organizes frame-wise CNN features into an adaptive sparse graph based on semantic affinities. Unlike traditional methods constrained by strict temporal continuity, OF-TGE captures intrinsic feature consistency across frames, making it resilient to shuffled, missing, or heavily corrupted inputs. We further impose a dual-level sparsity mechanism on both graph structure and node features to suppress the influence of invalid faces. Crucially, we introduce an explicit Graph Laplacian Spectral Prior that acts as a high-pass operator in the graph spectral domain, highlighting structural anomalies and forgery artifacts, which are then consolidated by a low-pass GCN aggregation. This sequential design effectively realizes a task-driven spectral band-pass mechanism that suppresses background information and random noise while preserving manipulation cues. Extensive experiments on FF++, Celeb-DFv2, and DFDC demonstrate that LR-GCN achieves state-of-the-art performance and significantly improved robustness under severe global and local disruptions, including missing faces, occlusions, and adversarially perturbed face detections.",
            "headline_zh": "提出拉普拉斯正则化图卷积网络以解决不稳定人脸序列下的DeepFake检测问题",
            "intro_zh": [
                "核心问题：现有检测器依赖时序一致且干净的人脸序列，但实际场景中压缩、遮挡和对抗攻击导致人脸检测不稳定",
                "方法要点：构建无序时序图嵌入，基于语义亲和度自适应稀疏图，并引入显式图拉普拉斯谱先验作为高通滤波器",
                "实验或效果：在FF++、Celeb-DFv2和DFDC数据集上实现SOTA性能，显著提升在缺失、遮挡和对抗扰动下的鲁棒性"
            ],
            "tags_zh": [
                "DeepFake检测",
                "图卷积网络",
                "无序时序表示",
                "拉普拉斯谱先验",
                "鲁棒性增强"
            ],
            "_index": 107
        },
        {
            "title": "How Do LLMs Fail In Agentic Scenarios? A Qualitative Analysis of Success and Failure Scenarios of Various LLMs in Agentic Simulations",
            "authors": [
                "JV Roig"
            ],
            "arxiv_id": "2512.07497v1",
            "summary": "We investigate how large language models (LLMs) fail when operating as autonomous agents with tool-use capabilities. Using the Kamiwaza Agentic Merit Index (KAMI) v0.1 benchmark, we analyze 900 execution traces from three representative models - Granite 4 Small, Llama 4 Maverick, and DeepSeek V3.1 - across filesystem, text extraction, CSV analysis, and SQL scenarios. Rather than focusing on aggregate scores, we perform fine-grained, per-trial behavioral analysis to surface the strategies that enable successful multi-step tool execution and the recurrent failure modes that undermine reliability. Our findings show that model scale alone does not predict agentic robustness: Llama 4 Maverick (400B) performs only marginally better than Granite 4 Small (32B) in some uncertainty-driven tasks, while DeepSeek V3.1's superior reliability derives primarily from post-training reinforcement learning rather than architecture or size. Across models, we identify four recurring failure archetypes: premature action without grounding, over-helpfulness that substitutes missing entities, vulnerability to distractor-induced context pollution, and fragile execution under load. These patterns highlight the need for agentic evaluation methods that emphasize interactive grounding, recovery behavior, and environment-aware adaptation, suggesting that reliable enterprise deployment requires not just stronger models but deliberate training and design choices that reinforce verification, constraint discovery, and adherence to source-of-truth data.",
            "headline_zh": "分析大语言模型在代理场景中的失败模式，揭示策略与可靠性因素",
            "intro_zh": [
                "研究大语言模型作为自主代理使用工具时的失败机制",
                "通过KAMI基准对三款模型进行细粒度行为分析，识别成功策略与失败模式",
                "发现模型规模非唯一决定因素，强化学习提升可靠性，并归纳四种常见失败类型"
            ],
            "tags_zh": [
                "大语言模型代理",
                "工具使用失败分析",
                "KAMI基准",
                "细粒度行为分析",
                "强化学习可靠性",
                "失败模式分类"
            ],
            "_index": 108
        },
        {
            "title": "Efficient Low-Tubal-Rank Tensor Estimation via Alternating Preconditioned Gradient Descent",
            "authors": [
                "Zhiyu Liu",
                "Zhi Han",
                "Yandong Tang",
                "Jun Fan",
                "Yao Wang"
            ],
            "arxiv_id": "2512.07490v1",
            "summary": "The problem of low-tubal-rank tensor estimation is a fundamental task with wide applications across high-dimensional signal processing, machine learning, and image science. Traditional approaches tackle such a problem by performing tensor singular value decomposition, which is computationally expensive and becomes infeasible for large-scale tensors. Recent approaches address this issue by factorizing the tensor into two smaller factor tensors and solving the resulting problem using gradient descent. However, this kind of approach requires an accurate estimate of the tensor rank, and when the rank is overestimated, the convergence of gradient descent and its variants slows down significantly or even diverges. To address this problem, we propose an Alternating Preconditioned Gradient Descent (APGD) algorithm, which accelerates convergence in the over-parameterized setting by adding a preconditioning term to the original gradient and updating these two factors alternately. Based on certain geometric assumptions on the objective function, we establish linear convergence guarantees for more general low-tubal-rank tensor estimation problems. Then we further analyze the specific cases of low-tubal-rank tensor factorization and low-tubal-rank tensor recovery. Our theoretical results show that APGD achieves linear convergence even under over-parameterization, and the convergence rate is independent of the tensor condition number. Extensive simulations on synthetic data are carried out to validate our theoretical assertions.",
            "headline_zh": "提出交替预条件梯度下降算法以解决低管秩张量估计中的过参数化收敛问题",
            "intro_zh": [
                "核心问题：低管秩张量估计中，传统方法计算成本高，梯度下降在秩过估计时收敛慢或发散",
                "方法要点：通过添加预条件项和交替更新因子，加速过参数化设置下的收敛，并建立线性收敛保证",
                "实验或效果：理论分析显示算法在线性收敛且收敛率独立于张量条件数，合成数据模拟验证了理论断言"
            ],
            "tags_zh": [
                "低管秩张量估计",
                "交替预条件梯度下降",
                "过参数化收敛",
                "张量分解",
                "张量恢复",
                "线性收敛保证"
            ],
            "_index": 109
        },
        {
            "title": "Artificial Intelligence and Nuclear Weapons Proliferation: The Technological Arms Race for (In)visibility",
            "authors": [
                "David M. Allison",
                "Stephen Herzog"
            ],
            "arxiv_id": "2512.07487v1",
            "summary": "A robust nonproliferation regime has contained the spread of nuclear weapons to just nine states. Yet, emerging and disruptive technologies are reshaping the landscape of nuclear risks, presenting a critical juncture for decision makers. This article lays out the contours of an overlooked but intensifying technological arms race for nuclear (in)visibility, driven by the interplay between proliferation-enabling technologies (PETs) and detection-enhancing technologies (DETs). We argue that the strategic pattern of proliferation will be increasingly shaped by the innovation pace in these domains. Artificial intelligence (AI) introduces unprecedented complexity to this equation, as its rapid scaling and knowledge substitution capabilities accelerate PET development and challenge traditional monitoring and verification methods. To analyze this dynamic, we develop a formal model centered on a Relative Advantage Index (RAI), quantifying the shifting balance between PETs and DETs. Our model explores how asymmetric technological advancement, particularly logistic AI-driven PET growth versus stepwise DET improvements, expands the band of uncertainty surrounding proliferation detectability. Through replicable scenario-based simulations, we evaluate the impact of varying PET growth rates and DET investment strategies on cumulative nuclear breakout risk. We identify a strategic fork ahead, where detection may no longer suffice without broader PET governance. Governments and international organizations should accordingly invest in policies and tools agile enough to keep pace with tomorrow's technology.",
            "headline_zh": "提出相对优势指数模型，分析人工智能驱动核扩散与检测技术竞赛对核风险的影响。",
            "intro_zh": [
                "核心问题：人工智能加速核扩散技术发展，挑战传统监测方法，增加核扩散不确定性。",
                "方法要点：构建相对优势指数模型，量化扩散技术与检测技术间的动态平衡。",
                "实验或效果：通过基于场景的模拟，评估不同技术增长率和投资策略对核突破风险的累积影响。"
            ],
            "tags_zh": [
                "核扩散技术",
                "检测技术",
                "人工智能驱动",
                "相对优势指数",
                "风险模拟",
                "技术治理"
            ],
            "_index": 110
        },
        {
            "title": "Materium: An Autoregressive Approach for Material Generation",
            "authors": [
                "Niklas Dobberstein",
                "Jan Hamaekers"
            ],
            "arxiv_id": "2512.07486v1",
            "summary": "We present Materium: an autoregressive transformer for generating crystal structures that converts 3D material representations into token sequences. These sequences include elements with oxidation states, fractional coordinates and lattice parameters. Unlike diffusion approaches, which refine atomic positions iteratively through many denoising steps, Materium places atoms at precise fractional coordinates, enabling fast, scalable generation. With this design, the model can be trained in a few hours on a single GPU and generate samples much faster on GPUs and CPUs than diffusion-based approaches. The model was trained and evaluated using multiple properties as conditions, including fundamental properties, such as density and space group, as well as more practical targets, such as band gap and magnetic density. In both single and combined conditions, the model performs consistently well, producing candidates that align with the requested inputs.",
            "headline_zh": "提出Materium自回归变换器以快速生成晶体结构，通过序列化表示实现高效材料设计。",
            "intro_zh": [
                "核心问题：传统扩散方法生成晶体结构需多次迭代去噪，速度慢且计算成本高。",
                "方法要点：将3D材料表示转换为包含元素、氧化态、分数坐标和晶格参数的token序列，实现精确原子放置。",
                "实验或效果：模型在单GPU上几小时可训练，生成速度快于扩散方法，支持多种属性条件生成，性能稳定。"
            ],
            "tags_zh": [
                "材料生成",
                "自回归变换器",
                "晶体结构",
                "序列化表示",
                "条件生成",
                "快速生成"
            ],
            "_index": 111
        },
        {
            "title": "From Real-World Traffic Data to Relevant Critical Scenarios",
            "authors": [
                "Florian Lüttner",
                "Nicole Neis",
                "Daniel Stadler",
                "Robin Moss",
                "Mirjam Fehling-Kaschek",
                "Matthias Pfriem",
                "Alexander Stolz",
                "Jens Ziehn"
            ],
            "arxiv_id": "2512.07482v1",
            "summary": "The reliable operation of autonomous vehicles, automated driving functions, and advanced driver assistance systems across a wide range of relevant scenarios is critical for their development and deployment. Identifying a near-complete set of relevant driving scenarios for such functionalities is challenging due to numerous degrees of freedom involved, each affecting the outcomes of the driving scenario differently. Moreover, with increasing technical complexity of new functionalities, the number of potentially relevant, particularly \"unknown unsafe\" scenarios is increasing. To enhance validation efficiency, it is essential to identify relevant scenarios in advance, starting with simpler domains like highways before moving to more complex environments such as urban traffic. To address this, this paper focuses on analyzing lane change scenarios in highway traffic, which involve multiple degrees of freedom and present numerous safetyrelevant scenarios. We describe the process of data acquisition and processing of real-world data from public highway traffic, followed by the application of criticality measures on trajectory data to evaluate scenarios, as conducted within the AVEAS project (www.aveas.org). By linking the calculated measures to specific lane change driving scenarios and the conditions under which the data was collected, we facilitate the identification of safetyrelevant driving scenarios for various applications. Further, to tackle the extensive range of \"unknown unsafe\" scenarios, we propose a way to generate relevant scenarios by creating synthetic scenarios based on recorded ones. Consequently, we demonstrate and evaluate a processing chain that enables the identification of safety-relevant scenarios, the development of data-driven methods for extracting these scenarios, and the generation of synthetic critical scenarios via sampling on highways.",
            "headline_zh": "提出基于真实高速公路数据的车道变换场景分析与合成方法，以识别和生成安全相关场景。",
            "intro_zh": [
                "核心问题：自动驾驶系统验证需覆盖广泛相关场景，但自由度多且未知不安全场景增加，导致识别挑战。",
                "方法要点：从真实高速公路交通数据采集处理轨迹，应用关键性度量评估车道变换场景，并基于记录生成合成场景。",
                "实验或效果：在AVEAS项目中实现处理链，支持安全相关场景识别、数据驱动方法开发及合成关键场景生成。"
            ],
            "tags_zh": [
                "自动驾驶验证",
                "车道变换场景",
                "真实交通数据",
                "关键性度量",
                "合成场景生成",
                "高速公路安全"
            ],
            "_index": 112
        },
        {
            "title": "Single-step Diffusion-based Video Coding with Semantic-Temporal Guidance",
            "authors": [
                "Naifu Xue",
                "Zhaoyang Jia",
                "Jiahao Li",
                "Bin Li",
                "Zihan Zheng",
                "Yuan Zhang",
                "Yan Lu"
            ],
            "arxiv_id": "2512.07480v1",
            "summary": "While traditional and neural video codecs (NVCs) have achieved remarkable rate-distortion performance, improving perceptual quality at low bitrates remains challenging. Some NVCs incorporate perceptual or adversarial objectives but still suffer from artifacts due to limited generation capacity, whereas others leverage pretrained diffusion models to improve quality at the cost of heavy sampling complexity. To overcome these challenges, we propose S2VC, a Single-Step diffusion based Video Codec that integrates a conditional coding framework with an efficient single-step diffusion generator, enabling realistic reconstruction at low bitrates with reduced sampling cost. Recognizing the importance of semantic conditioning in single-step diffusion, we introduce Contextual Semantic Guidance to extract frame-adaptive semantics from buffered features. It replaces text captions with efficient, fine-grained conditioning, thereby improving generation realism. In addition, Temporal Consistency Guidance is incorporated into the diffusion U-Net to enforce temporal coherence across frames and ensure stable generation. Extensive experiments show that S2VC delivers state-of-the-art perceptual quality with an average 52.73% bitrate saving over prior perceptual methods, underscoring the promise of single-step diffusion for efficient, high-quality video compression.",
            "headline_zh": "提出S2VC单步扩散视频编码器，结合语义与时序指导，在低码率下实现高效高质量视频压缩。",
            "intro_zh": [
                "传统与神经视频编码在低码率下感知质量提升困难，现有方法存在生成能力不足或采样复杂度高的问题。",
                "S2VC采用条件编码框架与单步扩散生成器，引入上下文语义指导和时序一致性指导，提升生成真实性与稳定性。",
                "实验表明S2VC在感知质量上达到先进水平，相比先前方法平均节省52.73%码率，验证单步扩散在视频压缩中的高效性。"
            ],
            "tags_zh": [
                "视频编码",
                "扩散模型",
                "语义指导",
                "时序一致性",
                "低码率压缩",
                "单步生成"
            ],
            "_index": 113
        },
        {
            "title": "Affordance Field Intervention: Enabling VLAs to Escape Memory Traps in Robotic Manipulation",
            "authors": [
                "Siyu Xu",
                "Zijian Wang",
                "Yunke Wang",
                "Chenghao Xia",
                "Tao Huang",
                "Chang Xu"
            ],
            "arxiv_id": "2512.07472v1",
            "summary": "Vision-Language-Action (VLA) models have shown great performance in robotic manipulation by mapping visual observations and language instructions directly to actions. However, they remain brittle under distribution shifts: when test scenarios change, VLAs often reproduce memorized trajectories instead of adapting to the updated scene, which is a failure mode we refer to as the \"Memory Trap\". This limitation stems from the end-to-end design, which lacks explicit 3D spatial reasoning and prevents reliable identification of actionable regions in unfamiliar environments. To compensate for this missing spatial understanding, 3D Spatial Affordance Fields (SAFs) can provide a geometric representation that highlights where interactions are physically feasible, offering explicit cues about regions the robot should approach or avoid. We therefore introduce Affordance Field Intervention (AFI), a lightweight hybrid framework that uses SAFs as an on-demand plug-in to guide VLA behavior. Our system detects memory traps through proprioception, repositions the robot to recent high-affordance regions, and proposes affordance-driven waypoints that anchor VLA-generated actions. A SAF-based scorer then selects trajectories with the highest cumulative affordance. Extensive experiments demonstrate that our method achieves an average improvement of 23.5% across different VLA backbones ($π_{0}$ and $π_{0.5}$) under out-of-distribution scenarios on real-world robotic platforms, and 20.2% on the LIBERO-Pro benchmark, validating its effectiveness in enhancing VLA robustness to distribution shifts.",
            "headline_zh": "提出Affordance Field Intervention，通过3D空间可操作场引导VLA模型，以解决机器人操作中的记忆陷阱问题。",
            "intro_zh": [
                "核心问题：VLA模型在分布偏移下易陷入记忆陷阱，重复记忆轨迹而非适应新场景。",
                "方法要点：使用3D空间可操作场作为轻量级插件，检测记忆陷阱并生成可操作驱动的路径点。",
                "实验或效果：在真实机器人平台和LIBERO-Pro基准上，平均性能提升23.5%和20.2%。"
            ],
            "tags_zh": [
                "视觉-语言-动作模型",
                "机器人操作",
                "3D空间可操作场",
                "分布偏移",
                "记忆陷阱",
                "轻量级框架"
            ],
            "_index": 114
        },
        {
            "title": "Unified Video Editing with Temporal Reasoner",
            "authors": [
                "Xiangpeng Yang",
                "Ji Xie",
                "Yiyuan Yang",
                "Yan Huang",
                "Min Xu",
                "Qiang Wu"
            ],
            "arxiv_id": "2512.07469v1",
            "summary": "Existing video editing methods face a critical trade-off: expert models offer precision but rely on task-specific priors like masks, hindering unification; conversely, unified temporal in-context learning models are mask-free but lack explicit spatial cues, leading to weak instruction-to-region mapping and imprecise localization. To resolve this conflict, we propose VideoCoF, a novel Chain-of-Frames approach inspired by Chain-of-Thought reasoning. VideoCoF enforces a ``see, reason, then edit\" procedure by compelling the video diffusion model to first predict reasoning tokens (edit-region latents) before generating the target video tokens. This explicit reasoning step removes the need for user-provided masks while achieving precise instruction-to-region alignment and fine-grained video editing. Furthermore, we introduce a RoPE alignment strategy that leverages these reasoning tokens to ensure motion alignment and enable length extrapolation beyond the training duration. We demonstrate that with a minimal data cost of only 50k video pairs, VideoCoF achieves state-of-the-art performance on VideoCoF-Bench, validating the efficiency and effectiveness of our approach. Our code, weight, data are available at https://github.com/knightyxp/VideoCoF.",
            "headline_zh": "提出VideoCoF方法，通过链式帧推理解决视频编辑中精度与统一性的冲突。",
            "intro_zh": [
                "现有视频编辑方法在专家模型精度与统一模型通用性间存在权衡，缺乏精确指令到区域映射。",
                "VideoCoF引入链式帧推理，强制模型先预测编辑区域潜在表示，再生成目标视频，实现无掩码精确编辑。",
                "仅用5万视频对训练，在VideoCoF-Bench上达到先进性能，并支持运动对齐和时长外推。"
            ],
            "tags_zh": [
                "视频编辑",
                "链式帧推理",
                "扩散模型",
                "无掩码编辑",
                "运动对齐",
                "时长外推"
            ],
            "_index": 115
        },
        {
            "title": "Gait-Adaptive Perceptive Humanoid Locomotion with Real-Time Under-Base Terrain Reconstruction",
            "authors": [
                "Haolin Song",
                "Hongbo Zhu",
                "Tao Yu",
                "Yan Liu",
                "Mingqi Yuan",
                "Wengang Zhou",
                "Hua Chen",
                "Houqiang Li"
            ],
            "arxiv_id": "2512.07464v1",
            "summary": "For full-size humanoid robots, even with recent advances in reinforcement learning-based control, achieving reliable locomotion on complex terrains, such as long staircases, remains challenging. In such settings, limited perception, ambiguous terrain cues, and insufficient adaptation of gait timing can cause even a single misplaced or mistimed step to result in rapid loss of balance. We introduce a perceptive locomotion framework that merges terrain sensing, gait regulation, and whole-body control into a single reinforcement learning policy. A downward-facing depth camera mounted under the base observes the support region around the feet, and a compact U-Net reconstructs a dense egocentric height map from each frame in real time, operating at the same frequency as the control loop. The perceptual height map, together with proprioceptive observations, is processed by a unified policy that produces joint commands and a global stepping-phase signal, allowing gait timing and whole-body posture to be adapted jointly to the commanded motion and local terrain geometry. We further adopt a single-stage successive teacher-student training scheme for efficient policy learning and knowledge transfer. Experiments conducted on a 31-DoF, 1.65 m humanoid robot demonstrate robust locomotion in both simulation and real-world settings, including forward and backward stair ascent and descent, as well as crossing a 46 cm gap. Project Page:https://ga-phl.github.io/",
            "headline_zh": "提出感知性人形机器人步态自适应框架，以解决复杂地形下可靠行走的挑战。",
            "intro_zh": [
                "核心问题：人形机器人在复杂地形（如长楼梯）行走时，感知有限、步态时序不适应易导致失衡。",
                "方法要点：集成深度相机实时重建地形高度图，通过统一强化学习策略联合调节步态时序和全身姿态。",
                "实验或效果：在31自由度人形机器人上验证，实现楼梯上下行和跨越46厘米间隙的稳健行走。"
            ],
            "tags_zh": [
                "人形机器人行走",
                "地形感知",
                "强化学习控制",
                "实时重建",
                "步态自适应",
                "全身控制"
            ],
            "_index": 116
        },
        {
            "title": "Parallel Algorithms for Combined Regularized Support Vector Machines: Application in Music Genre Classification",
            "authors": [
                "Rongmei Liang",
                "Zizheng Liu",
                "Xiaofei Wu",
                "Jingwen Tu"
            ],
            "arxiv_id": "2512.07463v1",
            "summary": "In the era of rapid development of artificial intelligence, its applications span across diverse fields, relying heavily on effective data processing and model optimization. Combined Regularized Support Vector Machines (CR-SVMs) can effectively handle the structural information among data features, but there is a lack of efficient algorithms in distributed-stored big data. To address this issue, we propose a unified optimization framework based on consensus structure. This framework is not only applicable to various loss functions and combined regularization terms but can also be effectively extended to non-convex regularization terms, showing strong scalability. Based on this framework, we develop a distributed parallel alternating direction method of multipliers (ADMM) algorithm to efficiently compute CR-SVMs when data is stored in a distributed manner. To ensure the convergence of the algorithm, we also introduce the Gaussian back-substitution method. Meanwhile, for the integrity of the paper, we introduce a new model, the sparse group lasso support vector machine (SGL-SVM), and apply it to music information retrieval. Theoretical analysis confirms that the computational complexity of the proposed algorithm is not affected by different regularization terms and loss functions, highlighting the universality of the parallel algorithm. Experiments on synthetic and free music archiv datasets demonstrate the reliability, stability, and efficiency of the algorithm.",
            "headline_zh": "提出基于共识结构的并行ADMM算法以解决分布式存储大数据中组合正则化支持向量机的计算效率问题",
            "intro_zh": [
                "核心问题：组合正则化支持向量机在分布式存储大数据中缺乏高效算法",
                "方法要点：开发分布式并行ADMM算法，引入高斯回代法确保收敛，并扩展至非凸正则化",
                "实验或效果：在合成和音乐档案数据集上验证算法的可靠性、稳定性和效率"
            ],
            "tags_zh": [
                "组合正则化支持向量机",
                "并行ADMM算法",
                "分布式计算",
                "音乐流派分类",
                "高斯回代法",
                "稀疏组套索支持向量机"
            ],
            "_index": 117
        },
        {
            "title": "Understanding LLM Agent Behaviours via Game Theory: Strategy Recognition, Biases and Multi-Agent Dynamics",
            "authors": [
                "Trung-Kiet Huynh",
                "Duy-Minh Dao-Sy",
                "Thanh-Bang Cao",
                "Phong-Hao Le",
                "Hong-Dan Nguyen",
                "Phu-Quy Nguyen-Lam",
                "Minh-Luan Nguyen-Vo",
                "Hong-Phat Pham",
                "Phu-Hoa Pham",
                "Thien-Kim Than",
                "Chi-Nguyen Tran",
                "Huy Tran",
                "Gia-Thoai Tran-Le",
                "Alessio Buscemi",
                "Le Hong Trang",
                "The Anh Han"
            ],
            "arxiv_id": "2512.07462v1",
            "summary": "As Large Language Models (LLMs) increasingly operate as autonomous decision-makers in interactive and multi-agent systems and human societies, understanding their strategic behaviour has profound implications for safety, coordination, and the design of AI-driven social and economic infrastructures. Assessing such behaviour requires methods that capture not only what LLMs output, but the underlying intentions that guide their decisions. In this work, we extend the FAIRGAME framework to systematically evaluate LLM behaviour in repeated social dilemmas through two complementary advances: a payoff-scaled Prisoners Dilemma isolating sensitivity to incentive magnitude, and an integrated multi-agent Public Goods Game with dynamic payoffs and multi-agent histories. These environments reveal consistent behavioural signatures across models and languages, including incentive-sensitive cooperation, cross-linguistic divergence and end-game alignment toward defection. To interpret these patterns, we train traditional supervised classification models on canonical repeated-game strategies and apply them to FAIRGAME trajectories, showing that LLMs exhibit systematic, model- and language-dependent behavioural intentions, with linguistic framing at times exerting effects as strong as architectural differences. Together, these findings provide a unified methodological foundation for auditing LLMs as strategic agents and reveal systematic cooperation biases with direct implications for AI governance, collective decision-making, and the design of safe multi-agent systems.",
            "headline_zh": "扩展FAIRGAME框架以评估LLM在重复社会困境中的战略行为，揭示合作偏差与语言影响",
            "intro_zh": [
                "核心问题：理解LLM作为自主决策者在多智能体系统中的战略意图，对AI安全与协调至关重要",
                "方法要点：通过收益缩放囚徒困境和动态收益公共物品游戏，系统评估LLM行为模式",
                "实验或效果：发现LLM表现出激励敏感合作、跨语言差异和背叛倾向，行为意图受模型和语言影响"
            ],
            "tags_zh": [
                "大型语言模型",
                "博弈论",
                "多智能体系统",
                "社会困境",
                "行为评估",
                "AI治理"
            ],
            "_index": 118
        },
        {
            "title": "Human Geometry Distribution for 3D Animation Generation",
            "authors": [
                "Xiangjun Tang",
                "Biao Zhang",
                "Peter Wonka"
            ],
            "arxiv_id": "2512.07459v1",
            "summary": "Generating realistic human geometry animations remains a challenging task, as it requires modeling natural clothing dynamics with fine-grained geometric details under limited data. To address these challenges, we propose two novel designs. First, we propose a compact distribution-based latent representation that enables efficient and high-quality geometry generation. We improve upon previous work by establishing a more uniform mapping between SMPL and avatar geometries. Second, we introduce a generative animation model that fully exploits the diversity of limited motion data. We focus on short-term transitions while maintaining long-term consistency through an identity-conditioned design. These two designs formulate our method as a two-stage framework: the first stage learns a latent space, while the second learns to generate animations within this latent space. We conducted experiments on both our latent space and animation model. We demonstrate that our latent space produces high-fidelity human geometry surpassing previous methods ($90\\%$ lower Chamfer Dist.). The animation model synthesizes diverse animations with detailed and natural dynamics ($2.2 \\times$ higher user study score), achieving the best results across all evaluation metrics.",
            "headline_zh": "提出基于分布表示和生成动画模型的两阶段框架，以解决有限数据下人体几何动画生成中服装动态建模的挑战。",
            "intro_zh": [
                "核心问题：在有限数据下生成具有自然服装动态和精细几何细节的人体几何动画。",
                "方法要点：采用两阶段框架，第一阶段学习紧凑分布表示，第二阶段基于身份条件生成动画。",
                "实验或效果：在潜在空间和动画模型上均取得最佳结果，如Chamfer距离降低90%，用户研究得分提高2.2倍。"
            ],
            "tags_zh": [
                "人体几何动画",
                "分布表示",
                "生成模型",
                "服装动态建模",
                "潜在空间学习",
                "身份条件生成"
            ],
            "_index": 119
        },
        {
            "title": "Optimized Machine Learning Methods for Studying the Thermodynamic Behavior of Complex Spin Systems",
            "authors": [
                "Dmitrii Kapitan",
                "Pavel Ovchinnikov",
                "Konstantin Soldatov",
                "Petr Andriushchenko",
                "Vitalii Kapitan"
            ],
            "arxiv_id": "2512.07458v1",
            "summary": "This paper presents a systematic study of the application of convolutional neural networks (CNNs) as an efficient and versatile tool for the analysis of critical and low-temperature phase states in spin system models. The problem of calculating the dependence of the average energy on the spatial distribution of exchange integrals for the Edwards-Anderson model on a square lattice with frustrated interactions is considered. We further construct a single convolutional classifier of phase states of the ferromagnetic Ising model on square, triangular, honeycomb, and kagome lattices, trained on configurations generated by the Swendsen-Wang cluster algorithm. Computed temperature profiles of the averaged posterior probability of the high-temperature phase form clear S-shaped curves that intersect in the vicinity of the theoretical critical temperatures and allow one to determine the critical temperature for the kagome lattice without additional retraining. It is shown that convolutional models substantially reduce the root-mean-square error (RMSE) compared with fully connected architectures and efficiently capture complex correlations between thermodynamic characteristics and the structure of magnetic correlated systems.",
            "headline_zh": "提出卷积神经网络方法以分析复杂自旋系统的热力学行为",
            "intro_zh": [
                "研究卷积神经网络在自旋系统临界和低温相态分析中的应用",
                "构建单卷积分类器，用于多种晶格铁磁伊辛模型的相态分类",
                "卷积模型显著降低均方根误差，有效捕捉热力学特性与磁相关系统结构间的复杂关联"
            ],
            "tags_zh": [
                "卷积神经网络",
                "自旋系统",
                "热力学分析",
                "相态分类",
                "临界温度"
            ],
            "_index": 120
        },
        {
            "title": "Persian-Phi: Efficient Cross-Lingual Adaptation of Compact LLMs via Curriculum Learning",
            "authors": [
                "Amir Mohammad Akhlaghi",
                "Amirhossein Shabani",
                "Mostafa Abdolmaleki",
                "Saeed Reza Kheradpisheh"
            ],
            "arxiv_id": "2512.07454v1",
            "summary": "The democratization of AI is currently hindered by the immense computational costs required to train Large Language Models (LLMs) for low-resource languages. This paper presents Persian-Phi, a 3.8B parameter model that challenges the assumption that robust multilingual capabilities require massive model sizes or multilingual baselines. We demonstrate how Microsoft Phi-3 Mini -- originally a monolingual English model -- can be effectively adapted to Persian through a novel, resource-efficient curriculum learning pipeline. Our approach employs a unique \"warm-up\" stage using bilingual narratives (Tiny Stories) to align embeddings prior to heavy training, followed by continual pretraining and instruction tuning via Parameter-Efficient Fine-Tuning (PEFT). Despite its compact size, Persian-Phi achieves competitive results on Open Persian LLM Leaderboard in HuggingFace. Our findings provide a validated, scalable framework for extending the reach of state-of-the-art LLMs to underrepresented languages with minimal hardware resources. The Persian-Phi model is publicly available at https://huggingface.co/amirakhlaghiqqq/PersianPhi.",
            "headline_zh": "提出Persian-Phi模型，通过课程学习高效适配低资源波斯语，挑战大规模多语言模型假设。",
            "intro_zh": [
                "核心问题：低资源语言训练大语言模型计算成本高，阻碍AI民主化。",
                "方法要点：采用课程学习，先双语叙事预热对齐嵌入，再持续预训练和指令调优。",
                "实验或效果：模型在Open Persian LLM Leaderboard上取得竞争性结果，提供可扩展框架。"
            ],
            "tags_zh": [
                "跨语言适配",
                "课程学习",
                "参数高效微调",
                "低资源语言",
                "波斯语模型"
            ],
            "_index": 121
        },
        {
            "title": "Social welfare optimisation in well-mixed and structured populations",
            "authors": [
                "Van An Nguyen",
                "Vuong Khang Huynh",
                "Ho Nam Duong",
                "Huu Loi Bui",
                "Hai Anh Ha",
                "Quang Dung Le",
                "Le Quoc Dung Ngo",
                "Tan Dat Nguyen",
                "Ngoc Ngu Nguyen",
                "Hoai Thuong Nguyen",
                "Zhao Song",
                "Le Hong Trang",
                "The Anh Han"
            ],
            "arxiv_id": "2512.07453v1",
            "summary": "Research on promoting cooperation among autonomous, self-regarding agents has often focused on the bi-objective optimisation problem: minimising the total incentive cost while maximising the frequency of cooperation. However, the optimal value of social welfare under such constraints remains largely unexplored. In this work, we hypothesise that achieving maximal social welfare is not guaranteed at the minimal incentive cost required to drive agents to a desired cooperative state. To address this gap, we adopt to a single-objective approach focused on maximising social welfare, building upon foundational evolutionary game theory models that examined cost efficiency in finite populations, in both well-mixed and structured population settings. Our analytical model and agent-based simulations show how different interference strategies, including rewarding local versus global behavioural patterns, affect social welfare and dynamics of cooperation. Our results reveal a significant gap in the per-individual incentive cost between optimising for pure cost efficiency or cooperation frequency and optimising for maximal social welfare. Overall, our findings indicate that incentive design, policy, and benchmarking in multi-agent systems and human societies should prioritise welfare-centric objectives over proxy targets of cost or cooperation frequency.",
            "headline_zh": "提出单目标优化方法以最大化社会福祉，在混合与结构化群体中分析激励策略影响",
            "intro_zh": [
                "核心问题：现有研究关注最小化激励成本与最大化合作频率，但社会福祉最优值未知",
                "方法要点：基于进化博弈论模型，采用单目标优化聚焦社会福祉最大化，分析局部与全局激励策略",
                "实验或效果：通过分析模型与基于代理的模拟，揭示优化社会福祉与成本效率间存在显著激励成本差距"
            ],
            "tags_zh": [
                "社会福祉优化",
                "进化博弈论",
                "激励策略",
                "多智能体系统",
                "结构化群体",
                "合作动力学"
            ],
            "_index": 122
        },
        {
            "title": "Forget and Explain: Transparent Verification of GNN Unlearning",
            "authors": [
                "Imran Ahsan",
                "Hyunwook Yu",
                "Jinsung Kim",
                "Mucheol Kim"
            ],
            "arxiv_id": "2512.07450v1",
            "summary": "Graph neural networks (GNNs) are increasingly used to model complex patterns in graph-structured data. However, enabling them to \"forget\" designated information remains challenging, especially under privacy regulations such as the GDPR. Existing unlearning methods largely optimize for efficiency and scalability, yet they offer little transparency, and the black-box nature of GNNs makes it difficult to verify whether forgetting has truly occurred. We propose an explainability-driven verifier for GNN unlearning that snapshots the model before and after deletion, using attribution shifts and localized structural changes (for example, graph edit distance) as transparent evidence. The verifier uses five explainability metrics: residual attribution, heatmap shift, explainability score deviation, graph edit distance, and a diagnostic graph rule shift. We evaluate two backbones (GCN, GAT) and four unlearning strategies (Retrain, GraphEditor, GNNDelete, IDEA) across five benchmarks (Cora, Citeseer, Pubmed, Coauthor-CS, Coauthor-Physics). Results show that Retrain and GNNDelete achieve near-complete forgetting, GraphEditor provides partial erasure, and IDEA leaves residual signals. These explanation deltas provide the primary, human-readable evidence of forgetting; we also report membership-inference ROC-AUC as a complementary, graph-wide privacy signal.",
            "headline_zh": "提出基于可解释性的验证器以透明验证图神经网络遗忘效果",
            "intro_zh": [
                "核心问题：图神经网络遗忘缺乏透明度，难以验证信息是否真正删除",
                "方法要点：利用归因偏移和局部结构变化作为透明证据，定义五种可解释性指标",
                "实验或效果：评估多种遗忘策略，结果显示Retrain和GNNDelete接近完全遗忘，解释差异提供主要证据"
            ],
            "tags_zh": [
                "图神经网络遗忘",
                "可解释性验证",
                "隐私保护",
                "归因分析",
                "图编辑距离"
            ],
            "_index": 123
        },
        {
            "title": "KAN-Dreamer: Benchmarking Kolmogorov-Arnold Networks as Function Approximators in World Models",
            "authors": [
                "Chenwei Shi",
                "Xueyu Luan"
            ],
            "arxiv_id": "2512.07437v1",
            "summary": "DreamerV3 is a state-of-the-art online model-based reinforcement learning (MBRL) algorithm known for remarkable sample efficiency. Concurrently, Kolmogorov-Arnold Networks (KANs) have emerged as a promising alternative to Multi-Layer Perceptrons (MLPs), offering superior parameter efficiency and interpretability. To mitigate KANs' computational overhead, variants like FastKAN leverage Radial Basis Functions (RBFs) to accelerate inference. In this work, we investigate integrating KAN architectures into the DreamerV3 framework. We introduce KAN-Dreamer, replacing specific MLP and convolutional components of DreamerV3 with KAN and FastKAN layers. To ensure efficiency within the JAX-based World Model, we implement a tailored, fully vectorized version with simplified grid management. We structure our investigation into three subsystems: Visual Perception, Latent Prediction, and Behavior Learning. Empirical evaluations on the DeepMind Control Suite (walker_walk) analyze sample efficiency, training time, and asymptotic performance. Experimental results demonstrate that utilizing our adapted FastKAN as a drop-in replacement for the Reward and Continue predictors yields performance on par with the original MLP-based architecture, maintaining parity in both sample efficiency and training speed. This report serves as a preliminary study for future developments in KAN-based world models.",
            "headline_zh": "提出KAN-Dreamer，将KAN架构集成到DreamerV3中作为函数逼近器进行基准测试。",
            "intro_zh": [
                "研究将Kolmogorov-Arnold Networks（KANs）作为MLP替代品集成到DreamerV3世界模型中。",
                "在JAX框架下实现向量化版本，用KAN和FastKAN替换特定MLP和卷积组件。",
                "在DeepMind Control Suite上实验，FastKAN在奖励和继续预测器上性能与MLP相当。"
            ],
            "tags_zh": [
                "基于模型的强化学习",
                "Kolmogorov-Arnold网络",
                "世界模型",
                "函数逼近",
                "样本效率",
                "JAX实现"
            ],
            "_index": 124
        },
        {
            "title": "LocalSearchBench: Benchmarking Agentic Search in Real-World Local Life Services",
            "authors": [
                "Hang He",
                "Chuhuai Yue",
                "Chengqi Dong",
                "Mingxue Tian",
                "Zhenfeng Liu",
                "Jiajun Chai",
                "Xiaohan Wang",
                "Yufei Zhang",
                "Qun Liao",
                "Guojun Yin",
                "Wei Lin",
                "Chengcheng Wan",
                "Haiying Sun",
                "Ting Su"
            ],
            "arxiv_id": "2512.07436v1",
            "summary": "Recent advances in large reasoning models (LRMs) have enabled agentic search systems to perform complex multi-step reasoning across multiple sources. However, most studies focus on general information retrieval and rarely explores vertical domains with unique challenges. In this work, we focus on local life services and introduce LocalSearchBench, which encompass diverse and complex business scenarios. Real-world queries in this domain are often ambiguous and require multi-hop reasoning across merchants and products, remaining challenging and not fully addressed. As the first comprehensive benchmark for agentic search in local life services, LocalSearchBench includes over 150,000 high-quality entries from various cities and business types. We construct 300 multi-hop QA tasks based on real user queries, challenging agents to understand questions and retrieve information in multiple steps. We also developed LocalPlayground, a unified environment integrating multiple tools for agent interaction. Experiments show that even state-of-the-art LRMs struggle on LocalSearchBench: the best model (DeepSeek-V3.1) achieves only 34.34% correctness, and most models have issues with completeness (average 77.33%) and faithfulness (average 61.99%). This highlights the need for specialized benchmarks and domain-specific agent training in local life services. Code, Benchmark, and Leaderboard are available at localsearchbench.github.io.",
            "headline_zh": "提出LocalSearchBench基准以评估本地生活服务中的智能搜索代理性能",
            "intro_zh": [
                "核心问题：现有智能搜索研究多关注通用信息检索，缺乏针对本地生活服务等垂直领域的基准，该领域查询模糊且需多跳推理。",
                "方法要点：构建包含15万条高质量条目的基准，基于真实用户查询设计300个多跳问答任务，并开发统一交互环境LocalPlayground。",
                "实验或效果：实验显示，即使最先进的大推理模型（如DeepSeek-V3.1）在基准上正确率仅34.34%，凸显领域特定训练的必要性。"
            ],
            "tags_zh": [
                "智能搜索代理",
                "本地生活服务",
                "多跳推理",
                "基准测试",
                "大推理模型",
                "垂直领域"
            ],
            "_index": 125
        },
        {
            "title": "Mitigating Bias in Graph Hyperdimensional Computing",
            "authors": [
                "Yezi Liu",
                "William Youngwoo Chung",
                "Yang Ni",
                "Hanning Chen",
                "Mohsen Imani"
            ],
            "arxiv_id": "2512.07433v1",
            "summary": "Graph hyperdimensional computing (HDC) has emerged as a promising paradigm for cognitive tasks, emulating brain-like computation with high-dimensional vectors known as hypervectors. While HDC offers robustness and efficiency on graph-structured data, its fairness implications remain largely unexplored. In this paper, we study fairness in graph HDC, where biases in data representation and decision rules can lead to unequal treatment of different groups. We show how hypervector encoding and similarity-based classification can propagate or even amplify such biases, and we propose a fairness-aware training framework, FairGHDC, to mitigate them. FairGHDC introduces a bias correction term, derived from a gap-based demographic-parity regularizer, and converts it into a scalar fairness factor that scales the update of the class hypervector for the ground-truth label. This enables debiasing directly in the hypervector space without modifying the graph encoder or requiring backpropagation. Experimental results on six benchmark datasets demonstrate that FairGHDC substantially reduces demographic-parity and equal-opportunity gaps while maintaining accuracy comparable to standard GNNs and fairness-aware GNNs. At the same time, FairGHDC preserves the computational advantages of HDC, achieving up to about one order of magnitude ($\\approx 10\\times$) speedup in training time on GPU compared to GNN and fairness-aware GNN baselines.",
            "headline_zh": "提出FairGHDC框架以缓解图超维计算中的偏见问题",
            "intro_zh": [
                "研究图超维计算中数据表示和决策规则偏见导致的不公平处理",
                "提出基于人口统计均等正则器的偏置校正项，转换为公平因子直接更新类超向量",
                "实验显示FairGHDC显著减少公平性差距，保持准确性，训练速度提升约10倍"
            ],
            "tags_zh": [
                "图超维计算",
                "公平性学习",
                "偏见缓解",
                "超向量编码",
                "人口统计均等",
                "计算效率"
            ],
            "_index": 126
        },
        {
            "title": "MIDG: Mixture of Invariant Experts with knowledge injection for Domain Generalization in Multimodal Sentiment Analysis",
            "authors": [
                "Yangle Li",
                "Danli Luo",
                "Haifeng Hu"
            ],
            "arxiv_id": "2512.07430v1",
            "summary": "Existing methods in domain generalization for Multimodal Sentiment Analysis (MSA) often overlook inter-modal synergies during invariant features extraction, which prevents the accurate capture of the rich semantic information within multimodal data. Additionally, while knowledge injection techniques have been explored in MSA, they often suffer from fragmented cross-modal knowledge, overlooking specific representations that exist beyond the confines of unimodal. To address these limitations, we propose a novel MSA framework designed for domain generalization. Firstly, the framework incorporates a Mixture of Invariant Experts model to extract domain-invariant features, thereby enhancing the model's capacity to learn synergistic relationships between modalities. Secondly, we design a Cross-Modal Adapter to augment the semantic richness of multimodal representations through cross-modal knowledge injection. Extensive domain experiments conducted on three datasets demonstrate that the proposed MIDG achieves superior performance.",
            "headline_zh": "提出MIDG框架，通过混合不变专家和跨模态适配器解决多模态情感分析中的领域泛化问题。",
            "intro_zh": [
                "现有方法在提取不变特征时忽视模态间协同，导致语义信息捕获不准确。",
                "MIDG结合混合不变专家模型提取领域不变特征，增强模态间协同学习能力。",
                "在三个数据集上的实验显示，MIDG在领域泛化任务中表现优异。"
            ],
            "tags_zh": [
                "多模态情感分析",
                "领域泛化",
                "不变特征提取",
                "跨模态知识注入",
                "混合专家模型"
            ],
            "_index": 127
        },
        {
            "title": "When normalization hallucinates: unseen risks in AI-powered whole slide image processing",
            "authors": [
                "Karel Moens",
                "Matthew B. Blaschko",
                "Tinne Tuytelaars",
                "Bart Diricx",
                "Jonas De Vylder",
                "Mustafa Yousif"
            ],
            "arxiv_id": "2512.07426v1",
            "summary": "Whole slide image (WSI) normalization remains a vital preprocessing step in computational pathology. Increasingly driven by deep learning, these models learn to approximate data distributions from training examples. This often results in outputs that gravitate toward the average, potentially masking diagnostically important features. More critically, they can introduce hallucinated content, artifacts that appear realistic but are not present in the original tissue, posing a serious threat to downstream analysis. These hallucinations are nearly impossible to detect visually, and current evaluation practices often overlook them. In this work, we demonstrate that the risk of hallucinations is real and underappreciated. While many methods perform adequately on public datasets, we observe a concerning frequency of hallucinations when these same models are retrained and evaluated on real-world clinical data. To address this, we propose a novel image comparison measure designed to automatically detect hallucinations in normalized outputs. Using this measure, we systematically evaluate several well-cited normalization methods retrained on real-world data, revealing significant inconsistencies and failures that are not captured by conventional metrics. Our findings underscore the need for more robust, interpretable normalization techniques and stricter validation protocols in clinical deployment.",
            "headline_zh": "提出图像比较度量以检测全切片图像归一化中的幻觉风险",
            "intro_zh": [
                "核心问题：深度学习驱动的全切片图像归一化可能引入幻觉内容，威胁下游分析。",
                "方法要点：设计新颖图像比较度量，自动检测归一化输出中的幻觉。",
                "实验或效果：在真实临床数据上评估方法，揭示传统指标未捕捉的显著不一致和失败。"
            ],
            "tags_zh": [
                "全切片图像归一化",
                "幻觉检测",
                "计算病理学",
                "深度学习",
                "图像比较度量",
                "临床验证"
            ],
            "_index": 128
        },
        {
            "title": "Microseismic event classification with a lightweight Fourier Neural Operator model",
            "authors": [
                "Ayrat Abdullin",
                "Umair bin Waheed",
                "Leo Eisner",
                "Abdullatif Al-Shuhail"
            ],
            "arxiv_id": "2512.07425v1",
            "summary": "Real-time monitoring of induced seismicity is crucial for mitigating operational hazards, relying on the rapid and accurate classification of microseismic events from continuous data streams. However, while many deep learning models excel at this task, their high computational requirements often limit their practical application in real-time monitoring systems. To address this limitation, a lightweight model based on the Fourier Neural Operator (FNO) is proposed for microseismic event classification, leveraging its inherent resolution-invariance and computational efficiency for waveform processing. In the STanford EArthquake Dataset (STEAD), a global and large-scale database of seismic waveforms, the FNO-based model demonstrates high effectiveness for trigger classification, with an F1 score of 95% even in the scenario of data sparsity in training. The new FNO model greatly decreases the computer power needed relative to current deep learning models without sacrificing the classification success rate measured by the F1 score. A test on a real microseismic dataset shows a classification success rate with an F1 score of 98%, outperforming many traditional deep-learning techniques. A combination of high success rate and low computational power indicates that the FNO model can serve as a methodology of choice for real-time monitoring of microseismicity for induced seismicity. The method saves computational resources and facilitates both post-processing and real-time seismic processing suitable for the implementation of traffic light systems to prevent undesired induced seismicity.",
            "headline_zh": "提出轻量级傅里叶神经算子模型以解决微震事件实时分类中的计算效率问题",
            "intro_zh": [
                "核心问题：实时监测诱发地震需快速准确分类微震事件，但现有深度学习模型计算需求高，限制实际应用。",
                "方法要点：基于傅里叶神经算子构建轻量模型，利用其分辨率不变性和计算高效性处理波形数据。",
                "实验或效果：在STEAD数据集上F1分数达95%，真实数据集上达98%，计算资源需求显著降低，适合实时监测。"
            ],
            "tags_zh": [
                "微震事件分类",
                "傅里叶神经算子",
                "轻量模型",
                "实时监测",
                "诱发地震",
                "波形处理"
            ],
            "_index": 129
        },
        {
            "title": "Revolutionizing Mixed Precision Quantization: Towards Training-free Automatic Proxy Discovery via Large Language Models",
            "authors": [
                "Haidong Kang",
                "Jun Du",
                "Lihong Lin"
            ],
            "arxiv_id": "2512.07419v1",
            "summary": "Mixed-Precision Quantization (MPQ) liberates the Deep Neural Networks (DNNs) from the Out-Of-Memory (OOM) bottleneck, which garnered increasing research attention. However, conventional methods either searched from costly differentiable optimization, which is neither efficient nor flexible, or learned a quantized DNN from the proxy (i.e., HAWQ) manually designed by human experts, which is labor-intensive and requires huge expert knowledge. Can we design a proxy without involving any human experts and training? In this paper, we provide an affirmative answer by proposing a novel Large Language Models (LLMs)-driven Training-free Automatic Proxy (dubbed TAP) discovery framework, which reforms the design paradigm of MPQ by utilizing LLMs to find superior TAP tailored for MPQ, automatically. In addition, to bridge the gap between black-box LLMs and the tough MPQ task, we ingeniously propose simple Direct Policy Optimization (DPO) based reinforcement learning to enhance LLMs' reasoning by optimizing prompts, which can construct a positive feedback loop between the LLM and the MPQ task, enabling LLMs to generate better TAP in the next evolution. Extensive experiments on mainstream benchmarks demonstrate that TAP achieves state-of-the-art performance. Finally, we truly believe that our TAP will significantly contribute to the MPQ community by providing a new perspective on LLM-driven design algorithms.",
            "headline_zh": "提出基于大语言模型的训练无关自动代理发现框架，以革新混合精度量化设计范式。",
            "intro_zh": [
                "核心问题：混合精度量化依赖人工设计代理或高成本优化，效率低且不灵活。",
                "方法要点：利用大语言模型自动发现代理，通过直接策略优化增强推理，形成正反馈循环。",
                "实验或效果：在主流基准测试中实现最先进性能，为混合精度量化社区提供新视角。"
            ],
            "tags_zh": [
                "混合精度量化",
                "大语言模型",
                "训练无关优化",
                "自动代理发现",
                "强化学习",
                "深度学习压缩"
            ],
            "_index": 130
        },
        {
            "title": "Adaptive Tuning of Parameterized Traffic Controllers via Multi-Agent Reinforcement Learning",
            "authors": [
                "Giray Önür",
                "Azita Dabiri",
                "Bart De Schutter"
            ],
            "arxiv_id": "2512.07417v1",
            "summary": "Effective traffic control is essential for mitigating congestion in transportation networks. Conventional traffic management strategies, including route guidance, ramp metering, and traffic signal control, often rely on state feedback controllers, used for their simplicity and reactivity; however, they lack the adaptability required to cope with complex and time-varying traffic dynamics. This paper proposes a multi-agent reinforcement learning framework in which each agent adaptively tunes the parameters of a state feedback traffic controller, combining the reactivity of state feedback controllers with the adaptability of reinforcement learning. By tuning parameters at a lower frequency rather than directly determining control actions at a high frequency, the reinforcement learning agents achieve improved training efficiency while maintaining adaptability to varying traffic conditions. The multi-agent structure further enhances system robustness, as local controllers can operate independently in the event of partial failures. The proposed framework is evaluated on a simulated multi-class transportation network under varying traffic conditions. Results show that the proposed multi-agent framework outperforms the no control and fixed-parameter state feedback control cases, while performing on par with the single-agent RL-based adaptive state feedback control, with a much better resilience to partial failures.",
            "headline_zh": "提出多智能体强化学习框架，自适应调整参数化交通控制器以应对动态交通",
            "intro_zh": [
                "核心问题：传统状态反馈控制器缺乏适应性，难以处理复杂时变交通动态。",
                "方法要点：多智能体强化学习框架，低频调整控制器参数，结合反应性与适应性。",
                "实验或效果：在模拟多类交通网络中评估，优于无控制和固定参数控制，对部分故障有强韧性。"
            ],
            "tags_zh": [
                "多智能体强化学习",
                "交通控制",
                "参数自适应",
                "状态反馈控制器",
                "系统韧性"
            ],
            "_index": 131
        },
        {
            "title": "Data-driven Exploration of Mobility Interaction Patterns",
            "authors": [
                "Gabriele Galatolo",
                "Mirco Nanni"
            ],
            "arxiv_id": "2512.07415v1",
            "summary": "Understanding the movement behaviours of individuals and the way they react to the external world is a key component of any problem that involves the modelling of human dynamics at a physical level. In particular, it is crucial to capture the influence that the presence of an individual can have on the others. Important examples of applications include crowd simulation and emergency management, where the simulation of the mass of people passes through the simulation of the individuals, taking into consideration the others as part of the general context. While existing solutions basically start from some preconceived behavioural model, in this work we propose an approach that starts directly from the data, adopting a data mining perspective. Our method searches the mobility events in the data that might be possible evidences of mutual interactions between individuals, and on top of them looks for complex, persistent patterns and time evolving configurations of events. The study of these patterns can provide new insights on the mechanics of mobility interactions between individuals, which can potentially help in improving existing simulation models. We instantiate the general methodology on two real case studies, one on cars and one on pedestrians, and a full experimental evaluation is performed, both in terms of performances, parameter sensitivity and interpretation of sample results.",
            "headline_zh": "提出数据驱动的移动交互模式挖掘方法，以改进人群模拟和应急管理模型。",
            "intro_zh": [
                "核心问题：现有模型基于预设行为，难以捕捉个体间移动交互的复杂动态。",
                "方法要点：从数据中直接挖掘移动事件，识别互动的证据和持久模式。",
                "实验或效果：在汽车和行人案例中验证方法性能，分析参数敏感性和结果解释。"
            ],
            "tags_zh": [
                "移动交互模式",
                "数据挖掘",
                "人群模拟",
                "应急管理",
                "行为建模"
            ],
            "_index": 132
        },
        {
            "title": "InterAgent: Physics-based Multi-agent Command Execution via Diffusion on Interaction Graphs",
            "authors": [
                "Bin Li",
                "Ruichi Zhang",
                "Han Liang",
                "Jingyan Zhang",
                "Juze Zhang",
                "Xin Chen",
                "Lan Xu",
                "Jingyi Yu",
                "Jingya Wang"
            ],
            "arxiv_id": "2512.07410v1",
            "summary": "Humanoid agents are expected to emulate the complex coordination inherent in human social behaviors. However, existing methods are largely confined to single-agent scenarios, overlooking the physically plausible interplay essential for multi-agent interactions. To bridge this gap, we propose InterAgent, the first end-to-end framework for text-driven physics-based multi-agent humanoid control. At its core, we introduce an autoregressive diffusion transformer equipped with multi-stream blocks, which decouples proprioception, exteroception, and action to mitigate cross-modal interference while enabling synergistic coordination. We further propose a novel interaction graph exteroception representation that explicitly captures fine-grained joint-to-joint spatial dependencies to facilitate network learning. Additionally, within it we devise a sparse edge-based attention mechanism that dynamically prunes redundant connections and emphasizes critical inter-agent spatial relations, thereby enhancing the robustness of interaction modeling. Extensive experiments demonstrate that InterAgent consistently outperforms multiple strong baselines, achieving state-of-the-art performance. It enables producing coherent, physically plausible, and semantically faithful multi-agent behaviors from only text prompts. Our code and data will be released to facilitate future research.",
            "headline_zh": "提出InterAgent框架，通过交互图扩散实现基于物理的多智能体人形控制",
            "intro_zh": [
                "核心问题：现有方法多局限于单智能体场景，缺乏物理合理的多智能体交互建模",
                "方法要点：采用自回归扩散变换器与多流块，解耦本体感知、外感知和动作，并引入交互图外感知表示与稀疏边注意力机制",
                "实验或效果：在实验中超越多个基线，实现从文本提示生成连贯、物理合理且语义忠实的行为"
            ],
            "tags_zh": [
                "多智能体控制",
                "物理模拟",
                "扩散模型",
                "交互图",
                "人形机器人",
                "文本驱动"
            ],
            "_index": 133
        },
        {
            "title": "Do LLMs Trust the Code They Write?",
            "authors": [
                "Francisco Ribeiro",
                "Claudio Spiess",
                "Prem Devanbu",
                "Sarah Nadi"
            ],
            "arxiv_id": "2512.07404v1",
            "summary": "Despite the effectiveness of large language models (LLMs) for code generation, they often output incorrect code. One reason is that model output probabilities are often not well-correlated with correctness, and reflect only the final output of the generation process. Inspired by findings that LLMs internally encode concepts like truthfulness, this paper explores if LLMs similarly represent code correctness. Specifically, we identify a correctness representation inside LLMs by contrasting the hidden states between pairs of correct and incorrect code for the same programming tasks. By experimenting on four LLMs, we show that exploiting this extracted correctness representation outperforms standard log-likelihood ranking, as well as verbalized model confidence. Furthermore, we explore how this internal correctness signal can be used to select higher-quality code samples, without requiring test execution. Ultimately, this work demonstrates how leveraging internal representations can enhance code generation systems and make LLMs more reliable, thus improving confidence in automatically generated code.",
            "headline_zh": "提出利用LLM内部正确性表示以提升代码生成质量，无需测试执行。",
            "intro_zh": [
                "核心问题：LLM生成代码时输出概率与正确性关联弱，导致错误代码频发。",
                "方法要点：通过对比正确与错误代码的隐藏状态，提取LLM内部编码的正确性表示。",
                "实验或效果：在四个LLM上实验，该表示优于标准对数似然排序和模型口头置信度，能选择更高质量代码。"
            ],
            "tags_zh": [
                "大语言模型",
                "代码生成",
                "内部表示",
                "正确性检测",
                "隐藏状态分析"
            ],
            "_index": 134
        },
        {
            "title": "Asymptotic analysis of shallow and deep forgetting in replay with Neural Collapse",
            "authors": [
                "Giulia Lanzillotta",
                "Damiano Meier",
                "Thomas Hofmann"
            ],
            "arxiv_id": "2512.07400v1",
            "summary": "A persistent paradox in continual learning (CL) is that neural networks often retain linearly separable representations of past tasks even when their output predictions fail. We formalize this distinction as the gap between deep feature-space and shallow classifier-level forgetting. We reveal a critical asymmetry in Experience Replay: while minimal buffers successfully anchor feature geometry and prevent deep forgetting, mitigating shallow forgetting typically requires substantially larger buffer capacities. To explain this, we extend the Neural Collapse framework to the sequential setting. We characterize deep forgetting as a geometric drift toward out-of-distribution subspaces and prove that any non-zero replay fraction asymptotically guarantees the retention of linear separability. Conversely, we identify that the \"strong collapse\" induced by small buffers leads to rank-deficient covariances and inflated class means, effectively blinding the classifier to true population boundaries. By unifying CL with out-of-distribution detection, our work challenges the prevailing reliance on large buffers, suggesting that explicitly correcting these statistical artifacts could unlock robust performance with minimal replay.",
            "headline_zh": "扩展神经坍缩框架分析经验回放中浅层与深层遗忘的不对称性",
            "intro_zh": [
                "持续学习中神经网络常保留过去任务的线性可分表示，但输出预测失败，定义为浅层与深层遗忘的差距。",
                "揭示经验回放中不对称性：小缓冲区可锚定特征几何防止深层遗忘，但缓解浅层遗忘需更大容量。",
                "扩展神经坍缩框架至序列设置，证明非零回放分数渐近保证线性可分性，而小缓冲区导致统计伪影。"
            ],
            "tags_zh": [
                "持续学习",
                "经验回放",
                "神经坍缩",
                "遗忘分析",
                "特征几何",
                "统计伪影"
            ],
            "_index": 135
        },
        {
            "title": "Reconstructing Objects along Hand Interaction Timelines in Egocentric Video",
            "authors": [
                "Zhifan Zhu",
                "Siddhant Bansal",
                "Shashank Tripathi",
                "Dima Damen"
            ],
            "arxiv_id": "2512.07394v1",
            "summary": "We introduce the task of Reconstructing Objects along Hand Interaction Timelines (ROHIT). We first define the Hand Interaction Timeline (HIT) from a rigid object's perspective. In a HIT, an object is first static relative to the scene, then is held in hand following contact, where its pose changes. This is usually followed by a firm grip during use, before it is released to be static again w.r.t. to the scene. We model these pose constraints over the HIT, and propose to propagate the object's pose along the HIT enabling superior reconstruction using our proposed Constrained Optimisation and Propagation (COP) framework. Importantly, we focus on timelines with stable grasps - i.e. where the hand is stably holding an object, effectively maintaining constant contact during use. This allows us to efficiently annotate, study, and evaluate object reconstruction in videos without 3D ground truth. We evaluate our proposed task, ROHIT, over two egocentric datasets, HOT3D and in-the-wild EPIC-Kitchens. In HOT3D, we curate 1.2K clips of stable grasps. In EPIC-Kitchens, we annotate 2.4K clips of stable grasps including 390 object instances across 9 categories from videos of daily interactions in 141 environments. Without 3D ground truth, we utilise 2D projection error to assess the reconstruction. Quantitatively, COP improves stable grasp reconstruction by 6.2-11.3% and HIT reconstruction by up to 24.5% with constrained pose propagation.",
            "headline_zh": "提出ROHIT任务与COP框架，通过手交互时间线约束优化物体姿态传播，提升第一人称视频中物体重建精度。",
            "intro_zh": [
                "核心问题：在第一人称视频中，如何基于手交互时间线（HIT）重建物体姿态，尤其关注稳定抓取阶段。",
                "方法要点：定义HIT并建模姿态约束，提出COP框架进行约束优化与姿态传播，无需3D真值标注。",
                "实验或效果：在HOT3D和EPIC-Kitchens数据集上评估，COP提升稳定抓取重建6.2-11.3%，HIT重建最高达24.5%。"
            ],
            "tags_zh": [
                "第一人称视频",
                "物体重建",
                "手交互时间线",
                "姿态传播",
                "约束优化",
                "稳定抓取"
            ],
            "_index": 136
        },
        {
            "title": "Empirical Results for Adjusting Truncated Backpropagation Through Time while Training Neural Audio Effects",
            "authors": [
                "Yann Bourdin",
                "Pierrick Legrand",
                "Fanny Roche"
            ],
            "arxiv_id": "2512.07393v1",
            "summary": "This paper investigates the optimization of Truncated Backpropagation Through Time (TBPTT) for training neural networks in digital audio effect modeling, with a focus on dynamic range compression. The study evaluates key TBPTT hyperparameters -- sequence number, batch size, and sequence length -- and their influence on model performance. Using a convolutional-recurrent architecture, we conduct extensive experiments across datasets with and without conditionning by user controls. Results demonstrate that carefully tuning these parameters enhances model accuracy and training stability, while also reducing computational demands. Objective evaluations confirm improved performance with optimized settings, while subjective listening tests indicate that the revised TBPTT configuration maintains high perceptual quality.",
            "headline_zh": "优化截断时间反向传播以提升神经音频效果建模性能",
            "intro_zh": [
                "研究截断时间反向传播在神经音频效果训练中的优化问题，聚焦动态范围压缩场景",
                "评估序列数、批次大小和序列长度等超参数对模型性能的影响，采用卷积-循环架构",
                "实验表明调优参数可提高准确性和训练稳定性，同时降低计算成本，客观和主观评估均验证改进"
            ],
            "tags_zh": [
                "截断时间反向传播",
                "神经音频效果",
                "动态范围压缩",
                "超参数优化",
                "卷积-循环网络",
                "音频建模"
            ],
            "_index": 137
        },
        {
            "title": "GlimmerNet: A Lightweight Grouped Dilated Depthwise Convolutions for UAV-Based Emergency Monitoring",
            "authors": [
                "Đorđe Nedeljković"
            ],
            "arxiv_id": "2512.07391v1",
            "summary": "Convolutional Neural Networks (CNNs) have proven highly effective for edge and mobile vision tasks due to their computational efficiency. While many recent works seek to enhance CNNs with global contextual understanding via self-attention-based Vision Transformers, these approaches often introduce significant computational overhead. In this work, we demonstrate that it is possible to retain strong global perception without relying on computationally expensive components. We present GlimmerNet, an ultra-lightweight convolutional network built on the principle of separating receptive field diversity from feature recombination. GlimmerNet introduces Grouped Dilated Depthwise Convolutions(GDBlocks), which partition channels into groups with distinct dilation rates, enabling multi-scale feature extraction at no additional parameter cost. To fuse these features efficiently, we design a novel Aggregator module that recombines cross-group representations using grouped pointwise convolution, significantly lowering parameter overhead. With just 31K parameters and 29% fewer FLOPs than the most recent baseline, GlimmerNet achieves a new state-of-the-art weighted F1-score of 0.966 on the UAV-focused AIDERv2 dataset. These results establish a new accuracy-efficiency trade-off frontier for real-time emergency monitoring on resource-constrained UAV platforms. Our implementation is publicly available at https://github.com/djordjened92/gdd-cnn.",
            "headline_zh": "提出GlimmerNet，一种基于分组扩张深度卷积的轻量网络，用于无人机应急监测。",
            "intro_zh": [
                "核心问题：现有视觉Transformer引入计算开销，难以在资源受限无人机上实现高效全局感知。",
                "方法要点：设计分组扩张深度卷积块，分离感受野多样性与特征重组，以零参数成本提取多尺度特征。",
                "实验或效果：在AIDERv2数据集上，仅31K参数，FLOPs减少29%，加权F1-score达0.966，创下新SOTA。"
            ],
            "tags_zh": [
                "轻量卷积网络",
                "无人机监测",
                "分组扩张卷积",
                "多尺度特征提取",
                "实时计算优化"
            ],
            "_index": 138
        },
        {
            "title": "Towards Reliable Test-Time Adaptation: Style Invariance as a Correctness Likelihood",
            "authors": [
                "Gilhyun Nam",
                "Taewon Kim",
                "Joonhyun Jeong",
                "Eunho Yang"
            ],
            "arxiv_id": "2512.07390v1",
            "summary": "Test-time adaptation (TTA) enables efficient adaptation of deployed models, yet it often leads to poorly calibrated predictive uncertainty - a critical issue in high-stakes domains such as autonomous driving, finance, and healthcare. Existing calibration methods typically assume fixed models or static distributions, resulting in degraded performance under real-world, dynamic test conditions. To address these challenges, we introduce Style Invariance as a Correctness Likelihood (SICL), a framework that leverages style-invariance for robust uncertainty estimation. SICL estimates instance-wise correctness likelihood by measuring prediction consistency across style-altered variants, requiring only the model's forward pass. This makes it a plug-and-play, backpropagation-free calibration module compatible with any TTA method. Comprehensive evaluations across four baselines, five TTA methods, and two realistic scenarios with three model architecture demonstrate that SICL reduces calibration error by an average of 13 percentage points compared to conventional calibration approaches.",
            "headline_zh": "提出SICL框架，利用风格不变性提升测试时自适应中的不确定性校准可靠性。",
            "intro_zh": [
                "核心问题：测试时自适应导致预测不确定性校准不佳，在动态测试条件下性能下降。",
                "方法要点：通过测量风格变换变体间的预测一致性，估计实例级正确性似然，无需反向传播。",
                "实验或效果：在多种基线、TTA方法和场景下，平均降低校准误差13个百分点。"
            ],
            "tags_zh": [
                "测试时自适应",
                "不确定性校准",
                "风格不变性",
                "预测一致性",
                "实例级估计"
            ],
            "_index": 139
        },
        {
            "title": "How Far are Modern Trackers from UAV-Anti-UAV? A Million-Scale Benchmark and New Baseline",
            "authors": [
                "Chunhui Zhang",
                "Li Liu",
                "Zhipeng Zhang",
                "Yong Wang",
                "Hao Wen",
                "Xi Zhou",
                "Shiming Ge",
                "Yanfeng Wang"
            ],
            "arxiv_id": "2512.07385v1",
            "summary": "Unmanned Aerial Vehicles (UAVs) offer wide-ranging applications but also pose significant safety and privacy violation risks in areas like airport and infrastructure inspection, spurring the rapid development of Anti-UAV technologies in recent years. However, current Anti-UAV research primarily focuses on RGB, infrared (IR), or RGB-IR videos captured by fixed ground cameras, with little attention to tracking target UAVs from another moving UAV platform. To fill this gap, we propose a new multi-modal visual tracking task termed UAV-Anti-UAV, which involves a pursuer UAV tracking a target adversarial UAV in the video stream. Compared to existing Anti-UAV tasks, UAV-Anti-UAV is more challenging due to severe dual-dynamic disturbances caused by the rapid motion of both the capturing platform and the target. To advance research in this domain, we construct a million-scale dataset consisting of 1,810 videos, each manually annotated with bounding boxes, a language prompt, and 15 tracking attributes. Furthermore, we propose MambaSTS, a Mamba-based baseline method for UAV-Anti-UAV tracking, which enables integrated spatial-temporal-semantic learning. Specifically, we employ Mamba and Transformer models to learn global semantic and spatial features, respectively, and leverage the state space model's strength in long-sequence modeling to establish video-level long-term context via a temporal token propagation mechanism. We conduct experiments on the UAV-Anti-UAV dataset to validate the effectiveness of our method. A thorough experimental evaluation of 50 modern deep tracking algorithms demonstrates that there is still significant room for improvement in the UAV-Anti-UAV domain. The dataset and codes will be available at {\\color{magenta}https://github.com/983632847/Awesome-Multimodal-Object-Tracking}.",
            "headline_zh": "提出UAV-Anti-UAV任务与MambaSTS基线，以解决移动无人机平台追踪目标无人机的挑战。",
            "intro_zh": [
                "核心问题：现有反无人机研究忽视移动平台追踪，UAV-Anti-UAV任务面临双动态干扰。",
                "方法要点：构建百万规模数据集，提出MambaSTS方法，集成时空语义学习。",
                "实验或效果：评估50种现代跟踪算法，显示该领域有显著改进空间，验证MambaSTS有效性。"
            ],
            "tags_zh": [
                "无人机追踪",
                "多模态视觉跟踪",
                "时空语义学习",
                "Mamba模型",
                "长序列建模",
                "基准数据集"
            ],
            "_index": 140
        },
        {
            "title": "LogicCBMs: Logic-Enhanced Concept-Based Learning",
            "authors": [
                "Deepika SN Vemuri",
                "Gautham Bellamkonda",
                "Aditya Pola",
                "Vineeth N Balasubramanian"
            ],
            "arxiv_id": "2512.07383v1",
            "summary": "Concept Bottleneck Models (CBMs) provide a basis for semantic abstractions within a neural network architecture. Such models have primarily been seen through the lens of interpretability so far, wherein they offer transparency by inferring predictions as a linear combination of semantic concepts. However, a linear combination is inherently limiting. So we propose the enhancement of concept-based learning models through propositional logic. We introduce a logic module that is carefully designed to connect the learned concepts from CBMs through differentiable logic operations, such that our proposed LogicCBM can go beyond simple weighted combinations of concepts to leverage various logical operations to yield the final predictions, while maintaining end-to-end learnability. Composing concepts using a set of logic operators enables the model to capture inter-concept relations, while simultaneously improving the expressivity of the model in terms of logic operations. Our empirical studies on well-known benchmarks and synthetic datasets demonstrate that these models have better accuracy, perform effective interventions and are highly interpretable.",
            "headline_zh": "提出LogicCBM以增强概念瓶颈模型，通过逻辑模块提升表达力与可解释性。",
            "intro_zh": [
                "概念瓶颈模型线性组合概念限制表达力，需超越简单加权。",
                "引入可微分逻辑模块连接概念，支持逻辑操作以捕获概念间关系。",
                "实验表明模型在基准数据集上提高准确性，并保持端到端可学习性。"
            ],
            "tags_zh": [
                "概念瓶颈模型",
                "逻辑增强学习",
                "可解释人工智能",
                "可微分逻辑",
                "概念关系建模"
            ],
            "_index": 141
        },
        {
            "title": "Tessellation GS: Neural Mesh Gaussians for Robust Monocular Reconstruction of Dynamic Objects",
            "authors": [
                "Shuohan Tao",
                "Boyao Zhou",
                "Hanzhang Tu",
                "Yuwang Wang",
                "Yebin Liu"
            ],
            "arxiv_id": "2512.07381v1",
            "summary": "3D Gaussian Splatting (GS) enables highly photorealistic scene reconstruction from posed image sequences but struggles with viewpoint extrapolation due to its anisotropic nature, leading to overfitting and poor generalization, particularly in sparse-view and dynamic scene reconstruction. We propose Tessellation GS, a structured 2D GS approach anchored on mesh faces, to reconstruct dynamic scenes from a single continuously moving or static camera. Our method constrains 2D Gaussians to localized regions and infers their attributes via hierarchical neural features on mesh faces. Gaussian subdivision is guided by an adaptive face subdivision strategy driven by a detail-aware loss function. Additionally, we leverage priors from a reconstruction foundation model to initialize Gaussian deformations, enabling robust reconstruction of general dynamic objects from a single static camera, previously extremely challenging for optimization-based methods. Our method outperforms previous SOTA method, reducing LPIPS by 29.1% and Chamfer distance by 49.2% on appearance and mesh reconstruction tasks.",
            "headline_zh": "提出Tessellation GS，基于网格面约束2D高斯以从单相机稳健重建动态物体",
            "intro_zh": [
                "核心问题：3D高斯溅射在稀疏视图和动态场景中因各向异性导致过拟合和泛化差",
                "方法要点：将2D高斯锚定在网格面上，通过自适应面细分和神经特征推断属性",
                "实验或效果：在单静态相机下，外观和网格重建任务中LPIPS降低29.1%，Chamfer距离减少49.2%"
            ],
            "tags_zh": [
                "动态场景重建",
                "高斯溅射",
                "单相机重建",
                "网格约束",
                "自适应细分"
            ],
            "_index": 142
        },
        {
            "title": "Enhancing Small Object Detection with YOLO: A Novel Framework for Improved Accuracy and Efficiency",
            "authors": [
                "Mahila Moghadami",
                "Mohammad Ali Keyvanrad",
                "Melika Sabaghian"
            ],
            "arxiv_id": "2512.07379v1",
            "summary": "This paper investigates and develops methods for detecting small objects in large-scale aerial images. Current approaches for detecting small objects in aerial images often involve image cropping and modifications to detector network architectures. Techniques such as sliding window cropping and architectural enhancements, including higher-resolution feature maps and attention mechanisms, are commonly employed. Given the growing importance of aerial imagery in various critical and industrial applications, the need for robust frameworks for small object detection becomes imperative. To address this need, we adopted the base SW-YOLO approach to enhance speed and accuracy in small object detection by refining cropping dimensions and overlap in sliding window usage and subsequently enhanced it through architectural modifications. we propose a novel model by modifying the base model architecture, including advanced feature extraction modules in the neck for feature map enhancement, integrating CBAM in the backbone to preserve spatial and channel information, and introducing a new head to boost small object detection accuracy. Finally, we compared our method with SAHI, one of the most powerful frameworks for processing large-scale images, and CZDet, which is also based on image cropping, achieving significant improvements in accuracy. The proposed model achieves significant accuracy gains on the VisDrone2019 dataset, outperforming baseline YOLOv5L detection by a substantial margin. Specifically, the final proposed model elevates the mAP .5.5 accuracy on the VisDrone2019 dataset from the base accuracy of 35.5 achieved by the YOLOv5L detector to 61.2. Notably, the accuracy of CZDet, which is another classic method applied to this dataset, is 58.36. This research demonstrates a significant improvement, achieving an increase in accuracy from 35.5 to 61.2.",
            "headline_zh": "提出基于YOLO的增强框架，通过裁剪优化与架构改进提升航拍图像中小目标检测的准确率与效率",
            "intro_zh": [
                "核心问题：航拍图像中小目标检测准确率低，现有方法依赖图像裁剪和网络架构调整",
                "方法要点：改进SW-YOLO的裁剪策略，在骨干网络集成CBAM，并设计新头部以增强特征提取",
                "实验或效果：在VisDrone2019数据集上，mAP .5.5从YOLOv5L的35.5提升至61.2，优于SAHI和CZDet"
            ],
            "tags_zh": [
                "小目标检测",
                "航拍图像",
                "YOLO框架",
                "特征增强",
                "滑动窗口裁剪",
                "注意力机制"
            ],
            "_index": 143
        },
        {
            "title": "LUNE: Efficient LLM Unlearning via LoRA Fine-Tuning with Negative Examples",
            "authors": [
                "Yezi Liu",
                "Hanning Chen",
                "Wenjun Huang",
                "Yang Ni",
                "Mohsen Imani"
            ],
            "arxiv_id": "2512.07375v1",
            "summary": "Large language models (LLMs) possess vast knowledge acquired from extensive training corpora, but they often cannot remove specific pieces of information when needed, which makes it hard to handle privacy, bias mitigation, and knowledge correction. Traditional model unlearning approaches require computationally expensive fine-tuning or direct weight editing, making them impractical for real-world deployment. In this work, we introduce LoRA-based Unlearning with Negative Examples (LUNE), a lightweight framework that performs negative-only unlearning by updating only low-rank adapters while freezing the backbone, thereby localizing edits and avoiding disruptive global changes. Leveraging Low-Rank Adaptation (LoRA), LUNE targets intermediate representations to suppress (or replace) requested knowledge with an order-of-magnitude lower compute and memory than full fine-tuning or direct weight editing. Extensive experiments on multiple factual unlearning tasks show that LUNE: (I) achieves effectiveness comparable to full fine-tuning and memory-editing methods, and (II) reduces computational cost by about an order of magnitude.",
            "headline_zh": "提出LUNE框架，通过LoRA微调与负例实现高效LLM遗忘，以解决隐私、偏见和知识修正问题。",
            "intro_zh": [
                "核心问题：LLM难以移除特定信息，传统遗忘方法计算成本高，不适用于实际部署。",
                "方法要点：基于LoRA的轻量级框架，仅更新低秩适配器，冻结主干，通过负例进行遗忘，定位编辑并避免全局变化。",
                "实验或效果：在多项事实遗忘任务中，效果与全微调和记忆编辑方法相当，计算成本降低约一个数量级。"
            ],
            "tags_zh": [
                "大语言模型遗忘",
                "LoRA微调",
                "负例学习",
                "计算效率",
                "隐私保护",
                "知识修正"
            ],
            "_index": 144
        },
        {
            "title": "Recover-to-Forget: Gradient Reconstruction from LoRA for Efficient LLM Unlearning",
            "authors": [
                "Yezi Liu",
                "Hanning Chen",
                "Wenjun Huang",
                "Yang Ni",
                "Mohsen Imani"
            ],
            "arxiv_id": "2512.07374v1",
            "summary": "Unlearning in large foundation models (e.g., LLMs) is essential for enabling dynamic knowledge updates, enforcing data deletion rights, and correcting model behavior. However, existing unlearning methods often require full-model fine-tuning or access to the original training data, which limits their scalability and practicality. In this work, we introduce Recover-to-Forget (R2F), a novel framework for efficient unlearning in LLMs based on reconstructing full-model gradient directions from low-rank LoRA adapter updates. Rather than performing backpropagation through the full model, we compute gradients with respect to LoRA parameters using multiple paraphrased prompts and train a gradient decoder to approximate the corresponding full-model gradients. To ensure applicability to larger or black-box models, the decoder is trained on a proxy model and transferred to target models. We provide a theoretical analysis of cross-model generalization and demonstrate that our method achieves effective unlearning while preserving general model performance. Experimental results demonstrate that R2F offers a scalable and lightweight alternative for unlearning in pretrained LLMs without requiring full retraining or access to internal parameters.",
            "headline_zh": "提出R2F框架，通过从LoRA重建梯度实现高效LLM遗忘学习",
            "intro_zh": [
                "核心问题：现有遗忘学习方法需全模型微调或原始数据，限制可扩展性",
                "方法要点：基于LoRA参数梯度，训练解码器近似全模型梯度方向",
                "实验或效果：在代理模型上训练解码器，可迁移至目标模型，保持性能"
            ],
            "tags_zh": [
                "大语言模型",
                "遗忘学习",
                "LoRA",
                "梯度重建",
                "模型迁移"
            ],
            "_index": 145
        },
        {
            "title": "ESPADA: Execution Speedup via Semantics Aware Demonstration Data Downsampling for Imitation Learning",
            "authors": [
                "Byungju Kim",
                "Jinu Pahk",
                "Chungwoo Lee",
                "Jaejoon Kim",
                "Jangha Lee",
                "Theo Taeyeong Kim",
                "Kyuhwan Shim",
                "Jun Ki Lee",
                "Byoung-Tak Zhang"
            ],
            "arxiv_id": "2512.07371v1",
            "summary": "Behavior-cloning based visuomotor policies enable precise manipulation but often inherit the slow, cautious tempo of human demonstrations, limiting practical deployment. However, prior studies on acceleration methods mainly rely on statistical or heuristic cues that ignore task semantics and can fail across diverse manipulation settings. We present ESPADA, a semantic and spatially aware framework that segments demonstrations using a VLM-LLM pipeline with 3D gripper-object relations, enabling aggressive downsampling only in non-critical segments while preserving precision-critical phases, without requiring extra data or architectural modifications, or any form of retraining. To scale from a single annotated episode to the full dataset, ESPADA propagates segment labels via Dynamic Time Warping (DTW) on dynamics-only features. Across both simulation and real-world experiments with ACT and DP baselines, ESPADA achieves approximately a 2x speed-up while maintaining success rates, narrowing the gap between human demonstrations and efficient robot control.",
            "headline_zh": "提出ESPADA框架，通过语义感知演示数据下采样加速模仿学习中的机器人控制。",
            "intro_zh": [
                "行为克隆策略继承人类演示的缓慢节奏，限制实际部署。",
                "ESPADA利用VLM-LLM管道和3D夹爪-物体关系进行语义分割，仅在非关键段进行激进下采样。",
                "在模拟和真实实验中，ESPADA实现约2倍加速，同时保持成功率。"
            ],
            "tags_zh": [
                "模仿学习",
                "行为克隆",
                "数据下采样",
                "语义分割",
                "机器人控制",
                "动态时间规整"
            ],
            "_index": 146
        },
        {
            "title": "Structure-Aware Feature Rectification with Region Adjacency Graphs for Training-Free Open-Vocabulary Semantic Segmentation",
            "authors": [
                "Qiming Huang",
                "Hao Ai",
                "Jianbo Jiao"
            ],
            "arxiv_id": "2512.07360v1",
            "summary": "Benefiting from the inductive biases learned from large-scale datasets, open-vocabulary semantic segmentation (OVSS) leverages the power of vision-language models, such as CLIP, to achieve remarkable progress without requiring task-specific training. However, due to CLIP's pre-training nature on image-text pairs, it tends to focus on global semantic alignment, resulting in suboptimal performance when associating fine-grained visual regions with text. This leads to noisy and inconsistent predictions, particularly in local areas. We attribute this to a dispersed bias stemming from its contrastive training paradigm, which is difficult to alleviate using CLIP features alone. To address this, we propose a structure-aware feature rectification approach that incorporates instance-specific priors derived directly from the image. Specifically, we construct a region adjacency graph (RAG) based on low-level features (e.g., colour and texture) to capture local structural relationships and use it to refine CLIP features by enhancing local discrimination. Extensive experiments show that our method effectively suppresses segmentation noise, improves region-level consistency, and achieves strong performance on multiple open-vocabulary segmentation benchmarks.",
            "headline_zh": "提出基于区域邻接图的结构感知特征校正方法，以提升无训练开放词汇语义分割的局部一致性。",
            "intro_zh": [
                "核心问题：CLIP模型在开放词汇语义分割中因全局语义对齐导致局部区域预测噪声和不一致。",
                "方法要点：利用低层特征构建区域邻接图，捕获局部结构关系，校正CLIP特征以增强局部判别力。",
                "实验或效果：在多个开放词汇分割基准上有效抑制噪声，提升区域一致性，实现强性能。"
            ],
            "tags_zh": [
                "开放词汇语义分割",
                "结构感知特征校正",
                "区域邻接图",
                "无训练方法",
                "局部一致性"
            ],
            "_index": 147
        },
        {
            "title": "Multi-Rigid-Body Approximation of Human Hands with Application to Digital Twin",
            "authors": [
                "Bin Zhao",
                "Yiwen Lu",
                "Haohua Zhu",
                "Xiao Li",
                "Sheng Yi"
            ],
            "arxiv_id": "2512.07359v1",
            "summary": "Human hand simulation plays a critical role in digital twin applications, requiring models that balance anatomical fidelity with computational efficiency. We present a complete pipeline for constructing multi-rigid-body approximations of human hands that preserve realistic appearance while enabling real-time physics simulation. Starting from optical motion capture of a specific human hand, we construct a personalized MANO (Multi-Abstracted hand model with Neural Operations) model and convert it to a URDF (Unified Robot Description Format) representation with anatomically consistent joint axes. The key technical challenge is projecting MANO's unconstrained SO(3) joint rotations onto the kinematically constrained joints of the rigid-body model. We derive closed-form solutions for single degree-of-freedom joints and introduce a Baker-Campbell-Hausdorff (BCH)-corrected iterative method for two degree-of-freedom joints that properly handles the non-commutativity of rotations. We validate our approach through digital twin experiments where reinforcement learning policies control the multi-rigid-body hand to replay captured human demonstrations. Quantitative evaluation shows sub-centimeter reconstruction error and successful grasp execution across diverse manipulation tasks.",
            "headline_zh": "提出多刚体手部近似方法，结合MANO与URDF，实现数字孪生中的实时物理模拟。",
            "intro_zh": [
                "核心问题：在数字孪生中平衡手部解剖保真度与计算效率，需处理MANO无约束旋转到刚体约束关节的映射。",
                "方法要点：从运动捕捉构建个性化MANO模型，转换为URDF表示，使用闭式解和BCH校正迭代法处理单/双自由度关节旋转。",
                "实验或效果：通过强化学习控制手部重放演示，验证了亚厘米级重建误差和多样化抓取任务的成功执行。"
            ],
            "tags_zh": [
                "数字孪生",
                "手部模拟",
                "多刚体模型",
                "MANO模型",
                "URDF转换",
                "旋转映射"
            ],
            "_index": 148
        },
        {
            "title": "A Geometric Unification of Concept Learning with Concept Cones",
            "authors": [
                "Alexandre Rocchi--Henry",
                "Thomas Fel",
                "Gianni Franchi"
            ],
            "arxiv_id": "2512.07355v1",
            "summary": "Two traditions of interpretability have evolved side by side but seldom spoken to each other: Concept Bottleneck Models (CBMs), which prescribe what a concept should be, and Sparse Autoencoders (SAEs), which discover what concepts emerge. While CBMs use supervision to align activations with human-labeled concepts, SAEs rely on sparse coding to uncover emergent ones. We show that both paradigms instantiate the same geometric structure: each learns a set of linear directions in activation space whose nonnegative combinations form a concept cone. Supervised and unsupervised methods thus differ not in kind but in how they select this cone. Building on this view, we propose an operational bridge between the two paradigms. CBMs provide human-defined reference geometries, while SAEs can be evaluated by how well their learned cones approximate or contain those of CBMs. This containment framework yields quantitative metrics linking inductive biases -- such as SAE type, sparsity, or expansion ratio -- to emergence of plausible\\footnote{We adopt the terminology of \\citet{jacovi2020towards}, who distinguish between faithful explanations (accurately reflecting model computations) and plausible explanations (aligning with human intuition and domain knowledge). CBM concepts are plausible by construction -- selected or annotated by humans -- though not necessarily faithful to the true latent factors that organise the data manifold.} concepts. Using these metrics, we uncover a ``sweet spot'' in both sparsity and expansion factor that maximizes both geometric and semantic alignment with CBM concepts. Overall, our work unifies supervised and unsupervised concept discovery through a shared geometric framework, providing principled metrics to measure SAE progress and assess how well discovered concept align with plausible human concepts.",
            "headline_zh": "提出概念锥几何框架，统一监督与非监督概念学习，并建立量化评估指标。",
            "intro_zh": [
                "核心问题：监督与非监督概念学习方法（如CBM与SAE）缺乏统一框架与量化评估标准。",
                "方法要点：将CBM与SAE统一为学习激活空间中的线性方向，其非负组合形成概念锥，并基于锥包含关系建立评估指标。",
                "实验或效果：发现稀疏性与扩展因子的“甜点”，最大化与CBM概念的几何和语义对齐。"
            ],
            "tags_zh": [
                "概念学习",
                "几何框架",
                "稀疏自编码器",
                "概念瓶颈模型",
                "可解释性",
                "量化评估"
            ],
            "_index": 149
        },
        {
            "title": "DeepAgent: A Dual Stream Multi Agent Fusion for Robust Multimodal Deepfake Detection",
            "authors": [
                "Sayeem Been Zaman",
                "Wasimul Karim",
                "Arefin Ittesafun Abian",
                "Reem E. Mohamed",
                "Md Rafiqul Islam",
                "Asif Karim",
                "Sami Azam"
            ],
            "arxiv_id": "2512.07351v1",
            "summary": "The increasing use of synthetic media, particularly deepfakes, is an emerging challenge for digital content verification. Although recent studies use both audio and visual information, most integrate these cues within a single model, which remains vulnerable to modality mismatches, noise, and manipulation. To address this gap, we propose DeepAgent, an advanced multi-agent collaboration framework that simultaneously incorporates both visual and audio modalities for the effective detection of deepfakes. DeepAgent consists of two complementary agents. Agent-1 examines each video with a streamlined AlexNet-based CNN to identify the symbols of deepfake manipulation, while Agent-2 detects audio-visual inconsistencies by combining acoustic features, audio transcriptions from Whisper, and frame-reading sequences of images through EasyOCR. Their decisions are fused through a Random Forest meta-classifier that improves final performance by taking advantage of the different decision boundaries learned by each agent. This study evaluates the proposed framework using three benchmark datasets to demonstrate both component-level and fused performance. Agent-1 achieves a test accuracy of 94.35% on the combined Celeb-DF and FakeAVCeleb datasets. On the FakeAVCeleb dataset, Agent-2 and the final meta-classifier attain accuracies of 93.69% and 81.56%, respectively. In addition, cross-dataset validation on DeepFakeTIMIT confirms the robustness of the meta-classifier, which achieves a final accuracy of 97.49%, and indicates a strong capability across diverse datasets. These findings confirm that hierarchy-based fusion enhances robustness by mitigating the weaknesses of individual modalities and demonstrate the effectiveness of a multi-agent approach in addressing diverse types of manipulations in deepfakes.",
            "headline_zh": "提出DeepAgent多智能体融合框架，通过双流协作提升多模态深度伪造检测的鲁棒性。",
            "intro_zh": [
                "核心问题：现有单模型多模态深度伪造检测易受模态不匹配、噪声和操纵影响，鲁棒性不足。",
                "方法要点：设计双智能体协作框架，Agent-1基于AlexNet检测视觉伪造痕迹，Agent-2结合音频特征和OCR检测视听不一致性，通过随机森林元分类器融合决策。",
                "实验或效果：在多个基准数据集上验证，元分类器在DeepFakeTIMIT上达到97.49%准确率，显示跨数据集鲁棒性。"
            ],
            "tags_zh": [
                "多模态深度伪造检测",
                "多智能体协作",
                "视听不一致性检测",
                "随机森林融合",
                "跨数据集验证",
                "鲁棒性增强"
            ],
            "_index": 150
        },
        {
            "title": "MICo-150K: A Comprehensive Dataset Advancing Multi-Image Composition",
            "authors": [
                "Xinyu Wei",
                "Kangrui Cen",
                "Hongyang Wei",
                "Zhen Guo",
                "Bairui Li",
                "Zeqing Wang",
                "Jinrui Zhang",
                "Lei Zhang"
            ],
            "arxiv_id": "2512.07348v1",
            "summary": "In controllable image generation, synthesizing coherent and consistent images from multiple reference inputs, i.e., Multi-Image Composition (MICo), remains a challenging problem, partly hindered by the lack of high-quality training data. To bridge this gap, we conduct a systematic study of MICo, categorizing it into 7 representative tasks and curate a large-scale collection of high-quality source images and construct diverse MICo prompts. Leveraging powerful proprietary models, we synthesize a rich amount of balanced composite images, followed by human-in-the-loop filtering and refinement, resulting in MICo-150K, a comprehensive dataset for MICo with identity consistency. We further build a Decomposition-and-Recomposition (De&Re) subset, where 11K real-world complex images are decomposed into components and recomposed, enabling both real and synthetic compositions. To enable comprehensive evaluation, we construct MICo-Bench with 100 cases per task and 300 challenging De&Re cases, and further introduce a new metric, Weighted-Ref-VIEScore, specifically tailored for MICo evaluation. Finally, we fine-tune multiple models on MICo-150K and evaluate them on MICo-Bench. The results show that MICo-150K effectively equips models without MICo capability and further enhances those with existing skills. Notably, our baseline model, Qwen-MICo, fine-tuned from Qwen-Image-Edit, matches Qwen-Image-2509 in 3-image composition while supporting arbitrary multi-image inputs beyond the latter's limitation. Our dataset, benchmark, and baseline collectively offer valuable resources for further research on Multi-Image Composition.",
            "headline_zh": "提出MICo-150K数据集以解决多图像组合中高质量训练数据缺乏的问题",
            "intro_zh": [
                "核心问题：多图像组合任务因缺乏高质量数据而受限，阻碍可控图像生成发展",
                "方法要点：构建包含7类任务的大规模数据集，通过合成与人工过滤确保身份一致性",
                "实验或效果：微调模型在基准测试中提升性能，基线模型支持任意多图像输入"
            ],
            "tags_zh": [
                "多图像组合",
                "可控图像生成",
                "数据集构建",
                "身份一致性",
                "基准测试",
                "模型微调"
            ],
            "_index": 151
        },
        {
            "title": "Debiasing Diffusion Priors via 3D Attention for Consistent Gaussian Splatting",
            "authors": [
                "Shilong Jin",
                "Haoran Duan",
                "Litao Hua",
                "Wentao Huang",
                "Yuan Zhou"
            ],
            "arxiv_id": "2512.07345v1",
            "summary": "Versatile 3D tasks (e.g., generation or editing) that distill from Text-to-Image (T2I) diffusion models have attracted significant research interest for not relying on extensive 3D training data. However, T2I models exhibit limitations resulting from prior view bias, which produces conflicting appearances between different views of an object. This bias causes subject-words to preferentially activate prior view features during cross-attention (CA) computation, regardless of the target view condition. To overcome this limitation, we conduct a comprehensive mathematical analysis to reveal the root cause of the prior view bias in T2I models. Moreover, we find different UNet layers show different effects of prior view in CA. Therefore, we propose a novel framework, TD-Attn, which addresses multi-view inconsistency via two key components: (1) the 3D-Aware Attention Guidance Module (3D-AAG) constructs a view-consistent 3D attention Gaussian for subject-words to enforce spatial consistency across attention-focused regions, thereby compensating for the limited spatial information in 2D individual view CA maps; (2) the Hierarchical Attention Modulation Module (HAM) utilizes a Semantic Guidance Tree (SGT) to direct the Semantic Response Profiler (SRP) in localizing and modulating CA layers that are highly responsive to view conditions, where the enhanced CA maps further support the construction of more consistent 3D attention Gaussians. Notably, HAM facilitates semantic-specific interventions, enabling controllable and precise 3D editing. Extensive experiments firmly establish that TD-Attn has the potential to serve as a universal plugin, significantly enhancing multi-view consistency across 3D tasks.",
            "headline_zh": "提出TD-Attn框架，通过3D注意力机制解决文本到图像扩散模型在3D任务中的多视角不一致问题。",
            "intro_zh": [
                "核心问题：文本到图像扩散模型存在先验视角偏差，导致3D任务中不同视角外观冲突。",
                "方法要点：引入3D-AAG模块构建视图一致的3D注意力高斯，HAM模块通过语义引导树调制跨注意力层。",
                "实验或效果：TD-Attn作为通用插件，显著提升3D任务的多视角一致性，支持可控编辑。"
            ],
            "tags_zh": [
                "3D生成",
                "扩散模型",
                "多视角一致性",
                "注意力机制",
                "文本到图像",
                "高斯溅射"
            ],
            "_index": 152
        },
        {
            "title": "Venus: An Efficient Edge Memory-and-Retrieval System for VLM-based Online Video Understanding",
            "authors": [
                "Shengyuan Ye",
                "Bei Ouyang",
                "Tianyi Qian",
                "Liekang Zeng",
                "Mu Yuan",
                "Xiaowen Chu",
                "Weijie Hong",
                "Xu Chen"
            ],
            "arxiv_id": "2512.07344v1",
            "summary": "Vision-language models (VLMs) have demonstrated impressive multimodal comprehension capabilities and are being deployed in an increasing number of online video understanding applications. While recent efforts extensively explore advancing VLMs' reasoning power in these cases, deployment constraints are overlooked, leading to overwhelming system overhead in real-world deployments. To address that, we propose Venus, an on-device memory-and-retrieval system for efficient online video understanding. Venus proposes an edge-cloud disaggregated architecture that sinks memory construction and keyframe retrieval from cloud to edge, operating in two stages. In the ingestion stage, Venus continuously processes streaming edge videos via scene segmentation and clustering, where the selected keyframes are embedded with a multimodal embedding model to build a hierarchical memory for efficient storage and retrieval. In the querying stage, Venus indexes incoming queries from memory, and employs a threshold-based progressive sampling algorithm for keyframe selection that enhances diversity and adaptively balances system cost and reasoning accuracy. Our extensive evaluation shows that Venus achieves a 15x-131x speedup in total response latency compared to state-of-the-art methods, enabling real-time responses within seconds while maintaining comparable or even superior reasoning accuracy.",
            "headline_zh": "提出Venus边缘内存检索系统，以解决VLM在线视频理解中的部署开销问题。",
            "intro_zh": [
                "核心问题：VLM在线视频理解部署时系统开销大，忽略实际约束。",
                "方法要点：采用边缘-云分离架构，通过场景分割、聚类和渐进采样实现高效内存构建与检索。",
                "实验或效果：相比现有方法，总响应延迟加速15-131倍，保持或提升推理精度。"
            ],
            "tags_zh": [
                "在线视频理解",
                "边缘计算",
                "内存检索系统",
                "视觉语言模型",
                "渐进采样"
            ],
            "_index": 153
        },
        {
            "title": "PrivORL: Differentially Private Synthetic Dataset for Offline Reinforcement Learning",
            "authors": [
                "Chen Gong",
                "Zheng Liu",
                "Kecen Li",
                "Tianhao Wang"
            ],
            "arxiv_id": "2512.07342v1",
            "summary": "Recently, offline reinforcement learning (RL) has become a popular RL paradigm. In offline RL, data providers share pre-collected datasets -- either as individual transitions or sequences of transitions forming trajectories -- to enable the training of RL models (also called agents) without direct interaction with the environments. Offline RL saves interactions with environments compared to traditional RL, and has been effective in critical areas, such as navigation tasks. Meanwhile, concerns about privacy leakage from offline RL datasets have emerged.\n  To safeguard private information in offline RL datasets, we propose the first differential privacy (DP) offline dataset synthesis method, PrivORL, which leverages a diffusion model and diffusion transformer to synthesize transitions and trajectories, respectively, under DP. The synthetic dataset can then be securely released for downstream analysis and research. PrivORL adopts the popular approach of pre-training a synthesizer on public datasets, and then fine-tuning on sensitive datasets using DP Stochastic Gradient Descent (DP-SGD). Additionally, PrivORL introduces curiosity-driven pre-training, which uses feedback from the curiosity module to diversify the synthetic dataset and thus can generate diverse synthetic transitions and trajectories that closely resemble the sensitive dataset. Extensive experiments on five sensitive offline RL datasets show that our method achieves better utility and fidelity in both DP transition and trajectory synthesis compared to baselines. The replication package is available at the GitHub repository.",
            "headline_zh": "提出PrivORL方法，利用差分隐私合成离线强化学习数据集以保护隐私。",
            "intro_zh": [
                "核心问题：离线强化学习数据集存在隐私泄露风险，需保护敏感信息。",
                "方法要点：基于扩散模型和扩散变换器，在差分隐私下合成过渡和轨迹，并引入好奇心驱动预训练。",
                "实验或效果：在五个敏感数据集上验证，相比基线在效用和保真度方面表现更优。"
            ],
            "tags_zh": [
                "差分隐私",
                "离线强化学习",
                "数据集合成",
                "扩散模型",
                "隐私保护"
            ],
            "_index": 154
        },
        {
            "title": "Generalized Referring Expression Segmentation on Aerial Photos",
            "authors": [
                "Luís Marnoto",
                "Alexandre Bernardino",
                "Bruno Martins"
            ],
            "arxiv_id": "2512.07338v1",
            "summary": "Referring expression segmentation is a fundamental task in computer vision that integrates natural language understanding with precise visual localization of target regions. Considering aerial imagery (e.g., modern aerial photos collected through drones, historical photos from aerial archives, high-resolution satellite imagery, etc.) presents unique challenges because spatial resolution varies widely across datasets, the use of color is not consistent, targets often shrink to only a few pixels, and scenes contain very high object densities and objects with partial occlusions. This work presents Aerial-D, a new large-scale referring expression segmentation dataset for aerial imagery, comprising 37,288 images with 1,522,523 referring expressions that cover 259,709 annotated targets, spanning across individual object instances, groups of instances, and semantic regions covering 21 distinct classes that range from vehicles and infrastructure to land coverage types. The dataset was constructed through a fully automatic pipeline that combines systematic rule-based expression generation with a Large Language Model (LLM) enhancement procedure that enriched both the linguistic variety and the focus on visual details within the referring expressions. Filters were additionally used to simulate historic imaging conditions for each scene. We adopted the RSRefSeg architecture, and trained models on Aerial-D together with prior aerial datasets, yielding unified instance and semantic segmentation from text for both modern and historical images. Results show that the combined training achieves competitive performance on contemporary benchmarks, while maintaining strong accuracy under monochrome, sepia, and grainy degradations that appear in archival aerial photography. The dataset, trained models, and complete software pipeline are publicly available at https://luispl77.github.io/aerial-d .",
            "headline_zh": "提出Aerial-D数据集与RSRefSeg架构，以解决航空影像中的指代表达分割挑战。",
            "intro_zh": [
                "核心问题：航空影像分辨率多变、色彩不一致、目标小且密集，导致指代表达分割困难。",
                "方法要点：构建大规模Aerial-D数据集，结合规则生成与LLM增强，并采用RSRefSeg架构进行训练。",
                "实验或效果：在当代基准上表现竞争性，对历史影像的单色、褪色和颗粒退化保持高精度。"
            ],
            "tags_zh": [
                "指代表达分割",
                "航空影像",
                "大规模数据集",
                "LLM增强",
                "语义分割",
                "历史影像处理"
            ],
            "_index": 155
        },
        {
            "title": "Machine learning in an expectation-maximisation framework for nowcasting",
            "authors": [
                "Paul Wilsens",
                "Katrien Antonio",
                "Gerda Claeskens"
            ],
            "arxiv_id": "2512.07335v1",
            "summary": "Decision making often occurs in the presence of incomplete information, leading to the under- or overestimation of risk. Leveraging the observable information to learn the complete information is called nowcasting. In practice, incomplete information is often a consequence of reporting or observation delays. In this paper, we propose an expectation-maximisation (EM) framework for nowcasting that uses machine learning techniques to model both the occurrence as well as the reporting process of events. We allow for the inclusion of covariate information specific to the occurrence and reporting periods as well as characteristics related to the entity for which events occurred. We demonstrate how the maximisation step and the information flow between EM iterations can be tailored to leverage the predictive power of neural networks and (extreme) gradient boosting machines (XGBoost). With simulation experiments, we show that we can effectively model both the occurrence and reporting of events when dealing with high-dimensional covariate information. In the presence of non-linear effects, we show that our methodology outperforms existing EM-based nowcasting frameworks that use generalised linear models in the maximisation step. Finally, we apply the framework to the reporting of Argentinian Covid-19 cases, where the XGBoost-based approach again is most performant.",
            "headline_zh": "提出基于期望最大化框架的机器学习方法，用于处理事件报告延迟的实时预测问题。",
            "intro_zh": [
                "核心问题：决策中信息不完整导致风险估计偏差，源于事件发生与报告过程的延迟。",
                "方法要点：在EM框架中集成机器学习模型，如神经网络和XGBoost，以建模高维协变量和非线性效应。",
                "实验或效果：模拟实验验证有效性，应用于阿根廷新冠病例报告，XGBoost方法表现最优。"
            ],
            "tags_zh": [
                "实时预测",
                "期望最大化框架",
                "机器学习建模",
                "事件报告延迟",
                "高维协变量",
                "XGBoost"
            ],
            "_index": 156
        },
        {
            "title": "Local-Curvature-Aware Knowledge Graph Embedding: An Extended Ricci Flow Approach",
            "authors": [
                "Zhengquan Luo",
                "Guy Tadmor",
                "Or Amar",
                "David Zeevi",
                "Zhiqiang Xu"
            ],
            "arxiv_id": "2512.07332v1",
            "summary": "Knowledge graph embedding (KGE) relies on the geometry of the embedding space to encode semantic and structural relations. Existing methods place all entities on one homogeneous manifold, Euclidean, spherical, hyperbolic, or their product/multi-curvature variants, to model linear, symmetric, or hierarchical patterns. Yet a predefined, homogeneous manifold cannot accommodate the sharply varying curvature that real-world graphs exhibit across local regions. Since this geometry is imposed a priori, any mismatch with the knowledge graph's local curvatures will distort distances between entities and hurt the expressiveness of the resulting KGE. To rectify this, we propose RicciKGE to have the KGE loss gradient coupled with local curvatures in an extended Ricci flow such that entity embeddings co-evolve dynamically with the underlying manifold geometry towards mutual adaptation. Theoretically, when the coupling coefficient is bounded and properly selected, we rigorously prove that i) all the edge-wise curvatures decay exponentially, meaning that the manifold is driven toward the Euclidean flatness; and ii) the KGE distances strictly converge to a global optimum, which indicates that geometric flattening and embedding optimization are promoting each other. Experimental improvements on link prediction and node classification benchmarks demonstrate RicciKGE's effectiveness in adapting to heterogeneous knowledge graph structures.",
            "headline_zh": "提出RicciKGE方法，通过扩展Ricci流耦合局部曲率与嵌入优化，以解决知识图谱嵌入中预定义均匀流形无法适应局部曲率变化的问题。",
            "intro_zh": [
                "核心问题：预定义均匀流形（如欧几里得、双曲）无法适应知识图谱局部曲率的剧烈变化，导致嵌入失真和表达能力下降。",
                "方法要点：将KGE损失梯度与局部曲率耦合在扩展Ricci流中，使嵌入与流形几何动态协同演化，实现相互适应。",
                "实验或效果：在链接预测和节点分类基准测试中表现提升，验证了RicciKGE适应异构知识图谱结构的有效性。"
            ],
            "tags_zh": [
                "知识图谱嵌入",
                "局部曲率适应",
                "Ricci流",
                "几何优化",
                "链接预测",
                "节点分类"
            ],
            "_index": 157
        },
        {
            "title": "The Inductive Bottleneck: Data-Driven Emergence of Representational Sparsity in Vision Transformers",
            "authors": [
                "Kanishk Awadhiya"
            ],
            "arxiv_id": "2512.07331v1",
            "summary": "Vision Transformers (ViTs) lack the hierarchical inductive biases inherent to Convolutional Neural Networks (CNNs), theoretically allowing them to maintain high-dimensional representations throughout all layers. However, recent observations suggest ViTs often spontaneously manifest a \"U-shaped\" entropy profile-compressing information in middle layers before expanding it for the final classification. In this work, we demonstrate that this \"Inductive Bottleneck\" is not an architectural artifact, but a data-dependent adaptation. By analyzing the layer-wise Effective Encoding Dimension (EED) of DINO-trained ViTs across datasets of varying compositional complexity (UC Merced, Tiny ImageNet, and CIFAR-100), we show that the depth of the bottleneck correlates strongly with the semantic abstraction required by the task. We find that while texture-heavy datasets preserve high-rank representations throughout, object-centric datasets drive the network to dampen high-frequency information in middle layers, effectively \"learning\" a bottleneck to isolate semantic features.",
            "headline_zh": "揭示视觉Transformer中数据驱动的表示稀疏性，即归纳瓶颈与任务语义抽象相关",
            "intro_zh": [
                "核心问题：视觉Transformer缺乏卷积网络的层次归纳偏置，但常自发形成U形熵分布，中间层压缩信息",
                "方法要点：通过分析DINO训练ViT的层间有效编码维度，探究数据集组成复杂度对表示稀疏性的影响",
                "实验或效果：发现纹理丰富数据集保持高秩表示，而对象中心数据集驱动网络在中间层抑制高频信息，学习语义特征隔离"
            ],
            "tags_zh": [
                "视觉Transformer",
                "表示稀疏性",
                "归纳瓶颈",
                "有效编码维度",
                "数据驱动适应",
                "语义抽象"
            ],
            "_index": 158
        },
        {
            "title": "Two-dimensional RMSD projections for reaction path visualization and validation",
            "authors": [
                "Rohit Goswami"
            ],
            "arxiv_id": "2512.07329v1",
            "summary": "Transition state or minimum energy path finding methods constitute a routine component of the computational chemistry toolkit. Standard analysis involves trajectories conventionally plotted in terms of the relative energy to the initial state against a cumulative displacement variable, or the image number. These dimensional reductions obscure structural rearrangements in high dimensions and may often be trajectory dependent. This precludes the ability to compare optimization trajectories of different methods beyond the number of calculations, time taken, and final saddle geometry. We present a method mapping trajectories onto a two-dimension surface defined by a permutation corrected root mean square deviation from the reactant and product configurations. Energy is represented as an interpolated color-mapped surface constructed from all optimization steps using radial basis functions. This representation highlights optimization trajectories, identifies endpoint basins, and diagnoses convergence concerns invisible in one-dimensional profiles. We validate the framework on a cycloaddition reaction, showing that a machine-learned potential saddle and density functional theory reference lie on comparable energy contours despite geometric displacements.",
            "headline_zh": "提出二维RMSD投影方法以可视化与验证反应路径优化轨迹",
            "intro_zh": [
                "核心问题：传统一维能量-位移图掩盖高维结构重排，难以比较不同优化方法轨迹",
                "方法要点：将轨迹映射到基于反应物和产物构型的二维RMSD表面，用径向基函数插值能量颜色映射",
                "实验或效果：在环加成反应中验证，显示机器学习势能鞍点与密度泛函理论参考位于可比能量等高线"
            ],
            "tags_zh": [
                "反应路径可视化",
                "过渡态搜索",
                "RMSD投影",
                "能量等高线",
                "计算化学分析"
            ],
            "_index": 159
        },
        {
            "title": "ContextAnyone: Context-Aware Diffusion for Character-Consistent Text-to-Video Generation",
            "authors": [
                "Ziyang Mai",
                "Yu-Wing Tai"
            ],
            "arxiv_id": "2512.07328v1",
            "summary": "Text-to-video (T2V) generation has advanced rapidly, yet maintaining consistent character identities across scenes remains a major challenge. Existing personalization methods often focus on facial identity but fail to preserve broader contextual cues such as hairstyle, outfit, and body shape, which are critical for visual coherence. We propose \\textbf{ContextAnyone}, a context-aware diffusion framework that achieves character-consistent video generation from text and a single reference image. Our method jointly reconstructs the reference image and generates new video frames, enabling the model to fully perceive and utilize reference information. Reference information is effectively integrated into a DiT-based diffusion backbone through a novel Emphasize-Attention module that selectively reinforces reference-aware features and prevents identity drift across frames. A dual-guidance loss combines diffusion and reference reconstruction objectives to enhance appearance fidelity, while the proposed Gap-RoPE positional embedding separates reference and video tokens to stabilize temporal modeling. Experiments demonstrate that ContextAnyone outperforms existing reference-to-video methods in identity consistency and visual quality, generating coherent and context-preserving character videos across diverse motions and scenes. Project page: \\href{https://github.com/ziyang1106/ContextAnyone}{https://github.com/ziyang1106/ContextAnyone}.",
            "headline_zh": "提出ContextAnyone框架，通过上下文感知扩散实现基于文本和单参考图像的字符一致视频生成。",
            "intro_zh": [
                "核心问题：现有文本到视频生成方法难以保持跨场景的字符身份一致性，如发型、服装和体型等上下文线索。",
                "方法要点：结合参考图像重建和新帧生成，采用Emphasize-Attention模块和Gap-RoPE位置嵌入，增强参考感知并稳定时序建模。",
                "实验或效果：在身份一致性和视觉质量上优于现有参考到视频方法，生成多样动作和场景下的连贯字符视频。"
            ],
            "tags_zh": [
                "文本到视频生成",
                "字符一致性",
                "上下文感知扩散",
                "参考图像重建",
                "Emphasize-Attention模块",
                "Gap-RoPE位置嵌入"
            ],
            "_index": 160
        },
        {
            "title": "Model Predictive Control for Cooperative Docking Between Autonomous Surface Vehicles with Disturbance Rejection",
            "authors": [
                "Gianpietro Battocletti",
                "Dimitris Boskos",
                "Bart De Schutter"
            ],
            "arxiv_id": "2512.07316v1",
            "summary": "Uncrewed Surface Vehicles (USVs) are a popular and efficient type of marine craft that find application in a large number of water-based tasks. When multiple USVs operate in the same area, they may be required to dock to each other to perform a shared task. Existing approaches for the docking between autonomous USVs generally consider one USV as a stationary target, while the second one is tasked to reach the required docking pose. In this work, we propose a cooperative approach for USV-USV docking, where two USVs work together to dock at an agreed location. We use a centralized Model Predictive Control (MPC) approach to solve the control problem, obtaining feasible trajectories that also guarantee constraint satisfaction. Owing to its model-based nature, this approach allows the rejection of disturbances, inclusive of exogenous inputs, by anticipating their effect on the USVs through the MPC prediction model. This is particularly effective in case of almost-stationary disturbances such as water currents. In simulations, we demonstrate how the proposed approach allows for a faster and more efficient docking with respect to existing approaches.",
            "headline_zh": "提出基于模型预测控制的协同对接方法，以解决无人水面艇在扰动下的高效对接问题。",
            "intro_zh": [
                "核心问题：现有无人水面艇对接方法通常假设一艇静止，另一艇主动接近，缺乏协同性，且难以处理扰动。",
                "方法要点：采用集中式模型预测控制，两艇协同运动至约定位置，通过预测模型实现扰动（如水流）的抑制。",
                "实验或效果：仿真显示，该方法相比现有方法能实现更快、更高效的对接，并保证约束满足。"
            ],
            "tags_zh": [
                "无人水面艇",
                "协同对接",
                "模型预测控制",
                "扰动抑制",
                "集中式控制",
                "仿真验证"
            ],
            "_index": 161
        },
        {
            "title": "M-STAR: Multi-Scale Spatiotemporal Autoregression for Human Mobility Modeling",
            "authors": [
                "Yuxiao Luo",
                "Songming Zhang",
                "Sijie Ruan",
                "Siran Chen",
                "Kang Liu",
                "Yang Xu",
                "Yu Zheng",
                "Ling Yin"
            ],
            "arxiv_id": "2512.07314v1",
            "summary": "Modeling human mobility is vital for extensive applications such as transportation planning and epidemic modeling. With the rise of the Artificial Intelligence Generated Content (AIGC) paradigm, recent works explore synthetic trajectory generation using autoregressive and diffusion models. While these methods show promise for generating single-day trajectories, they remain limited by inefficiencies in long-term generation (e.g., weekly trajectories) and a lack of explicit spatiotemporal multi-scale modeling. This study proposes Multi-Scale Spatio-Temporal AutoRegression (M-STAR), a new framework that generates long-term trajectories through a coarse-to-fine spatiotemporal prediction process. M-STAR combines a Multi-scale Spatiotemporal Tokenizer that encodes hierarchical mobility patterns with a Transformer-based decoder for next-scale autoregressive prediction. Experiments on two real-world datasets show that M-STAR outperforms existing methods in fidelity and significantly improves generation speed. The data and codes are available at https://github.com/YuxiaoLuo0013/M-STAR.",
            "headline_zh": "提出M-STAR框架，通过多尺度时空自回归解决长时轨迹生成效率与建模不足问题。",
            "intro_zh": [
                "核心问题：现有方法在长时轨迹生成中效率低且缺乏显式多尺度时空建模。",
                "方法要点：结合多尺度时空分词器与Transformer解码器，实现从粗到细的自回归预测。",
                "实验或效果：在真实数据集上优于现有方法，提升生成速度与保真度。"
            ],
            "tags_zh": [
                "人类移动建模",
                "多尺度时空预测",
                "自回归生成",
                "轨迹生成",
                "Transformer解码器"
            ],
            "_index": 162
        },
        {
            "title": "Learning-Augmented Ski Rental with Discrete Distributions: A Bayesian Approach",
            "authors": [
                "Bosun Kang",
                "Hyejun Park",
                "Chenglin Fan"
            ],
            "arxiv_id": "2512.07313v1",
            "summary": "We revisit the classic ski rental problem through the lens of Bayesian decision-making and machine-learned predictions. While traditional algorithms minimize worst-case cost without assumptions, and recent learning-augmented approaches leverage noisy forecasts with robustness guarantees, our work unifies these perspectives. We propose a discrete Bayesian framework that maintains exact posterior distributions over the time horizon, enabling principled uncertainty quantification and seamless incorporation of expert priors. Our algorithm achieves prior-dependent competitive guarantees and gracefully interpolates between worst-case and fully-informed settings. Our extensive experimental evaluation demonstrates superior empirical performance across diverse scenarios, achieving near-optimal results under accurate priors while maintaining robust worst-case guarantees. This framework naturally extends to incorporate multiple predictions, non-uniform priors, and contextual information, highlighting the practical advantages of Bayesian reasoning in online decision problems with imperfect predictions.",
            "headline_zh": "提出离散贝叶斯框架以解决带预测的滑雪租赁问题，统一传统与学习增强方法。",
            "intro_zh": [
                "核心问题：滑雪租赁问题中如何结合贝叶斯决策与机器学习预测，量化不确定性并整合先验知识。",
                "方法要点：基于离散分布维护精确后验，实现竞争性保证，并在最坏情况与完全信息间平滑插值。",
                "实验或效果：实验显示在多样场景下性能优越，准确先验下接近最优，同时保持鲁棒最坏情况保证。"
            ],
            "tags_zh": [
                "滑雪租赁问题",
                "贝叶斯决策",
                "学习增强算法",
                "不确定性量化",
                "在线决策",
                "离散分布"
            ],
            "_index": 163
        },
        {
            "title": "DCO: Dynamic Cache Orchestration for LLM Accelerators through Predictive Management",
            "authors": [
                "Zhongchun Zhou",
                "Chengtao Lai",
                "Yuhang Gu",
                "Wei Zhang"
            ],
            "arxiv_id": "2512.07312v1",
            "summary": "The rapid adoption of large language models (LLMs) is pushing AI accelerators toward increasingly powerful and specialized designs. Instead of further complicating software development with deeply hierarchical scratchpad memories (SPMs) and their asynchronous management, we investigate the opposite point of the design spectrum: a multi-core AI accelerator equipped with a shared system-level cache and application-aware management policies, which keeps the programming effort modest. Our approach exploits dataflow information available in the software stack to guide cache replacement (including dead-block prediction), in concert with bypass decisions and mechanisms that alleviate cache thrashing.\n  We assess the proposal using a cycle-accurate simulator and observe substantial performance gains (up to 1.80x speedup) compared with conventional cache architectures. In addition, we build and validate an analytical model that takes into account the actual overlapping behaviors to extend the measurement results of our policies to real-world larger-scale workloads. Experiment results show that when functioning together, our bypassing and thrashing mitigation strategies can handle scenarios both with and without inter-core data sharing and achieve remarkable speedups.\n  Finally, we implement the design in RTL and the area of our design is $\\mathbf{0.064mm^2}$ with 15nm process, which can run at 2 GHz clock frequency. Our findings explore the potential of the shared cache design to assist the development of future AI accelerator systems.",
            "headline_zh": "提出动态缓存编排方法，通过预测管理优化LLM加速器性能",
            "intro_zh": [
                "针对AI加速器中缓存层次复杂化问题，研究共享系统级缓存与软件栈数据流引导的管理策略",
                "结合死块预测、旁路决策和缓存颠簸缓解机制，在周期精确模拟中实现最高1.80倍加速",
                "通过RTL实现验证设计可行性，面积0.064mm²，时钟频率2GHz，支持大规模工作负载扩展"
            ],
            "tags_zh": [
                "AI加速器",
                "缓存管理",
                "数据流预测",
                "性能优化",
                "RTL实现",
                "大语言模型"
            ],
            "_index": 164
        },
        {
            "title": "Towards a Relationship-Aware Transformer for Tabular Data",
            "authors": [
                "Andrei V. Konstantinov",
                "Valerii A. Zuev",
                "Lev V. Utkin"
            ],
            "arxiv_id": "2512.07310v1",
            "summary": "Deep learning models for tabular data typically do not allow for imposing a graph of external dependencies between samples, which can be useful for accounting for relatedness in tasks such as treatment effect estimation. Graph neural networks only consider adjacent nodes, making them difficult to apply to sparse graphs. This paper proposes several solutions based on a modified attention mechanism, which accounts for possible relationships between data points by adding a term to the attention matrix. Our models are compared with each other and the gradient boosting decision trees in a regression task on synthetic and real-world datasets, as well as in a treatment effect estimation task on the IHDP dataset.",
            "headline_zh": "提出关系感知Transformer以解决表格数据中外部依赖图建模问题，适用于处理效应估计等任务。",
            "intro_zh": [
                "核心问题：现有深度学习模型难以在表格数据中融入样本间外部依赖图，如图神经网络仅考虑相邻节点，不适用于稀疏图。",
                "方法要点：基于改进的注意力机制，通过在注意力矩阵中添加项来建模数据点间可能的关系，提出多个解决方案。",
                "实验或效果：在合成和真实数据集上进行回归任务比较，以及在IHDP数据集上进行处理效应估计任务，与梯度提升决策树等模型对比。"
            ],
            "tags_zh": [
                "表格数据建模",
                "关系感知Transformer",
                "注意力机制改进",
                "处理效应估计",
                "稀疏图处理"
            ],
            "_index": 165
        },
        {
            "title": "Radiance-Field Reinforced Pretraining: Scaling Localization Models with Unlabeled Wireless Signals",
            "authors": [
                "Guosheng Wang",
                "Shen Wang",
                "Lei Yang"
            ],
            "arxiv_id": "2512.07309v1",
            "summary": "Radio frequency (RF)-based indoor localization offers significant promise for applications such as indoor navigation, augmented reality, and pervasive computing. While deep learning has greatly enhanced localization accuracy and robustness, existing localization models still face major challenges in cross-scene generalization due to their reliance on scene-specific labeled data. To address this, we introduce Radiance-Field Reinforced Pretraining (RFRP). This novel self-supervised pretraining framework couples a large localization model (LM) with a neural radio-frequency radiance field (RF-NeRF) in an asymmetrical autoencoder architecture. In this design, the LM encodes received RF spectra into latent, position-relevant representations, while the RF-NeRF decodes them to reconstruct the original spectra. This alignment between input and output enables effective representation learning using large-scale, unlabeled RF data, which can be collected continuously with minimal effort. To this end, we collected RF samples at 7,327,321 positions across 100 diverse scenes using four common wireless technologies--RFID, BLE, WiFi, and IIoT. Data from 75 scenes were used for training, and the remaining 25 for evaluation. Experimental results show that the RFRP-pretrained LM reduces localization error by over 40% compared to non-pretrained models and by 21% compared to those pretrained using supervised learning.",
            "headline_zh": "提出Radiance-Field Reinforced Pretraining，利用无标签无线信号提升室内定位模型的跨场景泛化能力。",
            "intro_zh": [
                "核心问题：现有基于深度学习的室内定位模型依赖场景特定标签数据，跨场景泛化能力不足。",
                "方法要点：采用非对称自编码器架构，结合定位模型和神经射频辐射场，通过重构射频谱进行自监督预训练。",
                "实验或效果：在100个场景的大规模数据上验证，预训练模型相比无预训练模型定位误差降低超40%，优于监督预训练。"
            ],
            "tags_zh": [
                "室内定位",
                "自监督学习",
                "射频信号处理",
                "跨场景泛化",
                "神经辐射场"
            ],
            "_index": 166
        },
        {
            "title": "Exact Synthetic Populations for Scalable Societal and Market Modeling",
            "authors": [
                "Thierry Petit",
                "Arnault Pachot"
            ],
            "arxiv_id": "2512.07306v1",
            "summary": "We introduce a constraint-programming framework for generating synthetic populations that reproduce target statistics with high precision while enforcing full individual consistency. Unlike data-driven approaches that infer distributions from samples, our method directly encodes aggregated statistics and structural relations, enabling exact control of demographic profiles without requiring any microdata. We validate the approach on official demographic sources and study the impact of distributional deviations on downstream analyses. This work is conducted within the Pollitics project developed by Emotia, where synthetic populations can be queried through large language models to model societal behaviors, explore market and policy scenarios, and provide reproducible decision-grade insights without personal data.",
            "headline_zh": "提出基于约束编程的合成人口生成框架，以精确控制人口统计特征并支持社会与市场建模。",
            "intro_zh": [
                "核心问题：传统方法依赖样本推断分布，难以确保个体一致性和精确统计匹配。",
                "方法要点：直接编码聚合统计和结构关系，无需微观数据，实现高精度合成人口生成。",
                "实验或效果：在官方人口数据上验证，并分析分布偏差对下游分析的影响。"
            ],
            "tags_zh": [
                "合成人口生成",
                "约束编程",
                "人口统计建模",
                "社会行为模拟",
                "市场场景分析"
            ],
            "_index": 167
        },
        {
            "title": "Reevaluating Automated Wildlife Species Detection: A Reproducibility Study on a Custom Image Dataset",
            "authors": [
                "Tobias Abraham Haider"
            ],
            "arxiv_id": "2512.07305v1",
            "summary": "This study revisits the findings of Carl et al., who evaluated the pre-trained Google Inception-ResNet-v2 model for automated detection of European wild mammal species in camera trap images. To assess the reproducibility and generalizability of their approach, we reimplemented the experiment from scratch using openly available resources and a different dataset consisting of 900 images spanning 90 species. After minimal preprocessing, we obtained an overall classification accuracy of 62%, closely aligning with the 71% reported in the original work despite differences in datasets. As in the original study, per-class performance varied substantially, as indicated by a macro F1 score of 0.28,highlighting limitations in generalization when labels do not align directly with ImageNet classes. Our results confirm that pretrained convolutional neural networks can provide a practical baseline for wildlife species identification but also reinforce the need for species-specific adaptation or transfer learning to achieve consistent, high-quality predictions.",
            "headline_zh": "复现研究评估预训练CNN在野生动物物种检测中的可重复性与泛化性",
            "intro_zh": [
                "核心问题：评估预训练模型在野生动物物种检测中的可重复性和泛化性，尤其当标签与ImageNet类别不直接对齐时。",
                "方法要点：从零开始复现实验，使用公开资源和不同数据集（900张图像，90个物种），进行最小预处理。",
                "实验或效果：整体分类准确率62%，接近原研究71%，但宏F1分数0.28显示类间性能差异大，确认预训练CNN可作为基线但需物种特定适应。"
            ],
            "tags_zh": [
                "野生动物物种检测",
                "预训练卷积神经网络",
                "可重复性研究",
                "泛化性评估",
                "相机陷阱图像",
                "迁移学习"
            ],
            "_index": 168
        },
        {
            "title": "Efficient Computation of a Continuous Topological Model of the Configuration Space of Tethered Mobile Robots",
            "authors": [
                "Gianpietro Battocletti",
                "Dimitris Boskos",
                "Bart De Schutter"
            ],
            "arxiv_id": "2512.07303v1",
            "summary": "Despite the attention that the problem of path planning for tethered robots has garnered in the past few decades, the approaches proposed to solve it typically rely on a discrete representation of the configuration space and do not exploit a model that can simultaneously capture the topological information of the tether and the continuous location of the robot. In this work, we explicitly build a topological model of the configuration space of a tethered robot starting from a polygonal representation of the workspace where the robot moves. To do so, we first establish a link between the configuration space of the tethered robot and the universal covering space of the workspace, and then we exploit this link to develop an algorithm to compute a simplicial complex model of the configuration space. We show how this approach improves the performances of existing algorithms that build other types of representations of the configuration space. The proposed model can be computed in a fraction of the time required to build traditional homotopy-augmented graphs, and is continuous, allowing to solve the path planning task for tethered robots using a broad set of path planning algorithms.",
            "headline_zh": "提出连续拓扑模型以高效计算系绳移动机器人的配置空间",
            "intro_zh": [
                "核心问题：现有路径规划方法依赖离散表示，缺乏同时捕获系绳拓扑和机器人连续位置的模型。",
                "方法要点：基于工作空间多边形表示，建立配置空间与通用覆盖空间的联系，开发算法计算单纯复形模型。",
                "实验或效果：模型计算时间远少于传统同伦增强图，支持多种路径规划算法，提升性能。"
            ],
            "tags_zh": [
                "系绳机器人",
                "配置空间",
                "拓扑模型",
                "路径规划",
                "单纯复形",
                "通用覆盖空间"
            ],
            "_index": 169
        },
        {
            "title": "Towards Accurate UAV Image Perception: Guiding Vision-Language Models with Stronger Task Prompts",
            "authors": [
                "Mingning Guo",
                "Mengwei Wu",
                "Shaoxian Li",
                "Haifeng Li",
                "Chao Tao"
            ],
            "arxiv_id": "2512.07302v1",
            "summary": "Existing image perception methods based on VLMs generally follow a paradigm wherein models extract and analyze image content based on user-provided textual task prompts. However, such methods face limitations when applied to UAV imagery, which presents challenges like target confusion, scale variations, and complex backgrounds. These challenges arise because VLMs' understanding of image content depends on the semantic alignment between visual and textual tokens. When the task prompt is simplistic and the image content is complex, achieving effective alignment becomes difficult, limiting the model's ability to focus on task-relevant information. To address this issue, we introduce AerialVP, the first agent framework for task prompt enhancement in UAV image perception. AerialVP proactively extracts multi-dimensional auxiliary information from UAV images to enhance task prompts, overcoming the limitations of traditional VLM-based approaches. Specifically, the enhancement process includes three stages: (1) analyzing the task prompt to identify the task type and enhancement needs, (2) selecting appropriate tools from the tool repository, and (3) generating enhanced task prompts based on the analysis and selected tools. To evaluate AerialVP, we introduce AerialSense, a comprehensive benchmark for UAV image perception that includes Aerial Visual Reasoning, Aerial Visual Question Answering, and Aerial Visual Grounding tasks. AerialSense provides a standardized basis for evaluating model generalization and performance across diverse resolutions, lighting conditions, and both urban and natural scenes. Experimental results demonstrate that AerialVP significantly enhances task prompt guidance, leading to stable and substantial performance improvements in both open-source and proprietary VLMs. Our work will be available at https://github.com/lostwolves/AerialVP.",
            "headline_zh": "提出AerialVP框架以增强任务提示，提升无人机图像感知中视觉语言模型的准确性",
            "intro_zh": [
                "无人机图像感知中，传统视觉语言模型因任务提示简单和图像复杂导致语义对齐困难，影响性能",
                "AerialVP通过主动提取多维辅助信息增强任务提示，包括任务分析、工具选择和提示生成三阶段",
                "实验基于AerialSense基准，显示AerialVP能稳定提升开源和专有视觉语言模型的性能"
            ],
            "tags_zh": [
                "无人机图像感知",
                "视觉语言模型",
                "任务提示增强",
                "AerialVP框架",
                "AerialSense基准"
            ],
            "_index": 170
        },
        {
            "title": "Equivariant Diffusion for Crystal Structure Prediction",
            "authors": [
                "Peijia Lin",
                "Pin Chen",
                "Rui Jiao",
                "Qing Mo",
                "Jianhuan Cen",
                "Wenbing Huang",
                "Yang Liu",
                "Dan Huang",
                "Yutong Lu"
            ],
            "arxiv_id": "2512.07289v1",
            "summary": "In addressing the challenge of Crystal Structure Prediction (CSP), symmetry-aware deep learning models, particularly diffusion models, have been extensively studied, which treat CSP as a conditional generation task. However, ensuring permutation, rotation, and periodic translation equivariance during diffusion process remains incompletely addressed. In this work, we propose EquiCSP, a novel equivariant diffusion-based generative model. We not only address the overlooked issue of lattice permutation equivariance in existing models, but also develop a unique noising algorithm that rigorously maintains periodic translation equivariance throughout both training and inference processes. Our experiments indicate that EquiCSP significantly surpasses existing models in terms of generating accurate structures and demonstrates faster convergence during the training process.",
            "headline_zh": "提出EquiCSP扩散模型以解决晶体结构预测中的对称性保持问题",
            "intro_zh": [
                "现有晶体结构预测模型在扩散过程中未能完全保证置换、旋转和周期平移等变性",
                "提出EquiCSP模型，通过独特加噪算法严格保持周期平移等变性，并解决晶格置换等变性",
                "实验表明模型在生成准确结构和训练收敛速度方面显著优于现有方法"
            ],
            "tags_zh": [
                "晶体结构预测",
                "等变扩散模型",
                "对称性保持",
                "条件生成",
                "周期平移等变性"
            ],
            "_index": 171
        },
        {
            "title": "SIT-Graph: State Integrated Tool Graph for Multi-Turn Agents",
            "authors": [
                "Sijia Li",
                "Yuchen Huang",
                "Zifan Liu",
                "Zijian Li",
                "Jingjing fu",
                "Lei Song",
                "Jiang Bian",
                "Jun Zhang",
                "Rui Wang"
            ],
            "arxiv_id": "2512.07287v1",
            "summary": "Despite impressive advances in agent systems, multi-turn tool-use scenarios remain challenging. It is mainly because intent is clarified progressively and the environment evolves with each tool call. While reusing past experience is natural, current LLM agents either treat entire trajectories or pre-defined subtasks as indivisible units, or solely exploit tool-to-tool dependencies, hindering adaptation as states and information evolve across turns. In this paper, we propose a State Integrated Tool Graph (SIT-Graph), which enhances multi-turn tool use by exploiting partially overlapping experience. Inspired by human decision-making that integrates episodic and procedural memory, SIT-Graph captures both compact state representations (episodic-like fragments) and tool-to-tool dependencies (procedural-like routines) from historical trajectories. Specifically, we first build a tool graph from accumulated tool-use sequences, and then augment each edge with a compact state summary of the dialog and tool history that may shape the next action. At inference time, SIT-Graph enables a human-like balance between episodic recall and procedural execution: when the next decision requires recalling prior context, the agent retrieves the state summaries stored on relevant edges and uses them to guide its next action; when the step is routine, it follows high-confidence tool dependencies without explicit recall. Experiments across multiple stateful multi-turn tool-use benchmarks show that SIT-Graph consistently outperforms strong memory- and graph-based baselines, delivering more robust tool selection and more effective experience transfer.",
            "headline_zh": "提出状态集成工具图以增强多轮工具使用，通过利用部分重叠经验平衡情景回忆与程序执行。",
            "intro_zh": [
                "核心问题：多轮工具使用中意图渐进澄清和环境动态变化，现有方法难以适应状态和信息演化。",
                "方法要点：构建工具图并增强边上的紧凑状态摘要，实现基于情景回忆和工具依赖的决策平衡。",
                "实验或效果：在多个状态多轮工具使用基准上优于强基线，提升工具选择和经验迁移效果。"
            ],
            "tags_zh": [
                "多轮工具使用",
                "状态集成工具图",
                "经验复用",
                "情景记忆",
                "程序记忆",
                "工具依赖"
            ],
            "_index": 172
        },
        {
            "title": "Verifiable Deep Quantitative Group Testing",
            "authors": [
                "Shreyas Jayant Grampurohit",
                "Satish Mulleti",
                "Ajit Rajwade"
            ],
            "arxiv_id": "2512.07279v1",
            "summary": "We present a neural network-based framework for solving the quantitative group testing (QGT) problem that achieves both high decoding accuracy and structural verifiability. In QGT, the objective is to identify a small subset of defective items among $N$ candidates using only $M \\ll N$ pooled tests, each reporting the number of defectives in the tested subset. We train a multi-layer perceptron to map noisy measurement vectors to binary defect indicators, achieving accurate and robust recovery even under sparse, bounded perturbations. Beyond accuracy, we show that the trained network implicitly learns the underlying pooling structure that links items to tests, allowing this structure to be recovered directly from the network's Jacobian. This indicates that the model does not merely memorize training patterns but internalizes the true combinatorial relationships governing QGT. Our findings reveal that standard feedforward architectures can learn verifiable inverse mappings in structured combinatorial recovery problems.",
            "headline_zh": "提出基于神经网络的定量群组测试框架，实现高精度解码与结构可验证性。",
            "intro_zh": [
                "核心问题：定量群组测试中，从少量池化测试中识别缺陷物品子集。",
                "方法要点：使用多层感知机映射噪声测量向量至二进制缺陷指示器，并可从雅可比矩阵恢复池化结构。",
                "实验或效果：模型在稀疏有界扰动下实现准确鲁棒恢复，并内部化组合关系而非记忆模式。"
            ],
            "tags_zh": [
                "定量群组测试",
                "神经网络解码",
                "结构可验证性",
                "雅可比矩阵分析",
                "组合恢复问题"
            ],
            "_index": 173
        },
        {
            "title": "Geo3DVQA: Evaluating Vision-Language Models for 3D Geospatial Reasoning from Aerial Imagery",
            "authors": [
                "Mai Tsujimoto",
                "Junjue Wang",
                "Weihao Xuan",
                "Naoto Yokoya"
            ],
            "arxiv_id": "2512.07276v1",
            "summary": "Three-dimensional geospatial analysis is critical to applications in urban planning, climate adaptation, and environmental assessment. Current methodologies depend on costly, specialized sensors (e.g., LiDAR and multispectral), which restrict global accessibility. Existing sensor-based and rule-driven methods further struggle with tasks requiring the integration of multiple 3D cues, handling diverse queries, and providing interpretable reasoning. We hereby present Geo3DVQA, a comprehensive benchmark for evaluating vision-language models (VLMs) in height-aware, 3D geospatial reasoning using RGB-only remote sensing imagery. Unlike conventional sensor-based frameworks, Geo3DVQA emphasizes realistic scenarios that integrate elevation, sky view factors, and land cover patterns. The benchmark encompasses 110k curated question-answer pairs spanning 16 task categories across three complexity levels: single-feature inference, multi-feature reasoning, and application-level spatial analysis. The evaluation of ten state-of-the-art VLMs highlights the difficulty of RGB-to-3D reasoning. GPT-4o and Gemini-2.5-Flash achieved only 28.6% and 33.0% accuracy respectively, while domain-specific fine-tuning of Qwen2.5-VL-7B achieved 49.6% (+24.8 points). These results reveal both the limitations of current VLMs and the effectiveness of domain adaptation. Geo3DVQA introduces new challenge frontiers for scalable, accessible, and holistic 3D geospatial analysis. The dataset and code will be released upon publication at https://github.com/mm1129/Geo3DVQA.",
            "headline_zh": "提出Geo3DVQA基准，评估仅用RGB遥感图像的视觉语言模型在三维地理空间推理中的性能。",
            "intro_zh": [
                "核心问题：现有方法依赖昂贵传感器，难以整合多3D线索处理多样化查询。",
                "方法要点：构建包含11万问答对的基准，涵盖16个任务类别和三个复杂度级别。",
                "实验或效果：评估10个先进VLM，GPT-4o和Gemini-2.5-Flash准确率低，领域微调Qwen2.5-VL-7B提升至49.6%。"
            ],
            "tags_zh": [
                "三维地理空间推理",
                "视觉语言模型评估",
                "遥感图像分析",
                "基准数据集",
                "领域适应",
                "RGB图像处理"
            ],
            "_index": 174
        },
        {
            "title": "Effective Attention-Guided Multi-Scale Medical Network for Skin Lesion Segmentation",
            "authors": [
                "Siyu Wang",
                "Hua Wang",
                "Huiyu Li",
                "Fan Zhang"
            ],
            "arxiv_id": "2512.07275v1",
            "summary": "In the field of healthcare, precise skin lesion segmentation is crucial for the early detection and accurate diagnosis of skin diseases. Despite significant advances in deep learning for image processing, existing methods have yet to effectively address the challenges of irregular lesion shapes and low contrast. To address these issues, this paper proposes an innovative encoder-decoder network architecture based on multi-scale residual structures, capable of extracting rich feature information from different receptive fields to effectively identify lesion areas. By introducing a Multi-Resolution Multi-Channel Fusion (MRCF) module, our method captures cross-scale features, enhancing the clarity and accuracy of the extracted information. Furthermore, we propose a Cross-Mix Attention Module (CMAM), which redefines the attention scope and dynamically calculates weights across multiple contexts, thus improving the flexibility and depth of feature capture and enabling deeper exploration of subtle features. To overcome the information loss caused by skip connections in traditional U-Net, an External Attention Bridge (EAB) is introduced, facilitating the effective utilization of information in the decoder and compensating for the loss during upsampling. Extensive experimental evaluations on several skin lesion segmentation datasets demonstrate that the proposed model significantly outperforms existing transformer and convolutional neural network-based models, showcasing exceptional segmentation accuracy and robustness.",
            "headline_zh": "提出基于多尺度残差和注意力机制的编码器-解码器网络，以解决皮肤病灶分割中形状不规则和对比度低的挑战。",
            "intro_zh": [
                "核心问题：皮肤病灶分割面临病灶形状不规则和图像对比度低的难题，影响早期检测和诊断准确性。",
                "方法要点：引入多分辨率多通道融合模块和交叉混合注意力模块，增强特征提取的深度和灵活性，并通过外部注意力桥补偿信息损失。",
                "实验或效果：在多个皮肤病灶数据集上验证，模型在分割准确性和鲁棒性上显著优于现有基于Transformer和卷积神经网络的方法。"
            ],
            "tags_zh": [
                "皮肤病灶分割",
                "多尺度特征融合",
                "注意力机制",
                "编码器-解码器网络",
                "医学图像处理"
            ],
            "_index": 175
        },
        {
            "title": "RVLF: A Reinforcing Vision-Language Framework for Gloss-Free Sign Language Translation",
            "authors": [
                "Zhi Rao",
                "Yucheng Zhou",
                "Benjia Zhou",
                "Yiqing Huang",
                "Sergio Escalera",
                "Jun Wan"
            ],
            "arxiv_id": "2512.07273v1",
            "summary": "Gloss-free sign language translation (SLT) is hindered by two key challenges: **inadequate sign representation** that fails to capture nuanced visual cues, and **sentence-level semantic misalignment** in current LLM-based methods, which limits translation quality. To address these issues, we propose a three-stage **r**einforcing **v**ision-**l**anguage **f**ramework (**RVLF**). We build a large vision-language model (LVLM) specifically designed for sign language, and then combine it with reinforcement learning (RL) to adaptively enhance translation performance. First, for a sufficient representation of sign language, RVLF introduces an effective semantic representation learning mechanism that fuses skeleton-based motion cues with semantically rich visual features extracted via DINOv2, followed by instruction tuning to obtain a strong SLT-SFT baseline. Then, to improve sentence-level semantic misalignment, we introduce a GRPO-based optimization strategy that fine-tunes the SLT-SFT model with a reward function combining translation fidelity (BLEU) and sentence completeness (ROUGE), yielding the optimized model termed SLT-GRPO. Our conceptually simple framework yields substantial gains under the gloss-free SLT setting without pre-training on any external large-scale sign language datasets, improving BLEU-4 scores by +5.1, +1.11, +1.4, and +1.61 on the CSL-Daily, PHOENIX-2014T, How2Sign, and OpenASL datasets, respectively. To the best of our knowledge, this is the first work to incorporate GRPO into SLT. Extensive experiments and ablation studies validate the effectiveness of GRPO-based optimization in enhancing both translation quality and semantic consistency.",
            "headline_zh": "提出RVLF框架以解决无注释手语翻译中的表示不足和语义对齐问题",
            "intro_zh": [
                "核心问题：无注释手语翻译存在视觉表示不足和句子级语义错配，影响翻译质量。",
                "方法要点：结合大型视觉语言模型与强化学习，通过语义表示学习和GRPO优化提升翻译性能。",
                "实验或效果：在多个数据集上BLEU-4分数显著提升，验证了GRPO优化的有效性。"
            ],
            "tags_zh": [
                "无注释手语翻译",
                "视觉语言模型",
                "强化学习",
                "语义对齐",
                "GRPO优化"
            ],
            "_index": 176
        },
        {
            "title": "A graph generation pipeline for critical infrastructures based on heuristics, images and depth data",
            "authors": [
                "Mike Diessner",
                "Yannick Tarant"
            ],
            "arxiv_id": "2512.07269v1",
            "summary": "Virtual representations of physical critical infrastructures, such as water or energy plants, are used for simulations and digital twins to ensure resilience and continuity of their services. These models usually require 3D point clouds from laser scanners that are expensive to acquire and require specialist knowledge to use. In this article, we present a graph generation pipeline based on photogrammetry. The pipeline detects relevant objects and predicts their relation using RGB images and depth data generated by a stereo camera. This more cost-effective approach uses deep learning for object detection and instance segmentation of the objects, and employs user-defined heuristics or rules to infer their relations. Results of two hydraulic systems show that this strategy can produce graphs close to the ground truth while its flexibility allows the method to be tailored to specific applications and its transparency qualifies it to be used in the high stakes decision-making that is required for critical infrastructures.",
            "headline_zh": "提出基于立体相机图像与深度数据的图生成流程，用于关键基础设施建模。",
            "intro_zh": [
                "核心问题：传统激光扫描获取3D点云成本高且需专业知识，阻碍关键基础设施虚拟建模。",
                "方法要点：结合深度学习进行对象检测与实例分割，利用启发式规则推断对象关系，生成图结构。",
                "实验或效果：在液压系统测试中，生成图接近真实情况，方法灵活透明，适用于高风险决策场景。"
            ],
            "tags_zh": [
                "图生成",
                "关键基础设施建模",
                "立体相机",
                "深度学习",
                "启发式规则",
                "数字孪生"
            ],
            "_index": 177
        },
        {
            "title": "Non-negative DAG Learning from Time-Series Data",
            "authors": [
                "Samuel Rey",
                "Gonzalo Mateos"
            ],
            "arxiv_id": "2512.07267v1",
            "summary": "This work aims to learn the directed acyclic graph (DAG) that captures the instantaneous dependencies underlying a multivariate time series. The observed data follow a linear structural vector autoregressive model (SVARM) with both instantaneous and time-lagged dependencies, where the instantaneous structure is modeled by a DAG to reflect potential causal relationships. While recent continuous relaxation approaches impose acyclicity through smooth constraint functions involving powers of the adjacency matrix, they lead to non-convex optimization problems that are challenging to solve. In contrast, we assume that the underlying DAG has only non-negative edge weights, and leverage this additional structure to impose acyclicity via a convex constraint. This enables us to cast the problem of non-negative DAG recovery from multivariate time-series data as a convex optimization problem in abstract form, which we solve using the method of multipliers. Crucially, the convex formulation guarantees global optimality of the solution. Finally, we assess the performance of the proposed method on synthetic time-series data, where it outperforms existing alternatives.",
            "headline_zh": "提出非负有向无环图学习方法，从时间序列数据中恢复因果结构。",
            "intro_zh": [
                "核心问题：从多元时间序列中学习有向无环图以捕捉瞬时依赖关系。",
                "方法要点：假设边权重非负，通过凸约束保证无环性，实现凸优化求解。",
                "实验或效果：在合成数据上评估，性能优于现有方法，凸公式保证全局最优性。"
            ],
            "tags_zh": [
                "有向无环图学习",
                "时间序列分析",
                "凸优化",
                "因果推断",
                "结构向量自回归模型"
            ],
            "_index": 178
        },
        {
            "title": "SINRL: Socially Integrated Navigation with Reinforcement Learning using Spiking Neural Networks",
            "authors": [
                "Florian Tretter",
                "Daniel Flögel",
                "Alexandru Vasilache",
                "Max Grobbel",
                "Jürgen Becker",
                "Sören Hohmann"
            ],
            "arxiv_id": "2512.07266v1",
            "summary": "Integrating autonomous mobile robots into human environments requires human-like decision-making and energy-efficient, event-based computation. Despite progress, neuromorphic methods are rarely applied to Deep Reinforcement Learning (DRL) navigation approaches due to unstable training. We address this gap with a hybrid socially integrated DRL actor-critic approach that combines Spiking Neural Networks (SNNs) in the actor with Artificial Neural Networks (ANNs) in the critic and a neuromorphic feature extractor to capture temporal crowd dynamics and human-robot interactions. Our approach enhances social navigation performance and reduces estimated energy consumption by approximately 1.69 orders of magnitude.",
            "headline_zh": "提出混合社会集成DRL方法，结合SNN与ANN，以解决机器人导航中训练不稳定和能耗高的问题。",
            "intro_zh": [
                "核心问题：自主移动机器人在人类环境中需类人决策和节能计算，但神经形态方法在DRL导航中因训练不稳定应用少。",
                "方法要点：采用混合DRL演员-评论家方法，演员用SNN，评论家用ANN，并集成神经形态特征提取器捕捉时空动态。",
                "实验或效果：提升社会导航性能，估计能耗降低约1.69个数量级。"
            ],
            "tags_zh": [
                "社会导航",
                "深度强化学习",
                "脉冲神经网络",
                "神经形态计算",
                "能耗优化",
                "人机交互"
            ],
            "_index": 179
        },
        {
            "title": "Affine Subspace Models and Clustering for Patch-Based Image Denoising",
            "authors": [
                "Tharindu Wickremasinghe",
                "Marco F. Duarte"
            ],
            "arxiv_id": "2512.07259v1",
            "summary": "Image tile-based approaches are popular in many image processing applications such as denoising (e.g., non-local means). A key step in their use is grouping the images into clusters, which usually proceeds iteratively splitting the images into clusters and fitting a model for the images in each cluster. Linear subspaces have emerged as a suitable model for tile clusters; however, they are not well matched to images patches given that images are non-negative and thus not distributed around the origin in the tile vector space. We study the use of affine subspace models for the clusters to better match the geometric structure of the image tile vector space. We also present a simple denoising algorithm that relies on the affine subspace clustering model using least squares projection. We review several algorithmic approaches to solve the affine subspace clustering problem and show experimental results that highlight the performance improvements in clustering and denoising.",
            "headline_zh": "提出仿射子空间模型与聚类方法以改进基于图像块的去噪性能",
            "intro_zh": [
                "核心问题：线性子空间模型不匹配图像块的非负特性，导致聚类效果不佳",
                "方法要点：使用仿射子空间模型更好地拟合图像块向量空间的几何结构",
                "实验或效果：通过最小二乘投影实现去噪，实验显示聚类和去噪性能提升"
            ],
            "tags_zh": [
                "图像去噪",
                "仿射子空间",
                "块聚类",
                "最小二乘投影",
                "非局部均值"
            ],
            "_index": 180
        },
        {
            "title": "DGGAN: Degradation Guided Generative Adversarial Network for Real-time Endoscopic Video Enhancement",
            "authors": [
                "Handing Xu",
                "Zhenguo Nie",
                "Tairan Peng",
                "Huimin Pan",
                "Xin-Jun Liu"
            ],
            "arxiv_id": "2512.07253v1",
            "summary": "Endoscopic surgery relies on intraoperative video, making image quality a decisive factor for surgical safety and efficacy. Yet, endoscopic videos are often degraded by uneven illumination, tissue scattering, occlusions, and motion blur, which obscure critical anatomical details and complicate surgical manipulation. Although deep learning-based methods have shown promise in image enhancement, most existing approaches remain too computationally demanding for real-time surgical use. To address this challenge, we propose a degradation-aware framework for endoscopic video enhancement, which enables real-time, high-quality enhancement by propagating degradation representations across frames. In our framework, degradation representations are first extracted from images using contrastive learning. We then introduce a fusion mechanism that modulates image features with these representations to guide a single-frame enhancement model, which is trained with a cycle-consistency constraint between degraded and restored images to improve robustness and generalization. Experiments demonstrate that our framework achieves a superior balance between performance and efficiency compared with several state-of-the-art methods. These results highlight the effectiveness of degradation-aware modeling for real-time endoscopic video enhancement. Nevertheless, our method suggests that implicitly learning and propagating degradation representation offer a practical pathway for clinical application.",
            "headline_zh": "提出DGGAN，通过退化感知建模实现实时内窥镜视频增强",
            "intro_zh": [
                "核心问题：内窥镜视频因光照不均、组织散射等退化影响手术安全，现有深度学习方法计算量大，难以实时应用。",
                "方法要点：采用对比学习提取退化表示，通过融合机制调制图像特征，结合循环一致性约束训练单帧增强模型，实现跨帧退化传播。",
                "实验或效果：在性能与效率间取得优越平衡，验证了退化感知建模对实时增强的有效性，为临床应用提供可行路径。"
            ],
            "tags_zh": [
                "内窥镜视频增强",
                "退化感知建模",
                "实时处理",
                "生成对抗网络",
                "对比学习",
                "循环一致性"
            ],
            "_index": 181
        },
        {
            "title": "See More, Change Less: Anatomy-Aware Diffusion for Contrast Enhancement",
            "authors": [
                "Junqi Liu",
                "Zejun Wu",
                "Pedro R. A. S. Bassi",
                "Xinze Zhou",
                "Wenxuan Li",
                "Ibrahim E. Hamamci",
                "Sezgin Er",
                "Tianyu Lin",
                "Yi Luo",
                "Szymon Płotka",
                "Bjoern Menze",
                "Daguang Xu",
                "Kai Ding",
                "Kang Wang",
                "Yang Yang",
                "Yucheng Tang",
                "Alan L. Yuille",
                "Zongwei Zhou"
            ],
            "arxiv_id": "2512.07251v1",
            "summary": "Image enhancement improves visual quality and helps reveal details that are hard to see in the original image. In medical imaging, it can support clinical decision-making, but current models often over-edit. This can distort organs, create false findings, and miss small tumors because these models do not understand anatomy or contrast dynamics. We propose SMILE, an anatomy-aware diffusion model that learns how organs are shaped and how they take up contrast. It enhances only clinically relevant regions while leaving all other areas unchanged. SMILE introduces three key ideas: (1) structure-aware supervision that follows true organ boundaries and contrast patterns; (2) registration-free learning that works directly with unaligned multi-phase CT scans; (3) unified inference that provides fast and consistent enhancement across all contrast phases. Across six external datasets, SMILE outperforms existing methods in image quality (14.2% higher SSIM, 20.6% higher PSNR, 50% better FID) and in clinical usefulness by producing anatomically accurate and diagnostically meaningful images. SMILE also improves cancer detection from non-contrast CT, raising the F1 score by up to 10 percent.",
            "headline_zh": "提出SMILE解剖感知扩散模型，以解决医学图像增强中过度编辑导致解剖失真和临床误判的问题。",
            "intro_zh": [
                "核心问题：现有医学图像增强模型因缺乏解剖和对比度动态理解，常过度编辑，导致器官失真、假阳性或漏检小肿瘤。",
                "方法要点：SMILE通过结构感知监督、免配准学习和统一推理，实现仅增强临床相关区域，保持解剖准确性。",
                "实验或效果：在六个外部数据集上，SMILE在图像质量和临床有用性上优于现有方法，提升癌症检测F1分数达10%。"
            ],
            "tags_zh": [
                "医学图像增强",
                "解剖感知扩散模型",
                "对比度增强",
                "免配准学习",
                "临床决策支持"
            ],
            "_index": 182
        },
        {
            "title": "IFFair: Influence Function-driven Sample Reweighting for Fair Classification",
            "authors": [
                "Jingran Yang",
                "Min Zhang",
                "Lingfeng Zhang",
                "Zhaohui Wang",
                "Yonggang Zhang"
            ],
            "arxiv_id": "2512.07249v1",
            "summary": "Because machine learning has significantly improved efficiency and convenience in the society, it's increasingly used to assist or replace human decision-making. However, the data-based pattern makes related algorithms learn and even exacerbate potential bias in samples, resulting in discriminatory decisions against certain unprivileged groups, depriving them of the rights to equal treatment, thus damaging the social well-being and hindering the development of related applications. Therefore, we propose a pre-processing method IFFair based on the influence function. Compared with other fairness optimization approaches, IFFair only uses the influence disparity of training samples on different groups as a guidance to dynamically adjust the sample weights during training without modifying the network structure, data features and decision boundaries. To evaluate the validity of IFFair, we conduct experiments on multiple real-world datasets and metrics. The experimental results show that our approach mitigates bias of multiple accepted metrics in the classification setting, including demographic parity, equalized odds, equality of opportunity and error rate parity without conflicts. It also demonstrates that IFFair achieves better trade-off between multiple utility and fairness metrics compared with previous pre-processing methods.",
            "headline_zh": "提出IFFair方法，基于影响函数动态调整样本权重以解决分类中的公平性问题。",
            "intro_zh": [
                "核心问题：机器学习算法可能学习并加剧样本偏见，导致对弱势群体的歧视性决策。",
                "方法要点：利用影响函数计算训练样本对不同群体的影响差异，动态调整样本权重，无需修改网络结构或数据特征。",
                "实验或效果：在多个真实数据集上验证，IFFair能缓解多种公平性指标偏见，并在效用与公平性间取得更好权衡。"
            ],
            "tags_zh": [
                "公平分类",
                "影响函数",
                "样本重加权",
                "预处理方法",
                "机器学习公平性"
            ],
            "_index": 183
        },
        {
            "title": "AdLift: Lifting Adversarial Perturbations to Safeguard 3D Gaussian Splatting Assets Against Instruction-Driven Editing",
            "authors": [
                "Ziming Hong",
                "Tianyu Huang",
                "Runnan Chen",
                "Shanshan Ye",
                "Mingming Gong",
                "Bo Han",
                "Tongliang Liu"
            ],
            "arxiv_id": "2512.07247v1",
            "summary": "Recent studies have extended diffusion-based instruction-driven 2D image editing pipelines to 3D Gaussian Splatting (3DGS), enabling faithful manipulation of 3DGS assets and greatly advancing 3DGS content creation. However, it also exposes these assets to serious risks of unauthorized editing and malicious tampering. Although imperceptible adversarial perturbations against diffusion models have proven effective for protecting 2D images, applying them to 3DGS encounters two major challenges: view-generalizable protection and balancing invisibility with protection capability. In this work, we propose the first editing safeguard for 3DGS, termed AdLift, which prevents instruction-driven editing across arbitrary views and dimensions by lifting strictly bounded 2D adversarial perturbations into 3D Gaussian-represented safeguard. To ensure both adversarial perturbations effectiveness and invisibility, these safeguard Gaussians are progressively optimized across training views using a tailored Lifted PGD, which first conducts gradient truncation during back-propagation from the editing model at the rendered image and applies projected gradients to strictly constrain the image-level perturbation. Then, the resulting perturbation is backpropagated to the safeguard Gaussian parameters via an image-to-Gaussian fitting operation. We alternate between gradient truncation and image-to-Gaussian fitting, yielding consistent adversarial-based protection performance across different viewpoints and generalizes to novel views. Empirically, qualitative and quantitative results demonstrate that AdLift effectively protects against state-of-the-art instruction-driven 2D image and 3DGS editing.",
            "headline_zh": "提出AdLift方法，通过提升2D对抗扰动至3D高斯表示，保护3D高斯泼溅资产免受指令驱动编辑的威胁。",
            "intro_zh": [
                "核心问题：3D高斯泼溅资产面临指令驱动编辑的未授权篡改风险，现有2D对抗扰动方法难以直接应用。",
                "方法要点：采用Lifted PGD优化，通过梯度截断和图像到高斯拟合，将严格有界的2D扰动提升为3D高斯保护表示。",
                "实验或效果：AdLift在定性和定量实验中有效抵御先进指令驱动编辑，实现多视角一致保护。"
            ],
            "tags_zh": [
                "3D高斯泼溅",
                "对抗扰动",
                "指令驱动编辑",
                "资产保护",
                "多视角泛化"
            ],
            "_index": 184
        },
        {
            "title": "Zero-Shot Textual Explanations via Translating Decision-Critical Features",
            "authors": [
                "Toshinori Yamauchi",
                "Hiroshi Kera",
                "Kazuhiko Kawamoto"
            ],
            "arxiv_id": "2512.07245v1",
            "summary": "Textual explanations make image classifier decisions transparent by describing the prediction rationale in natural language. Large vision-language models can generate captions but are designed for general visual understanding, not classifier-specific reasoning. Existing zero-shot explanation methods align global image features with language, producing descriptions of what is visible rather than what drives the prediction. We propose TEXTER, which overcomes this limitation by isolating decision-critical features before alignment. TEXTER identifies the neurons contributing to the prediction and emphasizes the features encoded in those neurons -- i.e., the decision-critical features. It then maps these emphasized features into the CLIP feature space to retrieve textual explanations that reflect the model's reasoning. A sparse autoencoder further improves interpretability, particularly for Transformer architectures. Extensive experiments show that TEXTER generates more faithful and interpretable explanations than existing methods. The code will be publicly released.",
            "headline_zh": "提出TEXTER方法，通过分离决策关键特征实现零样本文本解释，提升图像分类器透明度。",
            "intro_zh": [
                "现有零样本方法生成描述可见内容而非预测驱动因素，导致解释不忠实。",
                "TEXTER识别预测贡献神经元，强调决策关键特征，映射到CLIP空间检索文本解释。",
                "实验表明TEXTER比现有方法生成更忠实和可解释的文本解释，代码将公开。"
            ],
            "tags_zh": [
                "零样本文本解释",
                "决策关键特征",
                "图像分类器透明度",
                "CLIP特征映射",
                "稀疏自编码器"
            ],
            "_index": 185
        },
        {
            "title": "PINE: Pipeline for Important Node Exploration in Attributed Networks",
            "authors": [
                "Elizaveta Kovtun",
                "Maksim Makarenko",
                "Natalia Semenova",
                "Alexey Zaytsev",
                "Semen Budennyy"
            ],
            "arxiv_id": "2512.07244v1",
            "summary": "A graph with semantically attributed nodes are a common data structure in a wide range of domains. It could be interlinked web data or citation networks of scientific publications. The essential problem for such a data type is to determine nodes that carry greater importance than all the others, a task that markedly enhances system monitoring and management. Traditional methods to identify important nodes in networks introduce centrality measures, such as node degree or more complex PageRank. However, they consider only the network structure, neglecting the rich node attributes. Recent methods adopt neural networks capable of handling node features, but they require supervision. This work addresses the identified gap--the absence of approaches that are both unsupervised and attribute-aware--by introducing a Pipeline for Important Node Exploration (PINE). At the core of the proposed framework is an attention-based graph model that incorporates node semantic features in the learning process of identifying the structural graph properties. The PINE's node importance scores leverage the obtained attention distribution. We demonstrate the superior performance of the proposed PINE method on various homogeneous and heterogeneous attributed networks. As an industry-implemented system, PINE tackles the real-world challenge of unsupervised identification of key entities within large-scale enterprise graphs.",
            "headline_zh": "提出PINE管道，以无监督方式解决属性网络中关键节点识别问题。",
            "intro_zh": [
                "核心问题：属性网络中节点重要性识别，传统方法忽略节点语义特征，现有神经网络方法需监督。",
                "方法要点：基于注意力的图模型，结合节点语义特征学习结构属性，利用注意力分布计算重要性分数。",
                "实验或效果：在多种同质和异质属性网络上验证性能优越，适用于大规模企业图的无监督关键实体识别。"
            ],
            "tags_zh": [
                "属性网络",
                "无监督学习",
                "注意力机制",
                "节点重要性",
                "图神经网络",
                "企业图分析"
            ],
            "_index": 186
        },
        {
            "title": "Squeezed-Eff-Net: Edge-Computed Boost of Tomography Based Brain Tumor Classification leveraging Hybrid Neural Network Architecture",
            "authors": [
                "Md. Srabon Chowdhury",
                "Syeda Fahmida Tanzim",
                "Sheekar Banerjee",
                "Ishtiak Al Mamoon",
                "AKM Muzahidul Islam"
            ],
            "arxiv_id": "2512.07241v1",
            "summary": "Brain tumors are one of the most common and dangerous neurological diseases which require a timely and correct diagnosis to provide the right treatment procedures. Even with the promotion of magnetic resonance imaging (MRI), the process of tumor delineation is difficult and time-consuming, which is prone to inter-observer error. In order to overcome these limitations, this work proposes a hybrid deep learning model based on SqueezeNet v1 which is a lightweight model, and EfficientNet-B0, which is a high-performing model, and is enhanced with handcrafted radiomic descriptors, including Histogram of Oriented Gradients (HOG), Local Binary Patterns (LBP), Gabor filters and Wavelet transforms. The framework was trained and tested only on publicly available Nickparvar Brain Tumor MRI dataset, which consisted of 7,023 contrast-enhanced T1-weighted axial MRI slices which were categorized into four groups: glioma, meningioma, pituitary tumor, and no tumor. The testing accuracy of the model was 98.93% that reached a level of 99.08% with Test Time Augmentation (TTA) showing great generalization and power. The proposed hybrid network offers a compromise between computation efficiency and diagnostic accuracy compared to current deep learning structures and only has to be trained using fewer than 2.1 million parameters and less than 1.2 GFLOPs. The handcrafted feature addition allowed greater sensitivity in texture and the EfficientNet-B0 backbone represented intricate hierarchical features. The resulting model has almost clinical reliability in automated MRI-based classification of tumors highlighting its possibility of use in clinical decision-support systems.",
            "headline_zh": "提出Squeezed-Eff-Net混合网络，结合轻量SqueezeNet与高效EfficientNet，增强手工特征，用于MRI脑肿瘤分类。",
            "intro_zh": [
                "核心问题：MRI脑肿瘤分类依赖人工，耗时且易出错，需自动化高精度诊断。",
                "方法要点：融合SqueezeNet v1和EfficientNet-B0，集成HOG、LBP等手工特征，平衡计算效率与准确性。",
                "实验或效果：在Nickparvar数据集上测试准确率达98.93%，TTA后达99.08%，参数少于210万，计算量低于1.2 GFLOPs。"
            ],
            "tags_zh": [
                "脑肿瘤分类",
                "混合神经网络",
                "MRI图像分析",
                "手工特征增强",
                "边缘计算优化"
            ],
            "_index": 187
        },
        {
            "title": "Unified Camera Positional Encoding for Controlled Video Generation",
            "authors": [
                "Cheng Zhang",
                "Boying Li",
                "Meng Wei",
                "Yan-Pei Cao",
                "Camilo Cruz Gambardella",
                "Dinh Phung",
                "Jianfei Cai"
            ],
            "arxiv_id": "2512.07237v1",
            "summary": "Transformers have emerged as a universal backbone across 3D perception, video generation, and world models for autonomous driving and embodied AI, where understanding camera geometry is essential for grounding visual observations in three-dimensional space. However, existing camera encoding methods often rely on simplified pinhole assumptions, restricting generalization across the diverse intrinsics and lens distortions in real-world cameras. We introduce Relative Ray Encoding, a geometry-consistent representation that unifies complete camera information, including 6-DoF poses, intrinsics, and lens distortions. To evaluate its capability under diverse controllability demands, we adopt camera-controlled text-to-video generation as a testbed task. Within this setting, we further identify pitch and roll as two components effective for Absolute Orientation Encoding, enabling full control over the initial camera orientation. Together, these designs form UCPE (Unified Camera Positional Encoding), which integrates into a pretrained video Diffusion Transformer through a lightweight spatial attention adapter, adding less than 1% trainable parameters while achieving state-of-the-art camera controllability and visual fidelity. To facilitate systematic training and evaluation, we construct a large video dataset covering a wide range of camera motions and lens types. Extensive experiments validate the effectiveness of UCPE in camera-controllable video generation and highlight its potential as a general camera representation for Transformers across future multi-view, video, and 3D tasks. Code will be available at https://github.com/chengzhag/UCPE.",
            "headline_zh": "提出统一相机位置编码UCPE，通过相对射线编码和绝对方向编码增强相机可控视频生成。",
            "intro_zh": [
                "现有相机编码依赖简化针孔模型，限制了对真实相机多样内参和镜头畸变的泛化能力。",
                "引入相对射线编码统一相机6自由度位姿、内参和镜头畸变，并识别俯仰和横滚作为绝对方向编码的关键组件。",
                "在相机可控文本到视频生成任务中，UCPE以少于1%可训练参数实现最先进的相机控制性和视觉保真度。"
            ],
            "tags_zh": [
                "相机可控视频生成",
                "统一相机编码",
                "相对射线编码",
                "扩散变换器",
                "多视图任务",
                "镜头畸变建模"
            ],
            "_index": 188
        },
        {
            "title": "Dropout Prompt Learning: Towards Robust and Adaptive Vision-Language Models",
            "authors": [
                "Biao Chen",
                "Lin Zuo",
                "Mengmeng Jing",
                "Kunbin He",
                "Yuchen Wang"
            ],
            "arxiv_id": "2512.07234v1",
            "summary": "Dropout is a widely used regularization technique which improves the generalization ability of a model by randomly dropping neurons. In light of this, we propose Dropout Prompt Learning, which aims for applying dropout to improve the robustness of the vision-language models. Different from the vanilla dropout, we apply dropout on the tokens of the textual and visual branches, where we evaluate the token significance considering both intra-modal context and inter-modal alignment, enabling flexible dropout probabilities for each token. Moreover, to maintain semantic alignment for general knowledge transfer while encouraging the diverse representations that dropout introduces, we further propose residual entropy regularization. Experiments on 15 benchmarks show our method's effectiveness in challenging scenarios like low-shot learning, long-tail classification, and out-of-distribution generalization. Notably, our method surpasses regularization-based methods including KgCoOp by 5.10% and PromptSRC by 2.13% in performance on base-to-novel generalization.",
            "headline_zh": "提出Dropout Prompt Learning以提升视觉-语言模型在低样本学习和分布外泛化等场景的鲁棒性。",
            "intro_zh": [
                "核心问题：视觉-语言模型在低样本学习、长尾分类和分布外泛化等挑战性场景中鲁棒性不足。",
                "方法要点：基于token重要性评估，在文本和视觉分支应用自适应dropout，并引入残差熵正则化以平衡语义对齐和多样性。",
                "实验或效果：在15个基准测试中有效，在基础到新类别泛化上优于KgCoOp和PromptSRC等方法。"
            ],
            "tags_zh": [
                "视觉-语言模型",
                "提示学习",
                "鲁棒性增强",
                "低样本学习",
                "分布外泛化",
                "正则化技术"
            ],
            "_index": 189
        },
        {
            "title": "Cross-platform Product Matching Based on Entity Alignment of Knowledge Graph with RAEA model",
            "authors": [
                "Wenlong Liu",
                "Jiahua Pan",
                "Xingyu Zhang",
                "Xinxin Gong",
                "Yang Ye",
                "Xujin Zhao",
                "Xin Wang",
                "Kent Wu",
                "Hua Xiang",
                "Houmin Yan",
                "Qingpeng Zhang"
            ],
            "arxiv_id": "2512.07232v1",
            "summary": "Product matching aims to identify identical or similar products sold on different platforms. By building knowledge graphs (KGs), the product matching problem can be converted to the Entity Alignment (EA) task, which aims to discover the equivalent entities from diverse KGs. The existing EA methods inadequately utilize both attribute triples and relation triples simultaneously, especially the interactions between them. This paper introduces a two-stage pipeline consisting of rough filter and fine filter to match products from eBay and Amazon. For fine filtering, a new framework for Entity Alignment, Relation-aware and Attribute-aware Graph Attention Networks for Entity Alignment (RAEA), is employed. RAEA focuses on the interactions between attribute triples and relation triples, where the entity representation aggregates the alignment signals from attributes and relations with Attribute-aware Entity Encoder and Relation-aware Graph Attention Networks. The experimental results indicate that the RAEA model achieves significant improvements over 12 baselines on EA task in the cross-lingual dataset DBP15K (6.59% on average Hits@1) and delivers competitive results in the monolingual dataset DWY100K. The source code for experiments on DBP15K and DWY100K is available at github (https://github.com/Mockingjay-liu/RAEA-model-for-Entity-Alignment).",
            "headline_zh": "提出RAEA模型以解决跨平台产品匹配中的实体对齐问题，通过结合属性和关系三元组交互提升性能。",
            "intro_zh": [
                "核心问题：现有实体对齐方法未能充分利用属性和关系三元组及其交互，影响跨平台产品匹配准确性。",
                "方法要点：采用两阶段流程（粗筛和精筛），在精筛阶段引入RAEA框架，通过属性感知实体编码器和关系感知图注意力网络聚合对齐信号。",
                "实验或效果：在跨语言数据集DBP15K上相比12个基线平均Hits@1提升6.59%，在单语言数据集DWY100K上取得竞争性结果。"
            ],
            "tags_zh": [
                "实体对齐",
                "知识图谱",
                "产品匹配",
                "图注意力网络",
                "跨平台应用"
            ],
            "_index": 190
        },
        {
            "title": "STRinGS: Selective Text Refinement in Gaussian Splatting",
            "authors": [
                "Abhinav Raundhal",
                "Gaurav Behera",
                "P J Narayanan",
                "Ravi Kiran Sarvadevabhatla",
                "Makarand Tapaswi"
            ],
            "arxiv_id": "2512.07230v1",
            "summary": "Text as signs, labels, or instructions is a critical element of real-world scenes as they can convey important contextual information. 3D representations such as 3D Gaussian Splatting (3DGS) struggle to preserve fine-grained text details, while achieving high visual fidelity. Small errors in textual element reconstruction can lead to significant semantic loss. We propose STRinGS, a text-aware, selective refinement framework to address this issue for 3DGS reconstruction. Our method treats text and non-text regions separately, refining text regions first and merging them with non-text regions later for full-scene optimization. STRinGS produces sharp, readable text even in challenging configurations. We introduce a text readability measure OCR Character Error Rate (CER) to evaluate the efficacy on text regions. STRinGS results in a 63.6% relative improvement over 3DGS at just 7K iterations. We also introduce a curated dataset STRinGS-360 with diverse text scenarios to evaluate text readability in 3D reconstruction. Our method and dataset together push the boundaries of 3D scene understanding in text-rich environments, paving the way for more robust text-aware reconstruction methods.",
            "headline_zh": "提出STRinGS框架，通过选择性文本细化解决3D高斯泼溅中文本细节重建问题",
            "intro_zh": [
                "核心问题：3D高斯泼溅在重建精细文本细节时易导致语义损失，影响场景理解。",
                "方法要点：分离处理文本与非文本区域，先细化文本区域再合并优化，提升文本可读性。",
                "实验或效果：在7K迭代下相对3DGS提升63.6%，引入OCR CER评估指标和STRinGS-360数据集。"
            ],
            "tags_zh": [
                "3D高斯泼溅",
                "文本重建",
                "选择性细化",
                "OCR评估",
                "场景理解",
                "数据集构建"
            ],
            "_index": 191
        },
        {
            "title": "ReLKD: Inter-Class Relation Learning with Knowledge Distillation for Generalized Category Discovery",
            "authors": [
                "Fang Zhou",
                "Zhiqiang Chen",
                "Martin Pavlovski",
                "Yizhong Zhang"
            ],
            "arxiv_id": "2512.07229v1",
            "summary": "Generalized Category Discovery (GCD) faces the challenge of categorizing unlabeled data containing both known and novel classes, given only labels for known classes. Previous studies often treat each class independently, neglecting the inherent inter-class relations. Obtaining such inter-class relations directly presents a significant challenge in real-world scenarios. To address this issue, we propose ReLKD, an end-to-end framework that effectively exploits implicit inter-class relations and leverages this knowledge to enhance the classification of novel classes. ReLKD comprises three key modules: a target-grained module for learning discriminative representations, a coarse-grained module for capturing hierarchical class relations, and a distillation module for transferring knowledge from the coarse-grained module to refine the target-grained module's representation learning. Extensive experiments on four datasets demonstrate the effectiveness of ReLKD, particularly in scenarios with limited labeled data. The code for ReLKD is available at https://github.com/ZhouF-ECNU/ReLKD.",
            "headline_zh": "提出ReLKD框架，通过隐式类间关系学习与知识蒸馏解决广义类别发现中的新类分类问题",
            "intro_zh": [
                "核心问题：广义类别发现中，未标记数据包含已知和新类，现有方法常忽略类间关系，影响新类分类。",
                "方法要点：ReLKD包含目标粒度模块、粗粒度模块和蒸馏模块，利用隐式类间关系提升表示学习。",
                "实验或效果：在四个数据集上验证有效性，尤其在标记数据有限场景下表现优异。"
            ],
            "tags_zh": [
                "广义类别发现",
                "类间关系学习",
                "知识蒸馏",
                "表示学习",
                "新类分类"
            ],
            "_index": 192
        },
        {
            "title": "Towards Robust Protective Perturbation against DeepFake Face Swapping",
            "authors": [
                "Hengyang Yao",
                "Lin Li",
                "Ke Sun",
                "Jianing Qiu",
                "Huiping Chen"
            ],
            "arxiv_id": "2512.07228v1",
            "summary": "DeepFake face swapping enables highly realistic identity forgeries, posing serious privacy and security risks. A common defence embeds invisible perturbations into images, but these are fragile and often destroyed by basic transformations such as compression or resizing. In this paper, we first conduct a systematic analysis of 30 transformations across six categories and show that protection robustness is highly sensitive to the choice of training transformations, making the standard Expectation over Transformation (EOT) with uniform sampling fundamentally suboptimal. Motivated by this, we propose Expectation Over Learned distribution of Transformation (EOLT), the framework to treat transformation distribution as a learnable component rather than a fixed design choice. Specifically, EOLT employs a policy network that learns to automatically prioritize critical transformations and adaptively generate instance-specific perturbations via reinforcement learning, enabling explicit modeling of defensive bottlenecks while maintaining broad transferability. Extensive experiments demonstrate that our method achieves substantial improvements over state-of-the-art approaches, with 26% higher average robustness and up to 30% gains on challenging transformation categories.",
            "headline_zh": "提出EOLT框架以增强DeepFake人脸交换的防护扰动鲁棒性",
            "intro_zh": [
                "核心问题：现有防护扰动易被压缩或调整大小等基本变换破坏，鲁棒性不足。",
                "方法要点：引入EOLT，通过策略网络学习变换分布，自适应生成实例特定扰动。",
                "实验或效果：在30种变换上平均鲁棒性提升26%，挑战类别增益达30%。"
            ],
            "tags_zh": [
                "DeepFake防护",
                "扰动鲁棒性",
                "变换分布学习",
                "强化学习",
                "人脸交换防御"
            ],
            "_index": 193
        },
        {
            "title": "Clinical Interpretability of Deep Learning Segmentation Through Shapley-Derived Agreement and Uncertainty Metrics",
            "authors": [
                "Tianyi Ren",
                "Daniel Low",
                "Pittra Jaengprajak",
                "Juampablo Heras Rivera",
                "Jacob Ruzevick",
                "Mehmet Kurt"
            ],
            "arxiv_id": "2512.07224v1",
            "summary": "Segmentation is the identification of anatomical regions of interest, such as organs, tissue, and lesions, serving as a fundamental task in computer-aided diagnosis in medical imaging. Although deep learning models have achieved remarkable performance in medical image segmentation, the need for explainability remains critical for ensuring their acceptance and integration in clinical practice, despite the growing research attention in this area. Our approach explored the use of contrast-level Shapley values, a systematic perturbation of model inputs to assess feature importance. While other studies have investigated gradient-based techniques through identifying influential regions in imaging inputs, Shapley values offer a broader, clinically aligned approach, explaining how model performance is fairly attributed to certain imaging contrasts over others. Using the BraTS 2024 dataset, we generated rankings for Shapley values for four MRI contrasts across four model architectures. Two metrics were proposed from the Shapley ranking: agreement between model and ``clinician\" imaging ranking, and uncertainty quantified through Shapley ranking variance across cross-validation folds. Higher-performing cases (Dice \\textgreater0.6) showed significantly greater agreement with clinical rankings. Increased Shapley ranking variance correlated with decreased performance (U-Net: $r=-0.581$). These metrics provide clinically interpretable proxies for model reliability, helping clinicians better understand state-of-the-art segmentation models.",
            "headline_zh": "提出基于Shapley值的协议与不确定性指标，以提升医学图像分割模型的临床可解释性。",
            "intro_zh": [
                "核心问题：深度学习分割模型在医学影像中缺乏临床可解释性，影响临床接受度。",
                "方法要点：利用对比级Shapley值扰动输入，评估特征重要性，并衍生协议与不确定性指标。",
                "实验或效果：在BraTS 2024数据集上验证，高Dice分数案例与临床排名协议更强，不确定性指标与性能负相关。"
            ],
            "tags_zh": [
                "医学图像分割",
                "可解释性",
                "Shapley值",
                "临床评估",
                "不确定性量化"
            ],
            "_index": 194
        },
        {
            "title": "Pay Less Attention to Function Words for Free Robustness of Vision-Language Models",
            "authors": [
                "Qiwei Tian",
                "Chenhao Lin",
                "Zhengyu Zhao",
                "Chao Shen"
            ],
            "arxiv_id": "2512.07222v1",
            "summary": "To address the trade-off between robustness and performance for robust VLM, we observe that function words could incur vulnerability of VLMs against cross-modal adversarial attacks, and propose Function-word De-Attention (FDA) accordingly to mitigate the impact of function words. Similar to differential amplifiers, our FDA calculates the original and the function-word cross-attention within attention heads, and differentially subtracts the latter from the former for more aligned and robust VLMs. Comprehensive experiments include 2 SOTA baselines under 6 different attacks on 2 downstream tasks, 3 datasets, and 3 models. Overall, our FDA yields an average 18/13/53% ASR drop with only 0.2/0.3/0.6% performance drops on the 3 tested models on retrieval, and a 90% ASR drop with a 0.3% performance gain on visual grounding. We demonstrate the scalability, generalization, and zero-shot performance of FDA experimentally, as well as in-depth ablation studies and analysis. Code will be made publicly at https://github.com/michaeltian108/FDA.",
            "headline_zh": "提出函数词去注意力方法以提升视觉语言模型对抗跨模态攻击的鲁棒性",
            "intro_zh": [
                "核心问题：函数词导致视觉语言模型在跨模态对抗攻击下脆弱，需平衡鲁棒性与性能。",
                "方法要点：设计函数词去注意力，在注意力头中计算原始与函数词交叉注意力并差分相减，增强对齐与鲁棒性。",
                "实验效果：在检索和视觉定位任务上显著降低攻击成功率，性能下降极小或略有提升，验证了方法的可扩展性与泛化性。"
            ],
            "tags_zh": [
                "视觉语言模型",
                "对抗鲁棒性",
                "跨模态攻击",
                "注意力机制",
                "函数词处理",
                "零样本性能"
            ],
            "_index": 195
        },
        {
            "title": "Spatiotemporal Calibration and Ground Truth Estimation for High-Precision SLAM Benchmarking in Extended Reality",
            "authors": [
                "Zichao Shu",
                "Shitao Bei",
                "Lijun Li",
                "Zetao Chen"
            ],
            "arxiv_id": "2512.07221v1",
            "summary": "Simultaneous localization and mapping (SLAM) plays a fundamental role in extended reality (XR) applications. As the standards for immersion in XR continue to increase, the demands for SLAM benchmarking have become more stringent. Trajectory accuracy is the key metric, and marker-based optical motion capture (MoCap) systems are widely used to generate ground truth (GT) because of their drift-free and relatively accurate measurements. However, the precision of MoCap-based GT is limited by two factors: the spatiotemporal calibration with the device under test (DUT) and the inherent jitter in the MoCap measurements. These limitations hinder accurate SLAM benchmarking, particularly for key metrics like rotation error and inter-frame jitter, which are critical for immersive XR experiences. This paper presents a novel continuous-time maximum likelihood estimator to address these challenges. The proposed method integrates auxiliary inertial measurement unit (IMU) data to compensate for MoCap jitter. Additionally, a variable time synchronization method and a pose residual based on screw congruence constraints are proposed, enabling precise spatiotemporal calibration across multiple sensors and the DUT. Experimental results demonstrate that our approach outperforms existing methods, achieving the precision necessary for comprehensive benchmarking of state-of-the-art SLAM algorithms in XR applications. Furthermore, we thoroughly validate the practicality of our method by benchmarking several leading XR devices and open-source SLAM algorithms. The code is publicly available at https://github.com/ylab-xrpg/xr-hpgt.",
            "headline_zh": "提出连续时间最大似然估计器，结合IMU补偿运动捕捉抖动，实现XR中SLAM算法的高精度基准测试。",
            "intro_zh": [
                "核心问题：运动捕捉系统存在时空校准误差和固有抖动，限制SLAM基准测试精度，影响XR沉浸体验。",
                "方法要点：集成IMU数据补偿抖动，提出可变时间同步和基于螺旋同余约束的位姿残差，实现多传感器精确校准。",
                "实验或效果：方法优于现有技术，验证了在XR设备和开源SLAM算法基准测试中的实用性和高精度。"
            ],
            "tags_zh": [
                "SLAM基准测试",
                "时空校准",
                "运动捕捉抖动补偿",
                "扩展现实",
                "连续时间估计",
                "多传感器融合"
            ],
            "_index": 196
        },
        {
            "title": "Characterizing Lane-Changing Behavior in Mixed Traffic",
            "authors": [
                "Sungyong Chung",
                "Alireza Talebpour",
                "Samer H. Hamdar"
            ],
            "arxiv_id": "2512.07219v1",
            "summary": "Characterizing and understanding lane-changing behavior in the presence of automated vehicles (AVs) is crucial to ensuring safety and efficiency in mixed traffic. Accordingly, this study aims to characterize the interactions between the lane-changing vehicle (active vehicle) and the vehicle directly impacted by the maneuver in the target lane (passive vehicle). Utilizing real-world trajectory data from the Waymo Open Motion Dataset (WOMD), this study explores patterns in lane-changing behavior and provides insight into how these behaviors evolve under different AV market penetration rates (MPRs). In particular, we propose a game-theoretic framework to analyze cooperative and defective behaviors in mixed traffic, applied to the 7,636 observed lane-changing events in the WOMD. First, we utilize k-means clustering to classify vehicles as cooperative or defective, revealing that the proportions of cooperative AVs are higher than those of HDVs in both active and passive roles. Next, we jointly estimate the utilities of active and passive vehicles to model their behaviors using the quantal response equilibrium framework. Empirical payoff tables are then constructed based on these utilities. Using these payoffs, we analyze the presence of social dilemmas and examine the evolution of cooperative behaviors using evolutionary game theory. Our results reveal the presence of social dilemmas in approximately 4% and 11% of lane-changing events for active and passive vehicles, respectively, with most classified as Stag Hunt or Prisoner's Dilemma (Chicken Game rarely observed). Moreover, the Monte Carlo simulation results show that repeated lane-changing interactions consistently lead to increased cooperative behavior over time, regardless of the AV penetration rate.",
            "headline_zh": "提出博弈论框架以分析混合交通中换道行为的合作与缺陷模式",
            "intro_zh": [
                "核心问题：混合交通中自动驾驶车辆与人类驾驶车辆换道交互行为的特征与演化",
                "方法要点：基于真实轨迹数据，应用聚类和量化响应均衡框架估计车辆效用并构建收益表",
                "实验或效果：揭示约4%和11%换道事件存在社会困境，模拟显示重复交互促进合作行为"
            ],
            "tags_zh": [
                "混合交通",
                "换道行为",
                "博弈论",
                "自动驾驶",
                "社会困境",
                "进化博弈"
            ],
            "_index": 197
        },
        {
            "title": "NeSTR: A Neuro-Symbolic Abductive Framework for Temporal Reasoning in Large Language Models",
            "authors": [
                "Feng Liang",
                "Weixin Zeng",
                "Runhao Zhao",
                "Xiang Zhao"
            ],
            "arxiv_id": "2512.07218v1",
            "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of natural language processing tasks. However, temporal reasoning, particularly under complex temporal constraints, remains a major challenge. To this end, existing approaches have explored symbolic methods, which encode temporal structure explicitly, and reflective mechanisms, which revise reasoning errors through multi-step inference. Nonetheless, symbolic approaches often underutilize the reasoning capabilities of LLMs, while reflective methods typically lack structured temporal representations, which can result in inconsistent or hallucinated reasoning. As a result, even when the correct temporal context is available, LLMs may still misinterpret or misapply time-related information, leading to incomplete or inaccurate answers. To address these limitations, in this work, we propose Neuro-Symbolic Temporal Reasoning (NeSTR), a novel framework that integrates structured symbolic representations with hybrid reflective reasoning to enhance the temporal sensitivity of LLM inference. NeSTR preserves explicit temporal relations through symbolic encoding, enforces logical consistency via verification, and corrects flawed inferences using abductive reflection. Extensive experiments on diverse temporal question answering benchmarks demonstrate that NeSTR achieves superior zero-shot performance and consistently improves temporal reasoning without any fine-tuning, showcasing the advantage of neuro-symbolic integration in enhancing temporal understanding in large language models.",
            "headline_zh": "提出NeSTR框架以增强大语言模型在复杂时间约束下的推理能力",
            "intro_zh": [
                "核心问题：大语言模型在复杂时间推理中易产生不一致或幻觉，现有方法未能充分利用其推理能力或缺乏结构化表示。",
                "方法要点：NeSTR整合符号编码与混合反思推理，通过符号表示保留时间关系、验证逻辑一致性、使用溯因反思修正错误。",
                "实验或效果：在多个时间问答基准上，NeSTR实现零样本性能提升，无需微调即增强时间推理，展示神经符号集成的优势。"
            ],
            "tags_zh": [
                "时间推理",
                "神经符号集成",
                "溯因反思",
                "零样本学习",
                "大语言模型",
                "问答基准"
            ],
            "_index": 198
        },
        {
            "title": "MUSE: A Simple Yet Effective Multimodal Search-Based Framework for Lifelong User Interest Modeling",
            "authors": [
                "Bin Wu",
                "Feifan Yang",
                "Zhangming Chan",
                "Yu-Ran Gu",
                "Jiawei Feng",
                "Chao Yi",
                "Xiang-Rong Sheng",
                "Han Zhu",
                "Jian Xu",
                "Mang Ye",
                "Bo Zheng"
            ],
            "arxiv_id": "2512.07216v1",
            "summary": "Lifelong user interest modeling is crucial for industrial recommender systems, yet existing approaches rely predominantly on ID-based features, suffering from poor generalization on long-tail items and limited semantic expressiveness. While recent work explores multimodal representations for behavior retrieval in the General Search Unit (GSU), they often neglect multimodal integration in the fine-grained modeling stage -- the Exact Search Unit (ESU). In this work, we present a systematic analysis of how to effectively leverage multimodal signals across both stages of the two-stage lifelong modeling framework. Our key insight is that simplicity suffices in the GSU: lightweight cosine similarity with high-quality multimodal embeddings outperforms complex retrieval mechanisms. In contrast, the ESU demands richer multimodal sequence modeling and effective ID-multimodal fusion to unlock its full potential. Guided by these principles, we propose MUSE, a simple yet effective multimodal search-based framework. MUSE has been deployed in Taobao display advertising system, enabling 100K-length user behavior sequence modeling and delivering significant gains in top-line metrics with negligible online latency overhead. To foster community research, we share industrial deployment practices and open-source the first large-scale dataset featuring ultra-long behavior sequences paired with high-quality multimodal embeddings. Our code and data is available at https://taobao-mm.github.io.",
            "headline_zh": "提出MUSE框架，通过两阶段多模态搜索解决推荐系统中终身用户兴趣建模的泛化与语义表达问题。",
            "intro_zh": [
                "核心问题：现有方法依赖ID特征，在长尾物品上泛化差且语义表达有限。",
                "方法要点：在GSU阶段使用轻量余弦相似度，在ESU阶段结合多模态序列建模与ID-多模态融合。",
                "实验或效果：部署于淘宝广告系统，支持10万长度行为序列建模，显著提升指标且在线延迟可忽略。"
            ],
            "tags_zh": [
                "终身用户兴趣建模",
                "多模态搜索",
                "推荐系统",
                "行为序列建模",
                "工业部署"
            ],
            "_index": 199
        },
        {
            "title": "VFM-VLM: Vision Foundation Model and Vision Language Model based Visual Comparison for 3D Pose Estimation",
            "authors": [
                "Md Selim Sarowar",
                "Sungho Kim"
            ],
            "arxiv_id": "2512.07215v1",
            "summary": "Vision Foundation Models (VFMs) and Vision Language Models (VLMs) have revolutionized computer vision by providing rich semantic and geometric representations. This paper presents a comprehensive visual comparison between CLIP based and DINOv2 based approaches for 3D pose estimation in hand object grasping scenarios. We evaluate both models on the task of 6D object pose estimation and demonstrate their complementary strengths: CLIP excels in semantic understanding through language grounding, while DINOv2 provides superior dense geometric features. Through extensive experiments on benchmark datasets, we show that CLIP based methods achieve better semantic consistency, while DINOv2 based approaches demonstrate competitive performance with enhanced geometric precision. Our analysis provides insights for selecting appropriate vision models for robotic manipulation and grasping, picking applications.",
            "headline_zh": "比较CLIP与DINOv2在抓取场景3D姿态估计中的视觉基础模型与视觉语言模型方法",
            "intro_zh": [
                "核心问题：评估视觉基础模型和视觉语言模型在抓取场景6D物体姿态估计中的性能差异",
                "方法要点：对比CLIP基于语言接地的语义理解和DINOv2的密集几何特征提取",
                "实验或效果：实验显示CLIP在语义一致性上更优，DINOv2在几何精度上表现竞争性"
            ],
            "tags_zh": [
                "3D姿态估计",
                "视觉基础模型",
                "视觉语言模型",
                "抓取场景",
                "6D姿态估计"
            ],
            "_index": 200
        },
        {
            "title": "Sample from What You See: Visuomotor Policy Learning via Diffusion Bridge with Observation-Embedded Stochastic Differential Equation",
            "authors": [
                "Zhaoyang Liu",
                "Mokai Pan",
                "Zhongyi Wang",
                "Kaizhen Zhu",
                "Haotao Lu",
                "Jingya Wang",
                "Ye Shi"
            ],
            "arxiv_id": "2512.07212v1",
            "summary": "Imitation learning with diffusion models has advanced robotic control by capturing multi-modal action distributions. However, existing approaches typically treat observations as high-level conditioning inputs to the denoising network, rather than integrating them into the stochastic dynamics of the diffusion process itself. As a result, sampling must begin from random Gaussian noise, weakening the coupling between perception and control and often yielding suboptimal performance. We introduce BridgePolicy, a generative visuomotor policy that explicitly embeds observations within the stochastic differential equation via a diffusion-bridge formulation. By constructing an observation-informed trajectory, BridgePolicy enables sampling to start from a rich, informative prior rather than random noise, substantially improving precision and reliability in control. A key challenge is that classical diffusion bridges connect distributions with matched dimensionality, whereas robotic observations are heterogeneous and multi-modal and do not naturally align with the action space. To address this, we design a multi-modal fusion module and a semantic aligner that unify visual and state inputs and align observation and action representations, making the bridge applicable to heterogeneous robot data. Extensive experiments across 52 simulation tasks on three benchmarks and five real-world tasks demonstrate that BridgePolicy consistently outperforms state-of-the-art generative policies.",
            "headline_zh": "提出BridgePolicy，通过扩散桥嵌入观测的随机微分方程，以提升机器人视觉运动策略的精确性和可靠性。",
            "intro_zh": [
                "现有扩散模型模仿学习将观测作为去噪网络的高层条件输入，而非融入扩散过程的随机动态，导致采样从随机高斯噪声开始，感知与控制耦合弱。",
                "BridgePolicy通过扩散桥公式将观测嵌入随机微分方程，构建观测信息轨迹，使采样从丰富先验开始，改善控制性能。",
                "在52个模拟任务和5个真实世界任务中，BridgePolicy优于现有生成策略，未知具体基准名称。"
            ],
            "tags_zh": [
                "扩散模型模仿学习",
                "视觉运动策略",
                "随机微分方程",
                "多模态融合",
                "机器人控制"
            ],
            "_index": 201
        },
        {
            "title": "Object Pose Distribution Estimation for Determining Revolution and Reflection Uncertainty in Point Clouds",
            "authors": [
                "Frederik Hagelskjær",
                "Dimitrios Arapis",
                "Steffen Madsen",
                "Thorbjørn Mosekjær Iversen"
            ],
            "arxiv_id": "2512.07211v1",
            "summary": "Object pose estimation is crucial to robotic perception and typically provides a single-pose estimate. However, a single estimate cannot capture pose uncertainty deriving from visual ambiguity, which can lead to unreliable behavior. Existing pose distribution methods rely heavily on color information, often unavailable in industrial settings.\n  We propose a novel neural network-based method for estimating object pose uncertainty using only 3D colorless data. To the best of our knowledge, this is the first approach that leverages deep learning for pose distribution estimation without relying on RGB input. We validate our method in a real-world bin picking scenario with objects of varying geometric ambiguity. Our current implementation focuses on symmetries in reflection and revolution, but the framework is extendable to full SE(3) pose distribution estimation. Source code available at opde3d.github.io",
            "headline_zh": "提出基于神经网络的物体姿态分布估计方法，仅用3D无色数据解决工业场景中的姿态不确定性",
            "intro_zh": [
                "核心问题：现有姿态估计方法依赖颜色信息，无法处理工业场景中无色数据的视觉模糊性。",
                "方法要点：利用深度学习从3D点云估计姿态分布，专注于反射和旋转对称性，可扩展至完整SE(3)。",
                "实验或效果：在真实世界拣选场景中验证，处理几何模糊物体，代码已开源。"
            ],
            "tags_zh": [
                "姿态分布估计",
                "3D点云",
                "深度学习",
                "工业机器人",
                "对称性处理",
                "不确定性建模"
            ],
            "_index": 202
        },
        {
            "title": "Coherent Audio-Visual Editing via Conditional Audio Generation Following Video Edits",
            "authors": [
                "Masato Ishii",
                "Akio Hayakawa",
                "Takashi Shibuya",
                "Yuki Mitsufuji"
            ],
            "arxiv_id": "2512.07209v1",
            "summary": "We introduce a novel pipeline for joint audio-visual editing that enhances the coherence between edited video and its accompanying audio. Our approach first applies state-of-the-art video editing techniques to produce the target video, then performs audio editing to align with the visual changes. To achieve this, we present a new video-to-audio generation model that conditions on the source audio, target video, and a text prompt. We extend the model architecture to incorporate conditional audio input and propose a data augmentation strategy that improves training efficiency. Furthermore, our model dynamically adjusts the influence of the source audio based on the complexity of the edits, preserving the original audio structure where possible. Experimental results demonstrate that our method outperforms existing approaches in maintaining audio-visual alignment and content integrity.",
            "headline_zh": "提出基于视频编辑的音频生成模型以增强音视频编辑一致性",
            "intro_zh": [
                "核心问题：视频编辑后音频与视觉变化不协调，影响音视频一致性。",
                "方法要点：构建视频到音频生成模型，结合源音频、目标视频和文本提示进行条件生成。",
                "实验或效果：实验显示方法在音视频对齐和内容完整性上优于现有方法。"
            ],
            "tags_zh": [
                "音视频编辑",
                "条件音频生成",
                "视频到音频模型",
                "数据增强",
                "动态音频调整"
            ],
            "_index": 203
        },
        {
            "title": "Geometric Prior-Guided Federated Prompt Calibration",
            "authors": [
                "Fei Luo",
                "Ziwei Zhao",
                "Mingxuan Wang",
                "Duoyang Li",
                "Zhe Qian",
                "Jiayi Tuo",
                "Chenyue Zhou",
                "Yanbiao Ma"
            ],
            "arxiv_id": "2512.07208v1",
            "summary": "Federated Prompt Learning (FPL) offers a parameter-efficient solution for collaboratively training large models, but its performance is severely hindered by data heterogeneity, which causes locally trained prompts to become biased. Existing methods, focusing on aggregation or regularization, fail to address this root cause of local training bias. To this end, we propose Geometry-Guided Text Prompt Calibration (GGTPC), a novel framework that directly corrects this bias by providing clients with a global geometric prior. This prior, representing the shape of the global data distribution derived from the covariance matrix, is reconstructed on the server in a privacy-preserving manner. Clients then use a novel Geometry-Prior Calibration Layer (GPCL) to align their local feature distributions with this global prior during training. Extensive experiments show GGTPC's effectiveness. On the label-skewed CIFAR-100 dataset ($β$=0.1), it outperforms the state-of-the-art by 2.15\\%. Under extreme skew ($β$=0.01), it improves upon the baseline by 9.17\\%. Furthermore, as a plug-and-play module on the domain-skewed Office-Home dataset, it boosts FedAvg's performance by 4.60\\%. These results demonstrate that GGTPC effectively mitigates data heterogeneity by correcting the fundamental local training bias, serving as a versatile module to enhance various FL algorithms.",
            "headline_zh": "提出几何先验引导的联邦提示校准以解决数据异构性导致的本地训练偏差问题",
            "intro_zh": [
                "核心问题：联邦提示学习中数据异构性导致本地提示训练偏差，现有方法未能根治此问题",
                "方法要点：通过服务器重构全局几何先验，客户端使用几何先验校准层对齐本地特征分布",
                "实验或效果：在标签偏斜CIFAR-100上超越SOTA 2.15%，极端偏斜下提升9.17%，作为插件模块提升FedAvg性能4.60%"
            ],
            "tags_zh": [
                "联邦学习",
                "提示学习",
                "数据异构性",
                "几何先验",
                "特征校准",
                "隐私保护"
            ],
            "_index": 204
        },
        {
            "title": "AutoLugano: A Deep Learning Framework for Fully Automated Lymphoma Segmentation and Lugano Staging on FDG-PET/CT",
            "authors": [
                "Boyang Pan",
                "Zeyu Zhang",
                "Hongyu Meng",
                "Bin Cui",
                "Yingying Zhang",
                "Wenli Hou",
                "Junhao Li",
                "Langdi Zhong",
                "Xiaoxiao Chen",
                "Xiaoyu Xu",
                "Changjin Zuo",
                "Chao Cheng",
                "Nan-Jie Gong"
            ],
            "arxiv_id": "2512.07206v1",
            "summary": "Purpose: To develop a fully automated deep learning system, AutoLugano, for end-to-end lymphoma classification by performing lesion segmentation, anatomical localization, and automated Lugano staging from baseline FDG-PET/CT scans. Methods: The AutoLugano system processes baseline FDG-PET/CT scans through three sequential modules:(1) Anatomy-Informed Lesion Segmentation, a 3D nnU-Net model, trained on multi-channel inputs, performs automated lesion detection (2) Atlas-based Anatomical Localization, which leverages the TotalSegmentator toolkit to map segmented lesions to 21 predefined lymph node regions using deterministic anatomical rules; and (3) Automated Lugano Staging, where the spatial distribution of involved regions is translated into Lugano stages and therapeutic groups (Limited vs. Advanced Stage).The system was trained on the public autoPET dataset (n=1,007) and externally validated on an independent cohort of 67 patients. Performance was assessed using accuracy, sensitivity, specificity, F1-scorefor regional involvement detection and staging agreement. Results: On the external validation set, the proposed model demonstrated robust performance, achieving an overall accuracy of 88.31%, sensitivity of 74.47%, Specificity of 94.21% and an F1-score of 80.80% for regional involvement detection,outperforming baseline models. Most notably, for the critical clinical task of therapeutic stratification (Limited vs. Advanced Stage), the system achieved a high accuracy of 85.07%, with a specificity of 90.48% and a sensitivity of 82.61%.Conclusion: AutoLugano represents the first fully automated, end-to-end pipeline that translates a single baseline FDG-PET/CT scan into a complete Lugano stage. This study demonstrates its strong potential to assist in initial staging, treatment stratification, and supporting clinical decision-making.",
            "headline_zh": "提出AutoLugano框架，通过FDG-PET/CT扫描实现淋巴瘤自动分割与卢加诺分期",
            "intro_zh": [
                "核心问题：淋巴瘤诊断需从FDG-PET/CT扫描中自动分割病灶、定位解剖区域并完成卢加诺分期。",
                "方法要点：系统包含三个模块：基于3D nnU-Net的病灶分割、基于图谱的解剖定位和自动分期转换。",
                "实验或效果：在外部验证集上，区域受累检测准确率88.31%，治疗分层准确率85.07%，表现稳健。"
            ],
            "tags_zh": [
                "淋巴瘤分割",
                "卢加诺分期",
                "FDG-PET/CT",
                "深度学习",
                "自动诊断",
                "nnU-Net"
            ],
            "_index": 205
        },
        {
            "title": "MMRPT: MultiModal Reinforcement Pre-Training via Masked Vision-Dependent Reasoning",
            "authors": [
                "Xuhui Zheng",
                "Kang An",
                "Ziliang Wang",
                "Yuhang Wang",
                "Faqiang Qian",
                "Yichao Wu"
            ],
            "arxiv_id": "2512.07203v1",
            "summary": "Multimodal pre-training remains constrained by the descriptive bias of image-caption pairs, leading models to favor surface linguistic cues over grounded visual understanding. We introduce MMRPT, a masked multimodal reinforcement pre-training framework that strengthens visual reasoning in MLLMs. We are the first to incorporate reinforcement learning directly into the pre-training of large vision-language models, enabling learning signals that reward visual grounding rather than caption imitation. MMRPT constructs masked multimodal data by estimating sentence-level visual dependency via attention over visual tokens and masking highly vision-dependent segments; the model reconstructs these spans through vision-grounded reasoning guided by a semantic-visual reward. Experiments show consistent zero-shot gains across diverse benchmarks and substantially improved robustness under supervised fine-tuning, demonstrating that reinforcement-driven masked reasoning provides a more reliable and generalizable pre-training objective for multimodal models.",
            "headline_zh": "提出MMRPT框架，通过掩码多模态强化预训练增强视觉推理，解决多模态预训练中的描述性偏差问题。",
            "intro_zh": [
                "核心问题：多模态预训练受图像-文本对描述性偏差限制，模型依赖表面语言线索而非视觉理解。",
                "方法要点：首次将强化学习直接融入预训练，通过掩码视觉依赖片段并基于语义-视觉奖励重构，奖励视觉基础而非文本模仿。",
                "实验或效果：在零样本基准测试中一致提升，监督微调下显著增强鲁棒性，证明强化驱动的掩码推理提供更可靠预训练目标。"
            ],
            "tags_zh": [
                "多模态预训练",
                "强化学习",
                "视觉推理",
                "掩码学习",
                "零样本学习",
                "鲁棒性增强"
            ],
            "_index": 206
        },
        {
            "title": "Understanding Diffusion Models via Code Execution",
            "authors": [
                "Cheng Yu"
            ],
            "arxiv_id": "2512.07201v1",
            "summary": "Diffusion models have achieved remarkable performance in generative modeling, yet their theoretical foundations are often intricate, and the gap between mathematical formulations in papers and practical open-source implementations can be difficult to bridge. Existing tutorials primarily focus on deriving equations, offering limited guidance on how diffusion models actually operate in code. To address this, we present a concise implementation of approximately 300 lines that explains diffusion models from a code-execution perspective. Our minimal example preserves the essential components -- including forward diffusion, reverse sampling, the noise-prediction network, and the training loop -- while removing unnecessary engineering details. This technical report aims to provide researchers with a clear, implementation-first understanding of how diffusion models work in practice and how code and theory correspond. Our code and pre-trained models are available at: https://github.com/disanda/GM/tree/main/DDPM-DDIM-ClassifierFree.",
            "headline_zh": "提出基于代码执行的扩散模型简明实现，以弥合理论与实践的差距。",
            "intro_zh": [
                "核心问题：扩散模型理论复杂，论文数学公式与开源实现之间存在理解鸿沟。",
                "方法要点：提供约300行代码的简洁实现，涵盖前向扩散、反向采样、噪声预测网络和训练循环。",
                "实验或效果：通过代码优先视角，帮助研究者清晰理解扩散模型的实际运作和理论与代码对应关系。"
            ],
            "tags_zh": [
                "扩散模型",
                "代码实现",
                "理论实践差距",
                "生成建模",
                "噪声预测网络"
            ],
            "_index": 207
        },
        {
            "title": "Less is More: Non-uniform Road Segments are Efficient for Bus Arrival Prediction",
            "authors": [
                "Zhen Huang",
                "Jiaxin Deng",
                "Jiayu Xu",
                "Junbiao Pang",
                "Haitao Yu"
            ],
            "arxiv_id": "2512.07200v1",
            "summary": "In bus arrival time prediction, the process of organizing road infrastructure network data into homogeneous entities is known as segmentation. Segmenting a road network is widely recognized as the first and most critical step in developing an arrival time prediction system, particularly for auto-regressive-based approaches. Traditional methods typically employ a uniform segmentation strategy, which fails to account for varying physical constraints along roads, such as road conditions, intersections, and points of interest, thereby limiting prediction efficiency. In this paper, we propose a Reinforcement Learning (RL)-based approach to efficiently and adaptively learn non-uniform road segments for arrival time prediction. Our method decouples the prediction process into two stages: 1) Non-uniform road segments are extracted based on their impact scores using the proposed RL framework; and 2) A linear prediction model is applied to the selected segments to make predictions. This method ensures optimal segment selection while maintaining computational efficiency, offering a significant improvement over traditional uniform approaches. Furthermore, our experimental results suggest that the linear approach can even achieve better performance than more complex methods. Extensive experiments demonstrate the superiority of the proposed method, which not only enhances efficiency but also improves learning performance on large-scale benchmarks. The dataset and the code are publicly accessible at: https://github.com/pangjunbiao/Less-is-More.",
            "headline_zh": "提出基于强化学习的非均匀路段分割方法以提升公交到站时间预测效率",
            "intro_zh": [
                "传统均匀路段分割忽略道路物理约束，限制预测效率",
                "方法分两阶段：强化学习提取非均匀路段，线性模型预测",
                "实验显示方法在效率和性能上优于传统方法，线性模型表现佳"
            ],
            "tags_zh": [
                "公交到站时间预测",
                "非均匀路段分割",
                "强化学习",
                "线性预测模型",
                "道路网络优化"
            ],
            "_index": 208
        },
        {
            "title": "Generating Storytelling Images with Rich Chains-of-Reasoning",
            "authors": [
                "Xiujie Song",
                "Qi Jia",
                "Shota Watanabe",
                "Xiaoyi Pang",
                "Ruijie Chen",
                "Mengyue Wu",
                "Kenny Q. Zhu"
            ],
            "arxiv_id": "2512.07198v1",
            "summary": "An image can convey a compelling story by presenting rich, logically connected visual clues. These connections form Chains-of-Reasoning (CoRs) within the image, enabling viewers to infer events, causal relationships, and other information, thereby understanding the underlying story. In this paper, we focus on these semantically rich images and define them as Storytelling Images. Such images have diverse applications beyond illustration creation and cognitive screening, leveraging their ability to convey multi-layered information visually and inspire active interpretation. However, due to their complex semantic nature, Storytelling Images are inherently challenging to create, and thus remain relatively scarce. To address this challenge, we introduce the Storytelling Image Generation task, which explores how generative AI models can be leveraged to create such images. Specifically, we propose a two-stage pipeline, StorytellingPainter, which combines the creative reasoning abilities of Large Language Models (LLMs) with the visual synthesis capabilities of Text-to-Image (T2I) models to generate Storytelling Images. Alongside this pipeline, we develop a dedicated evaluation framework comprising three main evaluators: a Semantic Complexity Evaluator, a KNN-based Diversity Evaluator and a Story-Image Alignment Evaluator. Given the critical role of story generation in the Storytelling Image Generation task and the performance disparity between open-source and proprietary LLMs, we further explore tailored training strategies to reduce this gap, resulting in a series of lightweight yet effective models named Mini-Storytellers. Experimental results demonstrate the feasibility and effectiveness of our approaches. The code is available at https://github.com/xiujiesong/StorytellingImageGeneration.",
            "headline_zh": "提出StorytellingPainter两阶段流水线，结合大语言模型与文生图模型生成富含推理链的故事图像",
            "intro_zh": [
                "核心问题：故事图像语义复杂且稀缺，生成任务具挑战性",
                "方法要点：利用LLMs进行创意推理，T2I模型视觉合成，构建两阶段生成流水线",
                "实验或效果：开发评估框架验证方法可行性，训练Mini-Storytellers模型缩小开源与专有LLMs差距"
            ],
            "tags_zh": [
                "故事图像生成",
                "推理链",
                "大语言模型",
                "文生图模型",
                "评估框架",
                "轻量模型训练"
            ],
            "_index": 209
        },
        {
            "title": "SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting",
            "authors": [
                "Seokhyun Youn",
                "Soohyun Lee",
                "Geonho Kim",
                "Weeyoung Kwon",
                "Sung-Ho Bae",
                "Jihyong Oh"
            ],
            "arxiv_id": "2512.07197v1",
            "summary": "3D Gaussian Splatting (3DGS) has emerged as a powerful explicit representation enabling real-time, high-fidelity 3D reconstruction and novel view synthesis. However, its practical use is hindered by the massive memory and computational demands required to store and render millions of Gaussians. These challenges become even more severe in 4D dynamic scenes. To address these issues, the field of Efficient Gaussian Splatting has rapidly evolved, proposing methods that reduce redundancy while preserving reconstruction quality. This survey provides the first unified overview of efficient 3D and 4D Gaussian Splatting techniques. For both 3D and 4D settings, we systematically categorize existing methods into two major directions, Parameter Compression and Restructuring Compression, and comprehensively summarize the core ideas and methodological trends within each category. We further cover widely used datasets, evaluation metrics, and representative benchmark comparisons. Finally, we discuss current limitations and outline promising research directions toward scalable, compact, and real-time Gaussian Splatting for both static and dynamic 3D scene representation.",
            "headline_zh": "综述高效静态与动态高斯泼溅的紧凑性与压缩方法",
            "intro_zh": [
                "核心问题：3D高斯泼溅内存与计算需求大，动态场景更严重，阻碍实际应用。",
                "方法要点：系统分类现有方法为参数压缩和结构压缩，覆盖3D和4D场景。",
                "实验或效果：涵盖数据集、评估指标和基准比较，讨论局限与未来方向。"
            ],
            "tags_zh": [
                "高斯泼溅",
                "参数压缩",
                "结构压缩",
                "动态场景",
                "实时渲染",
                "3D重建"
            ],
            "_index": 210
        },
        {
            "title": "MASim: Multilingual Agent-Based Simulation for Social Science",
            "authors": [
                "Xuan Zhang",
                "Wenxuan Zhang",
                "Anxu Wang",
                "See-Kiong Ng",
                "Yang Deng"
            ],
            "arxiv_id": "2512.07195v1",
            "summary": "Multi-agent role-playing has recently shown promise for studying social behavior with language agents, but existing simulations are mostly monolingual and fail to model cross-lingual interaction, an essential property of real societies. We introduce MASim, the first multilingual agent-based simulation framework that supports multi-turn interaction among generative agents with diverse sociolinguistic profiles. MASim offers two key analyses: (i) global public opinion modeling, by simulating how attitudes toward open-domain hypotheses evolve across languages and cultures, and (ii) media influence and information diffusion, via autonomous news agents that dynamically generate content and shape user behavior. To instantiate simulations, we construct the MAPS benchmark, which combines survey questions and demographic personas drawn from global population distributions. Experiments on calibration, sensitivity, consistency, and cultural case studies show that MASim reproduces sociocultural phenomena and highlights the importance of multilingual simulation for scalable, controlled computational social science.",
            "headline_zh": "提出MASim框架以支持多语言代理模拟，用于计算社会科学研究",
            "intro_zh": [
                "现有模拟多为单语言，无法建模跨语言交互这一社会关键属性",
                "MASim支持多轮交互，提供全球舆论建模和媒体影响分析功能",
                "通过MAPS基准实验验证了模拟的校准性、敏感性和文化现象再现能力"
            ],
            "tags_zh": [
                "多语言代理模拟",
                "计算社会科学",
                "舆论建模",
                "信息扩散",
                "生成代理",
                "社会行为研究"
            ],
            "_index": 211
        },
        {
            "title": "HVQ-CGIC: Enabling Hyperprior Entropy Modeling for VQ-Based Controllable Generative Image Compression",
            "authors": [
                "Niu Yi",
                "Xu Tianyi",
                "Ma Mingming",
                "Wang Xinkun"
            ],
            "arxiv_id": "2512.07192v1",
            "summary": "Generative learned image compression methods using Vector Quantization (VQ) have recently shown impressive potential in balancing distortion and perceptual quality. However, these methods typically estimate the entropy of VQ indices using a static, global probability distribution, which fails to adapt to the specific content of each image. This non-adaptive approach leads to untapped bitrate potential and challenges in achieving flexible rate control. To address this challenge, we introduce a Controllable Generative Image Compression framework based on a VQ Hyperprior, termed HVQ-CGIC. HVQ-CGIC rigorously derives the mathematical foundation for introducing a hyperprior to the VQ indices entropy model. Based on this foundation, through novel loss design, to our knowledge, this framework is the first to introduce RD balance and control into vector quantization-based Generative Image Compression. Cooperating with a lightweight hyper-prior estimation network, HVQ-CGIC achieves a significant advantage in rate-distortion (RD) performance compared to current state-of-the-art (SOTA) generative compression methods. On the Kodak dataset, we achieve the same LPIPS as Control-GIC, CDC and HiFiC with an average of 61.3% fewer bits. We posit that HVQ-CGIC has the potential to become a foundational component for VQGAN-based image compression, analogous to the integral role of the HyperPrior framework in neural image compression.",
            "headline_zh": "提出基于VQ超先验的可控生成图像压缩框架，以解决VQ索引熵建模的非自适应问题。",
            "intro_zh": [
                "核心问题：VQ生成压缩中索引熵模型使用静态全局分布，无法适应图像内容，限制比特率潜力。",
                "方法要点：引入超先验到VQ索引熵模型，通过新颖损失设计实现率失真平衡与控制。",
                "实验或效果：在Kodak数据集上，相比SOTA方法，以61.3%更少比特实现相同LPIPS。"
            ],
            "tags_zh": [
                "生成图像压缩",
                "向量量化",
                "超先验熵建模",
                "率失真控制",
                "VQGAN压缩"
            ],
            "_index": 212
        },
        {
            "title": "RefLSM: Linearized Structural-Prior Reflectance Model for Medical Image Segmentation and Bias-Field Correction",
            "authors": [
                "Wenqi Zhao",
                "Jiacheng Sang",
                "Fenghua Cheng",
                "Yonglu Shu",
                "Dong Li",
                "Xiaofeng Yang"
            ],
            "arxiv_id": "2512.07191v1",
            "summary": "Medical image segmentation remains challenging due to intensity inhomogeneity, noise, blurred boundaries, and irregular structures. Traditional level set methods, while effective in certain cases, often depend on approximate bias field estimations and therefore struggle under severe non-uniform imaging conditions. To address these limitations, we propose a novel variational Reflectance-based Level Set Model (RefLSM), which explicitly integrates Retinex-inspired reflectance decomposition into the segmentation framework. By decomposing the observed image into reflectance and bias field components, RefLSM directly segments the reflectance, which is invariant to illumination and preserves fine structural details. Building on this foundation, we introduce two key innovations for enhanced precision and robustness. First, a linear structural prior steers the smoothed reflectance gradients toward a data-driven reference, providing reliable geometric guidance in noisy or low-contrast scenes. Second, a relaxed binary level-set is embedded in RefLSM and enforced via convex relaxation and sign projection, yielding stable evolution and avoiding reinitialization-induced diffusion. The resulting variational problem is solved efficiently using an ADMM-based optimization scheme. Extensive experiments on multiple medical imaging datasets demonstrate that RefLSM achieves superior segmentation accuracy, robustness, and computational efficiency compared to state-of-the-art level set methods.",
            "headline_zh": "提出RefLSM模型，通过反射率分解和线性结构先验解决医学图像分割中的强度不均匀和噪声问题。",
            "intro_zh": [
                "核心问题：医学图像分割受强度不均匀、噪声、模糊边界和不规则结构影响，传统水平集方法在严重非均匀成像条件下效果有限。",
                "方法要点：基于Retinex的反射率分解，结合线性结构先验和松弛二元水平集，通过ADMM优化实现稳定分割。",
                "实验或效果：在多个医学影像数据集上验证，RefLSM在分割精度、鲁棒性和计算效率上优于先进水平集方法。"
            ],
            "tags_zh": [
                "医学图像分割",
                "反射率分解",
                "水平集方法",
                "结构先验",
                "偏置场校正",
                "ADMM优化"
            ],
            "_index": 213
        },
        {
            "title": "Integrating Multi-scale and Multi-filtration Topological Features for Medical Image Classification",
            "authors": [
                "Pengfei Gu",
                "Huimin Li",
                "Haoteng Tang",
                "Dongkuan",
                "Xu",
                "Erik Enriquez",
                "DongChul Kim",
                "Bin Fu",
                "Danny Z. Chen"
            ],
            "arxiv_id": "2512.07190v1",
            "summary": "Modern deep neural networks have shown remarkable performance in medical image classification. However, such networks either emphasize pixel-intensity features instead of fundamental anatomical structures (e.g., those encoded by topological invariants), or they capture only simple topological features via single-parameter persistence. In this paper, we propose a new topology-guided classification framework that extracts multi-scale and multi-filtration persistent topological features and integrates them into vision classification backbones. For an input image, we first compute cubical persistence diagrams (PDs) across multiple image resolutions/scales. We then develop a ``vineyard'' algorithm that consolidates these PDs into a single, stable diagram capturing signatures at varying granularities, from global anatomy to subtle local irregularities that may indicate early-stage disease. To further exploit richer topological representations produced by multiple filtrations, we design a cross-attention-based neural network that directly processes the consolidated final PDs. The resulting topological embeddings are fused with feature maps from CNNs or Transformers. By integrating multi-scale and multi-filtration topologies into an end-to-end architecture, our approach enhances the model's capacity to recognize complex anatomical structures. Evaluations on three public datasets show consistent, considerable improvements over strong baselines and state-of-the-art methods, demonstrating the value of our comprehensive topological perspective for robust and interpretable medical image classification.",
            "headline_zh": "提出多尺度多过滤拓扑特征集成框架以增强医学图像分类的解剖结构识别能力",
            "intro_zh": [
                "核心问题：现有深度网络忽视解剖结构，仅依赖像素强度或简单拓扑特征。",
                "方法要点：计算多尺度立方体持久图，通过vineyard算法整合，并设计跨注意力网络处理。",
                "实验或效果：在三个公开数据集上超越基线方法，验证了拓扑特征的有效性。"
            ],
            "tags_zh": [
                "医学图像分类",
                "拓扑数据分析",
                "多尺度特征",
                "持久同调",
                "深度学习集成"
            ],
            "_index": 214
        },
        {
            "title": "START: Spatial and Textual Learning for Chart Understanding",
            "authors": [
                "Zhuoming Liu",
                "Xiaofeng Gao",
                "Feiyang Niu",
                "Qiaozi Gao",
                "Liu Liu",
                "Robinson Piramuthu"
            ],
            "arxiv_id": "2512.07186v1",
            "summary": "Chart understanding is crucial for deploying multimodal large language models (MLLMs) in real-world scenarios such as analyzing scientific papers and technical reports. Unlike natural images, charts pair a structured visual layout (spatial property) with an underlying data representation (textual property) -- grasping both is essential for precise, fine-grained chart reasoning. Motivated by this observation, we propose START, the Spatial and Textual learning for chART understanding. Specifically, we introduce (i) chart-element grounding and (ii) chart-to-code generation to strengthen an MLLM's understanding of both chart visual layout and data details. To facilitate spatial and textual learning, we propose the START-Dataset generated with a novel data-generation pipeline that first leverages an MLLM to translate real chart images into executable chart code, recovering the underlying data representation while preserving the visual distribution of real-world charts. We then evolve the code with a Large Language Model (LLM) to ascertain the positions of chart elements that capture the chart's visual structure, addressing challenges that existing methods cannot handle. To evaluate a model's ability to understand chart spatial structures, we propose the Chart Spatial understanding Benchmark (CS-Bench), filling a critical gap in comprehensive chart understanding evaluation. Leveraging spatial and textual learning, START delivers consistent gains across model sizes and benchmarks over the base models and surpasses prior state-of-the-art by a clear margin. Code, data and models will be publicly available.",
            "headline_zh": "提出START方法以增强多模态大语言模型在图表理解中的空间与文本学习能力",
            "intro_zh": [
                "核心问题：图表结合结构化视觉布局与底层数据表示，需同时理解两者以实现精确推理",
                "方法要点：引入图表元素定位和图表到代码生成，通过START-Dataset和CS-Bench支持学习与评估",
                "实验或效果：START在不同模型规模和基准上优于基线及先前方法，代码、数据和模型将公开"
            ],
            "tags_zh": [
                "图表理解",
                "多模态大语言模型",
                "空间学习",
                "文本学习",
                "图表元素定位",
                "图表到代码生成"
            ],
            "_index": 215
        },
        {
            "title": "UniDiff: A Unified Diffusion Framework for Multimodal Time Series Forecasting",
            "authors": [
                "Da Zhang",
                "Bingyu Li",
                "Zhuyuan Zhao",
                "Junyu Gao",
                "Feiping Nie",
                "Xuelong Li"
            ],
            "arxiv_id": "2512.07184v1",
            "summary": "As multimodal data proliferates across diverse real-world applications, leveraging heterogeneous information such as texts and timestamps for accurate time series forecasting (TSF) has become a critical challenge. While diffusion models demonstrate exceptional performance in generation tasks, their application to TSF remains largely confined to modeling single-modality numerical sequences, overlooking the abundant cross-modal signals inherent in complex heterogeneous data. To address this gap, we propose UniDiff, a unified diffusion framework for multimodal time series forecasting. To process the numerical sequence, our framework first tokenizes the time series into patches, preserving local temporal dynamics by mapping each patch to an embedding space via a lightweight MLP. At its core lies a unified and parallel fusion module, where a single cross-attention mechanism adaptively weighs and integrates structural information from timestamps and semantic context from texts in one step, enabling a flexible and efficient interplay between modalities. Furthermore, we introduce a novel classifier-free guidance mechanism designed for multi-source conditioning, allowing for decoupled control over the guidance strength of textual and temporal information during inference, which significantly enhances model robustness. Extensive experiments on real-world benchmark datasets across eight domains demonstrate that the proposed UniDiff model achieves state-of-the-art performance.",
            "headline_zh": "提出UniDiff统一扩散框架以解决多模态时间序列预测中异构信息融合的挑战",
            "intro_zh": [
                "核心问题：现有扩散模型在时间序列预测中多局限于单模态数值序列，忽略文本和时间戳等异构跨模态信号。",
                "方法要点：通过统一并行融合模块，使用单交叉注意力机制一步整合时间戳结构信息和文本语义上下文，并引入多源条件分类器自由引导机制。",
                "实验或效果：在八个领域的真实世界基准数据集上广泛实验，UniDiff实现了最先进的性能。"
            ],
            "tags_zh": [
                "多模态时间序列预测",
                "扩散模型",
                "异构信息融合",
                "交叉注意力机制",
                "分类器自由引导"
            ],
            "_index": 216
        },
        {
            "title": "PICKT: Practical Interlinked Concept Knowledge Tracing for Personalized Learning using Knowledge Map Concept Relations",
            "authors": [
                "Wonbeen Lee",
                "Channyoung Lee",
                "Junho Sohn",
                "Hansam Cho"
            ],
            "arxiv_id": "2512.07179v1",
            "summary": "With the recent surge in personalized learning, Intelligent Tutoring Systems (ITS) that can accurately track students' individual knowledge states and provide tailored learning paths based on this information are in demand as an essential task. This paper focuses on the core technology of Knowledge Tracing (KT) models that analyze students' sequences of interactions to predict their knowledge acquisition levels. However, existing KT models suffer from limitations such as restricted input data formats, cold start problems arising with new student enrollment or new question addition, and insufficient stability in real-world service environments. To overcome these limitations, a Practical Interlinked Concept Knowledge Tracing (PICKT) model that can effectively process multiple types of input data is proposed. Specifically, a knowledge map structures the relationships among concepts considering the question and concept text information, thereby enabling effective knowledge tracing even in cold start situations. Experiments reflecting real operational environments demonstrated the model's excellent performance and practicality. The main contributions of this research are as follows. First, a model architecture that effectively utilizes diverse data formats is presented. Second, significant performance improvements are achieved over existing models for two core cold start challenges: new student enrollment and new question addition. Third, the model's stability and practicality are validated through delicate experimental design, enhancing its applicability in real-world product environments. This provides a crucial theoretical and technical foundation for the practical implementation of next-generation ITS.",
            "headline_zh": "提出PICKT模型以解决知识追踪中的冷启动和输入数据限制问题，提升个性化学习系统实用性。",
            "intro_zh": [
                "核心问题：现有知识追踪模型存在输入格式受限、新学生或新问题冷启动、实际服务稳定性不足。",
                "方法要点：利用知识图谱结构化概念关系，结合问题和概念文本信息，有效处理多种输入数据。",
                "实验或效果：在真实操作环境中验证模型性能优异，显著改善冷启动挑战，增强实际产品适用性。"
            ],
            "tags_zh": [
                "知识追踪",
                "冷启动问题",
                "知识图谱",
                "个性化学习",
                "智能辅导系统"
            ],
            "_index": 217
        },
        {
            "title": "ContextualSHAP : Enhancing SHAP Explanations Through Contextual Language Generation",
            "authors": [
                "Latifa Dwiyanti",
                "Sergio Ryan Wibisono",
                "Hidetaka Nambo"
            ],
            "arxiv_id": "2512.07178v1",
            "summary": "Explainable Artificial Intelligence (XAI) has become an increasingly important area of research, particularly as machine learning models are deployed in high-stakes domains. Among various XAI approaches, SHAP (SHapley Additive exPlanations) has gained prominence due to its ability to provide both global and local explanations across different machine learning models. While SHAP effectively visualizes feature importance, it often lacks contextual explanations that are meaningful for end-users, especially those without technical backgrounds. To address this gap, we propose a Python package that extends SHAP by integrating it with a large language model (LLM), specifically OpenAI's GPT, to generate contextualized textual explanations. This integration is guided by user-defined parameters (such as feature aliases, descriptions, and additional background) to tailor the explanation to both the model context and the user perspective. We hypothesize that this enhancement can improve the perceived understandability of SHAP explanations. To evaluate the effectiveness of the proposed package, we applied it in a healthcare-related case study and conducted user evaluations involving real end-users. The results, based on Likert-scale surveys and follow-up interviews, indicate that the generated explanations were perceived as more understandable and contextually appropriate compared to visual-only outputs. While the findings are preliminary, they suggest that combining visualization with contextualized text may support more user-friendly and trustworthy model explanations.",
            "headline_zh": "提出ContextualSHAP包，通过集成GPT生成上下文文本解释，以增强SHAP在非技术用户中的可理解性。",
            "intro_zh": [
                "SHAP解释缺乏对非技术用户的上下文意义，导致理解困难。",
                "方法结合SHAP与GPT，基于用户参数生成定制化文本解释。",
                "在医疗案例中，用户评估显示生成解释比纯视觉输出更易理解和合适。"
            ],
            "tags_zh": [
                "可解释人工智能",
                "SHAP解释",
                "大语言模型集成",
                "上下文生成",
                "用户评估",
                "医疗应用"
            ],
            "_index": 218
        },
        {
            "title": "Using Vision-Language Models as Proxies for Social Intelligence in Human-Robot Interaction",
            "authors": [
                "Fanjun Bu",
                "Melina Tsai",
                "Audrey Tjokro",
                "Tapomayukh Bhattacharjee",
                "Jorge Ortiz",
                "Wendy Ju"
            ],
            "arxiv_id": "2512.07177v1",
            "summary": "Robots operating in everyday environments must often decide when and whether to engage with people, yet such decisions often hinge on subtle nonverbal cues that unfold over time and are difficult to model explicitly. Drawing on a five-day Wizard-of-Oz deployment of a mobile service robot in a university cafe, we analyze how people signal interaction readiness through nonverbal behaviors and how expert wizards use these cues to guide engagement. Motivated by these observations, we propose a two-stage pipeline in which lightweight perceptual detectors (gaze shifts and proxemics) are used to selectively trigger heavier video-based vision-language model (VLM) queries at socially meaningful moments. We evaluate this pipeline on replayed field interactions and compare two prompting strategies. Our findings suggest that selectively using VLMs as proxies for social reasoning enables socially responsive robot behavior, allowing robots to act appropriately by attending to the cues people naturally provide in real-world interactions.",
            "headline_zh": "提出两阶段视觉语言模型代理方法，以提升机器人在日常环境中基于非语言线索的社交互动决策能力。",
            "intro_zh": [
                "核心问题：机器人在日常环境中需基于微妙非语言线索决定是否与人互动，但此类线索难以显式建模。",
                "方法要点：结合轻量感知检测器与视觉语言模型，在社交关键时刻选择性触发视频查询，实现社交推理代理。",
                "实验或效果：通过回放现场交互评估，验证方法能促进机器人社交响应行为，使其更自然地关注现实互动中的线索。"
            ],
            "tags_zh": [
                "人机交互",
                "视觉语言模型",
                "非语言线索",
                "社交机器人",
                "选择性触发",
                "现场部署"
            ],
            "_index": 219
        },
        {
            "title": "SPACE: Noise Contrastive Estimation Stabilizes Self-Play Fine-Tuning for Large Language Models",
            "authors": [
                "Yibo Wang",
                "Qing-Guo Chen",
                "Zhao Xu",
                "Weihua Luo",
                "Kaifu Zhang",
                "Lijun Zhang"
            ],
            "arxiv_id": "2512.07175v1",
            "summary": "Self-play fine-tuning has demonstrated promising abilities in adapting large language models (LLMs) to downstream tasks with limited real-world data. The basic principle is to iteratively refine the model with real samples and synthetic ones generated from itself. However, the existing methods primarily focus on the relative gaps between the rewards for two types of data, neglecting their absolute values. Through theoretical analysis, we identify that the gap-based methods suffer from unstable evolution, due to the potentially degenerated objectives. To address this limitation, we introduce a novel self-play fine-tuning method, namely Self-PlAy via Noise Contrastive Estimation (SPACE), which leverages noise contrastive estimation to capture the real-world data distribution. Specifically, SPACE treats synthetic samples as auxiliary components, and discriminates them from the real ones in a binary classification manner. As a result, SPACE independently optimizes the absolute reward values for each type of data, ensuring a consistently meaningful objective and thereby avoiding the instability issue. Theoretically, we show that the optimal solution of the objective in SPACE aligns with the underlying distribution of real-world data, and SPACE guarantees a provably stable convergence to the optimal distribution. Empirically, we show that SPACE significantly improves the performance of LLMs over various tasks, and outperforms supervised fine-tuning that employs much more real-world samples. Compared to gap-based self-play fine-tuning methods, SPACE exhibits remarkable superiority and stable evolution.",
            "headline_zh": "提出SPACE方法，通过噪声对比估计稳定自博弈微调，解决目标退化问题",
            "intro_zh": [
                "现有自博弈微调方法基于奖励差距，忽略绝对值，导致目标退化与不稳定演化",
                "SPACE将合成样本视为辅助，以二元分类区分真实样本，独立优化绝对奖励值",
                "实验显示SPACE在多种任务上显著提升性能，优于监督微调与差距方法，确保稳定收敛"
            ],
            "tags_zh": [
                "自博弈微调",
                "噪声对比估计",
                "大语言模型",
                "稳定收敛",
                "分布对齐"
            ],
            "_index": 220
        },
        {
            "title": "Improving the Throughput of Diffusion-based Large Language Models via a Training-Free Confidence-Aware Calibration",
            "authors": [
                "Jucheng Shen",
                "Gaurav Sarkar",
                "Yeonju Ro",
                "Sharath Nittur Sridhar",
                "Zhangyang Wang",
                "Aditya Akella",
                "Souvik Kundu"
            ],
            "arxiv_id": "2512.07173v1",
            "summary": "We present CadLLM, a training-free method to accelerate the inference throughput of diffusion-based LLMs (dLLMs). We first investigate the dynamic nature of token unmasking confidence across blocks and steps. Based on this observation, we present a lightweight adaptive approach that controls the generation block size, step size, and threshold based on the average confidence of unmasked tokens. We further reduce softmax overhead by dynamically leveraging a subset of the vocabulary to regulate sampling breadth. CadLLM is a plug-and-play, model-agnostic method compatible with KV-cache-based dLLMs. Extensive experiments on four popular tasks demonstrate that CadLLM yields up to 2.28x throughput improvement over the state-of-the-art baseline with competitive accuracy.",
            "headline_zh": "提出CadLLM以加速基于扩散的大语言模型推理吞吐量",
            "intro_zh": [
                "核心问题：扩散大语言模型推理吞吐量低，需提升效率。",
                "方法要点：基于置信度动态调整生成参数，无需训练，兼容KV缓存模型。",
                "实验或效果：在四个任务上实现最高2.28倍吞吐量提升，保持准确率。"
            ],
            "tags_zh": [
                "扩散大语言模型",
                "推理加速",
                "训练无关方法",
                "置信度校准",
                "KV缓存",
                "吞吐量优化"
            ],
            "_index": 221
        },
        {
            "title": "TIDE: Two-Stage Inverse Degradation Estimation with Guided Prior Disentanglement for Underwater Image Restoration",
            "authors": [
                "Shravan Venkatraman",
                "Rakesh Raj Madavan",
                "Pavan Kumar S",
                "Muthu Subash Kavitha"
            ],
            "arxiv_id": "2512.07171v1",
            "summary": "Underwater image restoration is essential for marine applications ranging from ecological monitoring to archaeological surveys, but effectively addressing the complex and spatially varying nature of underwater degradations remains a challenge. Existing methods typically apply uniform restoration strategies across the entire image, struggling to handle multiple co-occurring degradations that vary spatially and with water conditions. We introduce TIDE, a $\\underline{t}$wo stage $\\underline{i}$nverse $\\underline{d}$egradation $\\underline{e}$stimation framework that explicitly models degradation characteristics and applies targeted restoration through specialized prior decomposition. Our approach disentangles the restoration process into multiple specialized hypotheses that are adaptively fused based on local degradation patterns, followed by a progressive refinement stage that corrects residual artifacts. Specifically, TIDE decomposes underwater degradations into four key factors, namely color distortion, haze, detail loss, and noise, and designs restoration experts specialized for each. By generating specialized restoration hypotheses, TIDE balances competing degradation factors and produces natural results even in highly degraded regions. Extensive experiments across both standard benchmarks and challenging turbid water conditions show that TIDE achieves competitive performance on reference based fidelity metrics while outperforming state of the art methods on non reference perceptual quality metrics, with strong improvements in color correction and contrast enhancement. Our code is available at: https://rakesh-123-cryp.github.io/TIDE.",
            "headline_zh": "提出TIDE框架，通过两阶段逆退化估计与先验解耦解决水下图像恢复问题",
            "intro_zh": [
                "核心问题：水下图像退化复杂且空间变化，现有方法难以处理多退化共存。",
                "方法要点：将退化分解为四因素，设计专家假设并自适应融合，再渐进细化。",
                "实验或效果：在标准基准和浑浊条件下，在保真度和感知质量上表现优异。"
            ],
            "tags_zh": [
                "水下图像恢复",
                "逆退化估计",
                "先验解耦",
                "多退化处理",
                "自适应融合",
                "渐进细化"
            ],
            "_index": 222
        },
        {
            "title": "Towards Unified Semantic and Controllable Image Fusion: A Diffusion Transformer Approach",
            "authors": [
                "Jiayang Li",
                "Chengjie Jiang",
                "Junjun Jiang",
                "Pengwei Liang",
                "Jiayi Ma",
                "Liqiang Nie"
            ],
            "arxiv_id": "2512.07170v1",
            "summary": "Image fusion aims to blend complementary information from multiple sensing modalities, yet existing approaches remain limited in robustness, adaptability, and controllability. Most current fusion networks are tailored to specific tasks and lack the ability to flexibly incorporate user intent, especially in complex scenarios involving low-light degradation, color shifts, or exposure imbalance. Moreover, the absence of ground-truth fused images and the small scale of existing datasets make it difficult to train an end-to-end model that simultaneously understands high-level semantics and performs fine-grained multimodal alignment. We therefore present DiTFuse, instruction-driven Diffusion-Transformer (DiT) framework that performs end-to-end, semantics-aware fusion within a single model. By jointly encoding two images and natural-language instructions in a shared latent space, DiTFuse enables hierarchical and fine-grained control over fusion dynamics, overcoming the limitations of pre-fusion and post-fusion pipelines that struggle to inject high-level semantics. The training phase employs a multi-degradation masked-image modeling strategy, so the network jointly learns cross-modal alignment, modality-invariant restoration, and task-aware feature selection without relying on ground truth images. A curated, multi-granularity instruction dataset further equips the model with interactive fusion capabilities. DiTFuse unifies infrared-visible, multi-focus, and multi-exposure fusion-as well as text-controlled refinement and downstream tasks-within a single architecture. Experiments on public IVIF, MFF, and MEF benchmarks confirm superior quantitative and qualitative performance, sharper textures, and better semantic retention. The model also supports multi-level user control and zero-shot generalization to other multi-image fusion scenarios, including instruction-conditioned segmentation.",
            "headline_zh": "提出DiTFuse扩散Transformer框架，实现语义可控的图像融合",
            "intro_zh": [
                "现有图像融合方法在鲁棒性、适应性和可控性方面受限，缺乏用户意图整合能力",
                "DiTFuse通过联合编码图像与自然语言指令，在单一模型中实现端到端语义感知融合",
                "实验在多个基准上显示优异性能，支持多级控制和零样本泛化"
            ],
            "tags_zh": [
                "图像融合",
                "扩散Transformer",
                "语义控制",
                "多模态对齐",
                "零样本泛化"
            ],
            "_index": 223
        },
        {
            "title": "JEPA as a Neural Tokenizer: Learning Robust Speech Representations with Density Adaptive Attention",
            "authors": [
                "Georgios Ioannides",
                "Christos Constantinou",
                "Aman Chadha",
                "Aaron Elkins",
                "Linsey Pang",
                "Ravid Shwartz-Ziv",
                "Yann LeCun"
            ],
            "arxiv_id": "2512.07168v1",
            "summary": "We introduce a two-stage self-supervised framework that combines the Joint-Embedding Predictive Architecture (JEPA) with a Density Adaptive Attention Mechanism (DAAM) for learning robust speech representations. Stage~1 uses JEPA with DAAM to learn semantic audio features via masked prediction in latent space, fully decoupled from waveform reconstruction. Stage~2 leverages these representations for efficient tokenization using Finite Scalar Quantization (FSQ) and a mixed-radix packing scheme, followed by high-fidelity waveform reconstruction with a HiFi-GAN decoder. By integrating Gaussian mixture-based density-adaptive gating into the JEPA encoder, the model performs adaptive temporal feature selection and discovers hierarchical speech structure at a low frame rate of 2.5~Hz. The resulting tokens (47.5 tokens/sec) provide a reversible, highly compressed, and language-model-friendly representation that is competitive with, and often more efficient than, existing neural audio codecs.",
            "headline_zh": "提出结合JEPA与密度自适应注意力的两阶段自监督框架，用于学习鲁棒语音表示和高效令牌化。",
            "intro_zh": [
                "核心问题：未知，但旨在学习鲁棒语音表示以支持高效压缩和语言模型友好性。",
                "方法要点：第一阶段使用JEPA与密度自适应注意力进行潜在空间掩码预测，第二阶段利用FSQ和混合基数打包进行令牌化，结合HiFi-GAN解码器重建波形。",
                "实验或效果：模型在2.5 Hz低帧率下实现自适应时间特征选择和分层结构发现，令牌速率47.5 tokens/sec，与现有神经音频编解码器竞争且更高效。"
            ],
            "tags_zh": [
                "自监督学习",
                "语音表示学习",
                "神经音频编解码",
                "注意力机制",
                "令牌化"
            ],
            "_index": 224
        },
        {
            "title": "When Privacy Meets Recovery: The Overlooked Half of Surrogate-Driven Privacy Preservation for MLLM Editing",
            "authors": [
                "Siyuan Xu",
                "Yibing Liu",
                "Peilin Chen",
                "Yung-Hui Li",
                "Shiqi Wang",
                "Sam Kwong"
            ],
            "arxiv_id": "2512.07166v1",
            "summary": "Privacy leakage in Multimodal Large Language Models (MLLMs) has long been an intractable problem. Existing studies, though effectively obscure private information in MLLMs, often overlook the evaluation of the authenticity and recovery quality of user privacy. To this end, this work uniquely focuses on the critical challenge of how to restore surrogate-driven protected data in diverse MLLM scenarios. We first bridge this research gap by contributing the SPPE (Surrogate Privacy Protected Editable) dataset, which includes a wide range of privacy categories and user instructions to simulate real MLLM applications. This dataset offers protected surrogates alongside their various MLLM-edited versions, thus enabling the direct assessment of privacy recovery quality. By formulating privacy recovery as a guided generation task conditioned on complementary multimodal signals, we further introduce a unified approach that reliably reconstructs private content while preserving the fidelity of MLLM-generated edits. The experiments on both SPPE and InstructPix2Pix further show that our approach generalizes well across diverse visual content and editing tasks, achieving a strong balance between privacy protection and MLLM usability.",
            "headline_zh": "提出统一方法以恢复多模态大语言模型编辑中的隐私内容，平衡保护与可用性。",
            "intro_zh": [
                "核心问题：现有隐私保护方法忽视隐私恢复质量评估，导致隐私泄露问题未全面解决。",
                "方法要点：将隐私恢复建模为基于多模态信号的引导生成任务，可靠重建隐私内容并保持编辑保真度。",
                "实验或效果：在SPPE和InstructPix2Pix数据集上验证，方法泛化性强，在多样视觉内容和编辑任务中表现良好。"
            ],
            "tags_zh": [
                "多模态大语言模型",
                "隐私保护",
                "隐私恢复",
                "引导生成",
                "数据集构建",
                "编辑保真度"
            ],
            "_index": 225
        },
        {
            "title": "MuSASplat: Efficient Sparse-View 3D Gaussian Splats via Lightweight Multi-Scale Adaptation",
            "authors": [
                "Muyu Xu",
                "Fangneng Zhan",
                "Xiaoqin Zhang",
                "Ling Shao",
                "Shijian Lu"
            ],
            "arxiv_id": "2512.07165v1",
            "summary": "Sparse-view 3D Gaussian splatting seeks to render high-quality novel views of 3D scenes from a limited set of input images. While recent pose-free feed-forward methods leveraging pre-trained 3D priors have achieved impressive results, most of them rely on full fine-tuning of large Vision Transformer (ViT) backbones and incur substantial GPU costs. In this work, we introduce MuSASplat, a novel framework that dramatically reduces the computational burden of training pose-free feed-forward 3D Gaussian splats models with little compromise of rendering quality. Central to our approach is a lightweight Multi-Scale Adapter that enables efficient fine-tuning of ViT-based architectures with only a small fraction of training parameters. This design avoids the prohibitive GPU overhead associated with previous full-model adaptation techniques while maintaining high fidelity in novel view synthesis, even with very sparse input views. In addition, we introduce a Feature Fusion Aggregator that integrates features across input views effectively and efficiently. Unlike widely adopted memory banks, the Feature Fusion Aggregator ensures consistent geometric integration across input views and meanwhile mitigates the memory usage, training complexity, and computational costs significantly. Extensive experiments across diverse datasets show that MuSASplat achieves state-of-the-art rendering quality but has significantly reduced parameters and training resource requirements as compared with existing methods.",
            "headline_zh": "提出MuSASplat框架，通过轻量级多尺度适配器高效训练稀疏视图3D高斯溅射模型。",
            "intro_zh": [
                "稀疏视图3D高斯溅射训练计算成本高，现有方法依赖大模型全微调。",
                "引入轻量级多尺度适配器，仅微调少量参数，降低GPU开销。",
                "实验表明，在保持渲染质量的同时，显著减少参数和训练资源需求。"
            ],
            "tags_zh": [
                "稀疏视图3D重建",
                "3D高斯溅射",
                "轻量级微调",
                "多尺度适配器",
                "特征融合聚合器"
            ],
            "_index": 226
        },
        {
            "title": "DeepSVM: Learning Stochastic Volatility Models with Physics-Informed Deep Operator Networks",
            "authors": [
                "Kieran A. Malandain",
                "Selim Kalici",
                "Hakob Chakhoyan"
            ],
            "arxiv_id": "2512.07162v1",
            "summary": "Real-time calibration of stochastic volatility models (SVMs) is computationally bottlenecked by the need to repeatedly solve coupled partial differential equations (PDEs). In this work, we propose DeepSVM, a physics-informed Deep Operator Network (PI-DeepONet) designed to learn the solution operator of the Heston model across its entire parameter space. Unlike standard data-driven deep learning (DL) approaches, DeepSVM requires no labelled training data. Rather, we employ a hard-constrained ansatz that enforces terminal payoffs and static no-arbitrage conditions by design. Furthermore, we use Residual-based Adaptive Refinement (RAR) to stabilize training in difficult regions subject to high gradients. Overall, DeepSVM achieves a final training loss of $10^{-5}$ and predicts highly accurate option prices across a range of typical market dynamics. While pricing accuracy is high, we find that the model's derivatives (Greeks) exhibit noise in the at-the-money (ATM) regime, highlighting the specific need for higher-order regularization in physics-informed operator learning.",
            "headline_zh": "提出DeepSVM以解决随机波动率模型实时校准中的计算瓶颈",
            "intro_zh": [
                "核心问题：随机波动率模型校准需重复求解耦合偏微分方程，计算成本高。",
                "方法要点：使用物理信息深度算子网络，无需标注数据，通过硬约束和自适应细化稳定训练。",
                "实验或效果：训练损失达10^{-5}，期权定价准确，但ATM区域导数存在噪声。"
            ],
            "tags_zh": [
                "随机波动率模型",
                "物理信息深度学习",
                "深度算子网络",
                "期权定价",
                "自适应训练",
                "计算金融"
            ],
            "_index": 227
        },
        {
            "title": "CHIMERA: Adaptive Cache Injection and Semantic Anchor Prompting for Zero-shot Image Morphing with Morphing-oriented Metrics",
            "authors": [
                "Dahyeon Kye",
                "Jeahun Sung",
                "MinKyu Jeon",
                "Jihyong Oh"
            ],
            "arxiv_id": "2512.07155v1",
            "summary": "Diffusion models exhibit remarkable generative ability, yet achieving smooth and semantically consistent image morphing remains a challenge. Existing approaches often yield abrupt transitions or over-saturated appearances due to the lack of adaptive structural and semantic alignments. We propose CHIMERA, a zero-shot diffusion-based framework that formulates morphing as a cached inversion-guided denoising process. To handle large semantic and appearance disparities, we propose Adaptive Cache Injection and Semantic Anchor Prompting. Adaptive Cache Injection (ACI) caches down, mid, and up blocks features from both inputs during DDIM inversion and re-injects them adaptively during denoising, enabling spatial and semantic alignment in depth- and time-adaptive manners and enabling natural feature fusion and smooth transitions. Semantic Anchor Prompting (SAP) leverages a vision-language model to generate a shared anchor prompt that serves as a semantic anchor, bridging dissimilar inputs and guiding the denoising process toward coherent results. Finally, we introduce the Global-Local Consistency Score (GLCS), a morphing-oriented metric that simultaneously evaluates the global harmonization of the two inputs and the smoothness of the local morphing transition. Extensive experiments and user studies show that CHIMERA achieves smoother and more semantically aligned transitions than existing methods, establishing a new state of the art in image morphing. The code and project page will be publicly released.",
            "headline_zh": "提出CHIMERA框架以解决扩散模型图像变形中的过渡不自然问题",
            "intro_zh": [
                "核心问题：现有方法在图像变形中常产生突兀过渡或过饱和外观，缺乏自适应结构和语义对齐。",
                "方法要点：采用自适应缓存注入和语义锚点提示，通过缓存反转特征和共享提示实现平滑变形。",
                "实验或效果：引入全局-局部一致性评分，实验显示CHIMERA在变形平滑度和语义对齐上优于现有方法。"
            ],
            "tags_zh": [
                "图像变形",
                "扩散模型",
                "零样本学习",
                "语义对齐",
                "自适应缓存",
                "视觉语言模型"
            ],
            "_index": 228
        },
        {
            "title": "FlowLPS: Langevin-Proximal Sampling for Flow-based Inverse Problem Solvers",
            "authors": [
                "Jonghyun Park",
                "Jong Chul Ye"
            ],
            "arxiv_id": "2512.07150v1",
            "summary": "Deep generative models have become powerful priors for solving inverse problems, and various training-free methods have been developed. However, when applied to latent flow models, existing methods often fail to converge to the posterior mode or suffer from manifold deviation within latent spaces. To mitigate this, here we introduce a novel training-free framework, FlowLPS, that solves inverse problems with pretrained flow models via a Langevin Proximal Sampling (LPS) strategy. Our method integrates Langevin dynamics for manifold-consistent exploration with proximal optimization for precise mode seeking, achieving a superior balance between reconstruction fidelity and perceptual quality across multiple inverse tasks on FFHQ and DIV2K, outperforming state of the art inverse solvers.",
            "headline_zh": "提出FlowLPS框架，通过Langevin-Proximal采样解决基于流模型的逆问题求解中的收敛与流形偏差问题。",
            "intro_zh": [
                "核心问题：现有训练免费方法在流模型逆问题求解中易收敛失败或产生流形偏差。",
                "方法要点：结合Langevin动力学进行流形一致探索和近端优化实现精确模式寻找。",
                "实验或效果：在FFHQ和DIV2K数据集上实现重建保真度与感知质量的平衡，优于现有方法。"
            ],
            "tags_zh": [
                "逆问题求解",
                "流模型",
                "Langevin采样",
                "近端优化",
                "训练免费方法"
            ],
            "_index": 229
        },
        {
            "title": "Winning the Lottery by Preserving Network Training Dynamics with Concrete Ticket Search",
            "authors": [
                "Tanay Arora",
                "Christof Teuscher"
            ],
            "arxiv_id": "2512.07142v1",
            "summary": "The Lottery Ticket Hypothesis asserts the existence of highly sparse, trainable subnetworks ('winning tickets') within dense, randomly initialized neural networks. However, state-of-the-art methods of drawing these tickets, like Lottery Ticket Rewinding (LTR), are computationally prohibitive, while more efficient saliency-based Pruning-at-Initialization (PaI) techniques suffer from a significant accuracy-sparsity trade-off and fail basic sanity checks. In this work, we argue that PaI's reliance on first-order saliency metrics, which ignore inter-weight dependencies, contributes substantially to this performance gap, especially in the sparse regime. To address this, we introduce Concrete Ticket Search (CTS), an algorithm that frames subnetwork discovery as a holistic combinatorial optimization problem. By leveraging a Concrete relaxation of the discrete search space and a novel gradient balancing scheme (GRADBALANCE) to control sparsity, CTS efficiently identifies high-performing subnetworks near initialization without requiring sensitive hyperparameter tuning. Motivated by recent works on lottery ticket training dynamics, we further propose a knowledge distillation-inspired family of pruning objectives, finding that minimizing the reverse Kullback-Leibler divergence between sparse and dense network outputs (CTS-KL) is particularly effective. Experiments on varying image classification tasks show that CTS produces subnetworks that robustly pass sanity checks and achieve accuracy comparable to or exceeding LTR, while requiring only a small fraction of the computation. For example, on ResNet-20 on CIFAR10, it reaches 99.3% sparsity with 74.0% accuracy in 7.9 minutes, while LTR attains the same sparsity with 68.3% accuracy in 95.2 minutes. CTS's subnetworks outperform saliency-based methods across all sparsities, but its advantage over LTR is most pronounced in the highly sparse regime.",
            "headline_zh": "提出Concrete Ticket Search算法，通过整体优化解决初始化剪枝的性能差距问题。",
            "intro_zh": [
                "核心问题：初始化剪枝方法依赖一阶显著性指标，忽略权重间依赖，导致精度-稀疏度权衡不佳。",
                "方法要点：使用Concrete松弛和梯度平衡方案，将子网络发现建模为组合优化问题，无需敏感超参数调优。",
                "实验或效果：在图像分类任务中，CTS生成子网络通过健全性检查，精度接近或超过LTR，计算成本大幅降低。"
            ],
            "tags_zh": [
                "彩票票假设",
                "初始化剪枝",
                "组合优化",
                "知识蒸馏",
                "稀疏网络",
                "训练动态"
            ],
            "_index": 230
        },
        {
            "title": "Think-Reflect-Revise: A Policy-Guided Reflective Framework for Safety Alignment in Large Vision Language Models",
            "authors": [
                "Fenghua Weng",
                "Chaochao Lu",
                "Xia Hu",
                "Wenqi Shao",
                "Wenjie Wang"
            ],
            "arxiv_id": "2512.07141v1",
            "summary": "As multimodal reasoning improves the overall capabilities of Large Vision Language Models (LVLMs), recent studies have begun to explore safety-oriented reasoning, aiming to enhance safety awareness by analyzing potential safety risks during the reasoning process before generating the final response. Although such approaches improve safety awareness and interpretability, this single-pass think-then-answer paradigm remains vulnerable to contextual or visual jailbreak attacks. This reveals a critical flaw: single-pass reasoning may overlook explicit harmful content in its own output. Our key insight is to exploit this wasted signal through reflection, which can effectively leverage the malicious content revealed in the first-pass reasoning to enable genuine self-correction and prevent unsafe generations. Motivated by this, we propose Think-Reflect-Revise (TRR), a three-stage training framework designed to enhance the safety alignment of LVLMs through policy-guided self-reflection. We first build a Reflective Safety Reasoning (ReSafe) dataset with 5,000 examples that follow a think-reflect-revise process. We then fine-tune the target model using the ReSafe dataset to initialize reflective behavior, and finally reinforce policy-guided reflection through reinforcement learning. Experimental results show that TRR substantially improves the safety performance of LVLMs across both safety-awareness benchmarks and jailbreak attack evaluations, increasing the overall safe response rate from 42.8% to 87.7% on Qwen2.5-VL-7B, while preserving stable performance on general benchmarks such as MMMU and MMStar. The project page is available at https://think-reflect-revise.github.io/.",
            "headline_zh": "提出Think-Reflect-Revise框架以增强大型视觉语言模型的安全对齐能力",
            "intro_zh": [
                "核心问题：单次推理范式易受上下文或视觉越狱攻击，可能忽略自身输出中的有害内容。",
                "方法要点：构建包含5000个示例的ReSafe数据集，通过三阶段训练（数据集微调、强化学习）引导模型进行策略指导的自我反思。",
                "实验或效果：在Qwen2.5-VL-7B上，安全响应率从42.8%提升至87.7%，同时保持通用基准性能稳定。"
            ],
            "tags_zh": [
                "大型视觉语言模型",
                "安全对齐",
                "自我反思",
                "强化学习",
                "越狱攻击",
                "多模态推理"
            ],
            "_index": 231
        },
        {
            "title": "Time-Varying Formation Tracking Control of Wheeled Mobile Robots With Region Constraint: A Generalized Udwadia-Kalaba Framework",
            "authors": [
                "Kang Yijie",
                "Hao Yuqing",
                "Wang Qingyun",
                "Chen Guanrong"
            ],
            "arxiv_id": "2512.07137v1",
            "summary": "In this paper, the time-varying formation tracking control of wheeled mobile robots with region constraint is investigated from a generalized Udwadia-Kalaba framework. The communication topology is directed, weighted and has a spanning tree with the leader being the root. By reformulating the time-varying formation tracking control objective as a constrained equation and transforming the region constraint by a diffeomorphism, the time-varying formation tracking controller with the region constraint is designed under the generalized Udwadia-Kalaba framework. Compared with the existing works on time-varying formation tracking control, the region constraint is takeninto account in this paper, which ensures the safety of the robots.Finally, some numerical simulations are presented to illustrate the effectiveness of the proposed control strategy.",
            "headline_zh": "提出基于广义Udwadia-Kalaba框架的轮式移动机器人时变编队跟踪控制方法，考虑区域约束以确保安全性。",
            "intro_zh": [
                "研究轮式移动机器人的时变编队跟踪控制问题，引入区域约束以保障机器人安全运行。",
                "采用广义Udwadia-Kalaba框架，将控制目标重构为约束方程，并通过微分同胚变换处理区域约束。",
                "通过数值仿真验证了所提控制策略的有效性，展示了在区域约束下的编队跟踪性能。"
            ],
            "tags_zh": [
                "轮式移动机器人",
                "时变编队跟踪",
                "区域约束",
                "Udwadia-Kalaba框架",
                "控制策略"
            ],
            "_index": 232
        },
        {
            "title": "A Large-Scale Multimodal Dataset and Benchmarks for Human Activity Scene Understanding and Reasoning",
            "authors": [
                "Siyang Jiang",
                "Mu Yuan",
                "Xiang Ji",
                "Bufang Yang",
                "Zeyu Liu",
                "Lilin Xu",
                "Yang Li",
                "Yuting He",
                "Liran Dong",
                "Wenrui Lu",
                "Zhenyu Yan",
                "Xiaofan Jiang",
                "Wei Gao",
                "Hongkai Chen",
                "Guoliang Xing"
            ],
            "arxiv_id": "2512.07136v1",
            "summary": "Multimodal human action recognition (HAR) leverages complementary sensors for activity classification. Beyond recognition, recent advances in large language models (LLMs) enable detailed descriptions and causal reasoning, motivating new tasks: human action understanding (HAU) and human action reasoning (HARn). However, most LLMs, especially large vision language models (LVLMs), struggle with non-RGB modalities such as depth, IMU, and mmWave due to the lack of large-scale data-caption resources. Existing HAR datasets mainly provide coarse data-label annotations, which are insufficient to capture fine-grained action dynamics needed for HAU and HARn. We consider two ground-truth pair types: (1) data label (discrete category) and (2) data caption (textual description). Naively generating captions from labels often lacks logical and spatiotemporal consistency. We introduce CUHK-X, a large-scale multimodal dataset and benchmark suite for HAR, HAU, and HARn. CUHK-X contains 58,445 samples covering 40 actions performed by 30 participants across two indoor environments. To improve caption consistency, we propose a prompt-based scene creation method that leverages LLMs to generate logically connected activity sequences, followed by human validation. CUHK-X includes three benchmarks with six evaluation tasks. Experiments report average accuracies of 76.52% (HAR), 40.76% (HAU), and 70.25% (HARn). CUHK-X aims to enable the community to apply and develop data-intensive learning methods for robust, multimodal human activity analysis. Project page and code: https://openaiotlab.github.io/CUHK-X/ and https://github.com/openaiotlab/CUHK-X.",
            "headline_zh": "提出大规模多模态数据集CUHK-X与基准套件，以支持人类活动理解与推理任务。",
            "intro_zh": [
                "核心问题：现有数据集缺乏非RGB模态的大规模文本描述资源，限制LVLMs在人类活动理解与推理中的应用。",
                "方法要点：采用基于提示的场景创建方法，利用LLMs生成逻辑连贯的活动序列，并通过人工验证提升描述一致性。",
                "实验或效果：在HAR、HAU和HARn任务上平均准确率分别为76.52%、40.76%和70.25%。"
            ],
            "tags_zh": [
                "多模态数据集",
                "人类活动理解",
                "人类活动推理",
                "大规模视觉语言模型",
                "基准评估",
                "非RGB模态"
            ],
            "_index": 233
        },
        {
            "title": "TrajMoE: Scene-Adaptive Trajectory Planning with Mixture of Experts and Reinforcement Learning",
            "authors": [
                "Zebin Xing",
                "Pengxuan Yang",
                "Linbo Wang",
                "Yichen Zhang",
                "Yiming Hu",
                "Yupeng Zheng",
                "Junli Wang",
                "Yinfeng Gao",
                "Guang Li",
                "Kun Ma",
                "Long Chen",
                "Zhongpu Xia",
                "Qichao Zhang",
                "Hangjun Ye",
                "Dongbin Zhao"
            ],
            "arxiv_id": "2512.07135v1",
            "summary": "Current autonomous driving systems often favor end-to-end frameworks, which take sensor inputs like images and learn to map them into trajectory space via neural networks. Previous work has demonstrated that models can achieve better planning performance when provided with a prior distribution of possible trajectories. However, these approaches often overlook two critical aspects: 1) The appropriate trajectory prior can vary significantly across different driving scenarios. 2) Their trajectory evaluation mechanism lacks policy-driven refinement, remaining constrained by the limitations of one-stage supervised training. To address these issues, we explore improvements in two key areas. For problem 1, we employ MoE to apply different trajectory priors tailored to different scenarios. For problem 2, we utilize Reinforcement Learning to fine-tune the trajectory scoring mechanism. Additionally, we integrate models with different perception backbones to enhance perceptual features. Our integrated model achieved a score of 51.08 on the navsim ICCV benchmark, securing third place.",
            "headline_zh": "提出TrajMoE，通过混合专家和强化学习实现场景自适应轨迹规划",
            "intro_zh": [
                "核心问题：现有自动驾驶轨迹规划方法忽视场景差异和缺乏策略驱动的轨迹评估机制",
                "方法要点：使用MoE为不同场景定制轨迹先验，并利用强化学习微调轨迹评分",
                "实验或效果：在navsim ICCV基准测试中得分51.08，排名第三"
            ],
            "tags_zh": [
                "自动驾驶",
                "轨迹规划",
                "混合专家",
                "强化学习",
                "场景自适应"
            ],
            "_index": 234
        },
        {
            "title": "DART: Leveraging Multi-Agent Disagreement for Tool Recruitment in Multimodal Reasoning",
            "authors": [
                "Nithin Sivakumaran",
                "Justin Chih-Yao Chen",
                "David Wan",
                "Yue Zhang",
                "Jaehong Yoon",
                "Elias Stengel-Eskin",
                "Mohit Bansal"
            ],
            "arxiv_id": "2512.07132v1",
            "summary": "Specialized visual tools can augment large language models or vision language models with expert knowledge (e.g., grounding, spatial reasoning, medical knowledge, etc.), but knowing which tools to call (and when to call them) can be challenging. We introduce DART, a multi-agent framework that uses disagreements between multiple debating visual agents to identify useful visual tools (e.g., object detection, OCR, spatial reasoning, etc.) that can resolve inter-agent disagreement. These tools allow for fruitful multi-agent discussion by introducing new information, and by providing tool-aligned agreement scores that highlight agents in agreement with expert tools, thereby facilitating discussion. We utilize an aggregator agent to select the best answer by providing the agent outputs and tool information. We test DART on four diverse benchmarks and show that our approach improves over multi-agent debate as well as over single agent tool-calling frameworks, beating the next-strongest baseline (multi-agent debate with a judge model) by 3.4% and 2.4% on A-OKVQA and MMMU respectively. We also find that DART adapts well to new tools in applied domains, with a 1.3% improvement on the M3D medical dataset over other strong tool-calling, single agent, and multi-agent baselines. Additionally, we measure text overlap across rounds to highlight the rich discussion in DART compared to existing multi-agent methods. Finally, we study the tool call distribution, finding that diverse tools are reliably used to help resolve disagreement.",
            "headline_zh": "提出DART多智能体框架，利用视觉智能体间的分歧来招募工具以增强多模态推理",
            "intro_zh": [
                "核心问题：多模态推理中，如何有效选择和调用视觉工具以解决智能体间的分歧",
                "方法要点：通过多智能体辩论识别分歧，引入工具提供新信息和一致性评分，聚合器选择最佳答案",
                "实验或效果：在四个基准测试中优于基线，如A-OKVQA和MMMU分别提升3.4%和2.4%，并适应新工具"
            ],
            "tags_zh": [
                "多智能体辩论",
                "视觉工具调用",
                "多模态推理",
                "分歧解决",
                "专家知识增强"
            ],
            "_index": 235
        },
        {
            "title": "Mimir: Hierarchical Goal-Driven Diffusion with Uncertainty Propagation for End-to-End Autonomous Driving",
            "authors": [
                "Zebin Xing",
                "Yupeng Zheng",
                "Qichao Zhang",
                "Zhixing Ding",
                "Pengxuan Yang",
                "Songen Gu",
                "Zhongpu Xia",
                "Dongbin Zhao"
            ],
            "arxiv_id": "2512.07130v1",
            "summary": "End-to-end autonomous driving has emerged as a pivotal direction in the field of autonomous systems. Recent works have demonstrated impressive performance by incorporating high-level guidance signals to steer low-level trajectory planners. However, their potential is often constrained by inaccurate high-level guidance and the computational overhead of complex guidance modules. To address these limitations, we propose Mimir, a novel hierarchical dual-system framework capable of generating robust trajectories relying on goal points with uncertainty estimation: (1) Unlike previous approaches that deterministically model, we estimate goal point uncertainty with a Laplace distribution to enhance robustness; (2) To overcome the slow inference speed of the guidance system, we introduce a multi-rate guidance mechanism that predicts extended goal points in advance. Validated on challenging Navhard and Navtest benchmarks, Mimir surpasses previous state-of-the-art methods with a 20% improvement in the driving score EPDMS, while achieving 1.6 times improvement in high-level module inference speed without compromising accuracy. The code and models will be released soon to promote reproducibility and further development. The code is available at https://github.com/ZebinX/Mimir-Uncertainty-Driving",
            "headline_zh": "提出Mimir框架，通过不确定性估计和多速率引导解决端到端自动驾驶中高层引导不准确和计算开销大的问题。",
            "intro_zh": [
                "核心问题：端到端自动驾驶中高层引导信号不准确和复杂引导模块计算开销大，限制性能提升。",
                "方法要点：采用拉普拉斯分布估计目标点不确定性以增强鲁棒性，并引入多速率引导机制提前预测扩展目标点以加速推理。",
                "实验或效果：在Navhard和Navtest基准测试中，驾驶分数EPDMS提升20%，高层模块推理速度提高1.6倍，且不损失准确性。"
            ],
            "tags_zh": [
                "端到端自动驾驶",
                "不确定性估计",
                "多速率引导",
                "轨迹生成",
                "拉普拉斯分布",
                "推理加速"
            ],
            "_index": 236
        },
        {
            "title": "MulCLIP: A Multi-level Alignment Framework for Enhancing Fine-grained Long-context CLIP",
            "authors": [
                "Chau Truong",
                "Hieu Ta Quang",
                "Dung D. Le"
            ],
            "arxiv_id": "2512.07128v1",
            "summary": "Vision-language models like CLIP show impressive ability to align images and text, but their training on short, concise captions makes them struggle with lengthy, detailed descriptions. Recent advances mitigate this challenge by leveraging region-proposal information to map visual regions with corresponding sentences from lengthy captions, yet incurring notable deployment costs. We introduce MulCLIP, a novel end-to-end multi-level alignment framework that bridges natural long-text structures with image components. MulCLIP first preserves global contrastive alignment between images and both summary and long captions, while extending positional embeddings for longer text sequences. To further enhance fine-grained understanding, we propose two novel strategies: (1) a token reconstruction alignment over locally calibrated features to strengthen semantic connections between words and image patches, and (2) a subcaption-aggregated patch alignment that automatically extracts and aggregates context-rich patches for each subcaption. Experimental results across diverse benchmarks demonstrate our method consistently improves downstream performance, while ablation studies confirm its multi-scale alignment is the key factor driving better fine-grained capability than region-proposal-assisted approaches, making it particularly suitable for diverse real-world applications.",
            "headline_zh": "提出MulCLIP多级对齐框架以增强细粒度长上下文CLIP能力",
            "intro_zh": [
                "CLIP模型在长文本描述上表现不佳，因训练数据为简短标题",
                "MulCLIP通过全局对比对齐、局部特征重建和子标题聚合补丁对齐实现多级对齐",
                "实验显示MulCLIP在多个基准上提升性能，优于区域提议方法"
            ],
            "tags_zh": [
                "视觉语言模型",
                "长文本对齐",
                "细粒度理解",
                "多级对齐",
                "CLIP增强"
            ],
            "_index": 237
        },
        {
            "title": "Training-free Clothing Region of Interest Self-correction for Virtual Try-On",
            "authors": [
                "Shengjie Lu",
                "Zhibin Wan",
                "Jiejie Liu",
                "Quan Zhang",
                "Mingjie Sun"
            ],
            "arxiv_id": "2512.07126v1",
            "summary": "VTON (Virtual Try-ON) aims at synthesizing the target clothing on a certain person, preserving the details of the target clothing while keeping the rest of the person unchanged. Existing methods suffer from the discrepancies between the generated clothing results and the target ones, in terms of the patterns, textures and boundaries. Therefore, we propose to use an energy function to impose constraints on the attention map extracted through the generation process. Thus, at each generation step, the attention can be more focused on the clothing region of interest, thereby influencing the generation results to be more consistent with the target clothing details. Furthermore, to address the limitation that existing evaluation metrics concentrate solely on image realism and overlook the alignment with target elements, we design a new metric, Virtual Try-on Inception Distance (VTID), to bridge this gap and ensure a more comprehensive assessment. On the VITON-HD and DressCode datasets, our approach has outperformed the previous state-of-the-art (SOTA) methods by 1.4%, 2.3%, 12.3%, and 5.8% in the traditional metrics of LPIPS, FID, KID, and the new VTID metrics, respectively. Additionally, by applying the generated data to downstream Clothing-Change Re-identification (CC-Reid) methods, we have achieved performance improvements of 2.5%, 1.1%, and 1.6% on the LTCC, PRCC, VC-Clothes datasets in the metrics of Rank-1. The code of our method is public at https://github.com/MrWhiteSmall/CSC-VTON.git.",
            "headline_zh": "提出无训练服装感兴趣区域自校正方法以提升虚拟试穿效果",
            "intro_zh": [
                "现有虚拟试穿方法在图案、纹理和边界上存在生成服装与目标服装的差异问题",
                "通过能量函数约束生成过程中的注意力图，使注意力更集中于服装区域，改善细节一致性",
                "在VITON-HD和DressCode数据集上，传统指标和新VTID指标均优于先前方法，并提升下游任务性能"
            ],
            "tags_zh": [
                "虚拟试穿",
                "注意力机制",
                "能量函数",
                "评估指标",
                "服装区域校正",
                "生成对抗网络"
            ],
            "_index": 238
        },
        {
            "title": "RisConFix: LLM-based Automated Repair of Risk-Prone Drone Configurations",
            "authors": [
                "Liping Han",
                "Tingting Nie",
                "Le Yu",
                "Mingzhe Hu",
                "Tao Yue"
            ],
            "arxiv_id": "2512.07122v1",
            "summary": "Flight control software is typically designed with numerous configurable parameters governing multiple functionalities, enabling flexible adaptation to mission diversity and environmental uncertainty. Although developers and manufacturers usually provide recommendations for these parameters to ensure safe and stable operations, certain combinations of parameters with recommended values may still lead to unstable flight behaviors, thereby degrading the drone's robustness. To this end, we propose a Large Language Model (LLM) based approach for real-time repair of risk-prone configurations (named RisConFix) that degrade drone robustness. RisConFix continuously monitors the drone's operational state and automatically triggers a repair mechanism once abnormal flight behaviors are detected. The repair mechanism leverages an LLM to analyze relationships between configuration parameters and flight states, and then generates corrective parameter updates to restore flight stability. To ensure the validity of the updated configuration, RisConFix operates as an iterative process; it continuously monitors the drone's flight state and, if an anomaly persists after applying an update, automatically triggers the next repair cycle. We evaluated RisConFix through a case study of ArduPilot (with 1,421 groups of misconfigurations). Experimental results show that RisConFix achieved a best repair success rate of 97% and an optimal average number of repairs of 1.17, demonstrating its capability to effectively and efficiently repair risk-prone configurations in real time.",
            "headline_zh": "提出基于大语言模型的实时修复方法RisConFix，以解决无人机风险配置导致的飞行不稳定问题。",
            "intro_zh": [
                "无人机配置参数组合可能引发飞行不稳定，降低鲁棒性。",
                "RisConFix利用LLM分析参数与状态关系，迭代生成修复更新。",
                "在ArduPilot案例中，修复成功率最高达97%，平均修复次数1.17。"
            ],
            "tags_zh": [
                "无人机配置修复",
                "大语言模型应用",
                "实时监控",
                "迭代修复",
                "飞行稳定性"
            ],
            "_index": 239
        },
        {
            "title": "Chromatic Feature Vectors for 2-Trees: Exact Formulas for Partition Enumeration with Network Applications",
            "authors": [
                "J. Allagan",
                "G. Morgan",
                "S. Langley",
                "R. Lopez-Bonilla",
                "V. Deriglazov"
            ],
            "arxiv_id": "2512.07120v1",
            "summary": "We establish closed-form enumeration formulas for chromatic feature vectors of 2-trees under the bichromatic triangle constraint. These efficiently computable structural features derive from constrained graph colorings where each triangle uses exactly two colors, forbidding monochromatic and rainbow triangles, a constraint arising in distributed systems where components avoid complete concentration or isolation. For theta graphs Theta_n, we prove r_k(Theta_n) = S(n-2, k-1) for k >= 3 (Stirling numbers of the second kind) and r_2(Theta_n) = 2^(n-2) + 1, computable in O(n) time. For fan graphs Phi_n, we establish r_2(Phi_n) = F_{n+1} (Fibonacci numbers) and derive explicit formulas r_k(Phi_n) = sum_{t=k-1}^{n-1} a_{n-1,t} * S(t, k-1) with efficiently computable binomial coefficients, achieving O(n^2) computation per component. Unlike classical chromatic polynomials, which assign identical features to all n-vertex 2-trees, bichromatic constraints provide informative structural features. While not complete graph invariants, these features capture meaningful structural properties through connections to Fibonacci polynomials, Bell numbers, and independent set enumeration. Applications include Byzantine fault tolerance in hierarchical networks, VM allocation in cloud computing, and secret-sharing protocols in distributed cryptography.",
            "headline_zh": "提出2-树双色三角约束下的色度特征向量精确公式，用于分布式系统结构分析。",
            "intro_zh": [
                "核心问题：在2-树中，每个三角形必须使用恰好两种颜色，避免单色或全色三角形，源于分布式系统中组件避免完全集中或隔离的需求。",
                "方法要点：为theta图和fan图建立闭式枚举公式，如r_k(Theta_n) = S(n-2, k-1)和r_2(Phi_n) = F_{n+1}，计算复杂度为O(n)或O(n^2)。",
                "实验或效果：特征向量可高效计算，应用于拜占庭容错、云虚拟机分配和分布式密码学秘密共享协议。"
            ],
            "tags_zh": [
                "图着色",
                "2-树",
                "分布式系统",
                "结构特征",
                "枚举公式",
                "计算复杂度"
            ],
            "_index": 240
        },
        {
            "title": "Surrogate compliance modeling enables reinforcement learned locomotion gaits for soft robots",
            "authors": [
                "Jue Wang",
                "Mingsong Jiang",
                "Luis A. Ramirez",
                "Bilige Yang",
                "Mujun Zhang",
                "Esteban Figueroa",
                "Wenzhong Yan",
                "Rebecca Kramer-Bottiglio"
            ],
            "arxiv_id": "2512.07114v1",
            "summary": "Adaptive morphogenetic robots adapt their morphology and control policies to meet changing tasks and environmental conditions. Many such systems leverage soft components, which enable shape morphing but also introduce simulation and control challenges. Soft-body simulators remain limited in accuracy and computational tractability, while rigid-body simulators cannot capture soft-material dynamics. Here, we present a surrogate compliance modeling approach: rather than explicitly modeling soft-body physics, we introduce indirect variables representing soft-material deformation within a rigid-body simulator. We validate this approach using our amphibious robotic turtle, a quadruped with soft morphing limbs designed for multi-environment locomotion. By capturing deformation effects as changes in effective limb length and limb center of mass, and by applying reinforcement learning with extensive randomization of these indirect variables, we achieve reliable policy learning entirely in a rigid-body simulation. The resulting gaits transfer directly to hardware, demonstrating high-fidelity sim-to-real performance on hard, flat substrates and robust, though lower-fidelity, transfer on rheologically complex terrains. The learned closed-loop gaits exhibit unprecedented terrestrial maneuverability and achieve an order-of-magnitude reduction in cost of transport compared to open-loop baselines. Field experiments with the robot further demonstrate stable, multi-gait locomotion across diverse natural terrains, including gravel, grass, and mud.",
            "headline_zh": "提出代理柔顺建模方法，在刚体模拟中实现软机器人强化学习步态",
            "intro_zh": [
                "软机器人模拟与控制面临精度与计算挑战，刚体模拟无法捕捉软材料动力学。",
                "引入间接变量表示软材料变形，在刚体模拟中通过强化学习训练步态策略。",
                "方法在硬件上实现高保真迁移，提升陆地机动性并显著降低运输成本。"
            ],
            "tags_zh": [
                "软机器人",
                "强化学习",
                "模拟到现实",
                "柔顺建模",
                "步态控制",
                "多环境运动"
            ],
            "_index": 241
        },
        {
            "title": "PlantBiMoE: A Bidirectional Foundation Model with SparseMoE for Plant Genomes",
            "authors": [
                "Kepeng Lin",
                "Qizhe Zhang",
                "Rui Wang",
                "Xuehai Hu",
                "Wei Xu"
            ],
            "arxiv_id": "2512.07113v1",
            "summary": "Understanding the underlying linguistic rules of plant genomes remains a fundamental challenge in computational biology. Recent advances including AgroNT and PDLLMs have made notable progress although, they suffer from excessive parameter size and limited ability to model the bidirectional nature of DNA strands respectively. To address these limitations, we propose PlantBiMoE, a lightweight and expressive plant genome language model that integrates bidirectional Mamba and a Sparse Mixture-of-Experts (SparseMoE) framework. The bidirectional Mamba enables the model to effectively capture structural dependencies across both the forward and reverse DNA strands, while SparseMoE significantly reduces the number of active parameters, improving computational efficiency without sacrificing modeling capacity. We evaluated and tested our model on the Modified Plants Genome Benchmark (MPGB), an enhanced genomic benchmark, which consolidates 31 datasets across 11 representative tasks, with input sequence lengths ranging from 50 to 6,000 bp. Experimental results demonstrate that PlantBiMoE achieves the best performance on 20 out of 31 datasets and the average best when comparing with existing models. In summary, all above results demonstrate that our model can effectively represent plant genomic sequences, serving as a robust computational tool for diverse genomic tasks, while making substantive contributions to plant genomics, gene editing, and synthetic biology. The code is available at: https://github.com/HUST-Keep-Lin/PlantBiMoE",
            "headline_zh": "提出PlantBiMoE，结合双向Mamba与稀疏专家混合，以轻量高效建模植物基因组双向依赖性。",
            "intro_zh": [
                "核心问题：现有模型参数过大或无法有效建模DNA链双向性，限制植物基因组语言理解。",
                "方法要点：集成双向Mamba捕获DNA正反向结构依赖，采用稀疏专家混合减少活跃参数提升效率。",
                "实验或效果：在MPGB基准31个数据集上，20个表现最佳，平均性能优于现有模型，验证其有效性。"
            ],
            "tags_zh": [
                "植物基因组语言模型",
                "双向Mamba",
                "稀疏专家混合",
                "计算生物学",
                "基因组基准测试",
                "轻量模型"
            ],
            "_index": 242
        },
        {
            "title": "FOAM: Blocked State Folding for Memory-Efficient LLM Training",
            "authors": [
                "Ziqing Wen",
                "Jiahuan Wang",
                "Ping Luo",
                "Dongsheng Li",
                "Tao Sun"
            ],
            "arxiv_id": "2512.07112v1",
            "summary": "Large language models (LLMs) have demonstrated remarkable performance due to their large parameter counts and extensive training data. However, their scale leads to significant memory bottlenecks during training, especially when using memory-intensive optimizers like Adam. Existing memory-efficient approaches often rely on techniques such as singular value decomposition (SVD), projections, or weight freezing, which can introduce substantial computational overhead, require additional memory for projections, or degrade model performance. In this paper, we propose Folded Optimizer with Approximate Moment (FOAM), a method that compresses optimizer states by computing block-wise gradient means and incorporates a residual correction to recover lost information. Theoretically, FOAM achieves convergence rates equivalent to vanilla Adam under standard non-convex optimization settings. Empirically, FOAM reduces total training memory by approximately 50\\%, eliminates up to 90\\% of optimizer state memory overhead, and accelerates convergence. Furthermore, FOAM is compatible with other memory-efficient optimizers, delivering performance and throughput that match or surpass both full-rank and existing memory-efficient baselines.",
            "headline_zh": "提出FOAM方法以解决大语言模型训练中的内存瓶颈问题",
            "intro_zh": [
                "核心问题：大语言模型训练因Adam等优化器状态占用大量内存，导致内存瓶颈",
                "方法要点：通过分块梯度均值压缩优化器状态，并引入残差校正恢复信息损失",
                "实验或效果：减少总训练内存约50%，消除高达90%优化器状态内存开销，加速收敛"
            ],
            "tags_zh": [
                "大语言模型训练",
                "内存优化",
                "优化器压缩",
                "梯度均值",
                "残差校正",
                "收敛加速"
            ],
            "_index": 243
        },
        {
            "title": "MSN: Multi-directional Similarity Network for Hand-crafted and Deep-synthesized Copy-Move Forgery Detection",
            "authors": [
                "Liangwei Jiang",
                "Jinluo Xie",
                "Yecheng Huang",
                "Hua Zhang",
                "Hongyu Yang",
                "Di Huang"
            ],
            "arxiv_id": "2512.07110v1",
            "summary": "Copy-move image forgery aims to duplicate certain objects or to hide specific contents with copy-move operations, which can be achieved by a sequence of manual manipulations as well as up-to-date deep generative network-based swapping. Its detection is becoming increasingly challenging for the complex transformations and fine-tuned operations on the tampered regions. In this paper, we propose a novel two-stream model, namely Multi-directional Similarity Network (MSN), to accurate and efficient copy-move forgery detection. It addresses the two major limitations of existing deep detection models in \\textbf{representation} and \\textbf{localization}, respectively. In representation, an image is hierarchically encoded by a multi-directional CNN network, and due to the diverse augmentation in scales and rotations, the feature achieved better measures the similarity between sampled patches in two streams. In localization, we design a 2-D similarity matrix based decoder, and compared with the current 1-D similarity vector based one, it makes full use of spatial information in the entire image, leading to the improvement in detecting tampered regions. Beyond the method, a new forgery database generated by various deep neural networks is presented, as a new benchmark for detecting the growing deep-synthesized copy-move. Extensive experiments are conducted on two classic image forensics benchmarks, \\emph{i.e.} CASIA CMFD and CoMoFoD, and the newly presented one. The state-of-the-art results are reported, which demonstrate the effectiveness of the proposed approach.",
            "headline_zh": "提出多方向相似性网络以提升手工与深度合成复制-移动伪造检测的准确性和效率",
            "intro_zh": [
                "核心问题：复制-移动伪造检测面临复杂变换和精细操作，现有模型在表示和定位方面存在局限",
                "方法要点：采用双流模型，通过多方向CNN分层编码图像，并设计基于2-D相似性矩阵的解码器以充分利用空间信息",
                "实验或效果：在CASIA CMFD、CoMoFoD及新数据库上取得先进结果，验证了方法的有效性"
            ],
            "tags_zh": [
                "复制-移动伪造检测",
                "多方向相似性网络",
                "双流模型",
                "2-D相似性矩阵",
                "深度合成伪造",
                "图像取证"
            ],
            "_index": 244
        },
        {
            "title": "A Neural Affinity Framework for Abstract Reasoning: Diagnosing the Compositional Gap in Transformer Architectures via Procedural Task Taxonomy",
            "authors": [
                "Miguel Ingram",
                "Arthur Joseph Merritt"
            ],
            "arxiv_id": "2512.07109v1",
            "summary": "Responding to Hodel et al.'s (2024) call for a formal definition of task relatedness in re-arc, we present the first 9-category taxonomy of all 400 tasks, validated at 97.5% accuracy via rule-based code analysis. We prove the taxonomy's visual coherence by training a CNN on raw grid pixels (95.24% accuracy on S3, 36.25% overall, 3.3x chance), then apply the taxonomy diagnostically to the original ARC-AGI-2 test set. Our curriculum analysis reveals 35.3% of tasks exhibit low neural affinity for Transformers--a distributional bias mirroring ARC-AGI-2. To probe this misalignment, we fine-tuned a 1.7M-parameter Transformer across 302 tasks, revealing a profound Compositional Gap: 210 of 302 tasks (69.5%) achieve >80% cell accuracy (local patterns) but <10% grid accuracy (global synthesis). This provides direct evidence for a Neural Affinity Ceiling Effect, where performance is bounded by architectural suitability, not curriculum. Applying our framework to Li et al.'s independent ViTARC study (400 specialists, 1M examples each) confirms its predictive power: Very Low affinity tasks achieve 51.9% versus 77.7% for High affinity (p<0.001), with a task at 0% despite massive data. The taxonomy enables precise diagnosis: low-affinity tasks (A2) hit hard ceilings, while high-affinity tasks (C1) reach 99.8%. These findings indicate that progress requires hybrid architectures with affinity-aligned modules. We release our validated taxonomy,",
            "headline_zh": "提出神经亲和力框架以诊断Transformer在抽象推理任务中的组合性差距",
            "intro_zh": [
                "核心问题：Transformer架构在抽象推理任务中存在组合性差距，导致局部模式学习与全局合成能力不匹配",
                "方法要点：基于规则代码分析构建9类任务分类法，用于评估任务与神经网络的亲和力",
                "实验或效果：在ARC-AGI-2测试集上，69.5%的任务显示高局部准确率但低全局准确率，验证了神经亲和力天花板效应"
            ],
            "tags_zh": [
                "抽象推理",
                "Transformer架构",
                "任务分类法",
                "神经亲和力",
                "组合性差距",
                "ARC-AGI-2"
            ],
            "_index": 245
        },
        {
            "title": "COREA: Coarse-to-Fine 3D Representation Alignment Between Relightable 3D Gaussians and SDF via Bidirectional 3D-to-3D Supervision",
            "authors": [
                "Jaeyoon Lee",
                "Hojoon Jung",
                "Sungtae Hwang",
                "Jihyong Oh",
                "Jongwon Choi"
            ],
            "arxiv_id": "2512.07107v1",
            "summary": "We present COREA, the first unified framework that jointly learns relightable 3D Gaussians and a Signed Distance Field (SDF) for accurate geometry reconstruction and faithful relighting. While recent 3D Gaussian Splatting (3DGS) methods have extended toward mesh reconstruction and physically-based rendering (PBR), their geometry is still learned from 2D renderings, leading to coarse surfaces and unreliable BRDF-lighting decomposition. To address these limitations, COREA introduces a coarse-to-fine bidirectional 3D-to-3D alignment strategy that allows geometric signals to be learned directly in 3D space. Within this strategy, depth provides coarse alignment between the two representations, while depth gradients and normals refine fine-scale structure, and the resulting geometry supports stable BRDF-lighting decomposition. A density-control mechanism further stabilizes Gaussian growth, balancing geometric fidelity with memory efficiency. Experiments on standard benchmarks demonstrate that COREA achieves superior performance in novel-view synthesis, mesh reconstruction, and PBR within a unified framework.",
            "headline_zh": "提出COREA框架，通过双向3D到3D监督实现可重光照3D高斯与SDF的联合学习，以提升几何重建和重光照精度。",
            "intro_zh": [
                "核心问题：现有3D高斯方法依赖2D渲染学习几何，导致表面粗糙和BRDF-光照分解不可靠。",
                "方法要点：采用粗到细双向3D到3D对齐策略，结合深度、梯度、法线优化几何，并引入密度控制机制。",
                "实验或效果：在标准基准测试中，在新视角合成、网格重建和PBR方面表现优异。"
            ],
            "tags_zh": [
                "3D高斯溅射",
                "符号距离场",
                "几何重建",
                "物理渲染",
                "3D表示对齐",
                "可重光照模型"
            ],
            "_index": 246
        },
        {
            "title": "Dual Refinement Cycle Learning: Unsupervised Text Classification of Mamba and Community Detection on Text Attributed Graph",
            "authors": [
                "Hong Wang",
                "Yinglong Zhang",
                "Hanhan Guo",
                "Xuewen Xia",
                "Xing Xu"
            ],
            "arxiv_id": "2512.07100v1",
            "summary": "Pretrained language models offer strong text understanding capabilities but remain difficult to deploy in real-world text-attributed networks due to their heavy dependence on labeled data. Meanwhile, community detection methods typically ignore textual semantics, limiting their usefulness in downstream applications such as content organization, recommendation, and risk monitoring. To overcome these limitations, we present Dual Refinement Cycle Learning (DRCL), a fully unsupervised framework designed for practical scenarios where no labels or category definitions are available.\n  DRCL integrates structural and semantic information through a warm-start initialization and a bidirectional refinement cycle between a GCN-based Community Detection Module (GCN-CDM) and a Text Semantic Modeling Module (TSMM). The two modules iteratively exchange pseudo-labels, allowing semantic cues to enhance structural clustering and structural patterns to guide text representation learning without manual supervision.\n  Across several text-attributed graph datasets, DRCL consistently improves the structural and semantic quality of discovered communities. Moreover, a Mamba-based classifier trained solely from DRCL's community signals achieves accuracy comparable to supervised models, demonstrating its potential for deployment in large-scale systems where labeled data are scarce or costly.",
            "headline_zh": "提出双循环精炼学习框架，以无监督方式整合结构与语义信息于文本属性图。",
            "intro_zh": [
                "核心问题：预训练语言模型依赖标注数据，社区检测方法忽略文本语义，限制在文本属性网络中的应用。",
                "方法要点：通过GCN社区检测模块与文本语义建模模块的双向循环精炼，迭代交换伪标签，实现无监督整合。",
                "实验或效果：在多个数据集上提升社区质量，基于Mamba的分类器达到接近监督模型的准确度，适用于标注稀缺场景。"
            ],
            "tags_zh": [
                "无监督学习",
                "文本属性图",
                "社区检测",
                "语义建模",
                "伪标签交换",
                "Mamba分类器"
            ],
            "_index": 247
        },
        {
            "title": "VIGIL: A Reflective Runtime for Self-Healing Agents",
            "authors": [
                "Christopher Cruz"
            ],
            "arxiv_id": "2512.07094v1",
            "summary": "Agentic LLM frameworks promise autonomous behavior via task decomposition, tool use, and iterative planning, but most deployed systems remain brittle. They lack runtime introspection, cannot diagnose their own failure modes, and do not improve over time without human intervention. In practice, many agent stacks degrade into decorated chains of LLM calls with no structural mechanisms for reliability. We present VIGIL (Verifiable Inspection and Guarded Iterative Learning), a reflective runtime that supervises a sibling agent and performs autonomous maintenance rather than task execution. VIGIL ingests behavioral logs, appraises each event into a structured emotional representation, maintains a persistent EmoBank with decay and contextual policies, and derives an RBT diagnosis that sorts recent behavior into strengths, opportunities, and failures. From this analysis, VIGIL generates both guarded prompt updates that preserve core identity semantics and read only code proposals produced by a strategy engine that operates on log evidence and code hotspots. VIGIL functions as a state gated pipeline. Illegal transitions produce explicit errors rather than allowing the LLM to improvise. In a reminder latency case study, VIGIL identified elevated lag, proposed prompt and code repairs, and when its own diagnostic tool failed due to a schema conflict, it surfaced the internal error, produced a fallback diagnosis, and emitted a repair plan. This demonstrates meta level self repair in a deployed agent runtime.",
            "headline_zh": "提出VIGIL反射运行时以增强自主代理的自我修复能力",
            "intro_zh": [
                "问题：现有代理框架缺乏运行时自省，无法诊断失败模式，依赖人工干预。",
                "方法：VIGIL通过行为日志分析、情感表示和RBT诊断，生成提示更新和代码修复提案。",
                "效果：在案例研究中，VIGIL识别延迟问题，实现元级自我修复，提升系统可靠性。"
            ],
            "tags_zh": [
                "自主代理",
                "反射运行时",
                "自我修复",
                "行为诊断",
                "提示工程",
                "代码修复"
            ],
            "_index": 248
        },
        {
            "title": "The Geometry of Persona: Disentangling Personality from Reasoning in Large Language Models",
            "authors": [
                "Zhixiang Wang"
            ],
            "arxiv_id": "2512.07092v1",
            "summary": "Background: The deployment of personalized Large Language Models (LLMs) is currently constrained by the stability-plasticity dilemma. Prevailing alignment methods, such as Supervised Fine-Tuning (SFT), rely on stochastic weight updates that often incur an \"alignment tax\" -- degrading general reasoning capabilities.\n  Methods: We propose the Soul Engine, a framework based on the Linear Representation Hypothesis, which posits that personality traits exist as orthogonal linear subspaces. We introduce SoulBench, a dataset constructed via dynamic contextual sampling. Using a dual-head architecture on a frozen Qwen-2.5 base, we extract disentangled personality vectors without modifying the backbone weights.\n  Results: Our experiments demonstrate three breakthroughs. First, High-Precision Profiling: The model achieves a Mean Squared Error (MSE) of 0.011 against psychological ground truth. Second, Geometric Orthogonality: T-SNE visualization confirms that personality manifolds are distinct and continuous, allowing for \"Zero-Shot Personality Injection\" that maintains original model intelligence. Third, Deterministic Steering: We achieve robust control over behavior via vector arithmetic, validated through extensive ablation studies.\n  Conclusion: This work challenges the necessity of fine-tuning for personalization. By transitioning from probabilistic prompting to deterministic latent intervention, we provide a mathematically rigorous foundation for safe, controllable AI personalization.",
            "headline_zh": "提出Soul Engine框架，基于线性表示假设，实现大语言模型中人格与推理能力的解耦，以解决个性化部署中的稳定性-可塑性困境。",
            "intro_zh": [
                "核心问题：大语言模型个性化部署面临稳定性-可塑性困境，现有对齐方法如监督微调可能导致推理能力下降。",
                "方法要点：基于线性表示假设，使用双头架构在冻结基座模型上提取解耦的人格向量，无需修改权重。",
                "实验或效果：实现高精度人格分析、几何正交性验证和确定性行为控制，支持零样本人格注入并保持原始智能。"
            ],
            "tags_zh": [
                "大语言模型个性化",
                "人格解耦",
                "线性表示假设",
                "零样本人格注入",
                "确定性控制",
                "SoulBench数据集"
            ],
            "_index": 249
        },
        {
            "title": "A Flexible Funnel-Shaped Robotic Hand with an Integrated Single-Sheet Valve for Milligram-Scale Powder Handling",
            "authors": [
                "Tomoya Takahashi",
                "Yusaku Nakajima",
                "Cristian Camilo Beltran-Hernandez",
                "Yuki Kuroda",
                "Kazutoshi Tanaka",
                "Masashi Hamaya",
                "Kanta Ono",
                "Yoshitaka Ushiku"
            ],
            "arxiv_id": "2512.07091v1",
            "summary": "Laboratory Automation (LA) has the potential to accelerate solid-state materials discovery by enabling continuous robotic operation without human intervention. While robotic systems have been developed for tasks such as powder grinding and X-ray diffraction (XRD) analysis, fully automating powder handling at the milligram scale remains a significant challenge due to the complex flow dynamics of powders and the diversity of laboratory tasks. To address this challenge, this study proposes a novel, funnel-shaped, flexible robotic hand that preserves the softness and conical sheet designs in prior work while incorporating a controllable valve at the cone apex to enable precise, incremental dispensing of milligram-scale powder quantities. The hand is integrated with an external balance through a feedback control system based on a model of powder flow and online parameter identification. Experimental evaluations with glass beads, monosodium glutamate, and titanium dioxide demonstrated that 80% of the trials achieved an error within 2 mg, and the maximum error observed was approximately 20 mg across a target range of 20 mg to 3 g. In addition, by incorporating flow prediction models commonly used for hoppers and performing online parameter identification, the system is able to adapt to variations in powder dynamics. Compared to direct PID control, the proposed model-based control significantly improved both accuracy and convergence speed. These results highlight the potential of the proposed system to enable efficient and flexible powder weighing, with scalability toward larger quantities and applicability to a broad range of laboratory automation tasks.",
            "headline_zh": "提出集成单阀的柔性漏斗形机械手，以解决毫克级粉末精确分装的实验室自动化难题。",
            "intro_zh": [
                "核心问题：毫克级粉末处理自动化因粉末流动复杂性和任务多样性而具挑战性。",
                "方法要点：设计柔性漏斗形机械手，集成可控阀和基于粉末流动模型的反馈控制系统。",
                "实验或效果：实验显示80%试验误差在2 mg内，最大误差约20 mg，模型控制优于PID控制。"
            ],
            "tags_zh": [
                "实验室自动化",
                "粉末处理",
                "柔性机械手",
                "反馈控制",
                "毫克级分装",
                "流动模型"
            ],
            "_index": 250
        },
        {
            "title": "Leveraging KV Similarity for Online Structured Pruning in LLMs",
            "authors": [
                "Jungmin Lee",
                "Gwangeun Byeon",
                "Yulhwa Kim",
                "Seokin Hong"
            ],
            "arxiv_id": "2512.07090v1",
            "summary": "Pruning has emerged as a promising direction for accelerating large language model (LLM) inference, yet existing approaches often suffer from instability because they rely on offline calibration data that may not generalize across inputs. In this work, we introduce Token Filtering, a lightweight online structured pruning technique that makes pruning decisions directly during inference without any calibration data. The key idea is to measure token redundancy via joint key-value similarity and skip redundant attention computations, thereby reducing inference cost while preserving critical information. To further enhance stability, we design a variance-aware fusion strategy that adaptively weights key and value similarity across heads, ensuring that informative tokens are retained even under high pruning ratios. This design introduces no additional memory overhead and provides a more reliable criterion for token importance. Extensive experiments on LLaMA-2 (7B/13B), LLaMA-3 (8B), and Mistral (7B) demonstrate that Token Filtering consistently outperforms prior structured pruning methods, preserving accuracy on commonsense reasoning benchmarks and maintaining strong performance on challenging tasks such as MMLU, even with 50% pruning.",
            "headline_zh": "提出Token Filtering在线结构化剪枝方法，利用KV相似性在LLM推理中跳过冗余计算以加速。",
            "intro_zh": [
                "现有LLM剪枝方法依赖离线校准数据，导致跨输入不稳定。",
                "基于键值相似性在线评估token冗余，自适应融合策略增强稳定性，无额外内存开销。",
                "在LLaMA和Mistral模型上实验，50%剪枝下保持MMLU等任务性能，优于先前方法。"
            ],
            "tags_zh": [
                "大语言模型剪枝",
                "在线推理加速",
                "键值相似性",
                "结构化剪枝",
                "注意力机制优化"
            ],
            "_index": 251
        },
        {
            "title": "ThinkTrap: Denial-of-Service Attacks against Black-box LLM Services via Infinite Thinking",
            "authors": [
                "Yunzhe Li",
                "Jianan Wang",
                "Hongzi Zhu",
                "James Lin",
                "Shan Chang",
                "Minyi Guo"
            ],
            "arxiv_id": "2512.07086v1",
            "summary": "Large Language Models (LLMs) have become foundational components in a wide range of applications, including natural language understanding and generation, embodied intelligence, and scientific discovery. As their computational requirements continue to grow, these models are increasingly deployed as cloud-based services, allowing users to access powerful LLMs via the Internet. However, this deployment model introduces a new class of threat: denial-of-service (DoS) attacks via unbounded reasoning, where adversaries craft specially designed inputs that cause the model to enter excessively long or infinite generation loops. These attacks can exhaust backend compute resources, degrading or denying service to legitimate users. To mitigate such risks, many LLM providers adopt a closed-source, black-box setting to obscure model internals. In this paper, we propose ThinkTrap, a novel input-space optimization framework for DoS attacks against LLM services even in black-box environments. The core idea of ThinkTrap is to first map discrete tokens into a continuous embedding space, then undertake efficient black-box optimization in a low-dimensional subspace exploiting input sparsity. The goal of this optimization is to identify adversarial prompts that induce extended or non-terminating generation across several state-of-the-art LLMs, achieving DoS with minimal token overhead. We evaluate the proposed attack across multiple commercial, closed-source LLM services. Our results demonstrate that, even far under the restrictive request frequency limits commonly enforced by these platforms, typically capped at ten requests per minute (10 RPM), the attack can degrade service throughput to as low as 1% of its original capacity, and in some cases, induce complete service failure.",
            "headline_zh": "提出ThinkTrap框架，在黑盒环境中通过无限思考对LLM服务进行拒绝服务攻击",
            "intro_zh": [
                "核心问题：黑盒LLM服务面临通过无限推理循环的拒绝服务攻击风险",
                "方法要点：将离散令牌映射到连续嵌入空间，在低维子空间进行高效黑盒优化",
                "实验或效果：在请求频率限制下，攻击可将服务吞吐量降至1%或导致完全失效"
            ],
            "tags_zh": [
                "拒绝服务攻击",
                "黑盒优化",
                "LLM安全",
                "无限推理",
                "嵌入空间"
            ],
            "_index": 252
        },
        {
            "title": "TRACE: A Generalizable Drift Detector for Streaming Data-Driven Optimization",
            "authors": [
                "Yuan-Ting Zhong",
                "Ting Huang",
                "Xiaolin Xiao",
                "Yue-Jiao Gong"
            ],
            "arxiv_id": "2512.07082v1",
            "summary": "Many optimization tasks involve streaming data with unknown concept drifts, posing a significant challenge as Streaming Data-Driven Optimization (SDDO). Existing methods, while leveraging surrogate model approximation and historical knowledge transfer, are often under restrictive assumptions such as fixed drift intervals and fully environmental observability, limiting their adaptability to diverse dynamic environments. We propose TRACE, a TRAnsferable C}oncept-drift Estimator that effectively detects distributional changes in streaming data with varying time scales. TRACE leverages a principled tokenization strategy to extract statistical features from data streams and models drift patterns using attention-based sequence learning, enabling accurate detection on unseen datasets and highlighting the transferability of learned drift patterns. Further, we showcase TRACE's plug-and-play nature by integrating it into a streaming optimizer, facilitating adaptive optimization under unknown drifts. Comprehensive experimental results on diverse benchmarks demonstrate the superior generalization, robustness, and effectiveness of our approach in SDDO scenarios.",
            "headline_zh": "提出TRACE以解决流数据驱动优化中的未知概念漂移检测问题",
            "intro_zh": [
                "核心问题：流数据驱动优化中未知概念漂移的检测挑战，现有方法受限于固定漂移间隔和完全环境可观测性假设",
                "方法要点：基于原则性标记化策略提取统计特征，利用注意力序列学习建模漂移模式，实现跨数据集可迁移检测",
                "实验或效果：在多样化基准测试中展示出优越的泛化性、鲁棒性和有效性，并集成到流优化器中实现自适应优化"
            ],
            "tags_zh": [
                "概念漂移检测",
                "流数据驱动优化",
                "注意力序列学习",
                "可迁移学习",
                "自适应优化"
            ],
            "_index": 253
        },
        {
            "title": "ClinNoteAgents: An LLM Multi-Agent System for Predicting and Interpreting Heart Failure 30-Day Readmission from Clinical Notes",
            "authors": [
                "Rongjia Zhou",
                "Chengzhuo Li",
                "Carl Yang",
                "Jiaying Lu"
            ],
            "arxiv_id": "2512.07081v1",
            "summary": "Heart failure (HF) is one of the leading causes of rehospitalization among older adults in the United States. Although clinical notes contain rich, detailed patient information and make up a large portion of electronic health records (EHRs), they remain underutilized for HF readmission risk analysis. Traditional computational models for HF readmission often rely on expert-crafted rules, medical thesauri, and ontologies to interpret clinical notes, which are typically written under time pressure and may contain misspellings, abbreviations, and domain-specific jargon. We present ClinNoteAgents, an LLM-based multi-agent framework that transforms free-text clinical notes into (1) structured representations of clinical and social risk factors for association analysis and (2) clinician-style abstractions for HF 30-day readmission prediction. We evaluate ClinNoteAgents on 3,544 notes from 2,065 patients (readmission rate=35.16%), demonstrating strong performance in extracting risk factors from free-text, identifying key contributing factors, and predicting readmission risk. By reducing reliance on structured fields and minimizing manual annotation and model training, ClinNoteAgents provides a scalable and interpretable approach to note-based HF readmission risk modeling in data-limited healthcare systems.",
            "headline_zh": "提出ClinNoteAgents，基于LLM多智能体系统从临床笔记预测和解释心衰30天再入院风险。",
            "intro_zh": [
                "核心问题：心衰再入院风险高，临床笔记信息丰富但利用不足，传统方法依赖专家规则且处理自由文本困难。",
                "方法要点：使用LLM多智能体框架，将自由文本临床笔记转化为结构化风险因素表示和临床风格抽象，用于关联分析和预测。",
                "实验或效果：在3,544份笔记上评估，展示强性能，减少对结构化字段依赖，提供可扩展和可解释的建模方法。"
            ],
            "tags_zh": [
                "心衰再入院预测",
                "临床笔记分析",
                "LLM多智能体系统",
                "医疗风险建模",
                "可解释人工智能"
            ],
            "_index": 254
        },
        {
            "title": "Procrustean Bed for AI-Driven Retrosynthesis: A Unified Framework for Reproducible Evaluation",
            "authors": [
                "Anton Morgunov",
                "Victor S. Batista"
            ],
            "arxiv_id": "2512.07079v1",
            "summary": "Progress in computer-aided synthesis planning (CASP) is obscured by the lack of standardized evaluation infrastructure and the reliance on metrics that prioritize topological completion over chemical validity. We introduce RetroCast, a unified evaluation suite that standardizes heterogeneous model outputs into a common schema to enable statistically rigorous, apples-to-apples comparison. The framework includes a reproducible benchmarking pipeline with stratified sampling and bootstrapped confidence intervals, accompanied by SynthArena, an interactive platform for qualitative route inspection. We utilize this infrastructure to evaluate leading search-based and sequence-based algorithms on a new suite of standardized benchmarks. Our analysis reveals a divergence between \"solvability\" (stock-termination rate) and route quality; high solvability scores often mask chemical invalidity or fail to correlate with the reproduction of experimental ground truths. Furthermore, we identify a \"complexity cliff\" in which search-based methods, despite high solvability rates, exhibit a sharp performance decay in reconstructing long-range synthetic plans compared to sequence-based approaches. We release the full framework, benchmark definitions, and a standardized database of model predictions to support transparent and reproducible development in the field.",
            "headline_zh": "提出RetroCast统一评估框架以解决计算机辅助合成规划中标准化评估缺失的问题",
            "intro_zh": [
                "核心问题：计算机辅助合成规划领域缺乏标准化评估基础设施，现有指标偏重拓扑完成度而非化学有效性",
                "方法要点：引入RetroCast框架，标准化异构模型输出，提供可复现基准测试流程和交互式路线检查平台SynthArena",
                "实验或效果：评估主流算法发现高可解性常掩盖化学无效性，搜索方法在长程合成计划重构中性能衰减明显"
            ],
            "tags_zh": [
                "计算机辅助合成规划",
                "标准化评估",
                "可复现基准测试",
                "化学有效性",
                "合成路线重构",
                "模型比较"
            ],
            "_index": 255
        },
        {
            "title": "DFIR-DETR: Frequency Domain Enhancement and Dynamic Feature Aggregation for Cross-Scene Small Object Detection",
            "authors": [
                "Bo Gao",
                "Jingcheng Tong",
                "Xingsheng Chen",
                "Han Yu",
                "Zichen Li"
            ],
            "arxiv_id": "2512.07078v1",
            "summary": "Detecting small objects in UAV remote sensing images and identifying surface defects in industrial inspection remain difficult tasks. These applications face common obstacles: features are sparse and weak, backgrounds are cluttered, and object scales vary dramatically. Current transformer-based detectors, while powerful, struggle with three critical issues. First, features degrade severely as networks downsample progressively. Second, spatial convolutions cannot capture long-range dependencies effectively. Third, standard upsampling methods inflate feature maps unnecessarily.\n  We introduce DFIR-DETR to tackle these problems through dynamic feature aggregation combined with frequency-domain processing. Our architecture builds on three novel components. The DCFA module uses dynamic K-sparse attention, cutting complexity from O(N2) down to O(NK), and employs spatial gated linear units for better nonlinear modeling. The DFPN module applies amplitude-normalized upsampling to prevent feature inflation and uses dual-path shuffle convolution to retain spatial details across scales. The FIRC3 module operates in the frequency domain, achieving global receptive fields without sacrificing efficiency.\n  We tested our method extensively on NEU-DET and VisDrone datasets. Results show mAP50 scores of 92.9% and 51.6% respectively-both state-of-the-art. The model stays lightweight with just 11.7M parameters and 41.2 GFLOPs. Strong performance across two very different domains confirms that DFIR-DETR generalizes well and works effectively in resource-limited settings for cross-scene small object detection.",
            "headline_zh": "提出DFIR-DETR，结合频域增强与动态特征聚合，用于跨场景小目标检测。",
            "intro_zh": [
                "核心问题：小目标特征稀疏、背景杂乱、尺度多变，现有Transformer检测器存在特征退化、长程依赖不足和特征图膨胀问题。",
                "方法要点：引入DCFA模块降低注意力复杂度，DFPN模块防止特征膨胀，FIRC3模块在频域实现全局感受野。",
                "实验或效果：在NEU-DET和VisDrone数据集上达到SOTA，模型轻量，参数11.7M，GFLOPs 41.2，跨场景泛化能力强。"
            ],
            "tags_zh": [
                "小目标检测",
                "Transformer检测器",
                "频域处理",
                "动态特征聚合",
                "轻量模型",
                "跨场景检测"
            ],
            "_index": 256
        },
        {
            "title": "Context-measure: Contextualizing Metric for Camouflage",
            "authors": [
                "Chen-Yang Wang",
                "Gepeng Ji",
                "Song Shao",
                "Ming-Ming Cheng",
                "Deng-Ping Fan"
            ],
            "arxiv_id": "2512.07076v1",
            "summary": "Camouflage is primarily context-dependent yet current metrics for camouflaged scenarios overlook this critical factor. Instead, these metrics are originally designed for evaluating general or salient objects, with an inherent assumption of uncorrelated spatial context. In this paper, we propose a new contextualized evaluation paradigm, Context-measure, built upon a probabilistic pixel-aware correlation framework. By incorporating spatial dependencies and pixel-wise camouflage quantification, our measure better aligns with human perception. Extensive experiments across three challenging camouflaged object segmentation datasets show that Context-measure delivers more reliability than existing context-independent metrics. Our measure can provide a foundational evaluation benchmark for various computer vision applications involving camouflaged patterns, such as agricultural, industrial, and medical scenarios. Code is available at https://github.com/pursuitxi/Context-measure.",
            "headline_zh": "提出Context-measure以解决伪装场景中现有度量忽略上下文依赖的问题。",
            "intro_zh": [
                "核心问题：现有伪装度量基于空间上下文无关假设，不符合伪装依赖上下文的特点。",
                "方法要点：基于概率像素感知相关框架，融入空间依赖性和像素级伪装量化。",
                "实验或效果：在三个伪装对象分割数据集上验证，比现有度量更可靠且更符合人类感知。"
            ],
            "tags_zh": [
                "伪装对象分割",
                "上下文度量",
                "概率像素感知",
                "空间依赖",
                "评估基准",
                "计算机视觉应用"
            ],
            "_index": 257
        }
    ]
}