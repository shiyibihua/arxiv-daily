{
    "papers": [
        {
            "title": "Dataset Distillation for Pre-Trained Self-Supervised Vision Models",
            "authors": [
                "George Cazenavette",
                "Antonio Torralba",
                "Vincent Sitzmann"
            ],
            "arxiv_id": "2511.16674v1",
            "summary": "The task of dataset distillation aims to find a small set of synthetic images such that training a model on them reproduces the performance of the same model trained on a much larger dataset of real samples. Existing distillation methods focus on synthesizing datasets that enable training randomly initialized models. In contrast, state-of-the-art vision approaches are increasingly building on large, pre-trained self-supervised models rather than training from scratch. In this paper, we investigate the problem of distilling datasets that enable us to optimally train linear probes on top of such large, pre-trained vision models. We introduce a method of dataset distillation for this task called Linear Gradient Matching that optimizes the synthetic images such that, when passed through a pre-trained feature extractor, they induce gradients in the linear classifier similar to those produced by the real data. Our method yields synthetic data that outperform all real-image baselines and, remarkably, generalize across pre-trained vision models, enabling us, for instance, to train a linear CLIP probe that performs competitively using a dataset distilled via a DINO backbone. Further, we show that our distilled datasets are exceptionally effective for fine-grained classification and provide a valuable tool for model interpretability, predicting, among other things, how similar two models' embedding spaces are under the platonic representation hypothesis or whether a model is sensitive to spurious correlations in adversarial datasets.",
            "headline_zh": "提出线性梯度匹配方法，为预训练自监督视觉模型蒸馏数据集以优化线性分类器训练。",
            "intro_zh": [
                "核心问题：现有数据集蒸馏方法未针对预训练自监督模型优化线性分类器训练。",
                "方法要点：优化合成图像，使其通过预训练特征提取器后，线性分类器梯度与真实数据相似。",
                "实验或效果：合成数据性能超越真实图像基线，并跨模型泛化，提升细粒度分类和模型可解释性。"
            ],
            "tags_zh": [
                "数据集蒸馏",
                "自监督学习",
                "线性分类器",
                "梯度匹配",
                "模型泛化",
                "细粒度分类"
            ],
            "_index": 0
        },
        {
            "title": "EvoLMM: Self-Evolving Large Multimodal Models with Continuous Rewards",
            "authors": [
                "Omkat Thawakar",
                "Shravan Venkatraman",
                "Ritesh Thawkar",
                "Abdelrahman Shaker",
                "Hisham Cholakkal",
                "Rao Muhammad Anwer",
                "Salman Khan",
                "Fahad Khan"
            ],
            "arxiv_id": "2511.16672v1",
            "summary": "Recent advances in large multimodal models (LMMs) have enabled impressive reasoning and perception abilities, yet most existing training pipelines still depend on human-curated data or externally verified reward models, limiting their autonomy and scalability. In this work, we strive to improve LMM reasoning capabilities in a purely unsupervised fashion (without any annotated data or reward distillation). To this end, we propose a self-evolving framework, named EvoLMM, that instantiates two cooperative agents from a single backbone model: a Proposer, which generates diverse, image-grounded questions, and a Solver, which solves them through internal consistency, where learning proceeds through a continuous self-rewarding process. This dynamic feedback encourages both the generation of informative queries and the refinement of structured reasoning without relying on ground-truth or human judgments. When using the popular Qwen2.5-VL as the base model, our EvoLMM yields consistent gains upto $\\sim$3\\% on multimodal math-reasoning benchmarks, including ChartQA, MathVista, and MathVision, using only raw training images. We hope our simple yet effective approach will serve as a solid baseline easing future research in self-improving LMMs in a fully-unsupervised fashion. Our code and models are available at https://github.com/mbzuai-oryx/EvoLMM.",
            "headline_zh": "提出EvoLMM框架以无监督方式提升大型多模态模型推理能力",
            "intro_zh": [
                "核心问题：现有大型多模态模型训练依赖人工数据或外部奖励，限制自主性和可扩展性。",
                "方法要点：使用提议者和求解者双代理，通过内部一致性和连续自奖励实现自我进化。",
                "实验或效果：在ChartQA等基准上，推理准确率提升约3%，仅使用原始训练图像。"
            ],
            "tags_zh": [
                "大型多模态模型",
                "无监督学习",
                "自我进化",
                "推理能力",
                "连续奖励",
                "多模态数学推理"
            ],
            "_index": 1
        },
        {
            "title": "NoPo-Avatar: Generalizable and Animatable Avatars from Sparse Inputs without Human Poses",
            "authors": [
                "Jing Wen",
                "Alexander G. Schwing",
                "Shenlong Wang"
            ],
            "arxiv_id": "2511.16673v1",
            "summary": "We tackle the task of recovering an animatable 3D human avatar from a single or a sparse set of images. For this task, beyond a set of images, many prior state-of-the-art methods use accurate \"ground-truth\" camera poses and human poses as input to guide reconstruction at test-time. We show that pose-dependent reconstruction degrades results significantly if pose estimates are noisy. To overcome this, we introduce NoPo-Avatar, which reconstructs avatars solely from images, without any pose input. By removing the dependence of test-time reconstruction on human poses, NoPo-Avatar is not affected by noisy human pose estimates, making it more widely applicable. Experiments on challenging THuman2.0, XHuman, and HuGe100K data show that NoPo-Avatar outperforms existing baselines in practical settings (without ground-truth poses) and delivers comparable results in lab settings (with ground-truth poses).",
            "headline_zh": "提出NoPo-Avatar从稀疏图像重建可动画3D人体化身，无需人体姿态输入",
            "intro_zh": [
                "核心问题：从单张或稀疏图像重建可动画3D人体化身，依赖姿态输入易受噪声影响",
                "方法要点：仅使用图像输入，消除测试时对人体姿态的依赖，提升鲁棒性",
                "实验效果：在THuman2.0等数据集上，无姿态输入时优于基线，有姿态时结果相当"
            ],
            "tags_zh": [
                "3D人体重建",
                "可动画化身",
                "稀疏图像输入",
                "姿态无关方法",
                "鲁棒性提升"
            ],
            "_index": 2
        },
        {
            "title": "Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation",
            "authors": [
                "Ziyu Guo",
                "Renrui Zhang",
                "Hongyu Li",
                "Manyuan Zhang",
                "Xinyan Chen",
                "Sifan Wang",
                "Yan Feng",
                "Peng Pei",
                "Pheng-Ann Heng"
            ],
            "arxiv_id": "2511.16671v1",
            "summary": "Recent advances in visual generation have increasingly explored the integration of reasoning capabilities. They incorporate textual reasoning, i.e., think, either before (as pre-planning) or after (as post-refinement) the generation process, yet they lack on-the-fly multimodal interaction during the generation itself. In this preliminary study, we introduce Thinking-while-Generating (TwiG), the first interleaved framework that enables co-evolving textual reasoning throughout the visual generation process. As visual content is progressively generating, textual reasoning is interleaved to both guide upcoming local regions and reflect on previously synthesized ones. This dynamic interplay produces more context-aware and semantically rich visual outputs. To unveil the potential of this framework, we investigate three candidate strategies, zero-shot prompting, supervised fine-tuning (SFT) on our curated TwiG-50K dataset, and reinforcement learning (RL) via a customized TwiG-GRPO strategy, each offering unique insights into the dynamics of interleaved reasoning. We hope this work inspires further research into interleaving textual reasoning for enhanced visual generation. Code will be released at: https://github.com/ZiyuGuo99/Thinking-while-Generating.",
            "headline_zh": "提出Thinking-while-Generating框架，在视觉生成过程中交织文本推理以增强语义丰富性。",
            "intro_zh": [
                "现有方法在视觉生成前或后集成文本推理，缺乏生成过程中的实时多模态交互。",
                "TwiG框架在视觉内容逐步生成时交织文本推理，指导局部区域并反思已合成内容。",
                "探索零样本提示、监督微调和强化学习策略，在TwiG-50K数据集上验证框架潜力。"
            ],
            "tags_zh": [
                "视觉生成",
                "文本推理",
                "多模态交互",
                "监督微调",
                "强化学习"
            ],
            "_index": 3
        },
        {
            "title": "Learning to Think Fast and Slow for Visual Language Models",
            "authors": [
                "Chenyu Lin",
                "Cheng Chi",
                "Jinlin Wu",
                "Sharon Li",
                "Kaiyang Zhou"
            ],
            "arxiv_id": "2511.16670v1",
            "summary": "When confronted with complex problems, we tend to think slowly; conversely, for simple questions, we think quickly. Such a two-system thinking mechanism allows us to efficiently allocate cognitive resources, enabling quick decision-making for straightforward issues while reserving deeper analytical thinking for more intricate challenges. However, existing reasoning-oriented visual language models (VLMs), whether trained with explicit chain-of-thought annotations or rule-based RL rewards, mainly pursue lengthy, detailed reasoning chains, which often lead to excessive computational costs. In this work, we propose a simple RL approach, which enables VLMs to automatically switch between fast and slow thinking modes depending on task difficulty. The approach consists of two stages: in the first stage, we label data as either requiring fast thinking or slow thinking based on the model output length, which is inspired by the observation that pre-trained VLMs typically produce answers of varying lengths for different types of questions; in the second stage, we train the model using GRPO along with the thinking mode labels to develop dual-mode thinking. Despite its simplicity, our model, named DualMindVLM, significantly outperforms the base model and achieves performance on par with state-of-the-art visual reasoning models, while maintaining exceptionally high token efficiency.",
            "headline_zh": "提出双模式思考方法以提升视觉语言模型的推理效率",
            "intro_zh": [
                "现有视觉语言模型推理链冗长，导致计算成本过高",
                "使用强化学习训练模型根据问题难度自动切换快慢思考模式",
                "模型性能媲美先进方法，同时显著提高令牌效率"
            ],
            "tags_zh": [
                "视觉语言模型",
                "强化学习",
                "双模式思考",
                "推理效率",
                "令牌优化"
            ],
            "_index": 4
        },
        {
            "title": "Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO",
            "authors": [
                "Junhao Cheng",
                "Liang Hou",
                "Xin Tao",
                "Jing Liao"
            ],
            "arxiv_id": "2511.16669v1",
            "summary": "While language models have become impactful in many real-world applications, video generation remains largely confined to entertainment. Motivated by video's inherent capacity to demonstrate physical-world information that is difficult to convey through language alone (e.g., imagine teaching someone to tie a tie using only text), we identify an underutilized opportunity to extend video as a new answer modality for Next-Event Prediction (NEP), formalized as Video-Next-Event Prediction (VNEP). While the established NEP task takes a video with a procedural or predictive question as input to predict the next event in text, VNEP requires dynamic video responses. This shift from telling to showing unlocks more intuitive and customized answers for procedural learning and creative exploration. However, this task remains challenging for existing models, as it demands an understanding of multimodal input, instruction-conditioned reasoning, and the generation of video with visual and semantic consistency. To address this, we introduce VANS, a model that leverages reinforcement learning to align a Vision-Language Model (VLM) with a Video Diffusion Model (VDM) for VNEP. The core of VANS is our proposed Joint-GRPO that orchestrates the VLM and VDM to function as a unit. Driven by a shared reward on their respective output, it optimizes the VLM to produce captions that are both accurate and friendly to visualize, while guiding the VDM to generate videos that are faithful to these captions and the input visual context. To enable this learning, we craft VANS-Data-100K, a dedicated dataset for the VNEP task. Experiments on procedural and predictive benchmarks demonstrate that VANS achieves state-of-the-art performance in both video event prediction and visualization. Codes are released in https://github.com/KlingTeam/VANS.",
            "headline_zh": "提出VANS模型以解决视频作为答案的下一事件预测任务",
            "intro_zh": [
                "核心问题：视频生成在下一事件预测中未充分利用，难以直观展示物理世界信息",
                "方法要点：使用Joint-GRPO强化学习对齐视觉语言模型和视频扩散模型",
                "实验或效果：在程序和预测基准上实现最先进的视频事件预测和可视化性能"
            ],
            "tags_zh": [
                "视频下一事件预测",
                "联合强化学习",
                "视频扩散模型",
                "视觉语言模型",
                "多模态生成"
            ],
            "_index": 5
        },
        {
            "title": "V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models",
            "authors": [
                "Yang Luo",
                "Xuanlei Zhao",
                "Baijiong Lin",
                "Lingting Zhu",
                "Liyao Tang",
                "Yuqi Liu",
                "Ying-Cong Chen",
                "Shengju Qian",
                "Xin Wang",
                "Yang You"
            ],
            "arxiv_id": "2511.16668v1",
            "summary": "Recent progress in generative video models, such as Veo-3, has shown surprising zero-shot reasoning abilities, creating a growing need for systematic and reliable evaluation. We introduce V-ReasonBench, a benchmark designed to assess video reasoning across four key dimensions: structured problem-solving, spatial cognition, pattern-based inference, and physical dynamics. The benchmark is built from both synthetic and real-world image sequences and provides a diverse set of answer-verifiable tasks that are reproducible, scalable, and unambiguous. Evaluations of six state-of-the-art video models reveal clear dimension-wise differences, with strong variation in structured, spatial, pattern-based, and physical reasoning. We further compare video models with strong image models, analyze common hallucination behaviors, and study how video duration affects Chain-of-Frames reasoning. Overall, V-ReasonBench offers a unified and reproducible framework for measuring video reasoning and aims to support the development of models with more reliable, human-aligned reasoning skills.",
            "headline_zh": "提出V-ReasonBench基准以评估视频生成模型的统一推理能力",
            "intro_zh": [
                "核心问题：视频生成模型缺乏系统推理评估，需可靠基准。",
                "方法要点：构建多维度基准，覆盖结构化、空间、模式和物理推理。",
                "实验或效果：评估六种模型，揭示推理维度差异和常见幻觉行为。"
            ],
            "tags_zh": [
                "视频推理基准",
                "多维度评估",
                "合成与真实数据",
                "答案可验证任务",
                "模型比较分析"
            ],
            "_index": 6
        },
        {
            "title": "SceneDesigner: Controllable Multi-Object Image Generation with 9-DoF Pose Manipulation",
            "authors": [
                "Zhenyuan Qin",
                "Xincheng Shuai",
                "Henghui Ding"
            ],
            "arxiv_id": "2511.16666v1",
            "summary": "Controllable image generation has attracted increasing attention in recent years, enabling users to manipulate visual content such as identity and style. However, achieving simultaneous control over the 9D poses (location, size, and orientation) of multiple objects remains an open challenge. Despite recent progress, existing methods often suffer from limited controllability and degraded quality, falling short of comprehensive multi-object 9D pose control. To address these limitations, we propose SceneDesigner, a method for accurate and flexible multi-object 9-DoF pose manipulation. SceneDesigner incorporates a branched network to the pre-trained base model and leverages a new representation, CNOCS map, which encodes 9D pose information from the camera view. This representation exhibits strong geometric interpretation properties, leading to more efficient and stable training. To support training, we construct a new dataset, ObjectPose9D, which aggregates images from diverse sources along with 9D pose annotations. To further address data imbalance issues, particularly performance degradation on low-frequency poses, we introduce a two-stage training strategy with reinforcement learning, where the second stage fine-tunes the model using a reward-based objective on rebalanced data. At inference time, we propose Disentangled Object Sampling, a technique that mitigates insufficient object generation and concept confusion in complex multi-object scenes. Moreover, by integrating user-specific personalization weights, SceneDesigner enables customized pose control for reference subjects. Extensive qualitative and quantitative experiments demonstrate that SceneDesigner significantly outperforms existing approaches in both controllability and quality. Code is publicly available at https://github.com/FudanCVL/SceneDesigner.",
            "headline_zh": "提出SceneDesigner以实现多对象9自由度姿态可控图像生成",
            "intro_zh": [
                "核心问题：现有方法难以同时控制多对象的9D姿态，导致可控性差和质量下降",
                "方法要点：引入分支网络和CNOCS图表示，采用两阶段训练和分离对象采样技术",
                "实验或效果：在可控性和质量上显著优于现有方法，支持用户自定义姿态控制"
            ],
            "tags_zh": [
                "可控图像生成",
                "多对象姿态控制",
                "9自由度姿态",
                "CNOCS图表示",
                "两阶段训练",
                "分离对象采样"
            ],
            "_index": 7
        },
        {
            "title": "TriDiff-4D: Fast 4D Generation through Diffusion-based Triplane Re-posing",
            "authors": [
                "Eddie Pokming Sheung",
                "Qihao Liu",
                "Wufei Ma",
                "Prakhar Kaushik",
                "Jianwen Xie",
                "Alan Yuille"
            ],
            "arxiv_id": "2511.16662v1",
            "summary": "With the increasing demand for 3D animation, generating high-fidelity, controllable 4D avatars from textual descriptions remains a significant challenge. Despite notable efforts in 4D generative modeling, existing methods exhibit fundamental limitations that impede their broader applicability, including temporal and geometric inconsistencies, perceptual artifacts, motion irregularities, high computational costs, and limited control over dynamics. To address these challenges, we propose TriDiff-4D, a novel 4D generative pipeline that employs diffusion-based triplane re-posing to produce high-quality, temporally coherent 4D avatars. Our model adopts an auto-regressive strategy to generate 4D sequences of arbitrary length, synthesizing each 3D frame with a single diffusion process. By explicitly learning 3D structure and motion priors from large-scale 3D and motion datasets, TriDiff-4D enables skeleton-driven 4D generation that excels in temporal consistency, motion accuracy, computational efficiency, and visual fidelity. Specifically, TriDiff-4D first generates a canonical 3D avatar and a corresponding motion sequence from a text prompt, then uses a second diffusion model to animate the avatar according to the motion sequence, supporting arbitrarily long 4D generation. Experimental results demonstrate that TriDiff-4D significantly outperforms existing methods, reducing generation time from hours to seconds by eliminating the optimization process, while substantially improving the generation of complex motions with high-fidelity appearance and accurate 3D geometry.",
            "headline_zh": "提出TriDiff-4D以解决文本到4D生成中的一致性与效率问题",
            "intro_zh": [
                "核心问题：现有4D生成方法存在时间不一致、运动不规则和高计算成本等限制",
                "方法要点：采用扩散模型和自回归策略，通过三平面重定位生成任意长度4D序列",
                "实验或效果：显著减少生成时间至秒级，提升运动准确性和视觉保真度"
            ],
            "tags_zh": [
                "4D生成",
                "扩散模型",
                "三平面表示",
                "骨架驱动动画",
                "时间一致性"
            ],
            "_index": 8
        },
        {
            "title": "Dexterity from Smart Lenses: Multi-Fingered Robot Manipulation with In-the-Wild Human Demonstrations",
            "authors": [
                "Irmak Guzey",
                "Haozhi Qi",
                "Julen Urain",
                "Changhao Wang",
                "Jessica Yin",
                "Krishna Bodduluri",
                "Mike Lambeta",
                "Lerrel Pinto",
                "Akshara Rai",
                "Jitendra Malik",
                "Tingfan Wu",
                "Akash Sharma",
                "Homanga Bharadhwaj"
            ],
            "arxiv_id": "2511.16661v1",
            "summary": "Learning multi-fingered robot policies from humans performing daily tasks in natural environments has long been a grand goal in the robotics community. Achieving this would mark significant progress toward generalizable robot manipulation in human environments, as it would reduce the reliance on labor-intensive robot data collection. Despite substantial efforts, progress toward this goal has been bottle-necked by the embodiment gap between humans and robots, as well as by difficulties in extracting relevant contextual and motion cues that enable learning of autonomous policies from in-the-wild human videos. We claim that with simple yet sufficiently powerful hardware for obtaining human data and our proposed framework AINA, we are now one significant step closer to achieving this dream. AINA enables learning multi-fingered policies from data collected by anyone, anywhere, and in any environment using Aria Gen 2 glasses. These glasses are lightweight and portable, feature a high-resolution RGB camera, provide accurate on-board 3D head and hand poses, and offer a wide stereo view that can be leveraged for depth estimation of the scene. This setup enables the learning of 3D point-based policies for multi-fingered hands that are robust to background changes and can be deployed directly without requiring any robot data (including online corrections, reinforcement learning, or simulation). We compare our framework against prior human-to-robot policy learning approaches, ablate our design choices, and demonstrate results across nine everyday manipulation tasks. Robot rollouts are best viewed on our website: https://aina-robot.github.io.",
            "headline_zh": "提出AINA框架，从野外人类演示学习多指机器人操作策略",
            "intro_zh": [
                "核心问题：人类与机器人间的具身差距及野外视频中运动线索提取困难",
                "方法要点：使用Aria Gen 2眼镜收集数据，学习基于3D点的多指策略",
                "实验或效果：在九个日常任务中验证，无需机器人数据直接部署"
            ],
            "tags_zh": [
                "机器人操作学习",
                "多指手策略",
                "野外人类演示",
                "3D点云策略",
                "Aria眼镜数据收集"
            ],
            "_index": 9
        },
        {
            "title": "PartUV: Part-Based UV Unwrapping of 3D Meshes",
            "authors": [
                "Zhaoning Wang",
                "Xinyue Wei",
                "Ruoxi Shi",
                "Xiaoshuai Zhang",
                "Hao Su",
                "Minghua Liu"
            ],
            "arxiv_id": "2511.16659v1",
            "summary": "UV unwrapping flattens 3D surfaces to 2D with minimal distortion, often requiring the complex surface to be decomposed into multiple charts. Although extensively studied, existing UV unwrapping methods frequently struggle with AI-generated meshes, which are typically noisy, bumpy, and poorly conditioned. These methods often produce highly fragmented charts and suboptimal boundaries, introducing artifacts and hindering downstream tasks. We introduce PartUV, a part-based UV unwrapping pipeline that generates significantly fewer, part-aligned charts while maintaining low distortion. Built on top of a recent learning-based part decomposition method PartField, PartUV combines high-level semantic part decomposition with novel geometric heuristics in a top-down recursive framework. It ensures each chart's distortion remains below a user-specified threshold while minimizing the total number of charts. The pipeline integrates and extends parameterization and packing algorithms, incorporates dedicated handling of non-manifold and degenerate meshes, and is extensively parallelized for efficiency. Evaluated across four diverse datasets, including man-made, CAD, AI-generated, and Common Shapes, PartUV outperforms existing tools and recent neural methods in chart count and seam length, achieves comparable distortion, exhibits high success rates on challenging meshes, and enables new applications like part-specific multi-tiles packing. Our project page is at https://www.zhaoningwang.com/PartUV.",
            "headline_zh": "提出PartUV基于部件分解的UV展开方法，以处理AI生成网格的噪声和碎片化问题。",
            "intro_zh": [
                "核心问题：现有UV展开方法对AI生成网格处理不佳，导致碎片化图表和边界问题。",
                "方法要点：结合语义部件分解与几何启发式，在递归框架中控制失真并最小化图表数。",
                "实验效果：在多个数据集上优于现有方法，图表数和接缝长度减少，失真可比较。"
            ],
            "tags_zh": [
                "UV展开",
                "部件分解",
                "网格参数化",
                "AI生成网格",
                "多图表打包"
            ],
            "_index": 10
        },
        {
            "title": "Solving Spatial Supersensing Without Spatial Supersensing",
            "authors": [
                "Vishaal Udandarao",
                "Shyamgopal Karthik",
                "Surabhi S. Nath",
                "Andreas Hochlehnert",
                "Matthias Bethge",
                "Ameya Prabhu"
            ],
            "arxiv_id": "2511.16655v1",
            "summary": "Cambrian-S aims to take the first steps towards improving video world models with spatial supersensing by introducing (i) two benchmarks, VSI-Super-Recall (VSR) and VSI-Super-Counting (VSC), and (ii) bespoke predictive sensing inference strategies tailored to each benchmark. In this work, we conduct a critical analysis of Cambrian-S across both these fronts. First, we introduce a simple baseline, NoSense, which discards almost all temporal structure and uses only a bag-of-words SigLIP model, yet near-perfectly solves VSR, achieving 95% accuracy even on 4-hour videos. This shows benchmarks like VSR can be nearly solved without spatial cognition, world modeling or spatial supersensing. Second, we hypothesize that the tailored inference methods proposed by Cambrian-S likely exploit shortcut heuristics in the benchmark. We illustrate this with a simple sanity check on the VSC benchmark, called VSC-Repeat: We concatenate each video with itself 1-5 times, which does not change the number of unique objects. However, this simple perturbation entirely collapses the mean relative accuracy of Cambrian-S from 42% to 0%. A system that performs spatial supersensing and integrates information across experiences should recognize views of the same scene and keep object-count predictions unchanged; instead, Cambrian-S inference algorithm relies largely on a shortcut in the VSC benchmark that rooms are never revisited. Taken together, our findings suggest that (i) current VSI-Super benchmarks do not yet reliably measure spatial supersensing, and (ii) predictive-sensing inference recipes used by Cambrian-S improve performance by inadvertently exploiting shortcuts rather than from robust spatial supersensing. We include the response from the Cambrian-S authors (in Appendix A) to provide a balanced perspective alongside our claims. We release our code at: https://github.com/bethgelab/supersanity",
            "headline_zh": "分析Cambrian-S基准与推理方法，揭示其未可靠测量空间超感知",
            "intro_zh": [
                "核心问题：当前VSI-Super基准可能无法有效评估空间超感知能力",
                "方法要点：提出NoSense基线，仅用词袋模型解决VSR；设计VSC-Repeat扰动测试",
                "实验或效果：NoSense在VSR达95%准确率；VSC-Repeat使Cambrian-S准确率降至0%"
            ],
            "tags_zh": [
                "空间超感知",
                "视频基准评估",
                "推理方法分析",
                "捷径启发式",
                "长视频理解"
            ],
            "_index": 11
        },
        {
            "title": "Teacher-Guided One-Shot Pruning via Context-Aware Knowledge Distillation",
            "authors": [
                "Md. Samiul Alim",
                "Sharjil Khan",
                "Amrijit Biswas",
                "Fuad Rahman",
                "Shafin Rahman",
                "Nabeel Mohammed"
            ],
            "arxiv_id": "2511.16653v1",
            "summary": "Unstructured pruning remains a powerful strategy for compressing deep neural networks, yet it often demands iterative train-prune-retrain cycles, resulting in significant computational overhead. To address this challenge, we introduce a novel teacher-guided pruning framework that tightly integrates Knowledge Distillation (KD) with importance score estimation. Unlike prior approaches that apply KD as a post-pruning recovery step, our method leverages gradient signals informed by the teacher during importance score calculation to identify and retain parameters most critical for both task performance and knowledge transfer. Our method facilitates a one-shot global pruning strategy that efficiently eliminates redundant weights while preserving essential representations. After pruning, we employ sparsity-aware retraining with and without KD to recover accuracy without reactivating pruned connections. Comprehensive experiments across multiple image classification benchmarks, including CIFAR-10, CIFAR-100, and TinyImageNet, demonstrate that our method consistently achieves high sparsity levels with minimal performance degradation. Notably, our approach outperforms state-of-the-art baselines such as EPG and EPSD at high sparsity levels, while offering a more computationally efficient alternative to iterative pruning schemes like COLT. The proposed framework offers a computation-efficient, performance-preserving solution well suited for deployment in resource-constrained environments.",
            "headline_zh": "提出教师引导的一次性剪枝框架，通过上下文感知知识蒸馏解决剪枝计算开销问题",
            "intro_zh": [
                "非结构化剪枝需迭代训练-剪枝-重训练，计算开销大",
                "方法在重要性评分中集成教师梯度，一次性剪枝保留关键参数",
                "实验在CIFAR等数据集实现高稀疏度，性能损失小，优于EPG等基线"
            ],
            "tags_zh": [
                "神经网络剪枝",
                "知识蒸馏",
                "一次性剪枝",
                "上下文感知",
                "稀疏训练"
            ],
            "_index": 12
        },
        {
            "title": "InternData-A1: Pioneering High-Fidelity Synthetic Data for Pre-training Generalist Policy",
            "authors": [
                "Yang Tian",
                "Yuyin Yang",
                "Yiman Xie",
                "Zetao Cai",
                "Xu Shi",
                "Ning Gao",
                "Hangxu Liu",
                "Xuekun Jiang",
                "Zherui Qiu",
                "Feng Yuan",
                "Yaping Li",
                "Ping Wang",
                "Junhao Cai",
                "Jia Zeng",
                "Hao Dong",
                "Jiangmiao Pang"
            ],
            "arxiv_id": "2511.16651v1",
            "summary": "Recent works explore how real and synthetic data contribute to Vision-Language-Action (VLA) models' generalization. While current VLA models have shown the strong effectiveness of large-scale real-robot pre-training, synthetic data has not previously demonstrated comparable capability at scale. This paper provides the first evidence that synthetic data alone can match the performance of the strongest $π$-dataset in pre-training a VLA model, revealing the substantial value of large-scale simulation. The resulting model also exhibits surprisingly zero-shot sim-to-real transfer on several challenging tasks. Our synthetic dataset, InternData-A1, contains over 630k trajectories and 7,433 hours across 4 embodiments, 18 skills, 70 tasks, and 227 scenes, covering rigid, articulated, deformable, and fluid-object manipulation. It is generated through a highly autonomous, fully decoupled, and compositional simulation pipeline that enables long-horizon skill composition, flexible task assembly, and heterogeneous embodiments with minimal manual tuning. Using the same architecture as $π_0$, we pre-train a model entirely on InternData-A1 and find that it matches the official $π_0$ across 49 simulation tasks, 5 real-world tasks, and 4 long-horizon dexterous tasks. We release the dataset and will open-source the generation pipeline to broaden access to large-scale robotic data and to lower the barrier to scalable data creation for embodied AI research.",
            "headline_zh": "提出InternData-A1合成数据集，用于预训练通用策略模型，匹配真实数据性能。",
            "intro_zh": [
                "核心问题：合成数据在VLA模型预训练中未达大规模真实数据效果。",
                "方法要点：构建大规模合成数据集，支持多技能、任务和场景的自主生成。",
                "实验或效果：模型在仿真和真实任务中匹配π_0，实现零样本迁移。"
            ],
            "tags_zh": [
                "合成数据生成",
                "视觉语言动作模型",
                "机器人预训练",
                "零样本迁移",
                "仿真到真实",
                "长时程技能"
            ],
            "_index": 13
        },
        {
            "title": "Late-decoupled 3D Hierarchical Semantic Segmentation with Semantic Prototype Discrimination based Bi-branch Supervision",
            "authors": [
                "Shuyu Cao",
                "Chongshou Li",
                "Jie Xu",
                "Tianrui Li",
                "Na Zhao"
            ],
            "arxiv_id": "2511.16650v1",
            "summary": "3D hierarchical semantic segmentation (3DHS) is crucial for embodied intelligence applications that demand a multi-grained and multi-hierarchy understanding of 3D scenes. Despite the progress, previous 3DHS methods have overlooked following two challenges: I) multi-label learning with a parameter-sharing model can lead to multi-hierarchy conflicts in cross-hierarchy optimization, and II) the class imbalance issue is inevitable across multiple hierarchies of 3D scenes, which makes the model performance become dominated by major classes. To address these issues, we propose a novel framework with a primary 3DHS branch and an auxiliary discrimination branch. Specifically, to alleviate the multi-hierarchy conflicts, we propose a late-decoupled 3DHS framework which employs multiple decoders with the coarse-to-fine hierarchical guidance and consistency. The late-decoupled architecture can mitigate the underfitting and overfitting conflicts among multiple hierarchies and can also constrain the class imbalance problem in each individual hierarchy. Moreover, we introduce a 3DHS-oriented semantic prototype based bi-branch supervision mechanism, which additionally learns class-wise discriminative point cloud features and performs mutual supervision between the auxiliary and 3DHS branches, to enhance the class-imbalance segmentation. Extensive experiments on multiple datasets and backbones demonstrate that our approach achieves state-of-the-art 3DHS performance, and its core components can also be used as a plug-and-play enhancement to improve previous methods.",
            "headline_zh": "提出晚解耦3D分层语义分割框架，解决多层级冲突和类别不平衡问题",
            "intro_zh": [
                "核心问题：多层级冲突和类别不平衡影响3D分层语义分割性能",
                "方法要点：采用晚解耦架构和语义原型双分支监督机制",
                "实验或效果：在多个数据集和骨干网络上实现最先进性能"
            ],
            "tags_zh": [
                "3D分层语义分割",
                "晚解耦架构",
                "语义原型",
                "双分支监督",
                "类别不平衡",
                "多层级优化"
            ],
            "_index": 14
        },
        {
            "title": "TRIM: Scalable 3D Gaussian Diffusion Inference with Temporal and Spatial Trimming",
            "authors": [
                "Zeyuan Yin",
                "Xiaoming Liu"
            ],
            "arxiv_id": "2511.16642v1",
            "summary": "Recent advances in 3D Gaussian diffusion models suffer from time-intensive denoising and post-denoising processing due to the massive number of Gaussian primitives, resulting in slow generation and limited scalability along sampling trajectories. To improve the efficiency of 3D diffusion models, we propose $\\textbf{TRIM}$ ($\\textbf{T}$rajectory $\\textbf{R}$eduction and $\\textbf{I}$nstance $\\textbf{M}$ask denoising), a post-training approach that incorporates both temporal and spatial trimming strategies, to accelerate inference without compromising output quality while supporting the inference-time scaling for Gaussian diffusion models. Instead of scaling denoising trajectories in a costly end-to-end manner, we develop a lightweight selector model to evaluate latent Gaussian primitives derived from multiple sampled noises, enabling early trajectory reduction by selecting candidates with high-quality potential. Furthermore, we introduce instance mask denoising to prune learnable Gaussian primitives by filtering out redundant background regions, reducing inference computation at each denoising step. Extensive experiments and analysis demonstrate that TRIM significantly improves both the efficiency and quality of 3D generation. Source code is available at $\\href{https://github.com/zeyuanyin/TRIM}{link}$.",
            "headline_zh": "提出TRIM方法以加速3D高斯扩散模型推理，通过时空修剪提升效率与质量",
            "intro_zh": [
                "核心问题：3D高斯扩散模型因高斯基元数量庞大，导致去噪和后处理耗时，推理缓慢且可扩展性差",
                "方法要点：结合轨迹缩减和实例掩码去噪，使用轻量选择器评估潜在基元，并修剪冗余背景区域",
                "实验或效果：广泛实验表明TRIM显著提升3D生成的效率与质量，支持推理时缩放"
            ],
            "tags_zh": [
                "3D高斯扩散模型",
                "推理加速",
                "轨迹缩减",
                "实例掩码去噪",
                "后训练优化",
                "可扩展生成"
            ],
            "_index": 15
        },
        {
            "title": "SurvAgent: Hierarchical CoT-Enhanced Case Banking and Dichotomy-Based Multi-Agent System for Multimodal Survival Prediction",
            "authors": [
                "Guolin Huang",
                "Wenting Chen",
                "Jiaqi Yang",
                "Xinheng Lyu",
                "Xiaoling Luo",
                "Sen Yang",
                "Xiaohan Xing",
                "Linlin Shen"
            ],
            "arxiv_id": "2511.16635v1",
            "summary": "Survival analysis is critical for cancer prognosis and treatment planning, yet existing methods lack the transparency essential for clinical adoption. While recent pathology agents have demonstrated explainability in diagnostic tasks, they face three limitations for survival prediction: inability to integrate multimodal data, ineffective region-of-interest exploration, and failure to leverage experiential learning from historical cases. We introduce SurvAgent, the first hierarchical chain-of-thought (CoT)-enhanced multi-agent system for multimodal survival prediction. SurvAgent consists of two stages: (1) WSI-Gene CoT-Enhanced Case Bank Construction employs hierarchical analysis through Low-Magnification Screening, Cross-Modal Similarity-Aware Patch Mining, and Confidence-Aware Patch Mining for pathology images, while Gene-Stratified analysis processes six functional gene categories. Both generate structured reports with CoT reasoning, storing complete analytical processes for experiential learning. (2) Dichotomy-Based Multi-Expert Agent Inference retrieves similar cases via RAG and integrates multimodal reports with expert predictions through progressive interval refinement. Extensive experiments on five TCGA cohorts demonstrate SurvAgent's superority over conventional methods, proprietary MLLMs, and medical agents, establishing a new paradigm for explainable AI-driven survival prediction in precision oncology.",
            "headline_zh": "提出SurvAgent分层多智能体系统以解决癌症生存预测中可解释性不足问题",
            "intro_zh": [
                "现有生存分析方法缺乏透明度，难以整合多模态数据和利用历史案例经验",
                "采用分层思维链增强案例库构建和多专家代理推理，提升可解释性",
                "在五个TCGA队列实验中优于传统方法、专有大语言模型和医疗代理"
            ],
            "tags_zh": [
                "生存预测",
                "多智能体系统",
                "思维链增强",
                "多模态数据整合",
                "可解释人工智能",
                "精准肿瘤学"
            ],
            "_index": 16
        },
        {
            "title": "SAM 3D: 3Dfy Anything in Images",
            "authors": [
                "SAM 3D Team",
                "Xingyu Chen",
                "Fu-Jen Chu",
                "Pierre Gleize",
                "Kevin J Liang",
                "Alexander Sax",
                "Hao Tang",
                "Weiyao Wang",
                "Michelle Guo",
                "Thibaut Hardin",
                "Xiang Li",
                "Aohan Lin",
                "Jiawei Liu",
                "Ziqi Ma",
                "Anushka Sagar",
                "Bowen Song",
                "Xiaodong Wang",
                "Jianing Yang",
                "Bowen Zhang",
                "Piotr Dollár",
                "Georgia Gkioxari",
                "Matt Feiszli",
                "Jitendra Malik"
            ],
            "arxiv_id": "2511.16624v1",
            "summary": "We present SAM 3D, a generative model for visually grounded 3D object reconstruction, predicting geometry, texture, and layout from a single image. SAM 3D excels in natural images, where occlusion and scene clutter are common and visual recognition cues from context play a larger role. We achieve this with a human- and model-in-the-loop pipeline for annotating object shape, texture, and pose, providing visually grounded 3D reconstruction data at unprecedented scale. We learn from this data in a modern, multi-stage training framework that combines synthetic pretraining with real-world alignment, breaking the 3D \"data barrier\". We obtain significant gains over recent work, with at least a 5:1 win rate in human preference tests on real-world objects and scenes. We will release our code and model weights, an online demo, and a new challenging benchmark for in-the-wild 3D object reconstruction.",
            "headline_zh": "提出SAM 3D模型，从单张图像生成3D对象重建，适用于自然场景。",
            "intro_zh": [
                "核心问题：从单张图像重建3D对象，处理遮挡和场景杂乱。",
                "方法要点：结合人工和模型标注，大规模数据训练，多阶段框架。",
                "实验或效果：人类偏好测试中胜率至少5:1，优于近期工作。"
            ],
            "tags_zh": [
                "3D对象重建",
                "单图像生成",
                "视觉基础数据",
                "多阶段训练",
                "自然场景处理"
            ],
            "_index": 17
        },
        {
            "title": "SAM2S: Segment Anything in Surgical Videos via Semantic Long-term Tracking",
            "authors": [
                "Haofeng Liu",
                "Ziyue Wang",
                "Sudhanshu Mishra",
                "Mingqi Gao",
                "Guanyi Qin",
                "Chang Han Low",
                "Alex Y. W. Kong",
                "Yueming Jin"
            ],
            "arxiv_id": "2511.16618v1",
            "summary": "Surgical video segmentation is crucial for computer-assisted surgery, enabling precise localization and tracking of instruments and tissues. Interactive Video Object Segmentation (iVOS) models such as Segment Anything Model 2 (SAM2) provide prompt-based flexibility beyond methods with predefined categories, but face challenges in surgical scenarios due to the domain gap and limited long-term tracking. To address these limitations, we construct SA-SV, the largest surgical iVOS benchmark with instance-level spatio-temporal annotations (masklets) spanning eight procedure types (61k frames, 1.6k masklets), enabling comprehensive development and evaluation for long-term tracking and zero-shot generalization. Building on SA-SV, we propose SAM2S, a foundation model enhancing \\textbf{SAM2} for \\textbf{S}urgical iVOS through: (1) DiveMem, a trainable diverse memory mechanism for robust long-term tracking; (2) temporal semantic learning for instrument understanding; and (3) ambiguity-resilient learning to mitigate annotation inconsistencies across multi-source datasets. Extensive experiments demonstrate that fine-tuning on SA-SV enables substantial performance gains, with SAM2 improving by 12.99 average $\\mathcal{J}$\\&$\\mathcal{F}$ over vanilla SAM2. SAM2S further advances performance to 80.42 average $\\mathcal{J}$\\&$\\mathcal{F}$, surpassing vanilla and fine-tuned SAM2 by 17.10 and 4.11 points respectively, while maintaining 68 FPS real-time inference and strong zero-shot generalization. Code and dataset will be released at https://jinlab-imvr.github.io/SAM2S.",
            "headline_zh": "提出SAM2S模型以解决手术视频中长时跟踪与语义分割的挑战",
            "intro_zh": [
                "核心问题：手术视频分割存在领域差异和长时跟踪不足，影响交互式分割模型性能。",
                "方法要点：引入DiveMem机制、时间语义学习和抗模糊学习，增强SAM2的鲁棒性。",
                "实验或效果：在SA-SV基准上，SAM2S达到80.42平均J&F，优于基线并保持实时推理。"
            ],
            "tags_zh": [
                "手术视频分割",
                "长时跟踪",
                "交互式分割",
                "语义学习",
                "基准数据集",
                "实时推理"
            ],
            "_index": 18
        },
        {
            "title": "TimeViper: A Hybrid Mamba-Transformer Vision-Language Model for Efficient Long Video Understanding",
            "authors": [
                "Boshen Xu",
                "Zihan Xiao",
                "Jiaze Li",
                "Jianzhong Ju",
                "Zhenbo Luo",
                "Jian Luan",
                "Qin Jin"
            ],
            "arxiv_id": "2511.16595v1",
            "summary": "We introduce TimeViper, a hybrid vision-language model designed to tackle challenges of long video understanding. Processing long videos demands both an efficient model architecture and an effective mechanism for handling extended temporal contexts. To this end, TimeViper adopts a hybrid Mamba-Transformer backbone that combines the efficiency of state-space models with the expressivity of attention mechanisms. Through this hybrid design, we reveal the vision-to-text information aggregation phenomenon, where information progressively flows from vision tokens to text tokens across increasing LLM depth, resulting in severe vision token redundancy. Motivated by this observation, we propose TransV, a token information transfer module that transfers and compresses vision tokens into instruction tokens while maintaining multimodal understanding capabilities. This design enables TimeViper to process hour-long videos exceeding 10,000 frames. Extensive experiments across multiple benchmarks demonstrate that TimeViper competes with state-of-the-art models while extending frame numbers. We further analyze attention behaviors of both Mamba and Transformer layers, offering new insights into hybrid model interpretability. This work represents an initial step towards developing, interpreting, and compressing hybrid Mamba-Transformer architectures.",
            "headline_zh": "提出TimeViper混合模型以高效处理长视频理解问题",
            "intro_zh": [
                "核心问题：长视频处理需高效架构和扩展时间上下文机制",
                "方法要点：结合Mamba状态空间模型与Transformer注意力，设计TransV模块压缩视觉令牌",
                "实验或效果：在多个基准测试中与先进模型竞争，可处理超万帧小时级视频"
            ],
            "tags_zh": [
                "长视频理解",
                "混合模型",
                "令牌压缩",
                "状态空间模型",
                "视觉语言模型",
                "模型解释性"
            ],
            "_index": 19
        },
        {
            "title": "Green Resilience of Cyber-Physical Systems: Doctoral Dissertation",
            "authors": [
                "Diaeddin Rimawi"
            ],
            "arxiv_id": "2511.16593v1",
            "summary": "Cyber-physical systems (CPS) combine computational and physical components. Online Collaborative AI System (OL-CAIS) is a type of CPS that learn online in collaboration with humans to achieve a common goal, which makes it vulnerable to disruptive events that degrade performance. Decision-makers must therefore restore performance while limiting energy impact, creating a trade-off between resilience and greenness. This research addresses how to balance these two properties in OL-CAIS. It aims to model resilience for automatic state detection, develop agent-based policies that optimize the greenness-resilience trade-off, and understand catastrophic forgetting to maintain performance consistency. We model OL-CAIS behavior through three operational states: steady, disruptive, and final. To support recovery during disruptions, we introduce the GResilience framework, which provides recovery strategies through multi-objective optimization (one-agent), game-theoretic decision-making (two-agent), and reinforcement learning (RL-agent). We also design a measurement framework to quantify resilience and greenness. Empirical evaluation uses real and simulated experiments with a collaborative robot learning object classification from human demonstrations. Results show that the resilience model captures performance transitions during disruptions, and that GResilience policies improve green recovery by shortening recovery time, stabilizing performance, and reducing human dependency. RL-agent policies achieve the strongest results, although with a marginal increase in CO2 emissions. We also observe catastrophic forgetting after repeated disruptions, while our policies help maintain steadiness. A comparison with containerized execution shows that containerization cuts CO2 emissions by half. Overall, this research provides models, metrics, and policies that ensure the green recovery of OL-CAIS.",
            "headline_zh": "提出GResilience框架以优化在线协作AI系统的绿色性与韧性平衡",
            "intro_zh": [
                "核心问题：在线协作AI系统在干扰事件中需平衡韧性恢复与能源影响。",
                "方法要点：通过多目标优化、博弈论和强化学习开发恢复策略。",
                "实验效果：GResilience策略缩短恢复时间、稳定性能并减少碳排放。"
            ],
            "tags_zh": [
                "网络物理系统",
                "在线协作AI",
                "绿色韧性",
                "多目标优化",
                "强化学习",
                "灾难性遗忘"
            ],
            "_index": 20
        },
        {
            "title": "Erase to Retain: Low Rank Adaptation Guided Selective Unlearning in Medical Segmentation Networks",
            "authors": [
                "Nirjhor Datta",
                "Md. Golam Rabiul Alam"
            ],
            "arxiv_id": "2511.16574v1",
            "summary": "The ability to selectively remove knowledge from medical segmentation networks is increasingly important for privacy compliance, ethical deployment, and continual dataset revision. We introduce Erase to Retain, a controllable unlearning framework for medical image segmentation that achieves targeted forgetting without full retraining. Our method uses a teacher-student distillation paradigm with Low-Rank Adaptation (LoRA) constrained subspace updates, enabling the student network to erase lesion-specific or class-specific representations in low-rank decoder spaces while preserving global anatomical understanding. During the strong unlearning phase, LoRA modules are adversarially optimized to contradict the teacher's confident predictions on a designated forget subset, enforcing semantic removal. This is followed by a gentle restoration phase that recovers generalization on retained data through head-only supervised refinement.\n  For ISIC segmentation, the student reduces forget-set IoU from 0.875 to 0.509 while maintaining competitive performance on the retain and validation splits (0.647 to 0.677 IoU). On the cross-domain CHASE dataset, Erase to Retain consistently lowers forget-set IoU while preserving utility on retain and validation sets. For ISIC classification, our method decreases accuracy on the forget subset from 87.0 percent to 64.1 percent while improving retain accuracy from 83.9 percent to 90.6 percent.\n  These results demonstrate that LoRA-based subspace unlearning provides a practical pathway toward responsible, controllable, and reversible unlearning in medical image analysis, enabling models to forget sensitive samples or structures while preserving performance where it matters most.",
            "headline_zh": "提出Erase to Retain框架，用于医学分割网络的选择性遗忘以实现隐私合规。",
            "intro_zh": [
                "核心问题：医学分割网络需选择性移除知识以符合隐私和伦理要求。",
                "方法要点：使用教师-学生蒸馏与LoRA约束子空间更新，实现可控遗忘。",
                "实验效果：在ISIC和CHASE数据集上，有效降低遗忘集性能，同时保持保留集性能。"
            ],
            "tags_zh": [
                "医学图像分割",
                "选择性遗忘",
                "低秩适应",
                "蒸馏训练",
                "隐私合规",
                "子空间更新"
            ],
            "_index": 21
        },
        {
            "title": "POMA-3D: The Point Map Way to 3D Scene Understanding",
            "authors": [
                "Ye Mao",
                "Weixun Luo",
                "Ranran Huang",
                "Junpeng Jing",
                "Krystian Mikolajczyk"
            ],
            "arxiv_id": "2511.16567v1",
            "summary": "In this paper, we introduce POMA-3D, the first self-supervised 3D representation model learned from point maps. Point maps encode explicit 3D coordinates on a structured 2D grid, preserving global 3D geometry while remaining compatible with the input format of 2D foundation models. To transfer rich 2D priors into POMA-3D, a view-to-scene alignment strategy is designed. Moreover, as point maps are view-dependent with respect to a canonical space, we introduce POMA-JEPA, a joint embedding-predictive architecture that enforces geometrically consistent point map features across multiple views. Additionally, we introduce ScenePoint, a point map dataset constructed from 6.5K room-level RGB-D scenes and 1M 2D image scenes to facilitate large-scale POMA-3D pretraining. Experiments show that POMA-3D serves as a strong backbone for both specialist and generalist 3D understanding. It benefits diverse tasks, including 3D question answering, embodied navigation, scene retrieval, and embodied localization, all achieved using only geometric inputs (i.e., 3D coordinates). Overall, our POMA-3D explores a point map way to 3D scene understanding, addressing the scarcity of pretrained priors and limited data in 3D representation learning. Project Page: https://matchlab-imperial.github.io/poma3d/",
            "headline_zh": "提出POMA-3D，通过点图实现自监督3D场景理解，解决3D表示学习中先验和数据稀缺问题。",
            "intro_zh": [
                "核心问题：3D表示学习缺乏预训练先验和有限数据，难以直接应用2D基础模型。",
                "方法要点：使用点图编码3D坐标，结合视图对齐和POMA-JEPA架构实现多视图几何一致性。",
                "实验效果：在3D问答、导航等任务中表现优异，仅使用几何输入作为强骨干网络。"
            ],
            "tags_zh": [
                "点图表示",
                "自监督学习",
                "3D场景理解",
                "多视图对齐",
                "几何一致性",
                "基础模型迁移"
            ],
            "_index": 22
        },
        {
            "title": "NutriScreener: Retrieval-Augmented Multi-Pose Graph Attention Network for Malnourishment Screening",
            "authors": [
                "Misaal Khan",
                "Mayank Vatsa",
                "Kuldeep Singh",
                "Richa Singh"
            ],
            "arxiv_id": "2511.16566v1",
            "summary": "Child malnutrition remains a global crisis, yet existing screening methods are laborious and poorly scalable, hindering early intervention. In this work, we present NutriScreener, a retrieval-augmented, multi-pose graph attention network that combines CLIP-based visual embeddings, class-boosted knowledge retrieval, and context awareness to enable robust malnutrition detection and anthropometric prediction from children's images, simultaneously addressing generalizability and class imbalance. In a clinical study, doctors rated it 4.3/5 for accuracy and 4.6/5 for efficiency, confirming its deployment readiness in low-resource settings. Trained and tested on 2,141 children from AnthroVision and additionally evaluated on diverse cross-continent populations, including ARAN and an in-house collected CampusPose dataset, it achieves 0.79 recall, 0.82 AUC, and significantly lower anthropometric RMSEs, demonstrating reliable measurement in unconstrained pediatric settings. Cross-dataset results show up to 25% recall gain and up to 3.5 cm RMSE reduction using demographically matched knowledge bases. NutriScreener offers a scalable and accurate solution for early malnutrition detection in low-resource environments.",
            "headline_zh": "提出NutriScreener以解决儿童营养不良筛查的可扩展性问题",
            "intro_zh": [
                "核心问题：儿童营养不良筛查方法劳动密集且难以扩展，阻碍早期干预。",
                "方法要点：结合CLIP视觉嵌入、知识检索和图注意力网络，实现多姿态检测。",
                "实验或效果：在临床研究中获高评分，跨数据集召回提升25%，RMSE降低3.5厘米。"
            ],
            "tags_zh": [
                "营养不良筛查",
                "图注意力网络",
                "检索增强",
                "多姿态检测",
                "CLIP嵌入",
                "低资源环境"
            ],
            "_index": 23
        },
        {
            "title": "Lite Any Stereo: Efficient Zero-Shot Stereo Matching",
            "authors": [
                "Junpeng Jing",
                "Weixun Luo",
                "Ye Mao",
                "Krystian Mikolajczyk"
            ],
            "arxiv_id": "2511.16555v1",
            "summary": "Recent advances in stereo matching have focused on accuracy, often at the cost of significantly increased model size. Traditionally, the community has regarded efficient models as incapable of zero-shot ability due to their limited capacity. In this paper, we introduce Lite Any Stereo, a stereo depth estimation framework that achieves strong zero-shot generalization while remaining highly efficient. To this end, we design a compact yet expressive backbone to ensure scalability, along with a carefully crafted hybrid cost aggregation module. We further propose a three-stage training strategy on million-scale data to effectively bridge the sim-to-real gap. Together, these components demonstrate that an ultra-light model can deliver strong generalization, ranking 1st across four widely used real-world benchmarks. Remarkably, our model attains accuracy comparable to or exceeding state-of-the-art non-prior-based accurate methods while requiring less than 1% computational cost, setting a new standard for efficient stereo matching.",
            "headline_zh": "提出Lite Any Stereo框架，实现高效零样本立体匹配",
            "intro_zh": [
                "核心问题：高效模型因容量有限，传统上被认为无法实现零样本泛化。",
                "方法要点：设计紧凑主干和混合成本聚合模块，结合三阶段训练策略。",
                "实验或效果：在四个真实基准测试中排名第一，计算成本低于1%。"
            ],
            "tags_zh": [
                "立体匹配",
                "零样本泛化",
                "高效模型",
                "成本聚合",
                "三阶段训练"
            ],
            "_index": 24
        },
        {
            "title": "Progressive Supernet Training for Efficient Visual Autoregressive Modeling",
            "authors": [
                "Xiaoyue Chen",
                "Yuling Shi",
                "Kaiyuan Li",
                "Huandong Wang",
                "Yong Li",
                "Xiaodong Gu",
                "Xinlei Chen",
                "Mingbao Lin"
            ],
            "arxiv_id": "2511.16546v1",
            "summary": "Visual Auto-Regressive (VAR) models significantly reduce inference steps through the \"next-scale\" prediction paradigm. However, progressive multi-scale generation incurs substantial memory overhead due to cumulative KV caching, limiting practical deployment.\n  We observe a scale-depth asymmetric dependency in VAR: early scales exhibit extreme sensitivity to network depth, while later scales remain robust to depth reduction. Inspired by this, we propose VARiant: by equidistant sampling, we select multiple subnets ranging from 16 to 2 layers from the original 30-layer VAR-d30 network. Early scales are processed by the full network, while later scales utilize subnet. Subnet and the full network share weights, enabling flexible depth adjustment within a single model.\n  However, weight sharing between subnet and the entire network can lead to optimization conflicts. To address this, we propose a progressive training strategy that breaks through the Pareto frontier of generation quality for both subnets and the full network under fixed-ratio training, achieving joint optimality.\n  Experiments on ImageNet demonstrate that, compared to the pretrained VAR-d30 (FID 1.95), VARiant-d16 and VARiant-d8 achieve nearly equivalent quality (FID 2.05/2.12) while reducing memory consumption by 40-65%. VARiant-d2 achieves 3.5 times speedup and 80% memory reduction at moderate quality cost (FID 2.97). In terms of deployment, VARiant's single-model architecture supports zero-cost runtime depth switching and provides flexible deployment options from high quality to extreme efficiency, catering to diverse application scenarios.",
            "headline_zh": "提出VARiant渐进超网训练以解决视觉自回归模型内存开销问题",
            "intro_zh": [
                "视觉自回归模型多尺度生成因KV缓存累积导致高内存开销，限制部署",
                "利用尺度-深度非对称依赖，通过权重共享子网实现灵活深度调整",
                "实验显示VARiant在ImageNet上显著降低内存和加速，保持生成质量"
            ],
            "tags_zh": [
                "视觉自回归建模",
                "渐进训练",
                "权重共享",
                "内存优化",
                "多尺度生成",
                "模型压缩"
            ],
            "_index": 25
        },
        {
            "title": "EOGS++: Earth Observation Gaussian Splatting with Internal Camera Refinement and Direct Panchromatic Rendering",
            "authors": [
                "Pierrick Bournez",
                "Luca Savant Aira",
                "Thibaud Ehret",
                "Gabriele Facciolo"
            ],
            "arxiv_id": "2511.16542v1",
            "summary": "Recently, 3D Gaussian Splatting has been introduced as a compelling alternative to NeRF for Earth observation, offering com- petitive reconstruction quality with significantly reduced training times. In this work, we extend the Earth Observation Gaussian Splatting (EOGS) framework to propose EOGS++, a novel method tailored for satellite imagery that directly operates on raw high-resolution panchromatic data without requiring external preprocessing. Furthermore, leveraging optical flow techniques we embed bundle adjustment directly within the training process, avoiding reliance on external optimization tools while improving camera pose estimation. We also introduce several improvements to the original implementation, including early stopping and TSDF post-processing, all contributing to sharper reconstructions and better geometric accuracy. Experiments on the IARPA 2016 and DFC2019 datasets demonstrate that EOGS++ achieves state-of-the-art performance in terms of reconstruction quality and effi- ciency, outperforming the original EOGS method and other NeRF-based methods while maintaining the computational advantages of Gaussian Splatting. Our model demonstrates an improvement from 1.33 to 1.19 mean MAE errors on buildings compared to the original EOGS models",
            "headline_zh": "提出EOGS++方法以改进卫星图像三维重建，通过内部相机优化和直接全色渲染提升精度与效率。",
            "intro_zh": [
                "核心问题：卫星图像三维重建依赖外部预处理和相机姿态估计，影响精度与效率。",
                "方法要点：引入内部相机优化和直接全色渲染，避免外部工具，改进几何准确性。",
                "实验效果：在IARPA 2016和DFC2019数据集上，MAE误差从1.33降至1.19，优于现有方法。"
            ],
            "tags_zh": [
                "地球观测三维重建",
                "高斯泼溅",
                "相机姿态优化",
                "全色渲染",
                "卫星图像处理",
                "几何精度提升"
            ],
            "_index": 26
        },
        {
            "title": "Supervised Contrastive Learning for Few-Shot AI-Generated Image Detection and Attribution",
            "authors": [
                "Jaime Álvarez Urueña",
                "David Camacho",
                "Javier Huertas Tato"
            ],
            "arxiv_id": "2511.16541v1",
            "summary": "The rapid advancement of generative artificial intelligence has enabled the creation of synthetic images that are increasingly indistinguishable from authentic content, posing significant challenges for digital media integrity. This problem is compounded by the accelerated release cycle of novel generative models, which renders traditional detection approaches (reliant on periodic retraining) computationally infeasible and operationally impractical.\n  This work proposes a novel two-stage detection framework designed to address the generalization challenge inherent in synthetic image detection. The first stage employs a vision deep learning model trained via supervised contrastive learning to extract discriminative embeddings from input imagery. Critically, this model was trained on a strategically partitioned subset of available generators, with specific architectures withheld from training to rigorously ablate cross-generator generalization capabilities. The second stage utilizes a k-nearest neighbors (k-NN) classifier operating on the learned embedding space, trained in a few-shot learning paradigm incorporating limited samples from previously unseen test generators.\n  With merely 150 images per class in the few-shot learning regime, which are easily obtainable from current generation models, the proposed framework achieves an average detection accuracy of 91.3\\%, representing a 5.2 percentage point improvement over existing approaches . For the source attribution task, the proposed approach obtains improvements of of 14.70\\% and 4.27\\% in AUC and OSCR respectively on an open set classification context, marking a significant advancement toward robust, scalable forensic attribution systems capable of adapting to the evolving generative AI landscape without requiring exhaustive retraining protocols.",
            "headline_zh": "提出监督对比学习与k-NN框架以解决少样本AI生成图像检测与溯源问题",
            "intro_zh": [
                "核心问题：生成AI模型快速迭代，传统检测方法重训练成本高且不实用",
                "方法要点：两阶段框架，先监督对比学习提取嵌入，再k-NN分类器进行少样本学习",
                "实验或效果：仅需每类150图像，检测准确率达91.3%，溯源指标显著提升"
            ],
            "tags_zh": [
                "监督对比学习",
                "少样本学习",
                "AI生成图像检测",
                "图像溯源",
                "k-NN分类器",
                "泛化能力"
            ],
            "_index": 27
        },
        {
            "title": "Investigating Optical Flow Computation: From Local Methods to a Multiresolution Horn-Schunck Implementation with Bilinear Interpolation",
            "authors": [
                "Haytham Ziani"
            ],
            "arxiv_id": "2511.16535v1",
            "summary": "This paper presents an applied analysis of local and global methods, with a focus on the Horn-Schunck algorithm for optical flow computation. We explore the theoretical and practical aspects of local approaches, such as the Lucas-Kanade method, and global techniques such as Horn-Schunck. Additionally, we implement a multiresolution version of the Horn-Schunck algorithm, using bilinear interpolation and prolongation to improve accuracy and convergence. The study investigates the effectiveness of these combined strategies in estimating motion between frames, particularly under varying image conditions.",
            "headline_zh": "实现多分辨率Horn-Schunck算法，结合双线性插值以提升光流估计精度",
            "intro_zh": [
                "核心问题：光流计算在图像条件变化下的运动估计准确性不足",
                "方法要点：比较局部与全局方法，实现多分辨率Horn-Schunck算法",
                "实验或效果：使用双线性插值和延长操作改进收敛与精度"
            ],
            "tags_zh": [
                "光流计算",
                "Horn-Schunck算法",
                "多分辨率方法",
                "双线性插值",
                "运动估计"
            ],
            "_index": 28
        },
        {
            "title": "Enhancing Multi-Camera Gymnast Tracking Through Domain Knowledge Integration",
            "authors": [
                "Fan Yang",
                "Shigeyuki Odashima",
                "Shoichi Masui",
                "Ikuo Kusajima",
                "Sosuke Yamao",
                "Shan Jiang"
            ],
            "arxiv_id": "2511.16532v1",
            "summary": "We present a robust multi-camera gymnast tracking, which has been applied at international gymnastics championships for gymnastics judging. Despite considerable progress in multi-camera tracking algorithms, tracking gymnasts presents unique challenges: (i) due to space restrictions, only a limited number of cameras can be installed in the gymnastics stadium; and (ii) due to variations in lighting, background, uniforms, and occlusions, multi-camera gymnast detection may fail in certain views and only provide valid detections from two opposing views. These factors complicate the accurate determination of a gymnast's 3D trajectory using conventional multi-camera triangulation. To alleviate this issue, we incorporate gymnastics domain knowledge into our tracking solution. Given that a gymnast's 3D center typically lies within a predefined vertical plane during \\revised{much of their} performance, we can apply a ray-plane intersection to generate coplanar 3D trajectory candidates for opposing-view detections. More specifically, we propose a novel cascaded data association (DA) paradigm that employs triangulation to generate 3D trajectory candidates when cross-view detections are sufficient, and resort to the ray-plane intersection when they are insufficient. Consequently, coplanar candidates are used to compensate for uncertain trajectories, thereby minimizing tracking failures. The robustness of our method is validated through extensive experimentation, demonstrating its superiority over existing methods in challenging scenarios. Furthermore, our gymnastics judging system, equipped with this tracking method, has been successfully applied to recent Gymnastics World Championships, earning significant recognition from the International Gymnastics Federation.",
            "headline_zh": "提出结合领域知识的多相机体操运动员跟踪方法，以应对检测不足问题。",
            "intro_zh": [
                "核心问题：体操馆相机数量有限，检测易受光照、遮挡影响，导致多视角检测不足。",
                "方法要点：引入体操领域知识，使用射线-平面相交生成共面3D轨迹候选，补偿不确定轨迹。",
                "实验或效果：在体操世界锦标赛中成功应用，验证方法在挑战性场景下优于现有方法。"
            ],
            "tags_zh": [
                "多相机跟踪",
                "体操运动员跟踪",
                "领域知识集成",
                "数据关联",
                "3D轨迹估计",
                "射线平面相交"
            ],
            "_index": 29
        },
        {
            "title": "Contrastive vision-language learning with paraphrasing and negation",
            "authors": [
                "Kwun Ho Ngan",
                "Saman Sadeghi Afgeh",
                "Joe Townsend",
                "Artur d'Avila Garcez"
            ],
            "arxiv_id": "2511.16527v1",
            "summary": "Contrastive vision-language models continue to be the dominant approach for image and text retrieval. Contrastive Language-Image Pre-training (CLIP) trains two neural networks in contrastive manner to align their image and text embeddings in a shared latent space. Recent results evaluating CLIP on negated or paraphrased text have shown mixed performance because negation changes meaning radically with minimal lexical changes, while paraphrasing can create very different textual expressions with the same intended meaning. This poses a significant challenge for improving the evaluation results and alignment of vision-language models. To address this challenge, this paper evaluates the combination of paraphrasing and negation, proposes a new CLIP contrastive loss function accounting for both paraphrasing and negation, and applies LLM-generated training triples consisting of original, paraphrased and negated textual captions to CLIP-like training models. The approach, called SemCLIP, is shown to move paraphrased captions towards the original image embeddings while pushing negated captions further away in embedding space. Empirically, SemCLIP is shown to be capable of preserving CLIP's performance while increasing considerably the distances to negated captions. On the CC-Neg benchmark using an original over negation image-retrieval accuracy metric, SemCLIP improves accuracy from 68.1% to 78.1%. Although results are mixed when compared with CLIP on the Sugarcrepe++ benchmark, SemCLIP's performance is generally better than the models trained with negated captions. This robustness to negation extends to downstream zero-shot classification tasks where SemCLIP pre-trained on Sugarcrepe++ performs better than CLIP on all tested downstream tasks. These results indicate that SemCLIP can achieve significant robustness to semantic transformations.",
            "headline_zh": "提出SemCLIP以增强视觉语言模型对语义变换的鲁棒性",
            "intro_zh": [
                "核心问题：CLIP模型在否定和转述文本上表现不稳定，影响图像检索准确性。",
                "方法要点：引入新对比损失函数，结合LLM生成原始、转述和否定文本三元组进行训练。",
                "实验或效果：在CC-Neg基准上，图像检索准确率从68.1%提升至78.1%。"
            ],
            "tags_zh": [
                "对比学习",
                "视觉语言模型",
                "语义鲁棒性",
                "否定处理",
                "转述处理",
                "零样本分类"
            ],
            "_index": 30
        },
        {
            "title": "BoxingVI: A Multi-Modal Benchmark for Boxing Action Recognition and Localization",
            "authors": [
                "Rahul Kumar",
                "Vipul Baghel",
                "Sudhanshu Singh",
                "Bikash Kumar Badatya",
                "Shivam Yadav",
                "Babji Srinivasan",
                "Ravi Hegde"
            ],
            "arxiv_id": "2511.16524v1",
            "summary": "Accurate analysis of combat sports using computer vision has gained traction in recent years, yet the development of robust datasets remains a major bottleneck due to the dynamic, unstructured nature of actions and variations in recording environments. In this work, we present a comprehensive, well-annotated video dataset tailored for punch detection and classification in boxing. The dataset comprises 6,915 high-quality punch clips categorized into six distinct punch types, extracted from 20 publicly available YouTube sparring sessions and involving 18 different athletes. Each clip is manually segmented and labeled to ensure precise temporal boundaries and class consistency, capturing a wide range of motion styles, camera angles, and athlete physiques. This dataset is specifically curated to support research in real-time vision-based action recognition, especially in low-resource and unconstrained environments. By providing a rich benchmark with diverse punch examples, this contribution aims to accelerate progress in movement analysis, automated coaching, and performance assessment within boxing and related domains.",
            "headline_zh": "提出BoxingVI数据集以解决拳击动作识别在动态无约束环境中的数据集瓶颈问题",
            "intro_zh": [
                "核心问题：动态无约束拳击动作识别缺乏鲁棒数据集，阻碍计算机视觉应用发展",
                "方法要点：构建高质量拳击视频数据集，含6,915个手动标注拳击片段，覆盖六种拳型",
                "实验或效果：数据集支持实时动作识别研究，促进拳击运动分析和自动化教练系统开发"
            ],
            "tags_zh": [
                "拳击动作识别",
                "多模态基准",
                "视频数据集",
                "动作定位",
                "计算机视觉",
                "实时识别"
            ],
            "_index": 31
        },
        {
            "title": "YOWO: You Only Walk Once to Jointly Map An Indoor Scene and Register Ceiling-mounted Cameras",
            "authors": [
                "Fan Yang",
                "Sosuke Yamao",
                "Ikuo Kusajima",
                "Atsunori Moteki",
                "Shoichi Masui",
                "Shan Jiang"
            ],
            "arxiv_id": "2511.16521v1",
            "summary": "Using ceiling-mounted cameras (CMCs) for indoor visual capturing opens up a wide range of applications. However, registering CMCs to the target scene layout presents a challenging task. While manual registration with specialized tools is inefficient and costly, automatic registration with visual localization may yield poor results when visual ambiguity exists. To alleviate these issues, we propose a novel solution for jointly mapping an indoor scene and registering CMCs to the scene layout. Our approach involves equipping a mobile agent with a head-mounted RGB-D camera to traverse the entire scene once and synchronize CMCs to capture this mobile agent. The egocentric videos generate world-coordinate agent trajectories and the scene layout, while the videos of CMCs provide pseudo-scale agent trajectories and CMC relative poses. By correlating all the trajectories with their corresponding timestamps, the CMC relative poses can be aligned to the world-coordinate scene layout. Based on this initialization, a factor graph is customized to enable the joint optimization of ego-camera poses, scene layout, and CMC poses. We also develop a new dataset, setting the first benchmark for collaborative scene mapping and CMC registration (https://sites.google.com/view/yowo/home). Experimental results indicate that our method not only effectively accomplishes two tasks within a unified framework, but also jointly enhances their performance. We thus provide a reliable tool to facilitate downstream position-aware applications.",
            "headline_zh": "提出YOWO方法以联合映射室内场景并注册天花板摄像头",
            "intro_zh": [
                "核心问题：天花板摄像头注册到室内场景布局困难，手动或自动方法效率低或精度差。",
                "方法要点：使用移动代理头戴RGB-D相机遍历场景，同步摄像头视频，通过轨迹关联和因子图优化实现联合映射与注册。",
                "实验或效果：在新建数据集上验证，方法统一框架下有效完成两任务并提升性能。"
            ],
            "tags_zh": [
                "室内场景映射",
                "摄像头注册",
                "因子图优化",
                "RGB-D视觉",
                "轨迹关联",
                "联合优化"
            ],
            "_index": 32
        },
        {
            "title": "MiMo-Embodied: X-Embodied Foundation Model Technical Report",
            "authors": [
                "Xiaoshuai Hao",
                "Lei Zhou",
                "Zhijian Huang",
                "Zhiwen Hou",
                "Yingbo Tang",
                "Lingfeng Zhang",
                "Guang Li",
                "Zheng Lu",
                "Shuhuai Ren",
                "Xianhui Meng",
                "Yuchen Zhang",
                "Jing Wu",
                "Jinghui Lu",
                "Chenxu Dang",
                "Jiayi Guan",
                "Jianhua Wu",
                "Zhiyi Hou",
                "Hanbing Li",
                "Shumeng Xia",
                "Mingliang Zhou",
                "Yinan Zheng",
                "Zihao Yue",
                "Shuhao Gu",
                "Hao Tian",
                "Yuannan Shen",
                "Jianwei Cui",
                "Wen Zhang",
                "Shaoqing Xu",
                "Bing Wang",
                "Haiyang Sun",
                "Zeyu Zhu",
                "Yuncheng Jiang",
                "Zibin Guo",
                "Chuhong Gong",
                "Chaofan Zhang",
                "Wenbo Ding",
                "Kun Ma",
                "Guang Chen",
                "Rui Cai",
                "Diyun Xiang",
                "Heng Qu",
                "Fuli Luo",
                "Hangjun Ye",
                "Long Chen"
            ],
            "arxiv_id": "2511.16518v1",
            "summary": "We open-source MiMo-Embodied, the first cross-embodied foundation model to successfully integrate and achieve state-of-the-art performance in both Autonomous Driving and Embodied AI. MiMo-Embodied sets new records across 17 embodied AI benchmarks in Task Planning, Affordance Prediction and Spatial Understanding, while also excelling in 12 autonomous driving benchmarks across Environmental Perception, Status Prediction, and Driving Planning. Across these tasks, MiMo-Embodied significantly outperforms existing open-source, closed-source, and specialized baselines. Our results indicate that through multi-stage learning, curated data construction, and CoT/RL fine-tuning, these two domains exhibit strong positive transfer and mutually reinforce one another. We provide a detailed analysis of our model design and training methodologies to facilitate further research. Code and models are available at https://github.com/XiaomiMiMo/MiMo-Embodied.",
            "headline_zh": "提出MiMo-Embodied跨具身基础模型，在自动驾驶与具身AI中实现最优性能",
            "intro_zh": [
                "核心问题：未知，但模型整合自动驾驶与具身AI，解决多任务性能挑战",
                "方法要点：采用多阶段学习、数据构建和CoT/RL微调，促进领域间正向迁移",
                "实验或效果：在29个基准测试中创纪录，显著超越开源、闭源及专用基线"
            ],
            "tags_zh": [
                "跨具身基础模型",
                "自动驾驶",
                "具身AI",
                "多任务学习",
                "基准测试",
                "正向迁移"
            ],
            "_index": 33
        },
        {
            "title": "Acquisition Time-Informed Breast Tumor Segmentation from Dynamic Contrast-Enhanced MRI",
            "authors": [
                "Rui Wang",
                "Yuexi Du",
                "John Lewin",
                "R. Todd Constable",
                "Nicha C. Dvornek"
            ],
            "arxiv_id": "2511.16498v1",
            "summary": "Dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) plays an important role in breast cancer screening, tumor assessment, and treatment planning and monitoring. The dynamic changes in contrast in different tissues help to highlight the tumor in post-contrast images. However, varying acquisition protocols and individual factors result in large variation in the appearance of tissues, even for images acquired in the same phase (e.g., first post-contrast phase), making automated tumor segmentation challenging. Here, we propose a tumor segmentation method that leverages knowledge of the image acquisition time to modulate model features according to the specific acquisition sequence. We incorporate the acquisition times using feature-wise linear modulation (FiLM) layers, a lightweight method for incorporating temporal information that also allows for capitalizing on the full, variables number of images acquired per imaging study. We trained baseline and different configurations for the time-modulated models with varying backbone architectures on a large public multisite breast DCE-MRI dataset. Evaluation on in-domain images and a public out-of-domain dataset showed that incorporating knowledge of phase acquisition time improved tumor segmentation performance and model generalization.",
            "headline_zh": "提出基于采集时间调制的乳腺肿瘤分割方法以提升DCE-MRI分割性能与泛化能力",
            "intro_zh": [
                "核心问题：DCE-MRI中采集协议和个体差异导致组织外观变化大，自动肿瘤分割困难",
                "方法要点：利用采集时间通过FiLM层调制模型特征，适应不同序列",
                "实验或效果：在域内和域外数据集上验证，时间信息提升分割性能和泛化性"
            ],
            "tags_zh": [
                "乳腺肿瘤分割",
                "动态对比增强MRI",
                "特征调制",
                "采集时间建模",
                "模型泛化"
            ],
            "_index": 34
        },
        {
            "title": "Physics-Informed Machine Learning for Efficient Sim-to-Real Data Augmentation in Micro-Object Pose Estimation",
            "authors": [
                "Zongcai Tan",
                "Lan Wei",
                "Dandan Zhang"
            ],
            "arxiv_id": "2511.16494v1",
            "summary": "Precise pose estimation of optical microrobots is essential for enabling high-precision object tracking and autonomous biological studies. However, current methods rely heavily on large, high-quality microscope image datasets, which are difficult and costly to acquire due to the complexity of microrobot fabrication and the labour-intensive labelling. Digital twin systems offer a promising path for sim-to-real data augmentation, yet existing techniques struggle to replicate complex optical microscopy phenomena, such as diffraction artifacts and depth-dependent imaging.This work proposes a novel physics-informed deep generative learning framework that, for the first time, integrates wave optics-based physical rendering and depth alignment into a generative adversarial network (GAN), to synthesise high-fidelity microscope images for microrobot pose estimation efficiently. Our method improves the structural similarity index (SSIM) by 35.6% compared to purely AI-driven methods, while maintaining real-time rendering speeds (0.022 s/frame).The pose estimator (CNN backbone) trained on our synthetic data achieves 93.9%/91.9% (pitch/roll) accuracy, just 5.0%/5.4% (pitch/roll) below that of an estimator trained exclusively on real data. Furthermore, our framework generalises to unseen poses, enabling data augmentation and robust pose estimation for novel microrobot configurations without additional training data.",
            "headline_zh": "提出物理信息生成对抗网络以高效增强微物体姿态估计的模拟到真实数据",
            "intro_zh": [
                "微物体姿态估计依赖高质量显微镜图像，但数据获取困难且成本高",
                "集成波光学物理渲染和深度对齐到GAN，合成高保真图像",
                "合成数据训练的姿态估计器精度接近真实数据，泛化到未知姿态"
            ],
            "tags_zh": [
                "物理信息机器学习",
                "模拟到真实数据增强",
                "微物体姿态估计",
                "生成对抗网络",
                "波光学渲染"
            ],
            "_index": 35
        },
        {
            "title": "Flow and Depth Assisted Video Prediction with Latent Transformer",
            "authors": [
                "Eliyas Suleyman",
                "Paul Henderson",
                "Eksan Firkat",
                "Nicolas Pugeault"
            ],
            "arxiv_id": "2511.16484v1",
            "summary": "Video prediction is a fundamental task for various downstream applications, including robotics and world modeling. Although general video prediction models have achieved remarkable performance in standard scenarios, occlusion is still an inherent challenge in video prediction. We hypothesize that providing explicit information about motion (via point-flow) and geometric structure (via depth-maps) will enable video prediction models to perform better in situations with occlusion and the background motion. To investigate this, we present the first systematic study dedicated to occluded video prediction. We use a standard multi-object latent transformer architecture to predict future frames, but modify this to incorporate information from depth and point-flow. We evaluate this model in a controlled setting on both synthetic and real-world datasets with not only appearance-based metrics but also Wasserstein distances on object masks, which can effectively measure the motion distribution of the prediction. We find that when the prediction model is assisted with point flow and depth, it performs better in occluded scenarios and predicts more accurate background motion compared to models without the help of these modalities.",
            "headline_zh": "提出结合点流和深度辅助的潜在变换器模型，以提升遮挡场景下的视频预测性能。",
            "intro_zh": [
                "核心问题：视频预测中遮挡和背景运动导致性能下降，现有模型难以处理。",
                "方法要点：在潜在变换器架构中整合点流和深度图，提供运动和几何结构信息。",
                "实验或效果：在合成和真实数据集上评估，辅助模型在遮挡场景和背景运动预测更准确。"
            ],
            "tags_zh": [
                "视频预测",
                "遮挡处理",
                "点流辅助",
                "深度图",
                "潜在变换器",
                "运动分布评估"
            ],
            "_index": 36
        },
        {
            "title": "FastSurfer-CC: A robust, accurate, and comprehensive framework for corpus callosum morphometry",
            "authors": [
                "Clemens Pollak",
                "Kersten Diers",
                "Santiago Estrada",
                "David Kügler",
                "Martin Reuter"
            ],
            "arxiv_id": "2511.16471v1",
            "summary": "The corpus callosum, the largest commissural structure in the human brain, is a central focus in research on aging and neurological diseases. It is also a critical target for interventions such as deep brain stimulation and serves as an important biomarker in clinical trials, including those investigating remyelination therapies. Despite extensive research on corpus callosum segmentation, few publicly available tools provide a comprehensive and automated analysis pipeline. To address this gap, we present FastSurfer-CC, an efficient and fully automated framework for corpus callosum morphometry. FastSurfer-CC automatically identifies mid-sagittal slices, segments the corpus callosum and fornix, localizes the anterior and posterior commissures to standardize head positioning, generates thickness profiles and subdivisions, and extracts eight shape metrics for statistical analysis. We demonstrate that FastSurfer-CC outperforms existing specialized tools across the individual tasks. Moreover, our method reveals statistically significant differences between Huntington's disease patients and healthy controls that are not detected by the current state-of-the-art.",
            "headline_zh": "提出FastSurfer-CC框架以自动化胼胝体形态测量，用于脑老化和神经疾病研究。",
            "intro_zh": [
                "核心问题：缺乏公开、全面的自动化胼胝体分割与分析工具。",
                "方法要点：自动识别中矢状面、分割胼胝体和穹窿，并提取形状指标。",
                "实验或效果：在亨廷顿病中检测到现有方法未发现的显著差异。"
            ],
            "tags_zh": [
                "胼胝体分割",
                "脑形态测量",
                "自动化框架",
                "神经影像分析",
                "亨廷顿病研究"
            ],
            "_index": 37
        },
        {
            "title": "Arctic-Extract Technical Report",
            "authors": [
                "Mateusz Chiliński",
                "Julita Ołtusek",
                "Wojciech Jaśkowski"
            ],
            "arxiv_id": "2511.16470v1",
            "summary": "Arctic-Extract is a state-of-the-art model designed for extracting structural data (question answering, entities and tables) from scanned or digital-born business documents. Despite its SoTA capabilities, the model is deployable on resource-constrained hardware, weighting only 6.6 GiB, making it suitable for deployment on devices with limited resources, such as A10 GPUs with 24 GB of memory. Arctic-Extract can process up to 125 A4 pages on those GPUs, making suitable for long document processing. This paper highlights Arctic-Extract's training protocols and evaluation results, demonstrating its strong performance in document understanding.",
            "headline_zh": "提出Arctic-Extract模型，用于从业务文档中提取结构化数据，并部署于资源受限硬件。",
            "intro_zh": [
                "核心问题：从扫描或数字业务文档中提取问答、实体和表格等结构化数据。",
                "方法要点：采用先进训练协议，模型仅6.6 GiB，可在A10 GPU等资源受限设备部署。",
                "实验或效果：评估显示强文档理解性能，单GPU可处理多达125页A4文档。"
            ],
            "tags_zh": [
                "文档理解",
                "结构化数据提取",
                "资源受限部署",
                "业务文档处理",
                "长文档处理"
            ],
            "_index": 38
        },
        {
            "title": "LLaVA$^3$: Representing 3D Scenes like a Cubist Painter to Boost 3D Scene Understanding of VLMs",
            "authors": [
                "Doriand Petit",
                "Steve Bourgeois",
                "Vincent Gay-Bellile",
                "Florian Chabot",
                "Loïc Barthe"
            ],
            "arxiv_id": "2511.16454v1",
            "summary": "Developing a multi-modal language model capable of understanding 3D scenes remains challenging due to the limited availability of 3D training data, in contrast to the abundance of 2D datasets used for vision-language models (VLM). As an alternative, we introduce LLaVA$^3$ (pronounced LLaVA-Cube), a novel method that improves the 3D scene understanding capabilities of VLM using only multi-view 2D images and without any fine-tuning. Inspired by Cubist painters, who represented multiple viewpoints of a 3D object within a single picture, we propose to describe the 3D scene for the VLM through omnidirectional visual representations of each object. These representations are derived from an intermediate multi-view 3D reconstruction of the scene. Extensive experiments on 3D VQA and 3D language grounding show that our approach outperforms previous 2D-based VLM solutions.",
            "headline_zh": "提出LLaVA³方法，通过多视角2D图像提升VLM的3D场景理解能力",
            "intro_zh": [
                "核心问题：3D场景理解因训练数据稀缺而受限，而2D数据丰富。",
                "方法要点：受立体主义启发，用多视角图像构建全向视觉表示描述3D场景。",
                "实验或效果：在3D VQA和语言接地任务中优于现有2D方法。"
            ],
            "tags_zh": [
                "3D场景理解",
                "视觉语言模型",
                "多视角图像",
                "立体主义表示",
                "3D视觉问答"
            ],
            "_index": 39
        },
        {
            "title": "VLA-Pruner: Temporal-Aware Dual-Level Visual Token Pruning for Efficient Vision-Language-Action Inference",
            "authors": [
                "Ziyan Liu",
                "Yeqiu Chen",
                "Hongyi Cai",
                "Tao Lin",
                "Shuo Yang",
                "Zheng Liu",
                "Bo Zhao"
            ],
            "arxiv_id": "2511.16449v1",
            "summary": "Vision-Language-Action (VLA) models have shown great promise for embodied AI, yet the heavy computational cost of processing continuous visual streams severely limits their real-time deployment. Token pruning (keeping salient visual tokens and dropping redundant ones) has emerged as an effective approach for accelerating Vision-Language Models (VLMs), offering a solution for efficient VLA. However, these VLM-specific token pruning methods select tokens based solely on semantic salience metrics (e.g., prefill attention), while overlooking the VLA's intrinsic dual-system nature of high-level semantic understanding and low-level action execution. Consequently, these methods bias token retention toward semantic cues, discard critical information for action generation, and significantly degrade VLA performance. To bridge this gap, we propose VLA-Pruner, a versatile plug-and-play VLA-specific token prune method that aligns with the dual-system nature of VLA models and exploits the temporal continuity in robot manipulation. Specifically, VLA-Pruner adopts a dual-level importance criterion for visual token retention: vision-language prefill attention for semantic-level relevance and action decode attention, estimated via temporal smoothing, for action-level importance. Based on this criterion, VLA-Pruner proposes a novel dual-level token selection strategy that adaptively preserves a compact, informative set of visual tokens for both semantic understanding and action execution under given compute budget. Experiments show that VLA-Pruner achieves state-of-the-art performance across multiple VLA architectures and diverse robotic tasks.",
            "headline_zh": "提出VLA-Pruner以解决视觉-语言-动作模型推理效率低的问题",
            "intro_zh": [
                "核心问题：现有视觉-语言模型剪枝方法忽略VLA模型的双系统特性，导致动作生成信息丢失。",
                "方法要点：采用双级重要性标准，结合语义级和动作级注意力，自适应保留关键视觉令牌。",
                "实验或效果：在多种VLA架构和机器人任务中实现最优性能，提升推理效率。"
            ],
            "tags_zh": [
                "视觉-语言-动作模型",
                "令牌剪枝",
                "双级重要性",
                "机器人操作",
                "推理加速"
            ],
            "_index": 40
        },
        {
            "title": "StreetView-Waste: A Multi-Task Dataset for Urban Waste Management",
            "authors": [
                "Diogo J. Paulo",
                "João Martins",
                "Hugo Proença",
                "João C. Neves"
            ],
            "arxiv_id": "2511.16440v1",
            "summary": "Urban waste management remains a critical challenge for the development of smart cities. Despite the growing number of litter detection datasets, the problem of monitoring overflowing waste containers, particularly from images captured by garbage trucks, has received little attention. While existing datasets are valuable, they often lack annotations for specific container tracking or are captured in static, decontextualized environments, limiting their utility for real-world logistics. To address this gap, we present StreetView-Waste, a comprehensive dataset of urban scenes featuring litter and waste containers. The dataset supports three key evaluation tasks: (1) waste container detection, (2) waste container tracking, and (3) waste overflow segmentation. Alongside the dataset, we provide baselines for each task by benchmarking state-of-the-art models in object detection, tracking, and segmentation. Additionally, we enhance baseline performance by proposing two complementary strategies: a heuristic-based method for improved waste container tracking and a model-agnostic framework that leverages geometric priors to refine litter segmentation. Our experimental results show that while fine-tuned object detectors achieve reasonable performance in detecting waste containers, baseline tracking methods struggle to accurately estimate their number; however, our proposed heuristics reduce the mean absolute counting error by 79.6%. Similarly, while segmenting amorphous litter is challenging, our geometry-aware strategy improves segmentation mAP@0.5 by 27% on lightweight models, demonstrating the value of multimodal inputs for this task. Ultimately, StreetView-Waste provides a challenging benchmark to encourage research into real-world perception systems for urban waste management.",
            "headline_zh": "提出StreetView-Waste数据集以解决城市垃圾管理中的多任务视觉感知问题",
            "intro_zh": [
                "核心问题：现有垃圾检测数据集缺乏对溢出容器跟踪和动态场景的标注，限制实际应用。",
                "方法要点：提供多任务数据集，并引入启发式跟踪和几何先验框架提升性能。",
                "实验或效果：启发式方法减少计数误差79.6%，几何策略提升分割mAP@0.5 27%。"
            ],
            "tags_zh": [
                "垃圾容器检测",
                "目标跟踪",
                "语义分割",
                "城市视觉数据集",
                "几何先验",
                "启发式方法"
            ],
            "_index": 41
        },
        {
            "title": "Beyond Visual Cues: Leveraging General Semantics as Support for Few-Shot Segmentation",
            "authors": [
                "Jin Wang",
                "Bingfeng Zhang",
                "Jian Pang",
                "Mengyu Liu",
                "Honglong Chen",
                "Weifeng Liu"
            ],
            "arxiv_id": "2511.16435v1",
            "summary": "Few-shot segmentation (FSS) aims to segment novel classes under the guidance of limited support samples by a meta-learning paradigm. Existing methods mainly mine references from support images as meta guidance. However, due to intra-class variations among visual representations, the meta information extracted from support images cannot produce accurate guidance to segment untrained classes. In this paper, we argue that the references from support images may not be essential, the key to the support role is to provide unbiased meta guidance for both trained and untrained classes. We then introduce a Language-Driven Attribute Generalization (LDAG) architecture to utilize inherent target property language descriptions to build robust support strategy. Specifically, to obtain an unbiased support representation, we design a Multi-attribute Enhancement (MaE) module, which produces multiple detailed attribute descriptions of the target class through Large Language Models (LLMs), and then builds refined visual-text prior guidance utilizing multi-modal matching. Meanwhile, due to text-vision modal shift, attribute text struggles to promote visual feature representation, we design a Multi-modal Attribute Alignment (MaA) to achieve cross-modal interaction between attribute texts and visual feature. Experiments show that our proposed method outperforms existing approaches by a clear margin and achieves the new state-of-the art performance. The code will be released.",
            "headline_zh": "提出语言驱动属性泛化架构以解决少样本分割中视觉支持偏差问题",
            "intro_zh": [
                "核心问题：少样本分割中视觉支持样本因类内变化导致元指导不准确",
                "方法要点：利用大语言模型生成多属性描述，通过多模态匹配构建鲁棒支持策略",
                "实验或效果：方法在实验中显著超越现有方法，达到新最优性能"
            ],
            "tags_zh": [
                "少样本分割",
                "语言驱动属性泛化",
                "多模态匹配",
                "大语言模型",
                "跨模态对齐"
            ],
            "_index": 42
        },
        {
            "title": "From Prompts to Printable Models: Support-Effective 3D Generation via Offset Direct Preference Optimization",
            "authors": [
                "Chenming Wu",
                "Xiaofan Li",
                "Chengkai Dai"
            ],
            "arxiv_id": "2511.16434v1",
            "summary": "The transition from digital 3D models to physical objects via 3D printing often requires support structures to prevent overhanging features from collapsing during the fabrication process. While current slicing technologies offer advanced support strategies, they focus on post-processing optimizations rather than addressing the underlying need for support-efficient design during the model generation phase. This paper introduces SEG (\\textit{\\underline{S}upport-\\underline{E}ffective \\underline{G}eneration}), a novel framework that integrates Direct Preference Optimization with an Offset (ODPO) into the 3D generation pipeline to directly optimize models for minimal support material usage. By incorporating support structure simulation into the training process, SEG encourages the generation of geometries that inherently require fewer supports, thus reducing material waste and production time. We demonstrate SEG's effectiveness through extensive experiments on two benchmark datasets, Thingi10k-Val and GPT-3DP-Val, showing that SEG significantly outperforms baseline models such as TRELLIS, DPO, and DRO in terms of support volume reduction and printability. Qualitative results further reveal that SEG maintains high fidelity to input prompts while minimizing the need for support structures. Our findings highlight the potential of SEG to transform 3D printing by directly optimizing models during the generative process, paving the way for more sustainable and efficient digital fabrication practices.",
            "headline_zh": "提出SEG框架以优化3D模型生成，减少3D打印支撑结构使用",
            "intro_zh": [
                "核心问题：3D打印中支撑结构导致材料浪费和生产延迟，现有技术仅事后优化",
                "方法要点：集成偏移直接偏好优化，在生成阶段模拟支撑结构以最小化需求",
                "实验或效果：在基准数据集上显著减少支撑体积，优于TRELLIS等基线模型"
            ],
            "tags_zh": [
                "3D模型生成",
                "直接偏好优化",
                "支撑结构优化",
                "3D打印效率",
                "材料节约"
            ],
            "_index": 43
        },
        {
            "title": "Graph Neural Networks for Surgical Scene Segmentation",
            "authors": [
                "Yihan Li",
                "Nikhil Churamani",
                "Maria Robu",
                "Imanol Luengo",
                "Danail Stoyanov"
            ],
            "arxiv_id": "2511.16430v1",
            "summary": "Purpose: Accurate identification of hepatocystic anatomy is critical to preventing surgical complications during laparoscopic cholecystectomy. Deep learning models often struggle with occlusions, long-range dependencies, and capturing the fine-scale geometry of rare structures. This work addresses these challenges by introducing graph-based segmentation approaches that enhance spatial and semantic understanding in surgical scene analyses.\n  Methods: We propose two segmentation models integrating Vision Transformer (ViT) feature encoders with Graph Neural Networks (GNNs) to explicitly model spatial relationships between anatomical regions. (1) A static k Nearest Neighbours (k-NN) graph with a Graph Convolutional Network with Initial Residual and Identity Mapping (GCNII) enables stable long-range information propagation. (2) A dynamic Differentiable Graph Generator (DGG) with a Graph Attention Network (GAT) supports adaptive topology learning. Both models are evaluated on the Endoscapes-Seg50 and CholecSeg8k benchmarks.\n  Results: The proposed approaches achieve up to 7-8% improvement in Mean Intersection over Union (mIoU) and 6% improvement in Mean Dice (mDice) scores over state-of-the-art baselines. It produces anatomically coherent predictions, particularly on thin, rare and safety-critical structures.\n  Conclusion: The proposed graph-based segmentation methods enhance both performance and anatomical consistency in surgical scene segmentation. By combining ViT-based global context with graph-based relational reasoning, the models improve interpretability and reliability, paving the way for safer laparoscopic and robot-assisted surgery through a precise identification of critical anatomical features.",
            "headline_zh": "提出图神经网络方法以提升手术场景分割的准确性和解剖一致性",
            "intro_zh": [
                "核心问题：深度学习模型在遮挡、长程依赖和精细几何结构分割中表现不佳",
                "方法要点：结合Vision Transformer编码器和图神经网络，建模解剖区域空间关系",
                "实验或效果：在基准测试中mIoU和mDice分别提升7-8%和6%，改善稀有结构分割"
            ],
            "tags_zh": [
                "图神经网络",
                "手术场景分割",
                "Vision Transformer",
                "长程依赖建模",
                "解剖一致性"
            ],
            "_index": 44
        },
        {
            "title": "CylinderDepth: Cylindrical Spatial Attention for Multi-View Consistent Self-Supervised Surround Depth Estimation",
            "authors": [
                "Samer Abualhanud",
                "Christian Grannemann",
                "Max Mehltretter"
            ],
            "arxiv_id": "2511.16428v1",
            "summary": "Self-supervised surround-view depth estimation enables dense, low-cost 3D perception with a 360° field of view from multiple minimally overlapping images. Yet, most existing methods suffer from depth estimates that are inconsistent between overlapping images. Addressing this limitation, we propose a novel geometry-guided method for calibrated, time-synchronized multi-camera rigs that predicts dense, metric, and cross-view-consistent depth. Given the intrinsic and relative orientation parameters, a first depth map is predicted per image and the so-derived 3D points from all images are projected onto a shared unit cylinder, establishing neighborhood relations across different images. This produces a 2D position map for every image, where each pixel is assigned its projected position on the cylinder. Based on these position maps, we apply an explicit, non-learned spatial attention that aggregates features among pixels across images according to their distances on the cylinder, to predict a final depth map per image. Evaluated on the DDAD and nuScenes datasets, our approach improves the consistency of depth estimates across images and the overall depth compared to state-of-the-art methods.",
            "headline_zh": "提出圆柱空间注意力方法以解决多视角深度估计不一致问题",
            "intro_zh": [
                "核心问题：多视角图像深度估计在重叠区域不一致，影响360度感知",
                "方法要点：将3D点投影到共享圆柱，应用非学习空间注意力聚合跨图像特征",
                "实验或效果：在DDAD和nuScenes数据集上提升深度一致性和整体精度"
            ],
            "tags_zh": [
                "自监督深度估计",
                "多视角一致性",
                "圆柱投影",
                "空间注意力",
                "360度感知"
            ],
            "_index": 45
        },
        {
            "title": "End-to-End Motion Capture from Rigid Body Markers with Geodesic Loss",
            "authors": [
                "Hai Lan",
                "Zongyan Li",
                "Jianmin Hu",
                "Jialing Yang",
                "Houde Dai"
            ],
            "arxiv_id": "2511.16418v1",
            "summary": "Marker-based optical motion capture (MoCap), while long regarded as the gold standard for accuracy, faces practical challenges, such as time-consuming preparation and marker identification ambiguity, due to its reliance on dense marker configurations, which fundamentally limit its scalability. To address this, we introduce a novel fundamental unit for MoCap, the Rigid Body Marker (RBM), which provides unambiguous 6-DoF data and drastically simplifies setup. Leveraging this new data modality, we develop a deep-learning-based regression model that directly estimates SMPL parameters under a geodesic loss. This end-to-end approach matches the performance of optimization-based methods while requiring over an order of magnitude less computation. Trained on synthesized data from the AMASS dataset, our end-to-end model achieves state-of-the-art accuracy in body pose estimation. Real-world data captured using a Vicon optical tracking system further demonstrates the practical viability of our approach. Overall, the results show that combining sparse 6-DoF RBM with a manifold-aware geodesic loss yields a practical and high-fidelity solution for real-time MoCap in graphics, virtual reality, and biomechanics.",
            "headline_zh": "提出基于刚性体标记和测地损失的方法，实现实时高精度运动捕捉",
            "intro_zh": [
                "核心问题：传统光学运动捕捉依赖密集标记，存在设置耗时和标记识别模糊问题",
                "方法要点：使用刚性体标记提供6自由度数据，结合测地损失端到端回归SMPL参数",
                "实验或效果：在合成和真实数据上达到先进精度，计算效率比优化方法高一个数量级"
            ],
            "tags_zh": [
                "运动捕捉",
                "刚性体标记",
                "测地损失",
                "SMPL参数估计",
                "端到端学习"
            ],
            "_index": 46
        },
        {
            "title": "LAOF: Robust Latent Action Learning with Optical Flow Constraints",
            "authors": [
                "Xizhou Bu",
                "Jiexi Lyu",
                "Fulei Sun",
                "Ruichen Yang",
                "Zhiqiang Ma",
                "Wei Li"
            ],
            "arxiv_id": "2511.16407v1",
            "summary": "Learning latent actions from large-scale videos is crucial for the pre-training of scalable embodied foundation models, yet existing methods often struggle with action-irrelevant distractors. Although incorporating action supervision can alleviate these distractions, its effectiveness is restricted by the scarcity of available action labels. Optical flow represents pixel-level motion between consecutive frames, naturally suppressing background elements and emphasizing moving objects. Motivated by this, we propose robust Latent Action learning with Optical Flow constraints, called LAOF, a pseudo-supervised framework that leverages the agent's optical flow as an action-driven signal to learn latent action representations robust to distractors. Experimental results show that the latent representations learned by LAOF outperform existing methods on downstream imitation learning and reinforcement learning tasks. This superior performance arises from optical flow constraints, which substantially stabilize training and improve the quality of latent representations under extremely label-scarce conditions, while remaining effective as the proportion of action labels increases to 10 percent. Importantly, even without action supervision, LAOF matches or surpasses action-supervised methods trained with 1 percent of action labels.",
            "headline_zh": "提出LAOF框架，利用光流约束学习鲁棒潜在动作表示以应对大规模视频中的干扰物。",
            "intro_zh": [
                "核心问题：现有方法难以处理视频中与动作无关的干扰物，且动作标签稀缺。",
                "方法要点：使用光流作为动作驱动信号，构建伪监督框架学习鲁棒潜在动作表示。",
                "实验或效果：在模仿学习和强化学习任务中表现优异，训练稳定且适应标签稀缺条件。"
            ],
            "tags_zh": [
                "潜在动作学习",
                "光流约束",
                "伪监督框架",
                "鲁棒表示学习",
                "视频预训练"
            ],
            "_index": 47
        },
        {
            "title": "Homogeneous Proportional-Integral-Derivative Controller in Mobile Robotic Manipulators",
            "authors": [
                "Luis Luna",
                "Isaac Chairez",
                "Andrey Polyakov"
            ],
            "arxiv_id": "2511.16406v1",
            "summary": "Mobile robotic manipulators (MRMs), which integrate mobility and manipulation capabilities, present significant control challenges due to their nonlinear dynamics, underactuation, and coupling between the base and manipulator subsystems. This paper proposes a novel homogeneous Proportional-Integral-Derivative (hPID) control strategy tailored for MRMs to achieve robust and coordinated motion control. Unlike classical PID controllers, the hPID controller leverages the mathematical framework of homogeneous control theory to systematically enhance the stability and convergence properties of the closed-loop system, even in the presence of dynamic uncertainties and external disturbances involved into a system in a homogeneous way. A homogeneous PID structure is designed, ensuring improved convergence of tracking errors through a graded homogeneity approach that generalizes traditional PID gains to nonlinear, state-dependent functions. Stability analysis is conducted using Lyapunov-based methods, demonstrating that the hPID controller guarantees global asymptotic stability and finite-time convergence under mild assumptions. Experimental results on a representative MRM model validate the effectiveness of the hPID controller in achieving high-precision trajectory tracking for both the mobile base and manipulator arm, outperforming conventional linear PID controllers in terms of response time, steady-state error, and robustness to model uncertainties. This research contributes a scalable and analytically grounded control framework for enhancing the autonomy and reliability of next-generation mobile manipulation systems in structured and unstructured environments.",
            "headline_zh": "提出同质PID控制器以解决移动机器人操纵器的非线性控制挑战",
            "intro_zh": [
                "移动机器人操纵器存在非线性动力学、欠驱动和子系统耦合等控制难题",
                "基于同质控制理论设计非线性PID，提升系统稳定性和收敛性",
                "实验验证控制器在轨迹跟踪中优于传统PID，响应更快、误差更小"
            ],
            "tags_zh": [
                "移动机器人操纵器",
                "同质控制",
                "PID控制器",
                "非线性控制",
                "轨迹跟踪",
                "稳定性分析"
            ],
            "_index": 48
        },
        {
            "title": "Robot Metacognition: Decision Making with Confidence for Tool Invention",
            "authors": [
                "Ajith Anil Meera",
                "Poppy Collis",
                "Polina Arbuzova",
                "Abián Torres",
                "Paul F Kinghorn",
                "Ricardo Sanz",
                "Pablo Lanillos"
            ],
            "arxiv_id": "2511.16390v1",
            "summary": "Robots today often miss a key ingredient of truly intelligent behavior: the ability to reflect on their own cognitive processes and decisions. In humans, this self-monitoring or metacognition is crucial for learning, decision making and problem solving. For instance, they can evaluate how confident they are in performing a task, thus regulating their own behavior and allocating proper resources. Taking inspiration from neuroscience, we propose a robot metacognition architecture centered on confidence (a second-order judgment on decisions) and we demonstrate it on the use case of autonomous tool invention. We propose the use of confidence as a metacognitive measure within the robot decision making scheme. Confidence-informed robots can evaluate the reliability of their decisions, improving their robustness during real-world physical deployment. This form of robotic metacognition emphasizes embodied action monitoring as a means to achieve better informed decisions. We also highlight potential applications and research directions for robot metacognition.",
            "headline_zh": "提出基于置信度的机器人元认知架构，用于自主工具发明决策。",
            "intro_zh": [
                "核心问题：机器人缺乏自我监控能力，影响智能决策与行为调节。",
                "方法要点：从神经科学汲取灵感，将置信度作为元认知度量融入决策方案。",
                "实验或效果：在自主工具发明用例中展示，提升决策可靠性和部署鲁棒性。"
            ],
            "tags_zh": [
                "机器人元认知",
                "置信度决策",
                "自主工具发明",
                "自我监控",
                "神经科学启发"
            ],
            "_index": 49
        },
        {
            "title": "CAMS: Towards Compositional Zero-Shot Learning via Gated Cross-Attention and Multi-Space Disentanglement",
            "authors": [
                "Pan Yang",
                "Cheng Deng",
                "Jing Yang",
                "Han Zhao",
                "Yun Liu",
                "Yuling Chen",
                "Xiaoli Ruan",
                "Yanping Chen"
            ],
            "arxiv_id": "2511.16378v1",
            "summary": "Compositional zero-shot learning (CZSL) aims to learn the concepts of attributes and objects in seen compositions and to recognize their unseen compositions. Most Contrastive Language-Image Pre-training (CLIP)-based CZSL methods focus on disentangling attributes and objects by leveraging the global semantic representation obtained from the image encoder. However, this representation has limited representational capacity and do not allow for complete disentanglement of the two. To this end, we propose CAMS, which aims to extract semantic features from visual features and perform semantic disentanglement in multidimensional spaces, thereby improving generalization over unseen attribute-object compositions. Specifically, CAMS designs a Gated Cross-Attention that captures fine-grained semantic features from the high-level image encoding blocks of CLIP through a set of latent units, while adaptively suppressing background and other irrelevant information. Subsequently, it conducts Multi-Space Disentanglement to achieve disentanglement of attribute and object semantics. Experiments on three popular benchmarks (MIT-States, UT-Zappos, and C-GQA) demonstrate that CAMS achieves state-of-the-art performance in both closed-world and open-world settings. The code is available at https://github.com/ybyangjing/CAMS.",
            "headline_zh": "提出CAMS方法，通过门控交叉注意力和多空间解耦改进组合零样本学习。",
            "intro_zh": [
                "核心问题：组合零样本学习中，全局语义表示能力有限，难以完全解耦属性和对象。",
                "方法要点：使用门控交叉注意力提取细粒度语义特征，并在多空间进行语义解耦。",
                "实验效果：在MIT-States等基准测试中，闭集和开集设置下均达到最优性能。"
            ],
            "tags_zh": [
                "组合零样本学习",
                "门控交叉注意力",
                "多空间解耦",
                "语义特征提取",
                "CLIP模型"
            ],
            "_index": 50
        },
        {
            "title": "Flow-Aided Flight Through Dynamic Clutters From Point To Motion",
            "authors": [
                "Bowen Xu",
                "Zexuan Yan",
                "Minghao Lu",
                "Xiyu Fan",
                "Yi Luo",
                "Youshen Lin",
                "Zhiqiang Chen",
                "Yeke Chen",
                "Qiyuan Qiao",
                "Peng Lu"
            ],
            "arxiv_id": "2511.16372v1",
            "summary": "Challenges in traversing dynamic clutters lie mainly in the efficient perception of the environmental dynamics and the generation of evasive behaviors considering obstacle movement. Previous solutions have made progress in explicitly modeling the dynamic obstacle motion for avoidance, but this key dependency of decision-making is time-consuming and unreliable in highly dynamic scenarios with occlusions. On the contrary, without introducing object detection, tracking, and prediction, we empower the reinforcement learning (RL) with single LiDAR sensing to realize an autonomous flight system directly from point to motion. For exteroception, a depth sensing distance map achieving fixed-shape, low-resolution, and detail-safe is encoded from raw point clouds, and an environment change sensing point flow is adopted as motion features extracted from multi-frame observations. These two are integrated into a lightweight and easy-to-learn representation of complex dynamic environments. For action generation, the behavior of avoiding dynamic threats in advance is implicitly driven by the proposed change-aware sensing representation, where the policy optimization is indicated by the relative motion modulated distance field. With the deployment-friendly sensing simulation and dynamics model-free acceleration control, the proposed system shows a superior success rate and adaptability to alternatives, and the policy derived from the simulator can drive a real-world quadrotor with safe maneuvers.",
            "headline_zh": "提出基于点流与强化学习的无人机动态障碍规避系统，实现从点云到动作的自主飞行。",
            "intro_zh": [
                "核心问题：动态环境中障碍物运动感知与规避行为生成效率低、不可靠。",
                "方法要点：使用LiDAR点云编码深度图与点流，结合强化学习隐式驱动规避策略。",
                "实验或效果：在仿真与真实四旋翼上展示高成功率与适应性，实现安全机动。"
            ],
            "tags_zh": [
                "动态障碍规避",
                "强化学习",
                "点云处理",
                "无人机控制",
                "环境感知"
            ],
            "_index": 51
        },
        {
            "title": "DetailSemNet: Elevating Signature Verification through Detail-Semantic Integration",
            "authors": [
                "Meng-Cheng Shih",
                "Tsai-Ling Huang",
                "Yu-Heng Shih",
                "Hong-Han Shuai",
                "Hsuan-Tung Liu",
                "Yi-Ren Yeh",
                "Ching-Chun Huang"
            ],
            "arxiv_id": "2511.16364v1",
            "summary": "Offline signature verification (OSV) is a frequently utilized technology in forensics. This paper proposes a new model, DetailSemNet, for OSV. Unlike previous methods that rely on holistic features for pair comparisons, our approach underscores the significance of fine-grained differences for robust OSV. We propose to match local structures between two signature images, significantly boosting verification accuracy. Furthermore, we observe that without specific architectural modifications, transformer-based backbones might naturally obscure local details, adversely impacting OSV performance. To address this, we introduce a Detail Semantics Integrator, leveraging feature disentanglement and re-entanglement. This integrator is specifically designed to enhance intricate details while simultaneously expanding discriminative semantics, thereby augmenting the efficacy of local structural matching. We evaluate our method against leading benchmarks in offline signature verification. Our model consistently outperforms recent methods, achieving state-of-the-art results with clear margins. The emphasis on local structure matching not only improves performance but also enhances the model's interpretability, supporting our findings. Additionally, our model demonstrates remarkable generalization capabilities in cross-dataset testing scenarios. The combination of generalizability and interpretability significantly bolsters the potential of DetailSemNet for real-world applications.",
            "headline_zh": "提出DetailSemNet以提升离线签名验证的准确性和泛化性",
            "intro_zh": [
                "离线签名验证中，传统方法依赖整体特征，忽略细粒度差异影响鲁棒性",
                "引入细节语义集成器，通过特征解缠与重缠增强局部结构匹配",
                "在基准测试中实现SOTA性能，并展示优异的跨数据集泛化能力"
            ],
            "tags_zh": [
                "离线签名验证",
                "局部结构匹配",
                "特征解缠",
                "细节语义集成",
                "跨数据集泛化"
            ],
            "_index": 52
        },
        {
            "title": "Multi-Order Matching Network for Alignment-Free Depth Super-Resolution",
            "authors": [
                "Zhengxue Wang",
                "Zhiqiang Yan",
                "Yuan Wu",
                "Guangwei Gao",
                "Xiang Li",
                "Jian Yang"
            ],
            "arxiv_id": "2511.16361v1",
            "summary": "Recent guided depth super-resolution methods are premised on the assumption of strictly spatial alignment between depth and RGB, achieving high-quality depth reconstruction. However, in real-world scenarios, the acquisition of strictly aligned RGB-D is hindered by inherent hardware limitations (e.g., physically separate RGB-D sensors) and unavoidable calibration drift induced by mechanical vibrations or temperature variations. Consequently, existing approaches often suffer inevitable performance degradation when applied to misaligned real-world scenes. In this paper, we propose the Multi-Order Matching Network (MOMNet), a novel alignment-free framework that adaptively retrieves and selects the most relevant information from misaligned RGB. Specifically, our method begins with a multi-order matching mechanism, which jointly performs zero-order, first-order, and second-order matching to comprehensively identify RGB information consistent with depth across multi-order feature spaces. To effectively integrate the retrieved RGB and depth, we further introduce a multi-order aggregation composed of multiple structure detectors. This strategy uses multi-order priors as prompts to facilitate the selective feature transfer from RGB to depth. Extensive experiments demonstrate that MOMNet achieves state-of-the-art performance and exhibits outstanding robustness.",
            "headline_zh": "提出多阶匹配网络以解决未对齐RGB-D场景的深度超分辨率问题",
            "intro_zh": [
                "核心问题：真实场景中RGB与深度图未严格对齐，导致现有方法性能下降",
                "方法要点：通过多阶匹配机制自适应检索和选择RGB信息，结合多阶聚合进行特征融合",
                "实验或效果：在广泛实验中实现先进性能，并表现出优异的鲁棒性"
            ],
            "tags_zh": [
                "深度超分辨率",
                "多阶匹配",
                "对齐自由",
                "RGB-D融合",
                "特征检索",
                "鲁棒性"
            ],
            "_index": 53
        },
        {
            "title": "CRISTAL: Real-time Camera Registration in Static LiDAR Scans using Neural Rendering",
            "authors": [
                "Joni Vanherck",
                "Steven Moonen",
                "Brent Zoomers",
                "Kobe Werner",
                "Jeroen Put",
                "Lode Jorissen",
                "Nick Michiels"
            ],
            "arxiv_id": "2511.16349v1",
            "summary": "Accurate camera localization is crucial for robotics and Extended Reality (XR), enabling reliable navigation and alignment of virtual and real content. Existing visual methods often suffer from drift, scale ambiguity, and depend on fiducials or loop closure. This work introduces a real-time method for localizing a camera within a pre-captured, highly accurate colored LiDAR point cloud. By rendering synthetic views from this cloud, 2D-3D correspondences are established between live frames and the point cloud. A neural rendering technique narrows the domain gap between synthetic and real images, reducing occlusion and background artifacts to improve feature matching. The result is drift-free camera tracking with correct metric scale in the global LiDAR coordinate system. Two real-time variants are presented: Online Render and Match, and Prebuild and Localize. We demonstrate improved results on the ScanNet++ dataset and outperform existing SLAM pipelines.",
            "headline_zh": "提出CRISTAL方法，在静态LiDAR扫描中实现实时相机注册，解决视觉定位漂移和尺度模糊问题。",
            "intro_zh": [
                "核心问题：视觉相机定位存在漂移、尺度模糊，依赖标记或闭环，影响机器人学和XR应用。",
                "方法要点：通过神经渲染合成视图，建立2D-3D对应，减少合成与真实图像差异，提升特征匹配。",
                "实验或效果：在ScanNet++数据集上优于现有SLAM，实现无漂移、正确尺度的实时跟踪。"
            ],
            "tags_zh": [
                "相机定位",
                "神经渲染",
                "LiDAR点云",
                "实时跟踪",
                "特征匹配"
            ],
            "_index": 54
        },
        {
            "title": "The Shawshank Redemption of Embodied AI: Understanding and Benchmarking Indirect Environmental Jailbreaks",
            "authors": [
                "Chunyang Li",
                "Zifeng Kang",
                "Junwei Zhang",
                "Zhuo Ma",
                "Anda Cheng",
                "Xinghua Li",
                "Jianfeng Ma"
            ],
            "arxiv_id": "2511.16347v1",
            "summary": "The adoption of Vision-Language Models (VLMs) in embodied AI agents, while being effective, brings safety concerns such as jailbreaking. Prior work have explored the possibility of directly jailbreaking the embodied agents through elaborated multi-modal prompts. However, no prior work has studied or even reported indirect jailbreaks in embodied AI, where a black-box attacker induces a jailbreak without issuing direct prompts to the embodied agent. In this paper, we propose, for the first time, indirect environmental jailbreak (IEJ), a novel attack to jailbreak embodied AI via indirect prompt injected into the environment, such as malicious instructions written on a wall. Our key insight is that embodied AI does not ''think twice'' about the instructions provided by the environment -- a blind trust that attackers can exploit to jailbreak the embodied agent. We further design and implement open-source prototypes of two fully-automated frameworks: SHAWSHANK, the first automatic attack generation framework for the proposed attack IEJ; and SHAWSHANK-FORGE, the first automatic benchmark generation framework for IEJ. Then, using SHAWSHANK-FORGE, we automatically construct SHAWSHANK-BENCH, the first benchmark for indirectly jailbreaking embodied agents. Together, our two frameworks and one benchmark answer the questions of what content can be used for malicious IEJ instructions, where they should be placed, and how IEJ can be systematically evaluated. Evaluation results show that SHAWSHANK outperforms eleven existing methods across 3,957 task-scene combinations and compromises all six tested VLMs. Furthermore, current defenses only partially mitigate our attack, and we have responsibly disclosed our findings to all affected VLM vendors.",
            "headline_zh": "提出间接环境越狱攻击与自动框架，以评估具身AI安全风险。",
            "intro_zh": [
                "核心问题：具身AI对环境的盲目信任易被利用，导致间接越狱攻击。",
                "方法要点：设计SHAWSHANK自动攻击生成和SHAWSHANK-FORGE基准生成框架。",
                "实验或效果：在3957个任务场景中优于11种方法，攻破所有测试VLM。"
            ],
            "tags_zh": [
                "具身AI安全",
                "间接越狱攻击",
                "视觉语言模型",
                "自动基准生成",
                "多模态提示注入"
            ],
            "_index": 55
        },
        {
            "title": "Aerial View River Landform Video segmentation: A Weakly Supervised Context-aware Temporal Consistency Distillation Approach",
            "authors": [
                "Chi-Han Chen",
                "Chieh-Ming Chen",
                "Wen-Huang Cheng",
                "Ching-Chun Huang"
            ],
            "arxiv_id": "2511.16343v1",
            "summary": "The study of terrain and landform classification through UAV remote sensing diverges significantly from ground vehicle patrol tasks. Besides grappling with the complexity of data annotation and ensuring temporal consistency, it also confronts the scarcity of relevant data and the limitations imposed by the effective range of many technologies. This research substantiates that, in aerial positioning tasks, both the mean Intersection over Union (mIoU) and temporal consistency (TC) metrics are of paramount importance. It is demonstrated that fully labeled data is not the optimal choice, as selecting only key data lacks the enhancement in TC, leading to failures. Hence, a teacher-student architecture, coupled with key frame selection and key frame updating algorithms, is proposed. This framework successfully performs weakly supervised learning and TC knowledge distillation, overcoming the deficiencies of traditional TC training in aerial tasks. The experimental results reveal that our method utilizing merely 30\\% of labeled data, concurrently elevates mIoU and temporal consistency ensuring stable localization of terrain objects. Result demo : https://gitlab.com/prophet.ai.inc/drone-based-riverbed-inspection",
            "headline_zh": "提出弱监督上下文感知时序一致性蒸馏方法以解决无人机河流地貌视频分割问题",
            "intro_zh": [
                "核心问题：无人机遥感中数据标注复杂、时序一致性差及数据稀缺限制地貌分类",
                "方法要点：采用师生架构结合关键帧选择与更新算法，实现弱监督学习和时序知识蒸馏",
                "实验或效果：仅用30%标注数据，同时提升mIoU和时序一致性，稳定定位地形对象"
            ],
            "tags_zh": [
                "无人机遥感",
                "弱监督学习",
                "时序一致性",
                "知识蒸馏",
                "视频分割",
                "地貌分类"
            ],
            "_index": 56
        },
        {
            "title": "Arbitrary-Resolution and Arbitrary-Scale Face Super-Resolution with Implicit Representation Networks",
            "authors": [
                "Yi Ting Tsai",
                "Yu Wei Chen",
                "Hong-Han Shuai",
                "Ching-Chun Huang"
            ],
            "arxiv_id": "2511.16341v1",
            "summary": "Face super-resolution (FSR) is a critical technique for enhancing low-resolution facial images and has significant implications for face-related tasks. However, existing FSR methods are limited by fixed up-sampling scales and sensitivity to input size variations. To address these limitations, this paper introduces an Arbitrary-Resolution and Arbitrary-Scale FSR method with implicit representation networks (ARASFSR), featuring three novel designs. First, ARASFSR employs 2D deep features, local relative coordinates, and up-sampling scale ratios to predict RGB values for each target pixel, allowing super-resolution at any up-sampling scale. Second, a local frequency estimation module captures high-frequency facial texture information to reduce the spectral bias effect. Lastly, a global coordinate modulation module guides FSR to leverage prior facial structure knowledge and achieve resolution adaptation effectively. Quantitative and qualitative evaluations demonstrate the robustness of ARASFSR over existing state-of-the-art methods while super-resolving facial images across various input sizes and up-sampling scales.",
            "headline_zh": "提出ARASFSR方法，实现任意分辨率和尺度的面部超分辨率，提升鲁棒性。",
            "intro_zh": [
                "现有面部超分辨率方法受限于固定上采样尺度和对输入尺寸变化的敏感性。",
                "使用隐式表示网络结合局部频率估计和全局坐标调制，支持任意上采样。",
                "实验显示ARASFSR在多种输入尺寸和上采样尺度下优于现有方法。"
            ],
            "tags_zh": [
                "面部超分辨率",
                "隐式表示网络",
                "任意尺度上采样",
                "局部频率估计",
                "全局坐标调制"
            ],
            "_index": 57
        },
        {
            "title": "Safe and Optimal Variable Impedance Control via Certified Reinforcement Learning",
            "authors": [
                "Shreyas Kumar",
                "Ravi Prakash"
            ],
            "arxiv_id": "2511.16330v1",
            "summary": "Reinforcement learning (RL) offers a powerful approach for robots to learn complex, collaborative skills by combining Dynamic Movement Primitives (DMPs) for motion and Variable Impedance Control (VIC) for compliant interaction. However, this model-free paradigm often risks instability and unsafe exploration due to the time-varying nature of impedance gains. This work introduces Certified Gaussian Manifold Sampling (C-GMS), a novel trajectory-centric RL framework that learns combined DMP and VIC policies while guaranteeing Lyapunov stability and actuator feasibility by construction. Our approach reframes policy exploration as sampling from a mathematically defined manifold of stable gain schedules. This ensures every policy rollout is guaranteed to be stable and physically realizable, thereby eliminating the need for reward penalties or post-hoc validation. Furthermore, we provide a theoretical guarantee that our approach ensures bounded tracking error even in the presence of bounded model errors and deployment-time uncertainties. We demonstrate the effectiveness of C-GMS in simulation and verify its efficacy on a real robot, paving the way for reliable autonomous interaction in complex environments.",
            "headline_zh": "提出认证高斯流形采样以解决机器人变阻抗控制中的不稳定问题",
            "intro_zh": [
                "核心问题：模型无关强化学习在变阻抗控制中易导致不稳定和不安全探索",
                "方法要点：通过采样稳定增益流形，保证李雅普诺夫稳定性和执行器可行性",
                "实验或效果：在仿真和真实机器人上验证了有界跟踪误差和可靠性"
            ],
            "tags_zh": [
                "强化学习",
                "变阻抗控制",
                "李雅普诺夫稳定",
                "机器人控制",
                "认证学习"
            ],
            "_index": 58
        },
        {
            "title": "ChangeDINO: DINOv3-Driven Building Change Detection in Optical Remote Sensing Imagery",
            "authors": [
                "Ching-Heng Cheng",
                "Chih-Chung Hsu"
            ],
            "arxiv_id": "2511.16322v1",
            "summary": "Remote sensing change detection (RSCD) aims to identify surface changes from co-registered bi-temporal images. However, many deep learning-based RSCD methods rely solely on change-map annotations and underuse the semantic information in non-changing regions, which limits robustness under illumination variation, off-nadir views, and scarce labels. This article introduces ChangeDINO, an end-to-end multiscale Siamese framework for optical building change detection. The model fuses a lightweight backbone stream with features transferred from a frozen DINOv3, yielding semantic- and context-rich pyramids even on small datasets. A spatial-spectral differential transformer decoder then exploits multi-scale absolute differences as change priors to highlight true building changes and suppress irrelevant responses. Finally, a learnable morphology module refines the upsampled logits to recover clean boundaries. Experiments on four public benchmarks show that ChangeDINO consistently outperforms recent state-of-the-art methods in IoU and F1, and ablation studies confirm the effectiveness of each component. The source code is available at https://github.com/chingheng0808/ChangeDINO.",
            "headline_zh": "提出ChangeDINO以解决光学遥感图像中建筑变化检测的鲁棒性问题",
            "intro_zh": [
                "核心问题：现有方法依赖变化图标注，忽视非变化区域语义信息，导致光照和视角变化下鲁棒性差。",
                "方法要点：结合轻量骨干与冻结DINOv3特征，使用多尺度差分变换器解码器和可学习形态模块。",
                "实验或效果：在四个公开基准上IoU和F1指标优于最新方法，消融研究验证组件有效性。"
            ],
            "tags_zh": [
                "遥感变化检测",
                "DINOv3特征融合",
                "多尺度差分变换器",
                "可学习形态模块",
                "建筑变化检测",
                "Siamese框架"
            ],
            "_index": 59
        },
        {
            "title": "WWE-UIE: A Wavelet & White Balance Efficient Network for Underwater Image Enhancement",
            "authors": [
                "Ching-Heng Cheng",
                "Jen-Wei Lee",
                "Chia-Ming Lee",
                "Chih-Chung Hsu"
            ],
            "arxiv_id": "2511.16321v1",
            "summary": "Underwater Image Enhancement (UIE) aims to restore visibility and correct color distortions caused by wavelength-dependent absorption and scattering. Recent hybrid approaches, which couple domain priors with modern deep neural architectures, have achieved strong performance but incur high computational cost, limiting their practicality in real-time scenarios. In this work, we propose WWE-UIE, a compact and efficient enhancement network that integrates three interpretable priors. First, adaptive white balance alleviates the strong wavelength-dependent color attenuation, particularly the dominance of blue-green tones. Second, a wavelet-based enhancement block (WEB) performs multi-band decomposition, enabling the network to capture both global structures and fine textures, which are critical for underwater restoration. Third, a gradient-aware module (SGFB) leverages Sobel operators with learnable gating to explicitly preserve edge structures degraded by scattering. Extensive experiments on benchmark datasets demonstrate that WWE-UIE achieves competitive restoration quality with substantially fewer parameters and FLOPs, enabling real-time inference on resource-limited platforms. Ablation studies and visualizations further validate the contribution of each component. The source code is available at https://github.com/chingheng0808/WWE-UIE.",
            "headline_zh": "提出WWE-UIE网络，结合小波与白平衡先验，高效增强水下图像并实现实时推理。",
            "intro_zh": [
                "水下图像因波长吸收和散射导致可见性差和颜色失真，现有方法计算成本高。",
                "方法集成自适应白平衡、小波分解和梯度感知模块，提升颜色和结构恢复。",
                "实验显示参数和FLOPs显著减少，在基准数据集上实现竞争性恢复质量。"
            ],
            "tags_zh": [
                "水下图像增强",
                "小波分解",
                "白平衡校正",
                "梯度感知",
                "高效网络",
                "实时推理"
            ],
            "_index": 60
        },
        {
            "title": "NaTex: Seamless Texture Generation as Latent Color Diffusion",
            "authors": [
                "Zeqiang Lai",
                "Yunfei Zhao",
                "Zibo Zhao",
                "Xin Yang",
                "Xin Huang",
                "Jingwei Huang",
                "Xiangyu Yue",
                "Chunchao Guo"
            ],
            "arxiv_id": "2511.16317v1",
            "summary": "We present NaTex, a native texture generation framework that predicts texture color directly in 3D space. In contrast to previous approaches that rely on baking 2D multi-view images synthesized by geometry-conditioned Multi-View Diffusion models (MVDs), NaTex avoids several inherent limitations of the MVD pipeline. These include difficulties in handling occluded regions that require inpainting, achieving precise mesh-texture alignment along boundaries, and maintaining cross-view consistency and coherence in both content and color intensity. NaTex features a novel paradigm that addresses the aforementioned issues by viewing texture as a dense color point cloud. Driven by this idea, we propose latent color diffusion, which comprises a geometry-awared color point cloud VAE and a multi-control diffusion transformer (DiT), entirely trained from scratch using 3D data, for texture reconstruction and generation. To enable precise alignment, we introduce native geometry control that conditions the DiT on direct 3D spatial information via positional embeddings and geometry latents. We co-design the VAE-DiT architecture, where the geometry latents are extracted via a dedicated geometry branch tightly coupled with the color VAE, providing fine-grained surface guidance that maintains strong correspondence with the texture. With these designs, NaTex demonstrates strong performance, significantly outperforming previous methods in texture coherence and alignment. Moreover, NaTex also exhibits strong generalization capabilities, either training-free or with simple tuning, for various downstream applications, e.g., material generation, texture refinement, and part segmentation and texturing.",
            "headline_zh": "提出NaTex框架以解决3D纹理生成中的对齐与一致性问题",
            "intro_zh": [
                "核心问题：传统方法依赖2D多视图扩散，存在遮挡、对齐和颜色一致性问题",
                "方法要点：采用潜在颜色扩散，结合几何感知VAE和多控制DiT，直接预测3D空间颜色",
                "实验或效果：在纹理对齐和一致性上显著优于先前方法，并支持多种下游应用"
            ],
            "tags_zh": [
                "3D纹理生成",
                "潜在颜色扩散",
                "几何感知VAE",
                "多控制扩散变换器",
                "纹理对齐",
                "下游应用"
            ],
            "_index": 61
        },
        {
            "title": "BioBench: A Blueprint to Move Beyond ImageNet for Scientific ML Benchmarks",
            "authors": [
                "Samuel Stevens"
            ],
            "arxiv_id": "2511.16315v1",
            "summary": "ImageNet-1K linear-probe transfer accuracy remains the default proxy for visual representation quality, yet it no longer predicts performance on scientific imagery. Across 46 modern vision model checkpoints, ImageNet top-1 accuracy explains only 34% of variance on ecology tasks and mis-ranks 30% of models above 75% accuracy. We present BioBench, an open ecology vision benchmark that captures what ImageNet misses. BioBench unifies 9 publicly released, application-driven tasks, 4 taxonomic kingdoms, and 6 acquisition modalities (drone RGB, web video, micrographs, in-situ and specimen photos, camera-trap frames), totaling 3.1M images. A single Python API downloads data, fits lightweight classifiers to frozen backbones, and reports class-balanced macro-F1 (plus domain metrics for FishNet and FungiCLEF); ViT-L models evaluate in 6 hours on an A6000 GPU. BioBench provides new signal for computer vision in ecology and a template recipe for building reliable AI-for-science benchmarks in any domain. Code and predictions are available at https://github.com/samuelstevens/biobench and results at https://samuelstevens.me/biobench.",
            "headline_zh": "提出BioBench基准以解决ImageNet在科学图像评估中的不足",
            "intro_zh": [
                "ImageNet-1K线性探针准确率在科学图像任务中预测性能差，方差解释仅34%",
                "BioBench整合9个生态任务、4个分类界和6种采集模态，提供统一评估API",
                "在A6000 GPU上6小时完成ViT-L模型评估，提供类平衡宏F1等指标"
            ],
            "tags_zh": [
                "生态视觉基准",
                "图像分类评估",
                "科学机器学习",
                "多模态数据集",
                "模型转移性能"
            ],
            "_index": 62
        },
        {
            "title": "Sparse Autoencoders are Topic Models",
            "authors": [
                "Leander Girrbach",
                "Zeynep Akata"
            ],
            "arxiv_id": "2511.16309v1",
            "summary": "Sparse autoencoders (SAEs) are used to analyze embeddings, but their role and practical value are debated. We propose a new perspective on SAEs by demonstrating that they can be naturally understood as topic models. We extend Latent Dirichlet Allocation to embedding spaces and derive the SAE objective as a maximum a posteriori estimator under this model. This view implies SAE features are thematic components rather than steerable directions. Based on this, we introduce SAE-TM, a topic modeling framework that: (1) trains an SAE to learn reusable topic atoms, (2) interprets them as word distributions on downstream data, and (3) merges them into any number of topics without retraining. SAE-TM yields more coherent topics than strong baselines on text and image datasets while maintaining diversity. Finally, we analyze thematic structure in image datasets and trace topic changes over time in Japanese woodblock prints. Our work positions SAEs as effective tools for large-scale thematic analysis across modalities. Code and data will be released upon publication.",
            "headline_zh": "提出SAE-TM框架，将稀疏自编码器视为主题模型，用于跨模态大规模主题分析。",
            "intro_zh": [
                "核心问题：稀疏自编码器在嵌入分析中的角色与实用价值存在争议。",
                "方法要点：将LDA扩展至嵌入空间，推导SAE目标为最大后验估计器。",
                "实验或效果：SAE-TM在文本和图像数据集上生成更连贯且多样的主题。"
            ],
            "tags_zh": [
                "稀疏自编码器",
                "主题建模",
                "嵌入分析",
                "跨模态分析",
                "最大后验估计"
            ],
            "_index": 63
        },
        {
            "title": "InEKFormer: A Hybrid State Estimator for Humanoid Robots",
            "authors": [
                "Lasse Hohmeyer",
                "Mihaela Popescu",
                "Ivan Bergonzani",
                "Dennis Mronga",
                "Frank Kirchner"
            ],
            "arxiv_id": "2511.16306v1",
            "summary": "Humanoid robots have great potential for a wide range of applications, including industrial and domestic use, healthcare, and search and rescue missions. However, bipedal locomotion in different environments is still a challenge when it comes to performing stable and dynamic movements. This is where state estimation plays a crucial role, providing fast and accurate feedback of the robot's floating base state to the motion controller. Although classical state estimation methods such as Kalman filters are widely used in robotics, they require expert knowledge to fine-tune the noise parameters. Due to recent advances in the field of machine learning, deep learning methods are increasingly used for state estimation tasks. In this work, we propose the InEKFormer, a novel hybrid state estimation method that incorporates an invariant extended Kalman filter (InEKF) and a Transformer network. We compare our method with the InEKF and the KalmanNet approaches on datasets obtained from the humanoid robot RH5. The results indicate the potential of Transformers in humanoid state estimation, but also highlight the need for robust autoregressive training in these high-dimensional problems.",
            "headline_zh": "提出InEKFormer混合状态估计方法以提升人形机器人动态运动稳定性",
            "intro_zh": [
                "核心问题：人形机器人在不同环境中双足运动难以实现稳定动态控制",
                "方法要点：结合不变扩展卡尔曼滤波与Transformer网络进行状态估计",
                "实验或效果：在RH5机器人数据集上优于InEKF和KalmanNet，显示Transformer潜力"
            ],
            "tags_zh": [
                "人形机器人",
                "状态估计",
                "混合方法",
                "Transformer网络",
                "不变扩展卡尔曼滤波"
            ],
            "_index": 64
        },
        {
            "title": "Upsample Anything: A Simple and Hard to Beat Baseline for Feature Upsampling",
            "authors": [
                "Minseok Seo",
                "Mark Hamilton",
                "Changick Kim"
            ],
            "arxiv_id": "2511.16301v1",
            "summary": "We present \\textbf{Upsample Anything}, a lightweight test-time optimization (TTO) framework that restores low-resolution features to high-resolution, pixel-wise outputs without any training. Although Vision Foundation Models demonstrate strong generalization across diverse downstream tasks, their representations are typically downsampled by 14x/16x (e.g., ViT), which limits their direct use in pixel-level applications. Existing feature upsampling approaches depend on dataset-specific retraining or heavy implicit optimization, restricting scalability and generalization. Upsample Anything addresses these issues through a simple per-image optimization that learns an anisotropic Gaussian kernel combining spatial and range cues, effectively bridging Gaussian Splatting and Joint Bilateral Upsampling. The learned kernel acts as a universal, edge-aware operator that transfers seamlessly across architectures and modalities, enabling precise high-resolution reconstruction of features, depth, or probability maps. It runs in only $\\approx0.419 \\text{s}$ per 224x224 image and achieves state-of-the-art performance on semantic segmentation, depth estimation, and both depth and probability map upsampling.",
            "headline_zh": "提出Upsample Anything框架，通过测试时优化实现特征上采样，解决像素级应用限制。",
            "intro_zh": [
                "核心问题：视觉基础模型特征下采样限制像素级任务应用，现有方法依赖重训练或复杂优化。",
                "方法要点：使用各向异性高斯核结合空间和范围线索，实现无需训练的轻量级每图像优化。",
                "实验或效果：在语义分割和深度估计等任务中达到SOTA，每图像处理约0.419秒。"
            ],
            "tags_zh": [
                "特征上采样",
                "测试时优化",
                "高斯核学习",
                "像素级重建",
                "边缘感知操作"
            ],
            "_index": 65
        },
        {
            "title": "Optimizing 3D Gaussian Splattering for Mobile GPUs",
            "authors": [
                "Md Musfiqur Rahman Sanim",
                "Zhihao Shu",
                "Bahram Afsharmanesh",
                "AmirAli Mirian",
                "Jiexiong Guan",
                "Wei Niu",
                "Bin Ren",
                "Gagan Agrawal"
            ],
            "arxiv_id": "2511.16298v1",
            "summary": "Image-based 3D scene reconstruction, which transforms multi-view images into a structured 3D representation of the surrounding environment, is a common task across many modern applications. 3D Gaussian Splatting (3DGS) is a new paradigm to address this problem and offers considerable efficiency as compared to the previous methods. Motivated by this, and considering various benefits of mobile device deployment (data privacy, operating without internet connectivity, and potentially faster responses), this paper develops Texture3dgs, an optimized mapping of 3DGS for a mobile GPU. A critical challenge in this area turns out to be optimizing for the two-dimensional (2D) texture cache, which needs to be exploited for faster executions on mobile GPUs. As a sorting method dominates the computations in 3DGS on mobile platforms, the core of Texture3dgs is a novel sorting algorithm where the processing, data movement, and placement are highly optimized for 2D memory. The properties of this algorithm are analyzed in view of a cost model for the texture cache. In addition, we accelerate other steps of the 3DGS algorithm through improved variable layout design and other optimizations. End-to-end evaluation shows that Texture3dgs delivers up to 4.1$\\times$ and 1.7$\\times$ speedup for the sorting and overall 3D scene reconstruction, respectively -- while also reducing memory usage by up to 1.6$\\times$ -- demonstrating the effectiveness of our design for efficient mobile 3D scene reconstruction.",
            "headline_zh": "提出Texture3dgs以优化3D高斯泼溅在移动GPU上的性能",
            "intro_zh": [
                "核心问题：移动GPU上3D高斯泼溅的二维纹理缓存优化不足，导致计算效率低。",
                "方法要点：设计新型排序算法，优化数据处理和内存布局，针对纹理缓存进行加速。",
                "实验或效果：排序速度提升4.1倍，整体重建加速1.7倍，内存使用减少1.6倍。"
            ],
            "tags_zh": [
                "3D高斯泼溅",
                "移动GPU优化",
                "纹理缓存",
                "排序算法",
                "3D场景重建"
            ],
            "_index": 66
        },
        {
            "title": "Explainable AI for Diabetic Retinopathy Detection Using Deep Learning with Attention Mechanisms and Fuzzy Logic-Based Interpretability",
            "authors": [
                "Abishek Karthik",
                "Pandiyaraju V",
                "Sreya Mynampati"
            ],
            "arxiv_id": "2511.16294v1",
            "summary": "The task of weed detection is an essential element of precision agriculture since accurate species identification allows a farmer to selectively apply herbicides and fits into sustainable agriculture crop management. This paper proposes a hybrid deep learning framework recipe for weed detection that utilizes Convolutional Neural Networks (CNNs), Vision Transformers (ViTs), and Graph Neural Networks (GNNs) to build robustness to multiple field conditions. A Generative Adversarial Network (GAN)-based augmentation method was imposed to balance class distributions and better generalize the model. Further, a self-supervised contrastive pre-training method helps to learn more features from limited annotated data. Experimental results yield superior results with 99.33% accuracy, precision, recall, and F1-score on multi-benchmark datasets. The proposed model architecture enables local, global, and relational feature representations and offers high interpretability and adaptability. Practically, the framework allows real-time, efficient deployment of edge devices for automated weed detecting, reducing over-reliance on herbicides and providing scalable, sustainable precision-farming options.",
            "headline_zh": "提出混合深度学习框架以解决杂草检测问题，实现高精度与可解释性。",
            "intro_zh": [
                "核心问题：杂草检测对精准农业至关重要，需准确识别物种以减少除草剂使用。",
                "方法要点：结合CNN、ViT和GNN，采用GAN增强和自监督预训练提升鲁棒性。",
                "实验或效果：在多个数据集上达到99.33%准确率，支持实时边缘设备部署。"
            ],
            "tags_zh": [
                "杂草检测",
                "混合深度学习",
                "GAN数据增强",
                "自监督学习",
                "精准农业",
                "边缘计算"
            ],
            "_index": 67
        },
        {
            "title": "Building temporally coherent 3D maps with VGGT for memory-efficient Semantic SLAM",
            "authors": [
                "Gergely Dinya",
                "Péter Halász",
                "András Lőrincz",
                "Kristóf Karacs",
                "Anna Gelencsér-Horváth"
            ],
            "arxiv_id": "2511.16282v1",
            "summary": "We present a fast, spatio-temporal scene understanding framework based on Vision Gated Generative Transformers (VGGT). The proposed pipeline is designed to enable efficient, close to real-time performance, supporting applications including assistive navigation. To achieve continuous updates of the 3D scene representation, we process the image flow with a sliding window, aligning submaps, thereby overcoming VGGT's high memory demands. We exploit the VGGT tracking head to aggregate 2D semantic instance masks into 3D objects. To allow for temporal consistency and richer contextual reasoning the system stores timestamps and instance-level identities, thereby enabling the detection of changes in the environment. We evaluate the approach on well-known benchmarks and custom datasets specifically designed for assistive navigation scenarios. The results demonstrate the applicability of the framework to real-world scenarios.",
            "headline_zh": "提出基于VGGT的快速时空场景理解框架，用于构建时间一致3D地图以支持辅助导航。",
            "intro_zh": [
                "核心问题：VGGT在语义SLAM中内存需求高，难以实时更新3D场景表示。",
                "方法要点：使用滑动窗口处理图像流和对齐子图，聚合2D语义实例掩码为3D对象。",
                "实验或效果：在标准基准和自定义数据集上评估，证明适用于真实世界辅助导航场景。"
            ],
            "tags_zh": [
                "语义SLAM",
                "3D地图构建",
                "视觉变换器",
                "时空一致性",
                "辅助导航",
                "实例分割"
            ],
            "_index": 68
        },
        {
            "title": "TetraSDF: Precise Mesh Extraction with Multi-resolution Tetrahedral Grid",
            "authors": [
                "Seonghun Oh",
                "Youngjung Uh",
                "Jin-Hwa Kim"
            ],
            "arxiv_id": "2511.16273v1",
            "summary": "Extracting meshes that exactly match the zero-level set of neural signed distance functions (SDFs) remains challenging. Sampling-based methods introduce discretization error, while continuous piecewise affine (CPWA) analytic approaches apply only to plain ReLU MLPs. We present TetraSDF, a precise analytic meshing framework for SDFs represented by a ReLU MLP composed with a multi-resolution tetrahedral positional encoder. The encoder's barycentric interpolation preserves global CPWA structure, enabling us to track ReLU linear regions within an encoder-induced polyhedral complex. A fixed analytic input preconditioner derived from the encoder's metric further reduces directional bias and stabilizes training. Across multiple benchmarks, TetraSDF matches or surpasses existing grid-based encoders in SDF reconstruction accuracy, and its analytic extractor produces highly self-consistent meshes that remain faithful to the learned isosurfaces, all with practical runtime and memory efficiency.",
            "headline_zh": "提出TetraSDF以精确提取神经SDF的网格，结合多分辨率四面体编码器。",
            "intro_zh": [
                "核心问题：基于采样的方法存在离散化误差，连续分段仿射方法仅适用于普通ReLU MLP。",
                "方法要点：使用多分辨率四面体位置编码器，保持全局CPWA结构，实现精确解析网格提取。",
                "实验或效果：在多个基准测试中，SDF重建精度高，网格自一致性强，运行和内存效率实用。"
            ],
            "tags_zh": [
                "神经符号距离函数",
                "网格提取",
                "多分辨率编码",
                "四面体网格",
                "解析方法"
            ],
            "_index": 69
        },
        {
            "title": "Weakly Supervised Segmentation and Classification of Alpha-Synuclein Aggregates in Brightfield Midbrain Images",
            "authors": [
                "Erwan Dereure",
                "Robin Louiset",
                "Laura Parkkinen",
                "David A Menassa",
                "David Holcman"
            ],
            "arxiv_id": "2511.16268v1",
            "summary": "Parkinson's disease (PD) is a neurodegenerative disorder associated with the accumulation of misfolded alpha-synuclein aggregates, forming Lewy bodies and neuritic shape used for pathology diagnostics. Automatic analysis of immunohistochemistry histopathological images with Deep Learning provides a promising tool for better understanding the spatial organization of these aggregates. In this study, we develop an automated image processing pipeline to segment and classify these aggregates in whole-slide images (WSIs) of midbrain tissue from PD and incidental Lewy Body Disease (iLBD) cases based on weakly supervised segmentation, robust to immunohistochemical labelling variability, with a ResNet50 classifier. Our approach allows to differentiate between major aggregate morphologies, including Lewy bodies and neurites with a balanced accuracy of $80\\%$. This framework paves the way for large-scale characterization of the spatial distribution and heterogeneity of alpha-synuclein aggregates in brightfield immunohistochemical tissue, and for investigating their poorly understood relationships with surrounding cells such as microglia and astrocytes.",
            "headline_zh": "提出弱监督分割与分类方法以自动分析帕金森病中α-突触核蛋白聚集物",
            "intro_zh": [
                "核心问题：帕金森病中α-突触核蛋白聚集物的自动分割与分类，以理解其空间分布。",
                "方法要点：基于弱监督分割和ResNet50分类器，处理免疫组化标记变异。",
                "实验或效果：在PD和iLBD病例中，区分Lewy体和神经突形态，平衡准确率达80%。"
            ],
            "tags_zh": [
                "弱监督分割",
                "α-突触核蛋白聚集物",
                "帕金森病",
                "全玻片图像",
                "ResNet50分类器",
                "免疫组化图像"
            ],
            "_index": 70
        },
        {
            "title": "Funabot-Upper: McKibben Actuated Haptic Suit Inducing Kinesthetic Perceptions in Trunk, Shoulder, Elbow, and Wrist",
            "authors": [
                "Haru Fukatsu",
                "Ryoji Yasuda",
                "Yuki Funabora",
                "Shinji Doki"
            ],
            "arxiv_id": "2511.16265v1",
            "summary": "This paper presents Funabot-Upper, a wearable haptic suit that enables users to perceive 14 upper-body motions, including those of the trunk, shoulder, elbow, and wrist. Inducing kinesthetic perception through wearable haptic devices has attracted attention, and various devices have been developed in the past. However, these have been limited to verifications on single body parts, and few have applied the same method to multiple body parts as well. In our previous study, we developed a technology that uses the contraction of artificial muscles to deform clothing in three dimensions. Using this technology, we developed a haptic suit that induces kinesthetic perception of 7 motions in multiple upper body. However, perceptual mixing caused by stimulating multiple human muscles has occurred between the shoulder and the elbow. In this paper, we established a new, simplified design policy and developed a novel haptic suit that induces kinesthetic perceptions in the trunk, shoulder, elbow, and wrist by stimulating joints and muscles independently. We experimentally demonstrated the induced kinesthetic perception and examined the relationship between stimulation and perceived kinesthetic perception under the new design policy. Experiments confirmed that Funabot-Upper successfully induces kinesthetic perception across multiple joints while reducing perceptual mixing observed in previous designs. The new suit improved recognition accuracy from 68.8% to 94.6% compared to the previous Funabot-Suit, demonstrating its superiority and potential for future haptic applications.",
            "headline_zh": "提出Funabot-Upper触觉套装，通过独立刺激关节和肌肉，减少感知混合并诱导上身运动感知。",
            "intro_zh": [
                "核心问题：现有可穿戴触觉设备在诱导上身运动感知时，存在肩肘间感知混合问题。",
                "方法要点：采用简化设计策略，使用人工肌肉收缩独立刺激关节和肌肉。",
                "实验或效果：实验显示识别准确率从68.8%提升至94.6%，有效减少感知混合。"
            ],
            "tags_zh": [
                "可穿戴触觉设备",
                "人工肌肉",
                "运动感知诱导",
                "感知混合减少",
                "上身运动",
                "触觉套装"
            ],
            "_index": 71
        },
        {
            "title": "Mem-MLP: Real-Time 3D Human Motion Generation from Sparse Inputs",
            "authors": [
                "Sinan Mutlu",
                "Georgios F. Angelis",
                "Savas Ozkan",
                "Paul Wisbey",
                "Anastasios Drosou",
                "Mete Ozay"
            ],
            "arxiv_id": "2511.16264v1",
            "summary": "Realistic and smooth full-body tracking is crucial for immersive AR/VR applications. Existing systems primarily track head and hands via Head Mounted Devices (HMDs) and controllers, making the 3D full-body reconstruction in-complete. One potential approach is to generate the full-body motions from sparse inputs collected from limited sensors using a Neural Network (NN) model. In this paper, we propose a novel method based on a multi-layer perceptron (MLP) backbone that is enhanced with residual connections and a novel NN-component called Memory-Block. In particular, Memory-Block represents missing sensor data with trainable code-vectors, which are combined with the sparse signals from previous time instances to improve the temporal consistency. Furthermore, we formulate our solution as a multi-task learning problem, allowing our MLP-backbone to learn robust representations that boost accuracy. Our experiments show that our method outperforms state-of-the-art baselines by substantially reducing prediction errors. Moreover, it achieves 72 FPS on mobile HMDs that ultimately improves the accuracy-running time tradeoff.",
            "headline_zh": "提出Mem-MLP方法，从稀疏输入实时生成3D人体运动以提升AR/VR沉浸感",
            "intro_zh": [
                "AR/VR中仅跟踪头手导致全身运动不完整，需从稀疏传感器数据生成完整运动",
                "方法基于MLP骨干，引入Memory-Block和残差连接，利用可训练码向量补全缺失数据",
                "实验显示预测误差显著降低，在移动HMD上达72 FPS，优化精度与运行时间权衡"
            ],
            "tags_zh": [
                "3D人体运动生成",
                "稀疏输入处理",
                "实时MLP模型",
                "AR/VR应用",
                "多任务学习"
            ],
            "_index": 72
        },
        {
            "title": "How Robot Dogs See the Unseeable",
            "authors": [
                "Oliver Bimber",
                "Karl Dietrich von Ellenrieder",
                "Michael Haller",
                "Rakesh John Amala Arokia Nathan",
                "Gianni Lunardi",
                "Marco Camurri",
                "Mohamed Youssef",
                "Santos Miguel Orozco Soto",
                "Jeremy E. Niven"
            ],
            "arxiv_id": "2511.16262v1",
            "summary": "Peering, a side-to-side motion used by animals to estimate distance through motion parallax, offers a powerful bio-inspired strategy to overcome a fundamental limitation in robotic vision: partial occlusion. Conventional robot cameras, with their small apertures and large depth of field, render both foreground obstacles and background objects in sharp focus, causing occluders to obscure critical scene information. This work establishes a formal connection between animal peering and synthetic aperture (SA) sensing from optical imaging. By having a robot execute a peering motion, its camera describes a wide synthetic aperture. Computational integration of the captured images synthesizes an image with an extremely shallow depth of field, effectively blurring out occluding elements while bringing the background into sharp focus. This efficient, wavelength-independent technique enables real-time, high-resolution perception across various spectral bands. We demonstrate that this approach not only restores basic scene understanding but also empowers advanced visual reasoning in large multimodal models, which fail with conventionally occluded imagery. Unlike feature-dependent multi-view 3D vision methods or active sensors like LiDAR, SA sensing via peering is robust to occlusion, computationally efficient, and immediately deployable on any mobile robot. This research bridges animal behavior and robotics, suggesting that peering motions for synthetic aperture sensing are a key to advanced scene understanding in complex, cluttered environments.",
            "headline_zh": "提出机器人摇摆运动合成孔径感知以解决部分遮挡问题",
            "intro_zh": [
                "核心问题：机器人视觉中部分遮挡导致前景障碍物模糊背景信息",
                "方法要点：通过机器人摇摆运动模拟动物窥视，合成宽孔径图像实现浅景深",
                "实验或效果：实时高分辨率感知，增强多模态模型在遮挡场景下的推理能力"
            ],
            "tags_zh": [
                "合成孔径感知",
                "机器人视觉",
                "运动视差",
                "生物启发方法",
                "遮挡鲁棒性",
                "实时感知"
            ],
            "_index": 73
        },
        {
            "title": "FT-NCFM: An Influence-Aware Data Distillation Framework for Efficient VLA Models",
            "authors": [
                "Kewei Chen",
                "Yayu Long",
                "Shuai Li",
                "Mingsheng Shang"
            ],
            "arxiv_id": "2511.16233v1",
            "summary": "The powerful generalization of Vision-Language-Action (VLA) models is bottlenecked by their heavy reliance on massive, redundant, and unevenly valued datasets, hindering their widespread application. Existing model-centric optimization paths, such as model compression (which often leads to performance degradation) or policy distillation (whose products are model-dependent and lack generality), fail to fundamentally address this data-level challenge. To this end, this paper introduces FT-NCFM, a fundamentally different, data-centric generative data distillation framework. Our framework employs a self-contained Fact-Tracing (FT) engine that combines causal attribution with programmatic contrastive verification to assess the intrinsic value of samples. Guided by these assessments, an adversarial NCFM process synthesizes a model-agnostic, information-dense, and reusable data asset. Experimental results on several mainstream VLA benchmarks show that models trained on just 5% of our distilled coreset achieve a success rate of 85-90% compared with training on the full dataset, while reducing training time by over 80%. Our work demonstrates that intelligent data distillation is a highly promising new path for building efficient, high-performance VLA models.",
            "headline_zh": "提出FT-NCFM数据蒸馏框架以解决VLA模型数据冗余问题",
            "intro_zh": [
                "核心问题：VLA模型依赖大规模冗余数据，阻碍广泛应用",
                "方法要点：结合因果归因和程序化对比验证，合成信息密集数据",
                "实验或效果：蒸馏数据集仅需5%数据，性能达85-90%，训练时间减80%"
            ],
            "tags_zh": [
                "视觉语言动作模型",
                "数据蒸馏",
                "因果归因",
                "模型无关数据",
                "高效训练",
                "核心集合成"
            ],
            "_index": 74
        },
        {
            "title": "SwiTrack: Tri-State Switch for Cross-Modal Object Tracking",
            "authors": [
                "Boyue Xu",
                "Ruichao Hou",
                "Tongwei Ren",
                "Dongming Zhou",
                "Gangshan Wu",
                "Jinde Cao"
            ],
            "arxiv_id": "2511.16227v1",
            "summary": "Cross-modal object tracking (CMOT) is an emerging task that maintains target consistency while the video stream switches between different modalities, with only one modality available in each frame, mostly focusing on RGB-Near Infrared (RGB-NIR) tracking. Existing methods typically connect parallel RGB and NIR branches to a shared backbone, which limits the comprehensive extraction of distinctive modality-specific features and fails to address the issue of object drift, especially in the presence of unreliable inputs. In this paper, we propose SwiTrack, a novel state-switching framework that redefines CMOT through the deployment of three specialized streams. Specifically, RGB frames are processed by the visual encoder, while NIR frames undergo refinement via a NIR gated adapter coupled with the visual encoder to progressively calibrate shared latent space features, thereby yielding more robust cross-modal representations. For invalid modalities, a consistency trajectory prediction module leverages spatio-temporal cues to estimate target movement, ensuring robust tracking and mitigating drift. Additionally, we incorporate dynamic template reconstruction to iteratively update template features and employ a similarity alignment loss to reinforce feature consistency. Experimental results on the latest benchmarks demonstrate that our tracker achieves state-of-the-art performance, boosting precision rate and success rate gains by 7.2\\% and 4.3\\%, respectively, while maintaining real-time tracking at 65 frames per second. Code and models are available at https://github.com/xuboyue1999/SwiTrack.git.",
            "headline_zh": "提出SwiTrack三态切换框架以解决跨模态目标跟踪中的特征提取不足和漂移问题",
            "intro_zh": [
                "核心问题：现有方法在RGB-NIR跨模态跟踪中难以提取模态特有特征，易导致目标漂移",
                "方法要点：采用三流架构，包括视觉编码器、NIR门控适配器和一致性轨迹预测模块",
                "实验或效果：在基准测试中精度和成功率分别提升7.2%和4.3%，实时跟踪达65帧/秒"
            ],
            "tags_zh": [
                "跨模态目标跟踪",
                "RGB-NIR跟踪",
                "三态切换框架",
                "特征校准",
                "轨迹预测",
                "动态模板重建"
            ],
            "_index": 75
        },
        {
            "title": "DynaMimicGen: A Data Generation Framework for Robot Learning of Dynamic Tasks",
            "authors": [
                "Vincenzo Pomponi",
                "Paolo Franceschi",
                "Stefano Baraldo",
                "Loris Roveda",
                "Oliver Avram",
                "Luca Maria Gambardella",
                "Anna Valente"
            ],
            "arxiv_id": "2511.16223v1",
            "summary": "Learning robust manipulation policies typically requires large and diverse datasets, the collection of which is time-consuming, labor-intensive, and often impractical for dynamic environments. In this work, we introduce DynaMimicGen (D-MG), a scalable dataset generation framework that enables policy training from minimal human supervision while uniquely supporting dynamic task settings. Given only a few human demonstrations, D-MG first segments the demonstrations into meaningful sub-tasks, then leverages Dynamic Movement Primitives (DMPs) to adapt and generalize the demonstrated behaviors to novel and dynamically changing environments. Improving prior methods that rely on static assumptions or simplistic trajectory interpolation, D-MG produces smooth, realistic, and task-consistent Cartesian trajectories that adapt in real time to changes in object poses, robot states, or scene geometry during task execution. Our method supports different scenarios - including scene layouts, object instances, and robot configurations - making it suitable for both static and highly dynamic manipulation tasks. We show that robot agents trained via imitation learning on D-MG-generated data achieve strong performance across long-horizon and contact-rich benchmarks, including tasks like cube stacking and placing mugs in drawers, even under unpredictable environment changes. By eliminating the need for extensive human demonstrations and enabling generalization in dynamic settings, D-MG offers a powerful and efficient alternative to manual data collection, paving the way toward scalable, autonomous robot learning.",
            "headline_zh": "提出DynaMimicGen框架以解决动态环境中机器人学习数据稀缺问题",
            "intro_zh": [
                "核心问题：动态环境下机器人学习需大量数据，人工收集耗时且不实用",
                "方法要点：基于少量演示，分割任务并使用动态运动基元生成适应动态变化的轨迹",
                "实验或效果：在长时程和接触丰富任务中，训练代理在动态变化下表现优异"
            ],
            "tags_zh": [
                "机器人学习",
                "动态任务",
                "数据生成",
                "模仿学习",
                "轨迹生成"
            ],
            "_index": 76
        },
        {
            "title": "Can MLLMs Read the Room? A Multimodal Benchmark for Assessing Deception in Multi-Party Social Interactions",
            "authors": [
                "Caixin Kang",
                "Yifei Huang",
                "Liangyang Ouyang",
                "Mingfang Zhang",
                "Ruicong Liu",
                "Yoichi Sato"
            ],
            "arxiv_id": "2511.16221v1",
            "summary": "Despite their advanced reasoning capabilities, state-of-the-art Multimodal Large Language Models (MLLMs) demonstrably lack a core component of human intelligence: the ability to `read the room' and assess deception in complex social interactions. To rigorously quantify this failure, we introduce a new task, Multimodal Interactive Deception Assessment (MIDA), and present a novel multimodal dataset providing synchronized video and text with verifiable ground-truth labels for every statement. We establish a comprehensive benchmark evaluating 12 state-of-the-art open- and closed-source MLLMs, revealing a significant performance gap: even powerful models like GPT-4o struggle to distinguish truth from falsehood reliably. Our analysis of failure modes indicates that these models fail to effectively ground language in multimodal social cues and lack the ability to model what others know, believe, or intend, highlighting the urgent need for novel approaches to building more perceptive and trustworthy AI systems. To take a step forward, we design a Social Chain-of-Thought (SoCoT) reasoning pipeline and a Dynamic Social Epistemic Memory (DSEM) module. Our framework yields performance improvement on this challenging task, demonstrating a promising new path toward building MLLMs capable of genuine human-like social reasoning.",
            "headline_zh": "提出多模态交互欺骗评估任务与数据集，以解决MLLMs在复杂社交中识别欺骗的不足。",
            "intro_zh": [
                "核心问题：MLLMs缺乏在多人社交互动中评估欺骗的能力，无法有效理解多模态社交线索。",
                "方法要点：引入MIDA任务和数据集，设计SoCoT推理管道和DSEM模块以提升模型性能。",
                "实验或效果：评估12个MLLMs显示性能差距大，新框架在任务上带来改进。"
            ],
            "tags_zh": [
                "多模态大语言模型",
                "社交互动分析",
                "欺骗检测",
                "多模态基准",
                "推理管道",
                "动态记忆模块"
            ],
            "_index": 77
        },
        {
            "title": "Unsupervised Image Classification with Adaptive Nearest Neighbor Selection and Cluster Ensembles",
            "authors": [
                "Melih Baydar",
                "Emre Akbas"
            ],
            "arxiv_id": "2511.16213v1",
            "summary": "Unsupervised image classification, or image clustering, aims to group unlabeled images into semantically meaningful categories. Early methods integrated representation learning and clustering within an iterative framework. However, the rise of foundational models have recently shifted focus solely to clustering, bypassing the representation learning step. In this work, we build upon a recent multi-head clustering approach by introducing adaptive nearest neighbor selection and cluster ensembling strategies to improve clustering performance. Our method, \"Image Clustering through Cluster Ensembles\" (ICCE), begins with a clustering stage, where we train multiple clustering heads on a frozen backbone, producing diverse image clusterings. We then employ a cluster ensembling technique to consolidate these potentially conflicting results into a unified consensus clustering. Finally, we train an image classifier using the consensus clustering result as pseudo-labels. ICCE achieves state-of-the-art performance on ten image classification benchmarks, achieving 99.3% accuracy on CIFAR10, 89% on CIFAR100, and 70.4% on ImageNet datasets, narrowing the performance gap with supervised methods. To the best of our knowledge, ICCE is the first fully unsupervised image classification method to exceed 70% accuracy on ImageNet.",
            "headline_zh": "提出ICCE方法，通过自适应近邻选择和聚类集成提升无监督图像分类性能",
            "intro_zh": [
                "核心问题：无监督图像分类需将未标注图像分组为语义类别，现有方法多忽略表示学习",
                "方法要点：使用多聚类头训练，结合自适应近邻选择和聚类集成生成共识聚类",
                "实验或效果：在CIFAR10、CIFAR100和ImageNet等基准上达到SOTA，ImageNet准确率超70%"
            ],
            "tags_zh": [
                "无监督图像分类",
                "聚类集成",
                "自适应近邻选择",
                "多聚类头",
                "伪标签训练"
            ],
            "_index": 78
        },
        {
            "title": "When Alignment Fails: Multimodal Adversarial Attacks on Vision-Language-Action Models",
            "authors": [
                "Yuping Yan",
                "Yuhan Xie",
                "Yinxin Zhang",
                "Lingjuan Lyu",
                "Yaochu Jin"
            ],
            "arxiv_id": "2511.16203v1",
            "summary": "Vision-Language-Action models (VLAs) have recently demonstrated remarkable progress in embodied environments, enabling robots to perceive, reason, and act through unified multimodal understanding. Despite their impressive capabilities, the adversarial robustness of these systems remains largely unexplored, especially under realistic multimodal and black-box conditions. Existing studies mainly focus on single-modality perturbations and overlook the cross-modal misalignment that fundamentally affects embodied reasoning and decision-making. In this paper, we introduce VLA-Fool, a comprehensive study of multimodal adversarial robustness in embodied VLA models under both white-box and black-box settings. VLA-Fool unifies three levels of multimodal adversarial attacks: (1) textual perturbations through gradient-based and prompt-based manipulations, (2) visual perturbations via patch and noise distortions, and (3) cross-modal misalignment attacks that intentionally disrupt the semantic correspondence between perception and instruction. We further incorporate a VLA-aware semantic space into linguistic prompts, developing the first automatically crafted and semantically guided prompting framework. Experiments on the LIBERO benchmark using a fine-tuned OpenVLA model reveal that even minor multimodal perturbations can cause significant behavioral deviations, demonstrating the fragility of embodied multimodal alignment.",
            "headline_zh": "提出VLA-Fool方法以评估具身视觉-语言-动作模型的多模态对抗鲁棒性",
            "intro_zh": [
                "核心问题：具身VLA模型在多模态和黑盒条件下的对抗鲁棒性未知，现有研究忽视跨模态错位影响",
                "方法要点：统一文本、视觉和跨模态攻击，并引入语义引导提示框架增强攻击效果",
                "实验或效果：在LIBERO基准上，微小多模态扰动可导致显著行为偏差，揭示模型脆弱性"
            ],
            "tags_zh": [
                "多模态对抗攻击",
                "视觉-语言-动作模型",
                "跨模态错位",
                "黑盒设置",
                "语义引导提示",
                "具身环境"
            ],
            "_index": 79
        },
        {
            "title": "PIPHEN: Physical Interaction Prediction with Hamiltonian Energy Networks",
            "authors": [
                "Kewei Chen",
                "Yayu Long",
                "Mingsheng Shang"
            ],
            "arxiv_id": "2511.16200v1",
            "summary": "Multi-robot systems in complex physical collaborations face a \"shared brain dilemma\": transmitting high-dimensional multimedia data (e.g., video streams at ~30MB/s) creates severe bandwidth bottlenecks and decision-making latency. To address this, we propose PIPHEN, an innovative distributed physical cognition-control framework. Its core idea is to replace \"raw data communication\" with \"semantic communication\" by performing \"semantic distillation\" at the robot edge, reconstructing high-dimensional perceptual data into compact, structured physical representations. This idea is primarily realized through two key components: (1) a novel Physical Interaction Prediction Network (PIPN), derived from large model knowledge distillation, to generate this representation; and (2) a Hamiltonian Energy Network (HEN) controller, based on energy conservation, to precisely translate this representation into coordinated actions. Experiments show that, compared to baseline methods, PIPHEN can compress the information representation to less than 5% of the original data volume and reduce collaborative decision-making latency from 315ms to 76ms, while significantly improving task success rates. This work provides a fundamentally efficient paradigm for resolving the \"shared brain dilemma\" in resource-constrained multi-robot systems.",
            "headline_zh": "提出PIPHEN框架以解决多机器人系统共享脑困境",
            "intro_zh": [
                "核心问题：多机器人协作中高维数据传输导致带宽瓶颈和决策延迟",
                "方法要点：通过语义蒸馏和哈密顿能量网络实现物理交互预测与控制",
                "实验效果：压缩数据至原体积5%以下，延迟从315ms降至76ms，提升任务成功率"
            ],
            "tags_zh": [
                "多机器人系统",
                "语义通信",
                "物理交互预测",
                "哈密顿能量网络",
                "知识蒸馏",
                "分布式控制"
            ],
            "_index": 80
        },
        {
            "title": "PrIntMesh: Precise Intersection Surfaces for 3D Organ Mesh Reconstruction",
            "authors": [
                "Deniz Sayin Mercadier",
                "Hieu Le",
                "Yihong Chen",
                "Jiancheng Yang",
                "Udaranga Wickramasinghe",
                "Pascal Fua"
            ],
            "arxiv_id": "2511.16186v1",
            "summary": "Human organs are composed of interconnected substructures whose geometry and spatial relationships constrain one another. Yet, most deep-learning approaches treat these parts independently, producing anatomically implausible reconstructions. We introduce PrIntMesh, a template-based, topology-preserving framework that reconstructs organs as unified systems. Starting from a connected template, PrIntMesh jointly deforms all substructures to match patient-specific anatomy, while explicitly preserving internal boundaries and enforcing smooth, artifact-free surfaces. We demonstrate its effectiveness on the heart, hippocampus, and lungs, achieving high geometric accuracy, correct topology, and robust performance even with limited or noisy training data. Compared to voxel- and surface-based methods, PrIntMesh better reconstructs shared interfaces, maintains structural consistency, and provides a data-efficient solution suitable for clinical use.",
            "headline_zh": "提出PrIntMesh框架以解决器官3D网格重建中的解剖结构不一致问题",
            "intro_zh": [
                "核心问题：现有方法独立处理器官子结构，导致解剖学上不合理的重建结果",
                "方法要点：基于模板联合变形所有子结构，保持内部边界和光滑表面",
                "实验或效果：在心脏、海马体和肺部实现高几何精度和拓扑正确性"
            ],
            "tags_zh": [
                "3D器官重建",
                "模板变形",
                "拓扑保持",
                "解剖结构约束",
                "深度学习框架"
            ],
            "_index": 81
        },
        {
            "title": "Domain-Shared Learning and Gradual Alignment for Unsupervised Domain Adaptation Visible-Infrared Person Re-Identification",
            "authors": [
                "Nianchang Huang",
                "Yi Xu",
                "Ruida Xi",
                "Ruida Xi",
                "Qiang Zhang"
            ],
            "arxiv_id": "2511.16184v1",
            "summary": "Recently, Visible-Infrared person Re-Identification (VI-ReID) has achieved remarkable performance on public datasets. However, due to the discrepancies between public datasets and real-world data, most existing VI-ReID algorithms struggle in real-life applications. To address this, we take the initiative to investigate Unsupervised Domain Adaptation Visible-Infrared person Re-Identification (UDA-VI-ReID), aiming to transfer the knowledge learned from the public data to real-world data without compromising accuracy and requiring the annotation of new samples. Specifically, we first analyze two basic challenges in UDA-VI-ReID, i.e., inter-domain modality discrepancies and intra-domain modality discrepancies. Then, we design a novel two-stage model, i.e., Domain-Shared Learning and Gradual Alignment (DSLGA), to handle these discrepancies. In the first pre-training stage, DSLGA introduces a Domain-Shared Learning Strategy (DSLS) to mitigate ineffective pre-training caused by inter-domain modality discrepancies via exploiting shared information between the source and target domains. While, in the second fine-tuning stage, DSLGA designs a Gradual Alignment Strategy (GAS) to handle the cross-modality alignment challenges between visible and infrared data caused by the large intra-domain modality discrepancies through a cluster-to-holistic alignment way. Finally, a new UDA-VI-ReID testing method i.e., CMDA-XD, is constructed for training and testing different UDA-VI-ReID models. A large amount of experiments demonstrate that our method significantly outperforms existing domain adaptation methods for VI-ReID and even some supervised methods under various settings.",
            "headline_zh": "提出DSLGA模型以解决无监督域自适应可见光-红外行人重识别中的模态差异问题",
            "intro_zh": [
                "核心问题：可见光与红外数据间存在域间和域内模态差异，影响模型泛化。",
                "方法要点：采用两阶段策略，先预训练共享信息，再逐步对齐跨模态数据。",
                "实验或效果：在多种设置下显著优于现有域自适应方法，甚至部分监督方法。"
            ],
            "tags_zh": [
                "无监督域自适应",
                "可见光-红外行人重识别",
                "模态差异对齐",
                "两阶段学习",
                "跨模态对齐"
            ],
            "_index": 82
        },
        {
            "title": "FOOTPASS: A Multi-Modal Multi-Agent Tactical Context Dataset for Play-by-Play Action Spotting in Soccer Broadcast Videos",
            "authors": [
                "Jeremie Ochin",
                "Raphael Chekroun",
                "Bogdan Stanciulescu",
                "Sotiris Manitsaris"
            ],
            "arxiv_id": "2511.16183v1",
            "summary": "Soccer video understanding has motivated the creation of datasets for tasks such as temporal action localization, spatiotemporal action detection (STAD), or multiobject tracking (MOT). The annotation of structured sequences of events (who does what, when, and where) used for soccer analytics requires a holistic approach that integrates both STAD and MOT. However, current action recognition methods remain insufficient for constructing reliable play-by-play data and are typically used to assist rather than fully automate annotation. Parallel research has advanced tactical modeling, trajectory forecasting, and performance analysis, all grounded in game-state and play-by-play data. This motivates leveraging tactical knowledge as a prior to support computer-vision-based predictions, enabling more automated and reliable extraction of play-by-play data. We introduce Footovision Play-by-Play Action Spotting in Soccer Dataset (FOOTPASS), the first benchmark for play-by-play action spotting over entire soccer matches in a multi-modal, multi-agent tactical context. It enables the development of methods for player-centric action spotting that exploit both outputs from computer-vision tasks (e.g., tracking, identification) and prior knowledge of soccer, including its tactical regularities over long time horizons, to generate reliable play-by-play data streams. These streams form an essential input for data-driven sports analytics.",
            "headline_zh": "提出FOOTPASS数据集以支持足球广播视频中多模态多智能体战术背景下的逐场动作识别",
            "intro_zh": [
                "核心问题：现有动作识别方法不足以可靠自动化生成足球逐场数据，需结合战术知识。",
                "方法要点：集成计算机视觉任务输出与足球战术先验知识，实现玩家中心动作识别。",
                "实验或效果：未知具体实验细节，但旨在开发可靠逐场数据流，支持体育分析。"
            ],
            "tags_zh": [
                "足球视频理解",
                "多模态动作识别",
                "多智能体跟踪",
                "战术建模",
                "逐场数据生成"
            ],
            "_index": 83
        },
        {
            "title": "Mantis: A Versatile Vision-Language-Action Model with Disentangled Visual Foresight",
            "authors": [
                "Yi Yang",
                "Xueqi Li",
                "Yiyang Chen",
                "Jin Song",
                "Yihan Wang",
                "Zipeng Xiao",
                "Jiadi Su",
                "You Qiaoben",
                "Pengfei Liu",
                "Zhijie Deng"
            ],
            "arxiv_id": "2511.16175v1",
            "summary": "Recent advances in Vision-Language-Action (VLA) models demonstrate that visual signals can effectively complement sparse action supervisions. However, letting VLA directly predict high-dimensional visual states can distribute model capacity and incur prohibitive training cost, while compressing visual states into more compact supervisory signals inevitably incurs information bottlenecks. Moreover, existing methods often suffer from poor comprehension and reasoning capabilities due to the neglect of language supervision. This paper introduces Mantis, a novel framework featuring a Disentangled Visual Foresight (DVF) to tackle these issues. Specifically, Mantis decouples visual foresight prediction from the backbone with the combination of meta queries and a diffusion Transformer (DiT) head. With the current visual state provided to the DiT via a residual connection, a simple next-state prediction objective enables the meta queries to automatically capture the latent actions that delineate the visual trajectory, and hence boost the learning of explicit actions. The disentanglement reduces the burden of the VLA backbone, enabling it to maintain comprehension and reasoning capabilities through language supervision. Empirically, pretrained on human manipulation videos, robot demonstrations, and image-text pairs, Mantis achieves a 96.7% success rate on LIBERO benchmark after fine-tuning, surpassing powerful baselines while exhibiting high convergence speed. Real-world evaluations show that Mantis outperforms $π_{0.5}$, a leading open-source VLA model, particularly in instruction-following capability, generalization to unseen instructions, and reasoning ability. Code and weights are released to support the open-source community.",
            "headline_zh": "提出Mantis模型，通过解耦视觉预测解决VLA模型训练成本高与推理能力弱的问题。",
            "intro_zh": [
                "核心问题：VLA模型直接预测高维视觉状态导致训练成本高，压缩视觉信号引发信息瓶颈，且忽视语言监督削弱推理能力。",
                "方法要点：引入解耦视觉预测，使用元查询和扩散Transformer头，结合残差连接自动捕获潜在动作以增强显式动作学习。",
                "实验或效果：在LIBERO基准上微调后成功率96.7%，超越基线，指令跟随、泛化和推理能力优于开源模型π0.5。"
            ],
            "tags_zh": [
                "视觉语言动作模型",
                "解耦视觉预测",
                "扩散Transformer",
                "元查询学习",
                "机器人操作",
                "指令跟随能力"
            ],
            "_index": 84
        },
        {
            "title": "Target Refocusing via Attention Redistribution for Open-Vocabulary Semantic Segmentation: An Explainability Perspective",
            "authors": [
                "Jiahao Li",
                "Yang Lu",
                "Yachao Zhang",
                "Yong Xie",
                "Fangyong Wang",
                "Yuan Xie",
                "Yanyun Qu"
            ],
            "arxiv_id": "2511.16170v1",
            "summary": "Open-vocabulary semantic segmentation (OVSS) employs pixel-level vision-language alignment to associate category-related prompts with corresponding pixels. A key challenge is enhancing the multimodal dense prediction capability, specifically this pixel-level multimodal alignment. Although existing methods achieve promising results by leveraging CLIP's vision-language alignment, they rarely investigate the performance boundaries of CLIP for dense prediction from an interpretability mechanisms perspective. In this work, we systematically investigate CLIP's internal mechanisms and identify a critical phenomenon: analogous to human distraction, CLIP diverts significant attention resources from target regions to irrelevant tokens. Our analysis reveals that these tokens arise from dimension-specific over-activation; filtering them enhances CLIP's dense prediction performance. Consequently, we propose ReFocusing CLIP (RF-CLIP), a training-free approach that emulates human distraction-refocusing behavior to redirect attention from distraction tokens back to target regions, thereby refining CLIP's multimodal alignment granularity. Our method achieves SOTA performance on eight benchmarks while maintaining high inference efficiency.",
            "headline_zh": "提出RF-CLIP以解决开放词汇语义分割中CLIP注意力分散问题",
            "intro_zh": [
                "核心问题：CLIP在密集预测中注意力分散，资源浪费于无关token",
                "方法要点：训练无关方法，重定向注意力至目标区域，提升对齐粒度",
                "实验或效果：在八个基准上实现SOTA性能，保持高效推理"
            ],
            "tags_zh": [
                "开放词汇语义分割",
                "注意力机制",
                "CLIP模型",
                "密集预测",
                "多模态对齐"
            ],
            "_index": 85
        },
        {
            "title": "EvoVLA: Self-Evolving Vision-Language-Action Model",
            "authors": [
                "Zeting Liu",
                "Zida Yang",
                "Zeyu Zhang",
                "Hao Tang"
            ],
            "arxiv_id": "2511.16166v1",
            "summary": "Long-horizon robotic manipulation remains challenging for Vision-Language-Action (VLA) models despite recent progress in zero-shot generalization and simulation-to-real-world transfer. Current VLA models suffer from stage hallucination, where agents exploit coarse evaluation signals to shortcut multi-step tasks, reporting high progress without truly completing them. We present EvoVLA, a self-supervised VLA framework that addresses this issue through three complementary components: Stage-Aligned Reward (SAR), which uses triplet contrastive learning with Gemini-generated hard negatives to prevent visual shortcuts; Pose-Based Object Exploration (POE), which grounds curiosity in relative object-gripper pose instead of raw pixels; and Long-Horizon Memory, which uses selective context retention and gated fusion to stabilize intrinsic shaping during extended rollouts. Extensive evaluations on Discoverse-L, a long-horizon manipulation benchmark with three multi-stage tasks, show that EvoVLA improves average task success by 10.2 percentage points over the strongest baseline (OpenVLA-OFT), reaching 69.2 percent. EvoVLA also achieves one-and-a-half times better sample efficiency and reduces stage hallucination from 38.5 percent to 14.8 percent. Real-world deployment on physical robots reaches an average success rate of 54.6 percent across four manipulation tasks, outperforming OpenVLA-OFT by 11 points, demonstrating effective sim-to-real transfer and strong generalization. Code: https://github.com/AIGeeksGroup/EvoVLA. Website: https://aigeeksgroup.github.io/EvoVLA.",
            "headline_zh": "提出EvoVLA自进化视觉-语言-动作模型，解决长程机器人操作中的阶段幻觉问题。",
            "intro_zh": [
                "核心问题：VLA模型在长程操作中易出现阶段幻觉，利用粗评估信号走捷径。",
                "方法要点：结合阶段对齐奖励、姿态对象探索和长程记忆，提升任务完成真实性。",
                "实验或效果：在Discoverse-L基准上，任务成功率提升10.2个百分点，达69.2%。"
            ],
            "tags_zh": [
                "视觉-语言-动作模型",
                "长程机器人操作",
                "阶段幻觉",
                "自监督学习",
                "模拟到真实迁移"
            ],
            "_index": 86
        },
        {
            "title": "An Image Is Worth Ten Thousand Words: Verbose-Text Induction Attacks on VLMs",
            "authors": [
                "Zhi Luo",
                "Zenghui Yuan",
                "Wenqi Wei",
                "Daizong Liu",
                "Pan Zhou"
            ],
            "arxiv_id": "2511.16163v1",
            "summary": "With the remarkable success of Vision-Language Models (VLMs) on multimodal tasks, concerns regarding their deployment efficiency have become increasingly prominent. In particular, the number of tokens consumed during the generation process has emerged as a key evaluation metric.Prior studies have shown that specific inputs can induce VLMs to generate lengthy outputs with low information density, which significantly increases energy consumption, latency, and token costs. However, existing methods simply delay the occurrence of the EOS token to implicitly prolong output, and fail to directly maximize the output token length as an explicit optimization objective, lacking stability and controllability.To address these limitations, this paper proposes a novel verbose-text induction attack (VTIA) to inject imperceptible adversarial perturbations into benign images via a two-stage framework, which identifies the most malicious prompt embeddings for optimizing and maximizing the output token of the perturbed images.Specifically, we first perform adversarial prompt search, employing reinforcement learning strategies to automatically identify adversarial prompts capable of inducing the LLM component within VLMs to produce verbose outputs. We then conduct vision-aligned perturbation optimization to craft adversarial examples on input images, maximizing the similarity between the perturbed image's visual embeddings and those of the adversarial prompt, thereby constructing malicious images that trigger verbose text generation. Comprehensive experiments on four popular VLMs demonstrate that our method achieves significant advantages in terms of effectiveness, efficiency, and generalization capability.",
            "headline_zh": "提出冗长文本诱导攻击以优化视觉语言模型输出长度，提升攻击稳定性。",
            "intro_zh": [
                "核心问题：现有方法无法直接最大化视觉语言模型输出令牌长度，缺乏稳定性和可控性。",
                "方法要点：采用两阶段框架，结合对抗提示搜索和视觉对齐扰动优化，注入不可察觉扰动。",
                "实验或效果：在四种流行视觉语言模型上验证，方法在有效性、效率和泛化能力上优势显著。"
            ],
            "tags_zh": [
                "视觉语言模型",
                "对抗攻击",
                "冗长文本生成",
                "扰动优化",
                "强化学习",
                "多模态安全"
            ],
            "_index": 87
        },
        {
            "title": "Layer-wise Noise Guided Selective Wavelet Reconstruction for Robust Medical Image Segmentation",
            "authors": [
                "Yuting Lu",
                "Ziliang Wang",
                "Weixin Xu",
                "Wei Zhang",
                "Yongqiang Zhao",
                "Yang Yu",
                "Xiaohong Zhang"
            ],
            "arxiv_id": "2511.16162v1",
            "summary": "Clinical deployment requires segmentation models to stay stable under distribution shifts and perturbations. The mainstream solution is adversarial training (AT) to improve robustness; however, AT often brings a clean--robustness trade-off and high training/tuning cost, which limits scalability and maintainability in medical imaging. We propose \\emph{Layer-wise Noise-Guided Selective Wavelet Reconstruction (LNG-SWR)}. During training, we inject small, zero-mean noise at multiple layers to learn a frequency-bias prior that steers representations away from noise-sensitive directions. We then apply prior-guided selective wavelet reconstruction on the input/feature branch to achieve frequency adaptation: suppress noise-sensitive bands, enhance directional structures and shape cues, and stabilize boundary responses while maintaining spectral consistency. The framework is backbone-agnostic and adds low additional inference overhead. It can serve as a plug-in enhancement to AT and also improves robustness without AT. On CT and ultrasound datasets, under a unified protocol with PGD-$L_{\\infty}/L_{2}$ and SSAH, LNG-SWR delivers consistent gains on clean Dice/IoU and significantly reduces the performance drop under strong attacks; combining LNG-SWR with AT yields additive gains. When combined with adversarial training, robustness improves further without sacrificing clean accuracy, indicating an engineering-friendly and scalable path to robust segmentation. These results indicate that LNG-SWR provides a simple, effective, and engineering-friendly path to robust medical image segmentation in both adversarial and standard training regimes.",
            "headline_zh": "提出层间噪声引导选择性小波重建以增强医学图像分割的鲁棒性",
            "intro_zh": [
                "医学图像分割模型在分布偏移和扰动下稳定性不足，主流对抗训练存在精度-鲁棒性权衡和高成本问题",
                "方法在训练中注入层间噪声学习频率偏置先验，通过选择性小波重建抑制噪声敏感频带并增强结构特征",
                "实验在CT和超声数据集上，结合或不结合对抗训练均提升干净指标并显著降低强攻击下的性能下降"
            ],
            "tags_zh": [
                "医学图像分割",
                "鲁棒性增强",
                "小波重建",
                "频率适应",
                "对抗训练",
                "噪声注入"
            ],
            "_index": 88
        },
        {
            "title": "Simba: Towards High-Fidelity and Geometrically-Consistent Point Cloud Completion via Transformation Diffusion",
            "authors": [
                "Lirui Zhang",
                "Zhengkai Zhao",
                "Zhi Zuo",
                "Pan Gao",
                "Jie Qin"
            ],
            "arxiv_id": "2511.16161v1",
            "summary": "Point cloud completion is a fundamental task in 3D vision. A persistent challenge in this field is simultaneously preserving fine-grained details present in the input while ensuring the global structural integrity of the completed shape. While recent works leveraging local symmetry transformations via direct regression have significantly improved the preservation of geometric structure details, these methods suffer from two major limitations: (1) These regression-based methods are prone to overfitting which tend to memorize instant-specific transformations instead of learning a generalizable geometric prior. (2) Their reliance on point-wise transformation regression lead to high sensitivity to input noise, severely degrading their robustness and generalization. To address these challenges, we introduce Simba, a novel framework that reformulates point-wise transformation regression as a distribution learning problem. Our approach integrates symmetry priors with the powerful generative capabilities of diffusion models, avoiding instance-specific memorization while capturing robust geometric structures. Additionally, we introduce a hierarchical Mamba-based architecture to achieve high-fidelity upsampling. Extensive experiments across the PCN, ShapeNet, and KITTI benchmarks validate our method's state-of-the-art (SOTA) performance.",
            "headline_zh": "提出Simba框架，通过变换扩散解决点云补全中细节保留与结构一致性问题",
            "intro_zh": [
                "核心问题：回归方法易过拟合且对输入噪声敏感，影响点云补全的鲁棒性",
                "方法要点：将点变换回归转为分布学习，结合对称先验与扩散模型生成",
                "实验或效果：在PCN等基准上验证了SOTA性能，提升细节与结构完整性"
            ],
            "tags_zh": [
                "点云补全",
                "扩散模型",
                "对称变换",
                "分布学习",
                "Mamba架构",
                "几何一致性"
            ],
            "_index": 89
        },
        {
            "title": "Video2Layout: Recall and Reconstruct Metric-Grounded Cognitive Map for Spatial Reasoning",
            "authors": [
                "Yibin Huang",
                "Wang Xu",
                "Wanyue Zhang",
                "Helu Zhi",
                "Jingjing Huang",
                "Yangbin Xu",
                "Yangang Sun",
                "Conghui Zhu",
                "Tiejun Zhao"
            ],
            "arxiv_id": "2511.16160v1",
            "summary": "Spatial intelligence is a critical frontier for Multimodal Large Language Models (MLLMs), empowering them to comprehend the physical world. Drawing inspiration from human perception mechanisms, existing studies attempt to construct a coherent spatial understanding via grid-based cognitive maps from multi-frame visual inputs. However, current grid-based map methods rely on discretized raster representations, which limit the model's ability in fine-grained spatial reasoning. To overcome this limitation, we propose Video2Layout, a framework for reconstructing metric-grounded spatial layouts from video. The framework employs continuous object boundary coordinates to quantify inter-object physical distances and object size. This empowers the model with quantitative spatial computation capabilities, effectively alleviating the inherent ambiguity when describing spatial relationships in natural language. Specifically, our method comprises two core stages. First, in supervised fine-tuning stage, we construct a high-quality dataset from the AI2THOR simulator, which enables the model to learn the mapping from visual inputs to precise boundary coordinates. Subsequently, a reinforcement fine-tuning stage further enhances the model's real-world generalization capabilities. To systematically evaluate the correlation between cognitive map accuracy and image quantity, as well as how the quantity of image inputs affects spatial reasoning accuracy, we introduce QVS-Bench, a diagnostic benchmark designed to analyze the relevant mechanisms. Evaluated on QVS-Bench and mainstream spatial reasoning benchmarks, our model, V2LO-7B achieves an average improvement of 4.92% over the model trained on grid maps, validating the superiority of our method. Our code is available at https://github.com/ybrrraway/Video2Layout.",
            "headline_zh": "提出Video2Layout框架，通过连续边界坐标重建空间布局以解决细粒度空间推理问题",
            "intro_zh": [
                "核心问题：现有网格认知地图方法依赖离散栅格表示，限制细粒度空间推理能力",
                "方法要点：使用连续对象边界坐标量化物理距离和大小，结合监督和强化微调阶段",
                "实验或效果：在QVS-Bench等基准上，V2LO-7B模型平均提升4.92%，验证方法优越性"
            ],
            "tags_zh": [
                "空间推理",
                "认知地图",
                "视频理解",
                "连续坐标",
                "强化微调",
                "基准评估"
            ],
            "_index": 90
        },
        {
            "title": "MagBotSim: Physics-Based Simulation and Reinforcement Learning Environments for Magnetic Robotics",
            "authors": [
                "Lara Bergmann",
                "Cedric Grothues",
                "Klaus Neumann"
            ],
            "arxiv_id": "2511.16158v1",
            "summary": "Magnetic levitation is about to revolutionize in-machine material flow in industrial automation. Such systems are flexibly configurable and can include a large number of independently actuated shuttles (movers) that dynamically rebalance production capacity. Beyond their capabilities for dynamic transportation, these systems possess the inherent yet unexploited potential to perform manipulation. By merging the fields of transportation and manipulation into a coordinated swarm of magnetic robots (MagBots), we enable manufacturing systems to achieve significantly higher efficiency, adaptability, and compactness. To support the development of intelligent algorithms for magnetic levitation systems, we introduce MagBotSim (Magnetic Robotics Simulation): a physics-based simulation for magnetic levitation systems. By framing magnetic levitation systems as robot swarms and providing a dedicated simulation, this work lays the foundation for next generation manufacturing systems powered by Magnetic Robotics. MagBotSim's documentation, videos, experiments, and code are available at: https://ubi-coro.github.io/MagBotSim/",
            "headline_zh": "提出MagBotSim物理模拟环境以支持磁悬浮机器人智能算法开发",
            "intro_zh": [
                "核心问题：磁悬浮系统在工业自动化中运输与操作潜力未充分开发",
                "方法要点：基于物理模拟磁悬浮系统，将其建模为机器人集群",
                "实验或效果：提供代码与文档，为下一代制造系统奠定基础"
            ],
            "tags_zh": [
                "磁悬浮机器人",
                "物理模拟",
                "强化学习环境",
                "工业自动化",
                "机器人集群"
            ],
            "_index": 91
        },
        {
            "title": "Pluggable Pruning with Contiguous Layer Distillation for Diffusion Transformers",
            "authors": [
                "Jian Ma",
                "Qirong Peng",
                "Xujie Zhu",
                "Peixing Xie",
                "Chen Chen",
                "Haonan Lu"
            ],
            "arxiv_id": "2511.16156v1",
            "summary": "Diffusion Transformers (DiTs) have shown exceptional performance in image generation, yet their large parameter counts incur high computational costs, impeding deployment in resource-constrained settings. To address this, we propose Pluggable Pruning with Contiguous Layer Distillation (PPCL), a flexible structured pruning framework specifically designed for DiT architectures. First, we identify redundant layer intervals through a linear probing mechanism combined with the first-order differential trend analysis of similarity metrics. Subsequently, we propose a plug-and-play teacher-student alternating distillation scheme tailored to integrate depth-wise and width-wise pruning within a single training phase. This distillation framework enables flexible knowledge transfer across diverse pruning ratios, eliminating the need for per-configuration retraining. Extensive experiments on multiple Multi-Modal Diffusion Transformer architecture models demonstrate that PPCL achieves a 50\\% reduction in parameter count compared to the full model, with less than 3\\% degradation in key objective metrics. Notably, our method maintains high-quality image generation capabilities while achieving higher compression ratios, rendering it well-suited for resource-constrained environments. The open-source code, checkpoints for PPCL can be found at the following link: https://github.com/OPPO-Mente-Lab/Qwen-Image-Pruning.",
            "headline_zh": "提出可插拔剪枝与连续层蒸馏方法，以压缩扩散变换器参数并保持图像生成质量。",
            "intro_zh": [
                "扩散变换器参数量大，计算成本高，阻碍资源受限部署。",
                "通过线性探测和相似度趋势分析识别冗余层，结合交替蒸馏实现深度和宽度剪枝。",
                "实验显示参数减少50%，关键指标退化小于3%，代码开源。"
            ],
            "tags_zh": [
                "扩散变换器",
                "结构化剪枝",
                "知识蒸馏",
                "图像生成",
                "模型压缩",
                "资源优化"
            ],
            "_index": 92
        },
        {
            "title": "Reasoning Guided Embeddings: Leveraging MLLM Reasoning for Improved Multimodal Retrieval",
            "authors": [
                "Chunxu Liu",
                "Jiyuan Yang",
                "Ruopeng Gao",
                "Yuhan Zhu",
                "Feng Zhu",
                "Rui Zhao",
                "Limin Wang"
            ],
            "arxiv_id": "2511.16150v1",
            "summary": "Multimodal embeddings are widely used in downstream tasks such as multimodal retrieval, enabling alignment of interleaved modalities in a shared representation space. While recent studies show that Multimodal Large Language Models (MLLMs) can serve as strong embedding extractors, existing approaches treat embedding extraction as a direct encoding step, overlooking the fact that MLLMs possess the generative capability for reasoning that could be leveraged to enhance representation quality. In this work, we explore how to explicitly incorporate reasoning into the embedding process. To this end, we propose Reasoning Guided Embeddings (RGE), which preserves the generative rationale process of MLLMs and couples it with contrastive training. Our method first enables the model to perform structured rationale generation conditioned on the instruction, and then extracts representations after reasoning has unfolded. This simple design enhances the context-conditional inference signals within the embedding, leading to improved multimodal representation quality. Experiments on the MMEB benchmark show that reasoning-guided conditioning improves multimodal retrieval performance by 4.9% over the non-reasoning baseline, confirming that explicit reasoning can effectively enhance embedding quality.",
            "headline_zh": "提出推理引导嵌入方法，利用MLLM推理提升多模态检索性能",
            "intro_zh": [
                "现有方法忽视MLLM的生成推理能力，影响多模态嵌入质量",
                "结合结构化推理生成与对比训练，增强嵌入中的上下文推理信号",
                "在MMEB基准上，多模态检索性能提升4.9%，验证推理有效性"
            ],
            "tags_zh": [
                "多模态检索",
                "推理引导嵌入",
                "MLLM应用",
                "对比训练",
                "嵌入质量提升"
            ],
            "_index": 93
        },
        {
            "title": "LEGO-SLAM: Language-Embedded Gaussian Optimization SLAM",
            "authors": [
                "Sibaek Lee",
                "Seongbo Ha",
                "Kyeongsu Kang",
                "Joonyeol Choi",
                "Seungjun Tak",
                "Hyeonwoo Yu"
            ],
            "arxiv_id": "2511.16144v1",
            "summary": "Recent advances in 3D Gaussian Splatting (3DGS) have enabled Simultaneous Localization and Mapping (SLAM) systems to build photorealistic maps. However, these maps lack the open-vocabulary semantic understanding required for advanced robotic interaction. Integrating language features into SLAM remains a significant challenge, as storing high-dimensional features demands excessive memory and rendering overhead, while existing methods with static models lack adaptability for novel environments. To address these limitations, we propose LEGO-SLAM (Language-Embedded Gaussian Optimization SLAM), the first framework to achieve real-time, open-vocabulary mapping within a 3DGS-based SLAM system. At the core of our method is a scene-adaptive encoder-decoder that distills high-dimensional language embeddings into a compact 16-dimensional feature space. This design reduces the memory per Gaussian and accelerates rendering, enabling real-time performance. Unlike static approaches, our encoder adapts online to unseen scenes. These compact features also enable a language-guided pruning strategy that identifies semantic redundancy, reducing the map's Gaussian count by over 60\\% while maintaining rendering quality. Furthermore, we introduce a language-based loop detection approach that reuses these mapping features, eliminating the need for a separate detection model. Extensive experiments demonstrate that LEGO-SLAM achieves competitive mapping quality and tracking accuracy, all while providing open-vocabulary capabilities at 15 FPS.",
            "headline_zh": "提出LEGO-SLAM以在3D高斯溅射SLAM中实现实时开放词汇语义映射",
            "intro_zh": [
                "核心问题：3DGS SLAM缺乏开放词汇语义理解，且高维语言特征存储和渲染开销大。",
                "方法要点：使用场景自适应编码器将语言嵌入压缩至16维，减少内存并加速渲染。",
                "实验或效果：在15 FPS下实现竞争性映射质量和跟踪精度，高斯数量减少超60%。"
            ],
            "tags_zh": [
                "3D高斯溅射",
                "同时定位与建图",
                "语言嵌入",
                "实时映射",
                "开放词汇语义",
                "特征蒸馏"
            ],
            "_index": 94
        },
        {
            "title": "A Spatial Semantics and Continuity Perception Attention for Remote Sensing Water Body Change Detection",
            "authors": [
                "Quanqing Ma",
                "Jiaen Chen",
                "Peng Wang",
                "Yao Zheng",
                "Qingzhan Zhao",
                "Yuchen Zheng"
            ],
            "arxiv_id": "2511.16143v1",
            "summary": "Remote sensing Water Body Change Detection (WBCD) aims to detect water body surface changes from bi-temporal images of the same geographic area. Recently, the scarcity of high spatial resolution datasets for WBCD restricts its application in urban and rural regions, which require more accurate positioning. Meanwhile, previous deep learning-based methods fail to comprehensively exploit the spatial semantic and structural information in deep features in the change detection networks. To resolve these concerns, we first propose a new dataset, HSRW-CD, with a spatial resolution higher than 3 meters for WBCD. Specifically, it contains a large number of image pairs, widely covering various water body types. Besides, a Spatial Semantics and Continuity Perception (SSCP) attention module is designed to fully leverage both the spatial semantics and structure of deep features in the WBCD networks, significantly improving the discrimination capability for water body. The proposed SSCP has three components: the Multi-Semantic spatial Attention (MSA), the Structural Relation-aware Global Attention (SRGA), and the Channel-wise Self-Attention (CSA). The MSA enhances the spatial semantics of water body features and provides precise spatial semantic priors for the CSA. Then, the SRGA further extracts spatial structure to learn the spatial continuity of the water body. Finally, the CSA utilizes the spatial semantic and structural priors from the MSA and SRGA to compute the similarity across channels. Specifically designed as a plug-and-play module for water body deep features, the proposed SSCP allows integration into existing WBCD models. Numerous experiments conducted on the proposed HSRW-CD and Water-CD datasets validate the effectiveness and generalization of the SSCP. The code of this work and the HSRW-CD dataset will be accessed at https://github.com/QingMa1/SSCP.",
            "headline_zh": "提出SSCP注意力模块和HSRW-CD数据集以提升遥感水体变化检测精度",
            "intro_zh": [
                "核心问题：高分辨率遥感水体变化检测数据集稀缺，现有方法未充分利用空间语义和结构信息。",
                "方法要点：设计SSCP模块，集成多语义空间、结构关系和通道自注意力，增强特征判别能力。",
                "实验或效果：在HSRW-CD和Water-CD数据集上验证，SSCP模块有效且泛化性强。"
            ],
            "tags_zh": [
                "遥感水体变化检测",
                "空间语义注意力",
                "高分辨率数据集",
                "深度学习模型",
                "变化检测网络"
            ],
            "_index": 95
        },
        {
            "title": "Real-Time 3D Object Detection with Inference-Aligned Learning",
            "authors": [
                "Chenyu Zhao",
                "Xianwei Zheng",
                "Zimin Xia",
                "Linwei Yue",
                "Nan Xue"
            ],
            "arxiv_id": "2511.16140v1",
            "summary": "Real-time 3D object detection from point clouds is essential for dynamic scene understanding in applications such as augmented reality, robotics and navigation. We introduce a novel Spatial-prioritized and Rank-aware 3D object detection (SR3D) framework for indoor point clouds, to bridge the gap between how detectors are trained and how they are evaluated. This gap stems from the lack of spatial reliability and ranking awareness during training, which conflicts with the ranking-based prediction selection used as inference. Such a training-inference gap hampers the model's ability to learn representations aligned with inference-time behavior. To address the limitation, SR3D consists of two components tailored to the spatial nature of point clouds during training: a novel spatial-prioritized optimal transport assignment that dynamically emphasizes well-located and spatially reliable samples, and a rank-aware adaptive self-distillation scheme that adaptively injects ranking perception via a self-distillation paradigm. Extensive experiments on ScanNet V2 and SUN RGB-D show that SR3D effectively bridges the training-inference gap and significantly outperforms prior methods in accuracy while maintaining real-time speed.",
            "headline_zh": "提出SR3D框架以解决室内点云实时3D检测中的训练-推理差距问题",
            "intro_zh": [
                "核心问题：训练与推理间存在空间可靠性和排序感知差距，影响模型性能。",
                "方法要点：采用空间优先最优传输分配和排序感知自适应自蒸馏方案。",
                "实验或效果：在ScanNet V2和SUN RGB-D上精度显著提升，保持实时速度。"
            ],
            "tags_zh": [
                "3D物体检测",
                "点云处理",
                "训练-推理对齐",
                "最优传输",
                "自蒸馏",
                "实时系统"
            ],
            "_index": 96
        },
        {
            "title": "Degradation-Aware Hierarchical Termination for Blind Quality Enhancement of Compressed Video",
            "authors": [
                "Li Yu",
                "Yingbo Zhao",
                "Shiyu Wu",
                "Siyue Yu",
                "Moncef Gabbouj",
                "Qingshan Liu"
            ],
            "arxiv_id": "2511.16137v1",
            "summary": "Existing studies on Quality Enhancement for Compressed Video (QECV) predominantly rely on known Quantization Parameters (QPs), employing distinct enhancement models per QP setting, termed non-blind methods. However, in real-world scenarios involving transcoding or transmission, QPs may be partially or entirely unknown, limiting the applicability of such approaches and motivating the development of blind QECV techniques. Current blind methods generate degradation vectors via classification models with cross-entropy loss, using them as channel attention to guide artifact removal. However, these vectors capture only global degradation information and lack spatial details, hindering adaptation to varying artifact patterns at different spatial positions. To address these limitations, we propose a pretrained Degradation Representation Learning (DRL) module that decouples and extracts high-dimensional, multiscale degradation representations from video content to guide the artifact removal. Additionally, both blind and non-blind methods typically employ uniform architectures across QPs, hence, overlooking the varying computational demands inherent to different compression levels. We thus introduce a hierarchical termination mechanism that dynamically adjusts the number of artifact reduction stages based on the compression level. Experimental results demonstrate that the proposed approach significantly enhances performance, achieving a PSNR improvement of 110% (from 0.31 dB to 0.65 dB) over a competing state-of-the-art blind method at QP = 22. Furthermore, the proposed hierarchical termination mechanism reduces the average inference time at QP = 22 by half compared to QP = 42.",
            "headline_zh": "提出退化感知分层终止机制以解决压缩视频盲质量增强中退化信息不足和计算效率低的问题",
            "intro_zh": [
                "核心问题：现有盲方法依赖全局退化向量，缺乏空间细节，且忽略不同QP的计算需求差异。",
                "方法要点：引入预训练退化表示学习模块提取多尺度退化表示，并设计分层终止机制动态调整处理阶段。",
                "实验或效果：在QP=22时PSNR提升110%，推理时间在QP=22比QP=42减少一半。"
            ],
            "tags_zh": [
                "压缩视频质量增强",
                "盲方法",
                "退化表示学习",
                "分层终止机制",
                "计算效率优化",
                "多尺度表示"
            ],
            "_index": 97
        },
        {
            "title": "How Noise Benefits AI-generated Image Detection",
            "authors": [
                "Jiazhen Yan",
                "Ziqiang Li",
                "Fan Wang",
                "Kai Zeng",
                "Zhangjie Fu"
            ],
            "arxiv_id": "2511.16136v1",
            "summary": "The rapid advancement of generative models has made real and synthetic images increasingly indistinguishable. Although extensive efforts have been devoted to detecting AI-generated images, out-of-distribution generalization remains a persistent challenge. We trace this weakness to spurious shortcuts exploited during training and we also observe that small feature-space perturbations can mitigate shortcut dominance. To address this problem in a more controllable manner, we propose the Positive-Incentive Noise for CLIP (PiN-CLIP), which jointly trains a noise generator and a detection network under a variational positive-incentive principle. Specifically, we construct positive-incentive noise in the feature space via cross-attention fusion of visual and categorical semantic features. During optimization, the noise is injected into the feature space to fine-tune the visual encoder, suppressing shortcut-sensitive directions while amplifying stable forensic cues, thereby enabling the extraction of more robust and generalized artifact representations. Comparative experiments are conducted on an open-world dataset comprising synthetic images generated by 42 distinct generative models. Our method achieves new state-of-the-art performance, with notable improvements of 5.4 in average accuracy over existing approaches.",
            "headline_zh": "提出PiN-CLIP方法以解决AI生成图像检测的泛化问题",
            "intro_zh": [
                "核心问题：AI生成图像检测模型在分布外泛化上存在困难，源于训练中的伪捷径。",
                "方法要点：通过变分正激励原则联合训练噪声生成器和检测网络，注入特征空间噪声以抑制捷径。",
                "实验或效果：在42种生成模型的开放世界数据集上，平均准确率提升5.4，达到新SOTA。"
            ],
            "tags_zh": [
                "AI生成图像检测",
                "特征空间噪声",
                "泛化能力",
                "CLIP模型",
                "变分优化"
            ],
            "_index": 98
        },
        {
            "title": "Heterogeneous Stroke: Using Unique Vibration Cues to Improve the Wrist-Worn Spatiotemporal Tactile Display",
            "authors": [
                "Taejun Kim",
                "Youngbo Aram Shim",
                "Geehyuk Lee"
            ],
            "arxiv_id": "2511.16133v1",
            "summary": "Beyond a simple notification of incoming calls or messages, more complex information such as alphabets and digits can be delivered through spatiotemporal tactile patterns (STPs) on a wrist-worn tactile display (WTD) with multiple tactors. However, owing to the limited skin area and spatial acuity of the wrist, frequent confusions occur between closely located tactors, resulting in a low recognition accuracy. Furthermore, the accuracies reported in previous studies have mostly been measured for a specific posture and could further decrease with free arm postures in real life. Herein, we present Heterogeneous Stroke, a design concept for improving the recognition accuracy of STPs on a WTD. By assigning unique vibrotactile stimuli to each tactor, the confusion between tactors can be reduced. Through our implementation of Heterogeneous Stroke, the alphanumeric characters could be delivered with high accuracy (93.8% for 26 alphabets and 92.4% for 10 digits) across different arm postures.",
            "headline_zh": "提出Heterogeneous Stroke设计概念，通过独特振动提示提升腕戴触觉显示器的识别准确率。",
            "intro_zh": [
                "核心问题：腕戴触觉显示器因皮肤区域有限和空间敏锐度低，导致相邻触觉器间混淆，识别准确率低。",
                "方法要点：为每个触觉器分配独特振动刺激，减少混淆，提高信息传递准确性。",
                "实验或效果：实现高准确率（字母93.8%，数字92.4%），并在不同手臂姿势下保持稳定。"
            ],
            "tags_zh": [
                "腕戴触觉显示器",
                "时空触觉模式",
                "振动触觉",
                "识别准确率",
                "人机交互"
            ],
            "_index": 99
        },
        {
            "title": "VTinker: Guided Flow Upsampling and Texture Mapping for High-Resolution Video Frame Interpolation",
            "authors": [
                "Chenyang Wu",
                "Jiayi Fu",
                "Chun-Le Guo",
                "Shuhao Han",
                "Chongyi Li"
            ],
            "arxiv_id": "2511.16124v1",
            "summary": "Due to large pixel movement and high computational cost, estimating the motion of high-resolution frames is challenging. Thus, most flow-based Video Frame Interpolation (VFI) methods first predict bidirectional flows at low resolution and then use high-magnification upsampling (e.g., bilinear) to obtain the high-resolution ones. However, this kind of upsampling strategy may cause blur or mosaic at the flows' edges. Additionally, the motion of fine pixels at high resolution cannot be adequately captured in motion estimation at low resolution, which leads to the misalignment of task-oriented flows. With such inaccurate flows, input frames are warped and combined pixel-by-pixel, resulting in ghosting and discontinuities in the interpolated frame. In this study, we propose a novel VFI pipeline, VTinker, which consists of two core components: guided flow upsampling (GFU) and Texture Mapping. After motion estimation at low resolution, GFU introduces input frames as guidance to alleviate the blurring details in bilinear upsampling flows, which makes flows' edges clearer. Subsequently, to avoid pixel-level ghosting and discontinuities, Texture Mapping generates an initial interpolated frame, referred to as the intermediate proxy. The proxy serves as a cue for selecting clear texture blocks from the input frames, which are then mapped onto the proxy to facilitate producing the final interpolated frame via a reconstruction module. Extensive experiments demonstrate that VTinker achieves state-of-the-art performance in VFI. Codes are available at: https://github.com/Wucy0519/VTinker.",
            "headline_zh": "提出VTinker通过引导流上采样和纹理映射解决高分辨率视频帧插值中的模糊和伪影问题",
            "intro_zh": [
                "核心问题：高分辨率视频帧插值中，低分辨率运动估计导致流模糊和像素错位，引发插值帧的鬼影和不连续",
                "方法要点：使用引导流上采样以输入帧为参考优化流边缘，结合纹理映射生成中间代理并映射清晰纹理块",
                "实验或效果：广泛实验显示VTinker在视频帧插值任务中达到最先进性能，代码已开源"
            ],
            "tags_zh": [
                "视频帧插值",
                "引导流上采样",
                "纹理映射",
                "高分辨率视频处理",
                "运动估计优化"
            ],
            "_index": 100
        },
        {
            "title": "Decoupling Complexity from Scale in Latent Diffusion Model",
            "authors": [
                "Tianxiong Zhong",
                "Xingye Tian",
                "Xuebo Wang",
                "Boyuan Jiang",
                "Xin Tao",
                "Pengfei Wan"
            ],
            "arxiv_id": "2511.16117v1",
            "summary": "Existing latent diffusion models typically couple scale with content complexity, using more latent tokens to represent higher-resolution images or higher-frame rate videos. However, the latent capacity required to represent visual data primarily depends on content complexity, with scale serving only as an upper bound. Motivated by this observation, we propose DCS-LDM, a novel paradigm for visual generation that decouples information complexity from scale. DCS-LDM constructs a hierarchical, scale-independent latent space that models sample complexity through multi-level tokens and supports decoding to arbitrary resolutions and frame rates within a fixed latent representation. This latent space enables DCS-LDM to achieve a flexible computation-quality tradeoff. Furthermore, by decomposing structural and detailed information across levels, DCS-LDM supports a progressive coarse-to-fine generation paradigm. Experimental results show that DCS-LDM delivers performance comparable to state-of-the-art methods while offering flexible generation across diverse scales and visual qualities.",
            "headline_zh": "提出DCS-LDM以解耦视觉生成中信息复杂度与尺度，实现灵活计算-质量权衡",
            "intro_zh": [
                "现有潜在扩散模型将内容复杂度与尺度耦合，导致潜在表示效率低下",
                "构建分层尺度无关潜在空间，通过多级令牌建模复杂度，支持任意分辨率解码",
                "实验显示性能媲美先进方法，支持多尺度高质量生成和渐进式粗到细生成"
            ],
            "tags_zh": [
                "潜在扩散模型",
                "视觉生成",
                "多尺度解码",
                "分层潜在空间",
                "计算-质量权衡",
                "渐进式生成"
            ],
            "_index": 101
        },
        {
            "title": "Clustered Error Correction with Grouped 4D Gaussian Splatting",
            "authors": [
                "Taeho Kang",
                "Jaeyeon Park",
                "Kyungjin Lee",
                "Youngki Lee"
            ],
            "arxiv_id": "2511.16112v1",
            "summary": "Existing 4D Gaussian Splatting (4DGS) methods struggle to accurately reconstruct dynamic scenes, often failing to resolve ambiguous pixel correspondences and inadequate densification in dynamic regions. We address these issues by introducing a novel method composed of two key components: (1) Elliptical Error Clustering and Error Correcting Splat Addition that pinpoints dynamic areas to improve and initialize fitting splats, and (2) Grouped 4D Gaussian Splatting that improves consistency of mapping between splats and represented dynamic objects. Specifically, we classify rendering errors into missing-color and occlusion types, then apply targeted corrections via backprojection or foreground splitting guided by cross-view color consistency. Evaluations on Neural 3D Video and Technicolor datasets demonstrate that our approach significantly improves temporal consistency and achieves state-of-the-art perceptual rendering quality, improving 0.39dB of PSNR on the Technicolor Light Field dataset. Our visualization shows improved alignment between splats and dynamic objects, and the error correction method's capability to identify errors and properly initialize new splats. Our implementation details and source code are available at https://github.com/tho-kn/cem-4dgs.",
            "headline_zh": "提出聚类误差校正与分组4D高斯泼溅以改进动态场景重建",
            "intro_zh": [
                "现有4DGS方法在动态场景重建中难以处理像素对应模糊和动态区域密度不足问题",
                "引入椭圆误差聚类和误差校正泼溅添加，结合分组4DGS提升泼溅与动态对象映射一致性",
                "在Neural 3D Video和Technicolor数据集上验证，PSNR提升0.39dB，增强时间一致性和渲染质量"
            ],
            "tags_zh": [
                "4D高斯泼溅",
                "动态场景重建",
                "误差聚类",
                "时间一致性",
                "渲染质量",
                "像素对应"
            ],
            "_index": 102
        },
        {
            "title": "T2T-VICL: Unlocking the Boundaries of Cross-Task Visual In-Context Learning via Implicit Text-Driven VLMs",
            "authors": [
                "Shao-Jun Xia",
                "Huixin Zhang",
                "Zhengzhong Tu"
            ],
            "arxiv_id": "2511.16107v1",
            "summary": "In large language models (LLM), in-context learning (ICL) refers to performing new tasks by conditioning on small demonstrations provided in the input context. Recent advances in visual in-context learning (VICL) demonstrate promising capabilities for solving downstream tasks by unified vision-language models (VLMs). When the visual prompt and the target images originate from different visual tasks, can VLMs still enable VICL? In the paper, we propose a fully collaborative pipeline, i.e. T2T-VICL, for VLMs to investigate the potential of cross-task VICL. Fundamentally, we design a mechanism to generate and select text prompts that best implicitly describe the differences between two distinct low-level vision tasks, and construct the first cross-task VICL dataset. Building upon this, we propose a novel inference framework that combines perceptual score-based reasoning with traditional evaluation metrics to perform cross-task VICL. Our approach achieves top-tier results across nine cross-task scenarios and second-tier performance in ten additional scenarios, unlocking the boundaries of cross-task VICL within VLMs.",
            "headline_zh": "提出T2T-VICL以解决视觉语言模型跨任务视觉上下文学习问题",
            "intro_zh": [
                "核心问题：视觉语言模型能否在不同视觉任务间实现上下文学习",
                "方法要点：设计文本提示生成与选择机制，构建跨任务数据集",
                "实验或效果：在多个跨任务场景中取得领先或次优性能"
            ],
            "tags_zh": [
                "视觉上下文学习",
                "跨任务学习",
                "视觉语言模型",
                "文本提示生成",
                "推理框架"
            ],
            "_index": 103
        },
        {
            "title": "Rad-GS: Radar-Vision Integration for 3D Gaussian Splatting SLAM in Outdoor Environments",
            "authors": [
                "Renxiang Xiao",
                "Wei Liu",
                "Yuanfan Zhang",
                "Yushuai Chen",
                "Jinming Chen",
                "Zilu Wang",
                "Liang Hu"
            ],
            "arxiv_id": "2511.16091v1",
            "summary": "We present Rad-GS, a 4D radar-camera SLAM system designed for kilometer-scale outdoor environments, utilizing 3D Gaussian as a differentiable spatial representation. Rad-GS combines the advantages of raw radar point cloud with Doppler information and geometrically enhanced point cloud to guide dynamic object masking in synchronized images, thereby alleviating rendering artifacts and improving localization accuracy. Additionally, unsynchronized image frames are leveraged to globally refine the 3D Gaussian representation, enhancing texture consistency and novel view synthesis fidelity. Furthermore, the global octree structure coupled with a targeted Gaussian primitive management strategy further suppresses noise and significantly reduces memory consumption in large-scale environments. Extensive experiments and ablation studies demonstrate that Rad-GS achieves performance comparable to traditional 3D Gaussian methods based on camera or LiDAR inputs, highlighting the feasibility of robust outdoor mapping using 4D mmWave radar. Real-world reconstruction at kilometer scale validates the potential of Rad-GS for large-scale scene reconstruction.",
            "headline_zh": "提出Rad-GS系统，集成雷达与视觉实现户外大规模3D高斯SLAM",
            "intro_zh": [
                "核心问题：户外大规模环境中动态物体导致渲染伪影和定位精度下降",
                "方法要点：利用雷达点云和多普勒信息引导动态掩码，结合非同步图像优化3D高斯表示",
                "实验或效果：在千米级真实场景中验证，性能媲美相机或LiDAR方法，减少内存消耗"
            ],
            "tags_zh": [
                "雷达视觉融合",
                "3D高斯SLAM",
                "动态物体掩码",
                "大规模场景重建",
                "内存优化"
            ],
            "_index": 104
        },
        {
            "title": "SpectralTrain: A Universal Framework for Hyperspectral Image Classification",
            "authors": [
                "Meihua Zhou",
                "Liping Yu",
                "Jiawei Cai",
                "Wai Kin Fung",
                "Ruiguo Hu",
                "Jiarui Zhao",
                "Wenzhuo Liu",
                "Nan Wan"
            ],
            "arxiv_id": "2511.16084v1",
            "summary": "Hyperspectral image (HSI) classification typically involves large-scale data and computationally intensive training, which limits the practical deployment of deep learning models in real-world remote sensing tasks. This study introduces SpectralTrain, a universal, architecture-agnostic training framework that enhances learning efficiency by integrating curriculum learning (CL) with principal component analysis (PCA)-based spectral downsampling. By gradually introducing spectral complexity while preserving essential information, SpectralTrain enables efficient learning of spectral -- spatial patterns at significantly reduced computational costs. The framework is independent of specific architectures, optimizers, or loss functions and is compatible with both classical and state-of-the-art (SOTA) models. Extensive experiments on three benchmark datasets -- Indian Pines, Salinas-A, and the newly introduced CloudPatch-7 -- demonstrate strong generalization across spatial scales, spectral characteristics, and application domains. The results indicate consistent reductions in training time by 2-7x speedups with small-to-moderate accuracy deltas depending on backbone. Its application to cloud classification further reveals potential in climate-related remote sensing, emphasizing training strategy optimization as an effective complement to architectural design in HSI models. Code is available at https://github.com/mh-zhou/SpectralTrain.",
            "headline_zh": "提出SpectralTrain框架以解决高光谱图像分类的计算效率问题",
            "intro_zh": [
                "高光谱图像分类面临大规模数据和计算密集型训练挑战",
                "集成课程学习与PCA降维，逐步引入光谱复杂性以提升学习效率",
                "实验显示训练时间减少2-7倍，在多个数据集上验证泛化能力"
            ],
            "tags_zh": [
                "高光谱图像分类",
                "课程学习",
                "PCA降维",
                "训练效率优化",
                "遥感应用"
            ],
            "_index": 105
        },
        {
            "title": "VideoSeg-R1:Reasoning Video Object Segmentation via Reinforcement Learning",
            "authors": [
                "Zishan Xu",
                "Yifu Guo",
                "Yuquan Lu",
                "Fengyu Yang",
                "Junxin Li"
            ],
            "arxiv_id": "2511.16077v1",
            "summary": "Traditional video reasoning segmentation methods rely on supervised fine-tuning, which limits generalization to out-of-distribution scenarios and lacks explicit reasoning. To address this, we propose \\textbf{VideoSeg-R1}, the first framework to introduce reinforcement learning into video reasoning segmentation. It adopts a decoupled architecture that formulates the task as joint referring image segmentation and video mask propagation. It comprises three stages: (1) A hierarchical text-guided frame sampler to emulate human attention; (2) A reasoning model that produces spatial cues along with explicit reasoning chains; and (3) A segmentation-propagation stage using SAM2 and XMem. A task difficulty-aware mechanism adaptively controls reasoning length for better efficiency and accuracy. Extensive evaluations on multiple benchmarks demonstrate that VideoSeg-R1 achieves state-of-the-art performance in complex video reasoning and segmentation tasks. The code will be publicly available at https://github.com/euyis1019/VideoSeg-R1.",
            "headline_zh": "提出VideoSeg-R1框架，通过强化学习解决视频推理分割的泛化与显式推理问题。",
            "intro_zh": [
                "传统方法依赖监督微调，泛化能力差且缺乏显式推理。",
                "采用解耦架构，结合文本引导采样、推理链生成和分割传播。",
                "在多个基准测试中实现最先进性能，代码将开源。"
            ],
            "tags_zh": [
                "视频推理分割",
                "强化学习",
                "解耦架构",
                "显式推理链",
                "自适应推理长度"
            ],
            "_index": 106
        },
        {
            "title": "Bi-AQUA: Bilateral Control-Based Imitation Learning for Underwater Robot Arms via Lighting-Aware Action Chunking with Transformers",
            "authors": [
                "Takeru Tsunoori",
                "Masato Kobayashi",
                "Yuki Uranishi"
            ],
            "arxiv_id": "2511.16050v1",
            "summary": "Underwater robotic manipulation is fundamentally challenged by extreme lighting variations, color distortion, and reduced visibility. We introduce Bi-AQUA, the first underwater bilateral control-based imitation learning framework that integrates lighting-aware visual processing for underwater robot arms. Bi-AQUA employs a hierarchical three-level lighting adaptation mechanism: a Lighting Encoder that extracts lighting representations from RGB images without manual annotation and is implicitly supervised by the imitation objective, FiLM modulation of visual backbone features for adaptive, lighting-aware feature extraction, and an explicit lighting token added to the transformer encoder input for task-aware conditioning. Experiments on a real-world underwater pick-and-place task under diverse static and dynamic lighting conditions show that Bi-AQUA achieves robust performance and substantially outperforms a bilateral baseline without lighting modeling. Ablation studies further confirm that all three lighting-aware components are critical. This work bridges terrestrial bilateral control-based imitation learning and underwater manipulation, enabling force-sensitive autonomous operation in challenging marine environments. For additional material, please check: https://mertcookimg.github.io/bi-aqua",
            "headline_zh": "提出Bi-AQUA框架以解决水下机器人手臂在极端光照下的操作问题",
            "intro_zh": [
                "核心问题：水下操作面临光照变化、颜色失真和低可见度挑战",
                "方法要点：集成三层光照适应机制，包括光照编码器、FiLM调制和光照令牌",
                "实验或效果：在真实水下拾放任务中，性能优于无光照建模基线，组件均关键"
            ],
            "tags_zh": [
                "水下机器人操作",
                "模仿学习",
                "双边控制",
                "光照适应",
                "Transformer模型"
            ],
            "_index": 107
        },
        {
            "title": "LiSTAR: Ray-Centric World Models for 4D LiDAR Sequences in Autonomous Driving",
            "authors": [
                "Pei Liu",
                "Songtao Wang",
                "Lang Zhang",
                "Xingyue Peng",
                "Yuandong Lyu",
                "Jiaxin Deng",
                "Songxin Lu",
                "Weiliang Ma",
                "Xueyang Zhang",
                "Yifei Zhan",
                "XianPeng Lang",
                "Jun Ma"
            ],
            "arxiv_id": "2511.16049v1",
            "summary": "Synthesizing high-fidelity and controllable 4D LiDAR data is crucial for creating scalable simulation environments for autonomous driving. This task is inherently challenging due to the sensor's unique spherical geometry, the temporal sparsity of point clouds, and the complexity of dynamic scenes. To address these challenges, we present LiSTAR, a novel generative world model that operates directly on the sensor's native geometry. LiSTAR introduces a Hybrid-Cylindrical-Spherical (HCS) representation to preserve data fidelity by mitigating quantization artifacts common in Cartesian grids. To capture complex dynamics from sparse temporal data, it utilizes a Spatio-Temporal Attention with Ray-Centric Transformer (START) that explicitly models feature evolution along individual sensor rays for robust temporal coherence. Furthermore, for controllable synthesis, we propose a novel 4D point cloud-aligned voxel layout for conditioning and a corresponding discrete Masked Generative START (MaskSTART) framework, which learns a compact, tokenized representation of the scene, enabling efficient, high-resolution, and layout-guided compositional generation. Comprehensive experiments validate LiSTAR's state-of-the-art performance across 4D LiDAR reconstruction, prediction, and conditional generation, with substantial quantitative gains: reducing generation MMD by a massive 76%, improving reconstruction IoU by 32%, and lowering prediction L1 Med by 50%. This level of performance provides a powerful new foundation for creating realistic and controllable autonomous systems simulations. Project link: https://ocean-luna.github.io/LiSTAR.gitub.io.",
            "headline_zh": "提出LiSTAR模型以解决自动驾驶中4D LiDAR数据合成挑战",
            "intro_zh": [
                "核心问题：4D LiDAR数据合成因球形几何、时间稀疏性和动态场景复杂性而困难。",
                "方法要点：采用混合柱面-球面表示和射线中心变换器，提升数据保真度和时间一致性。",
                "实验或效果：在重建、预测和生成任务中表现优异，MMD降低76%，IoU提高32%。"
            ],
            "tags_zh": [
                "4D LiDAR合成",
                "生成世界模型",
                "射线中心变换器",
                "自动驾驶模拟",
                "可控生成",
                "时空注意力"
            ],
            "_index": 108
        },
        {
            "title": "Semantic Glitch: Agency and Artistry in an Autonomous Pixel Cloud",
            "authors": [
                "Qing Zhang",
                "Jing Huang",
                "Mingyang Xu",
                "Jun Rekimoto"
            ],
            "arxiv_id": "2511.16048v1",
            "summary": "While mainstream robotics pursues metric precision and flawless performance, this paper explores the creative potential of a deliberately \"lo-fi\" approach. We present the \"Semantic Glitch,\" a soft flying robotic art installation whose physical form, a 3D pixel style cloud, is a \"physical glitch\" derived from digital archaeology. We detail a novel autonomous pipeline that rejects conventional sensors like LiDAR and SLAM, relying solely on the qualitative, semantic understanding of a Multimodal Large Language Model to navigate. By authoring a bio-inspired personality for the robot through a natural language prompt, we create a \"narrative mind\" that complements the \"weak,\" historically, loaded body. Our analysis begins with a 13-minute autonomous flight log, and a follow-up study statistically validates the framework's robustness for authoring quantifiably distinct personas. The combined analysis reveals emergent behaviors, from landmark-based navigation to a compelling \"plan to execution\" gap, and a character whose unpredictable, plausible behavior stems from a lack of precise proprioception. This demonstrates a lo-fi framework for creating imperfect companions whose success is measured in character over efficiency.",
            "headline_zh": "提出基于多模态大语言模型的语义导航框架，用于创造具有个性的低精度自主飞行艺术装置。",
            "intro_zh": [
                "核心问题：探索低精度机器人如何通过语义理解而非精确传感器实现自主导航和艺术表达。",
                "方法要点：使用多模态大语言模型进行语义导航，并通过自然语言提示赋予机器人生物启发式个性。",
                "实验或效果：通过13分钟自主飞行日志和后续研究验证框架鲁棒性，展示出地标导航和行为涌现。"
            ],
            "tags_zh": [
                "语义导航",
                "多模态大语言模型",
                "自主飞行机器人",
                "艺术装置",
                "低精度框架",
                "个性建模"
            ],
            "_index": 109
        },
        {
            "title": "AMS-KV: Adaptive KV Caching in Multi-Scale Visual Autoregressive Transformers",
            "authors": [
                "Boxun Xu",
                "Yu Wang",
                "Zihu Wang",
                "Peng Li"
            ],
            "arxiv_id": "2511.16047v1",
            "summary": "Visual autoregressive modeling (VAR) via next-scale prediction has emerged as a scalable image generation paradigm. While Key and Value (KV) caching in large language models (LLMs) has been extensively studied, next-scale prediction presents unique challenges, and KV caching design for next-scale based VAR transformers remains largely unexplored. A major bottleneck is the excessive KV memory growth with the increasing number of scales-severely limiting scalability. Our systematic investigation reveals that: (1) Attending to tokens from local scales significantly contributes to generation quality (2) Allocating a small amount of memory for the coarsest scales, termed as condensed scales, stabilizes multi-scale image generation (3) Strong KV similarity across finer scales is predominantly observed in cache-efficient layers, whereas cache-demanding layers exhibit weaker inter-scale similarity. Based on the observations, we introduce AMS-KV, a scale-adaptive KV caching policy for next-scale prediction in VAR models. AMS-KV prioritizes storing KVs from condensed and local scales, preserving the most relevant tokens to maintain generation quality. It further optimizes KV cache utilization and computational efficiency identifying cache-demanding layers through inter-scale similarity analysis. Compared to the vanilla next-scale prediction-based VAR models, AMS-KV reduces KV cache usage by up to 84.83% and self-attention latency by 60.48%. Moreover, when the baseline VAR-d30 model encounters out-of-memory failures at a batch size of 128, AMS-KV enables stable scaling to a batch size of 256 with improved throughput.",
            "headline_zh": "提出AMS-KV自适应KV缓存策略以优化多尺度视觉自回归模型的扩展性",
            "intro_zh": [
                "多尺度视觉自回归模型中KV缓存内存随尺度增加而过度增长，限制扩展性",
                "基于局部尺度和凝聚尺度优先存储KV，并通过层间相似性分析优化缓存利用",
                "实验显示KV缓存使用减少84.83%，自注意力延迟降低60.48，支持更大批次生成"
            ],
            "tags_zh": [
                "视觉自回归建模",
                "KV缓存优化",
                "多尺度图像生成",
                "自注意力机制",
                "计算效率提升"
            ],
            "_index": 110
        },
        {
            "title": "LLMs-based Augmentation for Domain Adaptation in Long-tailed Food Datasets",
            "authors": [
                "Qing Wang",
                "Chong-Wah Ngo",
                "Ee-Peng Lim",
                "Qianru Sun"
            ],
            "arxiv_id": "2511.16037v1",
            "summary": "Training a model for food recognition is challenging because the training samples, which are typically crawled from the Internet, are visually different from the pictures captured by users in the free-living environment. In addition to this domain-shift problem, the real-world food datasets tend to be long-tailed distributed and some dishes of different categories exhibit subtle variations that are difficult to distinguish visually. In this paper, we present a framework empowered with large language models (LLMs) to address these challenges in food recognition. We first leverage LLMs to parse food images to generate food titles and ingredients. Then, we project the generated texts and food images from different domains to a shared embedding space to maximize the pair similarities. Finally, we take the aligned features of both modalities for recognition. With this simple framework, we show that our proposed approach can outperform the existing approaches tailored for long-tailed data distribution, domain adaptation, and fine-grained classification, respectively, on two food datasets.",
            "headline_zh": "提出基于大语言模型的框架以解决长尾食品数据集中的领域适应问题",
            "intro_zh": [
                "核心问题：食品图像存在领域偏移、长尾分布和细粒度视觉差异。",
                "方法要点：利用LLMs解析图像生成文本，对齐多模态特征于共享嵌入空间。",
                "实验或效果：在多个食品数据集上优于现有长尾、领域适应和细粒度方法。"
            ],
            "tags_zh": [
                "食品识别",
                "领域适应",
                "长尾分布",
                "多模态学习",
                "大语言模型",
                "细粒度分类"
            ],
            "_index": 111
        },
        {
            "title": "Crossmodal learning for Crop Canopy Trait Estimation",
            "authors": [
                "Timilehin T. Ayanlade",
                "Anirudha Powadi",
                "Talukder Z. Jubery",
                "Baskar Ganapathysubramanian",
                "Soumik Sarkar"
            ],
            "arxiv_id": "2511.16031v1",
            "summary": "Recent advances in plant phenotyping have driven widespread adoption of multi sensor platforms for collecting crop canopy reflectance data. This includes the collection of heterogeneous data across multiple platforms, with Unmanned Aerial Vehicles (UAV) seeing significant usage due to their high performance in crop monitoring, forecasting, and prediction tasks. Similarly, satellite missions have been shown to be effective for agriculturally relevant tasks. In contrast to UAVs, such missions are bound to the limitation of spatial resolution, which hinders their effectiveness for modern farming systems focused on micro-plot management. In this work, we propose a cross modal learning strategy that enriches high-resolution satellite imagery with UAV level visual detail for crop canopy trait estimation. Using a dataset of approximately co registered satellite UAV image pairs collected from replicated plots of 84 hybrid maize varieties across five distinct locations in the U.S. Corn Belt, we train a model that learns fine grained spectral spatial correspondences between sensing modalities. Results show that the generated UAV-like representations from satellite inputs consistently outperform real satellite imagery on multiple downstream tasks, including yield and nitrogen prediction, demonstrating the potential of cross-modal correspondence learning to bridge the gap between satellite and UAV sensing in agricultural monitoring.",
            "headline_zh": "提出跨模态学习策略，利用卫星图像生成无人机级细节以估计作物冠层性状",
            "intro_zh": [
                "卫星图像空间分辨率低，限制其在微地块管理农业中的应用",
                "方法学习卫星与无人机图像间的精细光谱空间对应关系",
                "生成图像在产量和氮预测任务中优于真实卫星图像"
            ],
            "tags_zh": [
                "跨模态学习",
                "作物冠层性状估计",
                "卫星图像增强",
                "无人机图像",
                "农业监测",
                "光谱空间对应"
            ],
            "_index": 112
        },
        {
            "title": "CuriGS: Curriculum-Guided Gaussian Splatting for Sparse View Synthesis",
            "authors": [
                "Zijian Wu",
                "Mingfeng Jiang",
                "Zidian Lin",
                "Ying Song",
                "Hanjie Ma",
                "Qun Wu",
                "Dongping Zhang",
                "Guiyang Pu"
            ],
            "arxiv_id": "2511.16030v1",
            "summary": "3D Gaussian Splatting (3DGS) has recently emerged as an efficient, high-fidelity representation for real-time scene reconstruction and rendering. However, extending 3DGS to sparse-view settings remains challenging because of supervision scarcity and overfitting caused by limited viewpoint coverage. In this paper, we present CuriGS, a curriculum-guided framework for sparse-view 3D reconstruction using 3DGS. CuriGS addresses the core challenge of sparse-view synthesis by introducing student views: pseudo-views sampled around ground-truth poses (teacher). For each teacher, we generate multiple groups of student views with different perturbation levels. During training, we follow a curriculum schedule that gradually unlocks higher perturbation level, randomly sampling candidate students from the active level to assist training. Each sampled student is regularized via depth-correlation and co-regularization, and evaluated using a multi-signal metric that combines SSIM, LPIPS, and an image-quality measure. For every teacher and perturbation level, we periodically retain the best-performing students and promote those that satisfy a predefined quality threshold to the training set, resulting in a stable augmentation of sparse training views. Experimental results show that CuriGS outperforms state-of-the-art baselines in both rendering fidelity and geometric consistency across various synthetic and real sparse-view scenes. Project page: https://zijian1026.github.io/CuriGS/",
            "headline_zh": "提出课程引导高斯泼溅以解决稀疏视图合成中的监督不足问题",
            "intro_zh": [
                "核心问题：稀疏视图下3D高斯泼溅易过拟合，监督不足。",
                "方法要点：引入学生视图，通过课程计划逐步增加扰动，正则化训练。",
                "实验效果：在合成和真实稀疏场景中，渲染保真度和几何一致性优于基线。"
            ],
            "tags_zh": [
                "稀疏视图合成",
                "3D高斯泼溅",
                "课程学习",
                "学生视图",
                "深度正则化",
                "渲染保真度"
            ],
            "_index": 113
        },
        {
            "title": "Towards a Safer and Sustainable Manufacturing Process: Material classification in Laser Cutting Using Deep Learning",
            "authors": [
                "Mohamed Abdallah Salem",
                "Hamdy Ahmed Ashur",
                "Ahmed Elshinnawy"
            ],
            "arxiv_id": "2511.16026v1",
            "summary": "Laser cutting is a widely adopted technology in material processing across various industries, but it generates a significant amount of dust, smoke, and aerosols during operation, posing a risk to both the environment and workers' health. Speckle sensing has emerged as a promising method to monitor the cutting process and identify material types in real-time. This paper proposes a material classification technique using a speckle pattern of the material's surface based on deep learning to monitor and control the laser cutting process. The proposed method involves training a convolutional neural network (CNN) on a dataset of laser speckle patterns to recognize distinct material types for safe and efficient cutting. Previous methods for material classification using speckle sensing may face issues when the color of the laser used to produce the speckle pattern is changed. Experiments conducted in this study demonstrate that the proposed method achieves high accuracy in material classification, even when the laser color is changed. The model achieved an accuracy of 98.30 % on the training set and 96.88% on the validation set. Furthermore, the model was evaluated on a set of 3000 new images for 30 different materials, achieving an F1-score of 0.9643. The proposed method provides a robust and accurate solution for material-aware laser cutting using speckle sensing.",
            "headline_zh": "提出基于深度学习的散斑模式材料分类方法，以提升激光切割的安全性和可持续性。",
            "intro_zh": [
                "激光切割产生粉尘烟雾，危害环境和工人健康，需实时材料识别。",
                "使用卷积神经网络训练散斑模式，实现材料分类，适应激光颜色变化。",
                "实验显示高准确率，验证集达96.88%，F1分数0.9643，适用于多种材料。"
            ],
            "tags_zh": [
                "材料分类",
                "激光切割",
                "散斑传感",
                "卷积神经网络",
                "工业安全",
                "深度学习"
            ],
            "_index": 114
        },
        {
            "title": "Mixture of Ranks with Degradation-Aware Routing for One-Step Real-World Image Super-Resolution",
            "authors": [
                "Xiao He",
                "Zhijun Tu",
                "Kun Cheng",
                "Mingrui Zhu",
                "Jie Hu",
                "Nannan Wang",
                "Xinbo Gao"
            ],
            "arxiv_id": "2511.16024v1",
            "summary": "The demonstrated success of sparsely-gated Mixture-of-Experts (MoE) architectures, exemplified by models such as DeepSeek and Grok, has motivated researchers to investigate their adaptation to diverse domains. In real-world image super-resolution (Real-ISR), existing approaches mainly rely on fine-tuning pre-trained diffusion models through Low-Rank Adaptation (LoRA) module to reconstruct high-resolution (HR) images. However, these dense Real-ISR models are limited in their ability to adaptively capture the heterogeneous characteristics of complex real-world degraded samples or enable knowledge sharing between inputs under equivalent computational budgets. To address this, we investigate the integration of sparse MoE into Real-ISR and propose a Mixture-of-Ranks (MoR) architecture for single-step image super-resolution. We introduce a fine-grained expert partitioning strategy that treats each rank in LoRA as an independent expert. This design enables flexible knowledge recombination while isolating fixed-position ranks as shared experts to preserve common-sense features and minimize routing redundancy. Furthermore, we develop a degradation estimation module leveraging CLIP embeddings and predefined positive-negative text pairs to compute relative degradation scores, dynamically guiding expert activation. To better accommodate varying sample complexities, we incorporate zero-expert slots and propose a degradation-aware load-balancing loss, which dynamically adjusts the number of active experts based on degradation severity, ensuring optimal computational resource allocation. Comprehensive experiments validate our framework's effectiveness and state-of-the-art performance.",
            "headline_zh": "提出混合秩架构以解决真实世界图像超分辨率中的自适应退化处理问题",
            "intro_zh": [
                "核心问题：现有真实世界图像超分辨率模型难以自适应处理复杂退化样本，计算效率低。",
                "方法要点：引入混合秩架构，将LoRA秩作为专家，结合退化感知路由动态激活专家。",
                "实验或效果：综合实验验证了框架的有效性和最先进性能。"
            ],
            "tags_zh": [
                "图像超分辨率",
                "混合专家",
                "低秩适应",
                "退化估计",
                "路由机制"
            ],
            "_index": 115
        },
        {
            "title": "Physically Realistic Sequence-Level Adversarial Clothing for Robust Human-Detection Evasion",
            "authors": [
                "Dingkun Zhou",
                "Patrick P. K. Chan",
                "Hengxu Wu",
                "Shikang Zheng",
                "Ruiqi Huang",
                "Yuanjie Zhao"
            ],
            "arxiv_id": "2511.16020v1",
            "summary": "Deep neural networks used for human detection are highly vulnerable to adversarial manipulation, creating safety and privacy risks in real surveillance environments. Wearable attacks offer a realistic threat model, yet existing approaches usually optimize textures frame by frame and therefore fail to maintain concealment across long video sequences with motion, pose changes, and garment deformation. In this work, a sequence-level optimization framework is introduced to generate natural, printable adversarial textures for shirts, trousers, and hats that remain effective throughout entire walking videos in both digital and physical settings. Product images are first mapped to UV space and converted into a compact palette and control-point parameterization, with ICC locking to keep all colors printable. A physically based human-garment pipeline is then employed to simulate motion, multi-angle camera viewpoints, cloth dynamics, and illumination variation. An expectation-over-transformation objective with temporal weighting is used to optimize the control points so that detection confidence is minimized across whole sequences. Extensive experiments demonstrate strong and stable concealment, high robustness to viewpoint changes, and superior cross-model transferability. Physical garments produced with sublimation printing achieve reliable suppression under indoor and outdoor recordings, confirming real-world feasibility.",
            "headline_zh": "提出序列级优化框架以生成可打印对抗纹理，实现长视频中人体检测的稳定规避",
            "intro_zh": [
                "问题：现有可穿戴攻击方法在长视频中因运动、姿态变化和衣物变形而难以维持隐蔽性",
                "方法：采用序列级优化，结合物理模拟和参数化纹理，最小化检测置信度",
                "效果：实验显示强隐蔽性、高鲁棒性和跨模型可迁移性，物理验证可行"
            ],
            "tags_zh": [
                "人体检测规避",
                "序列级对抗攻击",
                "物理模拟",
                "可打印纹理",
                "鲁棒性评估"
            ],
            "_index": 116
        },
        {
            "title": "Exploiting Inter-Sample Information for Long-tailed Out-of-Distribution Detection",
            "authors": [
                "Nimeshika Udayangani",
                "Hadi M. Dolatabadi",
                "Sarah Erfani",
                "Christopher Leckie"
            ],
            "arxiv_id": "2511.16015v1",
            "summary": "Detecting out-of-distribution (OOD) data is essential for safe deployment of deep neural networks (DNNs). This problem becomes particularly challenging in the presence of long-tailed in-distribution (ID) datasets, often leading to high false positive rates (FPR) and low tail-class ID classification accuracy. In this paper, we demonstrate that exploiting inter-sample relationships using a graph-based representation can significantly improve OOD detection in long-tailed recognition of vision datasets. To this end, we use the feature space of a pre-trained model to initialize our graph structure. We account for the differences between the activation layer distribution of the pre-training vs. training data, and actively introduce Gaussianization to alleviate any deviations from a standard normal distribution in the activation layers of the pre-trained model. We then refine this initial graph representation using graph convolutional networks (GCNs) to arrive at a feature space suitable for long-tailed OOD detection. This leads us to address the inferior performance observed in ID tail-classes within existing OOD detection methods. Experiments over three benchmarks CIFAR10-LT, CIFAR100-LT, and ImageNet-LT demonstrate that our method outperforms the state-of-the-art approaches by a large margin in terms of FPR and tail-class ID classification accuracy.",
            "headline_zh": "提出基于图表示的方法以解决长尾分布下的离群检测问题",
            "intro_zh": [
                "核心问题：长尾分布数据集导致离群检测假阳性率高和尾类分类精度低",
                "方法要点：利用图卷积网络优化预训练模型特征空间，引入高斯化处理分布偏差",
                "实验效果：在多个基准数据集上显著优于现有方法，降低假阳性率并提升尾类精度"
            ],
            "tags_zh": [
                "离群检测",
                "长尾分布",
                "图卷积网络",
                "高斯化处理",
                "特征空间优化"
            ],
            "_index": 117
        },
        {
            "title": "PushingBots: Collaborative Pushing via Neural Accelerated Combinatorial Hybrid Optimization",
            "authors": [
                "Zili Tang",
                "Ying Zhang",
                "Meng Guo"
            ],
            "arxiv_id": "2511.15995v1",
            "summary": "Many robots are not equipped with a manipulator and many objects are not suitable for prehensile manipulation (such as large boxes and cylinders). In these cases, pushing is a simple yet effective non-prehensile skill for robots to interact with and further change the environment. Existing work often assumes a set of predefined pushing modes and fixed-shape objects. This work tackles the general problem of controlling a robotic fleet to push collaboratively numerous arbitrary objects to respective destinations, within complex environments of cluttered and movable obstacles. It incorporates several characteristic challenges for multi-robot systems such as online task coordination under large uncertainties of cost and duration, and for contact-rich tasks such as hybrid switching among different contact modes, and under-actuation due to constrained contact forces. The proposed method is based on combinatorial hybrid optimization over dynamic task assignments and hybrid execution via sequences of pushing modes and associated forces. It consists of three main components: (I) the decomposition, ordering and rolling assignment of pushing subtasks to robot subgroups; (II) the keyframe guided hybrid search to optimize the sequence of parameterized pushing modes for each subtask; (III) the hybrid control to execute these modes and transit among them. Last but not least, a diffusion-based accelerator is adopted to predict the keyframes and pushing modes that should be prioritized during hybrid search; and further improve planning efficiency. The framework is complete under mild assumptions. Its efficiency and effectiveness under different numbers of robots and general-shaped objects are validated extensively in simulations and hardware experiments, as well as generalizations to heterogeneous robots, planar assembly and 6D pushing.",
            "headline_zh": "提出基于组合混合优化的协作推动方法，解决多机器人推动任意物体问题",
            "intro_zh": [
                "核心问题：多机器人在复杂环境中协作推动任意形状物体，处理任务协调和接触模式切换",
                "方法要点：组合优化任务分配，关键帧引导混合搜索，扩散加速器提升规划效率",
                "实验或效果：仿真和硬件验证效率，适用于异构机器人和6D推动"
            ],
            "tags_zh": [
                "多机器人协作",
                "非抓取操作",
                "组合优化",
                "混合控制",
                "扩散加速器"
            ],
            "_index": 118
        },
        {
            "title": "Fairness in Multi-modal Medical Diagnosis with Demonstration Selection",
            "authors": [
                "Dawei Li",
                "Zijian Gu",
                "Peng Wang",
                "Chuhan Song",
                "Zhen Tan",
                "Mohan Zhang",
                "Tianlong Chen",
                "Yu Tian",
                "Song Wang"
            ],
            "arxiv_id": "2511.15986v1",
            "summary": "Multimodal large language models (MLLMs) have shown strong potential for medical image reasoning, yet fairness across demographic groups remains a major concern. Existing debiasing methods often rely on large labeled datasets or fine-tuning, which are impractical for foundation-scale models. We explore In-Context Learning (ICL) as a lightweight, tuning-free alternative for improving fairness. Through systematic analysis, we find that conventional demonstration selection (DS) strategies fail to ensure fairness due to demographic imbalance in selected exemplars. To address this, we propose Fairness-Aware Demonstration Selection (FADS), which builds demographically balanced and semantically relevant demonstrations via clustering-based sampling. Experiments on multiple medical imaging benchmarks show that FADS consistently reduces gender-, race-, and ethnicity-related disparities while maintaining strong accuracy, offering an efficient and scalable path toward fair medical image reasoning. These results highlight the potential of fairness-aware in-context learning as a scalable and data-efficient solution for equitable medical image reasoning.",
            "headline_zh": "提出公平感知演示选择方法以解决多模态医疗诊断中的公平性问题",
            "intro_zh": [
                "多模态大语言模型在医疗图像推理中存在跨人口群体的公平性问题",
                "通过聚类采样构建人口平衡且语义相关的演示，实现无需调优的公平提升",
                "实验表明该方法在多个基准上减少性别、种族和民族差异，同时保持高准确率"
            ],
            "tags_zh": [
                "多模态医疗诊断",
                "公平性",
                "演示选择",
                "上下文学习",
                "聚类采样",
                "医疗图像推理"
            ],
            "_index": 119
        },
        {
            "title": "UniDGF: A Unified Detection-to-Generation Framework for Hierarchical Object Visual Recognition",
            "authors": [
                "Xinyu Nan",
                "Lingtao Mao",
                "Huangyu Dai",
                "Zexin Zheng",
                "Xinyu Sun",
                "Zihan Liang",
                "Ben Chen",
                "Yuqing Ding",
                "Chenyi Lei",
                "Wenwu Ou",
                "Han Li"
            ],
            "arxiv_id": "2511.15984v1",
            "summary": "Achieving visual semantic understanding requires a unified framework that simultaneously handles object detection, category prediction, and attribute recognition. However, current advanced approaches rely on global similarity and struggle to capture fine-grained category distinctions and category-specific attribute diversity, especially in large-scale e-commerce scenarios. To overcome these challenges, we introduce a detection-guided generative framework that predicts hierarchical category and attribute tokens. For each detected object, we extract refined ROI-level features and employ a BART-based generator to produce semantic tokens in a coarse-to-fine sequence covering category hierarchies and property-value pairs, with support for property-conditioned attribute recognition. Experiments on both large-scale proprietary e-commerce datasets and open-source datasets demonstrate that our approach significantly outperforms existing similarity-based pipelines and multi-stage classification systems, achieving stronger fine-grained recognition and more coherent unified inference.",
            "headline_zh": "提出检测引导生成框架以解决电商场景中细粒度视觉识别问题",
            "intro_zh": [
                "核心问题：现有方法依赖全局相似性，难以捕捉细粒度类别差异和属性多样性。",
                "方法要点：提取ROI特征，使用BART生成器预测层次化类别和属性序列。",
                "实验或效果：在电商和开源数据集上优于相似性方法和多阶段系统。"
            ],
            "tags_zh": [
                "目标检测",
                "生成式模型",
                "细粒度识别",
                "属性识别",
                "电商视觉",
                "统一框架"
            ],
            "_index": 120
        },
        {
            "title": "Externally Validated Multi-Task Learning via Consistency Regularization Using Differentiable BI-RADS Features for Breast Ultrasound Tumor Segmentation",
            "authors": [
                "Jingru Zhang",
                "Saed Moradi",
                "Ashirbani Saha"
            ],
            "arxiv_id": "2511.15968v1",
            "summary": "Multi-task learning can suffer from destructive task interference, where jointly trained models underperform single-task baselines and limit generalization. To improve generalization performance in breast ultrasound-based tumor segmentation via multi-task learning, we propose a novel consistency regularization approach that mitigates destructive interference between segmentation and classification. The consistency regularization approach is composed of differentiable BI-RADS-inspired morphological features. We validated this approach by training all models on the BrEaST dataset (Poland) and evaluating them on three external datasets: UDIAT (Spain), BUSI (Egypt), and BUS-UCLM (Spain). Our comprehensive analysis demonstrates statistically significant (p<0.001) improvements in generalization for segmentation task of the proposed multi-task approach vs. the baseline one: UDIAT, BUSI, BUS-UCLM (Dice coefficient=0.81 vs 0.59, 0.66 vs 0.56, 0.69 vs 0.49, resp.). The proposed approach also achieves state-of-the-art segmentation performance under rigorous external validation on the UDIAT dataset.",
            "headline_zh": "提出基于可微分BI-RADS特征的一致性正则化方法，以提升乳腺超声肿瘤分割的多任务学习泛化性能。",
            "intro_zh": [
                "多任务学习存在破坏性任务干扰，导致泛化能力下降。",
                "方法使用可微分BI-RADS形态特征进行一致性正则化，减少分割与分类任务干扰。",
                "外部验证显示分割性能显著提升，在UDIAT数据集上达到最优。"
            ],
            "tags_zh": [
                "多任务学习",
                "一致性正则化",
                "乳腺超声分割",
                "可微分特征",
                "外部验证",
                "BI-RADS特征"
            ],
            "_index": 121
        },
        {
            "title": "InfoCLIP: Bridging Vision-Language Pretraining and Open-Vocabulary Semantic Segmentation via Information-Theoretic Alignment Transfer",
            "authors": [
                "Muyao Yuan",
                "Yuanhong Zhang",
                "Weizhan Zhang",
                "Lan Ma",
                "Yuan Gao",
                "Jiangyong Ying",
                "Yudeng Xin"
            ],
            "arxiv_id": "2511.15967v1",
            "summary": "Recently, the strong generalization ability of CLIP has facilitated open-vocabulary semantic segmentation, which labels pixels using arbitrary text. However, existing methods that fine-tune CLIP for segmentation on limited seen categories often lead to overfitting and degrade the pretrained vision-language alignment. To stabilize modality alignment during fine-tuning, we propose InfoCLIP, which leverages an information-theoretic perspective to transfer alignment knowledge from pretrained CLIP to the segmentation task. Specifically, this transfer is guided by two novel objectives grounded in mutual information. First, we compress the pixel-text modality alignment from pretrained CLIP to reduce noise arising from its coarse-grained local semantic representations learned under image-text supervision. Second, we maximize the mutual information between the alignment knowledge of pretrained CLIP and the fine-tuned model to transfer compact local semantic relations suited for the segmentation task. Extensive evaluations across various benchmarks validate the effectiveness of InfoCLIP in enhancing CLIP fine-tuning for open-vocabulary semantic segmentation, demonstrating its adaptability and superiority in asymmetric transfer.",
            "headline_zh": "提出InfoCLIP以解决CLIP微调中模态对齐退化问题",
            "intro_zh": [
                "核心问题：CLIP微调于分割任务易过拟合，破坏预训练视觉语言对齐",
                "方法要点：基于互信息压缩对齐噪声并最大化知识转移",
                "实验或效果：多基准测试验证其在开放词汇分割中的优越性"
            ],
            "tags_zh": [
                "开放词汇语义分割",
                "视觉语言预训练",
                "信息理论对齐",
                "CLIP微调",
                "模态对齐转移"
            ],
            "_index": 122
        },
        {
            "title": "The Role of Consequential and Functional Sound in Human-Robot Interaction: Toward Audio Augmented Reality Interfaces",
            "authors": [
                "Aliyah Smith",
                "Monroe Kennedy"
            ],
            "arxiv_id": "2511.15956v1",
            "summary": "As robots become increasingly integrated into everyday environments, understanding how they communicate with humans is critical. Sound offers a powerful channel for interaction, encompassing both operational noises and intentionally designed auditory cues. In this study, we examined the effects of consequential and functional sounds on human perception and behavior, including a novel exploration of spatial sound through localization and handover tasks. Results show that consequential sounds of the Kinova Gen3 manipulator did not negatively affect perceptions, spatial localization is highly accurate for lateral cues but declines for frontal cues, and spatial sounds can simultaneously convey task-relevant information while promoting warmth and reducing discomfort. These findings highlight the potential of functional and transformative auditory design to enhance human-robot collaboration and inform future sound-based interaction strategies.",
            "headline_zh": "探索功能性与后果性声音在人机交互中的作用，以增强协作与音频增强现实界面。",
            "intro_zh": [
                "核心问题：机器人声音如何影响人类感知与行为，包括操作噪音和设计听觉提示。",
                "方法要点：研究功能性和后果性声音，结合空间声音定位和交接任务实验。",
                "实验或效果：空间声音可准确传达任务信息，提升温暖感并减少不适。"
            ],
            "tags_zh": [
                "人机交互",
                "声音设计",
                "空间声音定位",
                "机器人感知",
                "音频增强现实"
            ],
            "_index": 123
        }
    ]
}