{
    "papers": [
        {
            "title": "Back to Basics: Let Denoising Generative Models Denoise",
            "authors": [
                "Tianhong Li",
                "Kaiming He"
            ],
            "arxiv_id": "2511.13720v1",
            "summary": "Today's denoising diffusion models do not \"denoise\" in the classical sense, i.e., they do not directly predict clean images. Rather, the neural networks predict noise or a noised quantity. In this paper, we suggest that predicting clean data and predicting noised quantities are fundamentally different. According to the manifold assumption, natural data should lie on a low-dimensional manifold, whereas noised quantities do not. With this assumption, we advocate for models that directly predict clean data, which allows apparently under-capacity networks to operate effectively in very high-dimensional spaces. We show that simple, large-patch Transformers on pixels can be strong generative models: using no tokenizer, no pre-training, and no extra loss. Our approach is conceptually nothing more than \"$\\textbf{Just image Transformers}$\", or $\\textbf{JiT}$, as we call it. We report competitive results using JiT with large patch sizes of 16 and 32 on ImageNet at resolutions of 256 and 512, where predicting high-dimensional noised quantities can fail catastrophically. With our networks mapping back to the basics of the manifold, our research goes back to basics and pursues a self-contained paradigm for Transformer-based diffusion on raw natural data.",
            "headline_zh": "提出JiT方法，通过直接预测干净图像解决扩散模型在高维空间失效问题",
            "intro_zh": [
                "核心问题：扩散模型预测噪声而非干净图像，违背流形假设，导致高维生成失败",
                "方法要点：使用大块Transformer直接预测干净数据，无需分词器、预训练或额外损失",
                "实验或效果：在ImageNet 256和512分辨率上，JiT取得竞争性结果，验证有效性"
            ],
            "tags_zh": [
                "扩散模型",
                "流形假设",
                "Transformer生成",
                "图像去噪",
                "高维数据生成"
            ],
            "_index": 0
        },
        {
            "title": "Scaling Spatial Intelligence with Multimodal Foundation Models",
            "authors": [
                "Zhongang Cai",
                "Ruisi Wang",
                "Chenyang Gu",
                "Fanyi Pu",
                "Junxiang Xu",
                "Yubo Wang",
                "Wanqi Yin",
                "Zhitao Yang",
                "Chen Wei",
                "Qingping Sun",
                "Tongxi Zhou",
                "Jiaqi Li",
                "Hui En Pang",
                "Oscar Qian",
                "Yukun Wei",
                "Zhiqian Lin",
                "Xuanke Shi",
                "Kewang Deng",
                "Xiaoyang Han",
                "Zukai Chen",
                "Xiangyu Fan",
                "Hanming Deng",
                "Lewei Lu",
                "Liang Pan",
                "Bo Li",
                "Ziwei Liu",
                "Quan Wang",
                "Dahua Lin",
                "Lei Yang"
            ],
            "arxiv_id": "2511.13719v1",
            "summary": "Despite remarkable progress, multimodal foundation models still exhibit surprising deficiencies in spatial intelligence. In this work, we explore scaling up multimodal foundation models to cultivate spatial intelligence within the SenseNova-SI family, built upon established multimodal foundations including visual understanding models (i.e., Qwen3-VL and InternVL3) and unified understanding and generation models (i.e., Bagel). We take a principled approach to constructing high-performing and robust spatial intelligence by systematically curating SenseNova-SI-8M: eight million diverse data samples under a rigorous taxonomy of spatial capabilities. SenseNova-SI demonstrates unprecedented performance across a broad range of spatial intelligence benchmarks: 68.7% on VSI-Bench, 43.3% on MMSI, 85.6% on MindCube, 54.6% on ViewSpatial, and 50.1% on SITE, while maintaining strong general multimodal understanding (e.g., 84.9% on MMBench-En). More importantly, we analyze the impact of data scaling, discuss early signs of emergent generalization capabilities enabled by diverse data training, analyze the risk of overfitting and language shortcuts, present a preliminary study on spatial chain-of-thought reasoning, and validate the potential downstream application. SenseNova-SI is an ongoing project, and this report will be updated continuously. All newly trained multimodal foundation models are publicly released to facilitate further research in this direction.",
            "headline_zh": "提出SenseNova-SI系列模型，通过数据扩展提升多模态基础模型的空间智能能力。",
            "intro_zh": [
                "多模态基础模型在空间智能方面存在显著不足，需要改进。",
                "构建SenseNova-SI-8M数据集，包含800万样本，系统训练模型。",
                "在多个基准测试中表现优异，如VSI-Bench达68.7%，并分析泛化与风险。"
            ],
            "tags_zh": [
                "空间智能",
                "多模态基础模型",
                "数据扩展",
                "基准测试",
                "泛化能力"
            ],
            "_index": 1
        },
        {
            "title": "Segment Anything Across Shots: A Method and Benchmark",
            "authors": [
                "Hengrui Hu",
                "Kaining Ying",
                "Henghui Ding"
            ],
            "arxiv_id": "2511.13715v1",
            "summary": "This work focuses on multi-shot semi-supervised video object segmentation (MVOS), which aims at segmenting the target object indicated by an initial mask throughout a video with multiple shots. The existing VOS methods mainly focus on single-shot videos and struggle with shot discontinuities, thereby limiting their real-world applicability. We propose a transition mimicking data augmentation strategy (TMA) which enables cross-shot generalization with single-shot data to alleviate the severe annotated multi-shot data sparsity, and the Segment Anything Across Shots (SAAS) model, which can detect and comprehend shot transitions effectively. To support evaluation and future study in MVOS, we introduce Cut-VOS, a new MVOS benchmark with dense mask annotations, diverse object categories, and high-frequency transitions. Extensive experiments on YouMVOS and Cut-VOS demonstrate that the proposed SAAS achieves state-of-the-art performance by effectively mimicking, understanding, and segmenting across complex transitions. The code and datasets are released at https://henghuiding.com/SAAS/.",
            "headline_zh": "提出SAAS模型与TMA数据增强，解决多镜头视频对象分割中的镜头不连续问题。",
            "intro_zh": [
                "核心问题：现有VOS方法难以处理多镜头视频中的镜头不连续，限制实际应用。",
                "方法要点：引入TMA数据增强策略，利用单镜头数据模拟跨镜头泛化；开发SAAS模型，有效检测和理解镜头转换。",
                "实验或效果：在YouMVOS和Cut-VOS基准上，SAAS实现最先进性能，验证跨复杂转换的分割能力。"
            ],
            "tags_zh": [
                "多镜头视频对象分割",
                "数据增强策略",
                "镜头转换检测",
                "半监督学习",
                "基准数据集"
            ],
            "_index": 2
        },
        {
            "title": "UnSAMv2: Self-Supervised Learning Enables Segment Anything at Any Granularity",
            "authors": [
                "Junwei Yu",
                "Trevor Darrell",
                "XuDong Wang"
            ],
            "arxiv_id": "2511.13714v1",
            "summary": "The Segment Anything Model (SAM) family has become a widely adopted vision foundation model, but its ability to control segmentation granularity remains limited. Users often need to refine results manually - by adding more prompts or selecting from pre-generated masks - to achieve the desired level of detail. This process can be ambiguous, as the same prompt may correspond to several plausible masks, and collecting dense annotations across all granularities is prohibitively expensive, making supervised solutions infeasible. To address this limitation, we introduce UnSAMv2, which enables segment anything at any granularity without human annotations. UnSAMv2 extends the divide-and-conquer strategy of UnSAM by discovering abundant mask-granularity pairs and introducing a novel granularity control embedding that enables precise, continuous control over segmentation scale. Remarkably, with only $6$K unlabeled images and $0.02\\%$ additional parameters, UnSAMv2 substantially enhances SAM-2, achieving segment anything at any granularity across interactive, whole-image, and video segmentation tasks. Evaluated on over $11$ benchmarks, UnSAMv2 improves $\\text{NoC}_{90}$ (5.69 $\\rightarrow$ 4.75), 1-IoU (58.0 $\\rightarrow$ 73.1), and $\\text{AR}_{1000}$ (49.6 $\\rightarrow$ 68.3), showing that small amounts of unlabeled data with a granularity-aware self-supervised learning method can unlock the potential of vision foundation models.",
            "headline_zh": "提出UnSAMv2以无监督方式实现任意粒度图像分割",
            "intro_zh": [
                "SAM模型难以控制分割粒度，需人工干预，标注成本高",
                "引入自监督学习，发现掩码-粒度对，使用粒度控制嵌入",
                "少量未标注数据显著提升SAM-2，在11个基准上改进指标"
            ],
            "tags_zh": [
                "自监督学习",
                "图像分割",
                "粒度控制",
                "无标注数据",
                "视觉基础模型"
            ],
            "_index": 3
        },
        {
            "title": "Free-Form Scene Editor: Enabling Multi-Round Object Manipulation like in a 3D Engine",
            "authors": [
                "Xincheng Shuai",
                "Zhenyuan Qin",
                "Henghui Ding",
                "Dacheng Tao"
            ],
            "arxiv_id": "2511.13713v1",
            "summary": "Recent advances in text-to-image (T2I) diffusion models have significantly improved semantic image editing, yet most methods fall short in performing 3D-aware object manipulation. In this work, we present FFSE, a 3D-aware autoregressive framework designed to enable intuitive, physically-consistent object editing directly on real-world images. Unlike previous approaches that either operate in image space or require slow and error-prone 3D reconstruction, FFSE models editing as a sequence of learned 3D transformations, allowing users to perform arbitrary manipulations, such as translation, scaling, and rotation, while preserving realistic background effects (e.g., shadows, reflections) and maintaining global scene consistency across multiple editing rounds. To support learning of multi-round 3D-aware object manipulation, we introduce 3DObjectEditor, a hybrid dataset constructed from simulated editing sequences across diverse objects and scenes, enabling effective training under multi-round and dynamic conditions. Extensive experiments show that the proposed FFSE significantly outperforms existing methods in both single-round and multi-round 3D-aware editing scenarios.",
            "headline_zh": "提出FFSE框架以支持真实图像上的多轮3D感知对象编辑",
            "intro_zh": [
                "现有文本到图像方法在3D感知对象编辑上不足，难以保持物理一致性",
                "FFSE采用自回归框架建模3D变换序列，支持平移、缩放和旋转等操作",
                "实验显示FFSE在单轮和多轮3D编辑场景中显著优于现有方法"
            ],
            "tags_zh": [
                "3D感知编辑",
                "自回归框架",
                "多轮操作",
                "物理一致性",
                "真实图像编辑"
            ],
            "_index": 4
        },
        {
            "title": "From Power to Precision: Learning Fine-grained Dexterity for Multi-fingered Robotic Hands",
            "authors": [
                "Jianglong Ye",
                "Lai Wei",
                "Guangqi Jiang",
                "Changwei Jing",
                "Xueyan Zou",
                "Xiaolong Wang"
            ],
            "arxiv_id": "2511.13710v1",
            "summary": "Human grasps can be roughly categorized into two types: power grasps and precision grasps. Precision grasping enables tool use and is believed to have influenced human evolution. Today's multi-fingered robotic hands are effective in power grasps, but for tasks requiring precision, parallel grippers are still more widely adopted. This contrast highlights a key limitation in current robotic hand design: the difficulty of achieving both stable power grasps and precise, fine-grained manipulation within a single, versatile system. In this work, we bridge this gap by jointly optimizing the control and hardware design of a multi-fingered dexterous hand, enabling both power and precision manipulation. Rather than redesigning the entire hand, we introduce a lightweight fingertip geometry modification, represent it as a contact plane, and jointly optimize its parameters along with the corresponding control. Our control strategy dynamically switches between power and precision manipulation and simplifies precision control into parallel thumb-index motions, which proves robust for sim-to-real transfer. On the design side, we leverage large-scale simulation to optimize the fingertip geometry using a differentiable neural-physics surrogate model. We validate our approach through extensive experiments in both sim-to-real and real-to-real settings. Our method achieves an 82.5% zero-shot success rate on unseen objects in sim-to-real precision grasping, and a 93.3% success rate in challenging real-world tasks involving bread pinching. These results demonstrate that our co-design framework can significantly enhance the fine-grained manipulation ability of multi-fingered hands without reducing their ability for power grasps. Our project page is at https://jianglongye.com/power-to-precision",
            "headline_zh": "提出联合优化控制与硬件设计以增强多指机器人手的精细操作能力",
            "intro_zh": [
                "核心问题：多指机器人手难以同时实现强力抓握和精细操作",
                "方法要点：通过轻量级指尖几何修改和动态控制切换简化精度控制",
                "实验或效果：在sim-to-real精度抓取中达到82.5%零样本成功率"
            ],
            "tags_zh": [
                "多指机器人手",
                "精细操作",
                "联合优化",
                "sim-to-real",
                "指尖几何设计"
            ],
            "_index": 5
        },
        {
            "title": "OpenRoboCare: A Multimodal Multi-Task Expert Demonstration Dataset for Robot Caregiving",
            "authors": [
                "Xiaoyu Liang",
                "Ziang Liu",
                "Kelvin Lin",
                "Edward Gu",
                "Ruolin Ye",
                "Tam Nguyen",
                "Cynthia Hsu",
                "Zhanxin Wu",
                "Xiaoman Yang",
                "Christy Sum Yu Cheung",
                "Harold Soh",
                "Katherine Dimitropoulou",
                "Tapomayukh Bhattacharjee"
            ],
            "arxiv_id": "2511.13707v1",
            "summary": "We present OpenRoboCare, a multimodal dataset for robot caregiving, capturing expert occupational therapist demonstrations of Activities of Daily Living (ADLs). Caregiving tasks involve complex physical human-robot interactions, requiring precise perception under occlusions, safe physical contact, and long-horizon planning. While recent advances in robot learning from demonstrations have shown promise, there is a lack of a large-scale, diverse, and expert-driven dataset that captures real-world caregiving routines. To address this gap, we collect data from 21 occupational therapists performing 15 ADL tasks on two manikins. The dataset spans five modalities: RGB-D video, pose tracking, eye-gaze tracking, task and action annotations, and tactile sensing, providing rich multimodal insights into caregiver movement, attention, force application, and task execution strategies. We further analyze expert caregiving principles and strategies, offering insights to improve robot efficiency and task feasibility. Additionally, our evaluations demonstrate that OpenRoboCare presents challenges for state-of-the-art robot perception and human activity recognition methods, both critical for developing safe and adaptive assistive robots, highlighting the value of our contribution. See our website for additional visualizations: https://emprise.cs.cornell.edu/robo-care/.",
            "headline_zh": "提出OpenRoboCare多模态数据集以解决机器人照护中缺乏专家示范数据的问题",
            "intro_zh": [
                "核心问题：机器人照护任务缺乏大规模、多样化的专家示范数据集，影响物理人机交互的感知与规划。",
                "方法要点：收集21名职业治疗师执行15项日常活动任务的多模态数据，包括RGB-D视频、姿态跟踪等五种模态。",
                "实验或效果：评估显示数据集对现有机器人感知和人类活动识别方法构成挑战，提升辅助机器人安全性。"
            ],
            "tags_zh": [
                "机器人照护",
                "多模态数据集",
                "专家示范",
                "日常活动任务",
                "物理人机交互",
                "感知挑战"
            ],
            "_index": 6
        },
        {
            "title": "TiViBench: Benchmarking Think-in-Video Reasoning for Video Generative Models",
            "authors": [
                "Harold Haodong Chen",
                "Disen Lan",
                "Wen-Jie Shu",
                "Qingyang Liu",
                "Zihan Wang",
                "Sirui Chen",
                "Wenkai Cheng",
                "Kanghao Chen",
                "Hongfei Zhang",
                "Zixin Zhang",
                "Rongjin Guo",
                "Yu Cheng",
                "Ying-Cong Chen"
            ],
            "arxiv_id": "2511.13704v1",
            "summary": "The rapid evolution of video generative models has shifted their focus from producing visually plausible outputs to tackling tasks requiring physical plausibility and logical consistency. However, despite recent breakthroughs such as Veo 3's chain-of-frames reasoning, it remains unclear whether these models can exhibit reasoning capabilities similar to large language models (LLMs). Existing benchmarks predominantly evaluate visual fidelity and temporal coherence, failing to capture higher-order reasoning abilities. To bridge this gap, we propose TiViBench, a hierarchical benchmark specifically designed to evaluate the reasoning capabilities of image-to-video (I2V) generation models. TiViBench systematically assesses reasoning across four dimensions: i) Structural Reasoning & Search, ii) Spatial & Visual Pattern Reasoning, iii) Symbolic & Logical Reasoning, and iv) Action Planning & Task Execution, spanning 24 diverse task scenarios across 3 difficulty levels. Through extensive evaluations, we show that commercial models (e.g., Sora 2, Veo 3.1) demonstrate stronger reasoning potential, while open-source models reveal untapped potential that remains hindered by limited training scale and data diversity. To further unlock this potential, we introduce VideoTPO, a simple yet effective test-time strategy inspired by preference optimization. By performing LLM self-analysis on generated candidates to identify strengths and weaknesses, VideoTPO significantly enhances reasoning performance without requiring additional training, data, or reward models. Together, TiViBench and VideoTPO pave the way for evaluating and advancing reasoning in video generation models, setting a foundation for future research in this emerging field.",
            "headline_zh": "提出TiViBench基准和VideoTPO策略以评估和提升视频生成模型的推理能力",
            "intro_zh": [
                "现有基准无法评估视频生成模型的高阶推理能力，如物理合理性和逻辑一致性",
                "TiViBench从四个维度系统评估推理能力，VideoTPO通过LLM自分析增强推理性能",
                "实验显示商业模型推理潜力更强，VideoTPO无需额外训练即可显著提升性能"
            ],
            "tags_zh": [
                "视频生成模型",
                "推理能力评估",
                "基准测试",
                "测试时优化",
                "逻辑一致性",
                "动作规划"
            ],
            "_index": 7
        },
        {
            "title": "Crossing Borders: A Multimodal Challenge for Indian Poetry Translation and Image Generation",
            "authors": [
                "Sofia Jamil",
                "Kotla Sai Charan",
                "Sriparna Saha",
                "Koustava Goswami",
                "Joseph K J"
            ],
            "arxiv_id": "2511.13689v1",
            "summary": "Indian poetry, known for its linguistic complexity and deep cultural resonance, has a rich and varied heritage spanning thousands of years. However, its layered meanings, cultural allusions, and sophisticated grammatical constructions often pose challenges for comprehension, especially for non-native speakers or readers unfamiliar with its context and language. Despite its cultural significance, existing works on poetry have largely overlooked Indian language poems. In this paper, we propose the Translation and Image Generation (TAI) framework, leveraging Large Language Models (LLMs) and Latent Diffusion Models through appropriate prompt tuning. Our framework supports the United Nations Sustainable Development Goals of Quality Education (SDG 4) and Reduced Inequalities (SDG 10) by enhancing the accessibility of culturally rich Indian-language poetry to a global audience. It includes (1) a translation module that uses an Odds Ratio Preference Alignment Algorithm to accurately translate morphologically rich poetry into English, and (2) an image generation module that employs a semantic graph to capture tokens, dependencies, and semantic relationships between metaphors and their meanings, to create visually meaningful representations of Indian poems. Our comprehensive experimental evaluation, including both human and quantitative assessments, demonstrates the superiority of TAI Diffusion in poem image generation tasks, outperforming strong baselines. To further address the scarcity of resources for Indian-language poetry, we introduce the Morphologically Rich Indian Language Poems MorphoVerse Dataset, comprising 1,570 poems across 21 low-resource Indian languages. By addressing the gap in poetry translation and visual comprehension, this work aims to broaden accessibility and enrich the reader's experience.",
            "headline_zh": "提出TAI框架以解决印度诗歌翻译与图像生成的挑战",
            "intro_zh": [
                "核心问题：印度诗歌语言复杂、文化内涵深，难以被非母语者理解。",
                "方法要点：结合LLM和潜在扩散模型，通过翻译和语义图生成图像。",
                "实验或效果：TAI在图像生成任务中优于基线，并引入新数据集。"
            ],
            "tags_zh": [
                "诗歌翻译",
                "图像生成",
                "多模态框架",
                "印度语言",
                "语义图",
                "数据集构建"
            ],
            "_index": 8
        },
        {
            "title": "Training-Free Multi-View Extension of IC-Light for Textual Position-Aware Scene Relighting",
            "authors": [
                "Jiangnan Ye",
                "Jiedong Zhuang",
                "Lianrui Mu",
                "Wenjie Zheng",
                "Jiaqi Hu",
                "Xingze Zou",
                "Jing Wang",
                "Haoji Hu"
            ],
            "arxiv_id": "2511.13684v1",
            "summary": "We introduce GS-Light, an efficient, textual position-aware pipeline for text-guided relighting of 3D scenes represented via Gaussian Splatting (3DGS). GS-Light implements a training-free extension of a single-input diffusion model to handle multi-view inputs. Given a user prompt that may specify lighting direction, color, intensity, or reference objects, we employ a large vision-language model (LVLM) to parse the prompt into lighting priors. Using off-the-shelf estimators for geometry and semantics (depth, surface normals, and semantic segmentation), we fuse these lighting priors with view-geometry constraints to compute illumination maps and generate initial latent codes for each view. These meticulously derived init latents guide the diffusion model to generate relighting outputs that more accurately reflect user expectations, especially in terms of lighting direction. By feeding multi-view rendered images, along with the init latents, into our multi-view relighting model, we produce high-fidelity, artistically relit images. Finally, we fine-tune the 3DGS scene with the relit appearance to obtain a fully relit 3D scene. We evaluate GS-Light on both indoor and outdoor scenes, comparing it to state-of-the-art baselines including per-view relighting, video relighting, and scene editing methods. Using quantitative metrics (multi-view consistency, imaging quality, aesthetic score, semantic similarity, etc.) and qualitative assessment (user studies), GS-Light demonstrates consistent improvements over baselines. Code and assets will be made available upon publication.",
            "headline_zh": "提出GS-Light，实现基于文本的3D高斯溅射场景重光照，无需训练扩展多视图输入。",
            "intro_zh": [
                "核心问题：文本引导的3D场景重光照需处理多视图一致性与用户意图准确反映。",
                "方法要点：融合LVLM解析光照先验与几何约束，生成初始潜码指导扩散模型。",
                "实验或效果：在室内外场景评估，多视图一致性与成像质量优于基线方法。"
            ],
            "tags_zh": [
                "3D高斯溅射",
                "文本引导重光照",
                "多视图扩散模型",
                "光照先验融合",
                "训练免费扩展"
            ],
            "_index": 9
        },
        {
            "title": "QUILL: An Algorithm-Architecture Co-Design for Cache-Local Deformable Attention",
            "authors": [
                "Hyunwoo Oh",
                "Hanning Chen",
                "Sanggeon Yun",
                "Yang Ni",
                "Wenjun Huang",
                "Tamoghno Das",
                "Suyeon Jang",
                "Mohsen Imani"
            ],
            "arxiv_id": "2511.13679v1",
            "summary": "Deformable transformers deliver state-of-the-art detection but map poorly to hardware due to irregular memory access and low arithmetic intensity. We introduce QUILL, a schedule-aware accelerator that turns deformable attention into cache-friendly, single-pass work. At its core, Distance-based Out-of-Order Querying (DOOQ) orders queries by spatial proximity; the look-ahead drives a region prefetch into an alternate buffer--forming a schedule-aware prefetch loop that overlaps memory and compute. A fused MSDeformAttn engine executes interpolation, Softmax, aggregation, and the final projection (W''m) in one pass without spilling intermediates, while small tensors are kept on-chip and surrounding dense layers run on integrated GEMMs. Implemented as RTL and evaluated end-to-end, QUILL achieves up to 7.29x higher throughput and 47.3x better energy efficiency than an RTX 4090, and exceeds prior accelerators by 3.26-9.82x in throughput and 2.01-6.07x in energy efficiency. With mixed-precision quantization, accuracy tracks FP32 within <=0.9 AP across Deformable and Sparse DETR variants. By converting sparsity into locality--and locality into utilization--QUILL delivers consistent, end-to-end speedups.",
            "headline_zh": "提出QUILL算法-架构协同设计，通过缓存局部变形注意力提升检测效率",
            "intro_zh": [
                "变形Transformer在检测中性能领先，但存在内存访问不规则和算术强度低的问题",
                "核心方法包括基于距离的无序查询排序和区域预取，实现单次融合计算",
                "实验显示吞吐量提升最高7.29倍，能效提高47.3倍，精度损失小于0.9 AP"
            ],
            "tags_zh": [
                "变形注意力",
                "硬件加速器",
                "缓存优化",
                "算法-架构协同设计",
                "目标检测",
                "能效提升"
            ],
            "_index": 10
        },
        {
            "title": "OlmoEarth: Stable Latent Image Modeling for Multimodal Earth Observation",
            "authors": [
                "Henry Herzog",
                "Favyen Bastani",
                "Yawen Zhang",
                "Gabriel Tseng",
                "Joseph Redmon",
                "Hadrien Sablon",
                "Ryan Park",
                "Jacob Morrison",
                "Alexandra Buraczynski",
                "Karen Farley",
                "Joshua Hansen",
                "Andrew Howe",
                "Patrick Alan Johnson",
                "Mark Otterlee",
                "Ted Schmitt",
                "Hunter Pitelka",
                "Stephen Daspit",
                "Rachel Ratner",
                "Christopher Wilhelm",
                "Sebastian Wood",
                "Mike Jacobi",
                "Hannah Kerner",
                "Evan Shelhamer",
                "Ali Farhadi",
                "Ranjay Krishna",
                "Patrick Beukema"
            ],
            "arxiv_id": "2511.13655v1",
            "summary": "Earth observation data presents a unique challenge: it is spatial like images, sequential like video or text, and highly multimodal. We present OlmoEarth: a multimodal, spatio-temporal foundation model that employs a novel self-supervised learning formulation, masking strategy, and loss all designed for the Earth observation domain. OlmoEarth achieves state-of-the-art performance compared to 12 other foundation models across a variety of research benchmarks and real-world tasks from external partners. When evaluating embeddings OlmoEarth achieves the best performance on 15 out of 24 tasks, and with full fine-tuning it is the best on 19 of 29 tasks. We deploy OlmoEarth as the backbone of an end-to-end platform for data collection, labeling, training, and inference of Earth observation models. The OlmoEarth Platform puts frontier foundation models and powerful data management tools into the hands of non-profits and NGOs working to solve the world's biggest problems. OlmoEarth source code, training data, and pre-trained weights are available at $\\href{https://github.com/allenai/olmoearth_pretrain}{\\text{https://github.com/allenai/olmoearth_pretrain}}$.",
            "headline_zh": "提出OlmoEarth多模态时空基础模型以解决地球观测数据挑战",
            "intro_zh": [
                "地球观测数据具有空间、时序和多模态特性，带来独特建模挑战。",
                "采用自监督学习、掩码策略和损失函数，专为地球观测领域设计。",
                "在多个基准测试和实际任务中，性能优于12个其他基础模型。"
            ],
            "tags_zh": [
                "多模态学习",
                "时空建模",
                "自监督学习",
                "地球观测",
                "基础模型",
                "掩码策略"
            ],
            "_index": 11
        },
        {
            "title": "Tuning for Two Adversaries: Enhancing the Robustness Against Transfer and Query-Based Attacks using Hyperparameter Tuning",
            "authors": [
                "Pascal Zimmer",
                "Ghassan Karame"
            ],
            "arxiv_id": "2511.13654v1",
            "summary": "In this paper, we present the first detailed analysis of how optimization hyperparameters -- such as learning rate, weight decay, momentum, and batch size -- influence robustness against both transfer-based and query-based attacks. Supported by theory and experiments, our study spans a variety of practical deployment settings, including centralized training, ensemble learning, and distributed training. We uncover a striking dichotomy: for transfer-based attacks, decreasing the learning rate significantly enhances robustness by up to $64\\%$. In contrast, for query-based attacks, increasing the learning rate consistently leads to improved robustness by up to $28\\%$ across various settings and data distributions. Leveraging these findings, we explore -- for the first time -- the optimization hyperparameter design space to jointly enhance robustness against both transfer-based and query-based attacks. Our results reveal that distributed models benefit the most from hyperparameter tuning, achieving a remarkable tradeoff by simultaneously mitigating both attack types more effectively than other training setups.",
            "headline_zh": "通过超参数调优增强对抗转移和查询攻击的鲁棒性",
            "intro_zh": [
                "核心问题：优化超参数如何影响对抗转移和查询攻击的鲁棒性。",
                "方法要点：分析学习率等超参数，理论实验结合，覆盖多种训练设置。",
                "实验效果：学习率降低提升转移攻击鲁棒性64%，增加提升查询攻击鲁棒性28%。"
            ],
            "tags_zh": [
                "超参数调优",
                "对抗鲁棒性",
                "转移攻击",
                "查询攻击",
                "分布式训练",
                "学习率优化"
            ],
            "_index": 12
        },
        {
            "title": "Distribution Matching Distillation Meets Reinforcement Learning",
            "authors": [
                "Dengyang Jiang",
                "Dongyang Liu",
                "Zanyi Wang",
                "Qilong Wu",
                "Xin Jin",
                "David Liu",
                "Zhen Li",
                "Mengmeng Wang",
                "Peng Gao",
                "Harry Yang"
            ],
            "arxiv_id": "2511.13649v1",
            "summary": "Distribution Matching Distillation (DMD) distills a pre-trained multi-step diffusion model to a few-step one to improve inference efficiency. However, the performance of the latter is often capped by the former. To circumvent this dilemma, we propose DMDR, a novel framework that combines Reinforcement Learning (RL) techniques into the distillation process. We show that for the RL of the few-step generator, the DMD loss itself is a more effective regularization compared to the traditional ones. In turn, RL can help to guide the mode coverage process in DMD more effectively. These allow us to unlock the capacity of the few-step generator by conducting distillation and RL simultaneously. Meanwhile, we design the dynamic distribution guidance and dynamic renoise sampling training strategies to improve the initial distillation process. The experiments demonstrate that DMDR can achieve leading visual quality, prompt coherence among few-step methods, and even exhibit performance that exceeds the multi-step teacher.",
            "headline_zh": "提出DMDR框架，结合强化学习与分布匹配蒸馏，提升少步扩散模型性能。",
            "intro_zh": [
                "核心问题：少步扩散模型性能受限于预训练多步模型，难以超越。",
                "方法要点：引入强化学习优化蒸馏过程，使用DMD损失作为正则化项。",
                "实验效果：DMDR在少步方法中实现领先视觉质量和提示一致性，甚至超越教师模型。"
            ],
            "tags_zh": [
                "分布匹配蒸馏",
                "强化学习",
                "扩散模型",
                "模型蒸馏",
                "少步生成"
            ],
            "_index": 13
        },
        {
            "title": "PhysX-Anything: Simulation-Ready Physical 3D Assets from Single Image",
            "authors": [
                "Ziang Cao",
                "Fangzhou Hong",
                "Zhaoxi Chen",
                "Liang Pan",
                "Ziwei Liu"
            ],
            "arxiv_id": "2511.13648v1",
            "summary": "3D modeling is shifting from static visual representations toward physical, articulated assets that can be directly used in simulation and interaction. However, most existing 3D generation methods overlook key physical and articulation properties, thereby limiting their utility in embodied AI. To bridge this gap, we introduce PhysX-Anything, the first simulation-ready physical 3D generative framework that, given a single in-the-wild image, produces high-quality sim-ready 3D assets with explicit geometry, articulation, and physical attributes. Specifically, we propose the first VLM-based physical 3D generative model, along with a new 3D representation that efficiently tokenizes geometry. It reduces the number of tokens by 193x, enabling explicit geometry learning within standard VLM token budgets without introducing any special tokens during fine-tuning and significantly improving generative quality. In addition, to overcome the limited diversity of existing physical 3D datasets, we construct a new dataset, PhysX-Mobility, which expands the object categories in prior physical 3D datasets by over 2x and includes more than 2K common real-world objects with rich physical annotations. Extensive experiments on PhysX-Mobility and in-the-wild images demonstrate that PhysX-Anything delivers strong generative performance and robust generalization. Furthermore, simulation-based experiments in a MuJoCo-style environment validate that our sim-ready assets can be directly used for contact-rich robotic policy learning. We believe PhysX-Anything can substantially empower a broad range of downstream applications, especially in embodied AI and physics-based simulation.",
            "headline_zh": "提出PhysX-Anything框架，从单张图像生成仿真就绪的物理3D资产，以解决现有方法忽略物理属性的问题。",
            "intro_zh": [
                "核心问题：现有3D生成方法忽视物理和关节属性，限制在具身AI中的应用。",
                "方法要点：基于VLM的物理3D生成模型，新3D表示将几何token数减少193倍。",
                "实验或效果：在PhysX-Mobility数据集和野外图像上验证强生成性能和泛化能力。"
            ],
            "tags_zh": [
                "物理3D生成",
                "仿真就绪资产",
                "视觉语言模型",
                "几何token化",
                "具身AI",
                "物理模拟"
            ],
            "_index": 14
        },
        {
            "title": "Part-X-MLLM: Part-aware 3D Multimodal Large Language Model",
            "authors": [
                "Chunshi Wang",
                "Junliang Ye",
                "Yunhan Yang",
                "Yang Li",
                "Zizhuo Lin",
                "Jun Zhu",
                "Zhuo Chen",
                "Yawei Luo",
                "Chunchao Guo"
            ],
            "arxiv_id": "2511.13647v1",
            "summary": "We introduce Part-X-MLLM, a native 3D multimodal large language model that unifies diverse 3D tasks by formulating them as programs in a structured, executable grammar. Given an RGB point cloud and a natural language prompt, our model autoregressively generates a single, coherent token sequence encoding part-level bounding boxes, semantic descriptions, and edit commands. This structured output serves as a versatile interface to drive downstream geometry-aware modules for part-based generation and editing. By decoupling the symbolic planning from the geometric synthesis, our approach allows any compatible geometry engine to be controlled through a single, language-native frontend. We pre-train a dual-encoder architecture to disentangle structure from semantics and instruction-tune the model on a large-scale, part-centric dataset. Experiments demonstrate that our model excels at producing high-quality, structured plans, enabling state-of-the-art performance in grounded Q\\&A, compositional generation, and localized editing through one unified interface. Project page: https://chunshi.wang/Part-X-MLLM/",
            "headline_zh": "提出Part-X-MLLM以统一3D多模态任务，通过结构化程序生成驱动几何模块。",
            "intro_zh": [
                "核心问题：统一3D多模态任务，如部分级检测、描述和编辑。",
                "方法要点：使用双编码器架构，生成结构化令牌序列，解耦符号规划与几何合成。",
                "实验效果：在问答、生成和编辑任务中实现先进性能，通过单一接口控制。"
            ],
            "tags_zh": [
                "3D多模态大语言模型",
                "部分感知",
                "结构化程序生成",
                "几何编辑",
                "双编码器架构"
            ],
            "_index": 15
        },
        {
            "title": "CacheFlow: Compressive Streaming Memory for Efficient Long-Form Video Understanding",
            "authors": [
                "Shrenik Patel",
                "Daivik Patel"
            ],
            "arxiv_id": "2511.13644v1",
            "summary": "Long-form video question answering (VQA) overwhelms current vision-language models (VLMs) because attention and key-value (KV) caches grow with runtime, forcing either expensive inference or near-sighted sliding windows. We introduce CacheFlow, a training-free pipeline that pairs Dynamic Token Dropping (DTD) with a compressive long-term memory. DTD prunes per-patch tokens online via cosine similarity to the previous frame, and surviving tokens are packed into fixed-size blocks. This online, per-frame processing makes our approach fundamentally suited for live streaming VQA. As blocks are processed, each one's keys are summarized by a tiny recurrent encoder to form a retrieval index, while the block's full KV pairs are offloaded and later rehydrated for generation, preserving answer fidelity. At inference, a consensus-based retrieval mechanism retrieves only the Top-K most relevant blocks and attends over both the retrieved and local context for precise, long-range reasoning. CacheFlow is drop-in, architecture-agnostic, and requires no fine-tuning. Experiments on both offline and streaming VQA benchmarks demonstrate that CacheFlow outperforms current strong baselines, while processing up to 87% less tokens. Our dual approach enables VLMs to be both efficient and context-aware, paving the way for practical long-form video understanding.",
            "headline_zh": "提出CacheFlow以解决长视频问答中注意力与KV缓存增长导致的效率问题",
            "intro_zh": [
                "核心问题：长视频问答中注意力机制和KV缓存随运行时间增长，导致推理成本高或视野受限",
                "方法要点：结合动态令牌丢弃和压缩长期记忆，在线处理令牌并构建检索索引，无需训练",
                "实验或效果：在离线与流式VQA基准测试中优于基线，处理令牌减少高达87%"
            ],
            "tags_zh": [
                "长视频理解",
                "动态令牌丢弃",
                "KV缓存压缩",
                "流式视频问答",
                "无需训练方法"
            ],
            "_index": 16
        },
        {
            "title": "Alpha Divergence Losses for Biometric Verification",
            "authors": [
                "Dimitrios Koutsianos",
                "Ladislav Mosner",
                "Yannis Panagakis",
                "Themos Stafylakis"
            ],
            "arxiv_id": "2511.13621v1",
            "summary": "Performance in face and speaker verification is largely driven by margin based softmax losses like CosFace and ArcFace. Recently introduced $α$-divergence loss functions offer a compelling alternative, particularly for their ability to induce sparse solutions (when $α>1$). However, integrating an angular margin-crucial for verification tasks-is not straightforward. We find this integration can be achieved in at least two distinct ways: via the reference measure (prior probabilities) or via the logits (unnormalized log-likelihoods). In this paper, we explore both pathways, deriving two novel margin-based $α$-divergence losses: Q-Margin (margin in the reference measure) and A3M (margin in the logits). We identify and address a critical training instability in A3M-caused by the interplay of penalized logits and sparsity-with a simple yet effective prototype re-initialization strategy. Our methods achieve significant performance gains on the challenging IJB-B and IJB-C face verification benchmarks. We demonstrate similarly strong performance in speaker verification on VoxCeleb. Crucially, our models significantly outperform strong baselines at low false acceptance rates (FAR). This capability is crucial for practical high-security applications, such as banking authentication, when minimizing false authentications is paramount.",
            "headline_zh": "提出基于α-散度的Q-Margin和A3M损失函数，提升生物特征验证性能",
            "intro_zh": [
                "核心问题：α-散度损失难以集成角度间隔，影响人脸和说话人验证性能",
                "方法要点：通过参考度量或logits集成间隔，并解决A3M训练不稳定性",
                "实验或效果：在IJB-B、IJB-C和VoxCeleb基准上显著提升性能，尤其在低误接受率下"
            ],
            "tags_zh": [
                "生物特征验证",
                "α-散度损失",
                "角度间隔",
                "人脸验证",
                "说话人验证",
                "训练稳定性"
            ],
            "_index": 17
        },
        {
            "title": "A Real-Time Driver Drowsiness Detection System Using MediaPipe and Eye Aspect Ratio",
            "authors": [
                "Ashlesha G. Sawant",
                "Shreyash S. Kamble",
                "Raj S. Kanade",
                "Raunak N. Kanugo",
                "Tanishq A. Kapse",
                "Karan A. Bhapse"
            ],
            "arxiv_id": "2511.13618v1",
            "summary": "One of the major causes of road accidents is driver fatigue that causes thousands of fatalities and injuries every year. This study shows development of a Driver Drowsiness Detection System meant to improve the safety of the road by alerting drivers who are showing signs of being drowsy. The system is based on a standard webcam that tracks the facial features of the driver with the main emphasis on the examination of eye movements that can be conducted with the help of the Eye Aspect Ratio (EAR) method. The Face Mesh by MediaPipe is a lightweight framework that can identify facial landmarks with high accuracy and efficiency, which is considered to be important in real time use. The system detects the moments of long eye shutdowns or a very low rate of blinking which are manifestations of drowsiness and alerts the driver through sound to get her attention back. This system achieves a high-performance and low-cost driver monitoring solution with the help of the computational power of OpenCV to process the image and the MediaPipe to identify faces. Test data experimental analyses indicate that the system is very accurate and responds quicker; this confirms that it can be a component of the current Advanced Driving Assistance System (ADAS).",
            "headline_zh": "提出基于MediaPipe和眼高宽比的实时驾驶员疲劳检测系统以提升道路安全",
            "intro_zh": [
                "核心问题：驾驶员疲劳是道路事故主因，导致大量伤亡。",
                "方法要点：使用MediaPipe Face Mesh实时追踪面部特征，结合眼高宽比检测闭眼和眨眼频率。",
                "实验或效果：系统准确率高、响应快，可集成到高级驾驶辅助系统中。"
            ],
            "tags_zh": [
                "驾驶员疲劳检测",
                "眼高宽比",
                "MediaPipe",
                "实时监控",
                "面部特征追踪",
                "高级驾驶辅助系统"
            ],
            "_index": 18
        },
        {
            "title": "Tissue Aware Nuclei Detection and Classification Model for Histopathology Images",
            "authors": [
                "Kesi Xu",
                "Eleni Chiou",
                "Ali Varamesh",
                "Laura Acqualagna",
                "Nasir Rajpoot"
            ],
            "arxiv_id": "2511.13615v1",
            "summary": "Accurate nuclei detection and classification are fundamental to computational pathology, yet existing approaches are hindered by reliance on detailed expert annotations and insufficient use of tissue context. We present Tissue-Aware Nuclei Detection (TAND), a novel framework achieving joint nuclei detection and classification using point-level supervision enhanced by tissue mask conditioning. TAND couples a ConvNeXt-based encoder-decoder with a frozen Virchow-2 tissue segmentation branch, where semantic tissue probabilities selectively modulate the classification stream through a novel multi-scale Spatial Feature-wise Linear Modulation (Spatial-FiLM). On the PUMA benchmark, TAND achieves state-of-the-art performance, surpassing both tissue-agnostic baselines and mask-supervised methods. Notably, our approach demonstrates remarkable improvements in tissue-dependent cell types such as epithelium, endothelium, and stroma. To the best of our knowledge, this is the first method to condition per-cell classification on learned tissue masks, offering a practical pathway to reduce annotation burden.",
            "headline_zh": "提出TAND框架以解决组织病理图像中细胞检测与分类的标注依赖问题",
            "intro_zh": [
                "核心问题：现有方法依赖专家详细标注且未充分利用组织上下文",
                "方法要点：结合ConvNeXt编码-解码器与Virchow-2组织分割，通过Spatial-FiLM调制分类流",
                "实验或效果：在PUMA基准上实现SOTA，显著提升组织依赖细胞类型分类"
            ],
            "tags_zh": [
                "组织病理图像",
                "细胞检测与分类",
                "点级监督",
                "组织掩码条件",
                "Spatial-FiLM",
                "ConvNeXt"
            ],
            "_index": 19
        },
        {
            "title": "AtlasMorph: Learning conditional deformable templates for brain MRI",
            "authors": [
                "Marianne Rakic",
                "Andrew Hoopes",
                "S. Mazdak Abulnaga",
                "Mert R. Sabuncu",
                "John V. Guttag",
                "Adrian V. Dalca"
            ],
            "arxiv_id": "2511.13609v1",
            "summary": "Deformable templates, or atlases, are images that represent a prototypical anatomy for a population, and are often enhanced with probabilistic anatomical label maps. They are commonly used in medical image analysis for population studies and computational anatomy tasks such as registration and segmentation. Because developing a template is a computationally expensive process, relatively few templates are available. As a result, analysis is often conducted with sub-optimal templates that are not truly representative of the study population, especially when there are large variations within this population. We propose a machine learning framework that uses convolutional registration neural networks to efficiently learn a function that outputs templates conditioned on subject-specific attributes, such as age and sex. We also leverage segmentations, when available, to produce anatomical segmentation maps for the resulting templates. The learned network can also be used to register subject images to the templates. We demonstrate our method on a compilation of 3D brain MRI datasets, and show that it can learn high-quality templates that are representative of populations. We find that annotated conditional templates enable better registration than their unlabeled unconditional counterparts, and outperform other templates construction methods.",
            "headline_zh": "提出AtlasMorph框架，学习条件可变形模板以优化脑MRI分析",
            "intro_zh": [
                "核心问题：脑MRI分析中模板构建昂贵，现有模板难以代表大变异人群。",
                "方法要点：使用卷积配准网络学习基于年龄、性别等属性的条件模板。",
                "实验或效果：在3D脑MRI数据集上验证，条件模板提升配准性能。"
            ],
            "tags_zh": [
                "可变形模板",
                "脑MRI分析",
                "条件学习",
                "卷积配准网络",
                "图像配准"
            ],
            "_index": 20
        },
        {
            "title": "ICLR: Inter-Chrominance and Luminance Interaction for Natural Color Restoration in Low-Light Image Enhancement",
            "authors": [
                "Xin Xu",
                "Hao Liu",
                "Wei Liu",
                "Wei Wang",
                "Jiayi Wu",
                "Kui Jiang"
            ],
            "arxiv_id": "2511.13607v1",
            "summary": "Low-Light Image Enhancement (LLIE) task aims at improving contrast while restoring details and textures for images captured in low-light conditions. HVI color space has made significant progress in this task by enabling precise decoupling of chrominance and luminance. However, for the interaction of chrominance and luminance branches, substantial distributional differences between the two branches prevalent in natural images limit complementary feature extraction, and luminance errors are propagated to chrominance channels through the nonlinear parameter. Furthermore, for interaction between different chrominance branches, images with large homogeneous-color regions usually exhibit weak correlation between chrominance branches due to concentrated distributions. Traditional pixel-wise losses exploit strong inter-branch correlations for co-optimization, causing gradient conflicts in weakly correlated regions. Therefore, we propose an Inter-Chrominance and Luminance Interaction (ICLR) framework including a Dual-stream Interaction Enhancement Module (DIEM) and a Covariance Correction Loss (CCL). The DIEM improves the extraction of complementary information from two dimensions, fusion and enhancement, respectively. The CCL utilizes luminance residual statistics to penalize chrominance errors and balances gradient conflicts by constraining chrominance branches covariance. Experimental results on multiple datasets show that the proposed ICLR framework outperforms state-of-the-art methods.",
            "headline_zh": "提出ICLR框架以解决低光图像增强中色度与亮度交互问题",
            "intro_zh": [
                "核心问题：色度与亮度分支分布差异大，导致特征提取受限和误差传播。",
                "方法要点：引入DIEM模块增强互补信息提取，CCL损失平衡梯度冲突。",
                "实验或效果：在多个数据集上优于现有方法，提升图像细节恢复。"
            ],
            "tags_zh": [
                "低光图像增强",
                "色度亮度交互",
                "双流交互模块",
                "协方差校正损失",
                "自然色彩恢复"
            ],
            "_index": 21
        },
        {
            "title": "VVS: Accelerating Speculative Decoding for Visual Autoregressive Generation via Partial Verification Skipping",
            "authors": [
                "Haotian Dong",
                "Ye Li",
                "Rongwei Lu",
                "Chen Tang",
                "Shu-Tao Xia",
                "Zhi Wang"
            ],
            "arxiv_id": "2511.13587v1",
            "summary": "Visual autoregressive (AR) generation models have demonstrated strong potential for image generation, yet their next-token-prediction paradigm introduces considerable inference latency. Although speculative decoding (SD) has been proven effective for accelerating visual AR models, its \"draft one step, then verify one step\" paradigm prevents a direct reduction of the forward passes, thus restricting acceleration potential. Motivated by the visual token interchangeability, we for the first time to explore verification skipping in the SD process of visual AR model generation to explicitly cut the number of target model forward passes, thereby reducing inference latency. Based on an analysis of the drafting stage's characteristics, we observe that verification redundancy and stale feature reusability are key factors to retain generation quality and speedup for verification-free steps. Inspired by these two observations, we propose a novel SD framework VVS to accelerate visual AR generation via partial verification skipping, which integrates three complementary modules: (1) a verification-free token selector with dynamical truncation, (2) token-level feature caching and reuse, and (3) fine-grained skipped step scheduling. Consequently, VVS reduces the number of target model forward passes by a factor of $2.8\\times$ relative to vanilla AR decoding while maintaining competitive generation quality, offering a superior speed-quality trade-off over conventional SD frameworks and revealing strong potential to reshape the SD paradigm.",
            "headline_zh": "提出VVS框架以加速视觉自回归生成，通过部分验证跳过减少推理延迟",
            "intro_zh": [
                "视觉自回归模型推理延迟高，推测解码无法直接减少前向传递次数",
                "利用视觉令牌可互换性，设计验证跳过、特征缓存和调度模块",
                "实验显示前向传递减少2.8倍，保持生成质量，优于传统方法"
            ],
            "tags_zh": [
                "视觉自回归生成",
                "推测解码",
                "推理加速",
                "验证跳过",
                "特征缓存",
                "令牌调度"
            ],
            "_index": 22
        },
        {
            "title": "Adaptive Multi-Scale Integration Unlocks Robust Cell Annotation in Histopathology Images",
            "authors": [
                "Yinuo Xu",
                "Yan Cui",
                "Mingyao Li",
                "Zhi Huang"
            ],
            "arxiv_id": "2511.13586v1",
            "summary": "Identifying cell types and subtypes from routine histopathology images is essential for improving the computational understanding of human disease. Existing tile-based models can capture detailed nuclear morphology but often fail to incorporate the broader tissue context that influences a cell's function and identity. In addition, available human annotations are typically coarse-grained and unevenly distributed across studies, making fine-grained subtype-level supervision difficult to obtain.\n  To address these limitations, we introduce NuClass, a pathologist workflow inspired framework for cell-wise multi-scale integration of nuclear morphology and microenvironmental context. NuClass includes two main components: Path local, which focuses on nuclear morphology from 224-by-224 pixel crops, and Path global, which models the surrounding 1024-by-1024 pixel neighborhood. A learnable gating module adaptively balances local detail and contextual cues. To encourage complementary learning, we incorporate an uncertainty-guided objective that directs the global path to prioritize regions where the local path is uncertain. We also provide calibrated confidence estimates and Grad-CAM visualizations to enhance interpretability.\n  To overcome the lack of high-quality annotations, we construct a marker-guided dataset from Xenium spatial transcriptomics assays, yielding single-cell resolution labels for more than two million cells across eight organs and 16 classes. Evaluated on three fully held-out cohorts, NuClass achieves up to 96 percent F1 for its best-performing class, outperforming strong baselines. Our results show that multi-scale, uncertainty-aware fusion can bridge the gap between slide-level pathological foundation models and reliable, cell-level phenotype prediction.",
            "headline_zh": "提出NuClass框架以解决组织病理图像中细胞注释的鲁棒性问题",
            "intro_zh": [
                "核心问题：现有模型难以整合细胞核形态与组织上下文，且细粒度标注稀缺。",
                "方法要点：通过多尺度集成和不确定性引导，自适应融合局部与全局特征。",
                "实验或效果：在三个独立队列中，最佳类别F1达96%，优于基线方法。"
            ],
            "tags_zh": [
                "细胞注释",
                "多尺度集成",
                "不确定性引导",
                "组织病理图像",
                "空间转录组学"
            ],
            "_index": 23
        },
        {
            "title": "Hierarchical Prompt Learning for Image- and Text-Based Person Re-Identification",
            "authors": [
                "Linhan Zhou",
                "Shuang Li",
                "Neng Dong",
                "Yonghang Tai",
                "Yafei Zhang",
                "Huafeng Li"
            ],
            "arxiv_id": "2511.13575v1",
            "summary": "Person re-identification (ReID) aims to retrieve target pedestrian images given either visual queries (image-to-image, I2I) or textual descriptions (text-to-image, T2I). Although both tasks share a common retrieval objective, they pose distinct challenges: I2I emphasizes discriminative identity learning, while T2I requires accurate cross-modal semantic alignment. Existing methods often treat these tasks separately, which may lead to representation entanglement and suboptimal performance. To address this, we propose a unified framework named Hierarchical Prompt Learning (HPL), which leverages task-aware prompt modeling to jointly optimize both tasks. Specifically, we first introduce a Task-Routed Transformer, which incorporates dual classification tokens into a shared visual encoder to route features for I2I and T2I branches respectively. On top of this, we develop a hierarchical prompt generation scheme that integrates identity-level learnable tokens with instance-level pseudo-text tokens. These pseudo-tokens are derived from image or text features via modality-specific inversion networks, injecting fine-grained, instance-specific semantics into the prompts. Furthermore, we propose a Cross-Modal Prompt Regularization strategy to enforce semantic alignment in the prompt token space, ensuring that pseudo-prompts preserve source-modality characteristics while enhancing cross-modal transferability. Extensive experiments on multiple ReID benchmarks validate the effectiveness of our method, achieving state-of-the-art performance on both I2I and T2I tasks.",
            "headline_zh": "提出分层提示学习框架以统一优化图像和文本行人重识别任务",
            "intro_zh": [
                "核心问题：图像和文本行人重识别任务分离导致表示纠缠和性能不佳",
                "方法要点：使用任务路由Transformer和分层提示生成，结合伪文本令牌增强语义对齐",
                "实验或效果：在多个基准测试中实现图像和文本行人重识别的先进性能"
            ],
            "tags_zh": [
                "行人重识别",
                "分层提示学习",
                "跨模态检索",
                "任务路由Transformer",
                "伪文本令牌"
            ],
            "_index": 24
        },
        {
            "title": "Opt3DGS: Optimizing 3D Gaussian Splatting with Adaptive Exploration and Curvature-Aware Exploitation",
            "authors": [
                "Ziyang Huang",
                "Jiagang Chen",
                "Jin Liu",
                "Shunping Ji"
            ],
            "arxiv_id": "2511.13571v1",
            "summary": "3D Gaussian Splatting (3DGS) has emerged as a leading framework for novel view synthesis, yet its core optimization challenges remain underexplored. We identify two key issues in 3DGS optimization: entrapment in suboptimal local optima and insufficient convergence quality. To address these, we propose Opt3DGS, a robust framework that enhances 3DGS through a two-stage optimization process of adaptive exploration and curvature-guided exploitation. In the exploration phase, an Adaptive Weighted Stochastic Gradient Langevin Dynamics (SGLD) method enhances global search to escape local optima. In the exploitation phase, a Local Quasi-Newton Direction-guided Adam optimizer leverages curvature information for precise and efficient convergence. Extensive experiments on diverse benchmark datasets demonstrate that Opt3DGS achieves state-of-the-art rendering quality by refining the 3DGS optimization process without modifying its underlying representation.",
            "headline_zh": "提出Opt3DGS以解决3D高斯泼溅优化中的局部最优和收敛质量问题",
            "intro_zh": [
                "核心问题：3D高斯泼溅优化易陷局部最优且收敛质量不足",
                "方法要点：采用自适应探索与曲率引导开发两阶段优化框架",
                "实验或效果：在多个基准数据集上实现最先进的渲染质量"
            ],
            "tags_zh": [
                "3D高斯泼溅",
                "优化算法",
                "新视角合成",
                "自适应探索",
                "曲率引导开发"
            ],
            "_index": 25
        },
        {
            "title": "TSE-Net: Semi-supervised Monocular Height Estimation from Single Remote Sensing Images",
            "authors": [
                "Sining Chen",
                "Xiao Xiang Zhu"
            ],
            "arxiv_id": "2511.13552v1",
            "summary": "Monocular height estimation plays a critical role in 3D perception for remote sensing, offering a cost-effective alternative to multi-view or LiDAR-based methods. While deep learning has significantly advanced the capabilities of monocular height estimation, these methods remain fundamentally limited by the availability of labeled data, which are expensive and labor-intensive to obtain at scale. The scarcity of high-quality annotations hinders the generalization and performance of existing models. To overcome this limitation, we propose leveraging large volumes of unlabeled data through a semi-supervised learning framework, enabling the model to extract informative cues from unlabeled samples and improve its predictive performance. In this work, we introduce TSE-Net, a self-training pipeline for semi-supervised monocular height estimation. The pipeline integrates teacher, student, and exam networks. The student network is trained on unlabeled data using pseudo-labels generated by the teacher network, while the exam network functions as a temporal ensemble of the student network to stabilize performance. The teacher network is formulated as a joint regression and classification model: the regression branch predicts height values that serve as pseudo-labels, and the classification branch predicts height value classes along with class probabilities, which are used to filter pseudo-labels. Height value classes are defined using a hierarchical bi-cut strategy to address the inherent long-tailed distribution of heights, and the predicted class probabilities are calibrated with a Plackett-Luce model to reflect the expected accuracy of pseudo-labels. We evaluate the proposed pipeline on three datasets spanning different resolutions and imaging modalities. Codes are available at https://github.com/zhu-xlab/tse-net.",
            "headline_zh": "提出TSE-Net半监督学习框架，从单张遥感图像估计高度以解决标注数据稀缺问题。",
            "intro_zh": [
                "核心问题：单目高度估计依赖标注数据，但标注成本高且稀缺，限制模型泛化。",
                "方法要点：采用教师-学生-考试网络自训练，教师生成伪标签，学生利用无标签数据学习。",
                "实验或效果：在三个不同分辨率和成像模式的数据集上评估，代码已开源。"
            ],
            "tags_zh": [
                "单目高度估计",
                "半监督学习",
                "自训练",
                "伪标签",
                "遥感图像",
                "长尾分布"
            ],
            "_index": 26
        },
        {
            "title": "Robust Defense Strategies for Multimodal Contrastive Learning: Efficient Fine-tuning Against Backdoor Attacks",
            "authors": [
                "Md. Iqbal Hossain",
                "Afia Sajeeda",
                "Neeresh Kumar Perla",
                "Ming Shao"
            ],
            "arxiv_id": "2511.13545v1",
            "summary": "The advent of multimodal deep learning models, such as CLIP, has unlocked new frontiers in a wide range of applications, from image-text understanding to classification tasks. However, these models are not safe for adversarial attacks, particularly backdoor attacks, which can subtly manipulate model behavior. Moreover, existing defense methods typically involve training from scratch or fine-tuning using a large dataset without pinpointing the specific labels that are affected. In this study, we introduce an innovative strategy to enhance the robustness of multimodal contrastive learning models against such attacks. In particular, given a poisoned CLIP model, our approach can identify the backdoor trigger and pinpoint the victim samples and labels in an efficient manner. To that end, an image segmentation ``oracle'' is introduced as the supervisor for the output of the poisoned CLIP. We develop two algorithms to rectify the poisoned model: (1) differentiating between CLIP and Oracle's knowledge to identify potential triggers; (2) pinpointing affected labels and victim samples, and curating a compact fine-tuning dataset. With this knowledge, we are allowed to rectify the poisoned CLIP model to negate backdoor effects. Extensive experiments on visual recognition benchmarks demonstrate our strategy is effective in CLIP-based backdoor defense.",
            "headline_zh": "提出高效微调策略以防御多模态对比学习中的后门攻击",
            "intro_zh": [
                "多模态模型如CLIP易受后门攻击，现有防御方法效率低且不精确",
                "引入图像分割oracle监督，识别触发器和受害样本，构建紧凑微调数据集",
                "在视觉识别基准上验证策略有效，能消除后门影响"
            ],
            "tags_zh": [
                "多模态对比学习",
                "后门攻击防御",
                "CLIP模型",
                "图像分割oracle",
                "高效微调",
                "视觉识别"
            ],
            "_index": 27
        },
        {
            "title": "BootOOD: Self-Supervised Out-of-Distribution Detection via Synthetic Sample Exposure under Neural Collapse",
            "authors": [
                "Yuanchao Wang",
                "Tian Qin",
                "Eduardo Valle",
                "Bruno Abrahao"
            ],
            "arxiv_id": "2511.13539v1",
            "summary": "Out-of-distribution (OOD) detection is critical for deploying image classifiers in safety-sensitive environments, yet existing detectors often struggle when OOD samples are semantically similar to the in-distribution (ID) classes. We present BootOOD, a fully self-supervised OOD detection framework that bootstraps exclusively from ID data and is explicitly designed to handle semantically challenging OOD samples. BootOOD synthesizes pseudo-OOD features through simple transformations of ID representations and leverages Neural Collapse (NC), where ID features cluster tightly around class means with consistent feature norms. Unlike prior approaches that aim to constrain OOD features into subspaces orthogonal to the collapsed ID means, BootOOD introduces a lightweight auxiliary head that performs radius-based classification on feature norms. This design decouples OOD detection from the primary classifier and imposes a relaxed requirement: OOD samples are learned to have smaller feature norms than ID features, which is easier to satisfy when ID and OOD are semantically close. Experiments on CIFAR-10, CIFAR-100, and ImageNet-200 show that BootOOD outperforms prior post-hoc methods, surpasses training-based methods without outlier exposure, and is competitive with state-of-the-art outlier-exposure approaches while maintaining or improving ID accuracy.",
            "headline_zh": "提出BootOOD自监督OOD检测框架，通过合成样本暴露处理语义相似OOD样本。",
            "intro_zh": [
                "核心问题：现有OOD检测器在语义相似OOD样本上表现不佳。",
                "方法要点：利用神经崩溃合成伪OOD特征，基于特征范数进行半径分类。",
                "实验效果：在多个数据集上优于现有方法，保持或提升ID准确率。"
            ],
            "tags_zh": [
                "自监督学习",
                "分布外检测",
                "神经崩溃",
                "特征范数分类",
                "图像分类"
            ],
            "_index": 28
        },
        {
            "title": "Accuracy is Not Enough: Poisoning Interpretability in Federated Learning via Color Skew",
            "authors": [
                "Farhin Farhad Riya",
                "Shahinul Hoque",
                "Jinyuan Stella Sun",
                "Olivera Kotevska"
            ],
            "arxiv_id": "2511.13535v1",
            "summary": "As machine learning models are increasingly deployed in safety-critical domains, visual explanation techniques have become essential tools for supporting transparency. In this work, we reveal a new class of attacks that compromise model interpretability without affecting accuracy. Specifically, we show that small color perturbations applied by adversarial clients in a federated learning setting can shift a model's saliency maps away from semantically meaningful regions while keeping the prediction unchanged. The proposed saliency-aware attack framework, called Chromatic Perturbation Module, systematically crafts adversarial examples by altering the color contrast between foreground and background in a way that disrupts explanation fidelity. These perturbations accumulate across training rounds, poisoning the global model's internal feature attributions in a stealthy and persistent manner. Our findings challenge a common assumption in model auditing that correct predictions imply faithful explanations and demonstrate that interpretability itself can be an attack surface. We evaluate this vulnerability across multiple datasets and show that standard training pipelines are insufficient to detect or mitigate explanation degradation, especially in the federated learning setting, where subtle color perturbations are harder to discern. Our attack reduces peak activation overlap in Grad-CAM explanations by up to 35% while preserving classification accuracy above 96% on all evaluated datasets.",
            "headline_zh": "提出联邦学习中通过颜色扰动毒化模型可解释性的攻击方法",
            "intro_zh": [
                "核心问题：模型预测准确但可解释性被破坏，挑战了准确预测即忠实解释的假设。",
                "方法要点：使用色度扰动模块，通过改变前景背景颜色对比来操纵显著性图。",
                "实验或效果：攻击使Grad-CAM峰值激活重叠降低达35%，同时保持分类准确率高于96%。"
            ],
            "tags_zh": [
                "联邦学习",
                "模型可解释性",
                "对抗攻击",
                "显著性图",
                "颜色扰动"
            ],
            "_index": 29
        },
        {
            "title": "Minimax Multi-Target Conformal Prediction with Applications to Imaging Inverse Problems",
            "authors": [
                "Jeffrey Wen",
                "Rizwan Ahmad",
                "Philip Schniter"
            ],
            "arxiv_id": "2511.13533v1",
            "summary": "In ill-posed imaging inverse problems, uncertainty quantification remains a fundamental challenge, especially in safety-critical applications. Recently, conformal prediction has been used to quantify the uncertainty that the inverse problem contributes to downstream tasks like image classification, image quality assessment, fat mass quantification, etc. While existing works handle only a scalar estimation target, practical applications often involve multiple targets. In response, we propose an asymptotically minimax approach to multi-target conformal prediction that provides tight prediction intervals while ensuring joint marginal coverage. We then outline how our minimax approach can be applied to multi-metric blind image quality assessment, multi-task uncertainty quantification, and multi-round measurement acquisition. Finally, we numerically demonstrate the benefits of our minimax method, relative to existing multi-target conformal prediction methods, using both synthetic and magnetic resonance imaging (MRI) data.",
            "headline_zh": "提出渐近极小极大多目标共形预测方法，以解决不适定成像逆问题中的不确定性量化挑战。",
            "intro_zh": [
                "核心问题：不适定成像逆问题中多目标不确定性量化困难，尤其在安全关键应用。",
                "方法要点：渐近极小极大方法确保联合边际覆盖，提供紧密预测区间。",
                "实验或效果：在合成和MRI数据上验证优于现有方法，应用于多指标盲图像质量评估等。"
            ],
            "tags_zh": [
                "共形预测",
                "成像逆问题",
                "不确定性量化",
                "多目标预测",
                "极小极大优化",
                "盲图像质量评估"
            ],
            "_index": 30
        },
        {
            "title": "Towards Affect-Adaptive Human-Robot Interaction: A Protocol for Multimodal Dataset Collection on Social Anxiety",
            "authors": [
                "Vesna Poprcova",
                "Iulia Lefter",
                "Matthias Wieser",
                "Martijn Warnier",
                "Frances Brazier"
            ],
            "arxiv_id": "2511.13530v1",
            "summary": "Social anxiety is a prevalent condition that affects interpersonal interactions and social functioning. Recent advances in artificial intelligence and social robotics offer new opportunities to examine social anxiety in the human-robot interaction context. Accurate detection of affective states and behaviours associated with social anxiety requires multimodal datasets, where each signal modality provides complementary insights into its manifestations. However, such datasets remain scarce, limiting progress in both research and applications. To address this, this paper presents a protocol for multimodal dataset collection designed to reflect social anxiety in a human-robot interaction context. The dataset will consist of synchronised audio, video, and physiological recordings acquired from at least 70 participants, grouped according to their level of social anxiety, as they engage in approximately 10-minute interactive Wizard-of-Oz role-play scenarios with the Furhat social robot under controlled experimental conditions. In addition to multimodal data, the dataset will be enriched with contextual data providing deeper insight into individual variability in social anxiety responses. This work can contribute to research on affect-adaptive human-robot interaction by providing support for robust multimodal detection of social anxiety.",
            "headline_zh": "提出多模态数据集收集协议以支持社交焦虑检测的人机交互研究",
            "intro_zh": [
                "社交焦虑影响人际互动，但相关多模态数据集稀缺，限制研究进展。",
                "设计协议收集同步音频、视频和生理数据，结合情境数据增强分析。",
                "在受控条件下，使用Furhat机器人进行角色扮演实验，涉及至少70名参与者。"
            ],
            "tags_zh": [
                "社交焦虑检测",
                "多模态数据集",
                "人机交互",
                "生理信号",
                "情感计算",
                "机器人实验"
            ],
            "_index": 31
        },
        {
            "title": "Mapping the Vanishing and Transformation of Urban Villages in China",
            "authors": [
                "Wenyu Zhang",
                "Yao Tong",
                "Yiqiu Liu",
                "Rui Cao"
            ],
            "arxiv_id": "2511.13507v1",
            "summary": "Urban villages (UVs), informal settlements embedded within China's urban fabric, have undergone widespread demolition and redevelopment in recent decades. However, there remains a lack of systematic evaluation of whether the demolished land has been effectively reused, raising concerns about the efficacy and sustainability of current redevelopment practices. To address the gap, this study proposes a deep learning-based framework to monitor the spatiotemporal changes of UVs in China. Specifically, semantic segmentation of multi-temporal remote sensing imagery is first used to map evolving UV boundaries, and then post-demolition land use is classified into six categories based on the \"remained-demolished-redeveloped\" phase: incomplete demolition, vacant land, construction sites, buildings, green spaces, and others. Four representative cities from China's four economic regions were selected as the study areas, i.e., Guangzhou (East), Zhengzhou (Central), Xi'an (West), and Harbin (Northeast). The results indicate: 1) UV redevelopment processes were frequently prolonged; 2) redevelopment transitions primarily occurred in peripheral areas, whereas urban cores remained relatively stable; and 3) three spatiotemporal transformation pathways, i.e., synchronized redevelopment, delayed redevelopment, and gradual optimization, were revealed. This study highlights the fragmented, complex and nonlinear nature of UV redevelopment, underscoring the need for tiered and context-sensitive planning strategies. By linking spatial dynamics with the context of redevelopment policies, the findings offer valuable empirical insights that support more inclusive, efficient, and sustainable urban renewal, while also contributing to a broader global understanding of informal settlement transformations.",
            "headline_zh": "提出基于深度学习的框架以监测中国城中村的时空变化",
            "intro_zh": [
                "核心问题：缺乏对城中村拆除后土地利用的系统评估，影响城市更新的可持续性。",
                "方法要点：使用多时相遥感影像语义分割，分类拆迁后土地利用为六种类型。",
                "实验或效果：在四个代表性城市分析，揭示拆迁过程漫长、外围优先转变等时空模式。"
            ],
            "tags_zh": [
                "城中村监测",
                "语义分割",
                "遥感影像分析",
                "土地利用分类",
                "城市更新",
                "深度学习框架"
            ],
            "_index": 32
        },
        {
            "title": "Language-Guided Invariance Probing of Vision-Language Models",
            "authors": [
                "Jae Joong Lee"
            ],
            "arxiv_id": "2511.13494v1",
            "summary": "Recent vision-language models (VLMs) such as CLIP, OpenCLIP, EVA02-CLIP and SigLIP achieve strong zero-shot performance, but it is unclear how reliably they respond to controlled linguistic perturbations. We introduce Language-Guided Invariance Probing (LGIP), a benchmark that measures (i) invariance to meaning-preserving paraphrases and (ii) sensitivity to meaning-changing semantic flips in image-text matching. Using 40k MS COCO images with five human captions each, we automatically generate paraphrases and rule-based flips that alter object category, color or count, and summarize model behavior with an invariance error, a semantic sensitivity gap and a positive-rate statistic.\n  Across nine VLMs, EVA02-CLIP and large OpenCLIP variants lie on a favorable invariance-sensitivity frontier, combining low paraphrase-induced variance with consistently higher scores for original captions than for their flipped counterparts. In contrast, SigLIP and SigLIP2 show much larger invariance error and often prefer flipped captions to the human descriptions, especially for object and color edits. These failures are largely invisible to standard retrieval metrics, indicating that LGIP provides a model-agnostic diagnostic for the linguistic robustness of VLMs beyond conventional accuracy scores.",
            "headline_zh": "提出语言引导不变性探测以评估视觉语言模型的语言鲁棒性",
            "intro_zh": [
                "核心问题：视觉语言模型对语言扰动的响应可靠性未知，标准检索指标可能掩盖缺陷",
                "方法要点：自动生成释义和语义翻转，定义不变性误差和语义敏感度差距指标",
                "实验或效果：EVA02-CLIP和大型OpenCLIP变体表现稳健，SigLIP系列易偏好翻转描述"
            ],
            "tags_zh": [
                "视觉语言模型",
                "语言鲁棒性",
                "不变性探测",
                "语义翻转",
                "零样本性能",
                "模型诊断"
            ],
            "_index": 33
        },
        {
            "title": "InterMoE: Individual-Specific 3D Human Interaction Generation via Dynamic Temporal-Selective MoE",
            "authors": [
                "Lipeng Wang",
                "Hongxing Fan",
                "Haohua Chen",
                "Zehuan Huang",
                "Lu Sheng"
            ],
            "arxiv_id": "2511.13488v1",
            "summary": "Generating high-quality human interactions holds significant value for applications like virtual reality and robotics. However, existing methods often fail to preserve unique individual characteristics or fully adhere to textual descriptions. To address these challenges, we introduce InterMoE, a novel framework built on a Dynamic Temporal-Selective Mixture of Experts. The core of InterMoE is a routing mechanism that synergistically uses both high-level text semantics and low-level motion context to dispatch temporal motion features to specialized experts. This allows experts to dynamically determine the selection capacity and focus on critical temporal features, thereby preserving specific individual characteristic identities while ensuring high semantic fidelity. Extensive experiments show that InterMoE achieves state-of-the-art performance in individual-specific high-fidelity 3D human interaction generation, reducing FID scores by 9% on the InterHuman dataset and 22% on InterX.",
            "headline_zh": "提出InterMoE框架以解决3D人体交互生成中个体特征保持和语义忠实度问题",
            "intro_zh": [
                "核心问题：现有方法难以保持个体特征和文本描述忠实度",
                "方法要点：使用动态时间选择性MoE，结合文本语义和运动上下文路由特征",
                "实验或效果：在InterHuman和InterX数据集上FID分数分别降低9%和22%"
            ],
            "tags_zh": [
                "3D人体交互生成",
                "混合专家模型",
                "动态路由机制",
                "语义忠实度",
                "个体特征保持"
            ],
            "_index": 34
        },
        {
            "title": "Semantic Document Derendering: SVG Reconstruction via Vision-Language Modeling",
            "authors": [
                "Adam Hazimeh",
                "Ke Wang",
                "Mark Collier",
                "Gilles Baechler",
                "Efi Kokiopoulou",
                "Pascal Frossard"
            ],
            "arxiv_id": "2511.13478v1",
            "summary": "Multimedia documents such as slide presentations and posters are designed to be interactive and easy to modify. Yet, they are often distributed in a static raster format, which limits editing and customization. Restoring their editability requires converting these raster images back into structured vector formats. However, existing geometric raster-vectorization methods, which rely on low-level primitives like curves and polygons, fall short at this task. Specifically, when applied to complex documents like slides, they fail to preserve the high-level structure, resulting in a flat collection of shapes where the semantic distinction between image and text elements is lost. To overcome this limitation, we address the problem of semantic document derendering by introducing SliDer, a novel framework that uses Vision-Language Models (VLMs) to derender slide images as compact and editable Scalable Vector Graphic (SVG) representations. SliDer detects and extracts attributes from individual image and text elements in a raster input and organizes them into a coherent SVG format. Crucially, the model iteratively refines its predictions during inference in a process analogous to human design, generating SVG code that more faithfully reconstructs the original raster upon rendering. Furthermore, we introduce Slide2SVG, a novel dataset comprising raster-SVG pairs of slide documents curated from real-world scientific presentations, to facilitate future research in this domain. Our results demonstrate that SliDer achieves a reconstruction LPIPS of 0.069 and is favored by human evaluators in 82.9% of cases compared to the strongest zero-shot VLM baseline.",
            "headline_zh": "提出SliDer框架，使用视觉语言模型将幻灯片图像转换为可编辑SVG格式。",
            "intro_zh": [
                "问题：现有几何光栅矢量化方法无法保留文档高层语义结构。",
                "方法：利用视觉语言模型检测元素属性，迭代生成SVG代码。",
                "效果：在Slide2SVG数据集上，LPIPS为0.069，人类评估偏好达82.9%。"
            ],
            "tags_zh": [
                "语义文档去渲染",
                "视觉语言模型",
                "SVG重建",
                "幻灯片处理",
                "光栅矢量化"
            ],
            "_index": 35
        },
        {
            "title": "Contact-Safe Reinforcement Learning with ProMP Reparameterization and Energy Awareness",
            "authors": [
                "Bingkun Huang",
                "Yuhe Gong",
                "Zewen Yang",
                "Tianyu Ren",
                "Luis Figueredo"
            ],
            "arxiv_id": "2511.13459v1",
            "summary": "Reinforcement learning (RL) approaches based on Markov Decision Processes (MDPs) are predominantly applied in the robot joint space, often relying on limited task-specific information and partial awareness of the 3D environment. In contrast, episodic RL has demonstrated advantages over traditional MDP-based methods in terms of trajectory consistency, task awareness, and overall performance in complex robotic tasks. Moreover, traditional step-wise and episodic RL methods often neglect the contact-rich information inherent in task-space manipulation, especially considering the contact-safety and robustness. In this work, contact-rich manipulation tasks are tackled using a task-space, energy-safe framework, where reliable and safe task-space trajectories are generated through the combination of Proximal Policy Optimization (PPO) and movement primitives. Furthermore, an energy-aware Cartesian Impedance Controller objective is incorporated within the proposed framework to ensure safe interactions between the robot and the environment. Our experimental results demonstrate that the proposed framework outperforms existing methods in handling tasks on various types of surfaces in 3D environments, achieving high success rates as well as smooth trajectories and energy-safe interactions.",
            "headline_zh": "提出任务空间能量安全框架，结合PPO与运动基元，解决接触丰富操作中的安全与鲁棒性问题。",
            "intro_zh": [
                "核心问题：传统强化学习在机器人任务空间操作中忽视接触安全和能量感知，导致交互不安全。",
                "方法要点：结合PPO与运动基元，引入能量感知笛卡尔阻抗控制器，确保安全交互。",
                "实验或效果：在多种3D环境表面任务中，优于现有方法，实现高成功率和平滑轨迹。"
            ],
            "tags_zh": [
                "强化学习",
                "机器人操作",
                "接触安全",
                "能量感知",
                "运动基元",
                "笛卡尔阻抗控制"
            ],
            "_index": 36
        },
        {
            "title": "Trust in Vision-Language Models: Insights from a Participatory User Workshop",
            "authors": [
                "Agnese Chiatti",
                "Lara Piccolo",
                "Sara Bernardini",
                "Matteo Matteucci",
                "Viola Schiaffonati"
            ],
            "arxiv_id": "2511.13458v1",
            "summary": "With the growing deployment of Vision-Language Models (VLMs), pre-trained on large image-text and video-text datasets, it is critical to equip users with the tools to discern when to trust these systems. However, examining how user trust in VLMs builds and evolves remains an open problem. This problem is exacerbated by the increasing reliance on AI models as judges for experimental validation, to bypass the cost and implications of running participatory design studies directly with users. Following a user-centred approach, this paper presents preliminary results from a workshop with prospective VLM users. Insights from this pilot workshop inform future studies aimed at contextualising trust metrics and strategies for participants' engagement to fit the case of user-VLM interaction.",
            "headline_zh": "通过用户工作坊探讨视觉语言模型信任问题，为未来研究提供初步见解",
            "intro_zh": [
                "核心问题：用户对视觉语言模型的信任如何形成和演变，缺乏系统研究",
                "方法要点：采用以用户为中心的方法，开展前瞻性用户工作坊收集数据",
                "实验或效果：工作坊结果指导未来研究，以情境化信任指标和用户参与策略"
            ],
            "tags_zh": [
                "视觉语言模型",
                "用户信任",
                "参与式设计",
                "用户工作坊",
                "信任指标"
            ],
            "_index": 37
        },
        {
            "title": "Unlocking the Forgery Detection Potential of Vanilla MLLMs: A Novel Training-Free Pipeline",
            "authors": [
                "Rui Zuo",
                "Qinyue Tong",
                "Zhe-Ming Lu",
                "Ziqian Lu"
            ],
            "arxiv_id": "2511.13442v1",
            "summary": "With the rapid advancement of artificial intelligence-generated content (AIGC) technologies, including multimodal large language models (MLLMs) and diffusion models, image generation and manipulation have become remarkably effortless. Existing image forgery detection and localization (IFDL) methods often struggle to generalize across diverse datasets and offer limited interpretability. Nowadays, MLLMs demonstrate strong generalization potential across diverse vision-language tasks, and some studies introduce this capability to IFDL via large-scale training. However, such approaches cost considerable computational resources, while failing to reveal the inherent generalization potential of vanilla MLLMs to address this problem. Inspired by this observation, we propose Foresee, a training-free MLLM-based pipeline tailored for image forgery analysis. It eliminates the need for additional training and enables a lightweight inference process, while surpassing existing MLLM-based methods in both tamper localization accuracy and the richness of textual explanations. Foresee employs a type-prior-driven strategy and utilizes a Flexible Feature Detector (FFD) module to specifically handle copy-move manipulations, thereby effectively unleashing the potential of vanilla MLLMs in the forensic domain. Extensive experiments demonstrate that our approach simultaneously achieves superior localization accuracy and provides more comprehensive textual explanations. Moreover, Foresee exhibits stronger generalization capability, outperforming existing IFDL methods across various tampering types, including copy-move, splicing, removal, local enhancement, deepfake, and AIGC-based editing. The code will be released in the final version.",
            "headline_zh": "提出Foresee训练免费管道以解决图像伪造检测泛化与解释性问题",
            "intro_zh": [
                "现有图像伪造检测方法泛化能力差且解释性有限",
                "Foresee无需训练，利用类型先验策略和FFD模块释放MLLM潜力",
                "实验显示在多种篡改类型中定位准确且提供丰富文本解释"
            ],
            "tags_zh": [
                "图像伪造检测",
                "多模态大语言模型",
                "训练免费方法",
                "篡改定位",
                "泛化能力",
                "文本解释"
            ],
            "_index": 38
        },
        {
            "title": "FUSE: A Flow-based Mapping Between Shapes",
            "authors": [
                "Lorenzo Olearo",
                "Giulio Viganò",
                "Daniele Baieri",
                "Filippo Maggioli",
                "Simone Melzi"
            ],
            "arxiv_id": "2511.13431v1",
            "summary": "We introduce a novel neural representation for maps between 3D shapes based on flow-matching models, which is computationally efficient and supports cross-representation shape matching without large-scale training or data-driven procedures. 3D shapes are represented as the probability distribution induced by a continuous and invertible flow mapping from a fixed anchor distribution. Given a source and a target shape, the composition of the inverse flow (source to anchor) with the forward flow (anchor to target), we continuously map points between the two surfaces. By encoding the shapes with a pointwise task-tailored embedding, this construction provides an invertible and modality-agnostic representation of maps between shapes across point clouds, meshes, signed distance fields (SDFs), and volumetric data. The resulting representation consistently achieves high coverage and accuracy across diverse benchmarks and challenging settings in shape matching. Beyond shape matching, our framework shows promising results in other tasks, including UV mapping and registration of raw point cloud scans of human bodies.",
            "headline_zh": "提出基于流匹配的神经映射方法，实现高效跨表示3D形状匹配。",
            "intro_zh": [
                "核心问题：3D形状间映射需高效处理点云、网格等多种表示。",
                "方法要点：使用流模型构建可逆映射，从锚分布连续变换形状。",
                "实验效果：在多个基准测试中实现高覆盖率和准确性。"
            ],
            "tags_zh": [
                "3D形状匹配",
                "流匹配模型",
                "可逆映射",
                "跨表示学习",
                "UV映射"
            ],
            "_index": 39
        },
        {
            "title": "VOPE: Revisiting Hallucination of Vision-Language Models in Voluntary Imagination Task",
            "authors": [
                "Xingming Long",
                "Jie Zhang",
                "Shiguang Shan",
                "Xilin Chen"
            ],
            "arxiv_id": "2511.13420v1",
            "summary": "Most research on hallucinations in Large Vision-Language Models (LVLMs) focuses on factual description tasks that prohibit any output absent from the image. However, little attention has been paid to hallucinations in voluntary imagination tasks, e.g., story writing, where the models are expected to generate novel content beyond the given image. In these tasks, it is inappropriate to simply regard such imagined novel content as hallucinations. To address this limitation, we introduce Voluntary-imagined Object Presence Evaluation (VOPE)-a novel method to assess LVLMs' hallucinations in voluntary imagination tasks via presence evaluation. Specifically, VOPE poses recheck-based questions to evaluate how an LVLM interprets the presence of the imagined objects in its own response. The consistency between the model's interpretation and the object's presence in the image is then used to determine whether the model hallucinates when generating the response. We apply VOPE to several mainstream LVLMs and hallucination mitigation methods, revealing two key findings: (1) most LVLMs hallucinate heavily during voluntary imagination, and their performance in presence evaluation is notably poor on imagined objects; (2) existing hallucination mitigation methods show limited effect in voluntary imagination tasks, making this an important direction for future research.",
            "headline_zh": "提出VOPE方法以评估大视觉语言模型在自愿想象任务中的幻觉问题",
            "intro_zh": [
                "核心问题：现有研究忽视大视觉语言模型在自愿想象任务中的幻觉，如故事写作",
                "方法要点：VOPE通过重检查问题评估模型对想象对象存在的解释一致性",
                "实验或效果：多数模型在自愿想象中幻觉严重，现有缓解方法效果有限"
            ],
            "tags_zh": [
                "大视觉语言模型",
                "幻觉评估",
                "自愿想象任务",
                "存在评估",
                "重检查问题"
            ],
            "_index": 40
        },
        {
            "title": "Delineate Anything Flow: Fast, Country-Level Field Boundary Detection from Any Source",
            "authors": [
                "Mykola Lavreniuk",
                "Nataliia Kussul",
                "Andrii Shelestov",
                "Yevhenii Salii",
                "Volodymyr Kuzin",
                "Sergii Skakun",
                "Zoltan Szantoi"
            ],
            "arxiv_id": "2511.13417v1",
            "summary": "Accurate delineation of agricultural field boundaries from satellite imagery is essential for land management and crop monitoring, yet existing methods often produce incomplete boundaries, merge adjacent fields, and struggle to scale. We present the Delineate Anything Flow (DelAnyFlow) methodology, a resolution-agnostic approach for large-scale field boundary mapping. DelAnyFlow combines the DelAny instance segmentation model, based on a YOLOv11 backbone and trained on the large-scale Field Boundary Instance Segmentation-22M (FBIS 22M) dataset, with a structured post-processing, merging, and vectorization sequence to generate topologically consistent vector boundaries. FBIS 22M, the largest dataset of its kind, contains 672,909 multi-resolution image patches (0.25-10m) and 22.9million validated field instances. The DelAny model delivers state-of-the-art accuracy with over 100% higher mAP and 400x faster inference than SAM2. DelAny demonstrates strong zero-shot generalization and supports national-scale applications: using Sentinel 2 data for 2024, DelAnyFlow generated a complete field boundary layer for Ukraine (603,000km2) in under six hours on a single workstation. DelAnyFlow outputs significantly improve boundary completeness relative to operational products from Sinergise Solutions and NASA Harvest, particularly in smallholder and fragmented systems (0.25-1ha). For Ukraine, DelAnyFlow delineated 3.75M fields at 5m and 5.15M at 2.5m, compared to 2.66M detected by Sinergise Solutions and 1.69M by NASA Harvest. This work delivers a scalable, cost-effective methodology for field delineation in regions lacking digital cadastral data. A project landing page with links to model weights, code, national-scale vector outputs, and dataset is available at https://lavreniuk.github.io/Delineate-Anything/.",
            "headline_zh": "提出Delineate Anything Flow方法，用于快速、大规模农业田块边界检测",
            "intro_zh": [
                "现有方法常产生不完整边界、合并相邻田块，且难以扩展。",
                "结合DelAny实例分割模型与结构化后处理，生成拓扑一致矢量边界。",
                "在乌克兰应用中，6小时内完成全国边界检测，精度和速度显著提升。"
            ],
            "tags_zh": [
                "实例分割",
                "农业遥感",
                "边界检测",
                "大规模数据集",
                "零样本泛化",
                "矢量边界"
            ],
            "_index": 41
        },
        {
            "title": "Attention Grounded Enhancement for Visual Document Retrieval",
            "authors": [
                "Wanqing Cui",
                "Wei Huang",
                "Yazhi Guo",
                "Yibo Hu",
                "Meiguang Jin",
                "Junfeng Ma",
                "Keping Bi"
            ],
            "arxiv_id": "2511.13415v1",
            "summary": "Visual document retrieval requires understanding heterogeneous and multi-modal content to satisfy information needs. Recent advances use screenshot-based document encoding with fine-grained late interaction, significantly improving retrieval performance. However, retrievers are still trained with coarse global relevance labels, without revealing which regions support the match. As a result, retrievers tend to rely on surface-level cues and struggle to capture implicit semantic connections, hindering their ability to handle non-extractive queries. To alleviate this problem, we propose a \\textbf{A}ttention-\\textbf{G}rounded \\textbf{RE}triever \\textbf{E}nhancement (AGREE) framework. AGREE leverages cross-modal attention from multimodal large language models as proxy local supervision to guide the identification of relevant document regions. During training, AGREE combines local signals with the global signals to jointly optimize the retriever, enabling it to learn not only whether documents match, but also which content drives relevance. Experiments on the challenging ViDoRe V2 benchmark show that AGREE significantly outperforms the global-supervision-only baseline. Quantitative and qualitative analyses further demonstrate that AGREE promotes deeper alignment between query terms and document regions, moving beyond surface-level matching toward more accurate and interpretable retrieval. Our code is available at: https://anonymous.4open.science/r/AGREE-2025.",
            "headline_zh": "提出AGREE框架以解决视觉文档检索中依赖表面线索的问题",
            "intro_zh": [
                "核心问题：检索器依赖全局相关标签，难以捕捉隐式语义连接",
                "方法要点：利用跨模态注意力作为局部监督，结合全局信号优化检索器",
                "实验或效果：在ViDoRe V2基准上显著优于仅全局监督基线"
            ],
            "tags_zh": [
                "视觉文档检索",
                "跨模态注意力",
                "局部监督",
                "检索增强",
                "多模态理解"
            ],
            "_index": 42
        },
        {
            "title": "What Color Is It? A Text-Interference Multimodal Hallucination Benchmark",
            "authors": [
                "Jinkun Zhao",
                "Lei Huang",
                "Wenjun Wu"
            ],
            "arxiv_id": "2511.13400v1",
            "summary": "With the rapid advancement of Large Models, numerous text-and-vision-fused Multimodal Large Models (MLMs) have emerged. However, these MLMs remain susceptible to informational interference in visual perception, particularly in color perception, which introduces an additional risk of hallucination. To validate this hypothesis, we introduce the \"What Color Is It\" dataset, a novel benchmark constructed using a simple method to trigger single-modality visual hallucination in MLMs. Based on this dataset, we further investigate the underlying causes of hallucination in the visual modality of MLMs and propose potential solutions to enhance their robustness.",
            "headline_zh": "提出'What Color Is It'基准以解决多模态大模型颜色感知幻觉问题",
            "intro_zh": [
                "多模态大模型易受文本干扰，在颜色感知中产生视觉幻觉。",
                "构建新基准数据集，通过简单方法触发单模态视觉幻觉。",
                "分析幻觉成因并提出增强模型鲁棒性的潜在解决方案。"
            ],
            "tags_zh": [
                "多模态大模型",
                "视觉幻觉",
                "颜色感知",
                "基准数据集",
                "模型鲁棒性"
            ],
            "_index": 43
        },
        {
            "title": "TripleFDS: Triple Feature Disentanglement and Synthesis for Scene Text Editing",
            "authors": [
                "Yuchen Bao",
                "Yiting Wang",
                "Wenjian Huang",
                "Haowei Wang",
                "Shen Chen",
                "Taiping Yao",
                "Shouhong Ding",
                "Jianguo Zhang"
            ],
            "arxiv_id": "2511.13399v1",
            "summary": "Scene Text Editing (STE) aims to naturally modify text in images while preserving visual consistency, the decisive factors of which can be divided into three parts, i.e., text style, text content, and background. Previous methods have struggled with incomplete disentanglement of editable attributes, typically addressing only one aspect - such as editing text content - thus limiting controllability and visual consistency. To overcome these limitations, we propose TripleFDS, a novel framework for STE with disentangled modular attributes, and an accompanying dataset called SCB Synthesis. SCB Synthesis provides robust training data for triple feature disentanglement by utilizing the \"SCB Group\", a novel construct that combines three attributes per image to generate diverse, disentangled training groups. Leveraging this construct as a basic training unit, TripleFDS first disentangles triple features, ensuring semantic accuracy through inter-group contrastive regularization and reducing redundancy through intra-sample multi-feature orthogonality. In the synthesis phase, TripleFDS performs feature remapping to prevent \"shortcut\" phenomena during reconstruction and mitigate potential feature leakage. Trained on 125,000 SCB Groups, TripleFDS achieves state-of-the-art image fidelity (SSIM of 44.54) and text accuracy (ACC of 93.58%) on the mainstream STE benchmarks. Besides superior performance, the more flexible editing of TripleFDS supports new operations such as style replacement and background transfer. Code: https://github.com/yusenbao01/TripleFDS",
            "headline_zh": "提出TripleFDS框架以解决场景文本编辑中的特征解耦与合成问题",
            "intro_zh": [
                "核心问题：场景文本编辑中文本风格、内容和背景特征解耦不完整，限制可控性和视觉一致性。",
                "方法要点：使用SCB Group数据集，通过组间对比正则化和组内多特征正交性实现三特征解耦与合成。",
                "实验或效果：在主流基准上达到SSIM 44.54和ACC 93.58%，支持风格替换和背景转移等新操作。"
            ],
            "tags_zh": [
                "场景文本编辑",
                "特征解耦",
                "图像合成",
                "对比学习",
                "正交正则化",
                "SCB数据集"
            ],
            "_index": 44
        },
        {
            "title": "Descriptor: Distance-Annotated Traffic Perception Question Answering (DTPQA)",
            "authors": [
                "Nikos Theodoridis",
                "Tim Brophy",
                "Reenu Mohandas",
                "Ganesh Sistu",
                "Fiachra Collins",
                "Anthony Scanlan",
                "Ciaran Eising"
            ],
            "arxiv_id": "2511.13397v1",
            "summary": "The remarkable progress of Vision-Language Models (VLMs) on a variety of tasks has raised interest in their application to automated driving. However, for these models to be trusted in such a safety-critical domain, they must first possess robust perception capabilities, i.e., they must be capable of understanding a traffic scene, which can often be highly complex, with many things happening simultaneously. Moreover, since critical objects and agents in traffic scenes are often at long distances, we require systems with not only strong perception capabilities at close distances (up to 20 meters), but also at long (30+ meters) range. Therefore, it is important to evaluate the perception capabilities of these models in isolation from other skills like reasoning or advanced world knowledge. Distance-Annotated Traffic Perception Question Answering (DTPQA) is a Visual Question Answering (VQA) benchmark designed specifically for this purpose: it can be used to evaluate the perception systems of VLMs in traffic scenarios using trivial yet crucial questions relevant to driving decisions. It consists of two parts: a synthetic benchmark (DTP-Synthetic) created using a simulator, and a real-world benchmark (DTP-Real) built on top of existing images of real traffic scenes. Additionally, DTPQA includes distance annotations, i.e., how far the object in question is from the camera. More specifically, each DTPQA sample consists of (at least): (a) an image, (b) a question, (c) the ground truth answer, and (d) the distance of the object in question, enabling analysis of how VLM performance degrades with increasing object distance. In this article, we provide the dataset itself along with the Python scripts used to create it, which can be used to generate additional data of the same kind.",
            "headline_zh": "提出距离标注交通感知问答基准DTPQA，用于评估视觉语言模型在自动驾驶场景中的感知能力。",
            "intro_zh": [
                "核心问题：自动驾驶中视觉语言模型需具备鲁棒感知能力，尤其在复杂交通场景和远距离对象识别。",
                "方法要点：构建合成和真实世界基准，包含图像、问题、答案和对象距离标注，以隔离感知评估。",
                "实验或效果：提供数据集和Python脚本，支持分析模型性能随对象距离增加而下降的情况。"
            ],
            "tags_zh": [
                "视觉语言模型",
                "交通场景感知",
                "距离标注基准",
                "自动驾驶评估",
                "视觉问答"
            ],
            "_index": 45
        },
        {
            "title": "Generalized Denoising Diffusion Codebook Models (gDDCM): Tokenizing images using a pre-trained diffusion model",
            "authors": [
                "Fei Kong"
            ],
            "arxiv_id": "2511.13387v1",
            "summary": "Recently, the Denoising Diffusion Codebook Models (DDCM) was proposed. DDCM leverages the Denoising Diffusion Probabilistic Model (DDPM) and replaces the random noise in the backward process with noise sampled from specific sets according to a predefined rule, thereby enabling image compression. However, DDCM cannot be applied to methods other than DDPM. In this paper, we propose the generalized Denoising Diffusion Compression Model (gDDCM), which extends DDCM to mainstream diffusion models and their variants, including DDPM, Score-Based Models, Consistency Models, and Rectified Flow. We evaluate our method on CIFAR-10 and LSUN Bedroom datasets. Experimental results demonstrate that our approach successfully generalizes DDCM to the aforementioned models and achieves improved performance.",
            "headline_zh": "提出广义去噪扩散码书模型以扩展图像压缩到主流扩散模型",
            "intro_zh": [
                "DDCM无法应用于DDPM以外的扩散模型，限制了图像压缩的通用性。",
                "gDDCM通过替换反向过程噪声，兼容DDPM、基于分数的模型等主流扩散模型。",
                "在CIFAR-10和LSUN Bedroom数据集上验证了方法的泛化性和性能提升。"
            ],
            "tags_zh": [
                "图像压缩",
                "扩散模型",
                "码书模型",
                "去噪过程",
                "泛化方法"
            ],
            "_index": 46
        },
        {
            "title": "Semi-Supervised Multi-Task Learning for Interpretable Quality As- sessment of Fundus Images",
            "authors": [
                "Lucas Gabriel Telesco",
                "Danila Nejamkin",
                "Estefanía Mata",
                "Francisco Filizzola",
                "Kevin Wignall",
                "Lucía Franco Troilo",
                "María de los Angeles Cenoz",
                "Melissa Thompson",
                "Mercedes Leguía",
                "Ignacio Larrabide",
                "José Ignacio Orlando"
            ],
            "arxiv_id": "2511.13353v1",
            "summary": "Retinal image quality assessment (RIQA) supports computer-aided diagnosis of eye diseases. However, most tools classify only overall image quality, without indicating acquisition defects to guide recapture. This gap is mainly due to the high cost of detailed annotations. In this paper, we aim to mitigate this limitation by introducing a hybrid semi-supervised learning approach that combines manual labels for overall quality with pseudo-labels of quality details within a multi-task framework. Our objective is to obtain more interpretable RIQA models without requiring extensive manual labeling. Pseudo-labels are generated by a Teacher model trained on a small dataset and then used to fine-tune a pre-trained model in a multi-task setting. Using a ResNet-18 backbone, we show that these weak annotations improve quality assessment over single-task baselines (F1: 0.875 vs. 0.863 on EyeQ, and 0.778 vs. 0.763 on DeepDRiD), matching or surpassing existing methods. The multi-task model achieved performance statistically comparable to the Teacher for most detail prediction tasks (p > 0.05). In a newly annotated EyeQ subset released with this paper, our model performed similarly to experts, suggesting that pseudo-label noise aligns with expert variability. Our main finding is that the proposed semi-supervised approach not only improves overall quality assessment but also provides interpretable feedback on capture conditions (illumination, clarity, contrast). This enhances interpretability at no extra manual labeling cost and offers clinically actionable outputs to guide image recapture.",
            "headline_zh": "提出半监督多任务学习框架以提升眼底图像质量评估的准确性和可解释性",
            "intro_zh": [
                "核心问题：现有眼底图像质量评估工具缺乏对采集缺陷的详细解释，且标注成本高。",
                "方法要点：结合人工整体质量标签与教师模型生成的伪标签，在多任务框架中训练模型。",
                "实验或效果：在EyeQ和DeepDRiD数据集上性能优于基线，提供可解释的捕获条件反馈。"
            ],
            "tags_zh": [
                "眼底图像质量评估",
                "半监督学习",
                "多任务学习",
                "伪标签生成",
                "可解释性增强",
                "图像采集缺陷检测"
            ],
            "_index": 47
        },
        {
            "title": "YOLO Meets Mixture-of-Experts: Adaptive Expert Routing for Robust Object Detection",
            "authors": [
                "Ori Meiraz",
                "Sharon Shalev",
                "Avishai Weizman"
            ],
            "arxiv_id": "2511.13344v1",
            "summary": "This paper presents a novel Mixture-of-Experts framework for object detection, incorporating adaptive routing among multiple YOLOv9-T experts to enable dynamic feature specialization and achieve higher mean Average Precision (mAP) and Average Recall (AR) compared to a single YOLOv9-T model.",
            "headline_zh": "提出自适应专家路由的混合专家框架，以提升目标检测的鲁棒性和精度。",
            "intro_zh": [
                "核心问题：单一YOLOv9-T模型在目标检测中可能缺乏动态特征专业化能力。",
                "方法要点：集成多个YOLOv9-T专家，通过自适应路由实现动态特征选择。",
                "实验或效果：相比单模型，实现了更高的mAP和AR指标。"
            ],
            "tags_zh": [
                "目标检测",
                "混合专家",
                "自适应路由",
                "YOLOv9-T",
                "特征专业化"
            ],
            "_index": 48
        },
        {
            "title": "ZeroDexGrasp: Zero-Shot Task-Oriented Dexterous Grasp Synthesis with Prompt-Based Multi-Stage Semantic Reasoning",
            "authors": [
                "Juntao Jian",
                "Yi-Lin Wei",
                "Chengjie Mou",
                "Yuhao Lin",
                "Xing Zhu",
                "Yujun Shen",
                "Wei-Shi Zheng",
                "Ruizhen Hu"
            ],
            "arxiv_id": "2511.13327v1",
            "summary": "Task-oriented dexterous grasping holds broad application prospects in robotic manipulation and human-object interaction. However, most existing methods still struggle to generalize across diverse objects and task instructions, as they heavily rely on costly labeled data to ensure task-specific semantic alignment. In this study, we propose \\textbf{ZeroDexGrasp}, a zero-shot task-oriented dexterous grasp synthesis framework integrating Multimodal Large Language Models with grasp refinement to generate human-like grasp poses that are well aligned with specific task objectives and object affordances. Specifically, ZeroDexGrasp employs prompt-based multi-stage semantic reasoning to infer initial grasp configurations and object contact information from task and object semantics, then exploits contact-guided grasp optimization to refine these poses for physical feasibility and task alignment. Experimental results demonstrate that ZeroDexGrasp enables high-quality zero-shot dexterous grasping on diverse unseen object categories and complex task requirements, advancing toward more generalizable and intelligent robotic grasping.",
            "headline_zh": "提出ZeroDexGrasp框架以解决零样本任务导向灵巧抓取泛化问题",
            "intro_zh": [
                "现有方法依赖标注数据，难以泛化到多样物体和任务指令",
                "结合多模态大语言模型与抓取优化，推理初始抓取配置并优化物理可行性",
                "实验显示在未见物体和复杂任务上实现高质量零样本抓取"
            ],
            "tags_zh": [
                "灵巧抓取合成",
                "零样本学习",
                "多模态推理",
                "任务导向抓取",
                "抓取优化"
            ],
            "_index": 49
        },
        {
            "title": "Computer Vision based group activity detection and action spotting",
            "authors": [
                "Narthana Sivalingam",
                "Santhirarajah Sivasthigan",
                "Thamayanthi Mahendranathan",
                "G. M. R. I. Godaliyadda",
                "M. P. B. Ekanayake",
                "H. M. V. R. Herath"
            ],
            "arxiv_id": "2511.13315v1",
            "summary": "Group activity detection in multi-person scenes is challenging due to complex human interactions, occlusions, and variations in appearance over time. This work presents a computer vision based framework for group activity recognition and action spotting using a combination of deep learning models and graph based relational reasoning. The system first applies Mask R-CNN to obtain accurate actor localization through bounding boxes and instance masks. Multiple backbone networks, including Inception V3, MobileNet, and VGG16, are used to extract feature maps, and RoIAlign is applied to preserve spatial alignment when generating actor specific features. The mask information is then fused with the feature maps to obtain refined masked feature representations for each actor. To model interactions between individuals, we construct Actor Relation Graphs that encode appearance similarity and positional relations using methods such as normalized cross correlation, sum of absolute differences, and dot product. Graph Convolutional Networks operate on these graphs to reason about relationships and predict both individual actions and group level activities. Experiments on the Collective Activity dataset demonstrate that the combination of mask based feature refinement, robust similarity search, and graph neural network reasoning leads to improved recognition performance across both crowded and non crowded scenarios. This approach highlights the potential of integrating segmentation, feature extraction, and relational graph reasoning for complex video understanding tasks.",
            "headline_zh": "提出基于掩码特征优化与图卷积网络的群体活动检测框架，以处理多人场景中的复杂交互。",
            "intro_zh": [
                "核心问题：多人场景中群体活动检测因遮挡、外观变化和复杂交互而具挑战性。",
                "方法要点：融合Mask R-CNN定位、多骨干网络特征提取和Actor Relation Graphs建模交互。",
                "实验或效果：在Collective Activity数据集上验证，提升拥挤和非拥挤场景的识别性能。"
            ],
            "tags_zh": [
                "群体活动检测",
                "图卷积网络",
                "掩码特征优化",
                "多人交互建模",
                "视频理解"
            ],
            "_index": 50
        },
        {
            "title": "EL3DD: Extended Latent 3D Diffusion for Language Conditioned Multitask Manipulation",
            "authors": [
                "Jonas Bode",
                "Raphael Memmesheimer",
                "Sven Behnke"
            ],
            "arxiv_id": "2511.13312v1",
            "summary": "Acting in human environments is a crucial capability for general-purpose robots, necessitating a robust understanding of natural language and its application to physical tasks. This paper seeks to harness the capabilities of diffusion models within a visuomotor policy framework that merges visual and textual inputs to generate precise robotic trajectories. By employing reference demonstrations during training, the model learns to execute manipulation tasks specified through textual commands within the robot's immediate environment. The proposed research aims to extend an existing model by leveraging improved embeddings, and adapting techniques from diffusion models for image generation. We evaluate our methods on the CALVIN dataset, proving enhanced performance on various manipulation tasks and an increased long-horizon success rate when multiple tasks are executed in sequence. Our approach reinforces the usefulness of diffusion models and contributes towards general multitask manipulation.",
            "headline_zh": "提出扩展潜在3D扩散模型，以语言条件实现多任务机器人操作",
            "intro_zh": [
                "核心问题：机器人需在人类环境中理解自然语言并执行物理任务",
                "方法要点：融合视觉与文本输入，利用扩散模型生成精确机器人轨迹",
                "实验或效果：在CALVIN数据集上验证，提升多任务操作性能和长时成功率"
            ],
            "tags_zh": [
                "扩散模型",
                "机器人操作",
                "语言条件控制",
                "多任务学习",
                "视觉运动策略"
            ],
            "_index": 51
        },
        {
            "title": "DriveLiDAR4D: Sequential and Controllable LiDAR Scene Generation for Autonomous Driving",
            "authors": [
                "Kaiwen Cai",
                "Xinze Liu",
                "Xia Zhou",
                "Hengtong Hu",
                "Jie Xiang",
                "Luyao Zhang",
                "Xueyang Zhang",
                "Kun Zhan",
                "Yifei Zhan",
                "Xianpeng Lang"
            ],
            "arxiv_id": "2511.13309v1",
            "summary": "The generation of realistic LiDAR point clouds plays a crucial role in the development and evaluation of autonomous driving systems. Although recent methods for 3D LiDAR point cloud generation have shown significant improvements, they still face notable limitations, including the lack of sequential generation capabilities and the inability to produce accurately positioned foreground objects and realistic backgrounds. These shortcomings hinder their practical applicability. In this paper, we introduce DriveLiDAR4D, a novel LiDAR generation pipeline consisting of multimodal conditions and a novel sequential noise prediction model LiDAR4DNet, capable of producing temporally consistent LiDAR scenes with highly controllable foreground objects and realistic backgrounds. To the best of our knowledge, this is the first work to address the sequential generation of LiDAR scenes with full scene manipulation capability in an end-to-end manner. We evaluated DriveLiDAR4D on the nuScenes and KITTI datasets, where we achieved an FRD score of 743.13 and an FVD score of 16.96 on the nuScenes dataset, surpassing the current state-of-the-art (SOTA) method, UniScene, with an performance boost of 37.2% in FRD and 24.1% in FVD, respectively.",
            "headline_zh": "提出DriveLiDAR4D以解决自动驾驶中LiDAR场景序列生成与可控性问题",
            "intro_zh": [
                "现有LiDAR点云生成方法缺乏序列生成能力，且前景对象定位和背景真实性不足",
                "采用多模态条件和LiDAR4DNet模型，实现端到端可控序列生成",
                "在nuScenes数据集上FRD和FVD分数超越SOTA方法，性能提升显著"
            ],
            "tags_zh": [
                "LiDAR点云生成",
                "自动驾驶仿真",
                "序列生成",
                "可控场景生成",
                "多模态条件",
                "噪声预测模型"
            ],
            "_index": 52
        },
        {
            "title": "DAP: A Discrete-token Autoregressive Planner for Autonomous Driving",
            "authors": [
                "Bowen Ye",
                "Bin Zhang",
                "Hang Zhao"
            ],
            "arxiv_id": "2511.13306v1",
            "summary": "Gaining sustainable performance improvement with scaling data and model budget remains a pivotal yet unresolved challenge in autonomous driving. While autoregressive models exhibited promising data-scaling efficiency in planning tasks, predicting ego trajectories alone suffers sparse supervision and weakly constrains how scene evolution should shape ego motion. Therefore, we introduce DAP, a discrete-token autoregressive planner that jointly forecasts BEV semantics and ego trajectories, thereby enforcing comprehensive representation learning and allowing predicted dynamics to directly condition ego motion. In addition, we incorporate a reinforcement-learning-based fine-tuning, which preserves supervised behavior cloning priors while injecting reward-guided improvements. Despite a compact 160M parameter budget, DAP achieves state-of-the-art performance on open-loop metrics and delivers competitive closed-loop results on the NAVSIM benchmark. Overall, the fully discrete-token autoregressive formulation operating on both rasterized BEV and ego actions provides a compact yet scalable planning paradigm for autonomous driving.",
            "headline_zh": "提出DAP离散令牌自回归规划器，联合预测BEV语义与自车轨迹以提升自动驾驶规划。",
            "intro_zh": [
                "核心问题：自动驾驶中数据与模型扩展难以持续提升性能，仅预测自车轨迹监督稀疏且约束弱。",
                "方法要点：采用离散令牌自回归模型，联合预测BEV语义和自车轨迹，强化表示学习与动态条件。",
                "实验或效果：在160M参数下，NAVSIM基准上实现开环SOTA和闭环竞争性结果。"
            ],
            "tags_zh": [
                "自动驾驶规划",
                "自回归模型",
                "BEV语义预测",
                "强化学习微调",
                "离散令牌表示"
            ],
            "_index": 53
        },
        {
            "title": "CorrectAD: A Self-Correcting Agentic System to Improve End-to-end Planning in Autonomous Driving",
            "authors": [
                "Enhui Ma",
                "Lijun Zhou",
                "Tao Tang",
                "Jiahuan Zhang",
                "Junpeng Jiang",
                "Zhan Zhang",
                "Dong Han",
                "Kun Zhan",
                "Xueyang Zhang",
                "XianPeng Lang",
                "Haiyang Sun",
                "Xia Zhou",
                "Di Lin",
                "Kaicheng Yu"
            ],
            "arxiv_id": "2511.13297v1",
            "summary": "End-to-end planning methods are the de facto standard of the current autonomous driving system, while the robustness of the data-driven approaches suffers due to the notorious long-tail problem (i.e., rare but safety-critical failure cases). In this work, we explore whether recent diffusion-based video generation methods (a.k.a. world models), paired with structured 3D layouts, can enable a fully automated pipeline to self-correct such failure cases. We first introduce an agent to simulate the role of product manager, dubbed PM-Agent, which formulates data requirements to collect data similar to the failure cases. Then, we use a generative model that can simulate both data collection and annotation. However, existing generative models struggle to generate high-fidelity data conditioned on 3D layouts. To address this, we propose DriveSora, which can generate spatiotemporally consistent videos aligned with the 3D annotations requested by PM-Agent. We integrate these components into our self-correcting agentic system, CorrectAD. Importantly, our pipeline is an end-to-end model-agnostic and can be applied to improve any end-to-end planner. Evaluated on both nuScenes and a more challenging in-house dataset across multiple end-to-end planners, CorrectAD corrects 62.5% and 49.8% of failure cases, reducing collision rates by 39% and 27%, respectively.",
            "headline_zh": "提出CorrectAD自校正系统以解决自动驾驶端到端规划的长尾问题",
            "intro_zh": [
                "核心问题：数据驱动方法因长尾问题导致罕见但安全关键的失败案例，影响鲁棒性。",
                "方法要点：结合PM-Agent和DriveSora生成模型，构建端到端模型无关的自校正管道。",
                "实验效果：在nuScenes和内部数据集上，CorrectAD校正62.5%和49.8%失败案例，碰撞率降低39%和27%。"
            ],
            "tags_zh": [
                "自动驾驶规划",
                "自校正系统",
                "扩散模型",
                "长尾问题",
                "端到端学习",
                "世界模型"
            ],
            "_index": 54
        },
        {
            "title": "SkyReels-Text: Fine-grained Font-Controllable Text Editing for Poster Design",
            "authors": [
                "Yunjie Yu",
                "Jingchen Wu",
                "Junchen Zhu",
                "Chunze Lin",
                "Guibin Chen"
            ],
            "arxiv_id": "2511.13285v1",
            "summary": "Artistic design such as poster design often demands rapid yet precise modification of textual content while preserving visual harmony and typographic intent, especially across diverse font styles. Although modern image editing models have grown increasingly powerful, they still fall short in fine-grained, font-aware text manipulation, limiting their utility in professional design workflows such as poster editing. To address this issue, we present SkyReels-Text, a novel font-controllable framework for precise poster text editing. Our method enables simultaneous editing of multiple text regions, each rendered in distinct typographic styles, while preserving the visual appearance of non-edited regions. Notably, our model requires neither font labels nor fine-tuning during inference: users can simply provide cropped glyph patches corresponding to their desired typography, even if the font is not included in any standard library. Extensive experiments on multiple datasets, including handwrittent text benchmarks, SkyReels-Text achieves state-of-the-art performance in both text fidelity and visual realism, offering unprecedented control over font families, and stylistic nuances. This work bridges the gap between general-purpose image editing and professional-grade typographic design.",
            "headline_zh": "提出SkyReels-Text框架以解决海报设计中细粒度字体可控文本编辑问题",
            "intro_zh": [
                "核心问题：现有图像编辑模型在细粒度、字体感知的文本操作方面不足，限制专业设计应用",
                "方法要点：无需字体标签或微调，用户提供裁剪字形补丁即可控制多文本区域字体编辑",
                "实验或效果：在多个数据集上实现文本保真度和视觉真实性的最先进性能"
            ],
            "tags_zh": [
                "字体可控文本编辑",
                "海报设计",
                "细粒度编辑",
                "图像编辑模型",
                "字形补丁控制"
            ],
            "_index": 55
        },
        {
            "title": "TabFlash: Efficient Table Understanding with Progressive Question Conditioning and Token Focusing",
            "authors": [
                "Jongha Kim",
                "Minseong Bae",
                "Sanghyeok Lee",
                "Jinsung Yoon",
                "Hyunwoo J. Kim"
            ],
            "arxiv_id": "2511.13283v1",
            "summary": "Table images present unique challenges for effective and efficient understanding due to the need for question-specific focus and the presence of redundant background regions. Existing Multimodal Large Language Model (MLLM) approaches often overlook these characteristics, resulting in uninformative and redundant visual representations. To address these issues, we aim to generate visual features that are both informative and compact to improve table understanding. We first propose progressive question conditioning, which injects the question into Vision Transformer layers with gradually increasing frequency, considering each layer's capacity to handle additional information, to generate question-aware visual features. To reduce redundancy, we introduce a pruning strategy that discards background tokens, thereby improving efficiency. To mitigate information loss from pruning, we further propose token focusing, a training strategy that encourages the model to concentrate essential information in the retained tokens. By combining these approaches, we present TabFlash, an efficient and effective MLLM for table understanding. TabFlash achieves state-of-the-art performance, outperforming both open-source and proprietary MLLMs, while requiring 27% less FLOPs and 30% less memory usage compared to the second-best MLLM.",
            "headline_zh": "提出TabFlash方法，通过渐进问题注入和令牌聚焦提升表格图像理解效率与效果",
            "intro_zh": [
                "核心问题：表格图像存在冗余背景和问题无关区域，导致MLLM视觉特征冗余且低效",
                "方法要点：采用渐进问题注入、令牌剪枝和令牌聚焦训练，生成紧凑且问题感知的视觉特征",
                "实验效果：在表格理解任务中实现SOTA性能，FLOPs和内存使用分别减少27%和30%"
            ],
            "tags_zh": [
                "表格图像理解",
                "多模态大语言模型",
                "渐进问题注入",
                "令牌剪枝",
                "令牌聚焦训练",
                "效率优化"
            ],
            "_index": 56
        },
        {
            "title": "Towards Metric-Aware Multi-Person Mesh Recovery by Jointly Optimizing Human Crowd in Camera Space",
            "authors": [
                "Kaiwen Wang",
                "Kaili Zheng",
                "Yiming Shi",
                "Chenyi Guo",
                "Ji Wu"
            ],
            "arxiv_id": "2511.13282v1",
            "summary": "Multi-person human mesh recovery from a single image is a challenging task, hindered by the scarcity of in-the-wild training data. Prevailing in-the-wild human mesh pseudo-ground-truth (pGT) generation pipelines are single-person-centric, where each human is processed individually without joint optimization. This oversight leads to a lack of scene-level consistency, producing individuals with conflicting depths and scales within the same image. To address this, we introduce Depth-conditioned Translation Optimization (DTO), a novel optimization-based method that jointly refines the camera-space translations of all individuals in a crowd. By leveraging anthropometric priors on human height and depth cues from a monocular depth estimator, DTO solves for a scene-consistent placement of all subjects within a principled Maximum a posteriori (MAP) framework. Applying DTO to the 4D-Humans dataset, we construct DTO-Humans, a new large-scale pGT dataset of 0.56M high-quality, scene-consistent multi-person images, featuring dense crowds with an average of 4.8 persons per image. Furthermore, we propose Metric-Aware HMR, an end-to-end network that directly estimates human mesh and camera parameters in metric scale. This is enabled by a camera branch and a novel relative metric loss that enforces plausible relative scales. Extensive experiments demonstrate that our method achieves state-of-the-art performance on relative depth reasoning and human mesh recovery. Code and data will be released publicly.",
            "headline_zh": "提出DTO和Metric-Aware HMR以解决多人网格恢复中的场景一致性和度量尺度问题",
            "intro_zh": [
                "核心问题：单中心伪真值生成导致多人场景深度和尺度不一致",
                "方法要点：DTO联合优化相机空间平移，Metric-Aware HMR直接估计度量尺度网格",
                "实验或效果：在相对深度推理和网格恢复上达到先进水平，构建DTO-Humans数据集"
            ],
            "tags_zh": [
                "多人网格恢复",
                "场景一致性优化",
                "度量尺度估计",
                "深度条件优化",
                "伪真值数据集"
            ],
            "_index": 57
        },
        {
            "title": "SF-Recon: Simplification-Free Lightweight Building Reconstruction via 3D Gaussian Splatting",
            "authors": [
                "Zihan Li",
                "Tengfei Wang",
                "Wentian Gan",
                "Hao Zhan",
                "Xin Wang",
                "Zongqian Zhan"
            ],
            "arxiv_id": "2511.13278v1",
            "summary": "Lightweight building surface models are crucial for digital city, navigation, and fast geospatial analytics, yet conventional multi-view geometry pipelines remain cumbersome and quality-sensitive due to their reliance on dense reconstruction, meshing, and subsequent simplification. This work presents SF-Recon, a method that directly reconstructs lightweight building surfaces from multi-view images without post-hoc mesh simplification. We first train an initial 3D Gaussian Splatting (3DGS) field to obtain a view-consistent representation. Building structure is then distilled by a normal-gradient-guided Gaussian optimization that selects primitives aligned with roof and wall boundaries, followed by multi-view edge-consistency pruning to enhance structural sharpness and suppress non-structural artifacts without external supervision. Finally, a multi-view depth-constrained Delaunay triangulation converts the structured Gaussian field into a lightweight, structurally faithful building mesh. Based on a proposed SF dataset, the experimental results demonstrate that our SF-Recon can directly reconstruct lightweight building models from multi-view imagery, achieving substantially fewer faces and vertices while maintaining computational efficiency. Website:https://lzh282140127-cell.github.io/SF-Recon-project/",
            "headline_zh": "提出SF-Recon方法，从多视角图像直接重建轻量建筑表面，避免后处理简化",
            "intro_zh": [
                "核心问题：传统多视角几何流程依赖密集重建和网格简化，导致繁琐和质量敏感",
                "方法要点：使用3D高斯溅射和法向梯度优化，选择结构对齐的高斯基元并修剪非结构伪影",
                "实验或效果：在SF数据集上验证，重建模型面数和顶点显著减少，保持计算效率"
            ],
            "tags_zh": [
                "建筑重建",
                "3D高斯溅射",
                "多视角图像",
                "轻量网格",
                "结构优化",
                "Delaunay三角化"
            ],
            "_index": 58
        },
        {
            "title": "Recognition of Abnormal Events in Surveillance Videos using Weakly Supervised Dual-Encoder Models",
            "authors": [
                "Noam Tsfaty",
                "Avishai Weizman",
                "Liav Cohen",
                "Moshe Tshuva",
                "Yehudit Aperstein"
            ],
            "arxiv_id": "2511.13276v1",
            "summary": "We address the challenge of detecting rare and diverse anomalies in surveillance videos using only video-level supervision. Our dual-backbone framework combines convolutional and transformer representations through top-k pooling, achieving 90.7% area under the curve (AUC) on the UCF-Crime dataset.",
            "headline_zh": "提出双主干框架以解决监控视频中罕见异常检测问题，使用视频级弱监督。",
            "intro_zh": [
                "核心问题：监控视频中罕见且多样异常的检测，仅依赖视频级弱监督。",
                "方法要点：结合卷积和Transformer表示，通过top-k池化融合特征。",
                "实验或效果：在UCF-Crime数据集上达到90.7% AUC。"
            ],
            "tags_zh": [
                "异常检测",
                "弱监督学习",
                "双主干网络",
                "视频分析",
                "监控视频"
            ],
            "_index": 59
        },
        {
            "title": "Is your VLM Sky-Ready? A Comprehensive Spatial Intelligence Benchmark for UAV Navigation",
            "authors": [
                "Lingfeng Zhang",
                "Yuchen Zhang",
                "Hongsheng Li",
                "Haoxiang Fu",
                "Yingbo Tang",
                "Hangjun Ye",
                "Long Chen",
                "Xiaojun Liang",
                "Xiaoshuai Hao",
                "Wenbo Ding"
            ],
            "arxiv_id": "2511.13269v1",
            "summary": "Vision-Language Models (VLMs), leveraging their powerful visual perception and reasoning capabilities, have been widely applied in Unmanned Aerial Vehicle (UAV) tasks. However, the spatial intelligence capabilities of existing VLMs in UAV scenarios remain largely unexplored, raising concerns about their effectiveness in navigating and interpreting dynamic environments. To bridge this gap, we introduce SpatialSky-Bench, a comprehensive benchmark specifically designed to evaluate the spatial intelligence capabilities of VLMs in UAV navigation. Our benchmark comprises two categories-Environmental Perception and Scene Understanding-divided into 13 subcategories, including bounding boxes, color, distance, height, and landing safety analysis, among others. Extensive evaluations of various mainstream open-source and closed-source VLMs reveal unsatisfactory performance in complex UAV navigation scenarios, highlighting significant gaps in their spatial capabilities. To address this challenge, we developed the SpatialSky-Dataset, a comprehensive dataset containing 1M samples with diverse annotations across various scenarios. Leveraging this dataset, we introduce Sky-VLM, a specialized VLM designed for UAV spatial reasoning across multiple granularities and contexts. Extensive experimental results demonstrate that Sky-VLM achieves state-of-the-art performance across all benchmark tasks, paving the way for the development of VLMs suitable for UAV scenarios. The source code is available at https://github.com/linglingxiansen/SpatialSKy.",
            "headline_zh": "提出SpatialSky-Bench和Sky-VLM以提升无人机导航中视觉语言模型的空间智能",
            "intro_zh": [
                "现有视觉语言模型在无人机导航中的空间智能能力未被充分探索，存在性能不足问题。",
                "开发SpatialSky-Bench基准和SpatialSky-Dataset数据集，用于评估和训练空间智能。",
                "Sky-VLM在基准任务中实现最优性能，显著提升无人机场景的空间推理能力。"
            ],
            "tags_zh": [
                "无人机导航",
                "空间智能基准",
                "视觉语言模型",
                "场景理解",
                "环境感知",
                "数据集构建"
            ],
            "_index": 60
        },
        {
            "title": "SymGS : Leveraging Local Symmetries for 3D Gaussian Splatting Compression",
            "authors": [
                "Keshav Gupta",
                "Akshat Sanghvi",
                "Shreyas Reddy Palley",
                "Astitva Srivastava",
                "Charu Sharma",
                "Avinash Sharma"
            ],
            "arxiv_id": "2511.13264v1",
            "summary": "3D Gaussian Splatting has emerged as a transformative technique in novel view synthesis, primarily due to its high rendering speed and photorealistic fidelity. However, its memory footprint scales rapidly with scene complexity, often reaching several gigabytes. Existing methods address this issue by introducing compression strategies that exploit primitive-level redundancy through similarity detection and quantization. We aim to surpass the compression limits of such methods by incorporating symmetry-aware techniques, specifically targeting mirror symmetries to eliminate redundant primitives. We propose a novel compression framework, \\textbf{\\textit{SymGS}}, introducing learnable mirrors into the scene, thereby eliminating local and global reflective redundancies for compression. Our framework functions as a plug-and-play enhancement to state-of-the-art compression methods, (e.g. HAC) to achieve further compression. Compared to HAC, we achieve $1.66 \\times$ compression across benchmark datasets (upto $3\\times$ on large-scale scenes). On an average, SymGS enables $\\bf{108\\times}$ compression of a 3DGS scene, while preserving rendering quality. The project page and supplementary can be found at \\textbf{\\color{cyan}{symgs.github.io}}",
            "headline_zh": "提出SymGS框架，利用局部对称性压缩3D高斯泼溅场景",
            "intro_zh": [
                "3D高斯泼溅内存占用高，随场景复杂度快速增加",
                "引入可学习镜像，消除局部和全局反射冗余以压缩",
                "在基准数据集上平均实现108倍压缩，保持渲染质量"
            ],
            "tags_zh": [
                "3D高斯泼溅",
                "场景压缩",
                "对称性利用",
                "可学习镜像",
                "渲染优化"
            ],
            "_index": 61
        },
        {
            "title": "Building Egocentric Procedural AI Assistant: Methods, Benchmarks, and Challenges",
            "authors": [
                "Junlong Li",
                "Huaiyuan Xu",
                "Sijie Cheng",
                "Kejun Wu",
                "Kim-Hui Yap",
                "Lap-Pui Chau",
                "Yi Wang"
            ],
            "arxiv_id": "2511.13261v1",
            "summary": "Driven by recent advances in vision language models (VLMs) and egocentric perception research, we introduce the concept of an egocentric procedural AI assistant (EgoProceAssist) tailored to step-by-step support daily procedural tasks in a first-person view. In this work, we start by identifying three core tasks: egocentric procedural error detection, egocentric procedural learning, and egocentric procedural question answering. These tasks define the essential functions of EgoProceAssist within a new taxonomy. Specifically, our work encompasses a comprehensive review of current techniques, relevant datasets, and evaluation metrics across these three core areas. To clarify the gap between the proposed EgoProceAssist and existing VLM-based AI assistants, we introduce novel experiments and provide a comprehensive evaluation of representative VLM-based methods. Based on these findings and our technical analysis, we discuss the challenges ahead and suggest future research directions. Furthermore, an exhaustive list of this study is publicly available in an active repository that continuously collects the latest work: https://github.com/z1oong/Building-Egocentric-Procedural-AI-Assistant",
            "headline_zh": "提出第一人称视角程序性AI助手，涵盖错误检测、学习和问答任务。",
            "intro_zh": [
                "核心问题：定义第一人称视角程序性AI助手的三个核心任务。",
                "方法要点：综述现有技术、数据集和评估指标，并引入新实验。",
                "实验或效果：评估代表性视觉语言模型方法，识别差距与挑战。"
            ],
            "tags_zh": [
                "第一人称视角",
                "程序性AI助手",
                "错误检测",
                "程序学习",
                "视觉语言模型",
                "评估基准"
            ],
            "_index": 62
        },
        {
            "title": "GeoX-Bench: Benchmarking Cross-View Geo-Localization and Pose Estimation Capabilities of Large Multimodal Models",
            "authors": [
                "Yushuo Zheng",
                "Jiangyong Ying",
                "Huiyu Duan",
                "Chunyi Li",
                "Zicheng Zhang",
                "Jing Liu",
                "Xiaohong Liu",
                "Guangtao Zhai"
            ],
            "arxiv_id": "2511.13259v1",
            "summary": "Large multimodal models (LMMs) have demonstrated remarkable capabilities across a wide range of tasks, however their knowledge and abilities in the cross-view geo-localization and pose estimation domains remain unexplored, despite potential benefits for navigation, autonomous driving, outdoor robotics, \\textit{etc}. To bridge this gap, we introduce \\textbf{GeoX-Bench}, a comprehensive \\underline{Bench}mark designed to explore and evaluate the capabilities of LMMs in \\underline{cross}-view \\underline{Geo}-localization and pose estimation. Specifically, GeoX-Bench contains 10,859 panoramic-satellite image pairs spanning 128 cities in 49 countries, along with corresponding 755,976 question-answering (QA) pairs. Among these, 42,900 QA pairs are designated for benchmarking, while the remaining are intended to enhance the capabilities of LMMs. Based on GeoX-Bench, we evaluate the capabilities of 25 state-of-the-art LMMs on cross-view geo-localization and pose estimation tasks, and further explore the empowered capabilities of instruction-tuning. Our benchmark demonstrate that while current LMMs achieve impressive performance in geo-localization tasks, their effectiveness declines significantly on the more complex pose estimation tasks, highlighting a critical area for future improvement, and instruction-tuning LMMs on the training data of GeoX-Bench can significantly improve the cross-view geo-sense abilities. The GeoX-Bench is available at \\textcolor{magenta}{https://github.com/IntMeGroup/GeoX-Bench}.",
            "headline_zh": "提出GeoX-Bench基准以评估大模型在跨视角地理定位和姿态估计中的能力",
            "intro_zh": [
                "核心问题：大模型在跨视角地理定位和姿态估计领域的能力尚未被探索",
                "方法要点：构建包含10,859对全景-卫星图像和755,976问答对的基准数据集",
                "实验或效果：评估25个模型，地理定位表现佳，姿态估计需改进，指令调优可提升能力"
            ],
            "tags_zh": [
                "跨视角地理定位",
                "姿态估计",
                "大模型基准",
                "图像问答对",
                "指令调优"
            ],
            "_index": 63
        },
        {
            "title": "Referring Camouflaged Object Detection With Multi-Context Overlapped Windows Cross-Attention",
            "authors": [
                "Yu Wen",
                "Shuyong Gao",
                "Shuping Zhang",
                "Miao Huang",
                "Lili Tao",
                "Han Yang",
                "Haozhe Xing",
                "Lihe Zhang",
                "Boxue Hou"
            ],
            "arxiv_id": "2511.13249v1",
            "summary": "Referring camouflaged object detection (Ref-COD) aims to identify hidden objects by incorporating reference information such as images and text descriptions. Previous research has transformed reference images with salient objects into one-dimensional prompts, yielding significant results. We explore ways to enhance performance through multi-context fusion of rich salient image features and camouflaged object features. Therefore, we propose RFMNet, which utilizes features from multiple encoding stages of the reference salient images and performs interactive fusion with the camouflage features at the corresponding encoding stages. Given that the features in salient object images contain abundant object-related detail information, performing feature fusion within local areas is more beneficial for detecting camouflaged objects. Therefore, we propose an Overlapped Windows Cross-attention mechanism to enable the model to focus more attention on the local information matching based on reference features. Besides, we propose the Referring Feature Aggregation (RFA) module to decode and segment the camouflaged objects progressively. Extensive experiments on the Ref-COD benchmark demonstrate that our method achieves state-of-the-art performance.",
            "headline_zh": "提出RFMNet以通过多上下文融合和重叠窗口交叉注意力提升参考伪装物体检测性能",
            "intro_zh": [
                "核心问题：参考伪装物体检测需结合参考信息识别隐藏物体，现有方法将参考图像转为1D提示，但性能可提升。",
                "方法要点：利用参考显著图像多编码阶段特征与伪装特征交互融合，并引入重叠窗口交叉注意力聚焦局部信息匹配。",
                "实验或效果：在Ref-COD基准测试中实现最先进性能，验证方法有效性。"
            ],
            "tags_zh": [
                "参考伪装物体检测",
                "多上下文特征融合",
                "重叠窗口交叉注意力",
                "显著图像特征",
                "局部信息匹配",
                "渐进式解码"
            ],
            "_index": 64
        },
        {
            "title": "Proceedings Seventh International Workshop on Formal Methods for Autonomous Systems",
            "authors": [
                "Matt Luckcuck",
                "Maike Schwammberger",
                "Mengwei Xu"
            ],
            "arxiv_id": "2511.13245v1",
            "summary": "This EPTCS volume contains the papers from the Seventh International Workshop on Formal Methods for Autonomous Systems (FMAS 2025), which was held between the 17th and 19th of November 2025. The goal of the FMAS workshop series is to bring together leading researchers who are using formal methods to tackle the unique challenges that autonomous systems present, so that they can publish and discuss their work with a growing community of researchers. FMAS 2025 was co-located with the 20th International Conference on integrated Formal Methods (iFM'25), hosted by Inria Paris, France at the Inria Paris Center. \n  In total, FMAS 2025 received 16 submissions from researchers at institutions in: Canada, China, France, Germany, Ireland, Italy, Japan, the Netherlands, Portugal, Sweden, the United States of America, and the United Kingdom. Though we received fewer submissions than last year, we are encouraged to see the submissions being sent from a wide range of countries. Submissions come from both past and new FMAS authors, which shows us that the existing community appreciates the network that FMAS has built over the past 7 years, while new authors also show the FMAS community's great potential of growth.",
            "headline_zh": "汇集形式化方法研究以应对自主系统挑战",
            "intro_zh": [
                "核心问题：自主系统带来的独特挑战，如安全性和可靠性问题。",
                "方法要点：使用形式化方法进行建模、验证和分析。",
                "实验或效果：接收16篇国际投稿，促进研究者社区交流与成长。"
            ],
            "tags_zh": [
                "形式化方法",
                "自主系统",
                "国际研讨会",
                "研究社区",
                "论文发表"
            ],
            "_index": 65
        },
        {
            "title": "Uncovering and Mitigating Transient Blindness in Multimodal Model Editing",
            "authors": [
                "Xiaoqi Han",
                "Ru Li",
                "Ran Yi",
                "Hongye Tan",
                "Zhuomin Liang",
                "Víctor Gutiérrez-Basulto",
                "Jeff Z. Pan"
            ],
            "arxiv_id": "2511.13243v1",
            "summary": "Multimodal Model Editing (MMED) aims to correct erroneous knowledge in multimodal models. Existing evaluation methods, adapted from textual model editing, overstate success by relying on low-similarity or random inputs, obscure overfitting. We propose a comprehensive locality evaluation framework, covering three key dimensions: random-image locality, no-image locality, and consistent-image locality, operationalized through seven distinct data types, enabling a detailed and structured analysis of multimodal edits. We introduce De-VQA, a dynamic evaluation for visual question answering, uncovering a phenomenon we term transient blindness, overfitting to edit-similar text while ignoring visuals. Token analysis shows edits disproportionately affect textual tokens. We propose locality-aware adversarial losses to balance cross-modal representations. Empirical results demonstrate that our approach consistently outperforms existing baselines, reducing transient blindness and improving locality by 17% on average.",
            "headline_zh": "提出多模态模型编辑的局部性评估框架和对抗损失以缓解瞬态盲视",
            "intro_zh": [
                "核心问题：现有多模态模型编辑评估方法高估成功，存在过拟合和瞬态盲视现象。",
                "方法要点：引入局部性评估框架和De-VQA动态评估，使用对抗损失平衡跨模态表示。",
                "实验或效果：方法优于基线，平均减少瞬态盲视并提升局部性17%。"
            ],
            "tags_zh": [
                "多模态模型编辑",
                "瞬态盲视",
                "局部性评估",
                "视觉问答",
                "对抗训练",
                "跨模态表示"
            ],
            "_index": 66
        },
        {
            "title": "MMD-Thinker: Adaptive Multi-Dimensional Thinking for Multimodal Misinformation Detection",
            "authors": [
                "Junjie Wu",
                "Guohong Fu"
            ],
            "arxiv_id": "2511.13242v1",
            "summary": "Multimodal misinformation floods on various social media, and continues to evolve in the era of AI-generated content (AIGC). The emerged misinformation with low creation cost and high deception poses significant threats to society. While recent studies leverage general-purpose multimodal large language models (MLLMs) to achieve remarkable results in detection, they encounter two critical limitations: (1) Insufficient reasoning, where general-purpose MLLMs often follow the uniform reasoning paradigm but generate inaccurate explanations and judgments, due to the lack of the task-specific knowledge of multimodal misinformation detection. (2) Reasoning biases, where a single thinking mode make detectors a suboptimal path for judgment, struggling to keep pace with the fast-growing and intricate multimodal misinformation. In this paper, we propose MMD-Thinker, a two-stage framework for multimodal misinformation detection through adaptive multi-dimensional thinking. First, we develop tailor-designed thinking mode for multimodal misinformation detection. Second, we adopt task-specific instruction tuning to inject the tailored thinking mode into general-purpose MLLMs. Third, we further leverage reinforcement learning strategy with a mixed advantage function, which incentivizes the reasoning capabilities in trajectories. Furthermore, we construct the multimodal misinformation reasoning (MMR) dataset, encompasses more than 8K image-text pairs with both reasoning processes and classification labels, to make progress in the relam of multimodal misinformation detection. Experimental results demonstrate that our proposed MMD-Thinker achieves state-of-the-art performance on both in-domain and out-of-domain benchmark datasets, while maintaining flexible inference and token usage. Code will be publicly available at Github.",
            "headline_zh": "提出MMD-Thinker框架，通过自适应多维思考解决多模态虚假信息检测问题",
            "intro_zh": [
                "核心问题：通用多模态大模型在多模态虚假信息检测中推理不足且存在偏见，难以应对快速演变的虚假信息",
                "方法要点：设计定制化思考模式，通过任务特定指令调优和强化学习策略增强推理能力",
                "实验或效果：在领域内外基准数据集上实现最优性能，并构建MMR数据集支持进展"
            ],
            "tags_zh": [
                "多模态虚假信息检测",
                "自适应多维思考",
                "指令调优",
                "强化学习",
                "MMR数据集",
                "推理能力增强"
            ],
            "_index": 67
        },
        {
            "title": "MRIQT: Physics-Aware Diffusion Model for Image Quality Transfer in Neonatal Ultra-Low-Field MRI",
            "authors": [
                "Malek Al Abed",
                "Sebiha Demir",
                "Anne Groteklaes",
                "Elodie Germani",
                "Shahrooz Faghihroohi",
                "Hemmen Sabir",
                "Shadi Albarqouni"
            ],
            "arxiv_id": "2511.13232v1",
            "summary": "Portable ultra-low-field MRI (uLF-MRI, 0.064 T) offers accessible neuroimaging for neonatal care but suffers from low signal-to-noise ratio and poor diagnostic quality compared to high-field (HF) MRI. We propose MRIQT, a 3D conditional diffusion framework for image quality transfer (IQT) from uLF to HF MRI. MRIQT combines realistic K-space degradation for physics-consistent uLF simulation, v-prediction with classifier-free guidance for stable image-to-image generation, and an SNR-weighted 3D perceptual loss for anatomical fidelity. The model denoises from a noised uLF input conditioned on the same scan, leveraging volumetric attention-UNet architecture for structure-preserving translation. Trained on a neonatal cohort with diverse pathologies, MRIQT surpasses recent GAN and CNN baselines in PSNR 15.3% with 1.78% over the state of the art, while physicians rated 85% of its outputs as good quality with clear pathology present. MRIQT enables high-fidelity, diffusion-based enhancement of portable ultra-low-field (uLF) MRI for deliable neonatal brain assessment.",
            "headline_zh": "提出MRIQT扩散模型以提升新生儿超低场MRI图像质量",
            "intro_zh": [
                "超低场MRI图像信噪比低，诊断质量差，影响新生儿脑部评估。",
                "采用3D条件扩散框架，结合K空间退化和SNR加权损失，实现物理一致增强。",
                "在新生儿数据集上超越GAN和CNN基线，85%输出被医生评为高质量。"
            ],
            "tags_zh": [
                "图像质量迁移",
                "扩散模型",
                "超低场MRI",
                "新生儿神经影像",
                "3D图像增强"
            ],
            "_index": 68
        },
        {
            "title": "Hybrid-Domain Adaptative Representation Learning for Gaze Estimation",
            "authors": [
                "Qida Tan",
                "Hongyu Yang",
                "Wenchao Du"
            ],
            "arxiv_id": "2511.13222v1",
            "summary": "Appearance-based gaze estimation, aiming to predict accurate 3D gaze direction from a single facial image, has made promising progress in recent years. However, most methods suffer significant performance degradation in cross-domain evaluation due to interference from gaze-irrelevant factors, such as expressions, wearables, and image quality. To alleviate this problem, we present a novel Hybrid-domain Adaptative Representation Learning (shorted by HARL) framework that exploits multi-source hybrid datasets to learn robust gaze representation. More specifically, we propose to disentangle gaze-relevant representation from low-quality facial images by aligning features extracted from high-quality near-eye images in an unsupervised domain-adaptation manner, which hardly requires any computational or inference costs. Additionally, we analyze the effect of head-pose and design a simple yet efficient sparse graph fusion module to explore the geometric constraint between gaze direction and head-pose, leading to a dense and robust gaze representation. Extensive experiments on EyeDiap, MPIIFaceGaze, and Gaze360 datasets demonstrate that our approach achieves state-of-the-art accuracy of $\\textbf{5.02}^{\\circ}$ and $\\textbf{3.36}^{\\circ}$, and $\\textbf{9.26}^{\\circ}$ respectively, and present competitive performances through cross-dataset evaluation. The code is available at https://github.com/da60266/HARL.",
            "headline_zh": "提出混合域自适应表示学习框架以提升跨域视线估计鲁棒性",
            "intro_zh": [
                "核心问题：跨域视线估计受表情、佩戴物等无关因素干扰，性能下降显著。",
                "方法要点：通过无监督域适应对齐高低质量图像特征，并融合头部姿态几何约束。",
                "实验或效果：在多个数据集上达到SOTA精度，如EyeDiap 5.02度，跨域评估表现优异。"
            ],
            "tags_zh": [
                "视线估计",
                "域适应",
                "表示学习",
                "头部姿态融合",
                "跨域评估",
                "无监督学习"
            ],
            "_index": 69
        },
        {
            "title": "GaRLILEO: Gravity-aligned Radar-Leg-Inertial Enhanced Odometry",
            "authors": [
                "Chiyun Noh",
                "Sangwoo Jung",
                "Hanjun Kim",
                "Yafei Hu",
                "Laura Herlant",
                "Ayoung Kim"
            ],
            "arxiv_id": "2511.13216v1",
            "summary": "Deployment of legged robots for navigating challenging terrains (e.g., stairs, slopes, and unstructured environments) has gained increasing preference over wheel-based platforms. In such scenarios, accurate odometry estimation is a preliminary requirement for stable locomotion, localization, and mapping. Traditional proprioceptive approaches, which rely on leg kinematics sensor modalities and inertial sensing, suffer from irrepressible vertical drift caused by frequent contact impacts, foot slippage, and vibrations, particularly affected by inaccurate roll and pitch estimation. Existing methods incorporate exteroceptive sensors such as LiDAR or cameras. Further enhancement has been introduced by leveraging gravity vector estimation to add additional observations on roll and pitch, thereby increasing the accuracy of vertical pose estimation. However, these approaches tend to degrade in feature-sparse or repetitive scenes and are prone to errors from double-integrated IMU acceleration. To address these challenges, we propose GaRLILEO, a novel gravity-aligned continuous-time radar-leg-inertial odometry framework. GaRLILEO decouples velocity from the IMU by building a continuous-time ego-velocity spline from SoC radar Doppler and leg kinematics information, enabling seamless sensor fusion which mitigates odometry distortion. In addition, GaRLILEO can reliably capture accurate gravity vectors leveraging a novel soft S2-constrained gravity factor, improving vertical pose accuracy without relying on LiDAR or cameras. Evaluated on a self-collected real-world dataset with diverse indoor-outdoor trajectories, GaRLILEO demonstrates state-of-the-art accuracy, particularly in vertical odometry estimation on stairs and slopes. We open-source both our dataset and algorithm to foster further research in legged robot odometry and SLAM. https://garlileo.github.io/GaRLILEO",
            "headline_zh": "提出GaRLILEO以解决腿式机器人在复杂地形中的垂直里程计漂移问题",
            "intro_zh": [
                "核心问题：腿式机器人里程计在垂直方向易漂移，源于接触冲击和姿态估计不准",
                "方法要点：结合雷达多普勒和腿运动学构建连续时间速度样条，并引入软S2约束重力因子",
                "实验或效果：在室内外数据集上验证，垂直里程计精度领先，尤其在楼梯和斜坡场景"
            ],
            "tags_zh": [
                "腿式机器人里程计",
                "雷达-腿-惯性融合",
                "重力对齐估计",
                "连续时间优化",
                "垂直姿态精度"
            ],
            "_index": 70
        },
        {
            "title": "3DAlign-DAER: Dynamic Attention Policy and Efficient Retrieval Strategy for Fine-grained 3D-Text Alignment at Scale",
            "authors": [
                "Yijia Fan",
                "Jusheng Zhang",
                "Kaitong Cai",
                "Jing Yang",
                "Jian Wang",
                "Keze Wang"
            ],
            "arxiv_id": "2511.13211v1",
            "summary": "Despite recent advancements in 3D-text cross-modal alignment, existing state-of-the-art methods still struggle to align fine-grained textual semantics with detailed geometric structures, and their alignment performance degrades significantly when scaling to large-scale 3D databases. To overcome this limitation, we introduce 3DAlign-DAER, a unified framework designed to align text and 3D geometry via the proposed dynamic attention policy and the efficient retrieval strategy, capturing subtle correspondences for diverse cross-modal retrieval and classification tasks. Specifically, during the training, our proposed dynamic attention policy (DAP) employs the Hierarchical Attention Fusion (HAF) module to represent the alignment as learnable fine-grained token-to-point attentions. To optimize these attentions across different tasks and geometric hierarchies, our DAP further exploits the Monte Carlo tree search to dynamically calibrate HAF attention weights via a hybrid reward signal and further enhances the alignment between textual descriptions and local 3D geometry. During the inference, our 3DAlign-DAER introduces an Efficient Retrieval Strategy (ERS) to leverage efficient hierarchical searching in the large-scale embedding spaces, outperforming traditional methods (e.g., KNN) in accuracy and efficiency. Furthermore, to facilitate text-3D alignment research and train our 3DAlign-DAER, we construct Align3D-2M, a large-scale dataset featuring 2M text-3D pairs, to provide sufficient fine-grained cross-modal annotations. Extensive and comprehensive experiments demonstrate the superior performance of our 3DAlign-DAER on diverse benchmarks. We will release our codes, models, and datasets.",
            "headline_zh": "提出3DAlign-DAER框架，通过动态注意力策略和高效检索策略解决大规模细粒度3D-文本对齐问题。",
            "intro_zh": [
                "现有方法难以对齐细粒度文本语义与3D几何结构，且在大规模数据库中性能下降。",
                "动态注意力策略使用分层注意力融合和蒙特卡洛树搜索优化细粒度对齐。",
                "高效检索策略在大规模嵌入空间实现分层搜索，实验显示在准确性和效率上优于传统方法。"
            ],
            "tags_zh": [
                "3D-文本对齐",
                "动态注意力策略",
                "高效检索",
                "分层注意力融合",
                "蒙特卡洛树搜索",
                "大规模数据集"
            ],
            "_index": 71
        },
        {
            "title": "End-to-End Multi-Person Pose Estimation with Pose-Aware Video Transformer",
            "authors": [
                "Yonghui Yu",
                "Jiahang Cai",
                "Xun Wang",
                "Wenwu Yang"
            ],
            "arxiv_id": "2511.13208v1",
            "summary": "Existing multi-person video pose estimation methods typically adopt a two-stage pipeline: detecting individuals in each frame, followed by temporal modeling for single-person pose estimation. This design relies on heuristic operations such as detection, RoI cropping, and non-maximum suppression (NMS), limiting both accuracy and efficiency. In this paper, we present a fully end-to-end framework for multi-person 2D pose estimation in videos, effectively eliminating heuristic operations. A key challenge is to associate individuals across frames under complex and overlapping temporal trajectories. To address this, we introduce a novel Pose-Aware Video transformEr Network (PAVE-Net), which features a spatial encoder to model intra-frame relations and a spatiotemporal pose decoder to capture global dependencies across frames. To achieve accurate temporal association, we propose a pose-aware attention mechanism that enables each pose query to selectively aggregate features corresponding to the same individual across consecutive frames.Additionally, we explicitly model spatiotemporal dependencies among pose keypoints to improve accuracy. Notably, our approach is the first end-to-end method for multi-frame 2D human pose estimation.Extensive experiments show that PAVE-Net substantially outperforms prior image-based end-to-end methods, achieving a \\textbf{6.0} mAP improvement on PoseTrack2017, and delivers accuracy competitive with state-of-the-art two-stage video-based approaches, while offering significant gains in efficiency.Project page: https://github.com/zgspose/PAVENet",
            "headline_zh": "提出PAVE-Net端到端视频多人姿态估计方法，消除启发式操作。",
            "intro_zh": [
                "现有方法依赖检测和NMS等启发式操作，限制精度和效率。",
                "引入PAVE-Net，结合空间编码器和时空姿态解码器，实现跨帧关联。",
                "在PoseTrack2017上mAP提升6.0，精度与先进方法相当，效率显著提高。"
            ],
            "tags_zh": [
                "端到端姿态估计",
                "视频姿态估计",
                "时空注意力",
                "多人姿态估计",
                "姿态感知网络"
            ],
            "_index": 72
        },
        {
            "title": "PIGEON: VLM-Driven Object Navigation via Points of Interest Selection",
            "authors": [
                "Cheng Peng",
                "Zhenzhe Zhang",
                "Cheng Chi",
                "Xiaobao Wei",
                "Yanhao Zhang",
                "Heng Wang",
                "Pengwei Wang",
                "Zhongyuan Wang",
                "Jing Liu",
                "Shanghang Zhang"
            ],
            "arxiv_id": "2511.13207v1",
            "summary": "Navigating to a specified object in an unknown environment is a fundamental yet challenging capability of embodied intelligence. However, current methods struggle to balance decision frequency with intelligence, resulting in decisions lacking foresight or discontinuous actions. In this work, we propose PIGEON: Point of Interest Guided Exploration for Object Navigation with VLM, maintaining a lightweight and semantically aligned snapshot memory during exploration as semantic input for the exploration strategy. We use a large Visual-Language Model (VLM), named PIGEON-VL, to select Points of Interest (PoI) formed during exploration and then employ a lower-level planner for action output, increasing the decision frequency. Additionally, this PoI-based decision-making enables the generation of Reinforcement Learning with Verifiable Reward (RLVR) data suitable for simulators. Experiments on classic object navigation benchmarks demonstrate that our zero-shot transfer method achieves state-of-the-art performance, while RLVR further enhances the model's semantic guidance capabilities, enabling deep reasoning during real-time navigation.",
            "headline_zh": "提出PIGEON方法，通过兴趣点选择解决未知环境中物体导航问题",
            "intro_zh": [
                "核心问题：未知环境中物体导航决策频率与智能性难以平衡，导致短视或不连续动作",
                "方法要点：使用VLM选择兴趣点，结合轻量语义记忆和低级规划器提高决策频率",
                "实验或效果：零样本转移在基准测试中达SOTA，RLVR增强语义引导和实时推理能力"
            ],
            "tags_zh": [
                "物体导航",
                "视觉语言模型",
                "兴趣点选择",
                "强化学习",
                "零样本转移"
            ],
            "_index": 73
        },
        {
            "title": "RefineVAD: Semantic-Guided Feature Recalibration for Weakly Supervised Video Anomaly Detection",
            "authors": [
                "Junhee Lee",
                "ChaeBeen Bang",
                "MyoungChul Kim",
                "MyeongAh Cho"
            ],
            "arxiv_id": "2511.13204v1",
            "summary": "Weakly-Supervised Video Anomaly Detection aims to identify anomalous events using only video-level labels, balancing annotation efficiency with practical applicability. However, existing methods often oversimplify the anomaly space by treating all abnormal events as a single category, overlooking the diverse semantic and temporal characteristics intrinsic to real-world anomalies. Inspired by how humans perceive anomalies, by jointly interpreting temporal motion patterns and semantic structures underlying different anomaly types, we propose RefineVAD, a novel framework that mimics this dual-process reasoning. Our framework integrates two core modules. The first, Motion-aware Temporal Attention and Recalibration (MoTAR), estimates motion salience and dynamically adjusts temporal focus via shift-based attention and global Transformer-based modeling. The second, Category-Oriented Refinement (CORE), injects soft anomaly category priors into the representation space by aligning segment-level features with learnable category prototypes through cross-attention. By jointly leveraging temporal dynamics and semantic structure, explicitly models both \"how\" motion evolves and \"what\" semantic category it resembles. Extensive experiments on WVAD benchmark validate the effectiveness of RefineVAD and highlight the importance of integrating semantic context to guide feature refinement toward anomaly-relevant patterns.",
            "headline_zh": "提出RefineVAD框架，通过语义引导特征重校准解决弱监督视频异常检测中异常多样性建模不足问题。",
            "intro_zh": [
                "核心问题：现有方法将异常视为单一类别，忽略其语义和时序多样性。",
                "方法要点：集成MoTAR模块动态调整时序焦点，CORE模块注入类别先验对齐特征。",
                "实验或效果：在WVAD基准上验证有效性，强调语义上下文对异常模式引导的重要性。"
            ],
            "tags_zh": [
                "弱监督视频异常检测",
                "语义引导特征重校准",
                "时序注意力机制",
                "类别原型对齐",
                "Transformer建模"
            ],
            "_index": 74
        },
        {
            "title": "Self-Supervised Ultrasound Screen Detection",
            "authors": [
                "Alberto Gomez",
                "Jorge Oliveira",
                "Ramon Casero",
                "Agis Chartsias"
            ],
            "arxiv_id": "2511.13197v1",
            "summary": "Ultrasound (US) machines display images on a built-in monitor, but routine transfer to hospital systems relies on DICOM. We propose a self-supervised pipeline to extract the US image from a photograph of the monitor. This removes the DICOM bottleneck and enables rapid testing and prototyping of new algorithms. In a proof-of-concept study, the rectified images retained enough visual fidelity to classify cardiac views with a balanced accuracy of 0.79 with respect to the native DICOMs.",
            "headline_zh": "提出自监督超声屏幕检测方法，从显示器照片提取图像以绕过DICOM瓶颈。",
            "intro_zh": [
                "核心问题：超声图像依赖DICOM传输，存在效率瓶颈，影响算法测试与原型开发。",
                "方法要点：设计自监督流程，从显示器照片中提取并校正超声图像，无需人工标注。",
                "实验或效果：校正图像视觉保真度高，心脏视图分类平衡准确率达0.79，接近原生DICOM。"
            ],
            "tags_zh": [
                "超声图像处理",
                "自监督学习",
                "屏幕检测",
                "图像校正",
                "医学影像分析"
            ],
            "_index": 75
        },
        {
            "title": "Difficulty-Aware Label-Guided Denoising for Monocular 3D Object Detection",
            "authors": [
                "Soyul Lee",
                "Seungmin Baek",
                "Dongbo Min"
            ],
            "arxiv_id": "2511.13195v1",
            "summary": "Monocular 3D object detection is a cost-effective solution for applications like autonomous driving and robotics, but remains fundamentally ill-posed due to inherently ambiguous depth cues. Recent DETR-based methods attempt to mitigate this through global attention and auxiliary depth prediction, yet they still struggle with inaccurate depth estimates. Moreover, these methods often overlook instance-level detection difficulty, such as occlusion, distance, and truncation, leading to suboptimal detection performance. We propose MonoDLGD, a novel Difficulty-Aware Label-Guided Denoising framework that adaptively perturbs and reconstructs ground-truth labels based on detection uncertainty. Specifically, MonoDLGD applies stronger perturbations to easier instances and weaker ones into harder cases, and then reconstructs them to effectively provide explicit geometric supervision. By jointly optimizing label reconstruction and 3D object detection, MonoDLGD encourages geometry-aware representation learning and improves robustness to varying levels of object complexity. Extensive experiments on the KITTI benchmark demonstrate that MonoDLGD achieves state-of-the-art performance across all difficulty levels.",
            "headline_zh": "提出MonoDLGD框架以解决单目3D检测中深度估计不准和实例难度忽略问题",
            "intro_zh": [
                "核心问题：单目3D检测因深度模糊和忽略实例难度（如遮挡、距离）导致性能不佳",
                "方法要点：基于检测不确定性自适应扰动和重建标签，提供显式几何监督",
                "实验或效果：在KITTI基准上实现所有难度级别的先进性能"
            ],
            "tags_zh": [
                "单目3D物体检测",
                "难度感知学习",
                "标签去噪",
                "几何监督",
                "KITTI基准"
            ],
            "_index": 76
        },
        {
            "title": "Birth of a Painting: Differentiable Brushstroke Reconstruction",
            "authors": [
                "Ying Jiang",
                "Jiayin Lu",
                "Yunuo Chen",
                "Yumeng He",
                "Kui Wu",
                "Yin Yang",
                "Chenfanfu Jiang"
            ],
            "arxiv_id": "2511.13191v1",
            "summary": "Painting embodies a unique form of visual storytelling, where the creation process is as significant as the final artwork. Although recent advances in generative models have enabled visually compelling painting synthesis, most existing methods focus solely on final image generation or patch-based process simulation, lacking explicit stroke structure and failing to produce smooth, realistic shading. In this work, we present a differentiable stroke reconstruction framework that unifies painting, stylized texturing, and smudging to faithfully reproduce the human painting-smudging loop. Given an input image, our framework first optimizes single- and dual-color Bezier strokes through a parallel differentiable paint renderer, followed by a style generation module that synthesizes geometry-conditioned textures across diverse painting styles. We further introduce a differentiable smudge operator to enable natural color blending and shading. Coupled with a coarse-to-fine optimization strategy, our method jointly optimizes stroke geometry, color, and texture under geometric and semantic guidance. Extensive experiments on oil, watercolor, ink, and digital paintings demonstrate that our approach produces realistic and expressive stroke reconstructions, smooth tonal transitions, and richly stylized appearances, offering a unified model for expressive digital painting creation. See our project page for more demos: https://yingjiang96.github.io/DiffPaintWebsite/.",
            "headline_zh": "提出可微分笔触重建框架以统一绘画、纹理生成和涂抹过程",
            "intro_zh": [
                "核心问题：现有方法缺乏显式笔触结构，无法生成平滑真实阴影",
                "方法要点：优化贝塞尔笔触，结合风格生成和可微分涂抹算子",
                "实验效果：在油画、水彩等风格中实现真实笔触重建和色调过渡"
            ],
            "tags_zh": [
                "可微分渲染",
                "笔触重建",
                "绘画合成",
                "风格生成",
                "涂抹操作"
            ],
            "_index": 77
        },
        {
            "title": "Video Spatial Reasoning with Object-Centric 3D Rollout",
            "authors": [
                "Haoran Tang",
                "Meng Cao",
                "Ruyang Liu",
                "Xiaoxi Liang",
                "Linglong Li",
                "Ge Li",
                "Xiaodan Liang"
            ],
            "arxiv_id": "2511.13190v1",
            "summary": "Recent advances in Multi-modal Large Language Models (MLLMs) have showcased remarkable capabilities in vision-language understanding. However, enabling robust video spatial reasoning-the ability to comprehend object locations, orientations, and inter-object relationships in dynamic 3D scenes-remains a key unsolved challenge. Existing approaches primarily rely on spatially grounded supervised fine-tuning or reinforcement learning, yet we observe that such models often exhibit query-locked reasoning, focusing narrowly on objects explicitly mentioned in the prompt while ignoring critical contextual cues. To address this limitation, we propose Object-Centric 3D Rollout (OCR), a novel strategy that introduces structured perturbations to the 3D geometry of selected objects during training. By degrading object-specific visual cues and projecting the altered geometry into 2D space, OCR compels the model to reason holistically across the entire scene. We further design a rollout-based training pipeline that jointly leverages vanilla and region-noisy videos to optimize spatial reasoning trajectories. Experiments demonstrate state-of-the-art performance: our 3B-parameter model achieves 47.5% accuracy on VSI-Bench, outperforming several 7B baselines. Ablations confirm OCR's superiority over prior rollout strategies (e.g., T-GRPO, NoisyRollout).",
            "headline_zh": "提出对象中心3D展开策略以解决视频空间推理中的查询锁定问题",
            "intro_zh": [
                "核心问题：现有多模态大模型在视频空间推理中易出现查询锁定，忽略上下文线索",
                "方法要点：通过结构化扰动3D几何并投影至2D，强制模型进行整体场景推理",
                "实验或效果：3B参数模型在VSI-Bench上达47.5%准确率，优于多个7B基线"
            ],
            "tags_zh": [
                "视频空间推理",
                "对象中心学习",
                "3D几何扰动",
                "多模态大语言模型",
                "强化学习优化"
            ],
            "_index": 78
        },
        {
            "title": "Large Language Models Meet Extreme Multi-label Classification: Scaling and Multi-modal Framework",
            "authors": [
                "Diego Ortego",
                "Marlon Rodríguez",
                "Mario Almagro",
                "Kunal Dahiya",
                "David Jiménez",
                "Juan C. SanMiguel"
            ],
            "arxiv_id": "2511.13189v1",
            "summary": "Foundation models have revolutionized artificial intelligence across numerous domains, yet their transformative potential remains largely untapped in Extreme Multi-label Classification (XMC). Queries in XMC are associated with relevant labels from extremely large label spaces, where it is critical to strike a balance between efficiency and performance. Therefore, many recent approaches efficiently pose XMC as a maximum inner product search between embeddings learned from small encoder-only transformer architectures. In this paper, we address two important aspects in XMC: how to effectively harness larger decoder-only models, and how to exploit visual information while maintaining computational efficiency. We demonstrate that both play a critical role in XMC separately and can be combined for improved performance. We show that a few billion-size decoder can deliver substantial improvements while keeping computational overhead manageable. Furthermore, our Vision-enhanced eXtreme Multi-label Learning framework (ViXML) efficiently integrates foundation vision models by pooling a single embedding per image. This limits computational growth while unlocking multi-modal capabilities. Remarkably, ViXML with small encoders outperforms text-only decoder in most cases, showing that an image is worth billions of parameters. Finally, we present an extension of existing text-only datasets to exploit visual metadata and make them available for future benchmarking. Comprehensive experiments across four public text-only datasets and their corresponding image enhanced versions validate our proposals' effectiveness, surpassing previous state-of-the-art by up to +8.21\\% in P@1 on the largest dataset. ViXML's code is available at https://github.com/DiegoOrtego/vixml.",
            "headline_zh": "提出ViXML框架以解决极端多标签分类中的效率与性能平衡问题。",
            "intro_zh": [
                "核心问题：极端多标签分类需在超大标签空间中平衡效率与性能。",
                "方法要点：结合解码器模型和视觉信息，通过单图像嵌入集成多模态能力。",
                "实验或效果：在多个数据集上超越现有方法，P@1提升最高达8.21%。"
            ],
            "tags_zh": [
                "极端多标签分类",
                "多模态学习",
                "解码器模型",
                "视觉增强",
                "效率优化"
            ],
            "_index": 79
        },
        {
            "title": "Collision-Free Navigation of Mobile Robots via Quadtree-Based Model Predictive Control",
            "authors": [
                "Osama Al Sheikh Ali",
                "Sotiris Koutsoftas",
                "Ze Zhang",
                "Knut Akesson",
                "Emmanuel Dean"
            ],
            "arxiv_id": "2511.13188v1",
            "summary": "This paper presents an integrated navigation framework for Autonomous Mobile Robots (AMRs) that unifies environment representation, trajectory generation, and Model Predictive Control (MPC). The proposed approach incorporates a quadtree-based method to generate structured, axis-aligned collision-free regions from occupancy maps. These regions serve as both a basis for developing safe corridors and as linear constraints within the MPC formulation, enabling efficient and reliable navigation without requiring direct obstacle encoding. The complete pipeline combines safe-area extraction, connectivity graph construction, trajectory generation, and B-spline smoothing into one coherent system. Experimental results demonstrate consistent success and superior performance compared to baseline approaches across complex environments.",
            "headline_zh": "提出基于四叉树模型预测控制的集成导航框架，实现自主移动机器人无碰撞导航。",
            "intro_zh": [
                "核心问题：自主移动机器人在复杂环境中需高效生成无碰撞轨迹。",
                "方法要点：使用四叉树从占据地图提取安全区域，并作为MPC线性约束。",
                "实验效果：在复杂环境中表现优于基线方法，实现可靠导航。"
            ],
            "tags_zh": [
                "自主移动机器人",
                "模型预测控制",
                "四叉树",
                "无碰撞导航",
                "轨迹生成"
            ],
            "_index": 80
        },
        {
            "title": "GenTract: Generative Global Tractography",
            "authors": [
                "Alec Sargood",
                "Lemuel Puglisi",
                "Elinor Thompson",
                "Mirco Musolesi",
                "Daniel C. Alexander"
            ],
            "arxiv_id": "2511.13183v1",
            "summary": "Tractography is the process of inferring the trajectories of white-matter pathways in the brain from diffusion magnetic resonance imaging (dMRI). Local tractography methods, which construct streamlines by following local fiber orientation estimates stepwise through an image, are prone to error accumulation and high false positive rates, particularly on noisy or low-resolution data. In contrast, global methods, which attempt to optimize a collection of streamlines to maximize compatibility with underlying fiber orientation estimates, are computationally expensive. To address these challenges, we introduce GenTract, the first generative model for global tractography. We frame tractography as a generative task, learning a direct mapping from dMRI to complete, anatomically plausible streamlines. We compare both diffusion-based and flow matching paradigms and evaluate GenTract's performance against state-of-the-art baselines. Notably, GenTract achieves precision 2.1x higher than the next-best method, TractOracle. This advantage becomes even more pronounced in challenging low-resolution and noisy settings, where it outperforms the closest competitor by an order of magnitude. By producing tractograms with high precision on research-grade data while also maintaining reliability on imperfect, lower-resolution data, GenTract represents a promising solution for global tractography.",
            "headline_zh": "提出GenTract生成模型以解决脑白质纤维束成像中的误差累积与计算成本问题",
            "intro_zh": [
                "局部纤维束成像方法易累积误差，全局方法计算昂贵，影响脑白质路径推断。",
                "GenTract作为首个生成模型，直接从dMRI学习映射生成完整、解剖合理的纤维束。",
                "在低分辨率和噪声数据上，GenTract精度比次优方法高一个数量级，表现优异。"
            ],
            "tags_zh": [
                "生成模型",
                "全局纤维束成像",
                "扩散磁共振成像",
                "纤维束轨迹推断",
                "计算效率优化"
            ],
            "_index": 81
        },
        {
            "title": "HDW-SR: High-Frequency Guided Diffusion Model based on Wavelet Decomposition for Image Super-Resolution",
            "authors": [
                "Chao Yang",
                "Boqian Zhang",
                "Jinghao Xu",
                "Guang Jiang"
            ],
            "arxiv_id": "2511.13175v1",
            "summary": "Diffusion-based methods have shown great promise in single image super-resolution (SISR); however, existing approaches often produce blurred fine details due to insufficient guidance in the high-frequency domain. To address this issue, we propose a High-Frequency Guided Diffusion Network based on Wavelet Decomposition (HDW-SR), which replaces the conventional U-Net backbone in diffusion frameworks. Specifically, we perform diffusion only on the residual map, allowing the network to focus more effectively on high-frequency information restoration. We then introduce wavelet-based downsampling in place of standard CNN downsampling to achieve multi-scale frequency decomposition, enabling sparse cross-attention between the high-frequency subbands of the pre-super-resolved image and the low-frequency subbands of the diffused image for explicit high-frequency guidance. Moreover, a Dynamic Thresholding Block (DTB) is designed to refine high-frequency selection during the sparse attention process. During upsampling, the invertibility of the wavelet transform ensures low-loss feature reconstruction. Experiments on both synthetic and real-world datasets demonstrate that HDW-SR achieves competitive super-resolution performance, excelling particularly in recovering fine-grained image details. The code will be available after acceptance.",
            "headline_zh": "提出基于小波分解的高频引导扩散模型HDW-SR，以解决图像超分辨率中细节模糊问题。",
            "intro_zh": [
                "核心问题：现有扩散方法在图像超分辨率中高频信息引导不足，导致细节模糊。",
                "方法要点：使用小波分解替换U-Net，在残差图上扩散，并引入稀疏交叉注意力进行高频引导。",
                "实验效果：在合成和真实数据集上表现优异，尤其在恢复精细图像细节方面。"
            ],
            "tags_zh": [
                "图像超分辨率",
                "扩散模型",
                "小波分解",
                "高频引导",
                "稀疏注意力"
            ],
            "_index": 82
        },
        {
            "title": "THIR: Topological Histopathological Image Retrieval",
            "authors": [
                "Zahra Tabatabaei",
                "Jon Sporring"
            ],
            "arxiv_id": "2511.13170v1",
            "summary": "According to the World Health Organization, breast cancer claimed the lives of approximately 685,000 women in 2020. Early diagnosis and accurate clinical decision making are critical in reducing this global burden. In this study, we propose THIR, a novel Content-Based Medical Image Retrieval (CBMIR) framework that leverages topological data analysis specifically, Betti numbers derived from persistent homology to characterize and retrieve histopathological images based on their intrinsic structural patterns. Unlike conventional deep learning approaches that rely on extensive training, annotated datasets, and powerful GPU resources, THIR operates entirely without supervision. It extracts topological fingerprints directly from RGB histopathological images using cubical persistence, encoding the evolution of loops as compact, interpretable feature vectors. The similarity retrieval is then performed by computing the distances between these topological descriptors, efficiently returning the top-K most relevant matches.\n  Extensive experiments on the BreaKHis dataset demonstrate that THIR outperforms state of the art supervised and unsupervised methods. It processes the entire dataset in under 20 minutes on a standard CPU, offering a fast, scalable, and training free solution for clinical image retrieval.",
            "headline_zh": "提出THIR框架，利用拓扑数据分析实现无监督组织病理图像检索。",
            "intro_zh": [
                "核心问题：乳腺癌诊断依赖准确图像检索，但传统方法需大量标注数据和GPU资源。",
                "方法要点：基于Betti数和持久同调提取拓扑特征，实现无监督图像相似性匹配。",
                "实验或效果：在BreaKHis数据集上优于现有方法，CPU处理全数据集不足20分钟。"
            ],
            "tags_zh": [
                "拓扑数据分析",
                "图像检索",
                "无监督学习",
                "组织病理学",
                "持久同调"
            ],
            "_index": 83
        },
        {
            "title": "SOMA: Feature Gradient Enhanced Affine-Flow Matching for SAR-Optical Registration",
            "authors": [
                "Haodong Wang",
                "Tao Zhuo",
                "Xiuwei Zhang",
                "Hanlin Yin",
                "Wencong Wu",
                "Yanning Zhang"
            ],
            "arxiv_id": "2511.13168v1",
            "summary": "Achieving pixel-level registration between SAR and optical images remains a challenging task due to their fundamentally different imaging mechanisms and visual characteristics. Although deep learning has achieved great success in many cross-modal tasks, its performance on SAR-Optical registration tasks is still unsatisfactory. Gradient-based information has traditionally played a crucial role in handcrafted descriptors by highlighting structural differences. However, such gradient cues have not been effectively leveraged in deep learning frameworks for SAR-Optical image matching. To address this gap, we propose SOMA, a dense registration framework that integrates structural gradient priors into deep features and refines alignment through a hybrid matching strategy. Specifically, we introduce the Feature Gradient Enhancer (FGE), which embeds multi-scale, multi-directional gradient filters into the feature space using attention and reconstruction mechanisms to boost feature distinctiveness. Furthermore, we propose the Global-Local Affine-Flow Matcher (GLAM), which combines affine transformation and flow-based refinement within a coarse-to-fine architecture to ensure both structural consistency and local accuracy. Experimental results demonstrate that SOMA significantly improves registration precision, increasing the CMR@1px by 12.29% on the SEN1-2 dataset and 18.50% on the GFGE_SO dataset. In addition, SOMA exhibits strong robustness and generalizes well across diverse scenes and resolutions.",
            "headline_zh": "提出SOMA框架以解决SAR与光学图像像素级配准问题",
            "intro_zh": [
                "SAR与光学图像因成像机制差异导致配准困难，现有深度学习方法效果不佳",
                "引入特征梯度增强器和全局-局部仿射流匹配器，提升特征区分度和配准精度",
                "在SEN1-2和GFGE_SO数据集上，CMR@1px分别提升12.29%和18.50%"
            ],
            "tags_zh": [
                "SAR-光学配准",
                "特征梯度增强",
                "仿射流匹配",
                "多模态图像处理",
                "深度学习框架"
            ],
            "_index": 84
        },
        {
            "title": "Skeletons Speak Louder than Text: A Motion-Aware Pretraining Paradigm for Video-Based Person Re-Identification",
            "authors": [
                "Rifen Lin",
                "Alex Jinpeng Wang",
                "Jiawei Mo",
                "Min Li"
            ],
            "arxiv_id": "2511.13150v1",
            "summary": "Multimodal pretraining has revolutionized visual understanding, but its impact on video-based person re-identification (ReID) remains underexplored. Existing approaches often rely on video-text pairs, yet suffer from two fundamental limitations: (1) lack of genuine multimodal pretraining, and (2) text poorly captures fine-grained temporal motion-an essential cue for distinguishing identities in video. In this work, we take a bold departure from text-based paradigms by introducing the first skeleton-driven pretraining framework for ReID. To achieve this, we propose Contrastive Skeleton-Image Pretraining for ReID (CSIP-ReID), a novel two-stage method that leverages skeleton sequences as a spatiotemporally informative modality aligned with video frames. In the first stage, we employ contrastive learning to align skeleton and visual features at sequence level. In the second stage, we introduce a dynamic Prototype Fusion Updater (PFU) to refine multimodal identity prototypes, fusing motion and appearance cues. Moreover, we propose a Skeleton Guided Temporal Modeling (SGTM) module that distills temporal cues from skeleton data and integrates them into visual features. Extensive experiments demonstrate that CSIP-ReID achieves new state-of-the-art results on standard video ReID benchmarks (MARS, LS-VID, iLIDS-VID). Moreover, it exhibits strong generalization to skeleton-only ReID tasks (BIWI, IAS), significantly outperforming previous methods. CSIP-ReID pioneers an annotation-free and motion-aware pretraining paradigm for ReID, opening a new frontier in multimodal representation learning.",
            "headline_zh": "提出骨架驱动的预训练框架CSIP-ReID，以解决视频行人重识别中文本模态无法捕捉细粒度运动的问题。",
            "intro_zh": [
                "核心问题：文本模态在视频行人重识别中难以捕捉细粒度时间运动，导致现有方法缺乏真正多模态预训练。",
                "方法要点：采用对比学习对齐骨架与视觉特征，并引入原型融合更新器和骨架引导时序建模模块。",
                "实验或效果：在标准视频ReID基准上达到新SOTA，并在骨架ReID任务中表现出强泛化能力。"
            ],
            "tags_zh": [
                "视频行人重识别",
                "骨架序列",
                "对比学习",
                "多模态预训练",
                "时序建模",
                "泛化能力"
            ],
            "_index": 85
        },
        {
            "title": "Automated Road Distress Detection Using Vision Transformersand Generative Adversarial Networks",
            "authors": [
                "Cesar Portocarrero Rodriguez",
                "Laura Vandeweyen",
                "Yosuke Yamamoto"
            ],
            "arxiv_id": "2511.13145v1",
            "summary": "The American Society of Civil Engineers has graded Americas infrastructure condition as a C, with the road system receiving a dismal D. Roads are vital to regional economic viability, yet their management, maintenance, and repair processes remain inefficient, relying on outdated manual or laser-based inspection methods that are both costly and time-consuming. With the increasing availability of real-time visual data from autonomous vehicles, there is an opportunity to apply computer vision (CV) methods for advanced road monitoring, providing insights to guide infrastructure rehabilitation efforts. This project explores the use of state-of-the-art CV techniques for road distress segmentation. It begins by evaluating synthetic data generated with Generative Adversarial Networks (GANs) to assess its usefulness for model training. The study then applies Convolutional Neural Networks (CNNs) for road distress segmentation and subsequently examines the transformer-based model MaskFormer. Results show that GAN-generated data improves model performance and that MaskFormer outperforms the CNN model in two metrics: mAP50 and IoU.",
            "headline_zh": "提出结合GAN生成数据和MaskFormer模型以提升道路病害分割性能",
            "intro_zh": [
                "核心问题：美国道路基础设施状况差，传统检测方法成本高、效率低。",
                "方法要点：使用GAN生成合成数据，并应用CNN和MaskFormer进行道路病害分割。",
                "实验或效果：GAN数据提升模型性能，MaskFormer在mAP50和IoU指标上优于CNN。"
            ],
            "tags_zh": [
                "道路病害检测",
                "生成对抗网络",
                "视觉变换器",
                "图像分割",
                "合成数据增强"
            ],
            "_index": 86
        },
        {
            "title": "WinMamba: Multi-Scale Shifted Windows in State Space Model for 3D Object Detection",
            "authors": [
                "Longhui Zheng",
                "Qiming Xia",
                "Xiaolu Chen",
                "Zhaoliang Liu",
                "Chenglu Wen"
            ],
            "arxiv_id": "2511.13138v1",
            "summary": "3D object detection is critical for autonomous driving, yet it remains fundamentally challenging to simultaneously maximize computational efficiency and capture long-range spatial dependencies. We observed that Mamba-based models, with their linear state-space design, capture long-range dependencies at lower cost, offering a promising balance between efficiency and accuracy. However, existing methods rely on axis-aligned scanning within a fixed window, inevitably discarding spatial information. To address this problem, we propose WinMamba, a novel Mamba-based 3D feature-encoding backbone composed of stacked WinMamba blocks. To enhance the backbone with robust multi-scale representation, the WinMamba block incorporates a window-scale-adaptive module that compensates voxel features across varying resolutions during sampling. Meanwhile, to obtain rich contextual cues within the linear state space, we equip the WinMamba layer with a learnable positional encoding and a window-shift strategy. Extensive experiments on the KITTI and Waymo datasets demonstrate that WinMamba significantly outperforms the baseline. Ablation studies further validate the individual contributions of the WSF and AWF modules in improving detection accuracy. The code will be made publicly available.",
            "headline_zh": "提出WinMamba以解决3D目标检测中效率与长程依赖的平衡问题",
            "intro_zh": [
                "核心问题：3D目标检测需平衡计算效率与长程空间依赖，现有方法因固定窗口扫描丢失空间信息",
                "方法要点：引入WinMamba块，结合窗口尺度自适应模块和窗口移位策略，增强多尺度特征与上下文",
                "实验或效果：在KITTI和Waymo数据集上显著超越基线，验证WSF和AWF模块提升检测精度"
            ],
            "tags_zh": [
                "3D目标检测",
                "状态空间模型",
                "多尺度特征",
                "窗口移位策略",
                "长程依赖",
                "自动驾驶"
            ],
            "_index": 87
        },
        {
            "title": "MedGEN-Bench: Contextually entangled benchmark for open-ended multimodal medical generation",
            "authors": [
                "Junjie Yang",
                "Yuhao Yan",
                "Gang Wu",
                "Yuxuan Wang",
                "Ruoyu Liang",
                "Xinjie Jiang",
                "Xiang Wan",
                "Fenglei Fan",
                "Yongquan Zhang",
                "Feiwei Qin",
                "Changmiao Wan"
            ],
            "arxiv_id": "2511.13135v1",
            "summary": "As Vision-Language Models (VLMs) increasingly gain traction in medical applications, clinicians are progressively expecting AI systems not only to generate textual diagnoses but also to produce corresponding medical images that integrate seamlessly into authentic clinical workflows. Despite the growing interest, existing medical visual benchmarks present notable limitations. They often rely on ambiguous queries that lack sufficient relevance to image content, oversimplify complex diagnostic reasoning into closed-ended shortcuts, and adopt a text-centric evaluation paradigm that overlooks the importance of image generation capabilities. To address these challenges, we introduce \\textsc{MedGEN-Bench}, a comprehensive multimodal benchmark designed to advance medical AI research. MedGEN-Bench comprises 6,422 expert-validated image-text pairs spanning six imaging modalities, 16 clinical tasks, and 28 subtasks. It is structured into three distinct formats: Visual Question Answering, Image Editing, and Contextual Multimodal Generation. What sets MedGEN-Bench apart is its focus on contextually intertwined instructions that necessitate sophisticated cross-modal reasoning and open-ended generative outputs, moving beyond the constraints of multiple-choice formats. To evaluate the performance of existing systems, we employ a novel three-tier assessment framework that integrates pixel-level metrics, semantic text analysis, and expert-guided clinical relevance scoring. Using this framework, we systematically assess 10 compositional frameworks, 3 unified models, and 5 VLMs.",
            "headline_zh": "提出MedGEN-Bench基准以解决医学多模态生成中上下文推理不足的问题",
            "intro_zh": [
                "现有医学视觉基准存在查询模糊、推理简化及图像生成评估缺失等核心问题",
                "方法要点包括构建专家验证的图像-文本对，并设计三种格式支持开放生成",
                "实验评估了18个模型，采用像素级、语义和临床相关性三层评估框架"
            ],
            "tags_zh": [
                "医学多模态生成",
                "上下文推理基准",
                "图像-文本对",
                "开放生成评估",
                "临床任务",
                "多模态模型"
            ],
            "_index": 88
        },
        {
            "title": "Shedding Light on VLN Robustness: A Black-box Framework for Indoor Lighting-based Adversarial Attack",
            "authors": [
                "Chenyang Li",
                "Wenbing Tang",
                "Yihao Huang",
                "Sinong Simon Zhan",
                "Ming Hu",
                "Xiaojun Jia",
                "Yang Liu"
            ],
            "arxiv_id": "2511.13132v1",
            "summary": "Vision-and-Language Navigation (VLN) agents have made remarkable progress, but their robustness remains insufficiently studied. Existing adversarial evaluations often rely on perturbations that manifest as unusual textures rarely encountered in everyday indoor environments. Errors under such contrived conditions have limited practical relevance, as real-world agents are unlikely to encounter such artificial patterns. In this work, we focus on indoor lighting, an intrinsic yet largely overlooked scene attribute that strongly influences navigation. We propose Indoor Lighting-based Adversarial Attack (ILA), a black-box framework that manipulates global illumination to disrupt VLN agents. Motivated by typical household lighting usage, we design two attack modes: Static Indoor Lighting-based Attack (SILA), where the lighting intensity remains constant throughout an episode, and Dynamic Indoor Lighting-based Attack (DILA), where lights are switched on or off at critical moments to induce abrupt illumination changes. We evaluate ILA on two state-of-the-art VLN models across three navigation tasks. Results show that ILA significantly increases failure rates while reducing trajectory efficiency, revealing previously unrecognized vulnerabilities of VLN agents to realistic indoor lighting variations.",
            "headline_zh": "提出室内光照对抗攻击框架，揭示视觉语言导航在真实光照变化下的脆弱性",
            "intro_zh": [
                "核心问题：视觉语言导航代理在真实室内光照变化下的鲁棒性不足，现有攻击方法不实用",
                "方法要点：设计黑盒攻击框架，通过静态和动态光照变化干扰导航决策",
                "实验或效果：在多个任务中显著提高失败率，降低轨迹效率，暴露新漏洞"
            ],
            "tags_zh": [
                "视觉语言导航",
                "对抗攻击",
                "室内光照",
                "黑盒框架",
                "鲁棒性评估"
            ],
            "_index": 89
        },
        {
            "title": "MM-Telco: Benchmarks and Multimodal Large Language Models for Telecom Applications",
            "authors": [
                "Gagan Raj Gupta",
                "Anshul Kumar",
                "Manish Rai",
                "Apu Chakraborty",
                "Ashutosh Modi",
                "Abdelaali Chaoub",
                "Soumajit Pramanik",
                "Moyank Giri",
                "Yashwanth Holla",
                "Sunny Kumar",
                "M. V. Kiran Sooraj"
            ],
            "arxiv_id": "2511.13131v1",
            "summary": "Large Language Models (LLMs) have emerged as powerful tools for automating complex reasoning and decision-making tasks. In telecommunications, they hold the potential to transform network optimization, automate troubleshooting, enhance customer support, and ensure regulatory compliance. However, their deployment in telecom is hindered by domain-specific challenges that demand specialized adaptation. To overcome these challenges and to accelerate the adaptation of LLMs for telecom, we propose MM-Telco, a comprehensive suite of multimodal benchmarks and models tailored for the telecom domain. The benchmark introduces various tasks (both text based and image based) that address various practical real-life use cases such as network operations, network management, improving documentation quality, and retrieval of relevant text and images. Further, we perform baseline experiments with various LLMs and VLMs. The models fine-tuned on our dataset exhibit a significant boost in performance. Our experiments also help analyze the weak areas in the working of current state-of-art multimodal LLMs, thus guiding towards further development and research.",
            "headline_zh": "提出MM-Telco基准与多模态大模型以解决电信领域应用挑战",
            "intro_zh": [
                "电信领域大模型部署面临领域特定挑战，需专门适应。",
                "构建多模态基准，涵盖文本与图像任务，支持网络运维等用例。",
                "基准实验显示微调模型性能显著提升，并识别当前模型弱点。"
            ],
            "tags_zh": [
                "多模态大语言模型",
                "电信应用基准",
                "网络运维自动化",
                "图像文本检索",
                "模型微调",
                "性能评估"
            ],
            "_index": 90
        },
        {
            "title": "VEIL: Jailbreaking Text-to-Video Models via Visual Exploitation from Implicit Language",
            "authors": [
                "Zonghao Ying",
                "Moyang Chen",
                "Nizhang Li",
                "Zhiqiang Wang",
                "Wenxin Zhang",
                "Quanchen Zou",
                "Zonglei Jing",
                "Aishan Liu",
                "Xianglong Liu"
            ],
            "arxiv_id": "2511.13127v1",
            "summary": "Jailbreak attacks can circumvent model safety guardrails and reveal critical blind spots. Prior attacks on text-to-video (T2V) models typically add adversarial perturbations to obviously unsafe prompts, which are often easy to detect and defend. In contrast, we show that benign-looking prompts containing rich, implicit cues can induce T2V models to generate semantically unsafe videos that both violate policy and preserve the original (blocked) intent. To realize this, we propose VEIL, a jailbreak framework that leverages T2V models' cross-modal associative patterns via a modular prompt design. Specifically, our prompts combine three components: neutral scene anchors, which provide the surface-level scene description extracted from the blocked intent to maintain plausibility; latent auditory triggers, textual descriptions of innocuous-sounding audio events (e.g., creaking, muffled noises) that exploit learned audio-visual co-occurrence priors to bias the model toward particular unsafe visual concepts; and stylistic modulators, cinematic directives (e.g., camera framing, atmosphere) that amplify and stabilize the latent trigger's effect. We formalize attack generation as a constrained optimization over the above modular prompt space and solve it with a guided search procedure that balances stealth and effectiveness. Extensive experiments over 7 T2V models demonstrate the efficacy of our attack, achieving a 23 percent improvement in average attack success rate in commercial models.",
            "headline_zh": "提出VEIL框架，通过隐式语言视觉利用实现文本到视频模型的越狱攻击",
            "intro_zh": [
                "核心问题：传统越狱攻击易被检测，需隐蔽诱导模型生成违反安全策略的视频",
                "方法要点：使用中性场景锚点、潜在听觉触发器和风格调制器设计模块化提示",
                "实验或效果：在7个T2V模型中测试，商业模型平均攻击成功率提升23%"
            ],
            "tags_zh": [
                "文本到视频模型",
                "越狱攻击",
                "模块化提示设计",
                "跨模态关联",
                "安全漏洞",
                "隐式语言利用"
            ],
            "_index": 91
        },
        {
            "title": "Region-Point Joint Representation for Effective Trajectory Similarity Learning",
            "authors": [
                "Hao Long",
                "Silin Zhou",
                "Lisi Chen",
                "Shuo Shang"
            ],
            "arxiv_id": "2511.13125v1",
            "summary": "Recent learning-based methods have reduced the computational complexity of traditional trajectory similarity computation, but state-of-the-art (SOTA) methods still fail to leverage the comprehensive spectrum of trajectory information for similarity modeling. To tackle this problem, we propose \\textbf{RePo}, a novel method that jointly encodes \\textbf{Re}gion-wise and \\textbf{Po}int-wise features to capture both spatial context and fine-grained moving patterns. For region-wise representation, the GPS trajectories are first mapped to grid sequences, and spatial context are captured by structural features and semantic context enriched by visual features. For point-wise representation, three lightweight expert networks extract local, correlation, and continuous movement patterns from dense GPS sequences. Then, a router network adaptively fuses the learned point-wise features, which are subsequently combined with region-wise features using cross-attention to produce the final trajectory embedding. To train RePo, we adopt a contrastive loss with hard negative samples to provide similarity ranking supervision. Experiment results show that RePo achieves an average accuracy improvement of 22.2\\% over SOTA baselines across all evaluation metrics.",
            "headline_zh": "提出RePo方法以联合编码区域和点特征，提升轨迹相似性学习效果",
            "intro_zh": [
                "现有方法未能充分利用轨迹信息进行相似性建模",
                "RePo联合编码区域特征和点特征，并自适应融合",
                "实验显示RePo在各项指标上平均准确率提升22.2%"
            ],
            "tags_zh": [
                "轨迹相似性学习",
                "区域点联合表示",
                "对比学习",
                "GPS轨迹分析",
                "自适应特征融合"
            ],
            "_index": 92
        },
        {
            "title": "CloseUpShot: Close-up Novel View Synthesis from Sparse-views via Point-conditioned Diffusion Model",
            "authors": [
                "Yuqi Zhang",
                "Guanying Chen",
                "Jiaxing Chen",
                "Chuanyu Fu",
                "Chuan Huang",
                "Shuguang Cui"
            ],
            "arxiv_id": "2511.13121v1",
            "summary": "Reconstructing 3D scenes and synthesizing novel views from sparse input views is a highly challenging task. Recent advances in video diffusion models have demonstrated strong temporal reasoning capabilities, making them a promising tool for enhancing reconstruction quality under sparse-view settings. However, existing approaches are primarily designed for modest viewpoint variations, which struggle in capturing fine-grained details in close-up scenarios since input information is severely limited. In this paper, we present a diffusion-based framework, called CloseUpShot, for close-up novel view synthesis from sparse inputs via point-conditioned video diffusion. Specifically, we observe that pixel-warping conditioning suffers from severe sparsity and background leakage in close-up settings. To address this, we propose hierarchical warping and occlusion-aware noise suppression, enhancing the quality and completeness of the conditioning images for the video diffusion model. Furthermore, we introduce global structure guidance, which leverages a dense fused point cloud to provide consistent geometric context to the diffusion process, to compensate for the lack of globally consistent 3D constraints in sparse conditioning inputs. Extensive experiments on multiple datasets demonstrate that our method outperforms existing approaches, especially in close-up novel view synthesis, clearly validating the effectiveness of our design.",
            "headline_zh": "提出CloseUpShot框架，通过点条件扩散模型解决稀疏视图下近景新视角合成的挑战",
            "intro_zh": [
                "核心问题：稀疏输入视图在近景场景中难以捕捉细粒度细节，导致重建质量差",
                "方法要点：采用分层扭曲和遮挡感知噪声抑制，结合全局结构引导提升条件图像质量",
                "实验或效果：在多个数据集上优于现有方法，尤其在近景新视角合成中表现突出"
            ],
            "tags_zh": [
                "新视角合成",
                "点条件扩散模型",
                "稀疏视图重建",
                "遮挡感知噪声抑制",
                "全局结构引导"
            ],
            "_index": 93
        },
        {
            "title": "Monolithic Units: Actuation, Sensing, and Simulation for Integrated Soft Robot Design",
            "authors": [
                "Trevor Exley",
                "Anderson Brazil Nardin",
                "Petr Trunin",
                "Diana Cafiso",
                "Lucia Beccai"
            ],
            "arxiv_id": "2511.13120v1",
            "summary": "This work introduces the Monolithic Unit (MU), an actuator-lattice-sensor building block for soft robotics. The MU integrates pneumatic actuation, a compliant lattice envelope, and candidate sites for optical waveguide sensing into a single printed body. In order to study reproducibility and scalability, a parametric design framework establishes deterministic rules linking actuator chamber dimensions to lattice unit cell size. Experimental homogenization of lattice specimens provides effective material properties for finite element simulation. Within this simulation environment, sensor placement is treated as a discrete optimization problem, where a finite set of candidate waveguide paths derived from lattice nodes is evaluated by introducing local stiffening, and the configuration minimizing deviation from baseline mechanical response is selected. Optimized models are fabricated and experimentally characterized, validating the preservation of mechanical performance while enabling embedded sensing. The workflow is further extended to scaled units and a two-finger gripper, demonstrating generality of the MU concept. This approach advances monolithic soft robotic design by combining reproducible co-design rules with simulation-informed sensor integration.",
            "headline_zh": "提出Monolithic Unit以集成软机器人驱动、传感与仿真设计",
            "intro_zh": [
                "核心问题：软机器人中驱动、传感与结构集成缺乏可重复性设计方法。",
                "方法要点：使用参数化框架和有限元仿真优化传感器布局。",
                "实验或效果：验证优化模型保持机械性能并实现嵌入式传感。"
            ],
            "tags_zh": [
                "软机器人",
                "Monolithic Unit",
                "参数化设计",
                "有限元仿真",
                "传感器优化",
                "集成制造"
            ],
            "_index": 94
        },
        {
            "title": "A Lightweight 3D Anomaly Detection Method with Rotationally Invariant Features",
            "authors": [
                "Hanzhe Liang",
                "Jie Zhou",
                "Can Gao",
                "Bingyang Guo",
                "Jinbao Wang",
                "Linlin Shen"
            ],
            "arxiv_id": "2511.13115v1",
            "summary": "3D anomaly detection (AD) is a crucial task in computer vision, aiming to identify anomalous points or regions from point cloud data. However, existing methods may encounter challenges when handling point clouds with changes in orientation and position because the resulting features may vary significantly. To address this problem, we propose a novel Rotationally Invariant Features (RIF) framework for 3D AD. Firstly, to remove the adverse effect of variations on point cloud data, we develop a Point Coordinate Mapping (PCM) technique, which maps each point into a rotationally invariant space to maintain consistency of representation. Then, to learn robust and discriminative features, we design a lightweight Convolutional Transform Feature Network (CTF-Net) to extract rotationally invariant features for the memory bank. To improve the ability of the feature extractor, we introduce the idea of transfer learning to pre-train the feature extractor with 3D data augmentation. Experimental results show that the proposed method achieves the advanced performance on the Anomaly-ShapeNet dataset, with an average P-AUROC improvement of 17.7\\%, and also gains the best performance on the Real3D-AD dataset, with an average P-AUROC improvement of 1.6\\%. The strong generalization ability of RIF has been verified by combining it with traditional feature extraction methods on anomaly detection tasks, demonstrating great potential for industrial applications.",
            "headline_zh": "提出旋转不变特征框架以解决3D点云异常检测中的方向变化问题",
            "intro_zh": [
                "3D异常检测中，点云方向和位置变化导致特征不稳定，影响检测性能。",
                "采用点坐标映射和轻量卷积变换网络，提取旋转不变特征并构建记忆库。",
                "在Anomaly-ShapeNet和Real3D-AD数据集上，P-AUROC指标显著提升，验证强泛化能力。"
            ],
            "tags_zh": [
                "3D异常检测",
                "旋转不变特征",
                "点云处理",
                "轻量网络",
                "迁移学习",
                "工业应用"
            ],
            "_index": 95
        },
        {
            "title": "Semantics and Content Matter: Towards Multi-Prior Hierarchical Mamba for Image Deraining",
            "authors": [
                "Zhaocheng Yu",
                "Kui Jiang",
                "Junjun Jiang",
                "Xianming Liu",
                "Guanglu Sun",
                "Yi Xiao"
            ],
            "arxiv_id": "2511.13113v1",
            "summary": "Rain significantly degrades the performance of computer vision systems, particularly in applications like autonomous driving and video surveillance. While existing deraining methods have made considerable progress, they often struggle with fidelity of semantic and spatial details. To address these limitations, we propose the Multi-Prior Hierarchical Mamba (MPHM) network for image deraining. This novel architecture synergistically integrates macro-semantic textual priors (CLIP) for task-level semantic guidance and micro-structural visual priors (DINOv2) for scene-aware structural information. To alleviate potential conflicts between heterogeneous priors, we devise a progressive Priors Fusion Injection (PFI) that strategically injects complementary cues at different decoder levels. Meanwhile, we equip the backbone network with an elaborate Hierarchical Mamba Module (HMM) to facilitate robust feature representation, featuring a Fourier-enhanced dual-path design that concurrently addresses global context modeling and local detail recovery. Comprehensive experiments demonstrate MPHM's state-of-the-art performance, achieving a 0.57 dB PSNR gain on the Rain200H dataset while delivering superior generalization on real-world rainy scenarios.",
            "headline_zh": "提出多先验分层Mamba网络以解决图像去雨中的语义和空间细节保真问题",
            "intro_zh": [
                "核心问题：现有去雨方法在语义和空间细节保真方面表现不足，影响自动驾驶等应用性能。",
                "方法要点：融合CLIP语义先验和DINOv2结构先验，通过渐进融合注入和分层Mamba模块增强特征表示。",
                "实验或效果：在Rain200H数据集上PSNR提升0.57 dB，并在真实雨天场景中表现出优越泛化能力。"
            ],
            "tags_zh": [
                "图像去雨",
                "多先验融合",
                "分层Mamba网络",
                "语义引导",
                "结构先验",
                "渐进融合注入"
            ],
            "_index": 96
        },
        {
            "title": "Learning Implicit Neural Degradation Representation for Unpaired Image Dehazing",
            "authors": [
                "Shuaibin Fan",
                "Senming Zhong",
                "Wenchao Yan",
                "Minglong Xue"
            ],
            "arxiv_id": "2511.13110v1",
            "summary": "Image dehazing is an important task in the field of computer vision, aiming at restoring clear and detail-rich visual content from haze-affected images. However, when dealing with complex scenes, existing methods often struggle to strike a balance between fine-grained feature representation of inhomogeneous haze distribution and global consistency modeling. Furthermore, to better learn the common degenerate representation of haze in spatial variations, we propose an unsupervised dehaze method for implicit neural degradation representation. Firstly, inspired by the Kolmogorov-Arnold representation theorem, we propose a mechanism combining the channel-independent and channel-dependent mechanisms, which efficiently enhances the ability to learn from nonlinear dependencies. which in turn achieves good visual perception in complex scenes. Moreover, we design an implicit neural representation to model haze degradation as a continuous function to eliminate redundant information and the dependence on explicit feature extraction and physical models. To further learn the implicit representation of the haze features, we also designed a dense residual enhancement module from it to eliminate redundant information. This achieves high-quality image restoration. Experimental results show that our method achieves competitive dehaze performance on various public and real-world datasets. This project code will be available at https://github.com/Fan-pixel/NeDR-Dehaze.",
            "headline_zh": "提出隐式神经退化表示方法以解决无配对图像去雾中非均匀雾分布建模问题",
            "intro_zh": [
                "核心问题：现有方法难以平衡非均匀雾分布的细粒度特征与全局一致性建模",
                "方法要点：结合通道独立与依赖机制，设计隐式神经表示建模雾退化连续函数",
                "实验或效果：在公共和真实数据集上实现竞争性去雾性能，代码开源"
            ],
            "tags_zh": [
                "图像去雾",
                "隐式神经表示",
                "无监督学习",
                "非线性依赖",
                "残差增强"
            ],
            "_index": 97
        },
        {
            "title": "DGS-Net: Distillation-Guided Gradient Surgery for CLIP Fine-Tuning in AI-Generated Image Detection",
            "authors": [
                "Jiazhen Yan",
                "Ziqiang Li",
                "Fan Wang",
                "Boyu Wang",
                "Zhangjie Fu"
            ],
            "arxiv_id": "2511.13108v1",
            "summary": "The rapid progress of generative models such as GANs and diffusion models has led to the widespread proliferation of AI-generated images, raising concerns about misinformation, privacy violations, and trust erosion in digital media. Although large-scale multimodal models like CLIP offer strong transferable representations for detecting synthetic content, fine-tuning them often induces catastrophic forgetting, which degrades pre-trained priors and limits cross-domain generalization. To address this issue, we propose the Distillation-guided Gradient Surgery Network (DGS-Net), a novel framework that preserves transferable pre-trained priors while suppressing task-irrelevant components. Specifically, we introduce a gradient-space decomposition that separates harmful and beneficial descent directions during optimization. By projecting task gradients onto the orthogonal complement of harmful directions and aligning with beneficial ones distilled from a frozen CLIP encoder, DGS-Net achieves unified optimization of prior preservation and irrelevant suppression. Extensive experiments on 50 generative models demonstrate that our method outperforms state-of-the-art approaches by an average margin of 6.6, achieving superior detection performance and generalization across diverse generation techniques.",
            "headline_zh": "提出DGS-Net以解决CLIP微调中的灾难性遗忘问题，提升AI生成图像检测性能",
            "intro_zh": [
                "核心问题：CLIP微调导致灾难性遗忘，损害预训练先验并限制跨域泛化",
                "方法要点：通过梯度空间分解，投影有害方向并对齐有益方向，实现先验保持与无关抑制",
                "实验或效果：在50个生成模型上实验，平均性能提升6.6，检测与泛化能力优越"
            ],
            "tags_zh": [
                "AI生成图像检测",
                "CLIP微调",
                "梯度手术",
                "蒸馏学习",
                "灾难性遗忘",
                "跨域泛化"
            ],
            "_index": 98
        },
        {
            "title": "Low-Level Dataset Distillation for Medical Image Enhancement",
            "authors": [
                "Fengzhi Xu",
                "Ziyuan Yang",
                "Mengyu Sun",
                "Joey Tianyi Zhou",
                "Yi Zhang"
            ],
            "arxiv_id": "2511.13106v1",
            "summary": "Medical image enhancement is clinically valuable, but existing methods require large-scale datasets to learn complex pixel-level mappings. However, the substantial training and storage costs associated with these datasets hinder their practical deployment. While dataset distillation (DD) can alleviate these burdens, existing methods mainly target high-level tasks, where multiple samples share the same label. This many-to-one mapping allows distilled data to capture shared semantics and achieve information compression. In contrast, low-level tasks involve a many-to-many mapping that requires pixel-level fidelity, making low-level DD an underdetermined problem, as a small distilled dataset cannot fully constrain the dense pixel-level mappings. To address this, we propose the first low-level DD method for medical image enhancement. We first leverage anatomical similarities across patients to construct the shared anatomical prior based on a representative patient, which serves as the initialization for the distilled data of different patients. This prior is then personalized for each patient using a Structure-Preserving Personalized Generation (SPG) module, which integrates patient-specific anatomical information into the distilled dataset while preserving pixel-level fidelity. For different low-level tasks, the distilled data is used to construct task-specific high- and low-quality training pairs. Patient-specific knowledge is injected into the distilled data by aligning the gradients computed from networks trained on the distilled pairs with those from the corresponding patient's raw data. Notably, downstream users cannot access raw patient data. Instead, only a distilled dataset containing abstract training information is shared, which excludes patient-specific details and thus preserves privacy.",
            "headline_zh": "提出低层数据集蒸馏方法以解决医学图像增强中的训练成本与隐私问题",
            "intro_zh": [
                "核心问题：低层任务像素级保真度要求高，数据集蒸馏为欠定问题，难以约束密集映射",
                "方法要点：利用解剖相似性构建共享先验，通过SPG模块个性化并注入患者知识",
                "实验或效果：蒸馏数据集仅含抽象信息，保护隐私，未知具体性能提升"
            ],
            "tags_zh": [
                "医学图像增强",
                "数据集蒸馏",
                "低层视觉任务",
                "隐私保护",
                "像素级映射"
            ],
            "_index": 99
        },
        {
            "title": "PlugTrack: Multi-Perceptive Motion Analysis for Adaptive Fusion in Multi-Object Tracking",
            "authors": [
                "Seungjae Kim",
                "SeungJoon Lee",
                "MyeongAh Cho"
            ],
            "arxiv_id": "2511.13105v1",
            "summary": "Multi-object tracking (MOT) predominantly follows the tracking-by-detection paradigm, where Kalman filters serve as the standard motion predictor due to computational efficiency but inherently fail on non-linear motion patterns. Conversely, recent data-driven motion predictors capture complex non-linear dynamics but suffer from limited domain generalization and computational overhead. Through extensive analysis, we reveal that even in datasets dominated by non-linear motion, Kalman filter outperforms data-driven predictors in up to 34\\% of cases, demonstrating that real-world tracking scenarios inherently involve both linear and non-linear patterns. To leverage this complementarity, we propose PlugTrack, a novel framework that adaptively fuses Kalman filter and data-driven motion predictors through multi-perceptive motion understanding. Our approach employs multi-perceptive motion analysis to generate adaptive blending factors. PlugTrack achieves significant performance gains on MOT17/MOT20 and state-of-the-art on DanceTrack without modifying existing motion predictors. To the best of our knowledge, PlugTrack is the first framework to bridge classical and modern motion prediction paradigms through adaptive fusion in MOT.",
            "headline_zh": "提出PlugTrack框架，通过自适应融合运动预测器解决多目标跟踪中线性与非线性运动模式问题",
            "intro_zh": [
                "核心问题：多目标跟踪中，卡尔曼滤波器无法处理非线性运动，而数据驱动预测器泛化差且计算开销大",
                "方法要点：使用多感知运动分析生成自适应融合因子，结合卡尔曼滤波器和数据驱动预测器",
                "实验或效果：在MOT17/MOT20上性能显著提升，在DanceTrack上达到最先进水平，无需修改现有预测器"
            ],
            "tags_zh": [
                "多目标跟踪",
                "运动预测",
                "自适应融合",
                "卡尔曼滤波器",
                "数据驱动方法",
                "多感知分析"
            ],
            "_index": 100
        },
        {
            "title": "CapeNext: Rethinking and refining dynamic support information for category-agnostic pose estimation",
            "authors": [
                "Yu Zhu",
                "Dan Zeng",
                "Shuiwang Li",
                "Qijun Zhao",
                "Qiaomu Shen",
                "Bo Tang"
            ],
            "arxiv_id": "2511.13102v1",
            "summary": "Recent research in Category-Agnostic Pose Estimation (CAPE) has adopted fixed textual keypoint description as semantic prior for two-stage pose matching frameworks. While this paradigm enhances robustness and flexibility by disentangling the dependency of support images, our critical analysis reveals two inherent limitations of static joint embedding: (1) polysemy-induced cross-category ambiguity during the matching process(e.g., the concept \"leg\" exhibiting divergent visual manifestations across humans and furniture), and (2) insufficient discriminability for fine-grained intra-category variations (e.g., posture and fur discrepancies between a sleeping white cat and a standing black cat). To overcome these challenges, we propose a new framework that innovatively integrates hierarchical cross-modal interaction with dual-stream feature refinement, enhancing the joint embedding with both class-level and instance-specific cues from textual description and specific images. Experiments on the MP-100 dataset demonstrate that, regardless of the network backbone, CapeNext consistently outperforms state-of-the-art CAPE methods by a large margin.",
            "headline_zh": "提出CapeNext框架，通过动态支持信息解决类别无关姿态估计中的语义歧义和细粒度差异问题。",
            "intro_zh": [
                "核心问题：静态关节嵌入存在跨类别语义歧义和细粒度变化区分不足。",
                "方法要点：集成层次跨模态交互与双流特征精炼，增强关节嵌入。",
                "实验效果：在MP-100数据集上大幅超越现有方法，不依赖网络骨干。"
            ],
            "tags_zh": [
                "类别无关姿态估计",
                "跨模态交互",
                "特征精炼",
                "语义嵌入",
                "姿态匹配"
            ],
            "_index": 101
        },
        {
            "title": "Count Every Rotation and Every Rotation Counts: Exploring Drone Dynamics via Propeller Sensing",
            "authors": [
                "Xuecheng Chen",
                "Jingao Xu",
                "Wenhua Ding",
                "Haoyang Wang",
                "Xinyu Luo",
                "Ruiyang Duan",
                "Jialong Chen",
                "Xueqian Wang",
                "Yunhao Liu",
                "Xinlei Chen"
            ],
            "arxiv_id": "2511.13100v1",
            "summary": "As drone-based applications proliferate, paramount contactless sensing of airborne drones from the ground becomes indispensable. This work demonstrates concentrating on propeller rotational speed will substantially improve drone sensing performance and proposes an event-camera-based solution, \\sysname. \\sysname features two components: \\textit{Count Every Rotation} achieves accurate, real-time propeller speed estimation by mitigating ultra-high sensitivity of event cameras to environmental noise. \\textit{Every Rotation Counts} leverages these speeds to infer both internal and external drone dynamics. Extensive evaluations in real-world drone delivery scenarios show that \\sysname achieves a sensing latency of 3$ms$ and a rotational speed estimation error of merely 0.23\\%. Additionally, \\sysname infers drone flight commands with 96.5\\% precision and improves drone tracking accuracy by over 22\\% when combined with other sensing modalities. \\textit{ Demo: {\\color{blue}https://eventpro25.github.io/EventPro/.} }",
            "headline_zh": "提出基于事件相机的无人机螺旋桨转速感知系统，以提升地面非接触式无人机感知性能。",
            "intro_zh": [
                "核心问题：无人机应用中，地面非接触式感知无人机面临挑战，需高精度实时监测。",
                "方法要点：利用事件相机估计螺旋桨转速，通过降噪和动态推断实现无人机状态感知。",
                "实验或效果：在真实交付场景中，实现3ms延迟、0.23%转速误差和96.5%命令推断精度。"
            ],
            "tags_zh": [
                "无人机感知",
                "事件相机",
                "螺旋桨转速估计",
                "非接触式传感",
                "动态推断"
            ],
            "_index": 102
        },
        {
            "title": "MergeSlide: Continual Model Merging and Task-to-Class Prompt-Aligned Inference for Lifelong Learning on Whole Slide Images",
            "authors": [
                "Doanh C. Bui",
                "Ba Hung Ngo",
                "Hoai Luan Pham",
                "Khang Nguyen",
                "Maï K. Nguyen",
                "Yasuhiko Nakashima"
            ],
            "arxiv_id": "2511.13099v1",
            "summary": "Lifelong learning on Whole Slide Images (WSIs) aims to train or fine-tune a unified model sequentially on cancer-related tasks, reducing the resources and effort required for data transfer and processing, especially given the gigabyte-scale size of WSIs. In this paper, we introduce MergeSlide, a simple yet effective framework that treats lifelong learning as a model merging problem by leveraging a vision-language pathology foundation model. When a new task arrives, it is: 1) defined with class-aware prompts, 2) fine-tuned for a few epochs using an MLP-free backbone, and 3) merged into a unified model using an orthogonal continual merging strategy that preserves performance and mitigates catastrophic forgetting. For inference under the class-incremental learning (CLASS-IL) setting, where task identity is unknown, we introduce Task-to-Class Prompt-aligned (TCP) inference. Specifically, TCP first identifies the most relevant task using task-level prompts and then applies the corresponding class-aware prompts to generate predictions. To evaluate MergeSlide, we conduct experiments on a stream of six TCGA datasets. The results show that MergeSlide outperforms both rehearsal-based continual learning and vision-language zero-shot baselines. Code and data are available at https://github.com/caodoanh2001/MergeSlide.",
            "headline_zh": "提出MergeSlide框架，通过模型合并和提示对齐推理解决全切片图像终身学习问题",
            "intro_zh": [
                "核心问题：全切片图像终身学习中资源消耗大且易发生灾难性遗忘",
                "方法要点：利用正交合并策略和任务到类提示对齐推理实现模型统一",
                "实验或效果：在TCGA数据集上优于基于排练和零样本的基线方法"
            ],
            "tags_zh": [
                "终身学习",
                "全切片图像",
                "模型合并",
                "提示对齐",
                "灾难性遗忘",
                "视觉语言模型"
            ],
            "_index": 103
        },
        {
            "title": "ResAlignNet: A Data-Driven Approach for INS/DVL Alignment",
            "authors": [
                "Guy Damari",
                "Itzik Klein"
            ],
            "arxiv_id": "2511.13096v1",
            "summary": "Autonomous underwater vehicles rely on precise navigation systems that combine the inertial navigation system and the Doppler velocity log for successful missions in challenging environments where satellite navigation is unavailable. The effectiveness of this integration critically depends on accurate alignment between the sensor reference frames. Standard model-based alignment methods between these sensor systems suffer from lengthy convergence times, dependence on prescribed motion patterns, and reliance on external aiding sensors, significantly limiting operational flexibility. To address these limitations, this paper presents ResAlignNet, a data-driven approach using the 1D ResNet-18 architecture that transforms the alignment problem into deep neural network optimization, operating as an in-situ solution that requires only sensors on board without external positioning aids or complex vehicle maneuvers, while achieving rapid convergence in seconds. Additionally, the approach demonstrates the learning capabilities of Sim2Real transfer, enabling training in synthetic data while deploying in operational sensor measurements. Experimental validation using the Snapir autonomous underwater vehicle demonstrates that ResAlignNet achieves alignment accuracy within 0.8° using only 25 seconds of data collection, representing a 65\\% reduction in convergence time compared to standard velocity-based methods. The trajectory-independent solution eliminates motion pattern requirements and enables immediate vehicle deployment without lengthy pre-mission procedures, advancing underwater navigation capabilities through robust sensor-agnostic alignment that scales across different operational scenarios and sensor specifications.",
            "headline_zh": "提出ResAlignNet数据驱动方法以解决水下自主航行器INS/DVL传感器对齐问题",
            "intro_zh": [
                "标准模型对齐方法收敛慢、依赖特定运动模式，限制操作灵活性",
                "使用1D ResNet-18架构，将对齐问题转化为神经网络优化，无需外部辅助",
                "实验显示25秒数据实现0.8°精度，收敛时间减少65%，支持Sim2Real迁移"
            ],
            "tags_zh": [
                "传感器对齐",
                "数据驱动方法",
                "水下导航",
                "Sim2Real迁移",
                "神经网络优化"
            ],
            "_index": 104
        },
        {
            "title": "MEGA-GUI: Multi-stage Enhanced Grounding Agents for GUI Elements",
            "authors": [
                "SeokJoo Kwak",
                "Jihoon Kim",
                "Boyoun Kim",
                "Jung Jae Yoon",
                "Wooseok Jang",
                "Jeonghoon Hong",
                "Jaeho Yang",
                "Yeong-Dae Kwon"
            ],
            "arxiv_id": "2511.13087v1",
            "summary": "Graphical User Interface (GUI) grounding - the task of mapping natural language instructions to screen coordinates - is essential for autonomous agents and accessibility technologies. Existing systems rely on monolithic models or one-shot pipelines that lack modularity and fail under visual clutter and ambiguous instructions. We introduce MEGA-GUI, a multi-stage framework that separates grounding into coarse Region-of-Interest (ROI) selection and fine-grained element grounding, orchestrated by specialized vision-language agents. MEGA-GUI features a bidirectional ROI zoom algorithm that mitigates spatial dilution and a context-aware rewriting agent that reduces semantic ambiguity. Our analysis reveals complementary strengths and weaknesses across vision-language models at different visual scales, and we show that leveraging this modular structure achieves consistently higher accuracy than monolithic approaches. On the visually dense ScreenSpot-Pro benchmark, MEGA-GUI attains 73.18% accuracy, and on the semantically complex OSWorld-G benchmark it reaches 68.63%, surpassing previously reported results. Code and the Grounding Benchmark Toolkit (GBT) are available at https://github.com/samsungsds-research-papers/mega-gui.",
            "headline_zh": "提出多阶段增强GUI元素定位框架以解决视觉杂乱和语义模糊问题",
            "intro_zh": [
                "核心问题：现有GUI定位系统缺乏模块化，在视觉杂乱和模糊指令下性能不佳",
                "方法要点：采用多阶段框架，分离粗粒度ROI选择和细粒度元素定位，使用双向ROI缩放和上下文重写代理",
                "实验或效果：在ScreenSpot-Pro和OSWorld-G基准上分别达到73.18%和68.63%准确率，超越先前方法"
            ],
            "tags_zh": [
                "GUI定位",
                "多阶段框架",
                "视觉语言模型",
                "ROI选择",
                "语义消歧",
                "基准测试"
            ],
            "_index": 105
        },
        {
            "title": "Real-time prediction of breast cancer sites using deformation-aware graph neural network",
            "authors": [
                "Kyunghyun Lee",
                "Yong-Min Shin",
                "Minwoo Shin",
                "Jihun Kim",
                "Sunghwan Lim",
                "Won-Yong Shin",
                "Kyungho Yoon"
            ],
            "arxiv_id": "2511.13082v1",
            "summary": "Early diagnosis of breast cancer is crucial, enabling the establishment of appropriate treatment plans and markedly enhancing patient prognosis. While direct magnetic resonance imaging-guided biopsy demonstrates promising performance in detecting cancer lesions, its practical application is limited by prolonged procedure times and high costs. To overcome these issues, an indirect MRI-guided biopsy that allows the procedure to be performed outside of the MRI room has been proposed, but it still faces challenges in creating an accurate real-time deformable breast model. In our study, we tackled this issue by developing a graph neural network (GNN)-based model capable of accurately predicting deformed breast cancer sites in real time during biopsy procedures. An individual-specific finite element (FE) model was developed by incorporating magnetic resonance (MR) image-derived structural information of the breast and tumor to simulate deformation behaviors. A GNN model was then employed, designed to process surface displacement and distance-based graph data, enabling accurate prediction of overall tissue displacement, including the deformation of the tumor region. The model was validated using phantom and real patient datasets, achieving an accuracy within 0.2 millimeters (mm) for cancer node displacement (RMSE) and a dice similarity coefficient (DSC) of 0.977 for spatial overlap with actual cancerous regions. Additionally, the model enabled real-time inference and achieved a speed-up of over 4,000 times in computational cost compared to conventional FE simulations. The proposed deformation-aware GNN model offers a promising solution for real-time tumor displacement prediction in breast biopsy, with high accuracy and real-time capability. Its integration with clinical procedures could significantly enhance the precision and efficiency of breast cancer diagnosis.",
            "headline_zh": "提出变形感知图神经网络以实时预测乳腺癌活检中的肿瘤位移",
            "intro_zh": [
                "核心问题：间接MRI引导活检中，实时精确预测乳腺变形模型存在挑战。",
                "方法要点：结合个体有限元模型和图神经网络，处理表面位移与距离图数据。",
                "实验或效果：验证显示位移误差小于0.2毫米，计算速度提升超4000倍。"
            ],
            "tags_zh": [
                "图神经网络",
                "乳腺癌活检",
                "变形预测",
                "实时推理",
                "有限元模拟"
            ],
            "_index": 106
        },
        {
            "title": "Rethinking Saliency Maps: A Cognitive Human Aligned Taxonomy and Evaluation Framework for Explanations",
            "authors": [
                "Yehonatan Elisha",
                "Seffi Cohen",
                "Oren Barkan",
                "Noam Koenigstein"
            ],
            "arxiv_id": "2511.13081v1",
            "summary": "Saliency maps are widely used for visual explanations in deep learning, but a fundamental lack of consensus persists regarding their intended purpose and alignment with diverse user queries. This ambiguity hinders the effective evaluation and practical utility of explanation methods.We address this gap by introducing the Reference-Frame $\\times$ Granularity (RFxG) taxonomy, a principled conceptual framework that organizes saliency explanations along two essential axes:Reference-Frame: Distinguishing between pointwise (\"Why this prediction?\") and contrastive (\"Why this and not an alternative?\") explanations.Granularity: Ranging from fine-grained class-level (e.g., \"Why Husky?\") to coarse-grained group-level (e.g., \"Why Dog?\") interpretations.Using the RFxG lens, we demonstrate critical limitations in existing evaluation metrics, which overwhelmingly prioritize pointwise faithfulness while neglecting contrastive reasoning and semantic granularity. To systematically assess explanation quality across both RFxG dimensions, we propose four novel faithfulness metrics. Our comprehensive evaluation framework applies these metrics to ten state-of-the-art saliency methods, four model architectures, and three datasets.By advocating a shift toward user-intent-driven evaluation, our work provides both the conceptual foundation and the practical tools necessary to develop visual explanations that are not only faithful to the underlying model behavior but are also meaningfully aligned with the complexity of human understanding and inquiry.",
            "headline_zh": "提出RFxG分类法与评估框架以解决显著图解释与用户意图对齐问题",
            "intro_zh": [
                "核心问题：显著图缺乏统一目的定义，难以评估其对用户查询的对齐性",
                "方法要点：引入RFxG分类法，基于参考框架和粒度组织解释类型",
                "实验或效果：提出四种新指标，评估十种方法，揭示现有指标局限性"
            ],
            "tags_zh": [
                "显著图解释",
                "解释评估框架",
                "RFxG分类法",
                "用户意图对齐",
                "深度学习可解释性"
            ],
            "_index": 107
        },
        {
            "title": "Decoupling Scene Perception and Ego Status: A Multi-Context Fusion Approach for Enhanced Generalization in End-to-End Autonomous Driving",
            "authors": [
                "Jiacheng Tang",
                "Mingyue Feng",
                "Jiachao Liu",
                "Yaonong Wang",
                "Jian Pu"
            ],
            "arxiv_id": "2511.13079v1",
            "summary": "Modular design of planning-oriented autonomous driving has markedly advanced end-to-end systems. However, existing architectures remain constrained by an over-reliance on ego status, hindering generalization and robust scene understanding. We identify the root cause as an inherent design within these architectures that allows ego status to be easily leveraged as a shortcut. Specifically, the premature fusion of ego status in the upstream BEV encoder allows an information flow from this strong prior to dominate the downstream planning module. To address this challenge, we propose AdaptiveAD, an architectural-level solution based on a multi-context fusion strategy. Its core is a dual-branch structure that explicitly decouples scene perception and ego status. One branch performs scene-driven reasoning based on multi-task learning, but with ego status deliberately omitted from the BEV encoder, while the other conducts ego-driven reasoning based solely on the planning task. A scene-aware fusion module then adaptively integrates the complementary decisions from the two branches to form the final planning trajectory. To ensure this decoupling does not compromise multi-task learning, we introduce a path attention mechanism for ego-BEV interaction and add two targeted auxiliary tasks: BEV unidirectional distillation and autoregressive online mapping. Extensive evaluations on the nuScenes dataset demonstrate that AdaptiveAD achieves state-of-the-art open-loop planning performance. Crucially, it significantly mitigates the over-reliance on ego status and exhibits impressive generalization capabilities across diverse scenarios.",
            "headline_zh": "提出AdaptiveAD以解决端到端自动驾驶中过度依赖自车状态的问题",
            "intro_zh": [
                "核心问题：现有架构中自车状态过早融合，导致规划模块过度依赖此捷径，影响泛化能力。",
                "方法要点：采用双分支结构解耦场景感知与自车状态，并通过场景感知融合模块自适应整合决策。",
                "实验或效果：在nuScenes数据集上实现最优开环规划性能，显著提升泛化能力。"
            ],
            "tags_zh": [
                "端到端自动驾驶",
                "场景感知解耦",
                "多上下文融合",
                "BEV编码",
                "规划轨迹生成",
                "泛化能力"
            ],
            "_index": 108
        },
        {
            "title": "Orientation-Free Neural Network-Based Bias Estimation for Low-Cost Stationary Accelerometers",
            "authors": [
                "Michal Levin",
                "Itzik Klein"
            ],
            "arxiv_id": "2511.13071v1",
            "summary": "Low-cost micro-electromechanical accelerometers are widely used in navigation, robotics, and consumer devices for motion sensing and position estimation. However, their performance is often degraded by bias errors. To eliminate deterministic bias terms a calibration procedure is applied under stationary conditions. It requires accelerom- eter leveling or complex orientation-dependent calibration procedures. To overcome those requirements, in this paper we present a model-free learning-based calibration method that estimates accelerometer bias under stationary conditions, without requiring knowledge of the sensor orientation and without the need to rotate the sensors. The proposed approach provides a fast, practical, and scalable solution suitable for rapid field deployment. Experimental validation on a 13.39-hour dataset collected from six accelerometers shows that the proposed method consistently achieves error levels more than 52% lower than traditional techniques. On a broader scale, this work contributes to the advancement of accurate calibration methods in orientation-free scenarios. As a consequence, it improves the reliability of low-cost inertial sensors in diverse scientific and industrial applications and eliminates the need for leveled calibration.",
            "headline_zh": "提出基于神经网络的校准方法以解决低成本加速度计在静止条件下的偏差估计问题",
            "intro_zh": [
                "核心问题：低成本加速度计偏差误差影响性能，传统校准需传感器水平或复杂定向过程",
                "方法要点：使用无模型学习方法估计偏差，无需传感器定向或旋转",
                "实验或效果：在13.39小时数据集上验证，误差比传统方法降低超52%"
            ],
            "tags_zh": [
                "加速度计校准",
                "神经网络方法",
                "偏差估计",
                "静止条件",
                "低成本传感器"
            ],
            "_index": 109
        },
        {
            "title": "RobustGait: Robustness Analysis for Appearance Based Gait Recognition",
            "authors": [
                "Reeshoon Sayera",
                "Akash Kumar",
                "Sirshapan Mitra",
                "Prudvi Kamtam",
                "Yogesh S Rawat"
            ],
            "arxiv_id": "2511.13065v1",
            "summary": "Appearance-based gait recognition have achieved strong performance on controlled datasets, yet systematic evaluation of its robustness to real-world corruptions and silhouette variability remains lacking. We present RobustGait, a framework for fine-grained robustness evaluation of appearance-based gait recognition systems. RobustGait evaluation spans four dimensions: the type of perturbation (digital, environmental, temporal, occlusion), the silhouette extraction method (segmentation and parsing networks), the architectural capacities of gait recognition models, and various deployment scenarios. The benchmark introduces 15 corruption types at 5 severity levels across CASIA-B, CCPG, and SUSTech1K, with in-the-wild validation on MEVID, and evaluates six state-of-the-art gait systems. We came across several exciting insights. First, applying noise at the RGB level better reflects real-world degradation, and reveal how distortions propagate through silhouette extraction to the downstream gait recognition systems. Second, gait accuracy is highly sensitive to silhouette extractor biases, revealing an overlooked source of benchmark bias. Third, robustness is dependent on both the type of perturbation and the architectural design. Finally, we explore robustness-enhancing strategies, showing that noise-aware training and knowledge distillation improve performance and move toward deployment-ready systems.",
            "headline_zh": "提出RobustGait框架以评估步态识别系统在真实世界扰动下的鲁棒性",
            "intro_zh": [
                "核心问题：步态识别在受控数据集表现良好，但缺乏对真实世界扰动和轮廓变化的鲁棒性评估",
                "方法要点：构建多维度评估框架，涵盖扰动类型、轮廓提取方法、模型架构和部署场景",
                "实验或效果：在多个数据集上测试15种扰动，发现噪声训练和知识蒸馏可提升鲁棒性"
            ],
            "tags_zh": [
                "步态识别",
                "鲁棒性评估",
                "轮廓提取",
                "噪声训练",
                "知识蒸馏",
                "扰动分析"
            ],
            "_index": 110
        },
        {
            "title": "FGNet: Leveraging Feature-Guided Attention to Refine SAM2 for 3D EM Neuron Segmentation",
            "authors": [
                "Zhenghua Li",
                "Hang Chen",
                "Zihao Sun",
                "Kai Li",
                "Xiaolin Hu"
            ],
            "arxiv_id": "2511.13063v1",
            "summary": "Accurate segmentation of neural structures in Electron Microscopy (EM) images is paramount for neuroscience. However, this task is challenged by intricate morphologies, low signal-to-noise ratios, and scarce annotations, limiting the accuracy and generalization of existing methods. To address these challenges, we seek to leverage the priors learned by visual foundation models on a vast amount of natural images to better tackle this task. Specifically, we propose a novel framework that can effectively transfer knowledge from Segment Anything 2 (SAM2), which is pre-trained on natural images, to the EM domain. We first use SAM2 to extract powerful, general-purpose features. To bridge the domain gap, we introduce a Feature-Guided Attention module that leverages semantic cues from SAM2 to guide a lightweight encoder, the Fine-Grained Encoder (FGE), in focusing on these challenging regions. Finally, a dual-affinity decoder generates both coarse and refined affinity maps. Experimental results demonstrate that our method achieves performance comparable to state-of-the-art (SOTA) approaches with the SAM2 weights frozen. Upon further fine-tuning on EM data, our method significantly outperforms existing SOTA methods. This study validates that transferring representations pre-trained on natural images, when combined with targeted domain-adaptive guidance, can effectively address the specific challenges in neuron segmentation.",
            "headline_zh": "提出FGNet框架，利用特征引导注意力优化SAM2，以解决电子显微镜神经元分割问题",
            "intro_zh": [
                "核心问题：电子显微镜图像中神经元分割面临形态复杂、噪声高和标注稀缺的挑战",
                "方法要点：引入特征引导注意力模块，桥接自然图像与EM领域的域差距",
                "实验或效果：在SAM2权重冻结时性能可比SOTA，微调后显著超越现有方法"
            ],
            "tags_zh": [
                "神经元分割",
                "特征引导注意力",
                "域适应",
                "SAM2迁移",
                "电子显微镜图像"
            ],
            "_index": 111
        },
        {
            "title": "Monocular 3D Lane Detection via Structure Uncertainty-Aware Network with Curve-Point Queries",
            "authors": [
                "Ruixin Liu",
                "Zejian Yuan"
            ],
            "arxiv_id": "2511.13055v1",
            "summary": "Monocular 3D lane detection is challenged by aleatoric uncertainty arising from inherent observation noise. Existing methods rely on simplified geometric assumptions, such as independent point predictions or global planar modeling, failing to capture structural variations and aleatoric uncertainty in real-world scenarios. In this paper, we propose MonoUnc, a bird's-eye view (BEV)-free 3D lane detector that explicitly models aleatoric uncertainty informed by local lane structures. Specifically, 3D lanes are projected onto the front-view (FV) space and approximated by parametric curves. Guided by curve predictions, curve-point query embeddings are dynamically generated for lane point predictions in 3D space. Each segment formed by two adjacent points is modeled as a 3D Gaussian, parameterized by the local structure and uncertainty estimations. Accordingly, a novel 3D Gaussian matching loss is designed to constrain these parameters jointly. Experiments on the ONCE-3DLanes and OpenLane datasets demonstrate that MonoUnc outperforms previous state-of-the-art (SoTA) methods across all benchmarks under stricter evaluation criteria. Additionally, we propose two comprehensive evaluation metrics for ONCE-3DLanes, calculating the average and maximum bidirectional Chamfer distances to quantify global and local errors. Codes are released at https://github.com/lrx02/MonoUnc.",
            "headline_zh": "提出MonoUnc以解决单目3D车道检测中的结构不确定性问题",
            "intro_zh": [
                "核心问题：单目3D车道检测存在观测噪声导致的不确定性，现有方法简化几何假设，无法捕捉真实场景结构变化。",
                "方法要点：基于曲线点查询动态生成嵌入，建模3D高斯段以估计局部结构和不确定性。",
                "实验或效果：在ONCE-3DLanes和OpenLane数据集上超越SoTA，并引入新评估指标量化全局和局部误差。"
            ],
            "tags_zh": [
                "单目3D车道检测",
                "不确定性建模",
                "曲线点查询",
                "3D高斯匹配",
                "鸟瞰图无关",
                "评估指标"
            ],
            "_index": 112
        },
        {
            "title": "ViSS-R1: Self-Supervised Reinforcement Video Reasoning",
            "authors": [
                "Bo Fang",
                "Yuxin Song",
                "Qiangqiang Wu",
                "Haoyuan Sun",
                "Wenhao Wu",
                "Antoni B. Chan"
            ],
            "arxiv_id": "2511.13054v1",
            "summary": "Complex video reasoning remains a significant challenge for Multimodal Large Language Models (MLLMs), as current R1-based methodologies often prioritize text-centric reasoning derived from text-based and image-based developments. In video tasks, such strategies frequently underutilize rich visual information, leading to potential shortcut learning and increased susceptibility to hallucination. To foster a more robust, visual-centric video understanding, we start by introducing a novel self-supervised reinforcement learning GRPO algorithm (Pretext-GRPO) within the standard R1 pipeline, in which positive rewards are assigned for correctly solving pretext tasks on transformed visual inputs, which makes the model to non-trivially process the visual information. Building on the effectiveness of Pretext-GRPO, we further propose the ViSS-R1 framework, which streamlines and integrates pretext-task-based self-supervised learning directly into the MLLM's R1 post-training paradigm. Instead of relying solely on sparse visual cues, our framework compels models to reason about transformed visual input by simultaneously processing both pretext questions (concerning transformations) and true user queries. This necessitates identifying the applied transformation and reconstructing the original video to formulate accurate final answers. Comprehensive evaluations on six widely-used video reasoning and understanding benchmarks demonstrate the effectiveness and superiority of our Pretext-GRPO and ViSS-R1 for complex video reasoning. Our codes and models will be publicly available.",
            "headline_zh": "提出ViSS-R1框架，通过自监督强化学习提升多模态大语言模型的复杂视频推理能力",
            "intro_zh": [
                "核心问题：当前R1方法在视频推理中过度依赖文本，忽视视觉信息，易导致捷径学习和幻觉",
                "方法要点：引入Pretext-GRPO算法，通过自监督任务奖励模型处理变换后的视觉输入",
                "实验或效果：在六个视频推理基准上验证了ViSS-R1的有效性和优越性"
            ],
            "tags_zh": [
                "自监督学习",
                "强化学习",
                "视频推理",
                "多模态大语言模型",
                "R1后训练",
                "视觉变换"
            ],
            "_index": 113
        },
        {
            "title": "Unidirectional-Road-Network-Based Global Path Planning for Cleaning Robots in Semi-Structured Environments",
            "authors": [
                "Yong Li",
                "Hui Cheng"
            ],
            "arxiv_id": "2511.13048v1",
            "summary": "Practical global path planning is critical for commercializing cleaning robots working in semi-structured environments. In the literature, global path planning methods for free space usually focus on path length and neglect the traffic rule constraints of the environments, which leads to high-frequency re-planning and increases collision risks. In contrast, those for structured environments are developed mainly by strictly complying with the road network representing the traffic rule constraints, which may result in an overlong path that hinders the overall navigation efficiency. This article proposes a general and systematic approach to improve global path planning performance in semi-structured environments. A unidirectional road network is built to represent the traffic constraints in semi-structured environments and a hybrid strategy is proposed to achieve a guaranteed planning result.Cutting across the road at the starting and the goal points are allowed to achieve a shorter path. Especially, a two-layer potential map is proposed to achieve a guaranteed performance when the starting and the goal points are in complex intersections. Comparative experiments are carried out to validate the effectiveness of the proposed method. Quantitative experimental results show that, compared with the state-of-art, the proposed method guarantees a much better balance between path length and the consistency with the road network.",
            "headline_zh": "提出基于单向路网的全局路径规划方法，以优化清洁机器人在半结构化环境中的导航效率。",
            "intro_zh": [
                "核心问题：现有方法在路径长度与交通规则约束间失衡，导致重规划频繁或路径过长。",
                "方法要点：构建单向路网表示交通约束，允许起点和终点跨越道路以缩短路径。",
                "实验或效果：实验验证方法在路径长度与路网一致性间取得更好平衡，优于现有技术。"
            ],
            "tags_zh": [
                "全局路径规划",
                "清洁机器人",
                "半结构化环境",
                "单向路网",
                "路径优化"
            ],
            "_index": 114
        },
        {
            "title": "DiffPixelFormer: Differential Pixel-Aware Transformer for RGB-D Indoor Scene Segmentation",
            "authors": [
                "Yan Gong",
                "Jianli Lu",
                "Yongsheng Gao",
                "Jie Zhao",
                "Xiaojuan Zhang",
                "Susanto Rahardja"
            ],
            "arxiv_id": "2511.13047v1",
            "summary": "Indoor semantic segmentation is fundamental to computer vision and robotics, supporting applications such as autonomous navigation, augmented reality, and smart environments. Although RGB-D fusion leverages complementary appearance and geometric cues, existing methods often depend on computationally intensive cross-attention mechanisms and insufficiently model intra- and inter-modal feature relationships, resulting in imprecise feature alignment and limited discriminative representation. To address these challenges, we propose DiffPixelFormer, a differential pixel-aware Transformer for RGB-D indoor scene segmentation that simultaneously enhances intra-modal representations and models inter-modal interactions. At its core, the Intra-Inter Modal Interaction Block (IIMIB) captures intra-modal long-range dependencies via self-attention and models inter-modal interactions with the Differential-Shared Inter-Modal (DSIM) module to disentangle modality-specific and shared cues, enabling fine-grained, pixel-level cross-modal alignment. Furthermore, a dynamic fusion strategy balances modality contributions and fully exploits RGB-D information according to scene characteristics. Extensive experiments on the SUN RGB-D and NYUDv2 benchmarks demonstrate that DiffPixelFormer-L achieves mIoU scores of 54.28% and 59.95%, outperforming DFormer-L by 1.78% and 2.75%, respectively. Code is available at https://github.com/gongyan1/DiffPixelFormer.",
            "headline_zh": "提出DiffPixelFormer以解决RGB-D室内场景分割中的特征对齐和表示问题",
            "intro_zh": [
                "RGB-D融合方法依赖计算密集型跨注意力，特征关系建模不足导致对齐不精确",
                "核心IIMIB模块通过自注意力捕获模态内依赖，DSIM模块解耦模态特定和共享线索",
                "在SUN RGB-D和NYUDv2基准上mIoU达54.28%和59.95%，优于DFormer-L"
            ],
            "tags_zh": [
                "RGB-D融合",
                "Transformer模型",
                "室内语义分割",
                "跨模态对齐",
                "动态融合策略"
            ],
            "_index": 115
        },
        {
            "title": "APP: A* Post-Processing Algorithm for Robots with Bidirectional Shortcut and Path Perturbation",
            "authors": [
                "Yong Li",
                "Hui Cheng"
            ],
            "arxiv_id": "2511.13042v1",
            "summary": "Paths generated by A* and other graph-search-based planners are widely used in the robotic field. Due to the restricted node-expansion directions, the resulting paths are usually not the shortest. Besides, unnecessary heading changes, or zig-zag patterns, exist even when no obstacle is nearby, which is inconsistent with the human intuition that the path segments should be straight in wide-open space due to the absence of obstacles. This article puts forward a general and systematic post-processing algorithm for A* and other graph-search-based planners. The A* post-processing algorithm, called APP, is developed based on the costmap, which is widely used in commercial service robots. First, a bidirectional vertices reduction algorithm is proposed to tackle the asymm- etry of the path and the environments. During the forward and backward vertices reduction, a thorough shortcut strategy is put forward to improve the path-shortening performance and avoid unnecessary heading changes. Second, an iterative path perturbation algorithm is adopted to locally reduce the number of unnecessary heading changes and improve the path smooth- ness. Comparative experiments are then carried out to validate the superiority of the proposed method. Quantitative performance indexes show that APP outperforms the existing methods in planning time, path length as well as the number of unnecessary heading changes. Finally, field navigation experiments are carried out to verify the practicability of APP.",
            "headline_zh": "提出APP算法以优化A*路径规划，减少路径长度和不必要转向",
            "intro_zh": [
                "A*等图搜索规划器生成路径存在非最短和不必要转向问题",
                "采用双向顶点缩减和路径扰动方法提升路径平滑度和缩短性能",
                "实验显示APP在规划时间、路径长度和转向次数上优于现有方法"
            ],
            "tags_zh": [
                "路径规划",
                "A*算法",
                "后处理优化",
                "机器人导航",
                "路径平滑"
            ],
            "_index": 116
        },
        {
            "title": "MGCA-Net: Multi-Grained Category-Aware Network for Open-Vocabulary Temporal Action Localization",
            "authors": [
                "Zhenying Fang",
                "Richang Hong"
            ],
            "arxiv_id": "2511.13039v1",
            "summary": "Open-Vocabulary Temporal Action Localization (OV-TAL) aims to recognize and localize instances of any desired action categories in videos without explicitly curating training data for all categories. Existing methods mostly recognize action categories at a single granularity, which degrades the recognition accuracy of both base and novel action categories. To address these issues, we propose a Multi-Grained Category-Aware Network (MGCA-Net) comprising a localizer, an action presence predictor, a conventional classifier, and a coarse-to-fine classifier. Specifically, the localizer localizes category-agnostic action proposals. For these action proposals, the action presence predictor estimates the probability that they belong to an action instance. At the same time, the conventional classifier predicts the probability of each action proposal over base action categories at the snippet granularity. Novel action categories are recognized by the coarse-to-fine classifier, which first identifies action presence at the video granularity. Finally, it assigns each action proposal to one category from the coarse categories at the proposal granularity. Through coarse-to-fine category awareness for novel actions and the conventional classifier's awareness of base actions, multi-grained category awareness is achieved, effectively enhancing localization performance. Comprehensive evaluations on the THUMOS'14 and ActivityNet-1.3 benchmarks demonstrate that our method achieves state-of-the-art performance. Furthermore, our MGCA-Net achieves state-of-the-art results under the Zero-Shot Temporal Action Localization setting.",
            "headline_zh": "提出多粒度类别感知网络以解决开放词汇时序动作定位中单粒度识别精度低的问题",
            "intro_zh": [
                "核心问题：现有方法在单粒度识别动作类别，导致基础和新增类别识别精度下降",
                "方法要点：结合定位器、动作存在预测器、常规分类器和粗到细分类器实现多粒度类别感知",
                "实验或效果：在THUMOS'14和ActivityNet-1.3基准上达到最先进性能，零样本设置表现优异"
            ],
            "tags_zh": [
                "开放词汇时序动作定位",
                "多粒度类别感知",
                "动作识别",
                "视频理解",
                "零样本学习"
            ],
            "_index": 117
        },
        {
            "title": "uCLIP: Parameter-Efficient Multilingual Extension of Vision-Language Models with Unpaired Data",
            "authors": [
                "Dahyun Chung",
                "Donghyun Shin",
                "Yujin Sung",
                "Seunggi Moon",
                "Jinwoo Jeon",
                "Byung-Jun Lee"
            ],
            "arxiv_id": "2511.13036v1",
            "summary": "Contrastive Language-Image Pre-training (CLIP) has demonstrated strong generalization across a wide range of visual tasks by leveraging large-scale English-image pairs. However, its extension to low-resource languages remains limited due to the scarcity of high-quality multilingual image-text data. Existing multilingual vision-language models exhibit consistently low retrieval performance in underrepresented languages including Czech, Finnish, Croatian, Hungarian, and Romanian on the Crossmodal-3600 (XM3600) benchmark. To address this, we propose a lightweight and data-efficient framework for multilingual vision-language alignment. Our approach requires no image-text pairs or text-text pairs and freezes both the pretrained image encoder and multilingual text encoder during training. Only a compact 1.7M-parameter projection module is trained, using a contrastive loss over English representations as semantic anchors. This minimal training setup enables robust multilingual alignment even for languages with limited supervision. Extensive evaluation across multiple multilingual retrieval benchmarks confirms the effectiveness of our method, showing significant gains in five underrepresented languages where existing models typically underperform. These findings highlight the effectiveness of our pivot-based, parameter-efficient alignment strategy for inclusive multimodal learning.",
            "headline_zh": "提出uCLIP框架以解决低资源语言视觉-语言对齐问题",
            "intro_zh": [
                "核心问题：CLIP模型在低资源语言中泛化差，因多语言图像-文本数据稀缺。",
                "方法要点：仅训练轻量投影模块，冻结图像和文本编码器，使用英语表示作为语义锚点。",
                "实验或效果：在XM3600基准上，对五种低资源语言检索性能显著提升。"
            ],
            "tags_zh": [
                "多语言视觉-语言模型",
                "参数高效对齐",
                "对比学习",
                "低资源语言",
                "图像-文本检索"
            ],
            "_index": 118
        },
        {
            "title": "Uni-Inter: Unifying 3D Human Motion Synthesis Across Diverse Interaction Contexts",
            "authors": [
                "Sheng Liu",
                "Yuanzhi Liang",
                "Jiepeng Wang",
                "Sidan Du",
                "Chi Zhang",
                "Xuelong Li"
            ],
            "arxiv_id": "2511.13032v1",
            "summary": "We present Uni-Inter, a unified framework for human motion generation that supports a wide range of interaction scenarios: including human-human, human-object, and human-scene-within a single, task-agnostic architecture. In contrast to existing methods that rely on task-specific designs and exhibit limited generalization, Uni-Inter introduces the Unified Interactive Volume (UIV), a volumetric representation that encodes heterogeneous interactive entities into a shared spatial field. This enables consistent relational reasoning and compound interaction modeling. Motion generation is formulated as joint-wise probabilistic prediction over the UIV, allowing the model to capture fine-grained spatial dependencies and produce coherent, context-aware behaviors. Experiments across three representative interaction tasks demonstrate that Uni-Inter achieves competitive performance and generalizes well to novel combinations of entities. These results suggest that unified modeling of compound interactions offers a promising direction for scalable motion synthesis in complex environments.",
            "headline_zh": "提出Uni-Inter统一框架，支持人-人、人-物、人-场景交互的3D人体运动合成。",
            "intro_zh": [
                "现有方法依赖任务特定设计，泛化能力有限，无法统一处理多种交互场景。",
                "引入统一交互体积（UIV）表示，编码异质实体到共享空间，实现关系推理和复合交互建模。",
                "在三种交互任务实验中，模型性能竞争性强，并能泛化到新实体组合。"
            ],
            "tags_zh": [
                "3D人体运动合成",
                "统一交互建模",
                "体积表示",
                "关系推理",
                "复合交互",
                "任务无关架构"
            ],
            "_index": 119
        },
        {
            "title": "Towards 3D Object-Centric Feature Learning for Semantic Scene Completion",
            "authors": [
                "Weihua Wang",
                "Yubo Cui",
                "Xiangru Lin",
                "Zhiheng Li",
                "Zheng Fang"
            ],
            "arxiv_id": "2511.13031v1",
            "summary": "Vision-based 3D Semantic Scene Completion (SSC) has received growing attention due to its potential in autonomous driving. While most existing approaches follow an ego-centric paradigm by aggregating and diffusing features over the entire scene, they often overlook fine-grained object-level details, leading to semantic and geometric ambiguities, especially in complex environments. To address this limitation, we propose Ocean, an object-centric prediction framework that decomposes the scene into individual object instances to enable more accurate semantic occupancy prediction. Specifically, we first employ a lightweight segmentation model, MobileSAM, to extract instance masks from the input image. Then, we introduce a 3D Semantic Group Attention module that leverages linear attention to aggregate object-centric features in 3D space. To handle segmentation errors and missing instances, we further design a Global Similarity-Guided Attention module that leverages segmentation features for global interaction. Finally, we propose an Instance-aware Local Diffusion module that improves instance features through a generative process and subsequently refines the scene representation in the BEV space. Extensive experiments on the SemanticKITTI and SSCBench-KITTI360 benchmarks demonstrate that Ocean achieves state-of-the-art performance, with mIoU scores of 17.40 and 20.28, respectively.",
            "headline_zh": "提出Ocean对象中心框架以解决语义场景完成中的细粒度细节缺失问题",
            "intro_zh": [
                "核心问题：现有方法忽视对象级细节，导致语义和几何模糊，尤其在复杂环境中",
                "方法要点：使用MobileSAM提取实例掩码，结合3D语义组注意力和全局相似性引导注意力",
                "实验或效果：在SemanticKITTI和SSCBench-KITTI360基准上达到SOTA，mIoU分别为17.40和20.28"
            ],
            "tags_zh": [
                "语义场景完成",
                "对象中心学习",
                "3D特征聚合",
                "注意力机制",
                "BEV空间优化"
            ],
            "_index": 120
        },
        {
            "title": "REVISOR: Beyond Textual Reflection, Towards Multimodal Introspective Reasoning in Long-Form Video Understanding",
            "authors": [
                "Jiaze Li",
                "Hao Yin",
                "Wenhui Tan",
                "Jingyang Chen",
                "Boshen Xu",
                "Yuxun Qu",
                "Yijing Chen",
                "Jianzhong Ju",
                "Zhenbo Luo",
                "Jian Luan"
            ],
            "arxiv_id": "2511.13026v1",
            "summary": "Self-reflection mechanisms that rely on purely text-based rethinking processes perform well in most multimodal tasks. However, when directly applied to long-form video understanding scenarios, they exhibit clear limitations. The fundamental reasons for this lie in two points: (1)long-form video understanding involves richer and more dynamic visual input, meaning rethinking only the text information is insufficient and necessitates a further rethinking process specifically targeting visual information; (2) purely text-based reflection mechanisms lack cross-modal interaction capabilities, preventing them from fully integrating visual information during reflection. Motivated by these insights, we propose REVISOR (REflective VIsual Segment Oriented Reasoning), a novel framework for tool-augmented multimodal reflection. REVISOR enables MLLMs to collaboratively construct introspective reflection processes across textual and visual modalities, significantly enhancing their reasoning capability for long-form video understanding. To ensure that REVISOR can learn to accurately review video segments highly relevant to the question during reinforcement learning, we designed the Dual Attribution Decoupled Reward (DADR) mechanism. Integrated into the GRPO training strategy, this mechanism enforces causal alignment between the model's reasoning and the selected video evidence. Notably, the REVISOR framework significantly enhances long-form video understanding capability of MLLMs without requiring supplementary supervised fine-tuning or external models, achieving impressive results on four benchmarks including VideoMME, LongVideoBench, MLVU, and LVBench.",
            "headline_zh": "提出REVISOR框架以增强多模态大模型在长视频理解中的反思推理能力",
            "intro_zh": [
                "核心问题：纯文本反思机制在长视频理解中因视觉信息丰富和缺乏跨模态交互而受限",
                "方法要点：引入多模态反思过程，结合DADR奖励机制强化视频证据与推理的因果对齐",
                "实验或效果：在多个基准测试中显著提升性能，无需额外监督微调或外部模型"
            ],
            "tags_zh": [
                "长视频理解",
                "多模态反思",
                "强化学习",
                "因果对齐",
                "工具增强推理"
            ],
            "_index": 121
        },
        {
            "title": "SpectralAdapt: Semi-Supervised Domain Adaptation with Spectral Priors for Human-Centered Hyperspectral Image Reconstruction",
            "authors": [
                "Yufei Wen",
                "Yuting Zhang",
                "Jingdan Kang",
                "Hao Ren",
                "Weibin Cheng",
                "Jintai Chen",
                "Kaishun Wu"
            ],
            "arxiv_id": "2511.13020v1",
            "summary": "Hyperspectral imaging (HSI) holds great potential for healthcare due to its rich spectral information. However, acquiring HSI data remains costly and technically demanding. Hyperspectral image reconstruction offers a practical solution by recovering HSI data from accessible modalities, such as RGB. While general domain datasets are abundant, the scarcity of human HSI data limits progress in medical applications. To tackle this, we propose SpectralAdapt, a semi-supervised domain adaptation (SSDA) framework that bridges the domain gap between general and human-centered HSI datasets. To fully exploit limited labels and abundant unlabeled data, we enhance spectral reasoning by introducing Spectral Density Masking (SDM), which adaptively masks RGB channels based on their spectral complexity, encouraging recovery of informative regions from complementary cues during consistency training. Furthermore, we introduce Spectral Endmember Representation Alignment (SERA), which derives physically interpretable endmembers from valuable labeled pixels and employs them as domain-invariant anchors to guide unlabeled predictions, with momentum updates ensuring adaptability and stability. These components are seamlessly integrated into SpectralAdapt, a spectral prior-guided framework that effectively mitigates domain shift, spectral degradation, and data scarcity in HSI reconstruction. Experiments on benchmark datasets demonstrate consistent improvements in spectral fidelity, cross-domain generalization, and training stability, highlighting the promise of SSDA as an efficient solution for hyperspectral imaging in healthcare.",
            "headline_zh": "提出SpectralAdapt框架以解决人中心高光谱图像重建中的领域适应问题",
            "intro_zh": [
                "核心问题：人中心高光谱数据稀缺，导致医疗应用受限。",
                "方法要点：引入光谱密度掩码和端元表示对齐，提升半监督领域适应。",
                "实验或效果：在基准数据集上提高光谱保真度和跨域泛化能力。"
            ],
            "tags_zh": [
                "高光谱图像重建",
                "半监督领域适应",
                "光谱先验",
                "医疗成像",
                "跨域泛化"
            ],
            "_index": 122
        },
        {
            "title": "MeanFlow Transformers with Representation Autoencoders",
            "authors": [
                "Zheyuan Hu",
                "Chieh-Hsin Lai",
                "Ge Wu",
                "Yuki Mitsufuji",
                "Stefano Ermon"
            ],
            "arxiv_id": "2511.13019v1",
            "summary": "MeanFlow (MF) is a diffusion-motivated generative model that enables efficient few-step generation by learning long jumps directly from noise to data. In practice, it is often used as a latent MF by leveraging the pre-trained Stable Diffusion variational autoencoder (SD-VAE) for high-dimensional data modeling. However, MF training remains computationally demanding and is often unstable. During inference, the SD-VAE decoder dominates the generation cost, and MF depends on complex guidance hyperparameters for class-conditional generation. In this work, we develop an efficient training and sampling scheme for MF in the latent space of a Representation Autoencoder (RAE), where a pre-trained vision encoder (e.g., DINO) provides semantically rich latents paired with a lightweight decoder. We observe that naive MF training in the RAE latent space suffers from severe gradient explosion. To stabilize and accelerate training, we adopt Consistency Mid-Training for trajectory-aware initialization and use a two-stage scheme: distillation from a pre-trained flow matching teacher to speed convergence and reduce variance, followed by an optional bootstrapping stage with a one-point velocity estimator to further reduce deviation from the oracle mean flow. This design removes the need for guidance, simplifies training configurations, and reduces computation in both training and sampling. Empirically, our method achieves a 1-step FID of 2.03, outperforming vanilla MF's 3.43, while reducing sampling GFLOPS by 38% and total training cost by 83% on ImageNet 256. We further scale our approach to ImageNet 512, achieving a competitive 1-step FID of 3.23 with the lowest GFLOPS among all baselines. Code is available at https://github.com/sony/mf-rae.",
            "headline_zh": "提出基于表示自编码器的MeanFlow方法，以高效稳定训练和采样，用于图像生成。",
            "intro_zh": [
                "核心问题：MeanFlow训练计算量大、不稳定，且依赖复杂超参数指导。",
                "方法要点：在表示自编码器潜在空间训练，采用一致性中训练和两阶段蒸馏方案。",
                "实验效果：在ImageNet 256上1步FID达2.03，降低采样GFLOPS 38%和训练成本83%。"
            ],
            "tags_zh": [
                "图像生成",
                "表示自编码器",
                "MeanFlow",
                "蒸馏训练",
                "高效采样"
            ],
            "_index": 123
        },
        {
            "title": "Geometry Meets Light: Leveraging Geometric Priors for Universal Photometric Stereo under Limited Multi-Illumination Cues",
            "authors": [
                "King-Man Tam",
                "Satoshi Ikehata",
                "Yuta Asano",
                "Zhaoyi An",
                "Rei Kawakami"
            ],
            "arxiv_id": "2511.13015v1",
            "summary": "Universal Photometric Stereo is a promising approach for recovering surface normals without strict lighting assumptions. However, it struggles when multi-illumination cues are unreliable, such as under biased lighting or in shadows or self-occluded regions of complex in-the-wild scenes. We propose GeoUniPS, a universal photometric stereo network that integrates synthetic supervision with high-level geometric priors from large-scale 3D reconstruction models pretrained on massive in-the-wild data. Our key insight is that these 3D reconstruction models serve as visual-geometry foundation models, inherently encoding rich geometric knowledge of real scenes. To leverage this, we design a Light-Geometry Dual-Branch Encoder that extracts both multi-illumination cues and geometric priors from the frozen 3D reconstruction model. We also address the limitations of the conventional orthographic projection assumption by introducing the PS-Perp dataset with realistic perspective projection to enable learning of spatially varying view directions. Extensive experiments demonstrate that GeoUniPS delivers state-of-the-arts performance across multiple datasets, both quantitatively and qualitatively, especially in the complex in-the-wild scenes.",
            "headline_zh": "提出GeoUniPS以在有限多光照线索下提升通用光度立体性能",
            "intro_zh": [
                "通用光度立体在多光照线索不可靠时性能下降，如偏置光照或阴影区域",
                "集成合成监督与来自大规模3D重建模型的几何先验，设计光-几何双分支编码器",
                "在多个数据集上实现最先进性能，尤其在复杂野外场景中表现优异"
            ],
            "tags_zh": [
                "通用光度立体",
                "几何先验",
                "双分支编码器",
                "3D重建模型",
                "透视投影数据集"
            ],
            "_index": 124
        },
        {
            "title": "You Only Look Omni Gradient Backpropagation for Moving Infrared Small Target Detection",
            "authors": [
                "Guoyi Zhang",
                "Guangsheng Xu",
                "Siyang Chen",
                "Han Wang",
                "Xiaohu Zhang"
            ],
            "arxiv_id": "2511.13013v1",
            "summary": "Moving infrared small target detection is a key component of infrared search and tracking systems, yet it remains extremely challenging due to low signal-to-clutter ratios, severe target-background imbalance, and weak discriminative features. Existing deep learning methods primarily focus on spatio-temporal feature aggregation, but their gains are limited, revealing that the fundamental bottleneck lies in ambiguous per-frame feature representations rather than spatio-temporal modeling itself. Motivated by this insight, we propose BP-FPN, a backpropagation-driven feature pyramid architecture that fundamentally rethinks feature learning for small target. BP-FPN introduces Gradient-Isolated Low-Level Shortcut (GILS) to efficiently incorporate fine-grained target details without inducing shortcut learning, and Directional Gradient Regularization (DGR) to enforce hierarchical feature consistency during backpropagation. The design is theoretically grounded, introduces negligible computational overhead, and can be seamlessly integrated into existing frameworks. Extensive experiments on multiple public datasets show that BP-FPN consistently establishes new state-of-the-art performance. To the best of our knowledge, it is the first FPN designed for this task entirely from the backpropagation perspective.",
            "headline_zh": "提出BP-FPN以解决移动红外小目标检测中的特征表示瓶颈问题",
            "intro_zh": [
                "核心问题：移动红外小目标检测因低信噪比和目标-背景不平衡导致特征表示模糊",
                "方法要点：引入梯度隔离低层捷径和方向梯度正则化，提升特征学习效率",
                "实验或效果：在多个公共数据集上实现新的最先进性能，计算开销可忽略"
            ],
            "tags_zh": [
                "红外小目标检测",
                "特征金字塔网络",
                "梯度反向传播",
                "特征一致性",
                "目标-背景不平衡"
            ],
            "_index": 125
        },
        {
            "title": "Beyond Darkness: Thermal-Supervised 3D Gaussian Splatting for Low-Light Novel View Synthesis",
            "authors": [
                "Qingsen Ma",
                "Chen Zou",
                "Dianyun Wang",
                "Jia Wang",
                "Liuyu Xiang",
                "Zhaofeng He"
            ],
            "arxiv_id": "2511.13011v1",
            "summary": "Under extremely low-light conditions, novel view synthesis (NVS) faces severe degradation in terms of geometry, color consistency, and radiometric stability. Standard 3D Gaussian Splatting (3DGS) pipelines fail when applied directly to underexposed inputs, as independent enhancement across views causes illumination inconsistencies and geometric distortion. To address this, we present DTGS, a unified framework that tightly couples Retinex-inspired illumination decomposition with thermal-guided 3D Gaussian Splatting for illumination-invariant reconstruction. Unlike prior approaches that treat enhancement as a pre-processing step, DTGS performs joint optimization across enhancement, geometry, and thermal supervision through a cyclic enhancement-reconstruction mechanism. A thermal supervisory branch stabilizes both color restoration and geometry learning by dynamically balancing enhancement, structural, and thermal losses. Moreover, a Retinex-based decomposition module embedded within the 3DGS loop provides physically interpretable reflectance-illumination separation, ensuring consistent color and texture across viewpoints. To evaluate our method, we construct RGBT-LOW, a new multi-view low-light thermal dataset capturing severe illumination degradation. Extensive experiments show that DTGS significantly outperforms existing low-light enhancement and 3D reconstruction baselines, achieving superior radiometric consistency, geometric fidelity, and color stability under extreme illumination.",
            "headline_zh": "提出DTGS框架，结合热监督与Retinex分解，解决极低光下新视角合成的退化问题。",
            "intro_zh": [
                "极低光条件下，新视角合成面临几何、颜色和辐射稳定性严重退化。",
                "DTGS通过循环增强-重建机制，联合优化增强、几何和热监督。",
                "在自建RGBT-LOW数据集上，DTGS在辐射一致性、几何保真和颜色稳定性上显著优于基线。"
            ],
            "tags_zh": [
                "新视角合成",
                "3D高斯泼溅",
                "低光增强",
                "热监督",
                "Retinex分解",
                "多模态重建"
            ],
            "_index": 126
        },
        {
            "title": "TR-Gaussians: High-fidelity Real-time Rendering of Planar Transmission and Reflection with 3D Gaussian Splatting",
            "authors": [
                "Yong Liu",
                "Keyang Ye",
                "Tianjia Shao",
                "Kun Zhou"
            ],
            "arxiv_id": "2511.13009v1",
            "summary": "We propose Transmission-Reflection Gaussians (TR-Gaussians), a novel 3D-Gaussian-based representation for high-fidelity rendering of planar transmission and reflection, which are ubiquitous in indoor scenes. Our method combines 3D Gaussians with learnable reflection planes that explicitly model the glass planes with view-dependent reflectance strengths. Real scenes and transmission components are modeled by 3D Gaussians and the reflection components are modeled by the mirrored Gaussians with respect to the reflection plane. The transmission and reflection components are blended according to a Fresnel-based, view-dependent weighting scheme, allowing for faithful synthesis of complex appearance effects under varying viewpoints. To effectively optimize TR-Gaussians, we develop a multi-stage optimization framework incorporating color and geometry constraints and an opacity perturbation mechanism. Experiments on different datasets demonstrate that TR-Gaussians achieve real-time, high-fidelity novel view synthesis in scenes with planar transmission and reflection, and outperform state-of-the-art approaches both quantitatively and qualitatively.",
            "headline_zh": "提出TR-Gaussians以高保真实时渲染室内场景中的平面透射和反射",
            "intro_zh": [
                "核心问题：室内场景中平面透射和反射的高保真渲染难以实时实现",
                "方法要点：结合3D高斯与可学习反射平面，通过Fresnel权重混合透射和反射组件",
                "实验或效果：在多个数据集上实现实时高保真新视角合成，优于现有方法"
            ],
            "tags_zh": [
                "3D高斯渲染",
                "平面反射",
                "实时渲染",
                "新视角合成",
                "室内场景"
            ],
            "_index": 127
        },
        {
            "title": "SAGE: Spuriousness-Aware Guided Prompt Exploration for Mitigating Multimodal Bias",
            "authors": [
                "Wenqian Ye",
                "Di Wang",
                "Guangtao Zheng",
                "Bohan Liu",
                "Aidong Zhang"
            ],
            "arxiv_id": "2511.13005v1",
            "summary": "Large vision-language models, such as CLIP, have shown strong zero-shot classification performance by aligning images and text in a shared embedding space. However, CLIP models often develop multimodal spurious biases, which is the undesirable tendency to rely on spurious features. For example, CLIP may infer object types in images based on frequently co-occurring backgrounds rather than the object's core features. This bias significantly impairs the robustness of pre-trained CLIP models on out-of-distribution data, where such cross-modal associations no longer hold. Existing methods for mitigating multimodal spurious bias typically require fine-tuning on downstream data or prior knowledge of the bias, which undermines the out-of-the-box usability of CLIP. In this paper, we first theoretically analyze the impact of multimodal spurious bias in zero-shot classification. Based on this insight, we propose Spuriousness-Aware Guided Exploration (SAGE), a simple and effective method that mitigates spurious bias through guided prompt selection. SAGE requires no training, fine-tuning, or external annotations. It explores a space of prompt templates and selects the prompts that induce the largest semantic separation between classes, thereby improving worst-group robustness. Extensive experiments on four real-world benchmark datasets and five popular backbone models demonstrate that SAGE consistently improves zero-shot performance and generalization, outperforming previous zero-shot approaches without any external knowledge or model updates.",
            "headline_zh": "提出SAGE方法以缓解多模态虚假偏见，提升零样本分类鲁棒性",
            "intro_zh": [
                "CLIP模型存在多模态虚假偏见，依赖虚假特征如背景而非核心对象特征",
                "SAGE通过引导提示选择，无需训练或外部知识，增强类间语义分离",
                "在多个基准数据集和骨干模型上实验，SAGE提升零样本性能和泛化能力"
            ],
            "tags_zh": [
                "多模态偏见缓解",
                "零样本分类",
                "提示工程",
                "CLIP模型",
                "鲁棒性提升"
            ],
            "_index": 128
        },
        {
            "title": "Infinite-Story: A Training-Free Consistent Text-to-Image Generation",
            "authors": [
                "Jihun Park",
                "Kyoungmin Lee",
                "Jongmin Gim",
                "Hyeonseo Jo",
                "Minseok Oh",
                "Wonhyeok Choi",
                "Kyumin Hwang",
                "Jaeyeul Kim",
                "Minwoo Choi",
                "Sunghoon Im"
            ],
            "arxiv_id": "2511.13002v1",
            "summary": "We present Infinite-Story, a training-free framework for consistent text-to-image (T2I) generation tailored for multi-prompt storytelling scenarios. Built upon a scale-wise autoregressive model, our method addresses two key challenges in consistent T2I generation: identity inconsistency and style inconsistency. To overcome these issues, we introduce three complementary techniques: Identity Prompt Replacement, which mitigates context bias in text encoders to align identity attributes across prompts; and a unified attention guidance mechanism comprising Adaptive Style Injection and Synchronized Guidance Adaptation, which jointly enforce global style and identity appearance consistency while preserving prompt fidelity. Unlike prior diffusion-based approaches that require fine-tuning or suffer from slow inference, Infinite-Story operates entirely at test time, delivering high identity and style consistency across diverse prompts. Extensive experiments demonstrate that our method achieves state-of-the-art generation performance, while offering over 6X faster inference (1.72 seconds per image) than the existing fastest consistent T2I models, highlighting its effectiveness and practicality for real-world visual storytelling.",
            "headline_zh": "提出Infinite-Story训练免费框架，解决多提示故事生成中的身份与风格不一致问题",
            "intro_zh": [
                "核心问题：多提示文本到图像生成中身份和风格不一致，现有方法需微调或推理慢",
                "方法要点：使用身份提示替换和统一注意力指导，无需训练，确保一致性与提示保真度",
                "实验或效果：在实验中实现最先进性能，推理速度比现有最快模型快6倍以上"
            ],
            "tags_zh": [
                "文本到图像生成",
                "一致性生成",
                "训练免费方法",
                "注意力机制",
                "多提示故事",
                "快速推理"
            ],
            "_index": 129
        },
        {
            "title": "Medal S: Spatio-Textual Prompt Model for Medical Segmentation",
            "authors": [
                "Pengcheng Shi",
                "Jiawei Chen",
                "Jiaqi Liu",
                "Xinglin Zhang",
                "Tao Chen",
                "Lei Li"
            ],
            "arxiv_id": "2511.13001v1",
            "summary": "We introduce Medal S, a medical segmentation foundation model that supports native-resolution spatial and textual prompts within an end-to-end trainable framework. Unlike text-only methods lacking spatial awareness, Medal S achieves channel-wise alignment between volumetric prompts and text embeddings, mitigating inaccuracies from resolution mismatches. By preserving full 3D context, it efficiently processes multiple native-resolution masks in parallel, enhancing multi-class segmentation performance. A lightweight 3D convolutional module enables precise voxel-space refinement guided by both prompt types, supporting up to 243 classes across CT, MRI, PET, ultrasound, and microscopy modalities in the BiomedSegFM dataset. Medal S offers two prompting modes: a text-only mode, where model predictions serve as spatial prompts for self-refinement without human input, and a hybrid mode, incorporating manual annotations for enhanced flexibility. For 24-class segmentation, parallel spatial prompting reduces inference time by more than 90% compared to sequential prompting. We propose dynamic resampling to address target-patch ratio imbalance, extending SAT and nnU-Net for data augmentation. Furthermore, we develop optimized text preprocessing, a two-stage inference strategy, and post-processing techniques to improve memory efficiency, precision, and inference speed. On the five-modality average on the validation set, Medal S outperforms SAT with a DSC of 75.44 (vs. 69.83), NSD of 77.34 (vs. 71.06), F1 of 38.24 (vs. 24.88), and DSC TP of 65.46 (vs. 46.97). Medal S achieves excellent performance by harmonizing spatial precision with semantic textual guidance, demonstrating superior efficiency and accuracy in multi-class medical segmentation tasks compared to sequential prompt-based approaches. Medal S will be publicly available at https://github.com/yinghemedical/Medal-S.",
            "headline_zh": "提出Medal S医学分割基础模型，支持空间和文本提示以提升多类分割效率与精度",
            "intro_zh": [
                "核心问题：现有文本提示方法缺乏空间感知，导致分辨率不匹配和分割不准确",
                "方法要点：通过通道对齐和轻量3D卷积模块，实现端到端空间与文本提示并行处理",
                "实验或效果：在五模态验证集上，DSC达75.44，推理时间减少超90%，优于SAT和nnU-Net"
            ],
            "tags_zh": [
                "医学图像分割",
                "空间文本提示",
                "多模态处理",
                "并行推理",
                "3D卷积优化"
            ],
            "_index": 130
        },
        {
            "title": "Scalable Vision-Guided Crop Yield Estimation",
            "authors": [
                "Harrison H. Li",
                "Medhanie Irgau",
                "Nabil Janmohamed",
                "Karen Solveig Rieckmann",
                "David B. Lobell"
            ],
            "arxiv_id": "2511.12999v1",
            "summary": "Precise estimation and uncertainty quantification for average crop yields are critical for agricultural monitoring and decision making. Existing data collection methods, such as crop cuts in randomly sampled fields at harvest time, are relatively time-consuming. Thus, we propose an approach based on prediction-powered inference (PPI) to supplement these crop cuts with less time-consuming field photos. After training a computer vision model to predict the ground truth crop cut yields from the photos, we learn a ``control function\" that recalibrates these predictions with the spatial coordinates of each field. This enables fields with photos but not crop cuts to be leveraged to improve the precision of zone-wide average yield estimates. Our control function is learned by training on a dataset of nearly 20,000 real crop cuts and photos of rice and maize fields in sub-Saharan Africa. To improve precision, we pool training observations across different zones within the same first-level subdivision of each country. Our final PPI-based point estimates of the average yield are provably asymptotically unbiased and cannot increase the asymptotic variance beyond that of the natural baseline estimator -- the sample average of the crop cuts -- as the number of fields grows. We also propose a novel bias-corrected and accelerated (BCa) bootstrap to construct accompanying confidence intervals. Even in zones with as few as 20 fields, the point estimates show significant empirical improvement over the baseline, increasing the effective sample size by as much as 73% for rice and by 12-23% for maize. The confidence intervals are accordingly shorter at minimal cost to empirical finite-sample coverage. This demonstrates the potential for relatively low-cost images to make area-based crop insurance more affordable and thus spur investment into sustainable agricultural practices.",
            "headline_zh": "提出基于预测驱动推理的视觉引导方法，以低成本图像提升作物产量估计精度。",
            "intro_zh": [
                "核心问题：传统作物产量估计方法耗时，需高效补充数据以改进精度。",
                "方法要点：训练计算机视觉模型预测产量，结合空间坐标学习控制函数进行校准。",
                "实验或效果：在非洲水稻和玉米田验证，有效样本量最高提升73%，置信区间更短。"
            ],
            "tags_zh": [
                "作物产量估计",
                "预测驱动推理",
                "计算机视觉",
                "空间校准",
                "农业监测",
                "置信区间构建"
            ],
            "_index": 131
        },
        {
            "title": "PerTouch: VLM-Driven Agent for Personalized and Semantic Image Retouching",
            "authors": [
                "Zewei Chang",
                "Zheng-Peng Duan",
                "Jianxing Zhang",
                "Chun-Le Guo",
                "Siyu Liu",
                "Hyungju Chun",
                "Hyunhee Park",
                "Zikun Liu",
                "Chongyi Li"
            ],
            "arxiv_id": "2511.12998v1",
            "summary": "Image retouching aims to enhance visual quality while aligning with users' personalized aesthetic preferences. To address the challenge of balancing controllability and subjectivity, we propose a unified diffusion-based image retouching framework called PerTouch. Our method supports semantic-level image retouching while maintaining global aesthetics. Using parameter maps containing attribute values in specific semantic regions as input, PerTouch constructs an explicit parameter-to-image mapping for fine-grained image retouching. To improve semantic boundary perception, we introduce semantic replacement and parameter perturbation mechanisms in the training process. To connect natural language instructions with visual control, we develop a VLM-driven agent that can handle both strong and weak user instructions. Equipped with mechanisms of feedback-driven rethinking and scene-aware memory, PerTouch better aligns with user intent and captures long-term preferences. Extensive experiments demonstrate each component's effectiveness and the superior performance of PerTouch in personalized image retouching. Code is available at: https://github.com/Auroral703/PerTouch.",
            "headline_zh": "提出PerTouch框架，基于扩散模型实现个性化语义图像润色",
            "intro_zh": [
                "核心问题：图像润色需平衡可控性与用户主观审美偏好。",
                "方法要点：使用参数映射和VLM代理，支持语义级润色与用户意图对齐。",
                "实验效果：组件验证有效，在个性化润色中表现优越。"
            ],
            "tags_zh": [
                "图像润色",
                "扩散模型",
                "语义控制",
                "VLM代理",
                "个性化偏好"
            ],
            "_index": 132
        },
        {
            "title": "Semantic Prioritization in Visual Counterfactual Explanations with Weighted Segmentation and Auto-Adaptive Region Selection",
            "authors": [
                "Lintong Zhang",
                "Kang Yin",
                "Seong-Whan Lee"
            ],
            "arxiv_id": "2511.12992v1",
            "summary": "In the domain of non-generative visual counterfactual explanations (CE), traditional techniques frequently involve the substitution of sections within a query image with corresponding sections from distractor images. Such methods have historically overlooked the semantic relevance of the replacement regions to the target object, thereby impairing the model's interpretability and hindering the editing workflow. Addressing these challenges, the present study introduces an innovative methodology named as Weighted Semantic Map with Auto-adaptive Candidate Editing Network (WSAE-Net). Characterized by two significant advancements: the determination of an weighted semantic map and the auto-adaptive candidate editing sequence. First, the generation of the weighted semantic map is designed to maximize the reduction of non-semantic feature units that need to be computed, thereby optimizing computational efficiency. Second, the auto-adaptive candidate editing sequences are designed to determine the optimal computational order among the feature units to be processed, thereby ensuring the efficient generation of counterfactuals while maintaining the semantic relevance of the replacement feature units to the target object. Through comprehensive experimentation, our methodology demonstrates superior performance, contributing to a more lucid and in-depth understanding of visual counterfactual explanations.",
            "headline_zh": "提出WSAE-Net以优化非生成视觉反事实解释的语义相关性和计算效率",
            "intro_zh": [
                "传统方法忽略替换区域语义相关性，损害模型可解释性和编辑流程",
                "引入加权语义图和自适应候选编辑序列，优化计算顺序和效率",
                "实验显示方法性能优越，提升视觉反事实解释的清晰度和深度理解"
            ],
            "tags_zh": [
                "视觉反事实解释",
                "加权语义图",
                "自适应编辑序列",
                "计算效率优化",
                "语义相关性"
            ],
            "_index": 133
        },
        {
            "title": "UNSEEN: Enhancing Dataset Pruning from a Generalization Perspective",
            "authors": [
                "Furui Xu",
                "Shaobo Wang",
                "Jiajun Zhang",
                "Chenghao Sun",
                "Haixiang Tang",
                "Linfeng Zhang"
            ],
            "arxiv_id": "2511.12988v1",
            "summary": "The growing scale of datasets in deep learning has introduced significant computational challenges. Dataset pruning addresses this challenge by constructing a compact but informative coreset from the full dataset with comparable performance. Previous approaches typically establish scoring metrics based on specific criteria to identify representative samples. However, these methods predominantly rely on sample scores obtained from the model's performance during the training (i.e., fitting) phase. As scoring models achieve near-optimal performance on training data, such fitting-centric approaches induce a dense distribution of sample scores within a narrow numerical range. This concentration reduces the distinction between samples and hinders effective selection. To address this challenge, we conduct dataset pruning from the perspective of generalization, i.e., scoring samples based on models not exposed to them during training. We propose a plug-and-play framework, UNSEEN, which can be integrated into existing dataset pruning methods. Additionally, conventional score-based methods are single-step and rely on models trained solely on the complete dataset, providing limited perspective on the importance of samples. To address this limitation, we scale UNSEEN to multi-step scenarios and propose an incremental selection technique through scoring models trained on varying coresets, and optimize the quality of the coreset dynamically. Extensive experiments demonstrate that our method significantly outperforms existing state-of-the-art (SOTA) methods on CIFAR-10, CIFAR-100, and ImageNet-1K. Notably, on ImageNet-1K, UNSEEN achieves lossless performance while reducing training data by 30\\%.",
            "headline_zh": "提出UNSEEN框架从泛化视角增强数据集剪枝，提升核心集性能",
            "intro_zh": [
                "数据集剪枝中样本评分密集化，降低选择区分度，影响核心集构建",
                "基于未训练模型评分样本，并扩展至多步增量选择优化核心集质量",
                "在CIFAR和ImageNet上显著优于SOTA方法，ImageNet-1K剪枝30%无损性能"
            ],
            "tags_zh": [
                "数据集剪枝",
                "泛化评分",
                "核心集优化",
                "多步选择",
                "模型训练效率"
            ],
            "_index": 134
        },
        {
            "title": "Angular Gradient Sign Method: Uncovering Vulnerabilities in Hyperbolic Networks",
            "authors": [
                "Minsoo Jo",
                "Dongyoon Yang",
                "Taesup Kim"
            ],
            "arxiv_id": "2511.12985v1",
            "summary": "Adversarial examples in neural networks have been extensively studied in Euclidean geometry, but recent advances in \\textit{hyperbolic networks} call for a reevaluation of attack strategies in non-Euclidean geometries. Existing methods such as FGSM and PGD apply perturbations without regard to the underlying hyperbolic structure, potentially leading to inefficient or geometrically inconsistent attacks. In this work, we propose a novel adversarial attack that explicitly leverages the geometric properties of hyperbolic space. Specifically, we compute the gradient of the loss function in the tangent space of hyperbolic space, decompose it into a radial (depth) component and an angular (semantic) component, and apply perturbation derived solely from the angular direction. Our method generates adversarial examples by focusing perturbations in semantically sensitive directions encoded in angular movement within the hyperbolic geometry. Empirical results on image classification, cross-modal retrieval tasks and network architectures demonstrate that our attack achieves higher fooling rates than conventional adversarial attacks, while producing high-impact perturbations with deeper insights into vulnerabilities of hyperbolic embeddings. This work highlights the importance of geometry-aware adversarial strategies in curved representation spaces and provides a principled framework for attacking hierarchical embeddings.",
            "headline_zh": "提出角梯度符号方法以攻击双曲网络中的漏洞",
            "intro_zh": [
                "现有对抗攻击方法未考虑双曲几何结构，导致攻击效率低或不一致",
                "在双曲空间切空间中分解梯度，仅沿角方向施加扰动以生成对抗样本",
                "实验显示在图像分类和跨模态检索任务中，攻击成功率高于传统方法"
            ],
            "tags_zh": [
                "对抗攻击",
                "双曲网络",
                "几何感知",
                "梯度分解",
                "图像分类",
                "跨模态检索"
            ],
            "_index": 135
        },
        {
            "title": "CUTE-Planner: Confidence-aware Uneven Terrain Exploration Planner",
            "authors": [
                "Miryeong Park",
                "Dongjin Cho",
                "Sanghyun Kim",
                "Younggun Cho"
            ],
            "arxiv_id": "2511.12984v1",
            "summary": "Planetary exploration robots must navigate uneven terrain while building reliable maps for space missions. However, most existing methods incorporate traversability constraints but may not handle high uncertainty in elevation estimates near complex features like craters, do not consider exploration strategies for uncertainty reduction, and typically fail to address how elevation uncertainty affects navigation safety and map quality. To address the problems, we propose a framework integrating safe path generation, adaptive confidence updates, and confidence-aware exploration strategies. Using Kalman-based elevation estimation, our approach generates terrain traversability and confidence scores, then incorporates them into Graph-Based exploration Planner (GBP) to prioritize exploration of traversable low-confidence regions. We evaluate our framework through simulated lunar experiments using a novel low-confidence region ratio metric, achieving 69% uncertainty reduction compared to baseline GBP. In terms of mission success rate, our method achieves 100% while baseline GBP achieves 0%, demonstrating improvements in exploration safety and map reliability.",
            "headline_zh": "提出CUTE-Planner框架以提升行星机器人在崎岖地形中的探索安全性和地图可靠性",
            "intro_zh": [
                "核心问题：现有方法难以处理高程估计不确定性，影响导航安全和地图质量",
                "方法要点：集成安全路径生成、自适应置信度更新和置信感知探索策略",
                "实验或效果：在模拟月球实验中，不确定性降低69%，任务成功率从0%提升至100%"
            ],
            "tags_zh": [
                "行星探索机器人",
                "崎岖地形导航",
                "置信度感知规划",
                "卡尔曼滤波估计",
                "图基探索规划",
                "不确定性减少"
            ],
            "_index": 136
        },
        {
            "title": "SafeGRPO: Self-Rewarded Multimodal Safety Alignment via Rule-Governed Policy Optimization",
            "authors": [
                "Xuankun Rong",
                "Wenke Huang",
                "Tingfeng Wang",
                "Daiguo Zhou",
                "Bo Du",
                "Mang Ye"
            ],
            "arxiv_id": "2511.12982v1",
            "summary": "Multimodal large language models (MLLMs) have demonstrated impressive reasoning and instruction-following capabilities, yet their expanded modality space introduces new compositional safety risks that emerge from complex text-image interactions. Such cross-modal couplings can produce unsafe semantics even when individual inputs are benign, exposing the fragile safety awareness of current MLLMs. While recent works enhance safety by guiding models to reason about potential risks, unregulated reasoning traces may compromise alignment; although Group Relative Policy Optimization (GRPO) offers self-rewarded refinement without human supervision, it lacks verifiable signals for reasoning safety. To address this, we propose SafeGRPO a self-rewarded multimodal safety alignment framework that integrates rule-governed reward construction into GRPO, enabling interpretable and verifiable optimization of reasoning safety. Built upon the constructed SafeTag-VL-3K dataset with explicit visual, textual, and combined safety tags, SafeGRPO performs step-guided safety thinking to enforce structured reasoning and behavior alignment, substantially improving multimodal safety awareness, compositional robustness, and reasoning stability across diverse benchmarks without sacrificing general capabilities.",
            "headline_zh": "提出SafeGRPO框架，通过规则治理策略优化解决多模态大模型组合安全风险",
            "intro_zh": [
                "多模态大模型在文本-图像交互中易产生组合安全风险，即使输入无害也可能输出不安全语义",
                "方法将规则治理奖励构建融入GRPO，实现可解释的安全推理优化，基于SafeTag-VL-3K数据集进行结构化推理",
                "实验显示，在多种基准测试中显著提升多模态安全意识和组合鲁棒性，未牺牲通用能力"
            ],
            "tags_zh": [
                "多模态安全对齐",
                "规则治理策略优化",
                "自奖励学习",
                "组合安全风险",
                "结构化推理"
            ],
            "_index": 137
        },
        {
            "title": "Concept Regions Matter: Benchmarking CLIP with a New Cluster-Importance Approach",
            "authors": [
                "Aishwarya Agarwal",
                "Srikrishna Karanam",
                "Vineet Gandhi"
            ],
            "arxiv_id": "2511.12978v1",
            "summary": "Contrastive vision-language models (VLMs) such as CLIP achieve strong zero-shot recognition yet remain vulnerable to spurious correlations, particularly background over-reliance. We introduce Cluster-based Concept Importance (CCI), a novel interpretability method that uses CLIP's own patch embeddings to group spatial patches into semantically coherent clusters, mask them, and evaluate relative changes in model predictions. CCI sets a new state of the art on faithfulness benchmarks, surpassing prior methods by large margins; for example, it yields more than a twofold improvement on the deletion-AUC metric for MS COCO retrieval. We further propose that CCI, when combined with GroundedSAM, automatically categorizes predictions as foreground- or background-driven, providing a crucial diagnostic ability. Existing benchmarks such as CounterAnimals, however, rely solely on accuracy and implicitly attribute all performance degradation to background correlations. Our analysis shows this assumption to be incomplete, since many errors arise from viewpoint variation, scale shifts, and fine-grained object confusions. To disentangle these effects, we introduce COVAR, a benchmark that systematically varies object foregrounds and backgrounds. Leveraging CCI with COVAR, we present a comprehensive evaluation of eighteen CLIP variants, offering methodological advances and empirical evidence that chart a path toward more robust VLMs.",
            "headline_zh": "提出CCI方法以评估CLIP模型对虚假相关性的鲁棒性",
            "intro_zh": [
                "CLIP等视觉语言模型易受背景虚假相关性影响，导致预测偏差",
                "CCI利用补丁嵌入聚类和掩码，评估预测变化，提升解释性",
                "结合COVAR基准，系统评估18个CLIP变体，推动模型鲁棒性"
            ],
            "tags_zh": [
                "视觉语言模型",
                "解释性方法",
                "虚假相关性",
                "基准测试",
                "聚类分析",
                "零样本识别"
            ],
            "_index": 138
        },
        {
            "title": "ArtiWorld: LLM-Driven Articulation of 3D Objects in Scenes",
            "authors": [
                "Yixuan Yang",
                "Luyang Xie",
                "Zhen Luo",
                "Zixiang Zhao",
                "Mingqi Gao",
                "Feng Zheng"
            ],
            "arxiv_id": "2511.12977v1",
            "summary": "Building interactive simulators and scalable robot-learning environments requires a large number of articulated assets. However, most existing 3D assets in simulation are rigid, and manually converting them into articulated objects is extremely labor- and cost-intensive. This raises a natural question: can we automatically identify articulable objects in a scene and convert them into articulated assets directly? In this paper, we present ArtiWorld, a scene-aware pipeline that localizes candidate articulable objects from textual scene descriptions and reconstructs executable URDF models that preserve the original geometry. At the core of this pipeline is Arti4URDF, which leverages 3D point cloud, prior knowledge of a large language model (LLM), and a URDF-oriented prompt design to rapidly convert rigid objects into interactive URDF-based articulated objects while maintaining their 3D shape. We evaluate ArtiWorld at three levels: 3D simulated objects, full 3D simulated scenes, and real-world scan scenes. Across all three settings, our method consistently outperforms existing approaches and achieves state-of-the-art performance, while preserving object geometry and correctly capturing object interactivity to produce usable URDF-based articulated models. This provides a practical path toward building interactive, robot-ready simulation environments directly from existing 3D assets. Code and data will be released.",
            "headline_zh": "提出ArtiWorld以自动将场景中的刚性3D对象转换为可交互的铰接模型",
            "intro_zh": [
                "核心问题：现有3D资产多为刚性，手动转换为铰接对象成本高且耗时",
                "方法要点：利用LLM先验知识和3D点云，通过Arti4URDF生成URDF模型",
                "实验或效果：在模拟和真实场景中优于现有方法，保持几何形状和交互性"
            ],
            "tags_zh": [
                "3D铰接对象",
                "大语言模型",
                "URDF生成",
                "点云处理",
                "机器人仿真"
            ],
            "_index": 139
        },
        {
            "title": "MCAQ-YOLO: Morphological Complexity-Aware Quantization for Efficient Object Detection with Curriculum Learning",
            "authors": [
                "Yoonjae Seo",
                "Ermal Elbasani",
                "Jaehong Lee"
            ],
            "arxiv_id": "2511.12976v1",
            "summary": "Most neural network quantization methods apply uniform bit precision across spatial regions, ignoring the heterogeneous structural and textural complexity of visual data. This paper introduces MCAQ-YOLO, a morphological complexity-aware quantization framework for object detection. The framework employs five morphological metrics - fractal dimension, texture entropy, gradient variance, edge density, and contour complexity - to characterize local visual morphology and guide spatially adaptive bit allocation. By correlating these metrics with quantization sensitivity, MCAQ-YOLO dynamically adjusts bit precision according to spatial complexity. In addition, a curriculum-based quantization-aware training scheme progressively increases quantization difficulty to stabilize optimization and accelerate convergence. Experimental results demonstrate a strong correlation between morphological complexity and quantization sensitivity and show that MCAQ-YOLO achieves superior detection accuracy and convergence efficiency compared with uniform quantization. On a safety equipment dataset, MCAQ-YOLO attains 85.6 percent mAP@0.5 with an average of 4.2 bits and a 7.6x compression ratio, yielding 3.5 percentage points higher mAP than uniform 4-bit quantization while introducing only 1.8 ms of additional runtime overhead per image. Cross-dataset validation on COCO and Pascal VOC further confirms consistent performance gains, indicating that morphology-driven spatial quantization can enhance efficiency and robustness for computationally constrained, safety-critical visual recognition tasks.",
            "headline_zh": "提出形态复杂度感知量化方法以提升受限计算场景下的目标检测效率",
            "intro_zh": [
                "核心问题：均匀量化忽略视觉数据空间异质性，导致检测精度下降。",
                "方法要点：基于形态学指标动态分配比特，结合课程学习稳定训练。",
                "实验效果：在安全装备数据集上，mAP@0.5达85.6%，优于均匀量化。"
            ],
            "tags_zh": [
                "目标检测",
                "神经网络量化",
                "形态复杂度感知",
                "课程学习",
                "空间自适应量化",
                "高效视觉识别"
            ],
            "_index": 140
        },
        {
            "title": "SplatSearch: Instance Image Goal Navigation for Mobile Robots using 3D Gaussian Splatting and Diffusion Models",
            "authors": [
                "Siddarth Narasimhan",
                "Matthew Lisondra",
                "Haitong Wang",
                "Goldie Nejat"
            ],
            "arxiv_id": "2511.12972v1",
            "summary": "The Instance Image Goal Navigation (IIN) problem requires mobile robots deployed in unknown environments to search for specific objects or people of interest using only a single reference goal image of the target. This problem can be especially challenging when: 1) the reference image is captured from an arbitrary viewpoint, and 2) the robot must operate with sparse-view scene reconstructions. In this paper, we address the IIN problem, by introducing SplatSearch, a novel architecture that leverages sparse-view 3D Gaussian Splatting (3DGS) reconstructions. SplatSearch renders multiple viewpoints around candidate objects using a sparse online 3DGS map, and uses a multi-view diffusion model to complete missing regions of the rendered images, enabling robust feature matching against the goal image. A novel frontier exploration policy is introduced which uses visual context from the synthesized viewpoints with semantic context from the goal image to evaluate frontier locations, allowing the robot to prioritize frontiers that are semantically and visually relevant to the goal image. Extensive experiments in photorealistic home and real-world environments validate the higher performance of SplatSearch against current state-of-the-art methods in terms of Success Rate and Success Path Length. An ablation study confirms the design choices of SplatSearch.",
            "headline_zh": "提出SplatSearch架构，利用3D高斯泼溅和扩散模型解决实例图像目标导航问题。",
            "intro_zh": [
                "核心问题：移动机器人在未知环境中使用单张参考图像搜索特定目标，面临任意视角和稀疏视图重建挑战。",
                "方法要点：结合稀疏视图3D高斯泼溅重建、多视图扩散模型补全图像和语义视觉前沿探索策略。",
                "实验或效果：在真实和仿真家庭环境中验证，成功率和路径长度优于现有方法，消融研究支持设计。"
            ],
            "tags_zh": [
                "实例图像目标导航",
                "3D高斯泼溅",
                "扩散模型",
                "前沿探索策略",
                "稀疏视图重建",
                "机器人导航"
            ],
            "_index": 141
        },
        {
            "title": "HiFusion: Hierarchical Intra-Spot Alignment and Regional Context Fusion for Spatial Gene Expression Prediction from Histopathology",
            "authors": [
                "Ziqiao Weng",
                "Yaoyu Fang",
                "Jiahe Qian",
                "Xinkun Wang",
                "Lee AD Cooper",
                "Weidong Cai",
                "Bo Zhou"
            ],
            "arxiv_id": "2511.12969v1",
            "summary": "Spatial transcriptomics (ST) bridges gene expression and tissue morphology but faces clinical adoption barriers due to technical complexity and prohibitive costs. While computational methods predict gene expression from H&E-stained whole-slide images (WSIs), existing approaches often fail to capture the intricate biological heterogeneity within spots and are susceptible to morphological noise when integrating contextual information from surrounding tissue. To overcome these limitations, we propose HiFusion, a novel deep learning framework that integrates two complementary components. First, we introduce the Hierarchical Intra-Spot Modeling module that extracts fine-grained morphological representations through multi-resolution sub-patch decomposition, guided by a feature alignment loss to ensure semantic consistency across scales. Concurrently, we present the Context-aware Cross-scale Fusion module, which employs cross-attention to selectively incorporate biologically relevant regional context, thereby enhancing representational capacity. This architecture enables comprehensive modeling of both cellular-level features and tissue microenvironmental cues, which are essential for accurate gene expression prediction. Extensive experiments on two benchmark ST datasets demonstrate that HiFusion achieves state-of-the-art performance across both 2D slide-wise cross-validation and more challenging 3D sample-specific scenarios. These results underscore HiFusion's potential as a robust, accurate, and scalable solution for ST inference from routine histopathology.",
            "headline_zh": "提出HiFusion框架，通过层次化建模和上下文融合从组织病理图像预测空间基因表达",
            "intro_zh": [
                "核心问题：现有方法难以捕捉斑点内生物异质性，且易受形态噪声影响",
                "方法要点：结合多分辨率子块分解和跨尺度注意力，增强特征一致性与上下文整合",
                "实验或效果：在多个数据集上实现最优性能，验证了2D和3D场景的鲁棒性"
            ],
            "tags_zh": [
                "空间转录组学",
                "基因表达预测",
                "深度学习框架",
                "组织病理图像",
                "多分辨率建模",
                "上下文融合"
            ],
            "_index": 142
        },
        {
            "title": "GrOCE:Graph-Guided Online Concept Erasure for Text-to-Image Diffusion Models",
            "authors": [
                "Ning Han",
                "Zhenyu Ge",
                "Feng Han",
                "Yuhua Sun",
                "Chengqing Li",
                "Jingjing Chen"
            ],
            "arxiv_id": "2511.12968v1",
            "summary": "Concept erasure aims to remove harmful, inappropriate, or copyrighted content from text-to-image diffusion models while preserving non-target semantics. However, existing methods either rely on costly fine-tuning or apply coarse semantic separation, often degrading unrelated concepts and lacking adaptability to evolving concept sets. To alleviate this issue, we propose Graph-Guided Online Concept Erasure (GrOCE), a training-free framework that performs precise and adaptive concept removal through graph-based semantic reasoning. GrOCE models concepts and their interrelations as a dynamic semantic graph, enabling principled reasoning over dependencies and fine-grained isolation of undesired content. It comprises three components: (1) Dynamic Topological Graph Construction for incremental graph building, (2) Adaptive Cluster Identification for multi-hop traversal with similarity-decay scoring, and (3) Selective Edge Severing for targeted edge removal while preserving global semantics. Extensive experiments demonstrate that GrOCE achieves state-of-the-art performance on Concept Similarity (CS) and Fréchet Inception Distance (FID) metrics, offering efficient, accurate, and stable concept erasure without retraining.",
            "headline_zh": "提出GrOCE框架以解决文本到图像扩散模型中的概念擦除问题",
            "intro_zh": [
                "现有概念擦除方法依赖微调或粗语义分离，易损害无关概念且适应性差",
                "GrOCE基于动态语义图进行训练无关的精确推理，实现细粒度概念隔离",
                "实验在CS和FID指标上达到SOTA，无需重训练即可高效稳定擦除概念"
            ],
            "tags_zh": [
                "概念擦除",
                "文本到图像扩散模型",
                "图推理",
                "训练无关方法",
                "语义图构建"
            ],
            "_index": 143
        },
        {
            "title": "CalibrateMix: Guided-Mixup Calibration of Image Semi-Supervised Models",
            "authors": [
                "Mehrab Mustafy Rahman",
                "Jayanth Mohan",
                "Tiberiu Sosea",
                "Cornelia Caragea"
            ],
            "arxiv_id": "2511.12964v1",
            "summary": "Semi-supervised learning (SSL) has demonstrated high performance in image classification tasks by effectively utilizing both labeled and unlabeled data. However, existing SSL methods often suffer from poor calibration, with models yielding overconfident predictions that misrepresent actual prediction likelihoods. Recently, neural networks trained with {\\tt mixup} that linearly interpolates random examples from the training set have shown better calibration in supervised settings. However, calibration of neural models remains under-explored in semi-supervised settings. Although effective in supervised model calibration, random mixup of pseudolabels in SSL presents challenges due to the overconfidence and unreliability of pseudolabels. In this work, we introduce CalibrateMix, a targeted mixup-based approach that aims to improve the calibration of SSL models while maintaining or even improving their classification accuracy. Our method leverages training dynamics of labeled and unlabeled samples to identify ``easy-to-learn'' and ``hard-to-learn'' samples, which in turn are utilized in a targeted mixup of easy and hard samples. Experimental results across several benchmark image datasets show that our method achieves lower expected calibration error (ECE) and superior accuracy compared to existing SSL approaches.",
            "headline_zh": "提出CalibrateMix以改进半监督图像模型的校准问题",
            "intro_zh": [
                "半监督学习模型常存在校准不佳，预测过于自信的问题",
                "利用训练动态识别易学和难学样本，进行目标性mixup混合",
                "实验显示在多数据集上降低预期校准误差并提升准确率"
            ],
            "tags_zh": [
                "半监督学习",
                "模型校准",
                "图像分类",
                "mixup方法",
                "预期校准误差"
            ],
            "_index": 144
        },
        {
            "title": "EndoSight AI: Deep Learning-Driven Real-Time Gastrointestinal Polyp Detection and Segmentation for Enhanced Endoscopic Diagnostics",
            "authors": [
                "Daniel Cavadia"
            ],
            "arxiv_id": "2511.12962v1",
            "summary": "Precise and real-time detection of gastrointestinal polyps during endoscopic procedures is crucial for early diagnosis and prevention of colorectal cancer. This work presents EndoSight AI, a deep learning architecture developed and evaluated independently to enable accurate polyp localization and detailed boundary delineation. Leveraging the publicly available Hyper-Kvasir dataset, the system achieves a mean Average Precision (mAP) of 88.3% for polyp detection and a Dice coefficient of up to 69% for segmentation, alongside real-time inference speeds exceeding 35 frames per second on GPU hardware. The training incorporates clinically relevant performance metrics and a novel thermal-aware procedure to ensure model robustness and efficiency. This integrated AI solution is designed for seamless deployment in endoscopy workflows, promising to advance diagnostic accuracy and clinical decision-making in gastrointestinal healthcare.",
            "headline_zh": "提出EndoSight AI深度学习架构，实现实时胃肠道息肉检测与分割以增强内窥镜诊断。",
            "intro_zh": [
                "核心问题：内窥镜过程中实时精确检测胃肠道息肉对早期诊断结直肠癌至关重要。",
                "方法要点：基于Hyper-Kvasir数据集开发深度学习模型，结合热感知程序提升鲁棒性。",
                "实验或效果：检测mAP达88.3%，分割Dice系数最高69%，实时推理速度超35帧/秒。"
            ],
            "tags_zh": [
                "息肉检测",
                "息肉分割",
                "深度学习",
                "内窥镜诊断",
                "实时推理",
                "热感知训练"
            ],
            "_index": 145
        },
        {
            "title": "Inertia-Informed Orientation Priors for Event-Based Optical Flow Estimation",
            "authors": [
                "Pritam P. Karmokar",
                "William J. Beksi"
            ],
            "arxiv_id": "2511.12961v1",
            "summary": "Event cameras, by virtue of their working principle, directly encode motion within a scene. Many learning-based and model-based methods exist that estimate event-based optical flow, however the temporally dense yet spatially sparse nature of events poses significant challenges. To address these issues, contrast maximization (CM) is a prominent model-based optimization methodology that estimates the motion trajectories of events within an event volume by optimally warping them. Since its introduction, the CM framework has undergone a series of refinements by the computer vision community. Nonetheless, it remains a highly non-convex optimization problem. In this paper, we introduce a novel biologically-inspired hybrid CM method for event-based optical flow estimation that couples visual and inertial motion cues. Concretely, we propose the use of orientation maps, derived from camera 3D velocities, as priors to guide the CM process. The orientation maps provide directional guidance and constrain the space of estimated motion trajectories. We show that this orientation-guided formulation leads to improved robustness and convergence in event-based optical flow estimation. The evaluation of our approach on the MVSEC, DSEC, and ECD datasets yields superior accuracy scores over the state of the art.",
            "headline_zh": "提出惯性引导方向先验的混合对比最大化方法，以改进事件相机光流估计的鲁棒性和收敛性。",
            "intro_zh": [
                "事件相机光流估计面临时间密集但空间稀疏的挑战，导致高度非凸优化问题。",
                "方法结合视觉和惯性运动线索，使用相机3D速度导出的方向图作为先验指导对比最大化过程。",
                "在MVSEC、DSEC和ECD数据集上评估，显示优于现有技术的精度和鲁棒性。"
            ],
            "tags_zh": [
                "事件相机",
                "光流估计",
                "对比最大化",
                "惯性传感器",
                "方向先验",
                "生物启发方法"
            ],
            "_index": 146
        },
        {
            "title": "T2I-Based Physical-World Appearance Attack against Traffic Sign Recognition Systems in Autonomous Driving",
            "authors": [
                "Chen Ma",
                "Ningfei Wang",
                "Junhao Zheng",
                "Qing Guo",
                "Qian Wang",
                "Qi Alfred Chen",
                "Chao Shen"
            ],
            "arxiv_id": "2511.12956v1",
            "summary": "Traffic Sign Recognition (TSR) systems play a critical role in Autonomous Driving (AD) systems, enabling real-time detection of road signs, such as STOP and speed limit signs. While these systems are increasingly integrated into commercial vehicles, recent research has exposed their vulnerability to physical-world adversarial appearance attacks. In such attacks, carefully crafted visual patterns are misinterpreted by TSR models as legitimate traffic signs, while remaining inconspicuous or benign to human observers. However, existing adversarial appearance attacks suffer from notable limitations. Pixel-level perturbation-based methods often lack stealthiness and tend to overfit to specific surrogate models, resulting in poor transferability to real-world TSR systems. On the other hand, text-to-image (T2I) diffusion model-based approaches demonstrate limited effectiveness and poor generalization to out-of-distribution sign types.\n  In this paper, we present DiffSign, a novel T2I-based appearance attack framework designed to generate physically robust, highly effective, transferable, practical, and stealthy appearance attacks against TSR systems. To overcome the limitations of prior approaches, we propose a carefully designed attack pipeline that integrates CLIP-based loss and masked prompts to improve attack focus and controllability. We also propose two novel style customization methods to guide visual appearance and improve out-of-domain traffic sign attack generalization and attack stealthiness. We conduct extensive evaluations of DiffSign under varied real-world conditions, including different distances, angles, light conditions, and sign categories. Our method achieves an average physical-world attack success rate of 83.3%, leveraging DiffSign's high effectiveness in attack transferability.",
            "headline_zh": "提出DiffSign框架以生成针对自动驾驶交通标志识别系统的物理世界外观攻击",
            "intro_zh": [
                "现有物理世界外观攻击存在隐蔽性差和泛化能力弱的问题",
                "采用CLIP损失和掩码提示改进攻击聚焦与可控性",
                "在多种真实条件下平均攻击成功率达83.3%"
            ],
            "tags_zh": [
                "交通标志识别",
                "物理世界攻击",
                "文本到图像模型",
                "对抗样本",
                "自动驾驶安全"
            ],
            "_index": 147
        },
        {
            "title": "GUIDE: Gaussian Unified Instance Detection for Enhanced Obstacle Perception in Autonomous Driving",
            "authors": [
                "Chunyong Hu",
                "Qi Luo",
                "Jianyun Xu",
                "Song Wang",
                "Qiang Li",
                "Sheng Yang"
            ],
            "arxiv_id": "2511.12941v1",
            "summary": "In the realm of autonomous driving, accurately detecting surrounding obstacles is crucial for effective decision-making. Traditional methods primarily rely on 3D bounding boxes to represent these obstacles, which often fail to capture the complexity of irregularly shaped, real-world objects. To overcome these limitations, we present GUIDE, a novel framework that utilizes 3D Gaussians for instance detection and occupancy prediction. Unlike conventional occupancy prediction methods, GUIDE also offers robust tracking capabilities. Our framework employs a sparse representation strategy, using Gaussian-to-Voxel Splatting to provide fine-grained, instance-level occupancy data without the computational demands associated with dense voxel grids. Experimental validation on the nuScenes dataset demonstrates GUIDE's performance, with an instance occupancy mAP of 21.61, marking a 50\\% improvement over existing methods, alongside competitive tracking capabilities. GUIDE establishes a new benchmark in autonomous perception systems, effectively combining precision with computational efficiency to better address the complexities of real-world driving environments.",
            "headline_zh": "提出GUIDE框架，利用3D高斯实现实例检测与占用预测，以解决自动驾驶中不规则障碍物感知问题。",
            "intro_zh": [
                "核心问题：传统3D边界框方法难以准确表示不规则形状障碍物，影响自动驾驶决策。",
                "方法要点：采用稀疏表示策略，通过高斯到体素投影提供细粒度实例级占用数据，降低计算成本。",
                "实验或效果：在nuScenes数据集上实例占用mAP达21.61，比现有方法提升50%，并具备竞争性跟踪能力。"
            ],
            "tags_zh": [
                "自动驾驶感知",
                "3D高斯建模",
                "实例检测",
                "占用预测",
                "稀疏表示",
                "障碍物跟踪"
            ],
            "_index": 148
        },
        {
            "title": "Recurrent Autoregressive Diffusion: Global Memory Meets Local Attention",
            "authors": [
                "Taiye Chen",
                "Zihan Ding",
                "Anjian Li",
                "Christina Zhang",
                "Zeqi Xiao",
                "Yisen Wang",
                "Chi Jin"
            ],
            "arxiv_id": "2511.12940v1",
            "summary": "Recent advancements in video generation have demonstrated the potential of using video diffusion models as world models, with autoregressive generation of infinitely long videos through masked conditioning. However, such models, usually with local full attention, lack effective memory compression and retrieval for long-term generation beyond the window size, leading to issues of forgetting and spatiotemporal inconsistencies. To enhance the retention of historical information within a fixed memory budget, we introduce a recurrent neural network (RNN) into the diffusion transformer framework. Specifically, a diffusion model incorporating LSTM with attention achieves comparable performance to state-of-the-art RNN blocks, such as TTT and Mamba2. Moreover, existing diffusion-RNN approaches often suffer from performance degradation due to training-inference gap or the lack of overlap across windows. To address these limitations, we propose a novel Recurrent Autoregressive Diffusion (RAD) framework, which executes frame-wise autoregression for memory update and retrieval, consistently across training and inference time. Experiments on Memory Maze and Minecraft datasets demonstrate the superiority of RAD for long video generation, highlighting the efficiency of LSTM in sequence modeling.",
            "headline_zh": "提出RAD框架结合LSTM与注意力，解决长视频生成中的记忆遗忘问题。",
            "intro_zh": [
                "核心问题：现有视频扩散模型在长序列生成中因缺乏有效记忆压缩而遗忘历史信息。",
                "方法要点：引入RNN和LSTM到扩散变换器，实现帧级自回归记忆更新与检索。",
                "实验或效果：在Memory Maze和Minecraft数据集上验证RAD在长视频生成中的优越性。"
            ],
            "tags_zh": [
                "长视频生成",
                "扩散模型",
                "循环神经网络",
                "记忆压缩",
                "自回归生成"
            ],
            "_index": 149
        },
        {
            "title": "Semi-Supervised High Dynamic Range Image Reconstructing via Bi-Level Uncertain Area Masking",
            "authors": [
                "Wei Jiang",
                "Jiahao Cui",
                "Yizheng Wu",
                "Zhan Peng",
                "Zhiyu Pan",
                "Zhiguo Cao"
            ],
            "arxiv_id": "2511.12939v1",
            "summary": "Reconstructing high dynamic range (HDR) images from low dynamic range (LDR) bursts plays an essential role in the computational photography. Impressive progress has been achieved by learning-based algorithms which require LDR-HDR image pairs. However, these pairs are hard to obtain, which motivates researchers to delve into the problem of annotation-efficient HDR image reconstructing: how to achieve comparable performance with limited HDR ground truths (GTs). This work attempts to address this problem from the view of semi-supervised learning where a teacher model generates pseudo HDR GTs for the LDR samples without GTs and a student model learns from pseudo GTs. Nevertheless, the confirmation bias, i.e., the student may learn from the artifacts in pseudo HDR GTs, presents an impediment. To remove this impediment, an uncertainty-based masking process is proposed to discard unreliable parts of pseudo GTs at both pixel and patch levels, then the trusted areas can be learned from by the student. With this novel masking process, our semi-supervised HDR reconstructing method not only outperforms previous annotation-efficient algorithms, but also achieves comparable performance with up-to-date fully-supervised methods by using only 6.7% HDR GTs.",
            "headline_zh": "提出双层不确定性掩码的半监督方法以解决HDR图像重建中标注数据稀缺问题",
            "intro_zh": [
                "核心问题：HDR图像重建依赖LDR-HDR图像对，但标注数据难以获取，导致性能受限",
                "方法要点：采用师生模型框架，通过像素和补丁级不确定性掩码过滤伪标签中的不可靠区域",
                "实验或效果：仅使用6.7%标注数据，性能优于现有半监督方法，并媲美全监督方法"
            ],
            "tags_zh": [
                "高动态范围图像重建",
                "半监督学习",
                "不确定性掩码",
                "师生模型",
                "计算摄影"
            ],
            "_index": 150
        },
        {
            "title": "ProtoAnomalyNCD: Prototype Learning for Multi-class Novel Anomaly Discovery in Industrial Scenarios",
            "authors": [
                "Botong Zhao",
                "Qijun Shi",
                "Shujing Lyu",
                "Yue Lu"
            ],
            "arxiv_id": "2511.12938v1",
            "summary": "Existing industrial anomaly detection methods mainly determine whether an anomaly is present. However, real-world applications also require discovering and classifying multiple anomaly types. Since industrial anomalies are semantically subtle and current methods do not sufficiently exploit image priors, direct clustering approaches often perform poorly. To address these challenges, we propose ProtoAnomalyNCD, a prototype-learning-based framework for discovering unseen anomaly classes of multiple types that can be integrated with various anomaly detection methods. First, to suppress background clutter, we leverage Grounded SAM with text prompts to localize object regions as priors for the anomaly classification network. Next, because anomalies usually appear as subtle and fine-grained patterns on the product, we introduce an Anomaly-Map-Guided Attention block. Within this block, we design a Region Guidance Factor that helps the attention module distinguish among background, object regions, and anomalous regions. By using both localized product regions and anomaly maps as priors, the module enhances anomalous features while suppressing background noise and preserving normal features for contrastive learning. Finally, under a unified prototype-learning framework, ProtoAnomalyNCD discovers and clusters unseen anomaly classes while simultaneously enabling multi-type anomaly classification. We further extend our method to detect unseen outliers, achieving task-level unification. Our method outperforms state-of-the-art approaches on the MVTec AD, MTD, and Real-IAD datasets.",
            "headline_zh": "提出ProtoAnomalyNCD以解决工业场景中多类未知异常发现与分类问题",
            "intro_zh": [
                "核心问题：现有方法仅检测异常存在，无法发现和分类多类未知异常类型。",
                "方法要点：结合Grounded SAM定位对象区域，并设计异常图引导注意力模块增强特征。",
                "实验或效果：在MVTec AD等数据集上优于现有方法，实现任务级统一。"
            ],
            "tags_zh": [
                "工业异常检测",
                "原型学习",
                "多类异常发现",
                "注意力机制",
                "异常图引导",
                "未知异常分类"
            ],
            "_index": 151
        },
        {
            "title": "Yanyun-3: Enabling Cross-Platform Strategy Game Operation with Vision-Language Models",
            "authors": [
                "Guoyan Wang",
                "Yanyan Huang",
                "Chunlin Chen",
                "Lifeng Wang",
                "Yuxiang Sun"
            ],
            "arxiv_id": "2511.12937v1",
            "summary": "Automated operation in cross-platform strategy games demands agents with robust generalization across diverse user interfaces and dynamic battlefield conditions. While vision-language models (VLMs) have shown considerable promise in multimodal reasoning, their application to complex human-computer interaction scenarios--such as strategy gaming--remains largely unexplored. Here, we introduce Yanyun-3, a general-purpose agent framework that, for the first time, enables autonomous cross-platform operation across three heterogeneous strategy game environments. By integrating the vision-language reasoning of Qwen2.5-VL with the precise execution capabilities of UI-TARS, Yanyun-3 successfully performs core tasks including target localization, combat resource allocation, and area control. Through systematic ablation studies, we evaluate the effects of various multimodal data combinations--static images, multi-image sequences, and videos--and propose the concept of combination granularity to differentiate between intra-sample fusion and inter-sample mixing strategies. We find that a hybrid strategy, which fuses multi-image and video data while mixing in static images (MV+S), substantially outperforms full fusion: it reduces inference time by 63% and boosts the BLEU-4 score by a factor of 12 (from 4.81% to 62.41%, approximately 12.98x). Operating via a closed-loop pipeline of screen capture, model inference, and action execution, the agent demonstrates strong real-time performance and cross-platform generalization. Beyond providing an efficient solution for strategy game automation, our work establishes a general paradigm for enhancing VLM performance through structured multimodal data organization, offering new insights into the interplay between static perception and dynamic reasoning in embodied intelligence.",
            "headline_zh": "提出Yanyun-3框架，实现跨平台策略游戏的自主操作。",
            "intro_zh": [
                "核心问题：跨平台策略游戏自动化需泛化处理多样界面和动态战场。",
                "方法要点：融合Qwen2.5-VL视觉语言推理与UI-TARS精确执行能力。",
                "实验或效果：MV+S混合策略提升BLEU-4分数约12.98倍，减少推理时间63%。"
            ],
            "tags_zh": [
                "跨平台游戏自动化",
                "视觉语言模型",
                "多模态数据融合",
                "实时操作",
                "泛化能力"
            ],
            "_index": 152
        },
        {
            "title": "PFAvatar: Pose-Fusion 3D Personalized Avatar Reconstruction from Real-World Outfit-of-the-Day Photos",
            "authors": [
                "Dianbing Xi",
                "Guoyuan An",
                "Jingsen Zhu",
                "Zhijian Liu",
                "Yuan Liu",
                "Ruiyuan Zhang",
                "Jiayuan Lu",
                "Rui Wang",
                "Yuchi Huo"
            ],
            "arxiv_id": "2511.12935v1",
            "summary": "We propose PFAvatar (Pose-Fusion Avatar), a new method that reconstructs high-quality 3D avatars from ``Outfit of the Day'' (OOTD) photos, which exhibit diverse poses, occlusions, and complex backgrounds. Our method consists of two stages: (1) fine-tuning a pose-aware diffusion model from few-shot OOTD examples and (2) distilling a 3D avatar represented by a neural radiance field (NeRF). In the first stage, unlike previous methods that segment images into assets (e.g., garments, accessories) for 3D assembly, which is prone to inconsistency, we avoid decomposition and directly model the full-body appearance. By integrating a pre-trained ControlNet for pose estimation and a novel Condition Prior Preservation Loss (CPPL), our method enables end-to-end learning of fine details while mitigating language drift in few-shot training. Our method completes personalization in just 5 minutes, achieving a 48$\\times$ speed-up compared to previous approaches. In the second stage, we introduce a NeRF-based avatar representation optimized by canonical SMPL-X space sampling and Multi-Resolution 3D-SDS. Compared to mesh-based representations that suffer from resolution-dependent discretization and erroneous occluded geometry, our continuous radiance field can preserve high-frequency textures (e.g., hair) and handle occlusions correctly through transmittance. Experiments demonstrate that PFAvatar outperforms state-of-the-art methods in terms of reconstruction fidelity, detail preservation, and robustness to occlusions/truncations, advancing practical 3D avatar generation from real-world OOTD albums. In addition, the reconstructed 3D avatar supports downstream applications such as virtual try-on, animation, and human video reenactment, further demonstrating the versatility and practical value of our approach.",
            "headline_zh": "提出PFAvatar方法，从日常穿搭照片重建高质量3D虚拟化身",
            "intro_zh": [
                "核心问题：从多样姿态、遮挡和复杂背景的日常穿搭照片中重建3D化身，避免传统分解方法的不一致性问题。",
                "方法要点：采用两阶段方法，先微调姿态感知扩散模型，再蒸馏NeRF表示，结合ControlNet和CPPL损失优化细节。",
                "实验效果：在重建保真度、细节保留和遮挡鲁棒性上优于现有方法，支持虚拟试穿等下游应用。"
            ],
            "tags_zh": [
                "3D虚拟化身重建",
                "神经辐射场",
                "姿态融合",
                "少样本学习",
                "日常穿搭照片",
                "扩散模型"
            ],
            "_index": 153
        },
        {
            "title": "Text2Traffic: A Text-to-Image Generation and Editing Method for Traffic Scenes",
            "authors": [
                "Feng Lv",
                "Haoxuan Feng",
                "Zilu Zhang",
                "Chunlong Xia",
                "Yanfeng Li"
            ],
            "arxiv_id": "2511.12932v1",
            "summary": "With the rapid advancement of intelligent transportation systems, text-driven image generation and editing techniques have demonstrated significant potential in providing rich, controllable visual scene data for applications such as traffic monitoring and autonomous driving. However, several challenges remain, including insufficient semantic richness of generated traffic elements, limited camera viewpoints, low visual fidelity of synthesized images, and poor alignment between textual descriptions and generated content. To address these issues, we propose a unified text-driven framework for both image generation and editing, leveraging a controllable mask mechanism to seamlessly integrate the two tasks. Furthermore, we incorporate both vehicle-side and roadside multi-view data to enhance the geometric diversity of traffic scenes. Our training strategy follows a two-stage paradigm: first, we perform conceptual learning using large-scale coarse-grained text-image data; then, we fine-tune with fine-grained descriptive data to enhance text-image alignment and detail quality. Additionally, we introduce a mask-region-weighted loss that dynamically emphasizes small yet critical regions during training, thereby substantially enhancing the generation fidelity of small-scale traffic elements. Extensive experiments demonstrate that our method achieves leading performance in text-based image generation and editing within traffic scenes.",
            "headline_zh": "提出Text2Traffic框架，通过可控掩码机制和多视角数据增强交通场景图像生成与编辑",
            "intro_zh": [
                "核心问题：交通场景生成中语义丰富度不足、视角受限、视觉保真度低和文本-图像对齐差",
                "方法要点：采用两阶段训练和掩码区域加权损失，提升小元素生成质量和文本对齐",
                "实验或效果：在交通场景文本驱动图像生成和编辑中实现领先性能"
            ],
            "tags_zh": [
                "文本驱动图像生成",
                "交通场景编辑",
                "可控掩码机制",
                "多视角数据增强",
                "两阶段训练",
                "掩码加权损失"
            ],
            "_index": 154
        },
        {
            "title": "Neo: Real-Time On-Device 3D Gaussian Splatting with Reuse-and-Update Sorting Acceleration",
            "authors": [
                "Changhun Oh",
                "Seongryong Oh",
                "Jinwoo Hwang",
                "Yoonsung Kim",
                "Hardik Sharma",
                "Jongse Park"
            ],
            "arxiv_id": "2511.12930v1",
            "summary": "3D Gaussian Splatting (3DGS) rendering in real-time on resource-constrained devices is essential for delivering immersive augmented and virtual reality (AR/VR) experiences. However, existing solutions struggle to achieve high frame rates, especially for high-resolution rendering. Our analysis identifies the sorting stage in the 3DGS rendering pipeline as the major bottleneck due to its high memory bandwidth demand. This paper presents Neo, which introduces a reuse-and-update sorting algorithm that exploits temporal redundancy in Gaussian ordering across consecutive frames, and devises a hardware accelerator optimized for this algorithm. By efficiently tracking and updating Gaussian depth ordering instead of re-sorting from scratch, Neo significantly reduces redundant computations and memory bandwidth pressure. Experimental results show that Neo achieves up to 10.0x and 5.6x higher throughput than state-of-the-art edge GPU and ASIC solution, respectively, while reducing DRAM traffic by 94.5% and 81.3%. These improvements make high-quality and low-latency on-device 3D rendering more practical.",
            "headline_zh": "提出重用更新排序算法以加速移动设备实时3D高斯泼溅渲染",
            "intro_zh": [
                "3D高斯泼溅渲染中排序阶段是内存带宽瓶颈，限制实时性能",
                "引入重用更新排序算法，利用帧间高斯顺序冗余减少计算和带宽",
                "实验显示吞吐量提升最高10倍，DRAM流量降低94.5%"
            ],
            "tags_zh": [
                "3D高斯泼溅",
                "实时渲染",
                "移动设备加速",
                "排序算法优化",
                "内存带宽优化"
            ],
            "_index": 155
        },
        {
            "title": "Generative Photographic Control for Scene-Consistent Video Cinematic Editing",
            "authors": [
                "Huiqiang Sun",
                "Liao Shen",
                "Zhan Peng",
                "Kun Wang",
                "Size Wu",
                "Yuhang Zang",
                "Tianqi Liu",
                "Zihao Huang",
                "Xingyu Zeng",
                "Zhiguo Cao",
                "Wei Li",
                "Chen Change Loy"
            ],
            "arxiv_id": "2511.12921v1",
            "summary": "Cinematic storytelling is profoundly shaped by the artful manipulation of photographic elements such as depth of field and exposure. These effects are crucial in conveying mood and creating aesthetic appeal. However, controlling these effects in generative video models remains highly challenging, as most existing methods are restricted to camera motion control. In this paper, we propose CineCtrl, the first video cinematic editing framework that provides fine control over professional camera parameters (e.g., bokeh, shutter speed). We introduce a decoupled cross-attention mechanism to disentangle camera motion from photographic inputs, allowing fine-grained, independent control without compromising scene consistency. To overcome the shortage of training data, we develop a comprehensive data generation strategy that leverages simulated photographic effects with a dedicated real-world collection pipeline, enabling the construction of a large-scale dataset for robust model training. Extensive experiments demonstrate that our model generates high-fidelity videos with precisely controlled, user-specified photographic camera effects.",
            "headline_zh": "提出CineCtrl框架以实现视频电影编辑中对摄影参数的精细控制",
            "intro_zh": [
                "核心问题：现有生成视频模型难以控制摄影元素如景深和曝光，仅限相机运动控制",
                "方法要点：引入解耦交叉注意力机制，分离相机运动与摄影输入，实现独立精细控制",
                "实验或效果：通过大规模数据集训练，生成高保真视频，精确控制用户指定摄影效果"
            ],
            "tags_zh": [
                "视频生成",
                "摄影控制",
                "解耦注意力",
                "电影编辑",
                "数据集构建"
            ],
            "_index": 156
        },
        {
            "title": "CoordAR: One-Reference 6D Pose Estimation of Novel Objects via Autoregressive Coordinate Map Generation",
            "authors": [
                "Dexin Zuo",
                "Ang Li",
                "Wei Wang",
                "Wenxian Yu",
                "Danping Zou"
            ],
            "arxiv_id": "2511.12919v1",
            "summary": "Object 6D pose estimation, a crucial task for robotics and augmented reality applications, becomes particularly challenging when dealing with novel objects whose 3D models are not readily available. To reduce dependency on 3D models, recent studies have explored one-reference-based pose estimation, which requires only a single reference view instead of a complete 3D model. However, existing methods that rely on real-valued coordinate regression suffer from limited global consistency due to the local nature of convolutional architectures and face challenges in symmetric or occluded scenarios owing to a lack of uncertainty modeling. We present CoordAR, a novel autoregressive framework for one-reference 6D pose estimation of unseen objects. CoordAR formulates 3D-3D correspondences between the reference and query views as a map of discrete tokens, which is obtained in an autoregressive and probabilistic manner. To enable accurate correspondence regression, CoordAR introduces 1) a novel coordinate map tokenization that enables probabilistic prediction over discretized 3D space; 2) a modality-decoupled encoding strategy that separately encodes RGB appearance and coordinate cues; and 3) an autoregressive transformer decoder conditioned on both position-aligned query features and the partially generated token sequence. With these novel mechanisms, CoordAR significantly outperforms existing methods on multiple benchmarks and demonstrates strong robustness to symmetry, occlusion, and other challenges in real-world tests.",
            "headline_zh": "提出CoordAR自回归框架以解决单参考视图下新物体6D姿态估计问题",
            "intro_zh": [
                "核心问题：新物体6D姿态估计依赖3D模型，现有方法在对称和遮挡场景下全局一致性不足",
                "方法要点：将3D-3D对应关系离散化为token，采用自回归和概率预测提升准确性",
                "实验效果：在多个基准测试中显著优于现有方法，对对称和遮挡具有强鲁棒性"
            ],
            "tags_zh": [
                "6D姿态估计",
                "自回归模型",
                "单参考视图",
                "3D-3D对应",
                "概率预测",
                "Transformer解码器"
            ],
            "_index": 157
        },
        {
            "title": "Explore How to Inject Beneficial Noise in MLLMs",
            "authors": [
                "Ruishu Zhu",
                "Sida Huang",
                "Ziheng Jiao",
                "Hongyuan Zhang"
            ],
            "arxiv_id": "2511.12917v1",
            "summary": "Multimodal Large Language Models (MLLMs) have played an increasingly important role in multimodal intelligence. However, the existing fine-tuning methods often ignore cross-modal heterogeneity, limiting their full potential. In this work, we propose a novel fine-tuning strategy by injecting beneficial random noise, which outperforms previous methods and even surpasses full fine-tuning, with minimal additional parameters. The proposed Multimodal Noise Generator (MuNG) enables efficient modality fine-tuning by injecting customized noise into the frozen MLLMs. Specifically, we reformulate the reasoning process of MLLMs from a variational inference perspective, upon which we design a multimodal noise generator that dynamically analyzes cross-modal relationships in image-text pairs to generate task-adaptive beneficial noise. Injecting this type of noise into the MLLMs effectively suppresses irrelevant semantic components, leading to significantly improved cross-modal representation alignment and enhanced performance on downstream tasks. Experiments on two mainstream MLLMs, QwenVL and LLaVA, demonstrate that our method surpasses full-parameter fine-tuning and other existing fine-tuning approaches, while requiring adjustments to only about $1\\sim2\\%$ additional parameters. The relevant code is uploaded in the supplementary.",
            "headline_zh": "提出多模态噪声生成器以优化MLLMs跨模态对齐，实现高效微调",
            "intro_zh": [
                "核心问题：现有MLLMs微调方法忽略跨模态异构性，限制性能提升",
                "方法要点：基于变分推理设计噪声生成器，动态分析跨模态关系注入有益噪声",
                "实验或效果：在QwenVL和LLaVA上超越全参数微调，仅需1~2%额外参数"
            ],
            "tags_zh": [
                "多模态大语言模型",
                "噪声注入",
                "跨模态对齐",
                "高效微调",
                "变分推理"
            ],
            "_index": 158
        },
        {
            "title": "DiffuDepGrasp: Diffusion-based Depth Noise Modeling Empowers Sim2Real Robotic Grasping",
            "authors": [
                "Yingting Zhou",
                "Wenbo Cui",
                "Weiheng Liu",
                "Guixing Chen",
                "Haoran Li",
                "Dongbin Zhao"
            ],
            "arxiv_id": "2511.12912v1",
            "summary": "Transferring the depth-based end-to-end policy trained in simulation to physical robots can yield an efficient and robust grasping policy, yet sensor artifacts in real depth maps like voids and noise establish a significant sim2real gap that critically impedes policy transfer. Training-time strategies like procedural noise injection or learned mappings suffer from data inefficiency due to unrealistic noise simulation, which is often ineffective for grasping tasks that require fine manipulation or dependency on paired datasets heavily. Furthermore, leveraging foundation models to reduce the sim2real gap via intermediate representations fails to mitigate the domain shift fully and adds computational overhead during deployment. This work confronts dual challenges of data inefficiency and deployment complexity. We propose DiffuDepGrasp, a deploy-efficient sim2real framework enabling zero-shot transfer through simulation-exclusive policy training. Its core innovation, the Diffusion Depth Generator, synthesizes geometrically pristine simulation depth with learned sensor-realistic noise via two synergistic modules. The first Diffusion Depth Module leverages temporal geometric priors to enable sample-efficient training of a conditional diffusion model that captures complex sensor noise distributions, while the second Noise Grafting Module preserves metric accuracy during perceptual artifact injection. With only raw depth inputs during deployment, DiffuDepGrasp eliminates computational overhead and achieves a 95.7% average success rate on 12-object grasping with zero-shot transfer and strong generalization to unseen objects.Project website: https://diffudepgrasp.github.io/.",
            "headline_zh": "提出DiffuDepGrasp框架，通过扩散模型模拟深度噪声，实现零样本Sim2Real抓取。",
            "intro_zh": [
                "核心问题：真实深度图中的传感器伪影（如空洞和噪声）阻碍仿真到现实的策略迁移。",
                "方法要点：使用扩散深度生成器合成仿真深度，结合噪声嫁接模块注入传感器真实噪声。",
                "实验效果：零样本转移下，在12个物体抓取中平均成功率95.7%，泛化性强。"
            ],
            "tags_zh": [
                "Sim2Real迁移",
                "扩散模型",
                "深度噪声建模",
                "机器人抓取",
                "零样本学习"
            ],
            "_index": 159
        },
        {
            "title": "TOPP-DWR: Time-Optimal Path Parameterization of Differential-Driven Wheeled Robots Considering Piecewise-Constant Angular Velocity Constraints",
            "authors": [
                "Yong Li",
                "Yujun Huang",
                "Yi Chen",
                "Hui Cheng"
            ],
            "arxiv_id": "2511.12910v1",
            "summary": "Differential-driven wheeled robots (DWR) represent the quintessential type of mobile robots and find extensive appli- cations across the robotic field. Most high-performance control approaches for DWR explicitly utilize the linear and angular velocities of the trajectory as control references. However, existing research on time-optimal path parameterization (TOPP) for mobile robots usually neglects the angular velocity and joint vel- ocity constraints, which can result in degraded control perfor- mance in practical applications. In this article, a systematic and practical TOPP algorithm named TOPP-DWR is proposed for DWR and other mobile robots. First, the non-uniform B-spline is adopted to represent the initial trajectory in the task space. Second, the piecewise-constant angular velocity, as well as joint velocity, linear velocity, and linear acceleration constraints, are incorporated into the TOPP problem. During the construction of the optimization problem, the aforementioned constraints are uniformly represented as linear velocity constraints. To boost the numerical computational efficiency, we introduce a slack variable to reformulate the problem into second-order-cone programming (SOCP). Subsequently, comparative experiments are conducted to validate the superiority of the proposed method. Quantitative performance indexes show that TOPP-DWR achieves TOPP while adhering to all constraints. Finally, field autonomous navigation experiments are carried out to validate the practicability of TOPP-DWR in real-world applications.",
            "headline_zh": "提出TOPP-DWR算法，为差动轮式机器人实现时间最优路径参数化，考虑角速度等约束。",
            "intro_zh": [
                "核心问题：现有TOPP方法忽略角速度和关节速度约束，导致实际控制性能下降。",
                "方法要点：采用非均匀B样条表示轨迹，将约束统一为线性速度约束，并转化为SOCP问题求解。",
                "实验或效果：比较实验验证算法优越性，现场导航实验证明其在实际应用中的实用性。"
            ],
            "tags_zh": [
                "时间最优路径参数化",
                "差动轮式机器人",
                "角速度约束",
                "二阶锥规划",
                "轨迹优化",
                "自主导航"
            ],
            "_index": 160
        },
        {
            "title": "CASL: Curvature-Augmented Self-supervised Learning for 3D Anomaly Detection",
            "authors": [
                "Yaohua Zha",
                "Xue Yuerong",
                "Chunlin Fan",
                "Yuansong Wang",
                "Tao Dai",
                "Ke Chen",
                "Shu-Tao Xia"
            ],
            "arxiv_id": "2511.12909v1",
            "summary": "Deep learning-based 3D anomaly detection methods have demonstrated significant potential in industrial manufacturing. However, many approaches are specifically designed for anomaly detection tasks, which limits their generalizability to other 3D understanding tasks. In contrast, self-supervised point cloud models aim for general-purpose representation learning, yet our investigation reveals that these classical models are suboptimal at anomaly detection under the unified fine-tuning paradigm. This motivates us to develop a more generalizable 3D model that can effectively detect anomalies without relying on task-specific designs. Interestingly, we find that using only the curvature of each point as its anomaly score already outperforms several classical self-supervised and dedicated anomaly detection models, highlighting the critical role of curvature in 3D anomaly detection. In this paper, we propose a Curvature-Augmented Self-supervised Learning (CASL) framework based on a reconstruction paradigm. Built upon the classical U-Net architecture, our approach introduces multi-scale curvature prompts to guide the decoder in predicting the spatial coordinates of each point. Without relying on any dedicated anomaly detection mechanisms, it achieves leading detection performance through straightforward anomaly classification fine-tuning. Moreover, the learned representations generalize well to standard 3D understanding tasks such as point cloud classification. The code is available at https://github.com/zyh16143998882/CASL.",
            "headline_zh": "提出曲率增强自监督学习框架以改进3D异常检测与通用表示学习",
            "intro_zh": [
                "核心问题：现有自监督点云模型在异常检测中表现不佳，缺乏通用性。",
                "方法要点：基于U-Net架构，引入多尺度曲率提示指导点坐标重建。",
                "实验或效果：仅用曲率作异常分数即超越经典模型，实现领先检测性能。"
            ],
            "tags_zh": [
                "3D异常检测",
                "自监督学习",
                "点云重建",
                "曲率提示",
                "通用表示学习"
            ],
            "_index": 161
        },
        {
            "title": "DeepSport: A Multimodal Large Language Model for Comprehensive Sports Video Reasoning via Agentic Reinforcement Learning",
            "authors": [
                "Junbo Zou",
                "Haotian Xia",
                "Zhen Ye",
                "Shengjie Zhang",
                "Christopher Lai",
                "Vicente Ordonez",
                "Weining Shen",
                "Hanjie Chen"
            ],
            "arxiv_id": "2511.12908v1",
            "summary": "Sports video understanding presents unique challenges, requiring models to perceive high-speed dynamics, comprehend complex rules, and reason over long temporal contexts. While Multimodal Large Language Models (MLLMs) have shown promise in genral domains, the current state of research in sports remains narrowly focused: existing approaches are either single-sport centric, limited to specific tasks, or rely on training-free paradigms that lack robust, learned reasoning process. To address this gap, we introduce DeepSport, the first end-to-end trained MLLM framework designed for multi-task, multi-sport video understanding. DeepSport shifts the paradigm from passive frame processing to active, iterative reasoning, empowering the model to ``think with videos'' by dynamically interrogating content via a specialized frame-extraction tool. To enable this, we propose a data distillation pipeline that synthesizes high-quality Chain-of-Thought (CoT) trajectories from 10 diverse data source, creating a unified resource of 78k training data. We then employ a two-stage training strategy, Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL) with a novel gated tool-use reward, to optimize the model's reasoning process. Extensive experiments on the testing benchmark of 6.7k questions demonstrate that DeepSport achieves state-of-the-art performance, significantly outperforming baselines of both proprietary model and open-source models. Our work establishes a new foundation for domain-specific video reasoning to address the complexities of diverse sports.",
            "headline_zh": "提出DeepSport多模态大模型，通过代理强化学习解决多运动视频理解问题",
            "intro_zh": [
                "核心问题：体育视频理解需处理高速动态、复杂规则和长时序上下文，现有方法局限于单运动或特定任务",
                "方法要点：采用端到端训练，结合监督微调和强化学习，使用门控工具奖励优化推理过程",
                "实验或效果：在6.7k问题测试集上达到最优性能，显著超越专有和开源基线模型"
            ],
            "tags_zh": [
                "多模态大语言模型",
                "体育视频理解",
                "代理强化学习",
                "数据蒸馏",
                "端到端训练",
                "多任务学习"
            ],
            "_index": 162
        },
        {
            "title": "FDP: A Frequency-Decomposition Preprocessing Pipeline for Unsupervised Anomaly Detection in Brain MRI",
            "authors": [
                "Hao Li",
                "Zhenfeng Zhuang",
                "Jingyu Lin",
                "Yu Liu",
                "Yifei Chen",
                "Qiong Peng",
                "Lequan Yu",
                "Liansheng Wang"
            ],
            "arxiv_id": "2511.12899v1",
            "summary": "Due to the diversity of brain anatomy and the scarcity of annotated data, supervised anomaly detection for brain MRI remains challenging, driving the development of unsupervised anomaly detection (UAD) approaches. Current UAD methods typically utilize artificially generated noise perturbations on healthy MRIs to train generative models for normal anatomy reconstruction, enabling anomaly detection via residual mapping. However, such simulated anomalies lack the biophysical fidelity and morphological complexity characteristic of true clinical lesions. To advance UAD in brain MRI, we conduct the first systematic frequency-domain analysis of pathological signatures, revealing two key properties: (1) anomalies exhibit unique frequency patterns distinguishable from normal anatomy, and (2) low-frequency signals maintain consistent representations across healthy scans. These insights motivate our Frequency-Decomposition Preprocessing (FDP) framework, the first UAD method to leverage frequency-domain reconstruction for simultaneous pathology suppression and anatomical preservation. FDP can integrate seamlessly with existing anomaly simulation techniques, consistently enhancing detection performance across diverse architectures while maintaining diagnostic fidelity. Experimental results demonstrate that FDP consistently improves anomaly detection performance when integrated with existing methods. Notably, FDP achieves a 17.63% increase in DICE score with LDM while maintaining robust improvements across multiple baselines. The code is available at https://github.com/ls1rius/MRI_FDP.",
            "headline_zh": "提出频率分解预处理框架以提升脑MRI无监督异常检测性能",
            "intro_zh": [
                "脑MRI异常检测面临解剖多样性和标注数据稀缺的挑战",
                "通过频率域分析，分离异常与正常解剖信号，实现病理抑制和结构保留",
                "集成现有方法，DICE分数提升17.63%，并在多基准上稳健改进"
            ],
            "tags_zh": [
                "无监督异常检测",
                "脑MRI分析",
                "频率分解",
                "病理抑制",
                "重建方法"
            ],
            "_index": 163
        },
        {
            "title": "Functional Mean Flow in Hilbert Space",
            "authors": [
                "Zhiqi Li",
                "Yuchen Sun",
                "Greg Turk",
                "Bo Zhu"
            ],
            "arxiv_id": "2511.12898v1",
            "summary": "We present Functional Mean Flow (FMF) as a one-step generative model defined in infinite-dimensional Hilbert space. FMF extends the one-step Mean Flow framework to functional domains by providing a theoretical formulation for Functional Flow Matching and a practical implementation for efficient training and sampling. We also introduce an $x_1$-prediction variant that improves stability over the original $u$-prediction form. The resulting framework is a practical one-step Flow Matching method applicable to a wide range of functional data generation tasks such as time series, images, PDEs, and 3D geometry.",
            "headline_zh": "提出Functional Mean Flow，在无限维希尔伯特空间中实现一步生成模型，适用于函数数据生成。",
            "intro_zh": [
                "核心问题：将一步生成模型扩展到无限维函数域，处理如时间序列、图像等数据。",
                "方法要点：提供Functional Flow Matching理论框架和实用实现，引入x1预测变体提升稳定性。",
                "实验或效果：框架高效训练和采样，适用于多种函数数据生成任务，如PDE和3D几何。"
            ],
            "tags_zh": [
                "函数数据生成",
                "一步生成模型",
                "希尔伯特空间",
                "Flow Matching",
                "时间序列生成",
                "图像生成"
            ],
            "_index": 164
        },
        {
            "title": "Air-Chamber Based Soft Six-Axis Force/Torque Sensor for Human-Robot Interaction",
            "authors": [
                "Jun Huo",
                "Hongge Ru",
                "Bo Yang",
                "Xingjian Chen",
                "Xi Li",
                "Jian Huang"
            ],
            "arxiv_id": "2511.12896v1",
            "summary": "Soft multi-axis force/torque sensors provide safe and precise force interaction. Capturing the complete degree-of-freedom of force is imperative for accurate force measurement with six-axis force/torque sensors. However, cross-axis coupling can lead to calibration issues and decreased accuracy. In this instance, developing a soft and accurate six-axis sensor is a challenging task. In this paper, a soft air-chamber type six-axis force/torque sensor with 16-channel barometers is introduced, which housed in hyper-elastic air chambers made of silicone rubber. Additionally, an effective decoupling method is proposed, based on a rigid-soft hierarchical structure, which reduces the six-axis decoupling problem to two three-axis decoupling problems. Finite element model simulation and experiments demonstrate the compatibility of the proposed approach with reality. The prototype's sensing performance is quantitatively measured in terms of static load response, dynamic load response and dynamic response characteristic. It possesses a measuring range of 50 N force and 1 Nm torque, and the average deviation, repeatability, non-linearity and hysteresis are 4.9$\\%$, 2.7$\\%$, 5.8$\\%$ and 6.7$\\%$, respectively. The results indicate that the prototype exhibits satisfactory sensing performance while maintaining its softness due to the presence of soft air chambers.",
            "headline_zh": "提出基于气室的软六轴力/力矩传感器，用于人机交互，解决交叉轴耦合问题。",
            "intro_zh": [
                "核心问题：软六轴传感器存在交叉轴耦合，导致校准困难和精度下降。",
                "方法要点：采用16通道气压计和刚软分层结构，将六轴解耦简化为两个三轴问题。",
                "实验或效果：原型测量范围50N力和1Nm力矩，平均偏差4.9%，重复性2.7%。"
            ],
            "tags_zh": [
                "软传感器",
                "六轴力/力矩测量",
                "人机交互",
                "交叉轴解耦",
                "气压计",
                "刚软结构"
            ],
            "_index": 165
        },
        {
            "title": "Reconstructing 3D Scenes in Native High Dynamic Range",
            "authors": [
                "Kaixuan Zhang",
                "Minxian Li",
                "Mingwu Ren",
                "Jiankang Deng",
                "Xiatian Zhu"
            ],
            "arxiv_id": "2511.12895v1",
            "summary": "High Dynamic Range (HDR) imaging is essential for professional digital media creation, e.g., filmmaking, virtual production, and photorealistic rendering. However, 3D scene reconstruction has primarily focused on Low Dynamic Range (LDR) data, limiting its applicability to professional workflows. Existing approaches that reconstruct HDR scenes from LDR observations rely on multi-exposure fusion or inverse tone-mapping, which increase capture complexity and depend on synthetic supervision. With the recent emergence of cameras that directly capture native HDR data in a single exposure, we present the first method for 3D scene reconstruction that directly models native HDR observations. We propose {\\bf Native High dynamic range 3D Gaussian Splatting (NH-3DGS)}, which preserves the full dynamic range throughout the reconstruction pipeline. Our key technical contribution is a novel luminance-chromaticity decomposition of the color representation that enables direct optimization from native HDR camera data. We demonstrate on both synthetic and real multi-view HDR datasets that NH-3DGS significantly outperforms existing methods in reconstruction quality and dynamic range preservation, enabling professional-grade 3D reconstruction directly from native HDR captures. Code and datasets will be made available.",
            "headline_zh": "提出原生高动态范围3D高斯泼溅以直接建模HDR数据，提升3D场景重建质量",
            "intro_zh": [
                "核心问题：现有3D重建方法依赖LDR数据，限制专业应用，HDR重建需复杂多曝光或逆色调映射",
                "方法要点：引入亮度-色度分解颜色表示，直接从原生HDR相机数据优化，保持全动态范围",
                "实验或效果：在合成和真实HDR数据集上，重建质量和动态范围保持显著优于现有方法"
            ],
            "tags_zh": [
                "高动态范围成像",
                "3D场景重建",
                "高斯泼溅",
                "亮度-色度分解",
                "原生HDR数据",
                "多视图重建"
            ],
            "_index": 166
        },
        {
            "title": "ActVAR: Activating Mixtures of Weights and Tokens for Efficient Visual Autoregressive Generation",
            "authors": [
                "Kaixin Zhang",
                "Ruiqing Yang",
                "Yuan Zhang",
                "Shan You",
                "Tao Huang"
            ],
            "arxiv_id": "2511.12893v1",
            "summary": "Visual Autoregressive (VAR) models enable efficient image generation via next-scale prediction but face escalating computational costs as sequence length grows. Existing static pruning methods degrade performance by permanently removing weights or tokens, disrupting pretrained dependencies. To address this, we propose ActVAR, a dynamic activation framework that introduces dual sparsity across model weights and token sequences to enhance efficiency without sacrificing capacity. ActVAR decomposes feedforward networks (FFNs) into lightweight expert sub-networks and employs a learnable router to dynamically select token-specific expert subsets based on content. Simultaneously, a gated token selector identifies high-update-potential tokens for computation while reconstructing unselected tokens to preserve global context and sequence alignment. Training employs a two-stage knowledge distillation strategy, where the original VAR model supervises the learning of routing and gating policies to align with pretrained knowledge. Experiments on the ImageNet $256\\times 256$ benchmark demonstrate that ActVAR achieves up to $21.2\\%$ FLOPs reduction with minimal performance degradation.",
            "headline_zh": "提出ActVAR框架以解决视觉自回归模型计算成本高的问题",
            "intro_zh": [
                "视觉自回归模型序列增长导致计算成本剧增，静态剪枝破坏预训练依赖",
                "动态激活权重和令牌，分解FFN为专家子网，路由选择专家，门控选择令牌",
                "ImageNet 256×256基准上，FLOPs减少21.2%，性能损失最小"
            ],
            "tags_zh": [
                "视觉自回归生成",
                "动态激活",
                "知识蒸馏",
                "计算效率优化",
                "令牌选择",
                "专家网络"
            ],
            "_index": 167
        },
        {
            "title": "Towards High-Consistency Embodied World Model with Multi-View Trajectory Videos",
            "authors": [
                "Taiyi Su",
                "Jian Zhu",
                "Yaxuan Li",
                "Chong Ma",
                "Zitai Huang",
                "Yichen Zhu",
                "Hanli Wang",
                "Yi Xu"
            ],
            "arxiv_id": "2511.12882v1",
            "summary": "Embodied world models aim to predict and interact with the physical world through visual observations and actions. However, existing models struggle to accurately translate low-level actions (e.g., joint positions) into precise robotic movements in predicted frames, leading to inconsistencies with real-world physical interactions. To address these limitations, we propose MTV-World, an embodied world model that introduces Multi-view Trajectory-Video control for precise visuomotor prediction. Specifically, instead of directly using low-level actions for control, we employ trajectory videos obtained through camera intrinsic and extrinsic parameters and Cartesian-space transformation as control signals. However, projecting 3D raw actions onto 2D images inevitably causes a loss of spatial information, making a single view insufficient for accurate interaction modeling. To overcome this, we introduce a multi-view framework that compensates for spatial information loss and ensures high-consistency with physical world. MTV-World forecasts future frames based on multi-view trajectory videos as input and conditioning on an initial frame per view. Furthermore, to systematically evaluate both robotic motion precision and object interaction accuracy, we develop an auto-evaluation pipeline leveraging multimodal large models and referring video object segmentation models. To measure spatial consistency, we formulate it as an object location matching problem and adopt the Jaccard Index as the evaluation metric. Extensive experiments demonstrate that MTV-World achieves precise control execution and accurate physical interaction modeling in complex dual-arm scenarios.",
            "headline_zh": "提出MTV-World模型，通过多视角轨迹视频控制解决具身世界模型物理交互不一致问题",
            "intro_zh": [
                "核心问题：现有模型难以将低级动作精确转换为机器人运动，导致预测帧与现实物理交互不一致",
                "方法要点：使用多视角轨迹视频作为控制信号，补偿空间信息损失，提升预测一致性",
                "实验或效果：在复杂双臂场景中实现精确控制和准确物理交互建模，采用Jaccard指数评估空间一致性"
            ],
            "tags_zh": [
                "具身世界模型",
                "多视角轨迹视频",
                "物理交互一致性",
                "机器人控制",
                "视觉预测",
                "空间信息补偿"
            ],
            "_index": 168
        },
        {
            "title": "Simple Lines, Big Ideas: Towards Interpretable Assessment of Human Creativity from Drawings",
            "authors": [
                "Zihao Lin",
                "Zhenshan Shi",
                "Sasa Zhao",
                "Hanwei Zhu",
                "Lingyu Zhu",
                "Baoliang Chen",
                "Lei Mo"
            ],
            "arxiv_id": "2511.12880v1",
            "summary": "Assessing human creativity through visual outputs, such as drawings, plays a critical role in fields including psychology, education, and cognitive science. However, current assessment practices still rely heavily on expert-based subjective scoring, which is both labor-intensive and inherently subjective. In this paper, we propose a data-driven framework for automatic and interpretable creativity assessment from drawings. Motivated by the cognitive understanding that creativity can emerge from both what is drawn (content) and how it is drawn (style), we reinterpret the creativity score as a function of these two complementary dimensions.Specifically, we first augment an existing creativity labeled dataset with additional annotations targeting content categories. Based on the enriched dataset, we further propose a multi-modal, multi-task learning framework that simultaneously predicts creativity scores, categorizes content types, and extracts stylistic features. In particular, we introduce a conditional learning mechanism that enables the model to adapt its visual feature extraction by dynamically tuning it to creativity-relevant signals conditioned on the drawing's stylistic and semantic cues.Experimental results demonstrate that our model achieves state-of-the-art performance compared to existing regression-based approaches and offers interpretable visualizations that align well with human judgments. The code and annotations will be made publicly available at https://github.com/WonderOfU9/CSCA_PRCV_2025",
            "headline_zh": "提出多模态多任务框架以自动评估绘画创造力，实现可解释性。",
            "intro_zh": [
                "核心问题：绘画创造力评估依赖专家主观评分，劳动密集且主观性强。",
                "方法要点：结合内容和风格维度，通过条件学习机制动态调整特征提取。",
                "实验或效果：模型性能优于现有回归方法，提供与人类判断一致的可视化。"
            ],
            "tags_zh": [
                "创造力评估",
                "多模态学习",
                "多任务学习",
                "可解释性",
                "绘画分析"
            ],
            "_index": 169
        },
        {
            "title": "Uni-Hand: Universal Hand Motion Forecasting in Egocentric Views",
            "authors": [
                "Junyi Ma",
                "Wentao Bao",
                "Jingyi Xu",
                "Guanzhong Sun",
                "Yu Zheng",
                "Erhang Zhang",
                "Xieyuanli Chen",
                "Hesheng Wang"
            ],
            "arxiv_id": "2511.12878v1",
            "summary": "Analyzing hand-object interaction in egocentric vision facilitates VR/AR applications and human-robot policy transfer. Existing research has mostly focused on modeling the behavior paradigm of interactive actions (i.e., \"how to interact\"). However, the more challenging and fine-grained problem of capturing the critical moments of contact and separation between the hand and the target object (i.e., \"when to interact\") is still underexplored, which is crucial for immersive interactive experiences in mixed reality and robotic motion planning. Therefore, we formulate this problem as temporal interaction localization (TIL). Some recent works extract semantic masks as TIL references, but suffer from inaccurate object grounding and cluttered scenarios. Although current temporal action localization (TAL) methods perform well in detecting verb-noun action segments, they rely on category annotations during training and exhibit limited precision in localizing hand-object contact/separation moments. To address these issues, we propose a novel zero-shot approach dubbed EgoLoc to localize hand-object contact and separation timestamps in egocentric videos. EgoLoc introduces hand-dynamics-guided sampling to generate high-quality visual prompts. It exploits the vision-language model to identify contact/separation attributes, localize specific timestamps, and provide closed-loop feedback for further refinement. EgoLoc eliminates the need for object masks and verb-noun taxonomies, leading to generalizable zero-shot implementation. Comprehensive experiments on the public dataset and our novel benchmarks demonstrate that EgoLoc achieves plausible TIL for egocentric videos. It is also validated to effectively facilitate multiple downstream applications in egocentric vision and robotic manipulation tasks. Code and relevant data will be released at https://github.com/IRMVLab/EgoLoc.",
            "headline_zh": "提出EgoLoc方法以零样本定位手-物接触/分离时刻，提升第一人称视频交互分析。",
            "intro_zh": [
                "核心问题：现有方法难以精确定位手与物体接触和分离的关键时刻，影响沉浸式交互和机器人规划。",
                "方法要点：引入手动态引导采样和视觉语言模型，无需对象掩码或类别注释，实现零样本时序定位。",
                "实验或效果：在公共数据集和新基准上验证，EgoLoc实现可信时序交互定位，并促进下游应用。"
            ],
            "tags_zh": [
                "时序交互定位",
                "零样本学习",
                "第一人称视觉",
                "手-物交互",
                "视觉语言模型"
            ],
            "_index": 170
        },
        {
            "title": "View-aware Cross-modal Distillation for Multi-view Action Recognition",
            "authors": [
                "Trung Thanh Nguyen",
                "Yasutomo Kawanishi",
                "Vijay John",
                "Takahiro Komamizu",
                "Ichiro Ide"
            ],
            "arxiv_id": "2511.12870v1",
            "summary": "The widespread use of multi-sensor systems has increased research in multi-view action recognition. While existing approaches in multi-view setups with fully overlapping sensors benefit from consistent view coverage, partially overlapping settings where actions are visible in only a subset of views remain underexplored. This challenge becomes more severe in real-world scenarios, as many systems provide only limited input modalities and rely on sequence-level annotations instead of dense frame-level labels. In this study, we propose View-aware Cross-modal Knowledge Distillation (ViCoKD), a framework that distills knowledge from a fully supervised multi-modal teacher to a modality- and annotation-limited student. ViCoKD employs a cross-modal adapter with cross-modal attention, allowing the student to exploit multi-modal correlations while operating with incomplete modalities. Moreover, we propose a View-aware Consistency module to address view misalignment, where the same action may appear differently or only partially across viewpoints. It enforces prediction alignment when the action is co-visible across views, guided by human-detection masks and confidence-weighted Jensen-Shannon divergence between their predicted class distributions. Experiments on the real-world MultiSensor-Home dataset show that ViCoKD consistently outperforms competitive distillation methods across multiple backbones and environments, delivering significant gains and surpassing the teacher model under limited conditions.",
            "headline_zh": "提出视图感知跨模态蒸馏以解决部分重叠多视图动作识别问题",
            "intro_zh": [
                "核心问题：部分重叠多视图场景中动作仅部分可见，且模态和标注有限。",
                "方法要点：使用跨模态注意力和视图一致性模块，蒸馏多模态教师知识到学生模型。",
                "实验或效果：在MultiSensor-Home数据集上超越竞争方法，并在有限条件下优于教师模型。"
            ],
            "tags_zh": [
                "多视图动作识别",
                "跨模态蒸馏",
                "视图感知一致性",
                "部分重叠视图",
                "知识蒸馏",
                "多模态学习"
            ],
            "_index": 171
        },
        {
            "title": "Video Finetuning Improves Reasoning Between Frames",
            "authors": [
                "Ruiqi Yang",
                "Tian Yun",
                "Zihan Wang",
                "Ellie Pavlick"
            ],
            "arxiv_id": "2511.12868v1",
            "summary": "Multimodal large language models (LLMs) have made rapid progress in visual understanding, yet their extension from images to videos often reduces to a naive concatenation of frame tokens. In this work, we investigate what video finetuning brings to multimodal LLMs. We propose Visual Chain-of-Thought (vCoT), an explicit reasoning process that generates transitional event descriptions between consecutive frames. Using vCoT, we systematically compare image-only LVLMs with their video-finetuned counterparts, both with and without access to these transitional cues. Our experiments show that vCoT significantly improves the performance of image-only models on long-form video question answering, while yielding only marginal gains for video-finetuned models. This suggests that the latter already capture frame-to-frame transitions implicitly. Moreover, we find that video models transfer this temporal reasoning ability to purely static settings, outperforming image models' baselines on relational visual reasoning tasks.",
            "headline_zh": "提出视觉思维链以提升多模态大模型在视频中的帧间推理能力",
            "intro_zh": [
                "核心问题：多模态大模型从图像扩展到视频时，常简单拼接帧令牌，缺乏帧间推理。",
                "方法要点：引入视觉思维链，生成连续帧间的过渡事件描述，用于系统比较模型。",
                "实验或效果：视频微调模型在长视频问答中表现更优，并能迁移到静态视觉推理任务。"
            ],
            "tags_zh": [
                "多模态大语言模型",
                "视频微调",
                "视觉思维链",
                "帧间推理",
                "长视频问答",
                "视觉推理"
            ],
            "_index": 172
        },
        {
            "title": "From Perception to Reasoning: Deep Thinking Empowers Multimodal Large Language Models",
            "authors": [
                "Wenxin Zhu",
                "Andong Chen",
                "Yuchen Song",
                "Kehai Chen",
                "Conghui Zhu",
                "Ziyan Chen",
                "Tiejun Zhao"
            ],
            "arxiv_id": "2511.12861v1",
            "summary": "With the remarkable success of Multimodal Large Language Models (MLLMs) in perception tasks, enhancing their complex reasoning capabilities has emerged as a critical research focus. Existing models still suffer from challenges such as opaque reasoning paths and insufficient generalization ability. Chain-of-Thought (CoT) reasoning, which has demonstrated significant efficacy in language models by enhancing reasoning transparency and output interpretability, holds promise for improving model reasoning capabilities when extended to the multimodal domain. This paper provides a systematic review centered on \"Multimodal Chain-of-Thought\" (MCoT). First, it analyzes the background and theoretical motivations for its inception from the perspectives of technical evolution and task demands. Then, it introduces mainstream MCoT methods from three aspects: CoT paradigms, the post-training stage, and the inference stage, while also analyzing their underlying mechanisms. Furthermore, the paper summarizes existing evaluation benchmarks and metrics, and discusses the application scenarios of MCoT. Finally, it analyzes the challenges currently facing MCoT and provides an outlook on its future research directions.",
            "headline_zh": "系统综述多模态思维链以增强多模态大语言模型的推理能力",
            "intro_zh": [
                "核心问题：多模态大语言模型推理路径不透明且泛化能力不足",
                "方法要点：从思维链范式、后训练和推理阶段分析多模态思维链机制",
                "实验或效果：总结评估基准、应用场景及未来研究方向"
            ],
            "tags_zh": [
                "多模态大语言模型",
                "思维链推理",
                "后训练方法",
                "推理阶段优化",
                "评估基准",
                "应用场景"
            ],
            "_index": 173
        }
    ]
}